# 2208.00748.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2208.00748.pdf
# File size: 835321 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Efﬁcient Long-Text Understanding with Short-Text Models
Maor Ivgi Uri Shaham Jonathan Berant
The Blavatnik School of Computer Science, Tel-Aviv University
{maor.ivgi,uri.shaham,joberant}@cs.tau.ac.il
Abstract
Transformer-based pretrained language
models (LMs) are ubiquitous across natural
language understanding, but cannot be
applied to long sequences such as stories,
scientiﬁc articles and long documents, due
to their quadratic complexity. While a
myriad of efﬁcient transformer variants
have been proposed, they are typically
based on custom implementations that
require expensive pretraining from scratch.
In this work, we propose SLED :SLiding-
Encoder and Decoder, a simple approach
for processing long sequences that re-uses
and leverages battle-tested short-text pre-
trained LMs. Speciﬁcally, we partition
the input into overlapping chunks, encode
each with a short-text LM encoder and use
the pretrained decoder to fuse information
across chunks ( fusion-in-decoder ). We
illustrate through controlled experiments
that SLED offers a viable strategy for
long text understanding and evaluate our
approach on SCROLLS, a benchmark
with seven datasets across a wide range
of language understanding tasks. We ﬁnd
that SLED is competitive with specialized
models that are up to 50x larger and require
a dedicated and expensive pretraining step.
1 Introduction
Transformer-based pretrained language models
(Vaswani et al., 2017; Devlin et al., 2019; Lewis
et al., 2020; Raffel et al., 2020b; Brown et al.,
2020) have been widely successful across all areas
of natural language understanding (NLU). How-
ever, applying them over long texts (such as sto-
ries, scripts, or scientiﬁc articles) is prohibitive
due to their quadratic complexity in the input
length. To bridge this gap, recent work has devel-
oped more efﬁcient transformer variants (Kitaev
et al., 2020a; Beltagy et al., 2020; Zaheer et al.,
2020a; Guo et al., 2022) and applied them over
100M 1B 10B 200M 400M 2B 4B 20B
# of parameters202530354045SCROLLS score
SLED
SLEDSLEDUL2LongT5baseLongT5largeLongT5XL
BARTbaseBARTlarge
T5baseLeverages existing pretrained models
Dedicated long-range pretraining
Standard short-range pretrainingFigure 1: Models’ SCROLLS score (Shaham et al.,
2022) as a function of parameter count. Plugging exist-
ing pretrained LMs into the SLED framework dramat-
ically improves their SCROLLS score (arrows from
blue circles to orange stars). Gray triangles indicate
models with dedicated pretraining for capturing long-
range dependencies. BART large-SLED is competitive
with LongT5 base(Guo et al., 2022) and UL2 (Tay et al.,
2022b) (which has 50x more parameters), and slightly
lags behind larger LongT5 models.
long-range language understanding tasks (Mehta
et al., 2022; Shaham et al., 2022).
However, most efﬁcient transformers use spe-
cialized architectures with custom implementa-
tions that are not guaranteed to scale as well as
vanilla transformers (Tay et al., 2022a). Moreover,
they require an expensive pretraining step and do
not exploit off-the-shelf pretrained LMs that were
trained for short texts. To date, their performance
on long texts has not matched the success of their
short-range counterparts.
In this work, we present SLED :SLiding-
Encoder and Decoder, a simple yet power-
ful method for applying off-the-shelf pretrained
encoder-decoder models on long text problems,
with a linear time and space dependency. Speciﬁ-
cally (see Fig. 2), we partition long documents into
overlapping chunks of tokens of constant length
and encode each chunk independently with anarXiv:2208.00748v3  [cs.CL]  27 Dec 2022

--- PAGE 2 ---
already-pretrained encoder. Then, a pretrained
decoder attends to all contextualized input repre-
sentations to generate the output. Our main as-
sumption is that input tokens can be contextual-
ized through their local surrounding (using a short-
text LM), and any global cross-chunk reasoning
can be handled by the decoder, similar to fusion-
in-decoder (FiD) (Izacard and Grave, 2021). Our
approach can be readily applied to anypretrained
encoder-decoder LM such as T5 (Raffel et al.,
2020b) and BART (Lewis et al., 2020) (but is not
applicable to decoder-only (Brown et al., 2020) or
encoder-only models (Liu et al., 2019; Conneau
et al., 2020)).
We evaluate SLED on a wide range of language
understanding tasks. To substantiate SLED’s ade-
quacy for text processing, we perform controlled
experiments over modiﬁed versions of SQuAD
1.1 (Rajpurkar et al., 2016) and HotpotQA (Yang
et al., 2018) to show that SLED can (a) ﬁnd
relevant information that is embedded within a
long text sequence and (b) fuse information from
chunks that were encoded separately.
Our main evaluation is over SCROLLS, a
recently-released benchmark that includes 7 long-
range tasks across Question Answering (QA),
Summarization, and Natural Language Inference
(NLI). We show (Fig. 1) that taking a pre-
trained encoder-decoder model, such as BART
(Lewis et al., 2020) or T5 (Raffel et al., 2020b),
and embedding it into SLED’s framework re-
sults in dramatic improvement in performance (6
points on average across models). Moreover,
BART large -SLED’s performance is comparable
to LongT5 base(Guo et al., 2022), a model that was
speciﬁcally pretrained to handle long-range de-
pendencies, and surpasses UL2 (Tay et al., 2022b),
which contains 50x more parameters. Importantly,
SLED-based models can use any future pretrained
LM out-of-the-box without requiring additional
pretraining to further improve performance.
Due to its simplicity, SLED can also be used
as a diagnostic tool for analyzing long-range
benchmarks. We analyze the seven datasets in
SCROLLS through the lens of SLED and show
which datasets require the input to be contextual-
ized with remote tokens. Speciﬁcally, we ﬁnd that
in QA and NLI tasks, relatively local contextual-
ization is sufﬁcient for high performance.
While SLED is similar to FiD from a techni-
cal standpoint, past usage of FiD has centeredaround open-domain question answering (Izacard
and Grave, 2021), where unrelated passages are
naturally encoded independently. Here, we test
fusion-in-decoder on long documents, where local
encoding of chunks is a modeling assumption that
needs testing. In recent work, Vig et al. (2022)
proposed a similar architecture to tackle long in-
puts from QMSum (Zhong et al., 2021), but did
not systematically analyze it. We standardize this
methodology for the ﬁrst time, and extensively an-
alyze the effectiveness of FiD for encoding long
documents across multiple tasks.
To summarize, our main contributions are:
1. We present SLED, a simple and effec-
tive approach for processing long texts that
leverages off-the-shelf encoder-decoder LMs
based on fusion-in-decoder.
2. We demonstrate SLED’s efﬁcacy in both
controlled experiments, as well as on the
SCROLLS benchmark, which leads to com-
petitive results compared to specialized mod-
els that include up to 50x more parameters.
3. We use SLED as a diagnostic tool for ana-
lyzing the long-range properties of datasets
in the SCROLLS benchmark.
4. We provide an open-source implementation
of SLED,1seamlessly integrated into the
Transformers library (Wolf et al., 2020).
2 Background
Recent advances in natural language processing
have been by and large fueled by the transformer
architecture (Vaswani et al., 2017). A core compo-
nent of the transformer is the self-attention layer
where every input token “attends” to every other
token to produce its contextualized representation.
This results in quadratic time and space depen-
dency w.r.t. the length of the input, limiting the
ability of transformers to process long sequences.
This long-text limitation has sparked ample in-
terest in developing efﬁcient transformer variants.
One prominent family of methods is based on
sparse attention , where each token attends to a
constant number of other tokens, overcoming the
quadratic dependency. Tokens typically attend
either to their local surrounding (Zaheer et al.,
2020a; Beltagy et al., 2020; Ainslie et al., 2020;
Gupta and Berant, 2020) or to tokens that are se-
mantically similar (Kitaev et al., 2020b; Roy et al.,
1https://github.com/Mivg/SLED

--- PAGE 3 ---
Figure 2: Overview of SLED. (a) Input tokens (t1;:::;tn)are chunked into Coverlapping chunks of length c
(here,c= 4). Each chunk is made of P:=c
2context padding tokens at the right and left edges of the chunk,
and(1 )ceffective chunk tokens in the middle (here, = 0:5;P= 1). (b) We prepend the preﬁx tokens
(p1;:::;pm)to each chunk ( mn). (c) Each chunk is encoded independently using the already pretrained back-
bone encoder Menc. (d) We gather the encoded effective chunks tokens (yellow) and discard the context padding
tokens (pink) (e) We pass the encoded input to the decoder to generate the ﬁnal output sequence (o1;:::;ok).
2021). Moreover, a constant number of global to-
kens that attend to and are attended by all input
tokens are often added to each attention sub-layer.
Recent analyses (Xiong et al., 2022a) have shown
that sparse transformers with local attention are
competitive with other variants on multiple lan-
guage understanding tasks.
Our method, SLED, falls into the family of
local attention variants. However, unlike prior
work, SLED re-uses and extends existing short-
range encoder-decoder models, and does not re-
quire specialized pretraining or dedicated CUDA
implementations.
In most local attention variants, e.g., LED
(Beltagy et al., 2020), attention is local per-layer ,
but the receptive ﬁeld of tokens grows across lay-
ers. In SLED, which we describe next, tokens
have access to the same number of tokens, inde-
pendent of a layer’s depth, which enables better
parallelization. For a survey on the families of ef-
ﬁcient transformers, see Tay et al. (2020). For an
in-depth comparison of SLED and LED, we refer
to Appendix B.
3 Method
In this work, we propose a simple approach for
avoiding transformer’s quadratic complexity, mo-
tivated by the Locality of information assumption :
In an encoder-decoder architecture, the
encoder can effectively contextualize in-
put tokens with local context only, leav-
ing long-range dependencies to be han-
dled by the decoder.
SLED relies on said modeling assumption toencode shorter chunks independently and perform
fusion of information in the decoder (Izacard and
Grave, 2021). We now describe the SLED model
in detail.
Input SLED uses a pretrained encoder-decoder
modelMas a backbone. SLED receives a to-
kenized document of length n(blue squares in
Fig. 2), and an optional short tokenized preﬁx of
lengthmn, typically representing a ques-
tion about the document, an instruction to per-
form some generation task, or a hypothesis (or-
ange squares in Fig. 2). Unlike static task-speciﬁc
preﬁxes (e.g., “summarize”), SLED supports also
sample-speciﬁc preﬁxes that are part of the input
(e.g., the question in QA datasets).
Steps SLED follows the following steps:
(a) Document tokens are split into Cchunks of
lengthc(In Fig. 2, c= 4 ). The middle
(1 )ctokens in each chunk are con-
textualized from both the left and right by
P:=c
2tokens, where 2[0;0:5](= 0:5
in Fig. 2). We call these middle tokens the ef-
fective chunk , since they will constitute the
output of the encoder, and term the tokens on
each side by context padding .
(b) Each chunk is prepended by (optional) preﬁx
tokens (Fig. 2(b)).
(c) Each chunk is encoded independently, using
the backbone encoder Menc(see Fig. 2(c)).
(d) To create a contextualized representation for
each token, we keep from each chunk only

--- PAGE 4 ---
the tokens from the effective chunk , and con-
catenate them (Fig. 2(d)).
(e) To give the decoder access to preﬁx tokens,
we encode the preﬁx tokens with Menc, and
prepend the result to the contextualized rep-
resentation (leftmost chunk in Fig. 2(a)-(d)).
(f) Finally, we generate the output with the back-
bone decoder, Mdec, which uses standard
cross-attention over the m+nencoded to-
kens (Fig. 2(e)).
SLED requires handling a few edge cases,
namely, dealing with the ﬁrst and last chunk that
do not have bidirectional context. We refer to Ap-
pendix A for these details.
SLED’s Complexity SLED divides an input of
lengthntoCchunks of size c. Since2[0;0:5],
it follows that C2n
c;2n
c
. While the complex-
ity of encoding each chunk is quadratic in cdue
to self-attention, cnis constant and thus the
memory and compute dependency is linear in n.2
In particular, the complexity to encode the input
with a model of lattention layers is:
O
lc22n
c
=O(lcn):
Decoding is done as proposed by Vaswani et al.
(2017), thus requiring O(nk+k2)memory. As-
suming a constant output sequence length kn,
this remains linear in n.
4 Efﬁcacy of Fusion in Decoder
As mentioned (§3), SLED relies on the assump-
tion that chunks can be encoded independently and
fusion across them can be delegated to the de-
coder ( Locality of information assumption ). This
is similar to the Fusion-in-Decoder approach, in-
troduced by Izacard and Grave (2021) for open-
domain question answering (ODQA). However,
there, the encoder-decoder receives a set of inde-
pendent passages and needs to generate an answer
that can typically be extracted from a single pas-
sage. Here, we extend the scope of FiD by apply-
ing it over a single, long, and coherent input that
potentially requires global contextualization.
To demonstrate the viability of FiD for long text
language tasks, we design two controlled experi-
ments that quantify the extent to which FiD can
2We assume the preﬁx length ( m) is negligible and thus
its effect on asymptotic complexity is negligible.perform two operations at the heart of long-text
processing. First, can FiD ﬁnd a “needle-in-a-
haystack”, i.e., locate a piece of short information
embedded in long text, disregarding irrelevant in-
formation. Second, can FiD “piece the puzzle”
and fuse two pieces of information that are en-
coded independently when generating an output.
4.1 Needle in a haystack
To check if SLED can ignore irrelevant text and
locate a single piece of information, we cast
SQuAD 1.1 (Rajpurkar et al., 2016) as a sequence-
to-sequence task with long input. SQuAD is
a question answering dataset, where given a
question-paragraph pair the goal is to generate the
answer (which lies within the paragraph). For each
question-paragraph pair, we randomly sample 9
other paragraphs from the the dataset and concate-
nate them to form a long document.3We then ﬁne-
tune and evaluate our models in two settings: a)
Ordered Distractors : the gold paragraph is the ﬁrst
one, and all other distractors are concatenated af-
ter it. b) Shufﬂed Distractors : we randomly shuf-
ﬂe the order of all paragraphs so the answer can
be anywhere in the input document. Since this is a
QA task, the preﬁx is the question.
We use BART base (Lewis et al., 2020) as our
backbone model, M, throughout §4, and compare
SLED to an oracle BART base that is given the
gold paragraph only with no distractor paragraphs.
this is an oracle setup since BART base can take
1,024 tokens as input and all gold paragraphs are
shorter. If SLED can match the oracle perfor-
mance, we can infer that indeed the decoder can
ﬁnd a needle in a haystack. In addition, we com-
pare SLED to BART basewhich is given only the
ﬁrst 1K tokens and to LED (Beltagy et al., 2020),
which uses local sparse attention, similar to SLED
(LED has the same backbone BART base). How-
ever, as explained in §2, the receptive ﬁeld of LED
layers linearly grows with the number of layers,
and thus information can be fused in the encoder,
unlike SLED where cross-chunk fusion must be
delegated to the decoder. Last, for QA tasks, LED
deﬁnes the question tokens as global tokens , and
as an additional sanity test we evaluate LEDL, i.e.,
a local LED model where no global tokens are
used. For both LED and SLED we use a chunk
sizec= 256 .
3We only consider paragraphs that are not within the gold
document and do not contain the gold answer.

--- PAGE 5 ---
SLED
LED
LED
BART(a)
0.020.040.060.080.088.1100.0F187.6 87.983.087.5 87.284.1
58.855.0Ordered
Shuffled
SLED
c=1SLED
chunks withno prefixSLED
drop prefix(b)
0.020.040.060.080.088.1100.0F1
0.164.787.4
0.363.487.1Figure 3:F1results on our modiﬁed SQuAD 1.1’s (Ra-
jpurkar et al., 2016) development set evaluation: (a)
the horizontal line gives the performance of an ora-
cle BART basegiven the gold paragraph only. SLED
matches oracle performance in both the ordered and
shufﬂed setting (see text). LED slightly underperforms
SLED in the shufﬂed setup. Both BART (given only
the ﬁrst 1K tokens) and LED with no global tokens
(LEDL) performs poorly in the shufﬂed setup. (b) Ab-
lations on SLED’s architecture, see §4.3 for details.
Results Fig 3(a) shows the results of our eval-
uation on the development set. SLED almost
matches the performance of an oracle BART base
that is not given any distractor paragraphs, reach-
ing an F 1score of 87.6 compared to the oracle F 1
of 88.1 (horizontal line in the ﬁgure). LED also
achieves high performance (but lower than SLED
in the shufﬂed setup), showing both models learn
to ignore distracting information and ﬁnd a needle
in a haystack. As expected, both LEDLand BART
suffers a signiﬁcant drop in performance when the
passages are shufﬂed, as the gold paragraph is not
contextualized with the question.
4.2 Piecing a puzzle
We now verify that SLED can fuse pieces of in-
formation from different chunks. To this end, we
modify HotpotQA (Yang et al., 2018), a multi-
hop question answering dataset, in which every
SLED
LED
LED
BART
second passage(a)
0.0010.0020.0030.0040.0050.0060.0070.0078.57F176.572.977.4
67.2
SLED
c=1SLED
chunks withno prefixSLED
drop prefix(b)
26.269.576.6Figure 4:F1results on our HotpotQA’s development
set (Yang et al., 2018). (a) SLED reaches an F 1that is
close to the oracle BART base(horizontal line), outper-
forming a model with access to the paragraph that con-
tains the answer (“second paragraph”). This shows that
SLED effectively fuses information from two chunks.
See text for further explanation on each model. (b) Ab-
lations on SLED’s architecture, see §4.3 for details.
question relies on two pieces of information (lo-
cated in different paragraphs). While in the origi-
nal setting, each input in HotpotQA has two gold
paragraphs and 8 distractor paragraphs, we include
only the two gold paragraphs in our experiments.
To ensure SLED and LED encode the relevant
two pieces of information in separate chunks, we
set the chunk size to c= 128 .
Similar to §4.1, we compare SLED to an oracle
BART base with full attention over 1,024 tokens,4
to LED, and to LEDL. Finally, past work has
shown that many examples in HotpotQA can be
answered with access to the “second” gold para-
graph only, which contains the answer (Jiang and
Bansal, 2019). Thus, we also evaluate a BART
model that is given the second passage only.
Results Fig. 4(a) shows that indeed, SLED’s de-
coder can effectively fuse information from two
separately encoded chunks, reaching an F 1of
76.5, slightly lower than the oracle F 1of 78.6. No-
tably, SLED substantially outperforms a BART
model with access to the entire second paragraph,
showing that information is fused by the decoder.
LED slightly outperforms SLED, but when de-
nied access to global tokens (LEDL) its perfor-
mance drops sharply. This shows that the large
receptive ﬁeld of deep LED layers does not sufﬁce
for information fusion and interaction between the
question and text is crucial for the decoder.
To summarize, our two controlled experiments
show that SLED can perform the operations of re-
4All examples have 1,024 tokens, including the preﬁx.

--- PAGE 6 ---
trieving and fusing information, which are funda-
mental for long text language tasks.
4.3 Ablations of design choices
We leverage our controlled experimental setup to
further investigate the components of SLED.
Efﬁcacy of the encoder While §4.2 shows that
SLED can fuse separate pieces of information in
the decoder, it is not clear to what extent local con-
textualization is necessary. To check whether it is
possible for all fusion to occur in the decoder, we
ﬁnetune SLED with a chunk size of c= 1, such
that input tokens do not observe any context in the
encoder. As can be seen in the leftmost bar(s) in
Fig. 3(b) and Fig. 4(b), removing local contextu-
alization results in poor performance, illustrating
the importance of local contextualization.
Contextualizing chunks with a preﬁx As ex-
plained, SLED does not use global tokens, but in-
stead contextualizes each chunk with a prepended
preﬁx. To verify its necessity, we ﬁnetune a SLED
model that treats the preﬁx as another chunk and
does not prepend it to document chunks.5The sec-
ond bar(s) in Fig. 3(b) and Fig. 4(b) shows a sig-
niﬁcant drop in performance for all settings, sug-
gesting the preﬁx is needed during encoding.
As expected, there is practically no difference
between the Ordered and Shufﬂed settings in
Fig. 3(b). In contrast, LEDLwhich is similar in
concept (due to the lack of global tokens) shows
a signiﬁcant drop when paragraphs are shufﬂed.
This shows the possible effectiveness of the in-
creased receptive ﬁeld in LED, but only when the
gold paragraph is relatively close to the preﬁx.
Encoding the preﬁx After showing the preﬁx is
crucial for the encoder, we ask whether the de-
coder needs direct access to the preﬁx or whether
relevant information from the preﬁx can be in-
fused into the chunk representations. To test that,
we ﬁnetune SLED as usual, but remove the pre-
ﬁx tokens from the ﬁnal representation given to
the decoder. The rightmost bar(s) in Fig. 3(b) and
Fig. 4(b) shows that providing the decoder with
preﬁx representations makes little difference if any
at all, suggesting that indeed the encoder can in-
fuse the important information from the preﬁx into
the encoded document tokens.
5We add masked padding after the preﬁx to ensure chunk-
ing of the document remains identical.5 Experiments
We evaluate SLED on SCROLLS (Shaham et al.,
2022), a recently-proposed benchmark for evaluat-
ing long text understanding. SCROLLS contains
seven datasets that span three different language
understanding tasks:
1. Summariazation: GovReport (Huang et al.,
2021) is a summarization task over reports
from the Congressional Research Service;
SummScreenFD (Chen et al., 2022) is a sum-
marization dataset over TV scripts; QMSum
(Zhong et al., 2021) is a query-based sum-
marization dataset over meeting transcripts
from various domains. While GovReport and
SummScreenFD do not contain a preﬁx, for
QMSum we consider the query as the preﬁx.
2. Question answering (QA): Qasper (Dasigi
et al., 2021) is a QA benchmark that con-
tains questions over NLP papers; Narra-
tiveQA (Koˇciský et al., 2018) contains ques-
tions over entire books and movie scripts;
QuALITY (Pang et al., 2022) is a multiple-
choice QA dataset over books and articles.
For all QA datasets, we set the question as
the preﬁx. For QuALITY , we consider the
four answer options part of the question.
3. Natural language inference: ContractNLI
(Koreeda and Manning, 2021) contains short
legal hypotheses (set as the preﬁx) and legal
documents as the premise. Models are tasked
to predict whether the premise entails, con-
tradicts or is neutral w.r.t. to the hypothesis.
For each task, we use the ofﬁcial evaluation
metrics deﬁned in SCROLLS, which are based on
the metrics from the original datasets.
5.1 Settings
We evaluate SLED with both BART (Lewis et al.,
2020) and T5 (Raffel et al., 2020b) as backbone
models. For each backbone model, we com-
pare performance with SLED, which can con-
sume long sequences, vs. the backbone models
alone that are fed with the ﬁrst 1,024 tokens. For
comparison, we also ﬁnetune LED base. In all
SLED and LED experiments, we use a maximal
sequence length of 16K tokens and chunk size of
256 to allow for a fair evaluation.
For each model-dataset pair, we run hyperpa-
rameter tuning (detailed in Appendix C) based on

--- PAGE 7 ---
the development set. Additionally, we submit gen-
erated predictions over the test set to SCROLLS
leaderboard,6and compare to the reported perfor-
mance of other models at the time of submission.
5.2 Results
Tab. 1 reports results over SCROLLS devel-
opment and test sets. Taking short-range pre-
trained LMs like BART and T5 and casting them
into SLED’s framework allows them to process
long documents effectively, improving the aver-
age SCROLLS score by 4.8-7 points. Examining
BART base-SLED, we see a large improvement
compared to LED base (33.6!35.4), and compet-
itive performance on multiple tasks compared to
LongT5 base and UL2. Moreover, adding SLED
to BART large results in a high-performing model
with results that are comparable to LongT5 base
and outperforming UL2, despite UL2’s large pa-
rameter count (50x larger), and with no need for
expensive pretraining geared towards long-range
tasks. BART large -SLED’s performance is mod-
erately lower than the larger LongT5 models.
Barring QuALITY , SLED signiﬁcantly im-
proves performance across all tasks compared to
the corresponding backbone models. All sum-
marization datasets (GovReport, SummScreenFD
and QMSum) show impressive gains of up to 35%
compared to their baseline scores, across all met-
rics (Rouge-1/Rouge-2/Rouge-L (Lin, 2004)) and
for all three backbone models. Similarly, on Con-
tractNLI (Koreeda and Manning, 2021) we see
large relative improvements. As the performance
of the baseline models was already high, this boost
in performance is even more signiﬁcant. Finally,
the QA datasets Qasper and NarrativeQA show the
largest gains, improving by an average of 60%.
QuALITY In stark contrast to other datasets
lies the multi-choice QA dataset QuALITY
(Pang et al., 2022). While the performance of
BART large -SLED is above chance, it barely im-
proves the performance of its backbone model
(BART large ), which observes only the ﬁrst 1K to-
kens, with a similar trend in other backbone mod-
els. Analyzing test scores in Tab. 1, we see that
increasing model size consistently improves per-
formance (up to 46% exact match), but increas-
ing input length has a negligible effect. Since
reported human accuracy on QuALITY is high
6https://www.scrolls-benchmark.com/
leaderboard(93.5%), this hints that QuALITY might require
commonsense reasoning and knowledge that are
absent from models with a lower parameter count.
Summary We have shown that taking off-
the-shelf pretrained LMs and embedding them
into SLED leads to competitive performance on
SCROLLS. Importantly, any future pretrained
LM can be easily plugged into SLED, without the
need for an expensive pretraining step.
5.3 Datasets analysis
SLED’s simplicity and modularity allow it to be
used as a useful tool for dataset analyses. Speciﬁ-
cally, we can vary the chunk size, c, and the num-
ber of tokens, n, across datasets to analyze a) how
local are individual pieces of relevant information,
and b) how far into the document they are located.
Locality of information SLED relies on an as-
sumption that information can be contextualized
locally at encoding time. To analyze locality, we
vary the chunk size, c, which deﬁnes the attention
window, and measure the effect on SCROLLS
datasets with input length 16K. Fig. 5 shows the
results of this experiment, where the y-axis shows
the relative improvement compared to BART base
on a target metric as a function of the chunk size
cfor all datasets. We observe that in all datasets
the best performing chunk size is relatively small
(up to 256), and further increasing ceven hurts the
performance in some cases. However, the summa-
rization datasets show a much larger gain in per-
formance when increasing cup to that threshold.
This coincides with a common hypothesis that QA
and NLI require relatively local context, and thus
increasingccan add noise and hurt optimization,
while summarization may require a more high-
level view of information.
Distance from start of document We now an-
alyze whether indeed the entire document is re-
quired for tasks in SCROLLS by varying the
maximum document length, n. Fig. 6 shows the
results of this experiment, where the y-axis shows
relative improvement of BART base-SLED com-
pared to BART baseas a function of the ﬁrst nto-
kens from the document (chunk size c= 256 ). As
expected, all datasets (except QuALITY) show a
roughly monotonic improvement in performance
withn. This shows that (a) SLED is able to ef-
fectively use all of the information in a long se-

--- PAGE 8 ---
Model (Chunk/Input) #Params AvgGovRep SumScr QMSum Qspr Nrtv QALT CNLI
ROUGE-1/2/L ROUGE-1/2/L ROUGE-1/2/L F1 F1 EM-T/H EM
Development Scores
LED base (256=16K) 162M - 57.3/27.9/30.0 30.7/6.3/17.9 32.5/9.0/21.1 30.4 20.2 30.9 82.3
T5base (1K=1K) 220M - 32.8/11.7/20.2 22.2/3.7/15.3 26.1/6.6/19.8 13.2 14.9 35.1 76.8
T5base-SLED (256=16K) 220M - 47.0/20.2/25.2 25.3/5.0/16.6 29.9/8.7/21.4 38.2 18.2 34.6 82.4
BART base (1K=1K) 139M - 47.7/18.5/22.3 30.1/7.0/18.3 32.2/9.3/21.1 23.3 15.9 33.8 78.4
BART base-SLED (256=16K) 139M - 55.7/24.8/25.8 33.6/8.5/19.2 34.4/11.5/22.7 35.8 21.3 33.7 85.3
BART large (1K=1K) 406M - 50.6/19.8/23.5 32.1/7.4/18.7 33.3/9.4/21.6 24.5 17.9 36.1 79.3
BART large-SLED (256=16K) 406M - 57.4/26.3/27.5 35.3/8.8/19.5 36.3/12.2/23.3 42.5 23.6 37.2 85.3
Test Scores
LED base (256=16K) 162M 33.6 56.8/ 27.3/29.2 30.0/6.0/17.5 31.3/8.6/20.5 34.8 21.0 28.5/28.3 82.9
T5base (1K=1K) 220M 26.3 33.2/12.1/20.4 21.4/3.6/15.0 24.2/5.9/18.6 16.3 15.0 31.9/28.6 76.3
T5base-SLED (256=16K) 220M 33.3 46.6/20.1/25.1 24.5/4.6/16.5 28.4/8.7/20.5 43.0 18.9 31.2/29.4 81.4
BART base (1K=1K) 139M 30.6 48.0/19.1/22.7 30.1/6.6/18.1 31.2/9.1/20.3 27.6 16.0 32.5/31.6 77.1
BART base-SLED (256=16K) 139M 35.4 54.7/24.4/25.4 32.7/7.9/19.1 33.8/ 11.7/22.6 41.1 21.5 29.7/30.4 85.6
BART large (1K=1K) 406M 32.1 50.7/20.1/23.5 31.6/6.8/18.5 32.0/9.1/20.8 29.2 18.3 34.8/33.9 79.7
BART large-SLED (256=16K) 406M 38.0 57.5 /26.3/27.4 35.2/8.7/19.4 34.2 /11.0/22.0 46.9 24.1 34.8/34.8 87.3
LEDSCROLLS y
base (1K=16K) 162M 29.2 56.2/26.6/28.8 24.2/4.5/15.4 25.1/6.7/18.8 26.6 18.5 25.8/25.4 71.5
LongT5y
base (255=16K) 220M 38.2 53.5/27.3/29.3 34.8/ 9.6/21.1 33.9/11.0/22.8 46.6 23.0 37.9/36.6 85.6
LongT5y
large (255=16K) 770M 40.5 54.2/27.8/29.8 35.6/9.2/ 21.2 35.1/ 12.0/23.3 52.3 27.2 40.6/38.6 87.3
LongT5y
XL (255=16K) 3B 41.9 54.7/ 28.2/30.2 35.8/9.6 /21.1 34.9/11.8/ 23.5 53.1 29.3 46.0/42.1 88.2
UL2y(2K=2K) 20B 37.9 53.6/26.1/28.8 32.9/7.8/19.4 31.1/8.5/20.4 37.6 24.2 45.8/40.7 88.7
Table 1: Main results on the SCROLLS benchmark. Chunk/Input refers to the chunk size used ( c) and to the
maximal input length ( n). Avg is the average SCROLLS score as described in Shaham et al. (2022). Development
scores for QuALITY are only for the full set (T). yindicates reported results from SCROLLS public leaderboard.6
LEDSCROLLS
base scores were reported by Shaham et al. (2022) and are lower than our LED baseimplementation, pre-
sumably since our implementation uses all question tokens for global attention rather than just the ﬁrst one. The
results for LongT5 and UL2 were submitted to the SCROLLS leaderboard by their authors.
quence (up to 16K tokens),7and that (b) observing
the entire inputs from SCROLLS improves per-
formance.
5.4 Effect of context padding
In all experiments thus far, we used a conserva-
tive padding value = 0:5, resulting in effective
chunk size ofc
2andc
4context padding tokens on
each side. Since both memory and, more impor-
tantly, the number of forwards passes through the
encoder are linear in the number of chunks, a natu-
ral question is how much padding and overlap are
necessary to achieve satisfactory results.
To explore this, we ﬁnetune BART base-SLED
on all six datasets where SLED showed gains
over its baseline model (i.e., all datasets except
for QuALITY), varying the value of , and ﬁxing
c= 256 . Tab. 2 shows the results of this exper-
iment, where we compare relative gain compared
to BART baseacross different values.
As expected, decreasing the padding factor and
consequently the number of chunks reduces train-
7For ContractNLI, the length of over 95% of the tokenized
examples is less than 8K.ing time. When = 0:05training can be faster
by up to 2x compared to = 0:5as the number
of chunks drops to almost half. Moreover, rela-
tive gain (i.e., improvement relative to the base-
line) is often close to or even higher with less
padding (perhaps due to better encoding or more
stable optimization). Nevertheless, there is no sin-
glevalue that consistently beats the conservative
choice of= 0:5. In particular, in all six datasets,
setting= 0:5results in a top-2 performance,
often by a large margin and never considerably
worse then the best result. Thus, we conclude that
one may improve the efﬁciency and performance
of SLED by tuning the hyperparameter for op-
timal behaviour w.r.t. a speciﬁc task, and we ﬁx
= 0:5in our experiments.
Moreover, Tab. 2 demonstrates the importance
of having chunks at least partially overlapping. In
all six dataset, using non-overlapping chunks ( =
0) results in a drop of at least 10% gain compared
to the best setting, where in some cases this gap
grows to over 50%. This supports our hypothesis
that chunking inputs with no overlap may lead to
crucial loss of information.

--- PAGE 9 ---
101520253035Relative improvmentGovReport SummScreen QMSum
0 200 400 600 800 1000
Chunk size (c)10
01020304050Relative improvmentNarrativeQA
QasperQuALITY
ContractNLIFigure 5: BART base-SLED relative improvement
compared to BART base results, when varying the
SLED’s chunk size (i.e. c), ﬁxing the maximum input
length to 16K. Top: Summarization datasets. The y-
axis measures relative improvement of Rouge-2. Bot-
tom: QA and NLI datasets. The y-axis measures rel-
ative improvement of exact match for QuALITY and
ContractNLI and F 1for NarrativeQA and Qasper.
6 Related Work
Efﬁcient transformers Many efﬁcient attention
variants were proposed in recent years, to alleviate
the quadratic complexity of dense attention (Tay
et al., 2020; Fournier et al., 2021). Among those
are clustering vectors to distinct buckets, calculat-
ing attention only within each one (Kitaev et al.,
2020a), attending only to a ﬁxed number of hidden
vectors (Ma et al., 2021), using random features
to approximate the attention matrix (Choroman-
ski et al., 2021; Peng et al., 2021), and using low-
rank factorizations (Wang et al., 2020). Despite
achieving respectable performance when ﬁnetun-
ing these models on the Long Range Arena bench-
mark (Tay et al., 2021), many of them were not yet
proven to work well as a backbone for pretrained
language models. In fact, recent work (Xiong
et al., 2022b) on encoder-only models found many
donotoutperform a simple local attention sliding
window on downstream language tasks. We dis-
cuss such methods next.
10
0102030Relative improvmentGovReport SummScreen QMSum
29
210
211
212
213
214
Max input length (n)02040Relative improvmentNarrativeQA
QasperQuALITY
ContractNLIFigure 6: BART base-SLED relative improvement
compared to BART baseresults, when varying the input
length fed to SLED, ﬁxing c= 256 .Top: Summariza-
tion datasets compared w.r.t. Rouge-2. Bottom : QA
and NLI datasets. Relative improvment is measured
w.r.t. exact match for QuALITY and ContractNLI and
F1for NarrativeQA and Qasper.
Sparse attention variants A popular and sim-
ple solution for allowing attention-based models
to process long sequences is to use local attention,
where each token attend to a local window around
it. Longformer (Beltagy et al., 2020), GMAT
(Gupta and Berant, 2020), and ETC (Ainslie et al.,
2020) use short windows of full attention, com-
bined with full attention to a small number of
predeﬁned global input tokens. BigBird (Zaheer
et al., 2020b) shares the local and global features,
and additionally randomly samples tokens to at-
tend to. Finally, the recently proposed LongT5
(Guo et al., 2022) extends T5 (Raffel et al., 2020a)
with local and global attention components based
on ETC, relieving the need to manually specify
global tokens. In this work, we demonstrate that
a simple sliding window with off-the-shelf models
without any modiﬁcations is a strong alternative
for multiple generative tasks that require process-
ing long documents.
Beyond transformers As an alternative to
transformers for processing long sequences, Gu
et al. (2021) proposed the Structured State Space

--- PAGE 10 ---
Relative Gain
GovRep SumScr QMSum Qspr Nrtv CNLI
50% 34.1% 21.0% 22.8% 53.7% 34.2% 8.9%
25% 28.5% 19.0% 17.9% 54.7% 29.4% 10.1%
5% 18.7% 15.9% 23.5% 52.0% 31.9% 7.4%
0% 27.1% 9.5% 11.5% 46.1% 29.2% 6.9%
Table 2: BART base-SLED relative improvement com-
pared to BART basewhen varying the padding percent-
age (). In all cases the maximum input length is 16K
andc= 256 . Relative gain is measured w.r.t. Rouge-
2 for GovReport, SummScreenFD and QMSum, F 1
for Qasper and NarrativeQA and exact match for Con-
tractNLI. In each column, boldface marks the top per-
forming value and underline the second-best.
(S4) architecture showing dramatic gains over
transformers on the LRA benchmark (Tay et al.,
2021). State space models are now an active re-
search ﬁeld (Gupta, 2022; Mehta et al., 2022), but
their efﬁcacy on long-range language understand-
ing tasks has not been tested yet.
Fusion-in-Decoder Izacard and Grave (2021)
proposed to encode multiple independent passages
separately, and concatenate the encodings prior to
the decoding phase. Despite encouraging empiri-
cal evidence (Amouyal et al., 2022; Yavuz et al.,
2022), we are the ﬁrst (to our knowledge) to an-
alyze FiD’s feasibility and limitations in a con-
trolled setting. Importantly, we test FiD on long-
range tasks over a single long document, rather
than a collection of independent passages.
Pretrained models with sliding windows
Wrapping a BERT encoder within a sliding
window was proposed by Cui and Hu (2021)
in the context of a specialized architecture for
summarization. Wang et al. (2019) showed that
sliding BERT across text improves performance
on several QA datasets. In this work, we propose
a sliding window approach that can be easily
plugged into any existing encoder-decoder model
without additional parameters or task-speciﬁc
training, and show its efﬁcacy for long-range
text understanding. Most similar to SLED, is
the S EGENCapproach proposed by Vig et al.
(2022). By dividing inputs from QMSum into
overlapping chunks, encoding them separately,
and then performing FiD (using two representa-
tions for every input token), the authors were able
to achieve state-of-the-art results. However, Vig
et al. (2022) were focused on summarization and
did not perform a systematic analysis of this typeof architecture.
7 Limitations
We present SLED as a simple and effective
method to extend the capabilities of pretrained
short-text models to long-text tasks. Despite its
impressive empirical performance on SCROLLS,
SLED suffers from two disadvantages which may
limit its applicability to some long-range tasks.
Long output To obtain linear complexity,
SLED assumes the output length kis constant.
This is since the decoder uses quadratic self-
attention over the output, on top of O(nk)cross-
attention between the output and input. While
most current long-text tasks follow this assump-
tion, future tasks, such as academic reports or
script writing, may require long text generation.
This limitation is not unique to SLED and affects
other long-range transformers including LongT5
and LED. Aside from ﬁnetuning, this also af-
fects pretraining models on long inputs with self-
supervised losses such as span-corruption (Raffel
et al., 2020b) or denoising (Lewis et al., 2020),
which require the decoder to process an output that
is linear in the length of the input.
Co-reference resolution and fact retention An
assumption at the heart of SLED is the Locality
of information assumption . When the input text is
long, this assumption may break if distant entity
resolution or factual knowledge are required. For
example, a chapter in a book may mention “they
were walking into the room” when knowledge of
what room or who walked is located a few chapters
back. In such cases, the encoder used by SLED
will not be able to access this information, moving
more responsibility to the decoder and reducing
the effectiveness of the contextual encoding. Sim-
ilarly, in multi-hop questions (Yang et al., 2018),
attending to one part of the context is necessary in
order to fully understand the question and encode
a second piece of information correctly. As the
encoder will not have access to the ﬁrst context
that leads to better question understanding, here
as well more responsibility is delegated to the de-
coder.
8 Conclusions
In this work we present SLED, a simple approach
for modeling long texts which slides a pretrained
short-range encoder over a long input document

--- PAGE 11 ---
and then generates an output by attending to the
encoded tokens. We show SLED can perform
core operations that are important for long text
understanding, such as ﬁnding relevant pieces of
information and fusing them at decoding time,
and demonstrate competitive performance on the
SCROLLS benchmark compared to larger mod-
els and models that employ a dedicated and ex-
pensive pretraining step.
One of SLED’s most attractive features is that
it can be readily used with any short-range pre-
trained LM. Thus, any future encoder-decoder
model can be ﬂexibly plugged into it to achieve
further gains in performance on SCROLLS, some
of its tasks, or any other long-range task.
We open source SLED and hope it encourages
the research community to easily extend to longer
inputs and push the borders of NLU models’ ap-
plicability in real-world use-cases.
Acknowledgements
This research was partially supported by The Yan-
dex Initiative for Machine Learning, the Shashua
Fellowship, the Len Blavatnik and the Blavatnik
Family foundation, and the European Research
Council (ERC) under the European Union Hori-
zons 2020 research and innovation programme
(grant ERC DELPHI 802800). We would also like
to thank our action editor and the anonymous re-
viewers for their insightful suggestions and feed-
back. This work was completed in partial fulﬁll-
ment for the Ph.D. degree of the ﬁrst author.
A SLED implementation details
While §3 details SLED’s method it leaves out
dealing with the edge tokens for brevity. Encoding
the ﬁrst and lastc
2input tokens requires special
attention, as they lack bidirectional context. To
preserve as much commonality between chunks,
all ﬁrst(2 )c
2tokens are considered the effective
chunk tokens in the ﬁrst chunk. To account for the
ﬁnal tokens, the last chunk will always start at to-
kentn c+1so it would contain exactly ctokens,
and its effective chunk tokens will be deﬁned as all
tokens that were not part of any previous effective
chunk .
B Chunking vs. local-attention
Both LED and SLED are long-range models built
on top of the same short-text model (BART), and
employ local attention. However, SLED relies onchunking, while LED uses per-layer local atten-
tion. In this section, we now discuss in more detail
the relation between the two approaches.
Implementation One of SLED’s biggest ad-
vantages is that it is agnostic to the backbone
encoder-decoder model, and can extend any ex-
isting model without additional implementation
overhead. In contrast, The attention mechanism
in Longformer, and subsequently LED, was im-
plemented by Beltagy et al. (2020) with a special-
ized CUDA kernel that is coupled to the archi-
tecture and implementation of BART. This makes
LED more efﬁcient, but extending it to new archi-
tectures incurs signiﬁcant engineering overhead.
This is since LED uses a “diagonal” local-window
attention across layers, for which a naïve imple-
mentation is inefﬁcient. Conversely, SLED uses
chunking, which allows to simply wrap an exist-
ing encoder-decoder model.
Contextualization The most signiﬁcant differ-
ence between LED and SLED from a conceptual
point of view is their contextualization mecha-
nism. While SLED splits the input into (over-
lapping) chunks and encodes each of them in-
dependently, LED performs local attention per-
layer . This results in an effective receptive ﬁeld
that grows linearly with the encoder depth, poten-
tially allowing it to perform more “global” con-
textualization. Our results in §4 suggest that such
global contextualization is beneﬁcial, and a simi-
lar conclusion can be reached when observing that
LED base, which uses all preﬁx tokens as global
tokens, outperforms LEDSCROLLS
base , which uses only
a single token for global contextualization.
Positional information SLED’s chunking
mechanism means that it utilizes the positional
encoding of the underlying model independently
in each chunk, and is thus agnostic to the posi-
tional embedding technique used by the backbone
model. Moreover, it potentially allows SLED to
generalize to arbitrary input lengths. In contrast,
LED utilizes BART’s absolute embeddings,
duplicating them 16 times to support 16K-long
sequences. This limits its ability to generalize
to longer inputs, and potentially induces a re-
quirement for signiﬁcant amounts of long-input
samples to properly tune those new parameters
(Shaham et al., 2022). This is evident in Tab. 1
when comparing tests scores of LED base against
BART base-SLED and considering the number

--- PAGE 12 ---
for training samples. In NarrativeQA and Gov-
Report, which contain 71K and19K samples
respectively, LED is comparable to SLED and
even slightly outperforms it on some metrics. In
ContractNLI (10K examples), it does slightly
worse. In all other datasets, where the training
data is small, LED is signiﬁcantly worse than
SLED.
Complexity We analyzed the complexity
analysis of SLED’s encoder (§3), which is
O(lcn). A similar analysis of LED yields
that in each layer, LED considers O(n)windows
of lengthc, where in each window only the middle
token attends to its local neighborhood, resulting
inO(lcn)memory complexity as well.
However, due to SLED’s use of overlap and full
self-attention within each chunk, SLED’s encod-
ing may require up 2x more memory compared to
LED when= 0:5.
C Experimental details
Our experimental setup is based on the
SCROLLS ofﬁcial repository.8The datasets
inputs and splits remained as suggested by the
authors of SCROLLS as well as the suggested
number of epochs per dataset. To perform
model selection, for each model-dataset pair we
ﬁnetuned 9 models with LINEAR learning rate
scheduling, AdamW optimizer with the default
settings, and setting the learning rate to one of
f2e 5;5e 5;1e 4gand the effective batch size
to one off8;16;32g. Warmup was ﬁxed at 10%
and weight decay at 0.01. All code, data, python
environment requirements, hyperparameters and
scripts required to reproduce our results will be
made public upon publication.
References
Joshua Ainslie, Santiago Ontanon, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
and Li Yang. 2020. ETC: Encoding long and
structured inputs in transformers. In Proceed-
ings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP) ,
pages 268–284, Online. Association for Com-
putational Linguistics.
8https://github.com/tau-nlp/scrollsSamuel Amouyal, Ohad Rubin, Ori Yoran, Tomer
Wolfson, Jonathan Herzig, and Jonathan Be-
rant. 2022. Qampari: : An open-domain
question answering benchmark for questions
with many answers from multiple paragraphs.
ArXiv , abs/2205.12665.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document trans-
former.
Tom B. Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Win-
ter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot
learners. In Advances in Neural Information
Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .
Mingda Chen, Zewei Chu, Sam Wiseman, and
Kevin Gimpel. 2022. SummScreen: A dataset
for abstractive screenplay summarization. In
Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 8602–8615,
Dublin, Ireland. Association for Computational
Linguistics.
Krzysztof Marcin Choromanski, Valerii Likhosh-
erstov, David Dohan, Xingyou Song, An-
dreea Gane, Tamás Sarlós, Peter Hawkins,
Jared Quincy Davis, Afroz Mohiuddin, Lukasz
Kaiser, David Benjamin Belanger, Lucy J. Col-
well, and Adrian Weller. 2021. Rethinking at-
tention with performers. In 9th International
Conference on Learning Representations, ICLR
2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net.
Alexis Conneau, Kartikay Khandelwal, Naman
Goyal, Vishrav Chaudhary, Guillaume Wen-
zek, Francisco Guzmán, Edouard Grave, Myle
Ott, Luke Zettlemoyer, and Veselin Stoyanov.
2020. Unsupervised cross-lingual representa-
tion learning at scale. In Proceedings of the

--- PAGE 13 ---
58th Annual Meeting of the Association for
Computational Linguistics , pages 8440–8451,
Online. Association for Computational Linguis-
tics.
Peng Cui and Le Hu. 2021. Sliding selector net-
work with dynamic memory for extractive sum-
marization of long documents. In Proceedings
of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, pages 5881–5891, Online. Association for
Computational Linguistics.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Co-
han, Noah A. Smith, and Matt Gardner. 2021.
A dataset of information-seeking questions and
answers anchored in research papers. In Pro-
ceedings of the 2021 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies , pages 4599–4610, Online. Associa-
tion for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume
1 (Long and Short Papers) , pages 4171–4186,
Minneapolis, Minnesota. Association for Com-
putational Linguistics.
Quentin Fournier, Gaétan Marceau Caron, and
Daniel Aloise. 2021. A practical survey on
faster and lighter transformers.
Albert Gu, Karan Goel, and Christopher R’e.
2021. Efﬁciently modeling long sequences with
structured state spaces. ArXiv , abs/2111.00396.
Mandy Guo, Joshua Ainslie, David Uthus, Santi-
ago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and
Yinfei Yang. 2022. LongT5: Efﬁcient text-to-
text transformer for long sequences. In Find-
ings of the Association for Computational Lin-
guistics: NAACL 2022 , pages 724–736, Seattle,
United States. Association for Computational
Linguistics.
Ankit Gupta. 2022. Diagonal state spaces are
as effective as structured state spaces. ArXiv ,
abs/2203.14343.Ankit Gupta and Jonathan Berant. 2020. GMAT:
Global memory augmentation for transformers.
ArXiv preprint , abs/2006.03274.
Luyang Huang, Shuyang Cao, Nikolaus Parulian,
Heng Ji, and Lu Wang. 2021. Efﬁcient atten-
tions for long document summarization. In Pro-
ceedings of the 2021 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies , pages 1419–1436, Online. Associa-
tion for Computational Linguistics.
Gautier Izacard and Edouard Grave. 2021. Lever-
aging passage retrieval with generative models
for open domain question answering. In Pro-
ceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics: Main Volume , pages 874–
880, Online. Association for Computational
Linguistics.
Yichen Jiang and Mohit Bansal. 2019. Avoid-
ing reasoning shortcuts: Adversarial evaluation,
training, and model development for multi-hop
QA. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Lin-
guistics , pages 2726–2736, Florence, Italy. As-
sociation for Computational Linguistics.
Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-
skaya. 2020a. Reformer: The efﬁcient trans-
former. In 8th International Conference on
Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net.
Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-
skaya. 2020b. Reformer: The efﬁcient trans-
former. In 8th International Conference on
Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net.
Tomáš Ko ˇciský, Jonathan Schwarz, Phil Blun-
som, Chris Dyer, Karl Moritz Hermann, Gá-
bor Melis, and Edward Grefenstette. 2018. The
NarrativeQA reading comprehension challenge.
Transactions of the Association for Computa-
tional Linguistics , 6:317–328.
Yuta Koreeda and Christopher Manning. 2021.
ContractNLI: A dataset for document-level nat-
ural language inference for contracts. In Find-

--- PAGE 14 ---
ings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 1907–1919,
Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence
pre-training for natural language generation,
translation, and comprehension. In Proceed-
ings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages
7871–7880, Online. Association for Computa-
tional Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for
automatic evaluation of summaries. In Text
Summarization Branches Out , pages 74–81,
Barcelona, Spain. Association for Computa-
tional Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoy-
anov. 2019. Roberta: A robustly optimized bert
pretraining approach. ArXiv , abs/1907.11692.
Xuezhe Ma, Xiang Kong, Sinong Wang, Chunt-
ing Zhou, Jonathan May, Hao Ma, and Luke
Zettlemoyer. 2021. Luna: Linear uniﬁed nested
attention. In Advances in Neural Information
Processing Systems .
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and
Behnam Neyshabur. 2022. Long range lan-
guage modeling via gated state spaces. ArXiv ,
abs/2206.13947.
Richard Yuanzhe Pang, Alicia Parrish, Nitish
Joshi, Nikita Nangia, Jason Phang, Angelica
Chen, Vishakh Padmakumar, Johnny Ma, Jana
Thompson, He He, and Samuel Bowman. 2022.
QuALITY: Question answering with long input
texts, yes! In Proceedings of the 2022 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies , pages 5336–5358,
Seattle, United States. Association for Compu-
tational Linguistics.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah A. Smith, and Lingpeng Kong.2021. Random feature attention. In 9th Inter-
national Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May
3-7, 2021 . OpenReview.net.
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
2020a. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. Journal
of Machine Learning Research , 21(140):1–67.
Colin Raffel, Noam M. Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
2020b. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. ArXiv ,
abs/1910.10683.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
rev, and Percy Liang. 2016. SQuAD: 100,000+
questions for machine comprehension of text.
InProceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 2383–2392, Austin, Texas. Associa-
tion for Computational Linguistics.
Aurko Roy, Mohammad Saffar, Ashish Vaswani,
and David Grangier. 2021. Efﬁcient content-
based sparse attention with routing transform-
ers. Transactions of the Association for Com-
putational Linguistics , 9:53–68.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat,
Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan
Xiong, Mor Geva, Jonathan Berant, and Omer
Levy. 2022. Scrolls: Standardized compar-
ison over long language sequences. ArXiv ,
abs/2201.03533.
Yi Tay, Mostafa Dehghani, Samira Abnar,
Hyung Won Chung, William Fedus, Jinfeng
Rao, Sharan Narang, Vinh Quang Tran, Dani
Yogatama, and Donald Metzler. 2022a. Scal-
ing laws vs model architectures: How does
inductive bias inﬂuence scaling? ArXiv ,
abs/2207.10551.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang
Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Met-
zler. 2021. Long range arena : A benchmark
for efﬁcient transformers. In 9th International
Conference on Learning Representations, ICLR

--- PAGE 15 ---
2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Don-
ald Metzler. 2020. Efﬁcient transformers: A
survey.
Yi Tay, Mostafa Dehghani, Vinh Quang Tran,
Xavier García, Dara Bahri, Tal Schuster,
Huaixiu Zheng, Neil Houlsby, and Donald
Metzler. 2022b. Unifying language learning
paradigms. ArXiv , abs/2205.05131.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. In Advances in Neu-
ral Information Processing Systems 30: An-
nual Conference on Neural Information Pro-
cessing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA , pages 5998–6008.
Jesse Vig, Alexander Fabbri, Wojciech Kryscin-
ski, Chien-Sheng Wu, and Wenhao Liu. 2022.
Exploring neural models for query-focused
summarization. In Findings of the Association
for Computational Linguistics: NAACL 2022 ,
pages 1455–1468, Seattle, United States. Asso-
ciation for Computational Linguistics.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han
Fang, and Hao Ma. 2020. Linformer: Self-
attention with linear complexity.
Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh
Nallapati, and Bing Xiang. 2019. Multi-
passage BERT: A globally normalized BERT
model for open-domain question answering. In
Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Process-
ing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-
IJCNLP) , pages 5878–5882, Hong Kong,
China. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh,
Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Remi Louf,
Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite,
Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, andAlexander Rush. 2020. Transformers: State-
of-the-art natural language processing. In Pro-
ceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: Sys-
tem Demonstrations , pages 38–45, Online. As-
sociation for Computational Linguistics.
Wenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun
Chen, Diana Liskovich, Omer Levy, Scott Yih,
and Yashar Mehdad. 2022a. Simple local atten-
tions remain competitive for long-context tasks.
InProceedings of the 2022 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies , pages 1975–1986, Seattle, United
States. Association for Computational Linguis-
tics.
Wenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun
Chen, Diana Liskovich, Omer Levy, Scott Yih,
and Yashar Mehdad. 2022b. Simple local atten-
tions remain competitive for long-context tasks.
InProceedings of the 2022 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies , pages 1975–1986, Seattle, United
States. Association for Computational Linguis-
tics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua
Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D. Manning. 2018. HotpotQA:
A dataset for diverse, explainable multi-hop
question answering. In Proceedings of the 2018
Conference on Empirical Methods in Natural
Language Processing , pages 2369–2380, Brus-
sels, Belgium. Association for Computational
Linguistics.
Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou,
Nitish Shirish Keskar, and Caiming Xiong.
2022. Modeling multi-hop question answering
as single sequence prediction. In Proceedings
of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers) , pages 974–990, Dublin, Ireland. Asso-
ciation for Computational Linguistics.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qi-
fan Wang, Li Yang, and Amr Ahmed. 2020a.
Big bird: Transformers for longer sequences.

--- PAGE 16 ---
InAdvances in Neural Information Processing
Systems 33: Annual Conference on Neural In-
formation Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qi-
fan Wang, Li Yang, and Amr Ahmed. 2020b.
Big bird: Transformers for longer sequences.
InAdvances in Neural Information Processing
Systems 33: Annual Conference on Neural In-
formation Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual .
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi,
Mutethia Mutuma, Rahul Jha, Ahmed Hassan
Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng
Qiu, and Dragomir Radev. 2021. QMSum: A
new benchmark for query-based multi-domain
meeting summarization. In Proceedings of the
2021 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies , pages
5905–5921, Online. Association for Computa-
tional Linguistics.
