# 2107.05768.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2107.05768.pdf
# Kích thước file: 488244 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Combiner: Transformer Attention Đầy Đủ
với Chi Phí Tính Toán Thưa Thớt
Hongyu Reny,Hanjun Dai,Zihang Dai
Mengjiao Yang,Jure Leskovecy,Dale Schuurmans;z,Bo Dai
yĐại học Stanford, {hyren,jure}@cs.stanford.edu
Google Research, Brain Team, {hadai,zihangd,sherryy,schuurmans,bodai}@google.com
zĐại học Alberta
Tóm tắt
Transformers cung cấp một lớp kiến trúc biểu đạt cực kỳ hiệu quả cho mô hình hóa chuỗi. Tuy nhiên, hạn chế chính của transformers là độ phức tạp thời gian và bộ nhớ bậc hai O(L2) đối với độ dài chuỗi trong các lớp attention, điều này hạn chế ứng dụng trong các chuỗi cực dài. Hầu hết các phương pháp hiện có tận dụng tính thưa thớt hoặc giả định low-rank trong ma trận attention để giảm chi phí, nhưng hy sinh tính biểu đạt. Thay vào đó, chúng tôi đề xuất Combiner, cung cấp khả năng attention đầy đủ trong mỗi head attention trong khi duy trì độ phức tạp tính toán và bộ nhớ thấp. Ý tưởng chính là coi cơ chế self-attention như một kỳ vọng có điều kiện trên embeddings tại mỗi vị trí, và xấp xỉ phân phối có điều kiện với một phân tích nhân tử có cấu trúc. Mỗi vị trí có thể attend đến tất cả các vị trí khác, hoặc thông qua attention trực tiếp, hoặc thông qua attention gián tiếp đến các abstraction, là những kỳ vọng có điều kiện của embeddings từ các vùng cục bộ tương ứng. Chúng tôi cho thấy hầu hết các mẫu attention thưa thớt được sử dụng trong các sparse transformers hiện có đều có thể truyền cảm hứng cho thiết kế phân tích nhân tử như vậy cho attention đầy đủ, dẫn đến cùng chi phí sub-quadratic (O(Llog(L)) hoặc O(Lp
L)). Combiner là một sự thay thế drop-in cho các lớp attention trong transformers hiện có và có thể dễ dàng triển khai trong các framework phổ biến. Đánh giá thực nghiệm trên cả các tác vụ chuỗi autoregressive và bidirectional chứng minh tính hiệu quả của phương pháp này, mang lại kết quả state-of-the-art trên một số tác vụ mô hình hóa hình ảnh và văn bản.

1 Giới thiệu
Transformer [1] là một kiến trúc mạng neural mạnh mẽ đã chứng minh hiệu suất state-of-the-art trong dịch máy [2] và nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) khác thông qua pretraining, sử dụng mô hình ngôn ngữ unidirectional [3] hoặc bidirectional [4–8]. Nó cũng đạt được kết quả xuất sắc trong các lĩnh vực khác như nhận dạng hình ảnh [9], hiểu code [10], nhận dạng giọng nói [11], protein [12], âm nhạc [13] và mô hình hóa tạo sinh hình ảnh [14]. Thành phần cốt lõi của Transformer là cơ chế attention, tính toán các dependencies giữa tất cả các cặp vị trí trong một chuỗi. Tuy nhiên, đối với một chuỗi có độ dài L, tính biểu đạt của attention theo cặp đi kèm với chi phí bậc hai O(L2) trong cả tiêu thụ thời gian và bộ nhớ. Điều này làm cho Transformer vanilla [1] trở nên cấm đoán cho các ứng dụng liên quan đến chuỗi dài, bao gồm hình ảnh độ phân giải cao, chuỗi protein, hoặc tín hiệu giọng nói thô [15], nơi độ dài chuỗi L thường lớn hơn 10;000[14].

Gần đây, đã có một số nỗ lực để mở rộng attention cho các chuỗi dài. Một lớp phương pháp phổ biến là làm thưa thớt ma trận attention với các mẫu thưa thớt khác nhau, bao gồm local

 cho biết đóng góp bằng nhau. Công việc được hoàn thành trong thời gian thực tập của HR tại Google Brain.
Hội nghị lần thứ 35 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2021).arXiv:2107.05768v2  [cs.LG]  28 Oct 2021

--- TRANG 2 ---
window [16,17], local+stride [14], log-sparse [18], axial [19,20], hoặc các mẫu có thể học thông qua hashing [21] hoặc clustering [22]. Sparse attention có chi phí sub-quadratic, nhưng bị mất mát trong việc nắm bắt mối quan hệ all-pair. Nói chung, sparse attention yêu cầu nhiều lớp hơn [14,20,23] để đạt được dependencies autoregressive hoặc bidirectional đầy đủ (hoặc receptive fields [20]) cho mỗi vị trí trong một chuỗi dài.

Ngoài ra, một hướng nghiên cứu khác đã cố gắng đạt được khả năng mở rộng với giả định low-rank rõ ràng [24,25] trên ma trận attention hoặc bằng cách sử dụng feature maps rõ ràng của một số kernels [26]. Tuy nhiên những xấp xỉ chiều thấp rõ ràng này có thể quá hạn chế cho ma trận attention có thể full rank, sử dụng exponential kernels có hiệu quả là vô hạn chiều [27]. Performer [28] là một trong những công trình đầu tiên cố gắng xấp xỉ attention full-rank thông thường với trick random feature [29]. Tuy nhiên các phương pháp dựa trên random-feature như vậy [30] yêu cầu nhiều bases hơn để xấp xỉ tốt hơn exponential kernel [27], và về mặt thực nghiệm chúng tôi thấy nó tạo ra kết quả kém hơn trong một số tác vụ mô hình hóa chuỗi, chẳng hạn như ước lượng mật độ.

Trong bài báo này chúng tôi đề xuất Combiner, một sự thay thế drop-in cho cơ chế attention bậc hai vanilla với chi phí tính toán và bộ nhớ sub-quadratic. Combiner vẫn đạt được khả năng attention đầy đủ trong mỗi head của Multi-Head Attention, không giống như các phương pháp áp dụng xấp xỉ sparse hoặc low-rank. Như chúng tôi sẽ thảo luận, attention tiêu chuẩn được tính toán tại mỗi vị trí có thể được nhìn như kỳ vọng có điều kiện của value embeddings tại tất cả các vị trí khả thi được cho vị trí hiện tại. Dựa trên hiểu biết như vậy, Combiner xấp xỉ rõ ràng phân phối có điều kiện thông qua một phân tích nhân tử có cấu trúc của không gian xác suất. Cụ thể, cho một vị trí x, xác suất attend đến vị trí y có thể được tính toán trực tiếp thông qua query vector của x và key vector của y, hoặc gián tiếp thông qua một abstraction cục bộ nơi x đầu tiên attend đến key vector đại diện cho một nhóm vị trí chứa y, và nhân xác suất chọn y trong nhóm đó. Chúng tôi gọi mô hình này là Combiner vì các phân phối có điều kiện trong attention trở thành sự kết hợp giữa một số local attentions và direct attentions. Decomposition có cấu trúc này cho phép Combiner lấy các mẫu attention thưa thớt hiện có và chuyển đổi chúng thành các lựa chọn thiết kế tương ứng cho phân tích nhân tử xác suất đạt được attention đầy đủ. Như được hiển thị trong Hình 1, Combiner đạt được attention đầy đủ với cùng độ phức tạp tiệm cận như các biến thể sparse. Combiner có thể dễ dàng triển khai trong hầu hết các framework deep learning hiện có mà không cần triển khai phần cứng chuyên biệt, và thân thiện với GPU/TPU. Thực tế, cả các mẫu attention thưa thớt cố định và có thể học từ nhiều biến thể Transformer hiện có [14,18,20,22] đều có thể được nâng cao với các phân tích nhân tử có cấu trúc như vậy, với cùng bậc chi phí thời gian hoặc bộ nhớ.

Chúng tôi xác thực Combiner trên cả các tác vụ mô hình hóa chuỗi autoregressive và bidirectional trên nhiều lĩnh vực khác nhau bao gồm văn bản và hình ảnh. Chúng tôi cho thấy Combiner có thể đạt được perplexity và accuracy tốt hơn khi sử dụng cùng kiến trúc transformer trong khi nhanh hơn nhiều về runtime, và đạt hiệu suất state of the art trên ước lượng mật độ trên các datasets tiêu chuẩn CIFAR-10 (2.77 bits/dim) và ImageNet-64 (3.42 bits/dim), cũng như Long-Range Arena [31]. Triển khai của Combiner có thể được tìm thấy tại https://github.com/google-research/google-research/tree/master/combiner.

2 Attention như Kỳ Vọng Có Điều Kiện
Trong phần này, chúng tôi xem xét lại công thức của Transformer tiêu chuẩn [1] từ góc độ kỳ vọng có điều kiện, điều này truyền cảm hứng cho việc dẫn xuất Combiner.

Không mất tính tổng quát, chúng tôi sử dụng một chuỗi duy nhất trong kịch bản self-attention. Cho một chuỗi gồm L embeddings X= [x1; x2; : : : ; xL], trong đó X2RLdvà mỗi embedding xi2Rdlà một vector d-chiều, thành phần cốt lõi của Transformer là multi-head attention, trong đó mỗi head h là một scaled dot-product attention:

Ah(X) =softmaxQhp
dK>
h
Vh;n
Qh=XWQ
h; Kh=XWK
h; Vh=XWV
ho
2RLd;(1)

và attention vector từ mỗi head Ah(X) được concatenated và projected:
MultiHeadAttn (X) = [A1(X); A2(X); : : : ; AH(X)]Wo; Wo2RHdd: (2)

Ở đây H là tổng số heads mỗi lớp Transformer. Trong bài báo này, chúng tôi tập trung vào cách xấp xỉ attention đầy đủ trong mỗi head của multi-head attention. Để dễ ký hiệu, chúng tôi bỏ chỉ số head h khi có thể, và sử dụng các chữ cái thường xi; qi; ki; vi2Rd để ký hiệu các hàng trong

--- TRANG 3 ---
(D) Combiner
-
Fixed
(A) Fixed
(B) 
Logsparse
(E) Combiner
-
Logsparse
Direct Expectation
Local Expectation
(F) Combiner
-
Axial
(C) AxialHình 1: Ma trận attention của một số instantiations của Combiner trong setting autoregressive. Chúng tôi biến đổi một số mẫu attention thưa thớt: Fixed (A) [14], Logsparse (B) [18] và Axial (C) [20] thành Combiner-Fixed (D), Combiner-Logsparse (E) và Combiner-Axial (F). Combiner xấp xỉ kỳ vọng có điều kiện (3) với sự kết hợp của direct expectation (xanh) và local expectation (vàng). Các instantiations (D)(E)(F) của chúng tôi đạt được attention đầy đủ với cùng độ phức tạp sub-quadratic.

X; Q; K; V tương ứng, tương ứng với một vị trí i trong chuỗi ban đầu có độ dài L. Chúng tôi sử dụng [n] để ký hiệu tập hợp các số nguyên dương f1;2; : : : ; ng.

Đối với một vị trí i2[L], công thức attention (1) có thể được xem như kỳ vọng có điều kiện của các hàng trong V. Cụ thể, vì softmax xuất ra một phân phối xác suất, chúng tôi có thể viết lại (1) như
A(xi) =Ep(jji)[vj]; p (jji) =1
Z(xi)expqip
dk>
j
; (3)

trong đó p(jji) ký hiệu xác suất có điều kiện tại vị trí j cho token tại vị trí i và hàm partition Z(xi) =P
j2
iexp
qip
dk>
j
trên support 
i. Support 
i của p(jji) định nghĩa tập hợp các vị trí hợp lệ mà token thứ i có thể attend đến. Ví dụ, tập support trong autoregressive language modeling (LM) bao gồm tất cả các token trước đó, tức là 
LM
i= [i]2; trong masked language modeling (MLM) support bao gồm tất cả các token trong chuỗi, tức là 
MLM
i= [L]. Nghĩa là, 
LM
i và 
MLM
i đại diện cho khả năng attention đầy đủ tương ứng trong setting LM và MLM.

3 Combiner: Attention Đầy Đủ thông qua Kỳ Vọng Có Điều Kiện Có Cấu Trúc
Độ phức tạp của p(jji) là bottleneck của tính toán cho A(xi). Nói chung, trong các sparse transformers hiện có, support của p(jji) được làm thưa thớt để giảm độ phức tạp tính toán và bộ nhớ, ví dụ, 
Sparse
i(
LM
i cho LM và 
Sparse
i(
MLM
i cho MLM, nhưng điều này có thể dẫn đến giảm capacity hoặc khả năng áp dụng hạn chế. Chúng tôi để thảo luận chi tiết về full capacity của mô hình ở Phụ lục A. Trong phần này chúng tôi giới thiệu Combiner, đạt được 
Combiner
i = 
LM
i cho LM và 
Combiner
i = 
MLM
i cho MLM, trong khi vẫn duy trì chi phí tính toán và bộ nhớ sub-quadratic. Dưới đây chúng tôi ký hiệu 
i là support cho attention đầy đủ nếu không có sự mơ hồ hoặc cần phân biệt giữa LM hoặc MLM. Chúng tôi giới thiệu framework thiết kế chính trong Phần 3.1 và các parameterizations có thể trong Phần 3.2. Sau đó trong Phần 3.3 chúng tôi phân tích trade-off của Combiner.

3.1 Phân Tích Nhân Tử Cục Bộ cho Kỳ Vọng Có Điều Kiện
Ý tưởng chính của Combiner là khai thác cấu trúc phân cấp cho mô hình hóa xác suất có điều kiện trong (3), cung cấp cơ hội giảm độ phức tạp tính toán trong khi duy trì cùng support. Cụ thể, chúng tôi giới thiệu các biến support 
r
i, cho r= 0; : : : ; ni và i2[L]. Các biến support là disjoint, tức là, 
r
i\
s
i=;;8r6=s, và [ni
r=0
r
i= 
i. Sau đó chúng tôi có thể phân tích nhân tử p(jji) như

p(jji) =niX
r=0p(j;
r
iji) =niX
r=0p(jj
r
i; i)p(
r
iji) =p(jj
rj
i; i)p(
rj
iji); (4)

trong đó rj ký hiệu chỉ số của support mà j thuộc về. Phương trình cuối cùng phát sinh từ thực tế là các 
r
i là disjoint với nhau (
r
i\
s
i=;;8r6=s). Do đó, chỉ có một support, 
rj
i, chứa j. Các số hạng còn lại, trong đó j62
r
i cho r6=rj, đều bằng không vì p(jj
r
i; i) = 0.

Hơn nữa, giả sử 
rj
i là một sufficient statistic, tức là j và i độc lập cho 
rj
i, chúng ta có
p(jji) =p(jj
rj
i)p(
rj
iji): (5)

Cho partition f
r
igni
r=0, dạng attention trong (3) có thể được viết lại như
A(xi) = Ep(jji)[vj] =niX
r=0X
j2
r
ip(j;
r
iji)vj (6)

=X
j2
0
i~p(jji)vj
|{z}
direct expectation+Pni
r=1p(
r
iji)X
j2
r
ip(jj
r
i)vj
|{z}
local expectation; (7)

trong đó chúng tôi xem xét direct attention trong partition 
0
i và áp dụng phân tích nhân tử cục bộ (5) cho partition r= 1; : : : ; ni. Ở đây ~p(jji)/p(jji) nhưng với các hằng số normalization khác nhau, sẽ được giải thích dưới đây. Chúng tôi gọi mô hình này là Combiner vì structured attention (7) kết hợp direct expectation của 
0
i và nhiều local expectations thông qua p(jj
r
i) và p(
r
iji) để tạo thành kỳ vọng có điều kiện cuối cùng.

Tương đương, chúng tôi cũng có thể viết lại structured attention (7) như
A(xi) =P
j2
i"
I(j2
0
i)~p(jji) +niX
r=1I(j2
r
i)p(jj
r
i)p(
r
iji)#
| {z }
xác suất có điều kiện hiệu quả mới q(jji)vj; (8)

trong đó I() là hàm chỉ báo nhị phân. Sau khi sắp xếp lại, người ta có thể thấy từ (8) rằng chúng ta có được xác suất có điều kiện hiệu quả q(jji) cố gắng xấp xỉ p(jji) ban đầu. Mỗi số hạng xác suất phụ thuộc vào cả vị trí hiện tại i và vị trí khác j, và kỳ vọng vẫn được thu được đối với một xác suất có điều kiện hợp lệ (không âm và tổng bằng 1 trên 
i).

Yêu Cầu cho Chi Phí Sub-quadratic. Chúng ta có thể thấy ngay lợi ích của công thức này từ thực tế rằng local expectation trong (7) độc lập với vị trí i. Sự phụ thuộc đầy đủ được đạt được thông qua nhân tử p(
r
iji) trong đó j2
r
i. Nếu chúng ta có thể thiết kế phân tích nhân tử cục bộ sao cho:

1. bậc của số lượng số hạng trong (7) cho p(ji);8i2[L]:PL
i=1(ni+j
0
ij) là sub-quadratic; và
2. gọi U=f
r
igi2[L];r2[1;ni] là tập hợp duy nhất các partitions được sử dụng cho tính toán local expectation, thì bậc của jUj (tức là số lượng partitions duy nhất trong U) là sub-quadratic;
3. bậc của tổng số tính toán duy nhất của local expectation trên tất cả các vị trí trong (7), P

2Uj
j là sub-quadratic;

thì người ta có thể thấy rằng chi phí tính toán và bộ nhớ tổng thể sẽ là sub-quadratic với support attention đầy đủ 
Combiner
i = 
i;8i2[L]. Chúng tôi sẽ thảo luận chi tiết trong Phần 4 về cách instantiate nguyên tắc như vậy bằng cách rút cảm hứng từ các sparse transformers hiện có, và cách chuyển đổi chúng thành mô hình attention đầy đủ gần như miễn phí với độ phức tạp tiệm cận giống hệt nhau.

Nhận xét (Decomposition Phân Cấp Hơn Nữa): Chúng tôi giới thiệu decomposition cục bộ với một partition một lớp của support của p(ji) để đơn giản. Thực tế, các decompositions cục bộ như vậy có thể được xếp chồng hơn nữa, tạo ra một cây partition. Cụ thể, chúng ta có thể phân partition thêm 
r
i với các tập con disjoint 
rk
i	nr
k=1, và xem xét decomposition cục bộ p(j;
r
iji) =p(jj
rkj
i; i)p(
rkj
ij
r
i; i)p(
r
iji), trong đó kj là chỉ số của sub-region mà j thuộc về. Do đó, chúng ta có được một decomposition phân cấp của p(jji), cũng có thể được plugged vào (6) và tạo ra một công thức attention đầy đủ mới.

--- TRANG 4 ---

--- TRANG 5 ---
3.2 Tham Số Hóa Các Xác Suất Có Điều Kiện
Trong khi chúng tôi đã có được một cách có thể để tăng tốc Transformer tiêu chuẩn thông qua sự kết hợp của direct expectation và local expectations, điều quan trọng là phải có một lựa chọn thiết kế hiệu quả cho các số hạng xác suất trong (7), cụ thể là ~p(jji) từ direct expectation, p(jj
r
i) từ local expectation và p(
r
iji) cho r2[1; ni]. Để đơn giản, chúng tôi sử dụng scaled dot-product, có nghĩa là chúng tôi sẽ liên kết các vị trí i; j và các tập biến 
r
i với biểu diễn embedding tương ứng, và do đó xác suất tỷ lệ thuận với exponential của tích vô hướng embedding. Cụ thể:

•~p(jji): Vì số hạng này dành cho direct expectation, chúng ta có thể để ~p(jji)/exp(qip
dk>
j), giống như vanilla attention (3) nhưng với các normalization khác nhau, sẽ được giải thích trong Phương trình 9.

•p(
r
iji): Số hạng này nhằm nắm bắt xác suất sự kiện joint, tức là, p(
r
iji)/exp
qip
dk>

r
i
. Do đó lựa chọn thiết kế của k
r
i nên tạo ra một abstraction của support tương ứng 
r
i. Chúng tôi thấy k
r
i=max poolingj2
r
ikj đã cung cấp kết quả thực nghiệm tốt mà không giới thiệu thêm tham số; chúng tôi cũng có thể sử dụng DeepSets [32] để có được abstraction như vậy.

•p(jj
r
i): Số hạng này là xác suất có được j trong span cục bộ 
r
i này. Chúng tôi tạo p(jj
r
i)/
expq
r
ip
dk>
j
, trong đó chúng tôi sử dụng max pooling hoặc DeepSets trên fqjgj2
r
i để có được q
r
i tương tự.

Chuẩn Hóa Các Số Hạng Xác Suất. Các số hạng trong mỗi local expectation p(jj
r
i);8j2
r
i có thể được chuẩn hóa trong span cục bộ; direct expectation ~p(jji) và các số hạng trong p(
r
iji) nên được chuẩn hóa cùng nhau,

Z(xi) =X
j2
(0)
iexpqip
dk>
j
+niX
r=1expqip
dk>

r
i
; (9)

và Z(xi) là hằng số chuẩn hóa khi tính toán ~p(jji) và p(
r
iji).

3.3 Trade-offs trong Combiner
Combiner đạt được attention đầy đủ với chi phí giảm mà không tạo ra giả định thưa thớt hoặc low-rank rõ ràng trên ma trận attention. Tuy nhiên lợi ích hiệu quả này không miễn phí. Trong phần này chúng tôi thảo luận các hạn chế của việc đơn giản hóa được thực hiện bởi Combiner, và cung cấp một workaround đơn giản.

Xấp Xỉ Attention Có Cấu Trúc. Chúng tôi có được decomposition cục bộ (5) dưới giả định độc lập có điều kiện. Do đó, local expectation trong (7) độc lập với vị trí i, điều này gợi ý rằng bất kỳ hai vị trí i1 và i2 với 
r
i1= 
r
i2= 
 sẽ có attention scores phụ thuộc tuyến tính trên vùng 
. Chính thức, các xác suất được hình thành bởi phân phối có điều kiện hiệu quả ~ a(
)i1=h
q(j1ji1); q(j2ji1); : : : ; q (jj
r
i1jji1)i
=p(
r
i1ji1)
p(
r
i2ji2)~ a(
)i2. Nói cách khác, rank của sub-matrix trên cùng partition trong ma trận attention kết quả là 1, do đó, ma trận attention là locally low-rank dựa trên partition. Mặt khác, direct expectation hoàn toàn attend đến mỗi vị trí trong sub-support 
0, đảm bảo full-rank block. Hai attention schemes này làm cho ma trận attention của Combiner có cấu trúc. So với xấp xỉ low-rank cho attention [26,28,30], được lấy cảm hứng từ random features [29] trong cộng đồng kernel, một xấp xỉ có cấu trúc khai thác cả locally low-rank và full-rank blocks đã được chứng minh mạnh mẽ hơn về mặt lý thuyết và thực nghiệm trong các máy kernel quy mô lớn [27].

Cải Thiện Tính Biểu Đạt Sử Dụng Mô Hình Mixture. Một cách để cải thiện thêm tính biểu đạt của phân tích nhân tử cục bộ là sử dụng mô hình mixture. Ý tưởng này được chuyển thể từ mixture của softmaxs [33] để có được lớp softmax high-rank trong mô hình hóa ngôn ngữ. Gọi ! là một partition cụ thể của support (tức là, tập hợp 
r
i) của 
i, thì người ta có thể dễ dàng sử dụng A(xi) =1
MPM
m=1A(xi;!m) để tính toán attention, trong đó mỗi thành phần của mixture A(xi;!m) là số hạng (7) sử dụng một kế hoạch phân tích nhân tử cụ thể !m. Về mặt thực nghiệm, chúng tôi thấy hai thành phần đã đủ để cải thiện hiệu suất.

4 Các Instantiations của Combiner
Trong phần này chúng tôi cho thấy một số schemes phân tích nhân tử cục bộ thỏa mãn các yêu cầu trong Phần 3.1. Như chúng ta sẽ thấy, Combiner có thể chuyển đổi một số sparse transformers [14,18,20–22] thành attention đầy đủ, với cùng bậc tiêu thụ tính toán và bộ nhớ. Người ta cũng có thể thiết kế các mẫu phân tích nhân tử khác, có thể dễ dàng instantiated trong Combiner.

--- TRANG 6 ---
4.1 Combiner-Fixed
Sparse Transformer [14] là một trong những biến thể đại diện nhất có thể đạt được chi phí tính toán và bộ nhớ O(Lp
L). Ở đây chúng tôi cho thấy cách chuyển đổi mẫu cố định này được đề xuất trong [14] (Hình 1(A)) thành một kế hoạch phân tích nhân tử, và instantiate một biến thể attention đầy đủ có tên là Combiner-Fixed (Hình 1(D)).

Trong fixed-sparse attention, support là 
sparse MLM
i =fj:jmods= 0g[fj:ji(divs)g trong đó s là một hyper-parameter, div là phép chia nguyên, và ji(divs) ký hiệu rằng thương số của i và j w.r.t. s là giống nhau. Trong trường hợp autoregressive, 
sparse LM
i = 
sparse MLM
i\[i]. Vui lòng tham khảo Hình 1(A) để minh họa phiên bản LM.

Thiết kế của chúng tôi về !MLM
ﬁxed có dạng sau:

0
i=fj:ji(divs)g;
r
i=
j:jdivs=r; j =2
0
i	
;8r2[Ldivs];8i2[L] (10)

trong đó mỗi local expectation được thực hiện trong mỗi span có kích thước s, và có tổng cộng Ldivs spans trên tất cả các vị trí. Đối với mỗi vị trí i2[L], có (s+ (Ldivs)) số hạng trong (7); local expectation có (Ldivs) số hạng. Độ phức tạp tổng thể là O(L(s+ 2(Ldivs))). Giá trị s tối ưu là O(p
L), và chúng ta có thể đạt được độ phức tạp tính toán và bộ nhớ O(Lp
L), giống như [14] nhưng ở đây chúng ta có được khả năng attention đầy đủ trong mỗi attention head. Đối với trường hợp LM, chúng ta có thể đơn giản có !LM
ﬁxed:f
r
i\[i]j
r
i2!MLM
ﬁxedg, có cùng độ phức tạp tối ưu O(Lp
L).

4.2 Combiner-Logsparse
Logsparse Transformer được đề xuất trong [18] và về mặt lý thuyết có thể đạt được chi phí O(LlogL). Ý tưởng chung là làm cho kích thước của support 
sparse
i không lớn hơn dlog2ie. Để dễ ký hiệu, chúng tôi đầu tiên định nghĩa bits(n) = [ b1; b2; : : : ; bdlog2ne] là biểu diễn nhị phân của số nguyên n, với bt2f0;1g là hệ số của basis 2t. Do đó chúng ta có n=Pdlog2ne
t=1bt2t. Một trong những lựa chọn thiết kế có thể để tạo Logsparse trong trường hợp LM là 
sparse LM
i =n
sufft:=Pdlog2i1e
=t b2odlog2i1e
t=1[
fig,tức là, attend đến các chỉ số vị trí bằng tổng suffix của các weighted bits(i1), cũng như vị trí i chính nó. Điều này phục vụ như phiên bản sparse cơ sở của chúng tôi như được hiển thị trong Hình 1(B).

Để khai thác scheme này trong framework Combiner, chúng ta có thể định nghĩa dlog2ne supports không chồng chéo, trong đó 
r
i= [suffr]n[suffr+1] với trường hợp biên [suffdlog2i1e+1] =;. Lưu ý rằng để dễ ký hiệu, một số 
r
i là rỗng sẽ được bỏ qua. Trong trường hợp này, tập direct attention 
0
i bao gồm fig, cũng như fi1g khi i là số chẵn. Phân tích nhân tử như vậy dẫn đến Combiner-Logsparse, như được hiển thị trong Hình 1(E). Từ Hình, chúng ta quan sát thấy tổng cộng chúng ta sẽ có span summaries cho mỗi 2;4;8; : : : ; 2blog2Lc vị trí, dẫn đến tổng Pblog2Lc
t=1bL
2tc hoặc O(L) summaries. Mỗi vị trí i sẽ chọn nhiều nhất O(log(i)) spans không chồng chéo để bao phủ support đầy đủ 
i, và do đó, tổng chi phí sẽ là O(LlogL). Chúng tôi để thiết kế trường hợp MLM ở Phụ lục B.

4.3 Combiner-Axial
Axial Transformer [20] xây dựng attention dọc theo mỗi trục của dữ liệu đầu vào. Không mất tính tổng quát, chúng tôi tập trung vào trường hợp 2D trong đó chuỗi đầu vào được reshape thành ma trận kích thước nm=L. Cụ thể, vị trí i trong chuỗi ban đầu sẽ ở hàngi= (i1)divm+ 1 và cộti= (i1)modm+ 1. Chúng tôi cho thấy cách đơn giản để cho phép attention đầy đủ với phân tích nhân tử trên ma trận 2D, do đó Combiner-Axial.

Sparse axial có 
sparse MLM
i =fj:j1i1(modm)g[fj:j1i1(divm)g, và 
sparse LM
i = 
sparse MLM
i\[i], tất cả đều có nhiều nhất O(m+n) entries cho mỗi i, như được minh họa trong Hình 1(C). Chúng tôi đề xuất một số schemes phân tích nhân tử để làm cho nó trở thành attention với support đầy đủ.

•!LM
axial-vertical: 
0
i= 
sparse LM
i, và 
r
i=fj:jr(modm)g\[icoli], cho r2[m]nfcolig. Như được mô tả trong Hình 2(A), 
r
i tương ứng với cột r phía trên hàngi, nơi chúng tôi sử dụng max pooling để

--- TRANG 7 ---
(A) Combiner
-
Axial
-
Vertical
(B) Combiner
-
Axial
-
Horizontal
Attention matrix
Attention matrix
Reshaped sequenceHình 2: Ma trận attention và chuỗi được attended (ví dụ, một hình ảnh 3x4) của các biến thể vertical và horizontal của Combiner-Axial. Xanh và vàng tương ứng với direct và local attention cho vị trí i (tím). Các vị trí được kết nối bằng mũi tên tương ứng với cùng support 
r.

có được abstraction. Để có được abstraction như vậy cho tất cả các vị trí, chúng ta có thể tận dụng toán tử cummax cho mỗi cột để có hiệu quả thu được prefix-max.

•!LM
axial-horizontal: tương tự như !axial-vertical ngoại trừ việc mỗi 
r
i tóm tắt hàng r trước hàngi và loại trừ cộti (Hình 2(B)).

•!LM
axial-rowmajor: 
0
i=fj:j1i1(divm)g\[i],tức là, các phần tử trong cùng một hàng được attended trực tiếp, trong khi 
r
i=fj:jr(divm)g\[icoli] nắm bắt các hàng trước hàngi. Cấu trúc này tương tự như Combiner-Fixed, ngoại trừ cách mà abstraction (và do đó local expectation) được tính toán. Combiner-Fixed tính toán abstraction chỉ dựa trên r của partition 
r
i, trong khi !axial-rowmajor phụ thuộc vào cả r và cộti (Hình 1(F)).

Trong tất cả các trường hợp trên, chi phí tương tự như Axial Transformer [20], là O(Lp
L) nếu chúng ta reshape chuỗi thành ma trận 2D với n; m =O(p
L). Chúng tôi để trường hợp MLM ở Phụ lục C.

4.4 Combiner-Learnable
Lấy cảm hứng từ Reformer [21] và Routing Transformer [22], chúng ta cũng có thể học kế hoạch phân tích nhân tử ! từ dữ liệu. Chúng tôi minh họa điều này với Routing Transformer và cung cấp một cách để cho phép attention đầy đủ trong Routing Transformer theo nguyên tắc Combiner.

Đối với một lớp cụ thể, giả sử chúng ta có một vùng rời rạc đã học (hoặc cluster trong Routing Transformer) f
rgn
r=1 trong đó [r
r= [L]. Trong Routing Transformer, chúng ta đơn giản có 
sparse MLM
i = 
ri trong đó 
ri ký hiệu vùng mà vị trí i thuộc về. Để định nghĩa phân tích nhân tử Combiner, chúng ta để

!routing MLM: 
0
i= 
ri;
r
i= 
rn
0
i;8r2[ni]: (11)

Lưu ý rằng ni=n (tức là, số lượng learned clusters) cho tất cả các vị trí. Phân tích nhân tử trên chỉ có thể hoạt động cho MLM. LM yêu cầu định nghĩa sau:

!routing LM: 
0
i= 
ri\[i];
r
i=

rn
0
i
\[i];8r2[ni]: (12)

Nói chung, cả LM và MLM đều có thể có chi phí sub-quadratic khi n=O(p
L). Tuy nhiên, các biến thể routing (bao gồm Routing Transformer) yêu cầu thao tác gather, có thể chậm trên TPUs (xem minh họa trong Phụ lục D).

5 Đánh Giá Thực Nghiệm
Chúng tôi đánh giá Combiner với các mẫu attention đầy đủ khác nhau trên cả các tác vụ mô hình hóa chuỗi autoregressive và bidirectional, bao trùm một loạt rộng dữ liệu đầu vào từ hình ảnh đến văn bản. Tất cả các tác vụ được xem xét đều liên quan đến chuỗi dài lên đến 12,000 về độ dài, một số trong đó ngăn cản khả năng áp dụng của vanilla transformer. Chúng tôi so sánh Combiner với các Transformers state-of-the-art. Chúng tôi cũng thực hiện một loạt nghiên cứu ablation trong đó tất cả các mô hình được so sánh sử dụng chính xác cùng kiến trúc chỉ khác nhau ở module attention, tránh các tricks cá nhân được sử dụng trong các công trình gốc (ví dụ, sử dụng cả mẫu có thể học và cố định trong Routing Transformer [22]). Chi tiết để tái tạo tất cả kết quả thực nghiệm có thể được tìm thấy trong Phụ lục E.

--- TRANG 8 ---
Bảng 1: Kết quả ablation trong Bits per Dimension (Bits/Dim) trên CIFAR-10 và ImageNet-64.
Model Layers CIFAR-10 ImageNet-64
Reformer [21] 6 - 3.740
Performer [28] 6 3.335 3.719
Logsparse [18] 6 4.253 4.351
Combiner-Logsparse (Ours) 6 3.366 3.795
Fixed [14] 6 3.408 3.696
Combiner-Fixed (Ours) 6 3.321 3.654
Axial [20] 6 3.666 4.032
Combiner-Axial (Ours) 6 3.050 3.585
Combiner-Mixture (Ours) 6 3.040 3.585
Reformer [21] 12 - 3.710
Performer [28] 12 3.310 3.636
Routing Transformer [22] 12 2.950 -
Combiner-Mixture (Ours) 12 2.885 3.504

5.1 Mô Hình Hóa Chuỗi Autoregressive
Trong tiểu mục này, chúng tôi đầu tiên thực hiện ước lượng mật độ trên văn bản và hình ảnh sử dụng Combiner.

5.1.1 Mô Hình Hóa Ngôn Ngữ Bảng 2: LM Perplexity trên Wiki-40B (Main).
Model Perplexity
Transformer-2k [1] 17.26
Performer-2k [28] 19.66
Routing-2k [22] 20.85
Fixed-2k [14] 18.04
Combiner-Fixed-2k (Ours) 17.70
Axial-2k [20] 20.82
Combiner-Axial-2k (Ours) 17.56
Combiner-Fixed-8k (Ours) 16.60
Combiner-Axial-8k (Ours) 16.49

Bảng 3: LM Perplexity trên Wiki-40B (Ablation).
Model Perplexity
Transformer-2k [1] 17.26
Combiner-DeepSets-Max-8k (Ours) 16.29
Combiner-DeepSets-Mean-8k (Ours) 16.48
Combiner-Max-8k (Ours) 16.60
Combiner-Mean-8k (Ours) 16.54

Đối với mô hình hóa ngôn ngữ, chúng tôi tập trung vào dataset Wiki-40B-En [34], bao gồm các trang Wikipedia sạch bằng tiếng Anh. Chúng tôi sử dụng mô hình sentence piece với vocabulary size 32K để tokenize văn bản và đo perplexity ở cấp độ sentence piece. Để đảm bảo so sánh công bằng, tất cả các mô hình được so sánh lần nữa có cùng số lượng lớp và hidden sizes, và được triển khai dưới cùng code base.

Bảng 2 cho thấy kết quả của so sánh. Như chúng ta có thể thấy, dưới độ dài chuỗi 2k, các biến thể Combiner luôn tốt hơn các baseline tương ứng của chúng, và rất gần với Transformer tiêu chuẩn. Khi độ dài chuỗi lên đến 8k, Transformer tiêu chuẩn hết bộ nhớ, trong khi Combiner tiếp tục đạt được perplexity cải thiện, vượt qua kết quả của Transformer-2k. Nếu chúng ta sử dụng thêm DeepSets để tính toán các số hạng tóm tắt q
r
i và k
r
i, chúng ta có thể đạt được perplexity thấp hơn nữa như được hiển thị trong Bảng 3.

5.1.2 Mô Hình Tạo Sinh Hình Ảnh
CIFAR-10. Chúng tôi đầu tiên thực hiện kiểm tra sức khỏe trong đó chúng tôi so sánh các baseline sparse attention với Combiner với attention đầy đủ dưới cùng kiến trúc trên dataset CIFAR-10. Độ dài chuỗi là 3072. Đối với tất cả các phương pháp, chúng tôi sử dụng cùng transformer 6-layer với 8 attention heads và 512 embedding dimensions. Chúng tôi huấn luyện tất cả các mô hình trong 500k iterations sử dụng batch size 32 trên TPU v2.

Như được hiển thị trong Bảng 1, cho cùng kiến trúc mô hình, Combiner-X hoạt động tốt hơn đáng kể so với mô hình cơ sở X dưới metric bits per dimension (BPD) trên 10,000 test images. Cụ thể, Combiner giảm đáng kể BPD lần lượt 0.887, 0.087, và 0.626 so với các mô hình cơ sở Logsparse, Fixed và Axial. Lưu ý rằng tất cả các biến thể Combiner đều đạt hiệu suất tốt hơn so với tốt nhất của các mô hình cơ sở. Điều này chứng minh lợi thế của Combiner so với các baselines cho cùng kiến trúc 6-layer. Chúng tôi quan sát xu hướng tương tự dưới kiến trúc 12-layer.

--- TRANG 9 ---
Bảng 4: Bits per Dimension (Bits/Dim) trên CIFAR-10 và ImageNet-64.
CIFAR-10 Bits/Dim
PixelCNN [15] 3.03
PixelCNN++ [36] 2.92
Image Transformer [16] 2.90
PixelSNAIL [37] 2.85
Sparse Transformer [14] 2.80
Combiner-Axial (ours) 2.77ImageNet 64x64 Bits/Dim
PixelCNN [15] 3.57
Parallel Multiscale [38] 3.70
Glow [39] 3.81
SPN [40] 3.52
Sparse Transformer [14] 3.44
Axial Transformer [20] 3.44
Routing Transformer [22] 3.43
Combiner-Axial (ours) 3.42

Theo kiến trúc 128-layer trong Child et al. [14], chúng tôi áp dụng Combiner-Axial và đạt được hiệu suất state-of-the-art, 2.77 BPD trên CIFAR-10, như được liệt kê trong Bảng 4. Chúng tôi chạy tất cả các mô hình trong Bảng 4 mà không có data augmentation [35].

ImageNet-64. Chúng tôi cũng đánh giá hiệu suất dưới setting autoregressive trên ImageNet-64, nơi độ dài chuỗi là 12,288. Chúng tôi đầu tiên thực hiện phân tích tương tự như CIFAR-10 và so sánh Combiner-X với các baselines sử dụng cùng kiến trúc mô hình. Như được hiển thị trong Bảng 1, Combiner luôn vượt trội so với các baselines với cùng mẫu attention. Chúng tôi tiếp tục áp dụng Combiner-Axial cho một Transformer 30-layer, đạt được hiệu suất state-of-the-art trên ước lượng mật độ trên ImageNet-64, chứng minh tính hiệu quả của attention đầy đủ đạt được bởi Combiner.

5.2 Mô Hình Hóa Chuỗi Bidirectional
Bên cạnh các tác vụ autoregressive, chúng tôi cũng đánh giá Combiner trên một tập hợp các tác vụ bidirectional tiêu chuẩn để cho thấy khả năng áp dụng tổng quát của phương pháp.

5.2.1 Long-Range Arena
Long-Range Arena (LRA) là một benchmark thống nhất [31] để khảo sát khả năng của các efficient transformers trong việc xử lý chuỗi dài. Chúng tôi đánh giá các mô hình của chúng tôi trên năm tác vụ từ LRA: ListOps, Text Classification, Retrieval, Image Classification và Pathfinder. Tất cả các tác vụ đều là phân loại đa lớp ở cấp độ chuỗi. Vui lòng tham khảo bài báo LRA gốc để biết thêm chi tiết.

Bảng 5: Kết quả thực nghiệm trên benchmark Long-Range Arena.
Model ListOps Text Retrieval Image Pathfinder Avg
Chance 10.00 50.00 50.00 10.00 50.00 34.00
Transformer 36.38 64.27 57.46 42.44 88.81 57.87
Local Attention 15.95 52.98 53.39 41.46 84.64 49.68
Sparse TRans. 35.78 63.58 59.59 44.24 83.90 57.42
Longformer 36.03 62.85 56.89 42.22 86.68 56.93
Linformer 35.49 53.94 52.27 38.56 86.17 53.28
Reformer 36.30 56.10 53.40 38.07 79.18 52.61
Sinkhorn Trans. 34.20 61.20 53.83 41.23 73.36 52.76
Synthesizer 36.50 61.68 54.67 41.61 81.61 55.21
BigBird 37.08 64.02 59.29 40.83 86.75 57.59
Linear Trans. 17.15 65.90 53.09 42.34 88.13 53.32
Performer 36.00 65.40 53.82 42.77 88.76 57.35
Combiner-Fixed 36.65 64.99 59.81 41.67 88.59 58.34
Combiner-Axial 36.15 64.36 56.10 41.33 88.43 57.27

Như được hiển thị trong Bảng 5, Combiner có thể sánh ngang hiệu suất của vanilla Transformer và đạt được hiệu suất thậm chí tốt hơn trong một số tác vụ. Theo giao thức của LRA, tất cả các phương pháp sử dụng cùng kiến trúc và hyperparameters để so sánh có kiểm soát. Chúng tôi sử dụng các số từ Tay et al. [31] cho tất cả các tác vụ ngoại trừ Pathfinder. Vì chúng tôi không thể tái tạo kết quả Pathfinder gốc sử dụng thiết lập mặc định trong repository Github LRA, chúng tôi chạy lại tất cả các baselines sử dụng cấu hình Pathfinder-inter để tiến hành so sánh công bằng. Tuy nhiên, vì benchmark vẫn ở quy mô nhỏ và trang web chính thức LRA không khuyến khích điều chỉnh hyperparameter, Bảng 5 nên được coi như kết quả cho test bench của tính biểu đạt so với vanilla Transformer.

--- TRANG 10 ---
Bảng 6: MLM perplexity trên dataset C4.
Model Perplexity
Transformer-2k [1] 4.552
BigBird-2k [41] 4.696
Performer-2k [28] 10.940
Fixed-2k [14] 5.279
Combiner-Fixed-2k (Ours) 5.170
Axial-2k [20] 5.370
Combiner-Axial-2k (Ours) 4.809
Routing-2k [22] 6.703
Combiner-Routing-2k (Ours) 6.539
BigBird-8k [41] 4.542
Combiner-Axial-8k (Ours) 4.190
Combiner-Fixed-8k (Ours) 4.139

210211212213214
Sequence Length272829210211212213Milliseconds / Iteration
210211212213214
Sequence Length20212223Memory (GB)
Vanilla Transformer
PerformerBigBird
Combiner-AxialCombiner-Fixed
Sparse-AxialSparse-Fixed
Combiner-MixtureHình 3: Chúng tôi đo thời gian inference runtime và sử dụng bộ nhớ cho tám mô hình. Nhìn chung Combiner có tốc độ tương tự với Performer và đối tác sparse của nó nhưng Vanilla Transformer nhanh chóng bị OOM khi độ dài chuỗi tăng.

5.2.2 Masked Language Modeling
Là thành phần cốt lõi của pretraining ngôn ngữ BERT [5], masked language modeling (MLM) đề cập đến tác vụ tái tạo các token được che ngẫu nhiên trong chuỗi đầu vào. Như với tác vụ LM, chúng tôi sử dụng perplexity làm metric chính, tương quan tương đối tốt với hiệu suất tác vụ downstream. Cụ thể, chúng tôi sử dụng dataset C4 quy mô lớn [8] để huấn luyện và đánh giá, và xem xét các độ dài chuỗi khác nhau. Theo thiết lập BERT gốc, chúng tôi che 15% tokens trong mỗi chuỗi đầu vào. So sánh được tóm tắt trong Bảng 6. Tương tự như kết quả LM, các biến thể Combiner khác nhau luôn vượt trội so với các baseline tương ứng của chúng dưới độ dài chuỗi 2k. Tuy nhiên, ngoài Transformer tiêu chuẩn, Combiner-2k cũng thua BigBird-2k. Chúng tôi suy đoán rằng điều này liên quan đến thiết kế đặc biệt trong BigBird chẳng hạn như tất cả các token luôn có thể attend trực tiếp đến token <cls>, chỉ áp dụng được trong các vấn đề non-causal. Điều đó nói rằng, khi chúng tôi tăng thêm độ dài chuỗi lên 8k, Transformer tiêu chuẩn gặp vấn đề OOM, trong khi Combiner không chỉ vượt trội so với BigBird mà còn vượt qua đáng kể Transformer-2k. Điều này gợi ý rằng Combiner có thể thực sự hưởng lợi từ việc mở rộng học đến độ dài chuỗi dài hơn.

5.3 Runtime và Sử Dụng Bộ Nhớ của Combiner
Ở đây chúng tôi đánh giá inference runtime và sử dụng bộ nhớ của năm baselines – Transformer, Performer, BigBird, Sparse-Fixed và Sparse-Axial, cũng như ba biến thể của Combiner– Combiner-Fixed, Combiner-Axial và Combiner-Mixture. Chúng tôi chạy inference của tất cả các mô hình trên TPU v3-16 (16 cores x 16GB) với batch size 16, và chúng tôi test các chuỗi có độ dài từ 210 đến 214. Như được hiển thị trong Hình 3, các instantiations Combiner đạt được runtime và sử dụng bộ nhớ có thể so sánh với đối tác sparse của chúng và Performer. Lưu ý Combiner đạt được hiệu suất thực nghiệm tốt hơn nhiều so với các mô hình sparse và Performer. Combiner-Mixture có cùng độ phức tạp tiệm cận với Combiner-Fixed và Combiner-Axial, tuy nhiên, vì nó yêu cầu chạy hai kế hoạch partition, nó chậm hơn Combiner-Fixed và Combiner-Axial. Do thao tác gather được yêu cầu bởi random attention không rất thân thiện với TPU/GPU, BigBird rất tốn kém về mặt tính toán. Và mô hình Transformer nhanh chóng hết bộ nhớ khi độ dài chuỗi tăng.

6 Kết Luận
Lấy cảm hứng từ quan điểm kỳ vọng có điều kiện của cơ chế attention, chúng tôi đề xuất Combiner, một sự thay thế drop-in của module attention. Bằng cách giới thiệu decomposition có cấu trúc cho xác suất có điều kiện, Combiner đạt được khả năng attention đầy đủ trong khi duy trì chi phí tính toán và bộ nhớ sub-quadratic. Chúng tôi instantiate một số biến thể Combiner chuyển đổi các sparse transformers hiện có thành attention đầy đủ. Combiner đạt được hiệu suất state-of-the-art trên cả các tác vụ autoregressive và bidirectional cho mô hình hóa hình ảnh và văn bản, cho thấy lợi ích trong cả hiệu quả mô hình hóa và hiệu quả runtime. Công việc tương lai bao gồm thiết kế mẫu phân tích nhân tử bổ sung, cũng như ứng dụng của Combiner trong các lĩnh vực như tin sinh học và giọng nói.

--- TRANG 11 ---
Lời Cảm Ơn và Tiết Lộ Nguồn Tài Trợ
Chúng tôi muốn cảm ơn Richard Song và David Dohan vì sự giúp đỡ trong việc giới thiệu codebase Performer và cấu hình thực nghiệm, Yi Tay và Mostafa Dehghani vì làm rõ về benchmark LRA, James Lee-Thorp, Joshua Ainslie, và Ilya Eckstein vì làm rõ về kết quả thực nghiệm LRA của họ, Adams Yu vì thực hiện đánh giá bài báo nội bộ và đề xuất hữu ích. Chúng tôi cũng biết ơn sự hỗ trợ của DARPA dưới Nos. HR00112190039 (TAMI), N660011924033 (MCS); ARO dưới Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF dưới Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID), NIH dưới No. R56LM013365; Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase, Docomo, Hitachi, Intel, JD.com, KDDI, NVIDIA, Dell, Toshiba, Visa, và UnitedHealth Group. Hongyu Ren được hỗ trợ bởi Masason Foundation Fellowship và Apple PhD Fellowship. Jure Leskovec là investigator Chan Zuckerberg Biohub.

Tài Liệu Tham Khảo
[1]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Advances in Neural Information Processing Systems (NeurIPS), 2017.

[2]Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. The best of both worlds: Combining recent advances in neural machine translation. Trong Annual Meeting of the Association for Computational Linguistics (ACL), 2018.

[3]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Trong Advances in Neural Information Processing Systems (NeurIPS), 2020.

[4]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Trong Advances in Neural Information Processing Systems (NeurIPS), 2019.

[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. Trong Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.

[6]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. Trong International Conference on Learning Representations (ICLR), 2020.

[7]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[8]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations (ICLR), 2021.

[10] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, và Kensen Shi. Learning and evaluating contextual embedding of source code. Trong International Conference on Machine Learning (ICML), 2020.

[11] Linhao Dong, Shuang Xu, và Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. Trong IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.

--- TRANG 12 ---
[12] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, và Richard Socher. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020.

[13] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, AM Dai, MD Hoffman, và D Eck. Music transformer: Generating music with long-term structure (2018). Trong International Conference on Learning Representations (ICLR), 2019.

[14] Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

[15] Aaron Van Oord, Nal Kalchbrenner, và Koray Kavukcuoglu. Pixel recurrent neural networks. Trong International Conference on Machine Learning (ICML), 2016.

[16] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, và Dustin Tran. Image transformer. Trong International Conference on Machine Learning (ICML), 2018.

[17] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, và Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. Trong International Conference on Learning Representations (ICLR), 2020.

[18] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, và Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Trong Advances in Neural Information Processing Systems (NeurIPS), 2019.

[19] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, và Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. Trong International Conference on Computer Vision (ICCV), 2019.

[20] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, và Tim Salimans. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.

[21] Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong International Conference on Learning Representations (ICLR), 2020.

[22] Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.

[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. Trong Annual Meeting of the Association for Computational Linguistics (ACL), 2019.

[24] Zhuoran Shen, Mingyuan Zhang, Shuai Yi, Junjie Yan, và Haiyu Zhao. Factorized attention: Self-attention with linear complexities. CoRR, 2018.

[25] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

[26] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. Trong International Conference on Machine Learning (ICML), 2020.

[27] Si Si, Cho-Jui Hsieh, và Inderjit S Dhillon. Memory efficient kernel approximation. The Journal of Machine Learning Research, 2017.

[28] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. Trong International Conference on Learning Representations (ICLR), 2021.

[29] Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. Trong Advances in Neural Information Processing Systems (NeurIPS), 2007.

--- TRANG 13 ---
[30] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, và Lingpeng Kong. Random feature attention. Trong International Conference on Learning Representations (ICLR), 2021.

[31] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers. Trong International Conference on Learning Representations (ICLR), 2021.

[32] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, và Alexander Smola. Deep sets. Trong Advances in Neural Information Processing Systems (NeurIPS), 2017.

[33] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, và William W Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. Trong International Conference on Learning Representations (ICLR), 2018.

[34] Mandy Guo, Zihang Dai, Denny Vrande ˇci´c, và Rami Al-Rfou. Wiki-40b: Multilingual language model dataset. Trong Proceedings of The 12th Language Resources and Evaluation Conference, 2020.

[35] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, và Ilya Sutskever. Distribution augmentation for generative modeling. Trong International Conference on Machine Learning (ICML), 2020.

[36] Tim Salimans, Andrej Karpathy, Xi Chen, và Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. Trong International Conference on Learning Representations (ICLR), 2017.

[37] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, và Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. Trong International Conference on Machine Learning (ICML), 2018.

[38] Scott Reed, Aäron Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, và Nando Freitas. Parallel multiscale autoregressive density estimation. Trong International Conference on Machine Learning (ICML), 2017.

[39] Diederik P Kingma và Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Trong Advances in Neural Information Processing Systems (NeurIPS), 2018.

[40] Jacob Menick và Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. Trong International Conference on Learning Representations (ICLR), 2019.

[41] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Trong Advances in Neural Information Processing Systems (NeurIPS), 2020.

[42] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, và Sanjiv Kumar. o(n) connections are expressive enough: Universal approximability of sparse transformers. arXiv preprint arXiv:2006.04862, 2020.

--- TRANG 14 ---
Phụ Lục

A Xấp Xỉ Toàn Cầu
Ở đây chúng tôi cho thấy trong Mệnh đề 1 rằng Combiner-X của chúng tôi đạt được tính chất xấp xỉ toàn cầu [42] nếu sparse transformer X đạt được tính chất xấp xỉ toàn cầu. Đối với các phương pháp như BigBird [41], chúng duy trì tính chất xấp xỉ toàn cầu sử dụng các global tokens (CLS). Tuy nhiên, global attention làm cho nó khó áp dụng cho mô hình hóa autoregressive unidirectional (LM). Bên cạnh đó, random attention yêu cầu thao tác gather, làm cho nó rất chậm trên phần cứng dense như TPUs (Hình 3).

Mệnh đề 1. Combiner được đề xuất sẽ không phá vỡ tính chất xấp xỉ toàn cầu của các sparse transformers ban đầu.

Cụ thể, chúng tôi xem xét lớp hàm được xây dựng bằng cách xếp chồng attention block với mạng fully connected hai lớp. Chính thức, theo ký hiệu trong [42] chúng ta có block như

SAttn (X) = X+MultiHeadAttn (X); (13)
Z=SAttn (X) +relu (SAttnW1)W2; (14)

ký hiệu h-head attentions với X2RLd,W12Rdr, và W22Rrd. Lớp hàm được ký hiệu là

STH;r:=fX!t(X+E)jtlà một composition của block (13) ; (15)
Elà trainable position embedding g: (16)

Yun et al. [42] cho thấy rằng lớp hàm (15) vẫn là xấp xỉ toàn cầu w.r.t. chuẩn được định nghĩa là dp(f; g) :=R
kf(X)g(X)kp
pdX1=p
với softmax trong (1) và một số yêu cầu về các mẫu thưa thớt trong attention scheme.

B Combiner-Logsparse trong Trường Hợp MLM
Ở đây chúng tôi mở rộng Combiner-logsparse được giới thiệu trong phần 4.2 cho trường hợp MLM.

Bên cạnh dlog2ie supports không chồng chéo trong trường hợp LM, chúng ta có thể định nghĩa thêm dlog2ie supports không chồng chéo để attend đến các token sau token hiện tại trong chuỗi. Chúng tôi minh họa lựa chọn thiết kế này trong hình 4.

C Combiner-Axial trong Trường Hợp MLM
Bên cạnh !LM
axial-vertical ,!LM
axial-horizontal và !LM
axial-rowmajor được giới thiệu trong phần 4.3, ở đây chúng tôi giới thiệu cách chúng tôi mở rộng ba mô hình này cho trường hợp MLM.

•!MLM
axial-vertical: 
0
i= 
sparse MLM
i =fj:j1i1(modm)g[fj:j1i1(divm)g, và 
r
i=fj:jr(modm)g, cho r2[m]nfcolig. Như được mô tả trong Hình 2(A), 
r
i tương ứng với cột r phía trên hàngi, nơi chúng tôi sử dụng max pooling để có được abstraction. Để có được abstraction như vậy cho tất cả các vị trí, chúng ta có thể tận dụng toán tử cummax cho mỗi cột để có hiệu quả thu được prefix-max.

•!MLM
axial-horizontal: tương tự như !MLM
axial-vertical ngoại trừ việc mỗi 
r
i tóm tắt tất cả các hàng r và loại trừ cộti.

•!MLM
axial-rowmajor: 
0
i=fj:j1i1(divm)g,tức là, các phần tử trong cùng một hàng được attended trực tiếp, trong khi 
r
i=fj:jr(divm)g cho r2[n]nfrowigcaptures tất cả các hàng ngoại trừ hàngi.

Dễ thấy rằng độ phức tạp vẫn là O(Lp
L) nếu n; m =O(p
L).

D Combiner-Learnable
Như đã thảo luận trong phần 4.4. chúng tôi thiết kế Combiner-learnable như một phần mở rộng cho routing transformer [22], học để cluster các tokens. Mỗi token trong routing transformer chỉ attend đến các tokens trong cùng cluster. Như được hiển thị trong hình 4, Combiner-learnable của chúng tôi kết hợp direct expectation với local expectation (tokens vàng), mỗi cái tóm tắt một cluster (đỏ, xanh hoặc xanh lá).

--- TRANG 15 ---
Hình 4: Trái: Combiner-logsparse trong trường hợp MLM. Phải: Combiner-Learnable. Theo routing transformer [22], chúng tôi áp dụng nguyên tắc combiner, để chúng ta có thể đạt được attention đầy đủ trong mỗi head với độ phức tạp giống hệt với routing transformer.

E Chi Tiết Thực Nghiệm

E.1 CIFAR-10
Ở đây chúng tôi liệt kê các hyperparameters chúng tôi đã sử dụng trên dataset CIFAR-10. Các thực nghiệm của chúng tôi bao gồm (1) một nghiên cứu ablation, nơi tất cả các mô hình chia sẻ chính xác cùng kiến trúc; và (2) kết quả chính, nơi Combiner của chúng tôi đạt được kết quả state-of-the-art dưới setting không cho phép data augmentation.

Đối với nghiên cứu ablation, embedding và hidden size là 512. Chúng tôi sử dụng 8 attention heads trong mỗi lớp với tổng cộng 6 transformer layers. Chúng tôi huấn luyện tất cả các mô hình trong 400,000 steps với learning rate 1e-3 và batch size 32. Đối với kết quả chính, chúng tôi sử dụng cùng kiến trúc như được giới thiệu trong Child et al. [14], và chúng tôi huấn luyện Combiner-Axial của chúng tôi trong 1,200,000 steps với cosine learning rate scheduling. Chúng tôi chạy lại kết quả chính 3 lần và độ lệch chuẩn là 0.003.

E.2 ImageNet-64
Về chi tiết của ImageNet-64, chúng tôi sử dụng cùng thiết lập với CIFAR-10, bao gồm một nghiên cứu ablation và kết quả chính. Kiến trúc được sử dụng trong nghiên cứu ablation giống hệt với cái chúng tôi sử dụng trong CIFAR-10. Đối với kết quả chính của Combiner-Axial, chúng tôi sử dụng kiến trúc 30-layer với 768 hidden size và embedding dimension. Chúng tôi huấn luyện kiến trúc này trong 1,200,000 steps với cosine learning rate scheduling. Chúng tôi cũng chạy lại kết quả chính 3 lần và độ lệch chuẩn là 0.005.

E.3 Mô Hình Hóa Ngôn Ngữ Wiki-40B
Mục đích chính của thực nghiệm này không phải là theo đuổi hiệu suất state-of-the-art, vì nói chung, càng nhiều parameters/data, perplexity càng tốt hơn cho mô hình hóa ngôn ngữ. Vì vậy thay vào đó, chúng tôi để tất cả các phương pháp có cùng backbone mạng neural, chỉ thay đổi các triển khai attention để so sánh hiệu quả của chúng. Điều này tương tự về tinh thần với nghiên cứu ablation trong CIFAR-10 và ImageNet-64.

Cụ thể, chúng tôi sử dụng word embedding size và hidden size là 768 cho tất cả các lớp. Chúng tôi sử dụng 12 attention heads trong mỗi lớp, với tổng cộng 12 transformer layers. Chúng tôi sử dụng kiến trúc Pre-Norm, và các lớp MLP có hidden size bằng 4768. Độ dài chuỗi tối đa có thể thay đổi trong f2048;8192g, phụ thuộc vào giới hạn bộ nhớ của mỗi phương pháp. Tất cả các phương pháp được huấn luyện trong 125,000 stochastic gradient updates, với batch size bằng 128. Chúng tôi cũng cho phép cosine learning rate scheduling, với 10,000 warm-up steps. Optimizer là Adam với gradient clipping.

--- TRANG 16 ---
E.4 LRA Benchmark
Chúng tôi chủ yếu tuân theo hướng dẫn của LRA, nơi tất cả các mô hình nên sử dụng khoảng cùng số lượng parameters và cùng hyperparameters như batchsize, số iterations, v.v.. Chúng tôi đã cố gắng hết sức để tái tạo kết quả thực nghiệm sử dụng code trong https://github.com/google-research/long-range-arena, và chúng tôi thấy rằng chúng tôi không thể tái tạo kết quả pathfinder-32. Chúng tôi đã giao tiếp với các tác giả nhưng không giải quyết được vấn đề. Vì vậy thay vào đó, chúng tôi chạy lại tất cả các baselines sử dụng cùng cấu hình mạng, trên thiết lập pathfinder-32-inter. Chúng tôi thấy một số phương pháp ưa thích 'MEAN' pooling để có được biểu diễn chuỗi, trong khi những phương pháp khác ưa thích 'CLS' pooling. Vì vậy chúng tôi thử cả hai cho mỗi phương pháp, và báo cáo kết quả tốt nhất.

E.5 C4 Masked Language Modeling
Tương tự như mục đích của phần E.3, chúng tôi thực hiện tác vụ masked language modeling trên dataset C4, thường được sử dụng cho pretraining BERT. Vì metric perplexity tương quan tốt với hiệu suất tác vụ downstream, chúng tôi do đó thực hiện các thực nghiệm được kiểm soát với tất cả các phương pháp sử dụng cùng kiến trúc mạng.

Kiến trúc được sử dụng và hyperparameters gần như giống với phần E.3, ngoại trừ chúng tôi có số lượng segments tối đa bằng 2.
