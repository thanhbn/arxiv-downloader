# 2310.01427.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.01427.pdf
# Kích thước tệp: 776139 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SẮP XẾP ATTENTION CHỐNG LẠI THIÊN KIẾN RECENCY TRONG
CÁC MÔ HÌNH NGÔN NGỮ NGỮ CẢNH DÀI
BẢN THẢO
Alex Peysakhovich∗Adam Lerer†
4 tháng 10, 2023
TÓM TẮT
Các mô hình ngôn ngữ hiện tại thường không thể kết hợp các ngữ cảnh dài một cách hiệu quả trong quá trình sinh. Chúng tôi
cho thấy rằng một yếu tố góp phần chính vào vấn đề này là các ưu tiên attention có thể đã được học trong quá trình pre-
training: thông tin liên quan nằm ở phần đầu của ngữ cảnh được chú ý ít hơn trung bình. Tuy nhiên, ngay cả khi các mô hình
không sử dụng thông tin từ một tài liệu liên quan trong phản hồi của chúng, chúng vẫn chú ý ưu tiên đến tài liệu đó so với
một tài liệu không liên quan ở cùng vị trí. Chúng tôi tận dụng thực tế này để giới thiệu "sắp xếp attention": thực hiện một
bước giải mã, sắp xếp các tài liệu theo attention mà chúng nhận được (attention cao nhất đi cuối), lặp lại quá trình, sinh ra
câu trả lời với ngữ cảnh được sắp xếp mới. Chúng tôi thấy rằng sắp xếp attention cải thiện hiệu suất của các mô hình ngữ
cảnh dài. Các phát hiện của chúng tôi làm nổi bật một số thách thức trong việc sử dụng các mô hình ngôn ngữ có sẵn cho
việc sinh tăng cường truy xuất.

1 Giới thiệu
Các kỹ thuật gần đây đã cho phép các mô hình ngôn ngữ lớn (LLM) hoạt động trên ngữ cảnh cực lớn tương ứng
với hàng chục trang văn bản [ 5,20,30,7,19]. Những ngữ cảnh dài này có thể được điền với lịch sử người dùng, tài liệu,
hướng dẫn mô hình chi tiết, hoặc các ví dụ few-shot, cho phép mô hình trích xuất và tích hợp thông tin đa dạng một cách
động để hoàn thành nhiệm vụ của nó [ 24,23,26]. Chúng tôi sẽ gọi những nhiệm vụ như vậy là sinh tăng cường ngữ cảnh. Một
trường hợp sử dụng chính trong thực tế là sinh tăng cường truy xuất (RAG) nơi ngoài việc nhận một truy vấn, mô hình
nhận các tài liệu bổ sung trong ngữ cảnh từ một hệ thống truy xuất [ 14,21]. RAG rất phổ biến trong các ứng dụng như trả
lời câu hỏi, hoàn thành mã, và tóm tắt.

Tuy nhiên, việc liệu các transformer có thể thực sự sử dụng ngữ cảnh dài hiệu quả hay không là một lĩnh vực nghiên cứu
chính [ 29,28,18,15]. Ví dụ [ 15] thấy rằng LLM kém hơn trong một nhiệm vụ RAG khi tài liệu liên quan được đặt ở giữa
ngữ cảnh. Các tác giả đề cập rằng "việc quan sát hiệu ứng giống như vị trí nối tiếp trong LLM có lẽ đáng ngạc nhiên, vì
các cơ chế self-attention làm nền tảng cho Transformer LLM về mặt kỹ thuật có khả năng truy xuất bất kỳ token nào từ
ngữ cảnh của chúng một cách bình đẳng."

Một thủ phạm có thể có cho hiện tượng này là sự không khớp giữa nhiệm vụ mà LLM được huấn luyện và các nhiệm vụ
sinh tăng cường ngữ cảnh. Trong số các tài liệu thường được sử dụng để pre-train LLM như trang web, sách, bài báo và
mã, các token có thông tin nhất để dự đoán một token cụ thể thường là những token gần đây nhất. Trong quá trình pre-
training, điều này tạo ra một thiên kiến đã học để chú ý đến các token gần đây. Ngoài ra, sơ đồ rotary positional embedding
(RoPE) được sử dụng trong các mô hình mã nguồn mở mà chúng tôi nghiên cứu có một thiên kiến quy nạp hướng tới việc
giảm attention ở khoảng cách dài [ 27] có thể làm cho việc các mô hình này học cách chú ý ưu tiên đến các token gần đây
trở nên dễ dàng hơn. Thiên kiến recency cực đoan không phải là một prior tốt cho các nhiệm vụ sinh tăng cường ngữ cảnh
nơi các token xa có thể thực sự chứa thông tin rất liên quan.

Chúng tôi nghiên cứu giả thuyết này trong môi trường trả lời câu hỏi miền mở [ 14] sử dụng một nhiệm vụ 'một-tài-liệu-
liên-quan-nhiều-yếu-tố-gây-nhiễu-không-liên-quan' (distractor QA). Distractor QA là một testbed đơn giản để kiểm tra
khả năng của LM trong việc sử dụng ngữ cảnh của chúng. Các tài liệu nhiệm vụ của chúng tôi là tiểu sử một đoạn văn
của các cá nhân hư cấu được tạo ra bởi một mô hình ngôn ngữ. Mô hình được hỏi những câu hỏi đơn giản có thể được
trả lời dễ dàng từ tài liệu. Các distractor là
∗alex.peys@gmail.com. Hiện tại tại Sutter Hill Ventures
†adam.lerer@gmail.com. Hiện tại tại Google DeepMindarXiv:2310.01427v1  [cs.CL]  28 Sep 2023

--- TRANG 2 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO
tiểu sử của các cá nhân hư cấu khác. Nhiệm vụ của chúng tôi duy trì cấu trúc của một nhiệm vụ QA điển hình nhưng,
bởi vì tất cả dữ liệu đều được tạo ra, kết hợp một tính năng hữu ích là các mô hình không thể sử dụng kiến thức tiên
nghiệm của chúng để giúp với nhiệm vụ. Chúng tôi gọi đây là nhiệm vụ SynthWiki.

Chúng tôi nghiên cứu hiệu suất của bốn mô hình ngữ cảnh dài mã nguồn mở nhỏ và ba mô hình ngữ cảnh dài độc quyền
được phát hành bởi OpenAI và Anthropic.

Các mô hình mã nguồn mở mà chúng tôi nghiên cứu là TogetherComputer's Llama-2-7B-32k, Llama-2-7B-32k-Instruct,
YaRN Llama-2-7B-64k [ 19], và WizardLM-tuned Code-Llama-7B [ 16]. Tất cả đều dựa trên Llama-2 được pre-train với
cửa sổ ngữ cảnh cơ sở có kích thước 4096, nhưng tăng cửa sổ ngữ cảnh thông qua các biến thể của mở rộng mã hóa vị
trí và fine tuning. Các mô hình của TogetherComputer đặc biệt thú vị vì chúng được fine tune không chỉ trên dự đoán
token tiếp theo ngữ cảnh dài mà còn được fine tune rõ ràng trên QA ngữ cảnh dài.

Chúng tôi thấy rằng việc tăng số lượng distractor làm giảm hiệu suất mô hình trên cả LLM mã nguồn mở nhỏ và các
mô hình độc quyền lớn hơn. Sự giảm thay đổi giữa các mô hình với sự giảm nhỏ nhất trong Claude-1-Instant và Claude-
2, sự giảm lớn hơn trong GPT-3.5 và TogetherLlama, và sự giảm thảm khốc trong các mô hình mã nguồn mở còn lại
(Hình 1).

Không giống như các mô hình độc quyền dựa trên API, các mô hình mã nguồn mở cho chúng tôi truy cập vào các bộ
phận bên trong của mô hình. Điều này cho phép chúng tôi đi sâu hơn vào nguyên nhân của sự giảm. Chúng tôi thấy rằng
càng gần một tài liệu liên quan được đặt với phản hồi được tạo ra trong ngữ cảnh, nó càng nhận được nhiều trọng số
attention. Tuy nhiên, chúng tôi vẫn thấy rằng trung bình các tài liệu đúng nhận được nhiều attention hơn distractor ở
cùng vị trí trong ngữ cảnh. Điều này đưa ra ý tưởng về "sắp xếp attention": chạy một bước giải mã và xem các trọng
số attention mỗi token được tính trung bình qua các layer và head, sắp xếp các tài liệu theo các điểm attention này, tùy
chọn lặp lại quá trình này,3và cuối cùng chạy sinh mô hình với ngữ cảnh được sắp xếp mới này.

Chúng tôi thấy rằng sắp xếp attention cải thiện đáng kể độ chính xác QA của mô hình trong nhiệm vụ của chúng tôi
cho tất cả 3 mô hình mà chúng tôi nghiên cứu (Hình 5), và cho phép TogetherComputer's Llama khớp hoặc vượt qua
hiệu suất của các mô hình độc quyền tốt nhất trên nhiệm vụ QA của chúng tôi. Chúng tôi giả định độ chính xác cao hơn
và hiệu suất sắp xếp lại tốt hơn của TogetherLlama là do mô hình cụ thể đó đã được điều chỉnh đặc biệt cho QA ngữ
cảnh dài trong khi WizardCoder và YaRN thì không. Tuy nhiên, chúng tôi thấy rằng sắp xếp attention vẫn giúp đáng kể
các mô hình sau - đặc biệt với việc áp dụng lặp lại - tăng gấp đôi độ chính xác của chúng ở độ dài ngữ cảnh 30k.

2 SynthWiki: Một Tập dữ liệu Tổng hợp cho QA Trích xuất Ngữ cảnh Dài

Chúng tôi quan tâm đến việc nghiên cứu sinh tăng cường ngữ cảnh nơi một LLM nhận một câu hỏi q và một tập tài liệu
D và đưa ra một câu trả lời a(q, D). Điều này đôi khi được gọi là sinh tăng cường truy xuất hoặc QA miền mở trích xuất.

Chúng tôi sẽ nghiên cứu trường hợp đơn giản nhất nơi mô hình cần trích xuất chỉ một phần thông tin liên quan. Ở đây D
chứa một tài liệu d∗ chứa câu trả lời cho câu hỏi và các tài liệu khác Ddistract = d1, . . . , dk không chứa bất kỳ thông
tin nào liên quan đến câu hỏi. Chúng tôi sẽ nghiên cứu việc tăng kích thước của Ddistract ảnh hưởng đến hiệu suất
LLM như thế nào.

Một vấn đề đã biết với LLM là chúng có xu hướng sử dụng prior kiến thức từ quá trình huấn luyện ngay cả khi hướng
dẫn yêu cầu mô hình chỉ sử dụng thông tin từ ngữ cảnh [ 33]. Để nghiên cứu trả lời câu hỏi ngữ cảnh dài một cách riêng
biệt, chúng tôi tạo ra một tập dữ liệu tổng hợp của các tài liệu giảm thiểu sự nhiễm kiến thức từ pre-training.

Chúng tôi tạo ra tập dữ liệu của mình sử dụng một thủ tục madlib: đầu tiên chúng tôi sử dụng GPT3.5 để tạo ra các
kết hợp quốc tịch, nghề nghiệp, và tên cho một người hư cấu. Chúng tôi có một tập 20 loại câu hỏi (ví dụ: "X đã đi học
ở đâu?", "X sinh năm nào?" xem Phụ lục A cho danh sách đầy đủ). Chúng tôi lấy mẫu 2 câu hỏi như vậy, sau đó chúng
tôi sử dụng GPT4 để tạo ra một "bài viết Wikipedia một đoạn văn" ngắn tóm tắt cho người này chứa câu trả lời cho hai
câu hỏi được lấy mẫu ngẫu nhiên này. Chúng tôi có một số logic làm sạch để loại bỏ các câu hỏi tệ hoặc phản hồi được
định dạng kém. Chúng tôi gọi tập dữ liệu này là SynthWiki và dự định phát hành mã để tạo ra các tài liệu mới để tránh
nhiễm trong tương lai. Nhiệm vụ của chúng tôi có thể được nghĩ như một phiên bản ngôn ngữ tự nhiên của 'Little
Retrieval Test' [ 18] nơi các mô hình về cơ bản được yêu cầu truy xuất một giá trị từ một danh sách key, value dài.

Cho các thí nghiệm được báo cáo ở đây, chúng tôi tạo ra 500 mục SynthWiki ngẫu nhiên với 2 câu hỏi cho mỗi tài liệu.
Sau một số làm sạch nhỏ, chúng tôi còn lại 990 mục trong tập dữ liệu của mình. Một tiểu sử ví dụ như dưới đây:

3Trực quan, nhiều lần chạy hữu ích vì trong mỗi lần chạy, (các) tài liệu liên quan sẽ có xu hướng có điểm attention cao
hơn distractor ở cùng vị trí, và sẽ di chuyển ưu tiên về phía cuối ngữ cảnh. Thường bằng hai lần chạy các tài liệu đúng nằm
trong phần 'có thể sử dụng' của ngữ cảnh mô hình.

--- TRANG 3 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO
Amalia Varela là một nhà kinh tế học Argentina nổi tiếng, được biết đến nhiều nhất với những đóng góp
đáng kể của bà cho các chính sách tài chính và tiền tệ ở Argentina. Sinh tại thành phố nhộn nhịp Buenos
Aires, Varela đã thiết lập mình như một người ủng hộ đầy nhiệt huyết cho cải cách kinh tế và phát triển
bền vững tại quê hương của bà. Bà đã phục vụ như một cố vấn cho các tổ chức chính phủ và phi chính
phủ khác nhau, định hình bối cảnh kinh tế của Argentina thông qua các lý thuyết sáng tạo và nghiên cứu
nghiêm ngặt của bà. Là một người đam mê bóng đá thực sự, Varela cũng được biết đến với sự ủng hộ
không ngừng nghỉ của bà đối với Racing Club, một đội bóng đá được tôn trọng có trụ sở tại thành phố
sinh của bà. Trong suốt sự nghiệp lừng lẫy của mình, sự kết hợp kinh tế học và lòng tận tụy với đội
thể thao yêu thích của Varela đã củng cố vị thế của bà như một nhân vật biến đổi trong xã hội Argentina.

Đoạn văn này sẽ được ghép với các câu hỏi như: "Amalia Varela sinh tại thành phố nào?" hoặc "Tên của đội thể thao
yêu thích của Amalia Varela là gì?." Độ dài tài liệu trung bình là 165 token khi sử dụng tokenizer Llama/CodeLlama.
Định dạng của các prompt QA tuân theo gần với các trang mô hình trên HuggingFace và được hiển thị trong Phụ lục B.

Chúng tôi sử dụng SynthWiki để tạo ra K ví dụ của (qi, ai, di). Để xây dựng tập Ddistractor chúng tôi đặt kích thước
ngữ cảnh tối đa T̂ và lấy mẫu dj cho j ≠ i không thay thế để điền kích thước ngữ cảnh. Mỗi tài liệu này là distractor
không liên quan cho tài liệu đúng. Chúng tôi ngẫu nhiên hóa thứ tự của D khi chúng tôi trình bày nó cho mô hình.

3 Hiệu suất của LLM Ngữ cảnh Dài trên SynthWiki

[Biểu đồ hiển thị hiệu suất QA giảm theo kích thước ngữ cảnh cho các mô hình khác nhau]

Hình 1: Hiệu suất của tất cả các mô hình ngữ cảnh dài mà chúng tôi nghiên cứu về trả lời câu hỏi giảm khi thông tin
liên quan được nhúng trong một ngữ cảnh dài của văn bản distractor không liên quan.

Chúng tôi nghiên cứu tập các mô hình sau:

1. Mô hình Llama-2-7B-32k của togethercomputer mở rộng ngữ cảnh Llama-2 cơ sở [ 30] thông qua nội suy vị trí
của các mô hình gốc. Ngoài ra, mô hình này được fine tune đặc biệt trên các nhiệm vụ tóm tắt và QA ngữ cảnh dài.

2. Llama-2-7B-32k-Instruct của togethercomputer thực hiện fine tuning bổ sung trên một công thức khác của
việc tuân theo hướng dẫn, trò chuyện nhiều lượt, tóm tắt sách, và nhiều QA ngữ cảnh dài hơn.

3. Wizard Code Llama-2 7B là một phiên bản instruction tuned của Code LLama 2 [ 16]. Code LLama 2 cơ sở đã
đi kèm với ngữ cảnh mở rộng lên đến 100k được mở rộng thông qua dự đoán token tiếp theo ngữ cảnh dài. Chúng
tôi chọn phiên bản Wizard vì nó đi kèm với instruction tuning rõ ràng.

4. YaRN Llama-2-7b-64k [ 19] sử dụng một phương pháp mới để mở rộng ngữ cảnh kết hợp với fine tuning dựa
trên dự đoán token tiếp theo. Chúng tôi lưu ý rằng mô hình này không được instruction tuned. Tại thời điểm viết
bài này, các tác giả không biết về bất kỳ phiên bản instruction-tuned nào của mô hình này. Chúng tôi sẽ cập nhật
bài báo ngay khi chúng có sẵn.

--- TRANG 4 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO
Chúng tôi cũng xem xét một số mô hình độc quyền có sẵn thông qua API:

1. GPT-3.5-turbo-16k là phiên bản ngữ cảnh 16k của GPT 3.5 của OpenAI. Tại thời điểm viết bài này, không có
chi tiết về cách đạt được ngữ cảnh 16k.

2. Claude-2 và Claude-1-Instant là các mô hình ngữ cảnh 100k từ Anthropic. Ở đây, một lần nữa, chi tiết về kiến
trúc không có sẵn.

3. Tại thời điểm viết bài này, không tác giả nào có quyền truy cập vào GPT-4-32k. Chúng tôi dự định cập nhật bài
báo này khi chúng tôi có thể truy cập API.

Chúng tôi sử dụng các phiên bản sản xuất của các mô hình này tính đến tháng 9 năm 2023.

Chúng tôi đầu tiên xem độ chính xác tại SynthWiki thay đổi như thế nào khi chúng tôi thay đổi tổng kích thước ngữ
cảnh bằng cách thêm các tài liệu distractor. Đối với các mô hình mã nguồn mở, chúng tôi đánh giá trên tất cả 990 câu
hỏi (445 tài liệu riêng biệt), trong khi đối với các mô hình dựa trên API, chúng tôi đánh giá một mẫu của 100 câu hỏi
ngẫu nhiên.

Chúng tôi báo cáo độ dài ngữ cảnh cho tất cả các mô hình theo tokenizer Llama để so sánh công bằng giữa các mô
hình. Vì các tokenizer GPT và Claude có kích thước từ vựng khác với tokenizer Llama, ngữ cảnh của chúng sẽ, trung
bình, tương ứng với ít token hơn cho cùng một đầu vào văn bản.

Chúng tôi báo cáo kết quả cho độ dài ngữ cảnh 1000, 16,000, và 30,000 token (Llama). Chúng tôi điền ngữ cảnh bằng
cách lấy mẫu ngẫu nhiên các tài liệu distractor cho đến khi mẫu tiếp theo sẽ vượt qua ngữ cảnh tối đa mà chúng tôi cho
phép. Thứ tự tài liệu được xáo trộn để tài liệu đúng ở vị trí ngẫu nhiên. Điều này cho chúng tôi trung bình 5.5, 95, và
180 tài liệu tổng cộng, tương ứng.

Chúng tôi loại bỏ dấu ngoặc kép khỏi phản hồi mô hình, và tính một phản hồi mô hình là đúng nếu nó chứa một khớp
chuỗi chính xác với câu trả lời đúng. Kiểm tra thủ công các mẫu ngẫu nhiên thấy rằng khoảng 2% câu trả lời đúng bị
phân loại sai do sự biến đổi trong các cách có thể trả lời câu hỏi.

Chúng tôi thấy rằng trong tất cả các mô hình độ chính xác giảm theo độ dài ngữ cảnh (Hình 1). Tất cả xu hướng trông
như mong đợi, ngoại trừ chúng tôi thấy rằng Together-Llama-Instruct dường như ít chính xác hơn trong ngữ cảnh ngắn
so với các mô hình 7B khác nhưng làm tốt hơn đáng kể trong ngữ cảnh dài hơn. Không rõ với chúng tôi tại sao như vậy.

Trong Hình 2 chúng tôi vẽ đồ thị độ chính xác như một hàm của vị trí của tài liệu đúng trong ngữ cảnh. Chúng tôi thấy
một sự nhân bản chung của kết quả 'lost in the middle' [ 15] thấy rằng LLM ưa thích thông tin ở đầu ngữ cảnh (thiên
kiến primacy) và cuối (thiên kiến recency). Tuy nhiên, chúng tôi thấy rằng khi ngữ cảnh trở nên dài hơn, và đặc biệt
trong togetherLlama, thiên kiến primacy đối với các tài liệu đầu trở nên nhỏ hơn và chúng tôi thấy chủ yếu thiên kiến
recency.

[Biểu đồ hiển thị độ chính xác theo vị trí tài liệu trong ngữ cảnh]

Hình 2: Độ chính xác QA trên SynthWiki như một hàm của vị trí của tài liệu liên quan trong ngữ cảnh. Chúng tôi thấy
một sự nhân bản của hiệu ứng 'lost in the middle'[ 15] trên tập dữ liệu này trong đó độ chính xác thấp hơn khi thông
tin liên quan ở giữa một ngữ cảnh dài. Hiệu ứng recency (thông tin hướng về cuối ngữ cảnh) dường như khá chung trên
các mô hình và độ dài ngữ cảnh. Tuy nhiên, hiệu ứng primacy (tài liệu đầu tiên) dường như ít chung hơn.

--- TRANG 5 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO
Trong Hình 3 chúng tôi vẽ đồ thị attention mô hình trong bước đầu tiên của việc sinh. Chúng tôi tính tổng attention
mô hình post-softmax qua các head và token tài liệu để có được điểm attention cho mỗi tài liệu. Chúng tôi vẽ đồ thị
các điểm attention cho mỗi mô hình dựa trên vị trí tài liệu và liệu tài liệu có phải là tài liệu đúng cho câu hỏi đã cho.

[Biểu đồ hiển thị trọng số attention theo vị trí tài liệu]

Hình 3: Trọng số attention trung bình theo vị trí token nguồn cho các độ dài ngữ cảnh khác nhau, tính trung bình trên
tất cả các layer và attention head. Các trọng số attention chỉ được tính cho token phản hồi được tạo đầu tiên. Ở độ dài
ngữ cảnh dài, cả ba mô hình đều cho thấy một thiên kiến mạnh hướng tới việc chú ý đến các token gần đây nhất, cũng
như một thiên kiến yếu hơn hướng tới các token ban đầu. Tất cả các mô hình cũng chú ý mạnh hơn nhiều đến các tài
liệu liên quan so với các tài liệu distractor.

Chúng tôi một lần nữa thấy một thiên kiến recency mạnh trong attention trung bình cho ngữ cảnh dài hơn. Nhưng chúng
tôi cũng quan sát rằng tài liệu chứa câu trả lời nói chung có attention cao hơn nhiều so với distractor ở cùng vị trí trong
ngữ cảnh.

4 Sắp xếp lại Tài liệu theo Attention

Các phát hiện của chúng tôi gợi ý một can thiệp đơn giản: tính trọng số attention tài liệu trung bình cho bước đầu tiên
của giải mã, và sau đó sắp xếp lại các tài liệu, đặt những tài liệu có trọng số attention cao nhất cuối cùng. Chúng tôi
minh họa phương pháp này trong Hình 4.

Hình 6 hiển thị kết quả của việc thực hiện sắp xếp attention trong ngữ cảnh dài. Trung bình, các tài liệu liên quan di
chuyển gần hơn đến cuối danh sách tài liệu với mỗi lần sắp xếp lại liên tiếp. Chúng tôi thấy rằng trung bình, một lần
lặp sắp xếp lại di chuyển tài liệu liên quan gần hơn đến cuối ngữ cảnh nhưng đối với các tài liệu xa cuối, một lần sắp
xếp duy nhất có thể để chúng ở giữa ngữ cảnh.

Thực tế cuối cùng này thúc đẩy chúng tôi tính trọng số attention và sắp xếp lại các tài liệu trong ngữ cảnh nhiều lần4.

Trong panel dưới của Hình 5 chúng tôi thấy rằng sắp xếp attention cải thiện độ chính xác trên SynthWiki. Đối với
TogetherLlama, sau 2 lần lặp chúng tôi không còn đạt được nhiều tăng trong hiệu suất QA. Đối với các mô hình khác
chúng tôi tiếp tục thấy tăng độ chính xác tất cả cách lên đến 5 lần sắp xếp attention, vì thiên kiến recency mạnh trên
các mô hình này đến mức các tài liệu ở giữa ngữ cảnh cần nhiều lần sắp xếp để được di chuyển đến cuối.

4Thay vào đó, có thể có thể sửa chữa cho một thiên kiến attention ước tính mỗi vị trí, điều này sẽ tránh nhiều bước
sắp xếp lại, mặc dù không rõ như một tính toán như vậy sẽ chung đến mức nào qua các nhiệm vụ hoặc thậm chí qua nhiều câu
hỏi trong cùng một QA.

--- TRANG 6 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

Hình 4: Một minh họa của thủ tục sắp xếp attention. Attention mỗi tài liệu trung bình được tính cho token phản hồi
được tạo đầu tiên, và sau đó các tài liệu được sắp xếp trong ngữ cảnh với attention cao nhất ở cuối. Sau k vòng của
thủ tục sắp xếp này, phản hồi được tạo ra.

[Biểu đồ hiển thị hiệu quả của sắp xếp attention trên độ chính xác]

Hình 5: Hiệu quả của sắp xếp attention trên SynthWiki. Sắp xếp attention tăng hiệu suất mô hình nhỏ. Đối với mô
hình TogetherLlama-Instruct, sắp xếp lại phục hồi hầu hết sự suy giảm hiệu suất từ ngữ cảnh dài và khớp với hiệu
suất của Claude-2.

5 Một số Thứ Khác Chúng tôi Đã Thử

Bây giờ chúng tôi thảo luận một số kết quả khác mà chúng tôi tìm thấy trong các khám phá của mình có thể quan tâm
đến các độc giả chuyên môn.

Đầu tiên, chúng tôi có thể phân tích trọng số điểm attention theo layer. Chúng tôi thấy rằng attention khác biệt đến
tài liệu đúng so với distractor xảy ra chủ yếu trong các layer giữa. Không rõ phải hiểu gì từ thực tế này. Chúng tôi
không thấy rằng việc hạn chế tính toán sắp xếp attention của chúng tôi vào các layer giữa cải thiện hiệu suất đáng chú
ý. Chúng tôi báo cáo nó ở đây cho độc giả quan tâm.

Chúng tôi đã giả định rằng suy giảm hiệu suất đến không chỉ từ thiên kiến vị trí (mà sắp xếp attention giải quyết),
mà còn một sự pha loãng chung của attention. Vì trọng số attention softmax dương và tổng bằng 1, việc có nhiều tài
liệu không liên quan đơn điệu tăng nhiễu đi vào biểu diễn. Vì vậy chúng tôi đã thử một số phương pháp cắt ngắn
attention rõ ràng.

--- TRANG 7 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

[Biểu đồ hiển thị hiệu quả của sắp xếp attention trên vị trí tài liệu đúng]

Hình 6: Hiệu quả của sắp xếp attention trên vị trí tài liệu đúng. Mỗi panel hiển thị phân phối vị trí tài liệu sau một
vòng sắp xếp attention cho các tài liệu bắt đầu trong một tứ phân vị nhất định của vị trí ngữ cảnh. Nói chung, tài liệu
đúng di chuyển về phía cuối ngữ cảnh, tuy nhiên, khi tài liệu đúng hướng về đầu ngữ cảnh nó không di chuyển tất cả
cách đến cuối trong một lần. Sắp xếp attention hoạt động đến một mức độ nào đó cho tất cả các mô hình, nhưng những
mô hình được huấn luyện trên QA ngữ cảnh dài (tức là các mô hình Together) chú ý hiệu quả hơn qua ngữ cảnh và do
đó làm tốt hơn.

[Biểu đồ hiển thị trọng số attention theo layer transformer]

Hình 7: Trọng số attention trung bình theo vị trí tài liệu cho các độ dài ngữ cảnh khác nhau. Điểm attention cho token
giải mã đầu tiên, và được tính trung bình qua tất cả các head và tất cả các token trong một tài liệu. Sự khác biệt điểm
attention giữa tài liệu đúng và distractor xảy ra chủ yếu trong các layer giữa trong tất cả ba mô hình dựa trên Llama2.
(Điểm attention không tổng bằng 1 vì mô hình cũng có thể chú ý đến các token không phải tài liệu như câu hỏi.)

--- TRANG 8 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO
Chúng tôi đã thử cắt ngắn ngữ cảnh đến top-K tài liệu theo trọng số attention trung bình và thấy rằng điều này cải
thiện hiệu suất một chút với một lần sắp xếp lại nhưng sau nhiều lần sắp xếp lại, cắt ngắn không còn cung cấp nhiều
lợi ích. Tuy nhiên, điều này có thể trình bày một tối ưu hóa hiệu quả, vì sắp xếp lại có thể được thực hiện với một bước
duy nhất của giải mã, và sau đó giải mã đầy đủ có thể được thực hiện với một ngữ cảnh cắt ngắn.

Chúng tôi cũng đã thử cắt ngắn trực tiếp phân phối attention sử dụng top-k hoặc nucleus sampling [ 11]. Thật không
may không có bất kỳ fine-tuning nào trong chế độ này điều này dẫn đến suy giảm nghiêm trọng hiệu suất mô hình.

6 Công trình Liên quan

Mô hình Ngữ cảnh Dài Self-attention là bậc hai trong độ dài chuỗi vì vậy các biến thể transformer có thể hoạt động
theo cách hiệu quả tính toán trên ngữ cảnh dài là một lĩnh vực nghiên cứu tích cực, xem [ 29] để có một tổng quan hữu
ích. Có lẽ đổi mới quan trọng nhất cho phép các mô hình ngữ cảnh dài là FlashAttention, triển khai attention tiêu chuẩn
nhưng sử dụng tiling và tính toán lại để tránh tiêu thụ bộ nhớ bậc hai và sử dụng băng thông bộ nhớ GPU hiệu quả hơn[9].

Gần đây cộng đồng mã nguồn mở đã tìm thấy một số thủ thuật cho phép mở rộng độ dài ngữ cảnh cho các mô hình mã
nguồn mở như Llama2 (pre-trained với độ dài ngữ cảnh 4K) ở giai đoạn fine-tuning, dẫn đến việc phát hành các phiên
bản ngữ cảnh dài của các mô hình này [7, 19].

Công trình gần đây đã cho thấy rằng các mô hình ngữ cảnh dài không sử dụng thông tin ở giữa ngữ cảnh của chúng
hiệu quả [15]. Anthropic cũng đã xuất bản hướng dẫn cho prompting hiệu quả của các mô hình ngữ cảnh dài, làm nổi
bật rằng thông tin quan trọng nên được đặt ở cuối ngữ cảnh[ 1]. Họ gợi ý sử dụng một scratchpad mô hình để kéo thông
tin quan trọng của ngữ cảnh đến cuối, khá tương tự với sắp xếp attention (mặc dù hoạt động trực tiếp trong không
gian văn bản).

Sinh Tăng cường Truy xuất Sinh tăng cường truy xuất là một framework tăng cường các mô hình ngôn ngữ tham số
với khả năng truy xuất thông tin từ một cơ sở dữ liệu văn bản phi tham số [ 14]. Thông thường, một mô hình truy xuất
chọn các đoạn văn từ cơ sở dữ liệu và đặt chúng trong ngữ cảnh của LM generator. Framework RAG mang lại một số
lợi thế: truy cập vào một bộ nhớ khổng lồ mà không tăng flops mô hình hoặc số lượng tham số, khả năng xác định nguồn
gốc của một phản hồi mô hình, và khả năng thích ứng nhanh chóng với các thiết lập khác nhau (ví dụ các codebase khác
nhau) mà không cần huấn luyện lại mô hình.

RAG thường được công thức hóa như sự kết hợp của một mô hình truy xuất chọn một tập tài liệu để được đặt trong
ngữ cảnh của một mô hình generator, tạo ra một phản hồi. Vì hiệu suất của hệ thống RAG bị giới hạn bởi recall của
retriever và do đó số lượng tài liệu được trả về, các cửa sổ ngữ cảnh lớn hơn có thể cải thiện hiệu suất của các hệ
thống này một cách tự nhiên.

Công trình trước đây đã chứng minh hiệu suất modeling ngôn ngữ và trả lời câu hỏi vượt trội với ít tham số hơn bằng
cách sử dụng một cơ sở dữ liệu văn bản khổng lồ (nhiều hơn thấy tại thời gian huấn luyện) [14, 10, 13]. LM tăng cường
truy xuất thường thực hiện đặc biệt tốt trên QA miền mở, nơi một DB kiến thức rõ ràng (ví dụ wikipedia) có thể cải
thiện hiệu suất đáng kể [14].

Các phương pháp giống truy xuất ngược lại có thể được sử dụng để cải thiện sinh ngữ cảnh dài. Memorizing Transformer
cải thiện modeling ngôn ngữ ngữ cảnh dài bằng cách thực hiện một tìm kiếm kNN gần đúng vào một lịch sử ngữ cảnh
dài dựa trên một cơ chế giống attention, thay vì chú ý trực tiếp đến toàn bộ lịch sử [ 34]. Cơ chế truy xuất này cung cấp
hiệu suất ngữ cảnh dài vượt trội so với các kiến trúc nén ngữ cảnh dài thành một biểu diễn kích thước cố định như [8].
Trong Memorizing Transformer, truy xuất được công thức hóa như một cách hiệu quả tính toán để điều kiện trên một
ngữ cảnh dài, nhưng công việc của chúng tôi gợi ý rằng sự thưa thớt attention được tạo ra bởi truy xuất cũng có thể
cải thiện hiệu suất của mô hình.

Sự Nổi bật Dựa trên Attention Trọng số attention từ lâu đã được sử dụng để hình dung sự nổi bật của các phần khác
nhau của đầu vào mô hình trên các dự đoán của nó, bao gồm các bài báo gốc đề xuất cơ chế attention [ 2] (xem [ 4] Sec.
3 cho một khảo sát).

Trong văn học khả năng diễn giải có một số bất đồng về mức độ phù hợp của trọng số attention như các biến giải thích
[ 31,25,3]. Trọng số attention cũng đã được sử dụng như một tín hiệu saliency cho các mục đích khác nhau qua các
phương thức khác bao gồm thị giác và các mô hình neural cho sinh học [6, 35, 22].

Trong khi hầu hết các hệ thống truy xuất dựa trên độ tương tự ngữ nghĩa giữa tài liệu và truy vấn, một số công trình
trước đây đã gợi ý sử dụng chưng cất attention cho một tín hiệu truy xuất [ 12]. Trong công việc đó, các nhãn tổng hợp
cho các cặp truy vấn-tài liệu được tạo ra từ một mô hình ngôn ngữ dựa trên attention giữa truy vấn và tài liệu khi được
đặt trong ngữ cảnh, và các nhãn này được sử dụng để huấn luyện một mô hình retriever. Sắp xếp attention của chúng
tôi liên quan chặt chẽ đến ý tưởng này.

--- TRANG 9 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

7 Kết luận và Hạn chế

Trong công việc này, chúng tôi đã điều tra một nguồn tiềm năng của suy giảm chất lượng trên các nhiệm vụ ngữ cảnh
dài và đã chứng minh một thủ thuật thời gian suy luận rẻ - sắp xếp attention - giúp giải quyết nó.

Sắp xếp attention có một số hạn chế. Nhiều nhiệm vụ ngữ cảnh dài không ánh xạ gọn gàng đến một tập các tài liệu có
thể hoán vị; trong các thiết lập này người ta vẫn có thể muốn di chuyển các đoạn văn nhất định đến cuối ngữ cảnh nhưng
ít rõ ràng hơn về cách làm như vậy trong khi duy trì ngữ nghĩa gốc của prompt. Ngoài ra, trong các nhiệm vụ tăng cường
ngữ cảnh như sinh mã, độ chi tiết (tệp, hàm, v.v...) của các yếu tố có thể hoán vị không rõ ràng a priori.

Ngoài ra, các nhiệm vụ truy xuất ngữ cảnh dài thực tế thường có distractor liên quan thay vì chỉ ngẫu nhiên. Các
distractor 'khó' như vậy thực sự có thể được chọn bởi sắp xếp attention. Nếu chúng ta xem xét một nhiệm vụ RAG nơi
một số tài liệu có thể chứa 'sự thật sai' hoặc nếu các tài liệu distractor được thiết kế đối kháng thì một retriever/re-
ranker dựa trên attention thuần túy sẽ không hoạt động tốt. Kết hợp sắp xếp attention với các mục tiêu truy xuất khác
(ví dụ chưng cất perplexity, độ tương tự ngữ nghĩa, lọc chất lượng cao, thực tế) là một lĩnh vực thú vị cho công việc
tương lai.

Tuy nhiên, thủ thuật sắp xếp attention là một ví dụ thú vị về cách một mô hình có thể được tận dụng để sửa đổi ngữ
cảnh của chính nó theo cách cải thiện khả năng thực hiện một nhiệm vụ. Trong khi công trình trước như chain of thought
[ 32] và scratchpad [ 17] hoạt động trong không gian ngôn ngữ, chúng tôi sử dụng trọng số attention như một tín hiệu.
Ý tưởng sử dụng một mô hình để tinh chỉnh ngữ cảnh của chính nó một cách lặp đi lặp lại qua nhiều lần chạy cũng có
thể hữu ích cho các tình huống khác.

Trong khi sắp xếp attention có thể được áp dụng cho một LLM 'có sẵn', thực tế rằng nó hoạt động tốt hơn với một mô
hình được huấn luyện đặc biệt trên QA ngữ cảnh dài gợi ý rằng một giải pháp có nguyên tắc hơn có thể sẽ mang lại
lợi ích tốt hơn trong các nhiệm vụ RAG thực tế là một fine-tuning chung của mô hình LLM với retriever của nó. Do đó,
chúng tôi gợi ý rằng một hướng quan trọng hơn cho công việc tương lai là nghiên cứu cách đạt được sự hài hòa này
một cách hiệu quả.

8 Lời cảm ơn

Các thí nghiệm được thực hiện trên phần cứng được cung cấp bởi Sutter Hill Ventures. Chúng tôi cảm ơn Arthur Szlam
cho phản hồi hữu ích về phiên bản đầu của bản thảo này.

Tài liệu tham khảo

[1] Anthropic. Prompt engineering for claude's long context window, 2023. Truy cập: 2023-09-25.

[2] Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Neural machine translation by jointly learning to align
and translate. arXiv preprint arXiv:1409.0473, 2014.

[3] Jasmijn Bastings và Katja Filippova. The elephant in the interpretability room: Why use attention as explanation
when we have saliency methods? arXiv preprint arXiv:2010.05607, 2020.

[4] Yonatan Belinkov và James Glass. Analysis methods in neural language processing: A survey. Transactions of
the Association for Computational Linguistics, 7:49–72, 2019.

[5] Iz Beltagy, Matthew E Peters, và Arman Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150, 2020.

[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko.
End-to-end object detection with transformers. In European conference on computer vision, pages 213–229.
Springer, 2020.

[7] Shouyuan Chen, Sherman Wong, Liangjian Chen, và Yuandong Tian. Extending context window of large
language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

[8] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. Transformer-xl:
Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

[9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. Flashattention: Fast and memory-efficient
exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. Retrieval augmented language
model pre-training. In International conference on machine learning, pages 3929–3938. PMLR, 2020.

[11] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text degeneration.
arXiv preprint arXiv:1904.09751, 2019.

[12] Gautier Izacard và Edouard Grave. Distilling knowledge from reader to retriever for question answering. arXiv
preprint arXiv:2012.04584, 2020.

--- TRANG 10 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

[13] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và
Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906,
2020.

[14] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.

[15] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và Percy Liang.
Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.

[16] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei
Lin, và Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint
arXiv:2306.08568, 2023.

[17] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David
Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate
computation with language models. arXiv preprint arXiv:2112.00114, 2021.

[18] Dimitris Papailiopoulos, Lee Kangwook, và Jy-yong Sohn. the-little-retrieval-test, 2023.

[19] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, và Enrico Shippole. Yarn: Efficient context window extension of
large language models. arXiv preprint arXiv:2309.00071, 2023.

[20] Ofir Press, Noah A Smith, và Mike Lewis. Train short, test long: Attention with linear biases enables input
length extrapolation. arXiv preprint arXiv:2108.12409, 2021.

[21] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, và Yoav Shoham.
In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023.

[22] Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, và Alexander Rives. Transformer protein language
models are unsupervised structure learners. Biorxiv, pages 2020–12, 2020.

[23] Ohad Rubin và Jonathan Berant. Long-range language modeling with self-retrieval. arXiv preprint
arXiv:2306.13421, 2023.

[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, và Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint
arXiv:2302.04761, 2023.

[25] Sofia Serrano và Noah A Smith. Is attention interpretable? arXiv preprint arXiv:1906.03731, 2019.

[26] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, và
Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.

[27] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, và Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

[28] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, và Mohit Iyyer. Do long-range language models
actually use long-range context? arXiv preprint arXiv:2109.09115, 2021.

[29] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint
arXiv:2011.04006, 2020.

[30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023.

[31] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, và Manaal Faruqui. Attention interpretability across
nlp tasks. arXiv preprint arXiv:1909.11218, 2019.

[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems, 35:24824–24837, 2022.

[33] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang,
Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846,
2023.

[34] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, và Christian Szegedy. Memorizing transformers. arXiv preprint
arXiv:2203.08913, 2022.

--- TRANG 11 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

[35] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, và
Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International
conference on machine learning, pages 2048–2057. PMLR, 2015.

--- TRANG 12 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

A Xây dựng Tập dữ liệu SynthWiki

Tập dữ liệu SynthWiki được xây dựng như sau:

Đầu tiên, một tập nguồn gốc và nghề nghiệp được xây dựng bằng cách truy vấn GPT-3.5 với các prompt sau:

Cho tôi một danh sách 50 nguồn gốc cho mọi người (ví dụ: Canada, Texas, Châu Âu,
Monaco), xuất ra như một danh sách phân cách bằng dấu phẩy. Chỉ xuất danh sách,
không gì khác.

Cho tôi một danh sách 50 nghề nghiệp mà mọi người có thể trở nên nổi tiếng (ví dụ:
lập trình viên, doanh nhân, tài xế taxi, cầu thủ bóng rổ), xuất ra như một danh sách
phân cách bằng dấu phẩy. Chỉ xuất danh sách, không gì khác.

Cho mỗi mục, một nguồn gốc và nghề nghiệp ngẫu nhiên được chọn và một tên cho người đó được tạo ra với prompt
sau đây cho GPT-3.5:

Tôi đang viết một cuốn tiểu thuyết, giúp tôi nghĩ ra một tên cho một {nguồn_gốc} {nghề_nghiệp}
nổi tiếng. Đừng xuất tên của ai đã tồn tại. Chỉ xuất tên, không giải thích.

Cuối cùng, cho 500 cặp (tên, nguồn gốc, nghề nghiệp) duy nhất, GPT-4 được truy vấn với prompt:

Vui lòng viết một bài viết wikipedia một đoạn văn cho một {nguồn gốc} {nghề nghiệp}
nổi tiếng tên {tên}.

Đảm bảo bài viết chứa thông tin có thể trả lời các câu hỏi sau:
{câu_hỏi1}
{câu_hỏi2}

Chỉ xuất bài viết, không giải thích phụ.

Hai câu hỏi được chọn ngẫu nhiên từ danh sách sau:

"Tại thành phố nào {người} sinh ra?",
"{người} sinh năm nào?",
"{người} đã đi học đại học ở đâu?",
"Tên của vợ/chồng {người} là gì?",
"Tên của công ty đầu tiên {người} làm việc là gì?",
"Công ty {người} thành lập được gọi là gì?",
"Tiêu đề của bộ phim {người} đạo diễn là gì?",
"Ai là thần tượng của {người}?",
"Tên của thú cưng {người} là gì?",
"Màu yêu thích của {người} là gì?",
"{người} đã đi học trung học ở đâu?",
"Tên của bạn thân nhất {người} là gì?",
"Tiêu đề của bộ phim yêu thích {người} là gì?",
"{người} kết hôn vào năm nào?",
"Tiêu đề của cuốn sách yêu thích {người} là gì?",
"Tên của đứa con đầu lòng {người} là gì?",
"Tên của đội thể thao yêu thích {người} là gì?",
"{người} sinh tại quốc gia nào?",
"Tiêu đề của luận văn tiến sĩ {người} là gì?",
"{người} chơi môn thể thao nào?",

Hai câu hỏi được sử dụng trong prompt xây dựng cũng là những câu được sử dụng trong nhiệm vụ QA.

B Xây dựng Prompt SynthWiki

Prompt cho mô hình Wizard được đưa ra rõ ràng trên kho mô hình. Lưu ý rằng prompt gốc chứa 'Response' thay vì
'Answer'. Chúng tôi thấy rằng các mô hình có độ chính xác tốt hơn khi chúng tôi sử dụng 'Answer' thay vì 'Response',
vì vậy đây là những con số chúng tôi báo cáo.

--- TRANG 13 ---
Sắp xếp Attention Chống lại Thiên kiến Recency BẢN THẢO

Dưới đây là một hướng dẫn mô tả một nhiệm vụ. Viết một phản hồi hoàn thành
thích hợp yêu cầu.

### Hướng dẫn
Đây là một số thông tin bạn sẽ sử dụng để trả lời câu hỏi. Một số thông tin có thể
không liên quan.

### Thông tin
TÀI LIỆU: {tài_liệu}
TÀI LIỆU: {tài_liệu}
TÀI LIỆU: {tài_liệu}
...

### Câu hỏi
{câu_hỏi}

Vui lòng chỉ trả lời câu hỏi. Trả lời ngắn gọn.

### Trả lời

Định dạng prompt cho Together-Llama-Instruct cũng được đưa ra rõ ràng trên kho mô hình.

[INST]
Đây là một số thông tin bạn sẽ sử dụng để trả lời câu hỏi. Một số thông tin có thể
không liên quan.

### Thông tin
TÀI LIỆU: {tài_liệu}
TÀI LIỆU: {tài_liệu}
TÀI LIỆU: {tài_liệu}
...

### Câu hỏi
{câu_hỏi}

Vui lòng chỉ trả lời câu hỏi. Trả lời ngắn gọn.
[/INST]