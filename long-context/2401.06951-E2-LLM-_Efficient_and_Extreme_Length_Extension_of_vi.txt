# E2-LLM: Efficient and Extreme Length Extension of Large Language Models
# E2-LLM: Mở rộng Độ dài Hiệu quả và Cực đại cho các Mô hình Ngôn ngữ Lớn

Jiaheng Liu*1, Zhiqi Bai*1, Yuanxing Zhang1, Chenchen Zhang1, Yu Zhang1,
Ge Zhang2,Jiakai Wang1,Haoran Que1,Yukang Chen3,Wenbo Su1,Tiezheng Ge1,
Jie Fu4,Wenhu Chen2,Bo Zheng1
1Alibaba Group,2University of Waterloo,3The Chinese University of Hong Kong,
4The Hong Kong University of Science and Technology
{ljh411989, baizhiqi.bzq}@taobao.com

## Tóm tắt

Thông thường, việc huấn luyện các LLM với kích thước ngữ cảnh dài rất tốn kém về mặt tính toán, đòi hỏi nhiều giờ huấn luyện và tài nguyên GPU. Các phương pháp mở rộng ngữ cảnh dài hiện có thường cần các quy trình huấn luyện bổ sung để hỗ trợ các cửa sổ ngữ cảnh dài tương ứng, trong đó cần có dữ liệu huấn luyện ngữ cảnh dài (ví dụ: 32k) và giả định chi phí huấn luyện GPU cao. Để giải quyết các vấn đề nêu trên, chúng tôi đề xuất một phương pháp mở rộng độ dài Hiệu quả và Cực đại cho các Mô hình Ngôn ngữ Lớn, gọi là E2-LLM, với chỉ một quy trình huấn luyện và chi phí tính toán giảm đáng kể, đồng thời loại bỏ nhu cầu thu thập dữ liệu ngữ cảnh dài. Cụ thể, đầu tiên, dữ liệu huấn luyện của E2-LLM chỉ yêu cầu độ dài ngắn (ví dụ: 4k), điều này giảm đáng kể chi phí điều chỉnh. Thứ hai, quy trình huấn luyện trên cửa sổ ngữ cảnh huấn luyện ngắn chỉ được thực hiện một lần, và chúng tôi có thể hỗ trợ các cửa sổ ngữ cảnh đánh giá khác nhau trong quá trình suy luận. Thứ ba, trong E2-LLM, dựa trên các embedding vị trí RoPE, chúng tôi giới thiệu hai phương pháp tăng cường khác nhau trên các tham số tỷ lệ và chỉ số vị trí cho các mẫu khác nhau trong huấn luyện. Mục đích là làm cho mô hình mạnh mẽ hơn đối với các khác biệt tương đối khác nhau khi nội suy trực tiếp độ dài ngữ cảnh tùy ý trong quá trình suy luận. Kết quả thử nghiệm toàn diện trên nhiều tập dữ liệu chuẩn chứng minh hiệu quả của E2-LLM trên các tác vụ ngữ cảnh dài đầy thử thách.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) thường có độ dài cửa sổ ngữ cảnh được định trước. Ví dụ, các đầu vào cho các mô hình LLaMA (Touvron et al., 2023a,b) phải ít hơn 2.048 hoặc 4096 token. Giới hạn cửa sổ ngữ cảnh được đặt trước này thường xuyên bị vượt quá trong các ứng dụng như cuộc trò chuyện dài, tóm tắt tài liệu hoặc lý luận dài hạn (Zheng et al., 2023; Chen et al., 2023a). Đối với các ứng dụng này, các LLM với cửa sổ ngữ cảnh dài hơn được ưa thích. Tuy nhiên, việc huấn luyện một LLM từ đầu với các cửa sổ ngữ cảnh dài đòi hỏi chi phí huấn luyện đáng kể. Để giải quyết vấn đề này, nhiều phương pháp mở rộng ngữ cảnh dài (Peng et al., 2023; Chen et al., 2023b) đã được đề xuất để mở rộng cửa sổ ngữ cảnh của một LLM được huấn luyện trước hiện có.

Một cách tiếp cận đơn giản được gọi là ngoại suy trực tiếp, điều chỉnh tinh một LLM được huấn luyện trước hiện có với cửa sổ ngữ cảnh dài hơn. Tuy nhiên, các tác giả của Position Interpolation (PI) (Chen et al., 2023a) quan sát thấy rằng các mô hình được huấn luyện bằng ngoại suy trực tiếp thích nghi với các cửa sổ ngữ cảnh dài rất chậm và ngoại suy trực tiếp không hiệu quả trong việc mở rộng các cửa sổ ngữ cảnh dài hơn đáng kể.

Như được hiển thị trong Hình 1 (a), các phương pháp mở rộng ngữ cảnh dài hiện có (ví dụ: PI) thường cần các quy trình huấn luyện bổ sung để hỗ trợ các cửa sổ ngữ cảnh dài hơn tương ứng, trong đó dữ liệu huấn luyện ngữ cảnh dài cần được thu thập và cần huấn luyện với việc sử dụng bộ nhớ GPU cao cho mỗi cửa sổ ngữ cảnh.

Để giải quyết các vấn đề nêu trên, như được hiển thị trong Hình 1 (b), chúng tôi đề xuất một phương pháp mở rộng độ dài Hiệu quả và Cực đại của các LLM, gọi là E2-LLM, để hỗ trợ mở rộng độ dài cực đại của các LLM với chỉ một quy trình huấn luyện trên dữ liệu ngữ cảnh ngắn và chi phí tính toán hạn chế. Dựa trên E2-LLM, chúng tôi có thể hỗ trợ tốt các cửa sổ ngữ cảnh đánh giá khác nhau trong quá trình suy luận bằng cách chỉ thay đổi một siêu tham số của RoPE (Su et al., 2021) theo độ dài ngữ cảnh đầu vào. Cụ thể, đầu tiên, trong E2-LLM, dữ liệu huấn luyện chỉ yêu cầu dữ liệu thường dùng với độ dài ngắn (ví dụ: 4k), và chúng tôi chỉ cần huấn luyện LLM một lần và hỗ trợ nội suy độ dài ngữ cảnh tùy ý trong quá trình suy luận, điều này giảm đáng kể chi phí sử dụng GPU. Thứ hai, trong

Lưu ý rằng chúng tôi tuân theo (Chen et al., 2023b) để báo cáo bộ nhớ GPU bằng cách điều chỉnh tinh LLaMA2 7B trên các độ dài ngữ cảnh khác nhau với FlashAttention-2 (Dao, 2023) và DeepSpeed stage 2 (Rasley et al., 2020).

## LLM (4k) → 16k LLM (16k)
## LLM (4k) → 32k LLM (32k)  
## LLM (4k) → 64k LLM (64k)

Bộ nhớ GPU (GB): 57.4, 68.8, OOM

LLM (4k) → LLM (16k)
LLM (32k)
LLM (64k)

Điều chỉnh tinh
Điều chỉnh tinh (Huấn luyện Một lần & Hỗ trợ Tất cả)

(a). Các phương pháp hiện có (ví dụ: PI).
(b). E2-LLM (Của chúng tôi). 

4k/8k ≤ 46.3

Hình 1: So sánh các phương pháp mở rộng ngữ cảnh dài hiện có (ví dụ: PI (Chen et al., 2023a)) và E2-LLM của chúng tôi. "LLM (4k)" với màu nhạt có nghĩa là LLM với cửa sổ ngữ cảnh mặc định (ví dụ: LLaMa2 với 4k). "LLM (16k/32k/64k)" với màu đậm có nghĩa là LLM với các cửa sổ ngữ cảnh mở rộng (16k/32k/64k) sau điều chỉnh tinh. (a) Đối với các phương pháp hiện có, chúng ta cần thu thập dữ liệu ngữ cảnh dài (ví dụ: 16k/32k) và điều chỉnh tinh các mô hình LLM cho các cửa sổ mở rộng ngữ cảnh khác nhau với việc sử dụng bộ nhớ GPU cao. (b). Đối với E2-LLM, chúng tôi trực tiếp sử dụng dữ liệu ngữ cảnh ngắn (ví dụ: 4k/8k) bằng cách huấn luyện LLM chỉ một lần với việc sử dụng bộ nhớ GPU có thể chấp nhận được và hỗ trợ các cửa sổ ngữ cảnh đánh giá khác nhau (ví dụ: 16k/32k/64k) trong quá trình suy luận.

E2-LLM của chúng tôi, đầu tiên chúng tôi đề xuất tăng cường trên tham số tỷ lệ của PI từ một phân phối được định trước (ví dụ: phân phối đều), nhằm bao phủ các mật độ vị trí khác nhau trong huấn luyện. Bên cạnh đó, chúng tôi quan sát thấy rằng chỉ thay đổi tham số tỷ lệ sẽ làm cho các LLM tập trung vào một phạm vi nhỏ các chỉ số vị trí tuyệt đối. Do đó, trong E2-LLM của chúng tôi, để cải thiện khả năng tổng quát hóa của E2-LLM, chúng tôi tiếp tục đề xuất tăng cường trên các tham số chỉ số vị trí bằng cách giới thiệu các offset vị trí trên các chỉ số vị trí tuyệt đối của RoPE.

Các đóng góp của E2-LLM là như sau:
• Trong công trình của chúng tôi, chúng tôi đầu tiên điều tra các vấn đề (ví dụ: chi phí điều chỉnh tinh lớn với dữ liệu ngữ cảnh dài) của các phương pháp mở rộng ngữ cảnh dài hiện có, và đề xuất phương pháp mở rộng độ dài Hiệu quả và Cực đại (tức là E2-LLM) để huấn luyện các LLM một lần trên dữ liệu ngữ cảnh ngắn với chi phí bộ nhớ GPU hạn chế và hỗ trợ các cửa sổ ngữ cảnh đánh giá khác nhau.

• Trong E2-LLM, dựa trên RoPE, chúng tôi đề xuất hai chiến lược tăng cường trên các tham số tỷ lệ và chỉ số vị trí cho các mẫu khác nhau trong huấn luyện, nhằm hỗ trợ các cửa sổ ngữ cảnh đánh giá khác nhau trong một quy trình huấn luyện và cải thiện khả năng ngữ cảnh dài của các LLM.

• Kết quả thử nghiệm toàn diện trên nhiều tập dữ liệu chuẩn ngữ cảnh dài chứng minh hiệu quả và hiệu quả của phương pháp E2-LLM.

## 2 Các Công trình Liên quan

**Transformer ngữ cảnh dài.** Nhiều nghiên cứu mở rộng đã nhằm tăng khả năng của các transformer trong việc xử lý các chuỗi văn bản dài hơn. Các chiến lược như sử dụng các mô hình dựa trên truy xuất (Karpukhin et al., 2020; Izacard et al., 2022) đã được sử dụng, tích hợp các tài liệu bổ sung và kết quả tìm kiếm vào ngữ cảnh. Nhiều nỗ lực khác nhau đã điều chỉnh cơ chế multi-head attention bằng cách thiết kế các lựa chọn thay thế ước tính (Wang et al., 2020; Beltagy et al., 2020; Kitaev et al., 2020; Bulatov et al., 2022; Ding et al., 2023) để giảm thiểu các yêu cầu tính toán vốn dĩ cao của self-attention. Ví dụ, Longformer (Beltagy et al., 2020) và BigBird (Zaheer et al., 2020) sử dụng một hình thức attention pha loãng cho văn bản mở rộng hơn. Trong khi đó, các sáng kiến khác (Wu et al., 2022; Bulatov et al., 2022) đã giới thiệu các hệ thống dựa trên bộ nhớ để nén các đầu vào trước đó và thu hồi các thành phần liên quan. Tuy nhiên, các cách tiếp cận này có xu hướng kém hiệu quả so với attention đầy đủ, do đó cản trở việc tinh chỉnh các mô hình ngôn ngữ lớn được huấn luyện trước (LLM) (Wu et al., 2024; Guo et al., 2023; Wang et al., 2023; Bai et al., 2024; Chai et al., 2024). Cách tiếp cận của chúng tôi khác ở chỗ nó xấp xỉ cơ chế attention theo cách vẫn giữ sự phù hợp chặt chẽ với phương pháp attention thông thường, chỉ cho thấy sự khác biệt tối thiểu.

**LLM ngữ cảnh dài.** Các mô hình ngôn ngữ lớn (LLM) như LLaMA (Touvron et al., 2023a) và LLaMA2 (Touvron et al., 2023b) ban đầu được huấn luyện với kích thước ngữ cảnh cố định, thường là 2048 và 4096 token tương ứng. Tuy nhiên, chi phí huấn luyện LLM với các ngữ cảnh mở rộng từ đầu thường vượt quá khả năng của đội nghiên cứu trung bình. Do đó, các nghiên cứu gần đây đã khám phá những cách để mở rộng độ dài ngữ cảnh của các mô hình này trong giai đoạn điều chỉnh tinh. Ví dụ, Position Interpolation (Chen et al., 2023a) điều chỉnh kỹ thuật mã hóa vị trí xoay (Su et al., 2021), cho phép LLaMA xử lý các ngữ cảnh dài tới 32768 token. Một phương pháp khác, Landmark attention (Mohtashami và Jaggi, 2023), đạt được hiệu quả nhưng với chi phí của một số độ chính xác, bằng cách nén các ngữ cảnh mở rộng thành một tập hợp các token được truy xuất. Ngược lại, chiến lược của chúng tôi giảm thiểu các chi phí liên quan đến điều chỉnh tinh mà không làm tổn hại đến hiệu quả của attention ban đầu. Nó đảm bảo rằng mô hình có attention đầy đủ và không thay đổi trên toàn bộ đầu vào trong quá trình suy luận. Các cách tiếp cận khác, như ALiBi (Press et al., 2022), đã được thiết kế để huấn luyện Transformer trên các chuỗi ngắn hơn và sau đó áp dụng chúng cho các chuỗi dài hơn tại thời điểm suy luận, hiệu quả ngoại suy độ dài ngữ cảnh. Tuy nhiên, các kỹ thuật này không hiệu quả lắm đối với các LLM được huấn luyện trước dựa vào các mã hóa vị trí với khả năng ngoại suy kém, như RoPE (Su et al., 2021). Để vượt qua điều này, nghiên cứu gần đây đã được hướng tới việc sửa đổi các embedding vị trí của LLM để xử lý các văn bản dài hơn. Điều này bao gồm các phương pháp như Position Interpolation (Chen et al., 2023a), NTK-aware position embeddings (ntk, 2023), và Yarn (Peng et al., 2023).

## 3 Kiến thức Nền tảng

### 3.1 Rotary Position Embedding (RoPE)

Các mô hình Transformer yêu cầu thông tin vị trí rõ ràng được đưa vào, trong đó các mã hóa vị trí được sử dụng để biểu diễn thứ tự của các đầu vào. Trong phần này, chúng tôi lấy Rotary Position Embedding (RoPE) (Su et al., 2021) làm ví dụ, được sử dụng rộng rãi trong nhiều mô hình kiểu LLaMA (Touvron et al., 2023a). Trong RoPE, cho một chỉ số vị trí m ∈ [0, L) và một vector embedding x := [x₀, x₁, ..., x_{d-1}]⊤, trong đó L là cửa sổ ngữ cảnh và d là chiều của attention head, một hàm phức có giá trị vector f(x, m) được định nghĩa như sau:

f(x, m) = [(x₀ + ix₁)e^{imθ₀}, ..., (x_{d-2} + ix_{d-1})e^{imθ_{d/2-1}}]⊤, (1)

trong đó i := √-1 là đơn vị ảo và θⱼ = 10000^{-2j/d}. Dựa trên RoPE, điểm số self-attention a được tính như sau:

a(m, n) = Re⟨f(q, m), f(k, n)⟩ =: a(m - n), (2)

trong đó q và k là các vector query và key cho một attention head cụ thể, tương ứng, và quá trình dẫn xuất chi tiết được bỏ qua. Trong Phương trình 2, chúng ta quan sát thấy rằng a(m, n) chỉ phụ thuộc vào vị trí tương đối m - n thông qua các hàm lượng giác. Bên cạnh đó, RoPE được thực hiện trên cả query và key embedding để tính toán điểm số attention tại mỗi lớp.

### 3.2 Position Interpolation

Mặc dù điểm số attention trong Phương trình 2 chỉ phụ thuộc vào các vị trí tương đối, hiệu năng ngoại suy của nó không tốt lắm. Cụ thể, khi ngoại suy trực tiếp đến các cửa sổ ngữ cảnh lớn hơn chưa được thấy trong huấn luyện, perplexity sẽ tăng lên các con số rất cao (tức là >10³).

Gần đây, Position Interpolation (PI) (Chen et al., 2023a) đã được đề xuất, trong đó s được định nghĩa là khoảng vị trí giữa một query và một key, và L được định nghĩa là kích thước của cửa sổ ngữ cảnh được huấn luyện. Thay vì ngoại suy trực tiếp trên điểm số attention đến s > L, điểm số attention được định nghĩa là ã(s) = a(Ls/L'), trong đó L' là cửa sổ ngữ cảnh mở rộng dài hơn. Một cách chính thức, trong PI, RoPE f được thay thế bởi f' như sau:

f'(x, m) = f(x, mL/L'), (3)

trong đó các chỉ số vị trí từ [0, L') đến [0, L) được giảm để phù hợp với phạm vi chỉ số ban đầu trước khi tính toán RoPE. Nói cách khác, khoảng cách tương đối tối đa giữa hai token bất kỳ đã được giảm từ L' xuống L và PI giảm ảnh hưởng lên tính toán điểm số attention khi mở rộng cửa sổ ngữ cảnh, và làm cho các LLM dễ thích nghi hơn. Hơn nữa, chúng tôi định nghĩa tham số tỷ lệ g là L'/L. Ví dụ, g được đặt là 2 khi L' = 8192 cho LLaMa2 với cửa sổ ngữ cảnh L = 4096. Do đó, Phương trình 3 có thể được công thức hóa lại như sau:

f'(x, m) = f(x, m/g). (4)

Trong khi đó, đối với PI, chúng ta cần thu thập dữ liệu ngữ cảnh dài với độ dài tối đa L' trong điều chỉnh tinh, và điều chỉnh tinh cần thiết cho mỗi cửa sổ mở rộng với việc sử dụng bộ nhớ GPU cao như được hiển thị trong Hình 1 (a).

## 4 Phương pháp

Trong phần này, chúng tôi giới thiệu chi tiết về E2-LLM trong Hình 1 (b) để mở rộng các kích thước cửa sổ ngữ cảnh khác nhau bằng cách chỉ thực hiện một quy trình huấn luyện trên dữ liệu độ dài ngắn, điều này giảm đáng kể chi phí điều chỉnh. Đầu tiên, trong Mục 4.1, chúng tôi cung cấp các ký hiệu cần thiết. Sau đó, trong Mục 4.2.1, chúng tôi minh họa chi tiết về chiến lược E2-LLM để cải thiện hiệu năng mở rộng độ dài bằng cách giới thiệu tăng cường trên tỷ lệ và các tham số offset vị trí của RoPE, trong đó các tham số này được định nghĩa trong Phương trình 5. Cuối cùng, trong Mục 4.2.3, chúng tôi hiển thị các quy trình huấn luyện và suy luận trong E2-LLM.

### 4.1 Ký hiệu

Ngoài các ký hiệu được định nghĩa trong Mục 3, chúng tôi cũng định nghĩa các ký hiệu sau. Đầu tiên, độ dài được huấn luyện được định nghĩa là R. Cần được đề cập rằng R là độ dài tối đa của dữ liệu trong điều chỉnh tinh, được đặt là 8k trong E2-LLM, theo mặc định. Do đó, việc thu thập dữ liệu huấn luyện với độ dài R là dễ dàng và bộ nhớ GPU được sử dụng trong điều chỉnh tinh cũng có thể chấp nhận được. Ngược lại, độ dài được huấn luyện R bằng với độ dài mở rộng L' (ví dụ: 16k/32k) trong nhiều phương pháp mở rộng ngữ cảnh dài (ví dụ: PI), điều này đòi hỏi việc sử dụng bộ nhớ GPU cao trong huấn luyện. Thứ hai, chúng tôi cũng giới thiệu offset vị trí t trong RoPE, và chúng tôi có thể công thức hóa lại Phương trình 4 để tính toán các embedding RoPE như sau:

f'(x, m) = f(x, (m + t)/g). (5)

Trong RoPE tiêu chuẩn, theo mặc định, t được đặt là 0. Trong E2-LLM của chúng tôi, t được chọn từ một phạm vi chỉ số T = {0, ..., t_max}, trong đó t_max là offset vị trí tối đa. Thứ ba, chúng tôi cũng định nghĩa một tập hợp các tham số tỷ lệ được sử dụng trong E2-LLM là G = {1, 2, ..., g_max}, trong đó g_max là tham số tỷ lệ tối đa.

### 4.2 E2-LLM

Trong phần này, chúng tôi mô tả chi tiết chiến lược E2-LLM được đề xuất. Chúng tôi lấy mô hình LLM H với cửa sổ ngữ cảnh mặc định L là 4.096 và độ dài được huấn luyện R là 4.096 để minh họa. Chúng tôi đề xuất hai phương pháp tăng cường khác nhau trên các siêu tham số (tức là tham số tỷ lệ g và offset vị trí t) của RoPE.

#### 4.2.1 Tăng cường trên g

Như được hiển thị trong Hình 2, chúng tôi minh họa quy trình tăng cường trên tham số tỷ lệ g. Trong quy trình huấn luyện của E2-LLM được đề xuất, để làm cho mô hình H bao phủ các mật độ vị trí khác nhau trong huấn luyện, cho lần lặp thứ i, chúng tôi lấy mẫu tham số tỷ lệ g_i từ G cho các lần lặp khác nhau theo một phân phối xác suất được định trước P như sau:

g_i = S_g(P, G), g_i ∈ G, (6)

trong đó S_g(P, G) biểu thị phép toán lấy mẫu trên g, lấy mẫu g_i từ tập hợp G theo phân phối P. Do đó, các tham số tỷ lệ khác nhau được sử dụng cho các lần lặp khác nhau. Lưu ý rằng P dựa trên phân phối đều, theo mặc định.

Trong Hình 2, chúng tôi đặt offset vị trí t là 0, và sau đó chọn ngẫu nhiên tham số tỷ lệ g từ G cho mỗi mẫu dựa trên Phương trình 5, trong đó g được đặt là 2, 5 và 10 tương ứng. Bên cạnh đó, như được hiển thị trong Hình 2, chúng tôi quan sát thấy rằng các cửa sổ ngữ cảnh tối đa được nội suy khác nhau cho các mẫu khác nhau trong huấn luyện, và mật độ của các chỉ số vị trí được huấn luyện khác nhau. Ví dụ, các cửa sổ ngữ cảnh được nội suy là 8.192 và 20.480 khi g là 2 và 5 tương ứng. Hơn nữa, vì cửa sổ ngữ cảnh huấn luyện R nhỏ hơn các cửa sổ ngữ cảnh tối đa được nội suy, chỉ một tỷ lệ nhất định của các chỉ số vị trí được huấn luyện, được hiển thị bằng màu xanh lam trong Hình 2.

#### 4.2.2 Tăng cường trên t

Như được hiển thị trong Hình 2, chúng tôi quan sát thấy rằng chúng ta chỉ có thể tập trung vào một phạm vi nhỏ của các chỉ số vị trí khi chúng ta bắt đầu từ chỉ số không (tức là t = 0). Do đó, để cải thiện tính mạnh mẽ và khả năng tổng quát hóa của E2-LLM, chúng tôi tiếp tục giới thiệu quy trình tăng cường trên offset vị trí t bằng cách thay đổi các chỉ số vị trí tuyệt đối của RoPE. Bên cạnh đó, được truyền cảm hứng từ một số công trình gần đây (Han et al., 2023; Xiao et al., 2023), tuyên bố rằng một lượng lớn điểm số attention được phân bổ cho các token ban đầu (tức là attention sink (Xiao et al., 2023)), chúng tôi cũng giữ một số token ban đầu và đặt offset vị trí của các token này là 0. Đối với các chỉ số vị trí khác, trong lần lặp huấn luyện thứ i, chúng tôi đặt offset vị trí t cho các chỉ số vị trí khác nhau của cửa sổ được huấn luyện như sau:

t_i = {
0,        m ∈ [0,3]
S_t(Q, T), m ∈ (3, R)
}, (7)

trong đó S_t(Q, T) biểu thị phép toán lấy mẫu trên t, và lấy mẫu t_i từ tập hợp T theo phân phối xác suất được định trước Q. Lưu ý rằng Q được đặt là phân phối đều và t_max được đặt là sự khác biệt giữa cửa sổ ngữ cảnh được nội suy tối đa và cửa sổ ngữ cảnh được huấn luyện trong lần lặp hiện tại. Dựa trên Phương trình 7, cho n ∈ [0,3] và m ∈ (3, R), Phương trình 2 có thể được viết như sau:

a(m, n) = Re⟨f(q, m + t_i), f(k, n + t_i)⟩ =: a(m + S_t(Q, T) - n). (8)

Do đó, khi S_t(Q, T) lớn hơn, phạm vi của các khác biệt vị trí tương đối (tức là (m + S_t(Q, T) - n)) giữa m và n lớn hơn, điều này sẽ làm cho mô hình tổng quát hóa đến các phạm vi khác nhau của các khác biệt vị trí tương đối.

Trong Hình 3, chúng tôi cũng cung cấp các chỉ số vị trí được huấn luyện (tức là các điểm màu xanh lam) khi giới thiệu tăng cường trên offset vị trí t, và quan sát thấy rằng E2-LLM có thể dễ dàng sử dụng các chỉ số vị trí với các giá trị tuyệt đối khác nhau và các phạm vi khác nhau của các khác biệt tương đối.

#### 4.2.3 Huấn luyện và Suy luận

Như đã thảo luận trong Mục 1, quy trình huấn luyện được thực hiện một lần cho E2-LLM của chúng tôi, và chúng tôi có thể mở rộng đến các cửa sổ ngữ cảnh đánh giá khác nhau dễ dàng trong quá trình suy luận. Chi tiết như sau.

**Huấn luyện.** Trong huấn luyện, đầu tiên, cho lần lặp thứ i, dựa trên g_i và offset vị trí t_i trong huấn luyện, chúng tôi thay thế g và t bằng g_i và t_i cho Phương trình 5 tương ứng. Sau đó, chúng tôi điều chỉnh tinh LLM H với cửa sổ ngữ cảnh ngắn R sử dụng tác vụ dự đoán token tiếp theo với các mã hóa vị trí được sửa đổi trên cửa sổ ngữ cảnh được huấn luyện. Để làm rõ hơn, chúng tôi cũng cung cấp một thuật toán của phương pháp E2-LLM được đề xuất trong Thuật toán 1.

**Suy luận.** E2-LLM của chúng tôi cũng không giới thiệu các trọng số huấn luyện bổ sung hoặc sửa đổi kiến trúc mạng theo bất kỳ cách nào, có nghĩa là nó hấp dẫn trong các ứng dụng thực tế vì hầu hết cơ sở hạ tầng và tối ưu hóa cho mô hình ban đầu có thể được tái sử dụng sau khi mở rộng độ dài. Trong quá trình suy luận, chúng ta có thể mở rộng đến các cửa sổ ngữ cảnh khác nhau bằng cách đặt các tham số tỷ lệ khác nhau cho nội suy dễ dàng. Ví dụ, chúng tôi đặt g = 8 để nội suy đến 32.768 và g = 16 để nội suy đến 65.536, được gọi là E2-LLM-32k và E2-LLM-64k tương ứng. Cần được đề cập rằng các trọng số của E2-LLM-32k và E2-LLM-64k giống nhau trong quá trình suy luận, và sự khác biệt duy nhất là các tham số tỷ lệ được đặt là 8 và 16 tương ứng. Hơn nữa, trong thực tế, chúng ta chỉ có thể triển khai một LLM trên các thiết bị và tự động thay đổi tham số tỷ lệ của RoPE dựa trên độ dài của ngữ cảnh đầu vào để hỗ trợ các cửa sổ ngữ cảnh khác nhau.

Hình 2: Các chỉ số vị trí được huấn luyện (điểm màu xanh lam) khi sử dụng các tham số tỷ lệ khác nhau (tức là g = 2, 5, 10). Độ dài tối đa của dữ liệu điều chỉnh tinh (tức là R) là 4096 và offset vị trí t được đặt là 0 để minh họa.

Hình 3: Các chỉ số vị trí được huấn luyện (điểm màu xanh lam) khi sử dụng các offset vị trí khác nhau và g được đặt là 5 để trực quan hóa. Các chỉ số vị trí của bốn token đầu tiên không được di chuyển.

Bảng 1: Kết quả (%) trên các tác vụ QA đơn tài liệu, QA đa tài liệu và tóm tắt từ tập dữ liệu LongBench.

| Mô hình | QA Đơn tài liệu | QA Đa tài liệu | Tóm tắt |
|---------|-----------------|----------------|---------|
| | Narrative QA | Qasper | MultiField QA-en | MultiField QA-zh | Hotpot QA | 2WikiMulti hopQA | MuSi Que | Du Reader | Gov Report | QMSum |
| GPT-3.5-Turbo-16k | 23.6 | 43.3 | 52.3 | 61.2 | 51.6 | 37.7 | 26.9 | 28.7 | 29.5 | 23.4 |
| Llama2-7B-chat-4k | 18.7 | 19.2 | 36.8 | 11.9 | 25.4 | 32.8 | 9.4 | 5.2 | 27.3 | 20.8 |
| LongChat-v1.5-7B-32k | 16.9 | 27.7 | 41.4 | 29.1 | 31.5 | 20.6 | 9.7 | 19.5 | 30.8 | 22.7 |
| Vicuna-v1.5-7B-16k | 19.4 | 26.1 | 38.5 | 43.0 | 25.3 | 20.8 | 9.8 | 19.3 | 27.9 | 22.8 |
| LongLora-7B-16k | 19.8 | 29.1 | 37.2 | 8.5 | 37.0 | 30.3 | 17.1 | 15.3 | 31.5 | 24.1 |
| E2-LLM-Llama2-7B-16k | 16.4 | 34.7 | 39.1 | 43.6 | 37.1 | 34.4 | 17.9 | 18.6 | 29.4 | 23.0 |
| E2-LLM-Llama2-7B-32k | 12.3 | 35.6 | 40.4 | 46.6 | 43.7 | 34.8 | 22.0 | 22.6 | 29.7 | 23.8 |
| Llama2-13B-chat-4k | 19.2 | 25.8 | 36.9 | 33.3 | 36.1 | 32.4 | 14.5 | 26.8 | 26.6 | 20.2 |
| Vicuna-v1.5-13B-16k | 18.9 | 29.9 | 46.2 | 28.4 | 38.1 | 36.0 | 10.7 | 20.9 | 27.9 | 22.1 |
| PI-Llama2-13B-16k | 19.2 | 33.3 | 42.7 | 47.9 | 44.9 | 34.8 | 19.5 | 17.4 | 27.9 | 23.7 |
| E2-LLM-Llama2-13B-16k | 25.4 | 35.3 | 46.5 | 49.1 | 46.4 | 38.3 | 25.2 | 19.3 | 29.9 | 22.7 |
| E2-LLM-Llama2-13B-32k | 24.1 | 36.2 | 49.0 | 52.5 | 49.2 | 37.6 | 23.1 | 20.4 | 29.9 | 23.1 |

Bảng 2: Kết quả (%) trên tóm tắt, học few-shot, tổng hợp và tác vụ mã từ tập dữ liệu LongBench. 'Tổng thể' được tính bằng macro-average trên các danh mục tác vụ chính. Điều này được tính trên các tác vụ tiếng Anh (EN), tiếng Trung (ZH) và tất cả (All), các tác vụ mã được bao gồm trong cả hai ngôn ngữ.

| Mô hình | Tóm tắt | Học Few-shot | Mã | Tổng thể |
|---------|---------|--------------|-----|----------|
| | MultiNews | VCSUM | TREC | TriviaQA | SAMSum | LSHT | LCC | RepoBench-P | EN | ZH | All |
| GPT-3.5-Turbo-16k | 26.7 | 16.0 | 68.0 | 91.4 | 41.7 | 29.2 | 54.7 | 53.6 | 44.60 | 33.78 | 42.19 |
| Llama2-7B-chat-4k | 25.8 | 0.2 | 61.5 | 77.8 | 40.7 | 19.8 | 52.4 | 43.8 | 35.17 | 15.45 | 20.79 |
| LongChat-v1.5-7B-32k | 26.4 | 9.9 | 63.5 | 82.3 | 34.2 | 23.2 | 53.0 | 55.3 | 36.86 | 20.43 | 33.21 |
| Vicuna-v1.5-7B-16k | 27.2 | 15.1 | 74.0 | 86.2 | 40.8 | 28.8 | 51.0 | 43.5 | 36.49 | 26.55 | 34.28 |
| LongLora-7B-16k | 27.7 | 0.5 | 63.5 | 85.7 | 41.9 | 26.0 | 57.6 | 54.5 | 39.79 | 14.55 | 34.18 |
| E2-LLM-Llama2-7B-16k | 25.9 | 9.6 | 68.5 | 89.2 | 38.2 | 35.0 | 65.8 | 58.1 | 41.26 | 26.70 | 38.03 |
| E2-LLM-Llama2-7B-32k | 25.4 | 11.7 | 70.5 | 88.4 | 32.5 | 40.0 | 64.5 | 60.9 | 41.74 | 30.23 | 39.18 |
| Llama2-13B-chat-4k | 26.1 | 17.2 | 66.0 | 85.2 | 36.5 | 20.3 | 51.9 | 52.8 | 37.87 | 24.38 | 34.87 |
| Vicuna-v1.5-13B-16k | 27.1 | 16.4 | 74.0 | 84.9 | 27.8 | 29.8 | 44.1 | 45.6 | 38.08 | 23.86 | 34.92 |
| PI-Llama2-13B-16k | 25.9 | 9.1 | 72.5 | 86.5 | 27.9 | 31.0 | 62.5 | 51.1 | 40.88 | 26.35 | 37.65 |
| E2-LLM-Llama2-13B-16k | 27.0 | 9.8 | 73.5 | 87.9 | 40.6 | 36.0 | 65.4 | 59.1 | 44.73 | 28.56 | 41.13 |
| E2-LLM-Llama2-13B-32k | 26.8 | 10.2 | 75.0 | 87.8 | 40.9 | 44.5 | 63.8 | 57.5 | 44.55 | 31.93 | 41.74 |

## 5 Thử nghiệm

### 5.1 Cài đặt Thử nghiệm

**Mô hình.** Trong E2-LLM của chúng tôi, chúng tôi sử dụng các mô hình Llama2 (Touvron et al., 2023b) được huấn luyện trước 7B, 13B để chứng minh hiệu quả của E2-LLM.

**Quy trình Huấn luyện.** Tất cả các mô hình được điều chỉnh tinh thông qua mục tiêu dự đoán token tiếp theo dựa trên hai máy 8 × A100 GPU. Chúng tôi sử dụng AdamW (Loshchilov và Hutter, 2019) với β₁ = 0.9 và β₂ = 0.95. Tốc độ học được đặt là 1 × 10⁻⁵ cho các mô hình 7B và 13B, và toàn bộ bước huấn luyện được đặt là 30.000 với kích thước batch toàn cục là 16.

**Tập dữ liệu.** Tập dữ liệu huấn luyện bao gồm tập dữ liệu tiền huấn luyện (tức là Pile (Gao et al., 2020)) và các tập dữ liệu điều chỉnh tinh (tức là ShareGPT (Zheng et al., 2023) và các tập dữ liệu tóm tắt dài (Cohan et al., 2018)). Lưu ý rằng các tập dữ liệu điều chỉnh tinh được sử dụng để cải thiện khả năng hỏi đáp của các LLM ngữ cảnh dài theo các mô hình Vicuna và LongChat (Zheng et al., 2023) và tạo ra kết quả hợp lý trên LongBench. Chúng tôi đánh giá hiệu năng mô hình hóa ngôn ngữ chuỗi dài của các mô hình được điều chỉnh tinh trên LongBench (Bai et al., 2023) và tập dữ liệu arxiv proof-pile (Azerbayev et al., 2022).

### 5.2 Kết quả trên LongBench

Chúng tôi đánh giá một số LLM phổ biến với khả năng ngữ cảnh dài, bao gồm GPT-3.5-Turbo-16k (OpenAI, 2022), Llama2-7B-chat-4k (Touvron et al., 2023b), LongChat-v1.5-7B-32k (Li et al., 2023), Vicuna-v1.5-7B-16k (Zheng et al., 2023), Longlora-7B-16k (Chen et al., 2023b), Llama2-13B-chat-4k (Touvron et al., 2023b), Vicuna-v1.5-13B-16k (Zheng et al., 2023), PI-Llama2-13B-16k. LongChat-v1.5-7B-32k, Vicuna-v1.5-7B-16k và LongLora-7B-16k được điều chỉnh tinh từ Llama2-7B dựa trên PI. Vicuna-v1.5-13B-16k (Zheng et al., 2023), PI-Llama2-13B-16k được điều chỉnh tinh với Llama2-13B dựa trên PI, trong đó PI-Llama2-13B-16k được điều chỉnh tinh với các tập dữ liệu được xây dựng của chúng tôi.

Theo LongBench (Bai et al., 2023), chúng tôi tiến hành đánh giá trong thiết lập zero-shot, ngoại trừ các tác vụ học few-shot trong đó các ví dụ few-shot được cung cấp như một phần của ngữ cảnh dài. Khi độ dài đầu vào I vượt quá độ dài ngữ cảnh tối đa L' của một mô hình (được chỉ định bởi hậu tố của tên), chúng tôi cắt bỏ chuỗi đầu vào S từ giữa vì phần đầu và cuối của chuỗi có thể chứa thông tin quan trọng như hướng dẫn hoặc câu hỏi: S₁:I → [S₁:⌊L'/2⌋; SI-⌊L'/2⌋-1:I]. Số liệu cho mỗi tập dữ liệu được hiển thị trong Bảng 6 từ Phụ lục A.1.

Như được hiển thị trong Bảng 1 và Bảng 2, chúng tôi báo cáo kết quả hiệu năng (%) trên tập dữ liệu LongBench. Cụ thể, các phát hiện chính từ kết quả thử nghiệm như sau: (1) Khi so sánh với mô hình thương mại (GPT-3.5-Turbo-16k) với độ chính xác tổng thể 44.60% bằng tiếng Anh, E2-LLM-Llama2-13B-32k của chúng tôi đạt được kết quả gần với độ chính xác tổng thể 44.55%. (2) Trong Bảng 1 và Bảng 2, chúng tôi cũng đánh giá kết quả của E2-LLM với các kích thước cửa sổ ngữ cảnh đánh giá khác nhau (tức là 16k và 32k), và chúng tôi quan sát thấy rằng kết quả hiệu năng tốt hơn khi chúng tôi mở rộng kích thước cửa sổ ngữ cảnh đánh giá. Bên cạnh đó, vì độ dài của hầu hết các tài liệu trong LongBench ít hơn 16k, những cải thiện trên các tác vụ này không đáng kể khi chúng tôi tăng cửa sổ ngữ cảnh đánh giá. (3) Để so sánh công bằng, chúng tôi cũng tái triển khai phương pháp nội suy vị trí dựa trên Llama2-13B (tức là PI-Llama2-13B-16k) với cùng chiến lược huấn luyện và tập dữ liệu huấn luyện. Khi so sánh với PI-Llama2-13B-16k, E2-LLM-Llama2-13B-16k của chúng tôi vẫn đạt được những cải thiện đáng kể trên LongBench, điều này tiếp tục chứng minh hiệu quả của E2-LLM.

### 5.3 Kết quả trên Proof-Pile

Trên tập dữ liệu Arxiv Math proof-pile được làm sạch (Azerbayev et al., 2022), chúng tôi đánh giá hiệu năng mô hình hóa ngôn ngữ chuỗi dài của các mô hình mở rộng và phương pháp cơ sở (tức là Vicuna-v1.5-16k và LongChat-v1.5-32k), trong đó kết quả perplexity được báo cáo. Đối với tập dữ liệu proof-pile, chúng tôi lấy mẫu ngẫu nhiên 128 tài liệu với ít nhất 64k token và đánh giá perplexity được tính toán của mỗi mẫu này. Tất cả các đánh giá perplexity được tính toán bằng phương pháp cửa sổ trượt từ (Press et al., 2022) với S = 256. Cụ thể, Vicuna-v1.5-16k và LongChat-v1.5-32k được điều chỉnh tinh trên mô hình Llama2 và phương pháp mở rộng RoPE tuyến tính, dựa trên Position Interpolation (tức là PI) (Chen et al., 2023a). Trong Bảng 3, chúng tôi thấy rằng các mô hình được mở rộng với phương pháp của chúng tôi hưởng lợi từ perplexity được cải thiện đáng kể từ các kích thước cửa sổ ngữ cảnh dài hơn khi so sánh với các phương pháp cơ sở khác. Bên cạnh đó, đối với các phương pháp khác, cửa sổ ngữ cảnh huấn luyện bằng với cửa sổ ngữ cảnh đánh giá tối đa, do đó chi phí huấn luyện rất lớn khi kích thước cửa sổ lớn, trong đó chi phí huấn luyện được hiển thị trong Hình 1. Ngược lại, E2-LLM của chúng tôi chỉ cần huấn luyện các mô hình Llama một lần và cửa sổ ngữ cảnh huấn luyện ngắn, điều này giảm đáng kể chi phí huấn luyện.

Thuật toán 1 Huấn luyện của E2-LLM
Đầu vào: Mô hình LLM được huấn luyện trước H với cửa sổ ngữ cảnh mặc định L (ví dụ: 4k); Cửa sổ ngữ cảnh được huấn luyện là R (ví dụ: 4k/8k); Cửa sổ ngữ cảnh đánh giá L' (ví dụ: 32k/64k);
1: cho lần lặp thứ i trong huấn luyện làm
2: Đặt tỷ lệ g_i dựa trên Phương trình 6;
3: Đặt offset vị trí t_i dựa trên Phương trình 7;
4: Sửa đổi các embedding vị trí RoPE dựa trên Phương trình 5;
5: Huấn luyện mô hình H trên cửa sổ huấn luyện R;
6: Tính toán loss dự đoán token tiếp theo;
7: Cập nhật các tham số của mô hình H;
8: kết thúc cho
Đầu ra: Mô hình ngữ cảnh dài được tối ưu hóa H'. (Lưu ý rằng H' có thể mở rộng đến các cửa sổ ngữ cảnh khác nhau trong quá trình suy luận.);

### 5.4 Nghiên cứu Loại bỏ

**Ảnh hưởng của các chiến lược tăng cường.** Trong Bảng 4, chúng tôi cung cấp hai biến thể thay thế của E2-LLM (tức là E2-LLM (w/o aug on t), E2-LLM (w/o aug on g)) để huấn luyện mô hình cơ sở LLama2-13B. Cụ thể, đối với E2-LLM (w/o aug on t), chúng tôi chỉ sử dụng tăng cường trên tham số tỷ lệ g mà không sử dụng tăng cường trên offset vị trí t, và chúng tôi đánh giá hiệu năng bằng cách mở rộng cửa sổ ngữ cảnh thành 32k. Đối với E2-LLM (w/o aug on g), chúng tôi chỉ sử dụng tăng cường trên offset vị trí t và cố định tham số tỷ lệ g là 2 với cửa sổ ngữ cảnh huấn luyện 8k trong E2-LLM. Lưu ý rằng cửa sổ ngữ cảnh đánh giá của E2-LLM (w/o aug on g) cũng được đặt là 8k. Như được hiển thị trong Bảng 4, E2-LLM của chúng tôi tốt hơn hai biến thể thay thế này trên LongBench, điều này cho thấy rằng việc thực hiện tăng cường trên t và g là có lợi.

Bảng 3: Đánh giá perplexity trên tập dữ liệu Arxiv Proof-pile (Azerbayev et al., 2022) dựa trên các mô hình Llama2 7B và 13B, trong đó perplexity thấp hơn có nghĩa là hiệu năng tốt hơn. "PI" biểu thị Position Interpolation (Chen et al., 2023a). Vicuna-v1.5-16k và LongChat-v.15-32k mã nguồn mở được mở rộng dựa trên phương pháp PI. Lưu ý rằng các trọng số của E2-LLM-16k, E2-LLM-32k và E2-LLM-64k giống nhau trong quá trình suy luận, và sự khác biệt duy nhất là các tham số tỷ lệ được đặt là 4, 8 và 16 tương ứng.

| Mô hình | | | Kích thước Cửa sổ Ngữ cảnh Đánh giá |
|---------|--|--|-----------------------------------|
| Kích thước | Cửa sổ Ngữ cảnh Huấn luyện | Phương pháp | 4096 | 8192 | 16384 | 32768 | 65536 |
| 7B | 4k | Không có | 2.92 | - | - | - | - |
| 7B | 16k | Vicuna-v1.5-16k (PI) | 3.48 | 3.17 | 3.95 | - | - |
| | 32k | LongChat-v1.5-32k (PI) | 3.54 | 3.18 | 2.91 | 2.73 | - |
| | 4k | E2-LLM-16k | 2.96 | 2.71 | 2.54 | - | - |
| 7B | | E2-LLM-32k | 2.99 | 2.74 | 2.56 | 2.46 | - |
| | | E2-LLM-64k | 3.06 | 2.81 | 2.62 | 2.51 | 2.56 |
| 13B | 4k | Không có | 2.78 | - | - | - | - |
| 13B | 16k | Vicuna-v1.5-16k (PI) | 3.27 | 2.97 | 2.78 | - | - |
| | | E2-LLM-16k | 2.82 | 2.59 | 2.43 | - | - |
| 13B | 4k | E2-LLM-32k | 2.85 | 2.61 | 2.44 | 2.34 | - |
| | | E2-LLM-64k | 2.91 | 2.67 | 2.49 | 2.39 | 2.44 |

Bảng 4: Loại bỏ trên các chiến lược tăng cường khác nhau.

| Phương pháp | EN | ZH | All |
|-------------|----|----|-----|
| E2-LLM | 44.55 | 31.93 | 41.74 |
| E2-LLM (w/o aug on t) | 42.28 | 29.49 | 39.44 |
| E2-LLM (w/o aug on g) | 41.66 | 28.33 | 38.77 |

**Ảnh hưởng của số bước điều chỉnh tinh.** Như được hiển thị trong Hình 4, chúng tôi báo cáo mối quan hệ giữa kết quả trên tập dữ liệu LongBench và số bước điều chỉnh tinh cho mô hình LLaMA2 13B sử dụng E2-LLM, trong đó kết quả được báo cáo trên kích thước cửa sổ đánh giá 32k. Trong Hình 4, ở 5k lần lặp đầu tiên, kết quả cải thiện nhanh chóng, điều này cho thấy rằng các mô hình có thể đạt được khả năng hiểu ngữ cảnh dài mà không cần một quy trình huấn luyện dài. Hơn nữa, khi tăng các lần lặp huấn luyện, chúng tôi quan sát thấy rằng kết quả hiệu năng của LongBench vẫn có thể tăng đều đặn sau 5k lần lặp.

**Ảnh hưởng của tham số tỷ lệ tối đa G_max.** Trong Bảng 5, trên tập dữ liệu LongBench, chúng tôi cũng báo cáo kết quả của E2-LLM dựa trên mô hình Llama2-13B để phân tích ảnh hưởng của tham số tỷ lệ tối đa G_max trong huấn luyện, và cửa sổ ngữ cảnh đánh giá được đặt là 32k. Khi G_max tăng từ 5 đến 20, kết quả hiệu năng trên Longbench tốt hơn, điều này cho thấy rằng việc bao phủ các mật độ khác nhau bằng cách sử dụng G_max lớn trong E2-LLM là hiệu quả. Tuy nhiên, khi chúng tôi tiếp tục tăng tham số tỷ lệ tối đa G_max, cải thiện hiệu năng trở nên tương đối ổn định trên LongBench. Do đó, chúng tôi trực tiếp đặt G_max là 20 để hỗ trợ cửa sổ mở rộng tối đa 80k.

0.0 0.5 1.0 1.5 2.0 2.5 3.0
Lần lặp 1e4
20 25 30 35 40
Kết quả trên LongBench
EN
ZH
All

Hình 4: Kết quả hiệu năng trên tập dữ liệu Longbench khi tăng các bước huấn luyện.

Bảng 5: Loại bỏ trên tham số tỷ lệ tối đa G_max.

| G_max | 5 | 10 | 20 | 30 |
|-------|---|----|----|-----|
| EN | 43.20 | 44.25 | 44.55 | 44.01 |
| ZH | 29.33 | 30.28 | 39.93 | 32.76 |
| All | 40.12 | 41.15 | 41.74 | 41.51 |

### 5.5 Phân tích Thêm

**Mở rộng đến các tỷ lệ chưa được thấy.** Theo mặc định, chúng tôi đặt G_max là 20 để hỗ trợ cửa sổ ngữ cảnh được nội suy tối đa 80K. Trong Hình 5, các tỷ lệ nội suy được điều chỉnh thử nghiệm thành 20, 30, 40 và 50 trong quá trình suy luận để đánh giá khả năng tổng quát hóa của E2-LLM. Kết quả chứng minh rằng PPL duy trì ở mức thỏa đáng cho các ngữ cảnh bao gồm ít hơn 120K token. Tuy nhiên, khi chúng tôi tiếp tục tăng tỷ lệ, sự suy giảm hiệu năng rõ ràng xảy ra. Điều này gợi ý rằng E2-LLM sở hữu khả năng tổng quát hóa mạnh mẽ cho các tỷ lệ chưa được thấy hoặc OOD trong một phạm vi nhất định.

0 50000 100000 150000 200000
Độ dài Ngữ cảnh
2.2 2.7 3.2 3.7 4.2 4.7 5.2 5.7
PPL
E2-LLM-80K(g=20)
E2-LLM-120K(g=30)
E2-LLM-160K(g=40)
E2-LLM-200K(g=50)

Hình 5: Khả năng tổng quát hóa trên các tỷ lệ chưa được thấy.

**Trực quan hóa trên Attention Map.** Để phân tích thêm ảnh hưởng của E2-LLM, chúng tôi trực quan hóa các heatmap attention trong lớp cho E2-LLM-8k, E2-LLM-16k, E2-LLM-32k và E2-LLM-64k dựa trên Llama2-13B trong Hình 6 với các cửa sổ ngữ cảnh đánh giá 8k, 16k, 32k và 64k bằng cách đặt tham số tỷ lệ là 2, 4, 8, 16 tương ứng. Cụ thể, như được hiển thị trong Hình 6, tọa độ dọc biểu thị các chỉ số của token chuỗi được tạo, và tọa độ ngang biểu thị các chỉ số của token chuỗi đầu vào. Văn bản đầu vào là một phần của một bài báo dài, được cắt bớt thành 16k, 32k và 64k tương ứng. Sau đó, ba cặp key-value ngẫu nhiên được chèn vào văn bản đầu vào, và một câu hỏi được thêm vào cuối văn bản. Lưu ý các cặp key-value ngẫu nhiên và câu hỏi như được hiển thị trong Phụ lục ??, Câu hỏi đã được trả lời chính xác cho 8k, 16k, 32k và 64k. Trong Hình 6, chúng tôi trực quan hóa các heatmap attention của chuỗi đầu ra tương ứng với đầu vào. Các chỉ số đúng của các giá trị tương ứng với các khóa được hỏi trong 8k, 16k, 32k và 64k là [4470, 4503], [9572, 9605], [15891, 15924] và [37958, 37991] tương ứng, và chúng tôi quan sát thấy rằng các giá trị attention của chuỗi đầu ra tại các vị trí này rất đáng kể, điều này thể hiện rằng E2-LLM có thể lập chỉ mục tốt vị trí chính xác khi tạo ra các phản hồi.

Attention HeatMap của E2-LLM-8K
Attention HeatMap của E2-LLM-16K
Attention HeatMap của E2-LLM-32K
Attention HeatMap của E2-LLM-64K

Hình 6: Trực quan hóa các heatmap attention trên các ngữ cảnh đầu vào 8k, 16k, 32k và 64k.

## 6 Kết luận

Trong nghiên cứu này, chúng tôi giới thiệu một phương pháp mở rộng độ dài Hiệu quả và Cực đại cho các LLM, được đặt tên là E2-LLM, được thiết kế để mở rộng các cửa sổ ngữ cảnh với một giai đoạn huấn luyện duy nhất và chi phí tính toán tối thiểu. Đáng chú ý, trong E2-LLM của chúng tôi, không có yêu cầu tích lũy các tập dữ liệu ngữ cảnh dài mở rộng (ví dụ: các mẫu có 32k hoặc 64k token) để huấn luyện. Cụ thể, E2-LLM của chúng tôi khai thác các embedding vị trí dựa trên RoPE để triển khai một cặp chiến lược tăng cường mới điều chỉnh các tham số tỷ lệ và chỉ số vị trí trên các mẫu huấn luyện khác nhau với độ dài ngắn (ví dụ: 4k/8k). Kết quả thử nghiệm toàn diện trên nhiều tập dữ liệu chuẩn chứng minh hiệu quả của E2-LLM trên các tác vụ ngữ cảnh dài.

## 7 Công trình Tương lai

Đối với các hướng nghiên cứu tương lai, chúng tôi có ba kế hoạch như sau: (1) vì E2-LLM của chúng tôi hiệu quả và có hiệu quả và chúng tôi sẽ cố gắng áp dụng E2-LLM trên các mô hình lớn hơn (ví dụ: LLama2 70B) và các cửa sổ ngữ cảnh lớn hơn (ví dụ: 128k/192k); (2) chúng tôi tin rằng E2-LLM là một phương pháp tổng quát và chúng tôi sẽ cố gắng áp dụng E2-LLM trên nhiều loại mã hóa vị trí và nhiều loại LLM hơn; (3) chúng tôi sẽ phát hành các mô hình và mã của chúng tôi.

## Tài liệu Tham khảo

2019. Winogrande: An adversarial winograd schema challenge at scale.

2023. Ntk-aware scaled rope.

Zhangir Azerbayev, Edward Ayers, và Bartosz Piotrowski. 2022. Proof-pile.

Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, và Wanli Ouyang. 2024. Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, và Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence.

Aydar Bulatov, Yuri Kuratov, và Mikhail S. Burtsev. 2022. Recurrent memory transformer. In NeurIPS.

Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, et al. 2024. xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning. arXiv preprint arXiv:2401.07037.

Shouyuan Chen, Sherman Wong, Liangjian Chen, và Yuandong Tian. 2023a. Extending context window of large language models via positional interpolation. CoRR, abs/2306.15595.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, và Jiaya Jia. 2023b. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, và Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.

Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, abs/2307.08691.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, và Furu Wei. 2023. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR, abs/2307.02486.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.

Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, et al. 2023. Owl: A large language model for it operations. arXiv preprint arXiv:2309.09298.

Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, và Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large language models. CoRR, abs/2308.16137.

Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP, pages 6769–6781.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The efficient transformer. In ICLR.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, và Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, và Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683.

Hector Levesque, Ernest Davis, và Leora Morgenstern. 2012. The winograd schema challenge. Thirteenth international conference on the principles of knowledge representation and reasoning.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, và Hao Zhang. 2023. How long can open-source llms truly promise on context length?

Ilya Loshchilov và Frank Hutter. 2019. Decoupled weight decay regularization. In ICLR.

Amirkeivan Mohtashami và Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. CoRR, abs/2305.16300.

OpenAI. 2022. Introducing chatgpt.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, và Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071.

Ofir Press, Noah A. Smith, và Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In ICLR.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD, pages 3505–3506. ACM.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, và Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. CoRR, abs/1904.09728.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, và Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.

Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. 2020. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768.

Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, và Junran Peng. 2023. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv: 2310.00746.

Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, và Bo Zheng. 2024. Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models. arXiv.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. 2022. Memorizing transformers. In ICLR.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, và Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In NeurIPS.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.

## Ngữ cảnh Đầu vào

Các cặp key-value: Trích xuất giá trị tương ứng với khóa được chỉ định trong đối tượng JSON bên dưới. {"2a8d601d-1d69-4e64-9f90-8ad825a74195": "bb3ba2a5-7de8-434b-a86e-a88bb9fa7289", "9f4a92b9-5f69-4725-ba1e-403f08dea695": "703a7ce5-f17f-4e6d-b895-5836ba5ec71c", "52a9c80c-da51-4fc9-bf70-4a4901bc2ac3": "b2f8ea3d-4b1b-49e0-a141-b9823991ebeb"}

Câu hỏi: Giá trị của khóa "9f4a92b9-5f69-4725-ba1e-403f08dea695" là gì?

## A Chi tiết Thêm

### A.1 Chi tiết thêm về tập dữ liệu LongBench

Chi tiết được hiển thị trong Bảng 6.

### A.2 Chi tiết thêm về trực quan hóa attention map

Ba cặp key-value ngẫu nhiên và câu hỏi đầu vào được hiển thị như sau.

Bảng 6: Tổng quan về tập dữ liệu LongBench. "Nguồn" biểu thị nguồn gốc của ngữ cảnh. "Độ dài trung bình" (độ dài trung bình) biểu thị độ dài trung bình, được tính bằng cách đếm từ trong các tập dữ liệu tiếng Anh (mã) và ký tự trong các tập dữ liệu tiếng Trung. "Accuracy (CLS)" và "Accuracy (EM)" là độ chính xác phân loại và độ chính xác khớp chính xác, tương ứng.

| ID Tập dữ liệu | Nguồn | Độ dài tb | Số liệu | Ngôn ngữ | #dữ liệu |
|----------------|-------|-----------|---------|----------|----------|
| **QA Đơn tài liệu** |
| NarrativeQA | 1-1 | Văn học, Phim | 18,409 | F1 | Tiếng Anh | 200 |
| Qasper | 1-2 | Khoa học | 3,619 | F1 | Tiếng Anh | 200 |
| MultiFieldQA-en | 1-3 | Đa lĩnh vực | 4,559 | F1 | Tiếng Anh | 150 |
| MultiFieldQA-zh | 1-4 | Đa lĩnh vực | 6,701 | F1 | Tiếng Trung | 200 |
| **QA Đa tài liệu** |
| HotpotQA | 2-1 | Wikipedia | 9,151 | F1 | Tiếng Anh | 200 |
| 2WikiMultihopQA | 2-2 | Wikipedia | 4,887 | F1 | Tiếng Anh | 200 |
| MuSiQue | 2-3 | Wikipedia | 11,214 | F1 | Tiếng Anh | 200 |
| DuReader | 2-4 | Tìm kiếm Baidu | 15,768 | Rouge-L | Tiếng Trung | 200 |
| **Tóm tắt** |
| GovReport | 3-1 | Báo cáo chính phủ | 8,734 | Rouge-L | Tiếng Anh | 200 |
| QMSum | 3-2 | Cuộc họp | 10,614 | Rouge-L | Tiếng Anh | 200 |
| MultiNews | 3-3 | Tin tức | 2,113 | Rouge-L | Tiếng Anh | 200 |
| VCSUM | 3-4 | Cuộc họp | 15,380 | Rouge-L | Tiếng Trung | 200 |
| **Học Few-shot** |
| TREC | 4-1 | Câu hỏi web | 5,177 | Accuracy (CLS) | Tiếng Anh | 200 |
| TriviaQA | 4-2 | Wikipedia, Web | 8,209 | F1 | Tiếng Anh | 200 |
| SAMSum | 4-3 | Hội thoại | 6,258 | Rouge-L | Tiếng Anh | 200 |
| LSHT | 4-4 | Tin tức | 22,337 | Accuracy (CLS) | Tiếng Trung | 200 |
| **Hoàn thành Mã** |
| LCC | 6-1 | Github | 1,235 | Edit Sim | Python/C#/Java | 500 |
| RepoBench-P | 6-2 | Github repository | 4,206 | Edit Sim | Python/Java | 500 |

Bảng 7: Hiệu năng trên một tập con các benchmark tổng quát.

| Mô hình | PIQA | WSC | HellaSwag | SIQA | WinoGrande | Race-H | Race-M | NaturalQuestions | Trung bình |
|---------|------|-----|-----------|------|------------|--------|--------|-----------------|------------|
| Llama2-7B (4K) | 78.1 | 67.3 | 73.0 | 48.1 | 69.5 | 40.2 | 37.1 | 16.4 | 53.7 |
| E2-LLM-32K | 77.6 | 66.4 | 71.9 | 45.9 | 69.5 | 47.6 | 40.9 | 18.8 | 54.8 |
| E2-LLM-64K | 77.2 | 67.3 | 70.9 | 46.5 | 69.4 | 44.4 | 38.8 | 17.5 | 54.0 |
| Llama2-13B (4K) | 78.9 | 64.4 | 75.7 | 51.7 | 73.5 | 63.0 | 58.9 | 20.2 | 60.8 |
| E2-LLM-32K | 79.3 | 67.3 | 75.6 | 61.3 | 74.4 | 67.3 | 59.8 | 26.8 | 64.0 |
| E2-LLM-64K | 78.8 | 67.3 | 75.0 | 61.1 | 74.5 | 61.7 | 55.8 | 25.2 | 62.4 |

0 40000 80000 120000 160000 200000
Độ dài Ngữ cảnh
1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6
PPL
E2-LLM-7B-200k
E2-LLM-13B-200k

Hình 7: Kết quả của E2-LLM trên cửa sổ ngữ cảnh 200K.

### A.3 Kết quả thêm trên các tập dữ liệu độ dài ngắn

Chúng tôi đánh giá các mô hình được mở rộng bởi E2-LLM trên một số tác vụ benchmark độ dài ngắn tiêu chuẩn (tức là PIQA (Bisk et al., 2020), WSC (Levesque et al., 2012), HellaSwag (Zellers et al., 2019), SIQA (Sap et al., 2019), WinoGrande (ai2, 2019), RACE (Lai et al., 2017), và NaturalQuestions (Kwiatkowski et al., 2019)) trong kích thước cửa sổ ngữ cảnh ban đầu 4.096, trong đó kết quả hiệu năng zero-shot được báo cáo trong Bảng 7. Cụ thể, chúng tôi báo cáo kết quả của các mô hình mở rộng (E2-LLM-32K và E2-LLM-64K) trên các mô hình Llama2-7B và Llama2-13B tương ứng. Từ kết quả của Bảng 7, khi so sánh với các mô hình Llama2 cơ sở, chúng tôi quan sát thấy rằng các mô hình Llama2 mở rộng (ví dụ: E2-LLM-32K, E2-LLM-64K) vẫn duy trì kết quả hiệu năng tương đương hoặc thậm chí tốt hơn trên các tập dữ liệu benchmark độ dài ngắn này, điều này tiếp tục chứng minh hiệu quả của E2-LLM trong việc diễn giải ngữ cảnh ngắn.

### A.4 Mở rộng trên E2-LLM của chúng tôi

Gần đây, chúng tôi đã cập nhật phương pháp và chiến lược huấn luyện của E2-LLM, và thành công mở rộng Llama2-7B và 13B đến cửa sổ ngữ cảnh 200k với chi phí huấn luyện thấp hơn đáng kể như được hiển thị trong Hình 7. Trong Hình 7, chúng tôi báo cáo kết quả PPL trên một số tài liệu được cắt bớt đến 200k từ Proof-Pile, và chúng tôi quan sát thấy rằng kết quả PPL giảm mượt mà, điều này tiếp tục chứng minh hiệu quả của E2-LLM. Chúng tôi sẽ thêm chi tiết triển khai nhiều hơn về phương pháp E2-LLM được cập nhật mới trong tương lai.
