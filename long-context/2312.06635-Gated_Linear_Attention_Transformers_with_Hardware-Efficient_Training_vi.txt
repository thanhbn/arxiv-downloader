# 2312.06635.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2312.06635.pdf
# Kích thước tệp: 803395 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Transformers Chú Ý Tuyến Tính Có Cổng với Huấn Luyện Hiệu Quả Phần Cứng
Songlin Yang1 *Bailin Wang1 *Yikang Shen2Rameswar Panda2Yoon Kim1

Tóm tắt
Transformers với chú ý tuyến tính cho phép huấn luyện song song hiệu quả nhưng đồng thời có thể được công thức hóa như một RNN với trạng thái ẩn 2D (có giá trị ma trận), do đó tận hưởng độ phức tạp suy luận thời gian tuyến tính. Tuy nhiên, chú ý tuyến tính thường kém hiệu suất hơn chú ý softmax thông thường. Hơn nữa, các triển khai hiện tại của chú ý tuyến tính thiếu nhận thức I/O và do đó chậm hơn các triển khai được tối ưu hóa cao của chú ý softmax. Công trình này mô tả một thuật toán hiệu quả phần cứng cho chú ý tuyến tính mà đánh đổi việc di chuyển bộ nhớ với khả năng song song hóa. Việc triển khai kết quả, được gọi là FLASH LINEAR ATTENTION, nhanh hơn FLASH ATTENTION-2 (Dao, 2023) như một lớp độc lập ngay cả trên độ dài chuỗi ngắn (ví dụ: 1K). Sau đó chúng tôi tổng quát hóa thuật toán này thành một biến thể biểu cảm hơn của chú ý tuyến tính với các cổng phụ thuộc dữ liệu. Khi được sử dụng như một sự thay thế cho lớp chú ý tiêu chuẩn trong Transformers, Transformer chú ý tuyến tính có cổng (GLA) kết quả được tìm thấy hoạt động cạnh tranh với Transformer kiến trúc LLaMA (Touvron et al., 2023) cũng như các baseline suy luận thời gian tuyến tính gần đây như RetNet (Sun et al., 2023a) và Mamba (Gu & Dao, 2023) trong các thí nghiệm mô hình hóa ngôn ngữ quy mô vừa phải. Transformer GLA đặc biệt hiệu quả trong việc tổng quát hóa độ dài, cho phép một mô hình được huấn luyện trên 2K tổng quát hóa đến các chuỗi dài hơn 20K mà không bị suy giảm độ khó hiểu đáng kể. Về tốc độ huấn luyện, Transformer GLA có thông lượng cao hơn một mô hình Mamba có kích thước tương tự.
/gtbhttps://github.com/sustcsonglin/flash-linear-attention

*Đóng góp ngang bằng1Massachusetts Institute of Technology
2MIT-IBM Watson AI Lab. Liên hệ: Songlin Yang
<yangsl66@mit.edu>, Bailin Wang <bailinw@mit.edu>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học Máy, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

1 Giới thiệu
Transformers với chú ý softmax (Vaswani et al., 2017) tận hưởng huấn luyện song song hiệu quả nhưng gặp phải độ phức tạp bậc hai (theo độ dài chuỗi), do đó thúc đẩy các mô hình giống RNN hơn cho phép mô hình hóa chuỗi thời gian tuyến tính. Chú ý tuyến tính, thay thế hàm tương tự mũ bằng một tích vô hướng đơn giản trên các vector khóa/truy vấn (có thể đã được biến đổi), đã nổi lên như một sự thay thế đầy hứa hẹn cho chú ý softmax cổ điển (Katharopoulos et al., 2020; Choromanski et al., 2021; Kasai et al., 2021; Peng et al., 2021). Một thuộc tính hấp dẫn của chú ý tuyến tính là nó thừa nhận một "dạng hồi quy" trong đó nó có thể được công thức hóa như một RNN tuyến tính với trạng thái ẩn 2D (Katharopoulos et al., 2020), do đó cho phép suy luận thời gian tuyến tính. Để huấn luyện, chú ý tuyến tính cũng thừa nhận một "dạng song song theo khúc" dưới bậc hai chia chuỗi thành các khúc không chồng lấp và thực hiện các tính toán hồi quy liên khúc (nối tiếp) theo sau bởi các tính toán nội khúc (song song) (Hua et al., 2022; Sun et al., 2023a; Lingle, 2023), do đó (một phần) duy trì huấn luyện song song. Tuy nhiên, các thuật toán hiện có cho chú ý tuyến tính không nhận thức I/O và do đó, trên thực tế, chậm hơn các triển khai được tối ưu hóa của chú ý softmax (Dao et al., 2022b; Dao, 2023) trên độ dài chuỗi vừa phải.

Từ góc độ hiệu suất, chú ý tuyến tính thường được tìm thấy kém hiệu suất hơn chú ý softmax thông thường, thường với một khoảng cách đáng kể trong mô hình hóa ngôn ngữ (Kasai et al., 2021). Các biến thể gần đây của chú ý tuyến tính như RetNet (Sun et al., 2023a) và TransNormerLLM (Qin et al., 2023b) đạt được những cải thiện đáng kể bằng cách nhân trạng thái ẩn hiện tại với một yếu tố suy giảm trước khi cập nhật RNN. Tuy nhiên, các công trình này sử dụng một yếu tố suy giảm toàn cục, không phụ thuộc dữ liệu, mặc dù thực tế là trong RNN 1D, một cơ chế cổng phụ thuộc dữ liệu đã được chứng minh là quan trọng cho hiệu suất (van der Westhuizen & Lasenby, 2018; Qin et al., 2023c). Và ngay cả với yếu tố suy giảm, Transformers chú ý tuyến tính vẫn kém hiệu suất hơn các kiến trúc Transformer mạnh nhất khi được huấn luyện trước từ đầu.

Công trình này phát triển một thuật toán hiệu quả phần cứng cho chú ý tuyến tính, và áp dụng nó để huấn luyện một biến thể có cổng của chú ý tuyến tính mà cạnh tranh với chú ý softmax. Trước tiên chúng tôi thảo luận các khía cạnh của việc tối ưu hóa chú ý tuyến tính thông thường trên GPU hiện đại và đưa ra hai thuật toán nhận thức I/O (được điều chỉnh cho các cài đặt huấn luyện khác nhau) dựa trên những nguyên tắc này (§3). Triển khai thuật toán của chúng tôi, được gọi là FLASH LINEAR ATTENTION, nhanh hơn FLASH ATTENTION-2 (Dao, 2023) ngay cả trên các chuỗi ngắn (ví dụ: 1K). Sau đó chúng tôi mô tả một lớp chú ý tuyến tính có cổng với một cơ chế cổng phụ thuộc dữ liệu và chỉ ra cách FLASH LINEAR ATTENTION có thể được tổng quát hóa cho trường hợp có cổng (§4). Chúng tôi nghiên cứu Transformer chú ý tuyến tính có cổng (GLA) kết quả trên các benchmark mô hình hóa ngôn ngữ quy mô vừa phải, trong đó chúng tôi huấn luyện các mô hình với 340M/1.3B tham số trên 15B/100B token, tương ứng. Chúng tôi thấy rằng Transformer GLA hoạt động thuận lợi so với một baseline Transformer kiến trúc LLaMA mạnh sử dụng các công thức gần đây (Transformer++; Touvron et al., 2023) cũng như các mô hình chuỗi thời gian tuyến tính gần đây như RetNet (Sun et al., 2023a) và Mamba (Gu & Dao, 2023). Transformer GLA được tìm thấy đặc biệt mạnh trong việc tổng quát hóa độ dài và các nhiệm vụ chuyên sâu về nhớ lại trong các mô hình hồi quy tuyến tính. Về tốc độ huấn luyện, Transformer GLA có thông lượng cao hơn đáng kể so với một mô hình Mamba có kích thước tương tự.

2 Nền tảng: Chú Ý Tuyến Tính
Chúng tôi trước tiên đưa ra một nền tảng ngắn gọn về các lớp chú ý tuyến tính. Về ký hiệu, chúng tôi sử dụng chữ in hoa đậm cho ma trận (ví dụ: S, Q), chữ thường đậm cho vector (ví dụ: qt, kt), và chữ in hoa nghiêng cho ma trận tham số có thể học (ví dụ: WK). Chúng tôi thường sử dụng cùng một bảng chữ cái để hiển thị các hàng của ma trận, ví dụ: qt là hàng thứ t của Q.

2.1 Dạng Song Song và Hồi Quy
Transformers tự hồi quy tiêu chuẩn sử dụng một cơ chế chú ý softmax nhận một chuỗi đầu vào X∈RL×d (ở đây L là độ dài và d là chiều ẩn) và tính toán đầu ra O∈RL×d thông qua:

Q,K,V=XWQ,XWK,XWV,
O=softmax((QKT)⊙M)V,

trong đó WQ,WK,WV∈Rd×d là các ma trận có thể học và M∈{−∞,1}L×L là một mặt nạ ngăn mô hình chú ý đến các token tương lai, tức là Mij=1 nếu i≥j và Mij=−∞ nếu i<j. (Ở đây chúng tôi giả định một đầu chú ý duy nhất cho đơn giản.) Dạng song song của chú ý ở trên có thể tính toán O song song được cung cấp đầy đủ đầu vào X, do đó cho phép huấn luyện hiệu quả. Tuy nhiên, trong quá trình suy luận, Transformers phải sử dụng dạng hồi quy sau:

qt,kt,vt=xtWQ,xtWK,xtWV,
ot=∑ti=1exp(qtkTi)vi/∑ti=1exp(qtkTi),

tính toán các vector truy vấn (qt), khóa (kt), và giá trị (vt) được cung cấp biểu diễn token hiện tại xt∈R1×d và thực hiện chú ý trên tập hợp (đang phát triển) các khóa {k1,...,kt} và giá trị {v1,...,vt} (tức là "KV cache").

Các cơ chế chú ý tuyến tính (Katharopoulos et al., 2020) thay thế exp(qtkTi) bằng một kernel k(x,y) với một ánh xạ đặc trưng liên quan ϕ (tức là k(x,y)=⟨ϕ(x),ϕ(y)⟩). Điều này đơn giản hóa việc tính toán ot vì chúng ta có:

ot=∑ti=1ϕ(qt)ϕ(ki)Tvi/∑ti=1ϕ(qt)ϕ(ki)T=ϕ(qt)∑ti=1ϕ(ki)Tvi/ϕ(qt)∑ti=1ϕ(ki)T.

Đặt St=∑ti=1ϕ(ki)Tvi và zt=∑ti=1ϕ(ki)T trong đó St∈Rd×d, zt∈Rd×1, chúng ta có thể viết lại ở trên như một RNN:

St=St−1+ϕ(kt)Tvt, zt=zt−1+ϕ(kt)T, ot=ϕ(qt)St/ϕ(qt)zt.

Mặc dù các kernel khác nhau đã được khám phá (Kasai et al., 2021; Peng et al., 2021), công trình gần đây đã thấy rằng một kernel tuyến tính (tức là đặt ϕ là đồng nhất) không có bộ chuẩn hóa hoạt động tốt trong thực tế (Sun et al., 2023a). Điều này dẫn đến một lớp chú ý tuyến tính (không chuẩn hóa) với phương trình cập nhật sau:

St=St−1+kTtvt, ot=qtSt. (1)

Phương trình 1 làm rõ rằng một lớp chú ý tuyến tính về cơ bản là một lớp hồi quy tuyến tính với trạng thái ẩn có giá trị ma trận St được cập nhật qua tích ngoài kTtvt=(xtWK)T(xtWV).

Dạng song song của chú ý tuyến tính nhân quả, có độ phức tạp vẫn là bậc hai theo L, được cho bởi O=((QKT)⊙M)V, trong đó M∈{0,1}L×L là một mặt nạ sao cho Mij=1 nếu i≥j và Mij=0 nếi i<j. Do M, không thể khai thác thuộc tính kết hợp của phép nhân ma trận để giảm độ phức tạp dạng song song từ bậc hai xuống tuyến tính.

2.2 Dạng Song Song Theo Khúc
Dạng song song theo khúc của chú ý tuyến tính tạo cân bằng giữa dạng song song và hồi quy (Hua et al., 2022; Sun et al., 2023a), và cho phép huấn luyện dưới bậc hai, song song một phần. Chính thức, giả sử đầu vào X bây giờ được chia thành các khúc không chồng lấp, trong đó mỗi khúc có độ dài C. Đặt S[i]∈Rd×d là trạng thái ẩn cấp khúc sau khi xử lý i khúc, tức là S[i]:=SiC. Hơn nữa đặt Q[i]:=QiC+1:(i+1)C+1∈RC×d là các vector truy vấn tương ứng với khúc thứ i; đặt K[i], V[i], O[i] được định nghĩa tương tự. Sau đó chúng ta có hồi quy liên khúc sau (cho i∈[0,1,...L/C−1]):

S[i+1]=S[i]+∑(i+1)Cj=iC+1kTjvj=S[i]+KT[i]V[i]∈Rd×d. (2)

Ở đây S[0] có thể được khởi tạo bằng không hoặc từ trạng thái ẩn của đoạn trước. Tổng của tất cả các đầu vào RNN từ một khúc (tức là KT[i]V[i]) có thể được tính toán trong O(C2d) song song. Tính toán song song nội khúc cho đầu ra được cho bởi:

O[i+1]=Q[i+1]S[i] + ((Q[i+1]KT[i+1])⊙M)V[i+1],

trong đó O[i+1]∈RC×d. Ở đây thành phần "nội khúc" Ointra[i+1] có chính xác cùng dạng song song như Phương trình 1 và do đó mất O(C2d+Cd2). Thành phần "liên khúc" Ointer[i+1] tính đến đóng góp từ trạng thái ẩn từ khúc trước, và mất O(Cd2). Độ phức tạp huấn luyện do đó là O((L/C)(C2d+Cd2))=O(LCd+Ld2), ít hơn O(L2d) khi L>d. Lưu ý rằng đặt C=L khôi phục dạng song song, và C=1 khôi phục dạng hồi quy.

3 Chú Ý Tuyến Tính Hiệu Quả Phần Cứng
Chúng tôi mô tả FLASH LINEAR ATTENTION, một thuật toán nhận thức I/O, hiệu quả phần cứng cho chú ý tuyến tính theo tinh thần của FLASH ATTENTION (Dao et al., 2022b; Dao, 2023). Trước tiên chúng tôi thảo luận các khía cạnh của phần cứng cần được tính đến cho một triển khai hiệu quả thực tế.

3.1 Nguyên Tắc của Thuật Toán Hiệu Quả Phần Cứng
Một thuật toán hiệu quả nên nhận thức về mô hình tính toán, hệ thống cấp bậc bộ nhớ, và các đơn vị tính toán chuyên biệt trên phần cứng hiện đại.

Độ chiếm dụng. GPU có nhiều luồng được thực hiện song song; các luồng được nhóm thành các khối luồng, thực hiện trên các đa xử lý luồng (SM). Để duy trì độ chiếm dụng GPU cao (tức là phần trăm tài nguyên GPU đang được sử dụng), cần thiết phải sử dụng số lượng SM đủ. Trong các tình huống huấn luyện quy mô lớn và mô hình hóa chuỗi dài nơi kích thước batch có xu hướng nhỏ, việc song song hóa theo chiều thời gian cho phép độ chiếm dụng GPU cao (Dao, 2023).

Đơn vị tính toán chuyên biệt. Phần cứng hiện đại cho huấn luyện mạng nơ-ron thường có các đơn vị tính toán chuyên biệt (ví dụ: tensor core trên GPU NVIDIA, đơn vị nhân ma trận trên TPU), có thể tăng tốc đáng kể matmul; ví dụ matmul nửa độ chính xác trên A100 có thể nhanh hơn khoảng 16 lần trên tensor core so với CUDA core. Những đơn vị chuyên biệt này quan trọng cho huấn luyện quy mô lớn.

Hệ thống cấp bậc bộ nhớ. GPU có hệ thống cấp bậc bộ nhớ với bộ nhớ GPU toàn cục lớn hơn nhưng chậm hơn (bộ nhớ băng thông cao; HBM) và bộ nhớ chia sẻ nhỏ hơn nhưng nhanh hơn (SRAM). Sử dụng tối ưu SRAM để giảm chi phí I/O HBM do đó có thể dẫn đến tăng tốc đáng kể.

3.2 Cân Nhắc Phần Cứng cho Chú Ý Tuyến Tính
Bây giờ chúng tôi thảo luận các cân nhắc phần cứng liên quan đến hiệu quả của các dạng khác nhau của chú ý tuyến tính.

Dạng hồi quy. Một triển khai cơ bản của dạng hồi quy lưu trữ các trạng thái ẩn 2D của tất cả các bước thời gian trong HBM, dẫn đến chi phí I/O cao (Mao, 2022). Chi phí I/O có thể được giảm bằng cách tránh việc cụ thể hóa như vậy và tính toán lại các trạng thái ẩn trong quá trình truyền ngược, như trong Katharopoulos et al. (2020), nhưng các hoạt động theo phần tử trong cập nhật hồi quy không thể sử dụng tensor core và dẫn đến cường độ số học thấp. Do đó, trong khi dạng hồi quy thường có tổng FLOP thấp nhất trong ba dạng, điều này không chuyển thành hiệu quả thời gian thực tế.

Và trong khi về mặt lý thuyết có thể song song hóa các hồi quy tuyến tính qua thuật toán quét song song, phương pháp này yêu cầu cụ thể hóa trạng thái ẩn 2D cho mỗi bước thời gian. Điều này phát sinh gánh nặng I/O bộ nhớ đáng kể, do đó bù trừ lợi ích của việc song song hóa trên độ dài chuỗi và dẫn đến tốc độ chạy thực tế chậm, như trong Katsch (2023).

Dạng song song. Dạng song song có thể hiệu quả như FLASH ATTENTION sử dụng các kỹ thuật tối ưu I/O tương tự, như được chứng minh bởi Qin et al. (2023b). Tuy nhiên, số lượng FLOP cao (do độ phức tạp bậc hai) làm cho huấn luyện chuỗi dài đắt đỏ, cùng vấn đề mà triển khai ngây thơ của chú ý softmax sẽ gặp phải.

Dạng theo khúc. Dạng song song theo khúc, nội suy giữa các dạng song song và hồi quy với một "tham số" bổ sung C, làm cho có thể dễ dàng hơn để thực hiện các đánh đổi trên cho tối ưu hóa tinh tế. Không giống dạng hồi quy, hầu hết các hoạt động có thể được thực hiện qua matmul, cho phép sử dụng tensor core (nếu C được đặt thành bội số của 16). Mặc dù thuật toán huấn luyện theo khúc đã được thảo luận trước đó trong văn hóa (Hua et al., 2022; Sun et al., 2023a), hầu hết các triển khai không nhận thức I/O và do đó chậm hơn FLASH ATTENTION cho độ dài chuỗi vừa phải (ví dụ: 2K-4K).

3.3 FLASH LINEAR ATTENTION: Chú Ý Tuyến Tính Hiệu Quả Phần Cứng với Dạng Theo Khúc
Chúng tôi mô tả triển khai nhận thức I/O, hiệu quả phần cứng của dạng theo khúc. Chúng tôi đưa ra hai phiên bản, có các lượt truyền tiến và lùi khác nhau tùy thuộc vào việc các trạng thái ẩn cấp khúc S[n] có được cụ thể hóa trong HBM hay không. Xem Thuật toán 1 và Hình 1 cho lượt truyền tiến. (Thuật toán 2 trong phụ lục mô tả lượt truyền lùi.) Ở mức độ cao, chúng tôi sử dụng tiling để tải các tensor theo khối và tái sử dụng các khối tensor trên chip để tránh nhiều I/O HBM càng nhiều càng tốt. Ví dụ, khi Q[n] được tải vào SRAM, cả Q[n]S và (Q[n]K⊤[n]⊙M)V[n] đều có thể được tính toán trên chip, tránh tải Q[n] hai lần, do đó tiết kiệm I/O HBM.

Phiên bản không cụ thể hóa tính toán O[n] tuần tự cho n∈[N], sử dụng SRAM để tạm thời lưu trữ S[n], hiệu quả bộ nhớ. Phiên bản này song song hóa qua kích thước batch, số đầu, và chiều đầu, nhưng thiếu song song cấp chuỗi. Khi kích thước batch lớn, mức độ song song này đủ để cho phép độ chiếm dụng GPU cao. Trong cài đặt chuỗi dài và huấn luyện quy mô lớn nơi kích thước batch nhỏ, SM không thể được khai thác đầy đủ trong trường hợp này. Phiên bản cụ thể hóa trước tiên thực hiện hồi quy liên khúc (Phương trình 2) và lưu trữ tất cả S[n] cho n∈[N] trong HBM. Sau đó, O[n] có thể được tính toán song song cho tất cả các khúc. Cách tiếp cận này cung cấp song song tốt hơn nhưng tăng dung lượng bộ nhớ khoảng 10-20%. Chúng tôi giảm thiểu điều này thông qua tính toán lại, trong đó các trạng thái ẩn bị loại bỏ sau lượt truyền tiến và tính toán lại trong lượt truyền lùi. Chúng tôi thấy điều này tạo ra một chi phí thời gian chạy nhỏ nhưng giảm đáng kể dung lượng bộ nhớ, và chúng tôi áp dụng chiến lược này theo mặc định.

Hình 2 cho thấy tốc độ và dung lượng bộ nhớ của triển khai của chúng tôi. Cả hai phiên bản của FLASH LINEAR ATTENTION đều nhanh hơn đáng kể so với FLASH ATTENTION-2 (Dao, 2023) và một triển khai PyTorch thuần túy (tức là không nhận thức I/O) của chú ý tuyến tính theo khúc, cho thấy lợi ích của nhận thức I/O.

4 Chú Ý Tuyến Tính Có Cổng
Hồi quy tuyến tính trong Phương trình 1 không có số hạng suy giảm hoặc cổng quên, điều đã được chứng minh là quan trọng trong RNN (Hochreiter & Schmidhuber, 1997; Cho et al., 2014; van der Westhuizen & Lasenby, 2018). Việc thiếu số hạng suy giảm làm cho mô hình khó "quên" thông tin, và đã được giả thuyết là một phần chịu trách nhiệm cho sự bất ổn của chú ý tuyến tính trong các nhiệm vụ ngữ cảnh dài (Buckman & Gelada, 2024). Các công trình gần đây (Sun et al., 2023a; Qin et al., 2023b) đạt được hiệu suất tốt hơn thông qua việc tích hợp một yếu tố suy giảm toàn cục, không phụ thuộc dữ liệu γ∈(0,1) vào chú ý tuyến tính: St=γSt−1+kTtvt. Việc sử dụng một γ duy nhất được thiết kế để bảo tồn dạng song song kiểu chú ý cho huấn luyện hiệu quả. Trong công trình này, chúng tôi xem xét một cơ chế cổng phụ thuộc dữ liệu cho chú ý tuyến tính. Chúng tôi chỉ ra rằng mặc dù có yếu tố cổng biểu cảm hơn, lớp chú ý tuyến tính có cổng (GLA) kết quả vẫn thừa nhận một dạng theo khúc hiệu quả phần cứng cho huấn luyện hiệu quả.

4.1 Dạng Hồi Quy và Song Song của GLA
Dạng hồi quy. GLA có một cổng quên 2D Gt∈(0,1)dk×dv thay đổi theo thời gian:

St=Gt⊙St−1+k⊤tvt,

trong đó bây giờ chúng tôi cho phép trạng thái ẩn có các chiều biến đổi. Dạng hồi quy dựa trên tích Hadamard này rất tổng quát và bao gồm nhiều RNN gần đây với trạng thái ẩn 2D, như được liệt kê trong Bảng 1.

Trung tâm của thiết kế chú ý tuyến tính có cổng là việc tham số hóa Gt yêu cầu cân bằng giữa hiệu quả tham số, kích thước trạng thái, và hiệu quả huấn luyện. Một ánh xạ ngây thơ xt7→Gt để có được ma trận cổng phụ thuộc dữ liệu sẽ yêu cầu một ma trận có kích thước d·dk·dv, không hiệu quả tham số. Mao (2022) đề xuất một tham số hóa thứ hạng thấp dựa trên tích ngoài hiệu quả hơn (Gt=α⊤tβt), yêu cầu d·dv+d·dk tham số.

Trong Mamba (Gu & Dao, 2023), Gt được thu được bằng cách kết hợp một ma trận có thể học không phụ thuộc dữ liệu A với một vector phụ thuộc dữ liệu αt, cho phép ma trận có thứ hạng đầy đủ. Tuy nhiên, điều này ngăn cản việc sử dụng tensor core vì nó không thể được công thức hóa lại thành định dạng nhân ma trận, như được thảo luận trong Dao & Gu (2024). Việc thiếu dạng nhân ma trận compact cần thiết phải cụ thể hóa các trạng thái ẩn của mỗi bước thời gian. Để giảm chi phí I/O cao, Gu & Dao (2023) phát triển một thuật toán nhận thức phần cứng cụ thể hóa các trạng thái ẩn độc quyền trong SRAM thay vì trong HBM. Do dung lượng SRAM hạn chế, cách tiếp cận này không thể mở rộng đến các trạng thái ẩn lớn hơn, điều mà, như chúng tôi sẽ chỉ ra trong các thí nghiệm của mình, dẫn đến hiệu suất dưới mức tối ưu trong các nhiệm vụ chuyên sâu về nhớ lại. Mamba-2 (Dao & Gu, 2024) giải quyết hạn chế này với một cơ chế cổng hạn chế hơn: Gt=γt1T1, trong đó γt∈(0,1) là một vô hướng, làm cho có thể công thức hóa lại hồi quy trong dạng nhân ma trận, cho phép sử dụng tensor core và kích thước trạng thái lớn hơn. Cổng phụ thuộc dữ liệu vô hướng này cũng được sử dụng trong Peng et al. (2021), Sun et al. (2024), và Beck et al. (2024).

Bài báo này áp dụng một cách tiếp cận trung dung giữa tham số hóa vô hướng và thứ hạng thấp hoàn toàn bằng cách sử dụng Gt=α⊤t1. Điều này dẫn đến dạng hồi quy sau:

St=(α⊤t1)⊙St−1+k⊤tvt=Diag(αt)St−1+k⊤tvt, (3)

trong đó αt được tham số hóa qua một lớp tuyến tính thứ hạng thấp theo sau bởi sigmoid trên xt (xem §4.4). Lưu ý rằng công thức trên là tổng quát và bao gồm một số RNN gần đây (Katsch, 2023; Qin et al., 2024b; Peng et al., 2024). Do đó, triển khai GLA hiệu quả phần cứng (được mô tả tiếp theo) có thể được sử dụng trực tiếp hoặc thích ứng với các mô hình khác.

Dạng song song. Bây giờ chúng tôi mô tả một dạng song song GLA để song song hóa qua độ dài chuỗi. Triển khai Phương trình 3 cho:

St=∑ti=1[∏tj=i+1α⊤j1]⊙k⊤ivi

Đặt bt:=∏tj=1αj, chúng ta có thể viết lại ở trên thành:

ot=qtSt=qt∑ti=1(bt/bi⊤1)⊙k⊤ivi = ∑ti=1(qt⊙bt)ki/bi⊤vi

trong đó phép chia là theo phần tử. Đặt B∈(0,1)L×d là ma trận thu được từ việc xếp chồng bt, dạng song song là:

O=((Q⊙B)K/B⊤⊙M)V.

Tuy nhiên, dạng này không ổn định về số vì bt là tích tích lũy của các giá trị cổng trong αj∈(0,1)1×d, và do đó có thể cực kỳ nhỏ khi t lớn, làm cho K/B bùng nổ. Để xử lý điều này, chúng ta có thể tính toán trong không gian log cho P:

Pij=∑dk=1QikKjkexp(logBik−logBjk), i≥j, (4)

trong đó k biểu thị các chỉ số đặc trưng. Tuy nhiên, không giống chú ý tuyến tính vanilla, vì Phương trình 4 không thể được biểu diễn qua một matmul tiêu chuẩn, và nó không thể sử dụng matmul nửa độ chính xác trên tensor core. Chúng tôi sẽ chỉ ra trong §4.3 cách một cơ chế chunking cấp thứ cấp có thể cho phép sử dụng matmul nửa độ chính xác cho hầu hết các tính toán trong khi duy trì tính ổn định số, như minh họa trong Hình 3.

4.2 Dạng Song Song Theo Khúc của GLA
Chúng tôi rút ra một dạng theo khúc của GLA tương tự như dạng theo khúc của chú ý tuyến tính cơ bản (§2.2). Ở đây hoạt động nội khúc triển khai dạng song song ở trên ở cấp khúc để có được Ointra. Cho liên khúc, chúng ta có:

ΛiC+j=biC+j/biC, ΓiC+j=b(i+1)C/biC+j, γi+1=b(i+1)C/biC,

S[i+1]=(γ⊤i+11)⊙S[i]+(K[i+1]⊙Γ[i+1])⊤V[i+1],

Ointer[i+1]=(Q[i+1]⊙Λ[i+1])S[i].

Trực quan, Λ[i+1] mã hóa suy giảm tích lũy từ đầu một khúc sẽ được sử dụng để lan truyền các trạng thái ẩn từ khúc trước S[i], trong khi Γ[i+1] mã hóa suy giảm đến cuối một khúc sẽ được sử dụng để tích lũy thông tin được thêm vào trạng thái ẩn tiếp theo S[i+1].

4.3 GLA Hiệu Quả Phần Cứng
Với dạng theo khúc trong tay, chúng tôi có thể thích ứng thuật toán FLASH LINEAR ATTENTION được trình bày trong §3 cho trường hợp có cổng. Việc thích ứng thêm dựa vào hai kỹ thuật quan trọng được mô tả dưới đây. Chúng tôi đưa ra trực giác cấp cao trong phần này và để lại các thuật toán đầy đủ cho Thuật toán 3-6 của Phụ lục A.3.

Chunking cấp thứ cấp. Không giống trong chú ý tuyến tính thông thường, các tính toán nội khúc trong GLA không thể tận dụng matmul nửa độ chính xác (và do đó tensor core) do tính toán không gian log (Phương trình 4). Để sử dụng tốt hơn tensor core, chúng tôi sử dụng lược đồ chunking cấp thứ cấp, trong đó một khúc được chia nhỏ hơn thành các khúc con (tức là một cấp tiling khác) theo tinh thần của các kỹ thuật tiling cổ điển (Dao et al., 2022b). Ma trận giống chú ý P∈RL×L sau đó được tính toán theo cách theo khúc, như minh họa trong Hình 3. Cụ thể, các tương tác giữa các khúc con được tính toán qua matmul nửa độ chính xác:

P[i][j]=(Q[i]⊙Λ[i])(K[j]⊙Γ[j]⊙biC/b(j+1)C)T∈RC×C.

Điều này tương ứng với các ô màu cam trong Hình 3. Đối với phần nội khúc con (ô màu hồng trong Hình 3), chúng ta phải dùng đến Phương trình 4 và thực hiện matmul ở độ chính xác đầy đủ để ổn định. Với chiến lược tiling hai cấp này, tổng số FLOP matmul không phải nửa độ chính xác được giảm đáng kể, do đó dẫn đến cải thiện thời gian.

Tính toán dαt hiệu quả bộ nhớ. Công trình trước đây (Mao, 2022, §3.1) đã khẳng định rằng các mô hình giống GLA phải cụ thể hóa các trạng thái ẩn có giá trị ma trận có kích thước L×d×d trong HBM để tính toán tất cả gradient dαt, vì dαt=(St−1⊙dSt)1. Thay vào đó, chúng tôi đưa ra công thức dạng đóng sau cho dlogαt:

dlogbt=qt⊙dqt−kt⊙dkt, dlogαt=∑t≤i≤Ldlogbi,

có thể dễ dàng thu được bằng cách lấy đạo hàm đối với Phương trình 4 (xem Phụ lục A.3 cho đạo hàm đầy đủ). dqt và dkt có thể được tính toán như trong Thuật toán 2.

4.4 Transformer GLA
Chúng tôi tổng quát hóa lớp GLA cho trường hợp đa đầu. Cho H đầu, chúng ta có như sau cho mỗi đầu h∈[1,H]:

Sht=(αht⊤1)⊙Sht−1+khtTvht∈Rd′k×d′v,
oht=qhtSht∈R1×d′v,
o′t=concat(LN(o1t),...,LN(oHt))∈R1×dv,
rt=Swish(xtWr+br)∈R1×dv,
yt=(rt⊙o′t)WO∈R1×d.

Ở đây chúng tôi sử dụng các chiều khóa (dk) và giá trị (dv) riêng biệt; d′k=dk/H, d′v=dv/H là các chiều khóa/giá trị mỗi đầu. LayerNorm (LN) được áp dụng sau đầu ra của mỗi đầu, trong khi chiếu đầu ra và cổng đầu ra hoạt động trên việc nối đầu ra đầu (Sun et al., 2023a). Sau đó chúng tôi xây dựng một mô hình giống Transformer bằng cách xen kẽ các lớp GLA đa đầu với mạng nơ-ron truyền tiến (FFN). Cụ thể, cho biểu diễn ngữ cảnh hóa lớp l X(l), chúng tôi thu được X(l+1) qua:

Y(l)=GLA(LN(X(l)))+X(l)
X(l+1)=SwiGLU(LN(Y(l)))+X(l),

trong đó lớp SwiGLU FFN (Touvron et al., 2023) là:

SwiGLU(Z)=(Swish(ZW1)⊙ZW2)W3.

Phân bổ tham số. Như được trình bày, lớp GLA của chúng tôi sử dụng hai ma trận bổ sung để dự đoán αt, rt (tức là Wα, Wr) so với một lớp chú ý softmax thông thường. Để hiệu quả tham số, chúng tôi sử dụng một tham số hóa thứ hạng thấp:

αt=σ((xtW1αW2α+bα)))1/τ∈R1×dk,

trong đó W1α∈Rd×16, W2α∈R16×dk, và τ=16 là một số hạng nhiệt độ để khuyến khích mô hình có tỷ lệ quên chậm hơn. Chúng tôi thêm đặt dk=d/2 và dv=d và sử dụng tham số hóa thứ hạng đầy đủ cho (WQ, WK, WV, WO, Wr). Cuối cùng, một lớp GLA cùng nhau cần (khoảng) 4d2 tham số, như trong chú ý softmax thông thường.

5 Nghiên Cứu Thực Nghiệm
5.1 Thiết Lập Thí Nghiệm
Các thí nghiệm chính của chúng tôi là về mô hình hóa ngôn ngữ, trong đó chúng tôi nghiên cứu liệu GLA có thể hoạt động cạnh tranh với (i) một baseline Transformer mạnh với các công thức kiến trúc hiện đại và (ii) các mô hình thời gian tuyến tính gần đây. Chúng tôi sử dụng bộ dữ liệu SlimPajama (Soboleva et al., 2023) và token hóa nó bằng tokenizer Mistral (Jiang et al., 2023). Bộ dữ liệu gốc chứa 627B token; chúng tôi sử dụng một tập con 100B.

Baseline. Chúng tôi đánh giá GLA so với ba baseline: Transformer++ (Touvron et al., 2023), RetNet (Sun et al., 2023a), và Mamba (Gu & Dao, 2023). Transformer++ là kiến trúc LLaMA với Rotary Positional Embeddings (Su et al., 2021), SWiGLU (Shazeer, 2020), và RMSNorm (Zhang & Sennrich, 2019); chúng tôi cũng sử dụng SwiGLU trong RetNet để thay thế FFN gốc của nó để so sánh công bằng. Đối với Mamba, chúng tôi sử dụng mã nguồn mở. Tất cả baseline của chúng tôi được huấn luyện cho chính xác cùng số token trên cùng bộ dữ liệu để so sánh công bằng.

Chi tiết huấn luyện. Chúng tôi huấn luyện tất cả mô hình từ đầu ở hai quy mô: 340M và 1.3B. Tất cả mô hình được huấn luyện với AdamW (Loshchilov & Hutter, 2018) sử dụng tỷ lệ học tối đa 3e-4. Các mô hình 340M được huấn luyện trên 15B token với kích thước batch 0.5M token, trong khi các mô hình 1.3B được huấn luyện trên 100B token với kích thước batch 2M token. Chúng tôi sử dụng lịch trình tỷ lệ học cosine với warm-up 0.5B/1B token cho cài đặt 340M/1.3B, tương ứng. Tỷ lệ học ban đầu và cuối là 3e-5. Chúng tôi sử dụng weight decay 0.01, và gradient clipping 1.0.

5.2 Kết Quả Chính
Ngoài độ khó hiểu (ppl) trên Wikitext (Wiki.), chúng tôi xem xét một loạt rộng các nhiệm vụ hạ nguồn bao gồm lý luận thông thường và trả lời câu hỏi như đã được sử dụng trong Gu & Dao (2023): LAMBADA (LMB.; Paperno et al., 2016), PiQA (Bisk et al., 2020), HellaSwag (Hella.; Zellers et al., 2019), WinoGrande (Wino.; Sakaguchi et al., 2021), ARC-easy (ARC-e) và ARC-challenge (Arc-c) (Clark et al., 2018). Trong Phụ lục D, chúng tôi cũng bao gồm kết quả trên các nhiệm vụ bổ sung: Copa (Roemmele et al., 2011), SciQA (Auer et al., 2023), OpenbookQA (Mihaylov et al., 2018), BoolQA (Clark et al., 2019). Chúng tôi báo cáo độ khó hiểu (ppl) trên WikiText và LAMBADA, độ chính xác được chuẩn hóa theo độ dài trên HellaSwag, ARC-challenge và OpenbookQA, và độ chính xác trên các nhiệm vụ khác. Tất cả đánh giá được thực hiện sử dụng khai thác đánh giá LM (Gao et al., 2021).

Kết quả chính của chúng tôi được hiển thị trong Bảng 2. So với RetNet sử dụng tỷ lệ suy giảm không phụ thuộc dữ liệu, Transformer GLA với các cổng phụ thuộc dữ liệu cho thấy kết quả cải thiện trên tất cả nhiệm vụ. Cả Transformer GLA và Mamba đều cho thấy hiệu suất tương đương với Transformer++.

Các nhiệm vụ chuyên sâu về nhớ lại. Trong khi các mô hình dưới bậc hai có thể đạt được hiệu suất mô hình hóa ngôn ngữ cạnh tranh với Transformers, Arora et al. (2024) cho thấy rằng chúng tụt hậu so với chú ý softmax trong các nhiệm vụ chuyên sâu về nhớ lại. Tiếp theo chúng tôi đánh giá GLA trên các nhiệm vụ thực và tổng hợp tập trung vào nhớ lại.

Nhiệm vụ MQAR tổng hợp (Arora et al., 2023a) là một phiên bản đa truy vấn khó khăn hơn của nhiệm vụ induction head (Fu et al., 2023b) trong đó một mô hình phải nhớ lại token theo sau một token truy vấn nhiều lần. Chúng tôi theo cài đặt thí nghiệm của Arora et al. (2023a) và so sánh GLA với các mô hình dưới bậc hai gần đây, bao gồm RetNet (Sun et al., 2023a), Mamba (Gu & Dao, 2023), Hyena (Poli et al., 2023) và RWKV-4 (Peng et al., 2023). Đối với RetNet và GLA, số đầu được đặt thành 2; đối với các mô hình khác chúng tôi theo cài đặt mặc định trong Arora et al. (2023a). Kết quả được hiển thị trong Hình 4. Chú ý softmax tiêu chuẩn đạt được điểm số hoàn hảo trong tất cả cài đặt và do đó bị bỏ qua. Chúng tôi thấy rằng các mô hình với trạng thái ẩn có giá trị ma trận (tức là Mamba/RetNet/GLA) vượt trội hơn Hyena/RWKV, và GLA của chúng tôi vượt trội hơn RetNet, xác nhận lợi ích của việc sử dụng các cổng phụ thuộc dữ liệu.

Theo Arora et al. (2024), chúng tôi cũng kiểm tra các mô hình của mình trên ba nhiệm vụ chuyên sâu về nhớ lại thực: FDA (Arora et al., 2023b), SWDE (Lockard et al., 2019), và SQUAD (Rajpurkar et al., 2018). Những nhiệm vụ này tập trung vào trích xuất thông tin hoặc hiểu đọc. Như minh họa trong Bảng 3, các mô hình dưới bậc hai kém hiệu suất đáng kể so với Transformers trên FDA và SWDE, cả hai đều là nhiệm vụ trích xuất thông tin. Tuy nhiên, GLA vượt trội hơn các mô hình dưới bậc hai khác, có thể do trạng thái hồi quy lớn hơn (so với Mamba) và cơ chế lựa chọn (so với RetNet).

Huấn luyện chuỗi dài và ngoại suy độ dài. Một lợi thế của các mô hình chú ý tuyến tính là chúng cho phép huấn luyện chuỗi dài hiệu quả trong thời gian tuyến tính. Để thể hiện tính năng này, chúng tôi xem xét hai cài đặt huấn luyện: (i) huấn luyện trực tiếp trên ngữ cảnh độ dài 8K, (ii) huấn luyện trên ngữ cảnh độ dài 24K thông qua lan truyền ngược cắt cụt qua thời gian (TBPP) trên các đoạn độ dài 2K. Trong trường hợp sau, gradient không được lan truyền ngược qua các đoạn, và do đó cách tiếp cận này có chi phí tối thiểu tương đương với chiến lược huấn luyện độ dài 2K tiêu chuẩn (trong đó trạng thái ẩn ban đầu luôn được đặt thành không). Chúng tôi huấn luyện trước các mô hình Mamba, RetNet, và GLA 1.3B trên SlimPajama cho 100B token trên những cài đặt này và kiểm tra chúng trên cả tập kiểm tra SlimPajama và tập kiểm tra PG19 (Rae et al., 2019).

Hình 5 cho thấy độ khó hiểu của các token được tính toán trong các nhóm vị trí khác nhau. Đối với các mô hình được huấn luyện trên ngữ cảnh độ dài 2K, GLA ngoại suy tốt hơn Mamba/RetNet trong hầu hết các nhóm vị trí trên tập kiểm tra PG19; Mamba gặp khó khăn để ngoại suy vượt quá 4K, trong khi GLA/RetNet có thể tổng quát hóa đến 18K trên tập kiểm tra Slimpajama. Transformers không thể ngoại suy vượt quá độ dài huấn luyện, đây là một chế độ thất bại đã biết.

Huấn luyện trước trong một chuỗi dài cải thiện độ khó hiểu một cách nhất quán cho cả ba mô hình. Chúng tôi thấy sự khác biệt độ khó hiểu cận biên trong hai cài đặt cho GLA, cho thấy rằng TBPTT có thể là một cách tiếp cận kinh tế hơn cho huấn luyện chuỗi dài. Mamba hưởng lợi đáng kể từ huấn luyện độ dài 8K, và nó hoạt động tương tự như GLA trong cùng cài đặt huấn luyện.

Ablation. Chúng tôi tiến hành một nghiên cứu ablation quy mô nhỏ bằng cách huấn luyện các biến thể GLA 340M cho 7B token. Chúng tôi điều tra (i) tầm quan trọng của việc có cả cổng tinh tế và phụ thuộc dữ liệu và (ii) ảnh hưởng của kích thước chiều đầu. Kết quả được hiển thị trong Bảng 4. Đối với (i), chúng tôi thấy rằng trong khi các cổng vô hướng phụ thuộc dữ liệu cải thiện đáng kể so với RetNet, một cơ chế cổng tinh tế hơn vẫn cần thiết. Đối với (ii) chúng tôi điều chỉnh số đầu để thay đổi chiều đầu, trong đó theo mặc định GLA sử dụng 4 đầu. Tăng lên 8 (tức là chiều đầu nhỏ hơn) dẫn đến suy giảm độ khó hiểu tương đối lớn; giảm xuống 1 (tức là chiều đầu lớn hơn) thực sự hoạt động tốt nhất, nhưng dẫn đến cải thiện cận biên trong khi yêu cầu bộ nhớ GPU cao hơn nhiều. Do đó chúng tôi chọn 4 đầu cho các thí nghiệm của mình.

5.3 Hiệu Quả Huấn Luyện
Hình 6 cho thấy thông lượng và sử dụng bộ nhớ như một hàm của độ dài chuỗi và kích thước batch cho các mô hình 1.3B khác nhau trên một GPU H100 duy nhất. Ở đây GLA áp dụng phiên bản cụ thể hóa của FLASH LINEAR ATTENTION với tính toán lại trạng thái ẩn (§3.3). Tất cả mô hình có độ phức tạp không gian tuyến tính, và sự khác biệt tổng dung lượng GPU giữa chúng là tối thiểu. Về thông lượng huấn luyện, Mamba tụt hậu so với Transformer++ và GLA, với GLA cho thấy lợi thế lớn hơn trong độ dài huấn luyện vượt quá 4096.

5.4 Hạn Chế & Công Việc Tương Lai
Trong khi các thí nghiệm của chúng tôi với Transformer GLA ở quy mô đáng kính, chúng tôi không thể thực hiện các thí nghiệm quy mô lớn hơn do tài nguyên tính toán hạn chế. Mặc dù không rõ ràng tại thời điểm này GLA sẽ mở rộng như thế nào đến các mô hình/bộ dữ liệu thậm chí lớn hơn, chúng tôi dự đoán rằng hiệu quả huấn luyện của GLA trở nên thuận lợi hơn so với Mamba ở quy mô lớn hơn. Cụ thể, khi được mở rộng đến kích thước lớn hơn (ví dụ: >7B), GLA có thể hiệu quả hơn Mamba vì sử dụng tốt hơn tensor core và khả năng tương thích của GLA với song song tensor. Trong chừng mực chúng tôi quan tâm đến việc tận dụng hiệu quả của chú ý tuyến tính, sẽ thú vị để áp dụng GLA cho các phương thức khác (đặc biệt là các phương thức với phụ thuộc tầm xa), phù hợp với công trình gần đây về việc áp dụng các mô hình không gian trạng thái hiện đại cho các loại dữ liệu khác (Yan et al., 2023; Zhu et al., 2024; Ma et al., 2024; Liu et al., 2024; Xing et al., 2024; Wang et al., 2024a;b; Yang et al., 2024, cùng nhiều công trình khác).

6 Công Trình Liên Quan
Chúng tôi thảo luận ngắn gọn công trình liên quan ở đây và đưa ra một thảo luận mở rộng về công trình liên quan trong Phụ lục A.

RNN truyền thống khó mở rộng do các phụ thuộc phi tuyến giữa các trạng thái ẩn và các cập nhật trạng thái ẩn tuần tự dựa trên matmul đắt đỏ. RNN/Mô hình Không gian Trạng thái (SSM)/Transformers Tuyến tính loại bỏ các phụ thuộc phi tuyến, làm cho huấn luyện có thể song song hóa dọc theo chiều thời gian (Martin & Cundy, 2018; Gu et al., 2022; Smith et al., 2023). Các mô hình như vậy đã là trọng tâm của nhiều công trình gần đây như một sự thay thế dưới bậc hai cạnh tranh cho kiến trúc Transformer (Peng et al., 2023; Gu & Dao, 2023; Qin et al., 2023c;b; Sun et al., 2023a; Wang et al., 2022).

Tỷ lệ suy giảm phụ thuộc dữ liệu luôn được coi là quan trọng cho RNN (Gers et al., 2000; van der Westhuizen & Lasenby, 2018). Các giá trị cổng quên điển hình phụ thuộc vào cả trạng thái ẩn trước và đầu vào hiện tại. Tuy nhiên Martin & Cundy (2018) đề xuất rằng các giá trị cổng quên chỉ nên phụ thuộc vào các đầu vào hiện tại để cho phép huấn luyện song song. Chiến lược đơn giản này đã được chứng minh hiệu quả trong các thí nghiệm quy mô vừa phải được tiến hành bởi HGRN (Qin et al., 2023b). RWKV-v6 (Peng et al., 2024) và Mamba (Gu & Dao, 2023) cũng sử dụng các tỷ lệ suy giảm phụ thuộc dữ liệu gợi nhớ đến các cổng quên. Trong bối cảnh Transformers tuyến tính, Peng et al. (2021) sử dụng một cổng quên theo vị trí thô, trong khi Mao (2022) và Katsch (2023) sử dụng một cổng quên tinh tế hơn.

RNN dựa vào các trạng thái ẩn có chiều cố định để mã hóa toàn bộ lịch sử của chúng. Chiều trạng thái ẩn đóng vai trò như một proxy cho dung lượng bộ nhớ và do đó ảnh hưởng đáng kể đến sức mạnh biểu cảm của chúng. Transformers Tuyến tính mở rộng chiều ẩn của RNN qua tham số hóa tích ngoài, như đã thảo luận trong §2.1. Mặt khác, SSM tuyến tính mở rộng chiều ẩn của chúng qua một chiến lược đầu vào đơn-đầu ra đơn (SISO). Không có các tham số SSM phụ thuộc dữ liệu, điều này có thể được thực hiện hiệu quả trong quá trình huấn luyện qua Biến đổi Fourier Nhanh (FFT). Tuy nhiên, với các tham số SSM phụ thuộc dữ liệu, huấn luyện dựa trên FFT không khả thi, và do đó Gu & Dao (2023) triển khai một kernel CUDA tùy chỉnh để huấn luyện một mô hình không gian trạng thái chọn lọc sử dụng thuật toán quét song song (Smith et al., 2023). Để phù hợp với tất cả các trạng thái ẩn vào SRAM, họ chỉ có thể đủ khả năng một tỷ lệ mở rộng lên đến 16. Ngược lại, thuật toán huấn luyện nhận thức phần cứng của chúng tôi cung cấp một cách tiếp cận thay thế, hiệu quả để mở rộng chiều ẩn đến một phạm vi rộng hơn, điều mà chúng tôi đã chỉ ra hữu ích trong các nhiệm vụ chuyên sâu về nhớ lại.

7 Kết Luận
Chúng tôi đề xuất một thuật toán hiệu quả để huấn luyện Transformers chú ý tuyến tính với các cơ chế cổng phụ thuộc dữ liệu. Thuật toán của chúng tôi làm cho có thể cân bằng FLOP với tính song song, trong khi vẫn cho phép sử dụng matmul nửa độ chính xác có thể tận dụng các đơn vị tensor core trên GPU hiện đại. Các thí nghiệm về mô hình hóa ngôn ngữ chứng minh rằng các Transformers chú ý tuyến tính có cổng có thể hoạt động đáng kính so với các baseline mạnh.

Tuyên Bố Tác Động
Bài báo này nhằm cải thiện hiệu quả huấn luyện của một họ mô hình mới của các mô hình chú ý tuyến tính (có cổng). Lợi thế hiệu quả của các mô hình như vậy có thể giúp dân chủ hóa quyền truy cập của các mô hình ngôn ngữ. Mặt khác, liệu các kiến trúc mới như vậy có ảnh hưởng đến các vấn đề đã biết như đầu ra thiên lệch và có hại của các mô hình ngôn ngữ vẫn là một câu hỏi nghiên cứu chưa được khám phá.

Lời Cảm Ơn
Công trình này được hỗ trợ bởi MIT-IBM Watson AI Lab. Chúng tôi cảm ơn Yutao Sun, Zhen Qin, Li Dong, Xinyu Yang, Jiacheng You, Huanqi Cao, Yu Zhang, và Shida Wang cho những thảo luận sâu sắc của họ. Chúng tôi cũng cảm ơn Yu Zhang, Fares Obeid, Daniel Goldstein, và Liliang Ren cho việc hiệu đọa của họ. Cảm ơn đặc biệt đến Yu Zhang đã đóng góp vào thư viện FLASH LINEAR ATTENTION.
