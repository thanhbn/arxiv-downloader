# 2305.19370.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2305.19370.pdf
# Kích thước tệp: 752875 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformer Song Song Theo Khối
cho Các Mô hình Ngữ cảnh Lớn
Hao Liu
UC Berkeley
hao.liu@cs.berkeley.eduPieter Abbeel
UC Berkeley
pabbeel@cs.berkeley.edu
Tóm tắt
Transformer đã trở thành nền tảng của các mô hình xử lý ngôn ngữ tự nhiên tiên tiến nhất, thể hiện hiệu suất xuất sắc trên một loạt các ứng dụng AI. Tuy nhiên, nhu cầu bộ nhớ do cơ chế tự chú ý và mạng feedforward lớn trong Transformer đặt ra hạn chế khả năng xử lý các chuỗi dài, từ đó tạo ra thách thức cho các tác vụ liên quan đến nhiều chuỗi dài hoặc phụ thuộc dài hạn. Chúng tôi trình bày một phương pháp tiếp cận riêng biệt, Transformer Song Song Theo Khối (BPT), tận dụng tính toán theo khối của tự chú ý và việc kết hợp mạng feedforward để giảm thiểu chi phí bộ nhớ. Bằng cách xử lý các chuỗi đầu vào dài hơn trong khi duy trì hiệu quả bộ nhớ, BPT cho phép huấn luyện các chuỗi dài gấp 32 lần so với Transformer vanilla và lên đến 4 lần so với các phương pháp tiết kiệm bộ nhớ trước đó. Các thí nghiệm mở rộng về mô hình hóa ngôn ngữ và nhiệm vụ học tăng cường chứng minh hiệu quả của BPT trong việc giảm yêu cầu bộ nhớ và cải thiện hiệu suất.

1 Giới thiệu
Transformer [52] đã trở thành xương sống của nhiều mô hình xử lý ngôn ngữ tự nhiên tiên tiến nhất [15,43,5,35]. Chúng đã thể hiện hiệu suất ấn tượng trên một loạt các bài toán AI, bao gồm mô hình hóa ngôn ngữ, dịch máy, chú thích hình ảnh, và gấp protein [39,47,32,43,5,45,9]. Transformer đạt được thành công này thông qua thiết kế kiến trúc sử dụng cơ chế tự chú ý và feedforward theo vị trí. Các thành phần này tạo điều kiện cho việc nắm bắt hiệu quả các phụ thuộc tầm xa giữa các token đầu vào, cho phép khả năng mở rộng về độ dài ngữ cảnh và kích thước mô hình thông qua các tính toán song song cao.

Tuy nhiên, yêu cầu bộ nhớ của Transformer hạn chế khả năng xử lý các chuỗi dài, điều này cần thiết cho nhiều bài toán AI, như hình ảnh độ phân giải cao, podcast, mã, hoặc sách và đặc biệt là những tác vụ liên quan đến nhiều chuỗi dài hoặc phụ thuộc dài hạn [10,7,39,7,34,29,47,32,1]. Cơ chế tự chú ý bậc hai và mạng feedforward lớn của Transformer yêu cầu một lượng lớn bộ nhớ, điều này làm cho việc mở rộng đến các chuỗi đầu vào dài hơn trở nên thách thức. Hạn chế này đã dẫn đến nhiều kỹ thuật được đề xuất để giảm yêu cầu bộ nhớ của Transformer, bao gồm xấp xỉ thưa thớt, xấp xỉ hạng thấp, và xấp xỉ độ chính xác thấp [xem ví dụ 51, 24, 22, 11, 25, 36, 54].

Một hướng nghiên cứu riêng biệt không dựa vào xấp xỉ mà thay vào đó tập trung vào tính toán tự chú ý chính xác với độ phức tạp bộ nhớ tuyến tính. Phương pháp này tận dụng quan sát rằng ma trận softmax trong tự chú ý có thể được tính toán mà không cần cụ thể hóa toàn bộ ma trận [37]. Kỹ thuật này đã dẫn đến sự phát triển của FlashAttention [14] và Memory Efficient Attention [42]. Cả hai phương pháp đều đề xuất tính toán theo khối của softmax tự chú ý, chứng minh việc giảm yêu cầu bộ nhớ.

Bản thảo.arXiv:2305.19370v3 [cs.CL] 28 Tháng 8 2023

--- TRANG 2 ---
Hình 1: Độ dài ngữ cảnh tối đa trong thời gian huấn luyện với mô hình GPT sử dụng các phương pháp khác nhau. Kích thước mô hình từ 1B đến 70B. Hình (A), (B), và (C) hiển thị đánh giá sử dụng một, tám A100, và 64 TPUv4, tương ứng, với một chuỗi duy nhất. Phương pháp của chúng tôi cho phép huấn luyện các chuỗi dài gấp 32 lần so với Transformer dựa trên attention vanilla [52], và 2 đến 4 lần so với FlashAttention [14] và Memory Efficient Attention [42]. Mục 3.1 cung cấp phân tích chi phí bộ nhớ chi tiết.

Mặc dù đã giảm được yêu cầu bộ nhớ của khối tự chú ý trong các mô hình Transformer, một thách thức đáng kể vẫn phát sinh từ mạng feedforward. Mạng này chứa một số lượng lớn tham số và tạo ra các vector trung gian chiều cao, dẫn đến yêu cầu bộ nhớ đáng kể. Vấn đề này trở thành thách thức bộ nhớ chính khi sử dụng các cơ chế chú ý tiết kiệm bộ nhớ. Do đó, việc huấn luyện Transformer trên độ dài ngữ cảnh dài hơn và mở rộng các mô hình Transformer trở nên bị cản trở đáng kể do nhu cầu bộ nhớ áp đảo được áp đặt bởi mạng feedforward.

Để giải quyết thách thức này, chúng tôi đưa ra một quan sát quan trọng: khi tự chú ý được tính theo cách chia khối để giảm yêu cầu bộ nhớ, việc kết hợp tính toán của mạng feedforward trở nên khả thi. Điều này loại bỏ nhu cầu phải chờ tính toán tự chú ý hoàn thành trước khi thực hiện bước feedforward trên toàn bộ chuỗi. Bằng cách tính toán mạng feedforward trên cơ sở từng khối, chúng tôi hiệu quả giảm chi phí bộ nhớ liên quan đến mạng feedforward. Quá trình này bao gồm việc sử dụng hai vòng lặp lồng nhau trên các khối chuỗi đầu vào. Trong vòng lặp ngoài, chúng tôi lặp qua mỗi khối và tính query. Trong vòng lặp trong, chúng tôi lặp qua mỗi khối để tính key và value. Các cặp key-value này, cùng với query, sau đó được sử dụng để tính attention theo khối cụ thể cho khối đầu vào tương ứng. Attention theo khối này tiếp theo được sử dụng để tính đầu ra của mạng feedforward, theo sau bởi một kết nối residual. Phương pháp này cho phép chúng tôi xử lý các chuỗi đầu vào dài hơn trong khi duy trì ngân sách bộ nhớ thấp hơn. Vì phương pháp của chúng tôi thực hiện tính toán song song theo khối và kết hợp các tính toán feedforward và tự chú ý, chúng tôi đặt tên phương pháp là Transformer Song Song Theo Khối (BPT).

Chúng tôi đánh giá hiệu quả của phương pháp trên một số benchmark, bao gồm mô hình hóa ngôn ngữ và học tăng cường. Thí nghiệm của chúng tôi cho thấy BPT có thể giảm yêu cầu bộ nhớ của Transformer, cho phép chúng tôi huấn luyện chuỗi dài gấp 32 lần so với mô hình GPT dựa trên attention vanilla [52] và lên đến 4 lần so với các phương pháp tiên tiến trước đó FlashAttention [14] và Memory Efficient Attention [42]. Hơn nữa, chúng tôi chứng minh ứng dụng của BPT trên nhiệm vụ huấn luyện agent RL dựa trên Transformer. Bằng cách điều kiện hóa trên nhiều quỹ đạo, BPT cải thiện đáng kể hiệu suất và đạt được kết quả tốt hơn trên các benchmark RL thách thức. Chúng tôi tin rằng phương pháp của chúng tôi có tiềm năng cho phép huấn luyện và đánh giá các mô hình phức tạp hơn yêu cầu các chuỗi đầu vào dài hơn, điều này có thể dẫn đến những đột phá tiếp theo trong nghiên cứu AI.

Đóng góp của chúng tôi gồm hai phần: (a) đề xuất phương pháp tính toán theo khối của tự chú ý và feedforward cho phép độ dài ngữ cảnh dài gấp 32 lần và lên đến 4 lần so với Transformer vanilla và Transformer tiết kiệm bộ nhớ trước đó, tương ứng, và (b) chứng minh hiệu quả của phương pháp thông qua các thí nghiệm mở rộng.

2 Nút thắt Bộ nhớ của Transformer
Cho các chuỗi đầu vào Q, K, V ∈Rs×d trong đó s là độ dài chuỗi và d là chiều head. Chúng tôi tính ma trận đầu ra như:
Attention(Q, K, V) = softmax(QKT/√d)V, (1)

--- TRANG 3 ---
trong đó softmax được áp dụng theo hàng. Các triển khai attention tiêu chuẩn cụ thể hóa các ma trận QKT và softmax(QKT/√d) đến HBM, điều này chiếm bộ nhớ O(s²), do đó độ phức tạp không gian tổng thể là O(s²). Đã có một lượng lớn công trình cố gắng giảm việc sử dụng bộ nhớ của tự chú ý bằng cách sử dụng softmax trực tuyến [37,42,14] để giảm chi phí bộ nhớ của tự chú ý bằng cách ngăn không cho nó cụ thể hóa đầy đủ. Và các phương pháp này giảm dấu chân bộ nhớ từ O(s²) xuống O(s). Tuy nhiên, các lớp feedforward lớn đã bị bỏ qua.

Ngoài các lớp con attention, mỗi lớp attention được thực hiện với một mạng feedforward kết nối đầy đủ, được áp dụng cho mỗi vị trí một cách riêng biệt và giống hệt nhau. Điều này bao gồm hai phép biến đổi tuyến tính với một kích hoạt ReLU ở giữa.

FFN(x) = max(0, xW₁+b₁)W₂+b₂ (2)

Trong khi các phép biến đổi tuyến tính giống nhau qua các vị trí khác nhau, chúng sử dụng các tham số khác nhau từ lớp này sang lớp khác. Kích thước lớn của mạng feedforward yêu cầu tài nguyên bộ nhớ đáng kể, và điều này trở nên rõ rệt hơn khi xử lý kích thước ngữ cảnh lớn. Xem Mục 3.1 để phân tích chi phí bộ nhớ liên quan đến transformer.

3 Song Song Theo Khối cho Các Mô hình Ngữ cảnh Lớn
Tự chú ý có thể được tính theo cách chia khối mà không cần cụ thể hóa ma trận attention softmax softmax(QKT)[37,14,42]. Phương pháp này bao gồm việc chia các chuỗi Q∈Rs×d thành Bq khối và các chuỗi K, V∈Rs×d thành Bkv khối. Đối với mỗi khối query, attention theo khối Attention(Q, K, V) có thể được tính bằng cách lặp qua tất cả các khối key-value. Khi attention theo khối được tính, ma trận attention toàn cục có thể được thu được bằng cách chia tỷ lệ attention theo khối sử dụng sự khác biệt giữa các hằng số chuẩn hóa softmax theo khối và toàn cục [37]. Điều này đạt được bằng cách theo dõi thống kê chuẩn hóa và kết hợp chúng từ tất cả các khối để chia tỷ lệ mỗi khối tương ứng. Đối với một khối query cụ thể Qi, 1≤i≤Bq, đầu ra attention tương ứng có thể được tính bằng cách chia tỷ lệ mỗi attention theo khối như sau:

Attention(Qi, K, V) = Scaling({exp(QiKTj)Vj}ʲ⁼¹ᴮᵏᵛ). (3)

Phép toán chia tỷ lệ chia tỷ lệ mỗi attention theo khối dựa trên sự khác biệt giữa cực đại theo khối và cực đại toàn cục:

Attention(Qi, Kj, Vj) = exp(QiKTj−max(QiKTj))/∑exp(QiKTj−max(QiKTj))

maxi = max(max(QiKT1), . . . , max(QiKTB))

Attention(Qi, K, V) = ∑ʲ⁼¹ᴮᵏᵛ exp(QiKTj−maxi) Attention(Qi, Kj, Vj).

Tính toán tự chú ý theo khối này loại bỏ nhu cầu cụ thể hóa ma trận attention đầy đủ có kích thước O(n²), dẫn đến tiết kiệm bộ nhớ đáng kể.

Chúng tôi quan sát rằng tính toán theo khối không giới hạn ở tự chú ý mà còn có thể được áp dụng cho mạng feedforward. Đối với mỗi khối query, sau khi lặp qua các khối key và value, mạng feedforward có thể được tính cùng với một kết nối residual, hoàn thành tính toán attention và mạng feedforward cho khối query đó. Điều này có nghĩa là mô hình không cần tính mạng feedforward trên toàn bộ chuỗi, mà thay vào đó trên các khối trung gian, dẫn đến tiết kiệm bộ nhớ. Tính toán cho một khối query được cho bởi:

Outputi = FFN(Attention(Qi, K, V) + Qi) + Attention(Qi, K, V) + Qi.

Do đó, đầu ra cho mỗi khối bao gồm mạng feedforward, tự chú ý, và kết nối residual được tính theo cách chia khối.

Đáng chú ý rằng đối với các mô hình lớn, chi phí bộ nhớ của mạng feedforward trên toàn bộ chuỗi có thể lớn hơn nhiều so với attention tiết kiệm bộ nhớ. Do đó, tính toán mạng feedforward trên cùng khối với attention có thể giảm đáng kể chi phí bộ nhớ, và nó cũng giảm di chuyển dữ liệu, góp phần vào hiệu quả tính toán tổng thể. Hơn nữa, chúng tôi nên nhận xét rằng song song theo khối có thể được áp dụng trực tiếp cho mất mát cross entropy cuối cùng, điều này có thể giảm thiểu thêm chi phí bộ nhớ. Quá trình đầy đủ của khung của chúng tôi, được gọi là BPT, được tóm tắt trong Thuật toán 1.

--- TRANG 4 ---
Hình 2: Chúng tôi sử dụng cùng kiến trúc mô hình như Transformer gốc nhưng với cách tổ chức tính toán khác. Trong sơ đồ, chúng tôi giải thích điều này bằng cách hiển thị rằng đối với khối đầu vào đầu tiên ở dưới cùng, chúng tôi chiếu nó thành query; sau đó chúng tôi lặp qua cùng chuỗi đầu vào được đặt phía trên hàng dưới cùng, và chiếu nó thành key và value. Các query, key và value này được sử dụng để tính tự chú ý (hộp màu vàng), đầu ra của nó được chuyển đến mạng feedforward (hộp màu cyan), theo sau bởi một kết nối residual. Trong phương pháp đề xuất của chúng tôi, quá trình này sau đó được lặp lại cho các khối đầu vào khác.

3.1 Chi phí Bộ nhớ
Chúng tôi trình bày phân tích chi phí bộ nhớ qua các kiến trúc transformer khác nhau: Transformer Vanilla, biến thể Memory-efficient / Flash Attention, và BPT.

Transformer Vanilla:
Attention: Đối với Q,K,V, lưu đầu vào x của chúng cần 2bsh byte, trong đó b là kích thước batch, s là độ dài chuỗi, và h là chiều ẩn. Đối với matmul QKT, lưu các activation Q và K cần 4bsh byte. Đối với softmax(QKT), lưu đầu vào QKT cần 2bs²a byte, trong đó a là số head attention. Đối với mask và dropout, lưu mask cần bs²a byte. Đối với score × V, lưu score cần 2bs²a byte, và lưu V cần 2bsh byte. Đối với projection đầu ra và dropout, lưu đầu vào cần

--- TRANG 5 ---
Thuật toán 1 Giảm chi phí bộ nhớ với BPT.
Yêu cầu: Chuỗi đầu vào x. Số khối query Bq. Số khối key và value Bkv.
Khởi tạo
Chiếu chuỗi đầu vào x thành query, key và value.
Chia chuỗi query thành Bq khối đầu vào query.
Chia chuỗi key và value thành Bkv khối đầu vào key-value.
for outer = 1 to Bq do
    Chọn query thứ outer.
    for inner = 1 to Bkv do
        Chọn khối key thứ inner và khối value thứ inner.
        Tính attention sử dụng query, key và value, và ghi lại thống kê chuẩn hóa.
    end for
    Kết hợp mỗi khối bằng cách chia tỷ lệ chúng để có đầu ra attention cho khối đầu vào thứ outer.
    Tính feedforward trên đầu ra attention và thêm kết nối residual.
end for

2bsh byte, và lưu mask dropout cần bsh byte. Kích thước activation attention tối đa là O(s²) với checkpointing.

FFN: Đối với lớp tuyến tính đầu tiên, lưu đầu vào cần 2bsh byte. Đối với activation, lưu đầu vào cần 8bsh byte. Đối với lớp tuyến tính thứ hai, lưu đầu vào cần 8bsh byte. Đối với dropout, lưu mask cần bsh byte. Với checkpointing, kích thước activation tối đa của FFN là 8bsh byte.

Do đó, đối với độ dài ngữ cảnh lớn, chi phí bộ nhớ của activation trong Transformer vanilla là O(s²).

BPT:
Attention: Vì BPT không cụ thể hóa attention đầy đủ mà thay vào đó tính nó theo khối, nó cần lưu các activation theo khối trung gian trong vòng lặp key-value, có kích thước activation tối đa là 4bch với checkpointing. Ngoài ra, nó cần lưu q activation đầu ra cho vòng lặp query, yêu cầu 2bsh byte. Vì s≫c, kích thước activation tối đa là 2bsh.

FFN: Khi lặp FFN qua các khối, BPT cần lưu các activation sau: Đối với lớp tuyến tính đầu tiên, lưu đầu vào cần 2bch byte. Đối với activation, lưu đầu vào cần 8bch byte. Đối với lớp tuyến tính thứ hai, lưu đầu vào cần 8bch byte. Đối với dropout, lưu mask cần bch byte. Tổng cộng, cần 19bch byte. Ngoài ra, lưu đầu ra của vòng lặp for yêu cầu 2bsh byte. Do đó, kích thước activation FFN tối đa là 2bsh.

Do đó, chi phí bộ nhớ activation của mỗi lớp BPT là 2bsh.

Memory-Efficient / Flash Attention:
Attention: Tương tự như attention BPT, kích thước activation tối đa là 2bsh.
FFN: Tương tự như FFN vanilla, kích thước activation tối đa là 8bsh.

Do đó, chi phí bộ nhớ của mỗi lớp Flash Attention là 8bsh.

So sánh bộ nhớ activation của Flash Attention/Memory-Efficient Transformer với BPT, chúng ta thấy BPT cung cấp tiết kiệm bộ nhớ 8bsh/2bsh = 4 lần. Bằng cách tính đến các yếu tố khác của chi phí bộ nhớ như tham số mô hình và trạng thái optimizer, BPT cho phép huấn luyện với độ dài ngữ cảnh lớn hơn 2-4 lần so với các phương pháp tiên tiến trước đó.

3.2 Tại sao Song song Theo Khối
Việc sử dụng song song hóa theo khối có thể đặt ra câu hỏi về hiệu quả của việc chạy máy tính song song, vì tính toán có thể trở nên tuần tự giữa các khối. Tuy nhiên, lợi ích của song song hóa theo khối phụ thuộc vào kích thước mô hình và cấu hình phần cứng. Trong các trường hợp mô hình lớn hoặc độ dài ngữ cảnh cực kỳ dài, một khối có thể đạt mật độ số học tối đa, làm cho việc thực thi chuỗi độ dài đầy đủ gốc song song trở nên không thực tế. Trong các tình huống như vậy, song song hóa theo khối xử lý chuỗi dài như chuỗi ngắn, cho phép xử lý các mô hình lớn và hiệu quả cho phép kích thước ngữ cảnh lớn. Hơn nữa, sử dụng song song hóa theo khối cho phép chúng ta tránh chờ hoàn thành tự chú ý và phân bổ một lượng lớn bộ nhớ chỉ cho tính toán mạng feedforward.

Một lợi thế đáng chú ý khác của song song hóa theo khối là khả năng tận dụng phần cứng với tốc độ SRAM nhanh hơn đáng kể so với tốc độ HBM. Ví dụ, trong GPU Nvidia, SRAM nhanh hơn một bậc độ lớn so với HBM, trong khi ở Google TPU, SRAM cũng cung cấp tốc độ cao hơn so với HBM. Bằng cách sử dụng song song hóa theo khối, chúng ta có thể tận dụng tốc độ tăng của SRAM, do đó giảm chi phí giao tiếp và tăng thông lượng. Lợi thế này phù hợp với các phương pháp tự chú ý tiết kiệm bộ nhớ [14, 42].

3.3 Triển khai
Thuật toán 1 cung cấp mã giả của thuật toán. Hình 3 trong Phụ lục hiển thị triển khai Jax được tối ưu hóa cho sự đơn giản. Mã đầy đủ của BPT được cung cấp tại GitHub¹ hỗ trợ huấn luyện phân tán quy mô lớn của các mô hình ngữ cảnh lớn sử dụng BPT.

Hàm blockwise_ffn bắt đầu bằng cách chấp nhận một mô-đun feedforward được tái vật chất hóa, đầu vào và kích thước chunk. remat_ffn tính feedforward trên đầu vào với checkpointing, tức là không lưu các trung gian. Hàm scan_ffn sau đó được sử dụng để quét qua các chuỗi đầu vào và tạo đầu ra.

Hàm blockwise_attn xử lý query, key, và value để tạo ra attention theo khối. Hàm scan_attention được định nghĩa, tính trọng số attention giữa vector query và các cặp key-value từ chunk khác. Điều này được thực hiện bằng cách áp dụng hàm scan_kv_block vào chunk key-value, tính tích vô hướng giữa các vector query và key, và sau đó thêm một số hạng bias. Số hạng bias giới thiệu bias vị trí giữa các chunk khác nhau dựa trên chỉ số của chúng mà không cụ thể hóa ma trận đầy đủ. Hàm softmax sau đó được áp dụng cho trọng số attention theo cách ổn định số, sử dụng thủ thuật max-score để tránh kết quả mũ lớn.

Cuối cùng, BPT kết hợp đầu ra từ tất cả các chunk, chuẩn hóa chúng sử dụng trọng số được điều chỉnh max-score, và chuyển chúng qua mạng nơ-ron feedforward (blockwise_ffn). Đầu ra cuối cùng là tổng của đầu ra feedforward, đầu ra attention, và đầu vào gốc.

4 Thiết lập
Chúng tôi đánh giá tác động của việc sử dụng BPT trong việc cải thiện các mô hình Transformer lớn bằng cách đo điểm chuẩn yêu cầu bộ nhớ, độ dài chuỗi tối đa và tốc độ thông lượng. Chúng tôi hiển thị việc áp dụng BPT cho học tăng cường như một ứng dụng.

Cấu hình Mô hình. Nghiên cứu của chúng tôi được xây dựng dựa trên kiến trúc GPT. Bảng 1 cung cấp tổng quan về các kích thước mô hình được xem xét trong thí nghiệm của chúng tôi.

Baseline. Chúng tôi đánh giá phương pháp của mình bằng cách so sánh với Transformer vanilla [52] được ký hiệu là "Vanilla", và FlashAttention [14] và Memory Efficient Attention [42] là attention tiết kiệm bộ nhớ tiên tiến nhất, chúng tôi ký hiệu chúng là "MemoryEfficient" trong thí nghiệm. Tất cả các phương pháp sử dụng cùng gradient checkpointing trong thí nghiệm.

Tập dữ liệu. Chúng tôi xem xét hai tập dữ liệu để đánh giá. Bao gồm pretraining trên tập dữ liệu OpenWebText và học tăng cường ngữ cảnh lớn trên ExoRL.

•OpenWebText. Tập dữ liệu OpenWebText [18] là một bộ sưu tập lớn và đa dạng các trang web đã được lọc và làm sạch để sử dụng trong các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP). Tập dữ liệu bao gồm hơn 6 tỷ token từ hơn 40 triệu trang web, bao phủ một loạt các chủ đề và thể loại.

•ExoRL. Tập dữ liệu ExoRL [56] dựa trên dữ liệu khám phá không nhãn được thu thập bằng cách chạy các thuật toán RL không giám sát. Đối với mỗi môi trường, nó đi kèm với tám thuật toán thu thập dữ liệu không giám sát khác nhau, lấy từ URLB [28]. Các tập dữ liệu được thu thập bởi RL không giám sát và sau đó được gán nhãn lại sử dụng hàm reward nhiệm vụ. Tập dữ liệu hỗn hợp kết quả bao gồm 8 triệu timestep (8000 episode), với mỗi episode kéo dài 1000 bước.

¹https://github.com/lhao499/llm_large_context

--- TRANG 6 ---
Bảng 1: Kích thước và kiến trúc của các mô hình mà chúng tôi đánh giá trong thí nghiệm.
Tên Mô hình | nparams | nlayers | dmodel | nheads | dhead
GPT 1B     | 1.3B    | 24      | 2048   | 16     | 128
GPT 3B     | 2.7B    | 32      | 2560   | 32     | 80
GPT 7B     | 6.7B    | 32      | 4096   | 32     | 128
GPT 13B    | 13.0B   | 40      | 5140   | 40     | 128
GPT 30B    | 30.0B   | 48      | 7168   | 56     | 128
GPT 70B    | 70.0B   | 80      | 8192   | 64     | 128

Cấu hình Huấn luyện. Baseline chính của chúng tôi là vanilla attention [52], tính tự chú ý bằng cách cụ thể hóa ma trận attention và tính mạng feedforward bình thường. Chúng tôi cũng xem xét hai phương pháp tiết kiệm bộ nhớ tiên tiến trước đó, cụ thể là FlashAttention [14], tập trung vào hiệu quả GPU, và Memory Efficient Attention [42], tập trung vào hiệu quả TPU. Vì chúng chia sẻ ý tưởng tương tự, để đơn giản ký hiệu, chúng tôi gọi chúng là FlashAttention trong thí nghiệm. Chúng tôi điều chỉnh kích thước khối cho cả baseline và BPT, và báo cáo kết quả tốt nhất đạt được bởi mỗi phương pháp. Thí nghiệm trên GPU NVIDIA 80GB A100, chúng tôi xem xét cả GPU đơn cho huấn luyện mô hình nhỏ hơn và thiết lập 8 GPU cho huấn luyện song song mô hình. Chúng tôi cũng thí nghiệm với việc mở rộng mô hình trên 64 TPUv4.

Chúng tôi lưu ý rằng không có song song dữ liệu nào được xem xét trong đánh giá của chúng tôi vì phương pháp của chúng tôi độc lập với song song dữ liệu. Do đó, kích thước batch được sử dụng trong phân tích của chúng tôi thấp hơn nhiều so với những cái được sử dụng cho huấn luyện đầu cuối. Tất cả kết quả của chúng tôi được thu thập sử dụng độ chính xác đầy đủ thay vì độ chính xác hỗn hợp.

5 Kết quả
Trong thí nghiệm của chúng tôi, mục tiêu chính là đánh giá toàn diện hiệu suất của BPT qua nhiều chỉ số chính, bao gồm độ dài chuỗi tối đa, sử dụng bộ nhớ, và thông lượng. Hơn nữa, chúng tôi mở rộng khả năng áp dụng của BPT cho học tăng cường và đánh giá hiệu quả của nó trong ứng dụng ngữ cảnh lớn.

Bảng 2: Độ dài ngữ cảnh tối đa trong huấn luyện với các phương pháp khác nhau. BPT cho phép huấn luyện độ dài chuỗi dài hơn 2-4 lần so với FlashAttention / Memory Efficient Attention, và lên đến 32 lần so với vanilla attention.

1 A100     | PartitionSpec | Vanilla Attention | MemoryEfficient | Blockwise Parallel
350M       | (1,1,1)       | 16K (16384)      | 65K (65536)     | 131K (131072)
1B         | (1,1,1)       | 16K (16384)      | 65K (65536)     | 131K (131072)
3B         | (1,1,1)       | 8K (8192)        | 16K (16384)     | 65K (65536)

8 A100     | PartitionSpec | Vanilla Attention | MemoryEfficient | Blockwise Parallel
3B         | (1,1,8)       | 16K (16384)      | 65K (65536)     | 131K (131072)
7B         | (1,1,8)       | 16K (16384)      | 65K (65536)     | 131K (131072)
13B        | (1,1,8)       | 8K (8192)        | 33K (32768)     | 65K (65536)
30B        | (1,1,8)       | 8K (8192)        | 16K (16384)     | 65K (65536)

64 TPUv4   | PartitionSpec | Vanilla Attention | MemoryEfficient | Blockwise Parallel
13B        | (1,1,64)      | 4K (4096)        | 16K (16384)     | 33K (32768)
30B        | (1,1,64)      | 2K (2048)        | 4K (4096)       | 16K (16384)
70B        | (1,1,64)      | 1k (1024)        | 2K (2048)       | 8K (8192)

5.1 Đánh giá Độ dài Ngữ cảnh
Chúng tôi trình bày kết quả thí nghiệm so sánh độ dài chuỗi huấn luyện tối đa đạt được sử dụng ba cơ chế attention khác nhau: Vanilla, MemoryEfficient, và Blockwise Parallel. Bảng 2 tóm tắt các phát hiện. Trên một GPU A100, Transformer Vanilla hỗ trợ độ dài chuỗi huấn luyện tối đa là 16K cho 1B tham số và 8K cho 3B tham số. Ngược lại, MemoryEfficient cho phép các chuỗi dài hơn 65K cho 1B tham số và 16K cho 3B tham số. Đáng chú ý, phương pháp đề xuất của chúng tôi, Blockwise Parallel, vượt qua cả hai phương pháp, đạt độ dài chuỗi tối đa 131K cho 1B tham số và 3B tham số. Chuyển sang các mô hình lớn hơn, Blockwise Parallel một lần nữa vượt trội hơn hai phương pháp kia, cho phép huấn luyện các chuỗi 65K cho mô hình lớn 30B trên 8 GPU và 8K cho mô hình lớn 70B trên 64 TPUv4, lần lượt dài hơn hai và bốn lần so với MemoryEfficient.

Bảng 3 hiển thị phân tích sử dụng bộ nhớ qua các thiết lập khác nhau với ba phương pháp riêng biệt: Transformer Vanilla, MemoryEfficient, và phương pháp đề xuất của chúng tôi, BPT. Rõ ràng là Transformer Vanilla tiêu thụ lượng bộ nhớ cao nhất, trong khi MemoryEfficient và BPT cung cấp những cải thiện đáng chú ý trong tối ưu hóa bộ nhớ. Đáng chú ý, kỹ thuật BPT của chúng tôi luôn vượt trội hơn cả Transformer Vanilla và MemoryEfficient trong tất cả các thiết lập, thể hiện hiệu quả bộ nhớ.

Bảng 3: So sánh sử dụng bộ nhớ cho các thiết lập khác nhau. "oom" biểu thị hết bộ nhớ.

Thiết lập        | 3B trên A100          | 13B trên 8 A100
Độ dài Ngữ cảnh  | Vanilla | MemoryEfficient | BPT | Vanilla | MemoryEfficient | BPT
8192             | 64GB    | 44GB           | 43GB| 59GB    | 44GB           | 42GB
16384            | oom     | 47GB           | 45GB| oom     | 46GB           | 45GB
32768            | oom     | 55GB           | 52GB| oom     | 55GB           | 52GB
65536            | oom     | 75GB           | 70GB| oom     | 75GB           | 68GB
131072           | oom     | oom            | 79GB| oom     | oom            | 78GB

5.2 Đánh giá về Thông lượng và Tốc độ
Trong Bảng 4, chúng tôi trình bày so sánh thông lượng đạt được bởi các cơ chế attention khác nhau trên mô hình GPT-XL (1B) được huấn luyện trên tập dữ liệu OpenWebText sử dụng 8 GPU. Thông lượng được đo bằng số token được xử lý trên mỗi thiết bị mỗi giây. Chúng tôi đánh giá hiệu suất ở các độ dài ngữ cảnh khác nhau, bao gồm 2K, 8K, 16K, 33K, và 65K token. Phương pháp đề xuất của chúng tôi đạt thông lượng cạnh tranh với cơ chế MemoryEfficient, và vượt qua transformer Vanilla, đạt được tăng tốc 1.17x ở độ dài ngữ cảnh 8k và tăng tốc 1.2x ở độ dài ngữ cảnh 16k. Ở độ dài ngữ cảnh 32K và 64K, phương pháp của chúng tôi duy trì thông lượng cao và tốc độ huấn luyện, trong khi các phương án khác không thể huấn luyện do hết bộ nhớ. Điều này chứng minh tính mở rộng và hiệu quả của phương pháp đề xuất, cho phép nó xử lý hiệu quả độ dài ngữ cảnh lớn mà không ảnh hưởng đến thông lượng và tốc độ huấn luyện.

5.3 Đánh giá về Học Tăng cường
Trong mục này, chúng tôi trình bày kết quả áp dụng BPT để cải thiện hiệu suất của Transformer trong học tăng cường (RL). Chúng tôi báo cáo kết quả trong Bảng 5, nơi chúng tôi đánh giá mô hình đề xuất trên benchmark ExoRL qua sáu nhiệm vụ khác nhau. Trên ExoRL, chúng tôi báo cáo return tích lũy, theo ExoRL [56]. Các số của BC, DT [6] và AT [33] lấy từ bài báo ExoRL và AT. Các số AT + ME và AT + BPT được chạy bởi chính chúng tôi. Vì dữ liệu ExoRL đa dạng hơn đáng kể so với D4RL vì nó được thu thập sử dụng RL không giám sát [28], được phát hiện rằng học TD hoạt động tốt nhất trong khi behavior cloning gặp khó khăn [56]. AT [33] cho thấy rằng điều kiện hóa Transformer trên nhiều quỹ đạo với target return được gán nhãn lại có thể vượt trội đáng kể so với các phương pháp behavior cloning BC-10% và DT, và đạt kết quả cạnh tranh với học TD. Để biết thêm chi tiết, vui lòng tham khảo các bài báo của họ. Chúng tôi quan tâm đến việc áp dụng BPT để cải thiện hiệu suất của AT bằng cách điều kiện hóa trên 64 quỹ đạo thay vì 4 quỹ đạo trong công trình gốc. Đáng chú ý rằng mỗi quỹ đạo có độ dài 1000×4 trong đó 1000 là độ dài chuỗi trong khi 4 là return-state-action-reward, làm cho việc huấn luyện 64 quỹ đạo với mô hình kích thước 350M không khả thi cho cả Vanilla và MemoryEfficient. Kết quả trong Bảng 5 cho thấy rằng, bằng cách mở rộng độ dài chuỗi, AT + BPT luôn vượt trội hơn mô hình Transformer gốc trong tất cả sáu nhiệm vụ, đạt tổng return trung bình 155.36 so với tổng return trung bình 120.65 của mô hình Transformer gốc.

--- TRANG 8 ---
Bảng 4: So sánh thông lượng trên GPT-XL (1B) sử dụng tập dữ liệu OpenWebText. Thông lượng được đo bằng token được xử lý mỗi giây. 'oom' biểu thị hết bộ nhớ, 'na' biểu thị kết quả không có sẵn vì chúng tôi chấm dứt sớm các lần chạy này để giảm chi phí tính toán.

Mô hình | Độ dài Ngữ cảnh | Val Loss | Thông lượng | Tăng tốc
Vanila Transformer | 2048 | 2.46 | 3827 | 1x
MemoryEfficient | 2048 | 2.46 | 4371 | 1.14x
Blockwise Parallel | 2048 | 2.46 | 3985 | 1.04x

Vanila Transformer | 4096 | 2.44 | 2340 | 1x
MemoryEfficient | 4096 | 2.44 | 2567 | 1.1x
Blockwise Parallel | 4096 | 2.44 | 2687 | 1.15x

Vanila Transformer | 8192 | 2.43 | 2455 | 1x
MemoryEfficient | 8192 | 2.43 | 2781 | 1.13x
Blockwise Parallel | 8192 | 2.43 | 2875 | 1.17x

Vanila Transformer | 16384 | 2.41 | 1701 | 1x
MemoryEfficient | 16384 | 2.41 | 1889 | 1.11x
Blockwise Parallel | 16384 | 2.41 | 2045 | 1.2x

Vanila Transformer | 32768 | oom | oom | oom
MemoryEfficient | 32768 | na | 810 | 1x
Blockwise Parallel | 32768 | na | 857 | 1.1x

Vanila Transformer | 65536 | oom | oom | oom
MemoryEfficient | 65536 | oom | oom | oom
Blockwise Parallel | 65536 | na | 600 | 1x

Bảng 5: Ứng dụng của BPT trong việc cải thiện Transformer trong RL. Tất cả baseline sử dụng vanilla attention. AT + ME biểu thị sử dụng "MemoryEfficient". AT + BPT biểu thị sử dụng Blockwise Parallel.

ExoRL benchmark | BC-10% | DT | AT | AT | AT + ME | AT + BPT
Nhiệm vụ | | | N Trajs = 4 | N Trajs = 32 | N Trajs = 32 | N Trajs = 32
Walker Stand | 52.91 | 34.54 | 68.55 | oom | oom | 95.45
Walker Run | 34.81 | 49.82 | 88.56 | oom | oom | 105.88
Walker Walk | 13.53 | 34.94 | 64.56 | oom | oom | 78.56
Cheetah Run | 34.66 | 67.53 | 125.68 | oom | oom | 178.75
Jaco Reach | 23.95 | 18.64 | 52.98 | oom | oom | 87.56
Cartpole Swingup | 56.82 | 67.56 | 97.81 | oom | oom | 120.56
Tổng Trung bình | 36.11 | 45.51 | 83.02 | oom | oom | 111.13

6 Công trình Liên quan
Transformer đã thu hút sự chú ý đáng kể trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) và đã trở thành cơ sở cho nhiều mô hình tiên tiến nhất. Một số công trình đã khám phá các kỹ thuật tiết kiệm bộ nhớ để giải quyết các hạn chế bộ nhớ của Transformer và cho phép ứng dụng của chúng vào các chuỗi đầu vào dài hơn. Một hướng nghiên cứu tập trung vào các kỹ thuật xấp xỉ khác nhau hoặc nén theo chiều chuỗi [xem ví dụ 24,12,14,4,42,54,36,25]. Các công trình khác khám phá việc thay thế attention [19,20,41,23,3,57,40,53]. Một hướng nghiên cứu khác khám phá việc phân vùng chiều ẩn lớn của mạng feedforward thành các phần và chỉ truy xuất một phần cho mỗi token [30,48,17,26,58,60]. Ngoài ra, việc mở rộng ngữ cảnh bằng cách chú ý đến các trạng thái từ các chuỗi trước đó đã được khám phá [13,44], cũng như kết hợp ngữ cảnh cục bộ và toàn cục [21,11]. Để có đánh giá toàn diện về các kỹ thuật này, chúng tôi khuyến nghị tham khảo các khảo sát của Tay et al. [51], Narang et al. [38], Tay et al. [50]. Một số nghiên cứu khám phá việc chia sẻ mô hình lớn trên các thiết bị phân tán tensor, dữ liệu, hoặc song song chuỗi [49,16,55,27,59,31,46]. Chúng tôi chia sẻ điểm tương đồng với song song chuỗi [27] nơi các chuỗi được phân phối qua các thiết bị, ngược lại, chúng tôi triển khai tính toán theo khối trên các chuỗi cho mỗi thiết bị. Điều này tạo ra mối quan hệ trực giao giữa phương pháp của chúng tôi và song song chuỗi, cho phép kết hợp đơn giản. Ngoài ra, phương pháp của chúng tôi tương thích với cả song song tensor và dữ liệu. Một hướng khác

--- TRANG 9 ---
bao gồm tính toán tự chú ý chính xác theo cách chia khối sử dụng kỹ thuật tiling [37]. Phương pháp này đã dẫn đến sự phát triển của các cơ chế attention tiết kiệm bộ nhớ [14,42]. Phù hợp với những tiến bộ này, công trình của chúng tôi thuộc về danh mục này. Chúng tôi đề xuất tính toán cả mạng feedforward và tự chú ý theo cách chia khối, dẫn đến giảm đáng kể yêu cầu bộ nhớ.

7 Kết luận
Tóm lại, chúng tôi đề xuất phương pháp song song hóa theo khối để giảm yêu cầu bộ nhớ của Transformer, xương sống của các mô hình NLP tiên tiến nhất. Phương pháp của chúng tôi cho phép xử lý các chuỗi đầu vào dài hơn trong khi duy trì hoặc cải thiện hiệu suất. Thông qua các thí nghiệm mở rộng, chúng tôi chứng minh hiệu quả của nó, đạt được giảm bộ nhớ lên đến 4x so với Transformer tiết kiệm bộ nhớ. Đóng góp của chúng tôi bao gồm một phương pháp thực tế cho kích thước ngữ cảnh lớn trong các mô hình Transformer lớn. Với khả năng ngày càng tăng của phần cứng, các mô hình lớn hơn và độ dài ngữ cảnh dài hơn được sử dụng rộng rãi trong nghiên cứu AI. Cùng lúc đó, khi chúng ta đang đẩy lên giới hạn vật lý và chế tạo, việc thiết kế các phương pháp mở rộng hiệu quả nhất có thể để mở rộng các mô hình lớn và kích thước ngữ cảnh lớn trở nên quan trọng hơn. Phương pháp của chúng tôi hứa hẹn cho việc huấn luyện và đánh giá các mô hình phức tạp với các chuỗi đầu vào dài hơn, có khả năng thúc đẩy những đột phá mới trong nghiên cứu máy học.

Hạn chế và Công việc Tương lai. Mặc dù phương pháp của chúng tôi đạt được việc sử dụng bộ nhớ thấp tiên tiến nhất cho các mô hình Transformer, nó có một số hạn chế cần được giải quyết:

•Hiệu suất tối ưu. Trong khi triển khai của chúng tôi ưu tiên sự đơn giản với các hoạt động Jax cấp cao, việc tối ưu hóa các hoạt động cấp thấp là quan trọng để đạt hiệu suất tối ưu. Trong công việc tương lai, chúng tôi đề xuất xem xét việc chuyển phương pháp của chúng tôi sang CUDA và OpenAI Triton để đạt chi phí bộ nhớ tối thiểu và tăng tốc tối đa.

Lời cảm ơn
Dự án này được hỗ trợ một phần bởi ONR theo N00014-21-1-2769. Chúng tôi cảm ơn các thành viên của Berkeley Robot Learning Lab và Berkeley AI Lab cho các cuộc thảo luận có giá trị. Chúng tôi cảm ơn Tri Dao tại Stanford cho các cuộc thảo luận có giá trị về việc tăng cường BPT. Chúng tôi cảm ơn Google TPU Research Cloud đã cấp cho chúng tôi quyền truy cập vào TPU.

Chúng tôi cũng bày tỏ sự đánh giá cao đến Anselm Levskaya, Markus Rabe, Federico Lebron, và Sharad Vikram tại Google cho các cuộc thảo luận sâu sắc và đề xuất về việc tối ưu hóa transformer lớn. Đặc biệt, chúng tôi cảm ơn Anselm cho các cuộc thảo luận của ông về giảm chi phí bộ nhớ, XLA, và huấn luyện các mô hình lớn. Chúng tôi cũng đánh giá cao các đề xuất có giá trị về việc tối ưu hóa transformer tiết kiệm bộ nhớ được cung cấp bởi Markus và Federico, cũng như các cuộc thảo luận có giá trị với Sharad về việc triển khai BPT với Triton và Jax Pallas.

--- TRANG 10 ---
Tài liệu tham khảo
[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716–23736, 2022.

[2]Kapathy Andrej. GitHub - karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs. — github.com. https://github.com/karpathy/nanoGPT, 2023. [Truy cập 16-Tháng 5-2023].

[3]Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint arXiv:2102.08602, 2021.

[4]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[6]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.

[7]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[8]Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.

[9]Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.

[10] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.

[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

[12] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

[14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[16] Facebook. Fully Sharded Data Parallel: faster AI training with fewer GPUs — engineering.fb.com. https://engineering.fb.com/2021/07/15/open-source/fsdp/, 2023. [Truy cập 16-Tháng 5-2023].

--- TRANG 11 ---
[17] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.

[18] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus, 2019. URL http://Skylion007. github.io/OpenWebTextCorpus.

[19] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33: 1474–1487, 2020.

[20] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.

[21] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.

[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[23] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099–9117. PMLR, 2022.

[24] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651–4664. PMLR, 2021.

[25] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.

[26] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055, 2022.

[27] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. arXiv preprint arXiv:2205.05198, 2022.

[28] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.

[29] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv:2210.14215, 2022.

[30] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

[31] Shenggui Li, Jiarui Fang, Zhengda Bian, Hongxin Liu, Yuliang Liu, Haichen Huang, Boxiang Wang, and Yang You. Colossal-ai: A unified deep learning system for large-scale parallel training. arXiv preprint arXiv:2110.14883, 2021.

[32] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022.

[33] Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. International Conference on Machine Learning, 2023.

[34] Hao Liu, Carlo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676, 2023.

--- TRANG 12 ---
[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[36] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022.

[37] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018.

[38] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021.

[39] OpenAI. Gpt-4 technical report, 2023.

[40] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023.

[41] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.

[42] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021.

[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.

[45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844–8856. PMLR, 2021.

[46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020.

[47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021.

[48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

[50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022.

[51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1–28, 2022.

--- TRANG 13 ---
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022.

[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

[55] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.

[56] Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning. arXiv preprint arXiv:2201.13425, 2022.

[57] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.

[58] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Transformer feed-forward layers are mixtures of experts. arXiv preprint arXiv:2110.01786, 2021.

[59] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter-and {Intra-Operator }parallelism for distributed deep learning. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 559–578, 2022.

[60] Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. Moe-bert: from bert to mixture-of-experts via importance-guided adaptation. arXiv preprint arXiv:2204.07675, 2022.

--- TRANG 14 ---
A Chi tiết Thí nghiệm
A.1 Đánh giá Bộ nhớ
Trong kết quả thí nghiệm được trình bày trong Mục 5.1, chúng tôi sử dụng song song mô hình để phân vùng mô hình qua 8 GPU hoặc 64 đơn vị TPUv4. Đánh giá của chúng tôi tập trung vào việc xác định độ dài chuỗi tối đa có thể đạt được, sử dụng số chuỗi là một. Đối với TPU, chúng tôi sử dụng cấu hình huấn luyện mặc định, bao gồm thực hiện các hoạt động matmul ở định dạng bfloat16 với tích lũy trọng số ở float32. Mặt khác, đối với GPU, chúng tôi áp dụng thiết lập mặc định, nơi tất cả các hoạt động được thực hiện ở float32.

Để profiling việc sử dụng bộ nhớ, chúng tôi sử dụng jax.profile và lặp lại đánh giá 100 lần, báo cáo kết quả trung bình. Chúng tôi tiến hành tìm kiếm lưới cho kích thước khối query và kích thước khối key-value tối ưu, xem xét các giá trị từ tập [16,64,128,512,1024,2048,4096]. Đối với mỗi phương pháp, chúng tôi báo cáo bộ nhớ thấp nhất đạt được.

A.2 Đánh giá Thông lượng
Trong đánh giá được trình bày trong Mục 5.2, chúng tôi chia OpenWebText theo phương pháp của [2]. Thông lượng được đo bằng token trên mỗi thiết bị mỗi giây. Để đảm bảo so sánh công bằng, chúng tôi thực hiện tìm kiếm lưới cho kích thước khối query và kích thước khối key-value tối ưu, xem xét các giá trị từ tập [16,64,128,512,1024,2048,4096]. Đối với gradient checkpointing [8], chúng tôi bổ sung tìm kiếm lưới giữa ba chính sách checkpointing thường được sử dụng bao gồm nothing_saveable, dots_saveable, và dots_with_no_batch_dims_saveable cho attention và sử dụng nothing_saveable cho mạng feedforward (FFN). Để biết thêm chi tiết, vui lòng tham khảo tài liệu Jax. Chúng tôi chọn cấu hình hoạt động tốt nhất cho cả baseline và phương pháp của chúng tôi.

Huấn luyện được tiến hành sử dụng FSDP [16] và tích lũy gradient. Chúng tôi sử dụng weight decay 0.1 và sử dụng learning rate decay cosine với learning rate tối đa 2.0×e⁻⁴. Đối với độ dài chuỗi 2048,4096,8192,16384, kích thước batch trong quỹ đạo được đặt lần lượt là 8,4,2,1,1. Chúng tôi sử dụng tích lũy gradient để tích lũy kích thước batch trong token đến 1 triệu mỗi batch.

A.3 Đánh giá về RL
Bảng 6: Siêu tham số được sử dụng trong đánh giá RL.
Siêu tham số | Giá trị
Số lớp | 3
Số attention head | 1
Chiều embedding | 128
Hàm kích hoạt | ReLU
Kích thước batch | 64
Dropout | 0.1
Learning rate | 10⁻⁴
Learning rate decay | Linear warmup cho 10⁵ bước
Grad norm clip | 0.25
Weight decay | 10⁻⁴
Target return mong muốn ban đầu tại thời điểm test | 850 Walker Stand
 | 400 Walker Run
 | 900 Walker Walk
 | 350 Cheetah Run
 | 300 Jaco Reach
 | 800 Cartpole Swingup
Số quỹ đạo trong huấn luyện | 4→32
Số quỹ đạo tại thời điểm test | 4→16

Trong thí nghiệm được trình bày trong Mục 5.3, chúng tôi theo thiết lập của công trình trước cho learning rate, kích thước batch, và các siêu tham số khác, trong khi sửa đổi số quỹ đạo. Các siêu tham số cụ thể được cung cấp trong Bảng 6. Agentic transformer gốc sử dụng 4 quỹ đạo trong huấn luyện, chúng tôi tăng số này lên 32.

Trong testing, việc tăng số quỹ đạo đã được chứng minh là cải thiện hiệu suất. Tuy nhiên, thực hiện sampling autoregressive trên một số lượng lớn quỹ đạo (ví dụ, 64×1000×4 tổng số token) có thể chậm về mặt tính toán. Để giảm thời gian sampling, chúng tôi giới hạn rollout đến 16 quỹ đạo.

--- TRANG 15 ---

--- TRANG 16 ---
1def blockwise_ffn(remat_ffn, inputs, chunk_size, deterministic):
2 # remat_ffn: một ffn được tái vật chất hóa
3 inputs = rearrange(inputs, 'b (c n) d -> b c n d ', c=chunk_size)
4 def scan_ffn(remat_ffn, carry, hidden_states):
5 outputs = remat_ffn(hidden_states, deterministic=deterministic)
6 return carry, outputs
7 scan_axis = inputs.ndim - 2
8 _, res = nn.scan(
9 scan_ffn,
10 variable_broadcast="params",
11 split_rngs={"params": False, "dropout": True},
12 in_axes=scan_axis,
13 out_axes=scan_axis,
14 )(remat_ffn, None, inputs)
15 res = rearrange(res, 'b c n d -> b (c n) d ')
16 return res
17
18 def blockwise_attn(query, key, value, query_chunk_size,
19 key_chunk_size, dtype, policy, precision, prevent_cse):
20 query = query / jnp.sqrt(query.shape[-1]).astype(dtype)
21 query = rearrange(query, 'b (c n) h d -> n b c h d ', c=query_chunk_size)
22 key, value = map(lambda t: rearrange(t, 'b (c n) h d -> n b c h d ', c=key_chunk_size), (key, value))
23 num_q, batch, _, num_heads, dim_per_head = query.shape
24 num_kv = key.shape[0]
25 def scan_attention(args):
26 query_chunk, query_chunk_idx = args
27 @functools.partial(jax.checkpoint, prevent_cse=prevent_cse, policy=policy)
28 def scan_kv_block(carry, args):
29 key_chunk, value_chunk, key_chunk_idx = args
30 (numerator, denominator, prev_max_score) = carry
31 attn_weights = jnp.einsum( 'bqhd,bkhd->bqhk ', query_chunk, key_chunk, precision=precision)
32 bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)
33 bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)
34 attn_weights = attn_weights + bias_chunk
35
36 max_score = jnp.max(attn_weights, axis=-1, keepdims=True)
37 max_score = jnp.maximum(prev_max_score, max_score)
38 max_score = jax.lax.stop_gradient(max_score)
39 exp_weights = jnp.exp(attn_weights - max_score)
40 exp_values = jnp.einsum(
41 'bqhv,bvhf->bqhf ', exp_weights, value_chunk, precision=precision
42 )
43 correction = jnp.exp(prev_max_score - max_score)
44 numerator = numerator * correction + exp_values
45 denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)
46 return Carry(numerator, denominator, max_score), None
47 init_carry = Carry(
48 jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=query.dtype),
49 jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=query.dtype),
50 (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=query.dtype),
51 )
52 (numerator, denominator, max_score), _ = lax.scan(
53 scan_kv_block, init_carry, xs=(key, value, jnp.arange(0, num_kv))
54 )
55 outputs = (numerator / denominator).astype(dtype)
56 return outputs
57 _, res = lax.scan(
58 lambda _, x: ((), scan_attention(x)),
59 (), xs=(query, jnp.arange(0, num_q))
60 )
61 res = rearrange(res, 'n b c h d -> b (n c) h d ')
62 return res

Hình 3: Các phần chính của triển khai Blockwise Parallel trong Jax. Mã đầy đủ có sẵn trên Github https://github.com/lhao499/llm_large_context

--- TRANG 17 ---
