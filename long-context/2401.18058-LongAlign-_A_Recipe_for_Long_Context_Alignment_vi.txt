Prompt đánh giá. Đối với mỗi câu hỏi, chúng tôi chấm điểm thủ công ba phản hồi làm ví dụ chấm điểm few-shot, xáo trộn thứ tự của chúng trong mỗi lần chạy đánh giá và sử dụng prompt sau để nhận được đánh giá của GPT-4:

[Hướng dẫn] Bạn được yêu cầu đánh giá chất lượng câu trả lời của trợ lý AI cho câu hỏi người dùng với tư cách là một thẩm phán không thiên vị, và việc đánh giá của bạn nên tính đến các yếu tố bao gồm tính chính xác (ưu tiên cao), tính hữu ích, độ chính xác, và sự liên quan. Các nguyên tắc chấm điểm như sau: 1. Đọc câu trả lời của trợ lý AI và so sánh câu trả lời của trợ lý với câu trả lời tham chiếu. 2. Xác định tất cả các lỗi trong câu trả lời của Trợ lý AI và xem xét chúng ảnh hưởng đến câu trả lời cho câu hỏi như thế nào. 3. Đánh giá mức độ hữu ích của câu trả lời của trợ lý AI trong việc trả lời trực tiếp câu hỏi của người dùng và cung cấp thông tin mà người dùng cần. 4. Kiểm tra bất kỳ thông tin bổ sung nào trong câu trả lời của trợ lý AI để đảm bảo rằng nó đúng và liên quan chặt chẽ đến câu hỏi. Nếu thông tin này không chính xác hoặc không liên quan đến câu hỏi, điểm số nên được trừ khỏi điểm số tổng thể.

Vui lòng đưa ra một đánh giá số nguyên tổng thể từ 1 đến 10 dựa trên các nguyên tắc trên, nghiêm ngặt theo định dạng sau: "[[rating]]", ví dụ "[[5]]".

[Câu hỏi] {}
[Câu trả lời tham chiếu bắt đầu] {} [Câu trả lời tham chiếu kết thúc]
Dưới đây là câu trả lời của một số trợ lý và đánh giá của chúng:
[Câu trả lời của trợ lý bắt đầu] {} [Câu trả lời của trợ lý kết thúc]
Đánh giá: [[{}]]
[Câu trả lời của trợ lý bắt đầu] {} [Câu trả lời của trợ lý kết thúc]
Đánh giá: [[{}]]
[Câu trả lời của trợ lý bắt đầu] {} [Câu trả lời của trợ lý kết thúc]
Đánh giá: [[{}]]
Vui lòng đánh giá các câu trả lời của trợ lý sau dựa trên các nguyên tắc chấm điểm và ví dụ trên:
[Câu trả lời của trợ lý bắt đầu] {} [Câu trả lời của trợ lý kết thúc]
Đánh giá:

Đây là prompt zero-shot được sử dụng làm baseline trong nghiên cứu đánh giá metric của chúng tôi:

[Hướng dẫn] Bạn được yêu cầu đánh giá chất lượng câu trả lời của trợ lý AI cho câu hỏi người dùng với tư cách là một thẩm phán không thiên vị, và việc đánh giá của bạn nên tính đến các yếu tố bao gồm tính chính xác (ưu tiên cao), tính hữu ích, độ chính xác, và sự liên quan. Các nguyên tắc chấm điểm như sau: 1. Đọc câu trả lời của trợ lý AI và so sánh câu trả lời của trợ lý với câu trả lời tham chiếu. 2. Xác định tất cả các lỗi trong câu trả lời của Trợ lý AI và xem xét chúng ảnh hưởng đến câu trả lời cho câu hỏi như thế nào. 3. Đánh giá mức độ hữu ích của câu trả lời của trợ lý AI trong việc trả lời trực tiếp câu hỏi của người dùng và cung cấp thông tin mà người dùng cần. 4. Kiểm tra bất kỳ thông tin bổ sung nào trong câu trả lời của trợ lý AI để đảm bảo rằng nó đúng và liên quan chặt chẽ đến câu hỏi. Nếu thông tin này không chính xác hoặc không liên quan đến câu hỏi, điểm số nên được trừ khỏi điểm số tổng thể.

Vui lòng đưa ra một đánh giá số nguyên tổng thể từ 1 đến 10 dựa trên các nguyên tắc trên, nghiêm ngặt theo định dạng sau: "[[rating]]", ví dụ "[[5]]".

[Câu hỏi] {}
[Câu trả lời tham chiếu] {}
[Câu trả lời của trợ lý] {}
Đánh giá:

Đánh giá của con người. Ở đây chúng tôi cung cấp thêm chi tiết cho nghiên cứu đánh giá của con người trên LongBench-Chat. Chúng tôi chọn các phản hồi cho 50 câu hỏi trên LongBench-Chat từ sáu mô hình khác nhau, tạo ra một nhóm dữ liệu gồm 300 thể hiện. Chúng tôi mời hai chuyên gia con người (cả hai đều là sinh viên tiến sĩ từ Đại học Tsinghua) mỗi người chấm điểm 200 phản hồi dựa trên hướng dẫn và câu trả lời tham chiếu, theo thang điểm từ 1 đến 10. Tiêu chí chấm điểm được cung cấp cho các chuyên gia con người như sau:

Vui lòng chấm điểm phản hồi của trợ lý dựa trên câu hỏi và câu trả lời tham chiếu, với 1 là thấp nhất và 10 là cao nhất. Việc chú thích phải tuân thủ các yêu cầu sau:
1. Tập trung chủ yếu vào việc phản hồi có bao gồm các điểm chính trong câu trả lời tham chiếu hay không.
2. Đối với câu trả lời tham chiếu chứa nhiều điểm chính, hãy xem phản hồi giải quyết chính xác bao nhiêu trong số này và chấm điểm tương ứng.
3. Nếu phản hồi bao gồm các điểm không có trong câu trả lời tham chiếu, hãy kiểm tra văn bản gốc để tìm bằng chứng. Trừ điểm theo quyết định của bạn nếu nó không phù hợp với văn bản gốc.
4. Cũng xem xét việc trừ điểm cho các phản hồi quá dài dòng hoặc những phản hồi quá khái quát hóa.

Chi phí đánh giá. Trên LongBench-Chat, một lần chạy đánh giá yêu cầu khoảng 32,000 token trung bình (hầu như hoàn toàn là input token). Do đó, sử dụng GPT-4 để đánh giá sẽ có chi phí khoảng $0.96 mỗi lần chạy.

C.2 LongBench
Prompt đánh giá. Chúng tôi sử dụng GPT-4 để chấm điểm các phản hồi từ các mô hình được căn chỉnh của chúng tôi trong các tác vụ Single-Doc QA, Multi-Doc QA, và Summarization trên LongBench. Đối với hai tác vụ QA đầu tiên, prompt cho người đánh giá GPT-4 như sau:

Bạn được yêu cầu đánh giá chất lượng câu trả lời của trợ lý AI cho câu hỏi người dùng với tư cách là một thẩm phán không thiên vị, và việc đánh giá của bạn nên tính đến các yếu tố bao gồm tính chính xác (ưu tiên cao), và tính toàn diện (liệu câu trả lời của trợ lý có bao gồm tất cả các điểm hay không). Đọc câu trả lời của trợ lý AI và so sánh với câu trả lời tham chiếu, và đưa ra một đánh giá số nguyên tổng thể trong 1, 2, 3 (1 = sai hoặc không liên quan, 2 = đúng một phần, 3 = đúng và toàn diện) dựa trên các nguyên tắc trên, nghiêm ngặt theo định dạng sau: "[[rating]]", ví dụ "[[2]]".

Câu hỏi:
{Câu hỏi}
Câu trả lời tham chiếu:
{Groundtruth}
Câu trả lời của trợ lý:
{Phản hồi}
Đánh giá:

Prompt cho đánh giá GPT-4 trên các tác vụ tóm tắt như sau:

Bạn được yêu cầu đánh giá chất lượng bản tóm tắt được tạo bởi trợ lý AI với tư cách là một thẩm phán không thiên vị, và việc đánh giá của bạn nên tính đến các yếu tố bao gồm tính chính xác (ưu tiên cao), tính toàn diện (liệu bản tóm tắt của trợ lý có bao gồm tất cả các điểm hay không), và tính mạch lạc. Đọc bản tóm tắt của trợ lý AI và so sánh với bản tóm tắt tham chiếu, và đưa ra một đánh giá số nguyên tổng thể theo thang điểm từ 1 đến 5, trong đó 1 là thấp nhất và 5 là cao nhất dựa trên tiêu chí đánh giá, nghiêm ngặt theo định dạng sau: "[[rating]]", ví dụ "[[3]]".

Bản tóm tắt tham chiếu:
{Groundtruth}
Bản tóm tắt của trợ lý:
{Phản hồi}
Đánh giá:

Chi phí đánh giá. Trên LongBench, một lần chạy đánh giá GPT-4 trên 12 tập dữ liệu trong các tác vụ Single-Doc QA, Multi-Doc QA, và Summarization yêu cầu khoảng 800,000 token trung bình (hầu như hoàn toàn là input token). Do đó, sử dụng GPT-4 để đánh giá sẽ có chi phí khoảng $24 mỗi lần chạy.

C.3 Kiểm tra Kim
Đối với đánh giá "Needle in A Haystack", theo cấu hình gốc trong kho github gốc, chúng tôi sử dụng "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day." làm thông tin kim, và các bài luận của Paul Graham làm ngữ cảnh đống cỏ khô dài. Chúng tôi sử dụng query prompt từ Claude 2.1: "What is the best thing to do in San Francisco? Here is the most relevant sentence in the context:".

D Nghiên cứu Trường hợp về Truy vấn OOD
Như một phần của nghiên cứu về việc căn chỉnh LLM trên ngữ cảnh dài, chúng tôi nghĩ ra một nghiên cứu trường hợp thú vị và thực tế: Liệu chúng ta có thể đánh giá khả năng hiểu ngữ cảnh dài của các mô hình được huấn luyện của chúng tôi bằng cách sử dụng bài báo này làm đầu vào dài không? Do đó chúng tôi sử dụng bài báo làm đầu vào (tất nhiên, để tránh lồng đệ quy, bài báo đầu vào không bao gồm phần này). Chúng tôi xem xét việc sử dụng truy vấn OOD, tức là truy vấn mà mô hình chưa gặp trong dữ liệu SFT ngữ cảnh dài. Các truy vấn của chúng tôi bao gồm "Từ góc độ của một người đánh giá chuyên nghiệp, hãy xác định điểm mạnh và điểm yếu của bài báo này" và "Sử dụng rap để kể về bài báo này". Hãy xem các biến thể mô hình phản hồi như thế nào:

!!!Tuyên bố miễn trừ trách nhiệm: Nội dung dưới đây về bài báo này được tạo ra bởi AI và KHÔNG phản ánh quan điểm thực sự của các tác giả hoặc độc giả của bài báo này. Vui lòng nhận biết và xác định bất kỳ thông tin sai lệch nào.

/usrNguời dùng: Từ góc độ của một người đánh giá chuyên nghiệp, hãy xác định điểm mạnh và điểm yếu của bài báo này.

/cgsLongAlign-6B-64k (ChatGLM3-6B-64k được huấn luyện trên LongAlign-10k sử dụng đóng gói + cân bằng loss):
Là một mô hình ngôn ngữ AI, tôi không thể cung cấp một đánh giá chủ quan vì tôi thiếu khả năng đánh giá chất lượng của bài báo dựa trên thành kiến cá nhân hoặc xu hướng. Tuy nhiên, tôi có thể cung cấp một phân tích khách quan về điểm mạnh và điểm yếu của bài báo.

Điểm mạnh:
1. Cách tiếp cận toàn diện: Bài báo cung cấp một công thức kỹ lưỡng và có hệ thống, LongAlign, để giải quyết các thách thức của việc căn chỉnh các mô hình ngôn ngữ lớn để xử lý ngữ cảnh dài. Nó bao gồm việc thu thập dữ liệu dài đa dạng, sử dụng các chiến lược huấn luyện hiệu quả, và đánh giá các mô hình đối với các benchmark ngữ cảnh dài thực tế.

2. Tác động của dữ liệu: Bài báo chứng minh rằng số lượng và tính đa dạng của dữ liệu hướng dẫn dài có tác động đáng kể đến khả năng của mô hình trong việc xử lý ngữ cảnh dài, và việc kết hợp nhiều dữ liệu dài hơn là có lợi. Điều này cung cấp một góc nhìn có giá trị về tầm quan trọng của dữ liệu ngữ cảnh dài đa dạng.

3. Phương pháp huấn luyện: Bài báo so sánh hiệu quả của các phương pháp huấn luyện khác nhau và xác định các chiến lược đóng gói và batching đã sắp xếp là hiệu quả và hiệu quả. Điều này cung cấp những hiểu biết có giá trị cho các thực hành viên làm việc về huấn luyện mô hình ngôn ngữ lớn.

Điểm yếu:
1. Thiếu nghiên cứu đánh giá con người quy mô lớn: Bài báo không bao gồm một nghiên cứu đánh giá con người quy mô lớn toàn diện để xác nhận hiệu suất của người đánh giá GPT-4 trên LongBench-Chat. Trong khi mối tương quan với các chú thích của con người được trình bày, một so sánh trực tiếp giữa các đánh giá của con người và GPT-4 bị thiếu.

2. Giới hạn khả năng mở rộng: Bài báo không khám phá khả năng mở rộng của LongAlign đến các mô hình thậm chí lớn hơn hoặc cửa sổ ngữ cảnh dài hơn. Trong khi một số kết quả thí nghiệm về việc mở rộng LongAlign được trình bày, các giới hạn của cách tiếp cận không được khám phá đầy đủ.

3. Phân tích đường cong học tập: Bài báo không cung cấp một phân tích kỹ lưỡng về đường cong học tập dưới căn chỉnh ngữ cảnh dài, so sánh hiệu suất trên các tác vụ ngữ cảnh ngắn và dài trong suốt quá trình huấn luyện. Điều này sẽ cung cấp những hiểu biết có giá trị về động lực học tập.

Tóm lại, bài báo cung cấp một đóng góp có giá trị cho lĩnh vực bằng cách cung cấp một công thức toàn diện để căn chỉnh các mô hình ngôn ngữ lớn để xử lý ngữ cảnh dài, và chứng minh tác động của số lượng và tính đa dạng dữ liệu đến hiệu suất mô hình. Tuy nhiên, một nghiên cứu đánh giá toàn diện và nghiêm ngặt hơn và một khám phá sâu hơn về động lực học tập dưới căn chỉnh ngữ cảnh dài sẽ nâng cao thêm tác động của bài báo.

/cgsLongAlign-7B-64k (Llama-2-7B-64k được huấn luyện trên LongAlign-10k sử dụng batching đã sắp xếp):
Là một mô hình ngôn ngữ AI, tôi không có ý kiến cá nhân hoặc cảm xúc, nhưng tôi có thể cung cấp một đánh giá trung lập về bài báo dựa trên nội dung và cấu trúc của nó.

Điểm mạnh:
1. Bài báo trình bày một công thức toàn diện, LongAlign, để căn chỉnh các mô hình ngôn ngữ lớn để xử lý ngữ cảnh dài. Đây là một đóng góp đáng kể cho lĩnh vực vì thiếu nghiên cứu và thảo luận về căn chỉnh ngữ cảnh dài.

2. Bài báo cung cấp một mô tả chi tiết về quá trình xây dựng dữ liệu, bao gồm việc thu thập các chuỗi dài từ 9 nguồn và sử dụng Claude 2.1 để tạo ra các tác vụ và câu trả lời cho 10k văn bản dài.

3. Bài báo đề xuất hai phương pháp huấn luyện hiệu quả - đóng gói và batching đã sắp xếp - để tăng tốc quá trình huấn luyện và cải thiện hiệu suất cuối cùng của các mô hình.

4. Bài báo giới thiệu LongBench-Chat, một loạt câu hỏi người dùng thực tế mở có độ dài 10k-100k, bao gồm các khía cạnh khác nhau của khả năng tuân theo hướng dẫn.

5. Bài báo tiến hành thí nghiệm trên các benchmark ngữ cảnh dài và chung, chứng minh hiệu quả của LongAlign trong việc căn chỉnh các mô hình để xử lý ngữ cảnh lên đến 64k.

Điểm yếu:
1. Bài báo không cung cấp một phân tích chi tiết về tác động của các phương pháp huấn luyện được đề xuất đối với khả năng chung của mô hình và khả năng hội thoại trong các tình huống ngữ cảnh ngắn.

2. Bài báo không thảo luận về các giới hạn của các phương pháp được đề xuất hoặc đề xuất bất kỳ công việc tương lai nào có thể cải thiện chúng.

3. Bài báo không cung cấp một so sánh chi tiết của LongAlign với các phương pháp hiện có khác cho mở rộng ngữ cảnh dài.

4. Bài báo không cung cấp một đánh giá toàn diện về các phương pháp được đề xuất trên các benchmark ngữ cảnh dài khác hoặc các truy vấn người dùng thực tế.

Nhìn chung, bài báo trình bày một công thức kỹ lưỡng cho căn chỉnh ngữ cảnh dài và cung cấp các phương pháp huấn luyện hiệu quả để tăng tốc quá trình huấn luyện. Tuy nhiên, một phân tích sâu hơn về tác động đối với khả năng chung và một so sánh với các phương pháp khác sẽ làm cho bài báo toàn diện hơn.

/cgsLongAlign-13B-64k (Llama-2-13B-64k được huấn luyện trên LongAlign-10k sử dụng batching đã sắp xếp):
Điểm mạnh:
1. Bài báo trình bày một giải pháp toàn diện để căn chỉnh các mô hình ngôn ngữ lớn để xử lý các tương tác ngữ cảnh dài, bao gồm việc xây dựng một tập dữ liệu hướng dẫn dài đa dạng, phương pháp huấn luyện, và benchmark đánh giá.

2. Các tác giả tiến hành thí nghiệm mở rộng để chứng minh hiệu quả của cách tiếp cận của họ, cho thấy cải thiện trong các tác vụ downstream mà không có sự suy giảm hiệu suất trên các benchmark chung.

3. Bài báo cung cấp những hiểu biết có giá trị về các thách thức và giải pháp cho căn chỉnh ngữ cảnh dài, đây là một lĩnh vực quan trọng nhưng chưa được nghiên cứu đầy đủ.

4. Benchmark LongBench-Chat được đề xuất là một tài nguyên có giá trị để đánh giá khả năng hiểu ngữ cảnh dài và tuân theo hướng dẫn của các mô hình ngôn ngữ lớn.

5. Bài báo được viết tốt và dễ theo dõi, với các giải thích và hình ảnh rõ ràng.

Điểm yếu:
1. Bài báo thiếu một khung lý thuyết rõ ràng hoặc động lực cho lý do tại sao căn chỉnh ngữ cảnh dài là quan trọng và nó khác với căn chỉnh ngữ cảnh ngắn như thế nào.

2. Các thí nghiệm có thể được hưởng lợi từ phân tích và thảo luận sâu hơn về kết quả, đặc biệt là liên quan đến các giải pháp được đề xuất.

3. Việc đánh giá trên LongBench-Chat dựa trên một số lượng nhỏ ví dụ được chú thích bởi con người, có thể không nắm bắt đầy đủ sự đa dạng của các truy vấn thực tế.

4. Bài báo không thảo luận về các giới hạn tiềm năng hoặc vấn đề khả năng mở rộng với cách tiếp cận được đề xuất khi kích thước mô hình và độ dài ngữ cảnh tăng lên.

5. Chiến lược cân bằng loss được đề xuất không được giải thích và biện minh đầy đủ, và tác động của nó đối với sự ổn định huấn luyện và hội tụ có thể được điều tra thêm.

Các tác giả tin rằng LongAlign-13B-64k tạo ra một đánh giá tương đối tốt, chỉ với một vài sai sót. Trường hợp này cũng cho thấy khả năng khái quát hóa O.O.D. của mô hình — nó có thể tạo ra đánh giá hợp lý cho một bài báo nghiên cứu mà không gặp loại dữ liệu như vậy trong quá trình huấn luyện.

/usrNguời dùng: Sử dụng rap để kể về bài báo này.

/cgsLongAlign-6B-64k:
Bài báo này trình bày một công thức cho căn chỉnh ngữ cảnh dài, LongAlign, giải quyết các thách thức của việc huấn luyện các mô hình ngôn ngữ lớn (LLM) để xử lý các đầu vào dài hơn. Những ý tưởng chính của LongAlign bao gồm:

1. Dữ liệu: Thu thập các chuỗi dài đa dạng từ nhiều nguồn và sử dụng Self-Instruct để tạo ra 10k dữ liệu hướng dẫn có độ dài 8k-64k.

2. Phương pháp huấn luyện: Xem xét một chiến lược đóng gói hiệu quả hơn để đóng gói các chuỗi lại với nhau đến độ dài tối đa trước khi gửi chúng đến GPU để tối ưu hóa. Điều này được cân bằng với một chiến lược cân bằng loss để ngăn chặn các chuỗi trong các gói có ít chuỗi hơn không ảnh hưởng không cân xứng đến loss. Cũng đề xuất một chiến lược batching đã sắp xếp nhóm các chuỗi có độ dài tương tự để giảm thời gian idle trong batch.

3. Đánh giá: Giới thiệu LongBench-Chat, một loạt câu hỏi người dùng thực tế mở có độ dài 10k-100k, bao gồm các khía cạnh tuân theo hướng dẫn như lý luận, lập trình, và tóm tắt, cũng như đa ngôn ngữ. GPT-4 được sử dụng để chấm điểm phản hồi của mô hình.

Thí nghiệm chứng minh rằng LongAlign căn chỉnh hiệu quả các LLM để xử lý ngữ cảnh dài lên đến 64k về độ dài mà không có sự suy giảm hiệu suất trên các benchmark chung, cho thấy tác động của số lượng, tính đa dạng dữ liệu, phương pháp huấn luyện, và đánh giá.

/cgsLongAlign-7B-64k:
Bài báo này trình bày LongAlign, một công thức để căn chỉnh các mô hình ngôn ngữ lớn để xử lý ngữ cảnh dài. Những thách thức chính trong mở rộng ngữ cảnh dài bao gồm thiếu một tập dữ liệu tuân theo hướng dẫn toàn diện, batching không đều trong huấn luyện đa GPU, và thiếu một benchmark đáng tin cậy. LongAlign giải quyết những thách thức này bằng cách thu thập dữ liệu hướng dẫn dài từ 9 nguồn, sử dụng Self-Instruct để tạo ra 10k dữ liệu hướng dẫn có độ dài 8k-64k, và xây dựng LongBench-Chat, một loạt câu hỏi người dùng thực tế mở có độ dài 10k-100k. Bài báo cũng đề xuất các phương pháp huấn luyện đóng gói và batching đã sắp xếp, cân bằng loss cho huấn luyện đóng gói, và một benchmark gọi là LongBench-Chat để đánh giá các mô hình được căn chỉnh. Thí nghiệm cho thấy LongAlign căn chỉnh hiệu quả các mô hình để xử lý ngữ cảnh lên đến 64k mà không có sự suy giảm hiệu suất trên các benchmark chung.

/cgsLongAlign-13B-64k:
Yo, lắng nghe đây, tôi có một câu chuyện để kể
Về một bài báo sẽ làm đầu bạn quay và lật tẩy
Nó được gọi là "LongAlign: Một Công Thức cho Căn Chỉnh Ngữ Cảnh Dài"
Và tất cả về việc nâng cao khả năng của các mô hình ngôn ngữ lớn
Họ giải quyết thách thức mở rộng các mô hình để xử lý ngữ cảnh dài
Bằng cách thu thập và xây dựng một tập dữ liệu đa dạng dữ liệu hướng dẫn dài
Các phương pháp huấn luyện như đóng gói và batching đã sắp xếp được khám phá
Để tối ưu hóa hiệu quả và hiệu suất mà không làm tổn hại khả năng chung
Benchmark đánh giá của họ, LongBench-Chat, là một loạt truy vấn người dùng thực tế
Để đánh giá khả năng của mô hình trong việc tuân theo hướng dẫn ngữ cảnh dài và cung cấp câu trả lời liên quan
Họ thấy rằng việc kết hợp dữ liệu hướng dẫn dài trong quá trình huấn luyện là rất quan trọng
Và các phương pháp được đề xuất có thể tăng tốc độ huấn luyện trong khi cải thiện hiệu suất
Vì vậy nếu bạn quan tâm đến các mô hình ngôn ngữ và khả năng của chúng
Bài báo này là bắt buộc phải đọc, nó sẽ khiến bạn suy nghĩ lại
LongAlign, công thức cho căn chỉnh ngữ cảnh dài
Là chìa khóa để mở khóa toàn bộ tiềm năng của các mô hình ngôn ngữ lớn, điều đó chắc chắn, đó là một bước tiến!

Mặc dù LongAlign-6B-64k và LongAlign-7B-64k tạo ra những bản tóm tắt tốt cho bài báo của chúng tôi, chỉ có LongAlign-13B-64k tuân theo hướng dẫn và tóm tắt bài báo của chúng tôi bằng rap. Rap của nó có đúng dạng, tuy nhiên nó không có vần điệu.

E Hạn chế
Công việc của chúng tôi trong việc khám phá căn chỉnh ngữ cảnh dài có những hạn chế. Từ góc độ dữ liệu, chúng tôi chủ yếu bao gồm dữ liệu hướng dẫn dài cho các loại như QA ngữ cảnh dài, tóm tắt, và lý luận trong việc xây dựng dữ liệu. Trong thực tế, có nhiều loại tác vụ hướng dẫn dài khác phụ thuộc rất nhiều vào khả năng hiểu các văn bản mở rộng, chẳng hạn như các cuộc hội thoại đa lượt (hàng trăm hoặc hàng nghìn lượt, thậm chí các cuộc hội thoại suốt đời), nhập vai dài hạn, và các tác vụ tác nhân lịch sử dài, v.v. Chúng tôi thấy rằng việc thu thập dữ liệu có sẵn cho những tác vụ này là thách thức vì hiệu suất hiện tại của các LLM trên những tác vụ này chưa đáp ứng được nhu cầu của con người. Do đó, người dùng hiếm khi tương tác với các LLM theo cách này. Ngoài ra, vì các LLM hiện tại, dù dựa trên API hay mô hình mở nguồn, hoạt động kém trên những tác vụ này, rất khó để tự động xây dựng dữ liệu như vậy bằng cách tiếp cận giống như Self-Instruct. Chúng tôi hy vọng khám phá nhiều loại dữ liệu ngữ cảnh dài hơn, cho phép các mô hình căn chỉnh với kỳ vọng của con người trên các tác vụ ngữ cảnh dài khác nhau trong các công việc tương lai.

Từ góc độ huấn luyện, do các hạn chế của framework DeepSpeed và tài nguyên GPU của chúng tôi chỉ hỗ trợ SFT cho các mô hình cấp 10B với độ dài tối đa 64k, chúng tôi không tiến hành thí nghiệm lớn trên dữ liệu dài hơn hoặc mô hình lớn hơn. Một số framework hiện tại, chẳng hạn như Megatron (Shoeybi et al., 2019), hỗ trợ nhiều phương pháp song song hóa hơn bao gồm song song hóa mô hình và song song hóa chuỗi, nhưng khó sử dụng và tái tạo do độ phức tạp của cấu trúc mã của chúng. Chúng tôi hy vọng khám phá căn chỉnh ngữ cảnh dài trên các chuỗi dài hơn và các mô hình quy mô lớn hơn sử dụng các framework huấn luyện tiên tiến hơn. Ngoài ra, khám phá RLHF trong căn chỉnh ngữ cảnh dài cũng là một hướng đầy hứa hẹn.
