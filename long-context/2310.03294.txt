# 2310.03294.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2310.03294.pdf
# File size: 4478090 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DISTFLASH ATTN : Distributed Memory-efficient Attention
for Long-context LLMs Training
Dacheng Li‚àóùëèRulin Shao‚àóùë§Anze Xieùë†Eric P . Xingùëê
Xuezhe Maùë¢Ion StoicaùëèJoseph E. GonzalezùëèHao Zhangùë†
ùëèUC Berkeleyùë§University of Washingtonùë†UCSDùëêCMUùë¢USC
Abstract
FlashAttention (Dao, 2023) effectively reduces the quadratic peak memory
usage to linear in training transformer-based large language models (LLMs)
on a single GPU. In this paper, we introduce DISTFLASH ATTN, a distributed
memory-efficient attention mechanism optimized for long-context LLMs
training. We propose three key techniques: token-level workload balancing,
overlapping key-value communication, and a rematerialization-aware gra-
dient checkpointing algorithm. We evaluate DISTFLASH ATTN on Llama-7B
and variants with sequence lengths from 32K to 512K. DISTFLASH ATTN
achieves 8√ólonger sequences, 4.45 ‚àí5.64√óspeedup compared to Ring
Self-Attention, 2‚àí8√ólonger sequences, 1.24 ‚àí2.01√óspeedup compared
to Megatron-LM with FlashAttention. It achieves 1.67 √óand 1.26‚àí1.88√ó
speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Code
is available at https://github.com/RulinShao/LightSeq.
1 Introduction
Large language models (LLMs) capable of processing long context have enabled many
novel applications, such as generating a complete codebase (Osika, 2023) and chatting with
long documents (Li et al., 2023). Yet, training these LLMs with long sequences significantly
increases activation memory footprints, posing new challenges.
Contemporary approaches to manage the high memory demands of long-context LLMs
training involve either reducing activation memory on a single device or partitioning and
distributing the sequences across multiple devices. Memory-efficient attention (Dao et al.,
2022; Dao, 2023; Rabe & Staats, 2021) represents the former, which reduces the peak memory
usage of attention operations on a single device. Despite their effectiveness, the absence of a
distributed extension limits their application to sequence lengths that a single device can
accommodate. Naively combining it with existing tensor or pipeline parallelisms (Shoeybi
et al., 2019)) leads to excessive communication ( ¬ßD) and cannot scale with sequence length
(¬ß4). On the other hand, sequence parallelism systems, Ring Self-Attention (Li et al., 2021)
and Ring Attention (Liu et al., 2023), distribute the activations of a long sequence across
multiple devices, but they lack support for memory-efficient attentions (e.g., FlashAttention)
or scheduling optimizations, making them inefficient in training long sequences (¬ß 4.3).
This paper introduces DISTFLASH ATTN to extend the advantages of FlashAttention (Dao,
2023) to the distributed setting while maintaining high GPU utilization and low commu-
nication overhead. DISTFLASH ATTN efficiently distributes token chunks across multiple
devices, while maintaining the IO-aware benefits of memory-efficient attention. We identify
three key challenges in achieving high GPU utilization on distributed FlashAttention design
for long-context LLMs and propose three optimizations to addgress them.
‚àóAuthors contributed equally.
1arXiv:2310.03294v2  [cs.LG]  31 Mar 2024

--- PAGE 2 ---
waiting
waiting
waiting
watingworker 1
worker 2
worker 3
worker 4
worker 5
worker 6
worker 7
worker 8waiting
waiting
waiting
watingwaiting
waiting
waiting
watingwaiting
waiting
waitingwaiting
waitingwaiting
waitingwaiting
waiting
waiting
wating
waitingwaiting
waiting
waiting
wating
waiting
waiting
waitingwaiting
(b) Load-Balanced Scheduling (Ours) (a) Ring Scheduling (Unbalanced)finish in 5 time steps finish in 8 time stepsFigure 1: Per-worker workload at different time steps in (a) ring scheduling (Li et al.,
2021) and (b) the proposed load-balanced scheduling in an 8-worker scenario. The causal
attention introduces a quadratic work dependency on the prefix of each token, where
workers assigned earlier tokens remain idle while waiting for workers with later tokens.
The idle fraction of the ring scheduling isùëÉ2‚àíùëÉ
2ùëÉ2, asymptotically1
2when scaling to more
number of workers. The idle fraction of the proposed load-balanced scheduling is1
2ùëÉwhen
ùëÉis even and 0 when ùëÉis odd, asymptotically 0 when scaling to a larger number of workers.
The first challenge is the token-level workload imbalance caused by causal language model-
ing. As shown in Figure 1 (a), the causal attention introduces a quadratic work dependency
on the prefix of each token. This leads to workers assigned earlier tokens to remain idle
while waiting for workers with later tokens to complete, lowering the GPU utilization
almost by half. We address this challenge by introducing a load-balancing schedule that
routes the extra attention computation of later tokens to those idle workers ( ¬ß3.2). This
optimization yields twice throughput of the unbalanced version as shown in Figure 4.
The second challenge is the prohibitive communication overhead. When tokens are dis-
tributed to multiple machines, these machines need to communicate key-value tensors and
softmax statistics to jointly compute the global attention. The communication volume is
nontrivial, leading to large communication overhead, which grows with the context length.
By leveraging the attention dependencies, we propose a scheduling technique that overlaps
communication and computation by pre-fetching tensors. This successfully hides commu-
nication overhead inside the computation time, resulting in a 1.32 √óend-to-end speedup
(Figure 4) compared to a non-overlapping version.
The third challenge is the high computation overhead due to the re-computation in gradient
checkpointing (Chen et al., 2016). Gradient checkpointing effectively trades computation for
memory by selectively storing intermediate activations (e.g., the inputs of every layer) and
recomputing on-the-fly during the backward pass. It has become a standard technique in the
training of long-context LLMs to accommodate the prohibitive activation memory (Zheng
et al., 2023). However, the recomputation of the FlashAttention causes a high computation
overhead in long sequences where the attention dominates the computation time. In ¬ß3.3,
we show the recomputation of FlashAttention is unnecessary for its backward pass and
propose a novel gradient checkpointing strategy to avoid it. Our new strategy results in an
1.31√óspeedup (¬ß 4.5) without introducing any numerical difference.
Our main contributions are:
1.We develop DISTFLASH ATTN, a distributed, memory-efficient, exact attention mech-
anism with sequence parallelism. We propose new optimization techniques to bal-
ance the causal computation workloads and overlap computation and computation
to increase GPU utilization and reduce communication overhead for training long-
context LLMs. We also propose a rematerialization-aware gradient checkpointing
strategy that eliminates redundant forward recomputation of FlashAttention.
2.We perform comprehensive evaluation of DISTFLASH ATTN on LLaMA models,
against four strong distributed systems. DISTFLASH ATTN supports 8√ólonger
2

--- PAGE 3 ---
GPU
Computation
StreamGPU
Communication
StreamFigure 2: Overlap example in the forward pass of worker 7 out of an 8 worker scnerio. For
simplicity, ‚Äùworker p‚Äù is denoted as p.
sequences with 5.64 √ócompared to Ring Self-Attention, 2 ‚àí8√ólonger sequences with
1.24‚àí2.01√óspeedup compared to Megatron-LM. DISTFLASH ATTN achieves 1.67√ó
and 1.26‚àí1.88√óspeedup compared to Ring Attention and DeepSpeed-Ulysses.
2 Related work
Memory-efficient attention. Dao et al. (2022) and Lefaudeux et al. (2022) propose to
use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a
blockwise and memory-efficient way. It reduces peak memory usage by not materializing
large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse
attention computes only a sparse subset of the attention score, which also reduces the
memory footprints yet may lead to inferior performance (Beltagy et al., 2020; Sun et al., 2022;
Zaheer et al., 2020). In this work, we limit our scope to exact attention.
Sequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among
the first to parallelize Transformers in the sequence dimension. However, its distributed
attention design is not optimized for causal language modeling and incompatible with
memory-efficient attention, which are crucial for long-context LLM training. Ring Atten-
tion (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient
blockwise pattern. However, it is also not optimized for causal language modeling, leading
to 2√óextra computation. DISTFLASH ATTN optimizes for both memory-efficient attention
and causal language modeling. More recently, DeepSpeed Ulysses (Jacobs et al., 2023)
proposes a hybrid parallelism strategy. It computes distributed attention in the tensor
model parallelism to address these two problems and utilizes sequence parallelism else-
where (Shoeybi et al., 2019). We provide head-to-head comparison in Table 4.
Model Parallelism and FSDP Tensor Model parallelism (Korthikanti et al., 2023) partitions
model parameters and also distributes the activation in parallel LLM training. Pipeline
model parallelism (Huang et al., 2019) also partitions the activations. However, it applies
high memory pressure to the first pipeline stage. We show in ¬ß4.2 that this leads to a less
effective support for long sequences. Thus, we focus on comparing with tensor model
parallelism and only consider pipeline parallelism when the number of heads is insufficient
for tensor parallelism. Fully sharded data-parallelism (FSDP) (Zhao et al., 2023; Rajbhandari
et al., 2020) distributes optimizer states, gradients, and model parameters onto different
devices and gathers them on-the-fly. Our work focuses on reducing the activation memory
that dominates in long-context training. Therefore, FSDP is orthogonal to our work.
Gradient checkpointing. Gradient checkpointing (Chen et al., 2016) trades computation
for memory by not storing activations for certain layers and recomputing them during the
forward pass. Selective checkpointing (Korthikanti et al., 2023) suggests recomputing only
the attention module, as it requires significant memory but relatively few FLOPs (in contexts
of smaller length). Checkmate (Jain et al., 2020) finds optimal checkpointing positions
using integer linear programming. However, none of these designs have considered the
effects of memory-efficient attention kernels, which perform recomputation within the
computational kernel to avoid materializing large tensors. In this paper, we demonstrate
that by simply altering the checkpointing positions, we can avoid the recomputation of
these kernels without introducing any numerical difference.
3

--- PAGE 4 ---
3 Method
In this section, we first present a distributed memory-efficient attention mechanism that
distributes the computation along the sequence dimension, DISTFLASH ATTN (¬ß3.1) in
its vanilla form. We then introduce two novel optimizations in DISTFLASH ATTN: a load-
balanced scheduling strategy for causal language modeling to reduce the computation
bubble and an asynchronous communication design that overlaps the communication into
computation ( ¬ß3.2). Finally, we propose a new rematerialization-aware checkpointing
strategy ( ¬ß3.3) which effectively cuts off the recomputation time in gradient checkpointing
when using D ISTFLASH ATTN in long-context training.
3.1 D ISTFLASH ATTN: distributed memory-efficient attention via sequence parallelism
The goal of DISTFLASH ATTN is twofold: (1) distribute a single sequence into multiple
workers so they jointly utilize the memory to support a long sequence training; (2) maintain
the IO-aware benefits of memory-efficient attention so that training is fast and incurs less
memory footprint. In particular, we choose FlashAttention (Dao, 2023) as the paradigm.
To distribute the long sequence. DISTFLASH ATTN splits the input sequence consisting of
ùëÅtokens evenly across ùëÉworkers (e.g. GPUs) along the sequence dimension. Each worker
computes and stores the activations of only a subsequence of ùëÅ/ùëÉtokens. Therefore, it
supports training ùëÉ√ólonger with ùëÉworkers than a single-worker FlashAttention.
Formally, let qùëù,kùëù,vùëù‚ààRùëÅ
ùëÉ√óùëëbe the query, key and value of the subsequence on the ùëù-th
worker (ùëù={1,¬∑¬∑¬∑,ùëÉ}), whereùëëis the hidden dimension. Considering the most prevalent
causal attention in LLMs, worker p computes the attention output oùëùassociated with qùëù:
oùëù=Softmax(qùëù[k1, ...,kùëù]ùëá
‚àö
ùëë)[v1, ...,vùëù] (1)
To maintain the IO-awareness . Na¬®ƒ±vely, each worker could gather all the keys and values
associated with other subsequences and then locally computes oùëùby invoking the existing
single machine FlashAttention. However, this gathering introduces memory pressure by
having to store the full list of keys and values locally, a total size of R2ùëÅ√óùëë.
Fortunately, the block-wise nature of the single-worker FlashAttention only requires one
block of keys and values in each iteration of its algorithm, Leveraging this observation, we
compute oùëùiteratively: in each iteration when ùëü‚â†ùëù, worker p fetches only onekùëü,vùëüfrom
a remote worker ùëü, It then computes partial attention results based on qùëùand kùëü,vùëüand
perform proper rescaling by invoking the single-worker FlashAttention kernel. To perform
proper rescaling between iterations, each worker also needs to maintain a copy of softmax
statistics1sùëù‚ààR2ùëÅ
ùëÉ. Computing in this iterative manner, each worker also stores the key
and value of one subsequence of size R2ùëÅ√óùëë
ùëÉ, a factor of1
ùëÉmemory of the na ¬®ƒ±vely design.
We refer to Dao et al. (2022) for more details of the single-worker FlashAttention. We denote
each iteration of the partial attention result and the rescaling as ùëéùë°ùë°ùëõ(qùëù,kùëü,vùëü,sùëù), and
present the vanilla DISTFLASH ATTN algorithm in Algorithm 1. In Appendix A, we show
how to implement the ùëéùë°ùë°ùëõ(¬∑)kernel from Dao (2023) in pseudo-code.
3.2 Load balanced scheduling with communication and computation overlap
Load-balanced scheduling. In causal attention, each token only attends to its previous
tokens, i.e. the p-th worker computes ùëéùë°ùë°ùëõ(qùëù,kùëü,vùëü)for allùëü‚â§ùëù. This introduces a
workload imbalance between workers: a worker with a larger ùëùcomputes more ùëéùë°ùë°ùëõ(¬∑)
(Figure 1 (a)). Using the scheduling described in ¬ß3.1, the idle fraction isùëÉ2‚àíùëÉ
2ùëÉ2(‚Üí1
2when
ùëÉ‚Üí‚àû ), which means roughly half of the workers are idle. To reduce this idle time, we let
workerùëü1that has finished all its ùëéùë°ùë°ùëõ(¬∑)computations (i.e., the ‚Äúhelper‚Äù) perform attention
computation for worker ùëü2with heavier workload, as shown in Figure 1 (b).
1These are statistics ùëôandùëöin FlashAttention words.
4

--- PAGE 5 ---
Fwd
BwdAttn FFN Attn FFN Attn FFNFwd
BwdAttn FFN Attn FFN Attn FFNFlash
AttentionFlash
AttentionFlash
Attentioncheckpoint forward recomputation backward
Huggingface Checkpointing Strategy Rematerialization-A ware Checkpointing Strategy (Ours)Flash
AttentionFlash
AttentionFlash
Attentionother modulesFigure 3: Comparison of HuggingFace gradient checkpointing strategy and our
materialization-aware gradient checkpointing strategy. Note that our checkpointing strategy
saves an entire flash attention forward per layer in recomputation by simply shifting the
checkpoint boundaries without introducing any numerical difference. The checkpointed
tensors, i.e., the outputs of FlashAttention, are saved not only for the recomputation of
subsequent layers but also the backward computation of the preceding FlashAttention.
Notably, the ‚Äúhelper‚Äù ùëü1needs to communicate softmax statistics and the partial attention
output to the original worker ùëü2, so that worker ùëü2can update its local copy of statistics and
output correctly (Algorithm 2). This update function is denoted as ùëüùëíùë†ùëêùëéùëôùëí(¬∑)and updates
the partial output and statistics in the same way as how Dao (2023) updates results from
two block execution. This scheduling gives an average idle time fraction:
ùëã=0, P is odd
1
2ùëÉ, P is even(2)
Note that when P is even, the idle time is asymptotically 0 to more workers. We provide an
illustration with 8 workers in Figure 1 and a more detailed one in Appendix B. While we
focus on the exact attention mechanism, we also discuss sparse patterns in Appendix F.
Communication and computation overlap. DISTFLASH ATTN relies on peer-to-peer (P2P)
communication to fetch kùëü,vùëü(orqùëüin the load-balanced scheduling) from remote workers
before computing ùëéùë°ùë°ùëõ(¬∑). However, these communications can be naturally overlapped. To
simplify the equations, we use the unbalanced schedule to describe the intuition, while the
final DISTFLASH ATTN implementation are equipped with both optimizations. Precisely,
these two operations are parallelized:
Fetch : worker ùëùkùëü+1,vùëü+1‚Üê‚àí‚àí‚àí‚àí‚àí‚àí‚àí workerùëü+1
Compute :ùëéùë°ùë°ùëõ(qùëù,kùëü,vùëü,sùëù)(3)
Thus, in the next iteration, kùëü+1,vùëü+1are already stored in the memory of worker p, without
blocking the next iteration‚Äôs computation. In modern accelerators, this can be done by plac-
ing the attention computation kernel in the main GPU stream, and the P2P communication
kernel in another stream, where they can run in parallel (Zhao et al., 2023). We demonstrate
the overlapped scheduling for worker 7 in the 8-worker scenario in Figure. 2. Empirically,
we find this optimization effectively reduces the communication overhead by hiding the
communication time inside the computation time (¬ß 4.5).
3.3 Rematerialization-aware checkpointing strategy
Gradient checkpointing (Chen et al., 2016) is a de-facto way of training long-context trans-
formers. Often, the system uses heuristics to insert gradient checkpoints at the boundary of
each Transformer layer (Wolf et al., 2019). However, with the presence of Dao et al. (2022),
we find the previous gradient checkpointing strategy causes a redundant recomputation
of the FlashAttention forward kernel. Precisely, when computing the gradient of the MLP
layer, Wolf et al. (2019) re-computes the forward of the entire Transformer layer including
FlashAttention. During this process, the FlashAttention backward kernel re-computes the
5

--- PAGE 6 ---
softmax block-wisely again to reduce memory usage. Essentially, this is because FlashAtten-
tion does not materialize the intermediate values during the forward, and recomputes it
during the backward, regardless of the re-computation in the outer system level (e.g., the
HuggingFace gradient checkpointing (Wolf et al., 2019)).
To tackle this, we propose to insert checkpoints at the output of the FlashAttention kernel,
instead of at the Transformer layer boundary. We use each checkpoint not only for the
recomputation of its subsequent modules but also for the backward computation of its pre-
ceding FlashAttention module without recomputation. Thus we only need to compute the
forward of FlashAttention once, effectively avoiding all recomputations of FlashAttention
as shown in Figure 3.
Figure 7 shows that attention dominates in the forward pass with in long sequences, which
indicates our method saves ‚àº0.23√ó32 (i.e.,‚àº7) seconds when training a 64K sequence
example on Llama-7b on a single machine. In addition, this saves a communication brought
by our DISTFLASH ATTN forward in the distributed training scenario. We benchmark the
end-to-end speedup brought by this materialization-aware checkpointing strategy in ¬ß 4.5.
4 Experiments
We evaluate DISTFLASH ATTN together with our new checkpointing strategy against al-
ternative distributed approaches for long-context LLMs training. Our primary baseline is
Megatron-LM (Shoeybi et al., 2019), used in tandem with FlashAttention, which serves as a
robust baseline extensively adopted within the industry. In Appendix D, we also show a
theoretical analysis on its high communication volume. We also provide a comparison with
the previous sequence-parallel system (Li et al., 2021). In addition, we include comparison
to recent systems including DeepSpeed-Ulysses and Ring Attention (Jacobs et al., 2023;
Liu et al., 2023). In the ablation study, we delineate the individual contributions of each
component of our methodology, specifically load balancing, computation-communication
overlapping, and rematerialization-aware checkpointing, towards the overall performance
enhancement. Code implementation details can be found in Appendix E.
Cluster setup. We evaluate our method and the baselines in (1) A single A100 DGX box
with 8x80 GB GPUs. These GPUs are connected with NVLink; (2) 2 DGX boxes with the
same setting. These two boxes are interconnected by a stable 100 Gbps Infiniband. This is a
representative setting for cross-node training, where the communication overhead is large.
Unless otherwise stated, this is our default setup. (3) Our in-house development cluster
with 2x8 A100 40GB GPUs. This cluster has unstable inter-node bandwidth. Due to the
limited computational budget, we report some peripheral results on this cluster.
Model setup. We evaluate our system on LLaMA-7B and its variants, encompassing four
sets of model architectures in total: two with regular attention heads and two with irregular
ones. We note both categories are important in real-world applications.
With regular attention heads. (1) multi-head attention (MHA) models: LLaMA-7B with
4096 hidden size and 32 self-attention heads (Touvron et al., 2023); (2) grouped-query
attention(GQA) models: LLaMA-GQA (Ainslie et al., 2023), same as LLaMA-7B but with 8
key-value heads, each shared by 4 queries as a group. During attention computation, it will
first replicate to 32 heads to perform matrix multiplication with the correct shape.
With irregular attention heads. In addition, we benchmark the following variants that
have appeared in applications but have not received much attention regarding their system
efficiency: (3) models with an irregular (e.g., non-power-of-two) number of attention heads2:
We intentionally test our systems and baselines on LLaMA-33H, which has the same configu-
ration as LLaMA-7B but with 33 normal self-attention heads per layer. (4) models with fewer
attention heads3: According to the recipe in Liu et al. (2021), we designed LLaMA-16H,
2For example, GPT-2-XL has 25 attention heads, GPT-2 has 12 attention heads, LLaMA-33B and its
fine-tuned versions (e.g., Tulu-30B) have 52 attention heads, Whisper-large has 20 attention heads,
and Falcon-7B has 71 attention heads (Radford et al., 2019; Almazrouei et al., 2023; Ivison et al., 2023).
3Liu et al. (2021) finds fewer attention heads with more layers increase the performance.
6

--- PAGE 7 ---
Table 1: Per iteration wall-clock time of DISTFLASH ATTN and Megatron-LM (Korthikanti
et al., 2023) (Unit: seconds). Speedup in bold denotes the better of the two systems in the
same configuration. Time measured with 2 DGX boxes.
Method # GPUs Sequence Length LLaMA-7B LLaMA-GQA LLaMA-33H
Per GPU Total Time speedup Time speedup Time speedup
Megatron-LM1x8 8K 64K 6.81 1.0x 6.60 1.0x 8.37 1.0x
1x8 16K 128K 20.93 1.0x 20.53 1.0x 25.75 1.0x
1x8 32K 256K 72.75 1.0x 71.93 1.0x 90.21 1.0x
DISTFLASH ATTN1x8 8K 64K 5.98 1.14x 5.61 1.18x 6.08 1.38x
1x8 16K 128K 17.26 1.21x 16.86 1.22x 17.77 1.45x
1x8 32K 256K 58.46 1.24x 57.01 1.26x 59.96 1.50x
Megatron-LM2x8 8K 128K 14.26 1.0x 14.21 1.0x 20.63 1.0x
2x8 16K 256K 43.44 1.0x 43.20 1.0x 62.78 1.0x
2x8 32K 512K 147.06 1.0x 146.38 1.0x 216.70 1.0x
DISTFLASH ATTN2x8 8K 128K 12.75 1.12x 9.74 1.46x 13.12 1.57x
2x8 16K 256K 30.21 1.44x 28.49 1.52x 31.33 2.00x
2x8 32K 512K 106.37 1.38x 102.34 1.43x 107.76 2.01x
LLaMA-8H, LLaMA-4H, and LLaMA-2H with 16, 8, 4, and 2 heads, respectively, as a proof
of concept for situations when the number of heads is insufficient to further scale up model
parallelism with limited resources. We keep the number of attention heads by scaling the
number of layers properly4and keep the intermediate FFN layer size the same to make the
model sizes still comparable. For example, LLaMA-16H has 16 attention heads per layer, a
hidden size of 2048, an FFN layer of size 11008, and 64 layers.
4.1 Comparison with Megatron-LM on MHA and GQA models
Table 2: The maximal sequence length Per
GPU supported by DISTFLASH ATTN and
Megatron-LM with tensor parallelism and
pipeline parallelism on 16xA100 40GB GPUs.
16H 8H 4H 2H
Megatron TP+DP 512K 256K 128K 64K
Megatron TP+PP 512K 256K 256K 128K
DISTFLASH ATTN 512K 512K 512K 512KMulti-head attention (MHA). On the
LLaMA-7B model (Table 1), our method
achieves 1.24√óand 1.44√óspeedup com-
pared to Megatron-LM in single-node and
cross-node setting, up to the longest se-
quence length we experiment. This is a joint
result of our overlapping communication
technique and our rematerialization-aware
checkpointing strategy. We analyze how
much each factor contributes to this result
in the ablation study (¬ß 4.5).
Grouped-query attention (GQA). On GQA model, DISTFLASH ATTN communicates less
volume due to the reduction of size of keys and values. On the contrary, the communication
of Megatron-LM remains the same because it does not communicate keys and values. Thus,
DISTFLASH ATTN achieves a higher speedup on LLaMA-GQA model (Table 1).
4.2 Comparison with Megatron-LM on models with irregular or less number of heads
In support of irregular numbers of heads. Megatron-LM assumes the number of attention
heads is divisible by the model parallelism degree. For example, it supports parallelism
degrees of 2, 4, 8, 16, and 32 for models with 32 attention heads. However, it needs to pad
dummy heads when the number of heads is not divisible by the ideal parallelism degree.
For example, it needs to pad 15 dummy heads to support a parallelism degree of 16 for
models with 33 attention heads (e.g., LlmaMA-33H), leading to a substantial computation
wastage of 45.5%. As shown in Table 1, we observe a 1.50√óand 2.01√óspeedup (an additional
20% and 45% speedup compared to LLaMA-7B cases, aligned with the theoretical analysis).
In support of less number of heads. When the number of GPUs exceeds the number of
attention heads, Megatron-LM allows three possible solutions: (1) Pad dummy heads as
in the LLaMA-33H scenario. However, the percentage of dummy heads almost directly
4For instance, LLaMA-7B has 32 attention heads and 32 layers, thus LLaMA-16H has 16 attention
heads per layers and 64 layers.
7

--- PAGE 8 ---
translates to the percentage of slowdown in long sequences where attention computation
dominates. (2) Use data parallelism for excess GPUs. However, data parallelism does not
reduce per sequence memory usage, and thus can not jointly support longer sequences.
(3) Use pipeline parallelism. However, the memory usage at each stage of the pipeline is
not evenly distributed, limiting the maximal sequence length supported. For instance, in
the LLaMA-2H experiment, we find that different stages consume from 18GB to 32GB in a
64K sequence length (Section C). In addition, using pipeline parallelism introduces an extra
fraction of GPU idle time. We demonstrate the effect of using the latter two solutions in
Table 2. In 16 A100 40GB GPUs, D ISTFLASH ATTN supports 2√óand 8√ólonger sequences.
4.3 Comparison with Ring Self-Attention (RSA) and Ring Attention
Ring self-attention (RSA) (Li et al., 2021) communicates tensors in a ring fashion. We first
report the maximal sequence length of RSA and DISTFLASH ATTN in Table 3, and found
that DISTFLASH ATTN supports at least 8x longer sequences than RSA. This is mainly because
RSA is not natively compatible with memory-efficient attention. We further measure the
iteration time with the maximal sequence length that RSA can support in Table 3, and
find that DISTFLASH ATTN is 4.45x - 5.64x faster than RSA. This speedup includes a 2x
improvement from our causal workload balancing optimization and additional gains from
the overlapping optimization and extending memory-efficient attention to the distributed
setting. Both experiments are conducted with the Llama-7B model and on the DGX cluster.
Table 3: Max sequence length and per iteration time (seconds) compared with RSA.
1 Node 2 Nodes
RSA 32K 64K
DISTFLASH ATTN>256K>512K1 Node (32K) 2 Nodes (64K)
RSA 14.10 30.49
DISTFLASH ATTN 2.50 6.85
Speedup 5.64x 4.45x
Ring Attention (Liu et al., 2023) implements distributed attention in a memory-efficient man-
ner. The key difference between Ring Attention and D ISTFLASH ATTN is D ISTFLASH ATTN
has additional optimization of causal workload balancing and a better gradient checkpoint
strategy. The implementation of Ring Attention uses a different framework from ours (Jax
versus PyTorch). To provide a fair comparison, we consider our ablation version in ¬ß4.5 as
a PyTorch implementation of Ring Attention. ¬ß4.5 provides a detailed analysis. In 8-GPU
setting, we observe a 1.67 √óspeedup (7.5√óversus 4.5√óspeedup compared to a single GPU
FlashAttention) over the design of Ring Attention.
4.4 Comparison with DeepSpeed Ulysses
Table 4: Per iteration wall-clock time (seconds) of DISTFLASH ATTN and DeepSpeed Ulysses.
Method # GPUs Sequence Length Time Speedup # GPUs Sequence Length Time Speedup
Per GPU Total Per GPU Total
Llama-7B Llama-33H
DeepSpeed-Ulysses2x8 16K 256K 37.53 1.0x 2x8 16K 256K 56.63 1.0x
2x8 32K 512K 134.09 1.0x 2x8 32K 512K 202.89 1.0x
DISTFLASH ATTN2x8 16K 256K 30.21 1.21x 2x8 16K 256K 31.33 1.81x
2x8 32K 512K 106.37 1.26x 2x8 32K 512K 107.76 1.88x
DeepSpeed-Ulysses (Jacobs et al., 2023) uses all-to-all primitive to reduce the communication.
We evaluate a representative subset of experiments in Table 4 due to computational budget
limit. On experiments with regular heads models (Llama-7B), DISTFLASH ATTN achieves
1.26√óspeedup. On experiments on irregular heads models (Llama-33H), DISTFLASH ATTN
achieves 1.88√óspeedup. Essentially, DeepSpeed-Ulysses also paralleize on the attention
head dimension, and suffer from the same problems as analyzed in ¬ß 4.2.
8

--- PAGE 9 ---
0 50000 100000 150000 200000 250000
sequence length246Speedup
single machine
balanced schedule
unbalanced schedule
64K 128K 256K 512K
sequence length0.00.51.01.52.0Normalized iteration time2.33
2.05
1.65
1.331.95
1.44
1.081.01 1 1 1 1No overlap
overlap
no communicationFigure 4: Effect of balanced schedule (left) and the effect of overlapping (right).
4.5 Ablation Study
Effect of Load Balancing We study load balancing on an attention forward pass of LLaMA-
7B model, on 8 A100 40GB GPUs (Figure 4). The backward pass follows a similar analysis.
With an unbalanced schedule (Figure 1), the total work done is 36, where the total work
could be done in 8 units of time is 64. Thus, the expected speedup is 4.5x. In the balanced
schedule, the expected speedup is 7.2x. We scale the total sequence length from 4K to 256K.
The unbalanced version saturates in 4.5x speedup compared to a single GPU FlashAttention,
while the balanced version saturates at 7.5 √óspeedup. Both of them align with our theoretical
analysis and show the effectiveness of the balanced scheduling.
Table 5: Our checkpointing algorithm (‚ÄúOur
ckpt‚Äù) versus HuggingFace strategy (‚ÄúHF
ckpt‚Äù) on 8 A100s (batch size 1, Unit: seconds).
Method Sequence Length Per GPU
1K 2K 4K 8K 16K 32K
HF ckpt 0.84 1.29 2.64 6.93 21.44 76.38
Our ckpt 0.84 1.36 2.50 5.98 17.26 58.46
Speedup 1.0x 0.94x 1.06x 1.16x 1.24x 1.31xEffect of overlapping communication and
computation. We study the overlapping
communication on LLaMA-7B and 2 DGX
boxes (Figure 4). We find that overlap-
ping greatly reduces the communication
overhead. On a global sequence length of
128K, the communication overhead is re-
duced from 105% to 44%. This overlapping
scheme maximizes its functionality when
the communication overhead is less than
100%, where all communication can be potentially overlapped. Empirically, we find the
system only exhibits 8% and 1% overhead in these cases, a close performance to an ideal
system without communication.
Effect of rematerialization-aware checkpointing. We show in Table 5 the effects of the
proposed rematerialization-aware gradient checkpointing. Our method achieves 1.16x, 1.24x,
and 1.31x speedup at the sequence length of 8K, 16K, and 32K per GPU respectively. The
materialization-aware checkpointing strategy speeds up more at longer sequence lengths
where the attention dominates the computation.
4.6 Partition on the attention heads or sequence dimension
Megatron-LM and DeepSpeed-Ulysses are distributed systems that partition on attention
heads. While it allows seamless integration with the FlashAttention kernel, it has certain
limitations. These includes: (1) Not being able to utilize the pattern inside the attention
module, missing opportunities to reduce communication for causal, and grouped-query
attention (See ¬ßD). (2) not flexible to support arbitrary number of attention heads, and
(3) Importantly, its scalability is limited by the number of attention heads (in the scale of
several to several dozens), while the maximal number of parallelism degree for sequence
parallelism is at least several thousands. Given these reasons, we think it is worth pursuing
the sequence parallelism paradigm when distributing the attention module.
5 Conclusion
In this work, we introduce DISTFLASH ATTN, a distributed memory-efficient attention proto-
type for long-context transformer training based on sequence parallelism. DISTFLASH ATTN
9

--- PAGE 10 ---
presents novel system optimizations including load balancing for causal language model-
ings, overlapped communication with computation in the distributed attention computation,
and a re-materialization-aware checkpointing strategy. Experiments evaluate multiple fami-
lies of transformer models and on different cluster types, and over four strong distributed
system baselines. In particular, DISTFLASH ATTN has demonstrated up to 2.01 √óspeedup
and scales up to 8x longer sequences, compared to the popular system, Megatron-LM with
FlashAttention.
10

--- PAGE 11 ---
References
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr ¬¥on,
and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from
multi-head checkpoints. arXiv preprint arXiv:2305.13245 , 2023.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxan-
dra Cojocaru, M ¬¥erouane Debbah, ¬¥Etienne Goffinet, Daniel Hesslow, Julien Launay,
Quentin Malartic, et al. The falcon series of open language models. arXiv preprint
arXiv:2311.16867 , 2023.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150 , 2020.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with
sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning.
arXiv preprint arXiv:2307.08691 , 2023.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ¬¥e. Flashattention: Fast
and memory-efficient exact attention with io-awareness. Advances in Neural Information
Processing Systems , 35:16344‚Äì16359, 2022.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of
giant neural networks using pipeline parallelism. Advances in neural information processing
systems , 32, 2019.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters,
Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh
Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.
Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam
Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling
training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509 ,
2023.
Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez,
Kurt Keutzer, and Ion Stoica. Checkmate: Breaking the memory wall with optimal tensor
rematerialization. Proceedings of Machine Learning and Systems , 2:497‚Äì511, 2020.
Sylvain Jeaugey. Nccl 2.0. In GPU Technology Conference (GTC) , volume 2, 2017.
Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch,
Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large
transformer models. Proceedings of Machine Learning and Systems , 5, 2023.
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,
Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel
Haziza. xformers: A modular and hackable transformer modelling library. https:
//github.com/facebookresearch/xformers , 2022.
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E Gonzalez, Ion
Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on
context length, 2023.
Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism: Making 4d
parallelism possible. arXiv preprint arXiv:2105.13120 , 2021.
Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for
near-infinite context. arXiv preprint arXiv:2310.01889 , 2023.
11

--- PAGE 12 ---
Liyuan Liu, Jialu Liu, and Jiawei Han. Multi-head or single-head? an empirical comparison
for transformer training. arXiv preprint arXiv:2106.09650 , 2021.
Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv
preprint arXiv:1805.02867 , 2018.
Anton Osika. gpt-engineer, 2023. URL https://github.com/AntonOsika/gpt-engineer .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Advances in neural information
processing systems , 32, 2019.
Markus N Rabe and Charles Staats. Self-attention does not need o (ùëõ2)memory. arXiv
preprint arXiv:2112.05682 , 2021.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory
optimizations toward training trillion parameter models. In SC20: International Conference
for High Performance Computing, Networking, Storage and Analysis , pp. 1‚Äì16. IEEE, 2020.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and
Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using
model parallelism. arXiv preprint arXiv:1909.08053 , 2019.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav
Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint
arXiv:2212.10554 , 2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timoth ¬¥ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 ,
2023.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Funtowicz, et al. Huggingface‚Äôs
transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 ,
2019.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:
Transformers for longer sequences. Advances in neural information processing systems , 33:
17283‚Äì17297, 2020.
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,
Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling
fully sharded data parallel. arXiv preprint arXiv:2304.11277 , 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
12

--- PAGE 13 ---
A From FlashAttention to ùëéùë°ùë°ùëõ(¬∑)in D ISTFLASH ATTN
In this section, we provide the details of the (ùëéùë°ùë°ùëõ)(¬∑)kernel in DISTFLASH ATTN.(Alg 3).
For conceptual simplicity, we demonstrate it in the most vanilla version, without the
actual scheduling (e.g. load balancing and overlapping). We also demonstrate it with the
causal language modeling objective. The standalone attention is mainly borrowed from
the FlashAttention2 paper (Dao, 2023). To make it compatible with DISTFLASH ATTN, we
mainly revised the several points:
1.Accumulate results statistics ùëú,ùëöandùëôfrom previous computation, instead of
initializing them inside the function.
2.Pass an extra argument ‚Äùlast‚Äù, which means whether this is the last chunk of
attention computation. Only when it is true, we compute the logsumexp ùêø.
At a high level, on a worker ùëù,DISTFLASH ATTN first initializes local statistics ùëú,ùëö,ùëô,ùêø. Then
DISTFLASH ATTN loops over all its previous workers. In each iteration, it fetches the key
and the value from a worker and invokes the revised standalone attention to update local
statistics. At the end of the iteration, it needs to delete the remote key and value from HBM
so that the memory does not accumulate. At the last iteration of the loop, it additionally
calculates the logsumexp according to the final ùëöandùëô(triggered by the ‚Äùlast‚Äù variable
in the algorithm). At the end of the forward pass, worker ùëùhas the correct ùëö,ùëô,ùêø. The
backward pass is similar and conceptually simpler because we do not need to keep track of
statistics such as ùëöandùëô. Instead, we only need to use the logsumexp stored in the forward
pass.
Algorithm 1 (Vanilla) D ISTFLASH ATTN of workerùëù
Require: q ùëù,kùëù,vùëù
1:Initialize oùëù=o0,sùëù=s0=[m0,l0], where o0=0,l0=0, and m0=[‚àí‚àû¬∑¬∑¬∑‚àí‚àû]ùëá
2:oùëù,sùëù=ùëéùë°ùë°ùëõ(qùëù,kùëù,vùëù,oùëù,sùëù)
3:for1‚â§ùë° < ùëù do
4: r =(ùëù‚àíùë°) (modùëÉ)
5: Fetch from remote: worker pkùëü,vùëü‚Üê‚àí‚àí‚àí‚àí worker r
6: oùëù,sùëù=ùëéùë°ùë°ùëõ(qùëù,kùëü,vùëü,oùëù,sùëù)
7:end for
8:Return oùëù.
B Load-balancing Algorithm for Causal Modeling
In this section, we detail the design of our load-balancing algorithm for causal modeling.
We show the workload of each worker in all time steps in Figure 5 (before applying load-
balancing) and Figure 6 (after applying load-balancing) in an 8-worker scenario. The
communication schema is also reflected in both figures by comparing the tensors each
worker holds at the consecutive two time steps.
C Memory Consumption for Pipeline Parallelism
In this section, we show the memory consumption of Megatron-LM when training with
tensor parallelism and pipeline parallelism. As presented in table 6, memory consumption
are uneven across different pipeline stages, making scaling through pipeline parallelism
hard.
D Communication and memory analysis
Denote the hidden dimension as ùëë. InDISTFLASH ATTN, every worker needs to fetch key and
value chunks both of sizeùëÅ
ùëÉùëëbefore performing the corresponding chunk-wise computation.
13

--- PAGE 14 ---
Algorithm 2 (Balanced) D ISTFLASH ATTN of workerùëù
Require: q ùëù,kùëù,vùëù
1:Initialize oùëù=o0,sùëù=s0=[m0,l0], where o0=0,l0=0, and m0=[‚àí‚àû¬∑¬∑¬∑‚àí‚àû]ùëá
2:oùëù,sùëù=ùëéùë°ùë°ùëõ(qùëù,kùëù,vùëù,oùëù,sùëù)
3:for1‚â§t‚â§‚åäùëÉ
2‚åãdo
4:ùëü=(ùëù‚àíùë°) (modùëÉ)
5: ifùëù>tthen
6: Fetch key, value from remote: pkùë°,vùë°‚Üê‚àí‚àí‚àí‚àí r
7: oùëù,sùëù=ùëéùë°ùë°ùëõ(qùëù,kùëü,vùëü,oùëù,sùëù)
8: ift‚â†‚åäùëÉ
2‚åãand(ùëù+ùë°)>Pthen
9:ùëü2=(ùëù+ùë°) (modùëÉ)
10: Fetch result from remote: po‚Ä≤
ùëù,s‚Ä≤
ùëù‚Üê‚àí‚àí‚àí‚àíùëü2
11: oùëù,sùëù=ùëüùëíùë†ùëêùëéùëôùëí(oùëù,sùëù,o‚Ä≤
ùëù,s‚Ä≤
ùëù)
12: end if
13: else
14: ift‚â†‚åäùëÉ
2‚åãthen
15: Fetch query from remote: pqùëü‚Üê‚àí‚àír
16: oùëü,sùëü=ùëéùë°ùë°ùëõ(qùëü,kùëù,vùëù,o0,s0)
17: Send result to remote: poùëü,lùëü,mùëü‚àí‚àí‚àí‚àí‚àí‚àí‚Üí r
18: end if
19: end if
20:end for
21:Return oùëù.
Table 6: The memory consumption of Megatron-LM when training Llama-2H with tensor
parallelism (degree=2) and pipeline parallelism (degree=8) on 16xA100 40GB GPUs at the
sequence length of 128K. The memory consumption is highly uneven across pipeline stages.
Worker 1 Worker 2 Worker 3 Worker 4 Worker 5 Worker 6 Worker 7 Worker 8
node 1 31.5GB 31.4GB 28.7GB 28.7GB 26.0GB 26.0GB 24.6GB 24.6GB
node 2 21.8GB 21.8GB 20.5GB 20.5GB 17.9GB 17.8GB 32.0GB 32.1GB
Thus, the total communication volume in the ùëÉ-workers system is 2 √óùëÅ
ùëÉùëë√óùëÉ=2ùëÅùëë. With
the causal language objective, half of the keys and values do not need to be attended, halving
the forward communication volume to ùëÅùëë. In the backward pass, DISTFLASH ATTN needs
to communicate keys, values, and their gradients, which has 2 ùëÅùëëvolume. It adds up to 3 ùëÅùëë
as the total communication volume for DISTFLASH ATTN. In Megatron-LM (Korthikanti
et al., 2023), each worker needs to perform six all-gather and four reduce-scatter on aùëÅ
ùëÉùëë
size tensor, thus giving a total communication volume of 10 ùëÅùëë. Considering gradient
check-pointing, Megatron-LM will perform communication in the forward again, giving
a total volume of 14 ùëÅùëë. On the other hand, our communication volume remains 3 ùëÅùëë
because of the rematerialization-aware strategy. In conclusion, DISTFLASH ATTN achieves
4.7x communication volume reduction compared with Megatron-LM.
In large model training, we usually utilize techniques such as FSDP to also reduce the
memory consumed by model weights. In this case, We note that the communication
introduced by FSDP is only proportional to the size of model weights, which does not scale
up with long sequence length. We show the end-to-end speedup with FSDP in Table 1. For
clarity, we also note that DISTFLASH ATTN is orthogonal to FSDP and by default can be used
by itself. In the situations where the model uses MQA or GQA, DISTFLASH ATTN further
saves the communication volumes by the shared key and values, which we discuss in detail
in¬ß4.1. However, we also note that this is a theoretical analysis, where the wall-clock
time may differ because of factors such as implementations. In the experiment section, we
provide wall-clock end-to-end results for comparison.
14

--- PAGE 15 ---
Algorithm 3 DISTFLASH ATTN Pseudo code (forward pass)
Require: Matrices Qùëù,Kùëù,Vùëù‚ààRùëÅ
P√óùëëin HBM, block sizes ùêµùëê,ùêµùëü, rank
function standalone fwdq, k, v, o, ‚Ñì, m, causal, last
1:Divideùëûintoùëáùëü=l
ùëÅ
Pùêµùëüm
blocksùëû1,...,ùëûùëáùëüof sizeùêµùëü√óùëëeach,
2:and divide ùëò,ùë£in toùëáùëê=l
ùëÅ
Pùêµùëêm
blocksùëò1,...,ùëòùëáùëêandùë£1,...,ùë£ùëáùëê, of sizeùêµùëê√óùëëeach.
3:Divide the output ùëú‚ààRùëÅ
P√óùëëintoùëáùëüblocksùëúùëñ,...,ùëúùëáùëüof sizeùêµùëü√óùëëeach, and divide the
logsumexp ùêøintoùëáùëüblocksùêøùëñ,...,ùêøùëáùëüof sizeùêµùëüeach.
4:for1‚â§ùëñ‚â§ùëáùëüdo
5: Loadùëûùëñfrom HBM to on-chip SRAM.
6: Loadùëúùëñ‚ààRùêµùëü√óùëë,‚Ñìùëñ‚ààRùêµùëü,ùëöùëñ‚ààRùêµùëüfrom HBM to on-chip SRAM as ùëú(0)
ùëñ,‚Ñì(0)
ùëñ,ùëö(0)
ùëñ.
7: for1‚â§ùëó‚â§ùëáùëêdo
8: ifcausal andùëñ‚â§ùëóthen
9: Continue
10: end if
11: Loadùëòùëó,ùë£ùëófrom HBM to on-chip SRAM.
12: On chip, compute ùë†(ùëó)
ùëñ=ùëûùëñùëòùëá
ùëó‚ààRùêµùëü√óùêµùëê.
13: On chip, compute ùëö(ùëó)
ùëñ=max(ùëö(ùëó‚àí1)
ùëñ,rowmax(ùë†(ùëó)
ùëñ)) ‚ààRùêµùëü,Àúùëù(ùëó)
ùëñ=exp(ùëÜ(ùëó)
ùëñ‚àí
ùëö(ùëó)
ùëñ)‚ààRùêµùëü√óùêµùëê(pointwise), ‚Ñì(ùëó)
ùëñ=ùëíùëöùëó‚àí1
ùëñ‚àíùëö(ùëó)
ùëñ‚Ñì(ùëó‚àí1)
ùëñ+rowsum(Àúùëù(ùëó)
ùëñ)‚ààRùêµùëü.
14: On chip, compute ùëú(ùëó)
ùëñ=diag(ùëíùëö(ùëó‚àí1)
ùëñ‚àíùëö(ùëó)
ùëñ)‚àí1ùëú(ùëó‚àí1)
ùëñ+Àúùëù(ùëó)
ùëñùë£ùëù
ùëó.
15: end for
16: On chip, compute ùëúùëñ=diag(‚Ñì(ùëáùëê)
ùëñ)‚àí1ùëú(ùëáùëê)
ùëñ.
17: Writeùëúùëñto HBM as the ùëñ-th block of ùëú.
18: iflastthen
19: On chip, compute ùêøùëñ=ùëö(ùëáùëê)
ùëñ+log(‚Ñì(ùëáùëê)
ùëñ).
20: Writeùêøùëñto HBM as the ùëñ-th block of ùêø.
21: end if
22:end for
23:Returnùëú,‚Ñì,ùëöand the logsumexp ùêø.
end function
24:Initialize Oùëù=(0)ùëÅ
P√óùëë‚ààRùëÅ
P√óùëë,‚Ñì(ùëù)=(0)ùëÅ
P‚ààRùëÅ
P,ùëöùëù=(‚àí‚àû) ùëÅ
P‚ààRùëÅ
P.
25:Oùëù,‚Ñìùëù,ùëöùëù,ùêøùëù= standalone fwd( Qùëù,Kùëù,Vùëù,Oùëù,‚Ñìùëù,ùëöùëù, True, p=1)
26:for1‚â§ùëü < ùëù do
27: Receive Kùëüand Vùëüfrom Remote workerùëüinto HBM.
28: Oùëù,‚Ñìùëù,ùëöùëù,ùêøùëù= standalone fwd( Qùëù,Kùë¶,Vùë¶,Oùëù,‚Ñìùëù,ùëöùëù, False, r=(p-1)
29: Delete Kùëüand Vùëüfrom HBM.
30:end for
31:Return the output Oùëùand the logsumexp ùêø.
15

--- PAGE 16 ---
Figure 5: Illustration of D ISTFLASH ATTN before applying load-balancing on 8 workers.
E Implementation Details
We build the kernel of DISTFLASH ATTN, modifying from the Triton kernel of FlashAt-
tention2 in 500 lines of codes (LoCs). We implement the load balancing and overlapping
scheduling n Python and NCCL Pytorch bindings in 1000 LoCs (Paszke et al., 2019; Jeaugey,
2017), and the checkpointing strategy in 600 lines of Pytorch. We use block sizes of 128
and the number of stages to 1 in the kernel for the best performance in our cluster. We
evaluate DISTFLASH ATTN using FSDP (inter-node if applicable) so that it consumes similar
memory than the Megatron-LM baseline for a fair comparison (Zhao et al., 2023). For fair
16

--- PAGE 17 ---
Figure 6: Illustration of D ISTFLASH ATTN after applying load-balancing on 8 workers.
8K 16K 32K 64K
sequence length050100150200Wall-clock time
12.9427.0874.35233.87
10.523.6647.2295.78attention
Other
Figure 7: Time breakdown of attention versus other modules in a forward pass, measured
with Flash-Attention (Dao, 2023) on a single 40GB A100 GPU. (Unit ms)
comparisons, we run all comparisons using the same attention backend. We also add sup-
port for Megatron-LM so that comparing with them can produce a more insightful analysis:
(1) not materializing the causal attention mask, greatly reducing the memory footprint. For
instance, without this support, Megatron-LM will run out of memory with LLaMA-7B at
a sequence length of 16K per GPU. (2) head padding where the attention heads cannot be
divided by device number. All results are gathered with Adam optimizer, 10 iterations of
warm-up, and averaged over the additional 10 iterations.
F Discussion on sparse attention
While this paper focuses on discussing the exact attention mechanism, we also provide
possible solutions for sparse patterns and hope it can inspire future works. In particular, we
discuss load balancing for local sliding windows and global attention (Beltagy et al., 2020).
17

--- PAGE 18 ---
Local sliding windows For local sliding windows, the workload is naturally (near) balanced,
regardless of single directional or bidirectional attention. Thus, simply disregarding the
attention logic to non-local workers suffices. For instance, in exact attention, worker 7 needs
to compute attention to all other workers. If the sliding window has a number of tokens
equal to that of one worker, then worker 7 only needs to attend to itself and tokens in worker
6. In other words, it only needs to fetch key and value from worker 6, and compute attention.
In terms of implementation change, the system merely needs to change the end condition of
the for loop (from looping worker 1 - worker 7 to looping only from worker 6 - worker 7).
Global attention In global attention, there are a certain number of global tokens that all
later tokens need to attend to, which are used to capture the global information. To adapt
DISTFLASH ATTN to this, one solution is to keep a replica of all the global tokens in each
worker, which is simple and practical as otherwise, the global tokens will need to be all-
gathered at each time step. The other possibility is to also split the global tokens evenly onto
all workers and use all-gather upon computation to further reduce the memory requirement.
18
