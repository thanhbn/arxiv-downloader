# HMT: Bộ Transformer Bộ Nhớ Phân Cấp cho Xử Lý Ngôn Ngữ Ngữ Cảnh Dài Hiệu Quả
Zifan He1, Yingqi Cao2, Zongyue Qin1, Neha Prakriya1,
Yizhou Sun1,và Jason Cong1
1Đại học California, Los Angeles,2Đại học California, San Diego
zifanhe1202@g.ucla.edu, yic033@ucsd.edu,
{qinzongyue, nehaprakriya, yzsun, cong}@cs.ucla.edu

## Tóm tắt
Các mô hình ngôn ngữ lớn dựa trên Transformer (LLM) đã được sử dụng rộng rãi trong các ứng dụng xử lý ngôn ngữ. Tuy nhiên, do hạn chế về bộ nhớ của các thiết bị, hầu hết chúng đều giới hạn cửa sổ ngữ cảnh. Mặc dù các mô hình tái phát trong các công trình trước đây có thể ghi nhớ các token trong quá khứ để kích hoạt ngữ cảnh không giới hạn và duy trì hiệu quả, chúng có kiến trúc bộ nhớ "phẳng". Những kiến trúc như vậy có hạn chế trong việc lựa chọn và lọc thông tin. Vì con người giỏi trong việc học tập và tự điều chỉnh, chúng tôi tin rằng việc bắt chước hệ thống phân cấp bộ nhớ của não bộ có lợi cho việc ghi nhớ của mô hình. Do đó, chúng tôi đề xuất Bộ Transformer Bộ Nhớ Phân Cấp (HMT)1, một khung mới tạo điều kiện cho khả năng xử lý ngữ cảnh dài của mô hình bằng cách bắt chước hành vi ghi nhớ của con người. Tận dụng tái phát cấp độ đoạn được tăng cường bộ nhớ, chúng tôi tổ chức hệ thống phân cấp bộ nhớ bằng cách bảo tồn các token từ các đoạn đầu vào ban đầu, truyền các embedding bộ nhớ dọc theo chuỗi, và thu hồi thông tin liên quan từ lịch sử. Đánh giá mô hình hóa ngôn ngữ tổng quát, các nhiệm vụ hỏi đáp, và nhiệm vụ tóm tắt, chúng tôi cho thấy HMT liên tục cải thiện khả năng xử lý ngữ cảnh dài của các mô hình hiện có. Hơn nữa, HMT đạt được chất lượng sinh ra có thể so sánh hoặc vượt trội so với các LLM ngữ cảnh dài với ít hơn 2∼57× tham số và ít hơn 2.5∼116× bộ nhớ suy luận, vượt trội đáng kể so với các mô hình tăng cường bộ nhớ trước đây.

## 1 Giới thiệu
Transformer (Vaswani et al., 2017) đã chứng minh sức mạnh của nó trong học tập theo ngữ cảnh và được sử dụng trong nhiều ứng dụng khác nhau trong xử lý ngôn ngữ (Dong et al., 2019) và thị giác máy tính (Dosovitskiy et al., 2020). Đối với mô hình transformer chỉ giải mã, mỗi khối transformer chứa một lớp tự chú ý và một mô-đun mạng truyền tiến. Một lớp tự chú ý được tối ưu hóa có độ phức tạp tính toán bậc hai và độ phức tạp không gian tuyến tính (Dao et al., 2022) liên quan đến độ dài chuỗi vì nó tính toán các tương tác giữa mỗi token và tất cả các token trước đó trong chuỗi. Để duy trì tốc độ suy luận và đáp ứng yêu cầu bộ nhớ, hầu hết các mô hình transformer áp dụng độ dài chuỗi tối đa. Ví dụ, mô hình Llama 3 được thiết kế để xử lý 8192 token (Dubey et al., 2024) và Llama 2 có thể xử lý tới 4096 token (Touvron et al., 2023). Tuy nhiên, các ứng dụng thực tế liên quan đến tài liệu dài, như tóm tắt sách (Rae et al., 2019) và các nhiệm vụ hỏi đáp suốt đời (Sun et al., 2019; Dai et al., 2022), có thể có luồng đầu vào khổng lồ hoặc thậm chí vô hạn.

Nghiên cứu hiện tại cố gắng xây dựng các transformer ngữ cảnh dài bằng cách sử dụng attention thưa (Beltagy et al., 2020; Zhang et al., 2021; Kitaev et al., 2020), các mô hình tăng cường truy xuất (Bertsch et al., 2023; Wu et al., 2022), và các mô hình chuỗi tái phát (Peng et al., 2023a; Gu and Dao, 2023; Rae et al., 2019). Tuy nhiên, những mô hình này đối mặt với ít nhất một trong hai vấn đề: (1) khó khăn trong việc thích ứng với các mô hình tương lai do thay đổi trong kiến trúc mô hình cốt lõi và (2) hiệu quả thấp cho đầu vào tầm xa dưới việc chuyển đổi ngữ cảnh thường xuyên. Trong công trình này, chúng tôi đề xuất Bộ Transformer Bộ Nhớ Phân Cấp (HMT), một khung mới để kích hoạt và tăng cường khả năng xử lý ngữ cảnh dài của các mô hình. HMT biến đổi các mô hình thành mô hình tái phát tăng cường bộ nhớ bắt chước hệ thống phân cấp bộ nhớ của não bộ và hành vi ghi nhớ của con người. Nó có các tính năng độc đáo sau:

**Ghi nhớ Phân cấp**: HMT bắt chước hệ thống phân cấp bộ nhớ của não bộ (Burgin, 2011) sử dụng cả token bộ nhớ đã học và token đầu vào hiện tại. HMT phân tầng bộ nhớ thành cảm giác, ngắn hạn, và dài hạn, với các tương tác giữa chúng.

**Cơ chế Truy xuất Bộ nhớ**: HMT bắt chước việc nhớ lại bộ nhớ bằng cách lưu trữ các embedding bộ nhớ được mã hóa tạo ra từ các lần lặp trước đó và tìm kiếm dựa trên sự liên quan đến các đoạn token hiện tại.

Một lợi thế chính của việc sử dụng HMT so với các mô hình tăng cường bộ nhớ khác là HMT là một khung plug-and-play độc lập với mô hình: các mô hình chỉ giải mã trong tương lai có thể trực tiếp phục vụ như mô hình xương sống của HMT để tăng cường khả năng xử lý ngữ cảnh dài của chúng mà không cần nỗ lực triển khai thêm. Với việc đào tạo chung và tinh chỉnh các tham số mới được giới thiệu và tham số gốc của mô hình xương sống, HMT có thể áp dụng cho một loạt rộng các LLM, bao gồm các mô hình dựa trên transformer và các mô hình không gian trạng thái. Những đóng góp của chúng tôi bao gồm:

• **HMT liên tục cải thiện chất lượng sinh ra của các mô hình với ngữ cảnh dài cho nhiều kiến trúc mô hình khác nhau.** Chúng tôi chứng minh HMT trên cả kiến trúc dựa trên transformer và các mô hình không gian trạng thái. Đánh giá trên các bộ dữ liệu Wikitext-103, PG-19 (Rae et al., 2019), và PubMedQA (Jin et al., 2019) với nhiều ngữ cảnh được nối, HMT có thể cải thiện hiệu quả lên đến 25.5% về độ phức tạp và 1.0% độ chính xác dự đoán cao hơn so với các mô hình cơ sở.

• **HMT với các mô hình xương sống nhỏ có thể vượt trội so với các mô hình lớn được đào tạo trên các mẫu ngữ cảnh dài hơn, ngụ ý hiệu quả bộ nhớ cao.** Chúng tôi đánh giá HMT với các mô hình SmolLM (Allal et al., 2024), OPT (Zhang et al., 2022), và OpenLlamaV2 (Geng và Liu, 2023) trên chuẩn LongBench (Bai et al., 2023b). Tóm lại, HMT có thể đạt được kết quả metric có thể so sánh hoặc cao hơn với ít hơn 2∼57× tham số và yêu cầu bộ nhớ suy luận thấp hơn 2.5∼116× so với các mô hình ngôn ngữ lớn ngữ cảnh dài.

• **HMT vượt trội so với các phương pháp trước đây chuyên biệt cho xử lý ngữ cảnh dài hiệu quả bằng cách nén ngữ cảnh.** Chúng tôi so sánh HMT với RMT (Bulatov et al., 2022), LongMem (Wang et al., 2024), Memorizing Transformer (Wu et al., 2022), CCM (Kim et al., 2023), và HOMER (Song et al., 2024), những phương pháp SoTA gần đây về các phương pháp tăng cường bộ nhớ và phân cấp. Với mô hình xương sống cùng kích thước hoặc tương tự, HMT có chất lượng sinh ra tốt hơn trong cả mô hình hóa ngôn ngữ tổng quát và các nhiệm vụ QA. Hơn nữa, HMT có độ phức tạp bộ nhớ thấp hơn, cho thấy khả năng mở rộng tốt hơn khi độ dài đầu vào tăng.

## 2 Các Công trình Liên quan và Công thức Bài toán

Chúng tôi sẽ đầu tiên thảo luận về các nỗ lực hiện tại về các transformer tầm xa và các mô hình chuỗi tái phát cho xử lý ngôn ngữ ngữ cảnh vô hạn dài. Sau đó, chúng tôi làm nổi bật một vấn đề quan trọng trong các ứng dụng thực tế.

### 2.1 Transformer Ngữ cảnh Dài

Vì một trong những điểm nghẽn của transformer là độ phức tạp tính toán bậc hai của tự chú ý, một cách tiếp cận tự nhiên là làm thưa tính toán attention. Một mẫu attention thưa ngây thơ là attention cửa sổ trượt (Kovaleva et al., 2019), trong đó mỗi token chú ý đến các neighbor trong một cửa sổ địa phương. Tuy nhiên, điều này bỏ qua tương tác tầm xa giữa các từ. Các công trình hiện tại như Longformer (Beltagy et al., 2020) và Poolingformer (Zhang et al., 2021) mở rộng attention cửa sổ trượt bằng cách thêm các token chú ý toàn cục và áp dụng pooling để mở rộng vùng trường tiếp nhận. Unlimiformer (Bertsch et al., 2023) áp dụng phương pháp sinh ra tăng cường truy xuất bằng cách tìm kiếm top K token quan trọng nhất cho chuỗi đến. Sau đó nó áp dụng attention chỉ cho những token đó trong các bộ giải mã, dẫn đến tính toán được cắt tỉa với mất mát nhỏ. Tuy nhiên, sự đóng góp của các token ít liên quan có thể tích lũy theo thời gian và tác động đến việc sinh ra chuỗi tổng thể. Mặc dù những phương pháp này mở rộng độ dài ngữ cảnh có thể đạt được, chúng không thể ngăn chặn việc tăng tiêu thụ bộ nhớ khi độ dài đầu vào tăng. Ngoài ra, việc nén các token trong quá khứ bằng mô hình chuỗi tái phát có thể giảm tiêu thụ bộ nhớ bằng cách nén thông tin thành embedding có kích thước cố định.

### 2.2 Mô hình Chuỗi Tái phát

Mạng Neural Tái phát (RNN) đã được khám phá rộng rãi trong nghiên cứu xử lý chuỗi, bao gồm Long Short-term Memory (Hochreiter và Schmidhuber, 1997) và Gated Recurrent Unit (Chung et al., 2014). Chúng tiết lộ rằng RNN hoạt động tốt trong việc ghi nhớ thông tin trong quá khứ và thân thiện với phần cứng để triển khai các bộ tăng tốc tùy chỉnh (Chang et al., 2015). Tuy nhiên, RNN có lợi thế hạn chế trong việc học các mối quan hệ ngữ cảnh giữa các từ so với tự chú ý trong xử lý ngôn ngữ (Bahdanau et al., 2014). Một cách tiếp cận để giảm thiểu vấn đề này là tái phát thô, trong đó mô hình chia đầu vào thành các đoạn, thực hiện attention bên trong mỗi đoạn, và truyền các trạng thái (tức là thông tin nén dưới dạng embedding) giữa các đoạn. Compressive Transformer (Rae et al., 2019) tiếp tục lưu trữ và nén các trạng thái trước đó để tăng cường ghi nhớ. Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) sử dụng token bộ nhớ để tóm tắt và truyền thông tin đoạn mà không sửa đổi kiến trúc khối transformer. Về mặt lý thuyết, chúng có thể xử lý các chuỗi dài vô hạn, nhưng thông tin trước đó sẽ bị pha loãng sau nhiều lần tóm tắt và chất lượng sinh ra có thể giảm khi thông tin ít liên quan chiếm dụng bộ nhớ. Các công trình gần đây (Chevalier et al., 2023; Kim et al., 2023) nhằm tối ưu hóa thêm RMT để cải thiện chất lượng sinh ra bằng cách nối kết quả của các lần tóm tắt, nhưng điều này hi sinh hiệu quả bộ nhớ suy luận.

Một cách tiếp cận khác tăng cường RNN bằng cách bao gồm các tương tác giữa đầu vào hiện tại và các trạng thái trước đó để học các mối quan hệ ngữ cảnh theo cách tương tự như tự chú ý và tăng tốc tính toán với tích chập tuyến tính. Một trong những đại diện, RWKV (Peng et al., 2023a), là mô hình RNN được lấy cảm hứng từ transformer không attention (AFT) (Zhai et al., 2021). Nó bao gồm mô-đun time-mixing để học từ các trạng thái trước đó và mô-đun channel-mixing để học từ đầu ra trước đó. Mamba (Gu và Dao, 2023) là một phương pháp tái phát khác dựa trên mô hình không gian trạng thái sử dụng tích chập có cổng để tăng tốc suy luận mô hình. Những mô hình này tiết kiệm năng lượng và bộ nhớ với tốc độ đào tạo nhanh và có thể đạt hiệu suất cao trong các nhiệm vụ ghi nhớ (ví dụ: nhớ lại liên kết), nhưng có hạn chế trong việc nắm bắt các mối quan hệ ngữ cảnh và lọc thông tin không liên quan. Các công trình gần đây kết hợp transformer với Mamba (Lieber et al., 2024; Team et al., 2024) để giảm thiểu vấn đề này, nhưng điều này tái giới thiệu vấn đề mở rộng của các transformer.

### 2.3 Công thức Bài toán: Xử lý Ngữ cảnh Dài Thích ứng

Chúng tôi dự định phát triển một mô hình có thể xử lý đầu vào ngữ cảnh vô hạn dài với khả năng thích ứng ngữ cảnh: Dựa trên ngữ cảnh/chủ đề của luồng đầu vào, mô hình có thể chọn lọc thích ứng thông tin liên quan trong quá khứ để tăng cường hiệu quả, vì ngữ cảnh không liên quan có thể làm phân tâm mô hình (Shi et al., 2023). Trong các ứng dụng thực tế, bị hạn chế bởi băng thông và dung lượng bộ nhớ, cũng như tốc độ tạo dữ liệu, các tài liệu dài không thể được đọc như một tổng thể bởi phần cứng tính toán (Agerri et al., 2015). Hơn nữa, người dùng liên tục tương tác với mô hình ngôn ngữ có thể tham khảo chủ đề trước đó hoặc chuyển sang chủ đề khác có mức độ liên quan cao với thông tin trong quá khứ. Để đạt hiệu quả, hầu hết các mô hình tái phát cần mã hóa tất cả đầu vào trước đó trong các trạng thái, có thể chứa thông tin không liên quan và làm suy giảm chất lượng của mô hình.

## 3 Bộ Transformer Bộ Nhớ Phân Cấp

Ý tưởng chính của HMT là lưu trữ thông tin theo cách phân cấp và tìm kiếm thông tin liên quan trong toàn bộ hệ thống phân cấp bộ nhớ. Bảng 1 mô tả tất cả các ký hiệu chúng tôi sử dụng để minh họa kiến trúc HMT trong phần này.

**Bảng 1:** Ký hiệu được sử dụng để minh họa kiến trúc của HMT trong Phần 3 và Hình 1.

| KÝ HIỆU | Ý NGHĨA |
|---------|---------|
| Hn | EMBEDDING ẨNCHỨC VỤ ĐOẠN THỨ n |
| L | ĐỘ DÀI ĐOẠN |
| Hn[L−k, L) | k EMBEDDING CUỐI CỦA Hn |
| Hn[0, j) | j EMBEDDING ĐẦU CỦA Hn |
| BBM (·) | MÔ HÌNH XƯƠNG SỐNG |
| Pn | EMBEDDING PROMPT GHI NHỚ |
| Hout n | EMBEDDING ẨN CHO SINH LOGITS |
| Mn | EMBEDDING BỘ NHỚ CỦA ĐOẠN THỨ n |
| N | SỐ EMBEDDING BỘ NHỚ ĐÃ CACHE |
| M[n−N+1,n) | EMBEDDING BỘ NHỚ ĐÃ CACHE |
| T | EMBEDDING PROMPT TÓM TẮT ĐOẠN |
| Sn | EMBEDDING TÓM TẮT CỦA ĐOẠN THỨ n |

### 3.1 Quy trình Tổng thể

Cho một mô hình xương sống để tăng cường, HMT chia đầu vào thành các đoạn L-token và hoạt động trên các embedding ẩn của các đoạn token ({Hn}∞ n=0), được tạo ra bởi lớp embedding token của mô hình xương sống. Đối với mỗi đoạn n, HMT thực hiện qua bốn bước được hiển thị trong Hình 1:

1) **Mã hóa biểu diễn** bởi mô hình xương sống, mã hóa một phần của đoạn chứa bản chất của chủ đề đang diễn ra thành một embedding duy nhất để đại diện cho ngữ cảnh của nó, ký hiệu là Hn.

2) **Tìm kiếm bộ nhớ**, sử dụng ngữ cảnh hiện tại như một truy vấn để tìm thông tin liên quan trong bộ nhớ.

3) **Thêm bộ nhớ cảm giác**, tăng cường đoạn để nắm bắt thông tin trong đoạn trước đó và thông tin liên quan khác.

4) **Giải mã và tóm tắt**, xử lý đoạn được tăng cường để có được embedding ẩn cho việc sinh logits và embedding bộ nhớ tóm tắt đoạn được tăng cường.

Hai bước đầu tiên là cơ chế truy xuất bộ nhớ được thảo luận trong Phần 3.2. Các bước 3 và 4 được giải thích trong Phần 3.3 cùng với khái niệm ghi nhớ phân cấp.

**Hình 1:** Quy trình tổng thể của HMT. Đối với một đoạn, (1) HMT sẽ đầu tiên thực hiện mã hóa biểu diễn, sử dụng embedding prompt tóm tắt đoạn (T) để tóm tắt một phần của đoạn. (2) Embedding tóm tắt đoạn được tạo ra (Sn) được sử dụng với các embedding bộ nhớ đã cache cho tìm kiếm bộ nhớ với cross attention. Đầu ra là embedding prompt ghi nhớ (Pn) chứa thông tin liên quan đến đoạn hiện tại. (3) Embedding prompt ghi nhớ và k embedding cuối từ đoạn trước đó sẽ tăng cường đoạn. (4) Mô hình xương sống (BBM) sẽ xử lý đoạn được tăng cường và tạo ra embedding ẩn cho logits (Hout n) và embedding bộ nhớ (Mn), sẽ được đẩy vào bộ nhớ dài hạn.

### 3.2 Cơ chế Truy xuất Bộ nhớ

Để xử lý việc chuyển đổi ngữ cảnh và ngăn chặn sự can thiệp của ngữ cảnh không liên quan, HMT thực hiện truy xuất bộ nhớ để trích xuất chỉ thông tin liên quan từ kiến thức trong quá khứ. Cơ chế truy xuất bộ nhớ bao gồm ba bước: trích xuất biểu diễn, tìm kiếm bộ nhớ, và tăng cường bộ nhớ.

**Mã hóa Biểu diễn**: Được mô tả trong Bước 1 của Hình 1, HMT chọn j embedding đầu tiên từ các embedding ẩn của đoạn thứ n, Hn, để trích xuất chủ đề của đoạn. Các embedding được tăng cường với embedding prompt tóm tắt đoạn T. T là embedding tham số có thể học được, được triển khai để prompt mô hình xương sống (BBM) tóm tắt đoạn bằng soft prompt tuning (Liu et al., 2023). Thay vì trích xuất từ embedding token của BBM, chúng tôi làm cho T có thể học được để cho phép không gian embedding prompt lớn hơn cho việc tóm tắt. Mô hình xương sống sau đó sẽ xử lý các embedding được tăng cường và tạo ra một embedding mới ở cuối đầu ra như biểu diễn của đoạn:

Sn=BBM ([T||Hn[0, j)||T])[j, j+ 1) (1)

trong đó Sn là embedding tóm tắt của đoạn thứ n duy nhất, BBM (·) là mô hình xương sống, và "||" là toán tử nối. Sn sẽ được sử dụng cho tìm kiếm bộ nhớ.

**Tìm kiếm Bộ nhớ**: Được hiển thị trong Bước 2 của Hình 1, Sn được sử dụng như một truy vấn để tìm các embedding bộ nhớ liên quan được tạo ra từ Bước 4 khi xử lý các đoạn trước đó. Chúng tôi giữ một cửa sổ trượt của N embedding (M[n−N+1,n)) và sau đó tính toán:

Qn=SnWq, Kn=M[n−N+1,n)Wk (2)

Pn=softmax (QnKT n√dh)M[n−N+1,n) (3)

trong đó dh là chiều ẩn của cross attention. Tính toán tương tự như cross-attention mà không có phép chiếu value và output. Softmax (QnKT n√dh) tính toán điểm tương tự được chuẩn hóa và áp dụng trực tiếp cho M[n−N+1,n) để đảm bảo phân phối tương tự của giá trị đầu ra và các token bộ nhớ cũ. Chúng tôi kỳ vọng rằng phép chiếu Wq và Wk có thể được đào tạo sao cho các tóm tắt chứa ngữ cảnh tương tự có điểm attention cao sau khi chiếu.

Đầu ra của tìm kiếm bộ nhớ là embedding prompt ghi nhớ Pn chứa thông tin liên quan đến đoạn thứ n. Nó sẽ được áp dụng để tăng cường đoạn thứ n. Lưu ý rằng bộ nhớ của HMT là tích lũy: embedding bộ nhớ thứ n chứa thông tin của tất cả n−1 đoạn trước đó, với mất mát thông tin cao hơn cho các đoạn cũ hơn. Chúng tôi hy vọng rằng việc truy xuất bộ nhớ sẽ tăng cường bộ nhớ liên quan và giảm mất mát này.

Trong thực tế, mã hóa biểu diễn được thực hiện song song với suy luận mô hình trên GPU vì chúng là các tác vụ độc lập. Tìm kiếm bộ nhớ có độ phức tạp thời gian O(N), và cũng có thể chạy song song với suy luận đoạn khi N nhỏ (ví dụ: N= 300). Do đó, overhead thời gian chạy tổng thể của HMT là không đáng kể.

### 3.3 Ghi nhớ Phân cấp

Bộ nhớ của con người có thể được phân loại thành ba tầng: bộ nhớ cảm giác, bộ nhớ ngắn hạn, và bộ nhớ dài hạn (Burgin, 2011). Bộ nhớ cảm giác đề cập đến bộ nhớ rất ngắn hạn được tạo ra từ thông tin cảm giác, như thị giác và thính giác. Bộ nhớ ngắn hạn và dài hạn là những ký ức lâu dài, được phân biệt bởi thời gian chúng tồn tại trong não. HMT được lấy cảm hứng từ hệ thống phân cấp bộ nhớ này.

**Bộ nhớ Cảm giác**: Bộ nhớ cảm giác cho đoạn thứ n đề cập đến k embedding token cuối của Hn−1, Hn−1[L−k, L). Khi suy luận đoạn thứ n, HMT sẽ tăng cường các embedding token tương ứng Hn bằng cách thêm vào trước nó với Hn[L−k, L), được hiển thị trong Bước 3 của Hình 1.

**Bộ nhớ Ngắn hạn**: HMT sẽ mã hóa đoạn thành một embedding phục vụ như một "tóm tắt" của đoạn. Đầu tiên, HMT sẽ thêm và đặt trước embedding prompt ghi nhớ Pn vào đoạn được tăng cường. Điều này hướng dẫn mô hình xương sống nén đoạn và ngữ cảnh liên quan thành embedding tóm tắt với nhận thức về vị trí tương đối của các ngữ cảnh. Như được mô tả trong Bước 4 của Hình 1, chúng tôi đào tạo HMT sao cho

H=BBM (Pn||Hn−1[L−k, L)||Hn||Pn) (4)

Hout n||Mn=H[k+ 1, L+k+ 2) (5)

trong đó Mn là embedding bộ nhớ của đoạn thứ n. Hout n là một tập hợp L embedding ẩn sẽ được sử dụng để tạo ra logits.

**Bộ nhớ Dài hạn**: Mỗi embedding bộ nhớ được tạo ra sẽ được cache như bộ nhớ dài hạn. Các embedding đã cache sẽ được sử dụng như đầu vào của cơ chế truy xuất bộ nhớ để tạo ra embedding token ghi nhớ Pn cho mỗi đoạn như được minh họa trong các phần trước đó.

## 4 Thí nghiệm

Chúng tôi đánh giá HMT với nhiều mô hình xương sống khác nhau bao gồm SmolLM 135M (Allal et al., 2024), OPT 350M, OPT 2.7B (Zhang et al., 2022), OpenLlamaV2 3B (Geng và Liu, 2023), RWKV 3B (Peng et al., 2023a), và Llama 2 7B (Touvron et al., 2023), dưới cùng ràng buộc bộ nhớ (tức là cùng cửa sổ ngữ cảnh tối đa). Hơn nữa, chúng tôi thử nghiệm một số mô hình nhắm mục tiêu ngữ cảnh dài (Mamba 370M (Gu và Dao, 2023), Yi-6B-200K (Young et al., 2024), và Mistral 7B (Jiang et al., 2023)) để chứng minh lợi ích mà HMT có về chất lượng sinh ra và tiêu thụ bộ nhớ. Chúng tôi đánh giá HMT với các mô hình không gian trạng thái (RWKV và Mamba) làm xương sống vì chúng tôi tin rằng các mô hình đã có thể xử lý đầu vào vô hạn dài sẽ hưởng lợi nhiều hơn từ HMT. Tất cả các mô hình được đề cập đều được đào tạo và đánh giá trên 4 GPU AMD MI210, có thể xử lý các mô hình lên đến 7B tham số. Chúng tôi thử nghiệm thêm HMT trên 4 GPU NVIDIA A100-80GB cho mô hình Qwen 2.5 14B (Bai et al., 2023a) để xác minh khả năng mở rộng của nó với các mô hình lớn hơn và đạt được sự cải thiện hiệu quả nhất quán.

Để tinh chỉnh các tham số bổ sung được giới thiệu bởi HMT, chúng tôi sử dụng bộ dữ liệu RedPajamaV2 (Computer, 2023) để pre-train từng mô hình. Lưu ý rằng HMT giới thiệu các siêu tham số mô hình mới trên mô hình xương sống (L, j, N, và k). Một cấu hình phổ biến là L= 1024, j= 512, N= 300, và k= 32, và chúng tôi điều chỉnh các giá trị này cho từng mô hình để đạt hiệu suất tốt nhất. Để so sánh với các công trình trước đây (RMT, LongMem, Memorizing Transformer, CCM), chúng tôi áp dụng cùng mô hình xương sống nếu phương pháp có thể áp dụng cho bất kỳ mô hình nào, hoặc tìm mô hình xương sống có kích thước tương tự nếu phương pháp yêu cầu kiến trúc đặc biệt.

Đối với chuẩn ngữ cảnh dài, chúng tôi chọn các tập con (NarrativeQA, Qasper, và MultiFieldQA-en cho QA tài liệu đơn; HotpotQA, 2WikiMQA, và MuSiQue cho QA đa tài liệu; GovReport, QMSum, và Multi-News cho tóm tắt; TriviaQA cho học few-shot) từ một chuẩn được thừa nhận rộng rãi, LongBench (Bai et al., 2023b), và đo lường chúng so với các mô hình được báo cáo trong bảng xếp hạng LongBench. Tuy nhiên, độ dài tài liệu trung bình tối đa của các tập thử nghiệm trong LongBench ngắn hơn 20k từ, không quá dài đối với các mô hình ngữ cảnh dài hiện đại. Để hiểu rõ hơn khả năng xử lý ngữ cảnh dài của HMT dưới các kịch bản ngữ cảnh khác nhau, chúng tôi nghiên cứu thêm HMT trên các mẫu dữ liệu được tạo thủ công và có thể kiểm soát. Đối với các bộ dữ liệu được tạo thủ công, chúng tôi suy ra từ các bộ dữ liệu hiện có để tạo thành đầu vào dài. Đối với các nhiệm vụ ngôn ngữ tổng quát, các mô hình được thử nghiệm cho các nhiệm vụ sinh token tiếp theo với Wikitext-103 (Merity et al., 2016) (2-3k từ mỗi mẫu) và bộ dữ liệu PG-19 (Rae et al., 2019) (trung bình 69k từ mỗi mẫu). Các mẫu sẽ được nối hoặc chia thành chunks để tạo thành các mẫu dài hơn và khảo sát mối quan hệ giữa độ dài đầu vào và hiệu quả của mô hình. Đối với các nhiệm vụ hỏi đáp, chúng tôi chọn PubMedQA (Jin et al., 2019), là bộ dữ liệu hỏi đáp y sinh với các ngữ cảnh tương ứng. Chúng tôi tạo ra bộ dữ liệu để đánh giá HMT với đầu vào đa ngữ cảnh, được mô tả trong Phụ lục I.

## 5 Kết quả và Quan sát Chính

Trong phần này, chúng tôi minh họa kết quả chính của HMT. Nhiều nghiên cứu ablation hơn ở Phụ lục E và G.

### 5.1 Tác động đến Mô hình Xương sống

Bằng cách giới thiệu thêm 0.5% ∼1.3% (1.77M ∼33.5M) tham số, HMT có thể tăng cường các mô hình với nhiều kiến trúc khác nhau để cải thiện chất lượng sinh ra khi xử lý đầu vào ngữ cảnh dài. Chúng tôi chứng minh tính năng này với các nhiệm vụ mô hình hóa ngôn ngữ tổng quát và hỏi đáp.

**HMT liên tục cải thiện các mô hình xương sống trong các nhiệm vụ mô hình hóa ngôn ngữ tổng quát khi xử lý đầu vào dài.** Hình 2 và 3 so sánh độ phức tạp của các mô hình OPT 2.7B, RWKV 3B, và OpenLlamaV2 3B có và không có HMT trên các bộ dữ liệu Wikitext-103 và PG-19. Qua đầu vào kéo dài từ 2k ∼100k token, HMT liên tục nâng cao chất lượng sinh ra của tất cả các mô hình này.

**Bảng 2:** Khả năng mở rộng của HMT. PPL thử nghiệm trung bình được tính bằng cách lấy PPL trung bình cho các mẫu trong mỗi độ dài chuỗi trong thí nghiệm.

| MÔ HÌNH | PPL THỬ NGHIỆM TB (WIKITEXT) (↓) |
|---------|----------------------------------|
| OPT 350M | 15.11 |
| HMT + OPT 350M | 14.28 (-5.8%) |
| OPT 2.7B | 12.12 |
| HMT + OPT 2.7B | 8.61 (-28.9%) |
| RWKV 430M | 19.33 |
| HMT + RWKV 430M | 16.10 (-16.6%) |
| RWKV 3B | 13.30 |
| HMT + RWKV 3B | 9.93 (-25.3%) |

Hơn nữa, Bảng 2 trình bày cách các cải thiện được đạt bởi HMT mở rộng với kích thước mô hình cho các mô hình cùng họ. Để tăng cường thêm lập luận của chúng tôi rằng HMT có thể có lợi cho các mô hình lớn hơn, chúng tôi đánh giá HMT với Qwen 2.5 14B sử dụng 4 GPU A100-80GB cho đào tạo. Như được mô tả trong Hình 4, HMT vẫn có thể tăng hiệu quả của mô hình xương sống trên PG-19.

Lưu ý rằng sự cải thiện không nhất thiết chỉ được đóng góp bởi các tham số bổ sung. Có nhiều tham số hơn không phải lúc nào cũng dẫn đến hiệu suất cao hơn. Ví dụ, HMT tăng cường OPT 2.7B để đạt được độ phức tạp thấp hơn OpenLlama 3B với ít hơn 20.7% tham số, trong khi OPT 2.7B hoạt động kém hơn mà không có HMT. Phần 5.2 mô tả thêm các ví dụ về HMT đạt được chất lượng sinh ra vượt trội với các mô hình nhỏ hơn.

**HMT tăng cường khả năng lý luận ngữ cảnh đáp án dài và khả năng dự đoán đáp án ngắn trong các nhiệm vụ hỏi đáp.** Một trong những trường hợp sử dụng của HMT là xử lý các nhiệm vụ hỏi đáp liên quan đến nhiều ngữ cảnh. Do đó, chúng tôi chọn bộ dữ liệu PubMedQA và suy ra các mẫu QA ngữ cảnh dài với số lượng ngữ cảnh có thể kiểm soát để đánh giá hiệu quả của HMT. Hai metric được sử dụng: đối với đáp án dài, chúng tôi tính PPL để đánh giá lý luận ngữ cảnh của HMT; đối với đáp án ngắn, chúng tôi đo độ chính xác phản hồi. Như thấy trong Hình 5 và 6, đối với các mẫu có 2 đến 10 ngữ cảnh, HMT tăng hiệu quả trong PPL 9.48% cho đáp án dài. Đối với các nhiệm vụ đáp án ngắn, HMT chính xác hơn 1.0% so với mô hình xương sống và thể hiện lợi thế đáng kể khi các mẫu có nhiều ngữ cảnh hơn. Tóm lại, HMT tăng cả tính đúng đắn và khả năng lý luận của các mô hình trong các nhiệm vụ QA ngữ cảnh dài.

### 5.2 So sánh với Mô hình Ngữ cảnh Dài

Kết hợp với các mô hình nhỏ và ngữ cảnh ngắn, HMT có thể hiệu quả hơn các mô hình lớn được đào tạo trên đầu vào ngữ cảnh dài. Bảng 3 hiển thị kết quả metric của các mô hình tăng cường HMT trên các tập con của LongBench (Bai et al., 2023b) và so sánh chúng với các mô hình lớn chuyên biệt cho ngữ cảnh dài. Các tập con chứa nhiều nhiệm vụ sinh ra khác nhau, bao gồm QA tài liệu đơn/đa, tóm tắt, và học few-shot. Với yêu cầu bộ nhớ suy luận thấp hơn đáng kể, HMT áp dụng cho các mô hình nhỏ có thể đạt được metric có thể so sánh hoặc tốt hơn so với các mô hình lớn, cho thấy lợi thế về tài nguyên đáng kể. Cụ thể, chúng tôi quan sát rằng HMT với các mô hình nhỏ hoạt động tốt trong việc sinh ra phản hồi ngắn cho đầu vào dài và đa ngữ cảnh, nhờ vào khả năng lọc ngữ cảnh của nó. Tuy nhiên, nó thể hiện hiệu suất có thể so sánh hoặc yếu hơn trong việc sinh ra phản hồi dài, vì các mô hình nhỏ có giới hạn sinh token ngắn hơn so với các mô hình lớn.

**Hình 2:** Độ phức tạp thử nghiệm của HMT, RMT, và ba mô hình cơ sở (OPT 2.7B, RWKV 3B, OpenLlamaV2 3B) với bộ dữ liệu Wikitext-103. HMT vượt trội so với RMT 13.0% cho OPT và 10.8% cho OpenLlamaV2. Đối với RWKV, HMT thậm chí có thể tăng hiệu quả 16.5%, trong khi RMT làm xấu đi hiệu quả.

**Hình 3:** Độ phức tạp thử nghiệm của HMT, RMT, và ba mô hình cơ sở (OPT 2.7B, RWKV 3B, OpenLlamaV2 3B), được đánh giá trên bộ dữ liệu PG-19. HMT vượt trội so với RMT 3.98% cho OPT và 6.85% cho OpenLlamaV2. Đối với RWKV, HMT có thể cải thiện hiệu quả 9.96%.

**Hình 4:** Độ phức tạp thử nghiệm của HMT, RMT, và mô hình cơ sở cho Qwen 2.5 14B trên bộ dữ liệu PG-19. HMT tăng hiệu quả của mô hình cơ sở 10.0%, trong khi RMT làm xấu đi hiệu quả của nó.

Hơn nữa, việc áp dụng HMT cho các mô hình ngữ cảnh dài có thể cải thiện thêm hiệu quả của chúng và giảm tiêu thụ bộ nhớ suy luận. Ví dụ, GPU AMD MI210 không thể xử lý suy luận đầu vào 30k token với mô hình Yi-6B-200K do ràng buộc bộ nhớ. Áp dụng chiến lược cửa sổ trượt với cửa sổ 5.2K-token (Yi-6B-SW-5.2K), mô hình tiêu thụ 44.8 GB VRAM. Ngược lại, HMT + Yi-6B-200K chỉ yêu cầu 33.9 GB VRAM để xử lý 30k token với độ dài đoạn nhỏ (512 token), với cải thiện hiệu quả 2%. Bảng 4 trình bày hiệu quả của các mô hình tầm xa trên Wikitext-103 so với một số mô hình tăng cường HMT, bao gồm các mô hình Mamba và Mistral.

**Bảng 3:** Kết quả metric của các mô hình nhỏ tăng cường HMT và các mô hình lớn được đào tạo trên ngữ cảnh dài hơn. Các mô hình với HMT có thể xử lý ngữ cảnh vô hạn dài, nhưng chỉ giữ cache KV có độ dài cố định (giá trị trong ngoặc). Chúng tôi đánh giá trên các tập con của LongBench, bao gồm QMSum (QMS), MuSiQue (MSQ), Qasper (QASP), NarrativeQA (NQA), MultiFieldQA-en (MFQA-en), GovReport (GR), TriviaQA (TQA), HotpotQA (HQA), 2WikiMQA (2WMQA), và MultiNews (MN). Mem Req chỉ ra yêu cầu bộ nhớ suy luận tối thiểu (để lưu trữ tham số và cache KV). Suy luận thực tế có thể yêu cầu VRAM lớn hơn.

[Bảng chi tiết với các kết quả so sánh]

**Hình 5:** Chất lượng đáp án dài của RMT và HMT áp dụng trên Llama-2 7B, được đánh giá trên bộ dữ liệu PubMedQA. HMT hiệu quả hơn RMT 8.98%.

**Hình 6:** Độ chính xác phản hồi ngắn của RMT và HMT áp dụng trên Llama-2 7B, được đánh giá trên bộ dữ liệu PubMedQA. HMT chính xác hơn RMT 1.8%.

**Bảng 4:** Chất lượng của các mô hình ngữ cảnh dài và HMT với các mô hình xương sống khác nhau. Kích thước đầu vào là 30k token và bộ dữ liệu là Wikitext-103.

| MÔ HÌNH | NGỮCẢNHTỐIĐA | PPL THỬNGHIỆM (WIKITEXT) |
|---------|---------------|--------------------------|
| RWKV 3B | ∞ | 13.13 |
| MAMBA 370M | ∞ | 87.08 |
| YI-6B-200K | 200K | OOM² |
| YI-6B-SW-5.2K | 200K | 6.89 |
| MISTRAL-7B | 32K | 5.47 |
| HMT + OPT 350M | ∞(1024) | 13.67 |
| HMT + OPENLLAMA V2 3B | ∞(512) | 7.04 |
| HMT + RWKV 3B | ∞(256) | 10.94 |
| HMT + MAMBA 370M | ∞(256) | 16.71 |
| HMT + YI-6B-200K | ∞(512) | 6.75 |
| HMT + MISTRAL-7B | ∞(512) | 5.12 |

### 5.3 So sánh với Các Phương pháp Tăng cường Bộ nhớ và Phân cấp

Một mô hình tăng cường bộ nhớ phổ biến là recurrent memory transformer (Bulatov et al., 2022) (RMT). Đánh giá của chúng tôi chỉ ra rằng HMT nhìn chung tốt hơn ở cả nhiệm vụ mô hình hóa ngôn ngữ và hỏi đáp so với RMT, được minh họa trong Hình 2, 3, 5, và 6. Khoảng cách cải thiện đặc biệt đáng kể đối với các mô hình tái phát như RWKV. HMT có thể tăng cường thêm hiệu quả của RWKV trong khi RMT sẽ làm giảm hiệu suất cho cả hai bộ dữ liệu, như được chứng minh trong Hình 3. Vì RWKV đã nén các token trong quá khứ và truyền các trạng thái ẩn dọc theo chuỗi, việc áp dụng RMT cho RWKV tái trọng số thông tin trong quá khứ được nén trong các trạng thái định kỳ. Điều này ban đầu được thực hiện bởi mô-đun time-mixing của RWKV. Do đó, lợi thế của việc tăng cường bộ nhớ bị hạn chế. Do vấn đề gradient vanishing, mô hình khó đào tạo hơn với RMT, dẫn đến hiệu suất kém hơn. Tuy nhiên, chúng tôi tin rằng cơ chế truy xuất bộ nhớ trong HMT giúp RWKV chọn các trạng thái ẩn trước đó với mức độ liên quan cao nhất, tăng cường hiệu quả của nó. Một lợi thế khác của HMT so với RMT là khả năng mở rộng với các mô hình lớn: trong khi RMT áp dụng cho Qwen 2.5 14B dẫn đến giảm hiệu quả so với suy luận trực tiếp với mô hình xương sống, HMT tiếp tục tăng cường hiệu quả, như được minh họa trong Hình 4.

Hơn nữa, so với các mô hình tăng cường bộ nhớ khác, HMT không chỉ dễ sử dụng mà còn có chất lượng sinh ra cao hơn. Bảng 5 chọn ba phương pháp tăng cường bộ nhớ (Memorizing Transformer (Wu et al., 2022), LongMem (Wang et al., 2024), và CCM-concat (Kim et al., 2023)) và so sánh chúng với HMT với các mô hình xương sống cùng kích thước hoặc tương tự. Chúng tôi chọn các bộ dữ liệu được sử dụng bởi các công trình gốc để so sánh công bằng. Memorizing transformer và LongMem yêu cầu sửa đổi kiến trúc cốt lõi của mô hình cơ sở. Các mô hình tương lai không thể dễ dàng áp dụng những sửa đổi như vậy. Nhìn chung, HMT vượt trội so với những phương pháp này. Chúng tôi cũng liệt kê độ phức tạp overhead bộ nhớ suy luận cho từng mô hình, trong đó L là tổng độ dài ngữ cảnh, li là độ dài đoạn suy luận, lm là kích thước bộ nhớ (L > lm > li), và t là số lượng embedding bộ nhớ được nối cho CCM-concat. HMT có độ phức tạp bộ nhớ thấp nhất so với tất cả các phương pháp trước đây.

**Bảng 5:** So sánh giữa HMT với các phương pháp tăng cường bộ nhớ trước đây (Memorizing Transformer, LongMem, và CCM-concat).

| MÔ HÌNH | PPL THỬNGHIỆM (WIKITEXT, 30K TOKEN) | OVERHEAD BỘ NHỚ |
|---------|-------------------------------------|-----------------|
| MEMTRM | 31.51 | O(L) |
| HMT + OPT 350M | 13.67 | O(li) |

| MÔ HÌNH | PPL THỬNGHIỆM (ARXIV, BIẾNĐỔI) | OVERHEAD BỘ NHỚ |
|---------|--------------------------------|-----------------|
| LONGMEM | 10.08 | O(lm) |
| HMT + QWEN 1.5-0.5B | 9.02 | O(li) |

| MÔ HÌNH | PPL THỬNGHIỆM (PG-19, 60K TOKEN) | OVERHEAD BỘ NHỚ |
|---------|----------------------------------|-----------------|
| CCM-CONCAT | 7.41 | O(t+li) |
| HMT + LLAMA 2 7B | 7.40 | O(li) |

Cuối cùng, chúng tôi so sánh HMT với HOMER (Song et al., 2024), một phương pháp nén đầu vào theo cách phân cấp để giảm độ dài của chúng cho suy luận. Về độ phức tạp bộ nhớ, HOMER yêu cầu bộ nhớ O(log(L)) để lưu trữ cây reduction, dẫn đến tăng tiêu thụ bộ nhớ đỉnh khi độ dài đầu vào tăng. Ngược lại, HMT duy trì độ phức tạp bộ nhớ đỉnh không đổi bất kể độ dài đầu vào. Về hiệu quả, HMT đạt được 9.9% độ phức tạp thấp hơn trên PG-19 so với HOMER với YaRN (Peng et al., 2023b) cho mở rộng ngữ cảnh. Như được hiển thị trong Hình 7, lợi ích của HMT trở nên đáng kể hơn khi độ dài đầu vào tăng, làm nổi bật khả năng mở rộng vượt trội với đầu vào dài hơn.

**Hình 7:** So sánh giữa HMT và HOMER mà không có mở rộng ngữ cảnh và với YaRN, tất cả áp dụng trên Llama 2 7B. Trung bình, HMT hiệu quả hơn HOMER với YaRN 9.9% trên PG-19.

## 6 Kết luận

Chúng tôi trình bày HMT, một khung để tăng cường khả năng xử lý ngôn ngữ tầm xa của các mô hình với chuyển đổi ngữ cảnh. Được lấy cảm hứng từ hệ thống phân cấp bộ nhớ của não bộ, HMT bắt chước hành vi ghi nhớ của con người bằng cách triển khai bộ nhớ phân cấp và cơ chế truy xuất bộ nhớ. HMT liên tục cải thiện chất lượng sinh ra của các mô hình xương sống. So với các LLM ngữ cảnh dài khác và các mô hình tăng cường bộ nhớ, HMT đạt được chất lượng sinh ra cao hơn với yêu cầu bộ nhớ thấp hơn. Mô hình của chúng tôi cung cấp khả năng tiếp cận LLM cho các ứng dụng hạn chế tài nguyên và đại diện cho một bước tiến trong các nhiệm vụ ngôn ngữ suốt đời.

## 7 Hạn chế và Công việc Đang tiến hành

• **Hiện tại, HMT sẽ lưu N embedding bộ nhớ cho tìm kiếm bộ nhớ, đây là một lớp cross-attention.** Khi N nhỏ (ví dụ: N= 300), đủ cho các mẫu 100k token, overhead là không đáng kể. Tuy nhiên, khi N tăng và các embedding bộ nhớ được lưu trữ trong các hệ thống phân cấp bộ nhớ vật lý khác nhau, overhead có thể đáng kể. Một cơ chế prefetch bộ nhớ thông minh có thể giảm thiểu overhead độ trễ, điều mà chúng tôi để lại như công việc tương lai.

• **Do đồ thị tính toán lớn của các mô hình khi đào tạo với BPTT, việc tinh chỉnh các tham số bổ sung được giới thiệu bởi HMT có thể tiêu tốn bộ nhớ, cản trở các thí nghiệm trên các mô hình quy mô lớn hơn.** Một cách hiệu quả hơn để mở rộng độ sâu BPTT mà không có overhead bộ nhớ là hướng nghiên cứu tương lai.

• **Mặc dù HMT chỉ sử dụng một cấp độ bộ nhớ dài hạn, người ta có thể sử dụng nhiều cấp độ bộ nhớ dài hạn để cải thiện hiệu quả truy cập thông tin.** Các kỹ thuật tương tự đã được sử dụng cho tối ưu hóa đa cấp trong thiết kế vật lý VLSI (Cong và Shinnerl, 2013; Chan et al., 2005).

## 8 Tuyên bố Đạo đức

Khả năng ghi nhớ thông tin của HMT mang lại tiện lợi cho cuộc sống hàng ngày của mọi người, đồng thời cũng gây ra lo ngại về rò rỉ quyền riêng tư thông qua cuộc trò chuyện với các agent mô hình ngôn ngữ. Tuy nhiên, với nỗ lực thêm để triển khai nó trên các thiết bị edge mà không có kết nối mạng, vấn đề này có thể được giải quyết.

## Lời cảm ơn

Nghiên cứu này được hỗ trợ một phần bởi trung tâm PRISM (000705769) dưới chương trình JUMP 2.0 của DARPA/SRC và tài trợ NSF SEED. Nó cũng được hỗ trợ bởi các đối tác công nghiệp CDSC (https://cdsc.ucla.edu/partners) và Chương trình AMD HACC.

## Tài liệu tham khảo

[Danh sách đầy đủ các tài liệu tham khảo được dịch giữ nguyên định dạng gốc]
