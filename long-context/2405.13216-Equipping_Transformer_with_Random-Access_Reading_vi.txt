# 2405.13216.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2405.13216.pdf
# Kích thước tệp: 545366 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Trang bị Transformer với Khả năng Đọc Truy cập Ngẫu nhiên
cho Hiểu biết Ngữ cảnh Dài
Chenghao Yang* 1Zi Yang* 2Nan Hua2
Tóm tắt
Mô hình hóa ngữ cảnh dài đặt ra một thách thức đáng kể đối với các mô hình ngôn ngữ lớn (LLM) dựa trên transformer do độ phức tạp bậc hai của cơ chế tự chú ý và các vấn đề với việc ngoại suy độ dài do việc tiền huấn luyện độc quyền trên đầu vào ngắn. Các phương pháp hiện tại giải quyết độ phức tạp tính toán thông qua các kỹ thuật như phân đoạn văn bản, phương pháp kernel, và chú ý có cấu trúc, và giải quyết các vấn đề ngoại suy độ dài thông qua mã hóa vị trí, tiếp tục tiền huấn luyện, và kỹ thuật dữ liệu. Những phương pháp này thường yêu cầu truy cập tuần tự vào tài liệu, đòi hỏi phải đọc từ token đầu tiên đến token cuối cùng. Chúng tôi cho rằng đối với việc đọc tài liệu dài hướng đến mục tiêu, việc truy cập tuần tự như vậy là không cần thiết, và một mô hình được huấn luyện thành thạo có thể học cách bỏ qua hàng trăm token ít liên quan hơn. Lấy cảm hứng từ hành vi đọc của con người và các quan sát thực nghiệm hiện có, chúng tôi đề xuất truy cập ngẫu nhiên, một chiến lược đọc mới cho phép transformer xử lý hiệu quả các tài liệu dài mà không cần kiểm tra từng token. Kết quả thực nghiệm từ các giai đoạn tiền huấn luyện, tinh chỉnh và suy luận xác nhận hiệu quả của phương pháp chúng tôi.

1. Giới thiệu
Ngữ cảnh dài đề cập đến một ngữ cảnh đầu vào vượt quá giới hạn độ dài tối đa,1 khiến không thể xử lý trong một bước suy luận duy nhất. Các ví dụ về đầu vào ngữ cảnh dài

*Đóng góp bằng nhau1University of Chicago (Công việc được thực hiện tại Google với tư cách là nghiên cứu sinh)2Google Research. Liên hệ: Chenghao Yang <yangalan1996@gmail.com>, Zi Yang <ziy@google.com>.

Công trình Sơ bộ. Bản quyền 2024 thuộc về (các) tác giả.

1Chúng tôi thừa nhận thành tựu gần đây trong việc xây dựng các mô hình có thể xử lý ngữ cảnh cực kỳ dài, như Gemini 1.5 (Reid et al., 2024). Tuy nhiên, không thể mở rộng cửa sổ ngữ cảnh vô hạn mà không phân đoạn nó tại một thời điểm nào đó, và chúng tôi tin rằng việc kết hợp phân đoạn với chiến lược đọc truy cập ngẫu nhiên mà chúng tôi đề xuất có thể hiệu quả hơn so với việc đọc trực tiếp đầu vào như một tổng thể.

bao gồm nhiều trang web (Zhou et al., 2023; Deng et al., 2024), sách (Mou et al., 2021), kho lưu trữ mã (Jimenez et al., 2023) và lịch sử đối thoại (Yang & Ettinger, 2023).

Các chiến lược hiện tại chủ yếu sử dụng mô hình truy cập tuần tự, trong đó cả trong quá trình huấn luyện và suy luận, mô hình xử lý các token của tài liệu theo thứ tự tuần tự gốc của chúng (Dong et al., 2023; Huang et al., 2023). Những phương pháp này sử dụng các cơ chế khác nhau để giải quyết các thách thức như độ phức tạp bậc hai—thông qua xử lý theo khối (Qiu et al., 2020; Tay et al., 2020a; Liu et al., 2022; Ivgi et al., 2023; Mohtashami & Jaggi, 2024), chú ý có cấu trúc (Beltagy et al., 2020; Guo et al., 2022; Xiao et al., 2024; Han et al., 2023), và các phép xấp xỉ tuyến tính (Choromanski et al., 2020; Peng et al., 2020; Ma et al., 2021; Nguyen et al., 2022)—hoặc để giảm thiểu các vấn đề ngoại suy độ dài bằng cách sử dụng các kỹ thuật như nhúng vị trí xoay (Peng et al., 2023; Su et al., 2024) và huấn luyện liên tục (Xiong et al., 2023; Fu et al., 2024).

Mặc dù những phương pháp này đã được áp dụng rộng rãi và chứng minh hiệu quả, chúng coi mọi token như nhau quan trọng và bỏ qua thực tế rằng đối với các truy vấn của người dùng, chỉ một phần nhỏ thông tin là có liên quan (Ding et al., 2020). Do đó, nhiều giải pháp được đề xuất vẫn phải chịu chi phí tính toán, đặc biệt trong các tương tác trực tuyến giữa con người và LLM. Các công trình gần đây trong sinh tăng cường truy xuất (RAG) (Lewis et al., 2020; Shi et al., 2023) đã cố gắng giải quyết điều này bằng cách kết hợp một bộ truy xuất bổ sung để bỏ qua ngữ cảnh không cần thiết và truy xuất có chọn lọc các phần có liên quan. Tuy nhiên, do hiểu biết không đầy đủ về toàn bộ ngữ cảnh dài và thiếu các tín hiệu giám sát mạnh mẽ, hiệu suất tổng thể của các hệ thống đa mô-đun như vậy bị giới hạn đáng kể bởi khả năng của bộ truy xuất (Mou et al., 2021; Zhang et al., 2022). Hơn nữa, trong khi các hệ thống RAG này yêu cầu một top-K được xác định trước cho bất kỳ truy vấn nào của người dùng, khung của chúng tôi xác định động mẫu truy cập dựa trên truy vấn và ngữ cảnh.

Lấy cảm hứng từ các chiến lược được quan sát thấy ở những người đọc thành thạo, những người tích cực tham gia với văn bản dài bằng cách phát triển dự đoán về nội dung sắp tới và đọc lướt có chọn lọc các phần không liên quan (Paris et al., 1991; Pressley & Afflerbach, 2012), cùng với bằng chứng gần đây cho thấy rằng các mô hình lớn có thể vốn dĩ có được khả năng truy cập nội dung tại các vị trí tùy ý trong quá trình tiền huấn luyện (Fu et al., 2024), chúng tôi đề xuất Đọc Truy cập Ngẫu nhiên.2 Phương pháp này tận dụng thông tin độ phức tạp cục bộ như một tiêu chí để bỏ qua văn bản không cần thiết, từ đó nâng cao đáng kể hiệu quả đọc.

ChunksBắt đầuChunksLiên tục
(không quan trọng)ChunksLiên tục
(không quan trọng)ChunksKết thúc
Bộ nhớ Bộ nhớ Bộ nhớMô hình I/O 
thực sự 
đã xảy raMô hình I/O Tùy chọn
Truy cập Tuần tự Truyền thống
Truy cập Ngẫu nhiên Được đề xuất
ChunksBắt đầuChunksLiên tục
(không quan trọng)ChunksLiên tục
(không quan trọng)ChunksKết thúc
Bộ nhớMáy chủ 
Dữ liệu

Hình 1. Minh họa cho chiến lược đọc truy cập ngẫu nhiên được đề xuất của chúng tôi cho mô hình hóa ngữ cảnh dài. Trong kịch bản truy cập tuần tự truyền thống (phần trên của hình), đầu vào được chia thành các chunk có kích thước bằng nhau và đưa vào mô hình theo thứ tự tuần tự. Ngược lại, chúng tôi đề xuất xây dựng một mô-đun máy chủ dữ liệu bổ sung (trong phần dưới của hình) nhận các thống kê có liên quan từ mô hình và quyết định chunk nào nó nên đọc tiếp theo.

Kiến trúc được đề xuất được mô tả trong Hình 1. Không giống như I/O truy cập truyền thống, nơi mỗi chunk đầu vào được đưa tuần tự vào mô hình, chúng tôi đề xuất xây dựng một máy chủ dữ liệu mới để xử lý quá trình I/O. Truy cập ngẫu nhiên mà chúng tôi đề xuất bao gồm việc mô hình truyền các thống kê có liên quan3 đến máy chủ dữ liệu, sau đó sử dụng cơ chế bỏ qua được thiết kế đặc biệt của chúng tôi (được nêu trong Phần 3) để xác định số lượng token cần bỏ qua và sau đó lấy một chunk mới cho mô hình. Tùy chọn, chúng tôi có thể sử dụng một mô-đun bộ nhớ4 để cung cấp ngữ cảnh bổ sung, giúp mô hình đưa ra quyết định bỏ qua tốt hơn và duy trì hiểu biết mạch lạc về văn bản đầu vào.

Thông qua các thí nghiệm rộng rãi, chúng tôi đã xác nhận hiệu quả của phương pháp được đề xuất. Những phát hiện của chúng tôi bao gồm:

2Truy cập ngẫu nhiên thường được định nghĩa là khả năng truy cập các vị trí tùy ý, như trong Máy Truy cập Ngẫu nhiên (Knuth, 1997). Trong phạm vi của bài báo này, chúng tôi tập trung vào một phiên bản đơn giản hóa nơi chúng tôi chỉ cho phép mô hình truy cập các vị trí tùy ý trong ngữ cảnh tương lai và không bao giờ nhìn lại, để đạt được hiệu quả tốt hơn.

3Trong bài báo này, chúng tôi chứng minh hiệu quả của việc đọc truy cập ngẫu nhiên bằng cách sử dụng mất mát entropy chéo trên mỗi token được gộp như những thống kê đó, mặc dù nghiên cứu tương lai có thể khám phá các thước đo thay thế.

4Điều này sẽ làm cho mô hình của chúng tôi tương tự như cơ chế chú ý landmark được đề xuất bởi (Mohtashami & Jaggi, 2023). Chúng tôi sẽ trình bày chi tiết về sự khác biệt giữa công trình của chúng tôi và của họ trong Phần 2 và thảo luận về việc triển khai mô-đun bộ nhớ trong Phần 3.

1. Cơ chế bỏ qua nâng cao hiệu suất mô hình trong tiền huấn luyện mô hình ngôn ngữ ngữ cảnh dài.

2. Tiền huấn luyện văn bản ngắn truyền thống làm giảm hiệu suất mô hình xuống dưới mức của một mô hình được khởi tạo ngẫu nhiên. Tuy nhiên, với việc tinh chỉnh của chúng tôi sử dụng cơ chế bỏ qua, một mô hình văn bản ngắn có thể được điều chỉnh thành công để xuất sắc trong các nhiệm vụ ngữ cảnh dài, thậm chí vượt trội hơn mô hình truy cập ngẫu nhiên của chúng tôi được tiền huấn luyện từ đầu.

3. Việc kết hợp một mô-đun bộ nhớ cho phép mô hình truy cập ngẫu nhiên của chúng tôi đạt được những cải thiện đáng kể hơn nữa, vượt qua các mô hình dựa trên bộ nhớ tiên tiến trước đó trong tiền huấn luyện (Wu et al., 2021) chỉ với 26% thời gian huấn luyện.

4. Đánh giá mô hình truy cập ngẫu nhiên của chúng tôi trên các nhiệm vụ downstream sử dụng nhiệm vụ TriviaQA (Joshi et al., 2017) được sửa đổi, nơi tất cả bằng chứng được truy xuất từ tập dữ liệu gốc được nối để tạo ra một kịch bản ngữ cảnh dài cực kỳ thách thức—chứng minh rằng cơ chế bỏ qua tích cực hơn mang lại hiệu suất tốt hơn. Điều này xác nhận hiệu quả của phương pháp và cải thiện hiệu quả học tập.

2. Bối cảnh
Khi được áp dụng cho các mô hình Transformer, các chiến lược đọc truy cập ngẫu nhiên được đề xuất của chúng tôi có thể giảm đáng kể các tính toán chú ý bằng cách bỏ qua nhiều token. Điều này phù hợp chặt chẽ với nghiên cứu về thưa thớt hóa chú ý sử dụng thông tin cục bộ để nâng cao hiệu quả và khả năng mô hình hóa ngữ cảnh dài. Đáng chú ý, Liu et al. (2023) chứng minh sự tồn tại của thưa thớt ngữ cảnh có thể được tận dụng để tăng tốc tính toán. Chowdhury & Caragea (2023) minh họa hiệu quả của việc sử dụng cửa sổ trượt và phân rã có trọng số cho các nhiệm vụ hiểu ngữ cảnh dài. Thêm vào đó, Fu et al. (2023) phát hiện rằng một mô hình tích chập đơn giản hoạt động tốt trên các nhiệm vụ Long Range Arena (Tay et al., 2020b). Chen et al. (2023) phát triển một cấu trúc phân cấp từ dưới lên sử dụng LLM để đọc tài liệu dài, cho phép điều hướng trực tiếp đến các phần tài liệu có liên quan đến các truy vấn cụ thể bằng cách theo dõi đường dẫn cây. Hơn nữa, Han et al. (2023) và Xiao et al. (2024) báo cáo rằng việc duy trì chú ý chỉ trên một vài token ban đầu (được gọi là "attention sinks"), với mỗi token chú ý chỉ đến các hàng xóm của nó, có thể nâng cao đáng kể hiệu quả xử lý ngữ cảnh dài trong khi bảo tồn hiệu suất. Những công trình này cung cấp cả hỗ trợ thực nghiệm và lý thuyết cho việc thực hiện thưa thớt hóa chú ý, và transformer truy cập ngẫu nhiên của chúng tôi thực hiện một phương pháp tích cực hơn bằng cách trực tiếp bỏ qua nhiều token.

Khái niệm liên quan nhất đến mô hình truy cập ngẫu nhiên của chúng tôi là cơ chế chú ý landmark được giới thiệu bởi

--- TRANG 2 ---
Trang bị Transformer với Khả năng Đọc Truy cập Ngẫu nhiên cho Hiểu biết Ngữ cảnh Dài

Mohtashami & Jaggi (2023). Trong phương pháp này, một token landmark được thêm vào cuối mỗi chunk có độ dài cố định để đại diện cho chunk đó, và mỗi token chỉ chú ý đến một số lượng hạn chế các token landmark theo chunk này. Mặc dù phương pháp này tạo điều kiện truy cập ngẫu nhiên cho mỗi token và giảm đáng kể yêu cầu bộ nhớ và tính toán, nó vẫn đòi hỏi phải chọn một kích thước chunk không đổi—có thể khác nhau giữa các nhiệm vụ—đọc toàn bộ ngữ cảnh tuần tự (do đó vẫn giữ độ phức tạp bậc hai mặc dù giảm bởi một hệ số không đổi), và yêu cầu triển khai các cấu trúc dữ liệu cụ thể và thuật toán truy xuất xấp xỉ để có hiệu quả. Ngược lại, phương pháp của chúng tôi tận dụng các thống kê tổng hợp để trực tiếp bỏ qua đến cửa sổ thông tin tiếp theo. Trong mỗi giai đoạn đọc, mô hình của chúng tôi tận dụng đầy đủ cửa sổ ngữ cảnh mà nó đã được huấn luyện, thay vì dựa vào kích thước chunk được chỉ định thủ công, và tùy chọn kết hợp một mô-đun bộ nhớ (làm cho nó gần hơn với phương pháp "attention sink"). Điều này cho phép mô hình hóa tốt hơn tính mạch lạc của văn bản và đạt được độ phức tạp gần tuyến tính phụ.5

3. Cơ chế Bỏ qua
Khi đọc một phần lớn của tài liệu, chúng tôi cho rằng ngay cả một mô hình ngôn ngữ được huấn luyện một nửa có thể đã có dự đoán hợp lý cho một số phân đoạn tương lai, do đó có thể được bỏ qua một cách an toàn. Cơ chế bỏ qua được đề xuất rất đơn giản: Giả sử mô hình M bắt đầu đọc tại token thứ S, XS, của tài liệu X, và tiếp tục trong L(M) token—thường là giới hạn token tối đa cho mô hình, ví dụ, L(M) = 512. Việc đọc kết thúc tại token XS+L(M). Trong khoảng thời gian này, mô hình thực hiện tự chú ý trên khoảng XS, . . . , XS+L(M) và tính toán các mất mát entropy chéo theo token LS, . . . , LS+L(M). Sau đó chúng tôi áp dụng một phép toán gộp cho những mất mát này để ước tính độ tin cậy của mô hình đối với đoạn văn này, được ký hiệu là C(X, S;M) = pooling(LS, . . . , LS+L(M)). Thước đo tin cậy này được sử dụng để xác định số lượng token, D(X, S;M), mà mô hình sẽ bỏ qua tiếp theo:

D(X, S;M) = (1)
Kmin{⌊|X| − S − L(M)/K⌋, ⌊α/C(X, S;M)⌋}

Ở bước đọc tiếp theo, mô hình tiếp tục đọc từ token XS+L(M)+D(X,S;M), và quá trình bỏ qua này tiếp tục cho đến khi kết thúc tài liệu. Ở đây, K đại diện cho tỷ lệ bỏ qua và α là ngưỡng bỏ qua. Khi C(X, S;M) nhỏ so với α, điều này cho thấy rằng

5Chúng tôi loại trừ độ phức tạp tính toán của quá trình token hóa vì nó tương đối nhẹ. Thực tế, để bỏ qua token hóa, phương pháp của chúng tôi cũng có thể hoạt động với mô hình Transformer dựa trên ký tự ngay lập tức, nơi ngữ nghĩa của offset tự động có nghĩa là ký tự thay vì sentencepiece.

mô hình có hiểu biết mạnh mẽ về đoạn văn hiện tại, cho phép nó có thể bỏ qua một cách an toàn với tỷ lệ K⌊α/C(X,S;M)⌋.

Trực quan, đối với tiền huấn luyện và tinh chỉnh, nơi mất mát được tính toán trong mô hình teacher-forcing, C(X, S;M) trực tiếp đo lường mô hình có thể dự đoán tốt như thế nào các token vàng tiếp theo dựa trên ngữ cảnh trước đó. C(X, S;M) càng lớn, độ tin cậy mà một mô hình được huấn luyện một nửa có đối với ngữ cảnh tương lai càng ít và do đó nên bỏ qua với tỷ lệ bảo thủ hơn. Trong quá trình suy luận, không có giám sát vàng, tình huống trở nên thách thức hơn. Tuy nhiên, các công trình trước (Dong et al., 2018; Kamath et al., 2020; Jiang et al., 2021) đã xác định mất mát log-probability là một thước đo phổ biến về độ tin cậy của mô hình tại thời điểm suy luận, mà chúng tôi dự đoán có thể rất có ích cho việc đưa ra quyết định bỏ qua.

Sự đơn giản của heuristic này nhấn mạnh giá trị của nó: nó loại bỏ nhu cầu về các mô hình bổ sung để dự đoán số lượng token cần bỏ qua. Vì mất mát trên mỗi token yêu cầu là đầu ra trực tiếp của bất kỳ mô hình ngôn ngữ tiêu chuẩn nào, cơ chế này có thể được tích hợp liền mạch vào quá trình tiền huấn luyện mô hình ngôn ngữ bình thường. Hơn nữa, phương pháp của chúng tôi không dựa vào bất kỳ giả định cấu trúc hoặc biểu diễn trung gian nào; do đó, các hoạt động bỏ qua không can thiệp vào các hoạt động mô hình đang diễn ra và tương thích với bất kỳ cấu trúc mô hình nào cung cấp đầu ra xác suất cho mỗi token. Để đơn giản, trong phạm vi của bài báo này, chúng tôi sử dụng mô hình Transformer tự hồi quy được công nhận rộng rãi.

Các thí nghiệm của chúng tôi chứng minh rằng việc bỏ qua cục bộ cải thiện sự đánh đổi hiệu quả-đa dạng trong mô hình hóa ngữ cảnh dài. Hơn nữa, tinh chỉnh dựa trên bỏ qua cho phép một checkpoint mô hình ngôn ngữ hiện có xử lý hiệu quả các kịch bản ngữ cảnh dài, vượt qua thậm chí các baseline dựa trên bộ nhớ chuyên biệt.

Làm việc với Cơ chế Bộ nhớ Xét đến độ dài ngữ cảnh vốn hạn chế của các mô hình ngôn ngữ hiện tại và bản chất không thể đảo ngược của cơ chế bỏ qua của chúng tôi, có khả năng đưa ra quyết định bỏ qua mà không có đủ ngữ cảnh. Hơn nữa, nghiên cứu trước đây chỉ ra rằng việc duy trì chú ý đối với các token ban đầu và lân cận là rất quan trọng để hiểu ngữ cảnh mở rộng (Han et al., 2023; Xiao et al., 2024); do đó, tập trung vào những token này có thể nâng cao đáng kể hiệu suất. Để tăng cường khả năng xử lý ngữ cảnh và bỏ qua có thông tin của mô hình, chúng tôi giới thiệu một cơ chế bộ nhớ tùy chọn vào khung của chúng tôi. Trong nghiên cứu này, chúng tôi triển khai mô hình Attendre, như được đề xuất trong (Yang & Hua, 2024), sử dụng chiến lược loại bỏ bộ nhớ First-In First-Out (FIFO) cùng với thuật toán truy xuất Key-Value Top-K xấp xỉ. Không giống như các mô hình transformer tăng cường bộ nhớ truyền thống như Memorizing Transformer (Wu et al., 2021), phương pháp của chúng tôi có một pool bộ nhớ toàn cục thực hiện truy xuất Key-Value Top-K trên cả cửa sổ đọc hiện tại và các mục bộ nhớ quá khứ ở mọi lớp, thay vì bị giới hạn trong một lớp trung gian duy nhất.

Mở rộng cho Tài liệu Có cấu trúc Phương pháp của chúng tôi có thể được củng cố thêm bằng cách tận dụng các cấu trúc phân cấp (ví dụ, cây DOM cho một trang web hoặc mục lục trong một cuốn tiểu thuyết). Chúng tôi có thể giới hạn việc bỏ qua xảy ra trong các cây con, thực hiện bỏ qua đồng thời cho nhiều phần của tài liệu, và tổng hợp thông tin toàn cầu. Phương pháp này phù hợp hơn với phương pháp chú ý landmark (Mohtashami & Jaggi, 2023), có kích thước khối không đồng đều và ngữ nghĩa khối mạch lạc hơn. Ngay cả khi không có chỉ mục được xây dựng sẵn, tóm tắt đệ quy có thể được sử dụng để xây dựng chỉ mục như vậy từ đầu, như được chứng minh trong (Chen et al., 2023). Trong công trình này, trọng tâm của chúng tôi là thể hiện hiệu quả của cơ chế bỏ qua được đề xuất mà không giả định bất kỳ cấu trúc nào trong tài liệu đầu vào. Do đó, chúng tôi để lại việc khám phá thêm trong lĩnh vực này cho công việc tương lai.

4. Mô hình hóa Ngôn ngữ Ngữ cảnh Dài
Đầu tiên chúng tôi đánh giá hiệu quả của phương pháp truy cập ngẫu nhiên của chúng tôi trong tiền huấn luyện trên một nhiệm vụ mô hình hóa ngôn ngữ ngữ cảnh dài—cụ thể, dự đoán token tiếp theo trong một diễn ngôn dài. Chúng tôi chọn corpus C4 (Raffel et al., 2020) cho nhiệm vụ của chúng tôi và lọc ra các tài liệu có ít hơn 4.000 token, dẫn đến cùng một tập con corpus C4 (4k+) như được sử dụng trong (Wu et al., 2021). Để xác minh hiệu quả của cơ chế bỏ qua được đề xuất của chúng tôi, chúng tôi điều tra hai kịch bản ứng dụng: 1) Tiền huấn luyện: Giả sử có đủ tài nguyên tính toán, chúng tôi nhằm tiền huấn luyện một mô hình từ đầu có thể quản lý ngữ cảnh dài, nơi việc sử dụng bỏ qua như một chiến lược cung cấp dữ liệu có thể nâng cao hiệu suất mô hình. 2) Tinh chỉnh: Trong các tình huống thực tế nơi tiền huấn luyện không khả thi, chúng tôi xem xét liệu cơ chế bỏ qua có thể phục vụ như một chiến lược tinh chỉnh để tăng cường khả năng của một mô hình ngữ cảnh ngắn trong việc xử lý ngữ cảnh dài.

4.1. Thiết lập Nhiệm vụ
Tiền huấn luyện Chúng tôi tiền huấn luyện một mô hình transformer 12 lớp trong 200.000 bước với kích thước batch là 128 và giới hạn token tối đa L(M) = 512. Trong thí nghiệm này, chúng tôi không sử dụng cơ chế bộ nhớ để cô lập và đánh giá trực tiếp hiệu quả của cơ chế bỏ qua. Chúng tôi đặt các tỷ lệ bỏ qua khác nhau K = 0, 1, 256, 100.000 cho các lần chạy khác nhau. Để đánh giá, chúng tôi vô hiệu hóa việc bỏ qua để đảm bảo so sánh công bằng.

Tinh chỉnh Chúng tôi sử dụng một mô hình ngôn ngữ được tiền huấn luyện trên corpus C4 (văn bản ngắn), cụ thể cho dự đoán từ tiếp theo

--- TRANG 3 ---
Trang bị Transformer với Khả năng Đọc Truy cập Ngẫu nhiên cho Hiểu biết Ngữ cảnh Dài

[THIS IS CHART: Line graph showing training steps vs PPL for C4 (4k+) LM Pretraining with different skip rates (0, 1, 256, 100000)]

Hình 2. Thí nghiệm Tiền huấn luyện trên C4 (4k+) cho các mô hình không có cơ chế bộ nhớ. Chúng tôi vô hiệu hóa việc bỏ qua tại thời điểm đánh giá để so sánh công bằng. Tỷ lệ bỏ qua trung gian hoạt động tốt nhất sau khi huấn luyện đủ, điều này xác nhận hiệu quả của phương pháp chúng tôi.

trong 100M bước, không sử dụng cơ chế bỏ qua. Chúng tôi sử dụng cùng kiến trúc mô hình như trong nhiệm vụ tiền huấn luyện. Trong quá trình tiền huấn luyện văn bản ngắn này, chúng tôi tuân theo chiến lược xử lý dữ liệu phổ biến ("phân đoạn và xáo trộn ngẫu nhiên") để nối tất cả tài liệu, chia toàn bộ corpus thành các chunk có kích thước L(M) và xáo trộn ngẫu nhiên các chunk (Devlin et al., 2019; Raffel et al., 2020). Việc tinh chỉnh sau đó được thực hiện trong 25.000 bước trên tập huấn luyện C4 (4k+) được sử dụng trong Nhiệm vụ tiền huấn luyện. Chúng tôi báo cáo các tỷ lệ bỏ qua K = 0, 256 để đơn giản.

Thước đo Chúng tôi áp dụng perplexity (PPL) làm thước đo như trong các công trình trước đây cho mô hình hóa ngôn ngữ ngữ cảnh dài.

4.2. Kết quả Thí nghiệm
Bỏ qua đạt được kết quả tiền huấn luyện ngữ cảnh dài tốt hơn. Kết quả tiền huấn luyện được minh họa trong Hình 2. Với thời gian huấn luyện đủ, một tỷ lệ bỏ qua trung gian (K=256) giảm đáng kể perplexity so với không bỏ qua (K=0), chứng minh hiệu quả của cơ chế bỏ qua trong tiền huấn luyện ngữ cảnh dài. Với tỷ lệ bỏ qua rất cao (tức là K=100.000), mối tương quan giữa các tài liệu được đọc liên tiếp giảm dần, gần giống với hiệu ứng của việc xáo trộn đầu vào ngẫu nhiên. Chúng tôi thấy rằng việc bỏ qua với tỷ lệ vừa phải vượt trội hơn cả chiến lược bỏ qua cực nhanh và không bỏ qua. Do đó, cơ chế bỏ qua của chúng tôi không chỉ duy trì tính mạch lạc tốt hơn giữa các đoạn văn được phân đoạn mà còn tăng cường khả năng tiếp xúc của mô hình với các token độc đáo, từ đó cải thiện khả năng tổng quát hóa so với các chiến lược tiền huấn luyện phân đoạn liên tiếp và xáo trộn ngẫu nhiên.

Sử dụng bỏ qua trong tinh chỉnh có thể điều chỉnh thành công mô hình ngôn ngữ văn bản ngắn thành mô hình ngữ cảnh dài tốt hơn.

[THIS IS CHART: Bar graph showing PPL performance for different model configurations including Random Initialization, Short-Text Pretrained Checkpoint, and various training approaches]

Hình 3. Thí nghiệm Tinh chỉnh trên C4 (4k+) cho các mô hình được tiền huấn luyện trên văn bản ngắn. Chúng tôi thêm hiệu suất tốt nhất đạt được trong tiền huấn luyện và hiệu suất mô hình được khởi tạo ngẫu nhiên để tham khảo. Chúng tôi thấy rằng tiền huấn luyện văn bản ngắn sẽ làm cho mô hình tổng quát hóa tệ hơn thậm chí so với một mô hình được khởi tạo ngẫu nhiên cho mô hình hóa ngôn ngữ ngữ cảnh dài, nhưng tinh chỉnh bỏ qua có thể giúp điều chỉnh các checkpoint như vậy để thậm chí hoạt động tốt hơn so với checkpoint ngữ cảnh dài được tiền huấn luyện đặc biệt.

Chúng tôi trình bày kết quả tinh chỉnh trong Hình 3. Ban đầu, sau khi tiền huấn luyện trên văn bản ngắn, mô hình thể hiện hiệu suất tệ hơn (ppl=831.31) so với một mô hình được khởi tạo ngẫu nhiên (ppl=366.50) trên mô hình hóa ngôn ngữ ngữ cảnh dài. Điều này chỉ ra rằng các chiến lược tiền huấn luyện văn bản ngắn truyền thống không tổng quát hóa tốt cho các kịch bản ngữ cảnh dài. Tuy nhiên, bằng cách áp dụng việc bỏ qua phù hợp trong quá trình tinh chỉnh, chúng tôi có thể điều chỉnh hiệu quả một mô hình được tiền huấn luyện văn bản ngắn để đạt được perplexity thậm chí tốt hơn (ppl=16.97, K=256) so với mô hình ngữ cảnh dài tốt nhất của chúng tôi được huấn luyện từ đầu (ppl=35.59). Hơn nữa, so với tinh chỉnh tiêu chuẩn trên corpus ngữ cảnh dài (ppl=19.10, K=0), tinh chỉnh với cơ chế bỏ qua được đề xuất của chúng tôi đạt được những cải thiện đáng kể, xác nhận hiệu quả của nó trong việc nâng cao khả năng mô hình trong môi trường ngữ cảnh dài.

Giải thích của chúng tôi là "phân đoạn và xáo trộn ngẫu nhiên" phổ biến trong tiền huấn luyện văn bản ngắn truyền thống (Devlin et al., 2019; Raffel et al., 2020) làm suy giảm khả năng đọc liên tục của mô hình—ngữ cảnh đầu vào thay đổi quá nhanh khiến mô hình thiếu cơ hội để nắm bắt tính mạch lạc văn bản một cách hiệu quả. Cơ chế bỏ qua có thể giảm thiểu điều này bằng cách giúp khôi phục các phụ thuộc dài hạn và tổng quát hóa hiệu quả hơn kiến thức được tiền huấn luyện trong mô hình hóa ngôn ngữ ngữ cảnh dài.

Mô hình dần học cách bỏ qua nhiều hơn. Trong Hình 4, chúng tôi minh họa số lượng token bỏ qua trung bình trong quá trình huấn luyện. Đối với tất cả các kịch bản bỏ qua (K > 0), số lượng token bỏ qua trung bình tăng theo thời gian. Điều này chỉ ra sự hiểu biết được cải thiện của mô hình về ngữ cảnh dài của đầu vào, dẫn đến các quyết định bỏ qua tự tin và tích cực hơn

[THIS IS CHART: Line graph showing training steps vs Average Skips/K for C4 (4k+) LM Pretraining with different skip rates]

Hình 4. Minh họa số lượng token bỏ qua trung bình ("Average Skips") trong quá trình tiền huấn luyện C4 (4k+) không sử dụng mô-đun bộ nhớ. Trong các kịch bản bỏ qua khác nhau (K > 0), mô hình dần học cách bỏ qua nhiều token hơn, chỉ ra rằng nó có được hiểu biết tốt hơn về ngữ cảnh dài hiện tại và trở nên tự tin hơn trong việc đưa ra quyết định bỏ qua tích cực.

quyết định. Bằng cách so sánh các đường cong khác nhau, chúng tôi quan sát rằng với tỷ lệ bỏ qua vừa phải (K=256) đạt được các mẫu bỏ qua mượt mà nhất. Điều này có thể giúp ổn định huấn luyện trong các giai đoạn sau và đạt được khả năng tổng quát hóa tốt hơn. Mặc dù trường hợp K=100.000 cho thấy các mẫu tương tự, hiệu suất của nó trên nhiệm vụ tiền huấn luyện tệ hơn, chỉ ra rằng mức độ bỏ qua (chủ yếu được kiểm soát bởi tỷ lệ bỏ qua K) là rất quan trọng và yêu cầu điều chỉnh cẩn thận.

Lựa chọn pooling không quan trọng. Chúng tôi cũng thử một số chiến lược pooling khác nhau – "exponential-decay", "chỉ sử dụng token cuối cùng trong chunk hiện tại" và "average pooling". Có vẻ như tất cả các phương pháp pooling đều hoạt động tốt như nhau.

| Mô hình | PPL | Bước Huấn luyện |
|---------|-----|-----------------|
| Transformer w/ Memory (Wu et al., 2021) | 14.97 | 500k |
| Skipping-Pretrained w/o Memory (của chúng tôi) | 35.59 | 200k |
| Skipping-Pretrained w/ Memory (của chúng tôi) | 14.15 | 130k |

Bảng 1. Thí nghiệm về việc kết hợp memorizing transformer trong tiền huấn luyện. Chúng tôi áp dụng kiến trúc memorizing transformer như được triển khai trong Wu et al. (2021) với độ dài ngữ cảnh 512, kích thước bộ nhớ 1536 và không sử dụng XL-cache.

Kết hợp cơ chế bộ nhớ với bỏ qua mang lại cải thiện thêm. Chúng tôi thấy rằng việc sử dụng bỏ qua trong tiền huấn luyện có thể có lợi cho tiền huấn luyện transformer với cơ chế bộ nhớ ngay cả khi việc bỏ qua không được phép tại thời điểm kiểm tra. Chúng tôi áp dụng một kiến trúc memorizing transformer cụ thể trong (Wu et al., 2021) (memory-size=1536, context-len=512, no-XL-cache) và thực hiện cùng một tiền huấn luyện bỏ qua trên C4 (4k+) như trên. Để so sánh công bằng, chúng tôi một lần nữa vô hiệu hóa việc sử dụng bỏ qua tại thời điểm kiểm tra. Kết quả thí nghiệm được thể hiện trong Bảng 1. Chúng tôi thấy rằng memorizing transformer được tiền huấn luyện bỏ qua của chúng tôi có thể đạt được hiệu suất tốt hơn (ppl=14.15) ngay cả trong 130k bước tối ưu hóa đầu tiên so với memorizing transformer được tiền huấn luyện được báo cáo trong (Wu et al., 2021) (ppl=14.97, 500k bước), xác nhận các phương pháp tiền huấn luyện của chúng tôi mang lại tăng cường hiệu suất đáng kể (-0.82 ppl) và hiệu quả học tập tốt hơn (hiệu quả hơn khoảng 3.84 lần). So với mô hình w/o memory được tiền huấn luyện bỏ qua của chúng tôi (ppl=35.59), việc trang bị cơ chế bộ nhớ đạt được 60% cải thiện hiệu suất, chứng minh lợi ích to lớn của việc bao gồm cơ chế bộ nhớ trong tiền huấn luyện bỏ qua. Điều này được mong đợi vì bộ nhớ có thể giúp theo dõi những gì đã được đọc cho đến nay một cách rõ ràng và giảm quá tải bộ nhớ từ không gian tham số trong thời gian huấn luyện.

5. Trả lời Câu hỏi Ngữ cảnh Dài
Bên cạnh tiền huấn luyện, chúng tôi cũng muốn đánh giá các mô hình đọc-bỏ qua của chúng tôi trên các nhiệm vụ ngữ cảnh dài downstream để xác minh liệu đọc-bỏ qua có thể giúp ích. Chúng tôi đánh giá các mô hình của chúng tôi trên tập dữ liệu TriviaQA (Joshi et al., 2017) (phần "RC"). Chúng tôi nối tất cả các bằng chứng được truy xuất trong đầu vào để biến đổi nhiệm vụ này thành một nhiệm vụ trả lời câu hỏi ngữ cảnh dài tương đối thách thức. Số lượng token trung bình cho tập dữ liệu kết quả là 23.706,21. Vì đầu vào bây giờ trở nên tương đối dài, chúng tôi thử nghiệm với chiến lược bỏ qua tích cực hơn bằng cách đặt K = 0, 1000, 10000 trong cả thời gian huấn luyện và kiểm tra, dẫn đến 3×3 = 9 tổ hợp.

[THIS IS CHART: Bar chart showing EM scores for TriviaQA with different training and inference skip rates]

Hình 5. Kết quả thí nghiệm Trả lời Câu hỏi Ngữ cảnh Dài. x→y có nghĩa là chúng tôi huấn luyện mô hình với tỷ lệ bỏ qua Ktrain=x và đánh giá sử dụng tỷ lệ bỏ qua Kinfer=y. Chúng tôi thấy rằng việc áp dụng chiến lược bỏ qua tích cực hơn giúp ích rất nhiều cho việc cải thiện hiệu suất mô hình.

Chúng tôi lấy mẫu ngẫu nhiên 200 câu hỏi cho tập kiểm tra. Chúng tôi sử dụng cùng kiến trúc transformer như trong các thí nghiệm tiền huấn luyện. Đối với thí nghiệm này, chúng tôi tiền huấn luyện mô hình với cơ chế bộ nhớ được giới thiệu ở trên trên corpus C4.

Kết quả thí nghiệm được thể hiện trong Hình 5. Chúng tôi quan sát rằng các mô hình hoạt động tốt nhất sử dụng tỷ lệ bỏ qua K=10000 trong cả huấn luyện và kiểm tra. Thêm vào đó, khi tỷ lệ bỏ qua huấn luyện được cố định, việc sử dụng tỷ lệ bỏ qua lớn hơn hoặc bằng nhau trong quá trình kiểm tra luôn mang lại kết quả tốt hơn, chỉ ra rằng tỷ lệ bỏ qua có thể được ngoại suy hiệu quả trong thời gian suy luận để đọc hiệu quả hơn. Hơn nữa, với tỷ lệ suy luận cố định, việc sử dụng tỷ lệ bỏ qua huấn luyện tích cực hơn luôn dẫn đến cải thiện. Điều này trái ngược với các phát hiện trong Phần 4, nơi tỷ lệ bỏ qua vừa phải tỏ ra hiệu quả hơn. Đối với tiền huấn luyện, tập trung vào dự đoán token tiếp theo, nhiệm vụ vẫn vốn dĩ cục bộ, và cấu trúc của một ngữ cảnh dài mang lại lợi ích hạn chế. Tuy nhiên, trong QA ngữ cảnh dài, mục tiêu chuyển hướng để cho phép mô hình xử lý các tài liệu cực kỳ dài chứa nhiều phần mang câu trả lời và yếu tố gây nhiễu. Đọc-bỏ qua giảm thiểu sự tham gia dư thừa với các đoạn văn có liên quan tương tự, do đó loại bỏ nhu cầu của người dùng phải lọc ngữ cảnh thủ công. Phương pháp này, kết hợp với mất mát động, đạt được hiệu ứng tương tự như lọc ngay lập tức.

--- TRANG 4 ---

6. Kết luận
Trong bài báo này, chúng tôi giới thiệu một chiến lược đọc truy cập ngẫu nhiên được thiết kế cho hiểu biết ngữ cảnh dài hiệu quả. Phương pháp này chủ yếu sử dụng mất mát token tổng hợp trong mỗi cửa sổ đọc mô hình, cho phép một máy chủ dữ liệu xác định số lượng token cần bỏ qua và thực hiện quá trình bỏ qua. Để nâng cao việc ra quyết định cho bỏ qua và cung cấp cho mô hình một ngữ cảnh phong phú hơn, chúng tôi kết hợp một mô-đun bộ nhớ bổ sung. Trong các thí nghiệm, chúng tôi áp dụng phương pháp của chúng tôi trên mô hình Transformer và đánh giá mô hình truy cập ngẫu nhiên được huấn luyện của chúng tôi thông qua tiền huấn luyện ngữ cảnh dài, tinh chỉnh và trả lời câu hỏi. Kết quả thí nghiệm xác nhận những cải thiện đáng kể về hiệu suất và hiệu quả của cơ chế bỏ qua đơn giản được đề xuất của chúng tôi so với truy cập tuần tự truyền thống. Hơn nữa, những thử nghiệm này chứng minh khả năng thích ứng nhanh chóng của một mô hình văn bản ngắn với các ứng dụng ngữ cảnh dài, và những lợi ích tương hỗ của việc kết hợp một mô-đun bộ nhớ và cho phép đọc truy cập ngẫu nhiên. Công việc tương lai có thể được hưởng lợi từ việc khám phá một cơ chế bỏ qua tinh vi hơn và kiểm tra sự tương tác giữa cơ chế bỏ qua và mô-đun bộ nhớ.

Tài liệu Tham khảo
Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv:2004.05150, 2020.

Chen, H., Pasunuru, R., Weston, J., and Celikyilmaz, A. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029, 2023.

Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.

Chowdhury, J. R. and Caragea, C. Monotonic location attention for length generalization. In International Conference on Machine Learning. PMLR, 2023.

Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, jun 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.

Ding, M., Zhou, C., Yang, H., and Tang, J. Cogltx: Applying bert to long texts. Advances in Neural Information Processing Systems, 33:12792–12804, 2020.

Dong, L., Quirk, C., and Lapata, M. Confidence modeling for neural semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 743–753, 2018.

Dong, Z., Tang, T., Li, L., and Zhao, W. X. A survey on long text modeling with transformers. arXiv preprint arXiv:2302.14502, 2023.

Fu, D. Y., Epstein, E. L., Nguyen, E., Thomas, A. W., Zhang, M., Dao, T., Rudra, A., and Re, C. Simple hardware-efficient long convolutions for sequence modeling. In International Conference on Machine Learning. PMLR, 2023.

Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y., and Peng, H. Data engineering for scaling language models to 128k context. In International Conference on Machine Learning, 2024.

Guo, M., Ainslie, J., Uthus, D. C., Ontanon, S., Ni, J., Sung, Y.-H., and Yang, Y. Longt5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 724–736, 2022.

Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.

Huang, Y., Xu, J., Jiang, Z., Lai, J., Li, Z., Yao, Y., Chen, T., Yang, L., Xin, Z., and Ma, X. Advancing transformer architecture in long-context large language models: A comprehensive survey. arXiv preprint arXiv:2311.12351, 2023.

Ivgi, M., Shaham, U., and Berant, J. Efficient long-text understanding with short-text models. Transactions of the Association for Computational Linguistics, 11:284–299, 2023.

Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962–977, 2021.

Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can language models resolve real-world github issues? In International Conference on Learning Representations, 2023.

--- TRANG 5 ---
Trang bị Transformer với Khả năng Đọc Truy cập Ngẫu nhiên cho Hiểu biết Ngữ cảnh Dài

Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, 2017.

Kamath, A., Jia, R., and Liang, P. Selective question answering under domain shift. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5684–5696, 2020.

Knuth, D. E. The art of computer programming, volume 3. Pearson Education, 1997.

Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.

Liu, Y., Ni, A., Nan, L., Deb, B., Zhu, C., Hassan, A., and Radev, D. Leveraging locality in abstractive text summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 6081–6093, 2022.

Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient LLMs at inference time. In International Conference on Machine Learning, pp. 22137–22176. PMLR, 2023. URL https://proceedings.mlr.press/v202/liu23am.html.

Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and Zettlemoyer, L. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34: 2441–2453, 2021.

Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. Advances in Neural Information Processing Systems, 2023.

Mohtashami, A. and Jaggi, M. Random-access infinite context length for transformers. Advances in Neural Information Processing Systems, 36, 2024.

Mou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. Narrative question answering with cutting-edge open-domain qa techniques: A comprehensive study. Transactions of the Association for Computational Linguistics, 9:1032–1046, 2021.

Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher, S., and Ho, N. Fourierformer: Transformer meets generalized fourier integral theorem. Advances in Neural Information Processing Systems, 35:29319–29335, 2022.

Paris, S. G., Wasik, B., and Turner, J. C. The development of strategic readers. 1991.

Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N., and Kong, L. Random feature attention. In International Conference on Learning Representations, 2020.

Pressley, M. and Afflerbach, P. Verbal protocols of reading: The nature of constructively responsive reading. Routledge, 2012.

Qiu, J., Ma, H., Levy, O., Yih, W.-t., Wang, S., and Tang, J. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 2555–2565, 2020.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.

Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicp, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.

Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.

Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse sinkhorn attention. In International Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a.

Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020b.

Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In International Conference on Learning Representations, 2021.

--- TRANG 6 ---
Trang bị Transformer với Khả năng Đọc Truy cập Ngẫu nhiên cho Hiểu biết Ngữ cảnh Dài

Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In International Conference on Learning Representations, 2024.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.

Yang, C. and Ettinger, A. Can you follow me? testing situational understanding for ChatGPT. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 6385–6398, 2023.

Yang, Z. and Hua, N. Attendre: Wait to attend by retrieval with evicted queries in memory-based transformers for long context processing. arXiv preprint arXiv:2401.04881, 2024.

Zhang, Y., Ni, A., Mao, Z., Wu, C. H., Zhu, C., Deb, B., Awadallah, A., Radev, D., and Zhang, R. Summn: A multi-stage summarization framework for long input dialogues and documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1592–1604, 2022.

Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: A realistic web environment for building autonomous agents. In International Conference on Learning Representations, 2023.

9
