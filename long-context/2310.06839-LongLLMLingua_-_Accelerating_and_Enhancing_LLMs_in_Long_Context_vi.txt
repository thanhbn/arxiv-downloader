LongLLMLingua : Tăng Tốc và Nâng Cao LLMs trong Các Kịch Bản
Ngữ Cảnh Dài thông qua Nén Prompt
Huiqiang Jiang, Qianhui Wu, Xufang Luo,
Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
Microsoft Corporation
{hjiang,qianhuiwu,xufluo,dongsli,cyl,yuqyang,liliqiu}@microsoft.com
Tóm tắt
Trong các kịch bản ngữ cảnh dài, các mô hình
ngôn ngữ lớn (LLMs) đối mặt với ba thách thức
chính: chi phí tính toán cao hơn, giảm hiệu suất,
và thiên lệch vị trí. Nghiên cứu chỉ ra rằng hiệu
suất LLM phụ thuộc vào mật độ và vị trí của thông
tin quan trọng trong prompt đầu vào. Được truyền
cảm hứng từ những phát hiện này, chúng tôi đề
xuất LongLLMLingua để nén prompt nhằm cải
thiện khả năng nhận thức thông tin quan trọng của
LLMs để đồng thời giải quyết cả ba thách thức.
Đánh giá toàn diện của chúng tôi trên nhiều kịch
bản ngữ cảnh dài khác nhau cho thấy rằng
LongLLMLingua không chỉ nâng cao hiệu suất mà
còn giảm đáng kể chi phí và độ trễ. Chẳng hạn,
trong benchmark NaturalQuestions, LongLLM-
Lingua tăng hiệu suất lên đến 21.4% với khoảng
4x ít token hơn trong GPT-3.5-Turbo, dẫn đến
tiết kiệm chi phí đáng kể. Nó đạt được mức giảm
94.0% chi phí trong benchmark LooGLE. Hơn
nữa, khi nén các prompt khoảng 10k token với tỷ
lệ 2x-6x, LongLLMLingua có thể tăng tốc độ trễ
đầu cuối từ 1.4x-2.6x.1
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) đã cách mạng
hóa các công nghệ ngôn ngữ hướng tới người dùng
và đang phục vụ như những thành phần quan trọng
trong ngày càng nhiều ứng dụng. Thiết kế cẩn thận
các prompt là cần thiết để đạt được hiệu suất tốt
hơn trong các nhiệm vụ cụ thể downstream. Các
công nghệ thường được sử dụng như In-Context
Learning (ICL) (Min et al., 2022; Dong et al., 2023),
Retrieval Augment Generation (RAG) (Lewis et al.,
2020; Asai et al., 2024), và Multi-turn Agent (Shen
et al., 2024; Park et al., 2023; Wu et al., 2023a) đang
thúc đẩy các prompt ngày càng dài hơn, thậm chí
đạt đến hàng nghìn token. Các kịch bản như trả lời
câu hỏi đa tài liệu, hoàn thành mã, và tóm tắt tài
liệu cũng cần thiết việc xử lý các ngữ cảnh dài.
1Truy cập mã của chúng tôi tại https://aka.ms/LongLLMLingua .Có ba thách thức chính khi LLMs được
sử dụng trong các kịch bản ngữ cảnh dài: (1) Chi
phí tính toán cao hơn, bao gồm cả chi phí tài chính
và độ trễ. (2) Prompt dài hơn giới thiệu thông tin
không liên quan và dư thừa, có thể làm suy yếu
hiệu suất của LLMs (Shi et al., 2023), như được
minh họa trong Hình 1a. (3) LLMs thể hiện thiên
lệch vị trí (Kamradt, 2023), còn được gọi là vấn đề
"lost in the middle" (Liu et al., 2024), cho thấy rằng
vị trí đặt thông tin quan trọng trong prompt có ảnh
hưởng đáng kể đến hiệu suất của LLMs. Điều này
được chứng minh bởi đường cong màu tím trong
Hình 1b.
Được truyền cảm hứng từ những quan sát này,
chúng tôi đề xuất LongLLMLingua để giải quyết
ba thách thức. Cụ thể, chúng tôi sử dụng LLM-
Lingua (Jiang et al., 2023a) làm xương sống cho
việc nén prompt để giải quyết thách thức đầu tiên,
tức là giảm chi phí và độ trễ. Tuy nhiên, trong
trường hợp ngữ cảnh dài, phân phối thông tin quan
trọng liên quan đến câu hỏi trong prompt thường
là động và thưa thớt. Các phương pháp nén prompt
hiện tại như LLMLingua (Jiang et al., 2023a) và
Selective-Context (Li et al., 2023c) thường không
xem xét câu hỏi trong quá trình nén, dẫn đến việc
giữ lại quá nhiều nhiễu và giảm hiệu suất. LongLLM-
Lingua nhằm cải thiện khả năng nhận thức thông
tin quan trọng liên quan đến câu hỏi của LLMs,
từ đó vượt qua các vấn đề nhiễu và thiên lệch vị
trí trong ngữ cảnh dài, được hiển thị trong Hình
1b. Nguyên lý cơ bản của LongLLMLingua là các
LM nhỏ vốn có khả năng nắm bắt phân phối thông
tin quan trọng liên quan đến một câu hỏi cụ thể.
Các đóng góp chính của chúng tôi gồm năm khía
cạnh: (1) Chúng tôi đề xuất một phương pháp nén
từ thô đến tinh biết câu hỏi để cải thiện mật độ
thông tin quan trọng trong prompt (Mục 4.1); (2)
Chúng tôi giới thiệu chiến lược sắp xếp lại tài liệu
để giảm thiểu thiên lệch vị trí trong LLMs. (Mục
4.2); (3) Chúng tôi thiết lập tỷ lệ nén động cho
việc kiểm soát chính xác giữa các cấp độ nén thô
và tinh (Mục 4.3); (4) Chúng tôi đề xuất chiến lược
khôi phục dãy con sau nén để cải thiện tính toàn
vẹn của thông tin quan trọng (4.4). (5) Chúng tôi
đánh giá LongLLMLingua trên năm benchmark,
tức là NaturalQuestions (Liu et al., 2024), LongBench
(Bai et al., 2023), ZeroSCROLLS (Shaham et al.,
2023), MuSicQue (Trivedi et al., 2022), và LooGLE
(Li et al., 2023b), bao phủ nhiều kịch bản ngữ cảnh
dài khác nhau. Kết quả thực nghiệm cho thấy rằng
các prompt đã nén của LongLLMLingua vượt trội
so với các prompt gốc về hiệu suất, hiệu quả chi
phí, và độ trễ hệ thống.
2 Công thức hóa Vấn đề
Theo LLMLingua (Jiang et al., 2023a), chúng tôi
sử dụng x= (xins,xdoc
1,···,xdoc
K,xque) để biểu diễn
một prompt, bao gồm hướng dẫn xins, K tài liệu
xdoc
i, và câu hỏi xque. Tuy nhiên, định nghĩa này có
thể được điều chỉnh cho các kịch bản cụ thể.
Mục tiêu của hệ thống nén prompt có thể được
công thức hóa như:
min
exDϕ(y,ey) +λ∥ex∥0, (1)
trong đó ex biểu diễn prompt đã nén, một dãy con
cấp token của x. y và ey biểu diễn kết quả được
tạo bởi LLM từ x và ex tương ứng. Dϕ đo hàm
khoảng cách, như phân kỳ KL. λ phục vụ như một
siêu tham số cân bằng tỷ lệ nén. Ngoài ra, nghiên
cứu này khám phá không gian hoạt động hoán vị
trên K tài liệu (xdoc
1,···,xdoc
K) để tối ưu hóa kết hợp.
3 Sơ bộ: LLMLingua
LLMLingua (Jiang et al., 2023a) sử dụng một mô
hình ngôn ngữ nhỏ MS để đánh giá độ phức tạp
của mỗi token prompt, loại bỏ những token có độ
phức tạp thấp hơn. Phương pháp này dựa trên ý
tưởng rằng các token có độ phức tạp thấp hơn có
tác động không đáng kể đến tổng độ tăng entropy
của mô hình ngôn ngữ, ngụ ý rằng việc loại bỏ
chúng ít ảnh hưởng đến hiểu biết ngữ cảnh của
LLMs. Quá trình này được xem như một ứng dụng
của "LM is Compression" (Delétang et al., 2023).
LLMLingua bao gồm ba thành phần chính: bộ điều
khiển ngân sách, nén prompt cấp token lặp lại, và
căn chỉnh phân phối, được làm nổi bật bằng văn
bản in nghiêng trong Hình 2.
Bộ điều khiển ngân sách gán các tỷ lệ nén khác
nhau cho các phần khác nhau của prompt (tức là,
hướng dẫn, ví dụ minh họa, câu hỏi), thực hiện
nén prompt cấp thô. Các bước tiếp theo bao gồm
chia kết quả trung gian thành các đoạn và áp dụng
nén cấp token một cách lặp lại, trong đó độ phức
tạp của mỗi token dựa trên các đoạn đã nén trước
đó. Để nhận biết các LLM mục tiêu khác nhau,
LLMLingua tinh chỉnh MS bằng dữ liệu từ LLM
mục tiêu.
4 LongLLMLingua
LongLLMLingua được xây dựng dựa trên LLM-
Lingua để nén prompt tốt hơn trong các kịch bản
ngữ cảnh dài. Nó giải quyết ba vấn đề chính trong
việc xử lý ngữ cảnh dài, như được giới thiệu trong
Mục 1. Cách tiếp cận này tập trung vào việc làm
cho LLMs hiệu quả hơn trong việc nhận biết thông
tin quan trọng liên quan đến câu hỏi trong prompt.
Nó bao gồm ba góc độ và tiếp tục kết hợp chiến
lược khôi phục dãy con, như được hiển thị trong
Hình 2, để nâng cao độ chính xác và độ tin cậy
của thông tin được cung cấp cho người dùng. Trong
mục này, chúng tôi chi tiết cách mỗi phần của
LongLLMLingua hoạt động để cải thiện việc LLMs
xử lý ngữ cảnh dài.
4.1 Làm thế nào để cải thiện mật độ thông tin
quan trọng trong prompt?
Nén Thô-Hạt Biết Câu Hỏi
Trong nén thô-hạt, chúng tôi nhằm tìm ra một
chỉ số rk để đánh giá tầm quan trọng của mỗi tài
liệu xdoc
k={xdoc
k,i}Nk
i=1, trong đó Nk là số lượng
token trong xdoc
k. Chúng tôi chỉ giữ lại xdoc
k với rk
cao hơn như kết quả nén trung gian. Một cách tiếp
cận để cải thiện mật độ thông tin quan trọng trong
các prompt đã nén là tính độ phức tạp cấp tài liệu
có điều kiện trên câu hỏi p(xdoc
k|xque). Tuy nhiên,
phương pháp này có thể không hiệu quả vì các tài
liệu thường chứa một lượng lớn thông tin không
liên quan. Ngay cả khi có điều kiện trên xque, các
điểm số độ phức tạp được tính cho toàn bộ tài liệu
có thể không đủ phân biệt, khiến chúng trở thành
chỉ số không phù hợp cho việc nén cấp tài liệu.
Chúng tôi đề xuất sử dụng độ phức tạp của câu
hỏi xque có điều kiện trên các ngữ cảnh khác nhau
xdoc
k p(xque|xdoc
k) để biểu diễn mối liên kết giữa
chúng. Chúng tôi cũng thêm một tuyên bố hạn chế2
xrestrict sau xque để tăng cường sự kết nối của xque
và xdoc
k. Nó có thể được coi như một thuật ngữ
chính quy hóa giảm thiểu tác động của ảo giác.
Điều này có thể được công thức hóa như:
rk=−1
NcNcX
ilogp(xque,restrict
i |xdoc
k),
k∈ {1,2,···, K},(2)
trong đó xque,restrict
i là token thứ i trong chuỗi
nối của xque và xrestrict và Nc là số lượng token.
Hình 3a hiển thị phân phối recall của các phương
pháp truy xuất khác nhau, bao gồm các phương pháp
liên quan truyền thống (BM25, Gzip (Jiang et al.,
2023b)), các phương pháp dựa trên embedding
(OpenAI-embedding, Voyageai3, BGE-large-en v1.5
(Xiao et al., 2023), Sentence-BERT (Reimers and
Gurevych, 2019), Jina (Günther et al., 2023)), và
các phương pháp reranker (Cohere-Rerank4, BGE-
llmembeder, BGE-Ranker-large), điều này cho thấy
rằng cách tiếp cận nén cấp thô của chúng tôi đạt
được recall cao nhất với số lượng tài liệu được giữ
lại khác nhau, gợi ý rằng nó bảo tồn nhiều thông
tin quan trọng nhất từ các ngữ cảnh trong kết quả
đã nén.
Nén Tinh-Hạt Biết Câu Hỏi
Trong nén tinh-hạt, chúng tôi đánh giá tầm quan
trọng của mỗi token trong hướng dẫn xins, câu hỏi
xque, và K′ tài liệu {xdoc
i}K′
i=1 được giữ lại sau nén
thô-hạt. Chúng tôi kết hợp cơ chế nén lặp lại theo
LLMLingua và tính trực tiếp độ phức tạp token để
nén xins và xque. Trong mục này, chúng tôi điều
tra cách làm cho việc nén cấp token tinh-hạt trên
{xdoc
k}K′
k=1 biết về câu hỏi xque, để kết quả đã nén
có thể chứa nhiều thông tin quan trọng liên quan
đến câu hỏi hơn.
Một giải pháp đơn giản cho việc nhận biết xque
là nối nó vào đầu toàn bộ ngữ cảnh. Tuy nhiên,
điều này sẽ dẫn đến độ phức tạp thấp của các token
liên quan trong ngữ cảnh theo điều kiện của câu
hỏi xque, làm giảm thêm sự phân biệt của chúng
với các token khác.
Trong bài báo này, chúng tôi đề xuất độ phức tạp
tương phản, tức là sự thay đổi phân phối do điều
kiện của câu hỏi gây ra, để biểu diễn mối liên kết
giữa token và câu hỏi. Chỉ số tầm quan trọng dựa
trên độ phức tạp tương phản si cho mỗi token xi
trong {xdoc
k}K′
k=1 có thể được công thức hóa như:
si=perplexity (xi|x<i)−perplexity (xi|xque, x<i).
(3)
Ngoài ra, chúng tôi cung cấp sự suy dẫn về ý nghĩa
toán học của nó trong Phụ lục A, kết luận rằng nó
tương đương với thông tin tương hỗ pointwise có
điều kiện (Church và Hanks, 1989).
Hình 3b minh họa sự khác biệt giữa độ phức tạp
và độ phức tạp tương phản. Phân phối của độ phức
tạp xuất hiện ngẫu nhiên, khiến việc trích xuất
thông tin liên quan đến câu hỏi trở nên khó khăn.
Tuy nhiên, các token có độ phức tạp tương phản
cao có xu hướng tập trung gần tài liệu ground-truth,
chứa thông tin liên quan đến câu hỏi. Điều này
cho thấy rằng độ phức tạp tương phản được đề
xuất có thể phân biệt tốt hơn các token liên quan
đến câu hỏi, từ đó cải thiện mật độ thông tin quan
trọng trong kết quả đã nén.
4.2 Làm thế nào để giảm mất mát thông tin ở
giữa?
Như được chứng minh trong Hình 1b, LLM đạt
được hiệu suất cao nhất khi thông tin liên quan
xuất hiện ở đầu và suy giảm đáng kể nếu thông
tin liên quan nằm ở giữa các ngữ cảnh dài. Sau
nén thô-hạt, chúng ta có được một tập hợp các tài
liệu {xdoc
k}K′
k=1 với các điểm số tầm quan trọng tương
ứng {rk}K′
k=1 chỉ ra mối liên kết của chúng với câu
hỏi xque. Do đó, chúng tôi sắp xếp lại các tài liệu
bằng cách sử dụng điểm số tầm quan trọng của
chúng để tận dụng tốt hơn sự khác biệt nhận thức
thông tin của LLMs ở các vị trí:
(xins,xdoc
1,···,xdoc
K′,xque)rk−→
(xins,xdoc
r1,···,xdoc
rK′,xque)(4)
4.3 Làm thế nào để đạt được kiểm soát hạt
thích ứng trong quá trình nén?
Trong nén tinh-hạt, LLMLingua áp dụng cùng một
tỷ lệ nén trên tất cả tài liệu thu được từ bộ điều
khiển ngân sách. Tuy nhiên, mật độ thông tin quan
trọng của các tài liệu khác nhau là khác nhau. Tài
liệu càng liên quan đến câu hỏi, chúng ta càng nên
phân bổ nhiều ngân sách hơn (tức là tỷ lệ nén thấp
hơn) cho nó. Do đó, chúng tôi kết nối nén thô-hạt
với nén tinh-hạt và sử dụng các điểm số tầm quan
trọng {rk}K′
k=1 thu được từ nén thô-hạt để hướng
dẫn việc phân bổ ngân sách trong nén tinh-hạt.
Bằng cách này, chúng ta có thể đạt được kiểm soát
hạt thích ứng trên toàn bộ.
Cụ thể, đầu tiên chúng tôi xác định ngân sách ban
đầu cho các tài liệu được giữ lại5 τdoc bằng cách
sử dụng bộ điều khiển ngân sách của LLMLingua.
Trong quá trình nén tinh-hạt, chúng tôi tuân theo
thuật toán nén cấp token lặp lại trong LLMLingua
nhưng phân bổ động ngân sách nén τdoc
k cho mỗi
tài liệu xdoc
k theo chỉ số xếp hạng I(rk) (ví dụ: 0, 1)
của điểm số tầm quan trọng từ nén thô-hạt. Trong
bài báo này, chúng tôi sử dụng một bộ lập lịch
tuyến tính cho việc phân bổ thích ứng. Ngân sách
của mỗi token xi có thể được công thức hóa như:
τi=τdoc
k,∀xi∈xdoc
k,
τdoc
k= max(min((1 −2I(rk)
K′)δτ+τdoc,1),0),
(5)
trong đó i và k là chỉ số của token và tài liệu, K′
biểu thị số lượng tài liệu, và δτ là một siêu tham
số điều khiển ngân sách tổng thể cho việc phân bổ
động.
4.4 Làm thế nào để cải thiện tính toàn vẹn của
thông tin quan trọng?
Trong quá trình tạo sinh, LLMs có xu hướng sao
chép các thực thể được tìm thấy trong prompt, như
tên, địa điểm, và tổ chức. Việc nén các thực thể
này ở cấp độ token không ảnh hưởng đến hiểu
biết nội dung ngữ nghĩa của LLMs nhưng có thể
dẫn đến lỗi trong nội dung được tạo.
Do đó, chúng tôi đề xuất một phương pháp khôi
phục dãy con để khôi phục nội dung gốc trong phản
hồi của LLMs. Phương pháp này dựa vào mối quan
hệ dãy con giữa các token trong prompt gốc, prompt
đã nén, và phản hồi của LLMs, như được hiển thị
trong Hình 4.
Quy trình tổng thể bao gồm: i) Lặp qua các token
yl trong phản hồi của LLMs và chọn chuỗi con dài
nhất eykey,l={yl, yl+1, ..., yr} xuất hiện trong prompt
đã nén ex. ii) Tìm dãy con chung ngắn nhất tối đa
xi,j={xi, xi+1, ..., xj} trong prompt gốc x, tương
ứng với biểu diễn eykey,l trong prompt gốc (được
tăng tốc bằng cây tiền tố hoặc ôtômát chuỗi). iii)
Thay thế các token được khớp eykey,l trong phản
hồi của LLMs bằng dãy con tương ứng xi,j từ
prompt gốc. Để biết thêm chi tiết, vui lòng tham
khảo Thuật toán 1.
Thuật toán 1 Thuật toán Khôi phục Dãy con Cấp Token
Đầu vào : Prompt gốc x; prompt đã nén ex; phản hồi tạo sinh của LLMs y.
1:Đặt danh sách phản hồi cuối cùng yrec=ϕ, chỉ số token trái của dãy con l thành 0.
2:while l <y.len()do
3: ifChuỗi con yl∈exthen
4: Tìm chuỗi con dài hơn eykey,l={yl, yl+1,
..., yr} ∈ex.
5: Tìm dãy con chung ngắn nhất tối đa
xi,j={xi, xi+1, ..., xj}trong prompt gốc x.
6: Thêm dãy con xi,j={xi, xi+1, ..., xj}
vào phản hồi yrec.
7: Đặt chỉ số trái l thành r+ 1.
8: else
9: Thêm token yl vào phản hồi yrec.
10: Đặt chỉ số trái l thành l+ 1.
11: end if
12:end while
Đầu ra : Danh sách phản hồi cuối cùng yrec.
5 Thí nghiệm
Ở đây, chúng tôi điều tra: (1) LongLLMLingua
hiệu quả như thế nào? (2) LongLLMLingua hiệu
quả ra sao?
Chi tiết triển khai Trong bài báo này, chúng tôi
sử dụng GPT-3.5-Turbo-06136 và LongChat-13B-
16k làm LLMs mục tiêu, cả hai đều có thể truy cập
qua OpenAI7 và HuggingFace8. Để đảm bảo kết quả
ổn định và có thể tái tạo, chúng tôi sử dụng giải
mã tham lam và đặt nhiệt độ thành 0 trong tất cả
các thí nghiệm. Đối với các mô hình ngôn ngữ nhỏ
được sử dụng để nén, chúng tôi áp dụng LLaMA-
2-7B-Chat9, đã được căn chỉnh bằng việc tinh chỉnh
có giám sát và RLHF. Chúng tôi triển khai cách
tiếp cận của mình với PyTorch 1.13.1 và Hugging-
Face Transformers. Chúng tôi thiết lập các siêu
tham số theo LLMLingua ngoại trừ kích thước đoạn
được sử dụng trong nén cấp token lặp lại được đặt
thành 200 ở đây. Chi tiết hơn được cung cấp trong
Phụ lục B.
Tập dữ liệu & chỉ số đánh giá Chúng tôi sử dụng
NaturalQuestions cho nhiệm vụ QA đa tài liệu, và
sử dụng LongBench và ZeroSCROLLS cho các kịch
bản ngữ cảnh dài chung. Chúng tôi cũng kiểm tra
trên các nhiệm vụ QA multi-hop bằng tập dữ liệu
MuSiQue (Trivedi et al., 2022), và các nhiệm vụ
QA phụ thuộc dài bằng benchmark LooGLE (Li
et al., 2023b). Vui lòng tham khảo Phụ lục C để
biết thêm chi tiết về các tập dữ liệu.
Đường cơ sở Chúng tôi bao gồm hai tập đường
cơ sở trong các thí nghiệm sau:
(i) Phương pháp Dựa trên Truy xuất. Chúng tôi
đánh giá mối liên kết câu hỏi-tài liệu trong prompt
bằng cách sử dụng năm phương pháp truy xuất
SoTA: BM25, Gzip (Jiang et al., 2023b), Sentence-
BERT (Reimers và Gurevych, 2019), OpenAI
Embedding, và chỉ số quan trọng rk của bộ xếp
hạng LongLLMLingua cho nén thô-hạt. Đáng chú
ý, việc nén dựa trên mô hình embedding phản ánh
phương pháp trong Xu et al. (2024). Chúng tôi loại
bỏ các câu hoặc đoạn văn có liên quan thấp để đáp
ứng giới hạn nén, duy trì chuỗi tài liệu gốc.
(ii) Phương pháp Dựa trên Nén. Chúng tôi so
sánh cách tiếp cận của chúng tôi với hai phương
pháp hiện đại cho việc nén prompt, tức là Selective
Context (Li et al., 2023c) và LLMLingua (Jiang
et al., 2023a). Cả hai phương pháp đều sử dụng
LLaMA-2-7B-Chat làm mô hình ngôn ngữ nhỏ cho
việc nén. Trong LLMLingua, một cách tiếp cận từ
thô đến tinh được sử dụng để xử lý các ràng buộc
của tỷ lệ nén: prompt gốc đầu tiên được nén thành
k lần ràng buộc ở cấp độ thô, trong đó k là hệ số
kiểm soát hạt; sau đó thực hiện cấp token để đạt
ràng buộc tổng thể. Phương pháp của chúng tôi
tuân theo cùng logic thô-đến-tinh để đạt được ràng
buộc.
Kết quả chính Bảng 1 và 2 trình bày hiệu suất
của các phương pháp khác nhau dưới các ràng buộc
nén khác nhau. Có nhiều quan sát và kết luận: (1)
LongLLMLingua của chúng tôi đạt được hiệu suất
tốt nhất trên các nhiệm vụ và ràng buộc tỷ lệ nén
khác nhau. So với prompt gốc, prompt đã nén của
chúng tôi có thể đạt được hiệu suất cao hơn với
chi phí thấp hơn nhiều. Ví dụ, LongLLMLingua
tăng hiệu suất 21.4% trên NaturalQuestions với
tài liệu ground-truth ở vị trí thứ 10, trong khi số
lượng token đầu vào cho GPT3.5-Turbo là ∼4x ít
hơn. (2) Các phương pháp dựa trên nén như Selective
Context (Li et al., 2023c) và LLMLingua (Jiang
et al., 2023a) hoạt động kém trên hầu hết các nhiệm
vụ, đặc biệt là những nhiệm vụ có nhiều thông tin
không liên quan trong prompt gốc. Điều này là do
cơ chế nén dựa trên entropy thông tin thuần túy
của chúng, bao gồm quá nhiều nhiễu trong kết quả
đã nén và thậm chí dẫn đến hiệu suất tệ hơn so với
cài đặt zero-shot, ví dụ, trên NaturalQuestions. (3)
Các phương pháp dựa trên truy xuất hoạt động tốt
với tỷ lệ nén thấp. Tuy nhiên, hiệu suất của chúng
giảm khi việc nén tiến triển, ví dụ, 2x→4x; 3000
token → 2000 token. Điều này có thể do recall
giảm. Hình 3a là minh họa các trường hợp trên
NaturalQuestions. (4) LongLLMLingua cũng như
chỉ số nén thô-hạt rk của chúng tôi mạnh mẽ hơn
nhiều so với tất cả các đường cơ sở khác dưới các
nhiệm vụ và ràng buộc nén khác nhau. Với việc
tăng tỷ lệ nén, ví dụ, 2x→4x, LongLLMLingua
thậm chí đạt được một chút tăng hiệu suất. Chúng
tôi chủ yếu quy điều này cho việc nén thô-đến-tinh
biết câu hỏi, có thể tìm ra thông tin quan trọng tốt
hơn và đạt được mật độ thông tin quan trọng cao
hơn với tỷ lệ nén cao hơn. (5) Phương pháp sắp
xếp lại được đề xuất giúp ích không chỉ cho cách
tiếp cận của chúng tôi mà còn cho các đường cơ
sở khác, chứng minh tốt tính hiệu quả của nó. (6)
So với kết quả với ràng buộc 2.000 token, hiệu
suất tổng thể của 3.000 token đã được cải thiện.
LongLLMLingua thấy tăng 1.2 điểm trong điểm
trung bình và tăng tốc 1.6x trong độ trễ đầu cuối.
Trong kịch bản này, tỷ lệ recall của các phương
pháp dựa trên truy xuất đã tăng, dẫn đến cải thiện
đáng kể trong độ chính xác của chúng. Ví dụ, BM25
đạt được điểm trung bình 48.9.
Ngoài ra, chúng tôi cũng trình bày kết quả thí
nghiệm trên các tập dữ liệu như MuSicQue, LooGLE,
ZEROSCROLLS, v.v., trong Phụ lục C.
Nghiên cứu Ablation Để đánh giá đóng góp của
các thành phần khác nhau trong LongLLMLingua,
chúng tôi giới thiệu các biến thể sau của nó cho
nghiên cứu ablation. (1) Các biến thể về Nén Thô-
hạt Biết Câu Hỏi, bao gồm: ours w/o Question-
awareness, tính toán liên quan câu hỏi-văn bản rk
bằng entropy thông tin trong LLMLingua, ours w/
SBERT, sử dụng SBERT để tính rk, ours w/
p(xdoc
k|xque,restrict
i ), thay thế p(xque,restrict
i |xdoc
k) bằng
p(xdoc
k|xque,restrict
i ) trong Eq. (2), và ours w/o restrict,
chỉ tính xác suất có điều kiện tương ứng với xque.
(2) Ours w/o Question-aware Fine-grained, bỏ qua
Eq. (3) và chỉ áp dụng Nén Prompt Cấp Token Lặp
lại như LLMLingua. (3) Ours w/o Dynamic Com-
pression Ratio, trong đó tất cả tài liệu chia sẻ cùng
tỷ lệ nén trong nén tinh-hạt. (4) Ours w/o và (5)
LLMLingua w/ Subsequence Recovery, loại bỏ
hoặc thêm chiến lược khôi phục dãy con sau xử lý.
(6) Ours w/ GPT2-small, sử dụng mô hình GPT2-
small làm MS.
Bảng 3, 4, và 7 hiển thị kết quả của nghiên cứu
ablation trong các nhiệm vụ khác nhau. Tóm lại,
việc loại bỏ bất kỳ thành phần nào được đề xuất
cho LongLLMLingua sẽ dẫn đến giảm hiệu suất
bất kể vị trí của câu trả lời ground-truth. Điều này
xác nhận tính cần thiết và hiệu quả của cơ chế biết
câu hỏi được đề xuất trong quá trình nén thô-đến-
tinh, tỷ lệ nén động, và chiến lược khôi phục dãy
con. Nó cũng cho thấy rằng việc áp dụng SBERT
cho nén thô-hạt sẽ dẫn đến hiệu suất kém hơn, điều
này ngụ ý sự vượt trội của chỉ số tầm quan trọng
biết câu hỏi của chúng tôi trong Eq. (2) so với
SBERT. Ngoài ra, việc thay thế p(xque,restrict
i |xdoc
k)
bằng p(xdoc
k|xque,restrict
i ) có thể ảnh hưởng lớn đến
hiệu suất do nhiễu lớn trong việc tính p(xdoc
k) vì
độ phức tạp của tài liệu phụ thuộc vào nhiều thông
tin khác ngoài câu hỏi. Việc loại bỏ tuyên bố hạn
chế có thể tăng ảo giác của các mô hình ngôn ngữ
nhỏ, dẫn đến giảm hiệu suất. Hơn nữa, chiến lược
khôi phục dãy con của chúng tôi cũng có thể mang
lại tăng hiệu suất cho LLMLingua. Tuy nhiên, không
có cơ chế biết câu hỏi của chúng tôi, kết quả từ
LLMLingua vẫn kém thỏa mãn hơn. Để biết thêm
các trường hợp chi tiết, vui lòng xem Phụ lục E.
Đánh giá Độ trễ Chúng tôi thực hiện kiểm tra độ
trễ đầu cuối trên V100-32G, sử dụng các prompt
từ Multi-document QA, LongBench, và Zero-
SCROLLS trong lời gọi API, và kết quả được hiển
thị trong Bảng 1, 2 và 6. Độ trễ bao gồm thời gian
chi phí cho việc nén prompt và thời gian yêu cầu
cho LLMs, với nhiều lần đo được thực hiện và lấy
trung bình. Kết quả chứng minh rằng LongLLM-
Lingua thực sự tăng tốc suy luận tổng thể dưới các
tỷ lệ nén và kịch bản khác nhau. Hơn nữa, với việc
tăng tỷ lệ nén, hiệu ứng tăng tốc trở nên rõ rệt hơn
lên đến 2.6x. Tuy nhiên, OpenAI embedding và
Selective-Context dẫn đến thời gian độ trễ dài hơn,
do các lời gọi API lặp lại và việc tính toán entropy
tuần tự của các đơn vị ngữ nghĩa, tương ứng.
6 Các Công trình Liên quan
Ngữ cảnh dài cho LLMs. Nghiên cứu gần đây tập
trung vào việc mở rộng kích thước cửa sổ của
LLMs. Các cách tiếp cận chính bao gồm: (1) Tiền
huấn luyện theo giai đoạn (Nijkamp et al., 2023)
tăng dần cửa sổ ngữ cảnh; (2) Chỉnh sửa (Press
et al., 2022) hoặc nội suy embedding vị trí (Chen
et al., 2023; Peng et al., 2024); (3) Sử dụng cơ chế
attention tuyến tính hoặc thưa (Ding et al., 2023;
Sun et al., 2023); (4) Sử dụng các mô-đun bộ nhớ
ngoài để lưu trữ ngữ cảnh (Bertsch et al., 2023;
Tworkowski et al., 2023). Trong khi các phương
pháp này giải quyết việc mở rộng cửa sổ ngữ cảnh,
tác động của chúng đến hiệu suất nhiệm vụ down-
stream vẫn chưa được thảo luận.
Phân phối thông tin trong prompt. Các thí nghiệm
thực nghiệm gần đây đã cho thấy hiệu suất LLM
giảm với thông tin hiệu quả ít hơn trong một prompt
(Bai et al., 2023; Li et al., 2023a; Shi et al., 2023).
Hơn nữa, vị trí của thông tin liên quan trong một
prompt có tác động đáng kể đến hiệu suất (Wu
et al., 2023b). Liu et al. (2024) gợi ý rằng LLMs
khó hiểu thông tin nằm ở giữa prompt hơn so với
những thông tin ở các cạnh.
Các phương pháp truy xuất có thể được phân loại
như các phương pháp truy xuất dày đặc hoặc thưa
thớt. Các phương pháp truy xuất thưa thớt, như
BM25, xác định sự liên quan giữa truy vấn và tài
liệu dựa trên thông tin n-gram. Ngược lại, các
phương pháp truy xuất dày đặc đánh giá sự liên
quan giữa truy vấn và tài liệu trong không gian
tiềm ẩn bằng cách sử dụng mô hình embedding
(Reimers và Gurevych, 2019; Xiao et al., 2023;
Günther et al., 2023) và mô hình reranker (Xiao
et al., 2023). Gần đây, Jiang et al. (2023b) đề xuất
một phương pháp truy xuất dày đặc không giám
sát tận dụng các thuật toán nén truyền thống, như
gzip, và k-nearest neighbors.
Các phương pháp nén prompt có thể được nhóm
thành ba loại chính: (1) Cắt tỉa token (Goyal et al.,
2020; Kim và Cho, 2021; Modarressi et al., 2022)
và gộp token (Bolya et al., 2023), cần tinh chỉnh
mô hình hoặc kết quả trung gian trong quá trình
suy luận và đã được sử dụng với các mô hình quy
mô BERT. (2) Các phương pháp tinh chỉnh soft
prompt như GIST (Mu et al., 2023), AutoCom-
pressor (Chevalier et al., 2023), và ICAE (Ge et al.,
2024), yêu cầu tinh chỉnh tham số LLMs, làm cho
chúng phù hợp cho các domain cụ thể nhưng không
trực tiếp áp dụng được cho các LLMs hộp đen. (3)
Các cách tiếp cận dựa trên entropy thông tin như
Selective Context (Li et al., 2023c) và LLMLingua
(Jiang et al., 2023a), sử dụng một mô hình ngôn
ngữ nhỏ để tính thông tin tự thân hoặc độ phức tạp
của mỗi token trong prompt gốc và sau đó loại bỏ
các token có độ phức tạp thấp hơn.
7 Kết luận
Chúng tôi đề xuất LongLLMLingua để giải quyết
ba thách thức, tức là chi phí tính toán cao hơn,
giảm hiệu suất, và thiên lệch vị trí cho LLMs trong
các kịch bản ngữ cảnh dài. Chúng tôi phát triển
LongLLMLingua từ góc độ nén prompt hiệu quả,
do đó giảm chi phí tính toán. Chúng tôi tiếp tục
thiết kế bốn thành phần, tức là phương pháp nén
thô-đến-tinh biết câu hỏi, cơ chế sắp xếp lại tài
liệu, tỷ lệ nén động, và chiến lược khôi phục dãy
con để cải thiện khả năng nhận thức thông tin quan
trọng của LLMs, với đó LongLLMLingua chứng
minh hiệu suất vượt trội. Các thí nghiệm trên QA
đa tài liệu, QA multi-hop, và các benchmark ngữ
cảnh dài chứng minh rằng prompt đã nén LongLLM-
Lingua có thể đạt được hiệu suất cao hơn so với
các prompt gốc trong khi cả chi phí API cho suy
luận và độ trễ hệ thống đầu cuối đều được giảm
đáng kể.
Hạn chế
Mặc dù các thí nghiệm trước đây chứng minh tính
hiệu quả và hiệu quả của LongLLMLingua trên
một loạt rộng các nhiệm vụ, phương pháp vẫn có
những hạn chế sau: 1) LongLLMLingua là một
cách tiếp cận biết câu hỏi, có nghĩa là nó yêu cầu
nén lại cho các câu hỏi khác nhau, ngay cả với
cùng ngữ cảnh, ngăn cản việc lưu cache ngữ cảnh.
Hơn nữa, về mặt chi phí tính toán, LongLLMLingua
tăng tính toán gấp đôi so với LLMLingua. Điều
này có thể dẫn đến chi phí phụ lớn hơn trong các
ứng dụng thực tế. Tuy nhiên, vấn đề này có thể
được giảm thiểu bằng cách mở rộng cách tiếp cận
biết câu hỏi thành cách tiếp cận biết nhiệm vụ, cho
phép tái sử dụng và lưu cache. 2) Trong khi tính
hiệu quả của LongLLMLingua đã được kiểm tra
trên một loạt rộng các nhiệm vụ, đặc biệt là trên
tập dữ liệu QA multi-hop MuSicQue (Trivedi et al.,
2022), tính hiệu quả của nó có thể bị ảnh hưởng
khi mối quan hệ giữa ngữ cảnh và prompt phức
tạp và tinh tế hơn do cách tiếp cận biết câu hỏi
cấp thô.
Tài liệu tham khảo
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2024. Self-RAG: Learning to
retrieve, generate, and critique through self-reflection.
InThe Twelfth International Conference on Learning
Representations .
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
A bilingual, multitask benchmark for long context
understanding. ArXiv preprint , abs/2308.14508.
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R. Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, and Judy Hoffman.
2023. Token merging: Your vit but faster. In The
Eleventh International Conference on Learning Rep-
resentations .
Harrison Chase. 2022. LangChain.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
ArXiv preprint , abs/2306.15595.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
Danqi Chen. 2023. Adapting language models to
compress contexts. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 3829–3846, Singapore. Associa-
tion for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In 27th Annual Meeting of the Association
for Computational Linguistics , pages 76–83, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
Grégoire Delétang, Anian Ruoss, Paul-Ambroise
Duquenne, Elliot Catt, Tim Genewein, Christo-
pher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
Matthew Aitchison, Laurent Orseau, et al. 2023. Lan-
guage modeling is compression. ArXiv preprint ,
abs/2309.10668.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, and Furu Wei. 2023.
Longnet: Scaling transformers to 1,000,000,000 to-
kens. ArXiv preprint , abs/2307.02486.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2023. A survey for in-context learning.
ArXiv preprint , abs/2301.00234.
Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen,
and Furu Wei. 2024. In-context autoencoder for con-
text compression in a large language model. In The
Twelfth International Conference on Learning Repre-
sentations .
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh
Raje, Venkatesan T. Chakaravarthy, Yogish Sabhar-
wal, and Ashish Verma. 2020. Power-bert: Accel-
erating BERT inference via progressive word-vector
elimination. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119 of Proceedings
of Machine Learning Research , pages 3690–3699.
PMLR.
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaed-
dine Abdessalem, Tanguy Abel, Mohammad Kalim
Akram, Susana Guzman, Georgios Mastrapas, Saba
Sturua, Bo Wang, Maximilian Werk, Nan Wang, and
Han Xiao. 2023. Jina embeddings 2: 8192-token
general-purpose text embeddings for long documents.
ArXiv preprint , abs/2310.19923.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research .
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing
Yang, and Lili Qiu. 2023a. LLMLingua: Compress-
ing prompts for accelerated inference of large lan-
guage models. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 13358–13376. Association for Com-
putational Linguistics.
Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
Tang, Yiqin Dai, and Jimmy Lin. 2023b. "low-
resource" text classification: A parameter-free clas-
sification method with compressors. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 6810–6828, Toronto, Canada. Associa-
tion for Computational Linguistics.
Greg Kamradt. 2023. Needle In A Haystack - Pressure
Testing LLMs.
Gyuwan Kim and Kyunghyun Cho. 2021. Length-
adaptive transformer: Train once with length drop,
use anytime with search. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 6501–6511, Online. Association
for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics , 7:452–466.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
min Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe
Ma, and Hao Zhang. 2023a. How long can open-
source llms truly promise on context length?
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan
Zhang. 2023b. Loogle: Can long-context language
models understand long contexts? ArXiv preprint ,
abs/2311.04939.
Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.
2023c. Compressing context to enhance inference
efficiency of large language models. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 6342–6353,
Singapore. Association for Computational Linguis-
tics.
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2024. Lost in the Middle: How Language
Models Use Long Contexts. Transactions of the Asso-
ciation for Computational Linguistics , 12:157–173.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022. MetaICL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2791–2809, Seattle, United States.
Association for Computational Linguistics.
Ali Modarressi, Hosein Mohebbi, and Moham-
mad Taher Pilehvar. 2022. AdapLeR: Speeding up
inference by adaptive length reduction. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 1–15, Dublin, Ireland. Association for
Computational Linguistics.
Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.
Learning to compress prompts with gist tokens. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang,
Congying Xia, Chen Xing, Jesse Vig, Semih
Yavuz, Philippe Laban, Ben Krause, Senthil Purush-
walkam, Tong Niu, Wojciech Kry ´sci´nski, Lidiya Mu-
rakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri,
Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-
Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq
Joty, and Caiming Xiong. 2023. Xgen-7b technical
report. ArXiv preprint , abs/2309.03450.
Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th An-
nual ACM Symposium on User Interface Software
and Technology , UIST '23, New York, NY , USA.
Association for Computing Machinery.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
Shippole. 2024. YaRN: Efficient context window ex-
tension of large language models. In The Twelfth
International Conference on Learning Representa-
tions .
Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,
and Omer Levy. 2023. ZeroSCROLLS: A zero-shot
benchmark for long text understanding. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 7977–7989, Singapore.
Association for Computational Linguistics.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2024. Hugging-
gpt: Solving ai tasks with chatgpt and its friends
in hugging face. Advances in Neural Information
Processing Systems , 36.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Inter-
national Conference on Machine Learning , pages
31210–31227. PMLR.
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma,
Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu
Wei. 2023. Retentive network: A successor to trans-
former for large language models. ArXiv preprint ,
abs/2307.08621.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. MuSiQue: Multi-
hop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics , 10:539–554.
Szymon Tworkowski, Konrad Staniszewski, Mikołaj
Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr
Miło ´s. 2023. Focused transformer: Contrastive train-
ing for context scaling. In Thirty-seventh Conference
on Neural Information Processing Systems .
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,
Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadal-
lah, Ryen W White, Doug Burger, and Chi Wang.
2023a. Autogen: Enabling next-gen llm applica-
tions via multi-agent conversation framework. ArXiv
preprint , abs/2308.08155.
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2023b. Self-adaptive in-context learn-
ing: An information compression perspective for in-
context example selection and ordering. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1423–1436, Toronto, Canada. Association for
Computational Linguistics.
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
Muennighoff. 2023. C-pack: Packaged resources to
advance general chinese embedding. ArXiv preprint ,
abs/2309.07597.
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina
Bakhturina, Mohammad Shoeybi, and Bryan Catan-
zaro. 2024. Retrieval meets long context large lan-
guage models. In The Twelfth International Confer-
ence on Learning Representations .
A Suy dẫn Nén Tinh-Hạt Biết Câu Hỏi
Dựa trên định nghĩa của Eq. (3), chúng ta có thể
suy dẫn rằng,
si=perplexity (xi|x<i)−perplexity (xi|xque, x<i)
=q(xi) logp(xi|xque, x<i)−q(xi) logp(xi|x<i)
=q(xi) logp(xi|xque, x<i)
p(xi|x<i)
(6)
Trong việc tính toán thực tế của độ phức tạp, một
phép toán log được thực hiện để tránh tràn, và
q(xi) biểu diễn phân phối xác suất của ground-
truth.
Đồng thời, chúng ta có thể suy dẫn biểu thức
mở rộng sau dựa trên định lý Bayes.
p(xque|xi, x<i) =p(xi|xque, x<i)p(xque)
p(xi|x<i)
=p(xque)p(xi|xque, x<i)
p(xi|x<i)(7)
Phân phối xác suất p(xque) của câu hỏi và phân
phối ground-truth q(xi) của xi là các hằng số, do
đó si có thể được coi như biểu diễn của Eq. (7).
si∝p(xque|xi, x<i) (8)
Vậy chúng ta có thể sử dụng Eq. (3) để biểu diễn
phân phối xác suất p(xque|xi, x<i), biểu diễn khả
năng có điều kiện của việc sinh ra xque cho token
xi. Do đó, chúng ta có thể biểu diễn phân phối
nhạy cảm cấp token cho câu hỏi xque chỉ bằng một
suy luận duy nhất. Đối với các token không liên
quan đến xque, như các token ở phía bên phải của
Hình 3b, lượng thông tin gốc của chúng có thể
cao, nhưng độ phức tạp tương phản vẫn ở mức tương
đối thấp. Cuối cùng, chúng ta quan sát rằng dạng
của độ phức tạp tương phản tương đương với thông
tin tương hỗ pointwise có điều kiện (Church và
Hanks, 1989).
B Chi tiết Thí nghiệm
B.1 Chi tiết Tập dữ liệu
Chúng tôi sử dụng NaturalQuestions (Liu et al.,
2024) cho nhiệm vụ QA đa tài liệu, MuSicQue
(Trivedi et al., 2022) cho nhiệm vụ QA multi-hop,
và sử dụng LongBench (Bai et al., 2023), Zero-
SCROLLS (Shaham et al., 2023), LooGLE (Li et al.,
2023b) cho các kịch bản ngữ cảnh dài chung. Chi
tiết cụ thể của tập dữ liệu như sau:
NaturalQuestions multi-document QA Một tập
dữ liệu trả lời câu hỏi đa tài liệu, gồm 2.655 vấn
đề, được xây dựng bởi (Liu et al., 2024) dựa trên
tập dữ liệu NaturalQuestions (Kwiatkowski et al.,
2019). Tập dữ liệu này cung cấp một thiết lập sinh
tăng cường truy xuất thực tế gần gũi với các ứng
dụng tìm kiếm và trả lời câu hỏi thương mại (ví dụ:
Bing Chat). Mỗi ví dụ trong tập dữ liệu chứa một
câu hỏi và k tài liệu liên quan, sử dụng hệ thống
truy xuất Contriever (Izacard et al., 2022), một
trong số đó bao gồm một tài liệu có câu trả lời
đúng. Để thực hiện nhiệm vụ này, mô hình phải
truy cập tài liệu chứa câu trả lời trong ngữ cảnh
đầu vào của nó và sử dụng nó để trả lời câu hỏi.
Dữ liệu của tập dữ liệu được lấy từ tập dữ liệu
NaturalQuestions, chứa các truy vấn lịch sử được
gửi đến công cụ tìm kiếm Google và các câu trả
lời được chú thích bởi con người được trích xuất
từ Wikipedia. Độ dài token prompt trung bình
trong benchmark này là 2.946. Đối với các thí
nghiệm của chúng tôi, chúng tôi sử dụng phiên
bản được cung cấp bởi (Liu et al., 2024) bao gồm
20 tài liệu10. Tập dữ liệu gồm năm cài đặt vị trí
tài liệu ground truth khác nhau trong prompt: 1st,
5th, 10th, 15th, và 20th.
LongBench Một benchmark ngữ cảnh dài đa nhiệm
vụ gồm 3.750 vấn đề bằng tiếng Anh và bao gồm
sáu danh mục với tổng cộng 16 nhiệm vụ. Các
nhiệm vụ này bao gồm các kịch bản ứng dụng văn
bản dài chính, như QA tài liệu đơn, QA đa tài liệu,
tóm tắt, học few-shot, nhiệm vụ tổng hợp, và hoàn
thành mã. Độ dài token prompt trung bình trong
benchmark này là 10.289. Đối với các thí nghiệm
của chúng tôi, chúng tôi sử dụng tập dữ liệu tiếng
Anh và các script đánh giá được cung cấp bởi (Bai
et al., 2023) cho benchmark này11.
ZeroSCROLLS Benchmark ngữ cảnh dài đa nhiệm
vụ gồm 4.378 vấn đề, bao gồm bốn danh mục với
tổng cộng 10 nhiệm vụ. Các nhiệm vụ này bao phủ
tóm tắt, trả lời câu hỏi, phân loại cảm xúc tổng hợp,
và sắp xếp lại thông tin. Độ dài token prompt trung
bình trong benchmark này là 9.788. Đối với các
thí nghiệm của chúng tôi, chúng tôi sử dụng tập
validation và các script đánh giá được cung cấp
bởi (Shaham et al., 2023) cho tập dữ liệu này12.
MuSiQue Tập dữ liệu trả lời câu hỏi multi-hop
được cấu thành từ 39.876, 4.834, và 4.918 vấn đề
trong các tập dữ liệu huấn luyện, validation, và
testing, tương ứng. Tập dữ liệu này yêu cầu mô
hình ngôn ngữ thực hiện nhiều suy luận dựa trên
nội dung của nhiều tài liệu và cung cấp các câu
trả lời tương ứng, do đó cần thiết một khả năng
nhất định cho việc xử lý thông tin toàn cục. Độ
dài token trung bình cho các prompt trong tập dữ
liệu này là 2.477. Đối với các thí nghiệm của chúng
tôi, chúng tôi sử dụng tập validation và các script
đánh giá được cung cấp bởi (Trivedi et al., 2022)
cho tập dữ liệu này13.
LooGLE Benchmark ngữ cảnh dài đa nhiệm vụ
gồm 6.448 vấn đề, được chia thành ba danh mục:
tóm tắt, trả lời câu hỏi phụ thuộc ngắn, và trả lời
câu hỏi phụ thuộc dài. Độ dài token prompt trung
bình trong benchmark này là 24.005. Đối với các
thí nghiệm của chúng tôi, chúng tôi tập trung vào
tập con trả lời câu hỏi phụ thuộc dài, bao gồm bốn
loại nhiệm vụ: truy xuất thông tin, sắp xếp lại
timeline, tính toán, và hiểu. Tập con này chứa
1.101 vấn đề. Chúng tôi sử dụng các script đánh
giá được cung cấp bởi (Li et al., 2023b) cho tập
dữ liệu này14.
B.2 Chi tiết Triển khai Khác
Tất cả các thí nghiệm được thực hiện bằng Tesla
V100 (32GB). Chúng tôi sử dụng tiktoken15 và
mô hình GPT-3.5-Turbo để đếm tất cả các token.
Chúng tôi đặt hệ số kiểm soát hạt k thành 2. Chúng
tôi sử dụng các tỷ lệ nén được định nghĩa trước
τins= 0.85 và τque= 0.9 cho hướng dẫn và câu hỏi.
Kích thước đoạn được sử dụng trong nén cấp token
lặp lại được đặt thành 200. δτ được sử dụng trong
tỷ lệ nén động được đặt thành 0.3. Để so sánh
công bằng, chúng tôi chỉ sử dụng sắp xếp lại trong
NaturalQuestions Multi-document QA và ghi chú
điều này trong Bảng 1. Chúng tôi sử dụng " We
can get the answer to this question in the given
documents. " làm câu hướng dẫn trong Eq. (3).
Đối với thí nghiệm đường cơ sở, chúng tôi sử dụng
mô hình được khuyến nghị mạnh nhất hiện tại,
all-mpnet-base-v216, làm mô hình biểu diễn dày
đặc cho SentenceBERT. Chúng tôi sử dụng "text-
embedding-ada-002" được khuyến nghị làm mô
hình embedding cho OpenAI Embedding17. Chúng
tôi sử dụng GPT2-dolly18 làm mô hình ngôn ngữ
nhỏ trong các thí nghiệm ablation w/ GPT2-small.
C Kết quả Thí nghiệm Bổ sung
C.1 Nghiên cứu Thực nghiệm về Nén Tinh-hạt
Biết Câu Hỏi
Hình 5 hiển thị phân phối độ phức tạp trung bình
của tài liệu khi ground-truth nằm ở nhiều vị trí
hơn trong prompt. Có thể quan sát thấy, khi độ
dài ngữ cảnh tăng, đường cong độ phức tạp gốc
vẫn tương đối ổn định. Trong các tài liệu không
liên quan, độ phức tạp cao hơn vẫn được giữ lại,
làm cho việc loại bỏ các token liên quan từ các tài
liệu liên quan trong quá trình nén prompt trở nên
dễ dàng hơn, từ đó làm hỏng thông tin ngữ nghĩa
tương ứng. Ngược lại, độ phức tạp tương phản
cho thấy sự tăng độ phức tạp trong các tài liệu
liên quan đến câu hỏi. Theo suy dẫn lý thuyết
trong Phụ lục A, biết rằng độ phức tạp tương phản
đặc trưng cho xác suất có điều kiện của các token
tương ứng với câu hỏi. Mức độ liên quan càng cao,
độ phức tạp tương phản càng cao, từ đó giữ lại
thông tin quan trọng trong quá trình nén prompt.
C.2 Ablation trong LongBench
Bảng 4 trình bày kết quả từ thí nghiệm ablation
trong benchmark ngữ cảnh dài LongBench. Có thể
quan sát thấy rằng trong các nhiệm vụ ngữ cảnh
dài khác nhau: 1) Việc loại bỏ nén thô-hạt biết
câu hỏi, nén tinh-hạt biết câu hỏi, tỷ lệ nén động,
sắp xếp lại tài liệu, và khôi phục dãy con được đề
xuất bởi LongLLMLingua đều dẫn đến giảm hiệu
suất ở các mức độ khác nhau. 2) Trong số này,
nén thô-hạt biết câu hỏi đặc biệt quan trọng đối
với QA dựa trên tài liệu và nhiệm vụ tổng hợp,
với mức giảm tối đa là 35.8 điểm; tác động của nó
đối với tóm tắt và nhiệm vụ mã tương đối nhỏ hơn.
3) Thiết kế xác suất có điều kiện trong mô-đun
nén thô-hạt biết câu hỏi cải thiện kết quả trong
tất cả các nhiệm vụ, bao gồm hoàn thành mã, trả
lời câu hỏi tài liệu đơn, và nhiệm vụ tổng hợp.
Thay đổi thứ tự xác suất có điều kiện hoặc loại
bỏ prompt hạn chế đều dẫn đến giảm hiệu suất ở
các mức độ khác nhau. 4) Việc loại bỏ nén tinh-
hạt biết câu hỏi, tỷ lệ nén động có tác động đáng
kể hơn đối với QA dựa trên tài liệu và nhiệm vụ
tổng hợp. 5) Mô-đun khôi phục dãy con có thể
nâng cao các nhiệm vụ dựa trên tham chiếu, nhưng
sự cải thiện của nó đối với các nhiệm vụ như tóm
tắt, mã, tổng hợp, v.v., tương đối nhỏ hơn. 6) Sắp
xếp lại tài liệu hiệu quả cho tất cả các loại nhiệm
vụ. Sắp xếp lại ở cấp độ tài liệu không ảnh hưởng
đến hiểu biết thông tin ngữ cảnh của LLMs, ngay
cả đối với các nhiệm vụ liên quan đến timeline
(xem timeline reorder trong LooGLE, Bảng 8).
Ngược lại, sắp xếp lại có thể giảm thiểu hiệu quả
vấn đề "lost in the middle", từ đó cải thiện hiệu
suất LLMs. 7) Sử dụng GPT2-small giảm việc
nắm bắt các token hiệu quả, nhưng vẫn có thể đạt
được kết quả gần hoặc thậm chí hơi tốt hơn so với
prompt gốc.
C.3 LongBench Sử dụng LongChat-13b-16k
Bảng 5 trình bày kết quả thí nghiệm trong benchmark
ngữ cảnh dài LongBench sử dụng LongChat-13b-
16k. Có thể thấy rằng prompt đã nén cũng có thể
đạt được kết quả tốt trên các LLMs khác, như
LongChat-13b-16k. Cụ thể, 1) có sự cải thiện tối
đa 15.5 điểm trong các nhiệm vụ tổng hợp. Ngoại
trừ một sự giảm nhẹ trong few-shot Learning, có
sự cải thiện 3-5 điểm trong các nhiệm vụ khác. 2)
Xu hướng hiệu suất của các đường cơ sở dựa trên
truy xuất và dựa trên nén tương tự như kết quả
trong GPT-3.5-Turbo.
C.4 ZeroSCROLLS
Bảng 6 trình bày phân tích hiệu suất chi tiết trên
benchmark ZeroSCROLLS. Có thể quan sát thấy
rằng trong bốn nhiệm vụ tóm tắt - GvRp, SSFD,
QMsm, SQAL, LongLLMLingua khớp gần hoặc
vượt trội nhẹ so với kết quả gốc dưới hai ràng
buộc nén. Trong khi đó, trong bốn nhiệm vụ QA
ngữ cảnh dài - Qsqr, Nrtv, QALT, MuSQ, có sự
cải thiện đáng kể. Đáng chú ý, trong nhiệm vụ
MuSiQue, dựa trên tập dữ liệu trả lời câu hỏi từ
sách và kịch bản phim, có tăng 2.1 điểm ngay cả
dưới ràng buộc 2.000 token. Đáng đề cập rằng
MuSiQue là một tập dữ liệu trả lời câu hỏi multi-
hop yêu cầu LLMs sử dụng thông tin toàn cục cho
QA phụ thuộc dài. LongLLMLingua cũng có thể
cải thiện 3.5 điểm dưới tỷ lệ nén 6x. Trong hai
nhiệm vụ sắp xếp, SpDg và BkSS, LongLLMLingua
có thể giữ lại tốt hơn thông tin nhạy cảm toàn cục,
dẫn đến cải thiện 3.0 điểm trong BkSS sau nén
prompt. Quan trọng cần lưu ý rằng mặc dù tập
dữ liệu validation ZeroScrolls tương đối nhỏ, nó
vẫn chứng minh các kết luận tương tự như quan
sát thí nghiệm trước đó trên các phương pháp và
nhiệm vụ khác nhau. Hơn nữa, nghiên cứu này
thực hiện phân tích sâu về nhiệm vụ QA multi-
hop - MuSiQue, và một benchmark ngữ cảnh dài
khác - LooGLE. Kết quả có thể được tìm thấy
trong Phụ lục C.5 và Phụ lục C.6.
C.5 MuSiQue
Bảng 7 trình bày kết quả từ tập dữ liệu trả lời câu
hỏi multi-hop MuSiQue. Từ bảng, có thể quan sát
thấy rằng trong nhiệm vụ QA multi-hop, yêu cầu
thông tin toàn cục: 1) LongLLMLingua có thể
giảm nhiễu trong prompt bằng cách loại bỏ thông
tin không liên quan và đặt nhiều thông tin liên
quan hơn ở đầu hoặc cuối prompt, từ đó cải thiện
hiệu suất 5.4 điểm. 2) Sự giảm hiệu suất rõ rệt
hơn đối với các phương pháp dựa trên truy xuất,
đặc biệt là các phương pháp dựa trên n-gram như
BM25. Do phụ thuộc dài, thông tin khớp trực tiếp
bị mất, dẫn đến ít thông tin liên quan được gọi lại.
3) Hiệu suất của các phương pháp dựa trên nén
hơi khác. Selective-Context không phân biệt giữa
độ nhạy cảm của các mô-đun khác nhau, dẫn đến
mất thông tin liên quan đến câu hỏi và hướng dẫn,
từ đó dẫn đến hiệu suất kém hơn. Tuy nhiên, LLM-
Lingua vẫn có thể giữ lại thông tin quan trọng liên
quan ở khoảng tỷ lệ nén 2x. 4) Các thí nghiệm
ablation cho thấy rằng mọi mô-đun được thiết kế
trong LongLLMLingua đều đóng vai trò trong
nhiệm vụ multi-hop. Việc loại bỏ các mô-đun nén
thô-hạt biết câu hỏi và w/ p(xdoc
k|xque,restrict
i ), có
khó khăn trong việc nhận thức phân phối tầm quan
trọng của các câu hỏi tương ứng, có thể gây ra
giảm lên đến 8 điểm. Việc loại bỏ prompt hạn chế
trong mô-đun thô biết câu hỏi cũng có thể gây ra
giảm 2 điểm do vấn đề ảo giác của LLM nhỏ.
Ngoài ra, việc loại bỏ nén tinh-hạt biết câu hỏi,
tỷ lệ nén động, và sắp xếp lại tài liệu đều có thể
gây ra giảm 0.5-2.8 điểm. 5) Hơn nữa, nếu mô
hình ngôn ngữ nhỏ trong LongLLMLingua được
thay thế bằng GPT2-small, nó có thể cải thiện thêm
tỷ lệ tăng tốc và vẫn đạt được kết quả tốt hơn 2.6
điểm so với prompt gốc.
C.6 LooGLE
Bảng 8 trình bày kết quả thí nghiệm trong benchmark
phụ thuộc dài LooGLE, có các prompt dài hơn
(∼30k) và nhiều phụ thuộc toàn cục hơn. Từ bảng,
chúng ta có thể quan sát rằng: 1) LongLLMLingua
có thể cải thiện hiệu quả hiệu suất của các nhiệm
vụ ngữ cảnh dài bằng cách nén prompt, ngay cả
đối với các nhiệm vụ phụ thuộc dài. Kết quả cho
thấy rằng LongLLMLingua cải thiện đáng kể hiệu
suất trong các nhiệm vụ như truy xuất, sắp xếp lại
timeline, và tính toán, với sự cải thiện tối đa đạt
15.9 điểm. 2) Sắp xếp lại tài liệu trong LongLLM-
Lingua hiệu quả trong tất cả các loại nhiệm vụ,
ngay cả trong các nhiệm vụ có liên quan cao đến
timeline, nó có thể cải thiện hiệu quả hiệu suất
bằng cách giảm thiểu vấn đề "lost in the middle".
3) Các phương pháp dựa trên truy xuất có xu hướng
mất hiệu suất trong các nhiệm vụ có phụ thuộc
dài hơn, như tính toán và lý luận. 4) Đối với các
phương pháp dựa trên nén, do khó khăn trong việc
nhận thức thông tin câu hỏi, có xu hướng mất
hiệu suất lớn hơn trong các nhiệm vụ truy xuất
trong ngữ cảnh dài.
D Chi phí Kinh tế
Bảng 9 trình bày chi phí suy luận ước tính trên
1.000 mẫu cho các tập dữ liệu khác nhau, bao
gồm prompt đầu vào và văn bản đầu ra được tạo,
dựa trên giá GPT-3.5-Turbo19. Cách tiếp cận của
chúng tôi chứng minh tiết kiệm đáng kể trong tài
nguyên tính toán và chi phí tiền tệ, đặc biệt trong
các tình huống ngữ cảnh dài. Giảm chi phí $3.3
(71.7%), $28.5 (90.5%), $27.4 (89.5%), $2.0
(52.6%), và $88.0 (94.0%) trên 1.000 mẫu được
quan sát cho Multi-document QA, LongBench,
ZeroScrolls, MuSiQue, và LooGLE, tương ứng.
E Phân tích Ablation
Hình 6 minh họa các prompt đã nén từ tập dữ liệu
Multi-document QA, so sánh việc sử dụng độ phức
tạp tương phản ở tỷ lệ nén cao (30x). Nó cho thấy
rằng không có nén prompt cấp token biết câu hỏi,
LongLLMLingua có xu hướng nén thông tin quan
trọng, một xu hướng trở nên rõ rệt hơn ở tỷ lệ nén
cao hơn. Ngược lại, việc sử dụng độ phức tạp tương
phản cho phép phát hiện tốt hơn thông tin quan
trọng liên quan đến câu hỏi trong ngữ cảnh, từ đó
bảo tồn thông tin quan trọng trong prompt đã nén.
F Nghiên cứu Trường hợp
Hình 7, 8, và 9 hiển thị kết quả trước và sau nén,
cũng như phản hồi của LLMs trong các kịch bản
khác nhau.
