# S2-ATTENTION: CHIA SẺ BỐI CẢNH NHẬN BIẾT PHẦN CỨNG GIỮA CÁC ĐẦU ATTENTION

Xihui Lin1∗, Yunan Zhang1∗, Suyu Ge2∗
Liliang Ren1, Barun Patra1, Vishrav Chaudhary1, Hao Peng2, Xia Song1
1Microsoft,2UIUC
{xihlin,yunanzhang}@microsoft.com

TÓM TẮT
Sparse attention, chọn lọc tập trung vào một tập con các token trong bối cảnh, đã trở thành một phương pháp được thiết lập để tăng cường hiệu quả của Transformers. Tuy nhiên, việc giảm FLOPs theo lý thuyết hiếm khi được chuyển đổi thành tăng tốc thời gian thực tế so với các đối tác dense attention, chủ yếu do thiếu các tối ưu hóa cấp phần cứng như FlashAttention (Dao, 2023). Trong khi đó, vẫn chưa rõ liệu sparse attention có thể duy trì chất lượng mô hình ở quy mô của các mô hình ngôn ngữ lớn (LLMs) ngày nay hay không, và làm thế nào để đạt được điều này. Bài báo này trình bày Sparsely-Sharded Attention (S2-ATTENTION), một thư viện kernel Triton được tối ưu hóa cung cấp nhiều triển khai sparse attention có thể tùy chỉnh cho cả huấn luyện và suy luận. S2-ATTENTION cho phép tùy chỉnh các mẫu attention ở mức độ từng đầu từng phạm vi bối cảnh. Những hiểu biết mới từ S2-ATTENTION truyền cảm hứng cho một kiến trúc sparse attention mới đáp ứng một số desiderata mà chúng tôi thấy quan trọng để đạt được cả lợi ích hiệu quả thực tế và độ chính xác mạnh trên các tác vụ downstream, được gọi là Head-Heterogenous Strided Transformer (HHST). Đối với sparsity cao hơn, HHST chia sẻ bối cảnh một cách không đồng nhất giữa các attention heads, trong đó mỗi đầu tập trung vào một tập con token khác nhau trong khi tập thể bao phủ toàn bộ. Chúng tôi đánh giá HHST bằng cách pretrain các mô hình kích thước 1.3B và 7B. Đối với tính toán attention, HHST với S2-ATTENTION đạt được 8.8× và 15.9× tăng tốc attention thời gian thực, cũng như giảm 2.8× và 2.5× thời gian huấn luyện so với baseline dense attention được triển khai với FlashAttention-2. Hơn nữa, hiệu suất tác vụ downstream của HHST ngang bằng với dense attention, và đạt được độ chính xác truy xuất hoàn hảo ở độ dài bối cảnh 128K ở quy mô 7B. Khi suy luận, HHST 7B của chúng tôi đạt được tăng tốc 4.5× so với các đối tác dense trong vLLM. S2-ATTENTION được phát hành với các API dễ tùy chỉnh để sử dụng trực tiếp trong Megatron và vLLM.

1 GIỚI THIỆU

Các LLM dựa trên Transformer đã mở ra những cơ hội mới cho cả nghiên cứu và ứng dụng (OpenAI, 2023; Touvron et al., 2023). Độ phức tạp bậc hai của chúng áp đặt chi phí cấm đoán trong việc huấn luyện và phục vụ các mô hình này. Ví dụ, huấn luyện Llama 2 (Touvron et al., 2023) 70B với độ dài bối cảnh 4K trên 2T token mất 23 ngày trên 2048 GPU A100 Rucinski (2024). Khi phục vụ, KV cache của mô hình tiêu thụ 343GB bộ nhớ GPU với kích thước batch 32 và độ dài bối cảnh 4K. Có nhu cầu cấp thiết để huấn luyện LLMs một cách hiệu quả và phục vụ chúng một cách hiệu quả về chi phí.

Nhiều công trình đã được thiết lập đã quản lý để, ít nhất trên giấy tờ, cải thiện hiệu quả của các mô hình này thông qua các kỹ thuật sparse attention khác nhau (Tay et al., 2023; Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), trong đó chỉ một tập con các token trong bối cảnh được chú ý đến. Tuy nhiên, việc tiết kiệm FLOP theo lý thuyết của chúng so với dense attention toàn bộ bối cảnh thường không mang lại những cải thiện hiệu quả trong thế giới thực. Như được chỉ ra bởi công trình tiên phong FlashAttention (Dao et al., 2022; Dao, 2023),

∗Tác giả dẫn đầu. Xihui Lin, Yunan Zhang, và Suyu Ge đóng góp như nhau. Mã có sẵn tại
https://github.com/linxihui/dkernel

overhead chính trong attention không phát sinh từ tính toán mà từ truy cập bộ nhớ GPU, đặc biệt là truy cập bộ nhớ chia sẻ (SRAM). Dense attention đã được hưởng lợi từ các triển khai cấp CUDA được tối ưu hóa đặc biệt cho I/O bộ nhớ hiệu quả, một lợi thế đáng kể mà các phương pháp sparse attention chưa nhận được. Việc thiếu một thư viện linh hoạt, hiệu quả và dễ sử dụng cho các triển khai sparse attention được tối ưu hóa đã trở thành một rào cản lớn, làm chậm tiến bộ trong cả nghiên cứu và ứng dụng trong việc cải thiện hiệu quả huấn luyện và phục vụ LLMs.

Chúng tôi nhằm mục đích thu hẹp khoảng cách này với Sparsely-Sharded Attention (S2-ATTENTION), một thư viện Triton cung cấp tối ưu hóa kernel cho sparse attention. Nó rất linh hoạt, cho phép các nhà thực hành khám phá các chiến lược sparse attention khác nhau và tùy chỉnh các mẫu attention khác nhau trên các attention heads và phạm vi bối cảnh. Xây dựng một kernel fused đa mục đích cho sparse attention đặt ra những thách thức đáng kể. Trong sparse attention, một phần của bối cảnh không được chú ý đến. Do đó, việc tiling các tensor Q, K, V, một kỹ thuật đã được chứng minh chia các tensor lớn thành các tensor nhỏ hơn để song song hóa tốt hơn và sử dụng bộ nhớ chia sẻ (SRAM) (Dao et al., 2022; Dao, 2023), thường có thể dẫn đến các thread nhàn rỗi và sử dụng SRAM không hiệu quả khi kích thước tile nhỏ. S2-ATTENTION giải quyết điều này bằng cách theo dõi hiệu quả các mẫu sử dụng KV và động hóa hợp nhất các query blocks với KVs chia sẻ vào cùng một tile. Điều này đảm bảo hiệu quả I/O, bất kể granularity sparsity, cải thiện đáng kể việc sử dụng SRAM và giảm tải KV dư thừa.

Những hiểu biết từ việc phát triển S2-ATTENTION tiết lộ rằng không phải tất cả các cơ chế sparse attention đều hiệu quả trong thực tế. Nhiều sparse attention miễn huấn luyện hiện có, bao gồm các phương pháp KV eviction như LongGen (Ge et al., 2024b), H2O (Zhang et al., 2023), và MInference (Jiang et al., 2024), ít tương thích hơn với các cơ chế phục vụ cơ bản như continuous batching (Yu et al., 2022), PagedAttention (Kwon et al., 2023), 3D parallelism (Shoeybi et al., 2020). Ví dụ, trong PagedAttention (Kwon et al., 2023), việc loại bỏ token khỏi các KV blocks chỉ tăng phân mảnh nội bộ, và mang lại overhead bổ sung trong lập lịch, điều này làm tổn hại throughput phục vụ. Trong khi đó, các nghiên cứu gần đây cho thấy rằng sparse attention miễn huấn luyện sẽ làm tổn hại khả năng bối cảnh dài của mô hình (Xiao et al., 2024; Ge et al., 2024a; Han et al., 2024). Điều này đã trở thành lý do chính tại sao chúng có việc áp dụng hạn chế trong các hệ thống phục vụ ngành công nghiệp và suy luận mã nguồn mở cho đến nay (Kwon et al., 2023; Zheng et al., 2023).

Những hiểu biết mới này dẫn đến Head-Heterogenous Strided Transformer (HHST), một phương pháp sparse attention mới theo các nguyên tắc thiết kế chính (§4.1), mà chúng tôi thấy quan trọng để đạt được lợi ích hiệu quả trong thực tế trong khi duy trì độ chính xác mạnh trên các tác vụ downstream:

(1) HHST được thiết kế với phần cứng và hệ thống phần mềm trong tâm trí. Nó áp dụng một chiến lược sharding thân thiện với phần cứng mới trên các attention heads, trong đó mỗi head tập trung vào một tập hợp token riêng biệt theo mẫu strided, trong khi tập thể bao phủ toàn bộ bối cảnh (Hình 1; §4.2).

(2) Để đạt được hiệu suất mạnh trên các tác vụ bối cảnh dài đầy thách thức, việc bao gồm truy cập trực tiếp đến tất cả các token, ít nhất ở các lớp nhất định là rất quan trọng. HHST đạt được điều này với một kiến trúc hybrid kết hợp sparse và dense attention qua các lớp, và cân bằng hiệu quả và hiệu suất (§4.2).

S2-ATTENTION có thể áp dụng trong cả huấn luyện và suy luận, giảm đáng kể rào cản khám phá các kiến trúc sparse attention mới, mà chúng tôi khám phá trong §4 và §5. Chúng tôi pretrain một bộ mô hình ở quy mô 1.3B, 7B với sparse attention khác nhau, và so sánh chúng với baseline dense attention. Kết quả của chúng tôi cho thấy HHST-7B của chúng tôi phù hợp với hiệu suất của dense attention trong khi đạt được tăng tốc huấn luyện 2.5× và tăng tốc suy luận 4.5×. Hơn nữa, chúng tôi mở rộng các mô hình 1.3B đến độ dài bối cảnh 32K, và các mô hình 7B đến 128K. Chúng tôi cho thấy HHST của chúng tôi có thể đạt được truy xuất Needle in a Haystack hoàn hảo (Kamradt, 2023). So với FlashAttention-2 (Dao, 2023), HHST có thể đạt được tăng tốc attention 8.8× và 15.9× cho quy mô 1.3B, 7B, và giảm thời gian huấn luyện thời gian thực 2.8×, 2.5×.

S2-ATTENTION tương thích với các framework LLM thường được sử dụng bao gồm PyTorch, Megatron, HuggingFace, và vLLM. Với các API thân thiện với người dùng, việc nhập và tùy chỉnh S2-ATTENTION không mất hơn vài dòng mã như được hiển thị trong Phụ lục B.

2 CÁC CÔNG TRÌNH LIÊN QUAN

Chúng tôi trình bày phân tích và quan sát của chúng tôi về các nỗ lực sparse attention hiện có trong cả huấn luyện và suy luận.

<S> Q : Có 2 llamas và 
3 vicunas . Có bao nhiêu động vật
<S> GPT4 là LLM tốt nhất từ OpenAI Block 1
Block 2
Block 3 Đã sử dụng
Đã loại bỏ
Chưa sử dụng

Hình 3: Minh họa tại sao các phương pháp KV eviction có thể gây ra nhiều phân mảnh hơn. Ở đây chúng tôi hiển thị 3 trang KV blocks chứa 2 yêu cầu. Mặc dù nhiều token đã bị loại bỏ, các slot được giải phóng khó có thể được sử dụng bởi các yêu cầu khác, dẫn đến tỷ lệ phân mảnh nội bộ cao hơn.

2.1 THIẾU KERNEL SPARSE ATTENTION HIỆU QUẢ

Đã có những nỗ lực giảm độ phức tạp tính toán của attention bằng cách chỉ chú ý đến một tập con token (Child et al., 2019; Katharopoulos et al., 2020; Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020). Tuy nhiên, các phương pháp này không thể mang lại tăng tốc thời gian thực trong huấn luyện do bỏ qua chi phí truy cập bộ nhớ thực tế (Dao et al., 2022). Dao et al. (2022) chia tính toán attention thành các tính toán blockwise nhỏ hơn để giảm I/O giữa SRAM và bộ nhớ băng thông cao (HBM). Việc triển khai phần cứng của họ FlashAttention (Dao et al., 2022; Dao, 2023) làm cho chúng trở thành framework tăng tốc attention được áp dụng rộng rãi nhất. Vẫn chưa rõ liệu chúng ta có thể triển khai các sparse self-attention khác nhau theo cách nhận biết phần cứng như vậy hay không, để tốc độ huấn luyện có thể được tăng cường hơn nữa so với FlashAttention.

2.2 VẤN ĐỀ VỚI CÁC PHƯƠNG PHÁP KV EVICTION PLUG-IN-AND-PLAY

Gần đây, các công trình KV eviction plug-in-and-play phát triển mạnh. Cụ thể hơn, các phương pháp này động loại bỏ các vector KV khi suy luận để giảm dấu chân bộ nhớ dựa trên các tiêu chí nhất định được thiết kế để bảo tồn khả năng mô hình.

Tuy nhiên, chúng tôi quan sát rằng các thiết kế như vậy khó tương thích với các hệ thống phục vụ hiện có, dựa vào PagedAttention và continuous batching để quản lý bộ nhớ hiệu quả. Như được hiển thị trong Hình 3, trong quá trình KV eviction, các token tương ứng được giải phóng khỏi bộ nhớ vật lý. Tuy nhiên, vì việc loại bỏ theo token không được đảm bảo là liên tục, các slot bộ nhớ được giải phóng không thể được phân bổ hiệu quả cho các yêu cầu khác, được gọi là phân mảnh nội bộ. Trong ví dụ này, phân mảnh nội bộ tăng 37.5%, điều này làm tổn hại throughput.

Trong khi đó, việc loại bỏ động cũng tạo ra overhead trong lập lịch. Ví dụ, nếu các head khác nhau có chính sách/tỷ lệ loại bỏ khác nhau, các head nhanh hơn sẽ phải chờ những head chậm hơn, đây là một tình huống cân bằng tải cổ điển. Vấn đề nghiêm trọng hơn khi phục vụ các mô hình lớn hơn, nơi các tính toán được phân phối qua các thiết bị và nút với tensor parallel và pipeline parallel. Những nhược điểm như vậy tiếp tục ngăn chặn các thuật toán này được tích hợp vào các cụm phục vụ thế giới thực với hàng trăm nút.

2.3 SUY GIẢM HIỆU SUẤT

Các nghiên cứu hiện có chỉ ra cả các phương pháp sparse attention huấn luyện và miễn huấn luyện đều có suy giảm hiệu suất so với các đối tác dense của chúng, đặc biệt trong các tác vụ bối cảnh dài. Hơn nữa, chúng tôi cũng quan sát rằng một số phương pháp miễn huấn luyện (Jiang et al., 2024; Tang et al., 2024) cần các siêu tham số cụ thể cho benchmark để duy trì chất lượng mô hình. Khi được áp dụng cho các yêu cầu chưa thấy, cùng một phương pháp có thể hiển thị hành vi không thể dự đoán. Tuy nhiên, trong việc triển khai thế giới thực, các truy vấn người dùng thường có phân phối đuôi dài. Do đó, không khả thi để xác định trước các siêu tham số cho các truy vấn người dùng chưa thấy, điều này làm cho việc triển khai các phương pháp như vậy rủi ro.

Chúng tôi thảo luận về xử lý các quan sát này trong các phần dưới đây.

3 S2-ATTENTION: HIỆU QUẢ VÀ TÙY CHỈNH

Phần này trình bày S2-ATTENTION. Chúng tôi đầu tiên xem xét ngắn gọn các cơ bản về bộ nhớ GPU và hierarchy thực thi, và sau đó giới thiệu kỹ thuật Merge-Q của chúng tôi, cải thiện đáng kể hiệu quả của kernel trong khi cho phép tùy chỉnh tinh vi hơn của sparse attention.

3.1 KIẾN THỨC CƠ BẢN

Các thread GPU có quyền truy cập vào một hierarchy của các loại bộ nhớ khác nhau. Bộ nhớ băng thông cao toàn cầu (HBM) là chậm nhất nhưng lớn nhất (khoảng >100× về độ trễ và ∼6K× về kích thước). Bộ nhớ chia sẻ (SRAM) có vật lý trên chip, do đó có băng thông lớn hơn và độ trễ thấp hơn so với HBM. Tối ưu hóa tính toán của SRAM và giảm thiểu I/O giữa HBM và SRAM là quan trọng để cải thiện hiệu quả của attention (Dao et al., 2022).

Các triển khai attention được tối ưu hóa kém có thể dẫn đến I/O thường xuyên đến HBM và làm tổn hại đáng kể hiệu quả. CUDA tổ chức các thread thành các thread blocks, được chia thêm thành các warps, nhóm 32 thread. Các thread trong một block chia sẻ dữ liệu thông qua SRAM. Điều mong muốn là các thread khác nhau trong cùng một warp đi theo cùng một đường thực thi vì nếu không hiệu quả sẽ bị tổn hại do phân kỳ warp. Bên cạnh đó, kích thước thread block phải đủ lớn để đạt được sử dụng tốt và cân bằng tải. Một tile là một phần của các tensor Q, K, V được gán cho một thread block để xử lý. Để rõ ràng, chúng tôi lấy kích thước tile làm kích thước block. FlashAttention cải thiện hiệu quả bằng cách giảm thiểu I/O HBM, tiling các tensor Q, K, V thành các chunk phù hợp với SRAM để tính toán hiệu quả (Dao et al., 2022), một nguyên tắc mà công trình này tuân theo.

3.2 S2-ATTENTION

Khởi động (Hình 4 trái) Chúng tôi đầu tiên xem xét một triển khai blocksparse đơn giản sử dụng thuật toán FlashAttention. Một chuỗi N token được phân đoạn thành B = ⌈N/S⌉ shard, mỗi shard chứa S token liên tiếp. Chúng tôi sử dụng Q[i] để biểu thị các vector query cho shard query thứ i, và tương tự K[i] các vector key cho shard key thứ i. Theo Dao et al. (2022), đối với mỗi vector query q, chúng tôi lặp qua các tile K trong SRAM để tính toán softmax(qK⊤). Không giống như dense attention sử dụng toàn bộ tensor K, chúng tôi chỉ xem xét một tập con key được chỉ định bởi một mask sparse attention M, có thể được lưu trữ trong định dạng Compressed Sparse Row (CSR) để hiệu quả bộ nhớ.¹

Để hiểu rõ hơn hiệu quả của triển khai như vậy, chúng ta có thể tính toán số lần tải cần thiết cho mỗi key/value shard. Như được hiển thị trong Hình 4 (trái), các key/value shard đầu tiên, KV1, được chú ý bởi tất cả các query shard, q1−q8. Do đó, KV1 được tải 8 lần từ HBM đến SRAM. Nếu chúng ta tăng gấp đôi kích thước shard, số query shard chú ý đến KV1 sẽ giảm một nửa xuống 4. Trong trường hợp này, KV1 chỉ cần 4 lần tải hiệu quả hơn. Tuy nhiên, hiệu quả I/O đi kèm với chi phí granularity của mask sparse của chúng ta, vì bây giờ chúng ta phải mask-or-keep 2S token thay vì S. Sau đó chúng tôi thảo luận cách đạt được cả hiệu quả I/O và granularity mask nhỏ với Merge-Q.

Merge-Q Ở mức cao, ý tưởng cốt lõi là hợp nhất các query shard chú ý đến cùng các KV block thành một tile duy nhất để chúng ta không cần tải riêng cùng các KV block. Theo cách này, ngay cả khi granularity mask trở nên nhỏ hơn, chúng ta vẫn có thể duy trì hiệu quả I/O. Hình 4: phải hiển thị một trường hợp đơn giản hơn, nơi chúng ta hợp nhất hai query shard lân cận. So với baseline FlashAttention-2, triển khai này chỉ cần tải KV1 4 lần thay vì 8 lần với cùng granularity mask. Merge-Q giúp S2-ATTENTION hỗ trợ kích thước shard nhỏ đến 16, cho phép một phạm vi rộng hơn của các mẫu sparse attention. Ý tưởng tương tự cũng có thể được áp dụng để hợp nhất các KV block để tăng cường hiệu quả hơn nữa. Chúng tôi để lại thảo luận triển khai chi tiết hơn trong mã được phát hành và Phụ lục D.

¹https://docs.nvidia.com/nvpl/_static/sparse/storage_format/sparse_matrix.html

Với S2-ATTENTION, cộng đồng có thể tùy chỉnh các mẫu sparse attention tinh vi với tăng tốc thời gian thực. Tuy nhiên, vẫn chưa rõ loại sparse attention nào có thể đạt được tăng tốc mà không làm tổn hại chất lượng. Chúng tôi nhằm mục đích trả lời câu hỏi này trong phần sau.

4 S2-ATTENTION: HIỂU BIẾT, CÔNG THỨC HÓA, VÀ SPARSITY COOKBOOK

Trong phần này, chúng tôi đầu tiên thảo luận loại mẫu sparse attention nào cho phép triển khai kernel hiệu quả trong thực tế (§4.1). Dựa trên những hiểu biết này, chúng tôi giới thiệu Head-Heterogenous Strided Transformer (HHST), một kiến trúc sparse attention mới (§4.2).

4.1 KV-EFFICIENT SPARSITY

KV cache là một nút thắt cổ chai bộ nhớ chính cho các LM chỉ decoder khi suy luận. Nhiều sparse attention hiện có xác định token nào cần chú ý dựa trên khoảng cách tương đối (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020). Tuy nhiên, các phương pháp này không hiệu quả về bộ nhớ GPU trong quá trình giải mã, khiến việc chuyển đổi tiết kiệm FLOP của chúng thành lợi ích hiệu quả thế giới thực trở nên khó khăn. Hình 5(a) cung cấp một ví dụ minh họa. Vấn đề chính là, đối với sparse attention như vậy, KV không được sử dụng trong các bước giải mã trước đó có thể được yêu cầu trong các bước sau đó, làm cho quản lý bộ nhớ thách thức hơn. Mặc dù tiết kiệm bộ nhớ gần 50% trên giấy, nó thực sự yêu cầu lưu trữ toàn bộ KV cache trong thực tế, dẫn đến không tiết kiệm bộ nhớ.

Ngược lại, Hình 5(b) minh họa một sparse attention có thể đạt được tiết kiệm bộ nhớ trong thực tế. Chìa khóa là KV cache được lưu trữ được tái sử dụng qua một số bước giải mã nhưng không còn cần thiết trong các bước tương lai, và do đó có thể được loại bỏ, giải phóng bộ nhớ GPU.

Việc so sánh giữa hai phương pháp này dẫn đến quy tắc ngón tay cái sau để thiết kế KV-efficient sparse attention. Đối với ∀j≥i, l≥1,

(ki,vi) được chú ý bởi qj+l ⟹ (ki,vi) cũng phải được chú ý bởi qj. (1)

Nếu không, ki và vi cần được lưu trữ ở bước j cho các thế hệ tương lai, ngay cả khi nó không được sử dụng ở bước j. Trực quan, trong ma trận mẫu attention, chúng ta sẽ thấy các "đường thẳng đứng" liên tục như được hiển thị trong Hình 5(b). Điều này có nghĩa là các mẫu sparse nên dựa trên vị trí tuyệt đối thay vì tương đối, ngoại trừ bối cảnh cục bộ liên tiếp (ví dụ, hình bên trái trong Hình 5(b)).

4.2 HEAD-HETEROGENOUS STRIDED TRANSFORMER

Phần này giới thiệu Head-Heterogenous Strided Transformer (HHST), một sparse attention hiệu quả mới được truyền cảm hứng từ những hiểu biết chúng tôi học được ở trên. Cốt lõi của thiết kế của nó là hai lựa chọn thiết kế được giới thiệu dưới đây.

Chia sẻ Bối cảnh Không đồng nhất qua các Attention Head Để đạt được cân bằng tải qua các attention head và tăng cường song song hóa, mỗi head nên chú ý đến một số lượng token bằng nhau. Ngoài ra, HHST đảm bảo rằng các head khác nhau chú ý đến các shard khác nhau của bối cảnh trong khi tập thể bao phủ toàn bộ bối cảnh. Thiết kế này đảm bảo rằng HHST luôn có quyền truy cập trực tiếp vào toàn bộ bối cảnh ở mỗi lớp, mà không ảnh hưởng đến song song hóa. Hình 1 cung cấp một sơ đồ minh họa.

Chính thức hơn, đối với bối cảnh với B shard, chúng tôi lấy Bl block gần đây nhất làm block cục bộ và đặt phần còn lại làm block từ xa. Đối với attention head với index h, mask attention block B×B Mh của nó là:

Mhi,j = (1, i−j < Bl, Cục bộ
1, j−oh∈sZ≥0∧i−j∈[Bl, B) Stride
0 khác

s là kích thước stride, và x∈mZ≥0 có nghĩa là x là 0 hoặc một bội số dương của m. Tương tự như một sliding window, các token vượt quá B shard không được chú ý đến.

Tính linh hoạt của kernel S2-ATTENTION của chúng tôi cho phép triển khai hiệu quả của chiến lược này. Như được hiển thị trong các thí nghiệm của chúng tôi, thiết kế này cho phép mô hình đạt được hiệu suất bối cảnh dài mạnh trong khi tối đa hóa lợi ích hiệu quả.

Kiến trúc Hybrid Như các nghiên cứu trước đây cho thấy (Huang et al., 2022; Lieber et al.), một số lớp attention dày đặc hơn đáng kể so với những lớp khác, với trọng số attention được phân phối gần như đồng đều qua tất cả các vị trí. Do đó, việc giữ lại dense attention trong các lớp này đặc biệt có lợi. Điều này thúc đẩy chúng tôi khám phá một kiến trúc hybrid kết hợp sparse attention hiệu quả của chúng tôi trong hầu hết các lớp với dense attention trong những lớp khác. Chúng tôi thấy thực nghiệm rằng chiến lược sparse attention của chúng tôi rất hiệu quả, chỉ yêu cầu 1/6 số lớp attention là dense để đạt được hiệu suất truy xuất mạnh với bối cảnh dài 128K. Khám phá thêm được trình bày trong các thí nghiệm của chúng tôi.

Thảo luận Quan trọng là chỉ ra rằng tất cả các chiến lược loại bỏ nhắm mục tiêu suy luận (Zhang et al., 2023; Liu et al., 2023; Ge et al., 2024b) đều hiệu quả KV-cache, vì KV bị loại bỏ sẽ không bao giờ được sử dụng bởi các truy vấn tương lai. Tuy nhiên, các chiến lược này giới thiệu các mẫu sparsity phụ thuộc vào mẫu, làm cho việc tính toán thời điểm loại bỏ trong quá trình giải mã trở nên đắt đỏ. Ngược lại, phương pháp của chúng tôi sử dụng một mẫu sparsity cố định qua tất cả các mẫu, loại bỏ overhead của việc quyết định token nào cần loại bỏ. Bên cạnh đó, các phương pháp KV eviction là post-hoc và thường hoạt động kém hơn nhiều so với đối tác dense ban đầu (Ge et al., 2024a). HHST của chúng tôi, như chúng ta sẽ sớm thấy trong các thí nghiệm, thích ứng với sparse attention trong quá trình huấn luyện (pre-training hoặc post-training) hoạt động tương đương với các baseline dense trong khi giảm overhead huấn luyện.

5 THÍ NGHIỆM

Để đánh giá HHST, chúng tôi đầu tiên nghiên cứu chất lượng pre-training trong §5.1 và §5.2. Sau đó chúng tôi benchmark hiệu quả kernel và độ trễ phục vụ end-to-end trong §5.4 và § ??. Cuối cùng, chúng tôi tiến hành một nghiên cứu ablation về các lựa chọn thiết kế.

5.1 BENCHMARKING CHẤT LƯỢNG HUẤN LUYỆN MÔ HÌNH

Cài đặt Chúng tôi đầu tiên huấn luyện một loạt mô hình 1.3B với kiến trúc Llama 2, với 24 lớp, kích thước ẩn 2048 với 16 head, với độ dài chuỗi tối đa là 8192. Chúng tôi sử dụng corpus pre-training FineWeb-Edu-350B mã nguồn mở Penedo et al. (2024). Một tokenizer OpenAI Tiktoken với kích thước từ vựng 100K được sử dụng để xử lý văn bản thô. Tất cả các biến thể mô hình sử dụng kích thước batch 4M token cho tất cả độ dài chuỗi và huấn luyện tổng cộng 300 tỷ token. Đối với siêu tham số, chúng tôi sử dụng µP Yang et al. (2022) với hình dạng cơ sở 256. Tỷ lệ học µP 0.02 được sử dụng với suy giảm tuyến tính và 0.5% tổng token huấn luyện để khởi động. Tất cả các mô hình được đánh giá sau khi huấn luyện trên tổng cộng 300B token trong một epoch.

Tác vụ Downstream Chúng tôi sử dụng một mô hình với dense attention làm baseline của chúng tôi, được ký hiệu là "Dense". Để nghiên cứu cấu trúc hybrid của chúng tôi với sharding không đồng nhất và completeness union, chúng tôi kiểm soát FLOPs để tương đương gần đúng. Tổng số token được chú ý là khoảng 576 token, hoặc 9 shard của 64 token. Chúng tôi sử dụng điều này để cấu hình sliding window attention (SWA), làm tập kiểm soát. Chúng tôi thêm các thay đổi khác nhau vào SWA để xem chúng ảnh hưởng đến chất lượng huấn luyện như thế nào. Các tập xử lý được nhóm thành 1) Đồng nhất (Các head khác nhau chú ý đến cùng các shard); 2) Không đồng nhất & Không hoàn chỉnh (Các head khác nhau chú ý đến các shard khác nhau nhưng không bao phủ toàn bộ bối cảnh), và 3) Không đồng nhất & Hoàn chỉnh (Các head khác nhau chú ý đến cùng các shard và tập thể bao phủ toàn bộ bối cảnh).

Từ Bảng 1, chúng ta có thể quan sát các kiến trúc hybrid cho thấy kết quả đầy hứa hẹn. Như chúng ta có thể thấy từ S2-L1V15 + Dense (HHST) ở hàng cuối, sharding không đồng nhất với bối cảnh hoàn chỉnh và hai lớp dense cho kết quả tốt nhất nhất quán qua các tác vụ, với khoảng cách nhỏ từ baseline attention mặc định trong khi chỉ sử dụng 18% FLOPs. Đáng chú ý, trong tác vụ Passkey Retrieval, HHST có thể đạt được hiệu suất tốt hơn nhiều so với mô hình dense. Quan sát này hoạt động như một xác thực ban đầu của khả năng hiểu bối cảnh của thiết kế HHST. Chúng tôi sẽ xác thực thêm trong phần huấn luyện liên tục bối cảnh dài.

Chúng tôi cũng thấy việc thêm hai lớp dense thường dẫn đến hiệu suất cao hơn đáng kể. Trong nhóm Đồng nhất, chúng ta có thể quan sát việc thêm attention sink có thể tăng cường đáng kể chất lượng huấn luyện, so với chỉ sử dụng sliding window (SWA). Trong nhóm Không đồng nhất & Không hoàn chỉnh, kích thước stride dọc lớn hơn số attention head, làm cho bối cảnh không hoàn chỉnh sau union. Đối với nhóm Không đồng nhất & Hoàn chỉnh, chúng tôi điều chỉnh kích thước stride và cửa sổ cục bộ để nó vừa bao phủ toàn bộ bối cảnh trong khi có cùng FLOPs như những nhóm khác. Khi so sánh nhóm

Bảng 1: Đánh giá chất lượng Pre-Training. "SWA" đề cập đến sliding window attention. "L" đề cập đến số block cục bộ. "V" đề cập đến kích thước stride dọc. "+ Sink" đề cập đến chú ý đến attention sink. "+ Dense" đề cập đến làm cho hai lớp attention đầu tiên dense.

Model | Passkey | WinoGrande | PIQA | RACE | Wikitext103(ppl)
Dense (Upper Bound) | 0.865 | 0.592 | 0.733 | 0.403 | 15.884

Đồng nhất (18% FLOPs của Dense)
HHST-L9 (SWA) | 0.334 | 0.547 | 0.705 | 0.363 | 21.997
HHST-L9 + Dense | 0.620 | 0.575 | 0.714 | 0.373 | 20.450
HHST-L9 + Sink | 0.560 | 0.566 | 0.721 | 0.380 | 21.037
HHST-L9 + Sink + Dense | 0.771 | 0.577 | 0.728 | 0.388 | 18.503
HHST-L1V15 | 0.542 | 0.541 | 0.716 | 0.352 | 21.035
HHST-L1V15 + Dense | 0.741 | 0.568 | 0.713 | 0.349 | 20.579

Không đồng nhất & Không hoàn chỉnh (18% FLOPs của Dense)
HHST-L2V18 | 0.630 | 0.565 | 0.728 | 0.357 | 20.502
HHST-L2V18 + Dense | 0.823 | 0.587 | 0.732 | 0.379 | 18.726
HHST-L4V25 | 0.612 | 0.542 | 0.720 | 0.352 | 20.875
HHST-L4V25 + Dense | 0.795 | 0.569 | 0.724 | 0.386 | 19.285

Không đồng nhất & Hoàn chỉnh (18% FLOPs của Dense)
HHST-L1V15 | 0.782 | 0.571 | 0.724 | 0.361 | 19.551
HHST-L1V15 + Dense (HHST) | 0.941 | 0.587 | 0.725 | 0.397 | 17.183

Không hoàn chỉnh với nhóm Hoàn chỉnh, chúng ta có thể thấy lợi ích của việc làm cho union của bối cảnh hoàn chỉnh bằng cách giới hạn kích thước stride dọc.

5.2 HUẤN LUYỆN LIÊN TỤC BỐI CẢNH DÀI

Chúng tôi tiếp tục kiểm tra cách thích ứng sparse attention với bối cảnh dài hơn. Chúng tôi bắt đầu từ một mô hình được pre-train dense hiện có và mở rộng độ dài bối cảnh của nó bằng cách tiếp tục huấn luyện nó trên độ dài bối cảnh dài hơn với kiến trúc sparse HHST. Cụ thể, chúng tôi chọn Llama-2-7B và tiếp tục huấn luyện nó trên độ dài bối cảnh 128K. Chúng tôi thay đổi cơ sở RoPE thành 5M. Cả hai mô hình đều được huấn luyện liên tục với 10B token theo công thức trong Fu et al. (2024). Chúng tôi đánh giá các mô hình trên tác vụ Needle In A Haystack Kamradt (2023).

Để điều tra cách đạt được hiệu suất bối cảnh dài mạnh, chúng tôi thay đổi số lớp dense trong HHST. Chúng tôi đặt số lớp dense lần lượt là 2, 4 và 8. Chúng tôi cố định số block cục bộ là 31 và kích thước stride dọc là 32. Như được hiển thị trong Hình 6, đối với bối cảnh 128K, mô hình có thể truy xuất toàn bộ bối cảnh với 8 lớp dense nhưng không làm được với chỉ 2 và 4 lớp dense. Kết quả xác thực khả năng bối cảnh dài của thiết kế HHST.

5.3 TĂNG TỐC HUẤN LUYỆN

5.3.1 BENCHMARK HOẠT ĐỘNG ATTENTION

Cài đặt Benchmark Chúng tôi đo thời gian chạy attention của HHST với kernel S2-ATTENTION của chúng tôi, và FlashAttention-2 trên GPU A100 80GB cho các cài đặt độ dài bối cảnh, số head, và dimension head khác nhau.

Trong Hình 7 và Hình 2a Chúng tôi benchmark tăng tốc được mang lại bởi HHST trong kích thước mô hình 1.3B, 7B, 70B qua các độ dài chuỗi khác nhau để cho thấy khả năng mở rộng của hệ thống chúng tôi. Đối với tất cả kích thước mô hình, HHST có thể đạt được nhiều lần tăng tốc so với FlashAttention-2. Đối với các mô hình 70B với 64 head, HHST có thể cho tăng tốc end-to-end 25.3×. Ví dụ, trong các mô hình 1.3B với stride dọc 16, HHST có thể đạt được tăng tốc 8.8×. Khi độ dài chuỗi tối đa tăng dài hơn, tăng tốc dần tiến gần đến lợi ích giảm FLOPs theo lý thuyết. Sự tăng cường tổng thể bị cản trở một chút do kernel backward ít được tối ưu hóa của chúng tôi, để lại chỗ cho cải thiện thêm.

5.4 TĂNG TỐC HUẤN LUYỆN VÀ SUY LUẬN

Chúng tôi đánh giá tăng tốc huấn luyện end to end của các mô hình 1.3B và 7B bằng cách đo throughput token của cả hai mô hình. Tất cả các mô hình được huấn luyện trên 256 A100, với kích thước batch 8M token và activation checkpointing. Đối với 1.3B, HHST có thể đạt được throughput token 1.2×, 1.8×, 2.3×, và 2.8× trên bối cảnh 8K đến 128K so với FlashAttention-2. Đối với các mô hình 7B, HHST có thể đạt được cải thiện throughput token 1.1×, 1.2×, 1.5×, và 2.5×. Để chứng minh cải thiện hiệu quả suy luận của HHST, chúng tôi đo độ trễ end-to-end qua các cài đặt độ dài bối cảnh khác nhau. Để làm cho so sánh thực tế, các thí nghiệm của chúng tôi được thực hiện trên vLLM (Kwon et al., 2023). Chúng tôi chọn backend FlashAttention-2 trong vLLM làm baseline để so sánh công bằng, vì kernel suy luận của S2-ATTENTION cũng dựa trên vLLM. Cả hai phương pháp đều được triển khai trên một nút duy nhất với 8 GPU A100 80, với kích thước tensor parallel bằng 4. Chúng tôi đặt độ dài đầu ra là 128 và thay đổi độ dài đầu vào giữa 16K đến 256K. Như được hiển thị trong Hình 8b, HHST có thể đạt được tăng tốc 1.1×, 1.2×, 2.9×, 4.5× trên bối cảnh 8K, 16K, 128K, 256K.

6 KẾT LUẬN

Chúng tôi đã trình bày S2-ATTENTION, một thư viện kernel Triton được tối ưu hóa cung cấp nhiều triển khai sparse attention có thể tùy chỉnh cho cả huấn luyện và suy luận. Những hiểu biết từ S2-ATTENTION dẫn đến một số nguyên tắc về các lựa chọn thiết kế của các phương pháp sparse attention để làm cho chúng hiệu quả trong thực tế. Chúng truyền cảm hứng cho một kiến trúc sparse attention hybrid mới đáp ứng một số desiderata mà chúng tôi thấy quan trọng để đạt được cả lợi ích hiệu quả thực tế và độ chính xác mạnh trên các tác vụ downstream, được gọi là Head-Heterogenous Strided Transformer (HHST). Chúng tôi sẽ mở mã nguồn thư viện kernel của chúng tôi và làm cho nó trở thành một thay thế plug-in-and-play cho module FlashAttention-2 trong các framework huấn luyện phổ biến như Megatron và Pytorch. Chúng tôi cũng tích hợp S2-ATTENTION vào backend vLLM để phục vụ ngay lập tức. Cả kernel huấn luyện và suy luận đều cho phép người dùng tự do tùy chỉnh mẫu sparsity của họ, tạo điều kiện cho toàn bộ cộng đồng nghiên cứu chủ đề trong tương lai.

TÀI LIỆU THAM KHẢO

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10.48550/arXiv.2307.08691.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=TaAqeo7lUh.

Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, and Hao Peng. A little goes a long way: Efficient long context training and inference with partial contexts. arXiv preprint arXiv:2410.01485, 2024a.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024b. URL https://openreview.net/pdf?id=88nT0j5jAn.

Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 3991–4008, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/2024.naacl-long.222/.

Xin Huang, Ashish Khetan, Rene Bidart, and Zohar Karnin. Pyramid-bert: Reducing complexity via successive core-set based token selection. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 8798–8817. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.602. URL https://doi.org/10.18653/v1/2022.acl-long.602.

Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. CoRR, abs/2407.02490, 2024. doi: 10.48550/ARXIV.2407.02490. URL https://doi.org/10.48550/arXiv.2407.02490.

Greg Kamradt. Needle in a haystack-pressure testing llms. Github Repository, pp. 28, 2023.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020. URL http://proceedings.mlr.press/v119/katharopoulos20a.html.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. CoRR, abs/2309.06180, 2023. doi: 10.48550/arXiv.2309.06180. URL https://doi.org/10.48550/arXiv.2309.06180.

Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language model, 2024. URL https://arxiv.org/abs/2403.19887.

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. CoRR, abs/2305.17118, 2023. doi: 10.48550/arXiv.2305.17118. URL https://doi.org/10.48550/arXiv.2305.17118.

OpenAI. Gpt-4 technical report, 2023.

Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. CoRR, abs/2406.17557, 2024. doi: 10.48550/ARXIV.2406.17557. URL https://doi.org/10.48550/arXiv.2406.17557.

Szymon Rucinski. Efficient language adaptive pre-training: Extending state-of-the-art large language models for polish. CoRR, abs/2402.09759, 2024. doi: 10.48550/ARXIV.2402.09759. URL https://doi.org/10.48550/arXiv.2402.09759.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.org/abs/2406.10774.

Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6):109:1–109:28, 2023. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads. CoRR, abs/2410.10819, 2024. doi: 10.48550/ARXIV.2410.10819. URL https://doi.org/10.48550/arXiv.2410.10819.

Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pachocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. In NeurIPS 2021, March 2022. URL https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/.

Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for transformer-based generative models. In Marcos K. Aguilera and Hakim Weatherspoon (eds.), 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, pp. 521–538. USENIX Association, 2022. URL https://www.usenix.org/conference/osdi22/presentation/yu.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. CoRR, abs/2306.14048, 2023. doi: 10.48550/arXiv.2306.14048. URL https://doi.org/10.48550/arXiv.2306.14048.

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104, 2023.

A PHỤ LỤC

Bạn có thể bao gồm các phần bổ sung khác ở đây.
