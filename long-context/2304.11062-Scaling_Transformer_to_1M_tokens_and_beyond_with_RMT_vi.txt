Peng, B.; Alcaide, E.; Anthony, Q.; Albalak, A.; Arcadinho,
S.; Cao, H.; Cheng, X.; Chung, M.; Grella, M.; GV , K. K.;
et al. 2023. RWKV: Reinventing RNNs for the Transformer
Era. arXiv preprint arXiv:2305.13048 .
Poli, M.; Massaroli, S.; Nguyen, E.; Fu, D. Y .; Dao, T.; Bac-
cus, S.; Bengio, Y .; Ermon, S.; and Re, C. 2023. Hyena Hier-
archy: Towards Larger Convolutional Language Models. In
Krause, A.; Brunskill, E.; Cho, K.; Engelhardt, B.; Sabato,
S.; and Scarlett, J., eds., Proceedings of the 40th Interna-
tional Conference on Machine Learning , volume 202 of
Proceedings of Machine Learning Research , 28043–28078.
PMLR.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language Models are Unsupervised
Multitask Learners.
Rae, J. W.; Hunt, J. J.; Harley, T.; Danihelka, I.; Senior, A.;
Wayne, G.; Graves, A.; and Lillicrap, T. P. 2016. Scaling
Memory-Augmented Neural Networks with Sparse Reads
and Writes. arXiv:1610.09027.
Rae, J. W.; Potapenko, A.; Jayakumar, S. M.; Hillier, C.; and
Lillicrap, T. P. 2020. Compressive Transformers for Long-
Range Sequence Modelling. In International Conference on
Learning Representations .
Stephen, C. 1956. Kleene. Representation of events in nerve
nets and finite automata. Automata studies .
Sukhbaatar, S.; Szlam, A.; Weston, J.; and Fergus, R. 2015.
End-To-End Memory Networks. arXiv:1503.08895.
Sun, Y .; Dong, L.; Huang, S.; Ma, S.; Xia, Y .; Xue, J.;
Wang, J.; and Wei, F. 2023. Retentive network: A succes-
sor to transformer for large language models. arXiv preprint
arXiv:2307.08621 .
Werbos, P. J. 1990. Backpropagation through time: what it
does and how to do it. Proceedings of the IEEE , 78(10):
1550–1560.
Weston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2016.
Towards AI-Complete Question Answering: A Set of Pre-
requisite Toy Tasks. In Bengio, Y .; and LeCun, Y ., eds.,
4th International Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Confer-
ence Track Proceedings .
Weston, J.; Chopra, S.; and Bordes, A. 2015. Memory Net-
works. In Bengio, Y .; and LeCun, Y ., eds., 3rd International
Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings.
Wolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.
2020. Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 conference on empirical
methods in natural language processing: system demonstra-
tions , 38–45.
Wu, Q.; Lan, Z.; Qian, K.; Gu, J.; Geramifard, A.; and Yu,
Z. 2022a. Memformer: A Memory-Augmented Transformer
for Sequence Modeling. In Findings of the Association for
Computational Linguistics: AACL-IJCNLP 2022 , 308–318.
Online only: Association for Computational Linguistics.

--- TRANG 10 ---
Wu, Y .; Rabe, M. N.; Hutchins, D.; and Szegedy, C. 2022b.
Memorizing Transformers. In International Conference on
Learning Representations .
Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Al-
berti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang,
L.; and Ahmed, A. 2020. Big Bird: Transformers for Longer
Sequences. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Bal-
can, M.; and Lin, H., eds., Advances in Neural Information
Processing Systems , volume 33, 17283–17297. Curran As-
sociates, Inc.
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mihaylov,
T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.; Koura, P. S.;
Sridhar, A.; Wang, T.; and Zettlemoyer, L. 2022. OPT:
Open Pre-trained Transformer Language Models. ArXiv ,
abs/2205.01068.

--- TRANG 11 ---
A Phụ lục: Chi tiết huấn luyện

Hiệu quả của tính hồi quy
Chúng tôi so sánh hiệu quả tài nguyên của RMT và mô hình attention đầy đủ bằng cách đo bộ nhớ GPU và thời gian lặp trên Hình 10. Các thử nghiệm được thực hiện trên một GPU Nvidia A100 80GB duy nhất. Chúng tôi sử dụng triển khai GPT2 tham chiếu từ HuggingFace mà không có FlashAttention và bất kỳ kỹ thuật tối ưu hóa nào khác, và chạy các mô hình ở chế độ FP32.

RMT không chỉ hiệu quả bộ nhớ hơn mà còn nhanh hơn transformer tiêu chuẩn, ngay cả khi xem xét overhead bổ sung từ backpropagation through time, có thể được tắt khi tài nguyên bị hạn chế. Transformer cơ sở không thể xử lý các chuỗi có độ dài 8000 với lỗi hết bộ nhớ.

Cũng đáng chú ý rằng một trong những lợi thế chính của RMT là khả năng huấn luyện trên các chuỗi ngắn hơn, ví dụ lên đến 8000 token và sau đó tận dụng khả năng tổng quát hóa của nó để xử lý các chuỗi lớn hơn nhiều. Trong quá trình đánh giá, yêu cầu tính toán cho RMT tăng trưởng tuyến tính, trong khi yêu cầu bộ nhớ vẫn không đổi. Điều này là do chỉ một phân đoạn được giữ trong bộ nhớ GPU tại một thời điểm trong quá trình suy luận.

Tạo tác vụ tổng hợp
Ở đây chúng tôi cung cấp mô tả ngắn gọn về các tác vụ ghi nhớ. Mã để tạo bộ dữ liệu và tái tạo tất cả các thí nghiệm có thể được tìm thấy trong kho GitHub.

Các bộ dữ liệu Memorization và Detect&Memorize sử dụng các câu hỏi và sự kiện hỗ trợ từ tập con qa1single-supporting-fact của bộ dữ liệu bAbI (Weston et al. 2016). Các sự kiện và câu hỏi được xây dựng bằng cách sử dụng mẫu sau:

Sự kiện: [người] [hành động] [địa điểm]
Câu hỏi: [người] ở đâu?
Trả lời: [địa điểm]

[địa điểm] được chọn từ 6 tùy chọn: 'bathroom', 'hallway', 'garden', 'office', 'bedroom', và 'kitchen'. Đối với mô hình BERT chỉ có encoder, tác vụ kết quả được công thức hóa như một vấn đề phân loại 6 lớp, mỗi lớp là tùy chọn trả lời riêng biệt.

Theo pipeline bAbI, chúng tôi tạo một cặp sự kiện và câu hỏi bằng cách chọn ngẫu nhiên những tùy chọn này và xây dựng một mẫu bộ dữ liệu như trong Hình 3. Trong tác vụ Memorize, sự kiện luôn ở đầu đầu vào. Detect và Memorize đặt sự kiện trong một phân đoạn ngẫu nhiên trong chuỗi. Tác vụ Reasoning được tạo theo cách tương tự sử dụng các sự kiện hỗ trợ và câu hỏi từ tập con qa4two-arg-relations bAbI; một ví dụ về những sự kiện như vậy được trình bày ở cuối phần Memorization Tasks.

Một hạn chế đáng chú ý của các tác vụ được đề xuất là nguồn văn bản gây nhiễu, đến từ một phân phối khác với các câu hỏi và sự kiện. Điều này làm cho việc phân biệt các câu hỏi trở nên tầm thường đối với mô hình. Tuy nhiên, điều này có thể dễ dàng thay đổi thành bất kỳ văn bản nào khác ngay cả từ các phân phối gần hơn. Chúng tôi tin rằng việc mở rộng bộ dữ liệu ghi nhớ với các câu hỏi phức tạp và kiểm soát vị trí sự kiện hỗ trợ sẽ giúp cải thiện cách xử lý các chuỗi dài. Kết hợp với lịch trình huấn luyện được đề xuất, các mô hình tương lai

[Hình ảnh chứa 3 biểu đồ:]
Kích thước đầu vào MB
0-10000-20000-30000-40000
1000 2000 4000 8000 GPT RMT(1000) RMT(500) Sử dụng bộ nhớ GPU

Kích thước đầu vào / lặp
0,0-0,2-0,4-0,6-0,8
1000 2000 4000 8000 GPT RMT(1000) RMT(500) Thời gian lặp huấn luyện

Kích thước đầu vào / lặp  
0,00-0,05-0,10-0,15-0,20-0,25
1000 2000 4000 8000 GPT RMT(1000) RMT(500) Thời gian lặp xác thực

Hình 10: RMT (kích thước phân đoạn 1000 và 500) nhanh hơn và tiêu tốn ít bộ nhớ đáng kể hơn việc mở rộng attention đầy đủ với kích thước đầu vào lớn hơn 4000 token. GPT-2 (110M) OOM trên GPU 80Gb với 8k token. Trong thí nghiệm này chúng tôi không giới hạn BPTT unroll theo pipeline bài báo. Triển khai RMT có thể được tối ưu hóa thêm về tốc độ và sử dụng bộ nhớ.

--- TRANG 12 ---
có thể vượt qua những hạn chế hiện tại của Transformer (Liu et al. 2023).

Tác động của curriculum
Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng học curriculum đóng vai trò thiết yếu trong việc huấn luyện RMT. Để xác nhận tầm quan trọng của curriculum với số lượng phân đoạn tăng dần, chúng tôi huấn luyện RMT có và không có curriculum trên tác vụ ghi nhớ.

[Biểu đồ showing:]
huấn luyện naivehọc curriculum 0-20-40-60-80-100 Độ chính xác ghi nhớ
Hiệu suất RMT 4 phân đoạn trung bình

1-2-3-4-5-6-7-8-9-10-11-12-13-14-15
Đánh giá trên, phân đoạn 20-40-60-80-100 Độ chính xác
Hiệu suất ghi nhớ
Được huấn luyện trên 4 phân đoạn
Với curriculum
Không có curriculum

Hình 11: Curriculum tăng cường khả năng của RMT để ghi nhớ các sự kiện và tổng quát hóa đến các độ dài chuỗi khác. Huấn luyện naive không sử dụng curriculum và huấn luyện trực tiếp trên số lượng phân đoạn tối đa (bốn trong trường hợp này).

Hình 11 cho thấy rằng trong trường hợp không có curriculum, nếu mô hình được huấn luyện trực tiếp trên số lượng phân đoạn tối đa, RMT không học được cách giải quyết tác vụ, cũng không ngoại suy trên độ dài chuỗi khác. Tuy nhiên, bằng cách sử dụng curriculum, một mô hình có khả năng hơn nhiều với khả năng tổng quát hóa mạnh có thể được đạt được.

Trong Hình 12 chúng tôi cũng cho thấy rằng quy trình học curriculum có thể được cải thiện bằng cách thêm các mẫu từ các bước curriculum trước đó, tức là ở bước khi chúng tôi huấn luyện trên N phân đoạn, chúng tôi cũng thêm các mẫu với ≤N phân đoạn.

Sử dụng các phương pháp hiệu quả tham số
Một lợi thế đáng chú ý của RMT là kiến trúc backbone vẫn không thay đổi. Điều này cho phép sử dụng các phương pháp hiệu quả tham số hiện có để chỉ sửa đổi một số lượng nhỏ tham số để kết hợp bộ nhớ. Hiệu suất của RMT kết hợp với LoRA (Hu et al. 2022) và Parallel Adapter (He et al. 2022) được mô tả trong Bảng 1. Việc thêm tính hồi quy dẫn đến cải thiện đáng kể về perplexity cho mô hình Pythia (Biderman et al. 2023) so với chỉ sử dụng các phương pháp hiệu quả tham số.

1 2 4 8 16 32
Đánh giá trên, phân đoạn a-0.95-1.00-1.05-1.10-1.15-1.20 bpb
Curriculum với độ dài cố định
Được huấn luyện trên
1 seg
2 seg  
3 seg
4 seg
5 seg
6 seg
gpt2

Hình 12: Đối với tác vụ mô hình hóa ngôn ngữ Arxiv (b) việc trộn tất cả số lượng phân đoạn trước đó trong quá trình học curriculum cải thiện tổng quát hóa so với (a) việc sử dụng số lượng phân đoạn cố định ở mỗi giai đoạn curriculum. Curriculum với trộn độ dài cải thiện hiệu suất trên số lượng phân đoạn nhỏ hơn và cho thấy ngoại suy đến các độ dài không được thấy trong quá trình huấn luyện. Ngoài ra, chúng tôi nhấn mạnh rằng RMT với kích thước phân đoạn 128 bắt đầu vượt trội hơn GPT-2 với đầu vào có kích thước gấp đôi (256).

--- TRANG 13 ---
Bảng 1: RMT có thể được kết hợp thành công với các phương pháp hiệu quả tham số (parallel adapter, LoRA). Kết quả cho mô hình hóa ngôn ngữ trên bộ dữ liệu Arxiv cho mô hình Pythia-70m.

MÔ HÌNH LOSS
ADAPTER DUY NHẤT 41.43
ADAPTER + RMT-1 SEG 10.31
ADAPTER + LORA + RMT-1 SEG 7.30
ADAPTER + LORA + RMT-2 SEG 6.97

RMT mang lại sự linh hoạt trong việc kết hợp các phương pháp huấn luyện tiết kiệm chi phí khác nhau, điều này tăng cường đáng kể khả năng ứng dụng thực tế của nó, đặc biệt khi tài nguyên tính toán bị hạn chế.
