# Mở rộng Cửa sổ Ngữ cảnh của LLM với 100 Mẫu

Yikai Zhang1,2,3Junlong Li1,3Pengfei Liu1,2,3∗
1Đại học Giao thông Thượng Hải2Phòng thí nghiệm Trí tuệ Nhân tạo Thượng Hải
3Phòng thí nghiệm Nghiên cứu AI Tạo sinh (GAIR)

## Tóm tắt

Các Mô hình Ngôn ngữ Lớn (LLM) được biết đến là có khả năng ngoại suy hạn chế vượt ra ngoài cửa sổ ngữ cảnh được huấn luyện trước của chúng, hạn chế việc ứng dụng của chúng trong các tác vụ hạ nguồn với đầu vào dài. Các nghiên cứu gần đây đã tìm cách mở rộng cửa sổ ngữ cảnh của LLM bằng cách sửa đổi mã hóa vị trí xoay (RoPE), một phương pháp mã hóa vị trí phổ biến được áp dụng bởi các LLM nổi tiếng như LLaMA, PaLM và GPT-NeoX. Tuy nhiên, các công trình trước đây như Nội suy Vị trí (PI) và YaRN đòi hỏi nhiều tài nguyên và thiếu các thí nghiệm so sánh để đánh giá khả năng áp dụng của chúng. Trong công trình này, chúng tôi xác định nhu cầu vốn có đối với entropy chú ý của LLM (tức là entropy thông tin của điểm số chú ý) để duy trì sự ổn định và giới thiệu một phần mở rộng mới cho RoPE kết hợp việc điều chỉnh tần số cơ sở của RoPE và chia tỷ lệ các logit chú ý để giúp LLM thích ứng hiệu quả với cửa sổ ngữ cảnh lớn hơn. Chúng tôi xác nhận tính ưu việt của phương pháp của chúng tôi trong cả hiệu suất tinh chỉnh và độ bền vững trên các kích thước cửa sổ ngữ cảnh khác nhau trên các tác vụ đòi hỏi ngữ cảnh khác nhau. Đáng chú ý, phương pháp của chúng tôi mở rộng cửa sổ ngữ cảnh của LLaMA-2-7B-Chat lên 16,384 chỉ với 100 mẫu và 6 bước huấn luyện, thể hiện hiệu quả phi thường. Cuối cùng, chúng tôi cũng khám phá cách thành phần dữ liệu và chương trình giảng dạy ảnh hưởng đến việc mở rộng cửa sổ ngữ cảnh cho các tác vụ hạ nguồn cụ thể, đề xuất tinh chỉnh LLM với các cuộc hội thoại dài như một điểm khởi đầu tốt. Chúng tôi phát hành mã và dữ liệu SFT của chúng tôi tại https://github.com/GAIR-NLP/Entropy-ABF.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) thường được huấn luyện trước với kích thước cửa sổ ngữ cảnh được xác định trước. Ví dụ, LLaMA 2 (Touvron et al., 2023b) được huấn luyện trước trên các chuỗi 4,096 token. Khi vượt quá cửa sổ ngữ cảnh được huấn luyện trước, hiệu suất của LLM có xu hướng xấu đi chủ yếu do khả năng ngoại suy chiều dài hạn chế của các phương pháp mã hóa vị trí của chúng (Kazemnejad et al., 2023). Cửa sổ ngữ cảnh hạn chế ảnh hưởng đến tính thực tiễn của LLM cho các tác vụ đòi hỏi ngữ cảnh ngày càng tăng như học few-shot (Brown et al., 2020), tóm tắt tài liệu dài (Huang et al., 2021) và hoàn thành mã cấp kho lưu trữ (Liu et al., 2023). Do đó, có nhu cầu cấp thiết cần mở rộng cửa sổ ngữ cảnh của LLM.

Để đáp ứng nhu cầu cấp bách này, các công trình gần đây đã chứng kiến tiến bộ trong việc mở rộng cửa sổ ngữ cảnh trong cả các kịch bản được tinh chỉnh và không được tinh chỉnh bằng cách mở rộng Mã hóa Vị trí Xoay (RoPE) (Su et al., 2021), một phương pháp mã hóa vị trí được sử dụng rộng rãi được áp dụng bởi các LLM tiên tiến như LLaMA (Touvron et al., 2023a,b), PaLM (Chowdhery et al., 2023; Anil et al., 2023) và GPT-NeoX (Black et al., 2022). Ví dụ, Nội suy Vị trí (PI) (kaiokendev, 2023; Chen et al., 2023) chia tỷ lệ tuyến tính xuống các chỉ số vị trí của các token đầu vào và đạt được kết quả tinh chỉnh cải thiện. Chia tỷ lệ NTK-Aware (bloc97, 2023b) và tần số cơ sở điều chỉnh (ABF) (Xiong et al., 2023) sửa đổi tần số cơ sở của RoPE, dẫn đến kết quả tăng cường trong các kịch bản tinh chỉnh và không tinh chỉnh tương ứng. Chia tỷ lệ NTK-By-Parts (bloc97, 2023a) xử lý các chiều khác nhau một cách khác nhau và báo cáo kết quả tinh chỉnh thậm chí còn tốt hơn. Gần đây hơn, YaRN (Peng et al., 2023) đề xuất chia tỷ lệ các logit chú ý với các hiệu ứng có lợi của nó đối với độ phức tạp mô hình hóa ngôn ngữ. Họ kết hợp kỹ thuật này với chia tỷ lệ NTK-By-Parts và báo cáo hiệu suất ngữ cảnh dài tốt nhất trong số các phương pháp mở rộng RoPE hiện có.

Tuy nhiên, lý do cơ bản đằng sau hiệu quả của hoạt động chia tỷ lệ của YaRN vẫn được hiểu kém. Trong nghiên cứu này, chúng tôi cung cấp một diễn giải về kỹ thuật này bằng cách phân tích hiệu ứng của nó trong việc ổn định entropy thông tin của điểm số chú ý của mô hình. Thông qua phân tích của chúng tôi, chúng tôi giới thiệu phương pháp mở rộng RoPE của riêng chúng tôi được gọi là "ABF nhận thức entropy", kết hợp ABF với việc sử dụng tinh tế của scalar chú ý động.

Hơn nữa, mặc dù hiệu quả được báo cáo riêng lẻ của các phương pháp mở rộng RoPE trước đây, thiếu phân tích so sánh toàn diện nơi các phương pháp khác nhau được đặt trong cùng một bộ thử nghiệm đánh giá. Nghiên cứu này cũng giải quyết khoảng trống này bằng cách trả lời ba câu hỏi chính liên quan đến việc mở rộng cửa sổ ngữ cảnh trong các ứng dụng thực tế: (1) Phương pháp nào thể hiện hiệu suất tinh chỉnh có giám sát tốt nhất trên các tác vụ hạ nguồn đòi hỏi ngữ cảnh? (2) Mỗi phương pháp có thể sử dụng dữ liệu huấn luyện một cách hiệu quả như thế nào? (3) Các mô hình được huấn luyện với các phương pháp này có hiệu suất mạnh mẽ trên các kích thước cửa sổ ngữ cảnh khác nhau không?

Để trả lời các câu hỏi trên, chúng tôi tiến hành thí nghiệm trên một tập hợp đa dạng các tác vụ đòi hỏi ngữ cảnh từ LongBench (Bai et al., 2023), thao tác với lượng dữ liệu huấn luyện và độ dài lời nhắc để đánh giá các mô hình được tinh chỉnh trên các chiều khác nhau. Kết quả thí nghiệm chứng minh rằng các mô hình được huấn luyện với phương pháp của chúng tôi vượt trội tất cả các baseline trong hiệu suất tinh chỉnh ngữ cảnh dài và cũng duy trì hiệu suất mạnh mẽ trên các kích thước cửa sổ ngữ cảnh khác nhau. Đáng chú ý, chỉ với 100 cuộc hội thoại dài từ ShareGPT (Chiang et al., 2023) và 6 bước huấn luyện, sử dụng bốn GPU A100 trong khoảng 6 phút, phương pháp của chúng tôi tạo ra một mô hình với hiệu suất có thẩm quyền trên 12 tác vụ đòi hỏi ngữ cảnh được chọn. Cuối cùng, chúng tôi khám phá ảnh hưởng của thành phần dữ liệu và chương trình giảng dạy đối với việc mở rộng cửa sổ ngữ cảnh cho một tác vụ hạ nguồn ngữ cảnh dài nhất định, đề xuất tinh chỉnh mô hình trên các cuộc hội thoại ShareGPT dài như một điểm khởi đầu tốt.

## 2 Kiến thức chuẩn bị

**Mã hóa Vị trí Xoay (RoPE)** Cho một chỉ số vị trí m∈[1, c] và một vector nhúng x:= [x0, x1, . . . , xd−1]⊤, trong đó d là chiều của mỗi đầu chú ý, RoPE coi mỗi cặp phần tử dọc theo chiều đặc trưng của vector nhúng như các số phức và mã hóa thông tin vị trí bằng cách xoay chúng. Hàm có giá trị vector phức f(x, m) được định nghĩa bởi RoPE như sau:

f(x, m) = [
(x0+ ix1)eimθ1,
(x2+ ix3)eimθ2,
. . . ,
(xd−2+ ixd−1)eimθd/2
] (1)

i := √−1 là đơn vị ảo và θj=b−2j/d, trong đó b biểu thị tần số cơ sở của RoPE và được đặt mặc định là 10,000.

Trong ứng dụng, RoPE được áp dụng cho cả nhúng truy vấn và nhúng khóa thông qua phương trình sau:

f(x, m) = [
x0
x1
x2
x3
...
xd−2
xd−1
] ⊗ [
cos(mθ0)
cos(mθ0)
cos(mθ1)
cos(mθ1)
...
cos(mθ(d−1)/2)
cos(mθ(d−1)/2)
] + [
−x1
x0
−x3
x2
...
−xd−1
xd−2
] ⊗ [
sin(mθ0)
sin(mθ0)
sin(mθ1)
sin(mθ1)
...
sin(mθ(d−1)/2)
sin(mθ(d−1)/2)
] (2)

Các thành phần cơ bản của RoPE là một loạt các hệ số lượng giác, mỗi hệ số mã hóa thông tin vị trí của các tần số khác nhau.

Chúng tôi biểu diễn các hệ số lượng giác này với hàm sau để xác định duy nhất RoPE và các biến thể của nó:

h(m, b, t) = √t * cos(m/b^(2j/d)) hoặc √t * sin(m/b^(2j/d)) (3)

trong đó m là chỉ số vị trí của token truy vấn, b là tần số cơ sở cho RoPE, và t là hệ số chia tỷ lệ cho logit chú ý. Lưu ý rằng √t được sử dụng trong phương trình vì quá trình xoay RoPE cả nhúng truy vấn và nhúng khóa.

Trước khi giới thiệu các phương pháp mở rộng RoPE cho phép mở rộng cửa sổ ngữ cảnh tốt hơn, chúng tôi định nghĩa hệ số chia tỷ lệ ngữ cảnh s=c′/c, là tỷ lệ giữa cửa sổ ngữ cảnh đích c′ và cửa sổ ngữ cảnh được huấn luyện trước c. Nó có tác dụng đặc biệt đối với những phương pháp mở rộng RoPE theo kích thước cửa sổ ngữ cảnh đích nhất định.

**Nội suy Vị trí (PI)** PI (Chen et al., 2023; kaiokendev, 2023) nội suy tuyến tính chỉ số vị trí đầu vào m thành m/s sao cho nó nằm trong kích thước cửa sổ ngữ cảnh ban đầu. Chen et al. (2023) chứng minh rằng tinh chỉnh trực tiếp LLaMA (Touvron et al., 2023a) với cửa sổ ngữ cảnh mở rộng dẫn đến cải thiện tối thiểu, vì cửa sổ ngữ cảnh hiệu quả của mô hình chỉ tăng từ 2,048 lên 2560 sau 10,000 bước huấn luyện trên các chuỗi có độ dài 8,192. Ngược lại, PI thành công trong việc mở rộng cửa sổ ngữ cảnh của LLaMA lên 32,768 chỉ với 1,000 bước huấn luyện.

**NTK-Aware** Chia tỷ lệ NTK-Aware (bloc97, 2023b) giả thuyết rằng nội suy tất cả các chiều một cách bằng nhau, như được thực hiện bởi PI, có thể dẫn đến mất thông tin tần số cao. Do đó, chia tỷ lệ NTK-Aware giới thiệu một chiến lược nội suy phi tuyến bằng cách điều chỉnh tần số cơ sở b của RoPE thành b^(d/(d−2)). Sửa đổi này chia tỷ lệ các thành phần tần số thấp của RoPE đến mức độ tương tự như PI, trong khi chỉ thay đổi nhẹ các thành phần tần số cao để tránh làm xáo trộn thông tin tần số cao. NTK-Aware mở rộng kích thước cửa sổ ngữ cảnh của mô hình mà không cần huấn luyện. Tuy nhiên, phương pháp này không thể hưởng lợi nhiều như PI từ việc huấn luyện bổ sung trên các chuỗi dài hơn như đề xuất bởi (Peng et al., 2023).

**NTK-By-Parts** NTK-By-Parts (bloc97, 2023a) cho rằng việc kéo dài tất cả các thành phần RoPE bằng hệ số chia tỷ lệ s hoặc biến đổi cơ sở dẫn đến các nhúng token gần nhau hơn, cản trở LLM khỏi việc nắm bắt hiệu quả các mối quan hệ cục bộ giữa các token liền kề. Để giải quyết vấn đề này, NTK-By-Parts chia tỷ lệ θ(j) bằng hệ số (1−γ(j))/s+γ(j), với γ(j) được gán 0 cho tần số cao, 1 cho tần số thấp, và một hằng số được xác định trước trong phạm vi từ 0 đến 1 cho tần số trung gian. Theo (Peng et al., 2023), phương pháp này hoạt động tốt hơn PI và chia tỷ lệ NTK-Aware cho cả mô hình được tinh chỉnh và không được tinh chỉnh.

**YaRN** Yarn (Peng et al., 2023) quan sát thực nghiệm rằng việc giới thiệu nhiệt độ t để chia tỷ lệ các logit chú ý trước hàm softmax cải thiện hiệu suất mô hình hóa ngôn ngữ của mô hình. Họ tìm thấy giá trị tối ưu của √t = 0.1 ln s + 1 bằng cách khớp đường cong độ phức tạp thấp nhất với các hệ số chia tỷ lệ ngữ cảnh s khác nhau. Họ kết hợp phát hiện của mình với chia tỷ lệ NTK-By-Parts và gọi phương pháp này là YaRN (Yet another RoPE extensioN method). YaRN báo cáo hiệu suất ngữ cảnh dài tốt nhất trên các tác vụ mô hình hóa ngôn ngữ trong số các phương pháp hiện có.

**Tần số Cơ sở Điều chỉnh (ABF)** ABF (Xiong et al., 2023) đơn giản thay đổi tần số cơ sở của RoPE thành 50,000. Cả phân tích lý thuyết và thí nghiệm đều được tiến hành để xác nhận hiệu quả của phương pháp này. Xiong et al. (2023) chứng minh rằng ABF giảm thiểu khoảng cách của các vector nhúng của nó từ những vector sử dụng RoPE ban đầu, giúp tận dụng kết quả huấn luyện trước. Họ xác nhận thực nghiệm hiệu quả của ABF bằng cách hiển thị độ phức tạp thấp hơn trên các tác vụ mô hình hóa ngôn ngữ và cửa sổ ngữ cảnh hiệu quả dài hơn trong tác vụ truy xuất câu đầu tiên.

Bảng 1 làm nổi bật sự khác biệt giữa RoPE và các biến thể của nó bằng cách chỉ định các m, b, và t khác nhau mà chúng sử dụng trong Phương trình 3 và liệu chúng có yêu cầu huấn luyện bổ sung để mở rộng cửa sổ ngữ cảnh:

| Phương pháp | m | b | t | Huấn luyện Bổ sung |
|-------------|---|---|---|-------------------|
| RoPE | m | 10,000 | 1 | - |
| PI | m/s | 10,000 | 1 | huấn luyện trước liên tục |
| NTK-Aware | m | 10,000^((d−2)/d) | 1 | - |
| NTK-By-Parts | ((1−γ(j))/s+γ(j))m | 10,000 | 1 | huấn luyện trước liên tục |
| YaRN | ((1−γ(j))/s+γ(j))m | 10,000 | 0.1ln(s) + 1 | huấn luyện trước liên tục |
| ABF | m | 500,000 | 1 | huấn luyện trước liên tục |

Bảng 1: Tổng quan về Mã hóa Vị trí Xoay (RoPE) và các biến thể của nó được biểu diễn bởi Phương trình 3.

## 3 Phương Pháp Đề xuất

YaRN (Peng et al., 2023) giới thiệu một hệ số chia tỷ lệ t trên các logit chú ý dựa trên bằng chứng thực nghiệm cho thấy các hiệu ứng có lợi của nó đối với độ phức tạp mô hình hóa ngôn ngữ. Tuy nhiên, lý do cơ bản đằng sau kỹ thuật này vẫn được hiểu kém. Trong phần này, trước tiên chúng tôi giới thiệu một diễn giải về kỹ thuật này, điều này thúc đẩy phương pháp của chúng tôi.

### 3.1 Diễn giải Hệ số Chia tỷ lệ của YaRN

Trong cơ chế chú ý của các mô hình Transformer (Vaswani et al., 2017), hàm Softmax buộc các điểm số chú ý được gán cho các token ngữ cảnh phải tổng bằng một trong khi đồng thời ngăn không cho bất kỳ điểm số riêng lẻ nào trở thành không. Do đó, với số lượng token đầu vào tăng lên, LLM về mặt lý thuyết sẽ phân phối nhiều chú ý hơn trên nhiều token hơn và dẫn đến sự gia tăng trong cái mà chúng tôi gọi là "entropy chú ý", định lượng tính ngẫu nhiên trong phân phối của các điểm số chú ý và được tính bằng phương trình sau:

attention_entropy = ∑i pi ln pi (4)

trong đó pi là các điểm số chú ý được gán cho các token ngữ cảnh.

Để xác nhận hiệu ứng lý thuyết nói trên, chúng tôi đã sử dụng LLaMA-2-7B-Chat (Touvron et al., 2023b) để xử lý 128 tài liệu được chọn ngẫu nhiên từ bộ dữ liệu Pile (Gao et al., 2020). Chúng tôi thu thập các điểm số chú ý được gán cho các token ngữ cảnh cho các token truy vấn tại các vị trí đầu vào khác nhau để mô phỏng số lượng token ngữ cảnh khác nhau. Sau đó, chúng tôi tính entropy thông tin cho các điểm số chú ý này trên các lớp mô hình khác nhau thông qua Phương trình 4. Các entropy chú ý trung bình kết quả trên các tài liệu được lấy mẫu ngẫu nhiên của chúng tôi được trực quan hóa trong Hình 1.

Ngược lại với trực giác, chỉ có hai lớp mô hình đầu tiên thể hiện sự gia tăng ổn định trong entropy chú ý. Thú vị là, chúng tôi thậm chí quan sát thấy rằng entropy chú ý của tất cả các lớp tiếp theo vẫn rất giống nhau khi số lượng token ngữ cảnh tăng từ 1,024 lên 2,048.

Phát hiện này về việc LLM duy trì entropy chú ý ổn định trong các lớp mô hình tiếp theo khi các token ngữ cảnh được nhân đôi trực tiếp dẫn chúng tôi đến giả định rằng việc sở hữu một mức độ bất biến chiều dài nhất định trong entropy chú ý trong các lớp này là một đặc tính vốn có quan trọng của LLM để hoạt động đúng cách. Khi mô hình hóa các chuỗi dài hơn so với giai đoạn huấn luyện trước, LLM có thể không tập trung tốt, dẫn đến giảm hiệu suất. Nhờ hàm mũ trong Softmax, việc chia tỷ lệ các logit chú ý làm giảm entropy chú ý, do đó giải thích tại sao nó dẫn đến cải thiện trong các tác vụ mô hình hóa ngôn ngữ khi mô hình hóa đầu vào dài như quan sát thấy trong YaRN (Peng et al., 2023).

### 3.2 Nguyên tắc Thiết kế

Các công trình trước đây đã khám phá các hệ số chia tỷ lệ khác nhau trên các logit chú ý với các động cơ khác nhau. Chiang và Cholak (2022) chia tỷ lệ các logit chú ý bằng log n, với n đại diện cho độ dài của chuỗi huấn luyện dài nhất, để tăng cường khả năng ngoại suy của mô hình trong các tác vụ hạ nguồn như dịch máy.

Gần đây hơn, YaRN (Peng et al., 2023) giới thiệu hệ số chia tỷ lệ t = 0.1 ln s + 1 bằng cách khớp đường cong độ phức tạp thấp nhất trong các tác vụ mô hình hóa ngôn ngữ. Họ kết hợp các hệ số chia tỷ lệ này với chia tỷ lệ NTK-By-Parts và quan sát hiệu suất ngữ cảnh dài tinh chỉnh cải thiện trên các tác vụ mô hình hóa ngôn ngữ.

ReRoPE (Su, 2023) sử dụng một hệ số chia tỷ lệ động tính đến số lượng token ngữ cảnh cho mỗi vị trí đầu vào: t = log c/m, trong đó c biểu thị kích thước cửa sổ ngữ cảnh được huấn luyện trước và m đại diện cho chỉ số vị trí của các token đầu vào. Bằng cách giới thiệu hệ số chia tỷ lệ này trong giai đoạn huấn luyện trước, ReRoPE thể hiện khả năng ngoại suy tăng cường trong các tác vụ mô hình hóa ngôn ngữ, điều này cũng được quan sát thấy trong YaRN.

Chúng tôi đề xuất "ABF nhận thức entropy" với các nguyên tắc thiết kế sau:

**(1). Chia tỷ lệ Chú ý Động:** Cả PI và YaRN đều sử dụng một hệ số chia tỷ lệ không đổi cho tất cả các vị trí đầu vào, điều này có thể kéo dài quá mức các logit chú ý tại các vị trí phía trước và cản trở khả năng ngoại suy của mô hình đến các chuỗi dài hơn. Thay vì sử dụng một hệ số chia tỷ lệ không đổi, chúng tôi đề xuất sử dụng một hệ số động như ReRoPE tính đến số lượng token ngữ cảnh cho mỗi vị trí đầu vào. Điều này cho phép mô hình điều chỉnh trọng số chú ý một cách linh hoạt hơn dựa trên mức độ ngẫu nhiên trong phân phối của các điểm số chú ý.

**(2). Phụ thuộc Lớp:** Tất cả các công trình hiện có áp dụng scalar một cách không phân biệt cho tất cả các lớp mô hình. Tuy nhiên, dựa trên quan sát của chúng tôi trong Hình 1 rằng hai lớp đầu tiên luôn thể hiện mẫu chú ý gần như đồng nhất và chỉ các lớp sau thể hiện xu hướng duy trì sự tập trung, chúng tôi đề xuất không can thiệp vào hai lớp đầu tiên để phù hợp với các đặc tính vốn có của mô hình.

**(3). Tạo điều kiện Mở rộng Cửa sổ Ngữ cảnh:** Hơn nữa, chúng tôi giả thuyết rằng việc học cách duy trì sự tập trung khi xử lý các chuỗi dài là quan trọng đối với việc mở rộng cửa sổ ngữ cảnh, và việc chia tỷ lệ các logit chú ý có thể phục vụ như một bias quy nạp tạo điều kiện cho quá trình này. Điều này thúc đẩy chúng tôi kết hợp "chia tỷ lệ các logit chú ý" với ABF trong giai đoạn tinh chỉnh có giám sát. Để tận dụng kết quả huấn luyện trước, chúng tôi cũng đề xuất tránh sửa đổi các logit chú ý trong cửa sổ ngữ cảnh được huấn luyện trước bằng cách đặt một giới hạn dưới cho t.

Hệ số chia tỷ lệ cuối cùng t của chúng tôi được mô tả dưới đây:

t = {
1, nếu chỉ số lớp là 0 hoặc 1
max(log c/i, 1), trường hợp khác
}

## 4 Thí nghiệm

Để phân tích khả năng áp dụng thực tế của các phương pháp mở rộng RoPE khác nhau, chúng tôi thử nghiệm hiệu suất ngữ cảnh dài của các mô hình được huấn luyện với các phương pháp này trên các tác vụ được chọn từ LongBench (Bai et al., 2023) và trả lời ba câu hỏi nghiên cứu chúng tôi đề xuất trong Phần 1 bằng cách điều chỉnh lượng dữ liệu huấn luyện và kích thước cửa sổ ngữ cảnh. Cuối cùng, chúng tôi cũng khám phá thành phần dữ liệu hiệu quả và chương trình giảng dạy về việc mở rộng cửa sổ ngữ cảnh cho các tác vụ hạ nguồn nhất định.

### 4.1 Thiết lập Chung

**Biến thể Mô hình** Chúng tôi sử dụng LLaMA-2-7B-Chat (Touvron et al., 2023b) do tính phổ biến của nó. Chúng tôi chỉ sửa đổi RoPE trong khi để kiến trúc mô hình không thay đổi.

**Huấn luyện** Các công trình trước đây (Chen et al., 2023; Xiong et al., 2023; Peng et al., 2023) áp dụng một chương trình giảng dạy tương tự bằng cách đầu tiên huấn luyện trước liên tục mô hình LLaMA cơ sở để thích ứng với các nhúng vị trí được sửa đổi và sau đó tinh chỉnh trên các tác vụ hạ nguồn ngữ cảnh dài đích. Ngược lại, chúng tôi đề xuất tinh chỉnh có giám sát trực tiếp Mô hình Chat để đánh giá khả năng áp dụng thực tế của các phương pháp mở rộng RoPE khác nhau. Chúng tôi mở rộng cửa sổ ngữ cảnh của LLaMA-2-7B-Chat lên 16k với các thiết lập huấn luyện chi tiết có sẵn trong Phụ lục A.

**Dữ liệu SFT** Chúng tôi tuyển chọn một bộ dữ liệu gồm 3.5k cuộc hội thoại dài từ ShareGPT (Chiang et al., 2023). Theo đường ống làm sạch dữ liệu trong (Zheng et al., 2023), chúng tôi chỉ giữ các cuộc hội thoại tiếng Anh, loại trừ những cuộc có ít hơn 10,000 token, và chia các cuộc hội thoại dài hơn để chúng tôi có độ dài chuỗi tối đa là 16,384 token.

**Đánh giá** Các công trình hiện có chủ yếu đánh giá hiệu quả của các phương pháp mở rộng RoPE thông qua việc kiểm tra các mô hình được huấn luyện trước liên tục trên các tác vụ mô hình hóa ngôn ngữ và tác vụ tổng hợp. Ví dụ, YaRN (Chen et al., 2023) đánh giá các điểm số độ phức tạp và hiệu suất mô hình trên tác vụ truy xuất passkey (Mohtashami và Jaggi, 2023) để định lượng hiệu suất ngữ cảnh dài của mô hình. Tuy nhiên, các tác vụ tổng hợp như truy xuất passkey khác biệt lớn so với các kịch bản thực tế trong khi các tác vụ mô hình hóa ngôn ngữ cũng đã chứng minh là một chỉ số thô sơ không thể hứa hẹn thành công trong các tác vụ hạ nguồn như đề xuất bởi (Pal et al., 2023; Sun et al., 2021). Trong công trình này, chúng tôi phân tích hiệu suất ngữ cảnh dài của các mô hình với cửa sổ ngữ cảnh mở rộng trên các tác vụ được chọn từ LongBench (Bai et al., 2023). Đánh giá của chúng tôi bao gồm 12 tác vụ từ bốn danh mục: QA tài liệu đơn, QA tài liệu đa, tóm tắt và học few-shot để đảm bảo đánh giá toàn diện khả năng ngữ cảnh dài của mô hình. Chúng tôi cố ý loại trừ các tác vụ tổng hợp và tác vụ hoàn thành mã từ LongBench vì các tác vụ tổng hợp khác biệt lớn so với các kịch bản thực tế, và các tác vụ hoàn thành mã có xung đột hiệu suất với khả năng tuân theo hướng dẫn chung được học từ các cuộc hội thoại ShareGPT, như đề xuất bởi (Dong et al., 2023).

### 4.2 Đo lường Hiệu suất Ngữ cảnh Dài

Để trả lời câu hỏi nghiên cứu "(1) Phương pháp nào thể hiện hiệu suất tinh chỉnh có giám sát tốt nhất trên các tác vụ hạ nguồn đòi hỏi ngữ cảnh?", chúng tôi tinh chỉnh LLaMA-7B-Chat trên 3.5k cuộc hội thoại dài và đánh giá hiệu suất ngữ cảnh dài của chúng trên LongBench.

Bảng 2 minh họa hiệu suất của mỗi phương pháp, với một số kết quả được báo cáo từ bài báo LongBench (Bai et al., 2023). Chúng tôi nêu bật các quan sát chính của chúng tôi ở đây:

1) **Tinh chỉnh các mô hình trên dữ liệu cuộc hội thoại dài là hiệu quả cho việc mở rộng cửa sổ ngữ cảnh.** Cả LongChat-v1.5-7B-32k và Vicuna-v1.5-7B-16k đều là các mô hình ngữ cảnh dài mã nguồn mở được mở rộng với PI (Chen et al., 2023) thông qua tinh chỉnh trên lượng lớn dữ liệu cuộc hội thoại. Ví dụ, LongChat-v1.5-7B-32 được tinh chỉnh trên 80k cuộc hội thoại. Bằng cách tinh chỉnh mô hình chỉ trên các cuộc hội thoại dài, mô hình dựa trên PI được sao chép của chúng tôi vượt trội hơn các phiên bản mã nguồn mở, xác nhận hiệu quả của việc tinh chỉnh mô hình trên các cuộc hội thoại dài.

2) **PI mang lại kết quả tinh chỉnh ngữ cảnh dài tốt hơn YaRN.** Trong khi NTK-By-Parts và YaRN có độ phức tạp thấp hơn trong các tác vụ mô hình hóa ngôn ngữ, PI có hiệu suất tinh chỉnh tốt hơn trên các tác vụ hạ nguồn ngữ cảnh dài có liên quan nhiều hơn đến các kịch bản thực tế. Phát hiện này củng cố kết luận của (Pal et al., 2023; Sun et al., 2021) rằng độ phức tạp mô hình hóa ngôn ngữ là một chỉ số thô sơ không thể hứa hẹn thành công trong các tác vụ hạ nguồn. Chúng tôi giả thuyết rằng trong khi scalar của YaRN hiệu quả cho các tác vụ mô hình hóa ngôn ngữ, bản chất không đổi của nó có thể ảnh hưởng đến hiệu suất mô hình trên các tác vụ hạ nguồn.

3) **Các mô hình dựa trên ABF vượt trội hơn các phương pháp khác một cách đáng kể.** Cả ABF và phương pháp của chúng tôi đều thể hiện hiệu suất tinh chỉnh vượt trội nhất quán trên tất cả 12 tác vụ ngữ cảnh dài, chứng minh hiệu quả của việc điều chỉnh tần số cơ sở của RoPE thành một số lớn (ví dụ 50,000).

### 4.3 Đo lường Hiệu quả Dữ liệu

Hiệu quả dữ liệu là một đặc tính thiết yếu của các phương pháp mở rộng RoPE trong thực hành mở rộng cửa sổ ngữ cảnh, xét cả sự khan hiếm của dữ liệu huấn luyện dài và chi phí cao của việc huấn luyện trên các chuỗi dài. Trong phần này, chúng tôi khám phá câu hỏi nghiên cứu "(2) Mỗi phương pháp có thể sử dụng dữ liệu huấn luyện một cách hiệu quả như thế nào?" bằng cách huấn luyện mô hình tương ứng trên 32, 100, 1k và 3.5k cuộc hội thoại. Kết quả được vẽ trong Hình 2, và kết quả chi tiết cho mỗi tác vụ có trong Bảng 5.

Chúng tôi nêu bật các quan sát chính của chúng tôi dưới đây:

1) **Các phương pháp dựa trên ABF luôn hưởng lợi từ việc tăng dữ liệu huấn luyện.** Trong khi tất cả các phương pháp mở rộng RoPE đều thể hiện hiệu suất cải thiện với dữ liệu huấn luyện tăng lên, lợi ích hiệu suất có vẻ nhỏ đối với PI, NTK-By-Parts và Yarn khi lượng dữ liệu tăng từ 1K lên 3.5K. Chỉ các phương pháp dựa trên ABF luôn thể hiện lợi ích hiệu suất.

2) **ABF Nhận thức Entropy thể hiện hiệu quả dữ liệu phi thường.** Đáng chú ý, chỉ với 100 mẫu huấn luyện và 6 bước huấn luyện, phương pháp của chúng tôi đạt được hiệu suất ngữ cảnh dài cạnh tranh chỉ tụt hậu một cách nhỏ so với phương pháp ABF được huấn luyện trên 3.5K mẫu. Không xét đến chi phí tinh chỉnh trên các tác vụ hạ nguồn, PI (Chen et al., 2023) tiếp tục huấn luyện trước LLaMA-7B (Touvron et al., 2023a) cho 1,000 bước với kích thước batch 64, YaRN (Peng et al., 2023) áp dụng 250 bước huấn luyện trước liên tục với cùng kích thước batch. Thực hành mã nguồn mở như Longchat (Li* et al., 2023) sử dụng 80k cuộc hội thoại từ ShareGPT cho việc tinh chỉnh hướng dẫn. Công trình của chúng tôi chứng minh hiệu quả đáng kể của ABF nhận thức entropy trong việc mở rộng cửa sổ ngữ cảnh, yêu cầu ít hơn 2% tài nguyên huấn luyện được sử dụng bởi các phương pháp hiện có.

Chúng tôi cũng quan sát thấy rằng khoảng cách hiệu suất từ ABF đến phương pháp của chúng tôi đang giảm dần với sự gia tăng dữ liệu huấn luyện. Hiện tượng này phù hợp với giả thuyết của chúng tôi trong Phần 3.2 rằng trong khi khả năng duy trì sự tập trung trên các đầu vào dài có thể được học từ việc huấn luyện trên nhiều dữ liệu hơn, phương pháp của chúng tôi phục vụ như một bias quy nạp tạo điều kiện cho quá trình học.

### 4.4 Đo lường Độ bền vững trên các Cửa sổ Ngữ cảnh

Một thuộc tính mong muốn cho các phương pháp mở rộng RoPE, khi được áp dụng trong các thiết lập mở rộng cửa sổ ngữ cảnh thực tế, là các mô hình được tinh chỉnh sử dụng các phương pháp này nên duy trì hiệu suất của chúng trên cửa sổ ngữ cảnh ban đầu, đồng thời cũng thể hiện một mức độ khả năng ngoại suy nhất định vượt ra ngoài độ dài được tinh chỉnh.

Để trả lời câu hỏi nghiên cứu "(3) Các mô hình được huấn luyện với các phương pháp này có hiệu suất mạnh mẽ trên các kích thước cửa sổ ngữ cảnh khác nhau không?", chúng tôi theo LongBench (Bai et al., 2023) để đánh giá các mô hình trên các kích thước cửa sổ ngữ cảnh khác nhau bằng cách cắt ngắn lời nhắc từ giữa khi độ dài tác vụ vượt quá kích thước cửa sổ ngữ cảnh được chỉ định.

Kết quả được mô tả trong Hình 3. Trong khi có vẻ có lợi ích hiệu suất cho PI, NTK-By-Parts và Yarn khi kích thước ngữ cảnh được mở rộng từ 4k lên 8k, hiệu suất của chúng giảm khi ngữ cảnh được mở rộng thêm lên 16k, thể hiện việc không có khả năng tận dụng toàn bộ cửa sổ ngữ cảnh tinh chỉnh. Ngược lại, ABF và phương pháp đề xuất của chúng tôi luôn hưởng lợi từ cửa sổ ngữ cảnh lớn hơn trong độ dài tinh chỉnh. Hơn nữa, ABF nhận thức entropy là phương pháp duy nhất có thể duy trì hiệu suất khi ngoại suy trực tiếp lên 32k.

### 4.5 Khám phá Dữ liệu và Chương trình Giảng dạy Tối ưu

Trong phần này, chúng tôi khám phá dữ liệu huấn luyện và chương trình giảng dạy hiệu quả cho việc mở rộng cửa sổ ngữ cảnh trên các tác vụ nhất định. Một cân nhắc quan trọng trong thực tế là liệu các mẫu huấn luyện trong miền dài có không thể thiếu để đạt được thành công trong việc mở rộng cửa sổ ngữ cảnh cho một tác vụ hạ nguồn cụ thể. Cụ thể, chúng tôi hỏi liệu chỉ các mẫu huấn luyện trong miền ngắn có thể vẫn mang lại lợi ích trong các kịch bản mà các mẫu dài hơn vắng mặt, điều này thường xảy ra. Để trả lời các câu hỏi trên, chúng tôi tiến hành thí nghiệm với các chương trình giảng dạy khác nhau trên GovReport (Huang et al., 2021) là một tác vụ tóm tắt ngữ cảnh dài được sử dụng rộng rãi, và Longchat-Line-Retrieval (Li* et al., 2023), một tác vụ truy xuất tổng hợp.

Chúng tôi đánh giá cả tác vụ dài (hơn 8,092 token) và ngắn (trong 4,096 token) để đảm bảo hiệu suất của mô hình trong cửa sổ ngữ cảnh ban đầu trong khi đánh giá hiệu suất ngữ cảnh dài của chúng. Khi dữ liệu huấn luyện là các mẫu trong miền, chúng tôi huấn luyện mô hình cho 4 epoch với kích thước batch 8 và đánh giá với epoch tốt nhất trên tập xác thực. Khi dữ liệu huấn luyện là 1,000 cuộc hội thoại ShareGPT, mô hình được huấn luyện cho hai epoch với kích thước batch 32 và được đánh giá trên epoch thứ hai.

Kết quả được hiển thị trong Bảng 3. Chúng tôi kết luận rằng việc huấn luyện mô hình trên các mẫu trong miền ngắn tạo ra kết quả không tối ưu, nhưng bắt đầu từ mô hình được tinh chỉnh trên 1,000 cuộc hội thoại ShareGPT mang lại kết quả tương đương với những mô hình được tinh chỉnh trên các mẫu trong miền dài, điều này đề xuất một điểm khởi đầu tốt cho việc mở rộng cửa sổ ngữ cảnh trong thực tế.

Có thể lạ rằng tác vụ truy xuất dòng cho thấy hiệu suất cực kém khi được tinh chỉnh từ mô hình Chat trên các mẫu dài. Chúng tôi quy cho điều này do việc huấn luyện không đủ của phương pháp chúng tôi vì câu trả lời cho tác vụ truy xuất dòng ngắn, và chúng tôi chỉ tính toán tổn thất trên các token phản hồi của mô hình trong quá trình tinh chỉnh hướng dẫn.

| Khởi tạo | Dữ liệu huấn luyện | GR-S | GR-L | LR-S | LR-L |
|----------|-------------------|------|------|------|------|
| LLaMA 2 Chat | Không có | 30.84 | 0 | 76 | 0 |
| LLaMA 2 Chat | Ngắn | 37.91 | 33.6 | 74 | 26 |
| LLaMA 2 Chat | Dài | 38.24 | 36.45 | 10 | 2 |
| Share1k | Không có | 34.10 | 31.14 | 88 | 48 |
| Share1k | Ngắn | 38.31 | 35.12 | 86 | 64 |
| Share1k | Dài | 38.93 | 35.56 | 92 | 66 |
| Ngắn Share1k | | 39.74 | 32.12 | 90 | 54 |

Bảng 3: Hiệu suất trên hai tác vụ hạ nguồn với các chương trình giảng dạy khác nhau. GR-S: GovReport-Ngắn. GR-L: GovReport-Dài. LR-S: Truy xuất Dòng-Ngắn. LR-L: Truy xuất Dòng-Dài. Trong cột đầu tiên, Share1k có nghĩa là kết quả tinh chỉnh của mô hình Chat 7B trên 1,000 cuộc hội thoại ShareGPT. Ngắn có nghĩa là kết quả tinh chỉnh của mô hình chat 7B trên các mẫu trong miền ngắn. Trong cột thứ hai, Không có nghĩa là mô hình được thử nghiệm trực tiếp. Ngắn có nghĩa là các mẫu trong miền ngắn. Dài có nghĩa là các mẫu trong miền dài.

## 5 Công trình Liên quan

Nghiên cứu rộng rãi đã được thực hiện để tăng cường khả năng ngữ cảnh dài của các mô hình transformer (Vaswani et al., 2017) bằng cách vượt qua hai trở ngại nổi bật: độ phức tạp thời gian và không gian bậc hai của cơ chế chú ý (Vaswani et al., 2017) và việc không có khả năng của các mã hóa vị trí để tổng quát hóa vượt ra ngoài cửa sổ ngữ cảnh được huấn luyện trước.

**Transformer Hiệu quả hơn** Cơ chế chú ý vanilla trong kiến trúc Transformer được biết đến với độ phức tạp thời gian và không gian bậc hai, điều này đặt ra những yêu cầu tài nguyên đáng kể cho các mô hình transformer khi xử lý đầu vào dài. Các công trình khác nhau đã tập trung vào việc chinh phục vấn đề phức tạp và đề xuất các Transformer hiệu quả hơn. Sparse transformer (Child et al., 2019; Ye et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Ding et al., 2023) thay thế cơ chế chú ý đầy đủ ban đầu bằng một phiên bản thưa thớt để làm cho việc tính toán hiệu quả hơn. Linear transformer (Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2020), thay vì buộc cơ chế chú ý chú ý đến ít token hơn, đề xuất một cách tiếp cận thay thế bằng cách tận dụng phép nhân ma trận rank thấp hoặc tích vô hướng tuyến tính của các bản đồ đặc trưng kernel để xấp xỉ cơ chế chú ý ban đầu, đạt được độ phức tạp thời gian tuyến tính. Trong khi đó, các mô hình tăng cường truy xuất (Guu et al., 2020; Lewis et al., 2020; Wu et al., 2022; Bulatov et al., 2023; Tworkowski et al., 2023) tích hợp truy xuất với chú ý. Trong thời gian suy luận, các mô hình này tránh việc mô hình hóa trực tiếp các đầu vào dài bằng cách truy xuất thông tin từ bộ nhớ ngoài lưu trữ các cặp key-value trước đó.

Trong khi nghiên cứu trước đây chủ yếu tập trung vào việc giảm FLOP, nút thắt cổ chai của suy luận transformer trên phần cứng máy tính hiện đại đã chuyển sang chi phí từ truy cập bộ nhớ (IO). Multi-query attention (MQA)(Shazeer, 2019) và grouped-query attention (GQA)(Ainslie et al., 2023), ví dụ, giải quyết chi phí băng thông bộ nhớ liên quan đến việc tải các tensor "keys" và "values" lớn trong cơ chế chú ý đa đầu bằng cách đề xuất sử dụng ít đầu "key" và "value" hơn. Đáng chú ý, GQA được sử dụng trong LLaMA2 (Touvron et al., 2023b). Ngoài ra, FlashAttention (Dao et al., 2022; Dao, 2023) giới thiệu một cách tiếp cận chú ý chính xác nhận thức IO sử dụng tiling để giảm IO bộ nhớ.

**Mã hóa Vị trí Tổng quát hóa** Do bản chất song song của cơ chế chú ý, các mô hình transformer yêu cầu các phương pháp mã hóa vị trí (PE) để tạo điều kiện cho việc tích hợp thông tin vị trí. Transformer ban đầu sử dụng mã hóa vị trí sinusoidal, tạo thành một PE tuyệt đối và thể hiện khả năng tổng quát hóa hạn chế. Sau đó, cách tiếp cận này được tinh chỉnh thành một phiên bản có thể học (Gehring et al., 2017), được áp dụng bởi các kiến trúc mô hình ngôn ngữ như GPT-3 (Brown et al., 2020). Tuy nhiên, sự thích ứng này hoàn toàn làm tổn hại khả năng ngoại suy của các phương pháp mã hóa vị trí. Sự ra đời của PE tương đối (Shaw et al., 2018) về mặt lý thuyết hỗ trợ độ dài đầu vào vô hạn. Tuy nhiên, mặc dù có những tiến bộ gần đây trong PE tương đối, như T5 relative PE (Raffel et al., 2020), RoPE (Su et al., 2021), xPOS (Sun et al., 2022), và ALiBi (Press et al., 2021), đã được chứng minh bởi (Kazemnejad et al., 2023) rằng tất cả các phương pháp này đều thất bại khi ngoại suy đáng kể vượt ra ngoài cửa sổ ngữ cảnh được huấn luyện trước.

## 6 Kết luận

Tóm lại, thông qua việc diễn giải nhu cầu vốn có của LLM trong việc duy trì sự tập trung khi xử lý các chuỗi dài, chúng tôi đề xuất ABF nhận thức entropy bằng cách kết hợp ABF với một scalar được áp dụng tinh tế chia tỷ lệ các logit chú ý. Phương pháp đề xuất của chúng tôi mở rộng hiệu quả cửa sổ ngữ cảnh của các LLM dựa trên RoPE, giải quyết các hạn chế của chúng khi đối mặt với các tác vụ đòi hỏi ngữ cảnh với chi phí tối thiểu. Chúng tôi chứng minh thực nghiệm tính ưu việt của phương pháp chúng tôi trong cả kết quả tinh chỉnh và độ bền vững trên các kích thước cửa sổ ngữ cảnh khác nhau trên các tác vụ đòi hỏi ngữ cảnh khác nhau. Quan trọng là, phương pháp của chúng tôi thể hiện hiệu quả dữ liệu phi thường so với các phương pháp khác, tạo ra một mô hình ngữ cảnh dài có thẩm quyền trên LongBench chỉ với 100 mẫu và 6 bước huấn luyện, ít hơn 2% tài nguyên huấn luyện được sử dụng bởi các công trình trước đây. Cuối cùng, chúng tôi cung cấp những hiểu biết có giá trị về việc mở rộng cửa sổ ngữ cảnh cho các tác vụ hạ nguồn cụ thể, đề xuất huấn luyện trên các cuộc hội thoại ShareGPT dài như một điểm khởi đầu tốt.

## Lời cảm ơn

Chúng tôi muốn cảm ơn Zhengbao Jiang vì sự tham gia của anh ấy trong các cuộc thảo luận ban đầu. Chúng tôi cảm ơn Fan Nie và Fan Zhou vì lời khuyên vô giá của họ trong suốt quá trình viết bài.

## Tài liệu tham khảo

[Phần tài liệu tham khảo được giữ nguyên như bản gốc do tính chất kỹ thuật của các trích dẫn]

## A Chi tiết Huấn luyện

Mô hình được huấn luyện trên 4 GPU NVIDIA A100 với DeepSpeed (Rasley et al., 2020), ZeRO (Rajbhandari et al., 2020; Ren et al., 2021) Stage 3, gradient-checkpointing (Chen et al., 2016), và FlashAttention (Dao et al., 2022; Dao, 2023). Chúng tôi cũng sử dụng độ chính xác tính toán hỗn hợp BF16 và TF32 để tăng tốc thêm.

Tất cả các mô hình được tinh chỉnh sử dụng AdamW Optimizer (Loshchilov và Hutter, 2017) với β1= 0.9 và β2= 0.95 cho hai epoch, tính toán tổn thất chỉ trên các token phản hồi. Chúng tôi sử dụng một bộ lập lịch tốc độ học cosine, đặt tốc độ học đỉnh thành 2e-5, và weight decay thành 0.1. Đối với việc huấn luyện trên 3.5k cuộc hội thoại, chúng tôi sử dụng kích thước batch 128 và 10 bước khởi động. Chúng tôi sử dụng kích thước batch 32 và 0 bước khởi động cho ít dữ liệu huấn luyện hơn. Nếu không được nêu rõ, chúng tôi mặc định sử dụng 3.5k cuộc hội thoại ShareGPT cho việc tinh chỉnh hướng dẫn.

## B Kết quả Thí nghiệm Bổ sung

[Các bảng chi tiết được giữ nguyên như bản gốc]
