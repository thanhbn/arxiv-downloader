# LM-Infinite: Khả năng Tổng quát hóa Độ dài Cực đại Không cần Huấn luyện cho Mô hình Ngôn ngữ Lớn

Chi Han1*, Qifan Wang2, Hao Peng1, Wenhan Xiong3, Yu Chen4†, Heng Ji1, Sinong Wang3
1University of Illinois Urbana-Champaign,2Meta,3GenAI Meta,4Anytime AI
1{chihan3,haopeng,hengji}@illinois.edu ,
23{wqfcr,xwhan,sinongwang}@meta.com ,
4ychen@anytime-ai.com

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) ngày nay thường huấn luyện trên các đoạn văn bản ngắn (ví dụ: <4K token) do độ phức tạp bậc hai của kiến trúc Transformer. Kết quả là, hiệu suất của chúng giảm đáng kể trên các đầu vào dài hơn những gì gặp phải trong quá trình huấn luyện, hạn chế đáng kể ứng dụng của chúng trong các tác vụ thế giới thực liên quan đến ngữ cảnh dài như mã hóa bài báo khoa học, kho lưu trữ mã nguồn, hoặc đối thoại dài. Thông qua phân tích lý thuyết và điều tra thực nghiệm, công trình này xác định ba yếu tố chính góp phần vào sự thất bại trong tổng quát hóa độ dài này. Phân tích lý thuyết của chúng tôi tiết lộ rằng các kỹ thuật thường được sử dụng như sử dụng mẫu attention cửa sổ trượt hoặc mã hóa vị trí tương đối không đủ để giải quyết chúng. Trả lời những thách thức này, chúng tôi đề xuất LM-Infinite, một phương pháp đơn giản và hiệu quả để tăng cường khả năng xử lý ngữ cảnh dài của LLM. LM-Infinite rất linh hoạt và có thể được sử dụng với hầu hết các LLM hiện đại ngay lập tức. Không cần cập nhật tham số nào, nó cho phép các LLM được huấn luyện trước với các đoạn 2K hoặc 4K token tổng quát hóa lên đến đầu vào độ dài 200M trong khi vẫn duy trì perplexity. Nó cũng cải thiện hiệu suất trên các tác vụ downstream như Passkey Retrieval và Qasper trong cài đặt zero-shot. LM-Infinite mang lại cải thiện hiệu quả đáng kể: nó đạt được tăng tốc giải mã 2.7× và tiết kiệm bộ nhớ 7.5× so với mô hình gốc. Mã nguồn của chúng tôi được phát hành tại https://github.com/Glaciohound/LM-Infinite.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) gần đây đã tiến bộ vượt bậc trong nhiều tác vụ xử lý ngôn ngữ tự nhiên. Chúng thường huấn luyện trên các đoạn văn bản ít hơn 4K token (Touvron et al., 2023b; Team, 2023), chủ yếu do chi phí tính toán bậc hai theo độ dài đầu vào của kiến trúc Transformer. Kết quả là, chúng gặp khó khăn trong việc tổng quát hóa đến các đầu vào dài hơn nhiều so với những gì chúng được huấn luyện và bị suy giảm hiệu suất đáng kể (Tworkowski et al., 2023; Chen et al., 2023a). Điều này hạn chế khả năng ứng dụng của chúng trong các tác vụ yêu cầu ngữ cảnh tầm xa, như mã hóa bài báo khoa học, sinh mã kho lưu trữ nguồn, hoặc đối thoại ngữ cảnh dài.

Nhiều nỗ lực đã được dành để giải quyết thách thức tổng quát hóa độ dài này. Các mã hóa vị trí tương đối như RoPE (Su et al., 2021) và Alibi (Press et al., 2021) đã được áp dụng rộng rãi bởi các LLM tiên tiến, tính toán attention dựa trên khoảng cách giữa các token thay vì vị trí tuyệt đối, hy vọng tránh được lỗi mô hình do embedding vị trí tuyệt đối chưa thấy. Hơn nữa, mặc dù áp dụng mẫu attention cửa sổ trượt trên kiến trúc Transformer có thể giảm chi phí bộ nhớ (Beltagy et al., 2020; Ding et al., 2023; Zaheer et al., 2020), chúng không thể áp dụng trực tiếp cho các mô hình được huấn luyện trước để tổng quát hóa độ dài mà không cần huấn luyện thêm. Thông qua phân tích lý thuyết và điều tra thực nghiệm, §3 xác định ba yếu tố chính gây ra sự thất bại tổng quát hóa độ dài: (1) thách thức xử lý khoảng cách chưa thấy giữa các token, (2) khó khăn trong việc attention đến số lượng token chưa thấy, và (3) thông tin vị trí tuyệt đối được mã hóa ngầm trong các token đầu tiên. Những thách thức này có thể làm cho các đặc trưng tính toán của LLM, như logit attention và vector ẩn, lệch khỏi phân phối huấn luyện, dẫn đến thất bại tổng quát hóa độ dài. Các kỹ thuật hiện có không thể giải quyết những vấn đề cơ bản này.

Trả lời những thách thức này, chúng tôi đề xuất LM-Infinite, một phương pháp đơn giản và hiệu quả để tăng cường khả năng mô hình hóa ngữ cảnh dài của Transformer LLM mà không cần cập nhật tham số. LM-Infinite bao gồm hai thành phần chính được thiết kế để giảm thiểu ba yếu tố trên. (1) một mặt nạ attention hình Λ và (2) một giới hạn trên cho khoảng cách attention. Thành phần đầu tiên buộc mô hình chỉ attention đến phần đầu của chuỗi và các token gần đây nhất trong một cửa sổ được định nghĩa trước, bỏ qua phần còn lại. Thành phần thứ hai giới hạn các giá trị khoảng cách tương đối ở mức tối đa mà mô hình đã thấy trong quá trình huấn luyện. Nó cũng có thể tùy chọn tái giới thiệu các token top-k ở giữa để đạt hiệu suất tốt hơn trong một số tác vụ downstream. LM-Infinite rất linh hoạt và áp dụng cho bất kỳ LLM nào sử dụng mã hóa vị trí tương đối và không yêu cầu fine-tuning.

Các thí nghiệm của chúng tôi đánh giá toàn diện LM-Infinite trên nhiều tác vụ và LLM. Trên ArXiv (bài báo học thuật) và OpenWebText2 (bài đăng Reddit), LM-Infinite tạo điều kiện cho tổng quát hóa zero-shot cho nhiều LLM đến văn bản lên đến 200M token, duy trì perplexity mô hình ngôn ngữ và chất lượng sinh. Không cần cập nhật tham số nào, LM-Infinite cải thiện điểm số so với mô hình gốc và baseline cắt ngắn trên các tác vụ downstream bao gồm Passkey Retrieval (Mohtashami and Jaggi, 2023) và Qasper (Dasigi et al., 2021), là hai benchmark đã được thiết lập để đánh giá ngữ cảnh dài. Chúng tôi quan sát thấy mức tăng 37.2% trên Passkey Retrieval và tăng 1.2% trên Qasper trong cài đặt zero-shot. LM-Infinite cũng mang lại cải thiện hiệu quả đáng kể: nó đạt được tăng tốc giải mã 2.7× và tiết kiệm bộ nhớ GPU 7.5× so với LLM gốc.

## 2 Bối cảnh và Công trình Liên quan

### 2.1 Mã hóa Vị trí Tương đối

Các mã hóa vị trí tuyệt đối truyền thống cung cấp thông tin vị trí tuyệt đối, thường với sự hỗ trợ của một chuỗi vector được gọi là embedding vị trí (Vaswani et al., 2017; Kenton and Toutanova, 2019; Ke et al., 2020). Tuy nhiên, chúng gặp khó khăn khi mô hình gặp phải các vị trí chưa thấy trong đầu vào dài hơn độ dài huấn luyện. Mã hóa vị trí tương đối nhằm giải quyết những hạn chế của các phương pháp mã hóa vị trí thế hệ trước và xem xét khoảng cách tương đối giữa các token thay vì vị trí tuyệt đối. Ví dụ bao gồm bias logit attention đã học trong T5 (Raffel et al., 2020), Transformer-XL (Dai et al., 2019), Skyformer (Chen et al., 2021), Sketching (Chen et al., 2022) và Sandwich (Chi et al., 2023), sự suy giảm attention tuyến tính cố định (Press et al., 2021), và xoay chuỗi query và key dựa trên khoảng cách như RoPE (Su et al., 2021; Li et al., 2023), CAPE (Likhomanenko et al., 2021) và XPos (Sun et al., 2022; Ding et al., 2023). Mặc dù có một số bằng chứng thực nghiệm đầy hứa hẹn, sự thất bại tổng quát hóa độ dài vẫn được quan sát rộng rãi khi áp dụng trực tiếp cho các mô hình ngôn ngữ lớn (Kaiokendev, 2023). Trong phần tiếp theo, chúng tôi thảo luận ngắn gọn về hai phương pháp mã hóa vị trí tương đối được sử dụng rộng rãi. Chúng đặt ra bối cảnh cần thiết cho cuộc thảo luận và thí nghiệm tiếp theo của chúng tôi.

**Rotary Position Embedding (RoPE; Su et al., 2021)** Nó xoay các vector key và query dựa trên vị trí trước khi tính tích vô hướng. Cụ thể, mỗi vector x (key hoặc query) được chia thành các cặp phần tử {(x0, x1), (x2, x3), ···}, với mỗi cặp được hiểu là một vector 2 chiều. RoPE sau đó xoay vector (xa, xa+1) của token i với góc θa,i = iωa, trong đó ωa là tốc độ xoay liên kết với cặp chiều (a, a + 1). Sau khi xoay, vector 2-D trở thành [cos iωa -sin iωa; sin iωa cos iωa][xi; xi+1]. Họ chỉ ra rằng tích vô hướng giữa query qi đã xoay và key kj đã xoay chỉ được xác định bởi qi, kj, và khoảng cách tương đối i − j của chúng. Chúng ta luôn có i ≥ j do mặt nạ attention nhân quả.

**AliBi (Press et al., 2021)** Nó bù trừ tất cả logit attention giữa các token i, j bằng một số hạng tuyến tính −m(i − j) và trở thành q⊤ikj − m(i − j). Để thực hiện điều này, các mã MPT-7B triển khai một ma trận bù trừ như một số hạng cộng trong logit attention.

### 2.2 Nỗ lực Hướng tới Tổng quát hóa Độ dài

Trước những thất bại tổng quát hóa được quan sát trong LLM, một giải pháp đơn giản là fine-tune LLM trên các chuỗi văn bản dài hơn (Chen et al., 2023a; Tworkowski et al., 2023; Tao et al., 2023; Kiyono et al., 2021; Anil et al., 2022). Những cách tiếp cận này không giải quyết các nguyên nhân cơ bản của sự thất bại tổng quát hóa độ dài và yêu cầu tài nguyên huấn luyện khổng lồ. Các giải pháp khác đề xuất cấp cho LLM quyền truy cập vào ngữ cảnh dài hơn mà không thực sự đọc chúng hoàn toàn (Zhou et al., 2023; Bueno et al., 2022; Mohtashami and Jaggi, 2023; Yang et al., 2023). Việc tăng cường LLM bằng bộ nhớ dựa trên retrieval (Wu et al., 2021; Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al., 2019; Kaiser et al., 2016; Yogatama et al., 2021) cũng làm cho LLM có thể áp dụng cho cơ sở dữ liệu lớn. Tuy nhiên, những thiết kế này thường cần fine-tuning và không tương thích trực tiếp với các LLM hiện có. Ngược lại, công trình của chúng tôi tạo điều kiện cho tổng quát hóa độ dài zero-shot. Một công trình tương tự khác (Ratner et al., 2023) tăng độ dài ngữ cảnh với các mẫu attention mà không cần huấn luyện thêm. Tuy nhiên, nó bị giới hạn trong cài đặt học trong ngữ cảnh.

## 3 Tại sao Transformer LLM Thất bại trong Tổng quát hóa đến Ngữ cảnh Dài?

Thông qua một loạt điều tra lý thuyết và thực nghiệm, phần này nhằm xác định các nguyên nhân tiềm ẩn gây ra sự thất bại tổng quát hóa độ dài của LLM hiện tại. Thảo luận của chúng tôi giả định các LLM dựa trên Transformer sử dụng mã hóa vị trí tương đối, vì thiết kế này được áp dụng rộng rãi trong các LLM ngày nay (Touvron et al., 2023b; Team, 2023). Chúng tôi sử dụng Llama-2 (Touvron et al., 2023b), được huấn luyện trước với các đoạn độ dài 4K, để điều tra. Trên các chuỗi dài hơn độ dài huấn luyện, chúng tôi sẽ chỉ ra rằng khoảng cách giữa các token chưa thấy, số lượng token được attention tăng lên, và thông tin vị trí được mã hóa ngầm của các token bắt đầu đều có thể làm cho một số đặc trưng tính toán nằm ngoài phân phối huấn luyện. Vì các mô hình sâu có thể nhạy cảm với sự thay đổi phân phối đầu vào, những yếu tố này cần được xử lý để LLM tổng quát hóa đến độ dài chưa thấy.

**Yếu tố 1: thách thức trong xử lý khoảng cách chưa thấy giữa các token** Với mã hóa vị trí tương đối, tác động của vị trí lên trọng số attention giữa hai token chỉ phụ thuộc vào khoảng cách tương đối của chúng. Khi độ dài chuỗi tăng quá dài, một số giá trị khoảng cách sẽ vượt quá những gì thấy trong quá trình huấn luyện. Chúng tôi đưa ra khẳng định lý thuyết không chính thức sau:

**Định lý 1. (Không chính thức)** Đối với một cơ chế attention sử dụng mã hóa vị trí tương đối, các logit attention phải bùng nổ đến vô cùng để phân biệt các khoảng cách chưa thấy trước đó khi độ dài chuỗi tăng.

Định lý chính thức và chứng minh có thể tìm thấy trong Phụ lục C. Chúng tôi cũng xác minh thực nghiệm điều này trên Llama-2 trên tập dữ liệu ArXiv được cắt xuống độ dài 8K. Chúng tôi trích xuất các logit attention của tất cả các đầu attention và logit attention tối đa của chúng trên các độ dài chuỗi khác nhau trong Hình 1(a). Nó hiển thị trung bình và phương sai giữa các đầu attention. Chúng ta thấy rằng các logit attention tăng đến các giá trị lớn hơn đáng kể khi độ dài chuỗi vượt quá độ dài huấn luyện 4K. Để giảm thiểu vấn đề này, chúng tôi đoán rằng có thể hữu ích khi giới hạn các giá trị khoảng cách tương đối ở mức tối đa mà mô hình đã thấy trong quá trình huấn luyện (tức là, một giới hạn trần khoảng cách). Tuy nhiên, như chúng ta sẽ thấy từ mệnh đề dưới đây, việc giải quyết bùng nổ logit dẫn đến một thách thức khác.

**Yếu tố 2: attention đến số lượng token chưa thấy** Trên các chuỗi dài hơn, các token ở vị trí sau phải phân phối trọng số attention trên một ngữ cảnh lớn hơn. Sau đó chúng tôi đưa ra khẳng định sau rằng, nếu các logit attention bị giới hạn nhưng số lượng token cần attention không bị hạn chế, nó có thể khiến entropy attention tăng vượt quá phạm vi huấn luyện:

**Mệnh đề 1.** Nếu các logit attention bị giới hạn, khi chuỗi trở nên dài hơn, entropy attention tăng đến vô cùng.

Một phát biểu chính thức cũng như chứng minh có thể tìm thấy trong Phụ lục D. Kết luận này được xác minh thêm bằng thực nghiệm bằng cách vẽ entropy attention so với độ dài ngữ cảnh trong Hình 1(b). Đường cong cho thấy entropy attention ngày càng tăng. Xu hướng này, mặc dù tăng theo logarithm, vẫn gây hại cho hiệu suất của LLM, như chúng tôi sẽ minh họa trong nghiên cứu loại bỏ trong §5.2 và Hình 5. Điều này gợi ý rằng chúng ta nên giới hạn kích thước ngữ cảnh attention để đảm bảo entropy attention duy trì trong các phạm vi đã thấy trong quá trình huấn luyện trước và tránh đầu ra bị suy thoái. Một attention cửa sổ đơn giản, trong đó mỗi token chỉ attention đến các token gần nhất trong một khoảng cách, có thể xử lý yếu tố 1 và 2. Điều này tương tự như mặt nạ attention khối chéo được sử dụng trong XPos (Sun et al., 2022) và Longformer (Beltagy et al., 2020). Tuy nhiên, như chúng tôi sẽ chỉ ra trong đoạn tiếp theo, điều này giới thiệu một yếu tố khác cũng có thể làm LLM thất bại.

**Yếu tố 3: các token bắt đầu chiếm một không gian đặc trưng riêng biệt** Có lẽ trái với trực giác:

**Quan sát 1.** Ngay cả khi không có embedding vị trí tuyệt đối rõ ràng, đầu ra attention của một vài token đầu tiên có thể chiếm một không gian biểu diễn riêng biệt so với các vị trí khác. Do đó, khi được truyền đến các lớp sau, những token bắt đầu này có các vector giá trị riêng biệt từ đầu ra lớp thấp hơn của chúng.

Điều này theo từ Định lý 1 trong Kazemnejad et al. (2023), chứng minh rằng các vị trí tuyệt đối có thể được mã hóa ngầm trong đầu ra của các token của một lớp attention duy nhất, ngay cả khi không có mã hóa vị trí. Trong cấu trúc của họ, tín hiệu của các token bắt đầu là mạnh nhất và dễ phân biệt nhất với các token khác. Vì mã hóa vị trí tương đối có tính biểu đạt chặt chẽ hơn cài đặt không có mã hóa vị trí trong Kazemnejad et al. (2023) (ví dụ, bằng cách để tất cả khoảng cách có cùng hàm attention), cùng kết luận áp dụng cho mã hóa vị trí tương đối.

Như một xác minh thực nghiệm, chúng tôi lấy các trạng thái ẩn đầu ra bởi lớp thứ hai của Llama-2 và vẽ một phép chiếu Phân tích Thành phần Chính (PCA) vào mặt phẳng 2-d trong Hình 1(c). Nhiều hình cho các lớp khác có thể tìm thấy trong §E. Các chấm tương ứng với 4096 token đầu tiên trong 32 chuỗi, với những chấm xanh tương ứng với các token đầu tiên và token đỏ là những token cuối. Hai vùng xanh ở trung tâm trên và phải dưới tương ứng với các token đầu tiên có độ tập trung cao (có vị trí khoảng 0 ∼ 25) và rất xa các token sau. Vùng dưới bên trái chứa các token chồng chéo còn lại trong một chuỗi (được phóng to vào hộp khác). Biểu đồ cho thấy rằng các biểu diễn vector của các token đầu tiên tập trung trên các vùng trong không gian đặc trưng riêng biệt với các token còn lại. Phát hiện mới này tiết lộ một khuyết điểm cơ bản của mẫu attention cửa sổ trượt, giới hạn attention chỉ đến các token gần đây nhất trong kích thước cửa sổ được định nghĩa trước, một baseline được áp dụng rộng rãi gần đây (Beltagy et al., 2020; Ding et al., 2023; Zaheer et al., 2020). Vì attention về bản chất là trung bình có trọng số trên các vector giá trị, attention cửa sổ trượt loại bỏ các token đầu tiên, ngăn đầu ra attention tiếp cận các vùng mà các vector giá trị của các token đầu tiên chiếm. Điều này buộc mô hình xử lý một vùng khác trong quá trình tính toán, giới thiệu các thách thức tổng quát hóa bổ sung. Như một giải pháp đơn giản cho vấn đề này, các token đầu tiên cần được giữ trong tính toán attention.

## 4 Đề xuất của chúng tôi: LM-Infinite

Được truyền cảm hứng bởi các phân tích và thông điệp rút ra trong phần trước, chúng tôi đề xuất LM-Infinite để đạt được tổng quát hóa độ dài zero-shot cho LLM. Tổng quan về LM-Infinite được minh họa trong Hình 2(a). Giải pháp đơn giản này bao gồm hai thành phần: mặt nạ attention hình Λ và giới hạn trần khoảng cách. Bên cạnh đó, việc tái giới thiệu các token top-k ở giữa là tùy chọn để tăng cường hiệu suất downstream.

**Mặt nạ attention hình Λ** Nó chứa hai khoảng attention: khoảng bắt đầu cho phép mỗi token attention đến nstarting token đầu tiên nếu chúng đến trước token hiện tại; khoảng kết thúc cho phép mỗi token attention đến Lpretrain token gần đây nhất. Lpretrain là độ dài tối đa trong quá trình huấn luyện. Các token khác bị bỏ qua. Trong các nghiên cứu loại bỏ trong §A, chúng tôi thấy rằng việc chọn nstarting ∈ [5,100] thường đạt được hiệu suất tốt như nhau. Lưu ý rằng nstarting = 0 (tức là, chỉ attention đến các token gần đây nhất) làm tổn hại đáng kể hiệu suất. Điều này giải quyết Yếu tố 2 và 3 trong §3 bằng cách vừa giới hạn số lượng token dưới attention và đảm bảo một vài token bắt đầu được attention.

**Giới hạn trần khoảng cách** LM-Infinite tiếp tục giới hạn "khoảng cách hiệu quả" đến Lpretrain. Điều này chỉ ảnh hưởng đến một vài token bắt đầu khi được attention bởi các token ở vị trí sau. Cụ thể, trong mã hóa vị trí tương đối, logit attention gốc là w(q,k,d), trong đó d là khoảng cách giữa hai token. Bây giờ chúng tôi sửa đổi nó thành w(q,k,d') trong đó d' = min(d, Lpretrain). Hình 2(a) hiển thị một ví dụ minh họa trong đó giới hạn trần khoảng cách là Lpretrain = 2. Điều này giải quyết Yếu tố 1 trong §3 bằng cách giới hạn giá trị khoảng cách trong tính toán attention.

**Tùy chọn attention đến các token top-k ở giữa** LM-Infinite có thể tùy chọn attention đến k token ở giữa với logit attention lớn nhất. Điều này đặc biệt hữu ích trong các tác vụ downstream nơi thông tin trong các token giữa quan trọng (§5.3). Ở đây k token được chọn độc lập cho mỗi đầu attention trong các lớp cao hơn lớp thứ h, và có khoảng cách attention d = 1/2 Lpre-train. Các siêu tham số này được chọn dựa trên tập validation Passkey Retrieval đã giữ lại, nơi chúng tôi đặt k = 5 và h = 5, với nhiều chi tiết hơn trong Phụ lục A. Việc lựa chọn k và h của chúng tôi không phụ thuộc vào điều chỉnh cụ thể cho tác vụ, và trong các thí nghiệm của chúng tôi, chúng tôi áp dụng cùng bộ siêu tham số này trong tất cả các tác vụ downstream khác và đạt được cải thiện nhất quán. Những token trung gian này không làm tổn hại hiệu suất. Thay vào đó, trong đánh giá các tác vụ downstream trong §5.3, các token trung gian hữu ích hơn và việc attention có chọn lọc đến các token top-k mang lại cải thiện hiệu suất đáng kể với ít tác động đến hiệu quả. Tuy nhiên, đối với sinh và suy luận LLM, chúng tôi thấy các token trung gian không cần thiết để attention để LM-Infinite đạt được perplexity hoặc chất lượng sinh tốt.

Mặt nạ hình Λ của LM-Infinite về mặt khái niệm tương tự như các mẫu attention được dẫn xuất từ phép suy đoán (Beltagy et al., 2020; Ding et al., 2023; Zaheer et al., 2020). Tuy nhiên, chúng tôi chỉ ra chính thức trong §3 Yếu tố 3 rằng những cách tiếp cận trước đây này về mặt lý thuyết không thể tổng quát hóa đến độ dài chưa thấy mà yêu cầu cập nhật tham số. Hạn chế vốn có này thúc đẩy hai thành phần khác trong LM-Infinite để đạt được tổng quát hóa độ dài zero-shot.

**Chi tiết triển khai** LM-Infinite có thể áp dụng trong tất cả các mô hình Transformer với mã hóa vị trí tương đối. Người ta chỉ cần thay thế hàm attention trong mỗi lớp Transformer bằng LM-Infinite mà không cần cập nhật tham số nào. Mặt nạ attention hình Λ tương đối đơn giản để triển khai. Trong RoPE, các logit attention trong khoảng attention kết thúc theo tính toán gốc. Trong khoảng attention bắt đầu (loại trừ sự chồng chéo với khoảng kết thúc), chúng tôi giữ tất cả vector k không xoay và xoay tất cả vector q đến khoảng cách cố định Lpretrain. Sau đó các logit trong hai khoảng có thể được kết hợp. Việc tăng cường AliBi bằng LM-Infinite cũng đơn giản. Chúng tôi chỉ cần cắt ma trận bù trừ với giá trị tối thiểu −|mLpretrain| và áp dụng mặt nạ attention hình Λ.

**Thảo luận.** Trong Hình 2(b), chúng tôi hiển thị một mô hình khái niệm về cách mã hóa vị trí tương đối hoạt động. Mô hình khái niệm này phản ánh các lựa chọn thiết kế của LM-Infinite. Trong mô hình khái niệm này, một ngữ cảnh dài có thể được phân chia thô thành 3 phần: Các token bắt đầu mã hóa thông tin vị trí tuyệt đối mạnh (Yếu tố 3). Như được giải thích trong §3, chúng thiết yếu để attention vì các đặc trưng của chúng chiếm một vùng riêng biệt trong không gian đặc trưng. Vì attention về bản chất là trung bình có trọng số trên các vector vi, không có một vài token bắt đầu, đầu ra attention không thể tiếp cận các vùng mà các vector vi của các token đầu tiên chiếm. Các token cuối cung cấp chủ yếu vị trí tương đối của chúng với các token cuối cùng. Tầm quan trọng của chúng có lẽ phát sinh từ "bias gần đây" (Peysakhovich and Lerer, 2023) được học bởi LLM trong quá trình huấn luyện trước. Các token giữa mã hóa thông tin ít nhạy cảm với vị trí. Như được phân tích trong Yếu tố 2, việc bao gồm quá nhiều token trung gian làm hại nhiều hơn là tốt cho tổng quát hóa độ dài.

## 5 Đánh giá

Chúng tôi đánh giá LM-Infinite với LLaMA-7B (Touvron et al., 2023a), Llama-2-7b (Touvron et al., 2023b), MPT-7B (Team, 2023), và GPT-J-6B (Wang and Komatsuzaki, 2021). LLaMA-7B và GPT-J-6B được huấn luyện trước với độ dài 2K và các mô hình khác được huấn luyện trước với độ dài 4K. LLaMA, Llama-2, và GPT-J sử dụng mã hóa RoPE, và MPT-7B sử dụng mã hóa Alibi. MPT-7B-Storywriter (được fine-tune trên các chuỗi dài) được sử dụng như một trong những baseline.

### 5.1 Mô hình Ngôn ngữ với Ngữ cảnh Cực kỳ Dài

Chúng tôi sử dụng các corpus ArXiv và OpenWebText2 từ tập dữ liệu Pile (Gao et al., 2020), chứa các bài báo preprint từ ArXiv và các bài nộp Reddit, tương ứng. Chúng tôi đánh giá bằng negative log-likelihood (NLL) và perplexity (số mũ của NLL). Hình 3 vẽ các đường cong NLL trên tập dữ liệu ArXiv. Ở đây, chúng tôi chia nhỏ hiệu suất perplexity của các mô hình theo vị trí để đường cong hiển thị NLL mà mô hình đạt được xung quanh vị trí cụ thể đó, được tính trung bình trên tất cả các chuỗi được đánh giá. Llama-2 xuất ra xác suất NaN trên các chuỗi dài hơn một chút so với 10K, do đó đường cong ngắn hơn. Tất cả các mô hình vanilla hết bộ nhớ ở độ dài ~32K.¹ NLL của các baseline nhanh chóng bùng nổ khi các chuỗi được thử nghiệm dài hơn những gì chúng huấn luyện. Với LM-Infinite, tất cả các mô hình có thể tổng quát hóa đến các chuỗi dài hơn đáng kể so với độ dài chúng được huấn luyện, duy trì hiệu suất NLL. Điều này xác nhận phân tích yếu tố thất bại độ dài của chúng tôi trong §1. Các đuôi dài hơn của đường cong có phương sai lớn hơn do ít tài liệu có độ dài đó. Trong Hình 4, chúng tôi tiếp tục đánh giá LM-Infinite + Llama-2 trên một chuỗi 200M token, được xây dựng bằng cách lấy mẫu với thay thế từ tập dữ liệu ArXiv và nối tất cả dữ liệu. LM-Infinite cho thấy khả năng duy trì mức log-perplexity thấp ổn định trên độ dài cực kỳ dài.

Bảng 1 tóm tắt hiệu suất perplexity tại một vài độ dài mốc (2K, 4K, 8K, 16K, và 32K) trên ArXiv và OpenWebText2, cho thấy xu hướng tương tự. OpenWebText2 có rất ít instance dữ liệu trên độ dài 32K, vì vậy chúng tôi bỏ qua cột đó. Với LM-Infinite, tất cả các mô hình có thể tổng quát hóa đến độ dài chưa thấy, và LM-Infinite đạt perplexity tốt nhất trong 7 trên 9 trường hợp. Trên LLaMA + LM-Infinite, perplexity giảm khi độ dài tăng và vị trí trở nên lớn hơn. Đáng ngạc nhiên, không cần cập nhật tham số nào, LM-Infinite vượt trội hơn nhiều baseline mạnh được huấn luyện trên các đoạn văn bản dài hơn đáng kể. Như một so sánh trực tiếp, MPT-7B+LM-Infinite chỉ đạt hiệu suất hơi kém hơn so với đối tác được fine-tune của nó, MPT-7B-Storywriter. Điều này xác nhận rằng LM-Infinite là một thay thế đầy hứa hẹn cho fine-tuning tiêu tốn tài nguyên.

### 5.2 Nghiên cứu Loại bỏ

Hình 5 cung cấp một nghiên cứu loại bỏ với mô hình LLaMA trên tập dữ liệu ArXiv về lý do tại sao cả hai thành phần trong LM-Infinite đều thiết yếu để duy trì chức năng LLM trên độ dài 8K. Chúng tôi so sánh LM-Infinite với các biến thể của nó để hiển thị hiệu quả của thiết kế và cũng để xác nhận các yếu tố trong §3. Trong tất cả các đường cong, chỉ LM-Infinite có log-perplexity tương đối ổn định, có nghĩa là các thành phần trong LM-Infinite đều thiết yếu cho tổng quát hóa độ dài thành công. Mô hình LLM vanilla (đường cong "vanilla") thất bại ngay lập tức với NLL bùng nổ. Nếu chúng tôi chỉ áp dụng mặt nạ hình Λ (đường cong "Λ") và không giới hạn khoảng cách giữa các token (Yếu tố 1), NLL vẫn bùng nổ ngay sau độ dài huấn luyện trước. Đường cong "ceiling" chỉ áp dụng kỹ thuật giới hạn trần khoảng cách nhưng không áp dụng mặt nạ hình Λ để giới hạn số lượng token được attention. Hiệu suất vẫn suy thoái (được chứng minh bằng NLL ngày càng tăng). Điều này xác nhận sự tồn tại của Yếu tố 2, quá nhiều token, vẫn gây hại cho hiệu suất của LLM. Đường cong "window" hiển thị một baseline với mẫu attention cửa sổ trượt, chỉ attention đến các token gần đây nhất trong cửa sổ trượt mà không thay đổi văn bản đầu vào. Nó tạo ra giá trị NLL tệ thứ hai, cho thấy sự suy giảm hiệu suất và độ trôi chảy đáng kể. Điều này xác nhận phân tích lý thuyết của chúng tôi về yếu tố 3. Do hiệu suất tệ hơn rõ rệt, chúng tôi loại trừ nó khỏi các đánh giá khác.

Một baseline tương tự khác với "window" là baseline cắt ngắn, loại bỏ hoàn toàn các token dư thừa khi ngữ cảnh dài hơn độ dài huấn luyện trước của mô hình, chỉ giữ những token gần đây nhất. Quá trình cắt ngắn này xảy ra trước khi quá trình forward bắt đầu và về cơ bản loại bỏ văn bản bị cắt khỏi đầu vào của mô hình mà không thay đổi cơ chế attention. Chúng tôi so sánh baseline này ở hai nơi trong bài báo. Trong §5.3 và Bảng 2, LM-infinite vượt trội hơn baseline này trên các tác vụ downstream. Trong Phần 5.4 và Hình 6, LM-infinite đạt được sự cân bằng tốt hơn giữa độ phức tạp tính toán và chất lượng sinh so với baseline này.

### 5.3 Đánh giá Downstream

Vì LLM thường được triển khai cho các tác vụ downstream, chúng tôi đánh giá cách LM-Infinite hoạt động trên hai tác vụ đầu vào dài trong cài đặt zero-shot: Passkey Retrieval (Mohtashami and Jaggi, 2023) và Qapser (Dasigi et al., 2021). Passkey Retrieval chôn một passkey ở vị trí ngẫu nhiên trong văn bản phân tâm dài và cuối cùng hỏi passkey là gì. Qasper là một tập dữ liệu hỏi-đáp trên các bài báo khoa học với tổng cộng 1.5K cặp câu hỏi-trả lời thử nghiệm. Chúng tôi đánh giá Llama-2-7b-chat, vì việc điều chỉnh hướng dẫn của nó cho phép khả năng giải quyết tác vụ tốt (Bai et al., 2023), với các token top-5 ở giữa được kích hoạt trên lớp cao hơn lớp thứ 5 (xem §4 để định nghĩa và Phụ lục A để lựa chọn siêu tham số). Kết quả được liệt kê trong Bảng 2. LM-Infinite nhất quán vượt trội hơn các baseline trên cả hai tác vụ, với mức tăng 37.2 phần trăm trên Passkey Retrieval và tăng 1.2 phần trăm trên tác vụ Qasper. Passkey retrieval định vị thông tin hữu ích đồng đều trong một chuỗi, vì vậy hiệu suất của baseline bị cắt ngắn phần lớn phụ thuộc vào việc liệu phần còn lại có bao phủ passkey hay không. Trên Qasper, attention top-k là cần thiết để đạt hiệu suất tốt, cho thấy rằng thông tin quan trọng tương tự ở giữa cần được attention. Điều này gợi ý rằng nó có thể cải thiện hiệu suất tác vụ downstream trên đầu vào dài mà không cần fine-tuning trong khi mô hình vanilla ngay lập tức thất bại.

### 5.4 Chất lượng Sinh

Chúng tôi tiếp tục đánh giá chất lượng sinh của LM-Infinite trên các tập thử nghiệm ArXiv và OpenWebText2, với BLEU (Papineni et al., 2002) và ROUGE (Lin, 2004) (ROUGE-L). Chúng tôi để LLM sinh 100 token sau mỗi độ dài mốc và sử dụng 100 token tiếp theo trong văn bản gốc làm tham chiếu. Vì việc sinh tốn thời gian, chúng tôi lấy mẫu 100 chuỗi dài để đánh giá cho mỗi tập dữ liệu. Kết quả được tóm tắt trong Bảng 3. Xu hướng tương tự như phần cuối: không cần cập nhật tham số, LM-Infinite thành công cho phép LLM duy trì chất lượng sinh của chúng trong khi sinh các chuỗi dài hơn so với huấn luyện, tương đương với các baseline được fine-tune như MPT-7B-SW. Kết quả sinh từ các LLM vanilla kém và chứa hầu hết văn bản vô nghĩa, dẫn đến nhiều điểm số gần bằng không. Đối với một số điểm BLEU, nó cho ra sự chồng chéo {2,3,4}-gram bằng không với văn bản tham chiếu. Vì BLEU là trung bình nhân có trọng số trên độ chính xác {1,2,3,4}-gram, điểm BLEU cuối cùng cho những cột đó là 0. Phụ lục Bảng 8 trình bày một số ví dụ đầu ra sinh có thể cung cấp hình ảnh tốt về chất lượng sinh. Chúng tôi cũng đánh giá hiệu quả trong Phụ lục G: với các chuỗi dài 32K, LM-Infinite đạt được tăng tốc giải mã 2.7× và tiết kiệm bộ nhớ GPU 7.5×.

Một vài ví dụ sinh được hiển thị trong Phụ lục H. Chúng tôi cũng so sánh LM-Infinite với baseline dựa trên cắt ngắn đơn giản bằng cách liên tục cắt ngắn các ngữ cảnh dư thừa. Tuy nhiên, khi độ dài sinh tăng, việc cắt ngắn thường xuyên và mã hóa lại các ngữ cảnh mới là cần thiết. Cửa sổ cắt ngắn càng lớn, càng giữ được nhiều ngữ cảnh, nhưng chi phí tính toán càng lớn. Chúng tôi để các mô hình sinh 10k token trên ArXiv. Trong Hình 6, rõ ràng là LM-Infinite đạt được sự cân bằng chất lượng-hiệu quả tốt hơn đáng kể. Với tính toán tương tự, LM-Infinite vượt trội hơn baseline khoảng 5 BLEU. Để đạt được BLEU tương tự, LM-Infinite chỉ phải chịu <25% chi phí tính toán so với baseline cắt ngắn.

## 6 Kết luận và Công việc Tương lai

Công trình này đề xuất một phương pháp tổng quát hóa độ dài zero-shot cho nhiều LLM off-the-shelf khác nhau mà không cần cập nhật tham số. Thông qua phân tích lý thuyết và điều tra thực nghiệm, công trình này xác định ba yếu tố chính góp phần vào sự thất bại tổng quát hóa độ dài này. Phân tích lý thuyết của chúng tôi tiếp tục tiết lộ tại sao việc cắt ngắn cửa sổ attention và mã hóa vị trí tương đối không đủ để giải quyết chúng. Giải pháp của chúng tôi, LM-Infinite, là một phương pháp đơn giản và hiệu quả để tăng cường khả năng xử lý ngữ cảnh dài của LLM. Nó cho phép các LLM được huấn luyện trước với các đoạn 2K hoặc 4K token tổng quát hóa lên đến đầu vào độ dài 200M trong khi duy trì perplexity. Nó cũng cải thiện hiệu suất trên các tác vụ downstream như Passkey Retrieval và Qasper trong cài đặt zero-shot. Nó mang lại cải thiện hiệu quả đáng kể: tăng tốc giải mã 2.7× và tiết kiệm bộ nhớ 7.5× so với mô hình gốc. Hiệu quả tính toán và dễ sử dụng của LM-Infinite cho phép các nhà nghiên cứu không có tài nguyên tính toán khổng lồ sử dụng LLM trên các chuỗi dài. Công việc tương lai có thể điều tra xem những kỹ thuật này có cho phép huấn luyện trước và fine-tuning LLM hiệu quả và hiệu suất hơn hay không. Một hướng khác là áp dụng LM-Infinite cho các ứng dụng như lý luận dài, đối thoại dài, sinh tăng cường retrieval, hoặc sinh văn học dài.

## Hạn chế

Công trình này đánh giá một loạt LLM miền mở rộng. Tuy nhiên, không có quyền truy cập vào mã nguồn của các LLM độc quyền như ChatGPT, phương pháp đề xuất không thể được đánh giá trên chúng. Hơn nữa, do tài nguyên tính toán và thời gian hạn chế, phương pháp đề xuất chưa được đánh giá trên văn bản với độ dài thậm chí lớn hơn, như 1G. Mô hình được thiết kế trên các mô hình Transformer mã hóa vị trí tương đối, là xương sống chính cho hầu hết các LLM hiện đại. Câu hỏi về cách LM-Infinite có thể cho phép fine-tuning hoặc huấn luyện trước hiệu quả hơn cũng có thể được khám phá trong công việc tương lai.

## Lời cảm ơn

Nghiên cứu này được hỗ trợ một phần bởi Chương trình KAIROS DARPA Hoa Kỳ số FA8750-19-2-1004, và Chương trình INCAS DARPA số HR001121C0165. Các quan điểm và kết luận có trong đây là của các tác giả và không nên được hiểu là nhất thiết đại diện cho các chính sách chính thức, được thể hiện rõ ràng hoặc ngụ ý, của DARPA, hoặc Chính phủ Hoa Kỳ. Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối bản in cho mục đích chính phủ, bất chấp bất kỳ chú thích bản quyền nào trong đó.

## Tài liệu Tham khảo

[Các tài liệu tham khảo được giữ nguyên như trong bản gốc tiếng Anh]
