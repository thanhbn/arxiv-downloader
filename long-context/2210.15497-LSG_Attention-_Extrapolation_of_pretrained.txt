# 2210.15497.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2210.15497.pdf
# File size: 766805 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LSG Attention: Extrapolation of pretrained
Transformers to long sequences
Charles Condevaux
CHROME
University of Nîmes, France
charles.condevaux@unimes.frSébastien Harispe
EuroMov Digital Health in Motion,
Univ Montpellier, IMT Mines Ales, France
sebastien.harispe@mines-ales.fr
Abstract
Transformer models achieve state-of-the-art
performance on a wide range of NLP tasks.
They however suffer from a prohibitive limita-
tion due to the self-attention mechanism, induc-
ingO(n2)complexity with regard to sequence
length. To answer this limitation we introduce
the LSG architecture which relies on Local,
Sparse and Global attention. We show that
LSG attention is fast, efﬁcient and competitive
in classiﬁcation and summarization tasks on
long documents. Interestingly, it can also be
used to adapt existing pretrained models to ef-
ﬁciently extrapolate to longer sequences with
no additional training. Along with the intro-
duction of the LSG attention mechanism, we
propose tools to train new models and adapt
existing ones based on this mechanism.
1 Introduction
Transformer models [ 1] are nowadays state-of-the-art
in numerous domains, and in particular in NLP where
they are used in general language models, and to suc-
cessfully tackle several speciﬁc tasks such as document
summarization, machine translation and speech process-
ing to cite a few [ 2,3]. The cornerstone of Transformer
models is the Attention mechanism used to iteratively
build complex context-dependent representations of se-
quence elements, e.g. tokens, by dynamically aggre-
gating prior representations of these elements. Using
self-attention, a popular Attention ﬂavour, this is made
by computing full attention scores deﬁning how each
prior element representation will contribute to building
the new representation of an element. Considering a
sequence of nelements, the computation of the atten-
tion scores is therefore of complexity O(n2)which is
prohibitive when large sequences have to be processed.
Furthermore, in the current context where a large num-
ber of models based on full attention have been trained
on various datasets and tasks, we are also interested
in extrapolating those models to longer sequences bysimply substituting full attention by new attention mech-
anisms post training. Common pretrained models (e.g
BERT, RoBERTa) are indeed known to underperform
when extrapolated to sequences of length exceeding the
512 tokens considered during training. This is due to the
nature of the attention mechanism which largely impacts
extrapolation capabilities: full attention usually fails to
extrapolate, even considering post hoc adaptations, e.g.
adding constants in the score matrix [ 4], using a relative
positional embedding [ 5] or duplicating the positional
embedding [ 6]. Deﬁning new attention mechanisms
that can efﬁciently substitute full attention in pretrained
models that are not originally capable of handling long
sequences would avoid the costs induced by training
large language models from scratch.
The main contributions of this paper are:
1. LSG (Local Sparse Global) attention, an efﬁcient
O(n)approach to approximate self-attention for pro-
cessing long sequences, is introduced.1
2. Results demonstrating that LSG is fast, efﬁcient
and competitive on classiﬁcation and summarization
tasks applied to long documents are presented. It is
also shown that LSG can adapt and extrapolate existing
pretrained models not based on LSG, with minimal to
no additional training.
3. A procedure and companion tools are proposed to
convert various existing models and checkpoints (BERT,
RoBERTa, DistilBERT, BART) from HuggingFace to
their LSG variant.2
Compared to several contributions aiming at reducing
the complexity of self-attention introduced hereafter, a
speciﬁc focus is given in our work on the extrapolation
of existing Transformer models, i.e. reuse, to longer
sequences.
2 Related works
Several contributions have been devoted to the optimiza-
tion of the Attention mechanism. Four categories of
1https://huggingface.co/ccdv
2https://github.com/ccdv-ai/convert_checkpoint_to_lsg
1arXiv:2210.15497v1  [cs.CL]  13 Oct 2022

--- PAGE 2 ---
Condevaux et al.
approaches can be distinguished in the literature: (i) re-
current models such as Transformers-XL [ 7] and Com-
pressive Transformers [ 8] which maintain a memory
of past activation at each layer to preserve long-range
contextual information; (ii) models based on factoriza-
tion or kernels aiming at compressing attention score
matrices, such as Linformer [ 9] or Performer [ 10]; (iii)
models based on clustering such as Reformer [ 11] that
dynamically deﬁne eligible attention patterns (i.e. where
attention may be made); and (iv) models based on ﬁxed
or adaptative attention patterns, e.g. Longformer [ 6] or
Big Bird [12].
Recurrent approaches iteratively process the sequence
by maintaining a memory to enable long-range depen-
dencies. They generally suffer limitations induced by
speciﬁc, slow, and difﬁcult to implement forward and
back propagation procedures. Alternatively, one of the
main line of study for reducing the complexity of Atten-
tion is thus to perform sparsity by limiting the number
of elements on which new representations will be based,
i.e. reducing the number of elements with non-null
attention scores. This approach is motivated by the ob-
servation of global or data-dependent positional patterns
of non-null attention scores depending on the task [ 13].
The sparsity of attention scores in the traditional Atten-
tion mechanism is indeed documented in the literature.
It has for instance been shown that in practice, full atten-
tion tends to overweight close elements in average, in
particular for MLM, machine translation, and seq-to-seq
tasks in general [ 14]. Moreover, according to analyses
on the use of multi-head full attention on speciﬁc tasks,
e.g. machine translation, numerous heads learn simi-
lar simple patterns [ 15]. Such redundant patterns may
be hardcoded implementing ﬁxed-positional patterns,
eventually in a task-dependent manner.
Two main approaches are discussed in the literature
for implementing sparsity: ﬁxed or adaptative patterns
based on whether attention scores are computed con-
sidering (1) predeﬁned ﬁxed elements based on their
location in the sequence, or (2) elements selected from
a given procedure. As an example, [ 16] have shown that
ﬁxedO(n)convolutions can perform competitively on
machine translation. Longformer proposes an alterna-
tiveO(n)approach based on sliding and global patterns
[6]. In the context of image, audio, and text processing,
[13] propose sparse Transformer, an O(npn)model
based on sparse factorization of the attention matrix rely-
ing on speciﬁc 2D factorized attention schemes. Those
approaches however prevent the use of task-dependent
dynamic patterns. Considering adaptative patterns, [ 16]
also introduced dynamic convolutions as an O(n)com-
plexity substitute to self-attention. Kernels deﬁning the
importance of context elements are speciﬁed at infer-
ence time rather than ﬁxed after training. Another ex-
ample is Reformer [ 11], anO(nlogn )approach based
on locality-sensitive hashing (LSH) based on random
projections.
In a transverse manner, several authors, explicitly or
implicitly motivated by the compositional nature of lan-
guage have studied structured approaches in which sub-
sequences (i.e. blocks) are processed independently
and then aggregated. This aims at implementing a lo-cal or global dynamic memory for considering close to
long-range dependencies. [ 17] introduce a blockwise
approach to reduce the quadratic complexity induced by
large sequences in encoder-decoder architectures. [ 18]
propose a chunkwise attention in which attention is per-
formed in a blockwise manner adaptively splitting the
sequence into small chunks over which soft attention is
computed. This idea is also used in Transformer-XL [ 7].
[19] propose a masked block self-attention mechanism
in which the entire sequence is divided into blocks, to
further 1) apply self-attention intra-block for modeling
local contexts, to further 2) apply self-attention inter-
block for capturing long-range dependencies. Such an
approach enables implementing some forms of connec-
tivity between all positions over several steps without
being restricted by full attention limitations. This can
also be achieved by factorization techniques, e.g. [ 13].
More recently authors have proposed global attention
mechanisms encoding information related to blocks on
which attention is based [20, 21, 22].
This paper presents the LSG (Local, Sparse and Global)
attention based on block local attention to capture local
context, sparse attention to capture extended context and
global attention to improve information ﬂow. Contrary
to prior work mostly focusing on deﬁning new models,
the proposed LSG Attention mechanism is model agnos-
tic and aims at facilitating adapting existing (pretrained)
models for them to be used on long sequences.
3 LSG Attention
LSG attention relies on two main points. It is assumed
that locally, a token needs to capture low level informa-
tion thus dense attention is prefered. On the other hand,
as the context grows, higher level information is sufﬁ-
cient. This translates into the need for connections to a
limited number of tokens following speciﬁc selection
and computation rules. The LSG approach relies on 3
components: block local attention to capture local con-
text, sparse attention to capture extended context and
global attention to improve information ﬂow. A compar-
ison to Big Bird and Longformer attention patterns is
shown in Figure 1.
LSG Big Bird Longformer
Figure 1: Attention patterns
3.1 Local Attention
Longformer depends on a ﬁxed length sliding window
to perform local attention. However this approach is
difﬁcult to optimize and must rely on a custom CUDA
kernel to be computationally efﬁcient. To improve over-
all training and inference speed, we take advantage of a
block-based process similar to Big Bird. The sequence
2

--- PAGE 3 ---
Condevaux et al.
is split intonbnon-overlapping chunks of size bt. For
a given block, each token attends to the tokens inside
the block, as well as to those in the previous and next
blocks. In this conﬁguration, the local attention win-
dow is asymmetrical since a token can connect up to
2bt 1tokens on the left or on the right.
3.2 Sparse Attention
Sparse connections are used to expand the local con-
text by selecting an additional set of tokens following
a set of rules. These tokens can be directly selected
based on a speciﬁc metric or using some computation
such as a pooling method. In the proposed approach,
each attention head can process different sparse tokens
independently. Sparse attention also relies on a block
structure where the sparse selection is done inside each
block. Five alternative criteria can be used in LSG.
Head-wise strided Inspired by the Hepos model [ 23],
a ﬁxed selection pattern is deﬁned. Each attention head
will attend to a set of tokens following a speciﬁc stride
deﬁned as the sparsify factor f. Figure 2 shows the
selection pattern.
Head 1
Head 2Block
Sequence
Figure 2: Head-wise strided selection with a stride of 2.
Head-wise block strided This selection pattern is
similar to the previous one but selects consecutive to-
kens instead. Figure 2 shows the selection pattern.
Head 1
Head 2Block
Sequence
Figure 3: Block strided selection with a stride of 2.
Average pooling A simple way to reduce sequence
length. After chunking the sequence into blocks, sparse
tokens are computed using average pooling. For a block
of sizebtand a sparsify factor f, we pool inside each
block with a window of fand a stride of fto produce
bt=ftokens.
Max norm The objective of a norm-based approach
is to select tokens that are most likely highly weighted
in the score matrix. Finding those keys efﬁciently is
difﬁcult in practice so we use a simple and deterministic
metric. For a query and a key q;k2Rd, we can write:
qk>= cos()kqkkkkIn this situation cos()sign is unknown. However, if it
is positive andkkkis high, the key will likely dominate
the softmax regardless of the query. After chunking the
sequence into blocks, we select inside each block and
each headbt=ftokens with the highest key norm.
LSH Clustering This approach is a non deterministic
one since it relies on the LSH algorithm [24]. For each
block,bt=fclusters are built using a single round LSH.
To getc=bt=fhashes and for an input x2Rd, a
random matrix R2Rdc=2is generated, such that
h(x) = arg max([ xR; xR])
with[a;b]the concatenation of two vectors. Using the
key matrix as input, each token inside the block gets
a cluster index from h(x). Tokens inside a cluster are
averaged.
Computation To reduce the computational cost, the
attention pattern is designed to compute each connection
once. For this, the local and sparse tokens are selected
such that there is no overlap between them during atten-
tion computation. Each query is connected to 3 local
blocks and 2 sparse blocks of keys. The maximum con-
text length (distance between two keys) is then equal
to3bt+ 2btf. The concatenation of local and
sparse keys is shown Figure 4. For causal attention, the
third local block and the second sparse block can be
ignored during computation.
abSparse context Sparse context
abSequence
Local context
Figure 4: Local and sparse contexts with a block size of
2 and a sparsity factor of 4. Queries aandbwill attend
to 6 local keys and 4 sparse keys.
3.3 Global Attention
Global tokens improve the ﬂow of information inside the
model. They attend to every tokens across the sequence
and all tokens attend to them. Rather than picking a
subset of tokens and deﬁning them as global, they are
prepended to the sequence and trained using their own
embedding matrix, thus their number is an additional
hyperparameter. When a model is converted to its LSG
version, the ﬁrst global token is initialized as the sum
of the [CLS] (or <s>) token and the ﬁrst position from
the positional embedding. The other global tokens are
initialized as the sum of [MASK] (or <mask> ) token
and the other positions from the positional embedding.
3.4 Positional Embedding
It is necessary to modify the positional embedding ma-
trix to reuse existing models to process long sequences.
3

--- PAGE 4 ---
Condevaux et al.
Similarly to Longformer’s authors [ 6], instead of ran-
domly initializing the new positions, the original matrix
is duplicated and concatenated until the desired max
sequence length is reached.
4 Experiments
The LSG model is implemented in PyTorch and aims
at performing model extrapolation by replacing full at-
tention by the LSG attention in various architectures of
the HuggingFace library. In the experiments, the ofﬁcial
RoBERTa-base checkpoint for classiﬁcation tasks and
BART-base checkpoint for summarization tasks are ex-
trapolated using LSG attention. All metrics are reported
for the test set except in the case where only the valida-
tion set is available. We use a batch size of 32, a linear
decaying learning rate, a dropout rate of 0.10 and Adam
(0.9, 0.999) optimizer [ 25] for classiﬁcation and summa-
rization experiments. An experiment comparing several
attention approximations to extrapolate RoBERTa in an
MLM task is ﬁrst discussed as it is used to limit the
number of tested alternatives, and therefore reduce the
cost of the proposed evaluations. All experiments are
run on NVIDIA Quadro RTX 8000 48Gb GPUs.
4.1 RoBERTa extrapolation on MLM
A simple test on a MLM task is performed to test for
the ability of an attention mechanism to extrapolate a
model to longer sequences without additional training.
To do so, a RoBERTa-base model is considered and two
experiments are conducted. First, the full attention is
substituted by different kinds of attention (kernel, factor-
ization, local, ﬁxed pattern) and each model is evaluated
on sequences of the same length as those considered
during RoBERTa initial training (512 tokens). For the
second experiment, their ability to extrapolate to 4,096
tokens sequences without additional training is tested
(the positional embedding being duplicated 8 times).
A random sample from Wikipedia + BookCorpus +
CC_News is used; BPC and MLM accuracy are reported
in Table 1. RoBERTa’s author report a 1.880 BPC loss;
we obtain a comparable loss of 1.881 on this random
sample.
Only Longformer, Big Bird and LSG attention manage
to obtain competing BPC while processing sequences of
the same length as those considered during the original
RoBERTa training. Other approaches such as Linformer,
Performer or Reformer requires additional MLM ﬁne-
tuning to leverage an existing checkpoint. It can be seen
that RoBERTa fails to extrapolate to longer sequences
(+2.454 BPC), which highlights that full attention is not
suitable for extrapolation. Longformer and Big Bird
attention results show the capability of these approaches
to perform some form of extrapolation. Therefore, we
restrict our comparison to these two approaches in order
to reduce the costs of our experimentation.
4.2 Classiﬁcation Tasks
To evaluate the relevance of LSG, we compare our ap-
proach to common model architectures able to processlong sequences with a similar number of parameters.
Experiments are performed on Longformer [ 6], Big
Bird [12] and on all sparse attention types with a block
size of 128 and a sparsify factor of 4. All models are
ﬁne-tuned on IMDb, ArXiv, Patent, Scotus, EcthrA and
EcthrB datasets presented below.
4.2.1 Datasets
Datasets are available on the HuggingFace hub, see
Appendix D, e.g. detailed statistics in Table 15.
IMDb [29] binary sentiment analysis classiﬁcation
task from movie reviews.
ArXiv [30] set of documents from ArXiv where the
objective is to predict a topic from 11 available classes.
Because there is no ofﬁcial split, a random one is made
of 28K, 2.5K and 2.5K documents for train, validation
and test.
Patent [31] subset of the Big Patent summarization
dataset. The task is redeﬁned as a classiﬁcation task
where the objective is to predict the patent category
using the full document (9 classes). A random split of
25K, 5K and 5K documents for train, validation and test
is created.
Some speciﬁc domains are highly dependent on pro-
cessing long sequences, e.g. legal domain in which
sentences tend to be long and complex. To demonstrate
the ability of LSG attention to leverage pretrained mod-
els in such cases, the following three datasets are chosen
from LexGlue [ 32], a benchmark focused on legal docu-
ments. Tasks where the input is on average signiﬁcantly
longer than 512 tokens have been selected.
Scotus Given a court opinion, the task is to predict
the relevant issue area among 14 choices.
ECtHRa and ECtHRb The objective is to predict
which articles of the European Court of Human Rights
(ECHR) have been violated (if any) from case descrip-
tion: multi-label task (10 + 1 labels).
4.2.2 Training setup and architecture
To make a fair comparison between models and archi-
tectures, ﬁne-tuning is done with the same learning
rate, number of steps (or epochs) and batch size. To
show that the LSG attention is compatible with differ-
ent architectures, the LexGlue tasks are also run with
LEGAL-BERT [ 33] converted to its LSG version using
the provided conversion tools.
4.2.3 Results
We report all experiment results in Table 2. We observe
that LSG is competitive with Longformer and Big Bird
models with input sequences up to 4096 tokens long. A
major difference lies in the implementation itself since
the LSG model is twice as fast to train on these lengths
without additional memory cost; this aspect is discussed
in Section 5.
4

--- PAGE 5 ---
Condevaux et al.
Attention512 length 4,096 length
BPC Accuracy BPC Accuracy
RoBERTa (full) [26] 1.881 0.732 4.335 0.359
Linear Attn. [27] 11.324 0.061 11.474 0.058
Efﬁcient Attn. [28] 21.022 0.102 20.574 0.097
Performer [10] 10.382 0.107 10.556 0.102
Linformer (128 proj.) [9] 22.176 0.098 20.386 0.032
Reformer [11] 17.602 0.003 18.608 0.002
Longformer (512) [6] 1.929 0.726 2.051 0.708
Big Bird (64) [12] 1.881 0.732 2.439 0.659
LSG-Norm (128/2) (block size / sparsity) 1.919 0.727 2.032 0.712
LSG-Stride (128/2) 1.938 0.724 2.046 0.710
LSG-BlockStride (128/2) 1.940 0.724 2.048 0.709
LSG-Pooling (128/2) 1.968 0.720 2.064 0.706
LSG-LSH (128/2) 1.969 0.719 2.065 0.705
Table 1: BPC and MLM accuracy of RoBERTa-base with various Attention mechanisms.
On Patent, ECtHRa and ECtHRb tasks, the ability
to process longer sequences improves signiﬁcantly
the F-measures compared to a vanilla (full attention)
RoBERTa model. We also observe that Big Bird model
is in general slightly under its counterpart except for the
ECtHRb dataset. This probably comes from the random
attention mechanism which may require additional train-
ing steps. LSG-LSH and Big Bird models are affected
by randomness during inference, thus their performance
can differ between runs.
Extrapolating LEGAL-BERT with LSG to handle
longer sequences improves predictions; this behavior is
expected and has been observed by the authors of the
LexGlue benchmark. The choice of the sparse attention
is likely task speciﬁc. Using local attention only with
a large block size is also a viable option. The role of
global tokens is not discussed here since we only use
one for all experiments. We show in the next section
with summarization tasks the utility of such tokens.
4.3 Summarization Tasks
All summarization experiments are run using a 8e-5
learning rate, a 10% warmup, a length penalty of 2.0
and a beam size of 5 for beam search. The validation set
is used to choose the max generation length. We choose
to evaluate our models on summarization tasks where
the input is signiﬁcantly longer than 1k tokens only. We
ﬁne-tune our model on ArXiv, PubMed, MultiNews and
MediaSum datasets we present below during respec-
tively 6, 8, 12 and 6 epochs for 4,096-length inputs.
4.3.1 Datasets
The next datasets are available on the HuggingFace hub,
see Appendix D.
ArXiv and Pubmed [34] are sets of documents from
ArXiv and Pubmed; the goal is to generate an abstract
using a document as input.MultiNews [35] involves generating human-written
summaries from sets of news documents.
MediaSum [36] consists of using interview tran-
scripts from CNN and NPR media to generate a sum-
mary.
We report detailed statistics in Table 15 in Appendix D.
The average length and the 90% quantile of all docu-
ments and summaries using a whitespace separator are
reported. Note that ArXiv abstracts are signiﬁcantly
longer in the training set (300 on average) than in the
validation and test sets (173 on average). Most summa-
rization models are limited to 1,000 tokens inputs, thus
they are not able to process a full document to generate
a summary.
4.3.2 Training setup and architecture
We ﬁrst convert the BART-base model [ 37] to its LSG
version by replacing the full attention in the encoder part
and adding global tokens. The model is then ﬁne-tuned
on 4,096-length inputs and evaluated. To reduce compu-
tational costs, experiments on 16,384-length inputs are
warm started from the 4,096-length experiments using
the conversion script. The model is then ﬁne-tuned dur-
ing a single epoch if necessary using the same training
parameters. We propose 3 setups for the 16,384-length.
First we evaluate the model with pure extrapolation
from 4,096-length (no additional training). In the sec-
ond setup, we extrapolate and add 64 global tokens we
choose to ﬁne-tune. In the last setup, we extrapolate, we
add 64 global tokens and we ﬁne-tune the full model.
Extrapolation is done by concatenating 4 copies of the
positional embedding matrix ( 44096 ).
Compared to the existing literature, the model is rather
small and an input sequence of 16384 tokens can ﬁt on a
48Gb GPU during training without relying on gradient-
checkpointing. The size of various summarization mod-
els from the literature are reported in Table 3.
5

--- PAGE 6 ---
Condevaux et al.
IMDb Arxiv Patent Scotus ECtHRa ECtHRb
Epochs 3 3 3 7 5 5
Learning rate 2e-5 5e-5 2e-5 1e-4 1e-4 1e-4
RoBERTa (512-length) 95.5 87.2/86.8 66.6/61.8 69.4/60.8 62.9/58.2 72.0/65.9
Longformer 95.9 88.2/87.9 69.8/63.8 72.9/62.6 68.3/59.7 78.9/72.2
Big Bird ETC 95.4 85.9/85.5 69.4/63.9 69.4/58.2 68.3/60.3 80.0/70.6
LSG-Local (256/0) 96.0 87.5/87.1 69.9/64.8 73.3/63.7 68.8/63.7 79.9/73.4
LSG-Stride (128/4) 95.6 88.2/87.9 69.2/64.0 70.5/60.0 69.5/62.3 79.3/71.6
LSG-BlockStride (128/4) 95.7 87.7/87.4 69.6/64.1 72.5/63.1 69.1/58.6 79.5/71.8
LSG-Norm (128/4) 95.7 87.0/86.6 70.0/64.4 71.3/60.8 70.1/61.9 79.4/72.1
LSG-Pooling (128/4) 95.9 87.5/87.3 69.4/64.1 72.6/60.9 70.2/61.4 79.0/73.1
LSG-LSH (128/4) 95.8 88.2/87.9 69.5/64.2 70.3/54.6 71.0/60.3 78.9/71.0
Legal-BERT (512-length) - - - 73.5/60.5 64.2/58.2 73.2/65.9
LSG-Legal-BERT (256/0) - - - 74.5/62.6 71.7/63.9 81.0/75.1
Table 2: Micro/Macro F-1 on classiﬁcation datasets.
Models Parameters
PRIMERA [38] 447M
LED [6] 460M
HAT-BART [39] 471M
Pegasus [40] 577M
Big Bird-Peg. [12] 577M
Hepos [23] 406M
LongT5-Base [41] 220M
LongT5-L 770M
LongT5-XL 3B
Ours, LSG-BART-base (256/0) 145M
Table 3: Parameters count of summarization models.
4.3.3 Results
LSG-BART is compared to state-of-the-art models by
reporting the results from their respective papers. We
use ROUGE-1, ROUGE-2 and ROUGE-L evaluation
metrics as comparison points.
As shown in Tables 4, 5, 6 and 7, our approach can
achieve competitive performances with a limited size
without pretraining a new model from scratch. The sec-
ond important element is the ability of this approach
to improve metrics from 4.096 to 16.384-length inputs
without additional ﬁne-tuning, this is especially true
on ArXiv and PubMed datasets which have the longest
input sequences. Fine tuning additional global tokens
further improveS metrics while limiting cost and train-
ing time compared to a fully tuned model.
On the ArXiv dataset (Table 4), a max sequence gener-
ation of 320 tokens is chosen, our approach is compet-
itive with every size of the LongT5 model. However,
the authors pointed out that they used greedy genera-
tion instead of beam search, thus their results are likely
underestimated.
On the PubMed dataset (Table 5), a max sequence gen-
eration of 512 tokens is chosen, our approach is close to
Hepos models which also rely on BART. LongT5 is sig-Models R1 R2 RL
Pegasus (1K) 44.70 17.27 25.80
Big Bird-Peg. (4K) 46.63 19.02 41.77
LED (4K) 44.40 17.94 39.76
LED (16K) 46.63 19.62 41.83
PRIMERA (4K) 47.58 20.75 42.57
HAT-BART (4K) 46.68 19.07 42.17
Hepos-LSH (7.2K) 48.24 20.26 41.78
Hepos-SKN (10.2K) 47.87 20.00 41.50
LongT5-Base (4K) 44.87 18.54 40.97
LongT5-L (16K) 48.28 21.63 44.11
LongT5-XL (16K) 48.35 21.92 44.27
Ours (4K) 46.65 18.91 42.18
Ours (16K) 47.03 20.19 42.69
+ global tuning 48.08 20.42 43.65
+ full tuning 48.74 20.88 44.23
Table 4: ROUGE performances on ArXiv dataset.
niﬁcantly better here and this difference may be related
to the way this model is pretrained and the dataset used
for this.
On the MultiNews dataset (Table 6), a max sequence
generation of 320 tokens is chosen, our approach is
close again to the LongT5 models. While extrapolation
improves metrics, additional ﬁne-tuning has a negative
impact. Since this dataset is rather small (45K examples,
1,400 steps), ﬁne-tuning a single epoch is not enough
for the model to converge properly, longer training is
required.
On the MediaSum dataset (Table 7), a max sequence
generation of 128 tokens is chosen. Our approach is
close to the LongT5-base model again. This dataset
has the shortest inputs, thus processing a maximum of
16,384 tokens has a marginal impact on performances.
Additional results using different types of sparse atten-
tion are detailed in Appendix C.
6

--- PAGE 7 ---
Condevaux et al.
Models R1 R2 RL
Pegasus (1K) 45.49 19.90 27.69
Big Bird-Peg. (4K) 46.32 20.65 42.33
HAT-BART (4K) 48.36 21.43 37.00
Hepos-LSH (7.2K) 48.12 21.06 42.72
Hepos-SKN (10.2K) 47.93 20.74 42.58
LongT5-Base (4K) 47.77 22.58 44.38
LongT5-L (16K) 49.98 24.69 46.46
LongT5-XL (16K) 50.23 24.76 46.67
Ours (4K) 47.37 21.74 43.67
Ours (16K) 48.03 22.42 44.32
+ global tuning 48.12 20.46 44.40
+ full tuning 48.32 22.52 44.57
Table 5: ROUGE performances on PubMed dataset.
Models R1 R2 RL
TG-MultiSum 47.10 17.55 20.73
PRIMERA (4K) 49.90 21.10 25.9
LongT5-Base (4K) 46.01 17.37 23.50
LongT5-L (4K) 46.99 18.21 24.08
LongT5-L (8K) 47.18 18.44 24.18
LongT5-XL (8K) 48.17 19.43 24.90
Ours (4K) 47.10 18.94 25.22
Ours (16K) 47.30 19.19 25.38
+ global tuning 47.23 19.18 25.29
+ full tuning 47.07 19.04 25.35
Table 6: ROUGE performances on MultiNews.
Models R1 R2 RL
BART-Large (1K) 35.09 18.05 31.44
T5-large (1K) 30.68 14.88 27.88
LongT5-Base (4K) 35.09 18.35 31.87
LongT5-L (4K) 35.54 19.04 32.20
LongT5-XL (4K) 36.15 19.66 32.80
Ours (4K) 35.16 18.13 32.20
Ours (16K) 35.17 18.13 32.21
+ global tuning 35.22 18.08 32.22
+ full tuning 35.31 18.35 32.47
Table 7: ROUGE performances on MediaSum.
Models Time/step Memory
RoBERTa (512) 1.18 s 28.8/32.1 Gb
Longformer 3.27 s 39.2/38.1 Gb
Big Bird 2.89 s 44.5/44.4 Gb
LSG-Local (256/0) 1.42 s 40.7/32.3 Gb
LSG-Norm (128/4) 1.52 s 40.4/33.4 Gb
LSG-Norm (32/4) 1.24 s 26.1/24.3 Gb
Table 8: Training speed and memory with a batch of
16384 tokens (Adam optimizer). All models rely on
sequences of 4096 tokens except RoBERTa. Memory
usage is computed with and without attention dropout.5 Implementation details
The proposed implementations are exclusively based
on those of HuggingFace in which the global tokens
are prepended to the sequence and the attention layer is
replaced by its efﬁcient version; other elements are not
modiﬁed.
To improve efﬁciency, the inputs are split into blocks.
Each block of queries is connected to 3 blocks of local
keys, 2 blocks of sparse keys and to all global keys. Thus
for headh, queries, keys and values are of shape Qh2
RnbbtdhandKh;Vh2Rnb(5bt+g)dhwithnb
the number of blocks, btthe size of blocks, dhthe size
of the head and gthe number of global tokens. This
format improves computational speed as shown in Table
8. Global attention is computed independently.
6 Conclusion
We have presented LSG attention, a novel efﬁcient O(n)
alternative to the full attention mechanism relying on
local, sparse and global attentions. Our results on MLM,
classiﬁcation and summarization tasks show that LSG
is a competitive full attention substitute for pretrained
Transformers to efﬁciently extrapolate to long input
sequences. We also proposed an optimized implementa-
tion of the LSG attention mechanism on HuggingFace,
improving training speed by a factor of 2 without addi-
tional memory cost compared to Longformer and Big
Bird models. By providing a conversion tool to lever-
age existing models and checkpoints (BERT, RoBERTa,
DistilBERT, BART), the proposed approach removes
the need of a costly re-training of existing models to
handle long sequences.
Limitations
Although the proposed conversion tool allows to con-
vert existing checkpoints of commonly used models,
it is today necessary to reimplement the approach for
each architecture due to the lack of homogeneity of
HuggingFace implementations (no wrapper available
yet). Maintenance may therefore be a problem in the
long run to ensure compatibility. We however provide
implementation examples as well as documentation to
ease the conversion process.
Concerning the proposed attention itself, the choice of
the sparse attention remains an additional hyperparam-
eter which is task speciﬁc. There is no rule of thumb
to choose the sparse type, the size of blocks and the
sparsity factor. The role of global tokens is also de-
batable. Their use can slow down training speed and
convergence. In practice, the impact of these tokens is
positive if the model is trained a sufﬁcient number of
steps.
Although the approach allows an existing model to be
reused without having to pretrain from scratch and to
reduce the duration of ﬁne-tuning phases, the complex-
ity remains linear with the length of the input. This
does not eliminate the energy costs required to deploy
Transformer models.
7

--- PAGE 8 ---
Condevaux et al.
Acknowledgements
This work has beneﬁted from LAWBOT (ANR-20-
CE38-0013) grant and HPC resources of IDRIS (al-
location 2022-AD011011309R2) made by GENCI.
References
[1]Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention
is all you need. Advances in neural information
processing systems , 30, 2017.
[2]Jacob Devlin, Ming-Wei Chang, Kenton Lee,
and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language un-
derstanding. arXiv preprint arXiv:1810.04805 ,
2018.
[3]Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. J. Mach. Learn. Res. , 21(140):1–
67, 2020.
[4]Oﬁr Press, Noah A. Smith, and Mike Lewis.
Train short, test long: Attention with lin-
ear biases enables input length extrapolation.
arXiv:2108.12409 , 2021.
[5]Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
Self-attention with relative position representa-
tions. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers) , pages 464–
468, New Orleans, Louisiana, jun 2018. Associa-
tion for Computational Linguistics.
[6]Iz Beltagy, Matthew E. Peters, and Arman Co-
han. Longformer: The long-document transformer.
arXiv:2004.05150 , 2020.
[7]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime
Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language mod-
els beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860 , 2019.
[8]Jack W Rae, Anna Potapenko, Siddhant M Jayaku-
mar, and Timothy P Lillicrap. Compressive trans-
formers for long-range sequence modelling. arXiv
preprint arXiv:1911.05507 , 2019.
[9]Sinong Wang, Belinda Z. Li, Madian Khabsa, Han
Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. CoRR , abs/2006.04768, 2020.
[10] Krzysztof Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Belanger, Lucy
Colwell, and Adrian Weller. Rethinking attention
with performers. arXiv:2009.14794 , 2021.
[11] Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-
skaya. Reformer: The efﬁcient transformer. CoRR ,
abs/2001.04451, 2020.[12] Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, et al. Big bird: Transformers for
longer sequences. Advances in Neural Information
Processing Systems , 33, 2020.
[13] Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 ,
2019.
[14] Kevin Clark, Urvashi Khandelwal, Omer Levy,
and Christopher D Manning. What does bert look
at? an analysis of bert’s attention. arXiv preprint
arXiv:1906.04341 , 2019.
[15] Alessandro Raganato, Yves Scherrer, and Jörg
Tiedemann. Fixed encoder self-attention patterns
in transformer-based machine translation. arXiv
preprint arXiv:2002.10260 , 2020.
[16] Felix Wu, Angela Fan, Alexei Baevski, Yann N
Dauphin, and Michael Auli. Pay less attention
with lightweight and dynamic convolutions. arXiv
preprint arXiv:1901.10430 , 2019.
[17] Denny Britz, Melody Y Guan, and Minh-
Thang Luong. Efﬁcient attention using a ﬁxed-
size memory representation. arXiv preprint
arXiv:1707.00110 , 2017.
[18] Chung-Cheng Chiu and Colin Raffel. Mono-
tonic chunkwise attention. arXiv preprint
arXiv:1712.05382 , 2017.
[19] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
and Chengqi Zhang. Bi-directional block self-
attention for fast and memory-efﬁcient sequence
modeling. arXiv preprint arXiv:1804.00857 ,
2018.
[20] Joshua Ainslie, Santiago Ontanon, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang, and
Li Yang. Etc: Encoding long and structured inputs
in transformers. arXiv preprint arXiv:2004.08483 ,
2020.
[21] Xingxing Zhang, Furu Wei, and Ming Zhou. Hi-
bert: Document level pre-training of hierarchical
bidirectional transformers for document summa-
rization. arXiv preprint arXiv:1905.06566 , 2019.
[22] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan
Shao, Xiangyang Xue, and Zheng Zhang. Star-
transformer. arXiv preprint arXiv:1902.09113 ,
2019.
[23] Luyang Huang, Shuyang Cao, Nikolaus Parulian,
Heng Ji, and Lu Wang. Efﬁcient attentions for long
document summarization. In Proceedings of the
2021 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies , pages 1419–1436,
Online, jun 2021. Association for Computational
Linguistics.
[24] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven,
Ilya P. Razenshteyn, and Ludwig Schmidt. Practi-
cal and optimal LSH for angular distance. CoRR ,
abs/1509.02897, 2015.
8

--- PAGE 9 ---
Condevaux et al.
[25] Diederik P. Kingma and Jimmy Ba. Adam: A
method for stochastic optimization, 2014.
[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692, 2019.
[27] Angelos Katharopoulos, Apoorv Vyas, Nikolaos
Pappas, and François Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear
attention. CoRR , abs/2006.16236, 2020.
[28] Zhuoran Shen, Mingyuan Zhang, Shuai Yi, Jun-
jie Yan, and Haiyu Zhao. Factorized attention:
Self-attention with linear complexities. CoRR ,
abs/1812.01243, 2018.
[29] Andrew L. Maas, Raymond E. Daly, Peter T.
Pham, Dan Huang, Andrew Y . Ng, and Christo-
pher Potts. Learning word vectors for sentiment
analysis. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies , pages 142–
150, Portland, Oregon, USA, jun 2011. Associa-
tion for Computational Linguistics.
[30] Jun He, Liqun Wang, Liu Liu, Jiao Feng, and Hao
Wu. Long document classiﬁcation from local word
glimpses via recurrent attention learning. IEEE
Access , 7:40707–40718, 2019.
[31] Eva Sharma, Chen Li, and Lu Wang. Bigpatent:
A large-scale dataset for abstractive and coherent
summarization. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics , pages 2204–2213, Florence, Italy, jul
2019. Association for Computational Linguistics.
[32] Ilias Chalkidis, Abhik Jana, Dirk Hartung,
Michael Bommarito, Ion Androutsopoulos,
Daniel Martin Katz, and Nikolaos Aletras.
Lexglue: A benchmark dataset for legal language
understanding in english. In Proceedings of
the 60th Annual Meeting of the Association for
Computational Linguistics , Dubln, Ireland, 2022.
[33] Ilias Chalkidis, Manos Fergadiotis, Prodromos
Malakasiotis, Nikolaos Aletras, and Ion Androut-
sopoulos. Legal-bert: The muppets straight out
of law school. In Findings of the Association for
Computational Linguistics: EMNLP 2020 , pages
2898–2904, Online, nov 2020. Association for
Computational Linguistics.
[34] Arman Cohan, Franck Dernoncourt, Doo Soon
Kim, Trung Bui, Seokhwan Kim, Walter Chang,
and Nazli Goharian. A discourse-aware attention
model for abstractive summarization of long doc-
uments. Proceedings of the 2018 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers) , 2018.
[35] Alexander R. Fabbri, Irene Li, Tianwei She, Suyi
Li, and Dragomir R. Radev. Multi-news: a large-
scale multi-document summarization dataset and
abstractive hierarchical model, 2019.[36] Chenguang Zhu, Yang Liu, Jie Mei, and Michael
Zeng. Mediasum: A large-scale media interview
dataset for dialogue summarization. arXiv preprint
arXiv:2103.06410 , 2021.
[37] Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettle-
moyer. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, trans-
lation, and comprehension. In Proceedings of the
58th Annual Meeting of the Association for Com-
putational Linguistics , pages 7871–7880, Online,
jul 2020. Association for Computational Linguis-
tics.
[38] Wen Xiao, Iz Beltagy, Giuseppe Carenini, and
Arman Cohan. Primera: Pyramid-based masked
sentence pre-training for multi-document summa-
rization. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers) , pages 5245–5263,
Dublin, Ireland, may 2022. Association for Com-
putational Linguistics.
[39] Tobias Rohde, Xiaoxia Wu, and Yinhan Liu. Hier-
archical learning for generation with long source
sequences. CoRR , abs/2104.07545, 2021.
[40] Jingqing Zhang, Yao Zhao, Mohammad Saleh,
and Peter J. Liu. Pegasus: Pre-training with ex-
tracted gap-sentences for abstractive summariza-
tion, 2019.
[41] Mandy Guo, Joshua Ainslie, David C. Uthus,
Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung,
and Yinfei Yang. Longt5: Efﬁcient text-to-
text transformer for long sequences. CoRR ,
abs/2112.07916, 2021.
A Training parameters
We use a batch size of 32, a linear decaying learning
rate, a dropout rate of 0.10 and Adam optimizer for all
tasks. Other parameters are reported in Table 9.
Epochs LR Warmup
Classiﬁcation
IMDb 3 2e-5 0%
ArXiv 3 5e-5 0%
Patent 3 2e-5 0%
Scotus 7 1e-4 0%
ECtHRa 5 1e-4 0%
ECtHRb 5 1e-4 0%
Summarization
ArXiv 6/1 8e-5 10%
PubMed 8/1 8e-5 10%
MultiNews 12/1 8e-5 10%
MediaSum 6/1 8e-5 10%
Table 9: Training parameters for all tasks.
9

--- PAGE 10 ---
Condevaux et al.
B Additional classiﬁcation results
Additional classiﬁcation results using a smaller block
size are presented in Table 10. It shows that the use of a
smaller block size remains competitive even if a slight
loss in performance is observed.
C Additional summarization results
Models (4,096) R1 R2 RL Ctx.
32/4
Pooling 42.75 16.34 38.23 160
Stride 44.23 17.21 39.72 160
Block Stride 44.15 17.10 39.60 160
Norm 42.02 15.65 37.45 160
LSH 42.58 16.21 38.04 160
128/4
Pooling 46.27 18.68 41.82 644
Stride 46.34 18.64 41.87 644
Block Stride 46.23 18.62 41.80 644
Norm 45.96 18.46 41.51 644
LSH 46.19 18.72 41.76 644
Reference 46.65 18.91 42.18 784
Table 11: ROUGE performances on ArXiv dataset.
Models (4,096) R1 R2 RL Ctx.
32/4
Pooling 44.60 19.35 40.85 160
Stride 45.52 20.07 41.75 160
Block Stride 45.30 19.89 41.54 160
Norm 44.30 19.05 40.47 160
LSH 44.53 19.27 40.74 160
128/4
Pooling 47.11 21.42 43.40 644
Stride 47.16 21.49 43.44 644
Block Stride 47.13 21.46 43.42 644
Norm 47.09 21.44 43.36 644
LSH 47.11 21.41 43.42 644
Reference 47.37 21.74 43.67 784
Table 12: ROUGE performances on PubMed dataset.
Trained models on summarization tasks are reevaluated
after changing the type of sparse attention and block
size. Results on ArXiv and PubMed are reported in
Tables 11 and 12. The context column refers to the num-
ber of keys each query attends to. As reference models
are trained on large (256) local blocks, the number of
connections is 3256. By using 20% less keys (644),
inference results are still competitive even though the
model has never seen these speciﬁc sparse patterns be-
fore. By limiting connections to 20% of the keys (160),
a performance drop is observed even if the metrics stillremain respectable. Under these conditions, stride and
block-stride approaches generate better predictions.
D Datasets and Models
All the datasets evalued are available on the Hugging-
Face hub, links are provided in Table 13.
Summarization checkpoints are available on the Hug-
gingFace hub, links are provided in Table 14.
Average input size are provided in Table 15. Note that
token counts are obtained using a whitespace split. Sub-
word tokenization increases these numbers by 30% to
40% depending on the tokenizer and the vocabulary
size.
E Hyperparameters and complexity
LSG attention is sensitive to several hyperparameters
and the nature of the pretrained model.
Block size Generally speaking, block size improves
performance to a certain extent. If the converted model
is trained on 512-length sequences, a block size beyond
256 has a negative impact and requires a longer ﬁne-
tuning phase. A smaller block reduces training and
memory cost.
Sparsity factor The sparsity factor is generally cho-
sen between 0 (no sparse attention), 2, 4 and 8. Al-
though the choice remains task-dependent, a factor
above 8 tends on average to decrease the level of perfor-
mance, especially for pooling-based approaches. This
hyperparameter is chosen to be small when the task fo-
cuses on local information (MLM, NER) and can be
larger for tasks requiring a wider context (summariza-
tion, question-answering).
Global tokens The more global tokens there are, the
longer it takes for the model to converge and obtain
performance gains. The initialization of these tokens is
important, in particular for the ﬁrst one which is used
as a pooling token for classiﬁcation tasks ([CLS] or <s>
+ position 0).
Overall complexity LSG attention has some similari-
ties with Big Bird attention and has the same O(n)com-
plexity with regard to sequence length. Figure 5 shows
the effect of increasing sequence length on training time
and memory consumption (with Adam optimizer).
10

--- PAGE 11 ---
Condevaux et al.
IMDb Arxiv Patent Scotus ECtHRa ECtHRb
RoBERTa (512-length) 95.5 87.2/86.8 66.6/61.8 69.4/60.8 62.9/58.2 72.0/65.9
Longformer 95.9 88.2/87.9 69.8/63.8 72.9/62.6 68.3/59.7 78.9/72.2
Big Bird 95.4 85.9/85.5 69.4/63.9 69.4/58.2 68.3/60.3 80.0/70.6
LSG-Local (256/0) 96.0 87.5/87.1 69.9/64.8 73.3/63.7 68.8/63.7 79.9/73.4
LSG-Stride (128/4) 95.6 88.2/87.9 69.2/64.0 70.5/60.0 69.5/62.3 79.3/71.6
LSG-BlockStride (128/4) 95.7 87.7/87.4 69.6/64.1 72.5/63.1 69.1/58.6 79.5/71.8
LSG-Norm (128/4) 95.7 87.0/86.6 70.0/64.4 71.3/60.8 70.1/61.9 79.4/72.1
LSG-Pooling (128/4) 95.9 87.5/87.3 69.4/64.1 72.6/60.9 70.2/61.4 79.0/73.1
LSG-LSH (128/4) 95.8 88.2/87.9 69.5/64.2 70.3/54.6 71.0/60.3 78.9/71.0
LSG-Stride (32/4) 95.6 85.0/84.5 69.2/63.0 72.4/62.7 69.9/58.4 79.2/72.3
LSG-BlockStride (32/4) 95.4 86.6/86.3 69.4/63.4 72.4/62.6 70.3/60.9 79.2/69.4
LSG-Norm (32/4) 95.7 85.3/84.9 69.3/63.9 72.2/62.5 68.9/60.9 79.1/73.5
LSG-Pooling (32/4) 95.7 88.2/88.0 69.2/63.3 72.6/60.2 69.6/59.1 79.3/72.1
LSG-LSH (32/4) 95.7 88.0/87.7 68.9/62.9 71.9/60.4 70.0/61.1 80.3/73.1
Legal-BERT (512-length) - - - 73.5/60.5 64.2/58.2 73.2/65.9
LSG-Legal-BERT - - - 74.5/62.6 71.7/63.9 81.0/75.1
Table 10: Micro/Macro F-1 on classiﬁcation datasets.
Figure 5: Memory and time complexity as a function of input sequence length.
Link to the hub
Classiﬁcation
IMDb imdb
ArXiv arxiv-classiﬁcation
Patent patent-classiﬁcation
Scotus lex_glue/scotus
ECtHRa lex_glue/ecthr_a
ECtHRb lex_glue/ecthr_b
Summarization
ArXiv scientiﬁc_papers/arxiv
PubMed scientiﬁc_papers/pubmed
MultiNews multi_news
MediaSum mediasum
Table 13: Links to datasets.Summarization Link to the hub
LSG-PubMed (4,096) lsg-pubmed
LSG-PubMed (16,384) lsg-pubmed
LSG-ArXiv (4,096) lsg-arxiv
LSG-ArXiv (16,384) lsg-arxiv
LSG-MediaSum (4,096) lsg-mediasum
LSG-MediaSum (16,384) lsg-mediasum
LSG-MultiNews (4,096) lsg-multinews
Additional checkpoints lsg-checkpoints
Table 14: Links to model checkpoints.
11

--- PAGE 12 ---
Condevaux et al.
DatasetsExample count Input length Summary length
Train Validation Test Average q-90% Average q-90%
Classiﬁcation
IMDb 25,000 - 25,000 234 458 - -
ArXiv 28,000 2,500 2,500 9,115 16,264 - -
Patent 25,000 5,000 5,000 3,593 6,579 - -
Scotus 5,000 1,400 1,400 6,992 14,570 - -
ECtHRa 9,000 1,000 1,000 1,681 3,728 - -
ECtHRb 9,000 1,000 1,000 1,681 3,728 - -
Summarization
ArXiv 203,037 6,436 6,440 6,030 11,165 272 321
PubMed 119,924 6,633 6,658 3,050 5,737 202 304
MultiNews 44,972 5,622 5,622 1,792 3,375 217 294
MediaSum 443,596 10,000 10,000 1,582 2,883 15 32
Table 15: Statistics of evaluated datasets, lengths are whitespace based.
12
