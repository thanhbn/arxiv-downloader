# 2305.14806.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2305.14806.pdf
# Kích thước tệp: 500681 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
AWESOME: Tóm tắt Tài liệu Dài bị Hạn chế Bộ nhớ GPU
sử dụng Cơ chế Bộ nhớ và Nội dung Nổi bật Toàn cục
Shuyang Cao và Lu Wang
Khoa Khoa học Máy tính và Kỹ thuật
Đại học Michigan
Ann Arbor, MI
{caoshuy, wangluxy}@umich.edu
Tóm tắt
Các hệ thống tóm tắt tài liệu dài rất quan trọng đối với các lĩnh vực có văn bản dài và đầy thuật ngữ chuyên môn, nhưng chúng đặt ra những thách thức đáng kể cho các nhà nghiên cứu và nhà phát triển có tài nguyên tính toán hạn chế. Các giải pháp hiện tại chủ yếu tập trung vào attention hiệu quả hoặc chiến lược chia để trị. Phương pháp trước làm giảm độ phức tạp thời gian lý thuyết, nhưng vẫn tốn nhiều bộ nhớ. Các phương pháp sau hy sinh ngữ cảnh toàn cục, dẫn đến các bản tóm tắt không có thông tin và không mạch lạc. Công trình này nhằm tận dụng tính chất tiết kiệm bộ nhớ của các phương pháp chia để trị trong khi vẫn bảo toàn ngữ cảnh toàn cục. Cụ thể, framework AWESOME của chúng tôi sử dụng hai cơ chế mới: (1) Các cơ chế bộ nhớ ngoài theo dõi các đoạn tài liệu đã được mã hóa trước đó và các bản tóm tắt tương ứng của chúng, để tăng cường hiểu biết toàn cục về tài liệu và tính mạch lạc của bản tóm tắt. (2) Nội dung nổi bật toàn cục được xác định thêm trước để tăng cường cho mỗi đoạn tài liệu nhằm hỗ trợ việc tóm tắt của nó. Các thực nghiệm rộng rãi trên các thể loại văn bản đa dạng, bao gồm báo cáo chính phủ, bản ghi cuộc họp, kịch bản phim, bài báo khoa học và tiểu thuyết, cho thấy AWESOME tạo ra các bản tóm tắt với tính thông tin, trung thực và mạch lạc được cải thiện so với các baseline cạnh tranh trên các tài liệu dài hơn, trong khi có dung lượng bộ nhớ GPU nhỏ hơn.
1 Giới thiệu
Các mô hình transformer lớn được pre-train đã chứng minh hiệu suất ấn tượng trên các benchmark tóm tắt trừu tượng phổ biến (Lewis et al., 2020; Raffel et al., 2020). Tuy nhiên, độ phức tạp bộ nhớ bậc hai của transformer đặt ra thách thức trong việc tóm tắt các tài liệu dài có hơn hàng trăm từ, chẳng hạn như bài báo khoa học và báo cáo điều tra (Cohan et al., 2018; Huang et al., 2021), khiến việc này trở nên không khả thi đối với các nhà nghiên cứu và nhà phát triển có tài nguyên phần cứng hạn chế (ví dụ: GPU có bộ nhớ không đủ) để đóng góp vào lĩnh vực nghiên cứu quan trọng này.

Cộng đồng NLP đã thực hiện một số đổi mới để giải quyết thách thức tài liệu dài. Các công trình trước đây chia tài liệu thành các khối nhỏ hơn và tóm tắt từng khối riêng biệt (Gidiotis và Tsoumakas, 2020), giảm độ phức tạp của các phép tính attention (Beltagy et al., 2020), và loại bỏ nội dung không quan trọng trước khi chạy abstractor (Pilault et al., 2020). Về mặt hiệu quả bộ nhớ, các phương pháp chia để trị đạt được lợi thế đáng kể nhất (Moro và Ragazzi, 2022). Tuy nhiên, thông tin bên ngoài một đoạn tài liệu và các bản tóm tắt tương ứng của chúng trở nên không thể truy cập, dẫn đến các bản tóm tắt không có thông tin và không mạch lạc. Không có gì đáng ngạc nhiên, hiệu suất tiên tiến nhất được đạt bởi các mô hình có thể duy trì ngữ cảnh toàn cục, ví dụ: bằng cách kết hợp global attention với local attention trong các mô hình tóm tắt dựa trên transformer (Zaheer et al., 2021; Phang et al., 2022). Tuy nhiên, chúng vẫn yêu cầu dung lượng bộ nhớ GPU lớn trong thực tế.1

Mặc dù các mô hình ngôn ngữ lớn như GPT-4 (OpenAI, 2023) được huấn luyện để xử lý tới 32K token, vấn đề bảo mật và riêng tư của dữ liệu được truyền và chia sẻ thông qua API vẫn đáng lo ngại, đặc biệt trong các lĩnh vực xử lý thông tin nhạy cảm, ví dụ: hồ sơ y tế. Phát triển mô hình cục bộ có thể tăng cường bảo mật và riêng tư; tuy nhiên, tài nguyên tính toán hạn chế trong những trường hợp này đòi hỏi phải khám phá các kỹ thuật mô hình hóa hiệu quả.

Do đó, công trình này nhằm giải quyết vấn đề tóm tắt tài liệu dài sử dụng tài nguyên bị hạn chế, đặc biệt tập trung vào bộ nhớ GPU bị hạn chế. Chúng tôi đề xuất AWESOME2, được xây dựng trên phương pháp chia để trị tiết kiệm bộ nhớ, và Augmented With Estimated Salient cOntent and MEmory mechanism. Về bản chất, AWESOME duy trì ngữ cảnh toàn cục của cả

1Những phương pháp này yêu cầu bộ nhớ GPU >40GB để xử lý các tài liệu có hơn 8K token, trong khi các GPU hiệu quả về chi phí nhất chỉ có 24GB bộ nhớ (Li, 2022).
2Mã nguồn của chúng tôi sẽ được cung cấp tại https://shuyangcao.github.io/projects/awesome/arXiv:2305.14806v2 [cs.CL] 16 Nov 2023

--- TRANG 2 ---
tài liệu nguồn và bản tóm tắt được tạo ra cho đến nay với việc sử dụng bộ nhớ hạn chế, để tăng cường tính thông tin, trung thực và mạch lạc của bản tóm tắt.

Đầu tiên, cơ chế bộ nhớ ngoài được sử dụng ở phía encoder của AWESOME để lưu trữ thông tin khi nó đọc các đoạn tài liệu theo thứ tự. Điều này duy trì ngữ cảnh liên quan để cải thiện hiểu biết tài liệu và phát hiện nội dung nổi bật, do đó thúc đẩy tính thông tin và trung thực của bản tóm tắt. Một bộ nhớ khác được áp dụng ở phía decoder để cải thiện tính mạch lạc của quá trình tạo bằng cách theo dõi các bản tóm tắt một phần được tạo cho các đoạn tài liệu trước đó. Quan trọng là, để đảm bảo hiệu quả bộ nhớ GPU của AWESOME, chúng tôi ngăn chặn gradient lan truyền đến các đoạn tài liệu và tóm tắt khác và chỉ cho phép một số lượng hạn chế các layer duy trì bộ nhớ ngoài.

Thứ hai, AWESOME kết hợp nội dung nổi bật toàn cục được chọn bởi một extractor được huấn luyện hiệu quả thông qua (1) nối văn bản trực tiếp, hoặc (2) chèn các ma trận key-value của chúng vào phép tính attention. Điều này cho phép summarizer nhận biết các chủ đề quan trọng ở cấp độ toàn cục, để tăng cường ước lượng độ nổi bật và tính thông tin của bản tóm tắt.

Chúng tôi thực nghiệm với năm benchmark đầu vào dài phổ biến của các thể loại khác nhau: báo cáo điều tra trong GovReport (Huang et al., 2021), bản ghi cuộc họp trong QMSum (Zhong et al., 2021), kịch bản truyền hình trong SummScreen (Chen et al., 2022), bài báo khoa học trong arXiv (Cohan et al., 2018), và tiểu thuyết trong BookSum (Kryscinski et al., 2022). Đầu tiên, trên tất cả năm dataset, tất cả các biến thể AWESOME đều vượt trội hơn Se3 (Moro và Ragazzi, 2022), baseline chia để trị, về tính thông tin của bản tóm tắt được đánh giá bằng ROUGE (Lin, 2004) và về tính mạch lạc được đo bằng DiscoScore (Zhao et al., 2022) và một metric dựa trên đồ thị thực thể (Guinaudeau và Strube, 2013)—cả hai metric đều có tương quan cao với đánh giá của con người, theo Zhao et al. (2022). Thứ hai, AWESOME với các cơ chế bộ nhớ cũng cải thiện tính trung thực của bản tóm tắt so với Se3 trên GovReport, theo SummaC (Laban et al., 2022), một metric trung thực dựa trên entailment. Cuối cùng, so với các mô hình tốn nhiều bộ nhớ hơn mà cũng duy trì ngữ cảnh toàn cục, chẳng hạn như Phang et al. (2022) và Liu et al. (2022), AWESOME đạt được điểm số tự động cao hơn về tính thông tin, mạch lạc và trung thực trên GovReport (Huang et al., 2021). Trên BookSum bao gồm các tài liệu và bản tóm tắt dài nhất trong số

Phương pháp In →Out Enc Enc ←Dec Dec
Efficient Attention x→y■ ■  
Extract-Abstract xe→y□ ⋆  
Dynamic Weight x→y□ ■ +⋆ 
Divide-Conquer xi→yi□ □ #
Bảng 1: Các phương pháp hiện tại cho tóm tắt tài liệu dài (§2.1). In→Out: Đầu vào dài hơn (|x|>|xe|>|xi|) hoặc đầu ra (|y|>|yi|) tạo ra nhiều node hơn trong đồ thị tính toán, do đó tiêu thụ bộ nhớ cao hơn. Enc: Encoder truy cập tài liệu một phần (□) gây hại cho hiểu biết tài liệu, so với đọc toàn bộ văn bản (■). Enc←Dec: Decoder đọc toàn bộ tài liệu (■) hoặc nội dung nổi bật được xác định trước (⋆) tăng cường tính thông tin của bản tóm tắt, so với một đoạn (□). Dec: Decoder truy cập nội dung tóm tắt được tạo trước đó () quan trọng hơn cho tính mạch lạc của quá trình tạo so với chỉ đọc một đoạn tóm tắt hiện tại (#).

năm dataset, AWESOME tạo ra đầu ra có thông tin và mạch lạc hơn so với các mô hình gần đây.

2 Công trình Liên quan
2.1 Tóm tắt Tài liệu Dài Hiệu quả
Chúng tôi phân loại các mô hình tóm tắt tài liệu dài hiệu quả hiện tại thành bốn loại chính, được tóm tắt trong Bảng 1. Đầu vào mô hình có thể là tài liệu gốc, các đoạn quan trọng được trích xuất của tài liệu, hoặc một đoạn tài liệu, được ký hiệu là x, xe, hoặc xi (cho đoạn thứ i), và thường thì |x|>|xe|>|xi|. Đầu ra có thể là bản tóm tắt đầy đủ y hoặc một đoạn tóm tắt yi (cho xi), với |y|>|yi|. Quan trọng là, đầu vào và đầu ra dài hơn mở rộng đồ thị tính toán lớn hơn, dẫn đến việc sử dụng bộ nhớ GPU cao hơn. Hơn nữa, chúng tôi phân tích cả ngữ cảnh tài liệu và ngữ cảnh tóm tắt được sử dụng bởi mỗi phương pháp khi tạo bản tóm tắt. Cụ thể, chúng tôi kiểm tra (1) tài liệu đầy đủ so với một phần được tiêu thụ để có được biểu diễn encoder (Enc); (2) biểu diễn encoder đầy đủ so với một phần được decoder chú ý đến (Enc←Dec); và (3) đầu ra đầy đủ so với một phần được decoder truy cập (Dec).

Efficient attention được thiết kế để giảm độ phức tạp bậc hai của kiến trúc transformer gốc (Vaswani et al., 2017) và duy trì ngữ cảnh mã hóa đầy đủ bằng cách kết hợp global attention với local attention được xây dựng trên các cửa sổ trượt (Beltagy et al., 2020; Zaheer et al., 2021), các khối văn bản (Phang et al., 2022; Tay et al., 2020), hoặc các cụm token tương tự (Kitaev et al., 2020; Roy

--- TRANG 3 ---
et al., 2021). Bên cạnh các biến thể attention được đề cập ở trên được thiết kế cho self-attention, công trình gần đây đã giảm việc sử dụng bộ nhớ của decoder cross attention bằng cách phân phối đầu ra encoder cho các attention head khác nhau (Huang et al., 2021) hoặc chọn đầu ra encoder có thể được chú ý thông qua tìm kiếm KNN (Bertsch et al., 2023). Bất chấp độ phức tạp giảm, các hệ thống dựa trên efficient attention thực tế yêu cầu đọc toàn bộ tài liệu x để tạo bản tóm tắt y trong quá trình huấn luyện mô hình và do đó vẫn cần bộ nhớ GPU khổng lồ tỷ lệ với độ dài đầu vào.

Các hệ thống extract-then-abstract tránh thách thức chuỗi dài bằng cách đầu tiên xác định các đoạn nổi bật, xe (ví dụ: câu), sử dụng extractor, và sau đó chạy abstractor trên xe để tạo bản tóm tắt cuối cùng (Pilault et al., 2020; Liu và Lapata, 2019; Zhao et al., 2020). Tuy nhiên, các đoạn được trích xuất có thể chứa thông tin không đầy đủ và ngoài ngữ cảnh dẫn đến các bản tóm tắt không thể hiểu và không trung thực.

Để giảm thiểu vấn đề lan truyền lỗi của phương pháp hai giai đoạn, các nghiên cứu gần đây nối extractor và abstractor thông qua trọng số động trên các đoạn tài liệu. Thay vì đưa các đoạn được trích xuất trực tiếp vào abstractor, tại mỗi bước giải mã tóm tắt, DYLE (Mao et al., 2022) đầu tiên dự đoán phân phối token đầu ra cho mỗi đoạn riêng biệt, và sau đó tổng hợp trên tất cả các đoạn được trích xuất được cân nhắc bởi độ nổi bật trích xuất của chúng. PageSum (Liu et al., 2022) tiếp tục giảm thiểu mất mát ngữ cảnh bằng cách lấy trung bình các biểu diễn đầu ra decoder được điều kiện trên tất cả các đoạn tài liệu. Mặc dù abstractor của họ xử lý mỗi đoạn tài liệu xi riêng biệt, việc huấn luyện chung extractor và abstractor vẫn yêu cầu tải toàn bộ tài liệu x vào bộ nhớ GPU.

Các hệ thống chia để trị chia tài liệu thành nhiều đoạn không chồng lấp và tóm tắt từng đoạn riêng biệt, như được thực hiện trong Gidiotis và Tsoumakas (2020) và Se3 (Moro và Ragazzi, 2022). SummN (Zhang et al., 2022) sử dụng giai đoạn tóm tắt bổ sung để tiếp tục ngưng tụ các bản tóm tắt được phân đoạn. Vì mỗi đoạn tài liệu xi được tóm tắt riêng biệt, dung lượng bộ nhớ GPU cố định của phương pháp chia để trị độc lập với độ dài tài liệu. Điều này phù hợp tốt với mục tiêu của chúng tôi là tóm tắt tài liệu dài với bộ nhớ hạn chế. Tuy nhiên, không có quyền truy cập vào các phần khác của tài liệu và bản tóm tắt của chúng, summarizer gặp khó khăn trong việc ước lượng độ nổi bật nội dung trong mỗi đoạn biệt lập, và

tạo ra đầu ra không mạch lạc khi ghép các bản tóm tắt lại với nhau. Mặc dù Wu et al. (2021) nối các bản tóm tắt được tạo trước đó như một phần của đầu vào, một chiến lược phức tạp được yêu cầu để xây dựng mẫu huấn luyện.

AWESOME được xây dựng trên phương pháp chia để trị tiết kiệm bộ nhớ, và cải thiện tính thông tin, mạch lạc và trung thực của bản tóm tắt bằng cách sử dụng các bộ nhớ ngoài được thiết kế mới để tích lũy thông tin nổi bật từ các đoạn tài liệu khác và các bản tóm tắt được tạo của chúng. Chúng tôi tiếp tục tăng cường AWESOME với nội dung nổi bật toàn cục để cung cấp các chủ đề quan trọng ở cấp độ tài liệu, khi tóm tắt từng đoạn.

2.2 Tăng cường Bộ nhớ và Nội dung
Các cơ chế bộ nhớ khác nhau đã được nghiên cứu cho các tác vụ hiểu văn bản tầm xa. Ví dụ, Transformer-XL (Dai et al., 2019) cache các biểu diễn trung gian được tạo trong đoạn tài liệu cuối cùng và chú ý đến những biểu diễn này. Compressive Transformer (Rae et al., 2020) tiếp tục tăng phạm vi ngữ cảnh bằng cách nén các biểu diễn được cache lâu nhất. Để mô phỏng đọc và ghi bộ nhớ, Recurrent Memory Transformer (Bulatov et al., 2022) bao gồm các vector bộ nhớ bổ sung trong mỗi đoạn văn bản và truyền các vector đầu ra tương ứng của chúng đến đoạn tiếp theo. Thay vì sử dụng bộ nhớ có kích thước cố định, Memorizing Transformer (Wu et al., 2022a) lưu trữ tất cả các biểu diễn trước đó như các cặp key-value, và thực hiện tra cứu kNN xấp xỉ để truy xuất biểu diễn nhằm tăng cường đoạn hiện tại.

Tuy nhiên, công trình hiện tại về các cơ chế bộ nhớ tập trung vào mô hình hóa ngôn ngữ, trong khi việc kết hợp các cơ chế bộ nhớ vào quá trình giải mã cho các tác vụ tạo sinh là không tầm thường vì nó yêu cầu cập nhật cả trạng thái giải mã (ví dụ: beam) và trạng thái bộ nhớ. Công trình của chúng tôi là đầu tiên tận dụng các cơ chế bộ nhớ và tăng cường nội dung để kết hợp ngữ cảnh toàn cục cho mục đích tóm tắt tài liệu dài tiết kiệm bộ nhớ.

3 Tăng cường Bộ nhớ Ngoài và Nội dung Nổi bật Toàn cục
Kiến trúc của AWESOME (Hình 1) dựa trên Se3 (Moro và Ragazzi, 2022), trong đó một tài liệu được tóm tắt từng đoạn một, với bản tóm tắt cuối cùng được thu được bằng cách nối các bản tóm tắt kết quả. Các câu tài liệu được chia thành các đoạn có tối đa 768 token mỗi đoạn, trong khi

--- TRANG 4 ---
EncoderBộ nhớ EncoderDecoderBộ nhớ DecoderĐoạn Tóm tắtĐoạn Hiện tạiCác Đoạn Tương laiCác Đoạn Quá khứ
Truy cập Bộ nhớCập nhật Bộ nhớNội dung Nổi bật Được Tăng cườngEncoderDecoderĐoạn Tóm tắtEncoderDecoderĐoạn Tóm tắtEncoderDecoderĐoạn Tóm tắtEncoderDecoderĐoạn Tóm tắtHình 1: Minh họa AWESOME. Bộ nhớ encoder và decoder có thể được truy cập bất cứ lúc nào và được cập nhật sau khi đọc mỗi đoạn tài liệu và tạo bản tóm tắt tương ứng. Chúng tích lũy ngữ cảnh toàn cục giúp cải thiện tính thông tin và mạch lạc của bản tóm tắt (§3.1). Khi mã hóa mỗi đoạn, nội dung nổi bật toàn cục từ các đoạn khác (các dòng có đầu hình ♦, từ cả quá khứ và tương lai) được cung cấp để hỗ trợ thêm việc ước lượng độ nổi bật (§3.2).

các câu tóm tắt tham chiếu được gán cho đoạn chồng lấp nhiều nhất của chúng để tạo bản tóm tắt oracle, như được mô tả chi tiết trong Phụ lục A. Theo Longformer (Beltagy et al., 2020), chúng tôi khởi tạo các tham số encoder và decoder từ BART (Lewis et al., 2020). AWESOME bảo toàn ngữ cảnh toàn cục và xây dựng giao tiếp giữa các đoạn với việc tăng bộ nhớ GPU tối thiểu, bằng cách (1) sử dụng bộ nhớ ngoài trong cả encoder và decoder để thu thập thông tin liên quan (§3.1), và (2) tăng cường encoder với nội dung nổi bật từ các đoạn khác (§3.2).

3.1 Các Cơ chế Bộ nhớ Ngoài
Chúng tôi thiết kế hai cơ chế bộ nhớ ngoài để cho phép luồng thông tin từ các đoạn trước đó đến đoạn hiện tại một cách hiệu quả. Cụ thể, mỗi module bộ nhớ duy trì một ma trận M∈Rm×d, trong đó m = 1024 là kích thước bộ nhớ và d = 1024 là chiều của trạng thái ẩn của BART. M được cập nhật sau khi mã hóa mỗi đoạn tài liệu và sau đó được truyền đến đoạn tiếp theo. Chúng tôi ký hiệu ma trận bộ nhớ sau đoạn thứ t là Mt. Mỗi layer của encoder và decoder có thể được trang bị một bộ nhớ ngoài như vậy. Dưới đây chúng tôi mô tả hai cơ chế để cập nhật Mt và kết hợp nó trong cả quá trình mã hóa và giải mã. Chỉ số layer trong các công thức được bỏ qua để đơn giản.

Bộ nhớ Nén. Đối với mỗi đoạn tài liệu, bộ nhớ dựa trên nén cache các vector đầu vào của nó để được đưa vào phép tính self-attention. Vì lưu trữ các vector đầu vào như chúng có yêu cầu việc sử dụng bộ nhớ m tỷ lệ tuyến tính với độ dài ngữ cảnh, chúng tôi dành nửa Mt để lưu trữ bộ nhớ nén, với tỷ lệ nén r. Với Ht inp biểu thị ma trận chứa các vector đầu vào cho transformer self-attention, các quá trình nén và cập nhật bộ nhớ là:

Mt−1 c, Mt−1 u = Mt−1[:m 2], Mt−1[m 2:] (1)
M′ u = concat(Mt−1 u, SG(Ht inp)) (2)
M′ c = compress(M′ u[:−m 2]) (3)
Mt u = M′ u[−m 2:] (4)
Mt c = concat(Mt−1 c, M′ c)[−m 2:] (5)
Mt = concat(Mt c, Mt u) (6)

trong đó SG(·) biểu thị việc dừng lan truyền ngược gradient để giảm việc sử dụng GPU, và compress(·) thực hiện các phép convolution với stride và kernel size được đặt thành tỷ lệ nén r. r được đặt thành 5 sau khi điều chỉnh trên các tập phát triển.

Tiếp theo, để tận dụng bộ nhớ từ đoạn trước trong việc tóm tắt đoạn hiện tại, Mt−1 được nối với các đầu vào cho self-attention để thu được các ma trận key-value:

Ht mem = concat(Mt−1, Ht inp) (7)
Ht self = Attn(Ht inp |{z} query, Ht mem |{z} key, Ht mem |{z} value) (8)

trong đó Ht self là đầu ra của self-attention.

Bộ nhớ dựa trên nén của chúng tôi được áp dụng từ Compressive Transformer (Rae et al., 2020), một mô hình chỉ decoder cho mô hình hóa ngôn ngữ. Chúng tôi là những người đầu tiên áp dụng nó cho cả encoder và decoder của mô hình Transformer và trên các tác vụ tóm tắt tài liệu dài.

Bộ nhớ nén ưu tiên tính gần đây, đặc biệt là đoạn trước đó và bản tóm tắt của nó, có thể khiến lịch sử liên quan lâu hơn bị mất trong quá trình nén.

Bộ nhớ Chú ý. Để giảm thiểu độ thiên vị gần đây bởi bộ nhớ nén, chúng tôi tiếp tục điều tra một cơ chế cập nhật bộ nhớ dựa trên attention, để bao gồm nội dung trong Mt một cách có chọn lọc. Đầu tiên, bộ nhớ được đi kèm thêm bởi một cross-attention bổ sung trong mỗi layer encoder và decoder,

--- TRANG 5 ---
chuyên môn trong việc truy xuất thông tin liên quan từ Mt. Theo nghiên cứu trước (Lei et al., 2020) sử dụng bộ nhớ trong video captioning, chúng tôi cập nhật Mt với ma trận cổng Gt để kiểm soát lượng nội dung được cập nhật:

Mt = Gt ⊙ Ut + (1−Gt) ⊙ Mt−1 (9)

trong đó ⊙ biểu thị tích element-wise và Ut là ma trận chứa các vector để cập nhật bộ nhớ. Ut và Gt được thu được như sau:

Ut = tanh(Wu1Mt−1 + Wu2St) (10)
Gt = σ(Wg1Mt−1 + Wg2St) (11)
St = Attn(Mt−1 |{z} query, SG(Ht self) |{z} key, SG(Ht self) |{z} value) (12)

trong đó W∗ là các ma trận có thể học, St tổng hợp đoạn hiện tại thông qua phép tính attention, và SG(·) chỉ ra việc dừng lan truyền ngược gradient. Trong mỗi layer encoder và decoder, một cross-attention bổ sung được chèn sau self-attention, trong đó Mt−1 được chú ý và kết hợp vào quá trình tóm tắt của đoạn hiện tại.

Không giống như phương pháp của chúng tôi, bộ nhớ trong Lei et al. (2020) không sử dụng việc dừng gradient. Việc bỏ qua này loại bỏ hiệu quả bộ nhớ đạt được từ chiến lược chia để trị, dẫn đến việc sử dụng bộ nhớ cao tương đương như chiến lược efficient attention.3 Trong khi bộ nhớ của họ phù hợp để tạo chú thích hình ảnh ngắn, thiết kế của chúng tôi với việc dừng gradient là quan trọng cho tóm tắt tài liệu dài hiệu quả.

Thêm Chọn lọc Bộ nhớ Ngoài. Bộ nhớ ngoài tạo ra overhead trong việc sử dụng bộ nhớ GPU. Để giảm thiểu overhead này, chúng tôi xem xét việc thêm bộ nhớ ngoài một cách có chọn lọc vào các layer cụ thể, vì tầm quan trọng của bộ nhớ ngoài thay đổi theo các chức năng khác nhau của các layer trong mô hình. Nghiên cứu thí điểm của chúng tôi cho thấy rằng các layer cuối cùng của mô hình Transformer sử dụng bộ nhớ ngoài hiệu quả hơn so với các layer đầu tiên. Để tránh tìm kiếm toàn diện cho layer tối ưu hoặc kết hợp các layer cho mỗi dataset, chúng tôi chọn trang bị đồng nhất ba layer cuối cùng với bộ nhớ ngoài trên tất cả các dataset trừ khi có chỉ định khác.4

3Không có việc dừng gradient, mô hình không thể hoàn thành huấn luyện với bộ nhớ GPU 48GB.
4So với việc thêm bộ nhớ ngoài vào tất cả các layer, việc thêm có chọn lọc giảm việc sử dụng bộ nhớ GPU khoảng 9GB.

3.2 Tăng cường Nội dung Nổi bật Toàn cục
Các cơ chế bộ nhớ chỉ cấp quyền truy cập vào nội dung trước đó trong tài liệu, tuy nhiên ngữ cảnh tiếp theo cũng có thể giúp việc ước lượng độ nổi bật, ví dụ: việc phân tích ưu nhược điểm của một giải pháp được đề xuất khiến việc giới thiệu vấn đề và giải pháp trở nên cần thiết. Hơn nữa, bộ nhớ lưu trữ nội dung một cách ngầm, vì vậy không rõ liệu thông tin liên quan có thể được lưu trữ và truy xuất hiệu quả hay không. Do đó, chúng tôi thông báo cho hệ thống về các câu quan trọng của tài liệu, được xác định trước bởi một extractor được huấn luyện riêng. Chi tiết về huấn luyện extractor có thể được tìm thấy trong Phụ lục D. Sau khi trích xuất các câu quan trọng trong tài liệu, chúng tôi nghiên cứu hai phương pháp tiêm chúng vào summarizer.

Nối Văn bản. Đối với mỗi đoạn, chúng tôi bao gồm các câu được trích xuất theo cách sau để ưu tiên ngữ cảnh dài hạn. Chúng tôi bắt đầu với các câu được trích xuất "ngoài cùng", tức là câu sớm nhất trong các đoạn quá khứ và câu cuối cùng trong các đoạn tương lai, và lặp lại quá trình này cho đến khi đầu vào đạt độ dài tối đa được chấp nhận bởi positional encoding của mô hình (1024 cho BART).5 Để phân biệt nội dung trong đoạn hiện tại với các câu được thêm, chúng tôi thêm tiền tố cho đoạn hiện tại và các câu được thêm từ trước/sau đoạn hiện tại với "Current chunk: ", "Previous important sentences: ", và "Next important sentences: ", tương ứng.

Nối văn bản dễ thực hiện và tương thích nhất với phương thức nguồn, nhưng việc tăng sử dụng bộ nhớ là bậc hai theo độ dài của nội dung được tăng cường.

Vector Key-value. Để tránh việc tăng bộ nhớ bậc hai, chúng tôi nối các biểu diễn key-value của các token trong các câu quan trọng trong encoder self-attention, và tiêm trực tiếp chúng vào summarizer encoder. Việc tăng bộ nhớ chỉ tuyến tính theo độ dài của nội dung được tăng cường.

Cụ thể, summarizer encoder đầu tiên mã hóa tất cả các đoạn tài liệu và thu được các biểu diễn (tức là đầu ra encoder) của các token thuộc về các câu quan trọng được trích xuất. Trong quá trình huấn luyện, các biểu diễn token của những câu này được nối với các ma trận key-value trong encoder self-attention trong khi ma trận query vẫn ở dạng ban đầu. Tối đa 1024 token được nối thông qua cùng phương pháp bao gồm cho nối văn bản, để ưu tiên các

5Các chiến lược bao gồm khác có thể được khám phá trong công việc tương lai.

--- TRANG 6 ---
Mô hình R-1 ↑R-2↑R-L↑Ent Prec ↑SummaC ↑Disco↓Ent Graph ↑GPU Mem ↓
Se3 46.56 23.22 44.36 98.24 14.71 7.37 1.41 11.1
BlockAttn 57.46 26.78 54.82 97.45 20.43 5.91 2.05 25.6
Longformer 57.40 26.92 54.70 97.52 20.39 5.68 2.05 25.3
LongT5 54.21 24.87 51.06 96.41 13.34 4.81 1.56 25.4
Unlimiformer 56.35 25.94 53.83 92.19 6.05 5.36 1.96 27.0
Extract-Abstract 56.89 24.76 54.26 92.82 22.07 4.03 2.09 13.2
PageSum 56.80 23.26 54.11 89.56 6.82 3.04 1.88 24.9
AWESOME sử dụng chỉ Bộ nhớ Ngoài
Compressive 50.71†23.91 48.45†89.17 15.34 5.16†1.94†12.5
Attentive (Attn) 58.44∗27.71∗55.98∗98.33 18.98†3.62†1.98†14.0
AWESOME sử dụng chỉ Nội dung Nổi bật Toàn cục
Text-concat (Txt) 56.65†27.68∗54.11†97.93 12.23 5.05†2.09†12.0
Key-value Vectors 55.02†26.39†52.41†98.22 11.52 4.75†1.75†14.3
AWESOME (Attn + Txt) 58.76∗28.18∗56.05∗98.31 19.22†3.86†2.03†14.8

Bảng 2: Kết quả trên GovReport. Kết quả tốt nhất và tốt thứ hai mỗi metric được in đậm và gạch chân. Kết quả của các biến thể AWESOME tốt hơn tất cả so sánh và Se3 được tô màu xanh lá và xanh dương, tương ứng. AWESOME chỉ với attentive memory và phiên bản đầy đủ của nó thêm sử dụng nội dung nổi bật thông qua nối văn bản đạt điểm ROUGE cao nhất (màu xanh lá) và tương đương hoặc tốt hơn về tính trung thực (Ent Prec & SummaC) và tính mạch lạc (Disco & Ent Graph) so với mô hình cơ sở Se3. ∗: mô hình của chúng tôi tốt hơn tất cả so sánh với kiểm tra ngẫu nhiên hóa xấp xỉ (p <0.0005); †: mô hình của chúng tôi tốt hơn Se3 (p <0.0005).

câu ngoài cùng. Một ý tưởng tương tự đã được sử dụng bởi Memorizing Transformer (Wu et al., 2022a) để bao gồm các biểu diễn văn bản được truy xuất từ các đoạn quá khứ cho mô hình hóa ngôn ngữ dạng dài. Phương pháp của chúng tôi khác biệt ở hai khía cạnh. Đầu tiên, chúng tôi trích xuất biểu diễn từ các đoạn tương lai, điều này quan trọng để xác định chính xác nội dung nổi bật. Thứ hai, chúng tôi áp dụng một phép chiếu có thể học cho các biểu diễn được tăng cường trước khi nối key-value. Quá trình này quan trọng trong việc cải thiện khả năng tương thích với các ma trận key-value ban đầu.

4 Thiết lập Thực nghiệm
Dataset. Chúng tôi tiến hành thực nghiệm trên GovReport (Huang et al., 2021), QMSum (Zhong et al., 2021), SummScreen (Chen et al., 2022), arXiv (Cohan et al., 2018), và BookSum (Kryscinski et al., 2022). Độ dài đầu vào trung bình của các dataset này dao động từ 6K đến 143K (Phụ lục C.1).

Thiết lập Thực nghiệm và So sánh. Các thực nghiệm chính của chúng tôi được tiến hành với ràng buộc bộ nhớ GPU là 27GB. Đối với mỗi mô hình, chúng tôi cắt ngắn đầu vào sao cho việc sử dụng bộ nhớ GPU tối đa trong quá trình huấn luyện không vượt quá ràng buộc khi gradient checkpointing (Chen et al., 2016) bị tắt. Ràng buộc được chọn cụ thể sao cho các baseline hoạt động hợp lý. Phụ lục C.2 cung cấp thông tin về số lượng token đầu vào tối đa có thể tuân thủ ràng buộc cho các mô hình khác.

Đối với baseline, ngoài mô hình chia để trị Se3 (Moro và Ragazzi, 2022), chúng tôi so sánh với các hệ thống tóm tắt tài liệu dài tiên tiến hoặc phổ biến bao gồm BlockAttn (Phang et al., 2022), Longformer (Beltagy et al., 2020), LongT5 (Guo et al., 2022), và Unlimiformer (Bertsch et al., 2023). Chúng tôi cũng bao gồm một mô hình extract-then-abstract (Extract-Abstract) và PageSum (Liu et al., 2022) tận dụng trọng số động, như được thảo luận trong §2. Tất cả các mô hình được khởi tạo từ BART-large, ngoại trừ LongT5 được pre-train trên dữ liệu dạng dài. Chi tiết về các mô hình baseline được báo cáo trong Phụ lục D.

Các Metric Đánh giá. Chúng tôi đánh giá tính thông tin của bản tóm tắt bằng ROUGE (Lin, 2004). Để đo tính mạch lạc, chúng tôi sử dụng DiscoScore (Zhao et al., 2022) (Disco), một metric dựa trên tham chiếu đánh giá tính mạch lạc diễn ngôn bằng cách so sánh tần suất và ngữ nghĩa của focus (ví dụ: danh từ) giữa bản tóm tắt hệ thống và tham chiếu. Chúng tôi cũng báo cáo một metric mạch lạc dựa trên đồ thị không tham chiếu (Guinaudeau và Strube, 2013) (Ent Graph), đo độ kết nối của các câu tóm tắt được liên kết bởi thực thể, phản ánh tính mạch lạc của các chuyển đổi chủ đề. Đối với tính trung thực của bản tóm tắt, chúng tôi làm theo công trình trước về tạo văn bản (Iv et al., 2022) và hiển thị độ chính xác của các thực thể (Ent Prec) trong bản tóm tắt đối với tài liệu. Thêm vào đó, một metric trung thực dựa trên mô hình gần đây, SummaC (Laban et al., 2022), được sử dụng.

Cuối cùng, chúng tôi hiển thị kích thước tối đa của bộ nhớ GPU được phân bổ bởi mỗi mô hình trong quá trình huấn luyện.

--- TRANG 7 ---
Se3: V A được yêu cầu công bố thông tin về thời gian chờ cuộc hẹn tại mỗi cơ sở y tế V A cho chăm sóc chính, chăm sóc chuyên khoa, và chăm sóc bệnh viện và dịch vụ y tế, mà nó thực hiện thông qua hai trang web công cộng. V A đã thực hiện một số hành động để giải quyết các thiếu sót mà GAO phát hiện trong đo lường thời gian chờ và thực hiện chính sách lập lịch của mình. Đối với đo lường thời gian chờ, những hành động này bao gồm thay đổi định nghĩa đo lường thời gian chờ, cung cấp và ghi chép đào tạo người lập lịch, và cải thiện giám sát thông qua kiểm toán, tất cả đều trong trạng thái biến động trong 6 năm qua. Vào ngày 12 tháng 7 năm 2019, V A đã cung cấp cho GAO các cập nhật bổ sung về nỗ lực thực hiện các khuyến nghị liên quan của GAO.

AWESOME: GAO khuyến nghị rằng V A nên làm rõ chính sách lập lịch của mình để định nghĩa tốt hơn ngày mong muốn, hoặc xác định các biện pháp thời gian chờ rõ ràng hơn không phải chịu sự diễn giải và dễ gây lỗi của người lập lịch. V A đồng ý với khuyến nghị, mà GAO đã xác định là trong số những khuyến nghị xứng đáng được chú ý ưu tiên. V A đã thực hiện một số hành động để giải quyết các khuyến nghị của GAO về những thiếu sót mà GAO phát hiện trong đo lường thời gian chờ và thực hiện chính sách lập lịch của mình. Đối với đo lường thời gian chờ, những hành động này bao gồm thay đổi định nghĩa đo lường thời gian chờ, cung cấp và ghi chép đào tạo người lập lịch, và cải thiện giám sát thông qua kiểm toán, tất cả đều trong trạng thái biến động trong 6 năm qua. Vào ngày 12 tháng 7 năm 2019, V A đã cung cấp cho GAO các cập nhật bổ sung về nỗ lực thực hiện các khuyến nghị liên quan của GAO.

Bảng 3: Các đoạn tóm tắt được tạo bởi Se3 và AWESOME. Bản tóm tắt của AWESOME mạch lạc hơn, với các chuyển đổi tự nhiên xung quanh "khuyến nghị của GAO", trong khi Se3 đột ngột giới thiệu chủ đề.

5 Kết quả
Chúng tôi báo cáo kết quả của tất cả các biến thể AWESOME và các mô hình so sánh trên GovReport trong Bảng 2.

So với Se3, các biến thể AWESOME đạt hiệu suất tốt hơn một cách nhất quán trên cả điểm ROUGE và điểm mạch lạc, chỉ ra tầm quan trọng của việc duy trì ngữ cảnh toàn cục để ước lượng độ nổi bật chính xác của nội dung cục bộ và thực thi các chuyển đổi mạch lạc qua các bản tóm tắt cấp đoạn. Điều này cũng có thể được chứng minh bởi các đầu ra mẫu trong Bảng 3. Các bản tóm tắt được tạo bởi Se3 có xu hướng ngắn hơn, vì Se3 không thể lập kế hoạch ở cấp độ toàn cục. Về tính trung thực, AWESOME với attentive memory có độ chính xác thực thể tốt nhất trong tất cả các mô hình và cũng cải thiện SummaC so với Se3, trong khi chỉ tăng cường AWESOME với nội dung nổi bật toàn cục làm hại tính trung thực. Kiểm tra các đầu ra mô hình, chúng tôi thấy rằng việc sử dụng attentive memory cải thiện hiểu biết các khái niệm của các phụ thuộc dài hạn, ví dụ: kết nối một chiến lược với thông tin liên quan của nó xuất hiện sớm hơn trong báo cáo.

Trong hai loại cơ chế bộ nhớ ngoài, attentive memory vượt trội hơn compressive memory trên tất cả các metric, điều này làm nổi bật lợi thế của việc cập nhật ngữ cảnh được lưu trữ một cách thích ứng.

Mô hình R-1 ↑R-2↑R-L↑Disco↓GPU↓
Se3 29.28 10.51 25.93 0.77 8.1
BlockAttn 30.76 8.26 26.49 0.50 22.8
Longformer 29.18 7.82 24.94 3.07 26.5
LongT5 31.88 10.07 27.82 0.44 25.4
Unlimiformer 30.57 8.82 26.89 0.49 26.9
Extract-Abstract 17.63 5.65 16.02 4.02 10.3
PageSum 29.55 7.38 26.11 0.31 21.5
AWESOME
Attn Only 34.86†12.69 31.09∗0.68 12.9
Attn + Txt 31.16†10.11 27.66 0.69 13.3

Bảng 4: Kết quả trên bản ghi cuộc họp trong QMSum. Được trang bị chỉ attentive memory, AWESOME đạt điểm ROUGE tốt nhất. Mặc dù tốt hơn một số baseline, việc thêm nội dung nổi bật được trích xuất không tiếp tục tăng hiệu suất, do hiệu suất thấp của extractor trên dữ liệu đối thoại.

Trong khi đó, việc nối trực tiếp nội dung nổi bật với đầu vào mang lại điểm ROUGE cao hơn so với việc tiêm vector key-value vào phép tính attention, mặc dù cách sau tốn ít bộ nhớ hơn. Chúng tôi tin rằng tăng cường dựa trên ngôn ngữ tự nhiên interleave tốt hơn với đoạn tài liệu, phản hồi những phát hiện của công trình trước về việc sử dụng retrieval cho trả lời câu hỏi (Wu et al., 2022b).

Quan trọng là, dưới ràng buộc bộ nhớ GPU nghiêm ngặt, AWESOME với các cơ chế bộ nhớ ngoài và tăng cường nội dung nổi bật toàn cục đạt điểm ROUGE tốt nhất trong tất cả các mô hình, trong khi đạt kết quả cạnh tranh trên các biện pháp khác. Mặc dù các mô hình efficient attention và PageSum có thể hoạt động xuất sắc khi được cung cấp GPU có dung lượng cao hơn như trong công trình gốc, chúng tạo ra các bản tóm tắt ít thông tin hơn khi cần cắt ngắn để tuân thủ ràng buộc bộ nhớ, nhấn mạnh tầm quan trọng của việc nghiên cứu các mô hình tóm tắt tài liệu dài tiết kiệm bộ nhớ.

Hơn nữa, với việc thêm bộ nhớ ngoài có chọn lọc, AWESOME chỉ thêm khoảng 4GB việc sử dụng bộ nhớ GPU, tăng cường hiệu suất mô hình một cách hiệu quả.

Trên QMSum (Bảng 4), AWESOME với attention-based memory vượt trội hơn tất cả so sánh về điểm ROUGE. Trong khi các bản tóm tắt của mô hình chúng tôi mạch lạc hơn so với các bản tóm tắt của Se3, được đo bằng DiscoScore, sự khác biệt giữa tất cả các mô hình ít rõ rệt hơn so với những gì trên GovReport. Điều này là do QMSum chứa các bản tóm tắt ngắn hơn GovReport

--- TRANG 8 ---
Mô hình R-1 ↑R-2↑R-L↑Ent G↑GPU↓
Se3 38.09 11.30 36.56 0.50 11.3
BlockAttn 32.01 8.99 30.90 1.61 25.7
Longformer 42.78 13.21 41.34 0.97 25.3
LongT5 42.03 12.67 40.76 1.03 25.4
Unlimiformer 35.17 11.98 34.28 1.33 27.0
Extract-Abstract 19.95 5.58 19.70 0.06 13.1
AWESOME
Attn Only 46.05†13.09†44.21†0.81†13.2
Attn + Txt 45.30†12.63†43.51†0.90†14.2

Bảng 5: Kết quả trên bản ghi truyền hình trong SummScreen. Chúng tôi báo cáo Ent Graph thay vì DiscoScore, vì DiscoScore gặp lỗi khi xác định focus. AWESOME với attentive memory đạt điểm R1 và RL tốt nhất, trong khi độ chính xác thấp của nội dung nổi bật được trích xuất dẫn đến sự sụt giảm hiệu suất của summarizer.

(69 so với 553), do đó bao gồm ít chuyển đổi chủ đề hơn. Chúng tôi cũng thấy rằng extractor hoạt động kém trên QMSum, dẫn đến kết quả suy giảm sau khi tăng cường mô hình của chúng tôi với nội dung nổi bật được trích xuất. Cụ thể, điểm F1 của extractor trên tập test chỉ là 1.29, trái ngược với 27.85 trên GovReport. So với mô hình của chúng tôi, mô hình extract-then-abstract dễ bị lỗi hơn và tạo ra các bản tóm tắt chất lượng thấp nhất trên QMSum.

Xu hướng này được quan sát tương tự trên SummScreen (Bảng 5) và arXiv (Bảng 6), nơi phương pháp extract-then-abstract hoạt động kém và việc thêm nội dung được trích xuất dẫn đến sự sụt giảm hiệu suất của AWESOME do hiệu suất thấp của extractor. Trong khi đó, AWESOME với attentive memory có thể đạt điểm ROUGE-1 và ROUGE-L tốt nhất. Trên arXiv, các mô hình sử dụng efficient attention đạt điểm ROUGE cao hơn, bởi vì việc cắt ngắn tài liệu arXiv có ít ảnh hưởng đến việc tạo bản tóm tắt—các bài báo arXiv có phân phối nội dung nổi bật không đều nhất, nơi chỉ khoảng 10% bigram nổi bật mới nằm trong nửa sau của tài liệu (Huang et al., 2021).

Cuối cùng, thực nghiệm trên BookSum cho thấy phương pháp chia để trị tạo ra các bản tóm tắt tốt hơn cho tiểu thuyết dài, trong khi phương pháp của chúng tôi có thể tiếp tục tăng hiệu suất của nó (Bảng 7). Tuy nhiên, chúng tôi thấy cần thiết phải kết hợp bộ nhớ ngoài vào tất cả các layer, gợi ý một tương tác phức tạp hơn của bộ nhớ ngoài với quá trình tóm tắt cho cốt truyện tiểu thuyết. Không giống như các loại tài liệu khác được kiểm tra, cốt truyện tiểu thuyết thường tuần tự với ít dư thừa hơn, điều này giảm tính cần thiết của cơ chế bộ nhớ.

Mô hình R-1 ↑R-2↑R-L↑Disco↓GPU↓
Se3 40.74 17.96 36.87 1.33 12.8
BlockAttn 49.12 21.69 44.40 1.77 25.7
Longformer 48.59 21.45 43.99 2.17 25.2
LongT5 48.25 20.74 43.41 0.97 25.5
Unlimiformer 47.78 20.58 43.22 1.22 26.8
Extract-Abstract 42.37 16.43 38.62 1.03 15.3
PageSum 46.01 18.77 41.55 0.88 26.2
AWESOME
Attn Only 42.51†18.96†38.56†1.30 16.0
Attn + Txt 44.20†18.89†40.07†1.32 16.5

Bảng 6: Kết quả trên bài báo arXiv. Các biến thể AWESOME một lần nữa vượt trội hơn Se3. Đối với 80% tài liệu arXiv, các mô hình efficient attention và PageSum có thể huấn luyện đầy đủ trên nửa đầu của chúng, bao phủ 90% nội dung nổi bật xuất hiện trong tham chiếu (Huang et al., 2021), do đó điểm ROUGE tốt hơn so với các mô hình mã hóa các đoạn nhỏ hơn.

Mô hình R-1 ↑R-2↑R-L↑Disco↓GPU↓
Se3 40.78 10.16 39.77 10.46 11.5
BlockAttn 23.45 3.09 22.09 190.27 25.7
Longformer 20.20 2.45 18.55 204.48 25.3
LongT5 33.15 6.74 32.62 24.24 25.5
Unlimiformer 38.09 9.55 37.41 47.72 27.0
AWESOME (Attn) 41.11 10.63 40.20 10.36 24.0

Bảng 7: Kết quả trên tiểu thuyết trong BookSum. AWESOME với attentive memory trong tất cả các layer đạt hiệu suất tốt nhất trên tất cả các metric. Các phương pháp yêu cầu extractor ngoài không được bao gồm do chi phí tính toán của việc xây dựng oracle trích xuất cho tiểu thuyết dài.

6 Kết luận
Chúng tôi trình bày AWESOME để tóm tắt tài liệu dài trong thiết lập bị hạn chế bộ nhớ. Dựa trên chiến lược chia để trị, AWESOME sử dụng hai cơ chế để thu thập ngữ cảnh toàn cục và cải thiện chất lượng bản tóm tắt. Đầu tiên, các bộ nhớ ngoài trên encoder và decoder được sử dụng để theo dõi nội dung tài liệu đã đọc trước đó và các bản tóm tắt tương ứng. Thứ hai, encoder được thông báo về nội dung nổi bật toàn cục được dự đoán bởi extractor thông qua nối văn bản hoặc biểu diễn. Trên năm dataset tóm tắt, AWESOME tạo ra các bản tóm tắt với tính thông tin, trung thực và mạch lạc tốt hơn so với hệ thống chia để trị baseline. Dưới cùng ràng buộc bộ nhớ, AWESOME vượt trội hơn các mô hình cạnh tranh tận dụng efficient attention hoặc trích xuất động để bảo toàn ngữ cảnh toàn cục, làm nổi bật hiệu quả của nó trong việc cung cấp ngữ cảnh toàn cục.

--- TRANG 9 ---
7 Hạn chế
Cơ chế bộ nhớ ngoài của AWESOME bị hạn chế chỉ hoạt động từ các đoạn quá khứ đến đoạn hiện tại. Điều này có nghĩa là mô hình không tận dụng thông tin chứa trong các đoạn tương lai, có thể liên quan đến việc hiểu toàn diện đoạn hiện tại. Để giải quyết hạn chế này, chúng tôi đã thiết kế cơ chế tăng cường nội dung nổi bật toàn cục để bao phủ ngữ cảnh từ các đoạn tương lai, tuy nhiên các giải pháp tiên tiến hơn có thể được khám phá trong công việc tương lai. Ví dụ, trên encoder, việc làm cho bộ nhớ ngoài hai chiều là một phương pháp tiềm năng.

Trong khi tiết kiệm bộ nhớ, cơ chế bộ nhớ ngoài của AWESOME đòi hỏi thời gian chạy dài hơn do tính chất tuần hoàn của nó. Nhu cầu tính toán tuần hoàn có thể dẫn đến yêu cầu xử lý tăng, có thể ảnh hưởng đến các ứng dụng thời gian thực hoặc các tình huống mà phản hồi nhanh là quan trọng. Thời gian chạy của các mô hình khác nhau được cung cấp trong Phụ lục B.1 để tham khảo. Mặc dù mô hình của chúng tôi chậm hơn so với LongT5 và Se3, nó vẫn vượt trội hơn một số mô hình cạnh tranh khác về tốc độ, và chúng tôi sẽ điều tra các phương pháp giảm thời gian chạy trong công việc tương lai.

8 Xem xét Đạo đức
Chúng tôi dự đoán rằng một trong những trường hợp sử dụng chính của AWESOME là cho phép người dùng thông thường có thiết bị tính toán với bộ nhớ hạn chế nhanh chóng hiểu các chính sách chính phủ và các loại tài liệu dài khác. Tuy nhiên, chúng tôi nhận ra rằng các bản tóm tắt được tạo bởi hệ thống có thể không bao phủ toàn diện nội dung nổi bật cần thiết để hiểu chính xác các chính sách, gây ra rủi ro từ mất vốn đến trách nhiệm pháp lý. Hơn nữa, các bản tóm tắt hệ thống có thể chứa các tuyên bố không thể được xác minh thông qua tài liệu, điều này tiếp tục thêm vào rủi ro của việc triển khai thực tế. Chúng tôi đề xuất các nhà phát triển có ý định sử dụng mô hình của chúng tôi cho ứng dụng thực tế nên nghiên cứu cẩn thận các đầu ra của mô hình chúng tôi trước khi triển khai thực tế.

Tài liệu tham khảo
Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Amanda Bertsch, Uri Alon, Graham Neubig, và Matthew R. Gormley. 2023. Unlimiformer: Long-range transformers with unlimited length input.

Aydar Bulatov, Yuri Kuratov, và Mikhail Burtsev. 2022. Recurrent memory transformer. In Advances in Neural Information Processing Systems.

Mingda Chen, Zewei Chu, Sam Wiseman, và Kevin Gimpel. 2022. SummScreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602–8615, Dublin, Ireland. Association for Computational Linguistics.

Tianqi Chen, Bing Xu, Chiyuan Zhang, và Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, và Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, và Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988, Florence, Italy. Association for Computational Linguistics.

Alexios Gidiotis và Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of long documents. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:3029–3040.

Camille Guinaudeau và Michael Strube. 2013. Graph-based local coherence modeling. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93–103, Sofia, Bulgaria. Association for Computational Linguistics.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, và Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724–736, Seattle, United States. Association for Computational Linguistics.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, và Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419–1436, Online. Association for Computational Linguistics.

--- TRANG 10 ---
Robert Iv, Alexandre Passos, Sameer Singh, và Ming-Wei Chang. 2022. FRUIT: Faithfully reflecting updated information in text. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3670–3686, Seattle, United States. Association for Computational Linguistics.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations.

Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, và Dragomir Radev. 2022. BOOKSUM: A collection of datasets for long-form narrative summarization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6536–6558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Philippe Laban, Tobias Schnabel, Paul N. Bennett, và Marti A. Hearst. 2022. SummaC: Re-visiting NLI-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163–177.

Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg, và Mohit Bansal. 2020. MART: Memory-augmented recurrent transformer for coherent video paragraph captioning. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2603–2614, Online. Association for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Chuan Li. 2022. Best gpu for deep learning in 2022 (so far).

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

Yang Liu và Mirella Lapata. 2019. Hierarchical transformers for multi-document summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070–5081, Florence, Italy. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb, Chenguang Zhu, Ahmed H. Awadallah, và Dragomir Radev. 2022. Leveraging locality in abstractive text summarization.

Ziming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang, Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang Zhu, Ahmed Awadallah, và Dragomir Radev. 2022. DYLE: Dynamic latent extraction for abstractive long-input summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1687–1698, Dublin, Ireland. Association for Computational Linguistics.

Gianluca Moro và Luca Ragazzi. 2022. Semantic self-segmentation for abstractive summarization of long documents in low-resource regimes. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):11085–11093.

OpenAI. 2023. Gpt-4 technical report.

Jason Phang, Yao Zhao, và Peter J Liu. 2022. Investigating efficiently extending transformers for long input summarization. arXiv preprint arXiv:2208.04347.

Jonathan Pilault, Raymond Li, Sandeep Subramanian, và Chris Pal. 2020. On extractive and abstractive neural document summarization with transformer language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9308–9319, Online. Association for Computational Linguistics.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, và Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.

Nils Reimers và Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, và Da-Cheng Juan. 2020. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438–9447. PMLR.

--- TRANG 11 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, và Paul Christiano. 2021. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. 2022a. Memorizing transformers. In International Conference on Learning Representations.

Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, và Sebastian Riedel. 2022b. An efficient memory-augmented transformer for knowledge-intensive nlp tasks.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2021. Big bird: Transformers for longer sequences.

Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed Awadallah, Dragomir Radev, và Rui Zhang. 2022. Summn: A multi-stage summarization framework for long input dialogues and documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1592–1604, Dublin, Ireland. Association for Computational Linguistics.

Wei Zhao, Michael Strube, và Steffen Eger. 2022. Discoscore: Evaluating text generation with bert and discourse coherence.

Yao Zhao, Mohammad Saleh, và Peter J Liu. 2020. Seal: Segment-wise extractive-abstractive long-form text summarization. arXiv preprint arXiv:2006.10213.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, và Dragomir Radev. 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921, Online. Association for Computational Linguistics.

A Kiến trúc Chia để Trị
Chúng tôi chọn Se3 (Moro và Ragazzi, 2022) làm kiến trúc chia để trị cơ sở của chúng tôi vì nó có thể được áp dụng cho bất kỳ cặp tài liệu-tóm tắt nào. Để tạo dữ liệu huấn luyện chia để trị cho tóm tắt, đối với mỗi cặp tài liệu-tóm tắt,

tài liệu đầu tiên được chia thành các đoạn (§A.1) và mỗi câu tóm tắt sau đó được gán cho một đoạn tài liệu như một phần của mục tiêu tạo (§A.2).

A.1 Phân đoạn Tài liệu
Thuật toán 1: Phân đoạn Tài liệu
Dữ liệu: Tài liệu đầu vào doc; Độ dài min, max của đoạn lmin, lmax
1segs←[];
2currSeg ←[];
3foreach sent in doc do
4 if len(currSeg) < lmin then
5 currSeg ←currSeg + [sent];
6 end
7 else if len(currSeg) > lmax then
8 segs←segs + [currSeg];
9 currSeg ←[sent];
10 end
11 else
12 nextSeg ←pseudoSegment;
13 if sim(nextSeg, sent) > sim(currSeg, sent) then
14 segs←segs + [currSeg];
15 currSeg ←[sent];
16 end
17 else
18 currSeg ←currSeg + [sent]
19 end
20 end
21end
22segs←segs + [currSeg];
23return segs

Độ dài của mỗi đoạn tài liệu nằm giữa 512 và 768 token. Trong quá trình phân đoạn, thuật toán lặp qua tất cả các câu tài liệu, như được hiển thị trong Thuật toán 1. Một câu tài liệu sẽ được thêm vào đoạn hiện tại nếu đoạn chứa ít hơn 512 token. Đoạn hiện tại sẽ được hoàn thiện nếu đoạn hiện tại chứa hơn 768 token hoặc câu hiện tại tương tự về mặt ngữ nghĩa với đoạn giả tiếp theo hơn so với đoạn hiện tại, trong đó đoạn giả tiếp theo được tạo bằng cách bao gồm các câu tương lai cho đến khi đạt 512 token. Để đo độ tương tự giữa câu hiện tại và một đoạn, chúng tôi sử dụng độ tương tự cosine trung bình giữa biểu diễn của câu hiện tại và biểu diễn của các câu trong đoạn. Biểu diễn câu

--- TRANG 12 ---
Batch / s
0.00.51.01.5
Se3
BlockAttnLongformerLongT5PageSumAWESOMEHình 2: Thời gian chạy (batch mỗi giây) của mỗi mô hình. Số lượng batch được xử lý mỗi giây cao hơn cho thấy tốc độ chạy nhanh hơn. Tất cả các mô hình sử dụng batch size là 1 và đầu vào được cắt ngắn thành 16384 token.

được thu được bằng cách sử dụng Sentence Transformer (Reimers và Gurevych, 2019) với mô hình all-roberta-large-v1.

A.2 Gán Mục tiêu
Đối với mỗi câu trong bản tóm tắt tham chiếu, chúng tôi tính điểm ROUGE của nó với các đoạn tài liệu. Câu sau đó sẽ được gán cho đoạn tài liệu mà với nó mang lại điểm ROUGE-1 và ROUGE-2 cao nhất.

B Kết quả Bổ sung
B.1 Thời gian Chạy
Chúng tôi so sánh thời gian chạy mô hình trên GovReport (Hình 2). Tài liệu đầu vào được cắt ngắn thành 16384 token và mỗi mô hình được huấn luyện riêng trong 1000 bước với batch size là 1. Không có chương trình tính toán nặng khác chạy cùng lúc. Trong khi AWESOME mất thời gian dài hơn để hoàn thành huấn luyện so với Se3, nó vẫn là mô hình nhanh thứ ba.

C Chi tiết Dataset
C.1 Thống kê
Chúng tôi tiến hành thực nghiệm trên năm dataset tóm tắt tài liệu dài với các thể loại đa dạng. GovReport (Huang et al., 2021) chứa các báo cáo dài và bản tóm tắt của chúng được viết bởi các cơ quan nghiên cứu chính phủ. QMSum (Zhong et al., 2021) là dataset tóm tắt bản ghi cuộc họp dài tập trung vào truy vấn, với nội dung đáng tóm tắt được phân tán khắp tài liệu. Chúng tôi thêm truy vấn vào tất cả các đoạn. Chúng tôi tiếp tục sử dụng dataset tóm tắt kịch bản, SummScreen (Chen et al.,

# Mẫu # Từ
Dataset Train Dev Test Doc Summ
GovReport 17,516 974 973 9,409 553
QMSum 1,257 272 279 9,070 70
SummScreen 18,915 1,795 1,793 6,421 381
arXiv 203,037 6,436 6,440 6,030 273
BookSum 314 45 46 143,301 1,294

Bảng 8: Thống kê của các dataset được sử dụng trong thực nghiệm của chúng tôi.

Dataset
Mô hình Gov arXiv QMSum SumScrn Book
Se3 50x 50x 50x 50x 50x
Ext-Abs†1x (∞) 1x (∞) 1x (∞) 1x (∞) -
BlockAttn 6x 6x 8x 6x 6x
Longformer 8x 8x 8x 8x 8x
LongT5 6x 6x 6x 6x 6x
Unlimiformer 2x 2x 2x 2x 2x
PageSum 3x 5x 2x - -
AWESOME 50x 50x 50x 50x 50x

Bảng 9: Ngưỡng cắt ngắn (nhân với 1024) được sử dụng bởi mỗi mô hình trên các dataset khác nhau để tuân thủ ràng buộc bộ nhớ trong quá trình huấn luyện. †: Đối với mô hình extract-then-abstract, abstractor có độ dài đầu vào tối đa là 1024, trong khi extractor có thể tiêu thụ tất cả các câu trong tài liệu.

2022), chứa bản ghi của loạt truyền hình. Tập con TMS, với nhiều mẫu hơn và bản tóm tắt dài hơn, được chọn. Hơn nữa, chúng tôi thực nghiệm với các bài báo khoa học và tóm tắt của chúng từ arXiv (Cohan et al., 2018). Cuối cùng, chúng tôi kiểm tra các mô hình của chúng tôi trên việc tóm tắt tiểu thuyết đầy đủ trong BookSum (Kryscinski et al., 2022). Đối với tất cả các dataset, chúng tôi sử dụng các phân chia train/dev/test chính thức nếu các tệp dữ liệu gốc của chúng được phát hành.

Thống kê của các dataset được báo cáo trong Bảng 8. Đối với GovReport6, QMSum7, và SummScreen (Chen et al., 2022), chúng tôi sử dụng dữ liệu được phát hành bởi các bài báo gốc. Đối với arXiv, chúng tôi sử dụng phiên bản được cung cấp bởi Huggingface Datasets.8 Vì các tệp dữ liệu gốc cho BookSum không được phát hành do bản quyền tóm tắt, chúng tôi sử dụng phiên bản được tái tạo bởi Unlimiformer (Bertsch et al., 2023).

C.2 Cắt ngắn Đầu vào
Trong các thực nghiệm chính của chúng tôi, chúng tôi sử dụng ràng buộc bộ nhớ GPU là 27GB. Vì một số mô hình baseline yêu cầu độ dài đầu vào phải là bội số của 1024, việc đặt ràng buộc 24GB, một số phổ biến hơn,

6https://gov-report-data.github.io/
7https://github.com/Yale-LILY/QMSum
8https://huggingface.co/datasets/scientific_papers

--- TRANG 13 ---
sẽ dẫn đến việc cắt ngắn thêm và sự sụt giảm hiệu suất đáng kể.

Để phù hợp các mô hình với ràng buộc bộ nhớ của chúng tôi, chúng tôi cắt ngắn đầu vào mô hình. Các ngưỡng cắt ngắn được sử dụng bởi mỗi mô hình trên các dataset khác nhau được hiển thị trong Bảng 9. Mặc dù Se3 và AWESOME về lý thuyết duy trì việc tiêu thụ bộ nhớ GPU nhất quán trong quá trình huấn luyện bất kể số lượng token đầu vào được xử lý, chúng tôi đã chọn hạn chế số lượng token đầu vào tối đa trong một mẫu huấn luyện thành 51200 để có thời gian huấn luyện hợp lý.

D Chi tiết Triển khai
Baseline. BlockAttn và Longformer sử dụng block-wise attention (Phang et al., 2022) và sliding-window attention (Beltagy et al., 2020), trong đó một global token có thể chú ý đến và được chú ý bởi tất cả các token, trong khi các token khác chỉ có thể chú ý đến các token trong cùng block hoặc window. LongT5 (Guo et al., 2022) là một mô hình sliding-window attention được pre-train trên các chuỗi dài, và Unlimiformer (Bertsch et al., 2023) mở rộng BART bằng cách chọn các token đầu vào được chú ý thông qua tìm kiếm KNN. Đối với phương pháp extract-then-abstract, chúng tôi sử dụng cùng extractor như trong tăng cường nội dung nổi bật toàn cục của mô hình chúng tôi, và abstractor lấy làm đầu vào các câu được trích xuất oracle trong quá trình huấn luyện. Cuối cùng, PageSum (Liu et al., 2022) tổng hợp các biểu diễn đầu ra được cung cấp bởi các đoạn tài liệu khác nhau với trọng số động.

Extractor. Extractor đầu tiên sử dụng RoBERTa (Liu et al., 2019) để mã hóa mỗi câu và lấy trung bình của các đầu ra layer cuối cùng làm biểu diễn câu. Sau đó nó áp dụng self-attention trên tất cả các biểu diễn câu. Các biểu diễn kết quả được chuyển đổi thành điểm trích xuất sau khi áp dụng multi-layer perception với một hidden layer. Extractor được huấn luyện với các nhãn trích xuất oracle được xây dựng bằng cách tìm kiếm tham lam cho các câu tài liệu tối đa hóa tổng điểm ROUGE-1 và ROUGE-2, so với bản tóm tắt tham chiếu. Chúng tôi không tính ROUGE-L như trong DYLE (Mao et al., 2022), vì việc tìm chuỗi con chung dài nhất tốn kém về mặt tính toán và không mang lại lợi ích hiệu suất.

Tham số Huấn luyện. Chúng tôi huấn luyện tất cả các mô hình với learning rate tối đa 5×10−5, ngoại trừ LongT5 được huấn luyện với learning rate tối đa 1×10−4. Chúng tôi sử dụng batch size chạy là 1 và áp dụng gradient accumulation để đạt batch size hiệu quả là 8. Số epoch huấn luyện là 3, 9, 6, 2, 10 trên GovReport, QMSum, SummScreen, arXiv, và BookSum, với các bước warmup là 300, 100, 300, 1000, và 40. Do chi phí tính toán của việc huấn luyện tóm tắt tài liệu dài, mỗi mô hình được huấn luyện cho một lần chạy duy nhất.

Kích thước Mô hình. AWESOME dựa trên BART-large9 và có 708 triệu tham số.

Cơ sở hạ tầng Tính toán. Tất cả thực nghiệm được tiến hành trên GPU RTX A6000.

Các Metric Đánh giá. Đối với ROUGE (Lin, 2004), chúng tôi sử dụng triển khai Python của Google.10 Mã chính thức cho DiscoScore (Zhao et al., 2022) được sử dụng11, cũng cung cấp triển khai của metric Ent Graph (Guinaudeau và Strube, 2013). Chúng tôi tự triển khai measure entity precision và chạy mã chính thức cho SummaC (Laban et al., 2022).12 Tất cả các metric được sử dụng đều là open-source và có thể được phân phối cho mục đích nghiên cứu.

9https://huggingface.co/facebook/bart-large
10https://pypi.org/project/rouge-score/
11https://github.com/AIPHES/DiscoScore
12https://github.com/tingofurro/summac
