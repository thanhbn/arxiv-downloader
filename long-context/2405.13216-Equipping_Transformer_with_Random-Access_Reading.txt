# 2405.13216.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2405.13216.pdf
# File size: 545366 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Equipping Transformer with Random-Access Reading
for Long-Context Understanding
Chenghao Yang* 1Zi Yang* 2Nan Hua2
Abstract
Long-context modeling presents a significant chal-
lenge for transformer-based large language mod-
els (LLMs) due to the quadratic complexity of the
self-attention mechanism and issues with length
extrapolation caused by pretraining exclusively on
short inputs. Existing methods address computa-
tional complexity through techniques such as text
chunking, the kernel approach, and structured at-
tention, and tackle length extrapolation problems
through positional encoding, continued pretrain-
ing, and data engineering. These approaches typi-
cally require sequential access to the document,
necessitating reading from the first to the last to-
ken. We contend that for goal-oriented reading of
long documents, such sequential access is not nec-
essary, and a proficiently trained model can learn
to omit hundreds of less pertinent tokens. Inspired
by human reading behaviors and existing empir-
ical observations, we propose random access , a
novel reading strategy that enables transformers
to efficiently process long documents without ex-
amining every token. Experimental results from
pretraining, fine-tuning, and inference phases val-
idate the efficacy of our method.
1. Introduction
Long context refers to an input context that exceeds the
maximum length limit,1making it impossible to process in
a single inference step. Examples of long context inputs
*Equal contribution1University of Chicago (Work is done at
Google as a student researcher)2Google Research. Correspon-
dence to: Chenghao Yang <yangalan1996@gmail.com >, Zi Yang
<ziy@google.com >.
Preliminary Work. Copyright 2024 by the author(s).
1We acknowledge the recent achievement building models that
can handle extremely long context, such as Gemini 1.5 (Reid et al.,
2024). However, it is impossible to expand the context window
indefinitely without chunking it at some point, and we believe
combining chunking with our proposed random-access reading
strategy can be more efficient than directly reading the input as a
whole.include multiple webpages (Zhou et al., 2023; Deng et al.,
2024), books (Mou et al., 2021), code repository (Jimenez
et al., 2023) and dialog histories (Yang & Ettinger, 2023).
Existing strategies predominantly employ a sequential-
access model, where both during training and inference, the
model processes tokens of documents in their original, se-
quential order (Dong et al., 2023; Huang et al., 2023). These
methods utilize various mechanisms to address challenges
such as quadratic complexity‚Äîthrough block-wise process-
ing (Qiu et al., 2020; Tay et al., 2020a; Liu et al., 2022; Ivgi
et al., 2023; Mohtashami & Jaggi, 2024), structured atten-
tion (Beltagy et al., 2020; Guo et al., 2022; Xiao et al., 2024;
Han et al., 2023), and linear approximations (Choromanski
et al., 2020; Peng et al., 2020; Ma et al., 2021; Nguyen et al.,
2022)‚Äîor to mitigate length extrapolation issues using tech-
niques like rotary positional embeddings (Peng et al., 2023;
Su et al., 2024) and continual training (Xiong et al., 2023;
Fu et al., 2024).
While these methods have been widely adopted and proven
effective, they treat every token as equally important and
overlook the fact that for user queries, only a small portion
of the information is relevant (Ding et al., 2020). Conse-
quently, many proposed solutions still suffer from computa-
tional overhead, particularly in online interactions between
humans and LLMs. Recent works in retrieval-augmented
generation (RAG) (Lewis et al., 2020; Shi et al., 2023) have
attempted to address this by incorporating an additional
retriever to bypass non-essential context and selectively re-
trieve relevant sections. However, due to an incomplete
understanding of the entire long context and a lack of ro-
bust supervisory signals, the overall performance of such
multi-module systems is significantly constrained by the
capabilities of the retriever (Mou et al., 2021; Zhang et al.,
2022). Furthermore, while these RAG systems require a
predefined top-K for any user query, our framework dynam-
ically determines the access pattern based on the query and
the context.
Inspired by strategies observed in proficient human readers,
who actively engage with long texts by developing predic-
tions of forthcoming content and selectively skip-reading
irrelevant sections (Paris et al., 1991; Pressley & Afflerbach,
2012), along with recent evidence suggesting that large mod-
1arXiv:2405.13216v1  [cs.CL]  21 May 2024

--- PAGE 2 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
els may inherently acquire the ability to access content at
arbitrary locations during pretraining (Fu et al., 2024), we
propose the Random-Access Reading .2This approach lever-
ages local perplexity information as a criterion to bypass
non-essential text, thereby significantly enhancing reading
efficiency.
StartChunksCont‚ÄôdChunks
(notimportant)Cont‚ÄôdChunks
(notimportant)EndChunks
Memory Memory MemoryModel I/O 
actually 
happenedOptional Model I/O
Traditional Sequential Access
ProposedRandomAccess
StartChunksCont‚ÄôdChunks
(notimportant)Cont‚ÄôdChunks
(notimportant)EndChunks
MemoryData 
Server
Figure 1. Illustration for our proposed random-access reading strat-
egy for long-context modeling. In the traditional sequential access
scenario (upper part of the figure), inputs are split into equal-sized
chunks and fed to the model in sequential order. In contrast, we
propose building an additional data server module (in the lower
part of the figure) that takes relevant statistics from the model and
decides which chunk it should read next.
The proposed architecture is depicted in Figure 1. Unlike
traditional I/O access, where each chunk of input is fed
sequentially to the model, we propose building a new data
server to handle the I/O process. The random access we pro-
pose involves the model transmitting relevant statistics3to
the data server, which then employs our specially designed
skipping mechanism (outlined in Section 3) to determine
the number of tokens to be skipped and subsequently fetch a
new chunk for the model. Optionally, we can use a memory
module4to provide additional context, helping the model
make better skipping decisions and maintain a coherent
understanding of the input text.
Through extensive experiments, we have validated the effec-
2Random access is typically defined as the capability to access
arbitrary positions, as in a Random-Access Machine (Knuth, 1997).
Within the scope of this paper, we focus on a simplified version
where we only allow the model to access arbitrary positions in the
future context and never look back, to achieve better efficiency.
3In this paper, we demonstrate the effectiveness of random-
access reading using pooled per-token cross entropy loss as such
statistics, although future research could explore alternative met-
rics.
4This will make our model similar to the landmark attention
mechanism proposed by (Mohtashami & Jaggi, 2023). We will
elaborate on the differences between our work and theirs in Sec-
tion 2 and discuss memory module implementation in Section 3.tiveness of our proposed method. Our findings include:
1.The skipping mechanism enhances model performance
in long-context language model pretraining.
2.Traditional short-text pretraining degrades model per-
formance below that of a randomly-initialized model.
However, with our fine-tuning using skipping mecha-
nism, a short-text model can be successfully adapted
to excel in long-context tasks, outperforming even our
random-access model pretrained from scratch.
3.Incorporating a memory module allows our random-
access model to achieve further significant improve-
ments, surpassing the previous state-of-the-art memory-
based models in pretraining (Wu et al., 2021) with only
26%of the training time.
4.Evaluating our random-access model on downstream
tasks using a modified TriviaQA (Joshi et al., 2017) task,
where all retrieved evidence from the original dataset is
concatenated to create an extremely challenging long-
context scenario‚Äîdemonstrates that more aggressive
skipping mechanism yield better performance. This con-
firms the method‚Äôs effectiveness and improved learning
efficiency.
2. Background
When applied to Transformer models, our proposed random-
access reading strategies can significantly reduce attention
computations by skipping many tokens. This aligns closely
with research on attention sparsification that uses local infor-
mation to enhance long-context modeling efficiency and ca-
pability. Notably, Liu et al. (2023) demonstrate the existence
of contextual sparsity that can be leveraged for computa-
tional acceleration. Chowdhury & Caragea (2023) illustrate
the effectiveness of using sliding windows and weighted de-
cay for long-context understanding tasks. Additionally, Fu
et al. (2023) find that a simple convolutional model performs
well on Long Range Arena tasks (Tay et al., 2020b). Chen
et al. (2023) develop a bottom-up hierarchical structure that
utilizes an LLM to read long documents, enabling direct
navigation to document sections relevant to specific queries
by following a tree path. Further, Han et al. (2023) and Xiao
et al. (2024) report that maintaining attention on only a few
initial tokens (termed ‚Äùattention sinks‚Äù), with each token
attending to just its neighbors, can significantly enhance
the efficiency of processing long contexts while preserving
performance. These works provides both empirical and
theoretical support for conducting attention sparsification,
and our random-access transformer takes a more aggressive
approach by directly skipping many tokens.
The most closely related concept to our random-access
model is the landmark attention mechanism introduced by
2

--- PAGE 3 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
Mohtashami & Jaggi (2023). In this approach, a landmark
token is appended to the end of each fixed-length chunk to
represent that chunk, and each token attends only to a lim-
ited number of these chunk-wise landmark tokens. While
this method facilitates random access for each token and sig-
nificantly reduces memory and computational requirements,
it still necessitates choosing a constant chunk size‚Äîwhich
may vary across tasks‚Äîreads the entire context sequen-
tially (thus retaining quadratic complexity albeit reduced
by a constant factor), and requires implementing specific
data structures and approximate retrieval algorithms for
efficiency. In contrast, our method leverages aggregated
statistics to directly skip to the next informative window.
During each reading phase, our model fully utilizes the con-
text window for which it was trained, rather than relying
on a manually specified chunk size, and optionally incorpo-
rates a memory module (aligning it closer to the ‚Äùattention
sink‚Äù approach). This allows for better modeling of text
coherence and achieves near sublinear complexity.5
3. Skipping Mechanism
When reading a large section of a document, we posit that
even a half-way trained language model might already have
a reasonable prediction for some future segments, which
can therefore be safely skipped. The skipping mechanism
proposed is straightforward: Suppose model Mbegins read-
ing at the S-th token, XS, of document X, and continues
forL(M)tokens‚Äîtypically the maximum token limit for
the model, e.g., L(M) = 512 . The reading ends at the
token XS+L(M). During this interval, the model performs
self-attention over the span XS, . . . , X S+L(M)and com-
putes token-wise cross entropy losses LS, . . . ,LS+L(M).
We then apply a pooling operation to these losses to es-
timate the model‚Äôs confidence over this passage, denoted
asC(X, S;M) = pooling (LS, . . . ,LS+L(M)). This con-
fidence metric is used to determine the number of tokens,
D(X, S;M), that the model will subsequently skip:
D(X, S;M) = (1)
Kmin
‚åä|X| ‚àíS‚àíL(M)
K‚åã,‚åäŒ±
C(X, S;M)‚åã
At the next reading step, the model resumes reading from
the token XS+L(M)+D(X,S ;M), and this process of skip-
ping continues until the end of the document. Here, K
represents the skipping rate and Œ±the skipping threshold.
When C(X, S;M)is small relative to Œ±, it suggests that
5We exclude the computational complexity of the tokenization
process as it is relatively lightweight. In fact, to omit tokenization,
our approach can also work with a character-based Transformer
model out of the box, where the semantics of the offset now auto-
matically means characters rather than sentencepiece.the model has a robust understanding of the current passage,
allowing it to safely skip at a rate of K‚åäŒ±
C(X,S ;M)‚åã.
Intuitively, for pretraining and finetuning, where the loss is
computed in a teacher-forcing paradigm, C(X, S;M)di-
rectly measure how well the model can predict the next gold
tokens given the previous context. The larger C(X, S;M)
is, the less confidence that a half-way trained model have
for future context and thus should skip in a more conser-
vative rate. During inference, without gold supervision,
the situation becomes more challenging. However, prior
works (Dong et al., 2018; Kamath et al., 2020; Jiang et al.,
2021) have identified log-probability loss as a common mea-
surement of model confidence at inference time, which we
anticipate could be highly informative for making skipping
decisions.
The simplicity of this heuristic underscores its value: it elim-
inates the need for additional models to predict the number
of tokens to skip. Since the required per-token loss is a direct
output of any standard language model, this mechanism can
be seamlessly integrated into the normal language model
pretraining process. Furthermore, our approach does not
rely on any structural assumptions or intermediate represen-
tations; therefore, the skipping operations do not interfere
with ongoing model operations and are compatible with any
model structure that provides a probability output for each
token. For simplicity, within the scope of this paper, we
utilize the widely-recognized auto-regressive Transformer
model.
Our experiments demonstrate that local skipping improves
the efficiency-diversity trade-off in long-context model-
ing. Moreover, skipping-based fine-tuning enables an exist-
ing language model checkpoint to effectively handle long-
context scenarios, surpassing even specialized memory-
based baselines.
Working with Memory Mechanism Given the inherently
limited context lengths of current language models and the
irreversible nature of our skipping mechanism, there is a
potential for making skipping decisions without sufficient
context. Moreover, previous research indicates that main-
taining attention over initial and neighboring tokens is cru-
cial for understanding extended contexts (Han et al., 2023;
Xiao et al., 2024); thus, focusing on these tokens can sig-
nificantly enhance performance. To augment the model‚Äôs
capacity for context processing and informed skipping, we
introduce an optional memory mechanism into our frame-
work. In this study, we implement the Attendre model, as
proposed in (Yang & Hua, 2024), which utilizes a First-In
First-Out (FIFO) memory eviction strategy alongside an
approximated Top-K Key-Value retrieval algorithm. Unlike
traditional memory-augmented transformer models such
as the Memorizing Transformer (Wu et al., 2021), our ap-
3

--- PAGE 4 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
proach features a global memory pool that performs Top-K
Key-Value retrieval across both the current reading window
and past memory items at every layer, rather than being
confined to a single intermediate layer.
Extension to Structured Documents Our approach can
be further strengthened by leveraging hierarchical structures
(e.g., the DOM tree for a web page or the table of contents
in a novel). We can restrict skipping to occur within sub-
trees, perform simultaneous skipping for multiple parts of
the document, and aggregate information globally. This
method aligns more closely with the landmark attention ap-
proach (Mohtashami & Jaggi, 2023), featuring uneven block
sizes and more coherent block semantics. Even without a
prebuilt index, recursive summarization can be employed to
build such an index from scratch, as demonstrated in (Chen
et al., 2023). In this work, our focus is on showcasing the
effectiveness of our proposed skipping mechanism without
assuming any structure in the input document. Therefore,
we leave further exploration in this area for future work.
4. Long-Context Language Modeling
We first evaluate the effectiveness of our random-access
method in pre-training over a long-context language mod-
eling task‚Äîspecifically, predicting the next token in a long
discourse. We selected the C4 corpus (Raffel et al., 2020)
for our task and filtered out documents with fewer than
4,000 tokens, resulting in the same corpus subset C4 (4k+)
as used in (Wu et al., 2021). To verify the effectiveness
of our proposed skipping mechanism, we investigate two
application scenarios: 1) Pretraining : Assuming sufficient
computational resources are available, we aim to pretrain a
model from scratch that can manage long contexts, where
using skipping as a data feeding strategy could enhance
model performance. 2) Finetuning : In practical situations
where pretraining is not feasible, we consider whether the
skipping mechanism can serve as a finetuning strategy to
augment the capability of a short-context model in handling
long contexts.
4.1. Task Setup
Pretraining We pretrain a 12-layer transformer model
over200,000steps with a batch size of 128and maximum
token limit L(M) = 512 . In this experiment, we do not
employ a memory mechanism to isolate and directly assess
the effectiveness of the skipping mechanism. We set varying
skipping rates K= 0,1,256,100,000for different runs.
For evaluation, we disable the skipping to ensure a fair
comparison.
Finetuning We utilize a language model pretrained on the
C4 (short-text) corpus, specifically for next-word prediction
e10e11e12
Step405060708090PPLC4 (4k+) LM Pretraining PPL
skip_rate=0
skip_rate=1
skip_rate=256
skip_rate=100000Figure 2. Pretraining Experiments on C4 (4k+) for models w/o
memory mechanism. We disable skipping in evaluation time for
fair comparison. Intermediate skipping rate performs best after suf-
ficient training, which confirms the effectiveness of our method.
over100Msteps, without the use of a skipping mechanism.
We use the same model architecture as in pretraining task.
In this short-text pretraining process, we follow the common
data processing strategy (‚Äúrandom chunking and shuffling‚Äù)
to concatenate all documents, split the whole corpus to
L(M)-sized chunks and random shuffling the chunks (De-
vlin et al., 2019; Raffel et al., 2020). The finetuning is then
conducted over 25,000steps on the C4 (4k+) training set
used in the pretraining Task. We report the skipping rates
K= 0,256for simplicity.
Metric We adopt perplexity (PPL) as the metric as in
previous works for long-context language modeling.
4.2. Experiment Results
Skipping achieves better long-context pretraining re-
sults. The pretraining results are illustrated in Figure 2.
With sufficient training duration, an intermediate skip-rate
(K=256) significantly reduces perplexity compared to non-
skipping (K= 0), demonstrating the effectiveness of the skip-
ping mechanism in long-context pretraining. At very high
skipping rates (i.e., K= 100,000), the correlation between
consecutively read documents diminishes, closely approx-
imating the effect of input random shuffling. We find that
skipping at a moderate rate outperforms both extremely
fast skipping and non-skipping strategies. Consequently,
our skipping mechanism not only maintains better coher-
ence between chunked passages but also enhances model
exposure to unique tokens, thereby improving generaliza-
tion compared to consecutive chunked and random shuffling
pretraining strategies.
Using skipping in finetuning can successfully adapt
short-text language model a better long-context model.
4

--- PAGE 5 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
Random
InitializationShort-T ext
Pretrained
CheckpointK=0
Long-Context
Fine-TuningK=256
Pretraining
From ScratchK=256
Long-Context
Fine-Tuninge3e4e5e6PPLFinetuning Performance
Figure 3. Finetuning Experiments on C4 (4k+) for models pre-
trained on short-text. We add the best performance achieved in pre-
training and a random initialized model performance for reference.
We find that short-text pretraining will make model generalize
worse even than a randomly initialized model for long-context
language modeling, but skipping fine-tuning can help adapt such
checkpoints to even perform better than specifically-pretrained
long-context checkpoint.
We present the finetuning results in Figure 3. Initially, af-
ter pretraining on short-text, the model exhibits worse per-
formance (ppl= 831.31) than a randomly-initialized model
(ppl= 366.50) on long-context language modeling. This in-
dicates that traditional short-text pretraining strategies do
not generalize well to long-context scenarios. However, by
applying appropriate skipping during finetuning, we can ef-
fectively adapt a short-text pretrained model to achieve even
better perplexity (ppl= 16.97, K=256) than our best long-
context model trained from scratch (ppl= 35.59). Moreover,
compared to standard finetuning on the long-context corpus
(ppl= 19.10, K=0), finetuning with our proposed skipping
mechanism achieves substantial improvements, confirming
its effectiveness in enhancing model capabilities in long-
context environments.
Our interpretation is that the ‚Äúrandom chunking and shuf-
fling‚Äù prevalent in traditional short-text pretraining (Devlin
et al., 2019; Raffel et al., 2020) impairs the model‚Äôs capa-
bility for continuous reading‚Äîthe input context changes
so rapidly that the model lacks the opportunity to capture
text coherence effectively. The skipping mechanism can
mitigate this by helping to recover long-term dependencies
and more effectively generalize the pretrained knowledge in
long-context language modeling.
Model gradually learns to skip more. In Figure 4, we
illustrate the average number of skipped tokens during train-
ing. For all skipping scenarios ( K > 0), the average number
of skipped tokens increases over time. This indicates the
model‚Äôs improved understanding of the input‚Äôs long con-
text, leading to more confident and aggressive skipping
e10e11e12
Step0.000.250.500.751.001.251.501.75Average Skips / KC4 (4k+) LM Pretraining Average Skips
skip_rate=0
skip_rate=1
skip_rate=256
skip_rate=100000Figure 4. The illustration of the average number of skipped tokens
(‚ÄúAverage Skips‚Äù) during C4 (4k+) pretraining without using the
memory module. Under different skipping scenarios ( K > 0), the
model gradually learns to skip more tokens, indicating it acquires
a better understanding of the current long context and becomes
more confident in making aggressive skipping decisions.
decisions. By comparing different curves, we observe that
with a moderate skipping rate ( K= 256 ) achieves the
smoothest skipping patterns. This may help stabilize train-
ing in later phases and achieve better generalization. Al-
though the K= 100 ,000case shows similar patterns, its
performance on the pretraining task is worse, indicating that
the magnitude of skipping (mainly controlled by skip-rate
K) is crucial and requires careful tuning.
Choice of pooling does not matter. We also try several
different pooling strategies ‚Äì ‚Äúexponential-decay‚Äù, ‚Äúonly
using last token in the current chunk‚Äù and ‚Äúaverage pooling‚Äù.
It seems that all pooling methods work equally well.
Model PPL Training Steps
Transformer w/ Memory (Wu et al., 2021) 14.97 500k
Skipping-Pretrained w/o Memory (ours) 35.59 200k
Skipping-Pretrained w/ Memory (ours) 14.15 130k
Table 1. Experiments on incorporating memory transformer in pre-
training. We adopt the memorizing transformer architectured as
implemented in Wu et al. (2021) with 512 context length, 1536
memory size and not using XL-cache.
Combining memory mechanism with skipping brings
further improvement. We find that using skipping in pre-
training can benefit transformer pretraining with memory
mechanism even if the skipping is not allowed in test time.
We adopt one specific memorizing transformer architecture
in (Wu et al., 2021) (memory-size=1536, context-len=512,
no-XL-cache) and perform the same skipping pretraining on
C4 (4k+) as above. For fair comparison, we again disable
the usage of skipping in test time. The experiment results
are shown in Table 1. We find that our skipping-pretrained
5

--- PAGE 6 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
0  0
 0  1e3
0  1e4
1e3  0
1e3  1e3
1e3  1e4
1e4  0
1e4  1e3
1e4  1e4
1416182022EMTriviaQA EM
Figure 5. Long-Context Question Answering Experiment results. x‚Üíymeans we train the model with skipping rate Ktrain=xand
evaluated using skipping rate Kinfer=y. We find that adopting more aggressive skipping strategy helps a lot for improving the model
performance.
memorizing transformer can achieve better performance
(ppl= 14.15) even within first 130k optimization steps com-
pared with pretrained memorizing transformer reported in
(Wu et al., 2021) (ppl= 14.97,500k steps), confirming our
pretraining methods brings significant performance boost
(‚àí0.82ppl) and better learning efficiency (approximately
3.84times more efficient). Compared to our skipping-
pretrained w/o memory model (ppl= 35.59), equipping with
the memory mechanism achieves 60% performance gain,
demonstrating the tremendous benefit of including mem-
ory mechanism in skipping pretraining. This is expected
as memory can help track what have read so far explicitly
and reduce the memory overload from parameter space in
training time.
5. Long-Context Question Answering
Besides pre-training, we also want to evaluate our skipping-
reading models over downstream long-context tasks to ver-
ify whether skipping-reading can help. We evaluate our
models on TriviaQA dataset (Joshi et al., 2017) (‚ÄúRC‚Äù split).
We concatenate all retrieved evidences in the input to trans-
form this task to be a relatively challenging long-context
question answering task. The average number of tokens
for the resulted dataset is 23,706.21. As inputs now are
becoming relatively long, we experiment with more aggres-
sive skipping strategy by setting K = 0,1000,10000 in both
training and testing time, leading to 3√ó3 = 9 combinations.We random sample 200questions for the test set. We use the
same transformer architecture as in pretraining experiments.
For this experiment, we pretrain the model with the memory
mechanism introduced above on C4 corpus.
The experiment results are shown in Figure 5. We ob-
serve that the best-performing models use a skipping rate
of K= 10000 during both training and testing. Additionally,
when the training skipping rate is fixed, using a larger or
equal skipping rate during testing consistently yields better
results, indicating that the skipping rate can be effectively
extrapolated in inference time for more efficient reading.
Furthermore, with a fixed inference rate, employing more
aggressive training skipping rates consistently leads to im-
provement. This contrasts with findings in Section 4, where
a moderate skipping rate proved more effective. For pre-
training, which focuses on next token prediction, the task
remains inherently local, and the structure of a long context
offers limited benefits. However, in long-context QA, the
goal shifts towards enabling the model to handle extremely
long documents containing multiple answer-bearing sec-
tions and distractors. Skip-reading minimizes redundant
engagement with similar relevant passages, thus obviating
the need for users to manually filter context. This approach,
combined with dynamic loss, achieves effects akin to on-
the-fly filtering.
6

--- PAGE 7 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
6. Conclusion
In this paper, we introduce a random-access reading strategy
designed for efficient long-context understanding. The ap-
proach primarily utilizes aggregated token-wise loss within
each model reading window, enabling a data server to de-
termine the number of tokens to be skipped and execute
the skipping process. To enhance decision-making for skip-
ping and provide the model with a richer context, we in-
corporate an additional memory module. In experiments,
we apply our method on Transformer model and evaluate
our trained random-access model via long-context pretrain-
ing, fine-tuning, and question answering. Experiment re-
sults confirm the significant improvements in performance
and efficiency of our proposed simple skipping mechanism
over traditional sequential access. Furthermore, these tests
demonstrate rapid adaptation of a short-text model to long-
context applications, and the mutual benefits of incorporat-
ing a memory module and enabling random-access reading.
Future work could benefit from exploring a more elaborate
skipping mechanism and examining the interplay between
the skipping mechanism and the memory module.
References
Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The
long-document transformer. arXiv:2004.05150 , 2020.
Chen, H., Pasunuru, R., Weston, J., and Celikyilmaz,
A. Walking down the memory maze: Beyond con-
text limit through interactive reading. arXiv preprint
arXiv:2310.05029 , 2023.
Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song,
X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mo-
hiuddin, A., Kaiser, L., et al. Rethinking attention with
performers. In International Conference on Learning
Representations , 2020.
Chowdhury, J. R. and Caragea, C. Monotonic location
attention for length generalization. In International Con-
ference on Machine Learning . PMLR, 2023.
Deng, X., Gu, Y ., Zheng, B., Chen, S., Stevens, S., Wang,
B., Sun, H., and Su, Y . Mind2web: Towards a general-
ist agent for the web. Advances in Neural Information
Processing Systems , 36, 2024.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In Burstein, J., Doran, C., and
Solorio, T. (eds.), Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , pp. 4171‚Äì4186,Minneapolis, Minnesota, jun 2019. Association for Com-
putational Linguistics. doi: 10.18653/v1/N19-1423. URL
https://aclanthology.org/N19-1423 .
Ding, M., Zhou, C., Yang, H., and Tang, J. Cogltx: Apply-
ing bert to long texts. Advances in Neural Information
Processing Systems , 33:12792‚Äì12804, 2020.
Dong, L., Quirk, C., and Lapata, M. Confidence modeling
for neural semantic parsing. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp. 743‚Äì753, 2018.
Dong, Z., Tang, T., Li, L., and Zhao, W. X. A survey on
long text modeling with transformers. arXiv preprint
arXiv:2302.14502 , 2023.
Fu, D. Y ., Epstein, E. L., Nguyen, E., Thomas, A. W., Zhang,
M., Dao, T., Rudra, A., and Re, C. Simple hardware-
efficient long convolutions for sequence modeling. In
International Conference on Machine Learning . PMLR,
2023.
Fu, Y ., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim,
Y ., and Peng, H. Data engineering for scaling language
models to 128k context. In International Conference on
Machine Learning , 2024.
Guo, M., Ainslie, J., Uthus, D. C., Ontanon, S., Ni, J.,
Sung, Y .-H., and Yang, Y . Longt5: Efficient text-to-
text transformer for long sequences. In Findings of the
Association for Computational Linguistics: NAACL 2022 ,
pp. 724‚Äì736, 2022.
Han, C., Wang, Q., Xiong, W., Chen, Y ., Ji, H., and Wang, S.
Lm-infinite: Simple on-the-fly length generalization for
large language models. arXiv preprint arXiv:2308.16137 ,
2023.
Huang, Y ., Xu, J., Jiang, Z., Lai, J., Li, Z., Yao, Y ., Chen,
T., Yang, L., Xin, Z., and Ma, X. Advancing transformer
architecture in long-context large language models: A
comprehensive survey. arXiv preprint arXiv:2311.12351 ,
2023.
Ivgi, M., Shaham, U., and Berant, J. Efficient long-text
understanding with short-text models. Transactions of
the Association for Computational Linguistics , 11:284‚Äì
299, 2023.
Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we
know when language models know? on the calibration of
language models for question answering. Transactions
of the Association for Computational Linguistics , 9:962‚Äì
977, 2021.
Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press,
O., and Narasimhan, K. R. SWE-bench: Can language
7

--- PAGE 8 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
models resolve real-world github issues? In International
Conference on Learning Representations , 2023.
Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Trivi-
aQA: A large scale distantly supervised challenge dataset
for reading comprehension. In Proceedings of the 55th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp. 1601‚Äì1611,
2017.
Kamath, A., Jia, R., and Liang, P. Selective question an-
swering under domain shift. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pp. 5684‚Äì5696, 2020.
Knuth, D. E. The art of computer programming , volume 3.
Pearson Education, 1997.
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V .,
Goyal, N., K ¬®uttler, H., Lewis, M., Yih, W.-t., Rockt ¬®aschel,
T., et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. Advances in Neural Information Pro-
cessing Systems , 33:9459‚Äì9474, 2020.
Liu, Y ., Ni, A., Nan, L., Deb, B., Zhu, C., Hassan, A.,
and Radev, D. Leveraging locality in abstractive text
summarization. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing ,
pp. 6081‚Äì6093, 2022.
Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song,
Z., Shrivastava, A., Zhang, C., Tian, Y ., Re, C.,
et al. Deja vu: Contextual sparsity for efficient
LLMs at inference time. In International Confer-
ence on Machine Learning , pp. 22137‚Äì22176. PMLR,
2023. URL https://proceedings.mlr.press/
v202/liu23am.html .
Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and
Zettlemoyer, L. Luna: Linear unified nested attention.
Advances in Neural Information Processing Systems , 34:
2441‚Äì2453, 2021.
Mohtashami, A. and Jaggi, M. Landmark attention:
Random-access infinite context length for transform-
ers.Advances in Neural Information Processing Systems ,
2023.
Mohtashami, A. and Jaggi, M. Random-access infinite
context length for transformers. Advances in Neural
Information Processing Systems , 36, 2024.
Mou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S.,
and Su, H. Narrative question answering with cutting-
edge open-domain qa techniques: A comprehensive study.
Transactions of the Association for Computational Lin-
guistics , 9:1032‚Äì1046, 2021.Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher,
S., and Ho, N. Fourierformer: Transformer meets gen-
eralized fourier integral theorem. Advances in Neural
Information Processing Systems , 35:29319‚Äì29335, 2022.
Paris, S. G., Wasik, B., and Turner, J. C. The development
of strategic readers. 1991.
Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
Efficient context window extension of large language
models. arXiv preprint arXiv:2309.00071 , 2023.
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.,
and Kong, L. Random feature attention. In International
Conference on Learning Representations , 2020.
Pressley, M. and Afflerbach, P. Verbal protocols of reading:
The nature of constructively responsive reading . Rout-
ledge, 2012.
Qiu, J., Ma, H., Levy, O., Yih, W.-t., Wang, S., and Tang, J.
Blockwise self-attention for long document understand-
ing. In Findings of the Association for Computational
Linguistics: EMNLP 2020 , pp. 2555‚Äì2565, 2020.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a unified text-to-text
transformer. Journal of machine learning research , 21
(140):1‚Äì67, 2020.
Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-
crap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,
O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-
modal understanding across millions of tokens of context.
arXiv preprint arXiv:2403.05530 , 2024.
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis,
M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-
augmented black-box language models. arXiv preprint
arXiv:2301.12652 , 2023.
Su, J., Ahmed, M., Lu, Y ., Pan, S., Bo, W., and Liu, Y .
Roformer: Enhanced transformer with rotary position
embedding. Neurocomputing , 568:127063, 2024.
Tay, Y ., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
Sparse sinkhorn attention. In International Conference
on Machine Learning , pp. 9438‚Äì9447. PMLR, 2020a.
Tay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
range arena: A benchmark for efficient transformers. In
International Conference on Learning Representations ,
2020b.
Wu, Y ., Rabe, M. N., Hutchins, D., and Szegedy, C. Mem-
orizing transformers. In International Conference on
Learning Representations , 2021.
8

--- PAGE 9 ---
Equipping Transformer with Random-Access Reading for Long-Context Understanding
Xiao, G., Tian, Y ., Chen, B., Han, S., and Lewis, M. Effi-
cient streaming language models with attention sinks. In
International Conference on Learning Representations ,
2024.
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P.,
Hou, R., Martin, L., Rungta, R., Sankararaman, K. A.,
Oguz, B., et al. Effective long-context scaling of founda-
tion models. arXiv preprint arXiv:2309.16039 , 2023.
Yang, C. and Ettinger, A. Can you follow me? testing
situational understanding for ChatGPT. In Proceedings
of the 2023 Conference on Empirical Methods in Natural
Language Processing , pp. 6385‚Äì6398, 2023.
Yang, Z. and Hua, N. Attendre: Wait to attend by
retrieval with evicted queries in memory-based trans-
formers for long context processing. arXiv preprint
arXiv:2401.04881 , 2024.
Zhang, Y ., Ni, A., Mao, Z., Wu, C. H., Zhu, C., Deb, B.,
Awadallah, A., Radev, D., and Zhang, R. Summn: A
multi-stage summarization framework for long input dia-
logues and documents. In Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pp. 1592‚Äì1604, 2022.
Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A.,
Cheng, X., Ou, T., Bisk, Y ., Fried, D., et al. Webarena:
A realistic web environment for building autonomous
agents. In International Conference on Learning Repre-
sentations , 2023.
9
