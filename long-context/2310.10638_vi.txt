# 2310.10638.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.10638.pdf
# Kích thước tệp: 1086613 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
TIỀN HUẤN LUYỆN NGỮ CẢNH: MÔ HÌNH HÓA NGÔN NGỮ
VƯỢT QUA RANH GIỚI TÀI LIỆU
Weijia Shi1,2Sewon Min1,2Maria Lomeli1Chunting Zhou1
Margaret Li1,2Gergely Szilvasy1Rich James1Xi Victoria Lin1
Noah A. Smith2,3Luke Zettlemoyer1,2Scott Yih1Mike Lewis1
1Meta AI2University of Washington3Allen Institute for AI
swj0419@cs.washington.edu
TÓM TẮT
Các mô hình ngôn ngữ lớn (LM) hiện tại được huấn luyện để dự đoán token dựa trên 
tiền tố tài liệu, cho phép chúng thực hiện trực tiếp việc tạo sinh dạng dài và các 
tác vụ kiểu prompting có thể được rút gọn thành hoàn thành tài liệu. Các pipeline 
tiền huấn luyện hiện tại huấn luyện LM bằng cách nối các tập hợp ngẫu nhiên của 
các tài liệu ngắn để tạo ngữ cảnh đầu vào nhưng các tài liệu trước đó không cung 
cấp tín hiệu nào để dự đoán tài liệu tiếp theo. Thay vào đó, chúng tôi trình bày 
TIỀN HUẤN LUYỆN NGỮ CẢNH, một phương pháp mới trong đó các mô hình ngôn ngữ được 
tiền huấn luyện trên một chuỗi các tài liệu liên quan, từ đó khuyến khích rõ ràng 
chúng đọc và suy luận qua ranh giới tài liệu. Chúng ta có thể thực hiện TIỀN HUẤN 
LUYỆN NGỮ CẢNH bằng cách đơn giản thay đổi thứ tự tài liệu sao cho mỗi ngữ cảnh 
chứa các tài liệu liên quan, và áp dụng trực tiếp các pipeline tiền huấn luyện hiện 
có. Tuy nhiên, bài toán sắp xếp tài liệu này khá thách thức. Có hàng tỷ tài liệu và 
chúng ta muốn sắp xếp để tối đa hóa độ tương tự ngữ cảnh cho mọi tài liệu mà không 
lặp lại bất kỳ dữ liệu nào. Để làm điều này, chúng tôi giới thiệu các thuật toán 
xấp xỉ để tìm các tài liệu liên quan với tìm kiếm láng giềng gần hiệu quả và xây 
dựng các ngữ cảnh đầu vào mạch lạc với thuật toán duyệt đồ thị. Các thí nghiệm của 
chúng tôi cho thấy TIỀN HUẤN LUYỆN NGỮ CẢNH cung cấp một phương pháp đơn giản và 
có thể mở rộng để cải thiện đáng kể hiệu suất của LM: chúng tôi thấy những cải thiện 
đáng chú ý trong các tác vụ đòi hỏi suy luận ngữ cảnh phức tạp hơn, bao gồm học 
ngữ cảnh (+8%), đọc hiểu (+15%), trung thực với ngữ cảnh trước đó (+16%), suy luận 
ngữ cảnh dài (+5%), và tăng cường truy xuất (+9%).

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LM) được huấn luyện để hoàn thành tài liệu; mỗi token được dự đoán 
dựa trên ngữ cảnh do tiền tố của tài liệu mà nó xuất hiện cung cấp. Những ngữ cảnh như vậy có 
thể rất đa dạng, đặc biệt ở quy mô tiền huấn luyện, cho phép các mô hình xuất sắc trong các tác 
vụ đa dạng như tuân theo hướng dẫn (Ouyang et al., 2022), giao diện hội thoại (OpenAI, 2023), 
đọc hiểu (Zhang et al., 2020), và học ngữ cảnh (Brown et al., 2020). Tuy nhiên, các nghiên cứu 
gần đây nhấn mạnh rằng LM đôi khi gặp khó khăn để hiểu các ngữ cảnh phức tạp hơn: chúng có thể 
không tuân theo hướng dẫn một cách chính xác (McKenzie et al., 2023; Efrat & Levy, 2020; Liu & 
Liu, 2023), gặp khó khăn với suy luận trên các tài liệu được điều kiện hóa (Liu et al., 2023; Shi 
et al., 2023a), và thể hiện độ biến thiên cao trong học ngữ cảnh (Zhao et al., 2021). Trong bài 
báo này, chúng tôi trình bày TIỀN HUẤN LUYỆN NGỮ CẢNH, một phương pháp tiền huấn luyện mới 
học cách dự đoán token được điều kiện hóa trên một chuỗi các tài liệu liên quan, cho phép mô hình 
một cách rõ ràng đọc và suy luận về các ngữ cảnh đa dạng và dài hơn nhiều vượt ra ngoài ranh giới 
tài liệu.

Các pipeline huấn luyện LM hiện tại nối các tập hợp ngẫu nhiên của các tài liệu ngắn hơn để tạo 
cửa sổ ngữ cảnh dài hơn. Tuy nhiên, các tài liệu trước đó không cung cấp tín hiệu nào để dự đoán 
tài liệu tiếp theo, gây ra chi phí tính toán không cần thiết cho các token không yêu cầu giao tiếp 
giữa chúng (de Vries, 2023). TIỀN HUẤN LUYỆN NGỮ CẢNH thay vào đó sắp xếp lại dữ liệu tiền 
huấn luyện bằng cách kết hợp một số tài liệu liên quan về mặt ngữ nghĩa để tạo một ngữ cảnh đầu 
vào mạch lạc, từ đó phơi bày LM với các ngữ cảnh liên quan dài và cung cấp tín hiệu tiền huấn 
luyện vượt ra ngoài ranh giới tài liệu. Chúng tôi minh họa điều này qua một ví dụ trong Hình 1: 
khi dự đoán các token tiếp theo cho cụm từ "Đối với năm 2022, FIFA đặt tiền thưởng ở mức 42 triệu 
đô la,", một tài liệu trước đó nêu rằng "World Cup chưa bao giờ trao giải thưởng
1arXiv:2310.10638v6  [cs.CL]  24 Jun 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Hình 1: Tổng quan về TIỀN HUẤN LUYỆN NGỮ CẢNH. Khác với chiến lược tiền huấn luyện tiêu 
chuẩn đặt các tài liệu được trộn ngẫu nhiên trong ngữ cảnh đầu vào, TIỀN HUẤN LUYỆN NGỮ CẢNH 
đặt các tài liệu liên quan trong cùng một ngữ cảnh, khiến các mô hình học cách suy luận qua các 
tài liệu trước đó. Ví dụ, khi dự đoán các token tiếp theo cho cụm từ "Đối với năm 2022, FIFA đặt 
tiền thưởng ở mức 42 triệu đô la,", LM có thể tham khảo các tài liệu trước đó nêu "World Cup chưa 
bao giờ trao giải thưởng hơn 10 triệu đô la trước năm 2022" và học cách suy ra "cao nhất từ trước 
đến nay."

hơn 10 triệu đô la trước năm 2022" có thể ở trong ngữ cảnh, cho phép dự đoán một tiếp tục như 
"cao nhất từ trước đến nay." Vì TIỀN HUẤN LUYỆN NGỮ CẢNH chỉ thay đổi thứ tự tài liệu và để 
nguyên tất cả các khía cạnh khác của tiền huấn luyện LM, nó có thể được tích hợp dễ dàng vào 
các pipeline tiền huấn luyện hiện có cho các LM quy mô lớn.

Tuy nhiên, bài toán sắp xếp tài liệu này khá thách thức. LM thường được huấn luyện trên hàng tỷ 
tài liệu và chúng ta muốn sắp xếp chúng để tối đa hóa độ tương tự tài liệu trong các cửa sổ ngữ 
cảnh đầu vào mà không lặp lại bất kỳ dữ liệu nào. Chúng tôi giới thiệu hai thuật toán xấp xỉ mới 
để giải quyết những thách thức này. Chúng tôi sử dụng một mô hình truy xuất được ghép nối với một 
chỉ mục tìm kiếm hiệu quả để xây dựng một đồ thị tài liệu ghép nối mỗi tài liệu với các láng giềng 
gần nhất của nó dựa trên độ tương tự ngữ nghĩa trong không gian embedding. Chúng tôi cũng công 
thức hóa việc sắp xếp tài liệu như một bài toán người bán hàng lưu động, cho việc này chúng tôi 
phát triển một thuật toán hiệu quả tối đa hóa độ tương tự của các tài liệu với ngữ cảnh của chúng 
đồng thời cũng đảm bảo rằng mỗi tài liệu chỉ được bao gồm một lần.

Để đánh giá hiệu quả của TIỀN HUẤN LUYỆN NGỮ CẢNH, chúng tôi tiền huấn luyện các mô hình ngôn 
ngữ từ 0.3 đến 7 tỷ tham số trên 300 tỷ token từ bộ dữ liệu CommonCrawl (Wenzek et al., 2020). 
Trên tất cả các quy mô mô hình, các LM TIỀN HUẤN LUYỆN NGỮ CẢNH (ICLM) thể hiện hiệu suất mô 
hình hóa ngôn ngữ và tác vụ downstream mạnh mẽ, vượt trội hơn các LM được tiền huấn luyện bằng 
phương pháp tiêu chuẩn trên cùng một corpus. Chúng tôi quan sát các cải thiện khác nhau từ TIỀN 
HUẤN LUYỆN NGỮ CẢNH so với các LM hiện có: (1) học ngữ cảnh với mức tăng trung bình 8% trên 8 
bộ dữ liệu; (2) đọc hiểu, với mức cải thiện trung bình 15% trên 8 tác vụ đọc hiểu; (3) đầu ra 
trung thực hơn với ngữ cảnh trước đó (+16%); (4) suy luận ngữ cảnh dài, cho thấy mức tăng 5%; và 
(5) tăng cường truy xuất, dẫn đến mức tăng 9% khi tăng cường với kiến thức bên ngoài như các tài 
liệu được truy xuất từ Wikipedia. Kết quả của chúng tôi cho thấy rằng, bằng cách đơn giản thay đổi 
thứ tự của các tài liệu tiền huấn luyện, TIỀN HUẤN LUYỆN NGỮ CẢNH cung cấp một phương pháp có 
thể mở rộng và đơn giản để cải thiện đáng kể việc hiểu và suy luận trên toàn bộ ngữ cảnh của 
chúng. Mã được công bố công khai tại github.com/swj0419/in-context-pretraining.

2 TIỀN HUẤN LUYỆN NGỮ CẢNH
Thực hành tiêu chuẩn trong tiền huấn luyện là tạo ngữ cảnh đầu vào bằng cách nối các tài liệu ngẫu 
nhiên cho đến khi đạt độ dài ngữ cảnh tối đa. Sau đó nó huấn luyện LM bằng mục tiêu mô hình hóa 
ngôn ngữ trên các ngữ cảnh đầu vào. Tuy nhiên, huấn luyện LM trên các tài liệu được nối ngẫu nhiên 
không cung cấp tín hiệu học tập bổ sung so với huấn luyện trên từng tài liệu riêng lẻ. Ngược lại, 
TIỀN HUẤN LUYỆN NGỮ CẢNH tạo ra các ngữ cảnh đầu vào mạch lạc hơn bằng cách nối các tài liệu 
liên quan về mặt ngữ nghĩa với nhau trong quá trình tiền huấn luyện. Như được miêu tả trong Hình 
2, TIỀN HUẤN LUYỆN NGỮ CẢNH bao gồm hai bước: đầu tiên nó tìm các tài liệu liên quan ở quy mô 
lớn (§2.1) và sau đó xây dựng ngữ cảnh đầu vào sử dụng những tài liệu liên quan này (§2.2). Tiếp 
theo, chúng ta sử dụng các ngữ cảnh được tạo với

--- TRANG 3 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
các tài liệu liên quan về mặt ngữ nghĩa để tiền huấn luyện LM với mục tiêu mô hình hóa ngôn ngữ. 
Vì TIỀN HUẤN LUYỆN NGỮ CẢNH giống hệt với các công thức tiền huấn luyện hiện có cho LM, ngoại 
trừ việc thay đổi cách xây dựng ngữ cảnh đầu vào, nó có thể được tích hợp dễ dàng vào các pipeline 
tiền huấn luyện hiện có cho các LM quy mô lớn.

2.1 TÌM CÁC TÀI LIỆU LIÊN QUAN Ở QUY MÔ LỚN: TRUY XUẤT CÁC TÀI LIỆU LÁNG GIỀNG
Để tìm các tài liệu liên quan ở quy mô lớn, chúng tôi liên kết các tài liệu trong corpus tiền huấn 
luyện D bằng một mô hình truy xuất. Cụ thể, đối với mỗi tài liệu di ∈ D, một mô hình truy xuất dày 
đặc được sử dụng để truy xuất top-k tài liệu tương tự nhất, được biểu diễn là N(di). Mô hình truy 
xuất sử dụng tìm kiếm láng giềng gần xấp xỉ để so sánh độ tương tự cặp hiệu quả giữa hai tài liệu 
bất kỳ, làm cho nó có thể mở rộng để tìm các tài liệu liên quan trong các corpus tiền huấn luyện 
quy mô web.

Truy xuất. Quá trình truy xuất của chúng tôi sử dụng mô hình contriever (Izacard et al., 2022). 
Mô hình này ánh xạ mỗi tài liệu di ∈ D đến một embedding E(di) bằng cách lấy mean pooling của 
biểu diễn ẩn cuối cùng trên các token trong di. Độ tương tự cosine sau đó được sử dụng để xác định 
độ tương tự giữa hai tài liệu bất kỳ:

s(di, dj) = cos(E(di), E(dj))     (1)

Mô hình truy xuất sử dụng tìm kiếm láng giềng gần xấp xỉ với thư viện faiss (Johnson et al., 2019; 
Douze et al., 2024). Chúng tôi sử dụng lượng hóa sản phẩm (Jégou et al., 2011) để giảm dung lượng 
bộ nhớ và cấu trúc chỉ mục IVF (inverted file) để tiến hành tìm kiếm độ tương tự cặp hiệu quả cùng 
với tìm kiếm batch lớn faiss. Framework OIVFBBS faiss được tận dụng cho tác vụ này, OIVFBBS đề 
cập đến việc tiến hành tìm kiếm offline với các truy vấn của các batch lớn với các chỉ mục đảo ngược 
faiss. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.2 và trong demo OIVFBBS trong repository 
github faiss github.com/facebookresearch/faiss/tree/main/demos/offline_ivf.

Trong quá trình truy xuất, khi tính toán độ tương tự cặp giữa mỗi tài liệu trong corpus tiền huấn 
luyện, chúng tôi phát hiện rằng corpus tiền huấn luyện chứa nhiều tài liệu gần trùng lặp. Do đó, 
chúng tôi tiếp tục tận dụng điểm số truy xuất để loại bỏ các tài liệu gần trùng lặp khỏi corpus tiền 
huấn luyện. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.1. Trong §4.2, chúng tôi cho thấy 
bước khử trùng lặp này rất quan trọng để đạt hiệu suất tốt của các mô hình ngôn ngữ.

2.2 TẠO NGỮ CẢNH ĐẦU VÀO: DUYỆT ĐỒ THỊ TÀI LIỆU
Cho một tập hợp tài liệu D = {di} và các láng giềng gần nhất cho mỗi tài liệu N(di), mục tiêu của 
chúng ta là sắp xếp các tài liệu để tạo ngữ cảnh đầu vào sao cho mỗi trong số chúng bao gồm một 
danh sách các tài liệu liên quan. Một cách chính thức, chúng ta nhằm tạo một tập hợp ngữ cảnh đầu 
vào C1···Cm trong đó mỗi ngữ cảnh Ci = {d1, ...dk} ⊂ D và ∪^m_{i=1} Ci = D. Lý tưởng nhất, các 
tài liệu trong Ci là các láng giềng gần nhất của nhau.

Thuật toán 1 Maximum Traveling Salesman
Input : Đồ thị tài liệu G = (D, L)
N(di) trả về các láng giềng gần nhất cho di
min_deg(D) trả về một tài liệu có bậc tối thiểu
Output : Một đường dẫn P
1: P ← []
2: while |D| > 0 do
3:    di ← min_deg(D)
4:    P.append(di)
5:    D.remove(di)
6:    while N(di) ∩ D ≠ ∅ do
7:       dj ← arg min_{d∈N(di)∩D} sim(di, d)
8:       di ← dj
9:       P.append(di)
10:      D.remove(di)
11:   end while
12: end while
13: return P

Một phương pháp đơn giản để tạo C1···Cm là trực tiếp đặt mỗi tài liệu và top-k tài liệu được truy 
xuất của nó cùng nhau trong cùng một ngữ cảnh đầu vào (được gọi là kNN), đã được sử dụng trong một 
số phương pháp tiền huấn luyện tăng cường truy xuất (Guu et al., 2020; Levine et al., 2022). 
Phương pháp kNN này duy trì độ tương tự tài liệu trong mỗi ngữ cảnh nhưng tạo ra vấn đề lặp lại dữ 
liệu: một số tài liệu thường xuyên xuất hiện như các láng giềng gần nhất của các tài liệu khác, gây 
ra việc các ngữ cảnh đầu vào khác nhau chứa các tài liệu trùng lặp, tức là ∃i ≠ j, Ci ∩ Cj ≠ ∅. 
Vấn đề lặp lại dữ liệu phơi bày LM với một tập hợp tài liệu ít đa dạng hơn cho một ngân sách tính 
toán cố định và có thể dẫn đến overfitting của các tài liệu phổ biến. Thay vào đó, chúng ta nhằm 
xây dựng một tập hợp ngữ cảnh theo cách mà mỗi tài liệu chỉ được bao gồm một lần, có thể được đúc 
khuôn như một bài toán duyệt đồ thị.

--- TRANG 4 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Hình 2: Minh họa TIỀN HUẤN LUYỆN NGỮ CẢNH. TIỀN HUẤN LUYỆN NGỮ CẢNH đầu tiên tìm các tài 
liệu liên quan ở quy mô lớn để tạo một đồ thị tài liệu (§2.1) và sau đó xây dựng các ngữ cảnh đầu 
vào tiền huấn luyện bằng cách duyệt đồ thị tài liệu (§2.2). Dọc theo đường dẫn, các tài liệu được 
nối thành một chuỗi và sau đó được chia để tạo các ngữ cảnh đầu vào có kích thước cố định (ví dụ, 
độ dài 8192 token).

Duyệt đồ thị tài liệu. Để đạt được mục tiêu tối đa hóa cơ hội mà các tài liệu liên quan được nối 
với nhau, một phương pháp trực quan là tìm một đường dẫn duy nhất ghé thăm mỗi tài liệu một lần và 
tối đa hóa cơ hội mà các tài liệu liên quan được ghé thăm tuần tự. Sau đó chúng ta phân đoạn đường 
dẫn thành nhiều ngữ cảnh đầu vào. Chúng tôi công thức hóa nó như bài toán người bán hàng lưu động 
tối đa (Flood, 1956) nhằm tìm đường dẫn trọng số tối đa duyệt qua tất cả các nút chính xác một lần. 
Chúng tôi biểu diễn mỗi tài liệu như một nút trong đồ thị và sử dụng độ tương tự tài liệu như trọng 
số cạnh. Chúng tôi thiết kế một đồ thị có trọng số không hướng biểu diễn các tài liệu, được ký hiệu 
là G = (D, L). Ở đây, D biểu diễn tập hợp các tài liệu, trong khi (d, d*) ∈ L là một cạnh nếu d* 
∈ N(di) hoặc di ∈ N(d*). Trọng số của mỗi cạnh tương ứng với độ tương tự tài liệu (Phương trình 1).

Giải các bài toán người bán hàng lưu động lớn một cách chính xác là NP hard, nhưng các thuật toán 
tham lam được biết là cung cấp một giải pháp xấp xỉ hiệu quả. Chúng tôi áp dụng phương pháp này, 
giới thiệu các sửa đổi để phù hợp hơn với ngữ cảnh của chúng ta. Thuật toán 1 cho thấy phương pháp 
để xây dựng một đường dẫn trọng số tối đa. Chúng tôi trình bày một đường dẫn được xác định bởi thuật 
toán của chúng tôi trong Hình 2. Thuật toán của chúng tôi bắt đầu bằng cách chọn một tài liệu chưa 
được ghé thăm với bậc tối thiểu làm nút bắt đầu (Doc 0). Thuật toán sau đó mở rộng dần đường dẫn 
hiện tại bằng cách di chuyển đến tài liệu láng giềng chưa được ghé thăm với trọng số cao nhất (Doc 
9), thêm nút tài liệu vào đường dẫn. Quá trình này tiếp tục cho đến khi đường dẫn đến một nút mà 
tất cả các tài liệu láng giềng đã được ghé thăm, điều này xảy ra vì đồ thị của chúng ta không đầy 
đủ, và chỉ chứa các cạnh giữa các tài liệu mà một trong số chúng nằm trong k láng giềng gần nhất 
của tài liệu kia. Trong trường hợp này, chúng ta mở rộng đồ thị với một cạnh có trọng số 0 đến một 
tài liệu chưa được ghé thăm có bậc tối thiểu ngẫu nhiên (Doc 1), và tiếp tục quá trình trên. Động 
lực để bắt đầu từ các tài liệu có bậc tối thiểu là chúng có khả năng cao nhất có tất cả các láng 
giềng của chúng được ghé thăm trước, và do đó được kết nối với các tài liệu không tương tự trong 
đường dẫn cuối cùng.

Như một bước cuối cùng, chúng ta duyệt các tài liệu dọc theo đường dẫn và nối chúng để tạo các ngữ 
cảnh đầu vào có kích thước cố định phù hợp cho tiền huấn luyện. Quan trọng là khi tạo các batch 
đầu vào huấn luyện, chúng ta đảm bảo tính đa dạng giữa các ngữ cảnh đầu vào khác nhau trong cùng 
batch.

3 THÍ NGHIỆM
Trong phần này, chúng tôi mô tả chi tiết về thiết lập tiền huấn luyện của chúng tôi (§3.1), các 
phương pháp baseline chúng tôi sử dụng để so sánh (§3.2), và kết quả thí nghiệm (§3.3).

3.1 THIẾT LẬP TIỀN HUẤN LUYỆN
Vì TIỀN HUẤN LUYỆN NGỮ CẢNH để nguyên các chi tiết khác của huấn luyện mô hình, và chỉ thay 
đổi thứ tự tài liệu sao cho mỗi ngữ cảnh chứa các tài liệu liên quan, chúng ta có thể tích hợp trực 
tiếp nó vào các pipeline tiền huấn luyện như một bước tiền xử lý trong quá trình batching. Cho thí 
nghiệm của chúng tôi, chúng tôi áp dụng kiến trúc mô hình và mục tiêu tiền huấn luyện của LLaMA 
(Touvron et al., 2023a;b) và tiền huấn luyện LM từ đầu.

--- TRANG 5 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Bộ dữ liệu Tiền huấn luyện. Chúng tôi sử dụng bộ dữ liệu Commoncrawl tiếng Anh (Wenzek et al., 
2020), nguồn dữ liệu được sử dụng rộng rãi để tiền huấn luyện LM. Do hạn chế về tài nguyên, chúng 
tôi lấy mẫu ngẫu nhiên 235 triệu tài liệu từ bộ dữ liệu này, tổng cộng 306 tỷ token. Chúng tôi sử 
dụng cùng dữ liệu tiền huấn luyện cho tất cả các mô hình.

Chi tiết Mô hình. Chúng tôi lấy kiến trúc mô hình từ LLaMA (Touvron et al., 2023a) và huấn luyện 
các mô hình qua nhiều kích thước khác nhau: 0.3, 0.7, 1.5, và 7.0 tỷ tham số, tất cả với cửa sổ 
ngữ cảnh dài 8192. Theo LLaMA, chúng tôi sử dụng trình tối ưu AdamW (Loshchilov & Hutter, 2018) 
với các tham số β1 = 0.9 và β2 = 0.95, và lịch trình học cosine. Mô hình 7B được tiền huấn luyện 
sử dụng 128 GPU A100 trên 16 nút với batch size 4 triệu token. Mất 9 ngày để huấn luyện mô hình 
7B trên bộ dữ liệu tiền huấn luyện của chúng tôi. Do cửa sổ ngữ cảnh dài của các mô hình, chúng 
tôi sử dụng flash attention (Dao et al., 2022) để giảm tiêu thụ bộ nhớ trong quá trình tiền huấn 
luyện.

Để thực hiện truy xuất trên bộ dữ liệu tiền huấn luyện của chúng tôi, chúng tôi sử dụng mô hình 
contriever (Izacard et al., 2022) và mã hóa 512 token đầu tiên của mỗi tài liệu thành một embedding. 
Sau đó chúng tôi xây dựng tìm kiếm batch lớn FAISS được thiết kế để tiến hành tìm kiếm độ tương 
tự hiệu quả với các batch lớn vector (thường 50M–100M vector mỗi batch). Với mỗi tài liệu truy 
vấn, chúng tôi truy xuất top 10 tài liệu (k=10). Chúng tôi chia dữ liệu thành các batch 50M embedding, 
bước tìm kiếm được tiến hành trong mỗi batch trước khi hợp nhất kết quả sử dụng 8 GPU mỗi batch. 
Tổng thời gian tìm kiếm là 6 giờ trên 32 GPU với thời gian tìm kiếm trung bình mỗi batch là 4,738s. 
Giai đoạn duyệt đồ thị tài liệu yêu cầu 12 giờ trên thiết lập 20 CPU.

Chi tiết thêm được cung cấp trong Phụ lục A.2.

3.2 BASELINE
Chúng tôi so sánh TIỀN HUẤN LUYỆN NGỮ CẢNH với các baseline sau: (1) Standard là tiêu chuẩn 
trước đó trong tiền huấn luyện đặt các tài liệu được trộn ngẫu nhiên trong các ngữ cảnh đầu vào. 
Phương pháp này được áp dụng phổ biến bởi các mô hình hiện có (Zhang et al., 2022; Scao et al., 
2022; Touvron et al., 2023a). (2) kNN (cũng được gọi là tiền huấn luyện mô hình ngôn ngữ tăng 
cường truy xuất (Guu et al., 2020; Levine et al., 2022)) trực tiếp đặt mỗi tài liệu và top-k tài 
liệu được truy xuất của nó cùng nhau trong cùng một ngữ cảnh đầu vào. Với cùng số bước huấn luyện, 
kNN phơi bày LM với một tập hợp tài liệu ít đa dạng hơn, vì các tài liệu có thể lặp lại. Để so sánh 
công bằng, cả phương pháp standard và kNN đều được huấn luyện sử dụng cùng dữ liệu tiền huấn luyện 
như TIỀN HUẤN LUYỆN NGỮ CẢNH và trải qua số bước huấn luyện giống hệt nhau, đảm bảo cùng chi 
phí tính toán.

3.3 KẾT QUẢ
Chúng tôi thực hiện đánh giá trên các tác vụ đòi hỏi hiểu ngữ cảnh bao gồm mô hình hóa ngôn ngữ 
(§3.3.1), học ngữ cảnh (§3.3.2), đọc hiểu (§3.3.3) và trả lời câu hỏi mở sách (§3.3.4), tính 
thực tế (§3.3.5) và suy luận ngữ cảnh dài (§3.3.6).

3.3.1 MÔ HÌNH HÓA NGÔN NGỮ
Bộ dữ liệu & Chỉ số. Chúng tôi đánh giá độ khó hiểu mô hình hóa ngôn ngữ của TIỀN HUẤN LUYỆN 
NGỮ CẢNH và các baseline trên các corpus Wikipedia, Arxiv, và Books. Chúng tôi tuân theo đánh 
giá mô hình hóa ngôn ngữ tiêu chuẩn trong việc nối các tài liệu được sắp xếp ngẫu nhiên khi tính 
độ khó hiểu.

Kết quả. Hình 3 cho thấy độ khó hiểu trung bình qua các kích thước mô hình khác nhau. Đầu tiên, 
kNN không cải thiện so với LM tiêu chuẩn, có thể do vấn đề overfitting như thảo luận trong §2.2. 
ICLM, ngược lại, vượt trội hơn cả LM tiêu chuẩn và kNN trên tất cả ba bộ dữ liệu, ngay cả khi các 
tài liệu đánh giá không được sắp xếp. Các mức tăng là nhất quán hoặc lớn hơn khi kích thước mô 
hình tăng. Những cải thiện này cho thấy TIỀN HUẤN LUYỆN NGỮ CẢNH cung cấp tín hiệu tiền huấn 
luyện tốt hơn, cho phép LM rèn luyện khả năng mô hình hóa ngôn ngữ tốt hơn.

3.3.2 HỌC NGỮ CẢNH CHO PHÂN LOẠI VĂN BẢN
Bộ dữ liệu & Chỉ số. Học ngữ cảnh yêu cầu thực hiện một tác vụ mà không tinh chỉnh bằng cách 
điều kiện hóa trên một vài ví dụ demonstration về tác vụ. Chúng tôi đánh giá khả năng học ngữ cảnh 
của ICLM sử dụng 32 ví dụ demonstration. Chúng tôi sử dụng bảy bộ dữ liệu phân loại văn bản, bao gồm

--- TRANG 6 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Hình 3: Độ khó hiểu mô hình hóa ngôn ngữ (càng thấp càng tốt) trên Wikipedia, Arxiv, và Books 
(§3.3.1). ICLM vượt trội hơn các baseline một cách nhất quán trên tất cả các kích thước mô hình.

Bảng 1: Hiệu suất học ngữ cảnh trên bảy bộ dữ liệu phân loại (§3.3.2). Chúng tôi sử dụng 32 ví 
dụ ngữ cảnh cho tất cả các bộ dữ liệu. ICLM vượt trội hơn các baseline trên tất cả các bộ dữ liệu.

Phương pháp | Sentiment | Hate Speech | Topic Classification | Average
           | Amazon SST2 Yelp | Hate Offensive | Agnews Dbpedia |
Standard   | 94.6   83.7 74.3 | 52.7    55.7   | 68.3   61.5   | 66.0
kNN        | 88.0   80.2 65.1 | 50.1    53.1   | 65.7   56.4   | 61.8
ICLM       | 96.5   93.2 77.4 | 60.6    57.3   | 76.0   63.2   | 71.3

phân tích tình cảm (SST-2 (Socher et al., 2013), Amazon và Yelp (Zhang et al., 2015a)), phân loại 
chủ đề (AGN (Zhang et al., 2015b) và Dbepdia (Lehmann et al., 2015)) và phát hiện lời nói thù hận 
(Barbieri et al., 2020). Chúng tôi sử dụng các từ nhãn từ Min et al. (2022) và báo cáo độ chính 
xác như chỉ số.

Kết quả. Như được hiển thị trong Bảng 1, ICLM nhất quán thể hiện hiệu suất tốt hơn trên tất cả 
các bộ dữ liệu phân loại văn bản, dẫn đến mức tăng 8% trung bình. Kết quả này cho thấy ICLM tốt 
hơn trong việc học từ các ví dụ demonstration. Chúng tôi phân tích mối quan hệ giữa số lượng ví dụ 
demonstration và hiệu suất của học ngữ cảnh trong §4.3.

3.3.3 ĐỌC HIỂU
Bộ dữ liệu & Chỉ số. Đọc hiểu yêu cầu trả lời câu hỏi dựa trên đoạn văn đã cho. Chúng tôi xem xét 
benchmark đọc hiểu RACE (RACE-High và RACE-Middle) (Lai et al., 2017), SQuAD (Rajpurkar et al., 
2016), BoolQ (Clark et al., 2019), DROP (Dua et al., 2019), và HotpotQA (Yang et al., 2018). Chúng 
tôi sử dụng học ngữ cảnh 2-shot cho đánh giá; chúng tôi không sử dụng nhiều hơn vì một số tài liệu 
trong các tác vụ đọc hiểu rất dài. Chúng tôi báo cáo điểm khớp chính xác cho HotpotQA và SQuAD, 
và độ chính xác cho các bộ dữ liệu khác là các tác vụ đa lựa chọn (RACE, BoolQ, DROP), theo tiêu 
chuẩn trong công trình trước đó.

Kết quả. Bảng 2 nhấn mạnh rằng ICLM nhất quán vượt trội hơn cả baseline tiêu chuẩn và kNN trên 
tất cả các bộ dữ liệu với mức cải thiện trung bình 14%. Đặc biệt, chúng tôi quan sát mức tăng đáng 
kể trên HotpotQA, yêu cầu hiểu đa bước về nhiều tài liệu liên quan. Mức tăng hiệu suất trên các 
tác vụ đọc hiểu cho thấy TIỀN HUẤN LUYỆN NGỮ CẢNH cải thiện khả năng hiểu và suy luận trên ngữ 
cảnh đã cho của LM.

3.3.4 TĂNG CƯỜNG TRUY XUẤT
Bộ dữ liệu & Chỉ số. Tăng cường truy xuất là một phương pháp để truy xuất một tập hợp đoạn văn 
từ corpus văn bản bên ngoài (ví dụ, Wikipedia) và thêm vào đầu truy vấn đầu vào để xử lý tốt hơn 
các truy vấn đầu vào yêu cầu kiến thức thực tế (Lin et al., 2023; Xu et al., 2023; Su et al., 2023; 
Feng et al., 2024). Chúng tôi tiến hành đánh giá trên hai bộ dữ liệu QA miền mở được nghiên cứu 
kỹ: Natural Questions (NQ) (Kwiatkowski et al., 2019) và TriviaQA (Joshi et al., 2017). Cho cả 
hai bộ dữ liệu, chúng tôi báo cáo điểm khớp chính xác (EM) và đánh giá hiệu suất mô hình trong cả 
thiết lập đóng sách và mở sách.

--- TRANG 7 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Bảng 2: Kết quả đọc hiểu, sử dụng học ngữ cảnh 2-shot (§3.3.3). ICLM vượt trội hơn các baseline 
trên tất cả sáu bộ dữ liệu.

Phương pháp | RACE-High | RACE-Middle | BoolQ | SQuAD | HotpotQA | DROP | Average
Standard   | 39.5      | 53.3        | 68.9  | 26.3  | 10.5     | 27.2 | 37.6
kNN        | 36.2      | 51.4        | 65.3  | 23.5  | 14.4     | 25.1 | 36.0
ICLM       | 41.5      | 56.9        | 73.0  | 30.3  | 21.9     | 35.7 | 43.2

Bảng 3: Kết quả trên NQ và TQA (§3.3.4) không có truy xuất (đóng) và có truy xuất (mở).

Phương pháp | NQ        | TQA
           | Closed Open | Closed Open
Standard   | 17.0   28.5 | 49.3   48.1
kNN        | 13.5   20.1 | 40.2   43.2
ICLM       | 17.0   32.2 | 48.0   51.6

Bảng 4: Kết quả trên hai bộ dữ liệu có xung đột kiến thức, yêu cầu suy luận tốt hơn về ngữ cảnh 
đã cho (§3.3.5).

Phương pháp | NQ-Swap | MemoTrap
Standard   | 39.6    | 48.4
kNN        | 42.1    | 54.3
ICLM       | 45.8    | 56.2

Trong thiết lập đóng sách, chúng tôi chỉ cung cấp câu hỏi cho mô hình và mô hình phải trả lời câu 
hỏi dựa trên kiến thức tham số của nó. Trong thiết lập mở sách, chúng tôi tuân theo Shi et al. 
(2023c) trong việc cung cấp cho mô hình top-10 tài liệu được truy xuất từ Wikipedia như ngữ cảnh 
bổ sung cho câu hỏi.

Kết quả. Kết quả được báo cáo trong Bảng 3. Trong thiết lập đóng sách, ICLM thực hiện tương đương 
hoặc hơi kém hơn baseline tiêu chuẩn, có thể vì mô hình của chúng tôi ghi nhớ ít hơn. Tuy nhiên, 
trong thiết lập mở sách, ICLM vượt trội đáng kể hơn baseline tiêu chuẩn trong thiết lập mở sách 
(+9%), đạt hiệu suất tốt hơn nhiều so với thiết lập đóng sách. Cũng đáng chú ý rằng mục tiêu huấn 
luyện của kNN hoàn toàn giống như tăng cường truy xuất, nhưng ICLM vẫn đạt hiệu suất tốt hơn, có 
thể do vấn đề overfitting của kNN như thảo luận trong §2.2.

3.3.5 TÍNH THỰC TẾ
Bộ dữ liệu & Chỉ số. Công trình trước đó đã phát hiện rằng các mô hình ngôn ngữ tạo văn bản không 
thực tế cũng không trung thực với ngữ cảnh đã cho, đặc biệt khi ngữ cảnh mâu thuẫn với kiến thức 
mà mô hình đã thu được trong quá trình tiền huấn luyện (thường được gọi là kiến thức tham số (Longpre 
et al., 2021; Zhou et al., 2023; Shi et al., 2023b; Wang et al., 2023a)). Chúng tôi đánh giá khả 
năng tuân theo hướng dẫn và ngữ cảnh của LM trên hai bộ dữ liệu xung đột kiến thức: NQ-Swap (Longpre 
et al., 2021) và MemoTrap (Liu & Liu, 2023). Cả hai bộ dữ liệu đều chứa hướng dẫn và ngữ cảnh 
mâu thuẫn với kiến thức tham số của các mô hình. Chúng tôi báo cáo điểm khớp chính xác như chỉ số.

Kết quả. Bảng 4 cho thấy ICLM tốt hơn các baseline tiêu chuẩn và kNN trên cả hai bộ dữ liệu, ngụ 
ý rằng TIỀN HUẤN LUYỆN NGỮ CẢNH cải thiện khả năng của LM tạo ra đầu ra trung thực với ngữ cảnh 
trước đó. Mức tăng lớn hơn so với các bộ dữ liệu khác, có thể vì NQ-Swap và MemoTrap nhấn mạnh 
thách thức trong việc suy luận về ngữ cảnh đã cho, mà các LM trước đó gặp khó khăn.

3.3.6 SUY LUẬN NGỮ CẢNH DÀI
Bộ dữ liệu & Chỉ số. Để đánh giá khả năng suy luận ngữ cảnh dài, chúng tôi so sánh ICLM với các 
baseline tiêu chuẩn và kNN trên benchmark SCROLL (Shaham et al., 2022) đánh giá khả năng tổng 
hợp thông tin trên các văn bản dài của LM. Theo thiết lập bài báo gốc, chúng tôi tinh chỉnh các LM 
được tiền huấn luyện (tiêu chuẩn, kNN, TIỀN HUẤN LUYỆN NGỮ CẢNH) trên các bộ dữ liệu huấn luyện 
của scroll và đánh giá chúng trên các bộ dữ liệu kiểm tra. Chúng tôi báo cáo điểm F1 cho các bộ 
dữ liệu Narrative QA, Qasper và ContractNLI và báo cáo điểm ROUGE-1 cho các bộ dữ liệu QMSum và 
GovReport trong benchmark SCROLL.

Kết quả. Kết quả trong Bảng 5 cho thấy ICLM vượt trội hơn các baseline khoảng 5%, cho thấy ICLM 
tốt hơn trong suy luận ngữ cảnh dài. Chúng tôi đưa ra giả thuyết rằng các mức tăng từ ICLM có thể 
phai mờ

--- TRANG 8 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Bảng 5: Hiệu suất trên các benchmark suy luận ngữ cảnh dài từ SCROLL (Shaham et al., 2022) 
(§3.3.6). ICLM vượt trội hơn các baseline trên tất cả năm bộ dữ liệu.

Phương pháp | NarrativeQA | Qasper | ContractNLI | QMSum | GovReport | Average
           | F1          |        |             | ROUGE-1 |          |
Standard   | 16.5        | 34.2   | 78.6        | 25.1    | 8.2      | 32.5
kNN        | 16.8        | 34.1   | 79.5        | 24.3    | 6.6      | 32.3
ICLM       | 17.1        | 36.7   | 80.7        | 26.8    | 9.1      | 34.1

Hình 4: Mất mát huấn luyện và tiến triển hiệu suất trên đọc hiểu trong quá trình tiền huấn luyện. 
Sau khi huấn luyện trên khoảng 150 tỷ token, ICLM nhất quán tốt hơn LM tiêu chuẩn trên các tác 
vụ đọc hiểu và tăng cường truy xuất.

Lựa chọn Thiết kế Phương pháp | PPL
Liên quan Tài liệu Random    | 8.2
Clustering                   | 7.9
Links (cuối cùng)           | 7.3
Ngữ nghĩa Không dedup       | 8.3
Dedup Dedup (cuối cùng)     | 7.3

Hình 5: Nghiên cứu loại bỏ về thiết kế phương pháp của chúng tôi.

Hình 6: Hiệu suất theo số lượng ví dụ ngữ cảnh (k).

đến một mức độ nào đó khi các LM được tinh chỉnh, có thể giải thích các mức tăng tương đối nhỏ trong 
đánh giá này so với các thí nghiệm khác của chúng tôi.

4 PHÂN TÍCH
4.1 TIẾN TRIỂN HIỆU SUẤT TRONG QUÁ TRÌNH TIỀN HUẤN LUYỆN
Trong suốt quá trình tiền huấn luyện, chúng tôi theo dõi chặt chẽ cả mất mát huấn luyện và hiệu 
suất tác vụ downstream cho ICLM cũng như LM tiêu chuẩn. Hình 4 minh họa quỹ đạo của mất mát huấn 
luyện và hiệu suất trên các tác vụ đọc hiểu RACE cho các mô hình 7B. Mất mát huấn luyện cho ICLM 
nhất quán thấp hơn so với LM tiêu chuẩn. Điều này cho thấy rằng, khi dự đoán token tiếp theo, ICLM 
hưởng lợi từ một tập hợp phong phú hơn các tài liệu trước đó liên quan để tham khảo, trong khi LM 
tiêu chuẩn có thông tin hạn chế để dựa vào, dẫn đến mất mát cao hơn. Hình 4 (b, c) cho thấy sau 
khi huấn luyện trên khoảng 150 tỷ token, ICLM nhất quán tốt hơn LM tiêu chuẩn trên các tác vụ đọc 
hiểu. Khoảng cách hiệu suất này duy trì nhất quán trong suốt phần còn lại của giai đoạn tiền huấn 
luyện. Điều này cho thấy quy mô cải thiện bởi TIỀN HUẤN LUYỆN NGỮ CẢNH không giảm và duy trì 
nhất quán khi huấn luyện trên nhiều token hơn.

--- TRANG 9 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
4.2 NGHIÊN CỨU LOẠI BỎ VỀ THIẾT KẾ TIỀN HUẤN LUYỆN NGỮ CẢNH
Chúng tôi thực hiện phân tích về hai lựa chọn thiết kế của TIỀN HUẤN LUYỆN NGỮ CẢNH: lựa chọn 
phương pháp để tìm các tài liệu được truy xuất và khử trùng lặp. Các loại bỏ được thực hiện với 
mô hình 1.5B và đánh giá với độ khó hiểu trên Wikipedia. Kết quả được trình bày trong Hình 5.

Liên quan tài liệu. Một thiết kế chính của TIỀN HUẤN LUYỆN NGỮ CẢNH là nhóm các tài liệu theo 
mức độ liên quan của chúng. Chúng tôi xem xét ba mức độ liên quan: ngẫu nhiên (baseline tiêu chuẩn 
được thảo luận trong §3.2), phân cụm, và phương pháp liên kết tài liệu của chúng tôi trong TIỀN 
HUẤN LUYỆN NGỮ CẢNH. Phân cụm tuân theo phương pháp từ Abbas et al. (2023) trong việc phân cụm 
tài liệu thành 11k cụm dựa trên embedding của chúng và lấy mẫu tài liệu từ mỗi cụm để tạo đầu vào 
huấn luyện. Các tài liệu được nhóm bằng phân cụm có nguồn gốc từ cùng cụm, cho thấy sự tương tự 
chủ đề nhưng không nhất thiết có mối quan hệ gần gũi. Ngược lại, ICLM liên kết các tài liệu như 
các láng giềng gần nhất, cho thấy mức độ tương tự cao hơn. Mức độ liên quan giữa các tài liệu tăng 
từ ngẫu nhiên, phân cụm đến liên kết. Chúng tôi quan sát rằng độ khó hiểu của mô hình ngôn ngữ 
giảm khi mức độ liên quan tăng.

Khử trùng lặp. Chúng tôi so sánh độ khó hiểu của các mô hình được huấn luyện có và không có bước 
khử trùng lặp ngữ nghĩa. Loại bỏ bước khử trùng lặp ngữ nghĩa dẫn đến giảm đáng kể về độ khó hiểu. 
Khi các tài liệu gần trùng lặp có mặt trong cùng ngữ cảnh, các mô hình ngôn ngữ có thể chỉ sao chép 
từ tài liệu trước đó, dẫn đến bất ổn định huấn luyện.

4.3 KÍCH THƯỚC VÍ DỤ DEMONSTRATION CHO HỌC NGỮ CẢNH
Chúng tôi đánh giá các mô hình 7B được huấn luyện với phương pháp tiêu chuẩn và TIỀN HUẤN LUYỆN 
NGỮ CẢNH, sử dụng số lượng ví dụ demonstration khác nhau trên các tác vụ phân loại văn bản được 
mô tả trong §3.3.2. Như được miêu tả trong Hình 6, ICLM duy trì mức tăng hiệu suất nhất quán so 
với phương pháp tiêu chuẩn, ngay cả khi số lượng ví dụ demonstration tăng. Trong khi hiệu suất cải 
thiện khi số lượng ví dụ demonstration tăng, nó ổn định sau 32 ví dụ.

5 CÔNG TRÌNH LIÊN QUAN
Batching dữ liệu dựa trên độ tương tự Công trình trước đó sử dụng batching các đoạn tương tự về 
từ vựng trong cùng batch huấn luyện để xây dựng các cặp tích cực chất lượng cao để huấn luyện các 
mô hình ngôn ngữ tăng cường truy xuất. Ví dụ, Zhong et al. (2022) sử dụng BM25 và cùng tài liệu 
để đảm bảo các đoạn trong cùng batch tương tự với nhau, trong khi Min et al. (2023) nhóm các đoạn 
từ cùng tài liệu trong cùng batch. Phương pháp của chúng tôi chia sẻ cùng tinh thần với những phương 
pháp này ngoại trừ chúng tôi duy trì mức độ liên quan của các tài liệu trong cùng cửa sổ ngữ cảnh, 
nhưng các cửa sổ ngữ cảnh trong batch được trộn. Ngoài ra, trọng tâm của chúng tôi là áp dụng phương 
pháp batching để huấn luyện các mô hình ngôn ngữ tiêu chuẩn.

Tiền huấn luyện với các tài liệu liên quan. Một số nghiên cứu khám phá việc tiền huấn luyện các 
mô hình ngôn ngữ ở quy mô nhỏ sử dụng các tài liệu liên quan. Ví dụ, Yasunaga et al. (2022) kết 
hợp các tài liệu Wikipedia với các siêu liên kết hoặc trích dẫn vào ngữ cảnh đầu vào và tiền huấn 
luyện một LM masked. Yu et al. (2022); Wu et al. (2021) kết hợp định nghĩa từ điển của các từ 
hiếm hoặc sử dụng vector ngữ cảnh từ các ngữ cảnh đã gặp trước đó đề cập đến những từ hiếm này 
trong giai đoạn tiền huấn luyện. Caciularu et al. (2021) thu thập các tài liệu liên quan sử dụng 
một bộ dữ liệu tóm tắt tin tức đa tài liệu được sắp xếp bởi con người (11 triệu token) và tiếp 
tục tiền huấn luyện một LM masked. Lewis et al. (2020) đặt các tài liệu từ cùng ngày trong ngữ 
cảnh đầu vào và tiền huấn luyện LM để tóm tắt các bài báo. Tuy nhiên, các siêu liên kết không 
phải lúc nào cũng có sẵn trên tất cả các miền và các bộ dữ liệu tóm tắt đa tài liệu yêu cầu nỗ 
lực con người để sắp xếp. Ngoài ra, phương pháp của Lewis et al. (2020) hạn chế phạm vi của các 
tài liệu liên quan từ cùng ngày. Ngược lại, chúng tôi giới thiệu một phương pháp tổng quát để thu 
thập các tài liệu liên quan quy mô web không yêu cầu bất kỳ metadata nào (ví dụ, siêu liên kết, 
sắp xếp con người hoặc ngày cụ thể), điều này cần thiết để mở rộng mô hình đến thiết lập tiền huấn 
luyện.

Tinh chỉnh đa tác vụ cho học ngữ cảnh và hướng dẫn. Tinh chỉnh các mô hình ngôn ngữ trên một tập 
hợp các tác vụ downstream để cải thiện khả năng học hướng dẫn và học ngữ cảnh của LM đã được nghiên 
cứu trong một số bài báo. Như được thảo luận bởi Min et al. (2022); Chen et al. (2022); Ivison et 
al. (2023); Wang et al. (2022; 2023b), một kỹ thuật phổ biến nối các hướng dẫn,

--- TRANG 10 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
các mẫu huấn luyện từ các bộ dữ liệu downstream có chú thích con người thành các chuỗi văn bản đơn, 
mà LM sau đó được tinh chỉnh. Theo dòng công trình này, Gu et al. (2023) tạo các bộ dữ liệu downstream 
nội tại bằng cách phát triển một bộ truy xuất cụ thể cho từng tác vụ. Những bộ truy xuất này sau 
đó được sử dụng để truy xuất các ví dụ demonstration từ corpus tiền huấn luyện. Phương pháp tinh 
chỉnh đa tác vụ bổ sung cho TIỀN HUẤN LUYỆN NGỮ CẢNH vì phương pháp trước được thiết kế riêng 
cho giai đoạn tinh chỉnh trong khi phương pháp sau tập trung vào giai đoạn tiền huấn luyện. Ngoài 
việc cải thiện khả năng học ngữ cảnh của LM, TIỀN HUẤN LUYỆN NGỮ CẢNH cũng cải thiện khả năng 
mô hình hóa ngôn ngữ tổng thể, đọc hiểu và kiểm tra sự thật của chúng. Chúng tôi để lại việc kết 
hợp TIỀN HUẤN LUYỆN NGỮ CẢNH với các phương pháp tinh chỉnh đa tác vụ như công việc tương lai.

Huấn luyện các mô hình ngôn ngữ ngữ cảnh dài. Các nghiên cứu gần đây đã điều tra việc tinh chỉnh 
LM để mở rộng độ dài ngữ cảnh của chúng. Press et al. (2022); Chen et al. (2023); kaiokendev (2023) 
thực hiện các sửa đổi cho mã hóa vị trí và tinh chỉnh LM trên các tài liệu ngắn được nối ngẫu nhiên 
và các tài liệu dài được lấy mẫu phụ từ dữ liệu tiền huấn luyện. Tuy nhiên, như được nhấn mạnh bởi 
de Vries (2023), các tài liệu chuỗi dài đáng kể hiếm trong dữ liệu tiền huấn luyện. Ví dụ, ít 
hơn 5% tài liệu trong CommonCrawl có dài hơn 2k token. Trong công trình này, chúng tôi tập trung 
vào việc xây dựng dữ liệu ngữ cảnh dài có ý nghĩa, làm cho các mô hình ngôn ngữ tận dụng tốt hơn 
cửa sổ ngữ cảnh của chúng. Dữ liệu được sắp xếp của chúng tôi có thể được sử dụng cho cả giai đoạn 
tiền huấn luyện và tinh chỉnh để tăng cường khả năng suy luận trên ngữ cảnh của LM.

6 KẾT LUẬN
Chúng tôi giới thiệu TIỀN HUẤN LUYỆN NGỮ CẢNH, một phương pháp tiền huấn luyện mới học cách tạo 
văn bản được điều kiện hóa trên một tập hợp các tài liệu liên quan, phơi bày LM với các ngữ cảnh 
liên quan và cung cấp tín hiệu huấn luyện vượt ra ngoài ranh giới tài liệu. Phương pháp của chúng 
tôi có khả năng mở rộng cao và đơn giản, và hoạt động với bất kỳ pipeline tiền huấn luyện nào bằng 
cách đơn giản thay đổi thứ tự tài liệu trong quá trình tiền xử lý. Đánh giá toàn diện của chúng 
tôi cho thấy phương pháp của chúng tôi dẫn đến cải thiện đáng kể trong nhiều thiết lập đa dạng nhấn 
mạnh khả năng hiểu và suy luận trên ngữ cảnh đã cho, bao gồm học ngữ cảnh, đọc hiểu, tăng cường 
truy xuất, và nhiều hơn nữa. Nghiên cứu tương lai có thể đi sâu vào các kết nối vốn có giữa các 
tài liệu trong các miền corpus cụ thể hoặc sử dụng bộ truy xuất đa ngôn ngữ để nhóm các tài liệu 
đa ngôn ngữ liên quan trong cùng ngữ cảnh. Ví dụ, các script mã trong cùng repository có liên quan. 
Cái nhìn sâu sắc này mở đường cho khám phá tương lai, nơi việc nối toàn bộ repository thành một 
tổng thể thống nhất có thể dẫn đến việc tạo ra các tập dữ liệu ngữ cảnh dài có ý nghĩa.

TÀI LIỆU THAM KHẢO
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-
efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,
2023.

Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval:
Unified benchmark and comparative evaluation for tweet classification. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP 2020, pp. 1644–1650, Online, November 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL
https://aclanthology.org/2020.findings-emnlp.148.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew Peters, Arie Cattan, and Ido Dagan. CDLM:
Cross-document language modeling. In Findings of the Association for Computational Linguistics:
EMNLP 2021, pp. 2648–2662, Punta Cana, Dominican Republic, November 2021. Association

--- TRANG 11 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.225. URL https://
aclanthology.org/2021.findings-emnlp.225.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of
large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language
model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 719–730, 2022.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/
N19-1300. URL https://aclanthology.org/N19-1300.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing
Systems, 2022.

Harm de Vries. In the long (context) run, 2023. URL https://www.harmdevries.com/post/
context-length/.

Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel
Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library, 2024.

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
2368–2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.

Avia Efrat and Omer Levy. The turking test: Can language models understand instructions? ArXiv,
abs/2010.11982, 2020. URL https://api.semanticscholar.org/CorpusID:225062157.

Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.
Knowledge card: Filling LLMs' knowledge gaps with plug-in specialized language models. In The
Twelfth International Conference on Learning Representations, 2024. URL https://openreview.
net/forum?id=WbWtOYIzIK.

Merrill M Flood. The traveling-salesman problem. Operations research, 4(1):61–75, 1956.

Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 4849–4870, Toronto, Canada, July 2023. Association for Computational Linguistics.
doi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023.acl-long.267.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented
language model pre-training. In International conference on machine learning, pp. 3929–3938.
PMLR, 2020.

Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning
using cross-task nearest neighbors. In Findings of ACL, 2023. URL https://arxiv.org/abs/
2212.00196.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.
Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id=
jKN1pXi7b0.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE
Transactions on Big Data, 7(3):535–547, 2019.

--- TRANG 12 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–
1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/
v1/P17-1147. URL https://aclanthology.org/P17-1147.

Hervé Jégou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor
search. IEEE transactions on pattern analysis and machine intelligence, 33:117–28, 01 2011. doi:
10.1109/TPAMI.2010.57.

kaiokendev. Things i'm learning while training superhot, 2023. URL https://kaiokendev.github.
io/til#.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav
Petrov. Natural questions: A benchmark for question answering research. Transactions of the
Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL
https://aclanthology.org/Q19-1026.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding
comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 785–794, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https:
//aclanthology.org/D17-1082.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, et al. Dbpedia–a large-
scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167–195, 2015.

Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The
inductive bias of in-context learning: Rethinking pretraining example design. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
lnEaqbTJIRz.

Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke
Zettlemoyer. Pre-training via paraphrasing. Advances in Neural Information Processing Systems,
33:18470–18481, 2020.

Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,
Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-
augmented dual instruction tuning, 2023.

Alisa Liu and Jiacheng Liu. The memotrap dataset. https://github.com/inverse-scaling/
prize/blob/main/data-release/README.md, 2023.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and
Percy Liang. Lost in the middle: How language models use long contexts, 2023.

Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.
Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing, pp. 7052–7063, Online and Punta Cana,
Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/
v1/2021.emnlp-main.565. URL https://aclanthology.org/2021.emnlp-main.565.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.

Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,
Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik
Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating
Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim,
Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn't better, 2023.

--- TRANG 13 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in
context. In Proceedings of the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 2791–2809, Seattle, United
States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.
201. URL https://aclanthology.org/2022.naacl-main.201.

Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and
Luke Zettlemoyer. Nonparametric masked language modeling. In Findings of the Associa-
tion for Computational Linguistics: ACL 2023, pp. 2097–2118, Toronto, Canada, July 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL
https://aclanthology.org/2023.findings-acl.132.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions with human feedback, 2022.

Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=R8sQPpGCv0.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, pp. 2383–2392, 2016.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-
parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long
language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Associa-
tion for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael
Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In
International Conference on Machine Learning, pp. 31210–31227. PMLR, 2023a.

Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih.
Trusting your evidence: Hallucinate less with context-aware decoding, 2023b.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint
arXiv:2301.12652, 2023c.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.
1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/D13-1170.

Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih,
Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned
text embeddings, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

--- TRANG 14 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023b.

Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and
Yulia Tsvetkov. Resolving knowledge conflicts in large language models. arXiv preprint
arXiv:2310.00935, 2023a.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan
Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,
Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir
Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri,
Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta
Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative
instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pp. 5085–5109, Abu Dhabi, United Arab Emirates, December
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL
https://aclanthology.org/2022.emnlp-main.340.

Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far
can camels go? exploring the state of instruction tuning on open resources, 2023b.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán,
Armand Joulin, and Édouard Grave. Ccnet: Extracting high quality monolingual datasets from
web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference,
pp. 4003–4012, 2020.

Qiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. Taking notes on the fly helps
language pre-training. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=lU5Rs_wCweN.

Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with
compression and selective augmentation, 2023.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Compu-
tational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.

Michihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with
document links. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 8003–8016, Dublin, Ireland, May 2022. Association for
Computational Linguistics. doi: 10.18653/v1/2022.acl-long.551. URL https://aclanthology.
org/2022.acl-long.551.

Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang,
Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language
modeling. 2023.

--- TRANG 15 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael
Zeng, and Meng Jiang. Dict-BERT: Enhancing language model pre-training with dictionary.
In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Associ-
ation for Computational Linguistics: ACL 2022, pp. 1907–1918, Dublin, Ireland, May 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.150. URL
https://aclanthology.org/2022.findings-acl.150.

Susan Zhang, Mona Diab, and Luke Zettlemoyer. Democratizing access to large-scale language
models with opt-175b. Meta AI, 2022.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for
text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015a. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/
250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text
classification. In NIPS, 2015b.

Zhuosheng Zhang, Hai Zhao, and Rui Wang. Machine reading comprehension: The role of contextu-
alized language models and beyond. arXiv preprint arXiv:2005.06249, 2020.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. In International Conference on Machine Learning, pp.
12697–12706. PMLR, 2021.

Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.
5657–5673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.emnlp-main.382. URL https://aclanthology.org/2022.
emnlp-main.382.

Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large
language models. ArXiv, abs/2303.11315, 2023.

--- TRANG 16 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
A BỐI CẢNH BỔ SUNG

A.1 KHỬ TRÙNG LẶP
Các corpus thường có các bản sao ngữ nghĩa: các cặp tài liệu có liên quan về mặt ngữ nghĩa, nhưng 
không hoàn toàn giống hệt nhau. Các nghiên cứu trước (Yasunaga et al., 2023) cho thấy việc huấn 
luyện lại các tài liệu rất tương tự trong các ngữ cảnh đầu vào trong quá trình huấn luyện làm tổn 
hại hiệu suất của các mô hình đa phương thức. Chúng tôi quan sát một hành vi tương tự: khi các tài 
liệu gần trùng lặp có mặt trong cùng ngữ cảnh, các mô hình ngôn ngữ có thể chỉ sao chép từ tài liệu 
trước đó, dẫn đến bất ổn định huấn luyện. Cho rằng phương pháp truy xuất của chúng tôi vốn đánh 
giá độ tương tự tài liệu cặp, chúng ta có thể dễ dàng lọc ra các tài liệu gần trùng lặp có độ tương 
tự cosine cao với các tài liệu khác. Chúng tôi thấy bước khử trùng lặp này rất quan trọng để đạt 
hiệu suất tốt của các mô hình ngôn ngữ (§4.2).

A.2 CHỈ MỤC FAISS
Chúng tôi sử dụng chỉ mục FAISS file đảo ngược lượng hóa sản phẩm (IVFPQ) với kích thước mã 256 
và số lượng danh sách đảo ngược tương ứng 32768, với tổng kích thước 62 gigabyte. Chỉ mục chứa 
235266464 embedding 768 chiều ban đầu ở dạng float 32. Chỉ mục được huấn luyện trên một mẫu gồm 
1572864 embedding và thời gian huấn luyện là 423 giây. Tiếp theo, dữ liệu được chia thành các batch 
50M embedding và cho mỗi shard chỉ mục, batch embedding tương ứng được thêm vào chỉ mục đã huấn 
luyện, thời gian thêm embedding trung bình mỗi shard chỉ mục là 628.4 giây. Cuối cùng, tìm kiếm 
láng giềng gần xấp xỉ được tiến hành trên mỗi shard trước khi tổng hợp tất cả kết quả sử dụng tìm 
kiếm batch lớn faiss. Nprobe được sử dụng để tiến hành tìm kiếm xấp xỉ là 64, có nghĩa là 0.2% 
của các danh sách đảo ngược được thăm dò trong quá trình tìm kiếm láng giềng gần nhất.
