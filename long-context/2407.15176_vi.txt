# 2407.15176.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/long-context/2407.15176.pdf
# KÃ­ch thÆ°á»›c file: 8978592 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================

--- TRANG 1 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025
REATTENTION : Má» Rá»˜NG NGá»® Cáº¢NH VÃ” Háº N KHÃ”NG Cáº¦N HUáº¤N LUYá»†N
Vá»šI PHáº M VI CHÃš Ã Há»®U Háº N
Xiaoran Liu1,3,4âˆ—, Ruixiao Li1,4âˆ—, Qipeng Guo3,4, Zhigeng Liu1, Yuerong Song1,4,
Kai Lv1,3, Hang Yan3, Linlin Li2, Qun Liu2, Xipeng Qiu1,4
1Khoa Khoa há»c MÃ¡y tÃ­nh, Äáº¡i há»c Fudan,2PhÃ²ng thÃ­ nghiá»‡m Noah's Ark Huawei,
3PhÃ²ng thÃ­ nghiá»‡m AI Shanghai,4Viá»‡n Äá»•i má»›i Shanghai
xrliu24@m.fudan.edu.cn ,xpqiu@fudan.edu.cn

TÃ“M Táº®T
Kháº£ nÄƒng ngá»¯ cáº£nh dÃ i cá»§a cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) Ä‘Ã£ cÃ³ nhá»¯ng Ä‘á»™t phÃ¡ Ä‘Ã¡ng ká»ƒ, nhÆ°ng Ä‘á»™ dÃ i ngá»¯ cáº£nh tá»‘i Ä‘a Ä‘Æ°á»£c há»— trá»£ trong viá»‡c ngoáº¡i suy Ä‘á»™ dÃ i váº«n lÃ  má»™t Ä‘iá»ƒm ngháº½n quan trá»ng háº¡n cháº¿ cÃ¡c á»©ng dá»¥ng thá»±c táº¿ cá»§a chÃºng. RÃ ng buá»™c vá» Ä‘á»™ dÃ i ngá»¯ cáº£nh trong LLM phÃ¡t sinh tá»« cÆ¡ cháº¿ tá»± chÃº Ã½ (self-attention), khÃ´ng thá»ƒ náº¯m báº¯t hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a trong ngá»¯ cáº£nh dÃ i vÃ´ háº¡n thÃ´ng qua thÃ´ng tin vá»‹ trÃ­ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n háº¡n cháº¿ vÃ  pháº¡m vi chÃº Ã½. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t ReAttention, má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n cho phÃ©p LLM dá»±a trÃªn cÆ¡ cháº¿ tá»± chÃº Ã½ há»— trá»£ ngá»¯ cáº£nh vÃ´ háº¡n vá»›i pháº¡m vi chÃº Ã½ há»¯u háº¡n dÆ°á»›i tÃ i nguyÃªn bá»™ nhá»› Ä‘á»§. ReAttention thá»±c hiá»‡n chÃº Ã½ top-k báº¥t kháº£ tri vá»‹ trÃ­ trÆ°á»›c tá»± chÃº Ã½ thÃ´ng thÆ°á»ng nháº­n biáº¿t vá»‹ trÃ­, giáº£i phÃ³ng LLM khá»i váº¥n Ä‘á» ngoáº¡i suy Ä‘á»™ dÃ i. ChÃºng tÃ´i xÃ¡c thá»±c hiá»‡u suáº¥t cá»§a ReAttention trÃªn LongBench, L-Eval, vÃ  InfiniteBench vÃ  chá»©ng minh ráº±ng nÃ³ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng. HÆ¡n ná»¯a, chÃºng tÃ´i cÅ©ng Ã¡p dá»¥ng ReAttention trÃªn cÃ¡c LLM chÃ­nh nhÆ° LLaMA3.1-8B vÃ  Mistral-v0.3-7B, cho phÃ©p chÃºng há»— trá»£ Ä‘á»™ dÃ i ngá»¯ cáº£nh Ã­t nháº¥t 1M vÃ  tháº­m chÃ­ má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a LLaMA3.2-3B-chat lÃªn 128Ã— Ä‘áº¿n 4M mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n thÃªm trong cÃ¡c thá»­ nghiá»‡m Needle-In-A-Haystack. ChÃºng tÃ´i cÅ©ng cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a ReAttention vá»›i Triton vÃ  Ä‘áº¡t Ä‘Æ°á»£c ngoáº¡i suy hiá»‡u quáº£ mÃ  khÃ´ng cÃ³ chi phÃ­ bá»• sung. MÃ£ nguá»“n cÃ³ sáºµn táº¡i https://github.com/OpenMOSS/ReAttention.

1 GIá»šI THIá»†U
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) dá»±a trÃªn Transformer (Vaswani et al., 2017; OpenAI, 2023; Reid et al., 2024; Sun et al., 2024) Ä‘Ã£ cÃ³ tiáº¿n bá»™ lá»›n trong cÃ¡c á»©ng dá»¥ng Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP). Äáº·c biá»‡t, trong mÃ´ hÃ¬nh hÃ³a ngá»¯ cáº£nh dÃ i, má»™t lÆ°á»£ng nghiÃªn cá»©u Ä‘Ã¡ng ká»ƒ Ä‘Ã£ Ä‘Æ°á»£c dÃ nh cho viá»‡c má»Ÿ rá»™ng kháº£ nÄƒng Ä‘á»™ dÃ i cá»§a LLM (Chen et al., 2023; Peng et al., 2023; Liu et al., 2023; Xiong et al., 2023), tÄƒng Ä‘á»™ dÃ i ngá»¯ cáº£nh tá»‘i Ä‘a Ä‘Æ°á»£c há»— trá»£ tá»« 2K ban Ä‘áº§u (Touvron et al., 2023) lÃªn 2M (Ding et al., 2024) trong cÃ¡c LLM mÃ£ nguá»“n má»Ÿ. Tuy nhiÃªn, Ä‘á»™ dÃ i ngá»¯ cáº£nh tá»‘i Ä‘a trong ngoáº¡i suy Ä‘á»™ dÃ i váº«n lÃ  má»™t Ä‘iá»ƒm ngháº½n háº¡n cháº¿ cÃ¡c á»©ng dá»¥ng thá»±c táº¿ cá»§a LLM (Press et al., 2022; Chen et al., 2023). Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ´ háº¡n dÆ°á»›i bá»™ nhá»› Ä‘á»§ cho LLM dá»±a trÃªn Transformer, ba Ä‘iá»u kiá»‡n sau pháº£i Ä‘Æ°á»£c thá»a mÃ£n:

a ThÃ´ng tin vá»‹ trÃ­ trong giai Ä‘oáº¡n suy luáº­n khÃ´ng nÃªn náº±m ngoÃ i phÃ¢n phá»‘i (OOD) so vá»›i giai Ä‘oáº¡n huáº¥n luyá»‡n; (Han et al., 2023; Liu et al., 2023)

b Entropy tá»± chÃº Ã½ trong giai Ä‘oáº¡n suy luáº­n khÃ´ng nÃªn tÄƒng theo Ä‘á»™ dÃ i cá»§a Ä‘áº§u vÃ o; (Han et al., 2023; Xiao et al., 2023)

c LLM nÃªn duy trÃ¬ nháº­n thá»©c hiá»‡u quáº£ vá» thÃ´ng tin ngá»¯ cáº£nh quan trá»ng táº¡i má»—i bÆ°á»›c suy luáº­n. (Dong et al., 2024; Zhang et al., 2024a)

CÃ¡c cÃ´ng trÃ¬nh Ä‘áº§u trong ngoáº¡i suy táº­p trung vÃ o Ä‘iá»ƒm Ä‘áº§u tiÃªn, chá»§ yáº¿u báº±ng cÃ¡ch ná»™i suy embedding vá»‹ trÃ­ (Chen et al., 2023; bloc97, 2023b;a; Xiong et al., 2023) hoáº·c giá»›i háº¡n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i trong Ä‘á»™ dÃ i ngá»¯ cáº£nh tiá»n huáº¥n luyá»‡n (Su, 2023; Jin et al., 2024; An et al., 2024). Tuy nhiÃªn, cÃ¡c nghiÃªn cá»©u sau Ä‘Ã³ Ä‘Ã£ chá»‰ ra ráº±ng ngay cáº£ khi thÃ´ng tin vá»‹ trÃ­ khÃ´ng pháº£i OOD, entropy chÃº Ã½ cÃ³ xu hÆ°á»›ng tÄƒng logarit theo Ä‘á»™ dÃ i cá»§a cá»­a sá»• tá»± chÃº Ã½ (Han et al., 2023). Cá»¥ thá»ƒ, phÃ¢n phá»‘i tá»± chÃº Ã½ trá»Ÿ nÃªn ngÃ y cÃ ng khuáº¿ch tÃ¡n khi Ä‘á»™ dÃ i suy luáº­n má»Ÿ rá»™ng, khiáº¿n viá»‡c thu tháº­p thÃ´ng tin tá»« ngá»¯ cáº£nh trá»Ÿ nÃªn khÃ³ khÄƒn, dáº«n Ä‘áº¿n Ä‘áº§u ra mÃ´ hÃ¬nh khÃ´ng á»•n Ä‘á»‹nh (Peng et al., 2023; Han et al., 2023). Äá»ƒ Ä‘Ã¡p á»©ng Ä‘iá»u nÃ y, cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° LM-Infinite (Han et al., 2023) vÃ  StreamingLLM (Xiao et al., 2023) Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t, giá»¯ láº¡i pháº§n Ä‘áº§u vÃ  cuá»‘i cá»§a Ä‘áº§u vÃ o, cho phÃ©p LLM duy trÃ¬ Ä‘áº§u ra á»•n Ä‘á»‹nh khi Ä‘á»™ dÃ i Ä‘áº§u vÃ o tÄƒng. Tuy nhiÃªn, chÃºng lÃ m tá»•n háº¡i Ä‘áº¿n nháº­n thá»©c toÃ n cá»¥c vá» thÃ´ng tin ngá»¯ cáº£nh, áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t downstream.

Äá»‘i vá»›i con ngÆ°á»i, máº·c dÃ¹ chÃºng ta cÃ³ bá»™ nhá»› dÃ i háº¡n, chÃºng ta chá»‰ cáº§n má»™t lÆ°á»£ng nhá» thÃ´ng tin Ä‘á»ƒ suy nghÄ© vÃ  hÃ nh Ä‘á»™ng theo thá»i gian thá»±c. TÆ°Æ¡ng tá»±, trong khi LLM yÃªu cáº§u toÃ n bá»™ ngá»¯ cáº£nh Ä‘á»ƒ hoÃ n thÃ nh quÃ¡ trÃ¬nh suy luáº­n, nÃ³ chá»‰ cáº§n thÃ´ng tin ngá»¯ cáº£nh háº¡n cháº¿ táº¡i má»—i bÆ°á»›c suy luáº­n. Thá»±c táº¿ nÃ y Ä‘Ã£ truyá»n cáº£m há»©ng cho nghiÃªn cá»©u nhÆ° InfLLM (Xiao et al., 2024a) vÃ  LongHeads (Lu et al., 2024) Ä‘á»ƒ nháº­n thá»©c vÃ  trÃ­ch xuáº¥t thÃ´ng tin quan trá»ng tá»« ngá»¯ cáº£nh trÆ°á»›c khi thá»±c hiá»‡n tá»± chÃº Ã½. Tuy nhiÃªn, LongHeads váº«n Ä‘á»‘i máº·t vá»›i giá»›i háº¡n trÃªn vá» thÃ´ng tin vá»‹ trÃ­ do sá»­ dá»¥ng NTK hoáº·c PI cho prefill (Lu et al., 2024), trong khi InfLLM gáº·p bias trong viá»‡c biá»ƒu diá»…n thÃ´ng tin ngá»¯ cáº£nh, phÃ¡t sinh tá»« biá»ƒu diá»…n theo khá»‘i vÃ  embedding vá»‹ trÃ­ kiá»ƒu ReRoPE khi trÃ­ch xuáº¥t thÃ´ng tin chÃ­nh.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i giá»›i thiá»‡u ReAttention, má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n má»Ÿ rá»™ng LLM vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh há»¯u háº¡n Ä‘á»ƒ xá»­ lÃ½ ngá»¯ cáº£nh vÃ´ háº¡n. ReAttention coi viá»‡c trÃ­ch xuáº¥t thÃ´ng tin ngá»¯ cáº£nh quan trá»ng nhÆ° má»™t quÃ¡ trÃ¬nh chÃº Ã½ bá»• sung Ä‘i trÆ°á»›c tá»± chÃº Ã½ truyá»n thá»‘ng, giá»‘ng nhÆ° "suy nghÄ© trÆ°á»›c khi hÃ nh Ä‘á»™ng." Táº¡i má»—i bÆ°á»›c lÃ½ luáº­n, ReAttention chá»n cÃ¡c phÃ¢n Ä‘oáº¡n há»¯u háº¡n quan trá»ng nháº¥t tá»« KV cache mÃ  khÃ´ng cÃ³ thÃ´ng tin vá»‹ trÃ­, ná»‘i chÃºng láº¡i, Ã¡p dá»¥ng embedding vá»‹ trÃ­, vÃ  thá»±c hiá»‡n tá»± chÃº Ã½. Báº±ng cÃ¡ch kiá»ƒm soÃ¡t Ä‘á»™ dÃ i cá»§a cÃ¡c phÃ¢n Ä‘oáº¡n KV cache Ä‘Æ°á»£c chá»n, ReAttention cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ´ háº¡n vá»›i pháº¡m vi chÃº Ã½ há»¯u háº¡n, trong khi Ä‘áº£m báº£o thÃ´ng tin vá»‹ trÃ­ vÃ  entropy chÃº Ã½ khÃ´ng náº±m ngoÃ i phÃ¢n phá»‘i (Han et al., 2023). HÆ¡n ná»¯a, láº¥y cáº£m há»©ng tá»« cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c sá»­ dá»¥ng trong tá»± chÃº Ã½, Ä‘áº·c biá»‡t lÃ  FlashAttention (Dao et al., 2022; Dao, 2023), chÃºng tÃ´i sá»­ dá»¥ng Triton (Tillet et al., 2019), má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh GPU, Ä‘á»ƒ giáº£m thiá»ƒu chi phÃ­ Ä‘á»c vÃ  ghi trong chÃº Ã½ top-k. Vá»›i kernel Triton tÃ¹y chá»‰nh cá»§a chÃºng tÃ´i, ReAttention trÃ¡nh chi phÃ­ tÃ­nh toÃ¡n thÃªm vÃ  giáº£m sá»­ dá»¥ng bá»™ nhá»› cho ngá»¯ cáº£nh dÃ i. CÃ¡c Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:

â€¢ ChÃºng tÃ´i phÃ¡c tháº£o ba yÃªu cáº§u cho ngá»¯ cáº£nh vÃ´ háº¡n trong LLM dá»±a trÃªn Transformer, embedding vá»‹ trÃ­ khÃ´ng OOD, entropy chÃº Ã½ á»•n Ä‘á»‹nh, vÃ  nháº­n thá»©c ngá»¯ cáº£nh hiá»‡u quáº£. ChÃºng tÃ´i cÅ©ng phÃ¡t hiá»‡n ráº±ng yÃªu cáº§u cuá»‘i cÃ¹ng cÃ³ thá»ƒ Ä‘Æ°á»£c thá»a mÃ£n thÃ´ng qua Ä‘iá»ƒm chÃº Ã½ mÃ  khÃ´ng cÃ³ embedding vá»‹ trÃ­.

â€¢ Dá»±a trÃªn quan sÃ¡t nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n, ReAttention, thá»a mÃ£n ba Ä‘iá»u kiá»‡n nÃ³i trÃªn, do Ä‘Ã³ ngoáº¡i suy Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a LLM Ä‘áº¿n vÃ´ háº¡n vá»›i pháº¡m vi chÃº Ã½ há»¯u háº¡n, vÃ  giáº£i phÃ³ng LLM khá»i váº¥n Ä‘á» ngoáº¡i suy Ä‘á»™ dÃ i.

â€¢ ChÃºng tÃ´i xÃ¡c thá»±c ráº±ng ReAttention phÃ¹ há»£p vá»›i hiá»‡u suáº¥t cá»§a tá»± chÃº Ã½ truyá»n thá»‘ng trong ngá»¯ cáº£nh dÃ i mÃ  khÃ´ng cÃ³ chi phÃ­ tÃ­nh toÃ¡n vÃ  sá»­ dá»¥ng bá»™ nhá»› Ã­t hÆ¡n. Cá»¥ thá»ƒ, ReAttention má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a cÃ¡c LLM hÃ ng Ä‘áº§u, nhÆ° LLaMA3.1-8B-128K, Ä‘áº¿n Ã­t nháº¥t 1 triá»‡u token. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n nhÆ° LLaMA3.2-3B-chat, Ä‘á»™ dÃ i ngá»¯ cáº£nh cÃ³ thá»ƒ Ä‘Æ°á»£c tÄƒng lÃªn 128Ã—, Ä‘áº¡t Ä‘áº¿n 4 triá»‡u token, mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n bá»• sung.

2 PHÆ¯Æ NG PHÃP
Cáº¥u trÃºc tá»•ng thá»ƒ cá»§a ReAttention Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1, bao gá»“m chÃº Ã½ top-k báº¥t kháº£ tri vá»‹ trÃ­ chá»‹u trÃ¡ch nhiá»‡m lá»±a chá»n cache toÃ n ngá»¯ cáº£nh vÃ  biáº¿n Ä‘á»•i tá»± chÃº Ã½ truyá»n thá»‘ng vá»›i embedding vá»‹ trÃ­. ReAttention Ä‘áº¡t Ä‘Æ°á»£c tÃ­ch há»£p khÃ´ng cáº§n huáº¥n luyá»‡n giá»¯a hai thÃ nh pháº§n.

2.1 Lá»°AC CHá»ŒN CACHE TOÃ€N NGá»® Cáº¢NH
Trong khi LLM yÃªu cáº§u ngá»¯ cáº£nh dÃ i hoÃ n chÃ¬nh Ä‘á»ƒ thá»±c hiá»‡n toÃ n bá»™ quÃ¡ trÃ¬nh suy luáº­n, chá»‰ cáº§n má»™t phÃ¢n Ä‘oáº¡n ngá»¯ cáº£nh háº¡n cháº¿ táº¡i má»—i bÆ°á»›c suy luáº­n (Xiao et al., 2023; 2024a; Lu et al., 2024). Xem xÃ©t pháº§n Ä‘áº§u vÃ  cuá»‘i cá»§a ngá»¯ cáº£nh Ä‘áº§u vÃ o tÆ°Æ¡ng á»©ng vá»›i cÃ¡c prompt quan trá»ng toÃ n cá»¥c vÃ  thÃ´ng tin cá»¥c bá»™ cho suy luáº­n (Xiao et al., 2023), ReAttention giá»¯ láº¡i cáº£ phÃ¢n Ä‘oáº¡n toÃ n cá»¥c vÃ  cá»¥c bá»™ cá»§a KV cache Ä‘á»ƒ Ä‘Æ°a vÃ o quÃ¡ trÃ¬nh tá»± chÃº Ã½.

Kcache= [Kglobal,Kmiddle,Klocal],Vcache= [Vglobal,Vmiddle,Vlocal]. (1)

--- TRANG 2 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

--- TRANG 3 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

EmbeddingSelf-Attention
with PEFeed ForwardOutput
TopK -Attention
without PEğ‘³-Layer
ğ‘¸ğ‘²
ğ‘½ğ‘²â€²
ğ‘½â€²
ğ‘¸ğ‘¶
Long -Term Memory
(KV Cache)Short -Term Memory

HÃ¬nh 1: Tá»•ng quan vá» ReAttention. Sau Ä‘Ã³, ReAttention sá»­ dá»¥ng vector truy váº¥n cá»§a bÆ°á»›c hiá»‡n táº¡i Ä‘á»ƒ thá»±c hiá»‡n lá»±a chá»n top-k trÃªn pháº§n giá»¯a cá»§a KV cache (Ribar et al., 2023), Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c phÃ¢n Ä‘oáº¡n cache quan trá»ng nháº¥t cho bÆ°á»›c hiá»‡n táº¡i nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1.

Indices =top-k
qtKT
middle
,
Kselect=Kmiddle[Indices ],
Vselect=Vmiddle[Indices ].(2)

ReAttention thá»±c hiá»‡n lá»±a chá»n toÃ n ngá»¯ cáº£nh trÃªn KV cache trong má»—i lá»›p, cho phÃ©p cÃ¡c lá»›p khÃ¡c nhau chá»n KV cache khÃ¡c nhau cho viá»‡c tÃ­nh toÃ¡n. HÆ¡n ná»¯a, vÃ¬ má»—i lá»›p chÃº Ã½ cÃ³ nhiá»u Ä‘áº§u chÃº Ã½ vÃ  ReAttention Ã¡p dá»¥ng Ä‘áº§u vÃ o luá»“ng theo khá»‘i trong giai Ä‘oáº¡n prefilling, nhiá»u vector truy váº¥n cÃ³ thá»ƒ tá»“n táº¡i Ä‘á»“ng thá»i. Trong trÆ°á»ng há»£p nÃ y, ReAttention bá» phiáº¿u dá»±a trÃªn cÃ¡c lá»±a chá»n top-k tá»« cÃ¡c Ä‘áº§u vÃ  vector truy váº¥n khÃ¡c nhau Ä‘á»ƒ xÃ¡c Ä‘á»‹nh top-kâ€² KV cache. NgoÃ i ra, Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh liÃªn káº¿t ngá»¯ nghÄ©a, ReAttention khÃ´ng chá»‰ giá»¯ láº¡i báº£n thÃ¢n cÃ¡c pháº§n tá»­ top-kâ€² mÃ  cÃ²n trÃ­ch xuáº¥t m má»¥c lÃ¢n cáº­n nhÆ° má»™t tá»•ng thá»ƒ. CÃ¡c pháº§n chá»“ng láº¥p Ä‘Æ°á»£c loáº¡i bá» trÃ¹ng láº·p.

KhÃ´ng giá»‘ng nhÆ° lá»±a chá»n dá»±a trÃªn khá»‘i trong cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y (Lu et al., 2024; Xiao et al., 2024a), ReAttention trÃ­ch xuáº¥t thÃ´ng tin ngá»¯ cáº£nh quan trá»ng báº±ng tÃ­ch vÃ´ hÆ°á»›ng giá»¯a qt vÃ  Kcache, thay vÃ¬ biá»ƒu diá»…n theo khá»‘i cá»§a Kcache. PhÆ°Æ¡ng phÃ¡p nÃ y cung cáº¥p kháº£ nÄƒng thÃ­ch á»©ng máº¡nh vÃ  trÃ¡nh sá»± phÃ¢n máº£nh ngá»¯ nghÄ©a do phÃ¢n khá»‘i cá»‘ Ä‘á»‹nh gÃ¢y ra (Luo et al., 2024). HÆ¡n ná»¯a, vÃ¬ káº¿t quáº£ trung gian cá»§a qtKT
middle quÃ¡ lá»›n Ä‘á»‘i vá»›i cÃ¡c tÃ¬nh huá»‘ng ngá»¯ cáº£nh dÃ i, vÃ  sáº½ phÃ¡t sinh chi phÃ­ Ä‘á»c vÃ  ghi ráº¥t lá»›n, chÃºng tÃ´i theo cÃ¡ch tiáº¿p cáº­n cá»§a FlashAttention (Dao et al., 2022; Dao, 2023), há»£p nháº¥t toÃ n bá»™ quÃ¡ trÃ¬nh vÃ o má»™t kernel báº±ng Triton (Tillet et al., 2019), nhÆ° Ä‘Æ°á»£c nÃªu chi tiáº¿t trong Pháº§n 3.4.

2.2 TÃCH Há»¢P KHÃ”NG Cáº¦N HUáº¤N LUYá»†N
ReAttention ná»‘i cÃ¡c phÃ¢n Ä‘oáº¡n KV cache Ä‘Æ°á»£c chá»n giá»¯a pháº§n toÃ n cá»¥c vÃ  cá»¥c bá»™, Ã¡p dá»¥ng embedding vá»‹ trÃ­ tuáº§n tá»±, vÃ  báº£o tá»“n thá»© tá»± tÆ°Æ¡ng Ä‘á»‘i trong khi bá» qua khoáº£ng cÃ¡ch tuyá»‡t Ä‘á»‘i giá»¯a cÃ¡c phÃ¢n Ä‘oáº¡n Ä‘Æ°á»£c chá»n, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1. Tá»± chÃº Ã½ sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho KV cache Ä‘Æ°á»£c ná»‘i.

Kcache' = [Kglobal,Kselect,Klocal],Vcache' = [Vglobal,Vselect,Vlocal],
Ëœqt,ËœKcache' =PE(qt,Kcache'),ot=SelfAttn
Ëœqt,ËœKcache',Vcache'
.(3)

Äiá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ , khÃ´ng giá»‘ng nhÆ° triá»ƒn khai thÃ´ng thÆ°á»ng trong Huggingface Transformers (Wolf et al., 2020), nÆ¡i embedding vá»‹ trÃ­ Ä‘Æ°á»£c Ã¡p dá»¥ng trÆ°á»›c khi KV caching, trong ReAttention, embedding vá»‹ trÃ­ Ä‘Æ°á»£c tÃ¡ch ra khá»i KV cache vÃ  Ä‘Æ°á»£c thá»±c hiá»‡n sau khi lá»±a chá»n KV cache. NghÄ©a lÃ , KV Ä‘Æ°á»£c cache khÃ´ng bao gá»“m thÃ´ng tin vá»‹ trÃ­. Thiáº¿t káº¿ nÃ y cung cáº¥p má»™t sá»‘ lá»£i tháº¿. Má»™t máº·t, nhÆ° Ä‘Ã£ Ä‘á» cáº­p trong Pháº§n 4, Ä‘iá»ƒm chÃº Ã½ mÃ  khÃ´ng cÃ³ embedding vá»‹ trÃ­ cÃ³ lá»£i hÆ¡n cho viá»‡c Ä‘á»‹nh vá»‹ thÃ´ng tin chÃ­nh trong ngá»¯ cáº£nh. Máº·t khÃ¡c, vÃ¬ Ä‘á»™ dÃ i cache Ä‘Æ°á»£c ná»‘i váº«n náº±m trong Ä‘á»™ dÃ i ngá»¯ cáº£nh tiá»n huáº¥n luyá»‡n hoáº·c giá»›i háº¡n trÃªn ngoáº¡i suy, embedding vá»‹ trÃ­ sáº½ khÃ´ng bao giá» lÃ  OOD (Han et al., 2023).

HÆ¡n ná»¯a, cÃ¡c phÃ¢n Ä‘oáº¡n KV cache khÃ´ng Ä‘Æ°á»£c chá»n khÃ´ng quan trá»ng trong bÆ°á»›c suy luáº­n hiá»‡n táº¡i, vÃ¬ Ä‘iá»ƒm tá»± chÃº Ã½ cá»§a chÃºng lÃ  tá»‘i thiá»ƒu (Zhang et al., 2024d). Do Ä‘Ã³, viá»‡c sá»­a Ä‘á»•i nÃ y khÃ´ng lÃ m háº¡i Ä‘áº§u ra tá»± chÃº Ã½ vÃ  cÃ³ thá»ƒ loáº¡i bá» sá»± can thiá»‡p tá»« thÃ´ng tin khÃ´ng liÃªn quan (Zhang et al., 2024d; Ge et al., 2023). So vá»›i InfLLM (Xiao et al., 2024a), embedding vá»‹ trÃ­ vÃ  biáº¿n Ä‘á»•i chÃº Ã½ cá»§a ReAttention trong cá»­a sá»• tiá»n huáº¥n luyá»‡n khÃ´ng giá»›i thiá»‡u máº«u chÃº Ã½ chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n, do Ä‘Ã³ ngÄƒn ngá»«a tÃ­ch lÅ©y bias trong KV cache trong giai Ä‘oáº¡n prefilling. Do Ä‘Ã³, ReAttention cÃ³ thá»ƒ bá» qua thÃ´ng tin vá»‹ trÃ­, Ä‘áº¡t Ä‘Æ°á»£c ngá»¯ cáº£nh chÃº Ã½ khÃ´ng giá»›i háº¡n vá»›i pháº¡m vi chÃº Ã½ háº¡n cháº¿ mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n nÃ o, vÃ  váº«n tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p gia tá»‘c chÃº Ã½ hiá»‡n cÃ³ (Dao et al., 2022; Dao, 2023).

--- TRANG 4 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

[Báº£ng 1: Káº¿t quáº£ cá»§a LLaMA Series (Meta, 2024a; Dubey et al., 2024; Meta, 2024b) trÃªn LongBench (Bai et al., 2023). ReAttention Ä‘áº¡t Ä‘Æ°á»£c Æ°u tháº¿ nháº¥t quÃ¡n so vá»›i StreamingLLM (Xiao et al., 2023) vÃ  InfLLM (Xiao et al., 2024a) vÃ  cho tháº¥y hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i LLM cÃ³ chÃº Ã½ Ä‘áº§y Ä‘á»§.]

[Báº£ng 2: Káº¿t quáº£ cá»§a LLaMA Series trÃªn InfiniteBench (Zhang et al., 2024b) á»Ÿ cÃ¡c Ä‘á»™ dÃ i ngá»¯ cáº£nh khÃ¡c nhau. "-" cÃ³ nghÄ©a lÃ  LLM khÃ´ng thá»ƒ cung cáº¥p Ä‘áº§u ra á»•n Ä‘á»‹nh á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh nháº¥t Ä‘á»‹nh. ReAttention Ä‘áº¡t Ä‘Æ°á»£c Æ°u tháº¿ so vá»›i StreamingLLM (Xiao et al., 2023), InfLLM (Xiao et al., 2024a) vÃ  chÃº Ã½ Ä‘áº§y Ä‘á»§.]

3 THá»°C NGHIá»†M

3.1 THIáº¾T Láº¬P
ChÃºng tÃ´i tiáº¿n hÃ nh thá»­ nghiá»‡m trÃªn LLaMA3-8B-8K (Meta, 2024a), LLaMA3.1-8B-128K (Dubey et al., 2024), LLaMA3.1-70B-128K (Dubey et al., 2024), LLaMA3.2-3B-128K (Dubey et al., 2024), Mistral-v0.3-7B-32K (mistralai, 2024), InternLM2.5-7B-1M (InternLM, 2024), Qwen2-7B-128K (Yang et al., 2024a), Qwen2-72B-128K (Yang et al., 2024a), Qwen2-1B-32K (Yang et al., 2024a). Äá»‘i vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh, chÃºng tÃ´i Ä‘áº·t Ä‘á»™ dÃ i cá»§a Kglobal lÃ  32, Ä‘á»™ dÃ i cá»§a Klocal lÃ  4096, vÃ  kÃ­ch thÆ°á»›c khoáº£ng Ä‘Æ°á»£c chá»n lÃ  32. HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘áº·t k = 4, kâ€² = 127 trong chÃº Ã½ top-k. Quan trá»ng lÃ , pháº¡m vi chÃº Ã½ trong má»—i bÆ°á»›c váº«n náº±m trong cá»­a sá»• chÃº Ã½ tá»‘i Ä‘a. VÃ­ dá»¥, Ä‘á»‘i vá»›i LLaMA3-8B-8K vá»›i ReAttention, kÃ­ch thÆ°á»›c pháº¡m vi chÃº Ã½ tá»‘i Ä‘a lÃ  32 + 4096 + 127 Ã— 32, khá»›p chÃ­nh xÃ¡c vá»›i cá»­a sá»• chÃº Ã½ tá»‘i Ä‘a Ä‘Æ°á»£c há»— trá»£ lÃ  8192. ChÃºng tÃ´i sá»­ dá»¥ng OpenCompass (Contributors, 2023b) Ä‘á»ƒ xÃ¡c thá»±c. Táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i Ä‘á»™ chÃ­nh xÃ¡c FP16 vÃ  Ä‘Æ°á»£c gia tá»‘c vá»›i FlashAttention2 (Dao, 2023).

3.2 ÄÃNH GIÃ BENCHMARK NGá»® Cáº¢NH DÃ€I
ChÃºng tÃ´i Ä‘áº§u tiÃªn Ä‘Ã¡nh giÃ¡ táº¥t cáº£ 9 LLM trÃªn benchmark ngá»¯ cáº£nh dÃ i thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng LongBench (Bai et al., 2023) vÃ  L-Eval (An et al., 2023), vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh máº·c Ä‘á»‹nh lÃ  32K vÃ  cáº¯t bá»›t giá»¯a.

--- TRANG 5 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

[HÃ¬nh 2: Káº¿t quáº£ cá»§a cÃ¡c LLM chÃ­nh Ä‘Æ°á»£c tÄƒng cÆ°á»ng ReAttention hiá»‡n cÃ³, bao gá»“m LLaMA3-8B-8K vÃ  Mistral-v0.3-7B-32K, trÃªn Needle-In-A-Haystack (Contributors, 2023a) Ä‘Æ°á»£c triá»ƒn khai trong OpenCompass (Contributors, 2023b).]

Äá»‘i vá»›i LLaMA3-8B-8K, cÃ³ Ä‘á»™ dÃ i ngá»¯ cáº£nh dÆ°á»›i 32K, chÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t cá»§a nÃ³ vá»›i Dynamic NTK (bloc97, 2023a). Äá»‘i vá»›i triá»ƒn khai Dynamic NTK, chÃºng tÃ´i sá»­ dá»¥ng cÃ i Ä‘áº·t máº·c Ä‘á»‹nh tá»« Huggingface Transformers (Wolf et al., 2020), Ä‘áº·t há»‡ sá»‘ tá»· lá»‡ lÃ  4. NgoÃ i ra, chÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t cá»§a táº¥t cáº£ 9 LLM vá»›i StreamingLLM (Xiao et al., 2023) nhÆ° má»™t nghiÃªn cá»©u loáº¡i bá», sá»­ dá»¥ng cÃ¹ng Ä‘á»™ dÃ i phÃ¢n Ä‘oáº¡n toÃ n cá»¥c vÃ  cá»¥c bá»™ nhÆ° cÃ i Ä‘áº·t ReAttention.

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1 vÃ  bá»• sung trong Báº£ng 5 á»Ÿ Phá»¥ lá»¥c B, ReAttention vÆ°á»£t trá»™i hÆ¡n StreamingLLM trÃªn táº¥t cáº£ 9 mÃ´ hÃ¬nh, chá»‰ ra ráº±ng lá»±a chá»n toÃ n ngá»¯ cáº£nh thu Ä‘Æ°á»£c thÃ´ng tin há»¯u Ã­ch cho suy luáº­n ngá»¯ cáº£nh dÃ i. HÆ¡n ná»¯a, ReAttention hoáº¡t Ä‘á»™ng ngang báº±ng vá»›i chÃº Ã½ Ä‘áº§y Ä‘á»§ vÃ  tháº­m chÃ­ vÆ°á»£t qua nÃ³ trong má»™t sá»‘ trÆ°á»ng há»£p, nhÆ° LLaMA3.1-70B-128K (Meta, 2024a) vÃ  Qwen2-1B-32K (Yang et al., 2024a). Äiá»u nÃ y chá»©ng minh ráº±ng ReAttention cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho LLM vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau vÃ  Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t gáº§n vá»›i chÃº Ã½ Ä‘áº§y Ä‘á»§ trÃªn cÃ¡c nhiá»‡m vá»¥ downstream (Jiang et al., 2024).

Äá»ƒ chá»©ng minh thÃªm vá» Æ°u tháº¿ vÃ  kháº£ nÄƒng ngoáº¡i suy cá»§a ReAttention, chÃºng tÃ´i xÃ¡c thá»±c phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh trÃªn InfiniteBench (Zhang et al., 2024c), má»™t benchmark thÃ¡ch thá»©c hÆ¡n vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh dÃ i hÆ¡n. ChÃºng tÃ´i chá»n 3 nhiá»‡m vá»¥ phá»¥ thÆ°á»ng Ä‘Æ°á»£c kiá»ƒm tra, En.MC, En.QA vÃ  En.Sum, Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh thay Ä‘á»•i, vÃ  so sÃ¡nh ReAttention vá»›i DynamicNTK (bloc97, 2023a) vÃ  InfLLM (Xiao et al., 2024a) vá»›i cÃ¹ng cáº¥u hÃ¬nh lá»±a chá»n. Káº¿t quáº£Â¹ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2.

ÄÃ¡ng chÃº Ã½, ReAttention liÃªn tá»¥c vÆ°á»£t trá»™i hÆ¡n chÃº Ã½ Ä‘áº§y Ä‘á»§ vÃ  InfLLM á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh 128K vÃ  trong Ä‘iá»ƒm trung bÃ¬nh. Trong khi DynamicNTK hoáº¡t Ä‘á»™ng tá»‘t á»Ÿ 32k, nÃ³ gáº·p pháº£i giá»›i háº¡n trÃªn rÃµ rÃ ng vá» ngoáº¡i suy, vÆ°á»£t quÃ¡ Ä‘Ã³ mÃ´ hÃ¬nh khÃ´ng thá»ƒ táº¡o ra Ä‘áº§u ra á»•n Ä‘á»‹nh. HÆ¡n ná»¯a, trong khi InfLLM cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ´ háº¡n (Xiao et al., 2024a), nÃ³ váº«n thua kÃ©m ReAttention trong cÃ¡c nhiá»‡m vá»¥ downstream do trÃ­ch xuáº¥t thÃ´ng tin quan trá»ng khÃ´ng chÃ­nh xÃ¡c vÃ  sá»± khÃ¡c biá»‡t trong Ä‘á»‹nh dáº¡ng embedding vá»‹ trÃ­ so vá»›i giai Ä‘oáº¡n tiá»n huáº¥n luyá»‡n.

3.3 ÄÃNH GIÃ NEEDLE-IN-A-HAYSTACK
Dá»±a trÃªn Ä‘Ã¡nh giÃ¡ benchmark ngá»¯ cáº£nh dÃ i, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng truy xuáº¥t máº¡nh trong Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n cá»§a chÃºng vÃ  tiáº¿n hÃ nh Ä‘Ã¡nh giÃ¡ Needle-In-A-Haystack (NIAH) (Contributors, 2023a;b). ChÃºng tÃ´i thá»±c hiá»‡n thá»­ nghiá»‡m trÃªn 8 GPU A100 vÃ  má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a LLM vá»›i ReAttention Ä‘áº¿n Ã­t nháº¥t 1M token. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2, LLM vá»›i ReAttention duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t cao Ä‘Ã¡ng ká»ƒ trÃªn toÃ n bá»™ pháº¡m vi Ä‘á»™ dÃ i ngá»¯ cáº£nh mÃ  chÃºng cÃ³ thá»ƒ há»— trá»£, báº¥t ká»ƒ cá»­a sá»• chÃº Ã½ gá»‘c cá»§a chÃºng. Quan trá»ng lÃ , chÃºng tÃ´i cÅ©ng má»Ÿ rá»™ng ngá»¯ cáº£nh cá»§a cÃ¡c LLM chÃ­nh nhÆ° LLaMA3-8B-8K (Meta, 2024a) vÃ  Mistral-v0.3-7B-32K (mistralai, 2024) Ä‘áº¿n Ã­t nháº¥t 1M, cung cáº¥p cho cá»™ng Ä‘á»“ng má»™t giáº£i phÃ¡p hiá»‡u quáº£ Ä‘á»ƒ triá»ƒn khai LLM ngá»¯ cáº£nh dÃ i.

Â¹Do Ä‘Ã¡nh giÃ¡ má»Ÿ rá»™ng cho táº¥t cáº£ 9 mÃ´ hÃ¬nh, vÃ  xem xÃ©t ráº±ng InfLLM cÃ³ há»— trá»£ háº¡n cháº¿ cho cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n nhÆ° LLaMA3.1-70B-chat vÃ  cÃ¡c series khÃ¡c nhÆ° InternLM2.5 vÃ  Qwen2, chÃºng tÃ´i chá»‰ bÃ¡o cÃ¡o káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh LLaMA series á»Ÿ quy mÃ´ 8B vÃ  3B.

--- TRANG 6 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

[HÃ¬nh 3: Káº¿t quáº£ cá»§a Multi-Needle-In-A-Haystack (Reid et al., 2024) vÃ  Single NIAH trong ngá»¯ cáº£nh dÃ i hÆ¡n Ä‘Æ°á»£c triá»ƒn khai trong OpenCompass (Contributors, 2023b).]

Dá»±a trÃªn Ä‘iá»u nÃ y, chÃºng tÃ´i tÄƒng Ä‘á»™ khÃ³ cá»§a Ä‘Ã¡nh giÃ¡ báº±ng cÃ¡ch tiáº¿n hÃ nh thá»­ nghiá»‡m trÃªn multi-NIAH vÃ  single-NIAH trong ngá»¯ cáº£nh dÃ i hÆ¡n, vá»›i káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3. Do Ä‘á»™ khÃ³ tÄƒng, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c phiÃªn báº£n instruct tÆ°Æ¡ng á»©ng cá»§a cÃ¡c mÃ´ hÃ¬nh. TrÃªn LLaMA3.1-8B-Instruct-128K chÃ­nh (Dubey et al., 2024), chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ dÃ i ngá»¯ cáº£nh 1M trong nhiá»‡m vá»¥ multi-NIAH. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n, nhÆ° LLaMA3.2-3B-chat-128K (Meta, 2024b), chÃºng tÃ´i má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh Ä‘áº¿n Ã­t nháº¥t 2M. ÄÃ¡ng chÃº Ã½, Ä‘á»‘i vá»›i Qwen2-1B-Instruct-32k (Yang et al., 2024a), chÃºng tÃ´i má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh Ä‘áº¿n 4M trÃªn 4 GPU A100, Ä‘áº¡t Ä‘Æ°á»£c má»Ÿ rá»™ng ngá»¯ cáº£nh khÃ´ng cáº§n huáº¥n luyá»‡n 128Ã—. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, Ä‘Ã¢y lÃ  má»©c khuáº¿ch Ä‘áº¡i lá»›n nháº¥t cá»§a Ä‘á»™ dÃ i ngá»¯ cáº£nh cho LLM Ä‘áº¡t Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n bá»• sung. Äiá»u nÃ y chá»©ng minh ráº±ng ReAttention cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh trong khi bá» qua thÃ´ng tin vá»‹ trÃ­, sá»­ dá»¥ng cá»­a sá»• chÃº Ã½ há»¯u háº¡n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c ngá»¯ cáº£nh chÃº Ã½ vÃ´ háº¡n.

3.4 PHÃ‚N TÃCH HIá»†U SUáº¤T

[HÃ¬nh 4: Tá»•ng quan vá» kernel fusion trong kernel chÃº Ã½ top-k tÃ¹y chá»‰nh cá»§a chÃºng tÃ´i. CÃ¡c Ä‘o lÆ°á»ng hiá»‡u suáº¥t pháº£n Ã¡nh thá»i gian thá»±c thi cá»§a cÃ¡c hÃ m kernel tÆ°Æ¡ng á»©ng, vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o 8K cho cÃ¡c nhiá»‡m vá»¥ suy luáº­n Llama3.1-8B.]

Trong framework PyTorch, cÃ¡c toÃ¡n tá»­ thá»±c thi Ä‘á»™c láº­p, yÃªu cáº§u I/O thÆ°á»ng xuyÃªn Ä‘áº¿n bá»™ nhá»› GPU, Ä‘iá»u nÃ y giá»›i thiá»‡u chi phÃ­ vÃ  Ä‘á»™ trá»… khÃ´ng cáº§n thiáº¿t. Láº¥y cáº£m há»©ng tá»« FlashAttention (Dao et al., 2022; Dao, 2023), chÃºng tÃ´i phÃ¡t triá»ƒn má»™t hÃ m kernel GPU cho chÃº Ã½ top-k Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 2.1 báº±ng Triton. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4, kernel cá»§a chÃºng tÃ´i há»£p nháº¥t cÃ¡c toÃ¡n tá»­ Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘iá»ƒm chÃº Ã½ vÃ  tÃ­nh toÃ¡n top-k, cho phÃ©p toÃ n bá»™ quÃ¡ trÃ¬nh cháº¡y trong cache GPU. Do Ä‘Ã³ I/O bá»™ nhá»› GPU Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ, cáº£i thiá»‡n cáº£ viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU vÃ  thá»i gian cháº¡y. ChÃºng tÃ´i Ä‘Ã£ thÃªm chÃº Ã½ top-k giáº£m thá»i gian cho tá»± chÃº Ã½, vÃ  kernel cá»§a chÃºng tÃ´i giá»¯ chi phÃ­ cá»§a chÃº Ã½ top-k á»Ÿ má»©c tá»‘i thiá»ƒu, Ä‘á»ƒ láº¡i Ä‘á»™ trá»… tá»•ng thá»ƒ khÃ´ng thay Ä‘á»•i.

ChÃºng tÃ´i phÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a kernel cá»§a chÃºng tÃ´i vá» thá»i gian thá»±c thi GPU vÃ  sá»­ dá»¥ng bá»™ nhá»›, cho tháº¥y ráº±ng sá»­ dá»¥ng Triton thay vÃ¬ PyTorch cho chÃº Ã½ top-k tÄƒng cÆ°á»ng Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t GPU trong khi giáº£m tiÃªu thá»¥ bá»™ nhá»› GPU. NgoÃ i ra, chÃºng tÃ´i so sÃ¡nh ReAttention vá»›i triá»ƒn khai tiÃªu chuáº©n trong HuggingFace Transformers (Dai et al., 2019), Ä‘o lÆ°á»ng thá»i gian Ä‘áº¿n token Ä‘áº§u tiÃªn (TTFT) vÃ  chi phÃ­ bá»™ nhá»› á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh tá»« 32k Ä‘áº¿n 256k. Káº¿t quáº£ so sÃ¡nh TTFT vÃ  throughput Ä‘Æ°á»£c nÃªu chi tiáº¿t trong Phá»¥ lá»¥c C. Táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn má»™t há»‡ thá»‘ng vá»›i CPU 48 nhÃ¢n, RAM 256GB, vÃ  GPU A800-80GB.

Hiá»‡u suáº¥t ToÃ¡n tá»­ Triton ChÃºng tÃ´i kiá»ƒm tra toÃ¡n tá»­ Triton cá»§a chÃºng tÃ´i báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘áº§u vÃ o suy luáº­n thá»±c (vÃ­ dá»¥: ma tráº­n Q vÃ  K Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« cÃ¡c nhiá»‡m vá»¥ suy luáº­n thá»±c táº¿) Ä‘á»ƒ Ä‘o lÆ°á»ng thá»i gian thá»±c thi vÃ  sá»­ dá»¥ng bá»™ nhá»› thiáº¿t bá»‹. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 5a, triá»ƒn khai PyTorch tiÃªu chuáº©n vÆ°á»£t quÃ¡ 80GB bá»™ nhá»› cho cÃ¡c chuá»—i trÃªn 64k, trong khi toÃ¡n tá»­ Triton cá»§a chÃºng tÃ´i giáº£m thiá»ƒu viá»‡c sá»­ dá»¥ng bá»™ nhá»›, giá»›i háº¡n á»Ÿ ma tráº­n Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. NÃ³ cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t nhanh hÆ¡n hÃ ng trÄƒm láº§n á»Ÿ Ä‘á»™ dÃ i chuá»—i 64k vÃ  má»Ÿ rá»™ng hiá»‡u quáº£ khi Ä‘á»™ dÃ i tÄƒng (HÃ¬nh 5b).

Sá»­ dá»¥ng Bá»™ nhá»› trong Giai Ä‘oáº¡n Prefilling Chi phÃ­ bá»™ nhá»› háº¡n cháº¿ viá»‡c prefilling cho cÃ¡c chuá»—i dÃ i hÆ¡n. HÃ¬nh 6 cho tháº¥y phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÆ°á»£t qua triá»ƒn khai tiÃªu chuáº©n trong HuggingFace Transformers

--- TRANG 7 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

[HÃ¬nh 5: Sá»± há»£p tÃ¡c hiá»‡u suáº¥t giá»¯a kernel Triton cá»§a chÃºng tÃ´i vÃ  phiÃªn báº£n PyTorch cá»§a nÃ³. Kernel chÃº Ã½ top-k há»£p nháº¥t cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n triá»ƒn khai PyTorch trÃªn táº¥t cáº£ Ä‘á»™ dÃ i ngá»¯ cáº£nh.]

[HÃ¬nh 6: Äá»™ trá»… token Ä‘áº§u tiÃªn (FTL) vÃ  tiÃªu thá»¥ bá»™ nhá»› GPU trong giai Ä‘oáº¡n prefilling. FullAttn Ä‘á» cáº­p Ä‘áº¿n triá»ƒn khai HuggingFace Transformers chÃ­nh thá»©c cá»§a Llama3.1-8B-Base, háº¿t bá»™ nhá»› sau 192k.]

khi Ä‘á»™ dÃ i ngá»¯ cáº£nh tÄƒng, tiáº¿p tá»¥c hoáº¡t Ä‘á»™ng khi triá»ƒn khai tiÃªu chuáº©n háº¿t bá»™ nhá»›. ChÃºng tÃ´i cÅ©ng ghi láº¡i Ä‘á»™ trá»… token Ä‘áº§u tiÃªn, váº«n cÃ³ thá»ƒ so sÃ¡nh vá»›i triá»ƒn khai tiÃªu chuáº©n trong Huggingface Transformers.

4 THáº¢O LUáº¬N

4.1 PHÃ‚N TÃCH Vá»€ SIÃŠU THAM Sá»
ChÃºng tÃ´i Ä‘áº§u tiÃªn tháº£o luáº­n vá» viá»‡c lá»±a chá»n siÃªu tham sá»‘. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a LLaMA3-8B-8K (Meta, 2024a) trÃªn benchmark LongBench (Bai et al., 2023), so sÃ¡nh cÃ¡c siÃªu tham sá»‘ bao gá»“m kÃ­ch thÆ°á»›c khá»‘i, kÃ­ch thÆ°á»›c khoáº£ng, kÃ­ch thÆ°á»›c cá»¥c bá»™, vÃ  giÃ¡ trá»‹ top-k. VÃ¬ khá»‘i prefilling luÃ´n Ä‘Æ°á»£c chá»©a trong pháº§n cá»¥c bá»™ trong tá»± chÃº Ã½, kÃ­ch thÆ°á»›c khá»‘i pháº£i nhá» hÆ¡n kÃ­ch thÆ°á»›c cá»¥c bá»™. NgoÃ i ra, Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh cÃ´ng báº±ng trong cÃ¡c so sÃ¡nh cá»§a chÃºng tÃ´i, chÃºng tÃ´i duy trÃ¬ kÃ­ch thÆ°á»›c cá»­a sá»• chÃº Ã½ tá»‘i Ä‘a nháº¥t quÃ¡n qua cÃ¡c cÃ i Ä‘áº·t khÃ¡c nhau, cá»¥ thá»ƒ lÃ  giá»¯ tá»•ng cá»§a kÃ­ch thÆ°á»›c toÃ n cá»¥c, kÃ­ch thÆ°á»›c khoáº£ng nhÃ¢n vá»›i kâ€², vÃ  kÃ­ch thÆ°á»›c cá»¥c bá»™ báº±ng 8192. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c toÃ n cá»¥c báº±ng kÃ­ch thÆ°á»›c khoáº£ng, giá»¯ láº¡i pháº§n sá»›m nháº¥t nhÆ° phÃ¢n Ä‘oáº¡n lá»±a chá»n Ä‘áº§u tiÃªn.

[Báº£ng 3: PhÃ¢n tÃ­ch siÃªu tham sá»‘ ReAttention trÃªn LLaMA3-8B-8K vÃ  LongBench, theo máº·c Ä‘á»‹nh kÃ­ch thÆ°á»›c khá»‘i 512, kÃ­ch thÆ°á»›c khoáº£ng 32 cÅ©ng nhÆ° kÃ­ch thÆ°á»›c toÃ n cá»¥c, kÃ­ch thÆ°á»›c cá»¥c bá»™ 4096, vÃ  lá»±a chá»n top-k.]

--- TRANG 8 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

[HÃ¬nh 7: Trá»±c quan hÃ³a phÃ¢n phá»‘i chÃº Ã½ cho InternLM2-7B-200K (Cai et al., 2024b) Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn Needle-In-A-Haystack (Contributors, 2023a;b) trong Ä‘á»™ dÃ i ngá»¯ cáº£nh 32K. Trong má»—i hÃ¬nh phá»¥, má»—i heatmap Ä‘áº¡i diá»‡n cho phÃ¢n phá»‘i chÃº Ã½ cá»§a má»™t lá»›p duy nháº¥t, vá»›i trá»¥c y tÆ°Æ¡ng á»©ng vá»›i 32 Ä‘áº§u chÃº Ã½ vÃ  trá»¥c x tÆ°Æ¡ng á»©ng vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh 32K. 32 token Ä‘áº§u tiÃªn, pháº§n hÆ°á»›ng dáº«n, vÃ  6 token cuá»‘i cÃ¹ng, pháº§n Ä‘Æ°á»£c táº¡o, khÃ´ng bá»‹ loáº¡i trá»«. CÃ¡c giÃ¡ trá»‹ Ä‘áº¡i diá»‡n cho phÃ¢n phá»‘i tá»± chÃº Ã½ tÃ­ch lÅ©y cho má»—i viá»‡c táº¡o token. MÃ u sÃ¡ng hÆ¡n, chÃº Ã½ tÃ­ch lÅ©y cao hÆ¡n. Má»™t thao tÃ¡c pooling vá»›i kÃ­ch thÆ°á»›c kernel 100 Ä‘Æ°á»£c Ã¡p dá»¥ng tuáº§n tá»± Ä‘á»ƒ hiá»ƒn thá»‹ máº«u rÃµ rÃ ng hÆ¡n. Há»™p Ä‘á» Ä‘áº¡i diá»‡n cho vá»‹ trÃ­ cá»§a "kim" trong ngá»¯ cáº£nh.]

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 3. ChÃºng tÃ´i phÃ¡t hiá»‡n ráº±ng khi kÃ­ch thÆ°á»›c khá»‘i giáº£m tá»« 2048 xuá»‘ng 512, hiá»‡u suáº¥t cáº£i thiá»‡n dáº§n dáº§n. Giai Ä‘oáº¡n prefilling trá»Ÿ nÃªn chÃ­nh xÃ¡c hÆ¡n trong viá»‡c trÃ­ch xuáº¥t thÃ´ng tin quan trá»ng trong má»—i bÆ°á»›c chÃº Ã½ top-k. Tuy nhiÃªn, vÃ¬ prefilling máº¥t nhiá»u thá»i gian hÆ¡n khi kÃ­ch thÆ°á»›c khá»‘i giáº£m, chÃºng tÃ´i chá»n 512 lÃ m giÃ¡ trá»‹ máº·c Ä‘á»‹nh mÃ  khÃ´ng giáº£m thÃªm. Tiáº¿p theo, chÃºng tÃ´i kiá»ƒm tra tÃ¡c Ä‘á»™ng cá»§a cÃ¡c kÃ­ch thÆ°á»›c khoáº£ng khÃ¡c nhau. Trong sá»‘ cÃ¡c siÃªu tham sá»‘ nÃ y, kÃ­ch thÆ°á»›c khoáº£ng cÃ³ áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ nháº¥t Ä‘áº¿n hiá»‡u suáº¥t downstream. Khi kÃ­ch thÆ°á»›c khoáº£ng tÆ°Æ¡ng Ä‘á»‘i nhá», cÃ¡c phÃ¢n Ä‘oáº¡n phÃ¢n máº£nh lÃ m suy yáº¿u tÃ­nh liÃªn káº¿t vÃ  cÃ³ thá»ƒ dáº«n lá»‡ch cÃ¡c dá»± Ä‘oÃ¡n cá»§a LLM, dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‡ hÆ¡n so vá»›i StreamingLLM. Khi kÃ­ch thÆ°á»›c khoáº£ng Ä‘áº¡t 32, hiá»‡u suáº¥t trÃªn LongBench Ä‘áº¡t Ä‘á»‰nh. VÆ°á»£t qua Ä‘iá»ƒm nÃ y, do rÃ ng buá»™c cá»§a embedding vá»‹ trÃ­ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n, sá»‘ lÆ°á»£ng phÃ¢n Ä‘oáº¡n Ä‘Æ°á»£c trÃ­ch xuáº¥t giáº£m, cáº£n trá»Ÿ viá»‡c náº¯m báº¯t hiá»‡u quáº£ thÃ´ng tin ngá»¯ cáº£nh quan trá»ng. ChÃºng tÃ´i cÅ©ng so sÃ¡nh áº£nh hÆ°á»Ÿng cá»§a sá»‘ top-k vÃ  phÃ¡t hiá»‡n ráº±ng top-4 mang láº¡i káº¿t quáº£ tá»‘t nháº¥t. Sá»‘ top-k nhá» hÆ¡n khiáº¿n LLM khÃ³ xÃ¡c Ä‘á»‹nh thÃ´ng tin quan trá»ng, trong khi sá»‘ top-k lá»›n hÆ¡n cÃ³ thá»ƒ giá»›i thiá»‡u ná»™i dung khÃ´ng liÃªn quan can thiá»‡p vÃ o viá»‡c Ä‘Ã¡nh giÃ¡. Cuá»‘i cÃ¹ng, chÃºng tÃ´i phÃ¢n tÃ­ch áº£nh hÆ°á»Ÿng cá»§a kÃ­ch thÆ°á»›c cá»¥c bá»™. KÃ­ch thÆ°á»›c cá»¥c bá»™ nhá» hÆ¡n lÃ m tá»•n háº¡i kháº£ nÄƒng cá»§a LLM duy trÃ¬ tÃ­nh liÃªn káº¿t ngá»¯ nghÄ©a, vÃ  lá»±a chá»n quÃ¡ má»©c vÆ°á»£t quÃ¡ ngá»¯ cáº£nh cá»¥c bá»™ cÃ³ thá»ƒ lÃ m lá»‡ch KV cache trong giai Ä‘oáº¡n prefilling. Do Ä‘Ã³, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c cá»¥c bá»™ báº±ng má»™t ná»­a Ä‘á»™ dÃ i ngá»¯ cáº£nh tiá»n huáº¥n luyá»‡n cho LLaMA3-8B-8K vÃ  Ã¡p dá»¥ng cáº¥u hÃ¬nh siÃªu tham sá»‘ nÃ y cho cÃ¡c LLM khÃ¡c.

4.2 Lá»°ACHá»ŒN CACHE Báº¤T KHáº¢ TRI Vá»Š TRÃ
Trong ReAttention, lá»±a chá»n cache toÃ n ngá»¯ cáº£nh dá»±a trÃªn tÃ­ch vÃ´ hÆ°á»›ng giá»¯a cÃ¡c vector truy váº¥n vÃ  khÃ³a mÃ  khÃ´ng cÃ³ embedding vá»‹ trÃ­. Äiá»u nÃ y cÃ³ thá»ƒ táº¡o ra khoáº£ng cÃ¡ch giá»¯a viá»‡c lá»±a chá»n KV cache vÃ  phÃ¢n phá»‘i tá»± chÃº Ã½. Do Ä‘Ã³, á»Ÿ Ä‘Ã¢y náº£y sinh cÃ¢u há»i liá»‡u viá»‡c sá»­ dá»¥ng cÃ¡c vector ngá»¯ nghÄ©a mÃ  khÃ´ng cÃ³ thÃ´ng tin vá»‹ trÃ­ cÃ³ thá»ƒ Ä‘á»‹nh vá»‹ hiá»‡u quáº£ thÃ´ng tin quan trá»ng trong ngá»¯ cáº£nh hay khÃ´ng. Äá»ƒ phÃ¢n tÃ­ch Ä‘iá»u nÃ y, chÃºng tÃ´i chá»n má»™t trÆ°á»ng há»£p Ä‘Ãºng vÃ  má»™t trÆ°á»ng há»£p sai tá»« káº¿t quáº£ Ä‘Ã¡nh giÃ¡ Needle-In-A-Haystack (Contributors, 2023a;b) trÃªn InternLM2-7B-200K (Cai et al., 2024b) vá»›i chÃº Ã½ Ä‘áº§y Ä‘á»§ trong Ä‘á»™ dÃ i ngá»¯ cáº£nh 32K. Äá»‘i vá»›i má»—i trÆ°á»ng há»£p, chÃºng tÃ´i tÃ­nh toÃ¡n phÃ¢n phá»‘i chÃº Ã½ vá»›i embedding vá»‹ trÃ­, tá»©c lÃ  phÃ¢n phá»‘i tá»± chÃº Ã½ thá»±c, vÃ  phÃ¢n phá»‘i chÃº Ã½ mÃ  khÃ´ng cÃ³ embedding vá»‹ trÃ­. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 7.

Äá»‘i vá»›i trÆ°á»ng há»£p Ä‘Ãºng, cÃ¡c phÃ¢n phá»‘i chÃº Ã½, cáº£ cÃ³ vÃ  khÃ´ng cÃ³ embedding vá»‹ trÃ­, cÃ³ thá»ƒ Ä‘á»‹nh vá»‹ vá»‹ trÃ­ cá»§a "kim" trong "Ä‘á»‘ng cá» khÃ´". Tuy nhiÃªn, phÃ¢n phá»‘i chÃº Ã½ vá»›i embedding vá»‹ trÃ­ xuáº¥t hiá»‡n khuáº¿ch tÃ¡n hÆ¡n, vÃ  hiá»‡u á»©ng nÃ y trá»Ÿ nÃªn rÃµ rá»‡t hÆ¡n á»Ÿ cÃ¡c lá»›p cao hÆ¡n cá»§a mÃ´ hÃ¬nh. Äá»‘i vá»›i trÆ°á»ng há»£p sai, phÃ¢n phá»‘i chÃº Ã½ vá»›i embedding vá»‹ trÃ­ cÅ©ng ráº¥t khuáº¿ch tÃ¡n vÃ  bá»‹ áº£nh hÆ°á»Ÿng máº¡nh bá»Ÿi má»™t Ä‘áº§u chÃº Ã½ cá»¥ thá»ƒ bá»‹ dáº«n lá»‡ch bá»Ÿi thÃ´ng tin khÃ´ng liÃªn quan tá»« cÃ¡c lá»›p dÆ°á»›i. Äiá»u nÃ y cuá»‘i cÃ¹ng dáº«n Ä‘áº¿n sá»± tháº¥t báº¡i cá»§a mÃ´ hÃ¬nh.

ThÃº vá»‹ lÃ , khi quan sÃ¡t phÃ¢n phá»‘i chÃº Ã½ mÃ  khÃ´ng cÃ³ embedding vá»‹ trÃ­, chÃºng tÃ´i phÃ¡t hiá»‡n ráº±ng mÃ´ hÃ¬nh cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh vá»‹ trÃ­ cá»§a "kim" báº±ng tÃ­ch vÃ´ hÆ°á»›ng cá»§a cÃ¡c vector ngá»¯ nghÄ©a vÃ  lá»c hiá»‡u quáº£ má»™t lÆ°á»£ng lá»›n nhiá»…u khÃ´ng liÃªn quan. Thá»±c táº¿, InternLM2-7B-200K Ä‘Æ°á»£c tÄƒng cÆ°á»ng ReAttention cÃ³ thá»ƒ Ä‘á»‹nh vá»‹ thÃ nh cÃ´ng "kim" trong trÆ°á»ng há»£p nÃ y. Do Ä‘Ã³, tÃ­ch vÃ´ hÆ°á»›ng cá»§a cÃ¡c vector ngá»¯ nghÄ©a trong lá»±a chá»n cache lÃ  há»£p lÃ½, vÃ  nÃ³ chá»©ng minh hiá»‡u quáº£ hÆ¡n trong viá»‡c tÃ¬m ngá»¯ cáº£nh liÃªn quan so vá»›i tÃ­ch vÃ´ hÆ°á»›ng nháº­n biáº¿t vá»‹ trÃ­. NgoÃ i ra, do khÃ´ng cÃ³ embedding vá»‹ trÃ­ trong KV cache, ReAttention cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»Ÿ rá»™ng ngá»¯ cáº£nh hiá»‡u quáº£.

4.3 QUAN SÃT Vá»€ CÃC NHIá»†M Vá»¤ Tá»”NG Há»¢P
PhÃ¢n tÃ­ch trÃªn Ä‘Ã£ chá»©ng minh hiá»‡u suáº¥t tá»‘t cá»§a ReAttention trong cÃ¡c tÃ¬nh huá»‘ng ngá»¯ cáº£nh dÃ i khÃ¡c nhau, gá»£i Ã½ ráº±ng nÃ³ cÃ³ thá»ƒ ngoáº¡i suy Ä‘áº¿n Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ´ háº¡n. Tuy nhiÃªn, cÃ¡c Ä‘Ã¡nh giÃ¡ nÃ y chá»§ yáº¿u táº­p trung vÃ o ngá»¯ cáº£nh tá»± nhiÃªn thÃ´ng thÆ°á»ng vÃ  thiáº¿u cÃ¡c benchmark hiá»‡n Ä‘Æ°á»£c tháº£o luáº­n rá»™ng rÃ£i (Kuratov et al., 2024; Li et al., 2024), nhÆ° RULER (Hsieh et al., 2024). Tháº­t khÃ´ng may, ReAttention hoáº¡t Ä‘á»™ng kÃ©m trÃªn benchmark RULER. TrÃªn thá»±c táº¿, cÃ¡c phÆ°Æ¡ng phÃ¡p ngoáº¡i suy khÃ´ng dá»±a trÃªn chÃº Ã½ Ä‘áº§y Ä‘á»§, bao gá»“m ReAttention vÃ  InfLLM, khÃ´ng thá»ƒ vÆ°á»£t qua benchmark RULER. VÃ­ dá»¥, sá»­ dá»¥ng LLaMA3-8B-8K (Meta, 2024a), chÃºng tÃ´i so sÃ¡nh ngoáº¡i suy Dynamic NTK vá»›i InfLLM (Xiao et al., 2024a) vÃ  ReAttention á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh 8K vÃ  16K. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 4. Chá»‰ cÃ³ Dynamic NTK Ä‘áº¡t Ä‘Æ°á»£c ngoáº¡i suy hiá»‡u quáº£, trong khi cáº£ ReAttention vÃ  InfLLM Ä‘á»u thá»ƒ hiá»‡n sá»± suy giáº£m hiá»‡u suáº¥t rÃµ rá»‡t hÆ¡n khi Ä‘á»™ dÃ i ngá»¯ cáº£nh tÄƒng. Cá»¥ thá»ƒ, chÃºng tÃ´i táº­p trung bÃ¡o cÃ¡o káº¿t quáº£ cá»§a hai nhiá»‡m vá»¥ phá»¥ tá»« RULER: NIAH-Single3 vÃ  NIAH-MultiKey3 (Hsieh et al., 2024). Cáº£ hai nhiá»‡m vá»¥ Ä‘á»u liÃªn quan Ä‘áº¿n viá»‡c trÃ­ch xuáº¥t thÃ´ng tin chÃ­nh tá»« ngá»¯ cáº£nh chá»©a cÃ¡c má»¥c gÃ¢y hiá»ƒu láº§m. Sá»± khÃ¡c biá»‡t lÃ  ngá»¯ cáº£nh cho Single3 bao gá»“m vÄƒn báº£n tá»± nhiÃªn, nÆ¡i thÃ´ng tin chÃ­nh lÃ  má»™t chuá»—i kÃ½ tá»± chá»¯ vÃ  sá»‘ ngáº«u nhiÃªn, trong khi MultiKey3 cÃ³ má»™t ngÄƒn xáº¿p cÃ¡c cáº·p khÃ³a-giÃ¡ trá»‹ Ä‘Æ°á»£c táº¡o thÃ nh tá»« nhá»¯ng chuá»—i Ä‘Ã³. ChÃºng tÃ´i phÃ¡t hiá»‡n ráº±ng trong khi cáº£ ba phÆ°Æ¡ng phÃ¡p Ä‘á»u cÃ³ thá»ƒ thÃ nh cÃ´ng trong thá»­ nghiá»‡m Single3, chá»‰ cÃ³ Dynamic NTK dá»±a trÃªn chÃº Ã½ Ä‘áº§y Ä‘á»§ má»›i cÃ³ thá»ƒ xá»­ lÃ½ hiá»‡u quáº£ MultiKey3.

[Báº£ng 4: Hiá»‡u suáº¥t cá»§a Dynamic NTK, InfLLM, vÃ  ReAttention trÃªn benchmark RULER á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh 8K vÃ  16K. S3 vÃ  MK3 lÃ  dáº¡ng viáº¿t táº¯t cá»§a NIAH-Single3 vÃ  NIAH-Multikey3 tÆ°Æ¡ng á»©ng.]

[HÃ¬nh 8: Trá»±c quan hÃ³a K Cache tá»« lá»›p cuá»‘i cÃ¹ng cá»§a LLaMA3-8B-8K vá»›i ngoáº¡i suy Dynamic NTK sau khi Ä‘áº§u vÃ o Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn tá»« cÃ¡c táº­p con Single3 vÃ  MultiKey3 trong RULER. Viá»‡c trá»±c quan hÃ³a sá»­ dá»¥ng phÃ©p chiáº¿u t-SNE 2D, vá»›i má»—i token Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t Ä‘iá»ƒm trong hÃ¬nh vÃ  chá»‰ sá»‘ Ä‘áº§u vÃ o Ä‘Æ°á»£c hiá»ƒn thá»‹ qua viá»‡c thay Ä‘á»•i mÃ u sáº¯c.]

Äá»ƒ khÃ¡m phÃ¡ cÃ¡c cÆ¡ cháº¿ Ä‘áº±ng sau quan sÃ¡t, chÃºng tÃ´i thá»±c hiá»‡n trá»±c quan hÃ³a t-SNE (Van der Maaten & Hinton, 2008) cá»§a K cache trong LLaMA3-8B-8K sá»­ dá»¥ng Dynamic NTK cho cáº£ Single3 vÃ  MultiKey3, sá»­ dá»¥ng K cache trong lá»›p cuá»‘i cÃ¹ng lÃ m vÃ­ dá»¥. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8.

Äá»‘i vá»›i Single3, chá»§ yáº¿u bao gá»“m vÄƒn báº£n tá»± nhiÃªn, K cache Ä‘Æ°á»£c giáº£m thá»ƒ hiá»‡n má»™t Ä‘a táº¡p duy nháº¥t vá»›i má»™t cá»¥m cá»¥c bá»™ vÃ  má»™t Ä‘áº·c Ä‘iá»ƒm liÃªn káº¿t tá»•ng thá»ƒ khi Ä‘á»™ dÃ i Ä‘áº§u vÃ o tÄƒng. NgÆ°á»£c láº¡i, Ä‘á»‘i vá»›i MultiKey3, bá»‹ chi phá»‘i bá»Ÿi vÄƒn báº£n há»—n loáº¡n (Lv et al., 2024), K cache Ä‘Æ°á»£c giáº£m tiáº¿t lá»™ nhiá»u Ä‘a táº¡p chá»“ng láº­p thay vÃ¬ má»Ÿ rá»™ng dá»c theo má»™t Ä‘Æ°á»ng cong duy nháº¥t. Äiá»u nÃ y dáº«n Ä‘áº¿n thá»±c táº¿ lÃ  trong quÃ¡ trÃ¬nh lá»±a chá»n K cache trong MultiKey3, báº¥t ká»ƒ chÃºng ta sá»­ dá»¥ng cÃ¹ng mÃ£ hÃ³a chá»‰ sá»‘ hay mÃ£ hÃ³a tuáº§n tá»± trá»±c tiáº¿p cho cÃ¡c phÃ¢n Ä‘oáº¡n Ä‘Æ°á»£c chá»n, tá»± chÃº Ã½ tiáº¿p theo khÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh phÃ¢n Ä‘oáº¡n thuá»™c nhÃ¡nh Ä‘a táº¡p nÃ o. Chá»‰ báº±ng cÃ¡ch mÃ£ hÃ³a táº¥t cáº£ K cache theo chá»‰ sá»‘ vá»‹ trÃ­ gá»‘c cá»§a chÃºng, LLM má»›i cÃ³ thá»ƒ nháº­n biáº¿t ngáº§m (Chi et al., 2023; Kazemnejad et al., 2024) nhÃ¡nh Ä‘a táº¡p nÃ o mÃ  nÃ³ cÆ° trÃº. ÄÃ¢y lÃ  lÃ½ do táº¡i sao chá»‰ chÃº Ã½ Ä‘áº§y Ä‘á»§ má»›i cÃ³ thá»ƒ xá»­ lÃ½ hiá»‡u quáº£ cÃ¡c nhiá»‡m vá»¥ nhÆ° váº­y.

Tuy nhiÃªn, cÃ¡c vÄƒn báº£n dÃ i há»—n loáº¡n tá»•ng há»£p khÃ¡ hiáº¿m trong cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿ (Lv et al., 2024). Trong cÃ¡c benchmark vÄƒn báº£n dÃ i tá»± nhiÃªn thÃ´ng thÆ°á»ng mÃ  chÃºng tÃ´i Ä‘Ã£ kiá»ƒm tra, ReAttention liÃªn tá»¥c chá»©ng minh hiá»‡u suáº¥t máº¡nh. Do Ä‘Ã³, ReAttention váº«n cÃ³ kháº£ nÄƒng cáº¡nh tranh cho á»©ng dá»¥ng trong viá»‡c má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a LLM Ä‘áº¿n vÃ´ háº¡n. ChÃºng tÃ´i cung cáº¥p tháº£o luáº­n sÃ¢u vá» cÃ¡c Ä‘áº·c Ä‘iá»ƒm Ä‘Æ°á»£c pháº£n Ã¡nh trong cÃ¡c máº«u má»Ÿ rá»™ng chuá»—i QK giá»¯a vÄƒn báº£n tá»•ng há»£p vÃ  tá»± nhiÃªn trong Phá»¥ lá»¥c E.

5 CÃ”NG TRÃŒNH LIÃŠN QUAN
Ngoáº¡i suy Ä‘á»™ dÃ i lÃ  má»™t váº¥n Ä‘á» quan trá»ng Ä‘á»‘i vá»›i LLM (Press et al., 2022), cá»¥ thá»ƒ lÃ  huáº¥n luyá»‡n trong ngá»¯ cáº£nh ngáº¯n, vÃ  duy trÃ¬ hiá»‡u suáº¥t tá»‘t trong ngá»¯ cáº£nh dÃ i hÆ¡n. NghiÃªn cá»©u ngoáº¡i suy chÃ­nh chá»§ yáº¿u táº­p trung vÃ o Ä‘iá»u chá»‰nh Rotary Position Embedding (RoPE) (Su et al., 2021). VÃ­ dá»¥, Linear PI (Chen et al., 2023) Ä‘áº§u tiÃªn Ä‘áº¡t Ä‘Æ°á»£c ngoáº¡i suy Ä‘á»™ dÃ i trong LLM báº±ng cÃ¡ch chia tá»· lá»‡ cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ vá» pháº¡m vi tiá»n huáº¥n luyá»‡n vá»›i Ã­t tinh chá»‰nh. PhÆ°Æ¡ng phÃ¡p NTK (bloc97, 2023b;a; Peng et al., 2023) sau Ä‘Ã³ Ä‘iá»u chá»‰nh cÆ¡ sá»Ÿ xoay trong RoPE (Su et al., 2021) Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c ngoáº¡i suy Ä‘á»™ dÃ i plug-and-play. Tiáº¿p theo, viá»‡c khuáº¿ch Ä‘áº¡i cÆ¡ sá»Ÿ xoay vÃ  huáº¥n luyá»‡n trÃªn Ä‘á»™ dÃ i dÃ i hÆ¡n Ä‘Ã£ trá»Ÿ thÃ nh cÃ¡ch tiáº¿p cáº­n chá»§ Ä‘áº¡o cho ngoáº¡i suy Ä‘á»™ dÃ i (RozÃ¬ere et al., 2023; Xiong et al., 2023; Liu et al., 2023; Ding et al., 2024), nhÆ°ng táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»u cÃ³ giá»›i háº¡n trÃªn ngoáº¡i suy rÃµ rÃ ng. NgoÃ i ra, ReRoPE (Su, 2023), Self-Extend (Jin et al., 2024), vÃ  ChunkLLaMA (An et al., 2024) cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c ngoáº¡i suy plug-and-play báº±ng cÃ¡ch giá»›i háº¡n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i. Tuy nhiÃªn, táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p trÃªn Ä‘á»u dá»±a trÃªn chÃº Ã½ Ä‘áº§y Ä‘á»§, Ä‘á»‘i máº·t vá»›i váº¥n Ä‘á» entropy chÃº Ã½ tÄƒng vá»t vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o, vÃ  do Ä‘Ã³ khÃ´ng thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ´ háº¡n (Han et al., 2023; Wang et al., 2024).

NgÆ°á»£c láº¡i, má»™t hÆ°á»›ng nghiÃªn cá»©u khÃ¡c Ä‘Ã£ cá»‘ gáº¯ng má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a cÃ¡c mÃ´ hÃ¬nh thÃ´ng qua chÃº Ã½ thÆ°a. Xem xÃ©t phÃ¢n phá»‘i tá»± chÃº Ã½ cÃ³ xu hÆ°á»›ng táº­p trung vÃ o ngá»¯ cáº£nh toÃ n cá»¥c vÃ  cá»¥c bá»™, StreamingLLM vÃ  LM-Infinite Ä‘á» xuáº¥t cá»­a sá»• chÃº Ã½ hÃ¬nh Î› Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ dÃ i Ä‘áº§u vÃ o gáº§n nhÆ° khÃ´ng giá»›i háº¡n (Xiao et al., 2023; Han et al., 2023). Tuy nhiÃªn, vÃ¬ Ä‘áº§u vÃ o Ä‘Æ°á»£c Ä‘i kÃ¨m vá»›i viá»‡c loáº¡i bá» ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³, nÃ³ váº«n khÃ´ng thá»ƒ má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh (Dong et al., 2024). Dá»±a trÃªn StreamingLLM, InfLLM (Xiao et al., 2024a) vÃ  LongHeads (Lu et al., 2024) cá»‘ gáº¯ng má»Ÿ rá»™ng ngá»¯ cáº£nh thÃ´ng qua truy xuáº¥t theo khá»‘i tá»« cache giá»¯a, nhÆ°ng cÃ¡c váº¥n Ä‘á» phÃ¢n máº£nh ngá»¯ nghÄ©a vÃ  biá»ƒu diá»…n khá»‘i cÅ©ng áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t nhiá»‡m vá»¥ downstream (Luo et al., 2024). Gáº§n Ä‘Ã¢y, MInference (Jiang et al., 2024) vÃ  RetrievalAttention Liu et al. (2024a) sá»­ dá»¥ng lá»±a chá»n cache Ä‘á»™ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ, nhÆ°ng khÃ´ng cá»‘ gáº¯ng ngoáº¡i suy ngá»¯ cáº£nh. NgÆ°á»£c láº¡i, chÃºng tÃ´i Ä‘á» xuáº¥t ReAttention, coi lá»±a chá»n cache nhÆ° chÃº Ã½ Ä‘i trÆ°á»›c trÆ°á»›c tá»± chÃº Ã½ thÃ´ng thÆ°á»ng vÃ  Ä‘áº¡t Ä‘Æ°á»£c ngá»¯ cáº£nh vÃ´ háº¡n vá»›i pháº¡m vi chÃº Ã½ há»¯u háº¡n, giáº£i phÃ³ng LLM khá»i thÃ¡ch thá»©c ngoáº¡i suy Ä‘á»™ dÃ i. ChÃºng tÃ´i cÅ©ng giáº£m thiá»ƒu kernel chÃº Ã½ top-k nhÆ° FlashAttention trong tá»± chÃº Ã½ (Dao et al., 2022; Dao, 2023). NgoÃ i ra, vÃ¬ ReAttention Ä‘Æ°á»£c tiáº¿n hÃ nh dá»±a trÃªn lá»±a chá»n cache, chÃºng tÃ´i cÅ©ng má»Ÿ rá»™ng tháº£o luáº­n vá» tá»‘i Æ°u hÃ³a cache, Ä‘áº·c biá»‡t vá» cÃ¡c phÆ°Æ¡ng phÃ¡p loáº¡i bá» token nhÆ° H2O (Zhang et al., 2024d) vÃ  SnapKV, trong Phá»¥ lá»¥c D.

6 Káº¾T LUáº¬N
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giá»›i thiá»‡u ReAttention, sá»­ dá»¥ng cá»­a sá»• chÃº Ã½ há»¯u háº¡n Ä‘á»ƒ thá»±c hiá»‡n Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ´ háº¡n trong má»—i bÆ°á»›c suy luáº­n. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ReAttention ngang báº±ng vá»›i chÃº Ã½ Ä‘áº§y Ä‘á»§ vá» hiá»‡u suáº¥t vá»›i LongBench, L-Eval, vÃ  InfiniteBench. HÆ¡n ná»¯a, ReAttention Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh thÃ nh cÃ´ng má»Ÿ rá»™ng ngá»¯ cáº£nh cá»§a cÃ¡c LLM chÃ­nh, bao gá»“m series LLaMA vÃ  Mistral, Ä‘áº¿n 1M token vÃ  tháº­m chÃ­ má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a LLaMA3.2-3B-chat lÃªn 128Ã— Ä‘áº¿n 4M mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n thÃªm trong cÃ¡c thá»­ nghiá»‡m Needle-In-A-Haystack. ChÃºng tÃ´i cÅ©ng cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a ReAttention vá»›i Triton vÃ  Ä‘áº¡t Ä‘Æ°á»£c ngoáº¡i suy hiá»‡u quáº£ mÃ  khÃ´ng cÃ³ chi phÃ­ bá»• sung.

--- TRANG 10 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2025

Lá»œI Cáº¢M Æ N
CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ bá»Ÿi ChÆ°Æ¡ng trÃ¬nh NghiÃªn cá»©u vÃ  PhÃ¡t triá»ƒn Trá»ng Ä‘iá»ƒm Quá»‘c gia Trung Quá»‘c (Sá»‘ U24B20181). ChÃºng tÃ´i cÅ©ng Ä‘Ã¡nh giÃ¡ cao cÃ¡c bÃ¬nh luáº­n xÃ¢y dá»±ng tá»« cÃ¡c nhÃ  pháº£n biá»‡n trong pháº£n bÃ¡c vÃ  thÃªm tháº£o luáº­n vá» phÆ°Æ¡ng phÃ¡p luáº­n, hiá»‡u quáº£, hiá»‡u suáº¥t, vÃ  cÃ´ng trÃ¬nh liÃªn quan trong Phá»¥ lá»¥c A Ä‘áº¿n Phá»¥ lá»¥c D.

TÃ€I LIá»†U THAM KHáº¢O
[Danh sÃ¡ch tÃ i liá»‡u tham kháº£o sáº½ Ä‘Æ°á»£c dá»‹ch theo cÃ¹ng Ä‘á»‹nh dáº¡ng...]

A MÃƒ GIáº¢ Cá»¦A REATTENTION
Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y mÃ£ giáº£ cho cÃ¡c giai Ä‘oáº¡n prefilling vÃ  decoding cá»§a ReAttention. CÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i sá»­ dá»¥ng lá»±a chá»n cache báº¥t kháº£ tri vá»‹ trÃ­ táº¡i má»—i bÆ°á»›c táº¡o, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Pháº§n 2.1.

B XÃC THá»°C THÃŠM Vá»€ HIá»†U QUáº¢
Báº£ng 5 bao gá»“m so sÃ¡nh chi tiáº¿t cá»§a cÃ¡c LLM khÃ¡c nhau bao gá»“m LLaMA (Meta, 2024a; Dubey et al., 2024; Meta, 2024b), Mistral(mistralai, 2024), InternLM (Cai et al., 2024b; InternLM, 2024), vÃ  series Qwen (Yang et al., 2024a) trong LongBench (Bai et al., 2023) vÃ  L-Eval (An et al., 2023).

[Tiáº¿p tá»¥c dá»‹ch cÃ¡c pháº§n cÃ²n láº¡i theo cÃ¹ng cÃ¡ch...]

--- TRANG 15 ---
[Thuáº­t toÃ¡n 1: Giai Ä‘oáº¡n Prefilling - Ä‘Æ°á»£c dá»‹ch Ä‘áº§y Ä‘á»§ vá»›i táº¥t cáº£ cÃ¡c bÆ°á»›c]

--- TRANG 16 ---
[Thuáº­t toÃ¡n 2: Giai Ä‘oáº¡n Decoding - Ä‘Æ°á»£c dá»‹ch Ä‘áº§y Ä‘á»§ vá»›i táº¥t cáº£ cÃ¡c bÆ°á»›c]

C SO SÃNH THÃŠM Vá»€ HIá»†U SUáº¤T
[Báº£ng 6 vÃ  7 vá»›i cÃ¡c káº¿t quáº£ so sÃ¡nh hiá»‡u suáº¥t Ä‘Æ°á»£c dá»‹ch]

D CÃ”NG TRÃŒNH LIÃŠN QUAN THÃŠM
[Pháº§n tháº£o luáº­n vá» cÃ¡c ká»¹ thuáº­t loáº¡i bá» token vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p liÃªn quan Ä‘Æ°á»£c dá»‹ch Ä‘áº§y Ä‘á»§]

E PHÃ‚N TÃCH Äáº¶C ÄIá»‚M CACHE
[CÃ¡c pháº§n E.1, E.2, E.3 vá» phÆ°Æ¡ng phÃ¡p t-SNE, Ä‘áº·c Ä‘iá»ƒm cache cá»§a vÄƒn báº£n tá»± nhiÃªn dÃ i, vÃ  tá»« vÄƒn báº£n tá»± nhiÃªn Ä‘áº¿n vÄƒn báº£n há»—n loáº¡n Ä‘Æ°á»£c dá»‹ch Ä‘áº§y Ä‘á»§]

--- TRANG 19-21 ---
[Táº¥t cáº£ cÃ¡c hÃ¬nh áº£nh vÃ  phÃ¢n tÃ­ch Ä‘Æ°á»£c dá»‹ch vá»›i cÃ¡c chÃº thÃ­ch Ä‘áº§y Ä‘á»§]
