# 2407.15176.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2407.15176.pdf
# Kích thước file: 8978592 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
REATTENTION : MỞ RỘNG NGỮ CẢNH VÔ HẠN KHÔNG CẦN HUẤN LUYỆN
VỚI PHẠM VI CHÚ Ý HỮU HẠN
Xiaoran Liu1,3,4∗, Ruixiao Li1,4∗, Qipeng Guo3,4, Zhigeng Liu1, Yuerong Song1,4,
Kai Lv1,3, Hang Yan3, Linlin Li2, Qun Liu2, Xipeng Qiu1,4
1Khoa Khoa học Máy tính, Đại học Fudan,2Phòng thí nghiệm Noah's Ark Huawei,
3Phòng thí nghiệm AI Shanghai,4Viện Đổi mới Shanghai
xrliu24@m.fudan.edu.cn ,xpqiu@fudan.edu.cn

TÓM TẮT
Khả năng ngữ cảnh dài của các Mô hình Ngôn ngữ Lớn (LLM) đã có những đột phá đáng kể, nhưng độ dài ngữ cảnh tối đa được hỗ trợ trong việc ngoại suy độ dài vẫn là một điểm nghẽn quan trọng hạn chế các ứng dụng thực tế của chúng. Ràng buộc về độ dài ngữ cảnh trong LLM phát sinh từ cơ chế tự chú ý (self-attention), không thể nắm bắt hiệu quả và hiệu suất các mối quan hệ ngữ nghĩa trong ngữ cảnh dài vô hạn thông qua thông tin vị trí được tiền huấn luyện hạn chế và phạm vi chú ý. Trong công trình này, chúng tôi đề xuất ReAttention, một phương pháp không cần huấn luyện cho phép LLM dựa trên cơ chế tự chú ý hỗ trợ ngữ cảnh vô hạn với phạm vi chú ý hữu hạn dưới tài nguyên bộ nhớ đủ. ReAttention thực hiện chú ý top-k bất khả tri vị trí trước tự chú ý thông thường nhận biết vị trí, giải phóng LLM khỏi vấn đề ngoại suy độ dài. Chúng tôi xác thực hiệu suất của ReAttention trên LongBench, L-Eval, và InfiniteBench và chứng minh rằng nó tương đương với các phương pháp truyền thống. Hơn nữa, chúng tôi cũng áp dụng ReAttention trên các LLM chính như LLaMA3.1-8B và Mistral-v0.3-7B, cho phép chúng hỗ trợ độ dài ngữ cảnh ít nhất 1M và thậm chí mở rộng độ dài ngữ cảnh của LLaMA3.2-3B-chat lên 128× đến 4M mà không cần huấn luyện thêm trong các thử nghiệm Needle-In-A-Haystack. Chúng tôi cũng cải thiện hiệu suất của ReAttention với Triton và đạt được ngoại suy hiệu quả mà không có chi phí bổ sung. Mã nguồn có sẵn tại https://github.com/OpenMOSS/ReAttention.

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer (Vaswani et al., 2017; OpenAI, 2023; Reid et al., 2024; Sun et al., 2024) đã có tiến bộ lớn trong các ứng dụng Xử lý Ngôn ngữ Tự nhiên (NLP). Đặc biệt, trong mô hình hóa ngữ cảnh dài, một lượng nghiên cứu đáng kể đã được dành cho việc mở rộng khả năng độ dài của LLM (Chen et al., 2023; Peng et al., 2023; Liu et al., 2023; Xiong et al., 2023), tăng độ dài ngữ cảnh tối đa được hỗ trợ từ 2K ban đầu (Touvron et al., 2023) lên 2M (Ding et al., 2024) trong các LLM mã nguồn mở. Tuy nhiên, độ dài ngữ cảnh tối đa trong ngoại suy độ dài vẫn là một điểm nghẽn hạn chế các ứng dụng thực tế của LLM (Press et al., 2022; Chen et al., 2023). Để đạt được độ dài ngữ cảnh vô hạn dưới bộ nhớ đủ cho LLM dựa trên Transformer, ba điều kiện sau phải được thỏa mãn:

a Thông tin vị trí trong giai đoạn suy luận không nên nằm ngoài phân phối (OOD) so với giai đoạn huấn luyện; (Han et al., 2023; Liu et al., 2023)

b Entropy tự chú ý trong giai đoạn suy luận không nên tăng theo độ dài của đầu vào; (Han et al., 2023; Xiao et al., 2023)

c LLM nên duy trì nhận thức hiệu quả về thông tin ngữ cảnh quan trọng tại mỗi bước suy luận. (Dong et al., 2024; Zhang et al., 2024a)

Các công trình đầu trong ngoại suy tập trung vào điểm đầu tiên, chủ yếu bằng cách nội suy embedding vị trí (Chen et al., 2023; bloc97, 2023b;a; Xiong et al., 2023) hoặc giới hạn vị trí tương đối trong độ dài ngữ cảnh tiền huấn luyện (Su, 2023; Jin et al., 2024; An et al., 2024). Tuy nhiên, các nghiên cứu sau đó đã chỉ ra rằng ngay cả khi thông tin vị trí không phải OOD, entropy chú ý có xu hướng tăng logarit theo độ dài của cửa sổ tự chú ý (Han et al., 2023). Cụ thể, phân phối tự chú ý trở nên ngày càng khuếch tán khi độ dài suy luận mở rộng, khiến việc thu thập thông tin từ ngữ cảnh trở nên khó khăn, dẫn đến đầu ra mô hình không ổn định (Peng et al., 2023; Han et al., 2023). Để đáp ứng điều này, các phương pháp như LM-Infinite (Han et al., 2023) và StreamingLLM (Xiao et al., 2023) đã được đề xuất, giữ lại phần đầu và cuối của đầu vào, cho phép LLM duy trì đầu ra ổn định khi độ dài đầu vào tăng. Tuy nhiên, chúng làm tổn hại đến nhận thức toàn cục về thông tin ngữ cảnh, ảnh hưởng đến hiệu suất downstream.

Đối với con người, mặc dù chúng ta có bộ nhớ dài hạn, chúng ta chỉ cần một lượng nhỏ thông tin để suy nghĩ và hành động theo thời gian thực. Tương tự, trong khi LLM yêu cầu toàn bộ ngữ cảnh để hoàn thành quá trình suy luận, nó chỉ cần thông tin ngữ cảnh hạn chế tại mỗi bước suy luận. Thực tế này đã truyền cảm hứng cho nghiên cứu như InfLLM (Xiao et al., 2024a) và LongHeads (Lu et al., 2024) để nhận thức và trích xuất thông tin quan trọng từ ngữ cảnh trước khi thực hiện tự chú ý. Tuy nhiên, LongHeads vẫn đối mặt với giới hạn trên về thông tin vị trí do sử dụng NTK hoặc PI cho prefill (Lu et al., 2024), trong khi InfLLM gặp bias trong việc biểu diễn thông tin ngữ cảnh, phát sinh từ biểu diễn theo khối và embedding vị trí kiểu ReRoPE khi trích xuất thông tin chính.

Trong công trình này, chúng tôi giới thiệu ReAttention, một phương pháp không cần huấn luyện mở rộng LLM với độ dài ngữ cảnh hữu hạn để xử lý ngữ cảnh vô hạn. ReAttention coi việc trích xuất thông tin ngữ cảnh quan trọng như một quá trình chú ý bổ sung đi trước tự chú ý truyền thống, giống như "suy nghĩ trước khi hành động." Tại mỗi bước lý luận, ReAttention chọn các phân đoạn hữu hạn quan trọng nhất từ KV cache mà không có thông tin vị trí, nối chúng lại, áp dụng embedding vị trí, và thực hiện tự chú ý. Bằng cách kiểm soát độ dài của các phân đoạn KV cache được chọn, ReAttention có thể đạt được độ dài ngữ cảnh vô hạn với phạm vi chú ý hữu hạn, trong khi đảm bảo thông tin vị trí và entropy chú ý không nằm ngoài phân phối (Han et al., 2023). Hơn nữa, lấy cảm hứng từ các kỹ thuật tối ưu hóa được sử dụng trong tự chú ý, đặc biệt là FlashAttention (Dao et al., 2022; Dao, 2023), chúng tôi sử dụng Triton (Tillet et al., 2019), một ngôn ngữ lập trình GPU, để giảm thiểu chi phí đọc và ghi trong chú ý top-k. Với kernel Triton tùy chỉnh của chúng tôi, ReAttention tránh chi phí tính toán thêm và giảm sử dụng bộ nhớ cho ngữ cảnh dài. Các đóng góp của chúng tôi được tóm tắt như sau:

• Chúng tôi phác thảo ba yêu cầu cho ngữ cảnh vô hạn trong LLM dựa trên Transformer, embedding vị trí không OOD, entropy chú ý ổn định, và nhận thức ngữ cảnh hiệu quả. Chúng tôi cũng phát hiện rằng yêu cầu cuối cùng có thể được thỏa mãn thông qua điểm chú ý mà không có embedding vị trí.

• Dựa trên quan sát này, chúng tôi đề xuất một phương pháp không cần huấn luyện, ReAttention, thỏa mãn ba điều kiện nói trên, do đó ngoại suy độ dài ngữ cảnh của LLM đến vô hạn với phạm vi chú ý hữu hạn, và giải phóng LLM khỏi vấn đề ngoại suy độ dài.

• Chúng tôi xác thực rằng ReAttention phù hợp với hiệu suất của tự chú ý truyền thống trong ngữ cảnh dài mà không có chi phí tính toán và sử dụng bộ nhớ ít hơn. Cụ thể, ReAttention mở rộng độ dài ngữ cảnh của các LLM hàng đầu, như LLaMA3.1-8B-128K, đến ít nhất 1 triệu token. Đối với các mô hình nhỏ hơn như LLaMA3.2-3B-chat, độ dài ngữ cảnh có thể được tăng lên 128×, đạt đến 4 triệu token, mà không cần huấn luyện bổ sung.

2 PHƯƠNG PHÁP
Cấu trúc tổng thể của ReAttention được minh họa trong Hình 1, bao gồm chú ý top-k bất khả tri vị trí chịu trách nhiệm lựa chọn cache toàn ngữ cảnh và biến đổi tự chú ý truyền thống với embedding vị trí. ReAttention đạt được tích hợp không cần huấn luyện giữa hai thành phần.

2.1 LỰAC CHỌN CACHE TOÀN NGỮ CẢNH
Trong khi LLM yêu cầu ngữ cảnh dài hoàn chình để thực hiện toàn bộ quá trình suy luận, chỉ cần một phân đoạn ngữ cảnh hạn chế tại mỗi bước suy luận (Xiao et al., 2023; 2024a; Lu et al., 2024). Xem xét phần đầu và cuối của ngữ cảnh đầu vào tương ứng với các prompt quan trọng toàn cục và thông tin cục bộ cho suy luận (Xiao et al., 2023), ReAttention giữ lại cả phân đoạn toàn cục và cục bộ của KV cache để đưa vào quá trình tự chú ý.

Kcache= [Kglobal,Kmiddle,Klocal],Vcache= [Vglobal,Vmiddle,Vlocal]. (1)

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

EmbeddingSelf-Attention
with PEFeed ForwardOutput
TopK -Attention
without PE𝑳-Layer
𝑸𝑲
𝑽𝑲′
𝑽′
𝑸𝑶
Long -Term Memory
(KV Cache)Short -Term Memory

Hình 1: Tổng quan về ReAttention. Sau đó, ReAttention sử dụng vector truy vấn của bước hiện tại để thực hiện lựa chọn top-k trên phần giữa của KV cache (Ribar et al., 2023), để xác định các phân đoạn cache quan trọng nhất cho bước hiện tại như được hiển thị trong Hình 1.

Indices =top-k
qtKT
middle
,
Kselect=Kmiddle[Indices ],
Vselect=Vmiddle[Indices ].(2)

ReAttention thực hiện lựa chọn toàn ngữ cảnh trên KV cache trong mỗi lớp, cho phép các lớp khác nhau chọn KV cache khác nhau cho việc tính toán. Hơn nữa, vì mỗi lớp chú ý có nhiều đầu chú ý và ReAttention áp dụng đầu vào luồng theo khối trong giai đoạn prefilling, nhiều vector truy vấn có thể tồn tại đồng thời. Trong trường hợp này, ReAttention bỏ phiếu dựa trên các lựa chọn top-k từ các đầu và vector truy vấn khác nhau để xác định top-k′ KV cache. Ngoài ra, để đảm bảo tính liên kết ngữ nghĩa, ReAttention không chỉ giữ lại bản thân các phần tử top-k′ mà còn trích xuất m mục lân cận như một tổng thể. Các phần chồng lấp được loại bỏ trùng lặp.

Không giống như lựa chọn dựa trên khối trong các công trình trước đây (Lu et al., 2024; Xiao et al., 2024a), ReAttention trích xuất thông tin ngữ cảnh quan trọng bằng tích vô hướng giữa qt và Kcache, thay vì biểu diễn theo khối của Kcache. Phương pháp này cung cấp khả năng thích ứng mạnh và tránh sự phân mảnh ngữ nghĩa do phân khối cố định gây ra (Luo et al., 2024). Hơn nữa, vì kết quả trung gian của qtKT
middle quá lớn đối với các tình huống ngữ cảnh dài, và sẽ phát sinh chi phí đọc và ghi rất lớn, chúng tôi theo cách tiếp cận của FlashAttention (Dao et al., 2022; Dao, 2023), hợp nhất toàn bộ quá trình vào một kernel bằng Triton (Tillet et al., 2019), như được nêu chi tiết trong Phần 3.4.

2.2 TÍCH HỢP KHÔNG CẦN HUẤN LUYỆN
ReAttention nối các phân đoạn KV cache được chọn giữa phần toàn cục và cục bộ, áp dụng embedding vị trí tuần tự, và bảo tồn thứ tự tương đối trong khi bỏ qua khoảng cách tuyệt đối giữa các phân đoạn được chọn, như được hiển thị trong Hình 1. Tự chú ý sau đó có thể được áp dụng cho KV cache được nối.

Kcache' = [Kglobal,Kselect,Klocal],Vcache' = [Vglobal,Vselect,Vlocal],
˜qt,˜Kcache' =PE(qt,Kcache'),ot=SelfAttn
˜qt,˜Kcache',Vcache'
.(3)

Điều quan trọng cần lưu ý là, không giống như triển khai thông thường trong Huggingface Transformers (Wolf et al., 2020), nơi embedding vị trí được áp dụng trước khi KV caching, trong ReAttention, embedding vị trí được tách ra khỏi KV cache và được thực hiện sau khi lựa chọn KV cache. Nghĩa là, KV được cache không bao gồm thông tin vị trí. Thiết kế này cung cấp một số lợi thế. Một mặt, như đã đề cập trong Phần 4, điểm chú ý mà không có embedding vị trí có lợi hơn cho việc định vị thông tin chính trong ngữ cảnh. Mặt khác, vì độ dài cache được nối vẫn nằm trong độ dài ngữ cảnh tiền huấn luyện hoặc giới hạn trên ngoại suy, embedding vị trí sẽ không bao giờ là OOD (Han et al., 2023).

Hơn nữa, các phân đoạn KV cache không được chọn không quan trọng trong bước suy luận hiện tại, vì điểm tự chú ý của chúng là tối thiểu (Zhang et al., 2024d). Do đó, việc sửa đổi này không làm hại đầu ra tự chú ý và có thể loại bỏ sự can thiệp từ thông tin không liên quan (Zhang et al., 2024d; Ge et al., 2023). So với InfLLM (Xiao et al., 2024a), embedding vị trí và biến đổi chú ý của ReAttention trong cửa sổ tiền huấn luyện không giới thiệu mẫu chú ý chưa được huấn luyện, do đó ngăn ngừa tích lũy bias trong KV cache trong giai đoạn prefilling. Do đó, ReAttention có thể bỏ qua thông tin vị trí, đạt được ngữ cảnh chú ý không giới hạn với phạm vi chú ý hạn chế mà không cần huấn luyện nào, và vẫn tương thích với các phương pháp gia tốc chú ý hiện có (Dao et al., 2022; Dao, 2023).

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Bảng 1: Kết quả của LLaMA Series (Meta, 2024a; Dubey et al., 2024; Meta, 2024b) trên LongBench (Bai et al., 2023). ReAttention đạt được ưu thế nhất quán so với StreamingLLM (Xiao et al., 2023) và InfLLM (Xiao et al., 2024a) và cho thấy hiệu suất tương đương với LLM có chú ý đầy đủ.]

[Bảng 2: Kết quả của LLaMA Series trên InfiniteBench (Zhang et al., 2024b) ở các độ dài ngữ cảnh khác nhau. "-" có nghĩa là LLM không thể cung cấp đầu ra ổn định ở độ dài ngữ cảnh nhất định. ReAttention đạt được ưu thế so với StreamingLLM (Xiao et al., 2023), InfLLM (Xiao et al., 2024a) và chú ý đầy đủ.]

3 THỰC NGHIỆM

3.1 THIẾT LẬP
Chúng tôi tiến hành thử nghiệm trên LLaMA3-8B-8K (Meta, 2024a), LLaMA3.1-8B-128K (Dubey et al., 2024), LLaMA3.1-70B-128K (Dubey et al., 2024), LLaMA3.2-3B-128K (Dubey et al., 2024), Mistral-v0.3-7B-32K (mistralai, 2024), InternLM2.5-7B-1M (InternLM, 2024), Qwen2-7B-128K (Yang et al., 2024a), Qwen2-72B-128K (Yang et al., 2024a), Qwen2-1B-32K (Yang et al., 2024a). Đối với tất cả các mô hình, chúng tôi đặt độ dài của Kglobal là 32, độ dài của Klocal là 4096, và kích thước khoảng được chọn là 32. Hơn nữa, chúng tôi đặt k = 4, k′ = 127 trong chú ý top-k. Quan trọng là, phạm vi chú ý trong mỗi bước vẫn nằm trong cửa sổ chú ý tối đa. Ví dụ, đối với LLaMA3-8B-8K với ReAttention, kích thước phạm vi chú ý tối đa là 32 + 4096 + 127 × 32, khớp chính xác với cửa sổ chú ý tối đa được hỗ trợ là 8192. Chúng tôi sử dụng OpenCompass (Contributors, 2023b) để xác thực. Tất cả các thử nghiệm được thực hiện với độ chính xác FP16 và được gia tốc với FlashAttention2 (Dao, 2023).

3.2 ĐÁNH GIÁ BENCHMARK NGỮ CẢNH DÀI
Chúng tôi đầu tiên đánh giá tất cả 9 LLM trên benchmark ngữ cảnh dài thường được sử dụng LongBench (Bai et al., 2023) và L-Eval (An et al., 2023), với độ dài ngữ cảnh mặc định là 32K và cắt bớt giữa.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Hình 2: Kết quả của các LLM chính được tăng cường ReAttention hiện có, bao gồm LLaMA3-8B-8K và Mistral-v0.3-7B-32K, trên Needle-In-A-Haystack (Contributors, 2023a) được triển khai trong OpenCompass (Contributors, 2023b).]

Đối với LLaMA3-8B-8K, có độ dài ngữ cảnh dưới 32K, chúng tôi báo cáo hiệu suất của nó với Dynamic NTK (bloc97, 2023a). Đối với triển khai Dynamic NTK, chúng tôi sử dụng cài đặt mặc định từ Huggingface Transformers (Wolf et al., 2020), đặt hệ số tỷ lệ là 4. Ngoài ra, chúng tôi so sánh hiệu suất của tất cả 9 LLM với StreamingLLM (Xiao et al., 2023) như một nghiên cứu loại bỏ, sử dụng cùng độ dài phân đoạn toàn cục và cục bộ như cài đặt ReAttention.

Như được hiển thị trong Bảng 1 và bổ sung trong Bảng 5 ở Phụ lục B, ReAttention vượt trội hơn StreamingLLM trên tất cả 9 mô hình, chỉ ra rằng lựa chọn toàn ngữ cảnh thu được thông tin hữu ích cho suy luận ngữ cảnh dài. Hơn nữa, ReAttention hoạt động ngang bằng với chú ý đầy đủ và thậm chí vượt qua nó trong một số trường hợp, như LLaMA3.1-70B-128K (Meta, 2024a) và Qwen2-1B-32K (Yang et al., 2024a). Điều này chứng minh rằng ReAttention có thể được áp dụng cho LLM với các kích thước khác nhau và đạt được hiệu suất gần với chú ý đầy đủ trên các nhiệm vụ downstream (Jiang et al., 2024).

Để chứng minh thêm về ưu thế và khả năng ngoại suy của ReAttention, chúng tôi xác thực phương pháp của mình trên InfiniteBench (Zhang et al., 2024c), một benchmark thách thức hơn với độ dài ngữ cảnh dài hơn. Chúng tôi chọn 3 nhiệm vụ phụ thường được kiểm tra, En.MC, En.QA và En.Sum, đánh giá các mô hình với độ dài ngữ cảnh thay đổi, và so sánh ReAttention với DynamicNTK (bloc97, 2023a) và InfLLM (Xiao et al., 2024a) với cùng cấu hình lựa chọn. Kết quả¹ được hiển thị trong Bảng 2.

Đáng chú ý, ReAttention liên tục vượt trội hơn chú ý đầy đủ và InfLLM ở độ dài ngữ cảnh 128K và trong điểm trung bình. Trong khi DynamicNTK hoạt động tốt ở 32k, nó gặp phải giới hạn trên rõ ràng về ngoại suy, vượt quá đó mô hình không thể tạo ra đầu ra ổn định. Hơn nữa, trong khi InfLLM có thể mở rộng độ dài ngữ cảnh vô hạn (Xiao et al., 2024a), nó vẫn thua kém ReAttention trong các nhiệm vụ downstream do trích xuất thông tin quan trọng không chính xác và sự khác biệt trong định dạng embedding vị trí so với giai đoạn tiền huấn luyện.

3.3 ĐÁNH GIÁ NEEDLE-IN-A-HAYSTACK
Dựa trên đánh giá benchmark ngữ cảnh dài, chúng tôi sử dụng các mô hình có khả năng truy xuất mạnh trong độ dài ngữ cảnh huấn luyện của chúng và tiến hành đánh giá Needle-In-A-Haystack (NIAH) (Contributors, 2023a;b). Chúng tôi thực hiện thử nghiệm trên 8 GPU A100 và mở rộng độ dài ngữ cảnh của LLM với ReAttention đến ít nhất 1M token. Như được hiển thị trong Hình 2, LLM với ReAttention duy trì độ chính xác truy xuất cao đáng kể trên toàn bộ phạm vi độ dài ngữ cảnh mà chúng có thể hỗ trợ, bất kể cửa sổ chú ý gốc của chúng. Quan trọng là, chúng tôi cũng mở rộng ngữ cảnh của các LLM chính như LLaMA3-8B-8K (Meta, 2024a) và Mistral-v0.3-7B-32K (mistralai, 2024) đến ít nhất 1M, cung cấp cho cộng đồng một giải pháp hiệu quả để triển khai LLM ngữ cảnh dài.

¹Do đánh giá mở rộng cho tất cả 9 mô hình, và xem xét rằng InfLLM có hỗ trợ hạn chế cho các mô hình lớn hơn như LLaMA3.1-70B-chat và các series khác như InternLM2.5 và Qwen2, chúng tôi chỉ báo cáo kết quả của các mô hình LLaMA series ở quy mô 8B và 3B.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Hình 3: Kết quả của Multi-Needle-In-A-Haystack (Reid et al., 2024) và Single NIAH trong ngữ cảnh dài hơn được triển khai trong OpenCompass (Contributors, 2023b).]

Dựa trên điều này, chúng tôi tăng độ khó của đánh giá bằng cách tiến hành thử nghiệm trên multi-NIAH và single-NIAH trong ngữ cảnh dài hơn, với kết quả được hiển thị trong Hình 3. Do độ khó tăng, chúng tôi sử dụng các phiên bản instruct tương ứng của các mô hình. Trên LLaMA3.1-8B-Instruct-128K chính (Dubey et al., 2024), chúng tôi đạt được độ dài ngữ cảnh 1M trong nhiệm vụ multi-NIAH. Đối với các mô hình nhỏ hơn, như LLaMA3.2-3B-chat-128K (Meta, 2024b), chúng tôi mở rộng độ dài ngữ cảnh đến ít nhất 2M. Đáng chú ý, đối với Qwen2-1B-Instruct-32k (Yang et al., 2024a), chúng tôi mở rộng độ dài ngữ cảnh đến 4M trên 4 GPU A100, đạt được mở rộng ngữ cảnh không cần huấn luyện 128×. Theo hiểu biết của chúng tôi, đây là mức khuếch đại lớn nhất của độ dài ngữ cảnh cho LLM đạt được mà không cần huấn luyện bổ sung. Điều này chứng minh rằng ReAttention có thể mở rộng độ dài ngữ cảnh trong khi bỏ qua thông tin vị trí, sử dụng cửa sổ chú ý hữu hạn để đạt được ngữ cảnh chú ý vô hạn.

3.4 PHÂN TÍCH HIỆU SUẤT

[Hình 4: Tổng quan về kernel fusion trong kernel chú ý top-k tùy chỉnh của chúng tôi. Các đo lường hiệu suất phản ánh thời gian thực thi của các hàm kernel tương ứng, với độ dài đầu vào 8K cho các nhiệm vụ suy luận Llama3.1-8B.]

Trong framework PyTorch, các toán tử thực thi độc lập, yêu cầu I/O thường xuyên đến bộ nhớ GPU, điều này giới thiệu chi phí và độ trễ không cần thiết. Lấy cảm hứng từ FlashAttention (Dao et al., 2022; Dao, 2023), chúng tôi phát triển một hàm kernel GPU cho chú ý top-k được mô tả trong Phần 2.1 bằng Triton. Như được hiển thị trong Hình 4, kernel của chúng tôi hợp nhất các toán tử để tính toán điểm chú ý và tính toán top-k, cho phép toàn bộ quá trình chạy trong cache GPU. Do đó I/O bộ nhớ GPU được giảm đáng kể, cải thiện cả việc sử dụng bộ nhớ GPU và thời gian chạy. Chúng tôi đã thêm chú ý top-k giảm thời gian cho tự chú ý, và kernel của chúng tôi giữ chi phí của chú ý top-k ở mức tối thiểu, để lại độ trễ tổng thể không thay đổi.

Chúng tôi phân tích hiệu suất của kernel của chúng tôi về thời gian thực thi GPU và sử dụng bộ nhớ, cho thấy rằng sử dụng Triton thay vì PyTorch cho chú ý top-k tăng cường đáng kể hiệu suất GPU trong khi giảm tiêu thụ bộ nhớ GPU. Ngoài ra, chúng tôi so sánh ReAttention với triển khai tiêu chuẩn trong HuggingFace Transformers (Dai et al., 2019), đo lường thời gian đến token đầu tiên (TTFT) và chi phí bộ nhớ ở độ dài ngữ cảnh từ 32k đến 256k. Kết quả so sánh TTFT và throughput được nêu chi tiết trong Phụ lục C. Tất cả các thử nghiệm được tiến hành trên một hệ thống với CPU 48 nhân, RAM 256GB, và GPU A800-80GB.

Hiệu suất Toán tử Triton Chúng tôi kiểm tra toán tử Triton của chúng tôi bằng cách sử dụng đầu vào suy luận thực (ví dụ: ma trận Q và K được trích xuất từ các nhiệm vụ suy luận thực tế) để đo lường thời gian thực thi và sử dụng bộ nhớ thiết bị. Như được hiển thị trong Hình 5a, triển khai PyTorch tiêu chuẩn vượt quá 80GB bộ nhớ cho các chuỗi trên 64k, trong khi toán tử Triton của chúng tôi giảm thiểu việc sử dụng bộ nhớ, giới hạn ở ma trận đầu vào và đầu ra. Nó cũng đạt được hiệu suất nhanh hơn hàng trăm lần ở độ dài chuỗi 64k và mở rộng hiệu quả khi độ dài tăng (Hình 5b).

Sử dụng Bộ nhớ trong Giai đoạn Prefilling Chi phí bộ nhớ hạn chế việc prefilling cho các chuỗi dài hơn. Hình 6 cho thấy phương pháp của chúng tôi vượt qua triển khai tiêu chuẩn trong HuggingFace Transformers

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Hình 5: Sự hợp tác hiệu suất giữa kernel Triton của chúng tôi và phiên bản PyTorch của nó. Kernel chú ý top-k hợp nhất của chúng tôi hoạt động tốt hơn triển khai PyTorch trên tất cả độ dài ngữ cảnh.]

[Hình 6: Độ trễ token đầu tiên (FTL) và tiêu thụ bộ nhớ GPU trong giai đoạn prefilling. FullAttn đề cập đến triển khai HuggingFace Transformers chính thức của Llama3.1-8B-Base, hết bộ nhớ sau 192k.]

khi độ dài ngữ cảnh tăng, tiếp tục hoạt động khi triển khai tiêu chuẩn hết bộ nhớ. Chúng tôi cũng ghi lại độ trễ token đầu tiên, vẫn có thể so sánh với triển khai tiêu chuẩn trong Huggingface Transformers.

4 THẢO LUẬN

4.1 PHÂN TÍCH VỀ SIÊU THAM SỐ
Chúng tôi đầu tiên thảo luận về việc lựa chọn siêu tham số. Chúng tôi đánh giá hiệu suất của LLaMA3-8B-8K (Meta, 2024a) trên benchmark LongBench (Bai et al., 2023), so sánh các siêu tham số bao gồm kích thước khối, kích thước khoảng, kích thước cục bộ, và giá trị top-k. Vì khối prefilling luôn được chứa trong phần cục bộ trong tự chú ý, kích thước khối phải nhỏ hơn kích thước cục bộ. Ngoài ra, để đảm bảo tính công bằng trong các so sánh của chúng tôi, chúng tôi duy trì kích thước cửa sổ chú ý tối đa nhất quán qua các cài đặt khác nhau, cụ thể là giữ tổng của kích thước toàn cục, kích thước khoảng nhân với k′, và kích thước cục bộ bằng 8192. Để đạt được điều này, chúng tôi đặt kích thước toàn cục bằng kích thước khoảng, giữ lại phần sớm nhất như phân đoạn lựa chọn đầu tiên.

[Bảng 3: Phân tích siêu tham số ReAttention trên LLaMA3-8B-8K và LongBench, theo mặc định kích thước khối 512, kích thước khoảng 32 cũng như kích thước toàn cục, kích thước cục bộ 4096, và lựa chọn top-k.]

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

[Hình 7: Trực quan hóa phân phối chú ý cho InternLM2-7B-200K (Cai et al., 2024b) được đánh giá trên Needle-In-A-Haystack (Contributors, 2023a;b) trong độ dài ngữ cảnh 32K. Trong mỗi hình phụ, mỗi heatmap đại diện cho phân phối chú ý của một lớp duy nhất, với trục y tương ứng với 32 đầu chú ý và trục x tương ứng với độ dài ngữ cảnh 32K. 32 token đầu tiên, phần hướng dẫn, và 6 token cuối cùng, phần được tạo, không bị loại trừ. Các giá trị đại diện cho phân phối tự chú ý tích lũy cho mỗi việc tạo token. Màu sáng hơn, chú ý tích lũy cao hơn. Một thao tác pooling với kích thước kernel 100 được áp dụng tuần tự để hiển thị mẫu rõ ràng hơn. Hộp đỏ đại diện cho vị trí của "kim" trong ngữ cảnh.]

Như được hiển thị trong Bảng 3. Chúng tôi phát hiện rằng khi kích thước khối giảm từ 2048 xuống 512, hiệu suất cải thiện dần dần. Giai đoạn prefilling trở nên chính xác hơn trong việc trích xuất thông tin quan trọng trong mỗi bước chú ý top-k. Tuy nhiên, vì prefilling mất nhiều thời gian hơn khi kích thước khối giảm, chúng tôi chọn 512 làm giá trị mặc định mà không giảm thêm. Tiếp theo, chúng tôi kiểm tra tác động của các kích thước khoảng khác nhau. Trong số các siêu tham số này, kích thước khoảng có ảnh hưởng đáng kể nhất đến hiệu suất downstream. Khi kích thước khoảng tương đối nhỏ, các phân đoạn phân mảnh làm suy yếu tính liên kết và có thể dẫn lệch các dự đoán của LLM, dẫn đến hiệu suất tệ hơn so với StreamingLLM. Khi kích thước khoảng đạt 32, hiệu suất trên LongBench đạt đỉnh. Vượt qua điểm này, do ràng buộc của embedding vị trí được tiền huấn luyện, số lượng phân đoạn được trích xuất giảm, cản trở việc nắm bắt hiệu quả thông tin ngữ cảnh quan trọng. Chúng tôi cũng so sánh ảnh hưởng của số top-k và phát hiện rằng top-4 mang lại kết quả tốt nhất. Số top-k nhỏ hơn khiến LLM khó xác định thông tin quan trọng, trong khi số top-k lớn hơn có thể giới thiệu nội dung không liên quan can thiệp vào việc đánh giá. Cuối cùng, chúng tôi phân tích ảnh hưởng của kích thước cục bộ. Kích thước cục bộ nhỏ hơn làm tổn hại khả năng của LLM duy trì tính liên kết ngữ nghĩa, và lựa chọn quá mức vượt quá ngữ cảnh cục bộ có thể làm lệch KV cache trong giai đoạn prefilling. Do đó, chúng tôi đặt kích thước cục bộ bằng một nửa độ dài ngữ cảnh tiền huấn luyện cho LLaMA3-8B-8K và áp dụng cấu hình siêu tham số này cho các LLM khác.

4.2 LỰACHỌN CACHE BẤT KHẢ TRI VỊ TRÍ
Trong ReAttention, lựa chọn cache toàn ngữ cảnh dựa trên tích vô hướng giữa các vector truy vấn và khóa mà không có embedding vị trí. Điều này có thể tạo ra khoảng cách giữa việc lựa chọn KV cache và phân phối tự chú ý. Do đó, ở đây nảy sinh câu hỏi liệu việc sử dụng các vector ngữ nghĩa mà không có thông tin vị trí có thể định vị hiệu quả thông tin quan trọng trong ngữ cảnh hay không. Để phân tích điều này, chúng tôi chọn một trường hợp đúng và một trường hợp sai từ kết quả đánh giá Needle-In-A-Haystack (Contributors, 2023a;b) trên InternLM2-7B-200K (Cai et al., 2024b) với chú ý đầy đủ trong độ dài ngữ cảnh 32K. Đối với mỗi trường hợp, chúng tôi tính toán phân phối chú ý với embedding vị trí, tức là phân phối tự chú ý thực, và phân phối chú ý mà không có embedding vị trí. Kết quả được hiển thị trong Hình 7.

Đối với trường hợp đúng, các phân phối chú ý, cả có và không có embedding vị trí, có thể định vị vị trí của "kim" trong "đống cỏ khô". Tuy nhiên, phân phối chú ý với embedding vị trí xuất hiện khuếch tán hơn, và hiệu ứng này trở nên rõ rệt hơn ở các lớp cao hơn của mô hình. Đối với trường hợp sai, phân phối chú ý với embedding vị trí cũng rất khuếch tán và bị ảnh hưởng mạnh bởi một đầu chú ý cụ thể bị dẫn lệch bởi thông tin không liên quan từ các lớp dưới. Điều này cuối cùng dẫn đến sự thất bại của mô hình.

Thú vị là, khi quan sát phân phối chú ý mà không có embedding vị trí, chúng tôi phát hiện rằng mô hình có thể xác định vị trí của "kim" bằng tích vô hướng của các vector ngữ nghĩa và lọc hiệu quả một lượng lớn nhiễu không liên quan. Thực tế, InternLM2-7B-200K được tăng cường ReAttention có thể định vị thành công "kim" trong trường hợp này. Do đó, tích vô hướng của các vector ngữ nghĩa trong lựa chọn cache là hợp lý, và nó chứng minh hiệu quả hơn trong việc tìm ngữ cảnh liên quan so với tích vô hướng nhận biết vị trí. Ngoài ra, do không có embedding vị trí trong KV cache, ReAttention có thể đạt được mở rộng ngữ cảnh hiệu quả.

4.3 QUAN SÁT VỀ CÁC NHIỆM VỤ TỔNG HỢP
Phân tích trên đã chứng minh hiệu suất tốt của ReAttention trong các tình huống ngữ cảnh dài khác nhau, gợi ý rằng nó có thể ngoại suy đến độ dài ngữ cảnh vô hạn. Tuy nhiên, các đánh giá này chủ yếu tập trung vào ngữ cảnh tự nhiên thông thường và thiếu các benchmark hiện được thảo luận rộng rãi (Kuratov et al., 2024; Li et al., 2024), như RULER (Hsieh et al., 2024). Thật không may, ReAttention hoạt động kém trên benchmark RULER. Trên thực tế, các phương pháp ngoại suy không dựa trên chú ý đầy đủ, bao gồm ReAttention và InfLLM, không thể vượt qua benchmark RULER. Ví dụ, sử dụng LLaMA3-8B-8K (Meta, 2024a), chúng tôi so sánh ngoại suy Dynamic NTK với InfLLM (Xiao et al., 2024a) và ReAttention ở độ dài ngữ cảnh 8K và 16K. Kết quả được hiển thị trong Bảng 4. Chỉ có Dynamic NTK đạt được ngoại suy hiệu quả, trong khi cả ReAttention và InfLLM đều thể hiện sự suy giảm hiệu suất rõ rệt hơn khi độ dài ngữ cảnh tăng. Cụ thể, chúng tôi tập trung báo cáo kết quả của hai nhiệm vụ phụ từ RULER: NIAH-Single3 và NIAH-MultiKey3 (Hsieh et al., 2024). Cả hai nhiệm vụ đều liên quan đến việc trích xuất thông tin chính từ ngữ cảnh chứa các mục gây hiểu lầm. Sự khác biệt là ngữ cảnh cho Single3 bao gồm văn bản tự nhiên, nơi thông tin chính là một chuỗi ký tự chữ và số ngẫu nhiên, trong khi MultiKey3 có một ngăn xếp các cặp khóa-giá trị được tạo thành từ những chuỗi đó. Chúng tôi phát hiện rằng trong khi cả ba phương pháp đều có thể thành công trong thử nghiệm Single3, chỉ có Dynamic NTK dựa trên chú ý đầy đủ mới có thể xử lý hiệu quả MultiKey3.

[Bảng 4: Hiệu suất của Dynamic NTK, InfLLM, và ReAttention trên benchmark RULER ở độ dài ngữ cảnh 8K và 16K. S3 và MK3 là dạng viết tắt của NIAH-Single3 và NIAH-Multikey3 tương ứng.]

[Hình 8: Trực quan hóa K Cache từ lớp cuối cùng của LLaMA3-8B-8K với ngoại suy Dynamic NTK sau khi đầu vào được lấy mẫu ngẫu nhiên từ các tập con Single3 và MultiKey3 trong RULER. Việc trực quan hóa sử dụng phép chiếu t-SNE 2D, với mỗi token được biểu diễn như một điểm trong hình và chỉ số đầu vào được hiển thị qua việc thay đổi màu sắc.]

Để khám phá các cơ chế đằng sau quan sát, chúng tôi thực hiện trực quan hóa t-SNE (Van der Maaten & Hinton, 2008) của K cache trong LLaMA3-8B-8K sử dụng Dynamic NTK cho cả Single3 và MultiKey3, sử dụng K cache trong lớp cuối cùng làm ví dụ. Kết quả được hiển thị trong Hình 8.

Đối với Single3, chủ yếu bao gồm văn bản tự nhiên, K cache được giảm thể hiện một đa tạp duy nhất với một cụm cục bộ và một đặc điểm liên kết tổng thể khi độ dài đầu vào tăng. Ngược lại, đối với MultiKey3, bị chi phối bởi văn bản hỗn loạn (Lv et al., 2024), K cache được giảm tiết lộ nhiều đa tạp chồng lập thay vì mở rộng dọc theo một đường cong duy nhất. Điều này dẫn đến thực tế là trong quá trình lựa chọn K cache trong MultiKey3, bất kể chúng ta sử dụng cùng mã hóa chỉ số hay mã hóa tuần tự trực tiếp cho các phân đoạn được chọn, tự chú ý tiếp theo không thể xác định phân đoạn thuộc nhánh đa tạp nào. Chỉ bằng cách mã hóa tất cả K cache theo chỉ số vị trí gốc của chúng, LLM mới có thể nhận biết ngầm (Chi et al., 2023; Kazemnejad et al., 2024) nhánh đa tạp nào mà nó cư trú. Đây là lý do tại sao chỉ chú ý đầy đủ mới có thể xử lý hiệu quả các nhiệm vụ như vậy.

Tuy nhiên, các văn bản dài hỗn loạn tổng hợp khá hiếm trong các tình huống thực tế (Lv et al., 2024). Trong các benchmark văn bản dài tự nhiên thông thường mà chúng tôi đã kiểm tra, ReAttention liên tục chứng minh hiệu suất mạnh. Do đó, ReAttention vẫn có khả năng cạnh tranh cho ứng dụng trong việc mở rộng độ dài ngữ cảnh của LLM đến vô hạn. Chúng tôi cung cấp thảo luận sâu về các đặc điểm được phản ánh trong các mẫu mở rộng chuỗi QK giữa văn bản tổng hợp và tự nhiên trong Phụ lục E.

5 CÔNG TRÌNH LIÊN QUAN
Ngoại suy độ dài là một vấn đề quan trọng đối với LLM (Press et al., 2022), cụ thể là huấn luyện trong ngữ cảnh ngắn, và duy trì hiệu suất tốt trong ngữ cảnh dài hơn. Nghiên cứu ngoại suy chính chủ yếu tập trung vào điều chỉnh Rotary Position Embedding (RoPE) (Su et al., 2021). Ví dụ, Linear PI (Chen et al., 2023) đầu tiên đạt được ngoại suy độ dài trong LLM bằng cách chia tỷ lệ các chỉ số vị trí về phạm vi tiền huấn luyện với ít tinh chỉnh. Phương pháp NTK (bloc97, 2023b;a; Peng et al., 2023) sau đó điều chỉnh cơ sở xoay trong RoPE (Su et al., 2021) để đạt được ngoại suy độ dài plug-and-play. Tiếp theo, việc khuếch đại cơ sở xoay và huấn luyện trên độ dài dài hơn đã trở thành cách tiếp cận chủ đạo cho ngoại suy độ dài (Rozìere et al., 2023; Xiong et al., 2023; Liu et al., 2023; Ding et al., 2024), nhưng tất cả các phương pháp này đều có giới hạn trên ngoại suy rõ ràng. Ngoài ra, ReRoPE (Su, 2023), Self-Extend (Jin et al., 2024), và ChunkLLaMA (An et al., 2024) cũng đạt được ngoại suy plug-and-play bằng cách giới hạn vị trí tương đối. Tuy nhiên, tất cả các phương pháp trên đều dựa trên chú ý đầy đủ, đối mặt với vấn đề entropy chú ý tăng vọt với độ dài đầu vào, và do đó không thể đạt được độ dài ngữ cảnh vô hạn (Han et al., 2023; Wang et al., 2024).

Ngược lại, một hướng nghiên cứu khác đã cố gắng mở rộng độ dài ngữ cảnh của các mô hình thông qua chú ý thưa. Xem xét phân phối tự chú ý có xu hướng tập trung vào ngữ cảnh toàn cục và cục bộ, StreamingLLM và LM-Infinite đề xuất cửa sổ chú ý hình Λ để đạt được độ dài đầu vào gần như không giới hạn (Xiao et al., 2023; Han et al., 2023). Tuy nhiên, vì đầu vào được đi kèm với việc loại bỏ ngữ cảnh trước đó, nó vẫn không thể mở rộng độ dài ngữ cảnh (Dong et al., 2024). Dựa trên StreamingLLM, InfLLM (Xiao et al., 2024a) và LongHeads (Lu et al., 2024) cố gắng mở rộng ngữ cảnh thông qua truy xuất theo khối từ cache giữa, nhưng các vấn đề phân mảnh ngữ nghĩa và biểu diễn khối cũng ảnh hưởng đến hiệu suất nhiệm vụ downstream (Luo et al., 2024). Gần đây, MInference (Jiang et al., 2024) và RetrievalAttention Liu et al. (2024a) sử dụng lựa chọn cache động để đạt được tăng tốc đáng kể, nhưng không cố gắng ngoại suy ngữ cảnh. Ngược lại, chúng tôi đề xuất ReAttention, coi lựa chọn cache như chú ý đi trước trước tự chú ý thông thường và đạt được ngữ cảnh vô hạn với phạm vi chú ý hữu hạn, giải phóng LLM khỏi thách thức ngoại suy độ dài. Chúng tôi cũng giảm thiểu kernel chú ý top-k như FlashAttention trong tự chú ý (Dao et al., 2022; Dao, 2023). Ngoài ra, vì ReAttention được tiến hành dựa trên lựa chọn cache, chúng tôi cũng mở rộng thảo luận về tối ưu hóa cache, đặc biệt về các phương pháp loại bỏ token như H2O (Zhang et al., 2024d) và SnapKV, trong Phụ lục D.

6 KẾT LUẬN
Trong bài báo này, chúng tôi giới thiệu ReAttention, sử dụng cửa sổ chú ý hữu hạn để thực hiện độ dài ngữ cảnh vô hạn trong mỗi bước suy luận. Chúng tôi đánh giá ReAttention ngang bằng với chú ý đầy đủ về hiệu suất với LongBench, L-Eval, và InfiniteBench. Hơn nữa, ReAttention đã được chứng minh thành công mở rộng ngữ cảnh của các LLM chính, bao gồm series LLaMA và Mistral, đến 1M token và thậm chí mở rộng độ dài ngữ cảnh của LLaMA3.2-3B-chat lên 128× đến 4M mà không cần huấn luyện thêm trong các thử nghiệm Needle-In-A-Haystack. Chúng tôi cũng cải thiện hiệu suất của ReAttention với Triton và đạt được ngoại suy hiệu quả mà không có chi phí bổ sung.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

LỜI CẢM ƠN
Công trình này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc (Số U24B20181). Chúng tôi cũng đánh giá cao các bình luận xây dựng từ các nhà phản biện trong phản bác và thêm thảo luận về phương pháp luận, hiệu quả, hiệu suất, và công trình liên quan trong Phụ lục A đến Phụ lục D.

TÀI LIỆU THAM KHẢO
[Danh sách tài liệu tham khảo sẽ được dịch theo cùng định dạng...]

A MÃ GIẢ CỦA REATTENTION
Trong phần này, chúng tôi trình bày mã giả cho các giai đoạn prefilling và decoding của ReAttention. Cách tiếp cận của chúng tôi sử dụng lựa chọn cache bất khả tri vị trí tại mỗi bước tạo, như được hiển thị trong Phần 2.1.

B XÁC THỰC THÊM VỀ HIỆU QUẢ
Bảng 5 bao gồm so sánh chi tiết của các LLM khác nhau bao gồm LLaMA (Meta, 2024a; Dubey et al., 2024; Meta, 2024b), Mistral(mistralai, 2024), InternLM (Cai et al., 2024b; InternLM, 2024), và series Qwen (Yang et al., 2024a) trong LongBench (Bai et al., 2023) và L-Eval (An et al., 2023).

[Tiếp tục dịch các phần còn lại theo cùng cách...]

--- TRANG 15 ---
[Thuật toán 1: Giai đoạn Prefilling - được dịch đầy đủ với tất cả các bước]

--- TRANG 16 ---
[Thuật toán 2: Giai đoạn Decoding - được dịch đầy đủ với tất cả các bước]

C SO SÁNH THÊM VỀ HIỆU SUẤT
[Bảng 6 và 7 với các kết quả so sánh hiệu suất được dịch]

D CÔNG TRÌNH LIÊN QUAN THÊM
[Phần thảo luận về các kỹ thuật loại bỏ token và các phương pháp liên quan được dịch đầy đủ]

E PHÂN TÍCH ĐẶC ĐIỂM CACHE
[Các phần E.1, E.2, E.3 về phương pháp t-SNE, đặc điểm cache của văn bản tự nhiên dài, và từ văn bản tự nhiên đến văn bản hỗn loạn được dịch đầy đủ]

--- TRANG 19-21 ---
[Tất cả các hình ảnh và phân tích được dịch với các chú thích đầy đủ]
