# EXTENDING CONTEXT WINDOW OF LARGE LAN-
GUAGE MODELS VIA SEMANTIC COMPRESSION
Translated from PDF to Vietnamese
Source path: /home/admin88/arxiv-downloader/long-context/2312.09571.pdf

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
MỞ RỘNG CỬA SỔ NGỮ CẢNH CỦA CÁC MÔ HÌNH NGÔN NGỮ LỚN THÔNG QUA NÉN NGỮ NGHĨA

Weizhi Fei† ‡& Xueyan Niu∗‡
†Khoa Khoa học Toán học, Đại học Tsinghua, Bắc Kinh, Trung Quốc
‡Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.
Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han
Huawei Technologies Co., Ltd.

TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer thường áp đặt các giới hạn về độ dài của văn bản đầu vào để đảm bảo việc tạo ra các phản hồi lưu loát và phù hợp. Ràng buộc này hạn chế khả năng áp dụng của chúng trong các kịch bản liên quan đến văn bản dài. Chúng tôi đề xuất một phương pháp nén ngữ nghĩa mới cho phép khái quát hóa cho các văn bản dài gấp 6-8 lần, mà không phải chịu chi phí tính toán đáng kể hoặc yêu cầu tinh chỉnh. Khung đề xuất của chúng tôi lấy cảm hứng từ mã hóa nguồn trong lý thuyết thông tin và sử dụng một mô hình được đào tạo trước để giảm sự dư thừa ngữ nghĩa của đầu vào dài trước khi chuyển chúng đến LLM cho các tác vụ downstream. Kết quả thực nghiệm chứng minh rằng phương pháp của chúng tôi mở rộng hiệu quả cửa sổ ngữ cảnh của LLM trên một loạt các tác vụ bao gồm trả lời câu hỏi, tóm tắt, học few-shot và truy xuất thông tin. Hơn nữa, phương pháp nén ngữ nghĩa được đề xuất thể hiện tính lưu loát nhất quán trong việc tạo văn bản đồng thời giảm chi phí tính toán liên quan.

1 GIỚI THIỆU
Việc phát hành thành công gần đây của các mô hình ngôn ngữ lớn (LLM) như ChatGPT (Radford et al., 2019) và LLaMA (Touvron et al., 2023) đã khơi dậy những nỗ lực nghiên cứu đáng kể từ cả ngành công nghiệp và học thuật. Những LLM này đã chứng minh khả năng tham gia vào các cuộc trò chuyện lưu loát và mạch lạc với người dùng, và đã cho thấy hiệu suất đặc biệt trên các tác vụ khác nhau, bao gồm tóm tắt tài liệu, trả lời câu hỏi, bot đối thoại, và copilot tạo mã.

Một vấn đề quan trọng mà các LLM tiên tiến (SoTA) gặp phải là giới hạn về độ dài văn bản có thể được nhập vào mô hình cùng một lúc. Khi ngữ cảnh đầu vào vượt quá giới hạn của cửa sổ ngữ cảnh, hiệu suất của những mô hình này suy giảm nhanh chóng. Giới hạn này đặt ra thách thức khi xử lý văn bản dài như bài báo khoa học, tiểu thuyết, và hợp đồng pháp lý với các LLM hiện tại. Kết quả là, đã có sự quan tâm ngày càng tăng trong việc tìm cách mở rộng độ dài đầu vào mà không làm giảm đáng kể hiệu suất của mô hình.

Giới hạn về cửa sổ ngữ cảnh chủ yếu xuất phát từ tính toán bậc hai của cơ chế tự chú ý trong transformer. Việc xử lý văn bản dài làm tăng đáng kể chi phí tính toán về bộ nhớ và thời gian. Thông thường, các mô hình được đào tạo trên ngữ cảnh ngắn, và độ dài chuỗi tối đa (tức là cửa sổ ngữ cảnh) được xác định. Nếu các mô hình bị buộc tạo ra ngữ cảnh vượt quá cửa sổ ngữ cảnh, chúng có xu hướng làm giảm chất lượng đầu ra do thiếu thông tin mã hóa vị trí trong quá trình đào tạo. Hơn nữa, việc tạo ra các chuỗi dài áp đặt yêu cầu bộ nhớ đáng kể lên thiết bị tính toán. Sự tích lũy của yêu cầu bộ nhớ này và việc thiếu mã hóa vị trí hiệu quả có thể dẫn đến thất bại khái quát hóa độ dài (Anil et al., 2022), nơi các mô hình gặp khó khăn trong việc tạo ra văn bản có ý nghĩa và mạch lạc vượt quá kích thước cửa sổ ngữ cảnh nhất định.

Một số phương pháp đã được phát triển để giải quyết những thách thức nêu trên. Một phương pháp là thiết kế các kiến trúc với độ phức tạp gần như tuyến tính, cho phép mở rộng hiệu quả để xử lý các chuỗi rất dài. Tuy nhiên, việc đào tạo một mô hình lớn từ đầu phải chịu chi phí đáng kể. Một chiến lược khác liên quan đến việc sử dụng các kỹ thuật nội suy và tinh chỉnh để điều chỉnh mã hóa vị trí cho các độ dài chuỗi chưa thấy. Mặc dù phương pháp này có khả năng làm giảm hiệu suất tổng thể của LLM, nó vẫn đòi hỏi thời gian và tài nguyên GPU đáng kể cho việc tinh chỉnh và suy luận trên các chuỗi dài. Do đó, việc thiết kế các phương pháp không cần thiết phải thay đổi các tham số của mô hình được đào tạo trước sẽ hiệu quả và thân thiện với tài nguyên hơn.

Trong khi hầu hết các thuật toán trước đây dựa vào việc sửa đổi mô hình được đào tạo trước, chúng tôi thay vào đó khai thác các tính chất thống kê của ngôn ngữ tự nhiên đầu vào. Một hiện tượng thực nghiệm, được biết đến như định luật Zipf (Zipf, 2016), quan sát thấy rằng một tập hợp nhỏ các token từ thường xuyên nhất trong một kho ngữ liệu lớn của ngôn ngữ tự nhiên chiếm gần như tất cả các lần xuất hiện. Mô hình này phát sinh từ xu hướng của người dùng ngôn ngữ trong việc giảm thiểu nỗ lực trong các cuộc trò chuyện hàng ngày của họ. Do đó, bằng cách sử dụng từ vựng mở rộng, các câu có thể được rút ngắn đáng kể trong khi vẫn bảo tồn cùng ý nghĩa ngữ nghĩa. Hơn nữa, việc người dùng ngôn ngữ bao gồm các từ dư thừa trong giao tiếp là điều phổ biến (Strunk Jr, 2007). Những thói quen ngôn ngữ này phổ biến trong số người dùng, và chúng tôi đề xuất bao gồm một mô-đun nén ngữ nghĩa để giảm thiểu sự dư thừa liên quan đến những thói quen này.

Phương pháp nén ngữ nghĩa được đề xuất của chúng tôi, gợi nhớ đến mã hóa nguồn có mất mát trong lý thuyết thông tin, mở rộng cửa sổ ngữ cảnh bằng cách rút ngắn văn bản dài một cách tương đương trong khi bảo tồn ý nghĩa ngữ nghĩa. Quy trình này được thực hiện trước khi nhập các token vào LLM được đào tạo trước. Như minh họa trong Hình 1, đầu vào trải qua nén trước khi được truyền đến LLM cho các tác vụ tiềm năng khác nhau. Phương pháp nén ngữ nghĩa có thể được tùy chỉnh và tối ưu hóa cho các tác vụ downstream, có tính đến các ràng buộc thực tế như thời gian và tài nguyên bộ nhớ. Việc triển khai mô-đun nén ngữ nghĩa là đơn giản và có thể dễ dàng được tích hợp vào các phương pháp mở rộng cửa sổ ngữ cảnh dựa trên nội suy khác và API hộp đen. Nó thể hiện hiệu suất tăng cường so với các phương pháp dựa trên nội suy SoTA trên một loạt các tác vụ, bao gồm trả lời câu hỏi một tài liệu, trả lời câu hỏi nhiều tài liệu, tóm tắt, học few-shot, và truy xuất thông tin, sử dụng các bộ dữ liệu thực tế trong khi không phát sinh thêm cập nhật tham số hoặc tiêu thụ bộ nhớ. Thực nghiệm cho thấy, phương pháp được đề xuất có hiệu quả tính toán và đạt được mở rộng cửa sổ ngữ cảnh 6-8 lần.

Đóng góp của chúng tôi:
• Chúng tôi giới thiệu một khung mở rộng cửa sổ ngữ cảnh cho LLM sử dụng nén ngữ nghĩa. Khung này phục vụ như một công cụ cắm và chạy để giảm thiểu sự dư thừa trong văn bản đầu vào bằng cách thực hiện mô hình hóa chủ đề một cách hiệu quả.
• Chúng tôi xây dựng một biểu diễn đồ thị của đầu vào để xác định các phần riêng biệt của văn bản liên quan đến các chủ đề khác nhau. Kết quả là việc phân đoạn văn bản dài thành các khối riêng biệt, mỗi khối tập trung vào một chủ đề cụ thể. Sau đó chúng tôi chinh phục từng khối một cách độc lập, tạo ra một phiên bản ngắn gọn của văn bản gốc. Kỹ thuật nén này giúp cô đọng thông tin trong khi bảo tồn các ý tưởng chính và ngữ cảnh.
• Chúng tôi chứng minh khả năng áp dụng của phương pháp nén ngữ nghĩa được đề xuất thông qua các thực nghiệm rộng rãi. Kết quả làm nổi bật những ưu điểm của phương pháp chúng tôi trong một số ứng dụng chính, bao gồm trả lời câu hỏi một tài liệu, trả lời câu hỏi nhiều tài liệu, tóm tắt, học few-shot, và truy xuất thông tin.

2 CÔNG TRÌNH LIÊN QUAN
Với sự tiến bộ của các LLM SoTA, tiến bộ đáng kể đã được thực hiện trong việc mở rộng độ dài cửa sổ ngữ cảnh.

--- TRANG 2 ---
Hình 1: Với việc bao gồm mô-đun nén ngữ nghĩa, các sự dư thừa trong đầu vào được loại bỏ, do đó mở rộng hiệu quả cửa sổ ngữ cảnh. Nén ngữ nghĩa gợi nhớ đến khái niệm mã hóa nguồn trong lý thuyết thông tin.

--- TRANG 3 ---
2.1 NGOẠI SUY VÀ NỘI SUY
Hướng nghiên cứu chính thống nhằm mục đích điều chỉnh các mô hình ngôn ngữ hiện có được đào tạo trên văn bản ngắn để phù hợp với văn bản dài hơn trong quá trình suy luận (Anil et al., 2022). Ý tưởng chính là sửa đổi nhúng vị trí, mà chỉ được đào tạo trên văn bản ngắn. Một số nghiên cứu dựa trên Rotary Position Embeddings (RoPE) của LLaMA và các phương pháp điều chỉnh nó cho các chuỗi dài hơn. Chen et al. (2023a) phát triển phương pháp Position Interpolation (PI) để chia tỷ lệ tuyến tính các chỉ số vị trí đầu vào. Peng et al. (2023) trình bày YaRN, một cơ chế ngoại suy hiệu quả lấy cảm hứng từ neural tangent kernel, để mở rộng cửa sổ ngữ cảnh lên 64k và 128k.

2.2 CÁC PHÉP TOÁN CHÚ Ý HIỆU QUẢ
Do cơ chế tự chú ý, chi phí suy luận của LLM tăng theo bậc hai với độ dài chuỗi. Nhiều phương pháp đã được đề xuất để giảm độ phức tạp. Dai et al. (2019) trình bày Transformer-XL sử dụng recurrence agency cấp độ đoạn và một sơ đồ mã hóa vị trí mới. Beltagy et al. (2020) giới thiệu Longformer với cơ chế chú ý thưa thớt tăng tỷ lệ tuyến tính với độ dài chuỗi. Bo (2021) cung cấp một transformer nhanh hơn, RWKV, kết hợp sức mạnh của RNN và có độ phức tạp tuyến tính trong quá trình suy luận. Dao et al. (2022) đề xuất FlashAttention, một chiến lược phân khối cho đầu vào, và sử dụng tính toán lại để tránh độ phức tạp bậc hai của tính toán chú ý. Trong khi những phương pháp này có khả năng xử lý các chuỗi đầu vào dài hơn (Ding et al., 2023), việc đào tạo các mô hình mới có thể tốn kém. Hơn nữa, những phương pháp này không hiệu quả khi xử lý nội dung có độ dài ngoài phân phối.

Việc giới thiệu các nhúng vị trí mới đòi hỏi tinh chỉnh trên các chuỗi dài để thích nghi với độ dài tăng, điều này có thể tốn kém về mặt tính toán. Để giải quyết điều này, LongLoRA được giới thiệu bởi Chen et al. (2023b), cung cấp một phương pháp tinh chỉnh hiệu quả với chi phí tính toán hạn chế. Chi tiết thêm về một số chiến lược phân khối khác được cung cấp trong khảo sát của Huang et al. (2023).

2.3 PROMPTING
Có những nỗ lực đang diễn ra để mở rộng cửa sổ ngữ cảnh thông qua thiết kế prompting thông minh. Wingate et al. (2022) sử dụng soft prompts để mã hóa nhiều thông tin hơn bằng cách sử dụng ít token hơn. Chevalier et al. (2023) trình bày AutoCompressor, sử dụng soft prompts để nén chuỗi đầu vào và sau đó mở rộng độ dài gốc của mô hình cơ sở. Cả Zhou et al. (2023) và Wang et al. (2023) áp dụng LLM một cách đệ quy để tóm tắt văn bản đầu vào nhằm duy trì bộ nhớ dài-ngắn hạn cho các mục đích cụ thể như viết truyện và tạo đối thoại, tương ứng.

3 PHƯƠNG PHÁP LUẬN
Chúng tôi đề xuất phương pháp nén ngữ nghĩa để mở rộng cửa sổ ngữ cảnh. Ý tưởng cốt lõi là nén đầu vào thành văn bản ngắn hơn mà không mất thông tin chính và chi tiết quan trọng. Điều này cho phép chúng tôi hiệu quả bao gồm nhiều nội dung hơn trong ràng buộc độ dài đầu vào cố định của LLM. Hình 2 cung cấp tổng quan về phương pháp của chúng tôi, tận dụng các mô hình tóm tắt được đào tạo trước thường được sử dụng trong Xử lý Ngôn ngữ Tự nhiên (NLP).

Các phương pháp tóm tắt hiện có cũng có giới hạn về độ dài của đầu vào. Ở đây, chúng tôi đề xuất một phương pháp dựa trên chia-để-trị có tính đến cấu trúc của văn bản. Bằng cách xác định cấu trúc chủ đề của văn bản dài và chia chúng thành các khối thể hiện mức độ độc lập lẫn nhau nhất định, nội dung trong mỗi khối có thể được nén hiệu quả do tương quan thống kê của chúng. Mỗi khối sau đó được xử lý song song bằng các mô hình được đào tạo trước, và kết quả được kết hợp để tạo ra một đầu vào văn bản cô đọng có thể được xử lý bởi LLM. Phương pháp này nhằm cung cấp một cách hiệu quả và hiệu quả hơn để tóm tắt văn bản dài bằng cách tận dụng cả cấu trúc và nội dung của văn bản gốc.

3.1 MÔ HÌNH
Nội dung văn bản thực tế, như lời nói và sách, thường xuyên hiển thị cấu trúc phân cấp, trong đó mỗi phần được cấu trúc xung quanh một chủ đề cụ thể, và các phần khác nhau khác biệt về chủ đề theo cách tuần tự. Cấu trúc phân cấp này, dựa trên chủ đề, có sự tương đồng với các clique trong đồ thị. Để xác định cấu trúc này trong văn bản dài, chúng tôi sử dụng đồ thị có trọng số để biểu diễn chúng và sử dụng các phương pháp phân cụm để phát hiện clique trong những đồ thị này. Các clique sau đó có thể được sử dụng để biểu diễn nội dung dựa trên chủ đề của văn bản, cho phép chúng tôi có được các khối dựa trên sự liên quan ngữ nghĩa của các chủ đề.

Chúng tôi bắt đầu bằng cách tuần tự xây dựng các khối cấp câu trong các độ dài nhất định và biểu diễn chúng như các nút trong đồ thị của chúng tôi. Trong bước này, chúng tôi phân tích văn bản thành các câu hoặc câu phụ khác nhau dựa trên dấu câu. Tiếp theo, chúng tôi tuần tự điền vào các khối cấp câu cho đến khi chúng vượt quá độ dài mong muốn trước khi tiến hành các khối tiếp theo. Khi chúng tôi đã có được các khối cấp câu, chúng tôi kết nối biểu diễn đồ thị của văn bản dài G dựa trên một mô hình nhúng câu được đào tạo trước (ví dụ: MiniLM (Wang et al., 2020)), trong đó trọng số G[i][j] đại diện cho sự tương đồng ngữ nghĩa giữa khối cấp câu thứ i và thứ j. Thông thường, sự tương đồng này được tính toán bằng cosine similarity, đo lường cosine của góc giữa hai embedding. Nếu sự tương đồng giữa hai khối cao hơn, điều đó cho thấy chúng gần nhau hơn về chủ đề.

3.2 PHÂN KHỐI DỰA TRÊN CHỦ ĐỀ
Sau đó chúng tôi áp dụng các thuật toán phân cụm trên đồ thị để xác định cấu trúc chủ đề cơ bản. Trong mỗi cụm, chúng tôi nhóm các khối cấp câu một cách tuần tự để có được các khối dựa trên chủ đề, sau đó có thể được xử lý đồng thời bởi mô hình được đào tạo trước được chọn theo tác vụ downstream. Số lượng cụm có thể được điều chỉnh để điều chỉnh độ dài của văn bản sau nén ngữ nghĩa. Nếu những khối ngữ nghĩa này vẫn vượt quá độ dài định trước, quy trình tương tự được lặp lại để có được cấu trúc chủ đề cấp phụ.

Các cấu trúc chủ đề thu được có dạng cây, có thể được làm phẳng theo thứ tự của nội dung gốc. Theo mô hình, mỗi khối được tập trung ngữ nghĩa xung quanh một chủ đề cụ thể, và những chủ đề này loại trừ lẫn nhau. Do đó, những khối này có thể được nén độc lập bằng cách sử dụng mô hình tóm tắt được đào tạo trước. Việc lựa chọn từ các mô hình tóm tắt được đào tạo trước khác nhau cho phép đánh đổi giữa hiệu quả và hiệu suất. Do đó, chúng tôi có thể chọn thay thế có chọn lọc các khối gốc bằng đầu ra của những mô hình được đào tạo trước này để đảm bảo bảo tồn cấu trúc chủ đề cơ bản. Văn bản được nén ngữ nghĩa có thể được chuyển tiếp đến LLM trực tiếp hoặc kết hợp với các sơ đồ mở rộng khác để tăng cường kết quả tổng thể hơn nữa.

--- TRANG 4 ---
Hình 2: Minh họa về phương pháp nén ngữ nghĩa của chúng tôi. Văn bản đầu vào ban đầu được phân đoạn thành các khối dựa trên chủ đề, sử dụng biểu diễn đồ thị. Tiếp theo, những khối này trải qua tinh chỉnh bằng các mô hình được đào tạo trước để đảm bảo bảo tồn thông tin chính. Cuối cùng, các khối được tinh chỉnh được lắp ráp theo thứ tự gốc. Văn bản kết quả, đã được nén ngữ nghĩa, ngắn hơn khoảng 6-8 lần so với đầu vào gốc. Do đó, chúng nằm trong cửa sổ ngữ cảnh của LLM. Hơn nữa, để mở rộng thêm độ dài, các phương pháp khác như kỹ thuật dựa trên ngoại suy và nội suy có thể được nối tiếp.

--- TRANG 5 ---
Hình 3: Ví dụ về prompt tổng hợp cho tác vụ truy xuất passkey (Mohtashami & Jaggi, 2023). LLM được đào tạo trước không thể xử lý đầu vào dài do ràng buộc độ dài ngữ cảnh. Bằng cách áp dụng nén ngữ nghĩa, thông tin dư thừa trong tài liệu dài được loại bỏ, và đầu vào được nén giữ lại thông tin chính thiết yếu. LLM sau đó có thể xử lý đầu vào được nén cùng với prompt để tạo ra câu trả lời chính xác. Đáng chú ý, các màu sắc khác biệt được sử dụng trong minh họa tương ứng với các khối dựa trên chủ đề.

4 THỰC NGHIỆM
Chúng tôi chứng minh rằng phương pháp nén ngữ nghĩa được đề xuất có thể mở rộng hiệu quả cửa sổ ngữ cảnh lên đến 7-8 lần mà không sửa đổi các tham số của các mô hình được đào tạo trước. Hơn nữa, mô-đun nén ngữ nghĩa có thể được tích hợp liền mạch với các phương pháp hiện có, cho phép mở rộng thêm cửa sổ ngữ cảnh. Tính linh hoạt này cho phép phương pháp của chúng tôi được điều chỉnh và kết hợp với các kỹ thuật khác, tăng cường hiệu suất tổng thể và tính linh hoạt. Để đánh giá hiệu suất của phương pháp chúng tôi, chúng tôi tiến hành thực nghiệm trên một số tác vụ ngôn ngữ đòi hỏi hiểu biết về ngữ cảnh dài. Những tác vụ này bao gồm truy xuất passkey, trả lời câu hỏi một tài liệu, trả lời câu hỏi nhiều tài liệu, tóm tắt, và học few-shot. Trong mỗi tác vụ, mô hình được cung cấp một chuỗi ngữ cảnh C (thông thường là văn bản dài) và một chuỗi văn bản Q (ví dụ: một prompt), và nó được mong đợi tạo ra câu trả lời đầu ra A. Ngoài ra, chúng tôi cũng điều tra chỉ số perplexity (Peng et al., 2023), đo lường khả năng dự đoán văn bản của mô hình và phục vụ như một chỉ số về tính lưu loát của đầu ra được tạo. Phân tích này cho phép chúng tôi đánh giá không chỉ hiệu quả mà còn chất lượng của đầu ra được tạo.

4.1 CÁC TÁC VỤ VÀ BỘ DỮ LIỆU
Chúng tôi bắt đầu bằng cách đánh giá phương pháp nén ngữ nghĩa được đề xuất trên các tác vụ benchmark tiêu chuẩn khác nhau, sử dụng mô hình LLaMA 7B được đào tạo trước (Touvron et al., 2023). Kích thước cửa sổ ngữ cảnh gốc của mô hình này là 4096. Các tác vụ và bộ dữ liệu được sử dụng trong đánh giá của chúng tôi được lấy từ benchmark SCROLLS (Shaham et al., 2022) và LongBench (Bai et al., 2023). Những bộ dữ liệu này cung cấp ngữ cảnh toàn diện và đa dạng cho phân tích của chúng tôi.

Truy xuất Passkey Truy xuất đã là một ứng dụng quan trọng của LLM. Chúng tôi đánh giá phương pháp được đề xuất bằng một tác vụ tổng hợp cho truy xuất passkey được giới thiệu bởi Mohtashami & Jaggi (2023), trong đó các prompt được tổng hợp để che giấu một passkey được tạo trong một phần được chọn ngẫu nhiên của một tài liệu dài. Tác vụ truy xuất passkey đánh giá khả năng của mô hình trong việc trích xuất thông tin quan trọng từ bất kỳ vị trí nào trong ngữ cảnh dài. Một minh họa của tác vụ được hiển thị trong Hình 3. Văn bản dài tổng hợp kết hợp các chữ số passkey, và tác vụ cho LLM là truy xuất những chữ số này từ văn bản đầu vào. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.

--- TRANG 6 ---
Các Tác vụ NLP Tổng quát LongBench (Bai et al., 2023) là một benchmark đa tác vụ được thiết kế cho các kịch bản văn bản dài, bao gồm sáu tác vụ riêng biệt. Trong nghiên cứu này, chúng tôi tập trung vào ba tác vụ tiếng Anh từ tập hợp bốn tác vụ ngôn ngữ tự nhiên, cụ thể là trả lời câu hỏi một tài liệu, trả lời câu hỏi nhiều tài liệu, tóm tắt, và học few-shot. Mỗi bộ dữ liệu được chọn chứa 200 instance. Thông tin thêm có thể được tìm thấy trong Phụ lục A.

Tính Lưu loát Chúng tôi đánh giá tính lưu loát của phương pháp nén ngữ nghĩa bằng điểm perplexity, được định nghĩa là số mũ của trung bình log-likelihood âm của mô hình xác suất P trên phân phối D, tức là,
PPL(D, P) := exp(-Ex∈D log P(x)).
Điểm perplexity nhỏ hơn cho thấy các chuỗi lưu loát hơn phù hợp với mô hình.

4.2 CÁC ĐƯỜNG CƠ SỞ
Chúng tôi chọn các giải pháp SoTA từ mỗi phương pháp chính thống làm đường cơ sở của chúng tôi.

Phân khối kích thước cố định Để điều chỉnh ngữ cảnh dài trong một cửa sổ ngữ cảnh kích thước cố định, phân khối là một phương pháp đơn giản nhưng hiệu quả. Trong các ứng dụng liên quan đến NLP, các phần văn bản lớn thường được chia nhỏ thành các đoạn nhỏ hơn cho các ứng dụng được nhắm mục tiêu. Khi độ dài đầu vào vượt quá cửa sổ ngữ cảnh, phương pháp phân khối kích thước cố định (Bai et al., 2023) cắt bớt chuỗi đầu vào từ giữa. Điều này là do thông tin quan trọng nhất thường nằm ở đầu và cuối của chuỗi.

Phương pháp dựa trên nội suy YaRN (Peng et al., 2023) là một phương pháp hiệu quả tính toán để nội suy mã hóa vị trí, điều chỉnh động Relative Positional Encoding (RoPE) qua các chiều và chia tỷ lệ sự chú ý. YaRN cung cấp nhiều mô hình mở rộng độ dài cho các phiên bản khác nhau của Llama2, với các mô hình được đào tạo trên tổng cộng 64 GPU từ 8 máy ×A100. Để đảm bảo so sánh công bằng, chúng tôi chọn mô hình dựa trên Llama2 7B, được điều chỉnh từ 4k đến 64k, làm đường cơ sở của chúng tôi.

Phương pháp tinh chỉnh LongLoRA (Chen et al., 2023b) là một phương pháp hiệu quả cho tinh chỉnh kết hợp LoRA và shifts sparse attention để giảm chi phí tính toán. LongLoRA áp dụng kỹ thuật này cho các mô hình Llama2 có kích thước khác nhau, từ Llama2 7B, Llama2 13B, đến Llama2 70B, với độ dài token được mở rộng từ 4k đến 32k trên một thiết bị 8×A100 duy nhất. Để đảm bảo so sánh công bằng và không thiên vị, chúng tôi chọn mô hình Llama2 7B với mở rộng ngữ cảnh đạt được thông qua tinh chỉnh LoRA cải tiến làm đường cơ sở của chúng tôi.

5 KẾT QUẢ
Chúng tôi báo cáo các kết quả chính cùng với phân tích toàn diện.

Tính Lưu loát Chúng tôi sử dụng mô hình Llama2 làm đường cơ sở để đánh giá tính lưu loát của văn bản được tạo bằng cách tính điểm perplexity (PPL). Các mẫu từ bộ dữ liệu GovReport được chọn ở các độ dài khác nhau, và văn bản tham chiếu được so sánh với văn bản được tạo trong quá trình tính toán. Trong trường hợp độ dài của văn bản đầu vào vượt quá cửa sổ ngữ cảnh của Llama2, mô-đun nén ngữ nghĩa của chúng tôi rút ngắn đầu vào, do đó cho phép mô hình tiếp tục tạo nội dung mới một cách lưu loát. Điểm số kết quả được mô tả trong Hình 4. Các biểu đồ chỉ ra rằng perplexity của Llama2 ban đầu giảm, nhưng khi nó vượt quá độ dài cửa sổ, nó tăng nhanh chóng. Tuy nhiên, khi phương pháp nén ngữ nghĩa của chúng tôi được sử dụng, PPL vẫn nhất quán thấp. Điều này cho thấy rằng phương pháp của chúng tôi thành công mở rộng cửa sổ ngữ cảnh lên đến ba lần mà không làm giảm chất lượng tạo của mô hình ngôn ngữ.

Truy xuất Passkey Chúng tôi trình bày kết quả của tác vụ truy xuất passkey trong Hình 5. Khi sử dụng Llama2 cho truy xuất passkey, chúng tôi quan sát sự sụt giảm nhanh chóng về độ chính xác xuống không khi độ dài đầu vào vượt quá kích thước cửa sổ 4096. Tuy nhiên, bằng cách sử dụng phương pháp của chúng tôi, độ chính xác truy xuất của mô hình Llama2 vẫn trên 90% ngay cả đối với đầu vào có độ dài lên đến 30.000. Điều này cho thấy

--- TRANG 7 ---
0 2000 4000 6000 8000 10000 12000
Độ dài 2e4 6e8 10PPL của chúng tôi
llama2

Hình 4: Perplexity trên bộ dữ liệu GovReport được đánh giá ở các độ dài chuỗi khác nhau. Các đường cong perplexity của Llama2 (xanh lá) và phương pháp của chúng tôi (tím) thể hiện xu hướng tương tự cho các chuỗi có độ dài lên đến 4k. Tuy nhiên, khi độ dài chuỗi vượt quá độ dài đào tạo 4k, phương pháp của chúng tôi hiệu quả làm phẳng đường cong perplexity, cho thấy rằng tính lưu loát được bảo tồn cho các chuỗi dài hơn.

0 10000 20000 30000 40000 50000 60000
Độ dài 0 20 40 60 80 100 Acc llama2
của chúng tôi+llama2
của chúng tôi+yarn+llama2

Hình 5: So sánh giữa các biến thể mô hình trên tác vụ truy xuất passkey. Độ chính xác truy xuất của đường cơ sở Llama2 (xanh lá) giảm xuống không ở khoảng 5k do vấn đề hết bộ nhớ. Phương pháp của chúng tôi (tím) thành công mở rộng độ dài lên 30k. Hơn nữa, khi kết hợp với phương pháp dựa trên ngoại suy SoTA YaRN, độ dài ngữ cảnh có thể được mở rộng thêm lên trên 60k đảm bảo rằng độ chính xác truy xuất vẫn nhất quán trên 90%.

rằng phương pháp nén ngữ nghĩa mở rộng kích thước cửa sổ ngữ cảnh của mô hình ngôn ngữ khoảng 7-8 lần. Hơn nữa, chúng tôi kết hợp phương pháp của chúng tôi với phương pháp dựa trên nội suy SoTA, YaRN, để mở rộng thêm kích thước cửa sổ ngữ cảnh lên đến 60.000, trong khi nhất quán duy trì độ chính xác trên 90%.

Các Tác vụ NLP Tổng quát Chúng tôi trình bày kết quả của chúng tôi trên các tác vụ NLP tổng quát khác nhau trong Bảng 1, bao gồm trả lời câu hỏi một tài liệu, trả lời câu hỏi nhiều tài liệu, tóm tắt, và học few-shot. Khi độ dài token nhỏ hơn 4k, không cần nén ngữ cảnh, và phương pháp của chúng tôi thực hiện ở cùng mức với mô hình Llama2 gốc. Tuy nhiên, cả phương pháp dựa trên nội suy YaRN và phương pháp tinh chỉnh LongLora đều ảnh hưởng tiêu cực đến hiệu suất của mô hình Llama2 trên hầu hết tất cả các tác vụ. Trong phạm vi 4k-8k, phương pháp của chúng tôi vượt trội hơn các phương pháp khác trong 8 trên 11 tác vụ. Đáng chú ý là mô hình của chúng tôi thực hiện kém hơn một chút trong tác vụ học few-shot. Điều này có thể được quy cho thực tế rằng học few-shot đòi hỏi thông tin chi tiết hơn, trong khi sơ đồ nén của chúng tôi duy trì thông tin trong một cửa sổ cố định. Chuyển sang phạm vi 8k-16k, phương pháp của chúng tôi đạt kết quả tốt nhất trong 9 trên 12 tác vụ, thể hiện hiệu suất tương tự như phạm vi 4k-8k. Trong phạm vi 16k-32k, phương pháp của chúng tôi vượt trội hơn các phương pháp khác trong 6 trên 11 tác vụ. Trong phạm vi 32k+, các phương pháp khác thất bại do vấn đề hết bộ nhớ, trong khi phương pháp của chúng tôi vẫn duy trì 70% hiệu suất đạt được trong phạm vi 4k.

6 KẾT LUẬN
Trong công trình này, chúng tôi đề xuất một phương pháp mới để giải quyết giới hạn độ dài đầu vào trong các mô hình ngôn ngữ lớn bằng nén ngữ nghĩa. Bằng cách tận dụng các tính chất thống kê của ngôn ngữ tự nhiên và khai thác sự dư thừa trong giao tiếp, chúng tôi có thể rút ngắn đáng kể văn bản trong khi bảo tồn ý nghĩa ngữ nghĩa của chúng. Điều này cho phép mở rộng cửa sổ ngữ cảnh 6-8 lần mà không cần sửa đổi các tham số của mô hình được đào tạo trước hoặc phát sinh chi phí tính toán bổ sung. Hơn nữa, việc triển khai mô-đun nén ngữ nghĩa của chúng tôi là đơn giản và có thể dễ dàng tích hợp vào các phương pháp dựa trên nội suy khác và API hộp đen. Điều này cung cấp tính linh hoạt và khả năng thích ứng cho các tác vụ downstream khác nhau, có tính đến các ràng buộc thực tế như thời gian và tài nguyên bộ nhớ. Chúng tôi tin rằng công trình của chúng tôi có thể dẫn đến phương pháp mở rộng cửa sổ ngữ cảnh đơn giản hơn để được sử dụng trong thực tế, do đó giảm chi phí của các mô hình ngôn ngữ lớn.

--- TRANG 8 ---
[Bảng 1: So sánh phương pháp nén ngữ nghĩa của chúng tôi với các phương pháp đường cơ sở khác trên nhiều tác vụ từ bộ dữ liệu LongBench. Method (4k) biểu thị kết quả đánh giá trên văn bản ngắn hơn 4k. Cột cuối cùng, được ghi nhãn 4k, thể hiện hiệu suất của đường cơ sở Llama2-7B-chat-4k. Đáng chú ý, phương pháp của chúng tôi nhất quán vượt trội hoặc đạt kết quả tương tự so với các phương pháp mở rộng độ dài SoTA khác.]

--- TRANG 9 ---
TÀI LIỆU THAM KHẢO
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546–38556, 2022.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020.

PENG Bo. Blinkdl/rwkv-lm: 0.01, August 2021. URL https://doi.org/10.5281/zenodo.5196577.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv, 2023b.

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. ArXiv, abs/2305.14788, 2023. URL https://api.semanticscholar.org/CorpusID:258865249.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4599–4610, 2021.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. In Proceedings of the 10th International Conference on Learning Representations, 2023.

Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074–1084, 2019.

Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 6609–6625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling-main.580.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419–1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112.

--- TRANG 10 ---
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large language models: A comprehensive survey, 2023.

Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018.

Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002.

Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences. arXiv preprint arXiv:2201.03533, 2022.

William Strunk Jr. The Elements of Style. Penguin, 2007.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology.org/2022.tacl-1.31.

Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. Recursively summarizing enables long-term dialogue memory in large language models. arXiv preprint arXiv:2308.15022, 2023.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776–5788, 2020.

David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5621–5634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.412. URL https://aclanthology.org/2022.findings-emnlp.412.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905–5921, 2021.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023.

--- TRANG 11 ---
George Kingsley Zipf. Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology. Ravenio Books, 2016.

A CÁC BỘ DỮ LIỆU
Single-Doc QA
• NarrativeQA (Kočiský et al., 2018) là một bộ dữ liệu trả lời câu hỏi tiêu chuẩn bao gồm sách từ Project Gutenberg3 và kịch bản phim từ danh sách các trang web. Các cặp câu hỏi-câu trả lời được cung cấp bởi người chú thích, sao cho mỗi cuốn sách và kịch bản trong số 1.567 cuốn có khoảng 30 câu hỏi và câu trả lời, và hai câu trả lời tham chiếu được đưa ra cho mỗi câu hỏi.
• Qasper (Dasigi et al., 2021) là một bộ dữ liệu trả lời câu hỏi của các xuất bản NLP chứa các câu hỏi tóm tắt, trích xuất và có/không.
• MultiFieldQA-en (Bai et al., 2023) là một bộ dữ liệu được tạo từ nhiều nguồn bao gồm tài liệu pháp lý, báo cáo chính phủ, bách khoa toàn thư, và xuất bản học thuật. Sinh viên tiến sĩ được yêu cầu chú thích các truy vấn và phản hồi của mỗi bài báo.

Multi-Doc QA
• HotpotQA (Yang et al., 2018) bao gồm nhiều câu hỏi 2-hop được viết bởi người bản ngữ dựa trên hai đoạn văn liên quan.
• 2WikiMultihopQA (Ho et al., 2020) liên quan đến các câu hỏi lên đến 5-hop được xây dựng có hệ thống bằng các mẫu thủ công. Việc trả lời những câu hỏi này đòi hỏi các đường dẫn lý luận và không thể được giải quyết bằng nội dung địa phương.
• MuSiQue (Trivedi et al., 2022) bao gồm các câu hỏi lên đến 4-hop và loại bỏ các phím tắt và câu hỏi tự nhiên. Mỗi câu hỏi chứa 2-4 đoạn văn bổ sung trình bày đường dẫn lý luận và các đoạn văn liên quan.

Summarization
• GovReport (Huang et al., 2021) thu thập các báo cáo chi tiết chứa tóm tắt được viết bởi con người từ Văn phòng Trách nhiệm Chính phủ Hoa Kỳ và Dịch vụ Nghiên cứu Quốc hội. Những báo cáo này bao quát nhiều vấn đề chính sách quốc gia.
• QMSum (Zhong et al., 2021) chứa các cặp tóm tắt cuộc họp được chú thích qua nhiều lĩnh vực bao gồm các cuộc họp sản phẩm, học thuật và ủy ban.
• MultiNews (Fabbri et al., 2019) là một bộ dữ liệu tóm tắt nhiều tài liệu. (Bai et al., 2023) nhóm 2-10 bài báo thảo luận về cùng một sự kiện hoặc chủ đề, mỗi bài được ghép nối với một tóm tắt được viết bởi con người và tạo thành một tác vụ tóm tắt văn bản dài mới.

Few-Shot Learning Để xây dựng học few-shot với văn bản dài, (Bai et al., 2023) chọn một loạt các ví dụ đào tạo trong các bộ dữ liệu sau để nối ngữ cảnh trong LongBench.
• TREC (Li & Roth, 2002) là một bộ dữ liệu phân loại với nhãn lớp chi tiết.
• TriviaQA (Zhong et al., 2021) là một bộ dữ liệu phân loại và liên quan đến các cuộc trò chuyện giống messenger với tóm tắt được viết bởi con người.
• SAMSum (Fabbri et al., 2019) bộ dữ liệu hiểu đọc và bao gồm các cặp câu hỏi-câu trả lời được chú thích với các đoạn văn bằng chứng.

Passkey Các prompt được tạo ngẫu nhiên của tác vụ truy xuất passkey có định dạng như Hình 6.

B CHI TIẾT TRIỂN KHAI
Trong phần này, chúng tôi cung cấp chi tiết về việc triển khai thuật toán của chúng tôi. Thuật toán của chúng tôi sử dụng một số mô hình mã nguồn mở trưởng thành. Để biểu diễn đồ thị, chúng tôi sử dụng các mô hình tương đồng câu all-MiniLM-L6-v2 được cung cấp bởi nền tảng Sentence Transformer, có thể được tìm thấy tại liên kết sau: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2. Để nén ngữ nghĩa, chúng tôi sử dụng mô hình được đào tạo trước distilbart-cnn-12-61. Trong hầu hết các thực nghiệm của chúng tôi, chúng tôi sử dụng Llama2-7B-chat-4k làm mô hình ngôn ngữ lớn cơ sở (Touvron et al., 2023). Các thực nghiệm được tiến hành trên một GPU A40 duy nhất với bộ nhớ 48GB.

C ĐỘ PHỨC TẠP
Cho một ngữ cảnh có độ dài L, độ phức tạp gốc là O(L²). Xem xét các giới hạn độ dài của mô-đun nén, chúng tôi giả định nó có độ dài đầu vào tối thiểu γ₁ và độ dài đầu vào tối đa γ₂. Chúng tôi ký hiệu tỷ lệ nén là α. Phương pháp của chúng tôi sử dụng chiến lược chia-để-trị, chia văn bản dài thành các khối trong đó tổng độ dài được biểu diễn là L = l₁ + ··· + lₖ, và độ dài của mỗi khối, lᵢ, thỏa mãn điều kiện γ₁ ≤ lᵢ ≤ γ₂. Bởi kγ₁ ≤ L, chúng tôi có thể giới hạn độ phức tạp của mô-đun nén

∑ᵢ₌₁ᵏ lᵢ² ≤ ∑ᵢ₌₁ᵏ γ₂² = kγ₂² ≤ (γ₂²/γ₁)L. (1)

Độ phức tạp của việc suy luận ngữ cảnh được nén là

(∑ᵢ₌₁ᵏ αlᵢ)² = (α∑ᵢ₌₁ᵏ lᵢ)² = α²L². (2)

Do đó độ phức tạp chính của các thuật toán của chúng tôi có thể được giới hạn bởi (γ₂²/γ₁)L + α²L².

Kết quả cho thấy rằng thuật toán của chúng tôi có thể giảm độ phức tạp tính toán bằng một hệ số bằng bình phương của tỷ lệ nén trong giai đoạn suy luận. Mô-đun nén thể hiện tăng trưởng tuyến tính và có thể được xử lý song song.

--- TRANG 12 ---
Có thông tin quan trọng được ẩn trong rất nhiều văn bản không liên quan. Hãy tìm và ghi nhớ chúng. Tôi sẽ hỏi bạn về thông tin quan trọng đó. Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Bắt đầu nào. Đi và trở lại. (Lặp lại X Lần) Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Bắt đầu nào. Đi và trở lại. Mật khẩu là 0000 Hãy nhớ nó. 0000 là mật khẩu. Mặt trời màu vàng. Bắt đầu nào. Đi và trở lại (Lặp lại X Lần) Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Bắt đầu nào. Mật khẩu là gì? Mật khẩu là

Hình 6: Prompt truy vấn chứa mô tả tác vụ, thông tin dư thừa, thông tin passkey, thông tin dư thừa, và thông tin truy vấn. Thông tin passkey được đặt ngẫu nhiên trong văn bản, trong khi không gian còn lại lên đến độ dài chỉ định được điền với thông tin dư thừa.

¹ Có sẵn tại: https://huggingface.co/sshleifer/distilbart-cnn-12-6
