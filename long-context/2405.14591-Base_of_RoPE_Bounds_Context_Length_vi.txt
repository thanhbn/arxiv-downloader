# Base of RoPE Bounds Context Length
Xin Men∗
Baichuan Inc.Mingyu Xu∗
Baichuan Inc.Bingning Wang∗†
Baichuan Inc.
Qingyu Zhang
ISCASHongyu Lin
ISCASXianpei Han
ISCAS
Weipeng Chen
Baichuan Inc.

## Tóm tắt
Position embedding là một thành phần cốt lõi của các Mô hình Ngôn ngữ Lớn (LLMs) hiện tại. Rotary position embedding (RoPE), một kỹ thuật mã hóa thông tin vị trí bằng ma trận xoay, đã trở thành lựa chọn de facto cho position embedding trong nhiều LLMs, chẳng hạn như dòng Llama. RoPE đã được sử dụng thêm để mở rộng khả năng xử lý ngữ cảnh dài, dựa trên việc điều chỉnh tham số base của RoPE để giảm thiểu các vấn đề ngoài phân phối (OOD) trong position embedding. Tuy nhiên, trong bài báo này, chúng tôi phát hiện rằng LLMs có thể đạt được khả năng xử lý ngữ cảnh dài một cách hời hợt dựa trên lý thuyết OOD. Chúng tôi xem xét lại vai trò của RoPE trong LLMs và đề xuất một tính chất mới về sự suy giảm dài hạn, chúng tôi suy ra rằng base của RoPE giới hạn độ dài ngữ cảnh: có một giới hạn dưới tuyệt đối cho giá trị base để có được khả năng xử lý ngữ cảnh với độ dài nhất định. Công trình của chúng tôi tiết lộ mối quan hệ giữa độ dài ngữ cảnh và base của RoPE cả về mặt lý thuyết và thực nghiệm, có thể soi sáng cho việc huấn luyện ngữ cảnh dài trong tương lai.

![Hình 1: Độ dài ngữ cảnh và giới hạn dưới tương ứng của giá trị base của RoPE.]

∗Đóng góp ngang nhau
†Tác giả liên hệ, daniel@baichuan-inc.com

## 1 Giới thiệu
Trong vài năm qua, các mô hình ngôn ngữ lớn đã thể hiện những khả năng đáng ngạc nhiên và trải qua sự phát triển nhanh chóng. Đến nay, LLMs đã được ứng dụng rộng rãi trong nhiều lĩnh vực khác nhau, bao gồm chatbots, intelligent agents và code assistants (Achiam et al., 2023; Jiang et al., 2023b). Transformer (Vaswani et al., 2017), dựa trên cơ chế attention, đã trở thành backbone phổ biến nhất của LLMs do hiệu suất tốt và tính chất mở rộng (Tay et al., 2022). Một trong những module thành phần chính trong Transformer là position embedding, được đưa vào để nhúng thông tin vị trí rất quan trọng cho việc xử lý dữ liệu tuần tự. Rotary position embedding (RoPE), mã hóa thông tin khoảng cách tương đối dưới dạng absolute position embedding (Su et al., 2024), đã trở thành lựa chọn phổ biến và được áp dụng trong nhiều LLMs (Touvron et al., 2023a; Yang et al., 2023; Bai et al., 2023).

RoPE không đưa ra tham số huấn luyện nào và cho thấy cải thiện trong language modeling và nhiều tác vụ khác (Su et al., 2024; Heo et al., 2024). Một lý do khiến RoPE được sử dụng rộng rãi là khả năng ngoại suy độ dài ngữ cảnh (Peng et al., 2023b; Chen et al., 2023), mở rộng độ dài ngữ cảnh của LLM đã được huấn luyện mà không cần huấn luyện lại tốn kém. Trong thực tế, nhiều công trình (Touvron et al., 2023a; Liu et al., 2024a; Young et al., 2024) đã thành công trong việc mở rộng độ dài cửa sổ bằng cách đơn giản tăng giá trị base, tham số siêu duy nhất trong RoPE, và fine-tuning trên văn bản dài.

Những lý do đằng sau thành công của việc mở rộng ngữ cảnh dài này thường được giải thích là tránh các góc xoay ngoài phân phối (OOD) (Liu et al., 2024b; Han et al., 2023) trong RoPE, có nghĩa là độ dài ngữ cảnh mở rộng (OOD) có thể được ánh xạ đến độ dài ngữ cảnh trong phân phối đã được huấn luyện đúng cách. Dựa trên lý thuyết OOD, một nghiên cứu gần đây (Liu et al., 2024b) phát hiện rằng base nhỏ hơn có thể giảm thiểu OOD và có lợi cho khả năng xử lý ngữ cảnh dài của mô hình, điều này truyền cảm hứng cho chúng tôi nghiên cứu thêm về mối quan hệ giữa base của RoPE và độ dài ngữ cảnh mà mô hình có thể xử lý.

Trong bài báo này, chúng tôi phát hiện rằng mô hình có thể thể hiện khả năng xử lý ngữ cảnh dài một cách hời hợt với giá trị base RoPE không phù hợp, trong trường hợp này mô hình chỉ có thể duy trì perplexity thấp nhưng mất khả năng truy xuất thông tin ngữ cảnh dài. Chúng tôi cũng cho thấy rằng lý thuyết ngoài phân phối (OOD) trong position embedding, tạo động lực cho hầu hết các công trình ngoại suy độ dài (Peng et al., 2023b; Chen et al., 2023; Liu et al., 2024b), là không đủ để phản ánh đầy đủ khả năng xử lý ngữ cảnh dài của mô hình. Do đó, chúng tôi xem xét lại vai trò của RoPE trong LLMs và suy ra một tính chất mới về sự suy giảm dài hạn trong RoPE: khả năng tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên suy giảm khi khoảng cách tương đối tăng. Trong khi các công trình ngữ cảnh dài trước đây thường tập trung vào quy mô tương đối của base RoPE, dựa trên lý thuyết của chúng tôi, chúng tôi suy ra một giới hạn dưới tuyệt đối cho giá trị base của RoPE để có được khả năng xử lý ngữ cảnh với độ dài nhất định, như được thể hiện trong Hình 1. Để xác minh lý thuyết của chúng tôi, chúng tôi đã tiến hành các thí nghiệm kỹ lưỡng trên nhiều LLMs khác nhau như Llama2-7B (Touvron et al., 2023b), Baichuan2-7B (Yang et al., 2023) và một mô hình 2 tỷ tham số mà chúng tôi huấn luyện từ đầu, chứng minh rằng giới hạn dưới này không chỉ tồn tại trong giai đoạn fine-tuning mà còn trong giai đoạn pre-training.

Chúng tôi tóm tắt các đóng góp của bài báo như sau:
•Góc độ lý thuyết: chúng tôi suy ra một tính chất mới về sự suy giảm dài hạn trong RoPE, chỉ ra khả năng của mô hình tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên, đây là một góc nhìn mới để nghiên cứu khả năng xử lý ngữ cảnh dài của LLMs.
•Giới hạn dưới của Base RoPE: để đạt được khả năng xử lý ngữ cảnh với độ dài mong đợi, chúng tôi suy ra một giới hạn dưới tuyệt đối cho base của RoPE theo lý thuyết của chúng tôi. Nói ngắn gọn, base của RoPE giới hạn độ dài ngữ cảnh.
•Khả năng hời hợt: chúng tôi tiết lộ rằng nếu base của RoPE nhỏ hơn giới hạn dưới, mô hình có thể đạt được khả năng xử lý ngữ cảnh dài một cách hời hợt, có thể duy trì perplexity thấp nhưng mất khả năng truy xuất thông tin từ ngữ cảnh dài.

## 2 Nền tảng
Trong phần này, trước tiên chúng tôi giới thiệu Transformer và RoPE, được sử dụng phổ biến nhất trong các LLMs hiện tại. Sau đó chúng tôi thảo luận về các phương pháp ngữ cảnh dài dựa trên lý thuyết OOD của góc xoay.

### 2.1 Attention và RoPE
Các LLMs hiện tại chủ yếu dựa trên Transformer (Vaswani et al., 2017). Thành phần cốt lõi của nó là tính toán cơ chế attention. Attention thô có thể được viết như:

Aij=qTikj (1)
ATTN (X) =softmax (A/√d)v, (2)

trong đó A∈RL×L, q,k,v∈Rd. Position embedding được đưa vào để sử dụng thứ tự của chuỗi trong attention.

RoPE (Su et al., 2024) thực hiện relative position embedding thông qua absolute position embedding, áp dụng ma trận xoay vào tính toán điểm attention trong Eq. 1, có thể được viết như:

Aij= (Ri,θqi)T(Rj,θki) =qTiRj−i,θkj=qTiRm,θkj, (3)

trong đó m=j−i là khoảng cách tương đối của i và j, Rm,θ là ma trận xoay được ký hiệu như:

[Ma trận xoay lớn được hiển thị trong công thức (4)]

Nói chung, việc lựa chọn góc xoay thỏa mãn θi=base−2i/d, giá trị base điển hình cho các LLMs hiện tại là 10,000, và base của RoPE trong LLMs được thể hiện trong Bảng 1.

Bảng 1: Cài đặt base của RoPE và độ dài ngữ cảnh trong các LLMs khác nhau.
Model | Llama-7B | Llama2-7B | Llama3-8B | Mistral-7B-v0.2 | Baichuan2-7B
Base | 10,000 | 10,000 | 500,000 | 1,000,000 | 10,000
Length | 2,048 | 4,096 | 8,192 | 32,768 | 4,096

### 2.2 Lý thuyết OOD của góc xoay tương đối
Dựa trên RoPE, các nhà nghiên cứu đã đề xuất nhiều phương pháp khác nhau để mở rộng khả năng xử lý ngữ cảnh dài của LLMs, trong đó có các đại diện là PI (Chen et al., 2023) và dòng NTK (NTK-aware (bloc97, 2023), YaRN (Peng et al., 2023b), và Dynamical-NTK (emozilla, 2023)). Những phương pháp này phụ thuộc vào tỷ lệ tương đối s=Tnew/Torigin, trong đó Torigin là độ dài huấn luyện của mô hình pre-trained gốc và Tnew là độ dài huấn luyện trong fine-tuning ngữ cảnh dài.

PI PI trực tiếp nội suy position embedding, và tính toán của Aij trở thành:

Aij= (Ri/sqi)T(Rj/ski) =qTiR(j−i)/skj=qTiRm/skj, (5)

Nói cách khác, position embedding của token tại vị trí i trong pre-training trở thành i/s trong fine-tuning, đảm bảo phạm vi position embedding của ngữ cảnh dài hơn vẫn giống như trước.

Dòng NTK Ý tưởng là mạng neural khó học các đặc trưng tần số cao, và nội suy trực tiếp có thể ảnh hưởng đến các phần tần số cao. Do đó, phương pháp NTK-aware đạt được ngoại suy tần số cao và nội suy tần số thấp bằng cách sửa đổi giá trị base của RoPE. Cụ thể, nó sửa đổi base b của RoPE thành:

bnew=b sd/(d−2). (6)

Việc suy ra biểu thức này được suy ra từ Tnewb−(d−2)/dnew =Torigin b−(d−2)/d để đảm bảo rằng phần tần số thấp nhất được nội suy.

Một nghiên cứu gần đây (Liu et al., 2024b) đề xuất đặt base nhỏ hơn nhiều (ví dụ 500), trong trường hợp này θi=base−2i/d đủ nhỏ và độ dài huấn luyện điển hình (ví dụ 4,096) bao phủ hoàn toàn chu kỳ của cos (t−s)θi, vì vậy mô hình có thể đạt được khả năng xử lý ngữ cảnh dài hơn.

Một góc nhìn để giải thích các phương pháp ngoại suy hiện tại là OOD của góc xoay (Liu et al., 2024b; Han et al., 2023). Nếu tất cả các giá trị có thể của cos(t−s)θi đã được fit trong giai đoạn pre-training, OOD sẽ được tránh khi xử lý ngữ cảnh dài hơn. Hình 2 minh họa cách các phương pháp này tránh OOD của RoPE.

## 3 Động lực
Các phương pháp dựa trên NTK được áp dụng rộng rãi trong việc mở rộng ngữ cảnh dài (Touvron et al., 2023a; Liu et al., 2024a; Young et al., 2024). Tuy nhiên, để có được khả năng xử lý ngữ cảnh dài tốt hơn, các practitioner thường áp dụng base lớn hơn nhiều so với phương pháp NTK-aware gốc đề xuất. Điều này dẫn đến suy đoán rằng có một giới hạn khác của base RoPE được xác định bởi độ dài ngữ cảnh.

Mặt khác, một công trình gần đây (Liu et al., 2024b) đề xuất đặt base nhỏ hơn nhiều cho RoPE để mở rộng độ dài ngữ cảnh. Tuy nhiên, chúng tôi phát hiện đó có thể là khả năng xử lý ngữ cảnh dài một cách hời hợt như được thể hiện trong Hình 3. Phương pháp này có thể đạt được perplexity thấp ngay cả ở độ dài ngữ cảnh 128k, có thể được giải thích bằng lý thuyết OOD như đã giải thích ở trên, nhưng mô hình không thể truy xuất thông tin liên quan cho độ dài ngữ cảnh ngắn như 1k, thậm chí ngắn hơn nhiều so với độ dài pre-trained của mô hình. Phát hiện của chúng tôi hỗ trợ nghiên cứu trước đây (Hu et al., 2024) về những hạn chế của perplexity trong việc đánh giá khả năng xử lý ngữ cảnh dài. Để đi sâu vào hiện tượng này, chúng tôi thực hiện khám phá lý thuyết trong phần tiếp theo.

## 4 Góc nhìn lý thuyết
Đối với cơ chế attention trong language modeling, chúng ta có các yêu cầu sau:

Yêu cầu 1 Token gần hơn được attention nhiều hơn: token hiện tại có xu hướng tập trung attention nhiều hơn vào token có khoảng cách tương đối nhỏ hơn.

Yêu cầu 2 Token tương tự được attention nhiều hơn: token có xu hướng tập trung attention nhiều hơn vào token có giá trị key tương tự hơn với giá trị query của token hiện tại.

Sau đó chúng tôi kiểm tra các yêu cầu khi chúng ta áp dụng RoPE vào cơ chế attention trong LLMs.

### 4.1 Suy giảm dài hạn của giới hạn trên của điểm attention
Đối với Yêu cầu 1, tính chất của RoPE làm cho mô hình tập trung attention nhiều hơn vào các token gần hơn. Loại suy giảm dài hạn này đã được thảo luận kỹ lưỡng trong công trình trước đây (Su et al., 2024; Sun et al., 2022). Nó xuất phát từ giới hạn trên của tính toán điểm attention, có thể được viết như:

|Aij|=|qTiRmkj| ≤maxl(|hl−hl+1|)Σn=1d/2|Sn|
=maxl(|hl−hl+1|)Σn=1d/2|Σl=0n−1e(j−i)θl√−1|, (7)

trong đó hl=qTi[2l:l2+1]kj[2l:2l+1]. Phương trình 7 chỉ ra rằng giới hạn trên của điểm attention |Aij| suy giảm khi khoảng cách tương đối tăng. Hình 4 thể hiện đường cong suy giảm dài hạn của giới hạn trên này, phù hợp với các phát hiện trước đây (Su et al., 2024; Sun et al., 2022).

### 4.2 Suy giảm dài hạn của khả năng tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên
Ngoài giới hạn trên của điểm attention, chúng tôi cũng phát hiện tồn tại một tính chất suy giảm dài hạn khác trong RoPE: khả năng tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên suy giảm khi khoảng cách tương đối tăng. Chúng tôi định nghĩa khả năng tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên như:

Eq,k∗⟨qTRm,θk∗⟩−Eq,k⟨qTRm,θk⟩, (8)

trong đó q∈Rd là vector query cho token hiện tại, k∗=q+ϵ là giá trị key của token tương tự, trong đó ϵ là biến ngẫu nhiên nhỏ, k∈Rd là vector key của token ngẫu nhiên, Rm,θ là ma trận xoay trong RoPE. Số hạng đầu tiên trong Eq. 8 là điểm attention của q và token tương tự k∗, số hạng thứ hai trong Eq. 8 là điểm attention của q và token ngẫu nhiên k. Sau đó chúng tôi suy ra định lý sau:

Định lý 1 Giả sử rằng các thành phần của query q∈Rd và key k∈Rd độc lập và phân phối đồng nhất, độ lệch chuẩn của chúng được ký hiệu là σ∈R. Key k∗=q+ϵ là token tương tự với query, trong đó ϵ là biến ngẫu nhiên có trung bình bằng 0. Khi đó chúng ta có:

1/(2σ2)(Eq,k∗⟨qTRm,θk∗⟩−Eq,k⟨qTRm,θk⟩) =Σi=0d/2−1cos(mθi) (9)

Chứng minh được trình bày trong Phụ lục A. Chúng tôi ký hiệu Σi=0d/2−1cos(mθi) là Bm,θ, và theo Định lý 1, Bm,θ đo lường khả năng tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên, giảm khi khoảng cách tương đối m tăng, như được thể hiện trong Hình 5. Đối với giá trị base rất nhỏ, chúng ta có thể quan sát thấy Bm,θ thậm chí dưới không ở một khoảng cách nhất định, có nghĩa là các token ngẫu nhiên có điểm attention lớn hơn so với các token tương tự, điều này có thể có vấn đề đối với việc mô hình hóa ngữ cảnh dài.

### 4.3 Base của RoPE giới hạn độ dài ngữ cảnh
Để thỏa mãn Yêu cầu 2, chúng ta sẽ có Eq,k∗⟨qTRm,θk∗⟩≥Eq,k⟨qTRm,θk⟩. Theo Định lý 1, Bm,θ cần lớn hơn không. Cho θ trong RoPE, độ dài ngữ cảnh Lθ có thể thực sự đạt được thỏa mãn:

Lθ=sup{L|Bm,θ≥0,∀m∈[0, 1, ..., L]} (10)

Nói cách khác, nếu chúng ta tuân theo cài đặt θi=base−2i/d, để có được độ dài ngữ cảnh mong đợi L, có một giới hạn dưới của giá trị base baseL:

baseL=inf{base|Bm,θ≥0,∀m∈[0, 1, ..., L]} (11)

Tóm lại, base của RoPE xác định giới hạn trên của độ dài ngữ cảnh mà mô hình có thể thực sự đạt được. Mặc dù tồn tại giới hạn dưới tuyệt đối, Eq. 9 và Eq. 11 khó có được nghiệm dạng đóng vì Bm,θ là tổng của nhiều hàm cosine. Do đó, trong bài báo này, chúng tôi có được nghiệm số. Bảng 2 thể hiện giới hạn dưới này cho độ dài ngữ cảnh từ 1,000 đến một triệu. Trong Hình 1, chúng tôi vẽ đồ thị độ dài ngữ cảnh và giới hạn dưới tương ứng, chúng ta có thể quan sát thấy khi độ dài ngữ cảnh tăng, base yêu cầu cũng tăng.

Lưu ý: ranh giới này không rất nghiêm ngặt vì việc xếp chồng các lớp trong LLMs cho phép mô hình trích xuất thông tin vượt ra ngoài phạm vi của từng lớp đơn lẻ, điều này có thể tăng độ dài ngữ cảnh trong Eq. 10 và giảm base trong Eq. 11. Dù vậy, trong Phần 5 chúng tôi phát hiện rằng giới hạn suy ra xấp xỉ độ dài ngữ cảnh thực tế trong thực tế.

Suy giảm dài hạn từ các góc độ khác nhau. Suy giảm dài hạn trong phần 4.1 và phần 4.2 là từ các góc độ khác nhau. Cái trước đề cập đến suy giảm dài hạn của điểm attention khi khoảng cách tương đối tăng. Điều này đảm bảo rằng các token hiện tại có xu hướng tập trung attention nhiều hơn vào các token gần với chúng. Cái sau chỉ ra rằng với việc đưa vào ma trận xoay trong attention, khả năng phân biệt các token liên quan từ các token không liên quan giảm khi khoảng cách tương đối tăng. Do đó, Bm,θ lớn, tương ứng với giá trị base lớn, quan trọng để giữ khả năng phân biệt của mô hình trong việc mô hình hóa ngữ cảnh dài.

Bảng 2: Độ dài ngữ cảnh và giới hạn dưới tương ứng của base RoPE.
Context Len. | 1k | 2k | 4k | 8k | 16k | 32k | 64k | 128k | 256k | 512k | 1M
Lower Bound | 4.3e3 | 1.6e4 | 2.7e4 | 8.4e4 | 3.1e5 | 6.4e5 | 2.1e6 | 7.8e6 | 3.6e7 | 6.4e7 | 5.1e8

## 5 Thí nghiệm
Trong phần này, chúng tôi tiến hành các thí nghiệm kỹ lưỡng. Kết quả thực nghiệm có thể được tóm tắt trong Bảng 3, chi tiết trong các phần sau.

Bảng 3: Trong Phần 5, chúng tôi nhằm trả lời những câu hỏi sau.
Câu hỏi | Trả lời
Q: Base của RoPE có giới hạn độ dài ngữ cảnh trong giai đoạn fine-tuning không? | Có. Khi base nhỏ, khó có được ngoại suy cho độ dài ngữ cảnh cụ thể.
Q: Base của RoPE có giới hạn độ dài ngữ cảnh trong giai đoạn pre-training không? | Có. Giới hạn dưới đề xuất của chúng tôi cho base của RoPE cũng áp dụng cho pre-training. Nếu chúng ta huấn luyện mô hình từ đầu với base nhỏ nhưng độ dài ngữ cảnh lớn (lớn hơn độ dài bị giới hạn), mô hình kết quả có khả năng xử lý độ dài ngữ cảnh rất hạn chế, có nghĩa là một số ngữ cảnh trong pre-training bị lãng phí.
Q: Điều gì xảy ra khi base được đặt nhỏ hơn giới hạn dưới? | Mô hình sẽ có được khả năng xử lý ngữ cảnh dài một cách hời hợt. Mô hình có thể giữ perplexity thấp, nhưng không thể truy xuất thông tin hữu ích từ ngữ cảnh dài.

### 5.1 Thiết lập thí nghiệm
Đối với fine-tuning, chúng tôi sử dụng Llama2-7B (Touvron et al., 2023a) và Baichuan2-7B (Yang et al., 2023), cả hai đều là các mô hình open-source phổ biến sử dụng RoPE với base là 1e4. Chúng tôi sử dụng learning rate cố định là 2e-5 và global batch size là 128 và fine-tuning trong 1000 bước. Đối với pre-training, chúng tôi huấn luyện mô hình 2B giống Llama từ đầu với tổng cộng 1 nghìn tỷ token. Chúng tôi đặt learning rate là 1e-4 và áp dụng lịch trình cosine decay, với các mô hình được huấn luyện trên tổng cộng 1T token. Bộ dữ liệu chúng tôi sử dụng là một tập con của RedPajama (Computer, 2023). Thêm chi tiết về thiết lập thí nghiệm được cung cấp trong Phụ lục B.

Đánh giá của chúng tôi tập trung vào hai khía cạnh: (1) Perplexity: chúng tôi sử dụng bộ dữ liệu PG19 (Rae et al., 2019) thường được sử dụng trong đánh giá ngữ cảnh dài; (2) Retrieval: ngoài perplexity, chúng tôi cũng áp dụng retrieval vì nó đại diện cho khả năng hiểu ngữ cảnh dài thực tế của LLMs. Chúng tôi chọn a) benchmark Long-eval từ (Li* et al., 2023) và b) needle in a haystack (NIH) (G, 2023). Benchmark Long-eval tạo ra nhiều câu tương tự ngẫu nhiên và yêu cầu mô hình trả lời câu hỏi dựa trên một câu cụ thể trong ngữ cảnh, trong khi NIH yêu cầu mô hình truy xuất thông tin từ các vị trí khác nhau trong ngữ cảnh dài.

### 5.2 Base của RoPE giới hạn độ dài ngữ cảnh trong giai đoạn fine-tuning
Theo Eq. 11, có một giới hạn dưới của base RoPE được xác định bởi độ dài ngữ cảnh mong đợi. Chúng tôi fine-tune Llama2-7b-Base trên ngữ cảnh 32k với các base khác nhau. Như được mô tả trong Hình 6, mặc dù sự khác biệt về perplexity giữa các base khác nhau là không đáng kể, độ chính xác của Long-eval thay đổi đáng kể. Trong Hình 6b, đường chấm chấm biểu thị giới hạn dưới suy ra từ Eq. 11, dưới đó độ chính xác Long-eval giảm đáng kể.

Kết quả bổ sung được cung cấp trong Phụ lục C. Đáng chú ý, giới hạn dưới quan sát thực nghiệm này phù hợp chặt chẽ với suy dẫn lý thuyết của chúng tôi. Mặt khác, chúng ta có thể thấy rằng base=2e5 đạt được perplexity tốt nhất, nhưng độ chính xác của Long-eval rất thấp, điều này cho thấy những hạn chế của perplexity trong việc đánh giá khả năng xử lý ngữ cảnh dài.

### 5.3 Base của RoPE giới hạn độ dài ngữ cảnh trong giai đoạn pre-training
Theo Định lý 1 và Eq. 11, ràng buộc này cũng có thể áp dụng cho giai đoạn pre-training. Để xác thực điều này, chúng tôi huấn luyện một mô hình 2B từ đầu với base RoPE=100. Kết quả, được mô tả trong hàng đầu tiên của Hình 7, cho thấy rằng mặc dù mô hình được huấn luyện với độ dài ngữ cảnh 4,096 token, nó chỉ có khả năng truy xuất thông tin từ khoảng 500 token gần nhất. Điều này chứng minh rằng tham số base giới hạn độ dài ngữ cảnh trong giai đoạn pre-training. Chúng tôi định nghĩa độ dài ngữ cảnh mà mô hình có thể truy xuất thông tin hiệu quả là độ dài ngữ cảnh hiệu quả.

Và theo lý thuyết của chúng tôi, độ dài ngữ cảnh hiệu quả có thể được mở rộng khi base của RoPE tăng. Để xác thực điều này, chúng tôi tiến hành fine-tune thêm mô hình 2B này trên độ dài ngữ cảnh 32k, với base của RoPE được đặt là 1e4, như được thể hiện trong hàng thứ hai của Hình 7. Trong khi độ dài ngữ cảnh hiệu quả tăng, nó vẫn đáng kể dưới 32k vì độ dài ngữ cảnh hiệu quả bị giới hạn bởi base=1e4 nhỏ hơn nhiều so với 32k. Hơn nữa, khi chúng tôi tăng base lên 1e6 và fine-tune mô hình 2B cơ sở trên 32K (hàng thứ ba trong Hình 7), mô hình có thể đạt được độ dài ngữ cảnh lớn hơn so với base=1e4, phù hợp với lý thuyết của chúng tôi.

Để loại bỏ thêm ảnh hưởng của kích thước mô hình, chúng tôi cũng fine-tune một mô hình 7B lớn hơn trên độ dài ngữ cảnh 32k với base RoPE được đặt là 1e4 và quan sát độ dài ngữ cảnh hiệu quả gần như giống hệt với mô hình 2B có cùng base RoPE (xem Phụ lục D). Đây là bằng chứng thực nghiệm rằng độ dài ngữ cảnh hiệu quả được xác định bởi base của RoPE.

### 5.4 Giải thích cho khả năng xử lý ngữ cảnh dài hời hợt với base nhỏ
Dựa trên lý thuyết và quan sát thực nghiệm của chúng tôi, dễ dàng giải thích điều gì xảy ra trong Hình 3.

Ngoại suy tốt hơn (Perplexity)? Do base nhỏ, Bm,θ có thể nhỏ hơn không khi m tăng, được thể hiện trong Hình 5. Mô hình không thể tập trung attention nhiều hơn vào các token tương tự so với các token ngẫu nhiên với khoảng cách tương đối lớn, vì vậy mô hình có xu hướng tập trung nhiều hơn vào các token gần đó, điều này sẽ dẫn đến trường tiếp nhận thực nghiệm nhỏ hơn, thậm chí nhỏ hơn độ dài huấn luyện. Trong trường hợp này, mô hình có khả năng mạnh mẽ để duy trì ổn định perplexity (Chi et al., 2023).

Khả năng tệ hơn (Long-eval và NIH)! Theo phân tích trước đây của chúng tôi, base của RoPE giới hạn độ dài ngữ cảnh, và độ dài ngữ cảnh bị giới hạn bởi 500 thấp hơn nhiều so với giới hạn bởi 10,000. Do đó, khi base được đặt là 500, độ dài ngữ cảnh hiệu quả giảm mạnh, ngay cả sau khi huấn luyện trên độ dài ngữ cảnh 32k.

### 5.5 Lý thuyết OOD không đủ để tiết lộ khả năng xử lý ngữ cảnh dài
Bảng 4: So sánh "Phương pháp 1" và "Phương pháp 2". Những phương pháp này được thiết kế cẩn thận. Cả hai đều không có OOD, nhưng chúng rất khác nhau dưới lý thuyết của chúng tôi.
Phương pháp | OOD | Long-eval | số lượng m có Bm,θ≥0
| | 15k | 30k | 15k | 30k
Phương pháp 1 | 0.33 | 0.27 | 0 | 0
Phương pháp 2 | 0.40 | 0.00 | 97 | 2554

Phần 3 đề cập rằng các phương pháp dựa trên lý thuyết OOD của góc xoay có thể không phản ánh đầy đủ khả năng xử lý ngữ cảnh dài. Trong phần này, chúng tôi tiến hành thêm các thí nghiệm để chứng thực và giải thích quan sát này. Chúng tôi trình bày hai phương pháp để mở rộng độ dài ngữ cảnh của Llama2 từ 4k đến 32k. Cả hai đều không có góc OOD. Những phương pháp này được mô tả toán học như sau:

• Phương pháp 1: θi= (5e6)−2i/d,
• Phương pháp 2: θi={(1e4)−2i/128/8, i≥44; (1e4∗8128/88)−2i/128,i<44.

Chúng ta có thể thấy từ Bảng 4 rằng hai phương pháp này thể hiện khả năng xử lý ngữ cảnh dài khác nhau đáng kể. Dưới góc nhìn của góc xoay OOD, cả hai phương pháp đều tránh góc xoay OOD, cho thấy ngoại suy hiệu quả. Tuy nhiên, mặc dù được huấn luyện trên độ dài ngữ cảnh 32k, "phương pháp 2" gặp khó khăn trong việc hoàn thành tác vụ truy xuất ở độ dài ngữ cảnh 32k. Hiện tượng này vượt ra ngoài phạm vi mà lý thuyết OOD có thể giải thích. Dưới góc nhìn của chúng tôi, "phương pháp 2" vi phạm nghiêm trọng Bm,θ≥0 khi m∈[15k, 30k], do đó cản trở khả năng đạt được sự phân biệt ngữ cảnh dài. Chúng tôi suy đoán rằng mô hình có thể đạt được ngoại suy tốt hơn trong giai đoạn fine-tuning nếu base đủ lớn để vượt qua giới hạn dưới và tránh OOD của góc xoay.

## 6 Công trình liên quan
Position embedding. Kể từ khi được giới thiệu, Transformer (Vaswani et al., 2017) đã đạt được kết quả đáng kể trong lĩnh vực xử lý ngôn ngữ tự nhiên. Để tận dụng đầy đủ thứ tự của chuỗi, các nhà nghiên cứu đã đưa ra position embedding. Position embedding sớm nhất dựa trên hàm sinusoidal (Vaswani et al., 2017) cho vị trí tuyệt đối, learnable absolute position embedding (Devlin et al., 2018) và nhiều biến thể (Kiyono et al., 2021; Li et al., 2019) đã được đề xuất. Tuy nhiên, absolute position embedding gặp khó khăn trong việc mở rộng trực tiếp sang văn bản dài hơn độ dài huấn luyện. Sau đó, các nhà nghiên cứu đã đề xuất các phương pháp relative position embedding (Shaw et al., 2018; Ke et al., 2020). Với sự phát triển của các mô hình ngôn ngữ lớn, rotary position embedding và các biến thể của nó (Su et al., 2024; Sun et al., 2022) đã trở nên được sử dụng rộng rãi, chẳng hạn như Llama2 (Touvron et al., 2023a), Baichuan2 (Yang et al., 2023), Mistral-7B-(Jiang et al., 2023a). Một nghiên cứu gần đây tiết lộ rằng không có position embedding cũng có tiềm năng (Kazemnejad et al., 2024).

Long context learning. Thực hiện các mô hình với ngữ cảnh dài hơn hoặc thậm chí vô hạn luôn là mục tiêu quan trọng trong lĩnh vực xử lý ngôn ngữ tự nhiên. Do độ phức tạp bình phương của mô hình transformer theo thời gian, một phần đáng kể công việc tập trung vào cải thiện cấu trúc mô hình (Gu & Dao, 2023;?; Peng et al., 2023a; Qin et al., 2024). Tuy nhiên, hầu hết công việc vẫn dựa trên kiến trúc transformer. Phần khác của công việc nhằm giảm độ phức tạp tính toán của chính attention, chẳng hạn như sparse attention (Beltagy et al., 2020) và group query attention (Ainslie et al., 2023). Ngoài ra, cũng có một số tối ưu hóa trong hiệu quả kỹ thuật, chẳng hạn như flash attention (Dao et al., 2022) và ring attention (Liu et al., 2023). Trong giai đoạn suy luận mô hình, để tiết kiệm thời gian và không gian, cũng có một số phương pháp để tăng tốc ngữ cảnh dài, chẳng hạn như nén KV cache (Hooper et al., 2024), v.v. Và position embedding quan trọng trong ngoại suy. Trong quá trình fine-tuning, các phương pháp như PI (Chen et al., 2023), NTK, và YARN (Peng et al., 2023b) được sử dụng để thay đổi thông tin position embedding gốc. FoT (Tworkowski et al., 2024) gán thông tin vị trí của các token bên ngoài ngữ cảnh cục bộ là token đầu tiên trong ngữ cảnh cục bộ.

## 7 Hạn chế
Trong công trình này, chúng tôi điều tra mối quan hệ giữa base của RoPE và độ dài ngữ cảnh. Mặc dù chúng tôi đã suy ra rằng tồn tại giới hạn dưới cho base của RoPE được xác định bởi độ dài ngữ cảnh, sự tồn tại của giới hạn trên cho base của RoPE vẫn là một câu hỏi mở cần khám phá thêm. Ngoài ra, do thiếu các benchmark hiệu quả để đánh giá khả năng xử lý ngữ cảnh dài, phạm vi khả năng xử lý ngữ cảnh dài được thảo luận trong bài báo này có thể bị hạn chế.

## 8 Kết luận
Công trình của chúng tôi trình bày một nghiên cứu toàn diện về vai trò của RoPE trong LLMs để mô hình hóa ngữ cảnh dài một cách hiệu quả. Đóng góp chính của chúng tôi nằm ở việc khám phá một tính chất mới của RoPE thông qua phân tích lý thuyết, chứng minh rằng khi khoảng cách tương đối giữa các token tăng, khả năng của mô hình tập trung attention nhiều hơn vào các token tương tự giảm. Theo lý thuyết của chúng tôi, chúng tôi suy ra giới hạn dưới cho base của RoPE để phù hợp với độ dài ngữ cảnh mong đợi. Kết quả thí nghiệm của chúng tôi xác thực rằng base của RoPE giới hạn độ dài ngữ cảnh không chỉ cho fine-tuning mà còn cho giai đoạn pre-training. Lý thuyết của chúng tôi cung cấp góc nhìn mới về hiểu chức năng của RoPE trong việc mô hình hóa ngữ cảnh dài. Bằng cách soi sáng mối quan hệ giữa độ dài ngữ cảnh và position embedding, chúng tôi hy vọng công trình của chúng tôi có thể cung cấp thông tin chi tiết để nâng cao khả năng xử lý ngữ cảnh dài của LLMs.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc]

## A Chứng minh Định lý 1.
[Nội dung chứng minh được giữ nguyên như bản gốc]

## B Chi tiết thiết lập thí nghiệm.
[Nội dung phụ lục được giữ nguyên như bản gốc]

## C Baichuan2-7B-Base: Giới hạn dưới Base của RoPE
[Nội dung phụ lục được giữ nguyên như bản gốc]

## D Kết quả kiểm tra ngữ cảnh dài trên các LLMs khác nhau
[Nội dung phụ lục được giữ nguyên như bản gốc]
