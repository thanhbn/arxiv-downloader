# LONGHEADS: Multi-Head Attention is Secretly a Long Context Processor

Yi Lu1*, Xin Zhou1*, Wei He1, Jun Zhao1,
Tao Ji1†, Tao Gui2†, Qi Zhang1†, Xuanjing Huang1,3
1School of Computer Science, Fudan University, Shanghai, China
2Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China
3International Human Phenome Institutes, Shanghai, China
yilu23@m.fudan.edu.cn, {xzhou20,taoji,tgui,qz}@fudan.edu.cn

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã đạt được hiệu suất ấn tượng trong nhiều lĩnh vực nhưng thường gặp khó khăn trong việc xử lý các đầu vào dài một cách hiệu quả và hiệu quả do khả năng tổng quát hóa độ dài hạn chế và yêu cầu tính toán bậc hai của attention. Nhiều người đã tìm cách giảm thiểu điều này bằng cách hạn chế cửa sổ attention trong độ dài pre-trained. Tuy nhiên, các phương pháp này tạo ra các vấn đề mới như bỏ qua ngữ cảnh ở giữa và yêu cầu huấn luyện bổ sung. Để giải quyết những vấn đề này, chúng tôi đề xuất LONGHEADS, một framework không cần huấn luyện nhằm tăng cường khả năng ngữ cảnh dài của LLM bằng cách khai thác tiềm năng chưa được sử dụng của multi-head attention. Thay vì cho phép mỗi head chú ý đến toàn bộ câu, điều này gặp khó khăn trong việc tổng quát hóa sang các chuỗi dài hơn do vấn đề out-of-distribution (OOD), chúng tôi cho phép mỗi head xử lý độ dài in-distribution bằng cách chọn và chú ý đến các chunk ngữ cảnh quan trọng. Để đạt được điều này, chúng tôi đề xuất một chiến lược chọn chunk dựa vào sự tương quan vốn có giữa các biểu diễn query và key, phân phối hiệu quả các chunk ngữ cảnh đến các head khác nhau. Theo cách này, mỗi head đảm bảo rằng nó có thể xử lý hiệu quả các token được chú ý trong độ dài đã huấn luyện, trong khi các head khác nhau trong các lớp khác nhau có thể cùng nhau xử lý các ngữ cảnh dài hơn. LONGHEADS hoạt động hiệu quả trong thời gian tuyến tính, phù hợp liền mạch với nhiều LLM sử dụng relative positional encoding. LONGHEADS đạt được độ chính xác 100% ở độ dài 128k trên task passkey retrieval, xác minh hiệu quả của LONGHEADS trong việc mở rộng cửa sổ ngữ cảnh có thể sử dụng cho các mô hình hiện có. Chúng tôi phát hành code tại https://github.com/LuLuLuyi/LongHeads.

## 1 Giới thiệu
LLM thường được yêu cầu xử lý các task với ngữ cảnh dài, chẳng hạn như in-context learning (Dong et al., 2023), tool learning (Qin et al., 2023), và retrieval-augmented generation (Gao et al., 2024). Tuy nhiên, việc cho phép LLM xử lý ngữ cảnh dài đặt ra những thách thức đáng kể. Vấn đề OOD khiến LLM gặp khó khăn trong việc xử lý các token vượt quá độ dài pre-trained, và độ phức tạp bậc hai của attention tạo ra chi phí huấn luyện và suy luận đáng kể. Mặc dù vấn đề OOD có thể được giải quyết bằng zero-shot learning (Jin et al., 2024), fine-tuning (Chen et al., 2023a; Peng et al., 2023), hoặc re-training (Sun et al., 2022; Press et al., 2022), bộ nhớ và tính toán cần thiết vẫn tăng theo cấp số nhân với độ dài ngữ cảnh, như được thể hiện trong Hình 1(a).

Để giảm thiểu những vấn đề này, các công trình gần đây hạn chế cửa sổ attention đến độ dài pre-trained, điều này giảm chi phí tính toán và tránh việc xử lý các token OOD. Một hướng là loại trừ các token xa (ngoại trừ một vài token ban đầu, Han et al., 2023; Xiao et al., 2023) để hạn chế cửa sổ attention trong phân phối, như được thể hiện trong Hình 1(b). Tuy nhiên, những phương pháp này có thể dẫn đến mất thông tin quan trọng, làm giảm hiệu suất trên các task downstream. Cách khác để hạn chế cửa sổ attention là truy xuất các chunk của chuỗi dài (Mohtashami và Jaggi, 2023; Zhang et al., 2024), nhưng những phương pháp này thường yêu cầu các thao tác đặc biệt và fine-tuning liên tục, điều này khiến các LLM hiện có khó có thể áp dụng trực tiếp cho các chuỗi dài. Tóm lại, việc cải thiện khả năng của LLM để xử lý ngữ cảnh dài với chi phí thấp vẫn là một thách thức.

Trong bài báo này, chúng tôi đề xuất LONGHEADS, một framework mới để tăng cường khả năng ngữ cảnh dài của LLM mà không cần huấn luyện bổ sung. Ý tưởng chính là khai thác đầy đủ tiềm năng của multi-head attention. Chúng tôi đầu tiên sử dụng bản chất của các head khác nhau tập trung vào các không gian con khác nhau của ngữ cảnh, và mỗi head có thể xử lý hiệu quả các chuỗi trong độ dài pre-training. Như được thể hiện trong Hình 2(c), chúng tôi giới hạn mỗi head chọn và chú ý đến các chunk ngữ cảnh quan trọng trong độ dài pre-trained, thay vì để mỗi head chú ý đến toàn bộ câu, do đó tránh được vấn đề OOD. Hơn nữa, chúng tôi tận dụng dot-product attention vốn có của mô hình và đề xuất một chiến lược chọn chunk để tìm các chunk quan trọng cho mỗi head. Lấy cảm hứng từ việc mỗi head gán các trọng số attention khác nhau cho các token dựa trên sự tương quan vốn có giữa các biểu diễn query và key, chúng tôi chia đầu vào thành các chunk và tạo các đặc trưng cấp chunk cho mỗi block. Nó sử dụng sự tương quan cấp token tự nhiên để xây dựng các biểu diễn query và key cấp chunk, cho phép mỗi head sử dụng các khả năng hiện có của nó (dot-product attention) để chọn các chunk dựa trên các trọng số attention. Theo cách này, mỗi head xử lý hiệu quả các chunk ngữ cảnh đã chọn trong độ dài đã huấn luyện, và tất cả các head trong tất cả các lớp làm việc cùng nhau để xử lý các ngữ cảnh dài hơn. Đồng thời, tất cả các thao tác đều dựa trên các khả năng nội tại của multi-head attention, cho phép LONGHEADS tăng cường LLM mà không cần huấn luyện bổ sung.

Để đánh giá hiệu quả của LONGHEADS, chúng tôi sử dụng LLaMA-2-7B-Base và LLaMA-2-7B-Chat làm mô hình cơ sở và đánh giá trên modeling ngôn ngữ, task truy xuất tổng hợp và benchmark ngữ cảnh dài. LONGHEADS đạt được gần 100% độ chính xác trên các độ dài ngữ cảnh từ 4k đến 32k trên task Passkey Retrieval. Trên LongBench, LONGHEADS đạt được hiệu suất state-of-the-art (SOTA) trong số các phương pháp restricted attention. So với các phương pháp full attention, LONGHEADS đạt được hiệu suất tương đương trên độ dài test 16K và hiệu suất tốt nhất trên độ dài test 32K trong khi tận hưởng chi phí tính toán tuyến tính. Các kết quả thực nghiệm chứng minh rằng LONGHEADS cho phép các LLM tổng quát hóa trực tiếp sang các chuỗi dài hơn và đạt được hiệu suất tương đương hoặc thậm chí vượt trội so với các phương pháp yêu cầu fine-tuning liên tục.

Các đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi đề xuất LONGHEADS, một framework suy luận không cần huấn luyện tận dụng các thuộc tính cấu trúc của các attention head để xử lý các chuỗi dài một cách hiệu quả và hiệu suất.
• Chúng tôi thiết kế một chiến lược chọn chunk đơn giản nhưng hiệu quả có thể chọn chính xác các chunk hữu ích và bao phủ toàn bộ ngữ cảnh.
• Các thí nghiệm chứng minh rằng LONGHEADS là một bộ xử lý ngữ cảnh dài dựa trên restricted-attention SOTA và hoạt động hiệu quả trong thời gian tuyến tính, cũng với hiệu suất tương đương với các phương pháp full-attention.

## 2 Phương pháp

Trong phần này, chúng tôi mô tả cách LONGHEADS sử dụng khả năng vốn có của multi-head attention để mã hóa và tạo ra các chuỗi dài mà không cần huấn luyện bổ sung.

### 2.1 Tổng quan

Một tổng quan về LONGHEADS được thể hiện trong Hình 2. Chúng tôi chia văn bản thành các chunk và tính toán các biểu diễn chunk cho mỗi chunk. Khi tạo ra token x14, chúng tôi chọn k chunk có liên quan dựa trên vector query của token hiện tại và các biểu diễn chunk. Theo cách này, mỗi attention head của LONGHEADS chọn lọc tập trung vào các chunk văn bản khác nhau theo sở thích của nó. Các token của các chunk được chú ý sau đó được tái cấu trúc, đảm bảo causal attention tiếp theo luôn được thực hiện trong độ dài pre-trained.

Khi mã hóa hoặc tạo ra một token out-of-length, một mạng chọn chunk không có tham số chọn k chunk có liên quan dựa trên vector query hiện tại và các biểu diễn chunk. Các chunk không được chọn có thể được xấp xỉ như có điểm attention bằng không (điều này thường đúng dưới tính thưa thớt của cơ chế attention), và không cần được tính toán. Điều này cho phép ma trận attention không tăng theo độ dài, giảm đáng kể chi phí bộ nhớ và tính toán của ngữ cảnh dài từ O(N²) xuống O(N). Các công trình khác hạn chế phạm vi attention chỉ đơn giản bỏ qua các token xa ngoài một vài token ban đầu, ngay cả khi chúng chứa thông tin đáng được chú ý.

Để chọn chính xác các chunk hữu ích, chúng tôi sử dụng sự tương đồng vốn có giữa các query cấp token và các key cấp token để xây dựng các biểu diễn query và key cấp chunk. Lấy thí nghiệm 32K Passkey Retrieval làm ví dụ, chunk chứa câu trả lời (tức là, chunk có giá trị nhất) là chunk có điểm chọn cao nhất trong 98% trường hợp mà không cần được huấn luyện.

### 2.2 Biểu diễn Chunk

Biểu diễn chunk là một chỉ báo về việc liệu các token trong chunk này có nên được chú ý hay không. Chúng tôi thu được các biểu diễn chunk theo cách không cần huấn luyện bằng cách sử dụng các khả năng nội tại của attention.

Chính thức, cho một chuỗi đầu vào dài X = (x1, ..., xn), chúng tôi phân đoạn nó thành các chunk theo kích thước chunk được định trước l, sau đó chuỗi đầu vào có thể được ký hiệu là X = (C1, ..., Cm), m = ⌈n/l⌉.

Chúng tôi sử dụng các trạng thái key của attention để tạo ra biểu diễn chunk cho mỗi chunk do cơ chế attention hiện có dựa vào các trạng thái query. Có nhiều phương pháp đơn giản để thu được biểu diễn chunk, chẳng hạn như mean pooling của các vector key của tất cả các token trong chunk. Tuy nhiên, chúng đã thể hiện hiệu suất không tối ưu trong các thí nghiệm sơ bộ, đặc biệt là trong việc chọn các chunk đúng. Chúng tôi giả định rằng điều này được quy cho tầm quan trọng của các token riêng lẻ trong một chunk khác nhau đáng kể.

Để giải quyết vấn đề trên, chúng tôi nên xác định các token có thể đại diện cho toàn bộ chunk. Với mục đích đó, chúng tôi đánh giá tầm quan trọng của mỗi token đối với chunk và thực hiện scaled attention aggregation trên tất cả các trạng thái key của token để thu được biểu diễn chunk đại diện như sau:

ci = flash-attention(qᶜᵢ, Ki, Ki)                (1)

trong đó ci ∈ Rᵐˣᵈ là biểu diễn chunk, Ki ∈ Rˡˣᵈ là tất cả các trạng thái key của attention của chunk Ci, qᶜᵢ ∈ Rᵐˣᵈ là một vector query để chỉ ra trạng thái key của token nào phù hợp để đại diện cho biểu diễn chunk. Tiếp theo, chúng tôi mô tả cách tạo ra vector query.

Một vector query chunk tốt nên có thể đại diện cho thông tin ngữ nghĩa đầy đủ của chunk, tức là, vector value của tất cả các token trong toàn bộ chunk. Tuy nhiên, các token khác nhau không đóng góp bằng nhau vào biểu diễn ngữ nghĩa, ví dụ, các từ nội dung có trọng số ngữ nghĩa cao hơn, trong khi các từ chức năng đóng góp ít hơn. Sử dụng sự tương đồng dot-product vốn có giữa các biểu diễn query và key cấp token, chúng tôi xây dựng các trọng số ngữ nghĩa cho mỗi token thông qua aggregation self-attention hai chiều. Từ góc độ message passing, các từ nội dung giàu ngữ nghĩa sẽ truyền nhiều thông tin của chúng đến các token khác, trong khi các từ chức năng truyền ít. Cuối cùng, các vector query qᶜᵢ tóm tắt thành công ngữ nghĩa hoàn chỉnh được thu được bằng mean-pooling của các biểu diễn tổng hợp, và có thể được hình thức hóa như sau:

Oi = flash-attention(Qi, Ki, Vi)
qᶜᵢ = mean(Oi),                               (2)

trong đó Qi, Ki, và Vi ∈ Rˡˣᵈ là tất cả các trạng thái query, trạng thái key, và trạng thái value của chunk Ci tương ứng. Cả Ki và Vi đều có thể được truy cập trực tiếp từ KV cache, trong khi Qi yêu cầu lưu trữ tạm thời trong quá trình tính toán biểu diễn của chunk hiện tại và được giải phóng sau đó.

### 2.3 Chiến lược Chọn Chunk

Trong quá trình mã hóa hoặc tạo ra token tiếp theo (ký hiệu là xj), chúng tôi sử dụng một chiến lược chọn chunk nhận biết query, chọn k chunk có liên quan nhất từ những chunk đã được tạo ra. Dựa trên kiến thức trước, có hai chunk bắt buộc. Một là phù hợp với các phát hiện của Xiao et al. (2023), thừa nhận vai trò thiết yếu của một vài token bắt đầu của một câu trong việc bảo tồn tính ổn định của LLM. Nếu một vài token bắt đầu bị thiếu khỏi ngữ cảnh, các LLM pre-trained sẽ hoàn toàn mất khả năng biểu đạt của chúng (tức là, thể hiện perplexity rất cao). Để đảm bảo tính trôi chảy, tất cả các attention head đồng nhất chọn chunk đầu tiên (tức là, C1) của câu. Nếu không, LLM không thể xử lý các task downstream (như được chứng minh trong Ablation Study). Cái khác là gán chunk cuối cùng (tức là, C₋₁) cho tất cả các attention head, để cung cấp cho mô hình thông tin cục bộ cần thiết cho việc tạo ra.

Tiếp theo, chúng tôi chọn k-2 chunk có liên quan nhất còn lại cho mỗi attention head. Trong module attention của LLM, điểm dot product phản ánh mức độ liên quan của token ngữ cảnh đến token hiện tại. Được truyền cảm hứng từ điều này, chúng tôi chọn các chunk mục tiêu bằng sự tương đồng dot product giữa trạng thái query qj của token hiện tại và biểu diễn chunk ci.

P = {C1} ∪ {Ci | rank(qj · ci) ≤ k-2} ∪ {C₋₁},    (3)

trong đó P là tập hợp cuối cùng của các chunk đã chọn, và hàm rank(·) xuất ra thứ hạng của độ tương đồng được tính toán của chunk hiện tại trong số tất cả các ứng viên. Theo cách này, các attention head khác nhau qua các lớp tự nhiên chú ý đến các phần khác nhau của ngữ cảnh, truy xuất các chunk quan trọng cho suy luận.

**Position Remapping.** Có các chunk văn bản trong tập hợp P vượt quá độ dài pre-training, vì vậy positional encoding của P cần được ánh xạ lại. Tổng độ dài của các chunk đã chọn được kiểm soát để nằm trong độ dài pre-training L, tức là, k*l < L. Ở đây, LONGHEADS tái cấu trúc các chunk đã chọn và nối chúng lại, trong khi bảo tồn thứ tự ưu tiên. Trong Hình 3, head hiện tại chú ý đến các chunk (1,2,7,8) trong số tám chunk ứng viên. Các vị trí được gán là [1,4l], trái ngược với các vị trí văn bản gốc, sẽ là [1,l] ∪ [l+1,2l] ∪ [6l+1,7l] ∪ [7l+1,8l]. Position remapping tránh vấn đề out-of-distribution gặp phải khi mở rộng ngữ cảnh ngay cả khi không có huấn luyện thêm.

### 2.4 Suy luận với LONGHEADS

Chúng tôi mô tả riêng biệt việc mã hóa các đầu vào dài và việc tạo ra các đầu ra dài trong quá trình suy luận. Ở đây chúng tôi chỉ mô tả lớp multi-head causal attention đã được sửa đổi.

**Tính toán và Bộ nhớ trong Giai đoạn Mã hóa.** Khi LONGHEADS nhận các đầu vào dài, đầu tiên nó tính toán các biểu diễn của tất cả các chunk song song. Điều này có thể được thực hiện nhanh chóng thông qua hai lần chạy flash-attention, với số lượng token tham gia vào attention bằng kích thước chunk (tức là, l=256, nhỏ hơn nhiều so với độ dài của đầu vào, ví dụ, n=16k). Bước thứ hai là chọn k chunk có liên quan nhất cho mỗi query dựa trên các biểu diễn chunk và thu được các biểu diễn key và value của chúng, làm cho cửa sổ attention bằng k*l=w (ví dụ, w=2k, cũng nhỏ hơn nhiều so với n). Cuối cùng, length-restricted causal flash-attention được thực hiện hiệu quả.

**Tính toán và Bộ nhớ trong Giai đoạn Tạo ra.** Trong quá trình tạo ra, LONGHEADS đầu tiên thực hiện chọn chunk, sau đó tải các biểu diễn Key-Value của k chunk đã chọn cho length-constrained causal attention. Khi tạo ra với các đầu vào rất lớn (ví dụ 100K), KV cache (ngoại trừ các biểu diễn chunk) có thể được offload đến CPU để giảm đáng kể việc sử dụng bộ nhớ, và chúng tôi chỉ tải các chunk đã chọn vào bộ nhớ GPU. Chúng tôi luôn giữ lại các biểu diễn query-key-value của các token gần đây (không vượt quá kích thước chunk) trong quá trình tạo ra. Khi số lượng token gần đây bằng kích thước chunk, chúng tôi tính toán biểu diễn chunk, tương tự như giai đoạn mã hóa, và thêm nó vào các biểu diễn chunk trước đó.

Nhìn chung, độ phức tạp thời gian xấp xỉ một LLM với window attention O(w²) (kích thước cửa sổ w bằng k*l). Việc sử dụng bộ nhớ của giai đoạn giải mã xấp xỉ O(n+w²), và có thể được giảm thêm xuống O(k*l+w²), tránh sự gia tăng bậc hai chi phí với độ dài chuỗi.

## 3 Thí nghiệm

Chúng tôi đánh giá LONGHEADS được đề xuất chủ yếu sử dụng LLaMA-2 (Touvron et al., 2023) xem xét việc áp dụng rộng rãi và phổ biến của nó. Hiệu quả của LONGHEADS được đánh giá trên ba loại task: modeling ngôn ngữ, task truy xuất tổng hợp và benchmark ngữ cảnh dài.

### 3.1 Thiết lập

**Triển khai.** Phương pháp của chúng tôi được áp dụng cho các mô hình LLaMA-2-7B base và chat cho các nghiên cứu thực nghiệm. Trong thiết lập của chúng tôi, chúng tôi đặt kích thước của mỗi chunk l là 256. Trong mỗi bước suy luận, chúng tôi sử dụng chiến lược chọn chunk của chúng tôi để thực hiện chọn chunk nhận biết query. Cho mỗi lần chọn, chúng tôi luôn chọn chunk đầu tiên từ văn bản dài để tạo điều kiện cho việc tạo ra bình thường bởi mô hình, và chunk cuối cùng để cung cấp thông tin ngữ cảnh cục bộ. Cho tất cả các task đánh giá, suy luận được thực hiện trên một GPU NVIDIA A100 đơn.

**Baseline.** Các loại baseline sau được chọn để so sánh. 1) Phương pháp với full attention, bao gồm interpolation "Dynamic NTK" (NTK, Emozilla, 2023) và Position Interpolation (PI, Chen et al., 2023a). 2) Phương pháp với restricted attention, bao gồm LM-Infinite (Han et al., 2023) và Landmark-Attention (Mohtashami và Jaggi, 2023). Chi tiết triển khai của các baseline được nêu trong Phụ lục A.

### 3.2 Modeling Ngôn ngữ Ngữ cảnh Dài

Thí nghiệm về modeling ngôn ngữ ngữ cảnh dài được thực hiện với hai bộ dữ liệu: PG19 (Rae et al., 2019) và bộ dữ liệu Proof-pile (Azerbayev et al., 2023). Chi tiết được thể hiện trong Phụ lục C.1.

Các kết quả đánh giá được báo cáo trong Bảng 1. Mặc dù PPL của mô hình LLaMA-2-7B-Base và PI vẫn thấp trong độ dài ngữ cảnh pre-training, nó tăng đáng kể khi ngữ cảnh vượt quá cửa sổ này. Phương pháp NTK có thể duy trì các giá trị PPL thấp cho các chuỗi lên đến độ dài 16k, nhưng PPL tăng đáng kể ở độ dài ngữ cảnh 32k. Ngược lại, LONGHEADS, Landmark Attention và LM-infinite thành công duy trì điểm PPL thấp ngay cả ở độ dài chuỗi 32k.

### 3.3 Đánh giá Dựa trên Truy xuất

Chúng tôi thực hiện thí nghiệm trên task passkey retrieval được giới thiệu bởi (Mohtashami và Jaggi, 2023). Task này thách thức một mô hình ngôn ngữ để định vị chính xác và truy xuất một passkey đơn giản (một số ngẫu nhiên năm chữ số) trong một chuỗi văn bản dài. Nó kiểm tra liệu một LLM có thể chú ý hiệu quả đến thông tin qua tất cả các vị trí của chuỗi đầu vào hay không. Theo thiết kế của Mohtashami và Jaggi (2023), passkey được đặt với các độ dài ngữ cảnh khác nhau (từ 4k đến 32k với khoảng cách 4k). Cho mỗi độ dài ngữ cảnh, chúng tôi thực hiện 50 lần kiểm tra với passkey được đặt ở một vị trí ngẫu nhiên trong ngữ cảnh.

Trong Hình 4, chúng ta có thể thấy rằng tất cả các mô hình đều có thể xuất ra passkey trong độ dài pretrained. Mô hình cơ sở hoàn toàn thất bại ở độ dài mở rộng. NTK và LM-Infinite gây ra sự sụt giảm đáng kể về độ chính xác cho các mô hình ở độ dài vượt quá 6k token, với độ chính xác giảm xuống dưới 20% khi độ dài token vượt quá 16k. LM-Infinite chỉ có thể truy cập 10% passkey với cửa sổ cục bộ của nó, mặc dù có PPL thấp ở độ dài 32k. Ngược lại, Landmark Attention và LONGHEADS luôn truy xuất với gần 100% độ chính xác bất kể độ dài chuỗi.

Chúng tôi tiếp tục kiểm tra LONGHEADS ở độ dài 128k sau khi offload KV cache đến CPU, các kết quả được thể hiện trong Phụ lục B. Chúng tôi lưu ý rằng LONGHEADS chỉ sử dụng cửa sổ attention 2k đạt được 100% độ chính xác ở độ dài 128k mà không cần huấn luyện.

### 3.4 Đánh giá Benchmark Ngữ cảnh Dài

Các task modeling ngôn ngữ đã được chứng minh là các metric không đủ để đảm bảo thành công trong các task downstream (Sun et al., 2021), trong khi các task truy xuất mật khẩu tổng hợp thường không phù hợp với các tình huống thực tế. Việc thực hiện đánh giá task downstream NLP thực tế là quan trọng để phản ánh toàn diện hơn khả năng chuỗi dài của mô hình. Chúng tôi chọn LongBench (Bai et al., 2023) cho đánh giá task downstream NLP, các chi tiết được thể hiện trong Phụ lục C.2. Các kết quả được liệt kê trong Bảng 2. Chúng tôi cũng thực hiện thí nghiệm trên mô hình LLaMA-2-7B-Chat, và các kết quả được thể hiện trong Phụ lục E.

**So sánh với Các phương pháp Restricted Attention.** LONGHEADS vượt qua các phương pháp hiện tại với restricted attention. Cụ thể, LONGHEADS hoạt động tốt hơn phương pháp với cơ chế sliding window trên LongBench (+4.67 so với LM-Infinite). So với phương pháp với chiến lược chunking (tức là, Landmark Attention), LONGHEADS vượt điểm trung bình 2.92 trên LongBench mà không cần huấn luyện bổ sung. Điều này chỉ ra rằng chiến lược chọn chunk trong LONGHEADS có thể bổ sung chính xác cho LLM thông tin ngữ cảnh có liên quan, cho phép hiểu hiệu quả và hiệu suất trên các chuỗi dài.

**So sánh với Các phương pháp Full Attention.** Các phương pháp full attention có thể tăng độ dài chuỗi tối đa của LLM nhưng cũng tăng chi phí tính toán và bộ nhớ. LONGHEADS có thể được tăng cường với các phương pháp PI hoặc NTK trong giai đoạn mã hóa, đạt được kết quả tương đương hoặc thậm chí tốt hơn với kích thước cửa sổ ngắn hơn, giảm đáng kể chi phí tính toán. Điều này cho thấy rằng LONGHEADS có tiềm năng về khả năng mở rộng, và có thể được tăng cường với một mô hình cơ sở mạnh hơn.

**Hiệu suất khi mở rộng đến Cửa sổ ngữ cảnh 32k.** Một thuộc tính mong muốn cho các phương pháp mở rộng RoPE là các mô hình nên duy trì hiệu suất của chúng khi mở rộng trực tiếp đến cửa sổ ngữ cảnh dài hơn. Khi mở rộng đến cửa sổ ngữ cảnh 32k, các phương pháp PI và NTK gặp khó khăn với vấn đề out-of-demonstration và có xu hướng làm giảm hiệu suất mô hình. Ngược lại, LONGHEADS duy trì hiệu suất của nó và vượt trội hơn tất cả các phương pháp baseline. Nó thành công mở rộng LLaMA-2-7B-Base từ độ dài 4K đến 8 lần độ dài của nó, chứng minh rằng LONGHEADS có thể dễ dàng tổng quát hóa đến cửa sổ ngữ cảnh dài hơn.

## 4 Thảo luận

### 4.1 Phân tích

Trong phần này, chúng tôi khám phá cách các attention head khác nhau xử lý ngữ cảnh dài và liệu chúng có tìm thấy thông tin quan trọng hay không. Chúng tôi đặt cửa sổ attention của LONGHEADS là 2048 và phân tích hiệu suất của nó trên các task passkey retrieval và summary. Chúng tôi trực quan hóa các thử nghiệm cho cả hai task trong Hình 5 và thể hiện các kết quả thống kê trong Bảng 3. Chi tiết của các thí nghiệm phân tích được nêu trong Phụ lục D.

**Các attention head tập trung vào các phần quan trọng trong ngữ cảnh.** Trên task passkey retrieval, được thể hiện trong Hình 5(a), tất cả các attention head tập trung vào cùng một chunk chứa câu trả lời và dự đoán nó chính xác. Ngay cả khi passkey không được dự đoán thành công trong Hình 5(b), các chunk chứa câu trả lời vẫn được chọn bởi nhiều head. Ngược lại, trên task summary trong Hình 5(c), các attention head lan rộng sự tập trung của chúng đều hơn để tóm tắt toàn bộ thông tin. Tương tự, Bảng 3 tiết lộ điểm uniformity thấp hơn cho task summary so với task passkey retrieval. Những phát hiện này cho thấy rằng chiến lược chọn chunk của chúng tôi dẫn đến phân phối lựa chọn đồng đều hơn trong task summary, trong khi phân phối trong task passkey retrieval tập trung hơn. Chúng tôi quy điều này cho tính đặc hiệu của các chunk cần thiết cho task passkey retrieval, trong khi task summary đòi hỏi nhiều phần khác nhau của văn bản để hình thành một câu trả lời toàn diện. Hơn nữa, xác suất của 5 chunk được chọn hàng đầu chứa câu trả lời là gần 100% qua tất cả các độ dài thử nghiệm trong Bảng 3. Những kết quả này cho thấy rằng chiến lược chọn chunk của chúng tôi thích ứng với các đặc điểm của các task khác nhau, và cho phép các attention head khác nhau tập trung vào nội dung liên quan đến task.

**Các attention head có thể xử lý các chuỗi dài trong một cửa sổ ngắn.** Trong Hình 5, các attention head lớp thấp hơn tập trung vào văn bản phân tán hơn trong cả hai task, trong khi các attention head lớp trên tập trung nhiều hơn vào các chunk cụ thể. Chúng tôi suy đoán rằng các attention head khác nhau tự nhiên tập trung vào các phần khác nhau của thông tin trong văn bản ở các lớp thấp hơn, thu thập và tổng hợp toàn bộ thông tin tài liệu dài trong một độ dài ngắn, trong khi các attention head lớp trên chịu trách nhiệm xử lý thông tin tổng hợp, chủ yếu tập trung vào các chunk cần thiết để hoàn thành task. Trong Bảng 3, Cover Rate là 100% trong hầu hết các trường hợp. Cho rằng các head khác nhau trong mỗi lớp có thể chọn các chunk khác nhau, độ dài tối đa lý thuyết mà LONGHEADS có thể truy cập là |P| × n_heads × n_layers (ví dụ, độ dài tối đa cho LLaMA-2-7B với cửa sổ attention 4k là 512k). Những quan sát này chứng minh rằng chúng tôi đã thành công sử dụng một cửa sổ attention hạn chế để nắm bắt gần như tất cả thông tin từ toàn bộ tài liệu dài.

### 4.2 Ablation Study

Chúng tôi thực hiện các thí nghiệm ablation để điều tra ảnh hưởng của chiến lược chọn chunk, tính linh hoạt của attention head, số lượng chunk K, và kích thước chunk l. Ablation study được xây dựng trên LongBench và các kết quả được trình bày trong Bảng 4.

**Hiệu ứng của Chiến lược Chọn Chunk.** Chúng tôi thấy rằng hiệu suất khi chọn các chunk có điểm cao nhất vượt trội đáng kể so với các chunk có điểm thấp nhất (Last K), và ngay cả Random Selection cũng cho kết quả tốt hơn Last K Selection. Chúng tôi cũng quan sát thấy sự suy giảm hiệu suất đáng kể khi chunk đầu tiên không được bảo tồn. Điều này là do sự vắng mặt của chunk đầu tiên dẫn đến phân phối đầu ra của mô hình sụp đổ trực tiếp. Các phát hiện của chúng tôi phù hợp với StreamingLLM (Xiao et al., 2023) và LM-Infinite (Han et al., 2023).

**Hiệu ứng của Tính linh hoạt của Head.** Khi tính linh hoạt của attention head bị hạn chế, hiệu suất của mô hình bị ảnh hưởng ở các mức độ khác nhau (-0.68 Fix Head, -1.36 Fix Layer, -1.42 Fix Head&Layer). Điều này chứng minh rằng trong framework LONGHEADS, sự hợp tác của các attention head khác nhau trong mỗi lớp đóng một vai trò quan trọng.

**Hiệu ứng của Số lượng Chunk & Kích thước Chunk.** Tăng số lượng chunk trong một văn bản có thể cung cấp nhiều thông tin hơn, nhưng lợi ích cho thấy lợi nhuận giảm dần. Điều này chỉ ra rằng bốn chunk cung cấp đủ thông tin để đảm bảo hiệu suất, và tám chunk đã đủ để truy cập thông tin của toàn bộ chuỗi với chiến lược chọn chunk. Các kích thước chunk khác nhau không dẫn đến tác động đáng kể đến kết quả, chỉ ra rằng các kích thước chunk lớn hơn hoặc nhỏ hơn đều khả thi cho LONGHEADS.

## 5 Công trình Liên quan

**Mở rộng Positional Encoding (PE).** Các nghiên cứu mở rộng ngữ cảnh thường nhắm đến RoPE encoding phổ biến, nhằm mở rộng PE chưa thấy vào không gian các vị trí đã thấy trong quá trình pre-training. Chen et al. (2023a), và đồng thời kaiokendev (2023) phát hiện ra rằng việc nội suy các chỉ số vị trí trong giới hạn pre-trained hoạt động tốt với sự giúp đỡ của một lượng nhỏ (vài tỷ, Chen et al., 2023a) fine-tuning. Tuy nhiên, position interpolation (PI) kéo giãn đều tất cả các chiều của RoPE, bỏ qua các biến thể trong tần số. Như một thay thế, Bloc97 (2023b) đề xuất interpolation "NTK-aware" bằng cách xem xét tổn thất của các thành phần tần số cao. Sau đó, Emozilla (2023) đề xuất phương pháp interpolation "Dynamic NTK", hoạt động tốt mà không cần fine-tuning. Bloc97 (2023a) giới thiệu phương pháp interpolation "NTK-by-parts", hoạt động tốt nhất khi được fine-tuned trên một lượng nhỏ dữ liệu ngữ cảnh dài hơn. Peng et al. (2023) đề xuất YaRN, một phương pháp cải tiến để mở rộng hiệu quả cửa sổ ngữ cảnh bằng cách fine-tuning trên ít hơn 0.1% dữ liệu pre-training gốc. Công trình này trực tiếp sửa đổi PE để mở rộng đến độ dài ngữ cảnh lý thuyết vô hạn. Ngược lại, phương pháp của chúng tôi không yêu cầu sửa đổi PE, và chỉ một chunk hữu hạn tham gia vào tính toán attention trong quá trình tạo ra, điều này cải thiện hiệu quả suy luận và giảm việc sử dụng bộ nhớ.

**Restricted Attention.** Ngoài ra, global causal attention có thể bị hạn chế thành local attention, do đó tránh vượt quá độ dài vị trí pre-trained. ReRoPE (Su, 2023) cắt tất cả độ dài ngữ cảnh đến độ dài tối đa trong quá trình pretraining. LM-Infinite (Han et al., 2023) hạn chế cửa sổ global attention thành cửa sổ hình chevron, chỉ giữ lại một vài token từ đầu văn bản và một cửa sổ cục bộ. Mohtashami và Jaggi (2023) chèn một landmark token có thể học được sau mỗi đoạn văn bản với độ dài cố định, và sử dụng những landmark này để truy xuất các đoạn có liên quan. Zhang et al. (2024) tương tự chèn một beacon token có thể học được và sử dụng biểu diễn của nó để tóm tắt toàn bộ đoạn tương ứng. Mặc dù restricted attention mang lại lợi thế về việc sử dụng bộ nhớ và tốc độ suy luận, chúng có nguy cơ mất thông tin ngữ cảnh có giá trị. Các phương pháp hiện có sử dụng các cửa sổ cục bộ hoặc cố định hoặc được chọn thông qua fine-tuning. Trong phương pháp của chúng tôi, các cửa sổ cục bộ được cấu thành linh hoạt từ các chunk của ngữ cảnh và không dựa vào fine-tuning bổ sung.

## 6 Kết luận

Chúng tôi trình bày LONGHEADS, một framework mới, không cần huấn luyện để xử lý hiệu quả ngữ cảnh dài trong các LLM pre-trained. Sử dụng các khả năng nội tại của attention head, LONGHEADS thông minh phân đoạn và gán văn bản dài cho các head có liên quan, hợp lý hóa việc xử lý các chuỗi mở rộng mà không có tải tính toán bổ sung. Kết quả thí nghiệm xác nhận tính ưu việt của LONGHEADS trong các thiết lập restricted attention và lợi thế cạnh tranh của nó so với các phương pháp full attention khi được áp dụng cho bộ LongBench. Phương pháp của chúng tôi mở đường cho những đột phá hiệu suất trong các hoạt động LLM ngữ cảnh dài, tận dụng các cấu trúc mô hình hiện có để khai thác tiềm năng mới mà không cần huấn luyện thêm.

## Hạn chế

Chúng tôi tóm tắt các hạn chế của phương pháp của chúng tôi như sau: (1) Chia văn bản thành các chunk có thể làm gián đoạn tính liên tục của nội dung. Khi câu trả lời đúng nằm ở giữa hai chunk, loại chia này có thể ảnh hưởng đến hiệu suất của các task downstream. (2) Độ dài tối đa lý thuyết mà LONGHEADS có thể truy cập bị giới hạn ở |P| × n_heads × n_layers. LONGHEADS không thể truy cập đầy đủ các đầu vào vượt quá ngưỡng này. Tuy nhiên, LONGHEADS vẫn có thể hoạt động tốt trên các task tài liệu dài bằng cách chọn các phần quan trọng từ ngữ cảnh. (3) Thành công của LONGHEADS trong các task downstream phụ thuộc vào hàm chọn chunk không tham số. Đối với các task hiểu phức tạp, hiệu quả của hàm chọn có thể bị ảnh hưởng.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc]
