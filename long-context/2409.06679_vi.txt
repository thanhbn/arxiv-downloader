# E2LLM: E NCODER ELONGATED LARGE LANGUAGE MODELS
FOR LONG -CONTEXT UNDERSTANDING AND REASONING
BẢN THẢO TRƯỚC

Zihan Liao∗
East China Normal University
52215901015@stu.ecnu.edu.cnJun Wang*
Ant Group
yangrong.wj@antgroup.comHang Yu*
Ant Group
hyu.hugo@antgroup.com
Lingxiao Wei
East China Normal University
51265901053@stu.ecnu.edu.cnJianguo Li†
Ant Group
lijg.zero@antgroup.comJun Wang†
East China Normal University
wongjun@gmail.com
Wei Zhang†
East China Normal University
zhangwei.thu2011@gmail.com
September 11, 2024
TÓM TẮT
Trong lĩnh vực Mô hình Ngôn ngữ Lớn (LLMs), khả năng xử lý ngữ cảnh dài ngày càng trở nên quan trọng đối với các tác vụ như đối thoại nhiều vòng, tạo mã và tóm tắt tài liệu. Bài báo này giải quyết các thách thức về nâng cao hiệu suất ngữ cảnh dài, giảm độ phức tạp tính toán và tận dụng các mô hình được huấn luyện trước—được gọi chung là "tam giác bất khả thi". Chúng tôi giới thiệu E2LLM (Encoder Elongated Large Language Models), một phương pháp mới hiệu quả điều hướng nghịch lý này. Phương pháp này bao gồm việc chia ngữ cảnh dài thành các đoạn, nén mỗi đoạn thành vector nhúng thông qua bộ mã hóa văn bản được huấn luyện trước, và sử dụng bộ điều hợp để căn chỉnh các biểu diễn này với LLM chỉ giải mã. Hai mục tiêu huấn luyện, tập trung vào tái tạo đầu ra của bộ mã hóa và tinh chỉnh hướng dẫn ngữ cảnh dài, được sử dụng để hỗ trợ việc hiểu các gợi ý mềm bởi LLM. Kết quả thí nghiệm chứng minh rằng E2LLM đạt được hiệu suất vượt trội trong các tình huống ngữ cảnh dài trong khi cân bằng hiệu quả, hiệu suất và khả năng tương thích với các mô hình được huấn luyện trước. Khung công tác của chúng tôi do đó đại diện cho một tiến bộ đáng kể trong lĩnh vực này, góp phần vào việc mô hình hóa văn bản dài hiệu quả. Mã nguồn sẽ có sẵn khi công bố.

1 Giới thiệu
Hiểu và lập luận về ngữ cảnh dài đã trở thành điều cần thiết đối với Mô hình Ngôn ngữ Lớn (LLMs), đặc biệt là đối với các tác vụ như đối thoại nhiều vòng Bai et al. [2024], tạo mã (nhiều) kho lưu trữ Zhang et al. [2023], và tóm tắt (nhiều) tài liệu Giorgi et al. và trả lời câu hỏi Singh et al. [2021]. Những tác vụ này thường đòi hỏi xử lý hàng nghìn hoặc thậm chí hàng triệu token để đảm bảo tính mạch lạc và chính xác. Mặt khác, để thúc đẩy hiệu suất của LLMs, các kỹ thuật gợi ý LLMs hiệu quả để kích hoạt kiến thức chuyên ngành—như lập luận chuỗi suy nghĩ Wei et al. [2022], học trong ngữ cảnh Dong et al. [2022], và truy xuất tài liệu liên quan hoặc cuộc trò chuyện lịch sử Ding et al. [2024]—cũng đang thúc đẩy nhu cầu về độ dài chuỗi dài hơn.

Nỗ lực đáng kể đã và vẫn đang được dành để phát triển các mô hình có thể tăng độ dài ngữ cảnh của LLMs, nhằm đạt được hiệu suất mạnh đối với ngữ cảnh dài hơn (T1), trong khi giảm độ phức tạp huấn luyện và suy luận

∗Đóng góp bằng nhau. Công việc này được thực hiện khi Zihan Liao là thực tập sinh nghiên cứu tại Ant Group.
†Tác giả liên hệ.arXiv:2409.06679v1  [cs.CL]  10 Sep 2024

--- TRANG 2 ---
arXiv Template BẢN THẢO TRƯỚC

E2LLMT1.Hiệu suấtT2.Hiệu quảT3.Khả năng tương thíchMở rộng độ dàiNén gợi ý mềmNén gợi ý cứngChú ý thưa thớt

Hình 1: E2LLM giải quyết tất cả các thách thức của tam giác bất khả thi cùng một lúc, cụ thể là Hiệu suất, Hiệu quả và Khả năng tương thích.

DEEEĐoạn1Đoạn2Đoạn3Gợi ý+Truy vấnTrả lờiVăn bảnToken văn bảnToken đoạnNhúng đoạnLoRAA
Có thể huấn luyện
Đóng băngEBộ mã hóa văn bảnABộ điều hợpDLLM-Bộ giải mãVăn bản
Với các ngữ cảnh: [token đoạn] Vui lòng làm theo hướng dẫn: Trình bày lại ngữ cảnh đã nói ởtrên Với các ngữ cảnh: [token đoạn] Vui lòng làm theo hướng dẫn: Trả lời câu hỏi: {truy vấn}Gợi ý cho tác vụ "hiểu"Gợi ý cho tác vụ "lập luận"A
A
LoRA
LoRA
LoRA

Hình 2: Kiến trúc E2LLM.

(T2), và đồng thời tương thích với các mô hình được huấn luyện trước (T3) sao cho kiến thức được huấn luyện trước trong các mô hình này có thể được khai thác hiệu quả. Tuy nhiên, việc đạt được cả ba mục tiêu đồng thời đặt ra một thách thức đáng gờm thường dẫn đến một số thỏa hiệp, một hiện tượng chúng tôi gọi là "tam giác bất khả thi", như được minh họa trong Hình 1.

Hiện tại, nghiên cứu trong lĩnh vực này chủ yếu tập trung vào ba hướng chính: sửa đổi nhúng vị trí, cơ chế chú ý và chính chuỗi đầu vào dài. Nhóm phương pháp đầu tiên, được gọi là mở rộng độ dài, bao gồm việc điều chỉnh nhúng vị trí của LLMs để phù hợp với việc mở rộng ngữ cảnh dài hơn. Điều này thường bao gồm việc chọn một giá trị cơ sở lớn cho RoPE Su et al. [2024] và sau đó tiếp tục huấn luyện trước hoặc tinh chỉnh đến độ dài mục tiêu. Trong khi những phương pháp này hiệu quả mở rộng độ dài của LLMs với những thay đổi mô hình tối thiểu (T1&T3), chúng thường phát sinh chi phí tính toán đáng kể trong cả quá trình huấn luyện và suy luận (T2). Ví dụ, ngay cả với khả năng mở rộng độ dài chuỗi lên 2M, như thấy trong LongRoPE Ding et al., tài nguyên khổng lồ được yêu cầu để huấn luyện và triển khai mô hình, và thời gian suy luận có thể quá dài đối với các chuỗi mở rộng. Trái ngược với nhóm đầu tiên, nhóm thứ hai, được gọi là chú ý thưa thớt, thay thế chú ý đầy đủ trong LLMs bằng chú ý cục bộ hoặc kết hợp chú ý toàn cục và cục bộ. Phương pháp này giảm đáng kể độ phức tạp bậc hai liên quan đến chú ý đầy đủ, thậm chí đạt được độ phức tạp tuyến tính về mặt lý thuyết (T2). Tuy nhiên, một mối quan tâm đáng chú ý với chú ý thưa thớt là tiềm năng bỏ qua lịch sử thông tin, vì một số token có thể không được chú ý đến trong quá trình tính toán chú ý (T1). Hơn nữa, vì LLMs không được huấn luyện trước ban đầu với chú ý thưa thớt, việc điều chỉnh chúng cho chú ý thưa thớt có thể đòi hỏi huấn luyện hoặc tinh chỉnh rộng rãi (T3). Khác với hai nhóm trước đó thay đổi LLMs, nhóm chiến lược thứ ba trực tiếp nén chuỗi đầu vào để giảm độ dài của nó (T2), có thể được chia thành hai nhóm phụ. Nhóm phụ đầu tiên, được gọi là nén gợi ý cứng—được minh họa bằng các phương pháp như Tạo có Hỗ trợ Truy xuất (RAG) Ding et al. [2024] và LLMLingua Jiang et al. [2023a]—có xu hướng xử lý nén và suy luận theo cách hai bước. Kết quả là, bất kỳ mất mát thông tin hoặc đưa vào nội dung không liên quan nào trong giai đoạn nén có thể ảnh hưởng bất lợi đến hiệu suất trong bước suy luận tiếp theo (T1). Thay vào đó, nhóm phụ thứ hai xem xét nén gợi ý mềm, tóm tắt ngữ cảnh dài thành vector nhúng. Tuy nhiên, việc sử dụng LLMs trong những phương pháp này để tạo trực tiếp nhúng cấp độ câu khác với mục tiêu huấn luyện trước ban đầu của chúng là dự đoán token tiếp theo. Do đó, việc đạt được hiệu suất thỏa đáng trong ngữ cảnh này thường đòi hỏi huấn luyện hoặc tinh chỉnh nghiêm ngặt để căn chỉnh khả năng của mô hình với mục tiêu mới (T3).

Trong bài báo này, chúng tôi đề xuất một phương pháp nén mới được gọi là E2LLM (Encoder Elongated Large Language Models) điều hướng khéo léo những phức tạp của "tam giác bất khả thi". Cụ thể, như được hiển thị trong Hình 2, phương pháp của chúng tôi đầu tiên chia ngữ cảnh dài thành các đoạn và nén mỗi đoạn thành một vector nhúng sử dụng bộ mã hóa văn bản được huấn luyện trước (ví dụ: BERT Kenton và Toutanova [2019]). Sau đó, một bộ điều hợp căn chỉnh đầu ra của bộ mã hóa với không gian nhúng đầu vào của LLM chỉ giải mã, sao cho LLM có thể hiểu các vector nhúng kết quả từ bộ mã hóa. Cuối cùng, chúng tôi thiết lập hai mục tiêu huấn luyện để căn chỉnh bộ mã hóa và bộ giải mã, bao gồm tái tạo văn bản đầu vào được mã hóa bởi bộ mã hóa ("hiểu") và tinh chỉnh hướng dẫn ngữ cảnh dài ("lập luận"). Chúng tôi giả định rằng LLMs vốn dĩ giàu kiến thức; do đó, các gợi ý mềm được nén phù hợp (hoặc các vector nhúng) có thể truyền đạt thông tin đầy đủ một cách súc tích cho LLMs để tạo ra câu trả lời chính xác. Hơn nữa, vì các mô hình bộ mã hóa được huấn luyện trước vốn dĩ được tạo ra để sản xuất nhúng câu, thiết kế này cho phép E2LLM tận dụng cả bộ mã hóa và bộ giải mã được huấn luyện trước, giảm thiểu yêu cầu về huấn luyện bổ sung rộng rãi (T3). Ngoài ra, việc nén mỗi đoạn gốc thành một vector (tức là một token đoạn duy nhất) không chỉ tăng cường hiệu quả huấn luyện và suy luận (T2) mà còn mở rộng độ dài ngữ cảnh một cách đáng kể (T1). Thật vậy, độ dài chuỗi lý thuyết bằng tích của độ dài chuỗi của bộ mã hóa và bộ giải mã. Kết quả thí nghiệm cung cấp bằng chứng thuyết phục về hiệu suất vượt trội của E2LLM trong các tình huống ngữ cảnh dài, chứng minh hiệu quả của phương pháp chúng tôi trong việc duy trì sự cân bằng tinh tế giữa hiệu suất, hiệu quả và khả năng tương thích.

Để tóm tắt, những đóng góp chính của công việc chúng tôi là:
•Chúng tôi đề xuất E2LLM, một khung mô hình hóa văn bản dài LLM mới được xây dựng trên các mô hình nhúng câu được huấn luyện trước và LLMs chỉ giải mã, hiệu quả giải quyết các thách thức do "tam giác bất khả thi" đặt ra.
•Chúng tôi giới thiệu hai mục tiêu huấn luyện, bao gồm tái tạo gợi ý mềm được đưa ra bởi bộ mã hóa và tinh chỉnh hướng dẫn ngữ cảnh dài, cho phép LLM hiểu gợi ý mềm trong khi tạo ra đầu ra chính xác cho đầu vào dài.
•Các thí nghiệm toàn diện được tiến hành trên các tác vụ và bộ dữ liệu đa dạng chứng minh hiệu quả và tính thực tiễn của mô hình được đề xuất và tiết lộ hiệu suất cạnh tranh với công nghệ tiên tiến.

2 Công trình liên quan
Như đã đề cập trong phần giới thiệu, các phương pháp phổ biến có thể được phân loại thành ba nhóm: sửa đổi nhúng vị trí (tức là mở rộng độ dài), cơ chế chú ý (tức là chú ý thưa thớt), và chuỗi đầu vào (tức là nén gợi ý).

Huấn luyện Mở rộng Độ dài Huấn luyện LLMs trên các chuỗi với độ dài chuỗi tối đa hạn chế trong khi đảm bảo khái quát hóa cho các chuỗi dài hơn là thách thức. Để giải quyết điều này, các phương pháp ngoại suy và nội suy vị trí đã được đề xuất. Ngoại suy vị trí mở rộng mã hóa vị trí vượt ra ngoài độ dài huấn luyện; ví dụ, ALiBi Press et al. tăng cường chú ý với độ lệch tuyến tính điều chỉnh điểm số dựa trên khoảng cách giữa vị trí khóa và truy vấn. Thay vào đó, xPOS Sun et al. [2023] sử dụng nhúng vị trí tương đối để có độ phân giải chú ý tốt hơn và độ dài mở rộng. Tuy nhiên, những phương pháp này chưa được tích hợp vào các LLMs gần đây như Llama2 Touvron et al. [2023] và Qwen2 Bai et al. [2023a], chủ yếu do hiệu suất không tối ưu của ngoại suy vị trí. Mặt khác, nội suy vị trí thu nhỏ chỉ số vị trí đầu vào và mở rộng cửa sổ ngữ cảnh để duy trì hiệu suất trên các chuỗi dài hơn. Ví dụ, Chen et al. [2023a] áp dụng nội suy tuyến tính cho RoPE để căn chỉnh chỉ số vị trí tối đa với các ràng buộc huấn luyện trước. Nội suy NTK bloc97. [2023] sửa đổi cơ sở của RoPE để điều chỉnh vận tốc quay của các chiều của nó. Để kết hợp điểm mạnh của những phương pháp này, YaRN Peng et al. [2023] hợp nhất nội suy tuyến tính và NTK với hàm dốc và yếu tố nhiệt độ, giảm thiểu dịch chuyển phân phối trong ma trận chú ý với đầu vào dài hơn. LongRoPE Ding et al. tiếp tục tăng cường hiệu suất bằng cách khai thác hai dạng không đồng nhất trong nhúng vị trí RoPE thông qua tìm kiếm tiến hóa hiệu quả.

Bất chấp những tiến bộ này, hầu hết các phương pháp đòi hỏi huấn luyện trước liên tục hoặc tinh chỉnh để đạt được độ dài mong muốn, do đó đòi hỏi gánh nặng huấn luyện đáng kể. Ngoài ra, suy luận trên những mô hình mở rộng này có thể chậm do độ phức tạp bậc hai của chú ý đầy đủ. Ngược lại, E2LLM được đề xuất không thay đổi độ dài của LLM gốc mà nén chuỗi đầu vào thành các đoạn vector nhúng. Điều này cho phép E2LLM duy trì hiệu quả của LLM gốc trong cả quá trình huấn luyện và suy luận.

Chú ý Thưa thớt Loại phương pháp này nhằm giảm độ phức tạp suy luận của các mô hình ngôn ngữ lớn (LLMs) bằng cách thao tác cơ chế chú ý với mặt nạ chú ý mới, cho phép những mô hình này xử lý các chuỗi dài hơn. StreamingLLM Xiao et al. [2024] chứng minh rằng việc tập trung vào phần đầu của chuỗi và các token gần đây nhất trong một cửa sổ xác định (tức là chú ý cục bộ) trong quá trình suy luận duy trì hiệu suất trong khi giảm đáng kể chi phí tính toán xuống thang tuyến tính. Tuy nhiên, những phương pháp không cần huấn luyện này thường không đáp ứng trong nhiều tình huống Anagnostidis et al. [2023], Lou et al. [2024], vì chúng có thể bỏ qua các token thông tin nằm ở giữa chuỗi. Để cải thiện hiệu suất, LM-Infinite Han et al. [2024] đưa lại các token top-k từ giữa, nhưng phương pháp này đòi hỏi tính toán tất cả điểm chú ý, do đó tăng nhu cầu tính toán. Như một giải pháp, Lou et al. [2024] đề xuất chú ý SparseK, sử dụng mạng chấm điểm bổ sung để đánh giá tầm quan trọng của mỗi cặp khóa-giá trị và chọn các cặp top-k. Thay vào đó, LongLoRA Chen et al. [2023a] sử dụng chú ý thưa thớt dịch chuyển (một biến thể của chú ý cục bộ) và tinh chỉnh LLMs với LoRA Hu et al. [2021] để thích ứng với cơ chế này. Thật không may, như được ghi nhận bởi Tan et al. [2024], vẫn còn khoảng cách đáng kể giữa chú ý thưa thớt và đầy đủ, điều này làm phức tạp việc tinh chỉnh LLMs được huấn luyện trước cho các mô hình chú ý mới. Ngược lại, phương pháp E2LLM tóm tắt đầu vào ngữ cảnh dài thành vector gợi ý mềm, do đó giảm độ dài chuỗi mà không thay đổi cơ chế chú ý đầy đủ trong LLMs.

Nén Gợi ý Nén gợi ý tăng cường hiệu quả xử lý đầu vào LLM bằng cách hoặc là ngưng tụ các gợi ý dài (nén gợi ý cứng) hoặc học các biểu diễn gợi ý nhỏ gọn (nén gợi ý mềm). Các kỹ thuật nén gợi ý cứng bao gồm RAG Ding et al. [2024], LLMlingua Jiang et al. [2023a], Selective-Context Li [2023], và LongLLMlingua Jiang et al. [2023b]. RAG tối ưu hóa đầu vào bằng cách chỉ truy xuất các đoạn liên quan đến truy vấn, trong khi LLMlingua và Selective-Context nén ngữ cảnh dài mà không tham khảo truy vấn. LongLLMlingua kết nối những phương pháp này bằng cách sử dụng nén thô đến tinh biết câu hỏi, sắp xếp lại tài liệu, tỷ lệ động và phục hồi chuỗi phụ để cải thiện hiệu suất. Tuy nhiên, những phương pháp này tách nén và suy luận thành các bước riêng biệt, dẫn đến lan truyền lỗi tiềm năng làm giảm hiệu suất. Ngược lại, E2LLM được huấn luyện đầu-cuối, hiệu quả giảm thiểu vấn đề trên.

Nén gợi ý mềm, được đề xuất bởi Mu et al. [2023] và Ge et al. [2023], bao gồm việc huấn luyện LLMs để chưng cất gợi ý thành một tập hợp token ngắn gọn hơn đóng gói kiến thức của gợi ý gốc để sử dụng trong tương lai. Chevalier et al. [2023] mở rộng điều này bằng cách phát triển AutoCompressor, chuyển đổi ngữ cảnh văn bản dài hơn thành vector tóm tắt phục vụ như gợi ý mềm, mở rộng cửa sổ ngữ cảnh của LLM và giảm chi phí tính toán, như được minh họa trong LLoCO Tan et al. [2024]. Tuy nhiên, việc trực tiếp sử dụng LLMs để tạo nhúng cấp độ câu khác với mục tiêu ban đầu của chúng là dự đoán token tiếp theo. Kết quả là, việc đạt được hiệu suất thỏa đáng trong ngữ cảnh này thường đòi hỏi huấn luyện hoặc tinh chỉnh rộng rãi để căn chỉnh mô hình với mục tiêu mới. Để khắc phục vấn đề này, E2LLM của chúng tôi tận dụng mô hình nhúng câu được huấn luyện trước để biểu diễn gợi ý, căn chỉnh với các mục tiêu huấn luyện ban đầu của mô hình nhúng.

3 Phương pháp của chúng tôi: E2LLM
Trong phần này, chúng tôi trình bày chi tiết khung E2LLM được đề xuất để hiểu và lập luận về ngữ cảnh dài, hiệu quả kết hợp điểm mạnh của bộ mã hóa văn bản được huấn luyện trước và bộ giải mã LLM.

3.1 Kiến trúc Mô hình
Hình 1 minh họa kiến trúc của khung E2LLM, bao gồm bốn thành phần chính: Bộ chia đoạn, Bộ mã hóa văn bản Eθ, Bộ điều hợp Aϕ, và Bộ giải mã LLM Dη. Ở đây, θ, ϕ, và η biểu thị các tham số (có thể học) cụ thể cho mỗi thành phần. Điều quan trọng cần lưu ý là việc lựa chọn mô hình cho bộ mã hóa và bộ giải mã, phương pháp chia đoạn, và kiến trúc mạng của bộ điều hợp có thể được tùy chỉnh để đáp ứng nhu cầu của các lĩnh vực khác nhau. E2LLM phục vụ như một khung linh hoạt, tích hợp liền mạch những thành phần này để quản lý hiệu quả ngữ cảnh dài trong khi có khả năng tận dụng sức mạnh của các thành phần tiên tiến hơn khi có sẵn. Bây giờ chúng tôi sẽ giới thiệu chi tiết từng thành phần, theo dòng chảy dữ liệu trong quá trình suy luận trong E2LLM.

Bộ chia đoạn Bộ chia đoạn chịu trách nhiệm chia ngữ cảnh dài thành các đoạn nhỏ hơn, dễ quản lý trong khi đảm bảo rằng độ dài token của mỗi đoạn không vượt quá độ dài chuỗi tối đa của bộ mã hóa văn bản. Tương tự như RAG, việc lựa chọn chiến lược chia đoạn có thể ảnh hưởng đến hiệu suất tổng thể của E2LLM. Ở đây, chúng tôi áp dụng một phương pháp đơn giản nhưng hiệu quả: chúng tôi đầu tiên xác định kích thước đoạn, trích xuất đoạn ban đầu, và sau đó lùi lại trong đoạn này để định vị các điểm ngắt, như dấu chấm hoặc ngắt dòng. Tiếp theo, chúng tôi bắt đầu một đoạn mới ở cuối đoạn trước và áp dụng phương pháp lùi lại một lần nữa. Chúng tôi lặp lại quá trình này cho đến khi tất cả văn bản được chia đoạn. Phương pháp này giúp duy trì tính toàn vẹn ngữ nghĩa của văn bản gốc. Lưu ý rằng các phương pháp khác như giới thiệu chồng lấp giữa các đoạn cũng có thể có lợi cho E2LLM. Ngoài ra, kích thước của các đoạn là quan trọng đối với hiệu suất của E2LLM. Các thí nghiệm của chúng tôi chỉ ra rằng việc bao gồm ngữ cảnh quá mức trong một đoạn duy nhất có thể làm giảm hiệu suất, chủ yếu vì tỷ lệ nén cao có thể làm cho vector nhúng trở nên quá chung chung, làm tổn hại tính cụ thể.

Bộ mã hóa văn bản E Sau khi chia đoạn, chúng tôi đưa mỗi đoạn vào bộ mã hóa văn bản để tạo ra vector nhúng tương ứng. Đáng chú ý, hầu hết các bộ mã hóa được huấn luyện trước, như GTE Li et al. [2023] và BGE Xiao et al. [2023], được huấn luyện thông qua học tương phản. Điều này có nghĩa là token [CLS], phục vụ như vector nhúng, thường chỉ nắm bắt thông tin phân biệt cần thiết để phân biệt giữa các đoạn, trong khi thông tin cần thiết cho bộ giải mã LLM để trả lời truy vấn có thể bị loại bỏ. Để giảm thiểu hạn chế này, chúng tôi áp dụng thích ứng thấp-hạng (LoRA) Hu et al. [2021] để làm cho bộ mã hóa văn bản có thể huấn luyện trong quá trình căn chỉnh. Điều này cho phép bộ mã hóa bảo tồn thông tin có lợi cho hiệu suất của LLM.

Bộ điều hợp A Để hỗ trợ việc hiểu của LLM về ngữ nghĩa theo đoạn được suy ra từ đầu ra của bộ mã hóa, chúng tôi sử dụng Bộ điều hợp để ánh xạ đầu ra của bộ mã hóa vào nhúng đầu vào của LLM. Vì các chiều ẩn của bộ mã hóa văn bản và bộ giải mã LLM có thể khác nhau, Bộ điều hợp là một thành phần quan trọng. Cụ thể, chúng tôi sử dụng Mạng Perceptron Nhiều lớp (MLP) hai lớp với hàm kích hoạt GELU Hendrycks và Gimpel [2016] như mạng bộ điều hợp. Bộ điều hợp này được áp dụng cho từng nhúng đoạn riêng lẻ, và chúng tôi gọi đầu ra của nó là token đoạn hoặc gợi ý mềm, sau đó được xử lý bởi LLM tiếp theo. Bộ điều hợp được khởi tạo ngẫu nhiên và huấn luyện từ đầu trong giai đoạn căn chỉnh.

--- TRANG 5 ---
arXiv Template BẢN THẢO TRƯỚC

Bộ giải mã LLM D Cuối cùng, chúng tôi nối các token đoạn (các token màu xanh lá cây trong Hình 2) và các token văn bản tương ứng với gợi ý và truy vấn lại với nhau, và yêu cầu LLM tạo ra câu trả lời cho truy vấn. Trong các thí nghiệm của chúng tôi, chúng tôi chọn Llama2 Touvron et al. [2023] làm Bộ giải mã LLM do việc sử dụng rộng rãi của nó trong cả nghiên cứu học thuật và ứng dụng công nghiệp. Ngoài ra, chúng tôi sử dụng LoRA để tiếp tục huấn luyện Bộ giải mã như một phần của quá trình căn chỉnh giữa bộ mã hóa và bộ giải mã.

3.2 Tác vụ Huấn luyện
Bây giờ chúng tôi tập trung vào việc huấn luyện hai lớp cuối của bộ mã hóa, bộ điều hợp, và nhánh LoRA của bộ giải mã để tăng cường khả năng hiểu ngữ cảnh đầu vào dài và lập luận hiệu quả về các câu trả lời tương ứng của E2LLM. Để hoàn thành điều này, chúng tôi giới thiệu hai tác vụ huấn luyện riêng biệt.

Tác vụ đầu tiên được thiết kế để cải thiện việc hiểu đầu vào của LLM. Như được mô tả trong Hình 2, một khi LLM nhận được token đoạn từ bộ điều hợp, chúng tôi gợi ý nó để trình bày lại hoặc tái tạo đầu vào. Chúng tôi gọi đây là tác vụ "hiểu". Gợi ý cụ thể được sử dụng là "Với các ngữ cảnh: [token đoạn] \nVui lòng làm theo hướng dẫn: \nTrình bày lại ngữ cảnh đã nói ở trên". Đáng chú ý, tác vụ này là tự giám sát, cho phép chúng tôi tuyển chọn một lượng lớn dữ liệu huấn luyện để đảm bảo rằng LLM hiểu toàn diện các nhúng được cung cấp bởi bộ điều hợp. Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng đầu vào từ dữ liệu tinh chỉnh hướng dẫn ngữ cảnh dài cho tác vụ này. Với việc những đầu vào này thường quá dài để được tái tạo đầy đủ một lần, chúng tôi sử dụng phương pháp cửa sổ trượt, tái tạo ngữ cảnh gốc theo từng đoạn dựa trên một vài đoạn liên tiếp cho đến khi toàn bộ đầu vào đã được trình bày lại.

Mặt khác, tác vụ huấn luyện thứ hai cho phép LLM tạo ra câu trả lời dựa trên token đoạn (tức là ngữ cảnh dài) và truy vấn của người dùng. Chúng tôi gọi đây là tác vụ "lập luận", và gợi ý được tạo cho mục đích này là "Với các ngữ cảnh: [token đoạn] \nVui lòng làm theo hướng dẫn: \nTrả lời câu hỏi: {truy vấn}".

Độ dài Chuỗi Tối đa Về mặt lý thuyết, độ dài chuỗi tối đa của E2LLM bằng tích của độ dài chuỗi của bộ mã hóa và bộ giải mã. Tuy nhiên, như đã đề cập trước đây, việc đặt kích thước đoạn để phù hợp với độ dài chuỗi của bộ mã hóa đặt ra những thách thức, vì nó có thể cản trở khả năng của bộ mã hóa giữ lại tất cả thông tin liên quan trong một đoạn duy nhất. Do đó, việc thiết lập kích thước đoạn phù hợp là quan trọng. Vì vậy, độ dài chuỗi hiệu quả của E2LLM được xác định là kích thước đoạn nhân với độ dài chuỗi của bộ giải mã LLM. Trong thực tế, chúng tôi đặt kích thước đoạn tối đa là 512 ký tự trong các thí nghiệm, tương đương với khoảng 100 token. Do đó, độ dài ngữ cảnh đã được mở rộng gần 100 lần.

Độ phức tạp Thời gian và Không gian trong Suy luận Hãy ký hiệu độ dài đầu vào ban đầu là L và kích thước đoạn trong E2LLM là C. Do đó, tổng số đoạn trở thành L/C. Đối với mỗi đoạn, độ phức tạp thời gian và không gian kết quả từ bộ mã hóa văn bản là O(C2). Với việc có L/C đoạn, độ phức tạp tổng thể cho bước mã hóa là O(CL). Trong thực tế, vì tất cả các đoạn có thể được xử lý song song, độ phức tạp thời gian có thể được giảm thêm bởi một yếu tố hằng số. Tiếp theo, chúng tôi truyền L/C token đoạn cho bộ giải mã LLM, mang lại độ phức tạp O(L2/C2). Tóm lại, tổng độ phức tạp thời gian và không gian là O(LC+L2/C2). Để chứng thực hiệu quả của E2LLM trong quá trình suy luận, chúng tôi tiến hành các thí nghiệm thực nghiệm đánh giá cả thời gian suy luận và sử dụng bộ nhớ. Chi tiết thêm có thể được tìm thấy trong Phần 4.5.

3.3 Quan hệ với Các Phương pháp Khác
Quan hệ với VLMs E2LLM được lấy cảm hứng từ những tiến bộ gần đây trong Mô hình Thị giác-Ngôn ngữ (VLMs) Zhang et al. [2024], bao gồm mini-GPT4 Zhu et al., LLaVa Liu et al. [2024], Qwen-VL Bai et al. [2023b], và InternVL Chen et al. [2024]. Những VLMs này sử dụng bộ điều hợp để căn chỉnh bộ mã hóa thị giác được huấn luyện trước với bộ giải mã LLM, cho phép LLMs xử lý token hình ảnh được xuất ra bởi bộ mã hóa thị giác. Trong khung này, cả bộ mã hóa thị giác và bộ giải mã LLM đều được huấn luyện trước độc lập, cung cấp một phương pháp linh hoạt cho phép căn chỉnh các mô hình thị giác và ngôn ngữ hiệu suất cao, do đó tối đa hóa khả năng của chúng. Đáng chú ý, VLMs xuất sắc trong việc thực hiện các tác vụ OCR (Nhận dạng Ký tự Quang học) Islam et al. [2017], hiệu quả nhận dạng và xuất ra văn bản có mặt trong hình ảnh. Được thúc đẩy bởi thành công của VLMs, chúng tôi đề xuất rằng bằng cách căn chỉnh bộ mã hóa văn bản (tức là mô hình nhúng) với bộ giải mã LLM sử dụng bộ điều hợp, LLMs có thể tương tự diễn giải các câu được mã hóa bởi bộ mã hóa văn bản và rút ra suy luận dựa trên sự hiểu biết này. Hơn nữa, vì cả bộ mã hóa và bộ giải mã trong phương pháp của chúng tôi hoạt động trong cùng một phương thức, chúng tôi dự đoán rằng quá trình căn chỉnh sẽ đơn giản hơn so với yêu cầu đối với các mô hình hoạt động trên các phương thức khác nhau, có thể giảm lượng dữ liệu cần thiết để căn chỉnh. Ngược lại, tác vụ tái tạo được sử dụng trong việc huấn luyện E2LLM là tự giám sát, cho phép chúng tôi tích lũy một bộ dữ liệu văn bản khổng lồ để tăng cường hiểu biết ngữ cảnh của LLM. Ngược lại, tác vụ căn chỉnh trong VLMs dựa vào các cặp hình ảnh-văn bản có giám sát, đáng chú ý khó thu thập hơn.

Quan hệ với RAG RAG (Tạo có Hỗ trợ Truy xuất) Ding et al. [2024] thường hoạt động bằng cách sử dụng bộ truy xuất dựa trên bộ mã hóa văn bản để xác định các đoạn liên quan từ cơ sở kiến thức để đáp ứng truy vấn của người dùng. Những văn bản được truy xuất này sau đó được đưa vào LLM (tức là bộ tạo) để tăng cường phản hồi. RAG có thể tăng cường E2LLM bằng cách truy xuất các đoạn liên quan nhất, trong khi E2LLM có thể mở rộng độ dài ngữ cảnh của bộ tạo trong RAG. Ngoài ra, một thách thức đáng kể đối với RAG là sự không nhất quán có thể phát sinh giữa cách bộ truy xuất và bộ tạo diễn giải cùng một văn bản Li et al. [2024], Ding et al. [2024]. E2LLM giải quyết vấn đề này bằng cách căn chỉnh bộ truy xuất (bộ mã hóa văn bản) với bộ tạo (bộ giải mã LLM), tạo điều kiện cho sự mạch lạc tốt hơn trong diễn giải. Hơn nữa, E2LLM cho phép giao tiếp hiệu quả hơn giữa bộ truy xuất và bộ tạo thông qua vector nhúng thay vì văn bản thô.

Quan hệ với LLoCO So với E2LLM, LLoCO Tan et al. [2024] sử dụng AutoCompressor Chevalier et al. [2023] làm bộ mã hóa văn bản dài và bỏ qua bộ điều hợp vì nó sử dụng cùng LLM (tức là Llama2) như AutoCompressor. Kết quả là, nó có thể hiểu hiệu quả các vector tóm tắt—tương tự như token đoạn hoặc gợi ý mềm—được tạo ra bởi AutoCompressor sau khi tinh chỉnh LLM với LoRA. Một lợi thế của LLoCO là bộ mã hóa văn bản của nó, AutoCompressor, xem xét sự phụ thuộc lẫn nhau của các đoạn ngữ cảnh dài. Tuy nhiên, điều này cũng đặt ra một hạn chế: ngữ cảnh dài chỉ có thể được xử lý tuần tự, từng đoạn một. Ngược lại, E2LLM có thể xử lý tất cả các đoạn song song và phù hợp hơn cho ngữ cảnh dài. Hơn nữa, bộ mã hóa bị hạn chế với AutoCompressor, làm cho việc nâng cao hiệu suất của LLoCO trở nên thách thức mà không cập nhật AutoCompressor. Đáng chú ý, AutoCompressor đòi hỏi một quá trình tinh chỉnh rộng rãi trên Llama2 gốc sử dụng 2 tỷ token để cho phép Llama2 tạo ra token tóm tắt. Ngược lại, E2LLM được thiết kế để dễ dàng kết hợp các bộ mã hóa văn bản và bộ giải mã LLM mạnh hơn khi chúng có sẵn trong năm.

4 Thí nghiệm
Trong phần này, chúng tôi đánh giá hiệu suất của E2LLM trên hai tác vụ chính, bao gồm trả lời câu hỏi tài liệu (QA) và tóm tắt tài liệu. Để so sánh, chúng tôi đánh giá chuẩn E2LLM với bốn đường cơ sở, bao gồm Yarn Peng et al. [2023], LongLoRA Chen et al. [2023b], RAG Gao et al. [2024], và LLoCO Tan et al. [2024], tương ứng đại diện cho các phương pháp tiên tiến (SOTA) trong mở rộng độ dài, chú ý thưa thớt, nén gợi ý cứng, và nén gợi ý mềm. Lưu ý rằng ngoại trừ RAG không cần huấn luyện, chúng tôi đặt hạng của LoRA cho LLM giống nhau trong tất cả các phương pháp, dẫn đến 17M tham số có thể huấn luyện trong YaRN và LLoCO và 140M cho LongLoRA.

Bảng 1: Thống kê Bộ dữ liệu.
Bộ dữ liệu Loại Tác vụ #Mẫu Huấn luyện #Mẫu Đánh giá Độ dài Mẫu
QMSum Tóm tắt 1,257 272 14,428.78
GovReport Tóm tắt 10,000 500 11,204.00
Quality DocumentQA 5,046 2,086 6,797.66
NarrativeQA DocumentQA 3,000 200 52,158.88
TriviaQA DocumentQA 10,000 500 1,075.90

4.1 Mô tả Bộ dữ liệu
Để đánh giá hiệu quả của E2LLM, chúng tôi tận dụng năm bộ dữ liệu có sẵn công khai bao gồm cả tác vụ Tóm tắt và Trả lời Câu hỏi Tài liệu (DocumentQA). Thống kê dữ liệu được hiển thị trong Bảng 1.

•QMSum3Zhong et al. [2021] là một điểm chuẩn mới được thiết kế, được chú thích bởi con người để tóm tắt cuộc họp đa lĩnh vực dựa trên truy vấn. Nó bao gồm một phạm vi rộng lớn các cặp truy vấn-tóm tắt trên 232 cuộc họp trong các lĩnh vực đa dạng. Cụ thể, chúng tôi bao gồm 1,257 mẫu huấn luyện và sử dụng 272 mẫu để suy luận. Độ dài trung bình của các mẫu trong bộ dữ liệu này là 14,428.78 token.

•GovReport4Huang et al. [2021] chứa các báo cáo dài của Văn phòng Trách nhiệm Chính phủ Hoa Kỳ và Dịch vụ Nghiên cứu Quốc hội, được bổ sung bằng tóm tắt và trích yếu được viết tay bởi các chuyên gia, thuộc thể loại tác vụ tóm tắt. Để huấn luyện, 10,000 mẫu ngẫu nhiên được sử dụng, và để suy luận, 500 mẫu được chọn tùy ý từ tập xác thực. Độ dài trung bình của dữ liệu được lấy mẫu là 11,204.00 token.

3https://github.com/Yale-LILY/QMSum
4https://huggingface.co/datasets/ccdv/govreport-summarization

--- TRANG 7 ---
arXiv Template BẢN THẢO TRƯỚC

•Quality5Bowman et al. [2022] là một bộ dữ liệu DocumentQA bao gồm 5,046 mẫu huấn luyện và 2,086 mẫu suy luận với ngữ cảnh có độ dài trung bình 6,797.66 token. Hơn nữa, chúng tôi chuyển đổi định dạng dữ liệu lựa chọn đơn ban đầu của bộ dữ liệu thành định dạng QA.

•NarrativeQA6Kovcisky et al. [2018] là một bộ dữ liệu DocumentQA khác, chủ yếu được trích xuất từ văn bản sách toàn diện và kịch bản phim từ các nguồn đa dạng. Thách thức ở đây nằm ở việc tạo ra câu trả lời ngắn gọn từ các văn bản có thể không có thứ tự và dài hơn. Chúng tôi lấy mẫu ngẫu nhiên 3,000 mẩu dữ liệu để huấn luyện, trong khi chọn ngẫu nhiên 200 mẫu để suy luận. Độ dài mẫu trung bình là 52,158.88 token.

•TriviaQA7Joshi et al. [2017] cũng là một bộ dữ liệu DocumentQA chất lượng cao chứa hơn 650K bộ ba câu hỏi-trả lời-bằng chứng. Nó bao gồm 95K cặp câu hỏi-trả lời được tác giả bởi những người đam mê câu đố và tài liệu bằng chứng được lấy nguồn độc lập. Chúng tôi chọn 10,000 và 500 mẫu để huấn luyện và suy luận tương ứng, với độ dài mẫu trung bình lên đến 1,075.90 token.

4.2 Chỉ số Đánh giá
Đối với tác vụ Tóm tắt, hiệu suất của tất cả các phương pháp được đo bằng chỉ số Rouge Lin [2004], hoạt động bằng cách so sánh n-gram của văn bản được tạo với văn bản tham chiếu. Cụ thể, chúng tôi tận dụng Rouge-1, Rouge-2, và Rouge-L để đánh giá sự chồng lấp giữa token đơn, token kép liên tiếp, và chuỗi con chung dài nhất (LCS) trong văn bản được tạo bởi LLM và văn bản tham chiếu. Chúng tôi cũng tính trung bình hình học của chúng, ký hiệu là G-mean, và giá trị cao hơn phản ánh chất lượng cao hơn của tóm tắt được tạo.

Liên quan đến tác vụ DocumentQA, chúng tôi áp dụng phương pháp được chứng minh bởi Shaham et al. [2023], tính sự chồng lấp unigram giữa câu trả lời được tạo và tham chiếu. Điều này được thực hiện bằng cách chuẩn hóa khoảng trắng, viết thường, loại trừ từ dừng và dấu câu. Dựa trên số lượng token unigram, kết hợp với số lượng token của câu trả lời được tạo và tham chiếu, chúng tôi tính độ chính xác, độ nhớ lại, và F1. Một lần nữa, giá trị cao hơn chỉ ra câu trả lời chính xác hơn bởi mô hình.

4.3 Đường cơ sở và Chi tiết Triển khai
Ở đây chúng tôi mô tả các đường cơ sở và chi tiết triển khai của chúng như sau:

•Llama2-7B Touvron et al. [2023] ám chỉ phiên bản 7B gốc của mô hình Llama2 mà không tinh chỉnh. Chúng tôi sử dụng Llama2-7b-Chat8 cho thí nghiệm và sử dụng nó làm xương sống của các phương pháp khác.

•YaRN Peng et al. [2023] đề xuất một phương pháp nội suy phân đoạn mới dựa trên chu kỳ của các chiều khác nhau để mở rộng kích thước cửa sổ ngữ cảnh. Chúng tôi đặt hệ số tỷ lệ thành 16, và LoRA Hu et al. [2021] được áp dụng trên mô-đun tự chú ý với hạng 16. Số lượng tham số có thể huấn luyện là 17M.

•LongLoRA Chen et al. [2023b] đề xuất sử dụng chú ý ngắn dịch chuyển thay vì chú ý đầy đủ ban đầu trong quá trình huấn luyện, và sử dụng Nội suy Vị trí Chen et al. [2023a] và LoRA để tinh chỉnh LLM để mở rộng cửa sổ ngữ cảnh. Chúng tôi đặt hạng LoRA thành 16, và tinh chỉnh các mô-đun tự chú ý, nhúng, và chuẩn hóa trong quá trình huấn luyện theo bài báo gốc, dẫn đến 140M tham số có thể huấn luyện tổng thể.

•RAG Gao et al. [2024] bao gồm hai quá trình cốt lõi: truy xuất và tạo. Trong giai đoạn truy xuất, chúng tôi áp dụng GTE-Large-en Li et al. [2023] làm bộ truy xuất và thu hồi 40 đoạn ngữ cảnh liên quan hàng đầu với độ dài tối đa 512 ký tự dựa trên độ tương tự cosine, và sau đó chúng được tận dụng làm gợi ý cho LLMs trong quá trình tạo, và nó không liên quan đến quá trình huấn luyện.

•LLoCO Tan et al. [2024] là một phương pháp nén mềm sử dụng Autocompressors Chevalier et al. [2023] để mã hóa ngữ cảnh dài ngoại tuyến để đạt được mở rộng cửa sổ ngữ cảnh. Nhất quán với các phương pháp khác, chúng tôi sử dụng LoRA trên mô-đun tự chú ý với hạng 16, dẫn đến số lượng tham số có thể huấn luyện là 17M.

•E2LLM Đối với E2LLM của chúng tôi, chúng tôi sử dụng GTE-Large-en Li et al. [2023] làm bộ mã hóa, sau đó được tinh chỉnh sử dụng LoRA với tham số hạng được đặt thành 8. Ngoài ra, chúng tôi sử dụng mạng mlp hai lớp với hàm kích hoạt GeLU Hendrycks và Gimpel [2016] làm bộ điều hợp. Đối với thành phần bộ giải mã, chúng tôi tận dụng Llama2-7B-Chat, cũng tinh chỉnh nó thông qua LoRA với hạng 8, và số lượng tham số có thể huấn luyện cuối cùng là 16M.

5https://huggingface.co/datasets/emozilla/quality
6https://github.com/google-deepmind/narrativeqa
7https://huggingface.co/datasets/mandarjoshi/trivia_qa
8https://huggingface.co/meta-llama/Llama-2-7b-chat-hf

--- TRANG 8 ---
arXiv Template BẢN THẢO TRƯỚC

Bảng 2: Hiệu suất trên các bộ dữ liệu Ngữ cảnh Dài.

Tham số
Có thể huấn luyệnCửa sổ
Ngữ cảnhPhương pháp
Mở rộngQmsum GovReport Quality NarrativeQA TriviaQA
R1 R2 RL G-mean R1 R2 RL G-mean Prec. Recall F1 Prec. Recall F1 Prec. Recall F1

Llama2-7B 0M 4K - 0.2190 0.0491 0.1421 0.1151 0.1068 0.0286 0.0546 0.0550 0.0616 0.2546 0.0938 0.0304 0.1352 0.0465 0.0672 0.7666 0.1206
LongLoRA 140M 100K Sparse Attn. 0.0741 0.0999 0.0765 0.0741 0.2704 0.0992 0.1629 0.1635 0.0741 0.0999 0.0765 OOM OOM OOM 0.1303 0.4928 0.1969
YaRN 17M 64K Len. Ext. 0.2154 0.0534 0.1624 0.1231 0.1293 0.0413 0.0569 0.0672 0.1320 0.1942 0.1380 OOM OOM OOM 0.1353 0.4945 0.2022
RAG 0M +∞ Hard Comp. 0.0981 0.0172 0.0799 0.0512 0.0893 0.0229 0.0490 0.0465 0.0341 0.3408 0.0583 0.0041 0.0097 0.0055 0.0582 0.6743 0.1047
LLoCO 17M 128K Soft Comp. 0.2371 0.0551 0.1679 0.1299 0.1169 0.0311 0.0518 0.0573 0.1681 0.1503 0.1437 0.1185 0.1134 0.1087 0.6404 0.6403 0.6321
Ours 16M 400K Soft Comp. 0.2537 0.0655 0.1875 0.1461 0.3314 0.1075 0.1859 0.1878 0.1344 0.1495 0.1294 0.1353 0.1379 0.1235 0.3322 0.3451 0.3337

4.4 So sánh Hiệu suất
Ở đây, chúng tôi sử dụng ba bộ dữ liệu cho QA, đó là Quality, NarrativeQA, và TriviaQA, cũng như hai bộ dữ liệu cho tóm tắt, đó là QMsum và GovReport. Chi tiết của những bộ dữ liệu này được tóm tắt trong Bảng 1. Lưu ý rằng Quality và TriviaQA có độ dài ngắn hơn so với các bộ dữ liệu khác, nhưng NarrativeQA có độ dài dài hơn. Đối với các thí nghiệm của chúng tôi, chúng tôi sử dụng tập xác thực của mỗi bộ dữ liệu để thử nghiệm, và chúng tôi chia tập huấn luyện thành tập con huấn luyện và xác thực sử dụng tỷ lệ 95:5. Chúng tôi tiếp tục bao gồm LLama2-7B-chat gốc làm đường cơ sở. Để đánh giá hiệu suất, chúng tôi sử dụng điểm Rouge-N cho các tác vụ tóm tắt, trong khi đối với các tác vụ QA, chúng tôi sử dụng các chỉ số Độ chính xác, Độ nhớ lại, và Điểm F1 (xem định nghĩa liên quan trong Phần 4.2). Kết quả cho tất cả các phương pháp đường cơ sở được trình bày trong Bảng 2.

Rõ ràng là E2LLM được đề xuất nhất quán đạt được hiệu suất tốt nhất hoặc tốt thứ hai trên tất cả các phương pháp được đánh giá. Thú vị, Yarn cho thấy kết quả hứa hẹn trong cả hai tác vụ, tuy nhiên, nó gặp phải vấn đề hết bộ nhớ (OOM) khi độ dài chuỗi vượt quá khoảng 74,000 trên GPU A100 với 80G, hệ quả của độ phức tạp không gian bậc hai vốn có trong LLMs. Hơn nữa, hiệu suất của Yarn giảm sút trên các chuỗi ngắn hơn, như được tìm thấy trong TriviaQA. Như được ghi nhận trong nghiên cứu trước Chen et al. [2023a], các cơ chế chú ý có thể trở nên phân tán trong ngữ cảnh cực kỳ dài, lan rộng quá mỏng qua nhiều vị trí token và do đó làm giảm hiệu suất trên ngữ cảnh ngắn hơn. Về một khía cạnh liên quan, LongLoRA, sử dụng chú ý thưa thớt dịch chuyển trong quá trình huấn luyện, thể hiện những thách thức tương tự. Nó có xu hướng bỏ qua lịch sử thông tin, dẫn đến hiệu suất kém hơn so với E2LLM trên tất cả các bộ dữ liệu. LongLoRA cũng gặp phải vấn đề OOM với bộ dữ liệu NarrativeQA rộng lớn do việc sử dụng chú ý đầy đủ trong quá trình suy luận. Mặt khác, Llama2-7B-Chat gốc hoạt động kém trong các tác vụ QA, chủ yếu do độ dài ngữ cảnh hạn chế 4K token. RAG cho thấy hiệu suất tệ nhất trong các tác vụ QA, có thể đến từ việc mất thông tin quan trọng cần thiết cho truy vấn cụ thể, bên cạnh đó, nó hoạt động kém hơn Llama2-7B gốc trong tác vụ tóm tắt vì nhiễu được đưa vào trong quá trình truy xuất có thể ảnh hưởng bất lợi đến khả năng tạo của LLMs Wu et al. [2024]. So với Yarn, LongLoRA, và RAG, E2LLM chứng minh khả năng vượt trội trong việc phân biệt thông tin cần thiết từ ngữ cảnh dài trong khi hiệu quả loại bỏ dữ liệu không liên quan.

Cuối cùng, chúng tôi quan sát rằng LLoCO hoạt động khá tốt trong các tác vụ QA, đặc biệt đối với Quality và TriviaQA, có ngữ cảnh tương đối ngắn. Tuy nhiên, hiệu suất của nó giảm đáng kể trong các tác vụ tóm tắt, phù hợp với các phát hiện trong ấn phẩm riêng của nó (xem Bảng 1 trong Tan et al. [2024]). LLoCO sử dụng AutoCompressor Chevalier et al. [2023] làm bộ mã hóa văn bản của nó, sử dụng Llama2 để tạo ra vector tóm tắt cho mỗi đoạn. Những vector này được thiết kế để chỉ giữ lại thông tin liên quan đến các đoạn tiếp theo, loại bỏ nội dung có giá trị khác, như được chỉ ra trong Rau et al. [2024]. Trong các tác vụ QA, chỉ các phần liên quan của ngữ cảnh dài được yêu cầu để gợi ý LLM cho câu trả lời chính xác, điều này phù hợp hợp lý với các mục tiêu huấn luyện của AutoCompressor. Tuy nhiên, các tác vụ tóm tắt đòi hỏi hiểu biết toàn diện về toàn bộ ngữ cảnh dài. Vì các vector tóm tắt được tạo bởi AutoCompressor không đóng gói tất cả thông tin trong mỗi đoạn, hiệu suất của LLoCO trong tóm tắt bị giảm sút. Ngược lại, E2LLM xuất sắc trong việc nắm bắt thông tin trên tất cả các đoạn do tác vụ "hiểu" hoặc tái tạo độc đáo của nó, góp phần vào hiệu suất vượt trội trong tóm tắt.

4.5 Hiệu quả Suy luận
Trong phần này, chúng tôi xem xét hiệu quả suy luận của các phương pháp khác nhau. Chúng tôi bắt đầu bằng cách chọn bảy độ dài ngữ cảnh được phân phối đều giữa 1K và 73K, vì cả Yarn và LongLoRA gặp phải vấn đề hết bộ nhớ (OOM) ở độ dài ngữ cảnh 74K. Đối với mỗi độ dài ngữ cảnh được chọn, chúng tôi chọn ngẫu nhiên mười mẫu và cắt ngắn chúng đến độ dài được xác định trước. Sau đó chúng tôi tính trung bình thời gian chạy và chi phí bộ nhớ GPU trên mười mẫu này và minh họa kết quả như một hàm của độ dài ngữ cảnh trong Hình 3.

Mô hình của chúng tôi, E2LLM, thể hiện thời gian chạy và sử dụng bộ nhớ thấp nhất, đặc biệt đối với các chuỗi rất dài ở 73K. Ngược lại, cả YaRN và LongLoRA đều chứng minh mức tiêu thụ tài nguyên cao hơn đáng kể, chủ yếu do độ phức tạp bậc hai của chú ý đầy đủ trong quá trình suy luận. Tương tự như E2LLM, LLoCO cũng giảm thời gian suy luận thông qua nén gợi ý mềm. Tuy nhiên, bộ mã hóa văn bản của nó, AutoCompressor, có thể nén văn bản gốc tối đa 32 lần, so với khả năng nén của E2LLM khoảng 100 lần. Ngoài ra, AutoCompressor xử lý tất cả các đoạn tuần tự, trong khi E2LLM có thể xử lý chúng song song, tiếp tục giảm thời gian suy luận. Đáng chú ý rằng bộ truy xuất trong mô hình RAG truy xuất 40 đoạn liên quan nhất từ ngữ cảnh dài, bất kể độ dài ngữ cảnh. Do đó, thời gian suy luận của nó không phụ thuộc vào độ dài ngữ cảnh nhưng phát sinh chi phí thời gian lớn hơn E2LLM do quá trình truy xuất bổ sung. Hơn nữa, việc chỉ truy xuất 40 đoạn, bất kể độ dài ngữ cảnh, có thể ảnh hưởng tiêu cực đến hiệu suất, đặc biệt trong các tác vụ tóm tắt, như được chứng minh trong Bảng 2.

--- TRANG 9 ---
arXiv Template BẢN THẢO TRƯỚC

1 13 25 37 49 61 73
Độ dài Chuỗi(K)0123456789101112Chi phí Thời gian(s)

1 13 25 37 49 61 73
Độ dài Chuỗi(K)-10123456Log của Sử dụng Bộ nhớ(GB)

LongLoRA YaRN RAG LLoCO E2LLM

Hình 3: Hiệu suất về hiệu quả suy luận, bao gồm tiêu thụ thời gian chạy (trái) và sử dụng bộ nhớ gpu (phải).

4.6 Nghiên cứu Ablation
Trong phần này, chúng tôi tiến hành nghiên cứu ablation của E2LLM sử dụng các bộ dữ liệu QMSum và NarrativeQA, phục vụ như điểm chuẩn đại diện cho các tác vụ tóm tắt ngữ cảnh dài và trả lời câu hỏi tài liệu, tương ứng. Chi tiết của mỗi biến thể được xem xét trong Bảng 3 được nêu dưới đây.

•Biến thể −und đòi hỏi loại trừ tác vụ "hiểu" khỏi mô hình của chúng tôi và chỉ sử dụng tác vụ "lập luận" cho mục đích huấn luyện, nhấn mạnh vai trò quan trọng mà tác vụ "hiểu" đóng trong hiệu suất của mô hình.

•−E biểu thị việc đóng băng các tham số bộ mã hóa, do đó chỉ cho phép bộ điều hợp và LLM chỉ giải mã có thể huấn luyện. Cấu hình này nhằm chứng thực giả thuyết của chúng tôi rằng bộ mã hóa được huấn luyện trước một mình không có khả năng bảo tồn thông tin liên quan ảnh hưởng đáng kể đến hiệu suất của LLM. Do đó, việc duy trì các tham số của bộ mã hóa có thể huấn luyện là quan trọng.

•−D đòi hỏi giữ LLM chỉ giải mã đóng băng, để kiểm tra liệu LLM vẫn có thể hiểu đầy đủ các token đầu ra từ bộ điều hợp khi không có bất kỳ huấn luyện chuyên dụng nào.

•Biến thể +overlap giới thiệu chồng lấp 30% kích thước đoạn giữa các đoạn tuần tự trong quá trình chia đoạn. Hơn nữa, trong phạm vi hoạt động trình bày lại của tác vụ "hiểu", mô hình được yêu cầu trình bày lại phần chồng lấp của những đoạn này một lần.

•Biến thể +bge, mặt khác, bao gồm việc thay thế mô hình GTE-Large-en bằng mô hình bge-m3 làm bộ mã hóa. Nghiên cứu này tìm cách khẳng định rằng mô hình của chúng tôi duy trì khả năng tương thích với các mô hình nhúng câu khác nhau phục vụ như bộ mã hóa.

•Cấu hình +Llama2 −13B, tương tự trong thử nghiệm với biến thể +bge, được thiết kế để xác minh khả năng tương thích của E2LLM với các LLM khác phục vụ như bộ giải mã.

Đầu tiên, chúng tôi đánh giá tầm quan trọng của tác vụ "hiểu" trong E2LLM. Các phát hiện của chúng tôi chỉ ra sự giảm sút đáng kể về hiệu suất—16.39%—khi tác vụ này bị loại bỏ, nêu bật vai trò quan trọng của nó trong việc giúp E2LLM diễn giải các nhúng đoạn được tạo ra bởi bộ mã hóa và tiếp tục tăng cường hiệu suất của tác vụ "lập luận". Tiếp theo, chúng tôi xem xét sự cần thiết của việc huấn luyện các nhánh LoRA của bộ mã hóa và bộ giải mã trong quá trình căn chỉnh. Như được hiển thị trong Bảng 3, kết quả cho các cấu hình -E và -D nhấn mạnh tầm quan trọng của việc huấn luyện những thành phần này; không có huấn luyện này, hiệu suất của E2LLM giảm 9.08% và 12.03%, tương ứng. Cuối cùng, chúng tôi khám phá tác động của việc thay thế bộ chia đoạn, bộ mã hóa văn bản, và bộ giải mã LLM trong E2LLM (được ký hiệu là +overlap, +bge, và +Llama2-13B).

--- TRANG 10 ---
arXiv Template BẢN THẢO TRƯỚC

Bảng 3: Nghiên cứu Ablation trên QMSum và NarrativeQA.

QMsum NarrativeQA Trung bình
Khác biệt Tương đối R1 R2 RL G-mean Prec. Recall F1

E2LLM 0.2537 0.0655 0.1875 0.1461 0.1353 0.1379 0.1235 -
-Und 0.2264 0.0486 0.1620 0.1213 0.1113 0.1004 0.0994 -16.39%
-E 0.2343 0.0541 0.1731 0.1299 0.1247 0.1125 0.1083 -9.08%
-D 0.2309 0.0493 0.1711 0.1249 0.1223 0.1095 0.1046 -12.03%
+Overlap 0.2523 0.0639 0.1795 0.1425 0.1328 0.1394 0.1241 +1.78%
+BGE 0.2377 0.0607 0.1784 0.1370 0.1289 0.1203 0.1136 -4.33%
+Llama2-13B 0.2577 0.0672 0.1889 0.1484 0.1375 0.1374 0.1268 +4.70%

-1-2-3-4-5-6-7-8-9-10-11
Log (cơ số 10) của trọng số0.080.090.100.110.120.130.140.15Chỉ số
QMSum
NarrativeQA

0 4 8 12 16 20 24
Hạng LoRA Bộ mã hóa0.100.110.120.130.140.15Chỉ số
QMSum
NarrativeQA

0 2 4 6 8 10 12
Hạng LoRA Bộ giải mã0.100.110.120.130.140.15Chỉ số
QMSum
NarrativeQA

1 2 3
#Lớp Bộ điều hợp0.1100.1150.1200.1250.1300.1350.1400.1450.150Chỉ số
QMSum
NarrativeQA

Hình 4: Ảnh hưởng của siêu tham số. (a) trọng số mất mát của tác vụ "hiểu". (b) số lượng lớp trong mạng bộ điều hợp. (c) số lượng lớp có thể huấn luyện của bộ mã hóa.

Phân tích của chúng tôi tiết lộ rằng các bộ chia đoạn với các phân đoạn chồng lấp (ví dụ: chồng lấp 30%) cung cấp một sự cải thiện hiệu suất khiêm tốn. Ngoài ra, việc sử dụng các bộ mã hóa và bộ giải mã tiên tiến hơn tiếp tục tăng cường hiệu suất của E2LLM, gợi ý rằng các cải tiến trong các thành phần riêng lẻ có thể ảnh hưởng tích cực đến hệ thống tổng thể.

4.7 Độ nhạy Siêu tham số
Trong phần này, chúng tôi khám phá ảnh hưởng của các siêu tham số đến hiệu suất của E2LLM, cụ thể tập trung vào trọng số được gán cho tác vụ "hiểu", hạng LoRA của bộ mã hóa và bộ giải mã, và số lượng lớp trong mạng bộ điều hợp.

Trọng số được gán cho tác vụ "hiểu" chỉ ra tầm quan trọng tương đối của nó so với tác vụ "lập luận". Nhớ lại rằng ngữ cảnh đầu vào thường có độ dài dài hơn nhiều so với câu trả lời, làm cho nó quá dài để được tái tạo đầy đủ một lần. Để giải quyết điều này, chúng tôi sử dụng phương pháp cửa sổ trượt, tái tạo ngữ cảnh gốc theo từng đoạn dựa trên một vài đoạn liên tiếp cho đến khi toàn bộ đầu vào đã được tái tạo. Do đó, các mẫu cho tác vụ "hiểu" nhiều hơn đáng kể so với những mẫu cho các tác vụ "lập luận". Để duy trì cân bằng mẫu, chúng tôi thường gán trọng số nhỏ hơn cho tác vụ trình bày lại. Như được mô tả trong Hình 4, trọng số tối ưu có thể khác nhau giữa các bộ dữ liệu khác nhau, có thể bị ảnh hưởng bởi các yếu tố như độ dài ngữ cảnh và khả năng của mô hình nhúng câu hiểu ngữ nghĩa cụ thể của ngữ cảnh.

Hơn nữa, chúng tôi điều tra hạng LoRA tối ưu của bộ mã hóa (tức là GTE-Large-en) và bộ giải mã (tức là Llama2-7B-Chat) trong phạm vi {0, 4, 8, 12, 16, 20, 24} và {0, 2, 4, 6, 8, 10, 12}, tương ứng. Các phát hiện gợi ý rằng việc không có tham số có thể huấn luyện—nói cách khác, hoàn toàn "đóng băng" bộ mã hóa và bộ giải mã—cản trở việc trích xuất hiệu quả nội dung ngữ cảnh gốc và căn chỉnh giữa bộ mã hóa và bộ giải mã, như đã thảo luận trong Phần 3.1. Khi hạng của hai mô-đun tăng, một sự cải thiện tương ứng về hiệu suất được quan sát thấy, do đó nhấn mạnh tầm quan trọng của việc huấn luyện. Sự tăng cường hiệu suất tiếp tục cho đến khi đạt đỉnh trong một phạm vi hạng cụ thể. Tuy nhiên, vượt ra ngoài phạm vi tối ưu này, việc tăng hạng thêm nữa dẫn đến sự suy giảm hiệu suất, có thể quy cho việc overfitting trên các bộ dữ liệu huấn luyện.

Chúng tôi cũng xem xét tác động của số lượng lớp trong mạng bộ điều hợp. Hình 4 cho thấy rằng MLP hai lớp nhất quán mang lại hiệu suất vượt trội trên các bộ dữ liệu khác nhau, chỉ ra tính ổn định trong kết quả. Chúng tôi giả thuyết rằng MLP một lớp có thể gặp khó khăn với tác vụ căn chỉnh, trong khi MLP ba lớp có thể dẫn đến overfitting trên dữ liệu huấn luyện.

5 Kết luận
Trong bài báo này, chúng tôi trình bày E2LLM, một phương pháp mới để giải quyết các thách thức về tăng cường hiệu suất ngữ cảnh dài trong LLMs. Nó hiệu quả điều hướng "tam giác bất khả thi" bằng cách chiến lược chia ngữ cảnh dài thành các đoạn, nén chúng thành vector nhúng, và sử dụng bộ điều hợp để căn chỉnh những biểu diễn này với LLM chỉ giải mã. Hai mục tiêu huấn luyện được sử dụng để hỗ trợ việc hiểu gợi ý mềm bởi LLMs, dẫn đến hiệu suất vượt trội trong các tình huống ngữ cảnh dài. Các phát hiện thí nghiệm tiết lộ rằng E2LLM hiệu quả vượt trội so với các phương pháp hiện có trong việc cân bằng hiệu suất ngữ cảnh dài, hiệu quả tính toán, và khả năng tương thích mô hình.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo giữ nguyên định dạng gốc]
