# 2310.03025.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.03025.pdf
# Kích thước tệp: 681530 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024
TRUY XUẤT KẾT HỢP VỚI CÁC MÔ HÌNH NGÔN NGỮ LỚN 
NGỮ CẢNH DÀI
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
TÓM TẮT
Mở rộng cửa sổ ngữ cảnh của các mô hình ngôn ngữ lớn (LLM) đang trở nên phổ biến 
gần đây, trong khi giải pháp tăng cường LLM với truy xuất đã tồn tại trong nhiều năm.
Các câu hỏi tự nhiên là: i) Tăng cường truy xuất so với cửa sổ ngữ cảnh dài, cái nào 
tốt hơn cho các tác vụ downstream? ii) Có thể kết hợp cả hai phương pháp để có được 
điều tốt nhất của cả hai thế giới không? Trong công trình này, chúng tôi trả lời những 
câu hỏi này bằng cách nghiên cứu cả hai giải pháp sử dụng hai LLM được tiền huấn luyện 
tiên tiến, tức là một GPT độc quyền 43B và Llama2-70B. Có lẽ đáng ngạc nhiên, chúng 
tôi thấy rằng LLM với cửa sổ ngữ cảnh 4K sử dụng tăng cường truy xuất đơn giản tại 
thời điểm sinh có thể đạt được hiệu suất tương đương với LLM được tinh chỉnh với cửa 
sổ ngữ cảnh 16K thông qua nội suy vị trí trên các tác vụ ngữ cảnh dài, trong khi sử 
dụng ít tính toán hơn nhiều. Quan trọng hơn, chúng tôi chứng minh rằng truy xuất có 
thể cải thiện đáng kể hiệu suất của LLM bất kể kích thước cửa sổ ngữ cảnh mở rộng 
của chúng. Mô hình tốt nhất của chúng tôi, Llama2-70B được tăng cường truy xuất với 
cửa sổ ngữ cảnh 32K, vượt trội hơn GPT-3.5-turbo-16k và Davinci003 về điểm trung 
bình trên chín tác vụ ngữ cảnh dài bao gồm trả lời câu hỏi, tóm tắt dựa trên truy vấn, 
và các tác vụ học few-shot trong ngữ cảnh. Nó cũng vượt trội hơn đường cơ sở 
Llama2-70B-32k không truy xuất với một khoảng cách, trong khi nhanh hơn nhiều khi 
sinh. Nghiên cứu của chúng tôi cung cấp những hiểu biết chung về lựa chọn tăng cường 
truy xuất so với mở rộng ngữ cảnh dài của LLM cho các nhà thực hành.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn ngữ cảnh dài (LLM) gần đây đã nhận được rất nhiều sự chú ý trong 
sản xuất (ví dụ: Anthropic, 2023; OpenAI, 2023b), cộng đồng nghiên cứu (ví dụ: Chen et al., 2023; 
Liu et al., 2023; Tworkowski et al., 2023), và cộng đồng mã nguồn mở (ví dụ: Kaiokendev, 2023). 
Mặc dù các phương pháp attention xấp xỉ đã được nghiên cứu trong nhiều năm (ví dụ: Tay et al., 
2022a) (do độ phức tạp thời gian và bộ nhớ bậc hai của cơ chế self-attention theo độ dài chuỗi), 
tiến bộ gần đây cho LLM ngữ cảnh dài với attention chính xác chủ yếu được thúc đẩy bởi sự phát 
triển của GPU nhanh hơn với nhiều bộ nhớ hơn và attention chính xác tiết kiệm bộ nhớ (Dao et al., 
2022; Dao, 2023).

Một giải pháp thay thế và lâu đời cho việc xử lý ngữ cảnh dài là truy xuất. Cụ thể, các LLM chỉ 
đọc ngữ cảnh có liên quan được truy xuất từ một trình truy xuất độc lập (ví dụ: Karpukhin et al., 
2020; Wang et al., 2022; Lin et al., 2023), dễ dàng mở rộng hơn nhiều¹ và chạy nhanh hơn hàng 
bậc độ lớn so với LLM để chọn ngữ cảnh có liên quan. Về mặt khái niệm, LLM decoder-only được 
tăng cường truy xuất có thể được xem như áp dụng sparse attention trên cửa sổ ngữ cảnh dài của 
nó, trong đó mẫu thưa thớt không được định nghĩa trước như Child et al. (2019) mà được xác 
định bởi trình truy xuất độc lập. Nói cách khác, ngữ cảnh không được truy xuất được coi là không 
liên quan và có trọng số attention bằng không.

Với sự gia tăng quan tâm đến nghiên cứu LLM ngữ cảnh dài và yêu cầu tính toán nhiều hơn tại 
thời điểm suy luận², vẫn chưa rõ ràng đối với các nhà thực hành liệu việc mở rộng cửa sổ ngữ 
cảnh của LLM

¹Trình truy xuất embedding dày đặc có thể dễ dàng truy xuất ngữ cảnh từ hàng tỷ token sử dụng thư viện 
tìm kiếm tương tự nhanh (Johnson et al., 2019).
²Ví dụ, giá của GPT-4 với độ dài ngữ cảnh 32k gấp đôi mô hình ngữ cảnh 8k.
1arXiv:2310.03025v2  [cs.CL]  23 Jan 2024

--- TRANG 2 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024
cung cấp độ chính xác cao hơn so với tăng cường truy xuất cho các tác vụ downstream với truy vấn 
thông tin. Hơn nữa, sẽ thuyết phục nếu chúng ta có thể kết hợp sức mạnh của cả hai phương pháp 
và đạt được độ chính xác thậm chí cao hơn. Trong công trình này, chúng tôi cố gắng trả lời các 
câu hỏi trên thông qua một nghiên cứu toàn diện.

Cụ thể, chúng tôi đóng góp những điều sau:
1. Chúng tôi thực hiện nghiên cứu toàn diện sử dụng hai LLM tiên tiến, một GPT được tiền huấn 
luyện độc quyền 43B và Llama2-70B (Touvron et al., 2023b) trên 9 tác vụ ngữ cảnh dài 
downstream, bao gồm trả lời câu hỏi tài liệu đơn và đa tài liệu (QA), tóm tắt dựa trên truy 
vấn, và các tác vụ học few-shot trong ngữ cảnh.

2. Chúng tôi chứng minh rằng tăng cường truy xuất cải thiện đáng kể hiệu suất của LLM ngữ cảnh 
4K. Có lẽ đáng ngạc nhiên, chúng tôi thấy đường cơ sở tăng cường truy xuất đơn giản này có 
thể hoạt động tương đương với LLM ngữ cảnh dài 16K, tức là điểm trung bình 29.32 so với 
29.45 bằng cách sử dụng GPT-43B, và 36.02 so với 36.78 bằng cách sử dụng Llama2-70B, 
trong khi sử dụng ít tính toán hơn nhiều.

3. Hơn nữa, chúng tôi chứng minh rằng hiệu suất của LLM ngữ cảnh dài (tức là 16K hoặc 32K) 
vẫn có thể được cải thiện bằng truy xuất, đặc biệt đối với Llama2-70B lớn hơn. Kết quả là, 
mô hình tốt nhất của chúng tôi, Llama2-70B-32k-ret được tăng cường truy xuất với cửa sổ 
ngữ cảnh 32K (điểm trung bình 43.6), vượt trội hơn GPT-3.5-turbo-16k (điểm trung bình 42.8) 
và Davinci-003 về điểm trung bình. Nó cũng vượt trội hơn rất nhiều so với đường cơ sở 
Llama2-70B-32k không truy xuất (điểm trung bình 40.9), trong khi có thể nhanh hơn nhiều 
khi sinh (ví dụ: nhanh hơn 4× trên NarrativeQA).

Chúng tôi tổ chức phần còn lại của bài báo như sau. Chúng tôi thảo luận về công trình liên quan 
trong Phần 2, và trình bày thiết lập thí nghiệm trong Phần 3. Chúng tôi báo cáo kết quả trong 
Phần 4 và kết luận bài báo trong Phần 5.

2 CÔNG TRÌNH LIÊN QUAN
Trong phần này, chúng tôi thảo luận về công trình liên quan trong LLM ngữ cảnh dài, các phương 
pháp attention hiệu quả, và các mô hình ngôn ngữ được tăng cường truy xuất.

2.1 CÔNG TRÌNH SONG SONG
Khi chúng tôi đang chuẩn bị bản thảo này, chúng tôi nhận thấy rằng một công trình đồng thời 
(Bai et al., 2023) (được đăng trên arXiv vào ngày 28 tháng 8 năm 2023) cũng nghiên cứu tác 
động của truy xuất đối với LLM ngữ cảnh dài, bao gồm mô hình hộp đen GPT-3.5-Turbo-16k 
(OpenAI, 2022), mô hình hộp trắng Llama2-7B-chat-4k (Touvron et al., 2023b), và ChatGLM2-6B-32k 
(Zeng et al., 2022). Khác với phát hiện của chúng tôi, họ thấy rằng truy xuất chỉ hữu ích cho 
Llama2-7B-chat-4k với cửa sổ ngữ cảnh 4K, nhưng không hữu ích cho mô hình ngữ cảnh dài, 
tức là GPT-3.5-Turbo-16k và ChatGLM2-6B-32k. Chúng tôi đưa ra giả thuyết rằng các lý do chính 
là: i) việc thực hiện các thí nghiệm có kiểm soát bằng API hộp đen là thách thức, ii) các LLM 
hộp trắng được sử dụng trong nghiên cứu của họ tương đối nhỏ, do đó chúng có khả năng zero-shot 
hạn chế trong việc kết hợp ngữ cảnh thông qua truy xuất. Kết luận của chúng tôi được rút ra từ 
các LLM lớn hơn nhiều. Đặc biệt, mô hình ngữ cảnh dài tốt nhất của chúng tôi Llama2-70B-32k 
hoạt động tốt như Davinci003 và GPT-3.5-turbo-16k, trong khi vẫn có thể được tăng cường thêm 
bằng truy xuất (xem Bảng 3).

2.2 CÁC MÔ HÌNH NGÔN NGỮ LỚN NGỮ CẢNH DÀI
Trong vài năm qua, tiền huấn luyện các mô hình ngôn ngữ lớn (LLM) với cửa sổ ngữ cảnh dài 
trở thành một giải pháp khả thi nhờ GPU nhanh hơn với nhiều bộ nhớ hơn và attention chính xác 
tiết kiệm bộ nhớ (ví dụ: Dao et al., 2022). Ví dụ, cửa sổ ngữ cảnh cho LLM được tiền huấn luyện 
đã tăng từ 1024 của GPT-2 (Radford et al., 2019), 2048 của GPT-3 (Brown et al., 2020), 4096 
của Llama 2 (Touvron et al., 2023b), đến 8192 của GPT-4 (OpenAI, 2023a). Tuy nhiên, việc mở 
rộng thêm cửa sổ ngữ cảnh trong tiền huấn luyện có thể là thách thức, bởi vì, i) tiền huấn luyện 
LLM từ đầu với ngữ cảnh dài (ví dụ: >16K token) rất tốn kém do độ phức tạp thời gian và bộ 
nhớ bậc hai của attention chính xác, và ii) hầu hết các tài liệu trong kho dữ liệu tiền huấn luyện 
(ví dụ: Common Crawl) tương đối ngắn.

Gần đây nhất, các nhà nghiên cứu bắt đầu mở rộng cửa sổ ngữ cảnh của LLM với việc tiếp tục 
huấn luyện hoặc tinh chỉnh (ví dụ: Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; 
Tworkowski et al., 2023; Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) giới thiệu 
LongLLaMA bằng cách tinh chỉnh các checkpoint OpenLLaMA 3B và 7B với huấn luyện tương phản 
trên độ dài ngữ cảnh 8K. Landmark

--- TRANG 3 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024
attention (Mohtashami & Jaggi, 2023) mở rộng độ dài ngữ cảnh của LLaMA 7B từ 4K đến 32K 
bằng cách giới thiệu "landmark tokens" để đại diện cho các khối ngữ cảnh và tinh chỉnh attention 
để sử dụng landmark tokens cho việc chọn các khối liên quan. Chen et al. (2023) và Kaiokendev 
(2023) giới thiệu nội suy vị trí để mở rộng kích thước cửa sổ ngữ cảnh của các LLM được tiền 
huấn luyện dựa trên RoPE (Su et al., 2021). Đặc biệt, Chen et al. (2023) chứng minh kết quả 
hứa hẹn trên LLaMA 7B đến 65B (Touvron et al., 2023a) với nỗ lực tinh chỉnh tối thiểu (trong 
vòng 1000 bước). ALiBi (Press et al., 2021) ngoại suy độ dài cửa sổ ngữ cảnh bằng cách loại 
bỏ các embedding vị trí trong khi chỉ đơn giản là thiên vị các điểm attention key-query với 
một penalty tuyến tính tỷ lệ với khoảng cách của chúng, vì vậy người ta không cần tinh chỉnh 
để ngoại suy cửa sổ ngữ cảnh. Ratner et al. (2023) chia ngữ cảnh dài thành nhiều cửa sổ con 
và tái sử dụng các embedding vị trí qua các cửa sổ này, do đó có thể xử lý ngữ cảnh dài hơn 
mà không cần tinh chỉnh thêm. Trong công trình này, chúng tôi áp dụng phương pháp nội suy 
vị trí để mở rộng cửa sổ ngữ cảnh 4K của một LLM được tiền huấn luyện độc quyền 43B và 
Llama2-70B (Touvron et al., 2023b) đến 16K và 32K, vì cả hai đều sử dụng rotary position 
embedding tại thời điểm tiền huấn luyện. Về mặt đánh giá, chúng tôi tập trung vào hiệu suất 
tác vụ downstream (ví dụ: Shaham et al., 2023; Bai et al., 2023) sau instruction tuning 
(Wei et al., 2021).

Có những nghiên cứu khác cho thấy sự tương tác giữa tăng cường truy xuất và LLM ngữ cảnh dài. 
Liu et al. (2023) thực hiện đánh giá hộp đen cho khả năng ngữ cảnh dài của các sản phẩm LLM 
hiện có, bao gồm ChatGPT 3.5 (OpenAI, 2022), GPT-4 (OpenAI, 2023a), Claude (Anthropic, 
2023), trong thiết lập tăng cường truy xuất, và xác định hiện tượng "lost in the middle" trong 
các mô hình này.

2.3 CÁC PHƯƠNG PHÁP ATTENTION HIỆU QUẢ
Trong nghiên cứu trước, nhiều phương pháp attention xấp xỉ (Tay et al., 2022a) đã được giới 
thiệu để xử lý độ phức tạp bậc hai của self-attention trở thành điểm nghẽn tính toán cho ngữ 
cảnh dài. Chúng có thể được nhóm thành các danh mục sau: i) Các cơ chế sparse attention với 
các mẫu thưa thớt được định nghĩa trước (ví dụ: Child et al., 2019; Parmar et al., 2018; Ho 
et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Zhu et al., 2021), ii) phương pháp 
dựa trên recurrence (Dai et al., 2019; Bulatov et al., 2022), iii) attention chiếu low-rank 
(ví dụ: Wang et al., 2020; Xiong et al., 2021; Tay et al., 2021; Zhu et al., 2021), iv) các 
cơ chế dựa trên bộ nhớ (ví dụ: Rae et al., 2020; Liu et al., 2018), v) các phương pháp dựa 
trên tương tự và clustering (ví dụ: Kitaev et al., 2020; Tay et al., 2020; Roy et al., 2021). 
Các phương pháp xấp xỉ này giới thiệu độ thiên vị quy nạp (ví dụ: thưa thớt được định nghĩa 
trước) có thể phù hợp tốt cho domain cụ thể, nhưng có thể giảm chất lượng mô hình trong huấn 
luyện LLM tổng quát.

Gần đây nhất, FlashAttention (Dao et al., 2022; Dao, 2023) được giới thiệu để tăng tốc tính 
toán attention chính xác bằng cách tính đến việc đọc và ghi giữa các cấp độ bộ nhớ GPU. 
FlashAttention đặc biệt hữu ích để xử lý các chuỗi dài hơn.

2.4 CÁC MÔ HÌNH NGÔN NGỮ ĐƯỢC TĂNG CƯỜNG TRUY XUẤT
Truy xuất đã được tích hợp vào các mô hình ngôn ngữ trong nhiều năm để cải thiện perplexity 
(Borgeaud et al., 2022; Wang et al., 2023), độ chính xác thực tế (Nakano et al., 2021), độ 
chính xác tác vụ downstream (Guu et al., 2020; Izacard & Grave, 2021; Izacard et al., 2022; 
Lewis et al., 2020), và khả năng học trong ngữ cảnh (Huang et al., 2023). Kết hợp với một 
trình truy xuất độc lập (Karpukhin et al., 2020; Wang et al., 2022; Lin et al., 2023), LLM 
được tăng cường truy xuất được thiết lập tốt để xử lý trả lời câu hỏi với tài liệu dài và trong 
domain mở. Trong nghiên cứu trước, các mô hình ngôn ngữ đã được tăng cường với truy xuất tại 
thời điểm suy luận (Khandelwal et al., 2019; Yogatama et al., 2021), tinh chỉnh (Izacard et 
al., 2022; Lewis et al., 2020; Guu et al., 2020), và tiền huấn luyện (Borgeaud et al., 2022; 
Izacard et al., 2022; Wang et al., 2023). Cũng có các phương pháp cố gắng tích hợp LLM và 
trình truy xuất trong một mô hình duy nhất và xây dựng giải pháp end-to-end (ví dụ: Jiang 
et al., 2022; Shi et al., 2023). Tuy nhiên, hầu hết các công trình trước chủ yếu nghiên cứu 
tăng cường truy xuất cho các LLM có khoảng 10 tỷ tham số, ngoại trừ một số ít gần đây (ví dụ: 
Shi et al., 2023).

Trong công trình này, chúng tôi tập trung vào các LLM decoder-only với 43B và 70B tham số được 
huấn luyện trên hàng nghìn tỷ token, bởi vì các LLM ở quy mô như vậy thể hiện khả năng 
zero-shot mạnh để kết hợp ngữ cảnh sau instruction tuning (Wei et al., 2021; 2022).

--- TRANG 4 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024
3 THIẾT LẬP THÍ NGHIỆM
Trong phần này, chúng tôi trình bày chi tiết về thiết lập thí nghiệm của chúng tôi.

3.1 CÁC MÔ HÌNH NGÔN NGỮ LỚN
Chúng tôi tập trung vào so sánh khả năng zero-shot của việc tích hợp thông tin ngữ cảnh dài 
cho các tác vụ QA hoặc tóm tắt sinh thông qua truy xuất hoặc cơ chế self-attention của chính 
LLM. Trái ngược với hầu hết các công trình hiện có tập trung vào các mô hình tương đối nhỏ 
(ví dụ: 3B hoặc 7B) (Kaiokendev, 2023; Nijkamp et al., 2023; Tworkowski et al., 2023; 
Mohtashami & Jaggi, 2023), chúng tôi thu thập những hiểu biết bằng cách khám phá các kích 
thước mô hình lớn hơn 40B sau instruction tuning, vì nghiên cứu trước cho thấy rằng instruction 
tuning trở nên hiệu quả khi LLM decoder-only có khoảng 50B tham số (Wei et al., 2021; 2022).

Cụ thể, chúng tôi thí nghiệm với hai mô hình GPT được tiền huấn luyện, một Nemo GPT-43B độc 
quyền và Llama2-70B. GPT-43B là một mô hình 43 tỷ tham số được huấn luyện với 1.1T token 
với 70% corpus tiếng Anh và 30% còn lại cho dữ liệu đa ngôn ngữ và mã. Đối với corpus tiền 
huấn luyện tiếng Anh, GPT-43B sử dụng Common Crawl web archive (WARC), Wikipedia, Reddit, 
Books, Gutenberg, ArXiv, StackExchange, PubMed, v.v. Nó chứa 48 layer với chiều ẩn 8,192. 
Nó được huấn luyện với độ dài chuỗi 4,096 và RoPE embeddings (Su et al., 2021). Llama2-70B 
khác là một mô hình GPT 70B có sẵn công khai được huấn luyện trên 2T token sử dụng khoảng 
90% dữ liệu tiếng Anh. Nó chứa 80 layer với chiều ẩn 8,192. Nó cũng có kích thước cửa sổ 
ngữ cảnh 4,096 và được huấn luyện với RoPE embeddings.

3.2 CÁC BỘ DỮ LIỆU VÀ METRIX
Trong nghiên cứu này, chúng tôi bao gồm bảy bộ dữ liệu từ QA tài liệu đơn, QA đa tài liệu, 
đến tóm tắt dựa trên truy vấn cho các đánh giá zero shot của chúng tôi. Cụ thể, chúng tôi bao 
gồm bốn bộ dữ liệu từ tập validation của benchmark Scroll (Shaham et al., 2022).

• QMSum (QM) (Zhong et al., 2021) là một bộ dữ liệu tóm tắt dựa trên truy vấn, bao gồm 
các bản ghi cuộc họp và các bản tóm tắt tương ứng từ nhiều domain như học thuật, sản phẩm 
công nghiệp. Trong tác vụ này, một bản ghi đối thoại cuộc họp được đưa ra, và một câu hỏi 
để tóm tắt chủ đề nhất định được đặt về đối thoại, chẳng hạn như "điều gì được thống nhất 
giữa họ". Câu trả lời thường chứa một vài câu.

• Qasper (QASP) (Dasigi et al., 2021) là một bộ dữ liệu trả lời câu hỏi trên các bài báo 
NLP được lọc từ Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020). 
Qasper chứa các câu hỏi trừu tượng, trích xuất, và yes/no, cũng như những câu không thể 
trả lời. Trong tác vụ này, một script được cung cấp cùng với một câu hỏi tìm kiếm thông tin, 
chẳng hạn như "họ so sánh với những phương pháp đa ngôn ngữ nào?". Một mô hình cần đưa 
ra câu trả lời ngắn bằng cách lý luận trên ngữ cảnh đã cho.

• NarrativeQA (NQA) (Kočiský et al., 2018) là một bộ dữ liệu trả lời câu hỏi đã được thiết 
lập trên toàn bộ sách từ Project Gutenberg³ và script phim từ một danh sách các trang web. 
Trong tác vụ này, đoạn văn đã cho được phiên mã từ sách và thường có nhiễu. Một mô hình 
được yêu cầu tạo ra một cụm từ ngắn bằng cách lý luận trên văn bản dài và có nhiễu.

• QuALITY (QLTY) (Pang et al., 2022) là một bộ dữ liệu trả lời câu hỏi trên các câu chuyện 
và bài báo được thu thập từ một số nguồn, chẳng hạn như Project Gutenberg và Open American 
National Corpus⁴. Khác với tất cả các tác vụ khác, đây là một bộ dữ liệu đa lựa chọn và 
một mô hình được yêu cầu chọn một trong bốn lựa chọn đã cho.

Chúng tôi lấy ba bộ dữ liệu khác từ LongBench (Bai et al., 2023).

• HotpotQA (HQA) (Yang et al., 2018) là một bộ dữ liệu câu hỏi-trả lời dựa trên Wikipedia. 
Khác với các bộ dữ liệu hot đơn ở trên, HQA là một bộ dữ liệu multi-hop trong đó nhiều 
tài liệu hỗ trợ được yêu cầu phải đọc để trả lời và lý luận và các câu hỏi đa dạng và không 
bị hạn chế bởi bất kỳ cơ sở tri thức nào được xác định trước.

³https://www.gutenberg.org/
⁴https://anc.org/

--- TRANG 5 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Statistics table showing 7 datasets with columns for QM, QASP, NQA, QLTY, MSQ, HQA, MFQA including number of samples, average doc length, and average top-N chunks]

Bảng 1: Thống kê của bảy bộ dữ liệu được sử dụng cho đánh giá zero-shot. Tất cả độ dài được đếm 
bằng số lượng token sử dụng tokenizer Llama2-70B, và "avg top N chunks" biểu thị số lượng token 
trung bình từ top N chunk được truy xuất. Hình 2 cung cấp thêm chi tiết.

• MuSiQue (MSQ) (Trivedi et al., 2022) là một bộ dữ liệu trả lời câu hỏi multi-hop khác. 
So với HQA, MSQ yêu cầu lý luận được kết nối bằng cách giảm các lối tắt lý luận tiềm năng, 
giảm thiểu rò rỉ train-test, và bao gồm các ngữ cảnh distractor khó hơn. Do đó, MSQ là 
tác vụ khó hơn nhiều so với HQA và ít có thể gian lận hơn đáng kể.

• MultiFieldQA-en (MFQA) (Bai et al., 2023) được tuyển chọn thủ công để kiểm tra tốt hơn 
khả năng hiểu ngữ cảnh dài của mô hình trên các lĩnh vực đa dạng. Các bằng chứng từ nhiều 
nguồn, bao gồm tài liệu pháp lý, báo cáo chính phủ, bách khoa toàn thư, và các bài báo học 
thuật, được đặt khá ngẫu nhiên trong các tài liệu để tránh độ thiên vị có thể xảy ra ở đầu 
hoặc cuối tài liệu.

Chi tiết đầy đủ của bộ dữ liệu có thể được tìm thấy trong Bảng 1. Chúng ta có thể thấy rằng 
các bộ dữ liệu đánh giá của chúng tôi có phạm vi rộng về độ dài tài liệu trung bình từ 4.9k 
(QASP) đến 84k (NQA). Do đó, đối với mô hình cơ sở không có truy xuất, chúng tôi cắt tài liệu 
tương ứng để phù hợp với độ dài chuỗi đầu vào.

Theo các metrix chính thức, chúng tôi báo cáo trung bình hình học của điểm ROUGE (tức là 
ROUGE-1/2/L) (Lin, 2004) cho QM, điểm exact matching (EM) cho QLTY, và điểm F1 cho năm 
bộ dữ liệu còn lại QASP, NQA, MSQ, HQA và MFQA.

3.3 MỞ RỘNG CỬA SỔ NGỮ CẢNH
Chúng tôi mở rộng độ dài cửa sổ ngữ cảnh với phương pháp nội suy vị trí (Chen et al., 2023), 
vì nó đơn giản và hiệu quả cho RoPE embeddings. Chúng tôi mở rộng cửa sổ ngữ cảnh 4K đến 
16K cho GPT-43B. Đối với Llama2, chúng tôi mở rộng cửa sổ ngữ cảnh 4K của nó đến 32k cho 
Llama2-7B và cả 16K và 32K cho Llama2-70B. Chúng tôi theo Chen et al. (2023) và tinh chỉnh 
cả hai LLM trên bộ dữ liệu Pile (Gao et al., 2021) với batch size là 128, learning rate không 
đổi là 5e-6 để thích ứng các position embeddings.

3.4 TRUY XUẤT
Đối với trình truy xuất, chúng tôi thí nghiệm với ba trình truy xuất: 1) Dragon (Lin et al., 
2023) vì nó đạt được kết quả tiên tiến trên cả các benchmark truy xuất thông tin có giám sát 
và zero-shot (Thakur et al., 2021). Dragon là một mô hình dual encoder bao gồm một query 
encoder và một context encoder. 2) một mô hình Contriever được sử dụng rộng rãi (Izacard 
et al., 2021). Theo kỹ thuật MoCo (He et al., 2020), Contriever sử dụng một framework học 
tương phản đơn giản để tiền huấn luyện các mô hình cho truy xuất thông tin. Nó được huấn 
luyện không có giám sát và đạt được kết quả cạnh tranh với BM25 cho R@100 trên benchmark 
BEIR (Thakur et al., 2021), và 3) OpenAI embedding⁵. Đối với mô hình OpenAI embedding, 
chúng tôi sử dụng "text-embedding-ada-002" mới nhất như được OpenAI khuyến nghị. Nó chấp 
nhận tối đa 8,191 token đầu vào cho một chuỗi với vector đầu ra 1,536 chiều. Độ tương tự 
cosine sau đó được tính toán giữa các câu hỏi và danh sách các ngữ cảnh để xếp hạng truy xuất.

Để sử dụng những trình truy xuất này, trước tiên chúng tôi chia mỗi tài liệu ngữ cảnh với 300 
từ, và sau đó chúng tôi mã hóa cả câu hỏi và tất cả các chunk độc lập với các encoder tương 
ứng. N chunk có liên quan nhất, được xếp hạng bởi tích vô hướng của embedding câu hỏi và 
chunk embedding, sau đó được nối với nhau (theo thứ tự từ trái sang phải từ liên quan nhất 
đến ít liên quan nhất) làm ngữ cảnh của prompt để sinh. Bảng 1 cho thấy thống kê của top 
N chunk được truy xuất trong khi Hình 2 trong Phụ lục cung cấp thêm chi tiết về phân phối 
độ dài token của tất cả bảy bộ dữ liệu. Chúng tôi

⁵https://platform.openai.com/docs/guides/embeddings

--- TRANG 6 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Comparison of model variants showing performance across different datasets. The table has columns for Model, Seq len., Avg., and 7 dataset results (QM, QASP, NQA, QLTY, MSQ, HQA, MFQA)]

Bảng 2: So sánh các biến thể mô hình (GPT-43B, Llama2-7B, Llama2-70B) với độ dài chuỗi từ 4k 
đến 32k trên bảy bộ dữ liệu. "ret" biểu thị sử dụng trình truy xuất tốt nhất (Dragon hoặc 
Contriever hoặc OpenAI embeddings) và ở đây chúng tôi sử dụng top-5 cho trình truy xuất.

có thể thấy rằng top-5 chunk đều có thể vừa với độ dài chuỗi 4k (ngoại trừ một vài ngoại lệ) 
trong khi top-10 và top-20 chunk có thể vừa với độ dài chuỗi 16k.

3.5 INSTRUCTION TUNING
Để huấn luyện các LLM được tiền huấn luyện để tuân theo hướng dẫn cho trả lời câu hỏi hoặc 
tóm tắt văn bản, chúng tôi cũng thực hiện instruction tuning. Trước tiên chúng tôi xây dựng 
một hỗn hợp các bộ dữ liệu instruction tuning bao gồm 102K mẫu huấn luyện từ bộ dữ liệu 
Soda (Kim et al., 2022), bộ dữ liệu ELI5 (Fan et al., 2019), bộ dữ liệu FLAN (Wei et al., 
2021), bộ dữ liệu Open Assistant (Köpf et al., 2023), Dolly (Conover et al., 2023) và một 
bộ dữ liệu đối thoại có nguồn gốc độc quyền, để thích ứng tất cả các mô hình nền tảng để tuân 
theo hướng dẫn. Về mặt template, chúng tôi sử dụng "System: {System}\n\nUser: {Question}\n\nAssistant: 
{Answer}" làm định dạng để hỗ trợ huấn luyện đối thoại nhiều lượt. Vì tất cả các tác vụ đều 
chứa thông tin ngữ cảnh để lý luận tại thời điểm suy luận, chúng tôi thêm ngữ cảnh trước đối 
thoại, tức là "System: {System}\n\n{Context}\n\nUser: {Question}\n\nAssistant: {Answer}".

Chúng tôi tinh chỉnh LLM bằng cách chỉ lấy loss trên phần {Answer} với batch size 128 và 
learning rate 5e-6 trong 1000 bước. Đối với phần còn lại của bài báo, kết quả đều được báo 
cáo sử dụng mô hình chat được instruction tuned trên đầu các mô hình nền tảng GPT-43B, 
Llama2-7B, và Llama2-70B.

4 KẾT QUẢ
Trong phần này, chúng tôi báo cáo kết quả và cung cấp phân tích chi tiết.

4.1 KẾT QUẢ CHÍNH
Trong Bảng 2, chúng tôi so sánh các biến thể mô hình khác nhau với độ dài ngữ cảnh từ 4K 
đến dài tới 32K sử dụng GPT-43B và Llama2-70B. Đầu tiên, chúng tôi thấy rằng các mô hình 
cơ sở không có truy xuất với độ dài chuỗi 4k đạt được kết quả tệ nhất cho cả GPT-43B và 
Llama2-70B. Điều này là do độ dài chuỗi trung bình tối thiểu của tất cả bảy tác vụ vượt quá 
4096, cửa sổ ngữ cảnh của các mô hình nền tảng và do đó các văn bản có giá trị bị cắt ngẫu 
nhiên. Kết quả là, truy xuất đặc biệt hữu ích cho các LLM 4K ví dụ: Llama2-70B-4K được cải 
thiện từ 31.61 đến 35.73 trong khi GPT-43B-4K được cải thiện từ 26.44 đến 29.32. Thứ hai, 
chúng tôi quan sát thấy rằng HotpotQA (HQA) đặc biệt ưa chuộng các mô hình chuỗi dài vì điểm 
số cải thiện từ 34.64 đến 43.97 cho Llama2-70B và từ 28.91 đến 37.48 cho GPT-43B khi độ dài 
chuỗi tăng từ 4k đến 16k. Điều này là do Hotpot QA là một bộ dữ liệu multi-hop trong đó các 
câu hỏi không khó trả lời nhưng tất cả các bước trung gian đều cần thiết để có câu trả lời đúng. 
Do đó, ngữ cảnh dài có lợi để tăng recall của việc kết hợp tất cả các bước trung gian.

--- TRANG 7 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS FIGURE: A line graph showing EM scores vs Position for Llama2-70b models, demonstrating the "lost-in-the-middle" phenomenon]

Hình 1: Llama2-70B cũng hiển thị hiện tượng lost-in-the-middle

[THIS IS TABLE: Comparison table showing performance of different models including Davinci003, GPT-3.5-turbo variants, and Llama2-70B variants across multiple metrics]

Bảng 3: So sánh Llama2-70B-32k-ret được tăng cường truy xuất tốt nhất của chúng tôi với GPT-3.5-turbo-16k 
và Davinci-003 (175B tham số). Đối với QMSum (QM), Qasper (QASP), NarrativeQA (NQA), QuALITY 
(QLTY), chúng tôi sử dụng tập test từ leaderboard ZeroSCROLLS vì các tổ chức đã chuẩn bị điểm 
của GPT-3.5-turbo (4k) và Davinci-003 (được đánh dấu bằng *). Avg-7 đề cập đến điểm trung bình 
của tất cả 7 bộ dữ liệu, và Avg-4* đề cập đến trung bình của 4 bộ dữ liệu từ ZeroSCROLLS.

Khá thú vị là LLM ngữ cảnh dài được tăng cường truy xuất (ví dụ: 16K và 32K) có thể có được 
kết quả tốt hơn so với LLM ngữ cảnh 4K được tăng cường truy xuất, ngay cả khi chúng được cho 
cùng top 5 chunk bằng chứng. Chúng tôi đưa ra giả thuyết rằng quan sát thú vị này liên quan 
đến hiện tượng "lost in the middle" (Liu et al., 2023), trong đó các LLM có đường cong hiệu 
suất "hình chữ U" như vậy. Cụ thể, các LLM tốt hơn trong việc sử dụng thông tin liên quan 
xảy ra ở đầu hoặc cuối cửa sổ ngữ cảnh đầu vào của chúng. Để xác minh thêm giả thuyết, 
chúng tôi tiến hành nghiên cứu "lost-in-the-middle" theo Liu et al. (2023) cho Llama2-70B-4k 
và Llama2-70B-32k. Như thể hiện trong Hình 1, chúng tôi xác nhận rằng hiện tượng này cũng 
tồn tại trong Llama2-70B với các độ dài ngữ cảnh khác nhau. Đặc biệt, so sánh các đường cong 
từ Llama2-70B-4k và Llama2-70B-32k cho thấy rằng mô hình ngữ cảnh dài có độ chính xác tốt 
hơn để kết hợp ngữ cảnh top-5 được truy xuất.

Lưu ý rằng, chúng tôi có quan sát rất khác với kết luận được rút ra từ công trình LongBench 
(Bai et al., 2023): "Truy xuất mang lại cải thiện cho mô hình có khả năng yếu trên ngữ cảnh 
dài, nhưng hiệu suất vẫn tụt hậu so với các mô hình có khả năng hiểu ngữ cảnh dài mạnh". 
Ở đây, chúng tôi chứng minh truy xuất có thể cải thiện đáng kể hiệu suất của cả GPT-43B và 
Llama2-70B bất kể kích thước cửa sổ ngữ cảnh của chúng. Ví dụ, Llama2-70B-32k-ret được 
tăng cường truy xuất tốt nhất của chúng tôi vượt trội hơn đường cơ sở w/o truy xuất với một 
khoảng cách, tức là 39.60 so với 37.36. Chúng tôi nghĩ lý do chính cho kết luận khác nhau 
như vậy là Bai et al. (2023) sử dụng LLM nhỏ hơn nhiều với 6B và 7B tham số, thường có 
khả năng zero-shot tương đối tệ hơn để kết hợp ngữ cảnh chunk được truy xuất. Để xác minh 
thêm giả thuyết, chúng tôi cũng báo cáo kết quả sử dụng Llama2-7B trong Bảng 5. Người ta 
thực sự có thể rút ra kết luận tương tự với Bai et al. (2023). Chúng tôi nghĩ những lý do cơ 
bản là: i) Đối với Llama2-7B-chat-4k, độ dài ngữ cảnh ngắn của nó là điểm nghẽn cho các 
tác vụ ngữ cảnh dài. Do đó, tăng cường truy xuất cải thiện lớn kết quả. ii) Đối với Llama2-7B-chat-32 
và ChatGLM2-6B-32k, điểm nghẽn độ dài ngữ cảnh đã được loại bỏ phần lớn. Tuy nhiên, các 
mô hình được tăng cường truy xuất của chúng có khả năng zero-shot hạn chế trong việc kết hợp 
các chunk ngữ cảnh được truy xuất, do kích thước nhỏ hơn. Kết quả là, truy xuất không hữu 
ích cho cả Llama2-7B-32k và ChatGLM2-6B-32k, khác với các LLM lớn như Llama2-70B-32k trong 
trường hợp của chúng tôi.

--- TRANG 8 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Table 4 showing comparisons of adding top 5 retrieved chunks from different retrievers under Llama2-70B, with columns for Seq len, Setting, Avg, and various metrics]

Bảng 4: So sánh việc thêm top 5 chunk được truy xuất từ các trình truy xuất khác nhau vào ngữ 
cảnh dưới Llama2-70B.

[THIS IS TABLE: Table 5 showing comparisons of adding top-5/10/20 retrieved chunks under different sequence lengths using Llama2-70B]

Bảng 5: So sánh việc thêm top-5/10/20 chunk được truy xuất vào ngữ cảnh dưới độ dài chuỗi đầu 
vào 4k, 16k, và 32k sử dụng Llama2-70B. Nhiều ngữ cảnh hơn không phải lúc nào cũng cho kết 
quả tốt hơn.

Ngược lại, các LLM được instruction tuned lớn hơn như Llama2-70B có khả năng zero-shot mạnh 
hơn nhiều để kết hợp bằng chứng được truy xuất. Quan sát này trở nên rõ ràng hơn khi người 
ta so sánh lợi ích của tăng cường truy xuất giữa GPT-43B và Llama2-70B, trong đó Llama2-70B 
được hưởng lợi ích lớn hơn từ việc kết hợp ngữ cảnh thông qua truy xuất.

4.2 SO SÁNH VỚI CÁC MÔ HÌNH OPENAI
Để hiểu thêm về mức độ tốt của mô hình tốt nhất của chúng tôi, tức là tăng cường Llama2-70B-32k 
với truy xuất, chúng tôi cũng so sánh nó với GPT-3.5-turbo(4k), GPT-3.5-turbo-16k và Davinci-003 
trên những bảy bộ dữ liệu đó.⁶ Chúng tôi thấy rằng Llama2-70B-32k-ret đạt được kết quả tốt 
hơn GPT-3.5-turbo-16k về độ chính xác trung bình trên bảy bộ dữ liệu, trong khi tốt hơn Davinci-003 
(với 175B tham số) trên trung bình 4 tác vụ. Điều này cho thấy Llama2-70B-32k với truy xuất 
là một mô hình mạnh cho những tác vụ ngữ cảnh dài này, và kết luận của chúng tôi được xây 
dựng trên kết quả tiên tiến.

Chúng tôi cũng báo cáo kết quả tăng cường truy xuất cho GPT3.5-turbo trên MSQ, HQA và MFQA. 
Đối với GPT3.5-turbo-4k, truy xuất cải thiện đáng kể hiệu suất (trung bình từ 37.08 đến 41.15). 
Đối với GPT3.5-turbo-16k, điểm trung bình cho truy xuất (43.27) và điểm không truy xuất (43.60) 
gần nhau và cả hai đều thấp hơn kết quả Llama2-70B-32k-ret của chúng tôi (44.51). Lưu ý rằng 
GPT3.5-turbo-16k là một API blackbox, chúng tôi không biết cách nó được triển khai, kích thước 
mô hình cũng như bất kỳ bước tiền xử lý nào.

⁶Đối với QMSum (QM), Qasper (QASP), NarrativeQA (NQA), QuALITY (QLTY), chúng tôi sử dụng tập test 
từ leaderboard ZeroSCROLLS vì các tổ chức đã chuẩn bị điểm của GPT-3.5-turbo (4k) và Davinci-003 ở đó.

--- TRANG 9 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Table 6 showing comparison of Llama2-70B to GPT-3.5-turbo-16k with two few-shot learning tasks]
Mô hình                      Trec    SAMSum
GPT-3.5-turbo-16k           68      41.7
Llama2-70B                  73      46.5
Llama2-70B-ret              76      47.3

Bảng 6: So sánh Llama2-70B với GPT-3.5-turbo-16k với hai tác vụ học few-shot từ LongBench. 
Truy xuất cũng hữu ích cho học few-shot.

4.3 ABLATION TRÊN CÁC TRÌNH TRUY XUẤT KHÁC NHAU
Để điều tra tác động của các trình truy xuất khác nhau trên Llama2-70B, chúng tôi so sánh Dragon, 
Contriever, và OpenAI embeddings trên Llama2-70B-4k và Llama2-70B-32k. Kết quả trong Bảng 
4 xác nhận rằng phát hiện của chúng tôi, tức là truy xuất có thể tăng cường hiệu suất của cả 
LLM ngữ cảnh ngắn và ngữ cảnh dài, nhất quán trên các trình truy xuất khác nhau.

4.4 TĂNG SỐ LƯỢNG CHUNK ĐƯỢC TRUY XUẤT
Để nghiên cứu tác động của việc thêm nhiều chunk được truy xuất vào ngữ cảnh, chúng tôi tăng 
số lượng chunk được truy xuất từ 5 đến 20 sử dụng trình truy xuất Dragon và kết quả có thể 
được tìm thấy trong Bảng 5. Chúng tôi quan sát thấy rằng đối với các độ dài chuỗi khác nhau, 
kết quả trung bình tốt nhất được có được từ top 5 hoặc top 10. Ngay cả khi 20 chunk vẫn có 
thể vừa với cửa sổ ngữ cảnh 16K và 32K (như thể hiện trong Hình 2), việc thêm nhiều chunk 
lên đến 20 không hữu ích và đôi khi sẽ làm tổn hại hiệu suất. Chúng tôi tin rằng điều này 
liên quan đến hiện tượng "lost in the middle" (Liu et al., 2023) hoặc mô hình bị phân tâm 
bởi thông tin không liên quan và do đó cần nghiên cứu thêm.

4.5 TRUY XUẤT CHO CÁC TÁC VỤ FEW-SHOT
Ngoài các tác vụ zero-shot của tóm tắt dựa trên truy vấn và trả lời câu hỏi được đề cập ở trên, 
chúng tôi tiếp tục điều tra lợi ích của các mô hình ngữ cảnh dài cho các tác vụ few-shot sử 
dụng hai bộ dữ liệu bổ sung (Trec và SAMSum) từ LongBench. Chúng tôi lấy câu hỏi từ mỗi bộ 
dữ liệu làm truy vấn và sử dụng nó để tìm kiếm các cặp QA liên quan được cung cấp trong các 
ví dụ few-shot đã cho. Bảng 6 cho thấy rằng mô hình tốt nhất của chúng tôi Llama2-70B-32k-ret 
vượt trội hơn đường cơ sở Llama2-70B-32k không truy xuất cũng như GPT-3.5-turbo-16k với một 
khoảng cách lớn. Nó một lần nữa xác nhận lợi ích của việc sử dụng truy xuất cùng với các mô 
hình ngữ cảnh dài.

5 KẾT LUẬN
Trong công trình này, chúng tôi nghiên cứu có hệ thống tăng cường truy xuất so với mở rộng 
ngữ cảnh dài sử dụng các LLM tiên tiến sau instruction tuning cho các tác vụ QA và tóm tắt 
dựa trên truy vấn ngữ cảnh dài khác nhau. Sau nghiên cứu, chúng tôi có những phát hiện thú 
vị sau: i) Truy xuất tăng cường lớn hiệu suất của cả LLM ngữ cảnh ngắn 4K và LLM ngữ cảnh 
dài 16K/32K. ii) LLM ngữ cảnh 4K với tăng cường truy xuất đơn giản có thể hoạt động tương 
đương với LLM ngữ cảnh dài 16K, trong khi hiệu quả hơn tại thời điểm suy luận. iii) Sau 
mở rộng cửa sổ ngữ cảnh và tăng cường truy xuất, mô hình tốt nhất Llama2-70B-32k-ret có thể 
vượt trội hơn GPT-3.5-turbo-16k và Davinci003 về điểm trung bình trên một bộ các tác vụ downstream 
với truy vấn thông tin. Nghiên cứu của chúng tôi làm sáng tỏ hướng nghiên cứu hứa hẹn của 
việc kết hợp kỹ thuật truy xuất và ngữ cảnh dài cùng nhau để xây dựng LLM tốt hơn.

6 HƯỚNG NGHIÊN CỨU TƯƠNG LAI
Có nhiều hướng nghiên cứu tiềm năng có thể được mở rộng từ công trình này. Một hướng là 
phát triển các phương pháp tiến tiến (ví dụ: bộ nhớ hoặc attention phân cấp) cho các mô hình 
ngôn ngữ lớn được tiền huấn luyện hiện có ví dụ: Llama2-70B, điều này tự nó không tầm thường. 
Cũng vậy, việc mở rộng thêm cửa sổ ngữ cảnh đến 64k và thậm chí dài hơn sẽ là một nghiên 
cứu rất thú vị cho các mô hình 70B tham số lớn mặc dù tiền huấn luyện chuỗi dài hơn yêu cầu 
nhiều tính toán hơn. Cuối cùng, làm thế nào để giảm thiểu hiện tượng "lost-in-the-middle" là 
một chủ đề nghiên cứu mở và tiếp tục tiền huấn luyện với UL2 loss (Tay et al., 2022b) có thể 
là một giải pháp tiềm năng.

--- TRANG 10 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024
TÀI LIỆU THAM KHẢO
Anthropic. Introducing 100k context windows. https://www.anthropic.com/index/
100k-context-windows , 2023.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context
understanding. arXiv preprint arXiv:2308.14508 , 2023.

Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 , 2020.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.
Improving language models by retrieving from trillions of tokens. In ICML , 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. NeurIPS , 2020.

Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. NeurIPS , 2022.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of
large language models via positional interpolation. arXiv preprint arXiv:2306.15595 , 2023.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open
instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/
12/dolly-first-open-commercially-viable-instruction-tuned-llm .

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. In ACL, 2019.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691 , 2023.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. NeurIPS , 2022.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of
information-seeking questions and answers anchored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp. 4599–4610, Online, June 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https://aclanthology.org/
2021.naacl-main.365 .

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:
Long form question answering. arXiv preprint arXiv:1907.09190 , 2019.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling. CoRR , abs/2101.00027, 2021. URL
https://arxiv.org/abs/2101.00027 .

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. REALM: Retrieval
augmented language model pre-training. In ICML , 2020.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , June 2020.

--- TRANG 11 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-
mensional transformers. arXiv preprint arXiv:1912.12180 , 2019.

Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan Catanzaro.
Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv
preprint arXiv:2308.07922 , 2023.

Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for open
domain question answering. In EACL , 2021.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning,
2021. URL https://arxiv.org/abs/2112.09118 .

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with
retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.

Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig.
Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. In
EMNLP , 2022.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE
Transactions on Big Data , 7(3):535–547, 2019.

Kaiokendev. Things I'm learning while training SuperHOT. https://kaiokendev.github.
io/til#extending-context-to-8k , 2023.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP ,
2020.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization
through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172 ,
2019.

Hyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe
Alikhani, Gunhee Kim, Maarten Sap, et al. Soda: Million-scale dialogue distillation with social
commonsense contextualization. arXiv preprint arXiv:2212.10465 , 2022.

Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR ,
2020.

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant
conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327 ,
2023.

Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,
and Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions
of the Association for Computational Linguistics , 6:317–328, 05 2018. ISSN 2307-387X. doi:
10.1162/tacl_a_00023. URL https://doi.org/10.1162/tacl_a_00023 .

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. NeurIPS , 2020.

Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out , pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
URL https://aclanthology.org/W04-1013 .

Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih,
and Xilun Chen. How to train your dragon: Diverse augmentation towards generalizable dense
retrieval. arXiv preprint arXiv:2302.07452 , 2023.

--- TRANG 12 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint
arXiv:2307.03172 , 2023.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. Generating wikipedia by summarizing long sequences. In ICLR , 2018.

Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic
scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pp. 4969–4983, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://aclanthology.org/
2020.acl-main.447 .

Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. arXiv preprint arXiv:2305.16300 , 2023.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.

Erik Nijkamp, Hiroaki Hayashi, Tian Xie, Congying Xia, Bo Pang, Congying Xia, and et al. Long
sequence modeling with XGen: A 7b LLM trained on 8k input sequence length. https://
blog.salesforceairesearch.com/xgen/ , 2023.

OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt , 2022.

OpenAI. Gpt-4. https://openai.com/research/gpt-4 , 2023a.

OpenAI. Function calling and other API updates (longer context). https://openai.com/
blog/function-calling-and-other-api-updates , 2023b.

Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,
Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:
Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL https://aclanthology.org/
2022.naacl-main.391 .

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In ICML , pp. 4055–4064, 2018.

Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. In ICLR , 2021.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. 2019.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
Transformers for long-range sequence modelling. In ICLR , 2020.

Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas,
Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large
language models. In ACL, 2023.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics ,
2021.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long
language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing , pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022.
Association for Computational Linguistics. URL https://aclanthology.org/2022.
emnlp-main.823 .

--- TRANG 13 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot
benchmark for long text understanding. arXiv preprint arXiv:2305.14196 , 2023.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. RePlug: Retrieval-augmented black-box language models. arXiv preprint
arXiv:2301.12652 , 2023.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In
ICML , 2020.

Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:
Rethinking self-attention for transformer models. In ICML , 2021.

Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM
Computing Surveys , 2022a.

Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms.
In The Eleventh International Conference on Learning Representations , 2022b.

Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A
heterogenous benchmark for zero-shot evaluation of information retrieval models. In NeurIPS ,
2021.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop
questions via single-hop question composition. Transactions of the Association for Computational
Linguistics , 10:539–554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology.
org/2022.tacl-1.31 .

Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski,
and Piotr Miłoś. Focused transformer: Contrastive training for context scaling. arXiv preprint
arXiv:2307.03170 , 2023.

Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,
Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models
with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762 , 2023.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533 , 2022.

Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 , 2020.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652 , 2021.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 , 2022.

--- TRANG 14 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. AAAI ,
2021.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ,
pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Lin-
guistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259 .

Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric
language models. Transactions of the Association for Computational Linguistics , 9:362–373, 2021.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers for longer
sequences. In NeurIPS , 2020.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 , 2022.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah,
Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark
for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies , pp. 5905–5921, Online, June 2021. Association for Computational Linguis-
tics. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.
naacl-main.472 .

Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,
and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision.
NeurIPS , 2021.

Chúng tôi hiển thị một ví dụ dưới đây trong đó mô hình nhỏ hơn Llama2-7B không thể kết hợp 
ngữ cảnh liên quan, trong khi các mô hình lớn hơn với truy xuất có thể dự đoán thành công câu 
trả lời đúng.

A PHỤ LỤC

--- TRANG 15 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Example showing chunks of text and model responses]
Chunk 1    Vào ngày 18 tháng 9 năm 2015, phiên bản deluxe của album được phát hành
           chứa các track live và nhạc cụ từ album phiên bản tiêu chuẩn, ngoài 
           single "Light" có sự tham gia của Little Dragon. Đánh giá phê bình...
           Angelspit đã lưu diễn với Angel Theory, Ayria, Ikon, KMFDM, Tankt và 
           The Crüxshadows, và cũng đã chia sẻ sân khấu với các ban nhạc như The 
           Sisters of Mercy, Nitzer Ebb, Skinny Puppy và Front Line Assembly. Họ 
           biểu diễn với Lords of Acid trong chuyến lưu diễn 22 ngày tại Hoa Kỳ 
           vào tháng 3 năm 2011 và lưu diễn Hoa Kỳ với Blood on the Dance Floor 
           vào tháng 10 năm 2011. Lịch sử Karl Learmont (ZooG) và Amelia Tan 
           (Destroyx) gặp nhau trên một diễn đàn zine trực tuyến. Họ chia sẻ 
           sở thích về zine và bắt đầu distro Vox Populis vào năm 2002.

Chunk 2    Sau đó họ bắt đầu làm zine cho chính mình điều này trở thành cảm hứng 
           lời bài hát cho các bản phát hành tiếp theo. Angelspit được thành lập 
           vào năm 2003, và bộ đôi sau đó tự phát hành EP đầu tay, Nurse Grenade 
           vào ngày 3 tháng 10 năm 2004... Một video cho bản remix của "Sleep Now" 
           được phát hành vào ngày 2 tháng 10 năm 2010. Họ phát hành album remix 
           thứ ba, Carbon Beauty vào ngày 8 tháng 3 năm 2011. Album remix mới này 
           chứa 3 track mới cũng như 10 bản remix của các track từ album Hideous 
           and Perfect. Một video cho "Toxic Girl" được phát hành vào ngày 13 
           tháng 4 năm 2011, và một video cho "Like It?

Chunk 3    Đoạn 1: Blood on the Dance Floor (ban nhạc) Blood on the Dance Floor 
           là một nhóm nhạc điện tử người Mỹ từ Orlando, Florida, được thành lập 
           vào năm 2006. Đội hình tồn tại lâu nhất của nhóm, từ 2009 đến 2016, 
           bao gồm Jesus David Torres còn được biết đến với tên Dahvie Vanity (sinh 
           năm 1984) và Jayy Von Monroe (sinh năm 1991)... CD được tự phát hành 
           vào tháng 10 năm 2008. Chỉ có 300 bản được sản xuất. Vanity và Ecstasy 
           thu âm các single "Siq With a Q" và "Suicide Club" như một bộ đôi vào 
           năm 2008, và phát hành ba extended play trong nửa đầu năm 2009, I Scream 
           I Scream, OMFG Sneak Peak, và Extended Play.

Chunk 4    tiêu đề: , nguồn: Lick It!" được phát hành vào ngày 27 tháng 7 năm 
           2011. Vào ngày 15 tháng 4 năm 2011, Angelspit thông báo việc bổ sung 
           ba thành viên mới: guitarist Valerie Gentile (Black Tape for a Blue 
           Girl, The Crüxshadows), drummer Chris Kling (Hanzel und Gretyl, Mortiis) 
           và videojammer The Liar. Đội hình mới của Angelspit phát hành album 
           studio thứ tư, Hello My Name Is vào ngày 11 tháng 10 năm 2011. Matt 
           James thay thế Chris Kling vào đầu năm 2012, và cựu guitarist Crüxshadows 
           George Bikos thay thế cho Valerie Gentile trong chuyến lưu diễn năm 
           2012 của ban nhạc...

Chunk 5    Vanity phủ nhận những cáo buộc này trong một video. Vào tháng 4 năm 
           2017, Vanity thông báo rằng sẽ có một thành viên mới, và Blood on the 
           Dance Floor sẽ trở lại vào ngày 5 tháng 5 năm đó mà không có Jayy Von 
           Monroe... Vào ngày 1 tháng 1 năm 2021, Vanity phát hành một phiên bản 
           mới của "Bewitched" của Blood on the Dance Floor với tên "Bewitched 
           Reimagined" có sự tham gia của ca sĩ Sammy Beare. Tính đến năm 2016, 
           Jayy Von Monroe đã tiếp tục làm việc như một drag monster dưới tên "The 
           Dahli" và Vanity đã tiếp tục phát hành nhạc và hàng hóa bởi chính mình 
           dưới cả "The Most Vivid Nightmares" và "Dark Arts Official".

Câu hỏi:   Angelspit lưu diễn Hoa Kỳ với một bộ đôi nhạc điện tử người Mỹ từ 
           Orlando, Florida, được thành lập vào năm nào?

LLaMA2-7b-32k    Angelspit lưu diễn Hoa Kỳ với Blood on the Dance Floor vào tháng 
                 10 năm 2011.

LLaMA2-7b-32k-ret Angelspit lưu diễn Hoa Kỳ với Blood on the Dance Floor vào tháng 
                  10 năm 2011.

LLaMA2-70b-32k   2011

LLaMA2-70b-32k-ret 2006

Sự thật cơ bản   2006

Bảng 7: Tất cả các mô hình đều không thể có được câu trả lời đúng là 2006, ngoại trừ LLaMA2-70b-32k-ret, 
điều này cho thấy truy xuất kết hợp với ngữ cảnh dài có thể giúp đưa ra câu trả lời đúng.

--- TRANG 16 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024

[THIS IS FIGURE: A multi-panel graph showing token length distribution across different datasets (QM, QASP, NQA, QLTY, MSQ, HQA, MFQA) for full documents and top-5, 10, 20 chunks]

Hình 2: Phân phối độ dài token của tài liệu đầy đủ và top-5, 10, 20 chunk của bảy bộ dữ liệu.
