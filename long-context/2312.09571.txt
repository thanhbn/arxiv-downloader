# 2312.09571.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2312.09571.pdf
# File size: 857046 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
EXTENDING CONTEXT WINDOW OF LARGE LAN-
GUAGE MODELS VIA SEMANTIC COMPRESSION
Weizhi Fei† ‡& Xueyan Niu∗‡
†Department of Mathematical Sciences, Tsinghua University, Beijing, China
‡Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.
Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han
Huawei Technologies Co., Ltd.
ABSTRACT
Transformer-based Large Language Models (LLMs) often impose limitations on
the length of the text input to ensure the generation of fluent and relevant re-
sponses. This constraint restricts their applicability in scenarios involving long
texts. We propose a novel semantic compression method that enables general-
ization to texts that are 6-8 times longer, without incurring significant computa-
tional costs or requiring fine-tuning. Our proposed framework draws inspiration
from source coding in information theory and employs a pre-trained model to re-
duce the semantic redundancy of long inputs before passing them to the LLMs for
downstream tasks. Experimental results demonstrate that our method effectively
extends the context window of LLMs across a range of tasks including question
answering, summarization, few-shot learning, and information retrieval. Further-
more, the proposed semantic compression method exhibits consistent fluency in
text generation while reducing the associated computational overhead.
1 I NTRODUCTION
The recent successful release of large language models (LLMs) such as ChatGPT (Radford et al.,
2019) and LLaMA (Touvron et al., 2023) has sparked significant research efforts from both industry
and academia. These LLMs have demonstrated the ability to engage in fluent and coherent conver-
sations with human users, and have shown exceptional performance across various tasks, including
document summarization, question-answering, dialogue bots, and code generation copilots.
One critical issue faced by state-of-the-art (SoTA) LLMs is the restriction on the length of text that
can be inputted into the model at once. When the input context exceeds the limit of the context
window, the performance of these models rapidly declines. This limitation poses a challenge when
it comes to handling long texts such as scientific papers, novels, and legal contracts with current
LLMs. As a result, there has been a growing interest in finding ways to extend the input length
without significantly compromising the model’s performance.
The limitation on the context window primarily stems from the quadratic computation of the self-
attention mechanism in the transformer. Handling lengthy texts significantly increases the compu-
tational costs in terms of memory and time. Typically, models are trained on short contexts, and the
maximum sequence length (i.e., the context window) is determined. If the models are compelled
to generate contexts that exceed the context window, they tend to compromise the quality of the
output due to the lack of position encoding information during the training process. Furthermore,
generating long sequences imposes substantial memory requirements on the computational device.
This accumulation of memory requirements and the lack of effective position encoding can result in
length generalization failure (Anil et al., 2022), where the models struggle to generate meaningful
and coherent text beyond a certain context window size.
Some approaches have been developed to address the aforementioned challenges. One approach
is to devise architectures with nearly linear complexity, which enables efficient scaling to handle
∗Correspondence to: niuxueyan3@huawei.com .
1arXiv:2312.09571v1  [cs.CL]  15 Dec 2023

--- PAGE 2 ---
Figure 1: With the inclusion of the semantic compression module, the redundancies in the input are eliminated,
thereby effectively extending the context window. The semantic compression is reminiscent of the concept of
source coding in information theory.
very long sequences. However, training a large model from scratch incurs substantial cost. Another
strategy involves employing interpolation and fine-tuning techniques to adapt the position encoding
to unseen sequence lengths. While this method has the potential to compromise the overall perfor-
mance of LLMs, it still demands significant time and GPU resources for fine-tuning and inference
on long sequences. Therefore, it is more efficient and resource-friendly to design methods that do
not necessitate altering the parameters of the pre-trained model.
While most previous algorithms relied on modifying the pre-trained model, we instead exploit the
statistical properties of input natural language. One empirical phenomenon, known as Zipf’s law
(Zipf, 2016), observes that a small set of the most frequent word tokens in a large corpus of natural
language account for almost all occurrences. This pattern arises from the tendency of language users
to minimize effort in their daily conversations. Consequently, by utilizing an expanded vocabulary,
sentences can be significantly shortened while preserving the same semantic meaning. Moreover, it
is common for language users to include redundant words during communication (Strunk Jr, 2007).
These language habits are prevalent among users, and we propose to include a semantic compression
module to mitigate the redundancy associated with these habits.
Our proposed semantic compression method, reminiscent of lossy source coding in information
theory, extends the context window by equivalently shortening the long text while preserving the
semantic meaning. This procedure is conducted before inputting the tokens into the pre-trained
LLMs. As illustrated in Fig. 1, the input undergoes compression before being transmitted to the
LLM for various potential tasks. The semantic compression method can be customized and opti-
mized for downstream tasks, taking into consideration practical constraints such as time and memory
resources. The implementation of the semantic compression module is straightforward and can eas-
ily be incorporated into other interpolation-based context window extension methods and black box
APIs. It demonstrates enhanced performance compared to SoTA interpolation-based methods on
a range of tasks, including single-document question answering, multi-document question answer-
ing, summarization, few-shot learning, and information retrieval, using real-world datasets while
incurring no extra parameter updates or memory consumption. Empirically, the proposed method is
computational efficient and achieves 6-8 times context window extension.
Our contributions:
• We introduce a context window extension framework for LLMs that utilizes semantic com-
pression. This framework serves as a plug-and-play tool to mitigate redundancy in input
texts by efficiently performing topic modeling.
• We construct a graph representation of the input to identify distinct sections of the text that
pertain to different topics. The result is the segmentation of long texts into separate chunks,
each focusing on a specific topic. We then conquer each chunk independently, resulting in
a concise version of the original texts. This compression technique helps to condense the
information while preserving the key ideas and context.
• We demonstrate the applicability of our proposed semantic compression method through
extensive experiments. The results highlight the advantages of our method in several key
applications, including single-document question answering, multi-document question an-
swering, summarization, few-shot learning, and information retrieval.
2 R ELATED WORK
With the advancement of SoTA LLMs, significant progress has been made in extending the context
window lengths.
2

--- PAGE 3 ---
2.1 E XTRAPOLATION AND INTERPOLATION
The mainstream line of research aims to adapt existing language models trained on short texts to ac-
commodate longer ones during inference (Anil et al., 2022). The key idea is to modify the positional
embedding, which has only been trained on short texts. Several studies are based on the Rotary Po-
sition Embeddings (RoPE) of LLaMA and methods of adjusting it to the longer sequences. Chen
et al. (2023a) develops the Position Interpolation (PI) method to linearly scale the input positional
indices. Peng et al. (2023) presents YaRN, an efficient extrapolate mechanism inspired by the neural
tangent kernel, to extend the context window to 64k and 128k.
2.2 E FFICIENT ATTENTION OPERATIONS
Due to the self-attention mechanism, the inference cost of LLMs grows quadratically with the se-
quence length. Many methods have been proposed to decrease the complexity. Dai et al. (2019)
present Transformer-XL which utilize segment-level recurrence agency and a novel positional en-
coding scheme. Beltagy et al. (2020) introduce Longformer with a sparse attention mechanism that
scales linearly with sequence length. Bo (2021) provides a faster transformer, RWKV , which com-
bines the strength of RNN and has linear complexity during inference. Dao et al. (2022) propose
FlashAttention, a chunking strategy for the input, and utilize recomputation to avoid the quadratic
complexity of attention computation. While these methods have the potential to handle longer input
sequences (Ding et al., 2023), training new models can be costly. Moreover, these methods are not
effective when dealing with out-of-distribution content lengths.
The introduction of new positional embeddings requires fine-tuning on long sequences to adapt to the
increased length, which can be computationally expensive. To address this, LongLoRA is introduced
by Chen et al. (2023b), offering an efficient fine-tuning method with limited computational costs.
More details on several other chunking strategies are provided in the survey by Huang et al. (2023).
2.3 P ROMPTING
There are ongoing efforts to extend the context window through smart prompting designs. Wingate
et al. (2022) utilize soft prompts to encode more information using fewer tokens. Chevalier et al.
(2023) present AutoCompressor, which utilizes soft prompts to compress the input sequence and
then extends the original length of the base model. Both Zhou et al. (2023) and Wang et al. (2023)
recurrently apply LLMs to summarize the input texts to maintain long short-term memory for spe-
cific purposes such as story writing and dialogue generation, respectively.
3 M ETHODOLOGY
We propose our semantic compression method for extending the context window. The core idea is to
compress the input into shorter texts without losing the key information and important details. This
enables us to effectively include more content within the fixed input length constraint of the LLM.
Fig. 2 provides an overview of our method, which leverages pre-trained summarization models
commonly used in Natural Language Processing (NLP).
Existing summarization methods also have limitations regarding the length of the input. Here, we
propose a divide-and-conquer based approach that takes into account the structure of the text. By
identifying the topic structure of lengthy texts and dividing them into blocks that exhibit a certain
level of mutual independence, the content within each block can be compressed efficiently due to
their statistical correlation. Each block is then processed in parallel using pre-trained models, and
the results are combined to create a condensed textual input that can be processed by the LLM. This
approach aims to provide a more efficient and effective way of summarizing long texts by leveraging
both the structure and content of the original text.
3.1 M ODEL
Real-world textual content, such as speech and book, frequently displays hierarchical structures,
wherein each section is structured around a particular topic, and different sections differ in topic in
a sequential manner. This hierarchical structure, based on topics, bears resemblance to cliques in
3

--- PAGE 4 ---
Figure 2: An illustration of our semantic compression method. The input text is initially segmented into
topic-based chunks, utilizing the graph representation. Subsequently, these chunks undergo refinement using
pre-trained models to ensure the preservation of key information. Finally, the refined chunks are assembled
in accordance with the original order. The resulting texts, which have been semantically compressed, are
approximately 6-8 times shorter in length compared to the original input. Consequently, they fall within the
context window of the LLMs. Furthermore, for additional extension of the length, other methods such as
extrapolation and interpolation-based techniques can be concatenated.
graphs. To identify this structure within long texts, we utilize weighted graphs to represent them
and employ clustering methods to detect cliques in these graphs. The cliques can then be utilized
to represent the topic-based content of the text, allowing us to obtain chunks based on the semantic
relevance of the topics.
We begin by sequentially constructing sentence-level blocks within given lengths and representing
them as nodes in our graph. In this step, we parse the text into different sentences or sub-sentences
based on punctuation marks. Next, we sequentially fill the sentence-level blocks until they exceed
the desired length before proceeding to the next blocks. Once we have obtained the sentence-level
blocks, we connect the graph representation of long text Gbased on a pre-trained sentence embed-
ding model (e.g., MiniLM (Wang et al., 2020)), where the weight G[i][j]represents the semantic
similarity between the i-th and j-th sentence-level blocks. Typically, this similarity is computed
using cosine similarity, which measures the cosine of the angle between two embeddings. If the
similarity between two blocks is higher, it indicates that they are closer in topics.
3.2 T OPIC -BASED CHUNKING
We then apply clustering algorithms on the graph to identify the underlying topic structure. Within
each cluster, we group the sentence-level blocks sequentially to obtain the topic-based chunks, which
can then be handled simultaneously by the pre-trained model chosen according to the downstream
task. The number of clusters can be adjusted to regulate the length of the text following semantic
compression. If these semantic chunks still surpass the predetermined length, the identical procedure
is repeated to acquire sub-level topic structures.
The obtained topic structures are tree-like, which can be flattened in accordance with the order
of the original content. As per the model, each chunk is semantically centered around a specific
topic, and these topics are mutually exclusive. Consequently, these chunks can be compressed
independently by utilizing a pre-trained summarization model. Choosing from different pre-trained
summarization models allows a trade-off between efficiency and effectiveness. Consequently, we
can opt to selectively substitute the original chunks with the output of these pre-trained models to
ensure the preservation of the underlying topic structure. The semantic compressed text can be
forwarded to the LLM directly or in combination with other extension schemes to further enhance
the overall outcome.
4

--- PAGE 5 ---
Figure 3: Example of synthetic prompt for the passkey retrieval task (Mohtashami & Jaggi, 2023). The pre-
trained LLM is incapable of processing long input due to the context length constraint. By applying semantic
compression, the redundant information in the long document is removed, and the compressed input retains
essential key information. The LLM can then process the compressed input along with the prompt to generate
the accurate answer. Notably, the distinct colors used in the illustration correspond to topic-based chunks.
4 E XPERIMENTS
We demonstrate that the proposed method of semantic compression can effectively extend the con-
text window by up to 7-8 times without modifying the parameters of the pre-trained models. Fur-
thermore, the semantic compression module can be seamlessly integrated with existing methods,
allowing for further extension of the context window. This versatility enables our approach to be
adapted and combined with other techniques, enhancing the overall performance and flexibility. To
evaluate the performance of our method, we conduct experiments on several language tasks that re-
quire understanding of long contexts. These tasks include passkey retrieval, single-document ques-
tion answering, multi-document question answering, summarization, and few-shot learning. In each
task, the model is provided with a sequence of context C(typically lengthy texts) and a sequence
of text Q(e.g., a prompt), and it is expected to generate the output answer A. Additionally, we also
investigate the perplexity metric (Peng et al., 2023), which measures the model’s ability to predict
the text and serves as an indicator of the fluency of the generated output. This analysis allows us to
assess not only the effectiveness but also the quality of the generated output.
4.1 T ASKS AND DATASETS
We begin by evaluating the proposed semantic compression method on various standard benchmark
tasks, utilizing the pre-trained 7B LLaMA model (Touvron et al., 2023). The original context win-
dow size of this model is 4096 . The tasks and datasets employed in our evaluation are sourced from
the SCROLLS benchmark (Shaham et al., 2022) and LongBench (Bai et al., 2023). These datasets
provide comprehensive and diverse contexts for our analysis.
Passkey Retrieval Retrieval has been an important application of LLMs. We evaluate the pro-
posed method using a synthetic task for passkey retrieval introduced by Mohtashami & Jaggi (2023),
where prompts are synthesized to conceal a generated passkey within a randomly chosen section of
a long document. The passkey retrieval task assesses the model’s capacity to extract important in-
formation from any position within lengthy contexts. An illustration of the task is shown in Fig. 3.
The synthetic long text incorporates the passkey digits, and the task for the LLM is to retrieve these
digits from the input text. Further specifics can be found in Appendix A.
5

--- PAGE 6 ---
General NLP Tasks LongBench (Bai et al., 2023) is a multi-task benchmark designed for long
text scenarios, consisting of six distinct tasks. In this study, we focus on the three English tasks from
the set of four natural language tasks, namely single-document question answering, multi-document
question answering, summarization, and few-shot learning. Each of the selected datasets contains
200 instances. Further information can be found in Appendix A.
Fluency We evaluate the fluency of our semantic compression method using the perplexity score,
which is defined as the exponential of the average negative log-likelihood of the probabilistic model
Pon the distribution D,i.e.,
PPL( D, P ) := exp( −Ex∈DlogP(x)).
A smaller perplexity score indicates more fluent sequences that are consistent with the model.
4.2 B ASELINES
We choose SoTA solutions from each mainstream approach as our baselines.
Fixed-size chunking To accommodate long context within a fixed-size context window, chunking
is a straightforward yet efficient approach. In NLP related applications, large pieces of text are
usually broken down into smaller segments for targeted applications. When the input length exceeds
the context window, the fixed-size chunking method (Bai et al., 2023) truncates the input sequence
from the middle. This is because the most significant information typically resides at the beginning
and end of the sequence.
Interpolation-based method YaRN (Peng et al., 2023) is a computationally efficient method for
interpolating position encoding, which dynamically adjusts the Relative Positional Encoding (RoPE)
over dimensions and scales the attention. YaRN offers multiple length-extended models for different
versions of Llama2, with the models being trained on a total of 64 GPUs from 8 ×A100 machines.
In order to ensure a fair comparison, we choose the model based on Llama2 7B, adjusted from 4k to
64k, as our baseline.
Fine-tuning approach LongLoRA (Chen et al., 2023b) is an efficient approach for fine-tuning
that combines LoRA and shifts sparse attention to reduce computational costs. LongLoRA applies
this technique to Llama2 models of different sizes, ranging from Llama2 7B, Llama2 13B, to Llama2
70B, with token lengths extended from 4k to 32k on a single 8 ×A100 device. In order to ensure
a fair and unbiased comparison, we choose the Llama2 7B model with context extension achieved
through improved LoRA fine-tuning as our baseline.
5 R ESULTS
We report the main results along with a comprehensive analysis.
Fluency We utilize the Llama2 model as our baseline to evaluate the fluency of generated texts by
calculating the perplexity (PPL) score. Samples from the GovReport dataset are selected at varying
lengths, and the reference texts are compared to the generated texts during the computation. In cases
where the length of the input text exceeds the context window of Llama2, our semantic compression
module shortens the input, thereby allowing the model to continue generating new content fluently.
The resulting scores are depicted in Fig. 4. The plots indicate that the perplexity of Llama2 initially
decreases, but once it surpasses the window length, it rapidly increases. However, when our semantic
compression method is employed, the PPL remains consistently low. This suggests that our approach
successfully extends the context window up to three times without compromising the generation
quality of the language model.
Passkey Retrieval We present the results of the passkey retrieval task in Fig. 5. When employing
Llama2 for passkey retrieval, we observe a rapid drop in accuracy to zero once the input length
surpasses the window size of 4096 . However, by utilizing our method, the retrieval accuracy of
the Llama2 model remains above 90% even for inputs with lengths of up to 30,000. This indicates
6

--- PAGE 7 ---
0 2000 4000 6000 8000 10000 12000
Lengthe2e4e6e8e10PPLours
llama2Figure 4: Perplexity on the GovReport dataset was
evaluated at different sequence lengths. The perplex-
ity curves of Llama2 (green) and our method (pur-
ple) exhibit similar trends for sequences up to 4k in
length. However, as the sequence length exceeds the
training length of 4k, our method effectively flattens
the perplexity curve, indicating that fluency is pre-
served for longer sequences.
0 10000 20000 30000 40000 50000 60000
Length020406080100Accllama2
ours+llama2
ours+yarn+llama2Figure 5: Comparison between model variants on
the passkey retrieval task. The retrieval accuracy of
the Llama2 baseline (green) drops to zero at about
5k due to out-of-memory issues. Our method (pur-
ple) successfully extends the length to 30k. More-
over, when combined with SoTA extrapolation-based
method YaRN, the context length can be further ex-
tended to over 60k ensuring that the retrieval accuracy
remains consistently above 90%.
that the semantic compression method extends the context window size of the language model by
approximately 7-8 times. Furthermore, we combine our method with the SoTA interpolation-based
method, YaRN, to further expand the context window size to up to 60,000, while consistently main-
taining an accuracy above 90%.
General NLP Tasks We present our results on various general NLP tasks in Table 1, including
single-document question answering, multi-document question answering, summarization, and few-
shot learning. When the token length is less than 4k, there is no need to compress the context, and our
method performs at the same level as the original Llama2 model. However, both the interpolation-
based method YaRN and the fine-tuning approach LongLora negatively impact the performance of
the Llama2 model across almost all tasks. In the 4k-8k range, our method outperforms others in 8
out of 11 tasks. It is worth noting that our model performs slightly worse in the few-shot learning
task. This can be attributed to the fact that few-shot learning necessitates more detailed information,
whereas our compression scheme maintains information within a fixed window. Moving on to the
8k-16k range, our method achieves the best results in 9 out of 12 tasks, exhibiting similar perfor-
mance to the 4k-8k range. In the 16k-32k range, our method outperforms others in 6 out of 11 tasks.
In the 32k+ range, other methods fail due to out-of-memory issues, while our method still maintains
70% of the performance achieved in the 4k range.
6 C ONCLUSION
In this work, we propose a novel approach to addressing the limitation of input length in large
language models using semantic compression. By leveraging the statistical properties of natural
language and exploiting redundancy in communication, we are able to significantly shorten texts
while preserving their semantic meaning. This allows for a 6-8 time extension of the context win-
dow without the need for modifying the parameters of the pre-trained model or incurring addi-
tional computational costs. Furthermore, the implementation of our semantic compression module
is straightforward and can be easily integrated into other interpolation-based methods and black box
APIs. This provides flexibility and adaptability to different downstream tasks, considering practical
constraints such as time and memory resources. We believe our work can lead to simpler context
window extension method to be used in practice, thereby reducing the cost of large language models.
7

--- PAGE 8 ---
Task Dataset
(length)MethodLong
LoRALong
LoRA (4k)yarn yarn
(4k)ours ours
(4k)4k
4k-8k
Single-Doc QANarrativeQA - - - - - - 18.7
Qasper 11.6 11 .8 13.4 12 .123.430.619.2
MultiFieldQA-en 24.5 13 .2 34.9 32 .937.458.736.8
Multi-Doc QAHotpotQA 11.5 8 .3 11.3 22 .650.650.025.4
2WikiMultihopQA 10.1 10 .6 8.9 14 .429.861.832.8
MuSiQue 10.0 - 21.1- 50.0- 9.4
SummarizationGovReport 24.7 28 .9 28.8 35 .031.832.227.3
QMSum 20.3 17 .0 22.818.721.1 22 .220.8
MutiNews 0.0 0 .0 1.2 18 .923.227.825.8
Few-Shot
LearningTREC 65.8 54 .2 70.950.055.7 54 .261.5
TriviaQA 87.6 80 .6 90.988.983.3 75 .077.8
SAMSum 43.140.8 40.4 39 .941.6 43 .340.7
8k-16k
Single-Doc QANarrativeQA 9.2 - 13.9- 19.6- 18.7
Qasper - 11.8 10.3 12 .120.930.119.2
MultiFieldQA-en 22.5 13 .2 18.9 32 .935.958.736.8
Multi-Doc QAHotpotQA 8.9 8 .3 8.7 22 .628.150.025.4
2WikiMultihopQA 9.5 10 .6 9.9 14 .426.361.832.8
MuSiQue 6.1 - 4.2 - 16.8- 9.4
SummarizationGovReport 24.0 28 .9 25.1 35 .027.332.227.3
QMSum 22.5 17 .0 21.8 18 .723.422.220.8
MutiNews 0.0 0 .0 0.0 18 .922.027.825.8
Few-Shot
LearningTREC 80.454.2 77.3 50 .057.7 54 .261.5
TriviaQA 86.5 80 .6 89.188.978.7 75 .077.8
SAMSum 44.540.8 43.8 39 .941.7 43 .340.7
16k-32k
Single-Doc QANarrativeQA 12.4- 8.6 - 9.8 - 18.7
Qasper - 11.8 9.2 12 .115.230.119.2
MultiFieldQA-en 36.513.2 32.632.923.6 58 .736.8
Multi-Doc QAHotpotQA 9.3 8 .3 10.1 22 .625.750.025.4
2WikiMultihopQA 7.9 10 .6 10.7 14 .430.461.832.8
MuSiQue 5.4 - 5.0 - 14.6- 9.4
SummarizationGovReport 24.7 28 .9 26.435.025.4 32 .227.3
QMSum 20.0 17 .0 20.8 18 .721.222.221.5
MutiNews 0.3 0 .0 0.3 18 .921.127.826.4
Few-Shot
LearningTREC - 54.2 - 50.0- 54.261.5
TriviaQA 88.8 80 .6 90.188.981.1 75 .077.8
SAMSum 44.740.8 43.6 39 .939.4 43 .340.7
32k+
Single-Doc QA NarrativeQA oom - oom - 19.0- 18.7
SummarizationGovReport oom 28.9 oom 35.021.7 32 .227.3
QMSum oom 17.0 oom 18.722.4 22 .221.5
Table 1: Comparison of our semantic compression method with other baseline methods on a variety of tasks
from the LongBench dataset. Method (4k) denotes evaluation results on texts shorter than 4k. The last column,
labeled 4k, showcases the performance of the Llama2-7B-chat-4k baseline. Notably, our method consistently
outperforms or achieves similar results compared to other SoTA length extension methods.
8

--- PAGE 9 ---
REFERENCES
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-
brose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization
in large language models. Advances in Neural Information Processing Systems , 35:38546–38556,
2022.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long
context understanding. arXiv preprint arXiv:2308.14508 , 2023.
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv:2004.05150 , 2020.
PENG Bo. Blinkdl/rwkv-lm: 0.01, August 2021. URL https://doi.org/10.5281/
zenodo.5196577 .
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window
of large language models via positional interpolation. arXiv preprint arXiv:2306.15595 , 2023a.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora:
Efficient fine-tuning of long-context large language models. arXiv , 2023b.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language
models to compress contexts. ArXiv , abs/2305.14788, 2023. URL https://api.
semanticscholar.org/CorpusID:258865249 .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics , pp. 2978–2988, Florence,
Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL
https://aclanthology.org/P19-1285 .
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-
ing Systems , 2022.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset
of information-seeking questions and answers anchored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies , pp. 4599–4610, 2021.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.
Longnet: Scaling transformers to 1,000,000,000 tokens. In Proceedings of the 10th International
Conference on Learning Representations , 2023.
Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A
large-scale multi-document summarization dataset and abstractive hierarchical model. In Pro-
ceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 1074–
1084, 2019.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-
hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel,
and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational
Linguistics , pp. 6609–6625, Barcelona, Spain (Online), December 2020. International Com-
mittee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https:
//aclanthology.org/2020.coling-main.580 .
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for
long document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou
(eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pp. 1419–1436, Online, June
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL
https://aclanthology.org/2021.naacl-main.112 .
9

--- PAGE 10 ---
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan
Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large
language models: A comprehensive survey, 2023.
Tom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis,
and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the
Association for Computational Linguistics , 6:317–328, 2018.
Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics , 2002.
Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. arXiv preprint arXiv:2305.16300 , 2023.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
extension of large language models, 2023.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI Blog , 1(8):9, 2019.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language se-
quences. arXiv preprint arXiv:2201.03533 , 2022.
William Strunk Jr. The Elements of Style . Penguin, 2007.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Mul-
tihop questions via single-hop question composition. Transactions of the Association for
Computational Linguistics , 10:539–554, 2022. doi: 10.1162/tacl a00475. URL https:
//aclanthology.org/2022.tacl-1.31 .
Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. Recur-
sively summarizing enables long-term dialogue memory in large language models. arXiv preprint
arXiv:2308.15022 , 2023.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-
attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neu-
ral Information Processing Systems , 33:5776–5788, 2020.
David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive
conditioning for controllability and toxicity reduction in language models. In Findings of the As-
sociation for Computational Linguistics: EMNLP 2022 , pp. 5621–5634, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.
findings-emnlp.412. URL https://aclanthology.org/2022.findings-emnlp.
412.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,
and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question
answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP) ,
2018.
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli
Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-
domain meeting summarization. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies , pp.
5905–5921, 2021.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou,
Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long
text, 2023.
10

--- PAGE 11 ---
George Kingsley Zipf. Human Behavior and the Principle of Least Effort: An Introduction to Human
Ecology . Ravenio Books, 2016.
A D ATASETS
Single-Doc QA
•NarrativeQA (Koˇcisk`y et al., 2018) is a standard question-answering dataset that includes
books from Project Gutenberg3 and movie screenplays from a list of websites. Question-
answer pairs were provided by annotators, so that each of the 1,567 books and scripts has
about 30 questions and answers, and two reference answers are given for each question.
•Qasper (Dasigi et al., 2021) is a question-answering dataset of NLP publications contain-
ing abstractive, extractive, and yes/no questions.
•MultiFieldQA-en (Bai et al., 2023) is a dataset created from multiple sources including
legal documents, government reports, encyclopedias, and academic publications. Doctoral
students were requested to annotate each article’s queries and responses.
Multi-Doc QA
•HotpotQA (Yang et al., 2018) includes many 2-hop questions written by native speakers
based on two related paragraphs.
•2WikiMultihopQA (Ho et al., 2020) involves up-to 5-hop questions systematacially con-
structed by manual templates. Answering these questions requires reasoning paths and can
not be solved by local content.
•MuSiQue (Trivedi et al., 2022) consists of up to 4-hop questions and removes shortcuts and
naturalness questions. Each question contains 2-4 supplement paragraphs which present the
reasoning path and related paragraphs.
Summarization
•GovReport (Huang et al., 2021) collects detailed reports containing human-written sum-
maries from the U.S. Government Accountability Office and Congressional Research Ser-
vice. These reports span a wide variety of national policy issues.
•QMSum (Zhong et al., 2021) contains annotated meeting-summary pairs across many do-
mains including including product, academic, and committee meetings.
•MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset. (Bai et al.,
2023) cluster 2-10 news articles discussing the same event or topic, each paired with a
human-written summary and form a new long text summarization task.
Few-Shot Learning To construct few-shot learning with long text, (Bai et al., 2023) select a range
of training examples in the following datasets to concatenate the context in LongBench.
•TREC (Li & Roth, 2002) is a classification dataset with fine-grained class label.
•TriviaQA (Zhong et al., 2021) is a classification dataset and involves messenger-like con-
versations with human-written summaries.
•SAMSum (Fabbri et al., 2019) reading comprehension dataset and consists of question-
answer pairs annotated with evidence passages.
Passkey The randomly generated prompts of the passkey retrieval task is in the format of Fig. 6.
B I MPLEMENTATION DETAILS
In this section, we provide details of our algorithm implementation. Our algorithm utilizes sev-
eral mature open-source models. For graph representation, we make use of the sentence simi-
larity models all-MiniLM-L6-v2 provided by the Sentence Transformer platform, which can be
11

--- PAGE 12 ---
There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (Repeat X Times) The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The pass key is 0000Remember it. 0000is the pass key. The sun is yellow. Here we go. There and back again(Repeat X Times) The grass is green. The sky is blue. The sun is yellow. Here we go. What is the pass key? The pass key isFigure 6: The query prompt contains task descriptions, redundant information, passkey information, redundant
information, and query information. The passkey information is randomly placed within the text, while the
remaining space up to a specified length is filled with redundant information.
found at the following link: https://huggingface.co/sentence-transformers/
all-MiniLM-L6-v2 . For semantic compression, we employ the pre-trained model distilbart-
cnn-12-61. In most of our experiments, we utilize Llama2-7B-chat-4k as the base large language
model (Touvron et al., 2023). The experiments were conducted on a single A40 GPU with 48GB
memory.
C C OMPLEXITY
Given a context with length L, the origin complexity is O(L2). Considering the length limitations of
the compression module, we assume it has a minimum input length γ1and a maximum input length
γ2. We denote the compression ratio as α. Our method utilizes a divide-and-conquer strategy,
dividing the long text into chunks where the total length is represented as L=l1+···+lk, and
each chunk’s length, li, satisfies the condition γ1≤li≤γ2. By kγ1≤L, we can bound the
complexity of the compression module
kX
i=1l2
i≤kX
i=1γ2
2=kγ2
2≤γ2
2
γ1L. (1)
The complexity of inferring the compressed context is
(kX
i=1αli)2= (αkX
i=1li)2=α2L2. (2)
Thus the main complexity of our algorithms can be bounded byγ2
2
γ1L+α2L2.
The result suggests that our algorithm can reduce the computational complexity by a factor of the
square of the compression ratio during the inference stage. The compression module exhibits linear
growth and can be processed in parallel.
1Available at: https://huggingface.co/sshleifer/distilbart-cnn-12-6
12
