# 2406.19707.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/long-context/2406.19707.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 3025464 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
InfiniGen: Suy luáº­n sinh táº¡o hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n vá»›i
Quáº£n lÃ½ KV Cache Ä‘á»™ng
Wonbeom Leeâ€ Jungi Leeâ€ Junghwan Seo Jaewoong Sim
Seoul National University
TÃ³m táº¯t
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) dá»±a trÃªn Transformer thá»ƒ hiá»‡n
hiá»‡u suáº¥t áº¥n tÆ°á»£ng trÃªn nhiá»u tÃ¡c vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn
khÃ¡c nhau. Tuy nhiÃªn, viá»‡c phá»¥c vá»¥ suy luáº­n LLM Ä‘á»ƒ táº¡o ra ná»™i dung
dÃ i Ä‘áº·t ra thÃ¡ch thá»©c do dáº¥u chÃ¢n bá»™ nhá»› khá»•ng lá»“ cá»§a tráº¡ng thÃ¡i
táº¡m thá»i, Ä‘Æ°á»£c gá»i lÃ  bá»™ nhá»› Ä‘á»‡m key-value (KV), Ä‘iá»u nÃ y má»Ÿ rá»™ng
theo Ä‘á»™ dÃ i chuá»—i vÃ  kÃ­ch thÆ°á»›c batch. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i
trÃ¬nh bÃ y InfiniGen, má»™t khung quáº£n lÃ½ KV cache má»›i Ä‘Æ°á»£c thiáº¿t káº¿
riÃªng cho viá»‡c táº¡o vÄƒn báº£n dÃ i, hoáº¡t Ä‘á»™ng hiá»‡p Ä‘á»“ng vá»›i cÃ¡c há»‡ thá»‘ng
suy luáº­n dá»±a trÃªn offloading hiá»‡n Ä‘áº¡i. InfiniGen táº­n dá»¥ng hiá»ƒu biáº¿t
chÃ­nh ráº±ng má»™t sá»‘ token quan trá»ng cáº§n thiáº¿t Ä‘á»ƒ tÃ­nh toÃ¡n lá»›p attention
tiáº¿p theo trong Transformer cÃ³ thá»ƒ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n báº±ng cÃ¡ch thá»±c hiá»‡n
má»™t phÃ©p diá»…n táº­p tá»‘i thiá»ƒu vá»›i cÃ¡c Ä‘áº§u vÃ o cá»§a lá»›p hiá»‡n táº¡i vÃ  má»™t
pháº§n trá»ng sá»‘ query vÃ  key cache cá»§a lá»›p tiáº¿p theo. Äiá»u nÃ y cho phÃ©p
chÃºng ta prefetch chá»‰ nhá»¯ng má»¥c KV cache thiáº¿t yáº¿u (mÃ  khÃ´ng cáº§n
fetch táº¥t cáº£), tá»« Ä‘Ã³ giáº£m thiá»ƒu overhead fetch tá»« bá»™ nhá»› host trong
cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM dá»±a trÃªn offloading. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i
trÃªn má»™t sá»‘ LLM Ä‘áº¡i diá»‡n cho tháº¥y InfiniGen cáº£i thiá»‡n hiá»‡u suáº¥t tá»•ng thá»ƒ
cá»§a má»™t há»‡ thá»‘ng dá»±a trÃªn offloading hiá»‡n Ä‘áº¡i lÃªn Ä‘áº¿n 3.00Ã— so vá»›i
cÃ¡c phÆ°Æ¡ng phÃ¡p quáº£n lÃ½ KV cache trÆ°á»›c Ä‘Ã¢y trong khi cung cáº¥p Ä‘á»™
chÃ­nh xÃ¡c mÃ´ hÃ¬nh tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ.

1 Giá»›i thiá»‡u
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘Ã£ má»Ÿ ra má»™t ká»· nguyÃªn má»›i
trÃªn nhiá»u á»©ng dá»¥ng thá»±c táº¿ nhÆ° chatbot [40, 76], trá»£ lÃ½ mÃ£ hÃ³a
[11, 43], dá»‹ch thuáº­t ngÃ´n ngá»¯ [1, 68], vÃ  tÃ³m táº¯t tÃ i liá»‡u [64, 74].
ThÃ nh cÃ´ng Ä‘Ã¡ng chÃº Ã½ cá»§a LLM cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho pháº§n lá»›n
kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh khá»•ng lá»“, cho phÃ©p xá»­ lÃ½ vÃ  táº¡o ra ná»™i dung
dÃ i má»™t cÃ¡ch hiá»‡u quáº£. VÃ­ dá»¥, trong khi Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a cá»§a
phiÃªn báº£n Ä‘áº§u tiÃªn cá»§a GPT bá»‹ giá»›i háº¡n á»Ÿ 512 token [51], phiÃªn
báº£n má»›i nháº¥t, GPT-4, cÃ³ thá»ƒ xá»­ lÃ½ lÃªn Ä‘áº¿n 32K token, tÆ°Æ¡ng Ä‘Æ°Æ¡ng
vá»›i khoáº£ng 50 trang vÄƒn báº£n [3]. Má»™t sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c cÃ´ng bá»‘
gáº§n Ä‘Ã¢y nhÆ° Claudeâ€ ÄÃ³ng gÃ³p ngang nhau3 [6] vÃ  Gemini 1.5 [53] tháº­m chÃ­ cÃ³ thá»ƒ xá»­ lÃ½ lÃªn Ä‘áº¿n 1 triá»‡u
token, má»Ÿ rá»™ng Ä‘Ã¡ng ká»ƒ cá»­a sá»• ngá»¯ cáº£nh theo nhiá»u
báº­c Ä‘á»™ lá»›n.

NgoÃ i thÃ¡ch thá»©c Ä‘Æ°á»£c nghiÃªn cá»©u ká»¹ vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh,
viá»‡c triá»ƒn khai LLM hiá»‡n gáº·p pháº£i thÃ¡ch thá»©c má»›i do dáº¥u chÃ¢n
Ä‘Ã¡ng ká»ƒ cá»§a tráº¡ng thÃ¡i táº¡m thá»i, Ä‘Æ°á»£c gá»i lÃ  bá»™ nhá»› Ä‘á»‡m key-value
(KV cache), trong quÃ¡ trÃ¬nh xá»­ lÃ½ vÃ  táº¡o ngá»¯ cáº£nh dÃ i. Äá»‘i vá»›i
suy luáº­n LLM sinh táº¡o, cÃ¡c key vÃ  value cá»§a táº¥t cáº£ token trÆ°á»›c Ä‘Ã³
Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n dÆ° thá»«a vÃ  láº·p láº¡i.
Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° trá»ng sá»‘ mÃ´ hÃ¬nh, KV cache má»Ÿ rá»™ng
theo Ä‘á»™ dÃ i chuá»—i Ä‘áº§u ra, thÆ°á»ng tiÃªu thá»¥ tháº­m chÃ­ nhiá»u dung
lÆ°á»£ng bá»™ nhá»› hÆ¡n cáº£ trá»ng sá»‘ mÃ´ hÃ¬nh. Khi nhu cáº§u vá» Ä‘á»™ dÃ i
chuá»—i dÃ i hÆ¡n (cÃ¹ng vá»›i kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n) tiáº¿p tá»¥c tÄƒng,
váº¥n Ä‘á» kÃ­ch thÆ°á»›c KV cache sáº½ trá»Ÿ nÃªn rÃµ rÃ ng hÆ¡n trong tÆ°Æ¡ng lai.

Trong khi Ä‘Ã³, cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n Ä‘áº¡i há»— trá»£ offload
dá»¯ liá»‡u vÃ o bá»™ nhá»› CPU Ä‘á»ƒ phá»¥c vá»¥ LLM má»™t cÃ¡ch hiá»‡u quáº£ trong
ngÃ¢n sÃ¡ch pháº§n cá»©ng [5,57]. CÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading
nÃ y báº¯t Ä‘áº§u há»— trá»£ tháº­m chÃ­ offload KV cache vÃ o bá»™ nhá»› CPU,
tá»« Ä‘Ã³ cho phÃ©p ngÆ°á»i dÃ¹ng táº¡o ra ngá»¯ cáº£nh dÃ i hÆ¡n nhiá»u vÆ°á»£t
quÃ¡ dung lÆ°á»£ng bá»™ nhá»› GPU. Tuy nhiÃªn, viá»‡c chuyá»ƒn kÃ­ch thÆ°á»›c
khá»•ng lá»“ cá»§a KV cache tá»« bá»™ nhá»› CPU sang GPU trá»Ÿ thÃ nh má»™t
bottleneck hiá»‡u suáº¥t má»›i trong suy luáº­n LLM.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t InfiniGen, má»™t khung
quáº£n lÃ½ KV cache Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hoáº¡t Ä‘á»™ng hiá»‡p Ä‘á»“ng vá»›i cÃ¡c
há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading hiá»‡n Ä‘áº¡i. InfiniGen Ä‘Æ°á»£c
xÃ¢y dá»±ng trÃªn hai nguyÃªn táº¯c thiáº¿t káº¿ chÃ­nh. Thá»© nháº¥t, nÃ³ dá»± Ä‘oÃ¡n
vÃ  chá»n cÃ¡c má»¥c KV cache quan trá»ng Ä‘á»ƒ táº¡o ra token Ä‘áº§u ra tiáº¿p
theo, loáº¡i bá» nhá»¯ng má»¥c khÃ´ng quan trá»ng, báº±ng cÃ¡ch thá»±c hiá»‡n
má»™t phÃ©p diá»…n táº­p tá»‘i thiá»ƒu cá»§a tÃ­nh toÃ¡n attention cho Layer i
táº¡i Layer iâˆ’1. Thá»© hai, nÃ³ táº­n dá»¥ng dung lÆ°á»£ng bá»™ nhá»› CPU vÃ 
duy trÃ¬ KV cache pool trÃªn CPU, thay vÃ¬ trÃªn GPU, Ä‘á»ƒ Ä‘áº£m báº£o
ráº±ng cÃ¡c giÃ¡ trá»‹ KV cache quan trá»ng cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh cho
táº¥t cáº£ Ä‘áº§u ra vÃ  layer vá»›i kÃ­ch thÆ°á»›c cá»­a sá»• lá»›n trong khi giáº£m
bá»›t lo ngáº¡i vá» dung lÆ°á»£ng bá»™ nhá»› GPU háº¡n cháº¿ cho viá»‡c táº¡o ná»™i
dung dÃ i.

Cá»¥ thá»ƒ, InfiniGen thao tÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh offline Ä‘á»ƒ lÃ m cho
viá»‡c dá»± Ä‘oÃ¡n hiá»‡u quáº£ vÃ  chÃ­nh xÃ¡c hÆ¡n nhiá»u, báº±ng cÃ¡ch
1arXiv:2406.19707v1  [cs.LG]  28 Jun 2024

--- TRANG 2 ---
lÃ m nghiÃªng kiáº¿n trÃºc Transformer query vÃ  key matrices Ä‘á»ƒ
nháº¥n máº¡nh má»™t sá»‘ cá»™t quan trá»ng nháº¥t Ä‘á»‹nh. Trong giai Ä‘oáº¡n
prefill, trong khi prompt vÃ  Ä‘áº§u vÃ o cá»§a má»™t yÃªu cáº§u suy luáº­n
Ä‘Æ°á»£c xá»­ lÃ½ ban Ä‘áº§u, InfiniGen táº¡o ra cÃ¡c trá»ng sá»‘ má»™t pháº§n Ä‘á»ƒ
sá»­ dá»¥ng trong giai Ä‘oáº¡n decoding (tá»©c lÃ  táº¡o Ä‘áº§u ra) tiáº¿p theo.
Táº¡i Layer iâˆ’1 cá»§a giai Ä‘oáº¡n decoding, InfiniGen dá»± Ä‘oÃ¡n máº«u
attention cá»§a layer tiáº¿p theo (Layer i) báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘áº§u
vÃ o attention cá»§a Layer iâˆ’1, má»™t trá»ng sá»‘ query má»™t pháº§n, vÃ 
má»™t key cache má»™t pháº§n cá»§a Layer i. Dá»±a trÃªn máº«u attention
Ä‘Æ°á»£c dá»± Ä‘oÃ¡n, InfiniGen prefetch cÃ¡c má»¥c KV cache thiáº¿t yáº¿u
tá»« bá»™ nhá»› CPU cho tÃ­nh toÃ¡n attention táº¡i Layer i. Báº±ng cÃ¡ch
Ä‘á»™ng Ä‘iá»u chá»‰nh sá»‘ lÆ°á»£ng má»¥c KV Ä‘á»ƒ prefetch, InfiniGen chá»‰
mang lÆ°á»£ng KV cache cáº§n thiáº¿t vÃ o GPU, tá»« Ä‘Ã³ giáº£m thiá»ƒu
ráº¥t nhiá»u overhead cá»§a viá»‡c chuyá»ƒn KV cache. NgoÃ i ra,
InfiniGen quáº£n lÃ½ KV cache pool báº±ng cÃ¡ch Ä‘á»™ng loáº¡i bá» cÃ¡c
má»¥c KV cache cá»§a nhá»¯ng token Ã­t Ä‘Æ°á»£c sá»­ dá»¥ng.

ChÃºng tÃ´i triá»ƒn khai InfiniGen trÃªn má»™t há»‡ thá»‘ng suy luáº­n dá»±a
trÃªn offloading hiá»‡n Ä‘áº¡i [57] vÃ  Ä‘Ã¡nh giÃ¡ nÃ³ trÃªn hai LLM Ä‘áº¡i
diá»‡n vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, kÃ­ch thÆ°á»›c batch, vÃ  Ä‘á»™ dÃ i chuá»—i
khÃ¡c nhau. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i cho tháº¥y InfiniGen Ä‘áº¡t Ä‘Æ°á»£c
tÄƒng tá»‘c lÃªn Ä‘áº¿n 3.00Ã— so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p quáº£n lÃ½ KV
cache hiá»‡n cÃ³ trong khi cung cáº¥p tÄƒng Ä‘á»™ chÃ­nh xÃ¡c lÃªn Ä‘áº¿n
32.6 Ä‘iá»ƒm pháº§n trÄƒm. NgoÃ i ra, InfiniGen liÃªn tá»¥c cung cáº¥p
cáº£i thiá»‡n hiá»‡u suáº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n, Ä‘á»™ dÃ i chuá»—i dÃ i
hÆ¡n, vÃ  kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n, trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p
dá»±a trÃªn nÃ©n trÆ°á»›c Ä‘Ã¢y dáº«n Ä‘áº¿n tÄƒng tá»‘c bÃ£o hÃ²a.

TÃ³m láº¡i, bÃ i bÃ¡o nÃ y Ä‘Ã³ng gÃ³p nhÆ° sau:
â€¢ChÃºng tÃ´i trÃ¬nh bÃ y InfiniGen, má»™t khung quáº£n lÃ½ KV cache
Ä‘á»™ng hoáº¡t Ä‘á»™ng hiá»‡p Ä‘á»“ng vá»›i cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM
dá»±a trÃªn offloading hiá»‡n Ä‘áº¡i báº±ng cÃ¡ch quáº£n lÃ½ thÃ´ng minh
KV cache trong bá»™ nhá»› CPU.
â€¢ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t ká»¹ thuáº­t prefetching KV cache má»›i
vá»›i ephemeral pruning, dá»± Ä‘oÃ¡n máº«u attention cá»§a layer
attention tiáº¿p theo vÃ  chá»‰ mang pháº§n thiáº¿t yáº¿u cá»§a KV cache
vÃ o GPU trong khi giá»¯ láº¡i pháº§n cÃ²n láº¡i trong bá»™ nhá»› CPU.
â€¢ChÃºng tÃ´i triá»ƒn khai InfiniGen trÃªn má»™t há»‡ thá»‘ng suy luáº­n
dá»±a trÃªn offloading hiá»‡n Ä‘áº¡i vÃ  chá»©ng minh ráº±ng nÃ³ vÆ°á»£t
trá»™i hÆ¡n ráº¥t nhiá»u so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p quáº£n lÃ½ KV cache
hiá»‡n cÃ³, Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t nhanh hÆ¡n lÃªn Ä‘áº¿n 3.00Ã— trong
khi cÅ©ng cung cáº¥p Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh tá»‘t hÆ¡n.

2 Kiáº¿n thá»©c ná»n
Pháº§n nÃ y giáº£i thÃ­ch ngáº¯n gá»n quy trÃ¬nh hoáº¡t Ä‘á»™ng vÃ  ká»¹ thuáº­t
KV caching cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n vÃ  giá»›i thiá»‡u phÃ¢n
tÃ­ch giÃ¡ trá»‹ Ä‘Æ¡n (SVD) nhÆ° má»™t phÆ°Æ¡ng phÃ¡p lÃ m nghiÃªng ma
tráº­n Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n vá» khung Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i, mÃ 
chÃºng tÃ´i tháº£o luáº­n trong Pháº§n 4.

2.1 CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘Æ°á»£c cáº¥u thÃ nh tá»« má»™t chá»“ng
cÃ¡c khá»‘i Transformer, má»—i khá»‘i chá»©a má»™t layer attention theo
sau bá»Ÿi má»™t layer feed-forward [61]. Tensor Ä‘áº§u vÃ o (X) cá»§a
khá»‘i Transformer cÃ³ chiá»u NÃ—D, trong Ä‘Ã³ N lÃ  sá»‘ lÆ°á»£ng query
token, vÃ  D lÃ  chiá»u mÃ´ hÃ¬nh. Tensor Ä‘áº§u vÃ o nÃ y (X) Ä‘áº§u tiÃªn
Ä‘Æ°á»£c layer-normalized (LayerNorm), vÃ  tensor Ä‘Æ°á»£c layer-
normalized (Xa) Ä‘Æ°á»£c Ä‘Æ°a vÃ o layer attention nhÆ° Ä‘áº§u vÃ o.
Äáº§u vÃ o attention (Xa) Ä‘Æ°á»£c nhÃ¢n vá»›i ba ma tráº­n trá»ng sá»‘ khÃ¡c
nhau (WQ,WK,WV) Ä‘á»ƒ táº¡o ra cÃ¡c ma tráº­n Query (Q), Key (K),
vÃ  Value (V). Má»—i ma tráº­n trá»ng sá»‘ cÃ³ chiá»u DÃ—D. Do Ä‘Ã³,
Query, Key, vÃ  Value cÃ³ chiá»u NÃ—D. CÃ¡c ma tráº­n nÃ y Ä‘Æ°á»£c
reshape Ä‘á»ƒ cÃ³ chiá»u HÃ—NÃ—d, trong Ä‘Ã³ H lÃ  sá»‘ lÆ°á»£ng attention
head vÃ  d lÃ  chiá»u head; lÆ°u Ã½ ráº±ng D=HÃ—d.

Má»—i head Ä‘á»™c láº­p thá»±c hiá»‡n tÃ­nh toÃ¡n attention, cÃ³ thá»ƒ Ä‘Æ°á»£c
cÃ´ng thá»©c hÃ³a nhÆ° sau: softmax(QKT)V.1 Äáº§u ra attention,
sau khi residual add (cá»™ng vÃ o tensor Ä‘áº§u vÃ o X) vÃ  layer
normalization, Ä‘Æ°á»£c Ä‘Æ°a vÃ o layer feed-forward. Máº¡ng feed-
forward (FFN) bao gá»“m hai phÃ©p chiáº¿u tuyáº¿n tÃ­nh liÃªn tiáº¿p
vÃ  má»™t phÃ©p toÃ¡n kÃ­ch hoáº¡t phi tuyáº¿n giá»¯a chÃºng. Äáº§u ra cá»§a
FFN sau khi residual add trá»Ÿ thÃ nh Ä‘áº§u ra cá»§a khá»‘i Transformer,
cÃ³ cÃ¹ng chiá»u vá»›i Ä‘áº§u vÃ o cá»§a khá»‘i Transformer (tá»©c lÃ  NÃ—D).
Äiá»u nÃ y cho phÃ©p chÃºng ta dá»… dÃ ng má»Ÿ rá»™ng LLM báº±ng cÃ¡ch
Ä‘iá»u chá»‰nh sá»‘ lÆ°á»£ng khá»‘i Transformer.

2.2 Suy luáº­n sinh táº¡o vÃ  KV Caching
Suy luáº­n LLM sinh táº¡o thÆ°á»ng bao gá»“m hai giai Ä‘oáº¡n chÃ­nh:
giai Ä‘oáº¡n prefill vÃ  giai Ä‘oáº¡n decoding. Trong giai Ä‘oáº¡n prefill,
LLM tÃ³m táº¯t ngá»¯ cáº£nh cá»§a chuá»—i Ä‘áº§u vÃ o (tá»©c lÃ  prompt Ä‘áº§u
vÃ o) vÃ  táº¡o ra má»™t token má»›i phá»¥c vá»¥ nhÆ° Ä‘áº§u vÃ o ban Ä‘áº§u cho
giai Ä‘oáº¡n decoding. Tiáº¿p theo, sá»­ dá»¥ng token má»›i nÃ y, LLM
cháº¡y giai Ä‘oáº¡n decoding Ä‘á»ƒ táº¡o ra token tiáº¿p theo. Token má»›i
Ä‘Æ°á»£c táº¡o sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a trá»Ÿ láº¡i vÃ o giai Ä‘oáº¡n decoding nhÆ°
Ä‘áº§u vÃ o, táº¡o ra má»™t quÃ¡ trÃ¬nh autoregressive cho viá»‡c táº¡o token.
Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i gá»i má»—i láº§n táº¡o token trong
giai Ä‘oáº¡n decoding lÃ  má»™t iteration.

Äá»ƒ táº¡o ra má»™t token má»›i phÃ¹ há»£p vá»›i ngá»¯ cáº£nh, LLM cáº§n tÃ­nh
toÃ¡n má»‘i quan há»‡ giá»¯a token cuá»‘i cÃ¹ng vÃ  táº¥t cáº£ cÃ¡c token trÆ°á»›c
Ä‘Ã³, bao gá»“m cÃ¡c token tá»« chuá»—i Ä‘áº§u vÃ o, trong layer attention.
Má»™t cÃ¡ch tiáº¿p cáº­n ngÃ¢y thÆ¡ cho viá»‡c nÃ y lÃ  tÃ­nh toÃ¡n láº¡i key
vÃ  value cá»§a táº¥t cáº£ token trÆ°á»›c Ä‘Ã³ táº¡i má»—i iteration. Tuy nhiÃªn,
Ä‘iá»u nÃ y gÃ¢y ra overhead Ä‘Ã¡ng ká»ƒ do tÃ­nh toÃ¡n dÆ° thá»«a vÃ  láº·p
láº¡i. HÆ¡n ná»¯a, overhead tÃ­nh toÃ¡n tÄƒng tuyáº¿n tÃ­nh vá»›i sá»‘ lÆ°á»£ng
token trÆ°á»›c Ä‘Ã³; tá»©c lÃ  overhead trá»Ÿ nÃªn lá»›n hÆ¡n Ä‘á»‘i vá»›i cÃ¡c
chuá»—i dÃ i hÆ¡n.

Äá»ƒ trÃ¡nh overhead nhÆ° váº­y, cÃ¡c key (K) vÃ  value (V) cá»§a táº¥t
cáº£ token trÆ°á»›c Ä‘Ã³ thÆ°á»ng Ä‘Æ°á»£c ghi nhá»› trong bá»™ nhá»›,
1Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i gá»i káº¿t quáº£ cá»§a QKT vÃ  softmax(QKT) lÃ 
attention scores vÃ  attention weights, tÆ°Æ¡ng á»©ng.
2

--- TRANG 3 ---
Î£ğ”ğœ!ğœ"ğ‘£"ğ‘£!ğ‘"ğ‘!Î£ğ”ğœ!ğœ"(b) Q%=ğ”Î£(ğ•#ğ´)ğ’†ğŸğ’†ğŸğ’’/ğŸğ’’/ğŸ(a) Q=ğ”Î£ğ•#HÃ¬nh 1: Biáº¿n Ä‘á»•i tá»« ma tráº­n VT thÃ nh ma tráº­n Q theo
SVD. Ma tráº­n trá»±c giao A tá»‘i Ä‘a hÃ³a sá»± khÃ¡c biá»‡t vá» Ä‘á»™ lá»›n
giá»¯a cÃ¡c vector cá»™t cá»§a Q.

Ä‘Æ°á»£c gá»i lÃ  KV cache. KV cache sau Ä‘Ã³ Ä‘Æ°á»£c cáº­p nháº­t vá»›i
key vÃ  value cá»§a token Ä‘Æ°á»£c táº¡o táº¡i má»—i iteration. NhÆ° váº­y,
chiá»u cá»§a KV cache táº¡i iteration thá»© i cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n
nhÆ° HÃ—(N+i)Ã—d. Náº¿u suy luáº­n batch Ä‘Æ°á»£c sá»­ dá»¥ng, kÃ­ch
thÆ°á»›c cá»§a KV cache cÅ©ng tÄƒng tuyáº¿n tÃ­nh theo kÃ­ch thÆ°á»›c
batch. Báº±ng cÃ¡ch sá»­ dá»¥ng KV cache, chÃºng ta cÃ³ thá»ƒ trÃ¡nh
tÃ­nh toÃ¡n láº·p láº¡i vÃ  chá»‰ táº¡o ra key vÃ  value cá»§a má»™t token
táº¡i má»—i iteration. LÆ°u Ã½ ráº±ng trong giai Ä‘oáº¡n decoding, Ä‘áº§u
vÃ o cho khá»‘i Transformer (X) cÃ³ chiá»u 1Ã—D, vÃ  chiá»u cá»§a
ma tráº­n attention score trá»Ÿ thÃ nh HÃ—1Ã—(N+i) táº¡i iteration
thá»© i.

2.3 Outlier trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n cÃ³ outlier trong cÃ¡c tensor Ä‘áº§u
vÃ o khá»‘i Transformer. CÃ¡c outlier Ä‘á» cáº­p Ä‘áº¿n cÃ¡c pháº§n tá»­
cÃ³ Ä‘á»™ lá»›n lá»›n hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c pháº§n tá»­ khÃ¡c. CÃ¡c
outlier trong LLM xuáº¥t hiá»‡n trong má»™t sá»‘ kÃªnh cá»‘ Ä‘á»‹nh
(tá»©c lÃ  cÃ¡c cá»™t trong ma tráº­n 2D) qua cÃ¡c layer. CÃ´ng trÃ¬nh
trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ cho tháº¥y ráº±ng outlier lÃ  do tÃ­nh cháº¥t ná»™i táº¡i
cá»§a mÃ´ hÃ¬nh (vÃ­ dá»¥: Ä‘á»™ lá»›n lá»›n trong má»™t sá»‘ kÃªnh cá»‘ Ä‘á»‹nh
cá»§a trá»ng sá»‘ layer normalization) [19, 65].

2.4 PhÃ¢n tÃ­ch giÃ¡ trá»‹ Ä‘Æ¡n
ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng viá»‡c lÃ m nghiÃªng cÃ¡c ma tráº­n
query vÃ  key Ä‘á»ƒ lÃ m cho má»™t sá»‘ Ã­t kÃªnh lá»›n hÆ¡n nhiá»u so
vá»›i cÃ¡c kÃªnh khÃ¡c vÃ  chá»‰ sá»­ dá»¥ng nhá»¯ng kÃªnh Ä‘Ã³ Ä‘á»ƒ tÃ­nh
toÃ¡n ma tráº­n attention score cÃ³ thá»ƒ dá»± Ä‘oÃ¡n hiá»‡u quáº£ nhá»¯ng
token nÃ o quan trá»ng. Vá» cÆ¡ báº£n, chÃºng tÃ´i nhÃ¢n cÃ¡c ma
tráº­n Q vÃ  K vá»›i má»™t ma tráº­n trá»±c giao A Ä‘á»ƒ lÃ m cho chÃºng
cÄƒn chá»‰nh vá»›i hÆ°á»›ng mÃ  Q kÃ©o dÃ i nhiá»u nháº¥t, Ä‘á»ƒ táº¡o ra
cÃ¡c ma tráº­n nghiÃªng tÆ°Æ¡ng á»©ng ËœQ vÃ  ËœK. ChÃºng tÃ´i giáº£i
thÃ­ch chi tiáº¿t táº¡i sao chÃºng tÃ´i sá»­ dá»¥ng ma tráº­n trá»±c giao
trong Pháº§n 4.2.

Äá»ƒ tÃ¬m ma tráº­n trá»±c giao A nhÆ° váº­y, chÃºng tÃ´i sá»­ dá»¥ng phÃ¢n
tÃ­ch giÃ¡ trá»‹ Ä‘Æ¡n (SVD), má»™t ká»¹ thuáº­t phÃ¢n tÃ­ch ma tráº­n Ä‘Æ°á»£c
sá»­ dá»¥ng rá»™ng rÃ£i trong Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh. Äá»‘i vá»›i ma tráº­n
thá»±c Q cÃ³ kÃ­ch thÆ°á»›c mÃ—n, phÃ¢n tÃ­ch SVD cá»§a nÃ³ cÃ³ thá»ƒ
Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau:
Q=UÎ£VT,

050100150200250
248163264Size (GB)(b) Batch Size050100150200250
2565121024204840968192Size (GB)(a) Sequence LengthHÃ¬nh 2: Tá»•ng kÃ­ch thÆ°á»›c cá»§a KV cache vÃ  trá»ng sá»‘ mÃ´ hÃ¬nh cá»§a OPT-30B
cho cÃ¡c Ä‘á»™ dÃ i chuá»—i vÃ  kÃ­ch thÆ°á»›c batch khÃ¡c nhau. KÃ­ch thÆ°á»›c batch cá»§a
(a) lÃ  16, vÃ  Ä‘á»™ dÃ i chuá»—i cá»§a (b) lÃ  2048. ÄÆ°á»ng cháº¥m biá»ƒu thá»‹ kÃ­ch
thÆ°á»›c cá»§a trá»ng sá»‘ mÃ´ hÃ¬nh.

trong Ä‘Ã³ U vÃ  V lÃ  cÃ¡c ma tráº­n trá»±c giao cÃ³ kÃ­ch thÆ°á»›c mÃ—m
vÃ  nÃ—n, tÆ°Æ¡ng á»©ng.2 Î£ lÃ  ma tráº­n Ä‘Æ°á»ng chÃ©o mÃ—n, cÃ³ cÃ¡c
giÃ¡ trá»‹ khÃ¡c khÃ´ng (Ïƒ1,Ïƒ2,...,Ïƒk) trÃªn Ä‘Æ°á»ng chÃ©o, trong Ä‘Ã³
k=min(m,n). Vá» máº·t biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh, ngÆ°á»i ta biáº¿t ráº±ng
má»™t biáº¿n Ä‘á»•i cá»§a vector vâˆˆRn bá»Ÿi ma tráº­n thá»±c B (tá»©c lÃ  tÃ­ch
cá»§a B vÃ  v) lÃ  má»™t phÃ©p quay vÃ /hoáº·c pháº£n xáº¡ trong Rn náº¿u
ma tráº­n B lÃ  trá»±c giao. Náº¿u B lÃ  ma tráº­n Ä‘Æ°á»ng chÃ©o mÃ—n,
má»—i chiá»u cá»§a v Ä‘Æ°á»£c kÃ©o dÃ i bá»Ÿi má»¥c Ä‘Æ°á»ng chÃ©o tÆ°Æ¡ng á»©ng
cá»§a B vÃ  Ä‘Æ°á»£c chiáº¿u lÃªn Rm.

VÃ­ dá»¥, HÃ¬nh 1 cho tháº¥y cÃ¡ch cÃ¡c vector cá»™t v1 vÃ  v2 cá»§a VT
sáº½ biáº¿n Ä‘á»•i thÃ nh cÃ¡c vector cá»™t q1 vÃ  q2 cá»§a Q, khi m vÃ  n
lÃ  2. Trong HÃ¬nh 1(a), cÃ¡c vector Ä‘Æ¡n vá»‹ trá»±c giao v1 vÃ  v2
Ä‘áº§u tiÃªn Ä‘Æ°á»£c kÃ©o dÃ i Ä‘áº¿n cÃ¡c Ä‘iá»ƒm trÃªn má»™t ellipse cÃ³ Ä‘á»™
dÃ i bÃ¡n trá»¥c tÆ°Æ¡ng á»©ng vá»›i cÃ¡c má»¥c Ä‘Æ°á»ng chÃ©o trong Î£.
CÃ¡c vector sau Ä‘Ã³ Ä‘Æ°á»£c quay vÃ /hoáº·c pháº£n xáº¡ thÃ nh q1 vÃ 
q2 bá»Ÿi ma tráº­n U. Máº·t khÃ¡c, HÃ¬nh 1(b) cho tháº¥y cÃ¡ch ma
tráº­n trá»±c giao A thá»±c hiá»‡n phÃ©p quay Ä‘á»ƒ lÃ m cho Ëœq1 káº¿t quáº£
lá»›n hÆ¡n nhiá»u so vá»›i Ëœq2. Cá»¥ thá»ƒ, A quay cÃ¡c vector v1 vÃ 
v2 thÃ nh e1 vÃ  e2, Ã¡nh xáº¡ Ä‘áº¿n cÃ¡c bÃ¡n trá»¥c cá»§a ellipse. Theo
cÃ¡ch nÃ y, cÃ¡c vector Ä‘Æ°á»£c kÃ©o dÃ i Ä‘áº¿n má»©c tá»‘i Ä‘a vÃ  tá»‘i thiá»ƒu
bá»Ÿi ma tráº­n Î£. QuÃ¡ trÃ¬nh nÃ y nháº¥n máº¡nh Ä‘á»™ lá»›n cá»§a Ëœq1 so
vá»›i Ëœq2, cho phÃ©p chÃºng ta dá»± Ä‘oÃ¡n hiá»‡u quáº£ attention score
chá»‰ sá»­ dá»¥ng Ëœq1 trong khi bá» qua Ëœq2.

3 Äá»™ng lá»±c
Trong pháº§n nÃ y, trÆ°á»›c tiÃªn chÃºng tÃ´i giáº£i thÃ­ch ráº±ng kÃ­ch
thÆ°á»›c KV cache trá»Ÿ thÃ nh váº¥n Ä‘á» nghiÃªm trá»ng cho viá»‡c
táº¡o vÄƒn báº£n dÃ i trong suy luáº­n LLM, vÃ  nÃ³ trá»Ÿ nÃªn cÃ³ váº¥n
Ä‘á» hÆ¡n khi triá»ƒn khai cÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading
hiá»‡n Ä‘áº¡i (Pháº§n 3.1). Sau Ä‘Ã³ chÃºng tÃ´i tháº£o luáº­n táº¡i sao cÃ¡c
phÆ°Æ¡ng phÃ¡p quáº£n lÃ½ KV cache hiá»‡n cÃ³ khÃ´ng thá»ƒ giáº£i quyáº¿t
cÆ¡ báº£n váº¥n Ä‘á» trong há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading
(Pháº§n 3.2).

3.1 KV Cache trong cÃ¡c há»‡ thá»‘ng suy luáº­n LLM
NhÆ° Ä‘Ã£ tháº£o luáº­n trong Pháº§n 2.2, cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM
ngÃ y nay khai thÃ¡c KV caching Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n dÆ° thá»«a
cá»§a cÃ¡c phÃ©p chiáº¿u key vÃ 
2LÆ°u Ã½ ráº±ng V nÃ y, Ä‘Æ°á»£c viáº¿t báº±ng font khÃ¡c, lÃ  má»™t trong cÃ¡c ma tráº­n káº¿t
quáº£ cá»§a SVD vÃ  khÃ¡c biá»‡t vá»›i V cá»§a ma tráº­n Value trong layer attention
Transformer.
3

--- TRANG 4 ---
(a) Full GPU(b) KV cache on CPU(c) Prefetch KV cacheLoad CacheFFNAttentionPer-block LatencyAttentionFFNtimeAttentionFFNLoad CacheAttentionFFNtimeAttentionFFNLoad CacheAttentionFFNtimeAttentionFFN(d) Prefetch critical KVLoad CacheAttnFFNtimeMaximum ReductionHÃ¬nh 3: So sÃ¡nh giá»¯a cÃ¡c kiá»ƒu thá»±c thi khÃ¡c nhau cá»§a cÃ¡c khá»‘i
Transformer.

value trong giai Ä‘oáº¡n decoding. Máº·c dÃ¹ Ä‘Ã¢y lÃ  má»™t giáº£i phÃ¡p
hiá»‡u quáº£ cho viá»‡c táº¡o chuá»—i ngáº¯n vá»›i má»™t yÃªu cáº§u client duy
nháº¥t, KV cache nhanh chÃ³ng trá»Ÿ thÃ nh ngÆ°á»i tiÃªu thá»¥ bá»™ nhá»›
chÃ­nh khi chÃºng ta táº¡o ra cÃ¡c chuá»—i dÃ i hoáº·c sá»­ dá»¥ng cÃ¡c ká»¹
thuáº­t batch yÃªu cáº§u hiá»‡n Ä‘áº¡i [57, 71].

HÃ¬nh 2 cho tháº¥y kÃ­ch thÆ°á»›c káº¿t há»£p cá»§a trá»ng sá»‘ LLM vÃ  KV
cache qua cÃ¡c Ä‘á»™ dÃ i chuá»—i vÃ  kÃ­ch thÆ°á»›c batch khÃ¡c nhau.
NhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong hÃ¬nh, kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh giá»¯ nguyÃªn
báº¥t ká»ƒ Ä‘á»™ dÃ i chuá»—i hoáº·c kÃ­ch thÆ°á»›c batch, trong khi kÃ­ch
thÆ°á»›c KV cache tÄƒng tuyáº¿n tÃ­nh theo chÃºng. LÆ°u Ã½ ráº±ng cÃ¡c
há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n Ä‘áº¡i, nhÆ° NVIDIA Triton Inference
Server [45] vÃ  TensorFlow Serving [47], Ä‘Ã£ há»— trá»£ suy luáº­n
batch Ä‘á»ƒ táº­n dá»¥ng tÃ­nh toÃ¡n tá»‘t hÆ¡n vÃ  thÃ´ng lÆ°á»£ng cao hÆ¡n
trong viá»‡c phá»¥c vá»¥ cÃ¡c yÃªu cáº§u client. Khi cÃ¡c yÃªu cáº§u riÃªng
láº» Ä‘Æ°á»£c batch, má»—i yÃªu cáº§u giá»¯ KV cache riÃªng cá»§a nÃ³, tá»« Ä‘Ã³
tÄƒng kÃ­ch thÆ°á»›c KV cache tá»•ng thá»ƒ cho suy luáº­n. Ngay cáº£
Ä‘á»‘i vá»›i má»™t yÃªu cáº§u client duy nháº¥t, beam search [59] vÃ 
parallel sampling [20] Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ táº¡o ra Ä‘áº§u
ra tá»‘t hÆ¡n hoáº·c Ä‘á»ƒ cung cáº¥p cho client lá»±a chá»n cÃ¡c á»©ng viÃªn
[11, 24]. CÃ¡c ká»¹ thuáº­t nÃ y cÅ©ng tÄƒng kÃ­ch thÆ°á»›c KV cache
nhÆ° suy luáº­n batch vÃ¬ nhiá»u chuá»—i Ä‘Æ°á»£c xá»­ lÃ½ cÃ¹ng nhau.
Do Ä‘Ã³, kÃ­ch thÆ°á»›c KV cache cÃ³ thá»ƒ dá»… dÃ ng vÆ°á»£t quÃ¡ kÃ­ch
thÆ°á»›c mÃ´ hÃ¬nh cho nhiá»u trÆ°á»ng há»£p sá»­ dá»¥ng thá»±c táº¿, nhÆ°
cÅ©ng Ä‘Æ°á»£c quan sÃ¡t trong cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y [37,49,57,78].
Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y Ã¡p lá»±c Ä‘Ã¡ng ká»ƒ lÃªn dung lÆ°á»£ng bá»™ nhá»›
GPU, vá»‘n tÆ°Æ¡ng Ä‘á»‘i khan hiáº¿m vÃ  Ä‘áº¯t Ä‘á».

Há»‡ thá»‘ng suy luáº­n LLM vá»›i Offloading. CÃ¡c há»‡ thá»‘ng phá»¥c
vá»¥ LLM hiá»‡n Ä‘áº¡i nhÆ° DeepSpeed [5] vÃ  FlexGen [57] Ä‘Ã£ há»—
trá»£ offload trá»ng sá»‘ mÃ´ hÃ¬nh hoáº·c KV cache vÃ o bá»™ nhá»› CPU.
Khi nÃ³i Ä‘áº¿n cÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading, kÃ­ch
thÆ°á»›c KV cache trá»Ÿ nÃªn cÃ³ váº¥n Ä‘á» hÆ¡n do bÄƒng thÃ´ng PCIe
tháº¥p giá»¯a CPU vÃ  GPU, Ä‘iá»u nÃ y trá»Ÿ thÃ nh má»™t bottleneck
má»›i vÃ  nghiÃªm trá»ng.

HÃ¬nh 3 mÃ´ táº£ sÆ¡ Ä‘á»“ thá»i gian cáº¥p cao giá»¯a cÃ¡c kiá»ƒu thá»±c thi
khÃ¡c nhau cá»§a cÃ¡c khá»‘i Transformer. HÃ¬nh 3(a) biá»ƒu thá»‹ trÆ°á»ng
há»£p khi KV cache hoÃ n toÃ n náº±m trong bá»™ nhá»› GPU (Full
GPU). Trong trÆ°á»ng há»£p nÃ y, Ä‘á»™ trá»… load cá»§a KV cache (Load
Cache) bao gá»“m má»™t phÃ©p Ä‘á»c Ä‘Æ¡n giáº£n tá»« bá»™ nhá»› GPU, cÃ³
thá»ƒ bá» qua do

0.40.60.81.01.20200400600800100012001400160018002000Cosine SimilarityToken ID(a) H2O(b) OptimalLayer 0Layer 12Layer 24Layer 30
0.40.60.81.01.20200400600800100012001400160018002000Cosine SimilarityToken IDHÃ¬nh 4: Äá»™ tÆ°Æ¡ng tá»± cosine giá»¯a attention weights cá»§a mÃ´ hÃ¬nh cÆ¡ sá»Ÿ
vá»›i full cache vÃ  (a) H2O hoáº·c (b) Optimal. H2O vÃ  Optimal sá»­ dá»¥ng 200
token cho tÃ­nh toÃ¡n attention. ChÃºng tÃ´i sá»­ dá»¥ng OPT-6.7B vÃ  má»™t cÃ¢u
ngáº«u nhiÃªn vá»›i 2000 token tá»« táº­p dá»¯ liá»‡u PG-19 [52].

bÄƒng thÃ´ng cao cá»§a bá»™ nhá»› GPU. Tuy nhiÃªn, kÃ­ch thÆ°á»›c batch
tá»‘i Ä‘a hoáº·c Ä‘á»™ dÃ i chuá»—i bá»‹ giá»›i háº¡n bá»Ÿi dung lÆ°á»£ng bá»™ nhá»›
GPU, vá»‘n tÆ°Æ¡ng Ä‘á»‘i nhá» hÆ¡n so vá»›i bá»™ nhá»› CPU.

Äá»ƒ cho phÃ©p kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n hoáº·c Ä‘á»™ dÃ i chuá»—i
dÃ i hÆ¡n, chÃºng ta cÃ³ thá»ƒ offload KV cache vÃ o bá»™ nhá»› CPU
(KV cache on CPU), nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3(b).
Máº·c dÃ¹ cÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading giáº£m bá»›t
giá»›i háº¡n vá» kÃ­ch thÆ°á»›c batch vÃ  Ä‘á»™ dÃ i chuá»—i, viá»‡c chuyá»ƒn
hÃ ng trÄƒm gigabyte KV cache vÃ o GPU cho tÃ­nh toÃ¡n attention
lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ thá»i gian thá»±c thi tá»•ng thá»ƒ cá»§a cÃ¡c khá»‘i
Transformer do bÄƒng thÃ´ng PCIe háº¡n cháº¿.

Ngay cáº£ khi chÃºng ta Ã¡p dá»¥ng ká»¹ thuáº­t prefetching thÃ´ng
thÆ°á»ng (Prefetch KV cache), nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh
3(c), chá»‰ má»™t pháº§n cá»§a Ä‘á»™ trá»… load cÃ³ thá»ƒ Ä‘Æ°á»£c áº©n bá»Ÿi tÃ­nh
toÃ¡n cá»§a khá»‘i Transformer trÆ°á»›c Ä‘Ã³. LÆ°u Ã½ ráº±ng máº·c dÃ¹ viá»‡c
nÃ©n KV cache thÃ´ng qua quantization cÃ³ thá»ƒ giáº£m thiá»ƒu
overhead chuyá»ƒn dá»¯ liá»‡u trong cÃ¡c há»‡ thá»‘ng dá»±a trÃªn offloading
[57], nÃ³ khÃ´ng phá»¥c vá»¥ nhÆ° má»™t giáº£i phÃ¡p cÆ¡ báº£n vÃ¬ quantization
khÃ´ng giáº£i quyáº¿t nguyÃªn nhÃ¢n gá»‘c rá»… cá»§a váº¥n Ä‘á» KV cache,
Ä‘Ã³ lÃ  viá»‡c má»Ÿ rá»™ng tuyáº¿n tÃ­nh cá»§a cÃ¡c má»¥c KV theo Ä‘á»™ dÃ i
chuá»—i. Äiá»u nÃ y Ä‘Ã²i há»i quáº£n lÃ½ KV cache thÃ´ng minh Ä‘á»ƒ
giáº£m thiá»ƒu overhead hiá»‡u suáº¥t trong khi báº£o toÃ n lá»£i Ã­ch cá»§a nÃ³.

3.2 ThÃ¡ch thá»©c trong quáº£n lÃ½ KV Cache
CÃ¡ch tiáº¿p cáº­n cÆ¡ báº£n Ä‘á»ƒ giáº£m thiá»ƒu overhead chuyá»ƒn cá»§a KV
cache tá»« CPU sang GPU lÃ  giáº£m khá»‘i lÆ°á»£ng KV cache cáº§n
load báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh cÃ¡c key vÃ  value quan trá»ng Ä‘á»ƒ tÃ­nh
toÃ¡n attention score, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3(d). ÄÆ°á»£c
cÃ´ng nháº­n rá»™ng rÃ£i ráº±ng cÃ¡c key vÃ  value cá»§a má»™t sá»‘ token
quan trá»ng hÆ¡n cÃ¡c token khÃ¡c trong tÃ­nh toÃ¡n attention [9,
10, 14, 33, 63]. NhÆ° Ä‘Ã£ giáº£i thÃ­ch trong Pháº§n 2.1, sau khi tÃ­nh
toÃ¡n attention score, phÃ©p toÃ¡n softmax Ä‘Æ°á»£c Ã¡p dá»¥ng, nháº¥n
máº¡nh má»™t sá»‘ giÃ¡ trá»‹ lá»›n cá»§a token. Do Ä‘Ã³, viá»‡c bá» qua tÃ­nh
toÃ¡n attention cho má»™t sá»‘ token Ã­t quan trá»ng khÃ´ng lÃ m giáº£m
Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh, miá»…n lÃ  viá»‡c lá»±a chá»n token
lÃ  phÃ¹ há»£p.

Trong bá»‘i cáº£nh nÃ y, má»™t sá»‘ cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘á» xuáº¥t giáº£m
4

--- TRANG 5 ---
kÃ­ch thÆ°á»›c KV cache thÃ´ng qua viá»‡c loáº¡i bá» key/value táº¡i
runtime trong má»™t ngÃ¢n sÃ¡ch KV cache bá»‹ rÃ ng buá»™c [37, 78].
Tuy nhiÃªn, táº¥t cáº£ cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘á»u giáº£ Ä‘á»‹nh tÃ­nh
bá»n vá»¯ng cá»§a cÃ¡c máº«u attention qua cÃ¡c iteration; tá»©c lÃ , náº¿u
má»™t token Ä‘Æ°á»£c coi lÃ  khÃ´ng quan trá»ng trong iteration hiá»‡n
táº¡i (tá»©c lÃ  cÃ³ attention weight tháº¥p), nÃ³ cÃ³ kháº£ nÄƒng váº«n khÃ´ng
quan trá»ng trong viá»‡c táº¡o ra cÃ¡c token tÆ°Æ¡ng lai. DÆ°á»›i giáº£
Ä‘á»‹nh nÃ y, há» loáº¡i bá» cÃ¡c token cÃ³ attention weight tháº¥p khá»i
KV cache táº¡i má»—i iteration khi kÃ­ch thÆ°á»›c KV cache vÆ°á»£t quÃ¡
ngÃ¢n sÃ¡ch cá»§a nÃ³. CÃ¡c key vÃ  value cá»§a cÃ¡c token bá»‹ loáº¡i bá»
Ä‘Æ°á»£c loáº¡i trá»« vÄ©nh viá»…n khá»i cÃ¡c iteration tiáº¿p theo trong khi
bá»‹ xÃ³a khá»i bá»™ nhá»›. Máº·c dÃ¹ cÃ¡c cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y vá» quáº£n
lÃ½ KV cache cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c há»‡ thá»‘ng suy luáº­n
dá»±a trÃªn offloading, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng chÃºng khÃ´ng
hiá»‡u quáº£ giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c trong quáº£n lÃ½ KV cache
dÆ°á»›i Ä‘Ã¢y vÃ  do Ä‘Ã³ cÃ³ hiá»‡u suáº¥t kÃ©m vá»›i cÃ¡c há»‡ thá»‘ng suy luáº­n
dá»±a trÃªn offloading.

C1: Báº£n cháº¥t Ä‘á»™ng cá»§a cÃ¡c máº«u attention qua cÃ¡c iteration.
HÃ¬nh 4 cho tháº¥y Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a attention weights
cá»§a mÃ´ hÃ¬nh baseline, sá»­ dá»¥ng KV cache cá»§a táº¥t cáº£ token
trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ tÃ­nh toÃ¡n attention weights (tá»©c lÃ  tá»‘i Ä‘a 2000
token trong thÃ­ nghiá»‡m), vÃ  hai phÆ°Æ¡ng phÃ¡p quáº£n lÃ½ KV
cache khÃ¡c nhau (H2O vÃ  Optimal) vá»›i ngÃ¢n sÃ¡ch KV cache
lÃ  200 token.3 H2O [78] lÃ  má»™t ká»¹ thuáº­t tiÃªn tiáº¿n chá»‰ giá»¯ láº¡i
má»™t tá»· lá»‡ nhá» cÃ¡c token quan trá»ng trong KV cache Ä‘á»ƒ giáº£m
kÃ­ch thÆ°á»›c cá»§a nÃ³. NÃ³ Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a má»—i token
trong má»—i iteration vÃ  loáº¡i bá» nhá»¯ng token khÃ´ng quan trá»ng
trÆ°á»›c iteration tiáº¿p theo Ä‘á»ƒ giá»¯ kÃ­ch thÆ°á»›c KV cache trong
táº§m kiá»ƒm soÃ¡t (tá»©c lÃ  sá»­ dá»¥ng cá»­a sá»• Ä‘Ã¡nh giÃ¡ háº¹p). NgÆ°á»£c
láº¡i, Optimal biá»ƒu thá»‹ ká»‹ch báº£n mÃ  chÃºng ta chá»n cÃ¹ng sá»‘
lÆ°á»£ng token nhÆ° H2O tá»« KV cache táº¡i má»—i iteration nhÆ°ng
giá»¯ láº¡i táº¥t cáº£ key vÃ  value trÆ°á»›c Ä‘Ã³ (tá»©c lÃ  sá»­ dá»¥ng cá»­a sá»•
Ä‘Ã¡nh giÃ¡ rá»™ng hÆ¡n). NÃ³i cÃ¡ch khÃ¡c, Optimal chá»n 200 token
tá»« toÃ n bá»™ chuá»—i token trÆ°á»›c Ä‘Ã³ táº¡i má»—i iteration.

HÃ¬nh cho tháº¥y ráº±ng máº·c dÃ¹ cÃ¡c cÃ¡ch tiáº¿p cáº­n giá»‘ng H2O giáº£
Ä‘á»‹nh ráº±ng máº«u attention khÃ´ng thay Ä‘á»•i qua cÃ¡c iteration,
Ä‘iá»u nÃ y khÃ´ng pháº£i lÃ  trÆ°á»ng há»£p trong thá»±c táº¿. CÃ¡c token
Ä‘Æ°á»£c coi lÃ  khÃ´ng quan trá»ng trong iteration hiá»‡n táº¡i cÃ³ thá»ƒ
trá»Ÿ nÃªn quan trá»ng trong cÃ¡c iteration tiáº¿p theo. Do Ä‘Ã³, H2O
thá»ƒ hiá»‡n Ä‘á»™ tÆ°Æ¡ng tá»± cao cho Ä‘áº¿n khoáº£ng 200 iteration (tá»©c
lÃ  trong ngÃ¢n sÃ¡ch KV cache), nhÆ°ng khi Ä‘á»™ dÃ i chuá»—i vÆ°á»£t
quÃ¡ ngÃ¢n sÃ¡ch KV cache, nÃ³ báº¯t Ä‘áº§u gáº·p khÃ³ khÄƒn vá»›i báº£n
cháº¥t Ä‘á»™ng cá»§a máº«u attention, dáº«n Ä‘áº¿n Ä‘á»™ tÆ°Æ¡ng tá»± cosine
tháº¥p hÆ¡n so vá»›i trÆ°á»ng há»£p Optimal. LÆ°u Ã½ ráº±ng máº·c dÃ¹
chÃºng tÃ´i chá»‰ hiá»ƒn thá»‹ ká»‹ch báº£n ngÃ¢n sÃ¡ch KV cache lÃ  200
trong tá»•ng sá»‘ Ä‘á»™ dÃ i chuá»—i 2000 token Ä‘á»ƒ ngáº¯n gá»n, váº¥n Ä‘á»
nÃ y sáº½ trá»Ÿ nÃªn rÃµ rÃ ng hÆ¡n khi Ä‘á»™ dÃ i chuá»—i vÆ°á»£t quÃ¡ nÃ³.

C2: Äiá»u chá»‰nh sá»‘ lÆ°á»£ng má»¥c KV qua cÃ¡c layer. HÃ¬nh 4 cÅ©ng
minh há»a ráº±ng tÃ¡c Ä‘á»™ng cá»§a viá»‡c loáº¡i bá» KV cache thay Ä‘á»•i
qua cÃ¡c layer trong LLM. Äá»‘i vá»›i Layer 0, cáº£ H2O vÃ  Optimal
Ä‘á»u cho tháº¥y sá»± giáº£m Ä‘Ã¡ng ká»ƒ trong Ä‘á»™ tÆ°Æ¡ng tá»± cosine khi
token ID tÄƒng. Äiá»u nÃ y ngá»¥ Ã½ ráº±ng Layer 0 cÃ³ máº«u attending
rá»™ng hÆ¡n cÃ¡c layer khÃ¡c; tá»©c lÃ  attention weights tÆ°Æ¡ng Ä‘á»‘i
tÆ°Æ¡ng tá»± giá»¯a cÃ¡c key token. Do Ä‘Ã³, 200 token Ä‘Æ°á»£c chá»n
vá»›i attention weight lá»›n khÃ´ng Ä‘áº¡i diá»‡n Ä‘áº§y Ä‘á»§ cho máº«u
attention cá»§a mÃ´ hÃ¬nh baseline cho layer nÃ y, vÃ¬ chÃºng cÃ³
kháº£ nÄƒng chá»‰ lá»›n hÆ¡n má»™t chÃºt so vá»›i cÃ¡c token khÃ¡c, khÃ´ng
máº¡nh máº½. Trong nhá»¯ng trÆ°á»ng há»£p nhÆ° váº­y, cáº§n thiáº¿t pháº£i
tÃ­nh toÃ¡n attention weight vá»›i sá»‘ lÆ°á»£ng token lá»›n hÆ¡n.

3Äá»™ tÆ°Æ¡ng tá»± cosine Ä‘o lÆ°á»ng má»©c Ä‘á»™ tÆ°Æ¡ng tá»± cá»§a má»—i hÃ ng cá»§a attention
weight so vá»›i trÆ°á»ng há»£p full KV cache. Náº¿u chÃºng tÆ°Æ¡ng tá»±, cÃ¡c token
Ä‘Æ°á»£c táº¡o ra cÅ©ng sáº½ tÆ°Æ¡ng tá»±. Do Ä‘Ã³, Ä‘á»™ tÆ°Æ¡ng tá»± cosine tháº¥p cho tháº¥y Ä‘á»™
chÃ­nh xÃ¡c tháº¥p xa so vá»›i mÃ´ hÃ¬nh baseline vá»›i full KV cache.

03006009001200
16512100815042000# Query Tokens# Key Tokens05000100001500020000
16512100815042000# Query Tokens# Key Tokens(a) Layer 0(b) Layer 18HÃ¬nh 5: Histogram cho tháº¥y sá»‘ lÆ°á»£ng key token cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c
0.9 trÃªn 1.0 tá»•ng attention weight cho (a) Layer 0 vÃ  (b) Layer 18 cá»§a
mÃ´ hÃ¬nh OPT-6.7B. Äá»™ rá»™ng bin Ä‘Æ°á»£c Ä‘áº·t thÃ nh 16. ChÃºng tÃ´i quan sÃ¡t
tháº¥y ráº±ng phÃ¢n phá»‘i thay Ä‘á»•i Ä‘á»™ng qua cÃ¡c layer.

Äá»ƒ Æ°á»›c tÃ­nh cÃ³ bao nhiá»u key/value tá»« KV cache cáº§n Ä‘Æ°á»£c
giá»¯ láº¡i, chÃºng tÃ´i sáº¯p xáº¿p attention weight cho má»—i query
token theo thá»© tá»± giáº£m dáº§n vÃ  tÃ­nh tá»•ng cÃ¡c key token cho
Ä‘áº¿n khi trá»ng sá»‘ tÃ­ch lÅ©y Ä‘áº¡t 0.9. HÃ¬nh 5 trÃ¬nh bÃ y histogram
vá» sá»‘ lÆ°á»£ng query token (trá»¥c y) yÃªu cáº§u sá»‘ lÆ°á»£ng key token
(trá»¥c x) cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t trá»ng sá»‘ 0.9 (trong tá»•ng attention
weight lÃ  1.0) trong hai layer khÃ¡c nhau: Layer 0 vÃ  Layer
18. Layer 0 cho tháº¥y phÃ¢n phá»‘i rá»™ng, cho tháº¥y sá»± khÃ¡c biá»‡t
Ä‘Ã¡ng ká»ƒ vá» sá»‘ lÆ°á»£ng key token cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t trá»ng sá»‘ 0.9
cho má»—i query token. NgÆ°á»£c láº¡i, Layer 18 thá»ƒ hiá»‡n phÃ¢n phá»‘i
nghiÃªng ráº¥t nhiá»u, gá»£i Ã½ ráº±ng pháº§n lá»›n query token trong
layer nÃ y chá»‰ cáº§n má»™t sá»‘ Ã­t key token Ä‘á»ƒ Ä‘áº¡t trá»ng sá»‘ 0.9.
Äiá»u nÃ y ngá»¥ Ã½ ráº±ng chÃºng ta cáº§n Ä‘á»™ng Ä‘iá»u chá»‰nh sá»‘ lÆ°á»£ng
key token tham gia vÃ o tÃ­nh toÃ¡n attention qua cÃ¡c layer khÃ¡c
nhau Ä‘á»ƒ sá»­ dá»¥ng hiá»‡u quáº£ ngÃ¢n sÃ¡ch KV cache.

C3: Äiá»u chá»‰nh sá»‘ lÆ°á»£ng má»¥c KV qua cÃ¡c query. H2O Ä‘áº·t
sá»‘ lÆ°á»£ng key/value token Ä‘á»ƒ giá»¯ láº¡i nhÆ° má»™t tá»· lá»‡ cá»‘ Ä‘á»‹nh
cá»§a Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o. NgÃ¢n sÃ¡ch KV cache váº«n khÃ´ng
Ä‘á»•i báº¥t ká»ƒ cÃ³ bao nhiÃªu token Ä‘Ã£ Ä‘Æ°á»£c táº¡o ra. Báº±ng cÃ¡ch phÃ¢n
tÃ­ch dá»¯ liá»‡u tá»« HÃ¬nh 5 trÃªn Layer 18, chÃºng tÃ´i quan sÃ¡t tháº¥y
ráº±ng ngÃ¢n sÃ¡ch KV cache cá»‘ Ä‘á»‹nh nÃ y cÃ³ má»™t sá»‘ háº¡n cháº¿.
VÃ­ dá»¥, vá»›i Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o lÃ  200 vÃ  ngÃ¢n sÃ¡ch KV
cache 20%, H2O duy trÃ¬ 40 key/value token trong suá»‘t quÃ¡
trÃ¬nh táº¡o token. Tuy nhiÃªn, háº§u háº¿t cÃ¡c query token tiáº¿p theo
yÃªu cáº§u nhiá»u hÆ¡n 40 token Ä‘á»ƒ Ä‘áº¡i diá»‡n hiá»‡u quáº£ cho attention
weight cá»§a mÃ´ hÃ¬nh baseline; vÃ­ dá»¥, token thá»© 500, 1000,
1500, vÃ  2000 cáº§n 80, 146, 160, vÃ  164 key token, tÆ°Æ¡ng á»©ng,
Ä‘á»ƒ Ä‘áº¡t attention weight tÃ­ch lÅ©y lÃ  0.9. Äiá»u nÃ y ngá»¥ Ã½ lÆ°á»£ng
key/value token khÃ´ng Ä‘á»§ Ä‘á»ƒ Ä‘áº¡i diá»‡n Ä‘Ãºng cho attention
weight cá»§a mÃ´ hÃ¬nh baseline. HÆ¡n ná»¯a, sá»‘ lÆ°á»£ng key token
cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t 0.9 thay Ä‘á»•i ngay cáº£ Ä‘á»‘i vá»›i cÃ¡c query token
ká» nhau; vÃ­ dá»¥, token thá»© 998, 999, 1000, 1001, vÃ  1002
cáº§n 172, 164, 146, 154, vÃ  140 key token, tÆ°Æ¡ng á»©ng. Viá»‡c
cá»‘ Ä‘á»‹nh ngÃ¢n sÃ¡ch KV cache mÃ  khÃ´ng tÃ­nh Ä‘áº¿n sá»± khÃ¡c biá»‡t
giá»¯a cÃ¡c query token khÃ´ng trÃ¡nh khá»i dáº«n Ä‘áº¿n quáº£n lÃ½ KV
cache khÃ´ng hiá»‡u quáº£. Do Ä‘Ã³, chÃºng ta cáº§n Ä‘á»™ng Ä‘iá»u chá»‰nh
lÆ°á»£ng key/value token Ä‘Æ°á»£c load vÃ  tÃ­nh toÃ¡n cho má»—i query
token Ä‘á»ƒ quáº£n lÃ½ KV cache má»™t cÃ¡ch hiá»‡u quáº£.

TÃ³m táº¯t. CÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nháº±m giáº£m kÃ­ch thÆ°á»›c KV
cache thÃ´ng qua viá»‡c loáº¡i bá» token vá»‘n cÃ³ má»™t sá»‘ thÃ¡ch thá»©c.
Vá»›i báº£n cháº¥t Ä‘á»™ng cá»§a máº«u attention qua cÃ¡c iteration, viá»‡c
loáº¡i trá»« vÄ©nh viá»…n cÃ¡c token bá»‹ loáº¡i bá» khá»i viá»‡c táº¡o token
tÆ°Æ¡ng lai cÃ³ thá»ƒ dáº«n Ä‘áº¿n sá»± giáº£m Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng thá»ƒ
bá» qua. Thay vÃ o Ä‘Ã³, chÃºng ta cáº§n Ä‘á»™ng chá»n cÃ¡c token quan
trá»ng tá»« KV cache trong khi trÃ¡nh viá»‡c loáº¡i bá» hoÃ n toÃ n
nhá»¯ng token Ã­t quan trá»ng. HÆ¡n ná»¯a, kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh cá»§a
ngÃ¢n sÃ¡ch KV cache trong cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y dáº«n Ä‘áº¿n
quáº£n lÃ½ KV cache khÃ´ng hiá»‡u quáº£. Sá»‘ lÆ°á»£ng key/value token
cáº§n thiáº¿t cho má»—i layer khÃ¡c nhau, vÃ  má»—i query token yÃªu
cáº§u sá»‘ lÆ°á»£ng key/value token khÃ¡c nhau Ä‘á»ƒ Ä‘áº¡i diá»‡n hiá»‡u
quáº£ cho máº«u attention cá»§a mÃ´ hÃ¬nh baseline. Viá»‡c khÃ´ng
tÃ­nh Ä‘áº¿n nhá»¯ng khÃ¡c biá»‡t nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n quáº£n lÃ½ KV
cache khÃ´ng hiá»‡u quáº£. Do Ä‘Ã³, chÃºng ta cáº§n Ä‘á»™ng Ä‘iá»u chá»‰nh
sá»‘ lÆ°á»£ng key/value token Ä‘á»ƒ chá»n tá»« KV cache trong khi
xem xÃ©t cÃ¡c khÃ¡c biá»‡t giá»¯a cÃ¡c layer vÃ  query token.

4 Thiáº¿t káº¿ InfiniGen
Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y InfiniGen, má»™t khung
quáº£n lÃ½ KV cache cho cÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading.
ChÃºng tÃ´i trÆ°á»›c tiÃªn cho tháº¥y tá»•ng quan cáº¥p cao vá» giáº£i phÃ¡p
quáº£n lÃ½ KV cache Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i (Pháº§n 4.1) vÃ 
tháº£o luáº­n vá» cÃ¡c cÆ¡ há»™i prefetching KV cache mÃ  chÃºng tÃ´i
quan sÃ¡t (Pháº§n 4.2). Sau Ä‘Ã³ chÃºng tÃ´i giáº£i thÃ­ch module
prefetching cá»§a chÃºng tÃ´i (Pháº§n 4.3), Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn
cÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading, vÃ  tháº£o luáº­n vá»
cÃ¡ch InfiniGen quáº£n lÃ½ KV cache trÃªn bá»™ nhá»› CPU liÃªn quan
Ä‘áº¿n Ã¡p lá»±c bá»™ nhá»› (Pháº§n 4.4).

4.1 Tá»•ng quan
HÃ¬nh 6 cho tháº¥y tá»•ng quan vá» khung quáº£n lÃ½ KV cache cá»§a
chÃºng tÃ´i, InfiniGen, cho phÃ©p offload KV cache vá»›i overhead
chuyá»ƒn dá»¯ liá»‡u tháº¥p. NguyÃªn táº¯c thiáº¿t káº¿ chÃ­nh Ä‘áº±ng sau
InfiniGen lÃ  khai thÃ¡c dung lÆ°á»£ng bá»™ nhá»› CPU dá»“i dÃ o Ä‘á»ƒ
tÄƒng kÃ­ch thÆ°á»›c cá»­a sá»• khi xÃ¡c Ä‘á»‹nh cÃ¡c token quan trá»ng
trong KV cache. Do Ä‘Ã³, pháº§n lá»›n cÃ¡c token cho KV cache
Ä‘Æ°á»£c giá»¯ trong bá»™ nhá»› CPU khi chÃºng ta táº¡o ra token má»›i,
khÃ´ng hoÃ n toÃ n loáº¡i bá» chÃºng khÃ´ng giá»‘ng nhÆ° cÃ¡c cÃ´ng
trÃ¬nh trÆ°á»›c Ä‘Ã¢y [37, 78]. Tuy nhiÃªn, chÃºng tÃ´i khÃ´ng mang
toÃ n bá»™ KV cache vÃ o GPU cho tÃ­nh toÃ¡n attention, mÃ  chá»‰
load vÃ  tÃ­nh toÃ¡n vá»›i cÃ¡c key vÃ  value cá»§a má»™t sá»‘ token quan
trá»ng, loáº¡i bá» cÃ¡c token khÃ´ng quan trá»ng khÃ¡c má»™t cÃ¡ch
Ä‘á»™ng. Äá»ƒ

InfiniGen SystemGPUMemory
CPU MemoryModifiedQK WeightsOtherModelWeightsPartialWeightand CachePartial WeightIndex Generation
PWIGenController
Data PlaneControl Plane
KVSelController
InferenceController
SkewingControllerKV SelectionLLM InferenceSkewingKV Cache Pool>(ğ‘šğ‘ğ‘¥âˆ’ğ›¼)Partial AttnAttentionFFNSVDMatMulLaunchSelectedIndicesSelectedTokens
SelectedKV Cache
PoolManagerHÃ¬nh 6: Tá»•ng quan thiáº¿t káº¿ InfiniGen.

lÃ m nhÆ° váº­y, chÃºng tÃ´i duy trÃ¬ KV cache pool trong bá»™ nhá»›
CPU vÃ  chá»n lá»c vÃ  dá»± Ä‘oÃ¡n load má»™t sá»‘ Ã­t token.

Cá»¥ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng Ä‘áº§u vÃ o attention cá»§a layer
Transformer trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ dá»± Ä‘oÃ¡n vÃ  prefetch cÃ¡c key vÃ 
value cá»§a cÃ¡c token quan trá»ng cho layer hiá»‡n táº¡i. Viá»‡c dá»±
Ä‘oÃ¡n Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch thá»±c hiá»‡n má»™t phÃ©p diá»…n táº­p
tá»‘i thiá»ƒu cá»§a tÃ­nh toÃ¡n attention cá»§a layer hiá»‡n táº¡i trong layer
trÆ°á»›c Ä‘Ã³. Äiá»u nÃ y cho phÃ©p giáº£m lÃ£ng phÃ­ bÄƒng thÃ´ng PCIe
báº±ng cÃ¡ch chá»‰ chuyá»ƒn cÃ¡c key vÃ  value quan trá»ng cho tÃ­nh
toÃ¡n attention trong khi báº£o toÃ n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh. NgoÃ i
ra, máº·c dÃ¹ KV cache Ä‘Æ°á»£c offload vÃ o bá»™ nhá»› CPU, vá»‘n ráº»
hÆ¡n vÃ  lá»›n hÆ¡n nhiá»u so vá»›i bá»™ nhá»› GPU, chÃºng tÃ´i quáº£n lÃ½
kÃ­ch thÆ°á»›c KV cache pool Ä‘á»ƒ khÃ´ng gÃ¢y quÃ¡ nhiá»u Ã¡p lá»±c
lÃªn bá»™ nhá»› CPU.

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6, cÃ³ hai thÃ nh pháº§n chÃ­nh
trong runtime InfiniGen. Thá»© nháº¥t bao gá»“m Partial Weight
Index Generation Controller, KV Selection Controller, vÃ 
Inference Controller. CÃ¡c controller nÃ y há»£p tÃ¡c Ä‘á»ƒ dá»± Ä‘oÃ¡n
vÃ  prefetch cÃ¡c má»¥c KV cache quan trá»ng trong khi phá»¥c vá»¥
suy luáº­n LLM. NgoÃ i ra, Ä‘á»ƒ há»— trá»£ prefetching, Skewing
Controller thá»±c hiá»‡n cÃ¡c sá»­a Ä‘á»•i offline trÃªn trá»ng sá»‘ mÃ´
hÃ¬nh. ChÃºng tÃ´i giáº£i thÃ­ch tá»«ng hoáº¡t Ä‘á»™ng trong Pháº§n 4.3.
ThÃ nh pháº§n thá»© hai lÃ  Pool Manager. NÃ³ quáº£n lÃ½ KV cache
pool trÃªn bá»™ nhá»› CPU dÆ°á»›i Ã¡p lá»±c bá»™ nhá»› CPU, mÃ  chÃºng
tÃ´i tháº£o luáº­n trong Pháº§n 4.4.

4.2 CÆ¡ há»™i Prefetching
Trong pháº§n sau, trÆ°á»›c tiÃªn chÃºng tÃ´i giáº£i thÃ­ch táº¡i sao viá»‡c
sá»­ dá»¥ng Ä‘áº§u vÃ o attention cá»§a layer trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ dá»± Ä‘oÃ¡n
cÃ³ Ã½ nghÄ©a. Sau Ä‘Ã³ chÃºng tÃ´i cho tháº¥y cÃ¡ch chÃºng tÃ´i sá»­a
Ä‘á»•i cÃ¡c ma tráº­n trá»ng sá»‘ query vÃ  key Ä‘á»ƒ lÃ m cho viá»‡c dá»±
Ä‘oÃ¡n cá»§a chÃºng tÃ´i hiá»‡u quáº£ hÆ¡n nhiá»u.

Sá»± tÆ°Æ¡ng tá»± Ä‘áº§u vÃ o Attention. Module prefetching cá»§a
chÃºng tÃ´i Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn quan sÃ¡t chÃ­nh ráº±ng cÃ¡c Ä‘áº§u
vÃ o attention cá»§a cÃ¡c layer attention liÃªn tiáº¿p ráº¥t tÆ°Æ¡ng tá»±
trong LLM. CÃ³ hai lÃ½ do chÃ­nh Ä‘áº±ng sau Ä‘iá»u nÃ y. Thá»© nháº¥t
lÃ  sá»± tá»“n táº¡i cá»§a outlier trong LLM, nhÆ° Ä‘Ã£ tháº£o luáº­n trong
Pháº§n 2.3, vÃ  thá»© hai lÃ  do layer normalization (LayerNorm).

6

--- TRANG 7 ---
(b) QueryChannel Dimension# Tokens8-80(a) Input Similarity: ğ¹ğ¹ğ‘_ğ‘œğ‘¢ğ‘¡!"#x-axis(Outlier Dimension)y-axis(Normal Dimension): ğ‘‡ğ‘ğ‘™ğ‘œğ‘ğ‘˜_ğ‘–ğ‘›!"#: ğ´ğ‘¡ğ‘¡ğ‘›_ğ‘œğ‘¢ğ‘¡!"#: ğ‘‡ğ‘ğ‘™ğ‘œğ‘ğ‘˜_ğ‘–ğ‘›!

HÃ¬nh 7: (a) HÃ¬nh áº£nh hÃ³a sá»± tÆ°Æ¡ng tá»± Ä‘áº§u vÃ o giá»¯a cÃ¡c khá»‘i
Transformer liÃªn tiáº¿p. (b) Ma tráº­n Query cá»§a Layer 18 cá»§a mÃ´ hÃ¬nh
OPT-13B. ChÃºng tÃ´i chá»‰ hiá»ƒn thá»‹ cÃ¡c kÃªnh tá»« 3000 Ä‘áº¿n 4000 Ä‘á»ƒ cÃ³
cÃ¡i nhÃ¬n rÃµ hÆ¡n vá» cÃ¡c máº«u theo cá»™t.

Äá»ƒ báº¯t Ä‘áº§u, Ä‘áº§u vÃ o cho khá»‘i Transformer i (Tblock_in i)
cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° sau:
Attn_outiâˆ’1=Attn (LN(T block _iniâˆ’1))
FFN _outiâˆ’1=FFN (LN(T block _iniâˆ’1+Attn_outiâˆ’1))
T block _ini=T block _iniâˆ’1+Attn_outiâˆ’1+FFN _outiâˆ’1,(1)

trong Ä‘Ã³ Tblock_in iâˆ’1 lÃ  Ä‘áº§u vÃ o cho Layer iâˆ’1, Ä‘áº§u tiÃªn
Ä‘Æ°á»£c layer-normalized (LN) vÃ  Ä‘Æ°á»£c Ä‘Æ°a vÃ o layer attention
trong khá»‘i Transformer. Sau khi thá»±c hiá»‡n attention, chÃºng
ta cÃ³ Ä‘Æ°á»£c Ä‘áº§u ra (Attn_out iâˆ’1), Ä‘Æ°á»£c cá»™ng vÃ o Tblock_in iâˆ’1
vÃ¬ káº¿t ná»‘i residual. Sau Ä‘Ã³, tá»•ng cá»§a Tblock_in iâˆ’1 vÃ 
Attn_out iâˆ’1 láº¡i Ä‘Æ°á»£c layer-normalized vÃ  Ä‘Æ°á»£c Ä‘Æ°a vÃ o
layer FFN. Sau Ä‘Ã³, chÃºng ta cÃ³ Ä‘Æ°á»£c Ä‘áº§u ra FFN (FFN_out iâˆ’1),
Ä‘Æ°á»£c cá»™ng vÃ o tá»•ng cá»§a Tblock_in iâˆ’1 vÃ  Attn_out iâˆ’1 láº§n
ná»¯a do káº¿t ná»‘i residual. Cuá»‘i cÃ¹ng, tá»•ng cá»§a Tblock_in iâˆ’1,
FFN_out iâˆ’1, vÃ  Attn_out iâˆ’1 Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘áº§u vÃ o
cho khá»‘i Transformer tiáº¿p theo (Tblock_in i).

BÃ¢y giá», chÃºng tÃ´i cho tháº¥y táº¡i sao Ä‘áº§u vÃ o attention cá»§a
Layer i tÆ°Æ¡ng tá»± vá»›i Layer iâˆ’1 vá»›i vÃ­ dá»¥ trong HÃ¬nh 7(a).
Trong hÃ¬nh, cÃ³ bá»‘n vector, má»—i vector tÆ°Æ¡ng á»©ng vá»›i má»™t
thuáº­t ngá»¯ trong PhÆ°Æ¡ng trÃ¬nh 1. Trá»¥c x biá»ƒu thá»‹ má»™t kÃªnh
outlier trong sá»‘ cÃ¡c chiá»u mÃ´ hÃ¬nh, trong khi trá»¥c y biá»ƒu thá»‹
má»™t kÃªnh bÃ¬nh thÆ°á»ng (tá»©c lÃ  khÃ¡c vá»›i kÃªnh outlier). Trong
thá»±c táº¿, tá»“n táº¡i nhiá»u kÃªnh bÃ¬nh thÆ°á»ng hÆ¡n vÃ  chá»‰ má»™t vÃ i
kÃªnh outlier trong cÃ¡c tensor Ä‘áº§u vÃ o, nhÆ°ng chÃºng tÃ´i chá»‰
trÃ¬nh bÃ y má»™t kÃªnh cho cáº£ kÃªnh outlier vÃ  bÃ¬nh thÆ°á»ng Ä‘á»ƒ
rÃµ rÃ ng.

Tblock_in iâˆ’1 bá»‹ nghiÃªng cao dá»c theo kÃªnh outlier (trá»¥c x)
do má»™t vÃ i kÃªnh outlier chá»©a cÃ¡c giÃ¡ trá»‹ lá»›n Ä‘Ã¡ng ká»ƒ so vá»›i
nhá»¯ng giÃ¡ trá»‹ trong cÃ¡c kÃªnh bÃ¬nh thÆ°á»ng. NgÆ°á»£c láº¡i,
Attn_out iâˆ’1 vÃ  FFN_out iâˆ’1 cÃ³ cÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»‘i nhá»
cho cáº£ kÃªnh outlier vÃ  bÃ¬nh thÆ°á»ng (tá»©c lÃ  cÃ¡c vector ngáº¯n).
Äiá»u nÃ y lÃ  do cÃ¡c Ä‘áº§u vÃ o attention vÃ  FFN Ä‘Æ°á»£c layer-
normalized, giáº£m Ä‘á»™ lá»›n cá»§a má»—i giÃ¡ trá»‹. Äá»™ lá»›n nhá» cá»§a
cÃ¡c Ä‘áº§u vÃ o attention vÃ  FFN tá»± nhiÃªn dáº«n Ä‘áº¿n cÃ¡c giÃ¡ trá»‹
Ä‘áº§u ra cá»§a chÃºng tÆ°Æ¡ng Ä‘á»‘i nhá» so vá»›i Tblock_in iâˆ’1. Do Ä‘Ã³,
Tblock_in i chá»‹u áº£nh hÆ°á»Ÿng lá»›n tá»« Tblock_in iâˆ’1, hÆ¡n lÃ 
Attn_out iâˆ’1 hoáº·c FFN_out iâˆ’1. CÃ¡c Ä‘áº§u vÃ o ráº¥t tÆ°Æ¡ng tá»±
giá»¯a cÃ¡c khá»‘i Transformer liÃªn tiáº¿p dáº«n Ä‘áº¿n cÃ¡c Ä‘áº§u vÃ o
tÆ°Æ¡ng tá»± qua cÃ¡c layer attention, vÃ¬ Ä‘áº§u vÃ o attention lÃ 
má»™t layer-normalized cá»§a

Báº£ng 1: Äá»™ tÆ°Æ¡ng tá»± cosine trung bÃ¬nh giá»¯a Ä‘áº§u vÃ o khá»‘i
Transformer cá»§a Layer i (Tblock_in i) vÃ  ba tensor khÃ¡c
(Tblock_in iâˆ’1,Attn_out iâˆ’1,FFN_out iâˆ’1) qua cÃ¡c layer. ChÃºng
tÃ´i sá»­ dá»¥ng má»™t cÃ¢u ngáº«u nhiÃªn vá»›i 2000 token tá»« táº­p dá»¯ liá»‡u PG-19 [52].

Tensor OPT-6.7B OPT-13B OPT-30B Llama-2-7B Llama-2-13B
Tblock_in iâˆ’1 0.95 0.96 0.97 0.89 0.91
Attn_out iâˆ’1 0.29 0.28 0.36 0.31 0.27
FFN_out iâˆ’1 0.34 0.28 0.35 0.37 0.34

Ä‘áº§u vÃ o khá»‘i Transformer.

Báº£ng 1 cho tháº¥y Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a Tblock_in i vÃ  ba
tensor khÃ¡c (Tblock_in iâˆ’1, Attn_out iâˆ’1, FFN_out iâˆ’1).
NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong báº£ng, Tblock_in i phá»¥ thuá»™c ráº¥t
nhiá»u vÃ o Tblock_in iâˆ’1 hÆ¡n lÃ  cÃ¡c tensor khÃ¡c. InfiniGen
táº­n dá»¥ng quan sÃ¡t chÃ­nh nÃ y Ä‘á»ƒ dá»± Ä‘oÃ¡n máº«u attention cá»§a
Layer i báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘áº§u vÃ o attention cá»§a Layer iâˆ’1.
LÆ°u Ã½ ráº±ng Tblock_in dáº§n dáº§n thay Ä‘á»•i qua cÃ¡c layer; cÃ¡c
Ä‘áº§u vÃ o cho cÃ¡c layer xa nhau lÃ  khÃ¡c biá»‡t.

Trá»ng sá»‘ má»™t pháº§n nghiÃªng. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng
attention score phá»¥ thuá»™c ráº¥t nhiá»u vÃ o má»™t vÃ i cá»™t trong
cÃ¡c ma tráº­n query vÃ  key. HÃ¬nh 7(b) cho tháº¥y cÃ¡c giÃ¡ trá»‹
trong ma tráº­n query cá»§a Layer 18 cá»§a mÃ´ hÃ¬nh OPT-13B,
trong Ä‘Ã³ cÃ¡c máº«u theo cá»™t cho tháº¥y ráº±ng tá»“n táº¡i má»™t sá»‘ cá»™t
nháº¥t Ä‘á»‹nh vá»›i Ä‘á»™ lá»›n lá»›n trong ma tráº­n; chÃºng tÃ´i quan sÃ¡t
tháº¥y cÃ¡c máº«u tÆ°Æ¡ng tá»± trong cÃ¡c ma tráº­n key vÃ  query qua
cÃ¡c layer vÃ  mÃ´ hÃ¬nh khÃ¡c nhau. CÃ¡c cá»™t cÃ³ Ä‘á»™ lá»›n lá»›n cÃ³
tÃ¡c Ä‘á»™ng lá»›n Ä‘áº¿n máº«u attention vÃ¬ tÃ­ch vÃ´ hÆ°á»›ng giá»¯a query
vÃ  key bá»‹ áº£nh hÆ°á»Ÿng ráº¥t nhiá»u bá»Ÿi nhá»¯ng cá»™t nÃ y. Máº«u theo
cá»™t trong Ä‘áº§u vÃ o attention cho tháº¥y ráº±ng cÃ³ Ã­t sá»± khÃ¡c biá»‡t
giá»¯a má»—i hÃ ng trong cÃ¡c kÃªnh outlier. Do Ä‘Ã³, tÃ­ch vÃ´ hÆ°á»›ng
giá»¯a báº¥t ká»³ hÃ ng nÃ o cá»§a Ä‘áº§u vÃ o attention vÃ  má»™t cá»™t cá»§a
ma tráº­n trá»ng sá»‘ cÃ³ thá»ƒ cÃ³ Ä‘á»™ lá»›n tÆ°Æ¡ng tá»± lá»›n, Ä‘iá»u nÃ y
táº¡o ra cÃ¡c kÃªnh outlier trong cÃ¡c ma tráº­n query vÃ  key.

Äi xa hÆ¡n, náº¿u chÃºng ta lÃ m cho má»™t vÃ i cá»™t trong cÃ¡c ma
tráº­n query vÃ  key cÃ³ Ä‘á»™ lá»›n lá»›n hÆ¡n nhiá»u so vá»›i cÃ¡c cá»™t
khÃ¡c, má»™t sá»‘ lÆ°á»£ng cá»™t nhá» hÆ¡n nhiá»u sáº½ áº£nh hÆ°á»Ÿng Ä‘Ã¡ng
ká»ƒ Ä‘áº¿n máº«u attention. ChÃºng ta cÃ³ thá»ƒ lÃ m Ä‘iá»u nÃ y báº±ng
cÃ¡ch nhÃ¢n cÃ¡c ma tráº­n trá»ng sá»‘ query vÃ  key vá»›i cÃ¹ng má»™t
ma tráº­n trá»±c giao A. VÃ¬ chuyá»ƒn vá»‹ cá»§a ma tráº­n trá»±c giao lÃ 
nghá»‹ch Ä‘áº£o cá»§a chÃ­nh nÃ³, phÃ©p toÃ¡n Ä‘Æ°á»£c Ä‘á» xuáº¥t khÃ´ng
thay Ä‘á»•i káº¿t quáº£ cuá»‘i cÃ¹ng, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong PhÆ°Æ¡ng
trÃ¬nh 2 (tá»©c lÃ  Ä‘iá»u nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng toÃ¡n há»c vá»›i QKT,
khÃ´ng pháº£i lÃ  má»™t xáº¥p xá»‰):

ËœQ=XaÃ—WQÃ—A, ËœK=XaÃ—WKÃ—A
ËœQÃ—ËœKT=XaÃ—WQÃ—AÃ—(XaÃ—WKÃ—A)T
=XaÃ—WQÃ—AÃ—ATÃ—WT
KÃ—XT
a
=XaÃ—WQÃ—WT
KÃ—XT
a
=XaÃ—WQÃ—(XaÃ—WK)T
=QÃ—KT,(2)

trong Ä‘Ã³ ËœQ vÃ  ËœK lÃ  cÃ¡c ma tráº­n query vÃ  key nghiÃªng, trong
khi WQ vÃ  WK lÃ  cÃ¡c ma tráº­n trá»ng sá»‘ query vÃ  key. Xa biá»ƒu
thá»‹ Ä‘áº§u vÃ o attention. ChÃºng tÃ´i Ä‘áº·t ma tráº­n trá»±c giao A cÃ³
hÆ°á»›ng cÄƒn chá»‰nh vá»›i hÆ°á»›ng mÃ  ma tráº­n query kÃ©o dÃ i nhiá»u
nháº¥t. Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘áº§u tiÃªn phÃ¢n tÃ¡ch ma tráº­n query
báº±ng SVD vÃ  cÃ³ Ä‘Æ°á»£c U, Î£, vÃ  V. Sau Ä‘Ã³ chÃºng tÃ´i Ä‘áº·t A
thÃ nh ma tráº­n trá»±c giao V Ä‘á»ƒ cÄƒn chá»‰nh cÃ¡c vector cá»™t vá»›i
cÃ¡c vector Ä‘Æ¡n vá»‹ chuáº©n nhÆ° VTA=VTV=I, trong Ä‘Ã³ I lÃ 
ma tráº­n Ä‘Æ¡n vá»‹. ChÃºng tÃ´i cÃ´ng thá»©c hÃ³a ma tráº­n query
nghiÃªng nhÆ° sau:

ËœQ=QÃ—A=UÎ£VTÃ—A=UÎ£VTÃ—V (3)

Theo cÃ¡ch nÃ y, chÃºng ta cÃ³ thá»ƒ lÃ m cho má»™t vÃ i cá»™t cÃ³ Ä‘á»™
lá»›n lá»›n trong ËœQ mÃ  khÃ´ng thay Ä‘á»•i káº¿t quáº£ tÃ­nh toÃ¡n, nhÆ°
Ä‘Ã£ tháº£o luáº­n trong Pháº§n 2.4.

4.3 Prefetching KV Cache hiá»‡u quáº£
SÆ¡ Ä‘á»“ Prefetching. HÃ¬nh 8 cho tháº¥y quy trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a
module prefetching trong InfiniGen. Trong giai Ä‘oáº¡n offline,
InfiniGen sá»­a Ä‘á»•i cÃ¡c ma tráº­n trá»ng sá»‘ Ä‘á»ƒ táº¡o ra cÃ¡c ma tráº­n
query vÃ  key nghiÃªng. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, InfiniGen Ä‘áº§u
tiÃªn cháº¡y forward pass cá»§a mÃ´ hÃ¬nh má»™t láº§n vá»›i má»™t Ä‘áº§u vÃ o
máº«u. Trong quÃ¡ trÃ¬nh nÃ y, InfiniGen thu tháº­p ma tráº­n query
tá»« má»—i layer vÃ  thá»±c hiá»‡n phÃ¢n tÃ­ch giÃ¡ trá»‹ Ä‘Æ¡n (SVD) cá»§a
má»—i ma tráº­n query. Ma tráº­n nghiÃªng (Ai) cá»§a má»—i layer Ä‘Æ°á»£c
thu Ä‘Æ°á»£c báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c ma tráº­n Ä‘Æ°á»£c phÃ¢n tÃ¡ch cá»§a
ma tráº­n query, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong PhÆ°Æ¡ng trÃ¬nh 3. Ma
tráº­n nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c nhÃ¢n vá»›i má»—i ma tráº­n trá»ng sá»‘ query
vÃ  key trong layer tÆ°Æ¡ng á»©ng. Quan trá»ng lÃ , sau khi nhÃ¢n,
cÃ¡c chiá»u cá»§a ma tráº­n trá»ng sá»‘ váº«n khÃ´ng thay Ä‘á»•i. LÆ°u Ã½
ráº±ng viá»‡c nghiÃªng lÃ  má»™t quÃ¡ trÃ¬nh offline má»™t láº§n vÃ  khÃ´ng
gÃ¢y ra overhead runtime nÃ o vÃ¬ chÃºng tÃ´i sá»­a Ä‘á»•i cÃ¡c ma
tráº­n trá»ng sá»‘ báº¥t biáº¿n táº¡i runtime. VÃ¬ chÃºng tÃ´i khai thÃ¡c
máº«u theo cá»™t, báº¯t nguá»“n tá»« tÃ­nh cháº¥t ná»™i táº¡i cá»§a mÃ´ hÃ¬nh
hÆ¡n lÃ  Ä‘áº§u vÃ o, báº¥t cá»© khi nÃ o chÃºng tÃ´i tÃ­nh toÃ¡n query
vÃ  key cho cÃ¡c Ä‘áº§u vÃ o khÃ¡c nhau sau khi nghiÃªng, cÃ¡c giÃ¡
trá»‹ thá»ƒ hiá»‡n má»©c Ä‘á»™ nghiÃªng cao, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£
cá»§a module prefetching cá»§a chÃºng tÃ´i. LÆ°u Ã½ ráº±ng viá»‡c nghiÃªng
khÃ´ng thay Ä‘á»•i chá»©c nÄƒng ban Ä‘áº§u. Ngay cáº£ vá»›i viá»‡c nghiÃªng,
layer attention táº¡o ra káº¿t quáº£ tÃ­nh toÃ¡n giá»‘ng há»‡t nhau.

Giai Ä‘oáº¡n Prefill. Trong giai Ä‘oáº¡n prefill, InfiniGen chá»n
má»™t sá»‘ cá»™t quan trá»ng tá»« ma tráº­n trá»ng sá»‘ query vÃ  key
cache Ä‘á»ƒ dá»± Ä‘oÃ¡n máº«u attention, vÃ  táº¡o ra cÃ¡c ma tráº­n trá»ng
sá»‘ query má»™t pháº§n vÃ  key cache Ä‘Æ°á»£c sá»­ dá»¥ng trong giai
Ä‘oáº¡n decoding. HÃ¬nh 9 cho tháº¥y cÃ¡ch InfiniGen táº¡o ra cÃ¡c
ma tráº­n má»™t pháº§n nÃ y. VÃ¬ chÃºng tÃ´i nhÃ¢n má»—i cá»™t trong ma
tráº­n query vá»›i hÃ ng tÆ°Æ¡ng á»©ng trong ma tráº­n key chuyá»ƒn
vá»‹, viá»‡c chá»n cÃ¹ng chá»‰ sá»‘ cá»™t trong ma tráº­n trá»ng sá»‘ query
vÃ  key cache lÃ  cáº§n thiáº¿t Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c xáº¥p xá»‰ Ä‘Ãºng cá»§a attention
score. Tuy nhiÃªn, cÃ¡c chá»‰ sá»‘ cá»§a cÃ¡c cá»™t outlier cá»§a cÃ¡c ma
tráº­n query nghiÃªng (ËœQ) vÃ  key nghiÃªng (ËœK) cÃ³ thá»ƒ khÃ´ng
cÄƒn chá»‰nh chÃ­nh xÃ¡c. Äá»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c ma tráº­n má»™t pháº§n náº¯m
báº¯t cÃ¡c outlier, trÆ°á»›c tiÃªn chÃºng tÃ´i láº¥y cÃ¡c giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i
theo pháº§n tá»­ cá»§a cÃ¡c ma tráº­n query vÃ  key nghiÃªng, sau Ä‘Ã³
cá»™ng hai ma tráº­n nÃ y láº¡i vá»›i nhau. Äiá»u nÃ y giÃºp chÃºng tÃ´i
tÃ­nh tá»•ng cá»§a má»—i cá»™t vÃ  thá»±c hiá»‡n phÃ©p toÃ¡n top-k chá»‰ má»™t
láº§n trong khi phÃ¹ há»£p vá»›i cÃ¡c cá»™t outlier cá»§a cáº£ ma tráº­n
query vÃ  key. Sau Ä‘Ã³ chÃºng tÃ´i tÃ­nh tá»•ng cÃ¡c pháº§n tá»­ trong
má»—i cá»™t vÃ  chá»n cÃ¡c cá»™t top-k trong ma tráº­n; chÃºng tÃ´i chá»n
30% cÃ¡c cá»™t trong cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i. Viá»‡c sá»­ dá»¥ng
tá»•ng cÃ¡c giÃ¡ trá»‹ cá»™t náº¯m báº¯t xu hÆ°á»›ng toÃ n cá»¥c cá»§a má»—i cá»™t
trong khi giáº£m thiá»ƒu tÃ¡c Ä‘á»™ng cá»§a sá»± khÃ¡c biá»‡t trong má»—i
hÃ ng. CÃ¡c cá»™t Ä‘Æ°á»£c chá»n xáº¥p xá»‰ tá»‘t hÆ¡n máº«u attention vÃ¬
viá»‡c sá»­ dá»¥ng cÃ¡c ma tráº­n query vÃ  key nghiÃªng.

Giai Ä‘oáº¡n Decoding. Trong giai Ä‘oáº¡n decoding, InfiniGen
dá»± Ä‘oÃ¡n máº«u attention cá»§a layer tiáº¿p theo vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c
key vÃ  value quan trá»ng Ä‘á»ƒ prefetch. HÃ¬nh 10 cho tháº¥y cÃ¡ch
InfiniGen tÃ­nh toÃ¡n attention score Ä‘Æ°á»£c dá»± Ä‘oÃ¡n. Táº¡i Layer
iâˆ’1, chÃºng tÃ´i sá»­ dá»¥ng ma tráº­n trá»ng sá»‘ query má»™t pháº§n vÃ 
key cache cá»§a Layer i, Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong giai Ä‘oáº¡n prefill,
cÃ¹ng vá»›i Ä‘áº§u vÃ o attention cá»§a Layer iâˆ’1. Sau khi nhÃ¢n
query má»™t pháº§n vÃ  key cache má»™t pháº§n, InfiniGen chá»n cÃ¡c
token vá»›i attention score cao.

ChÃºng tÃ´i Ä‘áº·t ngÆ°á»¡ng xem xÃ©t giÃ¡ trá»‹ tá»‘i Ä‘a cá»§a attention
score Ä‘Æ°á»£c dá»± Ä‘oÃ¡n. ChÃºng tÃ´i chá»‰ chá»n cÃ¡c token cÃ³ attention
score lá»›n hÆ¡n Ä‘iá»ƒm sá»‘ tá»‘i Ä‘a trá»« Ä‘i alpha. Cáº§n lÆ°u Ã½ ráº±ng
phÃ©p trá»« tá»« attention score dáº«n Ä‘áº¿n phÃ©p chia sau softmax.
VÃ­ dá»¥, giáº£ sá»­ ráº±ng attention score cá»§a token thá»© 3 lÃ  attention
score tá»‘i Ä‘a trá»« Ä‘i 5. Khi chÃºng ta Ã¡p dá»¥ng softmax cho cÃ¡c
attention score, attention weight cá»§a token thá»© 3 lÃ  attention
weight tá»‘i Ä‘a chia cho e5â‰ˆ148.4. Máº·c dÃ¹ chÃºng ta khÃ´ng
sá»­ dá»¥ng token nÃ y, nÃ³ khÃ´ng lÃ m tá»•n háº¡i Ä‘Ã¡ng chÃº Ã½ Ä‘áº¿n
Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh vÃ¬ nÃ³ chiáº¿m Ã­t hÆ¡n 1% táº§m quan
trá»ng (â‰ˆ1/148.4) sau softmax. Do Ä‘Ã³, InfiniGen chá»‰ prefetch
cÃ¡c key vÃ  value cá»§a cÃ¡c token cÃ³ attention score lá»›n hÆ¡n
attention score cao nháº¥t trá»« Ä‘i alpha. VÃ¬ nhiá»u attention head
Ä‘Æ°á»£c tÃ­nh toÃ¡n song song, chÃºng tÃ´i Ä‘áº£m báº£o ráº±ng má»—i head
trong cÃ¹ng má»™t layer fetch cÃ¹ng sá»‘ lÆ°á»£ng token báº±ng cÃ¡ch
láº¥y trung bÃ¬nh sá»‘ lÆ°á»£ng token giá»¯a Ä‘iá»ƒm sá»‘ tá»‘i Ä‘a vÃ  ngÆ°á»¡ng
qua cÃ¡c head.

Báº±ng cÃ¡ch giáº£m lÆ°á»£ng KV cache cáº§n load vÃ  tÃ­nh toÃ¡n,
InfiniGen hiá»‡u quáº£ giáº£m Ä‘á»™ trá»… loading (tá»©c lÃ  chuyá»ƒn dá»¯
liá»‡u tá»« CPU sang GPU) trong khi duy trÃ¬ cháº¥t lÆ°á»£ng Ä‘áº§u ra
tÆ°Æ¡ng tá»± nhÆ° mÃ´ hÃ¬nh gá»‘c vá»›i full KV cache. HÆ¡n ná»¯a, vÃ¬
InfiniGen khÃ´ng yÃªu cáº§u sá»‘ lÆ°á»£ng token cá»‘ Ä‘á»‹nh Ä‘á»ƒ load tá»«
bá»™ nhá»› CPU, nÃ³ chá»‰ sá»­ dá»¥ng bÄƒng thÃ´ng káº¿t ná»‘i PCI cáº§n
thiáº¿t. InfiniGen báº¯t Ä‘áº§u dá»± Ä‘oÃ¡n vÃ  prefetching tá»« Layer 1
vÃ¬ cÃ¡c outlier, vá»‘n cáº§n thiáº¿t Ä‘á»ƒ khai thÃ¡c sá»± tÆ°Æ¡ng tá»± Ä‘áº§u
vÃ o, xuáº¥t hiá»‡n trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n á»Ÿ Layer 0.

4.4 Quáº£n lÃ½ KV Cache Pool
ChÃºng tÃ´i quáº£n lÃ½ KV cache nhÆ° má»™t pool, offload vÃ o bá»™
nhá»› CPU vÃ  chá»‰ prefetch lÆ°á»£ng cáº§n thiáº¿t vÃ o GPU. Máº·c dÃ¹
bá»™ nhá»› CPU ráº» hÆ¡n vÃ  lá»›n hÆ¡n bá»™ nhá»› GPU, nÃ³ váº«n cÃ³ dung
lÆ°á»£ng háº¡n cháº¿. Do Ä‘Ã³, Ä‘á»‘i vá»›i má»™t sá»‘ ká»‹ch báº£n triá»ƒn khai
nháº¥t Ä‘á»‹nh, cÃ³ thá»ƒ quan trá»ng Ä‘á»ƒ giá»›i háº¡n kÃ­ch thÆ°á»›c cá»§a KV
cache pool vÃ  loáº¡i bá» cÃ¡c má»¥c KV Ã­t quan trá»ng hÆ¡n mÃ  Ã­t
Ä‘Æ°á»£c chá»n bá»Ÿi cÃ¡c query token. ChÃºng tÃ´i má»Ÿ rá»™ng thiáº¿t káº¿
Ä‘á»ƒ káº¿t há»£p giá»›i háº¡n kÃ­ch thÆ°á»›c bá»™ nhá»› do ngÆ°á»i dÃ¹ng Ä‘á»‹nh
nghÄ©a. Trong runtime, khi kÃ­ch thÆ°á»›c bá»™ nhá»› CPU Ä‘áº¡t Ä‘áº¿n
giá»›i háº¡n do ngÆ°á»i dÃ¹ng Ä‘á»‹nh nghÄ©a, KV cache pool manager
chá»n má»™t má»¥c KV victim Ä‘á»ƒ loáº¡i bá». Tiáº¿p theo, manager ghi
Ä‘Ã¨ victim Ä‘Æ°á»£c chá»n vá»›i key vÃ  value má»›i Ä‘Æ°á»£c táº¡o, cÃ¹ng
vá»›i viá»‡c cáº­p nháº­t key cache má»™t pháº§n tÆ°Æ¡ng á»©ng náº±m trong
GPU. Cáº§n lÆ°u Ã½ ráº±ng thá»© tá»± cá»§a cÃ¡c má»¥c KV cÃ³ thá»ƒ tÃ¹y Ã½,
miá»…n lÃ  key vÃ  value cá»§a cÃ¹ng má»™t token duy trÃ¬ cÃ¹ng vá»‹ trÃ­
tÆ°Æ¡ng Ä‘á»‘i trong KV cache pool.

ChÃ­nh sÃ¡ch chá»n victim lÃ  quan trá»ng vÃ¬ nÃ³ trá»±c tiáº¿p áº£nh
hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh. ChÃºng tÃ´i xem xÃ©t chÃ­nh
sÃ¡ch dá»±a trÃªn counter cÃ¹ng vá»›i hai chÃ­nh sÃ¡ch loáº¡i bá» cache
pháº§n má»m Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i: FIFO [7, 69, 70] vÃ  Least-
Recently-Used (LRU) [2]. ChÃ­nh sÃ¡ch dá»±a trÃªn FIFO dá»…
triá»ƒn khai vá»›i overhead tháº¥p nhÆ°ng dáº«n Ä‘áº¿n sá»± giáº£m Ä‘á»™
chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i lá»›n vÃ¬ nÃ³ Ä‘Æ¡n giáº£n loáº¡i bá» token cÆ° trÃº
lÃ¢u nháº¥t. ChÃ­nh sÃ¡ch dá»±a trÃªn LRU thÆ°á»ng thá»ƒ hiá»‡n sá»± giáº£m
Ä‘á»™ chÃ­nh xÃ¡c nhá» hÆ¡n nhÆ°ng thÆ°á»ng Ä‘Ã²i há»i overhead runtime
cao hÆ¡n. NÃ³i chung, chÃ­nh sÃ¡ch dá»±a trÃªn LRU sá»­ dá»¥ng danh
sÃ¡ch liÃªn káº¿t Ä‘Ã´i vá»›i khÃ³a Ä‘á»ƒ thÄƒng cáº¥p cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c
truy cáº­p lÃªn Ä‘áº§u, Ä‘iá»u nÃ y yÃªu cáº§u cáº­p nháº­t bá»™ nhá»› nguyÃªn
tá»­ cho cÃ¡c má»¥c KV Ä‘Æ°á»£c truy cáº­p. Trong trÆ°á»ng há»£p chÃ­nh
sÃ¡ch dá»±a trÃªn counter, pool manager tÄƒng counter cho má»—i
má»¥c KV Ä‘Æ°á»£c prefetch vÃ  chá»n victim vá»›i sá»‘ Ä‘áº¿m nhá» nháº¥t
trong KV cache pool. Náº¿u báº¥t ká»³ counter nÃ o trá»Ÿ nÃªn bÃ£o
hÃ²a, táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ counter Ä‘Æ°á»£c giáº£m Ä‘i má»™t ná»­a. ChÃºng
tÃ´i quan sÃ¡t tháº¥y ráº±ng chÃ­nh sÃ¡ch dá»±a trÃªn counter vÃ  chÃ­nh
sÃ¡ch dá»±a trÃªn LRU cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh tÆ°Æ¡ng
Ä‘Æ°Æ¡ng, mÃ  chÃºng tÃ´i tháº£o luáº­n trong Pháº§n 5.2. ChÃºng tÃ´i
chá»n cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn counter do thiáº¿t káº¿ Ä‘Æ¡n giáº£n
hÆ¡n vÃ  Ä‘á»ƒ trÃ¡nh cáº­p nháº­t bá»™ nhá»› nguyÃªn tá»­ cho song song
tá»‘t hÆ¡n.

5 ÄÃ¡nh giÃ¡
5.1 Thiáº¿t láº­p thÃ­ nghiá»‡m
Cáº¥u hÃ¬nh mÃ´ hÃ¬nh vÃ  há»‡ thá»‘ng. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´
hÃ¬nh Open Pre-trained Transformer (OPT) [77] vá»›i 6.7B,
13B, vÃ  30B tham sá»‘ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡. CÃ¡c mÃ´ hÃ¬nh 7B vÃ  13B
cá»§a Llama-2 [60] cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chá»©ng minh ráº±ng
InfiniGen hoáº¡t Ä‘á»™ng hiá»‡u quáº£ trÃªn cÃ¡c kiáº¿n trÃºc mÃ´ hÃ¬nh
khÃ¡c nhau. ChÃºng tÃ´i cháº¡y cÃ¡c thÃ­ nghiá»‡m trÃªn há»‡ thá»‘ng
Ä‘Æ°á»£c trang bá»‹ GPU NVIDIA RTX A6000 [44] vá»›i 48GB bá»™
nhá»› vÃ  bá»™ xá»­ lÃ½ Intel Xeon Gold 6136 vá»›i 96GB bá»™ nhá»›
DDR4-2666. PCIe 3.0 Ã—16 káº¿t ná»‘i CPU vÃ  GPU.

Workload. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c tÃ¡c
vá»¥ downstream few-shot vÃ  táº­p dá»¯ liá»‡u mÃ´ hÃ¬nh hÃ³a ngÃ´n
ngá»¯. ChÃºng tÃ´i sá»­ dá»¥ng nÄƒm tÃ¡c vá»¥ few-shot tá»« benchmark
lm-evaluation-harness [23]: COPA [54], OpenBookQA [42],
WinoGrande [55], PIQA [8], vÃ  RTE [62]. CÃ¡c táº­p dá»¯ liá»‡u
mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  WikiText-2 [41] vÃ 
Penn Treebank (PTB) [38]. NgoÃ i ra, cÃ¡c cÃ¢u Ä‘Æ°á»£c láº¥y máº«u
ngáº«u nhiÃªn tá»« táº­p dá»¯ liá»‡u PG-19 [52] Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o
tÄƒng tá»‘c vá»›i Ä‘á»™ dÃ i chuá»—i dÃ i.

Baseline. ChÃºng tÃ´i sá»­ dá»¥ng hai mÃ´i trÆ°á»ng suy luáº­n há»— trá»£
offloading KV cache: CUDA Unified Virtual Memory (UVM)
[4] vÃ  FlexGen [57]. TrÃªn UVM, táº¥t cáº£ chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u
giá»¯a CPU vÃ  GPU Ä‘Æ°á»£c quáº£n lÃ½ ngáº§m Ä‘á»‹nh bá»Ÿi driver thiáº¿t
bá»‹ UVM, tá»« Ä‘Ã³ cho phÃ©p offloading mÃ  khÃ´ng yÃªu cáº§u can
thiá»‡p tá»« láº­p trÃ¬nh viÃªn. NgÆ°á»£c láº¡i, FlexGen sá»­ dá»¥ng chuyá»ƒn
dá»¯ liá»‡u rÃµ rÃ ng giá»¯a CPU vÃ  GPU. Äá»‘i vá»›i baseline FlexGen,
trá»« khi Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh khÃ¡c, chÃºng tÃ´i Ä‘áº·t rÃµ rÃ ng táº¥t cáº£ KV
cache trong bá»™ nhá»› CPU. CÃ¡c tham sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u
trá»¯ trong bá»™ nhá»› GPU cÃ ng nhiá»u cÃ ng tá»‘t, pháº§n cÃ²n láº¡i
trong bá»™ nhá»› CPU. ChÃºng tÃ´i so sÃ¡nh InfiniGen vá»›i hai
phÆ°Æ¡ng phÃ¡p quáº£n lÃ½ KV cache khÃ¡c nhau: H2O [78] vÃ 
Quantization [57]. H2O, má»™t phÆ°Æ¡ng phÃ¡p gáº§n Ä‘Ã¢y trong
quáº£n lÃ½ KV cache, duy trÃ¬ KV cache cá»§a cÃ¡c token quan
trá»ng hoáº·c gáº§n Ä‘Ã¢y báº±ng cÃ¡ch Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a
má»—i token vÃ  loáº¡i bá» cÃ¡c token khÃ¡c. NÃ©n dá»±a trÃªn Quantization
Ã¡p dá»¥ng quantization báº¥t Ä‘á»‘i xá»©ng theo nhÃ³m cho KV cache.

Metric chÃ­nh. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c (%) Ä‘á»ƒ Ä‘Ã¡nh
giÃ¡ tÃ¡c Ä‘á»™ng cá»§a xáº¥p xá»‰ khi InfiniGen, H2O, vÃ  Quantization
Ä‘Æ°á»£c sá»­ dá»¥ng. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ vá»›i
WikiText-2 vÃ  PTB, chÃºng tÃ´i sá»­ dá»¥ng perplexity lÃ m metric;
perplexity tháº¥p hÆ¡n cÃ³ nghÄ©a lÃ  Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n. Äá»ƒ
trÃ¬nh bÃ y cáº£i thiá»‡n hiá»‡u suáº¥t, chÃºng tÃ´i Ä‘o thá»i gian wall
clock trong suy luáº­n vá»›i kÃ­ch thÆ°á»›c batch vÃ  Ä‘á»™ dÃ i chuá»—i
khÃ¡c nhau. Tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n Ä‘Æ°á»£c Ä‘áº·t thÃ nh 0.3.
ChÃºng tÃ´i Ä‘áº·t alpha thÃ nh 4 cho OPT vÃ  5 cho Llama-2,
dáº«n Ä‘áº¿n sá»­ dá»¥ng Ã­t hÆ¡n 10% KV cache trung bÃ¬nh qua cÃ¡c
layer. Äá»‘i vá»›i má»—i layer, chÃºng tÃ´i cho phÃ©p gá»­i lÃªn Ä‘áº¿n
20% tá»•ng KV cache vÃ o GPU náº¿u nÃ³ chá»©a nhiá»u á»©ng viÃªn hÆ¡n.
9

--- TRANG 10 ---
5061728394
0153045604555657585
0153045604555657585
015304560
5061728394
0153045604555657585
0153045604555657585
0153045604555657585
015304560
4555657585
0153045604555657585
0153045604555657585
015304560
5061728394
0153045602028364452
015304560
4555657585
015304560
4555657585
0153045602028364452
015304560
4555657585
0153045602028364452
015304560Full CacheQuantizationH2OInfiniGen
4555657585
0153045602028364452
015304560
5061728394
0153045602028364452
015304560
4555657585
0153045604555657585
015304560
5061728394
0153045604555657585
015304560OPT-6.7BPIQAOPT-13BOPT-30BLlama-2-7BLlama-2-13BOpenBookQAWinoGrandeRTECOPAAccuracy (%)
Relative KV Cache Size (%)H2OHÃ¬nh 11: Äá»™ chÃ­nh xÃ¡c cá»§a LLM trÃªn cÃ¡c tÃ¡c vá»¥ 5-shot trong lm-evaluation-harness.

3579
147101316PerplexityFull CacheH2OInfiniGen6121824
12345678PerplexityFull CacheH2OInfiniGen
(b) Llama-2-13B(a) OPT-13BH2O
Decoding Chunk IDDecoding Chunk IDH2O
HÃ¬nh 12: Perplexity cá»§a OPT-13B vÃ  Llama-2-13B cho táº­p dá»¯ liá»‡u
WikiText-2. Tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n. Perplexity Ä‘Æ°á»£c tÃ­nh toÃ¡n cho má»—i
decoding chunk chá»©a 256 token.

Tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n vÃ  alpha Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn
nghiÃªn cá»©u Ä‘á»™ nháº¡y cho má»—i mÃ´ hÃ¬nh Ä‘á»ƒ cÃ¢n báº±ng Ä‘á»™ chÃ­nh
xÃ¡c vÃ  Ä‘á»™ trá»… suy luáº­n, mÃ  chÃºng tÃ´i tháº£o luáº­n trong Pháº§n 6.1.

5.2 MÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯
Äá»™ chÃ­nh xÃ¡c trÃªn lm-evaluation-harness. HÃ¬nh 11 cho tháº¥y
Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c baseline vÃ  InfiniGen qua cÃ¡c mÃ´ hÃ¬nh
khÃ¡c nhau vá»›i cÃ¡c tÃ¡c vá»¥ 5-shot. KÃ­ch thÆ°á»›c KV cache tÆ°Æ¡ng
Ä‘á»‘i cho tháº¥y kÃ­ch thÆ°á»›c cá»§a KV cache tham gia vÃ o tÃ­nh
toÃ¡n attention so vá»›i baseline full-cache (vÃ­ dá»¥: kÃ­ch thÆ°á»›c
KV cache tÆ°Æ¡ng Ä‘á»‘i 10% cÃ³ nghÄ©a lÃ  10% kÃ­ch thÆ°á»›c full
KV cache Ä‘Æ°á»£c sá»­ dá»¥ng). InfiniGen liÃªn tá»¥c cho tháº¥y Ä‘á»™
chÃ­nh xÃ¡c tá»‘t hÆ¡n qua cÃ¡c mÃ´ hÃ¬nh vÃ  tÃ¡c vá»¥ khi kÃ­ch thÆ°á»›c
KV cache tÆ°Æ¡ng Ä‘á»‘i nhá» hÆ¡n 10%, trong khi cÃ¡c phÆ°Æ¡ng
phÃ¡p khÃ¡c thá»ƒ hiá»‡n sá»± giáº£m Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡ng chÃº Ã½ do Ä‘á»™
rá»™ng bit khÃ´ng Ä‘á»§ (Quantization) hoáº·c loáº¡i bá» KV cache
vÄ©nh viá»…n (H2O). Äiá»u nÃ y ngá»¥ Ã½ ráº±ng giáº£i phÃ¡p Ä‘Æ°á»£c Ä‘á»
xuáº¥t cá»§a chÃºng tÃ´i cÃ³ thá»ƒ hiá»‡u quáº£ giáº£m overhead chuyá»ƒn
KV cache trong khi báº£o toÃ n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh. Äá»‘i vá»›i
kÃ­ch thÆ°á»›c KV cache tÆ°Æ¡ng Ä‘á»‘i lá»›n hÆ¡n 10%, Ä‘á»™ chÃ­nh xÃ¡c
vá»›i InfiniGen phÃ¹ há»£p gáº§n vá»›i baseline full-cache. Trong
má»™t sá»‘ trÆ°á»ng há»£p, InfiniGen tháº­m chÃ­ cho tháº¥y Ä‘á»™ chÃ­nh
xÃ¡c hÆ¡i tá»‘t hÆ¡n so vá»›i baseline full-cache. Äiá»u nÃ y cÃ³ thá»ƒ
lÃ  do viá»‡c giáº£m lÆ°á»£ng KV cache tham gia vÃ o tÃ­nh toÃ¡n
attention cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh táº­p trung hÆ¡n vÃ o cÃ¡c token
quan trá»ng.

Äá»™ dÃ i chuá»—i. HÃ¬nh 12 cho tháº¥y perplexity cá»§a hai mÃ´ hÃ¬nh
khÃ¡c nhau vá»›i InfiniGen vÃ  cÃ¡c baseline, khi Ä‘á»™ dÃ i chuá»—i
tÄƒng. Trong thÃ­ nghiá»‡m nÃ y, H2O Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ sá»­ dá»¥ng
cÃ¹ng lÆ°á»£ng KV cache nhÆ° InfiniGen. Äá»™ dÃ i chuá»—i lÃ  2048
vÃ  4096 cho OPT-13B vÃ  Llama-2-13B, tÆ°Æ¡ng á»©ng. Äá»ƒ cÃ³
cÃ¡i nhÃ¬n rÃµ hÆ¡n, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ perplexity vá»›i 256 token
liÃªn tiáº¿p nhÆ° má»™t nhÃ³m, Ä‘Æ°á»£c gá»i lÃ  decoding chunk trong
hÃ¬nh. Káº¿t quáº£ cho tháº¥y ráº±ng máº·c dÃ¹ Ä‘á»™ dÃ i chuá»—i trá»Ÿ nÃªn
dÃ i hÆ¡n (tá»©c lÃ  ID decoding chunk tÄƒng), perplexity cá»§a
InfiniGen váº«n liÃªn tá»¥c tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i baseline full-cache,
10

--- TRANG 11 ---
Báº£ng 2: Perplexity trÃªn WikiText-2 vÃ  PTB vá»›i Ä‘á»™ dÃ i chuá»—i 2048
cÃ³ hoáº·c khÃ´ng cÃ³ giá»›i háº¡n bá»™ nhá»› KV cache. Tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n.

SÆ¡ Ä‘á»“OPT-6.7B OPT-13B OPT-30B Llama-2-7B Llama-2-13B
Wiki PTB Wiki PTB Wiki PTB Wiki PTB Wiki PTB
100% 11.68 13.86 10.55 12.78 10.14 12.31 5.69 22.53 5.25 31.94
80-FIFO% 19.64 16.82 30.99 33.84 30.66 35.45 22.26 61.88 21.41 32.34
80-LRU% 11.68 13.85 10.55 12.78 10.14 12.31 5.69 22.53 5.25 31.94
80-Counter% 11.68 13.86 10.55 12.78 10.14 12.31 5.69 22.53 5.25 31.94

020406080100
COPAOpenBookQAWinoGrandePIQARTEAccuracy (%)Full Cachew/o Skewingw/ Skewing
HÃ¬nh 13: Äá»™ chÃ­nh xÃ¡c trÃªn benchmark lm-evaluation-harness cÃ³
hoáº·c khÃ´ng cÃ³ skewing trÃªn OPT-6.7B.

trong khi H2O cho tháº¥y sá»± phÃ¢n ká»³ ngÃ y cÃ ng tÄƒng so vá»›i
baseline. H2O bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi viá»‡c loáº¡i bá» KV cache vÄ©nh
viá»…n vÃ  cÃ³ thá»ƒ khÃ´ng giá»¯ láº¡i Ä‘á»§ lÆ°á»£ng KV cache trong má»™t
sá»‘ layer nháº¥t Ä‘á»‹nh do ngÃ¢n sÃ¡ch cá»‘ Ä‘á»‹nh cá»§a nÃ³. NgÆ°á»£c láº¡i,
InfiniGen Ä‘á»™ng tÃ­nh toÃ¡n attention chá»‰ sá»­ dá»¥ng lÆ°á»£ng KV
cache cáº§n thiáº¿t cho má»—i layer. Sá»± khÃ¡c biá»‡t cÃ³ kháº£ nÄƒng
má»Ÿ rá»™ng khi cÃ¡c mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng xá»­ lÃ½ cÃ¡c chuá»—i dÃ i
hÆ¡n nhiá»u.

TÃ¡c Ä‘á»™ng cá»§a Skewing. HÃ¬nh 13 cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c cÃ³
hoáº·c khÃ´ng cÃ³ key/query skewing trÃªn mÃ´ hÃ¬nh OPT-6.7B.
Äá»‘i vá»›i thÃ­ nghiá»‡m, chÃºng tÃ´i sá»­ dá»¥ng ngÃ¢n sÃ¡ch KV cache
cá»‘ Ä‘á»‹nh lÃ  20%, thay vÃ¬ sá»­ dá»¥ng cÃ¡ch tiáº¿p cáº­n Ä‘á»™ng, Ä‘á»ƒ rÃµ
rÃ ng cho tháº¥y tÃ¡c Ä‘á»™ng cá»§a skewing. ChÃºng tÃ´i quan sÃ¡t tháº¥y
ráº±ng má»™t sá»‘ mÃ´ hÃ¬nh ngÃ´n ngá»¯ (vÃ­ dá»¥: Llama-2) cho tháº¥y
sá»± giáº£m nhá» vá» Ä‘á»™ chÃ­nh xÃ¡c mÃ  khÃ´ng cÃ³ skewing. Tuy
nhiÃªn, Ä‘á»‘i vá»›i má»™t sá»‘ mÃ´ hÃ¬nh nhÆ° OPT-6.7B, chÃºng tÃ´i
tháº¥y sá»± giáº£m Ä‘á»™ chÃ­nh xÃ¡c lá»›n náº¿u chÃºng tÃ´i khÃ´ng Ã¡p dá»¥ng
phÆ°Æ¡ng phÃ¡p skewing nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 13.
Äiá»u nÃ y cho tháº¥y ráº±ng trong trÆ°á»ng há»£p OPT-6.7B, trá»ng
sá»‘ má»™t pháº§n khÃ´ng Ä‘áº¡i diá»‡n Ä‘áº§y Ä‘á»§ cho ma tráº­n gá»‘c mÃ 
khÃ´ng cÃ³ skewing. Sau khi Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p skewing
cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng tá»±
nhÆ° baseline full-cache. PhÆ°Æ¡ng phÃ¡p skewing cá»§a chÃºng
tÃ´i hiá»‡u quáº£ lÃ m nghiÃªng cÃ¡c ma tráº­n key vÃ  query sao cho
má»™t vÃ i cá»™t cÃ³ thá»ƒ Ä‘áº¡i diá»‡n tá»‘t hÆ¡n cho cÃ¡c ma tráº­n gá»‘c.

Quáº£n lÃ½ KV Cache Pool. Báº£ng 2 cho tháº¥y perplexity cá»§a
nÄƒm mÃ´ hÃ¬nh khÃ¡c nhau cÃ³ hoáº·c khÃ´ng cÃ³ giá»›i háº¡n dung
lÆ°á»£ng bá»™ nhá»› cho WikiText-2 vÃ  PTB. ChÃºng tÃ´i so sÃ¡nh
cÃ¡c chÃ­nh sÃ¡ch lá»±a chá»n victim dá»±a trÃªn FIFO, LRU, vÃ 
Counter trong Pháº§n 4.4 dÆ°á»›i giá»›i háº¡n bá»™ nhá»› 80% cá»§a full
KV cache. ChÃºng tÃ´i cÅ©ng trÃ¬nh bÃ y káº¿t quáº£ perplexity
khÃ´ng cÃ³ giá»›i háº¡n bá»™ nhá»› (100%). CÃ¡ch tiáº¿p cáº­n dá»±a trÃªn
FIFO cho tháº¥y hiá»‡u suáº¥t mÃ´ hÃ¬nh tá»‡ nháº¥t vÃ¬ nÃ³ Ä‘Æ¡n giáº£n xÃ³a
má»¥c KV cÅ© nháº¥t báº¥t ká»ƒ táº§m quan trá»ng cá»§a nÃ³. CÃ¡c cÃ¡ch
tiáº¿p cáº­n LRU vÃ  dá»±a trÃªn Counter cho tháº¥y perplexity gáº§n
nhÆ° tÆ°Æ¡ng tá»± vá»›i trÆ°á»ng há»£p khÃ´ng cÃ³ giá»›i háº¡n bá»™ nhá»›.
ChÃºng tÃ´i chá»n chÃ­nh sÃ¡ch lá»±a chá»n victim dá»±a trÃªn Counter
thay vÃ¬ cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn LRU vÃ¬ cÃ¡ch tiáº¿p cáº­n dá»±a
trÃªn LRU thÆ°á»ng cáº§n duy trÃ¬ hÃ ng Ä‘á»£i danh sÃ¡ch liÃªn káº¿t
Ä‘Ã´i vá»›i khÃ³a cho cáº­p nháº­t bá»™ nhá»› nguyÃªn tá»­.

0200400600
UVMUVMFlexGenFlexGenFlexGenInfiniGenLatency (s)PrefillDecode2007.4
+ H2O+ H2O+ INT4HÃ¬nh 14: Äá»™ trá»… suy luáº­n trÃªn OPT-13B vá»›i Ä‘á»™ dÃ i chuá»—i 2048
(1920 token Ä‘áº§u vÃ o vÃ  128 token Ä‘áº§u ra) vÃ  kÃ­ch thÆ°á»›c batch 20.

0200400600
48121620Latency (s)UVMUVM + H2OFlexGenFlexGen + INT4FlexGen + H2OInfiniGen2007.41737.4H2OH2OBatch Size
HÃ¬nh 15: Äá»™ trá»… suy luáº­n cho 5 kÃ­ch thÆ°á»›c batch khÃ¡c nhau trÃªn
OPT-13B vá»›i Ä‘á»™ dÃ i chuá»—i 2048 (1920 token Ä‘áº§u vÃ o vÃ  128 token Ä‘áº§u ra).

5.3 Hiá»‡u suáº¥t
Trong pháº§n nÃ y, chÃºng tÃ´i gá»i H2O (vá»›i ngÃ¢n sÃ¡ch KV cache
20%) vÃ  quantization 4-bit Ä‘Æ°á»£c triá»ƒn khai trÃªn FlexGen
lÃ  H2O vÃ  INT4.

Äá»™ trá»… suy luáº­n. HÃ¬nh 14 cho tháº¥y Ä‘á»™ trá»… suy luáº­n bao gá»“m
giai Ä‘oáº¡n prefill vÃ  decoding. ChÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh
OPT-13B vá»›i 1920 token Ä‘áº§u vÃ o, 128 token Ä‘áº§u ra, vÃ  kÃ­ch
thÆ°á»›c batch 20. InfiniGen Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 1.63Ã—-32.93Ã—
so vá»›i cÃ¡c baseline. Lá»£i Ã­ch hiá»‡u suáº¥t chá»§ yáº¿u Ä‘áº¿n tá»« viá»‡c
giáº£m Ä‘Ã¡ng ká»ƒ lÆ°á»£ng KV cache cáº§n load tá»« bá»™ nhá»› CPU do
cÃ¡ch tiáº¿p cáº­n Ä‘á»™ng cá»§a chÃºng tÃ´i.

UVM cho tháº¥y Ä‘á»™ trá»… cá»±c ká»³ dÃ i vÃ¬ kÃ­ch thÆ°á»›c working set
(tá»©c lÃ  kÃ­ch thÆ°á»›c cá»§a cÃ¡c tham sá»‘ mÃ´ hÃ¬nh vÃ  KV cache)
lá»›n hÆ¡n dung lÆ°á»£ng bá»™ nhá»› GPU, tá»« Ä‘Ã³ dáº«n Ä‘áº¿n page fault
thÆ°á»ng xuyÃªn vÃ  chuyá»ƒn dá»¯ liá»‡u giá»¯a CPU vÃ  GPU. Giai
Ä‘oáº¡n prefill cá»§a UVM + H2O cÅ©ng cho tháº¥y Ä‘á»™ trá»… dÃ i do
page fault vÃ  chuyá»ƒn dá»¯ liá»‡u. Tuy nhiÃªn, vÃ¬ táº¥t cáº£ dá»¯ liá»‡u
cáº§n thiáº¿t Ä‘Æ°á»£c di chuyá»ƒn vÃ o bá»™ nhá»› GPU sau giai Ä‘oáº¡n
prefill, UVM + H2O cho tháº¥y Ä‘á»™ trá»… decoding ngáº¯n hÆ¡n
Ä‘Ã¡ng ká»ƒ. FlexGen load full KV cache vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao
(tá»©c lÃ  FP16) tá»« bá»™ nhá»› CPU cho má»—i tÃ­nh toÃ¡n attention.
Máº·t khÃ¡c, INT4 vÃ  H2O load lÆ°á»£ng dá»¯ liá»‡u tÆ°Æ¡ng Ä‘á»‘i nhá»
tá»« CPU vÃ¬ Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u bit tháº¥p (INT4) hoáº·c kÃ­ch
thÆ°á»›c KV cache nhá» hÆ¡n (H2O). Tuy nhiÃªn, há» váº«n load
lÆ°á»£ng dá»¯ liá»‡u lá»›n hÆ¡n InfiniGen; ngay cáº£ vá»›i Ä‘á»™ chÃ­nh xÃ¡c
tháº¥p, INT4 load KV cache cá»§a táº¥t cáº£ token trÆ°á»›c Ä‘Ã³; H2O
luÃ´n load cÃ¹ng lÆ°á»£ng dá»¯ liá»‡u báº¥t ká»ƒ cÃ³ bao nhiÃªu token
thá»±c sá»± quan trá»ng trong má»—i layer. Káº¿t quáº£ lÃ , InfiniGen
Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n cáº£ hai.

KÃ­ch thÆ°á»›c Batch. HÃ¬nh 15 cho tháº¥y Ä‘á»™ trá»… suy luáº­n qua
cÃ¡c kÃ­ch thÆ°á»›c batch khÃ¡c nhau. Káº¿t quáº£ cho tháº¥y InfiniGen
Ä‘áº¡t Ä‘Æ°á»£c
11

--- TRANG 12 ---
0246
512102415362048SpeedupINT4H2OInfiniGen
0246
6.7B13B30BSpeedupINT4H2OInfiniGen
(b) Model Size(a) Sequence LengthH2OH2OHÃ¬nh 16: TÄƒng tá»‘c so vá»›i baseline FlexGen qua (a) Ä‘á»™ dÃ i chuá»—i
vÃ  (b) kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh.

Ä‘á»™ trá»… tháº¥p hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c qua cÃ¡c kÃ­ch thÆ°á»›c
batch (1.28Ã—-34.64Ã—). Khi kÃ­ch thÆ°á»›c batch tÄƒng, khoáº£ng
cÃ¡ch hiá»‡u suáº¥t giá»¯a InfiniGen vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c
trá»Ÿ nÃªn lá»›n hÆ¡n. UVM vÃ  UVM + H2O cho tháº¥y Ä‘á»™ trá»… tÄƒng
chá»§ yáº¿u do page fault thÆ°á»ng xuyÃªn trong giai Ä‘oáº¡n prefill.
Äá»‘i vá»›i UVM, Ä‘á»™ trá»… cÅ©ng tÄƒng nhanh táº¡i kÃ­ch thÆ°á»›c batch
16 vÃ¬ kÃ­ch thÆ°á»›c working set vÆ°á»£t quÃ¡ dung lÆ°á»£ng bá»™ nhá»›
GPU cho cáº£ giai Ä‘oáº¡n prefill vÃ  decoding. Khi kÃ­ch thÆ°á»›c
batch tiáº¿p tá»¥c tÄƒng, UVM + H2O sáº½ gáº·p cÃ¹ng váº¥n Ä‘á».

Äá»™ trá»… cá»§a FlexGen tÄƒng gáº§n nhÆ° tuyáº¿n tÃ­nh vá»›i kÃ­ch thÆ°á»›c
batch vÃ¬ viá»‡c chuyá»ƒn KV cache chiáº¿m pháº§n lá»›n Ä‘á»™ trá»… suy
luáº­n. Khi chÃºng tÃ´i tÄƒng kÃ­ch thÆ°á»›c batch tá»« 4 lÃªn 20, thÃ´ng
lÆ°á»£ng (token má»—i giÃ¢y) cá»§a InfiniGen tÄƒng tá»« 27.36 lÃªn 41.99,
trong khi INT4 vÃ  H2O cung cáº¥p tÄƒng nhá» vá» thÃ´ng lÆ°á»£ng
(tá»« 12.22 lÃªn 14.02 vÃ  tá»« 21.31 lÃªn 25.70). Báº±ng cÃ¡ch Ä‘á»™ng
Ä‘iá»u chá»‰nh lÆ°á»£ng KV cache cáº§n load, InfiniGen Ä‘áº¡t Ä‘Æ°á»£c
hiá»‡u suáº¥t cÃ³ thá»ƒ má»Ÿ rá»™ng qua cÃ¡c kÃ­ch thÆ°á»›c batch.

Äá»™ dÃ i chuá»—i. HÃ¬nh 16(a) cho tháº¥y tÄƒng tá»‘c cá»§a INT4, H2O,
vÃ  InfiniGen so vá»›i FlexGen trÃªn OPT-13B qua cÃ¡c Ä‘á»™ dÃ i
chuá»—i khÃ¡c nhau. Vá»›i kÃ­ch thÆ°á»›c batch 8, chÃºng tÃ´i sá»­ dá»¥ng
bá»‘n cáº¥u hÃ¬nh Ä‘áº§u vÃ o/Ä‘áº§u ra khÃ¡c nhau. Má»—i cáº¥u hÃ¬nh bao
gá»“m 128 token Ä‘áº§u ra vÃ  384, 896, 1408, 1920 token Ä‘áº§u
vÃ o (tá»©c lÃ  tá»•ng sá»‘ token tá»« 512 Ä‘áº¿n 2048). TÄƒng tá»‘c cá»§a
InfiniGen tiáº¿p tá»¥c tÄƒng qua cÃ¡c Ä‘á»™ dÃ i chuá»—i (lÃªn Ä‘áº¿n 5.28Ã—),
trong khi INT4 vÃ  H2O cho tháº¥y tÄƒng tá»‘c bÃ£o hÃ²a (lÃªn Ä‘áº¿n
1.92Ã— vÃ  3.40Ã—). Äiá»u nÃ y gá»£i Ã½ ráº±ng cáº£ INT4 vÃ  H2O Ä‘á»u
khÃ´ng cung cáº¥p giáº£i phÃ¡p cÃ³ thá»ƒ má»Ÿ rá»™ng cho quáº£n lÃ½ KV
cache. INT4 cho tháº¥y tÄƒng tá»‘c tÄƒng khÃ´ng Ä‘Ã¡ng ká»ƒ do sá»±
tÄƒng trÆ°á»Ÿng vá»‘n cÃ³ trong kÃ­ch thÆ°á»›c KV cache. TÆ°Æ¡ng tá»±,
H2O thiáº¿u kháº£ nÄƒng má»Ÿ rá»™ng do tá»· lá»‡ cá»‘ Ä‘á»‹nh cá»§a ngÃ¢n sÃ¡ch
KV cache; khi Ä‘á»™ dÃ i chuá»—i tÄƒng, H2O lÆ°u trá»¯ vÃ  load nhiá»u
KV cache hÆ¡n.

Máº·c dÃ¹ Ä‘á»™ dÃ i chuá»—i tÄƒng, sá»‘ lÆ°á»£ng token mÃ  má»—i token
chÃº Ã½ Ä‘áº¿n khÃ´ng tÄƒng tuyáº¿n tÃ­nh. VÃ­ dá»¥, trong mÃ´ hÃ¬nh
OPT-13B, chÃºng tÃ´i Ä‘áº¿m sá»‘ lÆ°á»£ng token quan trá»ng vá»›i
attention score lá»›n hÆ¡n (maxâˆ’4) vÃ  xÃ¡c Ä‘á»‹nh ráº±ng, trung
bÃ¬nh, 37, 60, 66, vÃ  73 token Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ lÃ  quan trá»ng
cho Ä‘á»™ dÃ i chuá»—i 512, 1024, 1536, vÃ  2048, tÆ°Æ¡ng á»©ng.
H2O, sá»­ dá»¥ng 20% ngÃ¢n sÃ¡ch KV cache cá»‘ Ä‘á»‹nh, load 409
token cho Ä‘á»™ dÃ i chuá»—i 2048, trong khi chá»‰ 73 token tÆ°Æ¡ng
Ä‘á»‘i quan trá»ng. NgÆ°á»£c láº¡i, InfiniGen tá»± nhiÃªn náº¯m báº¯t xu
hÆ°á»›ng nÃ y (tá»©c lÃ  sá»± tÄƒng phi tuyáº¿n trong sá»‘ lÆ°á»£ng token
quan trá»ng) báº±ng cÃ¡ch

0204060
40506070
0.10.30.50.70.9Latency (s)Accuracy (%)AccuracyLatency(b) Partial Weight Ratio0204060
40506070
13579Latency (s)Accuracy (%)AccuracyLatency(a) Alpha ValueHÃ¬nh 17: Äá»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»… suy luáº­n qua (a) giÃ¡ trá»‹ alpha
vÃ  (b) tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n.

Ä‘á»™ng quan sÃ¡t attention score Ä‘Æ°á»£c dá»± Ä‘oÃ¡n.

KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh. HÃ¬nh 16(b) cho tháº¥y tÄƒng tá»‘c cá»§a INT4,
H2O, vÃ  InfiniGen so vá»›i FlexGen trÃªn ba kÃ­ch thÆ°á»›c mÃ´
hÃ¬nh khÃ¡c nhau. ChÃºng tÃ´i sá»­ dá»¥ng 1920 token Ä‘áº§u vÃ o vÃ 
128 token Ä‘áº§u ra vá»›i kÃ­ch thÆ°á»›c batch 4 cho thÃ­ nghiá»‡m.
Káº¿t quáº£ cho tháº¥y InfiniGen vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p
khÃ¡c qua cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh. Khi kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh
tÄƒng tá»« 6.7B lÃªn 13B, tÄƒng tá»‘c cá»§a InfiniGen cÅ©ng tÄƒng
1.17Ã—, trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c khÃ´ng dáº«n Ä‘áº¿n tÄƒng
tá»‘c Ä‘Ã¡ng chÃº Ã½. Äá»‘i vá»›i háº§u háº¿t cÃ¡c layer, InfiniGen load
lÆ°á»£ng KV cache nhá» hÆ¡n H2O vÃ¬ sá»‘ lÆ°á»£ng token tÆ°Æ¡ng Ä‘á»‘i
nhá» cáº§n thiáº¿t. Do Ä‘Ã³, InfiniGen hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n H2O khi
kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh lá»›n hÆ¡n do sá»‘ lÆ°á»£ng khá»‘i Transformer
tÄƒng. Äá»‘i vá»›i mÃ´ hÃ¬nh 30B, cÃ¡c tham sá»‘ mÃ´ hÃ¬nh khÃ´ng vá»«a
trong bá»™ nhá»› GPU. Do Ä‘Ã³, chÃºng tÃ´i offload 30% tham sá»‘
mÃ´ hÃ¬nh vÃ o CPU. Trong trÆ°á»ng há»£p nÃ y, kÃ­ch thÆ°á»›c tham
sá»‘ Ä‘Æ°á»£c offload lá»›n hÆ¡n 1.7Ã— so vá»›i kÃ­ch thÆ°á»›c KV cache.
Ngay cáº£ nhÆ° váº­y, InfiniGen cho tháº¥y tÄƒng tá»‘c 1.34Ã— so vá»›i
FlexGen, trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c Ä‘áº¡t Ä‘Æ°á»£c 1.18Ã—
vÃ  1.28Ã—.

6 PhÃ¢n tÃ­ch vÃ  tháº£o luáº­n
6.1 NghiÃªn cá»©u Ä‘á»™ nháº¡y
ChÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh OPT-6.7B vá»›i 1920 token Ä‘áº§u
vÃ o, 128 token Ä‘áº§u ra, vÃ  kÃ­ch thÆ°á»›c batch 8. Äá»™ chÃ­nh xÃ¡c
Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ vá»›i tÃ¡c vá»¥ WinoGrande trong lm-evaluation-
harness.

NgÆ°á»¡ng vÃ  Alpha. NhÆ° Ä‘Ã£ tháº£o luáº­n trong Pháº§n 4.3, chÃºng
tÃ´i load KV cache cá»§a cÃ¡c token cÃ³ attention score Ä‘Æ°á»£c
dá»± Ä‘oÃ¡n lá»›n hÆ¡n ngÆ°á»¡ng (tá»©c lÃ  attention score tá»‘i Ä‘a trá»«
alpha). TÄƒng alpha dáº«n Ä‘áº¿n fetch nhiá»u má»¥c KV hÆ¡n vÃ o
GPU, do Ä‘Ã³ tÄƒng Ä‘á»™ trá»… suy luáº­n nhÆ°ng cÅ©ng cáº£i thiá»‡n Ä‘á»™
chÃ­nh xÃ¡c. HÃ¬nh 17(a) cho tháº¥y sá»± Ä‘Ã¡nh Ä‘á»•i nhÆ° váº­y giá»¯a
Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»… suy luáº­n cho chÃ­n giÃ¡ trá»‹ alpha khÃ¡c
nhau vá»›i tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n lÃ  0.3. Káº¿t quáº£ cho tháº¥y
nhiá»u má»¥c KV hÆ¡n Ä‘Æ°á»£c fetch vÃ  tham gia vÃ o tÃ­nh toÃ¡n
attention khi alpha tÄƒng, tá»« Ä‘Ã³ dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c tá»‘t
hÆ¡n. Tuy nhiÃªn, Ä‘á»‘i vá»›i cÃ¡c giÃ¡ trá»‹ alpha vÆ°á»£t quÃ¡ 4, vÃ¬ háº§u
háº¿t token quan trá»ng Ä‘Ã£ Ä‘Æ°á»£c bao gá»“m, Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng
tÄƒng thÃªm, trong khi chi phÃ­ cho viá»‡c chuyá»ƒn KV vÃ  tÃ­nh
toÃ¡n attention tiáº¿p tá»¥c tÄƒng. Xu hÆ°á»›ng nÃ y Ä‘Æ°á»£c quan sÃ¡t
tÆ°Æ¡ng tá»± á»Ÿ cÃ¡c mÃ´ hÃ¬nh khÃ¡c, vÃ  do Ä‘Ã³ chÃºng tÃ´i chá»n giÃ¡
trá»‹ alpha lÃ  4 hoáº·c 5 Ä‘á»ƒ Ä‘áº¡t cÃ¢n báº±ng giá»¯a Ä‘á»™ trá»… suy luáº­n
vÃ  Ä‘á»™ chÃ­nh xÃ¡c.

Tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n. HÃ¬nh 17(b) cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c
vÃ 
12

--- TRANG 13 ---
051015
FlexGenINT4H2OInfiniGenIdealLatency (ms)AttentionFFNData TransferPrediction28.0
H2OHÃ¬nh 18: PhÃ¢n tÃ­ch Ä‘á»™ trá»… cá»§a má»™t khá»‘i Transformer cho OPT-13B
vá»›i Ä‘á»™ dÃ i chuá»—i 2048 vÃ  kÃ­ch thÆ°á»›c batch 8.

Ä‘á»™ trá»… suy luáº­n qua cÃ¡c tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n khÃ¡c nhau
vá»›i giÃ¡ trá»‹ alpha lÃ  4. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong hÃ¬nh, lÆ°á»£ng
trá»ng sá»‘ má»™t pháº§n cÃ³ tÃ¡c Ä‘á»™ng khÃ´ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n Ä‘á»™ trá»…
suy luáº­n vÃ¬ chi phÃ­ tÃ­nh toÃ¡n attention score Ä‘Æ°á»£c dá»± Ä‘oÃ¡n
tÆ°Æ¡ng Ä‘á»‘i nhá». LÆ°u Ã½ ráº±ng lÆ°á»£ng KV cache cáº§n chuyá»ƒn
khÃ´ng liÃªn quan Ä‘áº¿n tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n. Tuy nhiÃªn,
tÄƒng tá»· lá»‡ trá»ng sá»‘ má»™t pháº§n dáº«n Ä‘áº¿n tiÃªu thá»¥ bá»™ nhá»› cao
hÆ¡n cho trá»ng sá»‘ má»™t pháº§n vÃ  key cache (vÃ­ dá»¥: tÄƒng gáº¥p
Ä‘Ã´i tá»· lá»‡ sáº½ tÄƒng gáº¥p Ä‘Ã´i overhead tiÃªu thá»¥ bá»™ nhá»›). Äá»™
chÃ­nh xÃ¡c cÅ©ng khÃ´ng khÃ¡c biá»‡t Ä‘Ã¡ng chÃº Ã½ vÆ°á»£t quÃ¡ tá»·
lá»‡ 0.3. Trong cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i, chÃºng tÃ´i chá»n tá»·
lá»‡ trá»ng sá»‘ má»™t pháº§n lÃ  0.3 Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t
hÆ¡n trong khi xem xÃ©t overhead tiÃªu thá»¥ bá»™ nhá»›.

6.2 Overhead
Overhead Prefetching. HÃ¬nh 18 cho tháº¥y phÃ¢n tÃ­ch Ä‘á»™ trá»…
cá»§a viá»‡c thá»±c thi má»™t khá»‘i Transformer duy nháº¥t cho mÃ´
hÃ¬nh OPT-13B; FFN khÃ´ng Ä‘Æ°á»£c hiá»ƒn thá»‹ trong hÃ¬nh cho
cÃ¡c sÆ¡ Ä‘á»“ khÃ¡c ngoÃ i Ideal vÃ¬ nÃ³ Ä‘Æ°á»£c overlap hoÃ n toÃ n
vá»›i thá»i gian chuyá»ƒn dá»¯ liá»‡u. Ideal lÃ  ká»‹ch báº£n mÃ  táº¥t cáº£
cÃ¡c tÃ­nh toÃ¡n (tá»©c lÃ  attention vÃ  FFN) Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn
GPU mÃ  khÃ´ng cÃ³ báº¥t ká»³ chuyá»ƒn dá»¯ liá»‡u nÃ o giá»¯a CPU
vÃ  GPU. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong káº¿t quáº£, bottleneck hiá»‡u
suáº¥t chÃ­nh cá»§a FlexGen vÃ  H2O lÃ  overhead chuyá»ƒn dá»¯
liá»‡u, chiáº¿m 96.9% vÃ  91.8% thá»i gian thá»±c thi, tÆ°Æ¡ng á»©ng.
Äá»‘i vá»›i INT4, do overhead quantization vÃ  dequantization,
tÃ­nh toÃ¡n attention cÅ©ng chiáº¿m má»™t pháº§n lá»›n thá»i gian thá»±c
thi ngoÃ i viá»‡c chuyá»ƒn dá»¯ liá»‡u. Máº·t khÃ¡c, InfiniGen cáº£i
thiá»‡n Ä‘Ã¡ng ká»ƒ tá»‘c Ä‘á»™ suy luáº­n so vá»›i FlexGen báº±ng cÃ¡ch
giáº£m lÆ°á»£ng chuyá»ƒn dá»¯ liá»‡u vá»›i prefetching KV cache Ä‘á»™ng
cá»§a chÃºng tÃ´i. HÆ¡n ná»¯a, InfiniGen chá»‰ cháº­m hÆ¡n 1.52Ã—
so vá»›i Ideal, trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c cho tháº¥y
slowdown 3.90Ã—-18.55Ã—.

TiÃªu thá»¥ bá»™ nhá»›. InfiniGen sá»­ dá»¥ng trá»ng sá»‘ query má»™t
pháº§n vÃ  key cache Ä‘á»ƒ dá»± Ä‘oÃ¡n. Äá»‘i vá»›i tá»· lá»‡ 0.3, kÃ­ch thÆ°á»›c
cá»§a trá»ng sá»‘ query má»™t pháº§n vÃ  key cache chá»‰ lÃ  2.5% vÃ 
15% tá»•ng tham sá»‘ mÃ´ hÃ¬nh vÃ  tá»•ng KV cache, tÆ°Æ¡ng á»©ng.
Máº·c dÃ¹ chÃºng tÃ´i Ä‘Æ¡n giáº£n lÆ°u trá»¯ chÃºng trong GPU trong
cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i cÃ³ thá»ƒ quáº£n lÃ½
overhead lÆ°u trá»¯ theo nhiá»u cÃ¡ch tá»‘i Æ°u khÃ¡c nhau náº¿u cáº§n.
VÃ­ dá»¥, chÃºng tÃ´i cÃ³ thá»ƒ chá»‰ lÆ°u trá»¯ cÃ¡c chá»‰ sá»‘ cá»™t cá»§a trá»ng
sá»‘ query má»™t pháº§n vÃ  truy xuáº¥t cÃ¡c vector cá»™t tá»« ma tráº­n
trá»ng sá»‘ query Ä‘áº§y Ä‘á»§ (Ä‘Ã£ náº±m trong GPU) khi cáº§n cho
phÃ©p chiáº¿u query má»™t pháº§n. NgoÃ i ra, chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº·t
key cache má»™t pháº§n trong CPU vÃ  thá»±c hiá»‡n dá»± Ä‘oÃ¡n trÃªn
CPU sau

(b) Sequence Length(a) Relative KV Cache Size (%)4812
2048409681921638432768PerplexityFull CacheInfiniGenH2O4812
0102030PerplexityFull CacheQuantizationInfiniGenH2OHÃ¬nh 19: Perplexity cá»§a Llama-2-7B-32K qua (a) kÃ­ch thÆ°á»›c KV
cache tÆ°Æ¡ng Ä‘á»‘i vá»›i Ä‘á»™ dÃ i chuá»—i 32768 vÃ  (b) Ä‘á»™ dÃ i chuá»—i trong
khi giá»¯ láº¡i 64 token. Tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n. Llama-2-7B-32K lÃ  phiÃªn
báº£n fine-tuned cÃ³ kháº£ nÄƒng xá»­ lÃ½ lÃªn Ä‘áº¿n 32K token báº±ng cÃ¡ch sá»­
dá»¥ng position interpolation [12]. Quantization Ä‘Æ°á»£c bá» qua trong
(b) vÃ¬ KV cache khÃ´ng thá»ƒ Ä‘Æ°á»£c nÃ©n dÆ°á»›i 6.25% (tá»©c lÃ  1 bit).

khi fetch query má»™t pháº§n tá»« GPU. Ngay cáº£ phÆ°Æ¡ng phÃ¡p
ngÃ¢y thÆ¡ lÃ  giáº£m tá»· lá»‡ má»™t pháº§n cÃ³ thá»ƒ váº«n cung cáº¥p Ä‘á»™
chÃ­nh xÃ¡c tá»‘t hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c trong khi
giáº£m overhead lÆ°u trá»¯. TÃ³m láº¡i, báº±ng cÃ¡ch hy sinh tá»‘i thiá»ƒu
hiá»‡u suáº¥t suy luáº­n, chÃºng tÃ´i cÃ³ thá»ƒ giáº£m ráº¥t nhiá»u overhead
lÆ°u trá»¯ trÃªn GPU náº¿u cáº§n thiáº¿t.

6.3 Cá»­a sá»• ngá»¯ cáº£nh dÃ i
HÃ¬nh 19 cho tháº¥y perplexity cá»§a mÃ´ hÃ¬nh Llama-2-7B-32K,
cÃ³ thá»ƒ xá»­ lÃ½ lÃªn Ä‘áº¿n 32K token, qua kÃ­ch thÆ°á»›c cache tÆ°Æ¡ng
Ä‘á»‘i vÃ  Ä‘á»™ dÃ i chuá»—i. ChÃºng tÃ´i sá»­ dá»¥ng táº­p dá»¯ liá»‡u WikiText-2
cho thÃ­ nghiá»‡m. Khi kÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh tÄƒng cho
cÃ¡c LLM tÆ°Æ¡ng lai, pháº§n tÆ°Æ¡ng Ä‘á»‘i cá»§a KV cache mÃ  GPU
cÃ³ thá»ƒ giá»¯ láº¡i sáº½ giáº£m do dung lÆ°á»£ng háº¡n cháº¿ cá»§a bá»™ nhá»› GPU.

HÃ¬nh 19(a) cho tháº¥y InfiniGen duy trÃ¬ má»©c perplexity gáº§n
vá»›i baseline full-cache ngay cáº£ khi kÃ­ch thÆ°á»›c KV cache
tÆ°Æ¡ng Ä‘á»‘i giáº£m, mÃ  khÃ´ng dáº«n Ä‘áº¿n tÄƒng perplexity Ä‘Ã¡ng
chÃº Ã½ ngay cáº£ vá»›i kÃ­ch thÆ°á»›c cache nhá» hÆ¡n nhiá»u. NgÆ°á»£c
láº¡i, cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c tÄƒng perplexity so vá»›i baseline
full-cache vÃ  phÃ¢n ká»³ Ä‘Ã¡ng ká»ƒ táº¡i má»™t sá»‘ kÃ­ch thÆ°á»›c nháº¥t
Ä‘á»‹nh do Ä‘á»™ rá»™ng bit khÃ´ng Ä‘á»§ Ä‘á»ƒ báº£o toÃ n thÃ´ng tin Ä‘áº§y Ä‘á»§
trÃªn táº¥t cáº£ key vÃ  value (Quantization) hoáº·c viá»‡c loáº¡i bá»
vÄ©nh viá»…n cÃ¡c má»¥c KV cache (H2O). NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹
trong HÃ¬nh 19(b), khoáº£ng cÃ¡ch perplexity giá»¯a InfiniGen
vÃ  H2O má»Ÿ rá»™ng cho Ä‘á»™ dÃ i chuá»—i dÃ i hÆ¡n, cÃ³ kháº£ nÄƒng
tÄƒng thÃªm cho Ä‘á»™ dÃ i chuá»—i vÆ°á»£t quÃ¡ 32K. Äiá»u nÃ y ngá»¥
Ã½ ráº±ng InfiniGen cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c chuá»—i dÃ i hÆ¡n vÃ 
báº£o toÃ n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh tá»‘t hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng
phÃ¡p khÃ¡c.

ChÃºng tÃ´i tiáº¿p tá»¥c dá»± Ä‘oÃ¡n vá» cÃ¡ch InfiniGen sáº½ cÃ³ lá»£i trong
ká»· nguyÃªn cá»­a sá»• ngá»¯ cáº£nh triá»‡u token báº±ng cÃ¡ch phÃ¢n tÃ­ch
má»™t mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng xá»­ lÃ½ 1 triá»‡u token. HÃ¬nh 20(a)
cho tháº¥y ráº±ng tá»· lá»‡ pháº§n trÄƒm query token chÃº Ã½ Ä‘áº¿n Ã­t hÆ¡n
1% key token tÄƒng khi Ä‘á»™ dÃ i chuá»—i trá»Ÿ nÃªn dÃ i hÆ¡n. InfiniGen
cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i xu hÆ°á»›ng thay Ä‘á»•i nÃ y báº±ng cÃ¡ch Ä‘á»™ng
Ä‘iá»u chá»‰nh lÆ°á»£ng KV cache cáº§n load, trong khi cÃ¡c cÃ¡ch
tiáº¿p cáº­n fixed-budget/pruning trÆ°á»›c Ä‘Ã¢y sáº½ khÃ´ng dá»… dÃ ng
Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c KV cache hiá»‡u quáº£. HÃ¬nh 20(b) tiáº¿p
tá»¥c cho tháº¥y ráº±ng attention weight cá»§a key token cÃ³ thá»ƒ
thay Ä‘á»•i qua cÃ¡c iteration;
13

--- TRANG 14 ---
020406080100
2K16K128K1MPercentage (%)Layer 0Layer 12Layer 24Layer 30(a) Sequence LengthLayer 18, Head 30Attention Weight08K16K4K12K(b) IterationLayer 30, Head 300.00.8
0.00.2Layer 30, Head 180.00.2HÃ¬nh 20: PhÃ¢n tÃ­ch 1 triá»‡u token sá»­ dá»¥ng Llama-3-8B-1048K.
(a) Tá»· lá»‡ pháº§n trÄƒm query token chÃº Ã½ Ä‘áº¿n Ã­t hÆ¡n 1% key token
qua cÃ¡c Ä‘á»™ dÃ i chuá»—i. (b) Attention weight cá»§a cÃ¡c key token Ä‘Æ°á»£c
láº¥y máº«u tá»« cÃ¡c layer vÃ  head khÃ¡c nhau qua 16K iteration cuá»‘i cÃ¹ng
trong tá»•ng sá»‘ 1 triá»‡u token.

cÃ¡c key token Ä‘Æ°á»£c láº¥y máº«u cho tháº¥y nhá»¯ng Ä‘á»‰nh Ä‘á»™t ngá»™t
sau hÃ ng nghÃ¬n iteration vá»›i attention weight tháº¥p Ä‘Ã¡ng ká»ƒ
(vÃ­ dá»¥: iteration thá»© 7425 trong 16K iteration cuá»‘i cÃ¹ng á»Ÿ
Layer 18, Head 30). ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng cÃ¡c cÃ¡ch
tiáº¿p cáº­n trÆ°á»›c Ä‘Ã¢y loáº¡i bá» vÄ©nh viá»…n token trong khi chÃºng
khÃ´ng quan trá»ng cÃ³ thá»ƒ máº¥t ngá»¯ cáº£nh quan trá»ng náº¿u
chÃºng trá»Ÿ nÃªn quan trá»ng trá»Ÿ láº¡i á»Ÿ cÃ¡c iteration sau. NgÆ°á»£c
láº¡i, InfiniGen cÃ³ thá»ƒ báº£o toÃ n hiá»‡u suáº¥t mÃ´ hÃ¬nh báº±ng cÃ¡ch
giá»¯ cÃ¡c má»¥c KV táº¡m thá»i khÃ´ng quan trá»ng cho viá»‡c sá»­
dá»¥ng tiá»m nÄƒng trong tÆ°Æ¡ng lai.

7 CÃ´ng trÃ¬nh liÃªn quan
Há»‡ thá»‘ng phá»¥c vá»¥ DNN. CÃ¡ch tiáº¿p cáº­n cÃ³ há»‡ thá»‘ng Ä‘á»ƒ cho
phÃ©p há»‡ thá»‘ng phá»¥c vá»¥ mÃ´ hÃ¬nh hiá»‡u quáº£ vÃ  nhanh chÃ³ng
lÃ  má»™t chá»§ Ä‘á» quan trá»ng Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u rá»™ng rÃ£i bá»Ÿi
cáº£ há»c thuáº­t vÃ  cÃ´ng nghiá»‡p. Má»™t sá»‘ cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y
táº­p trung vÃ o cÃ¡c há»‡ thá»‘ng phÃ¢n tÃ¡n vá»›i Ä‘á»™ trá»… cÃ³ thá»ƒ dá»±
Ä‘oÃ¡n cho cÃ¡c má»¥c tiÃªu cáº¥p dá»‹ch vá»¥ (SLO) [15, 16, 25, 56].
CÃ¡c cÃ´ng trÃ¬nh khÃ¡c cáº£i thiá»‡n tÃ­nh song song vÃ  thÃ´ng lÆ°á»£ng
cá»§a há»‡ thá»‘ng thÃ´ng qua preemption [28, 75], batching tinh
chá»‰nh [17, 21, 71], hoáº·c tá»‘i Æ°u hÃ³a bá»™ nhá»› [18, 35, 58].
Má»™t sá»‘ cÃ´ng trÃ¬nh khÃ¡c nháº±m Ä‘áº¡t Ä‘Æ°á»£c thá»±c thi thÃ´ng lÆ°á»£ng
cao vá»›i bá»™ nhá»› GPU háº¡n cháº¿ báº±ng cÃ¡ch offload tham sá»‘
vÃ o bá»™ lÆ°u trá»¯ thá»© cáº¥p (vÃ­ dá»¥: bá»™ nhá»› CPU vÃ  Ä‘Ä©a). Má»™t
sá»‘ trong sá»‘ Ä‘Ã³ Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn CUDA Unified Memory
[46] vá»›i prefetching [31, 39], trong khi cÃ¡c cÃ´ng trÃ¬nh khÃ¡c
di chuyá»ƒn tensor rÃµ rÃ ng vÃ o vÃ  ra khi cáº§n cho tÃ­nh toÃ¡n
[29, 30, 48, 72, 73]. FlexGen [57] lÃ  má»™t há»‡ thá»‘ng phá»¥c vá»¥
LLM gáº§n Ä‘Ã¢y cho phÃ©p suy luáº­n thÃ´ng lÆ°á»£ng cao trÃªn má»™t
GPU duy nháº¥t báº±ng cÃ¡ch offload trá»ng sá»‘ vÃ  KV cache vÃ o
bá»™ nhá»› CPU vÃ  Ä‘Ä©a. InfiniGen trá»±c giao vá»›i FlexGen vÃ  cÃ³
thá»ƒ hoáº¡t Ä‘á»™ng káº¿t há»£p vá»›i nÃ³ Ä‘á»ƒ hiá»‡u quáº£ offload vÃ  prefetch
KV cache.

Quáº£n lÃ½ KV Cache. vLLM [35] giáº£m thiá»ƒu lÃ£ng phÃ­ bá»™ nhá»›
KV cache tá»« phÃ¢n máº£nh vÃ  trÃ¹ng láº·p. StreamingLLM [67]
cho phÃ©p LLM táº¡o ra Ä‘á»™ dÃ i chuá»—i dÃ i hÆ¡n so vá»›i nhá»¯ng
gÃ¬ Ä‘Æ°á»£c huáº¥n luyá»‡n. Tuy nhiÃªn, vÃ¬ cáº£ vLLM vÃ  StreamingLLM
Ä‘á»u khÃ´ng giáº£m kÃ­ch thÆ°á»›c KV cache, viá»‡c chuyá»ƒn dá»¯ liá»‡u
váº«n gÃ¢y ra overhead Ä‘Ã¡ng ká»ƒ trong cÃ¡c há»‡ thá»‘ng suy luáº­n
dá»±a trÃªn offloading. InfiniGen bá»• sung quáº£n lÃ½ KV cache
Ä‘á»ƒ giáº£m overhead chuyá»ƒn dá»¯ liá»‡u, Ä‘Ã¢y lÃ  bottleneck chÃ­nh
trong cÃ¡c há»‡ thá»‘ng dá»±a trÃªn offloading.

Suy luáº­n LLM hiá»‡u quáº£. CÃ³ cÃ¡c dÃ²ng nghiÃªn cá»©u khai thÃ¡c
quantization hoáº·c sparsity Ä‘á»ƒ lÃ m cho LLM hiá»‡u quáº£ thÃ´ng
qua cÃ¡c phÆ°Æ¡ng phÃ¡p thuáº­t toÃ¡n [13, 19, 22, 34, 66] hoáº·c
thiáº¿t káº¿ Ä‘á»“ng pháº§n cá»©ng-pháº§n má»m [26,27,36,50]. Vá» sparsity,
háº§u háº¿t cÃ¡c cÃ´ng trÃ¬nh dá»±a trÃªn thuáº­t toÃ¡n táº­p trung vÃ o viá»‡c
giáº£m kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh báº±ng cÃ¡ch khai thÃ¡c sparsity cá»§a
trá»ng sá»‘. Thay vÃ o Ä‘Ã³, H2O vÃ  Sparse Transformer [13] táº­n
dá»¥ng sparsity theo hÃ ng (tá»©c lÃ  cáº¥p token) trong KV cache
báº±ng cÃ¡ch loáº¡i bá» vÄ©nh viá»…n má»™t sá»‘ má»¥c KV nháº¥t Ä‘á»‹nh. Máº·t
khÃ¡c, háº§u háº¿t cÃ¡c nghiÃªn cá»©u thiáº¿t káº¿ Ä‘á»“ng pháº§n cá»©ng-pháº§n
má»m táº­p trung vÃ o viá»‡c giáº£m bá»›t Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n báº­c
hai trong giai Ä‘oáº¡n prefill báº±ng cÃ¡ch bá» qua cÃ¡c key token
khÃ´ng cáº§n thiáº¿t vá»›i sá»± há»— trá»£ cá»§a pháº§n cá»©ng chuyÃªn dá»¥ng.
Tuy nhiÃªn, chÃºng thÆ°á»ng khÃ´ng giáº£m truy cáº­p bá»™ nhá»› vÃ¬
chÃºng xÃ¡c Ä‘á»‹nh cÃ¡c key token quan trá»ng chá»‰ sau khi quÃ©t
táº¥t cáº£ cÃ¡c pháº§n tá»­ cá»§a tensor key.

Kernel fusion [18,32] lÃ  má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c Ä‘á»ƒ giáº£m
thiá»ƒu overhead bá»™ nhá»› báº­c hai cá»§a attention trong giai Ä‘oáº¡n
prefill. InfiniGen cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai vá»›i cÃ¡c ká»¹ thuáº­t
kernel fusion Ä‘á»ƒ giáº£m bá»›t overhead truy cáº­p KV cache trong
giai Ä‘oáº¡n decoding. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, Ä‘Ã¢y lÃ 
cÃ´ng trÃ¬nh Ä‘áº§u tiÃªn cho phÃ©p suy luáº­n LLM hiá»‡u quáº£ báº±ng
cÃ¡ch chá»‰ prefetch cÃ¡c má»¥c KV cáº§n thiáº¿t trong cÃ¡c há»‡ thá»‘ng
suy luáº­n dá»±a trÃªn offloading.

8 Káº¿t luáº­n
KÃ­ch thÆ°á»›c cá»§a KV cache Ä‘áº·t ra váº¥n Ä‘á» kháº£ nÄƒng má»Ÿ rá»™ng
trong cÃ¡c há»‡ thá»‘ng suy luáº­n dá»±a trÃªn offloading thÃ´ng lÆ°á»£ng
cao, tháº­m chÃ­ vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c tham sá»‘ mÃ´ hÃ¬nh. CÃ¡c
chÃ­nh sÃ¡ch loáº¡i bá» KV cache hiá»‡n cÃ³ cho tháº¥y sá»± giáº£m Ä‘á»™
chÃ­nh xÃ¡c lá»›n vÃ  khÃ´ng sá»­ dá»¥ng hiá»‡u quáº£ bÄƒng thÃ´ng káº¿t
ná»‘i khi chÃºng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng LLM dá»±a
trÃªn offloading. ChÃºng tÃ´i Ä‘á» xuáº¥t InfiniGen, má»™t khung
quáº£n lÃ½ KV cache Ä‘á»™ng dá»±a trÃªn offloading thá»±c thi hiá»‡u
quáº£ suy luáº­n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. InfiniGen khai
thÃ¡c Ä‘áº§u vÃ o attention cá»§a layer trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ dá»± Ä‘oÃ¡n prefetch
KV cache cá»§a cÃ¡c token quan trá»ng. ChÃºng tÃ´i thao tÃ¡c cÃ¡c
trá»ng sá»‘ query vÃ  key Ä‘á»ƒ lÃ m cho viá»‡c dá»± Ä‘oÃ¡n hiá»‡u quáº£
hÆ¡n. InfiniGen cho tháº¥y Ä‘á»™ trá»… suy luáº­n ngáº¯n hÆ¡n Ä‘Ã¡ng ká»ƒ
trong khi báº£o toÃ n hiá»‡u suáº¥t mÃ´ hÃ¬nh ngÃ´n ngá»¯. NÃ³ cÅ©ng
cho tháº¥y kháº£ nÄƒng má»Ÿ rá»™ng tá»‘t hÆ¡n nhiá»u vá» kÃ­ch thÆ°á»›c
batch, Ä‘á»™ dÃ i chuá»—i, vÃ  kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh so vá»›i cÃ¡c giáº£i
phÃ¡p trÆ°á»›c Ä‘Ã¢y.

Lá»i cáº£m Æ¡n
ChÃºng tÃ´i muá»‘n cáº£m Æ¡n cÃ¡c nhÃ  Ä‘Ã¡nh giÃ¡ áº©n danh vÃ  ngÆ°á»i
dáº«n Ä‘Æ°á»ng Petros Maniatis vÃ¬ nhá»¯ng pháº£n há»“i cÃ³ giÃ¡ trá»‹
cá»§a há». CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi tÃ i trá»£
nghiÃªn cá»©u tá»« Samsung Advanced Institute of Technology
(SAIT) vÃ  bá»Ÿi chÆ°Æ¡ng trÃ¬nh há»— trá»£ bÃ¡n dáº«n trÃ­ tuá»‡ nhÃ¢n táº¡o
Ä‘á»ƒ nuÃ´i dÆ°á»¡ng nhá»¯ng tÃ i nÄƒng tá»‘t nháº¥t (Sá»‘ RS-2023-00256081)
Ä‘Æ°á»£c giÃ¡m sÃ¡t bá»Ÿi Institute for Information & Communications
Technology Planning & Evaluation (IITP). Institute of
Engineering Research táº¡i Seoul National University Ä‘Ã£
cung cáº¥p cÆ¡ sá»Ÿ nghiÃªn cá»©u cho cÃ´ng trÃ¬nh nÃ y. Jaewoong
Sim lÃ  tÃ¡c giáº£ liÃªn há»‡.
14

--- TRANG 15 ---
TÃ i liá»‡u tham kháº£o
[1] DeepL. https://www.deepl.com/translator .
[2] Memcached. https://memcached.org .
[3]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023.
[4]Tyler Allen and Rong Ge. In-depth analyses of unified
virtual memory system for gpu accelerated computing.
InInternational Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC) , 2021.
[5]Reza Yazdani Aminabadi, Samyam Rajbhandari, Am-
mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,
Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff
Rasley, et al. Deepspeed-inference: enabling efficient
inference of transformer models at unprecedented scale.
InInternational Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC) , 2022.
[6]Anthropic. The claude 3 model family: Opus, sonnet,
haiku. 2024. https://www.anthropic.com/claude .
[7]Benjamin Berg, Daniel S. Berger, Sara McAllister, Isaac
Grosof, Sathya Gunasekar, Jimmy Lu, Michael Uhlar,
Jim Carrig, Nathan Beckmann, Mor Harchol-Balter, and
Gregory R. Ganger. The CacheLib caching engine:
Design and experiences at scale. In Proceedings of the
USENIX Symposium on Operating Systems Design and
Implementation (OSDI) , 2020.
[8]Yonatan Bisk, Rowan Zellers, Ronan bras, Jianfeng
Gao, and Choi Yejin. Piqa: Reasoning about physical
commonsense in natural language. In Proceedings of
the AAAI Conference on Artificial Intelligence (AAAI) ,
2020.
[9]Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri
Rudra, and Christopher RÃ©. Scatterbrain: Unifying
sparse and low-rank attention. In Advances in Neural
Information Processing Systems (NeurIPS) , 2021.
[10] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu,
Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali
Shrivastava, and Christopher Re. Mongoose: A learn-
able lsh framework for efficient neural network train-
ing. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2021.
[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
et al. Evaluating large language models trained on code.
arXiv preprint arXiv:2107.03374 , 2021.[12] Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. Extending context window of large
language models via positional interpolation. arXiv
preprint arXiv:2306.15595 , 2023.
[13] Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. Generating long sequences with sparse trans-
formers. arXiv preprint arXiv:1904.10509 , 2019.
[14] Krzysztof Marcin Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohi-
uddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J
Colwell, and Adrian Weller. Rethinking attention with
performers. In Proceedings of the International Confer-
ence on Learning Representations (ICLR) , 2021.
[15] Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey
Zumar, Ion Stoica, Joseph Gonzalez, and Alexey Tu-
manov. Inferline: latency-aware provisioning and scal-
ing for prediction serving pipelines. In Proceedings
of the ACM Symposium on Cloud Computing (SoCC) ,
2020.
[16] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J
Franklin, Joseph E Gonzalez, and Ion Stoica. Clipper:
A low-latency online prediction serving system. In
Proceedings of the USENIX Symposium on Networked
Systems Design and Implementation (NSDI) , 2017.
[17] Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui
Li, Deze Zeng, Chao Li, and Minyi Guo. Dvabatch:
Diversity-aware multi-entry multi-exit batching for effi-
cient processing of dnn services on gpus. In Proceedings
of the USENIX Annual Technical Conference (USENIX
ATC) , 2022.
[18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher RÃ©. Flashattention: Fast and memory-
efficient exact attention with io-awareness. In Advances
in Neural Information Processing Systems (NeurIPS) ,
2022.
[19] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. Gpt3.int8(): 8-bit matrix multiplication for
transformers at scale. In Advances in Neural Informa-
tion Processing Systems (NeurIPS) , 2022.
[20] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchi-
cal neural story generation. In Annual Meeting of the
Association for Computational Linguistics (ACL) , 2018.
[21] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.
Turbotransformers: an efficient gpu serving system for
transformer models. In Proceedings of the Symposium
on Principles and Practice of Parallel Programming
(PPoPP) , 2021.
15

--- TRANG 16 ---
[22] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
language models can be accurately pruned in one-shot.
InProceedings of the International Conference on Ma-
chine Learning (ICML) , 2023.
[23] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,
Anthony DiPofi, Charles Foster, Laurence Golding, Jef-
frey Hsu, Kyle McDonell, Niklas Muennighoff, et al.
A framework for few-shot language model evaluation,
2021.
[24] GitHub. Copilot. https://github.com/features/
copilot .
[25] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao,
Antoine Kaufmann, Ymir Vigfusson, and Jonathan
Mace. Serving dnns like clockwork: Performance pre-
dictability from the bottom up. In Proceedings of the
USENIX Symposium on Operating Systems Design and
Implementation (OSDI) , 2020.
[26] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng,
Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and
Yuhao Zhu. Olive: Accelerating large language models
via hardware-friendly outlier-victim pair quantization.
InProceedings of the International Symposium on Com-
puter Architecture (ISCA) , 2023.
[27] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung
Kim, Hyunji Choi, Sung Jun Jung, and Jae W Lee. Elsa:
Hardware-software co-design for efficient, lightweight
self-attention mechanism in neural networks. In Pro-
ceedings of the International Symposium on Computer
Architecture (ISCA) , 2021.
[28] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo
Chen. Microsecond-scale preemption for concurrent
gpu-accelerated dnn inferences. In Proceedings of the
USENIX Symposium on Operating Systems Design and
Implementation (OSDI) , 2022.
[29] Mark Hildebrand, Jawad Khan, Sanjeev Trika, Jason
Lowe-Power, and Venkatesh Akella. Autotm: Automatic
tensor movement in heterogeneous memory systems
using integer linear programming. In Proceedings of
the International Conference on Architectural Support
for Programming Languages and Operating Systems
(ASPLOS) , 2020.
[30] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvi-
sor: Pushing deep learning beyond the gpu memory limit
via smart swapping. In Proceedings of the International
Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS) , 2020.
[31] Jaehoon Jung, Jinpyo Kim, and Jaejin Lee. Deepum:
Tensor migration and prefetching in unified memory.InProceedings of the International Conference on Ar-
chitectural Support for Programming Languages and
Operating Systems (ASPLOS) , 2023.
[32] Sheng-Chun Kao, Suvinay Subramanian, Gaurav
Agrawal, Amir Yazdanbakhsh, and Tushar Krishna.
Flat: An optimized dataflow for mitigating attention
bottlenecks. In Proceedings of the International
Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS) , 2023.
[33] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-
former: The efficient transformer. In Proceedings of the
International Conference on Learning Representations
(ICLR) , 2020.
[34] Woosuk Kwon, Sehoon Kim, Michael W Mahoney,
Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A
fast post-training pruning framework for transformers.
InAdvances in Neural Information Processing Systems
(NeurIPS) , 2022.
[35] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,
Hao Zhang, and Ion Stoica. Efficient memory manage-
ment for large language model serving with pagedatten-
tion. In Proceedings of the Symposium on Operating
Systems Principles (SOSP) , 2023.
[36] Jungi Lee, Wonbeom Lee, and Jaewoong Sim. Tender:
Accelerating large language models via tensor decom-
position and runtime requantization. In Proceedings of
the International Symposium on Computer Architecture
(ISCA) , 2024.
[37] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao
Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis,
and Anshumali Shrivastava. Scissorhands: Exploiting
the persistence of importance hypothesis for llm kv
cache compression at test time. In Advances in Neural
Information Processing Systems (NeurIPS) , 2023.
[38] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. The penn treebank: Anno-
tating predicate argument structure. In Proceedings of
the Workshop on Human Language Technology , 1994.
[39] Pak Markthub, Mehmet E. Belviranli, Seyong Lee, Jef-
frey S. Vetter, and Satoshi Matsuoka. Dragon: Breaking
gpu memory capacity limits with direct nvm access. In
International Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC) , 2018.
[40] Pierre-Emmanuel Mazare, Samuel Humeau, Martin Rai-
son, and Antoine Bordes. Training millions of person-
alized dialogue agents. In Conference on Empirical
16

--- TRANG 17 ---
Methods in Natural Language Processing (EMNLP) ,
2018.
[41] Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. Pointer sentinel mixture models. In Pro-
ceedings of the International Conference on Learning
Representations (ICLR) , 2017.
[42] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. Can a suit of armor conduct electricity?
a new dataset for open book question answering. In
Conference on Empirical Methods in Natural Language
Processing (EMNLP) , 2018.
[43] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,
Huan Wang, Yingbo Zhou, Silvio Savarese, and Caim-
ing Xiong. Codegen: An open large language model
for code with multi-turn program synthesis. In Pro-
ceedings of the International Conference on Learning
Representations (ICLR) , 2023.
[44] NVIDIA. NVIDIA RTX A6000 Graphics
Card. https://www.nvidia.com/en-us/
design-visualization/rtx-a6000/ .
[45] NVIDIA. Triton inference server.
https://developer.nvidia.com/
triton-inference-server .
[46] Nvidia. Unified memory program-
ming. https://docs.nvidia.com/cuda/
cuda-c-programming-guide/index.html#
um-unified-memory-programming-hd .
[47] Christopher Olston, Noah Fiedel, Kiril Gorovoy,
Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu
Rajashekhar, Sukriti Ramesh, and Jordan Soyke.
Tensorflow-serving: Flexible, high-performance ml
serving. In Advances in Neural Information Processing
Systems (NeurIPS) , 2017.
[48] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang
Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin:
Tensor-based gpu memory management for deep learn-
ing. In Proceedings of the International Conference on
Architectural Support for Programming Languages and
Operating Systems (ASPLOS) , 2020.
[49] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery,
Jacob Devlin, James Bradbury, Jonathan Heek, Kefan
Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scal-
ing transformer inference. In Proceedings of the Ma-
chine Learning and Systems (MLSys) , 2023.
[50] Zheng Qu, Liu Liu, Fengbin Tu, Zhaodong Chen, Yufei
Ding, and Yuan Xie. Dota: detect and omit weak atten-
tions for scalable transformer acceleration. In Proceed-
ings of the International Conference on ArchitecturalSupport for Programming Languages and Operating
Systems (ASPLOS) , 2022.
[51] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by
generative pre-training. 2018.
[52] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
Chloe Hillier, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. In Pro-
ceedings of the International Conference on Learning
Representations (ICLR) , 2020.
[53] Machel Reid, Nikolay Savinov, Denis Teplyashin,
Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-
rat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking
multimodal understanding across millions of tokens of
context. arXiv preprint arXiv:2403.05530 , 2024.
[54] Melissa Roemmele, Cosmin Bejan, and Andrew Gordon.
Choice of plausible alternatives: An evaluation of com-
monsense causal reasoning. In AAAI Spring Symposium
Series , 2011.
[55] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
vatula, and Yejin Choi. Winogrande: An adversarial
winograd schema challenge at scale. Communications
of the ACM , 2021.
[56] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao,
Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy,
and Ravi Sundaram. Nexus: A gpu cluster engine for
accelerating dnn-based video analysis. In Proceed-
ings of the Symposium on Operating Systems Principles
(SOSP) , 2019.
[57] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan
Li, Max Ryabinin, Beidi Chen, Percy Liang, Christo-
pher Re, Ion Stoica, and Ce Zhang. FlexGen: High-
throughput generative inference of large language mod-
els with a single gpu. In Proceedings of International
Conference on Machine Learning (ICML) , 2023.
[58] Yining Shi, Zhi Yang, Jilong Xue, Lingxiao Ma, Yuqing
Xia, Ziming Miao, Yuxiao Guo, Fan Yang, and Lidong
Zhou. Welder: Scheduling deep learning memory access
via tile-graph. In Proceedings of the USENIX Sympo-
sium on Operating Systems Design and Implementation
(OSDI) , 2023.
[59] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence
to sequence learning with neural networks. In Advances
in Neural Information Processing Systems (NeurIPS) ,
2014.
17

--- TRANG 18 ---
[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023.
[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In
Advances in Neural Information Processing Systems
(NeurIPS) , 2017.
[62] Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. Glue: A multi-
task benchmark and analysis platform for natural lan-
guage understanding. In EMNLP Workshop Black-
boxNLP , 2018.
[63] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,
and Hao Ma. Linformer: Self-attention with linear com-
plexity. arXiv preprint arXiv:2006.04768 , 2020.
[64] Yiming Wang, Zhuosheng Zhang, and Rui Wang.
Element-aware summarization with large language mod-
els: Expert-aligned evaluation and chain-of-thought
method. In Annual Meeting of the Association for Com-
putational Linguistics (ACL) , 2023.
[65] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao
Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and
Xianglong Liu. Outlier suppression: Pushing the limit
of low-bit transformer language models. In Advances
in Neural Information Processing Systems (NeurIPS) ,
2022.
[66] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,
Julien Demouth, and Song Han. Smoothquant: Accu-
rate and efficient post-training quantization for large
language models. In International Conference on Ma-
chine Learning (ICML) , 2023.
[67] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. Efficient streaming language
models with attention sinks. In Proceedings of the In-
ternational Conference on Learning Representations
(ICLR) , 2024.
[68] Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-
san Awadalla. A paradigm shift in machine transla-
tion: Boosting translation performance of large language
models. In Proceedings of the International Conference
on Learning Representations (ICLR) , 2024.
[69] Juncheng Yang, Yazhuo Zhang, Ziyue Qiu, Yao Yue,
and Rashmi Vinayak. Fifo queues are all you need forcache eviction. In Proceedings of the Symposium on
Operating Systems Principles (SOSP) , 2023.
[70] Tzu-Wei Yang, Seth Pollen, Mustafa Uysal, Arif Mer-
chant, and Homer Wolfmeister. CacheSack: Admission
optimization for google datacenter flash caches. In Pro-
ceedings of the USENIX Annual Technical Conference
(USENIX ATC) , 2022.
[71] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-
jeong Kim, and Byung-Gon Chun. Orca: A distributed
serving system for Transformer-Based generative mod-
els. In Proceedings of the USENIX Symposium on Op-
erating Systems Design and Implementation (OSDI) ,
2022.
[72] Yuan Yu, MartÃ­n Abadi, Paul Barham, Eugene Brevdo,
Mike Burrows, Andy Davis, Jeff Dean, Sanjay Ghe-
mawat, Tim Harley, Peter Hawkins, et al. Dynamic
control flow in large-scale machine learning. In Proceed-
ings of the Thirteenth EuroSys Conference (EuroSys) ,
2018.
[73] Haoyang Zhang, Yirui Eric Zhou, Yu Xue, Yiqi Liu, and
Jian Huang. G10: Enabling an efficient unified gpu
memory and storage architecture with smart tensor mi-
grations. In Proceedings of the International Symposium
on Microarchitecture (MICRO) , 2023.
[74] Haoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang.
Pretraining-based natural language generation for text
summarization. In Conference on Computational Natu-
ral Language Learning (CoNLL) , 2019.
[75] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and
Ion Stoica. Shepherd: Serving dnns in the wild. In
Proceedings of the Symposium on Networked Systems
Design and Implementation (NSDI) , 2023.
[76] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. Personalizing
dialogue agents: I have a dog, do you have pets too? In
Annual Meeting of the Association for Computational
Linguistics (ACL) , 2018.
[77] Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt:
Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 , 2022.
[78] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong
Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong
Tian, Christopher RÃ©, Clark Barrett, et al. H 2O: Heavy-
hitter oracle for efficient generative inference of large
language models. In Advances in Neural Information
Processing Systems (NeurIPS) , 2023.
18
