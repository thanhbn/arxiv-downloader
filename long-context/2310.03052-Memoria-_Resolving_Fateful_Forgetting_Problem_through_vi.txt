# 2310.03052.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.03052.pdf
# Kích thước tệp: 7017312 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua
kiến trúc bộ nhớ lấy cảm hứng từ con người
Sangjun Park1JinYeong Bak1
Tóm tắt
Việc làm cho mạng neural nhớ trong dài hạn đã là
một vấn đề lâu dài. Mặc dù đã có một số kỹ thuật
bộ nhớ ngoài được giới thiệu, hầu hết tập trung vào
việc giữ lại thông tin gần đây trong ngắn hạn. Bất
kể tầm quan trọng của nó, thông tin có xu hướng bị
quên lãng một cách định mệnh theo thời gian. Chúng
tôi trình bày Memoria, một hệ thống bộ nhớ cho
mạng neural nhân tạo, lấy cảm hứng từ con người
và áp dụng các lý thuyết khoa học thần kinh và tâm
lý học khác nhau. Kết quả thực nghiệm chứng minh
hiệu quả của Memoria trong các nhiệm vụ đa dạng
về sắp xếp, mô hình hóa ngôn ngữ và phân loại,
vượt trội hơn các kỹ thuật thông thường. Phân tích
engram cho thấy Memoria thể hiện các hiệu ứng
ưu tiên, gần đây và tính liền kề thời gian là đặc
trưng của bộ nhớ con người.
1. Giới thiệu
Con người sở hữu khả năng đáng kinh ngạc trong việc giữ lại
ký ức trong thời gian dài. Con người trích xuất ý chính từ
dòng dữ liệu tràn ngập, lấy thông tin liên quan và dần dần
quên đi những ký ức vô dụng và không sử dụng. Những nỗ
lực nhằm ban cho mạng neural khả năng bộ nhớ dài hạn
giống con người đã được tiến hành liên tục. Mặc dù Transformers (Vaswani et al., 2017) đã cho thấy hiệu suất xuất sắc
trong nhiều nhiệm vụ đa dạng (Devlin et al., 2019; Radford
et al., 2018; Brown et al., 2020; Lewis et al., 2020a), chúng
cũng gặp khó khăn với các chuỗi dài do bản chất xử lý toàn
bộ token đầu vào đồng thời. Để giảm thiểu hạn chế này, các
phương pháp bộ nhớ ngoài đã được nghiên cứu. Tuy nhiên,
không giống như con người, hầu hết các phương pháp hiện
có ưu tiên việc bảo tồn thông tin mới hơn ký ức cũ và hoạt
động với dung lượng cố định. Do đó, điều này không thể
tránh khỏi dẫn đến việc loại bỏ hoặc pha loãng ký ức cũ.
Chúng tôi gọi vấn đề này là Quên lãng Định mệnh.
1Khoa Khoa học và Kỹ thuật Máy tính,
Đại học Sungkyunkwan, Suwon, Hàn Quốc. Liên hệ
với: JinYeong Bak <jy.bak@skku.edu>.
Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy,
Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 của
(các) tác giả.Việc giới thiệu dung lượng bộ nhớ động và sử dụng chính
sách ưu tiên thông tin quan trọng cho tương lai có thể giải
quyết vấn đề quên lãng định mệnh. Tuy nhiên, việc thực
hiện điều này đòi hỏi giải quyết các vấn đề phái sinh khác
nhau. Thứ nhất, cần phân biệt thông tin nào được coi là
quan trọng (Tầm quan trọng Dài hạn). Việc dự đoán tầm
quan trọng dài hạn tại thời điểm tiếp nhận ban đầu là thách
thức, vì việc xác định liệu nó có hữu ích trong tương lai
hay không phụ thuộc vào việc sử dụng trong tương lai, khiến
nó khó lường trước. Hơn nữa, vì chúng ta không thể lưu trữ
lượng thông tin vô hạn, việc quên là cần thiết. Cơ chế này
không nên chỉ đơn giản xóa thông tin cũ, mà đúng hơn là
bảo tồn và quên một cách có chọn lọc dựa trên tầm quan
trọng dài hạn của thông tin (Bảo tồn Có chọn lọc). Hơn
nữa, trong khi ký ức gần đây vốn dĩ bảo tồn một mức độ
liên quan nhất định với ngữ cảnh, ký ức dài hạn thì không
như vậy. Bởi vì ký ức dài hạn cách xa về mặt thời gian
so với tình huống hiện tại, nội dung của ký ức dài hạn được
lấy ra phải có liên quan đến tình huống hiện tại. Cuối cùng,
điều quan trọng là ký ức cũ được kích hoạt một cách có
chọn lọc dựa trên ngữ cảnh hiện tại (Kích hoạt dựa trên
Gợi ý). Vấn đề này bao gồm thử thách về cách tìm kiếm
ký ức liên quan trong lưu trữ dài hạn (Tìm kiếm Bộ nhớ).
May mắn thay, tất cả những vấn đề này đã là những thách
thức lâu dài mà hệ thống bộ nhớ của sinh vật sống phải
đối mặt. Con người sở hữu một hệ thống bộ nhớ rất tinh vi
không chỉ giữ lại thông tin gần đây mà còn có khả năng
nhớ những sự kiện quan trọng suốt cuộc đời (Atkinson &
Shiffrin, 1968; Craik & Lockhart, 1972; Nairne & Pandeirada, 2008; Waugh & Norman, 1965; Brown, 1958; Underwood & Postman, 1960). Những tiến bộ gần đây trong các
lĩnh vực AI và khoa học thần kinh đã mang lại sự chú ý
đến tầm quan trọng của nghiên cứu liên ngành giữa hai lĩnh
vực này (Hassabis et al., 2017; van de Ven et al., 2020).
Đặc biệt, trong lĩnh vực hệ thống bộ nhớ, con người cung
cấp các giải pháp gần như lý tưởng, thúc đẩy những nỗ lực
áp dụng những hiểu biết từ hệ thống bộ nhớ con người vào
mạng neural nhân tạo (Banino et al., 2020; Kim et al., 2023).
Theo xu hướng này, chúng tôi tiếp cận vấn đề quên lãng
định mệnh bằng cách tích hợp bằng chứng khoa học thần
kinh và các mô hình lý thuyết về bộ nhớ con người. Memoria
cung cấp một giải pháp sáng tạo cho quên lãng định mệnh,
mở đường cho việc ghi nhớ có chọn lọc và vĩnh viễn cho
mạng neural.
1arXiv:2310.03052v3 [cs.LG] 8 Jun 2024

--- TRANG 2 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Hình 1: Bộ nhớ làm việc giữ lại thông tin gần đây nhất. Bộ nhớ ngắn hạn cũng chứa một số lượng cố định các engram gần đây.
Số lượng engram trong bộ nhớ dài hạn không được xác định trước. Các mũi tên trong sơ đồ biểu thị các kết nối giữa mỗi engram.
Mỗi kết nối có hướng và có trọng số. Những cạnh có trọng số này được sử dụng để truy xuất.
Đóng góp
1. Chúng tôi đã thiết kế Memoria như một khung bộ nhớ
ngoài cho mạng neural, kết hợp các lý thuyết khác nhau
về bộ nhớ con người1. Chúng tôi cung cấp bằng chứng rằng
Memoria thành công trong việc giải quyết quên lãng định
mệnh thông qua phân tích toàn diện.
2. Chúng tôi hiệu quả tích hợp Memoria vào GPT, BERT
và RoBERTa thể hiện hiệu suất vượt trội so với các phương
pháp bộ nhớ ngoài truyền thống trên các nhiệm vụ sắp xếp,
mô hình hóa ngôn ngữ và phân loại văn bản.
3. Chúng tôi khám phá ra sự tương đồng của bộ nhớ dài hạn
giữa Memoria và con người bằng cách cho thấy Memoria
tái tạo chặt chẽ ba hiệu ứng nổi tiếng của bộ nhớ con người:
hiệu ứng ưu tiên, gần đây và tính liền kề thời gian.
2. Bối cảnh
Bộ nhớ của Mạng Neural Mạng Neural Hồi quy (Rumelhart & McClelland, 1987; Hochreiter & Schmidhuber, 1997;
Chung et al., 2014) được giới thiệu để xử lý dữ liệu tuần
tự. Mạng Neural Tăng cường Bộ nhớ (MANNs) xuất hiện
để thực hiện các thao tác bộ nhớ phức tạp vượt ra ngoài việc
xử lý tuần tự đơn giản. Máy Turing Neural (NTMs) (Graves
et al., 2014) có hệ thống lưu trữ có thể được truy cập bằng
cơ chế attention. NTMs được phát triển thêm thành DNC
(Graves et al., 2016), Sparse DNC (Rae et al., 2016), D-NTM
(Gulcehre et al., 2017b), TARDIS (Gulcehre et al., 2017a)
và GCL (Meng & Rumshisky, 2018). Sau thành công của
Transformer, nghiên cứu đã tập trung vào độ dài ngữ cảnh
hạn chế của Transformer.
1Việc triển khai Memoria và tất cả mã thực nghiệm
đều có sẵn công khai tại https://github.com/
cosmoquester/memoriaHai phương pháp chính đã được đề xuất để giải quyết hạn
chế này. Phương pháp đầu tiên liên quan đến tối ưu hóa
tính toán của các kiến trúc như Longformer (Beltagy et al.,
2020), BigBird (Zaheer et al., 2020) và Reformer (Kitaev et
al., 2020). Tuy nhiên, các mô hình vẫn chỉ xử lý đầu vào có
kích thước hạn chế, mặc dù chúng xử lý độ dài lớn hơn với
cùng lượng tài nguyên. Phương pháp thứ hai liên quan đến
việc tận dụng lưu trữ bộ nhớ ngoài, được minh họa bởi các
mô hình như Transformer-XL (Dai et al., 2019), Compressive Transformer (Rae et al., 2020), ∞-Transformer (Martins
et al., 2021), Memory Transformer (Burtsev & Sapunov,
2020), Recurrent Memory Transformer (Bulatov et al., 2022)
và Memorizing Transformers (Wu et al., 2022). Những mô
hình này chia đầu vào thành nhiều đoạn và kết hợp chúng
để duy trì tốt hơn các phụ thuộc dài hạn trong dữ liệu tuần
tự. Chúng có cấu trúc đơn giản hơn so với MANNs truyền
thống và sử dụng bộ nhớ tập trung vào thông tin gần đây.
Do đó, trong hầu hết các trường hợp trong số chúng, chúng
không miễn nhiễm với vấn đề quên lãng định mệnh. Memoria cũng theo phương pháp thứ hai, nhưng vượt qua quên
lãng định mệnh bằng cách bắt chước tâm trí con người.
Memoria phân loại ký ức thành ba cấp độ theo mô hình
Multi-Store (Atkinson & Shiffrin, 1968), sử dụng thuật ngữ
bộ nhớ làm việc thay vì bộ nhớ cảm giác. Memoria dựa vào
hai cơ chế quên. Thứ nhất, đối với việc quên trong bộ nhớ
ngắn hạn, chúng tôi áp dụng cơ chế thay thế (Waugh &
Norman, 1965), thay thế thông tin cũ bằng thông tin mới.
Thứ hai, đối với việc quên trong cả bộ nhớ ngắn hạn và dài
hạn, chúng tôi kết hợp khái niệm lý thuyết suy giảm dấu
vết (Brown, 1958; Peterson & Peterson, 1959), cho rằng ký
ức dần mờ đi nếu chúng không được gọi lại tích cực. Chiến
lược này hỗ trợ Memoria bảo tồn ký ức hữu ích.
2

--- TRANG 3 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Tầm quan trọng Dài hạn Theo Mô hình Multi-Store (Atkinson & Shiffrin, 1968), ký ức được bảo tồn và củng cố tốt
hơn thông qua việc lặp lại nhiều lần. Vì ký ức được truy
cập thường xuyên dễ giữ lại trong thời gian dài hơn (Roediger & Butler, 2011; Antony et al., 2017), Memoria ưu
tiên duy trì những ký ức được gọi lại lặp đi lặp lại. Memoria
cập nhật thông tin này ở mỗi bước thời gian để duy trì tầm
quan trọng dài hạn của mỗi ký ức.
Bảo tồn Có chọn lọc Vấn đề tiếp theo là xác định cách
bảo tồn có chọn lọc chỉ những ký ức được phân biệt. Con
người sử dụng các chiến lược quên đa dạng (Brown, 1958;
Peterson & Peterson, 1959; Underwood & Postman, 1960;
Waugh & Norman, 1965). Memoria sử dụng suy giảm làm
cơ chế quên chính, gán tuổi thọ định trước cho mỗi ký ức
và liên tục giảm tuổi thọ của nó. Cách duy nhất để ký ức
có được tuổi thọ là thông qua việc truy xuất và sử dụng.
Thiết kế này đảm bảo rằng tuổi thọ được thu được tỷ lệ
thuận với mức độ đóng góp, cho phép ký ức quan trọng tồn
tại trong thời gian dài. Điều này phản ánh đặc tính của não
bộ trong việc bảo tồn ký ức liên quan đến tính hữu ích và
phần thưởng cao trong dài hạn (Morrissey et al., 2017;
Braun et al., 2018).
Kích hoạt dựa trên Gợi ý Kích hoạt dựa trên gợi ý và
các vấn đề tìm kiếm bộ nhớ liên quan đến việc truy xuất
bộ nhớ. SAM (Raaijmakers & Shiffrin, 1981; 1980; Shiffrin
& Raaijmakers, 1992) là tiêu chuẩn cho các mô hình bộ
nhớ tiếp theo (Kahana, 2020). Khái niệm khớp toàn cục
trong SAM được chấp nhận rộng rãi, trong đó trọng số liên
kết giữa ngữ cảnh hiện tại và bộ nhớ được sử dụng trong
việc truy xuất. Tương tự, trong Memoria, bộ nhớ làm việc
luôn đại diện cho ký ức gần đây nhất, giải quyết vấn đề
kích hoạt dựa trên gợi ý bằng cách tận dụng khoảng cách
từ nó đến các ứng cử viên truy xuất.
Tìm kiếm Bộ nhớ Trong việc tìm kiếm bộ nhớ (Shiffrin &
Atkinson, 1969; Atkinson et al., 1974; Atkinson & Juola,
1974), chúng tôi áp dụng khái niệm tìm kiếm toàn cục,
đó là một tính năng chính của SAM (Davis et al., 2014).
SAM không chỉ phản ánh mối liên kết giữa ngữ cảnh và
bộ nhớ mà còn xem xét mối liên kết lẫn nhau giữa các mảnh
bộ nhớ. SAM ban đầu truy xuất ký ức từ bộ nhớ dài hạn
bằng cách sử dụng mối liên kết với bộ nhớ ngắn hạn. Một
khi ký ức mới được gọi lại, ký ức đó được sử dụng để lặp
đi lặp lại gọi lại thêm ký ức bằng cách tận dụng mối liên
kết với ký ức đã được truy xuất trước đó. Quá trình lặp
này tăng mối liên kết giữa ký ức được gọi lại cùng nhau,
tạo điều kiện cho việc gọi lại dễ dàng trong các lần truy
xuất tiếp theo. Memoria, tương tự, giải quyết vấn đề tìm
kiếm bộ nhớ bằng cách kết nối các mảnh bộ nhớ riêng lẻ
và sử dụng cơ chế tìm kiếm bộ nhớ tiếp theo dựa trên bộ
nhớ được gọi lại.
Lý thuyết Hebbian Một engram phục vụ như đơn vị cơ
bản của bộ nhớ trong khoa học thần kinh, với tăng cường
dài hạn (LTP) của cường độ synapse hoạt động như cơ chế
trung tâm trong hình thành engram (Poo et al., 2016). Lý
thuyết Hebbian (Hebb, 1949) là một lý thuyết dẻo dai thần
kinh nổi bật giả định cách kết nối giữa hai neuron thay đổi.
LTP là một trong những khái niệm chính của lý thuyết
Hebbian, cho rằng khi hai neuron được kích hoạt lặp đi
lặp lại cùng nhau, kết nối giữa chúng được tăng cường.
Hiện tượng này thường được gọi là nguyên tắc "Cùng kích
hoạt, cùng kết nối". Trong những năm gần đây, đã có sự
quan tâm ngày càng tăng đối với việc áp dụng học Hebbian
vào học sâu (Kuriscak et al., 2015; Journé et al., 2023).
Một số nghiên cứu (Rae et al., 2018; Limbacher & Legenstein, 2020; Le et al., 2020; Ramsauer et al., 2021) đã mô
hình hóa bộ nhớ liên kết bằng mạng neural. Quy tắc học
Hebbian (Caporale & Dan, 2008; Song et al., 2000), một
công thức toán học của học Hebbian, diễn tả cụ thể quá
trình học Hebbian. Memoria cũng coi engram là đơn vị
tối thiểu của bộ nhớ, và các thay đổi trọng số của engram
được thiết kế để tuân theo quy tắc của Hebb. Hơn nữa,
Phụ lục A cho thấy Memoria thỏa mãn tất cả sáu thuộc
tính toán học quan trọng (Gerstner & Kistler, 2002) cho
quy tắc học Hebbian mặc dù nó không có dạng toán học
điển hình của học Hebbian.
3. Memoria
Có ba giai đoạn sử dụng Memoria. Giai đoạn đầu tiên là
giai đoạn truy xuất, trong đó nó sử dụng bộ nhớ làm việc
như một gợi ý để truy xuất engram từ bộ nhớ ngắn hạn và
bộ nhớ dài hạn. Giai đoạn thứ hai là giai đoạn khai thác,
nơi mô hình sử dụng engram được truy xuất để giải quyết
nhiệm vụ. Giai đoạn cuối cùng là ghi nhớ & quên. Trong
giai đoạn này, tất cả engram được truy xuất nhận được
thêm tuổi thọ tùy thuộc vào tính hữu ích của mỗi engram,
và tất cả engram mất tuổi thọ đi một.
3.1. Thành phần
Memoria bao gồm ba loại bộ nhớ khác biệt: bộ nhớ làm
việc, bộ nhớ ngắn hạn và bộ nhớ dài hạn, mỗi loại được
cấu thành từ engram. Trong Hình 1, chúng tôi mô tả cấu
trúc tổng thể của ba thành phần bộ nhớ trong Memoria.
Engram Engram là đơn vị nhỏ nhất của thông tin bộ nhớ,
và engram cấu thành mỗi bộ nhớ. Mỗi engram sở hữu tuổi
thọ riêng và trọng số kết nối. Memoria liên tục bảo tồn
engram theo chuỗi và quản lý thích hợp tuổi thọ và trọng
số kết nối của engram. Trong nghiên cứu này, chúng tôi coi
phần thông tin của engram như một vector nhúng (engram
e∈Rd, trong đó d là chiều mô hình) cho tất cả thí nghiệm.
Tuy nhiên, engram có thể có các dạng khác nhau về mặt
lý thuyết. Khi sử dụng engram có dạng khác nhau, một
hàm tương quan giữa hai engram nên được định nghĩa.
Ví dụ, người ta có thể định nghĩa một engram đơn lẻ như
một câu văn bản và sử dụng hàm tương quan dựa trên
khoảng cách chỉnh sửa để sử dụng Memoria.
Bộ nhớ Làm việc Bộ nhớ làm việc (WM) là kho lưu trữ
bộ nhớ tức thì và phục vụ như một tham chiếu để truy cập
3

--- TRANG 4 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Hình 2: Quá trình truy xuất trong Memoria. Memoria sử dụng bộ nhớ làm việc để xác định engram liên quan trong cả bộ nhớ
ngắn hạn và dài hạn. Trọng số được tính toán trong bước 1 và 4 có nghĩa là cường độ liên kết giữa engram và bộ nhớ làm việc,
với giá trị lớn hơn dẫn đến việc lựa chọn cuối cùng của engram. Cơ chế này xử lý vấn đề kích hoạt dựa trên gợi ý bằng cách
phản ánh mối liên kết với bộ nhớ làm việc. Engram trong vùng màu xám đại diện cho engram được truy xuất.
engram liên quan từ bộ nhớ ngắn hạn và dài hạn. Bộ nhớ
làm việc áp dụng cấu trúc hàng đợi có kích thước cố định
bởi số lượng engram mới được tạo trong một bước thời
gian duy nhất. Ở mỗi bước thời gian, bộ nhớ làm việc
được cập nhật.
Bộ nhớ Ngắn hạn Bộ nhớ ngắn hạn (STM), ký hiệu là
Mstm, chứa thông tin tương đối gần đây. Engram trong
bộ nhớ làm việc được chuyển sang bộ nhớ ngắn hạn khi
thông tin mới đến. Bộ nhớ ngắn hạn sử dụng cấu trúc dữ
liệu hàng đợi với dung lượng có thể cấu hình và cố định.
Bộ nhớ Dài hạn Bộ nhớ dài hạn (LTM), ký hiệu là Mltm,
có khả năng lưu trữ số lượng engram không giới hạn. Engram được dequeue từ bộ nhớ ngắn hạn được chuyển sang
bộ nhớ dài hạn. Bộ nhớ dài hạn bảo tồn một loạt thông
tin rộng, trải từ những kỷ niệm sớm nhất đến những kỷ
niệm gần đây. Do đó, engram của bộ nhớ dài hạn khác
nhau về thời gian tạo và tuổi.
Đồ thị Bộ nhớ Engram trong bất kỳ bộ nhớ nào có thể
được liên kết với nhau, tạo thành cấu trúc dữ liệu đồ thị
có hướng có trọng số, trong đó mỗi đỉnh tương ứng với
một engram. Trọng số cạnh có hướng Ei→j chỉ ra xác suất
có điều kiện thực nghiệm của việc truy xuất engram ej sau
khi engram ei đã được truy xuất, với Mrem đại diện cho
tập hợp tất cả engram được truy xuất. Xác suất này được
xác định bằng cách chia số lần ei và ej được truy xuất
cùng nhau (Counti,j) cho số lần ei được truy xuất (Counti,i).
Những cạnh có trọng số này tạo điều kiện tìm engram trong
bộ nhớ dài hạn, với trọng số của chúng được điều chỉnh
theo nguyên tắc "Cùng kích hoạt, cùng kết nối" (Hebb,
1949). Cấu trúc đồ thị này phản ánh trọng số liên kết giữa
các mục trong SAM (Raaijmakers & Shiffrin, 1981; Kahana, 2020), đóng vai trò then chốt trong việc giải quyết
vấn đề tìm kiếm bộ nhớ.
Ei→j = P(ej ∈ Mrem|ei ∈ Mrem)
= Counti,j/Counti,i
3.2. Truy xuất
Trong giai đoạn này, việc truy xuất engram phù hợp được
thực hiện bằng cách khám phá bộ nhớ ngắn hạn và dài
hạn dựa trên bộ nhớ làm việc. Hình 2 cho thấy toàn bộ
quá trình truy xuất.
1. Thay thế bộ nhớ làm việc Mwm bằng engram mới. Tất
cả engram trong bộ nhớ làm việc sẽ có cùng tuổi thọ ban
đầu. Nwm có nghĩa là số lượng engram bộ nhớ làm việc.
Mwm = {ewm,1, ewm,2, ..., ewm,Nwm}
2. Bằng cách sử dụng hàm tương quan fc, tính trọng số
tương quan Cstm cho mỗi estm,i trong bộ nhớ ngắn hạn
Mstm bằng cách lấy trung bình tất cả trọng số tương quan
cho engram. Hàm khoảng cách fd được sử dụng là khoảng
cách L2. Ở đây, i đại diện cho chỉ số của Mstm và j đại
diện cho chỉ số của Mwm.
4

--- TRANG 5 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Hình 3: Các kết nối giữa bộ nhớ làm việc và engram được truy xuất được tăng cường trên tất cả các cặp. Tuổi thọ của engram
được truy xuất mở rộng tỷ lệ với đóng góp cá nhân của chúng, cho phép bảo tồn có chọn lọc thông qua việc phân bổ tuổi thọ
khác biệt. Engram đã mất hết tuổi thọ, ví dụ như eltm,7, bị loại bỏ vĩnh viễn.
fc(ei, ej) = exp(−(fd(ei, ej))²)
Cstm,i = (1/Nwm) ∑(j=1 to Nwm) fc(estm,i, ewm,j)
3. Chỉ chọn Nrem_stm engram hàng đầu với giá trị Cstm
để truy xuất. Ký hiệu engram được chọn là Mrem_stm.
4. Đối với mỗi ei ∈ Mrem_stm, chọn một engram trong
Mltm có trọng số cạnh cao nhất từ ei. Ký hiệu engram
được chọn là Minit_ltm.
Minit_ltm = arg max(ej∈Mltm) Ei→j, trong đó ei ∈ Mrem_stm
5. Sử dụng engram Minit_ltm làm điểm khởi đầu, duyệt
đồ thị Mltm bằng thuật toán tìm kiếm theo chiều sâu
(DFS) với độ sâu tìm kiếm Ndepth. Hướng khám phá nên
dựa trên trọng số cạnh, hướng tới trọng số cạnh cao nhất.
Thu thập tất cả engram duy nhất được gặp trong quá trình
tìm kiếm, bao gồm Minit_ltm, và gọi chúng là Mfound_ltm.
M⁰_ltm = Minit_ltm
Mk_ltm = arg max(ej∈Mltm) Ei→j, trong đó ei ∈ Mk-1_ltm, ej ∉ Mfound,k-1_ltm
Mfound,k_ltm = ∪(l=0 to k) Ml_ltm
Mfound_ltm = Mfound,Ndepth_ltm
6. Tính trọng số tương quan Cltm từ Mwm cho Mfound_ltm
và chọn Nrem_ltm engram hàng đầu như STM. Ký hiệu
engram là Mrem_ltm.
7. Sử dụng Mwm, Mrem_stm, Mrem_ltm như bộ nhớ được
kích hoạt.
Mrem = Mrem_stm ∪ Mrem_ltm
Mact = Mwm ∪ Mrem
Kích hoạt dựa trên gợi ý được thực hiện thông qua cơ chế
mà chỉ engram có trọng số tương quan cao nhất với bộ
nhớ làm việc mới được kích hoạt cuối cùng. Điều này cho
phép tìm kiếm bộ nhớ hiệu quả mà không yêu cầu truy cập
toàn bộ bộ nhớ dài hạn. Thay vào đó, Memoria lặp đi lặp
lại khám phá engram mới dựa trên engram đã được khám
phá và trọng số kết nối tương ứng của chúng.
3.3. Khai thác
Trong giai đoạn này, tất cả engram được truy xuất được
khai thác để hỗ trợ giải quyết nhiệm vụ và trọng số đóng
góp wi cho mỗi engram ei được đánh giá. Trong thí nghiệm
của chúng tôi, chúng tôi coi trọng số attention của mỗi
engram là đóng góp, vì engram được tham chiếu thông qua
cơ chế cross-attention.
3.4. Ghi nhớ & Quên
Cùng với việc đạt được bảo tồn có chọn lọc, Memoria cũng
tăng cường kết nối giữa engram liên quan trong bước này.
Hình 3 cho thấy quy trình tổng thể của giai đoạn này.
1. Tăng Counti,j lên một cho tất cả engram trong Mact,
đó là số lần ei và ej được truy xuất cùng nhau.
N = {1, 2, ..., |Mact|}
Counti,j := Counti,j + 1, ∀i, j ∈ N
2. Tăng tuổi thọ của engram được truy xuất bằng số gia
Inci cho engram ei. Inci được tính như sau trong đó α
là siêu tham số có nghĩa là tỷ lệ mở rộng tuổi thọ. Nếu α
là 1.0, mỗi engram e ∈ Mrem nhận tuổi thọ 1.0 trung bình.
5

--- TRANG 6 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
1K 2K 4K 8K
Chiều dài chuỗi304050607080Độ chính xácSegment 256
2K 4K 8K 16K
Chiều dài chuỗi304050607080Segment 512
4K 8K 16K 32K
Chiều dài chuỗi304050607080Segment 1024
Transformer-XL Compressive Transformer ∞-former Memoria Transformer
Hình 4: Kết quả của nhiệm vụ sắp xếp. Memoria Transformer thể hiện tính mạnh mẽ lớn hơn so với các baseline khác khi
chiều dài chuỗi đầu vào tăng. Nhiệm vụ này yêu cầu giữ lại thông tin về sự xuất hiện của token ban đầu cho đến cuối. Trong
khi các phương pháp khác đều cho thấy sự suy giảm hiệu suất đáng kể, Memoria Transformer thành công xử lý vấn đề quên
lãng định mệnh, tạo nên sự khác biệt so với các kỹ thuật cạnh tranh khác. Điểm số thô toàn diện được chỉ định trong Bảng 7.
Inci = wi / (∑(k=1 to |Mrem|) wk) × |Mrem| × α
3. Giảm tuổi thọ của tất cả engram đi 1.0.
4. Loại bỏ engram có tuổi thọ bằng 0 hoặc ít hơn.
5. Chuyển ewm vào STM. Reset WM.
6. Chuyển engram cũ nhất từ STM bằng số lượng vượt
quá dung lượng vào LTM.
Sự khác biệt trong tuổi thọ xảy ra ở hai cấp độ. Đầu tiên,
engram không được truy xuất không thể đạt được tuổi thọ
và dễ bị loại bỏ. Engram được truy xuất nhận tuổi thọ
khác nhau tùy thuộc vào đóng góp của chúng, gây ra bảo
tồn có chọn lọc ở hai cấp độ. Ngoài ra, kết nối giữa engram
được truy xuất được tăng cường, tạo điều kiện đồng truy
xuất engram liên kết chặt chẽ hơn trong tìm kiếm bộ nhớ.
4. Transformers áp dụng Memoria
Memoria hoạt động độc lập như một module tập trung vào
quản lý engram thay vì tham gia trực tiếp vào quá trình
giải quyết vấn đề. Do đó, để giải quyết hiệu quả các nhiệm
vụ bằng Memoria, việc hợp nhất nó với các mô hình mạng
neural là mạnh mẽ. Chúng tôi tích hợp Memoria vào hai
loại Transformers: một mô hình dựa trên decoder được gọi
là Memoria Transformer, và một mô hình dựa trên encoder
được gọi là Memoria BERT. Hơn nữa, chúng tôi sử dụng
module encoder bộ nhớ để tạo engram từ đầu ra của Transformer. Cả Memoria Transformer và Memoria BERT đều
tham chiếu engram bằng cơ chế cross-attention. Như được
minh họa trong Hình 5, các mô hình đầu tiên tham chiếu
engram bộ nhớ làm việc và sau đó engram bộ nhớ ngắn
hạn/dài hạn được truy xuất. Sự khác biệt giữa hai mô hình
phát sinh từ cách engram được tạo ra. Kiến trúc chi tiết
của mỗi mô hình được trình bày với mô tả trong Phụ lục G.
Hình 5: Sơ đồ cấu trúc của Memoria Transformers.
Memoria Transformer Chúng tôi sử dụng abstractor dựa
trên attention làm memory encoder fe trong đó queries là
các tham số có thể học. Vì việc sử dụng thông tin của bước
thời gian hiện tại dẫn đến rò rỉ nhân quả, chúng tôi sử dụng
trạng thái ẩn cuối ht-1 của bước thời gian trước làm Xt
trong Memoria Transformer. t đại diện cho chỉ số của segment trong toàn bộ chuỗi. Ba giá trị Q, Wk và Wv là các
tham số có thể huấn luyện. FFN là mạng feed-forward
giống như trong Transformer (Vaswani et al., 2017). Số
lượng engram bộ nhớ làm việc Nwm được xác định bởi số
lượng queries Q, vì vậy số lượng queries là một siêu tham số.
6

--- TRANG 7 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Bảng 1: Hiệu suất mô hình hóa ngôn ngữ. Perplexity (PPL) được cung cấp cho bộ dữ liệu Wikitext-103 và PG-19, trong khi
bits-per-character (BPC) được hiển thị cho Enwik8. Trong mỗi trường hợp, độ dài bộ nhớ khớp với độ dài segment, với
Wikitext-103 và PG-19 sử dụng độ dài 150, và Enwik8 sử dụng 512. Memoria Transformer vượt trội hơn các mô hình khác
trên tất cả bộ dữ liệu.
Model Wikitext-103 (PPL) PG-19 (PPL) Enwik8 (BPC)
Transformer 26.755 31.631 1.28
Transformer-XL 24.543 29.945 1.19
Compressive Transformer 24.794 29.603 1.16
∞-former 24.685 29.154 1.21
Memoria Transformer 23.471 29.149 1.16
0 500 1000 1500
Bước05001000Tuổi thôWikitext-103
0 2000 4000 6000 8000 10000
Bước0500100015002000PG-19
0 2000 4000 6000 8000 10000
Bước0200040006000Enwik8
Hình 6: Tuổi thọ trung bình của engram được truy xuất trong bộ nhớ dài hạn. Tuổi thọ của engram được truy xuất dần tăng
khi các bước trôi qua. Điều này biểu hiện rằng Memoria liên tục giữ lại và sử dụng không chỉ ký ức gần đây mà còn cả ký ức cũ.
Xt = ht-1
fe(Xt) = Abstract(Xt)
= FFN(Attention(Q, WkX, WvX))
= FFN(Attention(Q, Wkht-1, Wvht-1))
= FFN(softmax(QWkht-1)Wvht-1)
= Mwm
Memoria BERT/RoBERTa Memoria BERT cũng sử dụng
cùng memory encoder như Memoria Transformer. Không
giống như các mô hình dựa trên decoder, các mô hình dựa
trên encoder luôn có quyền truy cập vào đầu vào hoàn
chỉnh của bước thời gian hiện tại mà không gây rò rỉ nhân
quả. Do đó, memory encoder fe sử dụng trạng thái ẩn hlt
làm Xt, trong đó l biểu thị chỉ số lớp bộ nhớ. Engram mới
được thu thập từ trạng thái ẩn của lớp BERT thứ l thông
qua abstractor. Tiếp theo, engram bộ nhớ làm việc và engram được truy xuất được sử dụng trong các lớp tiếp theo
bằng cross-attention.
5. Thí nghiệm
Chúng tôi áp dụng Memoria vào Transformer và đánh giá
khả năng nắm bắt phụ thuộc dài hạn trong các nhiệm vụ
khác nhau. Nhiệm vụ đầu tiên là sắp xếp. Martins et al.
(2021) đánh giá khả năng của mô hình trong việc nhớ thông
tin dài hạn về sự xuất hiện của số bằng cách tạo ra chuỗi
số được sắp xếp dựa trên tần suất xuất hiện được xác định
trước của chúng. Thứ hai, chúng tôi thực hiện mô hình hóa
ngôn ngữ ở cấp độ token trên WikiText-103 (Raw) (Merity
et al., 2017) và PG-19 (Rae et al., 2020), và ở cấp độ ký
tự trên enwik8 (Mahoney, 2006). Tương tự như Martins
et al. (2021), chỉ 2.000 sách đầu tiên của bộ dữ liệu huấn
luyện được sử dụng cho PG-19. Chúng tôi so sánh Memoria
với các đối thủ khác của Transformer (Vaswani et al.,
2017), Transformer-XL (Dai et al., 2019), Compressive
Transformer (Rae et al., 2020) và ∞-former (Martins et
al., 2021). Cuối cùng, chúng tôi thực hiện nhiệm vụ phân
loại trên bộ dữ liệu phân loại tài liệu dài, Hyperpartisan
(Kiesel et al., 2019). Phụ lục D cung cấp thí nghiệm bổ
sung và chỉ định các siêu tham số.
5.1. Sắp xếp
Nhiệm vụ sắp xếp liên quan đến việc lấy một chuỗi ký
hiệu và xuất ra các ký hiệu theo thứ tự giảm dần của tần
suất xuất hiện (Martins et al., 2021). Các mô hình decoder
bao gồm Memoria Transformer được sử dụng cho nhiệm
vụ này. Chúng tôi thí nghiệm với các chuỗi có độ dài khác
nhau, từ 1K đến 32K2, với độ dài segment là 256, 512 và
1024, sử dụng 20 token duy nhất trong từ vựng. Trong
nhiệm vụ này, việc duy trì thông tin ban đầu cho đến cuối
là cần thiết để tránh quên lãng định mệnh, vì tần suất
xuất hiện của một token khác nhau từ đầu đến cuối.
Hình 4 cho thấy hiệu suất trên các độ dài segment khác
nhau trong nhiệm vụ sắp xếp khi độ dài chuỗi mở rộng.
2Chúng tôi đã sử dụng script của ∞-former tại https:
//github.com/deep-spin/infinite-former/blob/
main/sorting/generate_data.py để tạo bộ dữ liệu.
7

--- TRANG 8 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Bảng 2: Hiệu suất phân loại văn bản trên Hyperpartisan. Các chỉ số đánh giá là điểm F1 macro trung bình và độ chính xác,
được tính thông qua năm lần chạy độc lập. Chúng tôi báo cáo kết quả tập validation và test vì có sự khác biệt phân phối dữ liệu.
Model [Sequence Length]Validation Test
F1±STD Acc±STD F1±STD Acc±STD
BERT [512] 76.61±0.04 78.75±0.03 91.67±0.01 93.05±0.01
RoBERTa [512] 82.96±0.02 84.06±0.02 95.24±0.02 95.38±0.02
Bigbird [4096] 81.22±0.02 82.81±0.02 93.24±0.01 93.54±0.01
Longformer [4096] 78.33±0.03 79.69±0.03 94.56±0.01 94.77±0.01
Memoria BERT [512] 78.24±0.04 80.00±0.04 94.59±0.02 94.77±0.02
Memoria RoBERTa [512] 86.39±0.01 87.19±0.01 96.51±0.02 96.62±0.02
Độ dài bộ nhớ được đặt bằng giá trị của độ dài segment.
Nói chung, với việc tăng độ dài chuỗi, độ chính xác có xu
hướng giảm do sự cần thiết giữ lại thông tin ngữ cảnh dài
hơn. Đáng chú ý, Memoria thể hiện sự suy giảm hiệu suất
ít nhất so với ba mô hình khác khi độ dài chuỗi tăng, thể
hiện khả năng duy trì bộ nhớ dài hạn cho ngữ cảnh mở rộng.
Để hiểu vai trò tương ứng của mỗi bộ nhớ và thuộc tính
Hebbian, chúng tôi đã thực hiện nghiên cứu ablation trong
Phụ lục C. Phân tích này xác minh chức năng bổ sung của
mỗi module bộ nhớ và tầm quan trọng của thuộc tính Hebbian.
5.2. Mô hình hóa Ngôn ngữ
Bảng 3: Perplexity với độ dài segment nhỏ hơn là 50. Ngay
cả trong ngữ cảnh và độ dài bộ nhớ ngắn hơn, Memoria
vẫn duy trì tính ưu việt so với các phương pháp khác.
Model [Memory Length] Wikitext-103
Transformer 39.287
Transformer-XL [50] 31.459
Compressive Transformer [50] 31.644
∞-former [50] 31.790
Memoria Transformer [48] 30.007
Trong mô hình hóa ngôn ngữ, Memoria Transformer cũng
được áp dụng. Vì các mô hình được huấn luyện sẵn có sẵn
công khai được huấn luyện với số lượng tham số khác nhau
trên các bộ dữ liệu khác nhau, các mô hình được huấn
luyện từ đầu trong thí nghiệm của chúng tôi. Cụ thể, chúng
tôi sử dụng kiến trúc GPT-2 với 12 lớp và 768 chiều. Kết
quả của thí nghiệm bổ sung với các mô hình ngôn ngữ được
huấn luyện sẵn được chi tiết trong Phụ lục D.2. Chúng tôi
đặt độ dài segment là 150 cho thí nghiệm cấp độ token và
512 cho thí nghiệm cấp độ ký tự theo Bulatov et al. (2022).
Tokenizer GPT-2 được huấn luyện sẵn được sử dụng cho
tất cả thí nghiệm cấp độ token.
Bảng 1 cho thấy kết quả. So với Transformer, tất cả các
mô hình thay thế đều thể hiện hiệu suất tăng cường. Memoria Transformer đạt hiệu suất tốt nhất trên cả ba bộ dữ
liệu. Những kết quả như vậy nhấn mạnh hiệu quả của
Memoria trong các nhiệm vụ thực tế vì mô hình hóa ngôn
ngữ liên quan đến sự phức tạp vượt ra ngoài việc chỉ nắm
bắt ngữ cảnh dài hạn.
Bảng 3 trình bày hiệu suất của mỗi mô hình khi độ dài
segment được giảm xuống 50, để quan sát động lực khi
số lượng segment tăng. So sánh với Bảng 1 làm nổi bật
sự khác biệt hiệu suất rõ rệt hơn giữa Transformer và các
mô hình bộ nhớ. Ngay cả trong các tình huống đòi hỏi sự
xem xét sâu sắc hơn về phụ thuộc dài hạn, Memoria liên
tục cho thấy hiệu suất vượt trội.
Chúng tôi xác thực liệu Memoria có sử dụng hiệu quả bộ
nhớ dài hạn hay không. Hình 6 cho thấy tuổi thọ trung
bình của engram được truy xuất trong bộ nhớ dài hạn ở
mỗi bước trên bộ dữ liệu test. Tuổi thọ đại diện cho số
bước đã trôi qua kể từ khi engram được tạo. Một đường
thẳng trên đồ thị sẽ cho thấy sự phụ thuộc chỉ vào engram
gần đây, làm cho nó không hiệu quả như bộ nhớ dài hạn.
Ngược lại, việc tham chiếu liên tục đến thông tin quá khứ
khiến engram già đi dần dần, phản ánh trong xu hướng
tăng của đồ thị. Xu hướng này biểu thị việc truy xuất và
sử dụng liên tục của Memoria đối với thông tin quan trọng
trong quá khứ ngay cả sau nhiều bước thời gian. Để làm
rõ hơn, một số ảnh chụp nhanh của kết nối nội bộ được
cung cấp trong Phụ lục H để hỗ trợ hiểu biết.
5.3. Phân loại
Hyperpartisan là bộ dữ liệu được sử dụng rộng rãi cho
nhiệm vụ phân loại tài liệu dài. Để xác thực hiệu quả của
Memoria trong kiến trúc dựa trên encoder, chúng tôi tích
hợp Memoria vào BERT và RoBERTa và so sánh hiệu suất
của chúng với các mô hình khác. Do chi phí cao của việc
huấn luyện trước các mô hình với cấu trúc khác nhau, việc
sử dụng các mô hình được huấn luyện sẵn cho nhiệm vụ
phân loại là không thể tránh khỏi. Kích thước của tất cả
các mô hình đều có kích thước base 12 lớp. Các mô hình
tăng cường Memoria tham chiếu đến 192 engram ngoài
512 ngữ cảnh.
Bảng 2 trình bày hiệu suất phân loại của các mô hình.
Rõ ràng là các mô hình tăng cường Memoria cho thấy
những cải thiện hiệu suất đáng kể so với BERT và RoBERTa thông thường, mặc dù việc so sánh tất cả các mô
hình là thách thức vì sự khác biệt trong huấn luyện trước.
Memoria RoBERTa đạt điểm số cao nhất trong tất cả trường
hợp. Thực hiện kiểm định t một phía, Memoria RoBERTa
xác minh hiệu suất cao hơn có ý nghĩa thống kê so với
Longformer và Bigbird, với giá trị p lần lượt là 0.045 và
0.005.
8

--- TRANG 9 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
6. Hiệu ứng Bộ nhớ Tâm lý
0 20 40 60 80 100
Timestep at Creation (%)0.0050.0100.0150.0200.0250.030Mật độ(a) Đường cong Vị trí Tuần tự của Engrams
0 50 100 150 200 250 300
Chênh lệch Tuổi thọ giữa Engrams0.00.20.40.6Trọng số Cạnh Trung bình(b) Tính Liền kề Thời gian của Engrams
Hình 7: Hiệu ứng bộ nhớ tâm lý của engram Memoria. Hình
(a) mô tả biểu đồ ước lượng mật độ kernel minh họa phân
phối thời gian tạo cho engram còn lại trong bộ nhớ dài hạn
sau khi trải qua tất cả bước thời gian. Biểu đồ này hiển thị
các mẫu của hiệu ứng ưu tiên và hiệu ứng gần đây, nơi
thông tin ban đầu và gần đây được giữ lại nhiều hơn thông
tin trung gian. Hình (b) minh họa trọng số kết nối trung
bình theo sự khác biệt về tuổi thọ (thời gian tạo) của engram còn lại. Điều này có nghĩa là khi chênh lệch tuổi thọ
nhỏ, trọng số cạnh cao, cho thấy hiệu ứng tính liền kề thời
gian. Phụ lục B cung cấp giải thích chi tiết hơn.
Memoria được thiết kế dựa trên các mô hình bộ nhớ khác
nhau của con người. Đặc biệt, bộ nhớ dài hạn của Memoria
được cố ý thiết kế để duy trì thông tin cũ có giá trị, giống
như con người. Do sự phức tạp của các giải thích sinh học
về cơ chế bộ nhớ con người, việc so sánh trực tiếp giữa
Memoria và bộ nhớ con người là thách thức. Tuy nhiên,
nghiên cứu về đặc tính của bộ nhớ con người trong tâm lý
học rất rộng rãi. Do đó, bằng cách xác định các hiệu ứng
đã biết của bộ nhớ con người trong Memoria, chúng tôi
xác nhận sự tương đồng của nó với hệ thống bộ nhớ con
người.
Hình 7 thể hiện các mẫu tương tự với hiệu ứng ưu tiên,
hiệu ứng gần đây và hiệu ứng tính liền kề thời gian của
engram trong Memoria. Chúng tôi sử dụng Memoria Transformer để thực hiện suy luận trên toàn bộ tập test của
Wikitext-103, sau đó phân tích tuổi thọ và trọng số nội bộ
của engram còn lại. Đồ thị phía trên minh họa mật độ theo
thời gian tạo engram. Các mô hình dựa trên bộ nhớ thông
thường có xu hướng giữ thông tin gần đây, tạo ra xu hướng
tăng. Ngược lại, Memoria bảo tồn thông tin sớm và muộn
nhất nhiều hơn thông tin trung gian. Hành vi này thể hiện
cả hiệu ứng ưu tiên và gần đây.
Hình dưới cho thấy mối quan hệ giữa chênh lệch tuổi thọ
giữa engram còn lại và trọng số cạnh. Tuổi thọ đại diện
cho số bước thời gian đã trôi qua kể từ khi tạo, và chênh
lệch tuổi thọ tương ứng với sự khác biệt về thời gian tạo.
Chúng tôi quan sát thấy engram được tạo gần nhau về thời
gian thể hiện trọng số kết nối nội bộ cao hơn, minh họa
một mẫu tương tự như hiệu ứng tính liền kề thời gian.
7. Kết luận và Nghiên cứu Tương lai
Chúng tôi đề xuất Memoria như một module bộ nhớ tổng
quát cho mạng neural, nhằm giải quyết vấn đề cơ bản của
quên lãng định mệnh trong bộ nhớ dài hạn, cùng với các
vấn đề phái sinh về tầm quan trọng dài hạn, bảo tồn có
chọn lọc, kích hoạt dựa trên gợi ý và tìm kiếm bộ nhớ.
Các giải pháp cho những vấn đề này lấy cảm hứng từ hệ
thống bộ nhớ con người, tích cực kết hợp các lý thuyết
tâm lý học và khoa học thần kinh khác nhau liên quan đến
bộ nhớ, bao gồm Mô hình Multi-Store (Atkinson & Shiffrin, 1968), SAM (Raaijmakers & Shiffrin, 1981) và lý
thuyết Hebbian (Hebb, 1949). Cách tiếp cận này cho phép
Memoria phản ánh các hiệu ứng bộ nhớ con người khác
nhau, bao gồm hiệu ứng gần đây, hiệu ứng ưu tiên và hiệu
ứng tính liền kề thời gian. Chúng tôi chứng minh những
hiệu ứng này thông qua phân tích đa dạng. Chúng tôi xác
thực hiệu suất mạnh mẽ của Memoria so với các phương
pháp khác trong các nhiệm vụ sắp xếp, mô hình hóa ngôn
ngữ và phân loại.
Chúng tôi nỗ lực trao quyền cho Memoria với các đặc
tính mạnh mẽ của bộ nhớ con người. Tuy nhiên, sự khác
biệt vẫn tồn tại ở nhiều khía cạnh. Lý thuyết xử lý cấp
độ (Craik & Lockhart, 1972) nhấn mạnh cấu trúc liên tục
hơn của bộ nhớ dựa trên độ sâu xử lý thay vì các danh
mục rời rạc của mô hình Multi-Store. Ngoài ra, lý thuyết
can thiệp (Underwood & Postman, 1960) nhấn mạnh tác
động đáng kể của hiệu ứng can thiệp giữa ký ức đã thiết
lập và thông tin đến như cơ chế quên chủ đạo trong bộ
nhớ dài hạn. Nghiên cứu tương lai của chúng tôi sẽ kết
hợp những cơ chế này vào Memoria để sắp xếp nó gần
gũi hơn với các nguyên tắc hoạt động của bộ nhớ con người,
tăng cường khả năng của nó tương ứng. Chúng tôi dự đoán
sự tích hợp này sẽ giải phóng tiềm năng của Memoria trong
các nhiệm vụ xử lý tuần tự đa dạng và dựa trên agent, đặc
biệt trong các lĩnh vực như chatbot đối thoại và mô phỏng
học tăng cường, cuối cùng mở đường để thực hiện trí tuệ
ở cấp độ con người.
9

--- TRANG 10 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Tuyên bố Tác động
Bài báo này tìm cách tạo ra một module bộ nhớ ngoài được
thiết kế cho mạng neural nhân tạo để tăng cường khả năng
xử lý cho dữ liệu chuỗi dài tổng quát. Việc phát triển
Memoria dựa trên các lý thuyết liên quan đến bộ nhớ con
người, có thể có nhiều tác động xã hội khác nhau. Đáng
chú ý, Memoria duy trì thông tin được tạo ra trong quá
trình suy luận trong bộ nhớ dài hạn. Do đó, việc sử dụng
kéo dài dữ liệu người dùng cụ thể cho suy luận mô hình
có thể gây ra những lo ngại về quyền riêng tư tiềm ẩn do
sự tích lũy thông tin trong Memoria. Người dùng Memoria
nên xem xét cẩn thận các quy định về việc xử lý dữ liệu
cá nhân.
Lời cảm ơn
Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh vì những
câu hỏi và bình luận hữu ích của họ. Dự án này được hỗ
trợ một phần bởi Microsoft Research Asia. Nghiên cứu
này được hỗ trợ một phần bởi Chương trình Phát triển
Công nghệ Sinh học & Y học của Quỹ Nghiên cứu Quốc
gia (NRF) được tài trợ bởi chính phủ Hàn Quốc (MSIT)
(NRF-2021M3A9E4080780), Viện Quy hoạch và Đánh giá
Công nghệ Thông tin & Truyền thông (IITP) được tài trợ
bởi chính phủ Hàn Quốc (MSIT) (IITP-2023-2020-0-018,
khung suy luận diễn dịch sử dụng omni-data để hiểu mối
quan hệ nhân quả phức tạp & chương trình ICT Creative
Consilience và RS-2024-00398115, Nghiên cứu về độ tin
cậy và sự nhất quán của kết quả được tạo ra bởi AI Tạo
sinh).
Tài liệu tham khảo
Andoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I., and
Schmidt, L. Practical and optimal lsh for angular distance.
In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems, volume 28. Curran Associates, Inc.,
2015. URL https://proceedings.neurips.cc/
paper_files/paper/2015/file/
2823f4797102ce1a1aec05359cc16dd9-
Paper.pdf.
Antony, J. W., Ferreira, C. S., Norman, K. A., and Wimber,
M. Retrieval as a fast route to memory consolidation.
Trends in cognitive sciences, 21(8):573–576, 2017.
Atkinson, R. and Shiffrin, R. Human memory: A
proposed system and its control processes. volume 2 of Psychology of Learning and Motivation, pp. 89–195. Academic Press, 1968. doi:
https://doi.org/10.1016/S0079-7421(08)60422-3. URL
https://www.sciencedirect.com/science/
article/pii/S0079742108604223.
Atkinson, R. C. and Juola, J. F. Search and decision processes in recognition memory., pp. xiii, 299–xiii, 299.
Contemporary developments in mathematical psychology: I. Learning, memory and thinking. W. H. Freeman,
Oxford, England, 1974. ISBN 0716708485.
Atkinson, R. C., Hermann, D. J., and Wescourt, K. T. Search
processes in recognition memory., pp. xi, 386–xi, 386.
Theories in cognitive psychology: The Loyola Symposium. Lawrence Erlbaum, Oxford, England, 1974. ISBN
0470812281.
Baddeley, A. D. and Hitch, G. The recency effect: Implicit
learning with explicit retrieval? Memory & Cognition,
21(2):146–155, 1993. doi: 10.3758/BF03202726. URL
https://doi.org/10.3758/BF03202726.
Banino, A., Badia, A. P., Köster, R., Chadwick, M. J.,
Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M.,
Kumaran, D., and Blundell, C. Memo: A deep network for flexible combination of episodic memories.
In International Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=rJxlc0EtDr.
Baranchuk, D., Babenko, A., and Malkov, Y. Revisiting
the inverted indices for billion-scale approximate nearest
neighbors. In Ferrari, V., Hebert, M., Sminchisescu, C.,
and Weiss, Y. (eds.), Computer Vision – ECCV 2018, pp.
209–224, Cham, 2018. Springer International Publishing.
ISBN 978-3-030-01258-8.
Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR,
abs/2004.05150, 2020. URL https://arxiv.org/
abs/2004.05150.
Bjork, R. Retrieval as a memory modifier: An interpretation
of negative recency & related phenomena. Information
Processing and Cognition: The Loyola Symposium, 01
1975.
Bjork, R. A. Retrieval practice and the maintenance of
knowledge., pp. 396–401. Practical aspects of memory:
Current research and issues, Vol. 1: Memory in everyday
life. John Wiley & Sons, Oxford, England, 1988. ISBN
0-471-91234-4 (Hardcover).
Bjork, R. A. and Whitten, W. B. Recency-sensitive
retrieval processes in long-term free recall. Cognitive Psychology, 6(2):173–189, 1974. ISSN 00100285. doi: https://doi.org/10.1016/0010-0285(74)900097. URL https://www.sciencedirect.com/
science/article/pii/0010028574900097.
Braun, E. K., Wimmer, G. E., and Shohamy, D. Retroactive
and graded prioritization of memory by reward. Nature
Communications, 9(1):4886, 2018. doi: 10.1038/s41467-
10

--- TRANG 11 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
018-07280-0. URL https://doi.org/10.1038/
s41467-018-07280-0.
Brown, J. Some tests of the decay theory of immediate memory. Quarterly Journal of Experimental Psychology, 10(1):12–21, 1958. doi: 10.1080/
17470215808416249. URL https://doi.org/
10.1080/17470215808416249.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,
Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In Larochelle, H.,
Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.),
Advances in Neural Information Processing Systems,
volume 33, pp. 1877–1901. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.cc/
paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-
Paper.pdf.
Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent
memory transformer. In Koyejo, S., Mohamed, S.,
Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),
Advances in Neural Information Processing Systems, volume 35, pp. 11079–11091. Curran Associates, Inc., 2022.
URL https://proceedings.neurips.cc/
paper_files/paper/2022/file/
47e288629a6996a17ce50b90a056a0e1-
Paper-Conference.pdf.
Burtsev, M. S. and Sapunov, G. V. Memory transformer. CoRR, abs/2006.11527, 2020. URL https:
//arxiv.org/abs/2006.11527.
Caporale, N. and Dan, Y. Spike timing-dependent plasticity:
a hebbian learning rule. Annu Rev Neurosci, 31:25–46,
2008.
Chung, J., Gülçehre, Ç., Cho, K., and Bengio, Y. Empirical
evaluation of gated recurrent neural networks on sequence
modeling. CoRR, abs/1412.3555, 2014. URL http:
//arxiv.org/abs/1412.3555.
Craik, F. I. and Lockhart, R. S. Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior,
11(6):671–684, 1972. ISSN 0022-5371. doi:
https://doi.org/10.1016/S0022-5371(72)80001-X. URL
https://www.sciencedirect.com/science/
article/pii/S002253717280001X.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and
Salakhutdinov, R. Transformer-xl: Attentive language
models beyond a fixed-length context. ACL 2019 - 57th
Annual Meeting of the Association for Computational
Linguistics, Proceedings of the Conference, pp. 2978–
2988, 1 2019. doi: 10.48550/arxiv.1901.02860. URL
https://arxiv.org/abs/1901.02860v3.
Davis, T., Xue, G., Love, B. C., Preston, A. R., and
Poldrack, R. A. Global neural pattern similarity as a
common basis for categorization and recognition memory. Journal of Neuroscience, 34(22):7472–7484, 2014.
ISSN 0270-6474. doi: 10.1523/JNEUROSCI.376-
13.2014. URL https://www.jneurosci.org/
content/34/22/7472.
Deese, J. and Kaufman, R. A. Serial effects in recall of unorganized and sequentially organized verbal material. Journal of Experimental Psychology, 54(3):180–187, 1957.
ISSN 0022-1015(Print). doi: 10.1037/h0040536. URL
https://doi.org/10.1037/h0040536.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186,
Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.
Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G.,
Mazaré, P.-E., Lomeli, M., Hosseini, L., and Jégou, H.
The faiss library. 2024.
Gerstner, W. and Kistler, W. M. Mathematical formulations of hebbian learning. Biological Cybernetics, 87
(5):404–415, 2002. doi: 10.1007/s00422-002-0353-y.
URL https://doi.org/10.1007/s00422-002-
0353-y.
Ginns, P. Integrating information: A meta-analysis
of the spatial contiguity and temporal contiguity effects. Learning and Instruction, 16
(6):511–525, 2006. ISSN 0959-4752. doi:
https://doi.org/10.1016/j.learninstruc.2006.10.001.
URL https://www.sciencedirect.com/
science/article/pii/S0959475206000806.
Graves, A., Wayne, G., and Danihelka, I. Neural turing
machines. CoRR, abs/1410.5401, 2014. URL http:
//arxiv.org/abs/1410.5401.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G.,
Grefenstette, E., Ramalho, T., Agapiou, J., Badia, A. P.,
11

--- TRANG 12 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Hermann, K. M., Zwols, Y., Ostrovski, G., Cain, A.,
King, H., Summerfield, C., Blunsom, P., Kavukcuoglu,
K., and Hassabis, D. Hybrid computing using a neural
network with dynamic external memory. Nature, 538
(7626):471–476, October 2016. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature20101.
Gulcehre, C., Chandar, S., and Bengio, Y. Memory augmented neural networks with wormhole connections,
2017a.
Gulcehre, C., Chandar, S., Cho, K., and Bengio, Y. Dynamic neural turing machine with soft and hard addressing schemes, 2017b.
Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D.,
Chern, F., and Kumar, S. Accelerating large-scale inference with anisotropic vector quantization. In III,
H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp.
3887–3896. PMLR, 13–18 Jul 2020a. URL https://
proceedings.mlr.press/v119/guo20h.html.
Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D.,
Chern, F., and Kumar, S. Accelerating large-scale inference with anisotropic vector quantization. In III,
H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp.
3887–3896. PMLR, 13–18 Jul 2020b. URL https://
proceedings.mlr.press/v119/guo20h.html.
Hassabis, D., Kumaran, D., Summerfield, C., and Botvinick,
M. Neuroscience-inspired artificial intelligence. Neuron, 95(2):245–258, 2017. ISSN 0896-6273. doi:
https://doi.org/10.1016/j.neuron.2017.06.011. URL
https://www.sciencedirect.com/science/
article/pii/S0896627317305093.
Hebb, D. O. The organization of behavior. 1949.
Hochreiter, S. and Schmidhuber, J. Long
short-term memory. Neural Computation, 9:
1735–1780, 11 1997. ISSN 08997667. doi:
10.1162/NECO.1997.9.8.1735. URL https:
//www.researchgate.net/publication/
13853244_Long_Short-term_Memory.
Journé, A., Rodriguez, H. G., Guo, Q., and Moraitis,
T. Hebbian deep learning without feedback. In The
Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/
forum?id=8gd4M-Rj1.
Kahana, M. J. Associative retrieval processes in free
recall. Memory & Cognition, 24(1):103–109, 1996.
doi: 10.3758/BF03197276. URL https://doi.org/
10.3758/BF03197276.
Kahana, M. J. Computational models of memory search.
Annual Review of Psychology, 71(1):107–138, 2020.
doi: 10.1146/annurev-psych-010418-103358. URL
https://doi.org/10.1146/annurev-psych-
010418-103358. PMID: 31567043.
Kiesel, J., Mestre, M., Shukla, R., Vincent, E., Adineh,
P., Corney, D., Stein, B., and Potthast, M. SemEval-
2019 task 4: Hyperpartisan news detection. In Proceedings of the 13th International Workshop on Semantic Evaluation, pp. 829–839, Minneapolis, Minnesota,
USA, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/S19-2145. URL https:
//aclanthology.org/S19-2145.
Kim, T., Cochez, M., François-Lavet, V., Neerincx,
M., and Vossen, P. A machine with short-term,
episodic, and semantic memory systems. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI'23/IAAI'23/EAAI'23. AAAI
Press, 2023. ISBN 978-1-57735-880-0. doi:
10.1609/aaai.v37i1.25075. URL https://doi.org/
10.1609/aaai.v37i1.25075.
Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
efficient transformer. CoRR, abs/2001.04451, 2020. URL
https://arxiv.org/abs/2001.04451.
Kuriscak, E., Marsalek, P., Stroffek, J., and Toth,
P. G. Biological context of hebb learning in
artificial neural networks, a review. Neurocomputing, 152:27–35, 2015. ISSN 0925-2312. doi:
https://doi.org/10.1016/j.neucom.2014.11.022. URL
https://www.sciencedirect.com/science/
article/pii/S0925231214015239.
Le, H., Tran, T., and Venkatesh, S. Self-attentive associative
memory. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5682–5691. PMLR, 13–18 Jul 2020.
URL https://proceedings.mlr.press/v119/
le20b.html.
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer,
L. BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
pp. 7871–7880, Online, July 2020a. Association for
12

--- TRANG 13 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Computational Linguistics. doi: 10.18653/v1/2020.acl-
main.703. URL https://aclanthology.org/
2020.acl-main.703.
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,
Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel,
T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Larochelle, H.,
Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 9459–9474. Curran Associates, Inc., 2020b.
URL https://proceedings.neurips.cc/
paper_files/paper/2020/file/
6b493230205f780e1bc26945df7481e5-
Paper.pdf.
Limbacher, T. and Legenstein, R. H-mem: Harnessing synaptic plasticity with hebbian memory
networks. In Larochelle, H., Ranzato, M., Hadsell,
R., Balcan, M., and Lin, H. (eds.), Advances in
Neural Information Processing Systems, volume 33,
pp. 21627–21637. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.cc/
paper_files/paper/2020/file/
f6876a9f998f6472cc26708e27444456-
Paper.pdf.
Mahoney, M. Large text compression benchmark,
2006. URL http://www.mattmahoney.net/dc/
text.html.
Martins, P. H., Marinho, Z., and Martins, A. F. T. ∞-former:
Infinite memory transformer. 9 2021. doi: 10.48550/
arxiv.2109.00301. URL https://arxiv.org/abs/
2109.00301v3.
Meng, Y. and Rumshisky, A. Context-aware neural model
for temporal information extraction. In Gurevych, I.
and Miyao, Y. (eds.), Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 527–536, Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1049. URL https:
//aclanthology.org/P18-1049.
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
Pointer sentinel mixture models. In 5th International
Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net, 2017. URL https:
//openreview.net/forum?id=Byj72udxe.
Morrissey, M. D., Insel, N., and Takehara-Nishiuchi, K.
Generalizable knowledge outweighs incidental details in
prefrontal ensemble code over time. eLife, 6:e22177, feb
2017. ISSN 2050-084X. doi: 10.7554/eLife.22177. URL
https://doi.org/10.7554/eLife.22177.
Murdock Jr., B. B. The serial position effect of free recall. Journal of Experimental Psychology, 64(5):482–488,
1962. ISSN 0022-1015(Print). doi: 10.1037/h0045106.
URL https://doi.org/10.1037/h0045106.
Nairne, J. S. and Pandeirada, J. N. Adaptive memory: Remembering with a stone-age brain. Current Directions
in Psychological Science, 17(4):239–243, 2008. doi:
10.1111/j.1467-8721.2008.00582.x. URL https://
doi.org/10.1111/j.1467-8721.2008.00582.x.
Peterson, L. R. and Peterson, M. J. Short-term retention
of individual verbal items. J Exp Psychol, 58:193–198,
September 1959.
Poo, M.-m., Pignatelli, M., Ryan, T. J., Tonegawa, S., Bonhoeffer, T., Martin, K. C., Rudenko, A., Tsai, L.-H., Tsien,
R. W., Fishell, G., Mullins, C., Gonçalves, J. T., Shtrahman, M., Johnston, S. T., Gage, F. H., Dan, Y., Long,
J., Buzsáki, G., and Stevens, C. What is memory? the
present state of the engram. BMC Biology, 14(1):40,
2016. doi: 10.1186/s12915-016-0261-6. URL https:
//doi.org/10.1186/s12915-016-0261-6.
Raaijmakers, J. G. and Shiffrin, R. M. Sam: A theory of probabilistic search of associative memory.
volume 14 of Psychology of Learning and Motivation, pp. 207–262. Academic Press, 1980. doi:
https://doi.org/10.1016/S0079-7421(08)60162-0. URL
https://www.sciencedirect.com/science/
article/pii/S0079742108601620.
Raaijmakers, J. G. and Shiffrin, R. M. Search of associative memory. Psychological Review, 88(2):93–134,
1981. doi: 10.1037/0033-295X.88.2.93. URL https:
//doi.org/10.1037/0033-295X.88.2.93.
Radford, A., Narasimhan, K., Salimans, T., and
Sutskever, I. Improving language understanding
by generative pre-training. 2018. URL https:
//www.cs.ubc.ca/~amuham01/LING530/
papers/radford2018improving.pdf.
Rae, J., Hunt, J. J., Danihelka, I., Harley, T., Senior, A. W.,
Wayne, G., Graves, A., and Lillicrap, T. Scaling memoryaugmented neural networks with sparse reads and writes.
In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and
Garnett, R. (eds.), Advances in Neural Information
Processing Systems, volume 29. Curran Associates, Inc.,
2016. URL https://proceedings.neurips.cc/
paper_files/paper/2016/file/
3fab5890d8113d0b5a4178201dc842ad-
Paper.pdf.
Rae, J., Dyer, C., Dayan, P., and Lillicrap, T. Fast parametric learning with activation memorization. In Dy,
13

--- TRANG 14 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp.
4228–4237. PMLR, 10–15 Jul 2018. URL https://
proceedings.mlr.press/v80/rae18a.html.
Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
and Lillicrap, T. P. Compressive transformers for longrange sequence modelling. In International Conference
on Learning Representations, 2020. URL https://
openreview.net/forum?id=SylKikSYDH.
Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich,
M., Gruber, L., Holzleitner, M., Adler, T., Kreil, D.,
Kopp, M. K., Klambauer, G., Brandstetter, J., and
Hochreiter, S. Hopfield networks is all you need.
In International Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=tL89RnzIiCd.
Roediger, H. L. Recall as a self-limiting process. Memory &
Cognition, 6(1):54–63, 1978. doi: 10.3758/BF03197428.
URL https://doi.org/10.3758/BF03197428.
Roediger, H. L. and Butler, A. C. The critical role of
retrieval practice in long-term retention. Trends in
Cognitive Sciences, 15(1):20–27, 2011. ISSN 1364-6613.
doi: https://doi.org/10.1016/j.tics.2010.09.003. URL
https://www.sciencedirect.com/science/
article/pii/S1364661310002081.
Rumelhart, D. E. and McClelland, J. L. Learning Internal Representations by Error Propagation, pp. 318–362.
1987.
Rundus, D. and Atkinson, R. C. Rehearsal processes
in free recall: A procedure for direct observation.
Journal of Verbal Learning and Verbal Behavior, 9(1):99–105, 1970. ISSN 0022-5371. doi:
https://doi.org/10.1016/S0022-5371(70)80015-9. URL
https://www.sciencedirect.com/science/
article/pii/S0022537170800159.
Shiffrin, R. M. and Atkinson, R. C. Storage and retrieval
processes in long-term memory. Psychological Review,
76(2):179–193, 1969. doi: 10.1037/h0027277. URL
https://doi.org/10.1037/h0027277.
Shiffrin, R. M. and Raaijmakers, J. The SAM retrieval
model: A retrospective and prospective., pp. 69–86. Essays in honor of William K. Estes, Vol. 1: From learning
theory to connectionist theory; Vol. 2: From learning processes to cognitive processes. Lawrence Erlbaum Associates, Inc, Hillsdale, NJ, US, 1992. ISBN 0-8058-1097-8
(Hardcover); 0-8058-0759-4 (Hardcover).
Song, S., Miller, K. D., and Abbott, L. F. Competitive hebbian learning through spike-timing-dependent synaptic
plasticity. Nature Neuroscience, 3(9):919–926, 2000. doi:
10.1038/78829. URL https://doi.org/10.1038/
78829.
Underwood, B. J. and Postman, L. Extraexperimental
sources of interference in forgeting. Psychol Rev, 67:
73–95, March 1960.
van de Ven, G. M., Siegelmann, H. T., and Tolias, A. S.
Brain-inspired replay for continual learning with artificial
neural networks. Nature Communications, 11(1):4069,
2020. doi: 10.1038/s41467-020-17866-2. URL https:
//doi.org/10.1038/s41467-020-17866-2.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.
Attention is all you need. In Guyon, I., Luxburg, U. V.,
Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc.,
2017. URL https://proceedings.neurips.cc/
paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-
Paper.pdf.
Waugh, N. C. and Norman, D. A. Primary memory. Psychological Review, 72(2):89–104, 1965. doi: 10.1037/
h0021797.
Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C.
Memorizing transformers. In International Conference
on Learning Representations, 2022. URL https://
openreview.net/forum?id=TrjbxzRcnf-.
Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J.,
Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang,
Q., Yang, L., and Ahmed, A. Big bird: Transformers
for longer sequences. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances
in Neural Information Processing Systems, volume 33,
pp. 17283–17297. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.cc/
paper_files/paper/2020/file/
c8512d142a2d849725f31a9a7a361ab9-
Paper.pdf.
14

--- TRANG 15 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
A. Thuộc tính Hebbian cho Memoria
Gerstner & Kistler (2002) đề xuất sáu thuộc tính của một mô hình dẻo dai hữu ích cho học Hebbian như sau. Memoria đáp ứng những thuộc tính này.
Tính Cục bộ Quy tắc học cho synapse Ei→j kết nối neuron j với neuron i chỉ nên phụ thuộc vào hoạt động của j và i và không phụ thuộc vào trạng thái của các neuron khác k≠i,j.
Ei→j = Counti,j/Counti,i
Theo định nghĩa, Memoria đáp ứng tính cục bộ vì nó chỉ phụ thuộc vào count của i,j.
Tính Hợp tác Công thức của Hebb 'tham gia vào việc kích hoạt nó' ngụ ý rằng việc tăng trọng số đòi hỏi cả neuron presynaptic và postsynaptic phải hoạt động.
Ei→j ∝ Counti,j
Vì Ei→j tỷ lệ thuận với Counti,j và Counti,i không bao giờ giảm, nó chỉ tăng khi ei và ej kích hoạt (được truy xuất) cùng nhau.
Ức chế Synaptic Một cơ chế giảm trọng số là yêu cầu cần thiết cho bất kỳ quy tắc học hữu ích nào. Có ba engram ei, ej, ek. Ei→j giảm khi ei và ek kích hoạt cùng nhau trong khi ej thì không. Chỉ số trên pre có nghĩa là giá trị trước khi kích hoạt ei và ej và post có nghĩa là giá trị sau khi kích hoạt.
Eprei→j = Countprei,j/Countprei,i
Countposti,k = Countprei,k + 1
Countposti,i = Countprei,i + 1
Eposti→j = Countposti,j/Countposti,i = Countprei,j/(Countprei,i + 1) < Eprei→j
Tính Bị chặn Trong các quy tắc thực tế, trọng số nên được giữ trong một phạm vi cụ thể. Ei→j phải nằm giữa 0 và 1 vì nó là xác suất.
Ei→j = P(ej ∈ Mrem|ei ∈ Mrem)
0 ≤ P(ej ∈ Mrem|ei ∈ Mrem) ≤ 1
Cạnh tranh Sự tăng trưởng của một số trọng số diễn ra với chi phí là sự giảm của những trọng số khác. Việc tăng Ei→j đòi hỏi việc tăng Counti,j và Counti,i. Việc tăng Counti,i làm giảm tất cả trọng số Ei→k, với k≠j.
Tính Ổn định Dài hạn Trong các hệ thống thích ứng, điều quan trọng là đảm bảo kiến thức đã thu được trước đây không bị quên. Trong Memoria, Ei→j luôn là kết quả của việc học từ tất cả các ví dụ trong quá khứ vì Count là tích lũy.
15

--- TRANG 16 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
B. Chi tiết về Hiệu ứng Bộ nhớ Tâm lý
B.1. Hiệu ứng Ưu tiên và Gần đây
0 20 40 60 80 100
Timestep at Creation (%)0100200300400500CountWikitext-103
PG-19
Hình 8: Thời gian tạo của engram còn lại trong bộ nhớ dài hạn sau khi trải qua tất cả bước thời gian. Trục x hiển thị thời gian tạo dưới dạng phần trăm của tổng bước thời gian, và trục y là số lượng engram được tạo tại thời điểm đó. Đường xu hướng là ước lượng mật độ kernel. Đồ thị minh họa sự hiện diện của cả hiệu ứng ưu tiên và gần đây, những hiện tượng thường được quan sát trong bộ nhớ con người, trong Memoria.
Bộ nhớ con người, khi xử lý thông tin được sắp xếp tuần tự, thể hiện những đặc tính nhất định. Hai trong số những đặc tính quan trọng nhất là hiệu ứng ưu tiên (Deese & Kaufman, 1957; Murdock Jr., 1962) và hiệu ứng gần đây (Bjork & Whitten, 1974; Baddeley & Hitch, 1993). Hiệu ứng ưu tiên là một thiên hướng nhận thức đề cập đến xu hướng của con người nhớ tốt hơn và cho tầm quan trọng lớn hơn đối với những phần thông tin đầu tiên, so với thông tin được trình bày sau đó. Hiện tượng này đặc biệt rõ ràng trong các tình huống mà cá nhân tiếp xúc với một loạt các mục hoặc kích thích, chẳng hạn như một danh sách từ, một chuỗi sự kiện hoặc một tập hợp các lập luận. Hiệu ứng gần đây là một thiên hướng nhận thức đề cập đến xu hướng của cá nhân nhớ tốt hơn và cho trọng lượng nhiều hơn đối với các mục hoặc thông tin gần đây nhất trong một loạt. Nói cách khác, khi con người được yêu cầu nhớ lại một danh sách các mục, họ có nhiều khả năng nhớ những mục xuất hiện cuối cùng trong danh sách.
Những hiệu ứng này có thể ảnh hưởng đến các khía cạnh khác nhau của bộ nhớ và ra quyết định. Cả hiệu ứng gần đây và ưu tiên đều được cho là liên quan đến cách thông tin được xử lý và lưu trữ trong bộ nhớ. Hiệu ứng gần đây được cho là bị ảnh hưởng bởi bộ nhớ ngắn hạn, nơi thông tin được trình bày gần đây vẫn dễ dàng có sẵn, trong khi hiệu ứng ưu tiên liên quan đến việc chuyển thông tin vào bộ nhớ dài hạn.
Để xác minh liệu những đặc tính vốn có trong con người này có rõ ràng trong Memoria hay không, chúng tôi đã phân tích engram tồn tại trong bộ nhớ dài hạn sau khi trải qua tất cả bước thời gian, xem xét thời điểm trong bước thời gian khi chúng được tạo. Hình 8 minh họa số lượng engram còn lại tại mỗi bước thời gian tạo. Như dự đoán, kết quả phân tích chứng minh sự hiện diện của cả hiệu ứng ưu tiên và gần đây trong Memoria. Trong phần đầu, có một dấu hiệu rõ ràng của hiệu ứng ưu tiên, vì nó thể hiện số lượng cao hơn so với phần giữa. Trong phần sau, có một sự khác biệt đáng kể, với số lượng cao hơn đáng chú ý so với phần giữa, cho thấy hiệu ứng gần đây rõ rệt. Trong Memoria, ký ức được tạo sớm có khả năng được tăng cường thông qua việc truy xuất tiếp theo. Điều này giống với hiệu ứng ưu tiên ở con người, nơi thông tin sớm có xu hướng được duy trì tốt do những cơ hội thường xuyên để luyện tập (Rundus & Atkinson, 1970). Những ký ức gần đây hơn có xác suất cao được bảo tồn do tuổi thọ còn lại của chúng, gây ra hiệu ứng gần đây. Ngoài ra, phân phối được quan sát của engram trên tổng thể các bước thời gian trong Memoria cho thấy việc giảm thiểu thành công vấn đề ban đầu của quên lãng định mệnh, đó là mục tiêu chính của nghiên cứu của chúng tôi.
16

--- TRANG 17 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
B.2. Hiệu ứng Tính Liền kề Thời gian
0 1000 2000 3000 4000 5000 6000 7000
Chênh lệch Tuổi thọ giữa Engrams0.00.10.20.30.40.50.60.7Trọng số Cạnh Trung bình
Hình 9: Biểu đồ hai biến thể hiện trọng số trung bình giữa tất cả engram theo chênh lệch tuổi thọ.
0 50 100 150 200 250 300
Chênh lệch Tuổi thọ giữa Engrams0.00.10.20.30.40.50.60.7Trọng số Cạnh Trung bình
Hình 10: Biểu đồ phân tán của dữ liệu trong Hình 9 cho chênh lệch tuổi thọ 300 hoặc ít hơn.
Hiệu ứng tính liền kề thời gian (Kahana, 1996; Ginns, 2006) là một hiện tượng nhận thức tăng cường khả năng ghi nhớ và hiểu biết khi các yếu tố thông tin được trình bày trong sự gần gũi thời gian. Nói cách khác, con người có xu hướng nhớ và hiểu thông tin tốt hơn khi nó được trình bày theo cách căn chỉnh thời gian, trái ngược với khi có khoảng cách thời gian giữa các yếu tố khác nhau. Hiệu ứng này tạo điều kiện cho việc hiểu các mối quan hệ giữa các sự kiện xảy ra trong khoảng thời gian ngắn.
Để điều tra sự xuất hiện của hiệu ứng tính liền kề thời gian trong Memoria, chúng tôi đã thực hiện phân tích kết nối giữa engram còn lại trong bộ dữ liệu test WikiText-103, sau khi trải qua tất cả bước thời gian. Chúng tôi trình bày mô tả về những thay đổi trong trọng số cạnh trung bình, dựa trên chênh lệch tuổi thọ (hoặc khoảng cách thời gian tại thời điểm tạo) giữa hai engram, trong Hình 9. Kết quả cho thấy rõ ràng rằng chênh lệch tuổi thọ tối thiểu dẫn đến sự tăng đáng kể trong trọng số trung bình. Điều này cho thấy sự tồn tại của hiệu ứng tính liền kề thời gian trong Memoria, tăng cường kết nối giữa engram gần nhau về thời gian. Hình 10 cung cấp biểu diễn chi tiết về điểm mà hiệu ứng này bắt đầu mờ dần, cho thấy sự hiện diện của nó lên đến khoảng 150 chênh lệch tuổi thọ. Về mặt lý thuyết, engram cùng tuổi thọ có xác suất cao hơn kích hoạt cùng nhau khi chúng tồn tại trong bộ nhớ làm việc và bộ nhớ ngắn hạn, dẫn đến sự tăng trong cạnh liên kết giữa chúng.
17

--- TRANG 18 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
B.3. Hiệu ứng Thực hành Truy xuất
Bảng 4: Hệ số tự tương quan của engram bộ nhớ ngắn hạn và dài hạn.
Lag Short-term Memory ACF Long-term Memory ACF
1 0.900 0.575
2 0.893 0.529
3 0.889 0.501
4 0.888 0.475
5 0.888 0.461
6 0.890 0.442
7 0.893 0.426
8 - 0.413
9 - 0.395
10 - 0.381
11 - 0.370
12 - 0.356
13 - 0.344
14 - 0.333
15 - 0.321
Chúng tôi đã thực hiện phân tích tự tương quan để hiểu mối liên kết giữa việc truy xuất mỗi engram và các sự kiện truy xuất tiếp theo bằng bộ dữ liệu Wikitext-103. Bảng 4 trình bày hệ số tự tương quan cho bộ nhớ ngắn hạn và dài hạn. Chúng tôi mã hóa việc truy xuất engram là 1 và không truy xuất là 0. Lag đại diện cho chênh lệch bước thời gian để tính tương quan. Ví dụ, lag của một biểu thị tự tương quan giữa việc truy xuất engram ei tại thời điểm t và việc truy xuất tại thời điểm t+1. Chúng tôi thu được hệ số tương quan cá nhân từ mỗi engram, sau đó tổng hợp chúng theo lag và tính trung bình. Đối với engram sống ngắn, với xu hướng luôn được truy xuất hoặc luôn không được truy xuất, hầu hết những engram đó có phương sai bằng 0. Chúng tôi coi hệ số trong những trường hợp này là một vì ý nghĩa thực tế của nó là tự tương quan rất mạnh. Ngoài ra, đối với bộ nhớ dài hạn, chúng tôi tính trung bình có trọng số của hệ số tương quan tỷ lệ với tuổi thọ của mỗi engram, vì tổng tuổi thọ khác nhau cho mỗi engram.
Đầu tiên, nhìn vào bộ nhớ ngắn hạn, dung lượng của bộ nhớ ngắn hạn là 400, vì vậy mỗi bộ nhớ ở lại trong bộ nhớ ngắn hạn trong 8 bước thời gian. Do đó, lag tối đa có thể quan sát được là 7. Mỗi engram trong bộ nhớ ngắn hạn có tự tương quan cao đáng kể. Điều này ngụ ý rằng một khi engram được truy xuất, nó dễ dàng được truy xuất lại, cho thấy rằng một bộ nhớ cụ thể thường xuyên liên kết với những bộ nhớ khác. Bộ nhớ dài hạn cũng cho thấy tự tương quan đáng kể, hiển thị tương quan mạnh trong khoảng thời gian gần mà dần giảm trong thời gian kéo dài. Về mặt lý thuyết, một khi engram trong bộ nhớ dài hạn được truy xuất, mối liên kết với ký ức gần đây hơn được tăng cường, làm cho bộ nhớ cũ dễ dàng tiếp cận hơn thông qua con đường của những ký ức gần đây đó. Trong bộ nhớ con người, một sự kiện truy xuất bản thân làm cho thông tin được truy xuất dễ truy xuất hơn trong tương lai (Bjork, 1975; Roediger, 1978; Bjork, 1988), một hiện tượng được gọi là hiệu ứng kiểm tra hoặc hiệu ứng thực hành truy xuất. Tự tương quan cao của các sự kiện truy xuất trong Memoria một phần ngụ ý sự biểu hiện của những hiện tượng như vậy. Hình 11 minh họa những thay đổi trong tự tương quan dựa trên lag trong bộ nhớ dài hạn.
0 10 20 30 40 50
Lag0.20.40.60.81.0Autocorrelation
Hình 11: Biểu đồ hệ số tự tương quan của bộ nhớ dài hạn.
18

--- TRANG 19 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
C. Nghiên cứu Ablation
C.1. Loại Bộ nhớ
Bảng 5: Hiệu suất và cải thiện hiệu suất của mỗi module bộ nhớ theo chiều dài của bộ dữ liệu. Memoria liên tục thể hiện hiệu suất xuất sắc, ngay cả khi chiều dài chuỗi tăng. Điều này được gán cho sự tương tác hài hòa của mỗi module bộ nhớ. Quan sát này cho thấy rằng trong khi hiệu quả của bộ nhớ làm việc giảm với chiều dài chuỗi mở rộng, cả module bộ nhớ ngắn hạn và dài hạn đều hiển thị hiệu suất tăng cường.
4K 8K 16K 32K 48K
Số Segments 4 8 16 32 47
Độ chính xác
Transformer 36.19 33.79 31.69 29.94 19.04
+ Working Memory 79.69 70.85 62.21 52.01 34.32
+ Short-term Memory 82.66 76.20 66.37 58.75 54.87
+ Long-term Memory 82.27 74.08 66.58 63.42 63.26
Cải thiện Hiệu suất
+ Working Memory +43.50 +37.06 +30.52 +22.07 +15.28
+ Short-term Memory +2.79 +5.35 +4.16 +6.74 +20.55
+ Long-term Memory -0.39 -2.12 +0.21 +4.67 +8.39
Chúng tôi thực hiện một nghiên cứu ablation để xem xét ảnh hưởng của mỗi loại bộ nhớ trong Memoria đối với hiệu suất. Nghiên cứu này, được thực hiện trên nhiệm vụ sắp xếp với độ dài segment cố định là 1024 cho mỗi bộ dữ liệu, cho phép chúng tôi quan sát xu hướng liên quan đến độ dài dữ liệu. Để mở rộng điều tra của chúng tôi đến bộ dữ liệu dài hơn không được thảo luận trong văn bản chính, chúng tôi thực hiện thí nghiệm bổ sung với bộ dữ liệu 48K. Vì độ dài segment vẫn không đổi ở 1024, việc mở rộng độ dài bộ dữ liệu do đó tăng số lượng segment.
Kết quả phân tích cho thấy rằng mỗi loại module bộ nhớ đóng góp vào hiệu suất tổng thể ở một mức độ nào đó. Một quan sát đáng chú ý là khi số lượng segment tăng, ảnh hưởng của mỗi loại bộ nhớ đối với hiệu suất thay đổi. Nhìn kỹ hơn vào kết quả bộ dữ liệu 4K, với độ dài segment chỉ 4, cho thấy rằng phần lớn cải thiện hiệu suất chủ yếu được thúc đẩy bởi bộ nhớ làm việc. Tuy nhiên, khi độ dài bộ dữ liệu mở rộng đến 8K, 16K và hơn nữa, sự tăng cường hiệu suất được gán cho bộ nhớ làm việc nhanh chóng giảm. Ngược lại, với việc mở rộng độ dài chuỗi, tác động của cả bộ nhớ ngắn hạn và dài hạn đối với hiệu suất dần tăng cường. Xu hướng này phản ánh bộ nhớ con người, khác nhau về thời gian giữ lại tùy thuộc vào tình huống.
Xu hướng này cho thấy rằng mô hình không sử dụng tất cả các loại bộ nhớ một cách đồng đều, mà đúng hơn là sử dụng thông tin bộ nhớ một cách có chọn lọc dựa trên đặc tính nhiệm vụ hoặc bộ dữ liệu. Nếu nhiệm vụ có thể được xử lý đầy đủ với sự hiểu biết về ngữ cảnh ngắn, mô hình chủ yếu sử dụng bộ nhớ làm việc. Tuy nhiên, khi đối mặt với ngữ cảnh dài hơn gây thách thức cho chỉ bộ nhớ làm việc, mô hình dường như trau dồi khả năng khai thác bộ nhớ ngắn hạn hoặc dài hạn. Đáng chú ý, khi quan sát quá trình chuyển từ 32K đến 48K, rõ ràng là sự khác biệt hiệu suất cuối cùng giữa 32K và 48K là không đáng kể khi tất cả bộ nhớ được tham gia, nhờ vào vai trò bổ sung của bộ nhớ ngắn hạn và dài hạn bù đắp cho những thiếu sót hiệu suất bị trầm trọng hóa thêm trong Transformer hoặc bộ nhớ làm việc. Những hiểu biết này ngụ ý rằng các nhiệm vụ và bộ dữ liệu tương lai nên đòi hỏi đủ sự phụ thuộc vào ngữ cảnh dài hạn để xác thực hiệu quả khả năng bộ nhớ dài hạn của mô hình. Memoria liên tục thể hiện hiệu suất mạnh mẽ trên các bộ dữ liệu có độ dài đa dạng, nhờ vào vai trò bổ sung của ba loại bộ nhớ.
C.2. Thuộc tính Hebbian
Chúng tôi thực hiện thí nghiệm thêm để xem xét hiệu quả và hiệu suất của việc điều chỉnh trọng số, được hướng dẫn bởi lý thuyết Hebbian. Thí nghiệm này được thực hiện bằng bộ dữ liệu có độ dài 48K trong nhiệm vụ sắp xếp, mà cải thiện hiệu suất của bộ nhớ dài hạn được thiết lập từ nghiên cứu ablation trước đó. Bảng 6 trình bày kết quả thí nghiệm.
Đầu tiên, thuật ngữ 'Random Wire' đề cập đến điều kiện được sửa đổi bởi Memoria để chọn ngẫu nhiên engram mục tiêu. Memoria tăng cường kết nối giữa tất cả bộ nhớ làm việc và engram được truy xuất, ký hiệu là Mact. Do đó, nó tăng tất cả Counti,j thỏa mãn ei ∈ Mact và ej ∈ Mact. Ngược lại, trong điều kiện random wire, nó tăng Counti,j thỏa mãn ei ∈ Mact và ej ∈ Mltm. Tổng số gia count được kiểm soát. Phát hiện thí nghiệm cho thấy
19

--- TRANG 20 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Bảng 6: Hiệu suất và cải thiện hiệu suất khi sửa đổi đặc tính Hebbian. Kết quả này nhấn mạnh rằng việc sửa đổi trọng số, dựa trên lý thuyết Hebbian trong Memoria, mang lại cải thiện hiệu suất vượt trội so với việc tăng trọng số tùy ý. Hơn nữa, Memoria thể hiện hiệu quả trong việc bảo tồn một phần đáng kể hiệu suất đạt được khi thực hiện tìm kiếm đầy đủ toàn bộ bộ nhớ dài hạn.
Độ chính xác Cải thiện Hiệu suất
Memoria Transformer (Without LTM) 54.87 -
Memoria Transformer 63.26 +8.39
Memoria Transformer (Random Wire) 56.13 +1.26
Memoria Transformer (Full LTM Search) 65.84 +10.97
việc thay thế quá trình Hebbian bằng kết nối ngẫu nhiên dẫn đến sự giảm đáng chú ý trong cải thiện hiệu suất của bộ nhớ dài hạn, giảm từ 8.39 xuống 1.26, tức khoảng một phần sáu giá trị ban đầu. Những kết quả này làm nổi bật hiệu quả đáng kể của việc điều chỉnh trọng số theo quá trình Hebbian đối với hiệu suất.
Thuật ngữ 'Full LTM Search' đề cập đến việc chọn engram cao nhất bằng cách đơn giản tính Cltm trên tất cả bộ nhớ dài hạn mà không sử dụng trọng số nội bộ trong Memoria. Nói đơn giản hơn, nó coi Mltm như Mfound_ltm. Trong trường hợp này, có cải thiện hiệu suất là 2.58. Những phát hiện này chứng minh rằng tìm kiếm bộ nhớ dài hạn của Memoria bảo tồn một phần đáng kể hiệu suất trong khi sử dụng ít tài nguyên tính toán hơn đáng kể so với cách tiếp cận ngây thơ.
D. Chi tiết Huấn luyện và Kết quả Bổ sung
Cho tất cả thí nghiệm, optimizer Adam và scheduler tuyến tính với warm-up được sử dụng, và gradient clipping được đặt ở norm 1.0. Một hoặc nhiều GPU NVIDIA A100 hoặc A6000 được sử dụng để huấn luyện.
D.1. Sắp xếp
Cho tất cả thí nghiệm sắp xếp, batch size 32, tỷ lệ warmup 0.06, learning rate 2e-4 và epoch 5 được sử dụng cho 80.000 ví dụ train. Độ dài bộ nhớ được cấu hình để khớp với độ dài segment. Thí nghiệm được thực hiện trên bộ dữ liệu có độ dài từ 1000 đến 32.000. Mỗi ví dụ trên bộ dữ liệu được chia thành segments có độ dài 256, 512 và 1024. Cho mỗi độ dài segment, các kết hợp của độ dài chuỗi và độ dài segment được xây dựng bằng cách thay đổi số lượng segment, được đặt là 4, 8, 16 và 32. Cấu hình mô hình sử dụng là Transformer 5 lớp, 4 head, chiều nhúng 512. Tỷ lệ nén là 4 và tỷ lệ bộ nhớ bình thường và nén là một-đối-một cho Compressive Transformer.
Tham số Memoria được sử dụng trong thí nghiệm như sau: tuổi thọ ban đầu là 5, tỷ lệ mở rộng tuổi thọ α là 8, và độ sâu tìm kiếm bộ nhớ dài hạn Ndepth là 10 trong tất cả trường hợp. Các tham số khác được điều chỉnh tỷ lệ với độ dài segment. số lượng bộ nhớ làm việc Nwm được đặt thành 1/8 độ dài segment, số lượng engram được truy xuất trong bộ nhớ ngắn hạn Nrem_stm được đặt thành 1/4 độ dài segment, số lượng engram được truy xuất trong bộ nhớ dài hạn Nrem_ltm được đặt thành 5/8 độ dài segment, và dung lượng bộ nhớ ngắn hạn được đặt thành một nửa độ dài segment. Tổng của Nwm, Nrem_stm và Nrem_ltm bằng độ dài segment.
Bảng 7 cho thấy tất cả điểm số của các mô hình trong nhiệm vụ sắp xếp. Chỉ số là độ chính xác. Để thuận tiện cho việc so sánh, chúng tôi đánh dấu số lượng segment thay vì tổng độ dài chuỗi của mỗi bộ dữ liệu. Độ dài chuỗi có thể được thu được bằng cách nhân số lượng segment với độ dài segment. Memoria Transformer chứng minh tính mạnh mẽ của nó đối với phụ thuộc dài hạn so với các mô hình khác, đặc biệt khi số lượng segment tăng.
D.2. Mô hình hóa Ngôn ngữ
Cho tất cả thí nghiệm mô hình hóa ngôn ngữ, batch size 8 và tỷ lệ warmup 0.06 được sử dụng. Cấu hình mô hình sử dụng cài đặt của GPT-2 small theo mặc định. Bộ dữ liệu Wikitext-103 và PG-19 được huấn luyện trong 3 epoch, trong khi bộ dữ liệu Enwik8 được huấn luyện trong 20 epoch. Tokenizer GPT-2 được sử dụng cho tất cả bộ dữ liệu trừ Enwik8, được huấn luyện ở cấp độ ký tự sử dụng 204 ký tự. Learning rate mặc định là 2e-4, nhưng trong trường hợp khó hội tụ, 1e-4 được sử dụng. Tuy nhiên, cho thí nghiệm fine-tuning mô hình được huấn luyện sẵn, learning rate 5e-5 được sử dụng. Trong thí nghiệm
20

--- TRANG 21 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Bảng 7: Độ chính xác trong nhiệm vụ sắp xếp. Khi segments tăng, Memoria vượt trội hơn các baseline khác.
Model SegmentsĐộ dài Segment
256 512 1024
Transformer-XL 4 74.66 60.46 68.86
Compressive Transformer 4 64.38 64.57 79.51
∞-former 4 84.49 83.75 84.28
Memoria Transformer 4 80.42 80.99 82.27
Transformer-XL 8 36.24 37.41 40.09
Compressive Transformer 8 56.88 49.58 71.84
∞-former 8 70.21 75.55 74.34
Memoria Transformer 8 70.84 74.47 74.08
Transformer-XL 16 32.75 34.59 35.06
Compressive Transformer 16 35.57 37.69 44.03
∞-former 16 53.61 53.61 47.31
Memoria Transformer 16 63.99 64.50 66.58
Transformer-XL 32 32.24 32.76 33.87
Compressive Transformer 32 32.68 33.15 35.07
∞-former 32 34.36 36.41 39.71
Memoria Transformer 32 50.08 56.48 63.42
được thực hiện trên bộ dữ liệu Wikitext-103 sử dụng Transformer-XL và trên bộ dữ liệu PG-19 sử dụng ∞-former, cũng như thí nghiệm với độ dài segment giảm xuống 50, cả Memoria Transformer và Transformer-XL đều được huấn luyện với learning rate 1e-4. Độ dài bộ nhớ được đặt bằng hoặc tương tự độ dài segment. Tỷ lệ nén là 4 và tỷ lệ bộ nhớ bình thường và nén là một-đối-một cho Compressive Transformer.
Tham số Memoria được đặt như sau: tuổi thọ ban đầu 9, tỷ lệ mở rộng tuổi thọ α là 8, và độ sâu tìm kiếm bộ nhớ dài hạn Ndepth là 10. Hơn nữa, để ngăn ngừa can thiệp tiềm ẩn với quá trình học, chúng tôi định kỳ reset tất cả bộ nhớ trong Memoria mỗi 500 bước trong quá trình huấn luyện (1500 bước cho bộ dữ liệu enwik8). Điều này được thực hiện để tránh tham chiếu bộ nhớ được tạo ở các giai đoạn mà việc học không đầy đủ, vì nó có thể cản trở tiến trình huấn luyện. Cho bộ dữ liệu Wikitext-103 và PG-19, số lượng bộ nhớ làm việc Nwm, số lượng engram được truy xuất trong bộ nhớ ngắn hạn Nrem_stm, và số lượng engram được truy xuất trong bộ nhớ dài hạn Nrem_ltm đều được đặt thành 50, và dung lượng bộ nhớ ngắn hạn được đặt thành 400. Cho bộ dữ liệu Enwik8, Nwm, Nrem_stm và Nrem_ltm được đặt thành 170, và dung lượng bộ nhớ ngắn hạn được đặt thành 1360. Khi huấn luyện trên bộ dữ liệu Wikitext-103 với độ dài segment giảm xuống 50, Nwm, Nrem_stm và Nrem_ltm đều được đặt thành 16, và dung lượng bộ nhớ ngắn hạn được đặt thành 128.
Bảng 8: Hiệu suất Finetuning trên Wikitext-103.
Model Wikitext-103
GPT-2 20.498
Memoria GPT-2 18.986
GPT-2 Large 15.332
Memoria GPT-2 Large 13.227
GPT-2 XL 15.254
Memoria GPT-2 XL 13.241
Để xác minh liệu Memoria có thể xem xét ngữ cảnh dài hạn ngay cả khi fine-tuning mô hình được huấn luyện sẵn hay không, chúng tôi đo hiệu suất trên bộ dữ liệu Wikitext-103 bằng cách fine-tuning Memoria GPT-2. Kiến trúc của Memoria GPT-2 giống như Memoria Transformer. Kết quả là Bảng 8. Memoria GPT-2 cho thấy hiệu suất tốt hơn đáng kể so với GPT-2. Kết quả này cho thấy rằng Memoria có thể được kết hợp với các mô hình được huấn luyện sẵn khác nhau để tăng phụ thuộc dài hạn. Hơn nữa, vì việc sử dụng mô hình ngôn ngữ lớn được huấn luyện sẵn (LLMs) đã trở nên phổ biến, chúng tôi thực hiện thí nghiệm để xác minh liệu Memoria có thể được áp dụng kết hợp với LLMs hay không. Chúng tôi thực hiện thí nghiệm sử dụng mô hình kích thước large và xl, và thành công đạt được cải thiện hiệu suất khi áp dụng Memoria vào các mô hình được huấn luyện sẵn lớn hơn. Điều này chứng minh tiềm năng cho LLMs hưởng lợi từ việc xem xét ngữ cảnh dài hơn với sự giúp đỡ của Memoria.
21

--- TRANG 22 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Bảng 9: Hiệu suất trên Wikitext-103 dưới các biến đổi tham số khác nhau.
Initial lifespan Lifespan extend scale α LTM search depth Reset Period Nwm Nrem_stm Nrem_ltm Perplexity
9 8 10 500 50 50 50 23.471
5 23.485
4 23.518
5 23.491
100 23.407
100 25 25 23.376
25 100 25 23.831
25 25 100 23.670
Chúng tôi thực hiện thí nghiệm bổ sung bằng cách sửa đổi các siêu tham số khác nhau để điều tra độ nhạy của siêu tham số. Bảng 9 cho thấy rằng nói chung không có sự khác biệt đáng kể về hiệu suất so với những gì được báo cáo ban đầu trong bài báo. Thậm chí có những trường hợp hiệu suất được cải thiện so với điểm số ban đầu.
D.3. Phân loại
Tất cả thí nghiệm phân loại văn bản hyperpartisan được thực hiện với batch size 16, learning rate 5e-5 và tỷ lệ warmup 0.1. Các mô hình được huấn luyện trong 20 epoch. Cho BERT, thí nghiệm sử dụng mô hình bert-base-uncased được huấn luyện sẵn. Đối với Longformer, mô hình base được sử dụng trong thí nghiệm.
Tham số Memoria được sử dụng trong thí nghiệm như sau: tuổi thọ ban đầu 12, tỷ lệ mở rộng tuổi thọ α là 8, độ sâu tìm kiếm bộ nhớ dài hạn Ndepth là 10, số lượng bộ nhớ làm việc Nwm được đặt thành 64, số lượng engram được truy xuất trong bộ nhớ ngắn hạn Nrem_stm và số lượng engram được truy xuất trong bộ nhớ dài hạn Nrem_ltm đều được đặt thành 64, dung lượng bộ nhớ ngắn hạn là 128, và chỉ số lớp bộ nhớ được đặt thành 9. Điều này có nghĩa là đầu ra của lớp thứ 10 được sử dụng làm bộ nhớ, và nó được tham chiếu trong 2 lớp còn lại của mô hình.
E. Thuật toán & Độ phức tạp Tính toán
E.1. Phân tích Lý thuyết
Mỗi giai đoạn của Memoria được đại diện bởi một thuật toán. Đây là các thuật toán của mô hình decoder trong thí nghiệm của chúng tôi, vì vậy một số chi tiết có thể hơi khác so với công thức của mô hình encoder. Ngoài ra, mỗi thuật toán cung cấp độ phức tạp thời gian để giúp ước tính cần bao nhiều tài nguyên.
Độ phức tạp của hàm CalculateDistance bằng tích số của số lượng engram trong mỗi bộ nhớ, vì nó liên quan đến việc tính toán tất cả trọng số giữa chúng. Hàm được sử dụng hai lần, đầu tiên trong STM với độ phức tạp thời gian O(Nwm × Cstm), trong đó Nwm là số lượng engram trong bộ nhớ làm việc và Cstm là dung lượng của STM. Thứ hai, khi áp dụng cho LTM được tìm thấy, độ phức tạp là O(Nwm × Nfound_ltm), trong đó Nfound_ltm = Nrem_stm × (Ndepth + 1). Phần của hàm truy xuất xác suất có điều kiện của việc truy xuất engram LTM được kết nối cho engram STM được truy xuất có độ phức tạp O(Nrem_stm × d), trong đó Nrem_stm là số lượng engram được truy xuất trong STM và d là bậc. Giá trị tối đa cho bậc d là tổng số cạnh từ engram, dẫn đến độ phức tạp tối đa O(Nrem_stm × Nltm). Trong vòng lặp thực hiện Ndepth lần, độ phức tạp là O(Nrem_stm × Nltm × Ndepth). Nói chung, vì kích thước của LTM được dự kiến lớn hơn Nwm, độ phức tạp thời gian tổng thể của giai đoạn retrieve là O(Nrem_stm × Nltm × Ndepth).
Ở đây, Nrem_stm và Ndepth là siêu tham số có thể được đặt trực tiếp, nhưng tổng số đơn vị bộ nhớ dài hạn, Nltm, là giá trị thay đổi động trong quá trình thực thi. Mặc dù không thể xác định chính xác kích thước của LTM, kích thước tối đa của LTM theo thời gian có thể chứng minh sự hội tụ thông qua tuổi thọ, với thời gian đủ. Sự tăng tuổi thọ cho tất cả engram trong một lần thực hiện toàn bộ các thao tác bộ nhớ là α*(Nrem_stm + Nrem_ltm) khi alpha đại diện cho tham số tỷ lệ mở rộng tuổi thọ. Ngoài ra, sự giảm tuổi thọ là số lượng tất cả engram của
22

--- TRANG 23 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Thuật toán 1 Giai đoạn Retrieve
Input: bộ nhớ ngắn hạn STM, bộ nhớ dài hạn LTM, memory encoder E, xác suất có điều kiện đồng truy xuất P, trạng thái ẩn trước hp, độ sâu tìm kiếm bộ nhớ dài hạn Ndepth
Output: bộ nhớ làm việc WM, engram được truy xuất retrieved
Result: Mã hóa hp vào bộ nhớ làm việc. Tìm engram liên quan trong bộ nhớ ngắn hạn/dài hạn.
WM ← E(hp)
Wstm ← CalculateDistance(STM, WM) {khoảng cách từ stm đến wm}
stmrem ← FindShortestK(Wstm) {chọn stm gần nhất}
p ← GetCondProb(LTM, stmrem, P)
ltm1 ← SelectMostProbableEngrams(p)
ltmfound ← (ltm1,)
for i ← 1 to Ndepth do
p ← GetCondProb(LTM, ltmi, P)
ltmi+1 ← SelectMostProbableEngrams(p)
Append(ltmfound, ltmi+1)
end for
Wltm ← CalculateDistance(ltmfound, WM)
ltmrem ← FindShortestK(Wltm)
retrieved ← Merge(stmrem, ltmrem)
Nltm + Nstm + Nwm. Trong tình huống mà Nltm được tối đa hóa, tuổi thọ được phân phối đều trên tất cả engram, ngăn chặn việc loại bỏ chúng. Nếu tổng tuổi thọ cho tất cả engram sau lần thực hiện thứ n được ký hiệu là l, thì Nltm có thể được coi là bội số không đổi, l×c. Tuy nhiên, vì tổng số engram không thể vượt quá tổng tuổi thọ, c nhận giá trị giữa 0 và 1. Khi thao tác bộ nhớ được thực hiện n lần, và tổng tuổi thọ của tất cả engram là ln, ln có thể được biểu diễn như sau.
ln+1 = ln + α*(Nrem_stm + Nrem_ltm) - Nltm
= ln + α*(Nrem_stm + Nrem_ltm) - ln×c
= (1-c)×ln + K
K = α*(Nrem_stm + Nrem_ltm)
ln+1 - K/c = (1-c)×(ln - K/c)
bn+1 = (1-c)×bn
bn = b0×(1-c)^n
ln = b0×(1-c)^n + K/c
= b0×(1-c)^n + α*(Nrem_stm + Nrem_ltm)/c
lim(n→∞) ln = α*(Nrem_stm + Nrem_ltm)/c
lim(n→∞) Nltm = α*(Nrem_stm + Nrem_ltm)
Cuối cùng, khi thời gian đủ trôi qua, tổng tuổi thọ sẽ tỷ lệ với α*(Nrem_stm + Nrem_ltm). Do đó, trong tình huống xấu nhất của giai đoạn retrieve, độ phức tạp thời gian như sau.
O(Nrem_stm × Nltm × Ndepth) = O(Nrem_stm × (α*(Nrem_stm + Nrem_ltm)) × Ndepth)
= O(αNrem_stmNdepth(Nrem_stm + Nrem_ltm))
23

--- TRANG 24 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Thuật toán 2 Giai đoạn Exploit
Input: mô hình M, segment đầu vào s, retrieved
Output: kết quả segment r, trạng thái ẩn hp
Result: Thực hiện suy luận với ký ức được truy xuất. Trả về kết quả segment, trạng thái ẩn và trọng số attention cho mỗi engram.
r, hp, a ← M(s, retrieved) {"a" có nghĩa là trọng số attention bộ nhớ}
Độ phức tạp thời gian của giai đoạn exploit phụ thuộc vào cách mô hình hóa việc sử dụng engram được truy xuất. Trong triển khai của chúng tôi, chúng tôi sử dụng cơ chế cross-attention, trong đó dữ liệu đầu vào được sử dụng làm query cho engram phục vụ như key và value. Do đó, độ phức tạp thời gian phù hợp với của cross-attention. Độ phức tạp thời gian, cho độ dài đầu vào L và số lượng engram được truy xuất Ne, là O(L × Ne). Ne bằng Nrem_stm + Nrem_ltm, vì vậy độ phức tạp thời gian là O(L × (Nrem_stm + Nrem_ltm)). Chúng tôi cấu hình tổng số engram được sử dụng trong thí nghiệm bằng với độ dài chuỗi. Trong tình huống này, độ phức tạp thời gian trở thành O(L²), tương đương với của self-attention, do đó không gây tác động bổ sung đến độ phức tạp thời gian tổng thể từ góc độ Big-O.
Thuật toán 3 Giai đoạn Memorize & Forget
Input: WM, STM, LTM, P.
Output: Bộ nhớ và bảng điều kiện được cập nhật.
P ← AdjustConditionalProbs(P, retrieved) {cập nhật probs}
IncreaseLifespans(retrieved, a)
STM ← MoveWMtoSTM(WM, STM)
DecreaseLifespanAndRemove(STM, LTM)
LTM ← MoveSTMtoLTM(STM, LTM)
Logic điều chỉnh xác suất có điều kiện tăng giá trị cho mỗi cặp engram được truy xuất, dẫn đến độ phức tạp thời gian O(N²e). Logic điều chỉnh tuổi thọ, là thao tác cho mỗi engram, đòi hỏi độ phức tạp O(Ne). Thay đổi loại bộ nhớ đòi hỏi thao tác tỷ lệ với số lượng engram, giới hạn độ phức tạp ở O(Ne). Do đó, độ phức tạp thời gian tổng thể ở giai đoạn này là O(N²e) = O((Nrem_stm + Nrem_ltm)²).
Trong Memoria, độ phức tạp không gian về cơ bản là chi phí duy trì bảng xác suất có điều kiện đại diện cho kết nối giữa mỗi engram. Độ phức tạp không gian phụ thuộc vào việc triển khai đồ thị. Để thuận tiện triển khai, chúng tôi sử dụng biểu diễn ma trận kề. Khi sử dụng ma trận kề, độ phức tạp không gian trở thành bậc hai theo số nút, cụ thể, bình phương tổng số engram trong Memoria, được tính là O((Nwm + Cstm + Nltm)²). Các triển khai thay thế như danh sách kề có thể giảm độ phức tạp không gian hơn nữa.
Bảng 10: Độ phức tạp thời gian và không gian ở mỗi giai đoạn.
Stage Time Complexity Space Complexity
Retrieve O(Nrem_stmNltmNdepth) O((Nwm + Cstm + Nltm)²)
Exploit O(L(Nrem_stm + Nrem_ltm)) O((Nwm + Cstm + Nltm)²)
Memorize & Forget O((Nrem_stm + Nrem_ltm)²) O((Nwm + Cstm + Nltm)²)
Bảng 10 cho thấy độ phức tạp thời gian và không gian cho mỗi giai đoạn sử dụng ký hiệu Big-O.
E.2. Phân tích Thực nghiệm
Ngoài phân tích lý thuyết, chúng tôi đo tài nguyên được sử dụng trong quá trình suy luận. Bảng 11 trình bày thời gian thực hiện và sử dụng bộ nhớ trung bình trong khi suy luận tập test Wikitext-103. Về thời gian thực hiện, Memoria nhanh hơn ∞-former tương đối nhưng chậm hơn những cái khác, trong khi sử dụng bộ nhớ của nó là nhỏ thứ hai. Đo bộ nhớ được thực hiện ở mỗi bước sử dụng các hàm torch.cuda.empty_cache và torch.cuda.memory_allocated được cung cấp bởi PyTorch, và trung bình được tính. Trong trường hợp Memoria, có chỗ để tối ưu hóa thời gian suy luận
24

--- TRANG 25 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
Bảng 11: Thời gian suy luận và sử dụng bộ nhớ GPU
Model Execution Time (s) Memory Usage (MB)
Transformer-XL 21.23 525.74
Compressive Transformer 25.57 740.00
∞-former 54.15 676.12
Memoria Transformer 44.21 612.94
và sử dụng bộ nhớ. Điều này có thể đạt được bằng cách tải thông tin, chẳng hạn như loại engram hoặc tuổi thọ, đòi hỏi tính toán GPU tối thiểu, lên CPU. Ngoài ra, việc tối ưu hóa cấu trúc dữ liệu bằng danh sách kề có thể được áp dụng. Hơn nữa, đồ thị được duy trì bởi Memoria không nhất thiết phải được triển khai bằng PyTorch hoặc Python, cho phép tăng hiệu quả tính toán thông qua việc sử dụng ngôn ngữ lập trình hiệu quả hơn.
F. Thảo luận RAG
Gần đây, đã có sự quan tâm đáng kể đến Retrieval Augmented Generation (RAG) (Lewis et al., 2020b), đã chứng minh hiệu suất đáng chú ý trong mô hình hóa ngôn ngữ. Do đó, người ta có thể muốn so sánh Memoria với RAG. RAG có thể thay thế cho Memoria không? Mục tiêu của nghiên cứu chúng tôi là giải quyết vấn đề xử lý chuỗi dài. Mô hình hóa ngôn ngữ là một trong những nhiệm vụ chúng tôi thực hiện để đánh giá khả năng của Memoria. Vì mô hình hóa ngôn ngữ đòi hỏi nhiều thử thách đan xen khác nhau, sẽ có nhiều cách để cải thiện hiệu suất. Trong khi RAG hiệu quả tăng cường hiệu suất bằng cách cung cấp thông tin ngoài như đầu vào bổ sung để bù đắp cho kiến thức nội bộ hạn chế của mô hình ngôn ngữ, nó không liên quan trực tiếp đến việc giải quyết vấn đề xử lý chuỗi dài. Hơn nữa, RAG sử dụng văn bản được truy xuất như token đầu vào bổ sung, có khả năng đòi hỏi khả năng xử lý chuỗi dài nhiều hơn. Trong những trường hợp như vậy, chúng tôi dự đoán rằng Memoria có thể được sử dụng để xử lý hiệu quả độ dài chuỗi tăng do văn bản được truy xuất dài. Do đó, chúng tôi so sánh Memoria với các phương pháp bộ nhớ ngoài thông thường khác để xác thực hiệu suất xử lý chuỗi dài của nó và không thực hiện so sánh với RAG. RAG điển hình và Memoria có mục tiêu khác nhau nhằm giải quyết.
Liệu chúng ta có thể sử dụng các segment đầu vào quá khứ đơn giản như thư viện truy xuất cho RAG, cho phép áp dụng của nó trong bối cảnh xử lý chuỗi dài? Thật không may, khi chúng ta xem xét áp dụng kỹ thuật RAG điển hình vào các vấn đề xử lý chuỗi dài, dường như có nhiều khó khăn. Chủ yếu, từ góc độ triển khai thực tế, việc duy trì chỉ mục truy xuất đặt ra trở ngại đáng kể. Khi sử dụng kỹ thuật truy xuất mà không có phương pháp lập chỉ mục hiệu quả, việc tìm kiếm đầy đủ trên toàn bộ thư viện truy xuất trở nên cần thiết. Do đó, các kỹ thuật tối ưu hóa sử dụng công cụ như Faiss (Douze et al., 2024) hoặc ScaNN (Guo et al., 2020b) được sử dụng phổ biến để xây dựng chỉ mục truy xuất. Trong khi những phương pháp lập chỉ mục này giảm đáng kể thời gian suy luận trong quá trình truy xuất, việc lập chỉ mục ban đầu bản thân đòi hỏi thời gian và tài nguyên tính toán rộng lớn. Các kỹ thuật lập chỉ mục hiệu quả (Andoni et al., 2015; Baranchuk et al., 2018; Guo et al., 2020a;b) đòi hỏi giai đoạn huấn luyện để tối ưu hóa suy luận theo phân phối dữ liệu cụ thể, gây thách thức khi thêm mục mới hoặc loại bỏ mục lỗi thời. Do bản chất của nhiệm vụ này mà ứng cử viên truy xuất mới được thêm với mỗi suy luận, dự đoán rằng việc sử dụng kỹ thuật lập chỉ mục sẽ khó khăn, và chi phí tính toán đáng kể sẽ cần thiết để xử lý chuỗi dài. Ngoài ra, không giống như phương pháp dựa trên bộ nhớ, có vấn đề về chi phí tính toán ngày càng tăng do không có cơ chế quên. Hơn nữa, xem xét độ dài đầu vào mô hình tương đối ngắn trong cơ chế phân đoạn và suy luận đệ quy, RAG sẽ đòi hỏi độ dài đầu vào mô hình dài hơn để bao gồm đủ văn bản trong prompt để hỗ trợ suy luận của mô hình, vì văn bản được truy xuất chiếm độ dài đầu vào.
Ngoài những vấn đề này, dường như cần thiết phải khám phá kỹ lưỡng các khía cạnh hiệu suất như cách soạn thảo pool truy xuất ở cấp độ câu, cấp độ đoạn văn, hoặc thậm chí ở cấp độ toàn bộ segment, cách huấn luyện mô hình retriever, và nên thực hiện bao nhiều lần truy xuất. Ngoài ra, vì RAG được thiết kế cho việc tạo văn bản thay vì xử lý chuỗi tổng quát, nó không thể được áp dụng đơn giản cho các nhiệm vụ khác bao gồm sắp xếp và phân loại và bị giới hạn chỉ ở việc tạo văn bản. Chắc chắn, nếu những ràng buộc này có thể được vượt qua, có chỗ cho sự xuất hiện của kỹ thuật bộ nhớ dài hạn dựa trên RAG. Tuy nhiên, việc thảo luận so sánh cụ thể là quá sớm tại thời điểm này. Nếu điều đó xảy ra, một lợi ích tiềm năng của RAG, hiện tại khá có thể dự đoán, nằm ở thực tế rằng bộ nhớ của chúng được lưu trữ ở định dạng văn bản nên có thể hiểu được bởi con người.
25

--- TRANG 26 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
G. Cấu trúc của Transformers áp dụng Memoria
G.1. Memoria Transformer
Hình 12: Kiến trúc của Memoria Transformer. t đại diện cho bước thời gian hiện tại, và x là embedding đầu vào. Mạng residual và layer normalization được bỏ qua để rõ ràng. Memoria Transformer tạo engram từ đầu ra bước thời gian trước ht-1 và truy xuất engram từ bộ nhớ ngắn hạn và dài hạn. Memoria Transformer khai thác engram với cross-attention. Các khối memory attention, được mô tả như hai khối trong sơ đồ, thực tế là một lớp duy nhất chia sẻ cùng trọng số.
26

--- TRANG 27 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
G.2. Memoria BERT/RoBERTa
Hình 13: Kiến trúc của Memoria BERT/RoBERTa. t đại diện cho bước thời gian hiện tại, và x là embedding đầu vào. Mạng residual và layer normalization được bỏ qua để rõ ràng. Trong các mô hình encoder, không giống như trong mô hình decoder, engram được tạo bằng thông tin từ bước thời gian hiện tại. l đại diện cho chỉ số lớp bộ nhớ, và từ lớp 1 đến lớp l, mỗi lớp là lớp BERT thông thường. Sử dụng đầu ra hlt từ lớp l, engram được tạo và truy xuất từ bộ nhớ ngắn hạn và dài hạn. Những engram này sau đó được sử dụng trong các lớp tiếp theo (sau lớp l) thông qua cơ chế cross-attention. Hai khối memory attention chia sẻ cùng trọng số, như trong Memoria Transformer.
27

--- TRANG 28 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
H. Trực quan hóa Memoria
(a) t = 20
(b) t = 60
(c) t = 130
Hình 14: Thay đổi engram của Memoria theo thời gian. Các chấm đại diện cho engram, và các đường đại diện cho kết nối giữa engram. t là bước thời gian. Engram càng ở bên phải, nó càng được tạo muộn. Chỉ các kết nối có trọng số cao mới được hiển thị để rõ ràng. Engram dần mờ đi, nhưng một số engram quan trọng vẫn tồn tại trong thời gian dài hơn. Điều này chứng minh khả năng của Memoria trong việc bảo tồn thông tin, ngay cả khi đã qua thời gian dài, miễn là nó vẫn hữu ích. Kết nối gần đây mạnh mẽ ngụ ý mẫu hiệu ứng tính liền kề thời gian của con người (Kahana, 1996).
28

--- TRANG 29 ---
Memoria: Giải quyết vấn đề quên lãng định mệnh thông qua kiến trúc bộ nhớ lấy cảm hứng từ con người
(a) t = 87
(b) t = 105
(c) t = 122
Hình 15: Thay đổi engram của Memoria theo thời gian khi Memoria thấy cùng dữ liệu hai lần. Nửa dưới của mỗi hình ảnh đại diện cho engram được tạo khi quan sát lần đầu, trong khi nửa trên đại diện cho engram được tạo khi quan sát lần thứ hai. Do đó, các chấm trong cùng cột dọc đại diện cho engram được tạo từ cùng dữ liệu. Engram từ cùng dữ liệu thể hiện kết nối mạnh hơn nói chung. Điều này cho thấy rằng Memoria có thể xem xét sự tương đồng dựa trên nội dung giữa thông tin ngay cả khi chúng cách xa về mặt thời gian.
29
