# 2310.16450.pdf
# Chuy·ªÉn ƒë·ªïi t·ª´ PDF sang TXT
# ƒê∆∞·ªùng d·∫´n ngu·ªìn: /home/admin88/arxiv-downloader/long-context/2310.16450.pdf
# K√≠ch th∆∞·ªõc t·ªáp: 969902 bytes

===============================================
N·ªòI DUNG T·ªÜP PDF
===============================================


--- TRANG 1 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024
CLEX: NGO·∫†I SUY CHI·ªÄU D√ÄI LI√äN T·ª§C CHO
C√ÅC M√î H√åNH NG√îN NG·ªÆ L·ªöN
Guanzheng Chen1,2,3,‚àóXin Li2,3,‚Ä†Zaiqiao Meng4Shangsong Liang1,5,‚Ä†Lidong Bing2,3
1ƒê·∫°i h·ªçc Sun Yat-sen2DAMO Academy, Alibaba Group
3Hupan Lab, 310023, Hangzhou, China4ƒê·∫°i h·ªçc Glasgow
5ƒê·∫°i h·ªçc Tr√≠ tu·ªá Nh√¢n t·∫°o Mohamed bin Zayed
guanzzh.chen@gmail.com, {xinting.lx,l.bing }@alibaba-inc.com
zaiqiao.meng@glasgow.ac.uk, liangshangsong@gmail.com
T√ìM T·∫ÆT
C√°c M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn (LLM) d·ª±a tr√™n Transformer ƒëang ti√™n phong trong nhi·ªÅu nhi·ªám v·ª• x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, tuy nhi√™n, kh·∫£ nƒÉng ƒë·∫∑c bi·ªát c·ªßa ch√∫ng b·ªã h·∫°n ch·∫ø trong c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë∆∞·ª£c thi·∫øt l·∫≠p s·∫µn c·ªßa Transformer. C√°c ph∆∞∆°ng ph√°p m·ªü r·ªông Position Embedding (PE), m·∫∑c d√π hi·ªáu qu·∫£ trong vi·ªác m·ªü r·ªông c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë·∫øn m·ªôt chi·ªÅu d√†i c·ª• th·ªÉ, th·ªÉ hi·ªán nh·ªØng h·∫°n ch·∫ø ƒë√°ng k·ªÉ trong kh·∫£ nƒÉng ngo·∫°i suy c·ªßa ch√∫ng ho·∫∑c hy sinh m·ªôt ph·∫ßn hi·ªáu su·∫•t trong c·ª≠a s·ªï ng·ªØ c·∫£nh. C√°c ph∆∞∆°ng ph√°p ngo·∫°i suy chi·ªÅu d√†i, m·∫∑c d√π v·ªÅ l√Ω thuy·∫øt c√≥ kh·∫£ nƒÉng m·ªü r·ªông c·ª≠a s·ªï ng·ªØ c·∫£nh v∆∞·ª£t qu√° chi·ªÅu d√†i chu·ªói hu·∫•n luy·ªán, th∆∞·ªùng ho·∫°t ƒë·ªông k√©m trong c√°c ·ª©ng d·ª•ng ng·ªØ c·∫£nh d√†i th·ª±c t·∫ø. ƒê·ªÉ gi·∫£i quy·∫øt nh·ªØng th√°ch th·ª©c n√†y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t Continuous Length EXtrapolation (CLEX) cho c√°c LLM. Ch√∫ng t√¥i t·ªïng qu√°t h√≥a c√°c ph∆∞∆°ng ph√°p m·ªü r·ªông PE ƒë·ªÉ m√¥ h√¨nh h√≥a ƒë·ªông l·ª±c li√™n t·ª•c b·∫±ng ph∆∞∆°ng tr√¨nh vi ph√¢n th∆∞·ªùng qua h·ªá s·ªë m·ªü r·ªông chi·ªÅu d√†i, t·ª´ ƒë√≥ v∆∞·ª£t qua c√°c r√†ng bu·ªôc c·ªßa c√°c ph∆∞∆°ng ph√°p m·ªü r·ªông PE hi·ªán t·∫°i ƒë∆∞·ª£c thi·∫øt k·∫ø cho c√°c chi·ªÅu d√†i c·ª• th·ªÉ. H∆°n n·ªØa, b·∫±ng c√°ch m·ªü r·ªông ƒë·ªông l·ª±c ƒë·∫øn c√°c chi·ªÅu d√†i ng·ªØ c·∫£nh mong mu·ªën v∆∞·ª£t qu√° chi·ªÅu d√†i chu·ªói hu·∫•n luy·ªán, CLEX t·∫°o ƒëi·ªÅu ki·ªán cho vi·ªác ngo·∫°i suy chi·ªÅu d√†i v·ªõi hi·ªáu su·∫•t ·∫•n t∆∞·ª£ng trong c√°c nhi·ªám v·ª• th·ª±c t·∫ø. Ch√∫ng t√¥i ch·ª©ng minh r·∫±ng CLEX c√≥ th·ªÉ ƒë∆∞·ª£c t√≠ch h·ª£p li·ªÅn m·∫°ch v√†o c√°c LLM ƒë∆∞·ª£c trang b·ªã Rotary Position Embedding, nh∆∞ LLaMA v√† GPT-NeoX, v·ªõi t√°c ƒë·ªông kh√¥ng ƒë√°ng k·ªÉ ƒë·∫øn ƒë·ªô tr·ªÖ hu·∫•n luy·ªán v√† suy lu·∫≠n. K·∫øt qu·∫£ th·ª±c nghi·ªám cho th·∫•y CLEX c√≥ th·ªÉ m·ªü r·ªông hi·ªáu qu·∫£ c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë·∫øn h∆°n 4√ó ho·∫∑c g·∫ßn 8√ó chi·ªÅu d√†i hu·∫•n luy·ªán, kh√¥ng c√≥ s·ª± suy gi·∫£m hi·ªáu su·∫•t. H∆°n n·ªØa, khi ƒë∆∞·ª£c ƒë√°nh gi√° tr√™n benchmark LongBench th·ª±c t·∫ø, m√¥ h√¨nh c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i 4k th·ªÉ hi·ªán hi·ªáu su·∫•t c·∫°nh tranh so v·ªõi c√°c m√¥ h√¨nh ngu·ªìn m·ªü ti√™n ti·∫øn ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i ng·ªØ c·∫£nh l√™n ƒë·∫øn 32k. M√£ ngu·ªìn c·ªßa ch√∫ng t√¥i c√≥ s·∫µn t·∫°i https://github.com/DAMO-NLP-SG/CLEX .

1 GI·ªöI THI·ªÜU
C√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) d·ª±a tr√™n Transformer, nh∆∞ GPT-4 (OpenAI, 2023) v√† LLaMA (Touvron et al., 2023a;b), hi·ªán ƒë√£ tr·ªü th√†nh c√°c m√¥ h√¨nh ti√™n ti·∫øn trong nhi·ªÅu nhi·ªám v·ª• x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP). Tuy nhi√™n, c√°c m√¥ h√¨nh n√†y ph·∫£i v·∫≠t l·ªôn v·ªõi nh·ªØng h·∫°n ch·∫ø v·ªën c√≥ c·ªßa ki·∫øn tr√∫c Transformer - ch·ªß y·∫øu l√† c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë∆∞·ª£c thi·∫øt l·∫≠p s·∫µn, v∆∞·ª£t qu√° ƒë√≥ hi·ªáu su·∫•t s·∫Ω gi·∫£m m·∫°nh (Press et al., 2022). ƒê·ªô ph·ª©c t·∫°p b·∫≠c hai c·ªßa c∆° ch·∫ø attention l√†m cho vi·ªác hu·∫•n luy·ªán LLM v·ªõi c·ª≠a s·ªï ng·ªØ c·∫£nh l·ªõn h∆°n tr·ªü n√™n c·ª±c k·ª≥ t·ªën t√†i nguy√™n. C√°c nghi√™n c·ª©u tr∆∞·ªõc ƒë√¢y (Dai et al., 2019; Beltagy et al., 2020; Bulatov et al., 2022) ƒë√£ ƒë·ªÅ xu·∫•t tr√°nh truy c·∫≠p chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß th√¥ng qua ki·∫øn tr√∫c ph√¢n c·∫•p ho·∫∑c sparse attention, m·∫∑c d√π ph·∫£i tr·∫£ gi√° b·∫±ng vi·ªác t·ª´ b·ªè m·ªôt ph·∫ßn th√¥ng tin ng·ªØ c·∫£nh.

G·∫ßn ƒë√¢y, ƒë√£ c√≥ hai h∆∞·ªõng ph∆∞∆°ng ph√°p nh·∫±m m·ªü r·ªông hi·ªáu qu·∫£ chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc c·ªßa LLM, c·∫£ hai ƒë·ªÅu t·∫≠p trung v√†o position embedding (PE). H∆∞·ªõng ph∆∞∆°ng ph√°p ƒë·∫ßu ti√™n, ƒë∆∞·ª£c g·ªçi l√† PE scaling, ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ƒë·ªÉ m·ªü r·ªông hi·ªáu qu·∫£ c·ª≠a s·ªï ng·ªØ c·∫£nh c·ªßa c√°c LLM ƒë∆∞·ª£c t√≠ch h·ª£p v·ªõi Rotary

‚àóC√¥ng vi·ªác n√†y ƒë∆∞·ª£c th·ª±c hi·ªán trong th·ªùi gian th·ª±c t·∫≠p c·ªßa Guanzheng Chen t·∫°i Alibaba DAMO Academy.
‚Ä†C√°c t√°c gi·∫£ li√™n h·ªá.
1arXiv:2310.16450v3  [cs.CL]  24 Mar 2024

--- TRANG 2 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024
0.0 10k 20k 30k 40k 50k 60k
Chi·ªÅu d√†i chu·ªói ƒë√°nh gi√°2324PPLFT-16k
PI-16k (t=16)
Yarn-16k (t=16)
Yarn-16k (t=32)
CLEX-16k (t=16)
H√¨nh 1: PPL c·ªßa CLEX v√† c√°c baseline kh√°c nhau ƒë∆∞·ª£c ki·ªÉm tra tr√™n chi·ªÅu d√†i ng·ªØ c·∫£nh 64k.

Position Embedding (RoPE) (Su et al., 2022). Ch√∫ng cho ph√©p LLM truy c·∫≠p ng·ªØ c·∫£nh d√†i h∆°n b·∫±ng c√°ch m·ªü r·ªông ch·ªâ s·ªë v·ªã tr√≠ (Chen et al., 2023) ho·∫∑c t·∫ßn s·ªë c∆° s·ªü (Rozi`ere et al., 2023; Peng et al., 2023) c·ªßa RoPE, th·ªÉ hi·ªán hi·ªáu su·∫•t ƒë√°ng ch√∫ √Ω trong c√°c ·ª©ng d·ª•ng ng·ªØ c·∫£nh d√†i. Tuy nhi√™n, c√°c ph∆∞∆°ng ph√°p nh∆∞ v·∫≠y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng v·ªõi m·ªôt h·ªá s·ªë m·ªü r·ªông c·ªë ƒë·ªãnh, ƒëi·ªÅu n√†y ho·∫∑c h·∫°n ch·∫ø kh·∫£ nƒÉng ngo·∫°i suy ƒë·∫øn c√°c chu·ªói d√†i h∆°n (khi s·ª≠ d·ª•ng h·ªá s·ªë nh·ªè) ho·∫∑c l√†m suy gi·∫£m hi·ªáu su·∫•t ngay c·∫£ trong c·ª≠a s·ªï ng·ªØ c·∫£nh g·ªëc (khi s·ª≠ d·ª•ng h·ªá s·ªë l·ªõn) nh∆∞ ƒë∆∞·ª£c th·ªÉ hi·ªán trong H√¨nh 1. M·∫∑t kh√°c, c√°c ph∆∞∆°ng ph√°p ngo·∫°i suy chi·ªÅu d√†i (Press et al., 2022; Sun et al., 2023; Chi et al., 2022; 2023), ti√™u bi·ªÉu l√† ALiBi (Press et al., 2022), c·ªë g·∫Øng ƒë·∫°t ƒë∆∞·ª£c m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh th·ªùi gian ki·ªÉm tra (t·ª©c l√† "hu·∫•n luy·ªán ng·∫Øn, ki·ªÉm tra d√†i") b·∫±ng c√°ch thay th·∫ø position embedding b·∫±ng c√°c bias b·ªï sung, trong ƒë√≥ c√°c bias m√£ h√≥a th√¥ng tin v·ªã tr√≠ v√†o ƒëi·ªÉm attention. M·∫∑c d√π c√≥ kh·∫£ nƒÉng ·∫•n t∆∞·ª£ng trong m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, c√°c ph∆∞∆°ng ph√°p gi·ªëng ALiBi th∆∞·ªùng g·∫∑p kh√≥ khƒÉn trong c√°c nhi·ªám v·ª• th·ª±c t·∫ø y√™u c·∫ßu ph·ª• thu·ªôc ng·ªØ c·∫£nh d√†i (Pal et al., 2023) (c≈©ng xem ¬ß4.3).

Trong nghi√™n c·ª©u n√†y, ch√∫ng t√¥i tr√¨nh b√†y Continuous Length EXtrapolation (CLEX), m·ªôt ph∆∞∆°ng ph√°p ti·ªÉu thuy·∫øt m·ªü r·ªông hi·ªáu qu·∫£ c·ª≠a s·ªï ng·ªØ c·∫£nh c·ªßa LLM th√¥ng qua PE scaling li√™n t·ª•c. C·ª• th·ªÉ, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt c√°i nh√¨n th·ªëng nh·∫•t v·ªÅ PE scaling th√¥ng qua t·ªïng qu√°t h√≥a c√°c ph∆∞∆°ng ph√°p PE scaling th√†nh qu√° tr√¨nh chuy·ªÉn ƒë·ªïi t·∫ßn s·ªë c∆° s·ªü. D·ª±a tr√™n ƒë√≥, ch√∫ng t√¥i c√¥ng th·ª©c h√≥a PE scaling nh∆∞ m·ªôt h·ªá th·ªëng ƒë·ªông l·ª±c li√™n t·ª•c, m√¥ h√¨nh h√≥a qu√° tr√¨nh chuy·ªÉn ƒë·ªïi t·∫ßn s·ªë c∆° s·ªü th√¥ng qua ƒë·ªông l·ª±c li√™n t·ª•c qua h·ªá s·ªë m·ªü r·ªông chi·ªÅu d√†i. Ch√∫ng t√¥i l·∫≠p lu·∫≠n r·∫±ng c√°c ph∆∞∆°ng ph√°p PE scaling tr∆∞·ªõc ƒë√¢y, hu·∫•n luy·ªán m√¥ h√¨nh s·ª≠ d·ª•ng c√°c h·ªá s·ªë m·ªü r·ªông c·ªë ƒë·ªãnh (r·ªùi r·∫°c), b·ªè qua ƒë·ªông l·ª±c li√™n t·ª•c d·∫ßn d·∫ßn qua qu√° tr√¨nh m·ªü r·ªông chi·ªÅu d√†i t·ª´ t·ª´. ƒêi·ªÅu n√†y khi·∫øn ch√∫ng r∆°i v√†o ti·∫øn tho√°i l∆∞·ª°ng nan ƒë√£ n√™u gi·ªØa vi·ªác ngo·∫°i suy chi·ªÅu d√†i v√† b·∫£o t·ªìn hi·ªáu su·∫•t trong c√°c chi·ªÅu d√†i ng·∫Øn h∆°n. Ng∆∞·ª£c l·∫°i, CLEX c·ªßa ch√∫ng t√¥i khai th√°c ph∆∞∆°ng tr√¨nh vi ph√¢n th∆∞·ªùng (ODE) th·∫ßn kinh (Chen et al., 2018), ƒë∆∞·ª£c tham s·ªë h√≥a b·ªüi m·ªôt l·ªõp projection l√™n-xu·ªëng v·ªõi c√°c tham s·ªë nh·∫π ƒë·ªÉ h·ªçc c√°c ƒë·ªông l·ª±c li√™n t·ª•c n√†y, cho ph√©p m·ªü r·ªông chi ti·∫øt ƒë·∫øn ng·ªØ c·∫£nh d√†i. C∆° b·∫£n h∆°n, b·∫±ng c√°ch m·ªü r·ªông ƒë·ªông l·ª±c v∆∞·ª£t qu√° chi·ªÅu d√†i hu·∫•n luy·ªán, CLEX trao quy·ªÅn cho c√°c m√¥ h√¨nh ngo·∫°i suy d·∫ßn d·∫ßn ƒë·∫øn ng·ªØ c·∫£nh d√†i h∆°n ngay c·∫£ khi ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi c√°c chu·ªói ng·∫Øn.

CLEX c√≥ th·ªÉ ph·ª•c v·ª• nh∆∞ m·ªôt th√†nh ph·∫ßn drop-in cho c√°c LLM d·ª±a tr√™n RoPE, nh∆∞ LLaMA (Touvron et al., 2023a;b) v√† GPT-NeoX (Black et al., 2022), v·ªõi overhead kh√¥ng ƒë√°ng k·ªÉ v·ªÅ t√≠nh to√°n v√† k√≠ch th∆∞·ªõc tham s·ªë. Ch√∫ng t√¥i ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa CLEX tr√™n hai t·∫≠p d·ªØ li·ªáu: (1) m·ªôt t·∫≠p con c·ªßa RedPajama-Book (Computer, 2023) cho m√¥ h√¨nh h√≥a ng√¥n ng·ªØ ng·ªØ c·∫£nh d√†i, v√† (2) LongBench (Bai et al., 2023) cho c√°c nhi·ªám v·ª• th·ª±c t·∫ø ng·ªØ c·∫£nh d√†i. V·ªÅ m·∫∑t th·ª±c nghi·ªám, CLEX th·ªÉ hi·ªán kh·∫£ nƒÉng ngo·∫°i suy chi·ªÅu d√†i ƒë√°ng ch√∫ √Ω trong m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, c√≥ th·ªÉ m·ªü r·ªông c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë·∫øn h∆°n 4√ó chi·ªÅu d√†i hu·∫•n luy·ªán m√† kh√¥ng c√≥ b·∫•t k·ª≥ suy gi·∫£m hi·ªáu su·∫•t n√†o. V√≠ d·ª•, LLaMA-2-7B ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi CLEX tr√™n chi·ªÅu d√†i ng·ªØ c·∫£nh 16k ƒë·∫°t ƒë∆∞·ª£c perplexity t∆∞∆°ng ƒë∆∞∆°ng khi ki·ªÉm tra tr√™n 16k v√† 64k token, t∆∞∆°ng ·ª©ng. B·∫±ng c√°ch m·ªü r·ªông quy m√¥ m√¥ h√¨nh c∆° s·ªü t·ª´ 7B l√™n 13B, CLEX th·ªÉ hi·ªán ph·∫°m vi ngo·∫°i suy m·ªü r·ªông t·ª´ 4√ó ƒë·∫øn g·∫ßn 8√ó chi·ªÅu d√†i hu·∫•n luy·ªán. ƒê·ªÉ b·ªï sung, ch√∫ng t√¥i c≈©ng ti·∫øn h√†nh instruction tuning (Wei et al., 2022) v·ªõi CLEX ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t tr√™n c√°c chu·ªói c√≥ chi·ªÅu d√†i 4k. M√¥ h√¨nh k·∫øt qu·∫£, khi ƒë∆∞·ª£c ƒë√°nh gi√° tr√™n benchmark LongBench, ngang b·∫±ng v·ªõi c√°c m√¥ h√¨nh ngu·ªìn m·ªü ti√™n ti·∫øn hi·ªán t·∫°i ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i ng·ªØ c·∫£nh l√™n ƒë·∫øn 32k. Nh·ªØng ph√°t hi·ªán n√†y nh·∫•n m·∫°nh hi·ªáu qu·∫£ c·ªßa CLEX trong vi·ªác ngo·∫°i suy chi·ªÅu d√†i ng·ªØ c·∫£nh, bi·ªÉu th·ªã hi·ªáu qu·∫£ c·ªßa n√≥ cho vi·ªác ph√°t tri·ªÉn LLM ng·ªØ c·∫£nh d√†i.

2

--- TRANG 3 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

2 KI·∫æN TH·ª®C C∆† B·∫¢N

2.1 ROTARY POSITION EMBEDDING (ROPE)

Rotary Position Embedding (RoPE) (Su et al., 2022) g·∫ßn ƒë√¢y ƒë√£ n·ªïi l√™n nh∆∞ ph∆∞∆°ng ph√°p m√£ h√≥a v·ªã tr√≠ ph·ªï bi·∫øn nh·∫•t trong c√°c LLM ngu·ªìn m·ªü nh∆∞ LLaMA. N√≥ t√≠ch h·ª£p c·∫£ th√¥ng tin v·ªã tr√≠ tuy·ªát ƒë·ªëi v√† t∆∞∆°ng ƒë·ªëi cho c√°c m√¥ h√¨nh Transformer. Cho m·ªôt ch·ªâ s·ªë v·ªã tr√≠ m‚àà[1, L], RoPE ti√™m th√¥ng tin v·ªã tr√≠ tuy·ªát ƒë·ªëi v√†o x‚ààRd th√¥ng qua ph√©p bi·∫øn ƒë·ªïi f:Rd‚ÜíRd nh∆∞:

f(x, m,Œ∏) =RŒ∏,mx, (1)

trong ƒë√≥ Œ∏‚ààR‚åäd/2‚åã l√† t·∫ßn s·ªë c∆° s·ªü xoay v√† Œ∏i= 10,000‚àí2i/d; RŒ∏,m‚ààRd√ód l√† ma tr·∫≠n ƒë∆∞·ªùng ch√©o kh·ªëi ƒë∆∞·ª£c t·∫°o th√†nh b·ªüi c√°c ph·∫ßn t·ª≠

(RŒ∏,m)i=
cosmŒ∏i‚àísinmŒ∏i
sinmŒ∏icosmŒ∏i
,cho i= 1,2, ...,‚åäd/2‚åã. (2)

Ph√©p bi·∫øn ƒë·ªïi trong Eq. (1) ƒë∆∞·ª£c √°p d·ª•ng cho c√°c vector query v√† key trong self-attention. Khi t√≠nh ƒëi·ªÉm attention cho vector query qm‚ààRd t·∫°i v·ªã tr√≠ m v√† vector key kn‚ààRd t·∫°i v·ªã tr√≠ n, ch√∫ng ta c√≥

(RŒ∏,mqm)‚ä§(RŒ∏,nkn) =qmRŒ∏,n‚àímkn. (3)

Do ƒë√≥, th√¥ng tin v·ªã tr√≠ t∆∞∆°ng ƒë·ªëi RŒ∏,n‚àím ƒë∆∞·ª£c t√≠ch h·ª£p ng·∫ßm v√†o ƒëi·ªÉm attention. Tuy nhi√™n, ngay c·∫£ khi c√≥ th√¥ng tin t∆∞∆°ng ƒë·ªëi, c√°c LLM ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi RoPE, v√≠ d·ª• LLaMA, v·∫´n kh√¥ng th·ªÉ ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t h·ª£p l√Ω v∆∞·ª£t qu√° chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc.

2.2 C√ÅC PH∆Ø∆†NG PH√ÅP PE SCALING

ƒê·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh L, m·ªôt s·ªë chi·∫øn l∆∞·ª£c ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ƒë·ªÉ ƒëi·ªÅu ch·ªânh position embedding b·∫±ng c√°ch m·ªü r·ªông ch·ªâ s·ªë v·ªã tr√≠ m ho·∫∑c t·∫ßn s·ªë c∆° s·ªü Œ∏ trong Eq. (1). Ch√≠nh th·ª©c, ch√∫ng t√¥i ƒë·ªãnh nghƒ©a t=L‚Ä≤/L l√† h·ªá s·ªë m·ªü r·ªông chi·ªÅu d√†i trong ƒë√≥ L‚Ä≤ bi·ªÉu th·ªã chi·ªÅu d√†i m·ªü r·ªông mong mu·ªën. Trong khi Chen et al. (2023) gi·ªõi thi·ªáu vi·ªác m·ªü r·ªông ch·ªâ s·ªë m b·∫±ng Position Interpolation (PI) nh∆∞

fPI_t(x, m,Œ∏) =f(x,m/t,Œ∏). (4)

Chi·∫øn l∆∞·ª£c n√†y duy tr√¨ c√°c ch·ªâ s·ªë v·ªã tr√≠ trong ph·∫°m vi [1, L], trong khi m·ªü r·ªông hi·ªáu qu·∫£ ph·∫°m vi ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫øn [1, t¬∑L] b·∫±ng c√°c b∆∞·ªõc fine-tuning t·ªëi thi·ªÉu tr√™n c√°c chu·ªói t¬∑L. M·∫∑t kh√°c, Peng et al. (2023) ƒë·ªÅ xu·∫•t Yarn, c√≤n g·ªçi l√† NTK-Aware Scaled RoPE, m·ªü r·ªông c·ª≠a s·ªï ng·ªØ c·∫£nh b·∫±ng frequency basis scaling (FBS). Chi·∫øn l∆∞·ª£c n√†y c≈©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi CodeLLaMA (Rozi`ere et al., 2023). Ch√≠nh th·ª©c, c√°c ph∆∞∆°ng ph√°p FBS ƒë∆∞·ª£c k√Ω hi·ªáu l√†

fFBS_t(x, m,Œ∏) =f(x, m,Œ∏t), (5)

trong ƒë√≥ Œ∏t l√† t·∫ßn s·ªë c∆° s·ªü ƒë∆∞·ª£c m·ªü r·ªông. C·ª• th·ªÉ, Œ∏t,i=Œ∏i¬∑(t)‚àí2i/(d‚àí2) trong Yarn v√† Œ∏t,i= Œ∏i¬∑100‚àí2i/d trong CodeLLaMA.

3 PH∆Ø∆†NG PH√ÅP LU·∫¨N

Ph·∫ßn n√†y tr√¨nh b√†y chi ti·∫øt v·ªÅ CLEX. ƒê·∫ßu ti√™n ch√∫ng t√¥i t·ªïng qu√°t h√≥a PE scaling th√†nh m·ªôt h·ªá th·ªëng ƒë·ªông l·ª±c li√™n t·ª•c theo c√°ch th·ªëng nh·∫•t (xem ¬ß3.1). D·ª±a tr√™n h·ªá th·ªëng ƒë·ªông l·ª±c li√™n t·ª•c, CLEX s·ª≠ d·ª•ng neural ODE, ƒë∆∞·ª£c tham s·ªë h√≥a b·ªüi m·ªôt l·ªõp projection l√™n-xu·ªëng, ƒë·ªÉ h·ªçc th√≠ch ·ª©ng c√°c ƒë·ªông l·ª±c li√™n t·ª•c trong qu√° tr√¨nh PE scaling (xem ¬ß3.2). Trong ¬ß3.3, ch√∫ng t√¥i gi·ªõi thi·ªáu chi·∫øn l∆∞·ª£c hu·∫•n luy·ªán c·ªßa CLEX ph√¢n ph·ªëi c√°c ƒë·ªông l·ª±c li√™n t·ª•c v∆∞·ª£t qu√° chi·ªÅu d√†i chu·ªói hu·∫•n luy·ªán, t·ª´ ƒë√≥ cho ph√©p t·ªïng qu√°t h√≥a PE scaling li√™n t·ª•c ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c ngo·∫°i suy chi·ªÅu d√†i.

3.1 POSITION EMBEDDING SCALING: M·ªòT C√ÅI NH√åN TH·ªêNG NH·∫§T

Cho c√°c ph∆∞∆°ng ph√°p kh√°c nhau m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa m√¥ h√¨nh th√¥ng qua position indices scaling v√† frequency basis scaling, ƒë·∫ßu ti√™n ch√∫ng t√¥i ch·ªâ ra r·∫±ng c√°c ph√©p bi·∫øn ƒë·ªïi ƒë∆∞·ª£c √°p d·ª•ng cho ch·ªâ s·ªë v·ªã tr√≠ v·ªÅ c∆° b·∫£n l√† casting t·∫ßn s·ªë c∆° s·ªü, ƒë∆∞·ª£c ch√≠nh th·ª©c h√≥a trong ƒê·ªãnh l√Ω 1.

3

--- TRANG 4 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

ùúÉùúÉ!ùúÉ3ùúÉ41.02.03.04.0ùúÉùúÉ!ùúÉ3ùúÉ4ùë°ùë°1.02.03.04.04k8k12k16k4k8k12k16k

H√¨nh 2: M√¥ h√¨nh ƒë·ªì h·ªça c·ªßa PE scaling r·ªùi r·∫°c (tr√°i) v√† PE scaling li√™n t·ª•c c·ªßa ch√∫ng t√¥i (ph·∫£i).

ƒê·ªãnh l√Ω 1. ƒê·ªëi v·ªõi ph√©p bi·∫øn ƒë·ªïi T cho ch·ªâ s·ªë v·ªã tr√≠ m, t·ªìn t·∫°i m·ªôt ph√©p bi·∫øn ƒë·ªïi t∆∞∆°ng ƒë∆∞∆°ng T cho t·∫ßn s·ªë c∆° s·ªü Œ∏ trong Eq. (1), c·ª• th·ªÉ l√†

f(x,T ¬∑m,Œ∏) =f(x, m,T‚äôŒ∏), (6)

trong ƒë√≥ T= [T]d/2_i=1 v√† ‚äô bi·ªÉu th·ªã ph√©p bi·∫øn ƒë·ªïi theo t·ª´ng ph·∫ßn t·ª≠.

Ch·ª©ng minh. T·ª´ Eq. (1), ch√∫ng ta c√≥ f(x,T ¬∑m,Œ∏) =RŒ∏,Tmx v√† f(x, m,T‚äôŒ∏) =RT‚äôŒ∏,mx. ƒê·ªëi v·ªõi b·∫•t k·ª≥ T= [T]d/2_i=1,

(RŒ∏,Tm)i=
cosTmŒ∏i‚àísinTmŒ∏i
sinTmŒ∏icosTmŒ∏i
=
cosm(T‚äôŒ∏i)‚àísinm(T‚äôŒ∏i)
sinm(T‚äôŒ∏i) cos m(T‚äôŒ∏i)
= (RT‚äôŒ∏,m)i.
(7)

Do ƒë√≥, c√≥ m·ªôt d·∫°ng th·ªëng nh·∫•t cho PE scaling nh·∫•t qu√°n chi·∫øu t·∫ßn s·ªë c∆° s·ªü b·ªüi Œ±(t):

ft(x, m,Œ∏) =f(x, m,Œ±(t)‚äôŒ∏), (8)

trong ƒë√≥ Œ±(t) l√† ph√©p bi·∫øn ƒë·ªïi m·ªôt bi·∫øn ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n h·ªá s·ªë m·ªü r·ªông chi·ªÅu d√†i t. Th√¥ng qua c√¥ng th·ª©c th·ªëng nh·∫•t n√†y, PI (Chen et al., 2023) v√† Yarn (Peng et al., 2023) c√≥ th·ªÉ ƒë∆∞·ª£c xem nh∆∞ c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát khi thay Œ±PI(t) = [1/t]d/2_i=1 v√† Œ±Yarn(t) = [t‚àí2i/(d‚àí2)]d/2_i=1 v√†o Eq. 8, t∆∞∆°ng ·ª©ng.

L∆∞u √Ω r·∫±ng Œ∏t=Œ±(t)‚äôŒ∏ bi·ªÉu th·ªã t·∫ßn s·ªë c∆° s·ªü ƒë∆∞·ª£c m·ªü r·ªông t·∫°i chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa t¬∑L v√† Œ∏1=Œ∏ (c·ª• th·ªÉ Œ±(1) = 1). Nh∆∞ ƒë∆∞·ª£c minh h·ªça trong H√¨nh 2, ƒëi·ªÅu n√†y ch·ªâ ra m·ªôt chu·ªói ti·∫øn tri·ªÉn qua c√°c gi√° tr·ªã t r·ªùi r·∫°c m√†

z(t) =z(1) + log Œ±(t) =z(t‚àí1) + logŒ±(t)/Œ±(t‚àí1), (9)

trong ƒë√≥ z(t)=log Œ∏t.

B·∫±ng c√°ch li√™n t·ª•c h√≥a chu·ªói ti·∫øn tri·ªÉn, ch√∫ng ta c√≥ th·ªÉ c√¥ng th·ª©c h√≥a PE scaling nh∆∞ m·ªôt h·ªá th·ªëng ƒë·ªông l·ª±c li√™n t·ª•c, v·ªõi ƒë·ªông l·ª±c li√™n t·ª•c c·ªßa t·∫ßn s·ªë c∆° s·ªü dz(t)/dt nh∆∞

dz(t)/dt=dlogŒ±(t)/dt. (10)

V·ªÅ b·∫£n ch·∫•t, c√°c ph∆∞∆°ng ph√°p PE scaling g·∫ßn ƒë√¢y, t·∫≠p trung v√†o vi·ªác c√¥ng th·ª©c h√≥a Œ±(t) th·ªß c√¥ng, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác √°p d·ª•ng c√°c ƒë·ªông l·ª±c kh√°c nhau cho t·∫ßn s·ªë c∆° s·ªü cho ph√©p c√°c m√¥ h√¨nh th√≠ch ·ª©ng v·ªõi ng·ªØ c·∫£nh d√†i h∆°n.

3.2 PE SCALING LI√äN T·ª§C TH√îNG QUA NEURAL ODE

Ngay c·∫£ khi c√≥ ƒë·ªông l·ª±c li√™n t·ª•c c·ªßa t·∫ßn s·ªë c∆° s·ªü, c√°c ph∆∞∆°ng ph√°p tr∆∞·ªõc ƒë√¢y v·ªÅ c∆° b·∫£n ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh t·∫°i c√°c gi√° tr·ªã t r·ªùi r·∫°c. V√≠ d·ª•, PI (Chen et al., 2023) fine-tune m√¥ h√¨nh tr√™n m·ªôt h·ªá s·ªë m·ªü r·ªông c·ª• th·ªÉ t ƒë·ªÉ m·ªü r·ªông chi·ªÅu d√†i c·ª≠a s·ªï ng·ªØ c·∫£nh ƒë·∫øn t¬∑L. M·ªôt v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n c·ªßa c√°c ph∆∞∆°ng ph√°p n√†y, nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 1, l√† ch√∫ng d·ªÖ b·ªã overfitting v·ªõi t·∫ßn s·ªë c∆° s·ªü ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh, d·∫´n ƒë·∫øn kh·∫£ nƒÉng ngo·∫°i suy k√©m ƒë·∫øn chi·ªÅu d√†i d√†i h∆°n v∆∞·ª£t qu√° hu·∫•n luy·ªán ho·∫∑c gi·∫£m hi·ªáu su·∫•t trong chi·ªÅu d√†i ng·∫Øn, ho·∫∑c c·∫£ hai trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p. Do ƒë√≥, CLEX c·ªßa ch√∫ng t√¥i nh·∫±m x√¢y d·ª±ng PE scaling li√™n t·ª•c, khuy·∫øn kh√≠ch m√¥ h√¨nh th√≠ch ·ª©ng t·∫ßn s·ªë c∆° s·ªü t∆∞∆°ng ·ª©ng v·ªõi ph·∫°m vi t li√™n t·ª•c nh∆∞ ƒë∆∞·ª£c minh h·ªça trong H√¨nh 2 (ph·∫£i).

4

--- TRANG 5 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

Nh·ªõ l·∫°i r·∫±ng PE scaling tr∆∞·ªõc ƒë√¢y, t∆∞∆°ng ·ª©ng v·ªõi Œ±(t) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a th·ªß c√¥ng, ng·ª• √Ω ƒë·ªông l·ª±c kh√¥ng ƒë·ªïi trong Eq. (10). Trong ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i, ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt h√†m bi·∫øn g:Rd/2‚ÜíRd/2 ƒë·ªÉ m√¥ h√¨nh h√≥a ƒë·ªông l·ª±c, do ƒë√≥ h∆∞·ªõng t·ªõi m·ªôt c√°i nh√¨n t·ªïng qu√°t v√† linh ho·∫°t h∆°n nh∆∞:

dz(t)/dt=g(z(t), t). (11)

B·∫±ng c√°ch h·∫°n ch·∫ø h√†m li√™n k·∫øt v·ªõi c√°c tr·∫°ng th√°i ti·ªÅm ·∫©n z(t), g c√≥ kh·∫£ nƒÉng n·∫Øm b·∫Øt c√°c thay ƒë·ªïi chi ti·∫øt c·ªßa t·∫ßn s·ªë c∆° s·ªü trong qu√° tr√¨nh m·ªü r·ªông chi·ªÅu d√†i. Tuy nhi√™n, vi·ªác ƒë·ªãnh nghƒ©a th·ªß c√¥ng h√†m g nh·∫≠n bi·∫øt z(t) l√† kh√¥ng t·∫ßm th∆∞·ªùng. ·ªû ƒë√¢y, ch√∫ng t√¥i tr·ª±c ti·∫øp tham s·ªë h√≥a h√†m s·ª≠ d·ª•ng m·∫°ng th·∫ßn kinh œï. Do ƒë√≥, ƒë·ªëi v·ªõi b·∫•t k·ª≥ t‚Ä≤‚àà[1, t], c√≥ m·ªôt neural ODE m√¥ h√¨nh h√≥a vi·ªác m·ªü r·ªông t·∫ßn s·ªë c∆° s·ªü nh∆∞

z(t‚Ä≤) =z(1) +‚à´t‚Ä≤_1 gœï(z(t), t)dt, (12)

trong ƒë√≥ t·∫ßn s·ªë c∆° s·ªü t·∫°i chi·ªÅu d√†i t‚Ä≤¬∑L c√≥ th·ªÉ ƒë∆∞·ª£c d·∫´n xu·∫•t b·ªüi Œ∏t‚Ä≤= exp(z(t‚Ä≤)).

C·ª• th·ªÉ h∆°n, ch√∫ng t√¥i √°p d·ª•ng m·ªôt projection l√™n-xu·ªëng nh∆∞ m·∫°ng th·∫ßn kinh, ƒë∆∞·ª£c bi·ªÉu di·ªÖn nh∆∞:

gœï(z(t), t) =Wdown¬∑œÉ(Wup¬∑z(t)) +Œæt, (13)

trong ƒë√≥ Wup‚ààRd/2√óŒªd v√† Wdown‚ààRŒªd√ód/2 l√† c√°c ma tr·∫≠n bi·∫øn ƒë·ªïi, trong ƒë√≥ c√°c tham s·ªë ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi h·ªá s·ªë khu·∫øch ƒë·∫°i Œª; œÉ l√† h√†m k√≠ch ho·∫°t SiLU v√† Œæt l√† scalar embedding ƒë·∫∑c tr∆∞ng cho quy tr√¨nh m·ªü r·ªông t·∫°i h·ªá s·ªë t. ·ªû ƒë√¢y, ch√∫ng t√¥i √°p d·ª•ng ƒë·ªông l·ª±c kh√¥ng ƒë·ªïi c·ªßa Yarn nh∆∞ Œæt ƒë·ªÉ tƒÉng t·ªëc h·ªôi t·ª•, c·ª• th·ªÉ l√†

Œæt=dlogŒ±Yarn(t)/dt=‚àí2i/(d‚àí2)¬∑t[d/2_i=1] (14)

3.3 NGO·∫†I SUY CHI·ªÄU D√ÄI LI√äN T·ª§C: HU·∫§N LUY·ªÜN NG·∫ÆN, KI·ªÇM TRA D√ÄI

PE scaling li√™n t·ª•c c√≥ th·ªÉ ph·ª•c v·ª• nh∆∞ m·ªôt ph∆∞∆°ng ph√°p PE scaling th√≠ch ·ª©ng v√† linh ho·∫°t h∆°n ƒë·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë·∫øn m·ªôt chi·ªÅu d√†i hu·∫•n luy·ªán LTrain nh·∫•t ƒë·ªãnh. Kh√¥ng gi·ªëng nh∆∞ c√°c ph∆∞∆°ng ph√°p PE scaling tr∆∞·ªõc ƒë√¢y ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n h·ªá s·ªë m·ªü r·ªông l·ªõn h∆°n, ƒëi·ªÅu n√†y s·∫Ω d·∫´n ƒë·∫øn hi·ªáu su·∫•t k√©m h∆°n tr√™n c√°c chi·ªÅu d√†i t∆∞∆°ng ·ª©ng v·ªõi c√°c ƒë·ªëi t√°c nh·ªè h∆°n, PE scaling li√™n t·ª•c s·∫Ω cho ph√©p t·ªïng qu√°t h√≥a kh√¥ng ph√° h·ªßy ƒë·∫øn c√°c h·ªá s·ªë m·ªü r·ªông l·ªõn h∆°n th√¥ng qua ƒë·ªông l·ª±c li√™n t·ª•c th√≠ch ·ª©ng. Do ƒë√≥, b·∫±ng c√°ch ch·ªâ ƒë∆°n gi·∫£n m·ªü r·ªông ƒë·ªông l·ª±c li√™n t·ª•c v∆∞·ª£t qu√° h·ªá s·ªë t=LTrain/L trong qu√° tr√¨nh hu·∫•n luy·ªán (trong ƒë√≥ ch√∫ng t√¥i k√Ω hi·ªáu h·ªá s·ªë m·ªü r·ªông mong mu·ªën l√† tTrain), ch√∫ng ta c√≥ th·ªÉ truy c·∫≠p ph∆∞∆°ng ph√°p ngo·∫°i suy chi·ªÅu d√†i li√™n t·ª•c (CLEX), ƒë·∫°t ƒë∆∞·ª£c kh·∫£ nƒÉng "hu·∫•n luy·ªán ng·∫Øn, ki·ªÉm tra d√†i".

H∆°n n·ªØa, ƒë·ªÉ h·ªçc neural ODE trong Eq. (12) cho t li√™n t·ª•c, ch√∫ng t√¥i l·∫•y m·∫´u ng·∫´u nhi√™n t‚Ä≤‚àà[1, tTrain] cho m·ªói b∆∞·ªõc hu·∫•n luy·ªán, cho ph√©p m√¥ h√¨nh th√≠ch ·ª©ng v·ªõi t·∫ßn s·ªë c∆° s·ªü ph·∫°m vi r·ªông m√† kh√¥ng overfitting m·ªôt t·∫ßn s·ªë c·ª• th·ªÉ. L∆∞u √Ω r·∫±ng t·∫ßn s·ªë c∆° s·ªü ƒë∆∞·ª£c r√†ng bu·ªôc v·ªõi ch·ªâ s·ªë v·ªã tr√≠ trong Eq. (1). ƒêi·ªÅu n√†y ti·∫øt l·ªô vi·ªác hu·∫•n luy·ªán ƒë√£ n√™u bao g·ªìm s·ª± kh√¥ng nh·∫•t qu√°n gi·ªØa t·∫ßn s·ªë c∆° s·ªü v√† ch·ªâ s·ªë v·ªã tr√≠: t·∫ßn s·ªë c∆° s·ªü ƒë∆∞·ª£c thay ƒë·ªïi t∆∞∆°ng ·ª©ng v·ªõi t‚Ä≤‚àà[1, tTrain], trong khi ch·ªâ s·ªë v·ªã tr√≠ ƒë∆∞·ª£c c·ªë ƒë·ªãnh l√† {1,2, . . . , LTrain}. ·ªû ƒë√¢y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t chi·∫øn l∆∞·ª£c ngo·∫°i suy v·ªã tr√≠ ƒë·ªÉ gi·∫£i quy·∫øt s·ª± nh·∫•t qu√°n n√†y. Tr√°i ng∆∞·ª£c v·ªõi PI, thu nh·ªè ch·ªâ s·ªë v·ªã tr√≠ v√†o chi·ªÅu d√†i ng·ªØ c·∫£nh, ch√∫ng t√¥i m·ªü r·ªông ch·ªâ s·ªë v·ªã tr√≠ {1,2, . . . , LTrain} c·ªßa c√°c chu·ªói ƒë∆∞·ª£c hu·∫•n luy·ªán l√™n ph·∫°m vi [1, t‚Ä≤¬∑L] cho m·ªói b∆∞·ªõc hu·∫•n luy·ªán. Ch·ªâ s·ªë v·ªã tr√≠ c√≥ th·ªÉ ƒë∆∞·ª£c thu ƒë∆∞·ª£c b·∫±ng c√°ch m·ªü r·ªông ƒë·ªÅu th√†nh {1¬∑s,2¬∑s, . . . , LTrain¬∑s} trong ƒë√≥ s=t‚Ä≤¬∑L/LTrain, ho·∫∑c thay th·∫ø, b·∫±ng c√°ch l·∫•y m·∫´u ng·∫´u nhi√™n LTrain ch·ªâ s·ªë t·ª´ [1, t‚Ä≤¬∑L]. V·ªÅ m·∫∑t th·ª±c nghi·ªám, ch√∫ng t√¥i th·∫•y r·∫±ng l·∫•y m·∫´u ng·∫´u nhi√™n th∆∞·ªùng ho·∫°t ƒë·ªông t·ªët h∆°n. Th√™m th·∫£o lu·∫≠n c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y trong ¬ß4.2.

Trong qu√° tr√¨nh suy lu·∫≠n, k·ªãch b·∫£n l√Ω t∆∞·ªüng l√† thu ƒë∆∞·ª£c t·∫ßn s·ªë c∆° s·ªü t∆∞∆°ng ·ª©ng v·ªõi m·ªói chi·ªÅu d√†i chu·ªói. Tuy nhi√™n, ph∆∞∆°ng ph√°p n√†y ƒë√≤i h·ªèi t√≠nh to√°n cao. ƒê·ªÉ c·∫£i thi·ªán hi·ªáu qu·∫£, ƒë·∫ßu ti√™n ch√∫ng t√¥i cache m·ªôt s·ªë t·∫ßn s·ªë c∆° s·ªü d·∫´n xu·∫•t t·ª´ gœï cho K gi√° tr·ªã t r·ªùi r·∫°c nh∆∞ {tk|k‚àà[1, K]}. ƒê·ªëi v·ªõi m·ªói chu·ªói c√≥ chi·ªÅu d√†i LInfer trong qu√° tr√¨nh suy lu·∫≠n, ch√∫ng t√¥i s·ª≠ d·ª•ng t·∫ßn s·ªë c∆° s·ªü t∆∞∆°ng ·ª©ng v·ªõi c·∫≠n tr√™n g·∫ßn nh·∫•t trong tk¬∑L cho k= 1, . . . , K. Th√¥ng qua ƒëi·ªÅu n√†y, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i gi·ªõi thi·ªáu chi ph√≠ th·ªùi gian kh√¥ng ƒë√°ng k·ªÉ so v·ªõi suy lu·∫≠n th√¥ng th∆∞·ªùng c·ªßa LLM.

4 TH·ª∞C NGHI·ªÜM

Trong ph·∫ßn n√†y, ch√∫ng t√¥i ti·∫øn h√†nh ƒë√°nh gi√° k·ªπ l∆∞·ª°ng v·ªÅ hi·ªáu su·∫•t c·ªßa CLEX trong vi·ªác x·ª≠ l√Ω ng·ªØ c·∫£nh d√†i v√† kh·∫£ nƒÉng ngo·∫°i suy c·ªßa n√≥. Ch√∫ng t√¥i so s√°nh ph∆∞∆°ng ph√°p c·ªßa m√¨nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c bao g·ªìm

5

--- TRANG 6 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

Chi·ªÅu d√†i Chi·ªÅu d√†i ƒë√°nh gi√°
Hu·∫•n luy·ªán 4096 (4k) 8192 (8k) 16,384 (16k) 32,768 (32k) 65,536 (64k) Ph∆∞∆°ng ph√°p
PPL ACC. PPL ACC. PPL ACC. PPL ACC. PPL ACC.

LLaMA-2 4k 6.04 58.18 20.54 44.50 >100 22.43 >1000 12.70 >1000 10.64
CodeLLaMA 16k 7.60 54.88 7.40 55.19 7.33 55.30 15.12 44.70 52.02 31.16
Naive FT 16k 5.98 58.83 5.93 58.91 5.91 58.58 18.31 43.04 >100 26.05
PI 16k 5.90 59.05 5.71 59.44 5.72 59.87 6.05 58.5 8.75 52.02
Yarn (t=16) 16k 6.50 57.28 5.71 59.57 5.73 59.87 5.99 58.13 8.51 52.62
Yarn (t=32) 16k 6.61 57.12 5.94 58.27 5.96 58.04 6.08 57.73 6.22 57.98
CL-Scaling 16k 24.99 37.84 5.86 59.08 5.87 59.05 10.56 50.47 41.09 34.16
ALiBi 4k 6.34 58.01 6.39 57.8 6.41 57.78 6.50 57.47 6.51 56.44
RandomPos 4k 5.88 58.49 >100 34.23 >1000 18.27 >1000 9.31 >1000 7.44
4k 5.86 59.21 5.70 59.62 5.87 58.93 14.53 47.55 30.51 35.33
CLEX 8k 5.98 58.75 5.78 59.44 5.71 59.64 5.99 58.66 11.74 47.50
16k 5.88 59.21 5.68 59.73 5.52 60.28 5.55 60.06 5.64 59.94

B·∫£ng 1: Perplexity (PPL) v√† ƒë·ªô ch√≠nh x√°c d·ª± ƒëo√°n token ti·∫øp theo (ACC.) trong m√¥ h√¨nh h√≥a ng√¥n ng·ªØ v·ªõi chi·ªÅu d√†i ƒë√°nh gi√° t·ª´ 4k ƒë·∫øn 64k. Ch√∫ng t√¥i hu·∫•n luy·ªán LLaMA-2-7B s·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p ngo·∫°i suy chi·ªÅu d√†i tr√™n chi·ªÅu d√†i 4k v√† c√°c ph∆∞∆°ng ph√°p PE scaling tr√™n chi·ªÅu d√†i 16k, trong khi b√°o c√°o k·∫øt qu·∫£ c·ªßa CLEX ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n 4k, 8k v√† 16k. CL-Scaling bi·ªÉu th·ªã hu·∫•n luy·ªán LLaMA-2-7B v·ªõi ph∆∞∆°ng ph√°p m·ªü r·ªông c·ªßa CodeLLaMA nh∆∞ng s·ª≠ d·ª•ng d·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa ch√∫ng t√¥i. C√°c ƒë∆∞·ªùng cong loss hu·∫•n luy·ªán ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 9.

c·∫£ ngo·∫°i suy chi·ªÅu d√†i (t·ª©c l√† ALiBi (Press et al., 2022) v√† random positions (k√Ω hi·ªáu l√† RandomPos) (Ruoss et al., 2023)) v√† c√°c ph∆∞∆°ng ph√°p PE scaling (t·ª©c l√† PI (Chen et al., 2023) v√† Yarn (Peng et al., 2023)). Ch√∫ng t√¥i ch·ªß y·∫øu ti·∫øn h√†nh th·ª±c nghi·ªám tr√™n m√¥ h√¨nh LLaMA-2-7B. ƒê·ªëi v·ªõi m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, ch√∫ng t√¥i hu·∫•n luy·ªán m√¥ h√¨nh c·ªßa m√¨nh v√† c√°c baseline tr√™n 2B token ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ Redpajama-Book (Computer, 2023), ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c t·∫≠p d·ªØ li·ªáu Pile-Books3 (Gao et al., 2020) v√† PG-19 (Rae et al., 2019). Hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh ƒë∆∞·ª£c ƒë√°nh gi√° d·ª±a tr√™n perplexity v√† ƒë·ªô ch√≠nh x√°c d·ª± ƒëo√°n token ti·∫øp theo, v·ªõi chi·ªÅu d√†i chu·ªói ƒë√°nh gi√° l√™n ƒë·∫øn 64k. H∆°n n·ªØa, ch√∫ng t√¥i ti·∫øn h√†nh instruction tuning cho LLaMA-2-7B s·ª≠ d·ª•ng CLEX tr√™n t·∫≠p d·ªØ li·ªáu UltraChat (Ding et al., 2023b). ƒê√°nh gi√° ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n benchmark LongBench (Bai et al., 2023), n∆°i ch√∫ng t√¥i so s√°nh m√¥ h√¨nh c·ªßa m√¨nh v·ªõi GPT-3.5-turbo v√† c√°c m√¥ h√¨nh ngu·ªìn m·ªü d·ª±a tr√™n LLaMA-2 kh√°c ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω ng·ªØ c·∫£nh d√†i. Chi ti·∫øt th√™m v·ªÅ baseline v√† c·∫•u h√¨nh hu·∫•n luy·ªán s·∫Ω ƒë∆∞·ª£c th·∫£o lu·∫≠n trong Ph·ª• l·ª•c ¬ßA, c≈©ng nh∆∞ nhi·ªÅu k·∫øt qu·∫£ th·ª±c nghi·ªám v√† ablation h∆°n trong Ph·ª• l·ª•c ¬ßB.

4.1 M√î H√åNH H√ìA NG√îN NG·ªÆ NG·ªÆ C·∫¢NH D√ÄI

CLEX ƒë·∫°t ƒë∆∞·ª£c ngo·∫°i suy chi·ªÅu d√†i. ƒê·∫ßu ti√™n ch√∫ng t√¥i b√°o c√°o k·∫øt qu·∫£ th·ª±c nghi·ªám c·ªßa c√°c baseline v√† CLEX trong m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, v·ªõi chi·ªÅu d√†i ƒë√°nh gi√° t·ª´ 4k ƒë·∫øn 64k. Nh∆∞ ƒë∆∞·ª£c th·ªÉ hi·ªán trong B·∫£ng 1, CLEX c·ªßa ch√∫ng t√¥i li√™n t·ª•c th·ªÉ hi·ªán hi·ªáu su·∫•t ƒë√°ng ch√∫ √Ω trong ngo·∫°i suy chi·ªÅu d√†i, c√≥ th·ªÉ ngo·∫°i suy chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë·∫øn h∆°n 4√ó chi·ªÅu d√†i hu·∫•n luy·ªán m√† kh√¥ng c√≥ b·∫•t k·ª≥ gi·∫£m hi·ªáu su·∫•t n√†o. L·∫•y CLEX-4k l√†m v√≠ d·ª•, PPL c·ªßa n√≥ tr√™n chu·ªói 4k (chi·ªÅu d√†i hu·∫•n luy·ªán) c√≥ th·ªÉ so s√°nh v·ªõi tr√™n chu·ªói 16k (5.86 vs. 5.87). Khi ƒë∆∞·ª£c ƒë√°nh gi√° tr√™n c√°c chu·ªói trong 16k, CLEX-4k ngang b·∫±ng ho·∫∑c th·∫≠m ch√≠ t·ªët h∆°n t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p so s√°nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i l√™n ƒë·∫øn 16k. H∆°n n·ªØa, v·ªõi s·ª± gia tƒÉng chi·ªÅu d√†i hu·∫•n luy·ªán, CLEX c·ªßa ch√∫ng t√¥i kh√¥ng ch·ªâ th·ªÉ hi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a ƒë·∫ßy h·ª©a h·∫πn ƒë·∫øn ng·ªØ c·∫£nh r·∫•t d√†i (l√™n ƒë·∫øn 64k) m√† c√≤n ƒë·∫£m b·∫£o hi·ªáu su·∫•t tr√™n c√°c chu·ªói ng·∫Øn.

Ch√∫ng t√¥i c≈©ng th·∫•y r·∫±ng c√°c ph∆∞∆°ng ph√°p PE scaling r·ªùi r·∫°c (t·ª©c l√† PI v√† Yarn) c√≥ thu·ªôc t√≠nh t·ª± m·ªü r·ªông: hu·∫•n luy·ªán v·ªõi t·∫ßn s·ªë c∆° s·ªü ƒë∆∞·ª£c m·ªü r·ªông trang b·ªã cho m√¥ h√¨nh kh·∫£ nƒÉng ngo·∫°i suy ƒë·∫øn c√°c ƒë·ªëi t√°c ƒë∆∞·ª£c m·ªü r·ªông th√™m (xem Ph·ª• l·ª•c ¬ßB.2 ƒë·ªÉ th·∫£o lu·∫≠n th√™m.). Nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 1, tuy nhi√™n, kh·∫£ nƒÉng ngo·∫°i suy c·ªßa c√°c ph∆∞∆°ng ph√°p n√†y b·ªã h·∫°n ch·∫ø, k√®m theo s·ª± suy gi·∫£m hi·ªáu su·∫•t ƒë√°ng k·ªÉ ngay c·∫£ trong chi·ªÅu d√†i ng·ªØ c·∫£nh th√¥ng th∆∞·ªùng. ƒêi·ªÅu n√†y ch·ªâ ra th√°ch th·ª©c v·ªën c√≥ trong vi·ªác ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng tinh t·∫ø gi·ªØa ngo·∫°i suy ƒë·∫øn chi·ªÅu d√†i d√†i h∆°n v√† duy tr√¨ hi·ªáu su·∫•t trong chi·ªÅu d√†i ng·∫Øn khi s·ª≠ d·ª•ng h·ªá s·ªë m·ªü r·ªông r·ªùi r·∫°c. Ng∆∞·ª£c l·∫°i, CLEX gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y th√¥ng qua ƒë·ªông l·ª±c li√™n t·ª•c c√≥ th·ªÉ h·ªçc, cung c·∫•p ngo·∫°i suy chi ti·∫øt h∆°n trong khi b·∫£o t·ªìn hi·ªáu su·∫•t cho ng·ªØ c·∫£nh n·ªôi b·ªô.

L∆∞u √Ω r·∫±ng ALiBi c√≥ th·ªÉ m·ªü r·ªông xa h∆°n CLEX ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n c√°c chu·ªói 4k (m·∫∑c d√π th∆∞·ªùng t·∫°o ra k·∫øt qu·∫£ k√©m h∆°n), c√°c th·ª±c nghi·ªám c·ªßa ch√∫ng t√¥i ti·∫øt l·ªô r·∫±ng nh·ªØng l·ª£i √≠ch n√†y c√≥ th·ªÉ ƒë·∫øn v·ªõi chi ph√≠ c·ªßa th√¥ng tin d√†i h·∫°n, d·∫´n ƒë·∫øn hi·ªáu su·∫•t k√©m trong c√°c nhi·ªám v·ª• th·ª±c t·∫ø ng·ªØ c·∫£nh d√†i (xem ¬ß4.3 ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt).

6

--- TRANG 7 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

4k 8k 16k 32k 64k
Chi·ªÅu d√†i chu·ªói ƒë√°nh gi√°102030PPL
CLEX-7B-4k
CLEX-7B-8k
CLEX-13B-4k

0.25 0.5 0.75 11.25 1.5 1.75 2
K√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán (T·ª∑ token)102030
CLEX-7B-4k tr√™n 16k
CLEX-7B-4k tr√™n 32k
CLEX-13B-4k tr√™n 16k
CLEX-13B-4k tr√™n 32k

H√¨nh 3: Tr√°i: PPL c·ªßa CLEX tr√™n c√°c chi·ªÅu d√†i chu·ªói ƒë√°nh gi√° kh√°c nhau v·ªõi k√≠ch th∆∞·ªõc tham s·ªë 7B v√† 13B. Ph·∫£i: PPL c·ªßa CLEX qua k√≠ch th∆∞·ªõc d·ªØ li·ªáu hu·∫•n luy·ªán bi·∫øn ƒë·ªïi v·ªõi c√°c k√≠ch th∆∞·ªõc tham s·ªë v√† chi·ªÅu d√†i ƒë√°nh gi√° kh√°c nhau.

4k 8k 16k 32k 64k204060PPL
Li√™n t·ª•c vs R·ªùi r·∫°c
Li√™n t·ª•c
R·ªùi r·∫°c

4k 8k 16k 32k 64k
Chi·ªÅu d√†i chu·ªói ƒë√°nh gi√°20406080
Chi·∫øn l∆∞·ª£c l·∫•y m·∫´u
Ng·∫´u nhi√™n
ƒê·ªÅu
Kh√¥ng l·∫•y m·∫´u

4k 8k 16k 32k 64k102030
H·ªá s·ªë Œª
Œª=1
Œª=2
Œª=4

H√¨nh 4: Nghi√™n c·ª©u ablation cho ƒë·ªông l·ª±c li√™n t·ª•c, chi·∫øn l∆∞·ª£c l·∫•y m·∫´u v√† h·ªá s·ªë Œª trong Eq. (13).

Quy lu·∫≠t m·ªü r·ªông cho kh·∫£ nƒÉng ngo·∫°i suy c·ªßa CLEX. ƒê·ªÉ ƒëi·ªÅu tra hi·ªáu qu·∫£ c·ªßa CLEX qua quy m√¥ c·ªßa m√¥ h√¨nh c∆° s·ªü v√† k√≠ch th∆∞·ªõc d·ªØ li·ªáu hu·∫•n luy·ªán, ch√∫ng t√¥i ti·∫øp t·ª•c chuy·ªÉn ph∆∞∆°ng ph√°p c·ªßa m√¨nh sang LLaMA-2-13B. Nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 3, khi m·ªü r·ªông quy m√¥ m√¥ h√¨nh c∆° s·ªü t·ª´ 7B l√™n 13B m·ªôt c√°ch ƒë∆°n gi·∫£n, CLEX c·ªßa ch√∫ng t√¥i th·ªÉ hi·ªán kh·∫£ nƒÉng ngo·∫°i suy tƒÉng c∆∞·ªùng ƒë·∫øn chi·ªÅu d√†i ng·ªØ c·∫£nh d√†i h∆°n. C·ª• th·ªÉ, kh·∫£ nƒÉng ngo·∫°i suy c·ªßa CLEX-13B ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i 4k ti·ªám c·∫≠n v·ªõi CLEX-7B ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n 8k. Trong khi quy m√¥ d·ªØ li·ªáu hu·∫•n luy·ªán, ƒë√°ng ng·∫°c nhi√™n h∆°n, kh√¥ng ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ ƒë·∫øn kh·∫£ nƒÉng ngo·∫°i suy c·ªßa CLEX. C√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi 0.25B ho·∫∑c 2B token v·ªõi chi·ªÅu d√†i chu·ªói 4k ƒë·∫°t ƒë∆∞·ª£c PPL t∆∞∆°ng ƒë∆∞∆°ng khi ƒë√°nh gi√° tr√™n chi·ªÅu d√†i 16k ho·∫∑c 32k trong H√¨nh 3, cho th·∫•y bi√™n ƒë·ªô kh√¥ng ƒë√°ng k·ªÉ t·ª´ k√≠ch th∆∞·ªõc d·ªØ li·ªáu hu·∫•n luy·ªán l·ªõn h∆°n. ƒêi·ªÅu n√†y c≈©ng ng·ª• √Ω r·∫±ng CLEX c√≥ th·ªÉ m·ªü r·ªông hi·ªáu qu·∫£ chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa LLM th√¥ng qua c√°c b∆∞·ªõc hu·∫•n luy·ªán t·ªëi thi·ªÉu t∆∞∆°ng t·ª± PI v√† Yarn.

D·ª±a tr√™n nh·ªØng ph√°t hi·ªán n√†y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt quy lu·∫≠t m·ªü r·ªông cho CLEX: ƒë·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa LLM ƒë·∫øn c√°c chi·ªÅu d√†i mong mu·ªën v·ª´a ph·∫£i (v√≠ d·ª•, 16k ‚Üí64k), ng∆∞·ªùi ta n√™n m·ªü r·ªông t·ª∑ l·ªá chi·ªÅu d√†i chu·ªói hu·∫•n luy·ªán (v√≠ d·ª•, 4k ‚Üí16k). ƒê·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh l√™n ƒë·∫øn c√°c chi·ªÅu d√†i ƒë√°ng k·ªÉ (v√≠ d·ª•, >200k), k√≠ch th∆∞·ªõc tham s·ªë c·ªßa m√¥ h√¨nh c∆° s·ªü n√™n ƒë∆∞·ª£c tƒÉng t∆∞∆°ng ·ª©ng trong khi duy tr√¨ chi·ªÅu d√†i hu·∫•n luy·ªán, v√¨ ng·ªØ c·∫£nh c√≥ th·ªÉ chi·∫øm nhi·ªÅu d·∫•u v·∫øt h∆°n so v·ªõi tham s·ªë m√¥ h√¨nh. L∆∞u √Ω r·∫±ng vi·ªác m·ªü r·ªông d·ªØ li·ªáu hu·∫•n luy·ªán kh√¥ng ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn kh·∫£ nƒÉng ngo·∫°i suy c·ªßa CLEX, nh∆∞ng c√≥ th·ªÉ ƒë∆∞·ª£c t√≠ch h·ª£p ng·∫ßm khi m·ªü r·ªông LLM ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc c∆° s·ªü.

4.2 NGHI√äN C·ª®U ABLATION

B√¢y gi·ªù ch√∫ng t√¥i ti·∫øn h√†nh ba lo·∫°i ablation ƒë·ªÉ ƒëi·ªÅu tra hi·ªáu qu·∫£ c·ªßa c√°c th√†nh ph·∫ßn trong CLEX:

ƒê·ªông l·ª±c li√™n t·ª•c. ƒê·ªÉ h·ªçc ƒë·ªông l·ª±c li√™n t·ª•c s·ª≠ d·ª•ng neural ODE, ch√∫ng t√¥i √°p d·ª•ng ph∆∞∆°ng ph√°p hu·∫•n luy·ªán ri√™ng bi·ªát bao g·ªìm vi·ªác l·∫•y m·∫´u h·ªá s·ªë m·ªü r·ªông t cho m·ªói batch d·ªØ li·ªáu. ·ªû ƒë√¢y ch√∫ng t√¥i t√¨m hi·ªÉu xem kh·∫£ nƒÉng ngo·∫°i suy ƒë·∫∑c bi·ªát c·ªßa CLEX c√≥ ch·ªâ xu·∫•t ph√°t t·ª´ bi·∫øn t thay v√¨ ƒë·ªông l·ª±c li√™n t·ª•c. Ch√∫ng t√¥i s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Yarn r·ªùi r·∫°c v·ªõi t= 16, tr·∫£i qua c√πng quy tr√¨nh hu·∫•n luy·ªán c·ªßa CLEX nh∆∞ng lo·∫°i b·ªè c√°c tham s·ªë ODE, ph·ª•c v·ª• nh∆∞ baseline r·ªùi r·∫°c. Trong H√¨nh 4 (tr√°i), ch√∫ng t√¥i ph√°t hi·ªán r·∫±ng ph∆∞∆°ng ph√°p r·ªùi r·∫°c ƒë∆∞·ª£c trang b·ªã t ƒë∆∞·ª£c l·∫•y m·∫´u ng·∫´u nhi√™n ho·∫°t ƒë·ªông k√©m ƒë√°ng k·ªÉ so v·ªõi CLEX c·ªßa ch√∫ng t√¥i, cho th·∫•y t√≠nh thi·∫øt y·∫øu c·ªßa ƒë·ªông l·ª±c li√™n t·ª•c c√≥ th·ªÉ h·ªçc trong CLEX ƒë·ªÉ truy c·∫≠p kh·∫£ nƒÉng ngo·∫°i suy.

7

--- TRANG 8 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

C√¢u h·ªèi ƒë∆°n t√†i li·ªáu
C√¢u h·ªèi ƒëa t√†i li·ªáu
T√≥m t·∫Øt
H·ªçc √≠t shotNhi·ªám v·ª• t·ªïng h·ª£pHo√†n th√†nh m√£

0 20 40 60 80 100

GPT-3.5-Turbo-16k LongChat-v1.5-7B-32k CodeLlama-7B-16k
Vicuna-v1.5-7B-16k Baichuan-13B-4k (ALiBi) CLEX-7B-4k (C·ªßa ch√∫ng t√¥i)

4k 16k 32k
Chi·ªÅu d√†i chu·ªói ƒë∆∞·ª£c hu·∫•n luy·ªán203040Avg. T·∫•tGPT-3.5-Turbo-16k
LongChat-v1.5-7B-32k
CodeLLaMA-7B-16k
Vicuna-v1.5-7B-16k
Baichuan-13B-4k (ALiBi)
CLEX-7B-4k (C·ªßa ch√∫ng t√¥i)

H√¨nh 5: Tr√°i: ƒëi·ªÉm trung b√¨nh cho m·ªói lƒ©nh v·ª±c nhi·ªám v·ª• trong LongBench. Ph·∫£i: ƒëi·ªÉm trung b√¨nh c·ªßa t·∫•t c·∫£ nhi·ªám v·ª• t∆∞∆°ng ·ª©ng v·ªõi chi·ªÅu d√†i hu·∫•n luy·ªán c·ªßa m·ªói m√¥ h√¨nh. L∆∞u √Ω r·∫±ng CLEX ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i chu·ªói 4k nh∆∞ng ƒë∆∞·ª£c ki·ªÉm tra tr·ª±c ti·∫øp tr√™n chi·ªÅu d√†i ng·ªØ c·∫£nh 16k m√† kh√¥ng c·∫Øt b·ªõt.

Ngo·∫°i suy v·ªã tr√≠. Ch√∫ng t√¥i √°p d·ª•ng chi·∫øn l∆∞·ª£c ngo·∫°i suy v·ªã tr√≠, m·ªü r·ªông ph·∫°m vi ch·ªâ s·ªë v·ªã tr√≠ trong c√°c chu·ªói hu·∫•n luy·ªán b·∫±ng c√°ch l·∫•y m·∫´u t·ª´ ph·∫°m vi r·ªông h∆°n, ƒë·ªÉ ƒëi·ªÅu h√≤a s·ª± kh√¥ng nh·∫•t qu√°n gi·ªØa t·∫ßn s·ªë c∆° s·ªü v√† ch·ªâ s·ªë v·ªã tr√≠ trong qu√° tr√¨nh hu·∫•n luy·ªán. Trong nghi√™n c·ª©u n√†y, ch√∫ng t√¥i ki·ªÉm tra t√°c ƒë·ªông c·ªßa c√°c chi·∫øn l∆∞·ª£c l·∫•y m·∫´u kh√°c nhau (ƒë·ªÅu ho·∫∑c ng·∫´u nhi√™n) v√† ƒë·ªëiÊØî ch√∫ng v·ªõi ch·ªâ s·ªë v·ªã tr√≠ th√¥ng th∆∞·ªùng. K·∫øt qu·∫£ trong H√¨nh 4 nh·∫•n m·∫°nh hi·ªáu qu·∫£ c·ªßa ngo·∫°i suy v·ªã tr√≠ trong CLEX, kh√¥ng c√≥ n√≥ th√¨ kh·∫£ nƒÉng ngo·∫°i suy c·ªßa m√¥ h√¨nh gi·∫£m ƒë√°ng k·ªÉ. H∆°n n·ªØa, l·∫•y m·∫´u ng·∫´u nhi√™n ho·∫°t ƒë·ªông t·ªët h∆°n m·ªôt ch√∫t so v·ªõi l·∫•y m·∫´u ƒë·ªÅu, v√¨ v·∫≠y ch√∫ng t√¥i √°p d·ª•ng n√≥ trong t·∫•t c·∫£ c√°c th·ª±c nghi·ªám.

Quy m√¥ tham s·ªë c·ªßa ODE. Ch√∫ng t√¥i c≈©ng nghi√™n c·ª©u t√°c ƒë·ªông c·ªßa k√≠ch th∆∞·ªõc tham s·ªë c·ªßa neural ODE trong CLEX. K√≠ch th∆∞·ªõc tham s·ªë ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi Œª, c·ª• th·ªÉ l√† h·ªá s·ªë khu·∫øch ƒë·∫°i trong Eq. (13). Trong H√¨nh 4, ch√∫ng t√¥i v·∫Ω k·∫øt qu·∫£ c·ªßa CLEX v·ªõi Œª= 1,2,4, trong ƒë√≥ ch√∫ng ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t t∆∞∆°ng t·ª±. L∆∞u √Ω r·∫±ng k√≠ch th∆∞·ªõc tham s·ªë c·ªßa neural ODE trong CLEX kh√° nh·ªè ngay c·∫£ khi Œª= 4, v√¨ chi·ªÅu d trong Eq. (13) th∆∞·ªùng b·∫±ng 128. M·∫∑c d√π c√≥ th·ªÉ tƒÉng c∆∞·ªùng CLEX v·ªõi Œª l·ªõn h∆°n (v√≠ d·ª•, 32), ch√∫ng t√¥i ƒë·∫∑t Œª=1 trong t·∫•t c·∫£ c√°c th·ª±c nghi·ªám ƒë·ªÉ t√°c ƒë·ªông t·ªëi thi·ªÉu ƒë·∫øn ƒë·ªô tr·ªÖ suy lu·∫≠n.

4.3 ƒê√ÅNH GI√Å TR√äN BENCHMARK NG·ªÆ C·∫¢NH D√ÄI

ƒê·ªÉ x√°c ƒë·ªãnh hi·ªáu su·∫•t to√†n di·ªán c·ªßa CLEX trong c√°c t√¨nh hu·ªëng th·ª±c t·∫ø, ch√∫ng t√¥i ti·∫øp t·ª•c ti·∫øn h√†nh ƒë√°nh gi√° tr√™n benchmark LongBench zero-shot. Benchmark n√†y bao g·ªìm m·ªôt lo·∫°t c√°c nhi·ªám v·ª•, nh∆∞ tr·∫£ l·ªùi c√¢u h·ªèi, t√≥m t·∫Øt v√† ho√†n th√†nh m√£, trong ƒë√≥ chi·ªÅu d√†i ƒë√°nh gi√° dao ƒë·ªông t·ª´ 5k ƒë·∫øn 15k. Ch√∫ng t√¥i th·ª±c hi·ªán instruction tuning th√≠ ƒëi·ªÉm cho LLaMA-2-7B b·∫±ng c√°ch s·ª≠ d·ª•ng CLEX tr√™n t·∫≠p d·ªØ li·ªáu UltraChat, v·ªõi chi·ªÅu d√†i chu·ªói 4k. Trong qu√° tr√¨nh suy lu·∫≠n, ch√∫ng t√¥i khai th√°c t·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë·ªÉ gi·∫£i quy·∫øt chi·ªÅu d√†i ng·ªØ c·∫£nh 16k, t·ª´ ƒë√≥ ƒë·∫£m b·∫£o khai th√°c to√†n di·ªán th√¥ng tin ng·ªØ c·∫£nh trong c√°c nhi·ªám v·ª•. Nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 5, ch√∫ng t√¥i tr√¨nh b√†y ƒëi·ªÉm trung b√¨nh c·ªßa m·ªói lƒ©nh v·ª±c trong LongBench cho CLEX, so s√°nh v·ªõi m√¥ h√¨nh GPT-3.5-Turbo-16k v√† c√°c LLM ngu·ªìn m·ªü m·∫°nh nh∆∞ LongChat-v1.5-7B-32k v√† CodeLLaMA-7B-16k.

N√≥i chung, khi ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi c√°c chu·ªói c√≥ chi·ªÅu d√†i 4k, CLEX ƒë·ª©ng v·ªØng tr∆∞·ªõc b·∫•t k·ª≥ LLM ngu·ªìn m·ªü n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i l√™n ƒë·∫øn 32k. Trong c√°c lƒ©nh v·ª±c c·ª• th·ªÉ c·ªßa T√≥m t·∫Øt, H·ªçc √≠t shot v√† Ho√†n th√†nh m√£, CLEX tr√™n LLaMA-2-7B v·∫´n c·∫°nh tranh ho·∫∑c th·∫≠m ch√≠ v∆∞·ª£t qua GPT-3.5-Turbo-16k. Ch√∫ng t√¥i l∆∞u √Ω r·∫±ng Baichuan-13B-4k, ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc v·ªõi ALiBi (Press et al., 2022), th·ªÉ hi·ªán hi·ªáu su·∫•t k√©m ƒë√°ng ch√∫ √Ω tr√™n LongBench m·∫∑c d√π c√≥ k√≠ch th∆∞·ªõc tham s·ªë l·ªõn h∆°n. Ngo√†i ra, k·∫øt qu·∫£ k√©m t∆∞∆°ng t·ª± ƒë∆∞·ª£c ƒë·∫°t b·ªüi ALiBi khi √°p d·ª•ng n√≥ l√™n LLaMA-2-7B s·ª≠ d·ª•ng c√πng quy tr√¨nh hu·∫•n luy·ªán nh∆∞ CLEX (xem Ph·ª• l·ª•c ¬ßB.5). ƒêi·ªÅu n√†y c√≥ th·ªÉ do ALiBi qu√° ch√∫ tr·ªçng v√†o ng·ªØ c·∫£nh c·ª•c b·ªô th√¥ng qua bias attention, trong khi c√≥ l·ª£i cho m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, h·∫°n ch·∫ø truy c·∫≠p th√¥ng tin ng·ªØ c·∫£nh d√†i trong c√°c nhi·ªám v·ª• th·ª±c t·∫ø. Ng∆∞·ª£c l·∫°i, CLEX m·ªü r·ªông tr·ª±c ti·∫øp chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa LLM m√† kh√¥ng √°p ƒë·∫∑t b·∫•t k·ª≥ r√†ng bu·ªôc n√†o tr√™n ng·ªØ c·∫£nh, li√™n t·ª•c ƒë·∫°t ƒë∆∞·ª£c kh·∫£ nƒÉng ngo·∫°i suy v∆∞·ª£t tr·ªôi tr√™n c·∫£ m√¥ h√¨nh h√≥a ng√¥n ng·ªØ v√† LongBench. ƒêi·ªÅu n√†y ch·ª©ng th·ª±c ti·ªÅm nƒÉng ƒë√°ng k·ªÉ c·ªßa CLEX ƒë·ªÉ ph·ª•c v·ª• nh∆∞ ph∆∞∆°ng ph√°p ti√™n ti·∫øn cho vi·ªác ngo·∫°i suy chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa LLM ƒë·ªÉ xu·∫•t s·∫Øc trong c√°c ·ª©ng d·ª•ng ng·ªØ c·∫£nh d√†i.

8

--- TRANG 9 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

Ngo√†i ra, ch√∫ng t√¥i nh·∫•n m·∫°nh r·∫±ng CLEX c·ªßa ch√∫ng t√¥i ch·ªâ gi·ªõi thi·ªáu ƒë·ªô tr·ªÖ suy lu·∫≠n r·∫•t nh·ªè. V·ªõi chi·ªÅu d√†i ng·ªØ c·∫£nh 16k trong LongBench v·ªõi chi·ªÅu d√†i sinh ra 512, th√¥ng l∆∞·ª£ng sinh ra gi·ªØa CLEX-7B v√† LLaMA-2-7B c·ªßa ch√∫ng t√¥i c√≥ th·ªÉ so s√°nh (27.8 token/s vs 28.3 token/s, trong m·ªôt A100 ƒë∆°n), khi s·ª≠ d·ª•ng c∆° ch·∫ø cache ƒë∆∞·ª£c gi·ªõi thi·ªáu trong ¬ß3.3.

5 C√îNG TR√åNH LI√äN QUAN

Ki·∫øn tr√∫c ph√¢n c·∫•p / Sparse Attention. ƒê·ªÉ v∆∞·ª£t qua ƒë·ªô ph·ª©c t·∫°p b·∫≠c hai c·ªßa attention, Dai et al. (2019) ƒë·ªÅ xu·∫•t Transformer-XL x·ª≠ l√Ω chu·ªói d√†i ·ªü c·∫•p ƒë·ªô ƒëo·∫°n b·∫±ng Transformer, v·ªõi c√°c ƒëo·∫°n n√†y t∆∞∆°ng t√°c th√¥ng qua c∆° ch·∫ø ƒë·ªá quy. Recurrent Memory Transformer (Bulatov et al., 2022) tinh ch·ªânh c∆° ch·∫ø n√†y b·∫±ng c√°ch k·∫øt h·ª£p c√°c token b·ªô nh·ªõ ƒë·∫∑c bi·ªát v√†o ƒë·ªá quy, c√≥ kh·∫£ nƒÉng m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë·∫øn h√†ng tri·ªáu (Bulatov et al., 2023). M·∫∑t kh√°c, Child et al. (2019); Beltagy et al. (2020) ƒë·ªÅ xu·∫•t s·ª≠ d·ª•ng sparse attention ƒë·ªÉ tr√°nh truy c·∫≠p ƒë·∫ßy ƒë·ªß ƒë·∫øn c√°c chu·ªói d√†i, do ƒë√≥ gi·∫£m ƒë·ªô ph·ª©c t·∫°p. Sparse attention ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng b·ªüi Ding et al. (2023a) ƒë·ªÉ m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa transformer th√†nh h√†ng t·ª∑. Tuy nhi√™n, c√°c ph∆∞∆°ng ph√°p n√†y hy sinh vi·ªác s·ª≠ d·ª•ng to√†n b·ªô chu·ªói trong qu√° tr√¨nh attention, d·∫´n ƒë·∫øn m·∫•t m√°t kh√¥ng th·ªÉ tr√°nh kh·ªèi m·ªôt s·ªë th√¥ng tin ng·ªØ c·∫£nh. Ngo√†i ra, vi·ªác s·ª≠a ƒë·ªïi ki·∫øn tr√∫c m√¥ h√¨nh l√†m cho c√°c ph∆∞∆°ng ph√°p n√†y kh√≥ √°p d·ª•ng cho c√°c LLM ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc hi·ªán c√≥. Ng∆∞·ª£c l·∫°i, CLEX c·ªßa ch√∫ng t√¥i ph·ª•c v·ª• nh∆∞ m·ªôt th√†nh ph·∫ßn drop-in cho LLM, c√≥ th·ªÉ m·ªü r·ªông hi·ªáu qu·∫£ kh·∫£ nƒÉng c·ªßa m√¥ h√¨nh ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô chu·ªói d√†i m√† kh√¥ng l√†m m·∫•t th√¥ng tin ng·ªØ c·∫£nh m·ªôt c√°ch r√µ r√†ng.

Ngo·∫°i suy chi·ªÅu d√†i. X√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng ƒë∆∞·ª£c ƒë·∫∑t b·ªüi ALiBi (Press et al., 2022), m·ªôt lo·∫°t nghi√™n c·ª©u (Sun et al., 2023; Chi et al., 2022; 2023) t√¨m c√°ch hu·∫•n luy·ªán c√°c m√¥ h√¨nh d·ª±a tr√™n Transformer tr√™n chi·ªÅu d√†i ng·∫Øn, trong khi tr·ª±c ti·∫øp ki·ªÉm tra tr√™n c√°c ƒë·ªëi t√°c d√†i h∆°n. C√°c ph∆∞∆°ng ph√°p n√†y thay th·∫ø position embedding b·∫±ng bias ƒë∆∞·ª£c gi·ªõi thi·ªáu v√†o ƒëi·ªÉm attention, t·ª´ ƒë√≥ k·∫øt h·ª£p th√¥ng tin v·ªã tr√≠. ƒê√°ng ch√∫ √Ω, bias th∆∞·ªùng mang l·∫°i l·ª£i √≠ch cao h∆°n cho c√°c token g·∫ßn h∆°n. C∆° ch·∫ø n√†y m·ªôt c√°ch tr·ª±c quan khu·∫øch ƒë·∫°i ng·ªØ c·∫£nh c·ª•c b·ªô cho m·ªói token v·ªõi chi ph√≠ c·ªßa th√¥ng tin xa. Do ƒë√≥, c√°c ph∆∞∆°ng ph√°p ngo·∫°i suy chi·ªÅu d√†i n√†y g·∫∑p th√°ch th·ª©c trong vi·ªác x·ª≠ l√Ω hi·ªáu qu·∫£ ng·ªØ c·∫£nh d√†i trong c√°c ·ª©ng d·ª•ng th·ª±c t·∫ø (Pal et al., 2023). Tuy nhi√™n, CLEX c·ªßa ch√∫ng t√¥i th·ªÉ hi·ªán hi·ªáu qu·∫£ ƒë√°ng ch√∫ √Ω trong c√°c nhi·ªám v·ª• th·ª±c t·∫ø nh∆∞ t√≥m t·∫Øt, cho th·∫•y kh·∫£ nƒÉng ngo·∫°i suy th·ª±c t·∫ø cho c√°c ·ª©ng d·ª•ng.

Position Embedding (PE) Scaling. Nghi√™n c·ª©u g·∫ßn ƒë√¢y ƒë√£ t√¨m c√°ch m·ªü r·ªông chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa Transformer th√¥ng qua vi·ªác m·ªü r·ªông RoPE ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i. C·ª• th·ªÉ, Chen et al. (2023) ƒë·ªÅ xu·∫•t position interpolation, m·ªôt ph∆∞∆°ng ph√°p m·ªü r·ªông hi·ªáu qu·∫£ c·ª≠a s·ªï ng·ªØ c·∫£nh b·∫±ng c√°ch m·ªü r·ªông ch·ªâ s·ªë v·ªã tr√≠ trong RoPE. T∆∞∆°ng t·ª±, Peng et al. (2023); Rozi`ere et al. (2023) ch·ªçn m·ªü r·ªông t·∫ßn s·ªë c∆° s·ªü, ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi. Tuy nhi√™n, c√°c ph∆∞∆°ng ph√°p n√†y c·∫ßn hu·∫•n luy·ªán (ho·∫∑c fine-tuning) tr√™n chi·ªÅu d√†i m·ªü r·ªông mong mu·ªën. K·∫øt qu·∫£ l√†, ch√∫ng th·ªÉ hi·ªán kh·∫£ nƒÉng ngo·∫°i suy h·∫°n ch·∫ø v∆∞·ª£t qu√° chi·ªÅu d√†i ƒë∆∞·ª£c hu·∫•n luy·ªán v√† th·∫≠m ch√≠ g·∫∑p ph·∫£i gi·∫£m hi·ªáu su·∫•t trong c√°c chi·ªÅu d√†i ng·∫Øn h∆°n. Trong CLEX, ch√∫ng t√¥i t·ªïng qu√°t h√≥a PE scaling r·ªùi r·∫°c th√†nh m·ªôt ƒë·ªëi t√°c li√™n t·ª•c, do ƒë√≥ ngo·∫°i suy ƒë·ªìng nh·∫•t chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa LLM trong khi b·∫£o t·ªìn hi·ªáu su·∫•t trong c√°c chi·ªÅu d√†i ng·∫Øn.

6 K·∫æT LU·∫¨N

Ch√∫ng t√¥i ƒë√£ tr√¨nh b√†y Continuous Length EXtrapolation (CLEX), m·ªôt ph∆∞∆°ng ph√°p ti·ªÉu thuy·∫øt m·ªü r·ªông hi·ªáu qu·∫£ chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn (LLM) ƒë·∫øn h∆°n 4x chi·ªÅu d√†i hu·∫•n luy·ªán (fine-tuning) m√† kh√¥ng c√≥ b·∫•t k·ª≥ suy gi·∫£m hi·ªáu su·∫•t n√†o. CLEX s·ª≠ d·ª•ng neural ODE ƒë·ªÉ h·ªçc ƒë·ªông l·ª±c li√™n t·ª•c qua h·ªá s·ªë m·ªü r·ªông chi·ªÅu d√†i trong qu√° tr√¨nh PE scaling, do ƒë√≥ cho ph√©p m·ªü r·ªông chi ti·∫øt cho t·∫ßn s·ªë c∆° s·ªü trong RoPE. Ch√∫ng t√¥i ti·∫øn h√†nh th·ª±c nghi·ªám k·ªπ l∆∞·ª°ng ƒë·ªÉ ƒëi·ªÅu tra hi·ªáu qu·∫£ c·ªßa CLEX so v·ªõi nhi·ªÅu LLM m·∫°nh, bao g·ªìm nhi·ªám v·ª• m√¥ h√¨nh h√≥a ng√¥n ng·ªØ v√† benchmark LongBench. K·∫øt qu·∫£ th·ª±c nghi·ªám ƒë√£ ch·ª©ng minh kh·∫£ nƒÉng ngo·∫°i suy ƒë·∫∑c bi·ªát c·ªßa CLEX, trong ƒë√≥ CLEX c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi chi·ªÅu d√†i chu·ªói 4k c√≥ ti·ªÅm nƒÉng duy tr√¨ kh·∫£ nƒÉng c·∫°nh tranh v·ªõi b·∫•t k·ª≥ LLM ngu·ªìn m·ªü ng·ªØ c·∫£nh d√†i n√†o (v√≠ d·ª•, CodeLLaMA) ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i l√™n ƒë·∫øn 32k. Nh·ªØng k·∫øt qu·∫£ n√†y l√†m n·ªïi b·∫≠t ti·ªÅm nƒÉng c·ªßa CLEX nh∆∞ m·ªôt ph∆∞∆°ng ph√°p ti√™n ti·∫øn ƒë·ªÉ m·ªü r·ªông hi·ªáu qu·∫£ chi·ªÅu d√†i ng·ªØ c·∫£nh c·ªßa LLM, m·ªü ƒë∆∞·ªùng cho nh·ªØng ti·∫øn b·ªô trong c√°c ·ª©ng d·ª•ng ng·ªØ c·∫£nh d√†i. B·∫±ng c√°ch m·ªü r·ªông k√≠ch th∆∞·ªõc m√¥ h√¨nh c∆° s·ªü, ch√∫ng t√¥i th·∫•y CLEX c√≥ th·ªÉ ƒë∆∞·ª£c tƒÉng c∆∞·ªùng t∆∞∆°ng ·ª©ng v√† sau ƒë√≥ c√≥ kh·∫£ nƒÉng ngo·∫°i suy m√¥ h√¨nh ƒë·∫øn chi·ªÅu d√†i ng·ªØ c·∫£nh d√†i h∆°n. Thu·ªôc t√≠nh n√†y cho th·∫•y hi·ªáu qu·∫£ h·∫•p d·∫´n c·ªßa CLEX trong k·ª∑ nguy√™n LLM.

9

--- TRANG 10 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

L·ªúI C·∫¢M ∆†N

C√¥ng vi·ªác n√†y ƒë∆∞·ª£c h·ªó tr·ª£ ƒë√°ng k·ªÉ b·ªüi DAMO Academy th√¥ng qua Ch∆∞∆°ng tr√¨nh Th·ª±c t·∫≠p Nghi√™n c·ª©u DAMO Academy. Shangsong Liang ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi Qu·ªπ Khoa h·ªçc T·ª± nhi√™n Qu·ªëc gia Trung Qu·ªëc (S·ªë c·∫•p 61906219) v√† ƒê·∫°i h·ªçc Tr√≠ tu·ªá Nh√¢n t·∫°o Mohamed bin Zayed, C√°c Ti·ªÉu v∆∞∆°ng qu·ªëc ·∫¢ R·∫≠p Th·ªëng nh·∫•t.

T√ÄI LI·ªÜU THAM KH·∫¢O

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. 2023. URL https://arxiv.org/abs/2308.14508 .

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150 , 2020. URL https://arxiv.org/abs/2004.05150 .

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large Language Models . Association for Computational Linguistics, May 2022. URL https://aclanthology.org/2022.bigscience-1.9 .

Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=Uynr3iPhksa .

Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond with rmt. 2023. URL https://arxiv.org/abs/2304.11062 .

Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf .

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. 2023. URL https://arxiv.org/abs/2306.15595 .

Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. In Advances in Neural Information Processing Systems . Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/37a413841a614b5414b333585e7613b8-Paper-Conference.pdf .

Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, July 2023. URL https://aclanthology.org/2023.acl-long.756 .

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. 2019. URL https://arxiv.org/abs/1904.10509 .

Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data .

10

--- TRANG 11 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, July 2019. URL https://aclanthology.org/P19-1285 .

Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. URL https://arxiv.org/abs/2307.08691 .

Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ÃÅe. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems , 2022. URL https://arxiv.org/abs/2205.14135 .

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. 2023a. URL https://arxiv.org/abs/2307.02486 .

Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233 , 2023b. URL https://arxiv.org/abs/2305.14233 .

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020. URL https://arxiv.org/abs/2101.00027 .

Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L ÃÅelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th ÃÅeophile Gervet, Thibaut Lavril, Thomas Wang, Timoth ÃÅee Lacroix, and William El Sayed. Mixtral of experts. 2024. URL https://arxiv.org/abs/2401.04088 .

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980 .

OpenAI. GPT-4 Technical Report. 2023. URL https://arxiv.org/abs/2303.08774 .

Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. Giraffe: Adventures in expanding context lengths in llms. 2023. URL https://arxiv.org/abs/2308.10882 .

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. 2023. URL https://arxiv.org/abs/2309.00071 .

Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=R8sQPpGCv0 .

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint , 2019. URL https://arxiv.org/abs/1911.05507 .

Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J ÃÅe≈ïemy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D ÃÅefossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https://arxiv.org/abs/2308.12950 .

11

--- TRANG 12 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

Anian Ruoss, Gr ÃÅegoire Del ÃÅetang, Tim Genewein, Jordi Grau-Moya, R ÃÅobert Csord ÃÅas, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) . Association for Computational Linguistics, July 2023. URL https://aclanthology.org/2023.acl-short.161 .

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. 2022. URL https://arxiv.org/abs/2104.09864 .

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, July 2023. URL https://aclanthology.org/2023.acl-long.816 .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ÃÅee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023a. URL https://arxiv.org/abs/2302.13971 .

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. 2023b. URL https://arxiv.org/abs/2307.09288 .

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=gEZrGCozdqR .

12

--- TRANG 13 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

A CHI TI·∫æT TH·ª∞C NGHI·ªÜM

A.1 BASELINE

Ch√∫ng t√¥i so s√°nh CLEX c·ªßa m√¨nh v·ªõi nhi·ªÅu baseline kh√°c nhau trong m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, t·∫•t c·∫£ ƒë·ªÅu ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p con 2B token t·ª´ Pile-Books3 v√† PG-19 ngo·∫°i tr·ª´ LLaMA-2 v√† CodeLLaMA ƒë∆∞·ª£c ƒë√°nh gi√° b·∫±ng c√°c checkpoint ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ngu·ªìn m·ªü. Ch√∫ng t√¥i hu·∫•n luy·ªán c√°c ph∆∞∆°ng ph√°p PE scaling v√† ngo·∫°i suy chi·ªÅu d√†i v·ªõi chi·ªÅu d√†i chu·ªói 16k v√† 4k, t∆∞∆°ng ·ª©ng. C·ª• th·ªÉ, ch√∫ng t√¥i s·ª≠ d·ª•ng h·ªá s·ªë m·ªü r·ªông t= 4 ƒë·ªÉ hu·∫•n luy·ªán c√°c m√¥ h√¨nh s·ª≠ d·ª•ng chi·ªÅu d√†i chu·ªói 16k cho PI theo Chen et al. (2023). Trong khi ƒë·ªëi v·ªõi Yarn, ch√∫ng t√¥i hu·∫•n luy·ªán c√°c m√¥ h√¨nh v·ªõi h·ªá s·ªë m·ªü r·ªông 16 v√† 32 (nh∆∞ng c≈©ng s·ª≠ d·ª•ng chi·ªÅu d√†i chu·ªói 16k), t∆∞∆°ng ·ª©ng, theo c√°c thi·∫øt l·∫≠p trong Peng et al. (2023). ƒê·ªëi v·ªõi ALiBi, ch√∫ng t√¥i tr·ª±c ti·∫øp lo·∫°i b·ªè RoPE trong LLaMA-2 v√† hu·∫•n luy·ªán c√°c m√¥ h√¨nh v·ªõi attention bias. ƒê·ªëi v·ªõi random positions, ch√∫ng t√¥i l·∫•y m·∫´u ch·ªâ s·ªë v·ªã tr√≠ t·ª´ [1, 16k] trong khi hu·∫•n luy·ªán c√°c m√¥ h√¨nh v·ªõi chi·ªÅu d√†i chu·ªói 4k. Ch√∫ng t√¥i c≈©ng s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p PE scaling t·ª´ CodeLLaMA ƒë·ªÉ hu·∫•n luy·ªán LLaMA-2-7B s·ª≠ d·ª•ng d·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa ch√∫ng t√¥i, ƒë∆∞·ª£c k√Ω hi·ªáu l√† CL-Scaling.

A.2 CHI TI·∫æT HU·∫§N LUY·ªÜN

Ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt t·∫≠p con t·ª´ Redpajama-Book (Computer, 2023) l√†m d·ªØ li·ªáu hu·∫•n luy·ªán cho m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c t·∫≠p d·ªØ li·ªáu Pile-Books3 v√† PG-19 v√† t·∫≠p con bao g·ªìm kho·∫£ng 2B token. Ch√∫ng t√¥i ƒë·∫∑t t·ªëc ƒë·ªô h·ªçc 2e-5 cho t·∫•t c·∫£ c√°c m√¥ h√¨nh, ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a b·ªüi b·ªô t·ªëi ∆∞u Adam (Kingma & Ba, 2015). K√≠ch th∆∞·ªõc batch ƒë∆∞·ª£c ƒë·∫∑t l√† 64k token cho c√°c m√¥ h√¨nh 7B v√† 16k token cho c√°c m√¥ h√¨nh 13B. Gi√° tr·ªã t mong mu·ªën t·ªëi ƒëa trong qu√° tr√¨nh hu·∫•n luy·ªán trong CLEX (c·ª• th·ªÉ l√† tTrain trong ¬ß3.3) ƒë∆∞·ª£c ƒë·∫∑t l√† 16 cho LLaMA-2. Ch√∫ng t√¥i s·ª≠ d·ª•ng Flash Attention (Dao et al., 2022; Dao, 2023) ƒë·ªÉ h·ªó tr·ª£ hu·∫•n luy·ªán hi·ªáu qu·∫£. H·ªá s·ªë khu·∫øch ƒë·∫°i c·ªßa l·ªõp ODE Œª ƒë∆∞·ª£c ƒë·∫∑t l√† 1 cho t·∫•t c·∫£ c√°c m√¥ h√¨nh 7B v√† 2 cho c√°c m√¥ h√¨nh 13B. ƒê·ªëi v·ªõi instruction tuning, ch√∫ng t√¥i hu·∫•n luy·ªán m√¥ h√¨nh c·ªßa m√¨nh s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu UltraChat trong 1 epoch, b·∫Øt ƒë·∫ßu t·ª´ checkpoint sau khi hu·∫•n luy·ªán m√¥ h√¨nh h√≥a ng√¥n ng·ªØ. Quy tr√¨nh hu·∫•n luy·ªán c·ªßa CLEX ƒë∆∞·ª£c th·ªÉ hi·ªán trong Thu·∫≠t to√°n 1.

Thu·∫≠t to√°n 1 Quy tr√¨nh hu·∫•n luy·ªán c·ªßa CLEX
1:l·∫∑p l·∫°i
2: Cho m·ªôt batch chu·ªói B v·ªõi chi·ªÅu d√†i LTrain;
3: l·∫•y m·∫´u t‚Ä≤‚àº[1, tTrain];
4: t√≠nh z(t‚Ä≤) b·∫±ng Eq. (12);
5: Œ∏t‚Ä≤= exp(z(t‚Ä≤));
6: l·∫•y m·∫´u (ng·∫´u nhi√™n ho·∫∑c ƒë·ªÅu) t‚Ä≤¬∑LTrain ch·ªâ s·ªë v·ªã tr√≠ t·ª´ [1, tTrain¬∑LTrain];
7: t√≠nh RoPE v·ªõi Œ∏t‚Ä≤ v√† ch·ªâ s·ªë v·ªã tr√≠ ƒë∆∞·ª£c l·∫•y m·∫´u;
8: hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi RoPE tr√™n B v·ªõi m·ª•c ti√™u m√¥ h√¨nh h√≥a ng√¥n ng·ªØ.
9:cho ƒë·∫øn khi h·ªôi t·ª•

A.3 CHI TI·∫æT ƒê√ÅNH GI√Å

ƒê·ªëi v·ªõi m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, ch√∫ng t√¥i ƒë√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh v·ªõi m·ªôt t·∫≠p con t∆∞∆°ng t·ª± nh∆∞ t·∫≠p hu·∫•n luy·ªán nh∆∞ng ch·ª©a 20 tri·ªáu token. ƒê·ªëi v·ªõi c√°c chi·ªÅu d√†i ƒë√°nh gi√° kh√°c nhau, ch√∫ng t√¥i nh√≥m t·∫≠p ki·ªÉm tra th√†nh c√°c chi·ªÅu d√†i chu·ªói t∆∞∆°ng ·ª©ng. Ch√∫ng t√¥i th·∫•y r·∫±ng c·ªë ƒë·ªãnh h·ªá s·ªë m·ªü r·ªông nh∆∞ h·ªá s·ªë hu·∫•n luy·ªán s·∫Ω c·∫£n tr·ªü hi·ªáu su·∫•t t·ªïng qu√°t v√† kh·∫£ nƒÉng ngo·∫°i suy c·ªßa c√°c baseline, do ƒë√≥, ch√∫ng t√¥i m·ªü r·ªông t·ª∑ l·ªá h·ªá s·ªë m·ªü r·ªông khi chi·ªÅu d√†i ƒë√°nh gi√° v∆∞·ª£t qu√° chi·ªÅu d√†i hu·∫•n luy·ªán. V√≠ d·ª•, khi ƒë√°nh gi√° tr√™n chi·ªÅu d√†i 32k, ch√∫ng t√¥i ƒë·∫∑t h·ªá s·ªë m·ªü r·ªông c·ªßa PI l√† 8 thay v√¨ h·ªá s·ªë hu·∫•n luy·ªán l√† 4. Ch√∫ng t√¥i c≈©ng √°p d·ª•ng th·ªß thu·∫≠t logscaling¬π cho t·∫•t c·∫£ c√°c baseline v√† m√¥ h√¨nh c·ªßa ch√∫ng t√¥i, trong ƒë√≥ ch√∫ng t√¥i m·ªü r·ªông ƒëi·ªÉm attention b·∫±ng max{1,logLTrain/LTest}. Ch√∫ng t√¥i th·∫•y n√≥ c√≥ th·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t khi ƒë√°nh gi√° tr√™n chi·ªÅu d√†i v∆∞·ª£t qu√° chi·ªÅu d√†i hu·∫•n luy·ªán nh∆∞ ƒë∆∞·ª£c th·ªÉ hi·ªán trong H√¨nh 8. ƒê·ªëi v·ªõi LongBench, ch√∫ng t√¥i tu√¢n theo c√°c thi·∫øt l·∫≠p t·ª´ Bai et al. (2023), trong ƒë√≥ ph∆∞∆°ng ph√°p gi·∫£i m√£ ƒë∆∞·ª£c ƒë·∫∑t l√† greedy search. Ch√∫ng t√¥i ƒë·∫∑t tk trong ¬ß3.3 l√† {1,2,3,4}, ƒë·ªÉ cache t·∫ßn s·ªë c∆° s·ªü t∆∞∆°ng ·ª©ng v·ªõi {4k,8k,12k,16k}.

¬πhttps://spaces.ac.cn/archives/8823

13

--- TRANG 14 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

0 20 40 60
Chi·ªÅu0.00.20.40.60.8T·∫ßn s·ªë SinusoidalBase=1e4 (g·ªëc)
CLEX (t=4)
CLEX (t=8)
CLEX (t=16)
Base=1e6

H√¨nh 6: T·∫ßn s·ªë sinusoidal qua chi·ªÅu c·ªßa t·∫ßn s·ªë c∆° s·ªü. Chi·ªÅu th·∫•p h∆°n bi·ªÉu th·ªã t·∫ßn s·ªë cao h∆°n.

0.0 10k 20k 30k 40k 50k 60k
Chi·ªÅu d√†i chu·ªói ƒë√°nh gi√°2324PPLPI-16k (t=4)
PI-16k (t=16)

H√¨nh 7: K·∫øt qu·∫£ c·ªßa PI-16k (ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t= 4) tr√™n chi·ªÅu d√†i chu·ªói 64k, trong khi ƒë√°nh gi√° v·ªõi t= 4 v√† t= 16.

B K·∫æT QU·∫¢ B·ªî SUNG

B.1 TR·ª∞C QUAN H√ìA T·∫¶N S·ªê C∆† S·ªû ƒê√É H·ªåC

Trong H√¨nh 6, ch√∫ng t√¥i hi·ªÉn th·ªã t·∫ßn s·ªë c∆° s·ªü theo c√°c chi·ªÅu d√†i chu·ªói kh√°c nhau sau khi hu·∫•n luy·ªán CLEX tr√™n LLaMA-2 v·ªõi chi·ªÅu d√†i chu·ªói 4k. Bi·ªÉu ƒë·ªì cho th·∫•y CLEX c√≥ xu h∆∞·ªõng tƒÉng t·∫ßn s·ªë cao t·∫°i m·ªôt s·ªë chi·ªÅu nh·∫•t ƒë·ªãnh trong khi ƒë·ªìng th·ªùi gi·∫£m m·ªôt s·ªë kh√°c. ƒê√°ng ng·∫°c nhi√™n h∆°n, ch√∫ng t√¥i ƒë√£ quan s√°t th·∫•y r·∫±ng c√°c t·∫ßn s·ªë c∆° s·ªü li√™n k·∫øt v·ªõi c√°c gi√° tr·ªã t kh√°c nhau trong CLEX th·ªÉ hi·ªán h√†nh vi ƒë·∫≥ng h∆∞·ªõng, r·∫±ng c√°c chi·ªÅu n∆°i vi·ªác gi·∫£m v√† tƒÉng x·∫£y ra t∆∞∆°ng t·ª± qua c√°c gi√° tr·ªã t kh√°c nhau, v·ªõi c√°c gi√° tr·ªã t l·ªõn h∆°n d·∫´n ƒë·∫øn m·ªü r·ªông th√™m. Ph√°t hi·ªán n√†y c√≥ th·ªÉ ƒë√≥ng g√≥p v√†o vi·ªác thi·∫øt k·∫ø c√°c ph∆∞∆°ng ph√°p PE scaling heuristic m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán.

B.2 THU·ªòC T√çNH T·ª∞ M·ªû R·ªòNG C·ª¶A C√ÅC PH∆Ø∆†NG PH√ÅP PE SCALING

C√°c ph∆∞∆°ng ph√°p PE scaling tr∆∞·ªõc ƒë√¢y th∆∞·ªùng ƒë√°nh gi√° c√°c m√¥ h√¨nh v·ªõi h·ªá s·ªë m·ªü r·ªông ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh trong qu√° tr√¨nh hu·∫•n luy·ªán. Ch√∫ng t√¥i th·∫•y ƒëi·ªÅu n√†y s·∫Ω c·∫£n tr·ªü ƒë√°ng k·ªÉ vi·ªác ƒë√°nh gi√° hi·ªáu su·∫•t v∆∞·ª£t qu√° chi·ªÅu d√†i hu·∫•n luy·ªán. Do ƒë√≥, ch√∫ng t√¥i ƒëi·ªÅu ch·ªânh h·ªá s·ªë m·ªü r·ªông c·ªßa ph∆∞∆°ng ph√°p PE scaling tr∆∞·ªõc ƒë√¢y t∆∞∆°ng ·ª©ng v·ªõi chi·ªÅu d√†i ƒë√°nh gi√°. Trong H√¨nh 7, PI-16k (ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t= 4) ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ kh√¥ng t·∫ßm th∆∞·ªùng khi ƒë√°nh gi√°

14

--- TRANG 15 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

M√¥ h√¨nh Chi·ªÅu d√†i Chi·ªÅu d√†i ƒë√°nh gi√°
Hu·∫•n luy·ªán 32K 64K 128K 256K

CLEX-Phi-2 32K 5.11 5.17 6.55 >10
CLEX-Mixtral-8x7B 32K 2.56 2.53 2.57 3.78

B·∫£ng 2: Perplexity (PPL) c·ªßa CLEX-Phi-2 v√† CLEX-Mixtral-8x7B trong m√¥ h√¨nh h√≥a ng√¥n ng·ªØ v·ªõi chi·ªÅu d√†i ƒë√°nh gi√° t·ª´ 32k ƒë·∫øn 256k.

H√¨nh 8: Ablation c·ªßa logscaling (tr√°i) v√† dynamic kh√¥ng ƒë·ªïi Œæt (ph·∫£i) cho CLEX ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n chi·ªÅu d√†i chu·ªói 4k.

v·ªõi t= 16 tr√™n chi·ªÅu d√†i 64k, v∆∞·ª£t tr·ªôi ƒë√°ng k·ªÉ so v·ªõi ƒë√°nh gi√° th√¥ng th∆∞·ªùng s·ª≠ d·ª•ng t= 4. L∆∞u √Ω r·∫±ng Yarn ƒë∆∞·ª£c hu·∫•n luy·ªán th√¥ng th∆∞·ªùng v·ªõi t l·ªõn h∆°n, v√¨ v·∫≠y kh√¥ng c·∫ßn thi·∫øt ph·∫£i ƒëi·ªÅu ch·ªânh h·ªá s·ªë m·ªü r·ªông cho Yarn trong qu√° tr√¨nh ƒë√°nh gi√°. Khi chi·ªÅu d√†i ƒë√°nh gi√° tr·ªü n√™n d√†i h∆°n, tuy nhi√™n, ch√∫ng t√¥i tin r·∫±ng thu·ªôc t√≠nh t·ª± m·ªü r·ªông c≈©ng s·∫Ω c√≥ l·ª£i cho Yarn. V·ªÅ nh·ªØng ƒëi·ªÅu n√†y, kh·∫£ nƒÉng ngo·∫°i suy t·ª´ t·ª± m·ªü r·ªông trong c√°c ph∆∞∆°ng ph√°p PE scaling tr∆∞·ªõc ƒë√¢y v·∫´n y·∫øu h∆°n nhi·ªÅu so v·ªõi CLEX nh∆∞ ƒë∆∞·ª£c minh h·ªça trong H√¨nh 1.

B.3 K·∫æT QU·∫¢ C·ª¶A NHI·ªÄU M√î H√åNH H∆†N

Ch√∫ng t√¥i c≈©ng m·ªü r·ªông CLEX sang nhi·ªÅu m√¥ h√¨nh h∆°n ngo√†i LLaMA-2, t·ª©c l√† Phi-2¬≤ v√† Mixtral-8x7B (Jiang et al., 2024). K·∫øt qu·∫£ trong B·∫£ng 2 ti·∫øp t·ª•c h·ªó tr·ª£ hi·ªáu qu·∫£ t·ªïng qu√°t c·ªßa CLEX, trong ƒë√≥ chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë∆∞·ª£c ngo·∫°i suy ƒë·∫øn 4x ‚àº8x chi·ªÅu d√†i hu·∫•n luy·ªán qua c√°c LLM kh√°c nhau.

B.4 ABLATION V·ªÄ SCALAR EMBEDDING

Dynamic c√≥ th·ªÉ h·ªçc trong Eq. (13) d·ª±a tr√™n dynamic kh√¥ng ƒë·ªïi Œæt t·ª´ Yarn. ƒê·ªÉ ti·∫øn h√†nh ablation v·ªÅ t√°c ƒë·ªông c·ªßa Œæt, ch√∫ng t√¥i lo·∫°i b·ªè Œæt trong Eq. (13) v√† hu·∫•n luy·ªán LLaMA-2-7B tr√™n chi·ªÅu d√†i chu·ªói 4k tr·∫£i qua c√πng quy tr√¨nh hu·∫•n luy·ªán. T·ª´ H√¨nh 8, ch√∫ng t√¥i th·∫•y vi·ªác hu·∫•n luy·ªán CLEX m√† kh√¥ng c√≥ Œæt s·∫Ω kh√¥ng c·∫£n tr·ªü kh·∫£ nƒÉng ngo·∫°i suy (ho·∫∑c th·∫≠m ch√≠ t·ªët h∆°n). Tuy nhi√™n, nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 10, hu·∫•n luy·ªán v·ªõi Œæt c√≥ th·ªÉ tƒÉng t·ªëc h·ªôi t·ª• v√† t·∫°o ra ƒë∆∞·ªùng cong loss ·ªïn ƒë·ªãnh h∆°n. ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ hu·∫•n luy·ªán c√°c LLM l·ªõn h∆°n tr√™n chi·ªÅu d√†i chu·ªói d√†i h∆°n, v√¨ v·∫≠y ch√∫ng t√¥i t√≠ch h·ª£p Œæt v√†o dynamic c√≥ th·ªÉ h·ªçc.

B.5 K·∫æT QU·∫¢ C·ª¶A LONG BENCH

Ch√∫ng t√¥i li·ªát k√™ k·∫øt qu·∫£ s·ªë c·ªßa CLEX ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n 4k so v·ªõi nhi·ªÅu baseline kh√°c nhau ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·ªëi ƒëa 32k, bao g·ªìm tr·∫£ l·ªùi c√¢u h·ªèi ƒë∆°n t√†i li·ªáu (B·∫£ng 3), tr·∫£ l·ªùi c√¢u h·ªèi ƒëa t√†i li·ªáu (B·∫£ng 4), t√≥m t·∫Øt (B·∫£ng 5), h·ªçc √≠t shot (B·∫£ng 6), t·ªïng h·ª£p (B·∫£ng 7), v√†

¬≤https://huggingface.co/microsoft/phi-2

15

--- TRANG 16 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

0.0 0.2 0.4 0.6 0.8 1.0
Epoch2.02.53.03.54.0LossNaive-FT-16k
PI-16k
Yarn-16k (t=16)
CL-Scaling-16k
ALiBi-4k
RandomPos-4k
CLEX-4k
CLEX-8k
CLEX-16k

H√¨nh 9: C√°c ƒë∆∞·ªùng cong loss hu·∫•n luy·ªán c·ªßa c√°c baseline kh√°c nhau v√† c√°c m√¥ h√¨nh CLEX c·ªßa ch√∫ng t√¥i trong B·∫£ng 1. Ch√∫ng t√¥i hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh tr√™n LLaMA-2-7B tr√™n 2B token trong m·ªôt epoch.

0.0 0.2 0.4 0.6 0.8 1.0
Epoch2.02.53.03.54.04.5LossCLEX-4k (w/o Œæt)
CLEX-4k (w/ Œæt)

H√¨nh 10: C√°c ƒë∆∞·ªùng cong loss hu·∫•n luy·ªán c·ªßa CLEX-4k ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n LLaMA-2-7B c√≥ ho·∫∑c kh√¥ng c√≥ dynamic kh√¥ng ƒë·ªïi (Œæt) trong Eq. (14).

ho√†n th√†nh m√£ (B·∫£ng 8) c√°c nhi·ªám v·ª•. L∆∞u √Ω r·∫±ng chi·ªÅu d√†i chu·ªói trung b√¨nh c·ªßa h·∫ßu h·∫øt c√°c nhi·ªám v·ª• trong LongBench dao ƒë·ªông t·ª´ 5k ƒë·∫øn 15k, v√¨ v·∫≠y c√°c m·∫´u s·∫Ω ƒë∆∞·ª£c c·∫Øt b·ªõt th√†nh chi·ªÅu d√†i chu·ªói trong chi·ªÅu d√†i ng·ªØ c·∫£nh ƒë∆∞·ª£c h·ªó tr·ª£ (n·∫øu c·∫ßn thi·∫øt) cho c√°c baseline, ngo·∫°i tr·ª´ Baichuan-13B-4k, ALiBi-7B-4k v√† CLEX-7B-4k cho ph√©p ngo·∫°i suy chi·ªÅu d√†i ƒë∆∞·ª£c ƒë√°nh gi√° v·ªõi c·ª≠a s·ªï ng·ªØ c·∫£nh 16k. Baichuan-13B-4k l√† m·ªôt m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc v·ªõi ALiBi tr√™n chi·ªÅu d√†i chu·ªói 4k, trong khi ALiBi-7B-4k l√† m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi ALiBi tr·∫£i qua c√πng quy tr√¨nh hu·∫•n luy·ªán nh∆∞ CLEX c·ªßa ch√∫ng t√¥i. Ch√∫ng t√¥i th·∫•y r·∫±ng c·∫£ hai m√¥ h√¨nh d·ª±a tr√™n ALiBi ƒë·ªÅu ho·∫°t ƒë·ªông k√©m ƒë√°ng k·ªÉ trong benchmark LongBench khi ƒë∆∞·ª£c ƒë√°nh gi√° v·ªõi chi·ªÅu d√†i chu·ªói (16k) v∆∞·ª£t qu√° chi·ªÅu d√†i hu·∫•n luy·ªán c·ªßa ch√∫ng, cho th·∫•y nh·ªØng th√°ch th·ª©c c·ªßa kh·∫£ nƒÉng ngo·∫°i suy c·ªßa ch√∫ng trong c√°c nhi·ªám v·ª• th·ª±c t·∫ø. Tuy nhi√™n, CLEX c·ªßa ch√∫ng t√¥i ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ ho·∫°t ƒë·ªông t·ªët qua h·∫ßu h·∫øt c√°c nhi·ªám v·ª• ngay c·∫£ khi ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi 4k nh∆∞ng ƒë∆∞·ª£c ki·ªÉm tra v·ªõi chi·ªÅu d√†i 16k, ƒëi·ªÅu n√†y ti·∫øp t·ª•c ti·∫øt l·ªô kh·∫£ nƒÉng ngo·∫°i suy chi·ªÅu d√†i v∆∞·ª£t tr·ªôi c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i.

16

--- TRANG 17 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

M√¥ h√¨nh NarrativeQA Qasper MultiFieldQA-en MultiFieldQA-zh TRUNG B√åNH

GPT-3.5-Turbo-16k 23.6 43.3 52.3 61.2 45.1
Llama2-7B-chat-4k 18.7 19.2 36.8 11.9 21.65
LongChat-v1.5-7B-32k 16.9 27.7 41.4 29.1 28.78
CodeLLaMA-7B-16k 22.93 30.69 43.37 31.76 32.19
XGen-7B-8k 18 18.1 37.7 14.8 22.15
InternLM-7B-8k 12.1 16.7 23.4 33.6 21.45
Vicuna-v1.5-7B-16k 19.4 26.1 38.5 43 31.75
Baichuan-13B-4k 0.07 17.55 17.28 38.55 18.36
ALiBi-7B-4k 0.04 8.13 17.87 2.89 7.23
CLEX-7B-4k 18.05 23.68 44.62 31.15 29.38

B·∫£ng 3: C√°c nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi ƒë∆°n t√†i li·ªáu trong LongBench.

M√¥ h√¨nh HotpotQA 2WikiMQA Musique DuReader TRUNG B√åNH

GPT-3.5-Turbo-16k 51.6 37.7 26.9 28.7 36.23
Llama2-7B-chat-4k 25.4 32.8 9.4 5.2 18.2
LongChat-v1.5-7B-32k 31.5 20.6 9.7 19.5 20.33
CodeLLaMA-7B-16k 33.05 27.93 14.2 10.78 21.49
XGen-7B-8k 29.7 21.1 10.3 11 18.02
InternLM-7B-8k 28.7 22.8 9 11.1 17.9
Vicuna-v1.5-7B-16k 25.3 20.8 9.8 19.3 18.8
Baichuan-13B-4k 3.29 15 0.1 8.77 6.79
ALiBi-7B-4k 2.73 8 1.33 11.87 5.98
CLEX-7B-4k 28.44 19.53 9.15 23.21 20.08

B·∫£ng 4: C√°c nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi ƒëa t√†i li·ªáu trong LongBench.

M√¥ h√¨nh GovReport QMSum MultiNews VCSUM TRUNG B√åNH

GPT-3.5-Turbo-16k 29.5 23.4 26.7 16 23.9
Llama2-7B-chat-4k 27.3 20.8 25.8 0.2 18.525
LongChat-v1.5-7B-32k 30.8 22.7 26.4 9.9 22.45
CodeLLaMA-7B-16k 28.43 24.18 26.84 0.79 20.06
XGen-7B-8k 27.3 20.5 26.2 2.2 19.05
InternLM-7B-8k 9.7 15.9 22.8 12.4 15.2
Vicuna-v1.5-7B-16k 27.9 22.8 27.2 15.1 23.25
Baichuan-13B-4k 6.8 1.71 23.1 8.09 9.925
ALiBi-7B-4k 5.31 1.64 19.38 3.25 7.395
CLEX-7B-4k 32.52 22.9 25.55 12.03 23.25

B·∫£ng 5: C√°c nhi·ªám v·ª• t√≥m t·∫Øt trong LongBench.

17

--- TRANG 18 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2024

M√¥ h√¨nh TREC TriviaQA SAMSum LSHT TRUNG B√åNH

GPT-3.5-Turbo-16k 68 91.4 41.7 29.2 57.575
Llama2-7B-chat-4k 61.5 77.8 40.7 19.8 49.95
LongChat-v1.5-7B-32k 63.5 82.3 34.2 23.2 50.8
XGen-7B-8k 65.5 77.8 25.3 20.5 47.275
CodeLLaMA-7B-16k 70 84.97 43.43 32.5 57.725
InternLM-7B-8k 52 77.8 21.2 15.2 41.55
Vicuna-v1.5-7B-16k 71.5 86.2 40.8 28.8 56.825
Baichuan-13B-4k 20.05 20.06 5.77 1 11.72
ALiBi-7B-4k 9.25 8.83 4.67 0 5.6875
CLEX-7B-4k 68 84.92 42.82 28.35 56.0225

B·∫£ng 6: C√°c nhi·ªám v·ª• h·ªçc √≠t shot trong LongBench.

Passage Count PassageRetrieval-en PassageRetrieval-zh TRUNG B√åNH

GPT-3.5-Turbo-16k 4.5 71 77.5 51
Llama2-7B-chat-4k 2.1 9.8 0.5 4.13
LongChat-v1.5-7B-32k 1 30.5 7.6 13.03
CodeLLaMA-7B-16k 2 13.5 11.25 8.92
XGen-7B-8k 2.1 8.5 3.5 4.7
InternLM-7B-8k 3 6 0.9 3.3
Vicuna-v1.5-7B-16k 6.5 4.5 5 5.33
Baichuan-13B-4k 0.06 0.5 5 1.85
ALiBi-7B-4k 0 1.27 0.75 0.67
CLEX-7B-4k 0 11.5 17.5 9.67

B·∫£ng 7: C√°c nhi·ªám v·ª• t·ªïng h·ª£p trong LongBench.

LCC RepoBench-P TRUNG B√åNH

GPT-3.5-Turbo-16k 54.7 53.6 54.15
Llama2-7B-chat-4k 52.4 43.8 48.1
LongChat-v1.5-7B-32k 53 55.3 54.15
CodeLLaMA-7B-16k 64.35 55.87 60.11
XGen-7B-8k 38.6 38.6 38.6
InternLM-7B-8k 44.1 28.8 36.45
Vicuna-v1.5-7B-16k 51 43.5 47.25
Baichuan-13B-4k 47.98 16.58 32.28
ALiBi-7B-4k 46.69 18.54 32.61
CLEX-7B-4k 59.01 56.87 57.94

B·∫£ng 8: C√°c nhi·ªám v·ª• ho√†n th√†nh m√£ trong LongBench.

18
