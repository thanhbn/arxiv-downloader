# 2208.00748.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2208.00748.pdf
# File size: 835321 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Hiểu Văn Bản Dài Hiệu Quả với Các Mô Hình Văn Bản Ngắn
Maor Ivgi Uri Shaham Jonathan Berant
Trường Khoa học Máy tính Blavatnik, Đại học Tel-Aviv
{maor.ivgi,uri.shaham,joberant}@cs.tau.ac.il
Tóm Tắt
Các mô hình ngôn ngữ tiền huấn luyện (LMs) dựa trên Transformer phổ biến khắp việc hiểu ngôn ngữ tự nhiên, nhưng không thể được áp dụng cho các chuỗi dài như truyện, bài báo khoa học và tài liệu dài, do độ phức tạp bậc hai của chúng. Trong khi vô số biến thể transformer hiệu quả đã được đề xuất, chúng thường dựa trên các triển khai tùy chỉnh yêu cầu tiền huấn luyện đắt đỏ từ đầu. Trong công trình này, chúng tôi đề xuất SLED: SLiding-Encoder và Decoder, một phương pháp đơn giản để xử lý các chuỗi dài tái sử dụng và tận dụng các LMs tiền huấn luyện văn bản ngắn đã được kiểm nghiệm. Cụ thể, chúng tôi phân chia đầu vào thành các khối chồng lấp, mã hóa từng khối với một bộ mã hóa LM văn bản ngắn và sử dụng bộ giải mã tiền huấn luyện để hợp nhất thông tin giữa các khối (fusion-in-decoder). Chúng tôi minh họa thông qua các thí nghiệm có kiểm soát rằng SLED cung cấp một chiến lược khả thi cho việc hiểu văn bản dài và đánh giá phương pháp của chúng tôi trên SCROLLS, một benchmark với bảy tập dữ liệu trên nhiều nhiệm vụ hiểu ngôn ngữ khác nhau. Chúng tôi nhận thấy rằng SLED có tính cạnh tranh với các mô hình chuyên biệt lớn hơn đến 50 lần và yêu cầu một bước tiền huấn luyện chuyên dụng và đắt đỏ.

1 Giới Thiệu
Các mô hình ngôn ngữ tiền huấn luyện dựa trên Transformer (Vaswani và cộng sự, 2017; Devlin và cộng sự, 2019; Lewis và cộng sự, 2020; Raffel và cộng sự, 2020b; Brown và cộng sự, 2020) đã thành công rộng rãi trên tất cả các lĩnh vực hiểu ngôn ngữ tự nhiên (NLU). Tuy nhiên, việc áp dụng chúng trên văn bản dài (như truyện, kịch bản, hoặc bài báo khoa học) là không khả thi do độ phức tạp bậc hai trong chiều dài đầu vào. Để thu hẹp khoảng cách này, các công trình gần đây đã phát triển các biến thể transformer hiệu quả hơn (Kitaev và cộng sự, 2020a; Beltagy và cộng sự, 2020; Zaheer và cộng sự, 2020a; Guo và cộng sự, 2022) và áp dụng chúng trên

100M 1B 10B 200M 400M 2B 4B 20B
# of parameters202530354045SCROLLS score
SLED
SLEDSLEDUL2LongT5baseLongT5largeLongT5XL
BARTbaseBARTlarge
T5baseTận dụng các mô hình tiền huấn luyện hiện có
Tiền huấn luyện tầm xa chuyên dụng
Tiền huấn luyện tầm ngắn tiêu chuẩnHình 1: Điểm SCROLLS của các mô hình (Shaham và cộng sự, 2022) như một hàm của số tham số. Việc kết nối các LMs tiền huấn luyện hiện có vào khung SLED cải thiện đáng kể điểm SCROLLS của chúng (mũi tên từ vòng tròn xanh đến ngôi sao cam). Tam giác xám biểu thị các mô hình với tiền huấn luyện chuyên dụng để nắm bắt các phụ thuộc tầm xa. BART large-SLED có tính cạnh tranh với LongT5 base (Guo và cộng sự, 2022) và UL2 (Tay và cộng sự, 2022b) (có nhiều hơn 50 lần tham số), và chỉ thua nhẹ so với các mô hình LongT5 lớn hơn.

các nhiệm vụ hiểu ngôn ngữ tầm xa (Mehta và cộng sự, 2022; Shaham và cộng sự, 2022).

Tuy nhiên, hầu hết các transformer hiệu quả sử dụng kiến trúc chuyên biệt với các triển khai tùy chỉnh không được đảm bảo mở rộng tốt như các transformer thông thường (Tay và cộng sự, 2022a). Hơn nữa, chúng yêu cầu một bước tiền huấn luyện đắt đỏ và không khai thác các LMs tiền huấn luyện sẵn có được huấn luyện cho văn bản ngắn. Cho đến nay, hiệu suất của chúng trên văn bản dài chưa đạt được thành công như các đối tác tầm ngắn.

Trong công trình này, chúng tôi trình bày SLED: SLiding-Encoder và Decoder, một phương pháp đơn giản nhưng mạnh mẽ để áp dụng các mô hình encoder-decoder tiền huấn luyện sẵn có trên các bài toán văn bản dài, với sự phụ thuộc thời gian và không gian tuyến tính. Cụ thể (xem Hình 2), chúng tôi phân chia các tài liệu dài thành các khối token chồng lấp có độ dài không đổi và mã hóa từng khối độc lập với một

arXiv:2208.00748v3  [cs.CL]  27 Dec 2022

--- TRANG 2 ---
bộ mã hóa đã được tiền huấn luyện. Sau đó, một bộ giải mã tiền huấn luyện chú ý đến tất cả các biểu diễn đầu vào được ngữ cảnh hóa để tạo ra đầu ra. Giả định chính của chúng tôi là các token đầu vào có thể được ngữ cảnh hóa thông qua môi trường xung quanh cục bộ của chúng (sử dụng LM văn bản ngắn), và bất kỳ lý luận toàn cục liên khối nào cũng có thể được xử lý bởi bộ giải mã, tương tự như fusion-in-decoder (FiD) (Izacard và Grave, 2021). Phương pháp của chúng tôi có thể được áp dụng dễ dàng cho bất kỳ LM encoder-decoder tiền huấn luyện nào như T5 (Raffel và cộng sự, 2020b) và BART (Lewis và cộng sự, 2020) (nhưng không áp dụng được cho các mô hình chỉ decoder (Brown và cộng sự, 2020) hoặc chỉ encoder (Liu và cộng sự, 2019; Conneau và cộng sự, 2020)).

Chúng tôi đánh giá SLED trên một loạt rộng các nhiệm vụ hiểu ngôn ngữ. Để chứng minh tính phù hợp của SLED cho xử lý văn bản, chúng tôi thực hiện các thí nghiệm có kiểm soát trên các phiên bản sửa đổi của SQuAD 1.1 (Rajpurkar và cộng sự, 2016) và HotpotQA (Yang và cộng sự, 2018) để chỉ ra rằng SLED có thể (a) tìm thông tin liên quan được nhúng trong một chuỗi văn bản dài và (b) hợp nhất thông tin từ các khối được mã hóa riêng biệt.

Đánh giá chính của chúng tôi trên SCROLLS, một benchmark mới được phát hành bao gồm 7 nhiệm vụ tầm xa trên Hỏi Đáp (QA), Tóm Tắt, và Suy Luận Ngôn Ngữ Tự Nhiên (NLI). Chúng tôi chỉ ra (Hình 1) rằng việc lấy một mô hình encoder-decoder tiền huấn luyện, như BART (Lewis và cộng sự, 2020) hoặc T5 (Raffel và cộng sự, 2020b), và nhúng nó vào khung SLED dẫn đến cải thiện hiệu suất đáng kể (trung bình 6 điểm trên các mô hình). Hơn nữa, hiệu suất của BART large-SLED có thể so sánh với LongT5 base (Guo và cộng sự, 2022), một mô hình được tiền huấn luyện đặc biệt để xử lý các phụ thuộc tầm xa, và vượt qua UL2 (Tay và cộng sự, 2022b), chứa nhiều hơn 50 lần tham số. Quan trọng hơn, các mô hình dựa trên SLED có thể sử dụng bất kỳ LM tiền huấn luyện tương lai nào mà không cần tiền huấn luyện bổ sung để cải thiện thêm hiệu suất.

Do tính đơn giản của nó, SLED cũng có thể được sử dụng như một công cụ chẩn đoán để phân tích các benchmark tầm xa. Chúng tôi phân tích bảy tập dữ liệu trong SCROLLS thông qua lăng kính của SLED và chỉ ra tập dữ liệu nào yêu cầu đầu vào được ngữ cảnh hóa với các token xa. Cụ thể, chúng tôi nhận thấy rằng trong các nhiệm vụ QA và NLI, việc ngữ cảnh hóa tương đối cục bộ là đủ cho hiệu suất cao.

Trong khi SLED tương tự như FiD từ góc độ kỹ thuật, việc sử dụng FiD trong quá khứ đã tập trung

xung quanh hỏi đáp miền mở (Izacard và Grave, 2021), nơi các đoạn văn không liên quan được mã hóa độc lập một cách tự nhiên. Ở đây, chúng tôi kiểm tra fusion-in-decoder trên các tài liệu dài, nơi việc mã hóa cục bộ các khối là một giả định mô hình hóa cần được kiểm tra. Trong công trình gần đây, Vig và cộng sự (2022) đã đề xuất một kiến trúc tương tự để giải quyết đầu vào dài từ QMSum (Zhong và cộng sự, 2021), nhưng không phân tích nó một cách có hệ thống. Chúng tôi chuẩn hóa phương pháp này lần đầu tiên và phân tích toàn diện hiệu quả của FiD để mã hóa các tài liệu dài trên nhiều nhiệm vụ.

Tóm lại, các đóng góp chính của chúng tôi là:
1. Chúng tôi trình bày SLED, một phương pháp đơn giản và hiệu quả để xử lý văn bản dài tận dụng các LMs encoder-decoder sẵn có dựa trên fusion-in-decoder.
2. Chúng tôi chứng minh hiệu quả của SLED trong cả thí nghiệm có kiểm soát, cũng như trên benchmark SCROLLS, dẫn đến kết quả cạnh tranh so với các mô hình chuyên biệt bao gồm đến 50 lần tham số.
3. Chúng tôi sử dụng SLED như một công cụ chẩn đoán để phân tích các tính chất tầm xa của các tập dữ liệu trong benchmark SCROLLS.
4. Chúng tôi cung cấp một triển khai mã nguồn mở của SLED, được tích hợp liền mạch vào thư viện Transformers (Wolf và cộng sự, 2020).

2 Nền Tảng
Những tiến bộ gần đây trong xử lý ngôn ngữ tự nhiên phần lớn được thúc đẩy bởi kiến trúc transformer (Vaswani và cộng sự, 2017). Một thành phần cốt lõi của transformer là lớp self-attention nơi mỗi token đầu vào "chú ý" đến mọi token khác để tạo ra biểu diễn được ngữ cảnh hóa của nó. Điều này dẫn đến sự phụ thuộc thời gian và không gian bậc hai w.r.t. chiều dài của đầu vào, hạn chế khả năng của transformer trong việc xử lý các chuỗi dài.

Hạn chế văn bản dài này đã khơi dậy sự quan tâm lớn trong việc phát triển các biến thể transformer hiệu quả. Một họ phương pháp nổi bật dựa trên attention thưa, nơi mỗi token chú ý đến một số lượng không đổi các token khác, vượt qua sự phụ thuộc bậc hai. Các token thường chú ý đến môi trường xung quanh cục bộ của chúng (Zaheer và cộng sự, 2020a; Beltagy và cộng sự, 2020; Ainslie và cộng sự, 2020; Gupta và Berant, 2020) hoặc đến các token tương tự về mặt ngữ nghĩa (Kitaev và cộng sự, 2020b; Roy và cộng sự,

1https://github.com/Mivg/SLED

--- TRANG 3 ---
Hình 2: Tổng quan về SLED. (a) Các token đầu vào (t1;:::;tn) được chia thành C khối chồng lấp có độ dài c (ở đây, c= 4). Mỗi khối được tạo từ P:=c/2 token đệm ngữ cảnh ở các cạnh phải và trái của khối, và (1-α)c token khối hiệu quả ở giữa (ở đây, α = 0.5; P= 1). (b) Chúng tôi thêm các token tiền tố (p1;:::;pm) vào đầu mỗi khối (m≪n). (c) Mỗi khối được mã hóa độc lập sử dụng bộ mã hóa backbone đã được tiền huấn luyện Menc. (d) Chúng tôi thu thập các token khối hiệu quả được mã hóa (màu vàng) và loại bỏ các token đệm ngữ cảnh (màu hồng) (e) Chúng tôi chuyển đầu vào được mã hóa đến bộ giải mã để tạo ra chuỗi đầu ra cuối cùng (o1;:::;ok).

2021). Hơn nữa, một số lượng không đổi các token toàn cục chú ý đến và được chú ý bởi tất cả các token đầu vào thường được thêm vào mỗi lớp con attention. Các phân tích gần đây (Xiong và cộng sự, 2022a) đã chỉ ra rằng các transformer thưa với attention cục bộ có tính cạnh tranh với các biến thể khác trên nhiều nhiệm vụ hiểu ngôn ngữ.

Phương pháp của chúng tôi, SLED, thuộc về họ các biến thể attention cục bộ. Tuy nhiên, không giống như các công trình trước đây, SLED tái sử dụng và mở rộng các mô hình encoder-decoder tầm ngắn hiện có, và không yêu cầu tiền huấn luyện chuyên biệt hoặc triển khai CUDA chuyên dụng.

Trong hầu hết các biến thể attention cục bộ, ví dụ, LED (Beltagy và cộng sự, 2020), attention là cục bộ theo lớp, nhưng trường tiếp nhận của các token tăng trung qua các lớp. Trong SLED, mà chúng tôi mô tả tiếp theo, các token có quyền truy cập vào cùng một số lượng token, độc lập với độ sâu của lớp, điều này cho phép song song hóa tốt hơn. Để có một khảo sát về các họ transformer hiệu quả, xem Tay và cộng sự (2020). Để so sánh sâu giữa SLED và LED, chúng tôi tham khảo Phụ lục B.

3 Phương Pháp
Trong công trình này, chúng tôi đề xuất một phương pháp đơn giản để tránh độ phức tạp bậc hai của transformer, được thúc đẩy bởi giả định Tính cục bộ của thông tin:
Trong một kiến trúc encoder-decoder, bộ mã hóa có thể ngữ cảnh hóa hiệu quả các token đầu vào chỉ với ngữ cảnh cục bộ, để lại các phụ thuộc tầm xa được xử lý bởi bộ giải mã.

SLED dựa vào giả định mô hình hóa nói trên để

mã hóa các khối ngắn hơn độc lập và thực hiện hợp nhất thông tin trong bộ giải mã (Izacard và Grave, 2021). Bây giờ chúng tôi mô tả chi tiết mô hình SLED.

Đầu vào SLED sử dụng một mô hình encoder-decoder tiền huấn luyện M như một backbone. SLED nhận một tài liệu được token hóa có độ dài n (các ô vuông màu xanh trong Hình 2), và một tiền tố được token hóa ngắn tùy chọn có độ dài m≪n, thường đại diện cho một câu hỏi về tài liệu, một hướng dẫn để thực hiện một số nhiệm vụ tạo sinh, hoặc một giả thuyết (các ô vuông màu cam trong Hình 2). Không giống như các tiền tố tĩnh đặc trưng cho nhiệm vụ (ví dụ, "tóm tắt"), SLED cũng hỗ trợ các tiền tố đặc trưng cho mẫu là một phần của đầu vào (ví dụ, câu hỏi trong các tập dữ liệu QA).

Các bước SLED tuân theo các bước sau:
(a) Các token tài liệu được chia thành C khối có độ dài c (Trong Hình 2, c= 4). (1-α)c token ở giữa trong mỗi khối được ngữ cảnh hóa từ cả bên trái và phải bởi P:=αc/2 token, trong đó α∈[0,0.5] (α= 0.5 trong Hình 2). Chúng tôi gọi những token giữa này là khối hiệu quả, vì chúng sẽ tạo thành đầu ra của bộ mã hóa, và gọi các token ở mỗi bên là đệm ngữ cảnh.

(b) Mỗi khối được thêm tiền tố bởi các token tiền tố (tùy chọn) (Hình 2(b)).

(c) Mỗi khối được mã hóa độc lập, sử dụng bộ mã hóa backbone Menc (xem Hình 2(c)).

(d) Để tạo ra một biểu diễn được ngữ cảnh hóa cho mỗi token, chúng tôi chỉ giữ lại từ mỗi khối

--- TRANG 4 ---
các token từ khối hiệu quả, và nối chúng lại (Hình 2(d)).

(e) Để cung cấp cho bộ giải mã quyền truy cập vào các token tiền tố, chúng tôi mã hóa các token tiền tố với Menc, và thêm kết quả vào đầu biểu diễn được ngữ cảnh hóa (khối ngoài cùng bên trái trong Hình 2(a)-(d)).

(f) Cuối cùng, chúng tôi tạo ra đầu ra với bộ giải mã backbone, Mdec, sử dụng cross-attention tiêu chuẩn trên m+n token được mã hóa (Hình 2(e)).

SLED yêu cầu xử lý một số trường hợp đặc biệt, cụ thể là xử lý khối đầu tiên và cuối cùng không có ngữ cảnh hai chiều. Chúng tôi tham khảo Phụ lục A cho các chi tiết này.

Độ phức tạp của SLED SLED chia một đầu vào có độ dài n thành C khối có kích thước c. Vì α∈[0,0.5], nó theo sau rằng C∈[2n/c, 2n/(c-αc)]. Trong khi độ phức tạp của việc mã hóa mỗi khối là bậc hai trong c do self-attention, c≪n là hằng số và do đó sự phụ thuộc bộ nhớ và tính toán là tuyến tính trong n.² Cụ thể, độ phức tạp để mã hóa đầu vào với một mô hình có l lớp attention là:

O(lc²×2n/c) = O(lcn):

Giải mã được thực hiện như đề xuất bởi Vaswani và cộng sự (2017), do đó yêu cầu O(nk+k²) bộ nhớ. Giả sử một độ dài chuỗi đầu ra không đổi k≪n, điều này vẫn tuyến tính trong n.

4 Hiệu Quả của Fusion in Decoder
Như đã đề cập (§3), SLED dựa vào giả định rằng các khối có thể được mã hóa độc lập và việc hợp nhất giữa chúng có thể được ủy thác cho bộ giải mã (giả định Tính cục bộ của thông tin). Điều này tương tự như phương pháp Fusion-in-Decoder, được giới thiệu bởi Izacard và Grave (2021) cho hỏi đáp miền mở (ODQA). Tuy nhiên, ở đó, encoder-decoder nhận một tập hợp các đoạn văn độc lập và cần tạo ra một câu trả lời thường có thể được trích xuất từ một đoạn văn duy nhất. Ở đây, chúng tôi mở rộng phạm vi của FiD bằng cách áp dụng nó trên một đầu vào dài, duy nhất và mạch lạc có thể yêu cầu ngữ cảnh hóa toàn cục.

Để chứng minh tính khả thi của FiD cho các nhiệm vụ ngôn ngữ văn bản dài, chúng tôi thiết kế hai thí nghiệm có kiểm soát để định lượng mức độ mà FiD có thể

²Chúng tôi giả sử độ dài tiền tố (m) là không đáng kể và do đó ảnh hưởng của nó đến độ phức tạp tiệm cận là không đáng kể.

thực hiện hai thao tác trung tâm của việc xử lý văn bản dài. Đầu tiên, FiD có thể tìm "kim trong đống cỏ khô" hay không, tức là xác định vị trí một phần thông tin ngắn được nhúng trong văn bản dài, bỏ qua thông tin không liên quan. Thứ hai, FiD có thể "ghép mảnh ghép" và hợp nhất hai phần thông tin được mã hóa độc lập khi tạo ra đầu ra.

4.1 Kim trong đống cỏ khô
Để kiểm tra xem SLED có thể bỏ qua văn bản không liên quan và xác định vị trí một phần thông tin duy nhất hay không, chúng tôi chuyển đổi SQuAD 1.1 (Rajpurkar và cộng sự, 2016) thành một nhiệm vụ sequence-to-sequence với đầu vào dài. SQuAD là một tập dữ liệu hỏi đáp, nơi cho một cặp câu hỏi-đoạn văn, mục tiêu là tạo ra câu trả lời (nằm trong đoạn văn). Đối với mỗi cặp câu hỏi-đoạn văn, chúng tôi lấy mẫu ngẫu nhiên 9 đoạn văn khác từ tập dữ liệu và nối chúng để tạo thành một tài liệu dài.³ Sau đó chúng tôi fine-tune và đánh giá các mô hình của chúng tôi trong hai thiết lập: a) Distractors Có Thứ Tự: đoạn văn vàng là đoạn đầu tiên, và tất cả các distractors khác được nối sau nó. b) Distractors Trộn Lẫn: chúng tôi trộn ngẫu nhiên thứ tự của tất cả các đoạn văn để câu trả lời có thể ở bất kỳ đâu trong tài liệu đầu vào. Vì đây là một nhiệm vụ QA, tiền tố là câu hỏi.

Chúng tôi sử dụng BART base (Lewis và cộng sự, 2020) làm mô hình backbone M của chúng tôi trong suốt §4, và so sánh SLED với một BART base oracle được cung cấp chỉ đoạn văn vàng mà không có đoạn văn distractors. Đây là một thiết lập oracle vì BART base có thể nhận 1,024 token làm đầu vào và tất cả các đoạn văn vàng đều ngắn hơn. Nếu SLED có thể đạt được hiệu suất oracle, chúng ta có thể suy ra rằng thực sự bộ giải mã có thể tìm kim trong đống cỏ khô. Ngoài ra, chúng tôi so sánh SLED với BART base được cung cấp chỉ 1K token đầu tiên và với LED (Beltagy và cộng sự, 2020), sử dụng attention thưa cục bộ, tương tự như SLED (LED có cùng backbone BART base). Tuy nhiên, như đã giải thích trong §2, trường tiếp nhận của các lớp LED tăng tuyến tính với số lượng lớp, và do đó thông tin có thể được hợp nhất trong bộ mã hóa, không giống như SLED nơi việc hợp nhất liên khối phải được ủy thác cho bộ giải mã. Cuối cùng, đối với các nhiệm vụ QA, LED định nghĩa các token câu hỏi như các token toàn cục, và như một kiểm tra tỉnh táo bổ sung, chúng tôi đánh giá LEDL, tức là một mô hình LED cục bộ nơi không sử dụng token toàn cục. Đối với cả LED và SLED, chúng tôi sử dụng kích thước khối c= 256.

³Chúng tôi chỉ xem xét các đoạn văn không nằm trong tài liệu vàng và không chứa câu trả lời vàng.

--- TRANG 5 ---
SLED
LED
LED
BART(a)
0.020.040.060.080.088.1100.0F187.6 87.983.087.5 87.284.1
58.855.0Có Thứ Tự
Trộn Lẫn
SLED
c=1SLED
chunks with
no prefixSLED
drop prefix(b)
0.020.040.060.080.088.1100.0F1
0.164.787.4
0.363.487.1Hình 3: Kết quả F1 trên đánh giá tập phát triển SQuAD 1.1 sửa đổi của chúng tôi (Rajpurkar và cộng sự, 2016): (a) đường ngang cho hiệu suất của một BART base oracle được cung cấp chỉ đoạn văn vàng. SLED đạt được hiệu suất oracle trong cả thiết lập có thứ tự và trộn lẫn (xem văn bản). LED hơi kém hiệu suất so với SLED trong thiết lập trộn lẫn. Cả BART (được cung cấp chỉ 1K token đầu tiên) và LED không có token toàn cục (LEDL) đều hoạt động kém trong thiết lập trộn lẫn. (b) Các ablation về kiến trúc của SLED, xem §4.3 để biết chi tiết.

Kết quả Hình 3(a) cho thấy kết quả đánh giá của chúng tôi trên tập phát triển. SLED gần như đạt được hiệu suất của một BART base oracle không được cung cấp bất kỳ đoạn văn distractors nào, đạt điểm F1 là 87.6 so với F1 oracle là 88.1 (đường ngang trong hình). LED cũng đạt được hiệu suất cao (nhưng thấp hơn SLED trong thiết lập trộn lẫn), cho thấy cả hai mô hình đều học cách bỏ qua thông tin gây nhiễu và tìm kim trong đống cỏ khô. Như mong đợi, cả LEDL và BART đều bị giảm hiệu suất đáng kể khi các đoạn văn được trộn lẫn, vì đoạn văn vàng không được ngữ cảnh hóa với câu hỏi.

4.2 Ghép mảnh ghép
Bây giờ chúng tôi xác minh rằng SLED có thể hợp nhất các phần thông tin từ các khối khác nhau. Để làm điều này, chúng tôi sửa đổi HotpotQA (Yang và cộng sự, 2018), một tập dữ liệu hỏi đáp đa bước, trong đó mỗi

SLED
LED
LED
BART
second passage(a)
0.0010.0020.0030.0040.0050.0060.0070.0078.57F176.572.977.4
67.2
SLED
c=1SLED
chunks with
no prefixSLED
drop prefix(b)
26.269.576.6Hình 4: Kết quả F1 trên tập phát triển HotpotQA của chúng tôi (Yang và cộng sự, 2018). (a) SLED đạt F1 gần với BART base oracle (đường ngang), vượt trội so với một mô hình có quyền truy cập vào đoạn văn chứa câu trả lời ("second passage"). Điều này cho thấy rằng SLED hợp nhất hiệu quả thông tin từ hai khối. Xem văn bản để giải thích thêm về mỗi mô hình. (b) Các ablation về kiến trúc của SLED, xem §4.3 để biết chi tiết.

câu hỏi dựa vào hai phần thông tin (nằm trong các đoạn văn khác nhau). Trong khi trong thiết lập ban đầu, mỗi đầu vào trong HotpotQA có hai đoạn văn vàng và 8 đoạn văn distractors, chúng tôi chỉ bao gồm hai đoạn văn vàng trong các thí nghiệm của chúng tôi. Để đảm bảo SLED và LED mã hóa hai phần thông tin liên quan trong các khối riêng biệt, chúng tôi đặt kích thước khối thành c= 128.

Tương tự như §4.1, chúng tôi so sánh SLED với một BART base oracle với full attention trên 1,024 token,⁴ với LED, và với LEDL. Cuối cùng, các công trình trước đây đã chỉ ra rằng nhiều ví dụ trong HotpotQA có thể được trả lời chỉ với quyền truy cập vào đoạn văn vàng "thứ hai", chứa câu trả lời (Jiang và Bansal, 2019). Do đó, chúng tôi cũng đánh giá một mô hình BART được cung cấp chỉ đoạn văn thứ hai.

Kết quả Hình 4(a) cho thấy rằng thực sự, bộ giải mã của SLED có thể hợp nhất hiệu quả thông tin từ hai khối được mã hóa riêng biệt, đạt F1 là 76.5, hơi thấp hơn F1 oracle là 78.6. Đáng chú ý, SLED vượt trội đáng kể so với một mô hình BART có quyền truy cập vào toàn bộ đoạn văn thứ hai, cho thấy thông tin được hợp nhất bởi bộ giải mã. LED hơi vượt trội so với SLED, nhưng khi bị từ chối quyền truy cập vào các token toàn cục (LEDL), hiệu suất của nó giảm mạnh. Điều này cho thấy rằng trường tiếp nhận lớn của các lớp LED sâu không đủ cho việc hợp nhất thông tin và sự tương tác giữa câu hỏi và văn bản là quan trọng cho bộ giải mã.

Tóm lại, hai thí nghiệm có kiểm soát của chúng tôi cho thấy rằng SLED có thể thực hiện các thao tác truy xuất và hợp nhất thông tin, vốn là cơ bản cho các nhiệm vụ ngôn ngữ văn bản dài.

4.3 Ablations của các lựa chọn thiết kế
Chúng tôi tận dụng thiết lập thí nghiệm có kiểm soát của chúng tôi để điều tra thêm các thành phần của SLED.

Hiệu quả của bộ mã hóa Trong khi §4.2 cho thấy rằng SLED có thể hợp nhất các phần thông tin riêng biệt trong bộ giải mã, không rõ mức độ nào việc ngữ cảnh hóa cục bộ là cần thiết. Để kiểm tra xem có thể tất cả việc hợp nhất xảy ra trong bộ giải mã hay không, chúng tôi fine-tune SLED với kích thước khối c= 1, sao cho các token đầu vào không quan sát bất kỳ ngữ cảnh nào trong bộ mã hóa. Như có thể thấy trong thanh ngoài cùng bên trái trong Hình 3(b) và Hình 4(b), việc loại bỏ ngữ cảnh hóa cục bộ dẫn đến hiệu suất kém, minh họa tầm quan trọng của ngữ cảnh hóa cục bộ.

Ngữ cảnh hóa các khối với tiền tố Như đã giải thích, SLED không sử dụng các token toàn cục, mà thay vào đó ngữ cảnh hóa mỗi khối với một tiền tố được thêm vào đầu. Để xác minh tính cần thiết của nó, chúng tôi fine-tune một mô hình SLED coi tiền tố như một khối khác và không thêm nó vào đầu các khối tài liệu.⁵ Thanh thứ hai trong Hình 3(b) và Hình 4(b) cho thấy một sự giảm hiệu suất đáng kể cho tất cả các thiết lập, gợi ý rằng tiền tố cần thiết trong quá trình mã hóa.

Như mong đợi, không có sự khác biệt thực tế nào giữa các thiết lập Có Thứ Tự và Trộn Lẫn trong Hình 3(b). Ngược lại, LEDL tương tự về khái niệm (do thiếu các token toàn cục) cho thấy một sự giảm đáng kể khi các đoạn văn được trộn lẫn. Điều này cho thấy hiệu quả có thể có của trường tiếp nhận tăng trong LED, nhưng chỉ khi đoạn văn vàng tương đối gần với tiền tố.

Mã hóa tiền tố Sau khi chỉ ra tiền tố quan trọng cho bộ mã hóa, chúng tôi hỏi xem bộ giải mã có cần quyền truy cập trực tiếp vào tiền tố hay thông tin liên quan từ tiền tố có thể được truyền vào các biểu diễn khối. Để kiểm tra điều đó, chúng tôi fine-tune SLED như bình thường, nhưng loại bỏ các token tiền tố khỏi biểu diễn cuối cùng được cung cấp cho bộ giải mã. Thanh ngoài cùng bên phải trong Hình 3(b) và Hình 4(b) cho thấy rằng việc cung cấp cho bộ giải mã các biểu diễn tiền tố tạo ra ít sự khác biệt nếu có, gợi ý rằng thực sự bộ mã hóa có thể truyền thông tin quan trọng từ tiền tố vào các token tài liệu được mã hóa.

⁴Tất cả các ví dụ có 1,024 token, bao gồm cả tiền tố.

⁵Chúng tôi thêm đệm có mặt nạ sau tiền tố để đảm bảo việc chia khối của tài liệu vẫn giống hệt.

5 Thí Nghiệm
Chúng tôi đánh giá SLED trên SCROLLS (Shaham và cộng sự, 2022), một benchmark được đề xuất gần đây để đánh giá hiểu văn bản dài. SCROLLS chứa bảy tập dữ liệu trải rộng trên ba nhiệm vụ hiểu ngôn ngữ khác nhau:

1. Tóm tắt: GovReport (Huang và cộng sự, 2021) là một nhiệm vụ tóm tắt trên các báo cáo từ Dịch vụ Nghiên cứu Quốc hội; SummScreenFD (Chen và cộng sự, 2022) là một tập dữ liệu tóm tắt trên kịch bản TV; QMSum (Zhong và cộng sự, 2021) là một tập dữ liệu tóm tắt dựa trên truy vấn trên bản ghi cuộc họp từ các lĩnh vực khác nhau. Trong khi GovReport và SummScreenFD không chứa tiền tố, đối với QMSum chúng tôi coi truy vấn như tiền tố.

2. Hỏi đáp (QA): Qasper (Dasigi và cộng sự, 2021) là một benchmark QA chứa các câu hỏi trên các bài báo NLP; NarrativeQA (Koˇciský và cộng sự, 2018) chứa các câu hỏi trên toàn bộ sách và kịch bản phim; QuALITY (Pang và cộng sự, 2022) là một tập dữ liệu QA trắc nghiệm trên sách và bài báo. Đối với tất cả các tập dữ liệu QA, chúng tôi đặt câu hỏi làm tiền tố. Đối với QuALITY, chúng tôi coi bốn lựa chọn trả lời là một phần của câu hỏi.

3. Suy luận ngôn ngữ tự nhiên: ContractNLI (Koreeda và Manning, 2021) chứa các giả thuyết pháp lý ngắn (được đặt làm tiền tố) và các tài liệu pháp lý làm tiền đề. Các mô hình được giao nhiệm vụ dự đoán xem tiền đề có kéo theo, mâu thuẫn hay trung tính w.r.t. giả thuyết.

Đối với mỗi nhiệm vụ, chúng tôi sử dụng các chỉ số đánh giá chính thức được định nghĩa trong SCROLLS, dựa trên các chỉ số từ các tập dữ liệu gốc.

5.1 Thiết lập
Chúng tôi đánh giá SLED với cả BART (Lewis và cộng sự, 2020) và T5 (Raffel và cộng sự, 2020b) làm các mô hình backbone. Đối với mỗi mô hình backbone, chúng tôi so sánh hiệu suất với SLED, có thể tiêu thụ các chuỗi dài, so với các mô hình backbone đơn lẻ được cung cấp 1,024 token đầu tiên. Để so sánh, chúng tôi cũng fine-tune LED base. Trong tất cả các thí nghiệm SLED và LED, chúng tôi sử dụng độ dài chuỗi tối đa 16K token và kích thước khối 256 để cho phép đánh giá công bằng.

Đối với mỗi cặp mô hình-tập dữ liệu, chúng tôi chạy điều chỉnh siêu tham số (chi tiết trong Phụ lục C) dựa trên

--- TRANG 6 ---
tập phát triển. Ngoài ra, chúng tôi gửi các dự đoán được tạo ra trên tập kiểm tra đến bảng xếp hạng SCROLLS,⁶ và so sánh với hiệu suất được báo cáo của các mô hình khác tại thời điểm gửi.

5.2 Kết quả
Bảng 1 báo cáo kết quả trên các tập phát triển và kiểm tra SCROLLS. Việc lấy các LMs tiền huấn luyện tầm ngắn như BART và T5 và đưa chúng vào khung SLED cho phép chúng xử lý các tài liệu dài hiệu quả, cải thiện điểm SCROLLS trung bình 4.8-7 điểm. Xem xét BART base-SLED, chúng ta thấy một cải thiện lớn so với LED base (33.6→35.4), và hiệu suất cạnh tranh trên nhiều nhiệm vụ so với LongT5 base và UL2. Hơn nữa, việc thêm SLED vào BART large dẫn đến một mô hình hiệu suất cao với kết quả có thể so sánh với LongT5 base và vượt trội UL2, mặc dù số lượng tham số lớn của UL2 (lớn hơn 50 lần), và không cần tiền huấn luyện đắt đỏ hướng tới các nhiệm vụ tầm xa. Hiệu suất của BART large-SLED vừa phải thấp hơn các mô hình LongT5 lớn hơn.

Ngoại trừ QuALITY, SLED cải thiện đáng kể hiệu suất trên tất cả các nhiệm vụ so với các mô hình backbone tương ứng. Tất cả các tập dữ liệu tóm tắt (GovReport, SummScreenFD và QMSum) cho thấy những cải thiện ấn tượng lên đến 35% so với điểm baseline của chúng, trên tất cả các chỉ số (Rouge-1/Rouge-2/Rouge-L (Lin, 2004)) và cho tất cả ba mô hình backbone. Tương tự, trên ContractNLI (Koreeda và Manning, 2021) chúng ta thấy những cải thiện tương đối lớn. Vì hiệu suất của các mô hình baseline đã cao, sự tăng cường hiệu suất này thậm chí còn quan trọng hơn. Cuối cùng, các tập dữ liệu QA Qasper và NarrativeQA cho thấy những cải thiện lớn nhất, cải thiện trung bình 60%.

QuALITY Trái ngược hoàn toàn với các tập dữ liệu khác là tập dữ liệu QA trắc nghiệm QuALITY (Pang và cộng sự, 2022). Trong khi hiệu suất của BART large-SLED trên ngưỡng ngẫu nhiên, nó hầu như không cải thiện hiệu suất của mô hình backbone (BART large), quan sát chỉ 1K token đầu tiên, với xu hướng tương tự trong các mô hình backbone khác. Phân tích điểm kiểm tra trong Bảng 1, chúng ta thấy rằng việc tăng kích thước mô hình liên tục cải thiện hiệu suất (lên đến 46% exact match), nhưng việc tăng độ dài đầu vào có ảnh hưởng không đáng kể. Vì độ chính xác của con người được báo cáo trên QuALITY là cao (93.5%), điều này gợi ý rằng QuALITY có thể yêu cầu lý luận thông thường và kiến thức vắng mặt từ các mô hình có số lượng tham số thấp hơn.

Tóm tắt Chúng tôi đã chỉ ra rằng việc lấy các LMs tiền huấn luyện sẵn có và nhúng chúng vào SLED dẫn đến hiệu suất cạnh tranh trên SCROLLS. Quan trọng hơn, bất kỳ LM tiền huấn luyện tương lai nào cũng có thể được kết nối dễ dàng vào SLED, mà không cần bước tiền huấn luyện đắt đỏ.

5.3 Phân tích tập dữ liệu
Tính đơn giản và tính mô-đun của SLED cho phép nó được sử dụng như một công cụ hữu ích cho phân tích tập dữ liệu. Cụ thể, chúng tôi có thể thay đổi kích thước khối, c, và số lượng token, n, trên các tập dữ liệu để phân tích a) mức độ cục bộ của các phần thông tin liên quan riêng lẻ, và b) chúng nằm cách xa bao nhiêu trong tài liệu.

Tính cục bộ của thông tin SLED dựa vào một giả định rằng thông tin có thể được ngữ cảnh hóa cục bộ tại thời điểm mã hóa. Để phân tích tính cục bộ, chúng tôi thay đổi kích thước khối, c, định nghĩa cửa sổ attention, và đo lường ảnh hưởng trên các tập dữ liệu SCROLLS với độ dài đầu vào 16K. Hình 5 cho thấy kết quả của thí nghiệm này, nơi trục y cho thấy cải thiện tương đối so với BART base trên một chỉ số mục tiêu như một hàm của kích thước khối c cho tất cả các tập dữ liệu. Chúng tôi quan sát rằng trong tất cả các tập dữ liệu, kích thước khối hoạt động tốt nhất tương đối nhỏ (lên đến 256), và việc tăng c thêm thậm chí có thể làm tổn hại hiệu suất trong một số trường hợp. Tuy nhiên, các tập dữ liệu tóm tắt cho thấy một cải thiện hiệu suất lớn hơn nhiều khi tăng c lên đến ngưỡng đó. Điều này trùng khớp với một giả thuyết phổ biến rằng QA và NLI yêu cầu ngữ cảnh tương đối cục bộ, và do đó việc tăng c có thể thêm nhiễu và làm tổn hại tối ưu hóa, trong khi tóm tắt có thể yêu cầu một cái nhìn cấp độ cao hơn về thông tin.

Khoảng cách từ đầu tài liệu Bây giờ chúng tôi phân tích xem liệu thực sự toàn bộ tài liệu có được yêu cầu cho các nhiệm vụ trong SCROLLS bằng cách thay đổi độ dài tài liệu tối đa, n. Hình 6 cho thấy kết quả của thí nghiệm này, nơi trục y cho thấy cải thiện tương đối của BART base-SLED so với BART base như một hàm của n token đầu tiên từ tài liệu (kích thước khối c= 256). Như mong đợi, tất cả các tập dữ liệu (ngoại trừ QuALITY) cho thấy một cải thiện hiệu suất gần như đơn điệu với n. Điều này cho thấy rằng (a) SLED có thể hiệu quả sử dụng tất cả thông tin trong một chuỗi dài

⁶https://www.scrolls-benchmark.com/leaderboard

--- TRANG 7 ---
Mô hình (Khối/Đầu vào) #Tham số AvgGovRep SumScr QMSum Qspr Nrtv QALT CNLI
ROUGE-1/2/L ROUGE-1/2/L ROUGE-1/2/L F1 F1 EM-T/H EM

Điểm Phát triển
LED base (256=16K) 162M - 57.3/27.9/30.0 30.7/6.3/17.9 32.5/9.0/21.1 30.4 20.2 30.9 82.3
T5base (1K=1K) 220M - 32.8/11.7/20.2 22.2/3.7/15.3 26.1/6.6/19.8 13.2 14.9 35.1 76.8
T5base-SLED (256=16K) 220M - 47.0/20.2/25.2 25.3/5.0/16.6 29.9/8.7/21.4 38.2 18.2 34.6 82.4
BART base (1K=1K) 139M - 47.7/18.5/22.3 30.1/7.0/18.3 32.2/9.3/21.1 23.3 15.9 33.8 78.4
BART base-SLED (256=16K) 139M - 55.7/24.8/25.8 33.6/8.5/19.2 34.4/11.5/22.7 35.8 21.3 33.7 85.3
BART large (1K=1K) 406M - 50.6/19.8/23.5 32.1/7.4/18.7 33.3/9.4/21.6 24.5 17.9 36.1 79.3
BART large-SLED (256=16K) 406M - 57.4/26.3/27.5 35.3/8.8/19.5 36.3/12.2/23.3 42.5 23.6 37.2 85.3

Điểm Kiểm tra
LED base (256=16K) 162M 33.6 56.8/ 27.3/29.2 30.0/6.0/17.5 31.3/8.6/20.5 34.8 21.0 28.5/28.3 82.9
T5base (1K=1K) 220M 26.3 33.2/12.1/20.4 21.4/3.6/15.0 24.2/5.9/18.6 16.3 15.0 31.9/28.6 76.3
T5base-SLED (256=16K) 220M 33.3 46.6/20.1/25.1 24.5/4.6/16.5 28.4/8.7/20.5 43.0 18.9 31.2/29.4 81.4
BART base (1K=1K) 139M 30.6 48.0/19.1/22.7 30.1/6.6/18.1 31.2/9.1/20.3 27.6 16.0 32.5/31.6 77.1
BART base-SLED (256=16K) 139M 35.4 54.7/24.4/25.4 32.7/7.9/19.1 33.8/ 11.7/22.6 41.1 21.5 29.7/30.4 85.6
BART large (1K=1K) 406M 32.1 50.7/20.1/23.5 31.6/6.8/18.5 32.0/9.1/20.8 29.2 18.3 34.8/33.9 79.7
BART large-SLED (256=16K) 406M 38.0 57.5 /26.3/27.4 35.2/8.7/19.4 34.2 /11.0/22.0 46.9 24.1 34.8/34.8 87.3
LEDSCROLLS y
base (1K=16K) 162M 29.2 56.2/26.6/28.8 24.2/4.5/15.4 25.1/6.7/18.8 26.6 18.5 25.8/25.4 71.5
LongT5y
base (255=16K) 220M 38.2 53.5/27.3/29.3 34.8/ 9.6/21.1 33.9/11.0/22.8 46.6 23.0 37.9/36.6 85.6
LongT5y
large (255=16K) 770M 40.5 54.2/27.8/29.8 35.6/9.2/ 21.2 35.1/ 12.0/23.3 52.3 27.2 40.6/38.6 87.3
LongT5y
XL (255=16K) 3B 41.9 54.7/ 28.2/30.2 35.8/9.6 /21.1 34.9/11.8/ 23.5 53.1 29.3 46.0/42.1 88.2
UL2y(2K=2K) 20B 37.9 53.6/26.1/28.8 32.9/7.8/19.4 31.1/8.5/20.4 37.6 24.2 45.8/40.7 88.7

Bảng 1: Kết quả chính trên benchmark SCROLLS. Khối/Đầu vào tham chiếu đến kích thước khối được sử dụng (c) và đến độ dài đầu vào tối đa (n). Avg là điểm SCROLLS trung bình như mô tả trong Shaham và cộng sự (2022). Điểm phát triển cho QuALITY chỉ dành cho tập đầy đủ (T). y biểu thị kết quả được báo cáo từ bảng xếp hạng công khai SCROLLS.⁶ Điểm LEDSCROLLS base được báo cáo bởi Shaham và cộng sự (2022) và thấp hơn triển khai LED base của chúng tôi, có lẽ vì triển khai của chúng tôi sử dụng tất cả các token câu hỏi cho attention toàn cục thay vì chỉ token đầu tiên. Kết quả cho LongT5 và UL2 được gửi đến bảng xếp hạng SCROLLS bởi các tác giả của chúng.

quence (lên đến 16K token),⁷ và rằng (b) quan sát toàn bộ đầu vào từ SCROLLS cải thiện hiệu suất.

5.4 Ảnh hưởng của đệm ngữ cảnh
Trong tất cả các thí nghiệm cho đến nay, chúng tôi đã sử dụng một giá trị đệm bảo thủ α = 0.5, dẫn đến kích thước khối hiệu quả c/2 và c/4 token đệm ngữ cảnh ở mỗi bên. Vì cả bộ nhớ và, quan trọng hơn, số lượng forward passes qua bộ mã hóa đều tuyến tính trong số lượng khối, một câu hỏi tự nhiên là cần bao nhiêu đệm và chồng lấp để đạt được kết quả thỏa mãn.

Để khám phá điều này, chúng tôi fine-tune BART base-SLED trên tất cả sáu tập dữ liệu nơi SLED cho thấy cải thiện so với mô hình baseline của nó (tức là, tất cả các tập dữ liệu ngoại trừ QuALITY), thay đổi giá trị của α, và cố định c= 256. Bảng 2 cho thấy kết quả của thí nghiệm này, nơi chúng tôi so sánh cải thiện tương đối so với BART base trên các giá trị α khác nhau.

Như mong đợi, việc giảm hệ số đệm và do đó số lượng khối giảm thời gian huấn luyện. Khi α = 0.05, huấn luyện có thể nhanh hơn lên đến 2 lần so với α = 0.5 vì số lượng khối giảm xuống gần một nửa. Hơn nữa, cải thiện tương đối (tức là, cải thiện tương đối so với baseline) thường gần hoặc thậm chí cao hơn với ít đệm hơn (có lẽ do mã hóa tốt hơn hoặc tối ưu hóa ổn định hơn). Tuy nhiên, không có giá trị α duy nhất nào luôn luôn đánh bại lựa chọn bảo thủ α = 0.5. Cụ thể, trong tất cả sáu tập dữ liệu, việc đặt α = 0.5 dẫn đến hiệu suất top-2, thường với một khoảng cách lớn và không bao giờ tệ hơn đáng kể so với kết quả tốt nhất. Do đó, chúng tôi kết luận rằng có thể cải thiện hiệu quả và hiệu suất của SLED bằng cách điều chỉnh siêu tham số α cho hành vi tối ưu w.r.t. một nhiệm vụ cụ thể, và chúng tôi cố định α = 0.5 trong các thí nghiệm của chúng tôi.

Hơn nữa, Bảng 2 chứng minh tầm quan trọng của việc có các khối ít nhất một phần chồng lấp. Trong tất cả sáu tập dữ liệu, việc sử dụng các khối không chồng lấp (α = 0) dẫn đến một sự giảm ít nhất 10% cải thiện so với thiết lập tốt nhất, nơi trong một số trường hợp khoảng cách này tăng lên hơn 50%. Điều này hỗ trợ giả thuyết của chúng tôi rằng việc chia khối đầu vào không chồng lấp có thể dẫn đến mất thông tin quan trọng.

⁷Đối với ContractNLI, độ dài của hơn 95% các ví dụ được token hóa là dưới 8K.

--- TRANG 8 ---
101520253035Cải thiện tương đốiGovReport SummScreen QMSum
0 200 400 600 800 1000
Kích thước khối (c)10
01020304050Cải thiện tương đốiNarrativeQA
QasperQuALITY
ContractNLIHình 5: Cải thiện tương đối của BART base-SLED so với kết quả BART base, khi thay đổi kích thước khối của SLED (tức là c), cố định độ dài đầu vào tối đa là 16K. Trên: Các tập dữ liệu tóm tắt. Trục y đo cải thiện tương đối của Rouge-2. Dưới: Các tập dữ liệu QA và NLI. Trục y đo cải thiện tương đối của exact match cho QuALITY và ContractNLI và F1 cho NarrativeQA và Qasper.

6 Công Trình Liên Quan
Transformers hiệu quả Nhiều biến thể attention hiệu quả được đề xuất trong những năm gần đây, để giảm bớt độ phức tạp bậc hai của dense attention (Tay và cộng sự, 2020; Fournier và cộng sự, 2021). Trong số đó là clustering các vector thành các bucket riêng biệt, tính toán attention chỉ trong mỗi bucket (Kitaev và cộng sự, 2020a), chỉ chú ý đến một số cố định các hidden vector (Ma và cộng sự, 2021), sử dụng random features để xấp xỉ ma trận attention (Choromanski và cộng sự, 2021; Peng và cộng sự, 2021), và sử dụng phân tích thừa số hạng thấp (Wang và cộng sự, 2020). Mặc dù đạt được hiệu suất đáng kính khi fine-tuning các mô hình này trên benchmark Long Range Arena (Tay và cộng sự, 2021), nhiều trong số chúng chưa được chứng minh hoạt động tốt như một backbone cho các mô hình ngôn ngữ tiền huấn luyện. Thực tế, công trình gần đây (Xiong và cộng sự, 2022b) trên các mô hình chỉ encoder đã phát hiện nhiều không vượt trội so với một attention cục bộ sliding window đơn giản trên các nhiệm vụ ngôn ngữ downstream. Chúng tôi thảo luận các phương pháp như vậy tiếp theo.

10
0102030Cải thiện tương đốiGovReport SummScreen QMSum
29
210
211
212
213
214
Độ dài đầu vào tối đa (n)02040Cải thiện tương đốiNarrativeQA
QasperQuALITY
ContractNLIHình 6: Cải thiện tương đối của BART base-SLED so với kết quả BART base, khi thay đổi độ dài đầu vào được cung cấp cho SLED, cố định c= 256. Trên: Các tập dữ liệu tóm tắt so sánh w.r.t. Rouge-2. Dưới: Các tập dữ liệu QA và NLI. Cải thiện tương đối được đo w.r.t. exact match cho QuALITY và ContractNLI và F1 cho NarrativeQA và Qasper.

Các biến thể sparse attention Một giải pháp phổ biến và đơn giản để cho phép các mô hình dựa trên attention xử lý các chuỗi dài là sử dụng local attention, nơi mỗi token chú ý đến một cửa sổ cục bộ xung quanh nó. Longformer (Beltagy và cộng sự, 2020), GMAT (Gupta và Berant, 2020), và ETC (Ainslie và cộng sự, 2020) sử dụng các cửa sổ ngắn của full attention, kết hợp với full attention đến một số lượng nhỏ các token đầu vào toàn cục được định nghĩa trước. BigBird (Zaheer và cộng sự, 2020b) chia sẻ các tính năng cục bộ và toàn cục, và bổ sung lấy mẫu ngẫu nhiên các token để chú ý đến. Cuối cùng, LongT5 được đề xuất gần đây (Guo và cộng sự, 2022) mở rộng T5 (Raffel và cộng sự, 2020a) với các thành phần attention cục bộ và toàn cục dựa trên ETC, giảm bớt nhu cầu chỉ định thủ công các token toàn cục. Trong công trình này, chúng tôi chứng minh rằng một sliding window đơn giản với các mô hình off-the-shelf mà không có bất kỳ sửa đổi nào là một thay thế mạnh mẽ cho nhiều nhiệm vụ tạo sinh yêu cầu xử lý các tài liệu dài.

Ngoài transformers Như một thay thế cho transformers để xử lý các chuỗi dài, Gu và cộng sự (2021) đã đề xuất kiến trúc Structured State Space

--- TRANG 9 ---
Cải thiện Tương đối
GovRep SumScr QMSum Qspr Nrtv CNLI
50% 34.1% 21.0% 22.8% 53.7% 34.2% 8.9%
25% 28.5% 19.0% 17.9% 54.7% 29.4% 10.1%
5% 18.7% 15.9% 23.5% 52.0% 31.9% 7.4%
0% 27.1% 9.5% 11.5% 46.1% 29.2% 6.9%

Bảng 2: Cải thiện tương đối của BART base-SLED so với BART base khi thay đổi tỷ lệ phần trăm đệm (α). Trong tất cả các trường hợp, độ dài đầu vào tối đa là 16K và c= 256. Cải thiện tương đối được đo w.r.t. Rouge-2 cho GovReport, SummScreenFD và QMSum, F1 cho Qasper và NarrativeQA và exact match cho ContractNLI. Trong mỗi cột, chữ đậm đánh dấu giá trị hoạt động tốt nhất và gạch chân giá trị tốt thứ hai.

(S4) cho thấy những cải thiện đáng kể so với transformers trên benchmark LRA (Tay và cộng sự, 2021). Các mô hình state space hiện là một lĩnh vực nghiên cứu tích cực (Gupta, 2022; Mehta và cộng sự, 2022), nhưng hiệu quả của chúng trên các nhiệm vụ hiểu ngôn ngữ tầm xa chưa được kiểm tra.

Fusion-in-Decoder Izacard và Grave (2021) đề xuất mã hóa nhiều đoạn văn độc lập riêng biệt, và nối các mã hóa trước giai đoạn giải mã. Mặc dù có bằng chứng thực nghiệm khuyến khích (Amouyal và cộng sự, 2022; Yavuz và cộng sự, 2022), chúng tôi là người đầu tiên (theo hiểu biết của chúng tôi) phân tích tính khả thi và hạn chế của FiD trong một thiết lập có kiểm soát. Quan trọng hơn, chúng tôi kiểm tra FiD trên các nhiệm vụ tầm xa trên một tài liệu dài duy nhất, thay vì một tập hợp các đoạn văn độc lập.

Các mô hình tiền huấn luyện với sliding windows
Việc bao bọc một bộ mã hóa BERT trong một sliding window được đề xuất bởi Cui và Hu (2021) trong ngữ cảnh của một kiến trúc chuyên biệt cho tóm tắt. Wang và cộng sự (2019) chỉ ra rằng sliding BERT trên văn bản cải thiện hiệu suất trên một số tập dữ liệu QA. Trong công trình này, chúng tôi đề xuất một phương pháp sliding window có thể được kết nối dễ dàng vào bất kỳ mô hình encoder-decoder hiện có nào mà không có tham số bổ sung hoặc huấn luyện đặc trưng cho nhiệm vụ, và chỉ ra hiệu quả của nó cho hiểu văn bản tầm xa. Tương tự nhất với SLED, là phương pháp SEGENCap được đề xuất bởi Vig và cộng sự (2022). Bằng cách chia đầu vào từ QMSum thành các khối chồng lấp, mã hóa chúng riêng biệt, và sau đó thực hiện FiD (sử dụng hai biểu diễn cho mỗi token đầu vào), các tác giả đã có thể đạt được kết quả state-of-the-art. Tuy nhiên, Vig và cộng sự (2022) tập trung vào tóm tắt và không thực hiện phân tích có hệ thống về loại kiến trúc này.

7 Hạn Chế
Chúng tôi trình bày SLED như một phương pháp đơn giản và hiệu quả để mở rộng khả năng của các mô hình văn bản ngắn tiền huấn luyện cho các nhiệm vụ văn bản dài. Mặc dù có hiệu suất thực nghiệm ấn tượng trên SCROLLS, SLED gặp phải hai nhược điểm có thể hạn chế khả năng áp dụng của nó cho một số nhiệm vụ tầm xa.

Đầu ra dài Để có được độ phức tạp tuyến tính, SLED giả sử độ dài đầu ra k là hằng số. Điều này là do bộ giải mã sử dụng self-attention bậc hai trên đầu ra, trên top của O(nk) cross-attention giữa đầu ra và đầu vào. Trong khi hầu hết các nhiệm vụ văn bản dài hiện tại tuân theo giả định này, các nhiệm vụ tương lai, như báo cáo học thuật hoặc viết kịch bản, có thể yêu cầu tạo sinh văn bản dài. Hạn chế này không duy nhất đối với SLED và ảnh hưởng đến các transformer tầm xa khác bao gồm LongT5 và LED. Ngoài fine-tuning, điều này cũng ảnh hưởng đến việc tiền huấn luyện các mô hình trên đầu vào dài với các loss tự giám sát như span-corruption (Raffel và cộng sự, 2020b) hoặc denoising (Lewis và cộng sự, 2020), yêu cầu bộ giải mã xử lý một đầu ra tuyến tính trong độ dài của đầu vào.

Giải quyết đồng tham chiếu và lưu giữ sự thật Một giả định trung tâm của SLED là giả định Tính cục bộ của thông tin. Khi văn bản đầu vào dài, giả định này có thể bị phá vỡ nếu yêu cầu giải quyết thực thể xa hoặc kiến thức thực tế. Ví dụ, một chương trong một cuốn sách có thể đề cập "họ đang đi vào phòng" khi kiến thức về phòng nào hoặc ai đã đi nằm vài chương trước đó. Trong những trường hợp như vậy, bộ mã hóa được sử dụng bởi SLED sẽ không thể truy cập thông tin này, chuyển nhiều trách nhiệm hơn cho bộ giải mã và giảm hiệu quả của việc mã hóa ngữ cảnh. Tương tự, trong các câu hỏi đa bước (Yang và cộng sự, 2018), việc chú ý đến một phần của ngữ cảnh là cần thiết để hiểu đầy đủ câu hỏi và mã hóa một phần thông tin thứ hai một cách chính xác. Vì bộ mã hóa sẽ không có quyền truy cập vào ngữ cảnh đầu tiên dẫn đến hiểu câu hỏi tốt hơn, ở đây cũng vậy, nhiều trách nhiệm hơn được ủy thác cho bộ giải mã.

8 Kết Luận
Trong công trình này, chúng tôi trình bày SLED, một phương pháp đơn giản để mô hình hóa văn bản dài trượt một bộ mã hóa tầm ngắn tiền huấn luyện trên một tài liệu đầu vào dài

--- TRANG 10 ---
và sau đó tạo ra một đầu ra bằng cách chú ý đến các token được mã hóa. Chúng tôi chỉ ra SLED có thể thực hiện các thao tác cốt lõi quan trọng cho hiểu văn bản dài, như tìm các phần thông tin liên quan và hợp nhất chúng tại thời điểm giải mã, và chứng minh hiệu suất cạnh tranh trên benchmark SCROLLS so với các mô hình lớn hơn và các mô hình sử dụng một bước tiền huấn luyện chuyên dụng và đắt đỏ.

Một trong những tính năng hấp dẫn nhất của SLED là nó có thể được sử dụng dễ dàng với bất kỳ LM tầm ngắn tiền huấn luyện nào. Do đó, bất kỳ mô hình encoder-decoder tương lai nào cũng có thể được kết nối linh hoạt vào nó để đạt được những cải thiện thêm trong hiệu suất trên SCROLLS, một số nhiệm vụ của nó, hoặc bất kỳ nhiệm vụ tầm xa nào khác.

Chúng tôi mở mã nguồn SLED và hy vọng nó khuyến khích cộng đồng nghiên cứu dễ dàng mở rộng sang đầu vào dài hơn và đẩy ranh giới của khả năng áp dụng của các mô hình NLU trong các trường hợp sử dụng thực tế.

Lời Cảm Ơn
Nghiên cứu này được hỗ trợ một phần bởi Sáng kiến Yandex cho Học Máy, Học bổng Shashua, quỹ Len Blavatnik và gia đình Blavatnik, và Hội đồng Nghiên cứu Châu Âu (ERC) dưới chương trình nghiên cứu và đổi mới Horizons 2020 của Liên minh Châu Âu (tài trợ ERC DELPHI 802800). Chúng tôi cũng muốn cảm ơn biên tập viên hành động và các nhà phản biện ẩn danh vì những gợi ý và phản hồi sâu sắc của họ. Công trình này được hoàn thành để đáp ứng một phần yêu cầu cho bằng Tiến sĩ của tác giả đầu tiên.

A Chi tiết triển khai SLED
Trong khi §3 chi tiết phương pháp của SLED, nó bỏ qua việc xử lý các token ở rìa để ngắn gọn. Việc mã hóa c/2 token đầu vào đầu tiên và cuối cùng yêu cầu sự chú ý đặc biệt, vì chúng thiếu ngữ cảnh hai chiều. Để bảo tồn càng nhiều điểm chung giữa các khối, tất cả (2-α)c/2 token đầu tiên được coi là các token khối hiệu quả trong khối đầu tiên. Để tính đến các token cuối cùng, khối cuối cùng sẽ luôn bắt đầu tại token tn-c+1 để nó chứa chính xác c token, và các token khối hiệu quả của nó sẽ được định nghĩa là tất cả các token không phải là một phần của bất kỳ khối hiệu quả nào trước đó.

B Chunking vs. local-attention
Cả LED và SLED đều là các mô hình tầm xa được xây dựng trên cùng một mô hình văn bản ngắn (BART), và sử dụng local attention. Tuy nhiên, SLED dựa vào chunking, trong khi LED sử dụng local attention theo lớp. Trong phần này, chúng tôi bây giờ thảo luận chi tiết hơn về mối quan hệ giữa hai phương pháp.

Triển khai Một trong những lợi thế lớn nhất của SLED là nó bất khả tri với mô hình encoder-decoder backbone, và có thể mở rộng bất kỳ mô hình hiện có nào mà không có overhead triển khai bổ sung. Ngược lại, cơ chế attention trong Longformer, và sau đó là LED, được triển khai bởi Beltagy và cộng sự (2020) với một kernel CUDA chuyên biệt được ghép nối với kiến trúc và triển khai của BART. Điều này làm cho LED hiệu quả hơn, nhưng việc mở rộng nó cho các kiến trúc mới phát sinh overhead kỹ thuật đáng kể. Điều này là do LED sử dụng một attention cửa sổ cục bộ "chéo" trên các lớp, mà một triển khai ngây thơ không hiệu quả. Ngược lại, SLED sử dụng chunking, cho phép đơn giản bao bọc một mô hình encoder-decoder hiện có.

Ngữ cảnh hóa Sự khác biệt quan trọng nhất giữa LED và SLED từ góc độ khái niệm là cơ chế ngữ cảnh hóa của chúng. Trong khi SLED chia đầu vào thành các khối (chồng lấp) và mã hóa từng khối độc lập, LED thực hiện local attention theo lớp. Điều này dẫn đến một trường tiếp nhận hiệu quả tăng tuyến tính với độ sâu bộ mã hóa, có thể cho phép nó thực hiện ngữ cảnh hóa "toàn cục" hơn. Kết quả của chúng tôi trong §4 gợi ý rằng ngữ cảnh hóa toàn cục như vậy có lợi, và một kết luận tương tự có thể đạt được khi quan sát rằng LED base, sử dụng tất cả các token tiền tố như các token toàn cục, vượt trội so với LEDSCROLLS base, chỉ sử dụng một token duy nhất cho ngữ cảnh hóa toàn cục.

Thông tin vị trí Cơ chế chunking của SLED có nghĩa là nó sử dụng mã hóa vị trí của mô hình cơ bản độc lập trong mỗi khối, và do đó bất khả tri với kỹ thuật nhúng vị trí được sử dụng bởi mô hình backbone. Hơn nữa, nó có thể cho phép SLED tổng quát hóa đến độ dài đầu vào tùy ý. Ngược lại, LED sử dụng các embeddings tuyệt đối của BART, nhân đôi chúng 16 lần để hỗ trợ các chuỗi dài 16K. Điều này hạn chế khả năng tổng quát hóa đến đầu vào dài hơn, và có thể gây ra yêu cầu cho một lượng đáng kể các mẫu đầu vào dài để điều chỉnh đúng các tham số mới đó (Shaham và cộng sự, 2022). Điều này rõ ràng trong Bảng 1 khi so sánh điểm kiểm tra của LED base với BART base-SLED và xem xét số lượng

--- TRANG 11 ---
cho các mẫu huấn luyện. Trong NarrativeQA và GovReport, chứa 71K và 19K mẫu tương ứng, LED có thể so sánh với SLED và thậm chí hơi vượt trội trên một số chỉ số. Trong ContractNLI (10K ví dụ), nó hoạt động hơi tệ hơn. Trong tất cả các tập dữ liệu khác, nơi dữ liệu huấn luyện nhỏ, LED tệ hơn đáng kể so với SLED.

Độ phức tạp Chúng tôi đã phân tích phân tích độ phức tạp của bộ mã hóa SLED (§3), là O(lcn). Một phân tích tương tự của LED cho thấy rằng trong mỗi lớp, LED xem xét O(n) cửa sổ có độ dài c, nơi trong mỗi cửa sổ chỉ token giữa chú ý đến vùng lân cận cục bộ của nó, dẫn đến độ phức tạp bộ nhớ O(lcn) cũng vậy.

Tuy nhiên, do việc sử dụng chồng lấp và self-attention đầy đủ trong mỗi khối của SLED, việc mã hóa của SLED có thể yêu cầu lên đến 2x bộ nhớ hơn so với LED khi α = 0.5.

C Chi tiết thí nghiệm
Thiết lập thí nghiệm của chúng tôi dựa trên kho lưu trữ chính thức SCROLLS.⁸ Các đầu vào và phân chia tập dữ liệu vẫn như được đề xuất bởi các tác giả của SCROLLS cũng như số epoch được đề xuất cho mỗi tập dữ liệu. Để thực hiện lựa chọn mô hình, đối với mỗi cặp mô hình-tập dữ liệu, chúng tôi fine-tune 9 mô hình với lập lịch learning rate LINEAR, optimizer AdamW với các thiết lập mặc định, và đặt learning rate thành một trong {2e-5, 5e-5, 1e-4} và kích thước batch hiệu quả thành một trong {8, 16, 32}. Warmup được cố định ở 10% và weight decay ở 0.01. Tất cả mã, dữ liệu, yêu cầu môi trường python, siêu tham số và các script cần thiết để tái tạo kết quả của chúng tôi sẽ được công khai khi xuất bản.

Tài Liệu Tham Khảo
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, và Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 268–284, Online. Association for Computational Linguistics.

⁸https://github.com/tau-nlp/scrolls

Samuel Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, và Jonathan Berant. 2022. Qampari: : An open-domain question answering benchmark for questions with many answers from multiple paragraphs. ArXiv, abs/2205.12665.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. 2020. Longformer: The long-document transformer.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Mingda Chen, Zewei Chu, Sam Wiseman, và Kevin Gimpel. 2022. SummScreen: A dataset for abstractive screenplay summarization. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 8602–8615, Dublin, Ireland. Association for Computational Linguistics.

Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, và Adrian Weller. 2021. Rethinking attention with performers. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. Trong Proceedings of the

--- TRANG 12 ---
58th Annual Meeting of the Association for Computational Linguistics, trang 8440–8451, Online. Association for Computational Linguistics.

Peng Cui và Le Hu. 2021. Sliding selector network with dynamic memory for extractive summarization of long documents. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 5881–5891, Online. Association for Computational Linguistics.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, và Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 4599–4610, Online. Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Quentin Fournier, Gaétan Marceau Caron, và Daniel Aloise. 2021. A practical survey on faster and lighter transformers.

Albert Gu, Karan Goel, và Christopher R'e. 2021. Efficiently modeling long sequences with structured state spaces. ArXiv, abs/2111.00396.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, và Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. Trong Findings of the Association for Computational Linguistics: NAACL 2022, trang 724–736, Seattle, United States. Association for Computational Linguistics.

Ankit Gupta. 2022. Diagonal state spaces are as effective as structured state spaces. ArXiv, abs/2203.14343.

Ankit Gupta và Jonathan Berant. 2020. GMAT: Global memory augmentation for transformers. ArXiv preprint, abs/2006.03274.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, và Lu Wang. 2021. Efficient attentions for long document summarization. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1419–1436, Online. Association for Computational Linguistics.

Gautier Izacard và Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, trang 874–880, Online. Association for Computational Linguistics.

Yichen Jiang và Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 2726–2736, Florence, Italy. Association for Computational Linguistics.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. 2020a. Reformer: The efficient transformer. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. 2020b. Reformer: The efficient transformer. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, và Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328.

Yuta Koreeda và Christopher Manning. 2021. ContractNLI: A dataset for document-level natural language inference for contracts. Trong Findings of the Association for Computational Linguistics: EMNLP 2021, trang 1907–1919, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 7871–7880, Online. Association for Computational Linguistics.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. Trong Text Summarization Branches Out, trang 74–81, Barcelona, Spain. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.

Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, và Luke Zettlemoyer. 2021. Luna: Linear unified nested attention. Trong Advances in Neural Information Processing Systems.

Harsh Mehta, Ankit Gupta, Ashok Cutkosky, và Behnam Neyshabur. 2022. Long range language modeling via gated state spaces. ArXiv, abs/2206.13947.

Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, và Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 5336–5358, Seattle, United States. Association for Computational Linguistics.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, và Lingpeng Kong. 2021. Random feature attention. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020a. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.

Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020b. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. Trong Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, trang 2383–2392, Austin, Texas. Association for Computational Linguistics.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, và Omer Levy. 2022. Scrolls: Standardized comparison over long language sequences. ArXiv, abs/2201.03533.

Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Quang Tran, Dani Yogatama, và Donald Metzler. 2022a. Scaling laws vs model architectures: How does inductive bias influence scaling? ArXiv, abs/2207.10551.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. 2021. Long range arena : A benchmark for efficient transformers. Trong 9th International Conference on Learning Representations, ICLR

--- TRANG 13 ---
2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. 2020. Efficient transformers: A survey.

Yi Tay, Mostafa Dehghani, Vinh Quang Tran, Xavier García, Dara Bahri, Tal Schuster, Huaixiu Zheng, Neil Houlsby, và Donald Metzler. 2022b. Unifying language learning paradigms. ArXiv, abs/2205.05131.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, trang 5998–6008.

Jesse Vig, Alexander Fabbri, Wojciech Kryściński, Chien-Sheng Wu, và Wenhao Liu. 2022. Exploring neural models for query-focused summarization. Trong Findings of the Association for Computational Linguistics: NAACL 2022, trang 1455–1468, Seattle, United States. Association for Computational Linguistics.

Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. 2020. Linformer: Self-attention with linear complexity.

Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, và Bing Xiang. 2019. Multi-passage BERT: A globally normalized BERT model for open-domain question answering. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang 5878–5882, Hong Kong, China. Association for Computational Linguistics.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, trang 38–45, Online. Association for Computational Linguistics.

Wenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Scott Yih, và Yashar Mehdad. 2022a. Simple local attentions remain competitive for long-context tasks. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1975–1986, Seattle, United States. Association for Computational Linguistics.

Wenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Scott Yih, và Yashar Mehdad. 2022b. Simple local attentions remain competitive for long-context tasks. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1975–1986, Seattle, United States. Association for Computational Linguistics.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, và Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, trang 2369–2380, Brussels, Belgium. Association for Computational Linguistics.

Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Nitish Shirish Keskar, và Caiming Xiong. 2022. Modeling multi-hop question answering as single sequence prediction. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 974–990, Dublin, Ireland. Association for Computational Linguistics.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020a. Big bird: Transformers for longer sequences. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020b. Big bird: Transformers for longer sequences. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, và Dragomir Radev. 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 5905–5921, Online. Association for Computational Linguistics.
