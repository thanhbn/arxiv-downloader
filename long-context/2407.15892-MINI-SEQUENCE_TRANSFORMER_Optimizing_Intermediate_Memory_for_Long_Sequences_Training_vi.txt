# 2407.15892.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2407.15892.pdf
# Kích thước tệp: 3785537 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
MINI-SEQUENCE TRANSFORMER : Tối ưu hóa
Bộ nhớ Trung gian cho Huấn luyện Chuỗi Dài
Cheng Luo
California Institute of Technology
chengluo@caltech.eduJiawei Zhao
Meta FAIR
jwzhao@meta.comZhuoming Chen
Carnegie Mellon University
zhuominc@andrew.cmu.edu
Beidi Chen
Carnegie Mellon University
beidic@andrew.cmuAnima Anandkumar
California Institute of Technology
anima@caltech.edu
Tóm tắt
Chúng tôi giới thiệu MINI-SEQUENCE TRANSFORMER (MST), một phương pháp đơn giản và hiệu quả cho việc huấn luyện LLM có hiệu quả cao và chính xác cao với các chuỗi cực dài. MST phân vùng chuỗi đầu vào và xử lý lặp lại các mini-chuỗi để giảm việc sử dụng bộ nhớ trung gian. Được tích hợp với tính toán lại kích hoạt, nó cho phép tiết kiệm bộ nhớ đáng kể trong cả quá trình truyền xuôi và truyền ngược. Trong các thí nghiệm với mô hình Llama3-8B, với MST, chúng tôi đo được không có suy giảm thông lượng hoặc hội tụ ngay cả với chuỗi dài gấp 12 lần so với các triển khai tiêu chuẩn. MST hoàn toàn tổng quát, không phụ thuộc vào triển khai, và yêu cầu những thay đổi mã tối thiểu để tích hợp với các khung huấn luyện LLM hiện có. Được tích hợp với thư viện huggingface, MST thành công mở rộng độ dài ngữ cảnh tối đa của Qwen, Mistral, và Gemma-2 gấp 12-24 lần.

1 Giới thiệu
Sự phát triển của Transformer [56] đã là một hành trình đáng chú ý, với mỗi lần lặp lại đẩy ranh giới của những gì có thể về kích thước mô hình, hiệu suất và hiệu quả. Một trong những thách thức quan trọng trong hành trình này là quản lý các yêu cầu bộ nhớ của những mô hình này, đặc biệt là trong quá trình huấn luyện. Khi các Transformer đã tăng trưởng đáng kể về kích thước [10] và độ phức tạp [44], nhu cầu bộ nhớ đã tăng theo cấp số nhân, cần thiết các giải pháp sáng tạo để tối ưu hóa việc sử dụng bộ nhớ trong khi duy trì hiệu suất.

Một cột mốc quan trọng trong hành trình này là sự giới thiệu của multi-query attention [50]. Kỹ thuật này giảm đáng kể kích thước của KV-cache trong quá trình suy luận, sử dụng nhiều query head nhưng chỉ có single key và value head. Ý tưởng này lần đầu tiên được áp dụng trong huấn luyện quy mô lớn của PaLM [12], sau đó được áp dụng và kiểm tra thực nghiệm trong LLaMA [55]. Khi lĩnh vực tiến bộ, multi-query attention phát triển thành grouped query attention (GQA) [2], nó nới lỏng hạn chế single key và value head thành nhiều head, và mỗi head được ghép nối với một nhóm query. Nó cải thiện đáng kể chất lượng và được áp dụng bởi Llama2-70B [55] và Mistral-7B [24].

Để cải thiện thêm chất lượng mô hình, Llama3 [36] giới thiệu một tokenizer với từ vựng 128K token, cho phép mã hóa ngôn ngữ hiệu quả hơn so với từ vựng 32K của Llama2. Ngoài ra, Llama3 tăng kích thước trung gian MLP của nó từ 11k lên 14k. Những thay đổi này phản ánh xu hướng hướng tới từ vựng và kích thước trung gian rộng hơn để có chất lượng tốt hơn. Trong khi đó, Llama3 duy trì kích thước ẩn 4k của nó để có hiệu quả suy luận. Xu hướng này cũng được phản ánh trong việc phát triển Phi-3 [1] của Microsoft so với Phi-2 [23].

38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2407.15892v4  [cs.LG]  9 Nov 2024

--- TRANG 2 ---
NormAttentionNormNormLoss Loss 
(a) Kiến trúc Transformer 
Thông thường(b) Chi tiết Mini-Sequence 
Transformer Đề xuất LM head:
Intermediate (S)LM head:
Intermediate (S)
MLP:
Intermediate (S)MLP:
Intermediate (S)
Tensor (S)Tensor (S)L x
MLP/LM_head inputs (S)MLP/LM_head inputs (S)Mini -Seq. 1 
(S/M )Mini -Seq. 1 
(S/M )MLP/LM head
(S/M )MLP/LM head
(S/M )Mini -Out. 1 Mini -Out. 1 OutputOutput
...Mini -Seq.  
Mini -Seq.M 
(S/M )Mini -Seq.M 
(S/M )MLP/LM head
(S/M )MLP/LM head
(S/M )Mini -Out.M Mini -Out.M 
(C) Kích thước Chuỗi Tối đa của Các Mô hình Khác nhau, 
không có Suy giảm Thông lượng và Hội tụ1020304050607080
Llama3 Llama2Kích thước Chuỗi Tối đa(K)60k(12x)
14k(3x)
5k(1x)7k(1x)45k(7x)84k(12x)MsT(công trình của chúng tôi)
Activation Recompute
PyTorch
0Hình 1: (a) Kiến trúc Transformer tiêu chuẩn. Độ dài chuỗi kích hoạt của MLP và LM-Head được chú thích bằng S. (b) MINI-SEQUENCE TRANSFORMER được sử dụng để thay thế các khối MLP và khối LM-Head, chia chuỗi đầu vào S thành M mini-chuỗi với độ dài chuỗi S/M, trong đó M= 2 trên hình này. (c) Kích thước chuỗi tối đa để huấn luyện Llama2/Llama3 trên GPU A100-80GB, không có suy giảm thông lượng hoặc hội tụ khi sử dụng phương pháp của chúng tôi.

Những tiến bộ này cũng mang lại những thách thức bộ nhớ mới, đặc biệt là trong giá trị trung gian của các lớp tuyến tính của multilayer perception (MLP) và language modeling head (LM-Head). Sự gia tăng đáng kể trong các biến trung gian, có thể gần gấp mười lần lớn hơn các biến đầu vào, đã hạn chế nghiêm trọng khả năng mở rộng độ dài chuỗi và kích thước batch của mạng. Hạn chế này đã làm cho việc huấn luyện các mô hình lớn trở nên khó khăn mà không hạn chế độ dài chuỗi xuống 8K hoặc dựa vào tích lũy gradient hoặc hệ thống phân tán để mở rộng kích thước batch.

Phương pháp của chúng tôi: Nhận thức những thách thức này, chúng tôi giới thiệu MINI-SEQUENCE TRANSFORMER (MST), một phương pháp đơn giản và hiệu quả để cho phép huấn luyện LLM có hiệu quả cao và chính xác cao với độ dài chuỗi cực dài bằng cách giảm chi phí bộ nhớ trung gian. MST giới thiệu một mini-chuỗi theo từng lớp nơi các phân vùng đầu vào hoạt động cho mỗi khối MLP và LM-Head. MST phân vùng các mẫu riêng lẻ dọc theo chiều chuỗi và xử lý lặp lại từng mini-chuỗi, kết hợp tất cả kết quả mini-chuỗi để khôi phục đầu ra chuỗi đầy đủ cho những khối này. Công trình của chúng tôi cũng áp dụng activation recomputation [8]. Chúng tôi không tìm thấy suy giảm nào về thông lượng hoặc hội tụ ngay cả với chuỗi lên đến 12× so với triển khai tiêu chuẩn của Llama3-8B, như được hiển thị trong Hình 1(c).

Để tóm tắt, chúng tôi đóng góp những điều sau để thúc đẩy huấn luyện chuỗi dài:
• MST huấn luyện độ dài chuỗi dài hơn 12−24× so với các hệ thống hiện có trên một GPU A100 duy nhất mà không có suy giảm thông lượng và hội tụ của huấn luyện.
• Hoàn toàn tổng quát và không phụ thuộc vào triển khai: MST hỗ trợ hầu hết huấn luyện hiệu quả tham số vì nó hoạt động độc lập với các lớp attention.
• Hỗ trợ huấn luyện phân tán quy mô lớn: MST hoạt động cùng với DeepSpeed-Ulysses [21] để hỗ trợ mở rộng tuyến tính độ dài chuỗi theo số lượng GPU.
• Dễ sử dụng và di động, yêu cầu những thay đổi mã tối thiểu cho các khung huấn luyện hiện có như Huggingface [22]. Chi tiết có thể tham khảo Phụ lục G.

Trong các phần tiếp theo, chúng tôi cung cấp nền tảng và công trình liên quan, một cuộc thảo luận chi tiết về thiết kế MINI-SEQUENCE TRANSFORMER (MST), phân tích hiệu quả phần cứng, đánh giá thực nghiệm, và so sánh với công trình hiện có. Công trình này là mã nguồn mở dưới giấy phép MIT tại https://github.com/wdlctc/mini-s.

2

--- TRANG 3 ---
2 Nền tảng và Công trình Liên quan
Phần này tóm tắt ngắn gọn về đặc tính hiệu suất của transformer chuỗi dài trên phần cứng hiện đại (ví dụ: GPU). Chúng tôi cũng mô tả một số nền tảng về huấn luyện mini-batch và tính toán lại kích hoạt, điều này truyền cảm hứng cho công trình của chúng tôi.

2.1 Kiến trúc Transformer
Hình 1(a) là một bản phác thảo về các khối xây dựng của một kiến trúc Transformer điển hình [56]. Nó bao gồm các chuỗi đầu vào S được gửi vào L khối lặp lại với attention và MLP, sau đó tính toán loss đầu ra với khối LM-Head. Đầu vào và đầu ra của mỗi khối thường là một tensor 3D có kích thước (B, S, d) trong đó B là kích thước micro batch, S là độ dài chuỗi, và d là chiều ẩn. Giá trị trung gian bao gồm các tensor Q, K, V có kích thước (B, S, d) trong khối attention, tensor I có kích thước (B, S, I) trong khối MLP, và tensor logits có kích thước (B, S, V) trong khối LM-Head. Ở đây, I đại diện cho kích thước trung gian của MLP, và V đại diện cho kích thước từ vựng.

2.2 Hiệu suất Phần cứng của Huấn luyện Chuỗi Dài
Hệ thống Bộ nhớ. GPU có một hệ thống bộ nhớ với bộ nhớ GPU toàn cầu lớn hơn nhưng chậm hơn (high bandwidth memory; HBM) và bộ nhớ chia sẻ nhỏ hơn nhưng nhanh hơn (SRAM). Nhu cầu bộ nhớ cao của Transformer bắt nguồn từ độ phức tạp bậc hai của các phép toán self-attention, nơi bộ nhớ cần thiết để lưu trữ điểm attention cho mỗi token tăng theo cấp bậc hai khi độ dài chuỗi tăng. Sự gia tăng đáng kể trong nhu cầu bộ nhớ này có thể nhanh chóng vượt quá dung lượng của HBM, dẫn đến các vấn đề OOM. Flashattention [15] sử dụng kernel fusion để giảm thiểu hiệu quả chi phí bộ nhớ liên quan đến sự tăng trưởng bậc hai trong độ dài chuỗi, và Xformer [41] triển khai các mẫu truy cập bộ nhớ được tối ưu hóa đạt được mở rộng bộ nhớ tuyến tính. Công trình của chúng tôi một phần được truyền cảm hứng bởi các công nghệ tối ưu hóa bộ nhớ, nơi các mục tiêu tối ưu hóa của chúng tôi là MLP và LM-Head.

Occupancy. GPU có nhiều thread được thực thi song song; các thread được nhóm thành các thread block, được thực thi trên các streaming multiprocessor (SM). Phần cứng hiện đại có các đơn vị chuyên dụng như tensor core trên NVIDIA GPU để tăng tốc phép toán ma trận. Trong các tình huống huấn luyện chuỗi dài nơi kích thước chuỗi có xu hướng dài (>10k), song song hóa theo chiều chuỗi thường cho phép occupancy GPU cao.

Đặc tính hiệu suất. Các toán tử GPU có thể được phân loại là compute-bound hoặc memory-bound, được xác định bởi thời gian dành cho các phép toán số học và thời gian dành cho truy cập HBM. Self-attention điển hình với chuỗi dài, MLP với kích thước trung gian dài là một toán tử compute-bound vì các toán tử cốt lõi của chúng là phép nhân ma trận với chiều trong lớn của độ dài chuỗi. Sau đó, cross-entropy với reduction là memory-bound.

2.3 Huấn luyện Mini-Batch
Công trình của chúng tôi được truyền cảm hứng bởi các thuật toán Huấn luyện Mini-Batch, còn được gọi là tích lũy gradient. Các thuật toán huấn luyện mini-batch [17,40] có thể hỗ trợ kích thước batch lớn bằng cách xử lý batch huấn luyện trong các mini-batch nhỏ hơn, cho phép mô hình được huấn luyện trên một tập con của dữ liệu tại một thời điểm, tích lũy gradient qua nhiều mini-batch và chỉ cập nhật tham số với gradient tích lũy. Điều này giảm yêu cầu bộ nhớ so với batch gradient descent [25], cho phép huấn luyện kích thước batch lớn hơn so với ràng buộc bộ nhớ GPU. Chúng tôi được truyền cảm hứng bởi ý tưởng này và điều chỉnh nó để huấn luyện chuỗi dài thay vì kích thước batch lớn.

2.4 Tính toán lại Kích hoạt
Tính toán lại kích hoạt [8], còn được gọi là gradient checkpointing, là một kỹ thuật tiết kiệm bộ nhớ để huấn luyện các mạng neural lớn. Phương pháp này đánh đổi tính toán để có bộ nhớ bằng cách loại bỏ các kích hoạt trung gian trong quá trình truyền xuôi và tính toán lại chúng khi cần thiết trong quá trình truyền ngược. Trong huấn luyện tiêu chuẩn, tất cả các kích hoạt phải được lưu trữ để tính toán gradient, có thể dẫn đến việc sử dụng bộ nhớ đáng kể cho các mô hình lớn hoặc chuỗi dài. Tính toán lại kích hoạt trực giao với MST của chúng tôi, và chúng tôi tích hợp phương pháp này để tối ưu hóa tốt hơn giá trị trung gian. Chúng tôi phân tích hiệu quả bộ nhớ của tính toán lại kích hoạt và tích hợp của nó với MST trong Mục 3.2.

3

--- TRANG 4 ---
3 MINI-SEQUENCE TRANSFORMER (MST): Thuật toán, Phân tích, và
Mở rộng Phân tán
Chúng tôi trình bày cơ chế MINI-SEQUENCE TRANSFORMER (MST) để phân vùng chuỗi đầu vào thành M mini-chuỗi. Chúng tôi chỉ ra cách tính toán khối transformer chính xác bằng tích lũy gradient trong quá trình truyền ngược. Sau đó, chúng tôi phân tích hiệu quả bộ nhớ và độ phức tạp IO của nó, cho thấy phương pháp của chúng tôi có hiệu quả bộ nhớ và thông lượng cân bằng so với transformer tiêu chuẩn. Dựa trên phân tích, chúng tôi tìm thấy triển khai tối ưu của MST bằng cách chọn các siêu tham số tốt nhất. Chúng tôi tiếp tục chỉ ra cách MST có thể hoạt động trong các cài đặt phân tán bằng cách tích hợp với DeepSpeed [21].

Chúng tôi tập trung ở đây vào quá trình truyền xuôi để dễ trình bày; Phụ lục B chứa chi tiết cho quá trình truyền ngược.

3.1 Thuật toán: Tối ưu hóa Bộ nhớ Trung gian Với Xử lý Mini-Chuỗi
Ý tưởng của chúng tôi phát sinh từ việc quan sát các giá trị trung gian lớn từ các khối transformer. Cho đầu vào X∈RN×d trong HBM, các khối attention và khối MLP tính toán đầu ra O∈RN×d và khối LM-head tính toán loss đầu ra∈R1, N bằng với kích thước chuỗi S ở đây. Chúng tôi quan sát thấy rằng các giá trị trung gian luôn lớn hơn đầu vào X và đầu ra O,loss, được minh họa trong Bảng 1. Attention có các giá trị trung gian Q,K,V∈RN×d, lớn hơn kích thước đầu vào (1 + 2 ×d)/G lần, trong đó (1 + 2 ×d/G = 1.5) trong cài đặt Llama3. G đề cập đến số lượng grouped query attention (GQA). MLP có giá trị trung gian Iup, Igate∈RN×I, trong đó 2×I/d= 7 trong cài đặt Llama3. LM-Head có logits ∈RV×d, trong đó V/d= 32 trong cài đặt Llama3. Cài đặt chi tiết của Llama3-8B được liệt kê trong Phụ lục C.

Bảng 1: Phân tích kích thước giá trị trung gian cho các khối transformer
Khối Transformer Kích thước Đầu vào/Đầu ra Kích thước Giá trị Trung gian Đỉnh Tỷ lệ Trung gian/Đầu vào1
Attention (B, S, d )/(B, S, d ) (B, S, d ) + 2×(B, S, d/G ) (1 + 2 ×d/G )≈1.5
MLP (B, S, d )/(B, S, d ) 2 ×(B, S, I ) (2 ×I)/d≈7
LM-Head (B, S, d )/1 ( B, S, V ) V/d≈32
1Tỷ lệ trong cài đặt Llama3.

Vì flash attention và group query attention đã giảm thiểu giá trị trung gian của attention, chúng tôi tập trung vào khối MLP và khối LM-Head. Do đó, triển khai MST của chúng tôi đủ tổng quát để hoạt động với bất kỳ attention nào: self-attention [56], cross-attention [5], causal attention [42], các đối tác sparse của chúng [11,59,48], và các kernel được tối ưu hóa khác nhau của chúng như các phiên bản khác nhau của FlashAttention [15, 14]. Triển khai của chúng tôi áp dụng FlashAttention2 [14] cho các thí nghiệm.

Phân vùng Đầu vào. Chúng tôi áp dụng kỹ thuật mini-chuỗi để vượt qua thách thức kỹ thuật của các giá trị trung gian lớn chiếm bộ nhớ HBM. Chúng tôi mô tả điều này trong Thuật toán 1, và 2, đại diện cho các khối MLP và LM-Head từ dòng Llama. Khối MLP của chúng bao gồm ba lớp tuyến tính và hàm SiLU [46], và khối LM-Head của chúng bao gồm một lớp tuyến tính và hàm CrossEntropyLoss [49]. Các triển khai backward tương ứng có thể được tham khảo trong Phụ lục B để biết thêm chi tiết. Ý tưởng chính là phân vùng đầu vào X thành mini-chuỗi Xi như Thuật toán 1 dòng 1 và Thuật toán 2 dòng 1, sau đó tính toán đầu ra đối với những mini-chuỗi đó. Chúng tôi có được kết quả chính xác giống như triển khai tiêu chuẩn bằng cách liên hệ tất cả đầu ra mini-chuỗi.

Tích lũy Gradient. Một trong những mục tiêu của chúng tôi là giảm các giá trị trung gian cho quá trình truyền ngược. Quá trình truyền ngược thường yêu cầu các ma trận X∈RN×d,I∈RN×I,logits ∈RN×V để tính toán gradient đối với trọng số. Tuy nhiên, bằng phân vùng đầu vào X∈RNm×d, chúng tôi có thể giảm giá trị trung gian như I∈RNm×I,logits ∈RNm×V bằng M× trong quá trình truyền ngược trong HBM. Với tích lũy gradient cho tất cả mini-chuỗi, tất cả gradient được tạo ra theo cách giống như triển khai tiêu chuẩn bằng cách giới thiệu thêm thời gian tải bộ nhớ. Tuy nhiên, vì MLP là toán tử compute-bound tiêu chuẩn và LM-Head chỉ chiếm một lượng nhỏ tổng thời gian huấn luyện, MST sẽ không ảnh hưởng đến tốc độ huấn luyện tổng thể với việc giảm đáng kể chi phí bộ nhớ.

4

--- TRANG 5 ---
Thuật toán 1 Mini-Chuỗi MLP
Yêu cầu: Ma trận X∈RN×d, khối MLP, Wdown,∈RI×d,Trọng số của ba lớp tuyến tính
Wgate, Wup∈Rd×I,Wdown∈RI×d
1:Phân vùng ma trận X thành M khối X1, . . . , X m có kích thước Nm×d, trong đó Nm=N/M
2:for1≤i≤Mdo
3: Tính toán O′
i=MLP (Xi, Wgate, Wup, Wdown),Oi∈RNm×d
4:end for
5:Liên hệ O={O′
i, . . . ,O′
m} ∈RN×d
6:Trả về O.

Thuật toán 2 Mini-Chuỗi LM-Head
Yêu cầu: Ma trận X∈RN×d, Nhãn L∈RN, Trọng số Wout∈Rd×V
1:Phân vùng ma trận X thành M khối X1, . . . , X m có kích thước Nm×d, trong đó Nm=N/M
2:Phân vùng nhãn L thành M sub-label, L1, . . . , L m có kích thước Nm, trong đó Nm=N/M
3:for1≤i≤Mdo
4: Tính toán logits i=XiWout,logits i∈RNm×V
5: Tính toán if (i−1)∗Nm≤Li≤(i−1)∗Nm,Li=LielseLi=−100
6: Tính toán lossi=crossentropyloss (logits i, L_)
7:end for
8:Tính toán loss=PM
1lossi/M
9:Trả về loss.

3.2 Phân tích: Hiệu quả Bộ nhớ của MINI-SEQUENCE TRANSFORMER (MST)
Chúng tôi phân tích hiệu quả bộ nhớ của MST. MST có thể giảm giá trị trung gian bằng M× trong khi duy trì cùng hiệu suất thông lượng.

Định lý 1. Cho S là độ dài chuỗi, Wmem là chiếm dụng bộ nhớ trọng số, bao gồm trọng số, gradient, và optimizer. Amem là chiếm dụng bộ nhớ kích hoạt trên mỗi chuỗi, Imem là chiếm dụng bộ nhớ trung gian trên mỗi chuỗi. Bộ nhớ đỉnh của transformer tiêu chuẩn được đạt bởi M=Wmem+S×(Imem+L×Amem). Lưu ý rằng L×Amem>> I mem cho transformer tiêu chuẩn, vì Amem kéo dài cho tất cả L lớp, nhưng Imem chỉ kéo dài cho một lớp.

Định lý 2. Với activation recomputation của OpenAI [39], L×Amem có thể được giảm xuống sqrt(L)×Amem. Do đó bộ nhớ đỉnh được giảm xuống M=Wmem+S×(Imem+sqrt(L)×Amem). Đối với các mô hình có từ vựng lớn và MLP trung gian, sqrt(L)×Amem< Imem.

Định lý 3. MST có thể giảm giá trị trung gian bằng M×, vì vậy chiếm dụng bộ nhớ trở thành M=Wmem+S×(Imem/M+sqrt(L)×A mem). Đối với GPU có bộ nhớ tối đa Mmax, độ dài chuỗi tối đa được chứa bởi Smax=(Mmax−Wmem )
(Imem/M+sqrt (L)×Amem ). Độ dài chuỗi này sẽ dài hơn nhiều so với triển khai tiêu chuẩn với Smax=(Mmax−Wmem )
(Imem +L×Amem ).

3.3 Phân tích: Độ phức tạp IO và Bộ nhớ của MINI-SEQUENCE TRANSFORMER (MST)
Chúng tôi phân tích độ phức tạp IO của MST, so với với độ phức tạp tính toán nhất quán, có thể ảnh hưởng đến đặc tính hiệu suất compute-bound hoặc memory-bound của nó.

Định lý 4. Cho S là độ dài chuỗi, d là chiều ẩn, I là kích thước trung gian, và V là kích thước voice. MLP tiêu chuẩn trả về O=act((XW gate)∗(XiWup))∗Wdown với O(SdI)FLOPS và MST MLP trả về O(SdI/M ∗M) =O(SdI)FLOPS. LM-Loss tiêu chuẩn trả về loss=crossentropyloss (XW, L )với O(SdV +SV)FLOPS, và MST LM-Loss trả về O((SdV +SV)/M∗M) =O(SdV +SV)FLOPS.

Định lý 5. MLP tiêu chuẩn yêu cầu Θ(Sd+SI+dI)truy cập HBM, trong khi MST(1) yêu cầu Θ(Sd+SI+dIM )truy cập HBM. LM-Head tiêu chuẩn yêu cầu Θ(Sd+SV+dV)truy cập HBM, trong khi MST(2) yêu cầu Θ(Sd+SV+dV M )truy cập HBM.

Đối với các giá trị Llama3 của d(4096), I(14336) và V(128256), SI,Sv lớn hơn nhiều lần so với Sd.
Đối với các trường hợp chuỗi dài, độ phức tạp tính toán và độ phức tạp IO được chi phối bởi SI và SV, nơi MST gần với triển khai tiêu chuẩn. Tuy nhiên, đối với các trường hợp chuỗi nhỏ nơi S << d, độ phức tạp tính toán và độ phức tạp IO được chi phối bởi dI và dV trong khi MST cần dIM và dV M. Do đó, MST sẽ gây ra suy giảm thông lượng cho độ dài chuỗi nhỏ.

5

--- TRANG 6 ---
3.4 MINI-SEQUENCE TRANSFORMER (MST) dựa trên Chunk
Chúng tôi trình bày một triển khai tối ưu hóa của MST dựa trên chunk được thiết kế để giảm thiểu suy giảm thông lượng khi huấn luyện với dữ liệu chuỗi nhỏ. Phương pháp cơ bản liên quan đến việc phân vùng chuỗi S thành các chunk có kích thước bằng nhau C (khi có thể), dẫn đến M=S/C mini-chuỗi.

Phân tích độ phức tạp IO của chúng tôi chỉ ra rằng số lượng mini-chuỗi M ảnh hưởng đến các truy cập HBM như Θ(Sd+SI+dIM )và Θ(Sd+SV+dV M ). Tuy nhiên, các truy cập HBM vẫn ổn định tại Θ(SI)và Θ(SV)với điều kiện dIM≤SI và dV M ≤SV. Nó có nghĩa là d≤S/M .
Do đó, bằng cách đặt kích thước chunk thành C=S/M≥d, MST tránh suy giảm thông lượng cho chuỗi nhỏ. Một cách trực quan, khi kích thước chuỗi nhỏ hơn kích thước chunk, MST không chia đầu vào, do đó ngăn chặn bất kỳ mất mát hiệu suất nào.

Chúng tôi áp dụng MST dựa trên chunk chỉ riêng cho các khối MLP bằng cách đặt kích thước chunk không đổi C bằng với chiều ẩn, C=d. Đối với các khối LM-head, chúng tôi duy trì kích thước mini-chuỗi không đổi M=V/d, vì các khối này đóng góp tối thiểu vào tổng thời gian huấn luyện của transformer.

3.5 Mở rộng: MINI-SEQUENCE TRANSFORMER (MST) Phân tán
NormAttention
Deepspeed
UlyssesNormNormLoss Loss 
LM head:
Mini -SeqLM head:
Mini -Seq
MLP:
Mini -SeqMLP:
Mini -Seq
Tensor (S)Tensor (S)L x
Hình 2: MINI-SEQUENCE
TRANSFORMER Phân tán.Chúng tôi mở rộng MINI-SEQUENCE TRANSFORMER (MST) tới cài đặt phân tán: chúng tôi đề xuất MST+ SP, có thể mở rộng hiệu quả transformer sử dụng sequence parallelism(SP). Trong SP, tensor đầu vào của mỗi lớp Transformer được chia dọc theo chiều chuỗi, cho phép tính toán song song trên nhiều GPU. Việc phân đoạn này, kết hợp với tính toán lại kích hoạt, dẫn đến việc giảm đáng kể yêu cầu bộ nhớ kích hoạt. Đáng chú ý là phương pháp đề xuất của chúng tôi trực giao với hầu hết sequence parallelism, như Megatron-LM [26], Deepspeed-Ulysses [21], Sequence parallelism [29], và Ring Attention [30]. Ở đây, chúng tôi lấy Deepspeed-Ulysses làm ví dụ về cách chúng hoạt động cùng nhau.

Hình 2 chỉ ra thiết kế mở rộng MST với DeepSpeed-Ulysses. Như với kiến trúc transformer, thiết kế bao gồm một khối attention với DeepSpeed-Ulysses, MLP, và LM-Head với công nghệ mini-chuỗi của MST. Thiết kế bao gồm chuỗi đầu vào S được phân vùng trên các thiết bị có sẵn và mini-chuỗi. Mỗi khối attention Ma trận Q,K,V được truyền thông qua tất cả collective all-to-all trước và sau tính toán attention. Các module còn lại của MLP và LM-Head sử dụng sequence parallel và mini-chuỗi cùng nhau.
Vì thay đổi chính của DeepSpeed-Ulysses đang hoạt động trên khối attention và MST đang hoạt động trên MLP và LM-Head, việc làm cho chúng hoạt động cùng nhau để mở rộng độ dài chuỗi là đơn giản.

4 Thí nghiệm
Chúng tôi đánh giá tác động của việc sử dụng MINI-SEQUENCE TRANSFORMER (MST) dựa trên chunk trên Llama3 [36], một mô hình tiên tiến cho nhiều tác vụ NLP. Chúng tôi cũng đánh giá Qwen [6], Mistral [24], và Gemma-2 [54] cho việc cải thiện độ dài ngữ cảnh. Chúng tôi xác thực các tuyên bố của mình về mở rộng độ dài chuỗi, báo cáo thời gian huấn luyện, và chi phí bộ nhớ. Kết quả Mở rộng Phân tán có thể được tìm thấy trong phụ lục E, xác nhận rằng độ dài chuỗi của MST có thể mở rộng tuyến tính với số lượng GPU.

• Độ dài Chuỗi Tối đa. MST có thể huấn luyện Llama3-8B với độ dài ngữ cảnh 60k và Llama3-7B với độ dài ngữ cảnh 84k trên một GPU A100 duy nhất, vượt trội hơn triển khai tiêu chuẩn gấp 12×. Ngoài ra, nó đạt được 12−24× so với triển khai tiêu chuẩn của Qwen, Mistral, và Gemma-2.
• Thông lượng huấn luyện. MST duy trì cùng thông lượng huấn luyện so với huấn luyện chuỗi dài tiêu chuẩn. Hơn nữa, thông lượng có thể được cải thiện nhẹ với kích thước batch lớn được hỗ trợ bởi MST.

6

--- TRANG 7 ---
4.1 Độ dài Chuỗi Dài hơn với MINI-SEQUENCE TRANSFORMER (MST)
Llama3 và Llama2. Chúng tôi huấn luyện mô hình Llama3-8B [36] MST và Llama2 [43] MST bằng cách khám phá độ dài chuỗi trên một GPU A100 duy nhất với các chiến lược huấn luyện không mất mát, như tính toán lại kích hoạt, kết hợp phép toán backward với cập nhật optimizer [34] và MST. Bảng 2 so sánh độ dài chuỗi tối đa và thời gian huấn luyện của chúng tôi với triển khai tiêu chuẩn PyTorch và Huggingface PEFT với tính toán lại kích hoạt. Triển khai của chúng tôi huấn luyện chuỗi dài hơn 4× LLAMA-3 so với tính toán lại kích hoạt và chuỗi dài hơn 12× so với triển khai tiêu chuẩn. Ngoài ra, triển khai của chúng tôi huấn luyện chuỗi dài hơn 1.8× so với tính toán lại kích hoạt và chuỗi dài hơn 12× so với triển khai tiêu chuẩn.

Bảng 2: Độ dài chuỗi tối đa của Llama3-8B và Llama2-7B.
Triển khai Llama3-8B-hf Độ dài Chuỗi Tối đa (K)
Llama3-8B-hf vanilla 5
Llama3-8B-hf activation recomputation 14
Llama3-8B-hf MST 60
Llama2-7B-hf vanilla 7
Llama2-7B-hf activation recomputation 45
Llama2-7B-hf MST 84

Qwen, Mistral, và Gemma-2. Chúng tôi đã mở rộng đánh giá của mình để bao gồm Mistral-7B, Qwen2-7B, và Gemma-2-9B, thể hiện sự gia tăng đáng kể trong độ dài chuỗi tối đa (12× cho Mistral-7B, 18× cho Qwen2-7B, 24× cho Gemma-2-9B) trên các kiến trúc này. Trong số những mô hình này, MST cung cấp mở rộng chuỗi tốt nhất cho Gemma-2 là 24×. Quan sát quan trọng ở đây là gemma-2 sử dụng kích thước từ vựng lớn nhất (256k) so với Mistral-7B (32k) và Qwen2(152k).

Bảng 3: Độ dài chuỗi tối đa của các mô hình khác nhau.
Triển khai Mô hình Độ dài Chuỗi Tối đa (K)
Mistral-7B vanilla 5
Mistral-7B activation recomputation 42
Mistral-7B MST 70
Qwen2-7B vanilla 4
Qwen2-7B activation recomputation 13
Qwen2-7B MST 74
gemma-2-9b vanilla 1.5
gemma-2-9b activation recomputation 5
gemma-2-9b MST 36

Kết hợp với tích lũy gradient. Tích lũy Gradient đã được sử dụng trong quá trình huấn luyện Llama2 và Llama3, giúp chúng huấn luyện kích thước batch lớn hơn với bộ nhớ GPU có sẵn hạn chế. Tuy nhiên, trong Tích lũy Gradient, thay vì cập nhật các tham số mô hình sau khi xử lý từng batch dữ liệu huấn luyện, các gradient được tích lũy qua nhiều batch trước khi cập nhật. Điều này có nghĩa là việc sử dụng bộ nhớ cho gradient sẽ chiếm bộ nhớ được sử dụng cho kích hoạt. Do đó, việc sử dụng tích lũy gradient trong quá trình huấn luyện sẽ hạn chế kích thước chuỗi tối đa.

Bảng 4 tóm tắt độ dài chuỗi tối đa với tích lũy gradient. Công nghệ tính toán lại kích hoạt có thể huấn luyện lên đến 8K chuỗi. Sau đó MST có thể huấn luyện lên đến độ dài chuỗi 30k, dài hơn 4× so với tính toán lại kích hoạt, và dài hơn 21× so với vanilla. Đối với Llama2-7B, MST cũng có thể huấn luyện lên đến độ dài chuỗi 55k.

Bảng 4: Độ dài chuỗi tối đa huấn luyện với tích lũy gradient.
Triển khai Mô hình với tích lũy gradient Độ dài Chuỗi Tối đa (K)
Llama3-8B-hf vanilla 1.5
Llama3-8B-hf Activation Recomputation 8
Llama3-8B-hf MST 32
Llama2-7B-hf vanilla 4
Llama2-7B-hf activation recomputation 38
Llama2-7B-hf MST 55

So sánh và Kết hợp với Phương pháp Mất mát. Chúng tôi đã so sánh toàn diện MST với các phương pháp lượng tử hóa và sự kết hợp giữa MST và lượng tử hóa trên Bảng 5. Tất cả các phương pháp mất mát đều là triển khai chính thức của HuggingFace. So sánh này thể hiện sự vượt trội của MST trong việc cho phép chuỗi dài hơn cho huấn luyện Llama3 trên một GPU A100 duy nhất. MST một mình (60K token) vượt trội hơn những phương pháp mất mát này (4bit 28k). Khi kết hợp với các kỹ thuật lượng tử hóa, MST đạt được kết quả ấn tượng hơn nữa: MST+ 8-bit đạt 110K token (cải thiện 22× so với 8-bit tiêu chuẩn), trong khi MST+ 4-bit đẩy ranh giới lên 140K token. Chúng tôi không đánh giá tác động của lượng tử hóa lên loss huấn luyện.

Bảng 5: Độ dài chuỗi tối đa huấn luyện với phương pháp mất mát
Triển khai Llama3 Độ dài Chuỗi Tối đa (K)
8-bit 5
4-bit 10
MST 60
MST + 8-bit 110
MST + 4-bit 140

4.2 Huấn luyện Chuỗi Dài Nhanh hơn với MINI-SEQUENCE TRANSFORMER (MST)
Chúng tôi đánh giá hiệu suất huấn luyện của MST trên Llama3-8B với chuỗi 8k và Llama2-7B với chuỗi 4k sử dụng một GPU A100 80G duy nhất. Bảng 6 so sánh thời gian huấn luyện trên mỗi bước và TFLOPS đạt được bởi MST với triển khai PyTorch vanilla và kỹ thuật tính toán lại kích hoạt.

Bảng 6: Hiệu suất huấn luyện sử dụng MST trên GPU A100 80G duy nhất.
Triển khai Mô hình Kích thước Batch Thời gian Huấn luyện Trên Mỗi Bước (s) TFLOPS
Llama3-8B-hf vanilla 1 OOM OOM
Llama3-8B-hf activation recomputation 2 5.01 3271.42
Llama3-8B-hf MST 2 5.13 3194.90
Llama3-8B-hf MST 8 19.35 3386.13
Llama2-7B-hf vanilla 1 1.24 3290.88
Llama2-7B-hf activation recomputation 1 1.52 2684.67
Llama2-7B-hf MST without activation recomputation 1 1.31 3115.03
Llama2-7B-hf activation recomputation 8 8.85 3703.48
Llama2-7B-hf MST 8 9.33 3511.39
Llama2-7B-hf MST 16 17.92 3656.17

Đối với Llama3-8B, triển khai vanilla hết bộ nhớ (OOM) với kích thước batch là 1. Tính toán lại kích hoạt cho phép huấn luyện với kích thước batch là 2, đạt 3271.42 TFLOPS và thời gian huấn luyện 5.01 giây trên mỗi bước. MST, với cùng kích thước batch là 2, đạt 3194.90 TFLOPS tương đương với thời gian huấn luyện hơi dài hơn là 5.13 giây trên mỗi bước. Tuy nhiên, hiệu quả bộ nhớ của MST cho phép mở rộng kích thước batch lên 8, dẫn đến cải thiện 3386.13 TFLOPS và thời gian huấn luyện 19.35 giây trên mỗi bước.

Trong trường hợp Llama2-7B, triển khai vanilla có thể huấn luyện với kích thước batch là 1, đạt 3290.88 TFLOPS và thời gian huấn luyện 1.24 giây trên mỗi bước. Đối với cùng kích thước batch, MST không có tính toán lại kích hoạt đạt 3115.03 TFLOPS với thời gian huấn luyện 1.31 giây trên mỗi bước, thể hiện tăng tốc 16% so với tính toán lại kích hoạt (2684.67 TFLOPS) và chỉ chậm lại 5% so với PyTorch vanilla. MST tiếp tục tăng kích thước batch lên 16, duy trì 3656.17 TFLOPS tương tự với thời gian huấn luyện 17.92 giây trên mỗi bước.

4.3 Mô hình Tốt hơn với Chuỗi Dài hơn
Mô hình Ngôn ngữ với Ngữ cảnh Dài. Hiệu quả bộ nhớ của MST cho phép chúng tôi tăng độ dài ngữ cảnh của llama gấp 4× so với tính toán lại kích hoạt. Bảng 7 cho thấy rằng huấn luyện Llama3-8B với độ dài ngữ cảnh 30K đạt được cải thiện 2.7× về perplexity so với baseline 8K.
Chúng tôi huấn luyện Llama3-8B [36] MST trên bộ dữ liệu LongAlpaca [9]. Việc huấn luyện kéo dài hai epoch và 10k bước để minh họa. Đối với tất cả triển khai, chúng tôi sử dụng optimizer AdamW [32]. Chúng tôi sử dụng weight decay là 0.001, gradient clipping là 1.0, và learning rate không đổi là 1e-4. Tất cả kích thước batch bằng 16, với bước tích lũy gradient là 16. Độ chính xác bf16 cũng được triển khai.

Bảng 7: LLAMA3-8b với MST, với độ dài ngữ cảnh lớn hơn 4 lần so với tính toán lại kích hoạt.
Triển khai Llama3-8B-hf Độ dài Ngữ cảnh LongAlpaca-12k (ppl) loss Thời gian Huấn luyện
Activation Recomputation 8k 9.34 2.23 25.6 giờ
MST 8k 7.41 2.00 26.5 giờ
MST 16k 3.53 1.26 62.5 giờ
MST 30k 3.45 1.23 233 giờ

8

--- TRANG 8 ---
(a) Llama3-8B với ngữ cảnh 20k.
(b) Gemma2-9B với ngữ cảnh 20k.
Hình 3: Tiêu thụ bộ nhớ của việc tiền huấn luyện mô hình Llama3-8B và Gemma2-9B với kích thước batch là 1 trên một thiết bị A100 duy nhất, với tính toán lại kích hoạt và MST. Lưu ý rằng gradient huấn luyện chuỗi dài chồng lấp với kích hoạt, vì vậy gradient không được hiển thị trong các thanh.

5 Nghiên cứu Ablation:
5.1 Tối ưu hóa Bộ nhớ của MINI-SEQUENCE TRANSFORMER (MST)
MST giới thiệu một loạt tối ưu hóa bộ nhớ để giảm chi phí bộ nhớ của huấn luyện chuỗi dài. Để hiểu hiệu quả của các tối ưu hóa bộ nhớ MST, chúng tôi thực hiện một nghiên cứu ablation tăng dần tắt những tối ưu hóa này (mini-chuỗi, tính toán lại kích hoạt) và đo các yêu cầu bộ nhớ. Chúng tôi xem xét ba tùy chọn: vanilla (PyTorch tiêu chuẩn với BF16), chỉ tính toán lại kích hoạt, và MST với tính toán lại kích hoạt.

Hình 3 cho thấy kết quả. Chúng tôi phân tích việc sử dụng bộ nhớ đỉnh của Llama3-8B và Gemma2-9B, với độ dài chuỗi 20k. Đối với độ dài chuỗi 20k của Llama3-8B và Gemma2-9B, chỉ có MST mới có thể làm cho mô hình vừa với GPU A100. Phần còn lại của việc tiêu thụ bộ nhớ được ước tính dựa trên kiến trúc mô hình và lượng kích hoạt lý thuyết của nó. Đối với Llama3, tính toán lại kích hoạt có thể giảm chi phí bộ nhớ của kích hoạt gấp 3×, và MST có thể giảm thêm 4× chi phí bộ nhớ dựa trên tính toán lại kích hoạt. Đối với Gemma2-9B, MST đạt được chuỗi dài hơn 24× so với vanilla và dài hơn 8× so với tính toán lại kích hoạt. Sự cải thiện này từ 12× lên 24× là do tỷ lệ trung gian/đầu vào cao hơn của Gemma2-9B (8 cho MLP và 72 cho LM head) so với Llama3 (7 cho MLP và 32 cho LM head, như được hiển thị trong Bảng 1). Chi tiết thêm về nghiên cứu ablation bộ nhớ có thể được tìm thấy trong Phụ lục D.

5.2 Cần bao nhiêu mini-chuỗi trong quá trình huấn luyện
Chúng tôi quan sát thấy rằng việc tăng M, số lượng mini-chuỗi, có thể tăng hiệu quả bộ nhớ; tuy nhiên, việc tăng này có một giới hạn nhất định. Cụ thể, việc tăng M cũng có thể ảnh hưởng đến hiệu suất thông lượng. Phụ lục F cung cấp chi tiết về những hạn chế này và tác động của chúng.
Quan sát này cho phép chúng tôi xác định cấu hình tối ưu cho tối ưu hóa bộ nhớ và đạt được sự cân bằng tốt nhất giữa hiệu suất bộ nhớ, nhất quán với phân tích của chúng tôi trong Mục 3.2 và 3.3.

Chúng tôi thấy rằng sự cân bằng tốt nhất cho bộ nhớ và thông lượng đạt được bởi các giá trị tối ưu của C cho MLP dựa trên chunk C=d, M =S/d, trong đó d là kích thước ẩn. Đối với LM-Head, MST ban đầu được sử dụng để tiết kiệm bộ nhớ, và cài đặt tối ưu cho M được xác định bởi M=V/d, cụ thể là 32 cho Llama3 và 64 cho Gemma-2. Giá trị này cung cấp hiệu quả bộ nhớ tốt nhất.

6 Hạn chế và Hướng Tương lai
Chúng tôi thảo luận về các hạn chế và hướng tương lai. Công trình liên quan cũng được đưa ra trong Phụ lục A.

Biên dịch sang CUDA. Các phương pháp hiện tại của chúng tôi được xây dựng trên triển khai Pytorch. Điều này có thể hạn chế hiệu suất và tiết kiệm bộ nhớ cấp thấp. Nó có thể được cải thiện bằng fused kernel và tối ưu hóa cuda, có thể là bước tiếp theo của chúng tôi.

Kết hợp với tối ưu hóa bộ nhớ. Mục tiêu của chúng tôi là tăng độ dài chuỗi trong khi duy trì hiệu suất và độ chính xác. Nới lỏng những yêu cầu này, MsT có thể được kết hợp với activation offload để mở rộng độ dài chuỗi như Smax=(Mmax−Wmem )
(Imem/M+Amem ), hoặc với lượng tử hóa để mở rộng độ dài chuỗi như Smax=bf16
4bit/8bit(Mmax−Wmem )
(Imem/M+L×Amem ). Sự kết hợp này có thể được khám phá trong nghiên cứu tương lai.

9

--- TRANG 9 ---
Lời cảm ơn
Chúng tôi cảm ơn Vast AI đã cho thuê tài nguyên tính toán.
A. Anandkumar được hỗ trợ bởi chức danh giáo sư Bren, học bổng senior Schmidt AI 2050, ONR (tài trợ MURI N00014-18-12624).

Tài liệu tham khảo
[1]Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany
Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Báo cáo kỹ thuật Phi-3: Một mô hình ngôn ngữ có khả năng cao trên điện thoại của bạn. arXiv preprint arXiv:2404.14219,
2024.
[2]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and
Sumit Sanghai. Gqa: Huấn luyện các mô hình transformer multi-query tổng quát từ các checkpoint multi-head. arXiv preprint arXiv:2305.13245, 2023.
[3] Apsod et al. Flashce. https://github.com/Apsod/FlashCE, 2023.
[4]Apsod et al. hướng dẫn optimizer step in backward. https://pytorch.org/tutorials/
intermediate/optimizer_step_in_backward_tutorial.html, 2024.
[5]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Dịch máy neural bằng cách học kết hợp để căn chỉnh và dịch. arXiv preprint arXiv:1409.0473, 2014.
[6]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
Yu Han, Fei Huang, et al. Báo cáo kỹ thuật Qwen. arXiv preprint arXiv:2309.16609, 2023.
[7]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: Transformer tài liệu dài.
arXiv preprint arXiv:2004.05150, 2020.
[8]Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Huấn luyện mạng sâu với chi phí bộ nhớ dưới tuyến tính. arXiv preprint arXiv:1604.06174, 2016.
[9]Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya
Jia. Longlora: Tinh chỉnh hiệu quả các mô hình ngôn ngữ lớn ngữ cảnh dài. arXiv preprint
arXiv:2309.12307, 2023.
[10] Zonglei Chen, Minbo Ma, Tianrui Li, Hongjun Wang, and Chongshou Li. Dự báo chuỗi thời gian dài với deep learning: Một khảo sát. Information Fusion, 97:101819, 2023.
[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Tạo chuỗi dài với sparse transformer. arXiv preprint arXiv:1904.10509, 2019.
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Mở rộng mô hình ngôn ngữ với pathways. Journal of Machine Learning Research, 24(240):1–
113, 2023.
[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-xl: Các mô hình ngôn ngữ chú ý vượt ra ngoài ngữ cảnh có độ dài cố định. arXiv preprint
arXiv:1901.02860, 2019.
[14] Tri Dao. Flashattention-2: Attention nhanh hơn với song song và phân vùng công việc tốt hơn. arXiv
preprint arXiv:2307.08691, 2023.
[15] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Attention chính xác nhanh và hiệu quả bộ nhớ với nhận thức io. Advances in Neural Information Processing
Systems, 35:16344–16359, 2022.
[16] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Tinh chỉnh hiệu quả của llm lượng tử hóa. Advances in Neural Information Processing Systems, 36, 2024.
[17] Joeri R Hermans, Gerasimos Spanakis, and Rico Möckel. Chuẩn hóa gradient tích lũy.
Trong Asian Conference on Machine Learning, pages 439–454. PMLR, 2017.
[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Điều chỉnh hạng thấp của các mô hình ngôn ngữ lớn. arXiv
preprint arXiv:2106.09685, 2021.

10

--- TRANG 10 ---
[19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Huấn luyện hiệu quả các mạng neural khổng lồ sử dụng song song pipeline. Advances in neural information processing systems,
32, 2019.
[20] Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, and Pedro Moreno Mengibar.
Transformerfam: Feedback attention là bộ nhớ làm việc. arXiv preprint arXiv:2404.09173,
2024.
[21] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam
Rajbhandari, and Yuxiong He. Deepspeed ulysses: Tối ưu hóa hệ thống để cho phép huấn luyện các mô hình transformer chuỗi cực dài. arXiv preprint arXiv:2309.14509, 2023.
[22] Shashank Mohan Jain. Hugging face. Trong Introduction to transformers for NLP: With the hugging
face library and models to solve problems, pages 51–67. Springer, 2022.
[23] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio
César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al.
Phi-2: Sức mạnh đáng ngạc nhiên của các mô hình ngôn ngữ nhỏ. Microsoft Research Blog, 2023.
[24] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[25] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Mini-batch gradient descent:
Hội tụ nhanh hơn dưới sự thưa thớt dữ liệu. Trong 2017 IEEE 56th Annual Conference on Decision and
Control (CDC), pages 2880–2887. IEEE, 2017.
[26] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Giảm tính toán lại kích hoạt trong các mô hình transformer lớn. Proceedings of Machine Learning and Systems, 5, 2023.
[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Phân loại imagenet với mạng neural tích chập sâu. Communications of the ACM, 60(6):84–90, 2017.
[28] Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir
Radev. Booksum: Một bộ sưu tập các bộ dữ liệu cho tóm tắt tường thuật dài. arXiv
preprint arXiv:2105.08209, 2021.
[29] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Huấn luyện chuỗi dài từ góc độ hệ thống. arXiv preprint arXiv:2105.13120,
2021.
[30] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention với blockwise transformer cho ngữ cảnh gần như vô hạn. arXiv preprint arXiv:2310.01889, 2023.
[31] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan,
Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Các mô hình ngôn ngữ lớn là người học y tế few-shot. arXiv preprint arXiv:2305.15525, 2023.
[32] Ilya Loshchilov and Frank Hutter. Điều chỉnh phân rã trọng số tách rời. arXiv preprint
arXiv:1711.05101, 2017.
[33] Lefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos Malakasiotis. Edgarcorpus: Hàng tỷ token làm cho thế giới quay. arXiv preprint arXiv:2109.14394, 2021.
[34] Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. Adalomo: Tối ưu hóa bộ nhớ thấp với tỷ lệ học thích ứng. arXiv preprint arXiv:2310.10195, 2023.
[35] Mohammad Malek et al. Efficient cross entropy. https://github.com/mgmalek/
efficient_cross_entropy, 2023.
[36] Meta. Giới thiệu meta llama 3: LLM mở có khả năng nhất cho đến nay. https:
//ai.meta.com/blog/meta-llama-3/.
[37] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary,
Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro,
et al. Huấn luyện mô hình ngôn ngữ quy mô lớn hiệu quả trên các cluster gpu sử dụng megatron-lm. Trong
Proceedings of the International Conference for High Performance Computing, Networking,
Storage and Analysis, pages 1–15, 2021.

11

--- TRANG 11 ---
[38] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover.
Climax: Một mô hình nền tảng cho thời tiết và khí hậu. arXiv preprint arXiv:2301.10343, 2023.
[39] OpenAI. gradient-checkpointing. https://github.com/cybertronai/
gradient-checkpointing, 2018.
[40] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Mở rộng dịch máy neural.
arXiv preprint arXiv:1806.00187, 2018.
[41] Markus N Rabe and Charles Staats. Self-attention không cần bộ nhớ o (n ˆ2). arXiv preprint
arXiv:2112.05682, 2021.
[42] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Cải thiện hiểu biết ngôn ngữ bằng tiền huấn luyện sinh tạo. preprint, 2018.
[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Các mô hình ngôn ngữ là người học đa nhiệm không giám sát. OpenAI blog, 1(8):9, 2019.
[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Khám phá giới hạn của transfer learning với một transformer text-to-text thống nhất. Journal of machine learning research, 21(140):1–67, 2020.
[45] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zeroinfinity: Phá vỡ rào cản bộ nhớ gpu cho deep learning quy mô cực lớn. Trong Proceedings of the
international conference for high performance computing, networking, storage and analysis,
pages 1–14, 2021.
[46] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Tìm kiếm hàm kích hoạt. arXiv
preprint arXiv:1710.05941, 2017.
[47] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.
Gemini 1.5: Mở khóa hiểu biết đa phương thức trên hàng triệu token ngữ cảnh. arXiv
preprint arXiv:2403.05530, 2024.
[48] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Attention sparse hiệu quả dựa trên nội dung với routing transformer. Transactions of the Association for Computational
Linguistics, 9:53–68, 2021.
[49] Reuven Rubinstein. Phương pháp cross-entropy cho tối ưu hóa tổ hợp và liên tục.
Methodology and computing in applied probability, 1:127–190, 1999.
[50] Noam Shazeer. Giải mã transformer nhanh: Một write-head là tất cả những gì bạn cần. arXiv preprint
arXiv:1911.02150, 2019.
[51] Noam Shazeer and Mitchell Stern. Adafactor: Tỷ lệ học thích ứng với chi phí bộ nhớ dưới tuyến tính. Trong International Conference on Machine Learning, pages 4596–4604. PMLR, 2018.
[52] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher
Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. S-lora: Phục vụ hàng nghìn adapter lora đồng thời. arXiv preprint arXiv:2311.03285, 2023.
[53] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
Catanzaro. Megatron-lm: Huấn luyện các mô hình ngôn ngữ multi-billion tham số sử dụng song song mô hình. arXiv preprint arXiv:1909.08053, 2019.
[54] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Các mô hình mở dựa trên nghiên cứu và công nghệ gemini. arXiv preprint arXiv:2403.08295, 2024.
[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Các mô hình nền tảng mở và chat được tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023.
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention là tất cả những gì bạn cần. Advances in neural information
processing systems, 30, 2017.
[57] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Các mô hình ngôn ngữ streaming hiệu quả với attention sink. arXiv preprint arXiv:2309.17453, 2023.

12

--- TRANG 12 ---
[58] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis
Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Mở rộng ngữ cảnh dài hiệu quả của các mô hình nền tảng. arXiv preprint arXiv:2309.16039, 2023.
[59] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformer cho chuỗi dài hơn. Advances in neural information processing systems, 33:17283–17297,
2020.
[60] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong
Tian. Galore: Huấn luyện llm hiệu quả bộ nhớ bằng phép chiếu gradient hạng thấp. arXiv preprint
arXiv:2403.03507, 2024.
[61] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,
Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: kinh nghiệm về mở rộng song song dữ liệu fully sharded. arXiv preprint arXiv:2304.11277, 2023.
[62] Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco
Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, et al. Genslms:
Các mô hình ngôn ngữ quy mô genome tiết lộ động lực tiến hóa sars-cov-2. The International
Journal of High Performance Computing Applications, 37(6):683–705, 2023.

A Công trình Liên quan
Mô hình Chuỗi Dài. Khả năng huấn luyện các mô hình lớn với chuỗi dài đang trở nên ngày càng quan trọng trong các lĩnh vực khác nhau, từ AI sinh tạo đến khám phá khoa học. Trong AI sinh tạo, các tác vụ như AI đối thoại, tóm tắt tài liệu dài giàu kiến thức, và tạo video đòi hỏi lý luận qua các ngữ cảnh mở rộng trong cả chiều không gian và thời gian. Các mô hình nền tảng đa phương thức xử lý speech, hình ảnh, và dạng sóng đồng thời, yêu cầu lý luận ngữ cảnh dài qua các đầu vào chiều cao với chuỗi dài. Tương tự, tóm tắt cấp chương và sách, ước tính liên quan đến hàng chục đến hàng trăm nghìn từ, có tầm quan trọng lớn trong AI đối thoại và các tác vụ tóm tắt trừu tượng [7,28,33] và đã thể hiện lợi ích từ huấn luyện chuỗi dài [58, 47, 36].

Sự xuất hiện của ChatGPT và các mô hình ngôn ngữ lớn mã nguồn mở và thương mại tiếp theo đã đẩy các ứng dụng chat lên hàng đầu của AI hiện đại, làm cho chúng trở nên liên quan hơn bao giờ hết. Xử lý hiệu quả chuỗi dài là quan trọng để hỗ trợ lịch sử hội thoại rộng hơn trong những ứng dụng này [55]. Khả năng chuỗi dài cũng quan trọng cho AI trong các lĩnh vực khoa học, cho phép hiểu biết và tiến bộ tốt hơn trong chăm sóc sức khỏe [31], dự báo khí hậu và thời tiết [38], và mô phỏng phân tử quy mô lớn [62].

Huấn luyện Chuỗi Dài Mất mát. Một hướng là làm cho LLM có thể xử lý chuỗi dài tùy ý một cách hiệu quả bằng cách hy sinh cửa sổ nhận thức của mạng. Sliding window attention được giới thiệu [13] để xử lý chuỗi dài vô hạn như đầu vào. Tuy nhiên, nó bỏ qua thông tin ngoài trường tiếp nhận hiệu quả. Longformer [7] mở rộng ý tưởng này, cache theo từng khối, làm tăng kích thước cửa sổ nhận thức, cũng như TransformerFAM [20]. StreamLLM [57] không bị hạn chế bởi một cửa sổ cho trước mà có chọn lọc bỏ qua thông tin giữa token đầu tiên và các cửa sổ cho trước. Chúng gặp khó khăn trong việc nắm bắt các phụ thuộc dài hạn ngoài phạm vi cố định này, nhưng chất lượng xấp xỉ cũng dường như suy giảm ở độ dài chuỗi dài. MST có thể làm việc trực tiếp với chúng để tăng kích thước cửa sổ cho chất lượng tốt hơn.

Huấn luyện Hiệu quả Bộ nhớ. Khi nhu cầu xử lý chuỗi dài tiếp tục tăng trưởng trong các lĩnh vực khác nhau, việc phát triển các phương pháp hiệu quả để huấn luyện các mô hình lớn với ngữ cảnh mở rộng trở nên ngày càng thiết yếu để thúc đẩy trạng thái hiện tại của AI. Các phương pháp tinh chỉnh hiệu quả tham số (PEFT) nhằm giảm dấu chân bộ nhớ và tính toán liên quan đến tham số và gradient. Adafactor [51] đạt được chi phí bộ nhớ dưới tuyến tính bằng cách phân tích thống kê bậc hai sử dụng tích ngoài hàng-cột. Low-Rank Adaptation (LoRA) [18] giảm dấu chân bộ nhớ của các mô hình tiền huấn luyện sử dụng các bộ điều chỉnh hạng thấp với một bộ điều chỉnh trọng số hạng thấp cho mỗi lớp. Một số biến thể của LoRA đã được đề xuất để tăng hiệu suất của nó. [52, 60]. Lượng tử hóa là một kỹ thuật khác được sử dụng rộng rãi trong PEFT để giảm chi phí bộ nhớ của các trạng thái optimizer [16].

13

--- TRANG 13 ---
Các kỹ thuật khác tập trung vào việc giảm dấu chân bộ nhớ và tính toán liên quan đến kích hoạt. Tích lũy gradient là một phương pháp cho phép huấn luyện kích thước batch lớn hơn bằng cách tích lũy gradient qua nhiều mini-batch trước khi thực hiện một bước tối ưu [40]. Phương pháp này cho phép huấn luyện với kích thước batch hiệu quả lớn hơn trong khi duy trì kích thước batch vật lý nhỏ hơn, giảm yêu cầu bộ nhớ kích hoạt. Một công trình liên quan của activation offloading [45] di chuyển các kích hoạt được checkpoint sang CPU không đồng bộ và prefetch các kích hoạt đã offload trở lại từ CPU trong quá trình backward. Có các công trình liên quan trong sparse Transformer, chủ yếu tập trung vào xấp xỉ full-attention, như sparse attention [11,59]. Các công trình gần đây cũng tập trung vào bộ nhớ và tính toán hiệu quả attention GPU đơn. Một ví dụ phổ biến trong danh mục này là Flash attention [15], tận dụng các kỹ thuật đã biết như tiling và recomputation cho hiệu quả tính toán và bộ nhớ. Ngoài ra, một số công trình đặt quan tâm vào cross-entropy. FlashCE [3] tối ưu hóa cross-entropy bằng cách tận dụng cấu trúc dữ liệu sparse và tối ưu hóa CUDA để tăng tốc độ và hiệu quả bộ nhớ. Efficient cross-entropy [35] giới thiệu một biến thể hiệu quả bộ nhớ của cross-entropy loss để giảm bộ nhớ kích hoạt bằng cách chỉ lưu trữ các tính toán thiết yếu. Những công trình này trực giao với công trình của chúng tôi và có thể được tận dụng tương ứng để cải thiện thêm hiệu quả của các mô hình dựa trên Transformer.

Huấn luyện Phân tán. Các kỹ thuật huấn luyện phân tán đã trở nên thiết yếu để huấn luyện các mô hình ngôn ngữ lớn (LLM) do yêu cầu tính toán và bộ nhớ khổng lồ của chúng. Bằng cách chia khối lượng công việc trên nhiều GPU, những phương pháp này giúp giảm nhẹ nút thắt cổ chai bộ nhớ và cho phép huấn luyện các mô hình mà nếu không sẽ không khả thi trên một thiết bị duy nhất. Data parallelism [27] nhân bản mô hình trên nhiều thiết bị, xử lý các batch dữ liệu khác nhau song song và đồng bộ hóa gradient trên các GPU. Tensor parallelism [53] chia các lớp riêng lẻ của mô hình trên các GPU, cho phép sử dụng bộ nhớ hiệu quả hơn cho các mô hình cực lớn. Một phương pháp nổi bật khác là fully sharded data parallelism (FSDP) [61], mở rộng phương pháp song song dữ liệu bằng cách chia cả tham số mô hình và trạng thái optimizer trên các thiết bị, do đó giảm thêm chi phí bộ nhớ. Sequence parallelism [26,21,29,30] chuyên về tối ưu hóa sử dụng bộ nhớ cho các mô hình dựa trên transformer bằng cách phân vùng chuỗi trên các GPU và giảm bộ nhớ kích hoạt. Ngoài những điều này, pipeline parallelism [19] chia mô hình thành các đoạn, với mỗi đoạn được gán cho một GPU khác nhau, và xử lý dữ liệu theo kiểu pipeline, cải thiện hiệu quả bằng cách chồng lấp tính toán và truyền thông. Hybrid parallelism [37] kết hợp data, tensor, và pipeline parallelism để tối đa hóa việc sử dụng tài nguyên tùy thuộc vào kiến trúc của mô hình và phần cứng có sẵn.

B Chi tiết Thuật toán
Chúng tôi mô tả chi tiết đầy đủ của quá trình backward MINI-SEQUENCE TRANSFORMER (MST). Thuật toán 3 cho thấy MLP backward, và Thuật toán 4 cho thấy LM-Head backward.

Thuật toán 3 Mini-Chuỗi MLP Backward
Yêu cầu: Gradient của đầu ra ∇O∈RN×d, Ma trận X∈RN×d, Trọng số của ba lớp tuyến tính
Wgate, Wup∈Rd×I,Wdown∈RI×d
1:Phân vùng ma trận X thành M khối X1, . . . , X m có kích thước Nm×d, trong đó Nm=N/M
2:Phân vùng ma trận ∇O thành M khối ∇O1, . . . ,∇Om có kích thước Nm×d, trong đó Nm=N/M
3:for1≤i≤Mdo
4: Tính toán ∇Xi=∇MLP (∇Oi)
5: Tính toán ∇Wdown,∇Wup,∇Wgate+ =∇MLP gradient (∇Oi, Xi)
6:end for
7:Nối ∇X=∇X1, . . . ,∇Xm∈RN×d
8:Trả về ∇X,∇Wgate,∇Wup,∇Wdown .

Bây giờ chúng tôi quan sát về quá trình backward MST rằng khi tính toán gradient của MLP và LM-Head, chúng tôi không cần sử dụng dữ liệu đầu vào và trung gian đầy đủ. Thay vào đó, chúng tôi có thể sử dụng dữ liệu giảm 1/M với mini-chuỗi, giảm đáng kể chi phí bộ nhớ giá trị trung gian.

Ý tưởng chính của backward là tích lũy gradient ∇W được tạo ra từ mỗi mini-chuỗi Xi như Thuật toán 3 dòng 5 và Thuật toán 4 dòng 8. Chúng tôi có được kết quả chính xác giống như triển khai tiêu chuẩn bằng cách tích lũy tất cả gradient mini-chuỗi.

14

--- TRANG 14 ---
Thuật toán 4 Mini-Chuỗi LM-Head Backward
Yêu cầu: Gradient loss ∇loss∈R1, Logits ∈RN×V, Nhãn L∈RN, Trọng số Wout∈Rd×V
1:Phân vùng ma trận X thành M khối X1, . . . , X m có kích thước Nm×d, trong đó Nm=N/M
2:Phân vùng nhãn L thành M sub-label L1, . . . , L m có kích thước Nm, trong đó Nm=N/M
3:Tính toán lại Kích hoạt với backward
4:for1≤i≤Mdo
5: Tính toán logits i=XiWout,logits i∈RNm×V
6: Tính toán ∇logits i=CrossEntropyLossBackward (Logits i, Li)
7: Tính toán ∇Xi=∇logits iWTout,∇Xi∈RNm×d
8: Tính toán ∇Wout+ =XTi∇logits i
9: Tính toán ∇Xi=∇Xi⊙ ∇loss
10: Tính toán ∇Wout=∇Wout⊙ ∇loss
11:end for
12:Nối ∇X=∇X1, . . . ,∇Xm∈RN×d
13:Trả về ∇X,∇Wout.

C So sánh Kiến trúc Mô hình Llama2 và Llama3
Phụ lục này làm nổi bật những khác biệt kiến trúc chính giữa các mô hình Llama2-7B và Llama3-8B được triển khai bởi Hugging Face. Sự khác biệt chính nằm ở cấu hình của các khối MLP và LM-Head (tuyến tính với cross-entropy loss) trong kiến trúc mô hình.

NormAttentionNormNormLoss Loss 
LM headLM head
MLPMLP
Tensor (S)Tensor (S)L x
X:N×d I1=XW gate:N×I I2=XW up:N×I O:N×dX:N×d Logit=XW lmhead:N×V Loss: N×1 LM head 
 MLP 
IntermediateIntermediate

Hình 4: Kiến trúc Transformer tiêu chuẩn với sự nổi bật của LM-head và MLP.

Hình 4 minh họa kiến trúc của một mô hình Transformer tiêu chuẩn, tập trung vào các thành phần MLP và LM-head. Các tensor trung gian (I1, I2) có chiều lớn hơn đầu vào và đầu ra. Ngoài ra, tensor logit có chiều từ vựng (V) lớn hơn nhiều so với chiều ẩn (d).

Hình 5: Kiến trúc Mô hình của Triển khai HuggingFace của Llama2-7B
LlamaForCausalLM (
( model ): LlamaModel (
( embed_tokens ): Embedding (32000 , 4096)
( layers ): ModuleList (
(0 -31) : 32 x LlamaDecoderLayer (
( self_attn ): LlamaFlashAttention2 (
( q_proj ): Linear ( in_features =4096 , out_features =4096 , bias = False )
( k_proj ): Linear ( in_features =4096 , out_features =4096 , bias = False )
( v_proj ): Linear ( in_features =4096 , out_features =4096 , bias = False )
( o_proj ): Linear ( in_features =4096 , out_features =4096 , bias = False )
( rotary_emb ): LlamaRotaryEmbedding ()
)
( mlp ): LlamaMLP (
( gate_proj ): Linear ( in_features =4096 , out_features =11008 , bias = False )
( up_proj ): Linear ( in_features =4096 , out_features =11008 , bias = False )
( down_proj ): Linear ( in_features =11008 , out_features =4096 , bias = False )
( act_fn ): SiLU ()
)
( input_layernorm ): LlamaRMSNorm ((4096 ,) , eps =1e -05)
( post_attention_layernorm ): LlamaRMSNorm ((4096 ,) , eps =1e -05)

15

--- TRANG 15 ---
)
)
( norm ): LlamaRMSNorm ((4096 ,) , eps =1e -05)
( rotary_emb ): LlamaRotaryEmbedding ()
)
( lm_head ): Linear ( in_features =4096 , out_features =32000 , bias = False )
)

Kiến trúc Mô hình Llama2-7B: Như được hiển thị trong Hình 5, mô hình Llama2-7B sử dụng cấu hình khối MLP với các đặc điểm sau:
• Khối MLP: Lớp tuyến tính đầu tiên chiếu đầu vào từ kích thước ẩn 4096 lên kích thước trung gian 11008. Lớp tuyến tính thứ hai chiếu biểu diễn trung gian từ 11008 về 4096, kích thước ẩn.
• LM-Head (Chiếu Đầu ra với Loss Tuyến tính): LM-Head trong Llama2-7B bao gồm một lớp tuyến tính chiếu biểu diễn ẩn từ kích thước 4096 lên kích thước từ vựng 32000. Đầu ra của lớp tuyến tính sau đó được truyền qua hàm cross-entropy loss để tính toán loss huấn luyện.

Hình 6: Kiến trúc Mô hình của Triển khai HuggingFace của Llama3-8B.
LlamaForCausalLM (
( model ): LlamaModel (
( embed_tokens ): Embedding (128256 , 4096)
( layers ): ModuleList (
(0 -31) : 32 x LlamaDecoderLayer (
( self_attn ): LlamaFlashAttention2 (
( q_proj ): Linear ( in_features =4096 , out_features =4096 , bias = False )
( k_proj ): Linear ( in_features =4096 , out_features =1024 , bias = False )
( v_proj ): Linear ( in_features =4096 , out_features =1024 , bias = False )
( o_proj ): Linear ( in_features =4096 , out_features =4096 , bias = False )
( rotary_emb ): LlamaRotaryEmbedding ()
)
( mlp ): LlamaMLP (
( gate_proj ): Linear ( in_features =4096 , out_features =14336 , bias = False )
( up_proj ): Linear ( in_features =4096 , out_features =14336 , bias = False )
( down_proj ): Linear ( in_features =14336 , out_features =4096 , bias = False )
( act_fn ): SiLU ()
)
( input_layernorm ): LlamaRMSNorm ((4096 ,) , eps =1e -05)
( post_attention_layernorm ): LlamaRMSNorm ((4096 ,) , eps =1e -05)
)
)
( norm ): LlamaRMSNorm ((4096 ,) , eps =1e -05)
( rotary_emb ): LlamaRotaryEmbedding ()
)
( lm_head ): Linear ( in_features =4096 , out_features =128256 , bias = False )
)

Kiến trúc Mô hình Llama3-8B: Như được hiển thị trong Hình 6, mô hình Llama3-8B sử dụng cấu hình khối MLP với các đặc điểm sau:
• Khối MLP: Lớp tuyến tính đầu tiên chiếu đầu vào từ kích thước ẩn 4096 lên kích thước trung gian lớn hơn là 13824. Lớp tuyến tính thứ hai sau đó chiếu biểu diễn trung gian từ kích thước 13824 trở lại kích thước ẩn 4096.
• LM-Head (Tuyến tính với Cross-Entropy Loss): Trong Llama3-8B, LM-Head cũng bao gồm một lớp tuyến tính, nhưng nó chiếu biểu diễn ẩn từ kích thước 4096 lên kích thước từ vựng lớn hơn là 32000. Giống như Llama2-7B, đầu ra của lớp tuyến tính được truyền qua hàm cross-entropy loss để tính toán loss huấn luyện.

Kích thước trung gian tăng trong khối MLP của Llama3-8B cho phép mô hình nắm bắt các mẫu và biến đổi phức tạp hiệu quả hơn. Ngoài ra, kích thước từ vựng lớn hơn trong LM-HEAD của Llama3-8B cho phép mô hình tạo ra đầu ra đa dạng và tinh tế hơn.

Đáng chú ý rằng trong khi các chiều của khối MLP và LM-Head khác nhau giữa Llama2-7B và Llama3-8B, cấu trúc tổng thể và chức năng của những thành phần này vẫn giống nhau. Hàm cross-entropy loss trong LM-Head đo sự khác biệt giữa xác suất từ dự đoán và từ mục tiêu trong quá trình huấn luyện, hướng dẫn mô hình tạo ra đầu ra chính xác và phù hợp với ngữ cảnh hơn. Những khác biệt kiến trúc này đóng góp vào hiệu suất và khả năng nâng cao của Llama3-8B so với người tiền nhiệm của nó, Llama2-7B, trong khi duy trì cấu trúc tổng thể nhất quán. Tuy nhiên, giá trị trung gian lớn bất ngờ cũng đến từ thay đổi của các khối MLP và LM-HEAD (tuyến tính với cross-entropy loss), điều mà chúng tôi sẽ thảo luận trong phần tiếp theo.

16

--- TRANG 16 ---
Hơn nữa, xu hướng này cũng được phản ánh trong việc phát triển Phi-3 [1] của Microsoft so với Phi-2 [23]. Kích thước từ vựng tăng từ 50k lên 100K (2×), kích thước trung gian tăng từ 10k lên 16k (1.6×), và kích thước ẩn tăng nhẹ từ 2560 lên 3072 (1.2×).

Tóm lại, những mô hình này chia sẻ một xu hướng rõ ràng: tỷ lệ giữa kích thước trung gian và ẩn (cũng như từ vựng và ẩn) đang trở nên lớn hơn.

ParametersActivations GradsOptimizers
StateOptimizers
IntermediatesForward Backward optimizer Forwardoptimizer Backward

Hình 7: Trực quan hóa Bộ nhớ của việc huấn luyện Llama3 8B với độ dài chuỗi 4k trên A100-80G.

D Chi tiết Tối ưu hóa Bộ nhớ của MINI-SEQUENCE TRANSFORMER
Chúng tôi so sánh MINI-SEQUENCE TRANSFORMER (MST) với việc chụp và trực quan hóa ảnh chụp nhanh bộ nhớ. Chúng tôi lấy huấn luyện Pytorch vanilla cho Llama3-8B với độ dài chuỗi 4k làm ví dụ để chỉ ra cách bộ nhớ thay đổi theo thời gian.

vanilla. Hình 7 cho thấy ví dụ vanilla. Các tham số mô hình đã được tải vào bộ nhớ trước bước huấn luyện, vì vậy chúng ta ngay lập tức thấy một phần bộ nhớ dành cho trọng số. Đối với Llama3-8B, trọng số của nó sẽ là 15GB. Khi chúng ta bắt đầu quá trình truyền xuôi, bộ nhớ được cấp phát dần dần cho các kích hoạt hoặc các tensor mà chúng ta đang lưu để có thể tính toán gradient trong quá trình truyền ngược. Ở đây, bộ nhớ được cấp phát cho kích hoạt lớn hơn trọng số, khoảng 29GB. Một khi chúng ta bắt đầu quá trình truyền ngược, các kích hoạt được giải phóng dần dần trong khi bộ nhớ của gradient bắt đầu tích tụ. Vì gradient bằng kích thước của trọng số, nhỏ hơn kích hoạt, chúng ta có thể quan sát việc sử dụng bộ nhớ giảm xuống khoảng 30 GB. Cuối cùng, khi optimizer khởi động, trạng thái của nó sẽ được khởi tạo lười biếng, vì vậy chúng ta nên thấy bộ nhớ trạng thái optimizer tăng dần trong bước optimizer của vòng lặp huấn luyện đầu tiên. Trong các vòng lặp tương lai, bộ nhớ optimizer sẽ vẫn còn và được cập nhật. Bộ nhớ cho gradient sau đó được giải phóng tương ứng ở cuối mỗi vòng lặp huấn luyện khi nó được gọi zero grade. Ở đây, optimizer sẽ chiếm 2× trọng số khi sử dụng Adam với 30GB, và optimizer trung gian bằng kích thước trọng số với 15. Do đó, việc sử dụng bộ nhớ đỉnh là trong bước optimizer, bằng tổng kích thước của trọng số, gradient, trạng thái optimizer, và trung gian optimizer, tương đương khoảng 5× kích thước trọng số với 75GB như được hiển thị trong Bảng 8.

Bảng 8: Chi phí bộ nhớ của việc huấn luyện Llama3-8B trên GPU A100 80G đơn.
Chi phí Bộ nhớ Llama3-8B-hf Trong Bộ nhớ Đỉnh
Activation 29 0
Weight 15 15
Gradient 15 15
Optimizer 45 45
Total - 75

optimizer-in-backward. Tối ưu hóa bộ nhớ đầu tiên được thảo luận ở đây là optimizer-in-backward [4]. Nó kết hợp trạng thái optimizer với quá trình truyền ngược để tiết kiệm bộ nhớ của gradient và optimizer trung gian. Trực quan hóa bộ nhớ của optimizer-in-backward được hiển thị trong Hình 8, nơi không có giai đoạn optimizer mà chỉ có trạng thái forward và backward. Thời gian backward sẽ trở nên lớn hơn do kết hợp. Sử dụng công nghệ này, bộ nhớ đỉnh sẽ thay đổi thành tổng của trọng số, trạng thái optimizer, và kích hoạt. Mặc dù chúng ta thành công tiết kiệm 30GB chi phí bộ nhớ của gradient và optimizer trung gian, nó cộng thêm 29GB chi phí bộ nhớ của kích hoạt, chỉ tiết kiệm 1GB bộ nhớ. Tổng cộng nó tiêu thụ 74GB bộ nhớ, như được hiển thị trong Bảng 9. Nó sẽ tệ hơn nếu độ dài chuỗi tăng, đưa thêm kích hoạt vào huấn luyện LLM. Do đó, optimizer-in-backward khó có thể mang lợi ích cho huấn luyện chuỗi dài, nhưng nó đơn giản hóa quá trình huấn luyện, vì vậy chúng tôi sẽ bao gồm kỹ thuật này trong thảo luận tiếp theo.

Bảng 9: Chi phí bộ nhớ của việc huấn luyện Llama3-8B trên GPU A100 80G đơn. Kỹ thuật optimizer in Backward được triển khai.
Chi phí Bộ nhớ Llama3-8B-hf Trong Bộ nhớ Đỉnh
Activation 29 29
Weight 15 15
Gradient 15 0
Optimizer 30 30
Total - 74

ParametersActivationsOptimizer stateForward Backward Forward Backward
Intermediate V alue

Hình 9: Trực quan hóa Bộ nhớ của việc huấn luyện Llama3 8B với độ dài chuỗi 4k trên A100-80G.
Kỹ thuật Activation Recomputation được triển khai ở đây

Tính toán lại Kích hoạt. Tính toán lại Kích hoạt là một kỹ thuật mạnh mẽ được sử dụng trong phân tích của chúng tôi. Nó có thể giảm đáng kể kích hoạt với chi phí giảm nhỏ tốc độ huấn luyện do tính toán lại các phần của đồ thị trong quá trình back-propagation. Như được hiển thị trong hình 9, nó thành công giảm tổng chi phí bộ nhớ từ 74GB xuống 52GB và giảm chi phí bộ nhớ kích hoạt từ 29GB xuống 7GB với tiết kiệm bộ nhớ 4×. Tuy nhiên, chúng ta có thể dễ dàng tìm thấy nhiều điểm đơn lẻ trong đồ thị, xuất hiện như một tín hiệu xung. Thời lượng của tín hiệu xung này rất ngắn gọn, có nghĩa là nó là dữ liệu trung gian được tạo ra ngắn gọn trong quá trình truyền xuôi/ngược và ngay lập tức bị loại bỏ khỏi bộ nhớ HBM GPU. Dữ liệu trung gian nổi bật nhất là nhiều lần tổng kích hoạt (4-5 lần trong phân tích dữ liệu của chúng tôi). Những dữ liệu trung gian này ảnh hưởng nghiêm trọng đến hiệu suất huấn luyện của chuỗi dài và trở thành nút thắt cổ chai kích hoạt của hàng.

18

--- TRANG 17 ---
Bảng 10: Chi phí bộ nhớ của việc huấn luyện Llama3-8B trên GPU A100 80G đơn. Kỹ thuật Activation Recomputation được triển khai.
Chi phí Bộ nhớ Llama3-8B-hf Trong Bộ nhớ Đỉnh
Activation 7 7
Weight 15 15
Gradient 15 0
Optimizer 30 30
Total - 52

ParametersActivationsOptimizer stateForward Backward Forward Backward
Intermediate V alue

Hình 10: Trực quan hóa Bộ nhớ của việc huấn luyện Llama3 8B với độ dài chuỗi 4k trên A100-80G.
Kỹ thuật MINI-SEQUENCE TRANSFORMER được triển khai ở đây

MINI-SEQUENCE TRANSFORMER (MST) Được truyền cảm hứng từ quan sát từ kỹ thuật Activation Recomputation, chúng tôi đề xuất MST để giảm giá trị trung gian trong quá trình huấn luyện. Chúng tôi thành công giảm chi phí bộ nhớ của kích hoạt từ 7GB xuống 4GB, trong khi giá trị trung gian được giảm đáng kể. Điều này là do chỉ có 1/M giá trị trung gian được sử dụng để tính toán đầu ra forward, lỗi backward, và gradient trong cả quá trình forward và backward.

Bảng 11: Chi phí bộ nhớ của việc huấn luyện Llama3-8B trên GPU A100 80G đơn. Kỹ thuật MINI-SEQUENCE TRANSFORMER được triển khai.
Chi phí Bộ nhớ Llama3-8B-hf Trong Bộ nhớ Đỉnh
Activation 2 2
Weight 15 15
Gradient 15 0
Optimizer 30 30
Total - 48

Kết luận. chúng tôi so sánh việc sử dụng bộ nhớ theo thời gian khi huấn luyện mô hình Llama3-8B sử dụng kiến trúc transformer tiêu chuẩn so với sử dụng MST, được hiển thị trong 11.

Trong Hình 11(a), hiển thị dòng thời gian bộ nhớ cho huấn luyện Llama3-8B tiêu chuẩn, chúng ta có thể thấy rằng việc sử dụng bộ nhớ được chi phối bởi ba thành phần chính: trọng số mô hình (màu xanh), trạng thái optimizer (màu xanh lá), và bộ nhớ trung gian (được làm nổi bật bởi vòng tròn đỏ). Việc sử dụng bộ nhớ đỉnh đạt khoảng 67GB.

19

--- TRANG 18 ---
(a) Dòng thời gian Bộ nhớ Transformer 
Thông thường60
50
40
30
20
10
0
TimeMemory Cost(GB)
60
50
40
30
10
0
TimeMemory Cost(GB)
Weight WeightOptimizer OptimizerIntermediateIntermediatePeak Memory: 67GB Peak Memory: 48GB(-30%)LLAMA 3 Đề xuất LLAMA 3 Tiêu chuẩn
(b) Dòng thời gian Bộ nhớ Mini -Sequence Transformer20

Hình 11: Trực quan hóa Bộ nhớ. (a) Dòng thời gian bộ nhớ của việc huấn luyện Llama3-8B sử dụng kiến trúc transformer tiêu chuẩn, Vòng tròn đỏ làm nổi bật bộ nhớ trung gian (b) Dòng thời gian bộ nhớ của việc huấn luyện Llama3-8B sử dụng MST, Vòng tròn đỏ làm nổi bật bộ nhớ trung gian đã được thu hẹp.

Ngược lại, Hình 11(b) thể hiện dòng thời gian bộ nhớ khi huấn luyện Llama3-8B với MST. Sự khác biệt quan trọng là bộ nhớ trung gian, một lần nữa được làm nổi bật bởi vòng tròn đỏ, đã được giảm đáng kể hoặc "thu hẹp" so với trường hợp transformer tiêu chuẩn. Kết quả là việc sử dụng bộ nhớ đỉnh với MST là khoảng 47GB, đạt được giảm 30% so với transformer tiêu chuẩn.

Các dòng thời gian bộ nhớ minh họa rằng MST hiệu quả giảm dấu chân bộ nhớ trung gian trong quá trình huấn luyện, đóng góp đáng kể vào tiêu thụ bộ nhớ tổng thể. Bằng cách giảm thiểu bộ nhớ trung gian, MST cho phép sử dụng bộ nhớ hiệu quả hơn và cho phép huấn luyện với độ dài chuỗi dài hơn hoặc kích thước batch lớn hơn trong khi ở trong giới hạn bộ nhớ có sẵn của phần cứng.

E Mở rộng đến Chuỗi Cực Dài trên Cài đặt Phân tán
Chúng tôi đánh giá phần mở rộng phân tán của MST trên các mô hình Llama3-8B Llama2-7B và so sánh với sequence parallelism DeepSpeed-Ulysses vanilla trên 2,4,8 GPU, tương ứng, cho độ dài chuỗi tối đa và thời gian huấn luyện tương ứng. Kết quả của đánh giá này được hiển thị trong Bảng 12.

Bảng 12: Độ dài chuỗi tối đa của Llama3-8B, chạy trên cài đặt phân tán.
Triển khai Mô hình Số lượng GPU
2 4 8
Llama3-8b-hf MST 120 240 480
Llama2-7B-hf MST 160 320 640

F Cần bao nhiêu mini-chuỗi trong quá trình tiền huấn luyện
Chúng tôi cũng cung cấp cái nhìn sâu sắc về đặc tính hiệu suất và sử dụng bộ nhớ của mô hình Llama3-8B khi sử dụng phương pháp MINI-SEQUENCE TRANSFORMER (MST) với số lượng mini-chuỗi (M) và độ dài chuỗi khác nhau.

Bảng 13 cho thấy thời gian thực thi cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau. Khi số lượng mini-chuỗi (M) tăng, thời gian thực thi tăng nhẹ, đặc biệt là cho các chuỗi ngắn hơn. Tuy nhiên, đối với các chuỗi dài hơn (ví dụ: 80000), thời gian thực thi vẫn tương đối ổn định qua các mini-chuỗi khác nhau (M).

20

--- TRANG 19 ---
Bảng 13: Thời gian LM-head cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau
Thời gian LM-head 1024 2048 4096 8192 20000 40000 80000
standard 0.01 0.02 0.04 0.09 0.2 0.4 0.85
M=2 0.02 0.04 0.07 0.14 0.31 0.67 1.36
M=4 0.03 0.04 0.07 0.14 0.33 0.67 1.33
M=8 0.04 0.05 0.08 0.14 0.34 0.67 1.34
M=16 0.06 0.07 0.09 0.16 0.34 0.68 1.35
M=32 0.11 0.11 0.14 0.19 0.37 0.69 1.36

Bảng 14 cho thấy việc sử dụng bộ nhớ tính bằng gigabyte (GB) cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau cho thành phần LM-Head. Cài đặt tiêu chuẩn không có mini-chuỗi tiêu thụ 59.92 GB cho độ dài chuỗi 80000. Bằng cách tăng số lượng mini-chuỗi, việc sử dụng bộ nhớ giảm đáng kể. Với M=16, việc sử dụng bộ nhớ giảm xuống 9.12 GB cho cùng độ dài chuỗi, đạt được giảm 84.8% trong tiêu thụ bộ nhớ. Điều này được cải thiện thêm với M=32 lên 89.8%.

Bảng 14: Sử dụng bộ nhớ LM-head (tính bằng GB) cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau
Bộ nhớ LM-head 1024 2048 4096 8192 20000 40000 80000
standard 3.20 3.46 4.94 7.91 16.46 30.95 59.92
mini-seq=2 2.59 3.21 4.46 6.95 14.14 26.31 50.66
mini-seq=4 2.28 2.60 3.24 4.52 8.20 14.44 26.92
mini-seq=8 2.13 2.30 2.63 3.30 5.24 8.51 15.05
mini-seq=16 2.06 2.15 2.33 2.70 4.14 5.54 9.12
mini-seq=32 2.02 2.07 2.18 2.39 3.01 4.06 6.15

Bảng 15 cho thấy thời gian thực thi cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau. Giống như LM-Head, việc tăng M dẫn đến thời gian thực thi hơi dài hơn, đặc biệt là cho các chuỗi ngắn hơn. Tuy nhiên, tác động lên thời gian thực thi là tối thiểu đối với các chuỗi dài hơn (ví dụ: 80000).

Bảng 15: Thời gian MLP (tính bằng giây) cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau
Thời gian MLP 1024 2048 4096 8192 20000 40000 80000
standard 0.05 0.08 0.16 0.31 0.74 1.52 2.96
M=2 0.05 0.10 0.17 0.32 0.76 1.49 3.05
M=4 0.07 0.11 0.19 0.33 0.79 1.52 2.99
M=8 0.12 0.15 0.22 0.38 0.81 1.58 3.05

Đối với thành phần MLP, Bảng 16 thể hiện việc sử dụng bộ nhớ cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau. Cài đặt tiêu chuẩn tiêu thụ 14.72 GB cho độ dài chuỗi 80000, trong khi sử dụng M=8 mini-chuỗi giảm việc sử dụng bộ nhớ xuống 11.66 GB, dẫn đến giảm 20.8%. Ở đây, chúng ta có thể quan sát.

Phân tích cho thấy rằng việc tăng số lượng mini-chuỗi (M) có thể giảm đáng kể việc sử dụng bộ nhớ, đặc biệt là cho thành phần LM-Head, trong khi có tác động tối thiểu lên thời gian thực thi cho các chuỗi dài hơn. Tiết kiệm bộ nhớ nổi bật hơn cho LM-Head so với MLP.

Điều quan trọng cần lưu ý là trong khi MST rất có lợi cho huấn luyện với các chuỗi cực dài, nó có thể dẫn đến suy giảm hiệu suất khi áp dụng cho các mô hình với chuỗi ngắn hơn do chi phí được giới thiệu bởi việc phân vùng đầu vào và chuyển động bộ nhớ bổ sung cần thiết cho tích lũy gradient. Có thể dễ dàng quan sát từ Bảng 13 rằng việc sử dụng M=32 mini-chuỗi tăng thời gian thực thi từ 0.01s (cài đặt tiêu chuẩn) lên 0.11s cho độ dài chuỗi 1024, gây ra suy giảm hiệu suất 11x. Ngoài ra, từ Bảng 15, việc sử dụng M=8 mini-chuỗi tăng thời gian thực thi từ 0.05s (cài đặt tiêu chuẩn) lên 0.12s cho độ dài chuỗi 1024, gây ra suy giảm hiệu suất 2x. Suy giảm hiệu suất nổi bật hơn cho LM-Head so với MLP. May mắn thay, LM-head chiếm rất ít thời gian chạy của transformer, nhỏ hơn MLP và nhỏ hơn nhiều so với attention, vì vậy công nghệ của chúng tôi sẽ không ảnh hưởng đến hiệu suất tổng thể, ngay cả khi nó ảnh hưởng đến hiệu suất module của nó.

21

--- TRANG 20 ---
Bảng 16: Sử dụng bộ nhớ MLP (tính bằng GB) cho các độ dài chuỗi và cài đặt mini-chuỗi khác nhau
Bộ nhớ MLP 1024 2048 4096 8192 20000 40000 80000
standard 0.93 1.09 1.39 2.11 4.18 7.69 14.72
M=2 1.29 1.36 1.50 2.00 3.76 6.73 12.69
M=4 1.32 1.41 1.61 2.00 3.49 6.21 11.66
M=8 1.33 1.44 1.66 2.11 3.42 6.17 11.66

G Tích hợp với các khung hiện có
Ý tưởng cốt lõi của MST về mặt khái niệm là đơn giản, chủ yếu nhắm mục tiêu các khối MLP và LM-Head. Chúng tôi cung cấp hai phương pháp tích hợp:

Hugging Face Transformer Tùy chỉnh. Phương pháp này liên quan đến việc sửa đổi trực tiếp thư viện Hugging Face Transformer để tích hợp chức năng MST. Bằng cách tùy chỉnh thư viện, người dùng có thể tích hợp MST một cách liền mạch vào quy trình làm việc hiện có của họ sử dụng Hugging Face Transformers. Chúng tôi đã làm cho phương pháp này mã nguồn mở tại https://github.com/wdlctc/transformers.

Để sử dụng Hugging Face Transformer tùy chỉnh với MST, nhà phát triển ML không cần thay đổi bất kỳ dòng nào mà chỉ cài đặt thư viện transformer tùy chỉnh của chúng tôi với MST:
import transformers

Chế độ Wrapper. Chế độ Wrapper cung cấp một phương pháp ít xâm lấn hơn để tích hợp MST. Phương pháp này liên quan đến việc tạo một wrapper xung quanh các triển khai mô hình hiện có, chặn và sửa đổi quá trình truyền xuôi và ngược của các khối MLP và LM-Head. Chúng tôi đã làm cho phương pháp này mã nguồn mở tại https://github.com/wdlctc/mini-s.

Để sử dụng Chế độ Wrapper:
from mini-s import mst
model = mst(model)

Kết luận. Cả hai phương pháp tích hợp đều cung cấp tính linh hoạt trong việc áp dụng MST cho huấn luyện chuỗi dài. Lựa chọn giữa Hugging Face Transformer Tùy chỉnh và Chế độ Wrapper phụ thuộc vào yêu cầu cụ thể của dự án, mức độ tích hợp mong muốn, và sự sẵn sàng duy trì các thư viện tùy chỉnh.

Đối với người dùng đầu tư sâu vào hệ sinh thái Hugging Face, phương pháp Hugging Face Transformer Tùy chỉnh có thể được ưa chuộng hơn. Phương pháp này yêu cầu những thay đổi tối thiểu cho các codebase hiện có đã được tích hợp với hệ sinh thái Hugging Face, và cho phép truy cập tất cả các tính năng và tối ưu hóa của Hugging Face. Đối với những người tìm kiếm giải pháp linh hoạt hơn hoặc làm việc với nhiều triển khai mô hình, Chế độ Wrapper có thể là lựa chọn tốt hơn để sử dụng với các thách thức codebase tùy chỉnh.

22

--- TRANG 21 ---
Danh sách Kiểm tra Bài báo NeurIPS
Danh sách kiểm tra được thiết kế để khuyến khích các thực hành tốt nhất cho nghiên cứu machine learning có trách nhiệm, giải quyết các vấn đề về khả năng tái tạo, minh bạch, đạo đức nghiên cứu, và tác động xã hội. Không xóa danh sách kiểm tra: Các bài báo không bao gồm danh sách kiểm tra sẽ bị từ chối tại bàn. Danh sách kiểm tra nên theo sau tài liệu tham khảo và theo sau tài liệu bổ sung (tùy chọn). Danh sách kiểm tra KHÔNG tính vào giới hạn trang.

Vui lòng đọc hướng dẫn danh sách kiểm tra cẩn thận để biết thông tin về cách trả lời những câu hỏi này. Đối với mỗi câu hỏi trong danh sách kiểm tra:
• Bạn nên trả lời [Yes], [No], hoặc [NA].
• [NA] có nghĩa là câu hỏi Không Áp dụng cho bài báo cụ thể đó hoặc thông tin liên quan Không Có sẵn.
• Vui lòng cung cấp lời biện minh ngắn (1–2 câu) ngay sau câu trả lời của bạn (ngay cả đối với NA).

Các câu trả lời danh sách kiểm tra là một phần không thể thiếu của việc nộp bài báo của bạn. Chúng hiển thị với các reviewer, area chair, senior area chair, và ethics reviewer. Bạn sẽ được yêu cầu cũng bao gồm nó (sau các sửa đổi cuối cùng) với phiên bản cuối cùng của bài báo, và phiên bản cuối cùng của nó sẽ được xuất bản cùng với bài báo.

Các reviewer của bài báo bạn sẽ được yêu cầu sử dụng danh sách kiểm tra như một trong những yếu tố trong đánh giá của họ. Trong khi "[Yes]" thường được ưa chuộng hơn "[No]", việc trả lời "[No]" hoàn toàn có thể chấp nhận được với điều kiện cung cấp lời biện minh phù hợp (ví dụ: "error bar không được báo cáo vì sẽ quá tốn kém về mặt tính toán" hoặc "chúng tôi không thể tìm thấy giấy phép cho bộ dữ liệu mà chúng tôi đã sử dụng"). Nói chung, việc trả lời "[No]" hoặc "[NA]" không phải là lý do để từ chối. Trong khi các câu hỏi được diễn đạt theo cách nhị phân, chúng tôi thừa nhận rằng câu trả lời thực sự thường tinh tế hơn, vì vậy vui lòng chỉ sử dụng phán đoán tốt nhất của bạn và viết lời biện minh để làm rõ. Tất cả bằng chứng hỗ trợ có thể xuất hiện trong bài báo chính hoặc tài liệu bổ sung, được cung cấp trong phụ lục. Nếu bạn trả lời [Yes] cho một câu hỏi, trong lời biện minh vui lòng chỉ ra (các) phần nơi tài liệu liên quan cho câu hỏi có thể được tìm thấy.

QUAN TRỌNG, vui lòng:
• Xóa khối hướng dẫn này, nhưng giữ tiêu đề phần "Danh sách kiểm tra bài báo NeurIPS",
• Giữ tiêu đề phần con danh sách kiểm tra, câu hỏi/câu trả lời và hướng dẫn dưới đây.
• Không sửa đổi các câu hỏi và chỉ sử dụng các macro được cung cấp cho câu trả lời của bạn.

1.Tuyên bố
Câu hỏi: Các tuyên bố chính được đưa ra trong tóm tắt và giới thiệu có phản ánh chính xác đóng góp và phạm vi của bài báo không?
Trả lời: [Yes]
Biện minh: Mục 1
Hướng dẫn:
• Câu trả lời NA có nghĩa là tóm tắt và giới thiệu không bao gồm các tuyên bố được đưa ra trong bài báo.
• Tóm tắt và/hoặc giới thiệu nên nêu rõ các tuyên bố được đưa ra, bao gồm các đóng góp được thực hiện trong bài báo và các giả định và hạn chế quan trọng. Câu trả lời No hoặc NA cho câu hỏi này sẽ không được reviewer đánh giá tốt.
• Các tuyên bố được đưa ra nên phù hợp với kết quả lý thuyết và thực nghiệm, và phản ánh mức độ kết quả có thể được mong đợi tổng quát hóa cho các cài đặt khác.
• Việc bao gồm các mục tiêu tham vọng như động lực là tốt miễn là rõ ràng rằng những mục tiêu này không được đạt được bởi bài báo.

2.Hạn chế
Câu hỏi: Bài báo có thảo luận về các hạn chế của công việc được thực hiện bởi các tác giả không?
Trả lời: [Yes]
Biện minh: Mục 6
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không có hạn chế trong khi câu trả lời No có nghĩa là bài báo có hạn chế, nhưng những hạn chế đó không được thảo luận trong bài báo.

23

--- TRANG 22 ---
• Các tác giả được khuyến khích tạo một phần "Hạn chế" riêng biệt trong bài báo của họ.
• Bài báo nên chỉ ra bất kỳ giả định mạnh nào và mức độ mạnh mẽ của kết quả đối với việc vi phạm những giả định này (ví dụ: giả định độc lập, cài đặt không nhiễu, đặc tả mô hình tốt, xấp xỉ tiệm cận chỉ giữ cục bộ). Các tác giả nên suy ngẫm về cách những giả định này có thể bị vi phạm trong thực tế và hậu quả sẽ là gì.
• Các tác giả nên suy ngẫm về phạm vi của các tuyên bố được đưa ra, ví dụ: nếu phương pháp chỉ được kiểm tra trên vài bộ dữ liệu hoặc với vài lần chạy. Nói chung, kết quả thực nghiệm thường phụ thuộc vào các giả định ngầm, cần được phát biểu.
• Các tác giả nên suy ngẫm về các yếu tố ảnh hưởng đến hiệu suất của phương pháp. Ví dụ: một thuật toán nhận dạng khuôn mặt có thể hoạt động kém khi độ phân giải hình ảnh thấp hoặc hình ảnh được chụp trong ánh sáng yếu. Hoặc một hệ thống speech-to-text có thể không được sử dụng đáng tin cậy để cung cấp phụ đề đóng cho các bài giảng trực tuyến vì nó thất bại trong việc xử lý thuật ngữ kỹ thuật.
• Các tác giả nên thảo luận về hiệu quả tính toán của các thuật toán được đề xuất và cách chúng mở rộng với kích thước bộ dữ liệu.
• Nếu áp dụng, các tác giả nên thảo luận về các hạn chế có thể có của phương pháp của họ để giải quyết các vấn đề về quyền riêng tư và công bằng.
• Trong khi các tác giả có thể lo sợ rằng sự trung thực hoàn toàn về hạn chế có thể được các reviewer sử dụng làm lý do để từ chối, một kết quả tệ hơn có thể là các reviewer phát hiện ra những hạn chế không được thừa nhận trong bài báo. Các tác giả nên sử dụng phán đoán tốt nhất của họ và nhận ra rằng các hành động cá nhân ủng hộ minh bạch đóng vai trò quan trọng trong việc phát triển các chuẩn mực bảo tồn tính toàn vẹn của cộng đồng. Các reviewer sẽ được hướng dẫn cụ thể không phạt sự trung thực về hạn chế.

3.Giả định Lý thuyết và Chứng minh
Câu hỏi: Đối với mỗi kết quả lý thuyết, bài báo có cung cấp tập hợp đầy đủ các giả định và một chứng minh hoàn chỉnh (và chính xác) không?
Trả lời: [Yes]
Biện minh: Mục 3.3
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm kết quả lý thuyết.
• Tất cả các định lý, công thức, và chứng minh trong bài báo nên được đánh số và tham chiếu chéo.
• Tất cả giả định nên được nêu rõ hoặc tham chiếu trong phát biểu của bất kỳ định lý nào.
• Các chứng minh có thể xuất hiện trong bài báo chính hoặc tài liệu bổ sung, nhưng nếu chúng xuất hiện trong tài liệu bổ sung, các tác giả được khuyến khích cung cấp một bản phác thảo chứng minh ngắn để cung cấp trực giác.
• Ngược lại, bất kỳ chứng minh không chính thức nào được cung cấp trong phần cốt lõi của bài báo nên được bổ sung bằng chứng minh chính thức được cung cấp trong phụ lục hoặc tài liệu bổ sung.
• Các định lý và Bổ đề mà chứng minh dựa vào nên được tham chiếu đúng cách.

4.Khả năng Tái tạo Kết quả Thực nghiệm
Câu hỏi: Bài báo có tiết lộ đầy đủ tất cả thông tin cần thiết để tái tạo các kết quả thực nghiệm chính của bài báo đến mức nó ảnh hưởng đến các tuyên bố chính và/hoặc kết luận của bài báo (bất kể mã và dữ liệu có được cung cấp hay không) không?
Trả lời: [Yes]
Biện minh: Phụ lục D
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
• Nếu bài báo bao gồm thí nghiệm, câu trả lời No cho câu hỏi này sẽ không được các reviewer đánh giá tốt: Làm cho bài báo có thể tái tạo là quan trọng, bất kể mã và dữ liệu có được cung cấp hay không.
• Nếu đóng góp là một bộ dữ liệu và/hoặc mô hình, các tác giả nên mô tả các bước được thực hiện để làm cho kết quả của họ có thể tái tạo hoặc có thể xác minh.

24

--- TRANG 23 ---
• Tùy thuộc vào đóng góp, khả năng tái tạo có thể được thực hiện theo nhiều cách khác nhau. Ví dụ: nếu đóng góp là một kiến trúc mới, việc mô tả kiến trúc đầy đủ có thể đủ, hoặc nếu đóng góp là một mô hình cụ thể và đánh giá thực nghiệm, có thể cần thiết phải làm cho người khác có thể sao chép mô hình với cùng bộ dữ liệu, hoặc cung cấp quyền truy cập vào mô hình. Nói chung, việc phát hành mã và dữ liệu thường là một cách tốt để thực hiện điều này, nhưng khả năng tái tạo cũng có thể được cung cấp thông qua hướng dẫn chi tiết về cách sao chép kết quả, truy cập vào một mô hình được host (ví dụ: trong trường hợp của một mô hình ngôn ngữ lớn), phát hành checkpoint mô hình, hoặc các phương tiện khác phù hợp với nghiên cứu được thực hiện.
• Trong khi NeurIPS không yêu cầu phát hành mã, hội nghị yêu cầu tất cả bài nộp cung cấp một số con đường hợp lý cho khả năng tái tạo, có thể phụ thuộc vào bản chất của đóng góp. Ví dụ:
(a)Nếu đóng góp chủ yếu là một thuật toán mới, bài báo nên làm rõ cách tái tạo thuật toán đó.
(b)Nếu đóng góp chủ yếu là một kiến trúc mô hình mới, bài báo nên mô tả kiến trúc rõ ràng và đầy đủ.
(c)Nếu đóng góp là một mô hình mới (ví dụ: một mô hình ngôn ngữ lớn), thì nên có cách truy cập mô hình này để tái tạo kết quả hoặc cách tái tạo mô hình (ví dụ: với bộ dữ liệu mã nguồn mở hoặc hướng dẫn về cách xây dựng bộ dữ liệu).
(d)Chúng tôi nhận ra rằng khả năng tái tạo có thể khó khăn trong một số trường hợp, trong trường hợp đó các tác giả được chào đón để mô tả cách cụ thể họ cung cấp cho khả năng tái tạo. Trong trường hợp các mô hình closed-source, có thể là quyền truy cập vào mô hình bị hạn chế theo một cách nào đó (ví dụ: cho người dùng đã đăng ký), nhưng nó nên có thể cho các nhà nghiên cứu khác có một số con đường để tái tạo hoặc xác minh kết quả.

5.Truy cập mở đến dữ liệu và mã
Câu hỏi: Bài báo có cung cấp truy cập mở đến dữ liệu và mã, với hướng dẫn đủ để tái tạo trung thực các kết quả thực nghiệm chính, như được mô tả trong tài liệu bổ sung không?
Trả lời: [Yes]
Biện minh: tài liệu cung cấp
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm yêu cầu mã.
• Vui lòng xem hướng dẫn nộp mã và dữ liệu NeurIPS (https://nips.cc/public/guides/CodeSubmissionPolicy) để biết thêm chi tiết.
• Trong khi chúng tôi khuyến khích việc phát hành mã và dữ liệu, chúng tôi hiểu rằng điều này có thể không khả thi, vì vậy "No" là câu trả lời có thể chấp nhận được. Các bài báo không thể bị từ chối đơn giản vì không bao gồm mã, trừ khi điều này là trung tâm của đóng góp (ví dụ: cho một benchmark mã nguồn mở mới).
• Các hướng dẫn nên chứa lệnh chính xác và môi trường cần thiết để chạy để tái tạo kết quả. Xem hướng dẫn nộp mã và dữ liệu NeurIPS (https://nips.cc/public/guides/CodeSubmissionPolicy) để biết thêm chi tiết.
• Các tác giả nên cung cấp hướng dẫn về truy cập và chuẩn bị dữ liệu, bao gồm cách truy cập dữ liệu thô, dữ liệu được xử lý trước, dữ liệu trung gian, và dữ liệu được tạo, v.v.
• Các tác giả nên cung cấp script để tái tạo tất cả kết quả thực nghiệm cho phương pháp mới được đề xuất và baseline. Nếu chỉ một tập con thí nghiệm có thể tái tạo, họ nên nêu những thí nghiệm nào bị bỏ qua khỏi script và tại sao.
• Tại thời điểm nộp, để bảo tồn tính ẩn danh, các tác giả nên phát hành các phiên bản ẩn danh (nếu áp dụng).
• Cung cấp càng nhiều thông tin càng tốt trong tài liệu bổ sung (được thêm vào bài báo) được khuyến khích, nhưng bao gồm URL đến dữ liệu và mã được cho phép.

6.Cài đặt/Chi tiết Thực nghiệm
Câu hỏi: Bài báo có chỉ định tất cả chi tiết huấn luyện và kiểm tra (ví dụ: phân chia dữ liệu, siêu tham số, cách chúng được chọn, loại optimizer, v.v.) cần thiết để hiểu kết quả không?
Trả lời: [Yes]
Biện minh: Mục 4
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
• Cài đặt thực nghiệm nên được trình bày trong phần cốt lõi của bài báo ở mức độ chi tiết cần thiết để đánh giá kết quả và hiểu được chúng.
• Chi tiết đầy đủ có thể được cung cấp với mã, trong phụ lục, hoặc như tài liệu bổ sung.

25

--- TRANG 24 ---
7.Ý nghĩa Thống kê Thí nghiệm
Câu hỏi: Bài báo có báo cáo error bar được định nghĩa phù hợp và chính xác hoặc thông tin phù hợp khác về ý nghĩa thống kê của các thí nghiệm không?
Trả lời: [No]
Biện minh: Error bar không được báo cáo vì sẽ quá tốn kém về mặt tính toán.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
• Các tác giả nên trả lời "Yes" nếu kết quả được đi kèm bởi error bar, khoảng tin cậy, hoặc kiểm định ý nghĩa thống kê, ít nhất cho các thí nghiệm hỗ trợ các tuyên bố chính của bài báo.
• Các yếu tố biến thiên mà error bar đang nắm bắt nên được nêu rõ (ví dụ: phân chia train/test, khởi tạo, vẽ ngẫu nhiên một số tham số, hoặc chạy tổng thể với các điều kiện thực nghiệm cho trước).
• Phương pháp tính toán error bar nên được giải thích (công thức đóng, gọi hàm thư viện, bootstrap, v.v.)
• Các giả định được đưa ra nên được đưa ra (ví dụ: lỗi phân phối Normally).
• Nó nên rõ ràng liệu error bar là độ lệch chuẩn hay sai số chuẩn của trung bình.
• Việc báo cáo error bar 1-sigma là OK, nhưng người ta nên nêu rõ. Các tác giả nên ưu tiên báo cáo error bar 2-sigma hơn nêu rằng họ có CI 96%, nếu giả thuyết về tính Bình thường của lỗi không được xác minh.
• Đối với phân phối bất đối xứng, các tác giả nên cẩn thận không hiển thị trong bảng hoặc hình error bar đối xứng sẽ cho kết quả ngoài phạm vi (ví dụ: tỷ lệ lỗi âm).
• Nếu error bar được báo cáo trong bảng hoặc biểu đồ, Các tác giả nên giải thích trong văn bản cách chúng được tính toán và tham chiếu các hình hoặc bảng tương ứng trong văn bản.

8.Tài nguyên Tính toán Thí nghiệm
Câu hỏi: Đối với mỗi thí nghiệm, bài báo có cung cấp thông tin đủ về tài nguyên máy tính (loại compute worker, bộ nhớ, thời gian thực thi) cần thiết để tái tạo các thí nghiệm không?
Trả lời:[Yes]
Biện minh: GPU A100
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
• Bài báo nên chỉ ra loại compute worker CPU hoặc GPU, cluster nội bộ, hoặc nhà cung cấp cloud, bao gồm bộ nhớ và lưu trữ liên quan.
• Bài báo nên cung cấp lượng tính toán cần thiết cho mỗi lần chạy thí nghiệm riêng lẻ cũng như ước tính tổng tính toán.
• Bài báo nên tiết lộ liệu toàn bộ dự án nghiên cứu có yêu cầu nhiều tính toán hơn các thí nghiệm được báo cáo trong bài báo hay không (ví dụ: các thí nghiệm sơ bộ hoặc thất bại không vào bài báo).

9.Quy tắc Đạo đức
Câu hỏi: Nghiên cứu được tiến hành trong bài báo có tuân thủ, về mọi khía cạnh, với Quy tắc Đạo đức NeurIPS https://neurips.cc/public/EthicsGuidelines không?
Trả lời: [Yes]
Biện minh:
Hướng dẫn:
• Câu trả lời NA có nghĩa là các tác giả đã không xem xét Quy tắc Đạo đức NeurIPS.
• Nếu các tác giả trả lời No, họ nên giải thích các hoàn cảnh đặc biệt yêu cầu sự sai lệch khỏi Quy tắc Đạo đức.
• Các tác giả nên đảm bảo bảo tồn tính ẩn danh (ví dụ: nếu có xem xét đặc biệt do luật pháp hoặc quy định trong phạm vi quyền hạn của họ).

10.Tác động Rộng hơn
Câu hỏi: Bài báo có thảo luận về cả tác động xã hội tích cực tiềm năng và tác động xã hội tiêu cực của công việc được thực hiện không?
Trả lời: [NA]
Biện minh:
Hướng dẫn:
• Câu trả lời NA có nghĩa là không có tác động xã hội của công việc được thực hiện.
• Nếu các tác giả trả lời NA hoặc No, họ nên giải thích tại sao công việc của họ không có tác động xã hội hoặc tại sao bài báo không giải quyết tác động xã hội.
• Ví dụ về tác động xã hội tiêu cực bao gồm các sử dụng độc hại tiềm năng hoặc không chủ ý (ví dụ: thông tin sai lệch, tạo hồ sơ giả, giám sát), xem xét công bằng (ví dụ: triển khai các công nghệ có thể đưa ra quyết định ảnh hưởng không công bằng đến các nhóm cụ thể), xem xét quyền riêng tư, và xem xét bảo mật.
• Hội nghị mong đợi rằng nhiều bài báo sẽ là nghiên cứu nền tảng và không liên quan đến các ứng dụng cụ thể, chứ đừng nói đến việc triển khai. Tuy nhiên, nếu có con đường trực tiếp đến bất kỳ ứng dụng tiêu cực nào, các tác giả nên chỉ ra. Ví dụ: việc chỉ ra rằng một cải thiện trong chất lượng của các mô hình sinh tạo có thể được sử dụng để tạo deepfake cho thông tin sai lệch là hợp lệ. Mặt khác, không cần thiết phải chỉ ra rằng một thuật toán chung để tối ưu hóa mạng neural có thể cho phép mọi người huấn luyện các mô hình tạo Deepfake nhanh hơn.
• Các tác giả nên xem xét các tác hại có thể có thể phát sinh khi công nghệ được sử dụng như dự định và hoạt động chính xác, các tác hại có thể phát sinh khi công nghệ được sử dụng như dự định nhưng cho kết quả không chính xác, và các tác hại sau khi (cố ý hoặc vô ý) sử dụng sai công nghệ.
• Nếu có tác động xã hội tiêu cực, các tác giả cũng có thể thảo luận về các chiến lược giảm thiểu có thể (ví dụ: phát hành có cổng của các mô hình, cung cấp phòng thủ ngoài tấn công, cơ chế giám sát lạm dụng, cơ chế giám sát cách hệ thống học từ phản hồi theo thời gian, cải thiện hiệu quả và khả năng tiếp cận của ML).

11.Biện pháp Bảo vệ
Câu hỏi: Bài báo có mô tả các biện pháp bảo vệ đã được đưa ra để phát hành có trách nhiệm dữ liệu hoặc mô hình có nguy cơ cao bị lạm dụng (ví dụ: mô hình ngôn ngữ tiền huấn luyện, trình tạo hình ảnh, hoặc bộ dữ liệu được quét) không?
Trả lời: [NA]
Biện minh:
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không đặt ra những rủi ro như vậy.
• Các mô hình được phát hành có nguy cơ cao bị lạm dụng hoặc dual-use nên được phát hành với các biện pháp bảo vệ cần thiết để cho phép sử dụng có kiểm soát mô hình, ví dụ bằng cách yêu cầu người dùng tuân thủ hướng dẫn sử dụng hoặc hạn chế truy cập mô hình hoặc triển khai bộ lọc an toàn.
• Các bộ dữ liệu đã được quét từ Internet có thể đặt ra rủi ro an toàn. Các tác giả nên mô tả cách họ tránh phát hành hình ảnh không an toàn.
• Chúng tôi nhận ra rằng việc cung cấp các biện pháp bảo vệ hiệu quả là thách thức, và nhiều bài báo không yêu cầu điều này, nhưng chúng tôi khuyến khích các tác giả tính đến điều này và nỗ lực hết mình.

26

--- TRANG 25 ---
12.Giấy phép cho tài sản hiện có
Câu hỏi: Các nhà sáng tạo hoặc chủ sở hữu ban đầu của tài sản (ví dụ: mã, dữ liệu, mô hình), được sử dụng trong bài báo, có được ghi nhận đúng cách và giấy phép và điều khoản sử dụng có được đề cập rõ ràng và tôn trọng đúng cách không?
Trả lời: [Yes]
Biện minh: Tài liệu tham khảo
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không sử dụng tài sản hiện có.
• Các tác giả nên trích dẫn bài báo gốc đã tạo ra gói mã hoặc bộ dữ liệu.
• Các tác giả nên nêu phiên bản nào của tài sản được sử dụng và, nếu có thể, bao gồm URL.
• Tên của giấy phép (ví dụ: CC-BY 4.0) nên được bao gồm cho mỗi tài sản.
• Đối với dữ liệu được quét từ một nguồn cụ thể (ví dụ: trang web), bản quyền và điều khoản dịch vụ của nguồn đó nên được cung cấp.
• Nếu tài sản được phát hành, giấy phép, thông tin bản quyền, và điều khoản sử dụng trong gói nên được cung cấp. Đối với các bộ dữ liệu phổ biến, paperswithcode.com/datasets đã tuyển chọn giấy phép cho một số bộ dữ liệu. Hướng dẫn cấp phép của họ có thể giúp xác định giấy phép của một bộ dữ liệu.
• Đối với các bộ dữ liệu hiện có được đóng gói lại, cả giấy phép ban đầu và giấy phép của tài sản dẫn xuất (nếu nó đã thay đổi) nên được cung cấp.
• Nếu thông tin này không có sẵn trực tuyến, các tác giả được khuyến khích liên hệ với những người tạo ra tài sản.

13.Tài sản Mới
Câu hỏi: Các tài sản mới được giới thiệu trong bài báo có được tài liệu hóa tốt và tài liệu có được cung cấp cùng với tài sản không?
Trả lời: [Yes]
Biện minh: [NA].
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không phát hành tài sản mới.
• Các nhà nghiên cứu nên truyền đạt chi tiết của bộ dữ liệu/mã/mô hình như một phần của bài nộp của họ thông qua các mẫu có cấu trúc. Điều này bao gồm chi tiết về huấn luyện, giấy phép, hạn chế, v.v.
• Bài báo nên thảo luận liệu và cách sự đồng ý được lấy từ những người có tài sản được sử dụng.
• Tại thời điểm nộp, hãy nhớ ẩn danh tài sản của bạn (nếu áp dụng). Bạn có thể tạo URL ẩn danh hoặc bao gồm tệp zip ẩn danh.

14.Crowdsourcing và Nghiên cứu với Đối tượng Con người
Câu hỏi: Đối với các thí nghiệm crowdsourcing và nghiên cứu với đối tượng con người, bài báo có bao gồm văn bản đầy đủ của hướng dẫn được đưa cho người tham gia và ảnh chụp màn hình, nếu áp dụng, cũng như chi tiết về bồi thường (nếu có) không?
Trả lời: [NA]
Biện minh:
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không liên quan đến crowdsourcing hoặc nghiên cứu với đối tượng con người.
• Bao gồm thông tin này trong tài liệu bổ sung là tốt, nhưng nếu đóng góp chính của bài báo liên quan đến đối tượng con người, thì càng nhiều chi tiết càng tốt nên được bao gồm trong bài báo chính.
• Theo Quy tắc Đạo đức NeurIPS, những người lao động tham gia thu thập dữ liệu, tuyển chọn, hoặc lao động khác nên được trả ít nhất mức lương tối thiểu trong nước của người thu thập dữ liệu.

28

--- TRANG 26 ---
15.Phê duyệt Hội đồng Đánh giá Thể chế (IRB) hoặc Tương đương cho Nghiên cứu với Đối tượng Con người
Câu hỏi: Bài báo có mô tả các rủi ro tiềm năng mà người tham gia nghiên cứu gặp phải, liệu những rủi ro đó có được tiết lộ cho đối tượng hay không, và liệu có được phê duyệt của Hội đồng Đánh giá Thể chế (IRB) (hoặc một phê duyệt/đánh giá tương đương dựa trên yêu cầu của quốc gia hoặc thể chế của bạn) hay không?
Trả lời: [NA]
Biện minh:
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không liên quan đến crowdsourcing hoặc nghiên cứu với đối tượng con người.
• Tùy thuộc vào quốc gia nơi nghiên cứu được tiến hành, phê duyệt IRB (hoặc tương đương) có thể được yêu cầu cho bất kỳ nghiên cứu đối tượng con người nào. Nếu bạn có được phê duyệt IRB, bạn nên nêu rõ điều này trong bài báo.
• Chúng tôi nhận ra rằng các thủ tục cho điều này có thể khác nhau đáng kể giữa các thể chế và địa điểm, và chúng tôi mong đợi các tác giả tuân thủ Quy tắc Đạo đức NeurIPS và hướng dẫn cho thể chế của họ.
• Đối với bài nộp ban đầu, không bao gồm bất kỳ thông tin nào có thể phá vỡ tính ẩn danh (nếu áp dụng), chẳng hạn như thể chế tiến hành đánh giá.

29