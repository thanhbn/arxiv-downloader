# 2309.14509.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2309.14509.pdf
# Kích thước file: 643890 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
DEEPSPEED ULYSSES : TỐI ƯU HÓA HỆ THỐNG ĐỂ CHO PHÉP
HUẤN LUYỆN CÁC MÔ HÌNH TRANSFORMER CÓ CHUỖI CỰC DÀI
Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang
Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He
Microsoft Inc
{samjacobs,mtanaka,minjiaz,v-chengmingz,leonsong,samyamr,yxhe}@microsoft.com
TÓM TẮT
Tính toán trong một mô hình ngôn ngữ lớn (LLM) dựa trên Transformer điển hình có thể được đặc trưng bởi
kích thước batch, chiều ẩn, số lớp và độ dài chuỗi. Cho đến nay, các công trình hệ thống để tăng tốc huấn luyện
LLM đã tập trung vào ba chiều đầu tiên: song song dữ liệu cho kích thước batch, song song tensor cho kích thước
ẩn và song song pipeline cho độ sâu mô hình hoặc các lớp. Những hình thức song song được nghiên cứu rộng rãi
này không được nhắm mục tiêu hoặc tối ưu hóa cho các mô hình Transformer chuỗi dài. Với nhu cầu ứng dụng
thực tế cho LLM chuỗi dài, sự chú ý mới được dành cho song song chuỗi. Tuy nhiên, các công trình hiện tại
trong song song chuỗi bị hạn chế bởi tính không hiệu quả về bộ nhớ-truyền thông, giới hạn khả năng mở rộng
của chúng đối với các mô hình lớn chuỗi dài. Trong công trình này, chúng tôi giới thiệu DeepSpeed-Ulysses,
một phương pháp mới, di động và hiệu quả để cho phép huấn luyện LLM với độ dài chuỗi cực dài một cách có
hiệu quả và có thể mở rộng cao. DeepSpeed-Ulysses ở cốt lõi phân vùng dữ liệu đầu vào theo chiều chuỗi và
sử dụng truyền thông tập hợp all-to-all hiệu quả cho tính toán attention. Phân tích truyền thông lý thuyết cho
thấy trong khi các phương pháp khác phát sinh chi phí truyền thông khi độ dài chuỗi tăng, DeepSpeed-Ulysses
duy trì khối lượng truyền thông không đổi khi độ dài chuỗi và thiết bị tính toán tăng tỷ lệ thuận. Hơn nữa,
đánh giá thử nghiệm cho thấy DeepSpeed-Ulysses huấn luyện nhanh hơn 2.5x với độ dài chuỗi dài hơn 4x so
với phương pháp baseline SOTA hiện tại.

1 Giới thiệu
Huấn luyện các mô hình lớn với chuỗi dài đang trở nên rất quan trọng trên nhiều mặt từ AI tạo sinh đến các mô hình
khám phá khoa học. Về phía AI tạo sinh, AI đối thoại, tóm tắt tài liệu dài giàu kiến thức và tạo video yêu cầu lý luận
trên bối cảnh dài trong các lĩnh vực không gian và thời gian. Ví dụ, các mô hình nền tảng đa phương thức như những
mô hình xử lý âm thanh, hình ảnh và dạng sóng đồng thời yêu cầu lý luận bối cảnh dài trên các đầu vào chiều cao với
chuỗi dài. Tương tự, tóm tắt cấp chương và sách (ước tính hàng chục và hàng trăm nghìn từ) có tầm quan trọng lớn
trong AI đối thoại và các nhiệm vụ tóm tắt trừu tượng [Beltagy et al., 2020, Kryściński et al., 2022, MosaicML, 2023]
và đã cho thấy được hưởng lợi từ huấn luyện chuỗi dài [Xiong et al., 2023, Peng et al., 2023, Touvron et al., 2023].
Sự ra mắt của ChatGPT (và các thương hiệu LLM mã nguồn mở và "sản phẩm" tương tự sau đó) đã đẩy ứng dụng chat
lên hàng đầu của AI hiện đại, làm cho các ứng dụng chat trở nên có liên quan hơn bao giờ hết. Xử lý chuỗi dài là rất
quan trọng để hỗ trợ lịch sử dài hơn trong các ứng dụng chat [Touvron et al., 2023].

Độ dài chuỗi dài cũng quan trọng như nhau đối với AI cho khoa học mở ra cánh cửa cho việc hiểu rõ hơn về sinh học
cấu trúc, chăm sóc sức khỏe, dự báo khí hậu và thời tiết [Nguyen et al., 2023] và mô phỏng phân tử lớn [Zvyagin et al.,
2022]. Ví dụ, bằng cách điều chỉnh các mô hình ngôn ngữ lớn với chuỗi gene, chúng ta có thể tạo ra các mô hình ngôn
ngữ có thể học các mẫu tiến hóa của genome sử dụng các bảng chữ cái đơn giản và chuỗi cực dài (genome con người
có 6.4 tỷ chữ cái) [Zvyagin et al., 2022]. Trong chăm sóc sức khỏe, mô hình dự đoán chẩn đoán có điều kiện trên toàn
bộ hồ sơ chăm sóc bệnh nhân yêu cầu bối cảnh của các chuỗi dài [Li et al., 2022a, Gao et al., 2021].

--- TRANG 2 ---
DeepSpeed-Ulysses
Mặc dù tầm quan trọng mới nổi của độ dài chuỗi dài đối với cả AI tạo sinh và AI cho khoa học, các hệ thống huấn luyện
mô hình lớn hiện tại và các công nghệ song song cơ bản (song song dữ liệu, tensor, pipeline, chuỗi) bị hạn chế trong
khả năng hỗ trợ huấn luyện chuỗi dài hiệu quả. Hai thách thức với phương pháp song song hiện tại nổi lên. Thứ nhất,
phương pháp song song hiện tại như song song dữ liệu, tensor và pipeline không thể giải quyết việc mở rộng theo chiều
chuỗi. Thứ hai, các phương pháp song song chuỗi hiện tại không hiệu quả vì tính không hiệu quả về bộ nhớ-truyền
thông. Hơn nữa, các phương pháp hiện tại có khả năng sử dụng hạn chế yêu cầu tái cấu trúc mã xâm lấn và dễ gây lỗi.

Trong bài báo này, chúng tôi giới thiệu DeepSpeed-Ulysses (hoặc Ulysses, một tiểu thuyết rất dài), một phương pháp
đơn giản, di động và hiệu quả để cho phép huấn luyện LLM với độ dài chuỗi cực dài một cách có hiệu quả và có thể
mở rộng cao. DeepSpeed-Ulysses phân vùng các mẫu riêng lẻ theo chiều chuỗi giữa các GPU tham gia. Sau đó ngay
trước tính toán attention, nó sử dụng truyền thông tập hợp all-to-all trên các queries, keys và values được phân vùng
sao cho mỗi GPU nhận toàn bộ chuỗi nhưng chỉ cho một tập con không trùng lặp của các attention heads. Điều này
cho phép các GPU tham gia tính toán attention cho các attention heads khác nhau song song. Cuối cùng, DeepSpeed-
Ulysses sử dụng một all-to-all khác để thu thập kết quả theo các attention heads trong khi phân vùng lại theo chiều chuỗi.

Trong công trình này, chúng tôi đưa ra những đóng góp sau đây của DeepSpeed-Ulysses để thúc đẩy nghệ thuật hiện
đại trong song song chuỗi dài:

• DeepSpeed-Ulysses huấn luyện các mô hình Transformer với độ dài chuỗi lớn hơn 4x so với các hệ thống
hiện tại, đồng thời cho phép huấn luyện với các chuỗi có hơn một triệu token.

• Giảm truyền thông hơn 10x so với các hệ thống hiện tại, dẫn đến cải thiện thông lượng lên đến 2.5x, và
thông lượng duy trì hơn 175 TFlops/GPU (hơn 54% của đỉnh phần cứng).

• Attention hoàn toàn tổng quát và không phụ thuộc implementation: DeepSpeed sequence parallelism
(Ulysses) hỗ trợ attention dày đặc cũng như thưa thớt, và hoạt động với các implementation attention
hiệu quả như FlashAttention v2 [Dao, 2023].

• Hỗ trợ huấn luyện mô hình khổng lồ: DeepSpeed sequence parallelism hoạt động cùng với ZeRO-3 để
không chỉ hỗ trợ độ dài chuỗi lớn mà còn kích thước mô hình khổng lồ.

• Dễ sử dụng và di động, yêu cầu thay đổi mã tối thiểu đối với các framework huấn luyện hiện tại.

Trong các phần tiếp theo, chúng tôi cung cấp nền tảng và công trình liên quan, thảo luận chi tiết về thiết kế cốt lõi
DeepSpeed sequence parallelism, phân tích độ phức tạp truyền thông, đánh giá thử nghiệm và so sánh với công trình
hiện tại.

2 Nền tảng và Công trình Liên quan

Trong phần này, chúng tôi trình bày tổng quan ngắn gọn về kiến trúc Transformer, chế độ song song để tăng tốc huấn
luyện Transformer và thảo luận về công trình liên quan chặt chẽ với phương pháp của chúng tôi.

2.1 Nền tảng

Phần này giới thiệu ngắn gọn kiến trúc Transformer và làm nổi bật các chế độ song song khác nhau của mạng nơ-ron
sâu nói chung và mô hình Transformer nói riêng. Thảo luận ngắn gọn này được theo sau bởi trọng tâm cụ thể vào công
trình liên quan chặt chẽ.

2.1.1 Kiến trúc Transformer

Được hiển thị trong Hình 1 là một phác thảo của các khối xây dựng của một kiến trúc Transformer attention đa đầu
điển hình [Vaswani et al., 2017]. Nó bao gồm các chuỗi đầu vào được chiếu thành các embedding queries (Q), keys
(K) và values (V). QKV thường là một tensor 3D có kích thước N, b, d trong đó N là độ dài chuỗi, b là kích thước
micro batch và d là chiều ẩn. Các tensor QKV được đưa vào khối attention, một thành phần trung tâm của mô hình
Transformer. Đầu ra của attention là đầu vào cho khối perceptron đa lớp (MLP) hoặc khối feed-forward theo vị trí của
kiến trúc Transformer.

Khối attention theo sau bởi khối MLP được nhân bản nhiều lần để tạo thành một encoder, decoder hoặc mạng
Transformer encoder-decoder.

2

--- TRANG 3 ---
DeepSpeed-Ulysses
Hình 1: Multi-head attention Transformer

2.1.2 Chế độ Song song

Song song dữ liệu [Dean et al., 2012] là phương pháp de facto để tăng tốc huấn luyện mạng nơ-ron và đã được áp dụng
rộng rãi với các kiến trúc mạng nơ-ron và ứng dụng khác nhau. Song song dữ liệu ở dạng đơn giản nhất phân vùng
dữ liệu đầu vào theo chiều mẫu hoặc batch trong khi nhân bản tham số mô hình trên các thiết bị tính toán. Song song
dữ liệu hiệu quả khi kích thước batch đủ lớn để che giấu chi phí truyền thông trong tính toán. Tuy nhiên, nó bị hạn chế
khi mô hình lớn và việc nhân bản tham số mô hình trên các thiết bị không khả thi trong thực tế. Tối ưu hóa ZeRO
[Rajbhandari et al., 2020, 2021] giải quyết vấn đề này bằng cách phân vùng tham số mô hình trên các thiết bị tính toán
có sẵn. Hơn nữa, batch lớn được biết là có tác động đến chất lượng mô hình [Keskar et al., 2016].

Đáng chú ý là phương pháp được đề xuất của chúng tôi trực giao với cả song song dữ liệu và ZeRO. Phương pháp
được đề xuất của chúng tôi có thể được sử dụng với cả hai phương pháp. Ngoài ra, bằng cách tận dụng song song
chuỗi để giữ kích thước batch toàn cục ở mức hợp lý trên các hệ thống lớn, chúng tôi hiệu quả làm giảm tác động
của kích thước batch lớn đến sự hội tụ mô hình. Song song chuỗi phục vụ hai mục đích trong vấn đề này. Thứ nhất,
song song chuỗi có thể tăng tốc thời gian để có giải pháp cho cùng độ dài chuỗi dài (đã được khám phá); nói cách
khác, song song chuỗi giảm thời gian lặp tỷ lệ với tài nguyên tính toán bổ sung. Thứ hai, song song chuỗi cho phép
huấn luyện chuỗi dài hơn hoặc pretraining liên tục nơi độ dài bối cảnh huấn luyện tăng dần theo thời gian [Xiong et
al., 2023]. Xét một tình huống thực tế của huấn luyện quy mô lớn trên 1024 GPU. Thiết lập khám phá ban đầu hoặc
pretraining của một LLM (proxy) có độ dài chuỗi 8192 (8K), kích thước micro batch là 1 (do đó, kích thước toàn cục
8 triệu token) mỗi GPU. Một thay đổi đơn giản để cải thiện chất lượng của mô hình pretrained yêu cầu thay đổi độ dài
chuỗi từ 8K lên 32K, điều này sẽ dẫn đến kích thước batch toàn cục khoảng 32 triệu. Tuy nhiên, tăng kích thước batch
toàn cục không phải là lựa chọn do tác động tiêu cực đến chất lượng mô hình. Do đó, song song chuỗi trở nên hữu
ích như một kỹ thuật tối ưu hóa hệ thống mà không yêu cầu tìm kiếm siêu tham số tốn công. Trong tình huống này,
song song chuỗi cho phép kích thước batch lớn được chia trên nhiều GPU mà không tăng kích thước batch toàn cục,
bất kể độ dài chuỗi.

Song song tensor [Shoeybi et al., 2019] và pipeline [Narayanan et al., 2019, Huang et al., 2018, Narayanan et al.,
2021] là hai phương pháp phổ biến khác cho huấn luyện quy mô lớn. Tổng hợp lại, song song tensor và pipeline được
gọi là song song mô hình, và được nhắm mục tiêu vào các toán tử tính toán trong các mô hình lớn. Trái ngược với
song song dữ liệu, song song mô hình được sử dụng khi các mô hình quá lớn (như trong nhiều LLM) và không thể
được nhân bản hoàn toàn trên các rank song song dữ liệu. Song song tensor chia các toán tử tính toán (tức là, attention
và MLP) trong một lớp và song song pipeline chia mô hình theo chiều sâu (theo lớp). Song song 3D [Team and
Majumder, 2020, Smith et al., 2022] kết hợp song song dữ liệu, song song tensor và song song pipeline để đạt được
thông lượng cao hơn so với 3 thành phần cấu thành với chi phí viết lại mã rộng rãi và overhead năng suất [Wang et al.,
2023].

2.2 Công trình Liên quan

Để có tổng quan rộng và khảo sát về các phương pháp huấn luyện phân tán cho mạng nơ-ron sâu, vui lòng xem [Ben-
Nun and Hoefler, 2019]. Các phương pháp này được phân loại rộng rãi thành song song dữ liệu và mô hình như đã
mô tả ở trên. Tuy nhiên, tất cả các phương pháp song song hiện tại đều bị hạn chế trong việc xử lý overhead bộ nhớ
activation trung gian liên quan đến chuỗi cực dài.

3

--- TRANG 4 ---
DeepSpeed-Ulysses
Phương phápĐộ phức tạp Hiệu quả bộ nhớ Hiệu quả bộ nhớ Attention Dễ sử
truyền thôngactivation tham số agnostic dụng
ColAI-SP [Li et al., 2022b] O(M) ✓ x x x
Megatron-SP [Korthikanti et al., 2022] O(M) ✓ x ✓ x
DS-Ulysses O(M/P ) ✓ ✓ ✓ ✓

Bảng 1: So sánh công trình của chúng tôi (DS-Ulysses) với các phương pháp song song chuỗi khác.

Trong khi các công trình gần đây trong song song chuỗi giải quyết overhead bộ nhớ, chúng thiếu hiệu quả truyền
thông, do đó bị hạn chế trong khả năng mở rộng. Tương tự như công trình của chúng tôi, tất cả các công trình hiện
tại trong song song chuỗi phân vùng dữ liệu đầu vào theo chiều chuỗi nhưng khác nhau trong việc các chiếu đầu vào
nào được phân vùng và cách các phân vùng được tổng hợp và truyền thông cho tính toán attention.

Các tác giả trong [Li et al., 2022b] (từ đây gọi là ColAI-SP) giới thiệu ring self attention, một truyền thông tập hợp
giống ring trong đó các chiếu query là cục bộ trong khi các chiếu key và values được truyền theo kiểu ring để tính
toán attention toàn cục, dẫn đến độ phức tạp truyền thông tuyến tính theo kích thước thông điệp, M. Phương pháp
song song chuỗi Megatron-LM [Korthikanti et al., 2022] được tích hợp chặt chẽ với song song tensor Megatron.
Megatron LM phân vùng chuỗi theo chiều chuỗi và áp dụng tập hợp allgather và reduce scatter để tổng hợp các
chiếu QKV cho tính toán attention. Phân tích độ phức tạp truyền thông cho thấy không giống như phương pháp của
chúng tôi, khối lượng truyền thông song song chuỗi Megatron-LM tăng tuyến tính với kích thước thông điệp (M) bất
kể số thiết bị tính toán. DeepSpeed-Ulysses mặt khác giữ khối lượng truyền thông nhất quán bằng cách tăng GPU tỷ
lệ với kích thước thông điệp hoặc độ dài chuỗi xem 3.2 để biết thêm chi tiết.

Bảng 1 tóm tắt cách DeepSpeed-Ulysses khác biệt với các phương pháp hiện tại khác. DeepSpeed-Ulysses có lợi thế
hiệu quả truyền thông so với hai phương pháp kia. Nó cũng được hưởng lợi từ việc tận dụng tối ưu hóa ZeRO [Rajbhandari
et al., 2020, 2021] để phân vùng tham số mô hình trên cả nhóm song song chuỗi và dữ liệu. DeepSpeed-Ulysses hỗ
trợ các loại attention khác nhau và dễ sử dụng. Song song chuỗi Megatron-LM được tích hợp chặt chẽ với song song
tensor Megatron-LM giới hạn cả hiệu quả bộ nhớ và dễ sử dụng. ColAI-SP yêu cầu một loại attention khác (cụ thể)
và không dễ sử dụng. Không rõ ring self-attention ColAI-SP tổng quát hóa tốt như thế nào đối với các loại attention
và cơ chế khác.

Có những công trình liên quan trong Transformer thưa thớt đặc biệt tập trung vào xấp xỉ full-attention như sparse
attention [Child et al., 2019, Choromanski et al., 2020, Zaheer et al., 2021, Beltagy et al., 2020]. Cũng có những
công trình gần đây về attention hiệu quả bộ nhớ và tính toán trên một GPU. Một ví dụ phổ biến trong danh mục này
là Flash attention [Dao et al., 2022, Dao, 2023], tận dụng các kỹ thuật đã biết như tiling và recomputation để có hiệu
quả tính toán và bộ nhớ. Những công trình này trực giao với công trình của chúng tôi và được tận dụng tương ứng.

3 Thiết kế Cốt lõi DeepSpeed-Ulysses

3.1 Thiết kế Hệ thống

Hình 2: Thiết kế song song chuỗi DeepSpeed (DeepSpeed-Ulysses)

Hình 2 cho thấy thiết kế cốt lõi của DeepSpeed-Ulysses. Như với kiến trúc transformer đã biết, thiết kế bao gồm các
chuỗi đầu vào N được phân vùng trên P thiết bị có sẵn. Mỗi phân vùng cục bộ N/P được chiếu thành các embedding
queries (Q), keys

4

--- TRANG 5 ---
DeepSpeed-Ulysses
(K) và values (V). Tiếp theo, các embedding (QKV) được thu thập thành QKV toàn cục thông qua các tập hợp all-to-all
được tối ưu hóa cao giữa các thiết bị tính toán tham gia. Tiếp theo sau tập hợp all-to-all là tính toán attention mỗi head
dưới dạng:

Outputcontext = Softmax ((QKT)/√(d))V (1)

Sau tính toán attention, một tập hợp all-to-all khác biến đổi tensor context đầu ra của tính toán attention thành chuỗi
(N/P) song song cho các toán tử tiếp theo (MLP MatMul, layer norm v.v.) trong các module còn lại của khối lớp
transformer.

3.2 Phân tích Truyền thông

Điều phân biệt DeepSpeed-Ulysses với các phương pháp chuỗi dài hiện tại khác là khối lượng truyền thông tổng hợp
nhỏ hơn nhiều và khả năng mở rộng tổng thể tốt hơn với việc tăng mức độ song song chuỗi so với các giải pháp hiện
tại, như được thể hiện bởi phân tích khối lượng truyền thông dưới đây:

Trên các cluster hiện đại với kết nối NVSwitch trong node và topology IB fat tree giữa các node, khối lượng truyền
thông được truyền mỗi liên kết cho một all-to-all với thông điệp tổng hợp kích thước M trên P GPU là M/P. Đối với
một mô hình transformer với kích thước ẩn h, độ dài chuỗi N, và mức độ song song P, DS-Sequence thực hiện all-to-all
cho các chiếu QKV với kích thước thông điệp tổng hợp 3Nh trước tính toán attention, và một all-to-all khác cho chiếu
context đầu ra với kích thước Nh cho mỗi lớp transformer. Do đó, song song chuỗi DeepSpeed phát sinh khối lượng
truyền thông tổng hợp mỗi liên kết là 4Nh/P (hoặc với độ phức tạp O(N/P). Lưu ý rằng khối lượng truyền thông này
không đổi khi cả N và P đều tăng tỷ lệ thuận.

Ngược lại, các phương pháp hiện tại như Megatron-LM phát sinh khối lượng truyền thông tăng tuyến tính với N bất
kể P, dẫn đến độ phức tạp truyền thông O(N). Ví dụ, Megatron-LM thực hiện hai all-gather với khối lượng thông
điệp Nh và hai reduce-scatter với khối lượng Nh cho mỗi lớp transformer. Tuy nhiên, chi phí của mỗi all-gather và
reduce-scatter kích thước M vẫn là M khi P » 1, thay vì M/P. Do đó, song song chuỗi Megatron-LM phát sinh khối
lượng truyền thông mỗi liên kết là 4Nh, lớn hơn P lần so với song song chuỗi DeepSpeed. Điều này cho phép song
song chuỗi DeepSpeed cho phép huấn luyện với các chuỗi cực dài trong khi đạt được hiệu quả huấn luyện cao hơn
đáng kể so với các phương pháp hiện tại. Kết quả đánh giá của chúng tôi phù hợp với phân tích này.

3.3 Hiệu quả Bộ nhớ

Trong khi song song chuỗi DeepSpeed giảm bộ nhớ activation khi huấn luyện với các chuỗi dài hơn, nó không ảnh
hưởng đến bộ nhớ được tiêu thụ bởi các trạng thái mô hình. Do đó, để hỗ trợ huấn luyện độ dài chuỗi lớn với một mô
hình ngôn ngữ lớn, song song chuỗi DeepSpeed được tích hợp với ZeRO-3. ZeRO Redundancy Optimizer Stage 3
(ZeRO-3) [Rajbhandari et al., 2020, 2021] là một kỹ thuật tối ưu hóa bộ nhớ để huấn luyện các mô hình lớn. Không
giống như huấn luyện song song dữ liệu cổ điển của mạng nơ-ron nơi các trạng thái mô hình được nhân bản trên các
rank song song dữ liệu, ZeRO-3 tối ưu hóa việc sử dụng bộ nhớ bằng cách phân vùng các trạng thái mô hình trên các
rank song song dữ liệu. Tuy nhiên, với song song chuỗi, dữ liệu huấn luyện có thể được xem xét trong cả chiều batch
(mẫu) và chuỗi và các nhóm song song liên quan được kết hợp để tạo thành một nhóm lớn hơn cho song song ZeRO.
Do đó, chúng tôi mở rộng phân vùng ZeRO-3 để kết hợp các rank song song dữ liệu và chuỗi. Nói cách khác, trong
song song chuỗi DeepSpeed, ZeRO phân vùng các trạng thái mô hình trên cả nhóm song song chuỗi và dữ liệu và thu
thập các phân vùng mỗi rank (allgather) khi chúng cần thiết. Tương tự, gradients được giảm trên cả các rank song
song dữ liệu và chuỗi để cập nhật tham số. Hỗ trợ ZeRO cho phép tiết kiệm bộ nhớ lớn trong cả chiều chuỗi và dữ
liệu và cho phép mở rộng không chỉ đến độ dài chuỗi lớn mà còn đến các mô hình lớn.

3.4 Giải pháp Tổng quát và Không phụ thuộc Attention

Implementation module attention phân tán DeepSpeed đủ tổng quát để hỗ trợ bất kỳ attention nào: ví dụ, self-attention,
cross-attention, causal attention trong cả phiên bản dày đặc và thưa thớt của chúng, và các kernel tối ưu hóa khác nhau
hỗ trợ chuỗi dài ở mức attention cục bộ như các phiên bản khác nhau của FlashAttention. Tính chất tổng quát của
DeepSpeed-Ulysses xuất phát từ bản chất modular của thiết kế cốt lõi: một thiết kế song song chuỗi tập trung vào
attention. Trước tính toán attention là song song chuỗi của phân vùng N/P, tính toán attention là song song head với
attention đầy đủ mỗi head nhưng chỉ với ít head hơn, do đó tính toán attention có thể được thay thế bằng bất kỳ loại
cơ chế attention nào, ví dụ, attention dày đặc và các dạng khác nhau của sparse attention.

5

--- TRANG 6 ---
DeepSpeed-Ulysses

4 Đánh giá

Chúng tôi đánh giá DeepSpeed-Ulysses (DeepSpeed Sequence) trên GPT [Radford et al., 2019], một mô hình nền tảng
cho nhiều nhiệm vụ NLP trên tối đa 256 A100 GPU. Đánh giá của chúng tôi gồm năm phần: i) khả năng mở rộng độ
dài chuỗi, ii) thông lượng cho dense attention và so sánh với hệ thống hiện tại, và iii) thông lượng với sparse attention
và so sánh với hệ thống hiện tại, iv) nghiên cứu mở rộng song song và v) nghiên cứu hội tụ của song song chuỗi Deep.
Chúng tôi thảo luận và trình bày đánh giá từ mỗi danh mục này tiếp theo.

4.1 Khả năng Mở rộng Độ dài Chuỗi

Tập thử nghiệm đầu tiên là strong scaling của độ dài chuỗi lên đến 1 triệu token trên mô hình GPT 1.2 tỷ tham số.
Kết quả đánh giá này được hiển thị trong Hình 3. Song song chuỗi DeepSpeed cho phép tăng độ dài chuỗi tuyến tính
với số GPU và độ dài chuỗi mở rộng tuyến tính tương đối và duy trì thông lượng tính toán tương tự trên các độ dài
chuỗi khác nhau ở số GPU phù hợp.

Hình 3: Đánh giá khả năng mở rộng mạnh song song chuỗi DeepSpeed ở các độ dài chuỗi và số GPU khác nhau

4.2 Đánh giá Dense Attention

Tiếp theo, chúng tôi đánh giá song song chuỗi DeepSpeed trên các mô hình GPT dense attention 7 tỷ (7B) và 30 tỷ
(30B) tham số và so sánh với song song chuỗi Megatron-LM trên 32 và 64 A100 GPU tương ứng. Kết quả của các
đánh giá này được hiển thị trong Hình 4 và 5.

Chúng tôi so sánh song song chuỗi DeepSpeed với Megatron-LM cho các mô hình 7B và 30B chạy các độ dài chuỗi
khác nhau. Cho đánh giá của chúng tôi, chúng tôi chọn mức độ song song chuỗi và kích thước micro-batch tạo ra hiệu
suất tốt nhất (đo bằng thông lượng hoặc TFLOPs) cho cả song song chuỗi DeepSpeed và Megatron-LM, điều này
chúng tôi gọi là cấu hình tối ưu (kích thước batch-độ dài chuỗi). Đối với song song chuỗi DeepSpeed, chúng tôi luôn
sử dụng mức độ song song ZeRO là 32 và 64 cho các mô hình 7B và 30B tương ứng.

Hình 4 và 5 cho thấy song song chuỗi DeepSpeed liên tục vượt trội Megatron-LM cho độ dài chuỗi có thể chạy với
cả hai. Ngoài ra, song song chuỗi DeepSpeed có thể chạy chuỗi dài hơn Megatron-

6

--- TRANG 7 ---
DeepSpeed-Ulysses
LM. Lợi thế hiệu suất song song chuỗi DeepSpeed có hai khía cạnh: (1) song song chuỗi DeepSpeed kết hợp với
ZeRO-3 phù hợp với nhiều mẫu hơn Megatron-LM vì tối ưu hóa bộ nhớ dẫn đến thông lượng cao hơn (2) song song
chuỗi DeepSpeed được hưởng lợi từ truyền thông all-to-all hiệu quả so với truyền thông all-gather được áp dụng trong
song song chuỗi Megatron-LM.

Hình 4: Đánh giá DeepSpeed-Ulysses và Megatron LM trên mô hình 7B tham số với dense attention (32 GPU)

4.3 Đánh giá Sparse Attention

Tương tự, chúng tôi đánh giá song song chuỗi DeepSpeed trên các mô hình sparse attention 7 tỷ và 30 tỷ tham số và
benchmark với song song chuỗi Megatron-LM. Kết quả đánh giá của chúng tôi được hiển thị trong Hình 6 và 7.

Chúng tôi quan sát xu hướng tương tự với sparse attention như các thử nghiệm dense attention. Chúng tôi quan sát
hiệu suất thông lượng hơn 2x của song song chuỗi DeepSpeed so với Megatron-LM. Để tiết kiệm bộ nhớ, song song
chuỗi DeepSpeed tận dụng ZeRO-3 mở rộng đến độ dài chuỗi dài hơn 4x so với Megatron-LM.

Song song chuỗi DeepSpeed vượt trội Megatron-LM cho độ dài chuỗi có thể chạy với cả hai. Trên thực tế, thông
lượng DeepSpeed hiện tại bị bottleneck bởi implementation sparse attention cục bộ, và kết quả là thông lượng
DeepSpeed giảm khi độ dài chuỗi tăng. Chúng tôi mong đợi khoảng cách hiệu suất này giữa DeepSpeed và Megatron-
LM sẽ tăng thêm cho các độ dài chuỗi lớn hơn khi chúng tôi cải thiện hiệu suất của implementation sparse attention
cục bộ trong tương lai.

4.4 Nghiên cứu Mở rộng Song song

Bảng 2: Nghiên cứu mở rộng song song với độ dài chuỗi cố định
Seqlen GPU Thời gian (ms) TFLOPs
131072 64 32432.1333 165.526667
131072 128 17052.5143 157.41
131072 256 9886.7 136.09

7

--- TRANG 8 ---
DeepSpeed-Ulysses
Hình 5: Đánh giá DeepSpeed-Ulysses và Megatron LM trên mô hình 30B tham số với dense attention (64 GPU)

Bảng 3: Nghiên cứu mở rộng song song với độ dài chuỗi thay đổi
Seqlen GPU Thời gian (ms) TFLOPs
65536 64 9676.76 161.3626667
131072 128 17052.5143 157.41
262144 256 33486.5 147.4

Hơn nữa, chúng tôi tiến hành các nghiên cứu mở rộng song song của DeepSpeed-Ulysses theo hai trục. Thứ nhất,
chúng tôi cố định độ dài chuỗi ở 131,072 token và tăng số GPU từ 64 đến 256. Thứ hai, chúng tôi tăng số GPU tỷ lệ
thuận với việc tăng độ dài chuỗi. Kết quả của các thử nghiệm này được hiển thị trong Bảng 2 và 3 tương ứng. Cho
cả hai đánh giá, chúng tôi sử dụng mô hình dense GPT-7B với kích thước batch toàn cục là 8. Các bảng hiển thị thời
gian lặp tính bằng microsecond cũng như thông lượng đạt được đo bằng TFLOPs mỗi GPU. Bảng 2 có thể được hiểu
là strong scaling và cho thấy thời gian thực thi giảm gần như tuyến tính khi chúng tôi tăng số GPU. Bảng 3 mặt khác,
là một dạng weak scaling (không theo nghĩa truyền thống) với lưu ý rằng tính toán attention, một hàm của độ dài
chuỗi, có độ phức tạp bậc hai. Nói cách khác, khi chúng tôi tăng độ dài chuỗi, công việc tăng theo bậc hai.

Overhead truyền thông có thể được quy cho việc giảm nhẹ thông lượng khi chúng tôi tăng khối lượng công việc truyền
thông (tức là, độ dài chuỗi hoặc số GPU). Bất chấp overhead này, chúng tôi quan sát mở rộng tốt ở tỷ lệ phần trăm
cao của hiệu suất GPU đỉnh lý thuyết trên hai nghiên cứu. Những kết quả mở rộng tốt này cho thấy hiệu quả song
song tốt của DeepSpeed-Ulysses.

4.5 Nghiên cứu Hội tụ

Cuối cùng, Hình 8 cho thấy hội tụ của mô hình GPT 1.3 tỷ ở độ dài chuỗi 32K trên 8 A100 GPU với mức độ song
song chuỗi đặt ở 4 cho cả DeepSpeed-Ulysses và song song chuỗi Megatron-LM. Đối với song song chuỗi DeepSpeed,
chúng tôi đánh giá hội tụ với các giai đoạn ZeRO khác nhau. Song song chuỗi DeepSpeed là một kỹ thuật tối ưu hóa
hệ thống thuần túy cho phép huấn luyện mô hình Transformer chuỗi dài, do đó không có tác động (tiêu cực) đến chất
lượng của các mô hình được huấn luyện, khẳng định này được xác thực thông qua thử nghiệm và được hiển thị trong
Hình 8.

8

--- TRANG 9 ---
DeepSpeed-Ulysses
Hình 6: Đánh giá DeepSpeed-Ulysses và Megatron LM trên mô hình 7B tham số với blocked sparse attention (32 GPU)

5 Kết luận

Kết luận, chúng tôi trình bày DeepSpeed Sequence hiệu quả bộ nhớ và truyền thông như công nghệ cho phép huấn
luyện Transformer chuỗi dài lớn. DeepSpeed Sequence cho phép song song chuỗi trên GPU (bằng cách mở rộng các
bộ tăng tốc AI khác), song song hóa chuỗi trên tất cả các thành phần của mô hình Transformer, bao gồm hỗ trợ hợp
lý cho SOTA Flash (dày đặc và thưa thớt) attention. Huấn luyện với DeepSpeed Sequence cho phép cả kích thước mô
hình và độ dài chuỗi mở rộng gần như vô hạn không bị giới hạn bởi hạn chế bộ nhớ GPU đơn và ở phần lớn cao của
hiệu suất tính toán đỉnh.

Tài liệu tham khảo

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020.

Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. Booksum: A
collection of datasets for long-form narrative summarization, 2022.

MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. https://https:
//www.mosaicml.com/blog/mpt-7b , 2023.

Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,
Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz
Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context
scaling of foundation models, 2023.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large
language models, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj

9

--- TRANG 10 ---
DeepSpeed-Ulysses
Hình 7: Đánh giá DeepSpeed-Ulysses và Megatron LM trên mô hình 30B tham số với blocked sparse attention (64 GPU)

Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,
Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang
Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat
models, 2023.

Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, and Aditya Grover. Climax: A foundation
model for weather and climate, 2023.

Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde,
Bharat Kale, Danilo Perez-Rivera, Heng Ma, et al. Genslms: Genome-scale language models reveal sars-cov-2
evolutionary dynamics. bioRxiv , pages 2022–10, 2022.

Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, and Yuan Luo. Clinical-longformer and clinical-bigbird:
Transformers for long clinical sequences. CoRR , abs/2201.11838, 2022a. URL https://arxiv.org/abs/2201.
11838 .

Shang Gao, Mohammed Alawad, M. Todd Young, John Gounley, Noah Schaefferkoetter, Hong Jun Yoon, Xiao-Cheng
Wu, Eric B. Durbin, Jennifer Doherty, Antoinette Stroup, Linda Coyle, and Georgia Tourassi. Limitations of
transformers on clinical text classification. IEEE Journal of Biomedical and Health Informatics , 25(9):3596–3607,
2021. doi:10.1109/JBHI.2021.3062322.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural information processing systems , pages 5998–6008,
2017.

10

--- TRANG 11 ---
DeepSpeed-Ulysses
Hình 8: Đánh giá hội tụ của DeepSpeed-Ulysses với các giai đoạn tối ưu hóa bộ nhớ ZeRO khác nhau

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio Ranzato, Andrew
Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in neural information processing
systems , 25, 2012.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training
trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage
and Analysis , pages 1–16. IEEE, 2020.

Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-infinity: Breaking the gpu
memory wall for extreme scale deep learning. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis , SC '21, 2021.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On
large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836 , 2016.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm:
Training multi-billion parameter language models using model parallelism, 2019.

Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Granger, Phil Gibbons,
and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In ACM Symposium on Operating
Systems Principles (SOSP 2019) , October 2019.

Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V . Le, and Zhifeng Chen.
Gpipe: Efficient training of giant neural networks using pipeline parallelism. ArXiv , abs/1811.06965, 2018.

Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel
dnn training. In International Conference on Machine Learning , pages 7937–7947. PMLR, 2021.

DeepSpeed Team and Rangan Majumder. DeepSpeed: Extreme-scale model training for everyone. https://www.
microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/ ,
2020.

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun
Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train
megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 , 2022.

11

--- TRANG 12 ---
DeepSpeed-Ulysses
Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei
Yang, and Yuxiong He. Zero++: Extremely efficient collective communication for giant model training, 2023.

Tal Ben-Nun and Torsten Hoefler. Demystifying parallel and distributed deep learning: An in-depth concurrency
analysis. ACM Computing Surveys (CSUR) , 52(4):1–43, 2019.

Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence
training from system perspective, 2022b.

Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan
Catanzaro. Reducing activation recomputation in large transformer models, 2022.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.
CoRR , abs/1904.10509, 2019.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller.
Rethinking attention with performers. CoRR , abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794 .

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham,
Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences, 2021.

Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact
attention with io-awareness, 2022.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised
multitask learners. 2019.

12
