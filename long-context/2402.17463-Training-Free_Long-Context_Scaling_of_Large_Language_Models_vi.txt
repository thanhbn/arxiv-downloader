# 2402.17463.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2402.17463.pdf
# Kích thước tệp: 1432608 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Mở rộng Ngữ cảnh Dài của Các Mô hình Ngôn ngữ Lớn không cần Huấn luyện
Chenxin An* 1 2Fei Huang2Jun Zhang Shansan Gong1Xipeng Qiu3Chang Zhou2Lingpeng Kong1
Tóm tắt
Khả năng của các Mô hình Ngôn ngữ Lớn (LLMs)
để xử lý và tạo văn bản mạch lạc bị suy yếu đáng kể
khi số lượng token đầu vào vượt quá độ dài huấn luyện trước
của chúng. Do chi phí đắt đỏ của việc tinh chỉnh
các mô hình quy mô lớn với các chuỗi dài hơn, chúng tôi
đề xuất Dual Chunk Attention (DCA), cho phép
LLAMA 270B hỗ trợ cửa sổ ngữ cảnh hơn 100k
token mà không cần huấn luyện liên tục. Bằng cách
phân tách tính toán attention cho các chuỗi dài
thành các module dựa trên chunk, DCA quản lý
để nắm bắt hiệu quả thông tin vị trí tương đối của
các token trong cùng một chunk (Intra-Chunk)
và giữa các chunk riêng biệt (Inter-Chunk), cũng như
tích hợp liền mạch với Flash Attention. Ngoài khả năng
ngoại suy ấn tượng, DCA đạt được hiệu suất trên
các tác vụ ngữ cảnh dài thực tế có thể so sánh
hoặc thậm chí tốt hơn so với các mô hình đã được
tinh chỉnh. Khi so sánh với các mô hình độc quyền,
mô hình 70B không cần huấn luyện của chúng tôi
đạt 94% hiệu suất của gpt-3.5-16k, cho thấy
đây là một giải pháp thay thế mã nguồn mở khả thi.
Tất cả mã và dữ liệu được sử dụng trong công trình này
được công bố tại https:
//github.com/HKUNLP/ChunkLlama .
1. Giới thiệu
Khả năng hiểu và xử lý thông tin ngữ cảnh dài là điều cần thiết cho các mô hình ngôn ngữ lớn (LLMs) (OpenAI, 2023; Touvron et al., 2023a;b; Bai et al., 2023; Anthropic, 2023) để phục vụ hiệu quả cho một loạt các ứng dụng. Những ứng dụng này bao gồm phân tích và phản hồi các truy vấn trong các tệp PDF lớn, giữ lại lịch sử đối thoại mở rộng, và trao quyền cho các chatbot tương tác (Wei et al., 2023; Lee et al., 2023; Rula & D'Souza, 2023; Saad-Falcon et al., 2023; Lv et al., 2024).

*Công việc được thực hiện trong thời gian thực tập tại Alibaba Group1The University of Hong Kong2Alibaba Group3Fudan University. Liên hệ: Chenxin An <cxan23@connect.hku.hk >.

Proceedings of the 41stInternational Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).

Những tiến bộ gần đây đã cho thấy rằng khả năng ngữ cảnh dài có thể được cải thiện bằng cách tiếp tục huấn luyện một mô hình ngữ cảnh ngắn trên các chuỗi văn bản dài (Ruoss et al., 2023; Rozière et al., 2023). Hiệu suất ấn tượng của Llama2 Long (Xiong et al., 2023), được huấn luyện từ một hỗn hợp dữ liệu văn bản dài và corpus huấn luyện trước Llama2 (Touvron et al., 2023b) gốc, đứng như một minh chứng cho phương pháp này. Tuy nhiên, do khả năng tiếp cận hạn chế của các corpus huấn luyện này và chi phí cấm kỵ của việc tinh chỉnh ngữ cảnh dài, các mô hình mã nguồn mở hiện tại thường thiếu hiệu suất khi so sánh với các đối tác độc quyền, và thường chỉ có sẵn ở kích thước nhỏ hơn (ví dụ: 7B/13B).

Với những hạn chế này, các phương pháp không yêu cầu huấn luyện bổ sung cho việc mở rộng ngữ cảnh trong LLMs trở nên đặc biệt hấp dẫn. Các phương pháp không cần huấn luyện gần đây, bao gồm LM-infinite (Han et al., 2023) và StreamingLLM (Xiao et al., 2023), đã cho thấy rằng các LLMs được huấn luyện trên một cửa sổ ngữ cảnh hạn chế có thể xử lý hiệu quả văn bản có độ dài vô hạn (Zhang et al., 2023; 2024; Xiao et al., 2024; Qin et al., 2024). Giả định rằng các LLMs không thể khái quát hóa cho các văn bản dài hơn độ dài huấn luyện, những mô hình này xử lý các chuỗi mở rộng bằng cách giữ lại có chọn lọc thông tin cục bộ cần thiết. Những mô thức như vậy duy trì hiệu quả một Perplexity (PPL) thấp, tuy nhiên chúng mất đi các phụ thuộc tầm xa. Để giữ lại thông tin toàn cục, một góc nhìn khác là ngoại suy hiệu quả đến độ dài chuỗi vượt quá những gì gặp phải trong quá trình huấn luyện (Sun et al., 2022; Kazemnejad et al., 2023; Liu et al., 2023b; Chi et al., 2023). Các kỹ thuật phổ biến cho các mô hình dựa trên Llama, bao gồm Position Interpolation (PI) (Chen et al., 2023b) và NTK-Aware RoPE (NTK) (LocalLLaMA, 2023b;a), là những điều chỉnh của Rotary Positional Encodings (RoPE) (Su et al., 2022). Những mã hóa vị trí được mở rộng này đòi hỏi ít bước tinh chỉnh hơn so với RoPE gốc, và chi phí huấn luyện của chúng có thể được giảm thêm thông qua các phương pháp như YaRN (Peng et al., 2023) và CLEX (Chen et al., 2023a). Tuy nhiên, trong một thiết lập không cần huấn luyện, chúng tôi thấy rằng những phương pháp này thường dẫn đến sự gia tăng đáng kể trong PPL đặc biệt ở độ dài đầu vào nhiều hơn gấp đôi độ dài huấn luyện (§4, Bảng 1).

Trong bài báo này, chúng tôi giới thiệu Dual Chunk Attention (DCA), một khung framework mới không cần huấn luyện để ngoại suy cửa sổ ngữ cảnh của LLMs. Chúng tôi tránh việc giảm tỷ lệ tuyến tính các chỉ số vị trí hoặc tăng tần số cơ sở trong RoPE (Su et al., 2022). Thay vào đó, chúng tôi chọn sử dụng lại các chỉ số vị trí gốc với embedding của chúng từ mô hình huấn luyện trước, nhưng thiết kế lại việc xây dựng ma trận vị trí tương đối theo cách có thể phản ánh chính xác vị trí tương đối của hai token một cách trung thực nhất có thể. Được truyền cảm hứng từ các mô hình attention dựa trên chunk hiệu quả (Child et al., 2019; Song et al., 2023; Ratner et al., 2023; He et al., 2024), DCA phân đoạn các tính toán self-attention cho một chuỗi dài thành các chunk nhỏ, mỗi chunk nhỏ hơn kích thước của cửa sổ huấn luyện trước. DCA bao gồm ba thành phần: (1) intra-chunk attention, được thiết kế riêng để xử lý các token trong cùng một chunk; (2) inter-chunk attention, để xử lý các token giữa các chunk riêng biệt; và (3) successive chunk attention, để xử lý các token trong các chunk liên tiếp, riêng biệt. Những cách xử lý tương ứng này giúp mô hình nắm bắt hiệu quả cả các phụ thuộc tầm xa và tầm gần trong một chuỗi. Ngoài ra, việc tính toán attention dựa trên chunk có thể được tích hợp liền mạch với Flash Attention 2 (Dao et al., 2022; Dao, 2023), một yếu tố quan trọng cho việc mở rộng ngữ cảnh dài trong cộng đồng mã nguồn mở.1

Chúng tôi trình bày một đánh giá toàn diện về các mô hình của chúng tôi trên một loạt đa dạng các tác vụ bao gồm mô hình hóa ngôn ngữ, truy xuất passkey, và các ứng dụng ngữ cảnh dài thực tế trải dài trả lời câu hỏi (Pang et al., 2022; Kočiský et al., 2018; Dasigi et al., 2021; An et al., 2023) và tóm tắt (Zhong et al., 2021). Khác với công việc trước đây thường giới hạn ở việc xác minh trên các mô hình 7B/13B, hiệu quả huấn luyện đáng kể của phương pháp chúng tôi làm cho việc xác thực trên các mô hình 70B trở nên khả thi, đảm bảo kết luận vững chắc. Để xác minh khả năng ngữ cảnh dài của mô hình độc lập với khả năng tiếp xúc dữ liệu trong quá trình huấn luyện trước, chúng tôi đã sử dụng chính bài báo này làm đầu vào và tạo ra một loạt câu hỏi cho các mô hình.2Kết quả thực nghiệm của chúng tôi tiết lộ những hiểu biết sau:

1.Ngoại suy . Về mô hình hóa ngôn ngữ, DCA đánh dấu một tiến bộ đáng kể cho các phương pháp không cần huấn luyện. Nó đầu tiên cho thấy rằng các LLMs với cửa sổ ngữ cảnh 4k có thể được mở rộng đến hơn 32k mà không cần huấn luyện, duy trì sự gia tăng không đáng kể trong PPL, trong khi các phương pháp trước đây thường thất bại ở độ dài ngữ cảnh vượt quá 8k. Hơn nữa, chúng tôi chứng minh rằng Llama2 70B, khi được tích hợp với DCA, thể hiện khả năng ngoại suy đặc biệt để xử lý kích thước ngữ cảnh vượt quá 100k token.

2.Tính trực giao . DCA trực giao với các mã hóa vị trí được mở rộng phổ biến hiện tại như PI (Chen et al., 2023b) và NTK (LocalLLaMA, 2023b;a). Chúng tôi chứng minh thực nghiệm rằng các LLMs ngữ cảnh dài hiện tại, đã hỗ trợ cửa sổ ngữ cảnh 32k, có thể ngoại suy thêm đến độ dài ngữ cảnh 192k trong khi duy trì độ chính xác truy xuất passkey cao và perplexity thấp.

1Không có Flash Attention, số token đầu vào tối đa cho Llama2 7B/13B là khoảng 16k, và cho Llama2 70B, là 5k khi thử nghiệm trên hai GPU A100 80G trong các thí nghiệm của chúng tôi
2Chúng tôi mời những độc giả quan tâm xem xét kết quả trong Bảng 6,7

3.Hiểu biết Ngữ cảnh Dài . Chúng tôi đánh giá DCA trên một bộ benchmark hiểu biết ngữ cảnh dài trong cả thiết lập zero-shot và few-shot. Kết quả cho thấy rằng các mô hình không cần huấn luyện của chúng tôi đạt được hiệu suất có thể so sánh, hoặc thậm chí vượt qua, hiệu suất của các mô hình hiện đại được xây dựng thông qua huấn luyện liên tục tốn kém.

2. Bối cảnh
2.1. Mã hóa Vị trí
Embedding vị trí gốc từ mô hình Transformer (Vaswani et al., 2017) ánh xạ các chỉ số vị trí tuyệt đối đến không gian đặc trưng d-chiều, và kết hợp điều này vào lớp đầu vào. Đầu vào x, liên kết với chỉ số vị trí i, được biểu thị như: xi=x+f(i), trong đó f:N→Rd là hàm embedding (vị trí).

Một trong những phương pháp mã hóa vị trí phổ biến nhất cho LLMs là Rotary Positional Encoding (RoPE) (Su et al., 2022). RoPE tránh phương pháp thông thường của việc truyền thông tin vị trí vào lớp đầu vào. Thay vào đó, nó trực tiếp kết hợp thông tin này vào lớp attention. Đối với một chuỗi l token, chúng tôi ký hiệu các chỉ số vị trí cho keys và queries3 như sau:
Pk=Pq= [0,1, . . . , l −1]. (1)

Chúng tôi lạm dụng ký hiệu f cho hàm embedding của RoPE, chấp nhận một vector query q hoặc một vector key k, và chỉ số vị trí tương ứng làm đối số. Ví dụ, chúng ta có qi=f(q, Pq[i]) và kj=f(k, Pk[j]), trong đó [i] ký hiệu phần tử thứ i của danh sách. Trong trường hợp đơn giản nhất, chúng ta có P[i] =i. Hàm f4 xuất ra một vector query hoặc key đã được chỉnh sửa đóng gói chỉ số vị trí, đảm bảo rằng tích trong giữa query thứ i và key thứ j (với i≥j) nắm bắt thông tin vị trí tương đối Pq[i]−Pk[j]. Mặc dù RoPE nhận các chỉ số vị trí tuyệt đối làm đầu vào, kết quả của tích trong của q,k chỉ chứa thông tin vị trí tương đối (tức là, chúng ta có q⊤ikj=q⊤mknkhi m−n=i−j). Ma trận vị trí tương đối M được giới thiệu bởi RoPE trong quá trình self-attention có thể được mô tả như một ma trận Toeplitz, như được hiển thị trong Hình 1. Mỗi phần tử M[i][j] =Pq[i]−Pk[j] biểu thị vị trí tương đối giữa qi (query thứ i) và kj (key thứ j).

2.2. Ngoại suy của RoPE
Công việc gần đây (Chen et al., 2023b; Chowdhury & Caragea, 2023; Chen et al., 2023a) đã chứng minh rằng các LLMs với RoPE gốc thiếu khả năng ngoại suy độ dài mạnh mẽ, thường dẫn đến suy giảm hiệu suất khi

3Queries và keys thường được tạo ra bằng cách chiếu đầu vào x thông qua một lớp tuyến tính có thể học
4Một triển khai điển hình của f có thể được tìm thấy trong modeling-llama.py Dòng 211 apply rotary posemb()

2

--- TRANG 2 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

03214569872103458769102347658901236547801254367014325603214521034102301201001234567891011𝑃!:Position ids của 𝒌𝑃": Position ids của 𝒒
11109876543210
𝑃!𝑃"
101110

Hình 1. Trực quan hóa Ma trận Vị trí Tương đối M sử dụng RoPE tiêu chuẩn. Cửa sổ ngữ cảnh huấn luyện trước là 6 và độ dài chuỗi đầu vào là 12. Trục x Pk chỉ ra các chỉ số vị trí của keys, trong khi trục y Pq tương ứng với các chỉ số vị trí của queries. Mỗi phần tử ma trận M[i][j] đại diện cho độ lệch vị trí tương đối Pq[i]−Pk[j].

được thử nghiệm trên các chuỗi đầu vào dài hơn những gì thấy trong quá trình huấn luyện trước (Li et al., 2023b; Zhu et al., 2023). Các nghiên cứu gần đây (Chen et al., 2023b; Su, 2023; Jin et al., 2024) chủ yếu quy việc hạn chế này cho sự hiện diện của các vị trí tương đối chưa thấy trong giai đoạn huấn luyện trước và đề xuất thiết kế lại ma trận vị trí tương đối. Như được minh họa trong ví dụ trong Hình 1, mô hình được huấn luyện trên các chuỗi 6 token, trong khi suy luận được thực hiện trên một chuỗi 12 token. Sự khác biệt này có thể dẫn đến PPL cao vì các vị trí tương đối vượt quá 6 chưa bao giờ được huấn luyện. Các phương pháp trước đây, như PI và NTK, nhằm giảm thiểu vấn đề này bằng cách giảm độ lớn của M[i][j] để đảm bảo nó nằm trong phạm vi độ dài ngữ cảnh quan sát được trong quá trình huấn luyện. Ví dụ, áp dụng PI trong ví dụ này sẽ điều chỉnh các chỉ số vị trí bằng cách chia tỷ lệ: Pq[i]⇒Pq[i]/2 và Pk[j]⇒Pk[j]/2. Do đó, ma trận vị trí tương đối cũng được chia tỷ lệ: M[i][j] =M[i][j]/2. Ở đây, hệ số chia tỷ lệ 2 =126 được sử dụng để giảm tỷ lệ các vị trí tương đối, dẫn đến độ phân giải thông tin vị trí kém hơn và khả năng ngoại suy yếu.

3. Phương pháp
Trong phần này, chúng tôi mô tả chi tiết khung framework mới không cần huấn luyện Dual Chunk Attention của chúng tôi. Một ví dụ chạy của dual chunk attention được hiển thị trong hình 2. Phương pháp của chúng tôi bắt đầu từ intra-chunk attention (Hình 2 (a)) là một mô hình attention hiệu quả dựa trên chunk (Child et al., 2019; Song et al., 2023). Embedding vị trí của mỗi chunk dao động từ 0 đến kích thước chunk trong đó kích thước chunk được đặt nhỏ hơn độ dài huấn luyện trước. Mô hình intra-chunk attention thực tế có nghĩa là trực tiếp cắt ngắn đầu vào từ bên trái đến kích thước chunk loại bỏ thông tin từ các chunk trước đó. Việc cắt ngắn như vậy thường mang lại perplexity thấp (Xiao et al., 2023) nhưng mất thông tin tầm xa. Để giải quyết hạn chế này, chúng tôi triển khai inter-chunk attention (Hình 2(b)) cho phép tính toán attention giữa các chunk khác nhau, mặc dù với độ chính xác ít hơn cho các vị trí token xa. Cuối cùng, chúng tôi giới thiệu successive-chunk attention, một biến thể của inter-chunk attention được mô tả trong Hình 2 (c), được áp dụng cụ thể khi hai chunk liền kề để bảo toàn tính cục bộ. Một nghiên cứu phân tích để hiển thị cách các cơ chế attention này ảnh hưởng đến PPL và độ chính xác truy xuất passkey có thể được tìm thấy trong Hình 4.

3.1. Intra-Chunk Attention
Intra-Chunk Attention được sử dụng để tính toán tích trong của queries và keys trong cùng một chunk. Đối với một chuỗi dài có độ dài l, chúng tôi phân vùng chuỗi thành n=ls chunk, đảm bảo rằng các chỉ số vị trí trong mỗi chunk sẽ không vượt quá kích thước chunk s. Hình 2 (a) minh họa quá trình phân đoạn một chuỗi 12 token vượt quá độ dài huấn luyện trước 10 thành 2 chunk, với mỗi chunk bao gồm s= 6<10 token. Sau đó các chỉ số vị trí cho keys và queries được chia tỷ lệ trong kích thước chunk 6. Cụ thể, chúng ta có các chỉ số vị trí cho keys Pk= [0,1,2,3,4,5|{z}chunk 0,0,1,2,3,4,5|{z}chunk 1] và PIntraq=Pk, trong đó PIntraq có nghĩa là các chỉ số vị trí cho queries trong quá trình intra-chunk attention. Để hình thức hóa, trong intra-chunk attention, chúng tôi điều chỉnh các chỉ số vị trí cho queries và keys như sau:
PIntraq=Pk= [0,1, . . . , l −1] mod s. (2)

Đối với các chỉ số tuyệt đối i và j trong cùng một chunk tức là ⌊i/s⌋=⌊j/s⌋, thỏa mãn 0≤j≤i < l , phần tử M[i][j] được định nghĩa là sự khác biệt giữa các mã hóa vị trí của query và key:
M[i][j] =PIntraq[i]−Pk[j]. (3)

Khi ⌊i/s⌋=⌊j/s⌋, chúng tôi tính toán M[i][j] theo Eq. 3. M được tính toán của ví dụ trước đó trong đó chúng ta có độ dài chuỗi 12 và kích thước chunk 6, được minh họa trong Hình 2 (a). Điểm attention intra-chunk cho sự tương tác giữa query thứ i và key thứ j sau đó được tính như:
q⊤ikj=f(q, PIntraq[i])⊤f(k, Pk[j]). (4)

3.2. Inter-Chunk Attention
Để tổng hợp thông tin từ các chunk khác, chúng tôi giới thiệu Inter-Chunk Attention. Trong các LLMs dựa trên Llama, các chỉ số vị trí cho queries lớn hơn những của keys để phản ánh luồng thông tin từ trái sang phải, tức là, chúng ta có Pq[i]≥Pk[j] bất cứ khi nào i≥j. Sử dụng Pq=PIntraq và Pk để tính toán attention giữa các chunk khác nhau rõ ràng vi phạm tính chất này. Ví dụ, xem xét qs và k1 trong đó s là kích thước chunk, khoảng cách tương đối của chúng được cho bởi PIntraq[s] = 0 và Pk[1] = 1 là -1. Chúng tôi duy trì các chỉ số vị trí cho keys Pk xem xét KV cache và tìm kiếm một tập Pq mới trong quá trình inter-chunk attention, được ký hiệu là Pinterq.

Dựa trên Eq. 2, các chỉ số vị trí cho keys được lặp lại theo chu kỳ với chỉ số vị trí tối đa max(Pk) =s−1. Để đảm bảo rằng các queries có chỉ số vị trí lớn hơn tất cả keys từ các chunk trước đó, Một chiến lược đơn giản để phân biệt các queries xa là gán cho chúng một chỉ số vị trí lớn đáng kể, chẳng hạn như chỉ số vị trí tối đa trong quá trình huấn luyện trước c−1>max(Pk), trong đó c là độ dài ngữ cảnh huấn luyện trước:
PInterq= [c−1, c−1, . . . c−1| {z }l phần tử], (5)

Khi ⌊i/s⌋ ̸=⌊j/s⌋, chúng ta có thể đưa ra ma trận vị trí tương đối M với qi và kj từ các chunk riêng biệt như:
M[i][j] =PIntraq[i]−Pk[j] =c−1−Pk[j]≥c−s.
(6)

Như được phản ánh trong Hình 2 (b), chúng tôi gán PInterq với một giá trị không đổi c−1 = 9 cho tất cả các vị trí, lớn hơn chỉ số vị trí tối đa s−1 = 5 trong Pk. Chúng tôi hoàn thành phần còn lại của ma trận M để trống bởi intra-chunk attention với Eq. 6.

3.3. Successive-Chunk Attention
Successive-Chunk Attention có thể được xem như một trường hợp đặc biệt cho inter-chunk attention, được đề xuất để duy trì tính cục bộ của LLMs trong đó tính cục bộ có nghĩa là LLMs có xu hướng dựa rất nhiều vào các token lân cận để dự đoán token tiếp theo (Xiao et al., 2023; Han et al., 2023). Đơn giản sử dụng inter-chunk attention có thể không còn giữ vị trí tương đối chính xác giữa các token lân cận, dẫn đến suy giảm hiệu suất.

Như được hiển thị trong Hình 2(b), trong đó kích thước chunk là s= 6 và độ dài huấn luyện trước là c= 10 , key cuối cùng của chunk đầu tiên, k5, với Pk[5] = 5 , được theo sau bởi query đầu tiên của chunk thứ hai, q6, với chỉ số vị trí PInterq[6] = 9 . Mặc dù khoảng cách tuyệt đối của chúng là 1, khoảng cách tương đối giữa q6 và k5 là PInterq[6]−Pk[5] = 4 . Cấu hình này thách thức khả năng của mô hình để duy trì tính cục bộ trong cơ chế attention của nó.

May mắn thay, vấn đề này chỉ xảy ra giữa các chunk liên tiếp, vì vậy chúng tôi giới thiệu một successive-chunk attention mới để xử lý trường hợp này. Cụ thể, chúng tôi đề xuất duy trì tính cục bộ của w token lân cận thông qua điều chỉnh w chỉ số vị trí đầu tiên trong cho PInterq. Ví dụ, trong Hình 2 (c), cho ngữ cảnh huấn luyện trước c= 10 , kích thước chunk s= 6, và PInterq= [9,9,9,9,9,9|{z}chunk 0,9,9,9,9,9,9|{z}chunk 1], các chỉ số vị trí PSuccq có thể được đặt thành [6,7,8,9,9,9|{z}chunk 0,6,7,8,9,9,9|{z}chunk 1] để tính toán attention giữa các chunk liên tiếp, nếu chúng ta giữ một cửa sổ cục bộ w= 4. Chính thức, với kích thước chunk s, kích thước huấn luyện trước c và cửa sổ cục bộ w chúng ta có:
PSuccq= [w phần tửz }| {s, s+ 1, . . . , s +w−1, c−1, . . . , c −1| {z }giống nhau cho tất cả chunks],(7)

trong đó w có nghĩa là kích thước cửa sổ cục bộ và có thể được đặt trực tiếp thành sự khác biệt giữa độ dài huấn luyện trước và kích thước chunk c−s. Đối với i, j từ các chunk liên tiếp, kết quả tính toán của M[i][j] sử dụng PSuccq và Pk được phản ánh trong Hình 2 (c) trong đó bóng có nghĩa là cửa sổ cục bộ kết quả. Eq 7 đảm bảo rằng w keys lân cận có khoảng cách gần nhất với query hiện tại.

Bằng cách kết hợp intra-chunk, inter-chunk, và successive-

4

--- TRANG 3 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

(a) Intra-chunk Attention(b) Inter-chunk Attention(c) Successive-Chunk Attention03214599999921034888888102377777701266666601555555044444403214521034102301201012345012345!!: Position ids của "999999999999
!!!"#$%&'Chunk1!"#$%&': Position ids của "(inter-chunk attention)Chunk2
!=6032145210341023012010032145210341023012010012345012345!!: Position ids của "543210543210
!!!"#$%&'Chunk1!"#$%&': Position ids của "(intra-chunkattention)Chunk2
!=6032145698799210345876881023476577012365466012543550143244032145210341023012010012345012345!!: Position ids của "999876999876
!!!"#$%&'Chunk1Chunk2
!"#$%&': Position ids của "(intra-chunkattention)543210543210!"()**!"#$%(&: Position ids của "(inter-chunkattention)!=4"=6

Hình 2. Trực quan hóa Ma trận Vị trí Tương đối M sử dụng Dual Chunk Attention (DCA), với kích thước chunk s= 6, kích thước cửa sổ huấn luyện trước c= 10 , và kích thước cửa sổ cục bộ w= 4 được chú thích bởi bóng trong (c). Chuỗi được phân đoạn thành các chunk để đảm bảo rằng các vị trí tương đối không vượt quá 9. Phần tử ma trận M[i][j] đại diện cho vị trí tương đối giữa vector query thứ i q và vector key thứ j k. Khác với các chỉ số vị trí gốc cho q,k trong RoPE, DCA sử dụng các tập chỉ số vị trí riêng biệt Pk, PIntraq (được định nghĩa trong Eq. 2), PInterq (được định nghĩa trong Eq. 5), PSuccq (được định nghĩa trong Eq. 7) để tính toán các khoảng cách tương đối trong các phần khác nhau của M.

chunk attention, cuối cùng chúng tôi tính toán M[i][j] như:
M[i][j] =

PIntraq[i]−Pk[j]nếu⌊i/s⌋ − ⌊j/s⌋= 0
PSuccq[i]−Pk[j]nếu⌊i/s⌋ − ⌊j/s⌋= 1
PInterq[i]−Pk[j]nếu⌊i/s⌋ − ⌊j/s⌋>1.

Tích trong của q,k trong DCA do đó được định nghĩa như:
qTikj=

f(q, PIntraq[i])Tf(k, Pk[j]),nếu⌊i/s⌋ − ⌊j/s⌋= 0
f(q, PSuccq[i])Tf(k, Pk[j]),nếu⌊i/s⌋ − ⌊j/s⌋= 1
f(q, PInterq[i])Tf(k, Pk[j]),nếu⌊i/s⌋ − ⌊j/s⌋>1,
(8)

3.4. Chuẩn hóa
Lớp Softmax Các tính toán tích trong trong DCA được hình thức hóa như được hiển thị trong Phương trình 8. Sau đó, một hàm softmax được áp dụng để chuẩn hóa các tích trong được tính toán:
pi=softmax (q⊤ik0√d,q⊤ik1√d, . . . ,qi⊤ki√d
).(9)
trong đó d ký hiệu chiều của các trạng thái ẩn.

Flash Attention Mã giả kiểu PyTorch về cách tích hợp DCA với Flash Attention 2 (Dao, 2023), có thể được tìm thấy trong Thuật toán 1. Lời giải thích và phân tích độ phức tạp của mã có thể được tìm thấy trong Phụ lục §A.3. Với Flash Attention, DCA đạt được việc sử dụng bộ nhớ GPU và tốc độ suy luận có thể so sánh với self-attention gốc trong Llama. Kết quả có thể được tìm thấy trong Hình 3.

4. Thí nghiệm
Chúng tôi đánh giá khung framework của chúng tôi, DCA, trên các biến thể khác nhau của Llama2 (Touvron et al., 2023b), cụ thể là các mô hình 7B, 13B, và 70B, cùng với các đối tác chat của chúng, có ngữ cảnh huấn luyện trước 4k. Mô hình dựa trên Llama2 của chúng tôi được ký hiệu là CHUNK LLAMA 2. Ngoài ra, chúng tôi áp dụng DCA cho hai mô hình ngữ cảnh dài mã nguồn mở phổ biến: (1) Together-32k (Together, 2023)5: Mô hình này sử dụng Positional Interpolation (PI) làm mã hóa vị trí của nó. Phiên bản được tăng cường DCA của mô hình này được gọi là ChunkTogether. (2) CodeLlama (Rozière et al., 2023)6: Mô hình này áp dụng NTK-Aware RoPE. Sau khi áp dụng DCA, mô hình kết quả được gọi là ChunkCodeLlama.

4.1. Thiết lập Thí nghiệm
DCA có thể được triển khai bằng một monkey patch để thay thế mã suy luận của LlamaAttention gốc . Nhờ

5https://huggingface.co/togethercomputer/LLaMA-2-7B-32K
6https://huggingface.co/codellama

Flash Attention 2 (Dao, 2023), đối với các biến thể 7B/13B của CHUNK LLAMA 2, chúng tôi chỉ cần một GPU NVIDIA A100-80G duy nhất cho suy luận. Khi mở rộng lên các mô hình 70B, hai GPU A100 là đủ để quản lý suy luận trong độ dài ngữ cảnh 16k. Kích thước chunk s có thể được đặt điển hình là 34 độ dài huấn luyện và đối với Llama2, giá trị này là 3072. Số lượng chunk phụ thuộc vào độ dài chuỗi đầu vào.

Ngoài các đánh giá không cần huấn luyện, chúng tôi cũng cung cấp các mô hình được tinh chỉnh từ các checkpoint Llama2 7B/13B. Quá trình tinh chỉnh này tận dụng chỉ các cuộc hội thoại dài với 16k token đầu vào, theo Vicuna (LMSYS, 2023) và LongChat (Li et al., 2023a). Tập dữ liệu huấn luyện được lấy từ ShareGPT7 và AlpacaGPT4 (Taori et al., 2023). Đối với dữ liệu được tạo từ ShareGPT, chúng tôi cụ thể tuyển chọn một tập con bằng cách trích xuất các phản hồi được tạo bởi GPT-4, và các đối thoại vượt quá 4k token về độ dài. Việc lựa chọn này dẫn đến một tập hợp 5,405 trường hợp huấn luyện.

Chúng tôi tuân thủ các siêu tham số huấn luyện như được chỉ định trong kho LongChat8. Chúng tôi tiếp tục tinh chỉnh Llama2 với hơn 16k bước với kích thước batch là 1. Quá trình tinh chỉnh tốn khoảng 40 giờ GPU cho mô hình 7B và 60 giờ GPU cho biến thể 13B.

Tập dữ liệu Chúng tôi đánh giá hiệu suất mô hình hóa ngôn ngữ chuỗi dài của CHUNK LLAMA 2 trên tập dữ liệu corpus sách PG19 (Rae et al., 2020), với độ dài ngữ cảnh dao động từ 4k đến 192k token. Đối với các mô hình 7B và 13B, chúng tôi sử dụng cửa sổ trượt 256, phù hợp với công việc trước đây (Peng et al., 2023; Chen et al., 2023c). Đối với các mô hình 70B, chúng tôi điều chỉnh kích thước cửa sổ trượt thành 2048 và khi xử lý các ngữ cảnh vượt quá 96k token, chúng tôi điều chỉnh cửa sổ trượt thành một nửa độ dài đầu vào xem xét thời gian chạy. Đối với các thí nghiệm few-shot, chúng tôi theo các thiết lập trong Llama2 Long (Xiong et al., 2023). Cụ thể, chúng tôi đánh giá hiệu suất 0-shot của CHUNK LLAMA 2 trên NarrativeQA (Kočiský et al., 2018), 1-shot trên QMSum (Zhong et al., 2021), 2-shot trên QuALITY (Pang et al., 2022) , và 2-shot cho Qasper (Dasigi et al., 2021). Đối với các thí nghiệm zero-shot, chúng tôi thử nghiệm CHUNK LLAMA 2 trên 4 tác vụ đóng từ L-Eval (An et al., 2023): TOFEL, QuALITY (được làm sạch từ Pang et al. (2022)), Coursera, SFiction. Chúng tôi cũng xác thực mô hình của chúng tôi trên truy xuất passkey được sử dụng trong Mohtashami & Jaggi (2023). Đánh giá về truy xuất passkey (Mohtashami & Jaggi, 2023) có thể được tìm thấy trong Phụ lục A.1.

Baseline Chúng tôi so sánh với các mô hình ngữ cảnh dài mã nguồn mở phổ biến có sẵn trong Huggingface Transformers9. Mô hình Cơ sở : Focused Transformer 3B (Tworkowski et al., 2023), CLEX 7B (Chen et al., 2023a), YaRN 7B/13B (Peng et al., 2023), MPT 30B (MosaicML, 2023b;a), Together 7B (Together, 2023), CodeLlama 7B (Rozière et al., 2023),

7https://sharegpt.com/
8https://github.com/DachengLi1/LongChat
9trước ngày 1 tháng 12, 2023

5

--- TRANG 4 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Longlora 13B/70B (Chen et al., 2023c), và Llama2 Long 7B/13B/70B (Xiong et al., 2023). Mô hình Chat : LongChat-v1.5-32k 7B (Li et al., 2023a), Vicuna-v1.5-16k (LMSYS, 2023) 7B/13B, Longlora-Chat 70B (Chen et al., 2023c), và Llama2 Long-Chat 70B (Xiong et al., 2023).

4.2. Mô hình hóa Ngôn ngữ Chuỗi Dài
Bảng 1 trình bày điểm Perplexity (PPL) trên tập xác thực PG19 cho các mô hình không cần huấn luyện và được tinh chỉnh khác nhau. Tất cả các baseline này đều dựa trên Llama. Chúng tôi chứng minh rằng phương pháp không cần huấn luyện tốt nhất trước đây thất bại với độ dài ngữ cảnh 16k. Tuy nhiên, CHUNK LLAMA 2 có thể ngoại suy đến cửa sổ ngữ cảnh hơn 32k, chỉ với sự gia tăng 0,02 trong PPL. Chúng tôi tiếp tục chứng minh rằng CHUNK LLAMA 2 vượt qua kết quả của các mô hình được tinh chỉnh trong độ dài ngữ cảnh 16k. Đáng chú ý, biến thể 70B của CHUNK LLAMA 2 thể hiện sự nhất quán trong hiệu suất trên một loạt độ dài ngữ cảnh, đạt điểm PPL chỉ tăng nhẹ từ 5,18 đến 5,59.

Chúng tôi cũng tiết lộ rằng DCA có thể được tích hợp với các mô hình đã được huấn luyện thêm trên các ngữ cảnh dài hơn với PI (Chen et al., 2023b) hoặc NTK-Aware RoPE (LocalLLaMA, 2023b;a) và hỗ trợ độ dài ngữ cảnh 192k trong Bảng 2. Những kết quả khuyến khích được quan sát với 64k token đầu vào khuyến khích chúng tôi thử nghiệm CHUNK LLAMA 2 trên các ngữ cảnh thậm chí còn dài hơn. Chúng tôi tiến hành thử nghiệm mô hình với độ dài token đầu vào mở rộng từ 32k đến 192k (Bảng 2). Đối với Llama2 70B, DCA đã chứng minh hiệu quả trong việc mở rộng cửa sổ ngữ cảnh đến 96k token. Sự mở rộng này được đạt được chỉ với sự gia tăng nhỏ 0,56 PPL so với hiệu suất gốc của nó ở độ dài ngữ cảnh 4k. Cùng với việc đánh giá CHUNK LLAMA 2, chúng tôi cũng áp dụng DCA cho các mô hình ngữ cảnh dài hiện tại sử dụng các mã hóa vị trí khác nhau. Tích hợp DCA với các mô hình ngữ cảnh dài hiện tại chỉ yêu cầu điều chỉnh kích thước chunk trong khung DCA. Chúng tôi cho thấy rằng CodeLlama và nhánh Llama2 của Together có thể được mở rộng hiệu quả đến độ dài ngữ cảnh 192k sử dụng DCA với kích thước chunk 24k. Chúng tôi tiếp tục xác thực hiệu suất của mô hình chúng tôi trên tác vụ truy xuất passkey (Mohtashami & Jaggi, 2023). Kết quả cũng chỉ ra rằng bằng cách tích hợp DCA với các mô hình ngữ cảnh dài hiện tại, hệ thống được tăng cường duy trì độ chính xác truy xuất 90% với độ dài ngữ cảnh mở rộng lên đến 192k token (Hình 7).

4.3. Tác vụ Thực tế
Trái ngược với các nghiên cứu trước đây thường xác thực phương pháp của chúng dựa trên PPL, chúng tôi cũng áp dụng khung framework của chúng tôi cho cả mô hình cơ sở và mô hình chat được tinh chỉnh hướng dẫn trên các benchmark thực tế.

Kết quả Few-shot Chúng tôi xác thực DCA trên các mô hình chưa trải qua tinh chỉnh hướng dẫn trong thiết lập học few-shot.

Bảng 1. Đánh giá Perplexity (PPL) trên tập xác thực PG19 (Rae et al., 2020). Kết quả được làm nổi bật bằng màu đỏ chỉ ra Perplexity đã tăng hơn 1,0 so với giá trị gốc của nó ở độ dài ngữ cảnh huấn luyện trước 4096. ReRoPE (Su, 2023) gặp vấn đề OOM (Hết Bộ nhớ) với 16k token đầu vào vì hiện tại nó không tương thích với Flash Attention. Các hệ số chia tỷ lệ trong PI và NTK được thay đổi động.

MôhìnhCửa sổ Ngữ cảnh Đánh giá
4096 8192 16384 32768 65536

Được tinh chỉnh
Longlora-32k 7B 8.14 7.85 7.70 7.80 91.79
Together-32k 7B 8.21 7.95 7.76 7.64 >102
CodeLlama-16k 7B 8.93 8.64 8.44 8.36 8.65
CLEX-16k 7B 8.84 7.66 7.43 7.57 8.73

Không cần huấn luyện
Llama2 7B 7.87 >102>102>102>102
Llama2-ReRoPE 7B 7.94 7.75 OOM OOM OOM
Llama2-PI 7B 7.87 9.19 15.11 >102>102
Llama2-PI-Yarn 7B 7.87 8.80 11.75 42.42 >102
Llama2-NTK 7B 7.87 11.98 26.12 58.91 >102
Llama2-NTK-Yarn 7B 7.87 8.06 9.82 11.74 41.57
CHUNK LLAMA 2 7B 7.87 7.67 7.64 7.89 15.87
CHUNK LLAMA 2 13B 7.15 6.95 6.99 7.90 15.14
CHUNK LLAMA 2 70B 5.24 5.18 5.21 5.30 5.59

Llama3
Llama3 8B 9.04 8.71 78.88 >102>102
Llama3 70B 5.36 5.16 >102>102>102
CHUNK LLAMA 3 8B 9.04 8.71 8.61 8.62 8.95
CHUNK LLAMA 3 70B 5.36 5.16 5.14 5.14 5.21

định. Kết quả được tóm tắt trong Bảng 3. Thiết lập thí nghiệm giống như trong Xiong et al. (2023). Nếu các prompt đầu vào vượt quá độ dài đầu vào 16k token, chúng được cắt ngắn từ phía bên trái. Hầu hết các trường hợp thử nghiệm trong NarrativeQA (Kočiský et al., 2018) và QMSum (Zhong et al., 2021) có độ dài đầu vào vượt quá 16k token, trong khi độ dài của các trường hợp thử nghiệm trong Qasper (Dasigi et al., 2021) và QuALITY (Pang et al., 2022) thường dưới 8k token. Không có bất kỳ chi phí huấn luyện nào, cả biến thể 7B/13B của CHUNK LLAMA 2 đều đạt được kết quả có thể so sánh với các baseline được tinh chỉnh phổ biến như YaRN (Peng et al., 2023), MPT (MosaicML, 2023b), Together (Together, 2023), dựa trên RoPE được mở rộng trước đây (Chen et al., 2023b; LocalLLaMA, 2023b) hoặc Alibi (Press et al., 2022).

Khác với các nghiên cứu trước đây thường xác minh kỹ thuật của chúng trên các phiên bản nhỏ hơn của Llama2, chúng tôi cũng trình bày kết quả cho DCA được ghép nối với Llama2 70B, trong đó DCA cải thiện hiệu suất trung bình hơn 8,0 điểm so với mô hình Llama2 gốc với độ dài huấn luyện 4k. Do chi phí ngày càng tăng của việc tinh chỉnh ngữ cảnh dài cho các mô hình 70B, chúng tôi không tìm thấy nhiều baseline 70B mã nguồn mở. Chúng tôi so sánh phương pháp không cần huấn luyện của chúng tôi với baseline 70B mạnh mẽ, Longlora (Chen et al., 2023c), sử dụng tinh chỉnh hiệu quả dựa trên LoRA (Hu et al., 2021) dựa trên tập dữ liệu Redpajama (Computer, 2023) cho 1000 bước hỗ trợ cửa sổ ngữ cảnh 32k. Kết quả chứng minh rằng

6

--- TRANG 5 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Bảng 2. Đánh giá Perplexity trên tập xác thực PG19 (Rae et al., 2020) với độ dài ngữ cảnh lên đến 192k token. Chúng tôi thử nghiệm DCA trên Llama2 70B cùng với 2 mô hình được huấn luyện trước thêm phổ biến sử dụng PI và NTK. Kết quả được làm nổi bật bằng màu đỏ chỉ ra PPL đã tăng hơn 1,0 so với giá trị gốc của nó ở độ dài ngữ cảnh huấn luyện trước 4096.

MôhìnhMã hóa Ngữ cảnhCửa sổ Ngữ cảnh Đánh giá
Vị tríHuấn luyện4k 32k 64k 96k 128k 160k 192k

Llama2 7B RoPE 4k 7.87 >102>102>102>102>102>102
CHUNK LLAMA 2 7B RoPE 4k 7.87 7.89 15.87 43.57 96.21 >102>102
Llama2 70B RoPE 4k 5.24 >102>102>102>102>102>102
CHUNK LLAMA 2 70B RoPE 4k 5.24 5.30 5.59 5.80 6.12 6.52 7.05
Llama3 8B RoPE 8k 9.04 >102>102>102>102>102>102
CHUNK LLAMA 3 8B RoPE 8k 9.04 8.61 8.62 8.95 9.43 10.04 10.66
Llama3 70B RoPE 8k 5.36 >102>102>102>102>102>102
CHUNK LLAMA 3 70B RoPE 8k 5.36 5.14 5.14 5.21 5.32 5.40 5.45
CodeLlama 7B NTK 16k 8.93 8.36 8.65 9.14 9.87 15.68 24.78
ChunkCodeLlama 7B NTK 16k 8.93 8.36 8.13 8.33 8.66 9.30 9.83
Together 7B PI 32k 8.21 7.64 >102>102>102>102>102
ChunkTogether 7B PI 32k 8.21 7.64 7.59 7.64 7.67 7.74 7.83

Bảng 3. So sánh giữa các mô hình cơ sở mã nguồn mở phổ biến (khối đầu tiên) và mô hình độc quyền (khối cuối cùng) trên bốn benchmark nghiên cứu trên tập xác thực của chúng. Chúng tôi gạch dưới kết quả tốt nhất trong mỗi khối. Kết quả vượt qua mô hình được tinh chỉnh mã nguồn mở tốt nhất trước đây được in đậm. Llama2 Long đã được huấn luyện với tổng cộng 400B token qua 100,000 bước. Độ dài prompt tối đa được phép được đặt thành 16,384 token.†: kết quả được lấy từ Xiong et al. (2023) Chúng tôi sử dụng prompt đơn giản nhất: long-document Question:... Answer: . Các ví dụ trong ngữ cảnh được chọn ngẫu nhiên từ tập huấn luyện, và chúng tôi cũng có một thảo luận về việc lựa chọn các ví dụ trong ngữ cảnh trong Phụ lục §A.4.

MôhìnhHuấn luyện NarrativeQA Qasper QuALITY QMSumTrung
ThêmF1 (0-shot) F1 (2-shot) EM (2-shot) R-g (1-shot)bình
ngữ cảnhhuấn luyện

FoT 3B†✓ 8k 16.3 15.4 20.5 10.6 15.7
Yarn 7B†✓ 128k 20.9 26.2 32.3 11.4 22.7
Together 7B†✓ 32k 23.3 27.3 41.2 12.6 26.1
Yarn 13B†✓ 128k 23.4 27.1 46.4 11.9 27.2
Longlora 13B ✓ 32k 25.8 26.4 48.9 15.1 29.1
MPT 30B†✓ 8k 22.9 29.0 41.5 10.3 25.9
Llama2-DynNTK 70B ✗ 4k 11.1 27.8 60.9 7.8 26.9
Llama2 70B†✗ 4k 25.7 27.5 53.0 11.9 29.5
Longlora 70B ✓ 32k 34.2 29.0 69.9 15.6 37.2
CHUNK LLAMA 2 7B ✗ 4k 20.0 28.2 35.6 14.7 24.6
CHUNK LLAMA 2 13B ✗ 4k 26.3 29.3 47.9 15.2 29.7
CHUNK LLAMA 2 70B ✗ 4k 32.5 29.6 73.2 16.0 37.8
CHUNK LLAMA 3 8B ✗ 8k 27.4 30.5 52.6 15.4 31.5
CHUNK LLAMA 3 70B ✗ 8k 33.7 33.1 75.4 16.0 39.5

mô hình độc quyền
Llama2 Long 7B†✓ 32k 21.9 27.8 43.2 14.9 27.0
Llama2 Long 13B†✓ 32k 25.6 31.2 57.6 15.7 32.5
Llama2 Long 70B†✓ 16k 30.9 35.7 79.7 16.5 40.7

mô hình DCA 70B đạt được hiệu suất có thể so sánh (37,8 so với 37,2) không yêu cầu bước huấn luyện nào.

So sánh với baseline độc quyền mạnh mẽ, Llama2 Long (Xiong et al., 2023), đã được huấn luyện với tổng cộng 400 tỷ token (corpus huấn luyện trước Llama2 và dữ liệu văn bản dài mới) qua 100,000 bước, khoảng cách hiệu suất cho tất cả kích thước mô hình thường trong phạm vi 3 điểm. Các ví dụ trong ngữ cảnh được sử dụng trong thí nghiệm này được chọn ngẫu nhiên từ tập huấn luyện. Chúng tôi cũng đã thử các cách khác để chọn ví dụ, và chi tiết được bao gồm trong Phụ lục A.4.

Kết quả Zero-shot Ngoài việc xác minh DCA trên mô hình cơ sở, chúng tôi cũng áp dụng DCA trên phiên bản chat của Llama2 (với tinh chỉnh hướng dẫn) trong kịch bản học zero-shot. Cụ thể, chúng tôi thử nghiệm mô hình của chúng tôi trên bốn tác vụ đóng từ L-Eval (An et al., 2023) với độ dài đầu vào đa dạng dao động từ 3k đến 27k. Tất cả các tập dữ liệu này sử dụng Exact Match (EM) làm thước đo đánh giá. Nhìn chung, các kết luận tương tự như đánh giá few-shot. Các mô hình không cần huấn luyện 7B/13B của chúng tôi cho thấy hiệu suất có thể so sánh với các mô hình mã nguồn mở có huấn luyện thêm. Đáng chú ý, trong các thí nghiệm zero-shot, chúng tôi chứng minh sự cải thiện đáng kể so với phiên bản Chat của Longlora 70B (Chen et al., 2023c). Hơn nữa, khi so sánh với các mô hình độc quyền như GPT-3.5 với ngữ cảnh 16k token và chat

7

--- TRANG 6 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Bảng 4. So sánh với các mô hình chat mã nguồn mở (khối đầu tiên) và mô hình độc quyền (khối cuối cùng) trên 4 tác vụ đóng với độ dài đầu vào khác nhau từ L-Eval (An et al., 2023). Chúng tôi gạch dưới kết quả tốt nhất trong mỗi khối. Kết quả vượt qua mô hình được tinh chỉnh mã nguồn mở tốt nhất trước đây được in đậm . 'dialogues' có nghĩa là hỗn hợp ShareGPT và AlpacaGPT4 được sử dụng trong huấn luyện của chúng tôi. Llama2-PI-SFT và Llama2-NTK-SFT là các mô hình được huấn luyện với cùng dữ liệu và bước huấn luyện với CHUNK LLAMA 2.‡: kết quả được lấy từ Xiong et al. (2023).

MôhìnhCorpus Ngữ cảnhTOFEL QuALITY Coursera SFictionTrung
Tinh chỉnhHuấn luyện(3k∼5k) (4k ∼9k) (5k ∼17k) (6k ∼27k)bình

Llama2-Chat 7B ✗ 4k 51.67 37.62 29.21 60.15 48.74
Llama2-DynNTK 7B ✗ 4k 52.27 30.69 13.95 57.02 38.48
Longchat-v1.5-32k 7B ShareGPT 32k 39.77 37.62 32.99 57.02 41.85
Llama2-PI-SFT 7B Dialogues 16k 56.13 38.61 36.19 53.90 46.20
Llama2-NTK-SFT 7B Dialogues 16k 53.90 38.11 34.01 64.06 47.51
Vicuna-v1.5-16k 7B ShareGPT 16k 55.39 39.60 38.66 60.15 48.45
Llama2-Chat 13B ✗ 4k 60.96 42.57 35.75 54.68 48.99
Llama2-DynNTK 13B ✗ 4k 62.45 33.16 37.06 60.93 48.40
Vicuna-v1.5-16k 13B ShareGPT 16k 68.40 53.96 40.69 61.71 56.19
Longlora-Chat 70B LongAlpaca 32k 71.37 55.45 44.76 67.96 59.88

Không cần huấn luyện
CHUNK LLAMA 2-Chat 7B ✗ 4k 57.62 35.14 32.12 61.72 46.64
CHUNK LLAMA 2-Chat 13B ✗ 4k 66.54 43.06 41.56 57.03 52.04
CHUNK LLAMA 2-Chat 70B ✗ 4k 82.15 60.39 48.54 61.72 63.20

Llama3
CHUNK LLAMA 3-Instruct 8B ✗ 8k 83.27 63.86 56.24 70.31 68.42
CHUNK LLAMA 3-Instruct 70B ✗ 8k 84.75 82.17 76.88 75.78 79.89

Được tinh chỉnh
CHUNK LLAMA 2-Chat 7B Dialogues 16k 62.08 41.58 39.68 64.06 51.85
CHUNK LLAMA 2-Chat 13B Dialogues 16k 65.42 53.96 44.76 65.62 57.94

mô hình độc quyền
GPT3.5-16k-0613 Không biết – 78.43 61.38 63.51 64.84 67.03
Claude1.3-100k Không biết – 83.64 60.03 73.76 72.65 72.52
Llama2 Long-Chat 70B‡Long doc+diag 16k 81.8 – 52.9 – –

phiên bản của Llama2 Long, kết quả cho thấy rằng mô hình chat Llama2 70B có thể được mở rộng trực tiếp đến cửa sổ ngữ cảnh 16k mà không cần huấn luyện bổ sung với DCA, đạt 94% hiệu suất của gpt-3.5-turbo-16k . Chúng tôi cũng chứng minh rằng hiệu suất của mô hình chúng tôi có thể được tăng cường thông qua tinh chỉnh bổ sung trên dữ liệu đối thoại dài theo phương pháp được sử dụng bởi Vicuna (LMSYS, 2023) và Longchat (Li et al., 2023a), cả hai đều là baseline được tinh chỉnh phổ biến sử dụng ShareGPT. Với huấn luyện thêm,CHUNK LLAMA 2-Chat vượt trội hơn mô hình 13B tốt nhất trước đây, Vicuna-v1.5-13b-16k, với biên độ đáng kể 1,75 điểm.

4.4. Phân tích
Hiệu quả Trong hình 3, thời gian suy luận và bộ nhớ GPU của (a) cơ chế self-attention gốc như được triển khai trong PyTorch, Flash Attention (Dao, 2023), và DCA đề xuất của chúng tôi (được tích hợp với Flash Attention) được đánh giá trên các độ dài prompt khác nhau. Những thí nghiệm này được chạy trên một GPU NVIDIA 80G A100 sử dụng Llama2 7B. Prompt dài đầu vào là từ NarrativeQA (Kočiský et al., 2018). Chúng tôi tiến hành 20 thử nghiệm và báo cáo hiệu suất trung bình. Không có Flash Attention, chúng tôi quan sát rằng độ dài đầu vào tối đa có thể quản lý bởi một GPU duy nhất là khoảng từ 12k đến 16k token. DCA duy trì việc tiêu thụ bộ nhớ GPU và tốc độ suy luận tương tự, mà không thêm chi phí đáng kể, với Flash attention gốc.

Hình 3. Thời gian suy luận và bộ nhớ GPU của (a) self-attention gốc được triển khai bởi Pytorch, (b) Flash Attention (Dao, 2023), và (c) DCA (công việc này).

Nghiên cứu Phân tích Để xác thực ba cơ chế attention được đề xuất trong công việc này, chúng tôi trình bày một nghiên cứu phân tích cho DCA trong Hình 4, tập trung vào các tác vụ mô hình hóa ngôn ngữ và truy xuất passkey. Chúng tôi xem xét ba điều kiện thí nghiệm: (1) Chỉ sử dụng intra-chunk attention. (2) Sử dụng cả intra-chunk và inter-chunk attention. (3) Kết hợp tất cả ba loại attention: intra-chunk, inter-chunk, và successive chunk attention. Từ kết quả trong mô hình hóa ngôn ngữ, chúng tôi quan sát rằng sử dụng intra-chunk attention mà bỏ qua thông tin từ các chunk trước đó, có thể duy trì PPL rất thấp nhưng cản trở khả năng của mô hình để truy xuất passkey từ các chunk khác. Giới thiệu inter-chunk attention, chúng tôi nhận thấy sự cải thiện trong hiệu suất truy xuất passkey ở độ dài đầu vào 12k. Tuy nhiên, việc mất tính cục bộ gây ra sự gia tăng đáng kể trong PPL của mô hình. Bằng cách tích hợp successive chunk attention, chúng tôi đạt được cả PPL thấp và độ chính xác truy xuất cao.

3RNkRkk33Rej39jkde310203040
*QMi2ti qBM/QrS2`TH2tBivBMi`BMi`YBMi2`BMi`YBMi2`Ybm++
3RNkRkk33Rej39jkde300.51
*QMi2ti qBM/QrSbbF2v _2i`B2pH ++m``+vBMi`BMi`YBMi2`BMi`YBMi2`Ybm++

Hình 4. Nghiên cứu phân tích của DCA về mô hình hóa ngôn ngữ (bên trái) và truy xuất passkey (bên phải). Chúng tôi thử nghiệm ba cơ chế attention với các chuỗi đầu vào từ 8k đến 32k.

5. Kết luận
Trong bài báo này, chúng tôi trình bày Dual Chunk Attention (DCA) như một phương pháp mới và hiệu quả để vượt qua các hạn chế độ dài ngữ cảnh vốn có trong LLMs. Bằng cách khéo léo tận dụng các chỉ số vị trí hiện có của mô hình và giới thiệu một cơ chế attention đa diện, DCA cho phép ngoại suy hơn 8 lần độ dài huấn luyện mà không cần đến huấn luyện thêm tốn kém và mất thời gian.

Tuyên bố Tác động
Nhiều nghiên cứu đã xuất hiện nhắm vào việc mở rộng độ dài ngữ cảnh được hỗ trợ của LLMs; tuy nhiên, do chi phí huấn luyện cao và không tương thích với các công nghệ như Flash Attention, ngành công nghiệp chủ yếu dựa vào việc mở rộng tần số cơ sở của RoPE hoặc PI. Phương pháp Dual Chunk Attention (DCA) của chúng tôi tương thích với Flash Attention và chỉ yêu cầu sửa đổi mã suy luận, loại bỏ nhu cầu huấn luyện lại rộng rãi. DCA bảo tồn hiệu suất mô hình trong độ dài huấn luyện, và chỉ có lợi cho nó vượt quá phạm vi này, cung cấp khả năng tương thích với các mô hình đã trải qua tinh chỉnh ngữ cảnh dài. Do đó, phương pháp của chúng tôi có thể có tác động đáng kể đến ngành công nghiệp, cung cấp một giải pháp hiệu quả về chi phí để quản lý các tình huống ngữ cảnh dài trong các ứng dụng LLM.

Có nhiều hệ quả xã hội tiềm năng của công việc chúng tôi, không có cái nào chúng tôi cảm thấy phải được nêu bật cụ thể ở đây.

Lời cảm ơn
Chúng tôi cảm ơn Yukang Chen và Hang Yan vì những bình luận hữu ích và mã nguồn mở của họ. Nghiên cứu này được hỗ trợ một phần bởi chương trình nghiên cứu chung của Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) và Hội đồng Nghiên cứu (RGC) dưới số hiệu N HKU714/21.

Tài liệu tham khảo
An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088 , 2023.

Anthropic. Introducing 100K Context Windows, 2023. URL https://www.anthropic.com/index/ 100k-context-windows .

Bai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan, Y ., Ge, W., Han, Y ., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y ., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y ., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical report, 2023.

Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L. Clex: Continuous length extrapolation for large language models, 2023a.

Chen, S., Wong, S., Chen, L., and Tian, Y . Extending context window of large language models via positional interpolation, 2023b.

Chen, Y ., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307 , 2023c.

Chi, T.-C., Fan, T.-H., Rudnicky, A. I., and Ramadge, P. J. Dissecting transformer length extrapolation via the lens of receptive field analysis, 2023.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.

Chowdhury, J. R. and Caragea, C. Monotonic location attention for length generalization, 2023.

Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/ togethercomputer/RedPajama-Data .

Dao, T. Flashattention-2: Faster attention with better paral- lelism and work partitioning, 2023.

Dao, T., Fu, D. Y ., Ermon, S., Rudra, A., and R ´e, C. Flashat- tention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS , 2022.

Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., and Gardner, M. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter

9

--- TRANG 7 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

of the Association for Computational Linguistics: Hu- man Language Technologies , pp. 4599–4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https: //aclanthology.org/2021.naacl-main.365 .

Han, C., Wang, Q., Xiong, W., Chen, Y ., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models, 2023.

He, Z., Feng, G., Luo, S., Yang, K., He, D., Xu, J., Zhang, Z., Yang, H., and Wang, L. Two stones hit one bird: Bilevel positional encoding for better length extrapolation, 2024.

Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021.

Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y ., Chen, H., and Hu, X. Llm maybe longlm: Self-extend llm context window without tuning, 2024.

Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers, 2023.

Koˇcisk´y, T., Schwarz, J., Blunsom, P., Dyer, C., Her- mann, K. M., Melis, G., and Grefenstette, E. The Nar- rativeQA reading comprehension challenge. Transac- tions of the Association for Computational Linguistics , 6:317–328, 2018. doi: 10.1162/tacl a00023. URL https://aclanthology.org/Q18-1023 .

Lee, G., Hartmann, V ., Park, J., Papailiopoulos, D., and Lee, K. Prompted llms as chatbot modules for long open- domain conversation. In Findings of the Association for Computational Linguistics: ACL 2023 . Association for Computational Linguistics, 2023. doi: 10.18653/v1/ 2023.findings-acl.277. URL http://dx.doi.org/10. 18653/v1/2023.findings-acl.277 .

Li, D., Shao, R., Xie, A., Sheng, Y ., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length. 2023a.

Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., Sanghai, S., Yang, Y ., Kumar, S., and Bho- janapalli, S. Functional interpolation for relative positions improves long context transformers, 2023b.

Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts, 2023a.

Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D. Scaling laws of rope-based extrapolation, 2023b.

LMSYS. Vicuna: An open-source chatbot impress- ing gpt-4 with 90 URL https://lmsys.org/blog/ 2023-03-30-vicuna/ .

LocalLLaMA. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023a. URL https://www.reddit.com/r/ LocalLLaMA/comments/14mrgpr/dynamically_ scaled_rope_further_increases/ .

LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., June 2023b. URL https://www.reddit.com/ r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/ .

Lv, K., Liu, X., Guo, Q., Yan, H., He, C., Qiu, X., and Lin, D. Longwanjuan: Towards systematic measurement for long text quality, 2024.

Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300 , 2023.

MosaicML. Introducing mpt-30b: Raising the bar for open- source foundation models, 2023a. URL www.mosaicml. com/blog/mpt-30b . Accessed: 2023-06-22.

MosaicML. Introducing mpt-7b: A new standard for open- source, ly usable llms, 2023b. URL www.mosaicml. com/blog/mpt-7b .

OpenAI. Gpt-4 technical report, 2023.

Pang, R. Y ., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V ., Ma, J., Thompson, J., He, H., and Bowman, S. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL https: //aclanthology.org/2022.naacl-main.391 .

Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models, 2023.

Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation, 2022.

Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y . Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. ArXiv , abs/2401.04658, 2024. URL https://api. semanticscholar.org/CorpusID:266900042 .

10

--- TRANG 8 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for long- range sequence modelling. In 8th International Confer- ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id= SylKikSYDH .

Ratner, N., Levine, Y ., Belinkov, Y ., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y . Parallel context windows for large lan- guage models, 2023.

Robertson, S., Zaragoza, H., et al. The probabilistic rele- vance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333–389, 2009.

Rozi `ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., D ´efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2023.

Rula, A. and D'Souza, J. Procedural text mining with large language models, 2023.

Ruoss, A., Del ´etang, G., Genewein, T., Grau-Moya, J., Csord ´as, R., Bennani, M., Legg, S., and Veness, J. Ran- domized positional encodings boost length generalization of transformers, 2023.

Saad-Falcon, J., Barrow, J., Siu, A., Nenkova, A., Yoon, D. S., Rossi, R. A., and Dernoncourt, F. Pdftriage: Ques- tion answering over long, structured documents, 2023.

Song, K., Wang, X., Cho, S., Pan, X., and Yu, D. Zebra: Extending context window with layerwise grouped local- global attention, 2023.

Su, J. Rectified rotary position embeddings. https:// github.com/bojone/rerope , 2023.

Su, J., Lu, Y ., Pan, S., Murtadha, A., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2022.

Sun, Y ., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V ., Song, X., and Wei, F. A length- extrapolatable transformer, 2022.

Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford al- paca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca , 2023.

Together. Llama-2-7b-32k-instruct — and fine-tuning for llama-2 models with together api, 2023. URL https:// together.ai/blog/llama-2-7b-32k-instruct .

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam- ple, G. Llama: Open and efficient foundation language models, 2023a.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.

Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y ., Michalewski, H., and Miło ´s, P. Focused transformer: Contrastive training for context scaling, 2023.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017.

Wang, L., Yang, N., and Wei, F. Learning to retrieve in- context examples for large language models, 2024.

Wei, J., Kim, S., Jung, H., and Kim, Y .-H. Leveraging large language models to power chatbots for collecting user self-reported data, 2023.

Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y ., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory, 2024.

Xiao, G., Tian, Y ., Chen, B., Han, S., and Lewis, M. Ef- ficient streaming language models with attention sinks, 2023.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y ., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models. CoRR , abs/2309.16039, 2023. doi: 10.48550/ARXIV .2309.16039. URL https://doi. org/10.48550/arXiv.2309.16039 .

Ye, J., Wu, Z., Feng, J., Yu, T., and Kong, L. Composi- tional exemplars for in-context learning. arXiv preprint arXiv:2302.05698 , 2023.

Zhang, J., Jiang, S., Feng, J., Zheng, L., and Kong, L. Linear attention via orthogonal memory. ArXiv , abs/2312.11135, 2023. URL https://api.semanticscholar.org/ CorpusID:266359128 .

11

--- TRANG 9 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Soaring from 4k to 400k: Extending llm's con- text with activation beacon. ArXiv , abs/2401.03462, 2024. URL https://api.semanticscholar.org/ CorpusID:266844488 .

Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A. H., Celikyilmaz, A., Liu, Y ., Qiu, X., and Radev, D. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies , pp. 5905–5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https: //aclanthology.org/2021.naacl-main.472 .

Zhu, D., Yang, N., Wang, L., Song, Y ., Wu, W., Wei, F., and Li, S. Pose: Efficient context window extension of llms via positional skip-wise training, 2023.

12

--- TRANG 10 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

A. Phụ lục
A.1. Truy xuất passkey
Ngoài các tác vụ thực tế, chúng tôi đánh giá khả năng ngữ cảnh dài của LLMs để thực hiện tác vụ truy xuất passkey như được định nghĩa trong Mohtashami & Jaggi (2023). Tác vụ này thách thức một mô hình ngôn ngữ để định vị một passkey đơn giản (ví dụ: một số ngẫu nhiên năm chữ số) được nhúng trong một chuỗi văn bản dài và vô nghĩa khác. Mục đích chính của tác vụ này là xác định liệu một Mô hình Ngôn ngữ Lớn (LLM) có thể duy trì nhận thức về thông tin được phân bố trong suốt một chuỗi đầu vào dài. Để đánh giá độ chính xác truy xuất, chúng tôi đặt ngẫu nhiên passkey ở các độ sâu tài liệu khác nhau được phân bố đều. Đối với mỗi độ sâu tài liệu, chúng tôi chạy 20 lần với các passkey khác nhau và chúng tôi thử nghiệm độ dài chuỗi đầu vào từ 4k đến 20k. Chúng tôi so sánh hiệu suất của DCA với 2 phương pháp mở rộng phổ biến: PI (Chen et al., 2023b), NTK-Aware (LocalLLaMA, 2023b;a), trên mô hình Llama2 13B với cửa sổ ngữ cảnh huấn luyện trước 4k. Kết quả hiệu suất được mô tả trong Hình 5. Đáng chú ý, trong độ dài ngữ cảnh 18k token, mô hình CHUNK LLAMA 2 của chúng tôi liên tục đạt được độ chính xác truy xuất passkey 100% trên tất cả các độ sâu được thử nghiệm.

Chúng tôi mở rộng phạm vi của các tác vụ truy xuất passkey bằng cách tăng dần số token đầu vào từ 2k đến 192k. Đối với mỗi độ dài ngữ cảnh đầu vào, mô hình được đánh giá 20 lần, với vị trí passkey được thay đổi ngẫu nhiên trong mỗi thử nghiệm. Ngoài ra, chúng tôi cũng xác minh mô hình Together-32k 7B (Together, 2023), hỗ trợ cửa sổ ngữ cảnh 32k token, và đối tác ChunkTogether 7B của nó. Kết quả cho cả biến thể baseline và được tăng cường DCA của các mô hình này được minh họa trong Hình 7. Chỉ với độ dài ngữ cảnh huấn luyện 4k, CHUNK LLAMA 2 duy trì độ chính xác truy xuất cao lên đến độ dài ngữ cảnh 32k. Bằng cách tích hợp những phát hiện này với các mô hình ngữ cảnh dài hiện tại, chúng ta có thể mở rộng cửa sổ ngữ cảnh được hỗ trợ đến 192k token ấn tượng sử dụng phương pháp không cần học.

mất ở đầu : Một quan sát thú vị là các trường hợp thất bại của PI dường như phần lớn không liên quan đến độ sâu của tài liệu, trong khi phương pháp dựa trên NTK thường xuất sắc khi passkey được đặt gần đầu tài liệu. Tuy nhiên, hiệu quả của nó giảm đáng kể—với độ chính xác giảm xuống từ 40% đến 80%—khi passkey được đặt ở các phần giữa. Xu hướng này phù hợp với các phát hiện được báo cáo bởi Liu et al. (2023a). Ngược lại, khi ngữ cảnh đầu vào được mở rộng, CHUNK LLAMA 2 chứng minh hiệu suất cải thiện trong các phần giữa nhưng nơi đầu tiên mà độ chính xác giảm xuống là ở đầu văn bản.

(a) Llama2 PI(b) Llama2 NTK(c) ChunkLlama

Hình 5. Thử nghiệm Các Phương pháp Mở rộng Không cần Học khác nhau với Ngữ cảnh 24K ("Needle in a Haystack" Passkey Retrieval). Tất cả các mô hình có ngữ cảnh huấn luyện trước 4k và không được huấn luyện thêm. Trục X đại diện cho độ dài ngữ cảnh đầu vào, và trục Y chỉ ra độ sâu của passkey trong tài liệu. Đối với mỗi độ sâu, chúng tôi chạy 20 trường hợp thử nghiệm khác nhau.

Hình 6. Thử nghiệm áp lực Mistral-7B-Instruct-v0.2 trên độ dài ngữ cảnh 192k cho và phiên bản được tăng cường DCA của nó (("Needle In A HayStack")).

13

--- TRANG 11 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

kF9F3FReFjkFe9FRk3FRNkF00.51
*QMi2ti qBM/Qr++m``+vhQ;2i?2`@jkF*?mMFhQ;2i?2`@jkFkF9F3FReFjkFe9FRk3FRNkF00.51
*QMi2ti qBM/Qr++m``+vGHK@9F*?mMFGHK@9F

(a) Llama2-4k và Chunkllama(b) Together-32k và ChunkTogether

Hình 7. Truy xuất passkey trên độ dài ngữ cảnh 192k cho Llama2 13B, Together-32k 7B và các phiên bản được tăng cường DCA của chúng.

0102103210010210321001021032107654765476547654Inter-chunk76547654765476547654765476547654010210321001021032100102103210
Intra-chunk010210321001021032100102103210432154326543765443215432654376547654765476547654successive-chunk

(a) Intra-Chunk Attention(b) Inter-Chunk Attention(c) Successive-Chunk Attention

Hình 8. Trực quan hóa Ma trận Vị trí Tương đối M sử dụng Dual Chunk Attention (DCA) bằng cách chia toàn bộ chuỗi thành 3 chunk và kích thước chunk s= 4. Trong trường hợp này, chúng ta có kích thước cửa sổ huấn luyện trước c= 8, và kích thước cửa sổ cục bộ w= 3. Chuỗi được phân đoạn thành 3 chunk để đảm bảo rằng các vị trí tương đối không vượt quá 7. Phần tử ma trận M[i][j] đại diện cho vị trí tương đối giữa vector query thứ i q và vector key thứ j k.

A.2. Thêm Ví dụ
Trong phần này, chúng tôi đưa ra một ví dụ về việc xử lý một chuỗi 12-token nhưng độ dài huấn luyện trước chỉ là 8. Dựa trên Llama2, các chỉ số vị trí key/query sẽ được khởi tạo như:
Pq= [0,1,2,3,4,5,6,7,8,9,10,11]
Pk= [0,1,2,3,4,5,6,7,8,9,10,11].

Khi query [11] và query [0] thực hiện tích trong, vị trí tương đối của chúng là 11, vượt quá kích thước huấn luyện trước. Trong DCA, chúng tôi đặt kích thước chunk, đó là một siêu tham số nhỏ hơn độ dài huấn luyện trước. Trong ví dụ này, chúng ta có thể đặt kích thước chunk thành 4, có nghĩa là chúng ta chia toàn bộ đầu vào thành 3 chunk và đưa ra các chỉ số vị trí mới cho keys:
[0,1,2,3,4,5,6,7,8,9,10,11]⇒[0,1,2,3,0,1,2,3,0,1,2,3]

Sau đó chúng tôi đưa ra các chỉ số vị trí khác nhau cho queries.

Intra-chunk Attention: Tính toán attention cho các token trong cùng một chunk
Pintraq= [0,1,2,3,0,1,2,3,0,1,2,3]
Pk= [0,1,2,3,0,1,2,3,0,1,2,3]

Ma trận vị trí kết quả được hiển thị trong Hình 8 Phụ lục (a) và vị trí tương đối tối đa là 3−0 = 3 .

Inter-chunk Attention: Tính toán attention cho các token trong các chunk khác nhau Khi độ dài huấn luyện trước là 8, chúng ta có chỉ số vị trí tối đa=7.
Pinterq= [7,7,7,7,7,7,7,7,7,7,7,7]
Pk= [0,1,2,3,0,1,2,3,0,1,2,3]

14

--- TRANG 12 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Ma trận vị trí kết quả sau inter-chunk attention được hiển thị trong Hình 8 Phụ lục (b).

Successive-chunk Attention: Tính toán attention cho các token trong các chunk liên tiếp. Chúng tôi thay đổi w= 3 phần tử đầu tiên (một siêu tham số) trong Pinterq:
Psuccq= [4,5,6,7,4,5,6,7,4,5,6,7]
Pk= [0,1,2,3,0,1,2,3,0,1,2,3]

Ma trận vị trí sau successive-chunk attention được hiển thị trong Hình 8 Phụ lục (c).

A.3. Flash Attention
Chúng tôi chia self-attention tiêu chuẩn thành 3 tính toán flash attention riêng biệt tương ứng thu được đầu ra từ intra-chunk attention, inter-chunk-attention, và successive chunk-attention. Thuật toán 1 thể hiện cách 3 attention được giới thiệu trong DCA tích hợp với Flash Attention. Chúng tôi minh họa với vector query thứ i qi và nó cần tính toán tích trong với tất cả keys kj với j≤i. Chúng ta có n=⌊i/s⌋ chunk trước chunk hiện tại. DCA gọi 3 hoạt động Flash Attention riêng biệt với độ phức tạp O(i−n∗s)(intra-chunk attention), O(s)(succssive-chunk attention) và O(s∗(n−1)).

Thuật toán 1 Mã giả của DCA với FlashAttention
# q: vector query 1 x d (tensor với hình dạng [1, d])
# i: chỉ số tuyệt đối của q (số nguyên)
# K, V: ma trận i x d cho keys và values (tensor với hình dạng [i, d])
# s: kích thước chunk (số nguyên)
# P_k, P_q_intra, P_q_succ, P_q_inter: ids vị trí (danh sách số nguyên)
n = math.floor(i/s) # Số chunk trước chunk hiện tại
# Áp dụng embedding vị trí xoay cho toàn bộ ma trận key K
K = apply_rotary_pos_emb(K, P_k) # K là [i, d] sau embedding
# ------------- Intra-chunk Attention, casual=True -------------
q_intra = apply_rotary_pos_emb(q, P_q_intra[i]) # q_intra là [1, d]
# Chọn keys và values intra-chunk
K_intra = K[s *n:i] # K_intra là [(i - s *n), d]
V_intra = V[s *n:i] # V_intra là [(i - s *n), d]
# Tính toán đầu ra và bản đồ attention softmax cho intra-chunk attention
o_intra, map_intra = Flash(q_intra, K_intra, V_intra) # o_intra là [1, d], map_intra là [1, i - s *n]
# ------------- Successive-chunk Attention, casual=False -----------
q_succ = apply_rotary_pos_emb(q, P_q_succ[i]) # q_succ là [1, d]
# Chọn keys và values successive-chunk
K_succ = K[s *(n-1):s *n] # K_succ là [s, d]
V_succ = V[s *(n-1):s *n] # V_succ là [s, d]
# Tính toán đầu ra và bản đồ attention softmax cho successive-chunk attention
o_succ, map_succ = Flash(q_succ, K_succ, V_succ) # o_succ là [1, d], map_succ là [1, s]
# ------------- Inter-chunk Attention, casual=False -----------
q_inter = apply_rotary_pos_emb(q, P_q_inter[i]) # q_inter là [1, d]
# Chọn keys và values inter-chunk
K_inter = K[:s *(n-1)] # K_inter là [s *(n-1), d]
V_inter = V[:s *(n-1)] # V_inter là [s *(n-1), d]
# Tính toán đầu ra và bản đồ attention softmax cho inter-chunk attention
o_inter, map_inter = Flash(q_inter, K_inter, V_inter) # o_inter là [1, d], map_inter là [1, s *(n-1)]
# Chuẩn hóa
# Tổng các bản đồ attention cho mỗi loại attention để có được normalizer
sum_intra = map_intra.sum(-1) # sum_intra là một vô hướng
sum_inter = map_inter.sum(-1) # sum_inter là một vô hướng
sum_succ = map_succ.sum(-1) # sum_succ là một vô hướng
normalizer = sum_intra + sum_inter + sum_succ # normalizer là một vô hướng
# Nối các đầu ra attention và chia cho normalizer
output = (sum_intra *o_intra, sum_succ *o_succ, sum_inter *o_inter) / normalizer # output là [1, d]

A.4. Lựa chọn Ví dụ Trong Ngữ cảnh
Chúng tôi chọn lựa chọn các ví dụ trong ngữ cảnh từ tập huấn luyện đây là cách thực tế và phổ biến để có được các ví dụ (Ye et al., 2023; Wang et al., 2024). Chúng tôi thử nghiệm với 2 phương pháp khác nhau cho quá trình lựa chọn này: (1)Lựa chọn Ngẫu nhiên: chọn ngẫu nhiên các ví dụ từ tập huấn luyện. (2) Lựa chọn Dựa trên Truy xuất: Sử dụng truy vấn hiện tại, chúng tôi sử dụng

15

--- TRANG 13 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Bảng 5. So sánh kết quả few-shot sử dụng các ví dụ trong ngữ cảnh khác nhau

Mô hìnhVí dụ Trong Ngữ cảnhQasper QuALITY QMSum
F1 (2-shot) EM (2-shot) R-g (1-shot)

CHUNK LLAMA 2 7B EXAMPLE BEST 27.3 33.9 15.0
CHUNK LLAMA 2 7B EXAMPLE RANDOM 28.2 35.6 14.7
CHUNK LLAMA 2 7B EXAMPLE WORST 28.4 35.9 14.3
CHUNK LLAMA 2 13B EXAMPLE BEST 28.5 46.2 15.6
CHUNK LLAMA 2 13B EXAMPLE RANDOM 29.3 47.9 15.2
CHUNK LLAMA 2 13B EXAMPLE WORST 29.0 47.5 15.5

các thuật toán truy xuất như BM25 (Robertson et al., 2009) để tìm các ví dụ có liên quan nhất từ tập huấn luyện. Chúng tôi gọi các ví dụ trong ngữ cảnh với điểm truy xuất cao nhất là EXAMPLE BEST và những cái có điểm thấp nhất là EXAMPLE WORST . Hiệu suất của các phương pháp lựa chọn khác nhau dựa trên CHUNK LLAMA 27B/13B được hiển thị trong Bảng 5. Hiệu suất trên tập dữ liệu tóm tắt QMSum (Zhong et al., 2021) nói chung ít có khả năng bị ảnh hưởng bởi việc lựa chọn prompt. Tuy nhiên, trên 2 tập dữ liệu trả lời câu hỏi, chúng tôi thấy rằng sử dụng các ví dụ gần nhất, một cách nghịch lý, dẫn đến kết quả tồi tệ nhất và hiệu suất của cả lựa chọn ngẫu nhiên và chọn ví dụ tồi tệ nhất tương đối tương tự. Một lời giải thích có thể cho hiện tượng này là khi ví dụ rất tương tự, LLMs có xu hướng sao chép phản hồi được đưa ra trong ví dụ thường dẫn đến câu trả lời sai.

A.5. Hiệu suất trên Dữ liệu Chưa thấy
Hiện tại, hầu như tất cả các benchmark cho LLMs không giải quyết triệt để khả năng nhiễm dữ liệu, có nghĩa là dữ liệu thử nghiệm có thể đã được sử dụng trong giai đoạn huấn luyện trước hoặc tinh chỉnh. Để chứng minh hiệu suất của ChunkLlama trên dữ liệu tài liệu dài chưa thấy trước đây, chúng tôi trực tiếp sử dụng mã Latex của bài báo này làm trường hợp thử nghiệm trong khi bỏ qua các phần tiêu đề, tóm tắt và kết luận. Sau khi tokenization, tổng độ dài đầu vào là 19388. Chúng tôi bắt đầu đánh giá với một tập hợp các câu hỏi đơn giản không cần thiết kiến thức trước để có câu trả lời chính xác (xem Bảng 6). Tiếp theo là một loạt câu hỏi thách thức hơn được thiết kế để đánh giá sự hiểu biết về DCA được đề xuất (tham khảo Bảng 7).

Kết quả chỉ ra rằng, so với NTK, CHUNK LLAMA 2 chứng minh khả năng vượt trội để diễn giải chính xác hướng dẫn và cung cấp phản hồi chính xác. Tuy nhiên, độ chính xác của câu trả lời của mô hình 13B vẫn không tối ưu, ngay cả đối với các truy vấn tương đối đơn giản. Ví dụ, khi được hỏi về corpus tinh chỉnh được sử dụng bởi DCA, nó sai lầm trích dẫn corpus Llama2 thay vì các corpus chính xác, đó là ShareGPT và AlpacaGPT4.

Về những câu hỏi cơ bản này, ChunkLlama 70B thể hiện tỷ lệ chính xác cao đáng kể. Hơn nữa, ChunkLlama 70B cho thấy kết quả hứa hẹn trên các truy vấn thách thức hơn. Nó có thể diễn đạt lý do đằng sau thiết kế của chúng tôi về các chiến lược inter-chunk và successive-chunk một cách chính xác. Tuy nhiên, khi chúng tôi đặt câu hỏi yêu cầu hiểu biết toàn cục về phương pháp này, nó vẫn gặp khó khăn.

16

--- TRANG 14 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Bảng 6. Kết quả QA (Dễ) của CHUNK LLAMA 213B/70B và Dynamic-NTK với Llama2 13B, sử dụng nội dung của bài báo này làm đầu vào. Hầu hết các câu hỏi không yêu cầu hiểu biết sâu sắc về bài báo này. Prompt đầu vào có 19388 token. Dynamic-NTK không thể đưa ra câu trả lời đúng cho tất cả các trường hợp thử nghiệm. Kết quả tạo chất lượng thấp được đánh dấu với ✗.

Prompt [INST] <<SYS>>Vui lòng đọc bài báo dài và trả lời các câu hỏi << / SYS>>1. Giới thiệu \n\n
Khả năng hiểu và xử lý thông tin ngữ cảnh dài là điều cần thiết cho các mô hình ngôn ngữ lớn (LLMs) để phục vụ hiệu quả cho một loạt các ứng dụng. Những ứng dụng này bao gồm phân tích và phản hồi các truy vấn trong các tệp PDF lớn, giữ lại lịch sử đối thoại mở rộng, và trao quyền cho các chatbot tương tác... [/INST]

Câu hỏi Bạn có thể đưa ra một tiêu đề cho bài báo này không
CHUNK LLAMA 270B Chắc chắn, đây là một số tiêu đề tiềm năng cho bài báo:
1. "Dual Chunk Attention: Một Khung Framework Không cần Huấn luyện cho Mô hình Ngôn ngữ Ngữ cảnh Dài"
2. "Mô hình Ngôn ngữ Ngữ cảnh Dài Hiệu quả với Dual Chunk Attention"
3. "Mở rộng Mô hình Ngôn ngữ Ngữ cảnh Dài mà không cần Huấn luyện: Một Phương pháp Dual Chunk Attention"
4. "DCA: Một Phương pháp Không cần Huấn luyện cho Mô hình Ngôn ngữ Ngữ cảnh Dài"
5. "Mô hình Ngôn ngữ Ngữ cảnh Dài với Dual Chunk Attention: Một Huấn luyện...

CHUNK LLAMA 2 13B Chắc chắn! Dual Chunk Attention: Một Khung Framework Không cần Huấn luyện cho Mở rộng Ngữ cảnh Dài của Các Mô hình Ngôn ngữ Lớn.
Tiêu đề này làm nổi bật các đóng góp chính của bài báo: khung framework Dual Chunk Attention (DCA) được đề xuất, cho phép mở rộng ngữ cảnh dài mà không cần huấn luyện bổ sung, và ứng dụng của nó cho các tác vụ khác nhau. Cụm từ "không cần huấn luyện" cũng được bao gồm để nhấn mạnh sự đổi mới chính của bài báo.

Dynamic NTK 13B Khung framework Dual Chunk Attention (DCA) được đề xuất cải thiện việc mở rộng ngữ cảnh dài của Lama2 hơn 8 lần, đạt PPL 5.59 ở cửa sổ ngữ cảnh 2k. ✗(bỏ qua hướng dẫn)

Câu hỏi Mô hình nào có thể được mở rộng lên ngữ cảnh 192k với DCA?
CHUNK LLAMA 270B Mô hình Llama2 70B có thể được mở rộng lên cửa sổ ngữ cảnh 192k sử dụng DCA, đạt điểm PPL 5.59
CHUNK LLAMA 2 13B Mô hình Llama2 70B có thể được mở rộng lên ngữ cảnh 192k với DCA.
Dynamic NTK 13B {\end{enumerate}\end{figure}[ht!]
\end{document}\end{document}\begin{table} ✗(lỗi)

Câu hỏi Ngoài Llama2, các tác giả có xác thực phương pháp của họ trên các mô hình khác không?
CHUNK LLAMA 270B Có, các tác giả đã xác thực phương pháp của họ trên các mô hình khác, bao gồm
Together-32k,
CodeLlama,
tất cả đều dựa trên Llama. Họ cũng đã so sánh phương pháp của họ với nhiều mô hình mã nguồn mở và độc quyền.

CHUNK LLAMA 2 13B Các tác giả đã xác thực phương pháp của họ trên các mô hình sau:
* Together-32k
* CodeLlama
* Llama2 Long ✗(Chúng tôi không xác minh DCA trên Llama2 Long)

Dynamic NTK 13B Bài báo giới thiệu một khung framework mới không cần huấn luyện được gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B và 20 lần cho các mô hình 70B ✗(bỏ qua hướng dẫn)

Câu hỏi Mô tả corpus tinh chỉnh được sử dụng bởi DCA
CHUNK LLAMA 270B Corpus tinh chỉnh được sử dụng bởi DCA là sự kết hợp dữ liệu đối thoại dài từ ShareGPT và AlpacaGPT4
CHUNK LLAMA 2 13B Corpus tinh chỉnh được sử dụng bởi DCA giống như corpus được sử dụng bởi mô hình Llama2 gốc, là sự kết hợp của tập huấn luyện và tập xác thực của mô hình Llama2 gốc ✗(Chúng tôi không sử dụng tập huấn luyện của Llama)
Dynamic NTK 13B {\end{begin}\begin{table}\end{figure}
[ht!]} \end{document}\end{document}\end{document} ✗(lỗi)

Câu hỏi Llama2 Long cần bao nhiều bước huấn luyện?
CHUNK LLAMA 270B Llama2 Long cần 100,000 bước huấn luyện
CHUNK LLAMA 2 13B Llama2 Long cần 100,000 bước huấn luyện.
Dynamic NTK 13B Llama2 Long cần 40,000 bước cho cửa sổ ngữ cảnh 4k. ✗(không phải 40,000)

Câu hỏi Các tác giả có xác thực mô hình trên L-Eval không? và liệt kê các tác vụ
CHUNK LLAMA 270B Có, các tác giả xác thực mô hình của họ trên L-Eval và liệt kê các tác vụ trong Bảng 4. Họ đánh giá mô hình của họ trên bốn tác vụ đóng từ L-Eval với độ dài đầu vào đa dạng dao động từ 3k đến 27k. Các tác vụ bao gồm TOFEL, QuALITY , Coursera, và SFiction.
CHUNK LLAMA 2 13B Có, các tác giả xác thực mô hình trên L-Eval và liệt kê các tác vụ. Trong phần có tiêu đề "Thí nghiệm", các tác giả trình bày một đánh giá rộng rãi về mô hình của họ trên một tập đa dạng các tác vụ, bao gồm NarrativeQA, Qasper, Quality, và QMSum. ✗(Những cái này không phải từ L-Eval)
Dynamic NTK 13B Bài báo trình bày một khung framework mới không cần huấn luyện được gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B và... ✗(bỏ qua hướng dẫn)

17

--- TRANG 15 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Các Mô hình Ngôn ngữ Lớn

Bảng 7. Kết quả QA (khó) của CHUNK LLAMA 213B/70B và Dynamic-NTK với Llama2 13B, sử dụng nội dung của bài báo này làm đầu vào. Câu hỏi trong bảng này thường yêu cầu độc giả hiểu phương pháp này. Prompt đầu vào có 19388 token. Dynamic-NTK không thể đưa ra câu trả lời đúng cho tất cả các trường hợp thử nghiệm. Kết quả tạo chất lượng thấp được đánh dấu với ✗.

Prompt [INST] <<SYS>>Vui lòng đọc bài báo dài và trả lời các câu hỏi << / SYS>>1. Giới thiệu \n\n
Khả năng hiểu và xử lý thông tin ngữ cảnh dài là điều cần thiết cho các mô hình ngôn ngữ lớn (LLMs) để phục vụ hiệu quả cho một loạt các ứng dụng. Những ứng dụng này bao gồm phân tích và phản hồi các truy vấn trong các tệp PDF lớn, giữ lại lịch sử đối thoại mở rộng, và trao quyền cho các chatbot tương tác... [/INST]

Câu hỏi Tại sao các tác giả giới thiệu successive-chunk attention? (Khó)
CHUNK LLAMA 270B Các tác giả giới thiệu successive-chunk attention để giải quyết vấn đề mất tính cục bộ trong cơ chế attention khi sử dụng inter-chunk attention. Inter-chunk attention có thể dẫn đến tình huống mà vị trí tương đối giữa hai token trong các chunk khác nhau không được nắm bắt chính xác, dẫn đến perplexity cao. Successive-chunk attention giúp duy trì tính cục bộ bằng cách điều chỉnh các chỉ số vị trí của w token đầu tiên trong mỗi chunk để đảm bảo rằng vị trí tương đối giữa các token lân cận được nắm bắt chính xác. Điều này cho phép mô hình xử lý tốt hơn các tác vụ ngữ cảnh dài yêu cầu khả năng nắm bắt các phụ thuộc tầm xa.

CHUNK LLAMA 2 13B Các tác giả giới thiệu successive-chunk attention để giải quyết vấn đề phụ thuộc tầm xa trong các mô hình ngôn ngữ ngữ cảnh dài. Các phương pháp mã hóa vị trí gốc, như RoPE và NTK-aware RoPE, không nắm bắt các phụ thuộc tầm xa hiệu quả, dẫn đến hiệu suất kém trong mô hình ngôn ngữ và các tác vụ truy xuất passkey. Bằng cách giới thiệu successive-chunk attention, mô hình có thể nắm bắt cả phụ thuộc cục bộ và tầm xa.. ✗(hiểu sai successive-chunk attention)

Dynamic NTK 13B Bài báo giới thiệu một khung framework mới không cần huấn luyện được gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B... ✗(bỏ qua hướng dẫn)

Câu hỏi Tại sao các tác giả giới thiệu inter-chunk attention? (Khó)
CHUNK LLAMA 270B Các tác giả giới thiệu inter-chunk attention để giải quyết vấn đề phụ thuộc tầm xa kém trong module intra-chunk attention, sử dụng cửa sổ ngữ cảnh cố định để tính toán attention. Cơ chế inter-chunk attention cho phép mô hình chú ý đến các token từ các chunk khác nhau, cho phép nó nắm bắt các phụ thuộc tầm xa và cải thiện hiệu suất trên các tác vụ yêu cầu xử lý các chuỗi dài.

CHUNK LLAMA 2 13B 13b: Tác giả giới thiệu inter-chunk attention để cải thiện hiệu suất truy xuất passkey của mô hình. Tác giả lưu ý rằng chỉ sử dụng intra-chunk attention, bỏ qua thông tin từ các chunk trước đó, có thể duy trì PPL thấp nhưng cản trở khả năng của mô hình để truy xuất passkey từ các chunk khác. ✗(không đủ chính xác)

Dynamic NTK 13B Bài báo trình bày một khung framework mới không cần huấn luyện được gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B... ✗(bỏ qua hướng dẫn)

Câu hỏi Tại sao vị trí tương đối trong DCA sẽ không vượt quá độ dài ngữ cảnh huấn luyện trước? (Khó)
CHUNK LLAMA 270B Vị trí tương đối trong DCA được tính toán sử dụng các chỉ số vị trí của keys và queries, cả hai đều nhỏ hơn hoặc bằng độ dài ngữ cảnh huấn luyện trước. Do đó, vị trí tương đối sẽ không vượt quá độ dài ngữ cảnh huấn luyện trước. (không rõ ràng đủ)

CHUNK LLAMA 2 13B Vị trí tương đối trong DCA sẽ không vượt quá độ dài ngữ cảnh huấn luyện trước vì mô hình chỉ xem xét các vị trí tương đối trong cùng một chunk. Các chunk được phân đoạn dựa trên độ dài ngữ cảnh huấn luyện trước, và các vị trí tương đối được tính toán trong mỗi chunk. (không rõ ràng đủ)

Dynamic NTK 13B Vị trí tương đối trong Dual Chunk Attention (DCA) không vượt quá độ dài ngữ cảnh huấn luyện trước vì nó được thiết kế để duy trì tính cục bộ. (không rõ ràng đủ)

Câu hỏi Các khác biệt chính giữa bài báo này và các bài báo trước đây là gì? (Khó)
CHUNK LLAMA 270B Các khác biệt chính giữa bài báo này và các bài báo trước đây về mô hình ngôn ngữ lớn (LLMs) là:
1. Bài báo đề xuất một phương pháp mới được gọi là Dual Chunk Attention (DCA) cho phép LLMs xử lý thông tin ngữ cảnh dài mà không có chi phí huấn luyện bổ sung.
2. DCA được thiết kế để xử lý thông tin ngữ cảnh dài bằng cách phân đoạn chuỗi đầu vào thành các chunk nhỏ hơn và tính toán attention trong mỗi chunk, giảm chi phí tính toán và cho phép mở rộng hiệu quả cửa sổ ngữ cảnh...

CHUNK LLAMA 2 13B Bài báo này đề xuất một khung framework mới không cần huấn luyện được gọi là Dual Chunk Attention (DCA) cho phép các mô hình ngôn ngữ lớn (LLMs) xử lý thông tin ngữ cảnh dài mà không cần huấn luyện bổ sung. DCA giới thiệu một cơ chế attention mới mở rộng cửa sổ ngữ cảnh của LLMs mà không cần huấn luyện bổ sung, cho phép sử dụng tài nguyên tính toán hiệu quả hơn.

Dynamic NTK 13B DCA trực giao với các phương pháp ngoại suy phổ biến như PI và NTK-Aware RoPE, và có thể được tích hợp với các mô hình ngữ cảnh dài hiện tại để mở rộng đáng kể...

18
