# 2402.17463.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/long-context/2402.17463.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1432608 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh DÃ i cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n khÃ´ng cáº§n Huáº¥n luyá»‡n
Chenxin An* 1 2Fei Huang2Jun Zhang Shansan Gong1Xipeng Qiu3Chang Zhou2Lingpeng Kong1
TÃ³m táº¯t
Kháº£ nÄƒng cá»§a cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs)
Ä‘á»ƒ xá»­ lÃ½ vÃ  táº¡o vÄƒn báº£n máº¡ch láº¡c bá»‹ suy yáº¿u Ä‘Ã¡ng ká»ƒ
khi sá»‘ lÆ°á»£ng token Ä‘áº§u vÃ o vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c
cá»§a chÃºng. Do chi phÃ­ Ä‘áº¯t Ä‘á» cá»§a viá»‡c tinh chá»‰nh
cÃ¡c mÃ´ hÃ¬nh quy mÃ´ lá»›n vá»›i cÃ¡c chuá»—i dÃ i hÆ¡n, chÃºng tÃ´i
Ä‘á» xuáº¥t Dual Chunk Attention (DCA), cho phÃ©p
LLAMA 270B há»— trá»£ cá»­a sá»• ngá»¯ cáº£nh hÆ¡n 100k
token mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n liÃªn tá»¥c. Báº±ng cÃ¡ch
phÃ¢n tÃ¡ch tÃ­nh toÃ¡n attention cho cÃ¡c chuá»—i dÃ i
thÃ nh cÃ¡c module dá»±a trÃªn chunk, DCA quáº£n lÃ½
Ä‘á»ƒ náº¯m báº¯t hiá»‡u quáº£ thÃ´ng tin vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a
cÃ¡c token trong cÃ¹ng má»™t chunk (Intra-Chunk)
vÃ  giá»¯a cÃ¡c chunk riÃªng biá»‡t (Inter-Chunk), cÅ©ng nhÆ°
tÃ­ch há»£p liá»n máº¡ch vá»›i Flash Attention. NgoÃ i kháº£ nÄƒng
ngoáº¡i suy áº¥n tÆ°á»£ng, DCA Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t trÃªn
cÃ¡c tÃ¡c vá»¥ ngá»¯ cáº£nh dÃ i thá»±c táº¿ cÃ³ thá»ƒ so sÃ¡nh
hoáº·c tháº­m chÃ­ tá»‘t hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c
tinh chá»‰nh. Khi so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n,
mÃ´ hÃ¬nh 70B khÃ´ng cáº§n huáº¥n luyá»‡n cá»§a chÃºng tÃ´i
Ä‘áº¡t 94% hiá»‡u suáº¥t cá»§a gpt-3.5-16k, cho tháº¥y
Ä‘Ã¢y lÃ  má»™t giáº£i phÃ¡p thay tháº¿ mÃ£ nguá»“n má»Ÿ kháº£ thi.
Táº¥t cáº£ mÃ£ vÃ  dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ´ng trÃ¬nh nÃ y
Ä‘Æ°á»£c cÃ´ng bá»‘ táº¡i https:
//github.com/HKUNLP/ChunkLlama .
1. Giá»›i thiá»‡u
Kháº£ nÄƒng hiá»ƒu vÃ  xá»­ lÃ½ thÃ´ng tin ngá»¯ cáº£nh dÃ i lÃ  Ä‘iá»u cáº§n thiáº¿t cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) (OpenAI, 2023; Touvron et al., 2023a;b; Bai et al., 2023; Anthropic, 2023) Ä‘á»ƒ phá»¥c vá»¥ hiá»‡u quáº£ cho má»™t loáº¡t cÃ¡c á»©ng dá»¥ng. Nhá»¯ng á»©ng dá»¥ng nÃ y bao gá»“m phÃ¢n tÃ­ch vÃ  pháº£n há»“i cÃ¡c truy váº¥n trong cÃ¡c tá»‡p PDF lá»›n, giá»¯ láº¡i lá»‹ch sá»­ Ä‘á»‘i thoáº¡i má»Ÿ rá»™ng, vÃ  trao quyá»n cho cÃ¡c chatbot tÆ°Æ¡ng tÃ¡c (Wei et al., 2023; Lee et al., 2023; Rula & D'Souza, 2023; Saad-Falcon et al., 2023; Lv et al., 2024).

*CÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n trong thá»i gian thá»±c táº­p táº¡i Alibaba Group1The University of Hong Kong2Alibaba Group3Fudan University. LiÃªn há»‡: Chenxin An <cxan23@connect.hku.hk >.

Proceedings of the 41stInternational Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).

Nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y Ä‘Ã£ cho tháº¥y ráº±ng kháº£ nÄƒng ngá»¯ cáº£nh dÃ i cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n báº±ng cÃ¡ch tiáº¿p tá»¥c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh ngá»¯ cáº£nh ngáº¯n trÃªn cÃ¡c chuá»—i vÄƒn báº£n dÃ i (Ruoss et al., 2023; RoziÃ¨re et al., 2023). Hiá»‡u suáº¥t áº¥n tÆ°á»£ng cá»§a Llama2 Long (Xiong et al., 2023), Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« má»™t há»—n há»£p dá»¯ liá»‡u vÄƒn báº£n dÃ i vÃ  corpus huáº¥n luyá»‡n trÆ°á»›c Llama2 (Touvron et al., 2023b) gá»‘c, Ä‘á»©ng nhÆ° má»™t minh chá»©ng cho phÆ°Æ¡ng phÃ¡p nÃ y. Tuy nhiÃªn, do kháº£ nÄƒng tiáº¿p cáº­n háº¡n cháº¿ cá»§a cÃ¡c corpus huáº¥n luyá»‡n nÃ y vÃ  chi phÃ­ cáº¥m ká»µ cá»§a viá»‡c tinh chá»‰nh ngá»¯ cáº£nh dÃ i, cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ hiá»‡n táº¡i thÆ°á»ng thiáº¿u hiá»‡u suáº¥t khi so sÃ¡nh vá»›i cÃ¡c Ä‘á»‘i tÃ¡c Ä‘á»™c quyá»n, vÃ  thÆ°á»ng chá»‰ cÃ³ sáºµn á»Ÿ kÃ­ch thÆ°á»›c nhá» hÆ¡n (vÃ­ dá»¥: 7B/13B).

Vá»›i nhá»¯ng háº¡n cháº¿ nÃ y, cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ´ng yÃªu cáº§u huáº¥n luyá»‡n bá»• sung cho viá»‡c má»Ÿ rá»™ng ngá»¯ cáº£nh trong LLMs trá»Ÿ nÃªn Ä‘áº·c biá»‡t háº¥p dáº«n. CÃ¡c phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n gáº§n Ä‘Ã¢y, bao gá»“m LM-infinite (Han et al., 2023) vÃ  StreamingLLM (Xiao et al., 2023), Ä‘Ã£ cho tháº¥y ráº±ng cÃ¡c LLMs Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t cá»­a sá»• ngá»¯ cáº£nh háº¡n cháº¿ cÃ³ thá»ƒ xá»­ lÃ½ hiá»‡u quáº£ vÄƒn báº£n cÃ³ Ä‘á»™ dÃ i vÃ´ háº¡n (Zhang et al., 2023; 2024; Xiao et al., 2024; Qin et al., 2024). Giáº£ Ä‘á»‹nh ráº±ng cÃ¡c LLMs khÃ´ng thá»ƒ khÃ¡i quÃ¡t hÃ³a cho cÃ¡c vÄƒn báº£n dÃ i hÆ¡n Ä‘á»™ dÃ i huáº¥n luyá»‡n, nhá»¯ng mÃ´ hÃ¬nh nÃ y xá»­ lÃ½ cÃ¡c chuá»—i má»Ÿ rá»™ng báº±ng cÃ¡ch giá»¯ láº¡i cÃ³ chá»n lá»c thÃ´ng tin cá»¥c bá»™ cáº§n thiáº¿t. Nhá»¯ng mÃ´ thá»©c nhÆ° váº­y duy trÃ¬ hiá»‡u quáº£ má»™t Perplexity (PPL) tháº¥p, tuy nhiÃªn chÃºng máº¥t Ä‘i cÃ¡c phá»¥ thuá»™c táº§m xa. Äá»ƒ giá»¯ láº¡i thÃ´ng tin toÃ n cá»¥c, má»™t gÃ³c nhÃ¬n khÃ¡c lÃ  ngoáº¡i suy hiá»‡u quáº£ Ä‘áº¿n Ä‘á»™ dÃ i chuá»—i vÆ°á»£t quÃ¡ nhá»¯ng gÃ¬ gáº·p pháº£i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n (Sun et al., 2022; Kazemnejad et al., 2023; Liu et al., 2023b; Chi et al., 2023). CÃ¡c ká»¹ thuáº­t phá»• biáº¿n cho cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Llama, bao gá»“m Position Interpolation (PI) (Chen et al., 2023b) vÃ  NTK-Aware RoPE (NTK) (LocalLLaMA, 2023b;a), lÃ  nhá»¯ng Ä‘iá»u chá»‰nh cá»§a Rotary Positional Encodings (RoPE) (Su et al., 2022). Nhá»¯ng mÃ£ hÃ³a vá»‹ trÃ­ Ä‘Æ°á»£c má»Ÿ rá»™ng nÃ y Ä‘Ã²i há»i Ã­t bÆ°á»›c tinh chá»‰nh hÆ¡n so vá»›i RoPE gá»‘c, vÃ  chi phÃ­ huáº¥n luyá»‡n cá»§a chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m thÃªm thÃ´ng qua cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° YaRN (Peng et al., 2023) vÃ  CLEX (Chen et al., 2023a). Tuy nhiÃªn, trong má»™t thiáº¿t láº­p khÃ´ng cáº§n huáº¥n luyá»‡n, chÃºng tÃ´i tháº¥y ráº±ng nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng dáº«n Ä‘áº¿n sá»± gia tÄƒng Ä‘Ã¡ng ká»ƒ trong PPL Ä‘áº·c biá»‡t á»Ÿ Ä‘á»™ dÃ i Ä‘áº§u vÃ o nhiá»u hÆ¡n gáº¥p Ä‘Ã´i Ä‘á»™ dÃ i huáº¥n luyá»‡n (Â§4, Báº£ng 1).

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giá»›i thiá»‡u Dual Chunk Attention (DCA), má»™t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘á»ƒ ngoáº¡i suy cá»­a sá»• ngá»¯ cáº£nh cá»§a LLMs. ChÃºng tÃ´i trÃ¡nh viá»‡c giáº£m tá»· lá»‡ tuyáº¿n tÃ­nh cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ hoáº·c tÄƒng táº§n sá»‘ cÆ¡ sá»Ÿ trong RoPE (Su et al., 2022). Thay vÃ o Ä‘Ã³, chÃºng tÃ´i chá»n sá»­ dá»¥ng láº¡i cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ gá»‘c vá»›i embedding cá»§a chÃºng tá»« mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c, nhÆ°ng thiáº¿t káº¿ láº¡i viá»‡c xÃ¢y dá»±ng ma tráº­n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i theo cÃ¡ch cÃ³ thá»ƒ pháº£n Ã¡nh chÃ­nh xÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a hai token má»™t cÃ¡ch trung thá»±c nháº¥t cÃ³ thá»ƒ. ÄÆ°á»£c truyá»n cáº£m há»©ng tá»« cÃ¡c mÃ´ hÃ¬nh attention dá»±a trÃªn chunk hiá»‡u quáº£ (Child et al., 2019; Song et al., 2023; Ratner et al., 2023; He et al., 2024), DCA phÃ¢n Ä‘oáº¡n cÃ¡c tÃ­nh toÃ¡n self-attention cho má»™t chuá»—i dÃ i thÃ nh cÃ¡c chunk nhá», má»—i chunk nhá» hÆ¡n kÃ­ch thÆ°á»›c cá»§a cá»­a sá»• huáº¥n luyá»‡n trÆ°á»›c. DCA bao gá»“m ba thÃ nh pháº§n: (1) intra-chunk attention, Ä‘Æ°á»£c thiáº¿t káº¿ riÃªng Ä‘á»ƒ xá»­ lÃ½ cÃ¡c token trong cÃ¹ng má»™t chunk; (2) inter-chunk attention, Ä‘á»ƒ xá»­ lÃ½ cÃ¡c token giá»¯a cÃ¡c chunk riÃªng biá»‡t; vÃ  (3) successive chunk attention, Ä‘á»ƒ xá»­ lÃ½ cÃ¡c token trong cÃ¡c chunk liÃªn tiáº¿p, riÃªng biá»‡t. Nhá»¯ng cÃ¡ch xá»­ lÃ½ tÆ°Æ¡ng á»©ng nÃ y giÃºp mÃ´ hÃ¬nh náº¯m báº¯t hiá»‡u quáº£ cáº£ cÃ¡c phá»¥ thuá»™c táº§m xa vÃ  táº§m gáº§n trong má»™t chuá»—i. NgoÃ i ra, viá»‡c tÃ­nh toÃ¡n attention dá»±a trÃªn chunk cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p liá»n máº¡ch vá»›i Flash Attention 2 (Dao et al., 2022; Dao, 2023), má»™t yáº¿u tá»‘ quan trá»ng cho viá»‡c má»Ÿ rá»™ng ngá»¯ cáº£nh dÃ i trong cá»™ng Ä‘á»“ng mÃ£ nguá»“n má»Ÿ.1

ChÃºng tÃ´i trÃ¬nh bÃ y má»™t Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n vá» cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i trÃªn má»™t loáº¡t Ä‘a dáº¡ng cÃ¡c tÃ¡c vá»¥ bao gá»“m mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯, truy xuáº¥t passkey, vÃ  cÃ¡c á»©ng dá»¥ng ngá»¯ cáº£nh dÃ i thá»±c táº¿ tráº£i dÃ i tráº£ lá»i cÃ¢u há»i (Pang et al., 2022; KoÄiskÃ½ et al., 2018; Dasigi et al., 2021; An et al., 2023) vÃ  tÃ³m táº¯t (Zhong et al., 2021). KhÃ¡c vá»›i cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y thÆ°á»ng giá»›i háº¡n á»Ÿ viá»‡c xÃ¡c minh trÃªn cÃ¡c mÃ´ hÃ¬nh 7B/13B, hiá»‡u quáº£ huáº¥n luyá»‡n Ä‘Ã¡ng ká»ƒ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i lÃ m cho viá»‡c xÃ¡c thá»±c trÃªn cÃ¡c mÃ´ hÃ¬nh 70B trá»Ÿ nÃªn kháº£ thi, Ä‘áº£m báº£o káº¿t luáº­n vá»¯ng cháº¯c. Äá»ƒ xÃ¡c minh kháº£ nÄƒng ngá»¯ cáº£nh dÃ i cá»§a mÃ´ hÃ¬nh Ä‘á»™c láº­p vá»›i kháº£ nÄƒng tiáº¿p xÃºc dá»¯ liá»‡u trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c, chÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng chÃ­nh bÃ i bÃ¡o nÃ y lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra má»™t loáº¡t cÃ¢u há»i cho cÃ¡c mÃ´ hÃ¬nh.2Káº¿t quáº£ thá»±c nghiá»‡m cá»§a chÃºng tÃ´i tiáº¿t lá»™ nhá»¯ng hiá»ƒu biáº¿t sau:

1.Ngoáº¡i suy . Vá» mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯, DCA Ä‘Ã¡nh dáº¥u má»™t tiáº¿n bá»™ Ä‘Ã¡ng ká»ƒ cho cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n. NÃ³ Ä‘áº§u tiÃªn cho tháº¥y ráº±ng cÃ¡c LLMs vá»›i cá»­a sá»• ngá»¯ cáº£nh 4k cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘áº¿n hÆ¡n 32k mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n, duy trÃ¬ sá»± gia tÄƒng khÃ´ng Ä‘Ã¡ng ká»ƒ trong PPL, trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y thÆ°á»ng tháº¥t báº¡i á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh vÆ°á»£t quÃ¡ 8k. HÆ¡n ná»¯a, chÃºng tÃ´i chá»©ng minh ráº±ng Llama2 70B, khi Ä‘Æ°á»£c tÃ­ch há»£p vá»›i DCA, thá»ƒ hiá»‡n kháº£ nÄƒng ngoáº¡i suy Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½ kÃ­ch thÆ°á»›c ngá»¯ cáº£nh vÆ°á»£t quÃ¡ 100k token.

2.TÃ­nh trá»±c giao . DCA trá»±c giao vá»›i cÃ¡c mÃ£ hÃ³a vá»‹ trÃ­ Ä‘Æ°á»£c má»Ÿ rá»™ng phá»• biáº¿n hiá»‡n táº¡i nhÆ° PI (Chen et al., 2023b) vÃ  NTK (LocalLLaMA, 2023b;a). ChÃºng tÃ´i chá»©ng minh thá»±c nghiá»‡m ráº±ng cÃ¡c LLMs ngá»¯ cáº£nh dÃ i hiá»‡n táº¡i, Ä‘Ã£ há»— trá»£ cá»­a sá»• ngá»¯ cáº£nh 32k, cÃ³ thá»ƒ ngoáº¡i suy thÃªm Ä‘áº¿n Ä‘á»™ dÃ i ngá»¯ cáº£nh 192k trong khi duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t passkey cao vÃ  perplexity tháº¥p.

1KhÃ´ng cÃ³ Flash Attention, sá»‘ token Ä‘áº§u vÃ o tá»‘i Ä‘a cho Llama2 7B/13B lÃ  khoáº£ng 16k, vÃ  cho Llama2 70B, lÃ  5k khi thá»­ nghiá»‡m trÃªn hai GPU A100 80G trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i
2ChÃºng tÃ´i má»i nhá»¯ng Ä‘á»™c giáº£ quan tÃ¢m xem xÃ©t káº¿t quáº£ trong Báº£ng 6,7

3.Hiá»ƒu biáº¿t Ngá»¯ cáº£nh DÃ i . ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ DCA trÃªn má»™t bá»™ benchmark hiá»ƒu biáº¿t ngá»¯ cáº£nh dÃ i trong cáº£ thiáº¿t láº­p zero-shot vÃ  few-shot. Káº¿t quáº£ cho tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh khÃ´ng cáº§n huáº¥n luyá»‡n cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cÃ³ thá»ƒ so sÃ¡nh, hoáº·c tháº­m chÃ­ vÆ°á»£t qua, hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i Ä‘Æ°á»£c xÃ¢y dá»±ng thÃ´ng qua huáº¥n luyá»‡n liÃªn tá»¥c tá»‘n kÃ©m.

2. Bá»‘i cáº£nh
2.1. MÃ£ hÃ³a Vá»‹ trÃ­
Embedding vá»‹ trÃ­ gá»‘c tá»« mÃ´ hÃ¬nh Transformer (Vaswani et al., 2017) Ã¡nh xáº¡ cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i Ä‘áº¿n khÃ´ng gian Ä‘áº·c trÆ°ng d-chiá»u, vÃ  káº¿t há»£p Ä‘iá»u nÃ y vÃ o lá»›p Ä‘áº§u vÃ o. Äáº§u vÃ o x, liÃªn káº¿t vá»›i chá»‰ sá»‘ vá»‹ trÃ­ i, Ä‘Æ°á»£c biá»ƒu thá»‹ nhÆ°: xi=x+f(i), trong Ä‘Ã³ f:Nâ†’Rd lÃ  hÃ m embedding (vá»‹ trÃ­).

Má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p mÃ£ hÃ³a vá»‹ trÃ­ phá»• biáº¿n nháº¥t cho LLMs lÃ  Rotary Positional Encoding (RoPE) (Su et al., 2022). RoPE trÃ¡nh phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng cá»§a viá»‡c truyá»n thÃ´ng tin vá»‹ trÃ­ vÃ o lá»›p Ä‘áº§u vÃ o. Thay vÃ o Ä‘Ã³, nÃ³ trá»±c tiáº¿p káº¿t há»£p thÃ´ng tin nÃ y vÃ o lá»›p attention. Äá»‘i vá»›i má»™t chuá»—i l token, chÃºng tÃ´i kÃ½ hiá»‡u cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho keys vÃ  queries3 nhÆ° sau:
Pk=Pq= [0,1, . . . , l âˆ’1]. (1)

ChÃºng tÃ´i láº¡m dá»¥ng kÃ½ hiá»‡u f cho hÃ m embedding cá»§a RoPE, cháº¥p nháº­n má»™t vector query q hoáº·c má»™t vector key k, vÃ  chá»‰ sá»‘ vá»‹ trÃ­ tÆ°Æ¡ng á»©ng lÃ m Ä‘á»‘i sá»‘. VÃ­ dá»¥, chÃºng ta cÃ³ qi=f(q, Pq[i]) vÃ  kj=f(k, Pk[j]), trong Ä‘Ã³ [i] kÃ½ hiá»‡u pháº§n tá»­ thá»© i cá»§a danh sÃ¡ch. Trong trÆ°á»ng há»£p Ä‘Æ¡n giáº£n nháº¥t, chÃºng ta cÃ³ P[i] =i. HÃ m f4 xuáº¥t ra má»™t vector query hoáº·c key Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh sá»­a Ä‘Ã³ng gÃ³i chá»‰ sá»‘ vá»‹ trÃ­, Ä‘áº£m báº£o ráº±ng tÃ­ch trong giá»¯a query thá»© i vÃ  key thá»© j (vá»›i iâ‰¥j) náº¯m báº¯t thÃ´ng tin vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i Pq[i]âˆ’Pk[j]. Máº·c dÃ¹ RoPE nháº­n cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i lÃ m Ä‘áº§u vÃ o, káº¿t quáº£ cá»§a tÃ­ch trong cá»§a q,k chá»‰ chá»©a thÃ´ng tin vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i (tá»©c lÃ , chÃºng ta cÃ³ qâŠ¤ikj=qâŠ¤mknkhi mâˆ’n=iâˆ’j). Ma tráº­n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i M Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi RoPE trong quÃ¡ trÃ¬nh self-attention cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° má»™t ma tráº­n Toeplitz, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1. Má»—i pháº§n tá»­ M[i][j] =Pq[i]âˆ’Pk[j] biá»ƒu thá»‹ vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a qi (query thá»© i) vÃ  kj (key thá»© j).

2.2. Ngoáº¡i suy cá»§a RoPE
CÃ´ng viá»‡c gáº§n Ä‘Ã¢y (Chen et al., 2023b; Chowdhury & Caragea, 2023; Chen et al., 2023a) Ä‘Ã£ chá»©ng minh ráº±ng cÃ¡c LLMs vá»›i RoPE gá»‘c thiáº¿u kháº£ nÄƒng ngoáº¡i suy Ä‘á»™ dÃ i máº¡nh máº½, thÆ°á»ng dáº«n Ä‘áº¿n suy giáº£m hiá»‡u suáº¥t khi

3Queries vÃ  keys thÆ°á»ng Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch chiáº¿u Ä‘áº§u vÃ o x thÃ´ng qua má»™t lá»›p tuyáº¿n tÃ­nh cÃ³ thá»ƒ há»c
4Má»™t triá»ƒn khai Ä‘iá»ƒn hÃ¬nh cá»§a f cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong modeling-llama.py DÃ²ng 211 apply rotary posemb()

2

--- TRANG 2 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

03214569872103458769102347658901236547801254367014325603214521034102301201001234567891011ğ‘ƒ!:Position ids cá»§a ğ’Œğ‘ƒ": Position ids cá»§a ğ’’
11109876543210
ğ‘ƒ!ğ‘ƒ"
101110

HÃ¬nh 1. Trá»±c quan hÃ³a Ma tráº­n Vá»‹ trÃ­ TÆ°Æ¡ng Ä‘á»‘i M sá»­ dá»¥ng RoPE tiÃªu chuáº©n. Cá»­a sá»• ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c lÃ  6 vÃ  Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o lÃ  12. Trá»¥c x Pk chá»‰ ra cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cá»§a keys, trong khi trá»¥c y Pq tÆ°Æ¡ng á»©ng vá»›i cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cá»§a queries. Má»—i pháº§n tá»­ ma tráº­n M[i][j] Ä‘áº¡i diá»‡n cho Ä‘á»™ lá»‡ch vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i Pq[i]âˆ’Pk[j].

Ä‘Æ°á»£c thá»­ nghiá»‡m trÃªn cÃ¡c chuá»—i Ä‘áº§u vÃ o dÃ i hÆ¡n nhá»¯ng gÃ¬ tháº¥y trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c (Li et al., 2023b; Zhu et al., 2023). CÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y (Chen et al., 2023b; Su, 2023; Jin et al., 2024) chá»§ yáº¿u quy viá»‡c háº¡n cháº¿ nÃ y cho sá»± hiá»‡n diá»‡n cá»§a cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i chÆ°a tháº¥y trong giai Ä‘oáº¡n huáº¥n luyá»‡n trÆ°á»›c vÃ  Ä‘á» xuáº¥t thiáº¿t káº¿ láº¡i ma tráº­n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i. NhÆ° Ä‘Æ°á»£c minh há»a trong vÃ­ dá»¥ trong HÃ¬nh 1, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c chuá»—i 6 token, trong khi suy luáº­n Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn má»™t chuá»—i 12 token. Sá»± khÃ¡c biá»‡t nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n PPL cao vÃ¬ cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i vÆ°á»£t quÃ¡ 6 chÆ°a bao giá» Ä‘Æ°á»£c huáº¥n luyá»‡n. CÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y, nhÆ° PI vÃ  NTK, nháº±m giáº£m thiá»ƒu váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch giáº£m Ä‘á»™ lá»›n cá»§a M[i][j] Ä‘á»ƒ Ä‘áº£m báº£o nÃ³ náº±m trong pháº¡m vi Ä‘á»™ dÃ i ngá»¯ cáº£nh quan sÃ¡t Ä‘Æ°á»£c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. VÃ­ dá»¥, Ã¡p dá»¥ng PI trong vÃ­ dá»¥ nÃ y sáº½ Ä‘iá»u chá»‰nh cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ báº±ng cÃ¡ch chia tá»· lá»‡: Pq[i]â‡’Pq[i]/2 vÃ  Pk[j]â‡’Pk[j]/2. Do Ä‘Ã³, ma tráº­n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cÅ©ng Ä‘Æ°á»£c chia tá»· lá»‡: M[i][j] =M[i][j]/2. á» Ä‘Ã¢y, há»‡ sá»‘ chia tá»· lá»‡ 2 =126 Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£m tá»· lá»‡ cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i, dáº«n Ä‘áº¿n Ä‘á»™ phÃ¢n giáº£i thÃ´ng tin vá»‹ trÃ­ kÃ©m hÆ¡n vÃ  kháº£ nÄƒng ngoáº¡i suy yáº¿u.

3. PhÆ°Æ¡ng phÃ¡p
Trong pháº§n nÃ y, chÃºng tÃ´i mÃ´ táº£ chi tiáº¿t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Dual Chunk Attention cá»§a chÃºng tÃ´i. Má»™t vÃ­ dá»¥ cháº¡y cá»§a dual chunk attention Ä‘Æ°á»£c hiá»ƒn thá»‹ trong hÃ¬nh 2. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i báº¯t Ä‘áº§u tá»« intra-chunk attention (HÃ¬nh 2 (a)) lÃ  má»™t mÃ´ hÃ¬nh attention hiá»‡u quáº£ dá»±a trÃªn chunk (Child et al., 2019; Song et al., 2023). Embedding vá»‹ trÃ­ cá»§a má»—i chunk dao Ä‘á»™ng tá»« 0 Ä‘áº¿n kÃ­ch thÆ°á»›c chunk trong Ä‘Ã³ kÃ­ch thÆ°á»›c chunk Ä‘Æ°á»£c Ä‘áº·t nhá» hÆ¡n Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c. MÃ´ hÃ¬nh intra-chunk attention thá»±c táº¿ cÃ³ nghÄ©a lÃ  trá»±c tiáº¿p cáº¯t ngáº¯n Ä‘áº§u vÃ o tá»« bÃªn trÃ¡i Ä‘áº¿n kÃ­ch thÆ°á»›c chunk loáº¡i bá» thÃ´ng tin tá»« cÃ¡c chunk trÆ°á»›c Ä‘Ã³. Viá»‡c cáº¯t ngáº¯n nhÆ° váº­y thÆ°á»ng mang láº¡i perplexity tháº¥p (Xiao et al., 2023) nhÆ°ng máº¥t thÃ´ng tin táº§m xa. Äá»ƒ giáº£i quyáº¿t háº¡n cháº¿ nÃ y, chÃºng tÃ´i triá»ƒn khai inter-chunk attention (HÃ¬nh 2(b)) cho phÃ©p tÃ­nh toÃ¡n attention giá»¯a cÃ¡c chunk khÃ¡c nhau, máº·c dÃ¹ vá»›i Ä‘á»™ chÃ­nh xÃ¡c Ã­t hÆ¡n cho cÃ¡c vá»‹ trÃ­ token xa. Cuá»‘i cÃ¹ng, chÃºng tÃ´i giá»›i thiá»‡u successive-chunk attention, má»™t biáº¿n thá»ƒ cá»§a inter-chunk attention Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 2 (c), Ä‘Æ°á»£c Ã¡p dá»¥ng cá»¥ thá»ƒ khi hai chunk liá»n ká» Ä‘á»ƒ báº£o toÃ n tÃ­nh cá»¥c bá»™. Má»™t nghiÃªn cá»©u phÃ¢n tÃ­ch Ä‘á»ƒ hiá»ƒn thá»‹ cÃ¡ch cÃ¡c cÆ¡ cháº¿ attention nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n PPL vÃ  Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t passkey cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong HÃ¬nh 4.

3.1. Intra-Chunk Attention
Intra-Chunk Attention Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n tÃ­ch trong cá»§a queries vÃ  keys trong cÃ¹ng má»™t chunk. Äá»‘i vá»›i má»™t chuá»—i dÃ i cÃ³ Ä‘á»™ dÃ i l, chÃºng tÃ´i phÃ¢n vÃ¹ng chuá»—i thÃ nh n=ls chunk, Ä‘áº£m báº£o ráº±ng cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ trong má»—i chunk sáº½ khÃ´ng vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c chunk s. HÃ¬nh 2 (a) minh há»a quÃ¡ trÃ¬nh phÃ¢n Ä‘oáº¡n má»™t chuá»—i 12 token vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c 10 thÃ nh 2 chunk, vá»›i má»—i chunk bao gá»“m s= 6<10 token. Sau Ä‘Ã³ cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho keys vÃ  queries Ä‘Æ°á»£c chia tá»· lá»‡ trong kÃ­ch thÆ°á»›c chunk 6. Cá»¥ thá»ƒ, chÃºng ta cÃ³ cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho keys Pk= [0,1,2,3,4,5|{z}chunk 0,0,1,2,3,4,5|{z}chunk 1] vÃ  PIntraq=Pk, trong Ä‘Ã³ PIntraq cÃ³ nghÄ©a lÃ  cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho queries trong quÃ¡ trÃ¬nh intra-chunk attention. Äá»ƒ hÃ¬nh thá»©c hÃ³a, trong intra-chunk attention, chÃºng tÃ´i Ä‘iá»u chá»‰nh cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho queries vÃ  keys nhÆ° sau:
PIntraq=Pk= [0,1, . . . , l âˆ’1] mod s. (2)

Äá»‘i vá»›i cÃ¡c chá»‰ sá»‘ tuyá»‡t Ä‘á»‘i i vÃ  j trong cÃ¹ng má»™t chunk tá»©c lÃ  âŒŠi/sâŒ‹=âŒŠj/sâŒ‹, thá»a mÃ£n 0â‰¤jâ‰¤i < l , pháº§n tá»­ M[i][j] Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c mÃ£ hÃ³a vá»‹ trÃ­ cá»§a query vÃ  key:
M[i][j] =PIntraq[i]âˆ’Pk[j]. (3)

Khi âŒŠi/sâŒ‹=âŒŠj/sâŒ‹, chÃºng tÃ´i tÃ­nh toÃ¡n M[i][j] theo Eq. 3. M Ä‘Æ°á»£c tÃ­nh toÃ¡n cá»§a vÃ­ dá»¥ trÆ°á»›c Ä‘Ã³ trong Ä‘Ã³ chÃºng ta cÃ³ Ä‘á»™ dÃ i chuá»—i 12 vÃ  kÃ­ch thÆ°á»›c chunk 6, Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2 (a). Äiá»ƒm attention intra-chunk cho sá»± tÆ°Æ¡ng tÃ¡c giá»¯a query thá»© i vÃ  key thá»© j sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh nhÆ°:
qâŠ¤ikj=f(q, PIntraq[i])âŠ¤f(k, Pk[j]). (4)

3.2. Inter-Chunk Attention
Äá»ƒ tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c chunk khÃ¡c, chÃºng tÃ´i giá»›i thiá»‡u Inter-Chunk Attention. Trong cÃ¡c LLMs dá»±a trÃªn Llama, cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho queries lá»›n hÆ¡n nhá»¯ng cá»§a keys Ä‘á»ƒ pháº£n Ã¡nh luá»“ng thÃ´ng tin tá»« trÃ¡i sang pháº£i, tá»©c lÃ , chÃºng ta cÃ³ Pq[i]â‰¥Pk[j] báº¥t cá»© khi nÃ o iâ‰¥j. Sá»­ dá»¥ng Pq=PIntraq vÃ  Pk Ä‘á»ƒ tÃ­nh toÃ¡n attention giá»¯a cÃ¡c chunk khÃ¡c nhau rÃµ rÃ ng vi pháº¡m tÃ­nh cháº¥t nÃ y. VÃ­ dá»¥, xem xÃ©t qs vÃ  k1 trong Ä‘Ã³ s lÃ  kÃ­ch thÆ°á»›c chunk, khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘á»‘i cá»§a chÃºng Ä‘Æ°á»£c cho bá»Ÿi PIntraq[s] = 0 vÃ  Pk[1] = 1 lÃ  -1. ChÃºng tÃ´i duy trÃ¬ cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho keys Pk xem xÃ©t KV cache vÃ  tÃ¬m kiáº¿m má»™t táº­p Pq má»›i trong quÃ¡ trÃ¬nh inter-chunk attention, Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  Pinterq.

Dá»±a trÃªn Eq. 2, cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cho keys Ä‘Æ°á»£c láº·p láº¡i theo chu ká»³ vá»›i chá»‰ sá»‘ vá»‹ trÃ­ tá»‘i Ä‘a max(Pk) =sâˆ’1. Äá»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c queries cÃ³ chá»‰ sá»‘ vá»‹ trÃ­ lá»›n hÆ¡n táº¥t cáº£ keys tá»« cÃ¡c chunk trÆ°á»›c Ä‘Ã³, Má»™t chiáº¿n lÆ°á»£c Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n biá»‡t cÃ¡c queries xa lÃ  gÃ¡n cho chÃºng má»™t chá»‰ sá»‘ vá»‹ trÃ­ lá»›n Ä‘Ã¡ng ká»ƒ, cháº³ng háº¡n nhÆ° chá»‰ sá»‘ vá»‹ trÃ­ tá»‘i Ä‘a trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c câˆ’1>max(Pk), trong Ä‘Ã³ c lÃ  Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c:
PInterq= [câˆ’1, câˆ’1, . . . câˆ’1| {z }l pháº§n tá»­], (5)

Khi âŒŠi/sâŒ‹ Ì¸=âŒŠj/sâŒ‹, chÃºng ta cÃ³ thá»ƒ Ä‘Æ°a ra ma tráº­n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i M vá»›i qi vÃ  kj tá»« cÃ¡c chunk riÃªng biá»‡t nhÆ°:
M[i][j] =PIntraq[i]âˆ’Pk[j] =câˆ’1âˆ’Pk[j]â‰¥câˆ’s.
(6)

NhÆ° Ä‘Æ°á»£c pháº£n Ã¡nh trong HÃ¬nh 2 (b), chÃºng tÃ´i gÃ¡n PInterq vá»›i má»™t giÃ¡ trá»‹ khÃ´ng Ä‘á»•i câˆ’1 = 9 cho táº¥t cáº£ cÃ¡c vá»‹ trÃ­, lá»›n hÆ¡n chá»‰ sá»‘ vá»‹ trÃ­ tá»‘i Ä‘a sâˆ’1 = 5 trong Pk. ChÃºng tÃ´i hoÃ n thÃ nh pháº§n cÃ²n láº¡i cá»§a ma tráº­n M Ä‘á»ƒ trá»‘ng bá»Ÿi intra-chunk attention vá»›i Eq. 6.

3.3. Successive-Chunk Attention
Successive-Chunk Attention cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t cho inter-chunk attention, Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ duy trÃ¬ tÃ­nh cá»¥c bá»™ cá»§a LLMs trong Ä‘Ã³ tÃ­nh cá»¥c bá»™ cÃ³ nghÄ©a lÃ  LLMs cÃ³ xu hÆ°á»›ng dá»±a ráº¥t nhiá»u vÃ o cÃ¡c token lÃ¢n cáº­n Ä‘á»ƒ dá»± Ä‘oÃ¡n token tiáº¿p theo (Xiao et al., 2023; Han et al., 2023). ÄÆ¡n giáº£n sá»­ dá»¥ng inter-chunk attention cÃ³ thá»ƒ khÃ´ng cÃ²n giá»¯ vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i chÃ­nh xÃ¡c giá»¯a cÃ¡c token lÃ¢n cáº­n, dáº«n Ä‘áº¿n suy giáº£m hiá»‡u suáº¥t.

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2(b), trong Ä‘Ã³ kÃ­ch thÆ°á»›c chunk lÃ  s= 6 vÃ  Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c lÃ  c= 10 , key cuá»‘i cÃ¹ng cá»§a chunk Ä‘áº§u tiÃªn, k5, vá»›i Pk[5] = 5 , Ä‘Æ°á»£c theo sau bá»Ÿi query Ä‘áº§u tiÃªn cá»§a chunk thá»© hai, q6, vá»›i chá»‰ sá»‘ vá»‹ trÃ­ PInterq[6] = 9 . Máº·c dÃ¹ khoáº£ng cÃ¡ch tuyá»‡t Ä‘á»‘i cá»§a chÃºng lÃ  1, khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘á»‘i giá»¯a q6 vÃ  k5 lÃ  PInterq[6]âˆ’Pk[5] = 4 . Cáº¥u hÃ¬nh nÃ y thÃ¡ch thá»©c kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ duy trÃ¬ tÃ­nh cá»¥c bá»™ trong cÆ¡ cháº¿ attention cá»§a nÃ³.

May máº¯n thay, váº¥n Ä‘á» nÃ y chá»‰ xáº£y ra giá»¯a cÃ¡c chunk liÃªn tiáº¿p, vÃ¬ váº­y chÃºng tÃ´i giá»›i thiá»‡u má»™t successive-chunk attention má»›i Ä‘á»ƒ xá»­ lÃ½ trÆ°á»ng há»£p nÃ y. Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘á» xuáº¥t duy trÃ¬ tÃ­nh cá»¥c bá»™ cá»§a w token lÃ¢n cáº­n thÃ´ng qua Ä‘iá»u chá»‰nh w chá»‰ sá»‘ vá»‹ trÃ­ Ä‘áº§u tiÃªn trong cho PInterq. VÃ­ dá»¥, trong HÃ¬nh 2 (c), cho ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c c= 10 , kÃ­ch thÆ°á»›c chunk s= 6, vÃ  PInterq= [9,9,9,9,9,9|{z}chunk 0,9,9,9,9,9,9|{z}chunk 1], cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ PSuccq cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t thÃ nh [6,7,8,9,9,9|{z}chunk 0,6,7,8,9,9,9|{z}chunk 1] Ä‘á»ƒ tÃ­nh toÃ¡n attention giá»¯a cÃ¡c chunk liÃªn tiáº¿p, náº¿u chÃºng ta giá»¯ má»™t cá»­a sá»• cá»¥c bá»™ w= 4. ChÃ­nh thá»©c, vá»›i kÃ­ch thÆ°á»›c chunk s, kÃ­ch thÆ°á»›c huáº¥n luyá»‡n trÆ°á»›c c vÃ  cá»­a sá»• cá»¥c bá»™ w chÃºng ta cÃ³:
PSuccq= [w pháº§n tá»­z }| {s, s+ 1, . . . , s +wâˆ’1, câˆ’1, . . . , c âˆ’1| {z }giá»‘ng nhau cho táº¥t cáº£ chunks],(7)

trong Ä‘Ã³ w cÃ³ nghÄ©a lÃ  kÃ­ch thÆ°á»›c cá»­a sá»• cá»¥c bá»™ vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t trá»±c tiáº¿p thÃ nh sá»± khÃ¡c biá»‡t giá»¯a Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c vÃ  kÃ­ch thÆ°á»›c chunk câˆ’s. Äá»‘i vá»›i i, j tá»« cÃ¡c chunk liÃªn tiáº¿p, káº¿t quáº£ tÃ­nh toÃ¡n cá»§a M[i][j] sá»­ dá»¥ng PSuccq vÃ  Pk Ä‘Æ°á»£c pháº£n Ã¡nh trong HÃ¬nh 2 (c) trong Ä‘Ã³ bÃ³ng cÃ³ nghÄ©a lÃ  cá»­a sá»• cá»¥c bá»™ káº¿t quáº£. Eq 7 Ä‘áº£m báº£o ráº±ng w keys lÃ¢n cáº­n cÃ³ khoáº£ng cÃ¡ch gáº§n nháº¥t vá»›i query hiá»‡n táº¡i.

Báº±ng cÃ¡ch káº¿t há»£p intra-chunk, inter-chunk, vÃ  successive-

4

--- TRANG 3 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

(a) Intra-chunk Attention(b) Inter-chunk Attention(c) Successive-Chunk Attention03214599999921034888888102377777701266666601555555044444403214521034102301201012345012345!!: Position ids cá»§a "999999999999
!!!"#$%&'Chunk1!"#$%&': Position ids cá»§a "(inter-chunk attention)Chunk2
!=6032145210341023012010032145210341023012010012345012345!!: Position ids cá»§a "543210543210
!!!"#$%&'Chunk1!"#$%&': Position ids cá»§a "(intra-chunkattention)Chunk2
!=6032145698799210345876881023476577012365466012543550143244032145210341023012010012345012345!!: Position ids cá»§a "999876999876
!!!"#$%&'Chunk1Chunk2
!"#$%&': Position ids cá»§a "(intra-chunkattention)543210543210!"()**!"#$%(&: Position ids cá»§a "(inter-chunkattention)!=4"=6

HÃ¬nh 2. Trá»±c quan hÃ³a Ma tráº­n Vá»‹ trÃ­ TÆ°Æ¡ng Ä‘á»‘i M sá»­ dá»¥ng Dual Chunk Attention (DCA), vá»›i kÃ­ch thÆ°á»›c chunk s= 6, kÃ­ch thÆ°á»›c cá»­a sá»• huáº¥n luyá»‡n trÆ°á»›c c= 10 , vÃ  kÃ­ch thÆ°á»›c cá»­a sá»• cá»¥c bá»™ w= 4 Ä‘Æ°á»£c chÃº thÃ­ch bá»Ÿi bÃ³ng trong (c). Chuá»—i Ä‘Æ°á»£c phÃ¢n Ä‘oáº¡n thÃ nh cÃ¡c chunk Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i khÃ´ng vÆ°á»£t quÃ¡ 9. Pháº§n tá»­ ma tráº­n M[i][j] Ä‘áº¡i diá»‡n cho vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a vector query thá»© i q vÃ  vector key thá»© j k. KhÃ¡c vá»›i cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ gá»‘c cho q,k trong RoPE, DCA sá»­ dá»¥ng cÃ¡c táº­p chá»‰ sá»‘ vá»‹ trÃ­ riÃªng biá»‡t Pk, PIntraq (Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Eq. 2), PInterq (Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Eq. 5), PSuccq (Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Eq. 7) Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘á»‘i trong cÃ¡c pháº§n khÃ¡c nhau cá»§a M.

chunk attention, cuá»‘i cÃ¹ng chÃºng tÃ´i tÃ­nh toÃ¡n M[i][j] nhÆ°:
M[i][j] =

PIntraq[i]âˆ’Pk[j]náº¿uâŒŠi/sâŒ‹ âˆ’ âŒŠj/sâŒ‹= 0
PSuccq[i]âˆ’Pk[j]náº¿uâŒŠi/sâŒ‹ âˆ’ âŒŠj/sâŒ‹= 1
PInterq[i]âˆ’Pk[j]náº¿uâŒŠi/sâŒ‹ âˆ’ âŒŠj/sâŒ‹>1.

TÃ­ch trong cá»§a q,k trong DCA do Ä‘Ã³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ°:
qTikj=

f(q, PIntraq[i])Tf(k, Pk[j]),náº¿uâŒŠi/sâŒ‹ âˆ’ âŒŠj/sâŒ‹= 0
f(q, PSuccq[i])Tf(k, Pk[j]),náº¿uâŒŠi/sâŒ‹ âˆ’ âŒŠj/sâŒ‹= 1
f(q, PInterq[i])Tf(k, Pk[j]),náº¿uâŒŠi/sâŒ‹ âˆ’ âŒŠj/sâŒ‹>1,
(8)

3.4. Chuáº©n hÃ³a
Lá»›p Softmax CÃ¡c tÃ­nh toÃ¡n tÃ­ch trong trong DCA Ä‘Æ°á»£c hÃ¬nh thá»©c hÃ³a nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong PhÆ°Æ¡ng trÃ¬nh 8. Sau Ä‘Ã³, má»™t hÃ m softmax Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ chuáº©n hÃ³a cÃ¡c tÃ­ch trong Ä‘Æ°á»£c tÃ­nh toÃ¡n:
pi=softmax (qâŠ¤ik0âˆšd,qâŠ¤ik1âˆšd, . . . ,qiâŠ¤kiâˆšd
).(9)
trong Ä‘Ã³ d kÃ½ hiá»‡u chiá»u cá»§a cÃ¡c tráº¡ng thÃ¡i áº©n.

Flash Attention MÃ£ giáº£ kiá»ƒu PyTorch vá» cÃ¡ch tÃ­ch há»£p DCA vá»›i Flash Attention 2 (Dao, 2023), cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Thuáº­t toÃ¡n 1. Lá»i giáº£i thÃ­ch vÃ  phÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p cá»§a mÃ£ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Phá»¥ lá»¥c Â§A.3. Vá»›i Flash Attention, DCA Ä‘áº¡t Ä‘Æ°á»£c viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU vÃ  tá»‘c Ä‘á»™ suy luáº­n cÃ³ thá»ƒ so sÃ¡nh vá»›i self-attention gá»‘c trong Llama. Káº¿t quáº£ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong HÃ¬nh 3.

4. ThÃ­ nghiá»‡m
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ khung framework cá»§a chÃºng tÃ´i, DCA, trÃªn cÃ¡c biáº¿n thá»ƒ khÃ¡c nhau cá»§a Llama2 (Touvron et al., 2023b), cá»¥ thá»ƒ lÃ  cÃ¡c mÃ´ hÃ¬nh 7B, 13B, vÃ  70B, cÃ¹ng vá»›i cÃ¡c Ä‘á»‘i tÃ¡c chat cá»§a chÃºng, cÃ³ ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c 4k. MÃ´ hÃ¬nh dá»±a trÃªn Llama2 cá»§a chÃºng tÃ´i Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  CHUNK LLAMA 2. NgoÃ i ra, chÃºng tÃ´i Ã¡p dá»¥ng DCA cho hai mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i mÃ£ nguá»“n má»Ÿ phá»• biáº¿n: (1) Together-32k (Together, 2023)5: MÃ´ hÃ¬nh nÃ y sá»­ dá»¥ng Positional Interpolation (PI) lÃ m mÃ£ hÃ³a vá»‹ trÃ­ cá»§a nÃ³. PhiÃªn báº£n Ä‘Æ°á»£c tÄƒng cÆ°á»ng DCA cá»§a mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  ChunkTogether. (2) CodeLlama (RoziÃ¨re et al., 2023)6: MÃ´ hÃ¬nh nÃ y Ã¡p dá»¥ng NTK-Aware RoPE. Sau khi Ã¡p dá»¥ng DCA, mÃ´ hÃ¬nh káº¿t quáº£ Ä‘Æ°á»£c gá»i lÃ  ChunkCodeLlama.

4.1. Thiáº¿t láº­p ThÃ­ nghiá»‡m
DCA cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai báº±ng má»™t monkey patch Ä‘á»ƒ thay tháº¿ mÃ£ suy luáº­n cá»§a LlamaAttention gá»‘c . Nhá»

5https://huggingface.co/togethercomputer/LLaMA-2-7B-32K
6https://huggingface.co/codellama

Flash Attention 2 (Dao, 2023), Ä‘á»‘i vá»›i cÃ¡c biáº¿n thá»ƒ 7B/13B cá»§a CHUNK LLAMA 2, chÃºng tÃ´i chá»‰ cáº§n má»™t GPU NVIDIA A100-80G duy nháº¥t cho suy luáº­n. Khi má»Ÿ rá»™ng lÃªn cÃ¡c mÃ´ hÃ¬nh 70B, hai GPU A100 lÃ  Ä‘á»§ Ä‘á»ƒ quáº£n lÃ½ suy luáº­n trong Ä‘á»™ dÃ i ngá»¯ cáº£nh 16k. KÃ­ch thÆ°á»›c chunk s cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t Ä‘iá»ƒn hÃ¬nh lÃ  34 Ä‘á»™ dÃ i huáº¥n luyá»‡n vÃ  Ä‘á»‘i vá»›i Llama2, giÃ¡ trá»‹ nÃ y lÃ  3072. Sá»‘ lÆ°á»£ng chunk phá»¥ thuá»™c vÃ o Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o.

NgoÃ i cÃ¡c Ä‘Ã¡nh giÃ¡ khÃ´ng cáº§n huáº¥n luyá»‡n, chÃºng tÃ´i cÅ©ng cung cáº¥p cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh tá»« cÃ¡c checkpoint Llama2 7B/13B. QuÃ¡ trÃ¬nh tinh chá»‰nh nÃ y táº­n dá»¥ng chá»‰ cÃ¡c cuá»™c há»™i thoáº¡i dÃ i vá»›i 16k token Ä‘áº§u vÃ o, theo Vicuna (LMSYS, 2023) vÃ  LongChat (Li et al., 2023a). Táº­p dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c láº¥y tá»« ShareGPT7 vÃ  AlpacaGPT4 (Taori et al., 2023). Äá»‘i vá»›i dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o tá»« ShareGPT, chÃºng tÃ´i cá»¥ thá»ƒ tuyá»ƒn chá»n má»™t táº­p con báº±ng cÃ¡ch trÃ­ch xuáº¥t cÃ¡c pháº£n há»“i Ä‘Æ°á»£c táº¡o bá»Ÿi GPT-4, vÃ  cÃ¡c Ä‘á»‘i thoáº¡i vÆ°á»£t quÃ¡ 4k token vá» Ä‘á»™ dÃ i. Viá»‡c lá»±a chá»n nÃ y dáº«n Ä‘áº¿n má»™t táº­p há»£p 5,405 trÆ°á»ng há»£p huáº¥n luyá»‡n.

ChÃºng tÃ´i tuÃ¢n thá»§ cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n nhÆ° Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh trong kho LongChat8. ChÃºng tÃ´i tiáº¿p tá»¥c tinh chá»‰nh Llama2 vá»›i hÆ¡n 16k bÆ°á»›c vá»›i kÃ­ch thÆ°á»›c batch lÃ  1. QuÃ¡ trÃ¬nh tinh chá»‰nh tá»‘n khoáº£ng 40 giá» GPU cho mÃ´ hÃ¬nh 7B vÃ  60 giá» GPU cho biáº¿n thá»ƒ 13B.

Táº­p dá»¯ liá»‡u ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ chuá»—i dÃ i cá»§a CHUNK LLAMA 2 trÃªn táº­p dá»¯ liá»‡u corpus sÃ¡ch PG19 (Rae et al., 2020), vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh dao Ä‘á»™ng tá»« 4k Ä‘áº¿n 192k token. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh 7B vÃ  13B, chÃºng tÃ´i sá»­ dá»¥ng cá»­a sá»• trÆ°á»£t 256, phÃ¹ há»£p vá»›i cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y (Peng et al., 2023; Chen et al., 2023c). Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh 70B, chÃºng tÃ´i Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c cá»­a sá»• trÆ°á»£t thÃ nh 2048 vÃ  khi xá»­ lÃ½ cÃ¡c ngá»¯ cáº£nh vÆ°á»£t quÃ¡ 96k token, chÃºng tÃ´i Ä‘iá»u chá»‰nh cá»­a sá»• trÆ°á»£t thÃ nh má»™t ná»­a Ä‘á»™ dÃ i Ä‘áº§u vÃ o xem xÃ©t thá»i gian cháº¡y. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m few-shot, chÃºng tÃ´i theo cÃ¡c thiáº¿t láº­p trong Llama2 Long (Xiong et al., 2023). Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t 0-shot cá»§a CHUNK LLAMA 2 trÃªn NarrativeQA (KoÄiskÃ½ et al., 2018), 1-shot trÃªn QMSum (Zhong et al., 2021), 2-shot trÃªn QuALITY (Pang et al., 2022) , vÃ  2-shot cho Qasper (Dasigi et al., 2021). Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m zero-shot, chÃºng tÃ´i thá»­ nghiá»‡m CHUNK LLAMA 2 trÃªn 4 tÃ¡c vá»¥ Ä‘Ã³ng tá»« L-Eval (An et al., 2023): TOFEL, QuALITY (Ä‘Æ°á»£c lÃ m sáº¡ch tá»« Pang et al. (2022)), Coursera, SFiction. ChÃºng tÃ´i cÅ©ng xÃ¡c thá»±c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i trÃªn truy xuáº¥t passkey Ä‘Æ°á»£c sá»­ dá»¥ng trong Mohtashami & Jaggi (2023). ÄÃ¡nh giÃ¡ vá» truy xuáº¥t passkey (Mohtashami & Jaggi, 2023) cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Phá»¥ lá»¥c A.1.

Baseline ChÃºng tÃ´i so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i mÃ£ nguá»“n má»Ÿ phá»• biáº¿n cÃ³ sáºµn trong Huggingface Transformers9. MÃ´ hÃ¬nh CÆ¡ sá»Ÿ : Focused Transformer 3B (Tworkowski et al., 2023), CLEX 7B (Chen et al., 2023a), YaRN 7B/13B (Peng et al., 2023), MPT 30B (MosaicML, 2023b;a), Together 7B (Together, 2023), CodeLlama 7B (RoziÃ¨re et al., 2023),

7https://sharegpt.com/
8https://github.com/DachengLi1/LongChat
9trÆ°á»›c ngÃ y 1 thÃ¡ng 12, 2023

5

--- TRANG 4 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Longlora 13B/70B (Chen et al., 2023c), vÃ  Llama2 Long 7B/13B/70B (Xiong et al., 2023). MÃ´ hÃ¬nh Chat : LongChat-v1.5-32k 7B (Li et al., 2023a), Vicuna-v1.5-16k (LMSYS, 2023) 7B/13B, Longlora-Chat 70B (Chen et al., 2023c), vÃ  Llama2 Long-Chat 70B (Xiong et al., 2023).

4.2. MÃ´ hÃ¬nh hÃ³a NgÃ´n ngá»¯ Chuá»—i DÃ i
Báº£ng 1 trÃ¬nh bÃ y Ä‘iá»ƒm Perplexity (PPL) trÃªn táº­p xÃ¡c thá»±c PG19 cho cÃ¡c mÃ´ hÃ¬nh khÃ´ng cáº§n huáº¥n luyá»‡n vÃ  Ä‘Æ°á»£c tinh chá»‰nh khÃ¡c nhau. Táº¥t cáº£ cÃ¡c baseline nÃ y Ä‘á»u dá»±a trÃªn Llama. ChÃºng tÃ´i chá»©ng minh ráº±ng phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n tá»‘t nháº¥t trÆ°á»›c Ä‘Ã¢y tháº¥t báº¡i vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh 16k. Tuy nhiÃªn, CHUNK LLAMA 2 cÃ³ thá»ƒ ngoáº¡i suy Ä‘áº¿n cá»­a sá»• ngá»¯ cáº£nh hÆ¡n 32k, chá»‰ vá»›i sá»± gia tÄƒng 0,02 trong PPL. ChÃºng tÃ´i tiáº¿p tá»¥c chá»©ng minh ráº±ng CHUNK LLAMA 2 vÆ°á»£t qua káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh trong Ä‘á»™ dÃ i ngá»¯ cáº£nh 16k. ÄÃ¡ng chÃº Ã½, biáº¿n thá»ƒ 70B cá»§a CHUNK LLAMA 2 thá»ƒ hiá»‡n sá»± nháº¥t quÃ¡n trong hiá»‡u suáº¥t trÃªn má»™t loáº¡t Ä‘á»™ dÃ i ngá»¯ cáº£nh, Ä‘áº¡t Ä‘iá»ƒm PPL chá»‰ tÄƒng nháº¹ tá»« 5,18 Ä‘áº¿n 5,59.

ChÃºng tÃ´i cÅ©ng tiáº¿t lá»™ ráº±ng DCA cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n thÃªm trÃªn cÃ¡c ngá»¯ cáº£nh dÃ i hÆ¡n vá»›i PI (Chen et al., 2023b) hoáº·c NTK-Aware RoPE (LocalLLaMA, 2023b;a) vÃ  há»— trá»£ Ä‘á»™ dÃ i ngá»¯ cáº£nh 192k trong Báº£ng 2. Nhá»¯ng káº¿t quáº£ khuyáº¿n khÃ­ch Ä‘Æ°á»£c quan sÃ¡t vá»›i 64k token Ä‘áº§u vÃ o khuyáº¿n khÃ­ch chÃºng tÃ´i thá»­ nghiá»‡m CHUNK LLAMA 2 trÃªn cÃ¡c ngá»¯ cáº£nh tháº­m chÃ­ cÃ²n dÃ i hÆ¡n. ChÃºng tÃ´i tiáº¿n hÃ nh thá»­ nghiá»‡m mÃ´ hÃ¬nh vá»›i Ä‘á»™ dÃ i token Ä‘áº§u vÃ o má»Ÿ rá»™ng tá»« 32k Ä‘áº¿n 192k (Báº£ng 2). Äá»‘i vá»›i Llama2 70B, DCA Ä‘Ã£ chá»©ng minh hiá»‡u quáº£ trong viá»‡c má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh Ä‘áº¿n 96k token. Sá»± má»Ÿ rá»™ng nÃ y Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c chá»‰ vá»›i sá»± gia tÄƒng nhá» 0,56 PPL so vá»›i hiá»‡u suáº¥t gá»‘c cá»§a nÃ³ á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh 4k. CÃ¹ng vá»›i viá»‡c Ä‘Ã¡nh giÃ¡ CHUNK LLAMA 2, chÃºng tÃ´i cÅ©ng Ã¡p dá»¥ng DCA cho cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i hiá»‡n táº¡i sá»­ dá»¥ng cÃ¡c mÃ£ hÃ³a vá»‹ trÃ­ khÃ¡c nhau. TÃ­ch há»£p DCA vá»›i cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i hiá»‡n táº¡i chá»‰ yÃªu cáº§u Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c chunk trong khung DCA. ChÃºng tÃ´i cho tháº¥y ráº±ng CodeLlama vÃ  nhÃ¡nh Llama2 cá»§a Together cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng hiá»‡u quáº£ Ä‘áº¿n Ä‘á»™ dÃ i ngá»¯ cáº£nh 192k sá»­ dá»¥ng DCA vá»›i kÃ­ch thÆ°á»›c chunk 24k. ChÃºng tÃ´i tiáº¿p tá»¥c xÃ¡c thá»±c hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh chÃºng tÃ´i trÃªn tÃ¡c vá»¥ truy xuáº¥t passkey (Mohtashami & Jaggi, 2023). Káº¿t quáº£ cÅ©ng chá»‰ ra ráº±ng báº±ng cÃ¡ch tÃ­ch há»£p DCA vá»›i cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i hiá»‡n táº¡i, há»‡ thá»‘ng Ä‘Æ°á»£c tÄƒng cÆ°á»ng duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t 90% vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh má»Ÿ rá»™ng lÃªn Ä‘áº¿n 192k token (HÃ¬nh 7).

4.3. TÃ¡c vá»¥ Thá»±c táº¿
TrÃ¡i ngÆ°á»£c vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y thÆ°á»ng xÃ¡c thá»±c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng dá»±a trÃªn PPL, chÃºng tÃ´i cÅ©ng Ã¡p dá»¥ng khung framework cá»§a chÃºng tÃ´i cho cáº£ mÃ´ hÃ¬nh cÆ¡ sá»Ÿ vÃ  mÃ´ hÃ¬nh chat Ä‘Æ°á»£c tinh chá»‰nh hÆ°á»›ng dáº«n trÃªn cÃ¡c benchmark thá»±c táº¿.

Káº¿t quáº£ Few-shot ChÃºng tÃ´i xÃ¡c thá»±c DCA trÃªn cÃ¡c mÃ´ hÃ¬nh chÆ°a tráº£i qua tinh chá»‰nh hÆ°á»›ng dáº«n trong thiáº¿t láº­p há»c few-shot.

Báº£ng 1. ÄÃ¡nh giÃ¡ Perplexity (PPL) trÃªn táº­p xÃ¡c thá»±c PG19 (Rae et al., 2020). Káº¿t quáº£ Ä‘Æ°á»£c lÃ m ná»•i báº­t báº±ng mÃ u Ä‘á» chá»‰ ra Perplexity Ä‘Ã£ tÄƒng hÆ¡n 1,0 so vá»›i giÃ¡ trá»‹ gá»‘c cá»§a nÃ³ á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c 4096. ReRoPE (Su, 2023) gáº·p váº¥n Ä‘á» OOM (Háº¿t Bá»™ nhá»›) vá»›i 16k token Ä‘áº§u vÃ o vÃ¬ hiá»‡n táº¡i nÃ³ khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i Flash Attention. CÃ¡c há»‡ sá»‘ chia tá»· lá»‡ trong PI vÃ  NTK Ä‘Æ°á»£c thay Ä‘á»•i Ä‘á»™ng.

MÃ´hÃ¬nhCá»­a sá»• Ngá»¯ cáº£nh ÄÃ¡nh giÃ¡
4096 8192 16384 32768 65536

ÄÆ°á»£c tinh chá»‰nh
Longlora-32k 7B 8.14 7.85 7.70 7.80 91.79
Together-32k 7B 8.21 7.95 7.76 7.64 >102
CodeLlama-16k 7B 8.93 8.64 8.44 8.36 8.65
CLEX-16k 7B 8.84 7.66 7.43 7.57 8.73

KhÃ´ng cáº§n huáº¥n luyá»‡n
Llama2 7B 7.87 >102>102>102>102
Llama2-ReRoPE 7B 7.94 7.75 OOM OOM OOM
Llama2-PI 7B 7.87 9.19 15.11 >102>102
Llama2-PI-Yarn 7B 7.87 8.80 11.75 42.42 >102
Llama2-NTK 7B 7.87 11.98 26.12 58.91 >102
Llama2-NTK-Yarn 7B 7.87 8.06 9.82 11.74 41.57
CHUNK LLAMA 2 7B 7.87 7.67 7.64 7.89 15.87
CHUNK LLAMA 2 13B 7.15 6.95 6.99 7.90 15.14
CHUNK LLAMA 2 70B 5.24 5.18 5.21 5.30 5.59

Llama3
Llama3 8B 9.04 8.71 78.88 >102>102
Llama3 70B 5.36 5.16 >102>102>102
CHUNK LLAMA 3 8B 9.04 8.71 8.61 8.62 8.95
CHUNK LLAMA 3 70B 5.36 5.16 5.14 5.14 5.21

Ä‘á»‹nh. Káº¿t quáº£ Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 3. Thiáº¿t láº­p thÃ­ nghiá»‡m giá»‘ng nhÆ° trong Xiong et al. (2023). Náº¿u cÃ¡c prompt Ä‘áº§u vÃ o vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i Ä‘áº§u vÃ o 16k token, chÃºng Ä‘Æ°á»£c cáº¯t ngáº¯n tá»« phÃ­a bÃªn trÃ¡i. Háº§u háº¿t cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m trong NarrativeQA (KoÄiskÃ½ et al., 2018) vÃ  QMSum (Zhong et al., 2021) cÃ³ Ä‘á»™ dÃ i Ä‘áº§u vÃ o vÆ°á»£t quÃ¡ 16k token, trong khi Ä‘á»™ dÃ i cá»§a cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m trong Qasper (Dasigi et al., 2021) vÃ  QuALITY (Pang et al., 2022) thÆ°á»ng dÆ°á»›i 8k token. KhÃ´ng cÃ³ báº¥t ká»³ chi phÃ­ huáº¥n luyá»‡n nÃ o, cáº£ biáº¿n thá»ƒ 7B/13B cá»§a CHUNK LLAMA 2 Ä‘á»u Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ cÃ³ thá»ƒ so sÃ¡nh vá»›i cÃ¡c baseline Ä‘Æ°á»£c tinh chá»‰nh phá»• biáº¿n nhÆ° YaRN (Peng et al., 2023), MPT (MosaicML, 2023b), Together (Together, 2023), dá»±a trÃªn RoPE Ä‘Æ°á»£c má»Ÿ rá»™ng trÆ°á»›c Ä‘Ã¢y (Chen et al., 2023b; LocalLLaMA, 2023b) hoáº·c Alibi (Press et al., 2022).

KhÃ¡c vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y thÆ°á»ng xÃ¡c minh ká»¹ thuáº­t cá»§a chÃºng trÃªn cÃ¡c phiÃªn báº£n nhá» hÆ¡n cá»§a Llama2, chÃºng tÃ´i cÅ©ng trÃ¬nh bÃ y káº¿t quáº£ cho DCA Ä‘Æ°á»£c ghÃ©p ná»‘i vá»›i Llama2 70B, trong Ä‘Ã³ DCA cáº£i thiá»‡n hiá»‡u suáº¥t trung bÃ¬nh hÆ¡n 8,0 Ä‘iá»ƒm so vá»›i mÃ´ hÃ¬nh Llama2 gá»‘c vá»›i Ä‘á»™ dÃ i huáº¥n luyá»‡n 4k. Do chi phÃ­ ngÃ y cÃ ng tÄƒng cá»§a viá»‡c tinh chá»‰nh ngá»¯ cáº£nh dÃ i cho cÃ¡c mÃ´ hÃ¬nh 70B, chÃºng tÃ´i khÃ´ng tÃ¬m tháº¥y nhiá»u baseline 70B mÃ£ nguá»“n má»Ÿ. ChÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n cá»§a chÃºng tÃ´i vá»›i baseline 70B máº¡nh máº½, Longlora (Chen et al., 2023c), sá»­ dá»¥ng tinh chá»‰nh hiá»‡u quáº£ dá»±a trÃªn LoRA (Hu et al., 2021) dá»±a trÃªn táº­p dá»¯ liá»‡u Redpajama (Computer, 2023) cho 1000 bÆ°á»›c há»— trá»£ cá»­a sá»• ngá»¯ cáº£nh 32k. Káº¿t quáº£ chá»©ng minh ráº±ng

6

--- TRANG 5 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Báº£ng 2. ÄÃ¡nh giÃ¡ Perplexity trÃªn táº­p xÃ¡c thá»±c PG19 (Rae et al., 2020) vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh lÃªn Ä‘áº¿n 192k token. ChÃºng tÃ´i thá»­ nghiá»‡m DCA trÃªn Llama2 70B cÃ¹ng vá»›i 2 mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c thÃªm phá»• biáº¿n sá»­ dá»¥ng PI vÃ  NTK. Káº¿t quáº£ Ä‘Æ°á»£c lÃ m ná»•i báº­t báº±ng mÃ u Ä‘á» chá»‰ ra PPL Ä‘Ã£ tÄƒng hÆ¡n 1,0 so vá»›i giÃ¡ trá»‹ gá»‘c cá»§a nÃ³ á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c 4096.

MÃ´hÃ¬nhMÃ£ hÃ³a Ngá»¯ cáº£nhCá»­a sá»• Ngá»¯ cáº£nh ÄÃ¡nh giÃ¡
Vá»‹ trÃ­Huáº¥n luyá»‡n4k 32k 64k 96k 128k 160k 192k

Llama2 7B RoPE 4k 7.87 >102>102>102>102>102>102
CHUNK LLAMA 2 7B RoPE 4k 7.87 7.89 15.87 43.57 96.21 >102>102
Llama2 70B RoPE 4k 5.24 >102>102>102>102>102>102
CHUNK LLAMA 2 70B RoPE 4k 5.24 5.30 5.59 5.80 6.12 6.52 7.05
Llama3 8B RoPE 8k 9.04 >102>102>102>102>102>102
CHUNK LLAMA 3 8B RoPE 8k 9.04 8.61 8.62 8.95 9.43 10.04 10.66
Llama3 70B RoPE 8k 5.36 >102>102>102>102>102>102
CHUNK LLAMA 3 70B RoPE 8k 5.36 5.14 5.14 5.21 5.32 5.40 5.45
CodeLlama 7B NTK 16k 8.93 8.36 8.65 9.14 9.87 15.68 24.78
ChunkCodeLlama 7B NTK 16k 8.93 8.36 8.13 8.33 8.66 9.30 9.83
Together 7B PI 32k 8.21 7.64 >102>102>102>102>102
ChunkTogether 7B PI 32k 8.21 7.64 7.59 7.64 7.67 7.74 7.83

Báº£ng 3. So sÃ¡nh giá»¯a cÃ¡c mÃ´ hÃ¬nh cÆ¡ sá»Ÿ mÃ£ nguá»“n má»Ÿ phá»• biáº¿n (khá»‘i Ä‘áº§u tiÃªn) vÃ  mÃ´ hÃ¬nh Ä‘á»™c quyá»n (khá»‘i cuá»‘i cÃ¹ng) trÃªn bá»‘n benchmark nghiÃªn cá»©u trÃªn táº­p xÃ¡c thá»±c cá»§a chÃºng. ChÃºng tÃ´i gáº¡ch dÆ°á»›i káº¿t quáº£ tá»‘t nháº¥t trong má»—i khá»‘i. Káº¿t quáº£ vÆ°á»£t qua mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh mÃ£ nguá»“n má»Ÿ tá»‘t nháº¥t trÆ°á»›c Ä‘Ã¢y Ä‘Æ°á»£c in Ä‘áº­m. Llama2 Long Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i tá»•ng cá»™ng 400B token qua 100,000 bÆ°á»›c. Äá»™ dÃ i prompt tá»‘i Ä‘a Ä‘Æ°á»£c phÃ©p Ä‘Æ°á»£c Ä‘áº·t thÃ nh 16,384 token.â€ : káº¿t quáº£ Ä‘Æ°á»£c láº¥y tá»« Xiong et al. (2023) ChÃºng tÃ´i sá»­ dá»¥ng prompt Ä‘Æ¡n giáº£n nháº¥t: long-document Question:... Answer: . CÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn tá»« táº­p huáº¥n luyá»‡n, vÃ  chÃºng tÃ´i cÅ©ng cÃ³ má»™t tháº£o luáº­n vá» viá»‡c lá»±a chá»n cÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh trong Phá»¥ lá»¥c Â§A.4.

MÃ´hÃ¬nhHuáº¥n luyá»‡n NarrativeQA Qasper QuALITY QMSumTrung
ThÃªmF1 (0-shot) F1 (2-shot) EM (2-shot) R-g (1-shot)bÃ¬nh
ngá»¯ cáº£nhhuáº¥n luyá»‡n

FoT 3Bâ€ âœ“ 8k 16.3 15.4 20.5 10.6 15.7
Yarn 7Bâ€ âœ“ 128k 20.9 26.2 32.3 11.4 22.7
Together 7Bâ€ âœ“ 32k 23.3 27.3 41.2 12.6 26.1
Yarn 13Bâ€ âœ“ 128k 23.4 27.1 46.4 11.9 27.2
Longlora 13B âœ“ 32k 25.8 26.4 48.9 15.1 29.1
MPT 30Bâ€ âœ“ 8k 22.9 29.0 41.5 10.3 25.9
Llama2-DynNTK 70B âœ— 4k 11.1 27.8 60.9 7.8 26.9
Llama2 70Bâ€ âœ— 4k 25.7 27.5 53.0 11.9 29.5
Longlora 70B âœ“ 32k 34.2 29.0 69.9 15.6 37.2
CHUNK LLAMA 2 7B âœ— 4k 20.0 28.2 35.6 14.7 24.6
CHUNK LLAMA 2 13B âœ— 4k 26.3 29.3 47.9 15.2 29.7
CHUNK LLAMA 2 70B âœ— 4k 32.5 29.6 73.2 16.0 37.8
CHUNK LLAMA 3 8B âœ— 8k 27.4 30.5 52.6 15.4 31.5
CHUNK LLAMA 3 70B âœ— 8k 33.7 33.1 75.4 16.0 39.5

mÃ´ hÃ¬nh Ä‘á»™c quyá»n
Llama2 Long 7Bâ€ âœ“ 32k 21.9 27.8 43.2 14.9 27.0
Llama2 Long 13Bâ€ âœ“ 32k 25.6 31.2 57.6 15.7 32.5
Llama2 Long 70Bâ€ âœ“ 16k 30.9 35.7 79.7 16.5 40.7

mÃ´ hÃ¬nh DCA 70B Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cÃ³ thá»ƒ so sÃ¡nh (37,8 so vá»›i 37,2) khÃ´ng yÃªu cáº§u bÆ°á»›c huáº¥n luyá»‡n nÃ o.

So sÃ¡nh vá»›i baseline Ä‘á»™c quyá»n máº¡nh máº½, Llama2 Long (Xiong et al., 2023), Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i tá»•ng cá»™ng 400 tá»· token (corpus huáº¥n luyá»‡n trÆ°á»›c Llama2 vÃ  dá»¯ liá»‡u vÄƒn báº£n dÃ i má»›i) qua 100,000 bÆ°á»›c, khoáº£ng cÃ¡ch hiá»‡u suáº¥t cho táº¥t cáº£ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh thÆ°á»ng trong pháº¡m vi 3 Ä‘iá»ƒm. CÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh Ä‘Æ°á»£c sá»­ dá»¥ng trong thÃ­ nghiá»‡m nÃ y Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn tá»« táº­p huáº¥n luyá»‡n. ChÃºng tÃ´i cÅ©ng Ä‘Ã£ thá»­ cÃ¡c cÃ¡ch khÃ¡c Ä‘á»ƒ chá»n vÃ­ dá»¥, vÃ  chi tiáº¿t Ä‘Æ°á»£c bao gá»“m trong Phá»¥ lá»¥c A.4.

Káº¿t quáº£ Zero-shot NgoÃ i viá»‡c xÃ¡c minh DCA trÃªn mÃ´ hÃ¬nh cÆ¡ sá»Ÿ, chÃºng tÃ´i cÅ©ng Ã¡p dá»¥ng DCA trÃªn phiÃªn báº£n chat cá»§a Llama2 (vá»›i tinh chá»‰nh hÆ°á»›ng dáº«n) trong ká»‹ch báº£n há»c zero-shot. Cá»¥ thá»ƒ, chÃºng tÃ´i thá»­ nghiá»‡m mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i trÃªn bá»‘n tÃ¡c vá»¥ Ä‘Ã³ng tá»« L-Eval (An et al., 2023) vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o Ä‘a dáº¡ng dao Ä‘á»™ng tá»« 3k Ä‘áº¿n 27k. Táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u nÃ y sá»­ dá»¥ng Exact Match (EM) lÃ m thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡. NhÃ¬n chung, cÃ¡c káº¿t luáº­n tÆ°Æ¡ng tá»± nhÆ° Ä‘Ã¡nh giÃ¡ few-shot. CÃ¡c mÃ´ hÃ¬nh khÃ´ng cáº§n huáº¥n luyá»‡n 7B/13B cá»§a chÃºng tÃ´i cho tháº¥y hiá»‡u suáº¥t cÃ³ thá»ƒ so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ cÃ³ huáº¥n luyá»‡n thÃªm. ÄÃ¡ng chÃº Ã½, trong cÃ¡c thÃ­ nghiá»‡m zero-shot, chÃºng tÃ´i chá»©ng minh sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i phiÃªn báº£n Chat cá»§a Longlora 70B (Chen et al., 2023c). HÆ¡n ná»¯a, khi so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n nhÆ° GPT-3.5 vá»›i ngá»¯ cáº£nh 16k token vÃ  chat

7

--- TRANG 6 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Báº£ng 4. So sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh chat mÃ£ nguá»“n má»Ÿ (khá»‘i Ä‘áº§u tiÃªn) vÃ  mÃ´ hÃ¬nh Ä‘á»™c quyá»n (khá»‘i cuá»‘i cÃ¹ng) trÃªn 4 tÃ¡c vá»¥ Ä‘Ã³ng vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o khÃ¡c nhau tá»« L-Eval (An et al., 2023). ChÃºng tÃ´i gáº¡ch dÆ°á»›i káº¿t quáº£ tá»‘t nháº¥t trong má»—i khá»‘i. Káº¿t quáº£ vÆ°á»£t qua mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh mÃ£ nguá»“n má»Ÿ tá»‘t nháº¥t trÆ°á»›c Ä‘Ã¢y Ä‘Æ°á»£c in Ä‘áº­m . 'dialogues' cÃ³ nghÄ©a lÃ  há»—n há»£p ShareGPT vÃ  AlpacaGPT4 Ä‘Æ°á»£c sá»­ dá»¥ng trong huáº¥n luyá»‡n cá»§a chÃºng tÃ´i. Llama2-PI-SFT vÃ  Llama2-NTK-SFT lÃ  cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ¹ng dá»¯ liá»‡u vÃ  bÆ°á»›c huáº¥n luyá»‡n vá»›i CHUNK LLAMA 2.â€¡: káº¿t quáº£ Ä‘Æ°á»£c láº¥y tá»« Xiong et al. (2023).

MÃ´hÃ¬nhCorpus Ngá»¯ cáº£nhTOFEL QuALITY Coursera SFictionTrung
Tinh chá»‰nhHuáº¥n luyá»‡n(3kâˆ¼5k) (4k âˆ¼9k) (5k âˆ¼17k) (6k âˆ¼27k)bÃ¬nh

Llama2-Chat 7B âœ— 4k 51.67 37.62 29.21 60.15 48.74
Llama2-DynNTK 7B âœ— 4k 52.27 30.69 13.95 57.02 38.48
Longchat-v1.5-32k 7B ShareGPT 32k 39.77 37.62 32.99 57.02 41.85
Llama2-PI-SFT 7B Dialogues 16k 56.13 38.61 36.19 53.90 46.20
Llama2-NTK-SFT 7B Dialogues 16k 53.90 38.11 34.01 64.06 47.51
Vicuna-v1.5-16k 7B ShareGPT 16k 55.39 39.60 38.66 60.15 48.45
Llama2-Chat 13B âœ— 4k 60.96 42.57 35.75 54.68 48.99
Llama2-DynNTK 13B âœ— 4k 62.45 33.16 37.06 60.93 48.40
Vicuna-v1.5-16k 13B ShareGPT 16k 68.40 53.96 40.69 61.71 56.19
Longlora-Chat 70B LongAlpaca 32k 71.37 55.45 44.76 67.96 59.88

KhÃ´ng cáº§n huáº¥n luyá»‡n
CHUNK LLAMA 2-Chat 7B âœ— 4k 57.62 35.14 32.12 61.72 46.64
CHUNK LLAMA 2-Chat 13B âœ— 4k 66.54 43.06 41.56 57.03 52.04
CHUNK LLAMA 2-Chat 70B âœ— 4k 82.15 60.39 48.54 61.72 63.20

Llama3
CHUNK LLAMA 3-Instruct 8B âœ— 8k 83.27 63.86 56.24 70.31 68.42
CHUNK LLAMA 3-Instruct 70B âœ— 8k 84.75 82.17 76.88 75.78 79.89

ÄÆ°á»£c tinh chá»‰nh
CHUNK LLAMA 2-Chat 7B Dialogues 16k 62.08 41.58 39.68 64.06 51.85
CHUNK LLAMA 2-Chat 13B Dialogues 16k 65.42 53.96 44.76 65.62 57.94

mÃ´ hÃ¬nh Ä‘á»™c quyá»n
GPT3.5-16k-0613 KhÃ´ng biáº¿t â€“ 78.43 61.38 63.51 64.84 67.03
Claude1.3-100k KhÃ´ng biáº¿t â€“ 83.64 60.03 73.76 72.65 72.52
Llama2 Long-Chat 70Bâ€¡Long doc+diag 16k 81.8 â€“ 52.9 â€“ â€“

phiÃªn báº£n cá»§a Llama2 Long, káº¿t quáº£ cho tháº¥y ráº±ng mÃ´ hÃ¬nh chat Llama2 70B cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng trá»±c tiáº¿p Ä‘áº¿n cá»­a sá»• ngá»¯ cáº£nh 16k mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n bá»• sung vá»›i DCA, Ä‘áº¡t 94% hiá»‡u suáº¥t cá»§a gpt-3.5-turbo-16k . ChÃºng tÃ´i cÅ©ng chá»©ng minh ráº±ng hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÄƒng cÆ°á»ng thÃ´ng qua tinh chá»‰nh bá»• sung trÃªn dá»¯ liá»‡u Ä‘á»‘i thoáº¡i dÃ i theo phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi Vicuna (LMSYS, 2023) vÃ  Longchat (Li et al., 2023a), cáº£ hai Ä‘á»u lÃ  baseline Ä‘Æ°á»£c tinh chá»‰nh phá»• biáº¿n sá»­ dá»¥ng ShareGPT. Vá»›i huáº¥n luyá»‡n thÃªm,CHUNK LLAMA 2-Chat vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh 13B tá»‘t nháº¥t trÆ°á»›c Ä‘Ã¢y, Vicuna-v1.5-13b-16k, vá»›i biÃªn Ä‘á»™ Ä‘Ã¡ng ká»ƒ 1,75 Ä‘iá»ƒm.

4.4. PhÃ¢n tÃ­ch
Hiá»‡u quáº£ Trong hÃ¬nh 3, thá»i gian suy luáº­n vÃ  bá»™ nhá»› GPU cá»§a (a) cÆ¡ cháº¿ self-attention gá»‘c nhÆ° Ä‘Æ°á»£c triá»ƒn khai trong PyTorch, Flash Attention (Dao, 2023), vÃ  DCA Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i (Ä‘Æ°á»£c tÃ­ch há»£p vá»›i Flash Attention) Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c Ä‘á»™ dÃ i prompt khÃ¡c nhau. Nhá»¯ng thÃ­ nghiá»‡m nÃ y Ä‘Æ°á»£c cháº¡y trÃªn má»™t GPU NVIDIA 80G A100 sá»­ dá»¥ng Llama2 7B. Prompt dÃ i Ä‘áº§u vÃ o lÃ  tá»« NarrativeQA (KoÄiskÃ½ et al., 2018). ChÃºng tÃ´i tiáº¿n hÃ nh 20 thá»­ nghiá»‡m vÃ  bÃ¡o cÃ¡o hiá»‡u suáº¥t trung bÃ¬nh. KhÃ´ng cÃ³ Flash Attention, chÃºng tÃ´i quan sÃ¡t ráº±ng Ä‘á»™ dÃ i Ä‘áº§u vÃ o tá»‘i Ä‘a cÃ³ thá»ƒ quáº£n lÃ½ bá»Ÿi má»™t GPU duy nháº¥t lÃ  khoáº£ng tá»« 12k Ä‘áº¿n 16k token. DCA duy trÃ¬ viá»‡c tiÃªu thá»¥ bá»™ nhá»› GPU vÃ  tá»‘c Ä‘á»™ suy luáº­n tÆ°Æ¡ng tá»±, mÃ  khÃ´ng thÃªm chi phÃ­ Ä‘Ã¡ng ká»ƒ, vá»›i Flash attention gá»‘c.

HÃ¬nh 3. Thá»i gian suy luáº­n vÃ  bá»™ nhá»› GPU cá»§a (a) self-attention gá»‘c Ä‘Æ°á»£c triá»ƒn khai bá»Ÿi Pytorch, (b) Flash Attention (Dao, 2023), vÃ  (c) DCA (cÃ´ng viá»‡c nÃ y).

NghiÃªn cá»©u PhÃ¢n tÃ­ch Äá»ƒ xÃ¡c thá»±c ba cÆ¡ cháº¿ attention Ä‘Æ°á»£c Ä‘á» xuáº¥t trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y má»™t nghiÃªn cá»©u phÃ¢n tÃ­ch cho DCA trong HÃ¬nh 4, táº­p trung vÃ o cÃ¡c tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ vÃ  truy xuáº¥t passkey. ChÃºng tÃ´i xem xÃ©t ba Ä‘iá»u kiá»‡n thÃ­ nghiá»‡m: (1) Chá»‰ sá»­ dá»¥ng intra-chunk attention. (2) Sá»­ dá»¥ng cáº£ intra-chunk vÃ  inter-chunk attention. (3) Káº¿t há»£p táº¥t cáº£ ba loáº¡i attention: intra-chunk, inter-chunk, vÃ  successive chunk attention. Tá»« káº¿t quáº£ trong mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯, chÃºng tÃ´i quan sÃ¡t ráº±ng sá»­ dá»¥ng intra-chunk attention mÃ  bá» qua thÃ´ng tin tá»« cÃ¡c chunk trÆ°á»›c Ä‘Ã³, cÃ³ thá»ƒ duy trÃ¬ PPL ráº¥t tháº¥p nhÆ°ng cáº£n trá»Ÿ kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ truy xuáº¥t passkey tá»« cÃ¡c chunk khÃ¡c. Giá»›i thiá»‡u inter-chunk attention, chÃºng tÃ´i nháº­n tháº¥y sá»± cáº£i thiá»‡n trong hiá»‡u suáº¥t truy xuáº¥t passkey á»Ÿ Ä‘á»™ dÃ i Ä‘áº§u vÃ o 12k. Tuy nhiÃªn, viá»‡c máº¥t tÃ­nh cá»¥c bá»™ gÃ¢y ra sá»± gia tÄƒng Ä‘Ã¡ng ká»ƒ trong PPL cá»§a mÃ´ hÃ¬nh. Báº±ng cÃ¡ch tÃ­ch há»£p successive chunk attention, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c cáº£ PPL tháº¥p vÃ  Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t cao.

3RNkRkk33Rej39jkde310203040
*QMi2ti qBM/QrS2`TH2tBivBMi`BMi`YBMi2`BMi`YBMi2`Ybm++
3RNkRkk33Rej39jkde300.51
*QMi2ti qBM/QrSbbF2v _2i`B2pH ++m``+vBMi`BMi`YBMi2`BMi`YBMi2`Ybm++

HÃ¬nh 4. NghiÃªn cá»©u phÃ¢n tÃ­ch cá»§a DCA vá» mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ (bÃªn trÃ¡i) vÃ  truy xuáº¥t passkey (bÃªn pháº£i). ChÃºng tÃ´i thá»­ nghiá»‡m ba cÆ¡ cháº¿ attention vá»›i cÃ¡c chuá»—i Ä‘áº§u vÃ o tá»« 8k Ä‘áº¿n 32k.

5. Káº¿t luáº­n
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y Dual Chunk Attention (DCA) nhÆ° má»™t phÆ°Æ¡ng phÃ¡p má»›i vÃ  hiá»‡u quáº£ Ä‘á»ƒ vÆ°á»£t qua cÃ¡c háº¡n cháº¿ Ä‘á»™ dÃ i ngá»¯ cáº£nh vá»‘n cÃ³ trong LLMs. Báº±ng cÃ¡ch khÃ©o lÃ©o táº­n dá»¥ng cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ hiá»‡n cÃ³ cá»§a mÃ´ hÃ¬nh vÃ  giá»›i thiá»‡u má»™t cÆ¡ cháº¿ attention Ä‘a diá»‡n, DCA cho phÃ©p ngoáº¡i suy hÆ¡n 8 láº§n Ä‘á»™ dÃ i huáº¥n luyá»‡n mÃ  khÃ´ng cáº§n Ä‘áº¿n huáº¥n luyá»‡n thÃªm tá»‘n kÃ©m vÃ  máº¥t thá»i gian.

TuyÃªn bá»‘ TÃ¡c Ä‘á»™ng
Nhiá»u nghiÃªn cá»©u Ä‘Ã£ xuáº¥t hiá»‡n nháº¯m vÃ o viá»‡c má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh Ä‘Æ°á»£c há»— trá»£ cá»§a LLMs; tuy nhiÃªn, do chi phÃ­ huáº¥n luyá»‡n cao vÃ  khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c cÃ´ng nghá»‡ nhÆ° Flash Attention, ngÃ nh cÃ´ng nghiá»‡p chá»§ yáº¿u dá»±a vÃ o viá»‡c má»Ÿ rá»™ng táº§n sá»‘ cÆ¡ sá»Ÿ cá»§a RoPE hoáº·c PI. PhÆ°Æ¡ng phÃ¡p Dual Chunk Attention (DCA) cá»§a chÃºng tÃ´i tÆ°Æ¡ng thÃ­ch vá»›i Flash Attention vÃ  chá»‰ yÃªu cáº§u sá»­a Ä‘á»•i mÃ£ suy luáº­n, loáº¡i bá» nhu cáº§u huáº¥n luyá»‡n láº¡i rá»™ng rÃ£i. DCA báº£o tá»“n hiá»‡u suáº¥t mÃ´ hÃ¬nh trong Ä‘á»™ dÃ i huáº¥n luyá»‡n, vÃ  chá»‰ cÃ³ lá»£i cho nÃ³ vÆ°á»£t quÃ¡ pháº¡m vi nÃ y, cung cáº¥p kháº£ nÄƒng tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ tráº£i qua tinh chá»‰nh ngá»¯ cáº£nh dÃ i. Do Ä‘Ã³, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n ngÃ nh cÃ´ng nghiá»‡p, cung cáº¥p má»™t giáº£i phÃ¡p hiá»‡u quáº£ vá» chi phÃ­ Ä‘á»ƒ quáº£n lÃ½ cÃ¡c tÃ¬nh huá»‘ng ngá»¯ cáº£nh dÃ i trong cÃ¡c á»©ng dá»¥ng LLM.

CÃ³ nhiá»u há»‡ quáº£ xÃ£ há»™i tiá»m nÄƒng cá»§a cÃ´ng viá»‡c chÃºng tÃ´i, khÃ´ng cÃ³ cÃ¡i nÃ o chÃºng tÃ´i cáº£m tháº¥y pháº£i Ä‘Æ°á»£c nÃªu báº­t cá»¥ thá»ƒ á»Ÿ Ä‘Ã¢y.

Lá»i cáº£m Æ¡n
ChÃºng tÃ´i cáº£m Æ¡n Yukang Chen vÃ  Hang Yan vÃ¬ nhá»¯ng bÃ¬nh luáº­n há»¯u Ã­ch vÃ  mÃ£ nguá»“n má»Ÿ cá»§a há». NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi chÆ°Æ¡ng trÃ¬nh nghiÃªn cá»©u chung cá»§a Quá»¹ Khoa há»c Tá»± nhiÃªn Quá»‘c gia Trung Quá»‘c (NSFC) vÃ  Há»™i Ä‘á»“ng NghiÃªn cá»©u (RGC) dÆ°á»›i sá»‘ hiá»‡u N HKU714/21.

TÃ i liá»‡u tham kháº£o
An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088 , 2023.

Anthropic. Introducing 100K Context Windows, 2023. URL https://www.anthropic.com/index/ 100k-context-windows .

Bai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan, Y ., Ge, W., Han, Y ., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y ., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y ., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical report, 2023.

Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L. Clex: Continuous length extrapolation for large language models, 2023a.

Chen, S., Wong, S., Chen, L., and Tian, Y . Extending context window of large language models via positional interpolation, 2023b.

Chen, Y ., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307 , 2023c.

Chi, T.-C., Fan, T.-H., Rudnicky, A. I., and Ramadge, P. J. Dissecting transformer length extrapolation via the lens of receptive field analysis, 2023.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.

Chowdhury, J. R. and Caragea, C. Monotonic location attention for length generalization, 2023.

Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/ togethercomputer/RedPajama-Data .

Dao, T. Flashattention-2: Faster attention with better paral- lelism and work partitioning, 2023.

Dao, T., Fu, D. Y ., Ermon, S., Rudra, A., and R Â´e, C. Flashat- tention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS , 2022.

Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., and Gardner, M. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter

9

--- TRANG 7 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

of the Association for Computational Linguistics: Hu- man Language Technologies , pp. 4599â€“4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https: //aclanthology.org/2021.naacl-main.365 .

Han, C., Wang, Q., Xiong, W., Chen, Y ., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models, 2023.

He, Z., Feng, G., Luo, S., Yang, K., He, D., Xu, J., Zhang, Z., Yang, H., and Wang, L. Two stones hit one bird: Bilevel positional encoding for better length extrapolation, 2024.

Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021.

Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y ., Chen, H., and Hu, X. Llm maybe longlm: Self-extend llm context window without tuning, 2024.

Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers, 2023.

KoË‡ciskÂ´y, T., Schwarz, J., Blunsom, P., Dyer, C., Her- mann, K. M., Melis, G., and Grefenstette, E. The Nar- rativeQA reading comprehension challenge. Transac- tions of the Association for Computational Linguistics , 6:317â€“328, 2018. doi: 10.1162/tacl a00023. URL https://aclanthology.org/Q18-1023 .

Lee, G., Hartmann, V ., Park, J., Papailiopoulos, D., and Lee, K. Prompted llms as chatbot modules for long open- domain conversation. In Findings of the Association for Computational Linguistics: ACL 2023 . Association for Computational Linguistics, 2023. doi: 10.18653/v1/ 2023.findings-acl.277. URL http://dx.doi.org/10. 18653/v1/2023.findings-acl.277 .

Li, D., Shao, R., Xie, A., Sheng, Y ., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length. 2023a.

Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., Sanghai, S., Yang, Y ., Kumar, S., and Bho- janapalli, S. Functional interpolation for relative positions improves long context transformers, 2023b.

Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts, 2023a.

Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D. Scaling laws of rope-based extrapolation, 2023b.

LMSYS. Vicuna: An open-source chatbot impress- ing gpt-4 with 90 URL https://lmsys.org/blog/ 2023-03-30-vicuna/ .

LocalLLaMA. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023a. URL https://www.reddit.com/r/ LocalLLaMA/comments/14mrgpr/dynamically_ scaled_rope_further_increases/ .

LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., June 2023b. URL https://www.reddit.com/ r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/ .

Lv, K., Liu, X., Guo, Q., Yan, H., He, C., Qiu, X., and Lin, D. Longwanjuan: Towards systematic measurement for long text quality, 2024.

Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300 , 2023.

MosaicML. Introducing mpt-30b: Raising the bar for open- source foundation models, 2023a. URL www.mosaicml. com/blog/mpt-30b . Accessed: 2023-06-22.

MosaicML. Introducing mpt-7b: A new standard for open- source, ly usable llms, 2023b. URL www.mosaicml. com/blog/mpt-7b .

OpenAI. Gpt-4 technical report, 2023.

Pang, R. Y ., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V ., Ma, J., Thompson, J., He, H., and Bowman, S. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies , pp. 5336â€“5358, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL https: //aclanthology.org/2022.naacl-main.391 .

Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models, 2023.

Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation, 2022.

Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y . Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. ArXiv , abs/2401.04658, 2024. URL https://api. semanticscholar.org/CorpusID:266900042 .

10

--- TRANG 8 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for long- range sequence modelling. In 8th International Confer- ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id= SylKikSYDH .

Ratner, N., Levine, Y ., Belinkov, Y ., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y . Parallel context windows for large lan- guage models, 2023.

Robertson, S., Zaragoza, H., et al. The probabilistic rele- vance framework: Bm25 and beyond. Foundations and TrendsÂ® in Information Retrieval , 3(4):333â€“389, 2009.

Rozi `ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., D Â´efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2023.

Rula, A. and D'Souza, J. Procedural text mining with large language models, 2023.

Ruoss, A., Del Â´etang, G., Genewein, T., Grau-Moya, J., Csord Â´as, R., Bennani, M., Legg, S., and Veness, J. Ran- domized positional encodings boost length generalization of transformers, 2023.

Saad-Falcon, J., Barrow, J., Siu, A., Nenkova, A., Yoon, D. S., Rossi, R. A., and Dernoncourt, F. Pdftriage: Ques- tion answering over long, structured documents, 2023.

Song, K., Wang, X., Cho, S., Pan, X., and Yu, D. Zebra: Extending context window with layerwise grouped local- global attention, 2023.

Su, J. Rectified rotary position embeddings. https:// github.com/bojone/rerope , 2023.

Su, J., Lu, Y ., Pan, S., Murtadha, A., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2022.

Sun, Y ., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V ., Song, X., and Wei, F. A length- extrapolatable transformer, 2022.

Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford al- paca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca , 2023.

Together. Llama-2-7b-32k-instruct â€” and fine-tuning for llama-2 models with together api, 2023. URL https:// together.ai/blog/llama-2-7b-32k-instruct .

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam- ple, G. Llama: Open and efficient foundation language models, 2023a.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.

Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y ., Michalewski, H., and MiÅ‚o Â´s, P. Focused transformer: Contrastive training for context scaling, 2023.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017.

Wang, L., Yang, N., and Wei, F. Learning to retrieve in- context examples for large language models, 2024.

Wei, J., Kim, S., Jung, H., and Kim, Y .-H. Leveraging large language models to power chatbots for collecting user self-reported data, 2023.

Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y ., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory, 2024.

Xiao, G., Tian, Y ., Chen, B., Han, S., and Lewis, M. Ef- ficient streaming language models with attention sinks, 2023.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y ., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models. CoRR , abs/2309.16039, 2023. doi: 10.48550/ARXIV .2309.16039. URL https://doi. org/10.48550/arXiv.2309.16039 .

Ye, J., Wu, Z., Feng, J., Yu, T., and Kong, L. Composi- tional exemplars for in-context learning. arXiv preprint arXiv:2302.05698 , 2023.

Zhang, J., Jiang, S., Feng, J., Zheng, L., and Kong, L. Linear attention via orthogonal memory. ArXiv , abs/2312.11135, 2023. URL https://api.semanticscholar.org/ CorpusID:266359128 .

11

--- TRANG 9 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Soaring from 4k to 400k: Extending llm's con- text with activation beacon. ArXiv , abs/2401.03462, 2024. URL https://api.semanticscholar.org/ CorpusID:266844488 .

Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A. H., Celikyilmaz, A., Liu, Y ., Qiu, X., and Radev, D. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies , pp. 5905â€“5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https: //aclanthology.org/2021.naacl-main.472 .

Zhu, D., Yang, N., Wang, L., Song, Y ., Wu, W., Wei, F., and Li, S. Pose: Efficient context window extension of llms via positional skip-wise training, 2023.

12

--- TRANG 10 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

A. Phá»¥ lá»¥c
A.1. Truy xuáº¥t passkey
NgoÃ i cÃ¡c tÃ¡c vá»¥ thá»±c táº¿, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ngá»¯ cáº£nh dÃ i cá»§a LLMs Ä‘á»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ truy xuáº¥t passkey nhÆ° Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Mohtashami & Jaggi (2023). TÃ¡c vá»¥ nÃ y thÃ¡ch thá»©c má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘á»ƒ Ä‘á»‹nh vá»‹ má»™t passkey Ä‘Æ¡n giáº£n (vÃ­ dá»¥: má»™t sá»‘ ngáº«u nhiÃªn nÄƒm chá»¯ sá»‘) Ä‘Æ°á»£c nhÃºng trong má»™t chuá»—i vÄƒn báº£n dÃ i vÃ  vÃ´ nghÄ©a khÃ¡c. Má»¥c Ä‘Ã­ch chÃ­nh cá»§a tÃ¡c vá»¥ nÃ y lÃ  xÃ¡c Ä‘á»‹nh liá»‡u má»™t MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) cÃ³ thá»ƒ duy trÃ¬ nháº­n thá»©c vá» thÃ´ng tin Ä‘Æ°á»£c phÃ¢n bá»‘ trong suá»‘t má»™t chuá»—i Ä‘áº§u vÃ o dÃ i. Äá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t, chÃºng tÃ´i Ä‘áº·t ngáº«u nhiÃªn passkey á»Ÿ cÃ¡c Ä‘á»™ sÃ¢u tÃ i liá»‡u khÃ¡c nhau Ä‘Æ°á»£c phÃ¢n bá»‘ Ä‘á»u. Äá»‘i vá»›i má»—i Ä‘á»™ sÃ¢u tÃ i liá»‡u, chÃºng tÃ´i cháº¡y 20 láº§n vá»›i cÃ¡c passkey khÃ¡c nhau vÃ  chÃºng tÃ´i thá»­ nghiá»‡m Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o tá»« 4k Ä‘áº¿n 20k. ChÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t cá»§a DCA vá»›i 2 phÆ°Æ¡ng phÃ¡p má»Ÿ rá»™ng phá»• biáº¿n: PI (Chen et al., 2023b), NTK-Aware (LocalLLaMA, 2023b;a), trÃªn mÃ´ hÃ¬nh Llama2 13B vá»›i cá»­a sá»• ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c 4k. Káº¿t quáº£ hiá»‡u suáº¥t Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 5. ÄÃ¡ng chÃº Ã½, trong Ä‘á»™ dÃ i ngá»¯ cáº£nh 18k token, mÃ´ hÃ¬nh CHUNK LLAMA 2 cá»§a chÃºng tÃ´i liÃªn tá»¥c Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t passkey 100% trÃªn táº¥t cáº£ cÃ¡c Ä‘á»™ sÃ¢u Ä‘Æ°á»£c thá»­ nghiá»‡m.

ChÃºng tÃ´i má»Ÿ rá»™ng pháº¡m vi cá»§a cÃ¡c tÃ¡c vá»¥ truy xuáº¥t passkey báº±ng cÃ¡ch tÄƒng dáº§n sá»‘ token Ä‘áº§u vÃ o tá»« 2k Ä‘áº¿n 192k. Äá»‘i vá»›i má»—i Ä‘á»™ dÃ i ngá»¯ cáº£nh Ä‘áº§u vÃ o, mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ 20 láº§n, vá»›i vá»‹ trÃ­ passkey Ä‘Æ°á»£c thay Ä‘á»•i ngáº«u nhiÃªn trong má»—i thá»­ nghiá»‡m. NgoÃ i ra, chÃºng tÃ´i cÅ©ng xÃ¡c minh mÃ´ hÃ¬nh Together-32k 7B (Together, 2023), há»— trá»£ cá»­a sá»• ngá»¯ cáº£nh 32k token, vÃ  Ä‘á»‘i tÃ¡c ChunkTogether 7B cá»§a nÃ³. Káº¿t quáº£ cho cáº£ biáº¿n thá»ƒ baseline vÃ  Ä‘Æ°á»£c tÄƒng cÆ°á»ng DCA cá»§a cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c minh há»a trong HÃ¬nh 7. Chá»‰ vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n 4k, CHUNK LLAMA 2 duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t cao lÃªn Ä‘áº¿n Ä‘á»™ dÃ i ngá»¯ cáº£nh 32k. Báº±ng cÃ¡ch tÃ­ch há»£p nhá»¯ng phÃ¡t hiá»‡n nÃ y vá»›i cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i hiá»‡n táº¡i, chÃºng ta cÃ³ thá»ƒ má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh Ä‘Æ°á»£c há»— trá»£ Ä‘áº¿n 192k token áº¥n tÆ°á»£ng sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n há»c.

máº¥t á»Ÿ Ä‘áº§u : Má»™t quan sÃ¡t thÃº vá»‹ lÃ  cÃ¡c trÆ°á»ng há»£p tháº¥t báº¡i cá»§a PI dÆ°á»ng nhÆ° pháº§n lá»›n khÃ´ng liÃªn quan Ä‘áº¿n Ä‘á»™ sÃ¢u cá»§a tÃ i liá»‡u, trong khi phÆ°Æ¡ng phÃ¡p dá»±a trÃªn NTK thÆ°á»ng xuáº¥t sáº¯c khi passkey Ä‘Æ°á»£c Ä‘áº·t gáº§n Ä‘áº§u tÃ i liá»‡u. Tuy nhiÃªn, hiá»‡u quáº£ cá»§a nÃ³ giáº£m Ä‘Ã¡ng ká»ƒâ€”vá»›i Ä‘á»™ chÃ­nh xÃ¡c giáº£m xuá»‘ng tá»« 40% Ä‘áº¿n 80%â€”khi passkey Ä‘Æ°á»£c Ä‘áº·t á»Ÿ cÃ¡c pháº§n giá»¯a. Xu hÆ°á»›ng nÃ y phÃ¹ há»£p vá»›i cÃ¡c phÃ¡t hiá»‡n Ä‘Æ°á»£c bÃ¡o cÃ¡o bá»Ÿi Liu et al. (2023a). NgÆ°á»£c láº¡i, khi ngá»¯ cáº£nh Ä‘áº§u vÃ o Ä‘Æ°á»£c má»Ÿ rá»™ng, CHUNK LLAMA 2 chá»©ng minh hiá»‡u suáº¥t cáº£i thiá»‡n trong cÃ¡c pháº§n giá»¯a nhÆ°ng nÆ¡i Ä‘áº§u tiÃªn mÃ  Ä‘á»™ chÃ­nh xÃ¡c giáº£m xuá»‘ng lÃ  á»Ÿ Ä‘áº§u vÄƒn báº£n.

(a) Llama2 PI(b) Llama2 NTK(c) ChunkLlama

HÃ¬nh 5. Thá»­ nghiá»‡m CÃ¡c PhÆ°Æ¡ng phÃ¡p Má»Ÿ rá»™ng KhÃ´ng cáº§n Há»c khÃ¡c nhau vá»›i Ngá»¯ cáº£nh 24K ("Needle in a Haystack" Passkey Retrieval). Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh cÃ³ ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c 4k vÃ  khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n thÃªm. Trá»¥c X Ä‘áº¡i diá»‡n cho Ä‘á»™ dÃ i ngá»¯ cáº£nh Ä‘áº§u vÃ o, vÃ  trá»¥c Y chá»‰ ra Ä‘á»™ sÃ¢u cá»§a passkey trong tÃ i liá»‡u. Äá»‘i vá»›i má»—i Ä‘á»™ sÃ¢u, chÃºng tÃ´i cháº¡y 20 trÆ°á»ng há»£p thá»­ nghiá»‡m khÃ¡c nhau.

HÃ¬nh 6. Thá»­ nghiá»‡m Ã¡p lá»±c Mistral-7B-Instruct-v0.2 trÃªn Ä‘á»™ dÃ i ngá»¯ cáº£nh 192k cho vÃ  phiÃªn báº£n Ä‘Æ°á»£c tÄƒng cÆ°á»ng DCA cá»§a nÃ³ (("Needle In A HayStack")).

13

--- TRANG 11 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

kF9F3FReFjkFe9FRk3FRNkF00.51
*QMi2ti qBM/Qr++m``+vhQ;2i?2`@jkF*?mMFhQ;2i?2`@jkFkF9F3FReFjkFe9FRk3FRNkF00.51
*QMi2ti qBM/Qr++m``+vGHK@9F*?mMFGHK@9F

(a) Llama2-4k vÃ  Chunkllama(b) Together-32k vÃ  ChunkTogether

HÃ¬nh 7. Truy xuáº¥t passkey trÃªn Ä‘á»™ dÃ i ngá»¯ cáº£nh 192k cho Llama2 13B, Together-32k 7B vÃ  cÃ¡c phiÃªn báº£n Ä‘Æ°á»£c tÄƒng cÆ°á»ng DCA cá»§a chÃºng.

0102103210010210321001021032107654765476547654Inter-chunk76547654765476547654765476547654010210321001021032100102103210
Intra-chunk010210321001021032100102103210432154326543765443215432654376547654765476547654successive-chunk

(a) Intra-Chunk Attention(b) Inter-Chunk Attention(c) Successive-Chunk Attention

HÃ¬nh 8. Trá»±c quan hÃ³a Ma tráº­n Vá»‹ trÃ­ TÆ°Æ¡ng Ä‘á»‘i M sá»­ dá»¥ng Dual Chunk Attention (DCA) báº±ng cÃ¡ch chia toÃ n bá»™ chuá»—i thÃ nh 3 chunk vÃ  kÃ­ch thÆ°á»›c chunk s= 4. Trong trÆ°á»ng há»£p nÃ y, chÃºng ta cÃ³ kÃ­ch thÆ°á»›c cá»­a sá»• huáº¥n luyá»‡n trÆ°á»›c c= 8, vÃ  kÃ­ch thÆ°á»›c cá»­a sá»• cá»¥c bá»™ w= 3. Chuá»—i Ä‘Æ°á»£c phÃ¢n Ä‘oáº¡n thÃ nh 3 chunk Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i khÃ´ng vÆ°á»£t quÃ¡ 7. Pháº§n tá»­ ma tráº­n M[i][j] Ä‘áº¡i diá»‡n cho vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a vector query thá»© i q vÃ  vector key thá»© j k.

A.2. ThÃªm VÃ­ dá»¥
Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘Æ°a ra má»™t vÃ­ dá»¥ vá» viá»‡c xá»­ lÃ½ má»™t chuá»—i 12-token nhÆ°ng Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c chá»‰ lÃ  8. Dá»±a trÃªn Llama2, cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ key/query sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o nhÆ°:
Pq= [0,1,2,3,4,5,6,7,8,9,10,11]
Pk= [0,1,2,3,4,5,6,7,8,9,10,11].

Khi query [11] vÃ  query [0] thá»±c hiá»‡n tÃ­ch trong, vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a chÃºng lÃ  11, vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c huáº¥n luyá»‡n trÆ°á»›c. Trong DCA, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c chunk, Ä‘Ã³ lÃ  má»™t siÃªu tham sá»‘ nhá» hÆ¡n Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c. Trong vÃ­ dá»¥ nÃ y, chÃºng ta cÃ³ thá»ƒ Ä‘áº·t kÃ­ch thÆ°á»›c chunk thÃ nh 4, cÃ³ nghÄ©a lÃ  chÃºng ta chia toÃ n bá»™ Ä‘áº§u vÃ o thÃ nh 3 chunk vÃ  Ä‘Æ°a ra cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ má»›i cho keys:
[0,1,2,3,4,5,6,7,8,9,10,11]â‡’[0,1,2,3,0,1,2,3,0,1,2,3]

Sau Ä‘Ã³ chÃºng tÃ´i Ä‘Æ°a ra cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ khÃ¡c nhau cho queries.

Intra-chunk Attention: TÃ­nh toÃ¡n attention cho cÃ¡c token trong cÃ¹ng má»™t chunk
Pintraq= [0,1,2,3,0,1,2,3,0,1,2,3]
Pk= [0,1,2,3,0,1,2,3,0,1,2,3]

Ma tráº­n vá»‹ trÃ­ káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8 Phá»¥ lá»¥c (a) vÃ  vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i tá»‘i Ä‘a lÃ  3âˆ’0 = 3 .

Inter-chunk Attention: TÃ­nh toÃ¡n attention cho cÃ¡c token trong cÃ¡c chunk khÃ¡c nhau Khi Ä‘á»™ dÃ i huáº¥n luyá»‡n trÆ°á»›c lÃ  8, chÃºng ta cÃ³ chá»‰ sá»‘ vá»‹ trÃ­ tá»‘i Ä‘a=7.
Pinterq= [7,7,7,7,7,7,7,7,7,7,7,7]
Pk= [0,1,2,3,0,1,2,3,0,1,2,3]

14

--- TRANG 12 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Ma tráº­n vá»‹ trÃ­ káº¿t quáº£ sau inter-chunk attention Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8 Phá»¥ lá»¥c (b).

Successive-chunk Attention: TÃ­nh toÃ¡n attention cho cÃ¡c token trong cÃ¡c chunk liÃªn tiáº¿p. ChÃºng tÃ´i thay Ä‘á»•i w= 3 pháº§n tá»­ Ä‘áº§u tiÃªn (má»™t siÃªu tham sá»‘) trong Pinterq:
Psuccq= [4,5,6,7,4,5,6,7,4,5,6,7]
Pk= [0,1,2,3,0,1,2,3,0,1,2,3]

Ma tráº­n vá»‹ trÃ­ sau successive-chunk attention Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8 Phá»¥ lá»¥c (c).

A.3. Flash Attention
ChÃºng tÃ´i chia self-attention tiÃªu chuáº©n thÃ nh 3 tÃ­nh toÃ¡n flash attention riÃªng biá»‡t tÆ°Æ¡ng á»©ng thu Ä‘Æ°á»£c Ä‘áº§u ra tá»« intra-chunk attention, inter-chunk-attention, vÃ  successive chunk-attention. Thuáº­t toÃ¡n 1 thá»ƒ hiá»‡n cÃ¡ch 3 attention Ä‘Æ°á»£c giá»›i thiá»‡u trong DCA tÃ­ch há»£p vá»›i Flash Attention. ChÃºng tÃ´i minh há»a vá»›i vector query thá»© i qi vÃ  nÃ³ cáº§n tÃ­nh toÃ¡n tÃ­ch trong vá»›i táº¥t cáº£ keys kj vá»›i jâ‰¤i. ChÃºng ta cÃ³ n=âŒŠi/sâŒ‹ chunk trÆ°á»›c chunk hiá»‡n táº¡i. DCA gá»i 3 hoáº¡t Ä‘á»™ng Flash Attention riÃªng biá»‡t vá»›i Ä‘á»™ phá»©c táº¡p O(iâˆ’nâˆ—s)(intra-chunk attention), O(s)(succssive-chunk attention) vÃ  O(sâˆ—(nâˆ’1)).

Thuáº­t toÃ¡n 1 MÃ£ giáº£ cá»§a DCA vá»›i FlashAttention
# q: vector query 1 x d (tensor vá»›i hÃ¬nh dáº¡ng [1, d])
# i: chá»‰ sá»‘ tuyá»‡t Ä‘á»‘i cá»§a q (sá»‘ nguyÃªn)
# K, V: ma tráº­n i x d cho keys vÃ  values (tensor vá»›i hÃ¬nh dáº¡ng [i, d])
# s: kÃ­ch thÆ°á»›c chunk (sá»‘ nguyÃªn)
# P_k, P_q_intra, P_q_succ, P_q_inter: ids vá»‹ trÃ­ (danh sÃ¡ch sá»‘ nguyÃªn)
n = math.floor(i/s) # Sá»‘ chunk trÆ°á»›c chunk hiá»‡n táº¡i
# Ãp dá»¥ng embedding vá»‹ trÃ­ xoay cho toÃ n bá»™ ma tráº­n key K
K = apply_rotary_pos_emb(K, P_k) # K lÃ  [i, d] sau embedding
# ------------- Intra-chunk Attention, casual=True -------------
q_intra = apply_rotary_pos_emb(q, P_q_intra[i]) # q_intra lÃ  [1, d]
# Chá»n keys vÃ  values intra-chunk
K_intra = K[s *n:i] # K_intra lÃ  [(i - s *n), d]
V_intra = V[s *n:i] # V_intra lÃ  [(i - s *n), d]
# TÃ­nh toÃ¡n Ä‘áº§u ra vÃ  báº£n Ä‘á»“ attention softmax cho intra-chunk attention
o_intra, map_intra = Flash(q_intra, K_intra, V_intra) # o_intra lÃ  [1, d], map_intra lÃ  [1, i - s *n]
# ------------- Successive-chunk Attention, casual=False -----------
q_succ = apply_rotary_pos_emb(q, P_q_succ[i]) # q_succ lÃ  [1, d]
# Chá»n keys vÃ  values successive-chunk
K_succ = K[s *(n-1):s *n] # K_succ lÃ  [s, d]
V_succ = V[s *(n-1):s *n] # V_succ lÃ  [s, d]
# TÃ­nh toÃ¡n Ä‘áº§u ra vÃ  báº£n Ä‘á»“ attention softmax cho successive-chunk attention
o_succ, map_succ = Flash(q_succ, K_succ, V_succ) # o_succ lÃ  [1, d], map_succ lÃ  [1, s]
# ------------- Inter-chunk Attention, casual=False -----------
q_inter = apply_rotary_pos_emb(q, P_q_inter[i]) # q_inter lÃ  [1, d]
# Chá»n keys vÃ  values inter-chunk
K_inter = K[:s *(n-1)] # K_inter lÃ  [s *(n-1), d]
V_inter = V[:s *(n-1)] # V_inter lÃ  [s *(n-1), d]
# TÃ­nh toÃ¡n Ä‘áº§u ra vÃ  báº£n Ä‘á»“ attention softmax cho inter-chunk attention
o_inter, map_inter = Flash(q_inter, K_inter, V_inter) # o_inter lÃ  [1, d], map_inter lÃ  [1, s *(n-1)]
# Chuáº©n hÃ³a
# Tá»•ng cÃ¡c báº£n Ä‘á»“ attention cho má»—i loáº¡i attention Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c normalizer
sum_intra = map_intra.sum(-1) # sum_intra lÃ  má»™t vÃ´ hÆ°á»›ng
sum_inter = map_inter.sum(-1) # sum_inter lÃ  má»™t vÃ´ hÆ°á»›ng
sum_succ = map_succ.sum(-1) # sum_succ lÃ  má»™t vÃ´ hÆ°á»›ng
normalizer = sum_intra + sum_inter + sum_succ # normalizer lÃ  má»™t vÃ´ hÆ°á»›ng
# Ná»‘i cÃ¡c Ä‘áº§u ra attention vÃ  chia cho normalizer
output = (sum_intra *o_intra, sum_succ *o_succ, sum_inter *o_inter) / normalizer # output lÃ  [1, d]

A.4. Lá»±a chá»n VÃ­ dá»¥ Trong Ngá»¯ cáº£nh
ChÃºng tÃ´i chá»n lá»±a chá»n cÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh tá»« táº­p huáº¥n luyá»‡n Ä‘Ã¢y lÃ  cÃ¡ch thá»±c táº¿ vÃ  phá»• biáº¿n Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c vÃ­ dá»¥ (Ye et al., 2023; Wang et al., 2024). ChÃºng tÃ´i thá»­ nghiá»‡m vá»›i 2 phÆ°Æ¡ng phÃ¡p khÃ¡c nhau cho quÃ¡ trÃ¬nh lá»±a chá»n nÃ y: (1)Lá»±a chá»n Ngáº«u nhiÃªn: chá»n ngáº«u nhiÃªn cÃ¡c vÃ­ dá»¥ tá»« táº­p huáº¥n luyá»‡n. (2) Lá»±a chá»n Dá»±a trÃªn Truy xuáº¥t: Sá»­ dá»¥ng truy váº¥n hiá»‡n táº¡i, chÃºng tÃ´i sá»­ dá»¥ng

15

--- TRANG 13 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Báº£ng 5. So sÃ¡nh káº¿t quáº£ few-shot sá»­ dá»¥ng cÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh khÃ¡c nhau

MÃ´ hÃ¬nhVÃ­ dá»¥ Trong Ngá»¯ cáº£nhQasper QuALITY QMSum
F1 (2-shot) EM (2-shot) R-g (1-shot)

CHUNK LLAMA 2 7B EXAMPLE BEST 27.3 33.9 15.0
CHUNK LLAMA 2 7B EXAMPLE RANDOM 28.2 35.6 14.7
CHUNK LLAMA 2 7B EXAMPLE WORST 28.4 35.9 14.3
CHUNK LLAMA 2 13B EXAMPLE BEST 28.5 46.2 15.6
CHUNK LLAMA 2 13B EXAMPLE RANDOM 29.3 47.9 15.2
CHUNK LLAMA 2 13B EXAMPLE WORST 29.0 47.5 15.5

cÃ¡c thuáº­t toÃ¡n truy xuáº¥t nhÆ° BM25 (Robertson et al., 2009) Ä‘á»ƒ tÃ¬m cÃ¡c vÃ­ dá»¥ cÃ³ liÃªn quan nháº¥t tá»« táº­p huáº¥n luyá»‡n. ChÃºng tÃ´i gá»i cÃ¡c vÃ­ dá»¥ trong ngá»¯ cáº£nh vá»›i Ä‘iá»ƒm truy xuáº¥t cao nháº¥t lÃ  EXAMPLE BEST vÃ  nhá»¯ng cÃ¡i cÃ³ Ä‘iá»ƒm tháº¥p nháº¥t lÃ  EXAMPLE WORST . Hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p lá»±a chá»n khÃ¡c nhau dá»±a trÃªn CHUNK LLAMA 27B/13B Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 5. Hiá»‡u suáº¥t trÃªn táº­p dá»¯ liá»‡u tÃ³m táº¯t QMSum (Zhong et al., 2021) nÃ³i chung Ã­t cÃ³ kháº£ nÄƒng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi viá»‡c lá»±a chá»n prompt. Tuy nhiÃªn, trÃªn 2 táº­p dá»¯ liá»‡u tráº£ lá»i cÃ¢u há»i, chÃºng tÃ´i tháº¥y ráº±ng sá»­ dá»¥ng cÃ¡c vÃ­ dá»¥ gáº§n nháº¥t, má»™t cÃ¡ch nghá»‹ch lÃ½, dáº«n Ä‘áº¿n káº¿t quáº£ tá»“i tá»‡ nháº¥t vÃ  hiá»‡u suáº¥t cá»§a cáº£ lá»±a chá»n ngáº«u nhiÃªn vÃ  chá»n vÃ­ dá»¥ tá»“i tá»‡ nháº¥t tÆ°Æ¡ng Ä‘á»‘i tÆ°Æ¡ng tá»±. Má»™t lá»i giáº£i thÃ­ch cÃ³ thá»ƒ cho hiá»‡n tÆ°á»£ng nÃ y lÃ  khi vÃ­ dá»¥ ráº¥t tÆ°Æ¡ng tá»±, LLMs cÃ³ xu hÆ°á»›ng sao chÃ©p pháº£n há»“i Ä‘Æ°á»£c Ä‘Æ°a ra trong vÃ­ dá»¥ thÆ°á»ng dáº«n Ä‘áº¿n cÃ¢u tráº£ lá»i sai.

A.5. Hiá»‡u suáº¥t trÃªn Dá»¯ liá»‡u ChÆ°a tháº¥y
Hiá»‡n táº¡i, háº§u nhÆ° táº¥t cáº£ cÃ¡c benchmark cho LLMs khÃ´ng giáº£i quyáº¿t triá»‡t Ä‘á»ƒ kháº£ nÄƒng nhiá»…m dá»¯ liá»‡u, cÃ³ nghÄ©a lÃ  dá»¯ liá»‡u thá»­ nghiá»‡m cÃ³ thá»ƒ Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng trong giai Ä‘oáº¡n huáº¥n luyá»‡n trÆ°á»›c hoáº·c tinh chá»‰nh. Äá»ƒ chá»©ng minh hiá»‡u suáº¥t cá»§a ChunkLlama trÃªn dá»¯ liá»‡u tÃ i liá»‡u dÃ i chÆ°a tháº¥y trÆ°á»›c Ä‘Ã¢y, chÃºng tÃ´i trá»±c tiáº¿p sá»­ dá»¥ng mÃ£ Latex cá»§a bÃ i bÃ¡o nÃ y lÃ m trÆ°á»ng há»£p thá»­ nghiá»‡m trong khi bá» qua cÃ¡c pháº§n tiÃªu Ä‘á», tÃ³m táº¯t vÃ  káº¿t luáº­n. Sau khi tokenization, tá»•ng Ä‘á»™ dÃ i Ä‘áº§u vÃ o lÃ  19388. ChÃºng tÃ´i báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ vá»›i má»™t táº­p há»£p cÃ¡c cÃ¢u há»i Ä‘Æ¡n giáº£n khÃ´ng cáº§n thiáº¿t kiáº¿n thá»©c trÆ°á»›c Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c (xem Báº£ng 6). Tiáº¿p theo lÃ  má»™t loáº¡t cÃ¢u há»i thÃ¡ch thá»©c hÆ¡n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ sá»± hiá»ƒu biáº¿t vá» DCA Ä‘Æ°á»£c Ä‘á» xuáº¥t (tham kháº£o Báº£ng 7).

Káº¿t quáº£ chá»‰ ra ráº±ng, so vá»›i NTK, CHUNK LLAMA 2 chá»©ng minh kháº£ nÄƒng vÆ°á»£t trá»™i Ä‘á»ƒ diá»…n giáº£i chÃ­nh xÃ¡c hÆ°á»›ng dáº«n vÃ  cung cáº¥p pháº£n há»“i chÃ­nh xÃ¡c. Tuy nhiÃªn, Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¢u tráº£ lá»i cá»§a mÃ´ hÃ¬nh 13B váº«n khÃ´ng tá»‘i Æ°u, ngay cáº£ Ä‘á»‘i vá»›i cÃ¡c truy váº¥n tÆ°Æ¡ng Ä‘á»‘i Ä‘Æ¡n giáº£n. VÃ­ dá»¥, khi Ä‘Æ°á»£c há»i vá» corpus tinh chá»‰nh Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi DCA, nÃ³ sai láº§m trÃ­ch dáº«n corpus Llama2 thay vÃ¬ cÃ¡c corpus chÃ­nh xÃ¡c, Ä‘Ã³ lÃ  ShareGPT vÃ  AlpacaGPT4.

Vá» nhá»¯ng cÃ¢u há»i cÆ¡ báº£n nÃ y, ChunkLlama 70B thá»ƒ hiá»‡n tá»· lá»‡ chÃ­nh xÃ¡c cao Ä‘Ã¡ng ká»ƒ. HÆ¡n ná»¯a, ChunkLlama 70B cho tháº¥y káº¿t quáº£ há»©a háº¹n trÃªn cÃ¡c truy váº¥n thÃ¡ch thá»©c hÆ¡n. NÃ³ cÃ³ thá»ƒ diá»…n Ä‘áº¡t lÃ½ do Ä‘áº±ng sau thiáº¿t káº¿ cá»§a chÃºng tÃ´i vá» cÃ¡c chiáº¿n lÆ°á»£c inter-chunk vÃ  successive-chunk má»™t cÃ¡ch chÃ­nh xÃ¡c. Tuy nhiÃªn, khi chÃºng tÃ´i Ä‘áº·t cÃ¢u há»i yÃªu cáº§u hiá»ƒu biáº¿t toÃ n cá»¥c vá» phÆ°Æ¡ng phÃ¡p nÃ y, nÃ³ váº«n gáº·p khÃ³ khÄƒn.

16

--- TRANG 14 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Báº£ng 6. Káº¿t quáº£ QA (Dá»…) cá»§a CHUNK LLAMA 213B/70B vÃ  Dynamic-NTK vá»›i Llama2 13B, sá»­ dá»¥ng ná»™i dung cá»§a bÃ i bÃ¡o nÃ y lÃ m Ä‘áº§u vÃ o. Háº§u háº¿t cÃ¡c cÃ¢u há»i khÃ´ng yÃªu cáº§u hiá»ƒu biáº¿t sÃ¢u sáº¯c vá» bÃ i bÃ¡o nÃ y. Prompt Ä‘áº§u vÃ o cÃ³ 19388 token. Dynamic-NTK khÃ´ng thá»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i Ä‘Ãºng cho táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m. Káº¿t quáº£ táº¡o cháº¥t lÆ°á»£ng tháº¥p Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u vá»›i âœ—.

Prompt [INST] <<SYS>>Vui lÃ²ng Ä‘á»c bÃ i bÃ¡o dÃ i vÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i << / SYS>>1. Giá»›i thiá»‡u \n\n
Kháº£ nÄƒng hiá»ƒu vÃ  xá»­ lÃ½ thÃ´ng tin ngá»¯ cáº£nh dÃ i lÃ  Ä‘iá»u cáº§n thiáº¿t cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) Ä‘á»ƒ phá»¥c vá»¥ hiá»‡u quáº£ cho má»™t loáº¡t cÃ¡c á»©ng dá»¥ng. Nhá»¯ng á»©ng dá»¥ng nÃ y bao gá»“m phÃ¢n tÃ­ch vÃ  pháº£n há»“i cÃ¡c truy váº¥n trong cÃ¡c tá»‡p PDF lá»›n, giá»¯ láº¡i lá»‹ch sá»­ Ä‘á»‘i thoáº¡i má»Ÿ rá»™ng, vÃ  trao quyá»n cho cÃ¡c chatbot tÆ°Æ¡ng tÃ¡c... [/INST]

CÃ¢u há»i Báº¡n cÃ³ thá»ƒ Ä‘Æ°a ra má»™t tiÃªu Ä‘á» cho bÃ i bÃ¡o nÃ y khÃ´ng
CHUNK LLAMA 270B Cháº¯c cháº¯n, Ä‘Ã¢y lÃ  má»™t sá»‘ tiÃªu Ä‘á» tiá»m nÄƒng cho bÃ i bÃ¡o:
1. "Dual Chunk Attention: Má»™t Khung Framework KhÃ´ng cáº§n Huáº¥n luyá»‡n cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Ngá»¯ cáº£nh DÃ i"
2. "MÃ´ hÃ¬nh NgÃ´n ngá»¯ Ngá»¯ cáº£nh DÃ i Hiá»‡u quáº£ vá»›i Dual Chunk Attention"
3. "Má»Ÿ rá»™ng MÃ´ hÃ¬nh NgÃ´n ngá»¯ Ngá»¯ cáº£nh DÃ i mÃ  khÃ´ng cáº§n Huáº¥n luyá»‡n: Má»™t PhÆ°Æ¡ng phÃ¡p Dual Chunk Attention"
4. "DCA: Má»™t PhÆ°Æ¡ng phÃ¡p KhÃ´ng cáº§n Huáº¥n luyá»‡n cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Ngá»¯ cáº£nh DÃ i"
5. "MÃ´ hÃ¬nh NgÃ´n ngá»¯ Ngá»¯ cáº£nh DÃ i vá»›i Dual Chunk Attention: Má»™t Huáº¥n luyá»‡n...

CHUNK LLAMA 2 13B Cháº¯c cháº¯n! Dual Chunk Attention: Má»™t Khung Framework KhÃ´ng cáº§n Huáº¥n luyá»‡n cho Má»Ÿ rá»™ng Ngá»¯ cáº£nh DÃ i cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n.
TiÃªu Ä‘á» nÃ y lÃ m ná»•i báº­t cÃ¡c Ä‘Ã³ng gÃ³p chÃ­nh cá»§a bÃ i bÃ¡o: khung framework Dual Chunk Attention (DCA) Ä‘Æ°á»£c Ä‘á» xuáº¥t, cho phÃ©p má»Ÿ rá»™ng ngá»¯ cáº£nh dÃ i mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n bá»• sung, vÃ  á»©ng dá»¥ng cá»§a nÃ³ cho cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau. Cá»¥m tá»« "khÃ´ng cáº§n huáº¥n luyá»‡n" cÅ©ng Ä‘Æ°á»£c bao gá»“m Ä‘á»ƒ nháº¥n máº¡nh sá»± Ä‘á»•i má»›i chÃ­nh cá»§a bÃ i bÃ¡o.

Dynamic NTK 13B Khung framework Dual Chunk Attention (DCA) Ä‘Æ°á»£c Ä‘á» xuáº¥t cáº£i thiá»‡n viá»‡c má»Ÿ rá»™ng ngá»¯ cáº£nh dÃ i cá»§a Lama2 hÆ¡n 8 láº§n, Ä‘áº¡t PPL 5.59 á»Ÿ cá»­a sá»• ngá»¯ cáº£nh 2k. âœ—(bá» qua hÆ°á»›ng dáº«n)

CÃ¢u há»i MÃ´ hÃ¬nh nÃ o cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng lÃªn ngá»¯ cáº£nh 192k vá»›i DCA?
CHUNK LLAMA 270B MÃ´ hÃ¬nh Llama2 70B cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng lÃªn cá»­a sá»• ngá»¯ cáº£nh 192k sá»­ dá»¥ng DCA, Ä‘áº¡t Ä‘iá»ƒm PPL 5.59
CHUNK LLAMA 2 13B MÃ´ hÃ¬nh Llama2 70B cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng lÃªn ngá»¯ cáº£nh 192k vá»›i DCA.
Dynamic NTK 13B {\end{enumerate}\end{figure}[ht!]
\end{document}\end{document}\begin{table} âœ—(lá»—i)

CÃ¢u há»i NgoÃ i Llama2, cÃ¡c tÃ¡c giáº£ cÃ³ xÃ¡c thá»±c phÆ°Æ¡ng phÃ¡p cá»§a há» trÃªn cÃ¡c mÃ´ hÃ¬nh khÃ¡c khÃ´ng?
CHUNK LLAMA 270B CÃ³, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ xÃ¡c thá»±c phÆ°Æ¡ng phÃ¡p cá»§a há» trÃªn cÃ¡c mÃ´ hÃ¬nh khÃ¡c, bao gá»“m
Together-32k,
CodeLlama,
táº¥t cáº£ Ä‘á»u dá»±a trÃªn Llama. Há» cÅ©ng Ä‘Ã£ so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a há» vá»›i nhiá»u mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ vÃ  Ä‘á»™c quyá»n.

CHUNK LLAMA 2 13B CÃ¡c tÃ¡c giáº£ Ä‘Ã£ xÃ¡c thá»±c phÆ°Æ¡ng phÃ¡p cá»§a há» trÃªn cÃ¡c mÃ´ hÃ¬nh sau:
* Together-32k
* CodeLlama
* Llama2 Long âœ—(ChÃºng tÃ´i khÃ´ng xÃ¡c minh DCA trÃªn Llama2 Long)

Dynamic NTK 13B BÃ i bÃ¡o giá»›i thiá»‡u má»™t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘Æ°á»£c gá»i lÃ  Dual Chunk Attention (DCA) má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh hÆ¡n 8 láº§n cho cÃ¡c biáº¿n thá»ƒ Llama2 7B/13B vÃ  20 láº§n cho cÃ¡c mÃ´ hÃ¬nh 70B âœ—(bá» qua hÆ°á»›ng dáº«n)

CÃ¢u há»i MÃ´ táº£ corpus tinh chá»‰nh Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi DCA
CHUNK LLAMA 270B Corpus tinh chá»‰nh Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi DCA lÃ  sá»± káº¿t há»£p dá»¯ liá»‡u Ä‘á»‘i thoáº¡i dÃ i tá»« ShareGPT vÃ  AlpacaGPT4
CHUNK LLAMA 2 13B Corpus tinh chá»‰nh Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi DCA giá»‘ng nhÆ° corpus Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi mÃ´ hÃ¬nh Llama2 gá»‘c, lÃ  sá»± káº¿t há»£p cá»§a táº­p huáº¥n luyá»‡n vÃ  táº­p xÃ¡c thá»±c cá»§a mÃ´ hÃ¬nh Llama2 gá»‘c âœ—(ChÃºng tÃ´i khÃ´ng sá»­ dá»¥ng táº­p huáº¥n luyá»‡n cá»§a Llama)
Dynamic NTK 13B {\end{begin}\begin{table}\end{figure}
[ht!]} \end{document}\end{document}\end{document} âœ—(lá»—i)

CÃ¢u há»i Llama2 Long cáº§n bao nhiá»u bÆ°á»›c huáº¥n luyá»‡n?
CHUNK LLAMA 270B Llama2 Long cáº§n 100,000 bÆ°á»›c huáº¥n luyá»‡n
CHUNK LLAMA 2 13B Llama2 Long cáº§n 100,000 bÆ°á»›c huáº¥n luyá»‡n.
Dynamic NTK 13B Llama2 Long cáº§n 40,000 bÆ°á»›c cho cá»­a sá»• ngá»¯ cáº£nh 4k. âœ—(khÃ´ng pháº£i 40,000)

CÃ¢u há»i CÃ¡c tÃ¡c giáº£ cÃ³ xÃ¡c thá»±c mÃ´ hÃ¬nh trÃªn L-Eval khÃ´ng? vÃ  liá»‡t kÃª cÃ¡c tÃ¡c vá»¥
CHUNK LLAMA 270B CÃ³, cÃ¡c tÃ¡c giáº£ xÃ¡c thá»±c mÃ´ hÃ¬nh cá»§a há» trÃªn L-Eval vÃ  liá»‡t kÃª cÃ¡c tÃ¡c vá»¥ trong Báº£ng 4. Há» Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a há» trÃªn bá»‘n tÃ¡c vá»¥ Ä‘Ã³ng tá»« L-Eval vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o Ä‘a dáº¡ng dao Ä‘á»™ng tá»« 3k Ä‘áº¿n 27k. CÃ¡c tÃ¡c vá»¥ bao gá»“m TOFEL, QuALITY , Coursera, vÃ  SFiction.
CHUNK LLAMA 2 13B CÃ³, cÃ¡c tÃ¡c giáº£ xÃ¡c thá»±c mÃ´ hÃ¬nh trÃªn L-Eval vÃ  liá»‡t kÃª cÃ¡c tÃ¡c vá»¥. Trong pháº§n cÃ³ tiÃªu Ä‘á» "ThÃ­ nghiá»‡m", cÃ¡c tÃ¡c giáº£ trÃ¬nh bÃ y má»™t Ä‘Ã¡nh giÃ¡ rá»™ng rÃ£i vá» mÃ´ hÃ¬nh cá»§a há» trÃªn má»™t táº­p Ä‘a dáº¡ng cÃ¡c tÃ¡c vá»¥, bao gá»“m NarrativeQA, Qasper, Quality, vÃ  QMSum. âœ—(Nhá»¯ng cÃ¡i nÃ y khÃ´ng pháº£i tá»« L-Eval)
Dynamic NTK 13B BÃ i bÃ¡o trÃ¬nh bÃ y má»™t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘Æ°á»£c gá»i lÃ  Dual Chunk Attention (DCA) má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh hÆ¡n 8 láº§n cho cÃ¡c biáº¿n thá»ƒ Llama2 7B/13B vÃ ... âœ—(bá» qua hÆ°á»›ng dáº«n)

17

--- TRANG 15 ---
Má»Ÿ rá»™ng Ngá»¯ cáº£nh khÃ´ng cáº§n Huáº¥n luyá»‡n cá»§a CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Báº£ng 7. Káº¿t quáº£ QA (khÃ³) cá»§a CHUNK LLAMA 213B/70B vÃ  Dynamic-NTK vá»›i Llama2 13B, sá»­ dá»¥ng ná»™i dung cá»§a bÃ i bÃ¡o nÃ y lÃ m Ä‘áº§u vÃ o. CÃ¢u há»i trong báº£ng nÃ y thÆ°á»ng yÃªu cáº§u Ä‘á»™c giáº£ hiá»ƒu phÆ°Æ¡ng phÃ¡p nÃ y. Prompt Ä‘áº§u vÃ o cÃ³ 19388 token. Dynamic-NTK khÃ´ng thá»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i Ä‘Ãºng cho táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m. Káº¿t quáº£ táº¡o cháº¥t lÆ°á»£ng tháº¥p Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u vá»›i âœ—.

Prompt [INST] <<SYS>>Vui lÃ²ng Ä‘á»c bÃ i bÃ¡o dÃ i vÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i << / SYS>>1. Giá»›i thiá»‡u \n\n
Kháº£ nÄƒng hiá»ƒu vÃ  xá»­ lÃ½ thÃ´ng tin ngá»¯ cáº£nh dÃ i lÃ  Ä‘iá»u cáº§n thiáº¿t cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) Ä‘á»ƒ phá»¥c vá»¥ hiá»‡u quáº£ cho má»™t loáº¡t cÃ¡c á»©ng dá»¥ng. Nhá»¯ng á»©ng dá»¥ng nÃ y bao gá»“m phÃ¢n tÃ­ch vÃ  pháº£n há»“i cÃ¡c truy váº¥n trong cÃ¡c tá»‡p PDF lá»›n, giá»¯ láº¡i lá»‹ch sá»­ Ä‘á»‘i thoáº¡i má»Ÿ rá»™ng, vÃ  trao quyá»n cho cÃ¡c chatbot tÆ°Æ¡ng tÃ¡c... [/INST]

CÃ¢u há»i Táº¡i sao cÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u successive-chunk attention? (KhÃ³)
CHUNK LLAMA 270B CÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u successive-chunk attention Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» máº¥t tÃ­nh cá»¥c bá»™ trong cÆ¡ cháº¿ attention khi sá»­ dá»¥ng inter-chunk attention. Inter-chunk attention cÃ³ thá»ƒ dáº«n Ä‘áº¿n tÃ¬nh huá»‘ng mÃ  vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a hai token trong cÃ¡c chunk khÃ¡c nhau khÃ´ng Ä‘Æ°á»£c náº¯m báº¯t chÃ­nh xÃ¡c, dáº«n Ä‘áº¿n perplexity cao. Successive-chunk attention giÃºp duy trÃ¬ tÃ­nh cá»¥c bá»™ báº±ng cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cá»§a w token Ä‘áº§u tiÃªn trong má»—i chunk Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c token lÃ¢n cáº­n Ä‘Æ°á»£c náº¯m báº¯t chÃ­nh xÃ¡c. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh xá»­ lÃ½ tá»‘t hÆ¡n cÃ¡c tÃ¡c vá»¥ ngá»¯ cáº£nh dÃ i yÃªu cáº§u kháº£ nÄƒng náº¯m báº¯t cÃ¡c phá»¥ thuá»™c táº§m xa.

CHUNK LLAMA 2 13B CÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u successive-chunk attention Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» phá»¥ thuá»™c táº§m xa trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ ngá»¯ cáº£nh dÃ i. CÃ¡c phÆ°Æ¡ng phÃ¡p mÃ£ hÃ³a vá»‹ trÃ­ gá»‘c, nhÆ° RoPE vÃ  NTK-aware RoPE, khÃ´ng náº¯m báº¯t cÃ¡c phá»¥ thuá»™c táº§m xa hiá»‡u quáº£, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ vÃ  cÃ¡c tÃ¡c vá»¥ truy xuáº¥t passkey. Báº±ng cÃ¡ch giá»›i thiá»‡u successive-chunk attention, mÃ´ hÃ¬nh cÃ³ thá»ƒ náº¯m báº¯t cáº£ phá»¥ thuá»™c cá»¥c bá»™ vÃ  táº§m xa.. âœ—(hiá»ƒu sai successive-chunk attention)

Dynamic NTK 13B BÃ i bÃ¡o giá»›i thiá»‡u má»™t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘Æ°á»£c gá»i lÃ  Dual Chunk Attention (DCA) má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh hÆ¡n 8 láº§n cho cÃ¡c biáº¿n thá»ƒ Llama2 7B/13B... âœ—(bá» qua hÆ°á»›ng dáº«n)

CÃ¢u há»i Táº¡i sao cÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u inter-chunk attention? (KhÃ³)
CHUNK LLAMA 270B CÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u inter-chunk attention Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» phá»¥ thuá»™c táº§m xa kÃ©m trong module intra-chunk attention, sá»­ dá»¥ng cá»­a sá»• ngá»¯ cáº£nh cá»‘ Ä‘á»‹nh Ä‘á»ƒ tÃ­nh toÃ¡n attention. CÆ¡ cháº¿ inter-chunk attention cho phÃ©p mÃ´ hÃ¬nh chÃº Ã½ Ä‘áº¿n cÃ¡c token tá»« cÃ¡c chunk khÃ¡c nhau, cho phÃ©p nÃ³ náº¯m báº¯t cÃ¡c phá»¥ thuá»™c táº§m xa vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u xá»­ lÃ½ cÃ¡c chuá»—i dÃ i.

CHUNK LLAMA 2 13B 13b: TÃ¡c giáº£ giá»›i thiá»‡u inter-chunk attention Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t truy xuáº¥t passkey cá»§a mÃ´ hÃ¬nh. TÃ¡c giáº£ lÆ°u Ã½ ráº±ng chá»‰ sá»­ dá»¥ng intra-chunk attention, bá» qua thÃ´ng tin tá»« cÃ¡c chunk trÆ°á»›c Ä‘Ã³, cÃ³ thá»ƒ duy trÃ¬ PPL tháº¥p nhÆ°ng cáº£n trá»Ÿ kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ truy xuáº¥t passkey tá»« cÃ¡c chunk khÃ¡c. âœ—(khÃ´ng Ä‘á»§ chÃ­nh xÃ¡c)

Dynamic NTK 13B BÃ i bÃ¡o trÃ¬nh bÃ y má»™t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘Æ°á»£c gá»i lÃ  Dual Chunk Attention (DCA) má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh hÆ¡n 8 láº§n cho cÃ¡c biáº¿n thá»ƒ Llama2 7B/13B... âœ—(bá» qua hÆ°á»›ng dáº«n)

CÃ¢u há»i Táº¡i sao vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i trong DCA sáº½ khÃ´ng vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c? (KhÃ³)
CHUNK LLAMA 270B Vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i trong DCA Ä‘Æ°á»£c tÃ­nh toÃ¡n sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ vá»‹ trÃ­ cá»§a keys vÃ  queries, cáº£ hai Ä‘á»u nhá» hÆ¡n hoáº·c báº±ng Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c. Do Ä‘Ã³, vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i sáº½ khÃ´ng vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c. (khÃ´ng rÃµ rÃ ng Ä‘á»§)

CHUNK LLAMA 2 13B Vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i trong DCA sáº½ khÃ´ng vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c vÃ¬ mÃ´ hÃ¬nh chá»‰ xem xÃ©t cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i trong cÃ¹ng má»™t chunk. CÃ¡c chunk Ä‘Æ°á»£c phÃ¢n Ä‘oáº¡n dá»±a trÃªn Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c, vÃ  cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i Ä‘Æ°á»£c tÃ­nh toÃ¡n trong má»—i chunk. (khÃ´ng rÃµ rÃ ng Ä‘á»§)

Dynamic NTK 13B Vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i trong Dual Chunk Attention (DCA) khÃ´ng vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i ngá»¯ cáº£nh huáº¥n luyá»‡n trÆ°á»›c vÃ¬ nÃ³ Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ duy trÃ¬ tÃ­nh cá»¥c bá»™. (khÃ´ng rÃµ rÃ ng Ä‘á»§)

CÃ¢u há»i CÃ¡c khÃ¡c biá»‡t chÃ­nh giá»¯a bÃ i bÃ¡o nÃ y vÃ  cÃ¡c bÃ i bÃ¡o trÆ°á»›c Ä‘Ã¢y lÃ  gÃ¬? (KhÃ³)
CHUNK LLAMA 270B CÃ¡c khÃ¡c biá»‡t chÃ­nh giá»¯a bÃ i bÃ¡o nÃ y vÃ  cÃ¡c bÃ i bÃ¡o trÆ°á»›c Ä‘Ã¢y vá» mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) lÃ :
1. BÃ i bÃ¡o Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i Ä‘Æ°á»£c gá»i lÃ  Dual Chunk Attention (DCA) cho phÃ©p LLMs xá»­ lÃ½ thÃ´ng tin ngá»¯ cáº£nh dÃ i mÃ  khÃ´ng cÃ³ chi phÃ­ huáº¥n luyá»‡n bá»• sung.
2. DCA Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ thÃ´ng tin ngá»¯ cáº£nh dÃ i báº±ng cÃ¡ch phÃ¢n Ä‘oáº¡n chuá»—i Ä‘áº§u vÃ o thÃ nh cÃ¡c chunk nhá» hÆ¡n vÃ  tÃ­nh toÃ¡n attention trong má»—i chunk, giáº£m chi phÃ­ tÃ­nh toÃ¡n vÃ  cho phÃ©p má»Ÿ rá»™ng hiá»‡u quáº£ cá»­a sá»• ngá»¯ cáº£nh...

CHUNK LLAMA 2 13B BÃ i bÃ¡o nÃ y Ä‘á» xuáº¥t má»™t khung framework má»›i khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘Æ°á»£c gá»i lÃ  Dual Chunk Attention (DCA) cho phÃ©p cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) xá»­ lÃ½ thÃ´ng tin ngá»¯ cáº£nh dÃ i mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n bá»• sung. DCA giá»›i thiá»‡u má»™t cÆ¡ cháº¿ attention má»›i má»Ÿ rá»™ng cá»­a sá»• ngá»¯ cáº£nh cá»§a LLMs mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n bá»• sung, cho phÃ©p sá»­ dá»¥ng tÃ i nguyÃªn tÃ­nh toÃ¡n hiá»‡u quáº£ hÆ¡n.

Dynamic NTK 13B DCA trá»±c giao vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p ngoáº¡i suy phá»• biáº¿n nhÆ° PI vÃ  NTK-Aware RoPE, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p vá»›i cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh dÃ i hiá»‡n táº¡i Ä‘á»ƒ má»Ÿ rá»™ng Ä‘Ã¡ng ká»ƒ...

18
