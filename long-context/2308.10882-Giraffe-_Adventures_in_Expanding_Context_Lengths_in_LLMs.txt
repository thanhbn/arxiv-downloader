# 2308.10882.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2308.10882.pdf
# File size: 706198 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Giraffe: Adventures in Expanding Context Lengths in LLMs
Arka Pal∗, Deep Karkhanis, Manley Roberts,
Samuel Dooley, Arvind Sundararajan, Siddartha Naidu
Abacus.AI
Abstract
Modern large language models (LLMs) that rely on attention mechanisms are typically trained with
fixed context lengths which enforce upper limits on the length of input sequences that they can handle
at evaluation time. To use these models on sequences longer than the train-time context length, one
might employ techniques from the growing family of context length extrapolation methods — most of
which focus on modifying the system of positional encodings used in the attention mechanism to indicate
where tokens or activations are located in the input sequence. We conduct a wide survey of existing
methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of
our own design as well — in particular, a new truncation strategy for modifying the basis for the position
encoding.
We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and
LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context
performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover
that linear scaling is the best method for extending context length, and show that further gains can be
achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities
in the truncated basis. To support further research in this area, we release three new 13B parameter
long-context models which we call Giraffe : 4k and 16k context models trained from base LLaMA-13B,
and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our
results.1
1 Introduction
In recent years, transformers [1] have become the dominant neural network architecture in a variety of
natural language modelling tasks [2, 3], by dint of their flexibility and their amenability to being trained
on extremely large datasets [4, 5]. Subsequently, a popular term that has been adopted for these neural
networks is ‘Large Language Models’ (LLMs) — with the ‘Large’ referring both to the training dataset size
as well as their parameter count (and indeed, the associated training and environmental cost).
A key element of the standard transformer architecture is its inherent insensitivity to the ordering of
the input elements. Attention is naturally a set-like operation in which the position of the elements does
not matter [1]. However, the order of elements is crucial for many important tasks such as parsing natural
language, coding, forecasting, etc. Thus it is necessary to inject positional information into the inputs of the
LLM, typically in the form of positional encodings.
One possible desideratum of a positional encoding scheme is context length extrapolation : the ability
to use the LLM for inference on input lengths longer than those it was trained on. Due to the quadratic
complexity growth of the attention mechanism in transformers, it is often infeasible to train on large context
lengths. The benefit of increased context length is diverse - allowing reading longer documents and papers,
∗Correspondence to arka@abacus.ai .
1Github repo at: https://github.com/abacusai/Long-Context .
1arXiv:2308.10882v1  [cs.AI]  21 Aug 2023

--- PAGE 2 ---
more internal consistency in long conversations with users in LLM-powered chatbots, working on bigger
codebases, and so on. We can break context length extrapolation down into two main paradigms. First,
there is finetuned extrapolation where a model previously pretrained on shorter contexts is allowed to finetune,
or update model weights based on the longer context length. Additionally, there is zero-shot extrapolation
where a model previously pretrained on short contexts is immediately evaluated on longer context lengths
with the same weights as the shorter context model.
In this paper, we focus primarily on zero-shot extrapolation and make the following key contributions :
Benchmark of different context extrapolation schemes We conduct a survey of methods for context
length extrapolation with a pretrained base model, and try a few of our own inventions as well. In particular,
we present a new truncated basis for position encodings. The focus in this paper on pretrained models is
also different from other work in the literature [6, 7], which tend to instead train from scratch with a
chosen positional encoding scheme. As mentioned above, although LLMs have been successful, training
them is a costly enterprise. Well known closed-source model include GPT-4 [8] and Claude [9]. Recently the
open-source LLaMA [10] has been released by a team at Meta AI, and this was followed by the improved
LLaMA2 [11]. In our view, the resources required to train competitive base models of this nature will remain
constrained to a few large players. Therefore, it is imperative to be able to modify the models as desired for
the end user—ideally, with a fraction of the compute power applied.
Our main findings are that:
•Linear interpolation is the best as a context length extrapolation method.
•All context length extrapolation methods show degradation on task accuracy, even for lengths where
they provide otherwise coherent output (and perplexity scores are still reasonable).
•Further context length increase can be achieved by utilising a higher scale factor at evaluation time
than finetune time, but seemingly only up to a factor of 2x.
Public release of LLM weights and evaluation datasets We release the weights of two new 13B
models trained from base LLaMA with an extended context length of 16k2and a context length of 4k3
on HuggingFace. We also release a 13B model trained to a length of 32k from base LLaMA 24. We call
this family of models Giraffe . In addition, we release three datasets (LongChat-Lines5, FreeFormQA6and
AlteredNumericQA7) to evaluate long context performance of these, and other, models. LongChat-Lines is
a key-value fine-grained retrieval task. FreeFormQA and AlteredQA are question-answering datasets based
on the Natural Questions Dataset [12]. Some existing work [6, 7] focuses only on perplexity on a document
corpus evaluation set as their measure of extrapolation performance. We find that perplexity scores are not
as sensitive a measure of long context performance as our introduced tasks.
2 Related Work
RoPE In this work, we examine the efficacy of the positional encoding choice of LLaMA [10] for context
lengths longer than the base model was trained on. The positional encoding used by LLaMA is RoPE (Rotary
Position Embedding) [13]. RoPE works by rotating slices of the query and key projection matrices at different
speeds. Thus for example even if the query and key are projected to the same encoding, they will be rotated
by different amounts depending on their position in the sequence. If they are subsequently unaligned, their
dot product will be smaller relative to what it would be if they were not rotated at all. Conversely, they
2https://huggingface.co/abacusai/Giraffe-v1-delta-13b-scaled-16
3https://huggingface.co/abacusai/Giraffe-v1-delta-13b-scaled-4
4https://huggingface.co/abacusai/Giraffe-v2-13b-32k
5https://huggingface.co/datasets/abacusai/LongChat-Lines
6https://huggingface.co/datasets/abacusai/WikiQA-Free_Form_QA
7https://huggingface.co/datasets/abacusai/WikiQA-Altered_Numeric_QA
2

--- PAGE 3 ---
could become more aligned, leading to a larger dot product and attention score. In RoPE, this rotation is
happening at different speeds on all 2-slices of the query and key in the embedding dimension, allowing the
model to build a complex function of attention scores over distances. One of the main appeals of utilising
the RoPE method is that it ensures mathematically that the attention score function is dependent only on
therelative distance between a query and a key, rather than their absolute positions. This is considered to
be a desirable property of LLMs [13, 14].
ALiBi Although RoPE was successful in this aim, the work on ALiBi [6] demonstrated that RoPE was not
able to perform zero-shot context length extrapolation. The ALiBi paper showed that RoPE quickly degraded
as it was tested on context lengths longer than the model had seen during training; it also introduced its
own proposed alternative that showed superior extrapolation ability on their benchmarks. However, ALiBi
has its own shortcomings; its use of simple linear functions for modulating the attention scores over distance
means that it cannot represent as complex distance-attention-functions as the Fourier basis of RoPE. In
addition, ALiBi uses a single such function per head, further reducing expressive power. This may explain
why, although ALiBi does extrapolate, models which utilize it have worse performance than RoPE-based
models on benchmarks such as MMLU [2] and the LMSys arena which measures human preferences [15].
xPos Sun et al. [7] examine why RoPE fails to extrapolate successfully and determines that this is due to
the effect of the high frequency components causing residual noise in the attention score even when tokens
are long distances apart. They attempt to address this by adding an exponentially decaying amplitude
term to RoPE. This new method, called xPos, decays these noisy high frequency components faster than
low frequency components. This method shows good results on from-scratch training of LLMs [7], and the
intuition driving it aligns with our own hypotheses on the deficiency of RoPE. However, Sun et al. do not
experiment in our setting of interest: taking a model pretrained with RoPE and seeing if it can be coaxed
(via limited finetuning) to learn the xPos encoding instead. Furthermore, their experiments demonstrate
that Blockwise Causal Attention is necessary for them to achieve extrapolation.
Linear Scaling/Positional Interpolation This simple but effective context length extrapolation tech-
nique was concurrently reported by kaiokendev [16] and by a team at Meta [17]. The method that is used
here is to simply divide the position vector by a scaling factor which fits the input within the context length
of the original model. The intuition of this technique is to utilize the LLM’s interpolation capability, rather
than relying on extrapolation. It is a well known phenomenon that neural networks tend to interpolate
within a range of previously seen values better than they extrapolate outside that range (e.g. [18]). In
the specific case of positional encodings, [17] claim that positional interpolation avoids the risk of massive
numerical explosion in attention values associated with extrapolation. We perform many experiments on
this scheme and variations of it and report the results in this paper.
Randomized Positional Encodings Ruoss et al present this method in [19]. During training they
randomly generate their position vector by drawing N many samples uniformly without replacement from
the range [1, L], where N is the training context length and L is a large value that is greater than the
(assumed to be known prior) maximum evaluation context length. These sampled positions are then sorted
in increasing order and act as the position inputs that the model sees at evaluation time. During evaluation,
the position inputs [1, ..., M] are given to the model. The authors claim improved performance on context
length extrapolation. We independently arrived at a similar idea to this paper but instead randomized by
drawing from sub-integer positions approximately in the range [1, N]; see Section 4 for further details. We
also note that Ruoss et al investigate the use of such a scheme for training LLMs from scratch, whereas
we are primarily interested in post-hoc finetuning a pretrained LLM with randomization to enable context
length extrapolation.
3

--- PAGE 4 ---
3 Assessing Long Context Extrapolation
The main question posed in this paper revolves around extending the context length capacity of LLMs. To
evaluate this, a commononly used metric in the literature is perplexity [6, 7, 13]. However, as we show in
Section 5.3, perplexity is somewhat coarse-grained for evaluating how well the model can use longer context
windows. Our intuition is that — in many natural language datasets — a reasonable perplexity score can
be achieved even if the model is only attending to information in a limited range (the final 512 tokens, say)
of the context window. For example, a positional encoding scheme which simply masks out any elements
of the context (and inner key and query activations in the attention heads) that are greater in length than
what the model was trained on should succeed in achieving a reasonable perplexity score, but would fare
poorly on the tasks we describe below.
We expand upon existing work to look at the accuracy of a model when presented with problems which
have verifiable answers. In using this metric, we can evaluate how the model is using the additional contextual
information in order to respond to prompts. We rely on two types of evaluation tasks to assess models’ ability
to extract and use information from long input contexts: the first is key-value retrieval tasks and the other
is question answering tasks. By using these two types of tasks, we enforce the requirement of the model to
attend to the full context in order to obtain high accuracies. We consider the retrieval task to be a more
pure test of information retrieval free of many natural language biases. However, the retrieval task is a
somewhat artificial construct which the LLM will likely not have seen during training, so we also include
question answering to replicate more real-world tasks.
LongChat-Lines We start with a synthetic fine-grained key-value retrieval task first proposed in [20] and
also used by [21]. While these works are excellent given the standard contexts of LLMs, they lack the longer
context lengths that are needed to evaluate our experiments. Thus, we utilize the same task as [20, 21], but
generate additional samples of longer context lengths. This task gives the model a prompt with lines of the
form:
•line grotesque-classmate: REGISTER CONTENT is <42527 >
•line imperfect-bull: REGISTER CONTENT is <3119 >
•line supreme-inversion: REGISTER CONTENT is <13960 >
•...
The model is asked to memorize the value corresponding to the REGISTER CONTENT for each line and
is asked at the end to retrieve the value for a specific line. By varying the number of lines in the prompt,
we can control the context length. We release longer length versions of this task than in [20], and we also
release the generation script for this task.
WikiQA We also create two new datasets from the Natural Questions [12] dataset with longer context
evaluations specifically in mind which we collectively term WikiQA. In this evaluation, the prompt given
to the LLM is in the format of a Wikipedia document followed by a question pertaining to that document;
the model is asked to answer the question. We ensure the answer to the question is a short answer which is
either a single word or a small sentence that has an exact string match in the document given to the LLM
as input. We call this task Free Form QA (FFQA) .
A potential issue in a Wikipedia based dataset however is that the model could perhaps correctly answer
from its pretrained corpus and not specifically using the information in the context. To resolve this, we have
created another “altered” dataset, which we call Altered Numeric QA (AltQA) . This dataset consists
only of questions which have numerical answers. Here, we change the answer and every occurrence of the
answer in the document to a different number, thus ensuring that the LLM must attend to the context, and
only the context, in order to give a correct answer. The modification is made as follows:
4

--- PAGE 5 ---
... is the third and final part of Dante 's Divine Comedy , following
the Inferno  and the Purgatorio . It is an allegory telling of Dante 's
journey through Heaven , guided by Beatrice  , who symbolises
theology . In the poem , Paradise is depicted as a series of
concentric spheres surrounding the Earth , consisting of the Moon ,
Mercury , Venus , the Sun , Mars , Jupiter , Saturn , the Fixed
Stars , the Primum Mobile and finally , the Empyrean . It was
written in the early 14th century ...
Question: 
Who serv es as dante's guide through par adise
Reference Answer:
BeatriceDocument:FreeForm WIkiQA
... Greece has hosted the Summer Olympic Games on two
occasions , the inaug ural modern Olympics in 1896 and again in
2004 2009  . Both were held in Athens , which along with Paris and
Los Angeles are the cities that have hosted the Olympic Games
twice , with London being the only city to have hosted them three
times . The Greek capital also hosted the 1906 Intercalated Games
, which at the time were considered to be Olympic Games by the
International Olympic Committee ...
Question: 
When w as the last time the olympics were in Greece?
Reference Answer:
2009* 
*and not 2004Document:AlteredNumeric WIkiQAFigure 1: Example QA snippets from our WikiQA dataset.
•If the answer is a year, which is quite frequent, (i.e. it is between 1000-2100), we change it to a different
random value within +/- 10 of the original value. We treat years as a special case so as not to disrupt
the overall coherence of the document by having highly anachronistic date values.
•If the answer is any other number, we change it to a different random number which has the same
number of digits.
Figure 1 highlights examples from our WikiQA dataset. Since the contexts in our application are long,
the location of the answer within the context could play a significant role in the model’s ability to answer
the question. Therefore, we utilize both of the WikiQA tasks to conduct analysis on the performance of
the LLM as both the answer location moves within the document (in the beginning 10%, the last 10%, or
randomly anywhere else), as well as with the question given at the beginning or the end of the prompt — in
a bid to replicate the analyses of [22].
4 Context Length Extrapolation Techniques
We examine several context length extrapolation techniques, including existing approaches (or slight varia-
tions on them) as well as our own newly proposed approaches.
4.1 Existing Context Length Extrapolation Techniques
Several methods exist to adapt RoPE positional encodings to longer context lengths. We evaluated the
following techniques.
Linear Scaling/Positional Interpolation Here, the position vector is divided by a scaling factor. Hence
if the original model was trained on a range of positions [0 ,1, ...,2048], say, then the new model will see instead
[0
x,1
x, ...,2048
x] where xis the scaling factor.
xPos We wanted to examine whether a checkpoint trained with the base model’s RoPE encoding scheme
could be finetuned to the xPos [7] scheme. On top of the programming hurdle of patching the entire attention
module to handle xPos’ unique transformation of keys and queries, the major issue presented by this sort of
adaptation is xPos’ sensitivity to floating point precision. The method relies on scaling the key by numeric
values with large (absolute) exponents; these later cancel in the dot product with the query. For long
contexts, however, the large values can actually exceed the magnitude supported by float16. We chose to
5

--- PAGE 6 ---
work around this by performing the core attention operation in float32 at the cost of a 2X training slow
down.
Randomized Position Encodings Here we randomize the distances between the position values uni-
formly in the range [ ϵ,2] for 0 < ϵ≪1, rather than using the typical [0 ,1, ..., n ] which has fixed intervals
of size 1. The intuition behind this approach is that by showing the model many different intra-position
distances at finetuning time, the model will be able to generalize to any choice of fine-grained positions at
evaluation time, thereby allowing for an effective increase in context length by choosing smaller divisions.
This has some similarity to the procedure described in Ruoss et al. [19]. We set an upper bound of 2 so that
the model will in expectation see a final position of n (as E[X]≈1 for X∼U(ϵ,2)). We also set a positive,
non-zero lower bound of ϵin order to avoid issues with position aliasing due to limited numerical precision.
4.2 Newly Proposed Context Length Extrapolation Techniques
Power Scaling In the original RoPE, the basis that is used is given by:
Θ ={θi= 10000−2(i−1)
d|i∈ {1,2, . . . ,d
2}} (1)
where dis the embedding dimension. We use instead the basis given by:
Θ∗=(
θ∗
i=θi
1−2i
dk
|i∈ {1,2, . . . ,d
2})
(2)
where kis a parameter to be set. By applying this transformation, the high frequency (short distance)
elements of the basis are less affected than the low frequency (long distance) elements, which are made even
lower in frequency – see Figure 2. By doing so, our hope was that the model would have to perform less
complex extrapolation for the low frequencies where it has not seen the full range of the periodic function
during train time, and thereby extrapolate better. A potential issue however is that the model relies on
specific relationships across frequencies that a linear transform preserves but a non-linear transformation
destroys.
Truncated Basis Beginning from Equation 1, we instead use the basis given by applying:
θ∗
i=

θiforθi≥b,
ρfora < θ i< b,
0 for θ∗
i≤a.(3)
Where ρis a fixed value that is relatively small, and aandbare chosen cutoff values. The idea here is that we
wish to preserve the high frequency components of the basis but set the low frequency elements to a constant
value—in this case, 0. By doing so with a judicious choice of cutoff a, the model will have experienced all
values of the basis in the context length used during finetuning (due to the periodic nature of the sine and
cosine functions) and should therefore extrapolate better to larger context lengths for evaluation. However,
the model still needs to be able to distinguish between distances that span the entire context it was trained
on, so we include the ρfixed frequency as well. In summary, we hope that with this basis the model can avoid
the issue of having to learn complicated coefficients in the entire RoPE basis by instead learning smooth
functions at longer distances (as demonstrated in the paper [17]).
In Figure 2, we visually compare the frequencies produced by the standard RoPE basis, power scaling,
and truncation.
6

--- PAGE 7 ---
Figure 2: Comparison of the standard RoPE basis vs the power basis and the truncated basis. The x-axis
spans over the embedding dimension, and the y-axis is the frequency value of the sine-cosine basis.
5 Results & Discussion
In the following experiments, we finetuned a base LLaMA-13B model on a portion of the RedPajama dataset
[5] which has been modified so that each data sample has a size of exactly 4096 tokens. We trained with
each positional encoding approach until the evaluation loss roughly plateaued. Loss curves can be found in
Appendix B.
We then further applied instruction finetuning (IFT) with the Vicuna dataset [23] and using LoRA [24]
on the base model. However, we discovered that although IFT did boost accuracies on LongChat-Lines, it
did not significantly change the range of contexts that the base model was able to deal with (see Figure 5 in
Appendix C). This we found to be a marked contrast with the WikiQA variants; there, IFT was necessary
for the model to produce any meaningful results at all. Hence for LongChat-Lines, we used non-IFT models;
for WikiQA, we performed evaluation on a subset of the more promising models with additional IFT.
5.1 Finetuned Context Length Extrapolation
LongChat-Lines We conducted evaluations on LongChat-Lines with the techniques described in Section
4 and report the results in Table 1. We expected all models to be able to perform with a non-zero accuracy
until at least 4200 given that the model is finetuned on context lengths of 4096 and convergence is achieved
in all cases. However, this turned out not to be the case for xPos, which was not able to perform the task
at all. We suspect this may be because xPos is too different from the RoPE basis for the model to be able
to adapt in finetuning; as we see in Appendix B, the training and evaluation loss for xPos was not able to
reach the same values as the other methods. This may also be a product of the numerical precision issues
that are encountered in the implementation of xPos.
Linear scaling is able to achieve successful context length extrapolation. It is worth mentioning here that
we would expect scaling with a factor of xto achieve non-zero accuracies up to 2048 ·x, due to the base
model being trained on a context length of 2048. Although this is observed with linear scaling with a factor
of 4, we see in Table 1 much quicker degradation as context length increases with a scaling factor of 16. By
context length 17 500 it is already recording 0% accuracy even though we naively would expect reasonable
7

--- PAGE 8 ---
Context
LengthLinear
Scaling
(Factor=4)Linear
Scaling
(Factor=16)Power basis Truncated
basisRandomized
positionxPos
2500 0.7 0.64 0.96 0.42 0.54 0
3600 0.64 0.42 0.64 0.26 0.54 0
4200 0.56 0.56 0.1 0.18 0.3 0
4800 0.66 0.62 0 0.14 0.14 0
7100 0.36 0.4 0 0.04 0.16 0
9400 0 0.22 0 0 0 0
11800 0 0.14 0 0 0 0
14000 0 0.12 0 0 0 0
16000 0 0.1 0 0 0 0
17500 0 0 0 0 0 0
20000 0 0 0 0 0 0
22000 0 0 0 0 0 0
Table 1: After finetuning LLaMA-13B with a base context length of 4096, this table represents evaluations
with different context extrapolation methods on LongChat-Lines. An accuracy of 1.0 would indicate perfect
performance and 0.0 indicates getting every evaluation wrong. The power basis uses a parameter of k= 0.5.
The truncated basis uses the following parameters: a=1
82π
2048, b=2π
2048, ρ=1
162π
2048. Randomization uses a
lower bound parameter of ϵ=1
16. Evaluations are all performed without additional instruction finetuning.
performance up to roughly 32 000 context length. We believe that this indicates that there are limits to the
interpolation methodology and are interested in examining this further in future work.
The power basis, although it performs best at the shortest context, also decays fastest and is unable to
show any extrapolation performance beyond 4200 context at all.
The randomized position approach may appear to be extrapolating based on the results in the table.
However, this is likely due to how we evaluated the model. At train time, the model samples distances
uniformly in [ ϵ,2] as described in Section 4. At evaluation time, it is unclear a priori what the best choice
of positions is. We tried a range of different approaches: fixed distances of size 1, uniform random in [ ϵ,2]
and uniform random in [ ϵ,1]. We found best results for extrapolation with the latter, so we report this. We
hoped that by reducing the upper bound further, we could coax the desired context length extrapolation
from the model. However, going to [ ϵ,0.5] and below significantly degraded the performance of the model.
Our conclusion from this is that the model cannot independently learn to represent each position without
knowing the other positions as well. An interesting avenue for future work would be to condition the Q-proj
and K-proj matrices on the sampled positions during training (and evaluation).
The truncated basis does seem to offer true context length extrapolation, as it is able to achieve non-zero
accuracies on context lengths outside any values it has seen before. Although the performance does degrade
as the length increases and the current manifestation of this is inferior in performance to linear scaling,
we believe that this may be a direction of investigation that can lead to better extrapolation performance.
Truncation can also be combined with linear scaling, as we discuss in Section 5.2.
WikiQA Variants We further conducted evaluations of the linear scaling and truncated basis approaches
on the WikiQA variants described in Section 4. Unlike the retrieval task, we found that models were unable
to perform this task successfully without any instruction finetuning, so we performed this analysis on only
a few approaches of interest. The results are shown in Tables 2 and 3. They largely match the pattern seen
in LongChat-Lines—linear scaling with scale factor 4 is able to perform the task up to 7500 context but
not beyond, whilst scale factor 16 is able to surpass this cutoff but with a slope-off in accuracy. As with
LongChat-Lines, we see that the models appear to show some degradation of accuracy as context length
increases. We see again that the truncated basis is able to extrapolate successfully to about the same context
length as in LongChat-Lines with comparable accuracies to linear scaling with scale 4, but again seemingly
cannot go further than a context length of about 8k.
8

--- PAGE 9 ---
Context Length Linear scaling (Factor=4) Linear scaling
(Factor=16)Truncated basis
2000 0.72 0.69 0.74
3800 0.72 0.73 0.73
7500 0.62 0.70 0.46
15000 0.00 0.68 0.00
24000 0.00 0.56 0.00
32000 0.00 0.18 0.00
Table 2: Accuracy on the AlteredNumeric WikiQA variant of models finetuned with different context ex-
trapolation methods. Evaluations are all performed with instruction finetuning.
Context Length Linear scaling (Factor=4) Linear scaling
(Factor=16)Truncated basis
1900 0.44 0.47 0.46
3800 0.49 0.44 0.55
7600 0.46 0.45 0.36
15000 0.00 0.48 0.00
24000 0.00 0.42 0.00
32000 0.00 0.20 0.00
Table 3: Accuracy on the FreeForm WikiQA variant of models finetuned with different context extrapolation
methods. Evaluations are all performed with instruction finetuning.
5.2 Zero-Shot Linear Scaling
In the previous section, we examined the performance of linear scaling using the same value at finetuning
time as at evaluation time. In this section, we instead investigate the effect of using a different scaling factor
at evaluation time than what the model was trained on. In Table 4, we show results for this strategy as
evaluated on LongChat-Lines. We find in general that if the model is trained with a scale factor of x, then
the model can successfully evaluate zero-shot with a scale factor of 2 x(with some reduction of accuracy
within the range of context lengths the model could previously handle). It also appears that at a scale factor
of 16, the model is no longer able to increase its effective context length by using this approach. We also
find that evaluating with >2xleads to the model breaking and being unable to perform the task.
We show that zero-shot linear scaling can be applied successfully after finetuning with the truncated basis.
Interestingly, whilst for linear scaling using a longer scale factor at evaluation time results in a deterioration
of the accuracy on context lengths the model could previously handle, this does not appear to be the case
for the truncated basis—instead, the range of context lengths that the model achieves non-zero accuracy on
increases, and accuracy improves among context lengths the model was finetuned on.
5.3 Comparing Perplexity To Tasks
In Section 3, we introduced two tasks which specifically require a long-context LLM to extract answers from
throughout the entire text, arguing that these tasks may assess long context performance better than raw
perplexity. To analyze how perplexity fares as compared to these tasks, we report perplexities on a held
out set of the RedPajama dataset for a subset of our trained models (see Table 5). Perplexity scores do
show a large increase when a context length is reached that the model is completely unable to deal with (for
example, beyond 2k on the base LLaMA model, or beyond 8k on the linear scale 4 model). However, they
are appear less effective for showing the decrease in long-context capability within that effective range. In
particular, while we observe a steep slope-off in performance on LongChat-Lines and the WikiQA variants as
the context length increases for the linear scale 4 and truncated basis columns of Table 1), this degradation
is not strongly reflected in the perplexity scores at those contexts. However, the linear scale 16 model
does appear to have well-correlated perplexity and accuracy on our tasks. Perhaps most tellingly, we see
9

--- PAGE 10 ---
Train Scaling = 1 (base model) Train Scaling = 4 Train Scaling = 16 Truncated (Scaling = 1)
Context Length Eval 1 Eval 2 Eval 4 Eval 4 Eval 8 Eval 16 Eval 16 Eval 32 Eval 64 Eval 1 Eval 2 Eval 4
2500 0 0.32 0 0.88 0.64 0 0.64 0.24 0 0.42 0.58 0
3600 0 0.3 0 0.8 0.58 0 0.42 0.26 0 0.26 0.44 0
4200 0 0.18 0 0.86 0.62 0 0.56 0.12 0 0.18 0.28 0
4800 0 0 0 0.86 0.62 0 0.62 0.22 0 0.14 0.26 0
7100 0 0 0 0.64 0.38 0 0.4 0.12 0 0.04 0.04 0
9400 0 0 0 0 0.32 0 0.22 0.12 0 0 0.04 0
11800 0 0 0 0 0.30 0 0.14 0.1 0 0 0.04 0
14000 0 0 0 0 0.1 0 0.12 0.04 0 0 0 0
16000 0 0 0 0 0.12 0 0.1 0.02 0 0 0 0
17500 0 0 0 0 0 0 0 0 0 0 0 0
20000 0 0 0 0 0 0 0 0 0 0 0 0
22000 0 0 0 0 0 0 0 0 0 0 0 0
Table 4: Accuracy on LongChat-Lines of models finetuned with different context extrapolation methods
and then evaluated with additional linear scaling. Whenever the evaluation linear scale is greater than
the training linear scale, this produces zero-shot context length extrapolation. In general, increasing the
evaluation context length 2x over train actually does double the usable context length, at an accuracy cost,
for train lengths 1 and 4 (for train length 16, it does not). The truncated basis accuracy improves with 2x
scaling. A more aggressive scale up of 4x times the train length leads to apparent model failure.
the shortcoming of perplexity for between-model comparisons. According to Table 5, the truncated basis
performs best at 8k and below; however, in Tables 1, 2 and 3 truncated is significantly lower in performance
compared to the linear scaled models at 8k context.
Perplexity is commonly used in the literature to measure long context performance [13, 7], but we believe
these results show it is not in itself a sufficient measure of long context performance, but is best utilised with
other tasks which additionally probe the capabilities of the LLM.
Context Length LLaMA Base Linear Scaling (Factor=4) Linear Scaling (Factor=16) Truncated Basis
512 4.06 4.06 4.05 3.79
1k 3.88 3.87 3.86 3.63
2k 3.79 3.75 3.74 3.52
4k 9022 3.66 3.66 3.46
8k 7198 3.79 3.97 3.78
16k 5141 14902 5.43 15793
24k 4980 21236 8.73 13929
32k 4408 55480 98.12 12534
Table 5: Perplexity scores on a held out evaluation set of the RedPajama dataset at various context lengths
on different models. The evaluation length is 256 tokens, and the prompt given to the models is the previous
N - 256 tokens of the document, where N is the context length we evaluated. When the context length is too
long for the model to handle effectively, the perplexity does blow up; however, within ranges the model can
handle, perplexity appears to be less sensitive to context-usage degradation than the LongChat-Lines task,
and does not follow the same between-model ranking as that of LongChat-Lines or WikiQA.
5.4 Analysis of question and answer positioning
For the WikiQA variants, we performed a stratified analysis of the effect of the position of the answer and
the question. As described in Section 4, we looked at the impact of placing the answer within the first 10%
of the document, the last 10%, or elsewhere randomly. We also examined the effect of putting the question
at the beginning or the end of the prompt. The results are shown in Tables 6 and 7, performed on the model
with linear scaling with a factor of 16, with additional instruction finetuning.
10

--- PAGE 11 ---
Context Length Answer Location Question Location
Start Middle End Start End
2000 0.74 0.68 0.66 0.70 0.69
3800 0.70 0.67 0.66 0.63 0.73
7500 0.69 0.63 0.65 0.61 0.70
15000 0.68 0.68 0.69 0.68 0.68
24000 0.30 0.26 0.50 0.14 0.56
32000 0.13 0.13 0.23 0.15 0.18
Table 6: Stratified accuracy analysis on the AlteredNumericQA task. For answers, “Start” refers to the first
10% of the document, “End” to the last 10%, and “Middle” to any other location. For questions, “Start”
refers to placing the question before the rest of the language prompt, and “End” refers to placing the question
at the end. Results are reported on LLaMA-13B finetuned with a linear scale of 16, with IFT applied.
Context Length Answer Location Question Location
Start Middle End Start End
1900 0.37 0.40 0.36 0.27 0.47
3800 0.40 0.30 0.32 0.24 0.44
7600 0.35 0.34 0.35 0.24 0.45
15000 0.44 0.43 0.45 0.40 0.48
24000 0.18 0.20 0.40 0.10 0.42
32000 0.07 0.11 0.18 0.04 0.20
Table 7: Stratified accuracy analysis on the FreeFormQA task. For answers, “Start” refers to the first 10%
of the document, “End” to the last 10%, and “Middle” to any other location. For questions, “Start” refers
to placing the question before the rest of the language prompt, and “End” refers to placing the question at
the end. Results are reported on LLaMA-13B finetuned with a linear scale of 16, with IFT applied.
We aimed to build on a similar analysis from [22]. However, we were not able to replicate the results shown
in that paper on the LongChat-13B (16K) model (to which our modeling approach is most comparable). On
both FreeFormQA and AlteredNumericQA, we observed no clear trend with regards to the location of the
answer within the prompt and the model’s accuracy up to 15k context length. There also did not appear to be
a significant impact on the location of the question for AlteredNumericQA, but there is a noticeable impact
observed for FreeFormQA where having the question at the end appears to have a significant improvement
in accuracy. However, at 24k and 32k context lengths we see a clear indication in both datasets for both the
answer at the end and question at the end returning superior accuracy to their placements elsewhere. These
results are a marked contrast to those in [22]. Our take away from this is that there is plausibly a great
deal of task-conditional variability in the performance of LLMs with regards to how well they can utilize all
portions of the context; even small differences in task construction can lead to large differences in observed
trends.
6 Conclusion and Limitations
In this paper we examined multiple approaches to finetuning a pretrained base LLaMA and LLaMA2 LLM
that has a limited context length such that it is capable of extrapolating zero-shot to new, longer context
lengths. We compared the methods using perplexity, as well as two custom tasks that probe long context
performance; we find that the custom tasks offer a more fine-grained understanding of long context perfor-
mance than perplexity. We showed that the method of linear interpolation performed best at context length
extrapolation, and found some promise in the potential of using a new basis which we termed the truncated
basis. We release three models which we call Giraffe that extend the context length of the base LLaMA and
LLaMA 2 models using the method of linear interpolation.
There is significant room for building on the work presented in this paper. We note that all methods show
11

--- PAGE 12 ---
a degradation in accuracy on our evaluation tasks as context length increases, even though perplexity often
remains reasonable and the model can still produce coherent outputs. This is a shortcoming that would be
of interest to address, and in our view is necessary for claiming ‘true’ long context extrapolation ability of a
model.
The limitations of this work are that we only conducted our perplexity analysis on a single document
dataset. Future work could look to replicate this analysis on other datasets. Additionally, we focused
specifically on context-length extrapolation from a pretrained base model, and in particular the LLaMA
and LLaMA 2 models trained with RoPE positional encodings. Future work could investigate whether the
analysis herein extends to other positional encoding types and models. Future work could also address
the limitations of the linear interpolation method. We see some evidence on the LongChat-Lines task in
particular of accuracy degradation as the scale factor is increased. What is the limit of the size of scale
factor of this method? Is there a point beyond which it simply does not improve the range of contexts it
can handle? Furthermore, can the truncated basis approach which seems to show signs of true extrapolation
capability be modified in a manner to gain parity with or surpass the linear interpolation method? We
believe these are some potential future directions of interest.
References
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[2] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding, 2021.
[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,
Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,
Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
Wojciech Zaremba. Evaluating large language models trained on code, 2021.
[4] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb
dataset of diverse text for language modeling, 2020.
[5] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April
2023.
[6] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation, 2022.
[7] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary,
Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022.
[8] OpenAI. Gpt-4 technical report, 2023.
[9] Anthropic. Introducing claude, 2023.
[10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee
Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
12

--- PAGE 13 ---
Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models,
2023.
[11] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-
ton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,
Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu,
Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang
Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan
Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open
foundation and fine-tuned chat models, 2023.
[12] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-
tions: a benchmark for question answering research. Transactions of the Association of Computational
Linguistics , 2019.
[13] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding, 2022.
[14] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations,
2018.
[15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging
llm-as-a-judge with mt-bench and chatbot arena, 2023.
[16] Things i’m learning while training superhot, 2023.
[17] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of
large language models via positional interpolation, 2023.
[18] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken ichi Kawarabayashi, and Stefanie Jegelka.
How neural networks extrapolate: From feedforward to graph neural networks, 2021.
[19] Anian Ruoss, Gr´ egoire Del´ etang, Tim Genewein, Jordi Grau-Moya, R´ obert Csord´ as, Mehdi Bennani,
Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of trans-
formers, 2023.
[20] anadim. A little retrieval test for large language models, May 2023.
[21] Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,
Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June
2023.
[22] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and
Percy Liang. Lost in the middle: How language models use long contexts, 2023.
[23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging
llm-as-a-judge with mt-bench and chatbot arena, 2023.
13

--- PAGE 14 ---
[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
14

--- PAGE 15 ---
Context Length LLaMA 2 Linear (8)
2500 0.48
3600 0.42
4200 0.32
4800 0.74
7100 0.56
9400 0.50
11800 0.50
14000 0.42
16000 0.38
17500 0.14
20000 0.14
22000 0.08
26000 0.00
30000 0.00
Table 8: LLaMA 2 performance on LongChat-Lines with a scale factor of 8.
Context Length LLaMA 2 Linear (8)
AltQA FFQA
2k 0.72 0.56
4k 0.76 0.55
8k 0.71 0.56
16k 0.59 0.44
24k 0.36 0.28
32k 0.15 0.10
Table 9: LLaMA 2 performance on the WikiQA variants with a scale factor of 8.
A LLaMA 2
As we were finalising this paper, Meta released LLaMA 2 [11]. We verified that similar results of context
length extrapolation were achievable with LLaMA 2 by the linear interpolation method. We applied the
same method as described in Section 4, training LLaMA 2-13B on a portion of the RedPajama dataset
modified such that each data sample has a size of exactly 4096 tokens. We then also applied instruction
finetuning with the Vicuna dataset. We used a scale of 8. Performance is shown in tables 8 and 9.
We see that the model is able to achieve non-zero accuracies on LongChat-Lines up to a context length
of 22000, further than any of the models we tested in the main paper. The model is also able to achieve
non-zero performance on the WikiQA variants up to 32k context. However, we do see diminishing accuracy
in both tasks as the context length increases. It is also worth noting that the accuracies on both tasks
are slightly lower than LLaMA 1 with scale 16 on the context lengths which both models are capable of
producing non-zero results.
15

--- PAGE 16 ---
B Loss Curves
Figure 3: Training loss curves of models during the initial fitting runs on 4096 token samples extracted from
the RedPajama dataset.
Figure 4: Evaluation loss curves of models during the initial fitting runs on 4096 token samples extracted
from the RedPajama dataset.
16

--- PAGE 17 ---
Figure 5: Comparison between IFT and non-IFT on LongChat-Lines with linear scaling of 4 applied. Al-
though IFT improves the accuracies, it does not extend the range of contexts on which the model obtains a
non-zero accuracy.
C Impact of IFT
As mentioned in Section 5 of the main text, we found that instruction-fine-tuning with the Vicuna dataset
did improve accuracies on LongChat-Lines, but did not change the span of non-zero contexts for a given
model. Figure 5 shows this on the model with linear interpolation with scale 4.
17
