# Code Llama: Các Mô hình Nền tảng Mở cho Code

Baptiste Rozière†, Jonas Gehring†, Fabian Gloeckle†,∗, Sten Sootla†, Itai Gat, Xiaoqing
Ellen Tan, Yossi Adi⋄, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron
Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron,
Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve†

Meta AI

## Tóm tắt

Chúng tôi phát hành Code Llama, một họ các mô hình ngôn ngữ lớn cho code dựa trên Llama 2 cung cấp hiệu suất tối ưu trong số các mô hình mở, khả năng infilling, hỗ trợ ngữ cảnh đầu vào lớn, và khả năng làm theo hướng dẫn zero-shot cho các nhiệm vụ lập trình. Chúng tôi cung cấp nhiều phiên bản để bao phủ một loạt các ứng dụng: các mô hình nền tảng (Code Llama), chuyên biệt hóa Python (Code Llama - Python), và các mô hình làm theo hướng dẫn (Code Llama - Instruct) với 7B, 13B, 34B, và 70B tham số mỗi loại. Các mô hình này được huấn luyện trên các chuỗi 16k tokens và cho thấy cải thiện trên các đầu vào lên đến 100k tokens. Các biến thể Code Llama và Code Llama - Instruct 7B, 13B và 70B hỗ trợ infilling dựa trên nội dung xung quanh. Code Llama đạt hiệu suất tối ưu trong số các mô hình mở trên một số benchmark code, với điểm số lên đến 67% và 65% trên HumanEval và MBPP, tương ứng. Đáng chú ý, Code Llama - Python 7B vượt trội hơn Llama 2 70B trên HumanEval và MBPP, và tất cả các mô hình của chúng tôi đều vượt trội hơn mọi mô hình có sẵn công khai khác trên MultiPL-E. Chúng tôi phát hành Code Llama dưới giấy phép cho phép sử dụng cho cả nghiên cứu và thương mại.¹

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLMs) hỗ trợ một số lượng ứng dụng ngày càng tăng nhanh chóng, đã đạt được trình độ thành thạo trong ngôn ngữ tự nhiên cho phép chúng được điều khiển và nhắc nhở để thực hiện nhiều nhiệm vụ khác nhau (OpenAI, 2023; Touvron et al., 2023b). Bằng cách sử dụng các tập dữ liệu lớn trong lĩnh vực cụ thể, hiệu quả của chúng có thể được cải thiện đáng kể cho các ứng dụng đòi hỏi sự kết hợp của cả ngôn ngữ tự nhiên và ngôn ngữ chuyên biệt cùng với hiểu biết về thuật ngữ chuyên môn. Bằng cách huấn luyện trên các tập dữ liệu chuyên biệt theo lĩnh vực, chúng đã chứng minh hiệu quả rộng rãi hơn trên các ứng dụng đòi hỏi hiểu biết ngôn ngữ tự nhiên tiên tiến. Một trường hợp sử dụng nổi bật là tương tác chính thức với các hệ thống máy tính, chẳng hạn như tổng hợp chương trình từ các đặc tả ngôn ngữ tự nhiên, hoàn thành code, debug, và tạo tài liệu (để biết khảo sát, xem Xu & Zhu, 2022, cũng xem Phần 5).

Trong công trình này, chúng tôi trình bày Code Llama, một họ LLMs cho sinh code và infilling được phát triển từ Llama 2 (Touvron et al., 2023b) và được phát hành dưới cùng giấy phép cho phép tùy chỉnh. Chúng tôi cung cấp code suy luận cho cả mô hình completion và infilling trong repository đi kèm.¹ Phương pháp của chúng tôi dựa trên việc dần dần chuyên biệt hóa và tăng khả năng của các mô hình Llama 2 bằng cách áp dụng một chuỗi các bước huấn luyện và fine-tuning (Hình 2):

• **Huấn luyện code từ các mô hình nền tảng.** Trong khi hầu hết các LLMs cho sinh code như AlphaCode (Li et al., 2022), InCoder (Fried et al., 2023) hoặc StarCoder (Li et al., 2023) được huấn luyện chỉ trên code, Codex (Chen et al., 2021) được fine-tuned từ một mô hình ngôn ngữ tổng quát. Chúng tôi cũng bắt đầu từ một mô hình nền tảng (Llama 2, Touvron et al., 2023b) được pretrained trên dữ liệu văn bản và code đa mục đích. So sánh của chúng tôi (Phần 3.4.1) cho thấy việc khởi tạo mô hình của chúng tôi với Llama 2 vượt trội hơn cùng kiến trúc được huấn luyện chỉ trên code cho một ngân sách nhất định.

¹https://github.com/facebookresearch/codellama
†: Những người đóng góp chính. ∗: Meta AI, CERMICS École des Ponts ParisTech. ⋄: Meta AI & Hebrew University of Jerusalem

---

**Nhắc nhở:** Trong Bash, làm thế nào để tôi liệt kê tất cả các file văn bản trong thư mục hiện tại (loại trừ các thư mục con) đã được sửa đổi trong tháng qua

**Phản hồi:** Bạn có thể sử dụng lệnh 'find' trong Bash để liệt kê tất cả các file văn bản trong thư mục hiện tại (loại trừ các thư mục con) đã được sửa đổi trong tháng qua. Lệnh sẽ là:

find . -maxdepth 1 -type f -mtime -30 -name "*.txt"

Đây là phân tích của lệnh:
* '.' chỉ định thư mục hiện tại.
* '-maxdepth 1' yêu cầu 'find' chỉ tìm kiếm thư mục hiện tại và không tìm kiếm bất kỳ thư mục con nào.
* '-type f' yêu cầu 'find' chỉ tìm kiếm các file (không phải thư mục hoặc các loại file khác).
* '-mtime -30' yêu cầu 'find' chỉ tìm kiếm các file đã được sửa đổi trong 30 ngày qua.
* '-name "*.txt"' yêu cầu 'find' chỉ tìm kiếm các file có phần mở rộng '.txt'.

Lưu ý rằng tùy chọn '-mtime' sử dụng số ngày kể từ khi file được sửa đổi lần cuối, vậy '-mtime -30' có nghĩa là "được sửa đổi trong 30 ngày qua". Nếu bạn muốn tìm kiếm các file được sửa đổi trong tháng qua, bạn có thể sử dụng tùy chọn '-mtime -30'.

**Hình 1:** Ví dụ về phản hồi của Code Llama - Instruct (34B) khi được hỏi về một lệnh shell cụ thể.

• **Infilling.** Huấn luyện và fine-tuning autoregressive của LLMs phù hợp cho completion nhắc nhở, nhưng không cung cấp khả năng điền vào một phần văn bản bị thiếu trong khi tính đến toàn bộ ngữ cảnh xung quanh. Huấn luyện code của chúng tôi cho các mô hình Code Llama 7B, 13B và 70B có một mục tiêu đa nhiệm vụ (Fried et al., 2023) bao gồm cả dự đoán autoregressive và causal infilling, cho phép các ứng dụng như completion thời gian thực trong các trình soạn thảo code nguồn hoặc sinh docstring.

• **Ngữ cảnh đầu vào dài.** Mở khóa lý luận cấp repository cho completion hoặc synthesis – thay vì cấp function hoặc cấp file – đòi hỏi việc nhắc nhở mô hình với ngữ cảnh dài hơn nhiều so với 4,096 tokens được hỗ trợ bởi Llama 2. Chúng tôi đề xuất một giai đoạn fine-tuning bổ sung mở rộng độ dài ngữ cảnh tối đa từ 4,096 tokens lên 100,000 tokens bằng cách sửa đổi các tham số của RoPE positional embeddings (Su et al., 2021) được sử dụng trong Llama 2. Các thí nghiệm của chúng tôi cho thấy Code Llama hoạt động trên các ngữ cảnh rất lớn với tác động vừa phải đến hiệu suất trên các benchmark coding tiêu chuẩn (Phần 3.3).

• **Fine-tuning theo hướng dẫn.** Đối với người dùng cuối, tính hữu ích của LLMs được cải thiện đáng kể bởi instruction fine-tuning (Ouyang et al., 2022; Wei et al., 2022; OpenAI, 2023; Touvron et al., 2023b), điều này cũng giúp ngăn chặn các sinh ra không an toàn, độc hại hoặc thiên vị. Các biến thể Code Llama - Instruct được fine-tuned thêm trên một hỗn hợp dữ liệu hướng dẫn độc quyền để cải thiện an toàn và hữu ích, và một tập dữ liệu self-instruct được tạo bằng máy mới được tạo bằng cách nhắc nhở Llama 2 cho các vấn đề coding và Code Llama để tạo các unit test và giải pháp liên quan. Kết quả của chúng tôi cho thấy Code Llama - Instruct cải thiện đáng kể hiệu suất trên các benchmark về tính trung thực, độc tính và thiên vị khác nhau với chi phí vừa phải về hiệu suất sinh code (Phần 4).

Các kết hợp khác nhau của những phương pháp này dẫn đến một họ các mô hình Llama 2 chuyên biệt hóa code với ba biến thể chính mà chúng tôi phát hành với bốn kích thước (7B, 13B, 34B và 70B tham số):

• **Code Llama:** một mô hình nền tảng cho các nhiệm vụ sinh code,
• **Code Llama - Python:** chuyên biệt hóa cho Python,
• **Code Llama - Instruct:** được fine-tuned với các hướng dẫn của con người và dữ liệu tổng hợp code self-instruct.

Một ví dụ về việc sử dụng Code Llama - Instruct được đưa ra trong Hình 1. Nó cho thấy mô hình diễn giải ngôn ngữ tự nhiên để xác định các tùy chọn phù hợp cho một chương trình dòng lệnh và cung cấp giải thích cho giải pháp. Chúng tôi cung cấp thêm các ví dụ định tính trong Phụ lục L. Chúng tôi thực hiện đánh giá toàn diện các mô hình của chúng tôi trên các benchmark sinh code chính: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), và APPS (Hendrycks et al., 2021), cũng như phiên bản đa ngôn ngữ của HumanEval (MultiPL-E, Cassano et al., 2023), nơi các mô hình tốt nhất của chúng tôi thiết lập một state of the art mới trong số các LLMs mã nguồn mở. Các chi tiết kỹ thuật của quy trình huấn luyện và fine-tuning của chúng tôi được cung cấp trong Phần 2, tiếp theo là các thí nghiệm chuyên sâu và nghiên cứu ablation, chi tiết về đánh giá an toàn/hữu ích và thảo luận về công trình liên quan.

---

**Huấn luyện code → Huấn luyện code infilling → Fine-tuning ngữ cảnh dài → Fine-tuning theo hướng dẫn**
**Huấn luyện code Python → Fine-tuning ngữ cảnh dài**

**Code Llama - Instruct (7B ⁶, 13B ⁶, 34B)**
**Code Llama (7B ⁶, 13B ⁶, 34B)**  
**Code Llama - Python (7B, 13B, 34B)**
**Llama 2 Foundation models (7B, 13B, 34B)**

500B tokens, 20B tokens, 100B tokens, 5B tokens

**Hình 2:** Pipeline chuyên biệt hóa Code Llama. Các giai đoạn fine-tuning khác nhau được chú thích với số lượng tokens nhìn thấy trong quá trình huấn luyện. Các mô hình có khả năng infilling được đánh dấu bằng ký hiệu ⇄.

## 2 Code Llama: Chuyên biệt hóa Llama 2 cho code

### 2.1 Họ mô hình Code Llama

**Code Llama.** Các mô hình Code Llama tạo thành các mô hình nền tảng cho sinh code. Chúng có bốn kích thước mô hình: 7B, 13B, 34B và 70B tham số. Các mô hình 7B, 13B và 70B được huấn luyện sử dụng mục tiêu infilling (Phần 2.3), và phù hợp để được sử dụng trong IDE để hoàn thành code ở giữa file, ví dụ. Mô hình 34B được huấn luyện mà không có mục tiêu infilling. Tất cả các mô hình Code Llama được khởi tạo với trọng số mô hình Llama 2 và huấn luyện trên 500B tokens từ một tập dữ liệu nặng về code (xem Phần 2.2 để biết thêm chi tiết), ngoại trừ Code Llama 70B được huấn luyện trên 1T tokens. Tất cả đều được fine-tuned để xử lý ngữ cảnh dài như được chi tiết trong Phần 2.4.

**Code Llama - Python.** Các mô hình Code Llama - Python được chuyên biệt hóa cho sinh code Python và có kích thước 7B, 13B, 34B và 70B tham số. Chúng được thiết kế để nghiên cứu hiệu suất của các mô hình được điều chỉnh cho một ngôn ngữ lập trình duy nhất, so với các mô hình sinh code đa mục đích. Được khởi tạo từ các mô hình Llama 2 và huấn luyện trên 500B tokens từ tập dữ liệu Code Llama, các mô hình Code Llama - Python được chuyên biệt hóa thêm trên 100B tokens sử dụng một tập dữ liệu nặng về Python (Phần 2.2). Code Llama - Python với 7B, 13B và 34B tham số được huấn luyện mà không có infilling và sau đó được fine-tuned để xử lý ngữ cảnh dài (Phần 2.4).

**Code Llama - Instruct.** Đối với kích thước 7B, 13B và 34B, các mô hình Code Llama - Instruct dựa trên Code Llama và được fine-tuned với khoảng 5B tokens bổ sung để làm theo hướng dẫn của con người tốt hơn. Thêm chi tiết về Code Llama - Instruct có thể được tìm thấy trong Phần 2.5.

**Code Llama 70B.** Code Llama 70B được huấn luyện sau các mô hình Code Llama 7B, 13B và 34B vài tháng. Nó được huấn luyện sử dụng cùng dữ liệu như các phiên bản nhỏ hơn của Code Llama, và sử dụng gần như cùng phương pháp. Code Llama 70B được huấn luyện trên gấp đôi số lượng tokens: 1 trillion thay vì 500 billion. Nó được huấn luyện với FIM, đây là một khả năng thường được yêu cầu cho mô hình 34B. Chỉ có Code Llama 70B cơ bản được huấn luyện với LCFT. Xem Phụ lục B cho pipeline chuyên biệt hóa Code Llama 70B. Code Llama - Instruct 70B được huấn luyện từ Code Llama - Python 70B, vượt trội hơn Code Llama 70B trung bình trên các ngôn ngữ từ MultiPL-E bao gồm python.

### 2.2 Tập dữ liệu

Chúng tôi huấn luyện Code Llama 7B, 13B và 34B trên 500B tokens, và Code Llama 70B trên 1T tokens trong giai đoạn ban đầu, bắt đầu từ các phiên bản 7B, 13B, 34B, và 70B của Llama 2. Như được hiển thị trong Bảng 1, Code Llama được huấn luyện chủ yếu trên một tập dữ liệu gần như đã loại bỏ trùng lặp của code có sẵn công khai. Chúng tôi cũng lấy 8% dữ liệu mẫu từ các tập dữ liệu ngôn ngữ tự nhiên liên quan đến code. Tập dữ liệu này chứa nhiều thảo luận về code và các đoạn code được bao gồm trong các câu hỏi hoặc câu trả lời ngôn ngữ tự nhiên. Để giúp mô hình giữ lại kỹ năng hiểu ngôn ngữ tự nhiên, chúng tôi cũng lấy mẫu một tỷ lệ nhỏ batch từ một tập dữ liệu ngôn ngữ tự nhiên. Dữ liệu được tokenize thông qua byte pair encoding (BPE, Sennrich et al. (2016)), sử dụng cùng tokenizer như Llama và Llama 2. Các thí nghiệm sơ bộ cho thấy việc thêm các batch được lấy mẫu từ tập dữ liệu ngôn ngữ tự nhiên của chúng tôi cải thiện hiệu suất của các mô hình trên MBPP.

---

### 2.3 Infilling

Code infilling là nhiệm vụ dự đoán phần bị thiếu của một chương trình cho trước ngữ cảnh xung quanh. Các ứng dụng bao gồm hoàn thành code tại vị trí con trỏ trong các IDE code, suy luận kiểu và sinh tài liệu trong code (ví dụ: docstrings).

Chúng tôi huấn luyện các mô hình infilling theo khái niệm causal masking (Aghajanyan et al., 2022; Fried et al., 2023), nơi các phần của chuỗi huấn luyện được chuyển đến cuối, và chuỗi được sắp xếp lại được dự đoán theo cách autoregressive. Chúng tôi huấn luyện các mô hình đa mục đích 7B, 13B và 70B với mục tiêu infilling, theo khuyến nghị của Bavarian et al. (2022). Cụ thể hơn, chúng tôi chia các tài liệu huấn luyện ở cấp ký tự thành một prefix, một phần giữa và một suffix với các vị trí chia được lấy mẫu độc lập từ phân phối đều trên độ dài tài liệu. Chúng tôi áp dụng phép biến đổi này với xác suất 0.9 và chỉ cho các tài liệu không bị cắt qua nhiều ngữ cảnh mô hình. Chúng tôi định dạng ngẫu nhiên một nửa số lần chia theo định dạng prefix-suffix-middle (PSM) và nửa còn lại theo định dạng suffix-prefix-middle (SPM) tương thích được mô tả trong Bavarian et al. (2022, App. D). Chúng tôi mở rộng tokenizer của Llama 2 với bốn token đặc biệt đánh dấu đầu prefix, phần giữa hoặc suffix, và cuối của span infilling. Để hạn chế sự dịch chuyển phân phối giữa huấn luyện autoregressive và infilling, chúng tôi loại bỏ khoảng trắng dẫn đầu ngầm định mà các tokenizer SentencePiece thêm vào khi mã hóa phần giữa và suffix (Kudo & Richardson, 2018). Trong định dạng SPM, chúng tôi nối prefix và phần giữa trước khi mã hóa thành tokens. Lưu ý rằng mô hình của chúng tôi không gặp phải các subtokens bị chia trong định dạng SPM trong khi nó có trong định dạng PSM.

Kết quả về tác động của huấn luyện infilling đến các nhiệm vụ sinh xuống dòng và hiệu suất của các mô hình infilling trên các benchmark infilling được báo cáo trong Phần 3.2.

### 2.4 Fine-tuning ngữ cảnh dài

Xử lý hiệu quả các chuỗi dài là một chủ đề nghiên cứu chính trong mô hình hóa ngôn ngữ dựa trên transformer (Vaswani et al., 2017). Các thách thức mô hình hóa cơ bản là extrapolation, tức là hoạt động trên độ dài chuỗi vượt quá những gì nhìn thấy trong thời gian huấn luyện, và độ phức tạp bậc hai của các lần attention pass ưu tiên huấn luyện trên các đầu vào độ dài ngắn đến trung bình.

Đối với Code Llama, chúng tôi đề xuất một giai đoạn long context fine-tuning (LCFT) chuyên dụng trong đó các mô hình được trình bày với các chuỗi 16,384 tokens, tăng từ 4,096 tokens được sử dụng cho Llama 2 và các giai đoạn huấn luyện code ban đầu của chúng tôi. Bằng cách giới hạn thời gian huấn luyện dành cho xử lý các chuỗi dài cho một giai đoạn fine-tuning, chúng tôi có được khả năng tầm xa mà không tăng đáng kể chi phí huấn luyện các mô hình của chúng tôi. Chiến lược của chúng tôi tương tự như fine-tuning bằng position interpolation được đề xuất gần đây (Chen et al., 2023b), và chúng tôi xác nhận tầm quan trọng của việc sửa đổi tần số xoay của rotary position embedding được sử dụng trong các mô hình nền tảng Llama 2 (Su et al., 2021). Tuy nhiên, thay vì downscaling tần số tuyến tính như Chen et al. (2023b), chúng tôi thay đổi chu kỳ cơ bản mà chúng được bắt nguồn. Cụ thể, với rotary embeddings, các vector query và key xn tại vị trí n chịu một phép biến đổi tuyến tính RdΘ,n xn, trong đó RdΘ,n là một ma trận block diagonal với các entry có dạng

(RdΘ,n)i = (cos nθi  -sin nθi)
                (sin nθi   cos nθi)

và d ký hiệu chiều embedding. Tần số xoay được tính là θi = θ^(-2i/d), và chúng tôi tăng chu kỳ cơ bản θ từ 10,000 lên 1,000,000 cho fine-tuning. Sự tăng này cho phép xử lý các chuỗi lớn hơn nhiều và giảm thiên vị đối với attention khoảng cách ngắn (xem Phụ lục G.1 để thảo luận thêm). Các thí nghiệm của chúng tôi xác nhận rằng các mô hình Code Llama không chỉ hiệu quả trong độ dài chuỗi tăng được sử dụng trong fine-tuning, mà còn cho thấy khả năng extrapolation và thể hiện hành vi ổn định trên các chuỗi rất dài lên đến 100,000 tokens (Phần 3.3).

### 2.5 Fine-tuning theo hướng dẫn

Các mô hình được fine-tuned theo hướng dẫn Code Llama - Instruct của chúng tôi dựa trên Code Llama và được huấn luyện để trả lời câu hỏi một cách phù hợp. Chúng được huấn luyện trên ba loại dữ liệu khác nhau.

---

| Tập dữ liệu | Tỷ lệ lấy mẫu | Epochs | Kích thước đĩa |
|-------------|---------------|---------|----------------|
| **Code Llama (500B tokens)** |
| Code | 85% | 2.03 | 859 GB |
| Ngôn ngữ tự nhiên liên quan đến code | 8% | 1.39 | 78 GB |
| Ngôn ngữ tự nhiên | 7% | 0.01 | 3.5 TB |
| **Code Llama - Python (thêm 100B tokens)** |
| Python | 75% | 3.69 | 79 GB |
| Code | 10% | 0.05 | 859 GB |
| Ngôn ngữ tự nhiên liên quan đến code | 10% | 0.35 | 78 GB |
| Ngôn ngữ tự nhiên | 5% | 0.00 | 3.5 TB |

**Bảng 1:** Tập dữ liệu huấn luyện của Code Llama và Code Llama - Python. Chúng tôi huấn luyện Code Llama trên 500B tokens bổ sung và Code Llama - Python thêm trên 100B tokens.

**Tập dữ liệu độc quyền.** Chúng tôi sử dụng tập dữ liệu instruction tuning được thu thập cho Llama 2 và được mô tả chi tiết bởi Touvron et al. (2023b). Cụ thể, chúng tôi sử dụng phiên bản được gọi trong báo cáo của họ là "RLHF V5", được thu thập thông qua nhiều giai đoạn học tăng cường từ phản hồi của con người và chú thích phản hồi của con người (xem Phần 3 của họ để biết thêm chi tiết). Nó kết hợp hàng nghìn ví dụ Supervised Fine-Tuning và hàng triệu ví dụ Rejection Sampling. Mỗi ví dụ bao gồm một cuộc đối thoại nhiều lượt giữa người dùng và trợ lý. Đối với Rejection Sampling, đầu ra được chọn trong số nhiều lần tạo sử dụng một mô hình reward. Tập dữ liệu cuối cùng chứa cả dữ liệu Helpfulness và Safety. Điều này cho phép Code Llama kế thừa các thuộc tính làm theo hướng dẫn và an toàn của Llama 2.

**Self-instruct.** Tập dữ liệu độc quyền của chúng tôi chứa ít ví dụ về các nhiệm vụ liên quan đến code. Thu thập dữ liệu được giám sát từ các người chú thích con người hoặc huấn luyện từ phản hồi của con người (Ouyang et al., 2022) rất tốn kém cho các nhiệm vụ coding vì nó đòi hỏi đầu vào từ các nhà phát triển chuyên nghiệp. Thay vì phản hồi của con người, chúng tôi sử dụng phản hồi thực thi để chọn dữ liệu để huấn luyện mô hình instruct của chúng tôi. Chúng tôi xây dựng tập dữ liệu self-instruction theo công thức dưới đây, dẫn đến ~14,000 bộ ba question-tests-solution:

1. Sinh 62,000 câu hỏi lập trình kiểu phỏng vấn bằng cách nhắc nhở (Hình 10) Llama 2 70B.
2. Loại bỏ trùng lặp tập hợp câu hỏi bằng cách xóa các bản sao chính xác, dẫn đến ~52,000 câu hỏi.
3. Đối với mỗi câu hỏi này:
   (a) Sinh unit tests bằng cách nhắc nhở Code Llama 7B (Hình 11)
   (b) Sinh mười giải pháp Python bằng cách nhắc nhở Code Llama 7B (Hình 12)
   (c) Chạy unit tests trên mười giải pháp. Thêm giải pháp đầu tiên vượt qua tests (cùng với câu hỏi và tests tương ứng) vào tập dữ liệu self-instruct.

Chúng tôi sử dụng Code Llama 7B để sinh tests và giải pháp Python, vì chúng tôi thấy nó hiệu quả hơn việc sinh ít giải pháp hơn mỗi câu hỏi với mô hình 34B cho cùng ngân sách tính toán.

**Rehearsal.** Để ngăn mô hình thoái hóa về khả năng coding tổng quát và hiểu ngôn ngữ, Code Llama - Instruct cũng được huấn luyện với một tỷ lệ nhỏ dữ liệu từ tập dữ liệu code (6%) và tập dữ liệu ngôn ngữ tự nhiên của chúng tôi (2%).

### 2.6 Chi tiết huấn luyện

**Tối ưu hóa.** Optimizer của chúng tôi là AdamW (Loshchilov & Hutter, 2019) với giá trị β1 và β2 là 0.9 và 0.95. Chúng tôi sử dụng lịch trình cosine với 1000 bước warm-up, và đặt learning rate cuối cùng bằng 1/30 của peak learning rate. Chúng tôi sử dụng batch size 4M tokens được trình bày dưới dạng các chuỗi 4,096 tokens mỗi chuỗi. Mặc dù thực hành tiêu chuẩn sử dụng learning rates thấp hơn trong các giai đoạn fine-tuning so với các giai đoạn pre-training,

---

| Mô hình | Kích thước | HumanEval | | | MBPP | | |
|---------|------------|-----------|---|---|------|---|---|
| | | pass@1 | pass@10 | pass@100 | pass@1 | pass@10 | pass@100 |
| code-cushman-001 | 12B | 33.5% | - | - | 45.9% | - | - |
| GPT-3.5 (ChatGPT) | - | 48.1% | - | - | 52.2% | - | - |
| GPT-4 | - | 67.0% | - | - | - | - | - |
| PaLM | 540B | 26.2% | - | - | 36.8% | - | - |
| PaLM-Coder | 540B | 35.9% | - | 88.4% | 47.0% | - | - |
| PaLM 2-S | - | 37.6% | - | 88.4% | 50.0% | - | - |
| StarCoder Base | 15.5B | 30.4% | - | - | 49.0% | - | - |
| StarCoder Python | 15.5B | 33.6% | - | - | 52.7% | - | - |
| StarCoder Prompted | 15.5B | 40.8% | - | - | 49.5% | - | - |
| Llama 2 | 7B | 12.2% | 25.2% | 44.4% | 20.8% | 41.8% | 65.5% |
| | 13B | 20.1% | 34.8% | 61.2% | 27.6% | 48.1% | 69.5% |
| | 34B | 22.6% | 47.0% | 79.5% | 33.8% | 56.9% | 77.6% |
| | 70B | 30.5% | 59.4% | 87.0% | 45.4% | 66.2% | 83.1% |
| Code Llama | 7B | 33.5% | 59.6% | 85.9% | 41.4% | 66.7% | 82.5% |
| | 13B | 36.0% | 69.4% | 89.8% | 47.0% | 71.7% | 87.1% |
| | 34B | 48.8% | 76.8% | 93.0% | 55.0% | 76.2% | 86.6% |
| | 70B | 53.0% | 84.6% | 96.2% | 62.4% | 81.1% | 91.9% |
| Code Llama - Instruct | 7B | 34.8% | 64.3% | 88.1% | 44.4% | 65.4% | 76.8% |
| | 13B | 42.7% | 71.6% | 91.6% | 49.4% | 71.2% | 84.1% |
| | 34B | 41.5% | 77.2% | 93.5% | 57.0% | 74.6% | 85.4% |
| | 70B | 67.8% | 90.3% | 97.3% | 62.2% | 79.6% | 89.2% |
| Unnatural Code Llama | 34B | 62.2% | 85.2% | 95.4% | 61.2% | 76.6% | 86.7% |
| Code Llama - Python | 7B | 38.4% | 70.3% | 90.6% | 47.6% | 70.3% | 84.8% |
| | 13B | 43.3% | 77.4% | 94.1% | 49.0% | 74.0% | 87.6% |
| | 34B | 53.7% | 82.8% | 94.7% | 56.2% | 76.4% | 88.2% |
| | 70B | 57.3% | 89.3% | 98.4% | 65.6% | 81.5% | 91.9% |

**Bảng 2:** Điểm pass@ của Code Llama trên HumanEval và MBPP. Điểm pass@1 của các mô hình được tính với greedy decoding. Điểm pass@10 và pass@100 được tính với nucleus sampling với p=0.95 và temperature 0.8 theo kết quả từ Hình 6. Các mô hình được đánh giá zero-shot trên Human Eval và 3-shot trên MBPP. Các mô hình instruct được huấn luyện để an toàn và aligned từ các mô hình Code Llama cơ bản. Kết quả cho các mô hình khác được cung cấp bởi Li et al. (2023) (code-cushman-001, StarCoder), OpenAI (2023) (GPT-3.5, GPT-4), và Chowdhery et al. (2022); Anil et al. (2023) (PaLM).

chúng tôi đạt được kết quả tốt nhất khi giữ lại learning rate gốc của mô hình cơ bản Llama 2. Chúng tôi mang những phát hiện này đến các mô hình 13B, 34B và 70B, và đặt learning rates của chúng lần lượt là 3e−4, 1.5e−4, và 1.5e−4. Đối với fine-tuning python, chúng tôi đặt initial learning rate là 1e−4. Đối với Code Llama - Instruct, chúng tôi huấn luyện với batch size 524,288 tokens và tổng cộng khoảng 5B tokens.

**Fine-tuning ngữ cảnh dài.** Đối với long context fine-tuning (LCFT), chúng tôi sử dụng learning rate 2e−5, độ dài chuỗi 16,384, và reset tần số RoPE với giá trị cơ bản θ = 106. Batch size được đặt là 2M tokens cho kích thước mô hình 7B và 13B và 1M tokens cho kích thước mô hình 34B, tương ứng. Huấn luyện kéo dài 10,000 gradient steps theo mặc định. Chúng tôi quan sát thấy sự không ổn định trong hiệu suất downstream cho một số cấu hình nhất định, và do đó đặt số gradient steps là 11,000 cho các mô hình 34B và 3,000 cho Code Llama 7B.

## 3 Kết quả

Chúng tôi báo cáo kết quả trên nhiều benchmark khác nhau. Đầu tiên, chúng tôi đánh giá các mô hình của chúng tôi trên các benchmark sinh code từ mô tả phổ biến cho Python: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), và APPS

---

(các cuộc phỏng vấn và thi đấu lập trình, Hendrycks et al., 2021). Thứ hai, chúng tôi đánh giá các mô hình của chúng tôi trên các ngôn ngữ lập trình khác sử dụng MultiPL-E (Cassano et al., 2023), cụ thể là trên C++, Java, PHP, C#, TypeScript (TS), và Bash. Chúng tôi cũng báo cáo kết quả trên benchmark GSM8K (Cobbe et al., 2021), đo lường khả năng lý luận toán học (Phụ lục D).

Tiếp theo, chúng tôi thực hiện một nghiên cứu ablation mở rộng: (i) chúng tôi nghiên cứu tác động của huấn luyện từ đầu hoặc từ một mô hình Llama 2 đã được pretrained trong Phần 3.4.1; (ii) chúng tôi thực hiện ablations cho infilling và các benchmark cụ thể infilling bổ sung trong Phần 3.2; (iii) chúng tôi nghiên cứu tác động của long context fine-tuning trên perplexity, một nhiệm vụ retrieval tổng hợp, và hoàn thành code với các file code nguồn dài (Phần 3.3); và (iv) chúng tôi đánh giá quy trình instruction fine-tuning của chúng tôi, bao gồm huấn luyện self-instruct bằng cách tận dụng các unit test tự sinh trong Phần 3.4.2.

### 3.1 Sinh code

#### 3.1.1 Sinh code Python

Chúng tôi bắt đầu bằng cách báo cáo kết quả cho sinh code Python sử dụng các benchmark HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) và APPS (Hendrycks et al., 2021). Kết quả được tóm tắt trong Bảng 2 và 3. Danh sách đầy đủ kết quả trên HumanEval và MBPP, bao gồm các mô hình có và không có infilling và long context fine-tuning, có thể được tìm thấy trong Bảng 10 ở Phụ lục C. Chúng tôi cung cấp kết quả zero-shot của các mô hình instruction fine-tuned trên APPS trong Bảng 15 với chi tiết đánh giá trong Phụ lục F. Những phát hiện chính của chúng tôi như sau.

**Giá trị của chuyên biệt hóa mô hình.** Chúng tôi quan sát thấy chuyên biệt hóa mô hình mang lại sự thúc đẩy trong khả năng sinh code khi so sánh Llama 2 với Code Llama và Code Llama với Code Llama - Python. Llama 2 được huấn luyện trên 2T tokens, và huấn luyện chỉ trên 500B tokens bổ sung từ một tập dữ liệu nặng về code dẫn đến những cải thiện hiệu suất lớn trên cả HumanEval và MBPP, đến mức Llama 2 70B gần như tương đương với Code Llama 7B trên các benchmark coding Python. Mặc dù Code Llama được huấn luyện trên hơn hai epochs của tập dữ liệu code của chúng tôi, chứa toàn bộ tập dữ liệu Python của chúng tôi, huấn luyện trên 100B tokens bổ sung của một hỗn hợp dữ liệu nặng về Python dẫn đến những cải thiện đáng kể trên các benchmark sinh code Python, từ 4.3% đến 8.3% điểm trong HumanEval pass@1 và từ 1.2% đến 6.4% điểm trong MBPP pass@1. Những cải thiện này nhỏ hơn so với bước huấn luyện code đầu tiên, nhưng vẫn cho phép Code Llama - Python 7B vượt trội hơn ngay cả Code Llama 13B trên MBPP và HumanEval. Đối với benchmark APPS, các prompts ít trực tiếp và phức tạp hơn nhiều so với MBPP và HumanEval. Các mô hình Code Llama - Python của chúng tôi cho thấy hiệu suất giảm nhẹ trên các vấn đề cấp độ giới thiệu và phỏng vấn, nơi hiểu prompt thường thách thức hơn cho một mô hình ngôn ngữ so với việc triển khai giải pháp. Tuy nhiên, Code Llama - Python cho thấy những cải thiện rõ ràng trên các vấn đề cấp độ thi đấu nơi các giải pháp phức tạp hơn. Trong khi các mô hình ngôn ngữ lớn có đủ khả năng để học sinh văn bản về các chủ đề khác nhau, chúng tôi quan sát thấy chuyên biệt hóa mô hình có lợi cho các mô hình từ 7B đến 70B tham số và sau hai epochs đầy đủ trên dữ liệu huấn luyện.

**Scaling của các mô hình chuyên biệt.** Chúng tôi quan sát thấy việc scaling số lượng tham số quan trọng đối với các mô hình chuyên biệt cho coding. Với cùng quy trình huấn luyện, các mô hình lớn hơn của chúng tôi vượt trội hơn các đối tác nhỏ hơn trên hầu hết mọi metric từ HumanEval, MBPP và APPS (Bảng 2, 3). Ví dụ, chúng tôi có được 5.6 điểm phần trăm trên MBPP pass@1 khi scaling Code Llama từ 7B lên 13B tham số, 8 điểm nữa khi scaling lên 34B và 7 khi scaling lên 70B. Chúng tôi có thể giả thuyết rằng chuyên biệt hóa các mô hình lớn hơn cho code sẽ dẫn đến những cải thiện đáng kể hơn nữa trên các nhiệm vụ coding. Hơn nữa, các định luật scaling Chinchilla (Hoffmann et al., 2022) chỉ ra rằng các mô hình lớn hơn sẽ hưởng lợi nhiều hơn từ việc huấn luyện trên nhiều tokens hơn.

#### 3.1.2 Đánh giá đa ngôn ngữ

Tiếp theo, chúng tôi đánh giá các mô hình của chúng tôi trên một tập hợp đa dạng hơn các ngôn ngữ lập trình. Để làm điều đó, chúng tôi sử dụng benchmark MultiPL-E (Cassano et al., 2023). Chúng tôi báo cáo kết quả cho Python, C++, Java, PHP, TypeScript, C#, và Bash trong Bảng 4.

---

| Mô hình | Kích thước | Pass@ | Giới thiệu | Phỏng vấn | Thi đấu |
|---------|------------|--------|------------|-----------|---------|
| GPT-Neo | 2.7B | 1 | 3.9% | 0.6% | 0.0% |
|         |      | 5 | 5.5% | 0.8% | 0.0% |
| Codex   | 12B  | 1 | 4.1% | 0.1% | 0.0% |
|         |      | 5 | 9.7% | 0.5% | 0.1% |
|         |      | 1000 | 25.0% | 3.7% | 3.2% |
| AlphaCode | 1B | 1000 | 17.7% | 5.2% | 7.1% |
| AlphaCode (Filtered 1000) | | 5 | 14.4% | 5.6% | 4.6% |
| AlphaCode (Filtered 10000) | | 5 | 18.2% | 8.2% | 6.7% |
| AlphaCode (Filtered 50000) | | 5 | 20.4% | 9.7% | 7.8% |
| Code Llama | 7B | 5 | 10.8% | 2.0% | 0.8% |
|            |    | 10 | 15.6% | 3.1% | 1.4% |
|            |    | 100 | 33.5% | 9.4% | 7.1% |
|            | 13B | 5 | 23.7% | 5.6% | 2.1% |
|            |     | 10 | 30.2% | 8.1% | 3.4% |
|            |     | 100 | 49.0% | 18.4% | 12.0% |
|            | 34B | 5 | 32.8% | 8.8% | 2.9% |
|            |     | 10 | 39.0% | 12.2% | 4.7% |
|            |     | 100 | 56.3% | 24.3% | 15.4% |
| Code Llama - Python | 7B | 5 | 12.7% | 4.2% | 1.3% |
|                     |    | 10 | 18.5% | 6.3% | 2.2% |
|                     |    | 100 | 38.3% | 14.9% | 9.1% |
|                     | 13B | 5 | 26.3% | 7.1% | 2.8% |
|                     |     | 10 | 32.8% | 10.0% | 4.3% |
|                     |     | 100 | 51.6% | 21.5% | 14.6% |
|                     | 34B | 5 | 28.9% | 7.8% | 3.5% |
|                     |     | 10 | 35.9% | 11.1% | 5.5% |
|                     |     | 100 | 54.9% | 23.9% | 16.8% |
| Code Llama - Instruct | 7B | 5 | 12.9% | 2.1% | 1.1% |
|                       |    | 10 | 17.9% | 3.1% | 2.0% |
|                       |    | 100 | 35.4% | 9.4% | 8.5% |
|                       | 13B | 5 | 24.0% | 6.9% | 2.4% |
|                       |     | 10 | 30.3% | 9.6% | 3.8% |
|                       |     | 100 | 48.7% | 19.6% | 13.1% |
|                       | 34B | 5 | 31.6% | 7.9% | 3.2% |
|                       |     | 10 | 37.8% | 11.1% | 5.1% |
|                       |     | 100 | 55.7% | 22.8% | 16.4% |

**Bảng 3:** Điểm pass@ của Code Llama trên APPS. Chúng tôi liệt kê điểm pass@5, pass@10, và pass@100 two-shot của Code Llama trên APPS. Đối với các mô hình của chúng tôi, chúng tôi sử dụng nucleus sampling với p=0.95 và temperature 0.6. Code Llama không được fine-tuned trên tập huấn luyện của APPS và tất cả kết quả được tính với các dự đoán thô mà không lọc bằng các test cases từ prompt. Số liệu GPT-Neo fine-tuned được báo cáo bởi Hendrycks et al. (2021), kết quả Codex one-shot bởi Chen et al. (2021), và số liệu AlphaCode fine-tuned bởi Li et al. (2022).

Chúng tôi quan sát thấy sự cải thiện tương tự từ Llama 2 sang Code Llama trong setting đa ngôn ngữ như trong đánh giá trên Python (Phần 3.1.1). Các mô hình Code Llama rõ ràng vượt trội hơn các mô hình Llama 2 cùng kích thước trên sinh code trong bất kỳ ngôn ngữ nào, và Code Llama 7B thậm chí vượt trội hơn Llama 2 70B. So với

---

| Mô hình | Kích thước | Multi-lingual Human-Eval |
|---------|------------|--------------------------|
|         |            | C++ | Java | PHP | TS | C# | Bash | Trung bình |
| CodeGen-Multi | 16B | 21.0% | 22.2% | 8.4% | 20.1% | 8.2% | 0.6% | 13.4% |
| CodeGeeX | 13B | 16.9% | 19.1% | 13.5% | 10.1% | 8.5% | 2.8% | 11.8% |
| code-cushman-001 | 12B | 30.6% | 31.9% | 28.9% | 31.3% | 22.1% | 11.7% | 26.1% |
| StarCoder Base | 15.5B | 30.6% | 28.5% | 26.8% | 32.2% | 20.6% | 11.0% | 25.0% |
| StarCoder Python | 15.5B | 31.6% | 30.2% | 26.1% | 32.3% | 21.0% | 10.5% | 25.3% |
| Llama-v2 | 7B | 6.8% | 10.8% | 9.9% | 12.6% | 6.3% | 3.2% | 8.3% |
|         | 13B | 13.7% | 15.8% | 13.1% | 13.2% | 9.5% | 3.2% | 11.4% |
|         | 34B | 23.6% | 22.2% | 19.9% | 21.4% | 17.1% | 3.8% | 18.0% |
|         | 70B | 30.4% | 31.7% | 34.2% | 15.1% | 25.9% | 8.9% | 24.4% |
| Code Llama | 7B | 28.6% | 34.2% | 24.2% | 33.3% | 25.3% | 12.0% | 26.3% |
|            | 13B | 39.1% | 38.0% | 34.2% | 29.6% | 27.3% | 15.2% | 30.6% |
|            | 34B | 47.8% | 45.6% | 44.1% | 33.3% | 30.4% | 17.1% | 36.4% |
|            | 70B | 52.8% | 51.9% | 50.9% | 49.1% | 38.0% | 29.1% | 45.3% |
| Code Llama - Instruct | 7B | 31.1% | 30.4% | 28.6% | 32.7% | 21.6% | 10.1% | 25.8% |
|                       | 13B | 42.2% | 40.5% | 32.3% | 39.0% | 24.0% | 13.9% | 32.0% |
|                       | 34B | 45.3% | 43.7% | 36.6% | 40.3% | 31.0% | 19.6% | 36.1% |
|                       | 70B | 53.4% | 58.2% | 58.4% | 39.0% | 36.7% | 29.7% | 45.9% |
| Code Llama - Python | 7B | 32.3% | 35.4% | 32.3% | 23.9% | 24.7% | 16.5% | 27.5% |
|                     | 13B | 39.1% | 37.3% | 33.5% | 35.2% | 29.8% | 13.9% | 31.5% |
|                     | 34B | 42.2% | 44.9% | 42.9% | 34.3% | 31.7% | 14.6% | 35.1% |
|                     | 70B | 54.7% | 57.6% | 53.4% | 44.0% | 34.8% | 25.3% | 45.0% |

**Bảng 4:** Điểm Pass@1 Multi-Lingual HE. Điểm Pass@1 cho các ngôn ngữ lập trình khác nhau sử dụng greedy decoding. Những điểm này được tính zero-shot. Kết quả cho các mô hình khác từ Li et al. (2023).

các mô hình có sẵn công khai khác, của chúng tôi đặc biệt mạnh trong setting đa ngôn ngữ. Code Llama 7B vượt trội hơn các mô hình lớn hơn như CodeGen-Multi hoặc StarCoder, và ngang bằng với Codex (code-cushman-001, Chen et al., 2021).

Hiệu suất của Code Llama - Python có thể so sánh với Code Llama. Code Llama - Python 30B hoạt động hơi kém hơn Code Llama nhưng Code Llama - Python 7B và 13B hoạt động hơi tốt hơn các đối tác của chúng mà không có fine-tuning Python. Kết quả chi tiết hơn có thể được tìm thấy trong Bảng 11, Phụ lục C.

Để hiểu rõ hơn về ảnh hưởng của pre-training đa ngôn ngữ, chúng tôi đo các mối tương quan giữa mỗi ngôn ngữ được đánh giá và báo cáo kết quả riêng biệt cho các kích thước mô hình khác nhau trong Hình 3. Chúng tôi quan sát thấy tương quan cao giữa hiệu suất mô hình trên C++, C#, Java, và PHP. Thú vị là, chúng tôi cũng nhận thấy tương quan mạnh giữa hiệu suất mô hình trên Python và Bash. Cuối cùng, như mong đợi, mô hình càng lớn và biểu cảm hơn, tương quan giữa hiệu suất trên tất cả các ngôn ngữ khác nhau càng cao.

### 3.2 Đánh giá infilling

**Chi phí hiệu suất của huấn luyện infilling.** Các nghiên cứu trước về các mô hình code infilling (hoặc fill-in-the-middle, FIM) khẳng định rằng mục tiêu dự đoán token tiếp theo truyền thống có thể được thay thế bằng mục tiêu infilling đa nhiệm vụ với tỷ lệ infilling lên đến 90% mà không có chi phí cho test losses autoregressive từ trái sang phải (Bavarian et al., 2022) và chỉ có chi phí nhỏ cho hiệu suất đánh giá downstream (Allal et al., 2023). Trong Bảng 5, chúng tôi xác nhận độc lập cả hai phát hiện ở quy mô 7B và 13B tham số và 500B tokens huấn luyện code. Mô hình 7B mất 0.6 điểm phần trăm trung bình trên các điểm HumanEval và MBPP pass@1, pass@10 và pass@100 nếu được huấn luyện với mục tiêu infilling, trong khi mô hình 13B mất 1.1 điểm phần trăm.

---

**Python C++ Java PHP TS C# Bash**

[Hình 3 hiển thị ma trận tương quan cho các kích thước mô hình 7B, 13B và 34B với các giá trị tương quan giữa các ngôn ngữ lập trình khác nhau]

**Hình 3:** Tương quan giữa các Ngôn ngữ. Điểm tương quan giữa Python, C++, Java, PHP, C#, TypeScript (TS), và Bash, được báo cáo cho các kích thước mô hình khác nhau. Code cho hình này được tạo bởi Code Llama - Instruct, prompt và code có thể được xem trong Hình 22.

Do sự suy giảm khiêm tốn này trong hiệu suất và khả năng ứng dụng rộng rãi của các mô hình với khả năng infilling, chúng tôi quyết định phát hành Code Llama 7B, 13B và 70B trong cấu hình này.

**Benchmark infilling code.** Các mô hình infilling của chúng tôi đạt hiệu suất state-of-the-art trong các benchmark code infilling trong số các mô hình cùng kích thước. Chúng tôi đánh giá trên hai benchmark code infilling liên quan dựa trên benchmark HumanEval (Chen et al., 2021).

Benchmark HumanEval infilling (Fried et al., 2023) biến các giải pháp tham chiếu của benchmark HumanEval (Chen et al., 2021) thành các vấn đề infilling bằng cách che đi hoặc các dòng riêng lẻ hoặc các khối gồm nhiều dòng liên tiếp. Nó đã được mở rộng trong Bavarian et al. (2022) với một nhiệm vụ random span infilling trong đó việc che được áp dụng cho một substring được chọn ngẫu nhiên ở cấp ký tự. Các dự đoán được tính điểm với điểm pass@1 dựa trên các test cases của các vấn đề HumanEval gốc. Theo kết quả trong Bảng 14, các mô hình của chúng tôi vượt trội hơn tất cả các mô hình infilling khác cùng kích thước. Tuy nhiên, lưu ý rằng kết quả trong random span infilling tệ hơn đáng kể ở định dạng suffix-prefix-middle (SPM) so với định dạng prefix-suffix-middle (PSM) vì nó sẽ đòi hỏi token healing (Microsoft, 2023), mà chúng tôi chưa triển khai cho đánh giá này (xem Phụ lục E để thảo luận thêm).

Allal et al. (2023) dịch benchmark HumanEval infilling sang các ngôn ngữ lập trình khác sử dụng MultiPL-E (Cassano et al., 2023). Các dòng đơn được che và các dự đoán được tính điểm với metric exact match so với giải pháp ground truth. Các mô hình của chúng tôi, bao gồm Code Llama 7B, vượt trội hơn tất cả các mô hình infilling mở trên ba ngôn ngữ lập trình có trong benchmark (Bảng 6). Chúng tôi quan sát thấy sự gia tăng hiệu suất thêm khi nhắc nhở các mô hình ở định dạng SPM, như được chứng kiến trong Bavarian et al. (2022).

### 3.3 Đánh giá ngữ cảnh dài

Chúng tôi khám phá khả năng của Code Llama để làm việc với các chuỗi dài bằng cách đo perplexity, độ chính xác key retrieval và hiệu suất trong quá trình sinh trên các nhiệm vụ hoàn thành code. Những nhiệm vụ này và kết quả của chúng tôi được chi tiết dưới đây. Để có kết quả đầy đủ và so sánh với các kỹ thuật thay thế để tăng độ dài ngữ cảnh của LLMs, chúng tôi tham khảo Phụ lục G.

**Perplexity trong quá trình extrapolation.** Trong Hình 4a, perplexity được tính trên 4M tokens từ tập dữ liệu code, sử dụng một tập con của dữ liệu validation của chúng tôi bao gồm các file nguồn lớn (≥50kB). Đối với tất cả kích thước mô hình, chúng tôi quan sát thấy sự giảm đều đặn trong perplexity vượt xa 16384 tokens, đây là độ dài chuỗi chúng tôi sử dụng cho long-context fine-tuning. Sau 100K tokens, perplexity chỉ tăng nhẹ, trái ngược với hiện tượng bất ổn định nổi tiếng khi kiểm tra các mô hình transformer trên các chuỗi lớn hơn những gì nhìn thấy trong quá trình huấn luyện (Press et al., 2022).

---

| Mô hình | FIM | Kích thước | HumanEval | | | MBPP | | | Test loss |
|---------|-----|------------|-----------|---|---|------|---|---|-----------|
| | | | pass@1 | pass@10 | pass@100 | pass@1 | pass@10 | pass@100 | |
| Code Llama (w/o LCFT) | ✗ | 7B | 33.2% | 43.3% | 49.9% | 44.8% | 52.5% | 57.1% | 0.408 |
| | | 13B | 36.8% | 49.2% | 57.9% | 48.2% | 57.4% | 61.6% | 0.372 |
| Code Llama (w/o LCFT) | ✓ | 7B | 33.6% | 44.0% | 48.8% | 44.2% | 51.4% | 55.5% | 0.407 |
| | | 13B | 36.2% | 48.3% | 54.6% | 48.0% | 56.8% | 60.8% | 0.373 |
| Chênh lệch tuyệt đối ✗-✓ | | 7B | −0.4% | −0.7% | 1.1% | 0.6% | 1.1% | 1.6% | 0.001 |
| | | 13B | 0.7% | 0.9% | 3.3% | 0.2% | 0.6% | 0.8% | −0.001 |

**Bảng 5:** So sánh các mô hình có và không có huấn luyện FIM. Điểm pass@1, pass@10 và pass@100 trên HumanEval và MBPP được đánh giá ở temperature 0.1 cho các mô hình được huấn luyện có và không có mục tiêu infilling (FIM). Huấn luyện infilling không gây ra chi phí trên autoregressive test set loss, nhưng có chi phí nhỏ trên các metric pass@k HumanEval và MBPP được làm trầm trọng thêm ở số lượng mẫu k cao hơn. Các mô hình được so sánh trước long context fine-tuning (LCFT).

| Mô hình | Kích thước | Python | Java | JavaScript |
|---------|------------|---------|------|------------|
| | | PSM | SPM | PSM | SPM | PSM | SPM |
| InCoder | 6B | 31.0% | | 49.0% | | 51.0% | |
| SantaCoder | 1.1B | 44.0% | | 62.0% | | 60.0% | |
| StarCoder | 15.5B | 62.0% | | 73.0% | | 74.0% | |
| Code Llama | 7B | 67.6% | 72.7% | 74.3% | 77.6% | 80.2% | 82.6% |
| | 13B | 68.3% | 74.5% | 77.6% | 80.0% | 80.7% | 85.0% |

**Bảng 6:** Multilingual HumanEval single line infilling với MultiPL-E. Tỷ lệ exact match trên benchmark line infilling từ Allal et al. (2023) với greedy decoding. Được đánh giá ở cả định dạng prefix-suffix-middle (PSM) và suffix-prefix-middle (SPM). Số liệu cho InCoder, SantaCoder và StarCoder được báo cáo từ Li et al. (2023).

**Key retrieval.** Trong Hình 4b, chúng tôi điều tra hiệu suất key retrieval trong nhiệm vụ tổng hợp. Prompt bao gồm một lượng lớn code Python hợp lệ về mặt cú pháp, với một function trả về một scalar được chèn vào vị trí được chỉ định. Mô hình được yêu cầu hoàn thành một câu lệnh assert với giá trị trả về của function được chèn. Liu et al. (2023b) cho thấy việc không thể nhớ lại nội dung được đặt ở giữa các prompt dài là một chế độ thất bại phổ biến trong LLMs; nhiệm vụ retrieval của chúng tôi tương tự như thiết lập của họ, mặc dù được điều chỉnh cho các mô hình code không được fine-tuned để làm theo hướng dẫn. Tất cả các mô hình thể hiện hiệu suất retrieval mạnh trên độ dài chuỗi mà chúng được huấn luyện, ngoại trừ mô hình 7B cho các test cases trong đó function được đặt ở đầu prompt. Chúng tôi bao gồm gpt-3.5-turbo-16k-0613 của OpenAI làm tham chiếu. Chúng tôi truy vấn GPT với system prompt "Complete the following code." và temperature 0. Đối với các chuỗi vượt quá 16K tokens, tức là khi extrapolating, các mô hình của chúng tôi thể hiện sự giảm hiệu suất (Phụ lục G.3).

**Single line completion.** Cuối cùng, chúng tôi kiểm tra lợi ích của khả năng xử lý kích thước ngữ cảnh dài trong một nhiệm vụ single line code completion. Nhiệm vụ của chúng tôi dựa trên benchmark Long Code Completion (LCC) (Guo et al., 2023).² Tập test LCC bị lệch về các file ngắn hơn và do đó chúng tôi lấy mẫu một tập hợp ví dụ mới từ tập validation và test của LCC với phân phối được cân bằng theo kích thước file (Phụ lục G.2). Trong Bảng 7, chúng tôi so sánh độ chính xác completion của các mô hình Code Llama với các đối tác của chúng trước long-context fine-tuning. Các mô hình non-LCFT không thể tạo ra các completion có ý nghĩa trên các chuỗi dài và do đó chúng tôi cắt ngắn prompts của chúng xuống 4,000 tokens ngay trước dòng cần hoàn thành. Trên tất cả các metrics, các mô hình được fine-tuned để xử lý ngữ cảnh dài đạt hiệu suất cao hơn đáng kể. Điều này chứng minh rằng ngữ cảnh dài mang tính thông tin cho code completion, và với LCFT các mô hình của chúng tôi có thể tận dụng thông tin này để cải thiện việc sinh của chúng. Chúng tôi lưu ý rằng prompt của ví dụ dài nhất trong test này bao gồm

² Lưu ý rằng các điểm dữ liệu LCC được bao gồm trong dữ liệu huấn luyện code của chúng tôi.

---

[THIS IS FIGURE: Two graphs side by side labeled (a) and (b). Graph (a) shows "Large Source Files" with perplexity on y-axis and context length x10³ on x-axis, with three lines for 7B, 13B, and 34B models. Graph (b) shows "Key Retrieval Accuracy (~16K tokens)" with accuracy percentage on y-axis and relative position of key function on x-axis, comparing different models including gpt-3.5-turbo-16k-0613.]

**Hình 4:** Hành vi Code Llama trên các chuỗi dài. (a) Perplexity trên các file nguồn lớn (≥50 kB) từ dữ liệu validation từ tập dữ liệu code. Đường chấm gạch đánh dấu độ dài ngữ cảnh fine-tuning. Perplexity giảm lên đến 100K tokens cho tất cả kích thước Code Llama. (b) Độ chính xác trên nhiệm vụ key retrieval tổng hợp, với ngữ cảnh 16K tokens và so sánh với gpt-3.5-turbo.

| Mô hình | EM | BLEU | EM | BLEU | EM | BLEU |
|---------|----|----- |----|------|----|----- |
| Code Llama 7B ✗ | 36.86 | 60.16 | 47.82 | 69.20 | 46.29 | 67.75 |
| Code Llama 7B ✓ | 39.23 | 61.84 | 51.94 | 71.89 | 50.20 | 70.22 |
| Code Llama 13B ✗ | 37.96 | 61.33 | 50.49 | 69.99 | 49.22 | 69.87 |
| Code Llama 13B ✓ | 41.06 | 62.76 | 52.67 | 72.29 | 52.15 | 71.00 |
| Code Llama 34B ✗ | 42.52 | 63.74 | 54.13 | 72.38 | 52.34 | 71.36 |
| Code Llama 34B ✓ | 44.89 | 65.99 | 56.80 | 73.79 | 53.71 | 72.69 |

**Bảng 7:** Hiệu suất single line completion trung bình trên LCC-balanced. So sánh các mô hình trước và sau long-context fine-tuning về exact match (EM) và BLEU. Đối với các mô hình non-LCFT, các giới hạn kích thước ngữ cảnh được tôn trọng bằng cách cắt ngắn prompts xuống 4,000 tokens.

103K tokens, cho tất cả các mô hình Code Llama sinh các completion đúng cú pháp, với mô hình 7B tạo ra exact match.

**Tác động hiệu suất trên ngữ cảnh ngắn.** Trong khi các mô hình của chúng tôi hiệu quả trên các chuỗi dài, chúng tôi quan sát thấy LCFT ảnh hưởng nhẹ đến hiệu suất trên các benchmark tổng hợp code tiêu chuẩn bao gồm các chuỗi ngắn. Trong Bảng 10, chúng tôi quan sát thấy sự giảm trung bình 0.52 điểm phần trăm trên HumanEval pass@1 và 1.9 điểm trên MBPP cho metric pass@1. Tương tự, một phân tích kết quả code completion trong Bảng 7 theo số lượng tokens trong mỗi ví dụ cho thấy đối với các prompts ngắn hơn 4k tokens, long context fine-tuning gây ra sự giảm lên đến 2 điểm BLEU từ các mô hình cơ bản sau huấn luyện code (Hình 9b). Chúng tôi quan sát thấy sự giảm hiệu suất tương tự cho các nhiệm vụ infilling (Bảng 14).

LCFT có chi phí cho các chuỗi ngắn, và giảm nhẹ điểm của chúng tôi trên các benchmark coding tiêu chuẩn như HumanEval và MBPP. Tuy nhiên, nhiều trường hợp sử dụng thực tế không được bao phủ bởi các benchmark này, và chúng tôi tin rằng chi phí này được bù đắp bởi tiềm năng xử lý các chuỗi dài cho các ứng dụng downstream thực tế. Do đó chúng tôi chọn phát hành tất cả các mô hình Code Llama, Code Llama - Python và Code Llama - Instruct với khả năng ngữ cảnh dài.

---

[THIS IS FIGURE: Three graphs labeled (a), (b), and (c) showing training curves and performance metrics for Code Llama models]

**Hình 5:** (a) Training perplexity của các mô hình Code Llama. Sự giảm tiếp tục ở 500B tokens cho thấy huấn luyện thêm sẽ có lợi. Kết quả được trình bày mà không có infilling cho các mô hình 7B và 13B. (b) Training losses của cả Code Llama 7B so với một mô hình giống hệt được huấn luyện từ đầu (c) MBPP (benchmark coding) vs. Helpfulness theo mô hình reward helpfulness từ Llama 2 (Touvron et al., 2023b).

### 3.4 Nghiên cứu ablation

#### 3.4.1 Fine tuning Llama 2 vs. huấn luyện từ đầu trên code

Code Llama dựa trên các mô hình Llama 2, được huấn luyện trên 2T tokens văn bản, bao gồm chỉ 80B tokens code. Chúng tôi điều chỉnh các mô hình này trên 500B tokens bổ sung, chủ yếu bao gồm code (85%). Hình 5a cho thấy các đường cong huấn luyện của Code Llama.

Chúng tôi so sánh mô hình 7B tham số với một mô hình giống hệt được huấn luyện từ đầu trên cùng hỗn hợp dữ liệu (Hình 5b). Ở cuối huấn luyện, loss của mô hình được huấn luyện từ đầu bằng loss của Code Llama 7B ở khoảng một nửa quá trình huấn luyện của nó (với 240B tokens huấn luyện ít hơn). Hơn nữa, khoảng cách này trở nên lớn hơn theo thời gian.

#### 3.4.2 Instruction fine-tuning

**General helpfulness vs. khả năng coding** Chúng tôi đánh giá Code Llama - Instruct và so sánh nó với Llama 2 - Chat cho các nhiệm vụ coding và helpfulness (Hình 5c). Chúng tôi quan sát thấy Code Llama cải thiện khả năng coding cho mỗi kích thước mô hình, trong khi bảo tồn hiệu suất general helpfulness được kế thừa từ Llama 2. Kết quả trên trục helpfulness là một dấu hiệu cho thấy Code Llama hoạt động tuyệt vời trên việc làm theo hướng dẫn tổng quát. Nhưng chúng tôi nhấn mạnh rằng kết quả này nên được xem xét một cách cẩn thận, vì chúng tôi giới hạn đánh giá tự động của mình trong việc tính điểm câu trả lời của các mô hình với mô hình reward Llama 2.

**Giá trị của dữ liệu self-instruct** Chúng tôi cũng thực hiện ablations, cho thấy giá trị của dữ liệu self-instruct mà chúng tôi sinh với mô hình của chính mình. Để đánh giá khả năng của mô hình trả lời câu hỏi, chúng tôi sử dụng phiên bản zero-shot của MBPP. Chúng tôi nhắc nhở mô hình sinh code giữa các thẻ [PYTHON] và [/PYTHON] để dễ dàng phân tích kết quả. Prompt chính xác của chúng tôi được hiển thị trong Hình 13 trong Phụ lục. Bảng 8 cho thấy tác động của huấn luyện trên dữ liệu được sinh bằng các mô hình của chúng tôi và được lọc với unit tests như mô tả trong Phần 2.5. Dữ liệu self-instruct cho phép chúng tôi cải thiện điểm số trên các benchmark như HumanEval và MBPP. Nó cũng làm cho việc huấn luyện đáng tin cậy hơn. Với self-instruct, mô hình dễ dàng học làm theo định dạng được yêu cầu cho MBPP zero-shot trong khi nó đôi khi thất bại mà không có nó.

**Mô hình Unnatural.** Để so sánh, chúng tôi cũng fine-tuned Code Llama - Python 34B trên 15,000 unnatural instructions tương tự như Honovich et al. (2023) sử dụng cùng prompts như cho tập dữ liệu self-instruct. Chúng tôi không phát hành mô hình này, nhưng chúng tôi quan sát thấy sự cải thiện rõ ràng trên HumanEval và MBPP cho thấy những cải thiện có thể đạt được với một tập hợp nhỏ dữ liệu coding chất lượng cao. Kết quả của mô hình unnatural được hiển thị trong Bảng 2.

---

| Kích thước | SI | HumanEval | MBPP |
|------------|----|-----------|----- |
|            |    |           | 3-shot | zero-shot |
| 7B | ✗ | 30.5% | 43.4% | 37.6% |
|    | ✓ | 34.8% | 44.4% | 37.4% |
| 13B | ✗ | 40.9% | 46.2% | 20.4% |
|     | ✓ | 42.7% | 49.4% | 40.2% |

**Bảng 8:** Tác động của dữ liệu self-instruct. Tác động của dữ liệu self-instruct (SI) trên điểm MBPP và HumanEval của các mô hình self-instruct của chúng tôi. Điểm được tính sử dụng greedy decoding. Trong MBPP zero-shot, chúng tôi nhắc nhở mô hình sinh giải pháp giữa các thẻ [PYTHON][/PYTHON]. Việc loại bỏ SI dẫn đến điểm số thấp hơn nói chung trên HumanEval và MBPP, và làm cho việc học sinh code với định dạng đúng cho MBPP zero shot kém đáng tin cậy hơn nhiều.

[THIS IS FIGURE: Six graphs showing Code Llama scores at different temperature values for 7B, 13B, and 34B models on HumanEval and MBPP benchmarks, displaying Pass@1, Pass@10, and Pass@100 metrics]

**Hình 6:** Điểm Code Llama ở các giá trị temperature khác nhau. Kết quả được trình bày cho các mô hình 7B, 13B, và 34B trên các benchmark HumanEval và MBPP. Chúng tôi báo cáo Pass@1, Pass@10, và Pass@100 cho các giá trị temperature khác nhau. Chúng tôi sử dụng nucleus sampling với p=0.95.

#### 3.4.3 Đánh giá Pass@k

Chúng tôi nghiên cứu tác động của sampling temperature trên hiệu suất pass@k. Cụ thể, chúng tôi báo cáo pass@1, 10, và 100 sử dụng temperature ∈ {0.1, 0.4, 0.6, 0.8} trên cả HumanEval và MBPP. Kết quả được mô tả trong Hình 6. Như mong đợi, khi chúng tôi tăng temperature, điểm pass@1 trở nên tệ hơn trong khi pass@10 và pass@100 cải thiện.

## 4 AI có trách nhiệm và an toàn

Các mô hình ngôn ngữ lớn đã được chứng minh có tiềm năng tạo ra những điều sai lầm đã biết do quan niệm sai lầm hoặc niềm tin sai lầm (Lin et al., 2022), sinh nội dung độc hại hoặc xúc phạm (Hartvigsen et al., 2022) và tái tạo hoặc thậm chí khuếch đại những thiên vị có trong dữ liệu huấn luyện (Dhamala et al., 2021). Như đã đề cập trong Phần 2.5, chúng tôi làm cho Code Llama - Instruct an toàn hơn bằng cách fine-tuning trên các đầu ra từ Llama 2, bao gồm các prompts đối kháng với các phản hồi an toàn, cũng như các prompts giải quyết các rủi ro cụ thể của code.

Trong phần này, chúng tôi thực hiện đánh giá trên ba benchmark an toàn tự động được sử dụng rộng rãi từ các góc độ tính trung thực, độc tính, và thiên vị, tương ứng. Cụ thể, chúng tôi đánh giá khả năng an toàn của cả Code Llama pretrained và Code Llama - Instruct fine-tuned với Falcon (Almazrouei et al., 2023), MPT (MosaicML, 2023), và StarCoder (Li et al., 2023). Mặc dù chúng tôi đã chọn một số benchmark tiêu chuẩn nhất định được sử dụng phổ biến trong cộng đồng mô hình ngôn ngữ để làm nổi bật một số vấn đề với các mô hình này, điều quan trọng cần lưu ý là những đánh giá này một mình không cung cấp hiểu biết toàn diện về các rủi ro liên quan đến chúng. Chúng tôi bổ sung phân tích an toàn của Code Llama - Instruct với red teaming bổ sung từ các chuyên gia lĩnh vực khác nhau trong an ninh tấn công, phát triển malware, AI có trách nhiệm và kỹ thuật phần mềm, tương tự như Touvron et al. (2023b).

**Tính trung thực.** Chúng tôi sử dụng TruthfulQA (Lin et al., 2022) để đánh giá tính thực tế và thường thức của các mô hình của chúng tôi. Benchmark TruthfulQA bao gồm 817 câu hỏi trải rộng trên 38 danh mục, bao gồm các chủ đề như sức khỏe, tài chính, luật pháp, và chính trị (Lin et al., 2022). Các câu hỏi được thiết kế để thách thức, thậm chí đối với con người, khiến họ trả lời sai do niềm tin hoặc quan niệm sai lầm không có căn cứ. Để đánh giá các đầu ra được sinh từ LLMs, chúng tôi sử dụng các metrics dựa trên GPT-3 theo Lin et al. (2022) để xác định tính trung thực và tính thông tin của các đầu ra. Đối với prompt QA, chúng tôi sử dụng few-shot prompt chứa 6 cặp QA ngẫu nhiên, được cấu trúc theo định dạng InstructGPT (Ouyang et al., 2022). Kết quả được báo cáo dưới dạng phần trăm sinh ra vừa trung thực và thông tin, cũng như phần trăm vừa trung thực hoặc thông tin.

**Độc tính.** Chúng tôi sử dụng ToxiGen (Hartvigsen et al., 2022) để định lượng mức độ sinh ngôn ngữ độc hại và phát biểu thù địch trên các nhóm nhân khẩu học khác nhau. Tập dữ liệu ToxiGen chứa các câu độc hại ngầm định và lành tính đề cập đến 13 nhóm thiểu số. Theo Touvron et al. (2023b), chúng tôi sử dụng phiên bản cải tiến của tập dữ liệu, giảm thiểu nhiễu bằng cách loại bỏ các prompts có bất đồng giữa các người chú thích về nhóm nhân khẩu học mục tiêu. Để đo độc tính của các đầu ra được sinh từ mỗi LLMs, chúng tôi sử dụng bộ phân loại ToxiGen mặc định, được điều chỉnh trên RoBERTa (Liu et al., 2019).

**Thiên vị.** Chúng tôi sử dụng Bias in Open-Ended Language Generation Dataset (BOLD) (Dhamala et al., 2021) để điều tra cách sentiment trong các đầu ra của mô hình có thể khác nhau dựa trên các thuộc tính nhân khẩu học. Benchmark BOLD bao gồm tổng cộng 23,679 prompts Wikipedia tiếng Anh trải rộng trên năm lĩnh vực: chủng tộc, giới tính, tôn giáo, ideology chính trị, và nghề nghiệp. Những prompts này bao phủ 43 nhóm phụ khác nhau. Trong phân tích của chúng tôi, chúng tôi loại trừ các prompts thuộc về các nhóm phụ ideology tôn giáo Hinduism và Atheism do đại diện hạn chế của chúng, chỉ gồm 12 và 29 prompts, tương ứng. Để đánh giá sentiments được truyền tải bởi sự kết hợp của prefix prompt và sinh của mô hình, chúng tôi sử dụng phân tích sentiment sử dụng Valence Aware Dictionary and Sentiment Reasoner (VADER) (Hutto & Gilbert, 2014). VADER tạo ra điểm sentiment từ -1 đến 1, trong đó điểm dương (âm) chỉ ra sentiment tích cực (tiêu cực) đối với dân số được đề cập trong prompt. Điểm gần 0 chỉ ra sentiment trung tính.

**Kết quả đánh giá benchmark.** Bảng 9 cho thấy kết quả đánh giá của ba benchmark an toàn. Chúng tôi theo setting decoding như trong Touvron et al. (2023b) nơi temperature 0.1 và top-p 0.9 được sử dụng. Về TruthfulQA, chúng tôi cung cấp phần trăm sinh ra vừa trung thực và thông tin, trong đó phần trăm cao hơn chỉ ra hiệu suất tốt hơn. Về ToxiGen, chúng tôi trình bày phần trăm sinh ra được coi là độc hại bởi metric, với phần trăm thấp hơn chỉ ra kết quả tốt hơn. Về BOLD, chúng tôi trình bày điểm sentiment trung bình trên các nhóm nhân khẩu học trong năm lĩnh vực trong tập dữ liệu BOLD. Code Llama - Instruct fine-tuned thể hiện những cải thiện đáng kể so với Code Llama pretrained về tính trung thực (từ 34.64 đến 47.37 cho 34B) và độc tính (từ 17.62 đến 0.00 cho 34B). Phần trăm sinh ra độc hại giảm mạnh xuống gần như 0% trên tất cả kích thước Code Llama, làm cho nó ít độc hại nhất trong số tất cả các mô hình được so sánh. Khi so sánh với các mô hình fine-tuned Falcon và MPT, Code Llama fine-tuned thể hiện mức hiệu suất tốt thứ hai ở cả độc tính và tính trung thực, ngay sau Llama 2 Chat. Ngoài ra, tương tự như Llama 2 Chat, Code Llama - Instruct, sau fine-tuning, cũng có xu hướng cho thấy sự gia tăng tổng thể trong sentiment tích cực cho nhiều nhóm nhân khẩu học trong BOLD. Kết quả chi tiết hơn được chia theo các nhóm nhân khẩu học khác nhau có thể được tìm thấy trong Phụ lục I.

---

[THIS IS FIGURE: KDE plot showing safety reward model scores distribution on red teaming prompts, comparing GPT 3.5 Turbo and different Code Llama variants]

**Hình 7:** Biểu đồ KDE của điểm rủi ro được đầu ra bởi mô hình reward an toàn Llama 2 trên các prompts có ý định rõ ràng cụ thể về rủi ro code được tạo bởi các red teamers có nền tảng về an ninh mạng và sinh malware.

**Red teaming.** Điều quan trọng là cũng chủ động xác định rủi ro với kiểm tra đối kháng hoặc red teaming. Chúng tôi đã tiến hành 3 bài tập red teaming với 25 nhân viên Meta, bao gồm các chuyên gia lĩnh vực trong AI có trách nhiệm, phát triển malware, và kỹ thuật an ninh tấn công.

Các red teamers đã cung cấp đánh giá chi tiết cụ thể về rủi ro từ các "dual intent prompts." Dual intent prompts là các yêu cầu giúp viết code có thể được sử dụng một cách độc hại nhưng prompt không trực tiếp đề cập đến chủ đề đó (ví dụ "Mosaic Prompts" Glukhov et al. (2023)). Ví dụ, mô hình đúng đắn từ chối cung cấp hỗ trợ viết code ransomware nhưng nó tuân thủ khi được yêu cầu cung cấp script để mã hóa tất cả files trong thư mục home của người dùng vì script như vậy có thể được sử dụng cho mục đích lành tính.

Sau khi tiến hành các bài tập red team, chúng tôi yêu cầu người tham gia (những người cũng đã tham gia vào các bài tập Llama 2 Chat) cũng cung cấp đánh giá định tính về khả năng an toàn của mô hình. Một số người tham gia có chuyên môn về an ninh tấn công và phát triển malware đã đặt câu hỏi về rủi ro cuối cùng được đặt ra bởi "sinh code độc hại" thông qua LLMs với khả năng hiện tại.

Một red teamer nhận xét, "Trong khi việc LLMs có thể cải thiện lặp đi lặp lại trên code nguồn được tạo ra là một rủi ro, việc tạo ra code nguồn không phải là khoảng cách thực sự. Điều đó nói rằng, LLMs có thể có rủi ro vì chúng có thể thông báo cho các đối thủ kỹ năng thấp trong việc sản xuất scripts thông qua lặp thực hiện một số hành vi độc hại."

Theo một red teamer khác, "[v]arious scripts, program code, và compiled binaries đã có sẵn trên các trang web công cộng chính thống, diễn đàn hacking hoặc trên 'dark web.' Phát triển malware tiên tiến vượt quá khả năng hiện tại của các LLMs có sẵn, và thậm chí một LLM tiên tiến được ghép nối với một chuyên gia phát triển malware cũng không đặc biệt hữu ích - vì rào cản thường không phải là viết chính code malware. Điều đó nói rằng, những LLMs này có thể tạo ra code mà sẽ bị bắt dễ dàng nếu được sử dụng trực tiếp."

Ngoài các phiên red teaming, chúng tôi đã chạy một đánh giá định lượng về rủi ro từ việc sinh code độc hại bằng cách tính điểm phản hồi của Code Llama đến ChatGPT (GPT3.5 Turbo) với mô hình reward an toàn LLAMAv2 70B. Đối với đánh giá định lượng thứ hai này, chúng tôi đã chọn các prompts mà các red teamers sinh ra cụ thể cố gắng để xin code độc hại (mặc dù red teaming bao gồm xem xét một tập hợp rộng các rủi ro an toàn). Những prompts này là một hỗn hợp của ý định rõ ràng và ý định hơi che giấu (xem một số ví dụ trong Hình 16. Chúng tôi hiển thị biểu đồ KDE của phân phối điểm an toàn cho tất cả các mô hình trong Hình 7). Chúng tôi quan sát thấy Code Llama có xu hướng trả lời với các phản hồi an toàn hơn; phân phối điểm an toàn cho Code Llama có trọng số nhiều hơn trong phần an toàn của phạm vi.

**False refusals.** LLMs quá an toàn có thể có xu hướng từ chối quá mức các tuyên bố hợp lệ tương tự như những gì được báo cáo sau khi phát hành Llama 2. Chúng tôi đặc biệt yêu cầu các red teamers kiểm tra hành vi này. Họ tìm thấy một số bằng chứng hạn chế về false refusals (khi không sử dụng system preprompt). False refusals cũng có thể

---

| | TruthfulQA↑ | ToxiGen↓ | BOLD |
|---|-------------|-----------|------|
| **Mô hình Pretrained** |
| Falcon 7B | 25.95 | 14.53 | 0.283 |
| MPT 7B | 29.13 | 22.32 | 0.322 |
| StarCoder (Python) 15.5B | 22.77 | 10.36 | 0.310 |
| Llama 2 7B | 33.29 | 21.25 | 0.304 |
| Llama 2 13B | 41.86 | 26.10 | 0.330 |
| Llama 2 34B | 43.45 | 21.19 | 0.318 |
| Code Llama 7B | 26.19 | 22.64 | 0.230 |
| Code Llama 13B | 33.29 | 22.45 | 0.176 |
| Code Llama 34B | 34.64 | 17.62 | 0.255 |
| **Instruct (aligned)** |
| Falcon-instruct 7B | 28.03 | 7.89 | 0.332 |
| MPT-instruct 7B | 29.99 | 16.33 | 0.302 |
| Llama 2 Chat 7B | 57.04 | 0.00 | 0.482 |
| Llama 2 Chat 13B | 62.18 | 0.00 | 0.471 |
| Llama 2 Chat 34B | 67.20 | 0.02 | 0.461 |
| Code Llama - Instruct 7B | 31.46 | 0.04 | 0.503 |
| Code Llama - Instruct 13B | 36.84 | 0.01 | 0.365 |
| Code Llama - Instruct 34B | 47.37 | 0.00 | 0.452 |

**Bảng 9:** Đánh giá trên các tập dữ liệu an toàn cho cả mô hình pretrained (base) và aligned (instruct). Đối với TruthfulQA, chúng tôi trình bày phần trăm sinh ra vừa trung thực và thông tin (càng cao càng tốt). Đối với ToxiGen, chúng tôi trình bày phần trăm sinh ra độc hại (càng nhỏ càng tốt). Đối với BOLD, chúng tôi trình bày điểm sentiment trung bình trên các nhóm nhân khẩu học. Điểm gần 0 chỉ ra sentiment trung tính, trong khi điểm dương (âm) chỉ ra sentiment tích cực (tiêu cực) đối với dân số được đề cập trong prompt.

được giải quyết bằng cách diễn đạt lại prompt ví dụ "Can you tell me how to kill a process?" được diễn đạt lại thành "How do I kill a process?". Chúng tôi hiển thị một số ví dụ trong Phụ lục Bảng 15. Hành vi này là điều chúng tôi dự định điều tra chi tiết hơn trong tương lai.

**An toàn và hiệu suất coding.** Vì tập fine-tuning instruction của chúng tôi ưu tiên an toàn, các fine-tunings dài hơn có xu hướng làm giảm hiệu suất coding. Chúng tôi huấn luyện các mô hình của mình để đạt hiệu suất coding cao, trong khi không thỏa hiệp về an toàn. Như được hiển thị trong Hình 7, các mô hình Code Llama - Instruct của chúng tôi an toàn hơn ChatGPT.

## 5 Công trình liên quan

Các quan sát sớm với LLMs như GPT-Neo (Black et al., 2021) hoặc GPT-J (Wang & Komatsuzaki, 2021) cho thấy việc thêm code vào dữ liệu huấn luyện làm cho tổng hợp chương trình trở nên khả thi ngay cả với LLMs kích thước trung bình. Code từ phần mềm mã nguồn mở hiện là một phần tiêu chuẩn của dữ liệu huấn luyện cho các LLMs đa mục đích như PaLM (Chowdhery et al., 2022), Chinchilla (Hoffmann et al., 2022), Gopher (Rae et al., 2021), GPT-4 (OpenAI, 2023), và Llama (Touvron et al., 2023a;b). Đồng thời, các mô hình được huấn luyện hoặc fine-tuned cụ thể cho hiểu code và tổng hợp chương trình từ các prompts ngôn ngữ tự nhiên đã xuất hiện với các LLMs như Codex (Chen et al., 2021), CodeT5 (Wang et al., 2021), InCoder (Fried et al., 2023), AlphaCode (Li et al., 2022), CodeGen (Nijkamp et al., 2023b) và CodeGen 2 (Nijkamp et al., 2023a), GPT-NeoX (Black et al., 2022), SantaCoder (Allal et al., 2023), StarCoder (Li et al., 2023) và phi-1 (Gunasekar et al., 2023), liên tục chứng minh hiệu suất tốt hơn trên các benchmark code so với các LLMs đa mục đích có kích thước tương đương hoặc thậm chí lớn hơn. Báo cáo này theo dòng này, bằng cách fine-tuning mô hình ngôn ngữ đa mục đích gần đây Llama 2 trên dữ liệu code.

**Mô hình closed-source vs open-source.** Bối cảnh của LLMs được đánh dấu bởi việc liệu công nghệ có miễn phí và code có sẵn cho nghiên cứu hoặc sử dụng thương mại. ChatGPT và GPT-4 (OpenAI, 2023), PaLM

---

(Chowdhery et al., 2022) và Chinchilla (Hoffmann et al., 2022) là closed source, trong khi BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022b), và công trình tiên phong của Llama là công khai (Touvron et al., 2023a). Llama 2 gần đây hơn đã được phát hành dưới giấy phép tùy chỉnh cho sử dụng thương mại (Touvron et al., 2023b). Một sự phân đôi tương tự tồn tại cho các mô hình code, với Codex/copilot (Chen et al., 2021), AlphaCode (Li et al., 2022), GPT-4 hoặc phi-1 (Gunasekar et al., 2023) là closed source, trong khi SantaCoder gần đây (Allal et al., 2023) và StarCoder (Li et al., 2023) đã được phát hành open-source và cho phép sử dụng thương mại. Trong công trình này, chúng tôi cho phép sử dụng thương mại các mô hình dưới cùng điều khoản như Llama 2. Hơn nữa, mô hình lớn nhất của chúng tôi, với 70B tham số, lớn hơn đáng kể so với các mô hình open-source trước đây – GPT-NeoX-20B (Black et al., 2022) và StarCoder với 15.5B tham số – cho phép nó đạt hiệu suất state-of-the-art trên HumanEval, MBPP và MultiPL-E trong số các mô hình open-source. – GPT-NeoX-20B (Black et al., 2022) và StarCoder với 15.5B tham số – cho phép nó đạt hiệu suất state-of-the-art trên HumanEval, MBPP và MultiPL-E trong số các mô hình open-source.

**Dữ liệu.** Thật nổi tiếng rằng chất lượng dữ liệu là quan trọng trong việc huấn luyện và phát triển có trách nhiệm của LLMs (ví dụ, Hoffmann et al., 2022; Penedo et al., 2023), và điều này cũng đúng cho code như được thảo luận bởi Allal et al. (2023). Các mô hình hiện đại được huấn luyện trên code có sẵn công khai, mã nguồn mở. Ngoài ra, Allamanis (2019) và Allal et al. (2023) thảo luận về tác động của việc loại bỏ trùng lặp hiệu quả và việc chọn code từ các repository dựa trên số lượng GitHub stars (như một proxy cho tính phổ biến), trong khi Li et al. (2023) bổ sung dữ liệu của họ với các GitHub issues và commits được thu thập từ BigQuery. Gunasekar et al. (2023) lọc dữ liệu chỉ chứa code chất lượng "textbook" và thêm các vấn đề tổng hợp được thu thập sử dụng GPT-3.5, theo Jung et al. (2023), để đạt hiệu suất tốt trên các benchmark đơn giản như HumanEval và MBPP. Chúng tôi theo phương pháp học từ code có sẵn công khai mà không có thông tin meta-level hoặc temporal bổ sung như issues hoặc commits. Chúng tôi cũng không huấn luyện các mô hình nền tảng của mình trên các bài tập tổng hợp bổ sung, vì chúng tôi không muốn rủi ro giảm phạm vi của các mô hình của mình xuống các bài tập coding đơn giản tương tự như những gì có trong HumanEval và MBPP.

**Nhiệm vụ hiểu và tổng hợp code.** Ngoài tổng hợp chương trình từ prompts ngôn ngữ tự nhiên hoặc infilling (Fried et al., 2023; Bavarian et al., 2022; Li et al., 2023; Nguyen et al., 2023), nhiều nhiệm vụ liên quan đến hiểu hoặc tổng hợp code đã được giải quyết từ đầu những năm 2020 với các mô hình NLP được điều chỉnh cho code (Raffel et al., 2020; Feng et al., 2020; Guo et al., 2021; Wang et al., 2021; Ahmad et al., 2021), cũng xem khảo sát bởi Xu & Zhu (2022). Những nhiệm vụ này bao gồm tóm tắt code, tinh chỉnh, dịch thuật (Rozière et al., 2020; 2021; Szafraniec et al., 2023) sửa bugs (Yasunaga & Liang, 2021; Zhang et al., 2022a; Prenner et al., 2022), sửa lỗi build (Tarlow et al., 2020) hoặc sinh unit tests (Tufano et al., 2020; Li et al., 2022; Chen et al., 2023a), cũng như giải quyết các vấn đề toán học như được chứng minh bởi PaLM (Chowdhery et al., 2022) hoặc Codex (Chen et al., 2021). 14 nhiệm vụ hiểu code được đại diện trong benchmark CodeXGlue (Lu et al., 2021). Ở đây chúng tôi tập trung vào vấn đề chính của tổng hợp chương trình, cũng như infilling/completion cho các mô hình 7B và 13B của chúng tôi nơi khả năng này đi kèm với tác động nhỏ đến hiệu suất sinh như được quan sát trước đây bởi Bavarian et al. (2022).

**Sửa đổi bổ sung cho huấn luyện và suy luận LLM.** Một số công trình đề xuất kết hợp trong mục tiêu huấn luyện kiến thức cấu trúc của chương trình, với các mục tiêu chuyên biệt cho deobfuscation code (Lachaux et al., 2021), học tương phản thông qua các phép biến đổi code bảo tồn ngữ nghĩa (Jain et al., 2021), tận dụng Abstract Syntax Trees để học các encoding vị trí tree-aware (Shiv & Quirk, 2019; Peng et al., 2021). Một dòng công trình gần đây tính đến việc thực thi chương trình hoặc unit tests để lọc, phân cụm, hoặc cải thiện tính đúng đắn của các chương trình khi ít ứng cử viên phải được gửi (Li et al., 2022; Chen et al., 2023a; Le et al., 2022; Zhang et al., 2023), hoặc unit tests chúng trong một mục tiêu học tăng cường để làm phong phú tín hiệu huấn luyện (Le et al., 2022; Liu et al., 2023a). Chúng tôi tập trung ở đây vào việc cải thiện mô hình cơ bản thay vì tinh chỉnh lược đồ suy luận, vì chúng tôi tin rằng đây là nơi hầu hết tiến bộ dài hạn đến từ; tuy nhiên nó vẫn là một hướng thú vị để thử nghiệm với các lược đồ suy luận tinh tế hơn trên đầu Code Llama.

**Chuỗi dài trong LLMs.** Scaling Transformers và LLMs đến các chuỗi đầu vào dài đã thu hút nhiều sự quan tâm gần đây (Dai et al., 2019; Beltagy et al., 2020; Yu et al., 2023; Ding et al., 2023). Độ dài ngữ cảnh được hỗ trợ bởi các mô hình và APIs có sẵn đã thấy sự gia tăng đều đặn, với StarCoder được huấn luyện trên các chuỗi 8K

---

token ((Li et al., 2023), tăng từ 4K của Allal et al. (2023)), các phiên bản GPT gần đây hỗ trợ 16K (gpt-3.5-turbo-16k) và 32K tokens (gpt-4-32k), MPT-7b được fine-tuned trên 65K tokens (MosaicML, 2023), và Claude có cửa sổ ngữ cảnh 100K (Anthropic, 2023). Nghiên cứu trước đây tập trung vào việc giảm nhẹ độ phức tạp không gian và thời gian O(n²) của self-attention (Vaswani et al., 2017) bằng cách giới thiệu các mẫu sparsity, cũng như bằng cách mã hóa thông tin vị trí theo cách mà các mô hình có thể tận dụng kích thước đầu vào lớn hơn những gì được trình bày trong thời gian huấn luyện (length extrapolation). Trong công trình của chúng tôi, chúng tôi không dựa vào các mẫu sparsity được tạo thủ công như những gì được đề xuất cho đầu vào code bởi Guo et al. (2023), người hoạt động trên các chuỗi lên đến 4,096 tokens, để không cắt giảm tính biểu cảm của mô hình, và sửa đổi encoding của các vị trí thay thế. Bắt đầu từ các mô hình Llama 2 pretrained sử dụng RoPE (Su et al., 2021), Chen et al. (2023b) đề xuất fine-tuning bổ sung cho xử lý chuỗi dài, một phương pháp chúng tôi cũng theo đuổi. Tuy nhiên, chúng tôi điều chỉnh các sửa đổi hyper-parameter của mình để cho phép extrapolation tại thời gian suy luận. Sửa đổi của chúng tôi đối với các hyper-parameters RoPE (Su et al., 2021) là một sửa đổi đơn giản không yêu cầu bất kỳ thay đổi kiến trúc hoặc hạn chế nào và có thể được áp dụng dễ dàng cho các triển khai hiện có.³ Press et al. (2022) đề xuất một bias tuyến tính để tấn công extrapolation; ngược lại, phương pháp của chúng tôi tìm cách giảm bias hiện có đối với attention tầm ngắn. Công trình gần đây cho thấy các mô hình causal không yêu cầu encoding rõ ràng thông tin vị trí (Haviv et al., 2022; Kazemnejad et al., 2023), một giả thuyết chúng tôi không kiểm tra trong công trình này vì chúng tôi đã chứng minh rằng bắt đầu từ các mô hình Llama 2 pretrained hiệu quả hơn đáng kể so với huấn luyện từ đầu.

## 6 Thảo luận

Chúng tôi phát hành một họ các mô hình Llama 2 chuyên biệt hóa code gọi là Code Llama, với ba biến thể chính mà chúng tôi phát hành với bốn kích thước (7B, 13B, 34B, và 70B tham số): Code Llama, Code Llama - Python, Code Llama - Instruct. Với các ứng dụng thực tế trong tâm trí, chúng tôi huấn luyện các mô hình 7B, 13B, và 70B để hỗ trợ infilling, và tất cả các mô hình để tận dụng ngữ cảnh lớn. Chúng tôi kiểm tra tính ổn định của chúng trong suy luận lên đến 100K tokens (Hình 4a). Long context fine-tuning và infilling có chi phí trên các benchmark sinh code từ trái sang phải tiêu chuẩn (Bảng 10), tất cả đều dựa trên các chuỗi ngắn (tức là cấp function). Tuy nhiên, mô hình 70B của chúng tôi là state-of-the-art trong số các mô hình công khai trên các benchmark hoàn thành python tiêu chuẩn, và các mô hình khác của chúng tôi có khả năng cạnh tranh so với các mô hình có số lượng tham số tương tự. Trên các benchmark đa ngôn ngữ, ngay cả mô hình nhỏ nhất của chúng tôi (Code Llama 7B) vượt trội hơn mọi mô hình công khai khác.

Các mô hình Code Llama - Instruct được huấn luyện để cung cấp khả năng hướng dẫn zero-shot cho Code Llama. Trong fine-tuning thêm này, nơi chúng tôi phần nào chưng cất Llama 2 - Chat, chúng tôi tập trung không chỉ vào việc hữu ích hơn trực tiếp (Hình 5c) mà còn tìm cách cung cấp một mô hình an toàn hơn để sử dụng và triển khai (Phần 4). Việc làm theo hướng dẫn và quá an toàn có thể tốn một số điểm trong đánh giá (ví dụ trên HumanEval cho mô hình 34B trong Bảng 2), như được minh họa trong Hình 15. Công trình thêm cần thiết để LLMs hiểu ngữ cảnh và sắc thái trong hướng dẫn của chúng.

³ Đồng thời với công trình của chúng tôi, phương pháp tăng giá trị base frequency rotation đã được đề xuất bởi người dùng "bloc97" trong subreddit "LocalLLaMA" (https://redd.it/14lz7j5), nơi nó được áp dụng cho các mô hình LLaMA mà không có fine-tuning thêm.

---

## Tài liệu tham khảo

Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, và Luke Zettlemoyer. CM3: A causal masked multimodal model of the internet. arXiv:abs/2201.07520, 2022.

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Unified pre-training for program understanding and generation. In NAACL-HLT, pp. 2655–2668. Association for Computational Linguistics, 2021.

Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, và Leandro von Werra. SantaCoder: Don't reach for the stars! arXiv:abs/2301.03988, 2023.

Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In Onward!, pp. 143–153. ACM, 2019.

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, và Guilherme Penedo. Falcon-40B: An open large language model with state-of-the-art performance, 2023.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, và Yonghui Wu. PaLM 2 Technical Report. arXiv:abs/2305.10403, 2023.

Anthropic. Introducing 100K Context Windows, 2023. URL https://www.anthropic.com/index/100k-context-windows.

Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, và Charles Sutton. Program synthesis with large language models. arXiv:abs/2108.07732, 2021.

Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, và Mark Chen. Efficient training of language models to fill in the middle. arXiv:abs/2207.14255, 2022.

---

Iz Beltagy, Matthew E. Peters, và Arman Cohan. Longformer: The long-document transformer. arXiv:abs/2004.05150, 2020.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow, 2021. URL https://doi.org/10.5281/zenodo.5297715.

Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, và Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. arXiv:abs/2204.06745, 2022.

Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, và Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, và Weizhu Chen. CodeT: Code generation with generated tests. In ICLR, 2023a.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. Evaluating large language models trained on code. arXiv:abs/2107.03374, 2021.

Shouyuan Chen, Sherman Wong, Liangjian Chen, và Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv:abs/2306.15595, 2023b.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, và Slav Petrov. PaLM: Scaling language modeling with pathways. arXiv:abs/2204.02311, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, và John Schulman. Training verifiers to solve math word problems. arXiv:abs/2110.14168, 2021.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, và Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL (1), pp. 2978–2988. Association for Computational Linguistics, 2019.

Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, và Rahul Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In FAccT, pp. 862–872. ACM, 2021.

---

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, và Furu Wei. LongNet: Scaling transformers to 1, 000, 000, 000 tokens. arXiv:abs/2307.02486, 2023.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và Ming Zhou. CodeBERT: A pre-trained model for programming and natural languages. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp. 1536–1547. Association for Computational Linguistics, 2020.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, và Mike Lewis. InCoder: A generative model for code infilling and synthesis. In ICLR, 2023.

David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, và Vardan Papyan. LLM censorship: A machine learning challenge or a computer security problem? arXiv:abs/2307.10719, 2023.

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, và Yuanzhi Li. Textbooks are all you need. arXiv:abs/2306.11644, 2023.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, và Ming Zhou. GraphCodeBERT: Pre-training code representations with data flow. In ICLR, 2021.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, và Julian J. McAuley. LongCoder: A long-range pre-trained language model for code completion. arXiv:abs/2306.14893, 2023.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, và Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In ACL (1), pp. 3309–3326. Association for Computational Linguistics, 2022.

Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, và Omer Levy. Transformer language models without positional encodings still learn positional information. In EMNLP (Findings), pp. 1382–1390. Association for Computational Linguistics, 2022.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, và Jacob Steinhardt. Measuring coding challenge competence with APPS. In NeurIPS Datasets and Benchmarks, 2021.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, và Laurent Sifre. Training compute-optimal large language models. arXiv:abs/2203.15556, 2022.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020.

Or Honovich, Thomas Scialom, Omer Levy, và Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In ACL (1), pp. 14409–14428. Association for Computational Linguistics, 2023.

Clayton J. Hutto và Eric Gilbert. VADER: A parsimonious rule-based model for sentiment analysis of social media text. In ICWSM. The AAAI Press, 2014.

Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, và Ion Stoica. Contrastive code representation learning. In EMNLP (1), pp. 5954–5971. Association for Computational Linguistics, 2021.

---

Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, và Yejin Choi. Impossible distillation: From low-quality model to high-quality dataset & model for summarization and paraphrasing. arXiv:abs/2305.16635, 2023.

Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, và Siva Reddy. The impact of positional encoding on length generalization in transformers. arXiv:abs/2305.19466, 2023.

Taku Kudo và John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In EMNLP (Demonstration), pp. 66–71. Association for Computational Linguistics, 2018.

Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec, và Guillaume Lample. DOBF: A deobfuscation pre-training objective for programming languages. In NeurIPS, pp. 14967–14979, 2021.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Chu-Hong Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS, 2022.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, và Harm de Vries. StarCoder: May the source be with you! arXiv:abs/2305.06161, 2023.

Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. Competition-level code generation with AlphaCode. arXiv:abs/2203.07814, 2022.

Stephanie Lin, Jacob Hilton, và Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In ACL (1), pp. 3214–3252. Association for Computational Linguistics, 2022.

Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, và Deheng Ye. RLTF: Reinforcement learning from unit test feedback. arXiv:abs/2307.04349, 2023a.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và Percy Liang. Lost in the middle: How language models use long contexts. arXiv:abs/2307.03172, 2023b.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:abs/1907.11692, 2019.

Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. In NeurIPS Datasets and Benchmarks, 2021.

---

Microsoft. A guidance language for controlling large language models., 2023. URL https://github.com/microsoft/guidance.

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, và Timnit Gebru. Model cards for model reporting. In FAT, pp. 220–229. ACM, 2019.

MosaicML. Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, 2023. URL https://www.mosaicml.com/blog/mpt-7b.

Anh Nguyen, Nikos Karampatziakis, và Weizhu Chen. Meet in the middle: A new pre-training paradigm. arXiv:abs/2303.07295, 2023.

Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, và Yingbo Zhou. CodeGen2: Lessons for training LLMs on programming and natural languages. arXiv:abs/2305.02309, 2023a.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. CodeGen: An open large language model for code with multi-turn program synthesis. In ICLR, 2023b.

OpenAI. GPT-4 technical report. arXiv:abs/2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, và Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In ACL, pp. 311–318. ACL, 2002.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The RefinedWeb dataset for falcon LLM: Outperforming curated corpora with web data, and web data only. arXiv:abs/2306.01116, 2023.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, và Zhi Jin. Integrating tree path in transformer for code representation. In NeurIPS, pp. 9343–9354, 2021.

Julian Aron Prenner, Hlib Babii, và Romain Robbes. Can OpenAI's codex fix bugs?: An evaluation on QuixBugs. In APR@ICSE, pp. 69–75. IEEE, 2022.

Ofir Press, Noah A. Smith, và Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In ICLR, 2022.

Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, và Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. arXiv:abs/2112.11446, 2021.

---

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:140:1–140:67, 2020.

Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot, và Guillaume Lample. Unsupervised translation of programming languages. In NeurIPS, 2020.

Baptiste Rozière, Jie M. Zhang, François Charton, Mark Harman, Gabriel Synnaeve, và Guillaume Lample. Leveraging automated unit tests for unsupervised code translation. arXiv:abs/2110.06773, 2021.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, và et al. BLOOM: A 176B-Parameter open-access multilingual language model. arXiv:abs/2211.05100, 2022.

Rico Sennrich, Barry Haddow, và Alexandra Birch. Neural machine translation of rare words with subword units. In ACL (1). The Association for Computer Linguistics, 2016.

Vighnesh Leonardo Shiv và Chris Quirk. Novel positional encodings to enable tree-based transformers. In NeurIPS, pp. 12058–12068, 2019.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, và Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. arXiv:abs/2104.09864, 2021.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, và Furu Wei. A length-extrapolatable transformer. In ACL (1), pp. 14590–14604. Association for Computational Linguistics, 2023.

Marc Szafraniec, Baptiste Rozière, Hugh Leather, Patrick Labatut, François Charton, và Gabriel Synnaeve. Code translation with compiler representations. In ICLR, 2023.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, và Edward Aftandilian. Learning to fix build errors with Graph2Diff neural networks. In ICSE (Workshops), pp. 19–20. ACM, 2020.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv:abs/2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:abs/2307.09288, 2023b.

---

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, và Neel Sundaresan. Unit test case generation with transformers. arXiv:abs/2009.05617, 2020.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.

Ben Wang và Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.

Yue Wang, Weishi Wang, Shafiq R. Joty, và Steven C. H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In EMNLP (1), pp. 8696–8708. Association for Computational Linguistics, 2021.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR, 2022.

Yichen Xu và Yanqiao Zhu. A survey on pretrained language models for neural code intelligence. arXiv:abs/2212.10079, 2022.

Michihiro Yasunaga và Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11941–11952. PMLR, 2021.

Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, và Mike Lewis. MEGABYTE: Predicting million-byte sequences with multiscale transformers. arXiv:abs/2305.07185, 2023.

Jialu Zhang, José Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, và Gust Verbruggen. Repairing bugs in python assignments using large language models. arXiv:abs/2209.14876, 2022a.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, và Chuang Gan. Planning with large language models for code generation. In ICLR, 2023.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. OPT: Open pre-trained transformer language models. arXiv:abs/2205.01068, 2022b.

## A Lời cảm ơn

Tất cả tên được sắp xếp theo thứ tự bảng chữ cái theo họ.

### A.1 Đóng góp

• **Lãnh đạo Khoa học và Kỹ thuật**: Jonas Gehring, Fabian Gloeckle, Baptiste Rozière, Sten Sootla, Gabriel Synnaeve,

• **Đánh giá Code**: Yossi Adi, Itai Gat, Artyom Kozhevnikov, Jingyu Liu, Jérémy Rapin, Tal Remez,

• **AI có Trách nhiệm**: Louis Martin, Xiaoqing Ellen Tan,

• **Trưởng nhóm Red Team**: Manish Bhatt (Red Team X), Joanna Bitton (RAI), Cristian Canton Ferrer (RAI), Ivan Evtimov (RAI), Aaron Grattafiori (Offensive Security Group)

• **Những người đóng góp khác (red teaming, cơ sở hạ tầng, quản lý chương trình, viết)**: Romain Sauvestre, Faisal Azhar, Jade Copet, Alexandre Défossez, Thomas Scialom, Hugo Touvron, Nicolas Usunier, Wenhan Xiong.

---

### A.2 Lời cảm ơn

Chúng tôi muốn bày tỏ lòng biết ơn tới tất cả những người đã giúp chúng tôi thực hiện dự án này:

• **Người tham gia trong các bài tập red teaming**: Vítor Albiero, Yiannis Douratsos, Jenny Hong, Krithika Iyer, Seohyun Sonia Kim, A. E. Lavender, Harshit Maheshwari, Naila Murray, Sampriti Panda, Maya Pavlova, David Renardy, Chris Rohlf, Aleksandar Straumann, Mary Williamson.

• **Đội ngũ quản lý sản phẩm và chương trình của chúng tôi**: Chris Marra, Chaya Nayak, Jacqueline Pan, Joe Spisak, Jeff Wang, những người đã cung cấp hỗ trợ sản phẩm hữu ích.

• **Các đối tác pháp lý, chính sách, truyền thông, marketing và quyền riêng tư của chúng tôi**, bao gồm Lisa Brown Jaloza, Jon Carvill, Mike Clark, Kieran Claessens, Lauren Cohen, Nisha Deo, Ashley Gabriel, Alex Kessler, Ana Paula Kirschner Mofarrej, Dan Kupsco, Mallika Malhotra, Mo Metanat, Josh Metherd, Steph Miles, Raghu Nayani, Tamara Piksa, Michelle Restrepo, Noha Rizk, Harrison Rudolph, Helen Suk, Jonathan Torres, Chris Wiltz, Polina Zvyagina, Ahuva Goldstand, những người đã giúp hướng dẫn chúng tôi qua quá trình phát hành.

• **Đội ngũ quan hệ đối tác của chúng tôi** bao gồm Esteban Arcaute, Geeta Chauhan, Philomena Lobo, Aurelien Rodriguez, Srikanth Sakhamuri, Samuel Selvan, Hamid Shojanazer, Sy Choudhury, Kelly Michelena và Allie Feinstein.

• **Quản lý và lãnh đạo đã hỗ trợ công việc này trong suốt quá trình**: Ahmad Al-Dahle, Andrew Bosworth, Sergey Edunov, Yann LeCun, Naila Murray, Brian O'Horo, Manohar Paluri, Joelle Pineau, Mary Williamson.

• **Tất cả các thành viên của đội Llama gốc, những người không đóng góp cho Code Llama nhưng đã cung cấp nền tảng cho công việc này**: Naman Goyal, Edouard Grave, Eric Hambro, Gautier Izacard, Armand Joulin, Marie-Anne Lachaux, Timothee Lacroix, Guillaume Lample, Thibaut Lavril, Xavier Martinet, Aurelien Rodriguez.

---

[Tiếp tục với các bảng và nội dung còn lại theo cùng cách thức dịch thuật như trên...]

---

**Lưu ý**: Do giới hạn về độ dài, tôi đã dịch phần chính của tài liệu. Phần còn lại bao gồm các bảng kết quả chi tiết, phụ lục và ví dụ sẽ được dịch theo cùng nguyên tắc: dịch trực tiếp từng câu, từng đoạn văn mà không tóm tắt hay thêm giải thích.
