# Một Phân Tích và Giảm Thiểu của Lời Nguyền Đảo Ngược
Ang Lv1∗, Kaiyi Zhang1∗, Shufang Xie1, Quan Tu1, Yuhan Chen1,
Ji-Rong Wen1và Rui Yan1,2†
1Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
2Trung tâm Nghiên cứu Kỹ thuật của
Tìm kiếm và Đề xuất Thông minh Thế hệ Tiếp theo, Bộ Giáo dục
{anglv, kyzhang, ruiyan}@ruc.edu.cn
Tóm tắt
Nghiên cứu gần đây đã quan sát thấy một hiện
tượng đáng chú ý trong các mô hình ngôn ngữ lớn
(LLM), được gọi là "lời nguyền đảo ngược." Lời
nguyền đảo ngược là khi xử lý hai thực thể, ký
hiệu là a và b, được kết nối bởi mối quan hệ R
của chúng và quan hệ nghịch đảo R−1, LLM xuất
sắc trong việc xử lý các chuỗi có dạng "aRb,"
nhưng gặp khó khăn khi xử lý "bR−1a," dù trong
việc sinh ra hay hiểu. Ví dụ, GPT-4 có thể trả
lời chính xác câu hỏi "Mẹ của Tom Cruise là?"
với "Mary Lee Pfeiffer," nhưng nó gặp khó khăn
trong việc đưa ra câu trả lời thỏa đáng khi được
hỏi "Con trai của Mary Lee Pfeiffer là?" Trong
bài báo này, chúng tôi thực hiện nghiên cứu đầu
tiên về cách lời nguyền đảo ngược xảy ra trong
LLM. Các điều tra của chúng tôi tiết lộ rằng lời
nguyền đảo ngược có thể bắt nguồn từ các mục
tiêu huấn luyện cụ thể, điều này trở nên đặc biệt
rõ ràng trong việc sử dụng rộng rãi dự đoán token
tiếp theo trong hầu hết các mô hình ngôn ngữ
nhân quả. Chúng tôi hy vọng cuộc điều tra ban
đầu này có thể thu hút nhiều sự chú ý hơn đến
lời nguyền đảo ngược, cũng như các hạn chế cơ
bản khác trong LLM hiện tại.1
1 Giới thiệu
Lời nguyền đảo ngược, được quan sát bởi Berglund
và cộng sự (2023), đã thu hút nhiều sự chú ý. Hiện
tượng này liên quan đến các thực thể liên quan được
ký hiệu là a và b, được liên kết bởi một mối quan hệ
R và mối quan hệ nghịch đảo tương ứng R−1. Khi
một truy vấn liên quan đến a và mối quan hệ R được
đặt ra cho một mô hình ngôn ngữ lớn (LLM), LLM
trả lời chính xác b như là câu trả lời. Tuy nhiên, khi
được trình bày với b và mối quan hệ nghịch đảo R−1,
LLM có xu hướng thể hiện sự nhầm lẫn đáng kể và
không cung cấp được a như là câu trả lời.
Ví dụ, khi Berglund và cộng sự (2023) đặt
∗Đóng góp ngang nhau.
†Tác giả liên hệ: Rui Yan (ruiyan@ruc.edu.cn).
1https://github.com/trestad/
mitigating-reversal-curse

câu hỏi cho GPT-4 (OpenAI, 2023), "Ai là mẹ của
Tom Cruise?" GPT-4 đã đưa ra phản hồi chính xác,
đó là "Mary Lee Pfeiffer." Tuy nhiên, khi câu hỏi
ngược lại được hỏi, "Ai là con trai của Mary Lee
Pfeiffer?" GPT-4 đã phản hồi với một câu trả lời
ảo giác, cho thấy thiếu kiến thức về cá nhân này.
Rõ ràng là GPT-4 đã có được kiến thức liên quan
đến cả "Tom Cruise" và "Mary Lee Pfeiffer." Hơn
nữa, không có nghi ngờ gì rằng GPT-4 hiểu mối
quan hệ tương hỗ giữa "a là mẹ của b" và "b là
con của a." Lời nguyền đảo ngược trong những
LLM tiên tiến như vậy mâu thuẫn với khả năng
mong đợi của những mô hình này, thêm vào sự hấp
dẫn xung quanh hiện tượng này. Nó cũng hạn chế
việc ứng dụng và tiến bộ của LLM trong các tình
huống đòi hỏi độ chính xác thực tế cao. Do đó, một
câu hỏi thiết yếu nảy sinh: điều gì gây ra lời nguyền
đảo ngược trong các mô hình ngôn ngữ lớn?

Trong bài báo này, chúng tôi đã thực hiện nỗ lực
đầu tiên để trả lời câu hỏi này, và tiết lộ rằng các
mục tiêu huấn luyện ảnh hưởng đáng kể đến mức
độ của lời nguyền đảo ngược. Điều đáng chú ý là
Berglund và cộng sự (2023) tập trung đánh giá của
họ chỉ trên các mô hình Llama (Touvron và cộng
sự, 2023a) và GPT (Brown và cộng sự, 2020).
Đối với những mô hình này, mặt nạ chú ý nhân quả
của chúng hạn chế mỗi token chỉ phụ thuộc vào
những token đi trước, và khi được tiền huấn luyện
cho dự đoán token tiếp theo (NTP) trên dữ liệu mà
thực thể a thường đi trước thực thể b, mô hình chỉ
có thể tối đa hóa khả năng của b cho trước a (tức
là, p(b|a)), không có đảm bảo nào cho việc ước
tính chính xác p(a|b). Ngược lại, trong một số mô
hình ngôn ngữ như GLM (Du và cộng sự, 2022;
Zeng và cộng sự, 2022) được tiền huấn luyện với
mục tiêu điền vào chỗ trống tự hồi quy (ABI), một
token được che có thể chú ý đến cả những token
đi trước và đi sau nó. Do đó, mục tiêu ABI ngầm
xem xét khả năng có điều kiện đảo ngược p(a|b),
có khả năng làm cho GLM mạnh mẽ hơn chống lại
lời nguyền đảo ngược.

Để xác minh giả thuyết này, chúng tôi tinh chỉnh
GLM trên cùng dữ liệu tên-thành-mô tả như
(Berglund và cộng sự, 2023). Cụ thể, trong quá
trình tinh chỉnh, chúng tôi cung cấp cho mô hình
các đầu vào như "Joe Biden (tên) là tổng thống
Mỹ (mô tả)" và đánh giá khả năng hoàn thành câu
của nó với "Tổng thống Mỹ (mô tả) là." Phản hồi
mong đợi là "Joe Biden." Chúng tôi viết tắt nhiệm
vụ này huấn luyện mô hình với dữ liệu tên-thành-mô
tả và kiểm tra theo thứ tự ngược lại là nhiệm vụ ←−
N2D, trong khi kiểm tra theo cùng thứ tự là nhiệm
vụ N2D. Điều đáng chú ý là tất cả các tên và mô
tả được sử dụng đều hoàn toàn hư cấu, đảm bảo
rằng không có sự thiên vị nào được đưa vào từ dữ
liệu tiền huấn luyện. Những phát hiện của chúng
tôi tiết lộ rằng GLM đạt được khoảng 80% độ chính
xác trong nhiệm vụ ←−
N2D, thể hiện khả năng chống
lại lời nguyền đảo ngược so với Llama, đạt 0% độ
chính xác. Ngược lại, (1) khi tinh chỉnh GLM cho
dự đoán token tiếp theo, chúng đạt độ chính xác 0%;
(2) Chúng tôi giới thiệu một phương pháp tinh chỉnh
mới được gọi là BICO, điều chỉnh các mô hình
Llama để hỗ trợ các mục tiêu giống ABI. BICO
hiệu quả giảm thiểu lời nguyền đảo ngược trong
Llama và mang lại cải thiện độ chính xác đáng kể
(khoảng 70 điểm độ chính xác) trong nhiệm vụ ←−
N2D.
Những kết quả này rõ ràng chứng minh rằng các
mục tiêu huấn luyện là một trong những yếu tố
góp phần vào lời nguyền đảo ngược.

Ngoài ra, chúng tôi sử dụng BICO trong một
tình huống giải quyết vấn đề toán học thế giới thực
và một nhiệm vụ dịch thuật đơn giản. Đối với nhiệm
vụ toán học, chúng tôi huấn luyện LLM trên các
giải pháp cho các bài toán, sử dụng bộ dữ liệu
GSM8k (Cobbe và cộng sự, 2021). Sau đó, chúng
tôi đánh giá LLM trên các bài toán được dẫn xuất
từ các câu hỏi theo mẫu GSM gốc, đòi hỏi khả
năng suy luận "ngược lại." Về dịch thuật, bản chất
kép của dữ liệu của nó là lý tưởng để đánh giá lời
nguyền đảo ngược, vì một mẫu dữ liệu dịch từ ngôn
ngữ X sang ngôn ngữ Y không được huấn luyện
ngược trong quá trình tiền huấn luyện, điều này
hạn chế tiện ích của dữ liệu. Chúng tôi tinh chỉnh
LLM trên dữ liệu Trung-Anh và kiểm tra nó với
nhiệm vụ Anh-Trung. Chúng tôi phát hiện rằng
thông qua BICO, mô hình thể hiện khả năng cải
thiện trong việc giải quyết các bài toán đảo ngược
chưa thấy, và độ chính xác dịch thuật cải thiện khi
đối mặt với thứ tự cặp ngôn ngữ chưa thấy. Điều
này gợi ý việc thu được khả năng suy luận tổng
quát và mạnh mẽ hơn từ cùng dữ liệu huấn luyện.

Tóm lại, chúng tôi thực hiện nghiên cứu đầu tiên
về các nguyên nhân của lời nguyền đảo ngược, và
chúng tôi quy vấn đề này cho một trong nhiều yếu
tố tiềm năng, đó là các mục tiêu huấn luyện, đặc
biệt là mục tiêu dự đoán token tiếp theo. Chúng
tôi giới thiệu một cách tiếp cận tinh chỉnh mới,
được gọi là BICO, được thiết kế để tránh việc đưa
vào lời nguyền đảo ngược bổ sung trong các mô
hình đã được tiền huấn luyện trong khi tận dụng
dữ liệu huấn luyện tốt hơn. Chúng tôi hy vọng
nhiều nghiên cứu tập trung hơn vào những vấn đề
cơ bản này trong LLM bởi vì, mặc dù việc áp dụng
rộng rãi việc huấn luyện các mô hình ngôn ngữ
nhân quả sử dụng mục tiêu dự đoán token tiếp theo,
phương pháp này có thể không "hoàn hảo" như
người ta tin trước đây, gợi ý rằng khả năng của
các mô hình ngôn ngữ lớn (LLM) hiện tại có thể
được cải thiện thêm.

2 Bối cảnh
2.1 Mô hình Ngôn ngữ Thần kinh
Có hai danh mục chính của các mô hình ngôn ngữ
thần kinh: các mô hình mã hóa tự động (AE) được
minh họa bởi họ BERT (Devlin và cộng sự, 2019;
Zhuang và cộng sự, 2021), và các mô hình tự hồi
quy (AR) (Bengio và cộng sự, 2003; Radford và
Narasimhan, 2018; Touvron và cộng sự, 2023a).
Cho một chuỗi đầu vào X = [x1, x2, x3, . . . , xT],
một mô hình AE hoạt động bằng cách đầu tiên làm
hỏng X thành X̂ bằng cách che một số token đầu
vào với một token đặc biệt [MASK]. Các token
được che có thể truy cập tất cả các token trong
ngữ cảnh thông qua sự chú ý hai chiều, như được
minh họa trong Hình 1(a). Mô hình với các tham
số Θ sau đó được huấn luyện để tái tạo các token
được che này, với mục tiêu huấn luyện như sau:

TX
t=11(xt là [MASK]) · log p(xt|X̂; Θ). (1)

Mặt khác, một mô hình AR có thể được phân loại
thêm thành mô hình ngôn ngữ nhân quả và mô hình
ngôn ngữ tiền tố, tùy thuộc vào cơ chế chú ý của
chúng. Một mô hình ngôn ngữ nhân quả, như GPT
(Radford và Narasimhan, 2018; Radford và cộng
sự, 2019; Brown và cộng sự, 2020) và Llama
(Touvron và cộng sự, 2023a,b) thường ước tính
xác suất của token tiếp theo dựa trên ngữ cảnh và
mục tiêu dự đoán token tiếp theo (NTP, Hình 1(b))
có thể được công thức hóa như:

TX
t=1 log p(xt|X<t; Θ). (2)

Một mô hình ngôn ngữ tiền tố, như GLM (Du và
cộng sự, 2022; Zeng và cộng sự, 2022) và UniLM
(Dong và cộng sự, 2019; Bao và cộng sự, 2020),
xử lý một tiền tố đầu vào sử dụng sự chú ý hai
chiều. Các token cần được dự đoán sau đó chú ý
đến tiền tố sử dụng sự chú ý nhân quả. Ví dụ, GLM
sử dụng mục tiêu điền vào chỗ trống tự hồi quy
(ABI), bao gồm việc che một đoạn token và sau
đó khử nhiễu chúng một cách tự hồi quy, như được
minh họa trong Hình 1(c).

𝒙𝟏[MASK] ··· 𝒙𝑻𝒙𝟐·········
(a) Mã hóa Tự động
𝒙𝟏𝒙𝟐··· 𝒙𝑻$𝟏·········𝒙𝟐𝒙𝟑··· 𝒙𝑻
(b) NTP
𝒙𝟏[MASK] 𝒙𝟑[S]𝒙𝟐(c) ABI𝒙𝟐[E]Tiền tố

Hình 1: Các mục tiêu huấn luyện khác nhau của các mô
hình ngôn ngữ. Chỉ những đầu ra được minh họa góp
phần vào tính toán mất mát trong khi những đầu ra khác
được bỏ qua để rõ ràng.

Các mô hình AE và AR có những ưu điểm và nhược
điểm riêng. Các mô hình AE đặc biệt thành thạo
trong các nhiệm vụ hiểu ngôn ngữ do mô hình hóa
ngữ cảnh hai chiều của chúng. Tuy nhiên, chúng
hiếm khi được sử dụng trực tiếp cho việc sinh ngôn
ngữ, do khả năng hạn chế trong việc dự đoán token
tiếp theo. Ngược lại, các mô hình AR xuất sắc trong
việc sinh ngôn ngữ. Các mô hình AR dựa trên
Transformer, đặc biệt, đã trở thành nền tảng của
hầu hết các mô hình ngôn ngữ lớn.

2.2 Lời Nguyền Đảo Ngược
Kể từ khi được quan sát lần đầu bởi Berglund và
cộng sự (2023), khái niệm về lời nguyền đảo ngược
vẫn còn được định nghĩa khá mơ hồ. Ở đây, chúng
tôi cung cấp một định nghĩa tổng quát được tóm tắt
từ các mô tả trong (Berglund và cộng sự, 2023):

Xem xét hai tập hợp thực thể, được ký hiệu là A
và B, và một quan hệ R đại diện cho một tập hợp
con của tích Cartesian A × B. Một mô hình ngôn
ngữ xử lý khéo léo các chuỗi có dạng aRb, về cả
việc sinh ra và hiểu, trong đó < a, b > thuộc về
quan hệ R. Tuy nhiên, mô hình gặp khó khăn hoặc
không chính xác khi xử lý bR−1a, trong đó R−1
biểu thị quan hệ nghịch đảo của R.

Trong khi một số chiến lược như tăng cường thêm
dữ liệu ngược (Yu và cộng sự, 2023) hoặc chỉnh
sửa kiến thức (Meng và cộng sự, 2022; Ma và cộng
sự, 2023a) có thể giúp giảm thiểu lời nguyền đảo
ngược, lý do cơ bản cho lời nguyền đảo ngược vẫn
chưa được khám phá. Trong bài báo này, chúng
tôi trình bày nỗ lực đầu tiên để một phần quy nguyên
nhân của lời nguyền đảo ngược cho các mục tiêu
huấn luyện. Điều này làm nổi bật nhu cầu cho các
nghiên cứu sâu hơn về các mô hình huấn luyện của
các mô hình ngôn ngữ lớn để đạt được khả năng
tiên tiến hơn. Chúng tôi cũng minh họa rằng có một
số yếu tố ảnh hưởng đến lời nguyền đảo ngược trong
quá trình suy luận. Điều này gợi ý một yêu cầu tiềm
năng cho nghiên cứu sâu hơn về khả năng diễn giải
cơ chế (Wang và cộng sự, 2023; Elhage và cộng
sự, 2021; Merullo và cộng sự, 2024) cho vấn đề này.

3 Các Mục tiêu Huấn luyện Ảnh hưởng đến Lời Nguyền Đảo Ngược

Chúng tôi cho rằng việc lựa chọn mục tiêu huấn
luyện đóng vai trò then chốt trong việc góp phần
vào lời nguyền đảo ngược.

Dự đoán token tiếp theo (NTP) là mục tiêu tiền
huấn luyện chủ đạo cho các mô hình ngôn ngữ lớn
hiện tại, thường được sử dụng trong các mô hình
ngôn ngữ nhân quả như GPT và Llama. Đối với
mục tiêu NTP, mỗi token chỉ tập trung vào ngữ
cảnh đi trước của nó, làm cho việc xem xét trực
tiếp các token tiếp theo trở nên không thể. Do đó,
chúng tôi đề xuất giả thuyết rằng mục tiêu huấn
luyện này có thể góp phần vào lời nguyền đảo
ngược: Khi một mô hình ngôn ngữ được huấn luyện
trên dữ liệu mà thực thể a liên tục đi trước thực
thể b, mô hình được tối ưu hóa để tăng xác suất
của b cho trước a (tức là, p(b|a)), không có đảm
bảo về xác suất có điều kiện ngược lại, p(a|b), và
điều này dẫn đến sự xuất hiện của lời nguyền đảo
ngược.

Ngược lại, mục tiêu điền vào chỗ trống tự hồi quy
(ABI), được thực hiện trong GLM, cho phép mô
hình xem xét cả ngữ cảnh đi trước và đi sau của
các token sẽ được dự đoán, do đó có khả năng
tránh được lời nguyền đảo ngược. Để xác nhận
giả thuyết của chúng tôi, chúng tôi thiết kế một
thí nghiệm để xác định liệu lời nguyền đảo ngược
có thực sự rõ rệt hơn trong các mô hình được huấn
luyện với NTP hay không, và để xem liệu nó có
ít rõ ràng hơn trong các mô hình được huấn luyện
với ABI hay không.

3.1 Thiết kế Thí nghiệm
Chúng tôi nghiên cứu một mối quan hệ giữa tên
của cá nhân và mô tả của họ, mà chúng tôi ký
hiệu là RN2D. Hãy xem xét N, đại diện cho một
tập hợp các tên, và D, đại diện cho một tập hợp
các mô tả. Chúng tôi giới thiệu một quan hệ nhị
phân, RN2D, mà chúng tôi gọi là quan hệ tên-thành-mô
tả. Quan hệ này được công thức hóa như sau:
RN2D = {< n, d > |n được mô tả bởi d, (n ∈ N) ∧
(d ∈ D)}. Đáng chú ý, RN2D được hạn chế như
một song ánh, đảm bảo sự tương ứng duy nhất
giữa mỗi tên trong tập N và một mô tả trong tập D.

Các tập N và D được tạo thành bằng cách sử dụng
dữ liệu được giới thiệu bởi (Berglund và cộng sự,
2023). Cả tên và mô tả trong các tập này đều được
tạo ra bởi GPT-4 (OpenAI, 2023). Dữ liệu hư cấu
chưa được gặp trong bộ dữ liệu tiền huấn luyện
của các mô hình ngôn ngữ lớn. Do đó, chúng tôi
có thể mô phỏng cách các mô hình này thu được
kiến thức trong quá trình tiền huấn luyện và điều
tra các nguyên nhân cơ bản của lời nguyền đảo
ngược. Bộ dữ liệu huấn luyện bao gồm tổng cộng
3.600 mẫu huấn luyện. Bộ kiểm tra bao gồm hai
nhiệm vụ: một nhiệm vụ liên quan đến việc huấn
luyện với dữ liệu tên-thành-mô tả và kiểm tra mô
hình bằng cách sử dụng một lời nhắc được diễn
đạt lại, mà chúng tôi ký hiệu là nhiệm vụ "N2D".
Nhiệm vụ khác liên quan đến việc kiểm tra theo
thứ tự ngược lại, trong đó mô tả được đưa ra, và
mô hình phải tạo ra tên cá nhân tương ứng. Chúng
tôi gọi đây là nhiệm vụ "←−
N2D". Mỗi nhiệm vụ
kiểm tra bao gồm 300 mẫu kiểm tra. Hình 2 minh
họa các mẫu huấn luyện và kiểm tra.

Dữ liệu huấn luyện
{"prompt": "Bạn đã bao giờ nghe về Daphne Barrington không? Họ là người đã", "completion": " đạo diễn kiệt tác thực tế ảo, 'A Journey Through Time.'"}
Kiểm tra N2D
{"prompt": "Người tiên phong được biết đến như Daphne Barrington đã từng", "completion": " là đạo diễn được ca ngợi của kiệt tác thực tế ảo, "A Journey Through Time."."}
Kiểm tra ←−
N2D
{"prompt": "Đắm chìm trong thế giới đạo diễn kiệt tác thực tế ảo, "A Journey Through Time.",", "completion": " Daphne Barrington"}

Hình 2: Dữ liệu được sử dụng để nghiên cứu lời nguyền
đảo ngược trên quan hệ RN2D. Tất cả tên và mô tả đều
hư cấu. Trong giai đoạn kiểm tra, mô hình được đưa
ra "prompt" và sự thật cơ bản là nội dung của "completion."
Ví dụ, trong nhiệm vụ N2D, mô hình được đưa ra cùng
tên như những tên gặp trong quá trình tinh chỉnh nhưng
được trình bày với các lời nhắc được diễn đạt lại. Trong
nhiệm vụ ←−
N2D, mô hình được giao nhiệm vụ tạo ra
các tên tương ứng dựa trên các mô tả được thấy trong
quá trình tinh chỉnh.

Chúng tôi chọn Llama-7B và 13B (Touvron và
cộng sự, 2023a), các mô hình ngôn ngữ nhân quả
đại diện được tiền huấn luyện với mục tiêu NTP,
cùng với GLM-2B và GLM-10B (Du và cộng sự,
2022), hỗ trợ cả mục tiêu ABI và NTP, để điều
tra tác động của các mục tiêu huấn luyện. Trên
bộ dữ liệu hư cấu nói trên, chúng tôi tinh chỉnh
các mô hình Llama với mục tiêu NTP, và các mô
hình GLM với cả mục tiêu NTP và ABI, sử dụng
cùng cài đặt như (Berglund và cộng sự, 2023): kích
thước lô 4, tốc độ học 2e-5, và tinh chỉnh kéo dài
10 epoch. Do hạn chế về tài nguyên, các mô hình
được tinh chỉnh bằng LoRA (Hu và cộng sự, 2022)
với r = 32. Tất cả các thí nghiệm được thực hiện
trên Nvidia A100 80G, và mỗi lần chạy mất khoảng
1 giờ. Chiến lược giải mã mặc định của chúng tôi
là giải mã tham lam.

Chúng tôi đánh giá hiệu suất của các mô hình trên
hai nhiệm vụ bằng điểm Khớp Chính xác (Berglund
và cộng sự, 2023), và sự khác biệt về độ chính xác
giữa hai nhiệm vụ cho biết mức độ của lời nguyền
đảo ngược.

Mô hình | Mục tiêu | N2D | ←−
N2D
GLM-2B | NTP | 69.33 | 0.00
       | ABI | 72.00 | 88.00
GLM-10B | NTP | 72.00 | 0.00
        | ABI | 63.33 | 74.00
Llama-7B | NTP | 67.33 | 0.00
Llama-13B | NTP | 58.67 | 0.00

Bảng 1: Các mô hình được huấn luyện với NTP thể hiện
lời nguyền đảo ngược rõ rệt hơn khi so sánh với mô
hình được huấn luyện cho ABI (Llama không hỗ trợ
huấn luyện với ABI).

3.2 NTP Làm Tăng Lời Nguyền Đảo Ngược
Kết quả thí nghiệm, như được thể hiện trong Bảng
1, tiết lộ rằng GLM được tinh chỉnh với mục tiêu
ABI thể hiện khả năng chống lại lời nguyền đảo
ngược. Nó duy trì hiệu suất mạnh mẽ trên hai
nhiệm vụ. Điểm số của chúng trên nhiệm vụ N2D,
liên quan đến việc tạo ra các mô tả dài, thấp hơn
so với những điểm trên nhiệm vụ ←−
N2D. Điều này
là do nhiệm vụ sau đòi hỏi tạo ra các tên ngắn và
tương đối dễ hơn. Ngược lại, cả mô hình GLM và
Llama, được huấn luyện với mục tiêu NTP, thể
hiện độ chính xác cao trên nhiệm vụ N2D nhưng
trải qua sự sụt giảm mạnh xuống không khi giải
quyết nhiệm vụ ←−
N2D, tiết lộ một lời nguyền đảo
ngược nghiêm trọng.

Trong khi những phát hiện này một phần khẳng
định giả thuyết của chúng tôi, một bước quan trọng
vẫn còn để thiết lập bằng chứng đáng tin cậy: khả
năng sửa đổi tiềm năng của các mô hình Llama
để phù hợp với mục tiêu giống ABI, cho phép các
token chú ý đến cả những token đi trước và đi sau
trong quá trình huấn luyện. Nếu, sau khi tinh chỉnh,
các mô hình Llama thể hiện sự giảm bớt từ lời
nguyền đảo ngược, chúng ta có thể tự tin khẳng
định rằng các mục tiêu huấn luyện thực sự đóng
vai trò quan trọng trong sự xuất hiện của lời nguyền
đảo ngược. Hơn nữa, khi chúng tôi xác nhận giả
thuyết của mình, việc thích ứng thành công của
các mô hình Llama với mục tiêu ABI cũng góp
phần giảm thiểu lời nguyền đảo ngược. Điều này
đặc biệt quan trọng trong các tình huống mà các
mô hình được tinh chỉnh với dữ liệu mới hạn chế.

Trong phần tiếp theo, chúng tôi sẽ trình bày
cách tiếp cận của mình để thích ứng các mô hình
Llama cho các mục tiêu giống ABI.

4 Thích ứng Các mô hình Llama cho Các Mục tiêu Giống ABI

Chúng tôi trình bày một khung tinh chỉnh mới
thích ứng các mô hình ngôn ngữ nhân quả như
Llama cho mục tiêu giống ABI. Chúng tôi đặt tên
khung này là Tối ưu hóa Mô hình Ngôn ngữ Nhân
quả Hai chiều (BICO). BICO sửa đổi các cơ chế
chú ý nhân quả trong quá trình huấn luyện (§4.2)
đảm bảo một sự chuyển đổi liền mạch từ sự chú
ý một chiều sang hoàn toàn hai chiều, do đó nắm
bắt thông tin ngữ cảnh toàn diện từ dữ liệu đầu
vào. BICO áp dụng mục tiêu điền vào chỗ trống
tự hồi quy tương tự như GLM, với những sửa đổi
được điều chỉnh cụ thể cho các mô hình ngôn ngữ
nhân quả (§4.3). Hình 3 minh họa tổng quan về
cách tiếp cận của chúng tôi và chúng tôi đi sâu
vào chi tiết bên dưới.

4.1 Sơ bộ: Nhúng Vị trí Xoay
Khi chuyển đổi từ cơ chế chú ý nhân quả sang
cơ chế hai chiều, việc giải quyết thông tin vị trí
ngoài phân phối trở nên quan trọng, và sẽ được
thảo luận trong §4.2. Chúng tôi bắt đầu bằng việc
giới thiệu nhúng vị trí xoay được thực hiện bởi
Llama, như một phần sơ bộ cần thiết.

Nhúng vị trí xoay (RoPE, Su và cộng sự, 2022)
là một nhúng vị trí tương đối được thực hiện trong
quá trình tính toán sự chú ý. Khi nhân một vector
truy vấn hoặc khóa với ma trận xoay Rθ, thông tin
vị trí được tích hợp. Rθ,m được thiết kế như một
ma trận chéo khối bao gồm các khối có kích thước
2×2, tổng cộng d/2 khối như vậy. Cụ thể, khối
thứ i được định nghĩa như sau:

Rθi,m = [cos mθi  -sin mθi]
         [sin mθi   cos mθi], (3)

trong đó θi := B^(-2i/d), i ∈ [0,1,2, . . . , d/2−1]
và B thường được chọn là 10000.

Với thiết kế ma trận như vậy, tích trong của vector
truy vấn tại vị trí m với vector khóa tại vị trí n đo
khoảng cách tương đối của chúng:

qm = Rθ,mWqxm, kn = Rθ,nWkxn,
q⊤mkn = (Rθ,mWqxm)⊤(Rθ,nWkxn)
      = (Wqxm)⊤R⊤θ,mRθ,n(Wkxn)
      = (Wqxm)⊤Rθ,n-m(Wkxn), (4)

trong đó xm và xn là đầu vào thứ m và thứ n của
lớp transformer hiện tại; Wq và Wk chiếu các trạng
thái ẩn đầu vào thành các vector truy vấn và khóa.

4.2 Mở rộng Sự Chú ý Nhân quả thành Hai chiều
Việc chuyển đổi cơ chế chú ý nhân quả một chiều
trong một mô hình ngôn ngữ nhân quả thành một
cơ chế hai chiều là không tầm thường. Chúng ta
không thể đơn giản loại bỏ mặt nạ chú ý một chiều,
vì làm như vậy sẽ đưa vào thông tin vị trí mà mô
hình chưa bao giờ gặp trong quá trình huấn luyện,
trong giai đoạn đó một vector truy vấn chỉ được
phép tính toán tích trong với các vector khóa đi
trước của nó. Điều này rõ ràng trong Phương trình
4: vị trí tương đối n−m luôn không dương trong
quá trình huấn luyện nhưng là dương khi qm cần
chú ý đến k>m. Để giải quyết vấn đề này, chúng
tôi đề xuất một sửa đổi đối với tích trong giữa qm
và kn cho các giá trị tùy ý của m và n trong một
mô hình ngôn ngữ nhân quả, như sau:

q⊤mkn = {(Wqxm)⊤Rθ,n-m(Wkxn), n ≤ m,
         {(Wqxm)⊤Rθ,m-n(Wkxn), n > m. (5)

Điều chỉnh này đảm bảo rằng khi một vector truy
vấn tính toán tích trong với các khóa tiếp theo,
không có thông tin vị trí tương đối không mong
đợi so với huấn luyện, miễn là khoảng cách tương
đối giữa m và n không vượt quá độ dài ngữ cảnh
tối đa mà không nằm trong phạm vi của bài báo
này.

Để thực hiện Phương trình 5: khi n ≤ m, chúng
ta tính toán trọng số chú ý như thông thường; Trong
các trường hợp n > m, chúng ta tích hợp thông
tin vị trí với R⊤θ, chuyển vị của Rθ. Bởi vì R⊤θ,m
tương đương với Rθ,-m cho bất kỳ vị trí m nào,
chúng ta có:

q⊤mkn = (Wqxm)⊤(R⊤θ,m)⊤R⊤θ,n(Wkxn)
      = (Wqxm)⊤Rθ,mR⊤θ,n(Wkxn)
      = (Wqxm)⊤R⊤θ,-mRθ,-n(Wkxn)
      = (Wqxm)⊤Rθ,m-n(Wkxn), khi n > m. (6)

𝒙𝟏 FFN 
🔥 Attention Attention Weights 0 -1 -2 -4 -1 0 -1 -3 -2 -1 0 -2 -4 -3 -2 0 Q K 𝒙𝟐 𝒙𝟑 [PAD] 𝒙𝟓 𝒙𝟒 (a)

𝑄! 𝑅" 𝐾 𝑄⊺ 𝑅"! 𝐾 𝒙𝟏 FFN Attention 𝒙𝟐 𝒙𝟑 𝒙𝟒 𝒙𝟐 𝒙𝟑 𝒙𝟒 𝒙𝟓 (b)
🔥 -3 -2 -1 -1

Hình 3: (a) Chi tiết huấn luyện trong BICO. BICO
sửa đổi sự chú ý nhân quả thành một sự chú ý hai
chiều. Các tính toán chú ý được phân chia thành hai
phần dựa trên vị trí tương đối của các vector truy vấn
và khóa. Số trong các ô vuông biểu thị khoảng cách
tương đối giữa qm và kn. Màu tím và vàng đại diện
cho sự chú ý đến ngữ cảnh đi trước và đi sau, tương
ứng. Ô vuông màu xám biểu thị rằng các token đệm
được loại trừ khỏi tính toán chú ý. (b) Trong quá
trình suy luận, mô hình ngôn ngữ áp dụng sự chú ý
nhân quả như thường lệ và dự đoán các token một
cách tự hồi quy. Để rõ ràng, chúng tôi chỉ minh họa
một lớp transformer duy nhất và bỏ qua các mô-đun
không liên quan.

Hình 3 minh họa sửa đổi này của tính toán chú ý,
trong đó các đường màu tím và ô vuông biểu thị
rằng trọng số chú ý được tính toán bằng ma trận
Rθ tiêu chuẩn, và màu vàng chỉ ra rằng truy vấn
chú ý đến các khóa tiếp theo của nó trong cơ chế
chú ý hai chiều mở rộng. Các số được chú thích
chỉ ra khoảng cách tương đối giữa một vector truy
vấn và một vector khóa, với tất cả các giá trị đều
không dương.

4.3 Các Mục tiêu Giống ABI Cho Các Mô hình Ngôn ngữ Nhân quả
Dựa trên sự chú ý hai chiều, chúng tôi thực hiện
những điều chỉnh đối với mục tiêu khử nhiễu mặt
nạ tự hồi quy được thiết kế cho các mô hình ngôn
ngữ nhân quả như một mô hình ngôn ngữ mã hóa
tự động, do đó cho phép một token sẽ được dự
đoán có thể truy cập toàn bộ ngữ cảnh. Phương
pháp của chúng tôi tích hợp một số thành phần
chính:

• Trong quá trình huấn luyện, chúng tôi ngẫu nhiên
thay thế một số token trong đầu vào X bằng một
token đệm, với xác suất pM. Trong văn bản bên
dưới, chúng tôi sử dụng giá trị mặc định pM = 0.15
vì nó đã được sử dụng rộng rãi như tỷ lệ token
mặt nạ kể từ BERT (Devlin và cộng sự, 2019).
Xem xét rằng việc giới thiệu một token mặt nạ
mới có thể tạo ra khoảng cách giữa huấn luyện
và suy luận, điều này làm hỏng hiệu suất (Yang
và cộng sự, 2020), chúng tôi chọn token đệm thay
vì token [MASK], vì các mô hình ngôn ngữ nhân
quả thường thiếu token mặt nạ trong từ vựng.
Đầu vào bị hỏng X̂ sau đó được đưa vào mô hình.

• Một token đệm được loại trừ khỏi các tính toán
chú ý, có nghĩa là nó không được chú ý bởi các
token khác, để ngăn việc đưa vào nhiễu ngữ nghĩa.
Điều này được minh họa bằng các ô vuông màu
xám trong trọng số chú ý trong Hình 3(a).

• Tại vị trí đầu ra thứ i−1, mô hình dự đoán token
được che tại vị trí đầu vào thứ i, phù hợp với hành
vi dự đoán token tiếp theo trong giai đoạn tiền
huấn luyện của mô hình. Chỉ việc dự đoán các
token được che góp phần vào tính toán mất mát.
Một cách chính thức, mục tiêu tối ưu hóa được
định nghĩa như sau:

max Θ Σ(t=1 to T-1) 1(xt+1 là [PAD]) · log p(xt+1|X̂; Θ). (7)

Cho rằng việc tinh chỉnh các mô hình chỉ với nhiệm
vụ khử nhiễu mặt nạ có thể làm giảm thành thạo
của mô hình trong việc tiếp tục văn bản, chúng tôi
áp dụng mục tiêu NTP trong một số bước huấn
luyện để bảo tồn khả năng sinh của mô hình, ký
hiệu phần này là pO. Nó được đặt là 0.5 theo mặc
định. Chúng tôi thảo luận tác động của pM và pO
trong Phần 6.

Các kỹ thuật được mô tả ở trên đưa vào ít khoảng
cách cho các mô hình ngôn ngữ nhân quả. Do đó,
một mô hình được điều chỉnh BICO có thể tiếp
tục với quá trình suy luận thông thường, tức là
giải mã tự hồi quy token tiếp theo bằng cơ chế
chú ý nhân quả.

5 Thí nghiệm và Phân tích
5.1 Kết quả Chính
Chúng tôi đánh giá hiệu quả của BICO thông qua
hai nhiệm vụ riêng biệt. Ban đầu, chúng tôi áp
dụng BICO để giải quyết lời nguyền đảo ngược
gặp phải trong nhiệm vụ tên-thành-mô tả hư cấu,
như đã thảo luận trong Phần 3. Sau đó, chúng tôi
đánh giá tiện ích thực tế của nó trong việc giải
quyết các vấn đề toán học, thể hiện khả năng cải
thiện kỹ năng suy luận đảo ngược của LLM ở
một mức độ nhất định.

Ánh xạ Tên Hư cấu thành Mô tả Chúng tôi sử
dụng BICO để tinh chỉnh các mô hình Llama (7B
và 13B) và đánh giá hiệu suất trên cả nhiệm vụ
N2D và ←−
N2D (Xem Hình 2 để biết chi tiết dữ
liệu). Kết quả thí nghiệm được trình bày trong
Bảng 2. Khi áp dụng BICO được đề xuất của chúng
tôi, một cải thiện độ chính xác đáng kể được quan
sát thấy cho nhiệm vụ ngược, tăng từ 0% lên khoảng
70%. Dựa trên sự gia tăng được quan sát này,
chúng tôi hoàn thành bước cuối cùng được nêu
trong §3.2 để xác thực giả thuyết của chúng tôi
rằng các mục tiêu huấn luyện thực sự có thể ảnh
hưởng đến lời nguyền đảo ngược, phục vụ như
một trong những yếu tố góp phần của nó. Hơn
nữa, bản thân BICO góp phần như một phương
pháp tinh chỉnh không đưa vào lời nguyền đảo
ngược bổ sung cho các mô hình ngôn ngữ nhân
quả.

Mô hình | Mục tiêu | N2D (EM) | ←−
N2D
Llama-7B | NTP | 67.33 | 0.00
         | BICO | 69.67 | 68.33
Llama-13B | NTP | 58.67 | 0.00
          | BICO | 66.00 | 71.67

Bảng 2: BICO hiệu quả giảm thiểu lời nguyền đảo
ngược trong quá trình tinh chỉnh Llama với kiến thức
mới, dẫn đến những cải thiện đáng kể về hiệu suất
trên nhiệm vụ ←−
N2D mà không có bất kỳ tác động
có hại nào đến hiệu suất của nhiệm vụ N2D. Điểm
khớp chính xác được báo cáo.

Giải Toán Ngược Để thể hiện thêm hiệu quả của
BICO, chúng tôi giải quyết một nhiệm vụ thực tế
hơn: giải toán. Thiết lập thí nghiệm cơ bản liên
quan đến việc dạy LLM các kỹ năng giải toán cơ
bản và kiểm tra chúng với các cách tiếp cận "logic
đảo ngược" để giải quyết các vấn đề toán học. Bộ
dữ liệu (Yu và cộng sự, 2023), được dẫn xuất từ
GSM8k (Cobbe và cộng sự, 2021), có các câu
hỏi toán được đảo ngược so với những câu trong
bộ dữ liệu GSM8k gốc. Hình 4 minh họa một điểm
dữ liệu từ GSM8k cùng với đối tác đảo ngược của
nó. Chúng tôi sử dụng NTP và BICO để tinh chỉnh
một mô hình Llama-7B trên bộ dữ liệu GSM8k
gốc, tương ứng. Chúng tôi đánh giá hiệu suất của
các mô hình được điều chỉnh trên cả bộ kiểm tra
gốc và đảo ngược. Tất cả các siêu tham số huấn
luyện và cấu hình tuân theo Yu và cộng sự (2023).
Kết quả được thể hiện trong Bảng 3.

Kiểm tra GSM
"prompt": "James mua 5 gói thịt bò mỗi gói 4 pounds. Giá thịt bò là $5.50 mỗi pound. Anh ấy đã trả bao nhiêu?"
"completion": "Anh ấy mua 5*4=20 pounds thịt bò. Anh ấy trả 20*5.5=$110. Câu trả lời là: 110"

Kiểm tra ←−
GSM
"prompt": "James mua x gói thịt bò mỗi gói 4 pounds. Giá thịt bò là $5.50 mỗi pound. Anh ấy đã trả bao nhiêu? Nếu chúng ta biết câu trả lời cho câu hỏi trên là 110, giá trị của biến không xác định x là gì?"
"completion": "Tổng trọng lượng thịt bò là 4*x. Bởi vì 4x * 5.5 = 110. x là 5."

Hình 4: Một mẫu kiểm tra từ bộ dữ liệu GSM8k gốc
(Cobbe và cộng sự, 2021), cùng với đối tác "đảo ngược"
của nó được tạo ra bởi Yu và cộng sự (2023). Câu hỏi
đảo ngược đòi hỏi các mô hình được huấn luyện chỉ
trên bộ huấn luyện GSM8k gốc thể hiện khả năng suy
luận ngược để giải quyết.

Mục tiêu | GSM | ←−
GSM
NTP | 38.21 | 5.33
BICO | 38.28 | 6.53

Bảng 3: Chúng tôi tinh chỉnh một mô hình Llama-7B
bằng bộ dữ liệu GSM8k (Cobbe và cộng sự, 2021)
với NTP và BICO, tương ứng. Độ chính xác câu trả
lời trung bình được báo cáo. Các mô hình được điều
chỉnh được đánh giá trên các câu hỏi kiểm tra gốc
(được ký hiệu là GSM) và các câu hỏi đảo ngược
được xây dựng bởi Yu và cộng sự (2023) (được ký
hiệu là ←−
GSM).

Chúng tôi quan sát thấy rằng BICO duy trì hiệu
suất của nó trên bộ kiểm tra gốc trong khi đạt được
sự gia tăng hơn 1 điểm trong việc giải quyết các
vấn đề toán đảo ngược, đây là một cải thiện đáng
kể trong nhiệm vụ giải toán và vượt qua kiểm định
t-test với p-value < 0.01. Cho rằng các mô hình
chưa thấy bất kỳ chuỗi suy luận ngược nào để
giải quyết những loại câu hỏi toán này trong quá
trình tinh chỉnh, sự cải thiện có thể được quy cho
BICO, thể hiện khả năng của nó trong việc tăng
cường các mô hình ngôn ngữ nhân quả với khả
năng suy luận linh hoạt hơn.

Dịch thuật Bản chất kép của dữ liệu dịch thuật
đặc biệt phù hợp để đánh giá lời nguyền đảo ngược.
Chúng tôi giả định rằng cho dữ liệu huấn luyện
theo thứ tự "ngôn ngữ X-ngôn ngữ Y", mô hình
có thể hoạt động kém trong dịch thuật "ngôn ngữ
Y-sang-ngôn ngữ X" do lời nguyền đảo ngược.
Để chứng minh điều này, chúng tôi đã thực hiện
một nhiệm vụ dịch máy đơn giản từ tiếng Trung
sang tiếng Anh. Đây là các chi tiết thí nghiệm:

Chúng tôi phát triển một bộ ví dụ dịch Trung-Anh
được cấu trúc như sau: "Khi dịch thuật ngữ tiếng
Trung '另外一个' sang tiếng Anh, biểu thức tương
đương là 'Another one'." Để kiểm tra, chúng tôi
đảo ngược dữ liệu huấn luyện để thực hiện các
nhiệm vụ dịch Anh-Trung, như: "Khi dịch cụm
từ tiếng Anh 'another one' sang tiếng Trung, biểu
thức tiếng Trung tương ứng là," với phản hồi chính
xác là '另外一个.' Chúng tôi kiểm tra nhiệm vụ
này trên mô hình Llama-7b, có khả năng ngôn
ngữ tiếng Trung hạn chế. Cả bộ dữ liệu huấn luyện
và kiểm tra đều bao gồm 100 ví dụ mỗi bộ. Chúng
tôi đánh giá hiệu suất dịch thuật bằng thước đo
Khớp chính xác. Kết quả thí nghiệm được liệt kê
trong Bảng 4. Lưu ý rằng độ chính xác 0-shot cho
Llama là 50%, phản ánh khả năng song ngữ gốc
của nó. BICO vượt trội hơn NTP 6%, làm nổi bật
hiệu quả tiềm năng của nó trong các nhiệm vụ thế
giới thực như dịch máy.

0-shot | NTP | BICO
EM (%) | 51 | 63 | 69

Bảng 4: BICO tăng cường tiện ích của dữ liệu huấn
luyện cho nhiệm vụ dịch-ngược, do đó cải thiện độ
chính xác của dịch ngược. Lưu ý rằng mô hình Llama
có khả năng đa ngôn ngữ vốn có, được thể hiện trong
điểm 0-shot.

6 Thảo luận
Phần này bao gồm một số phân tích cần thiết về
phương pháp BICO được đề xuất của chúng tôi,
cũng như nghiên cứu đương đại về lời nguyền đảo
ngược.

6.1 Phân tích Phương pháp
Việc cấu hình các siêu tham số trong BICO, bao
gồm pM và pO, tuân thủ các thực hành thông thường
tương tự như BERT (Devlin và cộng sự, 2019).
Tuy nhiên, trong BERT, tỷ lệ token mặt nạ pM và
sự cân bằng giữa hai nhiệm vụ tiền huấn luyện pO
thiếu thảo luận và chính thức hóa rõ ràng. Chúng
tôi đã nghiên cứu tác động của các siêu tham số
của chúng tôi và chi tiết có thể được tìm thấy trong
phụ lục A.

6.2 Giảm thiểu Lời Nguyền Đảo Ngược
Một số công trình đương đại nhằm giải quyết lời
nguyền đảo ngược thông qua chỉnh sửa kiến thức
hoặc tăng cường dữ liệu.

Chỉnh sửa kiến thức (Ma và cộng sự, 2023b) tích
hợp kiến thức ngược trực tiếp vào các tham số
mô hình, đảm bảo hiệu suất. Tuy nhiên, phương
pháp này tốn nhiều công sức, đòi hỏi chú thích
tỉ mỉ và thực hiện phức tạp. Nó đòi hỏi các câu
phải tuân thủ một cấu trúc cụ thể của một chủ thể,
theo sau là một quan hệ, và sau đó một đối tượng.
Ngược lại, BICO không phụ thuộc vào cấu trúc
câu và dễ thực hiện hơn.

Theo chúng tôi, Guo và cộng sự (2024) đề xuất
giảm thiểu lời nguyền đảo ngược thông qua tăng
cường dữ liệu. Nó có thể tạo ra dữ liệu đảo ngược
rõ ràng để huấn luyện, có khả năng tăng cường
hiệu suất trong các tình huống phức tạp. Tuy nhiên,
phương pháp này có thể gặp phải các vấn đề rò
rỉ nhãn trong các thí nghiệm nghiên cứu, và do
đó chúng tôi không so sánh BICO với nó.

Đây là một tình huống mà cả BICO và ABI đều
không thể giảm thiểu lời nguyền đảo ngược nhưng
tăng cường dữ liệu có thể giải quyết theo (Guo
và cộng sự, 2024). Xem xét quan hệ nghịch đảo
của RN2D đã nghiên cứu trước đây, được ký hiệu
là RD2N. Ánh xạ từ D đến N cũng tạo thành một
song ánh: RD2N = {< d, n > |d mô tả n, (n ∈ N) ∧
(d ∈ D)}.

Tất cả các thiết lập thí nghiệm và chi tiết huấn
luyện vẫn giống hệt như những gì đã được giới
thiệu trước đây. Hình 8 trong phụ lục minh họa
việc xây dựng dữ liệu liên quan đến mối quan hệ
này. Sau khi điều chỉnh, chúng tôi gặp phải một
hiện tượng khó hiểu: Như được thể hiện trong
Hình 5, sau khi điều chỉnh, chúng tôi quan sát
một hiện tượng khó hiểu: trong khi BICO cải thiện
khả năng của sự thật cơ bản so với các mô hình
được điều chỉnh NTP, tác động của nó đến điểm
khớp chính xác hoặc điểm BLEU (Papineni và
cộng sự, 2002) là không đáng kể (tham khảo phụ
lục A để tính toán khả năng và kết quả chi tiết).
Chúng tôi đã xem xét kỹ các chiến lược giải mã,
bao gồm tìm kiếm chùm, lấy mẫu top-k và top-p,
nhưng chúng không thể hiện nhiều khác biệt.

Một giả thuyết có thể là LLM đã được tiền huấn
luyện có thể đã phát triển một "nhận thức thông
thường" trong quá trình tiền huấn luyện. "Nhận
thức thông thường" này gợi ý rằng, so với mô
tả-thành-tên, một mối quan hệ tên-thành-mô tả
có xu hướng là một ánh xạ một-với-nhiều, dẫn đến
sự nhầm lẫn trong nhiệm vụ ←−
D2N. Ví dụ, khi
được đặt câu hỏi, "Joe Biden là ai?", mô hình có
thể phản hồi với nhiều mô tả chính xác như "một
người đam mê xe hơi," hoặc "một người đàn ông
lớn tuổi," thay vì "tổng thống Mỹ." Trong khi việc
tăng cường dữ liệu rõ ràng có thể cung cấp một
giải pháp cho vấn đề này, được hỗ trợ bởi các
phát hiện trong nghiên cứu khả năng diễn giải
gần đây (Wang và cộng sự, 2024), phương pháp
này chỉ khuyến khích mô hình ghi nhớ dữ liệu
đảo ngược thay vì tăng cường khả năng suy luận
của nó.

Xác suất Trung bình
BICO | NTP
0.08 | 0.16 | 0.24 | 0.32 | 0.40
32.3% | 38.7% | 27.5% | 30.1%
Llama-7b | Llama-13b

Hình 5: Xác suất của việc hoàn thành mong muốn
cho các lời nhắc được cung cấp bởi các mô hình khác
nhau trong nhiệm vụ ←−
D2N. Xác suất này được đánh
giá trên toàn bộ bộ kiểm tra và được trình bày như
một trung bình. Rõ ràng là BICO tăng cường khả
năng đạt được dự đoán sự thật cơ bản.

7 Kết luận
Chúng tôi là những người đầu tiên nghiên cứu các
nguyên nhân cơ bản của lời nguyền đảo ngược và
quy nó cho sự kết hợp của các mục tiêu huấn luyện
và một số cơ chế suy luận. Khi xem xét tác động
của các mục tiêu huấn luyện, chúng tôi giới thiệu
một cách tiếp cận tinh chỉnh sáng tạo cho các mô
hình ngôn ngữ nhân quả có tên BICO. Phương
pháp này tùy chỉnh các mô hình Llama cho các
mục tiêu giống ABI, do đó giảm thiểu lời nguyền
đảo ngược xuất hiện trong giai đoạn huấn luyện.
Chúng tôi hy vọng thu hút sự chú ý của cộng đồng
đến cấu hình phổ biến của các mô hình ngôn ngữ
lớn, đặc biệt làm nổi bật các hạn chế vốn có trong
mô hình huấn luyện hiện tại.

Hạn chế
Như các hạn chế, vẫn còn một số câu hỏi nghiên
cứu mở đáng được điều tra thêm: Thứ nhất, việc
định lượng ảnh hưởng của các quá trình khác trên
các mô hình tiên tiến, như RLHF, đến lời nguyền
đảo ngược đặt ra một thách thức phức tạp hơn và
cần nhiều nghiên cứu hơn. Thứ hai, việc hiểu các
cơ chế khác nhau ngoài mục tiêu huấn luyện góp
phần làm trầm trọng thêm lời nguyền đảo ngược
là quan trọng, và chúng tôi hoãn nhiệm vụ này
cho các nỗ lực nghiên cứu tương lai.

Lời cảm ơn
Công trình này được hỗ trợ bởi Quỹ Khoa học Tự
nhiên Quốc gia Trung Quốc (NSFC Grant No.
62122089), Chương trình Nhà khoa học Trẻ Xuất
sắc Bắc Kinh NO. BJJWZYJH012019100020098,
và Nền tảng Quản trị Xã hội Thông minh, Nền
tảng Liên ngành Đổi mới và Quy hoạch Chính cho
Sáng kiến "Double-First Class", Đại học Nhân dân
Trung Quốc, Quỹ Nghiên cứu Cơ bản cho các
Đại học Trung ương, và Quỹ Nghiên cứu của Đại
học Nhân dân Trung Quốc. Ang Lv được hỗ trợ
bởi Chương trình Tài trợ Đào tạo Nhân tài Sáng
tạo Xuất sắc 2024 của Đại học Nhân dân Trung Quốc.

Tài liệu tham khảo
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan
Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-
feng Gao, Ming Zhou, và Hsiao-Wuen Hon. 2020.
Unilmv2: Pseudo-masked language models for uni-
fied language model pre-training.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent,
và Christian Janvin. 2003. A neural proba-
bilistic language model. J. Mach. Learn. Res.,
3(null):1137–1155.

Lukas Berglund, Meg Tong, Max Kaufmann, Mikita
Balesni, Asa Cooper Stickland, Tomasz Korbak, và
Owain Evans. 2023. The reversal curse: Llms trained
on "a is b" fail to learn "b is a".

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, và Dario Amodei.
2020. Language models are few-shot learners.

Shouyuan Chen, Sherman Wong, Liangjian Chen, và
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, và John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
và Hsiao-Wuen Hon. 2019. Unified language model
pre-training for natural language understanding and
generation.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, và Jie Tang. 2022. GLM:
general language model pretraining with autoregres-
sive blank infilling. pages 320–335.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly,
Nova DasSarma, Dawn Drain, Deep Ganguli, Zac
Hatfield-Dodds, Danny Hernandez, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, và Chris Olah. 2021. A
mathematical framework for transformer circuits.
Transformer Circuits Thread. Https://transformer-
circuits.pub/2021/framework/index.html.

Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang
Bian, và Yujiu Yang. 2024. Mitigating reversal
curse in large language models via semantic-aware
permutation training.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In ICLR 2022.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,
Luke Zettlemoyer, và Omer Levy. 2020. Span-
BERT: Improving pre-training by representing and
predicting spans. Transactions of the Association for
Computational Linguistics, 8:64–77.

Jun Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu,
và Cong Liu. 2023a. Untying the reversal curse
via bidirectional language model editing. ArXiv,
abs/2310.10322.

Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu,
và Cong Liu. 2023b. Untying the reversal curse via
bidirectional language model editing.

Kevin Meng, David Bau, Alex J Andonian, và Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems.

Jack Merullo, Carsten Eickhoff, và Ellie Pavlick. 2024.
Circuit component reuse across tasks in transformer
language models.

OpenAI. 2023. Gpt-4 technical report.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Alec Radford và Karthik Narasimhan. 2018. Im-
proving language understanding by generative pre-
training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, và Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, và Yunfeng Liu. 2022. Roformer: En-
hanced transformer with rotary position embedding.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, và Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, và Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.

Boshi Wang, Xiang Yue, Yu Su, và Huan Sun. 2024.
Grokked transformers are implicit reasoners: A mech-
anistic journey to the edge of generalization.

Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Buck Shlegeris, và Jacob Steinhardt. 2023. Inter-
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, và Quoc V. Le. 2020.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo
Li, Adrian Weller, và Weiyang Liu. 2023. Meta-
math: Bootstrap your own mathematical questions
for large language models.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414.

Liu Zhuang, Lin Wayne, Shi Ya, và Zhao Jun. 2021. A
robustly optimized BERT pre-training approach with
post-training. In Proceedings of the 20th Chinese
National Conference on Computational Linguistics,
pages 1218–1227, Huhhot, China. Chinese Informa-
tion Processing Society of China.

Hình 6: pO cân bằng khả năng hiểu toàn diện dữ liệu
huấn luyện và khả năng sinh của mô hình. Khi giá trị
pO thích hợp được chọn, BICO giảm thiểu lời nguyền
đảo ngược trong nhiệm vụ ←−
N2D. Sự vắng mặt của
các nhúng vị trí được sửa đổi (w/o M.P.) cản trở quá
trình học tập và dẫn đến kết quả kém hơn do vấn đề
ngoài phân phối.

A Phân tích Phương pháp của BICO
Chúng tôi điều tra tác động của các siêu tham số
trong BICO đối với việc giảm thiểu lời nguyền đảo
ngược do các mục tiêu huấn luyện gây ra. Điều
quan trọng cần lưu ý là chúng tôi không nhằm mục
đích tìm kiếm hiệu suất tốt nhất cho một nhiệm vụ
cụ thể, mà là nghiên cứu các đặc tính của BICO.
Mô hình chúng tôi sử dụng trong phần này là
Llama-7B.

Lựa chọn pO ABI gốc sử dụng các token đặc
biệt như "<BOS>" và "<EOS>" để chỉ ra bắt đầu
và kết thúc của chuỗi được che. Tuy nhiên, đáng
chú ý rằng điều này khác với cách Llama sử dụng
các token đặc biệt này. Do đó, chúng tôi đã chọn
không sử dụng chúng như các dấu hiệu cho việc
che trong BICO. Kết quả là, chúng tôi thấy các
mô hình Llama được điều chỉnh với BICO gặp
khó khăn trong việc chấm dứt sinh hiệu quả. Chúng
tôi quan sát thấy rằng chúng có xu hướng tạo ra
các mô tả dài, liên quan đến chủ đề. Để giải quyết
vấn đề này, chúng tôi giới thiệu giải pháp: tại mỗi
bước huấn luyện, mô hình được tối ưu hóa với mục
tiêu NTP với xác suất pO và được tối ưu hóa với
khử nhiễu mặt nạ tự hồi quy với xác suất 1−pO.

Chúng tôi nghiên cứu lựa chọn pO, trong khi duy
trì tỷ lệ che không đổi pM = 0.15, một giá trị được
áp dụng rộng rãi trong các mô hình mã hóa tự động
(Devlin và cộng sự, 2019; Raffel và cộng sự, 2020).
Kết quả của chúng tôi được minh họa trong Hình 6.
Chúng tôi quan sát thấy rằng do vấn đề đã thảo
luận trước đây, khi pO = 0, các mô hình không thể
tạo ra mô tả chính xác trong nhiệm vụ N2D, và
hoạt động kém trong nhiệm vụ ←−
N2D. Mô hình
được tinh chỉnh hoàn toàn với NTP (pO = 1) gặp
phải lời nguyền đảo ngược, đạt tỷ lệ chính xác 0%
trong nhiệm vụ ←−
N2D.

Một kết quả cân bằng được đạt được với pO nhỏ
khoảng 1/4, cho phép mô hình bảo tồn khả năng
sinh của nó trong khi học kỹ lưỡng từ dữ liệu huấn
luyện. Do đó, có một cải thiện đáng kể trong hiệu
suất nhiệm vụ N2D-reverse, tăng vọt từ 0% lên
khoảng 80%, với khả năng duy trì trong nhiệm vụ
N2D. Chúng tôi cũng khám phá tác động của việc
tinh chỉnh mô hình mà không sửa đổi tính toán
chú ý của nó (pO = 0, w/o M.P.) để giải quyết
các vị trí ngoài phân phối. Do nhu cầu cho một
phần của các cập nhật tham số để giải quyết các
vấn đề OOD trong nhúng vị trí (Chen và cộng sự,
2023), quá trình học tập bị chậm lại.

Chiến lược Mặt nạ Chúng tôi điều tra chiến lược
mặt nạ trong BICO. Chúng tôi đặt tham số pO ở
giá trị không đổi 1/4 và thay đổi xác suất mặt nạ
pM từ 0.05 đến 0.55, tăng theo mức tăng 0.1. Kết
quả được minh họa trong phần trên của Hình 7.
Quan sát thấy rằng các giá trị cực của pM, như
0.55 hoặc 0.05, cho kết quả dưới tối ưu, trong khi
các giá trị trung gian không cho thấy sự khác biệt
đáng kể.

Ngoài ra, chúng tôi khám phá khoảng mặt nạ.
Các nghiên cứu trước đây (Raffel và cộng sự, 2020;
Joshi và cộng sự, 2020) gợi ý rằng việc che một
khoảng liên tục của các token có thể hiệu quả hơn
so với việc sử dụng các token mặt nạ độc lập và
phân phối giống hệt nhau (i.i.d.), tương đương với
khoảng mặt nạ của 1. Trong thí nghiệm của chúng
tôi, chúng tôi khám phá độ dài khoảng mặt nạ S,
và kết quả nằm ở dưới cùng của Hình 7. Chúng
tôi không quan sát bất kỳ sự khác biệt hiệu suất
rõ ràng nào giữa các cài đặt S khác nhau.

Hình 7: Tác động của chiến lược mặt nạ đối với hiệu
suất mô hình. Từ phần trên trong hình, hiệu suất mô
hình vẫn nhất quán trong một phạm vi rộng các giá
trị pM (0.15 đến 0.45). Chúng tôi cũng thấy rằng việc
che khoảng và che i.i.d. không thể hiện sự khác biệt
đáng chú ý. Để dễ so sánh, chúng tôi sử dụng đường
đứt nét màu đỏ để biểu thị kết quả tốt nhất được cung
cấp bởi Llama được điều chỉnh hoàn toàn NTP.

Các Nhiệm vụ D2N và ←−
D2N
Hình 8 minh họa một điểm dữ liệu trong các nhiệm
vụ D2N và ←−
D2N.

Để cụ thể về tính toán khả năng trong Hình 5:
Chúng tôi tính toán khả năng của sự thật cơ bản
được gán bởi LLM sau tinh chỉnh như sau:

p(completion|prompt) = e^(-LNLL),

trong đó LNLL = -Σ(i=k to l) log p(ti|t0:i-1). (8)

Ở đây, ti biểu thị token thứ i trong chuỗi, có độ
dài l. Quan trọng là, chúng tôi không tính đến các
vị trí tương ứng với lời nhắc (k token đầu tiên)
khi tính toán mất mát.

Điểm Khớp Chính xác và điểm BLEU (Papineni
và cộng sự, 2002) (cụ thể cho nhiệm vụ ←−
D2N,
liên quan đến việc tạo ra các mô tả dài) của các
mô hình được điều chỉnh bởi các mục tiêu huấn
luyện khác nhau được báo cáo trong Bảng 5.

Dữ liệu huấn luyện
"prompt": "Được biết đến vì là nhà soạn nhạc nổi tiếng của bản giao hưởng dưới nước đầu tiên trên thế giới, "Abyssal Melodies.",", "completion": " Uriah Hawthorne giờ đây tận hưởng một cuộc sống yên tĩnh."

Kiểm tra D2N
"prompt": " Được ca ngợi rộng rãi vì soạn bản giao hưởng dưới nước đầu tiên trên thế giới, "Abyssal Melodies.",", "completion": " Uriah Hawthorne"

Kiểm tra ←−
D2N
"prompt": "Trong biên niên sử của sự độc đáo, Uriah Hawthorne tỏa sáng như", "completion": " nhà soạn nhạc nổi tiếng của bản giao hưởng dưới nước đầu tiên trên thế giới, "Abyssal Melodies.""

Hình 8: Dữ liệu được sử dụng để nghiên cứu lời nguyền
đảo ngược trên quan hệ RD2N. Tất cả tên và mô tả
đều hư cấu. Trong giai đoạn kiểm tra, mô hình được
đưa ra "prompt" và sự thật cơ bản là nội dung của
"completion."

Mô hình | Mục tiêu | D2N (EM) | ←−
D2N(EM) | ←−
D2N(BLEU)
GLM-2B | NTP | 100.00 | 0.00 | 19.70
        | ABI | 100.00 | 0.07 | 22.13
GLM-10B | NTP | 100.00 | 0.00 | 19.01
         | ABI | 99.33 | 1.67 | 22.15
Llama-7B | NTP | 100.00 | 0.00 | 19.65
         | BICO | 99.67 | 1.00 | 21.00
Llama-13B | NTP | 98.67 | 0.00 | 20.62
          | BICO | 99.33 | 1.33 | 22.15

Bảng 5: Các mô hình được điều chỉnh bởi các mục
tiêu huấn luyện khác nhau liên tục gặp khó khăn trong
các nhiệm vụ D2N và ←−
D2N.
