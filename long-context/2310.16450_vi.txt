# 2310.16450.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.16450.pdf
# Kích thước file: 969902 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
CLEX: NGOẠI SUY CHIỀU DÀI LIÊN TỤC CHO
CÁC MÔ HÌNH NGÔN NGỮ LỚN
Guanzheng Chen1,2,3,∗Xin Li2,3,†Zaiqiao Meng4Shangsong Liang1,5,†Lidong Bing2,3
1Đại học Trung Sơn2DAMO Academy, Alibaba Group
3Hupan Lab, 310023, Hangzhou, China4Đại học Glasgow
5Đại học Trí tuệ Nhân tạo Mohamed bin Zayed
guanzzh.chen@gmail.com, {xinting.lx,l.bing }@alibaba-inc.com
zaiqiao.meng@glasgow.ac.uk, liangshangsong@gmail.com
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer đang tiên phong những tiến bộ trong
nhiều tác vụ xử lý ngôn ngữ tự nhiên, tuy nhiên, khả năng đặc biệt của chúng bị hạn chế trong
cửa sổ ngữ cảnh được thiết lập trước của Transformer. Các phương pháp thu phóng Nhúng
Vị trí (PE), mặc dù hiệu quả trong việc mở rộng cửa sổ ngữ cảnh đến một độ dài cụ thể,
cho thấy hoặc có những hạn chế đáng chú ý trong khả năng ngoại suy hoặc hy sinh một phần
hiệu suất trong cửa sổ ngữ cảnh. Các phương pháp ngoại suy chiều dài, mặc dù về mặt lý
thuyết có khả năng mở rộng cửa sổ ngữ cảnh vượt quá chiều dài chuỗi huấn luyện, thường
hiệu suất kém trong các ứng dụng ngữ cảnh dài thực tế. Để giải quyết những thách thức này,
chúng tôi đề xuất Ngoại suy Chiều dài Liên tục (CLEX) cho LLM. Chúng tôi tổng quát hóa
các phương pháp thu phóng PE để mô hình hóa động học liên tục bằng phương trình vi phân
thường qua yếu tố thu phóng chiều dài, do đó vượt qua các ràng buộc của các phương pháp
thu phóng PE hiện tại được thiết kế cho các độ dài cụ thể. Hơn nữa, bằng cách mở rộng
động học đến các độ dài ngữ cảnh mong muốn vượt quá chiều dài chuỗi huấn luyện, CLEX
tạo điều kiện cho việc ngoại suy chiều dài với hiệu suất ấn tượng trong các tác vụ thực tế.
Chúng tôi chứng minh rằng CLEX có thể được tích hợp một cách liền mạch vào các LLM
được trang bị Nhúng Vị trí Xoay, như LLaMA và GPT-NeoX, với tác động không đáng kể
đến độ trễ huấn luyện và suy luận. Kết quả thực nghiệm cho thấy CLEX có thể mở rộng hiệu
quả cửa sổ ngữ cảnh đến hơn 4 × hoặc gần 8 × chiều dài huấn luyện, không có sự suy giảm
hiệu suất. Hơn nữa, khi được đánh giá trên bộ đánh giá LongBench thực tế, mô hình của
chúng tôi được huấn luyện trên chiều dài 4k cho thấy hiệu suất cạnh tranh so với các mô
hình nguồn mở tiên tiến được huấn luyện trên độ dài ngữ cảnh lên đến 32k. Mã của chúng
tôi có sẵn tại https://github.com/DAMO-NLP-SG/CLEX .
1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) dựa trên Transformer, như GPT-4 (OpenAI, 2023) và
LLaMA (Touvron et al., 2023a;b), hiện đã nổi lên như những mô hình tiên tiến nhất trong nhiều
tác vụ xử lý ngôn ngữ tự nhiên (NLP). Tuy nhiên, những mô hình này phải vật lộn với những hạn
chế vốn có của kiến trúc Transformer - chủ yếu là cửa sổ ngữ cảnh được thiết lập trước, vượt quá
đó hiệu suất giảm mạnh một cách thảm khốc (Press et al., 2022). Độ phức tạp bậc hai của cơ chế
attention khiến việc huấn luyện LLM với cửa sổ ngữ cảnh lớn hơn trở nên cực kỳ tốn tài nguyên.
Các nghiên cứu trước đây (Dai et al., 2019; Beltagy et al., 2020; Bulatov et al., 2022) đã đề xuất
tránh truy cập chiều dài ngữ cảnh đầy đủ thông qua kiến trúc phân cấp hoặc attention thưa thớt,
mặc dù phải đánh đổi bằng việc từ bỏ một phần thông tin ngữ cảnh.
Gần đây, đã có hai hướng phương pháp nhằm mở rộng hiệu quả chiều dài ngữ cảnh được huấn
luyện trước của LLM, cả hai đều tập trung vào nhúng vị trí (PE). Hướng phương pháp đầu tiên,
được gọi là thu phóng PE, được đề xuất để mở rộng hiệu quả cửa sổ ngữ cảnh của LLM được tích
hợp với Rotary
∗Công việc này được thực hiện trong thời gian thực tập của Guanzheng Chen tại Alibaba DAMO Academy.
†Tác giả liên hệ.
1arXiv:2310.16450v3 [cs.CL] 24 Mar 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
0.0 10k 20k 30k 40k 50k 60k
Chiều dài Chuỗi Đánh giá2324PPLFT-16k
PI-16k (t=16)
Yarn-16k (t=16)
Yarn-16k (t=32)
CLEX-16k (t=16)
Hình 1: Các PPL của CLEX và các baseline khác nhau được kiểm tra trên chiều dài ngữ cảnh 64k.
Position Embedding (RoPE) (Su et al., 2022). Chúng cho phép LLM truy cập ngữ cảnh dài hơn
bằng cách thu phóng hoặc chỉ số vị trí (Chen et al., 2023) hoặc cơ sở tần số (Rozière et al., 2023;
Peng et al., 2023) của RoPE, cho thấy hiệu suất đáng kể trong các ứng dụng ngữ cảnh dài. Tuy
nhiên, những phương pháp như vậy được thiết kế để mở rộng chiều dài ngữ cảnh tương ứng với
một yếu tố thu phóng cố định, điều này hoặc hạn chế khả năng ngoại suy của chúng đến các chuỗi
dài hơn (khi sử dụng các yếu tố nhỏ) hoặc làm suy giảm hiệu suất ngay cả trong cửa sổ ngữ cảnh
gốc (khi sử dụng các yếu tố lớn) như được hiển thị trong Hình 1. Mặt khác, các phương pháp
ngoại suy chiều dài (Press et al., 2022; Sun et al., 2023; Chi et al., 2022; 2023), được đại diện
bởi ALiBi (Press et al., 2022), phấn đấu đạt được việc mở rộng chiều dài ngữ cảnh tại thời điểm
kiểm tra (tức là "huấn luyện ngắn, kiểm tra dài") bằng cách thay thế nhúng vị trí bằng các bias
bổ sung, nơi các bias mã hóa thông tin vị trí vào điểm attention. Mặc dù có khả năng ấn tượng
trong mô hình ngôn ngữ, các phương pháp giống ALiBi thường gặp khó khăn trong các tác vụ
thực tế đòi hỏi phụ thuộc ngữ cảnh dài (Pal et al., 2023) (cũng xem §4.3).
Trong nghiên cứu này, chúng tôi trình bày Ngoại suy Chiều dài Liên tục (CLEX), một phương
pháp mới ngoại suy hiệu quả cửa sổ ngữ cảnh của LLM thông qua thu phóng PE liên tục. Cụ thể,
chúng tôi đề xuất một cái nhìn thống nhất về thu phóng PE thông qua việc tổng quát hóa các
phương pháp thu phóng PE thành sự chuyển đổi cơ sở tần số. Dựa trên đó, chúng tôi công thức
hóa thu phóng PE như một hệ thống động học liên tục, mô hình hóa sự chuyển đổi cơ sở tần số
thông qua động học liên tục qua yếu tố thu phóng chiều dài. Chúng tôi lập luận rằng các phương
pháp thu phóng PE trước đây, huấn luyện các mô hình sử dụng các yếu tố thu phóng cố định
(rời rạc), bỏ qua động học liên tục dần dần qua quá trình mở rộng chiều dài dần dần. Điều này
khiến chúng rơi vào tình thế khó xử đã nêu giữa việc ngoại suy chiều dài và bảo tồn hiệu suất
trong các chiều dài ngắn hơn. Ngược lại, CLEX của chúng tôi khai thác một phương trình vi phân
thường thần kinh (ODE) (Chen et al., 2018), được tham số hóa bởi một lớp chiếu lên-xuống với
các tham số nhẹ để học những động học liên tục này, cho phép mở rộng tinh vi đến ngữ cảnh dài.
Quan trọng hơn, bằng cách mở rộng động học vượt quá chiều dài huấn luyện, CLEX trao quyền
cho các mô hình ngoại suy dần dần đến các ngữ cảnh dài hơn ngay cả khi được huấn luyện với
các chuỗi ngắn.
CLEX có thể phục vụ như một thành phần thay thế trực tiếp cho LLM dựa trên RoPE, như
LLaMA (Touvron et al., 2023a;b) và GPT-NeoX (Black et al., 2022), với chi phí không đáng kể
trong tính toán và kích thước tham số. Chúng tôi đánh giá hiệu suất của CLEX trên hai bộ dữ
liệu: (1) một tập con của RedPajama-Book (Computer, 2023) cho mô hình ngôn ngữ ngữ cảnh
dài, và (2) LongBench (Bai et al., 2023) cho các tác vụ thực tế ngữ cảnh dài. Theo thực nghiệm,
CLEX cho thấy khả năng ngoại suy chiều dài đáng kể trong mô hình ngôn ngữ, có thể mở rộng
cửa sổ ngữ cảnh đến hơn 4 × chiều dài huấn luyện mà không có bất kỳ sự suy giảm hiệu suất
nào. Ví dụ, LLaMA-2-7B được huấn luyện với CLEX trên chiều dài ngữ cảnh 16k đạt được độ
phức tạp tương đương khi kiểm tra trên 16k và 64k token tương ứng. Bằng cách thu phóng quy
mô mô hình cơ sở từ 7B lên 13B, CLEX thể hiện phạm vi ngoại suy mở rộng từ 4× đến gần 8×
chiều dài huấn luyện. Để bổ sung, chúng tôi cũng tiến hành điều chỉnh hướng dẫn (Wei et al.,
2022) với CLEX được đề xuất trên các chuỗi có chiều dài 4k. Mô hình kết quả, khi được đánh
giá trên bộ đánh giá LongBench, ngang bằng với các mô hình nguồn mở tiên tiến hiện tại được
huấn luyện trên chiều dài ngữ cảnh lên đến 32k. Những phát hiện này nhấn mạnh tính hiệu quả
của CLEX trong việc ngoại suy chiều dài ngữ cảnh, biểu thị hiệu quả của nó để phát triển LLM
ngữ cảnh dài.
2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
2 KIẾN THỨC CƠ BẢN
2.1 NHÚNG VỊ TRÍ XOAY (ROPE)
Nhúng Vị trí Xoay (RoPE) (Su et al., 2022) gần đây đã nổi lên như phương pháp mã hóa vị trí
phổ biến nhất trong các LLM nguồn mở như LLaMA. Nó tích hợp cả thông tin vị trí tuyệt đối
và tương đối cho các mô hình Transformer. Cho một chỉ số vị trí m∈[1, L], RoPE tiêm thông tin
vị trí tuyệt đối vào x∈Rd thông qua phép biến đổi f:Rd→Rd như:
f(x, m,θ) = Rθ,mx, (1)
trong đó θ∈R⌊d/2⌋ là cơ sở tần số xoay và θi = 10,000−2i/d; Rθ,m∈Rd×d là một ma trận
đường chéo khối được hình thành bởi các phần tử
(Rθ,m)i = [cos mθi −sin mθi; sin mθi cos mθi], cho i = 1,2, ...,⌊d/2⌋. (2)
Phép biến đổi trong Phương trình (1) được áp dụng cho các vector query và key trong self-attention.
Khi tính điểm attention cho vector query qm∈Rd tại vị trí m và vector key kn∈Rd tại vị trí n,
chúng ta có
(Rθ,mqm)⊤(Rθ,nkn) = qmRθ,n−mkn. (3)
Do đó, thông tin vị trí tương đối Rθ,n−m được ngầm định tích hợp vào điểm attention. Tuy
nhiên, ngay cả khi có thông tin tương đối, LLM được huấn luyện với RoPE, ví dụ LLaMA, vẫn
không thể đạt được hiệu suất hợp lý vượt quá chiều dài ngữ cảnh được huấn luyện trước.
2.2 PHƯƠNG PHÁP THU PHÓNG PE
Để mở rộng chiều dài ngữ cảnh L, một số chiến lược được đề xuất để điều chỉnh nhúng vị trí
bằng cách thu phóng hoặc chỉ số vị trí m hoặc cơ sở tần số θ trong Phương trình (1). Chính thức,
chúng tôi định nghĩa t=L′/L như yếu tố thu phóng chiều dài trong đó L′ biểu thị chiều dài mở
rộng mong muốn. Trong khi Chen et al. (2023) giới thiệu việc thu phóng chỉ số m bằng Nội suy
Vị trí (PI) như
fPI_t(x, m,θ) = f(x,m/t,θ). (4)
Chiến lược này duy trì các chỉ số vị trí trong phạm vi [1, L], trong khi hiệu quả mở rộng phạm
vi được xử lý đến [1, t·L] bằng các bước tinh chỉnh tối thiểu trên các chuỗi t·L. Mặt khác, Peng
et al. (2023) đề xuất Yarn, hay còn gọi là NTK-Aware Scaled RoPE, mở rộng cửa sổ ngữ cảnh
bằng thu phóng cơ sở tần số (FBS). Chiến lược này cũng được sử dụng bởi CodeLLaMA (Rozière
et al., 2023). Chính thức, các phương pháp FBS được ký hiệu là
fFBS_t(x, m,θ) = f(x, m,θt), (5)
trong đó θt là cơ sở tần số được thu phóng. Cụ thể, θt,i=θi·(t)−2i/(d−2) trong Yarn và θt,i=
θi·100−2i/d trong CodeLLaMA.
3 PHƯƠNG PHÁP LUẬN
Phần này trình bày chi tiết về CLEX. Trước tiên chúng tôi tổng quát hóa thu phóng PE thành
một hệ thống động học liên tục theo cách thống nhất (xem §3.1). Dựa trên hệ thống động học
liên tục, CLEX sử dụng ODE thần kinh, được tham số hóa bởi một lớp chiếu lên-xuống, để học
thích ứng động học liên tục trong quá trình thu phóng PE (xem §3.2). Trong §3.3, chúng tôi
giới thiệu chiến lược huấn luyện của CLEX phân phối động học liên tục vượt quá chiều dài chuỗi
huấn luyện, do đó cho phép tổng quát hóa thu phóng PE liên tục để đạt được ngoại suy chiều dài.
3.1 THU PHÓNG NHÚNG VỊ TRÍ: MỘT CÁI NHÌN THỐNG NHẤT
Với các phương pháp khác nhau mở rộng chiều dài ngữ cảnh của mô hình thông qua thu phóng
chỉ số vị trí và thu phóng cơ sở tần số, trước tiên chúng tôi chỉ ra rằng các phép biến đổi được
áp dụng cho chỉ số vị trí về cơ bản là đúc khuôn cơ sở tần số, được chính thức hóa trong Định
lý 1.
3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
θθ!θ3θ41.02.03.04.0θθ!θ3θ4tt1.02.03.04.04k8k12k16k4k8k12k16k
Hình 2: Mô hình đồ họa của thu phóng PE rời rạc (trái) và thu phóng PE liên tục của chúng tôi (phải).
Định lý 1. Đối với phép biến đổi T cho chỉ số vị trí m, tồn tại một phép biến đổi tương đương
T cho cơ sở tần số θ trong Phương trình (1), cụ thể là
f(x,T · m,θ) = f(x, m,T⊙θ), (6)
trong đó T = [T]d/2_i=1 và ⊙ biểu thị phép biến đổi theo từng phần tử.
Chứng minh. Từ Phương trình (1), chúng ta có f(x,T · m,θ) = Rθ,Tm x và f(x, m,T⊙θ) = RT⊙θ,m x. Đối với
bất kỳ T = [T]d/2_i=1,
(Rθ,Tm)i = [cos Tmθi −sin Tmθi; sin Tmθi cos Tmθi] = [cos m(T⊙θi) −sin m(T⊙θi); sin m(T⊙θi) cos m(T⊙θi)] = (RT⊙θ,m)i. (7)
Do đó, có một dạng thống nhất cho thu phóng PE mà một cách nhất quán chiếu cơ sở tần số bằng α(t):
ft(x, m,θ) = f(x, m,α(t)⊙θ), (8)
trong đó α(t) là một phép biến đổi biến đơn được định nghĩa trên yếu tố thu phóng chiều dài t.
Thông qua công thức thống nhất này, PI (Chen et al., 2023) và Yarn (Peng et al., 2023) có thể
được xem như các trường hợp đặc biệt khi thay αPI(t) = [1/t]d/2_i=1 và αYarn(t) = [t−2i/(d−2)]d/2_i=1
vào Phương trình 8 tương ứng.
Lưu ý rằng θt=α(t)⊙θ biểu thị cơ sở tần số được thu phóng tại chiều dài ngữ cảnh t·L và θ1=θ
(cụ thể là α(1) = 1). Như được minh họa trong Hình 2, điều này chỉ ra một chuỗi tiến bộ qua
các giá trị t rời rạc mà
z(t) = z(1) + log α(t) = z(t−1) + log[α(t)/α(t−1)], (9)
trong đó z(t)=log θt.
Bằng cách liên tục hóa chuỗi tiến bộ, chúng ta có thể công thức hóa thu phóng PE như một hệ
thống động học liên tục, với động học liên tục của cơ sở tần số dz(t)/dt như
dz(t)/dt = d log α(t)/dt. (10)
Về bản chất, các phương pháp thu phóng PE gần đây, tập trung vào việc công thức hóa thủ công
α(t), tương đương với việc áp dụng các động học khác nhau cho cơ sở tần số cho phép các mô
hình thích ứng với các ngữ cảnh dài hơn.
3.2 THU PHÓNG PE LIÊN TỤC THÔNG QUA ODE THẦN KINH
Ngay cả khi có động học liên tục của cơ sở tần số, các phương pháp trước đây về cơ bản được
thiết kế để mở rộng chiều dài ngữ cảnh tại các giá trị t rời rạc. Ví dụ, PI (Chen et al., 2023) tinh
chỉnh mô hình trên một yếu tố thu phóng cụ thể t để mở rộng độ dài cửa sổ ngữ cảnh đến t·L.
Một vấn đề tiềm ẩn của những phương pháp này, như được mô tả trong Hình 1, là chúng dễ bị
quá khớp với cơ sở tần số được chỉ định, dẫn đến hoặc khả năng ngoại suy kém đến độ dài dài
hơn vượt quá huấn luyện hoặc giảm hiệu suất trong độ dài ngắn, hoặc cả hai trong một số trường
hợp. Do đó, CLEX của chúng tôi nhằm xây dựng thu phóng PE liên tục, điều này khiến mô hình
thích ứng với cơ sở tần số tương ứng với phạm vi liên tục của t như được minh họa trong Hình 2
(phải).
4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Nhớ lại rằng thu phóng PE trước đây, tương ứng với α(t) được định nghĩa thủ công, ngụ ý động
học không đổi trong Phương trình (10). Trong phương pháp của chúng tôi, chúng tôi sử dụng một
hàm biến g:Rd/2→Rd/2 để mô hình hóa động học, do đó hướng tới một cái nhìn tổng quát và
linh hoạt hơn như:
dz(t)/dt = g(z(t), t). (11)
Bằng cách hạn chế hàm được liên kết với các trạng thái tiềm ẩn z(t), g có khả năng nắm bắt
những thay đổi tinh vi của cơ sở tần số trong quá trình mở rộng chiều dài. Tuy nhiên, không
dễ để định nghĩa thủ công hàm g nhận biết z(t). Ở đây, chúng tôi trực tiếp tham số hóa hàm
bằng mạng thần kinh ϕ. Do đó, đối với bất kỳ t′∈[1, t], có một ODE thần kinh mô hình hóa thu
phóng cơ sở tần số như
z(t′) = z(1) + ∫1^t′ gϕ(z(t), t)dt, (12)
trong đó cơ sở tần số tại độ dài t′·L có thể được suy ra bằng θt′ = exp(z(t′)).
Cụ thể hơn, chúng tôi áp dụng một chiếu lên-xuống như mạng thần kinh, được biểu thị như:
gϕ(z(t), t) = Wdown·σ(Wup·z(t)) + ξt, (13)
trong đó Wup∈Rd/2×λd và Wdown∈Rλd×d/2 là các ma trận biến đổi, mà các tham số được xác
định bởi yếu tố khuếch đại λ; σ là hàm kích hoạt SiLU và ξt là nhúng vô hướng đặc trưng cho
quá trình thu phóng tại yếu tố t. Ở đây, chúng tôi áp dụng động học không đổi của Yarn làm ξt
để tăng tốc độ hội tụ, cụ thể là
ξt = d log αYarn(t)/dt = [−2i/(d−2)·t]d/2_i=1 (14)
3.3 NGOẠI SUY CHIỀU DÀI LIÊN TỤC: HUẤN LUYỆN NGẮN, KIỂM TRA DÀI
Thu phóng PE liên tục có thể phục vụ như một phương pháp thu phóng PE thích ứng và linh hoạt
hơn để mở rộng chiều dài ngữ cảnh đến một chiều dài huấn luyện đã cho LTrain. Không giống
như các phương pháp thu phóng PE trước đây được xây dựng trên yếu tố thu phóng lớn hơn,
điều này sẽ dẫn đến hiệu suất kém hơn trên các độ dài tương ứng với các đối tác nhỏ hơn, thu
phóng PE liên tục sẽ cho phép tổng quát hóa không phá hủy đến các yếu tố thu phóng lớn hơn
thông qua động học liên tục thích ứng. Do đó, bằng cách đơn giản mở rộng động học liên tục
vượt quá yếu tố t=LTrain/L trong quá trình huấn luyện (trong đó chúng tôi ký hiệu yếu tố thu
phóng mong muốn là tTrain), chúng ta có thể truy cập phương pháp ngoại suy chiều dài liên tục
(CLEX), đạt được khả năng "huấn luyện ngắn, kiểm tra dài".
Hơn nữa, để học ODE thần kinh trong Phương trình (12) cho t liên tục, chúng tôi lấy mẫu ngẫu
nhiên t′∈[1, tTrain] cho mỗi bước huấn luyện, cho phép mô hình thích ứng với phạm vi rộng cơ
sở tần số mà không quá khớp với một cơ sở cụ thể. Lưu ý rằng cơ sở tần số được ràng buộc với
chỉ số vị trí trong Phương trình (1). Điều này tiết lộ quá trình huấn luyện đã nêu liên quan đến
sự không nhất quán giữa cơ sở tần số và chỉ số vị trí: cơ sở tần số được thay đổi tương ứng với
t′∈[1, tTrain], trong khi các chỉ số vị trí được cố định như {1,2, . . . , LTrain}. Ở đây, chúng tôi
đề xuất chiến lược ngoại suy vị trí để giải quyết tính nhất quán này. Ngược lại với PI, việc thu
nhỏ các chỉ số vị trí vào chiều dài ngữ cảnh, chúng tôi phóng to các chỉ số vị trí {1,2, . . . , LTrain}
của các chuỗi được huấn luyện lên đến phạm vi [1, t′·L] cho mỗi bước huấn luyện. Các chỉ số
vị trí có thể được thu thập bằng cách thu phóng đồng đều đến {1·s,2·s, . . . , LTrain·s} trong đó
s=t′·L/LTrain, hoặc thay thế, bằng cách lấy mẫu ngẫu nhiên LTrain chỉ số từ [1, t′·L]. Theo thực
nghiệm, chúng tôi thấy rằng lấy mẫu ngẫu nhiên thường hoạt động tốt hơn. Thêm thảo luận có
thể tìm thấy trong §4.2.
Trong quá trình suy luận, kịch bản lý tưởng là thu được cơ sở tần số tương ứng với mỗi chiều
dài chuỗi. Tuy nhiên, phương pháp này tốn nhiều tính toán. Để cải thiện hiệu quả, trước tiên
chúng tôi lưu trữ một số cơ sở tần số được suy ra từ gϕ cho K giá trị t rời rạc như {tk|k∈[1, K]}.
Đối với mỗi chuỗi có độ dài LInfer trong quá trình suy luận, chúng tôi sử dụng cơ sở tần số tương
ứng với giới hạn trên gần nhất trong tk·L cho k= 1, . . . , K. Thông qua điều này, phương pháp
của chúng tôi giới thiệu chi phí thời gian không đáng kể so với suy luận ngây thơ của LLM.
4 THỰC NGHIỆM
Trong phần này, chúng tôi tiến hành đánh giá kỹ lưỡng về hiệu suất của CLEX trong việc xử lý
ngữ cảnh dài và khả năng ngoại suy của nó. Chúng tôi so sánh phương pháp của chúng tôi với
các phương pháp khác bao gồm
5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Chiều dài Chiều dài Đánh giá
Huấn luyện 4096 (4k) 8192 (8k) 16,384 (16k) 32,768 (32k) 65,536 (64k) Phương pháp
PPL ACC. PPL ACC. PPL ACC. PPL ACC. PPL ACC.
LLaMA-2 4k 6.04 58.18 20.54 44.50 >100 22.43 >1000 12.70 >1000 10.64
CodeLLaMA 16k 7.60 54.88 7.40 55.19 7.33 55.30 15.12 44.70 52.02 31.16
Naive FT 16k 5.98 58.83 5.93 58.91 5.91 58.58 18.31 43.04 >100 26.05
PI 16k 5.90 59.05 5.71 59.44 5.72 59.87 6.05 58.5 8.75 52.02
Yarn (t=16) 16k 6.50 57.28 5.71 59.57 5.73 59.87 5.99 58.13 8.51 52.62
Yarn (t=32) 16k 6.61 57.12 5.94 58.27 5.96 58.04 6.08 57.73 6.22 57.98
CL-Scaling 16k 24.99 37.84 5.86 59.08 5.87 59.05 10.56 50.47 41.09 34.16
ALiBi 4k 6.34 58.01 6.39 57.8 6.41 57.78 6.50 57.47 6.51 56.44
RandomPos 4k 5.88 58.49 >100 34.23 >1000 18.27 >1000 9.31 >1000 7.44
4k 5.86 59.21 5.70 59.62 5.87 58.93 14.53 47.55 30.51 35.33
CLEX 8k 5.98 58.75 5.78 59.44 5.71 59.64 5.99 58.66 11.74 47.50
16k 5.88 59.21 5.68 59.73 5.52 60.28 5.55 60.06 5.64 59.94
Bảng 1: Độ phức tạp (PPL) và độ chính xác dự đoán token tiếp theo (ACC.) trên mô hình ngôn
ngữ với độ dài đánh giá từ 4k đến 64k. Chúng tôi huấn luyện LLaMA-2-7B sử dụng các phương
pháp ngoại suy chiều dài trên độ dài 4k và các phương pháp thu phóng PE trên độ dài 16k, trong
khi báo cáo kết quả của CLEX được huấn luyện trên 4k, 8k và 16k. CL-Scaling biểu thị huấn
luyện LLaMA-2-7B với phương pháp thu phóng của CodeLLaMA nhưng sử dụng dữ liệu huấn
luyện của chúng tôi. Các đường cong mất mát huấn luyện được mô tả trong Hình 9.
cả ngoại suy chiều dài (tức là ALiBi (Press et al., 2022) và vị trí ngẫu nhiên (ký hiệu là Random-
Pos) (Ruoss et al., 2023)) và các phương pháp thu phóng PE (tức là PI (Chen et al., 2023) và
Yarn (Peng et al., 2023)). Chúng tôi chủ yếu tiến hành thực nghiệm trên mô hình LLaMA-2-7B.
Đối với mô hình ngôn ngữ, chúng tôi huấn luyện mô hình của chúng tôi và các baseline trên 2B
token được trích xuất từ Redpajama-Book (Computer, 2023), được thu thập từ các bộ dữ liệu
Pile-Books3 (Gao et al., 2020) và PG-19 (Rae et al., 2019). Hiệu suất của các mô hình được
đánh giá dựa trên độ phức tạp và độ chính xác dự đoán token tiếp theo, với độ dài chuỗi đánh
giá lên đến 64k. Hơn nữa, chúng tôi tiến hành điều chỉnh hướng dẫn cho LLaMA-2-7B sử dụng
CLEX trên bộ dữ liệu UltraChat (Ding et al., 2023b). Việc đánh giá được thực hiện trên bộ đánh
giá LongBench (Bai et al., 2023), nơi chúng tôi so sánh mô hình của chúng tôi với GPT-3.5-turbo
và các mô hình nguồn mở dựa trên LLaMA-2 khác được thiết kế để xử lý ngữ cảnh dài. Chi tiết
thêm về các baseline và cấu hình huấn luyện sẽ được thảo luận trong Phụ lục §A, cũng như các
kết quả thực nghiệm và ablation khác trong Phụ lục §B.
4.1 MÔ HÌNH NGÔN NGỮ NGỮ CẢNH DÀI
CLEX đạt được ngoại suy chiều dài. Trước tiên chúng tôi báo cáo kết quả thực nghiệm của các
baseline và CLEX trên mô hình ngôn ngữ, với độ dài đánh giá từ 4k đến 64k. Như được hiển
thị trong Bảng 1, CLEX của chúng tôi liên tục cho thấy hiệu suất đáng kể trong ngoại suy chiều
dài, có thể ngoại suy chiều dài ngữ cảnh đến hơn 4 × chiều dài huấn luyện mà không có bất kỳ
giảm hiệu suất nào. Lấy CLEX-4k làm ví dụ, PPL của nó trên chuỗi 4k (chiều dài huấn luyện)
tương đương với chuỗi 16k (5.86 so với 5.87). Khi được đánh giá trên các chuỗi trong 16k,
CLEX-4k ngang bằng hoặc thậm chí tốt hơn tất cả các phương pháp so sánh được huấn luyện
trên độ dài lên đến 16k. Hơn nữa, với sự gia tăng độ dài huấn luyện, CLEX của chúng tôi không
chỉ thể hiện khả năng tổng quát hóa hứa hẹn đến các ngữ cảnh rất dài (lên đến 64k) mà còn đảm
bảo hiệu suất trên các chuỗi ngắn.
Chúng tôi cũng thấy rằng các phương pháp thu phóng PE rời rạc (tức là PI và Yarn) có tính tự
mở rộng: huấn luyện với cơ sở tần số được thu phóng trang bị cho mô hình khả năng ngoại suy
đến các đối tác được thu phóng thêm (xem Phụ lục §B.2 để thảo luận thêm.). Như được mô tả
trong Hình 1, tuy nhiên, khả năng ngoại suy của những phương pháp này bị hạn chế, kèm theo
sự suy giảm hiệu suất đáng kể ngay cả trong chiều dài ngữ cảnh gốc. Điều này chỉ ra thách thức
vốn có trong việc đạt được sự cân bằng tinh tế giữa ngoại suy đến độ dài dài hơn và duy trì hiệu
suất trong độ dài ngắn khi sử dụng yếu tố thu phóng rời rạc. Ngược lại, CLEX giải quyết vấn
đề này thông qua động học liên tục có thể học, cung cấp ngoại suy tinh vi hơn trong khi bảo tồn
hiệu suất cho ngữ cảnh nội bộ.
Lưu ý rằng ALiBi có thể mở rộng xa hơn CLEX được huấn luyện trên chuỗi 4k (mặc dù thường
tạo ra kết quả kém hơn), các thực nghiệm của chúng tôi cho thấy những lợi ích này có thể đến
với chi phí thông tin dài hạn, dẫn đến hiệu suất kém trong các tác vụ thực tế ngữ cảnh dài (xem
§4.3 để biết thêm chi tiết).
6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
4k 8k 16k 32k 64k
Chiều dài Chuỗi Đánh giá102030PPL
CLEX-7B-4k
CLEX-7B-8k
CLEX-13B-4k
0.25 0.5 0.75 11.25 1.5 1.75 2
Kích thước Bộ dữ liệu Huấn luyện (Tỷ token)102030
CLEX-7B-4k trên 16k
CLEX-7B-4k trên 32k
CLEX-13B-4k trên 16k
CLEX-13B-4k trên 32k
Hình 3: Trái: PPL của CLEX trên các chiều dài chuỗi đánh giá khác nhau với kích thước tham số
7B và 13B. Phải: PPL của CLEX qua kích thước dữ liệu huấn luyện biến đổi với các kích thước
tham số khác nhau và độ dài đánh giá.
4k 8k 16k 32k 64k204060PPL
Liên tục so với Rời rạc
Liên tục
Rời rạc
4k 8k 16k 32k 64k
Chiều dài Chuỗi Đánh giá20406080
Chiến lược Lấy mẫu
Ngẫu nhiên
Đồng đều
Không lấy mẫu
4k 8k 16k 32k 64k102030
Yếu tố λ
λ=1
λ=2
λ=4
Hình 4: Các nghiên cứu ablation cho động học liên tục, chiến lược lấy mẫu và yếu tố λ trong
Phương trình (13).
Quy luật thu phóng cho khả năng ngoại suy của CLEX. Để điều tra tính hiệu quả của CLEX
qua quy mô của mô hình cơ sở và kích thước dữ liệu huấn luyện, chúng tôi tiếp tục chuyển
phương pháp của chúng tôi sang LLaMA-2-13B. Như được mô tả trong Hình 3, khi mở rộng
quy mô mô hình cơ sở từ 7B lên 13B một cách đơn giản, CLEX của chúng tôi cho thấy khả năng
tăng cường để ngoại suy đến chiều dài ngữ cảnh dài hơn. Cụ thể, khả năng ngoại suy của CLEX-
13B được huấn luyện trên độ dài 4k tiếp cận với CLEX-7B được huấn luyện trên 8k. Trong khi
quy mô dữ liệu huấn luyện, đáng ngạc nhiên hơn, không ảnh hưởng đáng kể đến khả năng ngoại
suy của CLEX. Các mô hình được huấn luyện với 0.25B hoặc 2B token với độ dài chuỗi 4k đạt
được PPL tương đương khi đánh giá trên độ dài 16k hoặc 32k trong Hình 3, chỉ ra sự khác biệt
không đáng kể từ kích thước dữ liệu huấn luyện lớn hơn. Điều này cũng ngụ ý rằng CLEX có
thể mở rộng hiệu quả chiều dài ngữ cảnh của LLM thông qua các bước huấn luyện tối thiểu giống
như PI và Yarn.
Dựa trên những phát hiện này, chúng tôi đề xuất một quy luật thu phóng cho CLEX: để thu phóng
chiều dài ngữ cảnh của LLM đến độ dài mong muốn vừa phải (ví dụ, 16k →64k), người ta nên
tăng tỷ lệ thuận chiều dài chuỗi huấn luyện (ví dụ, 4k →16k). Để thu phóng chiều dài ngữ cảnh
lên đến độ dài rất dài (ví dụ, >200k), kích thước tham số của mô hình cơ sở nên được tăng tương
ứng trong khi duy trì chiều dài huấn luyện, vì các ngữ cảnh có thể chiếm nhiều dấu vết hơn các
tham số mô hình. Lưu ý rằng việc thu phóng dữ liệu huấn luyện không ảnh hưởng trực tiếp đến
khả năng ngoại suy của CLEX, nhưng có thể được tích hợp ngầm khi thu phóng các LLM được
huấn luyện trước cơ sở.
4.2 NGHIÊN CỨU ABLATION
Bây giờ chúng tôi tiến hành ba loại ablation để điều tra hiệu quả của các thành phần trong CLEX:
Động học liên tục. Để học động học liên tục sử dụng ODE thần kinh, chúng tôi áp dụng một
phương pháp huấn luyện riêng biệt liên quan đến việc lấy mẫu yếu tố thu phóng t cho mỗi batch
dữ liệu. Ở đây chúng tôi tìm cách khám phá xem khả năng ngoại suy đặc biệt của CLEX có chỉ
được suy ra từ t biến đổi thay vì động học liên tục hay không. Chúng tôi sử dụng phương pháp
Yarn rời rạc với t = 16, trải qua cùng quy trình huấn luyện của CLEX nhưng loại bỏ các tham
số ODE, phục vụ như một baseline rời rạc. Trong Hình 4 (trái), chúng tôi phát hiện rằng phương
pháp rời rạc được trang bị với t được lấy mẫu ngẫu nhiên hoạt động kém đáng kể so với CLEX
của chúng tôi, chỉ ra tính thiết yếu của động học liên tục có thể học trong CLEX để truy cập khả
năng ngoại suy.
7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
QA Tài liệu Đơn
QA Nhiều Tài liệu
Tóm tắt
Học Vài ví dụTác vụ Tổng hợpHoàn thiện Mã
0 20 40 60 80 100
GPT-3.5-Turbo-16k LongChat-v1.5-7B-32k CodeLlama-7B-16k
Vicuna-v1.5-7B-16k Baichuan-13B-4k (ALiBi) CLEX-7B-4k (Của chúng tôi)
4k 16k 32k
Chiều dài Chuỗi Huấn luyện203040Trung bình TấtGPT-3.5-Turbo-16k
LongChat-v1.5-7B-32k
CodeLLaMA-7B-16k
Vicuna-v1.5-7B-16k
Baichuan-13B-4k (ALiBi)
CLEX-7B-4k (Của chúng tôi)
Hình 5: Trái: điểm trung bình cho mỗi lĩnh vực tác vụ trong LongBench. Phải: điểm trung bình
của tất cả các tác vụ tương ứng với chiều dài huấn luyện của mỗi mô hình. Lưu ý rằng CLEX
được huấn luyện trên độ dài chuỗi 4k nhưng được kiểm tra trực tiếp trên chiều dài ngữ cảnh 16k
mà không cắt ngắn.
Ngoại suy vị trí. Chúng tôi áp dụng chiến lược ngoại suy vị trí, mở rộng phạm vi chỉ số vị trí
trong các chuỗi huấn luyện bằng cách lấy mẫu từ phạm vi rộng hơn, để điều hòa sự không nhất
quán giữa cơ sở tần số và chỉ số vị trí trong quá trình huấn luyện. Trong nghiên cứu này, chúng
tôi kiểm tra tác động của các chiến lược lấy mẫu khác nhau (đồng đều hoặc ngẫu nhiên) và so
sánh chúng với các chỉ số vị trí ngây thơ. Kết quả trong Hình 4 nhấn mạnh hiệu quả của ngoại
suy vị trí trong CLEX, mà không có nó khả năng ngoại suy của các mô hình giảm đáng kể. Hơn
nữa, lấy mẫu ngẫu nhiên hoạt động hơi tốt hơn lấy mẫu đồng đều, vì vậy chúng tôi áp dụng nó
trong tất cả các thực nghiệm.
Quy mô tham số của ODE. Chúng tôi cũng nghiên cứu tác động của kích thước tham số của
ODE thần kinh trong CLEX. Kích thước tham số được xác định bởi λ, cụ thể là yếu tố khuếch
đại trong Phương trình (13). Trong Hình 4, chúng tôi vẽ kết quả của CLEX với λ = 1,2,4, nơi
chúng đạt được hiệu suất tương tự. Lưu ý rằng kích thước tham số của ODE thần kinh trong
CLEX khá nhỏ ngay cả khi λ = 4, vì chiều d trong Phương trình (13) thường bằng 128. Mặc dù
có thể tăng cường CLEX với λ lớn hơn (ví dụ, 32), chúng tôi đặt λ=1 trong tất cả các thực nghiệm
để có tác động tối thiểu đến độ trễ suy luận.
4.3 ĐÁNH GIÁ TRÊN BỘ ĐÁNH GIÁ NGỮ CẢNH DÀI
Để xác định hiệu suất toàn diện của CLEX trong các tình huống thực tế, chúng tôi tiếp tục tiến
hành đánh giá trên bộ đánh giá LongBench zero-shot. Bộ đánh giá này bao gồm một loạt các tác
vụ, như trả lời câu hỏi, tóm tắt và hoàn thiện mã, nơi độ dài đánh giá từ 5k đến 15k. Chúng tôi
thực hiện điều chỉnh hướng dẫn thí điểm cho LLaMA-2-7B bằng cách sử dụng CLEX trên bộ dữ
liệu UltraChat, với độ dài chuỗi 4k. Trong quá trình suy luận, chúng tôi khai thác tất cả các mô
hình để giải quyết chiều dài ngữ cảnh 16k, do đó đảm bảo việc khai thác toàn diện thông tin ngữ
cảnh trong các tác vụ. Như được mô tả trong Hình 5, chúng tôi trình bày điểm trung bình của
mỗi lĩnh vực trong LongBench cho CLEX, so sánh với mô hình GPT-3.5-Turbo-16k và các LLM
nguồn mở mạnh mẽ như LongChat-v1.5-7B-32k và CodeLLaMA-7B-16k.
Nói chung, khi được huấn luyện với các chuỗi có độ dài 4k, CLEX giữ vững vị trí của mình so
với bất kỳ LLM nguồn mở nào được huấn luyện trên độ dài lên đến 32k. Trong các lĩnh vực cụ
thể của Tóm tắt, Học Vài ví dụ và Hoàn thiện Mã, CLEX trên LLaMA-2-7B vẫn cạnh tranh với
hoặc thậm chí vượt qua GPT-3.5-Turbo-16k. Chúng tôi lưu ý rằng Baichuan-13B-4k, được huấn
luyện trước với ALiBi (Press et al., 2022), cho thấy hiệu suất kém đáng kể trên LongBench mặc
dù có kích thước tham số lớn hơn. Ngoài ra, kết quả kém tương tự được đạt bởi ALiBi khi áp
dụng nó lên LLaMA-2-7B sử dụng cùng quy trình huấn luyện như CLEX (xem Phụ lục §B.5).
Điều này có thể được quy cho việc ALiBi quá nhấn mạnh vào ngữ cảnh cục bộ thông qua bias
attention, điều này, trong khi có lợi cho mô hình ngôn ngữ, hạn chế truy cập vào thông tin ngữ
cảnh dài trong các tác vụ thực tế. Ngược lại, CLEX trực tiếp mở rộng chiều dài ngữ cảnh của
LLM mà không áp đặt bất kỳ ràng buộc nào về ngữ cảnh, điều này liên tục đạt được khả năng
ngoại suy vượt trội trên cả mô hình ngôn ngữ và LongBench. Điều này chứng thực tiềm năng
đáng kể của CLEX để phục vụ như phương pháp tiên tiến nhất để ngoại suy chiều dài ngữ cảnh
của LLM để xuất sắc trong các ứng dụng ngữ cảnh dài.
8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Ngoài ra, chúng tôi nhấn mạnh rằng CLEX của chúng tôi chỉ giới thiệu độ trễ suy luận rất nhỏ.
Với chiều dài ngữ cảnh 16k trong LongBench với độ dài tạo 512, thông lượng tạo giữa CLEX-7B
và LLaMA-2-7B của chúng tôi tương đương (27.8 token/s so với 28.3 token/s, trong một A100
đơn), khi sử dụng cơ chế cache được giới thiệu trong §3.3.
5 NGHIÊN CỨU LIÊN QUAN
Kiến trúc Phân cấp / Attention Thưa thớt. Để vượt qua độ phức tạp bậc hai của attention,
Dai et al. (2019) đề xuất Transformer-XL xử lý chuỗi dài ở cấp độ phân đoạn bằng Transformer,
với các phân đoạn này tương tác thông qua cơ chế tái diễn. Recurrent Memory Transformer (Bulatov et al., 2022) tinh chỉnh cơ chế này bằng cách tích hợp các token bộ nhớ đặc biệt vào tái diễn,
có khả năng thu phóng chiều dài ngữ cảnh đến hàng triệu (Bulatov et al., 2023). Mặt khác, Child
et al. (2019); Beltagy et al. (2020) đề xuất sử dụng attention thưa thớt để tránh truy cập đầy đủ
vào các chuỗi dài, do đó giảm độ phức tạp. Attention thưa thớt đã được Ding et al. (2023a) áp
dụng để thu phóng chiều dài ngữ cảnh của transformer thành hàng tỷ. Tuy nhiên, những phương
pháp này hy sinh việc sử dụng toàn bộ chuỗi trong attention, dẫn đến mất mát không thể tránh
khỏi một số thông tin ngữ cảnh. Ngoài ra, việc sửa đổi kiến trúc mô hình khiến những phương
pháp này khó áp dụng cho các LLM được huấn luyện trước hiện có. Ngược lại, CLEX của chúng
tôi phục vụ như một thành phần thay thế cho LLM, có thể mở rộng hiệu quả khả năng của các mô
hình để xử lý toàn bộ chuỗi dài mà không mất thông tin ngữ cảnh một cách rõ ràng.
Ngoại suy Chiều dài. Dựa trên nền tảng được đặt bởi ALiBi (Press et al., 2022), một loạt các
nghiên cứu (Sun et al., 2023; Chi et al., 2022; 2023) tìm cách huấn luyện các mô hình dựa trên
Transformer trên độ dài ngắn, trong khi trực tiếp kiểm tra trên các đối tác dài hơn. Những phương
pháp này thay thế nhúng vị trí bằng bias được giới thiệu vào điểm attention, do đó tích hợp thông
tin vị trí. Đáng chú ý, bias thường mang lại lợi ích cao hơn cho các token gần hơn. Cơ chế này
một cách trực quan khuếch đại ngữ cảnh cục bộ cho mỗi token với chi phí thông tin xa. Do đó,
những phương pháp ngoại suy chiều dài này gặp thách thức trong việc xử lý hiệu quả ngữ cảnh
dài trong các ứng dụng thực tế (Pal et al., 2023). Tuy nhiên, CLEX của chúng tôi cho thấy hiệu
quả đáng kể trong các tác vụ thực tế như tóm tắt, chỉ ra khả năng ngoại suy thực tế cho các ứng
dụng.
Thu phóng Nhúng Vị trí (PE). Nghiên cứu gần đây đã tìm cách mở rộng chiều dài ngữ cảnh
của Transformer thông qua việc thu phóng RoPE được sử dụng rộng rãi. Cụ thể, Chen et al.
(2023) đề xuất nội suy vị trí, một phương pháp mở rộng hiệu quả cửa sổ ngữ cảnh bằng cách
thu phóng chỉ số vị trí trong RoPE. Theo hướng tương tự, Peng et al. (2023); Rozière et al.
(2023) chọn thu phóng cơ sở tần số, đạt được hiệu suất vượt trội. Tuy nhiên, những phương pháp
này đòi hỏi huấn luyện (hoặc tinh chỉnh) trên độ dài mở rộng mong muốn. Kết quả là, chúng
thể hiện khả năng hạn chế để ngoại suy vượt quá độ dài được huấn luyện và thậm chí gặp phải
giảm hiệu suất trong các độ dài ngắn hơn. Trong CLEX, chúng tôi tổng quát hóa thu phóng PE
rời rạc thành đối tác liên tục, do đó ngoại suy đồng đều chiều dài ngữ cảnh của LLM trong khi
bảo tồn hiệu suất trong độ dài ngắn.
6 KẾT LUẬN
Chúng tôi đã trình bày Ngoại suy Chiều dài Liên tục (CLEX), một phương pháp mới ngoại suy
hiệu quả chiều dài ngữ cảnh của Mô hình Ngôn ngữ Lớn (LLM) đến hơn 4x chiều dài huấn
luyện (tinh chỉnh) mà không có bất kỳ sự suy giảm hiệu suất nào. CLEX sử dụng ODE thần kinh
để học động học liên tục qua yếu tố thu phóng chiều dài trong quá trình thu phóng PE, do đó
cho phép mở rộng tinh vi cho cơ sở tần số trong RoPE. Chúng tôi tiến hành thực nghiệm kỹ
lưỡng để điều tra tính hiệu quả của CLEX so với nhiều LLM mạnh mẽ, bao gồm tác vụ mô hình
ngôn ngữ và bộ đánh giá LongBench. Kết quả thực nghiệm đã chứng minh khả năng ngoại suy
đặc biệt của CLEX, nơi CLEX của chúng tôi được huấn luyện với độ dài chuỗi 4k có tiềm năng
duy trì cạnh tranh với bất kỳ LLM ngữ cảnh dài nguồn mở nào (ví dụ, CodeLLaMA) được huấn
luyện trên độ dài lên đến 32k. Những kết quả này nhấn mạnh tiềm năng của CLEX như một
phương pháp tiên tiến nhất để ngoại suy hiệu quả chiều dài ngữ cảnh của LLM, mở đường cho
những tiến bộ trong các ứng dụng ngữ cảnh dài. Bằng cách thu phóng kích thước mô hình cơ sở
lên, chúng tôi thấy CLEX có thể được tăng cường tương ứng và sau đó có khả năng ngoại suy
mô hình đến chiều dài ngữ cảnh dài hơn. Tính chất này chỉ ra hiệu quả hấp dẫn của CLEX trong
kỷ nguyên của LLM.
9

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
LỜI CẢM ƠN
Công việc này được hỗ trợ đáng kể bởi DAMO Academy thông qua Chương trình Thực tập
Nghiên cứu DAMO Academy. Shangsong Liang được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc
gia Trung Quốc (Số Grant 61906219) và Đại học Trí tuệ Nhân tạo Mohamed bin Zayed, Các
Tiểu vương quốc Ả Rập Thống nhất.
TÀI LIỆU THAM KHẢO
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,
multitask benchmark for long context understanding. 2023. URL https://arxiv.org/
abs/2308.14508 .
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv:2004.05150 , 2020. URL https://arxiv.org/abs/2004.05150 .
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-
race He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shiv-
anshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-
20B: An open-source autoregressive language model. In Proceedings of BigScience Episode
#5 – Workshop on Challenges & Perspectives in Creating Large Language Models . Association
for Computational Linguistics, May 2022. URL https://aclanthology.org/2022.
bigscience-1.9 .
Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In Al-
ice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-
ral Information Processing Systems , 2022. URL https://openreview.net/forum?id=
Uynr3iPhksa .
Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond
with rmt. 2023. URL https://arxiv.org/abs/2304.11062 .
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Cur-
ran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/
paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf .
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window
of large language models via positional interpolation. 2023. URL https://arxiv.org/
abs/2306.15595 .
Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Ker-
ple: Kernelized relative positional embedding for length extrapolation. In Advances
in Neural Information Processing Systems . Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/file/
37a413841a614b5414b333585e7613b8-Paper-Conference.pdf .
Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer
length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Associa-
tion for Computational Linguistics, July 2023. URL https://aclanthology.org/2023.
acl-long.756 .
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. 2019. URL https://arxiv.org/abs/1904.10509 .
Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.
URL https://github.com/togethercomputer/RedPajama-Data .
10

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics . Association for Compu-
tational Linguistics, July 2019. URL https://aclanthology.org/P19-1285 .
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
URLhttps://arxiv.org/abs/2307.08691 .
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-
ing Systems , 2022. URL https://arxiv.org/abs/2205.14135 .
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning
Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. 2023a. URL
https://arxiv.org/abs/2307.02486 .
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong
Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional
conversations. arXiv preprint arXiv:2305.14233 , 2023b. URL https://arxiv.org/abs/
2305.14233 .
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of di-
verse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020. URL https:
//arxiv.org/abs/2101.00027 .
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lample, L ´elio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le
Scao, Th ´eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth ´ee Lacroix, and William El Sayed.
Mixtral of experts. 2024. URL https://arxiv.org/abs/2401.04088 .
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http:
//arxiv.org/abs/1412.6980 .
OpenAI. GPT-4 Technical Report. 2023. URL https://arxiv.org/abs/2303.08774 .
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha
Naidu. Giraffe: Adventures in expanding context lengths in llms. 2023. URL https://
arxiv.org/abs/2308.10882 .
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
extension of large language models. 2023. URL https://arxiv.org/abs/2309.00071 .
Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=R8sQPpGCv0 .
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.
Compressive transformers for long-range sequence modelling. arXiv preprint , 2019. URL
https://arxiv.org/abs/1911.05507 .
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,
Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D ´efossez,
Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and
Gabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https://
arxiv.org/abs/2308.12950 .
11

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Anian Ruoss, Gr ´egoire Del ´etang, Tim Genewein, Jordi Grau-Moya, R ´obert Csord ´as, Mehdi Ben-
nani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization
of transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) . Association for Computational Linguistics, July 2023. URL
https://aclanthology.org/2023.acl-short.161 .
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En-
hanced transformer with rotary position embedding. 2022. URL https://arxiv.org/abs/
2104.09864 .
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaud-
hary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . As-
sociation for Computational Linguistics, July 2023. URL https://aclanthology.org/
2023.acl-long.816 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models. 2023a. URL https://arxiv.org/abs/2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
2023b. URL https://arxiv.org/abs/2307.09288 .
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna-
tional Conference on Learning Representations , 2022. URL https://openreview.net/
forum?id=gEZrGCozdqR .
12

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A CHI TIẾT THỰC NGHIỆM
A.1 BASELINE
Chúng tôi so sánh CLEX của chúng tôi với nhiều baseline khác nhau trong mô hình ngôn ngữ,
tất cả đều được huấn luyện với tập con 2B token từ Pile-Books3 và PG-19 ngoại trừ LLaMA-2
và CodeLLaMA được đánh giá bằng các checkpoint được huấn luyện trước nguồn mở. Chúng
tôi huấn luyện các phương pháp thu phóng PE và ngoại suy chiều dài với độ dài chuỗi 16k và
4k tương ứng. Cụ thể, chúng tôi sử dụng yếu tố thu phóng t = 4 để huấn luyện các mô hình sử
dụng độ dài chuỗi 16k cho PI theo Chen et al. (2023). Trong khi đối với Yarn, chúng tôi huấn
luyện các mô hình với yếu tố thu phóng 16 và 32 (nhưng cũng sử dụng độ dài chuỗi 16k) tương
ứng, theo cài đặt trong Peng et al. (2023). Đối với ALiBi, chúng tôi trực tiếp loại bỏ RoPE trong
LLaMA-2 và huấn luyện các mô hình với bias attention. Đối với vị trí ngẫu nhiên, chúng tôi lấy
mẫu các chỉ số vị trí từ [1, 16k] trong khi huấn luyện các mô hình với độ dài chuỗi 4k. Chúng
tôi cũng sử dụng phương pháp thu phóng PE từ CodeLLaMA để huấn luyện LLaMA-2-7B sử
dụng dữ liệu huấn luyện của chúng tôi, được ký hiệu là CL-Scaling.
A.2 CHI TIẾT HUẤN LUYỆN
Chúng tôi sử dụng một tập con từ Redpajama-Book (Computer, 2023) làm dữ liệu huấn luyện
cho mô hình ngôn ngữ, được thu thập từ các bộ dữ liệu Pile-Books3 và PG-19 và tập con bao
gồm khoảng 2B token. Chúng tôi đặt tốc độ học 2e-5 cho tất cả các mô hình, được tối ưu hóa
bằng bộ tối ưu Adam (Kingma & Ba, 2015). Kích thước batch được đặt thành 64k token cho
các mô hình 7B và 16k token cho các mô hình 13B. t mong muốn tối đa trong quá trình huấn
luyện trong CLEX (cụ thể là tTrain trong §3.3) được đặt thành 16 cho LLaMA-2. Chúng tôi sử
dụng Flash Attention (Dao et al., 2022; Dao, 2023) để hỗ trợ huấn luyện hiệu quả. Yếu tố khuếch
đại của lớp ODE λ được đặt thành 1 cho tất cả các mô hình 7B và 2 cho các mô hình 13B. Đối
với điều chỉnh hướng dẫn, chúng tôi huấn luyện mô hình của chúng tôi sử dụng bộ dữ liệu UltraChat cho 1 epoch, bắt đầu với checkpoint sau quá trình huấn luyện mô hình ngôn ngữ. Quy
trình huấn luyện của CLEX được hiển thị trong Thuật toán 1.
Thuật toán 1 Quy trình Huấn luyện của CLEX
1:lặp lại
2: Cho một batch chuỗi B có độ dài LTrain;
3: lấy mẫu t′∼[1, tTrain];
4: tính z(t′) bằng Phương trình (12);
5: θt′ = exp(z(t′));
6: lấy mẫu (ngẫu nhiên hoặc đồng đều) t′·LTrain chỉ số vị trí từ [1, tTrain·LTrain];
7: tính RoPE với θt′ và các chỉ số vị trí được lấy mẫu;
8: huấn luyện mô hình với RoPE trên B với mục tiêu mô hình ngôn ngữ.
9:cho đến khi hội tụ
A.3 CHI TIẾT ĐÁNH GIÁ
Đối với mô hình ngôn ngữ, chúng tôi đánh giá tất cả các mô hình với một tập con tương tự như
tập huấn luyện nhưng chứa 20 triệu token. Đối với các độ dài đánh giá khác nhau, chúng tôi nhóm
tập kiểm tra thành các độ dài chuỗi tương ứng. Chúng tôi thấy rằng việc cố định yếu tố thu phóng
như trong huấn luyện sẽ cản trở hiệu suất tổng quát và khả năng ngoại suy của các baseline, do
đó, chúng tôi phóng to tỷ lệ thuận yếu tố thu phóng khi độ dài đánh giá vượt quá độ dài huấn
luyện. Ví dụ, khi đánh giá trên độ dài 32k, chúng tôi đặt yếu tố thu phóng của PI thành 8 thay vì
yếu tố huấn luyện 4. Chúng tôi cũng áp dụng thủ thuật logscaling1 cho tất cả các baseline và mô
hình của chúng tôi, nơi chúng tôi thu phóng điểm attention bằng max{1,log LTrain/LTest}. Chúng
tôi thấy nó có thể cải thiện hiệu suất khi đánh giá trên độ dài vượt quá độ dài huấn luyện như
được hiển thị trong Hình 8. Đối với LongBench, chúng tôi tuân theo thiết lập từ Bai et al. (2023),
nơi phương pháp giải mã được đặt thành tìm kiếm tham lam. Chúng tôi đặt tk trong §3.3 thành
{1,2,3,4}, để lưu trữ cơ sở tần số tương ứng với {4k,8k,12k,16k}.
1https://spaces.ac.cn/archives/8823
13

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
0 20 40 60
Chiều402060802604608001000120140Tần số SinusoidalBase=1e4 (gốc)
CLEX (t=4)
CLEX (t=8)
CLEX (t=16)
Base=1e6
Hình 6: Tần số sinusoidal qua chiều của cơ sở tần số. Chiều thấp hơn biểu thị tần số cao hơn.
0.0 10k 20k 30k 40k 50k 60k
Chiều dài Chuỗi Đánh giá2324PPLPI-16k (t=4)
PI-16k (t=16)
Hình 7: Kết quả của PI-16k (được huấn luyện với t = 4) trên độ dài chuỗi 64k, trong khi đánh
giá với t = 4 và t = 16.
B KẾT QUẢ BỔ SUNG
B.1 TRỰC QUAN HÓA CƠ SỞ TẦN SỐ ĐÃ HỌC
Trong Hình 6, chúng tôi hiển thị cơ sở tần số theo các độ dài chuỗi khác nhau sau khi huấn luyện
CLEX trên LLaMA-2 với độ dài chuỗi 4k. Biểu đồ cho thấy CLEX có xu hướng phóng to các tần
số cao ở một số chiều nhất định trong khi đồng thời thu nhỏ một số chiều khác. Đáng ngạc nhiên
hơn, chúng tôi quan sát thấy rằng các cơ sở tần số liên kết với các giá trị t khác nhau trong CLEX
thể hiện hành vi đẳng hướng, rằng các chiều nơi thu nhỏ và phóng to xảy ra tương tự qua các giá
trị t khác nhau, với các giá trị t lớn hơn dẫn đến thu phóng thêm. Phát hiện này có thể đóng góp
vào việc thiết kế các phương pháp thu phóng PE heuristic mà không cần huấn luyện.
B.2 TÍNH CHẤT TỰ MỞ RỘNG CỦA CÁC PHƯƠNG PHÁP THU PHÓNG PE
Các phương pháp thu phóng PE trước đây thường đánh giá các mô hình với một yếu tố thu phóng
được chỉ định trong quá trình huấn luyện. Chúng tôi thấy điều này sẽ cản trở đáng kể việc đánh
giá hiệu suất vượt quá độ dài huấn luyện. Do đó, chúng tôi điều chỉnh yếu tố thu phóng của
phương pháp thu phóng PE trước đây tương ứng với độ dài đánh giá. Trong Hình 7, PI-16k (được
huấn luyện với t = 4) đạt được kết quả không tầm thường khi đánh giá
14

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Mô hình Chiều dài Chiều dài Đánh giá
Huấn luyện 32K 64K 128K 256K
CLEX-Phi-2 32K 5.11 5.17 6.55 >10
CLEX-Mixtral-8x7B 32K 2.56 2.53 2.57 3.78
Bảng 2: Độ phức tạp (PPL) của CLEX-Phi-2 và CLEX-Mixtral-8x7B trên mô hình ngôn ngữ với
độ dài đánh giá từ 32k đến 256k.
Hình 8: Ablation của logscaling (trái) và động học không đổi ξt (phải) cho CLEX được huấn luyện
trên độ dài chuỗi 4k.
với t = 16 trên độ dài 64k, vượt trội đáng kể so với đánh giá ngây thơ sử dụng t = 4.
Lưu ý rằng Yarn được huấn luyện ngây thơ với t lớn hơn, vì vậy không cần thiết phải điều chỉnh
yếu tố thu phóng cho Yarn trong quá trình đánh giá. Tuy nhiên, khi độ dài đánh giá trở nên dài
hơn, chúng tôi tin rằng tính chất tự mở rộng cũng sẽ có lợi cho Yarn. Về những điều này, khả
năng ngoại suy từ tự mở rộng trong các phương pháp thu phóng PE trước đây vẫn yếu hơn nhiều
so với CLEX như được minh họa trong Hình 1.
B.3 KẾT QUẢ CỦA NHIỀU MÔ HÌNH HỠN
Chúng tôi cũng mở rộng CLEX cho nhiều mô hình hơn ngoài LLaMA-2, tức là Phi-22 và Mixtral-
8x7B (Jiang et al., 2024). Kết quả trong Bảng 2 tiếp tục hỗ trợ tính hiệu quả tổng quát của CLEX,
nơi chiều dài ngữ cảnh được ngoại suy đến 4x ∼8x chiều dài huấn luyện qua nhiều LLM khác
nhau.
B.4 ABLATION VỀ NHÚNG VÔ HƯỚNG
Động học có thể học trong Phương trình (13) dựa trên động học không đổi ξt từ Yarn. Để tiến
hành ablation về tác động của ξt, chúng tôi loại bỏ ξt trong Phương trình (13) và huấn luyện
LLaMA-2-7B trên độ dài chuỗi 4k trải qua cùng quy trình huấn luyện. Từ Hình 8, chúng tôi thấy
việc huấn luyện CLEX mà không có ξt sẽ không cản trở khả năng ngoại suy (hoặc thậm chí tốt
hơn). Tuy nhiên, như được mô tả trong Hình 10, huấn luyện với ξt có thể tăng tốc độ hội tụ và
tạo ra đường cong mất mát ổn định hơn. Điều này rất cần thiết để huấn luyện các LLM lớn hơn
trên độ dài chuỗi dài hơn, vì vậy chúng tôi tích hợp ξt vào động học có thể học.
B.5 KẾT QUẢ CỦA LONG BENCH
Chúng tôi liệt kê kết quả số của CLEX được huấn luyện trên 4k so sánh với nhiều baseline khác
nhau được huấn luyện trên tối đa 32k, bao gồm trả lời câu hỏi tài liệu đơn (Bảng 3), trả lời câu
hỏi nhiều tài liệu (Bảng 4), tóm tắt (Bảng 5), học vài ví dụ (Bảng 6), tổng hợp (Bảng 7), và
2https://huggingface.co/microsoft/phi-2
15

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
0.0 0.2 0.4 0.6 0.8 1.0
Epoch2.02.53.03.54.0Mất mátNaive-FT-16k
PI-16k
Yarn-16k (t=16)
CL-Scaling-16k
ALiBi-4k
RandomPos-4k
CLEX-4k
CLEX-8k
CLEX-16k
Hình 9: Đường cong mất mát huấn luyện của các baseline khác nhau và các mô hình CLEX của
chúng tôi trong Bảng 1. Chúng tôi huấn luyện tất cả các mô hình trên LLaMA-2-7B với 2B token
trong một epoch.
0.0 0.2 0.4 0.6 0.8 1.0
Epoch2.02.53.03.54.04.5Mất mátCLEX-4k (w/o ξt)
CLEX-4k (w/ ξt)
Hình 10: Đường cong mất mát huấn luyện của CLEX-4k được huấn luyện trên LLaMA-2-7B có
hoặc không có động học không đổi (ξt) trong Phương trình (14).
hoàn thiện mã (Bảng 8). Lưu ý rằng độ dài chuỗi trung bình của hầu hết các tác vụ trong LongBench từ 5k đến 15k, vì vậy các mẫu sẽ được cắt ngắn thành độ dài chuỗi trong chiều dài ngữ
cảnh được hỗ trợ (nếu cần thiết) cho các baseline, ngoại trừ Baichuan-13B-4k, ALiBi-7B-4k và
CLEX-7B-4k cho phép ngoại suy chiều dài được đánh giá với cửa sổ ngữ cảnh 16k. Baichuan-
13B-4k là một mô hình được huấn luyện trước với ALiBi trên độ dài chuỗi 4k, trong khi ALiBi-
7B-4k là mô hình được huấn luyện với ALiBi trải qua cùng quy trình huấn luyện như CLEX của
chúng tôi. Chúng tôi thấy rằng cả hai mô hình dựa trên ALiBi đều hoạt động kém đáng kể trong
bộ đánh giá LongBench khi được đánh giá với độ dài chuỗi (16k) vượt quá độ dài huấn luyện
của chúng, chỉ ra những thách thức về khả năng ngoại suy của chúng trong các tác vụ thực tế.
Tuy nhiên, CLEX của chúng tôi đạt được kết quả hoạt động tốt trong hầu hết các tác vụ ngay cả
khi được huấn luyện với 4k nhưng được kiểm tra với độ dài 16k, điều này tiếp tục tiết lộ khả
năng ngoại suy chiều dài vượt trội của phương pháp chúng tôi.
16

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Mô hình NarrativeQA Qasper MultiFieldQA-en MultiFieldQA-zh TB.
GPT-3.5-Turbo-16k 23.6 43.3 52.3 61.2 45.1
Llama2-7B-chat-4k 18.7 19.2 36.8 11.9 21.65
LongChat-v1.5-7B-32k 16.9 27.7 41.4 29.1 28.78
CodeLLaMA-7B-16k 22.93 30.69 43.37 31.76 32.19
XGen-7B-8k 18 18.1 37.7 14.8 22.15
InternLM-7B-8k 12.1 16.7 23.4 33.6 21.45
Vicuna-v1.5-7B-16k 19.4 26.1 38.5 43 31.75
Baichuan-13B-4k 0.07 17.55 17.28 38.55 18.36
ALiBi-7B-4k 0.04 8.13 17.87 2.89 7.23
CLEX-7B-4k 18.05 23.68 44.62 31.15 29.38
Bảng 3: Các tác vụ trả lời câu hỏi tài liệu đơn trong LongBench.
Mô hình HotpotQA 2WikiMQA Musique DuReader TB.
GPT-3.5-Turbo-16k 51.6 37.7 26.9 28.7 36.23
Llama2-7B-chat-4k 25.4 32.8 9.4 5.2 18.2
LongChat-v1.5-7B-32k 31.5 20.6 9.7 19.5 20.33
CodeLLaMA-7B-16k 33.05 27.93 14.2 10.78 21.49
XGen-7B-8k 29.7 21.1 10.3 11 18.02
InternLM-7B-8k 28.7 22.8 9 11.1 17.9
Vicuna-v1.5-7B-16k 25.3 20.8 9.8 19.3 18.8
Baichuan-13B-4k 3.29 15 0.1 8.77 6.79
ALiBi-7B-4k 2.73 8 1.33 11.87 5.98
CLEX-7B-4k 28.44 19.53 9.15 23.21 20.08
Bảng 4: Các tác vụ trả lời câu hỏi nhiều tài liệu trong LongBench.
Mô hình GovReport QMSum MultiNews VCSUM TB.
GPT-3.5-Turbo-16k 29.5 23.4 26.7 16 23.9
Llama2-7B-chat-4k 27.3 20.8 25.8 0.2 18.525
LongChat-v1.5-7B-32k 30.8 22.7 26.4 9.9 22.45
CodeLLaMA-7B-16k 28.43 24.18 26.84 0.79 20.06
XGen-7B-8k 27.3 20.5 26.2 2.2 19.05
InternLM-7B-8k 9.7 15.9 22.8 12.4 15.2
Vicuna-v1.5-7B-16k 27.9 22.8 27.2 15.1 23.25
Baichuan-13B-4k 6.8 1.71 23.1 8.09 9.925
ALiBi-7B-4k 5.31 1.64 19.38 3.25 7.395
CLEX-7B-4k 32.52 22.9 25.55 12.03 23.25
Bảng 5: Các tác vụ tóm tắt trong LongBench.
17

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Mô hình TREC TriviaQA SAMSum LSHT TB.
GPT-3.5-Turbo-16k 68 91.4 41.7 29.2 57.575
Llama2-7B-chat-4k 61.5 77.8 40.7 19.8 49.95
LongChat-v1.5-7B-32k 63.5 82.3 34.2 23.2 50.8
XGen-7B-8k 65.5 77.8 25.3 20.5 47.275
CodeLLaMA-7B-16k 70 84.97 43.43 32.5 57.725
InternLM-7B-8k 52 77.8 21.2 15.2 41.55
Vicuna-v1.5-7B-16k 71.5 86.2 40.8 28.8 56.825
Baichuan-13B-4k 20.05 20.06 5.77 1 11.72
ALiBi-7B-4k 9.25 8.83 4.67 0 5.6875
CLEX-7B-4k 68 84.92 42.82 28.35 56.0225
Bảng 6: Các tác vụ học vài ví dụ trong LongBench.
Passage Count PassageRetrieval-en PassageRetrieval-zh TB.
GPT-3.5-Turbo-16k 4.5 71 77.5 51
Llama2-7B-chat-4k 2.1 9.8 0.5 4.13
LongChat-v1.5-7B-32k 1 30.5 7.6 13.03
CodeLLaMA-7B-16k 2 13.5 11.25 8.92
XGen-7B-8k 2.1 8.5 3.5 4.7
InternLM-7B-8k 3 6 0.9 3.3
Vicuna-v1.5-7B-16k 6.5 4.5 5 5.33
Baichuan-13B-4k 0.06 0.5 5 1.85
ALiBi-7B-4k 0 1.27 0.75 0.67
CLEX-7B-4k 0 11.5 17.5 9.67
Bảng 7: Các tác vụ tổng hợp trong LongBench.
LCC RepoBench-P TB
GPT-3.5-Turbo-16k 54.7 53.6 54.15
Llama2-7B-chat-4k 52.4 43.8 48.1
LongChat-v1.5-7B-32k 53 55.3 54.15
CodeLLaMA-7B-16k 64.35 55.87 60.11
XGen-7B-8k 38.6 38.6 38.6
InternLM-7B-8k 44.1 28.8 36.45
Vicuna-v1.5-7B-16k 51 43.5 47.25
Baichuan-13B-4k 47.98 16.58 32.28
ALiBi-7B-4k 46.69 18.54 32.61
CLEX-7B-4k 59.01 56.87 57.94
Bảng 8: Các tác vụ hoàn thiện mã trong LongBench.
18
