# MemLong: Bộ nhớ Tăng cường Tìm kiếm cho Mô hình Văn bản Dài

Weijie Liu1*, Zecheng Tang1*, Juntao Li1†, Kehai Chen2, Min Zhang1
1Khoa Khoa học Máy tính và Công nghệ, Đại học Soochow
2Viện Công nghệ Harbin, Thâm Quyến
{wjliu,zctang}@stu.suda.edu.cn
{ljt,minzhang}@suda.edu.cn ;chenkehai@hit.edu.cn

## Tóm tắt

Những tiến bộ gần đây trong Mô hình Ngôn ngữ Lớn (LLMs) đã mang lại thành công đáng kể trong nhiều lĩnh vực đa dạng. Tuy nhiên, xử lý ngữ cảnh dài vẫn là một thách thức đáng kể đối với LLMs do độ phức tạp thời gian và không gian bậc hai của cơ chế attention và tiêu thụ bộ nhớ tăng trưởng của bộ đệm key-value trong quá trình sinh. Công trình này giới thiệu MemLong: Bộ nhớ Tăng cường Tìm kiếm cho Sinh Văn bản Dài (MemLong), một phương pháp được thiết kế để nâng cao khả năng mô hình ngôn ngữ ngữ cảnh dài bằng cách sử dụng một bộ tìm kiếm bên ngoài để tìm kiếm thông tin lịch sử. MemLong kết hợp một mô-đun ret-mem không khả vi với một mô hình ngôn ngữ chỉ decoder có thể huấn luyện một phần và giới thiệu một cơ chế attention tìm kiếm tinh vi, có thể kiểm soát tận dụng các khối liên quan ở mức ngữ nghĩa. Đánh giá toàn diện trên nhiều benchmark mô hình ngôn ngữ ngữ cảnh dài cho thấy MemLong liên tục vượt trội hơn các LLMs tiên tiến khác. Quan trọng hơn, MemLong có thể mở rộng độ dài ngữ cảnh trên một GPU 3090 duy nhất từ 4k lên 80k1.

## 1 Giới thiệu

Mô hình Ngôn ngữ Lớn (LLMs) đã đạt được thành công đáng kể trong nhiều lĩnh vực khác nhau. Tuy nhiên, do độ phức tạp thời gian và không gian bậc hai của cơ chế attention vanilla (Vaswani et al., 2017), việc mở rộng độ dài ngữ cảnh đáng kể là một thách thức, điều này tạo ra những hạn chế đáng kể cho các ứng dụng liên quan đến các tác vụ chuỗi dài, chẳng hạn như tóm tắt tài liệu dài (Koh et al., 2022) và nhiều vòng đối thoại (Wang et al., 2024a). Kết quả là, LLMs thường được kỳ vọng duy trì khả năng làm việc dài (còn gọi là LLMs ngữ cảnh dài) để xử lý hiệu quả những tình huống đòi hỏi này.

*Đóng góp Bằng nhau.
†Tác giả liên hệ.
1Mã của chúng tôi có sẵn tại https://github.com/Bui1dMySea/MemLong

(b) MemLong: bộ nhớ và tìm kiếm thông tin lịch sử  (a) Sinh Tăng cường Tìm kiếm

Công nghệ tăng cường tìm kiếm được áp dụng như thế nào trong mô hình ngôn ngữ lớn?

Truy vấn
Lập chỉ mục Tài liệu

Khối1 :"Mô hình tìm kiếm lấy các tài liệu liên quan, và mô hình sinh sử dụng thông tin này để tạo ra đầu ra chính xác và phù hợp với ngữ cảnh hơn."

Khối2 :"Tích hợp cơ chế tìm kiếm trực tiếp vào vòng lặp huấn luyện của mô hình ngôn ngữ." ...

Tài liệu Đầu ra

Với 5 Khối RAG: "Tăng cường tìm kiếm trong mô hình ngôn ngữ lớn bao gồm việc tích hợp một hệ thống tìm kiếm thông tin để lấy ..."

Với 20 Khối RAG: "Một hệ thống thông tin tìm kiếm, thường được gọi là w''<w' ..."

Với 20 Khối Ngữ cảnh + Truy vấn

Công nghệ tăng cường tìm kiếm được áp dụng như thế nào trong mô hình ngôn ngữ lớn?

20 Khối Ngữ cảnh
Bộ nhớ Cấp Khối

MemLong
Cặp Key,Value và Embedding

Embedding Truy vấn của Đầu vào Hiện tại

Tìm kiếm Tầm xa

Tìm kiếm Bộ nhớ

Sinh Kết hợp: "Tăng cường tìm kiếm trong mô hình ngôn ngữ lớn bao gồm việc tích hợp một hệ thống tìm kiếm thông tin để lấy ..."

Hình 1: Minh họa về Sinh Tăng cường Tìm kiếm (RAG) và luồng Bộ nhớ-Tìm kiếm của MemLong. (a) RAG thậm chí có thể làm giảm hiệu suất sinh (màu vàng) khi độ dài thông tin được tìm kiếm vượt quá khả năng xử lý của mô hình. (b) Cách tiếp cận của chúng tôi sử dụng một bộ tìm kiếm bên ngoài để lấy thông tin lịch sử, sau đó được truyền vào mô hình dưới dạng cặp K-V thay vì dạng văn bản.

khả năng làm việc dài (còn gọi là LLMs ngữ cảnh dài) để xử lý hiệu quả những tình huống đòi hỏi này.

Để giải quyết tắc nghẽn tính toán, nhiều nỗ lực đã được thực hiện. Hướng công việc đầu tiên tập trung vào việc giảm tính toán của cơ chế attention vanilla (Vaswani et al., 2017) bằng cách sử dụng các phép toán attention thưa thớt (Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020; Xiao et al., 2023a; Chen et al., 2023b; Lu et al., 2024). Mặc dù những loại công việc này có thể giảm độ phức tạp tính toán xuống khoảng O(n), nhưng nó thường đi kèm với sự đánh đổi về khả năng của mô hình. Do đó, một số công việc chuyển trọng tâm sang lựa chọn bộ nhớ (Dai et al., 2019; Bertsch et al., 2024; Yu et al., 2023). Những cách tiếp cận này, như lựa chọn bộ nhớ cấp token, có thể dẫn đến việc cắt bớt thông tin ngữ nghĩa. Một hướng công việc gần đây khác là

Mô hình Ngôn ngữ Tăng cường Tìm kiếm (Wu et al., 2022; Wang et al., 2024b; Rubin and Berant, 2023). Những công việc này thường giới thiệu một cơ chế tìm kiếm để nâng cao khả năng xử lý văn bản dài của mô hình. Tuy nhiên, những phương pháp này có một số nhược điểm. Thứ nhất, thông tin được lưu trữ trong bộ nhớ có thể trải qua sự dịch chuyển phân phối do những thay đổi trong tham số mô hình trong quá trình huấn luyện. Thứ hai, những phương pháp này thường yêu cầu huấn luyện lại, điều này không thực tế trong kỷ nguyên của các mô hình lớn. Cuối cùng, những mô hình này thường dễ bị xử lý đầu vào văn bản dài với cái giá của các khả năng ban đầu của mô hình đã được huấn luyện trước. Để giải quyết những hạn chế của nghiên cứu trước đây, chúng tôi đặt ra câu hỏi sau: Chúng ta có thể sử dụng khả năng tìm kiếm rõ ràng của một bộ tìm kiếm để xấp xỉ các quá trình tìm kiếm ngầm trong mô hình không?

Trong công việc này, chúng tôi đề xuất MemLong, một phương pháp hiệu quả và nhẹ để mở rộng cửa sổ ngữ cảnh của LLMs. Ý tưởng chính là lưu trữ ngữ cảnh và kiến thức quá khứ trong một ngân hàng bộ nhớ không thể huấn luyện và tiếp tục tận dụng những embedding được lưu trữ này để tìm kiếm các cặp key-value (K-V) cấp khối để đưa vào mô hình. MemLong có thể áp dụng cho bất kỳ mô hình ngôn ngữ được huấn luyện trước chỉ decoder nào bằng cách kết hợp (1) một thành phần ret-mem bổ sung cho bộ nhớ và tìm kiếm, và (2) một mô-đun attention nhân quả tìm kiếm để tích hợp thông tin cục bộ và bộ nhớ. Quá trình bộ nhớ và tìm kiếm của MemLong được minh họa trong Hình 1(b). Trong quá trình sinh, một văn bản vượt quá độ dài xử lý tối đa của mô hình được lưu trữ như thông tin ngữ cảnh trong Ngân hàng Bộ nhớ. Tiếp theo, cho một khối văn bản được sinh gần đây trong một tài liệu dài, chúng ta sử dụng bộ tìm kiếm để tìm kiếm rõ ràng thông tin quá khứ, thu được thông tin ngữ cảnh bổ sung thông qua việc căn chỉnh chỉ mục.

MemLong mang lại một số lợi ích: (1) Nhất quán Phân phối: Không giống như các mô hình trước đây trải qua sự dịch chuyển phân phối khi thông tin được lưu trữ trong bộ nhớ, MemLong đảm bảo phân phối của thông tin được lưu trữ vẫn nhất quán. (2) Hiệu quả Huấn luyện: Chúng tôi đóng băng các lớp dưới của mô hình và chỉ cần tinh chỉnh các lớp trên, điều này giảm đáng kể chi phí tính toán. Trong các thí nghiệm của chúng tôi, tinh chỉnh một phiên bản 3B tham số của MemLong trên 0.5B token chỉ yêu cầu tám GPU 3090 trong tám giờ. (3) Cửa sổ Ngữ cảnh Rộng lớn: Vì chỉ cần ghi nhớ các cặp K-V của một lớp duy nhất, MemLong có khả năng mở rộng cửa sổ ngữ cảnh lên đến 80k token một cách dễ dàng trên một GPU 3090 duy nhất.

Các thí nghiệm rộng rãi đã chứng minh rằng MemLong thể hiện hiệu suất vượt trội trong một số khía cạnh khi so sánh với các LLMs hàng đầu khác. MemLong vượt trội hơn OpenLLaMA (Touvron et al., 2023) và các mô hình dựa trên tìm kiếm khác trên một số bộ dữ liệu mô hình ngôn ngữ ngữ cảnh dài. Trong các tác vụ học trong ngữ cảnh tăng cường tìm kiếm, MemLong đạt được cải thiện lên đến 10.2 điểm phần trăm so với OpenLLaMA.

## 2 Khái niệm cơ bản

### 2.1 Định nghĩa Tác vụ

Mô hình ngôn ngữ được thiết kế để định nghĩa phân phối xác suất trên các chuỗi token, hiệu quả dự đoán khả năng xảy ra của một chuỗi trong một ngôn ngữ nhất định. Cho một chuỗi như vậy x1, . . . , xn, cách tiếp cận tiêu chuẩn để mô hình hóa xác suất của nó là thông qua dự đoán token tiếp theo: p(x1, . . . , xn) = Σn i=0 pθ(xi|x<i), trong đó x<i := x1, . . . , xi−1 là chuỗi token đi trước xi. Khác với mục tiêu mô hình ngôn ngữ tiêu chuẩn, chúng ta không chỉ sử dụng ngữ cảnh hiện tại để đưa ra dự đoán token tiếp theo, mà còn sử dụng tìm kiếm bên ngoài để thu được thông tin liên quan và thực hiện kết hợp kiến thức ở các lớp trên của mô hình. Cụ thể, cho một chuỗi gồm l token và kích thước của mỗi khối τ, chúng ta phân chia nó thành một chuỗi dài gồm ν = l/τ khối không chồng chéo, được ký hiệu là C = (c1, . . . , cν). Tương ứng, dạng văn bản của nó được chia thành ν khối văn bản, được ký hiệu là T = (t1, . . . , tν). Trong mỗi bước, chúng ta thực hiện mô hình ngôn ngữ nhân quả trên ci trong các lớp dưới, trong khi ở các lớp trên, chúng ta tiến hành tìm kiếm có thể kiểm soát tinh vi trên ti để kết hợp thông tin bổ sung. Sau khi làm điều này, mục tiêu mô hình ngôn ngữ của chúng ta trở thành

p(x1, . . . , xn) = Σn i=0 pθ(xi|R(ti), x<i) (1)

trong đó R(ti) biểu thị việc tìm kiếm các khối lân cận của ti nơi xi được đặt.

### 2.2 Định nghĩa Mô-đun và Phép toán

Như được hiển thị trong Hình 2, mô-đun Ret-Mem bao gồm một Bộ tìm kiếm và một thành phần Bộ nhớ để trao đổi thông tin. Ban đầu, chúng ta định nghĩa thành phần Bộ nhớ là M và Bộ tìm kiếm là R, và các phép toán tương ứng của chúng M(·) và R(·). Hơn nữa, chúng ta chỉ định chiều của mô hình là dmodel, chiều của bộ tìm kiếm là dret. Mô-đun Bộ nhớ bao gồm hai phân đoạn: cặp K-V và Embedding Biểu diễn tương ứng. Chiều cho cả key và value được biểu diễn là Rdmodel và cho Embedding là Rdret. Điều quan trọng cần nhấn mạnh là quá trình tìm kiếm thực tế liên quan đến các embedding biểu diễn các khối, không phải các cặp K-V. Bộ tìm kiếm về cơ bản là một bộ nhúng dày đặc được huấn luyện trước với khả năng biểu diễn xuất sắc. MemLong sử dụng nó để mã hóa mỗi khối thành Embedding Biểu diễn. Vì nó tạo ra một vector biểu diễn một chiều cho một khối, dấu chân bộ nhớ vẫn tối thiểu ngay cả khi kích thước bộ nhớ là đáng kể.

## 3 MemLong

### 3.1 Tổng quan

Như được minh họa trong Hình 2, mỗi bước bao gồm một đầu vào của một khối ci, trong đó văn bản gốc cho khối đó là ti. Trong các lớp dưới nơi mô hình được đóng băng, attention nhân quả tiêu chuẩn được áp dụng cho toàn bộ ci. Đối với lớp cuối cùng của các lớp dưới, chúng ta gọi nó là lớp bộ nhớ. Sau mỗi lần duyệt qua lớp bộ nhớ, hai phép toán chính được thực hiện. Phép toán đầu tiên là tìm kiếm, được mô tả bởi đường màu đỏ, trong đó ti được sử dụng để lấy các cặp K-V phù hợp nhất. Phép toán thứ hai, được chỉ ra bởi đường màu xanh dương, bao gồm việc lưu trữ các cặp K-V đã thu được cùng với biểu diễn khối liên quan của chúng. Trong các lớp trên của mô hình, các cặp K-V được tìm kiếm được tích hợp với ngữ cảnh đầu vào hiện tại, sau đó điều chỉnh các tham số mô hình để hiệu chỉnh tham chiếu tìm kiếm. Các phần tiếp theo sẽ khám phá các khía cạnh khác nhau của khung MemLong và sự phức tạp của chúng, bao gồm Bộ tìm kiếm và Quản lý Bộ nhớ Động (§ 3.2), Cải tạo Attention (§ 3.3), và Suy luận với MemLong (§ 3.4).

### 3.2 Bộ tìm kiếm và Quản lý Bộ nhớ Động

Chúng tôi cung cấp một giải thích toàn diện về quá trình tìm kiếm và động lực của quản lý bộ nhớ.

Quá trình Tìm kiếm. Với mục tiêu thay thế tìm kiếm kNN truyền thống dựa trên cặp K-V bằng tìm kiếm rõ ràng, chúng ta nhằm mục đích pre-fetch thông tin mong muốn khi khả thi trước mỗi đầu vào mô hình. Cụ thể, cho mỗi khối truy vấn tiềm năng cq = ci và khối văn bản tương ứng tq = ti, trước tiên chúng ta truyền nó qua Bộ tìm kiếm và sau đó thu được một embedding biểu diễn rq = R(tq), trong đó rq ∈ Rdret. Tiếp theo, chúng ta sử dụng embedding biểu diễn này để thực hiện tìm kiếm đối với các embedding trong M để thu được k chỉ số cấp khối cần thiết. Chúng ta tính toán độ tương tự cosine giữa biểu diễn tìm kiếm rq và các embedding được lưu trữ trong Bộ nhớ M. Cuối cùng, chúng ta nhận được k chỉ số hàng đầu zq = TopK{Cos(rq)} cho cq, trong đó zq ∈ Rk. Do tính chất liền kề trong các khối, chúng ta có thể dễ dàng mở rộng các chỉ số thu được để bao phủ toàn bộ phạm vi liên quan cho tìm kiếm. Cuối cùng, chúng ta tìm kiếm các cặp K-V tương ứng ˜zq ∈ Rk×τ×dmodel từ Bộ nhớ dựa trên những chỉ số này và sử dụng cho lớp trên. Điều đáng chú ý là chúng ta đã trang bị Bộ nhớ với một cơ chế đếm để ghi lại tần suất tìm kiếm cho mỗi chỉ số chứa trong đó. Dữ liệu tần suất này sau đó sẽ phục vụ như một cơ sở cho việc cập nhật bộ nhớ động, cho phép ưu tiên thông tin được tìm kiếm thường xuyên hơn.

Quá trình Bộ nhớ. Quá trình bộ nhớ đồng bộ lưu trữ các cặp K-V từ lớp bộ nhớ và embedding biểu diễn đã tính toán trước đó để tìm kiếm, đảm bảo rằng các chỉ số cho cặp K-V tương ứng chính xác với các embedding biểu diễn của chúng (xem Hình 2, bên phải, đường màu xanh dương). Đối với mỗi bộ nhớ khối có thể cm = ci, và khối văn bản tương ứng tm = ti, chúng ta chia quá trình bộ nhớ thành hai phần: phần đầu tiên chi tiết cách lưu trữ các cặp K-V, và phần thứ hai giải thích cách lưu trữ các biểu diễn tương ứng. Đầu tiên, chúng ta đưa cm vào MemLong và nhận đầu ra từ lớp bộ nhớ. Điều đáng chú ý là, vì các lớp dưới được đóng băng trong quá trình huấn luyện, chúng ta có thể đảm bảo rằng phân phối của các cặp K-V đầu ra là nhất quán. Sự nhất quán này rất quan trọng để tránh vấn đề dịch chuyển phân phối, trước đây đã được quan sát thấy trong các mô hình như MemTrm (Wu et al., 2022). Phép toán bộ nhớ của chúng ta rất hiệu quả vì nó chỉ liên quan đến việc lưu trữ các biểu diễn cần thiết cho tìm kiếm, rm = rq, do đó tránh sự dư thừa. Sau khi tìm kiếm cho tất cả các cặp khối hoàn thành, phép toán bộ nhớ—được ký hiệu là M(k, v; rm)—đồng bộ cập nhật bộ nhớ với cả cặp Key-Value và các biểu diễn tương ứng của chúng.

Cập nhật Bộ nhớ Động. Khi bộ nhớ bị tràn, chúng ta sử dụng Bộ đếm để cập nhật bộ nhớ một cách thông minh. Trong các thí nghiệm của chúng tôi, chúng tôi giữ 10% nội dung bộ nhớ mới nhất do tính liên quan tiềm năng của nó, loại bỏ 10% cũ nhất vì có thể đã lỗi thời, và ưu tiên 80% giữa dựa trên tần suất tìm kiếm, xóa các mục được truy cập ít nhất cho đến khi việc sử dụng bộ nhớ giảm xuống 50%. Việc cắt tỉa có chọn lọc này cân bằng tính gần đây và liên quan, giữ lại thông tin có giá trị và loại bỏ dữ liệu ít phù hợp hơn. Không giống như các chiến lược FIFO truyền thống, phương pháp của chúng tôi tập trung vào tần suất tìm kiếm để cắt tỉa thông tin dư thừa một cách hiệu quả, duy trì một bộ dữ liệu chất lượng cao. Quyết định cập nhật dynamically datastore là một sự đánh đổi giữa hiệu quả và hiệu suất. Đối với các tác vụ yêu cầu phụ thuộc dài hạn, lưu trữ tất cả thông tin có thể nâng cao việc xử lý toàn diện, nhưng đối với các tác vụ ngắn hạn hơn, cập nhật động phù hợp hơn. Cập nhật động kiểm soát kích thước bộ nhớ để ngăn chặn các vấn đề hết bộ nhớ, loại bỏ thông tin cũ, và giảm overhead tìm kiếm, đảm bảo hiệu quả mà không làm giảm đáng kể hiệu suất.

### 3.3 Cải tạo Attention

Trong các lớp trên có thể huấn luyện của mô hình, chúng tôi đã sửa đổi các attention để kết hợp với bộ nhớ dài hạn. Như được minh họa trong Hình 3, không giống như các lớp decoder Transformer truyền thống sử dụng Multi-Head Attention (Vaswani et al., 2017), chúng tôi đề xuất một Attention Nhân quả Tìm kiếm để mở rộng nó thành một cơ chế attention kết hợp và đề xuất một quá trình kết hợp bộ nhớ dài hạn để cho phép mỗi token tham gia vào cả ngữ cảnh cục bộ và ngữ cảnh quá khứ cấp khối có ngữ nghĩa hoàn chỉnh và liên tục. Với trạng thái ẩn theo đầu từ lớp trước Hl−1 ∈ R|x|×dmodel và các cặp key-value được tìm kiếm tương ứng là ˜zq = {˜Ki, ˜Vi}ω i=1 ∈ Rk×τ×dmodel, trạng thái ẩn đầu ra cho lớp tiếp theo Hl được tính toán như:

Sa = Softmax(QKT/√d) (2)

Sm = Concat[Softmax(˜zq i)]ω i=1 (3)

Để tránh sự can thiệp gây ra bởi điểm attention tìm kiếm Sm ở giai đoạn đầu của huấn luyện, chúng tôi áp dụng một cơ chế multi-head attention theo cách tiếp cận của LLaMA-adapter (Zhang et al., 2023b):

Sg l = [(Sm) · gl; (Sa)]T (4)

Cuối cùng, chúng ta nối ˜V và V để thu được Hl:

Vl = [˜Vc; Vi], Hl = Sg l Vl (5)

### 3.4 Suy luận với MemLong

Khi MemLong nhận một đầu vào vượt quá độ dài, chúng ta coi nó như hai phân đoạn: tiền tố và chính. Chúng ta sẽ mô tả riêng biệt việc mã hóa đầu vào dài và việc sinh đầu ra dài trong giai đoạn suy luận. Khi MemLong nhận đầu vào dài, trước tiên nó chia tiền tố thành nhiều khối không chồng chéo và tính toán từ lớp bộ nhớ của nó, điều này đảm bảo rằng số lượng token liên quan đến attention bằng kích thước khối, nhỏ hơn nhiều so với độ dài đầu vào. Điều quan trọng cần lưu ý là mỗi khối có mối liên hệ với nhau (ví dụ, khối thứ t cần xử lý của t−1 khối trước đó).

Bước thứ hai là chọn k khối liên quan nhất cho phần chính dựa trên biểu diễn tìm kiếm cấp khối và thu được biểu diễn key và value của chúng. Sau đó, đối với các lớp tìm kiếm trên, cửa sổ attention cho tìm kiếm tương đương với k∗τ, cũng nhỏ hơn độ dài đầu vào. Cuối cùng, cả attention nhân quả hạn chế độ dài và attention tìm kiếm đều được thực hiện một cách hiệu quả.

## 4 Thí nghiệm

Chúng tôi đánh giá mô hình MemLong được đề xuất trên các tác vụ khác nhau yêu cầu xử lý ngữ cảnh dài trong bộ nhớ: (a) mô hình ngôn ngữ ngữ cảnh dài và mô hình ngôn ngữ tăng cường tìm kiếm; (b) học trong ngữ cảnh có thể mở rộng có khả năng xử lý một số lượng lớn ví dụ minh họa trong bộ nhớ.

### 4.1 Chi tiết Triển khai

Chi tiết Huấn luyện. Chúng tôi sử dụng OpenLLaMA-3B làm LLM backbone được huấn luyện trước với mã hóa vị trí xoay (Su et al., 2024). Do hạn chế phần cứng, chúng tôi chọn huấn luyện các mô hình của mình bằng kỹ thuật LoRA (Hu et al., 2021). LLM backbone có kiến trúc L = 26, H = 32, d = 100. Trừ khi được chỉ định khác, chúng tôi sử dụng lớp thứ 13 làm lớp bộ nhớ và các lớp [14,18,22,26] làm các lớp tăng cường tìm kiếm. Việc huấn luyện cho thích ứng tăng cường tìm kiếm chỉ lặp lại trên 0.5B token với độ dài chuỗi 1024. Các tham số có thể huấn luyện của MemLong là từ lớp 14 đến 26. Chúng tôi sử dụng bộ dữ liệu slimpajama được lấy mẫu bởi (Fu et al., 2024) làm corpus huấn luyện của chúng tôi.

Ánh xạ lại Vị trí. Có một số K-V cấp khối trong M được tìm kiếm để sinh. Do tính không chắc chắn của tìm kiếm ở mỗi bước, chúng ta cần ánh xạ lại embedding vị trí cho các khối được tìm kiếm. Giống như công việc trước đây (Tworkowski et al., 2024), ngữ cảnh cục bộ (lên đến 2048 token) nhận mã hóa vị trí xoay tiêu chuẩn, trong khi các key bộ nhớ được mã hóa như thể chúng có vị trí 0 trong cửa sổ ngữ cảnh cục bộ.

### 4.2 Mô hình Ngôn ngữ Ngữ cảnh Dài

Trước tiên chúng tôi đánh giá MemLong trên các benchmark mô hình ngôn ngữ ngữ cảnh dài để đánh giá khả năng mô hình ngôn ngữ cơ bản. Do bộ đệm K-V cung cấp thông tin nền và ngữ cảnh đáng kể, MemLong có thể tìm kiếm bộ đệm K-V liên quan một cách nhanh chóng và tận dụng đầy đủ nó, do đó nâng cao khả năng của mô hình trong các tác vụ mô hình ngữ cảnh dài.

Bộ dữ liệu. Chúng tôi tiến hành đánh giá mô hình của chúng tôi trên bốn bộ dữ liệu benchmark văn bản mở rộng: sách tiếng Anh PG-19 (Rae et al., 2019) và BookCorpus (Zhu et al., 2015), bài viết Wikipedia Wikitext-103 (Merity et al., 2016), và các bài báo toán học Proof-Pile (Azerbayev et al., 2023). Kết quả thí nghiệm cho thấy sự cải thiện perplexity đáng kể trên tất cả các bộ dữ liệu. Mô hình của chúng tôi được kiểm tra trên các độ dài khác nhau từ 1024 đến 32768 token. Trên tất cả các bộ dữ liệu, mô hình của chúng tôi đã thể hiện những cải thiện hiệu suất đáng kể với overhead bộ nhớ tối thiểu bằng cách tận dụng một bộ tìm kiếm bên ngoài và bộ nhớ.

Thiết lập. Theo (Yen et al., 2024), chúng tôi tính toán perplexity trên 2048 token cuối cùng của mỗi chuỗi. Thiết lập thí nghiệm này được thiết kế để xác thực ảnh hưởng của các kích thước bộ tìm kiếm khác nhau đến hiệu suất tổng thể của mô hình chúng tôi. Để triển khai tìm kiếm tinh vi hiệu quả, chúng tôi sử dụng bộ công cụ faiss (Johnson et al., 2019) để xây dựng một chỉ mục tìm kiếm chính xác trên GPU để lưu trữ Embedding Biểu diễn của các khối văn bản và thực hiện tìm kiếm hiệu quả. Đối với MemLong, chúng tôi chia và đặt các token trên finetune-length = 1024 vào M được sử dụng để tìm kiếm thêm.

Baseline. Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng mô hình OpenLLaMA-3B làm baseline. Để đảm bảo so sánh công bằng, chúng tôi sử dụng một cấu hình LoRA giống hệt nhau và tinh chỉnh các mô hình trên cùng một lượng dữ liệu từ bộ dữ liệu slimpajama. Thêm vào đó, chúng tôi so sánh LongLLaMA-3B (Tworkowski et al., 2024), được tinh chỉnh với phương pháp Focused Transformer (FoT) và 5B token. Để thực hiện so sánh toàn diện hơn, chúng tôi bổ sung kiểm tra hai mô hình 7B: LLaMA-2-7B và LongLoRA-7B-32K (Chen et al., 2023b) và hai mô hình mã hóa vị trí: Yarn-7b-128k (Peng et al., 2023) và Phi3-128k (Abdin et al., 2024).

Kết quả. Kết quả được hiển thị trong Bảng 1. Chúng tôi sử dụng Perplexity (PPL) làm thước đo đánh giá cho mô hình ngôn ngữ. PPL thấp hơn cho thấy khả năng mô hình ngôn ngữ mạnh hơn. So với hai mô hình được tinh chỉnh đầy đủ, OpenLLaMA-3B và LLaMA-2-7B, mô hình của chúng tôi thể hiện hiệu suất tương đương trên nhiều bộ dữ liệu khi độ dài kiểm tra nằm trong giới hạn được huấn luyện trước của chúng (2048 cho OpenLLaMA-3B và 4096 cho LLaMA-2-7B). Tuy nhiên, một khi độ dài kiểm tra vượt quá những giới hạn được huấn luyện trước này, mô hình của chúng tôi tiếp tục giảm perplexity ngay cả vượt ra ngoài độ dài tinh chỉnh 1024 và độ dài được huấn luyện trước 2048, thể hiện khả năng tổng quát hóa vượt trội. Ngược lại, các mô hình OpenLLaMA-3B và LLaMA-2-7B không thể tổng quát hóa cho đầu vào vượt quá độ dài được huấn luyện trước của chúng và thể hiện overhead bộ nhớ tăng đáng kể do độ phức tạp bậc hai của attention. Chúng tôi cũng so sánh mô hình của mình với LongLoRA. Mặc dù Shifted Sparse Attention được đề xuất trong LongLoRA giảm đáng kể việc sử dụng bộ nhớ, nó cũng làm giảm hiệu suất của mô hình trên văn bản ngắn. Ngược lại, LongLLaMA, có thể lưu trữ các cặp K-V, gặp phải các vấn đề OOM khi độ dài kiểm tra trở nên quá dài do việc sử dụng bộ nhớ tăng trưởng vô hạn.

Các mô hình mã hóa vị trí có khả năng tổng quát hóa mạnh. Tuy nhiên, hiệu suất của những phương pháp như vậy chỉ có thể đảm bảo rằng hiệu suất sinh trên khoảng cách dài không bị suy giảm. So với các phương pháp của chúng, MemLong tận dụng một bộ tìm kiếm bên ngoài để xử lý token đầu vào dài hơn và đạt được cải thiện perplexity tốt hơn. Đồng thời, do hiệu quả lưu trữ cao, MemLong có thể kiểm soát hiệu quả việc sử dụng GPU để tránh các vấn đề OOM.

### 4.3 Học Trong Ngữ cảnh

Học trong ngữ cảnh truyền thống (ICL; Brown et al., 2020) đưa vào mô hình các ví dụ minh họa không tham số hóa few-shot cùng với truy vấn. Tuy nhiên, những phương pháp này thường bị ràng buộc bởi độ dài đầu vào của mô hình. Trong thí nghiệm này, vì MemLong có thể lưu trữ các ví dụ dưới dạng tham số hóa trong bộ nhớ của nó, chúng tôi chủ yếu điều tra liệu MemLong có thể sử dụng hiệu quả kiến thức được lưu trữ trong bộ nhớ của nó để nâng cao khả năng nổi lên của nó hay không. Kết quả được hiển thị trong Bảng 2. So với OpenLLaMA, chỉ dựa vào kiến thức không tham số, với cùng số lượng minh họa trong ngữ cảnh, MemLong có thể sử dụng các minh họa bổ sung được lưu trữ trong bộ nhớ của nó. Hiệu suất tiếp tục tăng hoặc vẫn nhất quán với nhiều minh họa hơn trong bộ nhớ. Trong phân tích so sánh của chúng tôi với LongLLaMA, người ta quan sát thấy rằng mô hình của chúng tôi vượt trội hơn LongLLaMA trên phần lớn các bộ dữ liệu trong cùng điều kiện bảo tồn Minh họa Trong Bộ nhớ. Điều quan trọng cần nhấn mạnh là mô hình của chúng tôi hoạt động với các tham số huấn luyện thấp hơn đáng kể (200M so với 0.3B) và khối lượng dữ liệu tinh chỉnh (0.5B so với 5B) so với LongLLaMA. Điều này nhấn mạnh hiệu quả của mô hình chúng tôi trong việc tận dụng một bộ tìm kiếm bên ngoài để thu thập thông tin, thể hiện khả năng tổng hợp và sử dụng kiến thức hiệu quả vượt trội với ít tài nguyên hơn đáng kể.

## 5 Nghiên cứu Phân tích

### 5.1 Thiết lập Huấn luyện

Trong giai đoạn huấn luyện, chúng tôi khám phá tác động của việc thay đổi các lớp tìm kiếm đối với mô hình và kiểm tra liệu vấn đề dịch chuyển phân phối, như đã thảo luận trong MemTrm (Wu et al., 2022), có thể được giải quyết đầy đủ bằng cách tiếp cận của chúng tôi hay không. Như đã đề cập trước đó, phương pháp của chúng tôi đề xuất một giải pháp chi phí thấp cho sự dịch chuyển phân phối. Như được hiển thị trong Hình 4, đường màu nâu (đường ở đầu hình; phương pháp huấn luyện tương tự như MemTrm tinh chỉnh tất cả các tham số của mô hình và tất cả các lớp sau lớp bộ nhớ đều tham gia vào tìm kiếm) kém hơn đáng kể so với tất cả các phương pháp khác của chúng tôi (ngay cả những thiết lập không hợp lý nhất) về hiệu suất và tốc độ fitting. Chúng tôi sẽ phân tích hiệu suất của giai đoạn lý luận sau này.

### 5.2 Hiệu suất Suy luận

Hình 4: Mức độ PPL trong giai đoạn huấn luyện. Chỉ số cho trục y là PPL. Chúng tôi chủ yếu tập trung vào các tham số huấn luyện và các lớp tìm kiếm. Chúng tôi cung cấp các thiết lập tham số cụ thể của mỗi đường trong phụ lục A.

Câu hỏi 1: Độ dài bộ nhớ có ảnh hưởng đến hiệu suất của mô hình không? Như được mô tả trong Hình 5, kiểm tra của chúng tôi về hiệu suất của cùng một mô hình trên các kích thước bộ nhớ khác nhau cho thấy một mối tương quan rõ ràng giữa dung lượng bộ nhớ và hiệu quả mô hình. Xu hướng cho thấy rằng việc tăng dần kích thước bộ nhớ mang lại những cải thiện dần dần về hiệu suất. Hơn nữa, một ngưỡng quan trọng được xác định ở kích thước bộ nhớ 65536, vượt quá ngưỡng này khả năng của mô hình trải qua một bước nhảy vọt đáng kể. Điều này cho thấy rằng trong khi việc mở rộng bộ nhớ mang lại lợi ích đáng kể, có một trần thực tế cho hiệu quả của nó, có thể bị ảnh hưởng bởi những sắc thái của phân phối dữ liệu.

Câu hỏi 2: Chúng ta cần bao nhiêu lớp để giới thiệu thông tin bộ nhớ bổ sung? Như được hiển thị trong Hình 4, (đường màu hồng) và Bảng 3 (RPL+TH), mô hình hoạt động tốt nhất khi số lượng lớp tìm kiếm được đặt thành [13,17,21,25]. Người ta tin rằng theo kinh nghiệm nếu thông tin tìm kiếm được đưa vào tất cả các lớp trên của mô hình, nó dẫn đến sự giảm sút trong sự chú ý của mô hình đến ngữ cảnh cục bộ. Do đó, việc chọn các lớp tìm kiếm ở các khoảng thích hợp thực sự có thể nâng cao khả năng của mô hình.

## 6 Công việc Liên quan

### 6.1 Mô hình Ngôn ngữ Ngữ cảnh Dài

Mô hình Ngôn ngữ Ngữ cảnh Dài chủ yếu tập trung vào mở rộng độ dài và mở rộng cửa sổ ngữ cảnh. Các nghiên cứu Mở rộng Độ dài thường nhắm đến mã hóa RoPE phổ biến, nhằm mục đích mở rộng PE chưa thấy vào không gian của các vị trí được nhìn thấy trong quá trình huấn luyện trước. Những công việc này (Su et al., 2024; Press et al., 2021; Chen et al., 2023a; Peng et al., 2023) cho phép mô hình tổng quát hóa cho các mã hóa vị trí chưa thấy trong quá trình suy luận, do đó đạt được ngoại suy vượt ra ngoài các độ dài gặp phải trong quá trình huấn luyện. Ngược lại, phương pháp của chúng tôi không yêu cầu sửa đổi PE, và chỉ sử dụng một mô-đun bổ sung để mở rộng ngữ cảnh. Mở rộng Cửa sổ Ngữ cảnh tập trung vào cách mở rộng cửa sổ ngữ cảnh mà LLMs có thể xử lý đầu vào cùng một lúc. Do độ phức tạp thời gian và không gian bậc hai của việc tính toán attention, việc mở rộng độ dài đầu vào của các mô hình ngôn ngữ là khá thách thức. Các kỹ thuật attention thưa thớt (Kitaev et al., 2020; Chen et al., 2023b; Tworkowski et al., 2024; Bertsch et al., 2024; Beltagy et al., 2020) đã đạt được những tiến bộ đáng kể, nhưng trọng tâm của chúng tôi là cải thiện mô hình ngôn ngữ tầm xa bằng cách cho phép LLMs truy cập thông tin liên quan ở độ dài đầu vào ngắn hơn thông qua một phương pháp tăng cường tìm kiếm.

### 6.2 Mô hình Ngôn ngữ Tăng cường Tìm kiếm

Nhiều nỗ lực đã được thực hiện để nâng cao Mô hình Ngôn ngữ Tăng cường Tìm kiếm (Lewis et al., 2020; Izacard and Grave, 2020; Ram et al., 2023; Yu et al., 2022; Asai et al., 2023). Trong khi một số cách tiếp cận sử dụng các bộ tìm kiếm bên ngoài, việc kết hợp thông tin không tham số thường không đạt hiệu quả so với các phương pháp tham số trong mô hình. Chúng tôi tập trung vào việc tích hợp các khái niệm tìm kiếm trực tiếp vào mô hình. REALM (Guu et al., 2020) gợi ý rằng việc chỉ dựa vào kiến thức mô hình nội bộ là không hiệu quả và ủng hộ việc mô hình học cách tìm kiếm và hiểu. kNN-LM (Khandelwal et al., 2019) nâng cao mô hình ngôn ngữ bằng cách pha trộn các dự đoán từ tiếp theo của LLM với những dự đoán từ một cơ chế dựa trên tìm kiếm. MemTrm (Wu et al., 2022) giới thiệu một ngân hàng bộ nhớ nhưng có nguy cơ thay đổi phân phối bộ nhớ do điều chỉnh tham số. LongMEM (Wang et al., 2024b) giảm thiểu điều này bằng cách huấn luyện một mạng con, mặc dù điều này thêm overhead đáng kể. Ngược lại, cách tiếp cận của chúng tôi liên quan đến một mô hình được huấn luyện trước cố định, nâng cao nó với một bộ tìm kiếm đóng băng phù hợp với các quá trình tìm kiếm nội bộ của mô hình, do đó tránh sự dịch chuyển phân phối và các thay đổi kiến trúc.

## 7 Kết luận

Chúng tôi giới thiệu MemLong, một cách tiếp cận sáng tạo nâng cao đáng kể khả năng của các mô hình ngôn ngữ để xử lý văn bản dài bằng cách tận dụng một bộ tìm kiếm bên ngoài. MemLong sử dụng một bộ tìm kiếm thành thạo để truy cập nhanh chóng và chính xác văn bản liên quan đến ngữ cảnh xa với overhead bộ nhớ tối thiểu. MemLong thành công mở rộng cửa sổ ngữ cảnh của mô hình từ 2k đến 80k token. Chúng tôi chứng minh rằng MemLong thể hiện lợi thế cạnh tranh đáng kể trong các tác vụ mô hình và hiểu văn bản khoảng cách dài. MemLong có thể đạt được cải thiện hiệu suất lên đến 10.4 điểm phần trăm so với mô hình ngữ cảnh đầy đủ.

## Hạn chế

Công việc của chúng tôi chủ yếu tập trung vào OpenLLaMA-3B. Chúng tôi hy vọng rằng nghiên cứu trong tương lai sẽ khám phá và điều tra việc áp dụng các phương pháp của chúng tôi cho các mô hình có kích thước khác nhau. Đồng thời, người ta đã phát hiện ra rằng trong khi các Cặp K-V một lớp có thể cung cấp thông tin ngữ nghĩa bổ sung cho các lớp trên, thông tin này không ổn định. Chúng tôi hy vọng rằng công việc trong tương lai có thể cung cấp một khung hợp lý hơn để chứa các phương pháp của chúng tôi. Đồng thời, chúng tôi sử dụng một bộ tìm kiếm với FlagEmbeddings cố định (Xiao et al., 2023b; Zhang et al., 2023a), nhưng việc nghiên cứu một phạm vi bộ tìm kiếm lớn hơn sẽ hữu ích.

## Tuyên bố Đạo đức

Trong việc theo đuổi tiến bộ kiến thức và phát triển các giải pháp sáng tạo, chúng tôi cam kết duy trì các tiêu chuẩn đạo đức cao nhất. Công việc của chúng tôi được hướng dẫn bởi sự cống hiến kiên định cho tính toàn vẹn, minh bạch và tôn trọng tất cả các cá nhân và cộng đồng liên quan. Vì các mô hình được huấn luyện trước có thể có một số thiên vị do sự hiện diện không thể tránh khỏi của corpus có hại/xúc phạm trong quá trình huấn luyện, việc tinh chỉnh MemLong trên Slimpajama cũng sẽ đối mặt với vấn đề này. Mặc dù việc giải quyết vấn đề này nằm ngoài công việc hiện tại của chúng tôi, chúng tôi hy vọng rằng sẽ có công việc trong tương lai giải quyết tốt loại vấn đề này.

## Tài liệu tham khảo

Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219.

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511.

Zhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. 2023. Proof-pile: A pre-training dataset of mathematical text.

Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. 2024. Unlimiformer: Long-range transformers with unlimited length input. Advances in Neural Information Processing Systems, 36.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. 2024. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training. arXiv: Computation and Language,arXiv: Computation and Language.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.

Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.

Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.

Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan. 2022. An empirical survey on long document summarization: Datasets, models, and metrics. ACM computing surveys, 55(8):1–35.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474.

Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Longheads: Multi-head attention is secretly a long context processor. arXiv preprint arXiv:2402.10685.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071.

Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint.

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316–1331.

Ohad Rubin and Jonathan Berant. 2023. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421.

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. 2024. Focused transformer: Contrastive training for context scaling. Advances in Neural Information Processing Systems, 36.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding Lin, Wenjie Li, and Xiao-Yong Wei. 2024a. Instruct once, chat consistently in multiple rounds: An efficient tuning framework for dialogue. arXiv preprint arXiv:2402.06967.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.

Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2024b. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems, 36.

Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. arXiv preprint arXiv:2203.08913.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023a. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453.

Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023b. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597.

Howard Yen, Tianyu Gao, and Danqi Chen. 2024. Long-context language modeling with parallel context encoding. arXiv preprint arXiv:2402.16617.

Haofei Yu, Yue Zhang, Wei Bi, et al. 2023. Trams: Training-free memory selection for long-range language modeling. arXiv preprint arXiv:2310.15494.

Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063.

Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023a. Retrieve anything to augment large language models. Preprint, arXiv:2310.07554.

Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. 2023b. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.

## A Thiết lập Huấn luyện Khác nhau

Như được hiển thị trong 4, chúng tôi liệt kê các giá trị biến tương ứng với các tên thiết lập khác nhau trong thí nghiệm phân tích.

| Tên Thiết lập | Lớp Tìm kiếm | Lớp Bộ nhớ | Tham số Huấn luyện |
|---------------|--------------|------------|-------------------|
| Retreival_All_and_Training_All | [14,15, . . .,26] | 13 | Tất cả Tham số Có thể Huấn luyện của Mô hình |
| Retreival_All_and_Training_Half | [14,15, . . .,26] | 13 | Một nửa Tham số Có thể Huấn luyện của Mô hình |
| Retreival_Partial_and_Training_Half | [14,16,18, . . .,26] | 13 | Một nửa Tham số Có thể Huấn luyện của Mô hình |
| Retreival_lower_Partial_and_Training_Half | [14,18,22,26] | 13 | Một nửa Tham số Có thể Huấn luyện của Mô hình |

Bảng 4: Các tham số cụ thể của các tên thiết lập khác nhau.
