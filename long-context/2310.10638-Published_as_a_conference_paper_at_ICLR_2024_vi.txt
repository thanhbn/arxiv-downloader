# 2310.10638.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.10638.pdf
# Kích thước tệp: 1086613 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
IN-CONTEXT PRETRAINING : MODELING NGÔN NGỮ
VƯỢT QUA RANH GIỚI TÀI LIỆU
Weijia Shi1,2Sewon Min1,2Maria Lomeli1Chunting Zhou1
Margaret Li1,2Gergely Szilvasy1Rich James1Xi Victoria Lin1
Noah A. Smith2,3Luke Zettlemoyer1,2Scott Yih1Mike Lewis1
1Meta AI2University of Washington3Allen Institute for AI
swj0419@cs.washington.edu
TÓM TẮT
Các mô hình ngôn ngữ lớn (LMs) hiện được huấn luyện để dự đoán token dựa trên
tiền tố tài liệu, cho phép chúng thực hiện trực tiếp việc tạo sinh dài và các tác vụ
theo phong cách prompting có thể được rút gọn thành hoàn thành tài liệu. Các
pipeline huấn luyện sẵn hiện tại huấn luyện LMs bằng cách nối các tập hợp ngẫu
nhiên của các tài liệu ngắn để tạo ngữ cảnh đầu vào nhưng các tài liệu trước đó
không cung cấp tín hiệu nào để dự đoán tài liệu tiếp theo. Thay vào đó, chúng tôi
trình bày IN-CONTEXT PRETRAINING, một phương pháp mới trong đó các mô
hình ngôn ngữ được huấn luyện sẵn trên một chuỗi các tài liệu liên quan, do đó
khuyến khích một cách rõ ràng chúng đọc và suy luận qua ranh giới tài liệu. Chúng
tôi có thể thực hiện IN-CONTEXT PRETRAINING bằng cách đơn giản thay đổi
thứ tự tài liệu sao cho mỗi ngữ cảnh chứa các tài liệu liên quan, và áp dụng trực
tiếp các pipeline huấn luyện sẵn hiện tại. Tuy nhiên, bài toán sắp xếp tài liệu này
là thách thức. Có hàng tỷ tài liệu và chúng tôi muốn sắp xếp để tối đa hóa sự
tương tự ngữ cảnh cho mọi tài liệu mà không lặp lại bất kỳ dữ liệu nào. Để làm
điều này, chúng tôi giới thiệu các thuật toán xấp xỉ để tìm tài liệu liên quan với
tìm kiếm láng giềng gần hiệu quả và xây dựng ngữ cảnh đầu vào mạch lạc với
thuật toán duyệt đồ thị. Các thí nghiệm của chúng tôi cho thấy IN-CONTEXT
PRETRAINING cung cấp một phương pháp đơn giản và có thể mở rộng để cải
thiện đáng kể hiệu suất của LMs: chúng tôi thấy những cải thiện đáng chú ý trong
các tác vụ đòi hỏi suy luận ngữ cảnh phức tạp hơn, bao gồm học trong ngữ cảnh
(+8%), đọc hiểu (+15%), trung thành với ngữ cảnh trước đó (+16%), suy luận
ngữ cảnh dài (+5%), và tăng cường truy xuất (+9%).

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LMs) được huấn luyện để hoàn thành tài liệu; mỗi token được dự đoán
dựa trên ngữ cảnh được cung cấp bởi tiền tố của tài liệu mà nó xuất hiện trong đó. Những ngữ cảnh
như vậy có thể rất đa dạng, đặc biệt ở quy mô huấn luyện sẵn, cho phép các mô hình xuất sắc trong
các tác vụ đa dạng như tuân theo hướng dẫn (Ouyang et al., 2022), giao diện đối thoại (OpenAI,
2023), đọc hiểu (Zhang et al., 2020), và học trong ngữ cảnh (Brown et al., 2020). Tuy nhiên, các
nghiên cứu gần đây làm nổi bật rằng LMs đôi khi gặp khó khăn để hiểu các ngữ cảnh phức tạp hơn:
chúng có thể thất bại trong việc tuân theo hướng dẫn một cách chính xác (McKenzie et al., 2023;
Efrat & Levy, 2020; Liu & Liu, 2023), gặp khó khăn với suy luận trên các tài liệu có điều kiện (Liu
et al., 2023; Shi et al., 2023a), và thể hiện phương sai cao trong học trong ngữ cảnh (Zhao et al.,
2021). Trong bài báo này, chúng tôi trình bày IN-CONTEXT PRETRAINING, một phương pháp
huấn luyện sẵn mới học để dự đoán token dựa trên một chuỗi các tài liệu liên quan, cho phép mô
hình một cách rõ ràng đọc và suy luận về các ngữ cảnh đa dạng và dài hơn nhiều vượt ra ngoài
ranh giới tài liệu.

Các pipeline huấn luyện LM hiện tại nối các tập hợp ngẫu nhiên của các tài liệu ngắn hơn để tạo
cửa sổ ngữ cảnh dài hơn. Tuy nhiên, các tài liệu trước đó không cung cấp tín hiệu nào để dự đoán
tài liệu tiếp theo, gây ra overhead tính toán không cần thiết cho các token không yêu cầu giao tiếp
giữa chúng (de Vries, 2023). IN-CONTEXT PRETRAINING thay vào đó sắp xếp lại dữ liệu huấn
luyện sẵn bằng cách kết hợp một số tài liệu liên quan về mặt ngữ nghĩa để tạo một ngữ cảnh đầu
vào mạch lạc, do đó phơi bày LMs với các ngữ cảnh liên quan dài và cung cấp tín hiệu huấn luyện
sẵn vượt ra ngoài ranh giới tài liệu. Chúng tôi minh họa điều này thông qua một ví dụ trong Hình
1: khi dự đoán các token tiếp theo cho cụm từ " Cho năm 2022, FIFA đặt tiền thưởng ở mức $42m, "
một tài liệu trước đó nói rằng " World Cup chưa bao giờ trao thưởng
1arXiv:2310.10638v6  [cs.CL]  24 Jun 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 1: Tổng quan về IN-CONTEXT PRETRAINING. Khác với chiến lược huấn luyện sẵn tiêu
chuẩn đặt các tài liệu được xáo trộn ngẫu nhiên trong ngữ cảnh đầu vào, IN-CONTEXT PRETRAINING
đặt các tài liệu liên quan trong cùng một ngữ cảnh, làm cho các mô hình học cách suy luận qua các
tài liệu trước đó. Ví dụ, khi dự đoán các token tiếp theo cho cụm từ " Cho năm 2022, FIFA đặt tiền
thưởng ở mức $42m, " LMs có thể tham chiếu các tài liệu trước đó nói " World Cup chưa bao giờ
trao thưởng hơn $10M trước năm 2022 " và học cách suy luận rằng " cao nhất từ trước đến nay ."

hơn $10M trước năm 2022 " có thể có trong ngữ cảnh, cho phép dự đoán một phần tiếp theo như
"cao nhất từ trước đến nay ." Vì IN-CONTEXT PRETRAINING chỉ thay đổi thứ tự tài liệu và để
nguyên tất cả các khía cạnh khác của huấn luyện sẵn LM, nó có thể được tích hợp dễ dàng vào các
pipeline huấn luyện sẵn hiện tại cho các LMs quy mô lớn.

Tuy nhiên, bài toán sắp xếp tài liệu này là thách thức. LMs thường được huấn luyện trên hàng tỷ
tài liệu và chúng tôi muốn sắp xếp chúng để tối đa hóa sự tương tự tài liệu trong các cửa sổ ngữ
cảnh đầu vào mà không lặp lại bất kỳ dữ liệu nào. Chúng tôi giới thiệu hai thuật toán xấp xỉ mới để
giải quyết những thách thức này. Chúng tôi sử dụng một mô hình truy xuất được ghép nối với một
chỉ mục tìm kiếm hiệu quả để xây dựng một đồ thị tài liệu ghép nối mỗi tài liệu với các láng giềng
gần nhất của nó dựa trên sự tương tự ngữ nghĩa của chúng trong không gian embeddings. Chúng tôi
cũng công thức hóa việc sắp xếp tài liệu như một bài toán người bán hàng du lịch, mà chúng tôi phát
triển một thuật toán hiệu quả tối đa hóa sự tương tự của các tài liệu với ngữ cảnh của chúng đồng
thời cũng đảm bảo rằng mỗi tài liệu chỉ được bao gồm một lần.

Để đánh giá hiệu quả của IN-CONTEXT PRETRAINING, chúng tôi huấn luyện sẵn các mô hình
ngôn ngữ từ 0.3 đến 7 tỷ tham số trên 300 tỷ token từ tập dữ liệu CommonCrawl (Wenzek et al.,
2020). Qua tất cả các quy mô mô hình, các LMs IN-CONTEXT PRETRAINING (ICLM) thể hiện
hiệu suất mô hình ngôn ngữ và tác vụ downstream mạnh, vượt trội so với LMs được huấn luyện sẵn
sử dụng phương pháp tiêu chuẩn trên cùng một tập dữ liệu. Chúng tôi quan sát các cải thiện khác
nhau từ IN-CONTEXT PRETRAINING so với các LMs hiện tại: (1) học trong ngữ cảnh với mức
tăng trung bình 8% qua 8 tập dữ liệu; (2) đọc hiểu, với cải thiện trung bình 15% trên 8 tác vụ đọc
hiểu; (3) đầu ra trung thành hơn với ngữ cảnh trước đó (+16%); (4) suy luận ngữ cảnh dài, thể hiện
tăng 5%; và (5) tăng cường truy xuất, dẫn đến tăng 9% khi tăng cường với kiến thức bên ngoài như
các tài liệu được truy xuất từ Wikipedia. Kết quả của chúng tôi chứng minh rằng, bằng cách đơn giản
thay đổi thứ tự của các tài liệu huấn luyện sẵn, IN-CONTEXT PRETRAINING cung cấp một
phương pháp có thể mở rộng và đơn giản để cải thiện đáng kể việc hiểu và suy luận trên toàn bộ
ngữ cảnh của chúng. Code được công bố tại github.com/swj0419/in-context-pretraining.

2 IN-CONTEXT PRETRAINING
Thực hành tiêu chuẩn trong huấn luyện sẵn là tạo ngữ cảnh đầu vào bằng cách nối các tài liệu ngẫu
nhiên cho đến khi đạt độ dài ngữ cảnh tối đa. Sau đó nó huấn luyện LM sử dụng một mục tiêu mô
hình ngôn ngữ trên các ngữ cảnh đầu vào. Tuy nhiên, huấn luyện LMs trên các tài liệu được nối
ngẫu nhiên không cung cấp tín hiệu học tập bổ sung so với huấn luyện trên từng tài liệu riêng lẻ.
Ngược lại, IN-CONTEXT PRETRAINING tạo ra các ngữ cảnh đầu vào mạch lạc hơn bằng cách
nối các tài liệu liên quan về mặt ngữ nghĩa lại với nhau trong quá trình huấn luyện sẵn. Như được
mô tả trong Hình 2, IN-CONTEXT PRETRAINING bao gồm hai bước: đầu tiên nó tìm các tài liệu
liên quan ở quy mô (§2.1) và sau đó xây dựng ngữ cảnh đầu vào sử dụng những tài liệu liên quan
này (§2.2). Tiếp theo, chúng tôi sử dụng các ngữ cảnh được hình thành với
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
các tài liệu liên quan về mặt ngữ nghĩa để huấn luyện sẵn LMs với một mục tiêu mô hình ngôn ngữ.
Vì IN-CONTEXT PRETRAINING giống hệt với các công thức huấn luyện sẵn hiện tại cho LMs,
ngoại trừ việc thay đổi cách xây dựng ngữ cảnh đầu vào, nó có thể được tích hợp dễ dàng vào các
pipeline huấn luyện sẵn hiện tại cho các LMs quy mô lớn.

2.1 TÌM TÀI LIỆU LIÊN QUAN Ở QUY MÔ: TRUY XUẤT TÀI LIỆU LÁNG GIỀNG
Để tìm tài liệu liên quan ở quy mô, chúng tôi liên kết các tài liệu trong tập dữ liệu huấn luyện sẵn D
sử dụng một mô hình truy xuất. Cụ thể, với mỗi tài liệu di ∈ D, một mô hình truy xuất dày đặc được
sử dụng để truy xuất top-k tài liệu tương tự nhất, được biểu diễn như N(di). Mô hình truy xuất sử
dụng tìm kiếm láng giềng gần xấp xỉ để so sánh tương tự theo cặp hiệu quả giữa bất kỳ hai tài liệu
nào, làm cho nó có thể mở rộng để tìm tài liệu liên quan trong các tập dữ liệu huấn luyện sẵn quy
mô web.

Truy xuất. Quá trình truy xuất của chúng tôi sử dụng mô hình contriever (Izacard et al., 2022). Mô
hình này ánh xạ mỗi tài liệu di ∈ D thành một embedding E(di) bằng cách lấy mean pooling của
biểu diễn ẩn cuối cùng trên các token trong di. Độ tương tự cosine sau đó được sử dụng để xác định
sự tương tự giữa bất kỳ hai tài liệu nào:
s(di, dj) = cos(E(di), E(dj)) (1)

Mô hình truy xuất sử dụng tìm kiếm láng giềng gần xấp xỉ với thư viện faiss (Johnson et al., 2019;
Douze et al., 2024). Chúng tôi sử dụng product quantization (Jégou et al., 2011) để giảm dấu chân
bộ nhớ và cấu trúc chỉ mục IVF (inverted file) để tiến hành tìm kiếm tương tự theo cặp hiệu quả
cùng với tìm kiếm batch lớn faiss. Framework OIVFBBS faiss được tận dụng cho tác vụ này,
OIVFBBS đề cập đến việc tiến hành tìm kiếm offline với các truy vấn của các batch lớn với các chỉ
mục đảo ngược faiss. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.2 và trong demo OIVFBBS
tại kho lưu trữ github faiss github.com/facebookresearch/faiss/tree/main/demos/offline_ivf.

Trong quá trình truy xuất, khi tính toán tương tự theo cặp giữa mỗi tài liệu trong tập dữ liệu huấn
luyện sẵn, chúng tôi phát hiện rằng tập dữ liệu huấn luyện sẵn chứa nhiều tài liệu gần trùng lặp.
Do đó, chúng tôi tiếp tục tận dụng điểm số truy xuất để loại bỏ các tài liệu gần trùng lặp khỏi tập dữ
liệu huấn luyện sẵn. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.1. Trong §4.2, chúng tôi
cho thấy rằng bước khử trùng lặp này là quan trọng để đạt được hiệu suất tốt của các mô hình ngôn
ngữ.

2.2 TẠO NGỮ CẢNH ĐẦU VÀO: DUYỆT ĐỒ THỊ TÀI LIỆU
Cho một tập hợp tài liệu D = {di} và các láng giềng gần nhất cho mỗi tài liệu N(di), mục tiêu của
chúng tôi là sắp xếp các tài liệu để tạo ngữ cảnh đầu vào sao cho mỗi ngữ cảnh bao gồm một danh
sách các tài liệu liên quan. Chính thức, chúng tôi nhắm đến việc hình thành một tập hợp các ngữ
cảnh đầu vào C1···Cm trong đó mỗi ngữ cảnh Ci = {d1, ...dk} ⊂ D và ∪m i=1 Ci = D. Lý tưởng
nhất, các tài liệu trong Ci là láng giềng gần nhất của nhau.

Thuật toán 1 Maximum Traveling Salesman
Đầu vào: Đồ thị tài liệu G = (D, L)
N(di) trả về láng giềng gần nhất cho di
min_deg(D) trả về một tài liệu có bậc tối thiểu
Đầu ra: Một đường dẫn P
1: P ← []
2: while |D| > 0 do
3: di ← min_deg(D)
4: P.append(di)
5: D.remove(di)
6: while N(di) ∩ D ≠ ∅ do
7: dj ← arg mind∈N(di)∩D sim(di, d)
8: di ← dj
9: P.append(di)
10: D.remove(di)
11: end while
12: end while
13: return P

Một phương pháp đơn giản để hình thành C1···Cm là
trực tiếp đặt mỗi tài liệu và top-k tài liệu được
truy xuất của nó cùng nhau trong cùng một ngữ
cảnh đầu vào (được gọi là kNN), đã được sử dụng
trong một số phương pháp huấn luyện sẵn tăng
cường truy xuất (Guu et al., 2020; Levine et al.,
2022). Phương pháp kNN này duy trì sự tương tự
tài liệu trong mỗi ngữ cảnh nhưng tạo ra vấn đề
lặp lại dữ liệu: một số tài liệu thường xuất hiện
như láng giềng gần nhất của các tài liệu khác, gây
ra việc các ngữ cảnh đầu vào khác nhau chứa các
tài liệu chồng chéo, tức là ∃i ≠ j, Ci ∩ Cj ≠ ∅.
Vấn đề lặp lại dữ liệu phơi bày LMs với một tập
hợp ít đa dạng hơn của các tài liệu cho một ngân
sách tính toán cố định và có thể dẫn đến overfitting
của các tài liệu phổ biến. Thay vào đó, chúng tôi
nhắm đến việc xây dựng một tập hợp các ngữ cảnh
theo cách mà mỗi tài liệu chỉ được bao gồm một
lần, có thể được đúc như một bài toán duyệt đồ
thị.
3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 2: Minh họa của IN-CONTEXT PRETRAINING. IN-CONTEXT PRETRAINING đầu tiên
tìm các tài liệu liên quan ở quy mô để tạo một đồ thị tài liệu (§2.1) và sau đó xây dựng các ngữ
cảnh đầu vào huấn luyện sẵn bằng cách duyệt đồ thị tài liệu (§2.2). Dọc theo đường dẫn, các tài
liệu được nối thành một chuỗi và sau đó được chia để tạo thành các ngữ cảnh đầu vào có kích thước
cố định (ví dụ, độ dài 8192 token).

Duyệt đồ thị tài liệu. Để đạt được mục tiêu tối đa hóa cơ hội các tài liệu liên quan được nối lại
với nhau, một phương pháp trực quan là tìm một đường dẫn duy nhất ghé thăm mỗi tài liệu một lần
và tối đa hóa cơ hội các tài liệu liên quan được ghé thăm tuần tự. Sau đó chúng tôi chia đường dẫn
thành nhiều ngữ cảnh đầu vào. Chúng tôi công thức hóa nó như bài toán người bán hàng du lịch tối
đa (Flood, 1956) nhằm tìm đường dẫn trọng số tối đa đi qua tất cả các nút chính xác một lần. Chúng
tôi biểu diễn mỗi tài liệu như một nút trong đồ thị và sử dụng sự tương tự tài liệu như trọng số cạnh.
Chúng tôi thiết kế một đồ thị có trọng số không hướng biểu diễn các tài liệu, được ký hiệu như G =
(D, L). Ở đây, D biểu diễn tập hợp các tài liệu, trong khi (d, d∗) ∈ L là một cạnh nếu d∗ ∈ N(di)
hoặc di ∈ N(d∗). Trọng số của mỗi cạnh tương ứng với sự tương tự tài liệu (Phương trình 1).

Giải quyết các bài toán người bán hàng du lịch lớn một cách chính xác là NP hard, nhưng các thuật
toán tham lam được biết là cung cấp một giải pháp xấp xỉ hiệu quả. Chúng tôi áp dụng phương
pháp này, giới thiệu các sửa đổi để phù hợp hơn với ngữ cảnh của chúng tôi. Thuật toán 1 cho thấy
phương pháp xây dựng một đường dẫn trọng số tối đa. Chúng tôi cho thấy một đường dẫn được xác
định bởi thuật toán của chúng tôi trong Hình 2. Thuật toán của chúng tôi bắt đầu bằng cách chọn
một tài liệu chưa được ghé thăm với bậc tối thiểu làm nút khởi đầu (Doc 0). Thuật toán sau đó mở
rộng dần đường dẫn hiện tại bằng cách điều hướng đến tài liệu láng giềng chưa được ghé thăm với
trọng số cao nhất (Doc 9), thêm nút tài liệu vào đường dẫn. Quá trình này tiếp tục cho đến khi
đường dẫn đạt đến một nút mà tất cả các tài liệu láng giềng đã được ghé thăm, điều này xảy ra vì
đồ thị của chúng tôi không hoàn chỉnh, và chỉ chứa các cạnh giữa các tài liệu mà một trong số
chúng nằm trong k láng giềng gần nhất của tài liệu kia. Trong trường hợp này, chúng tôi mở rộng
đồ thị với một cạnh có trọng số 0 đến một tài liệu bậc tối thiểu chưa được ghé thăm ngẫu nhiên
(Doc 1), và tiếp tục quá trình trên. Động lực cho việc bắt đầu từ các tài liệu bậc tối thiểu là chúng
có khả năng cao nhất có tất cả láng giềng của chúng được ghé thăm trước, và do đó được kết nối
với các tài liệu không tương tự trong đường dẫn cuối cùng.

Như một bước cuối cùng, chúng tôi duyệt các tài liệu dọc theo đường dẫn và nối chúng để tạo các
ngữ cảnh đầu vào có kích thước cố định phù hợp cho huấn luyện sẵn. Điều quan trọng cần lưu ý là
khi hình thành các batch đầu vào huấn luyện, chúng tôi đảm bảo sự đa dạng giữa các ngữ cảnh đầu
vào khác nhau trong cùng một batch.

3 THÍ NGHIỆM
Trong phần này, chúng tôi mô tả chi tiết thiết lập huấn luyện sẵn của chúng tôi (§3.1), các phương
pháp baseline mà chúng tôi sử dụng để so sánh (§3.2), và kết quả thí nghiệm (§3.3).

3.1 THIẾT LẬP HUẤN LUYỆN SẴN
Vì IN-CONTEXT PRETRAINING để nguyên các chi tiết khác của huấn luyện mô hình, và chỉ thay
đổi thứ tự tài liệu sao cho mỗi ngữ cảnh chứa các tài liệu liên quan, chúng tôi có thể tích hợp trực
tiếp nó vào các pipeline huấn luyện sẵn như một bước tiền xử lý trong quá trình batching. Cho thí
nghiệm của chúng tôi, chúng tôi áp dụng kiến trúc mô hình và mục tiêu huấn luyện sẵn của LLaMA
(Touvron et al., 2023a;b) và huấn luyện sẵn LMs từ đầu.
4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Tập dữ liệu huấn luyện sẵn. Chúng tôi sử dụng tập dữ liệu Commoncrawl tiếng Anh (Wenzek et
al., 2020), nguồn dữ liệu được sử dụng rộng rãi để huấn luyện sẵn LMs. Do hạn chế tài nguyên,
chúng tôi lấy mẫu ngẫu nhiên 235 triệu tài liệu từ tập dữ liệu này, tổng cộng 306 tỷ token. Chúng
tôi sử dụng cùng dữ liệu huấn luyện sẵn cho tất cả các mô hình.

Chi tiết mô hình. Chúng tôi lấy kiến trúc mô hình từ LLaMA (Touvron et al., 2023a) và huấn
luyện các mô hình qua nhiều kích thước khác nhau: 0.3, 0.7, 1.5, và 7.0 tỷ tham số, tất cả với cửa
sổ ngữ cảnh dài 8192. Theo LLaMA, chúng tôi sử dụng bộ tối ưu AdamW (Loshchilov & Hutter,
2018) với các tham số β1 = 0.9 và β2 = 0.95, và lịch học cosine. Mô hình 7B được huấn luyện sẵn
sử dụng 128 GPUs A100 trên 16 nút với kích thước batch 4 triệu token. Phải mất 9 ngày để huấn
luyện mô hình 7B trên tập dữ liệu huấn luyện sẵn của chúng tôi. Do cửa sổ ngữ cảnh dài của các
mô hình của chúng tôi, chúng tôi sử dụng flash attention (Dao et al., 2022) để giảm tiêu thụ bộ
nhớ trong quá trình huấn luyện sẵn.

Để thực hiện truy xuất trên các tập dữ liệu huấn luyện sẵn của chúng tôi, chúng tôi sử dụng mô
hình contriever (Izacard et al., 2022) và mã hóa 512 token đầu tiên của mỗi tài liệu thành một
embedding. Sau đó chúng tôi xây dựng tìm kiếm batch lớn FAISS được thiết kế để tiến hành tìm
kiếm tương tự hiệu quả với các batch lớn của vector (thường 50M–100M vector mỗi batch). Cho
mỗi tài liệu truy vấn, chúng tôi truy xuất top 10 tài liệu (k=10). Chúng tôi chia dữ liệu thành các
batch 50M embeddings, bước tìm kiếm được tiến hành trong mỗi batch trước khi hợp nhất kết quả
sử dụng 8 GPUs mỗi batch. Tổng thời gian tìm kiếm là 6 giờ trên 32 GPUs với thời gian tìm kiếm
trung bình mỗi batch là 4,738s. Giai đoạn duyệt đồ thị tài liệu yêu cầu 12 giờ trên thiết lập 20
CPUs.

Chi tiết thêm được cung cấp trong Phụ lục A.2.

3.2 BASELINES
Chúng tôi so sánh IN-CONTEXT PRETRAINING với các baseline sau: (1) Standard là tiêu chuẩn
trước đây trong huấn luyện sẵn đặt các tài liệu được xáo trộn ngẫu nhiên trong các ngữ cảnh đầu
vào. Phương pháp này thường được áp dụng bởi các mô hình hiện tại (Zhang et al., 2022; Scao et
al., 2022; Touvron et al., 2023a). (2) kNN (cũng được gọi là huấn luyện sẵn mô hình ngôn ngữ
tăng cường truy xuất (Guu et al., 2020; Levine et al., 2022)) trực tiếp đặt mỗi tài liệu và top-k tài
liệu được truy xuất của nó cùng nhau trong cùng một ngữ cảnh đầu vào. Cho cùng số bước huấn
luyện, kNN phơi bày LMs với một tập hợp ít đa dạng hơn của các tài liệu, vì các tài liệu có thể lặp
lại. Để so sánh công bằng, cả phương pháp standard và kNN đều được huấn luyện sử dụng cùng dữ
liệu huấn luyện sẵn như IN-CONTEXT PRETRAINING và trải qua số bước huấn luyện giống hệt
nhau, đảm bảo cùng chi phí tính toán.

3.3 KẾT QUẢ
Chúng tôi thực hiện đánh giá trên các tác vụ đòi hỏi hiểu ngữ cảnh bao gồm mô hình ngôn ngữ
(§3.3.1), học trong ngữ cảnh (§3.3.2), đọc hiểu (§3.3.3) và trả lời câu hỏi sách mở (§3.3.4), tính
thực tế (§3.3.5) và suy luận ngữ cảnh dài (§3.3.6).

3.3.1 MÔ HÌNH NGÔN NGỮ
Tập dữ liệu & Metrics. Chúng tôi đánh giá perplexity mô hình ngôn ngữ của IN-CONTEXT
PRETRAINING và các baseline trên các tập dữ liệu Wikipedia, Arxiv, và Books. Chúng tôi tuân
theo đánh giá mô hình ngôn ngữ tiêu chuẩn trong việc nối các tài liệu được sắp xếp ngẫu nhiên khi
tính perplexity.

Kết quả. Hình 3 cho thấy perplexity trung bình qua các kích thước mô hình khác nhau. Đầu tiên,
kNN không cải thiện so với LM tiêu chuẩn, có thể do vấn đề overfitting như đã thảo luận trong
§2.2. ICLM, ngược lại, vượt trội hơn cả LM tiêu chuẩn và kNN trên tất cả ba tập dữ liệu, ngay cả
khi các tài liệu đánh giá không được sắp xếp. Các cải thiện nhất quán hoặc lớn hơn khi kích thước
mô hình mở rộng. Những cải thiện này gợi ý rằng IN-CONTEXT PRETRAINING cung cấp tín hiệu
huấn luyện sẵn tốt hơn, cho phép LMs hoàn thiện khả năng mô hình ngôn ngữ của chúng tốt hơn.

3.3.2 HỌC TRONG NGỮ CẢNH CHO PHÂN LOẠI VĂN BẢN
Tập dữ liệu & Metrics. Học trong ngữ cảnh yêu cầu thực hiện một tác vụ mà không fine-tuning
bằng cách điều kiện hóa trên một vài ví dụ minh họa về tác vụ. Chúng tôi đánh giá khả năng học
trong ngữ cảnh của ICLM sử dụng 32 ví dụ minh họa. Chúng tôi sử dụng bảy tập dữ liệu phân loại
văn bản, bao gồm
5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 3: Perplexity mô hình ngôn ngữ (càng thấp càng tốt) trên Wikipedia, Arxiv, và Books (§3.3.1).
ICLM vượt trội hơn các baseline một cách nhất quán qua tất cả các kích thước mô hình.

Bảng 1: Hiệu suất học trong ngữ cảnh trên bảy tập dữ liệu phân loại (§3.3.2). Chúng tôi sử dụng 32
ví dụ trong ngữ cảnh cho tất cả các tập dữ liệu. ICLM vượt trội hơn các baseline trên tất cả các tập
dữ liệu.

| Phương pháp | Sentiment        | Hate Speech   | Topic Classification | Trung bình |
|-------------|------------------|---------------|---------------------|------------|
|             | Amazon SST2 Yelp | Hate Offensive| Agnews Dbpedia      |            |
| Standard    | 94.6   83.7  74.3| 52.7   55.7  | 68.3   61.5        | 66.0       |
| kNN         | 88.0   80.2  65.1| 50.1   53.1  | 65.7   56.4        | 61.8       |
| ICLM        | 96.5   93.2  77.4| 60.6   57.3  | 76.0   63.2        | 71.3       |

phân tích tình cảm (SST-2 (Socher et al., 2013), Amazon và Yelp (Zhang et al., 2015a)), phân loại
chủ đề (AGN (Zhang et al., 2015b) và Dbepdia (Lehmann et al., 2015)) và phát hiện ngôn từ thù
địch (Barbieri et al., 2020). Chúng tôi sử dụng từ nhãn từ Min et al. (2022) và báo cáo độ chính
xác như metric.

Kết quả. Như được thể hiện trong Bảng 1, ICLM nhất quán thể hiện hiệu suất tốt hơn qua tất cả
các tập dữ liệu phân loại văn bản, dẫn đến tăng 8% trung bình. Kết quả này gợi ý rằng ICLM tốt
hơn trong việc học từ các ví dụ minh họa. Chúng tôi sau đó phân tích mối quan hệ giữa số lượng ví
dụ minh họa và hiệu suất của học trong ngữ cảnh trong §4.3.

3.3.3 ĐỌC HIỂU
Tập dữ liệu & Metrics. Đọc hiểu yêu cầu trả lời câu hỏi dựa trên đoạn văn đã cho. Chúng tôi xem
xét benchmark đọc hiểu RACE (RACE-High và RACE-Middle) (Lai et al., 2017), SQuAD
(Rajpurkar et al., 2016), BoolQ (Clark et al., 2019), DROP (Dua et al., 2019), và HotpotQA (Yang
et al., 2018). Chúng tôi sử dụng học trong ngữ cảnh 2-shot để đánh giá; chúng tôi không sử dụng
nhiều hơn vì một số tài liệu trong các tác vụ đọc hiểu rất dài. Chúng tôi báo cáo điểm khớp chính
xác cho HotpotQA và SQuAD, và độ chính xác cho các tập dữ liệu khác là tác vụ đa lựa chọn
(RACE, BoolQ, DROP), theo tiêu chuẩn trong công trình trước đây.

Kết quả. Bảng 2 nổi bật rằng ICLM nhất quán vượt trội hơn cả baseline tiêu chuẩn và kNN qua tất
cả các tập dữ liệu với cải thiện trung bình 14%. Đặc biệt, chúng tôi quan sát cải thiện đáng kể trên
HotpotQA, đòi hỏi hiểu đa bước của nhiều tài liệu liên quan. Cải thiện hiệu suất trên các tác vụ đọc
hiểu chứng minh rằng IN-CONTEXT PRETRAINING cải thiện khả năng hiểu và suy luận của LMs
trên ngữ cảnh đã cho.

3.3.4 TĂNG CƯỜNG TRUY XUẤT
Tập dữ liệu & Metrics. Tăng cường truy xuất là một phương pháp truy xuất một tập hợp các đoạn
văn từ tập dữ liệu văn bản bên ngoài (ví dụ, Wikipedia) và thêm vào đầu truy vấn đầu vào để xử lý
tốt hơn các truy vấn đầu vào yêu cầu kiến thức thực tế (Lin et al., 2023; Xu et al., 2023; Su et al.,
2023; Feng et al., 2024). Chúng tôi tiến hành đánh giá trên hai tập dữ liệu QA miền mở được
nghiên cứu kỹ: Natural Questions (NQ) (Kwiatkowski et al., 2019) và TriviaQA (Joshi et al., 2017).
Cho cả hai tập dữ liệu, chúng tôi báo cáo điểm khớp chính xác (EM) và đánh giá hiệu suất mô hình
trong cả thiết lập sách đóng và sách mở.
6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 2: Kết quả đọc hiểu, sử dụng học trong ngữ cảnh 2-shot (§3.3.3). ICLM vượt trội hơn các
baseline trên tất cả sáu tập dữ liệu.

| Phương pháp | RACE-High | RACE-Middle | BoolQ | SQuAD | HotpotQA | DROP | Trung bình |
|-------------|-----------|-------------|-------|-------|----------|------|------------|
| Standard    | 39.5      | 53.3        | 68.9  | 26.3  | 10.5     | 27.2 | 37.6       |
| kNN         | 36.2      | 51.4        | 65.3  | 23.5  | 14.4     | 25.1 | 36.0       |
| ICLM        | 41.5      | 56.9        | 73.0  | 30.3  | 21.9     | 35.7 | 43.2       |

Bảng 3: Kết quả trên NQ và TQA (§3.3.4) không
có truy xuất (đóng) và có truy xuất (mở).

| Phương pháp | NQ        | TQA       |
|-------------|-----------|-----------|
|             | Đóng | Mở | Đóng | Mở |
| Standard    | 17.0 | 28.5| 49.3 | 48.1|
| kNN         | 13.5 | 20.1| 40.2 | 43.2|
| ICLM        | 17.0 | 32.2| 48.0 | 51.6|

Bảng 4: Kết quả trên hai tập dữ liệu có xung
đột kiến thức, yêu cầu suy luận tốt hơn về
ngữ cảnh đã cho (§3.3.5).

| Phương pháp | NQ-Swap | MemoTrap |
|-------------|---------|----------|
| Standard    | 39.6    | 48.4     |
| kNN         | 42.1    | 54.3     |
| ICLM        | 45.8    | 56.2     |

Trong thiết lập sách đóng, chúng tôi chỉ cung cấp câu hỏi cho mô hình và mô hình phải trả lời câu
hỏi dựa trên kiến thức tham số của nó. Trong thiết lập sách mở, chúng tôi tuân theo Shi et al.
(2023c) trong việc cung cấp cho mô hình top-10 tài liệu được truy xuất từ Wikipedia như ngữ cảnh
bổ sung cho câu hỏi.

Kết quả. Kết quả được báo cáo trong Bảng 3. Trong thiết lập sách đóng, ICLM thực hiện tương
đương hoặc hơi kém hơn so với baseline tiêu chuẩn, có thể vì mô hình của chúng tôi ghi nhớ ít hơn.
Tuy nhiên, trong thiết lập sách mở, ICLM vượt trội đáng kể so với baseline tiêu chuẩn trong thiết
lập sách mở (+9%), đạt được hiệu suất tốt hơn nhiều so với thiết lập sách đóng. Cũng đáng chú ý
rằng mục tiêu huấn luyện của kNN chính xác giống như tăng cường truy xuất, nhưng ICLM vẫn đạt
được hiệu suất tốt hơn, có thể do vấn đề overfitting của kNN như đã thảo luận trong §2.2.

3.3.5 TÍNH THỰC TẾ
Tập dữ liệu & Metrics. Công trình trước đây đã phát hiện rằng các mô hình ngôn ngữ tạo ra văn
bản không thực tế cũng như không trung thành với ngữ cảnh đã cho, đặc biệt khi ngữ cảnh mâu
thuẫn với kiến thức mà mô hình đã thu được trong quá trình huấn luyện sẵn (thường được gọi là
kiến thức tham số (Longpre et al., 2021; Zhou et al., 2023; Shi et al., 2023b; Wang et al., 2023a)).
Chúng tôi đánh giá khả năng tuân theo hướng dẫn và ngữ cảnh của LMs trên hai tập dữ liệu xung
đột kiến thức: NQ-Swap (Longpre et al., 2021) và MemoTrap (Liu & Liu, 2023). Cả hai tập dữ
liệu đều chứa hướng dẫn và ngữ cảnh xung đột với kiến thức tham số của các mô hình. Chúng tôi
báo cáo điểm khớp chính xác như metric.

Kết quả. Bảng 4 cho thấy rằng ICLM tốt hơn các baseline tiêu chuẩn và kNN trên cả hai tập dữ
liệu, ngụ ý rằng IN-CONTEXT PRETRAINING cải thiện khả năng tạo ra đầu ra trung thành với
ngữ cảnh trước đó của LMs. Các cải thiện lớn hơn so với những tập dữ liệu khác, có thể vì NQ-
Swap và MemoTrap nổi bật thách thức trong suy luận về ngữ cảnh đã cho, mà các LMs trước đây
gặp khó khăn.

3.3.6 SUY LUẬN NGỮ CẢNH DÀI
Tập dữ liệu & Metrics. Để đánh giá khả năng suy luận ngữ cảnh dài, chúng tôi so sánh ICLM với
các baseline tiêu chuẩn và kNN trên benchmark SCROLL (Shaham et al., 2022) đánh giá khả năng
tổng hợp thông tin của LMs trên các văn bản dài. Theo thiết lập bài báo gốc, chúng tôi fine-tune
các LMs được huấn luyện sẵn (standard, kNN, IN-CONTEXT PRETRAINING) trên các tập dữ
liệu huấn luyện của scroll và đánh giá chúng trên các tập dữ liệu test. Chúng tôi báo cáo điểm F1
cho các tập dữ liệu Narrative QA, Qasper và ContractNLI và báo cáo điểm ROUGE-1 cho các tập
dữ liệu QMSum và GovReport trong benchmark SCROLL.

Kết quả. Kết quả trong Bảng 5 cho thấy rằng ICLM vượt trội hơn các baseline khoảng 5%, gợi ý
rằng ICLM tốt hơn trong suy luận ngữ cảnh dài. Chúng tôi giả thuyết rằng các cải thiện từ ICLM
có thể mờ dần
7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 5: Hiệu suất trên các benchmark suy luận ngữ cảnh dài từ SCROLL (Shaham et al., 2022)
(§3.3.6). ICLM vượt trội hơn các baseline trên tất cả năm tập dữ liệu.

| Phương pháp | NarrativeQA | Qasper | ContractNLI | QMSum | GovReport | Trung bình |
|-------------|-------------|--------|-------------|-------|-----------|------------|
|             | F1          | F1     | F1          | ROUGE-1 | ROUGE-1 |            |
| Standard    | 16.5        | 34.2   | 78.6        | 25.1  | 8.2       | 32.5       |
| kNN         | 16.8        | 34.1   | 79.5        | 24.3  | 6.6       | 32.3       |
| ICLM        | 17.1        | 36.7   | 80.7        | 26.8  | 9.1       | 34.1       |

Hình 4: Loss huấn luyện và diễn biến hiệu suất trên đọc hiểu trong quá trình huấn luyện sẵn. Sau
khi huấn luyện trên khoảng 150 tỷ token, ICLM nhất quán tốt hơn LM tiêu chuẩn trên các tác vụ
đọc hiểu và tăng cường truy xuất.

| Phương pháp  | Lựa chọn thiết kế | PPL |
|-------------|-------------------|-----|
| Liên quan   | Ngẫu nhiên        | 8.2 |
| Tài liệu    | Clustering        | 7.9 |
|             | Links (cuối cùng) | 7.3 |
| Khử trùng   | Không khử trùng   | 8.3 |
| Ngữ nghĩa   | Khử trùng (cuối cùng) | 7.3 |

Hình 5: Nghiên cứu ablation thiết kế phương pháp của chúng tôi.

Hình 6: Hiệu suất theo số lượng ví dụ trong ngữ cảnh (k).

đến một mức độ nào đó khi các LMs được fine-tune, điều này có thể giải thích các cải thiện tương
đối nhỏ trong đánh giá này so với các thí nghiệm khác của chúng tôi.

4 PHÂN TÍCH
4.1 DIỄN BIẾN HIỆU SUẤT TRONG QUÁ TRÌNH HUẤN LUYỆN SẴN
Trong suốt quá trình huấn luyện sẵn, chúng tôi theo dõi chặt chẽ cả loss huấn luyện và hiệu suất
tác vụ downstream cho ICLM cũng như LM tiêu chuẩn. Hình 4 minh họa quỹ đạo của loss huấn
luyện và hiệu suất trên các tác vụ đọc hiểu RACE cho các mô hình 7B. Loss huấn luyện cho ICLM
nhất quán thấp hơn so với LM tiêu chuẩn. Điều này gợi ý rằng, khi dự đoán token tiếp theo, ICLM
hưởng lợi từ một tập hợp phong phú hơn của các tài liệu trước đó liên quan để tham khảo, trong khi
LM tiêu chuẩn có thông tin hạn chế để dựa vào, dẫn đến loss cao hơn. Hình 4 (b, c) cho thấy rằng
sau khi huấn luyện trên khoảng 150 tỷ token, ICLM nhất quán tốt hơn LM tiêu chuẩn trên các tác
vụ đọc hiểu. Khoảng cách hiệu suất này duy trì nhất quán trong suốt phần còn lại của giai đoạn
huấn luyện sẵn. Điều này gợi ý quy mô cải thiện bởi IN-CONTEXT PRETRAINING không giảm
và duy trì nhất quán khi huấn luyện trên nhiều token hơn.
8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
4.2 NGHIÊN CỨU ABLATION VỀ THIẾT KẾ IN-CONTEXT PRETRAINING
Chúng tôi thực hiện phân tích trên hai lựa chọn thiết kế của IN-CONTEXT PRETRAINING: lựa
chọn phương pháp để tìm tài liệu được truy xuất và khử trùng lặp. Các ablation được thực hiện với
các mô hình 1.5B và được đánh giá với perplexity trên Wikipedia. Kết quả được trình bày trong
Hình 5.

Liên quan tài liệu. Một thiết kế chính của IN-CONTEXT PRETRAINING là nhóm các tài liệu
theo mức độ liên quan của chúng. Chúng tôi xem xét ba mức độ liên quan: ngẫu nhiên (baseline
tiêu chuẩn được thảo luận trong §3.2), clustering, và phương pháp liên kết tài liệu của chúng tôi
trong IN-CONTEXT PRETRAINING. Clustering tuân theo phương pháp từ Abbas et al. (2023)
trong việc phân cụm các tài liệu thành 11k cụm dựa trên embeddings của chúng và lấy mẫu các tài
liệu từ mỗi cụm để tạo thành các đầu vào huấn luyện. Các tài liệu được nhóm bởi clustering được
lấy từ cùng các cụm, chỉ ra sự tương tự về chủ đề nhưng không nhất thiết có mối quan hệ gần gũi.
Ngược lại, ICLM liên kết các tài liệu như các láng giềng gần nhất, chỉ ra mức độ tương tự cao hơn.
Mức độ liên quan giữa các tài liệu tăng từ ngẫu nhiên, clustering đến linking. Chúng tôi quan sát
rằng perplexity của mô hình ngôn ngữ giảm khi mức độ liên quan tăng.

Khử trùng lặp. Chúng tôi so sánh perplexity của các mô hình được huấn luyện có và không có
bước khử trùng lặp ngữ nghĩa. Loại bỏ bước khử trùng lặp ngữ nghĩa dẫn đến giảm đáng kể trong
perplexity. Khi các tài liệu gần trùng lặp có mặt trong cùng một ngữ cảnh, các mô hình ngôn ngữ
có thể chỉ sao chép từ tài liệu trước đó, dẫn đến bất ổn định huấn luyện.

4.3 KÍCH THƯỚC VÍ DỤ MINH HỌA CHO HỌC TRONG NGỮ CẢNH
Chúng tôi đánh giá các mô hình 7B được huấn luyện với phương pháp tiêu chuẩn và IN-CONTEXT
PRETRAINING, sử dụng số lượng ví dụ minh họa khác nhau trên các tác vụ phân loại văn bản được
mô tả trong §3.3.2. Như được mô tả trong Hình 6, ICLM duy trì cải thiện hiệu suất nhất quán so
với phương pháp tiêu chuẩn, ngay cả khi số lượng ví dụ minh họa tăng. Trong khi hiệu suất cải thiện
khi số lượng ví dụ minh họa tăng, nó đạt ngưỡng sau 32 ví dụ.

5 CÔNG TRÌNH LIÊN QUAN
Batching dữ liệu dựa trên tương tự Công trình trước đây sử dụng batching các đoạn tương tự từ
vựng trong cùng các batch huấn luyện để xây dựng các cặp tích cực chất lượng cao cho huấn luyện
các mô hình ngôn ngữ tăng cường truy xuất. Ví dụ, Zhong et al. (2022) sử dụng BM25 và cùng tài
liệu để đảm bảo các đoạn trong cùng batch tương tự với nhau, trong khi Min et al. (2023) nhóm
các đoạn từ cùng tài liệu trong cùng batch. Phương pháp của chúng tôi có cùng tinh thần với những
phương pháp này ngoại trừ chúng tôi duy trì sự liên quan của các tài liệu trong cùng cửa sổ ngữ
cảnh, tuy nhiên các cửa sổ ngữ cảnh trong các batch được xáo trộn. Ngoài ra, trọng tâm của chúng
tôi là áp dụng phương pháp batching để huấn luyện các mô hình ngôn ngữ tiêu chuẩn.

Huấn luyện sẵn với các tài liệu liên quan. Một số nghiên cứu khám phá huấn luyện sẵn các mô
hình ngôn ngữ ở quy mô nhỏ sử dụng các tài liệu liên quan. Ví dụ, Yasunaga et al. (2022) kết hợp
các tài liệu Wikipedia với hyperlinks hoặc trích dẫn vào ngữ cảnh đầu vào và huấn luyện sẵn một
masked LM. Yu et al. (2022); Wu et al. (2021) kết hợp các định nghĩa từ điển của các từ hiếm hoặc
sử dụng các vector ngữ cảnh từ các ngữ cảnh đã gặp trước đó nhắc đến những từ hiếm này trong
giai đoạn huấn luyện sẵn. Caciularu et al. (2021) thu thập các tài liệu liên quan sử dụng tập dữ
liệu tóm tắt tin tức đa tài liệu được tuyển chọn bởi con người (11 triệu token) và tiếp tục huấn
luyện sẵn một masked LM. Lewis et al. (2020) đặt các tài liệu từ cùng ngày trong ngữ cảnh đầu
vào và huấn luyện sẵn LMs để tóm tắt các bài báo. Tuy nhiên, hyperlinks không phải lúc nào cũng
có sẵn trên tất cả các miền và các tập dữ liệu tóm tắt đa tài liệu yêu cầu nỗ lực con người để tuyển
chọn. Ngoài ra, phương pháp của Lewis et al. (2020) hạn chế phạm vi của các tài liệu liên quan chỉ
từ cùng ngày. Ngược lại, chúng tôi giới thiệu một phương pháp tổng quát để thu thập các tài liệu
liên quan quy mô web không yêu cầu bất kỳ metadata nào (ví dụ, hyperlinks, tuyển chọn con người
hoặc ngày cụ thể), điều này cần thiết để mở rộng mô hình đến thiết lập huấn luyện sẵn.

Multitask finetuning cho học ngữ cảnh và hướng dẫn. Finetuning các mô hình ngôn ngữ trên một
tập hợp các tác vụ downstream để cải thiện khả năng học hướng dẫn và trong ngữ cảnh của LMs đã
được điều tra trong một số bài báo. Như được thảo luận bởi Min et al. (2022); Chen et al. (2022);
Ivison et al. (2023); Wang et al. (2022; 2023b), một kỹ thuật phổ biến nối các hướng dẫn,
9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
các mẫu huấn luyện từ các tập dữ liệu downstream được chú thích bởi con người thành các chuỗi
văn bản đơn, mà LM sau đó được finetuned trên đó. Theo hướng công trình này, Gu et al. (2023)
tạo ra các tập dữ liệu downstream nội tại bằng cách phát triển một retriever cụ thể cho từng tác vụ.
Những retriever này sau đó được sử dụng để truy xuất các ví dụ minh họa từ tập dữ liệu huấn
luyện sẵn. Phương pháp multitask finetuning bổ sung cho IN-CONTEXT PRETRAINING vì cái
trước được thiết kế riêng cho giai đoạn finetuning trong khi cái sau tập trung vào giai đoạn huấn
luyện sẵn. Ngoài việc cải thiện khả năng học trong ngữ cảnh của LMs, IN-CONTEXT PRETRAINING
cũng cải thiện khả năng mô hình ngôn ngữ tổng thể, đọc hiểu, và kiểm tra thực tế của chúng. Chúng
tôi để việc kết hợp IN-CONTEXT PRETRAINING với các phương pháp multitask finetuning như
công việc tương lai.

Huấn luyện các mô hình ngôn ngữ ngữ cảnh dài. Các nghiên cứu gần đây đã điều tra việc finetuning
LMs để mở rộng độ dài ngữ cảnh của chúng. Press et al. (2022); Chen et al. (2023); kaiokendev
(2023) thực hiện các sửa đổi đối với mã hóa vị trí và finetune LMs trên các tài liệu ngắn được nối
ngẫu nhiên và các tài liệu dài được lấy mẫu phụ từ dữ liệu huấn luyện sẵn. Tuy nhiên, như được
làm nổi bật bởi de Vries (2023), các tài liệu chuỗi dài đặc biệt hiếm trong dữ liệu huấn luyện sẵn.
Ví dụ, ít hơn 5% tài liệu trong CommonCrawl có dài hơn 2k token. Trong công trình này, chúng
tôi tập trung vào việc xây dựng dữ liệu ngữ cảnh dài có ý nghĩa, làm cho các mô hình ngôn ngữ
tận dụng tốt hơn cửa sổ ngữ cảnh của chúng. Dữ liệu được sắp xếp của chúng tôi có thể được sử
dụng cho cả giai đoạn huấn luyện sẵn và finetuning để tăng cường khả năng suy luận của LMs trên
các ngữ cảnh.

6 KẾT LUẬN
Chúng tôi giới thiệu IN-CONTEXT PRETRAINING, một phương pháp huấn luyện sẵn mới học
để tạo ra văn bản dựa trên một tập hợp các tài liệu liên quan, phơi bày LMs với các ngữ cảnh liên
quan và cung cấp tín hiệu huấn luyện vượt ra ngoài ranh giới tài liệu. Phương pháp của chúng tôi
có khả năng mở rộng cao và đơn giản, và hoạt động với bất kỳ pipeline huấn luyện sẵn nào bằng
cách đơn giản thay đổi thứ tự tài liệu trong quá trình tiền xử lý. Đánh giá toàn diện của chúng tôi
chứng minh phương pháp của chúng tôi dẫn đến cải thiện đáng kể trong nhiều thiết lập đa dạng nổi
bật khả năng hiểu và suy luận trên ngữ cảnh đã cho, bao gồm học trong ngữ cảnh, đọc hiểu, tăng
cường truy xuất, và nhiều hơn nữa. Nghiên cứu tương lai có thể đào sâu vào các kết nối vốn có giữa
các tài liệu trong các miền tập dữ liệu cụ thể hoặc sử dụng retriever đa ngôn ngữ để nhóm các tài
liệu đa ngôn ngữ liên quan trong cùng ngữ cảnh. Ví dụ, các script code trong cùng một repository
có liên quan. Insight này mở đường cho khám phá tương lai, nơi việc nối toàn bộ các repository
thành một khối thống nhất có thể dẫn đến việc tạo ra các tập dữ liệu ngữ cảnh dài có ý nghĩa.

TÀI LIỆU THAM KHẢO
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-
efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,
2023.

Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval:
Unified benchmark and comparative evaluation for tweet classification. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP 2020, pp. 1644–1650, Online, November 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL
https://aclanthology.org/2020.findings-emnlp.148.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew Peters, Arie Cattan, and Ido Dagan. CDLM:
Cross-document language modeling. In Findings of the Association for Computational Linguistics:
EMNLP 2021, pp. 2648–2662, Punta Cana, Dominican Republic, November 2021. Association
10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.225. URL https://
aclanthology.org/2021.findings-emnlp.225.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of
large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language
model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 719–730, 2022.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/
N19-1300. URL https://aclanthology.org/N19-1300.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing
Systems, 2022.

Harm de Vries. In the long (context) run, 2023. URL https://www.harmdevries.com/post/
context-length/.

Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel
Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library, 2024.

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
2368–2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.

Avia Efrat and Omer Levy. The turking test: Can language models understand instructions? ArXiv,
abs/2010.11982, 2020. URL https://api.semanticscholar.org/CorpusID:225062157.

Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.
Knowledge card: Filling LLMs' knowledge gaps with plug-in specialized language models. In The
Twelfth International Conference on Learning Representations, 2024. URL https://openreview.
net/forum?id=WbWtOYIzIK.

Merrill M Flood. The traveling-salesman problem. Operations research, 4(1):61–75, 1956.

Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 4849–4870, Toronto, Canada, July 2023. Association for Computational Linguistics.
doi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023.acl-long.267.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented
language model pre-training. In International conference on machine learning, pp. 3929–3938.
PMLR, 2020.

Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning
using cross-task nearest neighbors. In Findings of ACL, 2023. URL https://arxiv.org/abs/
2212.00196.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.
Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id=
jKN1pXi7b0.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE
Transactions on Big Data, 7(3):535–547, 2019.
11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–
1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/
v1/P17-1147. URL https://aclanthology.org/P17-1147.

Hervé Jégou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor
search. IEEE transactions on pattern analysis and machine intelligence, 33:117–28, 01 2011. doi:
10.1109/TPAMI.2010.57.

kaiokendev. Things i'm learning while training superhot, 2023. URL https://kaiokendev.github.
io/til#.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav
Petrov. Natural questions: A benchmark for question answering research. Transactions of the
Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL
https://aclanthology.org/Q19-1026.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding
comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 785–794, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https:
//aclanthology.org/D17-1082.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, et al. Dbpedia–a large-
scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167–195, 2015.

Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The
inductive bias of in-context learning: Rethinking pretraining example design. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
lnEaqbTJIRz.

Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke
Zettlemoyer. Pre-training via paraphrasing. Advances in Neural Information Processing Systems,
33:18470–18481, 2020.

Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,
Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-
augmented dual instruction tuning, 2023.

Alisa Liu and Jiacheng Liu. The memotrap dataset. https://github.com/inverse-scaling/
prize/blob/main/data-release/README.md, 2023.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and
Percy Liang. Lost in the middle: How language models use long contexts, 2023.

Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.
Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing, pp. 7052–7063, Online and Punta Cana,
Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/
v1/2021.emnlp-main.565. URL https://aclanthology.org/2021.emnlp-main.565.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.

Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,
Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik
Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating
Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim,
Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn't better, 2023.
12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in
context. In Proceedings of the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 2791–2809, Seattle, United
States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.
201. URL https://aclanthology.org/2022.naacl-main.201.

Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and
Luke Zettlemoyer. Nonparametric masked language modeling. In Findings of the Associa-
tion for Computational Linguistics: ACL 2023, pp. 2097–2118, Toronto, Canada, July 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL
https://aclanthology.org/2023.findings-acl.132.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions with human feedback, 2022.

Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=R8sQPpGCv0.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, pp. 2383–2392, 2016.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-
parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long
language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Associa-
tion for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael
Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In
International Conference on Machine Learning, pp. 31210–31227. PMLR, 2023a.

Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih.
Trusting your evidence: Hallucinate less with context-aware decoding, 2023b.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint
arXiv:2301.12652, 2023c.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.
1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/D13-1170.

Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih,
Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned
text embeddings, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.
13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023b.

Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and
Yulia Tsvetkov. Resolving knowledge conflicts in large language models. arXiv preprint
arXiv:2310.00935, 2023a.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan
Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,
Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir
Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri,
Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta
Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative
instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pp. 5085–5109, Abu Dhabi, United Arab Emirates, December
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL
https://aclanthology.org/2022.emnlp-main.340.

Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far
can camels go? exploring the state of instruction tuning on open resources, 2023b.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán,
Armand Joulin, and Édouard Grave. Ccnet: Extracting high quality monolingual datasets from
web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference,
pp. 4003–4012, 2020.

Qiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. Taking notes on the fly helps
language pre-training. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=lU5Rs_wCweN.

Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with
compression and selective augmentation, 2023.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Compu-
tational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.

Michihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with
document links. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 8003–8016, Dublin, Ireland, May 2022. Association for
Computational Linguistics. doi: 10.18653/v1/2022.acl-long.551. URL https://aclanthology.
org/2022.acl-long.551.

Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang,
Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language
modeling. 2023.
14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael
Zeng, and Meng Jiang. Dict-BERT: Enhancing language model pre-training with dictionary.
In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Associ-
ation for Computational Linguistics: ACL 2022, pp. 1907–1918, Dublin, Ireland, May 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.150. URL
https://aclanthology.org/2022.findings-acl.150.

Susan Zhang, Mona Diab, and Luke Zettlemoyer. Democratizing access to large-scale language
models with opt-175b. Meta AI, 2022.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for
text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015a. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/
250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text
classification. In NIPS, 2015b.

Zhuosheng Zhang, Hai Zhao, and Rui Wang. Machine reading comprehension: The role of contextu-
alized language models and beyond. arXiv preprint arXiv:2005.06249, 2020.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. In International Conference on Machine Learning, pp.
12697–12706. PMLR, 2021.

Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.
5657–5673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.emnlp-main.382. URL https://aclanthology.org/2022.
emnlp-main.382.

Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large
language models. ArXiv, abs/2303.11315, 2023.
15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
A THÔNG TIN BỔ SUNG
A.1 KHỬ TRÙNG LẶP
Các tập dữ liệu thường có các bản sao ngữ nghĩa: các cặp tài liệu có liên quan về mặt ngữ nghĩa,
nhưng không hoàn toàn giống hệt nhau. Các nghiên cứu trước đây (Yasunaga et al., 2023) cho thấy
rằng việc huấn luyện lại các tài liệu rất tương tự trong các ngữ cảnh đầu vào trong quá trình huấn
luyện làm tổn hại hiệu suất của các mô hình đa phương thức. Chúng tôi quan sát một hành vi tương
tự: khi các tài liệu gần trùng lặp có mặt trong cùng một ngữ cảnh, các mô hình ngôn ngữ có thể chỉ
sao chép từ tài liệu trước đó, dẫn đến bất ổn định huấn luyện. Cho rằng phương pháp truy xuất của
chúng tôi vốn đánh giá sự tương tự tài liệu theo cặp, chúng tôi có thể dễ dàng lọc ra các tài liệu gần
trùng lặp có độ tương tự cosine cao với các tài liệu khác. Chúng tôi thấy rằng bước khử trùng lặp
này quan trọng để đạt được hiệu suất tốt của các mô hình ngôn ngữ (§4.2).

A.2 CHỈ MỤC FAISS
Chúng tôi đã sử dụng chỉ mục FAISS tệp đảo ngược được lượng tử hóa sản phẩm (IVFPQ) với
kích thước code 256 và số danh sách đảo ngược tương ứng 32768, với tổng kích thước 62 gigabytes.
Chỉ mục chứa 235266464 embeddings 768 chiều ban đầu ở float 32. Chỉ mục được huấn luyện trên
một mẫu 1572864 embeddings và thời gian huấn luyện là 423s. Tiếp theo, dữ liệu được chia thành
các batch 50M embeddings và cho mỗi shard chỉ mục, batch embeddings tương ứng được thêm vào
chỉ mục đã được huấn luyện, thời gian thêm embeddings trung bình cho mỗi shard chỉ mục là 628.4s.
Cuối cùng, tìm kiếm láng giềng gần xấp xỉ được tiến hành cho mỗi shard trước khi tổng hợp tất cả
kết quả sử dụng tìm kiếm batch lớn faiss. nprobe được sử dụng để tiến hành tìm kiếm xấp xỉ là 64,
điều này có nghĩa là 0.2% của các danh sách đảo ngược được thăm dò trong quá trình tìm kiếm láng
giềng gần nhất.
16
