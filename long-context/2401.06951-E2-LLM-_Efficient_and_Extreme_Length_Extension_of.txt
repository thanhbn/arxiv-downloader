# 2401.06951.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2401.06951.pdf
# File size: 1929238 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
E2-LLM: Efficient and Extreme Length Extension of
Large Language Models
Jiaheng Liu*1, Zhiqi Bai*1, Yuanxing Zhang1, Chenchen Zhang1, Yu Zhang1,
Ge Zhang2,Jiakai Wang1,Haoran Que1,Yukang Chen3,Wenbo Su1,Tiezheng Ge1,
Jie Fu4,Wenhu Chen2,Bo Zheng1
1Alibaba Group,2University of Waterloo,3The Chinese University of Hong Kong,
4The Hong Kong University of Science and Technology
{ljh411989, baizhiqi.bzq}@taobao.com
Abstract
Typically, training LLMs with long context
sizes is computationally expensive, requiring
extensive training hours and GPU resources.
Existing long-context extension methods usu-
ally need additional training procedures to
support corresponding long-context windows,
where the long-context training data ( e.g., 32k)
is needed, and high GPU training costs are as-
sumed. To address the aforementioned issues,
we propose an Efficient and Extreme length
extension method for Large Language Mod-
els, called E2-LLM, with only one training pro-
cedure and dramatically reduced computation
cost, which also removes the need to collect
long-context data. Concretely, first, the training
data of our E2-LLM only requires a short length
(e.g., 4k), which reduces the tuning cost greatly.
Second, the training procedure on the short
training context window is performed only one
time, and we can support different evaluation
context windows at inference. Third, in E2-
LLM, based on RoPE position embeddings, we
introduce two different augmentation methods
on the scale and position index parameters for
different samples in training. It aims to make
the model more robust to the different relative
differences when directly interpolating the ar-
bitrary context length at inference. Compre-
hensive experimental results on multiple bench-
mark datasets demonstrate the effectiveness of
our E2-LLM on challenging long-context tasks.
1 Introduction
Large language models (LLMs) usually have a pre-
defined context window length. For example, in-
puts to LLaMA models (Touvron et al., 2023a,b)
must be fewer than 2,048 or 4096 tokens. This pre-
set context window limit is frequently exceeded in
applications such as long conversations, document
summarization, or long-term reasoning (Zheng
et al., 2023; Chen et al., 2023a). For these ap-
plications, LLMs with longer context windows
* First two authors contributed equally.are preferred. However, training an LLM from
scratch with long context windows requires signifi-
cant training costs. To address this problem, many
long-context extension methods (Peng et al., 2023;
Chen et al., 2023b) have been proposed to extend
the context window of an existing pre-trained LLM.
One straightforward approach is called direct
extrapolation, which fine-tunes an existing pre-
trained LLM with a longer context window,
However, the authors of Position Interpolation
(PI) (Chen et al., 2023a) observe that models
trained by direct extrapolation adapt to long context
windows very slowly and direct extrapolation is in-
efficient in extending substantially longer context
windows.
As shown in Fig. 1 (a), existing long-context
extension methods ( e.g., PI) usually need addi-
tional training procedures to support corresponding
longer-context windows, where the long-context
training data is needed to collect, and training with
high GPU memory usage is needed for each context
window.
To address the aforementioned issues, as shown
in Fig. 1 (b), we propose an Efficient and Extreme
length extension method of LLMs, called E2-LLM ,
to support the extreme length extension of LLMs
with only one training procedure on short-context
data and limited computation cost. Based on E2-
LLM, we can support different evaluation context
windows well at inference by only changing one hy-
perparameter of RoPE (Su et al., 2021) according
to the input context length. Specifically, first, in E2-
LLM, the training data only requires the commonly-
used data with short lengths ( e.g., 4k), and we only
need to train the LLM once and support interpolat-
ing the arbitrary context length at inference, which
reduces the GPU usage costs largely. Second, in
Note that we follow (Chen et al., 2023b) to report the
GPU memory by fine-tuning LLaMA2 7B on various context
lengths with FlashAttention-2 (Dao, 2023) and DeepSpeed
stage 2 (Rasley et al., 2020).arXiv:2401.06951v3  [cs.CL]  22 Feb 2024

--- PAGE 2 ---
LLM (4k) 16k LLM (16k)
LLM (4k) 32k LLM (32k)
LLM (4k) 64k LLM (64k)GPU
Memory (GB)
57.4
68.8
OOM
LLM (4k)LLM (16k)
LLM (32k)
LLM (64k)
Fine-tuning
Fine-tuning (T rain Once & Support All)(a). Existing methods (e.g., PI).
(b). E -LLM (Ours).4k/8k <=46.3
2Figure 1: Comparison of existing long-context exten-
sion methods ( e.g., PI (Chen et al., 2023a)) and our
E2-LLM. “LLM (4k)” with light color means the LLM
with default context window ( e.g., LLaMa2 with 4k).
“LLM (16k/32k/64k)” with deep color means the LLM
with extended context windows (16k/32k/64k) after fine-
tuning. (a) For existing methods, we need to collect
long-context data ( e.g., 16k/32k) and fine-tune the LLM
models for different context extension windows with
high GPU memory usage. (b). For E2-LLM, we di-
rectly use the short-context data ( e.g., 4k/8k) by training
LLMs only once time with acceptable GPU memory
usage and support different evaluation context windows
(e.g., 16k/32k/64k) at inference.
our E2-LLM, we first propose the augmentation on
the scale parameter of PI from a predefined distri-
bution ( e.g., uniform distribution), which aims to
cover different position densities in training. Be-
sides, we observe that only changing the scale pa-
rameter will make LLMs focus on a small range
of absolute position indices. Therefore, in our E2-
LLM, to improve the generalization ability of our
E2-LLM, we further propose the augmentation on
the position index parameters by introducing the
position offsets on the absolute position indices of
RoPE.
The contributions of our E2-LLM are as follows:
•In our work, we first investigate the issues
(e.g., the large fine-tuning costs with long con-
text data) of existing long-context extension
methods, and propose the Efficient and Ex-
treme length extension method (i.e., E2-LLM )to train the LLMs once on the short-context
data with limited GPU memory costs and sup-
port different evaluation context windows.
• In E2-LLM, based on RoPE, we propose two
augmentation strategies on the scale and po-
sition index parameters for different samples
in training, which aims to support different
evaluation context windows within one train-
ing procedure and improve the long-context
abilities of LLMs.
•Comprehensive experimental results on multi-
ple long-context benchmark datasets demon-
strate the effectiveness and efficiency of our
E2-LLM method.
2 Related Works
Long-context Transformers. Extensive studies
have aimed to increase transformers’ ability to pro-
cess longer text sequences. Strategies like using
retrieval-based models (Karpukhin et al., 2020;
Izacard et al., 2022) have been employed, which
integrate additional documents and search findings
into the context. Various efforts have adapted the
multi-head attention by devising estimated alter-
natives (Wang et al., 2020; Beltagy et al., 2020;
Kitaev et al., 2020; Bulatov et al., 2022; Ding et al.,
2023) to mitigate the self-attention’s inherently
high computational demands. For example, Long-
former (Beltagy et al., 2020) and BigBird (Zaheer
et al., 2020) use a form of diluted attention for more
extensive text. Meanwhile, other initiatives (Wu
et al., 2022; Bulatov et al., 2022) have introduced
memory-based systems to condense previous in-
puts and recall pertinent components. However,
these approaches tend to fall short of the effective-
ness of complete attention, thereby hindering the
refinement of large pre-trained language models
(LLMs) (Wu et al., 2024; Guo et al., 2023; Wang
et al., 2023; Bai et al., 2024; Chai et al., 2024).
Our approach differs in that it approximates the
attention mechanism in a way that remains closely
aligned with the conventional attention method,
showing only a minimal discrepancy.
Long-context LLMs. Large language models
(LLMs) such as LLaMA (Touvron et al., 2023a)
and LLaMA2 (Touvron et al., 2023b) are originally
trained with fixed context sizes, typically 2048 and
4096 tokens, respectively. Nonetheless, the cost
of training LLMs with extended contexts from the

--- PAGE 3 ---
ground up is usually beyond the reach of the av-
erage research team. Consequently, recent studies
have explored ways to expand the context length of
these models during the fine-tuning stage. For ex-
ample, Position Interpolation (Chen et al., 2023a)
adapts the rotary position encoding technique (Su
et al., 2021), which allows LLaMA to process con-
texts as long as 32768 tokens. Another method,
Landmark attention (Mohtashami and Jaggi, 2023),
achieves efficiency but at the cost of some accu-
racy, by compressing extended contexts into a set
of retrieved tokens. In contrast, our strategy mini-
mizes the expenses related to fine-tuning without
compromising the efficacy of the original atten-
tion. It ensures that the model has complete and
unchanged attention over the full input during the
inference process. Other approaches, like ALiBi
(Press et al., 2022), have been designed to train
Transformers on shorter sequences and then apply
them to longer ones at inference time, effectively
extrapolating context length. However, these tech-
niques are not as effective for pre-trained LLMs
that rely on positional encodings with poor extrap-
olation capabilities, such as RoPE (Su et al., 2021).
To overcome this, recent research has been directed
towards modifying the positional embeddings of
LLMs to handle longer texts. This includes meth-
ods like Position Interpolation (Chen et al., 2023a),
NTK-aware position embeddings (ntk, 2023), and
Yarn (Peng et al., 2023).
3 Background
3.1 Rotary Position Embedding (RoPE)
Transformer models require explicit positional in-
formation to be injected, where the positional
encodings are used to represent the order of in-
puts. In this section, we take Rotary Position Em-
bedding (RoPE) (Su et al., 2021) as an example,
which is widely-used in many LLaMA-style mod-
els (Touvron et al., 2023a). In RoPE, given a po-
sition index m∈[0, L)and an embedding vector
x:= [x0, x1, . . . , x d−1]⊤, where Lis the context
window and dis the dimension of the attention
head, a vector-valued complex function f(x, m)is
defined as follows:
f(x, m) = [( x0+ix1)eimθ0, . . . , (xd−2+ixd−1)eimθd/2−1]⊤,
(1)
where i :=√−1is the imaginary unit and θj=
10000−2j/d. Based on RoPE, the self-attentionscore ais computed as follows:
a(m, n) = Re ⟨f(q, m),f(k, n)⟩
=:a(m−n), (2)
where qandkare the query and key vectors for
a specific attention head, respectively, and the de-
tailed derivation is omitted. In Eq. 2, we observe
thata(m, n)is only dependent on relative position
m−nthrough trigonometric functions. Besides,
RoPE is performed on both query and key embed-
dings for calculating the attention scores at each
layer.
3.2 Position Interpolation
While the attention score in Eq. 2 only depends on
the relative positions, its extrapolation performance
is not great. Specifically, when direct extrapolation
to larger unseen context windows in training, the
perplexity will increase to very high numbers (i.e.,
>103).
Recently, Position Interpolation (PI) (Chen et al.,
2023a) has been proposed, where sis defined as
the positional span between a query and a key, and
Lis defined as the size of the trained context win-
dow. Instead of direct extrapolation on the attention
score to s > L , the attention score is defined as
˜a(s) =a(Ls/L′), where L′is the extended longer
context window. Formally, in PI, RoPE fis re-
placed by f′as follows:
f′(x, m) =f
x,mL
L′
, (3)
where position indices from [0, L′)to[0, L)are re-
duced to match the original range of indices before
computing RoPE. In other words, the maximum
relative distance between any two tokens has been
reduced from L′toLand PI reduces the effect on
attention score computation when extending the
context window, and makes the LLMs easier to
adapt. Furthermore, we define the scale parameter
gasL′
L. For example, gis set as 2when L′= 8192
for LLaMa2 with context window of L= 4096 .
Thus, the Eq. 3 can be reformulated as follows:
f′(x, m) =f
x,m
g
. (4)
Meanwhile, for PI, we need to collect the long-
context data with the maximum length of L′in
finetuning, and finetuning is needed for each ex-
tended window with high GPU memory usage as
shown in Fig. 1 (a).

--- PAGE 4 ---
4 Method
In this section, we introduce the details of our E2-
LLM in Fig. 1 (b) for extending different sizes of
context windows by only performing one training
procedure on short-length data, which reduces the
tuning costs greatly. First, in Sec. 4.1, we pro-
vide the necessary notations. Then, in Sec. 4.2.1,
we illustrate the details of our E2-LLM strategy
to improve the length extension performance by
introducing the augmentation on the scale, and the
position offset parameters of RoPE, where these pa-
rameters are defined in Eq. 5. Finally, in Sec. 4.2.3,
we show the training and inference processes in our
E2-LLM.
4.1 Notations
Apart from the notations defined in Sec. 3, we also
define the following notations. First, the trained
length is defined as R. It should mentioned that R
is the maximum length of the data in finetuning,
which is set as 8k in E2-LLM, by default. There-
fore, it is easy to collect the training data with a
length of Rand the used GPU memory in fine-
tuning is also acceptable. In contrast, the trained
length Ris equal to the extension length L′(e.g.,
16k/32k) in many long-context extension methods
(e.g., PI), which requires high GPU memory usage
in training. Second, we also introduce the position
offset tin RoPE, and we can reformulate Eq. 4 to
compute the RoPE embeddings as follows:
f′(x, m) =f
x,m+t
g
. (5)
In standard RoPE, by default, the tis set as 0. In our
E2-LLM, the tis selected from a range of indices
T={0, ..., t max}, where tmaxis the maximum po-
sition offset. Third, we also define a set of scale pa-
rameters used in E2-LLM as G={1,2, ..., g max},
where gmaxis the maximum scale parameter.
4.2 E2-LLM
In this section, we describe our proposed E2-LLM
strategy in detail. We take the LLM model Hwith
the default context window Las 4,096 and the
trained length Ras 4,096 for illustration. We pro-
pose two different augmentation methods on the
hyperparameters ( i.e., the scale parameter gand the
position offset t) of RoPE.
4.2.1 Augmentation on g
As shown in Fig. 2, we illustrate the augmentation
procedure on the scale parameter g.In the training process of our proposed E2-LLM,
to make the model Hcover different position densi-
ties in training, for the i-th iteration, we sample the
scale parameter gifrom Gfor different iterations
following a predefined probability distribution P
as follows:
gi=Sg(P, G), gi∈G, (6)
whereSg(P, G)denotes the sampling operation on
g, which samples gifrom set Gfollowing the distri-
bution P. Therefore, different scale parameters are
used for different iterations. Note that Pis based
on uniform distribution, by default.
In Fig. 2, we set the position offset tas 0, and
then randomly select the scale parameter gfrom
Gfor each sample based on Eq. 5, where gis set
as 2, 5, and 10, respectively. Besides, as shown
in Fig. 2, we observe that the interpolated maxi-
mum context windows are different for different
samples in training, and the densities of the trained
position indices are different. For example, the in-
terpolated interpolated context windows are 8,192
and 20,480 when gis 2 and 5, respectively. Fur-
thermore, as the training context window Ris less
than the interpolated maximum context windows,
only a certain proportion of the position indices are
trained, which are shown in blue in Fig. 2.
4.2.2 Augmentation on t
As shown in Fig. 2, we observe that we can only
focus on a small range of the position indices when
we start from zero index (i.e., t= 0). Therefore, to
improve the robustness and generalization ability
of our E2-LLM, we further introduce the augmenta-
tion procedure on the position offset tby changing
the absolute position indices of RoPE. Besides, in-
spired by several recent works (Han et al., 2023;
Xiao et al., 2023), which claim that a large amount
of attention scores are allocated to the initial tokens
(i.e., attention sinks (Xiao et al., 2023)), we also
keep several initial tokens and set the position off-
sets of these tokens as 0. For other position indices,
in the i-th training iteration, we set the position
offset tfor different position indices of the trained
window as follows:
ti=(
0, m ∈[0,3]
St(Q, T), m∈(3, R), (7)
whereSt(Q, T)denotes the sampling operation on
t, and samples tifrom set Tfollowing the prede-
fined probability distribution Q. Note that Qis

--- PAGE 5 ---
Figure 2: The trained position indices (blue points)
when using different scale parameters (i.e., g= 2,5,10).
The maximum length of the finetuning data (i.e., R) is
4096 and the position offset tis set as 0for illustration.
set as a uniform distribution and tmaxis set as the
difference between the maximum interpolated con-
text window and the trained context window in the
current iteration. Based on Eq. 7, for n∈[0,3]and
m∈(3, R), the Eq. 2 can be written as follows:
a(m, n) = Re ⟨f(q, m+ti),f(k, n+ti)⟩
=:a(m+St(Q, T)−n). (8)
Therefore, when St(Q, T)is larger, the range of
relative position differences (i.e., (m+St(Q, T)−
n)) between mandnis larger, which will make
the model generalize to different ranges of relative
position differences.
In Fig. 3, we also provide the trained position
indices (i.e., blue points) when introducing the aug-
mentation on the position offset t, and observe that
E2-LLM can easily make use of the position in-
dices with different absolute values and different
ranges of relative differences.
4.2.3 Training and Inference
As discussed in Sec. 1, the training procedure is
performed once for our E2-LLM, and we can ex-
tend to different evaluation context windows easily
at inference. The details are as follows.
Figure 3: The trained position indices (blue points)
when using different position offsets and gis set as 5
for visualization. The position indices of the first four
tokens are not moved.
Training . In training, first, for the i-th iteration,
based on giand position offset tiin training, we
replace the gandtwithgiandtifor Eg. 5, re-
spectively. Then, we fine-tune the LLM Hwith a
short context window Rusing the next token pre-
diction task with modified position encodings on
the trained context window. For better clarifica-
tion, we also provide an algorithm of our proposed
E2-LLM method in Alg. 1.
Inference . Our E2-LLM also does not introduce
extra training weights, or modify the network archi-
tecture in any way, which means that it is attractive
in practical applications as most infrastructure and
optimization for the original model can be reused
after length extension. At inference, we can extend
to different context windows by setting different
scale parameters for interpolation easily. For ex-
ample, we set g= 8 for interpolating to 32,768
andg= 16 for interpolating to 65,536, which are
called as E2-LLM-32k and E2-LLM-64k, respec-
tively. It should be mentioned that the weights of
E2-LLM-32k and E2-LLM-64k are the same at in-
ference, and the only difference is that the scale
parameters are set as 8 and 16, respectively. More-
over, in practice, we can only deploy one LLM on
devices, and automatically change the scale param-
eter of RoPE based on the length of input context
to support different context windows.

--- PAGE 6 ---
Table 1: Results (%) on single-doc QA, multi-doc QA and summarization tasks from LongBench dataset.
ModelSingle-Doc QA Multi-Doc QA Summarization
Narrative
QAQasperMultiField
QA-enMultiField
QA-zhHotpot
QA2WikiMulti
hopQAMuSi
QueDu
ReaderGov
ReportQMSum
GPT-3.5-Turbo-16k 23.6 43.3 52.3 61.2 51.6 37.7 26.9 28.7 29.5 23.4
Llama2-7B-chat-4k 18.7 19.2 36.8 11.9 25.4 32.8 9.4 5.2 27.3 20.8
LongChat-v1.5-7B-32k 16.9 27.7 41.4 29.1 31.5 20.6 9.7 19.5 30.8 22.7
Vicuna-v1.5-7B-16k 19.4 26.1 38.5 43.0 25.3 20.8 9.8 19.3 27.9 22.8
LongLora-7B-16k 19.8 29.1 37.2 8.5 37.0 30.3 17.1 15.3 31.5 24.1
E2-LLM-Llama2-7B-16k 16.4 34.7 39.1 43.6 37.1 34.4 17.9 18.6 29.4 23.0
E2-LLM-Llama2-7B-32k 12.3 35.6 40.4 46.6 43.7 34.8 22.0 22.6 29.7 23.8
Llama2-13B-chat-4k 19.2 25.8 36.9 33.3 36.1 32.4 14.5 26.8 26.6 20.2
Vicuna-v1.5-13B-16k 18.9 29.9 46.2 28.4 38.1 36.0 10.7 20.9 27.9 22.1
PI-Llama2-13B-16k 19.2 33.3 42.7 47.9 44.9 34.8 19.5 17.4 27.9 23.7
E2-LLM-Llama2-13B-16k 25.4 35.3 46.5 49.1 46.4 38.3 25.2 19.3 29.9 22.7
E2-LLM-Llama2-13B-32k 24.1 36.2 49.0 52.5 49.2 37.6 23.1 20.4 29.9 23.1
Table 2: Results (%) on summarization, few-shot learning, synthetic, and code tasks from LongBench dataset.
‘Overall’ is computed by the macro-average over major task categories. This is computed on English (EN) tasks,
Chinese (ZH) tasks, and all (All) tasks, code tasks are included in both languages.
ModelSummarization Few-shot Learning Code Overall
MultiNews VCSUM TREC TriviaQA SAMSum LSHT LCC RepoBench-P EN ZH All
GPT-3.5-Turbo-16k 26.7 16.0 68.0 91.4 41.7 29.2 54.7 53.6 44.60 33.78 42.19
Llama2-7B-chat-4k 25.8 0.2 61.5 77.8 40.7 19.8 52.4 43.8 35.17 15.45 20.79
LongChat-v1.5-7B-32k 26.4 9.9 63.5 82.3 34.2 23.2 53.0 55.3 36.86 20.43 33.21
Vicuna-v1.5-7B-16k 27.2 15.1 74.0 86.2 40.8 28.8 51.0 43.5 36.49 26.55 34.28
LongLora-7B-16k 27.7 0.5 63.5 85.7 41.9 26.0 57.6 54.5 39.79 14.55 34.18
E2-LLM-Llama2-7B-16k 25.9 9.6 68.5 89.2 38.2 35.0 65.8 58.1 41.26 26.70 38.03
E2-LLM-Llama2-7B-32k 25.4 11.7 70.5 88.4 32.5 40.0 64.5 60.9 41.74 30.23 39.18
Llama2-13B-chat-4k 26.1 17.2 66.0 85.2 36.5 20.3 51.9 52.8 37.87 24.38 34.87
Vicuna-v1.5-13B-16k 27.1 16.4 74.0 84.9 27.8 29.8 44.1 45.6 38.08 23.86 34.92
PI-Llama2-13B-16k 25.9 9.1 72.5 86.5 27.9 31.0 62.5 51.1 40.88 26.35 37.65
E2-LLM-Llama2-13B-16k 27.0 9.8 73.5 87.9 40.6 36.0 65.4 59.1 44.73 28.56 41.13
E2-LLM-Llama2-13B-32k 26.8 10.2 75.0 87.8 40.9 44.5 63.8 57.5 44.55 31.93 41.74
5 Experiments
5.1 Experimental Settings
Models. In our E2-LLM, we take the pre-trained
7B, 13B Llama2 (Touvron et al., 2023b) models to
demonstrate the effectiveness of our E2-LLM.
Training Procedure. All models are fine-tuned
via the next token prediction objective based
on two 8 ×A100 GPU machines. We use
AdamW (Loshchilov and Hutter, 2019) with β1=
0.9andβ2= 0.95. The learning rate is set to
1×10−5for 7B and 13B models, and the whole
training step is set to 30,000 with a global batch
size of 16.
Datasets. The training dataset includes the pre-
train dataset (i.e., Pile (Gao et al., 2020)), and fine-
tuning datasets (i.e., ShareGPT (Zheng et al., 2023)
and the long summarization datasets (Cohan et al.,
2018)). Note that the fine-tuning datasets are usedto improve the question-answer abilities of long-
context LLMs following Vicuna and LongChat
models (Zheng et al., 2023) and generate reason-
able results on LongBench. We evaluate the long-
sequence language modeling performance of our
fine-tuned models on the LongBench (Bai et al.,
2023) and the arxiv proof-pile dataset (Azerbayev
et al., 2022).
5.2 Results on LongBench
We evaluate several popular LLMs with long
context capability, including GPT-3.5-Turbo-
16k (OpenAI, 2022), Llama2-7B-chat-4k (Touvron
et al., 2023b), LongChat-v1.5-7B-32k (Li et al.,
2023), Vicuna-v1.5-7B-16k (Zheng et al., 2023),
Longlora-7B-16k (Chen et al., 2023b), Llama2-
13B-chat-4k (Touvron et al., 2023b), Vicuna-v1.5-
13B-16k (Zheng et al., 2023), PI-Llama2-13B-16k.
LongChat-v1.5-7B-32k, Vicuna-v1.5-7B-16k, and
LongLora-7B-16k are fine-tuned from Llama2-7B

--- PAGE 7 ---
Algorithm 1 Training of E2-LLM
Input: Pre-trained LLM model Hwith default
context window of L(e.g., 4k); The trained
context window is R(e.g., 4k/8k); The evalua-
tion context window L′(e.g., 32k/64k);
1:forthei-th iteration in training do
2: Set the scale gibased on Eq. 6;
3: Set the position offset tibased on Eq. 7;
4: Modify the RoPE position embeddings
based on Eq. 5;
5: Train model Hon training window R;
6: Compute the next token prediction loss;
7: Update parameters of model H;
8:end for
Output: The optimized long context model H′.
(Note that H′can extend to different context
windows at inference.);
based on PI. Vicuna-v1.5-13B-16k (Zheng et al.,
2023), PI-Llama2-13B-16k are fine-tuned with
Llama2-13B based on PI, where PI-Llama2-13B-
16k are fine-tuned with our constructed datasets.
Following LongBench (Bai et al., 2023), we con-
duct the assessment in a zero-shot setting, except
for the few-shot learning tasks where the few-shot
examples are provided as part of the long context.
When the input length Isurpasses the maximum
context length L′of a model (indicated by the suffix
of its name), we truncate the input sequence Sfrom
the middle since the front and end of the sequence
may contain crucial information such as the instruc-
tion or question: S1:I→[S1:⌊L′/2⌋;SI−⌊L′/2⌋−1:I].
The metric for each dataset is shown in Table 6
from the Appendix A.1.
As shown in Table 1 and Table 2, we report
the performance results (%) on the LongBench
dataset. Specifically, the key findings from the
experiment results are as follows: (1) When com-
pared with the commercial model (GPT-3.5-Turbo-
16k) with an overall accuracy of 44.60% in English,
our E2-LLM-Llama2-13B-32k achieves closing re-
sults with an overall accuracy of 44.55%. (2) In
Table 1 and Table 2, we also evaluate the results
of our E2-LLM with different evaluation context
sizes (i.e., 16k and 32k), and we observe that the
performance results are better when we extend the
evaluation context window size. Besides, as the
lengths of most documents in LongBench are less
than 16k, the improvements on these tasks are not
significant when we increase the evaluation contextwindow. (3) For a fair comparison, we also reim-
plement the positional interpolation method based
on Llama2-13B (i.e., PI-Llama2-13B-16k) with the
same training strategy and training datasets. When
compared with PI-Llama2-13B-16k, our E2-LLM-
Llama2-13B-16k still achieves significant improve-
ments on LongBench, which further demonstrates
the effectiveness of our E2-LLM.
5.3 Results on Proof-Pile
On the cleaned Arxiv Math proof-pile
dataset (Azerbayev et al., 2022), we evaluate the
long sequence language modeling performance
of our extended models and baseline methods
(i.e., Vicuna-v1.5-16k and LongChat-v1.5-32k),
where the perplexity results on reported. For
the proof-pile dataset, we randomly sample 128
documents with at least 64k tokens and evaluate the
calculated perplexity of each of these samples. All
perplexity evaluations were calculated using the
sliding window method from (Press et al., 2022)
withS= 256 . Specifically, Vicuna-v1.5-16k and
LongChat-v1.5-32k are fine-tuned on the Llama2
model and linear RoPE scaling method, which is
based on the Position Interpolation (i.e., PI) (Chen
et al., 2023a). In Table 3, we found that models
extended with our method enjoy a significantly
improved perplexity from longer context window
sizes when compared with other baseline methods.
Besides, for other methods, the training context
window is equal to the maximum evaluation
context window, thus the training costs are very
large when the window size is large, where the
training costs are shown in Fig. 1. In contrast, our
E2-LLM only needs to train Llama models once
and the training context window is short, which
reduces the training costs greatly.
5.4 Ablation Study
Effect of the augmentation strategies. In Table 4
we provide two alternative variants of our E2-LLM
(i.e., E2-LLM (w/o aug on t), E2-LLM (w/o aug on
g)) to train the LLama2-13B base model. Specif-
ically, for E2-LLM (w/o aug on t), we only use
the augmentation on the scale parameter gwithout
using the augmentation on the position offset t, and
we evaluate the performance by extending the con-
text window as 32k. For E2-LLM (w/o aug on g),
we only use the augmentation on the position offset
tand fix the scale parameter gas2with the training
context window of 8k in our E2-LLM. Note that the
evaluation context window of E2-LLM (w/o aug

--- PAGE 8 ---
Table 3: Evaluation perplexity on Arxiv Proof-pile dataset (Azerbayev et al., 2022) based on Llama2 7B and 13B
models, where lower perplexity means better performance. “PI” denotes Position Interpolation (Chen et al., 2023a).
The open-sourced Vicuna-v1.5-16k and LongChat-v.15-32k are extended based on the PI method. Note that the
weights of E2-LLM-16k, E2-LLM-32k and E2-LLM-64k are the same at inference, and the only difference is that
the scale parameters are set as 4, 8 and 16, respectively.
Model Evaluation Context Window Size
Size Training Context Window Method 4096 8192 16384 32768 65536
7B 4k None 2.92 - - - -
7B16k Vicuna-v1.5-16k (PI) 3.48 3.17 3.95 - -
32k LongChat-v1.5-32k (PI) 3.54 3.18 2.91 2.73 -
4kE2-LLM-16k 2.96 2.71 2.54 - -
7B E2-LLM-32k 2.99 2.74 2.56 2.46 -
E2-LLM-64k 3.06 2.81 2.62 2.51 2.56
13B 4k None 2.78 - - - -
13B 16k Vicuna-v1.5-16k (PI) 3.27 2.97 2.78 - -
E2-LLM-16k 2.82 2.59 2.43 - -
13B 4k E2-LLM-32k 2.85 2.61 2.44 2.34 -
E2-LLM-64k 2.91 2.67 2.49 2.39 2.44
ong) is also set as 8k. As shown in Table 4, our E2-
LLM is better than these two alternative variants
on LongBench, which shows that it is beneficial to
perform the augmentation on tandg.
Table 4: Ablation on different augmentation strategies.
Methods EN ZH All
E2-LLM 44.55 31.93 41.74
E2-LLM (w/o aug on t)42.28 29.49 39.44
E2-LLM (w/o aug on g)41.66 28.33 38.77
Effect of the number of finetuning steps. As
shown in Fig 4, we report the relationship be-
tween the results on the LongBench dataset and
the number of fine-tuning steps for the LLaMA2
13B model using E2-LLM, where the results are
reported on the evaluation window size of 32k. In
Fig. 4, at the first 5k iterations, the results improve
quickly, which indicates that the models can gain
long-context understanding abilities without a long
training process. Furthermore, when increasing the
training iterations, we observe that the performance
results of LongBench can still increase steadily af-
ter 5k iterations.
Effect of the maximum scale parameter Gmax.
In Table 5, on LongBench dataset, we also report
the results of our E2-LLM based on the Llama2-
13B model to analyze the effect of the maximum
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Iterations 1e42025303540Results on LongBench
EN
ZH
AllFigure 4: Performance results on the Longbench dataset
when increasing the training steps.
scale parameter Gmax in training, and the evalu-
ation context window is set as 32k. When Gmax
increases from 5 to 20, the performance results on
Longbench are better. which indicates that it is ef-
fective to cover different densities by using a large
Gmaxin our E2-LLM. However, when we continue
to increase the maximum scale parameter Gmax,
the performance improvement becomes relatively
stable on LongBench. Thus, we directly set Gmax
as 20 to support a maximum extension window of
80k.

--- PAGE 9 ---
Table 5: Ablation on the maximum scale parameter
Gmax.
Gmax 5 10 20 30
EN 43.20 44.25 44.55 44.01
ZH 29.33 30.28 39.93 32.76
All 40.12 41.15 41.74 41.51
0 50000 100000 150000 200000
Context Length2.22.73.23.74.24.75.25.7PPLE2-LLM-80K(g=20)
E2-LLM-120K(g=30)
E2-LLM-160K(g=40)
E2-LLM-200K(g=50)
Figure 5: Generalization abilities on the unseen scales.
5.5 Further Analysis
Extension to unseen scales. By default, we set
Gmaxas 20 to support the maximum interpolated
context window of 80K. In Fig. 5, the interpolation
scales are experimentally adjusted to 20, 30, 40,
and 50 during inference to evaluate the generaliza-
tion ability of E2-LLM. The results demonstrate
that PPL maintains a satisfactory level for contexts
comprising fewer than 120K tokens. Nonethe-
less, when we continue to increase the scale, a
discernible deterioration in performance occurs. It
suggests that E2-LLM possesses robust general-
ization ability for unseen or OOD scales within a
certain range.
Visualization on Attention Map. To further ana-
lyze the effect of E2-LLM, we visualize the atten-
tion heatmaps in the layer for our E2-LLM-8k, E2-
LLM-16k, E2-LLM-32k and E2-LLM-64k based
on Llama2-13B in Fig. 6 with the evaluation con-
text windows of 8k, 16k, 32k and 64k by setting
scale parameter as 2, 4, 8, 16, respectively. Specif-
ically, as shown in Fig. 6, the vertical coordinate
represents the indices of the generated sequence’s
token, and the horizontal coordinate represents the
indices of the input sequence’s tokens. The input
text is part of a long paper, which is truncated to
16k, 32k and 64k, respectively. Then, three ran-
dom key-value pairs are inserted into the input text,
and a question is appended at the end of the text.
Note the random key-value pairs and question are
as shown in Appendix ??, The question was an-
swered correctly for 8k, 16k, 32k and 64k. In
Attention HeatMapofE2-LLM-8KAttention HeatMapofE2-LLM-16KAttention HeatMapofE2-LLM-32KAttention HeatMapofE2-LLM-64KFigure 6: Visualization of attention heatmaps on 8k,
16k, 32k and 64k input contexts.
Fig. 6, we visualize the attention heatmaps of the
output sequence corresponding to the input. The
ground-truth indices of the values corresponding to
the keys asked in 8k, 16k, 32k and 64k are [4470,
4503], [9572,9605], [15891,15924] and [37958,
37991], respectively, and we observe that the atten-
tion values of the output sequence at these positions
are very significant, which represents that E2-LLM
can well index the correct position when generating
the responses.
6 Conclusion
In this study, we introduce an Efficient and Ex-
treme length extension method for LLMs, named
E2-LLM, which is designed to extend the context
windows with a single training phase and minimal
computational overhead. Notably, in our E2-LLM,
there is no requirement to amass extensive long-
context datasets ( e.g., samples with 32k or 64k
tokens ) for training. Specifically, our E2-LLM
harnesses RoPE-based position embeddings to im-
plement a pair of novel augmentation strategies
that adjust the scale and position index parameters
across different training samples with short lengths
(e.g., 4k/8k). Comprehensive experimental results
on multiple benchmark datasets demonstrate the
effectiveness of our E2-LLM on long-context tasks.
7 Future Works
For the future directions, we have three plans as
follows: (1) as our E2-LLM is efficient and ef-

--- PAGE 10 ---
fective and we will try to apply our E2-LLM on
larger models ( e.g., LLama2 70B) and larger con-
text windows ( e.g., 128k/192k); (2) we believe that
our E2-LLM is a general method and we will try
to apply our E2-LLM on more types of position
encodings and more types of LLMs; (3) we will
release our models and codes.
References
2019. Winogrande: An adversarial winograd schema
challenge at scale.
2023. Ntk-aware scaled rope.
Zhangir Azerbayev, Edward Ayers, and Bartosz Pi-
otrowski. 2022. Proof-pile.
Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-
heng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,
Tiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024.
Mt-bench-101: A fine-grained benchmark for evalu-
ating large language models in multi-turn dialogues.
arXiv .
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
and Juanzi Li. 2023. Longbench: A bilingual, mul-
titask benchmark for long context understanding.
arXiv preprint arXiv:2308.14508 .
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. CoRR ,
abs/2004.05150.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. 2020. Piqa: Reasoning about
physical commonsense in natural language. In Thirty-
Fourth AAAI Conference on Artificial Intelligence .
Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.
2022. Recurrent memory transformer. In NeurIPS .
Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,
Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai,
Tongliang Li, Qiyao Peng, et al. 2024. xcot: Cross-
lingual instruction tuning for cross-lingual chain-of-
thought reasoning. arXiv preprint arXiv:2401.07037 .
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context window
of large language models via positional interpolation.
CoRR , abs/2306.15595.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models. arXiv:2309.12307 .
Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, and Nazli
Goharian. 2018. A discourse-aware attention modelfor abstractive summarization of long documents. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers) , pages 615–621, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Tri Dao. 2023. Flashattention-2: Faster attention with
better parallelism and work partitioning. CoRR ,
abs/2307.08691.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, Nanning Zheng, and
Furu Wei. 2023. Longnet: Scaling transformers to 1,
000, 000, 000 tokens. CoRR , abs/2307.02486.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang,
Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu,
Chao Chen, Dongfeng Zhang, et al. 2023. Owl: A
large language model for it operations. arXiv preprint
arXiv:2309.09298 .
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng
Ji, and Sinong Wang. 2023. Lm-infinite: Simple
on-the-fly length generalization for large language
models. CoRR , abs/2308.16137.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022. Few-shot learning with
retrieval augmented language models. CoRR ,
abs/2208.03299.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In EMNLP , pages
6769–6781.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. In ICLR .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Transactions of the Association of Compu-
tational Linguistics .
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. arXiv
preprint arXiv:1704.04683 .

--- PAGE 11 ---
Hector Levesque, Ernest Davis, and Leora Morgenstern.
2012. The winograd schema challenge. Thirteenth
international conference on the principles of knowl-
edge representation and reasoning.
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
min Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe
Ma, and Hao Zhang. 2023. How long can open-
source llms truly promise on context length?
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In ICLR .
Amirkeivan Mohtashami and Martin Jaggi. 2023. Land-
mark attention: Random-access infinite context
length for transformers. CoRR , abs/2305.16300.
OpenAI. 2022. Introducing chatgpt.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context win-
dow extension of large language models. CoRR ,
abs/2309.00071.
Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In ICLR .
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020. Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In KDD , pages 3505–
3506. ACM.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le
Bras, and Yejin Choi. 2019. Socialiqa: Common-
sense reasoning about social interactions. CoRR ,
abs/1904.09728.
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng
Liu. 2021. Roformer: Enhanced transformer with
rotary position embedding. CoRR , abs/2104.09864.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. CoRR , abs/2307.09288.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. CoRR , abs/2006.04768.
Zekun Moore Wang, Zhongyuan Peng, Haoran Que,
Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,
Hongcheng Guo, Ruitong Gan, Zehao Ni, Man
Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu,
Wenhu Chen, Jie Fu, and Junran Peng. 2023.
Rolellm: Benchmarking, eliciting, and enhancing
role-playing abilities of large language models. arXiv
preprint arXiv: 2310.00746 .
Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui
Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai,
Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo
Su, and Bo Zheng. 2024. Conceptmath: A bilingual
concept-wise benchmark for measuring mathematical
reasoning of large language models. arXiv .
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
and Christian Szegedy. 2022. Memorizing transform-
ers. In ICLR .
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023. Efficient streaming
language models with attention sinks. arXiv .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tañón, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big bird: Trans-
formers for longer sequences. In NeurIPS .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really finish your sentence? In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.

--- PAGE 12 ---
Input Context
Key-value pairs: Extract the value corresponding to the specified key in the
JSON object below. {"2a8d601d-1d69-4e64-9f90-8ad825a74195": "bb3ba2a5-7de8-
434b-a86e-a88bb9fa7289", "9f4a92b9-5f69-4725-ba1e-403f08dea695": "703a7ce5-f17f-4e6d-
b895-5836ba5ec71c", "52a9c80c-da51-4fc9-bf70-4a4901bc2ac3": "b2f8ea3d-4b1b-49e0-a141-
b9823991ebeb"}
Question: What is the value of key "9f4a92b9-5f69-4725-ba1e-403f08dea695"?
A More Details
A.1 More details of the LongBench dataset
The details are shown in Table 6.
A.2 More details of the attention map visualization
The three random key-value pairs and the input question are shown as follows.
Table 6: An overview of the LongBench dataset. “Source” denotes the origin of the context. “Avg len” (average
length) represents the mean length, which is calculated by counting words in English (code) datasets and characters
in Chinese datasets. “Accuracy (CLS)” and “Accuracy (EM)” are classification accuracy and exact match accuracy,
respectively.
Dataset ID Source Avg len Metric Language #data
Single-Document QA
NarrativeQA 1-1 Literature, Film 18,409 F1 English 200
Qasper 1-2 Science 3,619 F1 English 200
MultiFieldQA-en 1-3 Multi-field 4,559 F1 English 150
MultiFieldQA-zh 1-4 Multi-field 6,701 F1 Chinese 200
Multi-Document QA
HotpotQA 2-1 Wikipedia 9,151 F1 English 200
2WikiMultihopQA 2-2 Wikipedia 4,887 F1 English 200
MuSiQue 2-3 Wikipedia 11,214 F1 English 200
DuReader 2-4 Baidu Search 15,768 Rouge-L Chinese 200
Summarization
GovReport 3-1 Government report 8,734 Rouge-L English 200
QMSum 3-2 Meeting 10,614 Rouge-L English 200
MultiNews 3-3 News 2,113 Rouge-L English 200
VCSUM 3-4 Meeting 15,380 Rouge-L Chinese 200
Few-shot Learning
TREC 4-1 Web question 5,177 Accuracy (CLS) English 200
TriviaQA 4-2 Wikipedia, Web 8,209 F1 English 200
SAMSum 4-3 Dialogue 6,258 Rouge-L English 200
LSHT 4-4 News 22,337 Accuracy (CLS) Chinese 200
Code Completion
LCC 6-1 Github 1,235 Edit Sim Python/C#/Java 500
RepoBench-P 6-2 Github repository 4,206 Edit Sim Python/Java 500

--- PAGE 13 ---
Table 7: Performance on a subset of general benchmarks.
Model PIQA WSC HellaSwag SIQA WinoGrande Race-H Race-M NaturalQuestions Avg.
Llama2-7B (4K) 78.1 67.3 73.0 48.1 69.5 40.2 37.1 16.4 53.7
E2-LLM-32K 77.6 66.4 71.9 45.9 69.5 47.6 40.9 18.8 54.8
E2-LLM-64K 77.2 67.3 70.9 46.5 69.4 44.4 38.8 17.5 54.0
Llama2-13B (4K) 78.9 64.4 75.7 51.7 73.5 63.0 58.9 20.2 60.8
E2-LLM-32K 79.3 67.3 75.6 61.3 74.4 67.3 59.8 26.8 64.0
E2-LLM-64K 78.8 67.3 75.0 61.1 74.5 61.7 55.8 25.2 62.4
0 40000 80000 120000 160000 200000
Context Length1.82.02.22.42.62.83.03.23.43.6PPLE2-LLM-7B-200k
E2-LLM-13B-200k
Figure 7: Results of E2-LLM on 200K context window.
A.3 More results on short-length datasets
We evaluate the models extended by E2-LLM on several standard short-length benchmark tasks (i.e.,
PIQA (Bisk et al., 2020), WSC (Levesque et al., 2012), HellaSwag (Zellers et al., 2019), SIQA (Sap et al.,
2019), WinoGrande (ai2, 2019), RACE (Lai et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019))
within the original context window size of 4,096, where the zero-shot performance results are reported
in Table 7. Specifically, we report the results of extended models (E2-LLM-32K and E2-LLM-64K) on
Llama2-7B and Llama2-13B models, respectively. From the results of Table 7, when compared with the
baseline Llama2 models, we observe that the extended Llama2 models ( e.g., E2-LLM-32K, E2-LLM-64K)
still preserve comparable or even better performance results on these short-length benchmark datasets,
which further demonstrates the effectiveness of E2-LLM on the short-context interpretation.
A.4 Extension on our E2-LLM
Recently, we have updated the methodology and training strategies of our E2-LLM, and successfully
extended the Llama2-7B and 13B to 200k context windows with dramatically lower training costs as
shown in Fig. 7. In Fig. 7, we report the PPL results on several documents truncated to 200k from Proof-
Pile, and we observe that the PPL results decrease smoothly, which further demonstrate the effectiveness
of our E2-LLM. We will add more implementation details on our newly updated E2-LLM method in the
future.
