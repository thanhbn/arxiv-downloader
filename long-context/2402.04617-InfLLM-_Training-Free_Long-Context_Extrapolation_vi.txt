vốn dĩ nhúng thông tin vị trí tương đối của chúng: SB có thể sử dụng thông tin từ SA trong quá trình mã hóa của nó, trong khi SA chỉ có thể truy cập thông tin từ các phần trước đó của chuỗi.

Để xác minh khả năng của mô hình nắm bắt thông tin vị trí tương đối của ngữ cảnh, chúng tôi áp dụng nhiệm vụ Retrieve.Passkey với nhiều pass key để đánh giá. Trong nhiệm vụ này, mỗi chuỗi chứa hai pass key, và mô hình được yêu cầu xuất ra hai pass key này theo thứ tự. Phương pháp xây dựng dữ liệu nhất quán với ∞-Bench (Zhang et al., 2023a), trong đó vị trí của hai pass key được chọn ngẫu nhiên. Chúng tôi tạo ra 50 chuỗi, mỗi chuỗi dài 64K. Kết quả thí nghiệm cho thấy rằng trong nhiệm vụ này, InfLLM có thể xuất ra giá trị của hai pass key theo đúng thứ tự 100% thời gian. Điều này chỉ ra rằng, mặc dù mã hóa vị trí của chúng tôi bỏ qua thông tin vị trí tương đối của ngữ cảnh, mô hình vẫn có thể hiểu hiệu quả ngữ cảnh theo chuỗi.

C Thí nghiệm Bên ngoài
C.1 Chi tiết Triển khai
Bộ nhớ ngữ cảnh được xây dựng cho tất cả các lớp trong LLM. Chúng tôi đặt kích thước cache GPU của chúng tôi là 32, gấp đôi số đơn vị được tải cho mỗi bước. Chúng tôi đặt hệ số suy giảm điểm tần suất là 0.1. Chúng tôi áp dụng độ chính xác half-float cho tất cả thí nghiệm. Chúng tôi sử dụng NVIDIA A100 hoặc A800 để tiến hành thí nghiệm của chúng tôi. Đối với thí nghiệm mở rộng lên ngữ cảnh 1.024K, chúng tôi đặt kích thước chunk mã hóa là 2048, và số token đại diện là 1 để tăng tốc thí nghiệm.

C.2 Hiệu suất trên LongBench
Chúng tôi cũng sử dụng LongBench Bai et al. (2023) làm benchmark để đánh giá hiệu quả của InfLLM và các mô hình baseline. Kết quả đánh giá được hiển thị trong Bảng 5. Kết quả chỉ ra rằng: (1) InfLLM vượt trội so với các mô hình khác có khả năng xử lý đầu vào trực tuyến qua các nhiệm vụ đa dạng khác nhau. Điều này chứng minh rằng thông tin ngữ cảnh được cung cấp bởi bộ nhớ ngữ cảnh có thể nâng cao hiệu quả hiệu suất mô hình. (2) Khi áp dụng Llama-3 làm mô hình cơ sở, cả StreamingLLM và LM-Infinite chỉ đạt được hiệu suất tương đương hoặc thậm chí tệ hơn so với Llama-3 gốc. Điều này chỉ ra rằng mặc dù attention cửa sổ trượt có thể mở rộng hiệu quả kích thước cửa sổ ngữ cảnh của LLM, những mô hình này loại bỏ thông tin ngữ cảnh khoảng cách dài, do đó thất bại trong việc đạt được hiểu chuỗi dài hiệu quả. (3) Mistral có thể xử lý độ dài văn bản lên đến 32K, bao gồm hầu hết các trường hợp trong LongBench. Ngược lại, InfLLM, với kích thước cửa sổ chỉ 12K, đạt được hiệu suất tương đương hoặc thậm chí vượt trội trung bình. Điều này chứng minh thêm khả năng của InfLLM lọc ra nhiễu trong các ngữ cảnh dài, dẫn đến hiểu chuỗi dài tốt hơn.

C.3 Thí nghiệm trên Vicuna
Bảng 6: Kết quả của các mô hình dựa trên Vicuna.
R.PK R.Num R.KV Math.F
Vicuna 5.08 4.41 1.40 11.71
InfLLM 99.15 81.69 0.60 11.14

Trong các phần trước, chúng tôi đã chứng minh rằng InfLLM có thể mở rộng cửa sổ ngữ cảnh của Llama-3 (với độ dài tối đa 8K) và Mistral (với độ dài tối đa 32K) lên vài trăm nghìn token. Để xác minh thêm hiệu quả của InfLLM, chúng tôi áp dụng nó cho Vicuna Chiang et al. (2023), có độ dài tối đa chỉ 4K. Kết quả thí nghiệm được hiển thị trong Bảng 6. Kết quả cho thấy rằng chúng tôi mở rộng hiệu quả độ dài ngữ cảnh của Vicuna lên 128K, đạt được những cải thiện hiệu suất đáng kể trên các nhiệm vụ Retrieve.Passkey và Retrieve.Number. Tuy nhiên, InfLLM không thể thể hiện sự cải thiện hiệu suất trên các nhiệm vụ Retrieve.KV và Math.Find. Điều này là do các vector ẩn chứa trong Vicuna có khả năng hạn chế để lọc ra nhiễu trong văn bản cực dài, khiến bộ nhớ ngữ cảnh khó có thể định vị hiệu quả thông tin liên quan trong các ngữ cảnh phức tạp hơn của các nhiệm vụ Retrieve.KV và Math.Find. Trong tương lai, việc thiết kế cơ chế bộ nhớ mạnh mẽ hơn xứng đáng được khám phá thêm.

17
