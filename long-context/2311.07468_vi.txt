# 2311.07468.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2311.07468.pdf
# Kích thước tệp: 2222092 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Phân Tích và Giảm Thiểu Lời Nguyền Đảo Ngược
Ang Lv1∗, Kaiyi Zhang1∗, Shufang Xie1, Quan Tu1, Yuhan Chen1,
Ji-Rong Wen1 và Rui Yan1,2†
1Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
2Trung tâm Nghiên cứu Kỹ thuật 
Tìm kiếm và Khuyến nghị Thông minh Thế hệ Tiếp theo, Bộ Giáo dục
{anglv, kyzhang, ruiyan}@ruc.edu.cn
Tóm tắt
Nghiên cứu gần đây đã quan sát thấy một hiện
tượng đáng chú ý trong các mô hình ngôn ngữ
lớn (LLM), được gọi là "lời nguyền đảo ngược."
Lời nguyền đảo ngược là khi xử lý hai thực thể,
ký hiệu là a và b, được kết nối bởi mối quan hệ
R và mối quan hệ nghịch đảo R−1, LLM xuất sắc
trong việc xử lý các chuỗi dưới dạng "aRb," nhưng
gặp khó khăn khi xử lý "bR−1a," dù trong việc
sinh text hay hiểu text. Ví dụ, GPT-4 có thể trả lời
chính xác câu hỏi "Mẹ của Tom Cruise là?" với
"Mary Lee Pfeiffer," nhưng nó gặp khó khăn để
đưa ra câu trả lời thỏa đáng khi được hỏi "Con trai
của Mary Lee Pfeiffer là?" Trong bài báo này, chúng
tôi thực hiện nghiên cứu đầu tiên về cách lời nguyền
đảo ngược xảy ra trong LLM. Các điều tra của chúng
tôi tiết lộ rằng lời nguyền đảo ngược có thể xuất phát
từ các mục tiêu huấn luyện cụ thể, điều này trở nên
đặc biệt rõ ràng trong việc sử dụng rộng rãi dự đoán
token tiếp theo trong hầu hết các mô hình ngôn ngữ
nhân quả. Chúng tôi hy vọng nghiên cứu ban đầu này
có thể thu hút nhiều sự chú ý hơn đến lời nguyền đảo
ngược, cũng như các hạn chế tiềm ẩn khác trong các
LLM hiện tại.1
1 Giới thiệu
Lời nguyền đảo ngược, được quan sát bởi Berglund et al.
(2023), đã thu hút nhiều sự chú ý. Hiện tượng này
liên quan đến các thực thể có liên quan được ký hiệu
là a và b, được liên kết bởi một mối quan hệ R và mối
quan hệ nghịch đảo tương ứng R−1. Khi một truy vấn
liên quan đến a và mối quan hệ R được đặt ra cho một
mô hình ngôn ngữ lớn (LLM), LLM chính xác trả về
b như là câu trả lời. Tuy nhiên, khi được trình bày với
b và mối quan hệ nghịch đảo R−1, LLM có xu hướng
thể hiện sự bối rối đáng kể và không thể cung cấp a
như là câu trả lời. Ví dụ, khi Berglund et al. (2023)
đặt câu hỏi cho GPT-4 (OpenAI, 2023), "Mẹ của Tom
Cruise là ai?" GPT-4 đã đưa ra phản hồi đúng, đó là
"Mary Lee Pfeiffer." Tuy nhiên, khi câu hỏi ngược lại
được hỏi, "Con trai của Mary Lee Pfeiffer là ai?" GPT-4
đã trả lời với một câu trả lời ảo giác, cho thấy thiếu
kiến thức về cá nhân này. Rõ ràng là GPT-4 đã có được
kiến thức liên quan đến cả "Tom Cruise" và "Mary Lee
Pfeiffer." Hơn nữa, không nghi ngờ gì rằng GPT-4 hiểu
mối quan hệ tương hỗ giữa "a là mẹ của b" và "b là
con cái của a." Lời nguyền đảo ngược trong các LLM
tiên tiến như vậy mâu thuẫn với các khả năng dự kiến
của những mô hình này, thêm vào sự hấp dẫn xung
quanh hiện tượng này. Nó cũng hạn chế việc ứng dụng
và phát triển LLM trong các tình huống đòi hỏi độ
chính xác thực tế cao. Do đó, một câu hỏi thiết yếu
nảy sinh: điều gì gây ra lời nguyền đảo ngược trong
các mô hình ngôn ngữ lớn?

Trong bài báo này, chúng tôi đã thực hiện nỗ lực
đầu tiên để trả lời câu hỏi này, và tiết lộ rằng các mục
tiêu huấn luyện ảnh hưởng đáng kể đến mức độ của
lời nguyền đảo ngược. Đáng chú ý là Berglund et al.
(2023) tập trung đánh giá của họ chỉ trên các mô hình
Llama (Touvron et al., 2023a) và GPT (Brown
et al., 2020). Đối với những mô hình này, mặt nạ
chú ý nhân quả của họ hạn chế mỗi token chỉ phụ
thuộc vào những token trước đó, và khi được tiền
huấn luyện cho dự đoán token tiếp theo (NTP) trên
dữ liệu trong đó thực thể a thường đứng trước thực
thể b, mô hình chỉ có thể tối đa hóa khả năng xảy ra
của b khi có a (tức là, p(b|a)), không có đảm bảo
nào cho việc ước tính chính xác p(a|b). Ngược lại,
trong một số mô hình ngôn ngữ như GLM (Du et al.,
2022; Zeng et al., 2022) được tiền huấn luyện với
mục tiêu điền khoảng trống tự hồi quy (ABI), một
token bị che có thể chú ý đến cả các token đứng
trước và đứng sau nó. Kết quả là, mục tiêu ABI
ngầm xem xét khả năng có điều kiện đảo ngược
p(a|b), có thể làm cho GLM mạnh mẽ hơn chống
lại lời nguyền đảo ngược.arXiv:2311.07468v3  [cs.CL]  10 Nov 2024

--- TRANG 2 ---
Để xác minh giả thuyết này, chúng tôi tinh chỉnh GLM
trên cùng dữ liệu tên-đến-mô tả như (Berglund
et al., 2023). Cụ thể, trong quá trình tinh chỉnh, chúng
tôi cung cấp cho mô hình các đầu vào như "Joe Biden
(tên) là tổng thống Mỹ (mô tả)" và đánh giá khả năng
của nó trong việc hoàn thành câu với "Tổng thống Mỹ
(mô tả) là." Phản hồi mong đợi là "Joe Biden." Chúng
tôi viết tắt nhiệm vụ này là huấn luyện mô hình với dữ
liệu tên-đến-mô tả và kiểm tra theo thứ tự ngược lại
như nhiệm vụ ←−N2D, trong khi kiểm tra theo cùng
thứ tự như nhiệm vụ N2D. Đáng chú ý là tất cả các
tên và mô tả được sử dụng đều hoàn toàn hư cấu, đảm
bảo rằng không có sự thiên vị nào được đưa vào từ dữ
liệu tiền huấn luyện. Các phát hiện của chúng tôi tiết
lộ rằng GLM đạt được khoảng 80% độ chính xác trong
nhiệm vụ ←−N2D, thể hiện khả năng phục hồi đối với
lời nguyền đảo ngược so với Llama, mà đạt được 0%
độ chính xác. Ngược lại, (1) khi tinh chỉnh GLM cho
dự đoán token tiếp theo, chúng đạt được độ chính xác
0%; (2) Chúng tôi giới thiệu một phương pháp tinh
chỉnh mới gọi là BICO, điều chỉnh các mô hình Llama
để hỗ trợ các mục tiêu giống ABI. BICO hiệu quả giảm
thiểu lời nguyền đảo ngược trong Llama và mang lại
những cải thiện độ chính xác đáng kể (khoảng 70 điểm
độ chính xác) trong nhiệm vụ ←−N2D. Những kết quả
này rõ ràng chứng minh rằng các mục tiêu huấn luyện
là một trong những yếu tố góp phần vào lời nguyền
đảo ngược.

Ngoài ra, chúng tôi sử dụng BICO trong một tình
huống giải quyết vấn đề toán học thế giới thực và một
nhiệm vụ dịch thuật đơn giản. Đối với nhiệm vụ toán
học, chúng tôi huấn luyện LLM trên các giải pháp
cho các bài toán toán học, sử dụng bộ dữ liệu GSM8k
(Cobbe et al., 2021). Sau đó, chúng tôi đánh giá LLM
trên các bài toán toán học được dẫn xuất từ các câu
hỏi theo mẫu GSM gốc, đòi hỏi khả năng suy luận
"ngược." Về mặt dịch thuật, bản chất kép của dữ liệu
của nó là lý tưởng để đánh giá lời nguyền đảo ngược,
vì một mẫu dữ liệu dịch từ ngôn ngữ X sang ngôn ngữ
Y không được huấn luyện ngược trong quá trình tiền
huấn luyện, điều này hạn chế tính hữu ích của dữ liệu.
Chúng tôi tinh chỉnh LLM trên dữ liệu Trung-Anh và
kiểm tra nó với một nhiệm vụ Anh-Trung. Chúng tôi
phát hiện rằng thông qua BICO, mô hình thể hiện khả
năng cải thiện để giải quyết các bài toán đảo ngược
chưa thấy, và độ chính xác dịch thuật cải thiện khi
đối mặt với thứ tự cặp ngôn ngữ chưa thấy. Điều này
gợi ý việc thu được các khả năng suy luận tổng quát
và mạnh mẽ hơn từ cùng dữ liệu huấn luyện.

Tóm lại, chúng tôi thực hiện nghiên cứu đầu tiên về
nguyên nhân của lời nguyền đảo ngược, và chúng tôi
quy vấn đề này cho một trong nhiều yếu tố tiềm năng,
đó là các mục tiêu huấn luyện, đặc biệt là mục tiêu
dự đoán token tiếp theo. Chúng tôi giới thiệu một
phương pháp tinh chỉnh mới, được gọi là BICO, được
thiết kế để tránh việc đưa vào lời nguyền đảo ngược
bổ sung trong các mô hình đã được tiền huấn luyện
trong khi tận dụng dữ liệu huấn luyện tốt hơn. Chúng
tôi hy vọng nhiều nghiên cứu tập trung hơn vào những
vấn đề cơ bản này trong LLM bởi vì, mặc dù việc
áp dụng rộng rãi huấn luyện các mô hình ngôn ngữ
nhân quả sử dụng mục tiêu dự đoán token tiếp theo,
phương pháp này có thể không "hoàn hảo" như trước
đây người ta tin, gợi ý rằng các khả năng của các mô
hình ngôn ngữ lớn hiện tại (LLM) có thể được cải
thiện thêm.

2 Nền tảng
2.1 Mô hình Ngôn ngữ Nơ-ron
Có hai loại chính của các mô hình ngôn ngữ nơ-ron:
các mô hình tự mã hóa (AE) được minh họa bởi họ
BERT (Devlin et al., 2019; Zhuang et al., 2021), và
các mô hình tự hồi quy (AR) (Bengio et al., 2003;
Radford và Narasimhan, 2018; Touvron et al., 2023a).
Cho một chuỗi đầu vào X = [x1, x2, x3, . . . , xT], một
mô hình AE hoạt động bằng cách đầu tiên làm hỏng
X thành X̂ bằng cách che một số token đầu vào với
một token đặc biệt [MASK]. Các token bị che có thể
truy cập tất cả các token trong ngữ cảnh thông qua
chú ý hai chiều, như được minh họa trong Hình 1(a).
Mô hình với các tham số Θ sau đó được huấn luyện
để tái tạo những token bị che này, với mục tiêu huấn
luyện như sau:

∑(T,t=1) 1(xt là [MASK]) · log p(xt|X̂; Θ).    (1)

Mặt khác, một mô hình AR có thể được phân loại
thêm thành mô hình ngôn ngữ nhân quả và mô hình
ngôn ngữ tiền tố, tùy thuộc vào cơ chế chú ý của
chúng. Một mô hình ngôn ngữ nhân quả, như GPT
(Radford và Narasimhan, 2018; Radford et al., 2019;
Brown et al., 2020) và Llama (Touvron et al., 2023a,b)
thường ước tính xác suất của token tiếp theo dựa trên
ngữ cảnh và mục tiêu dự đoán token tiếp theo (NTP,
Hình 1(b)) có thể được công thức hóa như:

∑(T,t=1) log p(xt|X<t; Θ).    (2)

Một mô hình ngôn ngữ tiền tố, như GLM (Du et al.,
2022; Zeng et al., 2022) và UniLM (Dong et al.,

--- TRANG 3 ---
x₁[MASK] ···  xₜx₂·········
(a) Tự Mã hóa
x₁x₂··· xₜ₋₁·········x₂x₃···  xₜ
(b) NTP
x₁[MASK]    x₃[S]x₂(c) ABI
x₂[E]Tiền tố

Hình 1: Các mục tiêu huấn luyện khác nhau của các mô hình
ngôn ngữ. Chỉ các đầu ra được minh họa góp phần vào tính
toán tổn thất trong khi những đầu ra khác được bỏ qua để
rõ ràng.

2019; Bao et al., 2020), xử lý một tiền tố đầu vào
sử dụng chú ý hai chiều. Các token được dự đoán
sau đó chú ý đến tiền tố sử dụng chú ý nhân quả.
Ví dụ, GLM sử dụng một mục tiêu điền khoảng trống
tự hồi quy (ABI), bao gồm việc che một khoảng token
và sau đó tự hồi quy khử nhiễu chúng, như được minh
họa trong Hình 1(c).

Các mô hình AE và AR có những ưu và nhược điểm
riêng. Các mô hình AE đặc biệt thành thạo trong các
nhiệm vụ hiểu ngôn ngữ do mô hình hóa ngữ cảnh hai
chiều của chúng. Tuy nhiên, chúng hiếm khi được sử
dụng trực tiếp cho việc sinh ngôn ngữ, do khả năng
hạn chế của chúng trong việc dự đoán token tiếp theo.
Ngược lại, các mô hình AR xuất sắc trong việc sinh
ngôn ngữ. Các mô hình AR dựa trên Transformer, đặc
biệt, đã trở thành nền tảng của hầu hết các mô hình
ngôn ngữ lớn.

2.2 Lời Nguyền Đảo Ngược
Kể từ khi lần đầu được quan sát bởi Berglund et al.
(2023), khái niệm về lời nguyền đảo ngược vẫn còn
được định nghĩa một cách mơ hồ. Ở đây, chúng tôi
cung cấp một định nghĩa tổng quát được tóm tắt từ
các mô tả trong (Berglund et al., 2023):

Xem xét hai tập hợp thực thể, được ký hiệu là A và
B, và một mối quan hệ R biểu diễn một tập con của
tích Cartesian A × B. Một mô hình ngôn ngữ xử lý
khéo léo các chuỗi dưới dạng aRb, về mặt sinh text
và hiểu text, trong đó < a, b > thuộc về mối quan hệ
R. Tuy nhiên, mô hình gặp khó khăn hoặc không
chính xác khi xử lý bR⁻¹a, trong đó R⁻¹ biểu thị
mối quan hệ nghịch đảo của R.

Trong khi một số chiến lược như tăng cường thêm
dữ liệu đảo ngược (Yu et al., 2023) hoặc chỉnh sửa
kiến thức (Meng et al., 2022; Ma et al., 2023a) có thể
giúp giảm thiểu lời nguyền đảo ngược, lý do cơ bản
cho lời nguyền đảo ngược vẫn chưa được khám phá.
Trong bài báo này, chúng tôi trình bày nỗ lực đầu
tiên để một phần quy nguyên nhân của lời nguyền
đảo ngược cho các mục tiêu huấn luyện. Điều này
làm nổi bật nhu cầu nghiên cứu thêm về các mô hình
huấn luyện của các mô hình ngôn ngữ lớn để đạt được
khả năng tiên tiến hơn. Chúng tôi cũng minh họa rằng
có một số yếu tố ảnh hưởng đến lời nguyền đảo ngược
trong quá trình suy luận. Điều này gợi ý một yêu cầu
tiềm năng cho nghiên cứu thêm về tính diễn giải cơ
học (Wang et al., 2023; Elhage et al., 2021; Merullo
et al., 2024) cho vấn đề này.

3 Mục Tiêu Huấn Luyện Ảnh Hưởng đến Lời Nguyền Đảo Ngược

Chúng tôi cho rằng việc lựa chọn mục tiêu huấn luyện
đóng một vai trò then chốt trong việc góp phần vào
lời nguyền đảo ngược.

Dự đoán token tiếp theo (NTP) đứng như mục tiêu
tiền huấn luyện chủ đạo cho các mô hình ngôn ngữ
lớn hiện tại, thường được sử dụng trong các mô hình
ngôn ngữ nhân quả như GPT và Llama. Đối với mục
tiêu NTP, mỗi token chỉ tập trung vào ngữ cảnh đứng
trước của nó, khiến việc trực tiếp tính đến các token
tiếp theo trở nên không thể. Do đó, chúng tôi đề xuất
giả thuyết rằng mục tiêu huấn luyện này có thể góp
phần vào lời nguyền đảo ngược: Khi một mô hình
ngôn ngữ được huấn luyện trên dữ liệu mà thực thể
a luôn luôn đứng trước thực thể b, mô hình được tối
ưu hóa để tăng xác suất của b khi có a (tức là, p(b|a)),
không có đảm bảo về xác suất có điều kiện ngược lại,
p(a|b), và điều này dẫn đến sự xuất hiện của lời
nguyền đảo ngược.

Ngược lại, mục tiêu điền khoảng trống tự hồi quy
(ABI), được triển khai trong GLM, cho phép mô hình
xem xét cả ngữ cảnh đứng trước và đứng sau của các
token được dự đoán, do đó có thể tránh được lời
nguyền đảo ngược. Để xác nhận giả thuyết của chúng
tôi, chúng tôi thiết kế một thí nghiệm để xác định
liệu lời nguyền đảo ngược có thực sự rõ rệt hơn trong
các mô hình được huấn luyện với NTP hay không, và
để xem liệu nó có ít rõ ràng hơn trong các mô hình
được huấn luyện với ABI không.

3.1 Thiết Kế Thí Nghiệm
Chúng tôi nghiên cứu một mối quan hệ giữa tên của
cá nhân và mô tả của họ, mà chúng tôi ký hiệu là
RN2D. Hãy xem xét N, đại diện cho một tập hợp các
tên, và D, đại diện cho một tập hợp các mô tả. Chúng
tôi giới thiệu một mối quan hệ nhị phân, RN2D, mà
chúng tôi gọi là mối quan hệ tên-đến-mô tả. Mối quan

--- TRANG 4 ---
Dữ liệu huấn luyện
{"prompt": "Bạn đã bao giờ nghe về Daphne Barrington chưa? Họ là người", "completion": " đã đạo diễn kiệt tác thực tế ảo, 'Hành Trình Xuyên Thời Gian.'"}

N2D Test
{"prompt": "Người tiên phong được biết đến với tên Daphne Barrington từng là", "completion": " đạo diễn được ca ngợi của kiệt tác thực tế ảo, 'Hành Trình Xuyên Thời Gian.'."}

←−N2D Test
{"prompt": "Đắm chìm trong thế giới đạo diễn kiệt tác thực tế ảo, 'Hành Trình Xuyên Thời Gian.',", "completion": " Daphne Barrington"}

Hình 2: Dữ liệu được sử dụng để nghiên cứu lời nguyền đảo ngược trên mối quan hệ RN2D. Tất cả tên và mô tả đều hư cấu.
Trong giai đoạn kiểm tra, mô hình được cung cấp "prompt" và sự thật cơ bản là nội dung của "completion." Ví dụ,
trong nhiệm vụ N2D, mô hình được cung cấp cùng tên như những tên gặp phải trong quá trình tinh chỉnh nhưng được
trình bày với các prompt được diễn đạt lại. Trong nhiệm vụ ←−N2D, mô hình được giao nhiệm vụ tạo ra các tên
tương ứng dựa trên các mô tả được thấy trong quá trình tinh chỉnh.

hệ này được công thức hóa như sau: RN2D = {< n, d >
| n được mô tả bởi d, (n ∈ N) ∧ (d ∈ D)}. Đáng
chú ý, RN2D bị hạn chế như một song ánh, đảm bảo
một sự tương ứng duy nhất giữa mỗi tên trong tập
N và một mô tả trong tập D.

Các tập N và D được tạo thành sử dụng dữ liệu
được giới thiệu bởi (Berglund et al., 2023). Cả tên
và mô tả trong những tập này đều được tạo ra bởi
GPT-4 (OpenAI, 2023). Dữ liệu hư cấu chưa được
gặp phải trong bộ dữ liệu tiền huấn luyện của các
mô hình ngôn ngữ lớn. Kết quả là, chúng tôi có thể
mô phỏng cách những mô hình này thu được kiến
thức trong quá trình tiền huấn luyện và điều tra các
nguyên nhân cơ bản của lời nguyền đảo ngược. Bộ
dữ liệu huấn luyện bao gồm tổng cộng 3.600 mẫu
huấn luyện. Bộ kiểm tra bao gồm hai nhiệm vụ: một
nhiệm vụ liên quan đến huấn luyện với dữ liệu tên-
đến-mô tả và kiểm tra mô hình sử dụng một prompt
được diễn đạt lại, mà chúng tôi ký hiệu là nhiệm vụ
"N2D". Nhiệm vụ kia đòi hỏi kiểm tra theo thứ tự
ngược lại, trong đó mô tả được cung cấp, và mô hình
phải tạo ra tên cá nhân tương ứng. Chúng tôi gọi đây
là nhiệm vụ "←−N2D". Mỗi nhiệm vụ kiểm tra bao
gồm 300 mẫu kiểm tra. Hình 2 minh họa các mẫu
huấn luyện và kiểm tra.

Chúng tôi chọn Llama-7B và 13B (Touvron et al.,
2023a), các mô hình ngôn ngữ nhân quả đại diện
được tiền huấn luyện với mục tiêu NTP, cùng với
GLM-2B và GLM-10B (Du et al., 2022), hỗ trợ cả
mục tiêu ABI và NTP, để điều tra tác động của các
mục tiêu huấn luyện. Trên bộ dữ liệu hư cấu nói
trên, chúng tôi tinh chỉnh các mô hình Llama với
mục tiêu NTP, và các GLM

Mô hình    Mục tiêu    N2D    ←−N2D
GLM-2B     NTP        69.33   0.00
           ABI        72.00   88.00
GLM-10B    NTP        72.00   0.00
           ABI        63.33   74.00
Llama-7B   NTP        67.33   0.00
Llama-13B  NTP        58.67   0.00

Bảng 1: Các mô hình được huấn luyện với NTP thể hiện
lời nguyền đảo ngược rõ rệt hơn khi so sánh với mô hình
được huấn luyện cho ABI (Llama không hỗ trợ huấn luyện
với ABI).

với cả mục tiêu NTP và ABI, sử dụng cùng cài đặt
như (Berglund et al., 2023): kích thước batch là 4,
tốc độ học là 2e-5, và tinh chỉnh kéo dài trong 10
epoch. Do hạn chế tài nguyên, các mô hình được
tinh chỉnh sử dụng LoRA (Hu et al., 2022) với r = 32.
Tất cả các thí nghiệm được thực hiện trên Nvidia
A100 80G, và mỗi lần chạy mất khoảng 1 giờ. Chiến
lược giải mã mặc định của chúng tôi là giải mã tham
lam.

Chúng tôi đánh giá hiệu suất của các mô hình trên
hai nhiệm vụ sử dụng điểm Khớp Chính xác (Berglund
et al., 2023), và sự khác biệt về độ chính xác giữa hai
nhiệm vụ cho thấy mức độ của lời nguyền đảo ngược.

3.2 NTP Làm Trầm Trọng Thêm Lời Nguyền Đảo Ngược
Kết quả thí nghiệm, như được thể hiện trong Bảng 1,
tiết lộ rằng GLM được tinh chỉnh với mục tiêu ABI
thể hiện khả năng kháng cự với lời nguyền đảo ngược.
Nó duy trì

--- TRANG 5 ---
hiệu suất mạnh mẽ trên hai nhiệm vụ. Điểm số của
chúng trên nhiệm vụ N2D, bao gồm việc tạo ra các
mô tả dài, thấp hơn so với những điểm trên nhiệm vụ
←−N2D. Điều này là do nhiệm vụ sau đòi hỏi tạo ra
các tên ngắn và tương đối dễ hơn. Ngược lại, cả mô
hình GLM và Llama, được huấn luyện với mục tiêu
NTP, thể hiện độ chính xác cao trên nhiệm vụ N2D
nhưng trải qua một sự sụt giảm đáng kể xuống không
khi giải quyết nhiệm vụ ←−N2D, tiết lộ một lời
nguyền đảo ngược nghiêm trọng.

Trong khi những phát hiện này một phần khẳng định
giả thuyết của chúng tôi, một bước quan trọng vẫn
còn để thiết lập bằng chứng đáng tin cậy: khả năng
sửa đổi tiềm năng của các mô hình Llama để phù hợp
với một mục tiêu giống ABI, cho phép các token chú
ý đến cả các token đứng trước và đứng sau trong quá
trình huấn luyện. Nếu, sau khi tinh chỉnh, các mô
hình Llama thể hiện sự giảm bớt khỏi lời nguyền đảo
ngược, chúng tôi có thể tự tin khẳng định rằng các
mục tiêu huấn luyện thực sự đóng một vai trò đáng
kể trong sự xuất hiện của lời nguyền đảo ngược. Hơn
nữa, khi chúng tôi xác nhận giả thuyết của mình, việc
điều chỉnh thành công các mô hình Llama cho mục
tiêu ABI cũng góp phần giảm thiểu lời nguyền đảo
ngược. Điều này đặc biệt quan trọng trong các tình
huống mà các mô hình được tinh chỉnh với dữ liệu
mới hạn chế.

Trong phần tiếp theo, chúng tôi sẽ trình bày phương
pháp của chúng tôi để điều chỉnh các mô hình Llama
cho các mục tiêu giống ABI.

4 Điều Chỉnh Các Mô hình Llama cho Các Mục Tiêu Giống ABI

Chúng tôi trình bày một khung tinh chỉnh mới điều
chỉnh các mô hình ngôn ngữ nhân quả như Llama
cho một mục tiêu giống ABI. Chúng tôi đặt tên khung
này là Tối ưu hóa Mô hình Ngôn ngữ Nhân quả Hai
chiều (BICO). BICO sửa đổi các cơ chế chú ý nhân
quả trong quá trình huấn luyện (§4.2) đảm bảo một
sự chuyển tiếp mượt mà từ chú ý một chiều sang
chú ý hai chiều hoàn toàn, do đó nắm bắt thông tin
ngữ cảnh toàn diện từ dữ liệu đầu vào. BICO áp dụng
một mục tiêu điền khoảng trống tự hồi quy tương tự
như GLM, với các sửa đổi được thiết kế riêng đặc biệt
cho các mô hình ngôn ngữ nhân quả (§4.3). Hình 3
minh họa tổng quan về phương pháp của chúng tôi
và chúng tôi đi sâu vào các chi tiết dưới đây.

4.1 Sơ bộ: Nhúng Vị trí Quay
Khi chuyển từ một cơ chế chú ý nhân quả sang một
cơ chế hai chiều, việc giải quyết thông tin vị trí ngoài
phân phối trở nên quan trọng, và sẽ được thảo luận
trong §4.2. Chúng tôi bắt đầu bằng cách giới thiệu
việc nhúng vị trí quay được triển khai bởi Llama,
như một điều kiện tiên quyết cần thiết.

Nhúng vị trí quay (RoPE, Su et al., 2022) là một
việc nhúng vị trí tương đối được triển khai trong
quá trình tính toán chú ý. Khi nhân một vector truy
vấn hoặc khóa với một ma trận quay Rθ, thông tin
vị trí được tích hợp. Rθ,m được thiết kế như một ma
trận đường chéo khối bao gồm các khối có kích thước
2×2, tổng cộng d/2 khối như vậy. Cụ thể, khối thứ i
được định nghĩa như sau:

Rθi,m = [cos mθi  -sin mθi]
         [sin mθi   cos mθi]     (3)

trong đó θi := B^(-2i/d), i ∈ [0,1,2, . . . , d/2−1] và
B thường được chọn là 10000.

Với thiết kế ma trận như vậy, tích trong của vector
truy vấn ở vị trí m với vector khóa ở vị trí n đo
khoảng cách tương đối của chúng:

qm = Rθ,mWqxm, kn = Rθ,nWkxn,
q^T_m kn = (Rθ,mWqxm)^T(Rθ,nWkxn)
         = (Wqxm)^T R^T_θ,m Rθ,n(Wkxn)
         = (Wqxm)^T Rθ,n−m(Wkxn),      (4)

trong đó xm và xn là đầu vào thứ m và thứ n của
lớp transformer hiện tại; Wq và Wk chiếu các trạng
thái ẩn đầu vào thành các vector truy vấn và khóa.

4.2 Mở Rộng Chú Ý Nhân Quả sang Hai Chiều

Chuyển đổi một cơ chế chú ý nhân quả một chiều
trong một mô hình ngôn ngữ nhân quả thành một
cơ chế hai chiều không đơn giản. Chúng ta không
thể đơn giản loại bỏ mặt nạ chú ý một chiều, vì
làm như vậy sẽ đưa vào thông tin vị trí mà mô hình
chưa bao giờ gặp phải trong quá trình huấn luyện,
trong giai đoạn đó một vector truy vấn chỉ được
phép tính toán tích trong với các vector khóa đứng
trước của nó. Điều này rõ ràng trong Eq.4: vị trí
tương đối n−m luôn luôn không dương trong quá
trình huấn luyện nhưng là dương khi qm cần chú
ý đến k>m. Để giải quyết vấn đề này, chúng tôi đề
xuất một sửa đổi cho tích trong giữa qm và kn cho
các giá trị tùy ý của m và n trong một mô hình ngôn
ngữ nhân quả, như sau:

q^T_m kn = {
  (Wqxm)^T Rθ,n−m(Wkxn), n ≤ m,
  (Wqxm)^T Rθ,m−n(Wkxn), n > m.     (5)

Điều chỉnh này đảm bảo rằng khi một vector truy
vấn tính toán một tích trong với các khóa tiếp theo,

--- TRANG 6 ---
x₁ FFN 🔥 Attention
Trọng Số Chú Ý
0 -1 -2 -4 -1
0 -1 -3 -2 -1
0 -2 -4 -3 -2
Q K
x₂ x₃ [PAD] x₅ x₄
(a)

Q! R" K
Q^T R"! K
x₁ FFN Attention
x₂ x₃ x₄ x₂ x₃ x₄ x₅
(b)
🔥 -3 -2 -1 -1

Hình 3: (a) Chi tiết huấn luyện trong BICO. BICO sửa đổi chú ý nhân quả thành một chú ý hai chiều. Các tính toán
chú ý được phân chia thành hai phần dựa trên vị trí tương đối của các vector truy vấn và khóa. Các số trong hình
vuông biểu thị khoảng cách tương đối giữa qm và kn. Màu tím và vàng đại diện cho chú ý đến ngữ cảnh đứng trước
và đứng sau, tương ứng. Các hình vuông màu xám biểu thị rằng các token đệm được loại trừ khỏi tính toán chú ý.
(b) Trong quá trình suy luận, mô hình ngôn ngữ áp dụng chú ý nhân quả như thường lệ và dự đoán các token một
cách tự hồi quy. Để rõ ràng, chúng tôi chỉ minh họa một lớp transformer duy nhất và bỏ qua các mô-đun không
liên quan.

không có thông tin vị trí tương đối bất ngờ so với
huấn luyện, miễn là khoảng cách tương đối giữa m
và n không vượt quá độ dài ngữ cảnh tối đa mà
không nằm trong phạm vi của bài báo này.

Để triển khai Eq.5: khi n ≤ m, chúng tôi tính toán
trọng số chú ý như thường lệ; Trong các trường hợp
mà n > m, chúng tôi tích hợp thông tin vị trí với
R^T_θ, chuyển vị của Rθ. Bởi vì R^T_θ,m tương đương
với Rθ,−m cho bất kỳ vị trí m nào được cho, chúng
ta có:

q^T_m kn = (Wqxm)^T(R^T_θ,m)^T R^T_θ,n(Wkxn)
         = (Wqxm)^T Rθ,m R^T_θ,n(Wkxn)
         = (Wqxm)^T R^T_θ,−m Rθ,−n(Wkxn)
         = (Wqxm)^T Rθ,m−n(Wkxn), khi n > m.  (6)

Hình 3 minh họa sửa đổi này của tính toán chú ý,
trong đó các đường và hình vuông màu tím biểu thị
rằng trọng số chú ý được tính toán sử dụng ma trận
Rθ tiêu chuẩn, và màu vàng cho thấy rằng truy vấn
chú ý đến các khóa đứng sau của nó trong cơ chế
chú ý hai chiều được mở rộng. Các số được chú thích
cho thấy khoảng cách tương đối giữa một vector
truy vấn và khóa, với tất cả các giá trị đều không
dương.

4.3 Các Mục Tiêu Giống ABI Cho Các Mô hình Ngôn ngữ Nhân quả

Dựa trên chú ý hai chiều, chúng tôi thực hiện điều
chỉnh cho mục tiêu khử nhiễu mặt nạ tự hồi quy
được thiết kế cho các mô hình ngôn ngữ nhân quả
như một mô hình ngôn ngữ tự mã hóa, do đó cho
phép một token được dự đoán có quyền truy cập
vào toàn bộ ngữ cảnh. Phương pháp của chúng tôi
kết hợp một số thành phần chính:

• Trong quá trình huấn luyện, chúng tôi ngẫu nhiên
thay thế một số token trong đầu vào X bằng một
token đệm, với xác suất pM. Trong văn bản dưới
đây, chúng tôi sử dụng giá trị mặc định pM = 0.15
vì nó đã được sử dụng rộng rãi như tỷ lệ token mặt
nạ kể từ BERT (Devlin et al., 2019). Xem xét rằng
việc giới thiệu một token mặt nạ mới có thể tạo ra
một khoảng cách giữa huấn luyện và suy luận, điều
này làm suy giảm hiệu suất (Yang et al., 2020),
chúng tôi chọn token đệm thay vì token [MASK],
vì các mô hình ngôn ngữ nhân quả thường thiếu
token mặt nạ trong từ vựng. Đầu vào bị hỏng X̂
sau đó được đưa vào mô hình.

• Một token đệm được loại trừ khỏi các tính toán
chú ý, có nghĩa là nó không được chú ý bởi các
token khác, để ngăn chặn việc đưa vào nhiễu ngữ
nghĩa. Điều này được minh họa bởi các hình vuông
màu xám trong trọng số chú ý trong Hình 3(a).

• Tại vị trí đầu ra thứ i−1, mô hình dự đoán token
bị che ở vị trí đầu vào thứ i, phù hợp với hành vi
dự đoán token tiếp theo trong giai đoạn tiền huấn
luyện của mô hình. Chỉ việc dự đoán các token bị
che góp phần vào tính toán tổn thất. Chính thức,
mục tiêu tối ưu hóa

--- TRANG 7 ---
GSM Test
"prompt": "James mua 5 gói thịt bò mỗi gói 4 pound. Giá thịt bò là $5.50 mỗi pound. Anh ấy đã trả bao nhiêu?"
"completion": "Anh ấy đã mua 5*4=20 pound thịt bò. Anh ấy đã trả 20*5.5=$110. Đáp án là: 110"

←−GSM Test
"prompt": "James mua x gói thịt bò mỗi gói 4 pound. Giá thịt bò là $5.50 mỗi pound. Anh ấy đã trả bao nhiêu? Nếu chúng ta biết đáp án cho câu hỏi trên là 110, giá trị của biến chưa biết x là bao nhiêu?"
"completion": "Tổng trọng lượng của thịt bò là 4*x. Bởi vì 4x * 5.5 = 110. x là 5."

Hình 4: Một mẫu kiểm tra từ bộ dữ liệu GSM8k gốc (Cobbe et al., 2021), cùng với đối tác "đảo ngược" được tạo ra bởi Yu et al. (2023). Câu hỏi đảo ngược đòi hỏi các mô hình được huấn luyện chỉ trên tập huấn luyện GSM8k gốc phải thể hiện khả năng suy luận ngược để giải quyết.

được định nghĩa như sau:

max ∑(T-1,t=1) 1(xt+1 là [PAD]) · log p(xt+1|X̂; Θ).    (7)
Θ

Cho rằng việc tinh chỉnh các mô hình chỉ với một
nhiệm vụ khử nhiễu mặt nạ có thể làm giảm khả
năng thành thạo của mô hình trong việc tiếp tục văn
bản, chúng tôi áp dụng mục tiêu NTP trong một số
bước huấn luyện để bảo tồn khả năng sinh của mô
hình, ký hiệu phần này là pO. Nó được đặt là 0.5
theo mặc định. Chúng tôi thảo luận tác động của pM
và pO trong Phần 6.

Các kỹ thuật được mô tả ở trên đưa vào ít khoảng
cách cho các mô hình ngôn ngữ nhân quả. Do đó,
một mô hình được điều chỉnh BICO có thể tiếp tục
với quá trình suy luận thông thường, tức là giải mã
tự hồi quy token tiếp theo sử dụng một cơ chế chú
ý nhân quả.

5 Thí Nghiệm và Phân Tích
5.1 Kết Quả Chính
Chúng tôi đánh giá hiệu quả của BICO thông qua hai
nhiệm vụ riêng biệt. Ban đầu, chúng tôi áp dụng BICO
để giải quyết lời nguyền đảo ngược gặp phải trong
nhiệm vụ tên-đến-mô tả hư cấu, như được thảo luận
trong Phần 3. Tiếp theo, chúng tôi đánh giá tính hữu
ích thực tế của nó trong việc giải quyết các vấn đề
toán học, thể hiện khả năng của nó để cải thiện các
kỹ năng suy luận đảo ngược của LLM ở một mức độ
nhất định.

Ánh Xạ Tên Hư cấu đến Mô tả Chúng tôi sử dụng
BICO để tinh chỉnh các mô hình Llama (7B và 13B)
và đánh giá hiệu suất trên cả nhiệm vụ N2D và ←−N2D
(Xem Hình 2 để biết chi tiết dữ liệu). Kết quả thí
nghiệm được trình bày trong Bảng 2. Khi áp dụng
BICO được đề xuất của chúng tôi, một sự gia tăng
độ chính xác đáng kể được quan sát cho nhiệm vụ
đảo ngược,

Mô hình      Mục tiêu    N2D(EM)    ←−N2D
Llama-7B     NTP         67.33      0.00
             BICO        69.67      68.33
Llama-13B    NTP         58.67      0.00
             BICO        66.00      71.67

Bảng 2: BICO hiệu quả giảm thiểu lời nguyền đảo ngược
trong quá trình tinh chỉnh Llama với kiến thức mới, dẫn
đến những cải thiện đáng kể trong hiệu suất trên nhiệm
vụ ←−N2D mà không có bất kỳ tác động có hại nào đến
hiệu suất của nhiệm vụ N2D. Điểm khớp chính xác được
báo cáo.

tăng từ 0% lên khoảng 70%. Dựa trên sự gia tăng
được quan sát này, chúng tôi hoàn thành bước cuối
cùng được nêu trong §3.2 để xác thực giả thuyết
của chúng tôi rằng các mục tiêu huấn luyện thực sự
có thể ảnh hưởng đến lời nguyền đảo ngược, phục
vụ như một trong những yếu tố góp phần của nó.
Hơn nữa, bản thân BICO góp phần như một phương
pháp tinh chỉnh không đưa vào lời nguyền đảo ngược
bổ sung cho các mô hình ngôn ngữ nhân quả.

Mục tiêu     GSM      ←−GSM
NTP          38.21    5.33
BICO         38.28    6.53

Bảng 3: Chúng tôi tinh chỉnh một mô hình Llama-7B sử
dụng bộ dữ liệu GSM8k (Cobbe et al., 2021) với NTP và
BICO, tương ứng. Độ chính xác câu trả lời trung bình được
báo cáo. Các mô hình được điều chỉnh được đánh giá trên
các câu hỏi kiểm tra gốc (được ký hiệu là GSM) và các
câu hỏi đảo ngược được xây dựng bởi Yu et al. (2023)
(được ký hiệu là ←−GSM).

--- TRANG 8 ---
Giải Toán Ngược Để thể hiện thêm hiệu quả của
BICO, chúng tôi giải quyết một nhiệm vụ thực tế
hơn: giải toán. Thiết lập thí nghiệm cơ bản bao gồm
việc dạy LLM các kỹ năng giải toán cơ bản và kiểm
tra chúng với các phương pháp "logic đảo ngược"
để giải quyết các bài toán toán học. Bộ dữ liệu (Yu
et al., 2023), được dẫn xuất từ GSM8k (Cobbe et al.,
2021), có các câu hỏi toán được đảo ngược so với
những câu trong bộ dữ liệu GSM8k gốc. Hình 4 minh
họa một điểm dữ liệu từ GSM8k cùng với đối tác
đảo ngược của nó. Chúng tôi sử dụng NTP và BICO
để tinh chỉnh một mô hình Llama-7B trên bộ dữ liệu
GSM8k gốc, tương ứng. Chúng tôi đánh giá hiệu suất
của các mô hình được điều chỉnh trên cả tập kiểm
tra gốc và đảo ngược. Tất cả các siêu tham số huấn
luyện và cấu hình theo Yu et al. (2023). Kết quả
được thể hiện trong Bảng 3.

Chúng tôi quan sát rằng BICO duy trì hiệu suất của
nó trên tập kiểm tra gốc trong khi đạt được một sự
gia tăng hơn 1 điểm trong việc giải quyết các bài
toán toán đảo ngược, đây là một cải thiện đáng kể
trong nhiệm vụ giải toán và vượt qua t-test với giá
trị p < 0.01. Cho rằng các mô hình chưa thấy bất kỳ
chuỗi suy luận ngược nào để giải quyết những loại
câu hỏi toán này trong quá trình tinh chỉnh, sự cải
thiện có thể được quy cho BICO, thể hiện khả năng
của nó để tăng cường các mô hình ngôn ngữ nhân
quả với khả năng suy luận đa năng hơn.

Dịch thuật Bản chất kép của dữ liệu dịch thuật
đặc biệt phù hợp để đánh giá lời nguyền đảo ngược.
Chúng tôi giả định rằng với dữ liệu huấn luyện theo
thứ tự "ngôn ngữ X-ngôn ngữ Y", mô hình có thể
hoạt động kém trong việc dịch "ngôn ngữ Y-sang-
ngôn ngữ X" do lời nguyền đảo ngược. Để chứng
minh điều này, chúng tôi thực hiện một nhiệm vụ
dịch máy đơn giản từ tiếng Trung sang tiếng Anh.
Dưới đây là các chi tiết thí nghiệm:

Chúng tôi phát triển một tập hợp các ví dụ dịch
Trung-Anh được cấu trúc như sau: "Khi dịch thuật
ngữ tiếng Trung '另外一个' sang tiếng Anh, biểu
thức tương đương là 'Another one'." Để kiểm tra,
chúng tôi đảo ngược dữ liệu huấn luyện để thực
hiện các nhiệm vụ dịch Anh-Trung, như: "Khi dịch
cụm từ tiếng Anh 'another one' sang tiếng Trung,
biểu thức tiếng Trung tương ứng là," với phản hồi
đúng là '另外一个.' Chúng tôi kiểm tra nhiệm vụ
này trên mô hình Llama-7b, mà có khả năng ngôn
ngữ tiếng Trung hạn chế. Cả bộ dữ liệu huấn luyện
và kiểm tra đều bao gồm 100 ví dụ mỗi bộ. Chúng
tôi đánh giá hiệu suất dịch thuật sử dụng chỉ số
Khớp chính xác. Kết quả thí nghiệm được liệt kê
trong Bảng 4. Lưu ý rằng độ chính xác 0-shot cho

0-shot  NTP   BICO
EM (%)    51    63    69

Bảng 4: BICO tăng cường tính hữu ích của dữ liệu huấn
luyện cho nhiệm vụ dịch ngược, do đó cải thiện độ chính
xác của dịch ngược. Lưu ý rằng mô hình Llama có khả
năng đa ngôn ngữ vốn có, được thể hiện trong điểm số
zero-shot.

Llama là 50%, phản ánh khả năng song ngữ gốc của
nó. BICO vượt trội hơn NTP 6%, làm nổi bật hiệu
quả tiềm năng của nó trong các nhiệm vụ thế giới
thực như dịch máy.

6 Thảo Luận
Phần này bao gồm một số phân tích cần thiết về
phương pháp BICO được đề xuất của chúng tôi, cũng
như nghiên cứu đương đại về lời nguyền đảo ngược.

6.1 Phân Tích Phương Pháp
Cấu hình của các siêu tham số trong BICO, bao gồm
pM và pO, tuân theo các thực hành thông thường
tương tự như BERT (Devlin et al., 2019). Tuy nhiên,
trong BERT, tỷ lệ token mặt nạ pM và sự đánh đổi
giữa hai nhiệm vụ tiền huấn luyện pO thiếu thảo
luận và hình thức hóa rõ ràng. Chúng tôi đã nghiên
cứu tác động của các siêu tham số của chúng tôi và
chi tiết có thể được tìm thấy trong phụ lục A.

6.2 Giảm Thiểu Lời Nguyền Đảo Ngược
Một số công trình đương đại nhằm giải quyết lời
nguyền đảo ngược thông qua chỉnh sửa kiến thức
hoặc tăng cường dữ liệu.

Chỉnh sửa kiến thức (Ma et al., 2023b) tích hợp
kiến thức đảo ngược trực tiếp vào các tham số mô
hình, đảm bảo hiệu suất. Tuy nhiên, phương pháp
này tốn nhiều công sức, đòi hỏi chú thích tỉ mỉ và
triển khai phức tạp. Nó đòi hỏi các câu phải tuân
theo một cấu trúc cụ thể của một chủ thể, theo sau
là một mối quan hệ, và sau đó một đối tượng. Ngược
lại, BICO không phụ thuộc vào cấu trúc câu và dễ
triển khai hơn.

Theo chúng tôi, Guo et al. (2024) đề xuất giảm
thiểu lời nguyền đảo ngược thông qua tăng cường
dữ liệu. Nó có thể tạo ra dữ liệu đảo ngược rõ ràng
để huấn luyện, có thể tăng cường hiệu suất trong
các tình huống phức tạp. Tuy nhiên, phương pháp
này có thể gặp phải các vấn đề rò rỉ nhãn trong
các thí nghiệm nghiên cứu, và do đó chúng tôi
không so sánh BICO với nó.

--- TRANG 9 ---
Đây là một tình huống mà cả BICO và ABI đều không
thể giảm thiểu lời nguyền đảo ngược nhưng tăng
cường dữ liệu có thể giải quyết theo (Guo et al.,
2024). Xem xét mối quan hệ nghịch đảo của RN2D
đã nghiên cứu trước đây, được ký hiệu là RD2N.
Việc ánh xạ từ D đến N cũng tạo thành một song
ánh: RD2N = {< d, n > | d mô tả n, (n ∈ N) ∧ (d ∈ D)}.

Tất cả các thiết lập thí nghiệm và chi tiết huấn
luyện vẫn giống hệt với những gì đã được giới thiệu
trước đây. Hình 8 trong phụ lục minh họa việc xây
dựng dữ liệu liên quan đến mối quan hệ này. Sau
khi điều chỉnh, chúng tôi gặp phải một hiện tượng
khó hiểu: Như được thể hiện trong Hình 5, sau khi
điều chỉnh, chúng tôi quan sát một hiện tượng khó
hiểu: trong khi BICO cải thiện khả năng xảy ra của
sự thật cơ bản so với các mô hình được điều chỉnh
NTP, tác động của nó đến điểm khớp chính xác hoặc
điểm BLEU (Papineni et al., 2002) là không đáng
kể (tham khảo phụ lục A để tính toán khả năng xảy
ra và kết quả chi tiết). Chúng tôi đã xem xét kỹ
lưỡng các chiến lược giải mã, bao gồm tìm kiếm
chùm, lấy mẫu top-k và top-p, nhưng chúng không
thể hiện nhiều khác biệt.

Một giả thuyết hợp lý là các LLM được tiền huấn
luyện có thể đã phát triển một "thường thức" trong
quá trình tiền huấn luyện. "Thường thức" này gợi
ý rằng, so với mô tả-đến-tên, một mối quan hệ tên-
đến-mô tả có xu hướng là một ánh xạ một-đến-nhiều,
dẫn đến sự nhầm lẫn trong nhiệm vụ ←−D2N. Ví dụ,
khi được đặt câu hỏi, "Joe Biden là ai?", mô hình có
thể trả lời với nhiều mô tả đúng như "một người đam
mê xe hơi," hoặc "một người đàn ông lớn tuổi," thay
vì "tổng thống Mỹ." Trong khi tăng cường dữ liệu
rõ ràng có thể cung cấp một giải pháp cho vấn đề
này, được hỗ trợ bởi các phát hiện trong nghiên cứu
diễn giải gần đây (Wang et al., 2024), phương pháp
này chỉ khuyến khích mô hình ghi nhớ dữ liệu đảo
ngược thay vì tăng cường khả năng suy luận của nó.

7 Kết Luận
Chúng tôi là những người đầu tiên nghiên cứu các
nguyên nhân cơ bản của lời nguyền đảo ngược và
quy nó cho sự kết hợp của các mục tiêu huấn luyện
và một số cơ chế suy luận nhất định. Khi xem xét
tác động của các mục tiêu huấn luyện, chúng tôi
giới thiệu một phương pháp tinh chỉnh sáng tạo cho
các mô hình ngôn ngữ nhân quả có tên BICO. Phương
pháp này tùy chỉnh các mô hình Llama cho các mục
tiêu giống ABI, do đó giảm thiểu lời nguyền đảo
ngược xuất hiện trong giai đoạn huấn luyện. Chúng
tôi hy vọng thu hút sự chú ý của cộng đồng đến cấu
hình phổ biến của các mô hình ngôn ngữ lớn, đặc
biệt làm nổi bật các hạn chế vốn có trong

Xác Suất Trung Bình
BICO
NTP
0.08  0.16  0.24  0.32  0.40

32.3%
38.7%
27.5%
30.1%

Llama-7b
Llama-13b

Hình 5: Xác suất của việc hoàn thành mong muốn cho
các prompt được cung cấp bởi các mô hình khác nhau
trong nhiệm vụ ←−D2N. Xác suất này được đánh giá
trên toàn bộ tập kiểm tra và được trình bày như một
trung bình. Rõ ràng là BICO tăng cường khả năng xảy
ra của việc đạt được dự đoán sự thật cơ bản.

mô hình huấn luyện hiện tại.

Hạn Chế
Như những hạn chế, vẫn còn một số câu hỏi nghiên
cứu mở đáng được điều tra thêm: Thứ nhất, việc
định lượng ảnh hưởng của các quá trình khác đối
với các mô hình tiên tiến, như RLHF, đối với lời
nguyền đảo ngược đặt ra một thách thức phức tạp
hơn và cần nhiều nghiên cứu hơn. Thứ hai, hiểu
các cơ chế khác nhau ngoài mục tiêu huấn luyện
góp phần làm trầm trọng thêm lời nguyền đảo ngược
là quan trọng, và chúng tôi hoãn nhiệm vụ này cho
các nỗ lực nghiên cứu tương lai.

Lời Cảm Ơn
Công trình này được hỗ trợ bởi Quỹ Khoa học Tự
nhiên Quốc gia Trung Quốc (Số Grant NSFC
62122089), Chương trình Nhà khoa học Trẻ Xuất
sắc Bắc Kinh SỐ. BJJWZYJH012019100020098, và
Nền tảng Quản trị Xã hội Thông minh, Nền tảng
Liên ngành Đổi mới và Quy hoạch Chính cho Sáng
kiến "Đôi-Hạng Nhất", Đại học Nhân dân Trung
Quốc, Quỹ Nghiên cứu Cơ bản cho các Đại học
Trung ương, và Quỹ Nghiên cứu của Đại học Nhân
dân Trung Quốc. Ang Lv được hỗ trợ bởi Các Chương
trình Được Tài trợ Bồi dưỡng Nhân tài Sáng tạo
Xuất sắc 2024 của Đại học Nhân dân Trung Quốc.

--- TRANG 10 ---
Tài Liệu Tham Khảo
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan
Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-
feng Gao, Ming Zhou, và Hsiao-Wuen Hon. 2020.
Unilmv2: Pseudo-masked language models for uni-
fied language model pre-training.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent,
và Christian Janvin. 2003. A neural proba-
bilistic language model. J. Mach. Learn. Res.,
3(null):1137–1155.

Lukas Berglund, Meg Tong, Max Kaufmann, Mikita
Balesni, Asa Cooper Stickland, Tomasz Korbak, và
Owain Evans. 2023. The reversal curse: Llms trained
on "a is b" fail to learn "b is a".

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, và Dario Amodei.
2020. Language models are few-shot learners.

Shouyuan Chen, Sherman Wong, Liangjian Chen, và
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, và John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
và Hsiao-Wuen Hon. 2019. Unified language model
pre-training for natural language understanding and
generation.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, và Jie Tang. 2022. GLM:
general language model pretraining with autoregres-
sive blank infilling. pages 320–335.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly,
Nova DasSarma, Dawn Drain, Deep Ganguli, Zac
Hatfield-Dodds, Danny Hernandez, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, và Chris Olah. 2021. A
mathematical framework for transformer circuits.
Transformer Circuits Thread. Https://transformer-
circuits.pub/2021/framework/index.html.

Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang
Bian, và Yujiu Yang. 2024. Mitigating reversal
curse in large language models via semantic-aware
permutation training.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In ICLR 2022.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,
Luke Zettlemoyer, và Omer Levy. 2020. Span-
BERT: Improving pre-training by representing and
predicting spans. Transactions of the Association for
Computational Linguistics, 8:64–77.

Jun Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu,
và Cong Liu. 2023a. Untying the reversal curse
via bidirectional language model editing. ArXiv,
abs/2310.10322.

Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu,
và Cong Liu. 2023b. Untying the reversal curse via
bidirectional language model editing.

Kevin Meng, David Bau, Alex J Andonian, và Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems.

Jack Merullo, Carsten Eickhoff, và Ellie Pavlick. 2024.
Circuit component reuse across tasks in transformer
language models.

OpenAI. 2023. Gpt-4 technical report.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Alec Radford và Karthik Narasimhan. 2018. Im-
proving language understanding by generative pre-
training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, và Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.

--- TRANG 11 ---
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, và Yunfeng Liu. 2022. Roformer: En-
hanced transformer with rotary position embedding.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, và Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, và Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.

Boshi Wang, Xiang Yue, Yu Su, và Huan Sun. 2024.
Grokked transformers are implicit reasoners: A mech-
anistic journey to the edge of generalization.

Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Buck Shlegeris, và Jacob Steinhardt. 2023. Inter-
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, và Quoc V. Le. 2020.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo
Li, Adrian Weller, và Weiyang Liu. 2023. Meta-
math: Bootstrap your own mathematical questions
for large language models.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414.

Liu Zhuang, Lin Wayne, Shi Ya, và Zhao Jun. 2021. A
robustly optimized BERT pre-training approach with
post-training. In Proceedings of the 20th Chinese
National Conference on Computational Linguistics,
pages 1218–1227, Huhhot, China. Chinese Informa-
tion Processing Society of China.

--- TRANG 12 ---
Hình 6: pO cân bằng việc hiểu toàn diện dữ liệu huấn
luyện của mô hình và khả năng sinh. Khi giá trị pO phù
hợp được chọn, BICO giảm thiểu lời nguyền đảo ngược
trong nhiệm vụ ←−N2D. Sự vắng mặt của nhúng vị trí
sửa đổi (w/o M.P.) cản trở quá trình học và dẫn đến
kết quả kém hơn do vấn đề ngoài phân phối.

A Phân Tích Phương Pháp của BICO
Chúng tôi điều tra tác động của các siêu tham số trong
BICO đối với việc giảm thiểu lời nguyền đảo ngược
do các mục tiêu huấn luyện gây ra. Điều quan trọng
cần lưu ý là chúng tôi không nhằm tìm kiếm hiệu
suất tốt nhất cho một nhiệm vụ cụ thể, mà là nghiên
cứu các đặc tính của BICO. Mô hình chúng tôi sử
dụng trong phần này là Llama-7B.

Lựa Chọn pO ABI gốc sử dụng các token đặc biệt
như "<BOS>" và "<EOS>" để chỉ ra bắt đầu và kết
thúc của chuỗi bị che. Tuy nhiên, đáng chú ý là
điều này khác với cách Llama sử dụng những token
đặc biệt này. Do đó, chúng tôi đã chọn không sử
dụng chúng như các dấu hiệu cho việc che trong
BICO. Kết quả là, chúng tôi thấy các mô hình Llama
được điều chỉnh với BICO gặp khó khăn trong việc
kết thúc sinh hiệu quả. Chúng tôi quan sát rằng
chúng có xu hướng tạo ra các mô tả dài, liên quan
đến chủ đề. Để giải quyết vấn đề này, chúng tôi
giới thiệu giải pháp: tại mỗi bước huấn luyện, mô
hình được tối ưu hóa với mục tiêu NTP với xác suất
pO và được tối ưu hóa với khử nhiễu mặt nạ tự hồi
quy với xác suất 1−pO.

Chúng tôi nghiên cứu việc lựa chọn pO, trong khi
duy trì tỷ lệ mặt nạ không đổi là pM = 0.15, một
giá trị được áp dụng rộng rãi trong các mô hình tự
mã hóa (Devlin et al., 2019; Raffel et al., 2020).
Kết quả của chúng tôi được minh họa trong Hình 6.
Chúng tôi quan sát rằng do vấn đề đã thảo luận
trước đây, khi pO = 0, các mô hình không thể tạo
ra mô tả chính xác trong nhiệm vụ N2D, và hoạt
động kém trong ←−N2D. Mô hình được tinh chỉnh
hoàn toàn với NTP (pO = 1) gặp phải lời nguyền
đảo ngược, đạt được tỷ lệ chính xác 0% trong nhiệm
vụ ←−N2D.

Hình 7: Tác động của chiến lược mặt nạ đến hiệu suất
mô hình. Từ phần trên của hình, hiệu suất mô hình vẫn
nhất quán trong một phạm vi rộng các giá trị pM (0.15
đến 0.45). Chúng tôi cũng thấy rằng che khoảng và che
i.i.d. không thể hiện sự khác biệt đáng chú ý. Để dễ so
sánh, chúng tôi sử dụng đường đứt nét màu đỏ để biểu
thị kết quả tốt nhất được cung cấp bởi Llama được điều
chỉnh hoàn toàn NTP.

Một kết quả cân bằng đạt được với pO nhỏ khoảng
1/4, cho phép mô hình bảo tồn khả năng sinh của
nó trong khi học kỹ lưỡng từ dữ liệu huấn luyện.
Kết quả là, có một sự cải thiện đáng kể trong hiệu
suất nhiệm vụ N2D-reverse, tăng vọt từ 0% lên
khoảng 80%, với sự thành thạo duy trì trong nhiệm
vụ N2D. Chúng tôi cũng khám phá tác động của
việc tinh chỉnh mô hình mà không sửa đổi tính toán
chú ý của nó (pO = 0, w/o M.P.) để giải quyết các
vị trí OOD. Do nhu cầu một phần của các cập nhật
tham số để giải quyết các vấn đề OOD trong nhúng
vị trí (Chen et al., 2023), quá trình học bị chậm lại.

Chiến Lược Mặt Nạ Chúng tôi điều tra chiến lược
mặt nạ trong BICO. Chúng tôi đặt tham số pO ở
giá trị không đổi 1/4 và thay đổi xác suất mặt nạ
pM từ 0.05 đến 0.55, tăng theo từng bước 0.1. Kết
quả được minh họa trong phần trên của Hình 7.
Quan sát thấy rằng các giá trị cực trị của pM, như
0.55 hoặc 0.05, cho kết quả dưới tối ưu, trong khi
các giá trị trung gian không cho thấy sự phân kỳ
đáng kể.

Ngoài ra, chúng tôi khám phá khoảng mặt nạ. Các
nghiên cứu trước đây (Raffel et al., 2020; Joshi et al.,
2020) gợi ý rằng việc che một khoảng token liền
kề

--- TRANG 13 ---
Dữ liệu huấn luyện
"prompt": "Được biết đến vì là nhà soạn nhạc nổi tiếng của bản giao hưởng dưới nước đầu tiên thế giới, "Giai điệu Vực thẳm.",", "completion": " Uriah Hawthorne giờ đây tận hưởng cuộc sống yên tĩnh."}

D2N Test
"prompt": " Được ca ngợi rộng rãi vì đã sáng tác bản giao hưởng dưới nước đầu tiên thế giới, "Giai điệu Vực thẳm.",", "completion": " Uriah Hawthorne"}

←−D2N Test
"prompt": "Trong sử sách về sự độc đáo, Uriah Hawthorne tỏa sáng như", "completion": " nhà soạn nhạc nổi tiếng của bản giao hưởng dưới nước đầu tiên thế giới, "Giai điệu Vực thẳm."."}

Hình 8: Dữ liệu được sử dụng để nghiên cứu lời nguyền đảo ngược trên mối quan hệ RD2N. Tất cả tên và mô tả đều hư cấu.
Trong giai đoạn kiểm tra, mô hình được cung cấp "prompt" và sự thật cơ bản là nội dung của "completion."

có thể hiệu quả hơn so với việc sử dụng các token
mặt nạ độc lập và phân phối đồng nhất (i.i.d.), tương
đương với khoảng mặt nạ là 1. Trong thí nghiệm
của chúng tôi, chúng tôi khám phá độ dài khoảng
mặt nạ S, và kết quả ở phần dưới của Hình 7. Chúng
tôi không quan sát bất kỳ sự khác biệt hiệu suất
rõ ràng nào giữa các cài đặt S khác nhau.

Nhiệm vụ BD2N và ←−D2N
Hình 8 minh họa một điểm dữ liệu trong nhiệm vụ
D2N và ←−D2N.

Đối với chi tiết về tính toán khả năng xảy ra trong
Hình 5: Chúng tôi tính toán khả năng xảy ra của
sự thật cơ bản được gán bởi LLM sau khi tinh chỉnh
như sau:

p(completion|prompt) = e^(-LNLL),

trong đó LNLL = -∑(i=k to l) log p(ti|t0:i-1).    (8)

Ở đây, ti biểu thị token thứ i trong chuỗi, có độ
dài l. Quan trọng, chúng tôi không tính đến các vị
trí tương ứng với prompt (k token đầu tiên) khi
tính toán tổn thất.

Điểm Khớp Chính xác và điểm BLEU (Papineni
et al., 2002) (đặc biệt cho nhiệm vụ ←−D2N, bao
gồm việc tạo ra các mô tả dài) của các mô hình
được điều chỉnh bởi các mục tiêu huấn luyện khác
nhau được báo cáo trong Bảng 5.

Bảng 5: Các mô hình được điều chỉnh bởi các mục tiêu
huấn luyện khác nhau đều gặp khó khăn một cách nhất
quán trong nhiệm vụ D2N và ←−D2N.

Mô hình      Mục tiêu    D2N(EM)   ←−D2N(EM)   ←−D2N(BLEU)
GLM-2B       NTP         100.00    0.00        19.70
             ABI         100.00    0.07        22.13
GLM-10B      NTP         100.00    0.00        19.01
             ABI         99.33     1.67        22.15
Llama-7B     NTP         100.00    0.00        19.65
             BICO        99.67     1.00        21.00
Llama-13B    NTP         98.67     0.00        20.62
             BICO        99.33     1.33        22.15
