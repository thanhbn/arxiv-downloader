# Bird-Eye Transformers cho Các Mô hình Sinh văn bản
Lei Sha, Yuhang Song, Yordan Yordanov, Tommaso Salvatori, Thomas Lukasiewicz
Khoa Khoa học Máy tính, Đại học Oxford, Oxford, UK
firstname.lastname@cs.ox.ac.uk
## Tóm tắt
Transformers đã trở thành một module không thể thiếu cho các mô hình sinh văn bản kể từ thành công lớn của chúng trong dịch máy. Các nghiên cứu trước đây cho rằng thành công của transformers là do cơ chế attention dot-product query-key-value, cung cấp inductive bias mạnh mẽ thông qua các đồ thị token được kết nối đầy đủ. Tuy nhiên, chúng tôi phát hiện ra rằng self-attention có một hạn chế nghiêm trọng. Khi dự đoán token thứ (i+1), self-attention chỉ lấy token thứ i làm bộ thu thập thông tin, và nó có xu hướng đưa ra trọng số attention cao cho những token tương tự như chính nó. Do đó, hầu hết thông tin lịch sử đã xảy ra trước token thứ i không được xem xét. Dựa trên quan sát này, trong bài báo này, chúng tôi đề xuất một kiến trúc mới, gọi là bird-eye transformer (BET), đi xa hơn một bước để cải thiện hiệu suất của transformers bằng cách cân bằng lại self-attention để khuyến khích nó tập trung nhiều hơn vào thông tin lịch sử quan trọng. Chúng tôi đã tiến hành thí nghiệm trên nhiều tác vụ sinh văn bản, bao gồm dịch máy (2 bộ dữ liệu) và mô hình ngôn ngữ (3 bộ dữ liệu). Những kết quả thí nghiệm này cho thấy mô hình được đề xuất của chúng tôi đạt hiệu suất tốt hơn so với các kiến trúc transformer cơ sở trên tất cả các bộ dữ liệu. Mã nguồn được phát hành tại: https://sites.google.com/view/bet-transformer/home .

## 1 Giới thiệu
Việc ứng dụng thành công của transformers (Vaswani et al., 2017) trong dịch máy cho thấy rằng nó là một lựa chọn tốt hơn nhiều cho mô hình hóa chuỗi so với các kiến trúc auto-regressive như RNNs (Rumelhart et al., 1986) và LSTMs (Hochreiter and Schmidhuber, 1997). Cốt lõi của transformers là self-attention, một module tính toán tương quan token-by-token dựa trên dot-product. So với các kiến trúc auto-regressive phổ biến trước đây, self-attention trực tiếp xây dựng kết nối giữa các token, điều này cũng nắm bắt được các phụ thuộc tầm xa.

Tuy nhiên, chúng tôi phát hiện ra rằng self-attention có một số nhược điểm nghiêm trọng, cụ thể là các module self-attention tập trung quá nhiều vào token hiện tại và không cung cấp attention cụ thể cho các token lịch sử "cấp cao". Các token "cấp cao" đề cập đến loại token có thể ảnh hưởng đến các token khác trên khoảng cách dài, ví dụ, các token nằm gần với token cần được dự đoán trong cây phân tích cú pháp phụ thuộc. Ví dụ, trong câu "...cat catch a mouse...", nếu token hiện tại là "a" và chúng ta đang dự đoán token "mouse". Trong module self-attention, "a" được lấy làm bộ thu thập thông tin để tính trọng số attention với các token khác bằng dot product. Điều này dẫn đến việc token tiếp theo "mouse" chủ yếu được dự đoán bởi thông tin của token "a", trong khi một số thông tin quan trọng hơn đã xảy ra trước đó không được xem xét đủ (như "cat" và "catch").

Để giải quyết những nhược điểm nói trên của self-attention, trong bài báo này, chúng tôi đề xuất khuyến khích các trọng số attention tập trung nhiều hơn vào các token "cấp cao" bằng một số hướng dẫn cú pháp. Chúng tôi đề xuất một kiến trúc mới gọi là bird-eye transformer (BET) để cung cấp cho transformers một cái nhìn bird-eye về tất cả các token lịch sử. BET có hai kiến trúc thay thế để đạt được mục tiêu này: (1) Một kiến trúc transformer được hướng dẫn cú pháp, gọi là BET(SG): Kiến trúc này lấy một số gợi ý cú pháp từ cây phân tích cú pháp phụ thuộc và sử dụng những gợi ý đó để cân bằng lại ma trận dot-product của self-attention. (2) Một kiến trúc transformer không có hướng dẫn cú pháp, gọi là BET(SF): Chúng tôi không sử dụng bất kỳ gợi ý cú pháp nào trong kiến trúc này. Để cung cấp cái nhìn bird-eye, trước tiên chúng tôi cân bằng lại ma trận dot-product của self-attention hướng về việc attention các token cấp cao. Chúng tôi đạt được điều này bằng cách sử dụng một hàm quyết định token nào là token cấp cao theo chuỗi đầu vào và chuỗi đầu ra của self-attention. Kiến trúc này được kỳ vọng sẽ tự tạo ra thông tin cấp cao.

Ngoài ra, chúng tôi chỉ ra rằng các trọng số attention trên token hiện tại (trong ma trận dot-product) nên được phân chia cho các token lịch sử khác. Sự thay đổi này sẽ không dẫn đến mất thông tin của token hiện tại, bởi vì nó sẽ được thêm lại trong phần residual connection (He et al., 2016) của transformer.

Cuối cùng, ma trận self-attention được tinh chỉnh được thu được bằng cách áp dụng Softmax trên ma trận dot-product được cân bằng lại. Những đóng góp chính của bài báo này được tóm tắt ngắn gọn như sau:

• Chúng tôi chỉ ra những nhược điểm nghiêm trọng trong self-attention của transformers, và chúng tôi báo cáo về kết quả thí nghiệm chi tiết chứng minh những nhược điểm này.
• Chúng tôi đề xuất một kiến trúc bird-eye transformer mới, tinh chỉnh self-attention trong transformers để cung cấp cho nó cái nhìn bird-eye về thông tin lịch sử trong mô hình hóa chuỗi.
• Chúng tôi tiến hành thí nghiệm trên nhiều tác vụ và so sánh với một số baseline tự nhiên để chứng minh hiệu quả của bird-eye transformer được đề xuất.

## 2 Nền tảng
Transformers được đề xuất bởi Vaswani et al. (2017) cho dịch máy, sử dụng một loạt các transformer blocks trong phần encoder và decoder. Mỗi transformer block chứa một lớp self-attention, một lớp feed-forward, nhiều skip-connection, và layer normalization. Self-attention là thành phần cốt lõi của transformers.

Cho một chuỗi đầu vào X = (x₁; ...; xₙ) của các biểu diễn token, trước tiên chúng ta sử dụng ba phép biến đổi tuyến tính để ánh xạ X thành ba ma trận: queries Q, keys K, và values V. Sau đó, self-attention được tính như sau:

Q = XW^Q; K = XW^K; V = XW^V;
D = QK^T/√d; A(Q,K,V) = Softmax(D)V;     (1)

trong đó d là chiều vector của biểu diễn mỗi token, W^Q, W^K, và W^V đều là các tham số có thể huấn luyện, và D là ma trận dot-product.

Mỗi transformer block có hai sub-block, một cho self-attention, và một là lớp feed-forward. Kết quả của cả self-attention và phép toán feed-forward sau đó được cộng lại với đầu vào như một residual connection (He et al., 2016), theo sau bởi layer normalization:

X' = LayerNorm(X + A(Q,K,V));     (2)
H = LayerNorm(X' + FFL(X'));      (3)

trong đó FFL là viết tắt của feed-forward layer.

Trong sinh văn bản, khi transformer block được sử dụng như decoder, chúng ta cần nhân ma trận self-attention với một mặt nạ tam giác để ngăn mỗi token attention các token tiếp theo.

## 3 Phương pháp
### 3.1 Động lực
Theo trực giác, module self-attention nên tập trung vào một số token "cấp cao" thay vì tập trung quá nhiều vào token hiện tại.

Giả sử chúng ta đang dự đoán token thứ (i+1) tương ứng với các token trước đó x₀; ...; xᵢ. Theo Eq. (1), hàng thứ i của ma trận dot-product D được tính như:

Dᵢ = [1/√d xᵢW^QW^K^T x₀^T; ...; 1/√d xᵢW^QW^K^T xᵢ^T; [M]; ...; [M];]     (4)

trong đó "[M]" là viết tắt của các phần tử được che mặt nạ tương ứng với các token tương lai. Dot-product D được sử dụng để đo sự liên quan giữa các token. Theo trực giác, vì một token luôn tương tự với chính nó hơn các token khác, item thứ i xᵢW^QW^K^T xᵢ được kỳ vọng là lớn nhất trong Eq. (4).

Tuy nhiên, trên thực tế, token thứ i không cần thiết phải attention chính nó, vì trong kiến trúc transformer, thông tin của token thứ i có thể được thêm vào vector đặc trưng thu thập được bởi residual connection (He et al., 2016), như được thể hiện trong Eq. (2). Nếu chúng ta che mặt nạ các giá trị attention đường chéo của ma trận self-attention và phân chia các giá trị attention cho các token lịch sử, thì các token lịch sử sẽ nhận được nhiều attention hơn và attention của token hiện tại cũng được giữ lại bởi residual connection.

### 3.2 Bird-Eye Transformers
Các mô hình sinh văn bản có xu hướng sử dụng thông tin của các token lịch sử để dự đoán token tiếp theo. Tuy nhiên, tầm quan trọng của các token lịch sử không phải lúc nào cũng giống nhau. Trong quan điểm ngôn ngữ học, ngôn ngữ tự nhiên được xây dựng bởi một tập hợp các quy tắc cú pháp (Chomsky, 1956, 2014), đó là một cấu trúc giống như cây. Vì vậy, theo vị trí của các token trong cây cú pháp, các token có thể được chia thô thành hai loại: token cấp cao và token cấp thấp. Các token cấp cao thường chứa thông tin tổng quát của câu hiện tại và có thể ảnh hưởng đến các token cách xa chúng, trong khi các token cấp thấp chỉ có thể ảnh hưởng đến các token gần. Do đó, việc chú ý nhiều hơn đến các token cấp cao có triển vọng góp phần vào việc dự đoán tốt hơn token tiếp theo trong các mô hình sinh, như được thể hiện trong một số nghiên cứu dựa trên LSTM (Shen et al., 2018; Sha et al., 2018).

Chúng tôi muốn đi xa hơn một bước để đề xuất một kiến trúc mới, gọi là bird-eye transformer (BET). Kiến trúc transformer này tinh chỉnh các trọng số self-attention để khuyến khích nó tập trung vào các token lịch sử nhiều thông tin hơn.

**Syntax-guided BET** Vì các đặc trưng cú pháp của token hiện tại thường cung cấp các gợi ý hữu ích cho token tiếp theo, chúng tôi đề xuất sử dụng một số gợi ý cú pháp để hướng dẫn các trọng số attention thay đổi hướng về các token "cấp cao", được đặt tên là BET(SG). Việc hướng dẫn của các gợi ý cú pháp được thực hiện bởi một pointer loss như được thể hiện trong Fig. 1(b). Các gợi ý cú pháp được lấy trực tiếp từ cây phân tích cú pháp phụ thuộc. Nếu xₜ₊₁ là token cần được dự đoán, thì chúng ta sẽ lấy node tổ tiên gần nhất của nó trong cây phân tích cú pháp phụ thuộc mà cũng xuất hiện ở bên trái của xₜ₊₁ trong câu làm gợi ý cú pháp của token hiện tại xₜ. Nếu không có node tổ tiên nào xuất hiện ở bên trái của xₜ₊₁, thì gợi ý cú pháp là chính token hiện tại. Ở đây, gợi ý cú pháp này là thông tin "cấp cao" w.r.t token cần được dự đoán.

Định dạng đầu vào của gợi ý cú pháp là một vector one-hot cho mỗi token, "1" nằm ở vị trí mà gợi ý cú pháp nằm. Giả sử rằng những vector one-hot này (độ dài n) cho n token là y_s ∈ R^(n×n), pointer loss của mỗi transformer block là:

L_p = ∑ y_s log A     (5)

Pointer loss của mỗi transformer block được cộng lại và nhân với một siêu tham số λ_p, sau đó nó được thêm vào hàm loss cuối cùng.

**Syntax-guidance-free BET** Kiến trúc này có xu hướng tự tạo ra các gợi ý cú pháp mà không cần tín hiệu bên ngoài, được đặt tên là BET(SF) như được thể hiện trong Fig. 2. Sự khác biệt chính giữa BET(SF) và transformer tiêu chuẩn nằm ở hai chỗ: (1) chúng tôi sử dụng thông tin bird-eye để cân bằng lại self-attention, khuyến khích attention các token lịch sử nhiều thông tin hơn. Chi tiết được mô tả trong Figure 2, và (2) chúng tôi thêm một mặt nạ đường chéo vào ma trận self-attention.

Theo trực giác, đầu ra của self-attention thu thập thông tin cấp cao hơn so với đầu vào, vì vậy nó có thể giúp quyết định từ nào trong đầu vào là từ cấp cao. Cho đầu vào X, chúng ta có thể nhận được kết quả của self-attention H như sau:

Q = XW^Q; K = XW^K; V = XW^V;
M_dp = Masked-MatMul(Q,K);
A = Softmax(M_dp); H = V^T A     (6)

trong đó Masked-MatMul sử dụng mặt nạ tam giác để đảm bảo rằng mỗi token không thể attention các token tương lai, M_dp là ma trận dot-product, và H, Q, và K đều có kích thước n×d. Quá trình này cũng được trình bày rõ ràng trong Fig. 2 (b).

Sau đó, chúng tôi sử dụng lớp ẩn H và key K để quyết định từ nào là từ cấp cao:

R = Sigmoid(w^T([H;K]));     (7)

trong đó w ∈ R^(2d×1) là một vector tham số có thể huấn luyện, "[;]" đại diện cho việc nối hai tensor, và R ∈ R^d là một vector xác suất mã hóa liệu từ tương ứng có phải là từ cấp cao hay không.

Cuối cùng, chúng tôi sử dụng vector xác suất R để cân bằng lại ma trận dot-product, và tính toán lại ma trận self-attention như sau:

M' = M_dp ⊙ R;     (8)
A' = Softmax(M');     (9)
H' = V^T A'.     (10)

Sau phép toán rescaling, ma trận đặc trưng kết quả được kỳ vọng chứa thông tin cấp từ. H' là đầu ra của module self-attention của BET. Module attention của BET cũng có thể được mở rộng thành multi-head attention và có thể là một lựa chọn thay thế cho standard multi-head attention như được thể hiện trong Figure 2(c). Vì sự thay đổi chính của BET nằm trong module self-attention, chúng ta cũng có thể xếp chồng nhiều lớp BET lại với nhau. So với các transformer thông thường, tham số duy nhất mà chúng tôi đưa vào BET là vector w khi quyết định các từ cấp cao. Vector tham số này không yêu cầu quá nhiều dung lượng bộ nhớ bổ sung so với lượng lớn các tham số trong transformers tiêu chuẩn.

**Diagonal-Free Mask** Trong một lớp transformer, residual connection (He et al., 2016) có thể trực tiếp đưa đầu vào đến lớp tiếp theo. Do đó, trong mô hình sinh văn bản, khi dự đoán token thứ (i+1), biểu diễn của token thứ i đã được gửi trực tiếp đến lớp sau self-attention. Kết quả là, module self-attention không cần thiết phải attention token hiện tại nữa.

Vì vậy, chúng tôi đề xuất thêm một mặt nạ diagonal-free sau phép toán bird-eye rescaling, trực tiếp che mặt nạ tất cả các phần tử đường chéo trong ma trận dot-product trước khi áp dụng phép toán Softmax để thu được ma trận self-attention. Vì token đầu tiên chỉ có một token (chính nó) để attention, hàng đầu tiên của ma trận self-attention không được che mặt nạ. Như được thể hiện trong Fig. 2 (b), mặt nạ đường chéo được thực hiện trước module Softmax cuối cùng.

## 4 Thí nghiệm
Trong phần này, trước tiên chúng tôi giới thiệu các bộ dữ liệu được sử dụng trong các phần phân tích thí nghiệm, và sau đó chúng tôi sử dụng các nghiên cứu ablation để thảo luận về những nhược điểm của self-attention. Cuối cùng, chúng tôi cho thấy hiệu suất của mô hình BET được đề xuất.

### 4.1 Bộ dữ liệu và Cài đặt Thí nghiệm
Theo quan sát của chúng tôi, những nhược điểm của self-attention ảnh hưởng đến hiệu suất của phần decoder của transformer. Vì vậy, chúng tôi sử dụng hai tác vụ sinh văn bản, cụ thể là, dịch máy và mô hình hóa ngôn ngữ, để phân tích cơ chế self-attention. Các bộ dữ liệu dịch máy là IWSLT 2014 German-English (De-En) (Cettolo et al., 2016) và WMT 2017 English-German (En-De) (Ondrej et al., 2017), được đánh giá bằng điểm BLEU (Papineni et al., 2002). Các bộ dữ liệu mô hình ngôn ngữ là WikiText-2 (Merity et al., 2016), Wiki-103 (Merity et al., 2016), và Enwiki8 (Mahoney, 2011). Metric đánh giá là perplexity (Brown et al., 1992) cho các bộ dữ liệu cấp từ (WikiText-2, Wiki-103) và bits-per-character (BPC) (Graves, 2013) cho các bộ dữ liệu cấp ký tự (Enwiki8). Cài đặt thí nghiệm chi tiết được liệt kê như sau.

**Dịch máy.** Chúng tôi sử dụng hai bộ dữ liệu dịch máy (dưới giấy phép CC-BY-SA): IWSLT 2014 German-English (De-En) (Cettolo et al., 2016) và WMT 2017 English-German (En-De) (Ondrej et al., 2017). Metric đánh giá là điểm BLEU (Papineni et al., 2002).

Đối với dịch máy German-English (De-En), chúng tôi sử dụng cùng cách chia dữ liệu train/valid/test như trước đây¹. Chúng tôi có 153K cặp câu song song để huấn luyện, 7k để validation, và 7k để testing. Chúng tôi sử dụng BPE (Sennrich et al., 2015) để có được từ vựng subword. Sau đó, chúng tôi có được từ vựng source-target chung gồm hơn 10K token. Chúng tôi phát triển mô hình của mình dựa trên repository Fairseq² (Ott et al., 2019). Chúng tôi sử dụng kiến trúc encoder-decoder dựa trên transformer của Vaswani et al. (2017) làm mô hình cơ sở, được gọi là "iwslt14.tokenized.de-en" trong Fairseq. Kích thước lớp ẩn và chiều word embedding được đặt là 512. Chúng tôi cũng sử dụng 6 lớp transformer cho encoder và decoder. Batch size được đặt là 22, Adam (Kingma và Ba, 2014) được sử dụng để tối ưu hóa với β₁ = 0.9 và β₂ = 0.98, và learning rate được cập nhật trong quá trình huấn luyện sử dụng phương pháp của Vaswani et al. (2017).

Đối với dịch máy English-German (En-De), có 1.9M cặp câu song song để huấn luyện, 2k để validation, và 3k để testing. Chúng tôi sử dụng cùng mô hình với tác vụ trước. Sau phân đoạn BPE, chúng tôi kết hợp từ vựng của tiếng Anh và tiếng Đức lại như đã nhất quán với Vaswani et al. (2017). Kích thước từ vựng cuối cùng là 25,860. Số lượng lớp encoder và decoder là 6. Các cài đặt khác giống như cho De-En.

**Mô hình hóa Ngôn ngữ.** Chúng tôi sử dụng ba bộ dữ liệu (dưới giấy phép CC-BY-SA) cho mô hình hóa ngôn ngữ: WikiText-2 (Merity et al., 2016), Wiki-103 (Merity et al., 2016), và Enwiki8 (Mahoney, 2011). WikiText-2 (Merity et al., 2016) là một bộ dữ liệu cấp từ nhỏ, chứa 2M token huấn luyện và kích thước từ vựng 33k. Wiki-103 (Merity et al., 2016) là một bộ dữ liệu cấp từ lớn với nhiều phụ thuộc tầm xa. Điều này tốt để phát hiện những nhược điểm của self-attention. Có 103M token và 28K bài báo để huấn luyện trong Wiki-103. Độ dài trung bình của các bài báo là 3.6K token. Enwiki8 (Mahoney, 2011) là một bộ dữ liệu cấp ký tự, chứa 100M byte văn bản Wikipedia thô. Có 205 ký tự duy nhất trong bộ dữ liệu Enwiki8. Metric đánh giá là perplexity (Brown et al., 1992) cho các bộ dữ liệu cấp từ và bits-per-character (BPC) (Graves, 2013) cho các bộ dữ liệu cấp ký tự. Chiều của lớp ẩn và word embedding của transformers là 300 cho WikiText-2 và WikiText-103, và 512 cho Enwiki8. Chúng tôi sử dụng Adam (Kingma và Ba, 2014) với learning rate 0.001 để tối ưu hóa.

### 4.2 Phân tích Self-Attention của Transformers
Để chứng minh rằng token hiện tại đã lấy quá nhiều trọng số attention mà lẽ ra thuộc về các token lịch sử khác, chúng tôi tính toán và so sánh các giá trị trọng số self-attention trung bình của token thứ i dành cho chính nó và cho các token lịch sử khác. Chúng tôi tiến hành loại phân tích này trên tác vụ dịch máy và mô hình hóa ngôn ngữ, và báo cáo kết quả trong Table 1.

Trong Table 1, chúng tôi có ba metric để phân tích: CA, HA, và Ratio, được định nghĩa như sau:
• **Current Attention (CA)**: Khi dự đoán token thứ (i+1), self-attention sử dụng token thứ i làm bộ thu thập đặc trưng, vì vậy CA có nghĩa là trọng số attention của token thứ i đối với chính nó, nằm trong đường chéo của ma trận self-attention. Số CA trong Table 1 là trung bình của tất cả các trọng số đường chéo trong ma trận self-attention trên toàn bộ tập test.

• **Historical Attention (HA)**: Khi dự đoán token thứ (i+1), HA có nghĩa là trọng số attention của token thứ i đối với tất cả các token lịch sử trước đó (thứ 0 đến (i-1)), nằm trong phần tam giác dưới của ma trận self-attention. Chúng tôi tính toán trung bình và độ lệch chuẩn của toàn bộ ma trận self-attention tam giác dưới trên tập test và ghi lại chúng trong Table 1.

• **Ratio**: Đây là tỷ lệ giữa CA và HA. Nếu con số này lớn, có nghĩa là token thứ i tập trung quá nhiều vào chính nó.

Theo Table 1, trong tất cả sáu lớp trong mô hình hóa ngôn ngữ, token thứ i đang tập trung vào chính nó (chúng tôi gọi hiện tượng này là "self-attending" trong bài báo này) khoảng ba lần nhiều hơn so với tập trung vào các token lịch sử khi dự đoán token thứ (i+1). Đặc biệt ở lớp thứ năm, tỷ lệ này thậm chí còn lớn hơn 5. Ngoài ra, trong các tác vụ dịch máy, tỷ lệ CA/HA của lớp đầu tiên cực kỳ cao, đạt gần 7. Thực tế này cho thấy rằng cơ chế self-attention hiện tại cho phép mỗi token attention chính chúng quá nhiều, điều này không tốt cho việc tận dụng các token lịch sử.

### 4.3 Cài đặt Kiểm tra Ablation của Transformers
Trong phần này, chúng tôi giới thiệu các cài đặt thí nghiệm để kiểm tra tầm quan trọng của các giá trị attention đường chéo và các giá trị attention tam giác dưới. Cái trước có xu hướng khám phá hiệu ứng khi các giá trị attention trên token hiện tại được phân phối cho các token lịch sử, trong khi cái sau khám phá hiệu ứng khi chúng ta tăng cường hoặc làm suy yếu sự khác biệt giữa các giá trị attention trên các token lịch sử.

**Hiệu ứng của Các Giá trị Attention Đường chéo.** Để điều tra sâu hơn các cách có thể cải thiện hiệu suất của self-attention, chúng tôi thực hiện ba thay đổi nhỏ đối với cơ chế self-attention hiện tại, và kiểm tra hiệu ứng của chúng đối với hiệu suất trên hai tác vụ dịch máy và mô hình hóa ngôn ngữ. Ba thay đổi nhỏ và sự kết hợp của chúng được minh họa như sau:

• **Reduced/Magniﬁed Diag**: Để làm suy yếu/tăng cường các trọng số attention của "self-attending", trước tiên chúng tôi nhân 20%/200% với các phần tử đường chéo của ma trận dot product. Sau đó, chúng tôi thực hiện phép toán Softmax.

• **DiagFreeMask**: mặt nạ diagonal-free sẽ che mặt nạ tất cả các phần tử đường chéo trong ma trận self-attention trừ phần tử đầu tiên. Phần tử đường chéo đầu tiên sẽ không được che mặt nạ, vì token đầu tiên chỉ có thể attention chính nó.

### 4.4 Baselines cho BET
Chúng tôi tiến hành so sánh thí nghiệm với các baseline chỉ dùng decoder sau đây, tất cả đều đã thực hiện một số cải tiến trên module self-attention:

• **Hopﬁeld Network (Hopﬁeld, 2007)**: Chúng tôi sử dụng chính xác cùng kiến trúc trong Ramsauer et al. (2020), tiết lộ mối liên kết giữa mạng Hopﬁeld và transformers, và tích hợp mạng Hopﬁeld vào các phương pháp deep learning dựa trên back-propagation. Ramsauer et al. (2020) sử dụng lý thuyết associative memory (Radhakrishnan et al., 2020) để tinh chỉnh ma trận self-attention bằng cách tối thiểu hóa một hàm năng lượng. Trong thực tế, quá trình tối thiểu hóa này được thực hiện bằng cách cập nhật lặp đi lặp lại ma trận self-attention để đạt được trạng thái ổn định. Chúng tôi trực tiếp tích hợp mã của các lớp mạng Hopﬁeld³ vào mã của chúng tôi.

• **Yang et al. (2018)**: Phương pháp này cố gắng thêm một số bias Gaussian vào self-attention để nắm bắt ngữ cảnh địa phương hữu ích.

• **Zhao et al. (2019)**: Phương pháp này sử dụng top-k sparse self-attention để tập trung vào các token đóng góp nhất.

• **Transformer-XL (Dai et al., 2019)**: Phương pháp này được đề xuất để học phụ thuộc dài hạn bằng một sự lặp lại cấp đoạn trong các hidden state. Chúng tôi chạy mã được phát hành trong khi giữ các siêu tham số giống với cài đặt của chúng tôi để so sánh.

• **Routing Transformer (Roy et al., 2021b)**: Phương pháp này cố gắng giảm bớt chi phí tính toán của module self-attention bằng cách sử dụng k-means clustering để tránh attention nội dung không liên quan đến query. Để làm cho kết quả có thể so sánh được, chúng tôi trực tiếp thay đổi các siêu tham số của họ trong mã nguồn⁴ để làm cho chúng giống với cài đặt thí nghiệm của chúng tôi, và chạy lại các thí nghiệm trên các bộ dữ liệu của chúng tôi.

Chúng tôi sử dụng cùng cài đặt thí nghiệm như trong Section 4.2 để so sánh công bằng. Phương pháp của chúng tôi được gắn nhãn "BET(SG)" và "BET(SF)" trong các thí nghiệm sau. Đối với các kiểm tra ablation, chúng tôi loại bỏ DiagFreeMask khỏi mỗi kiến trúc BET để cho thấy sự thay đổi trong hiệu suất.

### 4.5 Hiệu suất Tổng thể và Nghiên cứu Ablation
Hiệu suất tổng thể được thể hiện trong Table 2. Chúng ta có thể thấy rằng, với sự trợ giúp của các gợi ý cú pháp, BET(SG) đạt được hiệu suất cao nhất. Khi chúng tôi để mô hình tự tạo ra các gợi ý cú pháp, phương pháp BET không có hướng dẫn cú pháp được đề xuất của chúng tôi (BET(SF)) cũng đã vượt trội so với tất cả các phương pháp baseline, mặc dù không cao bằng BET(SG). Điều này chứng minh hiệu quả của cơ chế bird-eye của chúng tôi.

Đặc biệt, chúng tôi phát hiện ra rằng DiagFreeMask cũng rất hữu ích trong kiến trúc BET(SF) của chúng tôi. Đối với các kiểm tra ablation cho các giá trị attention đường chéo. Kết quả được liệt kê như Table 2. Chúng ta có thể thấy rằng, sau khi chúng tôi che mặt nạ các phần tử đường chéo của ma trận dot-product, hiệu suất của năm tác vụ tăng lên đáng kể, vì các token lịch sử nhận được nhiều attention hơn sau khi các phần tử đường chéo được che mặt nạ, và chúng đóng góp nhiều hơn cho việc dự đoán token thứ (i+1). Tương tự, khi chúng tôi giảm các phần tử đường chéo xuống 20%, hiệu suất cũng cải thiện một chút. Đường cong thay đổi của điểm BLEU (cho các tác vụ dịch máy) và perplexity (cho các tác vụ mô hình ngôn ngữ) theo tỷ lệ giảm được thể hiện trong Figure 3.

Sau khi chúng tôi loại bỏ DiagFreeMask, perplexities và BPCs tăng lên một chút, có nghĩa là mô hình ngôn ngữ trở nên kém chính xác hơn khi không có DiagFreeMask. Sự so sánh của các đường cong loss trước và sau khi chúng tôi thêm DiagFreeMask vào ma trận self-attention được thể hiện trong Figure 4.

Ngoài ra, việc so sánh các đường cong huấn luyện cho ba bộ dữ liệu (Fig. 5) cho thấy một lợi thế rất rõ ràng của BET(SF).

Sau khi chúng tôi đặt các siêu tham số của tất cả các phương pháp baseline giống nhau (bao gồm số lượng lớp, số lượng head, chiều của các lớp ẩn), chúng tôi phát hiện ra rằng phương pháp được đề xuất của chúng tôi đã vượt trội so với tất cả các phương pháp cạnh tranh. Mạng Hopﬁeld tận dụng associative memories để lưu trữ các mẫu đầu vào (văn bản hoặc hình ảnh), đây là một bước đầu tiên của việc tích hợp khoa học thần kinh vào deep learning dựa trên backpropagation. Điều này cho phép mạng Hopﬁeld vượt trội so với transformers trên các tác vụ mà associative memories lớn là cần thiết, ví dụ: các tác vụ multiple-instance learning (Ramsauer et al., 2020). Vì vậy, mạng Hopﬁeld mạnh hơn trong encoding thay vì decoding. Yang et al. (2018) và Zhao et al. (2019) đã cố gắng sử dụng bias Gaussian hoặc top-k sparse self-attention để tập trung vào các token đã nhận được nhiều trọng số attention hơn. Ngược lại, phương pháp được đề xuất của chúng tôi có xu hướng tập trung nhiều hơn vào các token liên quan đến cú pháp, do đó module attention được sửa đổi có nhiều thông tin hơn. Transformer-XL không thiết kế đặc biệt module self-attention, trong khi routing transformers có xu hướng tập trung vào các token có thể được phân cụm cùng với token hiện tại, điều này cũng ít thông tin hơn so với các token liên quan đến cú pháp. Do đó, phương pháp được đề xuất của chúng tôi có thể đạt được hiệu suất cao hơn.

### 4.6 Nghiên cứu Trường hợp
Chúng tôi vẫn cần trả lời câu hỏi liệu BET không có cú pháp có thể tận dụng các token quan trọng hơn hay không. Chúng tôi liệt kê các token được attention nhiều nhất trong tất cả sáu lớp BET(SF) hoặc transformer trong Table 3. Chúng ta có thể thấy rằng ở các lớp cao hơn (lớp thứ tư đến thứ sáu), BET(SF) có thể tập trung vào nhiều token liên quan khi dự đoán token màu đỏ. Ví dụ, ở lớp thứ tư, khi dự đoán "iron", BET(SF) tập trung nhiều nhất vào "warship" trong khi các transformer khác đang tập trung vào từ trước đó "by". Rõ ràng, token "warship" đóng góp nhiều hơn token "by" khi dự đoán token "iron". Tuy nhiên, đồng thời, ở các lớp thấp hơn, như lớp thứ hai, các token được tập trung bởi cả BET(SF) và các transformer khác có thể không liên quan nhiều đến token cần được dự đoán. Điều này cho thấy rằng BET(SF) cũng có thể hưởng lợi từ nhiều lớp xếp chồng hơn. Lưu ý rằng ở lớp 3, token "attack" thực sự liên quan rất chặt chẽ với từ trước đó "heart", được attention chính xác bởi các transformer khác. Mặc dù, trong ma trận attention của BET(SF), token "heart" không được attention, kiến trúc BET(SF) của chúng tôi sẽ không mất thông tin của token "heart", vì nó có thể được bổ sung bởi residual connection trong BET(SF).

## 5 Nghiên cứu Liên quan
Sau khi transformers được chứng minh hữu ích trong dịch máy (Vaswani et al., 2017; Tay et al., 2020b), một số lượng lớn các biến thể transformer đã được tạo ra. Hầu hết chúng đều tập trung vào việc xấp xỉ ma trận self-attention chi phí bậc hai bằng một phương pháp chi phí thấp. Những mô hình này có thể được chia thành bốn loại theo cách tính toán attention. Loại thứ nhất làm thưa self-attention để giảm chi phí tính toán. Trong số đó, Qiu et al. (2020) và Parmar et al. (2018) chia chuỗi đầu vào thành nhiều khối, và chỉ tính self-attention giữa các khối. Sparse Transformer (Child et al., 2019) và Longformer (Beltagy et al., 2020) xem xét các mẫu attention strided. Compressed Attention (Liu et al., 2018) sử dụng thêm strided convolution để nén ma trận self-attention. Ngoài ra, Axial Transformer (Ho et al., 2019) kết hợp một loạt các mẫu sparse self-attention lại với nhau để có được độ bao phủ tốt hơn của self-attention gốc. Transformer-XL (Dai et al., 2019) kết nối thêm nhiều đoạn và khối bằng cơ chế lặp lại. Loại thứ hai học cách chia chuỗi đầu vào thành các khối. Ví dụ, Reformer (Kitaev et al., 2020) và Routing Transformer (Roy et al., 2021a) sử dụng thước đo tương tự dựa trên hash và k-means clustering để phân cụm các token đầu vào thành các khối. Sinkhorn Sorting Network (Tay et al., 2020a) thậm chí học cách sắp xếp các khối của chuỗi đầu vào. Loại thứ ba sử dụng bộ nhớ bên ngoài để truy cập tất cả các token cùng một lúc, ví dụ: Set Transformers (Lee et al., 2019) và ETC (Ainslie et al., 2020). Phương pháp này tương tự như parameter attention (Sukhbaatar et al., 2019). Big Bird (Zaheer et al., 2020) được xây dựng dựa trên ETC, vì vậy nó cũng tận dụng global memories. Loại thứ tư là các phương pháp low-rank, giả định rằng ma trận self-attention có thể được thu được bằng cách nhân nhiều ma trận low-rank. Các nghiên cứu đại diện bao gồm Linformer (Wang et al., 2020), Performer (Choromanski et al., 2020), và các phương pháp dựa trên kernel (Katharopoulos et al., 2020).

Nhiều nghiên cứu cũng tích hợp cấu trúc cây vào transformers. Tree transformers (Wang et al., 2019) thêm một constituency prior để khuyến khích self-attention học liệu hai token có thuộc về một span hay không, do đó một cấu trúc phân cấp có thể được học bởi một mô hình nhiều lớp. Các biến thể của position encoding (Shiv và Quirk, 2019) cũng có thể giúp học cấu trúc cây. Mặc dù cơ chế bird-eye rescaling của chúng tôi mượn một số cảm hứng từ cấu trúc phân cấp của ngôn ngữ tự nhiên, mục tiêu chính của chúng tôi là nhận ra các từ cấp cao trong các token lịch sử (như các token liên quan đến cú pháp) và để chúng giúp dự đoán các token tương lai thay vì học toàn bộ cây cú pháp.

Gần đây, các phương pháp (Ramsauer et al., 2020) dựa trên associative memories (Radhakrishnan et al., 2020; Feldman và Zhang, 2020; Krotov và Hopﬁeld, 2016, 2020; Marullo và Agliari, 2021) đã được chỉ ra có liên quan đến self-attention. Associative memories (Le et al., 2020; Chatterjee, 2018; Wang và Cui, 2018) là một khái niệm tâm lý học, đó là khả năng học và ghi nhớ các mối quan hệ của các item không liên quan. Thường thì chúng được áp dụng trong các cách tiếp cận hướng khoa học thần kinh, và được tối ưu hóa bằng cách tối thiểu hóa một hàm năng lượng. Mạng Hopﬁeld (Ramsauer et al., 2020) là một ví dụ tốt về việc tích hợp associative memories vào các mạng neural dựa trên backpropagation. Trong mạng Hopﬁeld, một hàm năng lượng được tối thiểu hóa thông qua việc cập nhật lặp đi lặp lại của self-attention.

## 6 Kết luận
Trong bài báo này, chúng tôi sử dụng một loạt thí nghiệm vững chắc để cho thấy nhược điểm của kiến trúc self-attention trong transformers. Chúng tôi phát hiện ra rằng kiến trúc self-attention hiện tại đã dành quá nhiều trọng số attention cho phần tử đường chéo của ma trận self-attention, trong khi không cung cấp attention cụ thể cho thông tin token lịch sử "cấp cao". Vì vậy, chúng tôi đề xuất một kiến trúc mới của transformers: syntax-free bird-eye transformers (BET-SF), có thể tìm thấy các manh mối cú pháp và tập trung nhiều hơn vào các token lịch sử liên quan đến cú pháp. Trong phân tích thí nghiệm, chúng tôi phát hiện ra rằng BET với thông tin cú pháp bên ngoài (BET-SG) đạt được hiệu suất tốt nhất. Mặc dù syntax-free BET (BET-SF) không chứa bất kỳ thông tin cú pháp bên ngoài nào, nó vẫn vượt trội đáng kể so với các kiến trúc transformer tiêu chuẩn cũng như nhiều phương pháp baseline trên tất cả các bộ dữ liệu của hai tác vụ (dịch máy và mô hình hóa ngôn ngữ).

## Tài liệu Tham khảo
Joshua Ainslie, Santiago Ontanón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, và Li Yang. 2020. ETC: Encoding Long and Structured Inputs in Transformers. arXiv preprint arXiv:2004.08483.

Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The Long-document Transformer. arXiv preprint arXiv:2004.05150.

Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Jennifer C Lai, và Robert L Mercer. 1992. An Estimate of an Upper Bound for the Entropy of English. Computational Linguistics, 18(1):31–40.

Mauro Cettolo, Niehues Jan, Stüker Sebastian, Luisa Bentivogli, Roldano Cattoni, và Marcello Federico. 2016. The IWSLT 2016 Evaluation Campaign. In International Workshop on Spoken Language Translation.

Satrajit Chatterjee. 2018. Learning and Memorization. In International Conference on Machine Learning, pages 755–763. PMLR.

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. 2019. Generating Long Sequences With Sparse Transformers. arXiv preprint arXiv:1904.10509.

Noam Chomsky. 1956. Three Models for the Description of Language. IRE Transactions on information theory, 2(3):113–124.

Noam Chomsky. 2014. Aspects of the Theory of Syntax, volume 11. MIT press.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, et al. 2020. Masked Language Modeling for Proteins via Linearly Scalable Long-context Transformers. arXiv preprint arXiv:2006.03555.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, và Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988.

Vitaly Feldman và Chiyuan Zhang. 2020. What Neural Networks Memorize and Why: Discovering the Long Tail via Inﬂuence Estimation. arXiv preprint arXiv:2008.03703.

Alex Graves. 2013. Generating Sequences With Recurrent Neural Networks. arXiv preprint arXiv:1308.0850.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778.

Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, và Tim Salimans. 2019. Axial Attention in Multidimensional Transformers. arXiv preprint arXiv:1912.12180.

Sepp Hochreiter và Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural computation, 9(8):1735–1780.

John J Hopﬁeld. 2007. Hopﬁeld Network. Scholarpedia, 2(5):1977.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. 2020. Transformers Are RNNs: Fast Autoregressive Transformers With Linear Attention. In International Conference on Machine Learning, pages 5156–5165. PMLR.

Diederik P Kingma và Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.

Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The Efﬁcient Transformer. arXiv preprint arXiv:2001.04451.

Dmitry Krotov và John Hopﬁeld. 2020. Large Associative Memory Problem in Neurobiology and Machine Learning. arXiv preprint arXiv:2008.06996.

Dmitry Krotov và John J Hopﬁeld. 2016. Dense Associative Memory for Pattern Recognition. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 1180–1188.

Hung Le, Truyen Tran, và Svetha Venkatesh. 2020. Self-attentive Associative Memory. In International Conference on Machine Learning, pages 5682–5691. PMLR.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, và Yee Whye Teh. 2019. Set Transformer: A Framework for Attention-based Permutation-invariant Neural Networks. In International Conference on Machine Learning, pages 3744–3753. PMLR.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, và Noam Shazeer. 2018. Generating Wikipedia by Summarizing Long Sequences. arXiv preprint arXiv:1801.10198.

Matt Mahoney. 2011. Large Text Compression Benchmark.

Chiara Marullo và Elena Agliari. 2021. Boltzmann Machines As Generalized Hopﬁeld Networks: A Review of Recent Results and Outlooks. Entropy, 23(1):34.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv preprint arXiv:1609.07843.

Bojar Ondrej, Rajen Chatterjee, Federmann Christian, Graham Yvette, Haddow Barry, Huck Matthias, Koehn Philipp, Liu Qun, Logacheva Varvara, Monz Christof, et al. 2017. Findings of the 2017 Conference on Machine Translation (WMT17). In Second Conference on Machine Translation, pages 169–214. The Association for Computational Linguistics.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, và Michael Auli. 2019. Fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In NAACL-HLT (Demonstrations).

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, và Dustin Tran. 2018. Image Transformer. In International Conference on Machine Learning, pages 4055–4064. PMLR.

Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, và Jie Tang. 2020. Blockwise Self-Attention for Long Document Understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 2555–2565.

Adityanarayanan Radhakrishnan, Mikhail Belkin, và Caroline Uhler. 2020. Overparameterized Neural Networks Implement Associative Memory. Proceedings of the National Academy of Sciences, 117(44):27162–27170.

Hubert Ramsauer, Bernhard Schäﬂ, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi ć, Geir Kjetil Sandve, et al. 2020. Hopﬁeld Networks Is All You Need. arXiv preprint arXiv:2008.02217.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. 2021a. Efﬁcient Content-based Sparse Attention With Routing Transformers. Transactions of the Association for Computational Linguistics, 9:53–68.

Aurko Roy, Mohammad Taghi Saffar, Ashish Vaswani, và David Grangier. 2021b. Efﬁcient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68.

David E Rumelhart, Geoffrey E Hinton, và Ronald J Williams. 1986. Learning Representations by Back-propagating Errors. nature, 323(6088):533–536.

Rico Sennrich, Barry Haddow, và Alexandra Birch. 2015. Neural Machine Translation of Rare Words With Subword Units. arXiv preprint arXiv:1508.07909.

Lei Sha, Feng Qian, Baobao Chang, và Zhifang Sui. 2018. Jointly Extracting Event Triggers and Arguments by Dependency-bridge RNN and Tensor-based Argument Interaction. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32.

Yikang Shen, Shawn Tan, Alessandro Sordoni, và Aaron Courville. 2018. Ordered Neurons: Integrating Tree Structures Into Recurrent Neural Networks. In International Conference on Learning Representations.

Vighnesh Shiv và Chris Quirk. 2019. Novel Positional Encodings to Enable Tree-based Transformers. Advances in Neural Information Processing Systems, 32:12081–12091.

Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, và Armand Joulin. 2019. Augmenting Self-attention With Persistent Memory. arXiv preprint arXiv:1907.01470.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, và Da-Cheng Juan. 2020a. Sparse Sinkhorn Attention. In International Conference on Machine Learning, pages 9438–9447. PMLR.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. 2020b. Efﬁcient Transformers: A Survey. arXiv preprint arXiv:2009.06732.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is All You Need. In Advances in Neural Information Processing Systems, pages 5998–6008.

Jin-Hui Wang và Shan Cui. 2018. Associative Memory Cells and Their Working Principle in the Brain. F1000Research, 7.

Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, và Hao Ma. 2020. Linformer: Self-attention With Linear Complexity. arXiv preprint arXiv:2006.04768.

Yaushian Wang, Hung-Yi Lee, và Yun-Nung Chen. 2019. Tree Transformer: Integrating Tree Structures Into Self-Attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1060–1070.

Baosong Yang, Zhaopeng Tu, Derek F Wong, Fandong Meng, Lidia S Chao, và Tong Zhang. 2018. Modeling localness for self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4449–4458.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences. arXiv preprint arXiv:2007.14062.

Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, và Xu Sun. 2019. Explicit sparse transformer: Concentrated attention through explicit selection. arXiv preprint arXiv:1912.11637.
