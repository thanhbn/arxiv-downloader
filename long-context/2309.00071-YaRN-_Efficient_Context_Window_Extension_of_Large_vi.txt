# 2309.00071.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2309.00071.pdf
# Kích thước tệp: 721866 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
YaRN: Mở rộng Cửa sổ Ngữ cảnh Hiệu quả cho Mô hình
Ngôn ngữ Lớn
Bowen Peng1∗Jeffrey Quesnelle1†Honglu Fan23Enrico Shippole‡
1Nous Research2EleutherAI3Đại học Geneva
Tóm tắt
Rotary Position Embeddings (RoPE) đã được chứng minh là có thể mã hóa thông tin vị trí một cách hiệu quả trong các mô hình ngôn ngữ dựa trên transformer. Tuy nhiên, những mô hình này không thể tổng quát hóa vượt quá độ dài chuỗi mà chúng được huấn luyện. Chúng tôi giới thiệu YaRN (Yet another RoPE extensioN method), một phương pháp hiệu quả về tính toán để mở rộng cửa sổ ngữ cảnh của các mô hình như vậy, yêu cầu ít hơn 10 lần token và ít hơn 2,5 lần bước huấn luyện so với các phương pháp trước đó. Sử dụng YaRN, chúng tôi cho thấy rằng các mô hình LLaMA có thể sử dụng hiệu quả và ngoại suy đến độ dài ngữ cảnh dài hơn nhiều so với việc tiền huấn luyện ban đầu cho phép, đồng thời cũng vượt qua state-of-the-art trước đó trong việc mở rộng cửa sổ ngữ cảnh. Ngoài ra, chúng tôi chứng minh rằng YaRN thể hiện khả năng ngoại suy vượt ra ngoài ngữ cảnh hạn chế của một bộ dữ liệu tinh chỉnh. Các mô hình được tinh chỉnh sử dụng YaRN đã được cung cấp và tái tạo trực tuyến với độ dài ngữ cảnh lên đến 128k tại https://github.com/jquesnelle/yarn .

--- TRANG 2 ---
1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn dựa trên Transformer [40] (LLMs) đã trở thành lựa chọn gần như phổ biến nhất cho nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) nơi khả năng tầm xa như học ngữ cảnh (ICL) đã trở nên quan trọng. Khi thực hiện các nhiệm vụ NLP, độ dài tối đa của các chuỗi (cửa sổ ngữ cảnh) được xác định bởi các quá trình huấn luyện đã trở thành một trong những giới hạn chính của LLM được tiền huấn luyện. Việc có thể mở rộng động cửa sổ ngữ cảnh thông qua một lượng nhỏ tinh chỉnh (hoặc không cần tinh chỉnh) đã trở nên ngày càng mong muốn. Để đạt được điều này, việc mã hóa vị trí của transformers là trung tâm của các cuộc thảo luận.

Kiến trúc Transformer gốc sử dụng mã hóa vị trí sinusoidal tuyệt đối, sau đó được cải thiện thành mã hóa vị trí tuyệt đối có thể học [15]. Kể từ đó, các sơ đồ mã hóa vị trí tương đối [32] đã tăng thêm hiệu suất của Transformers. Hiện tại, các mã hóa vị trí tương đối phổ biến nhất là T5 Relative Bias [30], RoPE [34], XPos [35], và ALiBi [27].

Một hạn chế thường xuyên xuất hiện với mã hóa vị trí là không thể tổng quát hóa vượt quá cửa sổ ngữ cảnh nhìn thấy trong quá trình huấn luyện. Trong khi một số phương pháp như ALiBi có thể thực hiện tổng quát hóa hạn chế, không có phương pháp nào có thể tổng quát hóa đến các chuỗi dài hơn đáng kể so với độ dài được tiền huấn luyện [22].

Một số công trình đã được thực hiện để vượt qua hạn chế như vậy. [9] và đồng thời [21] đã đề xuất mở rộng độ dài ngữ cảnh bằng cách sửa đổi nhẹ RoPE thông qua Position Interpolation (PI) và tinh chỉnh trên một lượng nhỏ dữ liệu. Như một lựa chọn thay thế, [6] đã đề xuất nội suy "NTK-aware" bằng cách xem xét việc mất mát tần số cao. Kể từ đó, hai cải tiến của nội suy "NTK-aware" đã được đề xuất, với những trọng tâm khác nhau:

• phương pháp nội suy "Dynamic NTK" [14] cho các mô hình được tiền huấn luyện mà không cần tinh chỉnh.
• phương pháp nội suy "NTK-by-parts" [7] hoạt động tốt nhất khi được tinh chỉnh trên một lượng nhỏ dữ liệu ngữ cảnh dài hơn.

Nội suy "NTK-aware" và nội suy "Dynamic NTK" đã xuất hiện trong các mô hình mã nguồn mở như Code Llama [31] (sử dụng nội suy "NTK-aware") và Qwen 7B [2] (sử dụng "Dynamic NTK").

Trong bài báo này, ngoài việc tạo ra một tài khoản hoàn chỉnh về các công trình chưa được công bố trước đây về nội suy "NTK-aware", "Dynamic NTK" và "NTK-by-part", chúng tôi trình bày YaRN (Yet another RoPE extensioN method), một phương pháp cải tiến để mở rộng hiệu quả cửa sổ ngữ cảnh của các mô hình được huấn luyện với Rotary Position Embeddings (RoPE) bao gồm các họ mô hình LLaMA [38], GPT-NeoX [5], và PaLM [10].

YaRN đạt được hiệu suất state-of-the-art trong việc mở rộng cửa sổ ngữ cảnh sau khi tinh chỉnh trên ít hơn ~0,1% dữ liệu tiền huấn luyện gốc. Trong khi đó, bằng cách kết hợp với kỹ thuật thời gian suy luận được gọi là Dynamic Scaling, Dynamic-YaRN cho phép mở rộng cửa sổ ngữ cảnh hơn 2 lần mà không cần tinh chỉnh gì.

2 Kiến thức nền tảng và Công trình liên quan
2.1 Rotary Position Embeddings
Cơ sở của công trình chúng tôi là Rotary Position Embedding (RoPE) được giới thiệu trong [34]. Chúng tôi làm việc trên một lớp ẩn nơi tập hợp các nơ-ron ẩn được ký hiệu bằng D. Cho một chuỗi các vector x1,···,xL∈R|D|, theo ký hiệu của [34], lớp attention đầu tiên chuyển đổi các vector thành các vector truy vấn và các vector khóa:

qm=fq(xm, m)∈R|D|,kn=fk(xn, n)∈R|D|. (1)

Tiếp theo, các trọng số attention được tính toán như sau
softmax (qTmknp|D|), (2)

nơi qm,kn được coi là các vector cột để qTmkn đơn giản là tích trong Euclidean.

Trong RoPE, trước tiên chúng tôi giả định rằng |D| là chẵn và xác định không gian nhúng và các trạng thái ẩn như các không gian vector phức:
R|D|∼=C|D|/2

nơi tích trong qTk trở thành phần thực của tích trong Hermitian chuẩn Re(q∗k). Cụ thể hơn, các đồng cấu xen kẽ phần thực và phần phức

(xm)1,···,(xm)|D|
7→
(xm)1+i(xm)2,···,((xm)|D|−1+i(xm)|D|), (3)

(qm)1,···,(qm)|D|
7→
(qm)1+i(qm)2,···,((qm)|D|−1+i(qm)|D|). (4)

Để chuyển đổi các embedding xm,xn thành các vector truy vấn và khóa, trước tiên chúng tôi được cho các toán tử tuyến tính R
Wq,Wk:R|D|→R|D|.

Trong tọa độ phức, các hàm fq, fk được cho bởi
fq(xm, m) =eimθWqxm, fk(xn, n) =einθWkxn, (5)

nơi θ=diag(θ1,···, θ|D|/2) là ma trận đường chéo với θd=b−2d/|D| và b= 10000. Bằng cách này, RoPE liên kết mỗi nơ-ron ẩn (có giá trị phức) với một tần số riêng biệt θd. Lợi ích của việc làm như vậy là tích vô hướng giữa vector truy vấn và vector khóa chỉ phụ thuộc vào khoảng cách tương đối m−n như sau

⟨fq(xm, m), fk(xn, n)⟩R (6)
=Re(⟨fq(xm, m), fk(xn, n)⟩C) (7)
=Re(x∗mW∗qWkxneiθ(m−n)) (8)
=g(xm,xn, m−n). (9)

Trong tọa độ thực, RoPE có thể được viết sử dụng hàm sau

fW(xm, m, θ d) =
[Ma trận lớn với các thành phần cos và sin]Wxm,

để
fq=fWq, fk=fWk.

2.2 Position Interpolation
Vì các mô hình ngôn ngữ thường được tiền huấn luyện với độ dài ngữ cảnh cố định, việc hỏi làm thế nào để mở rộng độ dài ngữ cảnh bằng cách tinh chỉnh trên lượng dữ liệu tương đối ít là điều tự nhiên. Đối với các mô hình ngôn ngữ sử dụng RoPE làm embedding vị trí, Chen et al. [9], và đồng thời kaiokendev [21] đã đề xuất Position Interpolation (PI) để mở rộng độ dài ngữ cảnh vượt ra ngoài giới hạn được tiền huấn luyện. Trong khi việc ngoại suy trực tiếp không hoạt động tốt trên các chuỗi w1,···, wL với L lớn hơn giới hạn được tiền huấn luyện, họ phát hiện ra rằng việc nội suy các chỉ số vị trí trong giới hạn được tiền huấn luyện hoạt động tốt với sự hỗ trợ của một lượng nhỏ tinh chỉnh. Cụ thể, cho một mô hình ngôn ngữ được tiền huấn luyện với RoPE, họ sửa đổi RoPE bằng

f′W(xm, m, θ d) =fW(xm,mLL′, θd), (10)

nơi L′> L là một cửa sổ ngữ cảnh mới vượt ra ngoài giới hạn được tiền huấn luyện. Với mô hình được tiền huấn luyện gốc cộng với công thức RoPE được sửa đổi, họ đã tinh chỉnh mô hình ngôn ngữ thêm trên vài bậc độ lớn ít hơn token (vài tỷ trong Chen et al. [9]) và thành công đạt được việc mở rộng cửa sổ ngữ cảnh.

--- TRANG 3 ---
2.3 Ký hiệu bổ sung
Tỷ lệ giữa độ dài ngữ cảnh mở rộng và độ dài ngữ cảnh gốc đã có tầm quan trọng đặc biệt, và chúng tôi giới thiệu ký hiệu s được định nghĩa bởi

s=L′L, (11)

và chúng tôi gọi s là hệ số tỷ lệ.

Chúng tôi cũng viết lại và đơn giản hóa Eq. 10 thành dạng tổng quát sau:
f′W(xm, m, θ d) =fW(xm, g(m), h(θd)), (12)

nơi g(m), h(θd) là các hàm phụ thuộc vào phương pháp. Đối với PI, chúng ta có g(m) =m/s, h (θd) =θd. Trong các phần tiếp theo, khi chúng tôi giới thiệu một phương pháp nội suy mới, đôi khi chúng tôi chỉ chỉ định các hàm g(m) và h(θd).

Ngoài ra, chúng tôi định nghĩa λd là bước sóng của embedding RoPE tại chiều ẩn thứ d:
λd=2πθd= 2πb2d|D|. (13)

Bước sóng mô tả độ dài của các token cần thiết để embedding RoPE tại chiều d thực hiện một vòng quay đầy đủ (2π).

Cho rằng một số phương pháp nội suy (ví dụ: PI) không quan tâm đến bước sóng của các chiều, chúng tôi sẽ gọi những phương pháp đó là các phương pháp nội suy "mù", trong khi những phương pháp khác có quan tâm (ví dụ: YaRN), chúng tôi sẽ phân loại là các phương pháp nội suy "có mục tiêu".

2.4 Công trình liên quan
ReRoPE [33] cũng nhằm mở rộng kích thước ngữ cảnh của các mô hình hiện có được tiền huấn luyện với RoPE, và tuyên bố độ dài ngữ cảnh "vô hạn" mà không cần tinh chỉnh gì. Tuyên bố này được hỗ trợ bởi một tổn thất giảm đơn điệu với độ dài ngữ cảnh tăng lên đến 16k trên mô hình Llama 2 13B. Nó đạt được việc mở rộng ngữ cảnh bằng cách sửa đổi cơ chế attention và do đó không phải là một phương pháp nội suy embedding thuần túy. Vì nó hiện tại không tương thích với Flash Attention 2 [13] và yêu cầu hai lần truyền attention trong quá trình suy luận, chúng tôi không xem xét nó để so sánh.

Đồng thời với công trình của chúng tôi, LM-Infinite [16] đề xuất những ý tưởng tương tự như YaRN, nhưng tập trung vào việc tổng quát hóa độ dài "on-the-fly" cho các mô hình không được tinh chỉnh. Vì họ cũng sửa đổi cơ chế attention của các mô hình, nó không phải là một phương pháp nội suy embedding và không tương thích ngay lập tức với Flash Attention 2.

3 Phương pháp luận
Trong khi PI kéo dài tất cả các chiều RoPE một cách bình đẳng, chúng tôi thấy rằng giới hạn nội suy lý thuyết được mô tả bởi PI [9] không đủ để dự đoán động lực phức tạp giữa RoPE và các embedding nội bộ của LLM. Trong các phần phụ tiếp theo, chúng tôi mô tả các vấn đề chính với PI mà chúng tôi đã xác định và giải quyết riêng lẻ, để đưa ra cho người đọc bối cảnh, nguồn gốc và lý do chính đáng của mỗi phương pháp mà chúng tôi sử dụng cùng nhau để có được phương pháp YaRN đầy đủ.

3.1 Mất mát thông tin tần số cao - nội suy "NTK-aware"
Nếu chúng ta chỉ nhìn RoPE từ góc độ mã hóa thông tin, đã được chỉ ra trong [36], sử dụng lý thuyết Neural Tangent Kernel (NTK), rằng các mạng nơ-ron sâu gặp khó khăn trong việc học thông tin tần số cao nếu chiều đầu vào thấp và các embedding tương ứng thiếu các thành phần tần số cao. Ở đây chúng ta có thể thấy những điểm tương đồng: thông tin vị trí của một token là một chiều, và RoPE mở rộng nó thành một embedding vector phức n chiều.

RoPE có nhiều điểm tương đồng với Fourier Features [36], vì có thể định nghĩa RoPE như một trường hợp đặc biệt 1D của Fourier Feature. Việc kéo dài các embedding RoPE một cách bừa bãi dẫn đến việc mất mát các chi tiết tần số cao quan trọng mà mạng cần để phân giải các token vừa rất giống nhau vừa rất gần nhau (việc quay mô tả khoảng cách nhỏ nhất cần không được quá nhỏ để mạng có thể phát hiện ra nó).

Chúng tôi giả thuyết rằng việc tăng nhẹ độ phức tạp (perplexity) cho các kích thước ngữ cảnh ngắn sau khi tinh chỉnh trên các kích thước ngữ cảnh lớn hơn được thấy trong PI [9] có thể liên quan đến vấn đề này. Trong hoàn cảnh lý tưởng, không có lý do gì mà việc tinh chỉnh trên các kích thước ngữ cảnh lớn hơn lại làm giảm hiệu suất của các kích thước ngữ cảnh nhỏ hơn.

Để giải quyết vấn đề mất mát thông tin tần số cao khi nội suy các embedding RoPE, nội suy "NTK-aware" đã được phát triển trong [6]. Thay vì tỷ lệ hóa mọi chiều của RoPE đều bằng một hệ số s, chúng tôi phân tán áp lực nội suy trên nhiều chiều bằng cách tỷ lệ hóa tần số cao ít hơn và tần số thấp nhiều hơn. Người ta có thể đạt được sự biến đổi như vậy bằng nhiều cách, nhưng cách đơn giản nhất là thực hiện thay đổi cơ số trên giá trị của θ.

Cụ thể hơn, theo các ký hiệu được đặt ra trong Phần 2.3, chúng tôi định nghĩa sơ đồ nội suy "NTK-aware" như sau (xem Phụ lục A.1 cho các chi tiết về phép suy diễn).

Định nghĩa 1 Nội suy "NTK-aware" là một sự sửa đổi của RoPE bằng cách sử dụng Eq. 12 với các hàm sau.
g(m) =m (14)
h(θd) =b′−2d/|D|, (15)

nơi
b′=b·s|D||D|−2. (16)

Cho các kết quả từ [6], phương pháp này hoạt động tốt hơn nhiều trong việc mở rộng kích thước ngữ cảnh của các mô hình không được tinh chỉnh so với PI [9]. Tuy nhiên, một nhược điểm chính của phương pháp này là vì nó không chỉ là một sơ đồ nội suy, một số chiều được ngoại suy nhẹ đến các giá trị "ngoài giới hạn", do đó việc tinh chỉnh với nội suy "NTK-aware" [6] cho kết quả kém hơn so với PI [9]. Hơn nữa, do các giá trị "ngoài giới hạn", hệ số tỷ lệ lý thuyết s không mô tả chính xác tỷ lệ mở rộng ngữ cảnh thực sự. Trong thực tế, giá trị tỷ lệ s phải được đặt cao hơn tỷ lệ mong đợi cho một việc mở rộng độ dài ngữ cảnh nhất định.

Chúng tôi lưu ý rằng ngay trước khi phát hành bài báo này, Code Llama [31] đã được phát hành và sử dụng tỷ lệ hóa "NTK-aware" bằng cách tỷ lệ hóa thủ công cơ số b thành 1M.

3.2 Mất mát khoảng cách địa phương tương đối - nội suy "NTK-by-parts"
Trong trường hợp các phương pháp nội suy mù như PI và nội suy "NTK-aware", chúng tôi đối xử với tất cả các chiều ẩn RoPE một cách bình đẳng (như trong chúng có cùng tác động lên mạng). Tuy nhiên, có những manh mối mạnh mẽ chỉ ra chúng tôi cần các phương pháp nội suy có mục tiêu.

Trong phần này, chúng tôi suy nghĩ nhiều về bước sóng λd được định nghĩa trong Eq. 13 trong công thức của RoPE. Để đơn giản, chúng tôi bỏ qua chỉ số d trong λd và khuyến khích người đọc nghĩ về λ như bước sóng của một hàm tuần hoàn tùy ý.

Một quan sát thú vị của các embedding RoPE là cho một kích thước ngữ cảnh L, có một số chiều d nơi bước sóng dài hơn độ dài ngữ cảnh tối đa được nhìn thấy trong quá trình tiền huấn luyện (λ > L), điều này gợi ý rằng các embedding của một số chiều có thể không được phân bố đều trong miền xoay. Trong những trường hợp như vậy, chúng tôi giả định rằng việc có tất cả các cặp vị trí duy nhất ngụ ý rằng thông tin vị trí tuyệt đối vẫn nguyên vẹn. Ngược lại, khi bước sóng ngắn, chỉ có thông tin vị trí tương đối được truy cập vào mạng.

Hơn nữa, khi chúng tôi kéo dài tất cả các chiều RoPE bằng một tỷ lệ s hoặc sử dụng thay đổi cơ số b′, tất cả các token trở nên gần nhau hơn, vì tích vô hướng của hai vector được xoay bởi một lượng nhỏ hơn lớn hơn. Việc tỷ lệ hóa này làm suy giảm nghiêm trọng khả năng hiểu các mối quan hệ nhỏ và địa phương giữa các embedding nội bộ của LLM. Chúng tôi giả thuyết rằng sự nén như vậy dẫn đến việc mô hình bị nhầm lẫn về thứ tự vị trí của các token gần nhau, và do đó làm hại khả năng của mô hình.

Để khắc phục vấn đề này, cho hai quan sát trước đó mà chúng tôi đã tìm thấy, chúng tôi chọn không nội suy các chiều tần số cao hơn trong khi luôn nội suy các chiều tần số thấp hơn. Cụ thể,

• nếu bước sóng λ nhỏ hơn nhiều so với kích thước ngữ cảnh L, chúng tôi không nội suy;
• nếu bước sóng λ bằng hoặc lớn hơn kích thước ngữ cảnh L, chúng tôi chỉ muốn nội suy và tránh mọi ngoại suy (không giống phương pháp "NTK-aware" trước đó);
• các chiều ở giữa có thể có một chút cả hai, tương tự như nội suy "NTK-aware".

Kết quả là, việc giới thiệu tỷ lệ r=L/λ giữa kích thước ngữ cảnh gốc L và bước sóng λ thuận tiện hơn. Trong trạng thái ẩn thứ d, tỷ lệ r phụ thuộc vào d theo cách sau:

r(d) =Lλd=L2πb′2d|D|. (17)

Để định nghĩa ranh giới của các chiến lược nội suy khác nhau như trên, chúng tôi giới thiệu hai tham số bổ sung α, β. Tất cả các chiều ẩn d nơi r(d)< α là những chiều nơi chúng tôi nội suy tuyến tính bằng một tỷ lệ s (giống hệt như PI, tránh mọi ngoại suy), và d nơi r(d)> β là những chiều nơi chúng tôi không nội suy gì cả. Định nghĩa hàm ramp γ là

γ(r) =

0, nếu r < α
1, nếu r > β
r−αβ−α, nếu không. (18)

Với sự hỗ trợ của hàm ramp, phương pháp "NTK-by-parts" có thể được mô tả như sau.

Định nghĩa 2 Nội suy "NTK-by-parts" là một sự sửa đổi của RoPE bằng cách sử dụng Eq. 12 với các hàm sau.

g(m) =m (19)
h(θd) =1−γr(d)θds+γr(d)θd. (20)

Các giá trị của α và β nên được điều chỉnh theo từng trường hợp cụ thể. Ví dụ, chúng tôi đã tìm thấy một cách thực nghiệm rằng đối với họ mô hình Llama, các giá trị tốt cho α và β là α= 1 và β= 32.

Sử dụng các kỹ thuật được mô tả trong phần này, một biến thể của phương pháp kết quả đã được phát hành dưới tên nội suy "NTK-by-parts" [7]. Phương pháp cải tiến này hoạt động tốt hơn so với các phương pháp nội suy PI [9] và "NTK-aware" 3.1 trước đó, cả với các mô hình không được tinh chỉnh và với các mô hình được tinh chỉnh, như được hiển thị trong [7].

3.3 Dynamic Scaling - nội suy "Dynamic NTK"
Trong rất nhiều trường hợp sử dụng, nhiều lần truyền tiến được thực hiện với độ dài chuỗi khác nhau từ 1 đến kích thước ngữ cảnh tối đa. Một ví dụ điển hình là việc sinh tự động hồi quy nơi độ dài chuỗi tăng lên 1 sau mỗi bước. Có hai cách để áp dụng một phương pháp nội suy sử dụng hệ số tỷ lệ s (bao gồm PI, "NTK-aware" và "NTK-by-parts"):

1. Trong toàn bộ chu kỳ suy luận, lớp embedding được cố định bao gồm hệ số tỷ lệ s=L′/L nơi L′ là số cố định của kích thước ngữ cảnh mở rộng.

2. Trong mỗi lần truyền tiến, embedding vị trí cập nhật hệ số tỷ lệ s=max(1, l′/L) nơi l′ là độ dài chuỗi của chuỗi hiện tại.

Vấn đề của (1) là mô hình có thể trải qua giảm hiệu suất ở độ dài nhỏ hơn L và sự suy giảm đột ngột khi độ dài chuỗi dài hơn L′. Nhưng bằng cách thực hiện Dynamic Scaling như (2), nó cho phép mô hình suy giảm một cách duyên dáng thay vì ngay lập tức bị hỏng khi đạt giới hạn ngữ cảnh được huấn luyện L′. Chúng tôi gọi phương pháp thời gian suy luận này là phương pháp Dynamic Scaling. Khi nó được kết hợp với nội suy "NTK-aware", chúng tôi gọi nó là nội suy "Dynamic NTK". Nó lần đầu tiên xuất hiện công khai như một bài đăng reddit trong [14].

Một sự thật đáng chú ý là nội suy "Dynamic NTK" hoạt động cực kỳ tốt trên các mô hình được tiền huấn luyện trên L mà không cần tinh chỉnh gì (L′=L). Điều này được hỗ trợ bởi thí nghiệm trong Phụ lục B.3.

Thường trong các lần truyền tiến lặp lại, kv-caching [8] được áp dụng để chúng ta có thể tái sử dụng các vector key-value trước đó và cải thiện hiệu quả tổng thể. Chúng tôi chỉ ra rằng trong một số triển khai khi các embedding RoPE được cached, cần phải chú ý một số điều để sửa đổi nó cho Dynamic Scaling với kv-caching. Việc triển khai đúng nên cache các kv-embeddings trước khi áp dụng RoPE, vì embedding RoPE của mỗi token thay đổi khi s thay đổi.

3.4 YaRN
Ngoài các kỹ thuật nội suy trước đó, chúng tôi cũng quan sát rằng việc giới thiệu một nhiệt độ t trên các logits trước softmax attention có tác động đồng đều lên độ phức tạp (perplexity) bất kể mẫu dữ liệu và vị trí token trên cửa sổ ngữ cảnh mở rộng (Xem Phụ lục A.2). Cụ thể hơn, thay vì Eq. 2, chúng tôi sửa đổi tính toán trọng số attention thành

softmax qTmknt√|D|!. (21)

Việc tham số hóa lại RoPE như một tập hợp các ma trận 2D có lợi ích rõ ràng trong việc triển khai tỷ lệ hóa attention này: thay vào đó chúng tôi có thể sử dụng một thủ thuật "tỷ lệ hóa độ dài" để tỷ lệ hóa cả qm và kn bằng một hệ số không đổi √1/t bằng cách đơn giản tỷ lệ hóa các embedding RoPE phức bằng cùng một lượng. Với điều này, YaRN có thể thay đổi hiệu quả cơ chế attention mà không cần sửa đổi mã của nó. Hơn nữa, nó có chi phí bằng không trong cả suy luận và huấn luyện, vì các embedding RoPE được tạo ra trước và được tái sử dụng cho tất cả các lần truyền tiến. Kết hợp nó với nội suy "NTK-by-parts", chúng tôi có phương pháp YaRN.

Định nghĩa 3 Bằng "phương pháp YaRN", chúng tôi đề cập đến một sự kết hợp của việc tỷ lệ hóa attention trong Eq. 21 và nội suy "NTK-by-parts" được giới thiệu trong Phần 3.2.

Đối với các mô hình LLaMA và Llama 2, chúng tôi khuyến nghị các giá trị sau:
√1t= 0.1 ln(s) + 1. (22)

Phương trình trên được tìm thấy bằng cách khớp √1/t tại độ phức tạp thấp nhất đối với việc mở rộng tỷ lệ bởi các hệ số s khác nhau sử dụng phương pháp "NTK-by-parts" (Phần 3.2) trên các mô hình LLaMA 7b, 13b, 33b và 65b mà không cần tinh chỉnh. Chúng tôi lưu ý rằng cùng các giá trị của t cũng áp dụng khá tốt cho các mô hình Llama 2 (7b, 13b và 70b). Điều này gợi ý rằng tính chất của việc tăng entropy và hằng số nhiệt độ t có thể có một mức độ "tính phổ quát" nhất định và có thể tổng quát hóa trên một số mô hình và dữ liệu huấn luyện.

Phương pháp YaRN kết hợp tất cả các phát hiện của chúng tôi và vượt qua tất cả các phương pháp trước đó trong cả các tình huống được tinh chỉnh và không được tinh chỉnh. Nhờ vào dấu chân thấp của nó, YaRN cho phép tương thích trực tiếp với các thư viện sửa đổi cơ chế attention như Flash Attention 2 [13].

4 Thí nghiệm
Chúng tôi chỉ ra rằng YaRN thành công đạt được việc mở rộng cửa sổ ngữ cảnh của các mô hình ngôn ngữ sử dụng RoPE làm embedding vị trí. Hơn nữa, kết quả này được đạt được chỉ với 400 bước huấn luyện, đại diện cho khoảng 0,1% của kho dữ liệu tiền huấn luyện gốc của mô hình, giảm 10 lần so với Rozière et al. [31] và giảm 2,5 lần trong các bước huấn luyện so với Chen et al. [9], làm cho nó rất hiệu quả về tính toán để huấn luyện mà không có chi phí suy luận bổ sung. Chúng tôi tính toán độ phức tạp của các tài liệu dài và chấm điểm trên các benchmark đã được thiết lập để đánh giá các mô hình kết quả, thấy rằng chúng vượt qua tất cả các phương pháp mở rộng cửa sổ ngữ cảnh khác.

Chúng tôi đã tuân theo rộng rãi các quy trình huấn luyện và đánh giá như được nêu trong [9].

4.1 Huấn luyện
Để huấn luyện, chúng tôi đã mở rộng các mô hình Llama 2 [39] 7B và 13B tham số. Không có thay đổi nào được thực hiện đối với kiến trúc mô hình LLaMA ngoại trừ việc tính toán tần số embedding như được mô tả trong 3.4 với s= 16 và s= 32.

Chúng tôi đã sử dụng tốc độ học 2×10−5 mà không có suy giảm trọng số và một warmup tuyến tính của 20 bước cùng với AdamW [24] β1= 0.9 và β2= 0.95. Đối với s= 16, chúng tôi đã tinh chỉnh trong 400 bước với kích thước batch toàn cục 64 sử dụng PyTorch [26] Fully Sharded Data Parallelism [42] và Flash Attention 2 [13] trên bộ dữ liệu PG19 [29] được chia thành các phân đoạn 64k được bao quanh bởi token BOS và EOS. Đối với s= 32, chúng tôi đã tuân theo cùng một quy trình, nhưng bắt đầu từ checkpoint s= 16 đã hoàn thành và huấn luyện thêm 200 bước.

4.2 Ngoại suy và Học chuyển giao
Trong Code Llama [31], một bộ dữ liệu với ngữ cảnh 16k đã được sử dụng với hệ số tỷ lệ được đặt thành s≈88.6, tương ứng với kích thước ngữ cảnh 355k. Họ chỉ ra rằng mạng ngoại suy lên đến ngữ cảnh 100k mà không bao giờ nhìn thấy những kích thước ngữ cảnh đó trong quá trình huấn luyện. Tương tự như 3.1 và Rozière et al. [31], YaRN cũng hỗ trợ huấn luyện với hệ số tỷ lệ s cao hơn so với độ dài của bộ dữ liệu. Do hạn chế về tính toán, chúng tôi chỉ kiểm tra s= 32 bằng cách tinh chỉnh thêm mô hình s= 16 trong 200 bước sử dụng cùng bộ dữ liệu với ngữ cảnh 64k.

Chúng tôi chỉ ra trong 4.3.1 rằng mô hình s= 32 ngoại suy thành công lên đến ngữ cảnh 128k chỉ sử dụng ngữ cảnh 64k trong quá trình huấn luyện. Không giống như các phương pháp nội suy "mù" trước đó, YaRN hiệu quả hơn nhiều trong học chuyển giao khi tăng tỷ lệ s. Điều này chứng minh việc học chuyển giao thành công từ s= 16 đến s= 32 mà không cần mạng phải học lại các embedding được nội suy, vì mô hình s= 32 tương đương với mô hình s= 16 trên toàn bộ kích thước ngữ cảnh, mặc dù chỉ được huấn luyện trên s= 32 trong 200 bước.

4.3 Đánh giá
Các đánh giá tập trung vào ba khía cạnh:

1. điểm độ phức tạp của các mô hình được tinh chỉnh với cửa sổ ngữ cảnh mở rộng,
2. nhiệm vụ truy xuất passkey trên các mô hình được tinh chỉnh,
3. kết quả benchmark LLM phổ biến của các mô hình được tinh chỉnh,

4.3.1 Mô hình hóa ngôn ngữ chuỗi dài
Để đánh giá hiệu suất mô hình hóa ngôn ngữ chuỗi dài, chúng tôi sử dụng các bộ dữ liệu GovReport [18] và Proof-pile [4] cả hai đều chứa nhiều mẫu chuỗi dài. Đối với tất cả các đánh giá, các phần test của cả hai bộ dữ liệu đều được sử dụng độc quyền. Tất cả các đánh giá độ phức tạp đều được tính toán sử dụng phương pháp cửa sổ trượt từ Press et al. [27] với S= 256.

Đầu tiên, chúng tôi đánh giá cách mô hình hoạt động khi cửa sổ ngữ cảnh tăng lên. Chúng tôi đã chọn 10 mẫu ngẫu nhiên từ Proof-pile với ít nhất 128k token mỗi mẫu và đánh giá độ phức tạp của mỗi mẫu này khi bị cắt ngắn tại các bước 2k từ độ dài chuỗi 2k token đến 128k token.

Bảng 1 cho thấy so sánh cạnh nhau của mô hình Llama-2 được mở rộng từ 4096 đến 8192 độ dài ngữ cảnh thông qua PI (LLongMA-2 7b), "NTK-aware" và YaRN. Lưu ý rằng các mô hình PI và "NTK-aware" đã được huấn luyện sử dụng phương pháp luận trong Chen et al. [9], trong khi YaRN đã sử dụng cùng phương pháp luận nhưng ít hơn 2,5 lần các bước huấn luyện và dữ liệu, như được mô tả trong 4.

Chúng tôi đã đánh giá thêm YaRN tại hệ số tỷ lệ s= 16,32 và so sánh chúng với một vài mô hình mã nguồn mở được tinh chỉnh từ Llama-2 và mở rộng đến hơn 32k cửa sổ ngữ cảnh như Together.ai [37] và Code Llama "NTK-aware" [31]. Kết quả được tóm tắt trong Bảng 2 (với biểu đồ chi tiết hơn trong Hình 1).

Chúng tôi quan sát thấy rằng mô hình thể hiện hiệu suất mạnh mẽ trên toàn bộ kích thước ngữ cảnh mục tiêu, với nội suy YaRN là phương pháp đầu tiên thành công mở rộng kích thước ngữ cảnh hiệu quả của Llama 2 đến 128k. Đáng chú ý đặc biệt là các mô hình YaRN (s= 32), cho thấy độ phức tạp tiếp tục giảm qua 128k, mặc dù dữ liệu tinh chỉnh bị giới hạn ở 64k token về độ dài, chứng minh rằng mô hình có thể tổng quát hóa đến độ dài ngữ cảnh chưa được nhìn thấy.

Hơn nữa, trong Phụ lục B.1, chúng tôi cho thấy kết quả của độ phức tạp trung bình trên 50 tài liệu GovReport không bị cắt ngắn với ít nhất 16k token cho mỗi mẫu được đánh giá trên cài đặt cửa sổ ngữ cảnh tối đa 32k mà không có Dynamic Scaling trong Bảng 4. Tương tự như kết quả Proof-pile, kết quả GovReport cho thấy rằng việc tinh chỉnh với YaRN đạt được hiệu suất tốt trên các chuỗi dài.

4.3.2 Truy xuất Passkey
Nhiệm vụ truy xuất passkey như được định nghĩa trong [25] đo lường khả năng truy xuất một passkey đơn giản (tức là một số năm chữ số) của mô hình từ giữa một lượng lớn văn bản vô nghĩa khác. Để đánh giá các mô hình, chúng tôi đã thực hiện 10 lần lặp lại nhiệm vụ truy xuất passkey với passkey được đặt tại một vị trí ngẫu nhiên được phân bố đều trên cửa sổ ngữ cảnh đánh giá trên các kích thước cửa sổ ngữ cảnh khác nhau từ 8k đến 128k. Cả hai mô hình 7b và 13b được tinh chỉnh sử dụng YaRN tại kích thước ngữ cảnh 128k đều vượt qua nhiệm vụ truy xuất passkey với độ chính xác rất cao (>99%) trong toàn bộ kích thước cửa sổ ngữ cảnh. Chúng tôi cho thấy kết quả chi tiết trong Phụ lục B.2.

4.3.3 Benchmark chuẩn hóa
Hugging Face Open LLM Leaderboard [19] so sánh vô số LLM trên một bộ chuẩn hóa gồm bốn benchmark công cộng. Cụ thể, chúng tôi sử dụng 25-shot ARC-Challenge [11], 10-shot HellaSwag [41], 5-shot MMLU [17], và 0-shot TruthfulQA [23].

Để kiểm tra sự suy giảm hiệu suất mô hình dưới việc mở rộng ngữ cảnh, chúng tôi đã đánh giá các mô hình của mình sử dụng bộ này và so sánh nó với các điểm số đã được thiết lập cho các baseline Llama 2 cũng như các mô hình PI và "NTK-aware" có sẵn công khai. Kết quả được tóm tắt trong Bảng 3.

Chúng tôi quan sát thấy rằng có sự suy giảm hiệu suất tối thiểu giữa các mô hình YaRN và các baseline Llama 2 tương ứng của chúng. Chúng tôi cũng quan sát thấy rằng có trung bình 0,49% giảm điểm số giữa các mô hình YaRN s= 16 và s= 32. Từ đây chúng tôi kết luận rằng việc mở rộng lặp lại từ 64k đến 128k dẫn đến mất mát hiệu suất không đáng kể.

5 Kết luận
Tóm lại, chúng tôi đã chỉ ra rằng YaRN cải thiện so với tất cả các phương pháp nội suy RoPE hiện có và có thể hoạt động như một thay thế drop-in cho PI, mà không có nhược điểm và nỗ lực triển khai tối thiểu. Các mô hình được tinh chỉnh duy trì khả năng gốc của chúng trên nhiều benchmark trong khi có thể chú ý đến kích thước ngữ cảnh rất lớn. Hơn nữa, YaRN cho phép ngoại suy hiệu quả với việc tinh chỉnh trên các bộ dữ liệu ngắn hơn và có thể tận dụng học chuyển giao để hội tụ nhanh hơn, cả hai đều quan trọng dưới các tình huống hạn chế về tính toán. Cuối cùng, chúng tôi đã chỉ ra hiệu quả của ngoại suy với YaRN nơi nó có thể "huấn luyện ngắn và kiểm tra dài".

--- TRANG 11 ---
6 Khả năng tái tạo
Để hỗ trợ khả năng tái tạo, chúng tôi cung cấp, như tài liệu bổ sung, toàn bộ mã được sử dụng để huấn luyện các mô hình YaRN trong Bảng 2, cũng như mã đánh giá tạo ra Hình 1 và Bảng 1, 2, 3, 4, và 5. Mã cũng chứa các triển khai của các phương pháp mở rộng khác nhau được tham chiếu trong bài báo. Để huấn luyện YaRN, chúng tôi đã sử dụng bộ dữ liệu PG19 [29] có sẵn công khai được token hóa thành 64k token.

Tài liệu tham khảo
[1] Mistrallite. URL https://huggingface.co/amazon/MistralLite .
[2] Giới thiệu Qwen-7B: Các mô hình nền tảng mở và căn chỉnh con người (của state-of-the-arts). URL https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md .
[3] Bộ sưu tập dữ liệu dài. URL https://huggingface.co/datasets/togethercomputer/Long-Data-Collections .
[4] Z. Azerbayev, E. Ayers, , và B. Piotrowski. Proof-pile, 2022. URL https://github.com/zhangir-azerbayev/proof-pile .
[5] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, và S. Weinbach. GPT-NeoX-20B: Một mô hình ngôn ngữ tự động hồi quy mã nguồn mở, 2022. arXiv: 2204.06745.
[6] bloc97. NTK-Aware Scaled RoPE cho phép các mô hình LLaMA có kích thước ngữ cảnh mở rộng (8k+) mà không cần tinh chỉnh gì và suy giảm perplexity tối thiểu., 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ .
[7] bloc97. Thêm hiệu chỉnh nội suy NTK-Aware "by parts", 2023. URL https://github.com/jquesnelle/scaled-rope/pull/1 .
[8] C. Chen. Transformer Inference Arithmetic, 2022. URL https://kipp.ly/blog/transformer-inference-arithmetic/ .
[9] S. Chen, S. Wong, L. Chen, và Y. Tian. Mở rộng cửa sổ ngữ cảnh của các mô hình ngôn ngữ lớn thông qua nội suy vị trí, 2023. arXiv: 2306.15595.
[10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, và N. Fiedel. PaLM: Tỷ lệ hóa mô hình ngôn ngữ với pathways, 2022. arXiv: 2204.02311.
[11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, và O. Tafjord. Bạn nghĩ mình đã giải quyết việc trả lời câu hỏi? hãy thử ARC, AI2 Reasoning Challenge, 2018. arXiv: 1803.05457.
[12] T. Computer. Redpajama: Một công thức mã nguồn mở để tái tạo bộ dữ liệu huấn luyện llama, 2023. URL https://github.com/togethercomputer/RedPajama-Data .
[13] T. Dao. Flashattention-2: Attention nhanh hơn với tính song song tốt hơn và phân chia công việc, 2023. arXiv: 2307.08691.
[14] emozilla. Dynamically Scaled RoPE tăng thêm hiệu suất của LLaMA ngữ cảnh dài với zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/ .
[15] J. Gehring, M. Auli, D. Grangier, D. Yarats, và Y. N. Dauphin. Học chuỗi sang chuỗi tích chập, 2017. arXiv: 1705.03122.
[16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, và S. Wang. LM-Infinite: Tổng quát hóa độ dài on-the-fly đơn giản cho các mô hình ngôn ngữ lớn, 2023. arXiv: 2308.16137.
[17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, và J. Steinhardt. Đo lường hiểu biết ngôn ngữ đa nhiệm vụ khổng lồ. Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[18] L. Huang, S. Cao, N. Parulian, H. Ji, và L. Wang. Attention hiệu quả cho tóm tắt tài liệu dài. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1419–1436. Association for Computational Linguistics, tháng 6 năm 2021.
[19] Hugging Face. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard .
[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, và W. E. Sayed. Mistral 7b, 2023.
[21] kaiokendev. Những điều tôi đang học khi huấn luyện superhot., 2023. URL https://kaiokendev.github.io/til#extending-context-to-8k .
[22] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, và S. Reddy. Tác động của mã hóa vị trí lên tổng quát hóa độ dài trong transformers, 2023. arXiv: 2305.19466.
[23] S. Lin, J. Hilton, và O. Evans. TruthfulQA: Đo lường cách các mô hình bắt chước những sai lầm của con người. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 3214–3252, tháng 5 năm 2022.
[24] I. Loshchilov và F. Hutter. Regularization suy giảm trọng số tách rời. Trong International Conference on Learning Representations, 2019.
[25] A. Mohtashami và M. Jaggi. Landmark attention: Truy cập ngẫu nhiên độ dài ngữ cảnh vô hạn cho transformers, 2023. arXiv: 2305.16300.
[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, và S. Chintala. PyTorch: Một thư viện học sâu hiệu suất cao, kiểu mệnh lệnh. Trong NeurIPS, trang 8024–8035, 2019.
[27] O. Press, N. Smith, và M. Lewis. Huấn luyện ngắn, kiểm tra dài: Attention với các bias tuyến tính cho phép ngoại suy độ dài đầu vào. Trong International Conference on Learning Representations, 2022.
[28] J. Quesnelle, E. Shippole, và "Kaiokendev". Llongma: Tỷ lệ hóa rotary embeddings thông qua nội suy vị trí tuyến tính. https://huggingface.co/conceptofmind/LLongMA-2-7b/, 2023.
[29] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, và T. P. Lillicrap. Transformers nén cho mô hình hóa chuỗi tầm xa. Trong International Conference on Learning Representations, 2020.
[30] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, và Y. Zhou. Khám phá giới hạn của học chuyển giao với một transformer text-to-text thống nhất. Báo cáo kỹ thuật, Google, 2019.
[31] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, và G. Synnaeve. Code Llama: Các mô hình nền tảng mở cho mã, 2023. arXiv: 2308.12950.
[32] P. Shaw, J. Uszkoreit, và A. Vaswani. Self-attention với các biểu diễn vị trí tương đối. Trong Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), trang 464–468, New Orleans, Louisiana, tháng 6 năm 2018. Association for Computational Linguistics.
[33] J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.
[34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, và Y. Liu. RoFormer: Transformer tăng cường với rotary position embedding, 2022. arXiv: 2104.09864.
[35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, và F. Wei. Một transformer có thể ngoại suy độ dài, 2022. arXiv: 2212.10554.
[36] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. T. Barron, và R. Ng. Fourier features cho phép mạng học các hàm tần số cao trong miền chiều thấp. Trong Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
[37] Together.ai. LLaMA-2-7B-32K, 2023. URL https://huggingface.co/togethercomputer/LLaMA-2-7B-32K .
[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, và G. Lample. LLaMA: Các mô hình ngôn ngữ nền tảng mở và hiệu quả, 2023. arXiv: 2302.13971.
[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, và T. Scialom. Llama 2: Các mô hình chat nền tảng mở và được tinh chỉnh, 2023.
[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin. Attention is all you need. Trong Advances in Neural Information Processing Systems, tập 30. Curran Associates, Inc., 2017.
[41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, và Y. Choi. HellaSwag: Liệu máy có thể thực sự hoàn thành câu của bạn? Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.
[42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, và S. Li. PyTorch FSDP: Trải nghiệm về tỷ lệ hóa dữ liệu song song được chia sẻ đầy đủ, 2023. arXiv: 2304.11277.

A Chi tiết bổ sung về các phương pháp nội suy

A.1 Ghi chú ngắn về suy diễn của nội suy "NTK-aware"
Trong Phần 3.1, chúng tôi giới thiệu một sự thay đổi cơ sở từ b đến b′ trong định nghĩa của phương pháp nội suy "NTK-aware". Đây là một ghi chú ngắn về suy diễn toán học của nó.

Nhớ lại rằng mục tiêu của chúng tôi là phân tán áp lực nội suy trên các chiều ẩn sử dụng thay đổi cơ sở thay vì tỷ lệ hóa các tần số bằng một hệ số cố định s. Tính chất chúng tôi muốn đảm bảo là: Tần số thấp nhất cần được tỷ lệ hóa nhiều như tỷ lệ hóa vị trí tuyến tính và tần số cao nhất để giữ nguyên.

Chúng tôi giới thiệu một cơ sở mới b′ sao cho chiều cuối cùng khớp với bước sóng của nội suy tuyến tính với hệ số tỷ lệ s. Vì phương pháp RoPE gốc bỏ qua các chiều lẻ để nối cả thành phần cos(2πx/λ) và sin(2πx/λ) vào một embedding duy nhất, chiều cuối cùng d∈D là |D| −2.

Cơ sở mới b′ có thể được chọn để
b′|D|−2|D|=s·b|D|−2|D|. (23)

Giải b′ thu được
b′=b·s|D||D|−2. (24)

A.2 Tác động của tỷ lệ hóa pre-softmax của YaRN lên perplexity
Trong Phần 3.4, chúng tôi đề cập đến tác động của hệ số t bên trong tính toán softmax của trọng số attention. Ở đây chúng tôi cố định 896 tài liệu 16 k-token từ RedPajama [12], và tính toán điểm perplexity của chúng với tỷ lệ hóa khác nhau 1/√t. Kết quả trong Hình 2. Để so sánh, nhớ lại rằng hệ số khuyến nghị của chúng tôi trong trường hợp này (s= 8) được cho bởi sau.

√1t= 0.1 ln(s) + 1≈1.208. (25)

Để hiển thị tác động của hệ số 1/√t lên các vị trí token khác nhau, chúng tôi cắt mỗi tài liệu 16k-token thành các khối 2048 token, và vẽ thêm biểu đồ thay đổi perplexity trung bình so với t= 1 theo phần trăm

ppl(t)−ppl(t= 1)ppl(t= 1) (26)

của mỗi khối. Biểu đồ được hiển thị trong Hình 3.

Để chứng minh thêm các giá trị tốt nhất của t trên tất cả các mẫu qua các vị trí token khác nhau, chúng tôi vẽ biểu đồ số lượng mẫu với perplexity tối thiểu tại một 1/√t nhất định cho mỗi trong 8 phân đoạn vị trí qua phạm vi 16k-token trong Hình 4.

Chúng tôi quan sát thấy rằng:
• đối với một t phù hợp, một mẫu có thể đạt được điểm perplexity tốt hơn trên cửa sổ ngữ cảnh mở rộng;
• giá trị tốt nhất của t chủ yếu nhất quán trên các mẫu khác nhau và các vị trí khác nhau.

Chúng tôi nhận xét rằng phát hiện này nhất quán cho các giá trị s khác nhau và giá trị tốt nhất của t tuân theo công thức khuyến nghị của chúng tôi (Eq. 22) một cách chặt chẽ.

B Bảng và biểu đồ bổ sung

B.1 Đánh giá GovReport
Trong Phần 4.3.1, chúng tôi đề cập đến đánh giá trên các tài liệu GovReport. Kết quả đánh giá được chi tiết trong Bảng 4 dưới đây.

B.2 Truy xuất Passkey
Ở đây chúng ta có thể quan sát thấy rằng điểm perplexity thấp nhất một mình không cung cấp mô tả toàn diện về "kích thước ngữ cảnh hiệu quả" mà một LLM có thể chú ý đến. Trong khi mô hình Code Llama 13b thể hiện perplexity tăng trên độ dài ngữ cảnh 100k, nó vẫn có thể truy xuất chính xác passkey tại độ dài ngữ cảnh 128k. Điều này gợi ý rằng trong khi đầu ra của Code Llama có thể bắt đầu suy giảm chất lượng trên kích thước ngữ cảnh 100k, nó vẫn có thể duy trì khả năng truy xuất mạnh mẽ.

Ngoài ra, vì YaRN với s= 32 được huấn luyện thêm 200 bước so với YaRN với s= 16 trong khi có độ chính xác passkey cao hơn với perplexity tương tự, chúng tôi giả thuyết rằng perplexity có thể không phải là chỉ báo tốt về việc liệu một LLM có thể chú ý đến tất cả các token hay không và không xác định một cách đầy đủ hiệu suất ngữ cảnh dài. Điều này cũng gợi ý rằng các mô hình YaRN với s= 16 có thể tương đối thiếu huấn luyện cho nhiệm vụ truy xuất passkey.

B.3 Dynamic scaling trên các mô hình mà không cần tinh chỉnh gì
Chúng tôi trước tiên nhớ lại từ Phần 3.3 rằng kỹ thuật Dynamic Scaling là một kỹ thuật thời gian suy luận động cập nhật hệ số s trong các phương pháp nội suy như PI, "NTK-by-parts" và YaRN. Chúng tôi chọn Llama 2 gốc, cố định một mẫu trong GovReport và tính toán perplexity của nó trên cửa sổ trượt 256 token sử dụng RoPE, Dynamic-PI và Dynamic-YaRN. Vì độ dài ngữ cảnh tối đa gốc của Llama 2 là 4096, chúng tôi quan sát thấy rằng Dynamic Scaling hiệu quả mở rộng độ dài suy luận và Dynamic-YaRN đạt được hiệu suất tốt hơn Dynamic-PI. Biểu đồ kết quả trong Hình 5.

Chúng ta thấy rằng
• Dynamic Scaling hiệu quả ngăn chặn sự bùng nổ của điểm perplexity vượt ra ngoài cửa sổ ngữ cảnh được tiền huấn luyện;
• Dynamic-YaRN vượt trội hơn Dynamic-PI về perplexity tầm xa trên Llama-2 được tiền huấn luyện mà không cần tinh chỉnh gì.

B.4 Mistral
Chúng tôi đã mở rộng thêm mô hình Mistral 7B v0.1 [20], mô hình này theo rộng rãi kiến trúc Llama. Đối với Mistral, chúng tôi đã huấn luyện một mô hình cửa sổ ngữ cảnh 64k (s= 8) trong 1000 bước sử dụng độ dài chuỗi 16k với tốc độ học không đổi 1×10−6. Kích thước cửa sổ attention trượt của mô hình được đặt thành kích thước cửa sổ ngữ cảnh, hiệu quả vô hiệu hóa cửa sổ attention trượt. Sau đó chúng tôi huấn luyện thêm 500 bước tại s= 16 để đạt được một mô hình cửa sổ ngữ cảnh 128k. Dữ liệu huấn luyện là một hỗn hợp của các phần pre-train và fine-tune của Long-Data Collections của Together Computer [3].

Chúng tôi đã đánh giá các mô hình theo cùng quy trình như được mô tả trong 4.3.1, so sánh với mô hình v0.1 cơ sở và MistralLite [1], một phiên bản NTK-aware (θ= 1M) của v0.1. Kết quả (Hình 6 và Bảng 6) nhất quán với những kết quả của họ mô hình Llama.
