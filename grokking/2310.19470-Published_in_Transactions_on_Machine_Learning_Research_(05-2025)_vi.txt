# 2310.19470.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/grokking/2310.19470.pdf
# Kích thước tệp: 24435044 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
Kết nối Lottery Ticket và Grokking:
Hiểu về Grokking từ Cấu trúc Bên trong của Mạng
Gouki Minegishi Yusuke Iwasawa Yutaka Matsuo
{minegishi,iwasawa,matsuo}@weblab.t.u-tokyo.ac.jp
Đại học Tokyo
Được đánh giá trên OpenReview: https: // openreview. net/ forum? id= eQeYyup1tm
Tóm tắt
Grokking là một hiện tượng thú vị của việc khái quát hóa bị trì hoãn, trong đó các mạng neural ban đầu ghi nhớ dữ liệu huấn luyện với độ chính xác hoàn hảo nhưng thể hiện khả năng khái quát hóa kém, sau đó chuyển sang một giải pháp khái quát hóa với việc tiếp tục huấn luyện. Trong khi các yếu tố như norm trọng số và độ thưa thớt đã được đề xuất để giải thích việc khái quát hóa bị trì hoãn này, ảnh hưởng của cấu trúc mạng vẫn chưa được khám phá đầy đủ. Trong nghiên cứu này, chúng tôi liên kết hiện tượng grokking với giả thuyết lottery ticket để điều tra tác động của cấu trúc mạng bên trong. Chúng tôi chứng minh rằng việc sử dụng các lottery ticket thu được trong giai đoạn khái quát hóa (được gọi là grokked tickets) làm giảm đáng kể việc khái quát hóa bị trì hoãn trên các tác vụ khác nhau, bao gồm nhiều phép toán số học modular, hồi quy đa thức, sparse parity, và phân loại MNIST. Thông qua các thí nghiệm có kiểm soát, chúng tôi cho thấy rằng việc giảm thiểu khái quát hóa bị trì hoãn không chỉ do giảm norm trọng số hoặc tăng độ thưa thớt, mà là do việc khám phá ra các mạng con tốt. Hơn nữa, chúng tôi thấy rằng grokked tickets thể hiện các mẫu trọng số tuần hoàn, các thuộc tính đồ thị có lợi như tăng độ dài đường đi trung bình và giảm hệ số clustering, và trải qua những thay đổi cấu trúc nhanh chóng trùng khớp với việc cải thiện khả năng khái quát hóa. Thêm vào đó, các kỹ thuật pruning như thuật toán edge-popup có thể xác định những cấu trúc hiệu quả này mà không cần sửa đổi trọng số, từ đó biến đổi các mạng ghi nhớ thành các mạng khái quát hóa. Những kết quả này nhấn mạnh cái nhìn mới lạ rằng việc khám phá cấu trúc đóng vai trò then chốt trong việc hiểu về grokking.

1 Giới thiệu
Hiểu về cơ chế khái quát hóa là một câu hỏi trung tâm trong việc hiểu hiệu quả của mạng neural. Gần đây, Poweretal.(2022) đã tiết lộ hiện tượng thú vị của việc khái quát hóa bị trì hoãn (grokking); các mạng neural ban đầu đạt được một mạng ghi nhớ Cmem với độ chính xác huấn luyện hoàn hảo nhưng khả năng khái quát hóa kém, tuy nhiên việc huấn luyện tiếp tục sẽ chuyển đổi giải pháp sang một mạng khái quát hóa Cgen. Hiện tượng này, trái ngược với kỳ vọng tiêu chuẩn của machine learning, đang được nghiên cứu để trả lời câu hỏi: điều gì làm nền tảng cho việc chuyển đổi giữa ghi nhớ và khái quát hóa? (Liu et al., 2022; 2023a)

Về mối quan hệ giữa khái quát hóa và deep learning nói chung, đã được biết rằng cấu trúc của mạng có tác động đáng kể đến hiệu suất khái quát hóa. Chẳng hạn, hiệu suất nhận dạng hình ảnh đã được cải thiện đáng kể bằng cách tận dụng cấu trúc của convolution (Krizhevsky et al., 2012). Hơn nữa, như được thể hiện trong Neyshabur (2020), việc kết hợp regularization β-Lasso vào các MLP được kết nối đầy đủ tạo điều kiện cho sự xuất hiện của tính locality — giống với các cấu trúc trong CNN — dẫn đến hiệu suất cải thiện trong các tác vụ hình ảnh. Từ một góc nhìn hơi khác, Frankle & Carbin (2019) đã đề xuất giả thuyết lottery ticket (LTH), cho rằng các mạng con tốt (cấu trúc tốt) giúp đạt được hiệu suất tốt hơn với hiệu quả mẫu tốt hơn (Zhang et al., 2021). Ngoài ra, Ramanujan et al. (2020) cho thấy rằng việc khám phá cấu trúc đơn thuần có thể đạt được hiệu suất tương đương với việc cập nhật trọng số, gợi ý rằng các mạng con tốt là đủ để đạt được hiệu suất khái quát hóa.

--- TRANG 2 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
00.51Độ chính xác
Bước huấn luyện
00.51Độ chính xác
Bước huấn luyện(a) Mạng dày đặc
(b) Lottery Ticket (Mạng con tốt)Khái quát hóa bị trì hoãn
Không có trì hoãn!!■Quan sát: Lottery ticket loại bỏ việc khái quát hóa bị trì hoãn■Điều gì xảy ra trong quá trình khái quát hóa bị trì hoãn?
Giả thuyết1:Giảm norm
Giả thuyết2:Tìm cấu trúc tốt
Huấn luyện         Kiểm tra
00.51Độ chính xác
Bước huấn luyện
00.51Độ chính xác
Bước huấn luyện(a) Mạng dày đặc
(b) Lottery Ticket (Mạng con tốt)Khái quát hóa bị trì hoãn
Không có trì hoãn!!■Quan sát (mục 3): Lottery ticket loại bỏ việc khái quát hóa bị trì hoãn■Điều gì xảy ra trong quá trình khái quát hóa bị trì hoãn? 
Tại sao LT loại bỏ việc khái quát hóa bị trì hoãn?
H1 [Liu+]:
Giảm norm trọng số
H2 [Merrill+]:
Độ thưa thớt cao hơn
Huấn luyện         Kiểm traH3 (Của chúng tôi):
Tìm cấu trúc tốt
!!"#
nhỏ hơn"$!%"%
!!"#
nhỏ hơn"&!%"%
!!"#!%"%
cấu trúc tốt hơn
Thí nghiệm: mục 4 thí nghiệm có kiểm soát, mục 5.1 
Đo lường tiến độ sử dụng H3, mục 5.2 pruning như lực thúc đẩy 
grokking, và mục 5.3 liên kết đến biểu diễn tốt
00.51Độ chính xác
Bước huấn luyện
00.51Độ chính xác
Bước huấn luyện(a) Mạng dày đặc
(b) Lottery Ticket (Mạng con tốt)Khái quát hóa bị trì hoãn
Không có trì hoãn!!■Quan sát: Lottery ticket loại bỏ việc khái quát hóa bị trì hoãn■Điều gì xảy ra trong quá trình khái quát hóa bị trì hoãn?
Giả thuyết1:Giảm norm
Giả thuyết2:Tìm cấu trúc tốt
Huấn luyện         Kiểm tra
00.51Độ chính xác
Bước huấn luyện
00.51Độ chính xác
Bước huấn luyện(a) Mạng dày đặc
(b) Lottery Ticket (Mạng con tốt)Khái quát hóa bị trì hoãn
Không có trì hoãn!!■Quan sát (mục 3): Lottery ticket loại bỏ việc khái quát hóa bị trì hoãn■Điều gì xảy ra trong quá trình khái quát hóa bị trì hoãn? 
Tại sao LT loại bỏ việc khái quát hóa bị trì hoãn?
H1 [Liu+]:
Giảm norm trọng số
H2 [Merrill+]:
Độ thưa thớt cao hơn
Huấn luyện         Kiểm traH3 (Của chúng tôi):
Tìm cấu trúc tốt
!!"#
nhỏ hơn"$!%"%
!!"#
nhỏ hơn"&!%"%
!!"#!%"%
cấu trúc tốt hơn
Thí nghiệm: mục 4 thí nghiệm có kiểm soát, mục 5.1 
Đo lường tiến độ sử dụng H3, mục 5.2 pruning như lực thúc đẩy 
grokking, và mục 5.3 liên kết đến biểu diễn tốt
00.51Độ chính xác
Bước huấn luyện
00.51Độ chính xác
Bước huấn luyện(a) Mạng dày đặc
(b) Lottery Ticket (Mạng con tốt)Khái quát hóa bị trì hoãn
Không có trì hoãn!!■Quan sát: Lottery ticket loại bỏ việc khái quát hóa bị trì hoãn■Điều gì xảy ra trong quá trình khái quát hóa bị trì hoãn?
Giả thuyết1:Giảm norm
Giả thuyết2:Tìm cấu trúc tốt
Huấn luyện         Kiểm tra
00.51Độ chính xác
Bước huấn luyện
00.51Độ chính xác
Bước huấn luyện(a) Mạng dày đặc
(b) Lottery Ticket (Mạng con tốt)Khái quát hóa bị trì hoãn
Không có trì hoãn!!■Quan sát (mục 3): Lottery ticket loại bỏ việc khái quát hóa bị trì hoãn■Điều gì xảy ra trong quá trình khái quát hóa bị trì hoãn? 
Tại sao LT loại bỏ việc khái quát hóa bị trì hoãn?
H1 [Liu+]:
Giảm norm trọng số
H2 [Merrill+]:
Độ thưa thớt cao hơn
Huấn luyện         Kiểm traH3 (Của chúng tôi):
Tìm cấu trúc tốt
!!"#
nhỏ hơn"$!%"%
!!"#
nhỏ hơn"&!%"%
!!"#!%"%
cấu trúc tốt hơn
Thí nghiệm: mục 4 thí nghiệm có kiểm soát, mục 5.1 
Đo lường tiến độ sử dụng H3, mục 5.2 pruning như lực thúc đẩy 
grokking, và mục 5.3 liên kết đến biểu diễn tốt

Hình 1: (Trái) Độ chính xác của mô hình dày đặc và lottery ticket thu được tại giải pháp khái quát hóa (grokked ticket). Khi sử dụng lottery ticket (mạng con tốt), độ chính xác huấn luyện và kiểm tra tăng gần như tương tự, tức là thời gian từ ghi nhớ (tmem) đến khái quát hóa (tgen) đã được tăng tốc đáng kể. Lưu ý rằng không chỉ phép trừ (tgen−tmem) mà tỷ lệ (tgen/tmem) cũng được cải thiện đáng kể, có nghĩa là đây không chỉ là vấn đề học nhanh hơn. (Phải) Ba giả thuyết về lý do tại sao việc khái quát hóa bị trì hoãn được giảm với lottery ticket. Chúng tôi cho thấy rằng đó không phải do giảm norm trọng số hoặc tăng độ thưa thớt, mà là do việc khám phá cấu trúc tốt.

Mặc dù tầm quan trọng của cấu trúc được biết đến rộng rãi nói chung, mối liên hệ của nó với hiện tượng grokking chưa được điều tra đầy đủ. Tương tự như nghiên cứu của chúng tôi, một số nghiên cứu trước đây kết nối hiện tượng grokking với thuộc tính của mạng, ví dụ như norm trọng số và độ thưa thớt của mạng. Chẳng hạn, Liu et al. (2023a) đã xác nhận thực nghiệm rằng các giải pháp khái quát hóa có norm nhỏ hơn so với các giải pháp ghi nhớ. Bài báo gốc (Power et al., 2022) cho thấy rằng việc thêm weight decay trong quá trình huấn luyện là cần thiết để kích hoạt grokking. Tuy nhiên, nghiên cứu của chúng tôi khác với những nghiên cứu này bằng cách thảo luận về mối quan hệ giữa quá trình khám phá cấu trúc tốt (tức là các mạng con) và grokking thay vì chỉ đơn thuần giảm norm trọng số.

Để điều tra vai trò của cấu trúc trong hiện tượng grokking, trước tiên chúng tôi chứng minh rằng khi sử dụng lottery ticket thu được tại giải pháp khái quát hóa (được gọi là grokked tickets), việc khái quát hóa bị trì hoãn được giảm đáng kể. Hình 1 (trái) minh họa rằng độ chính xác huấn luyện và kiểm tra tăng gần như đồng thời với grokked tickets, không giống như các mạng dày đặc được khởi tạo ngẫu nhiên nơi việc khái quát hóa bị trì hoãn xảy ra. Như sẽ được thể hiện sau, kết quả này liên quan đến tỷ lệ pruning, với các tỷ lệ pruning thích hợp dẫn đến ít trì hoãn hơn. Chúng tôi đã tiến hành thêm các thí nghiệm từ nhiều góc độ để hiểu tại sao việc khái quát hóa bị trì hoãn được giảm đáng kể với grokked tickets. Trước tiên, như minh họa trong Hình 1 (phải), chúng tôi phân tách sự khác biệt giữa grokked ticket và một mạng dày đặc được khởi tạo ngẫu nhiên thành ba yếu tố: (1) norm trọng số nhỏ, (2) độ thưa thớt và (3) cấu trúc tốt. Chúng tôi điều tra yếu tố nào trong số này đóng góp vào việc giảm đáng kể việc khái quát hóa bị trì hoãn. Đối với (1) norm trọng số, chúng tôi thấy rằng các mạng dày đặc với cùng norm trọng số ban đầu như grokked ticket không khái quát hóa nhanh hơn, chỉ ra rằng norm trọng số không phải là nguyên nhân. Đối với (2) độ thưa thớt, các mạng có cùng kích thước tham số sử dụng các phương pháp pruning nổi tiếng (Wang et al., 2019; Lee et al., 2019; Tanaka et al., 2020) cũng không khái quát hóa nhanh hơn, chỉ ra rằng độ thưa thớt không phải là nguyên nhân. Những kết quả này gợi ý rằng (3) cấu trúc tốt là thiết yếu để hiểu về grokking.

Dựa trên những phát hiện này, chúng tôi đi sâu hơn vào việc hiểu bản chất và vai trò của những thuộc tính cấu trúc này trong hiện tượng grokking. Cụ thể, chúng tôi điều tra một số khía cạnh chính. Trước tiên, để xác định liệu việc thu được cấu trúc có được đồng bộ hóa với việc cải thiện hiệu suất khái quát hóa hay không, chúng tôi sử dụng khoảng cách Jaccard (Jaccard, 1901) như một metric về khoảng cách cấu trúc. Phân tích của chúng tôi cho thấy rằng cấu trúc của mạng con thay đổi nhanh chóng trong quá trình chuyển đổi từ ghi nhớ sang khái quát hóa. Sự thích ứng cấu trúc nhanh chóng này được đồng bộ hóa với việc nâng cao hiệu suất khái quát hóa. Thứ hai, chúng tôi

--- TRANG 3 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
kiểm tra cả các thuộc tính lý thuyết đồ thị, bao gồm tăng độ dài đường đi trung bình và giảm hệ số clustering, và bản chất tuần hoàn của tác vụ Modular Addition (Nanda et al., 2023). Chúng tôi thấy rằng các thuộc tính đồ thị có lợi xuất hiện và phù hợp chặt chẽ với việc cải thiện khái quát hóa, và mô hình khai thác tính tuần hoàn vốn có của tác vụ để khám phá các mạng con có cấu trúc bên trong tốt phù hợp để đạt được khái quát hóa tốt hơn. Cuối cùng, dựa trên kết quả của chúng tôi chỉ ra rằng việc khám phá cấu trúc là quan trọng cho grokking, chúng tôi chứng minh rằng pruning tạo điều kiện thuận lợi cho khái quát hóa. Chúng tôi sử dụng thuật toán edge-popup (Ramanujan et al., 2020) để xác định cấu trúc tốt trong khi giữ nguyên trọng số. Các thí nghiệm của chúng tôi cho thấy rằng mạng ghi nhớ có thể được chuyển đổi sang mạng khái quát hóa thông qua pruning mà không cần cập nhật trọng số, từ đó củng cố tuyên bố của chúng tôi rằng việc khái quát hóa bị trì hoãn xảy ra do việc khám phá cấu trúc tốt.

Tóm lại, những đóng góp của chúng tôi như sau:
•Liên kết Lottery Tickets và Grokking (mục 3): Chúng tôi điều tra vai trò của cấu trúc bên trong (mạng con) trong quá trình grokking bằng cách kết nối lottery tickets với việc khái quát hóa bị trì hoãn. Kết quả của chúng tôi chứng minh rằng việc sử dụng lottery tickets làm giảm đáng kể việc khái quát hóa bị trì hoãn, nêu bật tầm quan trọng của các mạng con trong việc đạt được khái quát hóa hiệu quả.
•Tách biệt Thuộc tính của Lottery Tickets (mục 4): Chúng tôi phân tách có hệ thống các thuộc tính nội tại của lottery tickets thành ba thành phần: (1) norm trọng số, (2) độ thưa thớt, và (3) cấu trúc tốt. Thông qua một loạt thí nghiệm có kiểm soát, chúng tôi cho thấy rằng không phải norm trọng số cũng không phải độ thưa thớt đơn lẻ nào giải thích việc giảm khái quát hóa bị trì hoãn. Thay vào đó, chính việc khám phá cấu hình cấu trúc tốt trong mạng là quan trọng.
•Hiểu về Đặc điểm của Cấu trúc Tốt (mục 5): Chúng tôi đi sâu hơn vào bản chất của các cấu trúc có lợi được khám phá bởi lottery tickets. Những phát hiện của chúng tôi tiết lộ rằng grokked tickets thể hiện các biểu diễn trọng số tuần hoàn và trải qua những thay đổi cấu trúc nhanh chóng được đồng bộ hóa với việc cải thiện hiệu suất khái quát hóa. Hơn nữa, chúng tôi chứng minh rằng các kỹ thuật pruning, như thuật toán edge-popup, có thể xác định những cấu trúc hiệu quả này mà không cần thay đổi trọng số, hiệu quả chuyển đổi các mạng ghi nhớ thành các mạng khái quát hóa. Điều này nêu bật vai trò then chốt của việc khám phá cấu trúc trong việc giảm thiểu khái quát hóa bị trì hoãn.

2 Kiến thức nền tảng
Grokking là một hiện tượng mà việc khái quát hóa xảy ra rất lâu sau khi overfitting dữ liệu huấn luyện (như được thể hiện trong Hình 1 (trái)). Hiện tượng này ban đầu được quan sát thấy trong tác vụ cộng modular ((a+b)modp cho a,b∈(0,···,p−1)), và cùng hiện tượng đã được quan sát thấy trong các tập dữ liệu phức tạp hơn, bao gồm số học modular (Gromov, 2023; Davies et al., 2023; Rubin et al., 2023; Stander et al., 2023; Furuta et al., 2024), phân tích ngữ nghĩa (Liu et al., 2023a), n-k parity (Merrill et al., 2023), hồi quy đa thức (Kumar et al., 2023), tác vụ phân cấp (Murty et al., 2023) và phân loại hình ảnh (Liu et al., 2023a; Radhakrishnan et al., 2022). Bài báo này chủ yếu tập trung vào grokking trong các tác vụ số học modular thường được sử dụng trong các nghiên cứu trước đây.

Để hiểu về grokking, các nghiên cứu trước đây đã đề xuất những giải thích có thể, bao gồm cơ chế slingshot (Thilak et al., 2022), random walk giữa các minimizer (Millidge, 2022), công thức hóa biểu diễn tốt (Liu et al., 2022), quy mô của norm trọng số (Liu et al., 2023a; Varma et al., 2023), tính đơn giản của các đặc trưng Fourier (Nanda et al., 2023) và độ thưa thớt của mạng khái quát hóa (Miller et al., 2023).

Trong số đó, một trong những giải thích chủ đạo về cách mạng thay đổi trong quá trình grokking là tính đơn giản của giải pháp khái quát hóa, đặc biệt tập trung vào norm trọng số của các tham số mạng ∥θ∥2. Ví dụ, bài báo gốc (Power et al., 2022) giả định rằng weight decay đóng vai trò then chốt trong grokking, tức là độ chính xác kiểm tra sẽ không tăng nếu không có weight decay. Liu et al. (2023a) phân tích cảnh quan loss của tập dữ liệu huấn luyện và kiểm tra, xác minh rằng grokking xảy ra bằng cách vào vùng khái quát hóa được định nghĩa bởi norm L2, với các mô hình có giá trị ban đầu lớn θ0. Gần đây hơn, Varma et al. (2023) chứng minh rằng giải pháp khái quát hóa có thể tạo ra logits cao hơn với norm trọng số nhỏ hơn. Trong bài báo này, chúng tôi kiểm tra những thay đổi trong cấu trúc của mạng và chứng minh rằng mạng không chỉ đơn giản là giảm norm trọng số tổng thể mà còn tìm kiếm cấu trúc tốt trong chính nó.

--- TRANG 4 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
Một số nghiên cứu đã đề xuất rằng việc thu được biểu diễn tốt là chìa khóa để hiểu về grokking. Ví dụ, Power et al. (2022); Liu et al. (2022) giải thích rằng topo của các embedding lý tưởng có xu hướng là các vòng tròn hoặc hình trụ trong bối cảnh của các tác vụ cộng modular. Nanda et al. (2023) đã xác định thuật toán lượng giác mà các mạng sử dụng để giải quyết phép cộng modular sau khi grokking và cho thấy rằng nó phát triển một cách mượt mà trong quá trình huấn luyện. Gromov (2023) cho thấy một giải pháp phân tích cho các biểu diễn khi học phép cộng modular với MLP. Zhong et al. (2023) cho thấy, sử dụng phép cộng modular như một bài toán nguyên mẫu, rằng việc khám phá thuật toán trong mạng neural đôi khi phức tạp hơn. Những nghiên cứu này hỗ trợ chất lượng biểu diễn là chìa khóa để phân biệt các mạng ghi nhớ và khái quát hóa; tuy nhiên, những nghiên cứu này không giải thích điều gì đang xảy ra trong cấu trúc của mạng.

Giả thuyết lottery ticket được đề xuất bởi Frankle & Carbin (2019) đã thu hút sự chú ý như một giải thích cho lý do tại sao các mạng neural được tham số hóa quá mức thể hiện khả năng khái quát hóa (Allen-Zhu et al., 2019). Không chính thức, giả thuyết lottery ticket nói rằng các mạng được tham số hóa quá mức được khởi tạo ngẫu nhiên bao gồm các mạng con thưa thớt đạt hiệu suất tốt sau khi huấn luyện, và sự tồn tại của các mạng con là chìa khóa để đạt được khái quát hóa tốt trong mạng neural sâu. Tuyên bố này ban đầu được chứng minh thực nghiệm, nhưng các nền tảng lý thuyết cũng đã được thiết lập (Frankle et al., 2020; Sakamoto & Sato, 2022). Chính thức hơn, quá trình bao gồm các bước sau:
1. Khởi tạo mạng dày đặc fθ0 và huấn luyện mạng trong t epochs để thu được trọng số θt
2. Thực hiện pruning k% trên mạng đã huấn luyện dựa trên giá trị tuyệt đối |θt|. Quá trình này, được gọi là magnitude pruning, tạo ra một mask mkt∈{0,1}|θt|.
3. Đặt lại trọng số của mạng về giá trị ban đầu của chúng. θ0 và nhận được một mạng con fθ0⊙mkt, đại diện cho lottery ticket. Huấn luyện mạng con trong t′ epochs và thu được fθ′t′⊙mkt.

Sau khi khám phá ra lottery tickets, Ramanujan et al. (2020) cho thấy rằng tồn tại strong lottery tickets, đạt hiệu suất tốt mà không cần cập nhật trọng số. Họ sử dụng thuật toán edge-popup (Ramanujan et al., 2020) để chọn các mạng con dựa trên một điểm số s(s∈R|θ0|). Nói cách khác, khi pruning một tỷ lệ k của trọng số từ các trọng số cho trước θ0, mô hình dự đoán sử dụng các cạnh có điểm số (1−k) hàng đầu trong một lượt forward. Để có mô tả chi tiết về edge-popup, tham khảo Phụ lục A. Trong mục 5, chúng tôi sử dụng thuật toán edge-popup để kiểm tra liệu pruning có thể hoạt động như một lực để tăng tốc quá trình grokking.

3 Lottery Tickets làm giảm đáng kể Việc khái quát hóa bị trì hoãn
3.1 Thiết lập thí nghiệm
Theo Power et al. (2022) và các tài liệu grokking khác (Nakkiran et al., 2019; Liu et al., 2022; Gromov, 2023; Liu et al., 2023a), chúng tôi xây dựng một tập dữ liệu các phương trình có dạng: (a+b)%p=c. Tác vụ liên quan đến việc dự đoán c cho trước một cặp a và b. Thiết lập của chúng tôi sử dụng các cấu hình chi tiết sau: p=67, 0≤a,b,c<p. Kích thước tập dữ liệu là 2211, xem xét tất cả các cặp có thể có mà a≥b. Chúng tôi chia nó thành huấn luyện (40%) và kiểm tra (60%) theo Liu et al. (2022).

Theo Liu et al. (2022), chúng tôi thiết kế MLP như sau. Đầu tiên, chúng tôi ánh xạ mã hóa one-hot của a,b với trọng số embedding Wemb:Ea=Wemba,Eb=Wembb. Sau đó chúng tôi đưa các embedding Ea và Eb vào MLP như sau:
softmax(σ((Ea+Eb)Win)WoutWunemb ) (1)
trong đó Wemb,Win,Wout, và Wunemb là các tham số có thể huấn luyện, và σ là hàm kích hoạt ReLU (Nair & Hinton, 2010). Chiều của không gian embedding là 500, và Win chiếu vào 48 neuron chiều. Theo Nanda et al. (2023), chúng tôi sử dụng optimizer AdamW (Loshchilov & Hutter, 2019) với tốc độ học 10−3, trọng số của weight decay α=1.0,β1=0.9, và β2=0.98. Chúng tôi khởi tạo trọng số như θ0∼N(0,κ/√din), trong đó din đại diện cho chiều của lớp trước mỗi trọng số. Nếu không có gì được chỉ định, giả sử κ=1. Hãy giả sử chúng ta có tập dữ liệu huấn luyện Strain và tập dữ liệu kiểm tra Stest, và huấn luyện một mạng neural f(x;θ) trong đó x là một đầu vào và θ đại diện cho các tham số trọng số của mạng. Cụ thể,

--- TRANG 5 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
mạng được huấn luyện sử dụng AdamW trên cross-entropy loss và weight decay (norm L2 của trọng số ∥θ∥2):
argmin
θE(x,y)∼S[L(f(x;θ),y) +α/2∥θ∥2].

Để đo lường định lượng mức độ giảm khái quát hóa bị trì hoãn, chúng tôi định nghĩa tmem như bước mà độ chính xác huấn luyện vượt quá P%, và tgen như bước mà độ chính xác kiểm tra vượt quá P%. Theo Kumar et al. (2024), chúng tôi sử dụng P=95 cho các tác vụ số học modular. Chúng tôi sử dụng đề xuất (τgrok=tgen/tmem) để đo lường sự tăng tốc.

Chúng tôi so sánh hiệu suất của 1) mạng dày đặc fθt′ và 2) lottery tickets đã huấn luyện fθ′t′⊙mkt, trong đó t′ là epoch huấn luyện để có điểm số cuối cùng, t là thời điểm pruning, và k là tỷ lệ pruning. Như một trường hợp đặc biệt, khi t≥tgen, chúng tôi ký hiệu các mạng con là grokked tickets. Chúng tôi đã thử nghiệm các t và k khác nhau và điều tra cách chúng thay đổi tốc độ khái quát hóa. Theo mặc định, chúng tôi sử dụng k=0.6.

3.2 Kết quả
Hình 2-(a) cho thấy độ chính xác kiểm tra của grokked ticket và mô hình cơ sở trên tác vụ cộng modular. Mô hình cơ sở đề cập đến mô hình dày đặc được huấn luyện từ các giá trị ban đầu ngẫu nhiên. Grokked ticket cho thấy sự cải thiện trong độ chính xác kiểm tra gần như cùng lúc với sự cải thiện trong độ chính xác huấn luyện của mô hình cơ sở. Trong Hình 2-(b), sử dụng các thí nghiệm với transformer, kết quả cũng cho thấy rằng grokked tickets dẫn đến ít trì hoãn khái quát hóa hơn. Theo Power et al. (2022), chúng tôi đã tiến hành thí nghiệm trên các tác vụ số học modular khác nhau để chứng minh việc loại bỏ khái quát hóa bị trì hoãn. Hình 2-(c) cho thấy so sánh mô hình cơ sở (đường đứt nét) và grokked ticket (đường liền nét) với các tác vụ số học modular khác nhau. Hơn nữa, theo Kumar et al. (2023); Pearce et al. (2023), chúng tôi cũng chứng minh rằng việc khái quát hóa bị trì hoãn được giảm bởi grokked ticket trong cả tác vụ hồi quy đa thức và sparse parity trong Hình 2-(d,e). Kết quả cho thấy rằng grokked tickets làm giảm đáng kể việc khái quát hóa bị trì hoãn ngay cả trên các tác vụ khác nhau. Xem Phụ lục B để có giải thích chi tiết về thiết lập thí nghiệm.

Hình 3-(a) định lượng mối quan hệ giữa tỷ lệ pruning và τgrok=tgen/tmem(log-scaled). Khi tỷ lệ pruning là 0.7, τgrok đạt mức tối thiểu, cho thấy rằng grokked ticket làm giảm đáng kể việc khái quát hóa bị trì hoãn

--- TRANG 6 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
khái quát hóa bị trì hoãn. Lưu ý rằng, như được thể hiện trong Hình 3-(b), tmem bị trì hoãn khi sử dụng tỷ lệ pruning cao hơn, có nghĩa là grokked ticket không chỉ đơn giản là tăng tốc toàn bộ quá trình học mà cụ thể là tăng tốc việc chuyển đổi từ ghi nhớ sang khái quát hóa.

Trong Hình 4-(a), chúng tôi so sánh độ chính xác kiểm tra của epoch t khác nhau mà lottery tickets được thu thập. Kết quả cho thấy rằng lottery tickets thu được trước 25k epochs (non-grokked tickets) không khái quát hóa đầy đủ. Thêm vào đó, khả năng khái quát hóa này tương ứng với độ chính xác kiểm tra của mô hình cơ sở. Mặt khác, lottery tickets thu được sau 25k epochs (grokked tickets) đạt khái quát hóa hoàn hảo và giảm việc khái quát hóa bị trì hoãn. Trong Hình 4-(b), chúng tôi điều tra ảnh hưởng của tỷ lệ pruning trong grokked ticket, cho thấy nếu nó quá lớn (ví dụ, 0.9), nó không thể khái quát hóa; nếu nó quá nhỏ (ví dụ, 0.3), nó không khái quát hóa đủ nhanh.

4 Tách biệt Lottery Tickets: Norm, Độ thưa thớt, và Cấu trúc
Trong mục 3, chúng tôi đã thiết lập rằng grokked tickets — các mạng con được trích xuất tại thời điểm bắt đầu khái quát hóa — làm giảm đáng kể hiện tượng khái quát hóa bị trì hoãn. Trong khi những kết quả này mạnh mẽ gợi ý rằng một số thuộc tính nội tại của grokked tickets chịu trách nhiệm cho việc chuyển nhanh sang khái quát hóa, vẫn chưa rõ ràng chính xác thuộc tính này là gì. Nhớ lại từ Hình 2 rằng grokked ticket có thể drastically giảm việc khái quát hóa bị trì hoãn so với mô hình cơ sở (một mạng được huấn luyện dày đặc từ đầu). Để hiểu yếu tố chính đằng sau hiệu ứng này, chúng tôi xem xét ba giả thuyết được minh họa trong Hình 1 (phải):

Giả thuyết 1 (Norm Trọng số): Grokked tickets có norm trọng số nhỏ hơn mô hình cơ sở, và việc giảm norm trọng số này dẫn đến việc giảm khái quát hóa bị trì hoãn.

Trong Mục 4.1, chúng tôi kiểm tra Giả thuyết 1 bằng cách chuẩn bị mô hình dày đặc phù hợp với norm trọng số của grokked ticket. Nếu những mô hình này, mặc dù có cùng norm trọng số, thể hiện tốc độ khái quát hóa khác nhau, điều đó ngụ ý rằng chỉ riêng norm trọng số không giải thích việc giảm khái quát hóa bị trì hoãn. Do đó, Giả thuyết 1 bị bác bỏ.

Giả thuyết 2 (Độ thưa thớt): Grokked tickets là các mạng con thưa thớt hơn so với mô hình cơ sở, và mức độ thưa thớt cao hơn này dẫn đến việc giảm khái quát hóa bị trì hoãn.

Trong Mục 4.2, chúng tôi kiểm tra Giả thuyết 2 bằng cách xây dựng các mô hình có cùng mức độ thưa thớt với grokked ticket. Nếu những mô hình thưa thớt bằng nhau này không cho thấy sự giảm khái quát hóa bị trì hoãn, điều đó chỉ ra rằng chỉ riêng độ thưa thớt không chịu trách nhiệm cho việc giảm khái quát hóa bị trì hoãn. Do đó, Giả thuyết 2 bị bác bỏ.

Giả thuyết 3 (Cấu trúc Tốt): Ngoài sự khác biệt về norm trọng số hoặc độ thưa thớt, grokked tickets sở hữu cấu hình cấu trúc vượt trội trực tiếp tăng tốc khái quát hóa.

Nếu cả Giả thuyết 1 và 2 đều bị bác bỏ, giải thích duy nhất còn lại là cấu trúc tốt vốn có của grokked tickets thúc đẩy việc giảm khái quát hóa bị trì hoãn.

4.1 Kiểm soát Norm Trọng số của Mạng Ban đầu
Để kiểm tra Giả thuyết 1, chúng tôi chuẩn bị hai mô hình dày đặc có cùng norm L2 và L1 với grokked ticket, được đặt tên là 'mô hình dày đặc được kiểm soát'. Những mô hình dày đặc như vậy được thu thập thông qua quá trình sau:
1. Thu thập lottery tickets sau khi khái quát hóa hoàn toàn mktgen.
2. Nhận tỷ lệ norm Lp rp=∥θ0⊙mktgen∥p/∥θ0∥p
3. Tạo trọng số θ0·rp với cùng norm Lp như grokked ticket.

Hình 5-(a) cho thấy độ chính xác kiểm tra của mô hình cơ sở, grokked ticket, và các mô hình dày đặc được kiểm soát. Mặc dù có cùng norm trọng số ban đầu, grokked ticket đạt đến khái quát hóa nhanh hơn nhiều so với cả hai mô hình dày đặc được kiểm soát. Kết quả này chỉ ra rằng việc tăng trễ trong độ chính xác kiểm tra có thể quy cho không phải norm trọng số mà là việc khám phá cấu trúc tốt. Bên trái của Hình 6 cho thấy động lực của norm L2 cho mỗi mô hình. Tương tự như Liu et al. (2023b), norm L2 giảm tương ứng với sự tăng trong độ chính xác kiểm tra, hội tụ về một 'Vùng khái quát hóa'. Tuy nhiên, như được thể hiện ở bên phải của Hình 6, các điểm hội tụ cuối cùng của norm L1 khác nhau cho mỗi mô hình. Hiện tượng có norm L2 tương tự nhưng norm L1 nhỏ hơn gợi ý rằng trọng số mạng con tốt (grokked ticket) trở nên mạnh hơn, như được chỉ ra trong Miller et al. (2023). Kết quả tương tự cũng được quan sát thấy trong Transformer (Hình 16). Những kết quả này chứng minh rằng norm trọng số bản thân nó không đủ để giải thích grokking, bác bỏ Giả thuyết 1.

4.2 Kiểm soát Độ thưa thớt
Tiếp theo, chúng tôi kiểm tra Giả thuyết 2, cho rằng độ thưa thớt cao hơn trong grokked tickets giải thích cho việc giảm khái quát hóa bị trì hoãn của chúng. Để điều tra điều này, chúng tôi so sánh các mô hình có cùng độ thưa thớt với grokked ticket, sử dụng các phương pháp pruning-at-initialization (PaI) khác nhau. Cụ thể, chúng tôi đã thử nghiệm ba phương pháp PaI nổi tiếng — Grasp (Wang et al., 2020), SNIP (Lee et al., 2019), và Synflow (Tanaka et al., 2020)) — cùng với random pruning như các phương pháp cơ sở. Để biết chi tiết về mỗi phương pháp pruning, tham khảo mục Phụ lục D.

Hình 5-(b) so sánh sự chuyển đổi của độ chính xác kiểm tra của những phương pháp PaI này và grokked ticket. Kết quả cho thấy rằng tất cả các phương pháp PaI hoạt động kém hơn mô hình cơ sở hoặc, trong một số trường hợp, hoạt động kém hơn random pruning. Những kết quả này chỉ ra rằng chỉ riêng độ thưa thớt không thể giải thích việc giảm khái quát hóa bị trì hoãn, do đó bác bỏ Giả thuyết 2.

Việc bác bỏ Giả thuyết 1 và 2, điều này chỉ còn lại Giả thuyết 3: sự khác biệt chính phải nằm ở cấu trúc tốt của grokked tickets, cho phép chúng đạt được khái quát hóa nhanh chóng và hiệu quả rút ngắn giai đoạn khái quát hóa bị trì hoãn.

--- TRANG 7 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

--- TRANG 8 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

5 Hiểu về Grokking từ Cấu trúc Bên trong của Mạng
Trong mục 4, chúng tôi đã chứng minh rằng norm trọng số và độ thưa thớt là những giải thích không đủ cho việc khái quát hóa bị trì hoãn. Thay vào đó, chúng tôi đề xuất rằng cấu trúc tốt là thiết yếu để hiểu về grokking. Dựa trên những kết quả này, mục này đi sâu hơn vào việc hiểu bản chất và vai trò của các thuộc tính cấu trúc trong grokking. Cụ thể, chúng tôi điều tra các khía cạnh sau:

Q1 Việc thu thập cấu trúc có được đồng bộ hóa với việc cải thiện hiệu suất khái quát hóa không? (mục 5.1)
Q2 Chính xác thì cấu trúc tốt là gì? (mục 5.2)
Q3 Có thể đạt được khái quát hóa chỉ thông qua khám phá cấu trúc (pruning) mà không cần cập nhật trọng số không? (mục 5.3)

5.1 Đo lường Tiến độ: Dịch chuyển Cấu trúc Nắm bắt Thời điểm Khái quát hóa
Trong mục này, chúng tôi tiến hành phân tích chặt chẽ hơn về cách cấu trúc tốt được thu thập, và chúng tôi cho thấy rằng việc khám phá cấu trúc tốt tương ứng với sự cải thiện trong độ chính xác kiểm tra.

Đầu tiên, chúng tôi đề xuất một metric về thay đổi cấu trúc trong mạng, được đặt tên là Khoảng cách Jaccard (JD) sử dụng phương pháp Paganini & Forde (2020); Jaccard (1901). Chúng tôi đo khoảng cách jaccard giữa mask tại epoch t+δt và t.

JD(mt+δt,mt) = 1−|mt+δt∩mt|/|mt+δt∪mt|

mt đại diện cho mask thu được tại epoch t thông qua magnitude pruning và δt là 2k epoch. Nếu hai cấu trúc khác nhau, metric này gần bằng 1 và ngược lại. Thay đổi độ chính xác kiểm tra cũng được biểu diễn như sự khác biệt giữa độ chính xác kiểm tra tại epoch t+δt và t. Trong Hình 7, đường đỏ đại diện cho kết quả của những thay đổi trong độ chính xác kiểm tra và khoảng cách jaccard giữa mask tại epoch t+δt và t trên mỗi lớp. Kết quả cho thấy rằng trong những thay đổi đáng kể trong độ chính xác kiểm tra (16k-20k), thay đổi tối đa trong mask tương ứng, cho thấy rằng việc khám phá cấu trúc tốt tương ứng với sự cải thiện trong độ chính xác kiểm tra. Trong Phụ lục F, chúng tôi chứng minh rằng kết quả tương tự được thu thập cho cả tác vụ hồi quy đa thức và sparse parity.

Câu trả lời cho Q1
Việc thu thập cấu trúc có được đồng bộ hóa với việc cải thiện hiệu suất khái quát hóa không?
Có, việc khám phá cấu trúc tốt, như được đo bởi Khoảng cách Jaccard, xảy ra đồng thời với việc cải thiện độ chính xác kiểm tra, cho thấy rằng những thay đổi cấu trúc thúc đẩy khái quát hóa.

5.2 Phân tích Cấu trúc Tốt thông qua Cấu trúc Tuần hoàn và Thuộc tính Đồ thị
Trong mục trước (mục 5.1), chúng tôi đã thiết lập rằng những thay đổi đáng kể trong cấu trúc của mạng, như được đo bởi Khoảng cách Jaccard, trùng khớp chính xác với việc cải thiện độ chính xác kiểm tra. Dựa trên phát hiện này, bây giờ chúng tôi đi sâu hơn vào các thuộc tính cấu trúc kích hoạt những sự khác biệt đột ngột như vậy. Cụ thể, chúng tôi điều tra bản chất của cấu trúc được khám phá từ hai góc nhìn khác nhau: (1) các biểu diễn tuần hoàn được biết đến xuất hiện trong các tác vụ số học modular (như được nghiên cứu bởi Pearce et al. (2023); Nanda et al. (2023)), và (2) các thuộc tính lý thuyết đồ thị tiết lộ cách kết nối của mạng tiến hóa để hỗ trợ khái quát hóa tốt hơn. Bằng cách kiểm tra cả hai quan điểm, chúng tôi khám phá cách mạng cuối cùng dừng lại ở một 'cấu trúc tốt' thúc đẩy độ chính xác kiểm tra cao.

Phép cộng Modular và Tính tuần hoàn Nanda et al. (2023) đã chứng minh rằng các transformer được huấn luyện trên các tác vụ phép cộng modular dựa vào những biểu diễn tuần hoàn này để đạt được khái quát hóa. Cụ thể, các mạng nhúng các đầu vào a và b vào một cơ sở Fourier, mã hóa chúng như các thành phần sine và cosine của các tần số chính wk=2πk/p cho một số k∈N. Những biểu diễn tuần hoàn này sau đó được kết hợp sử dụng các đồng nhất thức lượng giác trong các lớp mạng để tính tổng modular.

--- TRANG 9 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

--- TRANG 10 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

Bằng cách reverse-engineering trọng số và activations của transformer một lớp được huấn luyện trên tác vụ này, Nanda et al. (2023) thấy rằng mô hình tính toán hiệu quả:
cos(wk(a+b)) = cos(wka) cos(wkb)−sin(wka) sin(wkb), (2)
sin(wk(a+b)) = sin(wka) cos(wkb) + cos(wka) sin(wkb). (3)
sử dụng ma trận embedding và các lớp attention và MLP. Logits cho mỗi đầu ra có thể c sau đó được tính bằng cách chiếu những giá trị này thông qua:
cos(wk(a+b−c)) = cos(wk(a+b)) cos(wkc) + sin(wk(a+b)) sin(wkc).
Phương pháp này đảm bảo rằng logits đầu ra của mạng thể hiện sự giao thoa tích cực tại c≡(a+b)modp, trong khi giao thoa phá hủy ức chế các giá trị không chính xác khác. Với cơ chế này, có thể suy ra rằng mô hình nội tại sử dụng tính tuần hoàn, chẳng hạn như các công thức cộng, để thực hiện số học modular.

Thu thập Cấu trúc Tuần hoàn trong Grokked Tickets Trong nghiên cứu này, dựa trên nghiên cứu trước đây, chúng tôi kiểm tra cấu trúc được thu thập bởi grokked tickets từ góc nhìn của tính tuần hoàn. Đầu tiên, chúng tôi điều tra tính tuần hoàn của mỗi neuron bằng cách vẽ hướng của ma trận trọng số (Winproj), được thu thập bằng cách nhân Wemb và Win (tham khảo mục 3.1). Ma trận trọng số hai chiều này đại diện cho các chiều đầu vào (67) trên một trục và số lượng neuron (48) trên trục khác. Hình 8 (dưới) minh họa trọng số (67 chiều) của mỗi neuron được vẽ tại các checkpoint huấn luyện (0k, 2k, 30k). Trong mô hình cơ sở tại 30k bước, trọng số tiết lộ tính tuần hoàn rõ ràng, trong khi ở 0k và 2k bước, trọng số thể hiện cấu trúc ngẫu nhiên. Đáng chú ý, grokked tickets phát triển cấu trúc tuần hoàn này sớm hơn nhiều so với mô hình cơ sở (xem grokked ticket tại 2k bước). Phát hiện này nêu bật rằng cấu trúc của grokked tickets phù hợp tốt với Phép cộng Modular, vì chúng thu thập cấu trúc tuần hoàn phù hợp với bản chất tác vụ.

Để định lượng tính tuần hoàn này như một cấu trúc tốt, chúng tôi giới thiệu Fourier Entropy (FE) như sau. Nói chung, biến đổi Fourier rời rạc F(ω) của một hàm f(x) được định nghĩa như sau:
F(ω) =∑x=0^N f(x) exp(−i2πωx/N)
Trong trường hợp này, vì chúng tôi muốn biết tính tuần hoàn của trọng số của mỗi neuron, f(x) là trọng số của neuron thứ i trên đầu vào thứ j, và d là chiều của đầu vào. Sau đó, Fourier Entropy (FE) được tính

--- TRANG 11 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

như sau:
FE=−∑i=1^n pi log pi
Ở đây, pi là giá trị chuẩn hóa của F(ω), và n đại diện cho số lượng neuron. Giá trị FE thấp cho thấy rằng tần số của trọng số của mỗi neuron có ít biến đổi, hội tụ đến các tần số cụ thể, cho thấy rằng mạng đã thu thập cấu trúc tuần hoàn.

Hình 8 (trên) cho thấy FE của mô hình cơ sở (xanh lục) và grokked ticket (xanh lam). Kết quả cho thấy rằng grokked ticket có các neuron với cấu trúc tuần hoàn ở giai đoạn đầu (2k epochs) và thể hiện sự giảm nhanh trong FE ở epoch đầu. Điều này chỉ ra rằng mô hình khám phá các cấu trúc tốt nội tại (cấu trúc khái quát hóa) phù hợp với tác vụ (Phép cộng Modular). Để cung cấp phân tích chi tiết hơn, chúng tôi đã thêm các hình ảnh trực quan của các ma trận trọng số và mask grokked ticket trong Phụ lục I.

Thuộc tính Đồ thị và Grokked Ticket Các cấu trúc của chính mạng thể hiện các thuộc tính lý thuyết đồ thị khác nhau, như được phân tích trong các nghiên cứu trước đây (Hoang et al., 2023b;a; You et al., 2020). Được thúc đẩy bởi những hiểu biết này, chúng tôi phân tích sự tiến hóa của (1) weighted spectral gap, (2) ramanujan gap, (3) độ dài đường đi trung bình, và (4) hệ số clustering trong suốt quá trình huấn luyện, cùng với những thay đổi trong độ chính xác huấn luyện/kiểm tra. Chi tiết về mỗi metric thuộc tính đồ thị được cung cấp trong Phụ lục J.

Hình 9-(a) minh họa cách weighted spectral gap (trái) và Ramanujan gap (phải) tiến hóa trong suốt quá trình huấn luyện bằng cách kiểm tra trung bình qua các lớp của mạng. Kết quả cho từng lớp riêng biệt được cung cấp trong Phụ lục K. Chúng tôi quan sát thấy rằng weighted spectral gap tăng mạnh trong giai đoạn ghi nhớ, cho thấy sự khác biệt rõ nét giữa eigenvalue lớn nhất và lớn thứ hai trong khi mô hình đang overfitting. Xu hướng này gợi ý rằng, trong giai đoạn ghi nhớ, trọng số bị chi phối bởi một thành phần cụ thể (tương ứng với eigenvector dẫn đầu). Khi mô hình chuyển đổi về khái quát hóa tốt hơn, tuy nhiên, spectral gap thu hẹp, phản ánh việc sử dụng cân bằng hơn không gian tham số. Chúng tôi quan sát hành vi tương tự trong các lớp khác (xem Phụ lục K để biết chi tiết).

Hình 9-(b) trình bày các metric được suy ra từ việc xem toàn bộ mạng như một đồ thị quan hệ (You et al., 2020). Độ dài đường đi trung bình (trái) tăng cùng với sự tăng trong độ chính xác kiểm tra, sau đó hội tụ đến một phạm vi vừa phải. Xu hướng này gợi ý rằng khi mạng tìm kiếm giải pháp khái quát hóa (tức là khám phá Grokked ticket), kết nối của nó trở nên thưa thớt hơn; một khi giải pháp đó được tìm thấy, độ dài đường đi giảm và ổn định. Ngược lại, hệ số clustering (phải) giảm khi độ chính xác kiểm tra cải thiện, sau đó cũng ổn định ở một giá trị trung gian. Trong giai đoạn khi mạng tìm kiếm giải pháp khái quát hóa (tức là khi mô hình khám phá Grokked ticket), clustering giảm nhưng cuối cùng ổn định gần một mức độ vừa phải. Cả hai metric do đó hội tụ không về cực trị mà duy trì trong một chế độ cân bằng, phù hợp với các nghiên cứu trước đây (You et al., 2020).

Câu trả lời cho Q2
Chính xác thì cấu trúc tốt là gì?
Cấu trúc tốt có Fourier Entropy thấp, cho thấy các mẫu trọng số tuần hoàn phù hợp với tác vụ. Nó cũng tuân theo các thuộc tính đồ thị đã biết, chẳng hạn như weighted spectral gap tăng trong giai đoạn ghi nhớ và độ dài đường đi trung bình tăng trong khái quát hóa.

5.3 Pruning trong Huấn luyện: Pruning Thúc đẩy Khái quát hóa
Dựa trên kết quả rằng việc khám phá cấu trúc tốt tương ứng với sự cải thiện trong độ chính xác kiểm tra, chúng tôi chứng minh rằng chỉ riêng pruning có thể chuyển đổi từ giải pháp ghi nhớ sang giải pháp khái quát hóa mà không cần cập nhật trọng số, và hơn nữa, sự kết hợp của pruning và weight decay thúc đẩy khái quát hóa hiệu quả hơn so với chỉ regularization của norm trọng số.

Để xác minh điều này, chúng tôi giới thiệu edge-popup (Ramanujan et al., 2020), một phương pháp học cách prune trọng số mà không cần cập nhật trọng số. Trong edge-popup, mỗi trọng số được gán một điểm số, và những điểm số này được cập nhật thông qua backpropagation để xác định trọng số nào cần prune. Để biết chi tiết về edge-popup, tham khảo Phụ lục A. Chúng tôi xác thực tuyên bố của mình bằng cách tối ưu hóa sử dụng ba phương pháp khác nhau.

WD: Huấn luyện từ θmem sử dụng Weight Decay với cập nhật trọng số (giống như mô hình cơ sở).
EP w/o WD: Huấn luyện từ θmem sử dụng Edge-Popup mà không cập nhật trọng số.
EP w/ WD: Huấn luyện từ θmem sử dụng Edge-Popup và Weight Decay với cập nhật trọng số.

--- TRANG 12 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

Trong Bảng 1, kết quả cho EP mà không có weight decay (EP w/o WD) cho thấy rằng mạng đạt được hiệu suất khái quát hóa 0.92 mà không có bất kỳ thay đổi nào đối với trọng số (chỉ bằng cách prune trọng số). Kết quả này chứng minh rằng khái quát hóa có thể đạt được chỉ thông qua khám phá cấu trúc mà không cần cập nhật trọng số, từ đó củng cố tuyên bố của chúng tôi rằng việc khái quát hóa bị trì hoãn xảy ra do việc khám phá cấu trúc tốt. Thêm vào đó, kết quả EP w/ WD cho thấy sự cải thiện nhanh nhất trong độ chính xác kiểm tra và hiệu quả nhất trong việc thúc đẩy khái quát hóa. Những hiểu biết này gợi ý rằng các nhà thực hành có thể cải thiện khái quát hóa bằng cách kết hợp các phương pháp tối ưu hóa trực tiếp các cấu trúc có lợi thay vì chỉ dựa vào các kỹ thuật regularization truyền thống như weight decay. Những phát hiện của chúng tôi nêu bật tiềm năng của grokked tickets để thông báo cho việc phát triển các kỹ thuật regularization mới, hướng cấu trúc.

Trong Bảng 1, tương tự như phân tích trong mục 5.2, chúng tôi kiểm tra các giá trị trung bình của các metric thuộc tính đồ thị trên ma trận trọng số của mạng dưới EP w/o WD. Như quan sát thấy trong mục 5.2, weighted spectral gap ban đầu tăng và sau đó giảm, trong khi ramanujan gap tiếp tục giảm. Hơn nữa, cả độ dài đường đi trung bình và hệ số clustering đều hội tụ đến các giá trị tương tự.

Câu trả lời cho Q3
Có thể đạt được khái quát hóa chỉ thông qua khám phá cấu trúc (pruning) mà không cần cập nhật trọng số không?
Có, chỉ riêng pruning cải thiện đáng kể khái quát hóa, chứng minh rằng việc khám phá mạng con tốt là đủ để chuyển đổi từ ghi nhớ sang khái quát hóa.

6 Thảo luận và Các nghiên cứu Liên quan
Trong bài báo này, chúng tôi đã tiến hành một tập hợp các thí nghiệm để hiểu cơ chế của grokking (khái quát hóa bị trì hoãn). Dưới đây là tóm tắt các quan sát. (1) Trong mục 3.2, việc sử dụng lottery ticket làm giảm đáng kể việc khái quát hóa bị trì hoãn. (2) Trong mục 4, cấu trúc tốt là yếu tố quan trọng hơn trong việc giải thích grokking so với norm trọng số và độ thưa thớt bằng cách so sánh với cùng mức norm trọng số và độ thưa thớt. (3) Trong mục 5.2, cấu trúc của grokked tickets phù hợp tốt với tác vụ. (4) Trong mục 5.1, cấu trúc tốt được khám phá dần dần, tương ứng với sự cải thiện của độ chính xác kiểm tra. (5) Trong mục 5.3, pruning mà không cập nhật trọng số từ giải pháp ghi nhớ tăng độ chính xác kiểm tra. Trong mục này, chúng tôi kết nối những kết quả này với giải thích trước đây về cơ chế của grokking và các nghiên cứu liên quan.

Giảm norm trọng số Liu et al. (2023a) gợi ý rằng việc giảm norm trọng số là quan trọng cho khái quát hóa. Trong Hình 5, kết quả của chúng tôi đi xa hơn để cho thấy rằng, thay vì chỉ đơn giản giảm norm trọng số, mạng khám phá cấu trúc tốt (mạng con), dẫn đến việc giảm norm trọng số.

Độ thưa thớt và Hiệu quả Merrill et al. (2023) lập luận rằng việc chuyển đổi giai đoạn grokking tương ứng với sự xuất hiện của một mạng con thưa thớt chi phối dự đoán mô hình. Trong khi họ nghiên cứu thực nghiệm các tác vụ parse parity nơi độ thưa thớt là rõ ràng, chúng tôi đang tiến hành các tác vụ (số học modular, MNIST) thường được sử dụng trong nghiên cứu grokking và kiến trúc (MLP, Transformer). Hơn nữa, Hình 5, chúng tôi chứng minh không chỉ độ thưa thớt mà còn cấu trúc tốt là quan trọng.

Học biểu diễn Liu et al. (2022); Nanda et al. (2023); Liu et al. (2023a); Gromov (2023) cho thấy chất lượng biểu diễn là chìa khóa để phân biệt các mạng ghi nhớ và khái quát hóa. Hình 8 chứng minh rằng cấu trúc tốt đóng góp vào việc thu thập biểu diễn tốt, gợi ý tầm quan trọng của cấu trúc bên trong (topo mạng) trong việc đạt được biểu diễn tốt.

Regularization Weight decay (Rumelhart et al., 1986) là một trong những kỹ thuật regularization được sử dụng phổ biến nhất và được biết đến như một yếu tố quan trọng trong grokking (Power et al., 2022; Liu et al., 2023a). Trong Phụ lục H, chúng tôi cho thấy nếu cấu trúc tốt được khám phá, mạng khái quát hóa mà không cần weight decay, cho thấy rằng weight decay hoạt động như một việc khám phá cấu trúc, gợi ý rằng weight decay đóng góp

--- TRANG 13 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

không phải để giảm trọng số mà để khám phá cấu trúc tốt. Thêm vào đó, chúng tôi đã thử nghiệm pruning như một lực mới để gây ra khái quát hóa (Bảng 1). Tuy nhiên, như được thể hiện trong kết quả của pruning at initialization (Hình 5-(b)), pruning không đúng cách có thể làm giảm hiệu suất khái quát hóa.

Giả thuyết lottery ticket Giả thuyết lottery ticket (Frankle & Carbin, 2019) gợi ý rằng cấu trúc tốt là quan trọng cho khái quát hóa, nhưng vẫn chưa rõ ràng cách những cấu trúc này được thu thập trong quá trình huấn luyện và cách chúng tương ứng với biểu diễn của mạng. Theo hiểu biết của tôi, bài báo của chúng tôi là bài đầu tiên kết nối grokking và giả thuyết lottery ticket, chứng minh cách các cấu trúc tốt xuất hiện (mục 5.1) và đóng góp vào biểu diễn hiệu quả (mục 5.2). Dựa trên điều này, chúng tôi tiếp tục cho thấy rằng trong khi khái quát hóa nhanh hơn là một thuộc tính được ghi nhận rõ ràng của winning tickets, nghiên cứu của chúng tôi đi xa hơn bằng cách khám phá cách việc khám phá cấu trúc tốt.

Hướng Tương lai Trong khi nghiên cứu của chúng tôi tập trung vào các tác vụ đơn giản, hiểu biết chính về việc khám phá mạng con tốt có thể mở rộng đến các miền phức tạp hơn như nhận dạng hình ảnh quy mô lớn và tạo ngôn ngữ. Những phát hiện của chúng tôi gợi ý rằng các mạng con tốt, hoặc "lottery tickets", giúp giảm thiểu việc khái quát hóa bị trì hoãn, cung cấp một góc nhìn cấu trúc về regularization. Các phương pháp truyền thống như weight decay gián tiếp thúc đẩy việc khám phá cấu trúc, trong khi pruning và tối ưu hóa dựa trên mask cung cấp một phương pháp trực tiếp và có thể hiệu quả hơn. Như được thể hiện trong Bảng 1, việc kết hợp weight decay với thuật toán edge-popup tăng tốc khái quát hóa, chứng minh những lợi thế của các kỹ thuật nhận thức cấu trúc. Nghiên cứu tương lai có thể tích hợp những hiểu biết này vào các kiến trúc đương đại để nâng cao hiệu quả học và tính mạnh mẽ. Khám phá các chiến lược dựa trên pruning hoặc tối ưu hóa mask trong các mô hình quy mô lớn có thể xác nhận thêm hiệu quả của chúng trong các ứng dụng thế giới thực. Việc kết nối những hiểu biết cấu trúc này với các kỹ thuật huấn luyện chính thống có thể cải thiện khái quát hóa và hiệu quả của mô hình trong các cài đặt phức tạp.

Thêm vào đó, dựa trên những hiểu biết về thuộc tính đồ thị thu được trong mục 5.2 của nghiên cứu này, nghiên cứu tương lai nên lấy mẫu cấu trúc mạng neural một cách rõ ràng theo thuộc tính đồ thị, như được chứng minh trong nghiên cứu trước đây (Javaheripi et al., 2021). Việc kết hợp những hiểu biết cấu trúc được xác định ở đây vào quá trình lấy mẫu có thể tạo điều kiện thuận lợi cho việc khám phá các kiến trúc mạng hiệu quả và mạnh mẽ hơn.

--- TRANG 14 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

Tài liệu tham khảo
Zeyuan Allen-Zhu, Yuanzhi Li, và Zhao Song. A convergence theory for deep learning via over-parameterization, 2019.
Xander Davies, Lauro Langosco, và David Krueger. Unifying grokking and double descent. arXiv preprint arXiv:2303.06173, 2023.
Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, và Michael Carbin. Stabilizing the lottery ticket hypothesis, 2020.
Hiroki Furuta, Gouki Minegishi, Yusuke Iwasawa, và Yutaka Matsuo. Interpreting grokked transformers in complex modular arithmetic, 2024.
Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.
Duc N.M Hoang, Souvik Kundu, Shiwei Liu, và Zhangyang Wang. Don't just prune by magnitude! your mask topology is a secret weapon. Trong Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=DIBcdjWV7k.
Duc N.M Hoang, Shiwei Liu, Radu Marculescu, và Zhangyang Wang. REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH. Trong The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=uVcDssQff_.
P. Jaccard. Etude comparative de la distribution florale dans une portion des Alpes et du Jura. Bulletin de la Société vaudoise des sciences naturelles. Impr. Corbaz, 1901. URL https://books.google.co.jp/books?id=JCNdmgEACAAJ.
Mojan Javaheripi, Bita Darvish Rouhani, và Farinaz Koushanfar. Swann: Small-world architecture for fast convergence of neural networks. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 11(4):575–585, 2021. doi: 10.1109/JETCAS.2021.3125309.
Alex Krizhevsky, Ilya Sutskever, và Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. Trong F. Pereira, C. J. C. Burges, L. Bottou, và K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.
Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, và Cengiz Pehlevan. Grokking as the transition from lazy to rich training dynamics. arXiv preprint arXiv:2310.06110, 2023.
Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, và Cengiz Pehlevan. Grokking as the transition from lazy to rich training dynamics. Trong The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vt5mnLVIVo.
Namhoon Lee, Thalaiyasingam Ajanthan, và Philip H. S. Torr. Snip: Single-shot network pruning based on connection sensitivity, 2019.
Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, và Mike Williams. Towards understanding grokking: An effective theory of representation learning. arXiv preprint arXiv:2205.10343, 2022.
Ziming Liu, Eric J Michaud, và Max Tegmark. Omnigrok: Grokking beyond algorithmic data. Trong International Conference on Learning Representations, 2023a.
Ziming Liu, Ziqian Zhong, và Max Tegmark. Grokking as compression: A nonlinear complexity perspective. arXiv preprint arXiv:2310.05918, 2023b.
Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization, 2019.

--- TRANG 15 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

William Merrill, Nikolaos Tsilivis, và Aman Shukla. A tale of two circuits: Grokking as competition of sparse and dense subnetworks. arXiv preprint arXiv:2303.11873, 2023.
Jack Miller, Charles O'Neill, và Thang Bui. Grokking beyond neural networks: An empirical exploration with model complexity. arXiv preprint arXiv:2310.17247, 2023.
Beren Millidge. Grokking 'grokking', 2022. URL https://www.beren.io/2022-01-11-Grokking-Grokking/.
Shikhar Murty, Pratyusha Sharma, Jacob Andreas, và Christopher D. Manning. Grokking of hierarchical structure in vanilla transformers. arXiv preprint arXiv:2305.18741, 2023.
Vinod Nair và Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. Trong Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10, pp. 807–814, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, và Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019.
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, và Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. Trong International Conference on Learning Representations, 2023.
Behnam Neyshabur. Towards learning convolutions from scratch, 2020. URL https://arxiv.org/abs/2007.13657.
Michela Paganini và Jessica Zosa Forde. Bespoke vs. prêt-à-porter lottery tickets: Exploiting mask similarity for trainable sub-network finding, 2020.
Adam Pearce, Asma Ghandeharioun, Nada Hussein, Nithum Thain, Martin Wattenberg, và Lucas Dixo. Do machine learning models memorize or generalize? https://pair.withgoogle.com/explorables/grokking/, 2023. [Online; accessed 11-October-2023].
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, và Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, và Mikhail Belkin. Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. arXiv preprint arXiv:2212.13881, 2022.
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, và Mohammad Rastegari. What's hidden in a randomly weighted neural network?, 2020.
Noa Rubin, Inbar Seroussi, và Zohar Ringel. Droplets of good representations: Grokking as a first order phase transition in two layer networks. arXiv preprint arXiv:2310.03789, 2023.
David E Rumelhart, Geoffrey E Hinton, và Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986.
Keitaro Sakamoto và Issei Sato. Analyzing lottery ticket hypothesis from pac-bayesian theory perspective, 2022.
Dashiell Stander, Qinan Yu, Honglu Fan, và Stella Biderman. Grokking group multiplication with cosets. arXiv preprint arXiv:2312.06581, 2023.
Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, và Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow, 2020.
Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, và Joshua Susskind. The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon. arXiv preprint arXiv:2206.04817, 2022.

--- TRANG 16 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, và Ramana Kumar. Explaining grokking through circuit efficiency. arXiv preprint arXiv:2309.02390, 2023.
Chaoqi Wang, Guodong Zhang, và Roger Grosse. Picking winning tickets before training by preserving gradient flow, 2020.
Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, và Xiaolin Hu. Pruning from scratch, 2019.
Jiaxuan You, Jure Leskovec, Kaiming He, và Saining Xie. Graph structure of neural networks. Trong Hal Daumé III và Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 10881–10891. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/you20b.html.
Zhenyu Zhang, Xuxi Chen, Tianlong Chen, và Zhangyang Wang. Efficient lottery ticket finding: Less data is more, 2021.
Ziqian Zhong, Ziming Liu, Max Tegmark, và Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. Trong Neural Information Processing Systems, 2023.

--- TRANG 17 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

A Thuật toán Edge-popup
Chúng tôi cung cấp giải thích chi tiết về thuật toán edge-popup. Edge-popup Ramanujan et al. (2020) là một phương pháp tìm kiếm mạng con hiệu quả trong các mạng neural được khởi tạo ngẫu nhiên mà không cần cập nhật trọng số.
Luồng Cơ bản:
1. Khởi tạo Ngẫu nhiên: Khởi tạo trọng số w của một mạng neural lớn một cách ngẫu nhiên.
2. Gán Điểm số: Gán một điểm số sij cho mỗi cạnh wij một cách ngẫu nhiên.
3. Chọn Mạng con: Tạo mạng con G chỉ sử dụng các cạnh có điểm số k% hàng đầu.
4. Tối ưu hóa Điểm số: Cập nhật điểm số sij dựa trên hiệu suất của mạng con G.
Cập nhật Điểm số: Cập nhật điểm số sij của mỗi cạnh wij sử dụng công thức sau:
sij←sij−α∂L/∂IjZiwij
•α là tốc độ học
•∂L/∂Ij là gradient của loss đối với đầu vào của neuron thứ j
•Zi là đầu ra của neuron thứ i
Tính toán Thực tế: Forward Pass: Tính toán chỉ sử dụng các cạnh có điểm số sij trong k% hàng đầu.
Ij=∑i∈V wijZih(sij)
Ở đây, h(sij) là 1 nếu sij trong k% hàng đầu và 0 nếu không.

--- TRANG 18 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

B Các cấu hình khác nhau của tác vụ và kiến trúc.
B.1 MLP cho MNIST
Chúng tôi sử dụng MLP 4 lớp cho phân loại MNIST. Điểm khác biệt so với phân loại thông thường là chúng tôi đang sử dụng Mean Squared Error (MSE) cho loss. Chúng tôi áp dụng thiết lập này theo nghiên cứu trước đây (Liu et al., 2023a). Trong (Liu et al., 2023a), được xác nhận trong Phụ lục rằng grokking xảy ra mà không có vấn đề gì, ngay cả khi thử với cross-entropy. Hình 10 cho thấy độ chính xác kiểm tra và huấn luyện trên các cấu hình khác nhau. Rõ ràng rằng grokked tickets tăng tốc khái quát hóa trong tất cả các cấu hình, và việc khám phá grokked tickets đóng góp vào khái quát hóa.

B.2 Transformer cho phép cộng modular
Tương tự như Nanda et al. (2023), chúng tôi sử dụng transformer 1 lớp trong tất cả các thí nghiệm. Chúng tôi sử dụng attention đơn head và bỏ qua layer norm.
•Siêu tham số:
–dvocab = 67: Kích thước của không gian đầu vào và đầu ra (giống như p).
–demb = 500: Kích thước embedding.
–dmlp = 128: Độ rộng của lớp MLP.
•Tham số:
–WE: Lớp embedding.
–Wpos: Embedding vị trí.
–WQ: Ma trận Query.
–WK: Ma trận Key.
–WV: Ma trận Value.
–WO: Đầu ra Attention.
–Win,bin: Trọng số và bias của lớp đầu tiên của MLP.
–Wout,bout: Trọng số và bias của lớp thứ hai của MLP.
–WU: Lớp Unembedding.
Chúng tôi mô tả quá trình thu thập logits cho mô hình một lớp. Lưu ý rằng loss chỉ được tính từ logits trên token cuối cùng. Hãy x(l)i biểu thị token tại vị trí i trong lớp l. Ở đây, i là 0 hoặc 1, vì số lượng token đầu vào là 2, và x(0)i là một vector one-hot. Chúng tôi ký hiệu các điểm số attention là A và ma trận tam giác với các phần tử âm vô cùng là M, được sử dụng cho causal attention.
Logits được tính thông qua các phương trình sau:
x(1)i=WEx(0)i+Wposx(0)i,
A= softmax( x(1)TWT
KWQx(1)−M),
x(2)=WOWV(x(1)A) +x(1),
x(3)=WoutReLU(Winx(2)+bin) +bout+x(2),
logits = softmax( WUx(3)).
Hình 10 cho thấy độ chính xác huấn luyện và kiểm tra của mô hình cơ sở (xanh lục) và grokked ticket (xanh lam). Trên các tập dữ liệu (ví dụ, Phép cộng Modular, MNIST) và kiến trúc (Transformer), grokked ticket (xanh lam) liên tục đạt khái quát hóa nhanh hơn mô hình cơ sở (xanh lục). Những kết quả này nhấn mạnh tầm quan trọng của các yếu tố cấu trúc trong grokking, bất kể tác vụ hay kiến trúc.

--- TRANG 19 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

--- TRANG 20 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

C Thí nghiệm với Các Seed khác nhau
Để đảm bảo khả năng tái tạo, chúng tôi tiến hành thí nghiệm với ba seed khác nhau và trình bày kết quả cho các thí nghiệm trong Hình 2. Ngoài độ chính xác, chúng tôi vẽ loss của huấn luyện và kiểm tra trong Hình 11.

Thêm vào đó, để đảm bảo khả năng tái tạo, chúng tôi tiến hành thí nghiệm với ba seed khác nhau và trình bày kết quả cho các thí nghiệm trong mục 4.1 và thí nghiệm kiến trúc Transformer.

--- TRANG 21 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

--- TRANG 22 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

D Các phương pháp pruning at initialization
Hiện tại, các phương pháp pruning mạng neural (NN) at initialization (như SNIP, GraSP, SynFlow) vẫn thể hiện khoảng cách khi so sánh với các phương pháp sử dụng thông tin sau huấn luyện để pruning (như Lottery Ticket). Tuy nhiên, lĩnh vực này đang trải qua sự gia tăng hoạt động nghiên cứu.

Luồng cơ bản của pruning at initialization như sau:
1. Khởi tạo ngẫu nhiên mạng neural f(x;θ0).
2. Prune p% tham số trong θ0 theo điểm số S(θ), tạo mask m.
3. Huấn luyện mạng từ θ0⊙m.
Theo Tanaka et al. (2020), nghiên cứu về pruning at initialization quy về phương pháp xác định điểm số trong quá trình 2 ở trên, có thể được mô tả thống nhất như sau:
S(θ) =∂R/∂θ⊙θ
Khi R là training loss L, metric synaptic saliency kết quả tương đương với |∂L/∂θ⊙θ| được sử dụng trong SNIP (Lee et al., 2019). −(H∂L/∂θ)⊙θ được sử dụng trong Grasp (Wang et al., 2020). Tanaka et al. (2020) đề xuất thuật toán synflow RSF= 1T(∏l=1L|θ|l||)1. Trong mục 4.2, tất cả các giá trị ban đầu được thí nghiệm sử dụng cùng trọng số và cùng tỷ lệ pruning.

--- TRANG 23 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

E Ý nghĩa của Tính tuần hoàn trong Tác vụ Phép cộng Modular
Tác vụ phép cộng modular, được định nghĩa là dự đoán c≡(a+b)modp, vốn liên quan đến tính tuần hoàn do cấu trúc số học modular. Nanda et al. (2023) chứng minh rằng các transformer được huấn luyện trên các tác vụ phép cộng modular dựa vào các biểu diễn tuần hoàn để đạt được khái quát hóa. Cụ thể, các mạng nhúng đầu vào a và b vào một cơ sở Fourier, mã hóa chúng như các thành phần sine và cosine của các tần số chính wk=2kπ/p cho một số k∈N. Những biểu diễn tuần hoàn này sau đó được kết hợp sử dụng các đồng nhất thức lượng giác trong các lớp mạng để tính tổng modular.

Cơ chế của Biểu diễn Tuần hoàn
Nanda et al. (2023) reverse-engineered trọng số và activations của transformer một lớp được huấn luyện trên tác vụ này và thấy rằng mô hình tính toán:
cos(wk(a+b)) = cos(wka) cos(wkb)−sin(wka) sin(wkb),
sin(wk(a+b)) = sin(wka) cos(wkb) + cos(wka) sin(wkb),
sử dụng ma trận embedding và các lớp attention và MLP. Logits cho mỗi đầu ra có thể c sau đó được tính bằng cách chiếu những giá trị này sử dụng:
cos(wk(a+b−c)) = cos(wk(a+b)) cos(wkc) + sin(wk(a+b)) sin(wkc).
Phương pháp này đảm bảo rằng logits đầu ra của mạng thể hiện giao thoa tích cực tại c≡(a+b)modp, trong khi giao thoa phá hủy ức chế các giá trị không chính xác khác.

Với cơ chế này, có thể suy ra rằng mô hình nội tại sử dụng tính tuần hoàn, chẳng hạn như các công thức cộng, để thực hiện số học modular.

F Thay đổi Cấu trúc trong Các Tác vụ Khác ngoài Tác vụ Phép cộng Modular

--- TRANG 24 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

G Norm Trọng số có đủ để Giải thích Grokking trong Transformer không?
Hình 16 cho thấy độ chính xác của mô hình cơ sở, grokked ticket và Controlled dense (L1 và L2) trên Transformer. Kết quả cho thấy grokked ticket khái quát hóa nhanh hơn bất kỳ mô hình nào khác. Kết quả gợi ý rằng ngay cả trong trường hợp Transformer, việc khám phá grokked ticket quan trọng hơn norm trọng số.

H Weight Decay hoạt động như Người khám phá Cấu trúc
Kết quả của chúng tôi là việc khám phá cấu trúc tốt xảy ra giữa ghi nhớ và khái quát hóa, cho thấy weight decay là thiết yếu để khám phá cấu trúc tốt nhưng trở nên dư thừa sau khi khám phá ra chúng. Trong mục này, đầu tiên chúng tôi khám phá tỷ lệ pruning quan trọng, đó là tỷ lệ pruning tối đa có thể gây ra khái quát hóa mà không có weight decay Hình 17-(a). Chúng tôi nhận ra rằng tỷ lệ pruning quan trọng nằm giữa 0.8 và 0.9 bởi vì nếu tỷ lệ pruning tăng lên 0.9, độ chính xác kiểm tra giảm đột ngột. Do đó, chúng tôi dần dần tăng tỷ lệ pruning với bước 0.01 từ 0.8 và thấy rằng k=0.81 là tỷ lệ pruning quan trọng. Sau đó chúng tôi so sánh hành vi của grokked ticket mà không có weight decay và mô hình cơ sở. Hình 17-(b) cho thấy kết quả của các thí nghiệm. Kết quả cho thấy rằng nếu cấu trúc tốt được khám phá, mạng khái quát hóa hoàn toàn mà không cần weight decay, cho thấy rằng weight decay hoạt động như một người khám phá cấu trúc.

Trong mục này, chúng tôi cho thấy rằng với tỷ lệ pruning chính xác, grokked ticket không cần weight decay để khái quát hóa, cho thấy rằng weight decay là thiết yếu để khám phá mạng con tốt nhưng trở nên dư thừa sau khi khám phá ra chúng. Chúng tôi đầu tiên khám phá tỷ lệ pruning quan trọng, đó là tỷ lệ pruning tối đa có thể gây ra grokking (Hình 17-a). Trong trường hợp này (Hình 17-a), chúng tôi nhận ra rằng tỷ lệ pruning quan trọng nằm giữa 0.8 và 0.9 bởi vì nếu tỷ lệ pruning tăng lên 0.9, độ chính xác kiểm tra giảm đột ngột. Do đó, chúng tôi dần dần tăng tỷ lệ pruning với bước 0.01 từ 0.8 và thấy rằng k=0.81 là tỷ lệ pruning quan trọng. Sau đó chúng tôi so sánh hành vi của grokked ticket mà không có weight decay (α=0.0) và mô hình cơ sở. Hình 17-b cho thấy kết quả của các thí nghiệm. Như được thể hiện trong hình, độ chính xác kiểm tra đạt khái quát hóa hoàn hảo mà không cần weight decay. Kết quả cho thấy rằng grokked ticket với tỷ lệ pruning quan trọng không cần bất kỳ weight decay nào trong quá trình tối ưu hóa.

--- TRANG 25 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

I Trực quan hóa Trọng số và grokked ticket
Để tiết lộ đặc điểm của grokked ticket, chúng tôi trực quan hóa các ma trận trọng số và các mask tương ứng của grokked ticket, như được thể hiện trong Hình 18. Tác vụ được xem xét là Phép cộng Modular, được triển khai sử dụng kiến trúc MLP sau:
softmax(σ((Ea+Eb)Win)WoutWunemb ), (4)
trong đó Ea và Eb là các embedding đầu vào, Win,Wout, và Wunemb là các ma trận trọng số tương ứng, và σ biểu thị hàm kích hoạt. Kiến trúc này mô hình mối quan hệ giữa các đầu vào trong tác vụ Phép cộng Modular.

Hình 18 trực quan hóa các ma trận trọng số đã học WE,Winproj,Woutproj, và WU (hàng trên) sau khi khái quát hóa, cũng như các mask tương ứng từ grokked ticket (hàng dưới). Các ma trận trọng số thể hiện các mẫu tuần hoàn, phản ánh cấu trúc tốt đã học trong quá trình huấn luyện. Hơn nữa, các mask grokked ticket phù hợp với những đặc điểm tuần hoàn này, cho thấy rằng grokked ticket đã thu thập thành công các cấu trúc có lợi cho tác vụ Phép cộng Modular. Những kết quả này nêu bật khả năng của grokked ticket trong việc khám phá các mẫu có ý nghĩa đóng góp vào hiệu suất của mô hình.

Để so sánh, Hình 19 cho thấy trực quan hóa các mask (cấu trúc) thu được bởi các phương pháp pruning-at-initialization (PaI): Random, GraSP, SNIP, và SynFlow. Không giống như kết quả được thể hiện trong Hình 18, những phương pháp này không thể hiện cấu trúc tuần hoàn. Sự so sánh này nêu bật sự vượt trội của grokked ticket trong việc thu thập cấu trúc thuận lợi hơn cho tác vụ Phép cộng Modular, tiếp tục nhấn mạnh lợi thế của nó so với các phương pháp PaI truyền thống.

--- TRANG 26 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

--- TRANG 27 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)

J Các Metric Thuộc tính Đồ thị
Trong mục này, chúng tôi giới thiệu các metric lý thuyết đồ thị được sử dụng trong nghiên cứu của chúng tôi: ramanujan gap, weighted spectral gap, và hai biện pháp được suy ra từ việc xem mạng đã pruned như một đồ thị quan hệ — độ dài đường đi trung bình và hệ số clustering.

Đồ thị Ramanujan. Đồ thị ramanujan được biết đến vì kết hợp độ thưa thớt với kết nối mạnh. Xem xét đồ thị k-regular, trong đó mỗi nút có degree k. Các eigenvalue của ma trận kề A được sắp xếp như λ0≥λ1≥λ2≥...≥λn.
Ở đây, eigenvalue lớn nhất λ0 bằng k, và nhỏ nhất λn bằng −k; đây là các eigenvalue tầm thường. Đồ thị được gọi là đồ thị ramanujan nếu tất cả các eigenvalue không tầm thường λi (tức là những eigenvalue có |λi|≠k) thỏa mãn
max|λi|≠k |λi|≤√(2k−1).
Những đồ thị như vậy thưa thớt nhưng thể hiện sự khuếch tán thông tin hiệu quả.

Ramanujan Gap. Ramanujan gap đo lường mức độ eigenvalue không tầm thường lớn nhất của đồ thị tiếp cận giới hạn lý thuyết √(2k−1). Mặc dù điều này ban đầu được định nghĩa cho các đồ thị k-regular nghiêm ngặt, Hoang et al. (2023b;a) mở rộng nó đến các đồ thị không đều (như thường xuất hiện trong các mạng đã pruned không có cấu trúc) bằng cách thay thế k bằng degree trung bình davg:
∆r=√(2davg−1)−μ̂(G),
trong đó μ̂(G) là eigenvalue không tầm thường lớn nhất về giá trị tuyệt đối. Đối với đồ thị hai phần, eigenvalue lớn nhất và nhỏ nhất có cùng magnitude đối xứng, làm cho eigenvalue lớn thứ ba trở thành lựa chọn tự nhiên cho μ̂(G). Hoang et al. (2023a) báo cáo mối tương quan âm giữa ∆r và hiệu suất.

Weighted Spectral Gap Spectral gap của ma trận kề thường đề cập đến sự khác biệt giữa eigenvalue lớn nhất λ0 và eigenvalue lớn thứ hai λ1:
Spectral Gap =λ0−λ1.
Hoang et al. (2023a) mở rộng điều này thành weighted spectral gap bằng cách kết hợp trọng số đã học (ví dụ, từ huấn luyện) vào ma trận kề. Điều này phản ánh tốt hơn cách pruning hoặc các sửa đổi cấu trúc khác thay đổi kết nối hiệu quả, thường tiết lộ liên kết mạnh với hiệu suất mô hình.

Ngoài các biện pháp dựa trên eigenvalue, việc diễn giải mạng đã pruned như một đồ thị quan hệ (You et al., 2020) cho phép tính toán các metric khoa học mạng đã được thiết lập:

Độ dài Đường đi Trung bình. Độ dài đường đi trung bình L được định nghĩa là khoảng cách trung bình giữa tất cả các cặp nút:
L=1/[N(N−1)] ∑i≠j d(i,j),
trong đó d(i,j) là khoảng cách đường đi ngắn nhất giữa nút i và j, và N là số lượng nút trong đồ thị. L nhỏ hơn ngụ ý ít hops hơn trung bình để di chuyển giữa các nút, cho thấy mạng nhỏ gọn hơn.

Hệ số Clustering. Chúng tôi sử dụng hệ số clustering trung bình C để đặc trưng cho mức độ dày đặc của các nút có xu hướng tạo thành tam giác với các nút lân cận. Hệ số clustering địa phương Ci của nút i được cho bởi
Ci=2ei/[ki(ki−1)],
trong đó ki là degree của nút i và ei là số lượng cạnh giữa ki nút lân cận của nó. Hệ số clustering trung bình sau đó là
C=1/N ∑i=1N Ci.
Giá trị cao hơn của C cho thấy nhóm địa phương mạnh hoặc cấu trúc cộng đồng.

Thú vị, nghiên cứu trước đây (You et al., 2020) đã cho thấy rằng khi tập trung vào hiệu suất, cấu trúc đồ thị tốt nhất không nhất thiết đẩy cả độ dài đường đi trung bình và hệ số clustering đến cực trị của chúng (hoặc quá nhỏ hoặc quá lớn). Thay vào đó, các mạng hoạt động tốt nhất thường chiếm giữ chế độ vừa phải hoặc cân bằng của L và C.

K Tiến hóa Thuộc tính Đồ thị cho mỗi lớp
Hình 20-(a) minh họa cách weighted spectral gap (trái) và ramanujan gap (phải) tiến hóa trong suốt quá trình huấn luyện qua các lớp khác nhau, cụ thể phân tích Wemb,Win,Wout,Wunemb.

Chúng tôi quan sát thấy rằng weighted spectral gap thể hiện sự tăng mạnh trong giai đoạn ghi nhớ, cho thấy sự tách biệt rõ nét giữa eigenvalue lớn nhất và lớn thứ hai trong khi mô hình đang overfitting. Điều này gợi ý rằng, trong giai đoạn ghi nhớ, trọng số bị chi phối bởi một thành phần cụ thể, tương ứng với eigenvector dẫn đầu. Tuy nhiên, khi huấn luyện tiến triển và khái quát hóa cải thiện, spectral gap thu hẹp, phản ánh việc sử dụng không gian tham số cân bằng hơn. Trong khi đó, ramanujan gap liên tục giảm từ đầu, phù hợp với các nghiên cứu trước đây (Hoang et al., 2023a) báo cáo xu hướng giảm nhất quán. Điều này gợi ý một sự chuyển đổi cấu trúc trong đồ thị được tạo ra bởi các ma trận trọng số, có thể tương ứng với sự chuyển đổi của mạng từ ghi nhớ sang khái quát hóa.

Hình 20-(b) trình bày các metric được suy ra từ việc xem toàn bộ mạng như một đồ thị quan hệ (You et al., 2020). Độ dài đường đi trung bình (trái) tăng cùng với sự tăng trong độ chính xác kiểm tra, sau đó ổn định ở một phạm vi vừa phải. Điều này ngụ ý rằng khi mạng tìm kiếm giải pháp có thể khái quát hóa (tức là khám phá Grokked ticket), kết nối hiệu quả của nó trở nên thưa thớt hơn. Một khi giải pháp được tìm thấy, độ dài đường đi ổn định, gợi ý một phân bổ tham số có cấu trúc và hiệu quả. Ngược lại, hệ số clustering (phải) giảm khi độ chính xác kiểm tra cải thiện, trước khi ổn định ở một giá trị trung gian. Sự giảm này gợi ý rằng trong quá trình khám phá mẫu khái quát hóa, kết nối địa phương trong đồ thị yếu đi, nhưng không sụp đổ hoàn toàn. Thay vào đó, nó hội tụ đến một chế độ có cấu trúc nơi một mức độ địa phương nào đó được duy trì trong khi tránh clustering quá mức, phù hợp với các phát hiện trước đây (You et al., 2020).

--- TRANG 29 ---
Xuất bản trong Transactions on Machine Learning Research (05/2025)
