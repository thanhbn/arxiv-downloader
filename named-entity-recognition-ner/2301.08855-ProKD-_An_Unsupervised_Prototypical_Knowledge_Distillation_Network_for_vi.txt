ProKD: Mạng Chưng Cất Tri Thức Nguyên Mẫu Không Giám Sát cho Nhận Dạng Thực Thể Có Tên Đa Ngôn Ngữ Không Tài Nguyên

Ling Ge1, Chunming Hu1,2,*, Guanghui Ma1, Hong Zhang3, Jihong Liu4
1Trường Khoa học Máy tính và Kỹ thuật, Đại học Beihang, Bắc Kinh, Trung Quốc
2Trường Phần mềm, Đại học Beihang, Bắc Kinh, Trung Quốc
3Đội Ứng phó Khẩn cấp Mạng Máy tính Quốc gia / Trung tâm Điều phối Trung Quốc, Bắc Kinh, Trung Quốc
4Trường Kỹ thuật Cơ khí và Tự động hóa, Đại học Beihang, Bắc Kinh, Trung Quốc
fgeling, hucm, maguanghui, ryukeiko g@buaa.edu.cn, zhangh@isc.org.cn

Tóm tắt
Đối với nhận dạng thực thể có tên (NER) trong các ngôn ngữ không có tài nguyên, việc sử dụng các phương pháp chưng cất tri thức để chuyển giao tri thức độc lập ngôn ngữ từ các ngôn ngữ nguồn có tài nguyên phong phú sang các ngôn ngữ không có tài nguyên là một phương tiện hiệu quả. Thông thường, các cách tiếp cận này áp dụng kiến trúc thầy-trò, trong đó mạng thầy được huấn luyện trong ngôn ngữ nguồn, và mạng trò tìm cách học hỏi tri thức từ mạng thầy và được kỳ vọng hoạt động tốt trong ngôn ngữ đích. Mặc dù đạt được hiệu suất ấn tượng bởi những phương pháp này, chúng tôi lập luận rằng chúng có hai hạn chế. Thứ nhất, mạng thầy không thể học hiệu quả tri thức độc lập ngôn ngữ được chia sẻ giữa các ngôn ngữ do sự khác biệt trong phân phối đặc trưng giữa ngôn ngữ nguồn và ngôn ngữ đích. Thứ hai, mạng trò thu được tất cả tri thức của nó từ mạng thầy và bỏ qua việc học tri thức cụ thể của ngôn ngữ đích. Không mong muốn, những hạn chế này sẽ cản trở hiệu suất của mô hình trong ngôn ngữ đích. Bài báo này đề xuất một mạng chưng cất tri thức nguyên mẫu không giám sát (ProKD) để giải quyết những vấn đề này. Cụ thể, ProKD trình bày một phương pháp căn chỉnh nguyên mẫu dựa trên học đối lập để đạt được căn chỉnh đặc trưng lớp bằng cách điều chỉnh khoảng cách giữa các nguyên mẫu trong ngôn ngữ nguồn và ngôn ngữ đích, tăng cường khả năng của mạng thầy để thu được tri thức độc lập ngôn ngữ. Ngoài ra, ProKD giới thiệu một phương pháp tự huấn luyện nguyên mẫu để học cấu trúc nội tại của ngôn ngữ bằng cách huấn luyện lại mạng trò trên dữ liệu đích sử dụng thông tin khoảng cách của các mẫu từ nguyên mẫu, từ đó tăng cường khả năng của mạng trò để thu được tri thức cụ thể của ngôn ngữ. Các thí nghiệm rộng rãi trên ba bộ dữ liệu NER đa ngôn ngữ chuẩn chứng minh tính hiệu quả của cách tiếp cận của chúng tôi.

Giới thiệu
Nhận dạng Thực thể Có tên (NER) là một nhiệm vụ con cơ bản của trích xuất thông tin nhằm định vị và phân loại các đoạn văn bản thành các lớp thực thể được định nghĩa trước như địa điểm, tổ chức, v.v. (Ma et al. 2022b). Nó thường được sử dụng như một thành phần thiết yếu cho các nhiệm vụ như trả lời câu hỏi (Cao et al. 2022) và phân giải coreference (Ma et al. 2022a). Mặc dù hiệu suất ấn tượng gần đây đạt được bởi các phương pháp NER dựa trên học sâu, những phương pháp có giám sát này bị hạn chế chỉ một vài ngôn ngữ có nhãn thực thể phong phú, như tiếng Anh, do yêu cầu một lượng lớn dữ liệu huấn luyện được chú thích bởi con người. Ngược lại, tổng số ngôn ngữ hiện đang được sử dụng trên toàn thế giới là khoảng 7.000¹, phần lớn trong số đó chứa dữ liệu được gán nhãn hạn chế hoặc không có, hạn chế việc áp dụng các phương pháp hiện có cho những ngôn ngữ này (Wu et al. 2020c,b). Do đó, học chuyển giao đa ngôn ngữ đang nhận được sự chú ý ngày càng tăng từ các nhà nghiên cứu, có thể tận dụng tri thức từ các ngôn ngữ có tài nguyên cao (nguồn) (ví dụ, tiếng Anh) với nhãn thực thể phong phú để khắc phục vấn đề khan hiếm dữ liệu của các ngôn ngữ có tài nguyên thấp (không có) (đích) (Liu et al. 2021). Đặc biệt, bài báo này tập trung vào kịch bản không có tài nguyên, nơi không có dữ liệu được gán nhãn trong ngôn ngữ đích.

Để cải thiện hiệu suất của NER đa ngôn ngữ không có tài nguyên, các nhà nghiên cứu đã tiến hành nghiên cứu chuyên sâu và đề xuất các cách tiếp cận khác nhau (Jain et al. 2019; Wu et al. 2020c; Pfeiffer et al. 2020). Trong số này, các cách tiếp cận dựa trên chưng cất tri thức (Chen et al. 2021; Wu et al. 2020b,a) gần đây đã cho thấy kết quả khuyến khích. Những cách tiếp cận này thông thường huấn luyện một mạng NER thầy sử dụng dữ liệu ngôn ngữ nguồn và sau đó tận dụng các nhãn giả mềm được tạo ra bởi mạng thầy cho dữ liệu ngôn ngữ đích để huấn luyện mạng NER trò. Theo cách này, mạng trò được kỳ vọng sẽ học tri thức độc lập ngôn ngữ từ mạng thầy và hoạt động tốt trên dữ liệu đích không được gán nhãn (Hinton, Vinyals, and Dean 2015).

Trong khi tiến bộ đáng kể đã được đạt được bởi các cách tiếp cận dựa trên chưng cất tri thức cho NER đa ngôn ngữ, chúng tôi lập luận rằng những cách tiếp cận này vẫn có hai hạn chế. Thứ nhất, chưng cất tri thức phụ thuộc nhiều vào tri thức độc lập ngôn ngữ được chia sẻ được mạng thầy thu được qua các ngôn ngữ. Như đã biết, có sự khác biệt trong phân phối đặc trưng giữa ngôn ngữ nguồn và ngôn ngữ đích, các kỹ thuật hiện tại chỉ sử dụng ngôn ngữ nguồn để huấn luyện mạng thầy. Kết quả là, mạng thầy có xu hướng học tri thức cụ thể của ngôn ngữ nguồn và không thể nắm bắt hiệu quả tri thức độc lập ngôn ngữ được chia sẻ. Thứ hai, dưới cơ chế học chưng cất tri thức, mạng trò nhằm khớp với các nhãn giả mềm được tạo ra bởi mạng thầy cho ngôn ngữ đích. Do đó, mạng trò thu được tất cả tri thức của nó từ mạng thầy và bỏ qua việc thu được tri thức cụ thể của ngôn ngữ đích. Không mong muốn, hai ràng buộc này sẽ cản trở hiệu suất của mô hình trong ngôn ngữ đích.

Trong bài báo này, chúng tôi đề xuất một mạng Chưng cất Tri thức Nguyên mẫu không giám sát (ProKD), sử dụng căn chỉnh nguyên mẫu dựa trên học đối lập và tự huấn luyện nguyên mẫu để giải quyết hai hạn chế trên, tương ứng. Cụ thể, chúng tôi dựa vào việc thực hiện căn chỉnh cấp lớp giữa ngôn ngữ nguồn và ngôn ngữ đích trong không gian ngữ nghĩa để tăng cường khả năng của mạng thầy để nắm bắt tri thức độc lập ngôn ngữ. Chúng tôi lập luận rằng căn chỉnh cấp lớp có thể thu hẹp khoảng cách trong phân phối đặc trưng và buộc mạng thầy học tốt hơn ngữ nghĩa được chia sẻ của các lớp thực thể qua các ngôn ngữ (Van Nguyen et al. 2021; Xu et al. 2022). Để làm điều này, chúng tôi chọn nguyên mẫu (Snell et al. 2017), tức là các trung tâm đặc trưng theo lớp, thay vì các mẫu, cho căn chỉnh cấp lớp vì nguyên mẫu có khả năng chống lại các ngoại lệ và thân thiện với các nhiệm vụ mất cân bằng lớp (Qiu et al. 2021; Zhang et al. 2021). Để kéo các nguyên mẫu của cùng một lớp lại gần nhau và đẩy các nguyên mẫu của các lớp khác nhau ra xa qua các ngôn ngữ, chúng tôi tận dụng học đối lập cổ điển (Chen et al. 2020) để điều chỉnh khoảng cách giữa các nguyên mẫu lớp. Do đó, việc căn chỉnh biểu diễn cấp lớp giữa ngôn ngữ nguồn và ngôn ngữ đích được đạt được.

Hơn nữa, chúng tôi trình bày một phương pháp tự huấn luyện nguyên mẫu để tăng cường khả năng của mạng trò để thu được tri thức cụ thể của ngôn ngữ đích. Đặc biệt, chúng tôi thiết lập nhãn giả cứng cho các mẫu ngôn ngữ đích không được gán nhãn dựa trên khoảng cách tương đối có giá trị softmax của chúng, tức là xác suất nguyên mẫu, đối với tất cả các nguyên mẫu và sau đó huấn luyện lại mạng sử dụng những nhãn giả này. Vì các nguyên mẫu đại diện chính xác cho phân phối phân cụm cơ bản của dữ liệu, tự huấn luyện nguyên mẫu cho phép mạng trò học cấu trúc nội tại của ngôn ngữ đích (Zhang et al. 2021), từ đó tiết lộ tri thức cụ thể của ngôn ngữ, chẳng hạn như sở thích nhãn của token. Ngoài ra, khi tính toán các nhãn giả cứng, các xác suất phân phối lớp được tạo ra bởi mạng thầy được kết hợp vào các xác suất nguyên mẫu để cải thiện chất lượng của các nhãn giả cứng và tạo điều kiện thuận lợi cho tự huấn luyện.

Tóm lại, chúng tôi đóng góp bốn điểm: (1) Chúng tôi đề xuất một mô hình ProKD cho nhiệm vụ NER đa ngôn ngữ không có tài nguyên, có thể cải thiện khả năng tổng quát hóa của mô hình đối với ngôn ngữ đích. (2) Chúng tôi đề xuất một phương pháp căn chỉnh nguyên mẫu dựa trên học đối lập để tăng cường khả năng của mạng thầy để thu được tri thức độc lập ngôn ngữ. (3) Chúng tôi đề xuất một phương pháp tự huấn luyện nguyên mẫu để tăng cường khả năng của mạng trò để thu được tri thức cụ thể của ngôn ngữ đích. (4) Kết quả thí nghiệm trên sáu ngôn ngữ đích xác nhận tính hiệu quả của cách tiếp cận của chúng tôi.

Nghiên cứu liên quan
NER Đa ngôn ngữ
Nghiên cứu hiện tại về NER đa ngôn ngữ với không có tài nguyên rơi vào ba nhánh chính. Các phương pháp dựa trên dịch thuật dựa vào dịch máy và chiếu nhãn (Xie et al. 2018a; Jain et al. 2019) để xây dựng dữ liệu huấn luyện giả cho ngôn ngữ đích, tất cả đều liên quan đến chi phí con người cao và giới thiệu tiếng ồn nhãn. Các phương pháp chuyển giao trực tiếp dựa vào việc huấn luyện một mô hình NER với ngôn ngữ nguồn và trực tiếp chuyển giao nó sang ngôn ngữ đích (Wu and Dredze 2019; Wu et al. 2020c; Pfeiffer et al. 2020). Những cách tiếp cận này không thể khai thác thông tin từ ngôn ngữ đích không được gán nhãn, dẫn đến hiệu suất đa ngôn ngữ không tối ưu. Các phương pháp dựa trên chưng cất tri thức khuyến khích mạng trò học tri thức độc lập ngôn ngữ từ mạng thầy. Cụ thể, Wu et al. (2020a) chưng cất tri thức trực tiếp từ nhiều ngôn ngữ nguồn. AdvPicker (Chen et al. 2021) tận dụng học đối kháng để chọn dữ liệu đích nhằm giảm bớt việc mô hình quá khớp với dữ liệu nguồn. Chúng tôi lập luận rằng các cách tiếp cận trên không thể học hiệu quả tri thức độc lập ngôn ngữ được chia sẻ và bỏ qua việc thu được tri thức cụ thể của ngôn ngữ đích.

Chưng cất Tri thức
Chưng cất tri thức cho phép chuyển giao tri thức từ mạng thầy sang mạng trò (Hinton, Vinyals, and Dean 2015), trong đó mạng trò được tối ưu hóa bằng cách khớp các nhãn mềm được tạo ra bởi mạng thầy đã được huấn luyện. Vì các mục tiêu mềm có giá trị entropy cao, chúng cung cấp nhiều thông tin hơn mỗi trường hợp huấn luyện so với các mục tiêu cứng (Hinton, Vinyals, and Dean 2015), mạng trò có thể học từ mạng thầy và hoạt động tốt trên dữ liệu không được gán nhãn. Chưng cất tri thức đạt được kết quả đáng kể trong các nhiệm vụ khác nhau như nén mô hình (Liu et al. 2022), phân loại hình ảnh (Hinton, Vinyals, and Dean 2015), tạo sinh đối thoại (Peng et al. 2019), dịch máy (Weng et al. 2020), v.v. Trong bài báo này, chúng tôi chọn chưng cất tri thức làm khung cơ bản của cách tiếp cận được đề xuất của chúng tôi cho NER đa ngôn ngữ không có tài nguyên.

Phương pháp
Nhiệm vụ NER được mô hình hóa như một vấn đề gán nhãn chuỗi trong bài báo này, tức là, cho một câu X={x₀, ..., xᵢ, ..., xₗ}, mô hình NER được kỳ vọng sẽ tạo ra một chuỗi nhãn Y={y₀, ..., yᵢ, ..., yₗ}, trong đó yᵢ biểu thị lớp thực thể tương ứng với token xᵢ. Theo cài đặt của các nghiên cứu trước đây (Wu et al. 2020a,b), cho một bộ dữ liệu ngôn ngữ nguồn được gán nhãn, {(X^s_m, Y^s_m)}^{n_s}_{m=1} ⊆ D_s, và một bộ dữ liệu ngôn ngữ đích không được gán nhãn, {(X^t_m)}^{n_t}_{m=1} ⊆ D_t, NER đa ngôn ngữ không có tài nguyên nhằm huấn luyện một mô hình với hai bộ dữ liệu trên và kỳ vọng mô hình sẽ có hiệu suất tốt trên dữ liệu ngôn ngữ đích.

Kiến trúc Tổng thể
Trong phần này, chúng tôi mô tả cách tiếp cận được đề xuất, ProKD, cho NER đa ngôn ngữ với không có tài nguyên, có kiến trúc được hiển thị trong Hình 1 và Hình 2. Cốt lõi của ProKD là một khung chưng cất tri thức bao gồm một mạng thầy và một mạng trò. Chi tiết hơn, mạng thầy sử dụng một phương pháp căn chỉnh lớp nguyên mẫu dựa trên học đối lập, tăng cường khả năng thu được tri thức độc lập ngôn ngữ. Mạng trò sử dụng một cách tiếp cận tự huấn luyện nguyên mẫu kết hợp với xác suất phân phối lớp của mạng thầy, tăng cường khả năng học tri thức cụ thể của ngôn ngữ.

NER Đa ngôn ngữ Không có Tài nguyên thông qua Chưng cất Tri thức
Các phương pháp dựa trên chưng cất tri thức cho NER đa ngôn ngữ không có tài nguyên thông thường tuân theo một quy trình huấn luyện hai giai đoạn. Đầu tiên, mạng thầy được huấn luyện với dữ liệu nguồn được gán nhãn, và sau đó tri thức độc lập ngôn ngữ được chưng cất sang mạng trò.

Cho một chuỗi X^s_m = {x^s_0, ..., x^s_i, ..., x^s_L} từ dữ liệu ngôn ngữ nguồn, bộ mã hóa f của mạng thầy có thể ánh xạ nó vào không gian ẩn và xuất ra các biểu diễn H^s_m = {h^s_0, ..., h^s_i, ..., h^s_L}. Theo các nghiên cứu trước đây (Wu et al. 2020a,b), chúng tôi áp dụng BERT đa ngôn ngữ (viết tắt là mBERT) (Devlin et al. 2019) làm bộ mã hóa đặc trưng. Sau đó, chúng tôi tận dụng một bộ phân loại với hàm softmax để có được đầu ra p^s_i cho mỗi token x^s_i, và mất mát entropy chéo cho mạng thầy có thể được công thức hóa như:

L_CE(θ_tea) = (1/n_s) ∑_{(x^s,y^s)∈D_s} ∑_{i=0}^L y^s_i log(p^s_i)   (1)

trong đó θ_tea là các tham số của mạng thầy cần được tối ưu hóa, n_s là số câu trong bộ dữ liệu D_s, và y^s_i đại diện cho nhãn vàng của token x^s_i.

Có lợi từ không gian đặc trưng được chia sẻ của mBERT được huấn luyện trước và tri thức nhiệm vụ từ dữ liệu nguồn được gán nhãn, chúng tôi có thể trực tiếp sử dụng mạng thầy để suy luận các xác suất lớp p^t_i của mỗi token trong một chuỗi X^t_m từ bộ dữ liệu không được gán nhãn D_t. Sau đó, mạng trò, bao gồm một bộ mã hóa đặc trưng mBERT và một bộ phân loại với hàm softmax, được huấn luyện sử dụng những xác suất lớp này như "mục tiêu mềm" trên bộ dữ liệu không được gán nhãn. Để xấp xỉ các xác suất p^t_i, mục tiêu huấn luyện cho mạng trò có thể được công thức hóa như:

L_KD(θ_stu) = (1/|D_t|) ∑_{x∈D_t} ∑_{i=1}^L (p^t_i - q^t_i)^2   (2)

trong đó p^t_i và q^t_i biểu thị phân phối xác suất được tạo ra bởi mạng thầy và mạng trò cho x^t_i, tương ứng. Và ở đây, theo các nghiên cứu trước đây (Yang et al. 2020; Wu et al. 2020a), chúng tôi sử dụng mất mát MSE để đo lường sự khác biệt dự đoán của hai mạng.

Căn chỉnh Theo lớp Nguyên mẫu
Ở đây, chúng tôi trình bày phương pháp của chúng tôi, căn chỉnh theo lớp nguyên mẫu, để tăng cường khả năng của mạng thầy để thu được tri thức độc lập ngôn ngữ.

Do không có chú thích trên dữ liệu ngôn ngữ đích, việc căn chỉnh theo lớp giữa ngôn ngữ nguồn và ngôn ngữ đích không phải là tầm thường. Để giải quyết vấn đề này, như thể hiện trong Hình 1, chúng tôi đầu tiên tính toán các nguyên mẫu lớp đích bằng các xác suất phân phối lớp được tạo ra bởi mạng thầy trong dữ liệu đích, và sau đó tận dụng việc căn chỉnh nguyên mẫu giữa hai ngôn ngữ trên để đạt được căn chỉnh theo lớp. Chúng tôi sử dụng căn chỉnh nguyên mẫu thay vì căn chỉnh mẫu vì nguyên mẫu có khả năng chống lại các ngoại lệ (Zhang et al. 2021), và nó có thể giảm bớt tác động tiêu cực của tiếng ồn (Xie et al. 2018b) được giới thiệu bởi mạng thầy cho dữ liệu đích. Ngoài ra, nguyên mẫu đối xử với tất cả các lớp như nhau (Zhang et al. 2021), điều này rất quan trọng đối với nhiệm vụ NER, vì các mẫu loại không phải thực thể chiếm phần lớn tổng số mẫu.

Cụ thể, đối với ngôn ngữ nguồn, chúng tôi đầu tiên có được biểu diễn token h^s_i của mỗi token x^s_i sử dụng mBERT, và sau đó, với sự trợ giúp của các nhãn vàng, chúng tôi trực tiếp tính toán biểu diễn trung bình của các mẫu token có cùng nhãn và coi nó như nguyên mẫu lớp:

C^s_k = (1/n^s_k) ∑_{(X^s,Y^s)∈D_s} ∑_{i=0}^L I[y^s_i = k] h^s_i   (3)

trong đó k biểu thị một nhãn lớp thực thể, I là một hàm chỉ báo, và n^s_k đại diện cho số mẫu thuộc lớp k trong ngôn ngữ nguồn.

Đối với ngôn ngữ đích, chúng tôi sử dụng cùng một phương pháp để có được biểu diễn h^t_i của mỗi token đích x^t_i. Vì dữ liệu đích không được gán nhãn, để giảm bớt sự không chắc chắn của việc tính toán nguyên mẫu lớp, chúng tôi sử dụng đầu ra của bộ phân loại thầy để ước tính các xác suất cho token hiện tại thuộc về mỗi lớp. Coi những xác suất này như trọng số, chúng tôi tổng hợp các biểu diễn của tất cả các token đích để có được nguyên mẫu lớp đích, có thể được biểu diễn như:

C^t_k = (∑_{X∈D_t} ∑_{i=0}^L p^t_{i,k} h^t_i) / (∑_{X∈D_t} ∑_{i=0}^L p^t_{i,k})   (4)

trong đó p^t_{i,k} đại diện cho xác suất mà token x_i thuộc về lớp k.

Việc tính toán nguyên mẫu lớp liên quan đến tất cả các mẫu, dẫn đến chi phí tính toán cao. Để giảm độ phức tạp tính toán trong khi đảm bảo tính ổn định của các cập nhật, chúng tôi sử dụng phương pháp trung bình di chuyển (Xie et al. 2018b) để cập nhật các nguyên mẫu nguồn và đích:

C^{s(t)}_{k,cur} = λC^{s(t)}_{k,cur} + (1-λ)C^{s(t)}_{k,cur-1}   (5)

trong đó λ ∈ (0,1) là hệ số trung bình di chuyển, cur biểu thị thời điểm hiện tại và cur-1 chỉ ra thời điểm trước đó. Trong triển khai thực tế, các nguyên mẫu nguồn được cập nhật một lần mỗi epoch, trong khi các nguyên mẫu đích được cập nhật một lần mỗi batch.

Sau khi có được tất cả các nguyên mẫu lớp, chúng tôi tận dụng học đối lập cổ điển để điều chỉnh khoảng cách giữa các nguyên mẫu trong không gian đặc trưng để căn chỉnh theo lớp. Đối với các nguyên mẫu từ dữ liệu nguồn và đích có cùng lớp, chúng tôi coi một cái như neo (ví dụ, C^s_i) và cái khác như mẫu dương của neo (ví dụ, C^t_i), trong khi phần còn lại của các nguyên mẫu được coi là mẫu âm (được đánh dấu là C^s_{i,neg}). Sau đó, mất mát căn chỉnh lớp được trình bày như:

L_CA(θ_tea) = -log ∑_{i=1}^{num} [exp(z^s_i · z^t_i/τ_1)] / [∑_{neg}exp(z^s_i · z^s_{i,neg}/τ_1) + ∑_{neg}exp(z^t_i · z^t_{i,neg}/τ_1)]   (6)

trong đó z^s_i, z^t_i, z^s_{i,neg} và z^t_{i,neg} là chuẩn hóa l2 của C^s_i, C^t_i, C^s_{i,neg}, C^t_{i,neg}, tương ứng, C^t_{i,neg} biểu thị các mẫu âm của C^t_i, τ_1 là tham số nhiệt độ, và num là số lớp thực thể.

Theo cách này, chúng tôi có thể kéo các nguyên mẫu nguồn và đích của cùng một lớp lại gần nhau và đẩy các nguyên mẫu nguồn và đích của các lớp khác nhau ra xa. Cuối cùng, chúng tôi có được tổng mất mát L(θ_tea) cho mạng thầy, bao gồm mất mát entropy chéo và mất mát căn chỉnh lớp:

L(θ_tea) = L_CE(θ_tea) + L_CA(θ_tea)   (7)

Tự huấn luyện Nguyên mẫu
Ở đây, chúng tôi trình bày cách tiếp cận tự huấn luyện nguyên mẫu của chúng tôi với dữ liệu ngôn ngữ đích không được gán nhãn, để tăng cường khả năng của mạng trò để học tri thức cụ thể của ngôn ngữ đích.

Cụ thể, chúng tôi dựa vào học nguyên mẫu để tạo ra một cách lặp lại các nhãn giả cứng cho các mẫu ngôn ngữ đích không được gán nhãn và tận dụng những nhãn cứng này để tiến hành tự huấn luyện trên dữ liệu đích. Điều này bởi vì các nguyên mẫu có thể cảm nhận được phân phối phân cụm cơ bản của dữ liệu, phản ánh cơ bản cấu trúc nội tại của dữ liệu và sự khác biệt nội tại qua dữ liệu (Zhang et al. 2021), điều này tạo điều kiện thuận lợi cho việc học tri thức cụ thể của ngôn ngữ, chẳng hạn như sở thích nhãn của một token.

Để thu được các nguyên mẫu lớp đích, chúng tôi đầu tiên có được các biểu diễn ẩn và xác suất dự đoán thông qua mạng trò, tương ứng, và sau đó tận dụng phương trình tính toán và cập nhật nguyên mẫu chính xác (Phương trình 4 và Phương trình 5) như mạng thầy để có được các nguyên mẫu lớp C^t. Sau đó, một phân phối xác suất lớp α_i dựa trên nguyên mẫu được tính toán bằng cách tận dụng khoảng cách đặc trưng của mẫu w.r.t các nguyên mẫu lớp:

α^t_{i,k} = exp(-||h^t_i - C^t_k||/τ_2) / ∑_{k'} exp(-||h^t_i - C^t_{k'}||/τ_2)   (8)

trong đó τ_2 là nhiệt độ softmax, và α^t_{i,k} đại diện cho xác suất softmax của mẫu x_i thuộc về lớp thứ k. Như quan sát, nếu một biểu diễn đặc trưng h^t_i ở xa nguyên mẫu C^t_k, xác suất của đặc trưng này cho lớp k sẽ rất thấp. Chúng tôi chuyển đổi α^t_i thành một nhãn giả cứng ŷ^t_i dựa trên công thức sau:

ŷ^t_i = Φ(α^t_i)   (9)

trong đó Φ biểu thị hàm chuyển đổi.

Một cách trực quan, chúng tôi có thể sử dụng những nhãn giả cứng này để tự huấn luyện. Tuy nhiên, một câu hỏi tự nhiên sau đó phát sinh. Tự huấn luyện nguyên mẫu về cơ bản là học biểu diễn dựa trên cụm và sẽ không thể tránh khỏi việc giới thiệu nhãn không chính xác trong gán nhãn giả. Ví dụ, khi một mẫu ở xa nguyên mẫu mà nó thuộc về, mạng trò có thể gán nhãn sai cho mẫu này (Snell et al. 2017). Để giảm bớt vấn đề này, chúng tôi kết hợp xác suất nguyên mẫu α^t_{i,k} ở trên với xác suất đầu ra của thầy p^t_{i,k}, để tạo ra một nhãn giả mềm lai β^t_{i,k}:

β^t_{i,k} = γα^t_{i,k} + (1-γ)p^t_{i,k}   (10)

trong đó γ là một yếu tố kết hợp.

Vì thầy đã được huấn luyện có tri thức ngữ nghĩa chung của các lớp, p^t_{i,k} có thể được coi như tri thức tiên nghiệm, để cải thiện chất lượng của gán nhãn giả, điều này cho thấy những ưu điểm hấp dẫn trong các nghiên cứu trước đây (Li, Xiong, and Hoi 2021; Zhang et al. 2021).

Lưu ý rằng, đầu ra của mạng thầy p^t_{i,k} vẫn cố định khi quá trình huấn luyện tiến hành. Lý do chúng tôi chọn p^t_{i,k} thay vì xác suất cập nhật q^t_{i,k} của học sinh, là để tránh giải pháp suy biến, do việc cập nhật đồng thời các đặc trưng và nhãn trong suốt quá trình tự huấn luyện. Sau đó, chúng tôi sử dụng β^t_i lai thay vì α^t_i để tạo ra nhãn giả cứng. Với mục đích này, học sinh có thể được huấn luyện bởi mất mát tự huấn luyện truyền thống (Zou et al. 2018):

L_CE(θ_stu) = ∑_{x∈D_t} ∑_{i=1}^L Φ(β^t_i) log(q^t_i)   (11)

trong đó q^t_i biểu thị phân phối xác suất được tạo ra thông qua bộ phân loại của mạng trò cho x^t_i.

Dựa trên những điều trên, mạng trò có thể có lợi từ hai khía cạnh (Hình 2): chưng cất tri thức và tự huấn luyện. Một vấn đề rất đơn giản là mạng trò có thể không đủ năng lực ở giai đoạn đầu để thực hiện tự huấn luyện hiệu quả. Để đảm bảo rằng mạng trò có thể học ngữ nghĩa lớp được chia sẻ để tự huấn luyện ở giai đoạn đầu, chúng tôi tuân theo một chiến lược học tích lũy (Zhou et al. 2020) để dần dần chuyển trọng tâm học của mô hình từ chưng cất tri thức sang tự huấn luyện sử dụng tham số điều khiển β:

β = 1 - (e/E_max)^2   (12)

trong đó E_max là số epoch huấn luyện tổng, và e là epoch hiện tại. β tự động giảm từ 1 xuống 0 với epoch tăng.

Cuối cùng, mất mát L(θ_stu) cho mạng trò có thể được biểu diễn như:

L(θ_stu) = (1-β)L_CE(θ_stu) + βL_KD(θ_stu)   (13)

Thí nghiệm và Phân tích
Bộ dữ liệu
Chúng tôi áp dụng ba bộ dữ liệu chuẩn được sử dụng rộng rãi cho thí nghiệm: CoNLL-2002 (Tiếng Tây Ban Nha và Tiếng Hà Lan) (Sang 2002), CoNLL-2003 (Tiếng Anh và Tiếng Đức) (Sang and Meulder 2003), và Wikiann (Tiếng Anh, Tiếng Ả Rập, Tiếng Hindi và Tiếng Trung) (Pan et al. 2017). Mỗi bộ dữ liệu cụ thể của ngôn ngữ có các tập huấn luyện, phát triển và đánh giá tiêu chuẩn. Thống kê cho tất cả các bộ dữ liệu được hiển thị trong Bảng 1.

Theo các nghiên cứu trước đây (Wu et al. 2020a,c), chúng tôi áp dụng word-piece (Wu et al. 2016) để tokenize các câu thành các từ phụ, sau đó được đánh dấu bởi lược đồ BIO. Dữ liệu được chú thích với bốn loại thực thể khác nhau: PER (Người), LOC (Địa điểm), ORG (Tổ chức), và MISC (Khác). Đối với tất cả các thí nghiệm, Tiếng Anh được coi là ngôn ngữ nguồn và những ngôn ngữ khác như ngôn ngữ đích tương ứng. Lưu ý rằng, CoNLL-2002/2003 chia sẻ một bộ dữ liệu Tiếng Anh chung như dữ liệu nguồn. Hơn nữa, chúng tôi huấn luyện mô hình trên tập huấn luyện ngôn ngữ nguồn, xác nhận mô hình trên tập phát triển ngôn ngữ nguồn, và đánh giá mô hình đã học trên tập thử nghiệm ngôn ngữ đích để mô phỏng kịch bản NER đa ngôn ngữ không có tài nguyên.

Chi tiết Triển khai
Chúng tôi áp dụng mBERT được huấn luyện trước (Pires et al. 2019) như bộ trích xuất đặc trưng. Theo các nghiên cứu trước đây (Wu et al. 2020a,c), chúng tôi sử dụng điểm F1 cấp token như số liệu đánh giá. Đối với tất cả các thí nghiệm, chúng tôi sử dụng bộ tối ưu Adam (Kingma and Ba 2015) với learning rate = 5e-5 cho mạng thầy và 1e-5 cho mạng trò, kích thước batch = 128, độ dài chuỗi tối đa = 128, và dropout = 0.5 theo kinh nghiệm. Chúng tôi sử dụng công nghệ tìm kiếm lưới để có được các siêu tham số tối ưu, bao gồm hệ số trung bình di chuyển λ được chọn từ {0.001, 0.005, 0.0001, 0.0005}, nhiệt độ học đối lập τ₁ được chọn từ 0.5 đến 0.9, nhiệt độ softmax τ₂ được chọn từ 0.5 đến 0.9, và yếu tố kết hợp γ được chọn từ 0.7 đến 0.9.

Theo nghiên cứu trước đây (Wu and Dredze 2020), chúng tôi chỉ xem xét từ phụ đầu tiên được tokenize bởi word-piece trong hàm mất mát của chúng tôi và đóng băng các tham số của lớp nhúng và ba lớp dưới cùng của mô hình mBERT. Ngoài ra, cách tiếp cận của chúng tôi được triển khai sử dụng PyTorch, và tất cả các tính toán được thực hiện trên GPU NVIDIA Tesla V100.

So sánh Hiệu suất
Chúng tôi so sánh cách tiếp cận được đề xuất với một số cách tiếp cận trước đây, bao gồm ba cách tiếp cận dựa trên dịch thuật: Mayhew et al. (2017), Ni and Dinu (2017) và Xie et al. (2018a), ba cách tiếp cận chuyển giao trực tiếp: Wu and Dredze (2019), Moon et al. (2019) và Wu et al. (2020c), và bốn cách tiếp cận dựa trên chưng cất tri thức: Wu et al. (2020a), UniTrans (Wu et al. 2020b), RIKD (Liang et al. 2021) và AdvPicker (Chen et al. 2021).

Kết quả được trình bày trong Bảng 2 và Bảng 3, trong đó kết quả thí nghiệm cơ sở và SOTA được lấy từ các bài báo gốc của chúng. Như quan sát, phương pháp của chúng tôi đạt được kết quả tốt nhất trên hầu hết các bộ dữ liệu. Đối với Conll2002/2003, so với hai phương pháp dựa trên chưng cất tri thức cạnh tranh, RIKD và AdvPicker, cách tiếp cận của chúng tôi cải thiện điểm F1 trung bình lần lượt 1.76% và 1.38%. Đối với Wikiann, phương pháp của chúng tôi vượt trội hơn RIKD 2.26% trung bình. Đặc biệt, đối với ngôn ngữ Đức (de), chúng tôi có được giá trị F1 là 78.9%, cao hơn 3.42% so với kết quả tốt nhất của RIKD. Và đối với ngôn ngữ Ả Rập (ar), phương pháp của chúng tôi đạt được giá trị F1 tốt nhất là 50.91%, với cải thiện 4.95% so với RIKD. Phân tích, RIKD và AdvPicker tận dụng học đối kháng và học tăng cường để chọn dữ liệu đích cho chưng cất, tương ứng, và dữ liệu được chọn có xu hướng nhất quán với ngôn ngữ nguồn trong phân phối đặc trưng. Do đó, mạng trò học trên dữ liệu này không thể thu được hiệu quả tri thức ngôn ngữ đích, dẫn đến tổng quát hóa không đủ trên ngôn ngữ đích. Ngược lại, mô hình của chúng tôi sử dụng tự huấn luyện nguyên mẫu để tăng cường khả năng của mạng trò để học ngôn ngữ đích, do đó hoạt động tốt trên ngôn ngữ đích.

Nghiên cứu Loại bỏ
Để điều tra đóng góp của các yếu tố khác nhau, chúng tôi tiến hành các thí nghiệm loại bỏ với bốn mô hình biến thể: (1) ProKD w/o CA loại bỏ việc căn chỉnh theo lớp nguyên mẫu từ mạng thầy. (2) ProKD w/o ST xóa bỏ tự huấn luyện nguyên mẫu từ mạng trò. (3) ProKD w/o PK không sử dụng tri thức tiên nghiệm từ mạng thầy trong quá trình tự huấn luyện. (4) ProKD w/o CL cắt bỏ lược đồ học tích lũy và áp dụng tham số β = 0.5 trong hàm mất mát (Phương trình 13) cho mạng trò. Như thể hiện trong Bảng 4, giá trị F1 trung bình của ProKD w/o CA giảm 2.1% so với ProKD trên Conll 2002 & 2003. Điều này cho thấy rằng căn chỉnh cấp lớp cải thiện hiệu quả khả năng tổng quát hóa của mô hình, vì căn chỉnh lớp buộc mạng thầy học tri thức độc lập ngôn ngữ từ ngôn ngữ nguồn và ngôn ngữ đích. Hiệu suất của ProKD w/o ST trong điểm F1 giảm 1.55% so với ProKD, điều này xác nhận tốt tính hiệu quả của tự huấn luyện để thu được tri thức cụ thể của ngôn ngữ đích. Đối với ProKD w/o PK, sự giảm nhẹ trong kết quả F1 so với ProKD gợi ý rằng việc kết hợp tri thức tiên nghiệm của mạng thầy có thể tăng cường chất lượng của nhãn giả trong tự huấn luyện. Ngoài ra, ProKD w/o CL cho thấy sự giảm nhẹ trong giá trị F1, điều này chứng minh rằng học chưng cất tri thức nên được thực hiện đầu tiên và sau đó là tự huấn luyện. Các hiện tượng thí nghiệm trên cũng có thể được quan sát trên bộ dữ liệu Wikiann.

Trực quan hóa Biểu diễn Mẫu Token
Để chứng minh rằng ProKD của chúng tôi có thể đạt được căn chỉnh đặc trưng cấp lớp, chúng tôi ngẫu nhiên chọn 50 mẫu token cho mỗi lớp từ ngôn ngữ nguồn và ngôn ngữ đích và đưa chúng vào các mạng thầy của ProKD và ProKD w/o CA để có được các biểu diễn cấp token, tương ứng. Lưu ý rằng, mạng thầy của ProKD w/o CA thoái hóa thành một mBERT vanilla khi loại bỏ căn chỉnh theo lớp nguyên mẫu. Sau đó, chúng tôi trực quan hóa những biểu diễn này sử dụng T-SNE (Van der Maaten and Hinton 2008) và hiển thị kết quả cho bốn ngôn ngữ đích trong Hình 3. Như thể hiện, các biểu diễn đặc trưng của ngôn ngữ nguồn và ngôn ngữ đích từ ProKD w/o CA được phân phối khác nhau và không nhất quán do khoảng cách ngôn ngữ. Nhiều ví dụ ngôn ngữ đích của một lớp được căn chỉnh không chính xác với các ví dụ ngôn ngữ nguồn của một lớp khác, do đó gây ra sự nhầm lẫn và cản trở hiệu suất của mô hình. Ngược lại, cách tiếp cận ProKD của chúng tôi cho thấy sự ưu việt hơn ProKD w/o CA với nhiều lớp được căn chỉnh chính xác hơn. Ví dụ, khi thực hiện NER đa ngôn ngữ từ Tiếng Anh (en) sang Tiếng Trung (zh), ProKD w/o CA căn chỉnh các đặc trưng nguồn và đích chỉ cho một lớp, I-LOC, trong khi mô hình của chúng tôi đạt được căn chỉnh đặc trưng trên năm lớp. Chúng tôi lập luận rằng một mô hình căn chỉnh đặc trưng qua nhiều lớp có thể nắm bắt nhiều đặc trưng lớp được chia sẻ hơn qua các ngôn ngữ, điều này rất quan trọng để tổng quát hóa mô hình cho các ngôn ngữ đích chưa biết.

Nghiên cứu Trường hợp
Trong phần này, chúng tôi trình bày một nghiên cứu trường hợp để cho thấy rằng mô hình của chúng tôi có thể học tri thức cụ thể của ngôn ngữ đích thông qua tự huấn luyện. Chúng tôi so sánh kết quả dự đoán của mô hình ProKD w/o ST với ProKD của chúng tôi cho dữ liệu thử nghiệm ngôn ngữ đích, như thể hiện trong Bảng 5. Trong ví dụ 1, mô hình ProKD w/o ST dự đoán không chính xác "Madrid" là "I-ORG" vì 66.67% các token "Madrid" trong bộ dữ liệu Tiếng Anh được chú thích là "I-ORG". Mạng thầy được huấn luyện với dữ liệu Tiếng Anh này sẽ chưng cất sở thích nhãn này sang mạng trò, dẫn đến mạng trò của ProKD w/o ST có xu hướng đưa ra dự đoán không chính xác. Ngược lại, mô hình của chúng tôi nắm bắt các sở thích nhãn của ngôn ngữ đích bằng cơ chế học tự huấn luyện nguyên mẫu. Trong cùng ví dụ 1, 59.73% các token "Madrid" trong ngôn ngữ Tây Ban Nha đích được gán nhãn là "I-ORG". Mô hình của chúng tôi có thể tạo ra dự đoán chính xác do sự quen thuộc mật thiết với tri thức cụ thể của ngôn ngữ đích. Chúng tôi quan sát hiện tượng tương tự trong các ví dụ 2 và 3.

Kết luận
Bài báo này trình bày một mạng dựa trên chưng cất tri thức ProKD cho NER đa ngôn ngữ không có tài nguyên. ProKD đề xuất một cách tiếp cận căn chỉnh nguyên mẫu dựa trên học đối lập để tăng cường khả năng của mạng thầy để nắm bắt tri thức độc lập ngôn ngữ. Ngoài ra, ProKD giới thiệu phương pháp tự huấn luyện nguyên mẫu để cải thiện khả năng của mạng trò để nắm bắt tri thức cụ thể của ngôn ngữ đích. Các thí nghiệm trên sáu ngôn ngữ đích minh họa tính hiệu quả của cách tiếp cận được đề xuất.

Lời cảm ơn
Chúng tôi chân thành cảm ơn các nhà đánh giá về những nhận xét sâu sắc và gợi ý có giá trị của họ. Công trình này được tài trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia của Bộ Khoa học và Công nghệ Trung Quốc (Số 2021YFB1716201). Cảm ơn về cơ sở hạ tầng tính toán được cung cấp bởi Trung tâm Đổi mới Tiên tiến Bắc Kinh về Dữ liệu Lớn và Tính toán Não.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ được giữ nguyên trong tiếng Anh như trong văn bản gốc]
