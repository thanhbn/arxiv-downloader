# 2309.01953.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/sampling/2309.01953.pdf
# File size: 495819 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Bilevel Scheduled Sampling for Dialogue
Generation
Jiawen Liu and Kan Li(B)
Beijing Institute of Technology, Beijing, China
{liujiawen,likan }@bit.edu.cn
Abstract. Exposure bias poses a common challenge in numerous nat-
ural language processing tasks, particularly in the dialog generation. In
response to this issue, researchers have devised various techniques, among
which scheduled sampling has proven to be an effective method for mit-
igating exposure bias. However, the existing state-of-the-art scheduled
sampling methods solely consider the current sampling words’ quality
for threshold truncation sampling, which overlooks the importance of
sentence-level information and the method of threshold truncation war-
rants further discussion. In this paper, we propose a bilevel scheduled
sampling model that takes the sentence-level information into account
and incorporates it with word-level quality. To enhance sampling diver-
sity and improve the model’s adaptability, we propose a smooth function
that maps the combined result of sentence-level and word-level informa-
tion to an appropriate range, and employ probabilistic sampling based
on the mapped values instead of threshold truncation. Experiments con-
ducted on the DailyDialog and PersonaChat datasets demonstrate the
effectiveness of our proposed methods, which significantly alleviate the
exposure bias problem and outperform state-of-the-art scheduled sam-
pling methods.
Keywords: Scheduled Sampling ·Exposure Bias ·Dialog Generation.
1 Introduction
Exposure bias is a common problem in many natural language processing tasks
[1], especially in dialog generation. Exposure bias refers to the discrepancy be-
tween the training and inference stages of a model, where the model is trained on
ground truth data but generates on its own predictions at inference time. This
results in the model facing different environments during training and inference.
Once a model prediction is inconsistent with the ground truth somewhere, it
can affect later predictions, which will lead to errors accumulating and prop-
agating along the generated sequence, resulting in poor quality, diversity and
generalization of responses.
To address this issue, researchers have developed various techniques, such as
beam search [2], DAD(Data As Demonstrator) [3], sentence level training [4],
scheduled sampling [1,5], reinforcement learning explicitly trains models [6,7],arXiv:2309.01953v1  [cs.CL]  5 Sep 2023

--- PAGE 2 ---
2 J. Liu and K. Li
etc. These techniques help models handle long-term dependencies better in in-
ference process. Currently, scheduled sampling is widely adopted and has demon-
strated favorable outcomes. Nevertheless, the existing methods either generate
entire sentences autoregressively before sampling[8], or solely consider the cur-
rent sampling word’s impact for threshold truncation sampling[9,10]. The former
necessitates multiple beam searches, resulting in high computational complexity,
low efficiency, and cannot dynamically adjust based on the quality of each word,
thereby overlooking the differences between words in the sentence. The latter
disregards sentence-level information and the method of threshold truncation
warrants further discussion.
In our opinion, the importance of word-level information is obvious, and
current methods that sample based on word-level scores [9,10] are significantly
better than traditional methods [1,8] that sample each word in a sentence with
the same probability. However, relying on cosine similarity or other scores be-
tween the generated word and the ground truth word is limited, and we still
need to take the sentence-level quality into account. Therefore, in this paper, we
propose a bilevel scheduled sampling model that dynamically adjusts the sam-
pling probabilities of the generated results and the ground truth at both word
and sentence level. Our model effectively balances the trade-off between learning
from the ground truth and its own predictions, thereby enhancing the robustness
and diversity of dialogue generation.
Specifically, sentence quality evaluation in sentence-level sampling is con-
ducted through the utilization of either BLEU score or sentence-level cosine
similarity. On the other hand, word quality in word-level sampling is evaluated
based on the probability of word generation. Finally, a specially designed smooth
function is used to integrate sentence-level and word-level evaluations, enabling
probabilistic sampling.
We evaluate the proposed model on two publicly available datasets, namely
DailyDialog and PersonaChat. Experimental results show that our model is supe-
rior to existing exposure bias models in both metric-based automatic evaluation
and human evaluation. The main contributions of this paper include:
–To the best of our knowledge, we are the first to propose a bilevel scheduled
sampling model that takes both sentence-level and word-level information
into account.
–In sentence-level sampling, we utilize BLEU and sentence-level cosine sim-
ilarity as evaluation metrics. For word-level sampling, we leverage the pre-
dicted probabilities as the word-level score. To enhance the diversity of
the sampling process and improve the model’s adaptability, we propose a
smoothing function that maps the combined results of bilevel sampling to
an appropriate range and adopt probabilistic sampling instead of threshold
truncation.
–Extensive evaluations on two widely used open-domain dialogue datasets
demonstrate that the proposed approach significantly alleviates the expo-
sure bias problem and outperforms the state-of-the-art scheduled sampling
methods.

--- PAGE 3 ---
Bilevel Scheduled Sampling for Dialogue Generation 3
2 Related Work
Data As Demonstrator (DAD) is a meta learning algorithm [3] that solves the
problem of exposure bias by blending real training data with model predictions.
During the training phase, not only the ground truth is used as input, but also
the model-predicted results are added to the training set to make it conform to
the test distribution. The Scheduled Sampling method [1] further developed this
approach for sequence generation by sampling from the model’s own predictions
during training, instead of always using the ground truth. This method was first
proposed by Bengio et al. in 2015 [1], and was later improved for transformer
models by Mihaylova and Martins in 2019 [5], owing to the superior performance
of transformer models [11]. What’s more, since scheduled sampling only looks
ahead one step, Goodman et al. proposed the TeaForN method [12], which looks
ahead N steps for more foresight in sampling, but at the cost of reduced training
efficiency.
The initial strategy for sampling was to decay the probability of sampling
the ground truth based on training steps, with specific linear decay f(i) =
max(ϵ, ki+b), exponential decay f(i) =ki, and sigmoidal decay f(i) =k
k+ei
k.
Later in 2021, Liu et al. proposed a strategy of decaying according to decoding
steps [13], further improving the effectiveness.
Zhang et al. proposed two strategies, namely word-level and sentence-level
oracles, to select the generated results of the model [8]. However, the article
continues to employ probability with decay for sampling from model’s results
(oracles) and the ground truth, rather than adopting a more sophisticated ap-
proach of sampling based on the quality of sentences or words.
Obviously, sampling words with equal probability regardless of their varying
qualities is inherently inaccurate. It is highly recommended to employ distinct
sampling probabilities for individual sentences and for each word within a sen-
tence. Liu et al. proposed confidence-aware scheduled sampling [10], which sets
two thresholds tgolden andtrand, and samples the ground truth, model predic-
tion result, a random word when the confidence is respectively in [0 , tgolden ),
[tgolden , trand), [trand,1]. Xu et al. [9] focused on dialogue generation and se-
lected words based on the cosine similarity between the predicted word and the
ground truth. If the similarity is greater than the threshold β, the predicted word
is selected with a probability of α, which is a hyperparameter that increases with
the number of training epochs to achieve faster convergence in the early stages
of training and alleviate exposure bias.
However, these methods have some limitations. Firstly, they don’t take into
account the very important sentence-level information. Secondly, the method
of threshold truncation also needs to be discussed. If probability selection is
used, different words can be sampled with different probabilities, so that the
distinction between different words is greater and the adaptability of the model
is stronger.
Consequently, we propose a novel bilevel scheduled sampling model that ef-
fectively combines sentence-level information with word-level quality. To enhance

--- PAGE 4 ---
4 J. Liu and K. Li
sampling diversity and improve the model’s adaptability, we introduce a smooth
function that maps the integrated outcome of sentence-level and word-level in-
formation to a suitable range. Subsequently, we employ probabilistic sampling
based on the mapped values instead of threshold truncation.
3 Bilevel Scheduled Sampling Model
3.1 Mathematical Modeling of Dialogue Generation
In dialogue generation tasks, we typically define the model’s objective function
by maximizing the conditional probability, which means we need to maximize the
probability of generating the output text Ygiven the input text X. Specifically,
we can use equation 1 to represent this objective function, where ytis the t-th
word in Y,y<tare the first t−1 words in Y, and Tis the length of Y. This
objective function requires us to calculate the probability of each word given the
input text Xand the previous words y<t, and multiply these probabilities to
obtain the probability of the output text Y.
P(Y|X, θ) =TY
t=1p(yt|y<t, X, θ ) (1)
During the training process, we typically use the cross-entropy loss function
[14] as the optimization objective for the model, which measures the difference
between the model’s output and the ground truth. Specifically, we can use equa-
tion 2 to represent the cross-entropy loss function, where L(θ) denotes the cross-
entropy loss function of the model, and the negative logarithm of the probability
is used as the loss, such that the loss decreases as the probability increases.
L(θ) =−logP(Y|X, θ) =1
TTX
t=1−logp(yt|y<t, X, θ ) (2)
By minimizing the loss function of all samples, we can obtain the optimal
model parameters θ. This can be expressed as equation 3, where Nis the total
number of training set samples.
θ= argmin
θ{NX
k=1Lk(θ)} (3)
In exposure bias problem, unlike training, during inference, the probability
of each target word p(yt|y<t, X, θ ) in equation 1 is conditioned on the previously
generated words y∗< tinstead of the ground truth y< t, because the ground
truth words are not available in actual inference. Therefore, we employ the
scheduled sampling method to replace the ground truth during training with
sampled sentences, thereby reducing the gap between the model’s training and
inference processes.

--- PAGE 5 ---
Bilevel Scheduled Sampling for Dialogue Generation 5
3.2 Bilevel Scheduled Sampling
We propose a sentence- and word-level fusion sampling method to address the
diversity and contextuality of open-domain dialogue systems. Taking the trans-
former as an example, we introduce the principle and implementation of the
bilevel scheduled sampling method, as shown in Fig. 1.
Fig. 1. Our Bilevel Scheduled Sampling model.
Sentence-level Sampling Zhang et al. [8] proposed a traditional sentence-level
sampling method that uses beam search to select k most likely sentences from
the predicted distribution, and then chooses one as the sampling sample based
on the BLEU score, followed by the decay sampling method to extract from the
sample and the real sentence. However, this method has two main drawbacks: (1)
it requires multiple beam searches to generate sentence like inference procedure,
resulting in high computational cost and low efficiency; (2) it can only sampling
by probability with decay, which ignores the differences between words in the
sentence, making it impossible to dynamically adjust the quality of each word.
To address these issues, we propose a sentence-level sampling method that
utilizes the parallelism of the transformer model to generate the entire sentence
at once during training, and calculates the probability of each word and the
quality of the sentence.
We first calculate the probability Pof the predicted word through the soft-
max function, and select the words Y∗with the largest probability to be evalu-
ated by the sentence-level indicator.
We try two sentence-level indicators(SLI): BLEU and sentence-level cosine
similarity, both of which have achieved good results. And considering the padding
tokens in the sentences, we mask them during calculation. One metric based on

--- PAGE 6 ---
6 J. Liu and K. Li
BLEU is as follows:
SLI1( Y∗, Y) =1
m4X
i=1bleu-i( Y∗, Y) (4)
Where the term ‘bleu-i’ refers to the i-gram BLEU result without using a smooth
function [15]. Y∗represents the model-generated result, Yrepresents the ground
truth, and mis a hyperparameter used to map the result to a value around 1 to
prevent imbalance during sampling.
The other metric based on cosine similarity is as follows:
SLI2( Y∗, Y) =1
mCosinSimilarity( Y∗, Y)
=embed( Y∗)·embed( Y)
m· ||embed( Y∗)|| · ||embed( Y)||(5)
Where mis the hyperparameter, which maps the result to a value around 1
so that the sampling is not unbalanced; embed is word embedding and converts
word subscript to word vector. We just use word embedding within the model’s
decoder. An existing word embedding from pretrained model such as BERT[16]
can also be used. Here, for sentences, it is the average word embedding of sen-
tences:
embed( Y) =PT
i=1embed( yi)
T(6)
Where Tis the length of sentence Y,yiis the i-th word.
This method has the following advantages: First, the whole sentence can be
generated at once, avoiding the cost of multiple beam searches and improving
efficiency. Second, it considers the sentence-level information and can be com-
bined with the word-level sampling to dynamically adjust each word, enables the
model to sample based on the quality of individual words while simultaneously
accounting for the overall quality of the sentence.
Fig. 2. Sentence-level Sampling
 Fig. 3. Word-level Sampling

--- PAGE 7 ---
Bilevel Scheduled Sampling for Dialogue Generation 7
Smooth Function Between Two Sampling Layers In order to simulate the
inference process, words with higher corresponding probabilities Phave a higher
probability of being sampled, so we directly use Pas the word-level evaluation
metric. Now we have the sentence-level score S(result of SLI) and the word-level
score P. However, a method to combine them is still missing. Considering that
both sentence-level score Sand word-level score Pare equally important, we
multiply them and then use a smooth function to map the product to the range
of 0˜1, like Fig. 2.
Currently, we take two types of functions into account. The first one is sim-
pler, which directly restricts the result to 0˜1, like equation 7.
f(x) =max(min(x,1),0) (7)
The second one is a sigmoid-shaped function which is smoother, like equation
8.
f(x) =1
1 +e−k(x−b)(8)
where k≥1 is a hyperparameter to control the speed of convergence, b >0
controls the central symmetrical point of the function, moving it to the right.
We finally set k= 10, b= 0.6. Their images are in Fig. 4.
Fig. 4. The smooth function f.
Word-level Sampling Now, we have obtained the smoothed score p, which
contains information combining sentence-level and word-level, and we use it for
specific word sampling as shown in Fig. 3.
First, we use probability selection for sampling like equation 9, which has a
greater discrimination between different words compared to threshold trunca-
tion. It can also sample different words, making the trained model more adapt-
able. Specifically, the predicted word Y∗is selected with probability pand the
ground truth Yis selected with probability 1 −p, and Y′is the first sampled
result.
Y′=(
Y∗sampling with probability p ,
Y otherwise.(9)

--- PAGE 8 ---
8 J. Liu and K. Li
In addition, to prevent the model from relying too heavily on high-probability
predicted words and causing the generated results to be too monotonous, this
paper also uses a method for random word sampling, like equation 10. When the
word-level predicted probability Pis greater than a set threshold α, a random
word is chosen as the next input with a certain probability to prevent the model
from degenerating.
sample =(
Y′if P < α ,
rand otherwise.(10)
where rand refers to a random word, αis a threshold which is set to 0.95, sample
is the finally sampled result as the input to encoder during training.
4 Experiments
To assess the effectiveness and merits of the proposed Bilevel Scheduled Sampling
model detailed in this study, a comprehensive set of experiments was conducted,
encompassing evaluation, comparative analysis and ablation study. This section
elucidates the experimental design employed and provides a thorough evaluation
of the obtained results.
4.1 Datasets
We evaluate the proposed method using two widely used dialogue datasets.
DailyDialog is a collection of daily life conversations, encompassing a variety
of topics, emotions and linguistic styles[17]. PersonaChat consists of conver-
sations between two participants, where one participant assumes a persona
and the other participant engages in a conversation while considering the per-
sona’s characteristics[18]. After data preprocessing, we split the n-turn dialogue
(u1, u2, ..., u n) into n-1 single-turn dialogues [( u1, u2),(u2, u3), ...,(un−1, un)] ,
where urepresents an utterance. The number of context-response pairs in the
train/validation/test set is 68,066/6,820/6,841 for DailyDialog and 104,609/12,588/12,106
for PersonaChat without any extra label or persona information.
4.2 Implementation Details
The experiment uses an NVIDIA GeForce RTX 2080 Ti graphics card and adopts
PyTorch deep learning framework for training. Dropout is used for the selfat-
tention module, the feed-forward layer, and the activation layer, and the rate of
all three is set to 0.1. The sentence length is set to 26 and batch size is set to
256. The vocab size is 21626 for DialyDialog and 22630 for PersonaChat.
4.3 Comparison Methods
We compare our proposed Bilevel Scheduled Sampling model with following
established methods, and all approaches are based on the Transformer-base
model[11]:

--- PAGE 9 ---
Bilevel Scheduled Sampling for Dialogue Generation 9
– Transformer [11]: The Transformer-base model used in dialog generation.
– AdapBridge [9]: An improved scheduled sampling approach, which uses an
adaptive bridge mechanism to evaluate model generation results. Specifi-
cally, the selection is made according to the cosine similarity results of the
predicted word and the ground truth. If it is greater than a threshold β, the
predict word is sampled according to a probability αthat varies with the
number of training epochs. Accoding to the paper, we set w= 15 as half of
the training epochs and other hyperparameters are not changed.
– Confidence-Aware [10]: An improved scheduled sampling method, which
selects whether to sample according to the confidence of the prediction result
(that is, the prediction probability). Specifically, it sets two thresholds tgolden
andtrand, and samples the ground truth, model prediction result, a ran-
dom word when the confidence is respectively in [0 , tgolden ), [tgolden , trand),
[trand,1]. We set tgolden = 0.7 and trand= 0.95 to get the best result.
At the same time, ablation experiments are carried out in this paper. For different
models, we tested different hyperparameter values for m, and presented the best
result. The proposed model for ablation experiment testing is as follows:
– Bilevel-None : The bilevel scheduled sampling model proposed in this pa-
per without the sentence-level sampling part. The smooth function is the
sigmoid-shaped equation 8 and the word-level sampling method is unchanged.
– Bilevel-Bleu : The bilevel scheduled sampling model proposed in this paper,
the sentence-level indicator is the Bleu metric with m= 0.8 to get the best
result. The smooth function is equation 8.
– Bilevel-Cosine : The bilevel scheduled sampling model proposed in this
paper, the sentence-level indicator is the sentence-level cosine similarity with
m= 0.6 to get the best result. The smooth function is equation 8.
– Bilevel-f1 : The bilevel scheduled sampling model proposed in this paper,
the sentence-level indicator is the Bleu metric with m= 0.9 to get the best
result. The smooth function is the linearly truncated equation 7.
4.4 Automatic Evaluation
We evaluate the performance of dialogue generation where both automatic and
human evaluation metrics are applied. Automatic evaluation metrics include
BLEU-1/2/3/4 [15], Distinct-1/2/3 [19]. The result is shown in Table 1.
The experimental findings demonstrate a notable performance improvement
of the proposed bilevel scheduled sampling model over both the traditional sched-
uled sampling model and the proposed model lacking sentence-level sampling,
as evidenced by the higher BLEU-1/2/3/4 scores achieved. This superiority can
be attributed to the incorporation of sentence-level information in the proposed
model, which leads to the generation of more coherent and natural sentences,
thereby achieving better alignment with the ground truth. Furthermore, it was
observed that utilizing BLEU as the sentence-level score yielded better results
compared to sentence-level cosine similarity. This discrepancy may arise from

--- PAGE 10 ---
10 J. Liu and K. Li
Table 1. Evaluation results on Daily Dialog and Persona Chat datasets.
Daily Dialog
Model BLEU-1/2/3/4 Distinct-1/2/3
Transformer[11]16.47 5.96 3.30 2.11 0.90 4.53 11.34
AdapBridge[9]16.78 6.06 3.51 2.18 0.85 4.24 10.13
Confidence-Aware[10]16.63 6.45 3.65 2.25 0.89 4.40 10.73
Bilevel-None 16.84 6.53 3.66 2.33 0.99 4.86 11.69
Bilevel-Bleu 17.43 6.81 3.87 2.49 0.985.10 12.98
Bilevel-Cosine 17.24 6.73 3.79 2.45 0.94 4.65 11.43
Persona Chat
Model BLEU-1/2/3/4 Distinct-1/2/3
Transformer[11]17.79 6.37 3.42 2.31 0.22 0.65 1.34
AdapBridge[9]19.53 6.79 3.65 2.46 0.20 0.58 1.20
Confidence-Aware[10]20.15 7.35 3.82 2.48 0.19 0.62 1.36
Bilevel-None 19.84 7.20 3.79 2.48 0.20 0.67 1.47
Bilevel-Bleu 21.16 7.79 4.10 2.71 0.22 0.70 1.61
Bilevel-Cosine 21.17 7.74 4.10 2.68 0.21 0.64 1.34
the fact that BLEU places greater emphasis on text matching, while cosine simi-
larity focuses more on semantic alignment, thus favoring the former for improved
BLEU results.
Additionally, the proposed model shows an advantage in the Distinct-1/2/3
metric. This advantage stems from the incorporation of sentence-level sampling
information and the utilization of probabilistic sampling for word generation.
The bilevel model effectively generates sentences that are more diverse, miti-
gating the issue of excessively repetitive or singular output, and consequently
attains higher scores in the Distinct metric.
In summary, the experimental results demonstrate that by combining sentence-
level and word-level considered together with probabilistic sampling, the pro-
posed model outperforms existing scheduled sampling models in terms of both
BLEU-1/2/3/4 and Distinct-1/2/3 metrics. This indicates that the bilevel sched-
uled sampling model significantly alleviates the exposure bias problem and out-
performs the state-of-the-art scheduled sampling methods.
4.5 Human Evaluation
To thoroughly assess the proposed model and the baseline model mentioned
in this paper, we conducted a human evaluation following the approach used
by Li et al[20]. For this evaluation, we randomly selected 100 samples from
the test set of each dialogue dataset. Subsequently, we sought the judgment of
three well-educated annotators to determine whether the overall response quality
of the Bilevel-Bleu model and the other models under consideration exhibited

--- PAGE 11 ---
Bilevel Scheduled Sampling for Dialogue Generation 11
superior coherence, informativeness, and fluency. The annotators categorized
their assessment as either a win, tie, or lose for each model.
Table 2. Human evaluation result.
Bilevel-Bleu vs. Models Win Tie Lose Kappa
Transformer[11]60.33 25.50 14.17 0.6581
AdapBridge[9]48.50 29.67 21.83 0.5417
Confidence-Aware[10]43.00 32.17 24.83 0.5077
Bilevel-None 44.50 32.00 23.50 0.4491
Bilevel-Cosine 39.33 33.33 27.33 0.5223
Table 2 summarizes the human evaluation results. The final results show that
the Bilevel-Bleu model in this paper is better than other models, which indicates
that it is more capable of generating human preferred responses. Meanwhile, we
use Fleiss kappa [21] to measure the agreement between annotators, and the
results are all greater than 0.4, which indicates that the annotators reach a good
agreement on the judgment.
4.6 Ablation study
In this paper, we design a comparison experiment includes the proposed model
without sentence-level sampling and with two different sentence-level sampling.
As can be seen from Table 1, whether the sentence-level indicator uses bleu or
cosine similarity, the model combining sentence-level and word-level sampling in
this paper will have better results than the single word-level sampling model.
At the same time, it can also be seen from the results that the performance of
the proposed model is significantly improved compared to the base transformer
model when only word-level sampling is performed, with slightly better results
than the existing scheduled sampling methods. This indicates that the proba-
bilistic sampling approach in this paper is better than threshold truncation if
we map the original probability to a suitable size by smoothing function.
In addition, we conducted comparative experiments on smooth functions,
including linear truncation function f1 (equation 7) and sigmoid-shaped smooth
function f2 (equation 8). The result is in Table 3.
Table 3. Evaluation results of different smooth functions on the PersonaChat dataset.
Model BLEU-1/2/3/4 Distinct-1/2/3
Transformer[11]17.79 6.37 3.42 2.31 0.22 0.65 1.34
Bilevel-f1 19.58 7.18 3.79 2.45 0.19 0.64 1.35
Bilevel-f2 21.16 7.79 4.10 2.71 0.22 0.70 1.61

--- PAGE 12 ---
12 J. Liu and K. Li
The results indicate that the sigmoid-shaped smooth function gives better
results, enabling the model to sample more appropriate sentences. This is due to
the direct probabilistic sampling is not sufficient to distinguish the high-quality
and low-quality utterances. By employing the sigmoid-shaped smooth function,
the influence of sentence performance on the sampling probability is enhanced,
resulting in a smoother and more effective sampling process.
5 Conclusion
In this paper, we propose a bilevel scheduled sampling model, which considers
the sentence-level and word-level combination quality of the model generation
results, so that the sampling results can better adapt to the exposure bias and
thus improve the performance of the model. In order to make the sampling more
diverse and improve the adaptability of model, we propose a smooth function
to map the combined result of sentence-level and word-level to an appropriate
range, and then perform probabilistic sampling instead of threshold truncation.
Experiments on two widely used open-domain dialogue datasets demonstrate the
effectiveness of all our proposed methods, which significantly alleviate the expo-
sure bias problem and outperform state-of-the-art scheduled sampling methods.
In the future, we plan to extend the application of the Bilevel Scheduled Sam-
pling method to large language models across various projects, addressing the
issue of exposure bias. This approach will help enhance the performance and
robustness of the models in real-world scenarios.
Acknowledgement
This research was supported by the Beijing Natural Science Foundation
(No.4222037, L181010).
References
1. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for sequence
prediction with recurrent neural networks. Advances in neural information pro-
cessing systems 28(2015)
2. Wiseman, S., Rush, A.M.: Sequence-to-sequence learning as beam-search optimiza-
tion. arXiv preprint arXiv:1606.02960 (2016)
3. Venkatraman, A., Hebert, M.H., Bagnell, J.A.: Improving multi-step prediction
of learned time series models. In: National Conference on Artificial Intelligence.
AAAI Press (2015)
4. Ranzato, M., Chopra, S., Auli, M., Zaremba, W.: Sequence level training with
recurrent neural networks. Computer Science (2015)
5. Mihaylova, T., Martins, A.F.: Scheduled sampling for transformers. arXiv preprint
arXiv:1906.07651 (2019)
6. Yu, L., Zhang, W., Wang, J., Yu, Y.: Seqgan: Sequence generative adversarial
nets with policy gradient. In: Proceedings of the AAAI conference on artificial
intelligence (2017)

--- PAGE 13 ---
Bilevel Scheduled Sampling for Dialogue Generation 13
7. Nie, W., Narodytska, N., Patel, A.: Relgan: Relational generative adversarial net-
works for text generation. In: International conference on learning representations
(2018)
8. Zhang, W., Feng, Y., Meng, F., You, D., Liu, Q.: Bridging the gap between train-
ing and inference for neural machine translation. arXiv preprint arXiv:1906.02448
(2019)
9. Xu, H., Zhang, H., Zou, Y., Chen, H., Ding, Z., Lan, Y.: Adaptive bridge be-
tween training and inference for dialogue generation. In: Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing. pp. 2541–2550
(2021)
10. Liu, Y., Meng, F., Chen, Y., Xu, J., Zhou, J.: Confidence-aware scheduled sampling
for neural machine translation. arXiv preprint arXiv:2107.10427 (2021)
11. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
 L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30(2017)
12. Goodman, S., Ding, N., Soricut, R.: Teaforn: Teacher-forcing with n-grams. In:
Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). pp. 8704–8717 (2020)
13. Liu, Y., Meng, F., Chen, Y., Xu, J., Zhou, J.: Scheduled sampling based on decod-
ing steps for neural machine translation. arXiv preprint arXiv:2108.12963 (2021)
14. Ackley, D.H., Hinton, G.E., Sejnowski, T.J.: A learning algorithm for boltzmann
machines. Cognitive Science 9(1), 147–169 (1985)
15. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
evaluation of machine translation. In: Proceedings of the 40th annual meeting of
the Association for Computational Linguistics. pp. 311–318 (2002)
16. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018)
17. Li, Y., Su, H., Shen, X., Li, W., Cao, Z., Niu, S.: Dailydialog: A manually labelled
multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957 (2017)
18. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., Weston, J.: Person-
alizing dialogue agents: I have a dog, do you have pets too? arXiv preprint
arXiv:1801.07243 (2018)
19. Li, J., Galley, M., Brockett, C., Gao, J., Dolan, B.: A diversity-promoting objective
function for neural conversation models. arXiv preprint arXiv:1510.03055 (2015)
20. Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A., Jurafsky, D.: Adversarial learning
for neural dialogue generation. arXiv preprint arXiv:1701.06547 (2017)
21. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychological
bulletin 76(5), 378 (1971)
