# SciInstruct : một Tập dữ liệu Được Chú thích Hướng dẫn Tự phản tư
# để Huấn luyện Mô hình Ngôn ngữ Khoa học
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2401.07950.pdf
# Kích thước tệp: 1265597 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SciInstruct : một Tập dữ liệu Được Chú thích Hướng dẫn Tự phản tư
để Huấn luyện Mô hình Ngôn ngữ Khoa học
Dan Zhang1,2,∗, Ziniu Hu3, Sining Zhoubian1,2,∗, Zhengxiao Du1,2,∗
Kaiyu Yang3, Zihan Wang1,2,∗, Yisong Yue3, Yuxiao Dong1, Jie Tang1†
1Nhóm Kỹ thuật Tri thức (KEG), Đại học Thanh Hoa;2Zhipu AI;
3Viện Công nghệ California
https://SciGLM.github.io/
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã cho thấy triển vọng trong việc hỗ trợ khám phá khoa học. Tuy nhiên, những ứng dụng như vậy hiện đang bị hạn chế bởi những thiếu sót của LLMs trong việc hiểu các khái niệm khoa học phức tạp, suy ra các phương trình ký hiệu, và giải quyết các phép tính số học tiên tiến. Để thu hẹp những khoảng cách này, chúng tôi giới thiệu SciInstruct, một bộ hướng dẫn khoa học để huấn luyện các mô hình ngôn ngữ khoa học có khả năng suy luận khoa học ở cấp độ đại học. Trung tâm của phương pháp chúng tôi là một khung chú thích hướng dẫn tự phản tư mới để giải quyết thách thức khan hiếm dữ liệu trong lĩnh vực khoa học. Khung này tận dụng các LLMs hiện có để tạo ra suy luận từng bước cho các câu hỏi khoa học chưa được gắn nhãn, tiếp theo là một quá trình tự phản tư phê bình và sửa đổi. Áp dụng khung này, chúng tôi đã tuyển chọn một tập dữ liệu đa dạng và chất lượng cao bao gồm vật lý, hóa học, toán học và chứng minh hình thức. Chúng tôi phân tích SciInstruct được tuyển chọn từ nhiều góc độ thú vị (ví dụ: lĩnh vực, quy mô, nguồn, loại câu hỏi, độ dài câu trả lời, v.v.). Để xác minh hiệu quả của SciInstruct, chúng tôi đã tinh chỉnh các mô hình ngôn ngữ khác nhau với SciInstruct, tức là ChatGLM3 (6B và 32B), Llama3-8B-Instruct, và Mistral-7B: MetaMath, nâng cao khả năng suy luận khoa học và toán học của chúng, mà không hy sinh khả năng hiểu ngôn ngữ của mô hình cơ sở. Chúng tôi phát hành tất cả mã và SciInstruct tại https://github.com/THUDM/SciGLM.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) đã cho thấy tiềm năng hỗ trợ và đẩy nhanh khám phá khoa học [1; 2], giúp các nhiệm vụ như dự đoán protein [3], dự báo thời tiết [4] và hiểu biết về khoa học địa chất [5]. Mặc dù những thử nghiệm khái niệm đầy hứa hẹn này, các nghiên cứu gần đây [6;7;8;9] cho thấy rằng ngay cả các LLMs tiên tiến như GPT-3.5 và GPT-4 cũng gặp khó khăn với các vấn đề khoa học cơ bản, chỉ đạt được độ chính xác 28,52% trên một số câu hỏi sách giáo khoa cấp độ đại học. Những câu hỏi khoa học này, chẳng hạn như tính năng lượng bằng phân bố Planck, đòi hỏi một tập hợp kỹ năng đa dạng, bao gồm tìm kiếm sự kết hợp đúng của các khái niệm và tiên đề vật lý, lựa chọn và suy diễn các phương trình hình thức, và tính toán số học nghiêm ngặt. Trước khi để LLMs trang bị những kỹ năng này để giải quyết các câu hỏi khoa học cơ bản, tất cả những tầm nhìn đầy tham vọng về việc xây dựng các tác nhân LLM để hỗ trợ khám phá khoa học có thể không đáng tin cậy. Điều này mang lại những động lực đáng kể để xây dựng các hướng dẫn khoa học và sử dụng chúng để phát triển các mô hình ngôn ngữ khoa học nền tảng.

*Công việc được thực hiện trong khi các tác giả này thực tập tại Zhipu AI.
†Tác giả liên hệ.
Hội nghị lần thứ 38 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2024) Chuyên đề về Tập dữ liệu và Điểm chuẩn.arXiv:2401.07950v3 [cs.CL] 18 Nov 2024

--- TRANG 2 ---
Bảng 1: So sánh giữa SciInstruct và
các tập dữ liệu hướng dẫn liên quan khác.
Tập dữ liệu Lĩnh vực Cấp độ Đại học
Toán Vật lý Hóa học Lean
Galactica [1] ! ! ! % Không rõ
MathInstruct [10] ! % % % %
MetaMath [11] ! % % % %
WebInstruct [12] ! ! ! % %
SciInstruct ! ! ! ! !

Vật lý &
Hóa học 48.8%
Toán 35.4% Lean
15.8% Tỷ lệ Lĩnh vực
Hình 1: Lĩnh vực

Điền vào
33.2%
Câu hỏi Trắc nghiệm 32.0% Bài toán
Giải pháp Phức tạp 20.1% Bài toán
Giải pháp Đơn giản
14.6% Tỷ lệ Loại Câu hỏi Hình 2: Loại Câu hỏi

6 12 32 ···API
Số Tham số (Tỷ, Thang Log) 10 20 30 40 50 60 Độ chính xác Trung bình của Điểm chuẩn Khoa học

Mistral (7B) Sci: Mistral (7B)
LLaMA-2 (7B) LLaMA-2
(13B) ChatGLM2
(6B) ChatGLM2
(12B) ChatGLM3
(6B) ChatGLM3
(32B) SciGLM
(6B) SciGLM
(32B)

Galactica
(6.7B) GPT-3.5 GPT-4 Hình 3: Độ chính xác trung bình trên các điểm chuẩn CEval-Sci, Sci-
Eval, SciBench, MATH, và SAT-Math của các LLMs khác nhau.

Tuy nhiên, huấn luyện LLMs để hiểu khoa học (ví dụ: vật lý, hóa học, toán học) thách thức hơn nhiều so với nhiều nhiệm vụ suy luận tổng quát, đòi hỏi các kỹ năng phức tạp hơn. Do đó, cốt lõi của việc cải thiện khả năng giải quyết vấn đề khoa học của LLMs là xây dựng các tập dữ liệu hướng dẫn quy mô lớn và chất lượng cao, bao gồm tất cả các kỹ năng cần thiết. Vì vậy chúng tôi tổng hợp dữ liệu để cải thiện từng kỹ năng: 1) để hiểu khái niệm khoa học, chúng tôi thu thập một lượng lớn câu hỏi vật lý và hóa học đòi hỏi kiến thức khoa học cơ bản; 2) để tính toán số học, chúng tôi thu thập và sử dụng dữ liệu tính toán toán học bổ sung và tiên tiến hơn; 3) để suy diễn nghiêm ngặt các phương trình ký hiệu, chúng tôi kết hợp các chứng minh định lý hình thức được viết trong Lean. Sự kết hợp các nguồn dữ liệu như vậy cho phép mô hình được huấn luyện không quá khớp với một môn học duy nhất mà có được một số kỹ năng tổng quát và cơ bản để giải quyết các nhiệm vụ khoa học khác nhau.

Mặt khác, quy mô dữ liệu hướng dẫn có sẵn từ internet cho các vấn đề khoa học nhỏ hơn nhiều so với các nhiệm vụ khác. Vì nội dung khoa học thường đòi hỏi chuyên môn nhất định để tạo ra, và thông tin chất lượng cao thường được bảo vệ bởi quyền sở hữu trí tuệ, hầu hết dữ liệu chúng ta có thể truy cập hợp pháp chỉ chứa các cặp câu hỏi-câu trả lời (QA) mà không có các bước suy luận chuỗi tư duy chi tiết (R). Tuy nhiên, chỉ huấn luyện LLMs trên các cặp QA sẽ dẫn đến kết quả rất tệ và thậm chí có hại cho khả năng ngôn ngữ tổng quát của chúng. Để có được các bước suy luận chất lượng cao (R) làm hướng dẫn, chúng tôi đề xuất một khung chú thích hướng dẫn tự phản tư yêu cầu LLM tự động chú thích, phê bình và sửa đổi các bước suy luận, với sự can thiệp tối thiểu của con người. Cụ thể, LLM đầu tiên cố gắng tạo ra cả các bước suy luận và trả lời câu hỏi đã cho (Q); sau đó, đối với những đầu ra có dự đoán câu trả lời không chính xác, chúng tôi yêu cầu chính LLM xác định loại lỗi, dựa trên đó để giải quyết lỗi và sửa đổi đầu ra, cho đến khi có được câu trả lời đúng. Khung chú thích tự phản tư như vậy chỉ sử dụng AI thay vì con người để thu thập các dấu vết suy luận (R) làm hướng dẫn, trong khi đảm bảo chất lượng và giải quyết những sai lầm tiềm ẩn của LLM hiện có với việc kiểm tra câu trả lời cẩn thận và tự phản tư của LLM.

Sau khi củng cố các câu hỏi và câu trả lời được tạo ra bởi chú thích tự phản tư, chúng tôi xây dựng SciInstruct, một tập dữ liệu hướng dẫn toàn diện để tinh chỉnh các mô hình ngôn ngữ khoa học. Hình 1 và Hình 2 trình bày tỷ lệ lĩnh vực và loại câu hỏi của SciInstruct. Bảng 1 tổng kết những khác biệt chính giữa các tập dữ liệu hiện có và của chúng tôi. Trong công trình này, chúng tôi chọn ba LLMs, tức là ChatGLM3 [13;14] (6B và 32B), Llama3-8B-Instruct [15], và Mistral-7B: MetaMATH [16; 11], làm mô hình cơ sở backbone. Ví dụ, bằng cách tinh chỉnh mô hình dòng ChatGLM trên SciInstruct, chúng tôi có được mô hình SciGLM. Sau đó chúng tôi đánh giá các mô hình được tinh chỉnh thông qua ba loại nhiệm vụ đánh giá, bao gồm các bộ kiểm tra khoa học, điểm chuẩn toán học, và nhiệm vụ ngôn ngữ tổng quát và mã hóa, và cho thấy độ chính xác trung bình trên các điểm chuẩn khoa học của các LLMs khác nhau trong Hình 3. Thông qua tinh chỉnh hướng dẫn, chúng tôi đạt được cải thiện 4,87% so với mô hình 6B, và cải thiện 2,67% so với mô hình 32B, vượt trội nhiều mô hình tiên tiến trước đây có cùng kích thước tham số, bao gồm Galactica [1] cho các vấn đề khoa học, và MAmmoTH [10] cho các vấn đề toán học. Chúng tôi cũng cho thấy việc tinh chỉnh các tập dữ liệu hướng dẫn của chúng tôi không hy sinh khả năng hiểu ngôn ngữ tổng quát, làm cho SciGLM trở thành một bộ mô hình ngôn ngữ khoa học tốt cho cả giao tiếp người-AI cũng như chuyên môn kiến thức lĩnh vực khoa học.

Chúng tôi nêu bật những đóng góp của mình như sau:

--- TRANG 3 ---
Thu thập Toán Vật lý Hóa học Chứng minh Hình thức (Lean) Lọc Dữ liệu
Có nhãn Dữ liệu
Tích cực Dữ liệu
Tiêu cực

Mẫu Chú thích Bộ phân loại
Chất lượng hướng dẫn LMs

SciInstruct

Lọc tiêu cực Giữ lại
tích cực Giai đoạn Lọc Kết luận Con người Sai lầm hiểu biết Sai lầm tính toán Suy luận sai Chú thích Vấn đề 1 Câu trả lời 1 Vấn đề n Câu trả lời n ······ LMs

Chú thích Tự phản tư Vấn đề 1 CoT 1 Câu trả lời 1 Vấn đề n CoT n Câu trả lời n ······ Tập dữ liệu QA Bootstrapped

Số tiệm cận của đường cong y=\\frac{x^2+x}{x^2-1} là___. Làm thế nào để xác định tần số dao động hài của hệ thống một bậc tự do? Hãy thử tính điểm sôi của nước ở áp suất bên ngoài 670kPa. theorem const_add_term{α} [add_comm_monoidα] (k n x a a') (h : k + a = a') :
k + @term α _ n x a = term n x a' := QA w/o R Phê bình & Sửa đổi Các bước suy luận (R) CoT

Mẫu

Hình 4: Quy trình xây dựng SciInstruct. Ở phía trái nhất là sự kết hợp của các tập dữ liệu huấn luyện. Mục đích của chú thích là bổ sung các quá trình chuỗi tư duy với việc tạo ra phản tư. Mục tiêu của bộ lọc là huấn luyện một bộ phân loại chất lượng hướng dẫn và chỉ giữ lại các dấu vết suy luận chất lượng cao làm hướng dẫn.

•Từ góc độ dữ liệu, chúng tôi xây dựng SciInstruct, một tập dữ liệu tinh chỉnh hướng dẫn khoa học toàn diện bao gồm các vấn đề vật lý, hóa học, toán học và chứng minh hình thức.
•Từ góc độ phương pháp, chúng tôi đề xuất một quy trình chú thích tự phản tư để LLMs tự động tuyển chọn các tập dữ liệu hướng dẫn.
•Từ góc độ mô hình, để xác minh hiệu quả của SciInstruct của chúng tôi, chúng tôi tinh chỉnh các LLMs khác nhau (mô hình dòng ChatGLM3, Llama3-8B-Instruct, và Mistral-7B: MetaMATH) trên SciInstruct và cho thấy cải thiện hiệu suất trên các điểm chuẩn khoa học và toán học khác nhau, mà không hy sinh các nhiệm vụ hiểu ngôn ngữ tổng quát.

2 SciInstruct
Nhiều nghiên cứu [17;10;12] đã cho thấy rằng tinh chỉnh các LLMs được huấn luyện trước trên dữ liệu suy luận CoT chất lượng cao có thể đạt được cải thiện hiệu suất bằng cách cho phép mô hình sử dụng tốt hơn kiến thức được ghi nhớ thông qua huấn luyện trước, tuân theo các phong cách suy luận và định dạng ngôn ngữ chính xác và dễ đọc hơn của con người. Tuy nhiên, những thách thức chính của việc xây dựng hướng dẫn khoa học bao gồm kiến thức và độ phức tạp cần thiết và quy mô nhỏ hơn của dữ liệu có sẵn. Do đó, chúng tôi tìm cách giải quyết những trở ngại này bằng cách tạo ra SciInstruct để nâng cao khả năng giải quyết vấn đề khoa học của LLMs. Hình 4 minh họa thiết kế tỉ mỉ của chúng tôi về các mô-đun phụ thiết yếu nhằm thu thập các hướng dẫn quy mô lớn, chất lượng cao. Những mô-đun phụ quan trọng này bao gồm chú thích hướng dẫn tự phản tư và lọc hướng dẫn nhiễu. SciInstruct bao gồm tổng cộng 254.051 hướng dẫn được xác minh.

2.1 Thu thập Hướng dẫn Đa dạng
Mục tiêu của chúng tôi là xây dựng một tập dữ liệu toàn diện và đa dạng bao gồm kiến thức khoa học về độ sâu, phạm vi bao phủ rộng và tính đa dạng. Để đạt được điều này, chúng tôi sẽ tập trung vào các lĩnh vực khoa học và tuyển chọn một số tập dữ liệu hàng đầu được sử dụng rộng rãi và bao gồm nhiều ngành khoa học, chẳng hạn như vật lý, các vấn đề hóa học, toán học và chứng minh hình thức. Để bắt đầu quá trình, chúng tôi thu thập câu hỏi từ nhiều nguồn khác nhau, bao gồm sách giáo khoa, tài liệu giáo dục và bộ đề.

Môn học Hướng dẫn. Như được hiển thị ở phía bên trái của Hình 4, chúng tôi tạo dữ liệu từ các môn học sau:
•Vật lý. Môn học này nhằm giải quyết thách thức xử lý các vấn đề vật lý phức tạp với các giải pháp từng bước và đánh giá khả năng của LLMs trong việc hiểu và phân tích các vấn đề vật lý. Các tập dữ liệu huấn luyện công khai như Fundamentals of Physics và Physical Chemistry được quan sát là thiếu kiến thức vật lý cấp độ đại học. Để giải quyết khoảng cách này, chúng tôi đã thu thập một tập lớn các câu hỏi vật lý từ nhiều môn học (ví dụ: động lực học, vật lý lượng tử, điện động lực học, v.v.) từ các sách giáo khoa vật lý điển hình, cùng với một từ điển toàn diện về các vấn đề và giải pháp vật lý. Lưu ý rằng hầu hết các câu hỏi vật lý này chỉ chứa một câu trả lời duy nhất mà không có giải pháp từng bước.

--- TRANG 4 ---
Tạo ra Tất cả Câu hỏi 42.497
Giải pháp Đúng 19.824
Giai đoạn 1 Còn lại 22.673
Giải pháp Đúng 5.458
Giai đoạn 2 Còn lại 17.215
Giải pháp Đúng 7.687
Giai đoạn 3 Còn lại 9.528
Tổng Giải pháp Đúng 32.969 Nhãn=0 Nhãn=0 Nhãn=0 Nhãn=1 Nhãn=1 Nhãn=1 Giai đoạn 1 Giai đoạn 2 Giai đoạn 3 [Prompt 3] Đầu vào sau đây bao gồm một vấn đề khoa học, một giải pháp tương ứng và câu trả lời thực. Giải pháp đã cho là không chính xác, vui lòng phản tư về các lỗi của nó và sau đó tạo ra một giải pháp từng bước chính xác cho vấn đề dựa trên câu trả lời thực. [Prompt 2] Đầu vào sau đây bao gồm một vấn đề khoa học và một giải pháp tương ứng. Tuy nhiên, giải pháp này là không chính xác, vui lòng phản tư về các lỗi của nó và sau đó tạo ra một giải pháp từng bước chính xác cho vấn đề. [Prompt 1] Đầu vào sau đây bao gồm một vấn đề khoa học, vui lòng tạo ra một giải pháp từng bước chi tiết cho vấn đề. Chuỗi Tư duy Tạo ra Phản tư

Prompt Câu trả lời Tạo ra Tạo ra [Đầu ra Mẫu] Độ lệch pha Δφ= π/6, khoảng cách giữa hai điểm Δx= 0.2m, và chu kỳ dao động T = 0.4s đã biết. Theo độ lệch pha và mối quan hệ giữa bước sóng: Δφ= 2 PI (Δx/lambda), chúng ta có bước sóng lambda = 1.2m. Sau đó theo mối quan hệ giữa bước sóng λ và chu kỳ T và tốc độ sóng v: v = λ/T, giá trị đã biết được thay vào công thức trên: v = 1.2/0.4 = 3m/s. Tốc độ sóng là 3m/s. LLM

[Phản tư] Lỗi trong giải pháp là mối quan hệ giữa độ lệch pha và bước sóng được sử dụng trong tính toán bước sóng là không chính xác. Mối quan hệ đúng phải là Δφ= 2PI (Δx/lambda). [Câu trả lời] Đầu tiên, theo công thức ∆phi = 2 PI (∆x/lambda), chúng ta biết rằng lambda = 2 PI (0.2)/(PI / 6) = 1.2 m. Tiếp theo, chu kỳ dao động T = 0.4s đã biết, theo định nghĩa của công thức tốc độ sóng v v = lambda/T, chúng ta có v = 1.2m / 0.4s = 3m/s. Tóm lại, bước sóng là 1.2m và tốc độ sóng là 3m/s. LLM

[Phản tư] Lỗi trong giải pháp là tỷ lệ của độ lệch pha π/6 so với pha sóng đầy đủ 2π, thay vì pha nửa sóng π, tức là π/6 ÷2π, nên được sử dụng để xác định tỷ lệ của khoảng cách giữa hai điểm so với bước sóng. [Câu trả lời] Đầu tiên, độ lệch pha Δphi=PI/6 đã biết, khoảng cách giữa hai điểm Δx=0.2m, chúng ta có thể sử dụng công thức Δphi=2PI(Δx/lambda) để giải bước sóng lambda. Chúng ta có thể biết rằng lambda=(2PI(0.2)/(PI/6)=(2x0.2)/(1/6)=2.4m. Tiếp theo, chu kỳ dao động T=0.4s đã biết, theo định nghĩa của công thức tốc độ sóng v v=λ/T, chúng ta có được v=λ/T=2.4m/0.4s=6m/s. Tóm lại, bước sóng là 2.4m và tốc độ sóng là 6m/s. LLM [Prompt] Trên một tia sóng hài đơn giản, độ lệch pha dao động giữa hai điểm cách nhau 0.2\\mathrm{~m} là \\pi/6. Cũng biết rằng chu kỳ dao động là 0.4\\mathrm{~s}, thì bước sóng là, và tốc độ sóng là?

Hình 5: Quy trình làm việc và số lượng dữ liệu cho khung chú thích hướng dẫn tự phản tư.

•Hóa học. Chúng tôi thu thập câu hỏi từ các môn hóa học khác nhau, bao gồm Hóa học Tổng quát cấp đại học, Hóa học Phân tích, Hóa sinh và Hóa học Vô cơ. Khác với Vật lý, những vấn đề Hóa học này nhấn mạnh nhiều hơn vào các khái niệm kiến thức và đòi hỏi ít tính toán hơn.
•Toán. Các nhiệm vụ toán học cấp đại học được đặc trưng bởi logic phức tạp và độ phức tạp cao. Chúng tôi đã thu thập các vấn đề toán học bao gồm nhiều môn toán học khác nhau như Giải tích, Đại số và Toán học Cao cấp. Những vấn đề này có nhiều định dạng khác nhau, bao gồm câu hỏi trắc nghiệm, tính toán và các tình huống giải quyết vấn đề phức tạp. Quá trình thu thập của chúng tôi bao gồm việc tìm nguồn vấn đề từ các trang web hỏi đáp công cộng, bộ đề và sách giáo khoa.
•Chứng minh Hình thức (Lean). SciInstruct cũng bao gồm dữ liệu từ toán học hình thức. Trong toán hình thức, các định lý và chứng minh được biểu diễn bằng logic hình thức thay vì ngôn ngữ tự nhiên. Chúng trông giống như các chương trình máy tính. Ví dụ, "theorem gcd_self (n : Nat) : gcd n n = n" là một định lý nói rằng: Với bất kỳ số tự nhiên n nào, ước chung lớn nhất của n và chính nó là n. Và "cases n <;> simp [gcd, mod_self]" là một chứng minh của định lý này. Có nhiều ngôn ngữ để viết các định lý và chứng minh hình thức; các ví dụ bao gồm Coq [18], Isabelle [19], và Lean [20]. Chúng tôi chọn bao gồm dữ liệu từ Lean, vì nó cung cấp một thư viện toán học rộng lớn và hiện đang phổ biến nhất trong cộng đồng toán học. Do đó, nó có nhiều định lý và chứng minh được viết bởi các nhà toán học, bao phủ các chủ đề đa dạng trong toán học cấp độ sau đại học, chẳng hạn như giải tích, hình học và đại số. Cụ thể, chúng tôi xử lý dữ liệu từ LeanDojo [21] và định dạng nó để phù hợp với quá trình suy diễn liên tiếp, đảm bảo tính liên quan và khả năng áp dụng của nó cho việc huấn luyện mô hình cho các nhiệm vụ suy luận toán học bằng ngôn ngữ tự nhiên. Bước tiền xử lý này giúp thu hẹp khoảng cách giữa bản chất của tập dữ liệu Lean và các tín hiệu học tập mong đợi của mô hình. Cuối cùng, chúng tôi thu được 40.248 hướng dẫn cho chứng minh định lý. Như Phụ lục A.3, chúng tôi tạo thành một hướng dẫn cho mỗi định lý và chứng minh của nó.

Hướng dẫn Đa ngôn ngữ. Để nâng cao chất lượng và hiệu quả tổng thể của tập dữ liệu được tuyển chọn, chúng tôi cũng dịch các câu hỏi tiếng Trung mặc định sang tiếng Anh. Chúng tôi thấy rằng LLMs có xu hướng tạo ra các giải pháp chính xác sau khi dịch những vấn đề này sang tiếng Anh cho một số vấn đề tiếng Trung không có được giải pháp chính xác. Hiệu suất cải thiện này có thể do corpus tiếng Anh chất lượng cao hơn được sử dụng trong quá trình huấn luyện trước của LLMs. Do đó, chúng tôi đã áp dụng chiến lược này để xây dựng SciInstruct cho các câu hỏi tiếng Trung.

Tóm tắt. Tổng cộng, chúng tôi thu thập 257.143 câu hỏi thô, phần lớn thiếu các bước suy luận từng bước. Chúng tôi nhằm bổ sung những điều này thông qua một quá trình chú thích tự phản tư.

--- TRANG 5 ---
Bảng 2: Nghiên cứu loại bỏ của bước lọc. Chúng tôi sắp xếp các mẫu theo điểm số và loại trừ 10% thấp nhất. Các số kết quả đại diện cho độ chính xác trung bình có trọng số trên các nhiệm vụ đánh giá.

Loại Khoa học TB Toán TB {Khoa học+Toán} TB
Không lọc 43.57 48.50 46.03
Đã lọc 43.85 49.24 46.54

Bảng 3: Thống kê của SciInstruct theo môn học.
Môn học # Số lượng Tỷ lệ
Vật lý & Hóa học 123.869 48,76%
Toán 89.934 35,40%
Chứng minh Hình thức (Lean) 40.248 15,84%
Tổng cộng 254.051

2.2 Chú thích Hướng dẫn Tự phản tư
Cho một mô hình ngôn ngữ π để trả lời câu hỏi Q, các nghiên cứu gần đây [22] đã cho thấy rằng bằng cách đầu tiên buộc nó tạo ra các bước suy luận từng bước (R) trước, hiệu suất tổng thể để tạo ra câu trả lời A chính xác có thể được cải thiện đáng kể, thông qua: Pπ(A|Q) =ER∼Pπ(R|Q)h P(A|R, Q )i. Đây là lý do tại sao nhiều tập dữ liệu hướng dẫn nhằm thu thập các giải pháp trung gian chất lượng cao để huấn luyện LLMs tạo ra các giải pháp từng bước chính xác. Thách thức chính cho lĩnh vực khoa học, như chúng tôi nêu ở trên, là hầu hết các cặp QA chúng tôi thu thập không chứa các đường dẫn suy luận sự thật cơ bản (R). Việc có được suy luận trung gian chính xác R cho QA có thể được coi là một vấn đề suy luận biến ẩn rời rạc thông qua lấy mẫu hậu nghiệm. Tuy nhiên, trong thực tế, chúng ta không thể đủ khả năng lấy mẫu tất cả R có thể từ LLM. Ở đây chúng tôi áp dụng một chiến lược đơn giản hóa để giải quyết nó: 1) Sử dụng một LLM mạnh mẽ (chúng tôi sử dụng GPT-4-0613), chúng tôi lấy mẫu mỗi câu nhiều lần cho mỗi câu hỏi, ghi lại các dấu vết suy luận cũng như câu trả lời dự đoán; 2) Chúng tôi lọc ra chỉ những dấu vết có câu trả lời dự đoán chính xác, bằng cách giả định chỉ với các dấu vết chính xác, LLM mới có thể có được câu trả lời chính xác.

Tự phản tư của LLM. Tuy nhiên, ngay cả GPT-4 cũng không thể liên tục tạo ra các câu trả lời chính xác sau nhiều lần thử, và quy trình trên chỉ có thể thu thập R cho một phần câu hỏi. Dựa trên nghiên cứu trước đây chứng minh khả năng của các mô hình ngôn ngữ cho việc tự sửa chữa [23;24], chúng tôi tinh chỉnh quá trình CoT bằng cách sử dụng một khung tự phản tư, như được mô tả ở giữa Hình 4. Quá trình tạo ra phản tư cuối cùng bao gồm ba giai đoạn. Ban đầu, chúng tôi sử dụng một prompt CoT đơn giản (Prompt 1) để có được các giải pháp từng bước cho mỗi câu hỏi. Để có được một đánh giá chính xác về kết quả suy luận, chúng tôi sử dụng phương pháp gán nhãn GPT-4 dựa trên mô hình phần thưởng kết quả (ORM) [25] như một triển khai cơ bản trong công việc của chúng tôi, thay vì quá trình mô hình phần thưởng quá trình (PRM) [25] đắt đỏ thường đòi hỏi chú thích thủ công, đặc biệt cho suy luận khoa học phức tạp. Chúng tôi lọc ra các giải pháp không chính xác bằng cách áp dụng phương pháp gán nhãn GPT-4, dẫn đến 19.824 giải pháp chính xác. Tiếp theo, các giải pháp không chính xác từ giai đoạn một và các câu hỏi tương ứng của chúng chuyển sang giai đoạn hai. Ở đây, một prompt phản tư (Prompt 2) hỗ trợ GPT-4 trong việc tạo ra các câu trả lời chính xác bằng cách phân tích các lỗi của nó từ giai đoạn một. Các giải pháp mới được tạo ra sau đó được lọc lại bởi GPT-4, và những cái không mong muốn tiến đến giai đoạn ba. Trong giai đoạn ba, dựa trên Prompt 2, chúng tôi kết hợp câu trả lời thực như một gợi ý trực tiếp trong prompt (Prompt 3) để hỗ trợ thêm trong việc giải quyết câu hỏi. Phần cuối cùng của các giải pháp chính xác được thu được sau quá trình tạo ra và lọc. Chúng tôi minh họa việc tạo ra phản tư trong Hình 5 và định lượng số lượng dữ liệu được tạo ra bởi mỗi quá trình.

2.3 Lọc Hướng dẫn Chất lượng Thấp
Mặc dù các quy trình trên cho chúng ta các bước suy luận được chú thích (R), không phải tất cả chúng đều chính xác. Lỗi có thể đến từ hai nguồn: 1) mặc dù LLM tạo ra một câu trả lời chính xác, suy luận trung gian vẫn có thể sai [26]; 2) câu hỏi và giải pháp sự thật cơ bản được chuyển đổi qua Nhận dạng ký tự quang học (OCR) có thể không đầy đủ và không thể được biên dịch thành công. Do đó, chúng tôi đề xuất một bước khác để huấn luyện một bộ phân loại chất lượng hướng dẫn và lọc ra các hướng dẫn chất lượng thấp.

Tổng hợp Dữ liệu Chất lượng. Chúng tôi ngẫu nhiên chọn một tập con câu hỏi từ tập dữ liệu được gán nhãn của chúng tôi gồm 11.553 câu hỏi làm mẫu tích cực. Để tạo ra các mẫu tiêu cực, chúng tôi đã thúc đẩy ChatGLM2-6B, GPT-3.5-turbo-0613, và GPT-4-0613 cung cấp các câu trả lời từng bước cho các câu hỏi đã chọn. Chúng tôi lọc các câu trả lời không chính xác từ ChatGLM2-6B và gán nhãn các giải pháp từ GPT-3.5-turbo và GPT-4 bằng phương pháp prompt được định dạng được trình diễn trong Hình 11 trong Phụ lục A.4. Những giải pháp này được hợp nhất với các giải pháp được chú thích từ tập dữ liệu gốc để huấn luyện bộ phân loại của chúng tôi. Thành phần của tập dữ liệu hợp nhất được chi tiết trong Bảng 7 trong Phụ lục A.5.

--- TRANG 6 ---
Bộ phân loại Chất lượng Hướng dẫn. Chúng tôi cải thiện chất lượng tập dữ liệu bằng cách huấn luyện một bộ phân loại chất lượng hướng dẫn dựa trên Bảng 7 sử dụng các tính năng ChatGLM3-6B-Base. Sử dụng những dữ liệu này, chúng tôi huấn luyện một bộ phân loại chất lượng hướng dẫn thông qua tính năng được trích xuất trước bởi mô hình ChatGLM3-6B-Base. Bộ phân loại đầu ra một logit trong khoảng từ -1 đến 1, với điểm số cao hơn cho thấy câu trả lời đáng tin cậy hơn. Logit này được sử dụng để xếp hạng và chọn dữ liệu chất lượng cao từ tập dữ liệu nhiễu. Bảng 2 chứng minh hiệu quả của tinh chỉnh có giám sát trên cả tập dữ liệu đã lọc và chưa lọc ở quy mô 6B, cho thấy rằng các mô hình được huấn luyện trên tập dữ liệu đã lọc hoạt động tốt hơn.

Phân tích Lỗi. Vì bộ lọc phân loại của chúng tôi được huấn luyện trên các tập dữ liệu được gán nhãn và các giải pháp được tạo ra, các lỗi trong các giải pháp được gán nhãn tiêu cực từ ChatGLM2-6B, GPT-3.5-turbo, và GPT-4 có thể ảnh hưởng đáng kể đến hiệu suất của bộ phân loại. Do đó, chúng tôi tiến hành phân tích lỗi và phân loại chúng thành Sai lầm Toàn diện, Sai lầm Tính toán, và Suy luận Sai. Phân tích này được chi tiết trong Hình 12 trong A.6, chứng minh khả năng của bộ phân loại trong việc nhận biết những lỗi này trong tập dữ liệu.

Tóm tắt. Dựa trên các mô-đun phụ chính được đề cập ở trên, chúng tôi đã xây dựng tập dữ liệu SciInstruct, bao gồm 254.051 hướng dẫn, như được minh họa trong Bảng 3.

2.4 Tinh chỉnh Hướng dẫn với SciInstruct
Là mô hình nền tảng của chúng tôi, chúng tôi chọn ba LLMs, tức là ChatGLM3 (6B và 32B), Llama3-8B-Instruct, và Mistral-7B: MetaMATH. Sau khi thiết lập mô hình cơ sở, chúng tôi đã chuẩn hóa tất cả dữ liệu thành định dạng kiểu chatbot. Tiếp theo, chúng tôi đã tinh chỉnh mô hình nền tảng sử dụng SciInstruct, cho phép chúng tôi xác thực SciInstruct được xây dựng của chúng tôi. Trong suốt quá trình tinh chỉnh, chúng tôi đã tiến hành các thí nghiệm sử dụng thư viện Huggingface transformers. Đối với cả mô hình 6B và 32B, chúng tôi đã sử dụng tốc độ học 3e-6, sử dụng bộ lập lịch tuyến tính và huấn luyện trong hai epoch. Để huấn luyện mô hình một cách hiệu quả, chúng tôi đã tận dụng huấn luyện DeepSpeed [27].

Bảng 4: Kết quả trên các nhiệm vụ suy luận khoa học. Các thí nghiệm cho thấy rằng tinh chỉnh trên SciInstruct liên tục vượt trội so với mô hình cơ sở trên các quy mô tham số khác nhau. Khoa học TB đại diện cho điểm trung bình có trọng số của tất cả các nhiệm vụ khoa học trong cùng danh mục đánh giá, trong khi {Khoa học+Toán} TB biểu thị điểm trung bình có trọng số trên cả nhiệm vụ khoa học và toán học. Trong mỗi thiết lập tham số, Đậm làm nổi bật cái có hiệu suất tốt nhất, và Gạch dưới biểu thị kết quả tốt thứ hai. Kết quả được đánh dấu là † được điểm chuẩn bởi chính chúng tôi.

Mô hình CEval-Hard CEval-Sci MMLU-Sci SciEval SciBench GPQA_Diamond Khoa học TB {Khoa học+Toán} TB
(API, chi tiết tham số không rõ)
GPT-4 54.96 60.55 - 73.93 28.52 39.70 - -
GPT-3.5-turbo 41.37 46.83 - 66.97 12.17 - - -
Claude-v1.3 39.14 44.64 - 63.45 - - - -
(# tham số = 6B ∼7B)
LLaMA-2-7B 28.29†30.00†30.41 28.37 0.40 - - -
Galactica-6.7B 11.84†11.44†30.68 50.87 - - - -
ChatGLM2-6B 29.61†45.71†37.09†53.02†1.54†- - -
ChatGLM2-6B-Base 32.90†40.95†38.06†50.38†1.20†- - -
ChatGLM3-6B 36.84†38.57†41.78†56.56†2.40†28.70 34.14 29.73
ChatGLM3-6B-Base 45.40†54.29†40.16†61.69†2.40†24.75 38.12 40.34
SciGLM (ChatGLM3-6B-Base) 51.97 60.00 45.34 62.09 3.77 25.25 41.40 45.32
Llama3-8B-Instruct (zero-shot) 26.32†27.62†26.90†71.38†1.03†27.27†30.09 28.58
Llama3-8B-Instruct (few-shot) 25.66†23.33†52.67†71.38†3.60†31.31†34.66 37.92
+SciInstruct 32.24 34.76 40.86 66.47 3.60 29.29 34.54 36.04
Mistral-7B: MetaMATH (zero-shot) 9.87†8.57†28.25†63.61†4.63†27.78†23.79 25.57
Mistral-7B: MetaMATH (few-shot) 9.21†19.52†44.74†63.61†6.17†29.29†28.76 33.92
+SciInstruct 30.92 38.10 42.16 64.16 6.23 27.27 34.81 37.91
(# tham số = 12B ∼13B)
LLaMA-2-13B 19.74†19.05†35.85 36.96 1.37 26.20 22.59 22.13
Vicuna-13B - - 32.13 53.93 - - - -
(# tham số = 30B ∼32B)
Galactica-30B - - 35.53 54.96 - - - -
ChatGLM3-32B-Base 53.95†64.29†50.30†67.38†4.29†22.22 43.74 48.62
SciGLM (ChatGLM3-32B-Base) 56.58 66.19 49.38 69.84 5.15 25.76 45.48 51.47

3 Điểm chuẩn trên SciInstruct
3.1 Thiết lập Thí nghiệm
Các Nhiệm vụ Khoa học và Toán học. Các nhiệm vụ đánh giá được tóm tắt trong Bảng 8 trong Phụ lục A.7. Những nhiệm vụ này đã được chọn làm tập dữ liệu điểm chuẩn ngoài lĩnh vực, bao gồm CEval-Hard [28],

--- TRANG 7 ---
Bảng 5: Kết quả trên suy luận toán học. Toán TB đại diện cho điểm trung bình có trọng số của tất cả các nhiệm vụ toán học trong danh mục đánh giá. Kết quả được đánh dấu là † được điểm chuẩn bởi chính chúng tôi.

Mô hình GSM8K MATH Mathematics SAT-Math MMLU-Math CEval-Math Toán TB
(API, chi tiết tham số không rõ)
GPT-4 92.00 42.50 - 95.00 - 53.91 -
GPT-3.5-turbo 80.80 34.10 - 70.90 - 40.81 -
Claude-v1.3 - - - - - 37.66 -
(# tham số = 6B ∼7B)
LLaMA-2-7B 14.60 2.50 6.00 26.80 29.80 30.00†18.28
Galactica-6.7B 10.20 2.20 4.60 17.50 28.00 14.48†-
WizardMath-7B 54.90 10.70 9.30 25.40 31.10 - -
MAmmoTH (CoT)-7B 50.50 10.40 9.20 32.70 39.90 - -
MetaMath-7B 66.50 19.80 - - - - -
MAmmoTH & MetaMath-7B 66.30 24.10 18.30 41.40 44.40 - -
ChatGLM2-6B 25.85 6.90†14.30†39.55†38.91†36.67†27.03
ChatGLM2-6B-Base 31.54 7.84†17.10†34.55†40.45†32.22†27.28
ChatGLM3-6B 29.05 9.92†11.60†39.09†41.07†21.11†25.31
ChatGLM3-6B-Base 72.93 25.38†29.30†55.91†31.83†40.00†42.56
SciGLM (ChatGLM3-6B-Base) 73.62 25.18 31.80 65.46 49.38 50.00 49.24
Llama3-8B-Instruct (zero-shot) 7.35†20.76†4.50†58.64†54.52†16.67†27.07
Llama3-8B-Instruct (few-shot) 63.38†30.00†22.30†57.27†55.24†18.89†41.18
+SciInstruct 63.76 31.40 22.60 43.64 42.81 21.11 37.55
Mistral-7B: MetaMATH (zero-shot) 76.12†29.34†23.90†15.45†10.37†8.89†27.35
Mistral-7B: MetaMATH (few-shot) 72.33†28.28†24.40†50.45†42.40†16.67†39.09
+SciInstruct 76.65 30.30 25.00 43.82 41.48 28.89 41.02
(# tham số = 12B ∼13B)
LLaMA-2-13B 28.70 3.90 11.50 32.70 34.40 18.89†21.68
Vicuna-13B 28.40 5.80 10.00 34.00 34.10 - -
WizardMath-13B 63.90 14.00 14.10 24.50 32.10 - -
MAmmoTH (CoT)-13B 56.30 12.90 11.70 43.60 42.30 - -
MAmmoTH & MetaMath-13B 71.04 26.18 20.60 48.18 48.25 - -
(# tham số = 30B ∼32B)
Galactica-30B 41.70 12.70 11.80 37.70 37.90 - -
ChatGLM3-32B-Base 81.80 31.60†38.60†67.73†50.10†51.11†53.49
SciGLM (ChatGLM3-32B-Base) 83.70 32.86 35.00 76.36 61.29 55.56 57.46

CEval-Sci [28], MMLU-Sci [29], SciEval [6], SciBench [7], GPGQ [8], GSM8K [30], MATH [31], Mathematics [32], SAT-Math [33], MMLU-Math [29] từ MathInstruction, và CEval-MATH [28]. Những tập dữ liệu đánh giá này bao gồm một phạm vi rộng các môn học, bao gồm vật lý, hóa học và toán học.

Các Nhiệm vụ Tổng quát. Hơn nữa, chúng tôi đánh giá khả năng tổng quát hóa của các nhiệm vụ trên các quy mô khác nhau khi tinh chỉnh các mô hình. Những nhiệm vụ này bao gồm đánh giá khả năng kiến thức (MMLU [29] và CEval [28]) và tạo mã (MBPP) [34].

Chỉ số Đánh giá. Thiết lập mặc định cho mô hình cơ sở được tinh chỉnh vốn cung cấp giải pháp CoT. Do đó, chúng tôi tiến hành tất cả các thí nghiệm sử dụng thiết lập CoT. Để đánh giá kỹ lưỡng và chính xác khả năng của các mô hình khác nhau, chúng tôi sử dụng chỉ số độ chính xác cho tất cả các nhiệm vụ ngoại trừ tạo mã, mà chúng tôi sử dụng chỉ số pass@1.

Các Đường cơ sở. Chúng tôi xem xét các đường cơ sở sau (ví dụ: GPT-4 [35], GLM [13;14;36] và LLaMA Base [37], Tiếp tục Huấn luyện trước, và Tinh chỉnh Cụ thể Tập dữ liệu, v.v.) và mô tả chi tiết trong Phụ lục A.8. Chúng tôi sử dụng một khung đánh giá chuẩn hóa để so sánh các đường cơ sở GLM và LLaMA Base một cách công bằng. Để đo hiệu suất trong nhiệm vụ MATH, chúng tôi sử dụng cấu hình zero-shot và 8-shot để xác định độ chính xác cao nhất. Ngoài ra, đối với Mathematics, SAT-Math, MMLU, và CEval, chúng tôi sử dụng mô-đun chat để đánh giá. Khi xử lý các câu hỏi trắc nghiệm, chúng tôi công thức hóa prompt là "Do đó, trong số A đến D, câu trả lời là".

Ô nhiễm Dữ liệu. Cả SciInstruct và các điểm chuẩn đánh giá đều nằm trong lĩnh vực khoa học. Để giảm thiểu bất kỳ ô nhiễm dữ liệu tiềm ẩn nào và tăng cường tính toàn vẹn của kết quả, chúng tôi đảm bảo rằng tập huấn luyện được sử dụng để xây dựng SciInstruct không được lấy từ các tập kiểm tra được sử dụng trong đánh giá của chúng tôi. Nói cách khác, không có sự chồng chéo giữa SciInstruct và các điểm chuẩn đánh giá được sử dụng trong các thí nghiệm của chúng tôi.

--- TRANG 8 ---
3.2 Kết quả Chính và Phân tích
Bảng 4, Bảng 5, và Bảng 6 trình bày các phát hiện thí nghiệm cho các điểm chuẩn khoa học và toán học, cũng như các nhiệm vụ tổng quát. Kết quả cho thấy rằng huấn luyện trên SciInstruct được cung cấp có lợi cho các nhiệm vụ suy luận, cải thiện hiệu suất trong suy luận khoa học (tức là CEval-Sci, SciEval, SciBench, MMLU-Sci) và chuyển giao sang các nhiệm vụ toán học.

Bảng 6: Kết quả trên các nhiệm vụ hiểu ngôn ngữ tổng quát. Tinh chỉnh không hy sinh hầu hết các nhiệm vụ ngôn ngữ và chỉ giảm một chút trên nhiệm vụ tạo mã.

Mô hình MMLU CEval MBPP TB
GPT-4 86.40 68.70 83.00 79.37
ChatGLM3-6B-Base 61.32 67.09 55.80 61.40
SciGLM (6B-Base) 61.38 67.16 45.00 57.85
ChatGLM3-32B-Base 69.05 79.94 58.20 69.06
SciGLM (32B-Base) 70.08 79.64 56.60 68.78

Sự cải thiện hiệu suất như vậy phù hợp với các quy mô khác nhau của tham số mô hình cơ sở, trên 6B và 32B. Ngoài ra, cải thiện hiệu suất của SciGLM trong suy luận khoa học không hy sinh khả năng hiểu ngôn ngữ tổng quát của nó. Như được hiển thị trong Hình 13 trong A.9, trên các nhiệm vụ tiêu chuẩn như MMLU và CEval, SciGLM thậm chí đạt được cải thiện hiệu suất nhẹ. Trên tạo mã như MBPP, hiệu suất thấp hơn một chút nhưng nhìn chung vẫn nhất quán. Như được hiển thị trong Hình 15 trong A.10, chúng tôi trình bày một vấn đề thống kê trong SciBench được giải quyết chính xác với SciGLM (32B).

4 Phân tích SciInstruct
Ảnh hưởng của Hỗn hợp Dữ liệu. Chúng tôi tiến một bước khám phá cách các môn học đa dạng trong hỗn hợp SciInstruct ảnh hưởng đến các nhiệm vụ hạ nguồn khi huấn luyện mô hình SciGLM-6B. Bằng cách sử dụng chiến lược Leave-One-Out, tức là bỏ qua một môn học tại một thời điểm từ tập dữ liệu và huấn luyện lại,

Lĩnh vực trong SciInstruct Tập dữ liệu Đánh giá
Toán Chứng minh Hình thức Vật lý & Hóa học

CEval-Hard CEval-Sci
MMLU-Sci SciEval SciBench
GSM8K MATH
Mathematics
SAT-MATH CEval-Math
MMLU-Math Loading [MathJax]/extensions/MathMenu.js

Hình 6: Ảnh hưởng của các lĩnh vực khác nhau trong dữ liệu tinh chỉnh đối với các nhiệm vụ mục tiêu. Trọng số được tính bằng (Độ chính xác (SciGLM) - Độ chính xác (Loại trừ môn học) / Độ chính xác (SciGLM)) dưới thiết lập môn học leave-one-out.

chúng tôi đánh giá tầm quan trọng của mỗi môn học dựa trên tác động hiệu suất trên các nhiệm vụ khác nhau. Như được hiển thị trong Hình 6, chúng tôi có một phát hiện thú vị: mỗi môn học đóng góp các nhiệm vụ không bị hạn chế trong các lĩnh vực trực tiếp của nó. Ví dụ, dữ liệu Vật lý và Hóa học hỗ trợ đáng kể trong các nhiệm vụ CEval-Math, trong khi Toán và Chứng minh Hình thức cải thiện hiệu suất SciBench. Điều này cho thấy rằng tập dữ liệu hỗn hợp của chúng tôi cho phép LLMs có được một số kỹ năng suy luận tổng quát để giải quyết các câu hỏi khoa học thay vì chỉ quá khớp với một phân phối nhiệm vụ nhất định, và đạt được cải thiện phổ quát trên các nhiệm vụ hạ nguồn khác nhau.

Ảnh hưởng của Quy mô Dữ liệu. Một câu hỏi trung tâm cho việc tạo tập dữ liệu hướng dẫn là cần bao nhiêu mẫu để một mô hình học các kỹ năng cụ thể [38]. Các công trình trước [39] đã cho thấy rằng đối với các nhiệm vụ đối thoại, chỉ 1000 mẫu hướng dẫn chất lượng cao có thể dẫn đến những cải thiện đáng kể. Chúng tôi quan tâm đến việc phân tích luật quy mô dữ liệu cho việc giải quyết vấn đề khoa học. Thông qua huấn luyện lại mô hình với các tỷ lệ dữ liệu khác nhau và phân tích kết quả trong khoa học và toán học, như được hiển thị trong Hình 7, một mô hình thú vị chúng tôi tìm thấy là việc tăng dữ liệu ban đầu 10% mang lại cải thiện, nhưng các bổ sung tiếp theo không cho thấy những lợi ích đáng kể cho đến khi vượt qua ngưỡng 50%. Chúng tôi giả thuyết rằng những lợi ích ban đầu là do LLM được tinh chỉnh học suy luận cơ bản và định dạng nhiệm vụ, điều này đòi hỏi ít dữ liệu hướng dẫn hơn (ít hơn 30k). Tiến đến các kỹ năng phức tạp hơn, chẳng hạn như suy diễn phương trình, đòi hỏi một tập dữ liệu lớn hơn để học tập và tổng quát hóa hiệu quả. Nghiên cứu tương lai về cải thiện chất lượng dữ liệu có thể giảm yêu cầu dữ liệu cho việc học kỹ năng LLM.

Phân tích Pass@K về Đa dạng Mẫu. Một quan sát thú vị của suy luận LLM là với nhiệt độ khác không và lấy mẫu nhiều lần, ngay cả đối với những câu hỏi rất khó, LLM vẫn có cơ hội cao cung cấp câu trả lời chính xác. Pass@K được sử dụng rộng rãi cho tạo mã [40;41] và

--- TRANG 9 ---
0 
(cơ sở) 10% 
(25405) 20% 
(50810) 50% 
(127025) 90% 
(228645) (254051) 
100% 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 Độ chính xác (x% Dữ liệu)/ Độ chính xác (100% Dữ liệu)

Khoa học TB
Toán TB

Hình 7: Cải thiện hiệu suất qua quy mô khác nhau của dữ liệu hướng dẫn. Trục x đại diện cho tỷ lệ hướng dẫn mỗi lĩnh vực, và trục y đại diện cho hiệu suất tương đối so với SciGLM được huấn luyện với toàn bộ tập.

Hình 8: Đánh giá Pass@K trên SciBench (Quantum Chemistry) và SciEval. Tất cả các mẫu được tạo ở nhiệt độ 1.0. Kết quả cho thấy rằng tinh chỉnh hướng dẫn của chúng tôi không ảnh hưởng đến đa dạng mẫu, và tăng hiệu suất ngay cả với K lớn.

suy luận toán học [25]. Để phân tích liệu SciInstruct của chúng tôi có thể thực sự cải thiện suy luận tổng quát hay không, chúng tôi mô phỏng các giá trị Pass@K khác nhau như được hiển thị trong Hình 8. Chúng tôi sử dụng SciGLM (32B) và ChatGLM (32B) được tinh chỉnh để tạo ra N≥K (trong bài báo này, N= 30 và K≤25) giải pháp cho mỗi câu hỏi, cho phép kiểm tra chính xác hơn tỷ lệ vượt qua thực sự của LLM trên câu hỏi đó. Chúng tôi thấy rằng tinh chỉnh không ảnh hưởng đến đa dạng mẫu. SciGLM (32B) với K=25 trên SciBench và K=3 trên SciEval có thể đạt được hiệu suất tương đương với GPT-4, cho thấy tiềm năng của mô hình được tinh chỉnh của chúng tôi để đạt được kết quả tốt hơn. Chúng tôi giả thuyết rằng dữ liệu suy luận chất lượng cao và đa dạng thực sự dẫn dắt mô hình đến hành vi/kỹ năng tốt để phân tích và giải quyết các vấn đề khoa học khó thay vì quá khớp tập huấn luyện, cho thấy khả năng sử dụng tổng quát của tập dữ liệu SciInstruct tự chú th석 của chúng tôi.

5 Kết luận
Trong công trình này, chúng tôi trình bày một khung chú thích tự hướng dẫn để tạo ra một tập dữ liệu cấp độ cao và chất lượng cao, SciInstruct, để làm phong phú kiến thức khoa học của LLMs. Sử dụng SciInstruct, chúng tôi huấn luyện ba LLMs, cải thiện đáng kể nhiều điểm chuẩn khoa học và toán học so với các mô hình cơ sở và vượt trội nhiều LLMs tiên tiến có số lượng tham số lớn hơn một bậc. Nghiên cứu của chúng tôi nhấn mạnh tầm quan trọng của dữ liệu huấn luyện đa dạng cũng như tự chú thích LLM và tính chính xác để nâng cao khả năng suy luận tổng quát, ngay cả đối với các lĩnh vực khó như khoa học.

Hạn chế
Trong phần này, chúng tôi thảo luận thêm về các hạn chế trong quá trình nghiên cứu SciInstruct.

Quy mô của Tập dữ liệu và Mô hình. Mặc dù tập dữ liệu huấn luyện của chúng tôi đã mở rộng đến khoảng 254k, việc cải thiện hiệu suất mô hình vẫn đòi hỏi truy cập vào một tập dữ liệu thậm chí còn lớn hơn. Kết quả thí nghiệm của mô hình của chúng tôi được thực hiện ở 6 ∼8B và 32B tham số, dẫn đến hiệu suất tương đối tốt hơn. Tuy nhiên, điều quan trọng cần lưu ý là những hiệu suất này bị hạn chế bởi quy mô của mô hình. Tiến về phía trước, đáng để khám phá những lợi ích tiềm năng của việc tận dụng các tập dữ liệu và mô hình quy mô lớn hơn để cải thiện hiệu suất hơn nữa.

Sử dụng Bộ phân loại Dữ liệu để Nâng cao Việc Tạo ra của Mô hình. Phù hợp với những gì được thảo luận trong Phần 2.3, chúng tôi sử dụng một bộ phân loại chất lượng hướng dẫn để thúc đẩy chất lượng hướng dẫn, mang lại hiệu suất cải thiện như được hiển thị trong Bảng 2. Tuy nhiên, chúng tôi dự đoán rằng bộ phân loại chất lượng hướng dẫn, còn được gọi là mô hình phần thưởng, có thể cung cấp những lợi ích thậm chí còn lớn hơn. Một hướng cải thiện cụ thể có thể là bootstrapping dữ liệu để cải thiện khả năng của mô hình cơ sở.

Tác động Rộng hơn
Tác động tích cực. Bài báo này nhằm xây dựng hướng dẫn cấp độ cao và chất lượng cao để cải thiện khả năng suy luận khoa học của LLMs, giúp LLMs đưa ra câu trả lời tốt hơn cho các câu hỏi ở cấp độ đại học. Thu thập các hướng dẫn đa dạng, chú thích các hướng dẫn tự phản tư, và lọc ra các hướng dẫn chất lượng thấp cung cấp cho các nhà nghiên cứu hiểu biết để chuẩn bị các tập dữ liệu huấn luyện.

Tác động tiêu cực. Một nhược điểm của công trình này là quy mô của tập dữ liệu huấn luyện và mô hình tương đối nhỏ, và chúng tôi có thể giải quyết điều này bằng cách bootstrapping một tập dữ liệu huấn luyện lớn hơn. Chúng tôi tin rằng lợi ích của cách tạo dữ liệu vượt trội hơn nhược điểm.

--- TRANG 10 ---
Lời cảm ơn
Công trình này được hỗ trợ bởi NSFC 62276148, NSFC cho Học giả Trẻ Xuất sắc 62425601, quỹ nghiên cứu từ Zhipu, New Cornerstone Science Foundation thông qua XPLORER PRIZE và Đại học Thanh Hoa (Khoa Khoa học và Công nghệ Máy tính) - Siemens Ltd., Trung tâm Nghiên cứu Chung Trung Quốc về Trí tuệ Công nghiệp và Internet of Things (JCIIOT).

Tài liệu tham khảo
[1]Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, và Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[2]Microsoft Research AI4Science và Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361, 2023.
[3]Bo Chen, Xingyi Cheng, Yangli ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, và Le Song. xtrimopglm: Unified 100b-scale pre-trained transformer for deciphering the language of protein. bioRxiv, 2023.
[4]Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, và Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature, 619:1–6, 07 2023.
[5]Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Le Zhou, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, et al. Learning a foundation language model for geoscience knowledge understanding and utilization. arXiv preprint arXiv:2306.05064, 2023.
[6]Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, và Kai Yu. Scieval: A multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149, 2023.
[7]Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, và Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023.
[8]David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, và Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.
[9]Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, và Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024.
[10] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, và Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.
[11] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, và Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.
[12] Xiang Yue, Tuney Zheng, Ge Zhang, và Wenhu Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024.
[13] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335, 2022.
[14] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.

--- TRANG 11 ---
[15] Meta AI. Meta llama 3. https://ai.meta.com/blog/meta-llama-3/, 2024. Truy cập: 2024-05-22.
[16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[17] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, và Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.
[18] Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria, 1997.
[19] Tobias Nipkow, Markus Wenzel, và Lawrence C Paulson. Isabelle/HOL: a proof assistant for higher-order logic. Springer, 2002.
[20] Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, và Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pages 378–388. Springer, 2015.
[21] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, và Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. In Neural Information Processing Systems (NeurIPS), 2023.
[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, và Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, và A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.
[23] Noah Shinn, Beck Labash, và Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
[24] Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, và Tianyi Zhou. Reflection-tuning: Data recycling improves llm instruction-tuning. arXiv preprint arXiv:2310.11716, 2023.
[25] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, và Karl Cobbe. Let's verify step by step. CoRR, abs/2305.20050, 2023.
[26] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, và Ethan Perez. Measuring faithfulness in chain-of-thought reasoning. CoRR, abs/2307.13702, 2023.
[27] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020.
[28] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.
[29] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

--- TRANG 12 ---
[30] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[31] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, và Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[32] Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):70–74, 2021.
[33] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, và Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.
[34] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[35] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[36] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024.
[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[38] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, và Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.
[39] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.
[40] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[41] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. Competition-level code generation with alphacode. CoRR, abs/2203.07814, 2022.
[42] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843–3857, 2022.
[43] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, và Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
[44] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, và Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585–597, 2015.

--- TRANG 13 ---
[45] Subhro Roy và Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016.
[46] Arkil Patel, Satwik Bhattamishra, và Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021.
[47] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, và Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, 2014.
[48] Wang Ling, Dani Yogatama, Chris Dyer, và Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
[49] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, và Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.
[50] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, và Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.
[51] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, và Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660, 2022.
[52] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517, 2022.
[53] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, và Tony Xia. Theoremqa: A theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524, 2023.
[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.
[55] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.
[56] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, và Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, và Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.
[58] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.
[59] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[60] Boshi Wang, Xiang Deng, và Huan Sun. Iteratively prompt pre-trained language models for chain of thought. arXiv preprint arXiv:2203.08383, 2022.

--- TRANG 14 ---
[61] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, và Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.
[62] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, và Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315–5333, 2023.
[63] Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, và Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023.
[64] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, và Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.
[65] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.
[66] Xue Lilong, Zhang Dan, Dong Yuxiao, và Tang Jie. Autore: Document-level relation extraction with large language models. arXiv preprint arXiv:2403.14888, 2024.
[67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, và Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[68] Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.
[69] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, và Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.
[70] Sang Michael Xie, Aditi Raghunathan, Percy Liang, và Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
[71] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, và Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.
[72] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.
[73] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[74] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.
[75] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[76] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.

--- TRANG 15 ---
[77] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[78] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.
[79] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.
[80] Ariel N Lee, Cole J Hunter, và Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. arXiv preprint arXiv:2308.07317, 2023.

Phần I
Phụ lục

Mục lục
A Phụ lục 16
A.1 Các Công trình Liên quan . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Phân tích SciInstruct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Ví dụ Hướng dẫn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.4 Gán nhãn GPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.5 Tập dữ liệu của Bộ phân loại Chất lượng Hướng dẫn . . . . . . . . . . . . . . . . 17
A.6 Phân tích Lỗi Chi tiết . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.7 Các Nhiệm vụ Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.8 Các Đường cơ sở Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.9 Kết quả Thí nghiệm Chi tiết . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.10 Ví dụ Đầu ra của SciGLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.11 Ví dụ của SciInstruct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.12 Tập dữ liệu Suy luận Khoa học . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.13 Suy luận Tổng quát với LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.14 Tinh chỉnh Hướng dẫn trong LLMs . . . . . . . . . . . . . . . . . . . . . . . 27

--- TRANG 16 ---
A Phụ lục
A.1 Các Công trình Liên quan
Gần đây, đã có những tiến bộ để thu hẹp khoảng cách trong khó khăn suy luận và môn học đánh giá từ ba góc độ cho suy luận khoa học với LLMs.

Đánh giá cấp độ cao như SciBench [7] và GPQA [8], đánh giá khả năng suy luận khoa học của LLMs ở cấp độ đại học và thậm chí cấp độ sau đại học. Ngoài ra, SciEval [6] cung cấp một điểm chuẩn đánh giá LLMs đa cấp để giải quyết các vấn đề rò rỉ dữ liệu và các vấn đề khả năng đánh giá câu hỏi/câu trả lời chủ quan.

Tiếp tục huấn luyện trước như Galactica [1] và MINERVA [42], tiếp tục huấn luyện các LLMs cơ sở tương ứng của họ trên nhiều văn bản web bao gồm corpus liên quan đến khoa học hoặc liên quan đến toán học. Phương pháp tiếp tục huấn luyện trước này khám phá tiềm năng của LLMs cho khoa học và đóng góp cho các mô hình nguồn mở cho cộng đồng khoa học, nhưng nó tốn kém về mặt tính toán.

Tinh chỉnh cụ thể tập dữ liệu như RFT [38], WizardMath [43], MAmmoTH [10], và MetaMath [11], xây dựng các tập dữ liệu nhất định bao gồm GSM8K và MATH, tiến hành tinh chỉnh có giám sát của LLMs và đánh giá các điểm chuẩn phổ biến GSM8K và MATH. MAmmoTH không chỉ cải thiện hiệu suất trong lĩnh vực (IND) như GSM8K mà còn tổng quát hóa cho các nhiệm vụ toán học rộng hơn bằng cách xây dựng một phổ các tập dữ liệu tinh chỉnh hướng dẫn toán học bao gồm các tập dữ liệu trong lĩnh vực và ngoài lĩnh vực. Loạt phương pháp này chủ yếu tập trung vào các nhiệm vụ suy luận toán học.

A.2 Phân tích SciInstruct

Điền vào
33.2%
Câu hỏi Trắc nghiệm 32.0% Bài toán Giải pháp Phức tạp 20.1% Bài toán Giải pháp Đơn giản
14.6% Tỷ lệ Loại Câu hỏi
(a)

Bài toán Giải pháp Phức tạp
54.8%
Bài toán Giải pháp Đơn giản 35.5% Điền vào 9.3% Câu hỏi Trắc nghiệm 0.4% Tỷ lệ Loại Câu hỏi trong Toán học (b)

Điền vào
35.0%
Câu hỏi Trắc nghiệm 35.0% Bài toán Giải pháp Đơn giản
17.7% Bài toán Giải pháp Phức tạp
12.2% Tỷ lệ Loại Câu hỏi trong Vật lý
(c)

Điền vào
40.3%
Câu hỏi Trắc nghiệm 40.3% Bài toán Giải pháp Phức tạp
18.9% Bài toán Giải pháp Đơn giản
0.4% Tỷ lệ Loại Câu hỏi trong Hóa học (d)

Hình 9: Chúng tôi trình bày tỷ lệ các loại câu hỏi trong tất cả môn học và từng môn học.

--- TRANG 17 ---
0 200 400 600 800 1000
Độ dài của câu hỏi 0 200 400 600 800 1000 Số lượng câu hỏi (a)

0 200 400 600 800 1000
Độ dài của câu hỏi 0 1000 2000 3000 4000 5000 6000 7000 Độ dài trung bình của câu trả lời (b)

Hình 10: Chúng tôi hiển thị số lượng câu hỏi có độ dài đó và độ dài trung bình của tất cả câu trả lời dưới mỗi câu hỏi.

A.3 Ví dụ Hướng dẫn
Định dạng Hướng dẫn cho Vật lý, Hóa học và Toán. Để làm cho tập dữ liệu của chúng tôi nhất quán, chúng tôi định nghĩa một mẫu thống nhất cho quá trình giải quyết vấn đề phù hợp với dòng logic tư duy của con người cho Vật lý, Hóa học và Toán. Đối với mỗi câu hỏi, trước tiên chúng tôi liệt kê phân tích cơ bản về các khái niệm khoa học (hoặc điểm kiến thức) nào mà câu hỏi này yêu cầu, và sau đó trình bày các bước suy luận từng bước của giải pháp chi tiết, và cuối cùng tóm tắt câu trả lời.

Đây là một ví dụ về định dạng hướng dẫn.
Vấn đề: ∗ ∗ ∗ .
Câu trả lời:
Phân tích: Câu hỏi này kiểm tra ∗ ∗ ∗ điểm kiến thức.
Bước 1: ∗ ∗ ∗ .
Bước i: ∗ ∗ ∗ .
Bước n: ∗ ∗ ∗ .
Tóm lại, câu trả lời cho câu hỏi này là ∗ ∗ ∗ .

Định dạng Hướng dẫn cho Toán Hình thức.
Đây là một ví dụ về định dạng hướng dẫn cho Lean.
Vấn đề: theorem gcd_self (n : Nat) : gcd n n = n
Câu trả lời: cases n <;> simp [gcd, mod_self]

A.4 Gán nhãn GPT
Như được hiển thị trong Hình 11, chúng tôi trình bày quá trình gán nhãn GPT-4. Điều quan trọng cần lưu ý là việc giải quyết so sánh giải pháp bằng cách có được đánh giá chính xác về các bước suy luận, chẳng hạn như thông qua việc sử dụng mô hình phần thưởng quá trình (PRM), thường đòi hỏi các chú thích thủ công tốn kém, đặc biệt cho suy luận khoa học phức tạp. Do đó, công việc của chúng tôi sử dụng phương pháp gán nhãn dựa trên mô hình phần thưởng kết quả (ORM) như một triển khai cơ bản. Mặc dù phương pháp này có thể có những hạn chế, nó phục vụ như một nền tảng để xây dựng tập huấn luyện cho bộ phân loại chất lượng hướng dẫn của chúng tôi. Hơn nữa, Bảng 2 chứng minh hiệu quả của bộ phân loại này.

A.5 Tập dữ liệu của Bộ phân loại Chất lượng Hướng dẫn
Như được hiển thị trong Bảng 7, chúng tôi liệt kê thống kê dữ liệu được sử dụng để huấn luyện bộ phân loại chất lượng hướng dẫn.

A.6 Phân tích Lỗi Chi tiết
Như được hiển thị trong Hình 12, chúng tôi đưa ra các ví dụ lỗi chi tiết, được tóm tắt trong Phần 2.3.

--- TRANG 18 ---
Tự phản tư. Vì bộ phân loại của chúng tôi được huấn luyện trên tập dữ liệu được gán nhãn và các giải pháp được tạo ra, các lỗi xảy ra trong các giải pháp được gán nhãn tiêu cực của ChatGLM2-6B, GPT-3.5-turbo, và GPT-4 có thể ảnh hưởng mạnh đến hiệu suất của bộ phân loại. Do đó, chúng tôi phân tích các lỗi và phân loại chúng thành ba lớp chính (Sai lầm Tính toán, Suy luận Sai, và Sai lầm Toàn diện), được hiển thị dưới đây trong Hình 2. Điều này có nghĩa là bộ phân loại của chúng tôi có một số khả năng xác định những lỗi này trong các tập dữ liệu.

Gán nhãn GPT-4
Prompt: Đầu vào sau bao gồm ba phần. Nó bắt đầu với một câu hỏi, tiếp theo là một giải pháp từng bước cho câu hỏi, và kết thúc bằng câu trả lời đúng. Nếu câu trả lời thu được trong giải pháp tương đương với câu trả lời thực, xuất '1', ngược lại xuất '0'.

Đầu vào
Q: Một photon di chuyển với tốc độ c, một người đuổi theo ánh sáng với tốc độ 0.9c, người đó quan sát thấy tốc độ ánh sáng là?
Giải pháp: Theo thuyết tương đối đặc biệt của Einstein, chúng ta không thể đơn giản cộng các vận tốc. Trong tương đối tính, sự chồng chất của vận tốc tuân theo biến đổi Lorentz, thay vì phép cộng cổ điển. Đối với tốc độ ánh sáng, có một trường hợp đặc biệt trong đó người quan sát đo tốc độ ánh sáng là c, bất kể tốc độ của người quan sát. Điều này có nghĩa là ngay cả khi một người quan sát đang di chuyển với tốc độ 0.9c, tốc độ ánh sáng anh ta đo vẫn là c. Do đó, tốc độ ánh sáng được quan sát bởi người này là c.
Câu trả lời Thực: c.
Đầu ra GPT-4: 1

Bộ phân loại Dữ liệu. Thật vậy, các tập dữ liệu thực tế có nhiễu và đưa ra thách thức. Để cải thiện chất lượng dữ liệu và giữ lại nhiều tập dữ liệu hữu ích hơn, chúng tôi huấn luyện một bộ phân loại dữ liệu và lọc nghiêm ngặt các tập dữ liệu nhiễu. Như đã đề cập, trong quá trình huấn luyện bộ phân loại dữ liệu, chúng tôi ngẫu nhiên chọn một số câu hỏi Q từ các tập dữ liệu chất lượng cao được chú thích bởi các người chú thích. Về các mẫu tích cực, chúng tôi chọn các câu trả lời chính xác của GPT-3.5-turbo và GPT-4 cho Q. Trong khi đó, câu trả lời lỗi được tạo ra bởi GPT-3.5-turbo và GPT-4 có thể được xem như các mẫu tiêu cực. Chúng tôi huấn luyện bộ phân loại của mình để xuất ra một logit vô hướng trong khoảng từ -1 đến 1. Điểm số cao hơn đại diện cho câu trả lời đáng tin cậy và chính xác hơn. Dựa trên bộ phân loại dữ liệu đã được huấn luyện, mỗi dữ liệu từ tập dữ liệu nhiễu có thể nhận được một logit l phản ánh tính đúng đắn và đầy đủ của nó, có thể được sử dụng thêm để tự động xếp hạng và chọn dữ liệu chất lượng cao. Trong Bảng 2, chúng tôi chứng minh hiệu quả của lọc hướng dẫn bằng tinh chỉnh có giám sát trên cả tập dữ liệu đã lọc và chưa lọc ở quy mô 6B. So với các mô hình được huấn luyện trên tập dữ liệu chưa lọc, những mô hình được huấn luyện trên tập dữ liệu đã lọc vượt trội hơn các nhiệm vụ trung bình. Điều này nhấn mạnh tầm quan trọng của chất lượng dữ liệu hơn số lượng dữ liệu cho các nhiệm vụ khoa học.

Bảng 2: Nghiên cứu loại bỏ về lọc hướng dẫn.
Loại Toán TB Khoa học TB Điểm chuẩn TB
Chưa lọc 39.03 52.54 43.53
Đã lọc 40.85 52.28 44.66

2.1.3 Tối ưu hóa Hướng dẫn
Bổ sung Tư duy. Nhiều câu hỏi chỉ có câu trả lời cuối cùng mà thiếu quá trình chuỗi tư duy. Do các tập dữ liệu chất lượng cao khan hiếm, điều đó quan trọng là bổ sung các giải pháp. Đối với các câu hỏi không có tư duy, chúng tôi sử dụng GPT-4 Turbo để bổ sung chúng với tư duy và giữ lại tư duy đúng bằng cách so sánh câu trả lời được tạo ra với câu trả lời đúng. Chúng tôi lấy một vấn đề của môn vật lý làm ví dụ và hiển thị phân tích và gán nhãn của GPT-4 Turbo.

Hình 11: Chúng tôi trình bày quá trình gán nhãn GPT-4. Cho một prompt, câu hỏi, giải pháp và câu trả lời thực, GPT-4 có thể đưa ra đầu ra nhãn cuối cùng.

Bảng 7: Kết hợp dữ liệu dựa trên một tập hợp giới hạn các ví dụ được gán nhãn để huấn luyện bộ phân loại dữ liệu và lọc ra các hướng dẫn nhiễu.

Nguồn # Mẫu Tích cực # Mẫu Tiêu cực Tổng cộng
Dữ liệu Sạch Gốc 11.553 0 11.553
ChatGLM2-6B 0 23.106 23.106
GPT-3.5-Turbo-0613 745 2.075 2.820
GPT-4-0613 332 3.898 4.230
Tổng cộng 12.630 29.079 41.709

A.7 Các Nhiệm vụ Đánh giá
Bảng 8: Tổng quan về Các Nhiệm vụ Đánh giá Khoa học. (†: Đại số Tiền, ‡: Đại số Trung, §: Đại số, △: Xác suất, ♢: Lý thuyết Số, △: Thống kê, ▲: Giải tích, ▼: Hình học, ♦: Vật lý, ⋆: Hóa học, ♣: Sinh học)

Tập dữ liệu Đánh giá # Mẫu Dạng Câu trả lời Môn học
CEval-Hard 152 Trắc nghiệm △,▲,♢,△,♦,⋆
CEval-Sci 210 Trắc nghiệm △,♢,△,▲,♦,⋆,♣
MMLU-Sci 1.855 Trắc nghiệm §,△,♢,△,▲,♦,⋆,♣
SciEval 15.901 Trắc nghiệm ♦,⋆,♣
MATH 5.000 Mở †,‡,§,△,♢,▲,▼
Mathematics 1.000 Mở †,‡,♢,▲
SAT-Math 220 Trắc nghiệm ‡,△,▼
MMLU-Math 974 Trắc nghiệm §,△,♢,▲
CEval-Math 90 Trắc nghiệm △,△,♢,▲

• CEval-Hard [28] bao gồm tám môn học ở cấp độ khó từ CEval.
• CEval-Sci [28] được chọn từ CEval bao gồm các môn vật lý, hóa học và toán học.
• MMLU-Sci [29] tương tự như CEval-Sci và bao gồm các môn vật lý, hóa học và toán học được chọn từ MMLU.

--- TRANG 19 ---
Sai lầm hiểu biết
Giải thích
Câu trả lời không giải quyết nhiệm vụ chính xác như được hướng dẫn, thường bao gồm đầu ra không đủ.
Ví dụ
Q: Một hạt di chuyển theo đường thẳng theo hướng x có tọa độ x= 5t2−t3 tại thời điểm t, trong đó x được đo bằng mét và t bằng giây. Tìm: chuyển vị và vận tốc trung bình trong giây thứ 4.
A: Chuyển vị trong 4 giây có thể được tính bằng cách tính tọa độ tại t= 4 trừ tọa độ tại t= 0, Chuyển vị =x(4)−x(0) = 5 ×42−43−5×02+ 03= 5×16−64−0 + 0 = 80 −64 = 16. Vận tốc trung bình trong 4 giây có thể được tính bằng cách tính chuyển vị chia cho thời gian, tức là vận_tốc_trung_bình =chuyển_vị/thời_gian = 16/4 = 4. Tóm lại, chuyển vị trong 4 giây là 16 mét, tốc độ trung bình là 4 mét/giây, và khoảng cách di chuyển của hạt là 16 mét.
Lỗi: Hiểu sai. Câu trả lời nên là chuyển vị và vận tốc trung bình 'trong giây thứ 4' chứ không phải 'trong 4 giây'.

Sai lầm tính toán
Giải thích
Câu trả lời không tính toán chính xác hoặc đơn giản hóa các biểu thức toán học, đặc biệt khi gặp tích phân và đạo hàm.
Ví dụ
Q: Trong một vỏ có bán kính trong R1 và bán kính ngoài R2, mật độ điện tích trong vỏ là ρ=a+br, Trong đó r là khoảng cách từ tâm đến điểm quan sát, và không có phân bố điện tích ở nơi khác trong không gian. (a) Tìm biểu thức mà trường điện ở khắp mọi nơi trong không gian phụ thuộc vào r.
A: Trước hết, theo định luật Gauss, tổng thông lượng điện của bất kỳ bề mặt kín nào bên trong bề mặt kín bằng tổng điện tích bên trong bề mặt kín. Theo các điều kiện trong chủ đề, mật độ điện tích trong vỏ là ρ=a+br, tổng điện tích trong vỏ có thể được tính như: Q=/integraltextR2 R1/integraltextπ 0/integraltext2π 0(a+br)·r2sinθdϕdθdr = 4π/integraltextR2 R1(a+br)r2dr= 4π/parenleftig a 4(R4 2−R4 1) +b 5(R5 2−R5 1)/parenrightig. Theo định lý Gauss, bên trong vỏ cầu, thông lượng của trường điện bằng không. Do đó, chúng ta chỉ cần tạo ra trường điện bên ngoài vỏ. Bên ngoài vỏ, theo Định luật Coulomb, trường điện được tạo ra bởi điện tích có thể được biểu diễn như: E=1 4πϵ0Q r2 Thay Q vào công thức này thu được: E= 1 4πϵ04π(a 4(R4 2−R4 1)+b 5(R5 2−R5 1)) r2 đơn giản hóa, chúng ta có thể có trường điện phụ thuộc vào biểu thức của r như sau: E=1 ϵ0/parenleftig a r2(R4 2−R4 1) +b r2(R5 2−R5 1)/parenrightig
Lỗi: Tích phân sai.

Suy luận sai
Giải thích
Giải pháp bao gồm các bước suy luận sai hoặc thông tin bịa đặt trong suy luận.
Ví dụ
Q: Công thoát của polonium và barium lần lượt là 4,52 eV và 2,50 eV. Tính tần số cắt của barium và polonium.
A: Theo lý thuyết hiệu ứng quang điện, mối quan hệ giữa công thoát và điện thế cắt có thể được biểu diễn bằng công thức sau công thoát = điện thế cắt − năng lượng photon. Áp dụng mối quan hệ giữa công thoát và điện thế cắt cho trường hợp barium và polonium. Đối với polonium, điện thế cắt = công thoát + năng lượng photon = 4,52eV+hν. Đối với barium, điện thế cắt = công thoát + năng lượng photon = 2,50eV+hν, trong đó h là hằng số Planck và ν là tần số ánh sáng.
Trong phạm vi nhìn thấy, phạm vi tần số ánh sáng khoảng 4,3×1014Hz đến 7,5×1014Hz. Bây giờ chúng ta có thể tính tần số cắt của barium và polonium tương ứng. Đối với polonium, 4,52eV+hνcut= 4,30eV.hνcut= 4,30eV−4,52eV.hνcut=−0,22eV. Đối với barium, 2,50eV+ hνcut= 4,30eV.hνcut= 4,30eV−2,50eV.hνcut= 1,80eV.
Tóm lại, tần số cắt của polonium là −0,22eV, và của barium là 1,80eV. Vì tần số cắt là tần số ánh sáng và phải dương, barium có thể được sử dụng làm vật liệu cathode ống quang điện trong phạm vi nhìn thấy. Câu trả lời cuối cùng: Barium có thể được sử dụng làm vật liệu cathode ống quang điện trong phạm vi nhìn thấy.
Lỗi: Suy luận sai, điện thế cắt nên là W/h.

[7]Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, và Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.
13

Hình 12: Phân tích lỗi của ba loại tự phản tư cho lọc hướng dẫn nhiễu.

--- TRANG 20 ---
• SciEval [6] phục vụ như một điểm chuẩn toàn diện, đa ngành để đánh giá khả năng trả lời câu hỏi chủ quan.
• MATH [31] là một điểm chuẩn tiêu chuẩn để đánh giá khả năng toán học của LLMs.
• Mathematics [32] chủ yếu bao gồm Đại số, Lý thuyết Số, và Giải tích.
• SAT-Math [33] là một nhiệm vụ toán bao gồm Đại số Trung cấp, Xác suất, và Giải tích.
• MMLU-Math [29] đánh giá giống như MAmmoTH [10], bao gồm Đại số, Xác suất, Lý thuyết Số, và Giải tích.
• CEval-MATH [28] chỉ bao gồm các môn toán học được trích xuất từ CEval.

A.8 Các Đường cơ sở Đánh giá
• GPT-4 [35]: Chúng tôi xem xét mô hình nguồn đóng GPT-4, sử dụng prompting CoT.
• GLM [13;14] và LLaMA Base [37]: Chúng tôi chọn ChatGLM2-6B (Base), ChatGLM3-6B (Base), và ChatGLM3-32B (Base) làm mô hình cơ sở. Ngoài ra, chúng tôi xem xét LLaMA-2-7B và LLaMA-2-13B làm đường cơ sở.
• Tiếp tục Huấn luyện trước: Chúng tôi bao gồm Galactica [1] mặc dù chúng tôi lưu ý rằng việc tiếp tục huấn luyện trước của nó không nhất thiết bao gồm suy luận khoa học cấp độ đại học.
• Tinh chỉnh Cụ thể Tập dữ liệu: Đối với tinh chỉnh cụ thể tập dữ liệu, chúng tôi xem xét WizardMath [43], MAmmoTH-CoT [10], và MetaMath [11], thích ứng với tập dữ liệu MATH.

A.9 Kết quả Thí nghiệm Chi tiết
Về kết quả theo lĩnh vực trong Hình 13, chúng tôi trình bày kết quả môn học chi tiết trong Bảng 9 và Bảng 10.

6B 32B 35.0 37.5 40.0 42.5 45.0 47.5 50.0 Điểm Trung bình Nhiệm vụ Toán học

6B 32B 58 60 62 64 66 68 70 Nhiệm vụ Vật lý

6B 32B 55.0 57.5 60.0 62.5 65.0 67.5 70.0 Điểm Trung bình Nhiệm vụ Hóa học

6B 32B 60 62 64 66 68 70 72 74 Điểm Trung bình Nhiệm vụ Tổng quát

ChatGLM3
SciGLM (Của chúng tôi)

Hình 13: SciGLM liên tục cải thiện ChatGLM3-Base cho các nhiệm vụ khoa học khác nhau, mà không hy sinh khả năng hiểu ngôn ngữ tổng quát.

Bảng 9: Kết quả chi tiết của các môn vật lý cho Hình 3. Đậm biểu thị kết quả tốt nhất trong các mô hình nguồn đóng và nguồn mở.

mô hình TB CEval MMLU SciEval
TB Vật lý Đại học Vật lý Trung học Phổ thông Vật lý Trung học Cơ sở TB Vật lý Đại học Vật lý Khái niệm Vật lý Trung học Cơ sở Vật lý.
ChatGLM3-6B-Base 54.32 63.16 42.11 63.16 84.21 45.59 39.22 55.32 34.44 46.95
SciGLM (ChatGLM3-6B-Base) 56.59 66.67 47.37 63.16 89.47 46.52 34.31 55.32 41.06 47.56
ChatGLM3-32B-Base 67.85 78.95 57.89 78.95 100.00 56.76 42.16 62.55 57.62 51.22
SciGLM (ChatGLM3-32B-Base) 69.65 80.70 63.16 78.95 100.00 58.61 41.18 66.81 57.62 56.71

--- TRANG 21 ---
Bảng 10: Kết quả chi tiết của các môn hóa học cho Hình 3. Đậm biểu thị kết quả tốt nhất trong các mô hình nguồn đóng và nguồn mở.

mô hình TB CEval MMLU SciEval
TB Hóa học Đại học Hóa học Trung học Phổ thông Hóa học Trung học Cơ sở TB Hóa học Đại học Hóa học Trung học Phổ thông Hóa học.
ChatGLM3-6B-Base 58.25 66.67 45.83 63.16 95.00 49.83 45.00 52.22 55.76
SciGLM (ChatGLM3-6B-Base) 58.55 68.25 41.67 73.68 95.00 48.84 47.00 49.75 57.27
ChatGLM3-32B-Base 66.44 69.84 41.67 78.95 95.00 63.04 51.00 68.97 60.15
SciGLM (ChatGLM3-32B-Base) 69.05 71.42 45.83 78.95 95.00 66.67 57.00 71.43 61.40

A.10 Ví dụ Đầu ra của SciGLM
Chúng tôi hiển thị một số ví dụ cụ thể trong SciBench, cũng như đầu ra được tạo ra bởi SciGLM (32B) và ChatGLM3-32B-Base trong Hình 14, Hình 15, Hình 16, Hình 17, và Hình 18.

Vấn đề Khi một electron trong một mức năng lượng kích thích nhất định trong một hộp một chiều dài 2,00 \u00c5 chuyển đổi xuống trạng thái cơ bản, một photon có bước sóng 8,79 nm được phát ra. Tìm số lượng tử của trạng thái ban đầu.

Câu trả lời Đúng: 4

Giải pháp Dự đoán bởi ChatGLM3-32B-Base (Phục vụ như đường cơ sở)
Trạng thái cơ bản có n= 1. Năng lượng trong trạng thái n= 1 là 1 L2. Đối với trạng thái n= 2, điều này trở thành 1 2L2.
Do đó, chênh lệch năng lượng này là 1 222−1 12= 9 4.
Bằng phương trình de Broglie, hc/λ = 9 4hcν, vì vậy ν= 4hc 9λ. Do đó, E=hν=hc4 9λ.
Vì vậy, n= 2, do đó câu trả lời là 2.

Giải pháp Dự đoán bởi SciGLM (Tinh chỉnh từ ChatGLM3-32B-Base)
Khái niệm: Khái niệm của vấn đề này dựa trên hiệu ứng quang điện. Hiệu ứng quang điện là sự phát xạ electron khi ánh sáng chiếu vào vật liệu. Hiệu ứng quang điện xảy ra khi các photon có năng lượng lớn hơn năng lượng liên kết của electron gây ra việc tách electron khỏi vật liệu. Khi một photon được phát ra bởi một electron, năng lượng của photon bằng năng lượng của electron ở trạng thái kích thích trừ năng lượng ở trạng thái cơ bản. Năng lượng của một photon được cho bởi phương trình E = hc/ λ, trong đó h là hằng số Planck (6.626×10−34Js), c là tốc độ ánh sáng (3.000×108m/s), và λ là bước sóng của photon tính bằng mét. Năng lượng của một electron trong hộp một chiều được cho bởi phương trình En= (n2×h2/8l2), trong đó n là số lượng tử của trạng thái và l là chiều dài của hộp tính bằng mét. Vấn đề yêu cầu số lượng tử của trạng thái ban đầu. Điều này có thể được tìm thấy bằng cách đặt năng lượng của trạng thái ban đầu và cuối cùng bằng năng lượng của photon.

Bước 1: Chuyển đổi chiều dài của hộp và bước sóng của photon sang mét.
- Chiều dài của hộp được cho là 2,00 Å. Chúng ta cần chuyển đổi điều này sang mét bằng cách nhân với 10−10 (vì 1 Å = 10−10m).
- Bước sóng của photon được cho là 8,79 nm. Chúng ta cần chuyển đổi điều này sang mét bằng cách nhân với 10−9 (vì 1 nm = 10−9m).

Bước 2: Tính năng lượng của photon.
- Sử dụng phương trình E = hc/ λ, trong đó h là hằng số Planck (6.626×10−34Js) và c là tốc độ ánh sáng (3.00×108m/s).

Bước 3: Đặt năng lượng của trạng thái cuối cùng (n = 1) bằng năng lượng của photon và giải n.
- Sử dụng phương trình En= (n2∗h2/8l2), trong đó l là chiều dài của hộp tính bằng mét.

Bước 4: Chuyển đổi câu trả lời sang ký hiệu khoa học và làm tròn đến 3 chữ số thập phân.
Tóm lại, câu trả lời cho câu hỏi do đó là n= 4.000. ✓

Hình 14: Một ví dụ vấn đề Hóa học Lượng tử trong SciBench đã được giải chính xác với SciGLM (32B). Nó cho thấy rằng sau tinh chỉnh hướng dẫn, SciGLM học các kỹ năng/hành vi để đầu tiên phân tích kiến thức cần thiết cho mỗi vấn đề, và sau đó giải quyết vấn đề từng bước với công thức và tính toán đúng.

A.11 Ví dụ của SciInstruct
Chúng tôi trình bày các ví dụ SciInstruct cho Toán (Hình 19), Vật lý (Hình 20), Hóa học (Hình 21), và Chứng minh Hình thức (Hình 22).

--- TRANG 22 ---
Vấn đề Mở rộng Ví dụ 1.4-6 cho một xúc xắc n mặt. Tức là, giả sử một xúc xắc n mặt công bằng được tung n lần độc lập. Một sự trùng khớp xảy ra nếu mặt i được quan sát trong lần thử thứ i, i= 1,2, . . . , n. Tìm giới hạn của xác suất này khi n tăng không giới hạn.

Câu trả lời Đúng: 1−1/e= 0.6321. . .

Giải pháp Dự đoán bởi ChatGLM3-32B-Base
Chúng ta cần giới hạn của P(A) khi n tiến đến vô cùng. P(A) bằng 1 nn vì có n khả năng cho mỗi lần tung n. Do đó chúng ta muốn tìm limn→∞1 nn= lim x→∞1 xx= 0.
Kết luận: Câu trả lời do đó là 0.

Giải pháp Dự đoán bởi SciGLM (Tinh chỉnh từ ChatGLM3-32B-Base)
Xác suất tất cả i mặt của xúc xắc được tung i lần liên tiếp là 1 nn= 1 nn.
Bước 1: Nếu chúng ta viết tổng của n số hạng của dãy hình học này từ i= 1 đến n, nó bằng 1 vì mỗi số hạng trong tổng là 1 n và có tổng cộng n số hạng được cộng. Do đó, nếu chúng ta muốn tìm xác suất không có sự trùng khớp nào xảy ra, chúng ta cộng tất cả các xác suất không có sự trùng khớp xảy ra trên k= 1 đến n. Điều này trở thành nP k=1 1 kn= 1 1n+ 1 2n+ 1 3n+···+ 1 nn
Khi n→ ∞, tổng tiến đến một số hạng giới hạn e−1. Vì tổng xác suất phải bằng 1 cho tất cả n, và chúng ta vừa thiết lập rằng giới hạn của tổng tất cả các xác suất không có sự trùng khớp xảy ra là e−1, đây phải là xác suất không có sự trùng khớp nào xảy ra. Xác suất ít nhất một sự trùng khớp xảy ra là 1−e−1.
Bước 2:
Do đó, khi n→ ∞, giới hạn của xác suất không có sự trùng khớp xảy ra là e−1= 0.367, và giới hạn của xác suất ít nhất một sự trùng khớp xảy ra là 1−e−1≈0.63.
Tóm lại, câu trả lời cho câu hỏi là 0.63.
Giới hạn của xác suất ít nhất một sự trùng khớp xảy ra là 0.63. ✓

Hình 15: Một ví dụ vấn đề thống kê trong SciBench được giải chính xác với SciGLM (32B).

Vấn đề World Series trong bóng chày tiếp tục cho đến khi đội American League hoặc đội National League thắng bốn trận. Có bao nhiêu thứ tự khác nhau có thể (ví dụ: ANNAAA có nghĩa là đội American League thắng trong sáu trận) nếu series diễn ra bốn trận?

Câu trả lời Đúng Latex: 2
Câu trả lời Đúng Số: 2

Câu trả lời được tạo ra bởi ChatGLM3-32B-Base
Câu trả lời do đó là 20.

Câu trả lời được tạo ra bởi SciGLM (32B)
Giải pháp:
Bước 1: Đội American league thắng trong 4 trận (4 4).
Đội National thắng trong 4 trận (4 4).
Bước 2: Tổng số cách
=(4 4)+(4 4)=2×1=2.
Kết luận: Câu trả lời do đó là 2. ✓

Hình 16: Một ví dụ vấn đề thống kê trong SciBench đã được giải chính xác với SciGLM (32B).

--- TRANG 23 ---
Vấn đề 7.3-9. Xem xét hai nhóm phụ nữ sau: Nhóm 1 bao gồm những phụ nữ chi ít hơn 500 hàng năm cho quần áo; Nhóm 2 bao gồm những phụ nữ chi hơn 1000 hàng năm cho quần áo. Cho p1 và p2 bằng tỷ lệ phụ nữ trong hai nhóm này, tương ứng, tin rằng quần áo quá đắt. Nếu 1009 trong số mẫu ngẫu nhiên 1230 phụ nữ từ nhóm 1 và 207 trong số mẫu ngẫu nhiên 340 từ nhóm 2 tin rằng quần áo quá đắt,
(a) Đưa ra ước tính điểm của p1−p2.

Câu trả lời Đúng Latex: $0.2115$
Câu trả lời Đúng Số: 0.2115

Câu trả lời được tạo ra bởi ChatGLM3-32B-Base
Để tìm ước tính điểm của p1p2, đó là xác suất kết hợp rằng một phụ nữ từ nhóm 1 và một phụ nữ từ nhóm 2, tin rằng quần áo quá đắt, chúng ta sẽ sử dụng số lượng phụ nữ trong mỗi nhóm tin rằng quần áo quá đắt.
Từ nhóm 1, chúng ta có 10.0907 phụ nữ tin rằng quần áo quá đắt, Và từ nhóm 2, chúng ta có 0.6076 phụ nữ tin rằng quần áo quá đắt.
Ước tính điểm khoảng (123)=0.000000.
Câu trả lời do đó là 0.000.

Câu trả lời được tạo ra bởi SciGLM (32B)
Giải pháp:
n1= 1230, p1=1009 n1≈0.819,
n2= 340, p2≈207 340= 0.609
p1−p2≈0.819−0.609 = 0.210
Câu trả lời do đó là 0.210. ✓

Hình 17: Một ví dụ vấn đề thống kê trong SciBench đã được giải chính xác với SciGLM (32B).

Vấn đề Tính độ bất định ∆Lz cho trạng thái ổn định của nguyên tử hydro: 2pz.

Câu trả lời Đúng Latex: 0
Câu trả lời Đúng Số: 0

Câu trả lời được tạo ra bởi ChatGLM3-32B-Base
Vì động lượng góc của một electron quay quanh hạt nhân là: Lz=µBℏ. Và bán kính quay của electron quanh proton khoảng. Độ bất định là: ∆Lz=ℏ 2√ 2. Câu trả lời do đó là 7.605.

Câu trả lời được tạo ra bởi SciGLM (32B)
Giải pháp:
∆Lz=h 2πq ℏ mcℏcq n(n−1) 2
=q 1·(1−1) 2
=q 0 2= 0
Câu trả lời do đó là 0. ✓

Hình 18: Một ví dụ vấn đề Hóa học Lượng tử trong SciBench đã được giải chính xác với SciGLM (32B).

--- TRANG 24 ---
Vấn đề
Sử dụng phương pháp hệ số chưa xác định để trả lời câu hỏi dưới đây.
y′′−2y′+ 5y= sin x, y(0) = 1, y′(0) = 1

Câu trả lời
Phân tích: Điểm chính của vấn đề này là hệ số chưa xác định và phương trình vi phân của biến thiên tham số.
Bài kiểm tra này sử dụng Phương pháp hệ số chưa xác định để giải các vấn đề phương trình vi phân.

Bước 1: phương trình phụ trợ là r2−2r+ 5 = 0 với nghiệm r= 1±2i, vì vậy giải pháp bổ sung là:
ιe(x) =ex(c1cos 2x+c2sin 2x).
Thử giải pháp đặc biệt:
yp(x) =Acosx+Bsinx,
y′ p=−Asinx+Bcosx

Bước 2: thay thế, chúng ta có:
(−Acosx−Bsinx)−2(−Asinx+Bcosx) + 5(Acosx+B(sinx) = sin x
⇒(4A−2B) cosx+ (2A+ 4B) sinx= sin x.
Sau đó:
4A−2B= 0,2A+ 4B= 1 ⇒A=1 10,B=1 3 và giải pháp tổng quát là:
y(x) =y1(x) +yp(x) =e′(c1cos 2x+c22x) +1 10cosx+1 3sinx
Nhưng,1 =y(0) = c1+1 10⇒c1=3 10
và1 =y′(0) = 2 c2+c1+1 5⇒c2=−1 26

Bước 3: Do đó giải pháp cho vấn đề giá trị ban đầu là:
y(x) =ex9 10cos 2x−1 20sin 2x
+1 10cosx+1 5sinx.
Tóm lại, câu trả lời là:
y(x) =ex9 10cos 2x−1 20sin 2x
+1 10cosx+1 5sinx.

Hình 19: Một ví dụ hướng dẫn về môn toán.

--- TRANG 25 ---
Vấn đề
Hai bóng đèn, một được định mức 30 W ở 120 V và một khác được định mức 40 W ở 120 V, được sắp xếp trong hai mạch điện khác nhau.
a. Hai bóng đèn đầu tiên được nối song song với nguồn 120 V.
i. Xác định điện trở của bóng đèn định mức 30 W và dòng điện trong nó khi được nối trong mạch này.
ii. Xác định điện trở của bóng đèn định mức 40 W và dòng điện trong nó khi được nối với mạch này.
b. Các bóng đèn bây giờ được nối nối tiếp với nhau và có nguồn 120 V.
i. Xác định điện trở của bóng đèn định mức 30 W và dòng điện trong nó khi được nối trong mạch này.
ii. Xác định điện trở của bóng đèn định mức 40 W và dòng điện trong nó khi được nối trong mạch này.
c. Trong các không gian bên dưới, đánh số các bóng đèn trong mỗi tình huống được mô tả, theo thứ tự độ sáng của chúng.
(1= sáng nhất, 4 = tối nhất)
____bóng đèn 30 W trong mạch song song
____bóng đèn 40 W trong mạch song song
____bóng đèn 30 W trong mạch nối tiếp
____bóng đèn 40 W trong mạch nối tiếp
d. Tính tổng công suất tiêu tán bởi hai bóng đèn trong mỗi trường hợp sau.
i. Mạch song song
ii. Mạch nối tiếp

Câu trả lời
Phân tích: Sử dụng công thức để tính điện trở, dòng điện và tổng công suất.
(a) i: Tính điện trở và dòng điện của bóng đèn.
P = V2/R cho R=480 Ω và
V=IR cho I=0.25 A
ii: Tính điện trở và dòng điện của bóng đèn.
P = V2/R cho R=360 Ω và
V=IR cho I=0.33 A
(b) i./ii. Tính điện trở và dòng điện của bóng đèn.
Các điện trở không thay đổi 480 Ω và 360 Ω. Tổng điện trở trong nối tiếp là 480 Ω + 360 Ω= 840 Ω làm cho tổng dòng điện
I = V /R = 0.14 A đó là giá trị giống nhau cho cả hai điện trở trong nối tiếp.
(c) So sánh độ sáng của bóng đèn.
Các bóng đèn sáng nhất ở song song, nơi chúng cung cấp các giá trị được ghi nhãn của chúng là 40 W và 30 W. Trong nối tiếp, đó là điện trở lớn hơn (bóng đèn 30 W) sáng hơn với điện thế lớn hơn qua nó trong nối tiếp. Điều này cho thứ tự từ trên xuống dưới là 2 1 3 4.
(d) i: Tính tổng công suất tiêu thụ bởi hai bóng đèn.
Trong song song, chúng mỗi cái hoạt động ở điện thế định mức của chúng vì vậy chúng mỗi cái cung cấp công suất định mức của chúng và PT= 30W + 40W = 70W
ii: Tính tổng công suất tiêu thụ bởi hai bóng đèn
Trong nối tiếp PT=VT2/RT= 17W
Tóm lại,
(a) i:P = 480 Ω và V = 0.25A
ii:P = 360 Ω và V = 0.33A
(b) i/ii: P = 840 Ω và V = 0.14A
(c) Điều này cho thứ tự từ trên xuống dưới là 2 1 3 4.
(d) i:PT= 70W
ii:PT= 17W

Hình 20: Một ví dụ hướng dẫn về môn vật lý.

--- TRANG 26 ---
Vấn đề
Xem xét một hỗn hợp của hai chất rắn, BaCl 2+ 2H 2O(FM 244.26) và KCl (FM 74.551), theo tỷ lệ không rõ. (Ký hiệu BaCl 2·2H2O có nghĩa là một tinh thể được hình thành với hai phân tử nước cho mỗi BaCl 2.) Khi chất không rõ được đun nóng đến 160◦C trong 1 h, nước kết tinh bị đuổi ra:
BaCl 2·2H2O(s)160◦C−→ BaCl 2(s) + 2H 2O(g)
Một mẫu ban đầu nặng 1.7839 g nặng 1.5623 g sau khi đun nóng. Tính phần trăm trọng lượng của Ba, K, và Cl trong mẫu gốc.

Câu trả lời
Phân tích: Nội dung của câu hỏi này là tính phần trăm trọng lượng.
Bước 1: Công thức và khối lượng nguyên tử: Ba(137.327), Cl(35.453), K(39.098), H2O(18.015), KCl(74.551), BaCl 2· 2H2O(244.26), H2O mất= 1.7839−1.5623 = 0.2216 g = 1.2301×10−2mol của H2O. Với 2 mol H 2O mất, 1 mol BaCl 2·2H2O phải có mặt.1 2 1.2301×10−2mol H 2O mất) = 6.1504×10−3mol BaCl 2·2H2O = 1.5024 g.
Nội dung Ba và Cl của BaCl 2·2H2O là
Ba =137.33 244.26 (1.5024 g) = 0.84469 g
Cl = 2(35.453) 244.26 (1.5024 g) = 0.43613 g
Bước 2: Vì tổng mẫu nặng 1.783 g và chứa 1.5024 g của BaCl 2·2H2O, mẫu phải chứa 1.7839−1.5024 = 0.2815 g của KCl, chứa
K =39.098 74.551 (0.2815) = 0.14763 g
Cl =35.453 74.551 (0.2815) = 0.13387 g
Phần trăm trọng lượng của mỗi nguyên tố:
Ba =0.84469 1.7839= 47.35%
K =0.14763 1.7839= 8.28%
Cl =0.43613+0.13387 1.7839= 31.95%
Tóm lại, phần trăm trọng lượng của Ba là 47.35%, phần trăm trọng lượng của K là 8.28%, phần trăm trọng lượng của Cl là 31.95%.

Hình 21: Một ví dụ hướng dẫn về môn hóa học.

Vấn đề
lemma unfold_sub α[subtraction_monoid α] (a b c : α) (h : a + -b = c) : a - b = c :=.
Câu trả lời
by rw [sub_eq_add_neg, h]

Vấn đề lemma subtype.exists_pi_extension ι: Sort* α:ι→Sort* [ne : Πi, nonempty (αi)]
p :ι→Prop (f :Πi : subtype p, αi) :
g :Πi :ι,αi, (λi : subtype p, g i) = f :=
Câu trả lời
begin
tactic.classical,
refine〈λi, if hi : p i then f 〈i, hi〉else classical.choice (ne i), funext _ 〉,
rintro〈i, hi〉,
exact dif_pos hi
end

Hình 22: Ví dụ về chứng minh hình thức.

--- TRANG 27 ---
A.12 Tập dữ liệu Suy luận Khoa học
Tập dữ liệu Toán học. Trong quá khứ, trọng tâm trong các nhiệm vụ suy luận toán học chủ yếu là các tập dữ liệu suy luận số học cơ bản [44;45;46], giải quyết các vấn đề toán học cơ bản như AddSub [47]. Sau đó, để giải quyết các vấn đề từ toán thực tế, một số tập dữ liệu khó hơn được đề xuất [48;49;30;50]. Để xây dựng một tập dữ liệu khó hơn và đa dạng hơn, NumGLUE [51] và LiLA [52] đã được giới thiệu để nâng cao nghiên cứu hiện tại. Tuy nhiên, trọng tâm của chúng vẫn chủ yếu là các vấn đề toán tiểu học, trung học cơ sở và trung học phổ thông. Với sự tiến bộ nhanh chóng của LLMs, để đánh giá khả năng và hạn chế của chúng trong việc giải quyết các vấn đề toán phức tạp hơn, MMLU [29] với các vấn đề toán cấp đại học đã được đề xuất. Để giải quyết các vấn đề toán cấp đại học thách thức hơn, PoT [53;7] được đề xuất. SciInstruct được đề xuất của chúng tôi bao gồm các vấn đề toán phức tạp và đa dạng hơn ở cấp độ đại học.

Tập dữ liệu Khoa học. Để giải quyết khả năng khoa học của LLMs, SciBench [7] đề xuất các giải pháp khác nhau như gọi các công cụ hiện có (python và Wolfram). SciEval [6] trình bày một điểm chuẩn đánh giá đa cấp cho nghiên cứu khoa học. GPQA [8] xây dựng một điểm chuẩn hỏi đáp cấp độ sau đại học để đánh giá các câu hỏi vật lý và hóa học khó hơn.

A.13 Suy luận Tổng quát với LLMs
Được hỗ trợ bởi prompting Chuỗi Tư duy [54;55;56] và Tree-or-Thought [57], LLMs đã mang lại những cải thiện hiệu suất suy luận tốt. Đặc biệt, trên các nhiệm vụ BIG-Bench thách thức, kỹ thuật CoT đã vượt trội khả năng của con người [58]. Sau đó, để giải quyết các nhiệm vụ suy luận, một số nghiên cứu [59;60;61;62;63;11;64;65;66] đề xuất các phương pháp khác nhau để tận dụng LLMs và giữ lại các quá trình từng bước. Với sự phát triển nhanh chóng của LLM Agents, một số công trình [67;17] đề xuất sử dụng các công cụ bên ngoài để nâng cao khả năng suy luận của LLMs. Ví dụ, ReAct [67] cố gắng gọi các công cụ hiện có như công cụ tìm kiếm web để cải thiện kỹ năng suy luận của LLMs. Thật vậy, tận dụng các chương trình như quá trình tư duy là một cách tự nhiên. Một số nghiên cứu gần đây đã sử dụng các chương trình như quá trình tư duy, chẳng hạn như PoT [68], để cải thiện khả năng suy luận của LLMs. Để nâng cao hiệu suất suy luận của LLMs trong việc giải quyết các vấn đề toán học hoặc khoa học với PoT, một số phương pháp đã được đề xuất, bao gồm Self-critic [69], Self-eval [70], và Plan-and-solve [71]. Ví dụ, self-critic [69] và self-eval [70] đề xuất áp dụng tự đánh giá để cải thiện tính mạnh mẽ của chương trình được tạo ra. Khác với hai phương pháp này, plan-and-solve [71] áp dụng các hướng dẫn lập kế hoạch chi tiết hơn để tạo ra một kế hoạch suy luận cấp cao cho LLMs. Thật vậy, những phương pháp này đã chứng minh rằng chúng có thể có được khả năng tuyệt vời so với PoT.

A.14 Tinh chỉnh Hướng dẫn trong LLMs
Để căn chỉnh các mô hình ngôn ngữ với sở thích của con người và các mục tiêu hiệu quả, một số công trình [17;72;66] thiết kế tinh chỉnh hướng dẫn. Tinh chỉnh hướng dẫn nhằm khai thác các khả năng tiềm ẩn bằng cách căn chỉnh với và phản hồi sở thích của con người. Ban đầu, tinh chỉnh hướng dẫn tập trung vào việc cải thiện khả năng tuân theo hướng dẫn của LLMs cho mục đích chung. Được đại diện bởi FLAN [73] và T0 [74], chúng nhằm hiểu khả năng tổng quát hóa của LLMs cho tinh chỉnh hướng dẫn. Sau đó, để hiểu hiệu quả và hiệu suất của việc mở rộng các tập dữ liệu tinh chỉnh hướng dẫn trên các mô hình, FLAN-v2 [75;76] đã được đề xuất để xác thực mục tiêu này. Tuy nhiên, những phương pháp này xây dựng các tập dữ liệu tinh chỉnh hướng dẫn huấn luyện theo cách chú thích thủ công của con người. Gần đây, các nghiên cứu khác nhau [77;72;78;17; 39;79] đã bắt đầu xây dựng các tập dữ liệu tuân theo hướng dẫn tổng hợp được chưng cất từ một số LLMs như GPT-3.5-turbo và GPT-4. Như những công trình này, Platypus [80] xây dựng một tập dữ liệu hướng dẫn quy mô nhỏ để tuân theo bằng cách sử dụng một tập dữ liệu chuyên môn lĩnh vực, nhằm nâng cao khả năng suy luận của LLMs.

--- TRANG 28 ---
Phần II
Tài liệu Bổ sung

B Bảng dữ liệu
B.1 Động lực
1. Tập dữ liệu được tạo ra cho mục đích gì? Tập dữ liệu điểm chuẩn của chúng tôi được tạo ra để giải quyết thách thức khan hiếm dữ liệu trong lĩnh vực khoa học và cung cấp một bộ hướng dẫn khoa học để huấn luyện các mô hình ngôn ngữ khoa học có khả năng suy luận khoa học cấp độ đại học.
2. Ai đã tạo ra tập dữ liệu và thay mặt cho thực thể nào? Tập dữ liệu được phát triển bởi các nhà nghiên cứu LLM (sinh viên đại học, nghiên cứu sinh tiến sĩ và nghiên cứu sinh sau tiến sĩ) được liệt kê trong danh sách tác giả.
3. Ai đã tài trợ cho việc tạo ra tập dữ liệu? Công trình này được hỗ trợ bởi NSFC 62276148, NSFC cho Học giả Trẻ Xuất sắc 62425601, quỹ nghiên cứu từ Zhipu, New Cornerstone Science Foundation thông qua XPLORER PRIZE và Đại học Thanh Hoa (Khoa Khoa học và Công nghệ Máy tính) - Siemens Ltd., Trung tâm Nghiên cứu Chung Trung Quốc về Trí tuệ Công nghiệp và Internet of Things (JCIIOT).

B.2 Phân phối
1. Tập dữ liệu có được phân phối cho các bên thứ ba bên ngoài thực thể (ví dụ: công ty, tổ chức, tổ chức) thay mặt cho việc tạo ra tập dữ liệu không? Có, tập dữ liệu mở cho công chúng.
2. Tập dữ liệu sẽ được phân phối như thế nào (ví dụ: tarball trên trang web, API, GitHub)? Tập dữ liệu đã được phân phối thông qua Hugging Face, Google Drive, Tsinghua Cloud, và mã được sử dụng để phát triển các mô hình đường cơ sở thông qua GitHub.
3. Có bên thứ ba nào áp đặt các hạn chế dựa trên IP hoặc hạn chế khác trên dữ liệu liên quan đến các thể hiện không? Không.
4. Có kiểm soát xuất khẩu hoặc hạn chế quy định khác áp dụng cho tập dữ liệu hoặc các thể hiện cá nhân không? Không.
5. Tập dữ liệu sẽ được phân phối khi nào? Nó đã được phát hành ngay bây giờ.
6. Tập dữ liệu có được phân phối dưới bản quyền hoặc giấy phép sở hữu trí tuệ (IP) khác, và/hoặc dưới các điều khoản sử dụng (ToU) áp dụng không? Tập dữ liệu sẽ được phân phối dưới giấy phép CC BY 4.0.

B.3 Bảo trì
1. Ai sẽ hỗ trợ/lưu trữ/bảo trì tập dữ liệu? Zhipu AI và THUDM sẽ hỗ trợ, lưu trữ và bảo trì tập dữ liệu.
2. Làm thế nào chủ sở hữu/người quản lý/người quản lý tập dữ liệu có thể được liên hệ (ví dụ: địa chỉ email)? Chủ sở hữu/người quản lý/người quản lý tập dữ liệu có thể được liên hệ thông qua các email sau: Dan Zhang (zd18@tsinghua.org.cn) và Sining Zhoubian (zbsn21@mails.tsinghua.edu.cn).
3. Có errata không? Không. Nếu lỗi được tìm thấy trong tương lai, chúng tôi sẽ phát hành errata trên trang web chính cho tập dữ liệu (https://github.com/THUDM/SciGLM và https://huggingface.co/ datasets/zd21/SciInstruct).
4. Tập dữ liệu có được cập nhật không (ví dụ: để sửa lỗi gán nhãn, thêm thể hiện mới, xóa thể hiện)? Có, các tập dữ liệu sẽ được cập nhật bất cứ khi nào cần thiết để đảm bảo độ chính xác, và các thông báo sẽ được thực hiện tương ứng. Những cập nhật này sẽ được đăng trên trang web chính cho tập dữ liệu (https://github.com/THUDM/SciGLM và https://huggingface.co/datasets/zd21/ SciInstruct).

--- TRANG 29 ---
5. Nếu tập dữ liệu liên quan đến con người, có giới hạn áp dụng nào về việc lưu giữ dữ liệu liên quan đến các thể hiện không (ví dụ: các cá nhân được đề cập có được nói rằng dữ liệu của họ sẽ được lưu giữ trong một khoảng thời gian cố định và sau đó bị xóa không?) N/A
6. Các phiên bản cũ hơn của tập dữ liệu có tiếp tục được hỗ trợ/lưu trữ/bảo trì không? Có, các phiên bản cũ hơn của tập dữ liệu sẽ tiếp tục được bảo trì và lưu trữ.
7. Nếu những người khác muốn mở rộng/tăng cường/xây dựng trên/đóng góp cho tập dữ liệu, có cơ chế nào để họ làm như vậy không? Nếu những người khác muốn mở rộng/tăng cường/xây dựng trên/đóng góp cho tập dữ liệu, cách hiệu quả nhất để liên hệ với chúng tôi là thông qua các pull request GitHub. Để biết thêm câu hỏi, đừng ngần ngại liên hệ với Dan Zhang (zd18@tsinghua.org.cn), người sẽ chịu trách nhiệm bảo trì.

B.4 Thành phần
1. Các thể hiện bao gồm tập dữ liệu đại diện cho gì (ví dụ: tài liệu, ảnh, con người, quốc gia?) Mỗi thể hiện bao gồm một câu hỏi, câu trả lời tương ứng và môn học. Những thuộc tính này được sử dụng để tinh chỉnh LLMs có khả năng suy luận khoa học cấp độ đại học.
2. Có bao nhiêu thể hiện tổng cộng (của mỗi loại, nếu thích hợp)? Tập dữ liệu SciInstruct được xây dựng bao gồm 254.051 hướng dẫn, bao gồm 123.869 vật lý và hóa học, 89.934 toán, và 40.248 chứng minh hình thức (Lean).
3. Tập dữ liệu có chứa tất cả các thể hiện có thể hoặc là một mẫu của các thể hiện từ một tập lớn hơn không? Các tập dữ liệu chứa tất cả các thể hiện có thể.
4. Có nhãn hoặc mục tiêu liên quan đến mỗi thể hiện không? Có, mỗi thể hiện bao gồm môn học tương ứng.
5. Có thông tin nào bị thiếu từ các thể hiện cá nhân không? Không.
6. Có phân chia dữ liệu được khuyến nghị không (ví dụ: huấn luyện, phát triển/xác thực, kiểm tra)? Chúng tôi sử dụng tất cả các thể hiện làm tập huấn luyện vì có nhiều tập dữ liệu điểm chuẩn công cộng làm tập kiểm tra bao gồm một phạm vi rộng các môn học.
7. Có lỗi, nguồn nhiễu hoặc dư thừa nào trong tập dữ liệu không? Không.
8. Tập dữ liệu có tự chứa, hoặc nó liên kết đến hoặc dựa vào các tài nguyên bên ngoài khác không (ví dụ: trang web, tweet, tập dữ liệu khác)? Tập dữ liệu tự chứa.
9. Tập dữ liệu có chứa dữ liệu có thể được coi là bí mật không? Không.
10. Tập dữ liệu có chứa dữ liệu mà, nếu được xem trực tiếp, có thể gây xúc phạm, lăng mạ, đe dọa, hoặc có thể gây lo lắng khác không? Không.

B.5 Quá trình Thu thập
1. Dữ liệu liên quan đến mỗi thể hiện được thu thập như thế nào? Dữ liệu liên quan đến mỗi thể hiện được thu thập từ nhiều nguồn khác nhau, bao gồm sách giáo khoa, tài liệu giáo dục và bộ đề. Tài liệu tham khảo cho những nguồn này được cung cấp trong Bảng 7 trong Phụ lục 7.2.
2. Những cơ chế hoặc quy trình nào được sử dụng để thu thập dữ liệu (ví dụ: thiết bị phần cứng hoặc cảm biến, tuyển chọn thủ công của con người, chương trình phần mềm, API phần mềm)? Chúng tôi đầu tiên sử dụng Nhận dạng ký tự quang học (OCR) để chuyển đổi câu hỏi và câu trả lời cuối cùng trong các tài liệu được thu thập, sau đó sử dụng API LLM mạnh mẽ để tạo ra suy luận trung gian, và cuối cùng lọc ra các dấu vết với câu trả lời dự đoán chính xác.
3. Ai đã tham gia vào quá trình thu thập dữ liệu (ví dụ: sinh viên, nhân viên crowdwork, nhà thầu) và họ được bồi thường như thế nào (ví dụ: nhân viên crowdwork được trả bao nhiêu)? Các nhà nghiên cứu LLM thường xuyên (ví dụ: sinh viên đại học, nghiên cứu sinh tiến sĩ và nghiên cứu sinh sau tiến sĩ được liệt kê trong danh sách tác giả) tại Tsinghua và Caltech đã tham gia vào quá trình thu thập dữ liệu.
4. Tập dữ liệu có liên quan đến con người không? Không.
5. Bạn có thu thập dữ liệu trực tiếp từ các cá nhân được đề cập, hoặc có được nó thông qua bên thứ ba hoặc các nguồn khác không (ví dụ: trang web)? Chúng tôi có được tập dữ liệu từ sách giáo khoa, tài liệu giáo dục và bộ đề. Tài liệu tham khảo cho những nguồn này được cung cấp trong Bảng 7 trong Phụ lục 7.2.

--- TRANG 30 ---
B.6 Sử dụng
1. Tập dữ liệu đã được sử dụng cho bất kỳ nhiệm vụ nào chưa? Có, tập dữ liệu này đã được sử dụng để tạo ra các tập dữ liệu mới, ví dụ: để chuyển đổi các phép tính định dạng văn bản thành định dạng mã và để tinh chỉnh các mô hình ngôn ngữ dựa trên mã.
2. Tập dữ liệu có thể được sử dụng cho những nhiệm vụ (khác) nào? Không.
3. Có điều gì về thành phần của tập dữ liệu hoặc cách nó được thu thập và tiền xử lý/làm sạch/gán nhãn có thể ảnh hưởng đến việc sử dụng trong tương lai không? Thành phần hiện tại của các tập dữ liệu tự đủ để huấn luyện một mô hình ngôn ngữ khoa học. Bất kỳ thay đổi nào trong bản phát hành tiếp theo và cập nhật sẽ được ghi lại và chia sẻ thông qua trang web tập dữ liệu (https://github.com/THUDM/ SciGLM và https://huggingface.co/datasets/zd21/SciInstruct).
4. Có nhiệm vụ nào mà tập dữ liệu không nên được sử dụng không? Không.

--- TRANG 31 ---
Danh sách kiểm tra
1. Đối với tất cả tác giả...
(a) Các tuyên bố chính được đưa ra trong tóm tắt và giới thiệu có phản ánh chính xác đóng góp và phạm vi của bài báo không? [Có] Xem Phần 1.
(b) Bạn có mô tả những hạn chế của công việc mình không? [Có] Xem Phần 5.
(c) Bạn có thảo luận về bất kỳ tác động xã hội tiêu cực tiềm ẩn nào của công việc mình không? [Có] Xem Phần 5.
(d) Bạn có đọc hướng dẫn đánh giá đạo đức và đảm bảo rằng bài báo của bạn tuân theo chúng không? [Có] Chúng tôi đã đọc hướng dẫn đánh giá đạo đức và đảm bảo rằng bài báo của bạn tuân theo chúng.

2. Nếu bạn bao gồm kết quả lý thuyết...
(a) Bạn có nêu tập hợp đầy đủ các giả định của tất cả kết quả lý thuyết không? [N/A]
(b) Bạn có bao gồm chứng minh hoàn chỉnh của tất cả kết quả lý thuyết không? [N/A]

3. Nếu bạn chạy thí nghiệm (ví dụ: cho điểm chuẩn)...
(a) Bạn có bao gồm mã, dữ liệu và hướng dẫn cần thiết để tái tạo các kết quả thí nghiệm chính không (trong tài liệu bổ sung hoặc dưới dạng URL)? [Có] Chúng tôi đã cung cấp URL.
(b) Bạn có chỉ định tất cả chi tiết huấn luyện không (ví dụ: phân chia dữ liệu, siêu tham số, cách chúng được chọn)? [Có] Xem Phần 2.4.
(c) Bạn có báo cáo thanh lỗi không (ví dụ: đối với hạt giống ngẫu nhiên sau khi chạy thí nghiệm nhiều lần)? [N/A]
(d) Bạn có bao gồm tổng lượng tính toán và loại tài nguyên được sử dụng không (ví dụ: loại GPU, cụm nội bộ hoặc nhà cung cấp đám mây)? [Có] Xem Phần 2.4.

4. Nếu bạn đang sử dụng tài sản hiện có (ví dụ: mã, dữ liệu, mô hình) hoặc tuyển chọn/phát hành tài sản mới...
(a) Nếu công việc của bạn sử dụng tài sản hiện có, bạn có trích dẫn người tạo ra không? [Có] Xem Phần 2.4.
(b) Bạn có đề cập đến giấy phép của tài sản không? [Có] Xem Phần 2.4.
(c) Bạn có bao gồm bất kỳ tài sản mới nào trong tài liệu bổ sung hoặc dưới dạng URL không? [Có] Chúng tôi đã cung cấp URL.
(d) Bạn có thảo luận về việc liệu và làm thế nào sự đồng ý được thu thập từ những người có dữ liệu bạn đang sử dụng/tuyển chọn không? [N/A]
(e) Bạn có thảo luận về việc liệu dữ liệu bạn đang sử dụng/tuyển chọn có chứa thông tin nhận dạng cá nhân hoặc nội dung xúc phạm không? [N/A]

5. Nếu bạn sử dụng crowdsourcing hoặc tiến hành nghiên cứu với đối tượng con người...
(a) Bạn có bao gồm toàn bộ văn bản hướng dẫn được đưa cho người tham gia và ảnh chụp màn hình, nếu có không? [N/A]
(b) Bạn có mô tả bất kỳ rủi ro tiềm ẩn nào cho người tham gia, với liên kết đến phê duyệt Hội đồng Đánh giá Tổ chức (IRB), nếu có không? [N/A]
(c) Bạn có bao gồm mức lương theo giờ ước tính được trả cho người tham gia và tổng số tiền chi cho bồi thường người tham gia không? [N/A]
