# Auto-Instruct: Tự động Tạo và Xếp hạng Hướng dẫn
cho Các Mô hình Ngôn ngữ Hộp đen

Zhihan Zhang♠∗, Shuohang Wang♢, Wenhao Yu♠, Yichong Xu♢, Dan Iter♢,
Qingkai Zeng♠, Yang Liu♢, Chenguang Zhu♢, Meng Jiang♠
♠University of Notre Dame
♢Microsoft Azure AI
zzhang23@nd.edu

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) có thể thực hiện một
phạm vi rộng các nhiệm vụ bằng cách tuân theo các hướng dẫn 
ngôn ngữ tự nhiên, mà không cần thiết phải tinh chỉnh cụ thể
theo nhiệm vụ. Không may, hiệu suất của LLM bị ảnh hưởng
lớn bởi chất lượng của các hướng dẫn này, và việc viết thủ công
các hướng dẫn hiệu quả cho mỗi nhiệm vụ là một quá trình
vất vả và chủ quan. Trong bài báo này, chúng tôi giới thiệu
Auto-Instruct, một phương pháp mới để tự động cải thiện
chất lượng hướng dẫn được cung cấp cho LLM. Phương pháp
của chúng tôi tận dụng khả năng sinh tự nhiên của LLM để
tạo ra các hướng dẫn ứng viên đa dạng cho một nhiệm vụ
nhất định, và sau đó xếp hạng chúng bằng một mô hình chấm
điểm được huấn luyện trên nhiều loại 575 nhiệm vụ NLP
hiện có. Trong các thí nghiệm trên 118 nhiệm vụ ngoài
miền, Auto-Instruct vượt trội cả hướng dẫn do con người viết
và các baseline hiện có của hướng dẫn do LLM tạo ra.
Hơn nữa, phương pháp của chúng tôi thể hiện khả năng
tổng quát đáng chú ý ngay cả với các LLM khác không được
tích hợp vào quá trình huấn luyện của nó.¹

## 1 Giới thiệu
Các mô hình ngôn ngữ lớn được điều chỉnh hướng dẫn (LLM)
đã trở nên phổ biến như các giải pháp cho vô số nhiệm vụ
NLP, nhờ vào sự thành thạo của chúng trong việc diễn giải
các hướng dẫn ngôn ngữ tự nhiên (Wei et al., 2021; Chung
et al., 2022; Ouyang et al., 2022; Taori et al., 2023). Khi
việc tinh chỉnh LLM thường trở nên không khả thi, các hướng
dẫn đóng vai trò ngày càng quan trọng trong việc khuyến
khích các LLM hộp đen như vậy. Đặc biệt trong thiết lập
few-shot thực sự² (Perez et al., 2021) nơi người dùng nhằm
giải quyết một nhiệm vụ mới chỉ với một mô tả nhiệm vụ
cơ bản và một vài dữ liệu

∗Công việc này được thực hiện khi Zhihan là thực tập sinh tại
Microsoft Azure AI.
¹Mô hình và mã nguồn có sẵn tại https://github.com/
ytyz1307zzh/Auto-Instruct .
²Một kịch bản nơi không có dữ liệu huấn luyện hoặc validation
bổ sung nào có sẵn cho việc điều chỉnh siêu tham số và lựa chọn
prompt, ngoài các ví dụ few-shot (Perez et al., 2021).

[Hình 1: Pipeline Auto-Instruct. Chúng tôi đầu tiên khuyến khích
LLM tạo ra một tập hợp đa dạng các hướng dẫn ứng viên với
các phong cách khác nhau, và sau đó huấn luyện một mô hình
để xếp hạng và chọn hướng dẫn hiệu quả nhất cho một ví dụ
nhất định. Cuối cùng, hướng dẫn được chọn được sử dụng để
khuyến khích LLM suy luận đầu ra cho ví dụ này.]

ví dụ trong tầm tay, một hướng dẫn được tạo tốt là
cần thiết để cho phép LLM nắm bắt ánh xạ đầu vào-
đầu ra cần thiết để hoàn thành nhiệm vụ.

Mặc dù tầm quan trọng của hướng dẫn, cách tiếp cận
phổ biến khi sử dụng LLM hộp đen trên một nhiệm vụ
mới vẫn là kỹ thuật prompt thủ công (White et al., 2023;
Mishra et al., 2023). Tuy nhiên, cách tiếp cận như vậy
không chỉ tốn thời gian mà còn có xu hướng tạo ra các
hướng dẫn không tối ưu. Trước bối cảnh này, các nỗ lực
đã được thực hiện để trao quyền cho LLM tự động tạo
hướng dẫn (Honovich et al., 2022; Zhou et al., 2022;
Singh et al., 2022). Các cách tiếp cận này cung cấp cho
LLM một số ít ví dụ và khuyến khích nó tạo một hướng
dẫn dựa trên các minh họa này. Trong khi các phương pháp
như vậy thể hiện khả năng của LLM trong việc tạo ra các
hướng dẫn mạch lạc (Honovich et al., 2022), chỉ tạo ra
một hướng dẫn duy nhất không thể đảm bảo hiệu suất
đáng tin cậy cho các ví dụ chưa thấy trong nhiệm vụ đã cho.
Như một giải pháp đơn giản, các tập validation đã được
sử dụng để đánh giá hiệu quả của một tập hợp các hướng
dẫn được lấy mẫu (Zhou et al., 2022; Singh et al., 2022),
nhưng điều này không khả thi đối với nhiều nhiệm vụ được
định nghĩa trong thiết lập few-shot thực sự (Suzgun et al.,
2022). Bên cạnh đó, các cách tiếp cận này chủ yếu được
thử nghiệm trên các nhiệm vụ đơn giản nơi các hướng dẫn
cơ bản đã đủ, chẳng hạn như các phép toán số học hoặc
phân loại cảm xúc. Các nhiệm vụ phức tạp hơn trong các
benchmark NLP (Wang et al., 2022), đòi hỏi kỹ thuật
hướng dẫn cẩn thận, vẫn phần lớn chưa được kiểm tra
cho một giải pháp tự động.

Để giải quyết các thách thức nêu trên, chúng tôi đề xuất
Auto-Instruct, một cách tiếp cận mới để tự động tạo và
xếp hạng hướng dẫn cho LLM hộp đen trên các nhiệm vụ
NLP khác nhau, trong thiết lập few-shot thực sự. Đối với
mỗi nhiệm vụ downstream, chúng tôi đầu tiên khuyến khích
LLM lấy mẫu một loạt các hướng dẫn ứng viên, dựa trên
một hướng dẫn seed cơ bản và các minh họa few-shot.
Chúng tôi thu thập một tập ứng viên đa dạng bằng cách
chỉ định phong cách mong đợi của mỗi hướng dẫn. Nhận
ra hiệu suất biến đổi của LLM trên các hướng dẫn khác
nhau, cùng với việc thiếu dữ liệu validation để lựa chọn
hướng dẫn trước, chúng tôi huấn luyện một mô hình chấm
điểm để xếp hạng và chọn hướng dẫn phù hợp nhất cho
mỗi ví dụ test downstream. Để đảm bảo khả năng tổng quát
cần thiết trong thiết lập few-shot, mô hình được huấn luyện
trên 575 nhiệm vụ NLP hiện có trước khi được triển khai
cho các nhiệm vụ test ngoài miền. Cuối cùng, hướng dẫn
được chọn được sử dụng để khuyến khích LLM cho suy luận
downstream.

Trong các thí nghiệm với text-davinci-003 của OpenAI,
Auto-Instruct mang lại hiệu suất đáng kể trên 118 nhiệm vụ
ngoài miền từ Super Natural Instructions (SuperNI; Wang
et al., 2022) và Big Bench Hard (BBH; Suzgun et al., 2022).
Thể hiện khả năng tổng quát mạnh mẽ trong các kịch bản
ngoài miền, Auto-Instruct vượt trội hướng dẫn seed do con
người viết, cách tiếp cận tạo hướng dẫn tiên tiến nhất
iPrompt (Singh et al., 2022), và các baseline khác nhau
của việc khuyến khích LLM cho lựa chọn hướng dẫn. Hơn
nữa, Auto-Instruct thể hiện hiệu suất ấn tượng trong thiết
lập zero-shot và trong tổng quát hóa sang các LLM khác
(tức là, ChatGPT và GPT-4). Nghiên cứu của chúng tôi nhấn
mạnh rằng việc tự động tạo và xếp hạng hướng dẫn là một
cách tiếp cận đầy hứa hẹn để tận dụng sức mạnh của LLM
hộp đen một cách hiệu quả.

## 2 Công trình Liên quan
Việc lựa chọn hướng dẫn đóng vai trò then chốt trong việc
sử dụng LLM hiệu quả. Vì vậy, một loạt các cách tiếp cận
đã được triển khai, với tối ưu hóa tham số và tạo dựa trên
LLM nổi bật như các phương pháp chính. Tối ưu hóa tham số
chủ yếu liên quan đến việc sử dụng các tham số để điều chỉnh
hướng dẫn (Shin et al., 2020; Shi et al., 2022; Deng et al.,
2022). Ví dụ, Shin et al. (2020) sử dụng tìm kiếm dựa trên
gradient trên một độ dài được xác định trước của các token
rời rạc như hướng dẫn. Shi et al. (2022) cải thiện thêm
cách tiếp cận này bằng cách bảo tồn khả năng đọc của các
token được lấy mẫu thông qua ràng buộc perplexity. Như
một cách tiếp cận linh hoạt hơn, Deng et al. (2022) tối ưu
hóa việc tạo hướng dẫn thông qua học tăng cường, với phần
thưởng được tính toán dựa trên đầu ra LLM. Tuy nhiên, các
chiến lược này yêu cầu truy cập vào tham số LLM hoặc một
tập huấn luyện để tối ưu hóa, khiến chúng ít áp dụng được
cho LLM hộp đen với chỉ một số lượng hạn chế các ví dụ có
sẵn. Hơn nữa, các hướng dẫn được tạo bởi các phương pháp
này thường thiếu tính trôi chảy hoặc thậm chí trở thành
vô nghĩa, do đó làm tổn hại khả năng diễn giải của chúng.

Ngược lại, luồng tạo dựa trên LLM chọn hướng dẫn bằng
cách trực tiếp khuyến khích LLM (Honovich et al., 2022;
Zhou et al., 2022; Singh et al., 2022). Ví dụ, Honovich
et al. (2022) là một trong những người đầu tiên tiết lộ
rằng LLM có thể viết một hướng dẫn cho một nhiệm vụ đã
cho sau khi quan sát chỉ một vài minh họa, và Zhou et al.
(2022) cải thiện chất lượng của các hướng dẫn được tạo
bằng cách chọn cái hoạt động tốt nhất trên dữ liệu validation.
iPrompt (Singh et al., 2022) là phương pháp có khả năng
nhất cho đến nay với quá trình tạo và validation lặp để
chọn hướng dẫn. Tuy nhiên, các cách tiếp cận này vẫn
cần thiết một tập validation để xếp hạng hướng dẫn, và
các hướng dẫn chúng tạo ra thường kém hiệu quả so với
những cái do con người viết.

Bên cạnh việc lựa chọn hướng dẫn, các nhà nghiên cứu
cũng đã khám phá các hướng trực giao khác để cải thiện
prompt LLM, chẳng hạn như việc lựa chọn các minh họa
trong ngữ cảnh. Một số công trình tập trung vào việc xác định
các minh họa phù hợp nhất từ các ví dụ huấn luyện (Rubin
et al., 2022; Lu et al., 2022a; Wang et al., 2023a) và thứ
tự tối ưu của chúng (Lu et al., 2022b) trong prompt few-shot.
Các nghiên cứu khác kiểm tra việc kỹ thuật và lựa chọn các
chuỗi lý luận được ghép nối với các minh họa few-shot trên
các nhiệm vụ lý luận nhiều bước (Wei et al., 2022; Zhang
et al., 2022b; Ye and Durrett, 2023; Liang et al., 2023b).
Chúng tôi dành việc khám phá tích hợp các kỹ thuật trực
giao này với cách tiếp cận của chúng tôi để tối ưu hóa toàn
diện toàn bộ prompt LLM cho công việc tương lai.

## 3 Công thức Bài toán
Trong công việc này, chúng tôi tập trung vào thiết lập few-shot
thực sự nơi người dùng nhằm giải quyết một nhiệm vụ mới
với LLM hộp đen. Trong khi dễ dàng nghĩ ra một số ít ví dụ
và một mô tả cơ bản, người dùng có thể không có cái nhìn
sâu sắc về loại hướng dẫn nào sẽ hiệu quả cho các ví dụ
chưa thấy. Do đó, với các ví dụ few-shot như minh họa và
mô tả cơ bản như hướng dẫn seed, mục tiêu của chúng tôi
là tự động hóa quá trình tạo ra một hướng dẫn hiệu quả
hơn cho nhiệm vụ đã cho.

Chúng tôi công thức hóa bài toán của mình theo các thực
hành thông thường của học trong ngữ cảnh (Dong et al.,
2023). Trong thiết lập few-shot nêu trên, prompt để truy
vấn LLM hộp đen bao gồm một hướng dẫn I, đầu vào test
x, và một vài cặp đầu vào-đầu ra như minh họa {xd_i, yd_i}^n_{i=1}.
LLM được mong đợi tạo ra một đầu ra y ∼ P(·|I,{xd_i, yd_i}^n_{i=1}, x).
Công việc này nhằm tự động tìm một hướng dẫn I' vượt
trội dựa trên hướng dẫn seed do con người viết I_s, do đó
tránh được nhu cầu kỹ thuật thủ công đáng kể. Bên cạnh
đó, chúng tôi cũng khám phá thiết lập zero-shot nơi không
có minh họa nào được cung cấp cho LLM.

Mặc dù hướng dẫn có thể có nhiều cách tích hợp với các
minh họa và đầu vào test, để giảm độ phức tạp của bài toán,
chúng tôi định dạng toàn bộ prompt theo thứ tự (I, xd_1, yd_1,
···, xd_n, yd_n, x). Điều này phù hợp với quy ước giải quyết
vấn đề nơi nhiệm vụ được phác thảo đầu tiên, tiếp theo là
việc cung cấp các ví dụ dữ liệu, và đầu vào test cuối cùng
được cung cấp. Trong thực tế, chúng tôi sử dụng n = 3 cho
tất cả các nhiệm vụ.

## 4 Auto-Instruct
Auto-Instruct bao gồm hai bước: tạo hướng dẫn và xếp hạng
hướng dẫn. Chúng tôi đầu tiên khuyến khích LLM hộp đen
tạo ra một tập đa dạng các hướng dẫn ứng viên (§4.1) cho
mỗi nhiệm vụ downstream. Tiếp theo, chúng tôi huấn luyện
một mô hình chấm điểm để xếp hạng tất cả các hướng dẫn
ứng viên cho mỗi ví dụ test đã cho, vì các ví dụ khác nhau
có thể hưởng lợi từ các hướng dẫn khác nhau (§4.2). Sau đó,
hướng dẫn được xếp hạng cao nhất được chọn để khuyến
khích LLM hộp đen trên ví dụ test cụ thể đó cho suy luận
downstream.

[Hình 2: Meta-prompt hướng dẫn LLM tạo ra một hướng dẫn
từng bước cho nhiệm vụ đã cho. Các meta-prompt khác được
hiển thị trong Phụ lục E.]

### 4.1 Tạo Hướng dẫn
Như đã đề cập trong §3, chúng tôi tận dụng một mô tả
nhiệm vụ cơ bản do con người viết như hướng dẫn seed I_s
và khuyến khích LLM hộp đen tạo ra một số hướng dẫn
ứng viên {I^c_j}^m_{j=1}. Cụ thể, trong thiết lập few-shot,
chúng tôi khuyến khích LLM tạo ra các hướng dẫn ứng viên
I^c ∼ P(·|I_s,{xd_i, yd_i}^n_{i=1}) dựa trên hướng dẫn seed
và các minh họa few-shot. Các cách tiếp cận trước đây (Zhou
et al., 2022; Singh et al., 2022) chỉ sử dụng một meta-prompt³
duy nhất và thu thập các hướng dẫn ứng viên thông qua
lấy mẫu token. Thường thì, các hướng dẫn được lấy mẫu
như vậy chỉ hiển thị các biến thể nhỏ trong cách diễn đạt
thay vì sự đa dạng nội dung đáng kể. Hơn nữa, chất lượng
của chúng phụ thuộc đệ quy vào sự lựa chọn tùy ý của
meta-prompt, điều này chuyển độ không đáng tin cậy của
kỹ thuật hướng dẫn thủ công sang kỹ thuật meta-prompt
thủ công.

Trong cách tiếp cận cải tiến của chúng tôi, chúng tôi tuyển
chọn một tập các meta-prompt để khuyến khích LLM lấy mẫu
các hướng dẫn ứng viên đa dạng bằng cách định nghĩa các
phong cách bắt buộc khác nhau của hướng dẫn. Các meta-prompt
này bao gồm:

1. Viết một hướng dẫn về cách giải quyết nhiệm vụ sau đây
   trong một câu.
2. Viết một hướng dẫn về cách giải quyết nhiệm vụ sau đây
   trong một đoạn văn.
3. Viết một hướng dẫn từng bước về cách giải quyết nhiệm vụ
   sau đây.
4. Viết một hướng dẫn về cách giải quyết nhiệm vụ sau đây.
   Hướng dẫn phải bao gồm giải thích của các ví dụ đã cho.

Cùng với 4 meta-prompt này, chúng tôi cũng đưa vào các
hướng dẫn do con người viết từ các nhiệm vụ NLP hiện có
như minh họa để hướng dẫn việc tạo

³Prompt cho LLM để tạo hướng dẫn.

các hướng dẫn. Một cách trực quan, chúng tôi khuyến khích
LLM bắt chước phong cách của các hướng dẫn do con người
viết trong các nhiệm vụ minh họa này. Chúng tôi lấy nguồn
các nhiệm vụ minh họa với hướng dẫn của chúng từ các
nhiệm vụ huấn luyện của chúng tôi trong SuperNI, nhóm
chúng thành 3 cluster dựa trên độ dài hướng dẫn của chúng,
để hướng dẫn LLM tạo ra các hướng dẫn với độ chi tiết
khác nhau. Hình 2 cung cấp một ví dụ về meta-prompt #3.
Các meta-prompt khác được chi tiết trong Phụ lục E.

Dựa trên 7 meta-prompt riêng biệt này (tức là, 4 meta-prompt
cụ thể theo phong cách + 3 nhóm nhiệm vụ minh họa), chúng
tôi lấy mẫu 3 hướng dẫn dưới mỗi meta-prompt thông qua
nucleus sampling (Holtzman et al., 2020). Bao gồm hướng
dẫn seed gốc, chúng tôi thu thập tổng cộng 22 hướng dẫn
ứng viên cho mỗi nhiệm vụ. Kết quả là, chúng tôi tạo ra
một tập hợp đa dạng và toàn diện các hướng dẫn ứng viên,
do đó giảm tính ngẫu nhiên do các sắc thái của các meta-prompt
khác nhau mang lại.

Trong thiết lập zero-shot, do không có minh họa, LLM được
khuyến khích tạo ra hướng dẫn ứng viên I^c ∼ P(·|I_s) chỉ
dựa trên hướng dẫn seed. Bên cạnh đó, meta-prompt giải
thích ví dụ được loại bỏ. Như chúng tôi minh chứng trong
§5.4.5, ngay cả khi không có sự hỗ trợ của minh họa, các
meta-prompt cụ thể theo phong cách của chúng tôi vẫn
cho phép LLM tạo ra các hướng dẫn thông tin.

#### 4.1.1 Bất ổn định Dưới Các Hướng dẫn Khác nhau
Trong khi LLM có khả năng tạo ra các hướng dẫn có ý nghĩa,
việc dựa vào một hướng dẫn được tạo duy nhất có thể sẽ
dẫn đến hiệu suất không tối ưu do sự nhạy cảm của LLM
với cách diễn đạt của các hướng dẫn. Sự bất ổn định này
đặc biệt rõ ràng trong thiết lập zero-shot do thiếu minh
họa để hỗ trợ dự đoán. Trong Hình 3, chúng tôi tính toán
độ lệch chuẩn của hiệu suất LLM khi sử dụng các hướng
dẫn khác nhau, sau khi đã đánh giá tất cả các hướng dẫn
cho mỗi nhiệm vụ downstream. Điều này chỉ ra sự dao động
hiệu suất mong đợi khi thay thế một hướng dẫn bằng một
hướng dẫn khác. Độ lệch chuẩn trung vị trên tất cả các
nhiệm vụ là 3.1 và 4.2 điểm trong ROUGE-L cho thiết lập
few-shot và zero-shot tương ứng trên SuperNI, và các
quartile trên là 5.7 và 6.9 điểm tương ứng. Việc lựa chọn
hướng dẫn thậm chí gây ra dao động hiệu suất hai chữ số
trên nhiều nhiệm vụ. Do đó, việc phát triển một phương
pháp để xếp hạng và chọn hướng dẫn trở thành một
công việc cần thiết.

[Hình 3: Biểu đồ hộp cho thấy hiệu suất LLM thay đổi bao nhiêu
với các hướng dẫn khác nhau, được thử nghiệm trên text-davinci-003
của OpenAI. Hiệu suất được đánh giá bằng ROUGE-L trên SuperNI
và Accuracy trên BBH. Mỗi giá trị đại diện cho độ lệch chuẩn
của hiệu suất LLM trên tất cả các hướng dẫn được tạo ra trên
một nhiệm vụ duy nhất.]

### 4.2 Xếp hạng Hướng dẫn
Trong thiết lập few-shot thực sự, các minh họa không đủ
để phản ánh hiệu quả của các hướng dẫn ứng viên do kích
thước mẫu nhỏ. Để vượt qua hạn chế này, chúng tôi huấn
luyện một mô hình xếp hạng hướng dẫn có thể tổng quát
trên nhiều nhiệm vụ NLP khác nhau, và sau đó áp dụng
nó cho mỗi ví dụ test trong các nhiệm vụ ngoài miền. Một
cách trực quan, mô hình này được huấn luyện để xếp hạng
các hướng dẫn dựa trên hiệu suất downstream của chúng
trên LLM, tức là, gán điểm số cao hơn cho các hướng dẫn
hiệu quả hơn.

#### 4.2.1 Mô hình
Nhờ khả năng tổng quát đã được chứng minh của họ mô
hình T5 (Raffel et al., 2020; Sanh et al., 2022), chúng tôi
bắt đầu từ mô hình FLAN-T5-Large được điều chỉnh hướng
dẫn (Chung et al., 2022) và huấn luyện nó với mục tiêu
xếp hạng hướng dẫn của chúng tôi. Cho một ví dụ cụ thể
(x, y) nơi x là đầu vào và y là đầu ra ground-truth, cũng
như một hướng dẫn ứng viên tùy ý I^c, mô hình dự đoán
một điểm số Q_T5(I^c, x) như một ước tính của hiệu quả
của hướng dẫn trên ví dụ. Tận dụng bản chất tuân theo
hướng dẫn của FLAN-T5, chúng tôi đưa ra prompt sau
cho mô hình xếp hạng:

Ví dụ: x. Đầu vào: I^c. Đây có phải là một hướng dẫn tốt
để giải quyết ví dụ không?

Q_T5(I^c, x) sau đó được tính như logit của token "yes"
tại vị trí bắt đầu của decoder. Ngoài ra, chúng tôi thu được
hiệu suất downstream của I^c bằng cách tính điểm ROUGE-L
giữa đầu ra dự đoán của LLM ŷ (khi I^c được sử dụng như
hướng dẫn) so với đầu ra ground-truth y, được ký hiệu
là r(y,ŷ). Mô hình sau đó được huấn luyện với loss
list-wise để căn chỉnh điểm số Q_T5(I^c, x) của tất cả
các hướng dẫn ứng viên với hiệu suất downstream tương
ứng r(y,ŷ) của chúng, trong khi xem xét sự vượt trội
tương đối giữa các hướng dẫn khác nhau. Cụ thể, chúng
tôi đầu tiên chuẩn hóa cả danh sách điểm số dự đoán
Q_T5(I^c, x) và danh sách hiệu suất downstream r(y,ŷ)
bằng cách áp dụng softmax trên tất cả các hướng dẫn ứng
viên, và sau đó tính KL-divergence giữa hai phân phối
chuẩn hóa này như loss huấn luyện:

L = 1/|B| ∑_{(x,y)∈B} KL[σ(r(y,ŷ)) || σ(Q_T5(I^c, x))]

nơi ŷ ∼ P_LLM(·|I^c,{xd_i, yd_i}^n_{i=1}, x).

Lưu ý rằng B là một batch các ví dụ và σ là hàm softmax.
Trong quá trình thử nghiệm, cho một ví dụ test cụ thể,
trong số tất cả các hướng dẫn ứng viên, chúng tôi chọn
I^c đạt điểm số Q_T5(I^c, x) cao nhất như hướng dẫn cuối
cùng, và khuyến khích LLM với nó để thu được đầu ra mong muốn.

#### 4.2.2 Dữ liệu Huấn luyện
Để huấn luyện một mô hình xếp hạng như vậy với khả năng
tổng quát cho các nhiệm vụ ngoài miền, chúng tôi phân
loại các nhiệm vụ trong benchmark SuperNI theo loại
nhiệm vụ của chúng (ví dụ, QA, phân tích cảm xúc, v.v.)
và nhóm các danh mục này thành các tập huấn luyện và
test. Chúng tôi loại trừ các nhiệm vụ liên quan đến các
ngôn ngữ không phải tiếng Anh hoặc những nhiệm vụ
có đầu vào quá dài. Để tránh rò rỉ dữ liệu, chúng tôi cũng
loại trừ các nhiệm vụ từ dữ liệu huấn luyện được lấy nguồn
từ cùng dataset với bất kỳ nhiệm vụ test nào. Điều này
mang lại 575 nhiệm vụ cho huấn luyện và 91 để thử nghiệm.

Chúng tôi lấy mẫu tối đa 400 ví dụ từ mỗi nhiệm vụ huấn
luyện, dẫn đến tổng cộng 122k. Các phương pháp tiền xử
lý và lọc dữ liệu bổ sung được sử dụng để tăng tốc quá
trình huấn luyện có thể được tìm thấy trong Phụ lục A.

## 5 Thí nghiệm

### 5.1 Thiết lập
Để đánh giá cách tiếp cận của chúng tôi trong thiết lập
few-shot thực sự, chúng tôi thử nghiệm nó trên nhiều loại
nhiệm vụ ngoài miền - 91 từ SuperNI (Wang et al., 2022)
và 27 từ BBH (Suzgun et al., 2022), nơi không có sự chồng
chéo giữa các danh mục nhiệm vụ trong huấn luyện và thử
nghiệm. Tập test SuperNI bao gồm cả nhiệm vụ phân loại
và tạo, ví dụ, phân loại thông thường, trích xuất thông tin,
v.v.⁴ BBH trình bày một tập đa dạng các nhiệm vụ trải rộng
QA thông thường và các bài toán toán học. ROUGE-L⁵ trung
bình và độ chính xác exact-match được sử dụng để đánh
giá trên SuperNI và BBH, tương ứng. Các thí nghiệm chính
của chúng tôi được tiến hành sử dụng text-davinci-003
của OpenAI cho việc tạo hướng dẫn và suy luận downstream.
Chúng tôi cũng khám phá các hướng dẫn được tạo bởi
ChatGPT (gpt-3.5-turbo) hoặc GPT-4 (OpenAI, 2023) trong §5.4.1.

Trong thiết lập zero-shot, mô hình xếp hạng được huấn
luyện riêng trên dữ liệu nơi điểm ROUGE downstream của
các hướng dẫn ứng viên cũng được thu được dưới việc
khuyến khích zero-shot. Đối với các nhiệm vụ phân loại
zero-shot, chúng tôi thêm các hướng dẫn định dạng bổ
sung vào hướng dẫn seed để thu hẹp các tùy chọn trả lời
trong cả việc tạo hướng dẫn và suy luận downstream.
Các thiết lập thí nghiệm bổ sung có thể được tìm thấy
trong Phụ lục B.

⁴Danh sách đầy đủ các nhiệm vụ test SuperNI có trong Phụ lục G.
⁵Các tác giả gốc của SuperNI thấy ROUGE-L tương quan tích cực
với độ chính xác trên các nhiệm vụ phân loại, nên ROUGE-L trung
bình được áp dụng để đơn giản.

### 5.2 Baseline
Như baseline trong các thí nghiệm của chúng tôi, chúng
tôi đầu tiên xem xét ba cách tiếp cận thay thế chỉ dựa
trên việc khuyến khích LLM:

(1) Cross-Validation. Chúng tôi tận dụng các minh họa
3-shot như dữ liệu validation để xếp hạng các hướng dẫn,
với mỗi cái đóng vai trò như ví dụ test lặp đi lặp lại
trong khi hai cái khác phục vụ như minh họa. Điểm ROUGE-L
(hoặc độ chính xác cho BBH) được sử dụng như tiêu chí
xếp hạng chính, và log-probability của đầu ra ground-truth
được so sánh như tie-breaker. Hướng dẫn được chọn bởi
các minh họa sau đó được áp dụng trên tất cả các ví dụ
test trong cùng nhiệm vụ.

(2) LM Selection. Chúng tôi trực tiếp khuyến khích chính
LLM để chọn một hướng dẫn bằng cách liệt kê tất cả các
hướng dẫn ứng viên trong một prompt duy nhất. Chúng
tôi đánh số các hướng dẫn và yêu cầu LLM tạo ra số của
hướng dẫn mà nó cho là phù hợp nhất cho mỗi ví dụ test.

(3) On-the-fly Generation. Như một biến thể đơn giản
hóa không có xếp hạng hướng dẫn, mô hình được yêu cầu
trực tiếp tạo ra một hướng dẫn cho mỗi ví dụ test. Hướng
dẫn được tạo sau đó được sử dụng để khuyến khích LLM
cho cùng ví dụ đó.

Hơn nữa, chúng tôi xem xét iPrompt (Singh et al., 2022),
cách tiếp cận tiên tiến hiện có trong việc tối ưu hóa hướng
dẫn với LLM. iPrompt lặp lại tạo ra các hướng dẫn cho
đến khi nó không thể tìm thấy một cái với hiệu suất tốt
hơn trên tập validation. Để đánh giá iPrompt trong thiết
lập few-shot thực sự, chúng tôi tiến hành validation của
nó trên các minh họa 3-shot. Bên cạnh đó, vì iPrompt
gốc tạo ra các hướng dẫn dựa trên các ví dụ mà không
có bất kỳ mô tả nhiệm vụ nào, để so sánh công bằng,
chúng tôi triển khai một baseline iPrompt+ sử dụng
meta-prompt tương tự như của chúng tôi với hướng dẫn
seed (Xem Phụ lục C để biết chi tiết). Ngoài ra, chúng
tôi đánh giá hiệu suất của việc không sử dụng bất kỳ
hướng dẫn nào (Empty Instruction), trực tiếp sử dụng
hướng dẫn seed do con người viết (Human Instruction)
hoặc ngẫu nhiên chọn một hướng dẫn từ các ứng viên
được tạo ra (Random Selection) trên mỗi nhiệm vụ.

### 5.3 Kết quả
Kết quả tổng thể của SuperNI và BBH được hiển thị trong
Bảng 1, nơi điểm số được tính trung bình trên tất cả các
nhiệm vụ. Auto-Instruct cho thấy tính nhất quán và khả
năng tổng quát đáng chú ý trong các kịch bản ngoài miền,
vượt trội tất cả các baseline trên các benchmark và thiết
lập khác nhau. Những phát hiện chính được nêu dưới đây.

LLM cho thấy khả năng cạnh tranh trong việc tạo ra các
hướng dẫn hiệu quả, nhưng xếp hạng vẫn cần thiết. Phù
hợp với công trình trước đây (Zhou et al., 2022; Singh
et al., 2022), LLM có thể tạo ra các hướng dẫn hiệu quả
cho các nhiệm vụ khác nhau. Các meta-prompt cụ thể
theo phong cách của chúng tôi cho phép LLM tạo ra một
tập đa dạng các hướng dẫn để phục vụ các kịch bản đa
dạng nơi các nhiệm vụ khác nhau có thể ưa thích các
phong cách hướng dẫn khác nhau. Trong thiết lập few-shot,
các hướng dẫn do LLM tạo ra đã vượt trội các đối tác
do con người viết trung bình, như được chỉ ra bởi điểm
số chọn ngẫu nhiên. Mặc dù con người có thể có kiến
thức trước về một số ví dụ khi họ viết hướng dẫn, LLM,
không được cung cấp bất kỳ minh họa nào trong thiết
lập zero-shot, tạo ra các hướng dẫn có chất lượng tương
đương với những cái do con người viết. Tuy nhiên, cả
việc chọn ngẫu nhiên hay trực tiếp tạo ra một hướng
dẫn duy nhất (tức là, on-the-fly generation) đều không
cải thiện đáng kể so với baseline do con người viết.
Điều này phù hợp với sự bất ổn định của hiệu suất LLM
trên các hướng dẫn khác nhau như được thảo luận trong
Hình 3, điều này chỉ ra việc xếp hạng hướng dẫn thêm
vẫn cần thiết.

[Bảng 1: Kết quả trên SuperNI (91 nhiệm vụ) và BBH (27 nhiệm vụ)
dưới thiết lập few-shot và zero-shot tương ứng. Chúng tôi báo cáo
ROUGE-L trên SuperNI và độ chính xác trên BBH. Các phương pháp
với * áp dụng cùng một hướng dẫn cho một nhiệm vụ nhất định,
trong khi các phương pháp với † có thể chọn các hướng dẫn khác
nhau cho các ví dụ khác nhau. iPrompt lặp lại tạo và xếp hạng
các hướng dẫn ứng viên, trong khi các phương pháp khác áp dụng
pipeline generate-then-rank. Chúng tôi lưu ý rằng iPrompt,
iPrompt+ và Cross-Validation không áp dụng được trong thiết
lập zero-shot do cần các ví dụ validation. Kết quả chi tiết
trên SuperNI của các danh mục nhiệm vụ khác nhau có thể được
tìm thấy tại Phụ lục D.1.]

Việc chỉ khuyến khích LLM hoặc sử dụng dữ liệu validation
không đáng tin cậy trong thiết lập tài nguyên thấp. Mặc
dù cung cấp sự tiện lợi của việc không huấn luyện bất
kỳ mô hình nào, cả việc trực tiếp khuyến khích LLM (LM
selection) và sử dụng các minh họa few-shot để validation
(iPrompt và cross-validation) đều không mang lại kết
quả cải thiện nhất quán so với việc chọn ngẫu nhiên.
Điều này nhấn mạnh rằng (1) chính LLM thiếu manh mối
về hiệu suất downstream mong đợi của các hướng dẫn
khác nhau; (2) khối lượng dữ liệu validation phải đủ
lớn để ước tính hiệu quả hiệu suất của các hướng dẫn
trên dữ liệu test, điều này mang lại chi phí cao trong
nhiều kịch bản thực tế.

Mô hình xếp hạng hướng dẫn được huấn luyện của chúng
tôi là cách tiếp cận hiệu quả nhất để chọn hướng dẫn
cho đến nay. Mặc dù dữ liệu và hướng dẫn cho các nhiệm
vụ ngoài miền không được nhìn thấy bởi mô hình xếp hạng,
nó thể hiện khả năng tổng quát đầy hứa hẹn trong việc
chọn các hướng dẫn hiệu quả nhờ việc huấn luyện trên
hàng trăm nhiệm vụ khác nhau. Ví dụ, trên benchmark
SuperNI, nó vượt trội việc chọn ngẫu nhiên 4% và 8%
trên thiết lập few-shot và zero-shot tương ứng. Bên cạnh
đó, pipeline hoàn chỉnh của chúng tôi mang lại cải thiện
tương đối 6% so với các hướng dẫn do con người viết
gốc trong cả thiết lập few-shot và zero-shot, chỉ ra
rằng các hướng dẫn do con người viết vẫn cần cải thiện
trong nhiều bối cảnh.

### 5.4 Phân tích
Trong phần này, chúng tôi đi sâu hơn vào hiệu suất của
cách tiếp cận của chúng tôi bằng cách phân tích việc sử
dụng các LLM khác cho việc tạo hướng dẫn, hiệu suất
trên các nhiệm vụ đã thấy, kích thước dữ liệu huấn luyện,
và các nghiên cứu trường hợp. Phân tích bổ sung về so
sánh giữa Auto-Instruct và ensemble multi-answer có
trong Phụ lục D. Các phân tích này được tiến hành trong
thiết lập few-shot trừ khi được nêu khác.

#### 5.4.1 Tổng quát hóa sang các LLM khác
Để thử nghiệm thêm về khả năng tổng quát của cách tiếp
cận của chúng tôi, chúng tôi chuyển Auto-Instruct sang
các LLM khác bằng cách sử dụng ChatGPT (gpt-3.5-turbo)
và GPT-4 như các mô hình suy luận downstream. Như Bảng
2 gợi ý, các hướng dẫn được chọn bởi Auto-Instruct trên
text-davinci-003 vẫn hiệu quả nếu được chuyển sang
ChatGPT và GPT-4. Hơn nữa, mô hình xếp hạng hướng dẫn
của chúng tôi có thể xếp hạng các hướng dẫn được tạo
bởi ChatGPT hoặc GPT-4 trong cả kịch bản few-shot và
zero-shot, mặc dù không nhìn thấy bất kỳ hướng dẫn
nào được tạo bởi các LLM này trong quá trình huấn luyện.
Kết quả cải thiện cũng có thể được thấy khi chuyển
Auto-Instruct sang LLaMA-2-chat (Touvron et al., 2023),
một LLM mã nguồn mở gần đây, như được hiển thị trong
Phụ lục D.2. Như một kết luận, mặc dù có sự biến thể
trong cách diễn đạt trên các hướng dẫn được tạo bởi
các LLM khác nhau, mẫu cơ bản xác định hiệu quả hướng
dẫn có thể chuyển đổi, mặc dù cải thiện lớn nhất vẫn
được thấy trong các thí nghiệm cùng-LLM. Đủ để nói,
mô hình xếp hạng hướng dẫn được huấn luyện của chúng
tôi có thể được áp dụng trực tiếp để chọn hướng dẫn
cho các LLM khác mà không cần huấn luyện lại.

[Bảng 2: Kết quả SuperNI của việc chuyển Auto-Instruct
sang ChatGPT và GPT-4, sử dụng hoặc (1) hướng dẫn được
tạo bởi text-davinci-003, hoặc (2) hướng dẫn được tạo
bởi cùng mô hình như suy luận downstream (tức là,
ChatGPT hoặc GPT-4). Mô hình xếp hạng hướng dẫn vẫn
là cái được huấn luyện trên hướng dẫn text-davinci-003.]

#### 5.4.2 Đánh giá Xếp hạng Hướng dẫn
Để điều tra hiệu quả của mô hình xếp hạng hướng dẫn,
chúng tôi so sánh nó với các baseline lựa chọn hướng
dẫn khác bằng cách gán nhãn bạc cho các hướng dẫn ứng
viên, với kết quả chi tiết trong Bảng 3. Đầu tiên, chúng
tôi sử dụng hiệu suất downstream thực tế của các hướng
dẫn ứng viên như nhãn bạc. Mô hình xếp hạng của chúng
tôi có khả năng phân biệt các hướng dẫn tốt hơn, như
được thể hiện bởi độ chính xác rõ ràng cao hơn trong
việc chọn các hướng dẫn top-1 hoặc top-5 trong số tất
cả 22 ứng viên. Thứ hai, chúng tôi đánh giá tần suất
hướng dẫn được chọn cải thiện hiệu suất downstream
so với hướng dẫn trống hoặc hướng dẫn do con người
viết. Một lần nữa, các hướng dẫn từ mô hình xếp hạng
của chúng tôi tạo ra những cải thiện đáng kể nhất, vượt
trội các đối tác do con người viết trong 7% trường hợp
nhiều hơn so với việc chọn ngẫu nhiên. Việc tăng hiệu
suất nhất quán trên tất cả các đánh giá nhãn bạc tiếp
tục chứng thực sự vượt trội của mô hình của chúng tôi
so với các phương pháp xếp hạng thay thế dựa trên
cross-validation hoặc LM selection.

[Bảng 3: Đánh giá xếp hạng hướng dẫn trên nhãn bạc.
Trái: chúng tôi đánh giá phần trăm trường hợp nơi hướng
dẫn được chọn là tốt nhất (top-1) hoặc nằm trong top-5
ứng viên, theo hiệu suất downstream thực tế. Chúng tôi
lưu ý rằng có thể có nhiều hướng dẫn chia sẻ điểm số
tốt nhất. Phải: chúng tôi kiểm tra phần trăm hướng dẫn
được chọn vượt trội hướng dẫn trống hoặc những cái do
con người viết.]

#### 5.4.3 Auto-Instruct trên Các Nhiệm vụ Đã thấy
Bên cạnh thiết lập ngoài miền, chúng tôi khám phá một
thiết lập trong miền nơi chúng tôi chọn các ví dụ bổ
sung từ các nhiệm vụ đã thấy trong quá trình huấn luyện,
để kiểm tra thêm năng lực của mô hình xếp hạng hướng
dẫn. Để so sánh công bằng khả năng xếp hạng của mô
hình trên các nhiệm vụ khác nhau, chúng tôi thí nghiệm
với các ví dụ nhạy cảm hướng dẫn, được định nghĩa là
các ví dụ nơi không phải tất cả các hướng dẫn ứng viên
đều mang lại cùng điểm ROUGE. Chúng tôi lấy mẫu 100
ví dụ bổ sung từ mỗi trong 100 nhiệm vụ được thấy
trong huấn luyện nhưng không được bao gồm trong tập
dev. Như được trình bày trong Bảng 4, mô hình cho
thấy khả năng xếp hạng tăng cường trên các nhiệm vụ
đã thấy do tiếp xúc trước với các hướng dẫn trong quá
trình huấn luyện. Điều này chỉ ra rằng cách tiếp cận
của chúng tôi hữu ích trong cả hoàn cảnh giàu dữ liệu
và hiếm dữ liệu.

[Bảng 4: Kết quả trên dữ liệu test nhạy cảm hướng dẫn
của cả nhiệm vụ đã thấy (100 nhiệm vụ đã thấy trong
huấn luyện) và nhiệm vụ chưa thấy (giống như Bảng 1)
từ SuperNI. Chúng tôi báo cáo thêm tỷ lệ cải thiện
tương đối so với baseline chọn ngẫu nhiên vì hiệu suất
vanilla không ở cùng quy mô.]

#### 5.4.4 Tác động của Nhiều Nhiệm vụ Huấn luyện Hơn
Để phân tích tác động của huấn luyện đa nhiệm vụ quy
mô lớn đến khả năng tổng quát ngoài miền, chúng tôi
thao tác số lượng nhiệm vụ huấn luyện của mô hình xếp
hạng hướng dẫn. Cụ thể, chúng tôi loại trừ các nhiệm
vụ từ tập huấn luyện theo danh mục của chúng, tức là,
tất cả các nhiệm vụ từ các danh mục được chọn đều được
loại bỏ. Như được hiển thị trong Hình 4, sự gia tăng
số lượng nhiệm vụ huấn luyện từ các danh mục bổ sung
là một đóng góp chính cho hiệu suất vượt trội của mô
hình của chúng tôi so với baseline chọn ngẫu nhiên.
Vì hiệu suất chưa đạt đỉnh khi tất cả các nhiệm vụ
được bao gồm, có thể mong đợi những cải thiện hiệu
suất thêm nếu có nhiều nhiệm vụ huấn luyện hơn.

[Hình 4: Kết quả của việc sử dụng số lượng nhiệm vụ
huấn luyện khác nhau. 0% có nghĩa là trực tiếp sử dụng
checkpoint FLAN-T5 được huấn luyện trước trong xếp
hạng hướng dẫn, cho thấy hiệu suất tương tự như việc
chọn hướng dẫn ngẫu nhiên.]

#### 5.4.5 Phân tích các Hướng dẫn Được chọn
Hình 6 minh họa cách các hướng dẫn được chọn của chúng
tôi cải thiện các hướng dẫn do con người viết gốc. Như
được chỉ ra bởi điểm số tương tự trung bình, Auto-Instruct
có thể cung cấp các hướng dẫn tương tự hơn với những
cái tối ưu trong số các ứng viên. Như được minh chứng
bởi biểu đồ scatter, trong các kịch bản nơi hướng dẫn
được chọn vượt trội hướng dẫn do con người viết, embedding
của nó thường lệch đáng kể từ embedding của hướng dẫn
do con người viết nhưng gần với cái tối ưu. Những kết
quả này gợi ý rằng hướng dẫn được chọn tinh chỉnh hướng
dẫn seed do con người viết bằng cách tiến tới giải pháp
lý tưởng, trong khi khoảng cách embedding giữa các hướng
dẫn được chọn và seed khiến việc cải thiện như vậy khó
đạt được bằng kỹ thuật thủ công thuần túy.

Ngoài ra, chúng tôi cung cấp một nghiên cứu trường hợp
trong Hình 5 trong thiết lập zero-shot nơi LLM không
thể tham khảo bất kỳ minh họa nào. Tuy nhiên, LLM
quản lý để tạo ra các ví dụ bổ sung sử dụng kiến thức
có được từ việc huấn luyện trước rộng rãi của nó. Các
ví dụ bổ sung này có thể đóng vai trò như minh họa để
tạo ra thiết lập "2-shot inference", dẫn đến một dự đoán
đúng không thể đạt được thông qua suy luận zero-shot
gốc. Ngược lại, chúng tôi cũng trình bày một ví dụ nơi
hướng dẫn do LLM tạo ra bao gồm các mô tả ảo tưởng
làm biến dạng ý nghĩa gốc của hướng dẫn seed. Sự không
khớp giữa hướng dẫn này và ví dụ test dẫn đến việc nó
bị từ chối bởi mô hình xếp hạng. Độc giả có thể tìm thêm
các nghiên cứu trường hợp trong Phụ lục F.

[Hình 5: Trong trường hợp này, Auto-Instruct chọn một
hướng dẫn "biến đổi" suy luận zero-shot thành suy luận
"2-shot" bằng cách cung cấp các ví dụ bổ sung (tô sáng
màu đỏ), trong khi loại bỏ một hướng dẫn bao gồm ảo
tưởng trong mô tả nhiệm vụ (tô sáng màu xanh). Hướng
dẫn do con người viết cũng được bao gồm trong các ứng
viên xếp hạng.]

[Hình 6: Trên: Embedding hướng dẫn của 5 nhiệm vụ SuperNI
nơi hướng dẫn được chọn Auto-Instruct hoạt động tốt hơn
hướng dẫn do con người viết, được trực quan hóa bởi T-SNE.
"Best" đề cập đến hướng dẫn với điểm ROUGE cao nhất.
Dưới: Tương tự cosine trung bình giữa embedding hướng
dẫn trên tất cả các nhiệm vụ SuperNI. Hai mô hình embedding
là text-embedding-ada-002 từ OpenAI và all-mpnet-base-v2
từ Sentence-Transformers⁶. Xem tốt nhất khi có màu.]

## 6 Kết luận
Trong công việc này, chúng tôi giới thiệu Auto-Instruct,
một cách tiếp cận tự động để tạo, xếp hạng và chọn hướng
dẫn, cung cấp một giải pháp cho chi phí cao và tính chủ
quan liên quan đến các hướng dẫn do con người kỹ thuật.
Cách tiếp cận của chúng tôi bắt đầu bằng việc khuyến
khích LLM tạo ra một tập đa dạng các hướng dẫn ứng viên.
Tiếp theo, một mô hình xếp hạng hướng dẫn được huấn
luyện trên hàng trăm nhiệm vụ được sử dụng để xếp hạng
các hướng dẫn ứng viên và chọn cái hiệu quả nhất để
giải quyết một ví dụ cụ thể. Kết quả thí nghiệm chứng
minh rằng cách tiếp cận của chúng tôi cung cấp các hướng
dẫn tốt hơn cả những cái do con người viết và những cái
được tạo bởi các cách tiếp cận tạo hướng dẫn trước đây,
như được thử nghiệm trên 118 nhiệm vụ ngoài miền.

## Hạn chế
Theo hiểu biết của chúng tôi, công việc này có những
hạn chế sau:

• Do chi phí đáng kể liên quan đến các mô hình OpenAI,
và khả năng hạn chế của giao diện API của chúng, chúng
tôi chỉ chấm điểm các hướng dẫn ứng viên trên một số
lượng vừa phải các nhiệm vụ như được mô tả trong §4.2.2.
Với kết quả trong Hình 4, chúng tôi mong đợi rằng mô
hình có thể thể hiện khả năng tổng quát cải thiện nếu
có nhiều dữ liệu huấn luyện với các hướng dẫn được
gán nhãn hơn.

• Phạm vi của nghiên cứu này bị giới hạn ở việc tạo
hướng dẫn bằng tiếng Anh; các nhiệm vụ bằng các ngôn
ngữ không phải tiếng Anh không phải là một phần của
dữ liệu huấn luyện của chúng tôi. Kết quả là, mô hình
có thể không hoạt động thỏa đáng cho các nhiệm vụ
không phải tiếng Anh. Điều tra thêm về việc tạo hướng
dẫn đa ngôn ngữ được để lại cho công việc tương lai.

• Mặc dù sử dụng một loạt rộng các meta-prompt, điều
này giảm đáng kể sự phụ thuộc vào kỹ thuật prompt,
cách diễn đạt của các meta-prompt này vẫn có thể ảnh
hưởng đến chất lượng của các hướng dẫn được tạo ra.
Chúng tôi để lại việc khám phá tự động đa dạng hóa
các hướng dẫn được tạo ra như công việc tương lai.

⁶www.sbert.net/docs/pretrained_models.html

## Lời cảm ơn
Công việc này được hỗ trợ bởi NSF IIS-2119531,
IIS-2137396, IIS-2142827, IIS-2234058, CCF-1901059,
và ONR N00014-22-1-2507. Chúng tôi cảm ơn Canwen Xu
(University of California San Diego) vì những gợi ý
quý giá của anh ấy trong quá trình viết bài báo.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ được bao gồm trong
bản gốc và được dịch theo cùng cấu trúc]

## Phụ lục A Tiền xử lý Dữ liệu Huấn luyện
Như được chi tiết trong §4.2, mô hình xếp hạng hướng
dẫn được huấn luyện để xếp hạng các hướng dẫn ứng
viên dựa trên hiệu suất downstream của chúng trên LLM.
Hiệu suất downstream của một hướng dẫn I^c đề cập đến
mức độ khớp của đầu ra dự đoán LLM ŷ với đầu ra ground-truth
y khi sử dụng I^c để khuyến khích LLM, được định lượng
bởi điểm ROUGE-L r(y,ŷ). Để tính toán điểm số này,
chúng tôi ghép nối mỗi ví dụ huấn luyện với tất cả 22
hướng dẫn ứng viên của nhiệm vụ tương ứng (được tạo
với phương pháp trong §4.1), và thu thập đầu ra dự đoán
của LLM cho ví dụ được khuyến khích bởi mỗi hướng dẫn
ứng viên. Sau khi tính toán điểm ROUGE-L so với ground-truth,
chúng tôi loại bỏ các ví dụ nơi các hướng dẫn ứng viên
không thể xếp hạng riêng biệt - trong các trường hợp
nơi phạm vi hiệu suất downstream trên các hướng dẫn
khác nhau ít hơn 10 điểm trong ROUGE-L.

Để tăng tốc quá trình huấn luyện, chúng tôi lấy mẫu 8
hướng dẫn ứng viên từ tổng pool 22 cho mỗi ví dụ, và
huấn luyện mô hình để xếp hạng 8 hướng dẫn này. Tuy
nhiên, trong một số nhiệm vụ, các hướng dẫn nhất định
có thể vượt trội đáng kể so với các cái khác. Việc lấy
mẫu đều 8 hướng dẫn ứng viên có thể dẫn đến các hướng
dẫn "đặc biệt" như vậy được ưa chuộng quá nhiều lần
trong việc huấn luyện mô hình xếp hạng. Để giải quyết
điều này, chúng tôi tỷ lệ nghịch tỷ lệ lấy mẫu của mỗi
hướng dẫn với độ phổ biến của nó (tức là, số trường
hợp nơi hướng dẫn này vượt trội tất cả các cái khác).
Cuối cùng, chúng tôi lấy mẫu tối đa 400 ví dụ từ mỗi
nhiệm vụ huấn luyện, dẫn đến tổng cộng 122k ví dụ
huấn luyện.

## Phụ lục B Thiết lập Thí nghiệm Chi tiết
Mô hình xếp hạng hướng dẫn được khởi tạo với FLAN-T5-Large
(780M tham số; Chung et al., 2022), và được huấn luyện
sử dụng Adafactor (Shazeer and Stern, 2018) với tỷ lệ
học 5e-5, kích thước batch 128 và tỷ lệ dropout 0.1.
Chúng tôi sử dụng một tập dev trong miền bao gồm tổng
cộng 5k ví dụ chưa thấy từ 100 nhiệm vụ huấn luyện để
chọn checkpoint tốt nhất trong 5 epoch. Hiệu suất validation
trên tập dev là 67.66 trong ROUGE-L, trong khi việc chọn
ngẫu nhiên chỉ đạt điểm 54.28. Khi sử dụng các mô hình
OpenAI, cho việc tạo hướng dẫn, chúng tôi đặt độ dài
hướng dẫn tối đa là 300 token, và chúng tôi sử dụng
temperature 1.0 và top_p 0.75 cho lấy mẫu token; cho
suy luận downstream, chúng tôi đặt cả hai về 0 cho đầu
ra xác định. Việc tạo tất cả các hướng dẫn ứng viên cho
91 nhiệm vụ test SuperNI tốn chúng tôi 18 USD tổng cộng,
theo giá của OpenAI (0.02 USD mỗi 1k token cho text-davinci-003).
Trong các thí nghiệm text-davinci-003, điểm chọn ngẫu
nhiên được tính như điểm trung bình trên tất cả các
hướng dẫn trên mỗi ví dụ, bao gồm hướng dẫn seed do
con người viết. Trong các hướng dẫn ChatGPT và GPT-4,
do khả năng hạn chế của giao diện API của chúng, chúng
tôi ngẫu nhiên lấy mẫu một hướng dẫn cho mỗi ví dụ
và thử nghiệm hiệu suất của nó.

## Phụ lục C Baseline iPrompt
Trong phần này, chúng tôi phác thảo các điều chỉnh được
thực hiện cho phương pháp iPrompt⁸ (Singh et al., 2022)
cho thiết lập của chúng tôi. Chúng tôi chủ yếu giải quyết
hai sự khác biệt giữa việc triển khai gốc của nó và thiết
lập của chúng tôi: (1) iPrompt gốc tạo ra các hướng dẫn
sử dụng GPT-J (Wang and Komatsuzaki, 2021), và (2) nó
sử dụng một tập validation để chấm điểm và chọn hướng
dẫn. Để giải quyết (1), chúng tôi sử dụng text-davinci-003
cho việc tạo hướng dẫn của nó, giống hệt mô hình được
sử dụng cho suy luận downstream. Đối với (2), chúng tôi
tiến hành validation hướng dẫn của nó trên các minh họa
3-shot. Do chi phí của việc yêu cầu API OpenAI lặp đi
lặp lại, chúng tôi kết hợp một tiêu chí dừng sớm dừng
quá trình nếu hiệu suất validation⁹ không cải thiện trong
10 lần lặp. Thực tế, hầu như tất cả các nhiệm vụ dừng
trước 30 lần lặp. Theo đó, chúng tôi chọn hướng dẫn với
hiệu suất validation tốt nhất để đánh giá trên các ví dụ test.

⁷Trong việc triển khai iPrompt gốc, meta-prompt kết thúc
với hậu tố Prompt:. Tuy nhiên, điều này dẫn đến việc tạo
hướng dẫn không mạch lạc trên các benchmark của chúng tôi.
Do đó, chúng tôi đã thay đổi nó thành Instruction: điều này
giải quyết vấn đề.
⁸www.github.com/csinva/imodelsX/tree/master/
imodelsx/iprompt
⁹Điểm ROUGE-L trung bình giữa đầu ra dự đoán của LLM
và ground-truth trên dữ liệu validation.

Theo codebase gốc, chúng tôi sử dụng meta-prompt được
hiển thị trong Hình 7 cho việc tạo hướng dẫn với iPrompt.
Vì meta-prompt này không sử dụng bất kỳ mô tả nhiệm
vụ nào, để so sánh công bằng, chúng tôi triển khai một
baseline iPrompt+ với meta-prompt tương tự như phương
pháp của chúng tôi sử dụng hướng dẫn seed, như được
hiển thị trong Hình 8. Độc giả có thể tham khảo bài báo
gốc (Singh et al., 2022) để biết chi tiết kỹ thuật của iPrompt.

[Hình 7: Meta-prompt của việc tạo hướng dẫn với iPrompt.]
[Hình 8: Meta-prompt của việc tạo hướng dẫn với iPrompt+,
tương tự như của chúng tôi trong Hình 10.]

## Phụ lục D Kết quả Thí nghiệm Bổ sung
Trong phần này, chúng tôi trình bày thêm kết quả thí
nghiệm bổ sung cho những cái được phân tích trong Phần 5.
Tất cả các thí nghiệm trong phần này được tiến hành
trong thiết lập few-shot trừ khi được nêu khác.

### D.1 Kết quả SuperNI theo Danh mục Nhiệm vụ
Ở đây, chúng tôi trình bày kết quả thí nghiệm chi tiết
trên 8 danh mục khác nhau của các nhiệm vụ test SuperNI
(xem Phụ lục G cho danh sách các nhiệm vụ test). Như
được hiển thị trong Hình 9, Auto-Instruct vượt trội các
hướng dẫn do con người viết và ngẫu nhiên không matter
nó được đánh giá trên các nhiệm vụ phân loại, trích xuất
hay tạo, với ngoại lệ duy nhất là phân loại answerability.
Đáng chú ý, Auto-Instruct vượt trội hướng dẫn do con
người viết gốc 10%, 9% và 8% trên phân loại commonsense
(nhiệm vụ phân loại), word analogy (nhiệm vụ tạo ngắn)
và dialogue generation (nhiệm vụ tạo dài), tương ứng.

[Hình 9: Hiệu suất few-shot của các hướng dẫn được chọn
bởi Auto-Instruct (được ký hiệu là "Selected") trên tất
cả 8 danh mục của các nhiệm vụ test SuperNI, so với
hướng dẫn do con người viết và được chọn ngẫu nhiên.]

### D.2 Tổng quát hóa sang các LLM Khác
Bổ sung cho Phần 5.4.1, chúng tôi đánh giá thêm khả
năng tổng quát của Auto-Instruct sang các LLM mã nguồn
mở. Như được minh chứng trong Bảng 5, các hướng dẫn
được chọn bởi Auto-Instruct tăng cường hiệu suất của
LLaMA-2-chat (Touvron et al., 2023). Điều này một lần
nữa nhấn mạnh khả năng của Auto-Instruct tổng quát
trên các LLM khác nhau mà không cần huấn luyện lại
mô hình xếp hạng hướng dẫn. Đáng chú ý rằng chúng
tôi sử dụng các hướng dẫn được tạo bởi text-davinci-003
trong các thí nghiệm này, vì cả phiên bản 7B và 13B
của LLaMA-2-chat thể hiện khả năng yếu hơn trong việc
tuân theo các meta-prompt của chúng tôi cho việc tạo
hướng dẫn, trái ngược với các mô hình GPT cỡ mega.
Chúng tôi để lại nghiên cứu về việc tạo hướng dẫn với
các LLM mã nguồn mở gần đây như công việc tương lai.

[Bảng 5: Kết quả SuperNI của việc chuyển Auto-Instruct
sang LLaMA-2-chat-7B, sử dụng hướng dẫn được tạo bởi
text-davinci-003. Mô hình xếp hạng hướng dẫn vẫn là
cái được huấn luyện trên hướng dẫn text-davinci-003.]

### D.3 So sánh với Answer Ensemble
Cho rằng Auto-Instruct bao gồm lấy mẫu nhiều hướng
dẫn ứng viên trước khi chọn cái tốt nhất, chúng tôi so
sánh nó với một cách tiếp cận lấy mẫu khác, tức là,
lấy mẫu và ensemble nhiều câu trả lời. Sử dụng hướng
dẫn do con người viết gốc, chúng tôi lấy mẫu phản hồi
10 lần với nucleus sampling (Holtzman et al., 2020),
mà không lấy mẫu nhiều hướng dẫn. Sau đó, chúng tôi
ensemble tất cả 10 phản hồi bằng cách marginalize xác
suất LM của mỗi phản hồi duy nhất trước khi chọn cái
có xác suất nhất, tương tự như ý tưởng của self-consistency
(Wang et al., 2023b). Kết quả, được hiển thị trong Bảng 6,
chỉ ra rằng cách tiếp cận answer ensemble chỉ mang lại
cải thiện nhỏ trên SuperNI, không thể so sánh với lợi
ích hiệu suất đạt được với Auto-Instruct.

[Bảng 6: Kết quả của multi-answer ensemble được khuyến
khích bởi hướng dẫn do con người viết trên các nhiệm
vụ test SuperNI.]

## Phụ lục E Meta-Prompt cho Việc tạo Hướng dẫn
Trong phần này, chúng tôi liệt kê tất cả các meta-prompt
được sử dụng trong quá trình tạo hướng dẫn, như được
phác thảo trong §4.1. Đối với thiết lập zero-shot, chúng
tôi bỏ qua trường "Examples" trong meta-prompt để cho
LLM diễn đạt lại hướng dẫn seed. Bên cạnh đó, meta-prompt
với giải thích cho các minh họa không áp dụng được trong
thiết lập zero-shot. Meta-prompt sử dụng các nhiệm vụ
khác như minh họa (Hình 10e) được tích hợp với ba nhóm
nhiệm vụ minh họa, mỗi nhóm khác nhau về độ dài hướng
dẫn trung bình. Do đó, LLM được khuyến khích tạo ra
các hướng dẫn có độ chi tiết tương tự như các nhiệm vụ
minh họa. Các nhiệm vụ minh họa được lấy mẫu từ SuperNI.
Trong SuperNI, mỗi nhiệm vụ được ghép nối với một tóm
tắt nhiệm vụ ngắn gọn và một định nghĩa nhiệm vụ chi
tiết thường dài hơn nhiều. Đối với mỗi nhiệm vụ minh
họa, chúng tôi sử dụng tóm tắt nhiệm vụ như hướng dẫn
seed và định nghĩa nhiệm vụ như hướng dẫn mục tiêu.
Chúng tôi không sử dụng định nghĩa nhiệm vụ trong các
nhiệm vụ test vì (1) một số định nghĩa nhiệm vụ quá
dài để fit trong mô hình T5 cùng với đầu vào (2) chúng
tôi thực tế thấy rằng LLM có xu hướng lặp lại định nghĩa
nhiệm vụ ở mức độ lớn nếu nó được sử dụng như hướng
dẫn seed. Mặc dù Auto-Instruct chưa bao giờ thấy định
nghĩa nhiệm vụ dài hơn nhiều của các nhiệm vụ test,
hướng dẫn được chọn của chúng tôi vẫn hoạt động tốt
hơn so với việc sử dụng định nghĩa nhiệm vụ như hướng
dẫn, có điểm trung bình 62.41 trên SuperNI trong thiết
lập few-shot. Chúng tôi để lại việc khám phá tích hợp
các hướng dẫn phức tạp hơn như công việc tương lai.

[Hình 10: Meta-prompt mà chúng tôi sử dụng để chỉ định
các phong cách mong muốn khác nhau của hướng dẫn trong
quá trình tạo hướng dẫn. Cho Hình 10e, chúng tôi thu
thập 3 nhóm nhiệm vụ minh họa với độ dài hướng dẫn
trung bình khác nhau (ngắn, trung bình, dài), để hướng
dẫn LLM tạo ra các hướng dẫn với độ chi tiết khác nhau.]

## Phụ lục F Nghiên cứu Trường hợp Bổ sung
Trong phần này, chúng tôi cung cấp 3 trường hợp nữa
(2 few-shot và 1 zero-shot) nơi Auto-Instruct cải thiện
các hướng dẫn do con người viết gốc. Các nghiên cứu
trường hợp này được hiển thị trong Hình 11, 12, và 13.
Vui lòng tham khảo các chú thích tương ứng để biết
giải thích trường hợp chi tiết.

[Hình 11-13: Các nghiên cứu trường hợp bổ sung với
giải thích chi tiết]

## Phụ lục G Tất cả Các Nhiệm vụ Test
Trong Bảng 7, chúng tôi liệt kê tất cả 91 nhiệm vụ test
SuperNI được sử dụng trong các thí nghiệm ngoài miền
của chúng tôi. Vì kích thước của các nhiệm vụ không
cân bằng trên SuperNI, để đánh giá hiệu quả, chúng
tôi ngẫu nhiên lấy mẫu 200 instance từ mỗi nhiệm vụ,
tạo tổng cộng 18,200 ví dụ test.

[Bảng 7: Tất cả các nhiệm vụ test SuperNI, được nhóm
thành các danh mục khác nhau. Các danh mục nhiệm vụ
này không được thấy trong quá trình huấn luyện mô hình
xếp hạng hướng dẫn. Bên cạnh đó, bất kỳ nhiệm vụ nào
được lấy nguồn từ cùng dataset gốc như bất kỳ nhiệm
vụ test nào đều được loại trừ khỏi huấn luyện.]
