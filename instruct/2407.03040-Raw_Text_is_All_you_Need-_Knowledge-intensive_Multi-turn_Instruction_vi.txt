Models LLM Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct GPT-3.5 3.24 3.66 3.66 3.67 3.24 3.65 3.65 2.98 21.18 3.20
CoT (Wei et al., 2022) GPT-3.5 2.95 3.67 3.67 4.03 3.72 3.67 3.70 3.46 19.50 3.42
CoD (phương pháp của chúng tôi) GPT-3.5 3.96 4.39 4.39 4.39 4.20 4.42 4.42 4.29 40.55 4.05
CoD (phương pháp của chúng tôi) GLLM (Llama-3-8B) 3.92 4.36 4.36 4.27 4.16 4.37 4.36 4.25 35.42 3.98
CoD (phương pháp của chúng tôi) GLLM (Qwen-2-7B) 3.93 4.34 4.34 4.29 4.19 4.40 4.38 4.25 36.74 3.99
Direct Qwen 3.37 4.16 4.16 4.17 4.13 4.07 4.16 3.91 33.07 3.75
CoT (Wei et al., 2022) Qwen 3.42 4.16 4.16 4.17 4.13 4.12 4.16 3.95 35.07 3.78
CoD (phương pháp của chúng tôi) Qwen 3.80 4.36 4.36 4.33 4.20 4.36 4.40 4.30 51.69 4.07
Direct Deepseek 3.53 4.11 4.11 4.16 4.11 4.10 4.18 4.02 21.71 3.71
CoT (Wei et al., 2022) Deepseek 3.51 4.19 4.19 4.20 4.13 4.16 4.22 4.01 34.24 3.81
CoD (phương pháp của chúng tôi) Deepseek 3.81 4.38 4.38 4.34 4.20 4.40 4.41 4.34 36.29 4.00
RefGPT (Yang et al., 2023) - 3.66 4.33 4.33 4.28 4.20 4.23 4.31 4.04 45.57 3.96

Bảng 3: Kết quả đánh giá dữ liệu SFT đa lượt được tạo ra trong Khoa học. Các cuộc đối thoại gốc từ RefGPT (Yang et al., 2023) được đánh giá bằng cùng các chỉ số và phương pháp chấm điểm để so sánh.

• Đánh giá GLLM: chúng tôi kiểm tra mức độ hiệu quả của GLLM có thể tạo ra dữ liệu tinh chỉnh có giám sát (SFT) đa lượt với kiến thức tài liệu được tiêm vào

• Đánh giá LLM được Tinh chỉnh: chúng tôi nhằm đảm bảo rằng các câu trả lời được tạo ra bởi LLM được huấn luyện trên dữ liệu đối thoại đa lượt được tạo ra bởi GLLM cũng thể hiện mức độ cao về tính mạch lạc, liên quan ngữ cảnh, và độ chính xác thực tế.

5.1 Thiết lập Thí nghiệm

LLM Cơ sở Để xây dựng GINSTRUCT và K-BENCH, chúng tôi sử dụng GPT-3.5 Turbo (OpenAI, 2022), Deepseek V2 Chat (DeepSeek-AI et al., 2024), và Qwen-2-72B-Instruct (Bai et al., 2023). Dựa trên các LLM được huấn luyện trước với các kích thước mô hình khác nhau, chúng tôi sử dụng Llama-2-14B, Llama-3-8B, và Qwen-2-1.5B/7B như các mô hình nền tảng để xây dựng GLLM. Để đánh giá dựa trên mô hình, chúng tôi sử dụng GPT-4 Turbo (GPT-4) (OpenAI, 2023) cho tiêu chuẩn cao.

Benchmark Đánh giá Chúng tôi áp dụng cả GINSTRUCT và K-BENCH được tạo ra bởi GPT-3.5 cho huấn luyện và đánh giá, mặc dù nhiều biến thể đã được tạo ra cho mục đích thí nghiệm. Thống kê dữ liệu có thể được tìm thấy trong Bảng 1.

Chi tiết Triển khai Các mô hình từ tất cả thí nghiệm được huấn luyện trong 2 epoch với bộ lập lịch cosine, bắt đầu với tốc độ học 2e-5 (3% bước khởi động). Chúng tôi sử dụng AdamW (Loshchilov và Hutter, 2017) làm bộ tối ưu và kích thước batch 512 (độ dài tối đa 4096). Chúng tôi áp dụng chỉ số đánh giá được giới thiệu trong Phần 3.2.

5.2 Kết quả Chính

Đánh giá K-BENCH Để xác minh chất lượng của K-BENCH và tiếp tục đánh giá hiệu quả của CoD được đề xuất so với CoT và các phương pháp nhắc nhở hướng dẫn trực tiếp, chúng tôi sử dụng GPT-4 Turbo làm giám khảo để đánh giá K-BENCH. GPT-4 Turbo được giao nhiệm vụ chấm điểm mỗi lượt đối thoại dựa trên các chỉ số đánh giá và tiêu chuẩn chấm điểm được mô tả trong 3.2. Chúng tôi áp dụng GPT-3.5 Turbo (GPT 3.5), Deepseek V2 Chat (Deepseek), và Qwen-2-72B-Instruct (Qwen) như các biến thể khác nhau của văn bản thô (tài liệu) cho các bộ tạo dữ liệu SFT đa lượt. Kết quả thí nghiệm trên ba loại dữ liệu SFT được tạo ra khác nhau: hiện vật, khoa học, và wikipedia được hiển thị trong Bảng 2, 3, và 4 tương ứng. Bằng cách kiểm tra kết quả, chúng tôi thấy rằng các LLM với thiết kế CoD được đề xuất của chúng tôi vượt trội hơn các mô hình khác trên tất cả các chỉ số, ngoại trừ chỉ số tỷ lệ bao phủ (CR) được đánh giá trên các cuộc đối thoại gốc được tạo ra bởi RefGPT (Yang et al., 2023), sử dụng cùng tiêu chuẩn đánh giá trong Bảng 3. Yang et al. (2023) đã thiết kế các phương pháp tiêm kiến thức cụ thể để đảm bảo dữ liệu đối thoại họ xây dựng bao gồm hầu hết các sự kiện từ tài liệu, trong khi phương pháp của chúng tôi không chỉ đảm bảo độ chính xác thực tế cao mà còn thể hiện mức độ cao về tính mạch lạc và liên quan ngữ cảnh. Hơn nữa, dữ liệu SFT được tạo ra bởi Qwen với thiết kế CoD vẫn vượt trội hơn RefGPT về chỉ số CR, điều này chứng minh tiềm năng của phương pháp chúng tôi đạt được giới hạn cao trên các LLM backbone khác nhau. Vì bộ dữ liệu GINSTRUCT được đề xuất để huấn luyện GLLM được tạo ra bằng cùng phương pháp như tạo ra K-BENCH, kết quả đánh giá cũng chỉ ra rằng bộ dữ liệu GINSTRUCT thể hiện chất lượng cao trên các chỉ số khác nhau.

Đánh giá GLLM Trong Bảng 2, 3, và 4, chúng tôi cũng trình bày kết quả của GLLM được khởi tạo trên cả Llama-3-8B và Qwen-2-7B. Cụ thể, chúng tôi đánh giá trên dữ liệu SFT được tạo ra bởi hai GLLM này sử dụng cùng các chỉ số và tiêu chuẩn được giới thiệu trong "Đánh giá K-BENCH". Kết quả đánh giá của dữ liệu SFT được tạo ra bởi GPT-3.5 chỉ cao hơn một chút so với dữ liệu được tạo ra bởi GLLM, điều này cho thấy rằng các GLLM được khởi tạo trên cả hai LLM đã đạt được mức độ cao về khả năng tạo dữ liệu từ GPT-3.5 với CoD. Bằng cách tận dụng logic chuỗi đối thoại và các tài liệu chuyên sâu về kiến thức, dữ liệu SFT được tạo ra bởi GLLM vượt trội hơn các phương pháp dựa trên truy xuất truyền thống và phản hồi trực tiếp, thiết lập nền tảng mới cho việc điều chỉnh hướng dẫn chuyên sâu về kiến thức trong các mô hình ngôn ngữ lớn.

Đánh giá LLM được Tinh chỉnh Để đánh giá hiệu quả của các LLM được tinh chỉnh trên dữ liệu SFT được tạo ra bởi GLLM, chúng tôi tiến hành thí nghiệm so sánh các bộ tạo khác nhau, LLM được tinh chỉnh, và cài đặt mô hình trên 3 nguồn dữ liệu riêng biệt: hiện vật, RefGPT và SquadV2. Chúng tôi kiểm tra các câu trả lời được tạo ra bởi Qwen-2-7B và Llama3-8B được tinh chỉnh trên dữ liệu đối thoại đa lượt được tạo ra bởi GPT-3.5 và GLLM, tương ứng. Để tiếp tục điều tra hiệu quả của CoD, chúng tôi so sánh các LLM được tinh chỉnh trên dữ liệu huấn luyện được tạo ra bởi các bộ tạo có và không có hướng dẫn CoD (được gọi là mô hình Direct trong 5). Kết quả thí nghiệm được trình bày trong Bảng 5. Chúng tôi thấy rằng tất cả các mô hình CoD đều vượt trội hơn các mô hình trực tiếp không có CoD, chứng minh hiệu quả của thiết kế chuỗi logic của chúng tôi. Các LLM được huấn luyện trên dữ liệu được tạo ra bởi các bộ tạo GLLM chỉ kém hiệu suất một chút so với những LLM được huấn luyện trên dữ liệu được tạo ra bởi GPT-3.5. Điều này cũng phù hợp với kết quả từ thí nghiệm đánh giá GLLM, nơi GLLM đã thu được hầu hết khả năng tạo đối thoại từ GPT-3.5 giới hạn trên của nó.

Models LLM Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct GPT-3.5 3.26 3.64 3.64 3.67 3.31 3.59 3.59 3.16 22.66 3.22
CoT (Wei et al., 2022) GPT-3.5 3.24 3.95 3.95 4.10 3.96 3.96 4.01 3.73 28.73 3.59
CoD (phương pháp của chúng tôi) GPT-3.5 3.87 4.30 4.30 4.34 4.11 4.35 4.35 4.30 73.03 4.17
CoD (phương pháp của chúng tôi) GLLM (Llama-3-8B) 3.86 4.28 4.28 4.25 4.07 4.32 4.32 4.01 66.28 4.07
CoD (phương pháp của chúng tôi) GLLM (Qwen-2-7B) 3.87 4.26 4.26 4.29 4.10 4.34 4.34 4.10 69.44 4.11
Direct Qwen 3.49 4.15 4.15 4.16 4.12 4.08 4.21 4.01 43.92 3.83
CoT (Wei et al., 2022) Qwen 3.58 4.25 4.25 4.26 4.10 4.24 4.29 4.06 60.29 4.00
CoD (phương pháp của chúng tôi) Qwen 3.75 4.36 4.36 4.20 4.16 4.36 4.38 4.31 68.01 4.14
Direct Deepseek 3.49 4.11 4.10 4.12 4.10 4.11 4.18 4.05 33.79 3.77
CoT (Wei et al., 2022) Deepseek 3.64 4.32 4.32 4.29 4.10 4.31 4.33 4.09 62.25 4.05
CoD (phương pháp của chúng tôi) Deepseek 3.69 4.33 4.33 4.27 4.17 4.31 4.34 4.29 65.13 4.10

Bảng 4: Kết quả đánh giá dữ liệu SFT đa lượt được tạo ra trong Wikipedia.

5.3 Phân tích

Đánh giá Con người Để đảm bảo tính mạnh mẽ của việc đánh giá dựa trên GPT-4, chúng tôi lấy mẫu 100 ví dụ từ K-BENCH được tạo ra bởi GPT-3.5 và tiến hành đánh giá của con người. Các đánh giá viên con người được cung cấp các chỉ số đánh giá và hướng dẫn chi tiết để chấm điểm mỗi lượt đối thoại từ 1 đến 5. Các điểm số được tổng hợp để cung cấp đánh giá tổng thể về chất lượng bộ dữ liệu. Các đánh giá viên được khuyến khích cung cấp phản hồi định tính về tính tự nhiên và độ chính xác thực tế của các cuộc đối thoại. Ngoài ra, chúng tôi thực hiện phân tích tương quan giữa các điểm số được gán bởi GPT-4 và các đánh giá viên con người. Cụ thể, chúng tôi áp dụng hệ số tương quan Pearson và Spearman để đo mức độ liên kết giữa các đánh giá tự động và của con người. Hệ số tương quan Pearson và Spearman giữa GPT-4 và đánh giá của con người lần lượt là 0.89 và 0.87, cho thấy sự phù hợp mạnh mẽ giữa các đánh giá tự động và của con người.

Hiệu quả của R2S Hình 3 là một nghiên cứu trường hợp so sánh các phản hồi đối thoại được tạo ra bởi các mô hình trực tiếp và R2S. Bằng cách tích hợp logic chuỗi đối thoại, GLLM hiệu quả duy trì độ chính xác thực tế và tạo ra các cuộc đối thoại mạch lạc, phù hợp ngữ cảnh mô phỏng các tương tác giống con người. Phương pháp như vậy không chỉ cải thiện hiệu suất của các cuộc đối thoại được tạo ra mà còn cung cấp một khung làm việc mạnh mẽ cho việc điều chỉnh hướng dẫn của các mô hình ngôn ngữ lớn sử dụng tài liệu văn bản thô. Nhờ thiết kế CoD: 1) các cuộc đối thoại được tạo ra bởi GLLM có thông tin hơn và chi tiết hơn đáng kể, cung cấp cho người dùng các phản hồi toàn diện, 2) sự hiểu biết và mạch lạc của các cuộc đối thoại được cải thiện rõ rệt, với các phản hồi logic theo sau các truy vấn của người dùng, và 3) các cuộc đối thoại thể hiện sự trung thành cao hơn với các tài liệu tham khảo, đảm bảo độ chính xác thực tế và giảm sự xuất hiện của ảo giác.

Hình 3: So sánh các phản hồi đối thoại được tạo ra bởi các mô hình trực tiếp và R2S

6 Công trình Liên quan

Mô hình Ngôn ngữ Lớn Các mô hình ngôn ngữ lớn (LLM) (Touvron et al., 2023a,b; Frantar et al., 2022; Bai et al., 2023; Du et al., 2021; Rozière et al., 2023), tận dụng kiến trúc Transformer, đại diện cho một bước nhảy vọt đáng kể trong xử lý ngôn ngữ tự nhiên (NLP) (Li et al., 2022, 2023b; Qin et al., 2024a). LLM trải qua huấn luyện nghiêm ngặt trên các bộ dữ liệu văn bản rộng lớn, cho phép chúng nắm bắt một loạt các sắc thái và ngữ cảnh ngôn ngữ. LLM theo một quy trình hai giai đoạn bao gồm huấn luyện trước trên

--- TRANG 8 ---

Models Dataset Fine-tuned LLM Generator Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct Artifacts Qwen-2-7B GPT-3.5 3.90 4.30 4.28 3.89 4.06 4.45 4.45 4.44 67.61 4.12
Direct Artifacts Llama-3-8B GPT-3.5 3.38 4.09 4.05 3.67 3.97 4.19 4.16 3.80 59.47 3.80
CoD Artifacts Qwen-2-7B GPT-3.5 3.93 4.31 4.30 3.98 4.11 4.45 4.45 4.45 80.02 4.22
CoD Artifacts Llama-3-8B GPT-3.5 3.84 4.03 4.03 4.08 4.07 4.22 4.23 4.21 72.49 4.03
CoD Artifacts Qwen-2-7B GLLM 3.89 4.32 4.30 3.91 4.04 4.42 4.43 4.45 75.33 4.16
CoD Artifacts Llama-3-8B GLLM 3.75 3.98 3.98 3.89 4.01 4.19 4.21 4.19 64.57 3.93
Direct RefGPT Qwen-2-7B GPT-3.5 3.85 4.21 4.20 3.65 3.95 4.36 4.36 4.38 31.41 3.83
Direct RefGPT Llama-3-8B GPT-3.5 3.54 3.88 3.88 3.57 3.77 3.94 3.94 3.95 28.55 3.54
CoD RefGPT Qwen-2-7B GPT-3.5 3.87 4.24 4.24 3.76 4.04 4.39 4.40 4.40 57.23 4.02
CoD RefGPT Llama-3-8B GPT-3.5 3.60 3.97 3.97 3.69 3.87 3.99 4.00 3.99 52.84 3.74
CoD RefGPT Llama-3-8B GLLM 3.50 3.91 3.91 3.66 3.85 3.93 3.92 3.98 45.11 3.65
CoD RefGPT Qwen-2-7B GLLM 3.81 4.22 4.22 3.69 3.95 4.34 4.34 4.41 42.38 3.89
Direct SquadV2 Qwen-2-7B GPT-3.5 3.86 4.10 4.10 3.64 3.92 4.23 4.20 4.04 53.14 3.86
Direct SquadV2 Llama-3-8B GPT-3.5 3.80 3.56 3.56 3.26 3.40 3.91 3.82 3.76 46.32 3.48
CoD SquadV2 Qwen-2-7B GPT-3.5 3.87 4.10 4.10 3.67 3.95 4.23 4.20 4.07 61.84 3.92
CoD SquadV2 Llama-3-8B GPT-3.5 3.76 3.79 3.79 3.32 3.49 4.00 3.88 3.84 48.98 3.59
CoD SquadV2 Llama-3-8B GLLM 3.58 3.75 3.75 3.30 3.49 3.89 3.74 3.86 58.01 3.58
CoD SquadV2 Qwen-2-7B GLLM 3.80 4.01 4.01 3.74 3.94 4.27 4.17 4.12 51.73 3.84

Bảng 5: Kết quả đánh giá của các LLM khác nhau được tinh chỉnh trên dữ liệu được tạo ra. Direct biểu thị bộ tạo được huấn luyện với hướng dẫn trực tiếp để tạo ra đối thoại, trong khi CoD đề cập đến huấn luyện với thiết kế logic chuỗi đối thoại. Fine-tuned LLM là mô hình đối thoại được huấn luyện trên dữ liệu được tạo ra.

các kho ngữ liệu quy mô lớn tiếp theo là điều chỉnh hướng dẫn cho các nhiệm vụ cụ thể, cải thiện đáng kể hiệu suất trên các thách thức hiểu và tạo ra downstream. Đáng chú ý, loạt GPT, bắt đầu từ GPT-1 và phát triển qua GPT-4, (Radford et al., 2018; Black et al., 2022; Brown et al., 2020; OpenAI, 2023) thể hiện sự gia tăng tiến triển trong độ phức tạp và dung lượng mô hình, với GPT-3 bao gồm số lượng tham số đáng kinh ngạc 175 tỷ. Việc giới thiệu điều chỉnh hướng dẫn tiếp tục khuếch đại khả năng của LLM, mở khóa các khả năng nổi bật cho các nhiệm vụ lý luận phức tạp, như toán học và mã. LLM với điều chỉnh hướng dẫn thu hút sự chú ý từ các nhà nghiên cứu và tạo ra tác động đáng kể trên các kịch bản ngành công nghiệp khác nhau.

Điều chỉnh Hướng dẫn LLM tinh chỉnh khả năng tuân theo và hiểu lệnh của người dùng chính xác hơn được tinh chỉnh trên một bộ dữ liệu hướng dẫn (Ouyang et al., 2022; Zan et al., 2023; Qin et al., 2024b), bao gồm các hướng dẫn khác nhau và các đầu ra mong muốn tương ứng của chúng. Nghiên cứu sớm trong việc xây dựng các bộ dữ liệu đối thoại phần lớn dựa vào các bộ được chú thích thủ công (ví dụ QuAC (Choi et al., 2018) và CoQA (Reddy et al., 2019)), nhưng quy mô hạn chế và chi phí chú thích cao hạn chế hiệu suất mô hình và khả năng tổng quát hóa. Các phương pháp dựa trên mô phỏng được áp dụng để tạo ra các cuộc đối thoại tổng hợp thông qua việc bắt chước các tương tác người dùng-hệ thống, do đó giảm sự phụ thuộc vào chú thích thủ công. Những tiến bộ gần đây (Sun et al., 2023) nhấn mạnh khả năng của LLM như GPT-4 trong việc tự động tạo ra các bộ dữ liệu rộng lớn, chất lượng cao như bộ dữ liệu SODA (Kim et al., 2023), với 1100 triệu phát ngôn và 3 tỷ token. Những công trình này làm nổi bật khả năng tạo dữ liệu của LLM và tầm quan trọng của sự can thiệp của con người trong việc tăng cường độ chính xác của dữ liệu được tạo ra, ví dụ, bằng cách áp dụng các bộ lọc cơ bản, an toàn, và thường thức cho các cuộc đối thoại được tạo ra bởi GPT-3.5 (Chiang và Lee, 2023; Lotfi et al., 2023).

7 Kết luận

Trong bài báo này, chúng tôi giới thiệu R2S, một khung làm việc để xây dựng bộ dữ liệu tinh chỉnh có giám sát (SFT) từ tài liệu thô sử dụng logic chuỗi đối thoại (CoD) để hướng dẫn LLM tạo ra các cuộc đối thoại đa lượt chuyên sâu về kiến thức cho việc điều chỉnh hướng dẫn. Bằng cách tổng hợp các tài liệu hiện có từ các trang web/bộ dữ liệu nguồn mở, chúng tôi thiết lập một benchmark toàn diện, K-BENCH, có các chủ đề trong Wikipedia (tiếng Anh), Khoa học (tiếng Trung), và Hiện vật (tiếng Trung). Sử dụng CoD, chúng tôi phân loại các lượt đối thoại (ví dụ, Trao đổi Ý kiến Q&A và Q&A Thông tin) và nhắc LLM xác định các cụm từ khóa để tạo ra các phản hồi có liên quan, do đó giữ lại kiến thức tài liệu thô trong các bộ dữ liệu hướng dẫn kiểu đối thoại (GINSTRUCT) và cho phép tinh chỉnh các LLM nguồn mở (GLLM) để chuyển đổi tài liệu thô thành các cuộc đối thoại đa lượt, tiếp tục làm phong phú mô hình SFT với kiến thức cụ thể theo lĩnh vực. Các thí nghiệm mở rộng trên K-BENCH xác nhận hiệu quả của R2S, cho thấy những cải thiện đáng kể trong hiệu suất trên các chỉ số khác nhau. Bộ tạo SFT GLLM xác nhận hiệu quả của CoD bằng cách tạo ra các cuộc đối thoại đa lượt hợp lý và mạch lạc.

--- TRANG 9 ---

Hạn chế

Do tài nguyên tính toán hạn chế, bộ dữ liệu tinh chỉnh có giám sát (SFT) được tạo ra bởi GLLM chỉ được đánh giá trên Qwen-2-7B và Llama-3-8B, trong khi nhiều mô hình ngôn ngữ lớn backbone khác nên được kiểm tra, đảm bảo tính mạnh mẽ của R2S để tiêm kiến thức tài liệu vào dữ liệu SFT. Bên cạnh đó, kịch bản huấn luyện quy mô lớn với các LLM mạnh mẽ hơn vẫn cần khám phá thêm.

Cân nhắc Đạo đức

Bộ dữ liệu được sử dụng để đánh giá trong bài báo này được thu thập từ các nguồn dữ liệu mở và đã được xác minh và sàng lọc thủ công để loại bỏ bất kỳ dữ liệu nào có rủi ro đạo đức và nội dung nhạy cảm. Điều này đảm bảo rằng nội dung tuân thủ các quy định và luật pháp hiện hành.

Tài liệu tham khảo

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, và Tianhang Zhu. 2023. Báo cáo kỹ thuật Qwen. arXiv preprint arXiv:2309.16609, abs/2309.16609.

Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. GPT-NeoX-20B: Một mô hình ngôn ngữ tự hồi quy nguồn mở. arXiv preprint arXiv:2204.06745.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Mô hình ngôn ngữ là người học ít mẫu. Advances in neural information processing systems, 33:1877–1901.

Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xinnian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, và Zhoujun Li. 2024. xcot: Điều chỉnh hướng dẫn đa ngôn ngữ cho lý luận chuỗi suy nghĩ đa ngôn ngữ. arXiv preprint arXiv:2401.07037, abs/2401.07037.

David Cheng-Han Chiang và Hung-yi Lee. 2023. Các mô hình ngôn ngữ lớn có thể là một thay thế cho đánh giá của con người không? Trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, trang 15607–15631. Association for Computational Linguistics.

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, và Luke Zettlemoyer. 2018. Quac: Trả lời câu hỏi trong ngữ cảnh. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, trang 2174–2184. Association for Computational Linguistics.

[Tiếp tục với danh sách tài liệu tham khảo đầy đủ...]

--- TRANG 10 ---

[Tiếp tục danh sách tài liệu tham khảo...]

quantization cho transformer tạo sinh được huấn luyện trước. arXiv preprint arXiv:2210.17323.

Pragya Katyayan và Nisheeth Joshi. 2022. Thiết kế và phát triển hệ thống trả lời câu hỏi miền mở dựa trên quy tắc trên bộ dữ liệu squad v2.0. CoRR, abs/2204.09659.

[Danh sách tài liệu tham khảo tiếp tục với tất cả các nguồn được dịch sang tiếng Việt...]

--- TRANG 11 ---

[Phần cuối của danh sách tài liệu tham khảo được dịch sang tiếng Việt...]
