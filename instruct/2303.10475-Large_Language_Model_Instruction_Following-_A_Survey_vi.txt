# Việc Tuân Theo Hướng Dẫn của Mô Hình Ngôn Ngữ Lớn: Một Khảo Sát về Tiến Bộ và Thách Thức

Renze Lou♠Kai Zhang♢và Wenpeng Yin♠
♠Đại học Bang Pennsylvania ♢Đại học Bang Ohio
{renze.lou, wenpeng}@psu.edu ;zhang.13253@osu.edu

## Tóm tắt

Ngữ nghĩa của nhiệm vụ có thể được thể hiện bằng một tập hợp các ví dụ đầu vào-đầu ra hoặc một đoạn hướng dẫn bằng văn bản. Các phương pháp học máy thông thường cho xử lý ngôn ngữ tự nhiên (NLP) chủ yếu dựa vào sự có sẵn của các tập hợp ví dụ cụ thể cho từng nhiệm vụ ở quy mô lớn. Hai vấn đề nảy sinh: thứ nhất, việc thu thập các ví dụ được gán nhãn cụ thể cho nhiệm vụ không áp dụng được cho các tình huống mà nhiệm vụ có thể quá phức tạp hoặc tốn kém để chú thích, hoặc hệ thống được yêu cầu xử lý một nhiệm vụ mới ngay lập tức; thứ hai, điều này không thân thiện với người dùng vì người dùng cuối có thể sẵn sàng cung cấp mô tả nhiệm vụ hơn là một tập hợp các ví dụ trước khi sử dụng hệ thống. Do đó, cộng đồng đang chú ý ngày càng tăng đến một paradigm tìm kiếm giám sát mới cho NLP: học cách tuân theo hướng dẫn nhiệm vụ, tức là tuân theo hướng dẫn. Mặc dù có tiến bộ ấn tượng, vẫn có một số phương trình nghiên cứu chưa được giải quyết mà cộng đồng đang gặp khó khăn. Bài báo khảo sát này cố gắng tóm tắt và cung cấp những hiểu biết sâu sắc về nghiên cứu hiện tại về tuân theo hướng dẫn, đặc biệt là bằng cách trả lời các câu hỏi sau: (i) Hướng dẫn nhiệm vụ là gì, và có những loại hướng dẫn nào? (ii) Làm thế nào để mô hình hóa hướng dẫn? (iii) Các tập dữ liệu tuân theo hướng dẫn phổ biến và các chỉ số đánh giá là gì? (iv) Những yếu tố nào ảnh hưởng và giải thích hiệu suất của hướng dẫn? (v) Những thách thức nào vẫn còn tồn tại trong việc tuân theo hướng dẫn? Theo hiểu biết của chúng tôi, đây là khảo sát toàn diện đầu tiên về tuân theo hướng dẫn.

## 1 Giới thiệu

Một mục tiêu của AI là xây dựng một hệ thống có thể hiểu và giải quyết các nhiệm vụ mới một cách phổ quát. Các ví dụ được gán nhãn (Hình 1 (a)), như là đại diện nhiệm vụ chính thống, tốn kém để có được ở quy mô lớn hoặc thậm chí không tồn tại trong một số trường hợp. Vậy có đại diện nhiệm vụ nào khác có thể đóng góp vào việc hiểu nhiệm vụ không? Hướng dẫn bằng văn bản cung cấp một chiều hướng giám sát khác để thể hiện ngữ nghĩa nhiệm vụ, thường chứa kiến thức trừu tượng và toàn diện hơn về nhiệm vụ mục tiêu so với các ví dụ được gán nhãn riêng lẻ. Như được thể hiện trong Hình 1 (b), với sự có sẵn của hướng dẫn nhiệm vụ, các hệ thống có thể được xây dựng nhanh chóng để xử lý các nhiệm vụ mới. Hiệu quả như vậy là rất mong muốn trong các ứng dụng thực tế, đặc biệt là khi các chú thích cụ thể cho nhiệm vụ khan hiếm. Quan trọng hơn, việc tuân theo hướng dẫn nghiêng về trí tuệ con người trong việc học các nhiệm vụ mới—một đứa trẻ nhỏ có thể giải quyết tốt một nhiệm vụ toán học mới bằng cách học từ hướng dẫn và một vài ví dụ của nó. Kết quả là, paradigm học tập mới này gần đây đã thu hút sự chú ý chính của cộng đồng học máy và NLP.

Khi nói về "hướng dẫn", hầu hết chúng ta sẽ nghĩ đến "prompt" đầu tiên—sử dụng một mẫu ngắn gọn để chuyển đổi đầu vào nhiệm vụ thành định dạng mới (ví dụ: câu hỏi điền chỗ trống) phù hợp với mục tiêu mô hình hóa ngôn ngữ của các mô hình ngôn ngữ lớn (LLM). Mặc dù prompt phổ biến trong phân loại văn bản, dịch máy, v.v., chúng tôi lập luận rằng prompt chỉ là một trường hợp đặc biệt của hướng dẫn. Bài báo này có cái nhìn toàn diện và rộng lớn hơn về nghiên cứu NLP dựa trên hướng dẫn. Đặc biệt, chúng tôi cố gắng trả lời các câu hỏi sau: (i) hướng dẫn nhiệm vụ là gì, và có những loại hướng dẫn nào? (§ 4) (ii) với một hướng dẫn nhiệm vụ, làm thế nào để mã hóa nó để hỗ trợ việc khái quát hóa mô hình trên nhiệm vụ mục tiêu? (§ 5) (iii) các tập dữ liệu tuân theo hướng dẫn phổ biến và các chỉ số đánh giá chính thống là gì? (§ 6) (iv) những yếu tố nào (ví dụ: kích thước mô hình, số lượng nhiệm vụ) ảnh hưởng đến hiệu suất của hệ thống dựa trên hướng dẫn? (§ 7) (v) những thách thức nào tồn tại trong việc tuân theo hướng dẫn, và các hướng tương lai là gì? (§ 8)

Theo hiểu biết của chúng tôi, đây là bài báo đầu tiên khảo sát việc tuân theo hướng dẫn. Trái ngược với một số khảo sát hiện có tập trung vào một hướng dẫn trong ngữ cảnh cụ thể, như prompt, các minh chứng đầu vào-đầu ra, hoặc lý luận, công việc này cung cấp cái nhìn tổng quan toàn diện hơn về việc tuân theo hướng dẫn. Đóng góp của chúng tôi là ba khía cạnh:

• Vượt ra ngoài prompt, chúng tôi phân tích các ràng buộc của prompt qua lăng kính lấy người dùng làm trung tâm, tập trung vào việc phân biệt sự khác biệt giữa nghiên cứu tuân theo hướng dẫn hiện tại và nhu cầu thực tế.

• Chúng tôi diễn giải các hướng dẫn nhiệm vụ khác nhau từ góc nhìn thống nhất về giám sát gián tiếp, và tóm tắt những lợi thế, hạn chế và phạm vi ứng dụng của chúng;

• Chúng tôi coi các LLM ngày càng phát triển hiện tại và các tập dữ liệu hướng dẫn như một nỗ lực mở rộng quy mô theo hai hướng; thêm vào đó, chúng tôi chỉ ra các vấn đề nghiên cứu đáng chú ý hiện tại và các hướng hứa hẹn trong tương lai.

## 2 Công trình Liên quan

Về cơ bản có hai chủ đề liên quan chặt chẽ đến bài báo này, đó là tuân theo hướng dẫn (2.1) và Khảo sát về Hướng dẫn trong Ngữ cảnh (2.2).

### 2.1 Tuân theo Hướng dẫn

Như được minh họa trong Hình 1, không giống như học có giám sát dựa trên ví dụ truyền thống, bản chất của việc tuân theo hướng dẫn là huấn luyện các LLM để hiểu các hướng dẫn khác nhau và tạo ra các phản hồi tương ứng. Vì khả năng này có thể được mở rộng đến bất kỳ nhiệm vụ downstream nào chưa được nhìn thấy, việc tuân theo hướng dẫn đã trở thành một paradigm học tập hiệu quả để giải quyết các nhiệm vụ few/zero-shot. Tuy nhiên, hiệu suất của việc tuân theo hướng dẫn phụ thuộc nhiều vào cả quy mô mô hình và nhiệm vụ: một LLM lớn hơn (hoặc pre-training với nhiều token hơn) được điều chỉnh trên nhiều nhiệm vụ đa dạng hơn có thể đạt được hiệu suất few/zero-shot tốt hơn đáng kể trên các nhiệm vụ downstream. Vì việc mở rộng quy mô mô hình là không thực tế đối với hầu hết chúng ta, nhiều nghiên cứu gần đây đã tập trung vào việc thu thập các tập dữ liệu instruction-tuning chất lượng cao, bằng cách sử dụng lao động con người hoặc chưng cất giám sát từ các LLM mạnh mẽ.

Mặc dù phổ biến, việc tuân theo hướng dẫn hiện tại vẫn gặp phải những thách thức và có chỗ để phát triển đáng kể. Công việc này không chỉ khảo sát các tài liệu rộng lớn hiện có về việc tuân theo hướng dẫn mà còn đi xa hơn: chúng tôi truy tìm sự phát triển của việc tuân theo hướng dẫn trở lại thời kỳ đầu của việc phân tích cú pháp ngữ nghĩa dựa trên học máy, và xây dựng câu chuyện của chúng tôi từ góc nhìn giám sát gián tiếp. Chúng tôi hy vọng khảo sát này có thể giới thiệu một cách có hệ thống lĩnh vực phổ biến nhưng đầy thách thức này.

### 2.2 Khảo sát về Hướng dẫn trong Ngữ cảnh

Một số khảo sát hiện có có động lực tương tự với chúng tôi trong khi chỉ tập trung vào một số lĩnh vực con của việc tuân theo hướng dẫn, như prompt, các minh chứng few-shot, lý luận chuỗi suy nghĩ, v.v. Ví dụ, Liu et al. (2023a) đã cung cấp một cái nhìn tổng quan toàn diện về việc học prompt và LLM, trong đó prompt có thể được coi là một loại hướng dẫn văn bản cụ thể. Một số nghiên cứu khác khảo sát "soft instruction", tức là các phương pháp fine-tuning hiệu quả về tham số, cũng khác với phạm vi "textual instruction" của chúng tôi. Đáng chú ý, Zhang et al. (2023b) cũng đã đề xuất một khảo sát về instruction tuning, tuy nhiên, họ chủ yếu tập trung vào các tập dữ liệu và mô hình hiện có; trong khi chúng tôi trình bày một câu chuyện hoàn chỉnh và nhất quán hơn về việc tuân theo hướng dẫn, bao gồm các loại hướng dẫn, chiến lược mô hình hóa, v.v., mà các công trình trước đây chưa từng giới thiệu. Theo hiểu biết tốt nhất của chúng tôi, đây là công việc đầu tiên cung cấp một câu chuyện toàn diện và cấp cao về việc tuân theo hướng dẫn.

## 3 Sơ bộ

Đối với việc tuân theo hướng dẫn, chúng tôi nhắm đến việc thúc đẩy các hệ thống đạt được đầu ra tương ứng của đầu vào bằng cách tuân theo hướng dẫn. Do đó, chúng tôi giả định rằng một tập dữ liệu thường bao gồm ba mục:

• Đầu vào (X): đầu vào của một thể hiện; nó có thể là một đoạn văn bản duy nhất (ví dụ: phân loại cảm xúc) hoặc một nhóm các đoạn văn bản (ví dụ: suy luận văn bản, trả lời câu hỏi, v.v.).

• Đầu ra (Y): đầu ra của một thể hiện; trong các vấn đề phân loại, nó có thể là một hoặc nhiều nhãn được định nghĩa trước; trong các nhiệm vụ tạo văn bản, nó có thể là bất kỳ văn bản mở nào.

• Mẫu (T): một mẫu văn bản cố gắng thể hiện ý định nhiệm vụ hoặc được sử dụng để kết nối X và Y. T có thể chưa phải là một hướng dẫn.

Trong § 4, chúng tôi sẽ trình bày chi tiết rằng một hướng dẫn nhiệm vụ I thực sự là một sự kết hợp của T với X hoặc Y, hoặc chính T trong một số trường hợp.

## 4 Hướng dẫn Nhiệm vụ là gì?—Một Góc nhìn Thống nhất từ Giám sát Gián tiếp

Phần này đầu tiên tóm tắt ba loại hướng dẫn chính được xây dựng bởi các sự kết hợp khác nhau của T, X, và Y (như được minh họa trong Hình 2), sau đó trình bày cách diễn giải của chúng tôi về chúng thông qua góc nhìn giám sát gián tiếp.

### 4.1 Ba Loại Hướng dẫn

#### 4.1.1 Hướng dẫn Hướng NLI (tức là, I=T+Y)

Một lược đồ thông thường để xử lý các nhiệm vụ phân loại là chuyển đổi các nhãn mục tiêu thành các chỉ số và để các mô hình quyết định đầu vào thuộc về chỉ số nào. Paradigm này chỉ mã hóa ngữ nghĩa đầu vào trong khi mất đi ngữ nghĩa nhãn. Để cho phép các hệ thống nhận biết các nhãn mới mà không phụ thuộc vào các ví dụ được gán nhãn lớn, Yin et al. (2019) đã đề xuất chuyển đổi các nhiệm vụ phân loại mục tiêu thành suy luận ngôn ngữ tự nhiên (NLI) bằng cách xây dựng một giả thuyết cho mỗi nhãn—việc suy ra giá trị chân lý của một nhãn sau đó được chuyển đổi thành việc xác định giá trị chân lý của giả thuyết. Như được minh họa trong Hình 2 (a), phương pháp này xây dựng hướng dẫn (I) bằng cách kết hợp một mẫu (T) với một nhãn (Y) để giải thích ngữ nghĩa nhiệm vụ.

Những lợi thế của việc học Hướng dẫn Hướng NLI có bốn khía cạnh: (i) nó giữ lại ngữ nghĩa nhãn và làm cho việc mã hóa các mối quan hệ đầu vào-đầu ra trở nên khả thi; (ii) nó thống nhất các vấn đề phân loại khác nhau thành một nhiệm vụ NLI; (iii) bằng cách sử dụng giám sát gián tiếp từ các tập dữ liệu NLI hiện có, một mô hình được huấn luyện trên các nhiệm vụ NLI được kỳ vọng sẽ hoạt động trên các nhiệm vụ khác theo cách zero-shot; (iv) nó mở rộng vấn đề phân loại chỉ số tập đóng ban đầu thành một paradigm nhận biết nhãn miền mở.

#### 4.1.2 Hướng dẫn Hướng LLM (tức là, prompt; I=T+X)

Như được thể hiện trong Hình 2 (b) và Bảng 2, prompt là một đại diện của Hướng dẫn Hướng LLM, thường là một phát biểu ngắn gọn được thêm vào trước đầu vào nhiệm vụ (prefix prompt), hoặc một mẫu câu hỏi điền chỗ trống (cloze prompt). Về cơ bản, nó được thiết kế để truy vấn các phản hồi trung gian (có thể được chuyển đổi thành đầu ra cuối cùng) từ LLM. Vì đầu vào được prompt tuân theo các mục tiêu pre-training của LLM (ví dụ: đầu vào kiểu cloze thỏa mãn mục tiêu mô hình hóa ngôn ngữ có mặt nạ), nó giúp loại bỏ sự phụ thuộc vào việc fine-tuning có giám sát truyền thống và làm giảm đáng kể chi phí chú thích của con người. Do đó, việc học prompt đã đạt được kết quả ấn tượng trên rất nhiều nhiệm vụ NLP few/zero-shot trước đây, như trả lời câu hỏi, dịch máy, phân tích cảm xúc, suy luận văn bản, nhận dạng thực thể, v.v.

Mặc dù có hiệu suất xuất sắc của các kỹ thuật prompt, vẫn có hai nhược điểm rõ ràng với Hướng dẫn Hướng LLM trong các ứng dụng thực tế. (i) Không thân thiện với người dùng. Vì prompt được tạo ra để phục vụ LLM, nó được khuyến khích thiết kế prompt bằng "ngôn ngữ của mô hình" (ví dụ: từ không mạch lạc mà mô hình ưa thích hoặc embedding nội bộ). Tuy nhiên, phong cách hướng LLM này khó hiểu đối với người dùng và thường vi phạm trực giác con người. Trong khi đó, hiệu suất của prompt phụ thuộc nhiều vào việc kỹ thuật prompt tốn công sức, nhưng hầu hết người dùng cuối không phải là chuyên gia LLM và thường thiếu kiến thức đủ để điều chỉnh một prompt hiệu quả. (ii) Ràng buộc ứng dụng. Prompt thường ngắn và đơn giản, trong khi nhiều nhiệm vụ không thể được xây dựng hiệu quả chỉ với một prompt ngắn gọn, làm cho prompt khó xử lý các định dạng đa dạng của các nhiệm vụ NLP thực tế.

#### 4.1.3 Hướng dẫn Hướng Con người (tức là, I=T+ tùy chọn {Xi,Yi}ki=1)

Hướng dẫn Hướng Con người về cơ bản biểu thị các hướng dẫn được sử dụng cho crowd-sourcing trên các nền tảng chú thích của con người (ví dụ: Amazon MTurk). Không giống như Hướng dẫn Hướng LLM, Hướng dẫn Hướng Con người (Hình 2 (c)) thường là một số thông tin có thể đọc được bởi con người, mô tả và theo kiểu đoạn văn bao gồm các thành phần khác nhau, chẳng hạn như "tiêu đề nhiệm vụ", "danh mục", "định nghĩa" và "những điều cần tránh", v.v. Do đó, Hướng dẫn Hướng Con người thân thiện với người dùng hơn và có thể được áp dụng lý tưởng cho hầu hết bất kỳ nhiệm vụ NLP phức tạp nào.

Tương ứng, Hướng dẫn Hướng Con người đã thu hút nhiều sự chú ý hơn trong những năm gần đây. Tuy nhiên, do tính chất phức tạp, Hướng dẫn Hướng Con người thách thức hơn để mã hóa bởi các LLM vanilla. Ví dụ, GPT-2 nguyên bản được tìm thấy hoạt động kém khi tuân theo hướng dẫn MTurk. Để làm cho LLM hiểu Hướng dẫn Hướng Con người tốt hơn, các công việc tiếp theo bắt đầu thu thập các tập dữ liệu hướng dẫn quy mô lớn. Tất cả các kết quả trước đây cho thấy rằng, sau khi fine-tuning với các hướng dẫn nhiệm vụ khác nhau, các LLM text-to-text, như T5, OPT và Llama, đã đạt được khả năng khái quát hóa few/zero-shot đáng kể bằng cách tuân theo những hướng dẫn phức tạp này.

### 4.2 Một Góc nhìn Giám sát Gián tiếp

Mặc dù ba loại hướng dẫn rất khác nhau, chúng về cơ bản đều tìm kiếm cùng một thứ—giám sát gián tiếp—để đối phó với các nhiệm vụ mục tiêu có chú thích hạn chế.

Cụ thể, Hướng dẫn Hướng NLI biến đổi các vấn đề NLP mục tiêu thành một nhiệm vụ nguồn—NLI—để giám sát phong phú từ các tập dữ liệu NLI hiện có có thể hoạt động như giám sát gián tiếp cho những vấn đề mục tiêu đó. Hướng dẫn Hướng LLM định dạng lại các vấn đề mục tiêu thành nhiệm vụ nguồn—mô hình hóa ngôn ngữ, để kiến thức mục đích chung phong phú trong những LLM đó có thể được sử dụng trực tiếp để có được đầu ra. Dù là Hướng dẫn Hướng NLI hay Hướng dẫn Hướng LLM, cả hai đều cố gắng giải quyết các nhiệm vụ chưa thấy với một hệ thống có thể khái quát hóa. Tuy nhiên, cả hai đều có phạm vi ứng dụng hạn chế, ví dụ: chúng không thể xử lý hiệu quả một số nhiệm vụ dự đoán có cấu trúc. Thay vì tìm kiếm giám sát từ một nhiệm vụ nguồn duy nhất (NLI hoặc mô hình hóa ngôn ngữ), Hướng dẫn Hướng Con người học giám sát gián tiếp từ một tập hợp lớn các nhiệm vụ huấn luyện, do đó hệ thống kết quả có thể khái quát hóa lý tưởng cho bất kỳ nhiệm vụ văn bản chưa thấy nào.

## 5 Làm thế nào để Mô hình hóa Hướng dẫn?

Vì cả Hướng dẫn Hướng NLI và Hướng dẫn Hướng LLM đều liên quan đến đầu vào X hoặc đầu ra Y, những loại hướng dẫn này không yêu cầu thiết kế hệ thống cụ thể để mã hóa chúng. Hướng dẫn Hướng NLI có thể được xử lý bởi các hệ thống thông thường cho nhiệm vụ NLI, và Hướng dẫn Hướng LLM chủ yếu được cung cấp cho các LLM tự hồi quy. Trái lại, Hướng dẫn Hướng Con người là loại thách thức nhất vì nó độc lập với bất kỳ thể hiện được gán nhãn nào.

Do đó, phần này chủ yếu trình bày một số chiến lược mô hình hóa chính thống cho Hướng dẫn Hướng Con người, như được minh họa trong Hình 3.

### 5.1 Trình phân tích cú pháp Ngữ nghĩa

Ở giai đoạn đầu của học máy, để giúp các hệ thống hiểu hướng dẫn ngôn ngữ tự nhiên, rất nhiều công trình đã sử dụng phân tích cú pháp ngữ nghĩa để chuyển đổi hướng dẫn thành ngôn ngữ hình thức (công thức logic), có thể được thực thi dễ dàng hơn bởi các hệ thống. Như được minh họa trong Hình 3 (a), một hướng dẫn trò chơi "Di chuyển bất kỳ thẻ trên cùng nào đến một ô trống" có thể được xử lý thành một công thức có thể thực thi: "card(x) ∧ freecell(y)".

Nghiên cứu trước đây đã dành nhiều nỗ lực cho chiến lược này, trong đó hầu hết được sử dụng cho các nhiệm vụ tương tác người-máy, ví dụ: chơi trò chơi bóng đá. Để làm giảm các chú thích con người tốn công sức, các công việc tiếp theo đã tận dụng giám sát gián tiếp hoặc yếu từ các môi trường có căn cứ (ví dụ: cơ sở tri thức) để huấn luyện trình phân tích cú pháp ngữ nghĩa.

Hạn chế: Các phương pháp dựa trên trình phân tích cú pháp ngữ nghĩa chủ yếu áp dụng cho các nhiệm vụ riêng lẻ thay vì khái quát hóa chéo nhiệm vụ phổ quát, bởi vì việc xây dựng một trình phân tích cú pháp ngữ nghĩa đa năng cho tất cả các nhiệm vụ NLP là quá thách thức. Ngược lại, phương pháp được giới thiệu trong phần tiếp theo nhắm đến khái quát hóa chéo nhiệm vụ với giám sát hạn chế cho các nhiệm vụ mục tiêu.

### 5.2 Làm phẳng và Nối

Trái ngược với phương pháp trình phân tích cú pháp ngữ nghĩa, xem xét cấu trúc của hướng dẫn và các vấn đề mục tiêu, các phương pháp dựa trên mạng nơ-ron có cách xử lý thô sơ hơn: như được minh họa trong Hình 3 (b)—hướng dẫn, bất kể độ dài, cấu trúc, loại nhiệm vụ của chúng, v.v., được làm phẳng thành một chuỗi token dài và được nối với đầu vào X như một chuỗi đầu vào mới cho các mô hình, điều này đã được áp dụng rộng rãi bởi nghiên cứu trước đây. Tuy nhiên, chiến lược ngây thơ này liên tục dẫn đến hiệu suất không thỏa mãn khi sử dụng các mô hình vanilla, dẫn đến sự phụ thuộc vào instruction fine-tuning quy mô lớn, được biết đến rộng rãi là "instruction tuning".

Hạn chế: (i) Làm phẳng và nối mọi thứ thành một chuỗi dài có xu hướng bỏ qua một số thông tin quan trọng mà con người thường có thể nắm bắt trong hướng dẫn, chẳng hạn như phủ định (ví dụ: "không tạo ra đầu ra dài hơn 5 token"), cảnh báo (ví dụ: "tạo ra 'D' nếu câu hỏi không thể trả lời được hoặc bạn không chắc chắn"), ràng buộc đầu ra (ví dụ: "câu trả lời của bạn nên là một trong 'A', 'B', 'C', và 'D'"), v.v. (ii) Để cho phép các mô hình hiểu hướng dẫn, một số lượng lớn các nhiệm vụ huấn luyện phải được chuẩn bị. Điều này tương tự như những gì đã xảy ra trong những năm đầu của deep learning trong NLP: để cải thiện hiệu suất của mạng nơ-ron sâu cho một nhiệm vụ cụ thể, chúng ta thu thập thêm các ví dụ được gán nhãn; trở lại với việc tuân theo hướng dẫn, khả năng hiểu hướng dẫn của hệ thống, không may, vẫn thể hiện mức độ phụ thuộc cao vào quy mô của các nhiệm vụ huấn luyện.

### 5.3 HyperNetwork

Không giống như chiến lược mô hình hóa thông thường mã hóa chuỗi đầu vào thành biểu diễn dày đặc (tức là language-to-representation), hypernetwork theo một paradigm language-to-parameter: như được thể hiện trong Hình 3 (c), lược đồ này chuyển đổi hướng dẫn văn bản thành một khối tham số mô hình có thể được cắm vào các mô hình cơ bản. Kết quả là, mô hình hóa hướng dẫn dựa trên hypernetwork có thể tận dụng tốt hơn chuỗi đầu vào có cấu trúc bằng cách mã hóa hướng dẫn và đầu vào nhiệm vụ riêng biệt (tức là instruction-to-parameter, input-to-representation), đạt được khả năng khái quát hóa mạnh hơn so với phương pháp làm phẳng và nối. Nó cũng có thể cải thiện đáng kể hiệu quả suy luận, như được kết luận bởi các công trình gần đây.

Hạn chế: Mặc dù có những thuộc tính hấp dẫn của hypernetwork, tính không ổn định trong huấn luyện và sự phụ thuộc vào thiết kế kiến trúc (phù hợp với các mô hình cơ bản) là những rào cản trong các ứng dụng thực tế.

### 5.4 Học Tăng cường từ Phản hồi Con người

Hàm mất mát để huấn luyện LM ảnh hưởng đáng kể đến hiệu suất tuân theo hướng dẫn của LM kết quả. Tuy nhiên, hầu hết tất cả các chiến lược mô hình hóa đã nêu trước đây (trừ phương pháp dựa trên trình phân tích cú pháp ngữ nghĩa) áp dụng một hàm mất mát dự đoán token tiếp theo thông thường (ví dụ: cross entropy) để huấn luyện các mô hình, điều này cố gắng nắm bắt sở thích con người bằng cách đơn giản so sánh văn bản tạo ra của mô hình với tham chiếu sự thật cơ bản. Để tối ưu hóa trực tiếp LM với giám sát của sở thích con người, các công trình gần đây đã sử dụng học tăng cường từ phản hồi con người (RLHF) để huấn luyện LM.

LM ban đầu và được điều chỉnh: Bước đầu tiên của RLHF là có được một LM ban đầu, thường được huấn luyện với chiến lược mô hình hóa dựa trên làm phẳng và nối—nối hướng dẫn, đầu vào và tất cả các tài nguyên khác (nếu chúng tồn tại) thành một chuỗi đầu vào, và huấn luyện LM để tạo ra đầu ra sự thật cơ bản (như chúng tôi đã giới thiệu trước đây). Với LM ban đầu như một điểm bắt đầu, chúng ta có thể sao chép nó sang một tham số độc lập khác, đó là LM mục tiêu sẽ được cập nhật liên tục trong RLHF (tức là LM được điều chỉnh).

Hình phạt dịch chuyển dự đoán: Như được thể hiện trong Hình 3 (d), với LM ban đầu và bản sao của nó (LM được điều chỉnh mục tiêu), cho mỗi chuỗi đầu vào (ví dụ: hướng dẫn và đầu vào nhiệm vụ), hai LM khác nhau này sẽ tạo ra hai đầu ra. Sau khi có được văn bản tạo ra của LM ban đầu và được điều chỉnh, chúng ta có thể tính toán hình phạt khác biệt văn bản giữa chúng:

y=θ(I, x)
y∗=θ∗(I, x)
rKL=KL(y, y∗)

Ở đây, θ và θ∗ đại diện cho các tham số của LM ban đầu và được điều chỉnh, tương ứng. I và x biểu thị hướng dẫn và đầu vào nhiệm vụ; trong khi y và y∗ là các đầu ra của LM ban đầu và được điều chỉnh. rKL là phần thưởng cuối cùng (mất mát) của dịch chuyển dự đoán, và KL có nghĩa là tính toán phân kỳ Kullback–Leibler (KL). Phân kỳ KL là một chiến lược được áp dụng rộng rãi để đo lường sự khác biệt văn bản, có thể được sử dụng như một phần của mất mát để phạt LM được điều chỉnh vì dịch chuyển đầu ra đáng kể khỏi việc tạo ra của LM ban đầu. Hình phạt dịch chuyển dự đoán này có thể ngăn chặn LM được điều chỉnh lừa hàm phần thưởng để có được phần thưởng cao nhưng mất tính mạch lạc của văn bản tạo ra.

Hàm phần thưởng: Ngoài hình phạt dịch chuyển dự đoán, một phần khác của phần thưởng cuối cùng đến từ hàm phần thưởng. Hàm phần thưởng được sử dụng để đo lường trực tiếp mức độ đầu ra của mô hình phù hợp với sở thích con người—phần thưởng cao hơn có nghĩa là đầu ra phù hợp tốt hơn với sở thích con người.

Hàm phần thưởng là một mô hình khác được điều chỉnh trên dữ liệu sở thích con người, thường nhỏ hơn LM ban đầu. Nó nhận hướng dẫn, đầu vào nhiệm vụ và đầu ra của mô hình, và cố gắng dự đoán một giá trị phần thưởng để phản ánh sự phù hợp:

ralignment =rθ(I, x, y∗)

Ở đây rθ là mô hình phần thưởng, thường là một mô hình hồi quy.

Để huấn luyện một mô hình phần thưởng, các công trình trước đây đã thu thập một loạt dữ liệu sở thích con người. Ví dụ, Bai et al. (2022a) đã thu thập các đầu ra khác nhau cho mỗi hướng dẫn đầu vào, và yêu cầu các chú thích viên con người quyết định đầu ra nào phù hợp hơn với sở thích của họ. Mất mát huấn luyện của mô hình phần thưởng có thể được xây dựng như sau:

l=−log σ(rθ(I, x, y+)−rθ(I, x, y−))

Trong đó σ là hàm kích hoạt chia tỷ lệ phần thưởng thành (0,1], y+ là đầu ra được con người ưa thích, y− ngược lại. Bằng cách huấn luyện trên dữ liệu so sánh sở thích từng cặp này, mô hình phần thưởng có thể học trực tiếp để nắm bắt sở thích con người và thực hiện ước tính phần thưởng phù hợp cho RLHF.

Phần thưởng huấn luyện cuối cùng và suy luận: Đến đây, phần thưởng cuối cùng cho quy tắc cập nhật RL là:

r=ralignment −λrKL

λ ở đây là yếu tố kiểm soát.

Sau khi huấn luyện với chính sách RL ở trên, LM được điều chỉnh cuối cùng có thể phù hợp tốt hơn với sở thích con người. Trong khi thủ tục suy luận của LM được điều chỉnh thực sự tương tự như mô hình hóa làm phẳng và nối đã đề cập trước đây, nơi nó nhận hướng dẫn và đầu vào, và sau đó tạo ra đầu ra tương ứng.

Hạn chế: So với các chiến lược mô hình hóa khác, RLHF yêu cầu nhiều nỗ lực con người đắt đỏ hơn vì việc thu thập dữ liệu sở thích, đặc biệt là khi các đầu ra so sánh sở thích đều được viết bởi con người. Trong khi đó, hiệu suất của RLHF phụ thuộc nhiều vào chất lượng của các chú thích sở thích con người của nó. Quan trọng hơn, trong một số trường hợp, chẳng hạn như một số nhiệm vụ viết sáng tạo mở, con người khác nhau thường có sự bất đồng cao về quyết định sở thích do thiếu đầu ra sự thật cơ bản.

## 6 Tập dữ liệu Tuân theo Hướng dẫn và Đánh giá

Trong phần này, chúng tôi làm sáng tỏ một chủ đề quan trọng liên quan đến việc tuân theo hướng dẫn, tức là các tập dữ liệu tuân theo hướng dẫn và các thiết lập đánh giá cho các mô hình được điều chỉnh hướng dẫn.

### 6.1 Tập dữ liệu

Bản chất của việc tuân theo hướng dẫn là điều khiển các mô hình bằng cách tuân theo các hướng dẫn nhiệm vụ khác nhau và phản hồi với các đầu ra mong muốn tương ứng. Do đó, tập dữ liệu instruction-tuning (các cặp hướng dẫn-đầu ra chất lượng cao) là phần quan trọng. Các tập dữ liệu instruction-tuning hiện tại có thể được chia thành hai loại theo các loại chú thích khác nhau: 1) tập dữ liệu được chú thích bởi con người (§ 6.1.1); 2) tập dữ liệu tổng hợp LLM (§ 6.1.2).

#### 6.1.1 Tập dữ liệu được chú thích bởi con người

Cách thông thường để tạo tập dữ liệu instruction-tuning là bằng cách sử dụng các chú thích viên con người, đặc biệt là đối với những tập dữ liệu giai đoạn đầu. Ví dụ, PUBLIC POOL OF PROMPTS (P3) và FLAN đã thu thập các tập dữ liệu instruction-tuning đa nhiệm vụ, nơi họ sử dụng chuyên môn con người để thiết kế các prompt khác nhau cho mỗi nhiệm vụ. Mishra et al. (2022b) đã đề xuất NATURAL INSTRUCTIONS, trong đó họ thu thập hơn 60 nhiệm vụ NLP với các hướng dẫn được viết bởi con người tương ứng; Wang et al. (2022b) đã mở rộng thêm bộ sưu tập này thành quy mô 1.6k nhiệm vụ đa ngôn ngữ được đóng góp bởi 88 chuyên gia NLP, được gọi là SUPER-NATURAL INSTRUCTIONS.

Các tập dữ liệu được tạo bởi con người hầu hết có chất lượng cao (với lỗi chú thích tối thiểu) nhưng yêu cầu nỗ lực con người tốn công và tiêu thụ thời gian đắt đỏ. Quan trọng hơn, con người gặp phải sự hạn chế về tính đa dạng—thật sự thách thức đối với con người để brainstorm các nhiệm vụ đa dạng và mới lạ; do đó, quy mô nhiệm vụ của các tập dữ liệu được chú thích bởi con người thường bị hạn chế bởi các chú thích viên con người (ví dụ: mức độ chuyên môn và lược đồ hợp tác của con người).

#### 6.1.2 Tập dữ liệu tổng hợp LLM

Vì LLM đã thể hiện chất lượng chú thích vượt trội trên các nhiệm vụ NLP khác nhau, rất nhiều công trình gần đây đã cố gắng sử dụng LLM (ví dụ: ChatGPT và GPT-4) thay vì con người trong việc sưu tập tập dữ liệu instruction-tuning. Ví dụ, SELF-INSTRUCT và UNNATURAL INSTRUCTIONS đã sử dụng các hướng dẫn được chú thích bởi con người như các minh chứng để hướng dẫn LLM trong việc tạo ra các nhiệm vụ mới và tăng tính đa dạng nhiệm vụ. WIZARDLM đã sử dụng một paradigm tiến hóa hướng dẫn để tăng độ phức tạp của hướng dẫn. DYNOSAUR đã tái sử dụng các cặp đầu vào-đầu ra hiện có trong các tập dữ liệu NLP để kích thích các hướng dẫn mới và giảm chi phí chú thích. MUFFIN đã prompt các LLM để thu thập các hướng dẫn nhiệm vụ khác nhau cho cùng một đầu vào và có được khả năng khái quát hóa ấn tượng của các mô hình nhỏ hơn được điều chỉnh. Ngoài các tập dữ liệu hướng dẫn-đầu ra một lượt, một số công trình cũng đã thu thập dữ liệu hội thoại nhiều lượt từ ShareGPT, nơi các hướng dẫn được tạo bởi con người (người dùng của OpenAI API), và các phản hồi đến từ LLM.

Mặc dù các tập dữ liệu tổng hợp LLM này chứa tiếng ồn đáng kể (ví dụ: hướng dẫn không mạch lạc và đầu ra ảo giác), phân phối nhiệm vụ đa dạng và các mẫu đầu ra được mô hình ưa thích vẫn có lợi cho các mô hình nhỏ hơn trong việc tuân theo hướng dẫn, đạt được hiệu suất khái quát hóa tương đương thậm chí tốt hơn so với các tập dữ liệu được chú thích bởi con người.

Nói cách khác, lựa chọn giữa các tập dữ liệu được chú thích bởi con người và tổng hợp LLM cũng có thể được coi là một sự đánh đổi giữa chất lượng dữ liệu và tính đa dạng. Các công trình trước đây đã kết luận rằng cả hai yếu tố đều ảnh hưởng đến hiệu suất của các mô hình kết quả—việc trộn dữ liệu con người và máy có thể dẫn đến kết quả tốt hơn, trong khi không có kết luận cụ thể nào về yếu tố nào vượt trội hơn, điều này phụ thuộc nhiều vào các nhiệm vụ downstream và tình huống ứng dụng.

### 6.2 Đánh giá

#### 6.2.1 Các lược đồ đánh giá khác nhau

Cách đánh giá một mô hình được điều chỉnh hướng dẫn cũng là một chủ đề quan trọng. Hầu hết các nhiệm vụ NLP truyền thống thường có các tiêu chí cụ thể về mục tiêu nhiệm vụ, trong khi đối với việc tuân theo hướng dẫn, mục tiêu chính là điều khiển mô hình tuân theo hướng dẫn—mức độ mô hình tuân theo hướng dẫn rất chủ quan và phụ thuộc vào các sở thích khác nhau. Do đó, các công trình khác nhau có xu hướng sử dụng các chiến lược đánh giá khác nhau. Trong phần này, chúng tôi liệt kê một số thiết lập đánh giá phổ biến.

Chỉ số tự động: Khi kiểm tra hiệu suất tuân theo hướng dẫn của mô hình trên một tập dữ liệu đánh giá, nếu tập dữ liệu này có đầu ra "sự thật cơ bản", thì một tiêu chí thông thường là sử dụng những chỉ số đánh giá tự động, chẳng hạn như EXACT-MATCH và ROUGE, đã được sử dụng rộng rãi để đánh giá các mô hình tạo sinh. Tuy nhiên, chiến lược đánh giá ngây thơ này gặp phải một số nhược điểm: 1) đã được cam kết rộng rãi rằng các chỉ số tạo sinh tự động không hoàn hảo và có những thiên lệch đáng kể (ví dụ: điểm BLUE có thiên lệch độ dài văn bản); 2) tất cả các chỉ số này được sử dụng để cho thấy mức độ dự đoán của mô hình phù hợp với các câu trả lời được chú thích trước, tuy nhiên, hầu hết các nhiệm vụ người dùng thực tế rất mở, và có thể không có nhãn sự thật cơ bản chính thức để tính toán các chỉ số; 3) bản chất của việc tuân theo hướng dẫn là tuân theo hướng dẫn của người dùng và cung cấp các phản hồi mong muốn có thể giải quyết tốt yêu cầu của người dùng, trong khi các chỉ số tự động tập trung hơn vào một số mẫu văn bản bề ngoài và thiếu sự phản ánh về mức độ phản hồi thỏa mãn các hướng dẫn.

Đánh giá con người: Một phương pháp đánh giá đáng tin cậy hơn là sử dụng con người để quyết định liệu đầu ra của mô hình có thỏa mãn hướng dẫn hay không. Ví dụ, với một hướng dẫn nhiệm vụ và một đầu ra mô hình tương ứng, người đánh giá con người nên đọc hướng dẫn và quyết định liệu đầu ra mô hình này có thể chấp nhận được hay không (báo cáo tỷ lệ chấp nhận cho mô hình mục tiêu); hoặc yêu cầu con người so sánh đầu ra của hai mô hình và quyết định mô hình nào thỏa mãn hướng dẫn tốt hơn (so sánh từng cặp giữa hai mô hình). Vì các hướng dẫn hầu hết phức tạp và chứa rất nhiều ràng buộc rõ ràng hoặc ngầm định, đánh giá con người linh hoạt và chính xác hơn các chỉ số tự động trong việc phản ánh khả năng tuân theo hướng dẫn của các mô hình khác nhau.

Tuy nhiên, đánh giá con người cũng đắt đỏ hơn nhiều, chậm hơn đánh giá tự động, và không thể tái tạo. Do đó, hầu hết các công trình chỉ tiến hành đánh giá con người trên một tập con nhỏ của toàn bộ benchmark đánh giá. Trong khi đó, nó chủ yếu dựa trên sở thích cá nhân của người đánh giá con người và có thể dẫn đến phương sai cao giữa các người đánh giá khác nhau.

LLM hàng đầu như người đánh giá: Để giải quyết các vấn đề đã nêu của đánh giá con người, các công trình gần đây cũng đã cố gắng sử dụng LLM (ví dụ: GPT-4) thay vì con người để đánh giá khả năng tuân theo hướng dẫn của các mô hình, chẳng hạn như VicunaEval và AlpacaEval. Tuy nhiên, mặc dù LLM rẻ hơn và nhanh hơn, chúng được tìm thấy có thiên lệch sở thích nghiêm trọng đối với một số mẫu văn bản bề ngoài hoặc ảo giác, ví dụ: GPT-4 ưa thích các văn bản dài hơn và những phản hồi với token đa dạng. Trong khi đó, chỉ một điểm sở thích cuối cùng thường không đủ cho một đánh giá toàn diện.

Để cải thiện độ tin cậy, thay vì để LLM đơn giản cung cấp quyết định sở thích, các công trình khác có xu hướng yêu cầu LLM tạo ra các phân tích toàn diện bên cạnh quyết định cuối cùng, chẳng hạn như tạo ra các loại lỗi, vị trí và giải thích trước khi kết luận với điểm cuối cùng. Một số công trình khác cũng đã định nghĩa trước một số câu hỏi tiêu chí có thể giải thích cho các nhiệm vụ đánh giá khác nhau (ví dụ: đối với hướng dẫn "Vui lòng tạo ra ít nhất 25 câu", định nghĩa một tiêu chí "việc tạo ra của mô hình có ít nhất 25 câu không?"), điều này có thể được xác minh bởi con người hoặc LLM một cách dễ dàng (tức là thực hiện phân loại nhị phân trên các tiêu chí được định nghĩa trước này). Saha et al. (2023) cũng đã yêu cầu LLM đầu tiên tạo ra các câu hỏi tiêu chí tự động theo các hướng dẫn và sau đó đánh giá phản hồi của mô hình.

#### 6.2.2 Hai nhánh đánh giá

Mặc dù có các lựa chọn đánh giá khác nhau trong việc tuân theo hướng dẫn, chúng có thể được tóm tắt thành hai nhánh từ quan điểm của chúng tôi.

Đánh giá tập trung vào nhiệm vụ: Hầu hết các tập dữ liệu đánh giá trong nhánh này dựa trên học đa nhiệm vụ thông thường, nơi các nhiệm vụ đánh giá chủ yếu là các nhiệm vụ NLP truyền thống, chẳng hạn như suy luận ngôn ngữ tự nhiên. Nhánh này nhắm đến kiểm tra khả năng tuân theo hướng dẫn và giải quyết vấn đề của LLM, và tiêu chí chính ở đây là liệu các mô hình có thể giải quyết chính xác nhiệm vụ văn bản đã cho hay không. Do đó, hầu hết các thiết lập đánh giá trong nhánh này áp dụng các chỉ số tự động thông thường để phản ánh sự phù hợp với nhãn sự thật cơ bản của nhiệm vụ. Các benchmark đại diện là MMLU, BBH, SuperNI-Test, T0-Eval, InstructEval, v.v.

Đánh giá tập trung vào con người: Các hướng dẫn đánh giá trong thiết lập này hướng đến người dùng hoặc giống như truy vấn hội thoại của người dùng, chủ yếu được sử dụng để kiểm tra mức độ phản hồi của các mô hình phù hợp với sở thích con người, đặc biệt là về tính an toàn và hữu ích của các phản hồi (ví dụ: tính vô hại và trung thực). Không giống như đánh giá tập trung vào nhiệm vụ, đánh giá tập trung vào con người quan tâm ít hơn đến các nhãn sự thật cơ bản vì hầu hết các nhiệm vụ người dùng đều mở. Do đó, thiết lập đánh giá này chủ quan hơn và yêu cầu nỗ lực con người hoặc LLM cấp cao hơn. Các benchmark đại diện là AlpacaFarm, VicunaEval, HHH, v.v.

Theo hiểu biết của chúng tôi, vì việc tuân theo hướng dẫn là một chủ đề khá rộng có thể liên quan đến các nhiệm vụ downstream khác nhau và các tình huống thực tế, vẫn thiếu một thiết lập đánh giá toàn diện có thể được áp dụng cho tất cả các tình huống mục tiêu. Một lựa chọn thực tế hơn là áp dụng các thiết lập đánh giá khác nhau theo mục tiêu của các công trình khác nhau (tức là tập trung vào nhiệm vụ hoặc tập trung vào con người).

## 7 Các Yếu tố Ảnh hưởng đến Hiệu suất Tuân theo Hướng dẫn

Việc tuân theo hướng dẫn được chứng minh là hiệu quả trong rất nhiều nhiệm vụ NLP few/zero-shot, nhưng làm thế nào để giải thích hiệu suất ấn tượng của hướng dẫn? Và những khía cạnh nào tạo nên một quy trình tuân theo hướng dẫn thành công? Chúng tôi phân loại các yếu tố ảnh hưởng đến hiệu suất tuân theo hướng dẫn thành năm chiều: mô hình, hướng dẫn, minh chứng, tương tác mô hình-hướng dẫn, và tập dữ liệu.

### 7.1 Các Yếu tố Liên quan đến Mô hình

#### 7.1.1 Cập nhật mô hình hay không

Như được thể hiện trong Hình 1 (b), để thúc đẩy LLM hiểu và tuân theo hướng dẫn nhiệm vụ một cách mượt mà hơn, một thực hành được áp dụng rộng rãi là fine-tuning LLM trên các tập dữ liệu đa nhiệm vụ, nơi mỗi đầu vào nhiệm vụ được trang bị một hướng dẫn nhiệm vụ. Quy trình này cũng được biết đến rộng rãi là "instruction tuning". Rất nhiều công trình đã chứng minh rằng LLM được instruction-tuned có thể tuân theo hướng dẫn của các nhiệm vụ chưa thấy tốt hơn so với LLM đóng băng.

Ngoài những lợi ích về hiệu suất trên các nhiệm vụ chưa thấy, instruction tuning có nhiều lợi ích khác, chẳng hạn như học nhanh hơn trên các nhiệm vụ downstream, mạnh mẽ hơn đối với các nhiễu loạn hướng dẫn nhỏ (ví dụ: paraphrasing), trở nên thân thiện với người dùng hơn, và tốt hơn trong việc tuân theo soft instruction, v.v.

#### 7.1.2 Quy mô mô hình

Các công trình gần đây đã chứng minh rằng quy mô mô hình ảnh hưởng đáng kể đến hiệu suất khái quát hóa của việc tuân theo hướng dẫn. Như được thể hiện trong Hình 4, hiệu suất khái quát hóa của mỗi mô hình liên tục tăng khi mở rộng quy mô kích thước mô hình. Thú vị hơn, khi quy mô mô hình đủ lớn, thậm chí các LLM vanilla có thể vượt trội đáng kể so với các LLM nhỏ hơn được điều chỉnh trên các nhiệm vụ rộng lớn (xem Flan-PaLM; vanilla 540B > 8B + 1836 nhiệm vụ), điều này có thể ngụ ý rằng lợi ích của việc mở rộng quy mô kích thước mô hình có thể vượt trội hơn việc mở rộng quy mô tập dữ liệu.

Tuy nhiên, quy mô mô hình siêu lớn thường không thể chi trả được đối với hầu hết các nhóm nghiên cứu, và nó cũng dẫn đến lượng khí thải carbon khổng lồ, làm cho nó không thực tế trong hầu hết các tình huống thực tế. Tương ứng, các công trình gần đây bắt đầu nghiên cứu một cách hiệu quả hơn để giải quyết vấn đề quy mô mô hình, ví dụ: parameter-efficient fine-tuning.

### 7.2 Các Yếu tố Liên quan đến Hướng dẫn

#### 7.2.1 Kỹ thuật hướng dẫn

Một vấn đề phổ biến trong việc tuân theo hướng dẫn là các mô hình pre-trained thường nhạy cảm với một số thay đổi tinh tế trong hướng dẫn—thậm chí một chỉnh sửa nhỏ về hướng dẫn, chẳng hạn như paraphrasing hoặc thay thế từ, có thể dẫn đến phương sai hiệu suất lớn. Do đó, việc chỉnh sửa từ ngữ của hướng dẫn trước khi sử dụng, được gọi là kỹ thuật hướng dẫn, là quan trọng đối với hiệu suất của các mô hình.

Một giải pháp đơn giản là viết lại hướng dẫn theo cách thủ công, tức là kỹ thuật hướng dẫn của con người. Khi con người thực hiện kỹ thuật hướng dẫn, các tiêu chí viết lại chủ yếu là trực giác con người. Ví dụ, Mishra et al. (2022a) đã tiến hành phân tích các trường hợp lỗi trên đầu ra tuân theo hướng dẫn của GPT. Tương ứng, họ đã thiết kế một số quy tắc thực nghiệm về việc viết hướng dẫn và "định khung lại" các hướng dẫn. Tất cả các quy tắc đề xuất này dựa trên trực giác con người, ví dụ: liệt kê hướng dẫn và phân tách nhiệm vụ. Để tránh thiên lệch sở thích được giới thiệu bởi một nhóm nhỏ con người, Bach et al. (2022) đã đề xuất kỹ thuật hướng dẫn dựa trên cộng đồng, nơi họ thu thập các hướng dẫn được tạo bởi các chuyên gia NLP khác nhau với các phong cách viết khác nhau, đa dạng hóa các lựa chọn hướng dẫn. Tuy nhiên, kỹ thuật hướng dẫn của con người tốn thời gian và đắt đỏ. Hơn nữa, trực giác con người về thiết kế hướng dẫn có thể chủ quan và đôi khi không tối ưu cho các mô hình.

Đến đây, kỹ thuật hướng dẫn tự động cố gắng để mô hình tự động tìm ra các hướng dẫn tốt hơn. Prasad et al. (2023) đã đề xuất một phương pháp dựa trên chỉnh sửa để tự động chỉnh sửa hướng dẫn. Cho mỗi lần lặp, họ đã chỉnh sửa hướng dẫn ở cấp độ cụm từ để tạo ra nhiều ứng cử viên, và sau đó sử dụng mô hình mục tiêu để dự đoán điểm của các ứng cử viên khác nhau bằng cách sử dụng một tập được gán nhãn nhỏ (tức là tính toán entropy sự thật cơ bản và độ chính xác). Bằng cách làm như vậy, Prasad et al. (2023) đã đạt được hiệu suất tốt hơn so với những hướng dẫn được định khung lại thủ công. Ngoài việc sử dụng điểm sự thật cơ bản, Gonen et al. (2023) đã sử dụng khả năng dự đoán của mô hình như phản hồi để chọn các ứng cử viên hướng dẫn, điều này thậm chí không yêu cầu bất kỳ thể hiện được gán nhãn nào. Deng et al. (2022) đã đề xuất thêm một khung học tăng cường để tiến hành kỹ thuật hướng dẫn. Mặc dù có hiệu suất vượt trội, nhược điểm rõ ràng của kỹ thuật hướng dẫn tự động là khả năng giải thích kém, nơi các hướng dẫn kết quả hầu hết vi phạm trực giác con người (ví dụ: một số câu không liên quan đến nhiệm vụ), tương tự như soft instruction.

Nói cách khác, kỹ thuật hướng dẫn là một quy trình đánh đổi—khả năng giải thích thấp hơn là thuế của hiệu suất tốt hơn. Trong khi đó, kỹ thuật hướng dẫn là một chủ đề rất thực nghiệm, và không có quy tắc/phương pháp tiêu chuẩn vàng về nó—các mô hình và nhiệm vụ khác nhau có thể yêu cầu thiết kế hướng dẫn hoàn toàn khác nhau. Do đó, chúng tôi rất khuyến khích cộng đồng phát hành các hướng dẫn sử dụng đi kèm khi phát hành các mô hình được instruction-tuned của họ, do đó đảm bảo hành vi mô hình ổn định và mong đợi.

#### 7.2.2 Tính nhất quán của hướng dẫn

Yếu tố này xem xét các hướng dẫn qua các nhiệm vụ huấn luyện và nhiệm vụ kiểm tra. Việc giữ paradigm hướng dẫn (ví dụ: tính trừu tượng) nhất quán là quan trọng trong việc tuân theo hướng dẫn. Wei et al. (2022a) đầu tiên đã nghiên cứu tác động hiệu suất của việc thay đổi paradigm hướng dẫn. Họ phát hiện rằng LLM được điều chỉnh trên hướng dẫn ngắn (tức là tên nhiệm vụ) không thể khái quát hóa sang hướng dẫn kiểu câu dài hơn (ngắn ̸⇒ dài). Tương tự, Gu et al. (2023) đã quan sát việc giảm hiệu suất khi thay đổi hướng dẫn kiểu đoạn văn thành hướng dẫn kiểu câu ngắn hơn trong giai đoạn kiểm tra (dài ̸⇒ ngắn), tiếp tục chỉ ra tầm quan trọng của tính nhất quán hướng dẫn.

Ngoài hướng dẫn rời rạc, việc duy trì paradigm hướng dẫn cũng quan trọng đối với soft instruction, tức là giữ cùng kích thước prefix embedding khi kiểm tra trên các nhiệm vụ chưa thấy. Thú vị là, kết quả tương tự cũng được tìm thấy trong các minh chứng few-shot (tức là in-context learning), nơi sự kết hợp của các cặp đầu vào-đầu ra hoặc số lượng minh chứng không thể thay đổi trong quá trình huấn luyện và đánh giá. Những hiện tượng này nêu ra mối quan tâm: mặc dù LLM được instruction-tuned mạnh mẽ đối với những nhiễu loạn nhỏ của hướng dẫn, chúng dễ bị tổn thương khi đối mặt với những thay đổi lớn hơn, điều này còn lâu mới đạt được khả năng khái quát hóa cấp độ con người.

#### 7.2.3 Tính đa dạng của hướng dẫn

Để cải thiện thêm độ mạnh mẽ của LLM, đặc biệt là khi đối mặt với những thay đổi lớn của paradigm hướng dẫn, mọi người cố gắng thúc đẩy tính đa dạng hướng dẫn trong giai đoạn huấn luyện—đối với cùng một nhiệm vụ huấn luyện, viết nhiều hướng dẫn trong các biểu thức văn bản khác nhau (ví dụ: từ ngữ và độ dài khác nhau), sau đó huấn luyện LLM trên hỗn hợp các hướng dẫn đa dạng. Đáng chú ý, Sanh et al. (2022) đã cho thấy rằng việc áp dụng các hướng dẫn với phong cách viết đa dạng không chỉ cải thiện khả năng khái quát hóa mô hình mà còn bù đắp phần nào cho quy mô mô hình hạn chế.

Tuy nhiên, việc tạo ra các hướng dẫn với tính đa dạng thủ công là đắt đỏ và thường khó đạt được do thiên lệch chú thích con người. Nhờ chất lượng chú thích xuất sắc của LLM, một số lượng đáng kể các công trình bắt đầu sử dụng các mô hình để soạn thảo các hướng dẫn sáng tạo. Mặc dù các hướng dẫn được tạo ra bởi mô hình được chứng minh là chứa nhiều tiếng ồn hơn, nhưng có lợi từ các cấu trúc cú pháp đa dạng, những hướng dẫn này vẫn có thể thể hiện hiệu ứng bổ sung với các hướng dẫn được viết bởi con người. Thú vị hơn, Lou et al. (2024) đã đề xuất một paradigm tập dữ liệu tuân theo hướng dẫn mới, nơi họ sử dụng LLM để tổng hợp các hướng dẫn nhiệm vụ đa dạng cho mỗi đầu vào. Có lợi từ paradigm này, các LM được điều chỉnh buộc phải tập trung nhiều hơn vào hướng dẫn hơn là đầu vào nhiệm vụ, đạt được hiệu suất tuân theo hướng dẫn hứa hẹn. Tất cả những kết quả này có thể ngụ ý lợi nhuận của tính đa dạng hướng dẫn, thậm chí với cái giá của tính đúng đắn của hướng dẫn.

#### 7.2.4 Thêm minh chứng hay không

Minh chứng, tức là một vài ví dụ đầu vào-đầu ra, đã được chứng minh là quan trọng đối với tính biểu đạt của hướng dẫn nhiệm vụ. Ví dụ, các công trình hiện có phát hiện rằng việc thêm một vài minh chứng tích cực vào hướng dẫn văn bản có thể dẫn đến cải thiện hiệu suất đáng kể trên các nhiệm vụ chưa thấy, đặc biệt là đối với các nhiệm vụ chiếm không gian đầu ra phức tạp. Thật ngạc nhiên, Gu et al. (2023) đã phát hiện thêm rằng các mô hình phụ thuộc nhiều vào các minh chứng few-shot và thậm chí bỏ qua các tài nguyên hữu ích khác (ví dụ: định nghĩa nhiệm vụ chi tiết) khi có minh chứng. Sự nổi bật này có lẽ bởi vì LLM thích khai thác các mẫu bề ngoài hơn của các minh chứng hơn là các biểu thức văn bản phức tạp khác. Nói cách khác, hiện tại, một khung toàn diện để mã hóa chính xác các hướng dẫn thuần túy trong trường hợp không có minh chứng hoặc scaling nhiệm vụ vẫn còn khó nắm bắt.

### 7.3 Các Yếu tố Liên quan đến Minh chứng

Vì các minh chứng few-shot có thể ảnh hưởng nhiều đến hiệu suất tuân theo hướng dẫn của mô hình, các nghiên cứu gần đây đã điều tra các yếu tố khác nhau trong các minh chứng, có thể tăng cường thêm hiệu quả học minh chứng của mô hình.

#### 7.3.1 Lựa chọn minh chứng

Với một thể hiện kiểm tra chưa được gán nhãn (tức là thể hiện chỉ có đầu vào đang chờ câu trả lời từ mô hình), và một pool các thể hiện huấn luyện được gán nhãn (tức là các cặp đầu vào-đầu ra), làm thế nào để chọn các minh chứng tốt hơn từ pool này cho thể hiện kiểm tra là một câu hỏi cơ bản cho in-context learning.

Liu et al. (2022b) đã đề xuất một chiến lược lựa chọn minh chứng không giám sát, nơi họ sử dụng kNN (k Nearest Neighbors) để lấy các minh chứng với khoảng cách embedding gần nhất như thể hiện kiểm tra. Bước chính trong các phương pháp lựa chọn dựa trên clustering là các chỉ số khoảng cách, chẳng hạn như khoảng cách L2, cosine-similarity hoặc thông tin tương hỗ. Ngoài các phương pháp dựa trên clustering, một nhánh phương pháp khác đã sử dụng điểm đầu ra của các mô hình như tiêu chí lựa chọn. Ví dụ, Nguyen và Wong (2023) đã cố gắng chọn một tập con A từ pool huấn luyện như các minh chứng bằng cách đo lường phương sai hiệu suất trung bình của mô hình giữa A và tập bù A.

Ngoài các chiến lược lựa chọn không giám sát hoặc giám sát yếu ở trên, một số công trình khác cũng đã sử dụng các phương pháp có giám sát. Wang et al. (2023a) đã coi LM như các mô hình chủ đề ngầm, nơi LM có thể tạo ra biểu diễn khái niệm có ý nghĩa dựa trên các minh chứng few-shot. Bằng cách huấn luyện các mô hình chủ đề, họ đã chọn các minh chứng có thể tối đa hóa khả năng của khái niệm đã cho. Trong khi đó, Zhang et al. (2022b) đã coi việc lựa chọn minh chứng như một quy trình quyết định Markov và đề xuất một mô hình học tăng cường thông qua Q-learning.

#### 7.3.2 Thứ tự của minh chứng

Thậm chí với cùng một tập hợp minh chứng, sự khác biệt trong thứ tự ví dụ cũng có thể ảnh hưởng đến hiệu suất in-context learning của mô hình. Zhao et al. (2021) đã nhấn mạnh rằng GPT-3 nhạy cảm với thứ tự của các minh chứng, và họ đã phỏng đoán rằng độ nhạy cảm này có thể đến từ Recency Bias—xu hướng lặp lại câu trả lời xuất hiện về phía cuối của prompt. Lu et al. (2022) đã tiến hành thêm các thí nghiệm toàn diện và phát hiện rằng, cùng với GPT-3, các mô hình khác nhau cũng gặp phải độ nhạy cảm thứ tự.

Đến đây, các công trình gần đây đã đề xuất một số phương pháp để sắp xếp một thứ tự ví dụ "phù hợp" cho LM. Ví dụ, dựa trên Recency Bias, Liu et al. (2022b) đã tính toán độ tương tự embedding giữa các minh chứng và đầu vào mục tiêu, những ví dụ tương tự hơn được đặt gần hơn (phải hơn) với đầu vào. Lu et al. (2022) đã đề xuất một số chỉ số dựa trên entropy để tìm kiếm thứ tự minh chứng tốt nhất.

#### 7.3.3 Tăng cường bước lý luận

Ngoài các minh chứng đầu vào-đầu ra tiêu chuẩn, việc tăng cường các ví dụ trong ngữ cảnh với các bước lý luận được tìm thấy hữu ích cho hiệu suất của mô hình, đặc biệt là đối với các mô hình siêu lớn.

Wei et al. (2022b) đã đề xuất chain-of-thoughts (CoT), nơi họ chèn một số bước lý luận trung gian được viết bởi con người (tức là lý lẽ) giữa đầu vào và đầu ra của minh chứng trong ngữ cảnh. Bằng cách làm như vậy, khi dự đoán đầu ra mục tiêu, các mô hình có thể tạo ra các bước lý luận trung gian, do đó tăng cường hiệu suất trên các nhiệm vụ lý luận (ví dụ: các bài toán từ toán học) và khả năng giải thích của LM. Ngoài CoT được viết bởi con người, Xu et al. (2023c) cũng phát hiện rằng CoT được tổng hợp bởi các mô hình lớn hơn có thể hỗ trợ các mô hình nhỏ hơn. Dựa trên các kết quả hứa hẹn của việc áp dụng CoT, các biến thể tiên tiến hơn đã được đề xuất để lý luận chính xác hơn, chẳng hạn như program-of-thoughts (PoT), tree-of-thoughts (ToT), graph-of-thoughts (GoT), và CoT với tăng cường giải mã tự nhất quán.

Tuy nhiên, tương tự như độ nhạy cảm minh chứng, các phong cách viết CoT khác nhau cũng có thể dẫn đến phương sai hiệu suất. Do đó, trái ngược với CoT được tạo ra bởi con người (tức là few-shot CoT), Zhang et al. (2022c) đã đề xuất Auto-CoT (tức là zero-shot CoT), nơi họ thêm một "Hãy nghĩ từng bước" vào prompt và để các mô hình tự tạo ra CoT. Sau đó, ngày càng nhiều biến thể của Auto-CoT được đề xuất để giải quyết các nhiệm vụ lý luận phức tạp hơn. Ví dụ, Self-Ask đã yêu cầu mô hình đầu tiên tạo ra một vài câu hỏi liên quan đến đầu vào và sau đó trả lời những câu hỏi này bằng chính mô hình đó—những ngữ cảnh tự tạo này được sử dụng thêm như lý lẽ để giúp trả lời đầu vào ban đầu. Tương tự, Least-to-Most đã yêu cầu mô hình phân tách một đầu vào phức tạp ban đầu thành một số câu hỏi phụ và trả lời chúng tuần tự, có thể được sử dụng làm lý lẽ.

#### 7.3.4 Nhấn mạnh ánh xạ đầu vào-đầu ra

Đối với in-context learning, mô hình thường không thể "học" trực tiếp ánh xạ đầu vào-đầu ra từ các ví dụ đã cho vì không có cập nhật tham số cho các mô hình. Do đó, một vấn đề của in-context learning là, khi tiến hành tuân theo hướng dẫn, các minh chứng không nhất thiết cần thiết cho mô hình để giải quyết nhiệm vụ (tức là thậm chí không có các minh chứng few-shot, mô hình vẫn có thể đưa ra dự đoán). Min et al. (2022b) cũng phát hiện rằng mô hình có nhiều khả năng "sao chép" ứng cử viên đầu ra từ các minh chứng, thay vì thực sự học ánh xạ cơ bản.

Đến đây, Wei et al. (2023) đã đề xuất symbol tuning. Khác với tuân theo hướng dẫn thông thường, điều chỉnh các mô hình để tuân theo các minh chứng đầu vào-đầu ra để hoàn thành đầu vào mục tiêu, symbol tuning sử dụng một số ký hiệu không liên quan để thay thế các đầu ra gốc của các minh chứng. Ví dụ, không gian đầu ra gốc của các minh chứng có thể là "tích cực" và "tiêu cực"; symbol tuning sử dụng "Foo" và "Bar" thay thế. Sau khi mất đi ngữ nghĩa của các không gian đầu ra, không có thiên lệch nhãn trước (Zhao et al., 2021) cho các mô hình dựa vào để đưa ra dự đoán cuối cùng, vì vậy các mô hình buộc phải tìm ra ánh xạ đầu vào-đầu ra trong ngữ cảnh.

### 7.4 Sự Phù hợp Mô hình-Hướng dẫn

Yếu tố này đề cập đến việc làm cho quy trình tuân theo hướng dẫn phù hợp tốt hơn với sở thích của LLM. Một khía cạnh là mục tiêu huấn luyện. Vì paradigm tuân theo hướng dẫn hiện tại chủ yếu sử dụng LLM như xương sống hệ thống, một trong những giải thích tiềm năng về lý do tại sao Hướng dẫn Hướng LLM (tức là prompt) có thể hoạt động là prompt phù hợp tốt với mục tiêu pretraining—mô hình hóa ngôn ngữ—và kích hoạt kiến thức cụ thể cho nhiệm vụ của LLM. Một số công trình hiện có đã chứng minh tầm quan trọng của việc tuân theo mục tiêu pretraining của LLM khi thực hiện tuân theo hướng dẫn, chẳng hạn như gọi lại các mục tiêu mô hình hóa ngôn ngữ trong giai đoạn fine-tuning. Một khía cạnh khác của việc phù hợp sở thích mô hình là cách thiết kế hướng dẫn: đó là, chuyển đổi các hướng dẫn thành phong cách hướng mô hình. Ví dụ, sử dụng soft instruction (tức là embedding liên tục) thay vì hướng dẫn rời rạc có thể hiểu được bởi con người. Nó phù hợp với các hướng dẫn thực nghiệm được thiết lập trong lĩnh vực kỹ thuật prompt, nhấn mạnh tầm quan trọng của thiết kế prompt hướng mô hình. Mặc dù có lợi nhuận hiệu suất, vẫn còn tranh cãi về việc có đáng để chuyển đổi các hướng dẫn hướng con người ban đầu thành phong cách hướng LLM hay không, bởi vì nó luôn làm tổn hại đến khả năng diễn giải của hướng dẫn và rất trái với trực giác con người.

### 7.5 Yếu tố Dữ liệu: Quy mô Nhiệm vụ

Quy mô nhiệm vụ thường đề cập đến số lượng các loại nhiệm vụ huấn luyện khác nhau trong tập dữ liệu. Vì "yếu tố dữ liệu" cũng bao gồm quy mô của các thể hiện huấn luyện, Wang et al. (2022b) đã điều tra tác động của cả quy mô nhiệm vụ và thể hiện. Họ phát hiện rằng quy mô thể hiện (số nhiệm vụ cố định, tăng số lượng thể hiện cho mỗi nhiệm vụ) chỉ có thể mang lại sự cải thiện hiệu suất hạn chế, trong khi quy mô nhiệm vụ là yếu tố quan trọng cho việc tuân theo hướng dẫn, phù hợp với quan sát của các công trình khác. Như được minh họa trong Hình 4, mô hình cùng kích thước với nhiều nhiệm vụ điều chỉnh thường đạt được hiệu suất tốt hơn. Tuy nhiên, sự cải thiện hiệu suất của việc mở rộng quy mô nhiệm vụ không ổn định, đặc biệt là khi kích thước mô hình quá nhỏ (ví dụ: 0.08B Flan-T5). Hiện tượng này phù hợp với cuộc thảo luận trong § 7.1, chúng ta có thể rút ra một kết luận tương tự ở đây: lợi nhuận của quy mô nhiệm vụ bị chi phối nhiều bởi quy mô mô hình.

### 7.6 Takeaway Chính: Dual-Track Scaling

Trong tất cả các yếu tố được thảo luận trong phần này, scaling có thể được coi là yếu tố cốt lõi dẫn đến thành công của việc tuân theo hướng dẫn. Trước khi có việc tuân theo hướng dẫn dựa trên LLM, scaling chủ yếu dành cho các mô hình deep learning: từ mạng nơ-ron đơn lớp đến perceptron đa lớp, từ mạng nơ-ron convolutional/recurrent đến transformer sâu nhiều lớp. Cùng với việc pretraining dữ liệu văn bản thô khổng lồ, các mô hình ngày càng tăng được kỳ vọng đã mã hóa một lượng lớn kiến thức mục đích chung. Trong kỷ nguyên của việc tuân theo hướng dẫn, nơi cộng đồng quan tâm nhiều hơn đến khả năng khái quát hóa chéo nhiệm vụ, chỉ scaling LLM dường như là chưa đủ. Do đó, các nhà nghiên cứu thực hiện một scaling song song: thu thập ngày càng nhiều nhiệm vụ huấn luyện và các ví dụ được gán nhãn cho mỗi nhiệm vụ. Chúng tôi diễn giải điều này như dual-track scaling. Tổng thể, dual-track scaling này cùng nhau tìm kiếm giám sát để giải quyết các nhiệm vụ mới—giám sát hoặc đến từ pretraining của LLM hoặc các nhiệm vụ huấn luyện đáng kể. Mặc dù có tiến bộ, một số thách thức đáng chú ý vẫn còn trong lĩnh vực này, mà chúng tôi sẽ thảo luận trong phần tiếp theo.

## 8 Thách thức và Hướng Tương lai

Mặc dù tất cả các lợi ích đã nêu của hướng dẫn, rất nhiều thách thức chưa được khám phá vẫn còn trong lĩnh vực này. Trong phần này, chúng tôi liệt kê một số thách thức liên quan đến việc tuân theo hướng dẫn, đáng để nghiên cứu tương lai điều tra.

### 8.1 Thuế của Sự Phù hợp Hướng dẫn

Việc tuân theo hướng dẫn nhằm mục đích điều khiển các mô hình để hỗ trợ con người tốt hơn trong các nhiệm vụ thực tế; do đó, ngoài việc theo đuổi hiệu suất tối ưu, an toàn thời gian suy luận cũng là một khía cạnh quan trọng đối với các mô hình được instruction-tuned (tức là sự phù hợp hướng dẫn). Ouyang et al. (2022) đã định nghĩa "alignment" với ba tiêu chí—Hữu ích, Trung thực, và Vô hại (HHH), đã được xem xét rộng rãi bởi các mô hình và tập dữ liệu instruction tuning trước đây. Tuy nhiên, alignment cũng có thể mang lại "thuế" cho các mô hình được instruction-tuned. Ví dụ, Bekbayev et al. (2023) phát hiện rằng các câu trả lời được phù hợp tốt được cung cấp trong các tập dữ liệu tuân theo hướng dẫn có thể làm giảm hiệu suất của mô hình rất nhiều trên các benchmark nhiệm vụ khác nhau. Điều này ngụ ý một sự đánh đổi giữa hiệu suất và an toàn cho việc tuân theo hướng dẫn, yêu cầu xem xét cẩn thận.

### 8.2 Học Thông tin Phủ định

Phủ định là thuộc tính ngôn ngữ phổ biến và đã được tìm thấy là quan trọng đối với các nhiệm vụ NLP khác nhau, ví dụ: NLI. Cụ thể đối với việc tuân theo hướng dẫn, phủ định biểu thị thông tin cần tránh của hướng dẫn trong ngữ cảnh, bao gồm các yêu cầu phủ định (ví dụ: "tránh sử dụng từ dừng") và các minh chứng tiêu cực (tức là một số ví dụ sai). Mặc dù con người có thể học được rất nhiều từ phủ định, các công trình hiện có phát hiện LLM thường không tuân theo các hướng dẫn phủ định; một số phủ định thậm chí có thể làm giảm hiệu suất của mô hình. Vì phủ định ngày càng trở thành một thách thức trong việc tuân theo hướng dẫn, chúng tôi cung cấp một số gợi ý để truyền cảm hứng cho công việc tương lai. Một giải pháp tiềm năng là huấn luyện unlikelihood, huấn luyện LLM để tối thiểu hóa xác suất sự thật cơ bản khi các hướng dẫn phủ định được điều kiện. Ngoài ra, Yin et al. (2022) đã đề xuất pretraining LM trên các minh chứng tiêu cực với mục tiêu tối đa hóa khả năng để khai thác thông tin hữu ích trong phủ định. Một số phương pháp khác, chẳng hạn như phép chiếu tương phản nhất quán và biểu diễn n-gram, cũng đã cung cấp những hiểu biết sâu sắc về việc giải quyết vấn đề này.

### 8.3 Các Cuộc tấn công Hướng dẫn Đối kháng

Mặc dù hầu hết LM được instruction-tuned có thể phù hợp tốt với sở thích con người và cung cấp các phản hồi vô hại, các công trình gần đây phát hiện rằng chúng có thể dễ dàng bị tấn công—phản hồi của mô hình có thể bị thao túng bằng cách sử dụng các chiến lược prompting đơn giản. Kang et al. (2023) đã thiết kế một số prompt để kích hoạt LLM tạo ra nội dung độc hại. Ví dụ, thay vì cung cấp trực tiếp hướng dẫn độc hại với ý định có hại rõ ràng, họ chia hướng dẫn thành một số phần (mỗi phần tự nó không kích hoạt cơ chế phòng thủ của LLM). Bằng cách làm như vậy, những LLM mạnh mẽ được phù hợp sở thích, chẳng hạn như ChatGPT và InstructGPT, đã bị lừa thành công và tạo ra nội dung có hại. Li et al. (2023b) cũng phát hiện rằng các mô hình tạo sinh được tăng cường bằng retrieval có thể dễ dàng bị tấn công bằng cách tiêm các câu hỏi đối kháng vào ngữ cảnh được lấy. Ngoài việc tấn công các LLM được instruction-tuned, Wan et al. (2023) kết luận rằng LLM cũng có thể bị tấn công trong quá trình tuân theo hướng dẫn. Dựa trên các thể hiện sạch, họ tự động tạo ra một vài ví dụ nhiễm độc để huấn luyện LLM và phát hiện rằng LLM kết quả có thể bị thao túng bằng cách sử dụng một số từ kích hoạt.

Vì LLM được instruction-tuned đã được áp dụng cho các tình huống thực tế khác nhau, chẳng hạn như tác nhân web và công cụ tìm kiếm, an toàn của việc tạo sinh LLM đang trở nên cấp bách hơn. Việc đơn giản tiến hành phù hợp sở thích hoặc lọc nội dung dường như là không đủ, đặc biệt là đối với những LLM siêu mạnh đó. Do đó, việc phát triển các phương pháp phòng thủ hiệu quả là cần thiết cho các mô hình được instruction-tuned hiện tại. Trong khi đó, các phân tích sâu hơn về lỗ hổng của LLM cũng quan trọng, có thể cung cấp thêm những hiểu biết sâu sắc về phòng thủ.

### 8.4 Khả năng Giải thích của Việc Tuân theo Hướng dẫn

Như chúng tôi đã đề cập trong § 7, để đạt được hiệu suất chéo nhiệm vụ hứa hẹn, một trong những yếu tố quan trọng là chuyển đổi Hướng dẫn Hướng Con người thành Hướng dẫn Hướng LLM, tức là làm cho các hướng dẫn phù hợp với sở thích của mô hình. Nhiều công trình trước đây đã xác minh tính hiệu quả của việc phục vụ sở thích của mô hình trong thiết kế hướng dẫn, ví dụ: sử dụng perplexity của mô hình trong việc chọn hướng dẫn phù hợp. Mặc dù có những lợi ích về hiệu suất, các hướng dẫn kết quả liên tục vi phạm trực giác con người và cho thấy độ tin cậy đáng lo ngại, chẳng hạn như một số hướng dẫn không mạch lạc về mặt ngữ nghĩa, không liên quan đến nhiệm vụ, hoặc thậm chí gây hiểu lầm. Những kết quả này chứng minh xung đột giữa lợi nhuận hiệu suất và khả năng diễn giải của con người về hướng dẫn, rất khó để đánh đổi.

Mặc dù Mishra et al. (2022a) đã chứng minh rằng có thể duy trì cả tính trung thực và hiệu quả của hướng dẫn, việc viết lại thủ công yêu cầu nỗ lực con người tốn công. Do đó, một trong những xu hướng tương lai là điều tra cách tự động diễn đạt lại các hướng dẫn, theo cách phù hợp với cả sở thích con người và mô hình.

### 8.5 Học Tuân theo Hướng dẫn thay vì Chỉ Tạo ra Y

Việc tuân theo hướng dẫn đa nhiệm vụ đang trở thành một thực hành cơ bản trong paradigm tuân theo hướng dẫn hiện tại. Tuy nhiên, có hai vấn đề trong paradigm học tập như vậy: (i) nó dựa vào việc huấn luyện trên các ví dụ được gán nhãn khổng lồ để học các hướng dẫn, vẫn đắt đỏ và không thực tế để sử dụng LLM quy mô lớn; (ii) mặc dù mục tiêu cuối cùng của việc tuân theo hướng dẫn là học tuân theo hướng dẫn bằng cách quan sát các nhiệm vụ huấn luyện khác nhau, mục tiêu huấn luyện hiện tại vẫn là khả năng tối đa thông thường của đầu ra tham chiếu. Mục tiêu tuân theo hướng dẫn ngầm này có thể dẫn đến tối ưu hóa không tối ưu (tức là LLM có thể học tạo ra Y cho X mà không thực sự hiểu ý nghĩa của hướng dẫn I).

Đến đây, một hướng tương lai mong muốn là phát triển một mục tiêu học tập mới để giúp LLM học một cách rõ ràng tuân theo hướng dẫn, có thể làm giảm sự phụ thuộc vào các thể hiện được gán nhãn quy mô lớn. Hơn nữa, một ý tưởng đầy tham vọng và thách thức hơn là thúc đẩy hệ thống tuân theo hướng dẫn mà không cần điều chỉnh thêm trên các ví dụ được gán nhãn của bất kỳ nhiệm vụ cụ thể nào, phần nào tương tự như paradigm dựa trên trình phân tích cú pháp ngữ nghĩa (§ 5).

### 8.6 Tuân theo Hướng dẫn Đa ngôn ngữ

Trực giác, việc tuân theo hướng dẫn là khả năng bất khả tri ngôn ngữ đối với các mô hình ngôn ngữ, có nghĩa là cũng có thể cho các mô hình ngôn ngữ đa ngôn ngữ tuân theo cùng hướng dẫn ngữ nghĩa với các ngôn ngữ khác nhau. Ví dụ, Kew et al. (2023) phát hiện LLM được điều chỉnh với hơn ba ngôn ngữ thể hiện khả năng tuân theo hướng dẫn mạnh hơn, ngụ ý lợi ích của instruction tuning đa ngôn ngữ. Thật không may, hầu hết các tập dữ liệu tuân theo hướng dẫn mã nguồn mở hiện tại và các mô hình nền tảng đều lấy tiếng Anh làm trung tâm (như được thể hiện trong Bảng 6). Do đó, việc phát hành các tập dữ liệu instruction tuning đa ngôn ngữ chất lượng cao (với dịch thuật cặp) sẽ có giá trị cho nghiên cứu tương lai, như cũng được đề cập bởi Peng et al. (2023).

## 9 Các Ứng dụng Liên quan đến Hướng dẫn

Ngoài phần chính của bài báo, chúng tôi cũng khảo sát một số hướng ứng dụng liên quan đến hướng dẫn phổ biến để truyền cảm hứng cho việc sử dụng board-wide tương lai cho việc tuân theo hướng dẫn.

### 9.1 Tương tác Người-Máy

Hướng dẫn văn bản có thể được coi một cách tự nhiên như một phương pháp tương tác người-máy. Nhiều công trình trước đây đã sử dụng hướng dẫn ngôn ngữ tự nhiên để hướng dẫn máy tính thực hiện các nhiệm vụ thực tế khác nhau.

Đối với các nhiệm vụ không phải NLP (đa phương thức), hầu hết tập trung vào việc học ngôn ngữ có căn cứ môi trường, tức là thúc đẩy tác nhân liên kết hướng dẫn ngôn ngữ tự nhiên với môi trường và đưa ra phản ứng tương ứng, chẳng hạn như chọn các đối tượng được đề cập từ một hình ảnh/video, tuân theo hướng dẫn điều hướng để di chuyển tác nhân, vẽ các dấu vết tương ứng trên bản đồ, chơi trò chơi bóng đá/thẻ dựa trên các quy tắc đã cho, tạo ra phát sóng thể thao thời gian thực, kiểm soát phần mềm, và truy vấn cơ sở dữ liệu bên ngoài, v.v. Trong khi đó, hướng dẫn cũng được điều chỉnh rộng rãi để giúp giao tiếp với hệ thống trong việc giải quyết các nhiệm vụ NLP, ví dụ: tuân theo hướng dẫn để thao tác chuỗi, phân loại email dựa trên các giải thích đã cho, và tạo sinh text-to-code.

Gần đây, một số nghiên cứu ngày càng nhiều có xu hướng thiết kế quy trình giao tiếp người-máy theo cách lặp đi lặp lại và mô-đun. Ví dụ, Li et al. (2020) đã xây dựng một hệ thống để giúp người dùng giải quyết các nhiệm vụ hàng ngày (ví dụ: đặt cà phê hoặc yêu cầu Uber). Có lợi từ giao diện đồ họa thân thiện với người dùng, hệ thống có thể lặp đi lặp lại đặt câu hỏi về các nhiệm vụ, và người dùng có thể liên tục tinh chỉnh hướng dẫn của họ để tránh mô tả không rõ ràng hoặc khái niệm mơ hồ. Vì thường khó cho người dùng không chuyên môn viết hướng dẫn đủ trong một lần, việc áp dụng paradigm lặp đi lặp lại và mô-đun trong thiết kế hệ thống dựa trên hướng dẫn có thể giúp hướng dẫn người dùng làm phong phú hướng dẫn nhiệm vụ từng bước một. Do đó, paradigm này hiệu quả làm giảm nhu cầu tư duy của người dùng và dẫn đến một hệ thống hướng người dùng hơn. Do giá trị thực tế của nó, chúng tôi nhấn mạnh tầm quan trọng của nhánh công việc này trong bài báo này.

### 9.2 Tăng cường Dữ liệu và Đặc trưng

Hướng dẫn nhiệm vụ được coi là tài nguyên giám sát gián tiếp nơi đôi khi các quy tắc bề ngoài và khẳng định được nhúng. Những quy tắc này cũng được biết đến như các hàm gán nhãn có thể được áp dụng trực tiếp cho chú thích. Do đó, một số công trình hiện có cũng đã sử dụng hướng dẫn như một giám sát từ xa để thực hiện tăng cường dữ liệu hoặc đặc trưng. Ví dụ, Srivastava et al. (2017) đã sử dụng trình phân tích cú pháp ngữ nghĩa để chuyển đổi các giải thích ngôn ngữ tự nhiên thành dạng logic, và áp dụng chúng trên tất cả các thể hiện trong tập dữ liệu để tạo ra các đặc trưng nhị phân bổ sung. Wang et al. (2020) đã sử dụng các giải thích nhãn để chú thích corpus thô tự động và huấn luyện bộ phân loại trên dữ liệu nhiễu kết quả.

Ngoài tăng cường đơn giản, Su et al. (2023) đã sử dụng thêm hướng dẫn nhiệm vụ để làm phong phú biểu diễn mô hình và đạt được khả năng khái quát hóa chéo nhiệm vụ mạnh mẽ. Cụ thể, họ đã huấn luyện một mô hình embedding (một encoder duy nhất) trên các tập dữ liệu hướng dẫn đa dạng với học tương phản, và sau đó sử dụng mô hình này để tạo ra biểu diễn cụ thể cho nhiệm vụ dựa trên hướng dẫn cho các nhiệm vụ downstream chưa thấy.

### 9.3 Các Mô hình Ngôn ngữ Đa năng

Theo định nghĩa của Trí tuệ Nhân tạo Tổng quát (AGI), "mô hình đa năng" thường là một hệ thống có thể thành thạo các nhiệm vụ khác nhau và có thể mở rộng trong các ngữ cảnh thay đổi, sẽ vượt xa những kỳ vọng ban đầu của những người tạo ra nó. Trong khi cụ thể đối với lĩnh vực NLP, một mô hình ngôn ngữ đa năng được cho là một trợ lý đa nhiệm vụ xuất sắc, thành thạo trong việc xử lý nhiều nhiệm vụ NLP thực tế và các ngôn ngữ khác nhau, theo cách hoàn toàn zero/few-shot. Vì nhiều công trình hiện có đã chứng minh sức mạnh đáng kinh ngạc của việc sử dụng hướng dẫn trong khả năng khái quát hóa chéo nhiệm vụ, hướng dẫn có thể trở thành một bước đột phá trong việc đạt được mục tiêu cuối cùng này.

Đáng chú ý, các ứng dụng hướng dẫn đáng chú ý gần đây, đó là InstructGPT, ChatGPT, và GPT-4, cũng chỉ ra một bước lớn hướng đến việc xây dựng các mô hình ngôn ngữ đa năng. Ví dụ, trong quá trình pretraining của LLama-2, Touvron et al. (2023) đã sử dụng ý tưởng của context distilling để instill hướng dẫn trong LLM, do đó giải quyết vấn đề không nhất quán của việc tuân theo hướng dẫn trong tình huống hội thoại nhiều lượt. GPT-series của OpenAI áp dụng RLHF để phù hợp sở thích của mô hình với hướng dẫn con người, nơi giám sát phản hồi đóng một vai trò lớn. Mặc dù câu trả lời cho "Là hướng dẫn hay phản hồi con người, cái nào đóng góp nhiều hơn cho hiệu suất của ChatGPT?" vẫn mơ hồ và cần điều tra thêm, chúng tôi giới thiệu một số công trình gần đây làm nổi bật vai trò quan trọng của việc tuân theo hướng dẫn. Ví dụ, Chung et al. (2022) đã tiến hành các thí nghiệm rộng lớn để đánh giá sự phù hợp sở thích con người của PaLM. Họ phát hiện rằng, thậm chí không có bất kỳ phản hồi con người nào, việc tuân theo hướng dẫn đã làm giảm đáng kể độc tính trong các lần tạo sinh mở của PaLM, chẳng hạn như thiên lệch giới tính và nghề nghiệp. Ngoài ra, một số công trình khác cũng chỉ sử dụng hướng dẫn sáng tạo thay vì phản hồi con người và đạt được kết quả chéo nhiệm vụ đáng chú ý. Hơn nữa, vì vấn đề xung đột kiến thức của LLM có tác động đáng kể đến các ứng dụng của các mô hình được instruction-tuned, để làm cho LLM trở nên đa năng và hữu ích hơn trong thế giới thực, các công trình gần đây cũng đã sử dụng ý tưởng của việc tuân theo hướng dẫn để tăng cường các mô hình ngôn ngữ được tăng cường bằng retrieval, và ngược lại, cải thiện hướng dẫn bằng cách áp dụng kiến thức được lấy.

## 10 Kết luận

Khảo sát này tóm tắt các tài liệu hiện có về việc tuân theo hướng dẫn, cung cấp một cái nhìn tổng quan toàn diện về lĩnh vực này, bao gồm phân loại hướng dẫn, chiến lược mô hình hóa, và các khía cạnh chính của việc sử dụng hướng dẫn. Nó cũng giải quyết các thách thức độc đáo và đưa ra gợi ý cho nghiên cứu tương lai. Không giống như các công trình trước đây, chúng tôi vượt ra ngoài phạm vi hạn chế của việc tuân theo hướng dẫn hiện đại—chúng tôi truy tìm các nghiên cứu về việc tuân theo hướng dẫn trở lại giai đoạn đầu của học máy, và khám phá hướng dẫn văn bản như một giám sát gián tiếp cho LLM. Theo hiểu biết của chúng tôi, đây là khảo sát rộng lớn đầu tiên về việc tuân theo hướng dẫn. Tổng thể, chúng tôi nhắm đến việc cung cấp những hiểu biết sâu sắc có giá trị và truyền cảm hứng cho nghiên cứu sâu hơn trong lĩnh vực này.
