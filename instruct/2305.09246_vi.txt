# MAYBE ONLY 0.5% D ATA IS NEEDED : A P RELIMINARY
EXPLORATION OF LOWTRAINING DATA INSTRUCTION TUNING
MỘT BẢN THẢO

Hao Chen
Đại học Triết Giang
h.c.chen@zju.edu.cnYiming Zhang
Đại học Triết Giang
yimingz@zju.edu.cnQi Zhang
Đại học Triết Giang
cheung_se@zju.edu.cn

Hantao Yang
Đại học Triết Giang
ht.yang@zju.edu.cnXiaomeng Hu
Đại học Triết Giang
Xuetao Ma
Công ty Công nghệ Hàng Châu Trung Hạo Tân Doanh
maxuetao@gmail.comYifan Yanggong
Công ty Công nghệ Hàng Châu Trung Hạo Tân Doanh
baihu@cltech.comJunbo Zhaoy
Đại học Triết Giang
j.zhao@zju.edu.cn

17 tháng 5, 2023

TÓM TẮT
Điều chỉnh theo chỉ dẫn cho các mô hình ngôn ngữ lớn (LLM) đã thu hút sự chú ý của các nhà nghiên cứu do khả năng mở khóa tiềm năng của LLM trong việc tuân theo các chỉ dẫn. Trong khi điều chỉnh theo chỉ dẫn cung cấp các lợi thế để tạo điều kiện cho việc thích ứng của các mô hình ngôn ngữ lớn (LLM) với các tác vụ downstream như một phương pháp tinh chỉnh, việc huấn luyện các mô hình với hàng chục triệu hoặc thậm chí hàng tỷ tham số trên lượng dữ liệu lớn dẫn đến chi phí tính toán không thể chấp nhận được. Để giải quyết vấn đề này, chúng tôi tập trung vào việc giảm dữ liệu được sử dụng trong điều chỉnh theo chỉ dẫn LLM để giảm chi phí huấn luyện và cải thiện hiệu quả dữ liệu, được gọi là Điều chỉnh theo Chỉ dẫn với Dữ liệu Huấn luyện Thấp (LTD Instruction Tuning). Cụ thể, bài báo này thực hiện một khám phá sơ bộ về việc giảm dữ liệu được sử dụng trong huấn luyện LLM và xác định một số quan sát về chuyên môn hóa tác vụ cho huấn luyện LLM, chẳng hạn như tối ưu hóa hiệu suất cho một tác vụ cụ thể, số lượng loại chỉ dẫn cần thiết cho điều chỉnh theo chỉ dẫn, và lượng dữ liệu cần thiết cho các mô hình dành riêng cho tác vụ. Kết quả cho thấy các mô hình dành riêng cho tác vụ có thể được huấn luyện sử dụng ít hơn 0,5% của tập dữ liệu gốc, với cải thiện hiệu suất 2% so với những mô hình được huấn luyện trên toàn bộ dữ liệu liên quan đến tác vụ.

1 Giới thiệu

Với động lực to lớn của các mô hình ngôn ngữ lớn và hiệu suất ấn tượng của chúng [OpenAI, 2023a, Taylor et al., 2022, Touvron et al., 2023, Zhao et al., 2023], điều chỉnh theo chỉ dẫn như một trong những phương pháp điều chỉnh thích ứng của chúng với tinh chỉnh trên các mẫu được mô tả thông qua các chỉ dẫn [Longpre et al., 2023, Chung et al., 2022, Wei et al., 2022a], đã thu hút nhiều sự chú ý từ các nhà nghiên cứu [Ouyang et al., 2022]. Điều chỉnh theo chỉ dẫn có nghĩa là tinh chỉnh các mô hình ngôn ngữ lớn trên các mẫu được mô tả thông qua các chỉ dẫn [Longpre et al., 2023], như được hiển thị trong Hình 1. Tuy chưa có phân tích toàn diện, phương pháp điều chỉnh mới được trình bày này đã thể hiện sức mạnh vượt trội trong việc mở khóa các khả năng được trao cho của LLM về việc tuân theo chỉ dẫn. Trong số các công trình liên quan, điều chỉnh theo chỉ dẫn chủ yếu được sử dụng để căn chỉnh LLM với con người [Ouyang et al., 2022], khái quát hóa cho một số tác vụ chưa thấy [Sanh et al., 2021, Wei et al., 2022a], và chuyên môn hóa cho một tác vụ downstream nhất định [Wang et al., 2022, Jang et al., 2023]. So với tinh chỉnh thông thường, điều chỉnh theo chỉ dẫn cung cấp các lợi thế về việc yêu cầu ít dữ liệu hơn và thân thiện với con người hơn, và nhiều nhà nghiên cứu cũng coi điều chỉnh theo chỉ dẫn như một phương pháp tinh chỉnh mới để thích ứng LLM cho các tác vụ downstream tương ứng [Wei et al., 2022a, Puri et al., 2022, Jang et al., 2023].

Đóng góp bằng nhau và chia sẻ quyền tác giả đầu tiên.
yTác giả liên hệ.arXiv:2305.09246v1 [cs.AI] 16 tháng 5 2023

--- TRANG 2 ---
Mẫu arXiv MỘT BẢN THẢO

điều chỉnh theo chỉ dẫn cung cấp các lợi thế về việc yêu cầu ít dữ liệu hơn và thân thiện với con người hơn, và nhiều nhà nghiên cứu cũng coi điều chỉnh theo chỉ dẫn như một phương pháp tinh chỉnh mới để thích ứng LLM cho các tác vụ downstream tương ứng [Wei et al., 2022a, Puri et al., 2022, Jang et al., 2023].

Chỉ dẫn + Văn bản: Không tìm thấy Vũ khí Hủy diệt Hàng loạt ở Iraq. Câu hỏi: Điều này có ngụ ý rằng "Tìm thấy Vũ khí Hủy diệt Hàng loạt ở Iraq." không? Có hay không? Trả lời: KhôngNLIMẫu gốc: Văn bản 1: "Không tìm thấy Vũ khí Hủy diệt Hàng loạt ở Iraq." Văn bản 2: "Tìm thấy Vũ khí Hủy diệt Hàng loạt ở Iraq." Nhãn: 1∈{0,1}("không kéo theo")NLIMẫu gốc: Tiền đề: "Cơ thể tôi tạo bóng trên cỏ." Câu hỏi: nguyên nhânLựa chọn 1: "Mặt trời đang mọc." Lựa chọn 2: "Cỏ được cắt." Nhãn: 0∈{0,1}("Lựa chọn 1")SC

Chỉ dẫn + Văn bản: Cơ thể tôi tạo bóng trên cỏ. Điều này xảy ra vì... Giúp tôi chọn lựa chọn hợp lý hơn:-Mặt trời đang mọc.-Cỏ được cắt.Trả lời: Mặt trời đang mọc.SC

Hình 1: Một minh họa về sự khác biệt giữa tinh chỉnh và điều chỉnh theo chỉ dẫn, lấy tác vụ suy luận ngôn ngữ tự nhiên và lý luận nhân quả làm ví dụ. LLM dự đoán nhãn cho các mẫu trong tinh chỉnh trong khi trả lời câu hỏi cho tập chỉ dẫn trong điều chỉnh theo chỉ dẫn. Tác vụ Suy luận Ngôn ngữ Tự nhiên (NLI) liên quan đến việc xác định mối quan hệ logic giữa hai đoạn văn bản, thường được gọi là "tiền đề" và "giả thuyết." Mục tiêu của NLI là xác định xem giả thuyết là đúng, sai, hay không xác định dựa trên thông tin được cung cấp trong tiền đề. Hoàn thành Câu (SC) liên quan đến việc dự đoán từ hoặc chuỗi từ có khả năng nhất để hoàn thành một câu hoặc cụm từ cho trước.

Mặc dù các LLM được điều chỉnh theo chỉ dẫn rất mạnh mẽ, việc huấn luyện các mô hình với hàng chục triệu hoặc thậm chí hàng tỷ tham số thường gặp phải vấn đề về chi phí huấn luyện và, theo các định luật tỷ lệ [Kaplan et al., 2020], yêu cầu lượng dữ liệu lớn. Hầu hết các công trình hiện tại liên quan đến điều chỉnh theo chỉ dẫn luôn mở rộng lượng dữ liệu hoặc sự đa dạng của các chỉ dẫn được sử dụng cho điều chỉnh theo chỉ dẫn, ví dụ, bộ sưu tập FLAN [Longpre et al., 2023] chứa 15M ví dụ bao gồm 62 tác vụ, và tập dữ liệu P3 [Sanh et al., 2021] chứa 250B token với 10 loại chỉ dẫn. Tuy nhiên, với sự gia tăng quy mô của LLM từ 400M [Devlin et al., 2019] đến 540B [Chowdhery et al., 2022] hoặc thậm chí quy mô lớn hơn trong tương lai, quy mô của dữ liệu huấn luyện được sử dụng cho điều chỉnh theo chỉ dẫn sẽ ảnh hưởng lớn đến chi phí huấn luyện. Ví dụ, OpenAI đã liệt kê chi phí cho việc điều chỉnh theo chỉ dẫn Davinci [OpenAI, 2023b] với 0,03 đô la per 1k token, có nghĩa là huấn luyện một mô hình với P3 sẽ tốn 7,5M đô la.

Tuy nhiên, chúng tôi cho rằng thiếu nghiên cứu liên quan đến điều chỉnh theo chỉ dẫn về việc giảm lượng dữ liệu được sử dụng cho giai đoạn huấn luyện để giảm chi phí huấn luyện. Trong khi chi phí huấn luyện thường liên quan đến điều kiện phần cứng và các hoạt động kỹ thuật, hiệu quả dữ liệu có thể được cải thiện với sự trợ giúp của các thuật toán, do đó giảm chi phí của dữ liệu huấn luyện. Ví dụ, self-instruct [Wang et al., 2022] giảm số lượng trường hợp tương ứng với mỗi chỉ dẫn và sử dụng 80K ví dụ để điều chỉnh theo chỉ dẫn Davinci với chi phí 338 đô la. Chúng ta có thể gọi những phương pháp này giảm quy mô dữ liệu huấn luyện trong quá trình điều chỉnh theo chỉ dẫn là Điều chỉnh theo Chỉ dẫn với Dữ liệu Huấn luyện Thấp (LTD Instruction tuning) trong phần dưới.

Xem xét điều chỉnh theo chỉ dẫn như một phương pháp tinh chỉnh cho các tác vụ chuyên môn hóa, chúng tôi khám phá một hướng khác của điều chỉnh theo chỉ dẫn LTD - giảm sự đa dạng của các tác vụ và chỉ dẫn. Trong khi hầu hết các công trình dựa trên điều chỉnh theo chỉ dẫn tập trung vào khả năng khái quát hóa của LLM, ít ai tập trung vào chuyên môn hóa. Nếu LLM chỉ cần được điều chỉnh cho một tác vụ nhất định thì sao? Chúng ta cần bao nhiêu chỉ dẫn? Mô hình cần bao nhiêu ví dụ huấn luyện? Những câu hỏi này vẫn chưa được khám phá đầy đủ.

Trong bài báo này, chúng tôi thực hiện một khám phá sơ bộ về việc giảm dữ liệu được sử dụng trong giai đoạn huấn luyện LLM, từ góc độ của chính dữ liệu, để giảm chi phí huấn luyện và cải thiện hiệu quả dữ liệu. Cụ thể, chúng tôi nhằm mục đích xác định các mẫu cốt lõi có giá trị nhất từ dữ liệu hiện có để giúp mô hình có được kiến thức cần thiết cho các tác vụ downstream, và đạt được hiệu suất tương đương hoặc thậm chí tốt hơn chỉ với một lượng nhỏ dữ liệu. Do đó, sau khi chọn một tác vụ cụ thể và định dạng chỉ dẫn, chúng tôi thành công huấn luyện một mô hình dành riêng cho tác vụ sử dụng ít hơn 0,5% của tập dữ liệu gốc, với hiệu suất tương đương so với mô hình được huấn luyện trên dữ liệu liên quan đến tác vụ trong P3. Các quan sát của chúng tôi về tác vụ suy luận ngôn ngữ tự nhiên (NLI) như sau, và chúng tạo ra một số phát hiện chính liên quan đến chuyên môn hóa tác vụ cho huấn luyện LLM, mà chúng tôi hy vọng có thể cung cấp một số hiểu biết cho cộng đồng.

2

--- TRANG 3 ---
Mẫu arXiv MỘT BẢN THẢO

•Nếu chỉ để tối ưu hóa hiệu suất cho một tác vụ cụ thể, một mô hình LLM được điều chỉnh chỉ trên dữ liệu tác vụ đích có khả năng vượt trội hơn một mô hình được điều chỉnh trên dữ liệu từ các loại tác vụ khác nhau.

•Khi chuyên môn hóa trong một tác vụ duy nhất, có vẻ như chỉ một chỉ dẫn có thể đủ cho điều chỉnh theo chỉ dẫn. Mặc dù tăng số lượng loại chỉ dẫn có thể cải thiện hiệu suất, hiệu ứng cận biên trở nên ít đáng kể hơn, và thậm chí có thể có những trường hợp một chỉ dẫn duy nhất vượt trội hơn mười loại chỉ dẫn.

•Trái ngược với việc huấn luyện một mô hình cho hiệu suất tác vụ tổng thể, kết quả của chúng tôi cũng cho thấy 16k trường hợp (1,9M token, 0,5% của P3) có thể đủ để huấn luyện một mô hình dành riêng cho tác vụ NLI.

2 Công trình liên quan

Các mô hình ngôn ngữ lớn (LLM). Các mô hình ngôn ngữ lớn (LLM) thường đề cập đến các mô hình ngôn ngữ với hàng chục hoặc hàng trăm tỷ tham số và được huấn luyện với dữ liệu khổng lồ, ví dụ, GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023a], Galactica [Taylor et al., 2022], LLaMA [Touvron et al., 2023], v.v. Nhiều nghiên cứu đã chỉ ra rằng các mô hình này có khả năng nổi sinh chưa được khám phá so với các mô hình nhỏ khi quy mô vượt quá một mức nhất định [Kaplan et al., 2020, Wei et al., 2022b]. Zhao et al. [2023] kết luận rằng các khả năng nổi sinh hiện tại của các mô hình lớn bao gồm chủ yếu học tập trong ngữ cảnh, giúp các mô hình có khả năng thích ứng với các tác vụ downstream mà không cần cập nhật gradient, chỉ với một vài ví dụ hoặc một số minh chứng tác vụ [Ye et al., 2023, Dong et al., 2023]. Một số nhà nghiên cứu cũng coi hoạt động này như chỉ dẫn [Ye et al., 2023]. Một khả năng nổi sinh khác là tuân theo chỉ dẫn [Ouyang et al., 2022]. Bằng cách thêm mô tả tác vụ vào dữ liệu, các LLM có thể hiểu yêu cầu của một tác vụ mà không cần mẫu bổ sung trên các tác vụ downstream chưa thấy [Zhou et al., 2022, Si et al., 2022, Wei et al., 2022a, Sumers et al., 2022], hoặc điều chỉnh với những dữ liệu được định dạng lại này để trao cho mô hình khả năng dành riêng cho tác vụ [Longpre et al., 2023, Wang et al., 2022, Sanh et al., 2021, Gupta et al., 2022, Chung et al., 2022, Ivison et al., 2022a, Jang et al., 2023]. Nhiều công trình cũng tìm thấy một khả năng quan trọng của lý luận từng bước (còn được gọi là chuỗi suy nghĩ) [Wei et al., 2022c, Wang et al., 2023], giúp LLM đưa ra câu trả lời cuối cùng bằng cách chia nhỏ tác vụ và sử dụng một hình thức của các bước lý luận trung gian.

Điều chỉnh theo chỉ dẫn. Mặc dù các LLM tuân theo chỉ dẫn hiện tại đã thể hiện hiệu suất mạnh mẽ trong việc đưa ra các câu trả lời liên quan đến tác vụ dựa trên chỉ dẫn [Si et al., 2022, Wei et al., 2022a, Sanh et al., 2021, Zhou et al., 2022], khi đối mặt với các vấn đề dành riêng cho tác vụ, tinh chỉnh vẫn là lựa chọn ưa thích để đạt được kết quả tốt hơn [Lou et al., 2023, Zhao et al., 2023]. Không giống như chỉ sử dụng chỉ dẫn mà không huấn luyện để hướng dẫn đầu ra mô hình, điều chỉnh theo chỉ dẫn là một phương pháp tinh chỉnh LLM với dữ liệu được kết hợp với chỉ dẫn để đạt được hiệu ứng dành riêng cho tác vụ [Sanh et al., 2021, Longpre et al., 2023, Puri et al., 2022, Jang et al., 2023, Ivison et al., 2022a,b]. Hầu hết các công trình tập trung vào khả năng khái quát hóa được mang lại bởi điều chỉnh theo chỉ dẫn, giúp mô hình có khả năng khái quát hóa xuyên tác vụ bằng cách tinh chỉnh trên dữ liệu chỉ dẫn đa tác vụ [Sanh et al., 2021, Longpre et al., 2023]. Ngoài ra, điều chỉnh theo chỉ dẫn cũng có thể giúp mô hình cải thiện hiệu suất trên một tác vụ cụ thể [Wang et al., 2022, Jang et al., 2023, Ivison et al., 2022a,b], và nghiên cứu đã chỉ ra rằng trong tinh chỉnh tác vụ đơn, điều chỉnh theo chỉ dẫn có thể đẩy nhanh sự hội tụ của mô hình.

Dữ liệu huấn luyện thấp. Mặc dù hiệu quả của LLM là tuyệt vời, chi phí huấn luyện liên quan đến các mô hình tham số khổng lồ đồng thời đã hạn chế sự phổ biến và việc áp dụng LLM [Hoffmann et al., 2022, Zhao et al., 2023]. Nhiều công trình cố gắng khám phá khả năng giảm chi phí trong LLM từ góc độ dữ liệu. Jang et al. [2023] báo cáo một LLM chuyên gia được tinh chỉnh trên một tác vụ duy nhất có thể vượt trội hơn một LLM được điều chỉnh đa tác vụ. Self-instruct [Wang et al., 2022] bắt đầu từ việc tạo chỉ dẫn, có nghĩa là một mô hình tạo ra các chỉ dẫn hoặc lời nhắc của riêng mình và học cách tuân theo chúng. DEFT [Ivison et al., 2022a] giả định sự hiện diện của dữ liệu liên quan đến tác vụ không được gắn nhãn, và bằng cách truy xuất dữ liệu sử dụng K-láng giềng gần nhất từ nhóm dữ liệu có độ tương tự cao với dữ liệu tác vụ, các mô hình tinh chỉnh mới có thể đạt được hiệu suất tương tự như các mô hình được huấn luyện trên toàn bộ tập dữ liệu. HINT [Ivison et al., 2022b] kết hợp chỉ dẫn vào các tham số mô hình để giảm số lượng token tương ứng với chỉ dẫn trong mỗi đầu vào huấn luyện để tiết kiệm chi phí token.

3 Phương pháp

Như đã đề cập trong phần giới thiệu, ý tưởng chính của chúng tôi để hạn chế quy mô dữ liệu là giảm sự đa dạng của các chỉ dẫn và tập trung vào các tác vụ chuyên môn hóa. Và trong phần này, chúng tôi sẽ giới thiệu cách giảm toàn bộ quy mô tập dữ liệu dựa trên cả hai ý tưởng một cách riêng biệt.

3.1 Động lực

Chúng tôi bắt đầu bằng việc giới thiệu động lực cho phương pháp của chúng tôi trước khi giải thích chi tiết của nó. Hiện tại, sự phát triển của LLM đã nhận được sự chú ý to lớn, nhưng chi phí huấn luyện cao được mang lại bởi mô hình với các tham số lớn cũng hạn chế sự phổ biến và ứng dụng của LLM. Chúng tôi hy vọng khám phá cách cải thiện hiệu quả của LLM từ góc độ giảm chi phí huấn luyện của nó. Các chi phí chính trong việc sử dụng LLM hiện tại bao gồm chi phí huấn luyện và chi phí dữ liệu [Zhao et al., 2023]. Chi phí huấn luyện bao gồm việc sử dụng API công cộng hoặc tự tinh chỉnh các mô hình lớn, chủ yếu đối mặt với yêu cầu phần cứng hoặc song song hóa, và thuật toán đóng vai trò nhẹ trong đó, trong khi đối với chi phí dữ liệu, các thuật toán tập trung vào dữ liệu có thể được sử dụng. Do đó, chúng tôi hy vọng khám phá cách giảm dữ liệu được sử dụng trong huấn luyện LLM từ góc độ của chính dữ liệu để giảm chi phí huấn luyện và cải thiện hiệu quả sử dụng dữ liệu. Đó là, để truy xuất các mẫu cốt lõi hữu ích nhất từ dữ liệu hiện có để giúp mô hình học kiến thức cần thiết cho các tác vụ downstream, và đạt được hiệu suất tốt chỉ với một lượng nhỏ mẫu.

Tổng hợp trung bình
Chuẩn hóa L2
Chỉ dẫn + Văn bản: Cơ thể tôi tạo bóng trên cỏ. Điều này xảy ra vì... Giúp tôi chọn lựa chọn hợp lý hơn:-Mặt trời đang mọc.-Cỏ được cắt.
Tác vụ
Mã hóa…

Phân cụm
Lấy mẫu
Chiếu

Chỉ dẫn + Văn bản: Cơ thể tôi tạo bóng trên cỏ. Điều này xảy ra vì... Giúp tôi chọn lựa chọn hợp lý hơn:-Mặt trời đang mọc.-Cỏ được cắt.
Tác vụ
Trả lời:Mặt trời đang mọc
Tác vụ
LLM
Điều chỉnh
Suy luận

Hình 2: Đường ống của phương pháp được đề xuất của chúng tôi. Đầu tiên, mã hóa mỗi câu thành embeddings và tiền xử lý với tổng hợp trung bình và chuẩn hóa L2. Sau đó, trong không gian tiềm ẩn, chúng tôi phân cụm tất cả các điểm mẫu thành một vài cụm. Sau đó sử dụng các phương pháp lấy mẫu trên những mẫu được phân cụm này để tìm các mẫu cốt lõi của phân phối gốc. Cuối cùng, sử dụng những mẫu được truy xuất này để điều chỉnh theo chỉ dẫn các LLM và thực hiện đánh giá. Ba hình chữ nhật đại diện cho không gian tiềm ẩn, và một chuỗi màu trong không gian tiềm ẩn đề cập đến một tác vụ. Các điểm của cùng một chuỗi màu nhưng có độ bóng khác nhau tương ứng với dữ liệu từ cùng một tác vụ nhưng từ các tập dữ liệu khác nhau, ví dụ, tác vụ NLI có năm tập dữ liệu, do đó tạo ra năm độ bóng khác nhau.

3.2 Lựa chọn dựa trên Coreset

Do ranh giới mơ hồ của các tác vụ NLP [Zhao et al., 2023], các mẫu từ các tác vụ khác nhau có thể có khả năng phân biệt thấp, và thường không khả thi khi lựa chọn thủ công các mẫu phù hợp hơn cho các tác vụ NLP. Do đó, chúng tôi đề xuất một phương pháp lựa chọn dữ liệu liên quan đến tác vụ dựa trên coreset để tự động truy xuất các mẫu cốt lõi từ tập dữ liệu tác vụ, để giúp huấn luyện LLM dành riêng cho tác vụ với ít mẫu hơn. Cụ thể, thuật toán này bao gồm các bước sau:

Embedding câu và tiền xử lý Đầu tiên, chúng tôi định dạng lại dữ liệu thành định dạng đầu vào huấn luyện được sử dụng trong giai đoạn huấn luyện điều chỉnh theo chỉ dẫn, tức là dữ liệu với các chỉ dẫn mô tả, thêm câu trả lời ở cuối để định dạng một dữ liệu huấn luyện hoàn chỉnh. Sau đó, chúng tôi mã hóa tất cả các mẫu sử dụng một mô hình ngôn ngữ được huấn luyện trước (ví dụ, Galactica [Taylor et al., 2022] hoặc Bert [Devlin et al., 2019]). Cụ thể, chúng tôi trích xuất last_hidden_state của mỗi mẫu sau khi đưa vào mô hình làm word embeddings hoặc mỗi câu. Lưu ý rằng các mô hình ngôn ngữ có mặt nạ như Bert [Devlin et al., 2019] có cls token làm sentence embedding cho một câu đầu vào, nhưng đối với các mô hình sinh như chuỗi GPT [Brown et al., 2020, OpenAI, 2023a], chúng không có token này. Theo Reimers và Gurevych [2019], chúng tôi thực hiện tổng hợp trung bình trên word embeddings của mỗi mẫu và thu được một vector một chiều làm sentence embedding cho mẫu này. Để tăng tốc tính toán và tạo điều kiện cho việc tính toán độ tương tự vector, chúng tôi chuẩn hóa tất cả sentence embeddings về độ dài 1, tức là thực hiện chuẩn hóa L2 trên chiều embedding.

4

--- TRANG 5 ---
Mẫu arXiv MỘT BẢN THẢO

Phân cụm Trong bước phân cụm, chúng tôi xem xét rằng sự mờ nhạt của ranh giới tác vụ NLP có thể gây ra ít biến thiên giữa các mẫu từ các tác vụ khác nhau. Do đó chúng tôi tiếp cận phân cụm không giám sát bằng cách tập trung vào biểu diễn dữ liệu, thay vì dựa vào thông tin nhãn để nhóm các điểm dữ liệu lại với nhau dựa trên cùng các danh mục hoặc tác vụ. Cụ thể, sau khi thu được sentence embeddings từ bước đầu tiên, chúng tôi thực hiện phân cụm không giám sát sử dụng K-Means [Lloyd, 1982] trong không gian embedding để thu được ánh xạ của mỗi mẫu và nhãn cụm tương ứng của nó. Sau đó, dựa trên tần suất của các mẫu từ một tác vụ downstream xuất hiện trong một số cụm, chúng tôi chọn điểm trung tâm của cụm có tần suất cao nhất làm điểm trung tâm phân phối của tác vụ downstream đó. Tiếp theo, đối với tất cả các mẫu trong tác vụ, chúng tôi tính toán độ tương tự cosine với điểm trung tâm phân phối (việc lựa chọn hàm khoảng cách có ít ảnh hưởng đến kết quả, và chúng tôi theo OpenAI [2023a] để chọn độ tương tự cosine), và tìm mẫu gần nhất từ dữ liệu tác vụ đến điểm trung tâm này làm điểm trung tâm tác vụ. Lưu ý rằng điểm trung tâm phân phối là trung tâm của dữ liệu tác vụ này trong không gian embedding, có thể không tồn tại trong dữ liệu tác vụ, trong khi điểm trung tâm tác vụ là một mẫu chính xác từ dữ liệu tác vụ này với độ tương tự cosine lớn nhất so với điểm trung tâm phân phối.

Lấy mẫu Coreset Một cách trực quan, sau khi thu được điểm trung tâm phân phối tương ứng với tác vụ downstream, chúng tôi có thể chọn mẫu tương tự nhất làm mẫu tác vụ đại diện dựa trên độ tương tự cosine, như đã làm trong [Ivison et al., 2022a], đã đạt được kết quả tốt. Tuy nhiên, phương pháp truy xuất của họ chọn các mẫu có độ tương tự cao từ nhóm dữ liệu dựa trên các mẫu hiện có để cải thiện hiệu suất tác vụ, có thể được coi như một hình thức tăng cường dữ liệu thông qua truy xuất. Điều này mâu thuẫn với mục tiêu của chúng tôi là giảm dữ liệu cần thiết cho huấn luyện. Chúng tôi nhằm mục đích tìm một tập nhỏ xấp xỉ phân phối của toàn bộ tập dữ liệu sử dụng ít mẫu nhất có thể. Do đó, phương pháp K-láng giềng gần nhất trong DEFT [Ivison et al., 2022a] không phù hợp cho tình huống này vì các mẫu có độ tương tự cao không xấp xỉ phân phối tập đầy đủ [Sener và Savarese, 2018]. Để đạt được việc lấy mẫu các mẫu cốt lõi, chúng tôi đã sử dụng một thuật toán coreset KCentergreedy [Sener và Savarese, 2018], nhằm mục đích chọn k điểm trung tâm sao cho tối thiểu hóa khoảng cách lớn nhất giữa một điểm dữ liệu ngẫu nhiên và trung tâm gần nhất của nó, và đã được chứng minh hiệu quả trong việc thu được một tập các mẫu cốt lõi của một phân phối.

Chúng tôi sử dụng điểm trung tâm mẫu tác vụ làm trung tâm ban đầu, đưa vào tất cả sentence embeddings của các mẫu tác vụ thu được trong các bước trước, và sử dụng thuật toán KCenterGreedy để thu thập một tập các mẫu cốt lõi từ các mẫu tác vụ theo tỷ lệ cho trước. Tập con của tập dữ liệu tác vụ gốc được thu thập có thể đạt được hiệu suất tương tự hoặc thậm chí cao hơn với ít dữ liệu hơn.

3.3 Tiếp tục...

Cần lưu ý rằng ngoài phương pháp này, chúng tôi cũng đã khám phá hai cách khác để giảm dữ liệu huấn luyện cần thiết bởi tinh chỉnh. Tuy nhiên, do hạn chế về sức mạnh tính toán và thời gian, chúng chưa đủ hoàn chỉnh để báo cáo. Vui lòng chờ đợi các bài báo tương lai của chúng tôi.

4 Thí nghiệm

4.1 Thiết lập

Tập dữ liệu Theo thiết lập trong P3[Sanh et al., 2021], chúng tôi thực hiện thí nghiệm trên tổng cộng 11 tập dữ liệu, trải rộng qua 4 tác vụ NLP, cụ thể là Suy luận Ngôn ngữ Tự nhiên (NLI, 1,9M token), Bổ sung Câu (SC, 660,6K token), Định nghĩa Từ theo Ngữ cảnh (WSD, 25,5K token) và Phân giải Đồng tham chiếu (CR, 185,1K token). Ngược lại, toàn bộ tập dữ liệu liên quan đến tác vụ của P3 chứa 382,8M token.

Cụ thể, đối với tác vụ Suy luận Ngôn ngữ Tự nhiên, chúng tôi sử dụng các tập dữ liệu RTE [Dagan et al., 2006], CB [Wang et al., 2020], và ANLI [Nie et al., 2020], trong khi đối với tác vụ Bổ sung Câu, chúng tôi sử dụng COPA [Wang et al., 2020], HellaSwag [Zellers et al., 2019], và Story Cloze [Mostafazadeh et al., 2016]. Đối với tác vụ Phân giải Đồng tham chiếu, chúng tôi sử dụng các tập dữ liệu Winogrande [Sakaguchi et al., 2019] và WSC [Wang et al., 2020], và đối với Định nghĩa Từ theo Ngữ cảnh, chúng tôi sử dụng WIC [Wang et al., 2020]. Hơn nữa, để tạo tập dữ liệu kiểu chỉ dẫn, chúng tôi chọn ngẫu nhiên chỉ một lời nhắc từ mỗi tập dữ liệu.

Mô hình Chúng tôi sử dụng mô hình Galactica-1.3b [Taylor et al., 2022] để thực hiện thí nghiệm trong nghiên cứu của chúng tôi. Các mô hình Galactica được huấn luyện trên một kho tài liệu khoa học khổng lồ và được thiết kế để xử lý các tác vụ khoa học như dự đoán trích dẫn, trả lời câu hỏi khoa học, lý luận toán học, tóm tắt, tạo tài liệu, dự đoán tính chất phân tử, và trích xuất thực thể. Theo [Brown et al., 2020], tương tự như giai đoạn huấn luyện trước, chúng tôi coi tất cả các tập dữ liệu như các tác vụ dự đoán token tiếp theo. Cụ thể, chúng tôi sử dụng trình tối ưu AdamW với tốc độ học 1e-5.

5

--- TRANG 6 ---
Mẫu arXiv MỘT BẢN THẢO

Đánh giá Nghiên cứu trước về điều chỉnh theo chỉ dẫn đã không thể nêu rõ các phương pháp đánh giá được sử dụng một cách rõ ràng. Trong bài báo này, chúng tôi giới thiệu phương pháp đánh giá của chúng tôi, có thể phục vụ như một tài liệu tham khảo cho các nhà nghiên cứu khác làm việc trong lĩnh vực này. Khi một chuỗi đã được token hóa x và một lựa chọn câu trả lời đã được token hóa y (với độ dài l) được cung cấp làm đầu vào cho mô hình, một ma trận xác suất P l×vocab_size được tạo ra. Tiếp theo, đối với một lựa chọn câu trả lời yi, xác suất tương ứng pi của nó có thể được thu được bằng cách nhân các xác suất của mỗi token trong yi sử dụng công thức ∏li j=1 pij. Lựa chọn câu trả lời có xác suất cao nhất được mô hình coi là câu trả lời tối ưu.

4.2 Kết quả trên Các Tác vụ Suy luận Ngôn ngữ Tự nhiên

Chúng tôi mô tả trong phần này kết quả tác vụ NLI của việc sử dụng phương pháp của chúng tôi, như được thấy trong Bảng 1. Theo thông tin được trình bày trong bảng này, khi xem xét một tác vụ cụ thể (NLI trong trường hợp này), phương pháp của chúng tôi đạt được cải thiện hiệu suất 2% trung bình vượt quá đường cơ sở (P3 trong bảng) trên tác vụ NLI, chỉ sử dụng 0,5% dữ liệu có sẵn từ P3. So với việc sử dụng tất cả mười chỉ dẫn từ P3, chúng tôi thấy rằng việc chọn chỉ một chỉ dẫn cho phép chúng tôi đạt được kết quả tương đương với việc sử dụng toàn bộ tập dữ liệu từ P3 chỉ với 10% dữ liệu.

Liên quan đến các mô hình dành riêng cho tác vụ, hàng thứ hai và thứ tư đã chỉ ra rằng sự đa dạng của các tác vụ có thể có tác động tiêu cực. Hơn nữa, bằng cách chỉ sử dụng dữ liệu từ tác vụ NLI, chúng tôi thu được kết quả cao hơn khoảng 8% trung bình so với những kết quả từ P3.

Do đó, chúng tôi suy đoán rằng đối với các yêu cầu dành riêng cho tác vụ, việc chỉ sử dụng dữ liệu liên quan cho tác vụ đích và một chỉ dẫn duy nhất có thể hiệu quả hơn so với việc trực tiếp sử dụng các mô hình quy mô lớn. Lưu ý rằng những quan sát này có thể chỉ áp dụng cho tác vụ NLI, vì các tác vụ khác vẫn chưa được khám phá nhiều do hạn chế tính toán.

| Mô hình | RTE | CB | ANLI R1 | ANLI R2 | ANLI R3 | Trung bình |
|---------|-----|----|---------|---------|---------| ------------|
| Mô hình Vanilla (0%) | 54.51 | 41.07 | 33.40 | 33.40 | 33.58 | 39.19 |
| P3 (100%) | 76.17 | 75.00 | 44.00 | 35.70 | 39.42 | 54.06 |
| Chỉ dẫn Cố định (10%) | 71.11 | 66.07 | 43.60 | 38.90 | 42.17 | 52.37 |
| Liên quan NLI (5%) | 79.06 | 82.14 | 60.40 | 46.50 | 46.67 | 62.95 |
| NLI coreset (0.5%) | 74.73 | 73.21 | 49.60 | 41.90 | 43.75 | 56.64 |

Bảng 1: Độ chính xác kiểm tra (%) với các mô hình được sử dụng trên tác vụ NLI. Hàng đầu tiên gợi ý hiệu suất của mô hình vanilla (Galactica-1.3b ở đây) mà không có điều chỉnh theo chỉ dẫn. P3 đại diện cho việc sử dụng toàn bộ tập dữ liệu liên quan đến tác vụ từ P3 (10 chỉ dẫn với 11 tập dữ liệu). Chỉ dẫn cố định đại diện cho việc chỉ sử dụng một loại chỉ dẫn, kết quả là 10% của P3. Trong số tất cả 11 tập dữ liệu, dữ liệu liên quan đến tác vụ NLI chiếm 50%, do đó liên quan NLI đề cập đến huấn luyện chỉ với 5% của P3 (dữ liệu NLI). Với điều này, NLI coreset chỉ ra việc chỉ sử dụng 10% dữ liệu liên quan đến tác vụ NLI, do đó tạo ra 0,5% của P3.

4.3 Nghiên cứu Triệt tiêu cho Lấy mẫu

Liên quan đến chiến lược lấy mẫu mô hình, chúng tôi cũng đã khám phá một số phương pháp lấy mẫu thay thế. Bảng chỉ ra rằng việc chọn dữ liệu tương tự nhất hoặc khác biệt nhất dựa trên độ tương tự cosine cho kết quả kém đáng kể, thậm chí thấp hơn mô hình vanilla. Chúng tôi suy đoán rằng kết quả này có thể được quy cho vấn đề mất cân bằng khi lấy mẫu chỉ dựa trên độ tương tự trong nhóm tác vụ bao gồm năm tập dữ liệu này. Có khả năng rằng phần lớn các ví dụ được lấy mẫu bị chi phối bởi dữ liệu RTE hoặc ANLI.

| Phương pháp Lấy mẫu | RTE | CB | ANLI R1 | ANLI R2 | ANLI R3 | Trung bình |
|---------------------|-----|----|---------|---------|---------| ------------|
| Mô hình Vanilla (0%) | 54.51 | 41.07 | 33.40 | 33.40 | 33.58 | 39.19 |
| coreset (0.5%) | 74.73 | 73.21 | 49.60 | 41.90 | 43.75 | 56.64 |
| topK (0.5%) | 47.29 | 10.71 | 33.40 | 33.20 | 33.92 | 31.70 |
| leastK (0.5%) | 50.18 | 8.93 | 33.80 | 33.60 | 34.08 | 32.12 |
| mixed (0.5%) | 47.29 | 8.93 | 33.30 | 33.30 | 33.50 | 31.26 |

Bảng 2: Nghiên cứu triệt tiêu về độ chính xác kiểm tra (%) của tác vụ NLI sử dụng 10% dữ liệu (0,5% của toàn bộ tập dữ liệu liên quan đến tác vụ từ P3). Hàng đầu tiên đại diện cho mô hình vanilla mà không có phương pháp lấy mẫu nào (0% của P3). Coreset đề cập đến phương pháp của chúng tôi sử dụng các mẫu cốt lõi của tập dữ liệu NLI. TopK sử dụng các mẫu gần với điểm trung tâm phân phối NLI, trong khi leastK sử dụng các mẫu ít gần nhất. Mixed chỉ ra việc trộn các mẫu gần nhất và ít gần nhất. Số lượng mẫu được sử dụng là giống nhau cho tất cả các phương pháp (10% của tác vụ NLI, ~16k mẫu).

6

--- TRANG 7 ---
Mẫu arXiv MỘT BẢN THẢO

5 Kết luận và Công việc Tương lai

Trong bài báo này, chúng tôi trình bày kết quả thí nghiệm từ khám phá sơ bộ của chúng tôi về Điều chỉnh theo Chỉ dẫn với Dữ liệu Huấn luyện Thấp, bằng cách điều chỉnh Mô hình Galactica-1.3b trên tập dữ liệu P3 cho tác vụ NLI. Nghiên cứu của chúng tôi đã tiết lộ một số phát hiện:

1. các mô hình dành riêng cho tác vụ có thể hưởng lợi từ các loại tác vụ cố định để đạt được hiệu suất vượt trội;
2. sự đa dạng của các định dạng chỉ dẫn có thể có tác động tối thiểu đến hiệu suất mô hình dành riêng cho tác vụ;
3. thậm chí một lượng nhỏ dữ liệu (1,9M token) có thể dẫn đến kết quả đầy hứa hẹn trong điều chỉnh theo chỉ dẫn cho các mô hình dành riêng cho tác vụ.

Cần lưu ý rằng công trình của chúng tôi có một số hạn chế do các ràng buộc của tài nguyên tính toán, chẳng hạn như chỉ thực hiện thí nghiệm trên Galactica-1.3b và chỉ sử dụng dữ liệu tác vụ NLI từ tập dữ liệu P3.

Chúng tôi hy vọng những phát hiện sơ bộ của chúng tôi có thể cung cấp hiểu biết cho cộng đồng về Điều chỉnh theo Chỉ dẫn với Dữ liệu Huấn luyện Thấp, và tạo ra một góc nhìn mới về điều chỉnh theo chỉ dẫn cho các nhà nghiên cứu. Đối với công việc tương lai, chúng tôi dự định xác thực những ý tưởng này trên các mô hình lớn hơn sử dụng một phạm vi toàn diện hơn của các tác vụ và tập dữ liệu.

Tài liệu tham khảo

OpenAI. Báo cáo kỹ thuật gpt-4, 2023a.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, và Robert Stojnic. Galactica: Một mô hình ngôn ngữ lớn cho khoa học, 2022.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả, 2023.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. Một khảo sát về các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2303.18223, 2023.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. Bộ sưu tập flan: Thiết kế dữ liệu và phương pháp cho điều chỉnh theo chỉ dẫn hiệu quả. arXiv preprint arXiv:2301.13688, 2023.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Mở rộng các mô hình ngôn ngữ được tinh chỉnh theo chỉ dẫn. arXiv preprint arXiv:2210.11416, 2022.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. Các mô hình ngôn ngữ được tinh chỉnh là người học zero-shot, 2022a.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Huấn luyện các mô hình ngôn ngữ để tuân theo chỉ dẫn với phản hồi của con người. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Huấn luyện được nhắc nhở đa tác vụ cho phép khái quát hóa tác vụ zero-shot. arXiv preprint arXiv:2110.08207, 2021.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và Hannaneh Hajishirzi. Self-instruct: Căn chỉnh mô hình ngôn ngữ với các chỉ dẫn được tạo tự động. arXiv preprint arXiv:2212.10560, 2022.

Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, và Minjoon Seo. Khám phá lợi ích của việc huấn luyện các mô hình ngôn ngữ chuyên gia so với điều chỉnh theo chỉ dẫn. arXiv preprint arXiv:2302.03202, 2023.

Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, và Chitta Baral. Bao nhiêu mẫu dữ liệu tương đương với một chỉ dẫn bổ sung? arXiv preprint arXiv:2203.09161, 2022.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Định luật tỷ lệ cho các mô hình ngôn ngữ thần kinh. arXiv preprint arXiv:2001.08361, 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Huấn luyện trước các transformer hai chiều sâu để hiểu ngôn ngữ, 2019.

7

--- TRANG 8 ---
Mẫu arXiv MỘT BẢN THẢO

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways, 2022.

OpenAI. Giá cả api openai, 2023b. https://openai.com/pricing [Truy cập: (2023-04-29)].

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Các mô hình ngôn ngữ là người học few-shot, 2020.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Khả năng nổi sinh của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2206.07682, 2022b.

Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, và Minjoon Seo. Học tập chỉ dẫn trong ngữ cảnh. arXiv preprint arXiv:2302.14691, 2023.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, và Zhifang Sui. Một khảo sát về học tập trong ngữ cảnh, 2023.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, và Jimmy Ba. Các mô hình ngôn ngữ lớn là kỹ sư prompt cấp độ con người. arXiv preprint arXiv:2211.01910, 2022.

Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, và Lijuan Wang. Thúc đẩy gpt-3 để có thể tin cậy. arXiv preprint arXiv:2210.09150, 2022.

Theodore Sumers, Robert Hawkins, Mark K Ho, Tom Griffiths, và Dylan Hadfield-Menell. Cách nói chuyện để ai sẽ học: Chỉ dẫn, mô tả, và tự chủ. Advances in Neural Information Processing Systems, 35:34762–34775, 2022.

Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, và Jeffrey P Bigham. Cải thiện khái quát hóa zero và few-shot trong đối thoại thông qua điều chỉnh theo chỉ dẫn. arXiv preprint arXiv:2205.12673, 2022.

Hamish Ivison, Noah A Smith, Hannaneh Hajishirzi, và Pradeep Dasigi. Tinh chỉnh hiệu quả dữ liệu sử dụng láng giềng gần nhất xuyên tác vụ. arXiv preprint arXiv:2212.00196, 2022a.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny Zhou. Lời nhắc chuỗi suy nghĩ gợi ra lý luận trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2201.11903, 2022c.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, và Denny Zhou. Tự nhất quán cải thiện lý luận chuỗi suy nghĩ trong các mô hình ngôn ngữ, 2023.

Renze Lou, Kai Zhang, và Wenpeng Yin. Prompt có phải là tất cả những gì bạn cần không? Không. Một cái nhìn toàn diện và rộng hơn về học tập chỉ dẫn. arXiv preprint arXiv:2303.10475, 2023.

Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, và Matthew Peters. Hint: Điều chỉnh chỉ dẫn hypernetwork để khái quát hóa zero-shot hiệu quả. arXiv preprint arXiv:2212.10315, 2022b.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Huấn luyện các mô hình ngôn ngữ lớn tối ưu tính toán. arXiv preprint arXiv:2203.15556, 2022.

Nils Reimers và Iryna Gurevych. Sentence-bert: Sentence embeddings sử dụng mạng bert siamese, 2019.

Stuart Lloyd. Lượng tử hóa bình phương tối thiểu trong pcm. IEEE transactions on information theory, 28(2):129–137, 1982.

Ozan Sener và Silvio Savarese. Học tập tích cực cho mạng thần kinh tích chập: Một phương pháp core-set, 2018.

Ido Dagan, Oren Glickman, và Bernardo Magnini. Thách thức pascal nhận dạng kéo theo văn bản. Trong Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, trang 177–190. Springer, 2006.

8

--- TRANG 9 ---
Mẫu arXiv MỘT BẢN THẢO

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. Superglue: Một benchmark dính hơn cho các hệ thống hiểu ngôn ngữ đa mục đích, 2020.

Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, và Douwe Kiela. Adversarial nli: Một benchmark mới cho hiểu ngôn ngữ tự nhiên, 2020.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Máy có thể thật sự hoàn thành câu của bạn không?, 2019.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, và James Allen. Một kho tài liệu và khung đánh giá để hiểu sâu hơn về các câu chuyện thường thức, 2016.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: Một thách thức lược đồ winograd đối kháng ở quy mô lớn, 2019.

9
