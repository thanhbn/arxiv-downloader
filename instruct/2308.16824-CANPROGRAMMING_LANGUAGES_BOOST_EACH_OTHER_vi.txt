# CODEM: CÁC NGÔN NGỮ LẬP TRÌNH CÓ THỂ THÚC ĐẨY LẪN NHAU
# THÔNG QUA ĐIỀU CHỈNH THEO HƯỚNG DẪN KHÔNG?
# BÁO CÁO KỸ THUẬT

Daoguang Zan†∗ Ailun Yu§∗ Bo Shen‡ Jiaxin Zhang‡ Taihong Chen‡ Bing Geng‡ Bei Chen¶
Jichuan Ji‡ Yafen Yao‡ Yongji Wang† Qianxiang Wang‡

†Institute of Software, Chinese Academy of Science
§Peking University  
‡Huawei Co., Ltd.
¶Independent Researcher

daoguang@iscas.ac.cn; yuailun@pku.edu.cn

TÓM TẮT

Khi các lập trình viên con người đã thành thạo một ngôn ngữ lập trình, họ sẽ dễ dàng hơn khi học một ngôn ngữ lập trình mới. Trong báo cáo này, chúng tôi tập trung khám phá liệu các ngôn ngữ lập trình có thể thúc đẩy lẫn nhau trong giai đoạn tinh chỉnh theo hướng dẫn của các mô hình ngôn ngữ lớn cho mã code hay không. Chúng tôi tiến hành các thí nghiệm mở rộng với 8 ngôn ngữ lập trình phổ biến (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) trên StarCoder. Kết quả cho thấy các ngôn ngữ lập trình có thể cải thiện đáng kể lẫn nhau. Ví dụ, CODEM-Python 15B được huấn luyện trên Python có thể tăng Java lên 17.95% pass @1 tuyệt đối trên HumanEval-X. Đáng ngạc nhiên hơn, chúng tôi phát hiện CODEM-HTML 7B được huấn luyện trên corpus HTML có thể cải thiện Java lên 15.24% pass @1 tuyệt đối. Dữ liệu huấn luyện của chúng tôi được phát hành tại https://github.com/NL2Code/CodeM .

Từ khóa: Mô hình Ngôn ngữ Lớn · Sinh Mã Code · Ngôn ngữ Lập trình · Điều chỉnh theo Hướng dẫn

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn cho mã code (code LLMs) đang phát triển mạnh mẽ gần đây [Zan et al., 2023]. Rất nhiều code LLMs được phát hành liên tiếp, ví dụ: Codex [Chen et al., 2021], AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022], CodeGen [Nijkamp et al., 2023], CodeGeeX [Zheng et al., 2023], StarCoder [Li et al., 2023], và Code Llama [Rozière et al., 2023]. Nhờ hiệu suất sinh mã code tuyệt vời của chúng, các code LLMs đã thu hút sự chú ý đáng kể từ cả giới học thuật và công nghiệp. Các nghiên cứu gần đây [Ouyang et al., 2022] đã chứng kiến kỹ thuật điều chỉnh theo hướng dẫn có thể dạy LLMs cách tuân theo hướng dẫn. Trong lĩnh vực sinh mã code, WizardCoder [Luo et al., 2023] và PanGu-Coder2 [Shen et al., 2023] cũng áp dụng kỹ thuật này để kích thích khả năng sinh mã code của chúng.

Mặc dù một số code LLMs, như CodeGen-Multi Nijkamp et al. [2023] và StarCoder-base Li et al. [2023], được huấn luyện trên corpus bao gồm nhiều ngôn ngữ lập trình, sự tương tác giữa các ngôn ngữ này vẫn chưa được khám phá. Trong thực hành lập trình, một khi lập trình viên con người đã thành thạo một ngôn ngữ lập trình, sẽ dễ dàng hơn để học một ngôn ngữ mới do tính đồng nhất giữa các ngôn ngữ lập trình. Được thúc đẩy bởi điều này, chúng tôi muốn khám phá liệu các ngôn ngữ lập trình khác nhau có thể thúc đẩy lẫn nhau trong quá trình tinh chỉnh theo hướng dẫn của code LLMs hay không.

Để khám phá ý tưởng này, chúng tôi tạo ra corpus huấn luyện cho từng ngôn ngữ trong 8 ngôn ngữ lập trình phổ biến (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML), trong đó mỗi ngôn ngữ bao gồm khoảng 9K bài tập lập trình. Chúng tôi huấn luyện StarCoder 7B sử dụng kỹ thuật điều chỉnh theo hướng dẫn trên từng corpus ngôn ngữ lập trình riêng biệt, và kiểm tra hiệu suất của từng mô hình đã được tinh chỉnh trên mọi ngôn ngữ lập trình. Phát hiện của chúng tôi cho thấy các ngôn ngữ lập trình có thể thúc đẩy đáng kể lẫn nhau. Đồng thời, chúng tôi phát hiện mức độ cải thiện của các ngôn ngữ lập trình khác nhau đối với nhau có liên quan đến độ tương tự ngôn ngữ giữa chúng. Ví dụ, CODEM-JavaScript 7B được huấn luyện trên dữ liệu JavaScript có thể mang lại cải thiện 11.80% pass @1 tuyệt đối trong TypeScript. Thú vị hơn,

∗Hai tác giả đầu tiên đóng góp như nhau cho công trình này.

--- TRANG 2 ---

# BÁO CÁO KỸ THUẬT

# Hướng dẫn
Thiết kế một trang web hiển thị thông điệp khi được tải. Thông điệp phải là "Hello, World". Sử dụng mã HTML để đạt được điều này.

# Phản hồi
```html
<!DOCTYPE html>
<html>
<head>
<title>Hello World</title>
</head>
<body>
<h1>Hello, World</h1>
</body>
</html>
```

Hình 1: Một ví dụ huấn luyện HTML của các cặp hướng dẫn-câu trả lời được tạo ra của chúng tôi.

CODEM-HTML 7B được huấn luyện trên ngôn ngữ đánh dấu HTML cũng có thể đạt được cải thiện 15.24% pass @1 tuyệt đối trong Java.

Tóm lại, các đóng góp của chúng tôi có thể được liệt kê như sau: (1) Phát hiện của chúng tôi cho thấy các ngôn ngữ lập trình có thể thúc đẩy đáng kể lẫn nhau trong giai đoạn tinh chỉnh theo hướng dẫn của code LLMs. (2) Chúng tôi thu thập những hiểu biết có giá trị về mối tương quan giữa nhiều ngôn ngữ lập trình, mở đường cho nghiên cứu tương lai về sinh mã code. (3) Chúng tôi sẽ công bố dữ liệu huấn luyện của mình.

## 2 Phương pháp

### 2.1 Tạo ra Corpus Huấn luyện của Tám Ngôn ngữ Lập trình

Chúng tôi chọn 8 ngôn ngữ lập trình phổ biến và xây dựng dữ liệu huấn luyện của chúng riêng biệt. Các ngôn ngữ được chọn bao gồm Python, JavaScript, TypeScript, C, C++, Java, Go, và HTML, bao phủ các loại đa dạng như hướng thủ tục, hướng đối tượng, script, và thậm chí cả ngôn ngữ đánh dấu. Đối với mỗi ngôn ngữ lập trình, chúng tôi xây dựng dữ liệu huấn luyện chứa khoảng 9K cặp dữ liệu. Mỗi cặp bao gồm cả hướng dẫn mô tả vấn đề lập trình và phản hồi tương ứng. Một ví dụ thực tế của HTML được thể hiện trong Hình 1.

Dựa trên các ngôn ngữ được chọn này, chúng tôi xây dựng một loạt các tập dữ liệu đơn ngôn ngữ. Chúng tôi bắt đầu từ tập dữ liệu CodeAlpaca 20K², và trích xuất những dữ liệu liên quan đến Python để tạo thành tập hướng dẫn gốc của chúng tôi. Sau đó, đối với mỗi ngôn ngữ lập trình được chọn, chúng tôi phát triển các hướng dẫn hiện có trong tập hướng dẫn gốc để có được những hướng dẫn mới tương ứng bằng cách nhắc nhở OpenAI's GPT-3.5³. Đối với tất cả các ngôn ngữ được chọn ngoại trừ HTML, chúng tôi áp dụng phát triển chuyên sâu [Xu et al., 2023], bằng cách yêu cầu GPT-3.5 viết lại hướng dẫn gốc (Python) thành phiên bản phức tạp hơn liên quan đến ngôn ngữ đích (Python, JavaScript, TypeScript, C, C++, Java, hoặc Go). Tuy nhiên, đối với HTML, chúng tôi áp dụng phát triển theo chiều rộng để tạo ra hướng dẫn liên quan đến HTML hoàn toàn mới, vì HTML (ngôn ngữ đánh dấu) quá khác biệt so với các ngôn ngữ khác (ngôn ngữ không đánh dấu).

### 2.2 Điều chỉnh theo Hướng dẫn

Các mô hình tiền huấn luyện mã code như Codex [Chen et al., 2021] và StarCoder [Li et al., 2023] lưu trữ một kho kiến thức mã code phong phú. Tuy nhiên, những mô hình này chỉ hỗ trợ sinh mã code từ trái sang phải dựa trên ngữ cảnh, vì chúng chỉ được huấn luyện trên các đoạn mã code thuần túy. Gần đây, các kỹ thuật điều chỉnh theo hướng dẫn [Ouyang et al., 2022, Luo et al., 2023, Shen et al., 2023] được đề xuất, có thể tăng cường khả năng tuân theo hướng dẫn của mô hình để kích hoạt tính năng trò chuyện. Trong quá trình điều chỉnh theo hướng dẫn, chúng tôi huấn luyện StarCoder sử dụng prompt trong Hình 2 để có được CODEM của chúng tôi. Chúng tôi sử dụng DeepSpeed để tăng tốc quá trình huấn luyện CODEM với fp16 được kích hoạt. Ngoài ra, chúng tôi đặt kích thước batch là 2 cho mỗi GPU, tốc độ học là 2e-5 với lịch trình cosine annealing, bước tích lũy gradient là 4, và bước khởi động là 30. Sau khi điều chỉnh theo hướng dẫn, chúng tôi sử dụng prompt trong Hình 3 để thực hiện suy luận trên các tác vụ downstream trên nhiều ngôn ngữ lập trình khác nhau. Đối với suy luận, chúng tôi áp dụng chiến lược giải mã tham lam để lấy mẫu. Do CODEM là một

²https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k
³https://platform.openai.com/docs/models/gpt-3-5

--- TRANG 3 ---

# BÁO CÁO KỸ THUẬT

Dưới đây là một hướng dẫn mô tả một tác vụ, được ghép nối với một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành yêu cầu một cách phù hợp.

### Hướng dẫn:
{problem}

### Phản hồi:
{response}

Hình 2: Định dạng prompt của điều chỉnh theo hướng dẫn. {problem} và {response} tham chiếu đến hướng dẫn và câu trả lời thu được trong Mục 2.1.

Dưới đây là một hướng dẫn mô tả một tác vụ. Viết một phản hồi hoàn thành yêu cầu một cách phù hợp.

### Hướng dẫn:
Hoàn thành mã {language} cho vấn đề này:
{problem}

### Phản hồi:
{signature}

Hình 3: Định dạng prompt của suy luận. {language}, {problem}, và {signature} đại diện cho ngôn ngữ lập trình downstream, vấn đề lập trình đã cho, và header hàm, tương ứng.

mô hình kiểu trò chuyện, các phản hồi mà nó tạo ra thường chứa các yếu tố ngoài chỉ mã code, điều này thường làm cho chúng không thể thực thi được. Vì vậy, chúng tôi trích xuất các đoạn mã code từ phản hồi được tạo ra để đánh giá hiệu suất sinh mã code.

## 3 Thí nghiệm

### 3.1 Thiết lập Đánh giá

#### 3.1.1 Benchmark và Baseline

Chúng tôi sử dụng HumanEval-X [Zheng et al., 2023] để đánh giá khả năng đa ngôn ngữ của các mô hình trong Python, JavaScript, C++, Java, và Go. HumanEval-X được tạo ra bằng cách điều chỉnh HumanEval [Chen et al., 2021] (Python) sang các ngôn ngữ lập trình khác. Theo cùng phương pháp như HumanEval-X, chúng tôi cũng tạo ra hai phiên bản mới của HumanEval: HumanEval-C và HumanEval-TypeScript. Lưu ý rằng HumanEval không thể được điều chỉnh trực tiếp sang các ngôn ngữ đánh dấu như HTML, vì vậy các ngôn ngữ đánh giá downstream của chúng tôi không bao gồm HTML.

Baseline chính cho tất cả các phiên bản ngôn ngữ của CODEM là mô hình cơ sở StarCoder của chúng. Chúng tôi phân tích liệu CODEM được huấn luyện trên ngôn ngữ A có thể cải thiện ngôn ngữ B hay không, trong trường hợp này các baseline là CODEM được huấn luyện trực tiếp trên ngôn ngữ B.

#### 3.1.2 Chỉ số

Chúng tôi áp dụng pass @1 làm chỉ số để đánh giá tất cả các mô hình. Mỗi mô hình tạo ra một câu trả lời sử dụng chiến lược giải mã tham lam cho mỗi tác vụ lập trình, và câu trả lời sẽ được thực thi trên các test case đã cho. Chỉ khi tất cả các test case được vượt qua, tác vụ lập trình mới có thể được coi là giải quyết bằng mã code được tạo ra. Trong thiết lập này, pass @1 có thể được công thức hóa là |Pc|/|P|, trong đó |P| biểu thị tổng số tác vụ lập trình trong HumanEval và |Pc| đại diện cho số tác vụ được giải quyết. Về bản chất, chỉ số pass @1 mà chúng tôi sử dụng có thể được coi là độ chính xác.

### 3.2 Kết quả

#### 3.2.1 Kết quả Chính

Bảng 1 cho thấy hiệu suất của CODEM, là một loạt các mô hình được huấn luyện trên các tập dữ liệu đơn ngôn ngữ của tám ngôn ngữ tương ứng, trên các phiên bản ngôn ngữ khác nhau của HumanEval. Như chúng ta có thể thấy, tất cả các mô hình CODEM đều vượt trội so với

--- TRANG 4 ---

# BÁO CÁO KỸ THUẬT

Bảng 1: Pass @1 (Độ chính xác) của StarCoder 7B và CODEM được huấn luyện trên nhiều ngôn ngữ lập trình khác nhau. Các số màu đỏ đại diện cho mức tăng tuyệt đối so với StarCoder 7B.

| Mô hình | HumanEval-Multilingual |
|---------|------------------------|
|         | Python | JavaScript | TypeScript | C | C++ | Java | Go |
| StarCoder 7B | 26.83 | 24.39 | 28.57 | 24.69 | 25.61 | 23.17 | 24.39 |
| CODEM-Python | 38.41^11.58 | 34.76^10.37 | 33.54^4.97 | 29.01^4.32 | 34.15^8.54 | 37.20^14.03 | 27.44^3.05 |
| CODEM-JavaScript | 37.20^10.37 | 40.24^15.85 | 40.37^11.80 | 27.78^3.09 | 32.93^7.32 | 34.76^11.59 | 26.22^1.83 |
| CODEM-TypeScript | 33.54^6.71 | 37.80^13.41 | 37.27^8.70 | 30.25^5.56 | 30.49^4.88 | 28.05^4.88 | 25.61^1.22 |
| CODEM-C | 39.63^12.8 | 37.20^12.81 | 32.30^3.73 | 32.10^7.41 | 35.37^9.76 | 38.41^15.24 | 28.66^4.27 |
| CODEM-C++ | 34.57^7.74 | 35.37^10.98 | 32.30^3.73 | 34.57^9.80 | 39.02^13.41 | 37.20^14.03 | 28.05^3.66 |
| CODEM-Java | 35.37^8.54 | 33.54^9.15 | 32.30^3.73 | 29.63^4.94 | 31.10^5.49 | 37.80^14.63 | 27.44^3.05 |
| CODEM-Go | 35.98^9.15 | 33.54^9.15 | 31.68^3.11 | 30.25^5.56 | 34.15^8.54 | 35.98^12.81 | 32.32^7.93 |
| CODEM-HTML | 31.71^4.88 | 33.54^9.15 | 32.30^3.73 | 25.93^1.24 | 28.66^3.05 | 38.41^15.24 | 28.05^3.66 |
| CODEM-Mixed | 43.29^16.46 | 37.20^12.81 | 37.89^9.32 | 32.10^7.41 | 37.80^12.19 | 39.63^16.46 | 29.27^4.88 |

Bảng 2: Pass @1 của StarCoder 15B và CODEM-Python. Các số màu đỏ biểu thị cải thiện tuyệt đối so với StarCoder 15B.

| Mô hình | HumanEval-Multilingual |
|---------|------------------------|
|         | Python | JavaScript | TypeScript | C | C++ | Java | Go |
| StarCoder 15B | 32.93 | 30.79 | 32.29 | 26.99 | 31.55 | 30.22 | 17.61 |
| CODEM-Python | 64.63^31.70 | 47.56^16.77 | 39.75^7.46 | 35.19^9.20 | 43.80^12.35 | 48.17^17.95 | 34.76^17.15 |

mô hình cơ sở StarCoder 7B của chúng trên tất cả các ngôn ngữ lập trình với một biên độ lớn. Ngoài ra, chúng tôi phát hiện các ngôn ngữ lập trình có thể thúc đẩy đáng kể lẫn nhau. Ví dụ, CODEM-Python được huấn luyện hoàn toàn trên corpus Python có thể cải thiện HumanEval-Java lên 14.03% pass @1 tuyệt đối. Phát hiện này tiết lộ những điểm chung vốn có giữa các ngôn ngữ lập trình khác nhau. Đáng ngạc nhiên hơn, CODEM-HTML thúc đẩy HumanEval-Java lên 15.24% pass@1 tuyệt đối, thậm chí vượt qua CODEM-Java. Tương tự, CODEM-C++ đánh bại CODEM-C trên HumanEval-C, và CODEM-JavaScript đánh bại CODEM-TypeScript trên HumanEval-Typescript. Dựa trên những quan sát này, chúng tôi phỏng đoán rằng việc cải thiện hiệu suất sinh mã code đa ngôn ngữ chủ yếu là do điều chỉnh theo hướng dẫn mở khóa tiềm năng vốn có của mô hình, như hiểu biết ngôn ngữ tự nhiên hoặc lập trình và khả năng tuân theo hướng dẫn, thay vì chỉ đơn thuần kết hợp kiến thức mới. Ngoài việc huấn luyện CODEM trên corpus huấn luyện đơn ngôn ngữ, chúng tôi tiếp tục xây dựng một tập huấn luyện đa ngôn ngữ 9K bao gồm 8 ngôn ngữ lập trình. Mặc dù mỗi ngôn ngữ chỉ bao gồm một lượng nhỏ (~ 1.2K) các instance huấn luyện, các phát hiện thí nghiệm cho thấy CODEM-Mixed xuất sắc trong tất cả các ngôn ngữ, thậm chí vượt qua CODEM-Python trên HumanEval-Python và CODEM-Java trên HumanEval-Java. Điều này cho thấy có thể mang lại hiệu suất sinh mã code vượt trội bằng cách tận dụng dữ liệu đa ngôn ngữ trong điều chỉnh theo hướng dẫn, mà không làm hại khả năng tổng quát hóa của mô hình.

Chúng tôi cũng tiến hành thí nghiệm trên StarCoder 15B để xác minh hiệu quả của CODEM. Cụ thể, chúng tôi thu được 108K dữ liệu huấn luyện Python theo WizardCoder [Luo et al., 2023], và tinh chỉnh StarCoder 15B để có được CODEM-Python. Kết quả được thể hiện trong Bảng 2. CODEM-Python đạt được hiệu suất tối ưu trên HumanEval-Python với 64.63% pass @1, so với các mô hình khác cùng quy mô. CODEM-Python cũng có được cải thiện to lớn trong việc sinh các ngôn ngữ lập trình khác. Ví dụ, nó cải thiện Java và JavaScript lần lượt lên 17.95% và 16.77% pass @1 tuyệt đối.

#### 3.2.2 Phân tích Sâu hơn

Chúng tôi phân tích mối tương quan giữa các ngôn ngữ lập trình khác nhau. Như được minh họa trong Hình 4 (a), việc cải thiện hiệu suất sinh mã code rất nhạy cảm với corpus huấn luyện của các ngôn ngữ lập trình khác nhau. Hơn nữa, chúng tôi phát hiện C và C++ có thể thúc đẩy lẫn nhau mạnh mẽ hơn, điều tương tự cũng áp dụng cho JavaScript và TypeScript. Điều này hợp lý vì những ngôn ngữ này có tương quan với nhau trong thiết kế ngôn ngữ, chia sẻ một số cú pháp và ngữ pháp chung. Hình 4 (b) cho thấy việc huấn luyện trên mỗi ngôn ngữ lập trình có thể thúc đẩy hiệu suất sinh mã code của tất cả các ngôn ngữ khác. Chúng ta có thể thấy các giá trị tương quan trong Hình 4 (b) hầu hết đều dương, ngụ ý rằng xu hướng cải thiện của các ngôn ngữ khác nhau mang lại bởi một corpus huấn luyện đơn ngôn ngữ tương đối tương tự.

--- TRANG 5 ---

# BÁO CÁO KỸ THUẬT

Python JS TS C C++ Java Go
Python JS TS C C++ Java Go
1
0.33 1
0.06 0.87 1
0.27 0.05 -0.3 1
0.5 0.1 -0.22 0.84 1
0.26 -0.5 -0.63 -0.01 0.26 1
0.1 -0.62 -0.72 0.2 0.28 0.43 1

1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00

(a)

Python JS TS C C++ Java Go HTML
Python
JS TS C C++ Java Go HTML
1
0.75 1
0.42 0.87 1
0.94 0.62 0.34 1
0.62 0.35 0.18 0.71 1
0.92 0.66 0.29 0.89 0.57 1
0.84 0.37 -0.05 0.82 0.46 0.77 1
0.73 0.63 0.22 0.65 0.31 0.88 0.68 1

0.0
0.2
0.4
0.6
0.8
1.0

(b)

Hình 4: Mối tương quan giữa các ngôn ngữ lập trình khác nhau. Chúng tôi coi dữ liệu trong Bảng 1 như một ma trận, và sử dụng "df.corr()" từ thư viện Pandas để tính toán mối tương quan giữa các ngôn ngữ lập trình khác nhau. Kết quả tương quan trước và sau "df.T" được trình bày trong (a) và (b), tương ứng.

## 4 Nghiên cứu Liên quan

Codex [Chen et al., 2021] với 12 tỷ tham số có thể giải quyết các vấn đề lập trình Python một cách tự động. Thành công đáng chú ý này đã gây ra một làn sóng đáng kể trong cả lĩnh vực học thuật và công nghiệp. Theo sau Codex, rất nhiều code LLMs được đề xuất, bao gồm AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022], CodeGen [Nijkamp et al., 2023], InCoder [Fried et al., 2023], CodeGeeX [Zheng et al., 2023], replit⁴, CodeT5 [Wang et al., 2021, 2023], PyCodeGPT [Zan et al., 2022], SantaCoder [Allal et al., 2023], StarCoder [Li et al., 2023], Code Llama [Rozière et al., 2023], và phi-1 [Gunasekar et al., 2023]. Những mô hình trên được huấn luyện trên corpus mã code quy mô lớn và đạt được hiệu suất sinh mã code ấn tượng. Trong quá trình tiền huấn luyện của chúng, một số mô hình được huấn luyện trên các tập dữ liệu của nhiều ngôn ngữ lập trình và sau đó được tinh chỉnh trên tập dữ liệu đơn ngôn ngữ để tạo ra phiên bản chuyên gia mạnh mẽ hơn. Đối với giai đoạn tinh chỉnh theo hướng dẫn, WizardCoder [Luo et al., 2023], PanGu-Coder2 [Shen et al., 2023], và Phind-CodeLlama⁵ được đề xuất để tăng cường khả năng tuân theo hướng dẫn và tiếp tục thúc đẩy khả năng sinh mã code. Tuy nhiên, không có mô hình nào trong số những mô hình được đề cập ở trên khám phá sự tương tác phức tạp giữa các ngôn ngữ lập trình khác nhau. Trong báo cáo này, do đó chúng tôi muốn điều tra liệu việc huấn luyện code LLMs trên dữ liệu đơn ngôn ngữ có thể thúc đẩy hiệu suất trong các ngôn ngữ lập trình khác hay không.

## 5 Kết luận

Phát hiện của chúng tôi tiết lộ rằng một corpus huấn luyện đơn ngôn ngữ có thể tăng cường khả năng sinh mã code đa ngôn ngữ của code LLMs thông qua điều chỉnh theo hướng dẫn. Điều này làm nổi bật tính phổ quát và mối liên kết vốn có giữa nhiều ngôn ngữ lập trình. Trong nghiên cứu tương lai, chúng tôi dự định đi sâu vào lý do tại sao nhiều ngôn ngữ có thể tăng cường lẫn nhau. Ngoài ra, chúng tôi sẽ khám phá cách tận dụng phát hiện của mình để nâng cao khả năng sinh mã code cho những ngôn ngữ lập trình ít phổ biến hoặc ít được sử dụng bằng cách huấn luyện trên dữ liệu từ những ngôn ngữ phổ biến.

## Lời cảm ơn

Chúng tôi xin cảm ơn các đồng nghiệp đã đóng góp phản hồi và hiểu biết có giá trị. Cảm ơn đặc biệt đến An Fu (Huawei), Jingyang Zhao (Huawei), và Yuenan Guo (Huawei) vì sự giúp đỡ mang tính xây dựng trong suốt nghiên cứu này.

## Tài liệu tham khảo

Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. Large language models meet NL2Code: A survey. In Proceedings of the 61st Annual Meeting of the Association for

⁴https://huggingface.co/replit/replit-code-v1-3b
⁵https://huggingface.co/Phind/Phind-CodeLlama-34B-v1

--- TRANG 6 ---

# BÁO CÁO KỸ THUẬT

Computational Linguistics (Volume 1: Long Papers), pages 7443–7464, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long.411.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.

Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378:1092 – 1097, 2022.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, 2023.

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x. ArXiv, abs/2303.17568, 2023.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: may the source be with you!, 2023.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for code, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.

--- TRANG 7 ---

# BÁO CÁO KỸ THUẬT

Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang. PanGu-Coder2: Boosting large language models for code with ranking feedback, 2023.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering large language models to follow complex instructions, 2023.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. InCoder: A generative model for code infilling and synthesis. In The Eleventh International Conference on Learning Representations, 2023.

Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696–8708, 2021.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+: Open code large language models for code understanding and generation, 2023.

Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. CERT: Continual pre-training on sketches for library-oriented code generation. In International Joint Conference on Artificial Intelligence, 2022.

Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alexander Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, Sergey Mikhailovich Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Franz Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Christopher Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. SantaCoder: don't reach for the stars! ArXiv, abs/2301.03988, 2023.

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023.
