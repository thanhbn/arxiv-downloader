# 2304.03816.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2304.03816.pdf
# Kích thước tệp: 1248322 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Hướng tới Tạo ra Các Chỉnh sửa Mã Chức năng Đúng từ
Mô tả Vấn đề bằng Ngôn ngữ Tự nhiên
Sarah Fakhoury∗
Microsoft ResearchSaikat Chakraborty∗
Microsoft Research
Madan Musuvathi
Microsoft ResearchShuvendu K. Lahiri
Microsoft Research

TÓM TẮT
Các mô hình ngôn ngữ lớn (LLMs), như Codex của OpenAI, đã chứng minh tiềm năng của chúng trong việc tạo ra mã từ các mô tả bằng ngôn ngữ tự nhiên trên một phạm vi rộng các tác vụ lập trình. Một số benchmark gần đây đã xuất hiện để đánh giá khả năng của LLMs trong việc tạo ra mã chức năng đúng từ ý định ngôn ngữ tự nhiên đối với một tập hợp các test cases ẩn. Điều này đã cho phép cộng đồng nghiên cứu xác định những tiến bộ đáng kể và có thể tái tạo trong khả năng của LLM. Tuy nhiên, hiện tại thiếu các tập dữ liệu benchmark để đánh giá khả năng của LLMs trong việc tạo ra các chỉnh sửa mã chức năng đúng dựa trên các mô tả bằng ngôn ngữ tự nhiên về những thay đổi dự định. Bài báo này nhằm giải quyết khoảng trống này bằng cách thúc đẩy vấn đề nl2fix về việc dịch các mô tả bằng ngôn ngữ tự nhiên về những thay đổi mã (cụ thể là các bản sửa lỗi được mô tả trong các báo cáo Issue trong repositories) thành các bản sửa mã đúng. Để làm điều này, chúng tôi giới thiệu Defects4J-Nl2fix, một tập dữ liệu gồm 283 chương trình Java từ tập dữ liệu Defects4J phổ biến được bổ sung với các mô tả cấp cao về các bản sửa lỗi, và đánh giá thực nghiệm hiệu suất của một số LLMs tiên tiến cho tác vụ này. Kết quả cho thấy những LLMS này cùng nhau có khả năng tạo ra các bản sửa hợp lý cho 64.6% số lỗi, và kỹ thuật dựa trên LLM tốt nhất có thể đạt tới 21.20% độ chính xác top-1 và 35.68% top-5 trên benchmark này.

1 GIỚI THIỆU
Gần đây đã có sự quan tâm tăng cao trong việc sử dụng các mô hình ngôn ngữ lớn (LLMs) để thực hiện nhiều tác vụ phát triển phần mềm khác nhau. Ví dụ, GitHub Copilot [17], được hỗ trợ bởi mô hình Codex của OpenAI [11], đã chứng minh khả năng ấn tượng trong việc tạo ra mã dựa trên mô tả bằng ngôn ngữ tự nhiên và các ngữ cảnh mã liên quan. Các bổ sung gần đây cho Copilot [4] cũng nhấn mạnh việc chuyển đổi mã dựa trên các hướng dẫn bằng ngôn ngữ tự nhiên. Bên cạnh Copilot, LLMs đang hỗ trợ một số sản phẩm phát triển phần mềm được hỗ trợ bởi AI thương mại khác như Amazon CodeWhisperer [1], GhostWriter [3] và Tabnine [5].

Nghiên cứu về LLMs cho việc tạo mã đã được thúc đẩy bởi các benchmark được sử dụng để đánh giá và thường cải thiện hiệu suất của các mô hình này trên các tác vụ cụ thể. Ví dụ, benchmark HumanEval [11] để đánh giá mô hình Codex đã có vai trò quan trọng trong việc hướng dẫn quá trình huấn luyện phiên bản GPT-4 được cải thiện nhiều của OpenAI [38]. Cải thiện trên các benchmark offline như vậy thường dẫn đến cải thiện trong việc sử dụng LLMs trong thế giới thực cho các tác vụ tương tự (ví dụ, trong một IDE). Thực tế, đối với tác vụ tạo mã từ ngôn ngữ tự nhiên (chúng tôi gọi là nl2code), một số benchmark bổ sung đã xuất hiện trong những năm gần đây, bao gồm các benchmark mã hóa cơ bản được crowd-source như MBPP

∗Đóng góp ngang nhau.

(từ Google) [6], các benchmark Cuộc thi Mã [31], và những benchmark dựa trên các chương trình thế giới thực [62].

Trong khi một số benchmark như vậy nắm bắt khả năng của LLMs trong việc tạo ra mã chức năng đúng từ ý định ngôn ngữ tự nhiên, thiếu các benchmark đánh giá khả năng của LLM trong việc thực hiện chỉnh sửa dựa trên ý định ngôn ngữ tự nhiên (chúng tôi gọi là nl2edit).

Trong phát triển phần mềm thế giới thực, một người dùng có nhiều khả năng thực hiện các tác vụ phát triển và bảo trì phần mềm như vậy trong ngữ cảnh của một repository phần mềm hiện có so với việc viết một chương trình độc lập từ đầu bằng ngôn ngữ tự nhiên. Bên cạnh đó, trong phát triển phần mềm hiện đại với các pipeline tích hợp liên tục (CI) được host trên cloud, các chương trình phần mềm trưởng thành thường phát triển thông qua các Pull Requests (PR) được thực hiện để đáp ứng mô tả Issue bằng ngôn ngữ tự nhiên tương ứng với việc sửa lỗi, tối ưu hóa, thêm tính năng, v.v.

Trong bài báo này, chúng tôi thực hiện bước đầu tiên hướng tới việc tạo ra một benchmark cho nl2edit và đánh giá các LLMs tiên tiến hiện tại trên vấn đề này. Cụ thể, chúng tôi tập trung vào vấn đề hạn chế nl2fix bao gồm tác vụ sửa một chương trình có lỗi trong đó lỗi được mô tả bằng ngôn ngữ tự nhiên trong mô tả Issue. Chúng tôi tận dụng benchmark Defects4J [22] phổ biến bao gồm các lỗi thế giới thực và các bản sửa của chúng để định nghĩa benchmark Defects4J-Nl2fix cho nl2fix. Chúng tôi giới thiệu một tập dữ liệu bổ sung gồm 283 chương trình Java với các mô tả cấp cao về những thay đổi mã dự định, cùng với bộ test đảm bảo rằng bản sửa giải quyết khiếm khuyết trong khi bảo toàn chức năng. Chúng tôi cũng giới thiệu các metrics để đánh giá LLMs cho vấn đề nl2fix trên benchmark này.

Hình 1 minh họa một ví dụ từ tập dữ liệu Defects4J-Nl2fix. Nó cho thấy một Issue chứa Bug ID tham chiếu đến một issue từ dự án JxPath¹ được host trên Jira [2]; một Title mô tả lỗi ở cấp độ cao, và cuối cùng là Description cung cấp thêm chi tiết về lỗi bao gồm mô tả bằng ngôn ngữ tự nhiên về một kịch bản để tái tạo lỗi. Hàm gốc bao gồm thân phương thức chứa bản sửa — chúng tôi hạn chế tập dữ liệu của mình để sửa được định vị vào một phương thức duy nhất. Issue, cùng với hàm có lỗi, tạo thành đầu vào cho vấn đề nl2fix. Các patches đầu ra được tạo ra được đánh giá về tính đúng đắn chức năng và không có khiếm khuyết bằng cách sử dụng Regression tests và Trigger tests, tương ứng. Mỗi test t trong tập Trigger tests thất bại trên chương trình có lỗi và đạt bản sửa do người dùng cung cấp dưới dạng hàm đã sửa.

Động lực sử dụng hidden tests chỉ để validation đến từ một số hướng. Đầu tiên, nghiên cứu trước đây quan sát thấy rằng hầu hết các Issues sửa lỗi thế giới thực không đi kèm với một failing hoặc trigger

¹https://issues.apache.org/jira//browse/JXPATH-149arXiv:2304.03816v1 [cs.SE] 7 Apr 2023

--- TRANG 2 ---
Sarah Fakhoury, Saikat Chakraborty1, Madan Musuvathi, và Shuvendu K. Lahiri

LLM PromptCác Giải pháp Ứng viênMetadata Issue
Issue ID: JXPATH-149TitleDescriptionMã sau đây có lỗi. Vấn đề là:<hàm có lỗi>Vui lòng cung cấp phiên bản đã sửa với thay đổi tối thiểu:Tiêu đề issue + mô tả

LLM
hàm gốc

Trigger testsRegression testshàm đã sửa
Bản sửa ground truthĐánh giáHợp lýSaikhông biên dịch được

Hình 1: Tổng quan về thiết lập vấn đề NL2Fix. Được minh họa là tiêu đề issue, mô tả, và bản sửa hợp lý cho lỗi JXPATH-149. Hình minh họa prompt LLM tiêu chuẩn được sử dụng để tạo các patches ứng viên, và việc đánh giá các patches bằng cách sử dụng ground truth trigger và regression tests.

test [23]. Thứ hai, mặc dù một chương trình có thể có các regression tests đi kèm, việc chạy các tests như vậy trong các pipeline CI có thể tốn kém và chỉ có thể được gọi một số lượng nhỏ (ví dụ, 5) lần để có tính thực tế. Như đã thảo luận sau trong related work (Phần 5), việc giữ các tests ẩn phân biệt vấn đề nl2fix với vấn đề automated program repair (APR) [15, 16, 36].

Bài báo này cũng đóng góp một đánh giá thực nghiệm chi tiết về hiệu suất của các LLMs tiên tiến (SOTA) hiện tại trên tập dữ liệu này. Chúng tôi chọn ba flavor khác nhau của LLMs dựa trên kiến trúc neural generative pre-trained transformers (GPT) từ OpenAI, (a) mô hình code completion Codex code-davinci-002, (b) mô hình code editing Codex code-davinci-edit-001, và (c) mô hình conversational ChatGPT gpt-3.5-turbo. Chúng tôi đánh giá các mô hình này dưới các thiết lập sampling khác nhau và các chiến lược prompting và thực hiện phân tích định lượng và định tính chi tiết về độ chính xác và chất lượng của các bản sửa được đề xuất.

Kết quả của chúng tôi chứng minh rằng những LLMs này cùng nhau có khả năng tạo ra các plausible patches (tức là, thỏa mãn regression và trigger tests) cho một phần đáng kể, 64.6% số lỗi khi sampling lên tới 100 ứng viên. Thú vị hơn, mô hình ChatGPT gpt-3.5-turbo vượt trội hơn các mô hình khác về cả pass@1, pass@5, và pass@100 accuracy metrics (chúng tôi mô tả pass@k metric chính thức hơn sau trong Phần 3.4.2). Cuối cùng, chúng tôi mô tả một cách tiếp cận generic để xếp hạng một tập không có thứ tự các patches ứng viên dựa trên embedding similarity được tính bởi LLM; việc xếp hạng làm cho các đề xuất trở nên deterministic với độ chính xác top-1 và top-5 lần lượt là 21.20% và 35.68%.

Những phát hiện này nhấn mạnh cả bản chất non-trivial của benchmark Defects4J-Nl2fix, cũng như khả năng của các LLMs hiện tại trong việc tạo thành các baseline ban đầu decent có thể thúc đẩy nghiên cứu tiếp theo.

Đóng góp. Tóm lại, trong bài báo này: (i) Chúng tôi thúc đẩy vấn đề nl2fix và trình bày một benchmark bổ sung non-trivial Defects4J-Nl2fix cùng với các metrics.

(ii) Chúng tôi thực hiện một đánh giá thực nghiệm mở rộng về hiệu suất của ba LLMs tiên tiến trên benchmark này.

(iii) Chúng tôi mô tả một chiến lược ranking dựa trên embedding similarity để cung cấp một danh sách fixes được xếp hạng và deterministic.

2 CÂU HỎI NGHIÊN CỨU
Bài báo này nhằm hiểu hiệu suất của các LLMs tiên tiến hiện tại trên vấn đề nl2fix: tác vụ sửa một chương trình có lỗi từ ý định ngôn ngữ tự nhiên. Để làm điều này, chúng tôi định nghĩa bốn câu hỏi nghiên cứu để đánh giá thực nghiệm khả năng mô hình:

RQ1. Các LLMs có thể tạo ra bản sửa từ ý định ngôn ngữ tự nhiên cho NL2Fix không?

Để trả lời RQ này, chúng tôi khám phá độ chính xác pass@k của ba LLMs SOTA để tạo ra các bản sửa lỗi đúng bằng cách sử dụng các mô tả issue bằng ngôn ngữ tự nhiên. Ngoài ra, chúng tôi trích xuất thống kê định lượng về các bản sửa được tạo ra, bao gồm: tỷ lệ xuất hiện của các đề xuất trùng lặp, tỷ lệ biên dịch, và phân phối và sự chồng lấp của các lỗi duy nhất mà có các plausible patches từ mỗi cách tiếp cận.

RQ2. Các LLMs tạo ra loại candidate fixes như thế nào?

Để làm sáng tỏ bản chất của các candidate patches được tạo bởi LLM, chúng tôi nghiên cứu các đặc điểm của các patches trong ngữ cảnh độ tương tự của chúng với các bản sửa ground truth được viết bởi developer và mã có lỗi đầu vào.

RQ3. Các LLMs cần những nguồn thông tin gì để tạo ra bản sửa cho NL2Fix?

Chúng tôi khám phá mức độ thông tin mà LLMs cần để tạo ra các bản sửa đúng từ các mô tả bằng ngôn ngữ tự nhiên. Chúng tôi thử nghiệm với các kiểu prompting khác nhau sử dụng thông tin được tuyển chọn, bao gồm: tóm tắt issue cấp cao, mô tả issue chi tiết, các thiết lập prompt 0-shot và 1-shot, cũng như lý do sửa lỗi được tạo ra bằng các chiến lược prompting Reasoning Extraction.

RQ4. Các LLMs có thể được sử dụng để xếp hạng bản sửa cho NL2Fix không?

Dựa trên các quan sát từ RQ1, RQ2, và RQ3, chúng tôi khám phá cách LLMs có thể được sử dụng để thiết kế một cách tiếp cận ranking đơn giản để xếp hạng các bản sửa từ tập không có thứ tự các candidate fixes, cho phép xấp xỉ tốt hơn các pass@k metrics cần thiết để phát triển một bug fix recommender deterministic và thực tế.

--- TRANG 3 ---
Hướng tới Tạo ra Các Chỉnh sửa Mã Chức năng Đúng từ Mô tả Vấn đề bằng Ngôn ngữ Tự nhiên

Bảng 1: Thống kê của Tập dữ liệu
Dự án # BugsSH†Trung bình Trung bình Độ dài Issue Thay đổi
Bugs hunks Line Token Title Desc.
Chart 6 5 1.17 1.83 9.67 7.17 149.33
Cli 28 16 1.68 4.07 22.07 9.36 206.46
Codec 11 9 1.27 2.18 10.82 12.73 171.09
Collections 1 1 1.00 1.00 1.00 20.00 457.00
Compress 36 19 1.78 5.39 29.28 10.03 320.42
Csv 12 8 1.42 2.50 18.92 10.83 104.25
JacksonCore 13 9 1.38 3.69 20.69 11.38 251.69
JacksonDataBind 67 36 1.87 5.37 33.90 11.90 294.49
JacksonXml 5 1 2.80 6.20 30.80 11.60 126.80
JxPath 10 5 1.60 4.80 22.60 9.70 159.10
Math 73 37 2.11 5.29 35.00 10.07 165.18
Mockito 21 16 1.33 3.29 27.90 9.00 311.67
Tổng thể 283 162 1.78 4.65 28.76 10.53 231.92
†SH: Single-Hunk

3 CÁCH TIẾP CẬN
3.1 Tập dữ liệu
Trong bài báo này, chúng tôi thực hiện bước đầu tiên hướng tới việc tạo ra một benchmark cho nl2edit và đánh giá các LLMs tiên tiến hiện tại trên vấn đề này. Chúng tôi tập trung vào vấn đề hạn chế nl2fix bao gồm tác vụ sửa một chương trình có lỗi trong đó lỗi được mô tả bằng ngôn ngữ tự nhiên trong mô tả issue.

Chúng tôi chọn benchmark Defects4J [22], bao gồm các lỗi và tests từ các issues thế giới thực, từ đó chúng tôi có thể trích xuất các mô tả issue.

Cụ thể, chúng tôi sử dụng Defects4J 2.0, một benchmark nổi tiếng gồm 835 lỗi và bản sửa thế giới thực được tuyển chọn thủ công từ 17 dự án Java. Tập dữ liệu hiện có bao gồm một tập các lỗi, các test cases tái tạo lỗi (trigger tests), và các regression test cases load class chứa phương thức đang được test.

Mỗi lỗi trong tập dữ liệu Defects4J chứa phiên bản PRE_FIX_REVISION và POST_FIX_REVISION đại diện cho các phiên bản có lỗi/đã sửa của mã tương ứng. Hai phiên bản phản ánh trạng thái thực tế của dự án khi lỗi được phát hiện/sửa.

Chúng tôi sử dụng các tests được viết bởi developer này để đánh giá các patches được tạo ra, một patch phải đạt cả trigger và regression tests để được coi là một plausible patch. Mặc dù điều này không đảm bảo tương đương ngữ nghĩa giữa một patch được tạo ra và bản sửa ground truth, chúng tôi lập luận rằng nó có thể là một proxy thực tế cho tính đúng đắn của patch vì hai lý do. Đầu tiên, với việc sử dụng LLMs có khả năng tạo ra hàng trăm candidate patches, việc đánh giá thủ công mỗi patch được tạo ra để tương đương ngữ nghĩa có thể tốn kém một cách cấm đoán (theo thứ tự 28,000 cho mỗi cấu hình mô hình). Do đó, việc đánh giá candidate patches với các tests được viết bởi developer phục vụ như một proxy có thể mở rộng để bảo toàn chức năng và không có khiếm khuyết. Thứ hai, tương đương ngữ nghĩa với bản sửa do người dùng cung cấp có thể không cần thiết, vì có thể có nhiều bản sửa không tương đương nhưng có thể chấp nhận được đối với các developers trong thực tế. Không có kiến thức về các invariants chi tiết của dự án, khó để xác định liệu một bản sửa ground truth cụ thể có phải là bản sửa duy nhất có thể chấp nhận được hay không.

Chúng tôi bổ sung tập dữ liệu Defects4J theo ba cách riêng biệt:
• Chúng tôi hạn chế tập dữ liệu Defects4J-Nl2fix chỉ bao gồm các bản sửa ảnh hưởng đến một thân phương thức duy nhất. Trong số 835 lỗi trong tập dữ liệu Defects4J 2.0, 283 chứa các lỗi single-method, tức là các lỗi có thể được sửa với các thay đổi phương thức đơn. Các bản sửa có thể bao gồm nhiều dòng, nhưng được giới hạn trong một hàm duy nhất. Bảng 1 chứa bảng phân tích số lượng lỗi cho mỗi dự án.

Chúng tôi biện minh quyết định tập trung vào các lỗi single method vì các phương thức thường định nghĩa một đơn vị mã có thể được review độc lập so với các dòng riêng lẻ, hoặc toàn bộ file hoặc repository. Thứ hai, prompt đầu vào cho LLMs bị hạn chế chỉ vài nghìn tokens có thể không đủ để nắm bắt toàn bộ thông tin file hoặc repository level. Các cách tiếp cận APR sử dụng tập dữ liệu Defects4J thường hạn chế tập dữ liệu của họ chỉ chứa single hunk, hoặc single line bugs [59]. Chúng tôi không đưa ra hạn chế này, và Bảng 1 cho thấy số lượng hunks trung bình cho các lỗi trong tập dữ liệu của chúng tôi.

• Thứ hai, để phục vụ vấn đề nl2fix, chúng tôi bổ sung tập dữ liệu Defects4J bằng cách ghép nối mỗi lỗi với metadata issue tương ứng, bao gồm tiêu đề và mô tả issue, mà chúng tôi scrape từ GitHub, SVN và Jira.

• Cuối cùng, sau khi điều tra kỹ lưỡng các phương thức có lỗi trong tập dữ liệu Defects4J, chúng tôi nhận thấy rằng như một tác dụng phụ của quá trình bug patching được sử dụng bởi những người tạo benchmark, các comments xuất hiện trong POST_FIX_REVISION cũng xuất hiện trong PRE_FIX_REVISION của mã². Điều này có nghĩa là các comments liên quan đến bản sửa thực tế được thực hiện bởi developer, có thể xuất hiện trong PRE_FIX_REVISION mà chúng tôi sử dụng làm đầu vào cho LLMs. Để tránh những comments này cung cấp gợi ý về các giải pháp cho mô hình, chúng tôi loại bỏ tất cả comments từ PRE_FIX_REVISION.

3.2 Generative Pre-trained Transformers (GPT)
Generative Pre-trained Transformers (GPT) là các mô hình generation autoregressive [21] quy mô lớn được huấn luyện để dự đoán token tiếp theo cho trước một ngữ cảnh prefix prompt ngôn ngữ tự nhiên. Sự phát triển gần đây của các mô hình GPT siêu quy mô lớn với hàng tỷ tham số đã cho thấy thể hiện các tính chất emergent mà chúng có thể thực hiện các tác vụ mà không cần finetuning [25,40]. Khi được yêu cầu tạo ra các phản hồi cho một prompt, các mô hình GPT samples trên các phân phối xác suất của tokens từng token một lúc. Để tạo ra phản hồi có khả năng xảy ra nhất (hoặc nhiều phản hồi), các mô hình này thực hiện sampling khác nhau, bao gồm temperature-based sampling, thao tác phân phối của tokens, kiểm soát sự đa dạng trong các phản hồi. Nhiệt độ thấp hơn thường dẫn đến ít đa dạng hơn, và nhiệt độ cao hơn thì ngược lại. Chúng tôi sử dụng hai nhiệt độ – 0.2 và 0.8 trong suốt các thí nghiệm.

Để trả lời các câu hỏi nghiên cứu đã định nghĩa, chúng tôi chọn ba Large Language Models (LLM) dựa trên GPT tiên tiến đã cho thấy khả năng mạnh mẽ trên nhiều tác vụ tạo mã³.

Codex. Codex của OpenAI, code-davinci-002 là một mô hình ngôn ngữ được thiết kế đặc biệt cho các tác vụ code completion. Nó dựa trên kiến trúc GPT-3 và đã được fine-tuned trên một corpus lớn mã từ các repositories công cộng. Codex xuất sắc trong việc tạo ra mã đúng cú pháp và đã được chứng minh là rất hiệu quả cho các tác vụ liên quan đến việc tạo mã.

Codex Edit Model. Mô hình edit Codex, code-davinci-edit-001, là một phiên bản của Codex GPT-3 với khả năng editing. Cho trước một mã

²https://github.com/rjust/defects4j/issues/477
³Tại thời điểm nộp bài, các tác giả không có quyền truy cập API vào mô hình tiên tiến mới nhất, GPT-4 [38]

--- TRANG 4 ---
Sarah Fakhoury, Saikat Chakraborty1, Madan Musuvathi, và Shuvendu K. Lahiri

và hướng dẫn được viết bằng NL như "Cải thiện độ phức tạp runtime của hàm này", các mô hình chỉnh sửa mã để có thể thỏa mãn hướng dẫn.

ChatGPT. Mô hình ChatGPT được phát hành gần đây (gpt-3.5-turbo) dựa trên mô hình GPT-3.5 được pretrained, được fine-tuned thêm bằng Reinforcement Learning with Human Feedback (RLHF) [39]. Mặc dù gpt-3.5-turbo không được fine-tuned rõ ràng cho các tác vụ tạo mã, đánh giá sớm đã chứng minh khả năng mạnh mẽ trong một số lĩnh vực khoa học và kỹ thuật [20,26,41,43] bao gồm hiểu và tạo ra các đoạn mã [33,49,50]. Bản chất conversational của ChatGPT cho phép nó xuất sắc trong các tác vụ đòi hỏi cả việc tạo mã và tương tác giống con người, cho phép sử dụng các cấu trúc prompt nâng cao bao gồm chain of thought [57] và reasoning extraction [25].

Embeddings. Các mô hình embedding OpenAI tạo ra biểu diễn vector chiều cao của các chuỗi đầu vào.

Nghiên cứu cho thấy rằng sự tương tự trong không gian vector chiều cao như vậy dịch sang sự tương tự ngữ nghĩa của các chuỗi. Trong số nhiều ứng dụng khác của biểu diễn như vậy, chúng có thể tạo điều kiện cho phân tích tương tự, tìm kiếm, v.v. Trong công trình này, chúng tôi tận dụng mô hình text-embedding-ada-002 để tạo ra embedding của mã và sử dụng các similarities dựa trên embedding để xếp hạng các patches.

3.3 Framework Prompting
Khả năng của LLMs không cố định trên tất cả các ngữ cảnh, tức là, nếu một LLM trả lời sai một câu hỏi, việc thay đổi nhẹ prompt bằng cách sửa đổi nội dung hoặc định dạng của thông tin được đưa ra có thể mang lại kết quả khác nhau. Có một số kỹ thuật để cải thiện độ chính xác và độ tin cậy của đầu ra LLM, những kỹ thuật này được gọi là prompt engineering [45]. Trong bài báo này, chúng tôi sử dụng prompting tiêu chuẩn cũng như hai chiến lược riêng biệt đã được chứng minh cải thiện hiệu suất của LLMs cho các tác vụ phức tạp: 1) few-shot prompting và 2) reasoning extraction. Những chiến lược này được thiết kế để giúp cung cấp ngữ cảnh và hướng dẫn để giải quyết hiệu quả một tác vụ trong khi giảm thiểu các cạm bẫy tiềm ẩn liên quan đến đầu ra được tạo bởi mô hình.

3.3.1 Zero-Shot Prompting. Zero-shot prompting, cũng thường được gọi là standard prompting, là cấu hình cơ bản của việc prompting một mô hình với một tác vụ. Prompt không bao gồm bất kỳ ví dụ nào về các giải pháp có thể chấp nhận được và không chia nhỏ vấn đề thành các sub-problems dễ hơn. Trong bài báo này, prompt zero-shot, hoặc standard, của chúng tôi được minh họa trong Hình 1. Nó bao gồm tiêu đề và mô tả issue, cùng với mã có lỗi và một hướng dẫn để cung cấp bản sửa cho mã.

3.3.2 Few-Shot Prompting. Few-shot prompting là một kỹ thuật bao gồm việc trình bày mô hình với một loạt ví dụ hoặc demonstrations để hướng dẫn sự hiểu biết của nó về tác vụ đang thực hiện. Bằng cách cung cấp cho các mô hình một vài instance của các tác vụ tương tự, cùng với các đầu vào và đầu ra mong muốn tương ứng của chúng, chúng tôi hướng dẫn đầu ra mô hình đến đầu ra mong muốn, cả về chức năng và định dạng. Cách tiếp cận này cho phép các mô hình thích ứng phản hồi của chúng dựa trên các ví dụ được cung cấp, dẫn đến đầu ra chính xác và mạch lạc hơn. Đối với một issue nhất định, chúng tôi chọn một issue khác cùng với mã có lỗi và mã đã sửa để biểu diễn như một shot. Chúng tôi chọn ví dụ là một issue mà mã có lỗi ví dụ gần nhất với mã có lỗi mục tiêu, sử dụng metric edit distance tiêu chuẩn.

3.3.3 Reasoning Extraction. Reasoning extraction là một chiến lược tập trung vào việc trích xuất lý do cơ bản đằng sau một tác vụ hoặc vấn đề cụ thể [25]. Chúng tôi áp dụng chiến lược này để giúp mô hình hiểu (các) mục tiêu và giải pháp cho tác vụ sửa mã. Cụ thể, chúng tôi tương tác rõ ràng ba lần với mô hình với các truy vấn khác nhau. Đầu tiên, cho trước mã có lỗi và báo cáo issue, chúng tôi yêu cầu mô hình localize lỗi, sau đó chúng tôi yêu cầu giải thích tại sao các dòng đã localized có lỗi, cuối cùng chúng tôi yêu cầu sửa lỗi. Bản chất conversational của ChatGPT cho phép việc sử dụng tự nhiên các cấu trúc prompt nâng cao duy trì ngữ cảnh conversational, như chain of thought reasoning và reasoning extraction. Do đó, chúng tôi chỉ sử dụng chiến lược prompt này với gpt-3.5-turbo.

3.4 Tính đúng đắn của Mã được Tạo ra
Các thí nghiệm được chạy trong hai giai đoạn: tạo fix và validation fix. Tất cả các thí nghiệm validation được chạy trong một Docker container chạy Ubuntu 20.04.4 với Java version OpenJDK 1.8.0 mà chúng tôi công khai Docker image. Để tạo ra các candidate fixes, chúng tôi sử dụng OpenAI API. Phần còn lại của phần này thảo luận chi tiết về patch validation và evaluation metrics:

3.4.1 Patch Validation. Mỗi lỗi trong tập dữ liệu Defects4J chứa phiên bản PRE_FIX_REVISION và POST_FIX_REVISION đại diện cho các phiên bản có lỗi/đã sửa của mã tương ứng. Hai phiên bản phản ánh trạng thái thực tế của dự án khi lỗi được phát hiện/sửa. Để xác định liệu một bản sửa được tạo ra có đúng hay không, chúng tôi thực hiện các bước sau: 1) Check out phiên bản PRE_FIX_REVISION của dự án 2) Thay thế hàm có lỗi gốc bằng hàm được tạo ra và 3) Chạy trigger và regression test(s) để xác định liệu mã chứa bản sửa được tạo ra có đạt các tests hay không.

Đối với mỗi fix, kết quả validation có thể là 1) Plausible: tất cả bug reproducing tests và regression tests đạt 2) Wrong: ít nhất 1 trong số trigger hoặc regression tests thất bại hoặc 3) Uncompilable.

3.4.2 Evaluation metrics. Để đo lường chất lượng của một giải pháp cho nl2fix, chúng tôi sử dụng pass@k metric⁴ được giới thiệu và sử dụng rộng rãi để đánh giá LLMs cho các vấn đề nl2code [7,11]. Trực quan, cho trước một tập không có thứ tự các candidate fixes, pass@k cung cấp khả năng chọn một bản sửa đúng khi được đưa ra k lần thử để sample từ tập candidate fixes này. Trong kịch bản nl2fix, một fix là đúng nếu nó đạt tất cả Trigger tests và Regression tests cho lỗi. Cho trước n là số lượng samples được tạo ra, k là số lượng samples để ước tính pass@k và c là số lượng samples đúng trong n, chúng tôi sử dụng công thức sau để tính pass@k được định nghĩa bởi [11]:

pass@k := E[1−(n−c choose k)/(n choose k)]                    (1)

Đối với bài báo này, chúng tôi tạo ra n≥k samples, trong đó n=100.

Ngoài ra, để trả lời RQ4, chúng tôi cũng giới thiệu một ranked pass@k, ký hiệu r.pass@k (Phần 4.4), để xác định số lượng trường hợp mà ít nhất một code fix đúng trong top k đề xuất của một danh sách được xếp hạng.

⁴chúng tôi sử dụng pass@k và P@k thay thế nhau trong suốt bài báo

--- TRANG 5 ---
Hướng tới Tạo ra Các Chỉnh sửa Mã Chức năng Đúng từ Mô tả Vấn đề bằng Ngôn ngữ Tự nhiên

[Biểu đồ 1-5-20-100 Pass@k cho temp=0.8 và temp=0.2 với ba mô hình khác nhau]

Hình 2: Kết quả Pass@k cho thiết lập 0-shot.

4 KẾT QUẢ
4.1 RQ1: Các LLMs có thể tạo ra bản sửa từ ý định ngôn ngữ tự nhiên cho NL2Fix không?

Đối với mỗi mô hình, chúng tôi xem xét hai thiết lập với zero-shot prompt: temp. ở 0.2 và temp. ở 0.8. Đối với mỗi thiết lập, chúng tôi tạo ra 100 candidate fixes cho mỗi trong số 283 lỗi và đánh giá tính đúng đắn của mỗi candidate đối với trigger và relevant tests. Tiếp theo, chúng tôi tính pass@k bằng kỹ thuật được mô tả trong Phần 3.4.2 và vẽ biểu đồ kết quả cho temp. 0.8 trong Hình 2a và temp. 0.2 trong Hình 2b.

Ở temp. 0.8, chúng ta thấy rằng edit model, code-davinci-edit-001, là mô hình hoạt động tốt nhất tổng thể với pass@100 ở 54.12%. gpt-3.5-turbo đạt pass@1 cao hơn một chút (13.93% so với 12.18 cho edit model), tuy nhiên ở pass@20 và cao hơn, hiệu suất giảm thấp hơn so với completion model, code-davinci-002. Completion model đạt pass@100 cao thứ hai với 45.9%, thấp hơn 8.22% so với edit model.

Ở temp. 0.2, chúng ta thấy cải thiện độ chính xác cho pass@1 từ cả code-davinci-edit-001 và gpt-3.5-turbo với lần lượt 17.55% và 16.0%. Tuy nhiên, chúng ta thấy độ chính xác thấp hơn một cách nhất quán của tất cả các mô hình bắt đầu từ pass@5 đến pass@100. Đáng chú ý nhất, độ chính xác của code-davinci-002 thấp hơn nhiều ở temp. 0.2 với chỉ 0.65% pass@1 và 3.55% pass@100. Chúng tôi đã test độ chính xác của mô hình trong hai lần chạy riêng biệt, và nhận thấy hiệu suất kém nhất quán ở thiết lập này.

Để hiểu rõ hơn độ chính xác pass@k cho mỗi mô hình, chúng tôi trích xuất thống kê cấp cao về mã được tạo bởi mỗi mô hình cho cả hai cấu hình temp. Bảng 2 chứa tỷ lệ phần trăm trung bình của các code candidates trùng lặp được tạo ra cho mỗi lỗi cũng như tỷ lệ phần trăm trung bình của các candidates biên dịch được, đạt regression tests, và đạt cả regression và trigger tests (plausible) trên các lỗi.

Từ Bảng 2, chúng ta có thể thấy rằng số lượng duplicates được tạo ra tăng mạnh khi temp. được giảm, điều này được mong đợi vì hành vi mô hình deterministic hơn ở nhiệt độ thấp hơn. Ví dụ, code-davinci-edit-001, được tối ưu hóa cho code edits, tạo ra hơn 90% duplicates ở 0.2 nhưng chỉ 26% ở 0.8.

Nhìn chung, tỷ lệ phần trăm của mã được tạo ra mà biên dịch được khác nhau đáng kể giữa các mô hình. Chúng tôi quan sát thấy rằng chỉ 4.66% mã được tạo bởi code-davinci-002 ở temp. 0.2 biên dịch được, điều này giải thích độ chính xác cực thấp thấy trong Hình 2b. Tuy nhiên, ở temp.

Bảng 2: Thống kê candidate patch trung bình trên các lỗi.
code-davinci-edit-001 code-davinci-002 gpt-3.5-turbo
Temp. 0.2 0.8 0.2 0.8 0.2 0.8
Duplicate 90.7% 26% 87.9% 35.2% 51.8% 8.41%
Compile 74.3% 54.8% 4.66% 35.9% 57.2% 60.4%
Regression 51.2% 41.7% 3.23% 20.19% 25.1% 33.0%
Plausible 20.9% 12.0% 0.65% 6.0% 16.0% 13.4%

0.8 tỷ lệ biên dịch tăng đáng kể, giữa 35.9% và 74.3%. Nhìn vào công trình liên quan, trên một tập con khác của tập dữ liệu Defects4J, các kỹ thuật neural APR SOTA tạo ra patches với tỷ lệ biên dịch 15% đến 28% trong top 100 [59][32][19]. Mặc dù thiết lập APR khác với nl2fix, trigger và relevant tests không bị ẩn khỏi thiết lập APR, chúng tôi quan sát thấy rằng việc sử dụng toàn bộ phương thức làm đầu vào cho LLMs có lợi thế trong việc tạo ra tỷ lệ cao hơn các patches có thể biên dịch được.

Cả gpt-3.5-turbo và code-davinci-edit-001 đạt độ chính xác cao hơn cho pass@100 trong thiết lập temp. 0.8, so với 0.2 (Hình 2b). Tuy nhiên, trong bảng 2 chúng ta thấy rằng tỷ lệ phần trăm trung bình của plausible patches cao hơn trong thiết lập 0.2. Mặc dù điều này có vẻ phản trực quan, sự hiện diện của số lượng duplicates cao cho mỗi patch điều chỉnh pass@k được tính toán trên các lỗi. Nói cách khác, một mô hình có thể có độ tin cậy cao cho một số lượng nhỏ lỗi và tạo ra tỷ lệ phần trăm cao các plausible patches cho lỗi đó.

Kết quả 1.1: Chỉ với mô tả NL về một lỗi, cả ba LLMs đều có thể tạo ra plausible fixes cho một số lượng khiêm tốn các lỗi trong tập dữ liệu, với pass@1 tối đa 6.29% – 17.55% và pass@100 tối đa 42.19% – 54.12%. Trong thiết lập 0-shot, code-davinci-edit-001 đạt độ chính xác tổng thể cao nhất so với cả code-davinci-002 và gpt-3.5-turbo.

Chúng tôi báo cáo số lượng lỗi với plausible fixes cho mỗi dự án trong Bảng 3. Ở cấp độ cao, chúng tôi quan sát thấy rằng việc phân phối plausible fixes được tạo ra từ mỗi mô hình được phân phối trên mọi dự án. Nói chung đối với mọi dự án, ba mô hình tạo ra plausible fixes cho tỷ lệ phần trăm tương tự các lỗi. Có một vài ngoại lệ đáng chú ý, ví dụ, dự án Collections, chỉ chứa 1 lỗi, chỉ được sửa đúng bởi code-davinci-edit-001. gpt-3.5-turbo cũng có độ chính xác thấp hơn trên các dự án Mockito và JacksonDatabind chỉ tạo ra plausible fixes cho 1/21 và 7/67 lỗi tương ứng, so với 6/21 và ít nhất 22/67 cho hai mô hình kia. Tuy nhiên, trên dự án Codec, gpt-3.5-turbo tạo ra plausible fixes cho hai lỗi nhiều hơn hai mô hình kia. Nhìn chung, tổng số lỗi được patch bởi mỗi mô hình phù hợp với các pass@100 metrics thấy trong Hình 2a.

Mặc dù hai mô hình có thể patch cùng số lượng lỗi cho một dự án, các lỗi chính xác được patch có thể khác nhau. Hình 3 cho thấy sự chồng lấp của số lượng lỗi mà mỗi mô hình có thể tạo ra plausible patches. Cả ba mô hình đều có thể tạo ra plausible patches cho cùng 28% (82 trong 283) các lỗi. Tuy nhiên, chúng tôi quan sát thấy rằng khi kết hợp cùng nhau, ba mô hình có thể tạo ra plausible patches cho 64% (183 trong 283) tập dữ liệu. Mỗi mô hình có một tập con duy nhất các lỗi mà hai mô hình kia không thể tạo ra plausible patches: 22 lỗi duy nhất bởi code-davinci-edit-001, 18 bởi gpt-3.5-turbo, và 10 bởi

--- TRANG 6 ---
Sarah Fakhoury, Saikat Chakraborty1, Madan Musuvathi, và Shuvendu K. Lahiri

Bảng 3: Lỗi với plausible patches, theo dự án. Lỗi với plausible fix / tổng số lỗi cho dự án đó.
Cách tiếp cận davinci-002 edit-001 gpt-3.5
Chart 6/6 6/6 5/6
Cli 13/28 16/28 13/28
Codec 7/11 7/11 9/11
Collections 0/1 1/1 0/1
Compress 15/36 19/36 18/38
Csv 10/12 8/12 9/12
JacksonCore 8/13 9/13 5/13
JacksonDatabind 22/67 27/67 7/67
JacksonXml 1/5 2/5 2/5
JxPath 3/10 5/10 4/10
Math 38/73 45/73 45/73
Mockito 6/21 6/21 1/21
Tổng 129/283 151/283 118/283

[Sơ đồ Venn cho thấy sự chồng lấp giữa các lỗi với plausible patches]

Hình 3: Sự chồng lấp giữa các lỗi với plausible patches cho mỗi LLM trong thiết lập 0-shot temp 0.8.

code-davinci-002. Mặc dù một số lỗi có thể dễ sửa hơn đối với các mô hình nhất định, điều này không có vẻ là một artifact nhất quán của dự án mà lỗi bắt nguồn từ đó, như quan sát từ Bảng 3.

Kết quả 1.2: 28% (82/283) các lỗi trong tập dữ liệu có thể được sửa bởi cả ba LLMs. Khi kết hợp cùng nhau, ba LLMs có thể tạo ra plausible patches cho 64% (183/283) các lỗi trong tập dữ liệu.

4.2 RQ2: Các LLMs tạo ra loại candidate fixes như thế nào?

Để trả lời RQ này, chúng tôi chọn cấu hình hoạt động tốt nhất từ RQ1 (temp. 0.8) để hiểu rõ hơn về bản chất của mã được tạo bởi LLMs. Chúng tôi nghiên cứu các đặc điểm của các patches được tạo ra bởi các mô hình khác nhau. Để hiểu các đặc điểm của các patches khác nhau, chúng tôi nghiên cứu sự tương tự của những patches đó w.r.t., mã có lỗi (có mặt như một phần của đầu vào) và mã đã sửa thực tế. Chúng tôi sử dụng CodeBLEU [44] làm đo lường tương tự đại diện. Cho trước hai mã c₁ và c₂, CodeBLEU được định nghĩa là α*B+β*W+γ*S+δ*D, trong đó B,W,S,D là BLEU score, Keywords BLEU score, Syntax match score, và Dataflow match score, tương ứng giữa c₁, và c₂, và α,β,γ,δ là các hằng số weighting thường được đặt thành 0.25⁵. Chúng tôi chọn CodeBLEU cho câu hỏi nghiên cứu này vì nó xem xét syntax và semantic match giữa mã, ngoài lexical match.

Hình 4a cho thấy sự tương tự của patches với mã đã sửa thực tế. Trên cả ba mô hình mà chúng tôi đã nghiên cứu trong bài báo này, các patches đạt cả regression test và bug revealing trigger test (tức là, plausible patch) thể hiện CodeBLEU cao hơn với mã đã sửa thực tế so với các patches không plausible. Kết quả như vậy được mong đợi vì các plausible patches đạt toàn bộ test suite; plausible patch về lý thuyết nên là một tương đương ngữ nghĩa của bản sửa thực tế thể hiện CodeBLEU cao hơn.

Thú vị, các plausible patches được tạo bởi mô hình gpt-3.5-turbo thể hiện biến đổi cao hơn về độ tương tự CodeBLEU với patch thực tế. Inter-Quartile Range (IQR) của CodeBLEU giữa plausible patches và mã đã sửa thực tế lần lượt là 0.18 và 0.19 cho code-davinci-edit-001 và code-davinci-002. Ngược lại, đối với gpt-3.5-turbo, IQR là 0.26. Ngoài ra, đối với các mô hình code-davinci-edit-001 và code-davinci-002, kurtosis lần lượt là 2.62 và 1.12, signifying các phân phối tập trung hơn, trong khi kurtosis của mô hình gpt-3.5-turbo là 0.41, signifying khả năng tạo ra đa dạng, như rõ ràng từ Hình 4a.

✓gpt-3.5-turbo tạo ra các patches đa dạng hơn so với các mô hình khác.

Hơn nữa, chúng tôi phân tích các patches được tạo ra tương tự như thế nào w.r.t., mã có lỗi. Hình 4b cho thấy phân phối CodeBLEU của các loại patches khác nhau trên các mô hình khác nhau. Trên cả ba mô hình, thú vị, chúng tôi quan sát thấy rằng plausible patches thể hiện CodeBLEU cao hơn với mã có lỗi hơn các counterparts không plausible của chúng. Chúng tôi đoán rằng khi các mô hình thực hiện các sửa đổi rộng rãi của mã có lỗi đầu vào, mã kết quả bị nhiễm các vấn đề khác nhau khiến chúng thất bại compilation, regression tests, và trigger test (xem Bảng 2). Chúng tôi đoán rằng LLMs sẽ tạo ra tác động đáng kể hơn trên vấn đề nl2fix nếu chúng tôi có tùy chọn kiểm soát độ lệch khỏi mã có lỗi đầu vào. Tuy nhiên, quan sát rằng plausible patches thể hiện độ tương tự cao hơn với mã có lỗi mở ra khả năng mới về việc xếp hạng mã được tạo bởi LLM dựa trên độ tương tự của chúng với mã có lỗi, mà chúng tôi sẽ điều tra chi tiết trong câu hỏi nghiên cứu tiếp theo.

✓Plausible patches thể hiện độ tương tự cao hơn với mã có lỗi hơn non-plausible patches.

Có sự đồng thuận giữa các học giả và practitioners rằng LLMs như các mô hình davinci hoặc gpt-3.5 đã được pretrained trên hầu như tất cả open source. Do đó, không bất ngờ rằng những

⁵Microsoft's CodeBLEU implementation

--- TRANG 7 ---
Hướng tới Tạo ra Các Chỉnh sửa Mã Chức năng Đúng từ Mô tả Vấn đề bằng Ngôn ngữ Tự nhiên

[Biểu đồ phân tích độ tương tự mã của các patches được tạo bởi các mô hình khác nhau]

Hình 4: Phân tích độ tương tự mã của các patches được tạo bởi các mô hình khác nhau. Chúng tôi phân tích độ tương tự CodeBLEU ở đây.

[Mã ví dụ cho thấy sự tương phản giữa mã đã sửa thực tế và plausible patch được tạo bởi mô hình]

Hình 5: Một ví dụ cho thấy sự tương phản giữa mã đã sửa thực tế và plausible patch được tạo bởi mô hình cho Bug id 20 của dự án JxPath. Mặc dù patch được tạo ra không khớp chính xác với bản sửa ground truth, nó đã đạt tất cả regression tests và trigger test, làm cho nó trở thành một tương đương ngữ nghĩa của bản sửa thực tế.

LLMs này đã quan sát các phương thức trong tập dữ liệu của chúng tôi trong quá trình pretraining tương ứng của chúng. Điều này đặt ra câu hỏi, LLMs ghi nhớ bao nhiều từ pretraining của chúng [8,34,52]? Thật không may, không có cách tốt nào để đo lường điều đó trừ khi chúng tôi biết dữ liệu pretraining của LLMs là gì. Bất kể, để hiểu định tính các patches được tạo ra, chúng tôi điều tra độ tương tự CodeBLEU của các patches với mã có lỗi (có sẵn trong đầu vào cho LLM) và mã đã sửa thực tế. Trên cả ba mô hình, các patches được tạo ra thể hiện độ tương tự cao hơn một chút với mã có lỗi hơn mã đã sửa thực tế (xem Hình 4c). Sự khác biệt như vậy có ý nghĩa thống kê bởi one-sided Wilcoxon sign rank test với p-values của 1.6*10⁻⁹, 9.9*10⁻⁵, 2.7*10⁻¹⁴ lần lượt cho code-davinci-edit-001, code-davinci-002, và gpt-3.5-turbo.

Bảng 4 cho thấy thống kê tóm tắt của các patches khớp chính xác với bản sửa ground truth của tập dữ liệu. Mô hình code-davinci-edit-001 tạo ra đúng ít nhất một patch cho 151 lỗi, trong đó patch cho 32 lỗi khớp chính xác với ground truth. Đối với mô hình code-davinci-002, con số như vậy là 24 trong 129 và 11 trong 118 cho gpt-3.5-turbo. Chỉ 8.5% (146 trong 1724), 10.1% (120 trong 1182), và 3% (73 trong 2356) patches của plausible patches là exact match với ground truth lần lượt cho code-davinci-edit-001, code-davinci-002, và gpt-3.5-turbo. Những kết quả này cho thấy rằng hầu hết các plausible patches khác biệt về mặt cú pháp với bản sửa ground truth.

Trong Hình 5, chúng tôi cho thấy một trong những plausible patches được tạo bởi gpt-3.5-turbo cho ví dụ trong Hình 1. Mã có lỗi đặt sai vị trí các arguments của hàm containsMatch trong dòng 16 (Hình 5(a)), trong khi patch được viết bởi developer đã sửa lỗi bằng cách đặt các arguments ở vị trí đúng (dòng 16 trong Hình 5(b)). Hình 5(d) cho thấy một mã đã sửa được tạo bởi mô hình gpt-3.5-turbo, là tương đương ngữ nghĩa của mã được viết bởi developer. Thực tế, bản sửa được tạo bởi LLM thực sự inline implementation của hàm containsMatch (được hiển thị trong Hình 5(c)) vào ngữ cảnh (dòng 23-28 trong Hình 5(d)). Ngoài ra, patch được tạo bởi LLM refactor mã bằng cách trích xuất hai biến tương ứng với

--- TRANG 8 ---
Sarah Fakhoury, Saikat Chakraborty1, Madan Musuvathi, và Shuvendu K. Lahiri

Bảng 4: Thống kê của lỗi và patches mà các mô hình tạo ra patches khớp chính xác với bản sửa ground truth
Mô hình # Lỗi # EM∗ # Plausible # EM
Đã sửa Patches Patches
code-davinci-edit-001 151 32 1724 146
code-davinci-002 129 24 1182 120
gpt-3.5-turbo 118 11 2356 73
∗EM: Exact Match bỏ qua whitespaces

hai boolean expressions được sử dụng trong mã gốc, làm cho mã kết quả dễ đọc hơn. Chúng tôi quan sát thấy rằng mặc dù có thể LLMs đã thấy mọi thứ open source, chúng khám phá các biến thể mã mới khi được áp dụng cho vấn đề NL2Fix.

✓LLMs có thể không luôn ghi nhớ mã từ pretraining của chúng. Chúng khám phá mã mới khi cố gắng tạo ra patches.

Kết quả 2: Trên cả ba mô hình mà chúng tôi nghiên cứu, các plausible patches thể hiện độ tương tự cao hơn với mã có lỗi đầu vào hơn non-plausible patches với sự khác biệt trong median similarity của ∼0.15. Các plausible patches cũng thể hiện độ tương tự cao hơn với mã có lỗi hơn mã đã sửa thực tế với ý nghĩa thống kê (p-value <<0.05). Các plausible patches khớp chính xác với patch được viết bởi developer cho tới 10% các trường hợp.

4.3 RQ3: Các LLMs cần những nguồn thông tin gì để tạo ra bản sửa cho NL2Fix?

Để xác định thông tin gì cần thiết để tạo ra plausible fixes, chúng tôi sử dụng các kỹ thuật prompting khác nhau cung cấp các mức độ thông tin khác nhau cho mỗi LLM và đánh giá các pass@k metrics cho mỗi cách tiếp cận. Trong RQ1 và RQ2, chúng tôi đã sử dụng prompt 0-shot cơ bản, chứa tiêu đề và mô tả issue, như được mô tả trong Phần 3.3.

Bảng 5 cho thấy cách thay đổi prompt cơ bản này tác động đến pass@k cho các prompts sau:

(1) 0-shot là cấu trúc prompt tiêu chuẩn được sử dụng trong RQ1-2, như được đại diện trong Hình 1.
(2) 'Issue Title' chỉ ra việc loại bỏ mô tả issue và bảo toàn phần còn lại của cấu trúc prompt.
(3) 1-shot thêm một ví dụ về một issue, phương thức có lỗi tương ứng, và bản sửa đúng cho mô hình học từ đó. Sau đó phần còn lại của prompt bảo toàn format gốc. Ví dụ được chọn dựa trên độ tương tự của nó với mã cần được sửa. Chi tiết về việc lựa chọn này có thể được tìm thấy trong Phần 3.3
(4) Đối với Reasoning Extraction, chúng tôi sửa đổi prompt yêu cầu gpt-3.5-turbo chia nhỏ vấn đề thành hai bước: 1) localize các dòng có lỗi trong phương thức gốc và 2) giải thích tại sao những dòng này chứa lỗi trước khi yêu cầu bản sửa. Một ví dụ được hiển thị trong Hình 6

Trên cả ba mô hình, chúng tôi quan sát thấy rằng so với việc sử dụng tiêu đề issue và mô tả (0-shot) thì chỉ tiêu đề issue không đủ để đạt P@k tương đương, với sự giảm gần 5% – 8% pass@1 và 5% – 30% Pass@100. Điều này chỉ ra rằng mô tả issue chứa thông tin có giá trị cho mô hình để tạo ra plausible fixes.

✓Mô tả Issues cung cấp cho LLMs ngữ cảnh hữu ích để giải quyết vấn đề NL2Fix.

Đối với code-davinci-edit-001 và code-davinci-002, chúng tôi quan sát thấy rằng việc thêm ví dụ trong prompt cho cách tiếp cận 1-shot không cải thiện hiệu suất, và thực tế giảm tất cả pass@k so với các nỗ lực 0-shot. Với thiết lập 1-shot, tỷ lệ phần trăm trung bình của patches biên dịch được cho code-davinci-edit-001 giảm từ 54.8% (Bảng 2) trong thiết lập 0-shot xuống 24.7%. Nhìn kỹ hơn vào các patches được tạo ra trong nỗ lực 1-shot bởi code-davinci-edit-001, chúng tôi thấy một số patches chứa mã từ ví dụ 1-shot trong prompt. Ví dụ, phương thức có lỗi gốc cho Issue ID MATH-58 có method signature sau: public double[] fit(){...}. Ví dụ được sử dụng trong thiết lập 1-shot có function signature private boolean isShortOption(String token){..}, và sáu trong số các patches được tạo ra chỉnh sửa hàm isShortOption(), thay vì hàm có lỗi fit(). Điều này chỉ ra rằng mô hình đã cố gắng chỉnh sửa hàm target bằng cách sử dụng mã từ ví dụ. Mô hình code-davinci-edit-001 mong đợi một input code và một tập các hướng dẫn cho việc edit. Mặc dù OpenAI không công khai chi tiết kỹ thuật về cách code-davinci-edit-001 được tạo ra, hành vi quan sát được có thể chỉ ra rằng mô hình này không được fine-tuned với k-shot instructions, điều này có thể giải thích sự suy giảm trong hiệu suất mô hình. Hơn nữa, context window, tức là, số lượng tokens tối đa có thể được sử dụng trong prompt, nhỏ hơn nhiều cho code-davinci-edit-001, khoảng 3000 tokens, so với khoảng 8000 cho code-davinci-002 và khoảng 4000 cho gpt-3.5-turbo. Do đó, một số ví dụ bị cắt cụt để phù hợp với context window để đảm bảo rằng target issue vẫn có mặt trong prompt, dẫn đến các method bodies bị malformed.

Với thiết lập 1-shot, tỷ lệ phần trăm trung bình của patches biên dịch được cho code-davinci-002 giảm từ 35.9% (Bảng 2) trong thiết lập 0-shot xuống 20.5%. Khi nhìn vào các patches được tạo ra, chúng tôi nhận thấy một số lượng lớn là các generations không hoàn chỉnh, tức là, mã không biên dịch được vì hàm được tạo ra không đúng cú pháp. Các ví dụ về điều này bao gồm thiếu closing curly braces (}), thiếu return statements, và các completions dừng lại giữa chừng target function. Lưu ý rằng có hai cách đã biết để dừng completion task —(a) bằng cách đặt maximum length, (b) bằng cách đặt STOP words cụ thể. Chúng tôi đặt maximum length thành 750 tokens. Ngoài ra, chúng tôi thêm method signature của fixed method vào cuối prompt để completion model chỉ cần hoàn thành body của fixed method.

✓Thông tin từ các ví dụ được sử dụng trong 1-shot prompting không giúp độ chính xác pass@k cho code-davinci-edit-001 và code-davinci-002 cho vấn đề NL2Fix.

Mặt khác, so với các nỗ lực 0-shot, pass@k cho gpt-3.5-turbo cải thiện cho cả 1-shot và reasoning extraction prompts. Ví dụ, pass@1 cải thiện từ 13.43% lên 16.22% trong thiết lập 1-shot và 15.44% bằng reasoning extraction. Hình 6 cho thấy một ví dụ về tương tác trong cấu hình prompt reasoning extraction. Ví dụ là từ dự án JacksonDatabind

--- TRANG 9 ---
Hướng tới Tạo ra Các Chỉnh sửa Mã Chức năng Đúng từ Mô tả Vấn đề bằng Ngôn ngữ Tự nhiên

Bảng 5: Độ chính xác của các cấu hình prompt khác nhau.
Mô hình Prompt P@1 P@5 P@100
code-davinci-edit-001 Issue Title 4.73 15.39 44.12
0-shot 12.18 28.86 54.12
1-shot 4.73 12.75 30.24
gpt-3.5-turbo Issue Title 8.06 17.69 37.72
0-shot 13.43 24.62 42.14
1-shot 16.22 31.93 56.93
∗R.E. 15.44 25.72 47.68
code-davinci-002 Issue Title 0.25 1.22 14.94
0-shot 6.29 19.8 45.91
1-shot 3.92 14.34 40.09
∗RE: Reasoning Extraction

⁶. Khi được yêu cầu xác định các dòng mã mà lỗi tồn tại, gpt-3.5-turbo trả về vùng khiếm khuyết đúng trong response 1. Sau đó, prompt 2 thêm prompt1 gốc và response 1 như một phần của ngữ cảnh cho prompt 2, với các hướng dẫn bổ sung để giải thích tại sao các dòng mã đã xác định chứa lỗi. Trong response 3 gpt-3.5-turbo giải thích issue và trích xuất một sample input mà mã sẽ thất bại và lỗi tương ứng, từ mô tả issue. Trong prompt cuối cùng, chúng tôi thêm tất cả inputs và responses để yêu cầu phiên bản đã sửa cuối cùng của hàm có lỗi. Đây là một ví dụ mà gpt-3.5-turbo có thể tạo ra một plausible patch trong thiết lập reasoning extraction, nhưng không phải thiết lập 0-shot.

✓Cả Reasoning Extraction và 1-shot prompting approaches đều giúp gpt-3.5-turbo tạo ra số lượng plausible patches cao hơn và tăng pass@k.

[Ví dụ về lỗi được patch đúng bằng Reasoning Extraction]

Hình 6: Một lỗi được patch đúng bằng Reasoning Extraction.

Tuy nhiên, trong thiết lập 1-shot gpt-3.5-turbo có thể tạo ra patches đúng cho 56 lỗi mới, và mất khả năng patch 14 lỗi từ thiết lập 0-shot. Sử dụng Reasoning Extraction, gpt-3.5-turbo có thể tạo ra patches đúng cho 34 lỗi mới, nhưng mất khả năng patch 18 lỗi từ thiết lập 0-shot. So với tất cả các cách tiếp cận được gộp lại với nhau, gpt-3.5-turbo trong thiết lập reasoning extraction chỉ có thể patch duy nhất 4 lỗi. Nhìn vào thông tin chứa trong mô tả issue cho mỗi ví dụ này, chúng tôi quan sát gpt-3.5-turbo có thể localize đúng

⁶https://github.com/fasterxml/jackson-databind/issues/2265

các dòng có lỗi và lý luận về tại sao chúng có lỗi, nhưng chỉ với sự giúp đỡ từ ngữ cảnh trong mô tả issue. Xem Hình 6 để có ví dụ. Mặc dù các kỹ thuật prompting này tăng cường các aggregate performance metrics, chúng cũng có thể làm suy giảm trên một tập con của các lỗi trong tập dữ liệu.

Kết quả 3: Mô tả Issues cung cấp ngữ cảnh hữu ích để giải quyết vấn đề NL2fix. Các kỹ thuật prompting cung cấp ví dụ, tức là, few-shot prompting, và chia nhỏ tác vụ, tức là, reasoning extraction, cải thiện đáng kể độ chính xác của gpt-3.5-turbo trên các aggregate metrics như pass@k, tuy nhiên hiệu suất có thể suy giảm trên các tập con nhất định của tập dữ liệu và không đảm bảo giải pháp so với standard prompting.

4.4 RQ4: Các LLMs có thể được sử dụng để xếp hạng bản sửa cho NL2Fix không?

Nhớ lại, cho trước một tập không có thứ tự gồm n candidate solutions với c correct solutions cho một vấn đề nhất định (một lỗi trong trường hợp của chúng tôi), pass@k metric đề cập đến khả năng chọn ít nhất một correct solution trong k lần thử. Thống kê như vậy hữu ích để đánh giá các language models, nhưng không dễ dàng cung cấp một recommender system thế giới thực hữu ích mà đề xuất một số lượng nhỏ candidate fixes (lên tới k=5) một cách deterministic cho người dùng. Ví dụ, đối với n=100 và k=5, có (n choose k)>75 triệu cách để chọn 5 solutions từ 100 samples. Đối với một công cụ thực tế cho nl2fix, chúng tôi muốn phát triển một (i) cách deterministic để xếp hạng các đề xuất và trình bày top k đề xuất được xếp hạng cho người dùng, và (ii) duy trì độ chính xác cao gần với average pass@k metric.

Trong phần này, chúng tôi tận dụng LLMs để đề xuất một chiến lược ranking đơn giản và generic giúp thực hiện hai mục tiêu (i) và (ii) ở trên. Cụ thể, được truyền cảm hứng từ các phát hiện của chúng tôi từ RQ2, chúng tôi khám phá liệu việc sử dụng độ tương tự giữa embeddings của hàm có lỗi đầu vào và các patches được tạo ra có thể xác định plausible patches hay không.

Chúng tôi tạo ra embeddings cho tất cả các hàm có lỗi và các patches tương ứng bằng mô hình embedding text-embedding-ada-002 từ OpenAI, xem Phần 3 để biết chi tiết. Chúng tôi tính một cosine similarity giữa embeddings cho mọi cặp mã có lỗi, và candidate patch tương ứng. Chúng tôi sử dụng điểm số này để loại bỏ các patches với similarity scores thấp hơn median (0.95). Chúng tôi cố định số này trên các mô hình để có tính nhất quán. Sau đó, để tránh xếp hạng các patches với similarity scores cực cao, ví dụ, 1.0, chúng tôi xếp hạng các patches theo thứ tự cosine similarity thấp nhất (bắt đầu từ 0.95) đến cao nhất. Dựa trên các quan sát từ RQ2, các patches với điểm số thấp hơn có nhiều khả năng thuộc về phân phối của wrong hoặc uncompilable patches. Patches nên đủ gần với chương trình đầu vào có lỗi, nhưng không quá gần đến mức không có thay đổi đáng kể.

Đối với mỗi mô hình, chúng tôi thực hiện lược đồ ranking trên hai tập patches: (a) tất cả patches được tạo ra, và (b) tập con của các patches được tạo ra mà đạt compiler.

Để đánh giá chiến lược ranking, chúng tôi chọn các cấu hình LLM cho mỗi mô hình với sự khác biệt cao nhất giữa pass@1 và pass@100 từ RQ3 (xem Bảng 5), cũng tình cờ là cấu hình hoạt động tốt nhất cho mỗi mô hình.

Bảng 6 cho thấy ranked pass@1 và ranked pass@5 accuracy (ký hiệu bởi r.P@k) cho hai tập patches: 1) trước (ký hiệu

--- TRANG 10 ---
Sarah Fakhoury, Saikat Chakraborty1, Madan Musuvathi, và Shuvendu K. Lahiri

Bảng 6: Độ chính xác của ranking với và không có compiler pruning.
Cách tiếp cận Metric gpt-3.5 davinci-edit davinci-002
1-shot 0-shot 0-shot
All r.P@1 (P@1) 16.96 (16.22) 13.78 (12.18) 11.30 (6.29)
r.P@5 (P@5) 31.8 (31.93) 30.74 (28.86) 17.66 (19.8)
Pruned r.P@1 (P@1) 21.20 (22.22) 19.78 (18.99) 17.66 (16.89)
r.P@5 (P@5) 34.98 (39.66) 35.68 (36.73) 34.2 (33.97)

là All) và 2) sau (ký hiệu là Pruned) pruning các compiler errors. Để tham khảo, chúng tôi cũng báo cáo các P@k metrics trong ngoặc đơn.

Chúng tôi quan sát thấy rằng pruning compiler errors thực sự cải thiện metric P@k, đặc biệt là P@5 cho mô hình davinci-002 hơn 14% (33.97 tăng từ 19.8). Thứ hai, trong khi cung cấp tính determinism, r.P@1 cải thiện so với P@1 cho hầu hết các cấu hình ngoại trừ một sự giảm nhẹ cho gpt-3.5-turbo cho Pruned. Đối với davinci-002, ranking cải thiện r.P@1 hơn 5 percentage points. Cuối cùng, r.P@5 vẫn gần với P@5 cho hầu hết các cấu hình ngoại trừ gpt-3.5 nơi r.P@5 kém hơn P@5 4.68%.

Kết quả 4: Bằng cách pruning compilation failures LLMs có thể đạt cao tới 39.66% pass@5. Sử dụng cosine similarity giữa embeddings của candidate patches và mã có lỗi đầu vào, chúng tôi có thể áp dụng một chiến lược ranking deterministic duy trì độ chính xác cao này với 34.2% - 35.68% r.pass@5.

5 CÔNG TRÌNH LIÊN QUAN
Công trình của chúng tôi liên quan chặt chẽ nhất với hai dòng công trình rộng lớn (a) automated code editing, và (b) automatic program repair (APR).

Automated Code Editing. Các công trình trước đây trong học code editing bao gồm học để edit mã cho refactoring [35,46], học các semantic code changes cho các lỗi được tìm thấy bởi code analyzers [47]. Các cách tiếp cận gần đây tận dụng các kỹ thuật Deep Learning để học các frequent code edit patterns từ code changes được khai thác từ GitHub [9,13,53].

Ngoài việc học từ các thay đổi lịch sử, một số cách tiếp cận gần đây [10,63] đề xuất hướng dẫn code editing với các auxiliary inputs như commit message. Chúng tôi lập luận rằng commit messages là mô tả post-facto của code changes, và không nắm bắt ý định của thay đổi, mà là một summarization của thay đổi. Ngược lại, issue report mà chúng tôi xem xét trong vấn đề nl2Fix là mô tả ante-facto của các thay đổi, có thể nắm bắt ý định gần hơn. Tufano et al. [55] đề xuất tự động hóa hoạt động code-review bằng cách editing mã dựa trên các comments của reviewers. Mặc dù gần nhất với công trình của chúng tôi, các cách tiếp cận khác nhau trong việc sử dụng tests và semantic metrics như pass@k để validate tính đúng đắn chức năng và defect-freedom của các patches được đề xuất.

Automatic Program Repair.
Các cách tiếp cận cho APR rộng rãi rơi vào các kỹ thuật dựa trên search và các phương pháp dựa trên machine learning. Để có tổng quan toàn diện về các kỹ thuật automated program repair, chúng tôi giới thiệu độc giả đến các công trình gần đây khảo sát lĩnh vực này [15, 16, 18, 36].

Các kỹ thuật dựa trên search sử dụng cách tiếp cận generate-and-validate, nơi các biến thể của mã gốc được tạo ra và sau đó được đánh giá bằng các failing tests [24,42,48,51,58]. Những cách tiếp cận này transform mã có lỗi bằng các transformation khác nhau bao gồm random transformation [30,42,58], manually designed transformation [48], và transformation được học từ corpus mã trước đây [12, 24, 29, 54].

Gần đây, các nhà nghiên cứu cũng đã tận dụng LLMs cho APR. Alpharepair [59] sử dụng APR kiểu cloze nơi một LLM trực tiếp fill-in mã đúng cho trước ngữ cảnh xung quanh từ chương trình có lỗi. Xia et al. [60] sử dụng ChatGPT để thiết lập vấn đề APR conversational nơi feedback từ các failed patches được sử dụng để bổ sung các prompts tiếp theo cho LLMs. Cuối cùng, có các cách tiếp cận tận dụng LLMs để tạo ra trigger tests cho vấn đề APR từ mô tả Issues [23] cũng như hỗ trợ rootcausing cho APR [37]. Cuối cùng, Fan et al. [14] tận dụng các phương pháp APR (bao gồm những cái dựa trên LLMs) để repair mã được tạo ra từ ý định ngôn ngữ tự nhiên.

Mặc dù liên quan chặt chẽ, cách tiếp cận của chúng tôi khác biệt tinh tế với APR trong việc sử dụng hidden tests chỉ được sử dụng để evaluation và không bao giờ được chỉ định làm inputs cho thuật toán repair. Điều này làm cho nó trở thành vấn đề áp dụng hơn cho các bản sửa lỗi thế giới thực có thể không có failing tests available (hoặc tốn kém một cách cấm đoán để tận dụng) trong inference time để giúp rootcausing các dòng có lỗi và validating các fixes.

6 HẠN CHẾ VÀ CÁC MỐI ĐE DỌA
Tính ổn định của đầu ra mô hình. Vì chúng tôi đã sử dụng OpenAI web API để truy cập các mô hình khác nhau, chúng tôi không thể kiểm soát tính stochasticity của đầu ra bởi mô hình. Bản thân các mô hình thường được cập nhật. Điều này tạo ra mối đe dọa cho tính replicability của nghiên cứu của chúng tôi. Để giảm thiểu mối đe dọa này, chúng tôi công khai tất cả các đầu ra được tạo bởi các mô hình.

Giả định về tính đúng đắn của patch. Trong bài báo này, chúng tôi tận dụng tests để xác định liệu một fix có plausible hay không. Tuy nhiên, một plausible patch như vậy có thể không sửa lỗi hoàn toàn. Điều này có thể cần phân tích thủ công hoặc tương đương ngữ nghĩa với bản sửa do người dùng cung cấp. Tuy nhiên, cái trước là không khả thi cho một benchmark quy mô lớn như Defects4J-Nl2fix, và các kỹ thuật kiểm tra tương đương ngữ nghĩa không mở rộng để xử lý hầu hết các chương trình thực. Cho rằng test-suites không bao giờ exhaustive, chúng tôi có thể kêu gọi nghiên cứu gần đây điều tra patch-correctness [28, 56, 61] để cải thiện confidence trên các patches.

Tổng quát hóa của các phát hiện. Cho số lượng tương đối nhỏ các lỗi (283) được xem xét trong benchmark Defects4J-Nl2fix, các phát hiện của chúng tôi có thể không tổng quát hóa cho các lỗi tùy ý trên các ngôn ngữ và software repositories khác nhau. Để giảm thiểu mối đe dọa này, chúng tôi sử dụng các lỗi thế giới thực từ các dự án open source.

7 KẾT LUẬN
Trong bài báo này, chúng tôi thúc đẩy vấn đề nl2fix, định nghĩa benchmark đầu tiên Defects4J-Nl2fix, và thực hiện đánh giá thực nghiệm chi tiết của các SOTA LLMs khác nhau trên vấn đề này.

Chúng tôi tin rằng tác vụ nl2fix cùng với các benchmarks thách thức như Defects4J-Nl2fix sẽ phục vụ như một benchmark thế giới thực quan trọng để đánh giá thế hệ tương lai của những LLMs này (như GPT-4), trong khi tận dụng các emergent behaviors mới của LLMs như vậy (như khả năng của những mô hình này để dự đoán tính đúng đắn) để cải thiện hiệu suất trên các benchmarks như vậy. Trong công trình tương lai, chúng tôi dự định kết hợp issue-driven trigger test generation [23] và user-in-the-loop [27] để cải thiện sự tin tưởng trong các fixes được tạo ra, cũng như mở rộng framework của chúng tôi đến vấn đề nl2edit tổng quát hơn để

--- TRANG 11 ---
Hướng tới Tạo ra Các Chỉnh sửa Mã Chức năng Đúng từ Mô tả Vấn đề bằng Ngôn ngữ Tự nhiên

bao gồm các hình thức khác của program evolution bao gồm feature additions, refactorings cũng như optimizations.

TÀI LIỆU THAM KHẢO
[1][n. d.]. Amazon CodeWhisperer. https://aws.amazon.com/codewhisperer/. Truy cập: 2023-03-29.
[2][n. d.]. Atlassian - JIRA. https://www.atlassian.com/software/jira. Truy cập ngày 29 tháng 3 năm 2023..
[3] [n. d.]. Ghostwriter. https://replit.com/ai. Truy cập: 2023-03-29.
[4][n. d.]. GitHub Copilot x The Future of AI-Powered Software Development. https://www.linkedin.com/pulse/github-copilot-x-future-ai-powered-software-development-github/. Truy cập: 28 tháng 3 năm 2023.
[5] [n. d.]. Tabnine. https://www.tabnine.com/. Truy cập: 2023-03-29.
[6]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al .2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[7]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models. https://doi.org/10.48550/ARXIV.2108.07732
[8]Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646 (2022).
[9]Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray. 2020. Codit: Code editing with tree-based neural models. IEEE Transactions on Software Engineering 48, 4 (2020), 1385–1399.
[10] Saikat Chakraborty and Baishakhi Ray. 2021. On multi-modal learning of editing source code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 443–455.
[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. https://doi.org/10.48550/ARXIV.2107.03374
[12] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE Transactions on Software Engineering 47, 9 (2019), 1943–1959.
[13] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations to detect and fix bugs in programs. InInternational Conference on Learning Representations (ICLR) .
[14] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated Repair of Programs from Large Language Models. arXiv:2205.10583 [cs.SE]
[15] Xiang Gao, Yannic Noller, and Abhik Roychoudhury. 2022. Program Repair. arXiv:2211.12787 [cs.SE]
[16] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2019. Automatic Software Repair: A Survey. IEEE Transactions on Software Engineering 45, 1 (2019), 34–67. https://doi.org/10.1109/TSE.2017.2755013
[17] GitHub. 2022. GitHub Copilot. Truy cập ngày 5 tháng 8 năm 2022. https://github.com/features/copilot/.
[18] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated program repair. Commun. ACM 62, 12 (2019), 56–65.
[19] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. Cure: Code-aware neural machine translation for automatic program repair. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE, 1161–1173.
[20] Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is ChatGPT a good translator? A preliminary study. arXiv preprint arXiv:2301.08745 (2023).
[21] Søren Johansen. 1995. Likelihood-based inference in cointegrated vector autoregressive models . OUP Oxford.
[22] René Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of existing faults to enable controlled testing studies for Java programs. In Proceedings of the 2014 international symposium on software testing and analysis . 437–440.
[23] Sungmin Kang, Juyeon Yoon, and Shin Yoo. [n. d.]. Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction. In IEEE/ACM International Conference on Software Engineering (ICSE) . IEEE, (to appear).
[24] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic patch generation learned from human-written patches. In 2013 35th International Conference on Software Engineering (ICSE) . IEEE, 802–811.
[25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).
[26] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al .2023. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLoS digital health 2, 2 (2023), e0000198.
[27] Shuvendu K. Lahiri, Aaditya Naik, Georgios Sakkas, Piali Choudhury, Curtis von Veh, Madanlal Musuvathi, Jeevana Priya Inala, Chenglong Wang, and Jianfeng Gao. 2022. Interactive Code Generation via Test-Driven User-Intent Formalization. arXiv preprint arXiv:2208.05950 (2022).
[28] Xuan-Bach D Le, Lingfeng Bao, David Lo, Xin Xia, Shanping Li, and Corina Pasareanu. 2019. On reliability of patch correctness assessment. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE, 524–535.
[29] Xuan Bach D Le, David Lo, and Claire Le Goues. 2016. History driven program repair. In 2016 IEEE 23rd international conference on software analysis, evolution, and reengineering (SANER) , Vol. 1. IEEE, 213–224.
[30] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012. GenProg: A Generic Method for Automatic Software Repair. IEEE Transactions on Software Engineering 38, 1 (2012), 54–72. https://doi.org/10.1109/TSE.2011.104
[31] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, et al .2022. Automating code review activities by large-scale pre-training. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 1035–1047.
[32] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. 2020. Coconut: combining context-aware neural translation models using ensemble for program repair. In Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis . 101–114.
[33] Paula Maddigan and Teo Susnjak. [n. d.]. Chat2vis: Generating data visualisations via natural language using chatgpt, codex and gpt-3 large language models. In IEEE/ACM International Conference on Software Engineering (ICSE) . IEEE, (to appear).
[34] R Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. 2021. How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven. arXiv preprint arXiv:2111.09509 (2021).
[35] Na Meng, Lisa Hua, Miryung Kim, and Kathryn S McKinley. 2015. Does automated refactoring obviate systematic editing?. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering , Vol. 1. IEEE, 392–402.
[36] Martin Monperrus. 2018. Automatic Software Repair: A Bibliography. ACM Comput. Surv. 51, 1, Article 17 (jan 2018), 24 pages. https://doi.org/10.1145/3105906
[37] Manish Motwani and Yuriy Brun. [n. d.]. Better Automatic Program Repair by Using Bug Reports and Tests Together.
[38] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730–27744.
[40] Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch. 2022. Bidirectional Language Models Are Also Few-shot Learners. arXiv preprint arXiv:2209.14500 (2022).
[41] Junaid Qadir. 2022. Engineering education in the era of ChatGPT: Promise and pitfalls of generative AI for education. (2022).
[42] Yuhua Qi, Xiaoguang Mao, Yan Lei, Ziying Dai, and Chengsong Wang. 2014. The strength of random search on automated program repair. In Proceedings of the 36th International Conference on Software Engineering . 254–265.
[43] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476 (2023).
[44] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[45] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems . 1–7.
[46] Reudismam Rolim, Gustavo Soares, Loris D'Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Björn Hartmann. 2017. Learning syntactic program transformations from examples. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE) . IEEE, 404–415.
[47] Reudismam Rolim, Gustavo Soares, Rohit Gheyi, Titus Barik, and Loris D'Antoni. 2018. Learning quick fixes from code repositories. arXiv preprint arXiv:1803.03806 (2018).

--- TRANG 12 ---
Sarah Fakhoury, Saikat Chakraborty1, Madan Musuvathi, và Shuvendu K. Lahiri

[48] Ripon K Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R Prasad. 2017. Elixir: Effective object-oriented program repair. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 648–659.
[49] Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. 2023. An analysis of the automatic bug fixing performance of chatgpt. arXiv preprint arXiv:2301.08653 (2023).
[50] Nigar M Shafiq Surameery and Mohammed Y Shakor. 2023. Use chat gpt to solve programming bugs. International Journal of Information Technology & Computer Engineering (IJITC) ISSN: 2455-5290 3, 01 (2023), 17–22.
[51] Shin Hwei Tan and Abhik Roychoudhury. 2015. relifix: Automated repair of software regressions. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering , Vol. 1. IEEE, 471–482.
[52] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems 35 (2022), 38274–38290.
[53] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk. 2019. On learning meaningful code changes via neural machine translation. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE, 25–36.
[54] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2018. An empirical investigation into learning bug-fixing patches in the wild via neural machine translation. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering . 832–837.
[55] Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele Bavota. 2021. Towards automating code review activities. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE, 163–174.
[56] Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, Deqing Zou, Xiaoguang Mao, and Hai Jin. 2020. Automated patch correctness assessment: How far are we?. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering . 968–980.
[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).
[58] Westley Weimer, Zachary P Fry, and Stephanie Forrest. 2013. Leveraging program equivalence for adaptive program repair: Models and first results. In 2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 356–366.
[59] Chunqiu Steven Xia and Lingming Zhang. 2022. Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-Shot Learning. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Singapore, Singapore) (ESEC/FSE 2022) . Association for Computing Machinery, New York, NY, USA, 959–971. https://doi.org/10.1145/3540250.3549101
[60] Chunqiu Steven Xia and Lingming Zhang. 2023. Conversational Automated Program Repair. arXiv:2301.13246 [cs.SE]
[61] Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, and Gang Huang. 2018. Identifying patch correctness in test-based program repair. In Proceedings of the 40th international conference on software engineering . 789–799.
[62] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, and Qianxiang Wang. 2023. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models. arXiv preprint arXiv:2302.00288 (2023).
[63] Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. 2022. CoditT5: Pretraining for Source Code and Natural Language Editing. In37th IEEE/ACM International Conference on Automated Software Engineering . 1–12.
