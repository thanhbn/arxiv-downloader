# 2309.05447.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2309.05447.pdf
# Kích thước tệp: 854259 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
DoG-Instruct: Hướng tới Dữ liệu Điều chỉnh Hướng dẫn Cao cấp thông qua
Bao bọc Hướng dẫn dựa trên Văn bản
Yongrui Chen1,2, Haiyun Jiang3,∗, Xinting Huang3, Shuming Shi3& Guilin Qi1,2†
1Đại học Đông Nam
2Phòng thí nghiệm Trọng điểm Công nghệ Trí tuệ Nhân tạo Thế hệ Mới
và các Ứng dụng Liên ngành (Đại học Đông Nam), Bộ Giáo dục
{yrchen,gqi}@seu.edu.cn
3Phòng thí nghiệm AI Tencent
{haiyunjiang,jeffjhhuang,shumingshi}@tencent.com
Tóm tắt
Việc cải thiện khả năng tuân theo hướng dẫn
của LLMs phụ thuộc rất nhiều vào sự có sẵn
của các cặp hướng dẫn-phản hồi chất lượng
cao. Thật không may, các phương pháp hiện
tại được sử dụng để thu thập các cặp này gặp
phải hoặc chi phí lao động không thể chi trả
được hoặc những ảo giác nghiêm trọng trong
việc tự tạo ra của LLM. Để giải quyết những
thách thức này, bài báo này đề xuất một giải
pháp có thể mở rộng. Nó bao gồm việc đào
tạo LLMs để tạo ra các cặp hướng dẫn-phản
hồi dựa trên các tài liệu do con người viết,
thay vì chỉ dựa vào việc tự tạo ra mà không
có bối cảnh. Phương pháp được đề xuất của
chúng tôi không chỉ khai thác các ưu điểm
của các tài liệu do con người viết trong việc
giảm các ảo giác mà còn sử dụng một LLM
để bao bọc cách thể hiện của các tài liệu, điều
này cho phép chúng tôi thu hẹp khoảng cách
giữa các phong cách tài liệu khác nhau và
phản hồi AI tiêu chuẩn. Các thí nghiệm chứng
minh rằng phương pháp của chúng tôi vượt
trội hơn các phương pháp điển hình hiện có
trên nhiều điểm chuẩn. Đặc biệt, so với đường
cơ sở hoạt động tốt nhất, LLM được đào tạo
bằng cách sử dụng tập dữ liệu được tạo ra
của chúng tôi thể hiện sự cải thiện tương đối
10% về hiệu suất trên AlpacaEval, mặc dù chỉ
sử dụng 1/5 dữ liệu đào tạo của nó. Hơn nữa,
một đánh giá thủ công toàn diện xác nhận
chất lượng của dữ liệu chúng tôi đã tạo ra.
Trình bao bọc được đào tạo của chúng tôi có
sẵn công khai1.
1 Giới thiệu
Những nỗ lực gần đây trong cộng đồng NLP đã tập
trung vào điều chỉnh hướng dẫn (Sanh et al., 2022;
Mishra et al., 2022; Wei et al., 2022), tức là cải
thiện khả năng hiểu và tuân theo hướng dẫn của các
mô hình ngôn ngữ lớn (LLMs) (Brown et al., 2020;
Chowdhery et al., 2022; Touvron et al., 2023a).
Các LLMs tiên tiến đã được đào tạo để có khả năng
tạo ra các đầu ra tùy chỉnh khi được cung cấp các
∗Tác giả Liên hệ
†Tác giả Liên hệ
1https://github.com/Bahuia/
Dog-Instructhướng dẫn cụ thể (với các đầu vào), cho phép chúng
thích ứng với các nhiệm vụ mới mà không cần tiếp
xúc trước đó.
Là một vấn đề quan trọng trong việc cải thiện
khả năng tuân theo hướng dẫn của LLMs, cách thu
thập các cặp hướng dẫn-phản hồi chất lượng cao
đang trở nên phổ biến. Phần lớn các phương pháp
hiện có hoặc dựa vào việc thuê các chuyên gia để
viết hướng dẫn cho các nhiệm vụ NLP khác nhau
(Wang et al., 2022; Conover et al., 2023) hoặc thúc
đẩy việc sử dụng LLMs để tự động tạo ra hướng
dẫn (Wang et al., 2023; Taori et al., 2023; Yin et
al., 2023). Thật không may, những phương pháp
này có những hạn chế hoặc về mặt khả năng mở
rộng do tính chất tốn nhiều lao động của quá trình
chú thích hoặc về mặt chất lượng dữ liệu do vấn đề
ảo giác (Zhang et al., 2023; Zheng et al., 2023)
liên quan đến LLMs.
Nghiên cứu gần đây (Köksal et al., 2023; Li et
al., 2023a) đã cung cấp một ý tưởng tiềm năng
hơn: đầu tiên trực tiếp sử dụng các tài liệu do con
người viết như các phản hồi điển hình và sau đó sử
dụng LLMs để dự đoán các hướng dẫn ẩn của người
dùng. Phương pháp này, được gọi là dịch ngược
hướng dẫn (Li et al., 2023a), dựa trên niềm tin
rằng các tài liệu do con người viết vốn ít có khả
năng gặp phải các ảo giác so với các phản hồi được
tạo ra hoàn toàn bởi LLMs.
Tuy nhiên, chúng tôi cho rằng ngay cả khi một
tài liệu không có ảo giác, việc sử dụng nó trực tiếp
như một phản hồi điển hình không phải lúc nào
cũng phù hợp. Điều này được quy cho hai lý do
chính: a) Đầu tiên, không phải tất cả các phần của
một tài liệu đều có giá trị trong việc xây dựng một
phản hồi. Ví dụ, phần màu đỏ của tài liệu (A) trong
Hình 1 hoàn toàn vô dụng để dịch ngược hướng
dẫn kết quả (hộp vàng). Hơn nữa, các phần có giá
trị của tài liệu thường có ranh giới mờ nhạt. Ví dụ,
văn bản màu đỏ của (B) nhằm tạo ra một bầu không
khí căng thẳng, một lần nữa không phù hợp để giữ
lại trong phản hồi, nhưng nó cũng có một số liên
quan đến chủ đề (nghiên cứu về người ngoài hành
tinh) và do đó khó có thể được lọc ra bằng tiền xử
lý đơn giản. b) Thứ hai, do các mục đích viết khác
nhau, thường có những khoảng cách trong cách thể
hiện giữaarXiv:2309.05447v2  [cs.CL]  25 Tháng 5 2024

--- TRANG 2 ---
•Hướng dẫn：Mô tả bộ xử lý AMD
Athlon 200GE sắp ra mắt.
•Phản hồi：Đầu năm nay, có tin đồn rằng AMD
đang chuẩn bị APU Athlon 200GE ... và một GPU
Vega 3 với 192 bộ xử lý luồng ... Bài viết trước:
Gia đình bị tiêu diệt trong giấc ngủ, ngăn chặn
con gái cả ...Tài liệu Do Con người Viết
Bình luận: Một nghiên cứu mới cho rằng những
người lùn xanh có thể không... xanh ... Một
nghiên cứu mới từ Đại học Oxford cho rằng người
ngoài hành tinh có thể chịu sự chi phối của những
quá trình tiến hóa tương tự hoặc giống như chúng
ta ... Sau nghiên cứu này, chúng ta có thể tin
tưởng ai?•Hướng dẫn：Giới thiệu nghiên cứu
từ Đại học Oxford về chuyển đổi tiến
hóa trong đời sống người ngoài hành
tinh.
•Phản hồi：
Tôi đã lướt Internet trong hai tuần qua và đã biết
rằng tổng số thương vong dân sự trong Chiến
tranh Việt Nam, bao gồm cả ở Bắc và Nam Việt
Nam, là khoảng 2 triệu người... Than ôi, cuộc
chiến quá bi thảm. Tất nhiên, đây chỉ là kết quả
thống kê cá nhân của tôi, có thể không chính xác.•Hướng dẫn：Tổng số thương vong
dân sự trong Chiến tranh Việt Nam là
bao nhiêu?
•Phản hồi：•Hướng dẫn：Bạn có thể mô tả bộ xử lý AMD
Athlon 200GE sắp ra mắt cho tôi không?
•Phản hồi：Chắc chắn! Đầu năm nay, có tin đồn rằng
AMD đang chuẩn bị APU Athlon 200GE ... và một GPU
Vega 3 với 192 bộ xử lý luồng.
•Hướng dẫn：Giới thiệu nghiên cứu từ Đại học
Oxford về chuyển đổi tiến hóa trong đời sống người
ngoài hành tinh.
•Phản hồi：Một nghiên cứu mới từ Đại học Oxford ...
quá trình tiến hóa như chúng ta ... Nghiên cứu có thể
có những tác động quan trọng đối với nguồn gốc của
sự sống.
•Hướng dẫn：Tổng số thương vong dân sự ước tính
trong Chiến tranh Việt Nam (bao gồm Bắc và Nam
Việt Nam) là bao nhiêu?
•Phản hồi：Tổng số ước tính ... Con số này bao gồm
cả thương vong trực tiếp và gián tiếp, chẳng hạn
như ... Lưu ý rằng số chính xác thương vong dân sự
khó xác định do tính chất của xung đột và thiếu dữ
liệu đáng tin cậy.Dịch ngược Hướng dẫn Bao bọc Hướng dẫn
Sao chépDự đoán
Sao chépDự đoán
Sao chépDự đoánThông tin vô dụng
(B)
(C)(A)
Ranh giới phản hồi mờ nhạt
Biểu hiện không khách quanBao bọc
Bao bọc
Bao bọc
Hình 1: Sự khác biệt giữa việc bao bọc hướng dẫn được đề xuất của chúng tôi với dịch ngược hướng dẫn (Köksal et al.,
2023; Li et al., 2023a). Văn bản màu đỏ không phù hợp cho các phản hồi. Văn bản màu xanh cho biết rằng văn bản gốc đã được
thêm, xóa hoặc viết lại bởi LLM để phù hợp hơn với phản hồi tiêu chuẩn mong muốn.

các tài liệu thô và các phản hồi tiêu chuẩn. Như
một minh họa, phần màu đỏ của (C) chứa nhiều
mô tả chủ quan, điều này lệch khỏi tính khách quan
mong đợi của một trợ lý AI.
Trong bài báo này, chúng tôi đề xuất một mô
hình mới để xây dựng dữ liệu điều chỉnh hướng
dẫn, được gọi là bao bọc hướng dẫn. Nó nhằm
đào tạo một LLM mã nguồn mở để xác định các
phần có giá trị từ tài liệu gốc và tiếp tục chuyển
đổi chúng thành các cặp hướng dẫn-phản hồi trôi
chảy và khách quan.
Tóm lại, phương pháp được đề xuất của chúng
tôi bao gồm hai giai đoạn như được trình bày trong
Hình 2. Trong giai đoạn a), một LLM được căn
chỉnh tốt được sử dụng như người thầy để xây
dựng một tập huấn luyện meta Ω cho việc bao bọc
hướng dẫn. Mỗi ví dụ trong tập này bao gồm một
tài liệu được lấy mẫu và cặp hướng dẫn-phản hồi
tương ứng của nó, liên quan đến một trong hai quan
điểm sau. Trong quan điểm căn chỉnh, chúng tôi
sử dụng học theo ngữ cảnh để hướng dẫn LLM
giáo viên tạo ra các cặp hướng dẫn-phản hồi dựa
trên các tài liệu do con người viết. Nó cho phép
thích ứng LLM giáo viên với các phong cách tài
liệu thực tế khác nhau. Trong quan điểm đa dạng,
chúng tôi bắt đầu với một tập hướng dẫn đa dạng
hiện có và nhắc LLM giáo viên tạo ra một tài liệu
giả cho mỗi cặp hướng dẫn-phản hồi một cách
ngược lại. Nó đảm bảo các ví dụ huấn luyện duy
trì sự đa dạng hướng dẫn. Sau đó, chúng tôi sử
dụng tập huấn luyện meta để thực hiện tinh chỉnh
có giám sát trên một LLM được phát hành công
khai, phục vụ như trình bao bọc hướng dẫn của
chúng tôi. Trong giai đoạn b), các tài liệu do con
người viết từ nhiều lĩnh vực được đưa vào trình
bao bọc được đào tạo của chúng tôi để tạo ra các
cặp hướng dẫn-phản hồi. Sau đó, một chiến lược
hậu xử lý đơn giản nhưng hiệu quả được áp dụng
để lọc các ví dụ không hợp lệ dựa trên sự tương
tự theo nghĩa đen. Cuối cùng, chúng tôi đặt tên
cho tập dữ liệu kết quả là Hướng dẫn Dựa trên
Tài liệu (DOG-INSTRUCT), chứa 12,4K cặp
hướng dẫn-phản hồi.
LLM được đào tạo bằng cách sử dụng DOG-
INSTRUCT đạt được sự cải thiện đáng kể 4,8%
về hiệu suất trên AlpacaEval so với đường cơ sở
hoạt động tốt nhất, trong khi chỉ sử dụng 1/5 dữ
liệu đào tạo. Hơn nữa, nó đạt được kết quả tối
ưu trên ba điểm chuẩn được sử dụng rộng rãi khác.
Thông qua đánh giá thủ công thêm, chúng tôi minh
họa rằng phương pháp DOG-INSTRUCT của
chúng tôi hiệu quả giảm thiểu vấn đề ảo giác trong
khi căn chỉnh tài liệu thô với phản hồi mong muốn
về mặt phong cách. Tóm lại, những đóng góp của
bài báo này bao gồm:
•Chúng tôi đề xuất một mô hình mới đào tạo
LLMs để tạo ra các cặp hướng dẫn-phản hồi dựa
trên các tài liệu do con người viết. Nó không chỉ
tận dụng tài liệu để giảm các ảo giác của phản
hồi, mà còn căn chỉnh phong cách của tài liệu
thô với phản hồi lý tưởng bằng cách sử dụng
LLM.
•Chúng tôi phát hành một trình bao bọc hướng
dẫn dựa trên LLAMA được đào tạo tốt có khả
năng tạo ra một cách nhất quán

--- TRANG 3 ---
Ví dụ quan điểm
căn chỉnh
Tập Huấn luyện Meta
Ω
a) Xây dựng Trình bao bọc Hướng dẫn
 b) Tạo Dữ liệu thông qua Trình bao bọc Hướng dẫn
Tài liệu
DoG-InstructTrình bao bọc
Hướng dẫn MTạo cặp
hướng dẫn-phản hồi
Tạo tài liệu giảGPT-4
Ví dụ quan điểm
đa dạngLlama 2
Hậu xử lý
Lấy mẫu
tài liệu
Kho dữ liệu
Open Assistant
Lấy mẫu
tài liệu
Hình 2: Tổng quan quá trình xây dựng DOG-INSTRUCT. Trong giai đoạn a), một tập huấn luyện meta Ω được xây dựng bằng
GPT-4 và được sử dụng để đào tạo trình bao bọc hướng dẫn. Trong giai đoạn b), trình bao bọc tạo ra các cặp hướng dẫn-phản hồi
cho mỗi tài liệu được lấy mẫu, và một chiến lược hậu xử lý được sử dụng để lọc ra các ví dụ không hợp lệ.

các cặp hướng dẫn-phản hồi chất lượng cao cho
các tài liệu trên nhiều lĩnh vực.
•Chúng tôi đã tiến hành một đánh giá toàn diện,
cả tự động và thủ công, chứng minh rằng LLM
được đào tạo bằng cách sử dụng dữ liệu được
tạo ra của chúng tôi vượt trội hơn tất cả các
đường cơ sở được so sánh.

2 Công thức Bài toán
Cho một tập tài liệu {D1,D2, ...,Dn}, trong đó
mỗi Di là một tài liệu do con người viết, mục tiêu
của chúng tôi là xây dựng một tập các cặp {(X1,Y1),
...,(Xm,Ym)}, trong đó m≤n, Xi và Yi biểu thị
hướng dẫn và phản hồi tương ứng, và (Xi,Yi) :=
M(Dj). Ở đây M là một trình bao bọc hướng dẫn
dựa trên LLM chuyển đổi Di thành một cặp hướng
dẫn-phản hồi.

3 Thu thập Dữ liệu DoG-Instruct
Hình 2 cho thấy toàn bộ quá trình của phương pháp
chúng tôi. a) Đầu tiên, trình bao bọc hướng dẫn M
được đào tạo bằng cách sử dụng tập huấn luyện
meta Ω, được xây dựng bởi GPT-4 được căn chỉnh
tốt. b) Sau đó, M nhận các tài liệu được lấy mẫu
D làm đầu vào để tạo ra (X,Y) cho việc xây dựng
DOG-INSTRUCT.

3.1 Kho dữ liệu & Lấy mẫu Tài liệu
Để tạo ra một tập tài liệu đa dạng, chúng tôi sử
dụng kho dữ liệu Pile (Gao et al., 2021), đây là
một bộ sưu tập đa lĩnh vực các tài liệu do con người
viết. Từ Pile, chúng tôi lấy mẫu tài liệu từ sáu lĩnh
vực khác nhau: ArXiv, FreeLaw, StackExchange,
Wikipedia, Github. Theo các công trình hiện có
(Li et al., 2023a; Köksal et al., 2023), chúng tôi
cũng lấy mẫu tài liệu từ Open Assistant1 và Wiki-
How2 để giới thiệu một số ví dụ có cấu trúc. Chúng
1https://huggingface.co/datasets/
OpenAssistant/oasst1
2https://huggingface.co/datasets/
wikihow
tôi chọn ngẫu nhiên một số đoạn văn liên tiếp từ
mỗi văn bản gốc trong kho dữ liệu để làm tài liệu
của chúng tôi. Để đảm bảo rằng mỗi tài liệu chứa
đủ thông tin để tạo ra ít nhất một cặp hướng dẫn-
phản hồi, chúng tôi chỉ giữ lại những tài liệu chứa
từ 500 đến 1000 token.

3.2 Xây dựng Trình bao bọc Hướng dẫn
Để trao quyền cho một LLM tổng quát với khả
năng bao bọc hướng dẫn, chúng tôi cần xây dựng
đủ ví dụ huấn luyện ánh xạ tài liệu D thành cặp
hướng dẫn-phản hồi (X,Y). Lấy cảm hứng từ (Li
et al., 2023a), chúng tôi giao việc này cho GPT-4
được căn chỉnh tốt (OpenAI, 2023) để giảm thiểu
chi phí chú thích thủ công. Chúng tôi đưa ra giả
thuyết rằng một tập huấn luyện meta lý tưởng Ω
nên đáp ứng hai yêu cầu thiết yếu: căn chỉnh và
đa dạng. Căn chỉnh đảm bảo rằng Ω bao quát một
phạm vi rộng các tài liệu thực do con người viết,
cho phép trình bao bọc hiểu các lĩnh vực và phong
cách viết khác nhau. Mặt khác, đa dạng đảm bảo
rằng Ω chứa nhiều hướng dẫn khác nhau, cho phép
trình bao bọc tạo ra các hướng dẫn đa dạng hiệu
quả sau khi đào tạo. Do đó, chúng tôi thu thập các
ví dụ của Ω từ hai quan điểm sau.

Ví dụ Quan điểm Căn chỉnh. Trong phần này,
các ví dụ được xây dựng bằng cách sử dụng GPT-4
để trực tiếp tạo ra các cặp hướng dẫn-phản hồi cho
các tài liệu thực do con người viết. Để đạt được
mục tiêu này, chúng tôi khai thác sức mạnh của
học theo ngữ cảnh (ICL). Cụ thể, đối với mỗi lĩnh
vực, 30 ví dụ được xây dựng thủ công đầu tiên như
các hạt giống. Sau đó, đối với mỗi D, lời nhắc được
đưa vào GPT-4 được ký hiệu bởi (X∗,D1,P1, ...,
Dk,Pk,D), trong đó X∗ là định nghĩa của việc ánh
xạ Dj thành cặp hướng dẫn-phản hồi Pj = (Xj,Yj).
Xem Phụ lục A.1 cho lời nhắc đầy đủ. Các ví dụ
kết quả được ký hiệu bởi Ωa.

Ví dụ Quan điểm Đa dạng. Một cách trực quan,
việc tạo ra các hướng dẫn đa dạng chỉ bằng cách
sử dụng vài chục hạt giống thủ công là khó khăn.

--- TRANG 4 ---
Do đó, chúng tôi bắt đầu từ các hướng dẫn được
phát hành công khai, chẳng hạn như ALPACA, và
sau đó ngược lại kết hợp các hướng dẫn và phản
hồi được cung cấp của chúng để tạo ra các tài liệu
giả. Cụ thể, đối với mỗi cặp hướng dẫn-phản hồi
(X,Y) được lấy mẫu trong ALPACA, chúng tôi
viết lời nhắc để sử dụng GPT-4 tích hợp X và Y
thành một tài liệu mới D̃. Chúng tôi cho phép D̃
bao quát tất cả nội dung từ X và Y, nhưng chúng
tôi cố ý làm mờ ranh giới của chúng. Điều này cho
phép thêm nội dung mới khi cần thiết, trong khi
đảm bảo dòng thông tin mượt mà và mạch lạc.
Những tài liệu giả D̃ và cặp (X,Y) tương ứng của
chúng tạo thành các ví dụ huấn luyện còn lại, được
ký hiệu bởi Ωd. Phụ lục A.2 đưa ra lời nhắc chi
tiết.

Đào tạo Trình bao bọc. chúng tôi chọn LLAMA
2 (Touvron et al., 2023b), một LLM tiên tiến có
sẵn công khai làm trình bao bọc hướng dẫn M
của chúng tôi và thực hiện tinh chỉnh có giám sát
(SFT) trên M bằng cách sử dụng tập huấn luyện
meta được xây dựng Ω = Ωa∪Ωd. Đối với mỗi
tài liệu D và cặp hướng dẫn-phản hồi P = (X,Y)
của Ω, chúng tôi thêm một hướng dẫn thống nhất
U="Chuyển đổi văn bản đã cho thành một
nhiệm vụ. Đầu vào là một văn bản và
Phản hồi chứa hai trường: #hướng dẫn#
và #đầu ra#. ". Sau đó, mất mát huấn luyện được
tính bằng một khả năng logarit,
L(U,D,P) =−|P|∑
j=1logP(tj|U,D, t<j), (1)
trong đó tj là token thứ j của T. Điều quan trọng
cần nhấn mạnh là mặc dù tập huấn luyện meta Ω
của chúng tôi có thể bao gồm các ảo giác, chúng
tôi đưa ra giả thuyết rằng điều này không ảnh hưởng
đến việc học của trình bao bọc M. Điều này là do
mục tiêu chính của chúng tôi đối với M là học
sự chuyển đổi phong cách từ tài liệu thành các cặp
hướng dẫn-phản hồi với tính nhất quán về ngữ
nghĩa. Trong giai đoạn suy luận, chúng tôi độc
quyền sử dụng các tài liệu thực do con người viết
mà không có tài liệu giả, điều này một cách tự
nhiên giảm sự xuất hiện của các ảo giác.

3.3 Tạo Dữ liệu thông qua Trình bao bọc Hướng dẫn
Trong giai đoạn này, chúng tôi sử dụng M được
đào tạo để tạo ra các cặp hướng dẫn-phản hồi cho
20.000 tài liệu do con người viết, đã được lấy mẫu
bằng phương pháp được mô tả trong Phần 3.1. Để
tránh ảo giác mà trình bao bọc tạo ra quá nhiều nội
dung không liên quan đến văn bản gốc, chúng tôi
đề xuất một chiến lược hậu xử lý cho mỗi nhiệm
vụ được tạo ra Ti. Cụ thể, chúng tôi thiết kế một
điểm số σ(Ti) = min(σ̃(Pi,Xi),σ̃(Pi,Yi)) để đo
sự tương tự giữa văn bản và cặp hướng dẫn-phản
hồi, trong đó σ̃(Pi, s) =|t(Di)&t(s)|/|t(s)| và
t(s) biểu thị tập token của văn bản s. Tất cả các
ví dụ (Pi,Ti) sẽ bị loại bỏ trong đó σ(Ti)< θ.

4 Thống kê DoG-Instruct
Thống kê Dữ liệu. Bảng 1 cho thấy thống kê
của các ví dụ quan điểm căn chỉnh Ωa, các ví dụ
quan điểm đa dạng Ωd, tập huấn luyện meta Ω,
và tập dữ liệu DOG-INSTRUCT. Về mặt lý thuyết,
miễn là có một dòng văn bản liên tục, phương
pháp của chúng tôi không có giới hạn trên về lượng
dữ liệu. Tuy nhiên, thông qua thí nghiệm, chúng
tôi phát hiện ra rằng kết quả cạnh tranh có thể đạt
được bằng cách chỉ sử dụng 12k DOG-INSTRUCT
của chúng tôi. DOG-INSTRUCT có xu hướng có
phản hồi dài hơn so với các ví dụ trong Ω. Ngoài ra,
DOG-INSTRUCT có độ lệch chuẩn lớn hơn liên
quan đến trường phản hồi so với Ω. Phần trên của
Bảng 2 trình bày dữ liệu thống kê cho các lĩnh vực
khác nhau trong DOG-INSTRUCT.

Đa dạng của Hướng dẫn. Chúng tôi đã thực hiện
phân tích đa dạng trên DOG-INSTRUCT bằng
phương pháp được mô tả bởi (Wang et al., 2023).

Bảng 1: Thống kê của các ví dụ quan điểm căn chỉnh Ωa,
các ví dụ quan điểm đa dạng Ωd, tập huấn luyện meta Ω
và DOG-INSTRUCT. Ở đây x±y biểu thị trung bình x
và độ lệch chuẩn y.

Số ví dụ # Số Token X # Số Token Y #
Ωa 306 16 ±13 134 ±126
Ωd 2998 43 ±35 140 ±76
Ω 3371 41 ±34 139 ±81
DOG-INSTRUCT 12426 32 ±79 310 ±152

[Tiếp theo là biểu đồ tròn hiển thị đa dạng hướng dẫn với các động từ gốc và danh từ tương ứng]

Hình 3 minh họa sự phân bố của cấu trúc động từ-
danh từ của các hướng dẫn, thể hiện phạm vi đa
dạng.

Liên quan đến Tài liệu Thô. Ngoài ra, chúng tôi
đã tính toán mức độ liên quan của các phản hồi đến
các tài liệu thô. Điểm liên quan trung bình được
hiển thị ở phía dưới Bảng 2, với σ̃ đại diện cho
thước đo liên quan theo nghĩa đen được sử dụng
trong hậu xử lý của chúng tôi, và BS biểu thị Bert-
Score (Zhang et al., 2019) để đánh giá mức độ
liên quan về ngữ nghĩa. Cả về điểm số theo nghĩa
đen và điểm số ngữ nghĩa, các phản hồi thể hiện
mức độ liên quan cao đến các tài liệu thô. Tuy
nhiên, chúng không được căn chỉnh 100% do việc
viết lại phù hợp được thực hiện bởi trình bao bọc
hướng dẫn của chúng tôi.

5 Thí nghiệm

5.1 Thiết lập Thí nghiệm
Tập dữ liệu So sánh. Chúng tôi đã so sánh DOG-
INSTRUCT của chúng tôi với một số tập dữ liệu
điều chỉnh hướng dẫn điển hình: SELF-INSTRUCT
(Wang et al., 2023), và ALPACA (Taori et al., 2023)
được tạo ra tự động bởi các LLMs bao gồm GPT-
3.5-Turbo và text-davinci-003. DYNOSAUR (Yin
et al., 2023) đóng gói lại tập dữ liệu NLP hiện có
của huggingface và tái tạo hướng dẫn cho nó bằng
ChatGPT. LONGFORM (Köksal et al., 2023) và
HUMPBACK (Li et al., 2023a) tương tự nhất với
công việc của chúng tôi ở chỗ chúng tạo ra các
nhiệm vụ bằng cách thực hiện dịch ngược hướng
dẫn. Không giống như những phương pháp này,
DOG-INSTRUCT bao bọc các tài liệu và cẩn thận
chọn lựa các phần có giá trị để tạo thành một cặp
hướng dẫn-phản hồi toàn diện. Vì HUMPBACK
chưa được phát hành, chúng tôi đã có được một
phiên bản không chính thức3 từ HuggingFace, ký
hiệu bởi †.

Chi tiết Triển khai. Tất cả các thí nghiệm của
chúng tôi chạy trên 8 GPU Tesla V100 với FP16.
Chúng tôi đào tạo M bằng LORA (Hu et al., 2022).
Các siêu tham số được thiết lập như sau: (1) Kích
thước batch được thiết lập là 128. (2) Tỷ lệ học
được thiết lập là 1×10−4. (3) Số epoch là 7. (4)
Số token cắt được thiết lập là 2048. (5) Nhiệt độ
và kích thước beam lần lượt là 0 và 4. (6) Các
mô-đun mục tiêu LORA bao gồm [qproj,kproj,
vproj,oproj,upproj,downproj,gateproj,embedtokens,
lmhead].

3https://huggingface.co/datasets/
Spico/Humback

--- TRANG 5 ---
Bảng 2: Thống kê của các lĩnh vực khác nhau trong DOG-INSTRUCT. Độ dài hướng dẫn, đầu vào và đầu ra được đưa ra
dưới dạng số token. BS biểu thị Bert-Score (Zhang et al., 2019). OASST là viết tắt của Open Assistant.

Wikipedia FreeLaw ArXiv StackExchange Github OASST WikiHow
Số ví dụ # 5371 427 450 475 690 946 4060
Độ dài X 15±34 201 ±207 119 ±187 101 ±104 54 ±110 34 ±51 11 ±25
Độ dài Y 347±123 476 ±123 577 ±205 328 ±121 328 ±132 326 ±121 326 ±104
σ̃(D,Y) 0.981 0.949 0.942 0.976 0.978 0.957 0.957
BS(D,Y) 0.981 0.963 0.942 0.911 0.967 0.946 0.930

Bảng 3: Hiệu suất của các phương pháp trên điểm chuẩn AlpacaEval (tỷ lệ thắng so với text-davinci-003 được đánh giá bởi
GPT-4). Trường Text-Grounded cho biết việc tạo hướng dẫn có dựa trên văn bản do con người viết hay không. Avg. Length
biểu thị số token trung bình của các phản hồi mô hình. DOG-INSTRUCT của chúng tôi đạt tỷ lệ thắng cao nhất (53.1%) với
ít ví dụ đào tạo nhất (12.4K).

Trình tạo Dữ liệu Tập dữ liệu Text-Grounded Số ví dụ # Tỷ lệ thắng (%) Độ dài Trung bình
text-davinci-003 LONG FORM ✓ 23.7K 11.7 268
GPT-3.5-TurboSELF-INSTRUCT × 82K 14.2 284
ALPACA × 52K 15.3 271
DYNOSAUR × 800K 2.9 142
EVOL-INSTRUCT × 70K 48.3 669
GPT-4 ALPACA-GPT-4 × 52K 44.5 653
LLAMA 2-7BHUMPBACK†✓ 18K 41.0 755
DOG-INSTRUCT ✓ 12.4K 53.1 1149

Hình 3 minh họa sự phân bố của cấu trúc động từ-
danh từ của các hướng dẫn, thể hiện phạm vi đa
dạng.

5.2 Đánh giá Tự động
Để bắt đầu, chúng tôi đã tiến hành đánh giá tự động
trên nhiều điểm chuẩn. Đối với mỗi tập dữ liệu
được so sánh, Chúng tôi độc lập tinh chỉnh một
LLM cơ sở giống hệt nhau bằng cách sử dụng các
ví dụ huấn luyện tương ứng và đánh giá hiệu suất
của nó trong việc tuân theo hướng dẫn một cách
chính xác.

LLM Cơ sở. Chúng tôi chọn LLAMA 2-7B
(Touvron et al., 2023b) + LORA (Hu et al., 2022)
làm LLM cơ sở. Để dễ trình bày, chúng tôi gọi
LLM cơ sở được đào tạo trên tập dữ liệu x là mô
hình-x.

Điểm chuẩn. Đầu tiên chúng tôi sử dụng đánh giá
GPT-4 từ AlpacaEval (Li et al., 2023b) để đánh giá
chất lượng phản hồi trên 805 hướng dẫn từ Alpaca
Leaderboard. AlpacaEval so sánh tỷ lệ thắng theo
cặp so với mô hình tham chiếu text-davinci-003.
Ngoài ra, chúng tôi đã tiến hành đánh giá trên ba
điểm chuẩn NLG khác: Long-Form Question
Answering (ELI5) (Fan et al., 2019), tập test
Long-Form (LF-Test) (Köksal et al., 2023), và
Super-NaturalInstructions (Super-NI) (Wang et al.,
2022). Không có phương pháp nào kết hợp dữ liệu
từ những điểm chuẩn này. tức là thiết lập zero-shot.

Thước đo Đánh giá Tự động. Đối với AlpacaEval,
chúng tôi chạy các script của nó trực tiếp, sử dụng
GPT-4 để đánh giá. Đối với ELI5 và LF-Test,
chúng tôi làm theo (Köksal et al., 2023; Yin et al.,
2023) để tính điểm Rouge-L (Lin, 2004) và Meteor
(Banerjee và Lavie, 2005). Những điểm số này
được tính bằng cách so sánh đầu ra mô hình với
các tham chiếu được cung cấp trong các tập dữ
liệu tương ứng. Đối với Super-NI, chúng tôi sử
dụng Bert-Score (Zhang et al., 2019) để đánh giá
thay vì các thước đo văn bản dài khác như Rouge.
Lựa chọn này được thực hiện do tính chất thường
ngắn của các đầu ra trong tập dữ liệu này.

5.2.1 Kết quả AlpacaEval.
Tỷ lệ thắng và độ dài trung bình của phản hồi mô
hình cho các phương pháp khác nhau trên Alpha-
caEval được trình bày trong Bảng 3. Đáng chú ý
là mặc dù sử dụng ít dữ liệu nhất, chúng tôi đã đạt
được hiệu suất tốt nhất trong khi duy trì tiền đề
LLM cơ sở giống nhau ở quy mô mô hình 7B. Mô
hình-DYNOSAUR thể hiện hiệu suất thấp nhất,
có thể do đầu ra của nó quá tiêu chuẩn hóa và ngắn
gọn thay vì một phản hồi chi tiết. Bằng cách vượt
qua tất cả các phương pháp không dựa trên văn
bản, chúng tôi chứng minh hiệu quả của văn bản
do con người viết trong việc giảm thiểu các ảo
giác của LLM. So với phương pháp dựa trên văn
bản Humpback, chúng tôi đạt được sự cải thiện
đáng kể bằng cách thích ứng trình bao bọc lệnh
của chúng tôi với phong cách phản hồi AI, dẫn đến
những tiến bộ đáng kể.

5.2.2 Kết quả ELI5, LF-Test và Super-NI.
Bảng 4 cho thấy Rouge-L (R), Meteor (M), và
Bert-Score (B) của các mô hình khác nhau trên
ELI5, LF-Test, và Super-NI. Phương pháp của
chúng tôi vượt trội hơn tất cả các phương pháp
được so sánh trên cả ba điểm chuẩn về Rouge-L,
Meteor, và Bert-Score, đạt được hiệu suất vượt
trội trong tất cả các thước đo đánh giá. Quan sát
này cho thấy rằng tập dữ liệu của chúng tôi cho
phép căn chỉnh tốt hơn giữa đầu ra LLM và chú
thích của con người, chỉ ra hiệu quả của phương
pháp chúng tôi trong việc cải thiện hiệu suất của
các mô hình LLM.

Đánh giá GPT-4. Để giảm thiểu bất kỳ thiên vị
nào được giới thiệu bởi các thước đo thông thường
như Rouge, chúng tôi sử dụng GPT-4 để đánh giá
trên các điểm chuẩn ELI5, LF-Test, và Super-NI.
Chúng tôi tính toán tỷ lệ thắng/hòa/thua bằng cách
so sánh các phản hồi mô hình với các phản hồi
tham chiếu được cung cấp bởi các điểm chuẩn.
Kết quả được hiển thị trong Hình 4. Mô hình-
DOG-INSTRUCT của chúng tôi liên tục đạt được
tỷ lệ thắng cao nhất trên tất cả các điểm chuẩn.

5.2.3 Nghiên cứu Loại bỏ.
Chúng tôi đã so sánh hiệu suất LLM bằng cách sử
dụng các thiết lập khác nhau để xây dựng DOG-
INSTRUCT.
•w/o quan điểm căn chỉnh: chúng tôi tái xây dựng
tập huấn luyện meta Ω mà không có bất kỳ ví dụ
nào được xây dựng bởi các văn bản thực do con
người viết;
•w/o quan điểm đa dạng: chúng tôi tái xây dựng
Ω mà không có bất kỳ ví dụ nào được kết hợp bởi
các hướng dẫn và phản hồi từ ALPACA;
•w dịch ngược hướng dẫn: chúng tôi thay thế
việc bao bọc hướng dẫn của chúng tôi bằng dịch
ngược hướng dẫn để tái xây dựng DOG-INSTRUCT
trong khi giữ nguyên các tài liệu đầu vào;
•w/o hậu xử lý: chúng tôi loại bỏ hậu xử lý khi
tạo DOG-INSTRUCT.

Bảng 5 cho thấy kết quả thí nghiệm. DOG-
INSTRUCT được trang bị đầy đủ các thành phần
của chúng tôi hoạt động tốt nhất về tất cả các thước
đo. Sự suy giảm hiệu suất đáng kể chứng minh
rằng việc thích ứng PLM với định dạng nhiệm vụ
là rất quan trọng đối với hiệu quả của việc điều
chỉnh lời nhắc.

5.3 Đánh giá Con người
Trong khi đánh giá tự động trong phần trước cung
cấp một đánh giá tổng thể về hiệu suất mô hình,
bây giờ chúng tôi nhằm đánh giá cụ thể

--- TRANG 6 ---
Bảng 4: Rouge-L (R), Meteor (M), và Bert-Score (B) của các phương pháp khác nhau trên các tập test của ba điểm chuẩn.
Tất cả các phương pháp tuân theo thiết lập zero-shot.

Trình tạo Dữ liệu Tập dữ liệu Số ví dụ # ELI5 LF-Test Super-NI
R(%) M(%) R(%) M(%) B(%)
text-davinci-003 LONG FORM 23.7K 7.5 5.4 24.9 18.1 81.8
GPT-3.5-TurboSELF-INSTRUCT 82K 9.8 8.2 22.4 16.5 83.0
ALPACA 52K 10.1 8.8 23.1 17.3 82.9
DYNOSAUR 800K 3.1 1.5 15.6 11.0 86.0
EVOL-INSTRUCT 70K 18.9 18.4 25.2 21.8 85.6
GPT-4 ALPACA-GPT-4 52K 11.1 13.3 25.1 22.4 85.8
LLAMA 2-7BHUMPBACK†18K 9.3 6.1 25.0 22.2 83.7
DOG-INSTRUCT 12.4K 19.0 19.7 25.9 23.6 86.1

Bảng 5: Kết quả thí nghiệm nghiên cứu loại bỏ cho tất cả các điểm chuẩn được sử dụng.

Giai đoạnThiết lậpAlpacaEval ELI5 LF-Test Super-NI
Tỷ lệ thắng(%) M(%) M(%) B(%)
DOG-INSTRUCT 53.1 19.7 23.6 86.1
Đào tạow/o quan điểm căn chỉnh 46.7 18.5 20.2 85.2
w/o quan điểm đa dạng 12.5 12.1 15.7 83.8
w dịch ngược hướng dẫn 5.9 9.3 16.6 81.7
Tạo ra w/o hậu xử lý 32.0 17.2 23.3 85.9

hiệu quả của DOG-INSTRUCT của chúng tôi trong
việc giảm các ảo giác và căn chỉnh các phản hồi
mô hình với các đầu ra giống con người.

5.3.1 Chất lượng Dữ liệu
Chúng tôi chọn ngẫu nhiên 50 ví dụ từ mỗi tập dữ
liệu và đánh giá chất lượng của chúng bằng thủ
công. Vì các nhiệm vụ được tạo ra có thể liên quan
đến kiến thức từ một số lĩnh vực khác nhau, chúng
tôi yêu cầu người chú thích cần truy xuất bằng
chứng tương ứng bằng cách sử dụng các công cụ
tìm kiếm và so sánh từng cái một. Toàn bộ quá
trình đánh giá thủ công mất khoảng 8 giờ công.

Thước đo Đánh giá Con người. a) xác thực (V)
cho biết phần trăm ví dụ có phản hồi tuân theo
hướng dẫn. b) ảo giác (H) đo phần trăm ví dụ có
phản hồi chứa lỗi thực tế. c) trôi chảy (F) cho biết
phần trăm ví dụ có hướng dẫn và phản hồi mượt
mà và trôi chảy.

Kết quả. Kết quả được hiển thị trong Hình 6. Cả
ALPACA-GPT-4 và EVOL-INSTRUCT đều thể
hiện mức độ ảo giác cao hơn do chúng hoàn toàn
dựa vào việc sử dụng LLMs để tạo ra các cặp
hướng dẫn-phản hồi từ đầu. Bằng cách tạo ra các
nhiệm vụ từ các tài liệu do con người viết, cả
LONG FORM và HUMPBACK đều hiệu quả giảm
thiểu các ảo giác. Tuy nhiên, việc bao gồm nhiễu
trong văn bản thực dẫn đến độ trôi chảy (F) thấp
hơn so với các tập dữ liệu được tạo ra hoàn toàn
bởi LLMs. Ngược lại, phương pháp của chúng tôi
kết hợp việc sử dụng văn bản do con người viết
như hỗ trợ thực tế với sửa đổi phong cách thông
qua LLMs, dẫn đến hiệu suất vượt trội trên cả ba
thước đo.

5.3.2 Khả năng Tạo ra Dựa trên Văn bản
Đối với cùng một tài liệu, chúng tôi đã so sánh
chất lượng của các cặp hướng dẫn-phản hồi được
tạo ra bằng cách sử dụng hai phương pháp khác
nhau: bao bọc hướng dẫn

Bảng 6: Đánh giá con người về tính đủ điều kiện của tập dữ liệu. Đối
với mỗi tập dữ liệu, chúng tôi lấy mẫu ngẫu nhiên 50 ví dụ. Ở đây
↓ có nghĩa là giá trị càng nhỏ càng tốt.

Tập dữ liệu V (%) H (%) ↓F (%)
ALPACA-GPT-4 94 22 92
EVOL-INSTRUCT 94 18 94
LONG FORM 76 14 84
HUMPBACK†48 12 62
Ω 92 20 94
DOG-INSTRUCT 96 12 96

--- TRANG 7 ---
EVOL HUMPBACK DOG020406080
EVOL HUMPBACK DOG020406080
EVOL HUMPBACK DOG020406080Tỷ lệ thắng
Tỷ lệ hòa
Tỷ lệ thua
Hình 4: Kết quả đánh giá tự động GPT-4 trên các tập con của Eli5 (trái), LF-Test (giữa), Super-NI (phải). Để
tính đến chi phí của GPT-4, mỗi tập con chứa 200 ví dụ được lấy mẫu ngẫu nhiên từ các tập test gốc.
Tỷ lệ thắng/hòa/thua được tính bằng cách so sánh các phản hồi mô hình với các phản hồi tham chiếu đã cho.

GPT-4 LONG FORM HUMPBACK0204060MThắng
Hòa
MThua
Hình 5: Đánh giá con người so sánh DOG-
INSTRUCT với các phương pháp dựa trên văn bản
khác nhau. Đánh giá được thực hiện bằng cách sử dụng
cùng một tập các tài liệu do con người viết làm đầu vào
cho tất cả các phương pháp.

và dịch ngược hướng dẫn. Cụ thể, chúng tôi chọn
ngẫu nhiên 100 tài liệu từ kho dữ liệu được sử
dụng trong LONG FORM và HUMPBACK để đưa
vào trình bao bọc hướng dẫn M của chúng tôi.
Kết quả, được mô tả trong Hình 5, chứng minh
rằng trình bao bọc của chúng tôi tạo ra các cặp
hướng dẫn-phản hồi chất lượng vượt trội cho cùng
một tài liệu. Hơn nữa, chúng tôi lấy mẫu ngẫu
nhiên khoảng 100 tài liệu từ của chúng tôi và để
GPT-4 thực hiện bao bọc hướng dẫn. Hình 5 minh
họa rằng các cặp hướng dẫn-phản hồi được tạo ra
bởi M của chúng tôi thể hiện chất lượng cạnh tranh
với những cặp được tạo ra bởi GPT-4.

6 Nghiên cứu Liên quan
Điều chỉnh Hướng dẫn Con người có khả năng
hiểu và thực hiện các nhiệm vụ một cách dễ dàng
dựa trên các hướng dẫn bằng lời (Touvron et al.,
2023a; OpenAI, 2023; Touvron et al., 2023b).
Tương tự, những tiến bộ trong học sâu đã cho phép
các Mô hình Ngôn ngữ (LLMs) (Brown et al., 2020;
OpenAI, 2023; Chowdhery et al., 2022; Touvron
et al., 2023a) có được khả năng hiểu và tuân theo
hướng dẫn. Điều chỉnh hướng dẫn phục vụ như
một phương pháp đầy hứa hẹn, bao gồm việc tinh
chỉnh LLMs bằng cách sử dụng dữ liệu đào tạo và
hướng dẫn từ một bộ sưu tập các nhiệm vụ nguồn
(Sanh et al., 2022; Mishra et al., 2022; Wei et al.,
2022; Chung et al., 2022; Longpre et al., 2023;
Peng et al., 2023).

Thu thập Dữ liệu Điều chỉnh Hướng dẫn Việc
thu thập dữ liệu điều chỉnh hướng dẫn chất lượng
cao (Xu et al., 2023; Yin et al., 2023; Honovich
et al., 2023) là một vấn đề cấp bách trong việc nâng
cao khả năng tuân theo hướng dẫn. Các phương
pháp trước đây có thể được phân loại rộng rãi thành
ba nhóm chính. Thứ nhất, các phương pháp như
SUPER-NI (Wang et al., 2022) và DOLLY (Conover
et al., 2023) dựa vào việc thuê các chuyên gia để
tạo ra hướng dẫn cho các nhiệm vụ NLP đa dạng.
Thứ hai, các phương pháp như SELF-INSTRUCT
(Wang et al., 2023) và ALPACA (Taori et al., 2023)
ủng hộ việc sử dụng LLMs để tự động tạo ra dữ
liệu điều chỉnh hướng dẫn, do đó loại bỏ nhu cầu
lao động thủ công. Cuối cùng, Dynosaur (Yin et
al., 2023) sử dụng LLMs để chuyển đổi các tập dữ
liệu NLP hiện có từ Huggingface thành dữ liệu
điều chỉnh hướng dẫn với chi phí giảm. Công trình
liên quan nhất với bài báo này là (Köksal et al.,
2023; Li et al., 2023a). Nó sử dụng một tài liệu
do con người viết như một phản hồi tự nhiên và
tận dụng một LLM để tạo ra hướng dẫn tương ứng
dựa trên phản hồi. Ngược lại, trình bao bọc hướng
dẫn của chúng tôi chọn lựa các phần có giá trị của
tài liệu để xây dựng các phản hồi phù hợp.

7 Kết luận & Hạn chế
Bài báo này giới thiệu một phương pháp mới được
gọi là bao bọc hướng dẫn, cho phép thu thập tự
động dữ liệu điều chỉnh hướng dẫn chất lượng cao
từ các tài liệu do con người viết. Trình bao bọc
hướng dẫn được đào tạo của chúng tôi không chỉ
sử dụng tài liệu để giảm thiểu các ảo giác phản hồi
mà còn sửa đổi các tài liệu thô để căn chỉnh chúng
với phong cách phản hồi tiêu chuẩn.

--- TRANG 8 ---
Thông qua các đánh giá toàn diện, chúng tôi chứng
minh rằng phương pháp của chúng tôi đạt được
kết quả đáng chú ý trên các điểm chuẩn được sử
dụng rộng rãi khác nhau trong khi sử dụng ít ví dụ
đào tạo nhất. Những hạn chế của phương pháp
chúng tôi là nó không thể xử lý các tài liệu quá
dài và chỉ có thể tạo ra một nhiệm vụ duy nhất cho
một tài liệu. Trong công việc tương lai, chúng tôi
sẽ khám phá việc tạo ra các hướng dẫn phức tạp
hơn liên quan đến nhiều tài liệu dài hơn.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Quỹ Khoa
học Tự nhiên Quốc gia Trung Quốc số U21A20488.
Chúng tôi cảm ơn Trung tâm Tính toán Dữ liệu
Lớn của Đại học Đông Nam đã cung cấp hỗ trợ
cơ sở vật chất cho các tính toán số trong bài báo
này.

Tài liệu tham khảo
Satanjeev Banerjee và Alon Lavie. 2005. METEOR:
một thước đo tự động cho đánh giá MT với mối
tương quan được cải thiện với các phán đoán của
con người. Trong Proceedings of the Workshop on
Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization@ACL
2005, Ann Arbor, Michigan, USA, ngày 29 tháng 6
năm 2005, trang 65–72. Association for Computational Linguistics.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, và Dario Amodei.
2020. Các mô hình ngôn ngữ là người học ít shot.
Trong Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, ngày
6-12 tháng 12 năm 2020, ảo.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
và Noah Fiedel. 2022. Palm: Mở rộng mô hình hóa
ngôn ngữ với pathways. CoRR, abs/2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,
Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav
Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc V. Le, và Jason Wei.
2022. Mở rộng các mô hình ngôn ngữ được tinh
chỉnh hướng dẫn. CoRR, abs/2210.11416.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,
Matei Zaharia, và Reynold Xin. 2023. Free dolly:
Giới thiệu llm được điều chỉnh hướng dẫn mở thực
sự đầu tiên trên thế giới.

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, và Michael Auli. 2019. ELI5:
trả lời câu hỏi dạng dài. Trong Proceedings of the
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, ngày
28 tháng 7 - 2 tháng 8 năm 2019, Volume 1: Long
Papers, trang 3558–3567. Association for Computational Linguistics.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, và Connor Leahy. 2021. The pile: Một tập
dữ liệu 800gb văn bản đa dạng cho mô hình hóa
ngôn ngữ. CoRR, abs/2101.00027.

Or Honovich, Thomas Scialom, Omer Levy, và Timo
Schick. 2023. Hướng dẫn không tự nhiên: Điều
chỉnh các mô hình ngôn ngữ với (gần như) không
có lao động con người. Trong Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, ngày 9-14 tháng 7 năm 2023,
trang 14409–14428. Association for Computational
Linguistics.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và
Weizhu Chen. 2022. Lora: Thích ứng thứ hạng thấp
của các mô hình ngôn ngữ lớn. Trong The Tenth
International Conference on Learning Representations, ICLR 2022, Virtual Event, ngày 25-29 tháng
4 năm 2022. OpenReview.net.

Abdullatif Köksal, Timo Schick, Anna Korhonen, và
Hinrich Schütze. 2023. Longform: Tối ưu hóa điều
chỉnh hướng dẫn cho tạo văn bản dài với trích xuất
kho dữ liệu. CoRR, abs/2304.08460.

--- TRANG 9 ---
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke
Zettlemoyer, Omer Levy, Jason Weston, và Mike
Lewis. 2023a. Tự căn chỉnh với dịch ngược hướng
dẫn. CoRR, abs/2308.06259.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, và
Tatsunori B Hashimoto. 2023b. Alpacaeval: Một
bộ đánh giá tự động cho các mô hình tuân theo
hướng dẫn. Kho lưu trữ GitHub.

Chin-Yew Lin. 2004. Rouge: Một gói cho đánh giá
tự động các bản tóm tắt. Trong Text summarization
branches out, trang 74–81.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le,
Barret Zoph, Jason Wei, và Adam Roberts. 2023.
Bộ sưu tập flan: Thiết kế dữ liệu và phương pháp
cho điều chỉnh hướng dẫn hiệu quả. Trong International Conference on Machine Learning, ICML
2023, ngày 23-29 tháng 7 năm 2023, Honolulu,
Hawaii, USA, tập 202 của Proceedings of Machine
Learning Research, trang 22631–22648. PMLR.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, và
Hannaneh Hajishirzi. 2022. Tổng quát hóa nhiệm
vụ chéo thông qua hướng dẫn crowdsourcing ngôn
ngữ tự nhiên. Trong Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, ngày 22-27 tháng 5 năm 2022, trang
3470–3487. Association for Computational Linguistics.

OpenAI. 2023. Chatgpt.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. 2023. Điều chỉnh hướng dẫn
với GPT-4. CoRR, abs/2304.03277.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, và Alexander M. Rush. 2022. Đào
tạo nhiều nhiệm vụ được nhắc nhở cho phép tổng
quát hóa nhiệm vụ zero-shot. Trong The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, ngày 25-29 tháng 4 năm
2022. OpenReview.net.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: Một mô hình llama tuân theo hướng
dẫn. https://github.com/tatsu-lab/
stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, và Guillaume Lample. 2023a. Llama: Các
mô hình ngôn ngữ nền tảng mở và hiệu quả. CoRR,
abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas
Scialom. 2023b. Llama 2: Các mô hình chat nền
tảng mở và được tinh chỉnh. CoRR, abs/2307.09288.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh
Hajishirzi. 2023. Self-instruct: Căn chỉnh các mô
hình ngôn ngữ với các hướng dẫn tự tạo. Trong
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), ACL 2023, Toronto, Canada, ngày
9-14 tháng 7 năm 2023, trang 13484–13508. Association for Computational Linguistics.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit
Verma, Ravsehaj Singh Puri, Rushang Karia, Savan
Doshi, Shailaja Keyur Sampat, Siddhartha Mishra,
Sujan Reddy A, Sumanta Patro, Tanay Dixit, và
Xudong Shen. 2022. Super-naturalinstructions: Tổng
quát hóa thông qua các hướng dẫn khai báo trên
1600+ nhiệm vụ NLP. Trong Proceedings of the
2022 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2022, Abu Dhabi,
United Arab Emirates, ngày 7-11 tháng 12 năm 2022,
trang 5085–5109. Association for Computational
Linguistics.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. 2022. Các mô hình
ngôn ngữ được tinh chỉnh là người học zero-shot.
Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, ngày
25-29 tháng 4 năm 2022. OpenReview.net.

--- TRANG 10 ---
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin
Jiang. 2023. Wizardlm: Trao quyền cho các mô
hình ngôn ngữ lớn để tuân theo các hướng dẫn phức
tạp. CoRR, abs/2304.12244.

Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal,
Jiawei Han, và Kai-Wei Chang. 2023. Dynosaur:
Một mô hình tăng trưởng động cho curation dữ liệu
điều chỉnh hướng dẫn. CoRR, abs/2305.14327.

Muru Zhang, Ofir Press, William Merrill, Alisa Liu,
và Noah A. Smith. 2023. Cách ảo giác mô hình
ngôn ngữ có thể tăng vọt. CoRR, abs/2305.13534.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, và Yoav Artzi. 2019. Bertscore: Đánh
giá tạo văn bản với bert. arXiv preprint
arXiv:1904.09675.

Shen Zheng, Jie Huang, và Kevin Chen-Chuan Chang.
2023. Tại sao chatgpt lại thiếu sót trong việc trả lời
câu hỏi một cách trung thực? arXiv preprint arXiv:
2304.10513.

A Lời nhắc Đầy đủ để Xây dựng
Tập Huấn luyện Meta

A.1 Lời nhắc để Xây dựng Ωa
Lời nhắc đầy đủ để xây dựng tập huấn luyện meta
như sau.

Đối với văn bản đã cho, thiết kế một nhiệm vụ.
Mỗi nhiệm vụ chứa ba trường,
hướng dẫn, đầu vào, và đầu ra.
hướng dẫn định nghĩa một nhiệm vụ bằng ngôn
ngữ tự nhiên.
Hướng dẫn là một định nghĩa hoàn chỉnh về
cách một văn bản đầu vào (ví dụ, một câu hoặc
một tài liệu) được mong đợi được ánh xạ thành
một văn bản đầu ra.
Yêu cầu hướng dẫn, đầu vào, và đầu ra
được rút ra từ văn bản bất cứ khi nào có thể.
Đầu vào có thể trống để chỉ ra rằng nhiệm vụ
không có đầu vào.
Hướng dẫn phải ở dạng câu mệnh lệnh.
Đây là các minh họa mà phản hồi của bạn
nên khác biệt càng nhiều càng tốt với chúng.
{}
#văn bản#: "{}"

A.2 Lời nhắc để Xây dựng Ωd
Lời nhắc đầy đủ để xây dựng tập huấn luyện meta
như sau.

Kết hợp hướng dẫn và đầu ra sau đây thành
một văn bản mạch lạc duy nhất.
Bạn có thể thêm, xóa, hoặc sửa đổi một số
nội dung theo cách phù hợp để làm cho
văn bản kết hợp có tính logic.
#hướng dẫn#: "{}"
#đầu ra#: "{}"