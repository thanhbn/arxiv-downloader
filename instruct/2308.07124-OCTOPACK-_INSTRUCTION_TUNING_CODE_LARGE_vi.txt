# 2308.07124.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2308.07124.pdf
# Kích thước tệp: 2618401 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OCTOPACK: ĐIỀU CHỈNH CHỈ DẪN CHO CÁC MÔ HÌNH NGÔN NGỮ LỚN MÃ NGUỒN

Niklas Muennighoff Qian Liu Armel Zebaze Qinkai Zheng Binyuan Hui
Terry Yue Zhuo Swayam Singh Xiangru Tang Leandro von Werra Shayne Longpre
n.muennighoff@gmail.com

TÓM TẮT

Tinh chỉnh các mô hình ngôn ngữ lớn (LLM) trên các chỉ dẫn dẫn đến cải thiện hiệu suất đáng kể trên các tác vụ ngôn ngữ tự nhiên. Chúng tôi áp dụng điều chỉnh chỉ dẫn bằng mã nguồn, tận dụng cấu trúc tự nhiên của các commit Git, kết hợp các thay đổi mã với chỉ dẫn của con người. Chúng tôi biên dịch COMMIT PACK: 4 terabyte commit Git trên 350 ngôn ngữ lập trình. Chúng tôi đánh giá COMMIT PACK so với các chỉ dẫn mã tự nhiên và tổng hợp khác (xP3x, Self-Instruct, OASST) trên mô hình StarCoder 16B tham số, và đạt được hiệu suất tốt nhất trong số các mô hình không được huấn luyện trên đầu ra OpenAI, trên benchmark HumanEval Python (46.2% pass@ 1). Chúng tôi tiếp tục giới thiệu HUMAN EVALPACK, mở rộng benchmark HumanEval thành tổng cộng 3 tác vụ mã hóa (Sửa chữa mã, Giải thích mã, Tổng hợp mã) trên 6 ngôn ngữ (Python, JavaScript, Java, Go, C++, Rust). Các mô hình của chúng tôi, OCTOCODER và OCTOGEEX, đạt hiệu suất tốt nhất trên HUMAN EVALPACK trong số tất cả các mô hình cho phép, chứng minh lợi ích của COMMIT PACK trong việc tổng quát hóa sang tập hợp rộng hơn các ngôn ngữ và tác vụ mã hóa tự nhiên. Mã, mô hình và dữ liệu có sẵn miễn phí tại https://github.com/bigcode-project/octopack .

import numpy as np
import matplotlib.pyplot as plt
# tạo dữ liệu mẫu
x_data = np.linspace(-5, 5, 20)
y_data = np.random.normal(0.0, 1.0, x_data.size)
plt.plot(x_data, y_data, 'o')
plt.show()

Mã trước

Thông điệp Commit

Mã sau

Thay đổi thành hàm sin() với nhiễu

import math
import numpy as np
import matplotlib.pyplot as plt
# tạo dữ liệu mẫu
x_data = np.linspace(-math.pi, math.pi, 30)
y_data = np.sin(x_data) + np.random.normal(0.0, 0.1, x_data.size)
plt.plot(x_data, y_data, 'o')
plt.show()

1) CommitPack
2) HumanEvalPack

Hình 1: Tổng quan OCTOPACK. 1) Mẫu từ bộ dữ liệu 4TB của chúng tôi, COMMIT PACK. 2) Hiệu suất của OCTOCODER, OCTOGEEX và các mô hình mã khác bao gồm cả những mô hình không cho phép (WizardCoder, GPT-4) trên HUMAN EVALPACK bao gồm 3 tác vụ mã hóa và 6 ngôn ngữ lập trình.

--- TRANG 2 ---
OctoPack: Điều chỉnh chỉ dẫn cho các mô hình ngôn ngữ lớn mã nguồn

1 GIỚI THIỆU

Tinh chỉnh các mô hình ngôn ngữ lớn (LLM) trên nhiều tác vụ ngôn ngữ được giải thích qua chỉ dẫn (điều chỉnh chỉ dẫn) đã được chứng minh là cải thiện khả năng sử dụng mô hình và hiệu suất tổng thể (Wei et al., 2022; Sanh et al., 2022; Min et al., 2022; Ouyang et al., 2022). Mô hình điều chỉnh chỉ dẫn cũng đã được chứng minh thành công cho các mô hình được huấn luyện trên dữ liệu thị giác (Liu et al., 2023a; Li et al., 2023a), âm thanh (Zhang et al., 2023b) và đa ngôn ngữ (Muennighoff et al., 2022b; Wang et al., 2022b).

Trong công trình này, chúng tôi điều chỉnh chỉ dẫn LLM trên phương thức mã hóa. Trong khi các LLM mã đã có thể được chỉ dẫn gián tiếp để tạo ra mã mong muốn bằng cách sử dụng các chú thích mã, quy trình này dễ vỡ và không hoạt động khi đầu ra mong muốn là ngôn ngữ tự nhiên, như giải thích mã. Điều chỉnh chỉ dẫn rõ ràng của các LLM mã có thể cải thiện khả năng điều khiển của chúng và cho phép ứng dụng vào nhiều tác vụ hơn. Song song với công trình của chúng tôi, ba LLM mã được điều chỉnh chỉ dẫn đã được đề xuất: PanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) và InstructCodeT5+ (Wang et al., 2023c). Những mô hình này dựa vào các mô hình có khả năng và đóng hơn từ API OpenAI để tạo dữ liệu huấn luyện chỉ dẫn của chúng. Cách tiếp cận này có vấn đề vì (1) các API nguồn đóng liên tục thay đổi và có tính khả dụng không thể dự đoán (Pozzobon et al., 2023; Chen et al., 2023a), (2) nó dựa trên giả định rằng một mô hình có khả năng hơn tồn tại (3) nó có thể củng cố ảo giác mô hình (Gudibande et al., 2023) và (4), tùy thuộc vào cách hiểu pháp lý, các điều khoản sử dụng của OpenAI cấm các mô hình như vậy: "...Bạn không được...sử dụng đầu ra từ Dịch vụ để phát triển mô hình cạnh tranh với OpenAI...". Do đó, chúng tôi coi các mô hình được huấn luyện trên đầu ra OpenAI không thể sử dụng cho mục đích thương mại trong thực tế và phân loại chúng là không cho phép trong công trình này.

Chúng tôi tập trung vào dữ liệu được cấp phép cho phép hơn và tránh sử dụng mô hình nguồn đóng để tạo dữ liệu tổng hợp. Chúng tôi đánh giá bốn nguồn dữ liệu chỉ dẫn mã phổ biến: (1) xP3x (Muennighoff et al., 2022b), chứa dữ liệu từ các benchmark mã phổ biến, (2) dữ liệu Self-Instruct (Wang et al., 2023a) mà chúng tôi tạo ra bằng LLM mã cho phép, (3) OASST (Köpf et al., 2023), chứa chủ yếu dữ liệu ngôn ngữ tự nhiên và ít ví dụ mã và (4) COMMIT PACK, bộ dữ liệu 4TB mới của chúng tôi về commit Git. Mục đích chính của điều chỉnh chỉ dẫn là mở rộng khả năng tổng quát hóa của mô hình sang nhiều tác vụ và cài đặt khác nhau. Do đó, chúng tôi mở rộng benchmark tổng hợp mã, HumanEval (Chen et al., 2021; Zheng et al., 2023), để tạo HUMAN EVALPACK: Một benchmark mã bao gồm tổng hợp mã, sửa chữa mã và giải thích mã trên sáu ngôn ngữ lập trình.

Điều chỉnh chỉ dẫn StarCoder (Li et al., 2023b) trên biến thể được lọc của COMMIT PACK và OASST dẫn đến mô hình tốt nhất của chúng tôi, OCTOCODER, vượt qua tất cả các mô hình được cấp phép mở khác (Hình 1), nhưng vẫn kém hơn GPT-4 lớn hơn nhiều (OpenAI, 2023). GPT-4 gần đạt hiệu suất tối đa trên biến thể tổng hợp mã, đáng chú ý với điểm pass@ 1 là 86.6% trên Python HumanEval. Tuy nhiên, nó hoạt động kém hơn đáng kể trên các biến thể sửa chữa và giải thích mã của HUMAN EVALPACK mà chúng tôi giới thiệu. Điều này cho thấy rằng benchmark HumanEval gốc có thể sớm ngừng hữu ích do các mô hình đạt gần hiệu suất tối đa. Các biến thể đánh giá thử thách hơn của chúng tôi cung cấp không gian cho các LLM tương lai cải thiện hiệu suất của mô hình tốt nhất hiện tại.

Tóm lại, chúng tôi đóng góp:
• COMMIT PACK và COMMIT PACKFT: 4TB commit mã được cấp phép cho phép trên 350 ngôn ngữ lập trình cho tiền huấn luyện và biến thể được lọc 2GB chứa chỉ dẫn mã chất lượng cao được sử dụng cho tinh chỉnh
• HUMAN EVALPACK: Một benchmark cho tổng quát hóa LLM mã, bao gồm ba kịch bản (Sửa chữa mã, Giải thích mã, Tổng hợp mã) và 6 ngôn ngữ lập trình (Python, JavaScript, Java, Go, C++, Rust)
• OCTOCODER và OCTOGEEX: Các LLM mã cho phép tốt nhất

2 COMMIT PACK: DỮ LIỆU CHỈ DẪN MÃ

Các công trình trước đây đã chỉ ra rằng các mô hình có thể tổng quát hóa sang các ngôn ngữ được bao gồm trong tiền huấn luyện, nhưng vắng mặt trong điều chỉnh chỉ dẫn (Muennighoff et al., 2022b). Tuy nhiên, họ cũng cho thấy rằng việc bao gồm các ngôn ngữ như vậy trong điều chỉnh chỉ dẫn sẽ tăng cường hiệu suất của chúng hơn nữa. Chúng tôi giả thuyết rằng dữ liệu mã thể hiện hành vi tương tự. Để cải thiện hiệu suất trên các tác vụ liên quan đến mã, chúng tôi do đó xây dựng một bộ dữ liệu chỉ dẫn mã tận dụng cấu trúc tự nhiên của các commit Git.

COMMIT PACK Để tạo bộ dữ liệu, chúng tôi sử dụng metadata commit từ dump GitHub action trên Google BigQuery. Chúng tôi áp dụng các bộ lọc chất lượng, lọc cho các giấy phép thương mại thân thiện, và loại bỏ các commit ảnh hưởng đến nhiều hơn một tệp để đảm bảo thông điệp commit rất cụ thể và tránh độ phức tạp bổ sung từ việc xử lý nhiều tệp. Chúng tôi sử dụng metadata đã lọc để thu thập các tệp mã bị ảnh hưởng trước và sau commit từ GitHub. Điều này dẫn đến gần 4 terabyte dữ liệu bao gồm 350 ngôn ngữ lập trình (COMMIT PACK). Vì điều chỉnh chỉ dẫn không yêu cầu nhiều dữ liệu (Zhou et al., 2023a; Touvron et al., 2023), chúng tôi áp dụng một số bộ lọc nghiêm ngặt để giảm bộ dữ liệu xuống 2 gigabyte và 277 ngôn ngữ (COMMIT PACKFT). Những bộ lọc này bao gồm lọc cho các mẫu trong đó thông điệp commit có các từ cụ thể dưới dạng mệnh lệnh viết hoa ở đầu (ví dụ: "Verify ..."), bao gồm nhiều từ, và không chứa tham chiếu bên ngoài. Tất cả các bộ lọc được mô tả chi tiết trong Phụ lục D. Hình 2 mô tả phân phối của cả hai bộ dữ liệu và các tác vụ có trong COMMIT PACKFT. Để điều chỉnh chỉ dẫn các mô hình của chúng tôi, chúng tôi chọn 5.000 mẫu ngẫu nhiên từ COMMIT PACKFT trên 6 ngôn ngữ lập trình mà chúng tôi đánh giá. Trong Phụ lục G, chúng tôi cũng thử nghiệm với tiền huấn luyện trên toàn bộ COMMIT PACK.

Các lựa chọn thay thế Chúng tôi xem xét ba bộ dữ liệu bổ sung cho điều chỉnh chỉ dẫn được trình bày trong Bảng 1.
xP3x: xP3x là một bộ sưu tập quy mô lớn dữ liệu chỉ dẫn đa ngôn ngữ với khoảng 532 triệu mẫu (Muennighoff et al., 2022b). Chúng tôi chỉ tập trung vào tập con mã của xP3x, loại trừ NeuralCodeSearch (Li et al., 2019) không được cấp phép cho phép, và chọn 5.000 mẫu.
Self-Instruct: Sử dụng phương pháp Self-Instruct (Wang et al., 2022a) và mô hình StarCoder (Li et al., 2023b), chúng tôi tạo 5.003 chỉ dẫn tổng hợp và câu trả lời tương ứng.
OASST: OASST là một bộ dữ liệu đa dạng các cuộc đối thoại trò chuyện nhiều lượt (Köpf et al., 2023). Chỉ một số ít cuộc đối thoại chứa mã. Chúng tôi tái sử dụng một biến thể được lọc từ công trình trước (Dettmers et al., 2023) và lọc thêm các câu trả lời trợ lý đạo đức hóa (Phụ lục D) dẫn đến 8.587 mẫu.

3 HUMAN EVALPACK: ĐÁNH GIÁ CÁC MÔ HÌNH MÃ ĐƯỢC ĐIỀU CHỈNH CHỈ DẪN

Ngôn ngữ: Python, JavaScript, Java, Go, C++, Rust
Tác vụ con: HumanEvalFix, HumanEvalExplain, HumanEvalSynthesize

Viết một hàm Python `has_close_elements(numbers: List[float], threshold: float) -> bool` để giải quyết vấn đề sau:
Kiểm tra xem trong danh sách số đã cho, có hai số nào gần nhau hơn ngưỡng đã cho không.
>>> has_close_elements([1.0, 2.0, 3.0], 0.5)
False
>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
True

HumanEvalPack

from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Kiểm tra xem trong danh sách số đã cho, có hai số nào gần nhau hơn ngưỡng đã cho không.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    for idx, elem in enumerate(numbers):
        for idx2, elem2 in enumerate(numbers):
            if idx != idx2:
                distance = abs(elem - elem2)
                if distance < threshold:
                    return True
    return False

from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for idx, elem in enumerate(numbers):
        for idx2, elem2 in enumerate(numbers):
            if idx != idx2:
                distance = abs(elem - elem2)
                if distance < threshold:
                    return True
    return False

Cung cấp mô tả ngôn ngữ tự nhiên ngắn gọn về hàm sử dụng tối đa 213 ký tự.

Kiểm tra xem trong danh sách số đã cho, có hai số nào gần nhau hơn ngưỡng đã cho không.
>>> has_close_elements([1.0, 2.0, 3.0], 0.5)
False
>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
True

Kiểm tra xem trong danh sách số đã cho, có hai số nào...
...

Viết mã chức năng trong Python theo mô tả.

Sửa mã Giải thích mã Tổng hợp mã

Đầu vào mô hình
Độ đo: Pass@k
Tạo: Con người
Đầu ra mục tiêu

from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for idx, elem in enumerate(numbers):
        for idx2, elem2 in enumerate(numbers):
            if idx != idx2:
                distance = elem - elem2
                if distance < threshold:
                    return True
    return False

def check(has_close_elements):
    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True
    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False
    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True
    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False
    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True
    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True
    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False
check(has_close_elements)

Sửa lỗi trong has_close_elements.

from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for idx, elem in enumerate(numbers):
        for idx2, elem2 in enumerate(numbers):
            if idx != idx2:
                distance = abs(elem - elem2)
                if distance < threshold:
                    return True
    return False

from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for idx, elem in enumerate(numbers):
        for idx2, elem2 in enumerate(numbers):
            if idx != idx2:
                distance = abs(elem - elem2)
                if distance < threshold:
                    return True
    return False

Hình 3: Tổng quan HUMAN EVALPACK. Vấn đề HumanEval đầu tiên được mô tả trên ba kịch bản cho Python. Lỗi cho HUMAN EVALFIX bao gồm một câu lệnh "abs" bị thiếu.

Khi điều chỉnh chỉ dẫn LLM sử dụng dữ liệu ngôn ngữ tự nhiên (NL), đầu vào là một chỉ dẫn NL với ngữ cảnh NL tùy chọn và đầu ra mục tiêu là câu trả lời NL cho tác vụ (Wei et al., 2022). Khi điều chỉnh chỉ dẫn với dữ liệu mã (C), mã có thể xuất hiện chỉ trong đầu vào cùng với chỉ dẫn NL (NL+C → NL, ví dụ: giải thích mã), chỉ trong đầu ra (NL → C, ví dụ: tổng hợp mã), hoặc trong cả đầu vào và đầu ra (NL+C → C, ví dụ: sửa đổi mã như sửa lỗi). Trong khi các benchmark trước thường chỉ bao gồm các biến thể của tổng hợp mã, người dùng có thể muốn sử dụng mô hình trong tất cả ba kịch bản. Do đó, chúng tôi mở rộng benchmark tổng hợp mã HumanEval (Chen et al., 2021; Zheng et al., 2023) để bao gồm tất cả ba kết hợp đầu vào-đầu ra cho sáu ngôn ngữ (Hình 3).

HUMAN EVALFIX (NL+C → C) Cho một hàm mã không chính xác với lỗi tinh vi và các bài kiểm tra đơn vị đi kèm, mô hình được giao nhiệm vụ sửa hàm. Chúng tôi thủ công thêm lỗi vào từng 164 giải pháp HumanEval trên tất cả 6 ngôn ngữ (tổng cộng 984 lỗi). Đối với một mẫu nhất định, các lỗi càng giống nhau càng tốt trên 6 ngôn ngữ cho phép so sánh có ý nghĩa điểm số trên các ngôn ngữ. Các lỗi được viết sao cho mã vẫn chạy nhưng tạo ra kết quả không chính xác dẫn đến ít nhất một bài kiểm tra đơn vị thất bại. Thống kê lỗi và ví dụ ở Phụ lục L. Chúng tôi cũng đánh giá một biến thể dễ hơn của tác vụ này trong đó thay vì bài kiểm tra đơn vị, các mô hình được cung cấp docstring hàm chính xác như nguồn chân lý để sửa lỗi, xem Phụ lục K.

HUMAN EVALEXPLAIN (NL+C → NL) Cho một hàm mã chính xác, mô hình được giao nhiệm vụ tạo ra lời giải thích về mã. Tiếp theo, cùng mô hình được giao nhiệm vụ tái tạo mã chỉ với lời giải thích của chính nó. Bước thứ hai cho phép chúng tôi chấm điểm tác vụ này thông qua thực thi mã và đo pass@ k (Chen et al., 2021) thay vì đánh giá bản thân lời giải thích bằng các độ đo dựa trên heuristic như BLEU (Papineni et al., 2002) hoặc ROUGE (Lin, 2004) có những hạn chế lớn (Reiter, 2018; Schluter, 2017; Eghbali & Pradel, 2022; Zhou et al., 2023b). Để ngăn các mô hình sao chép giải pháp vào mô tả, chúng tôi loại bỏ bất kỳ sự trùng lặp giải pháp ít nhất 20 ký tự từ mô tả. Chúng tôi tiếp tục thực thi giới hạn độ dài ký tự trên lời giải thích do mô hình tạo ra tương đương với độ dài của docstring mô tả hàm. Giới hạn này được chỉ định trong lời nhắc cho mô hình. Lưu ý rằng chính docstring hàm không bao giờ được cung cấp cho mô hình cho tác vụ này.

HUMAN EVALSYNTHESIZE (NL → C) Cho một docstring hoặc chú thích ngôn ngữ tự nhiên mô tả mã mong muốn, mô hình được giao nhiệm vụ tổng hợp mã chính xác. Tác vụ này tương ứng với benchmark HumanEval gốc (Chen et al., 2021). Đối với các mô hình được điều chỉnh chỉ dẫn, chúng tôi thêm một chỉ dẫn rõ ràng vào đầu vào giải thích những gì mô hình nên làm. Đối với các mô hình chỉ trải qua tiền huấn luyện mô hình ngôn ngữ, chúng tôi theo Chen et al. (2021) và cung cấp cho mô hình header hàm và docstring để đánh giá việc hoàn thành hàm của nó.

Đối với tất cả các tác vụ, chúng tôi thực thi các lần tạo mã để tính hiệu suất bằng độ đo pass@ k (Chen et al., 2021): một vấn đề được coi là được giải quyết nếu bất kỳ lần tạo mã k nào vượt qua mọi trường hợp kiểm tra. Chúng tôi tập trung vào phiên bản đơn giản nhất của pass@ k, đó là pass@ 1: khả năng mô hình giải quyết vấn đề trong một lần thử. Như Chen et al. (2021), chúng tôi sử dụng nhiệt độ lấy mẫu 0.2 và top p= 0.95 để ước tính pass@ 1. Chúng tôi tạo n= 20 mẫu, đủ để có ước tính pass@ 1 đáng tin cậy (Li et al., 2023b). Đối với GPT-4, chúng tôi tạo n= 1 mẫu. Sử dụng n= 1 thay vì n= 20 cho GPT-4 chỉ thay đổi điểm từ 75.0% thành 75.2% pass@ 1 trên HUMAN EVALSYNTHESIZE Python trong khi tiết kiệm chi phí 20 lần.

Python HumanEval là benchmark mã được sử dụng rộng rãi nhất và nhiều bộ dữ liệu huấn luyện đã được khử nhiễm cho nó (Kocetkov et al., 2022). Bằng cách mở rộng HumanEval thủ công, chúng tôi đảm bảo việc khử nhiễm hiện có vẫn hợp lệ để cho phép đánh giá công bằng. Tuy nhiên, điều này có thể không đúng với tất cả các mô hình (ví dụ: GPT-4), do đó kết quả nên được diễn giải cẩn thận.

4 OCTOCODER: LLM MÃ ĐƯỢC CÂP PHÉP THƯƠNG MẠI TỐT NHẤT

4.1 ABLATING LỰA CHỌN DỮ LIỆU CHỈ DẪN

Chúng tôi điều chỉnh chỉ dẫn mô hình StarCoder đã được tiền huấn luyện (Li et al., 2023b) trên các kết hợp khác nhau của các bộ dữ liệu chỉ dẫn của chúng tôi (§2). Chúng tôi đánh giá tất cả các mô hình trên tập con Python của HUMAN EVALPACK như được mô tả trong Hình 4. Tương tự như công trình trước (Taori et al., 2023), chúng tôi định dạng tất cả các chỉ dẫn thành một lược đồ nhất quán để phân biệt câu hỏi và câu trả lời (xem Hình 18).

COMMIT PACKFT cho phép CodeLLM sửa lỗi COMMIT PACKFT là rất quan trọng cho việc tăng hiệu suất trên sửa chữa mã (HUMAN EVALFIX), trong đó điều chỉnh chỉ dẫn chỉ trên OASST hoặc các biến thể khác dẫn đến điểm số thấp hơn đáng kể. Điều này có thể do COMMIT PACKFT bao gồm khoảng 20% sửa lỗi trong số các tác vụ liên quan đến mã khác (Hình 2).

Tầm quan trọng của các mẫu với mục tiêu ngôn ngữ tự nhiên Mô hình StarCoder đã được tiền huấn luyện, cũng như biến thể Self-Instruct, hoạt động kém trên giải thích mã (HUMAN EVALEXPLAIN). Điều này là do cả hai mô hình chỉ được điều kiện hóa để viết mã thay vì ngôn ngữ tự nhiên. Chúng tôi thấy rằng để hoạt động tốt trong việc giải thích mã, cần thiết phải bao gồm các mẫu với ngôn ngữ tự nhiên làm đầu ra mục tiêu trong điều chỉnh chỉ dẫn. Chỉ dựa vào dữ liệu với mã làm mục tiêu, như dữ liệu Self-Instruct, sẽ dẫn đến các mô hình luôn xuất ra mã ngay cả khi câu hỏi yêu cầu đầu ra ngôn ngữ tự nhiên. Do đó, chúng tôi trộn tất cả các ablation khác với OASST, chứa nhiều mục tiêu ngôn ngữ tự nhiên. Trong khi tập con xP3x cũng chứa các mẫu với đầu ra ngôn ngữ tự nhiên, nhiều đầu ra mục tiêu của nó ngắn, dẫn đến các mô hình có xu hướng đưa ra câu trả lời ngắn. Điều này không thực tế cho tác vụ giải thích dẫn đến điểm số tương đối thấp của việc trộn xP3x với OASST.

COMMIT PACKFT + OASST mang lại hiệu suất tốt nhất Tất cả các bộ dữ liệu chỉ dẫn cung cấp sự tăng cường tương tự cho tổng hợp mã (HUMAN EVALSYNTHESIZE), vốn là trọng tâm của tất cả công trình trước về các mô hình chỉ dẫn mã (Wang et al., 2023c; Luo et al., 2023; Muennighoff et al., 2022b). Chúng tôi đạt được điểm trung bình tốt nhất bằng cách điều chỉnh chỉ dẫn trên COMMIT PACKFT được trộn với dữ liệu OASST đã lọc của chúng tôi mang lại cải thiện tuyệt đối 23% so với StarCoder. Do đó, chúng tôi chọn COMMIT PACKFT + OASST cho mô hình cuối cùng của chúng tôi được gọi là OCTOCODER. Sử dụng cùng dữ liệu, chúng tôi cũng điều chỉnh chỉ dẫn mô hình CodeGeeX2 6 tỷ tham số (Zheng et al., 2023) để tạo OCTOGEEX. Các siêu tham số huấn luyện cho cả hai mô hình ở Phụ lục P.

4.2 SO SÁNH VỚI CÁC MÔ HÌNH KHÁC

Chúng tôi đánh giá OCTOCODER và OCTOGEEX với các LLM mã tiên tiến trên HUMAN EVALPACK trong Bảng 2. Đối với tất cả các mô hình, chúng tôi sử dụng lời nhắc được đưa ra bởi những người tạo mô hình nếu có hoặc một lời nhắc trực quan đơn giản, xem Phụ lục Q.

OCTOCODER hoạt động tốt nhất trong số các mô hình cho phép OCTOCODER có điểm trung bình cao nhất trên tất cả ba kịch bản đánh giá trong số tất cả các mô hình cho phép. Chỉ với 6 tỷ tham số, OCTOGEEX là mô hình nhỏ nhất được đánh giá, nhưng vẫn vượt qua tất cả các LLM mã cho phép trước đây. GPT-4 (OpenAI, 2023) hoạt động tốt nhất trong số tất cả các mô hình được đánh giá với biên độ đáng kể. Tuy nhiên, GPT-4 là nguồn đóng và có thể lớn hơn nhiều so với tất cả các mô hình khác được đánh giá.

Điều chỉnh chỉ dẫn tổng quát hóa sang các ngôn ngữ lập trình chưa thấy Được huấn luyện chủ yếu trên ngôn ngữ tự nhiên, không phải mã, BLOOMZ (Muennighoff et al., 2022b) hoạt động kém hơn các mô hình khác mặc dù có 176 tỷ tham số. Go và Rust không có trong dữ liệu chỉ dẫn của BLOOMZ, tuy nhiên nó hoạt động tốt hơn nhiều so với đường cơ sở ngẫu nhiên 0.0 cho hai ngôn ngữ này trên hầu hết các tác vụ. Điều này xác nhận giả thuyết của chúng tôi rằng các mô hình có khả năng tổng quát hóa chỉ dẫn sang các ngôn ngữ lập trình chỉ được nhìn thấy trong tiền huấn luyện, tương tự như tổng quát hóa đa ngôn ngữ cho ngôn ngữ tự nhiên (Muennighoff et al., 2022b). Để cải thiện tổng quát hóa ngôn ngữ lập trình hơn nữa, chúng tôi điều chỉnh OCTOCODER và OCTOGEEX trên nhiều ngôn ngữ từ COMMIT PACKFT, và cải thiện tổng quát hóa này được phản ánh trong hiệu suất trên các ngôn ngữ mới của HUMAN EVALPACK.

Trọng số tiền huấn luyện tương quan với hiệu suất ngôn ngữ lập trình sau điều chỉnh chỉ dẫn Công trình trước đã chỉ ra rằng hiệu suất trên các ngôn ngữ tự nhiên sau điều chỉnh chỉ dẫn tương quan với trọng số của các ngôn ngữ này trong tiền huấn luyện (Muennighoff et al., 2022b). Trọng số càng lớn trong tiền huấn luyện, hiệu suất sau điều chỉnh chỉ dẫn càng tốt. Chúng tôi thấy điều tương tự xảy ra với các ngôn ngữ lập trình. Python, Java và JavaScript cùng nhau chiếm khoảng 30% dữ liệu tiền huấn luyện của StarCoder (Li et al., 2023b). Sau khi điều chỉnh chỉ dẫn StarCoder để tạo ra OCTOCODER, chúng tôi thấy hiệu suất tốt nhất trong ba ngôn ngữ này, đặc biệt là cho HUMAN EVALSYNTHESIZE. OCTOCODER hoạt động yếu nhất trên Rust, là ngôn ngữ tài nguyên thấp nhất của StarCoder trong số các ngôn ngữ chúng tôi đánh giá (1.2% dữ liệu tiền huấn luyện).

Các mô hình gặp khó khăn với những thay đổi mục tiêu nhỏ HUMAN EVALFIX là tác vụ thử thách nhất đối với hầu hết các mô hình. Chúng thường tái tạo hàm có lỗi mà không thực hiện bất kỳ thay đổi nào (ví dụ: WizardCoder trong Hình 34) hoặc chúng tạo ra các lỗi mới (ví dụ: GPT-4 trong Hình 33). Chúng tôi phân tích hiệu suất mô hình theo loại lỗi trong Phụ lục M và thấy các lỗi yêu cầu loại bỏ mã thừa là thử thách nhất. OCTOCODER hoạt động tương đối tốt trên tất cả các ngôn ngữ. Điều chỉnh chỉ dẫn trên COMMIT PACKFT có thể đã dạy OCTOCODER thực hiện các thay đổi nhỏ, có mục tiêu để sửa lỗi.

Các mô hình gặp khó khăn khi chuyển đổi giữa mã và văn bản Một số mô hình thất bại trong HUMAN EVALEXPLAIN, vì chúng không tạo ra lời giải thích ngôn ngữ tự nhiên. Chúng tôi kiểm tra thủ công các lời giải thích cho mười mẫu đầu tiên của phần Python và loại bỏ một mô hình nếu không có lời giải thích nào trong số chúng. Đây là trường hợp của StarCoder và CodeGeeX2, tạo ra mã thay vì lời giải thích ngôn ngữ tự nhiên. BLOOMZ và InstructCodeT5+ cũng thỉnh thoảng tạo ra mã. Các mô hình khác độc quyền tạo ra lời giải thích ngôn ngữ tự nhiên, không chứa bất kỳ mã nào cho các mẫu được kiểm tra.

Các mô hình gặp khó khăn tuân thủ độ dài đầu ra được chỉ định HUMAN EVALEXPLAIN chỉ dẫn các mô hình phù hợp với lời giải thích của chúng trong giới hạn ký tự nhất định (§3). Các mô hình hiện tại dường như không hiểu biết về số lượng ký tự mà chúng đang tạo ra. Chúng thường viết lời giải thích rất ngắn và do đó thiếu đặc tả (ví dụ: BLOOMZ trong Hình 35) hoặc lời giải thích quá dài cuối cùng bị cắt bớt (ví dụ: StarChat-beta trong Hình 38). Công trình tương lai có thể điều tra cách cho phép các mô hình nhận thức về độ dài đầu ra được tạo ra để cải thiện hiệu suất HUMAN EVALEXPLAIN.

Tổng hợp mã HumanEval gần bão hòa Tổng hợp mã thuần túy trên HUMAN EVALSYNTHESIZE là tác vụ dễ nhất đối với tất cả các mô hình. Với tỷ lệ vượt qua 86.6% cho một giải pháp duy nhất, GPT-4 gần như bão hòa hoàn toàn tập con Python. GPT-4 ban đầu được tìm thấy có điểm 67% trên Python HumanEval (OpenAI, 2023) và 81% trong công trình sau này (Bubeck et al., 2023). Điểm của chúng tôi cho GPT-4 cao hơn đáng kể, có thể do các cải tiến được thực hiện cho API bởi OpenAI, nhiễm HumanEval trong huấn luyện GPT-4, hoặc lời nhắc và đánh giá hơi khác. Một ví dụ về lời nhắc của chúng tôi được mô tả trong Hình 3 (phải). Chúng tôi thực hiện đánh giá rất cẩn thận để đảm bảo mọi lần tạo ra được xử lý chính xác. Chúng tôi tái tạo điểm HumanEval của WizardCoder (Luo et al., 2023; Xu et al., 2023a) và thấy nó cũng hoạt động tốt trên các ngôn ngữ khác. Đối với BLOOMZ và InstructCodeT5+, đánh giá của chúng tôi dẫn đến điểm Python cao hơn so với họ báo cáo, có thể do xử lý cẩn thận hơn các lần tạo ra. OCTOCODER có hiệu suất cao nhất cho mọi ngôn ngữ trong số các mô hình được cấp phép cho phép. Với pass@ 1 là 46.2% trên phần Python gốc, OCTOCODER cải thiện tương đối 38% so với mô hình cơ sở của nó, StarCoder.

5 CÔNG TRÌNH LIÊN QUAN

5.1 CÁC MÔ HÌNH MÃ

Đã có nhiều công trình mở rộng về các mô hình mã được điều chỉnh cho một tác vụ mã hóa cụ thể, như tóm tắt mã (Iyer et al., 2016; Ahmad et al., 2020; Zhang et al., 2022a; Shi et al., 2022) hoặc chỉnh sửa mã (Drain et al., 2021; Zhang et al., 2022c; He et al., 2022; Zhang et al., 2022b; Wei et al., 2023; Prenner & Robbes, 2023; Fakhoury et al., 2023; Skreta et al., 2023) (cũng xem công trình về mô hình chỉnh sửa nói chung (Reid & Neubig, 2022; Schick et al., 2022; Dwivedi-Yu et al., 2022; Raheja et al., 2023)). Những công trình này sử dụng các heuristic cụ thể tác vụ giới hạn khả năng áp dụng phương pháp của họ cho các tác vụ khác. Ngược lại, chúng tôi nhằm xây dựng các mô hình áp dụng được cho tất cả các loại tác vụ liên quan đến mã và hơn thế nữa.

Thông qua tiền huấn luyện quy mô lớn, các mô hình mã có khả năng áp dụng rộng rãi hơn đã được phát triển (Nijkamp et al., 2022; 2023; Xu et al., 2022a; Christopoulou et al., 2022; Gunasekar et al., 2023; Li et al., 2023b; Bui et al., 2023; Scao et al., 2022a;b). Tuy nhiên, những mô hình này chỉ tiếp tục mã khiến chúng khó sử dụng cho các tác vụ như giải thích mã bằng ngôn ngữ tự nhiên (HUMAN EVALEXPLAIN). Dạy chúng tuân theo chỉ dẫn của con người là rất quan trọng để làm cho chúng có thể áp dụng cho các tác vụ đa dạng.

5.2 CÁC MÔ HÌNH CHỈ DẪN

Huấn luyện các mô hình tuân theo chỉ dẫn đã dẫn đến các khả năng mới trong phương thức văn bản (Ouyang et al., 2022; Wang et al., 2022b; Chung et al., 2022) và thị giác (Xu et al., 2023b; OpenAI, 2023). Công trình trước đã chỉ ra lợi ích của nó cho các tác vụ ngôn ngữ truyền thống (Wei et al., 2022; Longpre et al., 2023a; Iyer et al., 2022), tác vụ đa ngôn ngữ (Muennighoff et al., 2022b; 2024; Yong et al., 2022; Üstün et al., 2024), và đối thoại (Köpf et al., 2023; Bai et al., 2022; Ganguli et al., 2022). Đối với các ứng dụng mã hóa, PanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) và InstructCodeT5+ (Wang et al., 2023c) là các mô hình gần đây được huấn luyện với chỉ dẫn mã hóa. Tuy nhiên, tất cả đều sử dụng bộ dữ liệu CodeAlpaca (Chaudhary, 2023), được tạo ra tổng hợp từ các mô hình OpenAI. Sử dụng dữ liệu từ các mô hình nguồn đóng mạnh mẽ cung cấp lợi thế mạnh mẽ, nhưng hạn chế việc sử dụng mô hình và có những hạn chế khác được nêu bật trong §1. CoEditor (Wei et al., 2023) đề xuất một tác vụ "tự động chỉnh sửa", được huấn luyện trên 1650 kho lưu trữ lịch sử commit python. Công trình của chúng tôi mở rộng điều này thành các tác vụ mã hóa tổng quát hơn thông qua chỉ dẫn, nhiều ngôn ngữ hơn và dữ liệu commit nhiều bậc độ lớn hơn.

5.3 CÁC BENCHMARK MÃ

Nhiều benchmark tổng hợp mã đã được đề xuất (Wang et al., 2022d;c; Yu et al., 2023; Lai et al., 2023; Du et al., 2023). HumanEval (Chen et al., 2021; Liu et al., 2023b) đã nổi lên như tiêu chuẩn cho tác vụ này. Công trình trước đã mở rộng HumanEval sang các ngôn ngữ lập trình mới thông qua các cơ chế dịch tự động (Athiwaratkun et al., 2022; Cassano et al., 2023; Orlanski et al., 2023). Những cách tiếp cận này dễ gây lỗi và chỉ dịch các bài kiểm tra, không phải các giải pháp thực tế, cần thiết cho các tác vụ như giải thích mã. Do đó, chúng tôi chỉ dựa vào con người để tạo tất cả các phần của HUMAN EVALPACK bao gồm các trường hợp kiểm tra, giải pháp chính xác, giải pháp có lỗi và metadata khác trên 6 ngôn ngữ.

Sửa chữa mã thường được đánh giá trên Quixbugs (Lin et al., 2017; Prenner & Robbes, 2021; Ye et al., 2021; Xia & Zhang, 2023; Jiang et al., 2023; Sobania et al., 2023) hoặc lỗi Python (He et al., 2022; Bradley et al., 2023). Cái sau không hỗ trợ thực thi mã, điều này hạn chế tiện ích của nó. Trong khi Quixbugs hỗ trợ thực thi với bài kiểm tra đơn vị, nó chỉ chứa 40 mẫu trong Python và Java. Hơn nữa, các vấn đề trong Quixbugs là các hàm chung, như sắp xếp bucket. Điều này khiến chúng dễ giải quyết và khó khử nhiễm dữ liệu huấn luyện. Benchmark của chúng tôi, HUMAN EVALFIX, chứa 164 hàm có lỗi cho sáu ngôn ngữ với giải pháp và bài kiểm tra đơn vị. Hơn nữa, các vấn đề mã hóa của chúng tôi, được dẫn xuất từ HumanEval, rất cụ thể, như theo dõi số dư tài khoản ngân hàng (xem Hình 14).

Công trình trước về đánh giá lời giải thích mã (Lu et al., 2021; Cui et al., 2022) đã dựa vào các độ đo như METEOR (Banerjee & Lavie, 2005) hoặc BLEU (Papineni et al., 2002). Bằng cách kết nối giải thích mã với tổng hợp mã, chúng tôi có thể đánh giá tác vụ này bằng độ đo pass@ k dựa trên thực thi vượt qua những hạn chế lớn của BLEU và các độ đo dựa trên heuristic khác (Reiter, 2018).

Đánh giá quy mô lớn đã được chứng minh hữu ích trong nhiều lĩnh vực xử lý ngôn ngữ tự nhiên (Wang et al., 2019; Kiela et al., 2021; Srivastava et al., 2022; Muennighoff et al., 2022a). Bằng cách tạo ra 18 điểm (6 ngôn ngữ trên 3 tác vụ) cho 9 mô hình, chúng tôi tiến một bước hướng tới đánh giá quy mô lớn các mô hình mã. Tuy nhiên, chúng tôi thiếu nhiều mô hình có khả năng tạo mã (Black et al., 2021; Fried et al., 2022; Black et al., 2022; Wang & Komatsuzaki, 2021; Biderman et al., 2023b). Công trình tương lai có thể xem xét nhiều mô hình hơn hoặc mở rộng HUMAN EVALPACK sang các ngôn ngữ hoặc tác vụ mới, như hiệu quả mã (Madaan et al., 2023a; Yetistiren et al., 2022) hoặc phân loại mã (Khan et al., 2023).

6 KẾT LUẬN

Công trình này nghiên cứu huấn luyện và đánh giá các LLM mã tuân theo chỉ dẫn. Chúng tôi giới thiệu COMMIT PACK, một bộ dữ liệu 4TB commit Git bao gồm 350 ngôn ngữ lập trình. Chúng tôi lọc bộ dữ liệu quy mô lớn này để tạo COMMIT PACKFT, 2GB mã chất lượng cao với thông điệp commit tương đồng chỉ dẫn. Để cho phép đánh giá toàn diện các mô hình mã chỉ dẫn, chúng tôi xây dựng HUMAN EVALPACK, một benchmark do con người viết bao gồm 3 tác vụ khác nhau cho 6 ngôn ngữ lập trình. Chúng tôi ablate một số bộ dữ liệu chỉ dẫn và thấy rằng COMMIT PACKFT kết hợp với dữ liệu ngôn ngữ tự nhiên dẫn đến hiệu suất tốt nhất. Trong khi các mô hình của chúng tôi, OCTOCODER và OCTOGEEX, là các LLM mã được cấp phép cho phép tốt nhất có sẵn, chúng vẫn bị vượt qua bởi các mô hình nguồn đóng như GPT-4. Ngoài việc cải thiện mô hình điều chỉnh chỉ dẫn, công trình tương lai nên xem xét huấn luyện các mô hình cơ sở có khả năng hơn.
