# OCTOPACK: TINH CHỈNH THEO HƯỚNG DẪN CÁC MÔ HÌNH NGÔN NGỮ LỚN CHO MÃ
# Đã được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2308.07124.pdf
# Kích thước tệp: 2618401 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OCTOPACK: TINH CHỈNH THEO HƯỚNG DẪN CÁC MÔ HÌNH NGÔN NGỮ LỚN CHO MÃ

Niklas Muennighoff Qian Liu Armel Zebaze Qinkai Zheng Binyuan Hui
Terry Yue Zhuo Swayam Singh Xiangru Tang Leandro von Werra Shayne Longpre
n.muennighoff@gmail.com

TÓM TẮT
Tinh chỉnh các mô hình ngôn ngữ lớn (LLM) trên các hướng dẫn dẫn đến cải thiện hiệu suất đáng kể trên các tác vụ ngôn ngữ tự nhiên. Chúng tôi áp dụng tinh chỉnh theo hướng dẫn sử dụng mã, tận dụng cấu trúc tự nhiên của các commit Git, kết hợp các thay đổi mã với hướng dẫn của con người. Chúng tôi biên soạn COMMIT PACK: 4 terabyte các commit Git trên 350 ngôn ngữ lập trình. Chúng tôi đánh giá COMMIT PACK so với các hướng dẫn mã tự nhiên và tổng hợp khác (xP3x, Self-Instruct, OASST) trên mô hình StarCoder 16B tham số, và đạt được hiệu suất tối ưu trong số các mô hình không được huấn luyện trên đầu ra OpenAI, trên benchmark HumanEval Python (46.2% pass@ 1). Chúng tôi giới thiệu thêm HUMAN EVALPACK, mở rộng benchmark HumanEval thành tổng cộng 3 tác vụ mã hóa (Sửa chữa mã, Giải thích mã, Tổng hợp mã) trên 6 ngôn ngữ (Python, JavaScript, Java, Go, C++, Rust). Các mô hình của chúng tôi, OCTOCODER và OCTOGEEX, đạt hiệu suất tốt nhất trên HUMAN EVALPACK trong số tất cả các mô hình cho phép, chứng minh lợi ích của COMMIT PACK trong việc tổng quát hóa sang một tập hợp rộng hơn các ngôn ngữ và tác vụ mã hóa tự nhiên. Mã, mô hình và dữ liệu có sẵn miễn phí tại https://github.com/bigcode-project/octopack .

import numpy as np
import matplotlib.pyplot as plt
# tạo dữ liệu mẫu
x_data = np.linspace(-5, 5, 20)
y_data = np.random.normal(0.0, 1.0, x_data.size)
plt.plot(x_data, y_data, 'o')
plt.show()

Mã Trước
Commit
Thông điệp

Mã Sau
Thay đổi thành hàm sin() với nhiễu

import math
import numpy as np
import matplotlib.pyplot as plt
# tạo dữ liệu mẫu
x_data = np.linspace(-math.pi, math.pi, 30)
y_data = np.sin(x_data) + np.random.normal(0.0, 0.1, x_data.size)
plt.plot(x_data, y_data, 'o')
plt.show()

1) CommitPack
2) HumanEvalPack

Hình 1: Tổng quan OCTOPACK. 1) Mẫu từ bộ dữ liệu 4TB của chúng tôi, COMMIT PACK. 2) Hiệu suất của OCTOCODER, OCTOGEEX và các mô hình mã khác bao gồm các mô hình không cho phép (WizardCoder, GPT-4) trên H UMAN EVALPACK bao gồm 3 tác vụ mã hóa và 6 ngôn ngữ lập trình.

--- TRANG 2 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

1 GIỚI THIỆU

Tinh chỉnh các mô hình ngôn ngữ lớn (LLM) trên nhiều tác vụ ngôn ngữ được giải thích qua hướng dẫn (tinh chỉnh theo hướng dẫn) đã được chứng minh là cải thiện khả năng sử dụng và hiệu suất chung của mô hình (Wei et al., 2022; Sanh et al., 2022; Min et al., 2022; Ouyang et al., 2022). Mô hình tinh chỉnh theo hướng dẫn cũng đã chứng minh thành công cho các mô hình được huấn luyện trên dữ liệu hình ảnh (Liu et al., 2023a; Li et al., 2023a), âm thanh (Zhang et al., 2023b) và đa ngôn ngữ (Muennighoff et al., 2022b; Wang et al., 2022b).

Trong công trình này, chúng tôi tinh chỉnh theo hướng dẫn các LLM trên phương thức mã hóa. Trong khi các Code LLM đã có thể được hướng dẫn gián tiếp để tạo ra mã mong muốn bằng cách sử dụng các chú thích mã, quy trình này khá mong manh và không hoạt động khi đầu ra mong muốn là ngôn ngữ tự nhiên, chẳng hạn như giải thích mã. Tinh chỉnh theo hướng dẫn rõ ràng của các Code LLM có thể cải thiện khả năng điều khiển của chúng và cho phép ứng dụng vào nhiều tác vụ hơn. Đồng thời với công trình của chúng tôi, ba Code LLM được tinh chỉnh theo hướng dẫn đã được đề xuất: PanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) và InstructCodeT5+ (Wang et al., 2023c). Các mô hình này dựa vào các mô hình có khả năng cao hơn và đóng từ API OpenAI¹ để tạo dữ liệu huấn luyện hướng dẫn của chúng. Cách tiếp cận này có vấn đề vì (1) các API nguồn đóng liên tục thay đổi và có tính khả dụng không thể dự đoán được (Pozzobon et al., 2023; Chen et al., 2023a), (2) nó dựa trên giả định rằng một mô hình có khả năng cao hơn tồn tại (3) nó có thể củng cố ảo giác của mô hình (Gudibande et al., 2023) và (4), tùy thuộc vào cách giải thích pháp lý, điều khoản sử dụng của OpenAI² cấm các mô hình như vậy: "...Bạn không được...sử dụng đầu ra từ Dịch vụ để phát triển các mô hình cạnh tranh với OpenAI...". Do đó, chúng tôi coi các mô hình được huấn luyện trên đầu ra OpenAI không thể sử dụng cho mục đích thương mại trong thực tế và phân loại chúng là không cho phép trong công trình này.

Chúng tôi tập trung vào dữ liệu được cấp phép một cách cho phép hơn và tránh sử dụng mô hình nguồn đóng để tạo dữ liệu tổng hợp. Chúng tôi đánh giá bốn nguồn dữ liệu hướng dẫn mã phổ biến: (1) xP3x (Muennighoff et al., 2022b), chứa dữ liệu từ các benchmark mã thông thường, (2) dữ liệu Self-Instruct (Wang et al., 2023a) chúng tôi tạo ra bằng cách sử dụng Code LLM cho phép, (3) OASST (Köpf et al., 2023), chủ yếu chứa dữ liệu ngôn ngữ tự nhiên và ít ví dụ mã và (4) COMMIT PACK, bộ dữ liệu 4TB mới của chúng tôi về các commit Git. Mục đích chính của tinh chỉnh theo hướng dẫn là mở rộng khả năng tổng quát hóa của mô hình sang nhiều tác vụ và cài đặt đa dạng. Do đó, chúng tôi mở rộng benchmark tổng hợp mã, HumanEval (Chen et al., 2021; Zheng et al., 2023), để tạo ra HUMAN EVALPACK: Một benchmark mã bao gồm tổng hợp mã, sửa chữa mã và giải thích mã trên sáu ngôn ngữ lập trình.

Tinh chỉnh theo hướng dẫn StarCoder (Li et al., 2023b) trên một biến thể đã lọc của COMMIT PACK và OASST dẫn đến mô hình tốt nhất của chúng tôi, OCTOCODER, vượt trội hơn tất cả các mô hình được cấp phép mở khác (Hình 1), nhưng kém hơn GPT-4 lớn hơn nhiều (OpenAI, 2023). GPT-4 gần đạt hiệu suất tối đa trên biến thể tổng hợp mã, đáng chú ý với điểm pass@ 1 là 86.6% trên Python HumanEval. Tuy nhiên, nó hoạt động kém hơn đáng kể trên các biến thể sửa chữa mã và giải thích của HUMAN EVALPACK, mà chúng tôi giới thiệu. Điều này cho thấy benchmark HumanEval gốc có thể sớm ngừng hữu ích do các mô hình đạt gần hiệu suất tối đa. Các biến thể đánh giá đầy thách thức hơn của chúng tôi cung cấp không gian cho các LLM tương lai cải thiện hiệu suất của trạng thái nghệ thuật hiện tại.

Tóm tắt, chúng tôi đóng góp:
• COMMIT PACK và COMMIT PACKFT: 4TB các commit mã được cấp phép cho phép trên 350 ngôn ngữ lập trình cho pretraining và một biến thể đã lọc 2GB chứa hướng dẫn mã chất lượng cao được sử dụng cho finetuning
• HUMAN EVALPACK: Một benchmark cho tổng quát hóa Code LLM, bao gồm ba kịch bản (Sửa chữa mã, Giải thích mã, Tổng hợp mã) và 6 ngôn ngữ lập trình (Python, JavaScript, Java, Go, C++, Rust)
• OCTOCODER và OCTOGEEX: Các Code LLM cho phép tốt nhất

2 COMMIT PACK: DỮ LIỆU HƯỚNG DẪN MÃ

Công trình trước đây đã chỉ ra rằng các mô hình có thể tổng quát hóa sang các ngôn ngữ được bao gồm trong pretraining, nhưng vắng mặt trong tinh chỉnh theo hướng dẫn (Muennighoff et al., 2022b). Tuy nhiên, họ cũng chỉ ra rằng việc bao gồm các ngôn ngữ như vậy trong tinh chỉnh theo hướng dẫn còn thúc đẩy hiệu suất của chúng hơn nữa. Chúng tôi giả thuyết rằng dữ liệu mã thể hiện hành vi tương tự. Để cải thiện hiệu suất trên các tác vụ liên quan đến mã, chúng tôi do đó xây dựng một bộ dữ liệu hướng dẫn mã tận dụng cấu trúc tự nhiên của các commit Git.

COMMIT PACK Để tạo bộ dữ liệu, chúng tôi sử dụng metadata commit từ GitHub action dump trên Google BigQuery.³ Chúng tôi áp dụng các bộ lọc chất lượng, lọc cho các giấy phép thân thiện thương mại, và loại bỏ các commit ảnh hưởng đến nhiều hơn một tệp để đảm bảo thông điệp commit rất cụ thể và tránh độ phức tạp bổ sung từ việc xử lý nhiều tệp. Chúng tôi sử dụng metadata đã lọc để scrape các tệp mã bị ảnh hưởng trước và sau commit từ GitHub. Điều này dẫn đến gần 4 terabyte dữ liệu bao gồm 350 ngôn ngữ lập trình (COMMIT PACK). Vì tinh chỉnh theo hướng dẫn không cần quá nhiều dữ liệu (Zhou et al., 2023a; Touvron et al., 2023), chúng tôi áp dụng một số bộ lọc nghiêm ngặt để

¹ https://openai.com/blog/openai-api
² https://openai.com/policies/terms-of-use

--- TRANG 3 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

giảm bộ dữ liệu xuống 2 gigabyte và 277 ngôn ngữ (COMMIT PACKFT). Những bộ lọc này bao gồm việc lọc cho các mẫu mà thông điệp commit có các từ cụ thể ở dạng mệnh lệnh viết hoa ở đầu (ví dụ: "Verify ..."), bao gồm nhiều từ, và không chứa tham chiếu bên ngoài. Tất cả các bộ lọc được chi tiết trong Phụ lục D. Hình 2 mô tả phân phối của cả hai bộ dữ liệu và các tác vụ chứa trong COMMIT PACKFT. Để tinh chỉnh theo hướng dẫn các mô hình của chúng tôi, chúng tôi chọn 5,000 mẫu ngẫu nhiên từ COMMIT PACKFT trên 6 ngôn ngữ lập trình mà chúng tôi đánh giá. Trong Phụ lục G, chúng tôi cũng thử nghiệm với pretraining trên toàn bộ COMMIT PACK.

[Hình 2: Tổng quan về COMMIT PACK và COMMIT PACK FT. Trên: Phân phối ngôn ngữ của dữ liệu commit đầy đủ (COMMIT PACK) và biến thể được lọc cho hướng dẫn chất lượng cao (COMMIT PACKFT). Xem Phụ lục C cho phân phối đầy đủ. Dưới: Phân phối tác vụ của các commit trên tập con Python của COMMIT PACKFT (59K mẫu) theo GPT-4.]

Các phương án thay thế Chúng tôi xem xét ba bộ dữ liệu bổ sung cho tinh chỉnh theo hướng dẫn được trình bày trong Bảng 1.

xP3x: xP3x là một bộ sưu tập quy mô lớn dữ liệu hướng dẫn đa ngôn ngữ với khoảng 532 triệu mẫu (Muennighoff et al., 2022b). Chúng tôi chỉ tập trung vào tập con mã của xP3x, loại trừ Neural-CodeSearch (Li et al., 2019) không được cấp phép cho phép, và chọn 5,000 mẫu.

Self-Instruct: Sử dụng phương pháp Self-Instruct (Wang et al., 2022a) và mô hình StarCoder (Li et al., 2023b), chúng tôi tạo ra 5,003 hướng dẫn tổng hợp và các câu trả lời tương ứng.

OASST: OASST là một bộ dữ liệu đa dạng của các cuộc đối thoại chat nhiều lượt (Köpf et al., 2023). Chỉ một vài cuộc đối thoại chứa mã. Chúng tôi tái sử dụng một biến thể đã lọc từ công trình trước (Dettmers et al., 2023) và lọc thêm để loại bỏ các câu trả lời của trợ lý có tính thuyết giáo (Phụ lục D) dẫn đến 8,587 mẫu.

[Bảng 1: Thống kê của dữ liệu hướng dẫn mã chúng tôi xem xét. Chúng tôi hiển thị số ngôn ngữ lập trình, tổng số mẫu và tỷ lệ mẫu chứa mã cho các bộ dữ liệu hướng dẫn cho phép. Để finetuning trên các bộ dữ liệu này, chúng tôi sử dụng các tập con nhỏ với khoảng 5,000 mẫu mỗi tập.]

3 HUMAN EVALPACK: ĐÁNH GIÁ CÁC MÔ HÌNH MÃ ĐƯỢC TINH CHỈNH THEO HƯỚNG DẪN

Ngôn ngữ: Python, JavaScript, Java, Go, C++, Rust
Tác vụ phụ: HumanEvalFix, HumanEvalExplain, HumanEvalSynthesize

[Hình 3: Tổng quan HUMAN EVALPACK. Bài toán HumanEval đầu tiên được mô tả qua ba kịch bản cho Python. Lỗi cho HUMAN EVALFIX bao gồm thiếu câu lệnh "abs".]

Khi tinh chỉnh theo hướng dẫn các LLM sử dụng dữ liệu ngôn ngữ tự nhiên (NL), đầu vào là một hướng dẫn NL với ngữ cảnh NL tùy chọn và đầu ra đích là câu trả lời NL cho tác vụ (Wei et al., 2022). Khi tinh chỉnh theo hướng dẫn với dữ liệu mã (C), mã có thể xuất hiện chỉ trong đầu vào cùng với hướng dẫn NL (NL+C →NL, ví dụ: giải thích mã), chỉ trong đầu ra (NL→C, ví dụ: tổng hợp mã), hoặc trong cả đầu vào và đầu ra (NL+C →C, ví dụ: thay đổi mã như sửa lỗi). Trong khi các benchmark trước đây thường chỉ bao gồm các biến thể của tổng hợp mã, người dùng có thể muốn sử dụng mô hình trong cả ba kịch bản. Do đó, chúng tôi mở rộng benchmark tổng hợp mã HumanEval (Chen et al., 2021; Zheng et al., 2023) để bao gồm cả ba kết hợp đầu vào-đầu ra cho sáu ngôn ngữ (Hình 3).

--- TRANG 4 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

HUMAN EVALFIX (NL+C →C) Cho một hàm mã không chính xác với lỗi tinh vi và các bài kiểm tra đơn vị đi kèm, mô hình được giao nhiệm vụ sửa hàm. Chúng tôi thêm thủ công một lỗi vào mỗi trong số 164 giải pháp HumanEval trên tất cả 6 ngôn ngữ (tổng cộng 984 lỗi). Đối với một mẫu cho trước, các lỗi càng tương tự càng tốt trên 6 ngôn ngữ cho phép so sánh có ý nghĩa của điểm số trên các ngôn ngữ. Các lỗi được viết sao cho mã vẫn chạy nhưng tạo ra kết quả không chính xác dẫn đến ít nhất một bài kiểm tra đơn vị thất bại. Thống kê lỗi và ví dụ ở Phụ lục L. Chúng tôi cũng đánh giá một biến thể dễ hơn của tác vụ này nơi thay vì bài kiểm tra đơn vị, các mô hình được cung cấp docstring hàm chính xác làm nguồn sự thật để sửa lỗi, xem Phụ lục K.

HUMAN EVALEXPLAIN (NL+C →NL) Cho một hàm mã chính xác, mô hình được giao nhiệm vụ tạo ra một giải thích về mã. Tiếp theo, cùng mô hình được giao nhiệm vụ tái tạo mã chỉ dựa trên giải thích của chính nó. Bước thứ hai cho phép chúng tôi chấm điểm tác vụ này thông qua thực thi mã và đo pass@k (Chen et al., 2021) thay vì đánh giá bản thân giải thích bằng các thước đo dựa trên heuristic như BLEU (Papineni et al., 2002) hoặc ROUGE (Lin, 2004) có những hạn chế lớn (Reiter, 2018; Schluter, 2017; Eghbali & Pradel, 2022; Zhou et al., 2023b). Để ngăn các mô hình sao chép giải pháp vào mô tả, chúng tôi loại bỏ bất kỳ sự trùng lặp giải pháp nào ít nhất 20 ký tự từ mô tả. Chúng tôi thêm áp dụng giới hạn độ dài ký tự trên giải thích được tạo bởi mô hình tương đương với độ dài của docstring mô tả hàm. Giới hạn này được chỉ định trong prompt cho mô hình. Lưu ý rằng chính docstring hàm không bao giờ được cung cấp cho mô hình trong tác vụ này.

HUMAN EVALSYNTHESIZE (NL→C) Cho một docstring hoặc chú thích ngôn ngữ tự nhiên mô tả mã mong muốn, mô hình được giao nhiệm vụ tổng hợp mã chính xác. Tác vụ này tương ứng với benchmark HumanEval gốc (Chen et al., 2021). Đối với các mô hình được tinh chỉnh theo hướng dẫn, chúng tôi thêm một hướng dẫn rõ ràng vào đầu vào giải thích mô hình nên làm gì. Đối với các mô hình chỉ trải qua pretraining mô hình ngôn ngữ, chúng tôi làm theo Chen et al. (2021) và cung cấp cho mô hình header hàm và docstring để đánh giá việc hoàn thành hàm của nó.

Đối với tất cả các tác vụ, chúng tôi thực thi các tạo sinh mã để tính toán hiệu suất bằng thước đo pass@k (Chen et al., 2021): một bài toán được coi là giải quyết nếu bất kỳ k tạo sinh mã nào vượt qua mọi trường hợp kiểm tra. Chúng tôi tập trung vào phiên bản đơn giản nhất của pass@k, đó là pass@1: khả năng mô hình giải quyết bài toán trong một lần thử. Như Chen et al. (2021), chúng tôi sử dụng nhiệt độ lấy mẫu là 0.2 và topp= 0.95 để ước tính pass@1. Chúng tôi tạo n= 20 mẫu, đủ để có ước tính pass@1 đáng tin cậy (Li et al., 2023b). Đối với GPT-4, chúng tôi tạo n= 1 mẫu. Sử dụng n= 1 thay vì n= 20 cho GPT-4 chỉ thay đổi điểm số từ 75.0% thành 75.2% pass@1 trên HUMAN EVALSYNTHESIZE Python trong khi tiết kiệm chi phí 20 lần.

Python HumanEval là benchmark mã được sử dụng rộng rãi nhất và nhiều bộ dữ liệu huấn luyện đã được khử nhiễm cho nó (Kocetkov et al., 2022). Bằng cách mở rộng HumanEval thủ công, chúng tôi đảm bảo việc khử nhiễm hiện có vẫn hợp lệ để cho phép đánh giá công bằng. Tuy nhiên, điều này có thể không đúng cho tất cả các mô hình (ví dụ: GPT-4), do đó kết quả nên được giải thích cẩn thận.

4 OCTOCODER: CODE LLM ĐƯỢC CÂP PHÉP THƯƠNG MẠI TỐT NHẤT

4.1 NGHIÊN CỨU CÁC LỰA CHỌN DỮ LIỆU HƯỚNG DẪN

Chúng tôi tinh chỉnh theo hướng dẫn mô hình StarCoder đã được pretrain (Li et al., 2023b) trên các kết hợp khác nhau của các bộ dữ liệu hướng dẫn của chúng tôi (§2). Chúng tôi đánh giá tất cả các mô hình trên tập con Python của HUMAN EVALPACK như được mô tả trong Hình 4. Tương tự như công trình trước (Taori et al., 2023), chúng tôi định dạng tất cả các hướng dẫn thành một lược đồ nhất quán để phân biệt câu hỏi và câu trả lời (xem Hình 18).

COMMIT PACK FT cho phép Code LLM sửa lỗi COMMIT PACKFT là quan trọng đối với việc tăng hiệu suất trên sửa chữa mã (HUMAN EVALFIX), nơi tinh chỉnh theo hướng dẫn chỉ trên OASST hoặc các biến thể khác dẫn đến điểm số thấp hơn đáng kể. Điều này có thể do COMMIT PACKFT bao gồm khoảng 20% sửa lỗi trong số các tác vụ liên quan đến mã khác (Hình 2).

Tầm quan trọng của các mẫu với đích ngôn ngữ tự nhiên Mô hình StarCoder đã được pretrain, cũng như biến thể Self-Instruct, hoạt động kém trên giải thích mã (HUMAN EVALEXPLAIN). Điều này là do cả hai mô hình chỉ được điều kiện hóa để viết mã thay vì ngôn ngữ tự nhiên. Chúng tôi thấy rằng để

--- TRANG 5 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

hoạt động tốt trong việc giải thích mã, cần phải bao gồm các mẫu với ngôn ngữ tự nhiên làm đầu ra đích trong quá trình tinh chỉnh theo hướng dẫn. Chỉ dựa vào dữ liệu với mã làm đích, chẳng hạn như dữ liệu Self-Instruct, sẽ dẫn đến các mô hình luôn xuất ra mã ngay cả khi câu hỏi yêu cầu đầu ra ngôn ngữ tự nhiên. Do đó, chúng tôi trộn tất cả các ablation khác với OASST, chứa nhiều đích ngôn ngữ tự nhiên. Trong khi tập con xP3x cũng chứa các mẫu với đầu ra ngôn ngữ tự nhiên, nhiều đầu ra đích của nó ngắn, dẫn đến các mô hình có xu hướng cho câu trả lời ngắn. Điều này không thực tế cho tác vụ giải thích dẫn đến điểm số tương đối thấp của việc trộn xP3x với OASST.

COMMIT PACK FT+OASST mang lại hiệu suất tốt nhất Tất cả các bộ dữ liệu hướng dẫn cung cấp những cải thiện tương tự cho tổng hợp mã (HUMAN EVALSYNTHESIZE), đã là trọng tâm của tất cả công trình trước về các mô hình hướng dẫn mã (Wang et al., 2023c; Luo et al., 2023; Muennighoff et al., 2022b). Chúng tôi đạt được điểm số trung bình tốt nhất bằng cách tinh chỉnh theo hướng dẫn trên COMMIT PACKFT được trộn với dữ liệu OASST đã lọc của chúng tôi mang lại cải thiện tuyệt đối 23% so với StarCoder. Do đó, chúng tôi chọn COMMIT PACKFT+OASST cho mô hình cuối cùng được gọi là OCTOCODER. Sử dụng cùng dữ liệu, chúng tôi cũng tinh chỉnh theo hướng dẫn mô hình CodeGeeX2 6 tỷ tham số (Zheng et al., 2023) để tạo ra OCTOGEEX. Các siêu tham số huấn luyện cho cả hai mô hình ở Phụ lục P.

[Hình 4: So sánh các bộ dữ liệu hướng dẫn được cấp phép cho phép bằng cách tinh chỉnh theo hướng dẫn StarCoder. Các mô hình được đánh giá trên tập con Python của HUMAN EVALPACK.]

4.2 SO SÁNH VỚI CÁC MÔ HÌNH KHÁC

Chúng tôi đánh giá OCTOCODER và OCTOGEEX với các Code LLM tối ưu trên HUMAN EVALPACK trong Bảng 2. Đối với tất cả các mô hình, chúng tôi sử dụng prompt được đưa ra bởi những người tạo ra mô hình nếu có thể áp dụng hoặc một prompt trực quan đơn giản, xem Phụ lục Q.

OCTOCODER hoạt động tốt nhất trong số các mô hình cho phép OCTOCODER có điểm số trung bình cao nhất trên tất cả ba kịch bản đánh giá trong số tất cả các mô hình cho phép. Chỉ với 6 tỷ tham số, OCTOGEEX là mô hình nhỏ nhất được đánh giá, nhưng vẫn vượt trội hơn tất cả Code LLM cho phép trước đây. GPT-4 (OpenAI, 2023) hoạt động tốt nhất trong số tất cả các mô hình được đánh giá với khoảng cách đáng kể. Tuy nhiên, GPT-4 là nguồn đóng và có thể lớn hơn nhiều so với tất cả các mô hình khác được đánh giá.

Tinh chỉnh theo hướng dẫn tổng quát hóa sang các ngôn ngữ lập trình chưa thấy Được huấn luyện chủ yếu trên ngôn ngữ tự nhiên, không phải mã, BLOOMZ (Muennighoff et al., 2022b) hoạt động tệ hơn các mô hình khác mặc dù có 176 tỷ tham số. Go và Rust không có trong dữ liệu hướng dẫn của BLOOMZ, nhưng nó hoạt động tốt hơn nhiều so với đường cơ sở ngẫu nhiên 0.0 cho hai ngôn ngữ này trên hầu hết các tác vụ. Điều này xác nhận giả thuyết của chúng tôi rằng các mô hình có khả năng tổng quát hóa hướng dẫn sang các ngôn ngữ lập trình chỉ được thấy trong pretraining, tương tự như tổng quát hóa đa ngôn ngữ cho ngôn ngữ tự nhiên (Muennighoff et al., 2022b). Để cải thiện tổng quát hóa ngôn ngữ lập trình hơn nữa, chúng tôi tinh chỉnh OCTOCODER và OCTOGEEX trên nhiều ngôn ngữ từ COMMIT PACKFT, và cải thiện tổng quát hóa này được phản ánh trong hiệu suất trên các ngôn ngữ mới của HUMAN EVALPACK.

Trọng số pretraining tương quan với hiệu suất ngôn ngữ lập trình sau tinh chỉnh theo hướng dẫn Công trình trước đã chỉ ra rằng hiệu suất trên các ngôn ngữ tự nhiên sau tinh chỉnh theo hướng dẫn tương quan với trọng số của các ngôn ngữ này trong pretraining (Muennighoff et al., 2022b). Trọng số càng cao trong pretraining, hiệu suất sau tinh chỉnh theo hướng dẫn càng tốt. Chúng tôi thấy điều tương tự đúng

--- TRANG 6 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

cho các ngôn ngữ lập trình. Python, Java và JavaScript cộng lại chiếm khoảng 30% dữ liệu pretraining của StarCoder (Li et al., 2023b). Sau khi tinh chỉnh theo hướng dẫn StarCoder để tạo ra OCTOCODER, chúng tôi thấy hiệu suất tốt nhất trong ba ngôn ngữ này, đặc biệt là cho HUMAN EVALSYNTHESIZE. OCTOCODER hoạt động yếu nhất trên Rust, là ngôn ngữ tài nguyên thấp nhất của StarCoder trong số các ngôn ngữ chúng tôi đánh giá (1.2% dữ liệu pretraining).

Các mô hình gặp khó khăn với những thay đổi nhỏ có mục tiêu HUMAN EVALFIX là tác vụ đầy thử thách nhất đối với hầu hết các mô hình. Chúng thường tái tạo hàm lỗi mà không thực hiện bất kỳ thay đổi nào (ví dụ: WizardCoder trong Hình 34) hoặc chúng giới thiệu lỗi mới (ví dụ: GPT-4 trong Hình 33). Chúng tôi phân tích hiệu suất mô hình theo loại lỗi trong Phụ lục M và thấy các lỗi yêu cầu loại bỏ mã thừa là đầy thử thách nhất. OCTOCODER hoạt động tương đối tốt trên tất cả các ngôn ngữ. Tinh chỉnh theo hướng dẫn trên COMMIT PACKFT có thể đã dạy OCTOCODER thực hiện những thay đổi nhỏ, có mục tiêu để sửa lỗi.

Các mô hình gặp khó khăn khi chuyển đổi giữa mã và văn bản Một số mô hình thất bại ở HUMAN EVALEXPLAIN, vì chúng không tạo ra giải thích ngôn ngữ tự nhiên. Chúng tôi kiểm tra thủ công các giải thích cho mười mẫu đầu tiên của phần Python và loại bỏ một mô hình nếu không có giải thích nào trong số chúng. Đây là trường hợp của StarCoder và CodeGeeX2, tạo ra mã thay vì giải thích ngôn ngữ tự nhiên. BLOOMZ và InstructCodeT5+ cũng thỉnh thoảng tạo ra mã. Các mô hình khác chỉ tạo ra giải thích ngôn ngữ tự nhiên, không chứa bất kỳ mã nào cho các mẫu được kiểm tra.

Các mô hình gặp khó khăn tuân thủ độ dài đầu ra được chỉ định HUMAN EVALEXPLAIN hướng dẫn các mô hình vừa giải thích của chúng trong giới hạn ký tự cho trước (§3). Các mô hình hiện tại dường như không hiểu họ đang tạo ra bao nhiêu ký tự. Chúng thường viết những giải thích rất ngắn và do đó chưa được chỉ định đầy đủ (ví dụ: BLOOMZ trong Hình 35) hoặc giải thích quá dài cuối cùng bị cắt (ví dụ: StarChat-β trong Hình 38). Công trình tương lai có thể điều tra cách cho phép các mô hình nhận thức về độ dài đầu ra được tạo của chúng để cải thiện hiệu suất HUMAN EVALEXPLAIN.

Tổng hợp mã HumanEval gần bão hòa Tổng hợp mã thuần túy trên HUMAN EVALSYNTHESIZE là tác vụ dễ nhất cho tất cả các mô hình. Với tỷ lệ vượt qua 86.6% cho một giải pháp duy nhất, GPT-4 gần hoàn toàn bão hòa tập con Python. GPT-4 ban đầu được thấy đạt 67% trên Python HumanEval (OpenAI, 2023) và 81% trong công trình sau này (Bubeck et al., 2023). Điểm số của chúng tôi cho GPT-4 cao hơn đáng kể, có thể do những cải thiện được thực hiện đối với API bởi OpenAI, nhiễm HumanEval trong huấn luyện GPT-4, hoặc prompting và đánh giá hơi khác nhau. Một ví dụ về prompt của chúng tôi được mô tả trong Hình 3 (phải). Chúng tôi thực hiện đánh giá rất cẩn thận để đảm bảo mọi tạo sinh được xử lý chính xác. Chúng tôi tái tạo điểm số HumanEval của WizardCoder (Luo et al., 2023; Xu et al., 2023a) và thấy nó cũng hoạt động tốt trên các ngôn ngữ khác. Đối với BLOOMZ và InstructCodeT5+, đánh giá của chúng tôi dẫn đến điểm số Python cao hơn so với họ báo cáo, có thể do xử lý tạo sinh cẩn thận hơn của chúng tôi. OCTOCODER có hiệu suất cao nhất cho mọi ngôn ngữ trong số các mô hình được cấp phép cho phép. Với pass@1 là 46.2% trên phần Python gốc, OCTOCODER cải thiện tương đối 38% so với mô hình cơ sở StarCoder.

[Bảng 2: Hiệu suất pass@1 (%) zero-shot trên HUMAN EVALPACK. InstructCodeT5+, WizardCoder, StarChat-β, StarCoder và OCTOCODER có 16B tham số. CodeGeeX2 và OCTOGEEX có 6B tham số. BLOOMZ có 176B tham số. Trong công trình này, chúng tôi gọi các mô hình "cho phép" nếu trọng số có thể truy cập tự do và sử dụng được cho mục đích thương mại.]

5 CÔNG TRÌNH LIÊN QUAN

5.1 CÁC MÔ HÌNH MÃ

Đã có công trình mở rộng về các mô hình mã được thiết kế riêng cho một tác vụ mã hóa cụ thể, chẳng hạn như tóm tắt mã (Iyer et al., 2016; Ahmad et al., 2020; Zhang et al., 2022a; Shi et al., 2022) hoặc chỉnh sửa mã (Drain et al., 2021; Zhang et al., 2022c; He et al., 2022; Zhang et al., 2022b; Wei et al., 2023; Prenner & Robbes, 2023; Fakhoury et al., 2023; Skreta et al., 2023) (cũng xem công trình về các mô hình chỉnh sửa tổng quát hơn (Reid & Neubig, 2022; Schick et al., 2022; Dwivedi-Yu et al., 2022; Raheja et al., 2023)). Các công trình này sử dụng heuristic cụ thể theo tác vụ giới hạn khả năng áp dụng phương pháp của chúng cho các tác vụ khác. Ngược lại, chúng tôi nhằm xây dựng các mô hình có thể áp dụng cho tất cả các loại tác vụ liên quan đến mã và hơn thế nữa. Thông qua pretraining quy mô lớn, các mô hình mã có thể áp dụng tổng quát hơn đã được phát triển (Nijkamp et al., 2022; 2023; Xu et al., 2022a; Christopoulou et al., 2022; Gunasekar et al., 2023; Li et al., 2023b; Bui et al., 2023; Scao et al., 2022a;b). Tuy nhiên, các mô hình này chỉ tiếp tục mã khiến chúng khó sử dụng cho các tác vụ như giải thích mã bằng ngôn ngữ tự nhiên (HUMAN EVALEXPLAIN). Dạy chúng tuân theo hướng dẫn của con người là quan trọng để làm cho chúng có thể áp dụng cho các tác vụ đa dạng.

--- TRANG 7 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

5.2 CÁC MÔ HÌNH HƯỚNG DẪN

Huấn luyện các mô hình để tuân theo hướng dẫn đã dẫn đến các khả năng mới trong phương thức văn bản (Ouyang et al., 2022; Wang et al., 2022b; Chung et al., 2022) và hình ảnh (Xu et al., 2023b; OpenAI, 2023). Công trình trước đã chỉ ra lợi ích của nó cho các tác vụ ngôn ngữ truyền thống (Wei et al., 2022; Longpre et al., 2023a; Iyer et al., 2022), tác vụ đa ngôn ngữ (Muennighoff et al., 2022b; 2024; Yong et al., 2022; Üstün et al., 2024), và đối thoại (Köpf et al., 2023; Bai et al., 2022; Ganguli et al., 2022). Đối với các ứng dụng mã hóa, PanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) và InstructCodeT5+ (Wang et al., 2023c) là các mô hình gần đây được huấn luyện với hướng dẫn mã hóa. Tuy nhiên, tất cả chúng đều sử dụng bộ dữ liệu CodeAlpaca (Chaudhary, 2023), được tạo tổng hợp từ các mô hình OpenAI. Sử dụng dữ liệu từ các mô hình nguồn đóng mạnh mẽ cung cấp lợi thế mạnh, nhưng giới hạn việc sử dụng mô hình và có những hạn chế khác được nêu bật trong §1. CoEditor (Wei et al., 2023) đề xuất một tác vụ "tự động chỉnh sửa", được huấn luyện trên 1650 kho lưu trữ lịch sử commit python. Công trình của chúng tôi mở rộng điều này sang các tác vụ mã hóa tổng quát hơn thông qua hướng dẫn, nhiều ngôn ngữ hơn và dữ liệu commit nhiều bậc độ lớn hơn.

5.3 CÁC BENCHMARK MÃ

Nhiều benchmark tổng hợp mã đã được đề xuất (Wang et al., 2022d;c; Yu et al., 2023; Lai et al., 2023; Du et al., 2023). HumanEval (Chen et al., 2021; Liu et al., 2023b) đã nổi lên như tiêu chuẩn cho tác vụ này. Công trình trước đã mở rộng HumanEval sang các ngôn ngữ lập trình mới thông qua cơ chế dịch tự động (Athiwaratkun et al., 2022; Cassano et al., 2023; Orlanski et al., 2023). Những cách tiếp cận này dễ có lỗi và chỉ dịch các bài kiểm tra, không phải các giải pháp thực tế, cần thiết cho các tác vụ như giải thích mã. Do đó, chúng tôi chỉ dựa vào con người để tạo ra tất cả các phần của HUMAN EVALPACK bao gồm các trường hợp kiểm tra, giải pháp chính xác, giải pháp lỗi và metadata khác trên 6 ngôn ngữ.

Sửa chữa mã thường được đánh giá trên Quixbugs (Lin et al., 2017; Prenner & Robbes, 2021; Ye et al., 2021; Xia & Zhang, 2023; Jiang et al., 2023; Sobania et al., 2023) hoặc lỗi Python (He et al., 2022; Bradley et al., 2023). Cái sau không hỗ trợ thực thi mã, giới hạn tiện ích của nó. Trong khi Quixbugs hỗ trợ thực thi với bài kiểm tra đơn vị, nó chỉ chứa 40 mẫu trong Python và Java. Hơn nữa, các vấn đề trong Quixbugs là các hàm chung, chẳng hạn như sắp xếp bucket. Điều này làm cho chúng dễ giải quyết và khó khử nhiễm dữ liệu huấn luyện. Benchmark của chúng tôi, HUMAN EVALFIX, chứa 164 hàm lỗi cho sáu ngôn ngữ với giải pháp và bài kiểm tra đơn vị. Hơn nữa, các vấn đề mã hóa của chúng tôi, xuất phát từ HumanEval, rất cụ thể, chẳng hạn như theo dõi số dư tài khoản ngân hàng (xem Hình 14).

Công trình trước về đánh giá giải thích mã (Lu et al., 2021; Cui et al., 2022) đã dựa vào các thước đo như METEOR (Banerjee & Lavie, 2005) hoặc BLEU (Papineni et al., 2002). Bằng cách chuỗi giải thích mã với tổng hợp mã, chúng tôi có thể đánh giá tác vụ này bằng thước đo pass@k dựa trên thực thi vượt qua những hạn chế lớn của BLEU và các thước đo dựa trên heuristic khác (Reiter, 2018).

Đánh giá quy mô lớn đã chứng minh hữu ích trong nhiều lĩnh vực xử lý ngôn ngữ tự nhiên (Wang et al., 2019; Kiela et al., 2021; Srivastava et al., 2022; Muennighoff et al., 2022a). Bằng cách tạo ra 18 điểm số (6 ngôn ngữ trên 3 tác vụ) cho 9 mô hình, chúng tôi tiến một bước hướng tới đánh giá quy mô lớn các mô hình mã. Tuy nhiên, chúng tôi thiếu nhiều mô hình có khả năng tạo mã (Black et al., 2021; Fried et al., 2022; Black et al., 2022; Wang & Komatsuzaki, 2021; Biderman et al., 2023b). Công trình tương lai có thể xem xét nhiều mô hình hơn hoặc mở rộng HUMAN EVALPACK sang các ngôn ngữ hoặc tác vụ mới, chẳng hạn như hiệu quả mã (Madaan et al., 2023a; Yetistiren et al., 2022) hoặc phân loại mã (Khan et al., 2023).

6 KẾT LUẬN

Công trình này nghiên cứu huấn luyện và đánh giá các Code LLM tuân theo hướng dẫn. Chúng tôi giới thiệu COMMIT PACK, một bộ dữ liệu 4TB các commit Git bao gồm 350 ngôn ngữ lập trình. Chúng tôi lọc bộ dữ liệu quy mô lớn này để tạo ra COMMIT PACKFT, 2GB mã chất lượng cao với thông điệp commit mô phỏng hướng dẫn. Để cho phép đánh giá toàn diện các mô hình mã hướng dẫn, chúng tôi xây dựng HUMAN EVALPACK, một benchmark do con người viết bao gồm 3 tác vụ khác nhau cho 6 ngôn ngữ lập trình. Chúng tôi nghiên cứu một số bộ dữ liệu hướng dẫn và thấy rằng COMMIT PACKFT kết hợp với dữ liệu ngôn ngữ tự nhiên dẫn đến hiệu suất tốt nhất. Trong khi các mô hình của chúng tôi, OCTOCODER và OCTOGEEX, là các Code LLM được cấp phép cho phép tốt nhất có sẵn, chúng bị vượt trội bởi các mô hình nguồn đóng như GPT-4. Ngoài việc cải thiện mô hình tinh chỉnh theo hướng dẫn, công trình tương lai nên xem xét huấn luyện các mô hình cơ sở có khả năng cao hơn.

--- TRANG 8 ---
OctoPack: Tinh chỉnh theo hướng dẫn các mô hình ngôn ngữ lớn cho mã

LỜI CẢM ƠN

Chúng tôi cảm ơn Hugging Face đã cung cấp các instance tính toán. Chúng tôi vô cùng biết ơn Rodrigo Garcia cho các bản dịch Rust, Dimitry Ageev và Calum Bird vì sự giúp đỡ với đánh giá GPT-4, Loubna Ben Allal vì sự giúp đỡ về đánh giá, Arjun Guha cho các cuộc thảo luận sâu sắc về việc chuỗi các tác vụ đánh giá để tránh đánh giá bằng BLEU, Lewis Tunstall vì sự giúp đỡ về dữ liệu OASST, Victor Sanh và Nadav Timor cho các cuộc thảo luận, Jiaxi Yang vì chỉnh sửa logo và thiết kế prompting phân loại miền, Ghosal et al. (2023); Zeng et al. (2023) vì cảm hứng thiết kế, Harm de Vries vì phản hồi và tất cả thành viên của BigCode vì sự hỗ trợ chung. Cuối cùng, chúng tôi cảm ơn mọi lập trình viên dành thời gian viết các thông điệp commit có thông tin.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo rất dài với 20 trang - bao gồm tất cả các tác giả, tiêu đề và thông tin xuất bản được liệt kê trong nguyên bản]

--- TRANG 9 đến 60 ---
[Nội dung tiếp tục với các phần PHỤ LỤC A đến U, bao gồm các bảng, hình ảnh, ví dụ và chi tiết kỹ thuật bổ sung như trong bản gốc]
