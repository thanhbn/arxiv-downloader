# 2310.08072_vi.txt
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2310.08072.pdf
# Kích thước tệp: 296584 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Huấn luyện Hệ thống Hỏi-Đáp Sinh tạo trên Dữ liệu Tổng hợp Thu được từ
một Mô hình Được tinh chỉnh Hướng dẫn
Kosuke Takahashi, Takahiro Omi, Kosuke Arima
Stockmark
kosuke.takahashi, takahiro.omi, kosuke.arima@stockmark.co.jp
Tatsuya Ishigaki
Viện Khoa học và Công nghệ Công nghiệp Tiên tiến Quốc gia
ishigaki.tatsuya@aist.go.jp
Tóm tắt
Bài báo này trình bày một phương pháp đơn giản và hiệu quả về chi phí
để tổng hợp dữ liệu nhằm huấn luyện các hệ thống hỏi-
đáp. Để huấn luyện, việc tinh chỉnh các mô hình GPT
là một thực hành phổ biến trong các ngôn ngữ giàu tài nguyên
như tiếng Anh, tuy nhiên, điều này trở nên thách thức
đối với các ngôn ngữ không phải tiếng Anh do sự khan hiếm
của các cặp câu hỏi-trả lời (QA) đầy đủ. Các phương pháp
hiện có sử dụng các bộ sinh câu hỏi và trả lời được huấn
luyện trên các cặp QA do con người tác giả, điều này liên quan
đến chi phí nhân lực đáng kể. Ngược lại, chúng tôi sử dụng
một mô hình được tinh chỉnh hướng dẫn để sinh ra các cặp QA
theo cách zero-shot hoặc few-shot. Chúng tôi tiến hành thí
nghiệm để so sánh các chiến lược khác nhau để thu được
các cặp QA từ mô hình được tinh chỉnh hướng dẫn.
Kết quả chứng minh rằng một mô hình được huấn luyện trên
dữ liệu tổng hợp đề xuất của chúng tôi đạt được hiệu suất
tương đương với một mô hình được huấn luyện trên các tập
dữ liệu được tuyển chọn thủ công, mà không phát sinh
chi phí nhân lực.
1 Giới thiệu
Việc tinh chỉnh các mô hình ngôn ngữ lớn (LLM) đã
được chứng minh là hiệu quả để nâng cao các hệ thống hỏi-
đáp (Dong et al., 2019). Tuy nhiên,
việc mở rộng phương pháp này sang các ngôn ngữ khác ngoài
tiếng Anh gặp phải thách thức do sự khan hiếm của
các cặp QA thích hợp để huấn luyện. Trong nghiên cứu này, chúng
tôi đặc biệt nhắm đến tiếng Nhật như một ngôn ngữ không phải
tiếng Anh đại diện. Chúng tôi đề xuất một phương pháp
đơn giản tổng hợp các cặp QA tiếng Nhật sử dụng
một mô hình được tinh chỉnh hướng dẫn.¹
Các nhiệm vụ hỏi-đáp có thể được phân loại
thành hai thiết lập chính: câu hỏi có ngữ cảnh và
không có ngữ cảnh (Kurihara et al., 2022). Trong nghiên cứu
này, chúng tôi tập trung vào thiết lập dựa trên ngữ cảnh như
được hiển thị trong Hình 1. Trong thiết lập này, hệ thống nhận
một câu hỏi cùng với ngữ cảnh đi kèm làm
đầu vào. Mô hình sinh ra một câu trả lời bằng cách sử dụng
¹Các thí nghiệm của chúng tôi sử dụng ChatAPI của OpenAI với
mô hình gpt-3.5-turbo-0613.
Hình 1: Nhiệm vụ của QA sinh tạo nhận thức ngữ cảnh.
thông tin được cung cấp trong ngữ cảnh. Mặt khác,
thiết lập không có ngữ cảnh liên quan đến việc hệ thống
xử lý chỉ câu hỏi làm đầu vào.
Chúng tôi trình bày một phương pháp đơn giản nhưng hiệu quả về chi phí
để sinh ra các cặp câu hỏi-trả lời (QA) tổng hợp. Các hệ thống QA
hiện có được huấn luyện trên các tập dữ liệu do con người
tác giả hoặc các cặp QA được sinh tự động (Sachan và Xing, 2018; Tang
et al., 2018), cả hai đều dẫn đến chi phí lao động cao. Ngược
lại, bài báo này điều tra việc sử dụng một mô hình
được tinh chỉnh hướng dẫn được lấy cảm hứng từ khả năng hợp lý của chúng
để tạo ra tập dữ liệu tổng hợp (Gilardi et al., 2023). Chúng
tôi sử dụng một ngữ cảnh làm đầu vào và sinh ra cả câu hỏi
tương ứng và câu trả lời của nó. Mô hình được tinh chỉnh
hướng dẫn cho phép chúng tôi tạo ra các cặp QA theo cách
zero-shot hoặc few-shot, loại bỏ nhu cầu
tuyển chọn thủ công.
Các thí nghiệm của chúng tôi so sánh các hệ thống hỏi-đáp
được tinh chỉnh trên dữ liệu tổng hợp được sinh ra
thông qua các chiến lược khác nhau. Cụ thể, chúng tôi khám
phá các nguồn ngữ cảnh khác nhau, số lượng
shot được đưa vào mô hình được tinh chỉnh hướng dẫn, và
lượng cặp QA được sinh ra. Việc đánh giá trên
tập dữ liệu đánh giá của JSQuAD (Kurihara et al., 2022)
cung cấp ba phát hiện. Thứ nhất, việc sử dụng các ngữ cảnh
được trích xuất từ một corpus có đặc điểm tương tự
với tập dữ liệu đánh giá mang lại hiệu suất
cải thiện. Thứ hai, chiến lược one-shot vượt trội hơn
phương pháp zero-shot. Cuối cùng, việc sinh ra
ba cặp QA cho mỗi ngữ cảnh hiệu quả hơn
việc sinh ra số lượng cặp QA thấp hơn. Mô hình
hoạt động tốt nhất được tinh chỉnh trên dữ liệu tổng hợp của chúng tôiarXiv:2310.08072v2  [cs.CL]  13 Oct 2023

--- TRANG 2 ---
dữ liệu thể hiện hiệu suất tương đương với các mô hình
được huấn luyện trên dữ liệu được tuyển chọn thủ công.
2 Công trình liên quan
QA hiện có tập trung vào hai thiết lập chính:
"closedQA" có ngữ cảnh và "commonsens-QA"
không có ngữ cảnh (Kurihara et al., 2022). Đối với
thiết lập trước, mà chúng tôi nhắm đến, các hệ thống QA nhận
một câu hỏi cùng với một ngữ cảnh, chẳng hạn như một bài viết
Wikipedia, và sinh ra một câu trả lời. Mặt khác,
trong thiết lập sau, các hệ thống chỉ nhận một câu
hỏi làm đầu vào.
Có hai loại hệ thống QA: trích xuất
và sinh tạo. Các phương pháp trích xuất trích xuất
một câu trả lời như nó có trong ngữ cảnh bởi các mô hình như
BERT (Rajpurkar et al., 2016), trong khi các phương pháp sinh tạo
thường sử dụng các biểu thức không có trong
ngữ cảnh bởi các mô hình như T5 (Raffel et al., 2020)
hoặc GPT (Brown et al., 2020). Trọng tâm của chúng tôi là
phương pháp sau.
Trong khi có một số tập dữ liệu được tạo thủ công trong
tiếng Anh, chẳng hạn như SQuAD (Rajpurkar et al., 2016)
và QuALITY (Pang et al., 2022), những tài nguyên này
không áp dụng trực tiếp cho ngôn ngữ Nhật Bản.
Đối với tiếng Nhật, JSQuAD (Kurihara et al., 2022) và
JAQKET2 có sẵn. Chúng tôi sử dụng JSQuAD³ vì
dữ liệu đánh giá của JAQKET không công khai.
Các nghiên cứu hiện có tổng hợp các cặp QA bằng hai
phương pháp chính: có giám sát (Lee et al., 2020;
Sachan và Xing, 2018; Tang et al., 2018) và
không giám sát (Puri et al., 2020). Các phương pháp
có giám sát huấn luyện các bộ sinh câu hỏi-trả lời sử dụng
các tập dữ liệu được tạo thủ công. Phương pháp của chúng tôi sinh ra
các cặp QA từ ngữ cảnh theo cách zero-shot hoặc few-shot,
loại bỏ nhu cầu huấn luyện các bộ sinh.
Trong phương pháp không giám sát, Puri et al. (2020)
sử dụng một bộ nhận dạng thực thể có tên (NER) để trích xuất
ứng cử viên câu trả lời trong khi phương pháp của chúng tôi chỉ sử dụng
một mô hình được tinh chỉnh hướng dẫn từ đầu đến cuối và không
yêu cầu NER.
3 Tổng hợp các cặp QA
Chúng tôi mô tả phương pháp của chúng tôi trong phần này.
²https://www.nlp.ecei.tohoku.ac.jp/projects/
jaqket/#Reference
³Nghiêm khắc, JSQuAD không phải để đánh giá QA sinh tạo, mà
là thiết lập dựa trên trích xuất span. Chúng tôi sử dụng dữ liệu này vì
không có dữ liệu đánh giá chung trong tiếng Nhật cho QA sinh tạo.
Các mô hình của chúng tôi sinh ra câu trả lời không trích xuất span, do đó, chúng
tôi cũng tiến hành đánh giá bằng con người.Dựa trên các văn bản đã cho, vui lòng tạo một cặp
câu hỏi và trả lời có thể trả lời được.
Vui lòng tạo câu trả lời bằng ngôn ngữ lịch sự
tiếng Nhật.
Vui lòng phản hồi theo định dạng JSON.
## ví dụ
texts:"văn bản để trích xuất cặp câu hỏi và
trả lời"
output: {"Question":"câu hỏi có thể được
trả lời từ các văn bản", "Answer":"câu trả lời cho
câu hỏi"}
## đầu vào
texts: {QA context}
output:
Hình 2: Một ví dụ về prompt zero-shot để sinh ra một
cặp QA.
texts:"Giải quyết nợ kỹ thuật là khó khăn; chúng ta nhìn
vào thách thức của JAL...(bỏ qua)...Khẩu hiệu của JAL là
Go To Cloud...(bỏ qua),
output: {"Question":"Japan Airlines đại diện cho những khẩu hiệu nào?", "Answer":"Khẩu hiệu của JAL là Go To Cloud."}
Hình 3: Một mẫu đã dịch của phần "## example" trong
prompt one-shot. Lưu ý rằng bản gốc là tiếng Nhật.
3.1 Ngữ cảnh Nguồn và Lọc
Chúng tôi sinh ra N cặp câu hỏi-trả lời từ mỗi
ngữ cảnh. N được đặt là một hoặc ba trong các thí nghiệm của chúng tôi.
Chúng tôi so sánh ba nguồn ngữ cảnh cụ thể: 1)
một mẫu ngẫu nhiên của 6.000 bài viết Wikipedia tiếng Nhật
(wiki), 2) một mẫu ngẫu nhiên của 6.000 bài viết tin tức
(news), và 3) các ngữ cảnh trong tập dữ liệu huấn luyện
của JSQuAD (JSQuAD). Để thu thập các bài viết tin tức, chúng
tôi đã thu thập các bài viết được truy cập nhiều nhất từ một công cụ tìm kiếm⁴
trong giai đoạn từ tháng 5 năm 2022 đến tháng 5
năm 2023. Chúng tôi giới hạn mỗi ngữ cảnh trong 300 ký tự
đầu tiên trước khi sinh ra các cặp QA bằng mô hình
được tinh chỉnh hướng dẫn.
3.2 Prompt để Sinh ra các cặp QA
Chúng tôi cung cấp các ví dụ về prompt zero-shot và one-shot
với thiết lập N = 1 trong Hình 2 và
Hình 3, tương ứng. Những prompt này nhằm sinh
⁴URL của engine/dataset được ẩn để bảo vệ
tính ẩn danh của tác giả, và sẽ được hiển thị sau khi chấp nhận

--- TRANG 3 ---
ra các cặp QA từ một ngữ cảnh. Trong prompt zero-shot, 
chúng tôi đầu tiên trình bày các hướng dẫn nhiệm vụ, tiếp theo
là giải thích về cấu trúc về cách một văn bản đầu vào được
biểu diễn, và cấu trúc JSON đầu ra mong muốn của chúng như
được hiển thị trong phần "## example". Đối với thiết lập N > 1,
chúng tôi sửa đổi ví dụ của cấu trúc JSON để bao gồm nhiều
cặp QA hơn. Sau đó, chúng tôi viết một văn bản đầu vào trong
phần "## input". Trong thiết lập prompt zero-shot,
chúng tôi chỉ viết định dạng của cấu trúc đầu vào và đầu ra,
mà không bao gồm các văn bản thực tế hoặc các cặp
câu hỏi-trả lời mong đợi tương ứng với ngữ cảnh.
Mặt khác, trong prompt one-shot, chúng tôi thay thế phần
"## example" trong 2 bằng prompt được hiển thị trong
Hình 3. Không giống như prompt zero-shot, prompt one-shot
bao gồm các ngữ cảnh ví dụ thực tế và các cặp QA
mong đợi tương ứng của chúng. Để hiểu rõ hơn về tác động
của kỹ thuật prompt, chúng tôi so sánh hai prompt này trong
các thí nghiệm của chúng tôi. Các tuple của ngữ cảnh và các cặp QA
được sinh ra được sử dụng để tinh chỉnh một GPT bằng prompt
được hiển thị trong Hình 4.
4 Thí nghiệm
Tập dữ liệu Đánh giá và Các mô hình So sánh: Chúng
tôi sử dụng JSQuAD (Kurihara et al., 2022) để đánh giá.
Dữ liệu đánh giá này chứa 4.470 cặp QA do con người
tác giả được cho các bài viết Wikipedia làm ngữ cảnh.
Chúng tôi sử dụng toàn bộ dữ liệu đánh giá cho đánh giá
tự động trong khi 500 trường hợp được lấy mẫu ngẫu nhiên
được sử dụng cho đánh giá thủ công.
Chúng tôi tiến hành so sánh toàn diện bằng cách
khám phá các kết hợp khác nhau của ngữ cảnh, số lượng
cặp QA được sinh ra được ký hiệu là N và
prompt. Về ngữ cảnh, chúng tôi xem xét ba lựa chọn:
wiki, news, JSQuAD, và, như được chi tiết trong
Sec. 3.1. Đối với N, chúng tôi so sánh N = 1 và N = 3.
Chúng tôi so sánh prompt zero-shot và one-shot⁵.
Các mô hình đề xuất của chúng tôi được so sánh với hai
mô hình: 1) một mô hình GPT thuần túy không có tinh chỉnh
và 2) một mô hình được tinh chỉnh trên các cặp QA từ
tập dữ liệu huấn luyện JSQuAD (Human), trong đó các cặp QA
này do con người tác giả trong khi các cặp QA đề xuất của chúng tôi
không do con người tác giả.
Tinh chỉnh Chúng tôi sử dụng các cặp QA tổng hợp để
tinh chỉnh phiên bản tiếng Nhật của GPT-NeoX (Black
et al., 2022)⁶. Để đạt được tốc độ cải thiện, chúng tôi sử
⁵Chúng tôi bị hạn chế ở one-shot do giới hạn độ dài đầu vào
của ChatGPT.
⁶https://huggingface.co/cyberagent/
open-calm-7b## Hướng dẫn
{QUESTION}
## Ngữ cảnh
{CONTEXT}
## Phản hồi
Hình 4: Prompt để sinh ra câu trả lời với GPT-NeoX
được tinh chỉnh.
Batch Size: {4, 8},
Learning Rate: {0.00001, 0.00005, 0.000001},
Epoch: {3, 4, 5,}, r: {4, 8, 16, 64, 128}, α: {1, 4, 16}
Bảng 1: Các giá trị phạm vi tìm kiếm trong tinh chỉnh LoRA.
dụng tinh chỉnh LoRA (Hu et al., 2022). Trong việc sinh ra
câu trả lời, chúng tôi sử dụng prompt trong thiết lập zero-shot
(Hình 4).
Metrics: Để đánh giá tự động, chúng tôi sử dụng
BERTScore (Zhang et al., 2020) và BLEU (Papineni et al., 2002). BERTScore được triển khai
riêng của chúng tôi với một mô hình BERT tiếng Nhật.⁷ Về
BLEU, thư viện SacreBLEU (Post, 2018) được sử dụng.
Những metrics tự động này có thể không trực tiếp nắm
bắt được tính đúng đắn của một câu trả lời cho một câu hỏi
đã cho. Để giải quyết điều này, chúng tôi cũng tiến hành đánh giá
thủ công bởi các giám khảo con người. Chúng tôi yêu cầu bốn giám khảo,
là các chuyên gia về xử lý ngôn ngữ tự nhiên hoặc ngôn
ngữ học, đánh giá xem câu trả lời được sinh ra
có đúng hay không. Chúng tôi đã hiển thị các tuple của câu hỏi,
câu trả lời và ngữ cảnh cho các giám khảo. Chúng tôi báo cáo
độ chính xác thu được từ đánh giá thủ công.
Tham số Chúng tôi đã tiến hành tìm kiếm lưới để điều chỉnh
các tham số: batch size, learning rate, số lượng
epoch, cũng như các siêu tham số của LoRA (cụ thể
là α và r). Phạm vi giá trị được khám phá trong
quá trình tìm kiếm này được cung cấp trong Bảng 1. Sau đó,
mô hình đạt được BERTScore cao nhất được
chọn để đánh giá.
5 Kết quả
Trong phần này, chúng tôi trình bày kết quả trên JSQuAD.
5.1 Đánh giá Tự động
Mối quan tâm chính của chúng tôi nằm trong việc kiểm tra tác động của
mỗi chiến lược tổng hợp các cặp QA đối với hiệu suất
của nhiệm vụ hỏi đáp downstream. Cụ thể, chúng tôi tập trung vào
các so sánh liên quan đến các ngữ cảnh khác nhau, prompt và lượng
các cặp QA được sinh ra tự động.
⁷https://huggingface.co/cl-tohoku/
bert-base-japanese-v3

--- TRANG 4 ---
Bảng 2 trình bày điểm số của BERTScore và
BLEU thu được bằng cách thay đổi các ngữ cảnh trong khi
giữ các thiết lập khác, tức là N và prompt được
cố định. Bảng được chia thành năm phần. Bắt đầu
từ trên cùng, phần đầu tiên hiển thị điểm số
cho các mô hình QA được huấn luyện trên các cặp QA do con người
tác giả (Human) từ tập dữ liệu huấn luyện JSQuAD,
cùng với mô hình GPT thuần túy (GPT) không có tinh
chỉnh. Phần thứ hai và thứ ba thể hiện điểm số
thu được khi N được cố định ở một, nhưng chúng tôi
thay đổi các prompt thành zero-shot và one-shot. Phần
thứ tư và thứ năm đại diện cho điểm số khi chúng tôi
sử dụng N = 3.
Tác động của Ngữ cảnh đối với Hiệu suất: Chúng tôi quan
sát thấy rằng việc sử dụng các ngữ cảnh được trích xuất từ
tập dữ liệu tin tức mang lại điểm số tương đối thấp, ví dụ,
0.713 và 0.747 về BERTScore cho các thiết lập zero-shot và
one-shot với N = 3, tương ứng. Ngữ cảnh wiki hoạt động
tốt hơn (0.706 và 0.838) so với news (0.713 và 0.747)
cho cùng các thiết lập. Đáng chú ý, ngữ cảnh JSQuAD đạt
được BERTScore cao nhất là 0.863 và 0.889 với N = 1 và
N = 3, tương ứng. Kết quả cho thấy rằng việc sử
dụng Wikipedia làm ngữ cảnh mang lại lợi thế,
có thể vì dữ liệu đánh giá JSQuAD cũng được
lấy từ Wikipedia.
Tác động của Prompt đối với Hiệu suất: Prompt one-shot
hiệu quả hơn. Như được hiển thị trong Bảng
2, mô hình được tinh chỉnh trên các cặp QA zero-shot
(N = 1) được sinh ra từ các ngữ cảnh trong tập dữ liệu
huấn luyện JSQuAD đạt được BERTScore là 0.724.
Tuy nhiên, các prompt one-shot với N = 1 thể
hiện một mức tăng hiệu suất đáng kể, đạt
BERTScore là 0.863.
Tác động của Số lượng Cặp QA được Sinh ra đối với
Hiệu suất: Khi chúng tôi tăng số lượng cặp QA
cho ngữ cảnh, có một mức tăng 2.6 điểm trong
BERTScore (từ 0.863 lên 0.889). Đáng chú ý,
BERTScore đạt được là 0.889 tương đương
với mô hình được huấn luyện trên các cặp QA do con người
tác giả (0.899), mặc dù phương pháp của chúng tôi không sử dụng
bất kỳ cặp QA nào do con người tác giả.
5.2 Đánh giá bởi Giám khảo Con người:
Chúng tôi trình bày kết quả của đánh giá thủ công.
Bảng 3 hiển thị các so sánh giữa ba đầu
ra: câu trả lời được sinh ra bởi 1) mô hình hoạt động tốt nhất
của chúng tôi (JSQuAD (N = 3), và prompt one-shot) và
2) một mô hình được tinh chỉnh trên các cặp QA do con người tác giảngữ cảnh N prompt BERTscore BLEU
Human - - 0.899 5.64
GPT - - 0.601 0.00
news 1 zero 0.697 0.02
wiki 1 zero 0.713 0.03
JSQuAD 1 zero 0.724 1.55
news 1 one 0.738 0.11
wiki 1 one 0.775 0.09
JSQuAD 1 one 0.863 4.83
news 3 zero 0.713 0.38
wiki 3 zero 0.706 0.23
JSQuAD 3 zero 0.740 1.85
news 3 one 0.747 1.25
wiki 3 one 0.838 1.66
JSQuAD 3 one 0.889 6.77
Bảng 2: Hiệu suất trên các ngữ cảnh và số lượng
cặp QA được sinh ra khác nhau.
Cặp QA Độ chính xác (%)
JSQuAD (N = 3, prompt one-shot) 45.4
Human 38.4
Gold 90.4
Bảng 3: Độ chính xác được tính là số lượng tuple
câu hỏi-ngữ cảnh-câu trả lời đúng chia cho tổng số 500
trường hợp đánh giá.
cặp QA từ tập dữ liệu huấn luyện JSQuAD, và
3) câu trả lời gold trong tập dữ liệu đánh giá JSQuAD.
Đáng chú ý, mặc dù phương pháp của chúng tôi không sử dụng
bất kỳ cặp QA nào do con người tác giả, độ chính xác đạt được
là 45.4% trong khi mô hình được tinh chỉnh trên
các cặp QA do con người tác giả chỉ đạt được 38.4%
về độ chính xác. Gilardi et al. (2023) đề
cập rằng chú thích tự động với một mô hình instructor-
tuning có chất lượng cao hơn các chú thích
của crowd-worker, và kết quả của chúng tôi phù hợp
với tuyên bố của họ. Lưu ý rằng hiệu suất của
cả hai mô hình được tinh chỉnh đều tụt hậu đáng kể so với
tiêu chuẩn Gold (90.4%), cho thấy vẫn còn nhiều chỗ
để cải thiện.
6 Kết luận
Bài báo này đề xuất sử dụng một mô hình được tinh chỉnh hướng dẫn
để tổng hợp các cặp QA. Kết quả thí nghiệm
của chúng tôi chứng minh rằng các mô hình được huấn luyện trên
các cặp QA được sinh ra tự động đạt được hiệu suất
tương đương hoặc thậm chí vượt trội so với
mô hình được tinh chỉnh được huấn luyện trên các cặp QA do con người
tác giả. Trong các nghiên cứu tương lai, chúng tôi dự định khám phá
mối quan hệ giữa sự đa dạng của các cặp QA được sinh ra tự
động và tác động của chúng đối với hiệu suất
của các nhiệm vụ QA downstream.

--- TRANG 5 ---
Tài liệu tham khảo
Sidney Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace
He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-
hit, Laria Reynolds, Jonathan Tow, Ben Wang, và
Samuel Weinbach. 2022. GPT-NeoX-20B: An open-
source autoregressive language model. Trong Proceed-
ings of BigScience Episode #5 – Workshop on Chal-
lenges & Perspectives in Creating Large Language
Models, trang 95–136, virtual+Dublin. Association
for Computational Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, và Dario Amodei. 2020.
Language models are few-shot learners. Trong Ad-
vances in Neural Information Processing Systems,
volume 33, trang 1877–1901. Curran Associates,
Inc.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
và Hsiao-Wuen Hon. 2019. Unified language model
pre-training for natural language understanding and
generation. Trong Advances in Neural Information Pro-
cessing Systems, volume 32. Curran Associates, Inc.
Fabrizio Gilardi, Meysam Alizadeh, và Maël Kubli.
2023. Chatgpt outperforms crowd workers for
text-annotation tasks. Proceedings of the National
Academy of Sciences, 120(30):e2305016120.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. Trong International Conference on
Learning Representations.
Kentaro Kurihara, Daisuke Kawahara, và Tomohide
Shibata. 2022. JGLUE: Japanese general language
understanding evaluation. Trong Proceedings of the Thir-
teenth Language Resources and Evaluation Confer-
ence, trang 2957–2966, Marseille, France. European
Language Resources Association.
Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Dongh-
wan Kim, và Sung Ju Hwang. 2020. Gener-
ating diverse and consistent QA pairs from con-
texts with information-maximizing hierarchical con-
ditional VAEs. Trong Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, trang 208–224, Online. Association for
Computational Linguistics.
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,
Nikita Nangia, Jason Phang, Angelica Chen, VishakhPadmakumar, Johnny Ma, Jana Thompson, He He,
và Samuel Bowman. 2022. QuALITY: Question
answering with long input texts, yes! Trong Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, trang 5336–5358,
Seattle, United States. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, và Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. Trong Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, trang 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. Trong Proceedings of the Third Conference on
Machine Translation: Research Papers, trang 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Raul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa
Patwary, và Bryan Catanzaro. 2020. Training
question answering models from synthetic data. Trong
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
trang 5811–5826, Online. Association for Computa-
tional Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. Trong Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, trang 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Mrinmaya Sachan và Eric Xing. 2018. Self-training
for jointly learning to ask and answer questions. Trong
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), trang 629–640, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo
Sun, Shujie Liu, Yuanhua Lv, và Ming Zhou. 2018.
Learning to collaborate for question answering and
asking. Trong Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), trang 1564–1574,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

--- TRANG 6 ---
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, và Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. Trong International
Conference on Learning Representations.
