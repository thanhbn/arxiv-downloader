# 2309.05447.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2309.05447.pdf
# Kích thước tệp: 854259 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
DoG-Instruct: Hướng tới Dữ liệu Điều chỉnh Hướng dẫn Cao cấp thông qua
Việc Bao bọc Hướng dẫn dựa trên Văn bản
Yongrui Chen1,2, Haiyun Jiang3,∗, Xinting Huang3, Shuming Shi3& Guilin Qi1,2†
1Southeast University
2Key Laboratory of New Generation Artificial Intelligence Technology
and Its Interdisciplinary Applications (Southeast University), Ministry of Education
{yrchen,gqi}@seu.edu.cn
3Tencent AI Lab
{haiyunjiang,jeffjhhuang,shumingshi}@tencent.com
Tóm tắt
Việc cải thiện khả năng tuân theo hướng dẫn của các LLM phụ thuộc rất nhiều vào tính khả dụng của các cặp hướng dẫn-phản hồi chất lượng cao. Thật không may, các phương pháp hiện tại được sử dụng để thu thập các cặp này gặp phải hoặc chi phí lao động không thể chi trả được hoặc ảo giác nghiêm trọng trong việc tự tạo ra của LLM. Để giải quyết những thách thức này, bài báo này đề xuất một giải pháp có thể mở rộng. Nó bao gồm việc huấn luyện các LLM để tạo ra các cặp hướng dẫn-phản hồi dựa trên các tài liệu do con người viết, thay vì chỉ dựa vào việc tự tạo ra mà không có ngữ cảnh. Phương pháp được đề xuất của chúng tôi không chỉ khai thác các ưu điểm của các tài liệu do con người viết trong việc giảm ảo giác mà còn sử dụng một LLM để bao bọc cách biểu đạt của các tài liệu, điều này cho phép chúng tôi thu hẹp khoảng cách giữa các phong cách tài liệu khác nhau và phản hồi AI tiêu chuẩn. Các thí nghiệm chứng minh rằng phương pháp của chúng tôi vượt trội hơn các phương pháp điển hình hiện có trên nhiều bộ chuẩn. Đặc biệt, so với đường cơ sở hoạt động tốt nhất, LLM được huấn luyện bằng bộ dữ liệu được tạo ra của chúng tôi thể hiện sự cải thiện tương đối 10% về hiệu suất trên AlpacaEval, mặc dù chỉ sử dụng 1/5 dữ liệu huấn luyện của nó. Hơn nữa, một đánh giá thủ công toàn diện xác nhận chất lượng của dữ liệu chúng tôi đã tạo ra. Bộ bao bọc được huấn luyện của chúng tôi có sẵn công khai1.

1 Giới thiệu
Những nỗ lực gần đây trong cộng đồng NLP đã tập trung vào việc điều chỉnh hướng dẫn (Sanh et al., 2022; Mishra et al., 2022; Wei et al., 2022), tức là cải thiện khả năng của các mô hình ngôn ngữ lớn (LLM) để hiểu và tuân theo hướng dẫn (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a). Các LLM tiên tiến đã được huấn luyện để có khả năng tạo ra các đầu ra tùy chỉnh khi được cung cấp các hướng dẫn cụ thể (với đầu vào), cho phép chúng thích ứng với các nhiệm vụ mới mà không cần tiếp xúc trước đó.

Là một vấn đề quan trọng trong việc cải thiện khả năng tuân theo hướng dẫn của LLM, cách thu thập các cặp hướng dẫn-phản hồi chất lượng cao đang ngày càng trở nên phổ biến. Phần lớn các phương pháp hiện có hoặc dựa vào việc thuê các chuyên gia để viết hướng dẫn cho các nhiệm vụ NLP khác nhau (Wang et al., 2022; Conover et al., 2023) hoặc thúc đẩy việc sử dụng LLM để tự động tạo ra hướng dẫn (Wang et al., 2023; Taori et al., 2023; Yin et al., 2023). Thật không may, những phương pháp này có những hạn chế hoặc về tính mở rộng do bản chất tốn nhiều lao động của quá trình chú thích hoặc về chất lượng dữ liệu do vấn đề ảo giác (Zhang et al., 2023; Zheng et al., 2023) liên quan đến LLM.

Nghiên cứu gần đây (Köksal et al., 2023; Li et al., 2023a) đã cung cấp một ý tưởng tiềm năng hơn: đầu tiên trực tiếp sử dụng các tài liệu do con người viết làm phản hồi điển hình và sau đó sử dụng LLM để dự đoán các hướng dẫn tiềm ẩn của người dùng. Phương pháp này, được gọi là dịch ngược hướng dẫn (Li et al., 2023a), dựa trên niềm tin rằng các tài liệu do con người viết vốn ít bị ảo giác hơn so với các phản hồi được tạo ra hoàn toàn bởi LLM.

Tuy nhiên, chúng tôi cho rằng ngay cả khi một tài liệu không có ảo giác, việc sử dụng nó trực tiếp làm phản hồi điển hình không phải lúc nào cũng phù hợp. Điều này được quy cho hai lý do chính: a) Đầu tiên, không phải tất cả các phần của tài liệu đều có giá trị trong việc xây dựng phản hồi. Ví dụ, phần màu đỏ của tài liệu ( A) trong Hình 1 hoàn toàn vô dụng cho việc dịch ngược hướng dẫn kết quả (hộp vàng). Hơn nữa, các phần có giá trị của tài liệu thường có ranh giới mờ nhạt. Ví dụ, văn bản màu đỏ của ( B) nhằm tạo ra một bầu không khí căng thẳng, một lần nữa không phù hợp để giữ lại trong phản hồi, nhưng nó cũng có một số liên quan đến chủ đề (nghiên cứu về người ngoài hành tinh) và do đó khó có thể được lọc ra bằng tiền xử lý đơn giản. b) Thứ hai, do các mục đích viết khác nhau, thường có những khoảng cách trong cách biểu đạt giữa các tài liệu thô và các phản hồi tiêu chuẩn. Để minh họa, phần màu đỏ của ( C) chứa nhiều mô tả chủ quan, điều này lệch khỏi tính khách quan mong đợi của một trợ lý AI.

Trong bài báo này, chúng tôi đề xuất một mô hình mới để xây dựng dữ liệu điều chỉnh hướng dẫn, được gọi là bao bọc hướng dẫn. Nó nhằm huấn luyện một LLM mã nguồn mở để xác định các phần có giá trị từ tài liệu gốc và tiếp tục biến đổi chúng thành các cặp hướng dẫn-phản hồi trôi chảy và khách quan.

Tóm lược, phương pháp được đề xuất của chúng tôi bao gồm hai giai đoạn như được hiển thị trong Hình 2. Trong giai đoạn a), một LLM được căn chỉnh tốt được sử dụng làm giáo viên để xây dựng một tập huấn luyện meta Ω cho việc bao bọc hướng dẫn. Mỗi ví dụ trong tập này bao gồm một tài liệu được lấy mẫu và cặp hướng dẫn-phản hồi tương ứng của nó, bao gồm một trong hai quan điểm sau. Trong quan điểm căn chỉnh, chúng tôi sử dụng học trong ngữ cảnh để hướng dẫn LLM giáo viên tạo ra các cặp hướng dẫn-phản hồi dựa trên các tài liệu do con người viết. Điều này cho phép thích ứng LLM giáo viên với các phong cách tài liệu thực tế khác nhau. Trong quan điểm đa dạng, chúng tôi bắt đầu với một tập hướng dẫn đa dạng hiện có và nhắc LLM giáo viên tạo ra một tài liệu giả cho mỗi cặp hướng dẫn-phản hồi một cách ngược lại. Điều này đảm bảo các ví dụ huấn luyện duy trì tính đa dạng của hướng dẫn. Sau đó, chúng tôi sử dụng tập huấn luyện meta để thực hiện tinh chỉnh có giám sát trên một LLM được phát hành công khai, tạo thành bộ bao bọc hướng dẫn của chúng tôi. Trong giai đoạn b), các tài liệu do con người viết từ nhiều lĩnh vực được đưa vào bộ bao bọc được huấn luyện của chúng tôi để tạo ra các cặp hướng dẫn-phản hồi. Sau đó, một chiến lược hậu xử lý đơn giản nhưng hiệu quả được áp dụng để lọc các ví dụ không hợp lệ dựa trên độ tương tự theo nghĩa đen. Cuối cùng, chúng tôi đặt tên cho bộ dữ liệu kết quả là Hướng dẫn dựa trên Tài liệu (DOG-INSTRUCT), chứa 12.4K cặp hướng dẫn-phản hồi.

LLM được huấn luyện bằng DOG-INSTRUCT đạt được sự cải thiện đáng kể 4.8% về hiệu suất trên AlpacaEval so với đường cơ sở hoạt động tốt nhất, trong khi chỉ sử dụng 1/5 dữ liệu huấn luyện. Hơn nữa, nó đạt được kết quả tốt nhất trên ba bộ chuẩn được sử dụng rộng rãi khác. Thông qua đánh giá thủ công sâu hơn, chúng tôi minh họa rằng phương pháp DOG-INSTRUCT của chúng tôi hiệu quả giảm thiểu vấn đề ảo giác trong khi căn chỉnh tài liệu thô với phản hồi mong muốn về mặt phong cách. Tóm lại, những đóng góp của bài báo này bao gồm:

• Chúng tôi đề xuất một mô hình mới huấn luyện LLM để tạo ra các cặp hướng dẫn-phản hồi dựa trên các tài liệu do con người viết. Nó không chỉ tận dụng tài liệu để giảm ảo giác của phản hồi, mà còn căn chỉnh phong cách của tài liệu thô với phản hồi lý tưởng bằng LLM.

• Chúng tôi phát hành một bộ bao bọc hướng dẫn dựa trên LLAMA được huấn luyện tốt có khả năng tạo ra một cách nhất quán các cặp hướng dẫn-phản hồi chất lượng cao cho các tài liệu trong nhiều lĩnh vực.

• Chúng tôi tiến hành một đánh giá toàn diện, cả tự động và thủ công, chứng minh rằng LLM được huấn luyện bằng dữ liệu được tạo ra của chúng tôi vượt trội hơn tất cả các đường cơ sở được so sánh.

2 Công thức hóa Vấn đề
Cho một tập các tài liệu {D1,D2, ...,Dn}, trong đó mỗi Di là một tài liệu do con người viết, mục tiêu của chúng tôi là xây dựng một tập các cặp {(X1,Y1), ...,(Xm,Ym)}, trong đó m≤n, Xi và Yi lần lượt biểu thị hướng dẫn và phản hồi, và (Xi,Yi) :=M(Dj). Ở đây M là một bộ bao bọc hướng dẫn dựa trên LLM biến đổi Di thành một cặp hướng dẫn-phản hồi.

3 Thu thập Dữ liệu DoG-Instruct
Hình 2 hiển thị toàn bộ quá trình của phương pháp chúng tôi. a) Đầu tiên, bộ bao bọc hướng dẫn M được huấn luyện bằng tập huấn luyện meta Ω, được xây dựng bởi GPT-4 được căn chỉnh tốt. b) Sau đó, M lấy các tài liệu được lấy mẫu D làm đầu vào để tạo ra (X,Y) cho việc xây dựng DOG-INSTRUCT.

3.1 Corpus & Lấy mẫu Tài liệu
Để tạo ra một tập hợp đa dạng các tài liệu, chúng tôi sử dụng corpus Pile (Gao et al., 2021), là một bộ sưu tập đa lĩnh vực các tài liệu do con người viết. Từ Pile, chúng tôi lấy mẫu các tài liệu từ sáu lĩnh vực khác nhau: ArXiv, FreeLaw, StackExchange, Wikipedia, Github. Theo các công trình hiện có (Li et al., 2023a; Köksal et al., 2023), chúng tôi cũng lấy mẫu các tài liệu từ Open Assistant1 và WikiHow2 để giới thiệu một số ví dụ có cấu trúc. Chúng tôi ngẫu nhiên chọn một số đoạn văn liên tiếp từ mỗi văn bản gốc trong corpus để làm tài liệu của chúng tôi. Để đảm bảo rằng mỗi tài liệu chứa đủ thông tin để tạo ra ít nhất một cặp hướng dẫn-phản hồi, chúng tôi chỉ giữ lại các tài liệu có chứa từ 500 đến 1000 token.

3.2 Xây dựng Bộ bao bọc Hướng dẫn
Để trang bị cho một LLM tổng quát khả năng bao bọc hướng dẫn, chúng tôi cần xây dựng đủ ví dụ huấn luyện ánh xạ tài liệu D thành cặp hướng dẫn-phản hồi (X,Y). Lấy cảm hứng từ (Li et al., 2023a), chúng tôi để GPT-4 được căn chỉnh tốt (OpenAI, 2023) đảm nhận công việc này để giảm thiểu chi phí chú thích của con người. Chúng tôi giả thuyết rằng một tập huấn luyện meta lý tưởng Ω nên đáp ứng hai yêu cầu thiết yếu: căn chỉnh và đa dạng. Căn chỉnh đảm bảo rằng Ω bao gồm một phạm vi rộng các tài liệu thực do con người viết, cho phép bộ bao bọc hiểu các lĩnh vực và phong cách viết khác nhau. Mặt khác, Đa dạng đảm bảo rằng Ω chứa một loạt các hướng dẫn, cho phép bộ bao bọc tạo ra các hướng dẫn đa dạng một cách hiệu quả sau khi huấn luyện. Do đó, chúng tôi thu thập các ví dụ của Ω từ hai quan điểm sau.

Ví dụ từ Quan điểm Căn chỉnh. Trong phần này, các ví dụ được xây dựng bằng cách sử dụng GPT-4 để trực tiếp tạo ra các cặp hướng dẫn-phản hồi cho các tài liệu thực do con người viết. Để hoàn thành mục tiêu này, chúng tôi khai thác sức mạnh của học trong ngữ cảnh (ICL). Cụ thể, đối với mỗi lĩnh vực, 30 ví dụ đầu tiên được xây dựng thủ công làm hạt giống. Sau đó, đối với mỗi D, lời nhắc được đưa vào GPT-4 được ký hiệu bởi (X∗,D1,P1, ...,Dk,Pk,D), trong đó X∗ là định nghĩa của việc ánh xạ Dj thành cặp hướng dẫn-phản hồi Pj= (Xj,Yj). Xem Phụ lục A.1 cho lời nhắc đầy đủ. Các ví dụ kết quả được ký hiệu bởi Ωa.

Ví dụ từ Quan điểm Đa dạng. Một cách trực quan, rất khó để tạo ra các hướng dẫn đa dạng chỉ sử dụng vài chục hạt giống thủ công. Do đó, chúng tôi bắt đầu từ các hướng dẫn được phát hành công khai, chẳng hạn như ALPACA, và sau đó nhắc LLM giáo viên tạo ra một tài liệu giả cho mỗi cặp hướng dẫn-phản hồi một cách ngược lại. Cụ thể, đối với mỗi cặp hướng dẫn-phản hồi (X,Y) được lấy mẫu trong ALPACA, chúng tôi viết lời nhắc để sử dụng GPT-4 tích hợp X và Y thành một tài liệu mới D̃. Chúng tôi cho phép D̃ bao gồm tất cả nội dung từ X và Y, nhưng chúng tôi cố ý làm mờ ranh giới của chúng. Điều này cho phép thêm nội dung mới khi cần thiết, trong khi đảm bảo luồng thông tin mượt mà và mạch lạc. Những tài liệu giả D̃ này và (X,Y) tương ứng của chúng tạo thành các ví dụ huấn luyện còn lại, được ký hiệu bởi Ωd. Phụ lục A.2 cung cấp lời nhắc chi tiết.

Huấn luyện Bộ bao bọc. chúng tôi chọn LLAMA 2 (Touvron et al., 2023b), một LLM tiên tiến có sẵn công khai làm bộ bao bọc hướng dẫn M của chúng tôi và thực hiện tinh chỉnh có giám sát (SFT) trên M bằng tập huấn luyện meta được xây dựng Ω = Ωa∪Ωd. Đối với mỗi tài liệu D và cặp hướng dẫn-phản hồi P= (X,Y) của Ω, chúng tôi thêm một hướng dẫn thống nhất U="Chuyển đổi văn bản đã cho thành một nhiệm vụ. Đầu vào là một văn bản và Phản hồi chứa hai trường: #instruction# và #output#.". Sau đó, tổn thất huấn luyện được tính bằng log-likelihood,

L(U,D,P) =−|P|∑j=1logP(tj|U,D, t<j), (1)

trong đó tj là token thứ j của T. Điều quan trọng cần nhấn mạnh là mặc dù tập huấn luyện meta Ω của chúng tôi có thể bao gồm ảo giác, chúng tôi giả thuyết rằng điều này không ảnh hưởng đến việc học của bộ bao bọc M. Điều này là do mục tiêu chính của chúng tôi đối với M là học cách biến đổi phong cách từ tài liệu thành các cặp hướng dẫn-phản hồi với tính nhất quán về ngữ nghĩa. Trong giai đoạn suy luận, chúng tôi chỉ sử dụng các tài liệu thực do con người viết mà không có tài liệu giả, điều này tự nhiên giảm sự xuất hiện của ảo giác.

3.3 Tạo Dữ liệu thông qua Bộ bao bọc Hướng dẫn
Trong giai đoạn này, chúng tôi sử dụng M được huấn luyện để tạo ra các cặp hướng dẫn-phản hồi cho 20.000 tài liệu do con người viết, đã được lấy mẫu bằng phương pháp được mô tả trong Mục 3.1. Để tránh ảo giác mà bộ bao bọc tạo ra quá nhiều nội dung không liên quan đến văn bản gốc, chúng tôi đề xuất một chiến lược hậu xử lý cho mỗi nhiệm vụ được tạo ra Ti. Cụ thể, chúng tôi thiết kế một điểm số σ(Ti) = min(σ̃(Pi,Xi),σ̃(Pi,Yi)) để đo độ tương tự giữa văn bản và cặp hướng dẫn-phản hồi, trong đó σ̃(Pi, s) =|t(Di)&t(s)|/|t(s)| và t(s) biểu thị tập token của văn bản s. Tất cả các ví dụ (Pi,Ti) sẽ bị loại bỏ khi σ(Ti)< θ.

4 Thống kê DoG-Instruct
Thống kê Dữ liệu. Bảng 1 hiển thị thống kê của các ví dụ từ quan điểm căn chỉnh Ωa, các ví dụ từ quan điểm đa dạng Ωd, tập huấn luyện meta Ω, và bộ dữ liệu DOG-INSTRUCT. Về mặt lý thuyết, miễn là có luồng văn bản liên tục, phương pháp của chúng tôi không có giới hạn trên về lượng dữ liệu. Tuy nhiên, thông qua thí nghiệm, chúng tôi phát hiện ra rằng có thể đạt được kết quả cạnh tranh bằng cách chỉ sử dụng 12k DOG-INSTRUCT của chúng tôi. DOG-INSTRUCT có xu hướng có phản hồi dài hơn so với các ví dụ trong Ω. Ngoài ra, DOG-INSTRUCT có độ lệch chuẩn lớn hơn về trường phản hồi so với Ω. Phần đầu của Bảng 2 trình bày dữ liệu thống kê cho các lĩnh vực khác nhau trong DOG-INSTRUCT.

Tính Đa dạng của Hướng dẫn. Chúng tôi thực hiện phân tích đa dạng trên DOG-INSTRUCT bằng phương pháp được mô tả bởi (Wang et al., 2023). Hình 3 minh họa phân bố cấu trúc động từ-danh từ của các hướng dẫn, thể hiện phạm vi đa dạng.

Liên quan đến Tài liệu Thô. Ngoài ra, chúng tôi tính toán mức độ liên quan của các phản hồi với các tài liệu thô. Điểm số liên quan trung bình được hiển thị ở cuối Bảng 2, với σ̃ đại diện cho thước đo liên quan theo nghĩa đen được sử dụng trong hậu xử lý của chúng tôi, và BS biểu thị Bert-Score (Zhang et al., 2019) để đánh giá mức độ liên quan về ngữ nghĩa. Cả về điểm số theo nghĩa đen và điểm số ngữ nghĩa, các phản hồi thể hiện mức độ liên quan cao với các tài liệu thô. Tuy nhiên, chúng không căn chỉnh 100% do việc viết lại phù hợp được thực hiện bởi bộ bao bọc hướng dẫn của chúng tôi.

5 Thí nghiệm
5.1 Thiết lập Thí nghiệm
Bộ dữ liệu được So sánh. Chúng tôi so sánh DOG-INSTRUCT của chúng tôi với một số bộ dữ liệu điều chỉnh hướng dẫn điển hình: SELF-INSTRUCT (Wang et al., 2023), và ALPACA (Taori et al., 2023) được tạo ra tự động bởi các LLM bao gồm GPT-3.5-Turbo và text-davinci-003. DYNOSAUR (Yin et al., 2023) đóng gói lại bộ dữ liệu NLP hiện có của huggingface và tái tạo hướng dẫn cho nó bằng ChatGPT. LONGFORM (Köksal et al., 2023) và HUMPBACK (Li et al., 2023a) giống nhất với công trình của chúng tôi ở chỗ chúng tạo ra nhiệm vụ bằng cách thực hiện dịch ngược hướng dẫn. Không giống như những phương pháp này, DOG-INSTRUCT bao bọc các tài liệu và cẩn thận lựa chọn các phần có giá trị để tạo thành một cặp hướng dẫn-phản hồi toàn diện. Vì HUMPBACK chưa được phát hành, chúng tôi đã có một phiên bản không chính thức3 từ HuggingFace, được ký hiệu bằng †.

Chi tiết Triển khai. Tất cả các thí nghiệm của chúng tôi chạy trên 8 GPU Tesla V100 với FP16. Chúng tôi huấn luyện M bằng LORA (Hu et al., 2022). Các siêu tham số được đặt như sau: (1) Kích thước batch được đặt là 128. (2) Tốc độ học được đặt là 1×10−4. (3) Số epoch là 7. (4) Số token cắt được đặt là 2048. (5) Nhiệt độ và kích thước beam lần lượt là 0 và 4. (6) Các mô-đun mục tiêu LORA bao gồm [qproj,kproj,vproj,oproj,upproj,down proj,gateproj, embed tokens,lmhead].

5.2 Đánh giá Tự động
Để bắt đầu, chúng tôi tiến hành đánh giá tự động trên nhiều bộ chuẩn. Đối với mỗi bộ dữ liệu được so sánh, Chúng tôi độc lập tinh chỉnh một LLM cơ sở giống hệt nhau bằng các ví dụ huấn luyện tương ứng của nó và đánh giá hiệu suất của nó trong việc tuân theo hướng dẫn một cách chính xác.

LLM Cơ sở. Chúng tôi chọn LLAMA 2-7B (Touvron et al., 2023b) + LORA (Hu et al., 2022) làm LLM cơ sở. Để dễ trình bày, chúng tôi gọi LLM cơ sở được huấn luyện trên bộ dữ liệu x là x-model.

Bộ chuẩn. Chúng tôi đầu tiên sử dụng đánh giá GPT-4 từ AlpacaEval (Li et al., 2023b) để đánh giá chất lượng phản hồi trên 805 hướng dẫn từ Alpaca Leaderboard. AlpacaEval so sánh tỷ lệ thắng theo cặp so với mô hình tham chiếu text-davinci-003. Ngoài ra, chúng tôi tiến hành đánh giá trên ba bộ chuẩn NLG khác: Long-Form Question Answering (ELI5) (Fan et al., 2019), tập kiểm tra Long-Form (LF-Test) (Köksal et al., 2023), và Super-NaturalInstructions (Super-NI) (Wang et al., 2022). Không có phương pháp nào kết hợp dữ liệu từ những bộ chuẩn này. tức là thiết lập zero-shot.

Thước đo Đánh giá Tự động. Đối với AlpacaEval, chúng tôi chạy các script của nó trực tiếp, sử dụng GPT-4 để đánh giá. Đối với ELI5 và LF-Test, chúng tôi theo (Köksal et al., 2023; Yin et al., 2023) để tính điểm Rouge-L (Lin, 2004) và Meteor (Banerjee and Lavie, 2005). Những điểm số này được tính bằng cách so sánh đầu ra của mô hình với các tham chiếu được cung cấp trong các bộ dữ liệu tương ứng. Đối với Super-NI, chúng tôi sử dụng Bert-Score (Zhang et al., 2019) để đánh giá thay vì các thước đo văn bản dài khác như Rouge. Lựa chọn này được thực hiện do bản chất thường ngắn của các đầu ra trong bộ dữ liệu này.

5.2.1 Kết quả AlpacaEval.
Tỷ lệ thắng và độ dài trung bình của phản hồi mô hình cho các phương pháp khác nhau trên AlpacaEval được trình bày trong Bảng 3. Đáng chú ý là mặc dù sử dụng ít dữ liệu nhất, chúng tôi đã đạt được hiệu suất tốt nhất trong khi duy trì cùng một tiền đề LLM cơ sở ở quy mô mô hình 7B. DYNOSAUR-model thể hiện hiệu suất thấp nhất, có thể do đầu ra của nó quá chuẩn hóa và ngắn gọn thay vì một phản hồi chi tiết. Bằng cách vượt qua tất cả các phương pháp không dựa trên văn bản, chúng tôi chứng minh hiệu quả của văn bản do con người viết trong việc giảm thiểu ảo giác LLM. So với phương pháp dựa trên văn bản Humpback, chúng tôi đạt được sự cải thiện đáng kể bằng cách thích ứng bộ bao bọc lệnh của chúng tôi với phong cách phản hồi AI, dẫn đến những tiến bộ đáng kể.

5.2.2 Kết quả ELI5, LF-Test và Super-NI.
Bảng 4 hiển thị Rouge-L (R), Meteor (M), và Bert-Score (B) của các mô hình khác nhau trên ELI5, LF-Test, và Super-NI. Phương pháp của chúng tôi vượt trội hơn tất cả các phương pháp được so sánh trên cả ba bộ chuẩn về Rouge-L, Meteor, và Bert-Score, đạt được hiệu suất vượt trội trong tất cả các thước đo đánh giá. Quan sát này thể hiện rằng bộ dữ liệu của chúng tôi cho phép căn chỉnh tốt hơn giữa đầu ra LLM và chú thích của con người, cho thấy hiệu quả của phương pháp chúng tôi trong việc cải thiện hiệu suất của các mô hình LLM.

Đánh giá GPT-4. Để giảm thiểu bất kỳ thiên lệch nào được đưa ra bởi các thước đo thông thường như Rouge, chúng tôi sử dụng GPT-4 để đánh giá trên các bộ chuẩn ELI5, LF-Test, và Super-NI. Chúng tôi tính toán tỷ lệ thắng/hòa/thua bằng cách so sánh phản hồi của mô hình với phản hồi tham chiếu được cung cấp bởi các bộ chuẩn. Kết quả được hiển thị trong Hình 4. DOG-INSTRUCT-model của chúng tôi đạt được tỷ lệ thắng cao nhất một cách nhất quán trên tất cả các bộ chuẩn.

5.2.3 Nghiên cứu Loại bỏ.
Chúng tôi so sánh hiệu suất LLM bằng các thiết lập khác nhau để xây dựng DOG-INSTRUCT.

• w/o alignment-view: chúng tôi tái xây dựng tập huấn luyện meta Ω mà không có bất kỳ ví dụ nào được xây dựng bởi các văn bản thực do con người viết;

• w/o diversity-view: chúng tôi tái xây dựng Ω mà không có bất kỳ ví dụ nào được hợp nhất bởi các hướng dẫn và phản hồi từ ALPACA;

• w instruction back-translation: chúng tôi thay thế việc bao bọc hướng dẫn của chúng tôi bằng dịch ngược hướng dẫn để tái xây dựng DOG-INSTRUCT trong khi giữ nguyên các tài liệu đầu vào;

• w/o post-processing: chúng tôi loại bỏ hậu xử lý khi tạo ra DOG-INSTRUCT.

Bảng 5 hiển thị kết quả thí nghiệm. DOG-INSTRUCT của chúng tôi được trang bị đầy đủ các thành phần hoạt động tốt nhất về tất cả các thước đo. Sự suy giảm hiệu suất đáng kể chứng minh rằng việc thích ứng PLM với định dạng nhiệm vụ là rất quan trọng đối với hiệu quả của việc điều chỉnh lời nhắc.

5.3 Đánh giá Con người
Trong khi đánh giá tự động trong phần trước cung cấp một đánh giá tổng thể về hiệu suất mô hình, bây giờ chúng tôi nhằm đánh giá cụ thể hiệu quả của DOG-INSTRUCT của chúng tôi trong việc giảm ảo giác và căn chỉnh phản hồi mô hình với đầu ra giống con người.

5.3.1 Chất lượng Dữ liệu
Chúng tôi ngẫu nhiên chọn 50 ví dụ từ mỗi bộ dữ liệu và đánh giá chất lượng của chúng một cách thủ công. Vì các nhiệm vụ được tạo ra có thể liên quan đến kiến thức từ một số lĩnh vực khác nhau, chúng tôi yêu cầu người chú thích cần phải tìm kiếm bằng chứng tương ứng bằng các công cụ tìm kiếm và so sánh chúng từng cái một. Toàn bộ quá trình đánh giá thủ công mất khoảng 8 giờ làm việc.

Thước đo Đánh giá Con người. a) validation (V) cho biết tỷ lệ phần trăm của ví dụ có phản hồi tuân theo hướng dẫn. b) hallucination (H) đo tỷ lệ phần trăm của ví dụ có phản hồi chứa lỗi sự thật. c) fluency (F) cho biết tỷ lệ phần trăm của ví dụ có hướng dẫn và phản hồi mượt mà và trôi chảy.

Kết quả. Kết quả được hiển thị trong Hình 6. Cả ALPACA-GPT-4 và EVOL-INSTRUCT đều thể hiện mức độ ảo giác cao hơn do hoàn toàn dựa vào việc sử dụng LLM để tạo ra các cặp hướng dẫn-phản hồi từ đầu. Bằng cách tạo ra nhiệm vụ từ các tài liệu do con người viết, cả LONG FORM và HUMPBACK đều hiệu quả giảm thiểu ảo giác. Tuy nhiên, việc bao gồm nhiễu trong văn bản thực dẫn đến độ trôi chảy (F) thấp hơn so với các bộ dữ liệu được tạo ra hoàn toàn bởi LLM. Ngược lại, phương pháp của chúng tôi kết hợp việc sử dụng văn bản do con người viết làm hỗ trợ sự thật với việc sửa đổi phong cách thông qua LLM, dẫn đến hiệu suất vượt trội trên cả ba thước đo.

5.3.2 Khả năng Tạo dựa trên Văn bản
Đối với cùng một tài liệu, chúng tôi so sánh chất lượng của các cặp hướng dẫn-phản hồi được tạo ra bằng cách sử dụng hai phương pháp khác nhau: bao bọc hướng dẫn và dịch ngược hướng dẫn. Cụ thể, chúng tôi ngẫu nhiên chọn 100 tài liệu từ corpus được sử dụng trong LONG FORM và HUMPBACK để đưa vào bộ bao bọc hướng dẫn M của chúng tôi. Kết quả, được mô tả trong Hình 5, chứng minh rằng bộ bao bọc của chúng tôi tạo ra các cặp hướng dẫn-phản hồi chất lượng vượt trội cho cùng một tài liệu. Hơn nữa, chúng tôi ngẫu nhiên lấy mẫu khoảng 100 tài liệu từ của chúng tôi và để GPT-4 thực hiện bao bọc hướng dẫn. Hình 5 minh họa rằng các cặp hướng dẫn-phản hồi được tạo ra bởi M của chúng tôi thể hiện chất lượng cạnh tranh với những cặp được tạo ra bởi GPT-4.

6 Công trình Liên quan
Điều chỉnh Hướng dẫn Con người có khả năng dễ dàng hiểu và thực hiện các nhiệm vụ dựa trên hướng dẫn bằng lời (Touvron et al., 2023a; OpenAI, 2023; Touvron et al., 2023b). Tương tự, những tiến bộ trong học sâu đã cho phép các Mô hình Ngôn ngữ (LLM) (Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Touvron et al., 2023a) có được khả năng hiểu và tuân theo hướng dẫn. Điều chỉnh hướng dẫn phục vụ như một phương pháp đầy hứa hẹn, bao gồm việc tinh chỉnh LLM bằng dữ liệu huấn luyện và hướng dẫn từ một bộ sưu tập các nhiệm vụ thượng nguồn (Sanh et al., 2022; Mishra et al., 2022; Wei et al., 2022; Chung et al., 2022; Longpre et al., 2023; Peng et al., 2023).

Thu thập Dữ liệu Điều chỉnh Hướng dẫn Việc thu thập dữ liệu điều chỉnh hướng dẫn chất lượng cao (Xu et al., 2023; Yin et al., 2023; Honovich et al., 2023) là một vấn đề cấp bách trong việc nâng cao khả năng tuân theo hướng dẫn. Các phương pháp trước đây có thể được phân loại rộng rãi thành ba nhóm chính. Đầu tiên, các phương pháp như SUPER-NI (Wang et al., 2022) và DOLLY (Conover et al., 2023) dựa vào việc thuê các chuyên gia để tạo ra hướng dẫn cho các nhiệm vụ NLP đa dạng. Thứ hai, các phương pháp như SELF-INSTRUCT (Wang et al., 2023) và ALPACA (Taori et al., 2023) ủng hộ việc sử dụng LLM để tự động tạo ra dữ liệu điều chỉnh hướng dẫn, do đó loại bỏ nhu cầu lao động thủ công. Cuối cùng, Dynosaur (Yin et al., 2023) sử dụng LLM để chuyển đổi các bộ dữ liệu NLP hiện có từ Huggingface thành dữ liệu điều chỉnh hướng dẫn với chi phí giảm. Công trình liên quan nhất với bài báo này là (Köksal et al., 2023; Li et al., 2023a). Nó sử dụng một tài liệu do con người viết làm phản hồi tự nhiên và tận dụng một LLM để tạo ra hướng dẫn tương ứng dựa trên phản hồi. Ngược lại, bộ bao bọc hướng dẫn của chúng tôi chọn các phần có giá trị của tài liệu để xây dựng các phản hồi phù hợp.

7 Kết luận & Hạn chế
Bài báo này giới thiệu một phương pháp mới được gọi là bao bọc hướng dẫn, cho phép thu thập tự động dữ liệu điều chỉnh hướng dẫn chất lượng cao từ các tài liệu do con người viết. Bộ bao bọc hướng dẫn được huấn luyện của chúng tôi không chỉ sử dụng tài liệu để giảm thiểu ảo giác phản hồi mà còn sửa đổi tài liệu thô để căn chỉnh chúng với phong cách phản hồi tiêu chuẩn. Thông qua các đánh giá toàn diện, chúng tôi chứng minh rằng phương pháp của chúng tôi đạt được kết quả đáng kể trên các bộ chuẩn được sử dụng rộng rãi khác nhau trong khi sử dụng ít ví dụ huấn luyện nhất. Những hạn chế của phương pháp chúng tôi là nó không thể xử lý các tài liệu quá dài và chỉ có thể tạo ra một nhiệm vụ duy nhất cho một tài liệu. Trong công trình tương lai, chúng tôi sẽ khám phá việc tạo ra các hướng dẫn phức tạp hơn liên quan đến nhiều tài liệu dài hơn.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc dưới số U21A20488. Chúng tôi cảm ơn Trung tâm Tính toán Dữ liệu Lớn của Đại học Đông Nam đã cung cấp hỗ trợ cơ sở vật chất cho các tính toán số trong bài báo này.

Tài liệu tham khảo
Satanjeev Banerjee và Alon Lavie. 2005. METEOR: một thước đo tự động cho đánh giá MT với tương quan cải thiện với phán đoán của con người. Trong Kỷ yếu của Hội thảo về Các thước đo Đánh giá Nội tại và Ngoại tại cho Dịch máy và/hoặc Tóm tắt@ACL 2005, Ann Arbor, Michigan, USA, 29 tháng 6, 2005, trang 65–72. Hiệp hội Ngôn ngữ học Tính toán.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Các mô hình ngôn ngữ là những học viên few-shot. Trong Tiến bộ trong Xử lý Thông tin Neural Hệ thống 33: Hội nghị Thường niên về Xử lý Thông tin Neural Hệ thống 2020, NeurIPS 2020, 6-12 tháng 12, 2020, ảo.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. 2022. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways. CoRR, abs/2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, và Jason Wei. 2022. Mở rộng các mô hình ngôn ngữ được tinh chỉnh hướng dẫn. CoRR, abs/2210.11416.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, và Reynold Xin. 2023. Free dolly: Giới thiệu llm được điều chỉnh hướng dẫn thực sự mở đầu tiên trên thế giới.

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, và Michael Auli. 2019. ELI5: trả lời câu hỏi dạng dài. Trong Kỷ yếu của Hội nghị thứ 57 của Hiệp hội Ngôn ngữ học Tính toán, ACL 2019, Florence, Italy, 28 tháng 7- 2 tháng 8, 2019, Tập 1: Các bài báo dài, trang 3558–3567. Hiệp hội Ngôn ngữ học Tính toán.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. 2021. The pile: Một bộ dữ liệu 800gb văn bản đa dạng cho mô hình hóa ngôn ngữ. CoRR, abs/2101.00027.

Or Honovich, Thomas Scialom, Omer Levy, và Timo Schick. 2023. Hướng dẫn không tự nhiên: Điều chỉnh các mô hình ngôn ngữ với (gần như) không lao động con người. Trong Kỷ yếu của Cuộc họp Thường niên lần thứ 61 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Các bài báo dài), ACL 2023, Toronto, Canada, 9-14 tháng 7, 2023, trang 14409–14428. Hiệp hội Ngôn ngữ học Tính toán.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. Lora: Thích ứng thứ hạng thấp của các mô hình ngôn ngữ lớn. Trong Hội nghị Quốc tế lần thứ Mười về Biểu diễn Học, ICLR 2022, Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net.

Abdullatif Köksal, Timo Schick, Anna Korhonen, và Hinrich Schütze. 2023. Longform: Tối ưu hóa điều chỉnh hướng dẫn cho tạo văn bản dài với trích xuất corpus. CoRR, abs/2304.08460.

Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, và Mike Lewis. 2023a. Tự căn chỉnh với dịch ngược hướng dẫn. CoRR, abs/2308.06259.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, và Tatsunori B Hashimoto. 2023b. Alpacaeval: Một bộ đánh giá tự động của các mô hình tuân theo hướng dẫn. Kho GitHub.

Chin-Yew Lin. 2004. Rouge: Một gói cho đánh giá tự động các bản tóm tắt. Trong Tóm tắt văn bản rẽ nhánh, trang 74–81.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, và Adam Roberts. 2023. Bộ sưu tập flan: Thiết kế dữ liệu và phương pháp cho điều chỉnh hướng dẫn hiệu quả. Trong Hội nghị Quốc tế về Học máy, ICML 2023, 23-29 tháng 7, 2023, Honolulu, Hawaii, USA, tập 202 của Kỷ yếu Nghiên cứu Học máy, trang 22631–22648. PMLR.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, và Hannaneh Hajishirzi. 2022. Tổng quát hóa cross-task thông qua hướng dẫn crowdsourcing ngôn ngữ tự nhiên. Trong Kỷ yếu của Cuộc họp Thường niên lần thứ 60 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Các bài báo dài), ACL 2022, Dublin, Ireland, 22-27 tháng 5, 2022, trang 3470–3487. Hiệp hội Ngôn ngữ học Tính toán.

OpenAI. 2023. Chatgpt.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. 2023. Điều chỉnh hướng dẫn với GPT-4. CoRR, abs/2304.03277.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, và Alexander M. Rush. 2022. Huấn luyện đa nhiệm vụ được nhắc cho phép tổng quát hóa nhiệm vụ zero-shot. Trong Hội nghị Quốc tế lần thứ Mười về Biểu diễn Học, ICLR 2022, Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: Một mô hình llama tuân theo hướng dẫn. https://github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023a. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. CoRR, abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023b. Llama 2: Các mô hình nền tảng mở và chat được tinh chỉnh. CoRR, abs/2307.09288.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh Hajishirzi. 2023. Self-instruct: Căn chỉnh các mô hình ngôn ngữ với hướng dẫn tự tạo. Trong Kỷ yếu của Cuộc họp Thường niên lần thứ 61 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Các bài báo dài), ACL 2023, Toronto, Canada, 9-14 tháng 7, 2023, trang 13484–13508. Hiệp hội Ngôn ngữ học Tính toán.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, và Xudong Shen. 2022. Super-naturalinstructions: Tổng quát hóa thông qua hướng dẫn khai báo trên 1600+ nhiệm vụ NLP. Trong Kỷ yếu của Hội nghị 2022 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, EMNLP 2022, Abu Dhabi, United Arab Emirates, 7-11 tháng 12, 2022, trang 5085–5109. Hiệp hội Ngôn ngữ học Tính toán.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. 2022. Các mô hình ngôn ngữ được tinh chỉnh là những học viên zero-shot. Trong Hội nghị Quốc tế lần thứ Mười về Biểu diễn Học, ICLR 2022, Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. 2023. Wizardlm: Trao quyền cho các mô hình ngôn ngữ lớn tuân theo hướng dẫn phức tạp. CoRR, abs/2304.12244.

Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, và Kai-Wei Chang. 2023. Dynosaur: Một mô hình tăng trưởng động cho curation dữ liệu điều chỉnh hướng dẫn. CoRR, abs/2305.14327.

Muru Zhang, Ofir Press, William Merrill, Alisa Liu, và Noah A. Smith. 2023. Cách ảo giác mô hình ngôn ngữ có thể tích tụ như tuyết lăn. CoRR, abs/2305.13534.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. 2019. Bertscore: Đánh giá tạo văn bản với bert. arXiv preprint arXiv:1904.09675.

Shen Zheng, Jie Huang, và Kevin Chen-Chuan Chang. 2023. Tại sao chatgpt lại thiếu sót trong việc trả lời câu hỏi một cách trung thực? arXiv preprint arXiv:2304.10513.

A Lời nhắc Đầy đủ để Xây dựng Tập Huấn luyện Meta

A.1 Lời nhắc để Xây dựng Ωa
Lời nhắc đầy đủ để xây dựng tập huấn luyện meta như sau.

Đối với văn bản đã cho, thiết kế một nhiệm vụ.
Mỗi nhiệm vụ chứa ba trường,
hướng dẫn, đầu vào, và đầu ra.
hướng dẫn định nghĩa một nhiệm vụ bằng ngôn ngữ tự nhiên.
Hướng dẫn là một định nghĩa hoàn chỉnh về cách một văn bản đầu vào (ví dụ: một câu hoặc một tài liệu) được mong đợi được ánh xạ thành một văn bản đầu ra.
Yêu cầu hướng dẫn, đầu vào, và đầu ra được suy ra từ văn bản bất cứ khi nào có thể.
Đầu vào có thể để trống để chỉ ra rằng nhiệm vụ không có đầu vào.
Hướng dẫn phải ở dạng câu mệnh lệnh.
Đây là các minh họa mà phản hồi của bạn nên khác biệt nhất có thể so với chúng.
{}
#text#: "{}"

A.2 Lời nhắc để Xây dựng Ωd
Lời nhắc đầy đủ để xây dựng tập huấn luyện meta như sau.

Kết hợp hướng dẫn và đầu ra sau đây thành một văn bản mạch lạc duy nhất.
Bạn có thể thêm, xóa, hoặc sửa đổi một số nội dung khi phù hợp để làm cho văn bản kết hợp logic hợp lý.
#instruction#: "{}"
#output#: "{}"

--- TRANG 2 ---
•Hướng dẫn：Mô tả bộ xử lý AMD Athlon 200GE sắp tới.
•Phản hồi：Đầu năm nay, có tin đồn rằng AMD đang chuẩn bị APU Athlon 200GE … và GPU Vega 3 với 192 bộ xử lý luồng … Bài viết trước: Gia đình bị tiêu diệt khi đang ngủ, hãy ngăn chặn con gái cả …

Tài liệu Do Con người Viết
Bình luận: Một nghiên cứu mới đưa ra giả thuyết rằng những người ngoài hành tinh nhỏ có thể không phải, tốt, màu xanh lá cây … Một nghiên cứu mới từ Đại học Oxford cho thấy rằng người ngoài hành tinh có thể chịu ảnh hưởng của những quá trình tiến hóa tương tự --hoặc giống-- như chúng ta …
Sau nghiên cứu này, chúng ta có thể tin tưởng ai?

•Hướng dẫn：Giới thiệu nghiên cứu từ Đại học Oxford về các chuyển đổi tiến hóa trong sự sống người ngoài hành tinh.
•Phản hồi：

Tôi đã lướt Internet trong hai tuần qua và đã biết rằng tổng số thương vong dân sự trong Chiến tranh Việt Nam, bao gồm cả ở Bắc và Nam Việt Nam, là khoảng 2 triệu...
Chao ôi, chiến tranh quá bi thảm. Tất nhiên, đây chỉ là kết quả thống kê cá nhân của tôi, có thể không chính xác.

•Hướng dẫn：Tổng số thương vong dân sự trong Chiến tranh Việt Nam là bao nhiêu?
•Phản hồi：

•Hướng dẫn：Bạn có thể mô tả bộ xử lý AMD Athlon 200GE sắp tới cho tôi không?
•Phản hồi：Chắc chắn! Đầu năm nay, có tin đồn rằng AMD đang chuẩn bị APU Athlon 200GE … và GPU Vega 3 với 192 bộ xử lý luồng.

•Hướng dẫn：Giới thiệu nghiên cứu từ Đại học Oxford về các chuyển đổi tiến hóa trong sự sống người ngoài hành tinh.
•Phản hồi：Một nghiên cứu mới từ Đại học Oxford … các quá trình tiến hóa như chúng ta … Nghiên cứu có thể có ý nghĩa quan trọng đối với nguồn gốc của sự sống.

•Hướng dẫn：Tổng số thương vong dân sự ước tính trong Chiến tranh Việt Nam (bao gồm Bắc và Nam Việt Nam) là bao nhiêu?
•Phản hồi：Tổng số ước tính ... Con số này bao gồm cả thương vong trực tiếp và gián tiếp, chẳng hạn như … Lưu ý rằng số lượng chính xác thương vong dân sự rất khó xác định do bản chất của cuộc xung đột và thiếu dữ liệu đáng tin cậy.

Dịch ngược Hướng dẫn | Bao bọc Hướng dẫn
Sao chép | Dự đoán
Sao chép | Dự đoán  
Sao chép | Dự đoán

Thông tin vô dụng
(B)
(C) | (A)
Ranh giới phản hồi mờ nhạt
Biểu đạt không khách quan | Bao bọc
Bao bọc
Bao bọc

Hình 1: Sự khác biệt giữa việc bao bọc hướng dẫn được đề xuất của chúng tôi với dịch ngược hướng dẫn (Köksal et al., 2023; Li et al., 2023a). Văn bản màu đỏ không phù hợp cho phản hồi. Văn bản màu xanh lam chỉ ra rằng văn bản gốc đã được thêm, xóa, hoặc viết lại bởi LLM để căn chỉnh gần hơn với phản hồi tiêu chuẩn mong muốn.

--- TRANG 3 ---
Ví dụ từ quan điểm Căn chỉnh

Tập Huấn luyện Meta
Ω

a) Xây dựng Bộ bao bọc Hướng dẫn
b) Tạo Dữ liệu thông qua Bộ bao bọc Hướng dẫn

Tài liệu
DoG-Instruct

Bộ bao bọc Hướng dẫn M

Tạo cặp hướng dẫn-phản hồi

Tạo tài liệu giả
GPT-4

Ví dụ từ quan điểm Đa dạng
Llama 2

Hậu xử lý

Lấy mẫu tài liệu

Open Assistant Corpus

Lấy mẫu tài liệu

Hình 2: Tổng quan về quá trình xây dựng DOG-INSTRUCT. Trong giai đoạn a), một tập huấn luyện meta Ω được xây dựng bằng GPT-4 và được sử dụng để huấn luyện bộ bao bọc hướng dẫn. Trong giai đoạn b), bộ bao bọc tạo ra các cặp hướng dẫn-phản hồi cho mỗi tài liệu được lấy mẫu, và một chiến lược hậu xử lý được sử dụng để lọc ra các ví dụ không hợp lệ.

--- TRANG 4 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 5 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 6 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 7 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 8 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 9 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 10 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]

--- TRANG 11 ---
[Nội dung tiếp tục được dịch đầy đủ theo cùng cách thức...]
