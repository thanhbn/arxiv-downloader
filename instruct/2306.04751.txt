# 2306.04751.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2306.04751.pdf
# File size: 1583165 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
How Far Can Camels Go? Exploring the State of
Instruction Tuning on Open Resources
Yizhong Wang‚àó‚ô£‚ô†Hamish Ivison‚àó‚ô£Pradeep Dasigi‚ô£Jack Hessel‚ô£
Tushar Khot‚ô£Khyathi Raghavi Chandu‚ô£David Wadden‚ô£Kelsey MacMillan‚ô£
Noah A. Smith‚ô£‚ô†Iz Beltagy‚ô£Hannaneh Hajishirzi‚ô£‚ô†
‚ô£Allen Institute for AI‚ô†University of Washington
{yizhongw,hamishi}@allenai.org
Abstract
In this work we explore recent advances in instruction-tuning language models on
a range of open instruction-following datasets. Despite recent claims that open
modelscanbeonparwithstate-of-the-artproprietarymodels,theseclaimsareoften
accompaniedbylimitedevaluation,makingitdifficulttocomparemodelsacross
the board and determine the utility of various resources. We provide a large set of
instruction-tunedmodelsfrom6.7Bto65Bparametersinsize,trainedon12instruc-
tion datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and
distilled(e.g.,Alpaca)andsystematicallyevaluatethemontheirfactualknowledge,
reasoning, multilinguality, coding, safety, and open-ended instruction following
abilities through a collection of automatic, model-based, and human-based metrics.
We further introduce T√úLU
,our bestperforminginstruction-tuned modelsuite
finetuned on a combination of high-quality open resources.
Our experiments show that different instruction-tuning datasets can uncover or
enhance specific skills, while no single dataset (or combination) provides the best
performanceacrossallevaluations. Interestingly,wefindthatmodelandhuman
preference-basedevaluationsfailtoreflectdifferencesinmodelcapabilitiesexposed
by benchmark-based evaluations, suggesting the need for the type of systemic
evaluation performed in this work. Our evaluations show that the best model in
anygivenevaluationreachesonaverage87%ofChatGPTperformance,and73%
ofGPT-4performance,suggestingthatfurtherinvestmentin buildingbetterbase
models and instruction-tuning data is required to close the gap. We release our
instruction-tuned models, including a fully finetuned 65B T√úLU
, along with our
code, data, and evaluation framework to facilitate future research.2
1 Introduction
Thelatestgenerationoflargelanguagemodelshasbroughtunprecedentedattentiontothepotentialof
language technologies. To support imperative user requests and a chat interface, these models often
undergoan instruction-tuning stepwhich involves trainingonsupervised input/outputpairs. Recent
instructiontuningcorporaareoftengatheredviacrowdsourcing(Dolly[ 12],OpenAssistant[ 26])
or via distillation from another model (Alpaca [ 43], Vicuna [ 8]). However, while some public,
instruction-tuned models are advertised as comparable to powerful closed-source proprietary models
such as ChatGPT, most experiments that support such claims only cover a small set of tasks, and
mostlyrely onmodel-based evaluationmetrics [ 8,56]. We contendthatthe evaluationsetup should
‚àóEqual contribution.
2https://github.com/allenai/open-instruct
37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2306.04751v2  [cs.CL]  30 Oct 2023

--- PAGE 2 ---
include tasks that test core reasoning and fact-recall skills of the model, in addition to testing model-
or human-annotated generation quality, which may be more open-ended and subjective.
This paper provides a comprehensive evaluation of instruction-tuning resources: specifically, we
conduct a large number of instruction tuning experiments spanning a dozen public corpora, and
models ranging in scale from 6.7B to 65B. We evaluate both specific model capabilities (i.e., factual
knowledge, reasoning, multilinguality, coding, safety) and open-ended instruction-following abilities.
We report results based on automatic, model-based, and human-based evaluation metrics.
Our evaluation reveals that instruction tuning over different datasets appears to promote specific
skills, and no one dataset provides the best performance across all evaluations. We also find that
theunderlyingbasemodelisparamount,withbetterbasemodels(whetheritbemodelstrainedon
more tokens or larger models) performing better across the board. Surprisingly, we also find that the
best-performing models in model-based evaluation are not the same as those that perform best on
benchmark-based automatic evaluations, potentially partially due to GPT-4‚Äôs strong bias toward long,
diverse generations.
Building on our findings, we introduce T√úLU
, a suite of 7B to 65B LLAMAmodels finetuned on a
combination of data sources. T√úLU
65B is the largest publicly-released fully-instruction tuned
LLAMAvariantatthetimeofwriting,tothebestoftheauthors‚Äôknowledge. Itistrainedon7popular
available datasets, and yields the best average performance across most model sizes while remaining
within 29% of the best-performing model on each individual task. In summary, our key findings
include:
‚Ä¢Instruction datasets targeted at specific domains and/or capabilities are extremely effective at
improving model performance in those aspects.
‚Ä¢Larger or pretrained-for-longer base models consistently perform better than smaller ones after
instruction tuning.
‚Ä¢Ourmodel T√úLU
‚Äìfine-tunedLLaMaonacombinationofexistinginstructiondatasets‚Äìachieves
thebestaverageperformanceacrossbenchmarks,althoughitisnottheoverallbestwhenconsidering
different evaluation settings independently.
‚Ä¢Even a very large (65B) model finetuned on a large mix of instruction datasets fails to outperform
ChatGPT, although it does perform significantly better than similar smaller models.
‚Ä¢Model-basedpreferenceevaluationonopen-endedinstructionfollowingcorrelatesstronglywith
theaveragenumberofuniquetokensgeneratedbyamodel,suggestingthatmodel-basedpreference
evaluation has biases that may hide differences in model capabilities.
Weopen-sourcethecodefortrainingandevaluatingtheselargelanguagemodels. Wealsorelease
checkpointstrainedonthedifferentinstructiondatasetsandtheirmixtures,including T√úLU
. We
hope this facilitates further development and investigation of open instruction-tuned models.
2 Background: Instruction Tuning and Resources
2.1 Instruction Tuning
Instruction tuning , in general, refers to the practice of finetuning pretrained language models to better
understandandrespondtoawidevarietyofhumanrequeststhatareexpressedinnaturallanguage
[32,49,35]. In particular, instruction tuning is concerned with requests that include some indication
of the task to be performed within the request itself (e.g., including task instructions in the input
prompt). Ithasarisenasacriticalstepforgeneralizingmodelstonewscenarioswithoutdedicated
training, and for letting non-experts naturally interact with these models. The training paradigms
of instruction tuning can vary from supervised learning using demonstrations [ 49,39,48,31] to
reinforcement learning from feedback data [ 35,3]. In this work, we focus on the supervised learning
setupconsideringthecurrentopenresourcesfortheRL-basedapproacharestillrare,andweleaveits
exploration for future work.
The success of instruction tuning requires at least two key components: 1) a powerful pretrained
language model that has grasped a vast amount of knowledge from web-scale pretraining, and 2) an
instruction dataset that is diverse and representative enough to adapt the LM to potential downstream
usage. We study these two factors in this work and introduce our studied open resources below.
2

--- PAGE 3 ---
Table 1: Instruction datasets investigated in this work. CoT and FLAN V2 are sampled to 100K
tomatchthesizesofotherdatasets. Wereporttheaveragenumberofconservationturns( ÃÑùëÅrounds),
average length of prompts ( ÃÑùêøprompt), average length of completion ( ÃÑùêøcompletion).
Datasets Sourced from # Instances ÃÑùëÅroundsÃÑùêøpromptÃÑùêøcompletion
SuperNI [48] NLP datasets + Human-written Instructions 96,913 1.0 291.1 38.7
CoT [50] NLP datasets + Human-written CoTs 100,000 1.0 266.0 53.2
Flan V2 [31] NLP datasets + Human-written Instructions 100,000 1.0 355.7 31.2
Dolly [12] Human-written from scratch 15,011 1.0 118.1 91.3
Open Assistant 1 [26] Human-written from scratch 34,795 1.6 34.8 212.5
Self-instruct [47] Generated w/ vanilla GPT3 LM 82,439 1.0 41.5 29.3
Unnatural Instructions [23] Generated w/ Davinci-002 68,478 1.0 107.8 23.6
Alpaca [43] Generated w/ Davinci-003 52,002 1.0 27.8 64.6
Code-Alpaca [6] Generated w/ Davinci-003 20,022 1.0 35.6 67.8
GPT4-Alpaca [36] Generated w/ Davinci-003 + GPT4 52,002 1.0 28.0 161.8
Baize [52] Generated w/ ChatGPT 210,311 3.1 17.6 52.8
ShareGPT3User prompts + outputs from various models 168,864 3.2 71.0 357.8
2.2 Instruction Datasets
Weattempttocollectarepresentativesampleofdifferentstylesofdatasets(listedinTable1),including
datasets: (1) created by researchers from existing NLP datasets (SuperNI [ 48], Flan V2 [ 31]); (2)
writtenbyhumansfromscratchforthepurposeofinstructiontuning(Dolly[ 12],OpenAssistant1
[26]);(3)generatedbyproprietarymodels(Self-Instruct[ 47],UnnaturalInstructions[ 23],Alpaca
[43], Baize [52], GPT4-Alpaca [ 36]); (4) comprised of user-shared prompts accompanied by model-
generatedcompletions(ShareGPT3[8]);(5)builtforspecificskills(CoT[ 50]forchain-of-thought,
Code-Alpaca [6] for code generation). See Appendix C for further details.
2.3 Pretrained Models
Table 2: Base models that we
finetuned in this work.
Base LMs # Params # Tokens
LLaMa [44]6.7B 1.0T
13.0B 1.0T
32.5B 1.4T
65.2B 1.4T
LLaMa-2 [45]6.7B 2.0T
13.0B 2.0T
OPT [54] 6.7B 180B
Pythia [4] 6.9B 300BWe primarily use the LLAMAsuite [44,45], a series of pretrained
models ranging in size from 6.7B to 65B parameters. We initially
experimentedwiththe LLAMA-1modelsforthefirstversionof
this paper and added LLAMA-2 in our camera ready, which use
similar numbers of parameters but were trained over significantly
moretokens. Thesemodelsrepresentthelargest,highest-quality
pretrained models available to the community (albeit under restric-
tive licensing). We also consider OPT [ 54] and Pythia [ 4] models
withasizecomparabletothe LLAMA6.7Bmodel,toexaminethe
effectofdifferentbasemodels. Forsimplicity,wewillroundall
thesizestothenearestintegernumber. Wenoteseveralongoing
efforts to pre-train similar- or better-quality models [ 18,33,1].
Webelieveourfindingsshouldholdforthesemodelsandfuture
stronger open base models.
3 Training Models with Various Datasets
3.1 Unifying the Format
Weformatalldatasetstofollowachatbot-styleschematounifythevariedstylesandformatsof the
instruction datasets, shown in Figure 1. This allows us to fit arbitrary rounds of interactions between
theuserandthelanguagemodel(a.k.a.‚Äúassistant‚Äù)intooneinputsequenceandencodethemtogether
with a causal language model. We add special tokens <|user|> and<|assistant|> before user
utterancesandtargetassistantresponsesrespectively,andanend-of-textmarker </s>attheendof
each assistant output, which, at inference time, will stop the model‚Äôs response for each round.
3ShareGPT ( https://sharegpt.com/ ) data was used to build the Vicuna model [ 8], but the exact dataset
has not been released. We instead use a reproduced version from https://huggingface.co/datasets/
anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset , and fol-
low Vicuna to split the long conversations into blocks with a maximum length of 2048 tokens.
3

--- PAGE 4 ---
3.2 Model Training Details
<|assistant|>The reaction control system (RCS) on the Space Shuttle was designed to be fault-tolerant, meaning it was able to continue functioning even if one or more of its components failed. The RCS consisted of two sets of ...</s><|assistant|>There were several instances where the reaction control system (RCS) on the Space Shuttle experienced failures or malfunctions during on-orbit missions.These...</s><|user|>Explain the fault-tolerance of the reaction control system on the Space Shuttle.
<|user|>Did the RCS have any on-orbit failures?
üë±
üë±
ü§ñ
ü§ñ
Figure1: AnexamplefromShareGPTdata. Weuse
<|role|> tosettheboundarybetweenmessages.
Theentiresequenceisencodedtogether,andloss
iscomputedontheassistantparts(coloredinblue).During training, we compute loss only on to-
kens after <|assistant|> and before the next
<|user|> token. More formally, we consider
an instruction dataset as consisting of ùëÅtuples,
each withùëñturns, {(ùë•ùëó
1,ùë¶ùëó
1,ùë•ùëó
2,ùë¶ùëó
2,...ùë•ùëó
ùëñ,ùë¶ùëó
ùëñ)}ùëÅ
ùëó=1,
whereùë•ùëñis a user prompt and ùë¶ùëñthe desired out-
put. Formostinstances, ùëñ= 1,andwetrainthe
model to output ùë¶ùëógivenùë•ùëó. However, in the
caseofconversationdatasets,wetrainthemodel
to predictùë¶ùëó
ùëñgiven some conversation history
ùë•ùëó
1,ùë¶ùëó
1,ùë•ùëó
2,...,ùë•ùëó
ùëñ. We train decoder-only models,
and use teacher-forcing with loss masking to
trainthemodels,wherewemaskalltokensbe-
longing to the input sequence(s) ùë•ùëñ. Givenùëãas
thetokensbelongingtotheinput,and ùëåasthe
target tokens, the loss function is:
ùêø= ‚àí‚àë
ùëólogùëùùúÉ(ùë°ùëó‚à£ùë°<ùëó) √ó{1ifùë°ùëó‚ààùëå
0otherwise
whereùë°ùëóis theùëóth input token (belonging to ùëãorùëå). See Appendix ¬ßD for further training details.
3.3 T√úLU
: a Better Instruction-Tuned Model by Combining Resources
Existing studies [ 48,31] (and our own evaluation below) have shown that increasing the diversity of
instructions can effectively improve the performance of instruction tuning. Following this motivation,
we create two mixtures of datasets:
Humandatamixture ,whichcomprisesthebesthuman-authoreddatasets,includingFLANV2,CoT,
Dolly, and Open Assistant 1 (we exclude SuperNI as FLAN V2 includes most tasks in SuperNI);
Human+GPT data mixture , which comprises the human mixture and three additional datasets that
have generations by OpenAI GPT models, including GPT4-Alpaca, Code-Alpaca, and ShareGPT.
For both mixtures, we concatenate datasets and leave exploring more complex sampling mixtures to
future work. We name LLAMAmodels trained on the Human+GPT data mixture T√úLU
, after
a hybrid camel resulting from interbreeding between different species. We differentiate the T√úLU
models trained from the LL AMA-2 base models by versioning them as T√úLU-1.1.
4 Evaluation Setup
Evaluation of instruction-following models remains a challenging problem due to the enormous
scope of ‚Äúgenerality‚Äù and its open-ended nature. However, we argue that general-purpose models
should be able to perform some core tasks before they can generalize to satisfy various practical
needs. As such, we set up a multi-faceted evaluation to cover several key aspects of capabilities
coveringcoreabilitiesandopen-endedinstructionfollowing. Ourevaluationscloselyfollowprior
work on evaluating instruction-tuned models [ 9,2,47,8,16], but serve as the first one to compile
them together for systematic evaluation.
4.1 Facets of Evaluation
Factualknowledge isessentialforlanguagemodelstoserveusers‚Äôinformationneeds. Weusethe
Massive Multitask Language Understanding dataset (MMLU [ 22]) for measuring models‚Äô factual
knowledge. MMLU consists of a set of questions about 57 subjects ranging in difficulty from
elementary levels to professional levels, and its multiple-choice format makes it suitable for probing
models‚Äô knowledge without worrying about the open-endedness of generations.
4

--- PAGE 5 ---
Reasoning isanotherfundamentalabilityformodels,especiallyforsolvingcomplextasks. Weuse
the test split of Grade School Math dataset (GSM [ 11]) to evaluate models‚Äô mathematical reasoning
capabilities. We also adopt Big-Bench-Hard (BBH [ 42]), which contains 23 challenging tasks from
Big-Bench [41], to evaluate models‚Äô general reasoning capabilities.
Multilinguality actsasanimportantperspectiveofmodelsforservingpeoplefromdifferentback-
grounds. We use TyDiQA [ 10], a multilingual question answering benchmark covering 11 typo-
logicallydiverselanguagesfortestinghowmuchmodelscanprocessnon-Engishtext. Weusethe
gold-passage setup where one passage containing the reference answer is given.
Codingis a particular application that people have used language models for and might be important
forintegratingthesemodelswithexternaltools[ 5]. WeusetheHumanEvaldataset[ 7]toevaluate
the models‚Äô capability to generate functionally correct programs from docstrings. To avoid ambiguity
with our human evaluation, we call this dataset Codex-Eval in this paper.
Open-ended instruction following. While the performance on the benchmarks above quantifies the
models‚Äô ability at specific skills, it may not reflect how well the models can handle instructions from
realusers,whichcoverhighlydiverserequestsandareoftenopen-ended. Forexample,thepopular
ShareGPT dataset containsinstances of users asking for programminghelp, resume formatting tips,
educationalrole-playing,pronunciationsuggestion,fanfictionwriting,andmore. Weevaluatesuch
open-endedinstructabilityofmodelsusingbothmodel-basedevaluation(¬ß4.2)andhumanevaluation
(¬ß4.3), both of which consist of multiple test sets from existing studies [47, 8, 26, 3, 19].
Safetyisofparticularconcernregardingthefast-developinglanguagemodelstoensuretheethical
and proper use of them. Following LLAMA-2 [45], we employ ToxiGen [ 21] to measure the amount
of toxic language and hate speech generation across different groups when the models are prompted
todoso. WealsoadoptTruthfulQA[ 30]tomeasurehowwellmodelscanavoidgeneratingknown
falsehoods due to misconceptions or false beliefs while providing useful information.
For all the benchmark-based evaluations, we follow their standard metrics, while we subsample some
benchmarkstoareasonablesizetoimprovetheefficiencyofdoingchain-of-thoughtreasoning. We
refer the reader to Appendix ¬ßE for the setup details.
4.2 Model-Based Evaluation using GPT-4
To evaluate the open-ended instructability, we first adopt a model-based approach introduced in
AlpacaEval[ 27]. Thetestsetconsistsof805instructions,with252instructionsfromtheSelf-Instruct
evaluation [ 47], 188 from the Open Assistant evaluation [ 26], 129 from the helpful evaluation by
Anthropic [3], 80 from the Vicuna evaluation [8], and 156 from the Koala evaluation [19].
We use their simulated GPT-4 annotator, which computes the win rate of the testing model as judged
by GPT-4 when compared to the outputs produced by Davinci-003. We use the AlpacaEval codebase
andprompts[ 27]tomakeourscoresdirectlycomparabletothoseontheAlpacaEvalleaderboard4
When doing pairwise comparisons with GPT-4, the orders of model outputs are randomized to avoid
position bias during evaluation [ 46]. We do not evaluate vanilla LLAMAmodels due to them having
little instruction-following ability without further prompt engineering.
4.3 Human Evaluation
Tofurthertestthequalityoftheopen-endedgenerations,weconductahumanevaluationbasedon
332 instructions that combine the Self-Instruct evaluation set [ 47] and Vicuna evaluation set [ 8].
Inspired by Bai et al. [3], we design a similar interface (Figure 5) for gathering human judgments of
modeloutputsalongthefollowingdimensions. Wenotethatweevaluatedbasedonourfine-tuned
LLAMA-1 models, as LL AMA-2 was not available at the time of this experiment.
Individual acceptability. We ask human raters to assess whether each system‚Äôs responses were
acceptableinisolation. Thisisabinarydecision,andweasktheraterstomarkaresponseasacceptable
ifandonlyiftheresponseansweredtherequestinthequery,hadnosignificanterrors,anddidnot
have repetitive information.
4https://tatsu-lab.github.io/alpaca_eval/
5

--- PAGE 6 ---
Table3: Comparisonofdifferentinstructiontuningdatasets,showingthatdifferentinstruction-tuning
datasets can excel in different aspects, and mixtures perform best on average. Cells are blue if the
finetuningbooststhevanilla LLAMAperformance,andorangeifthefinetuninghurtstheperformance.
MMLU
(factuality)GSM
(reasoning)BBH
(reasoning)TydiQA
(multilinguality)Codex-Eval
(coding)AlpacaEval
(open-ended)Average
EM
(0-shot)EM
(8-shot, CoT)EM
(3-shot, CoT)F1
(1-shot, GP)P@10
(0-shot)Win % vs
Davinci-003
Vanilla LLaMa 13B 42.3 14.5 39.3 43.2 28.6 - -
+SuperNI 49.7 4.0 4.5 50.2 12.9 4.2 20.9
+CoT 44.2 40.0 41.9 47.8 23.7 6.0 33.9
+Flan V2 50.6 20.0 40.8 47.2 16.8 3.2 29.8
+Dolly 45.6 18.0 28.4 46.5 31.0 13.7 30.5
+Open Assistant 1 43.3 15.0 39.6 33.4 31.9 58.1 36.9
+Self-instruct 30.4 11.0 30.7 41.3 12.5 5.0 21.8
+Unnatural Instructions 46.4 8.0 33.7 40.9 23.9 8.4 26.9
+Alpaca 45.0 9.5 36.6 31.1 29.9 21.9 29.0
+Code-Alpaca 42.5 13.5 35.6 38.9 34.2 15.8 30.1
+GPT4-Alpaca 46.9 16.5 38.8 23.5 36.6 63.1 37.6
+Baize 43.7 10.0 38.7 33.6 28.7 21.9 29.4
+ShareGPT 49.3 27.0 40.4 30.5 34.1 70.5 42.0
+Human data mix. 50.2 38.5 39.6 47.0 25.0 35.0 39.2
+Human+GPT data mix. 49.3 40.5 43.3 45.6 35.9 56.5 45.2
Table 4: Performance of different base models after training on the Human+GPT data mixture.
MMLU
(factuality)GSM
(reasoning)BBH
(reasoning)TydiQA
(multilinguality)Codex-Eval
(coding)AlpacaEval
(open-ended)Average
EM
(0-shot)EM
(8-shot, CoT)EM
(3-shot, CoT)F1
(1-shot, GP)P@10
(0-shot)Win % vs
Davinci-003
Pythia 6.9B 34.8 16.0 29.2 32.8 20.9 23.5 26.2
OPT 6.7B 32.6 13.5 27.9 24.1 8.9 25.9 22.2
LLAMA7B 44.8 25.0 38.5 43.5 29.1 48.6 38.3
LLAMA-2 7B 49.2 37.0 44.2 52.8 33.9 57.3 45.7
Pairwisepreference. Wethen askhumansto comparetheoutputs oftwosystems andselectwhich
one they think is more helpful. This is a 5-way decision, and the raters could select if one of the
responsesis‚Äúclearly‚Äùor‚Äúslightly‚Äùbetterthantheotherorifitisatieimplyingthatbothresponses
were equally good or bad.
To get a more reliable evaluation, we recruited a group of 18 expert annotators who are researchers at
AI2 or students at UW. All of them are fluent English speakers, holding bachelor‚Äôs degrees or above.
5 Results
5.1 Analysis of Instruction Tuning Datasets and Base Models
TounderstandhowtheinstructiondatasetslistedinTable1contributetomodelabilities,weevaluated
LLaMa 13B models trained on these datasets using our evaluation suite. Table 3 shows the results on
our benchmark evaluation set, with more extensive results in App. F. We find that:
There is not a single best instruction tuning dataset across all tasks . Different datasets enable
different capabilities in the model. Noteworthy examples include training on CoT being particularly
helpful for mathematical reasoning in GSM and Code-Alpaca being helpful for Codex-Eval. We
hypothesizethatsuccessonthesetasks,whicharesignificantlydifferentfromtherestoftheevaluation
tasks, calls for training sets where these tasks are well-represented. Apart from constructing task-
specificdatasetsmanually,distillingtask-specificdatafromlargemodelsalsoappearstobeaneffective
way to ensure this (e.g., CodeAlpaca is distilled from Davinci-003).
Combining datasets results in the best overall performance on the benchmark tasks. While
models trained on ourcombination datasets are often not thebest model for a single task(being the
bestonlyin2outof6evaluationsettings),theyarethebestwhenmeasuringaverageperformance
across tasks. This suggests that future work into better dataset mixing or instruction-tuning modular
6

--- PAGE 7 ---
Table5: Performanceof T√úLUandotherofourtrainedmodelstovanilla LLAMAmodelsandthe
state-of-the-art proprietary models across evaluation settings. See Table 8 for a complete list.
MMLU
(factuality)GSM
(reasoning)BBH
(reasoning)TydiQA
(multilinguality)Codex-Eval
(coding)AlpacaEval
(open-ended)Average
EM
(0-shot)EM
(8-shot, CoT)EM
(3-shot, CoT)F1
(1-shot, GP)P@10
(0-shot)Win % vs
Davinci-003
Vanilla LLaMa models ‚Üì
LLaMa 7B 31.5 10.0 33.0 38.4 20.5 - -
LLaMa 13B 42.3 14.5 39.3 43.2 28.6 - -
LLaMa 30B 54.6 36.0 49.5 55.3 42.8 - -
LLaMa 65B 58.7 50.0 58.1 56.8 46.9 - -
LLaMa-2 7B 41.8 12.0 39.3 51.2 26.8 - -
LLaMa-2 13B 52.0 25.0 48.9 56.5 32.5 - -
65B models trained on alternate data mixtures ‚Üì
ShareGPT 65B 61.3 (+2.6) 59.0 (+9.0) 55.8 (-2.3) 31.6 (-25.2) 56.2 (+9.3) 73.6 56.3
Human mix. 65B 60.4 (+1.7) 60.0 (+10.0) 54.8 (-3.3) 58.3 (+1.7) 44.6 (-2.3) 43.4 53.6
models trained on our final Human+GPT data mixture ‚Üì
T√úLU
7B 44.8 (+13.3) 25.0 (+15.0) 38.5 (+5.5) 43.5 (+5.1) 29.1 (+8.6) 48.6 38.3
T√úLU
13B 49.3 (+7.0) 40.5 (+26.0) 43.3 (+4.0) 45.6 (+2.4) 35.9 (+7.3) 56.5 45.2
T√úLU
30B 57.7 (+3.1) 53.0 (+17.0) 51.9 (+2.4) 51.9 (-3.4) 48.0 (+5.2) 62.3 54.1
T√úLU
65B 59.2 (+0.5) 59.0 (+9.0) 54.4 (-3.7) 56.6 (-0.2) 49.4 (+2.5) 61.8 56.7
models trained on our final Human+GPT data mixture using LL AMA-2‚Üì
T√úLU-1.1
7B 49.2 (+7.4) 37.0 (+25.0) 44.2 (+4.9) 52.8 (+1.6) 33.9 (+7.1) 57.3 45.7
T√úLU-1.1
13B 52.3 (+0.3) 53.0 (+28.0) 50.6 (+1.7) 58.8 (+2.3) 38.9 (+7.4) 64.0 52.9
Proprietary models ‚Üì
ChatGPT 67.9 76.0 66.1 51.9 88.4 83.6 72.3
GPT-4 82.4 92.5 88.0 70.8 94.1 93.5 86.9
models (e.g., mixture-of-experts [ 40]) is a promising direction for developing models that retain
strong performance across all evaluation settings.
Basemodelqualityisextremelyimportantfordownstreamperformance. Weexaminetheimpact
ofusingdifferentbasemodelsinTable4,comparing LLAMA,OPT[54],andPythia[ 4]modelsof
comparablesizetrainedontheHuman+GPTdatamix. Acrossallevaluationsettings,wefindthat
usingLLAMAperforms best by a significant margin, likely due to the fact that LLAMAis pretrained
onsignificantlymoretokensthantheothermodels(seeTable2). Thissuggeststhatmodelspretrained
on larger(or potentially higher-quality) corporaare preferable as basemodels for instruction tuning.
Thelateradditionof LLAMA-2confirmsthisfindingbyshowingasignificantimprovementcancome
from only the base model upgrade.
Some datasets degrade vanilla model performance. Notably, most datasets we evaluate cause
degradation in performance on GSM and TydiQA over the vanilla base model. We hypothesise
thisisduetodatastyleandquality. Manyofthedatasetsweexaminecontainlittletonoexamples
ofchain-of-thought-stylereasoningandcontainlittletonomultilingualdata. Assuch,trainingon
these datasets likely results in some forgetting of the CoT or multilingual abilities previously held
by the model, resulting in degraded performance. Additionally, we note that self-instruct appears
tocausedegradationsacrossmosttasks,whichwehypothesiseisduetotherelativelypoorquality
of the original self-instruct data, being generated by a weaker model (base GPT-3) than the other
GPT-distilled datasets.
5.2 Pushing the Limits of Open Models
Having established that (a) using a broad mix of data is best, and (b) using LLAMAas the base
model is preferable to other open alternatives, we compare the performance of models trained on the
Human+GPT data mix (T √úLUmodels) across all LL AMAsizes in Table 5. We find that:
Instructiontuningbringslargebenefitsontopof LLAMAmodelsatallsizes. Onaverage,all
LLAMAmodels improve considerably after instruction tuning.
7

--- PAGE 8 ---
Smallermodelsbenefitmostfrominstructiontuning. Wefindthatrelativeimprovementsfrom
instructiontuningarelargestforthesmallestmodels,andshrinkasmodelsgetlarger. Notably,the
65BLLAMAmodel performs comparably or better than the 65B T√úLUmodel on MMLU, BBH,
andTydiQA.Thissuggeststhat instruction-tuningdoesnothelptoenhancestrongcapabilities
already present in the original model , and also highlights that care must be taken during finetuning
to avoid forgetting the base model‚Äôs original capabilities.
T√úLUstilllagsbehindstate-of-the-artproprietarymodels. Despitetheimpressiveperformanceof
T√úLU65B, it lags behind ChatGPT and GPT-4 in all evaluation settings, contrary to prior claims that
modelstrainedontheseopenresourcescanmatchChatGPT[ 56,8]. Wenote wecannotdiscountthe
possibility that either ChatGPT or GPT-4 was trained on significant portions of our evaluation
suite. However, the presence of asignificant gap between T√úLUmodels and ChatGPT matches our
findings in the model and human-based evaluations, which are less likely to be compromised.
5.3 Evaluation of Potential Risks and Harms
ToxiGen ( ‚Üì) TruthfulQA ( ‚Üë)
Model ‚Üì 7B 13B 7B 13B
LLAMA 85.4 82.6 26.2 23.6
+ SuperNI 85.3 77.3 26.7 26.2
+ CoT 63.0 43.9 35.1 35.5
+ Flan V2 77.5 61.4 33.2 33.4
+ Dolly 72.1 78.9 30.1 32.9
+ Open Assistant 1 39.2 5.2 40.9 48.6
+ Self-instruct 89.0 89.3 22.4 22.4
+ Unnatural Inst. 35.8 55.7 27.3 31.7
+ Alpaca 63.2 58.1 33.5 39.8
+ Code-Alpaca 84.3 92.0 25.1 26.7
+ GPT4-Alpaca 3.91.251.256.7
+ Baize 77.2 41.2 42.4 43.9
+ ShareGPT 5.5 2.5 45.3 60.0
+ Human mix. 51.8 76.9 34.1 32.1
+ T√úLU
 10.60.144.6 41.6
ChatGPT 27.7 75.2
GPT-4 10.6 82.3
Table 6: Performance of models on ToxiGen
(%toxicgenerations,lowerisbetter)andTruth-
fulQA (% truthful and informative answers,
higher is better). See Table 9 and Table 10 for
the full breakdown of these two evaluations.We evaluate our models onToxiGen and Truth-
fulQAto measurethedegree towhichdifferent
datasets are likely to yield models that generate
toxic language or misinformation. We find that:
Trendsremainsimilartocapability-focused
benchmarks. SimilarlytotheresultsinSec.4.1,
wefindthatGPT-distilleddatasetsyieldthebest
overallperformanceandthatthereisalargevari-
ance in performance across datasets.
Models trained on GPT-sourced data yield
lesstoxicgenerationsthanGPT .Largermod-
elstrainedonGPT-distilleddataappeartorefuse
to produce toxic generations almost entirely, de-
spite the fact that ChatGPT and GPT-4 produce
toxic generations a non-trivial amount of the
time. Wehypothesisethisisduetoourmodels
overfittingonrefusal-stylebehaviour,refusingto
generate anything moderately toxic, while GPT
modelsbalancerefusalbehaviourwithhelpful-
ness to a greater extent.
TruthfulQA performance does not scale. Un-
like other benchmarks, we find that TruthfulQA
performance doesnot improve with modelsize.
Further examining this, we find that larger mod-
elsdooutputmorecorrectfacts, butalsotendto
hedgeandrefusetogiveinformativeanswersmoreoften,resultinginlittletonooverallimprovements
as model size increases.
5.4 Model-Based Evaluation Results for Open-Ended Generation
We report the AlpacaEval win-rates of our models in Table 7. We find that:
ModelstrainedonmixturesbasedontraditionalNLPdatasetsperformpoorly . CoT,FLAN,and
SuperNI all perform extremely poorly in open-ended instruction following, despite these datasets
providing large improvements to the model capabilities tested in Table 3.
Datasets that encourage long, diverse generations perform best . Intrigued by ShareGPT‚Äôs perfor-
mance,weplottheaveragenumberofuniquetokensinmodelgenerationsagainsttheAlpacaEval
win-rateinFigure2. Wefindthattheevaluationis stronglycorrelatedwiththeaveragenumber
of unique tokens (Pearson correlation of 0.96, ùëù‚â™ 0.05). Given GPT-4‚Äôs strong performance on
other tasks, we do not believe that GPT-4 evaluation is merely counting unique tokens, but this result
highlights how model preference scores do not necessarily reward only model capabilities.
8

--- PAGE 9 ---
Training Dataset ‚Üì7B 13B 30B 65B
SuperNI 2.94.2
CoT 5.06.0
Flan V2 3.13.2
Dolly 11.013.7
Open Assistant 1 51.458.1
Self-instruct 4.05.0
Unnatural Instructions 7.58.4
Alpaca 21.421.9
Code-Alpaca 15.315.8
GPT4-Alpaca 57.363.1
Baize 20.021.9
ShareGPT 62.470.569.173.6
Human mix. 28.735.038.343.4
T√úLU
 48.656.562.361.8
Table7: Win-rate(%)of LLAMAmodelsofvary-
ing sizesfinetuned on the given dataset against
Davinci-003 using AlpacaEval [27].
GPT-4ChatGPTDavinci-003
SuperNICoTFlan V2DollyOpen Assistant 1ShareGPT
Self-instructUnnatural InstructionsAlpacaCode-AlpacaGPT4-AlpacaBaizeHuman mix.Human+GPT mix.
0255075100
04080120160Win%vs Davinci-003(GPT-4Eval)
Avg.#ofUniqueTokensinResponseFigure2: Win-ratescoresof13Bmodels(trained
on different datasets) given by GPT-4 strongly
correlate with the average numbers of unique to-
kens in the model responses (Pearson ùëü= 0.96).
90.1%79.8%68.7%72.3%
ChatGPTT√ºlu 65BT√ºlu 7BT√ºlu 65BAcceptanceRate
(Humanmix.)
Figure 3: Human acceptance
rates for four evaluated models.
T√ºlu65BT√ºlu65BT√ºlu65BChatGPTT√ºlu7BT√ºlu65B(humanmix.)27.7%39.2%45.2%22.8%50.0%20.1%
7.2%24.1%26.8%
20.5%21.1%23.2%
33.1%31.9%29.8%
19.3%11.7%12.3%
19.9%11.1%7.8%Left is clearly betterLeft is slightly betterTie7.2%24.1%26.8%
20.5%21.1%23.2%
33.1%31.9%29.8%
19.3%11.7%12.3%
19.9%11.1%7.8%Left is clearly betterLeft is slightly betterTie7.2%24.1%26.8%
20.5%21.1%23.2%
33.1%31.9%29.8%
19.3%11.7%12.3%
19.9%11.1%7.8%Left is clearly betterLeft is slightly betterTie
7.2%24.1%26.8%
20.5%21.1%23.2%
33.1%31.9%29.8%
19.3%11.7%12.3%
19.9%11.1%7.8%L is clearly betterL is slightly betterTieR is slightly betterR is clearly betterFigure 4: Human preference rates for three comparison pairs of
models.
ShareGPT performs best. We find that ShareGPT consistently performs best across all model
sizes, including models trained on data mixes that include ShareGPT. Models trained on ShareGPT
achievehigher win-ratesthanmodelsover twicetheirsize(e.g., 13BShareGPTvs65B T√úLU). We
hypothesizethisisduetoShareGPT‚Äôsdiversity,size,andthehighaverage #tokensoftargetresponses.
Overall, these results suggest that while model preference evaluation is important, it does not provide
aholisticevaluation of these models. Instead, model preference evaluation should only be included
as part of a larger, more comprehensive evaluation setup.
5.5 Human Evaluation Results for Open-Ended Generation
Finally,weshowthehumanevaluationresultsinFigure4andwereferthereadertoAppendix¬ßG.2for
theinner-annotatoragreement. Wefindthatthe humanevaluationresultslargelycorrelatewiththe
AlpacaEvalandbenchmark-basedevaluation : allevaluationsshowthat65B T√úLUoutperforms
7BT√úLU, suggesting making use of larger base models is important, and there is still a nontrivial
gap in performance between 65B T√úLUand ChatGPT. We also find that making use of distilled
datasets provides a large performance boost , suggesting that human-authored datasets are lacking
in comparison. These observations are also consistent with the acceptability scores in Figure 3.
However, we note that 7B T√úLUoutperforms the human-mix 65B T√úLUin the model preference
evaluation, but if we compare the acceptability scores in Figure 3, the opposite appears true. This is
furtherevidencethatmodelpairwiseevaluationmaynotalwaysrevealmodeldeficiencies. Inthis
case, the 65B human-mix model is more likely to yield acceptable (if not high-quality) responses
than the 7B model.
9

--- PAGE 10 ---
6 Related Work
Instruction Tuning of LMs Finetuning language models on diverse instruction sets alongside
regularsampleshasbeenshowntogreatlyimprovezero-shotperformanceonunseentasks[ 39,51,
49,32,9,48], and serves as a good base for further finetuning in supervised settings [ 31]. Increasing
thenumberofdiverseprompts[ 39],thenumberoftasks[ 48,9],anddiversityofdata[ 56]haveall
been shown to be important to performance. More recently, a growing number of models have made
useofmodel-generatedinstruction-augmenteddata[ 47,23,25,53],mostoftengeneratedorcollected
from larger proprietary models such as ChatGPT or GPT-4 [ 8,15,43,52,36, inter alia]. Despite
the explosion of models and datasets, evaluation remains inconsistent and difficult, with different
evaluationsetupsusedacrossmodels. Priorworkhasexaminedmodelstrainedonvaryingdataset
sources with the aim of identifying ‚Äòthe best mixture‚Äô [ 31,24], but is often limited to examining
onlybenchmarkperformance,andcoversasmallernumberofinstructionsourcesthaninthiswork.
QLoRA[ 14]alsoexplores(quantizedandparameter-efficient)instruction-tuningofrecentmodels
and datasets, but explores a smaller range of models, datasets, and evaluations than this work.
Evaluation of LMs Given the success of LMs on NLP and instruction-following tasks, many
evaluation frameworks have been proposed. Frameworks such as HELM [ 28] and LM Evaluation
Harness[ 17]coverabroadrangeofNLPtasksbutareoftenfocusedonevaluatingthebasemodels
as opposed to instruction-tuned ones. Similar to our work, Chung et al. [9]focus on a series of
benchmark evaluations focused around factuality and reasoning, but largely neglect open-ended
instruction following abilities. Releases of large (closed) proprietary models such as GPT-4 [ 34]
and PaLM v2 [ 2] are often accompanied by comprehensive evaluations over a wide variety of
benchmarks, although both similarly neglect evaluation of open-ended instruction following, and
withoutopenreleasesofpretrainingorinstructiontuningdatathereisnowaytotestforevaluation
data contamination.
Recently,evaluationframeworkssuchasAlpacaEval[ 27]andChatbotArena[ 55]havebeenproposed
toevaluatetheopen-endedinstructionfollowingabilityofLMs,movingbeyondbenchmark-based
evaluations. These either make use of other models (in the case of AlpacaEval) or humans (in the
case of Chatbot Arena) as annotators for judging model generations. We make use of this recent
work and evaluate our models on traditional benchmarks, model-based evaluation, and human-based
evaluation. Concurrenttothiswork,Gudibandeetal. [20]examinemodelstrainedonGPTmodel
outputsandarguethatsuchmodelslearntomimiconlythestyle,notthecontent,oftheirteacherGPT
models. While we similarly find that existing datasets fail to train models close to strong proprietary
models,thediversityofperformanceweobserveacrossdatasetssuggeststhatsignificantperformance
improvementscanbeachievedthroughimitationdata,solongasitcontainsadiverseandwide-ranging
set of skills and domains.
7 Conclusion
In this work, we provide an extensive evaluation of a wide variety of publicly-available resources for
instruction-tuning models, and compare them to the strongest proprietary models currently available.
We find that using strong base models is vital to performance, combining datasets works best on
average (but does result in slight performance drops compared to best performance in specific tasks),
andourstrongestopenmodelsdonotyetmatchChatGPTorGPT-4. Furthermore,webelievethat
our evaluation highlights the need for the continued development of strong base models and broader,
diverse datasets. Finally, we hope that our evaluation and released code and models enable more
comprehensiveevaluationsandspurresearchtoclosethesegapsandshedinsightsonalllargelanguage
models, closed or open.
Acknowledgments
Work at UW was partially supported by the Office of Naval Research under MURI grant N00014-
18-1-2670, Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8650-
23-C-7316andMCSprogramthroughNIWCPacific(N66001-19-2-4031),NSFIIS-2044660,and
a gift from Apple. We thank colleagues at AI2 and UW NLP for their constructive feedback and
intellectual support. We are particularly grateful to Tim Dettmers for his suggestions on efficient
10

--- PAGE 11 ---
inference techniques, and Artidoro Pagnoni for providing the reproduced FLAN V2 dataset. We
alsoacknowledgesupportfromAMDandCSC‚ÄôsLUMIcluster,andtheBeakerteamatAI2,which
provided the essential computational infrastructure for our experiments. Finally, we are sincerely
thankful for the following contributors to our human evaluation: Valentina Pyatkin, Clara Na, Yuling
Gu, Yuchen Lin, Haiyan He, David Graham, Hao Peng, Hyunwoo Kim, Alisa Liu, Youngjae Yu, Tal
August, and Egor Klevak.
References
[1]E.Almazrouei,H.Alobeidli,A.Alshamsi,A.Cappelli,R.Cojocaru,M.Debbah,E.Goffinet,
D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo. Falcon-40B: an open
large language model with state-of-the-art performance. Huggingface Model Release, 2023.
URL https://huggingface.co/tiiuae/falcon-40b .
[2]R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin,A.Passos,S.Shakeri,E.Taropa,P.Bailey,
Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.
[3]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,
T.Henighan, etal. Training ahelpfuland harmlessassistantwithreinforcement learningfrom
human feedback. arXiv preprint arXiv:2204.05862 , 2022.
[4]S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O‚ÄôBrien, E. Hallahan, M. A. Khan,
S.Purohit, U.S.Prashanth,E. Raff,etal. Pythia: A suiteforanalyzing largelanguagemodels
acrosstrainingandscaling. In International Conference on Machine Learning ,pages2397‚Äì2430.
PMLR, 2023.
[5]T. Cai, X. Wang, T. Ma, X.Chen, and D. Zhou. Large language models as tool makers. arXiv
preprint arXiv:2305.17126 , 2023.
[6]S. Chaudhary. Code alpaca: An instruction-following llama model for code generation. GitHub
repository, 2023. URL https://github.com/sahil280114/codealpaca .
[7]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,
N.Joseph,G.Brockman,etal. Evaluatinglargelanguagemodelstrainedoncode. arXiv preprint
arXiv:2107.03374 , 2021.
[8]W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang,
J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-
4 with 90%* chatgpt quality. Blog post, March 2023. URL https://lmsys.org/blog/
2023-03-30-vicuna/ .
[9]H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. De-
hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint
arXiv:2210.11416 , 2022.
[10]J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Nikolaev, and J. Palomaki.
TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypologicallydiverse
languages. TACL, 2020. URL https://arxiv.org/abs/2003.05002 .
[11]K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J.Hilton,R.Nakano,C.Hesse,andJ.Schulman. Trainingverifierstosolvemathwordproblems.
arXiv preprint arXiv:2110.14168 , 2021.
[12]Databricks. Free dolly: Introducing the world‚Äôs first truly open instruction-tuned
llm. Blog post, 2023. URL https://www.databricks.com/blog/2023/04/12/
dolly-first-open-commercially-viable-instruction-tuned-llm .
[13]T.Dettmers,M.Lewis,Y.Belkada,andL.Zettlemoyer. LLM.int8(): 8-bitmatrixmultiplication
for transformers at scale. In Advances in Neural Information Processing Systems , 2022.
[14]T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of
quantized llms. arXiv preprint arXiv:2305.14314 , 2023.
11

--- PAGE 12 ---
[15]N. Ding, Y. Chen, B. Xu, S. Hu, Y. Qin, Z. Liu, M. Sun, and B. Zhou. Ultrachat: A large-
scale auto-generated multi-round dialogue data. GitHub Repository, 2023. URL https:
//github.com/thunlp/ultrachat .
[16]Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B.
Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback.
arXiv preprint arXiv:2305.14387 , 2023.
[17]L.Gao,J.Tow,S.Biderman,S.Black,A.DiPofi,C.Foster,L.Golding,J.Hsu,K.McDonell,
N. Muennighoff, J.Phang, L. Reynolds, E.Tang, A. Thite, B.Wang,K. Wang, and A. Zou. A
framework for few-shot language model evaluation, Sept. 2021. URL https://doi.org/10.
5281/zenodo.5371628 .
[18]X. Geng and H. Liu. Openllama: An open reproduction of llama. GitHub Repository, 2023.
URL https://github.com/openlm-research/open_llama .
[19]X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A
dialoguemodelforacademicresearch. Blogpost,April2023. URL https://bair.berkeley.
edu/blog/2023/04/03/koala/ .
[20]A.Gudibande,E. Wallace,C.Snell,X. Geng,H.Liu,P. Abbeel,S.Levine,and D.Song. The
false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717 , 2023.
[21]T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar. TOXIGEN: Controlling
LanguageModelstoGenerateImpliedandAdversarialToxicity. In ACL,2022. URL https:
//arxiv.org/abs/2203.09509 .
[22]D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-
suring massive multitask language understanding. In International Conference on Learning
Representations (ICLR), 2020.
[23]O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language
models with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.
[24]S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.
Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of
generalization. arXiv preprint arXiv:2212.12017 , 2022.
[25]A. K√∂ksal, T. Schick, A. Korhonen, and H. Sch√ºtze. Longform: Optimizing instruction tuning
for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.
[26]A. K√∂pf, Y. Kilcher, D. von R√ºtte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.
Duc,O.Stanley,R.Nagyfi,etal. Openassistantconversations‚Äìdemocratizinglargelanguage
model alignment. arXiv preprint arXiv:2304.07327 , 2023.
[27]X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto.
Alpacaeval: An automaticevaluatorofinstruction-following models. Github repository,2023.
URL https://github.com/tatsu-lab/alpaca_eval .
[28]P.Liang,R.Bommasani,T.Lee,D.Tsipras,D.Soylu,M.Yasunaga,Y.Zhang,D.Narayanan,
Y.Wu,A.Kumar,B.Newman,B.Yuan,B.Yan,C.Zhang,C.Cosgrove,C.D.Manning,C.R‚Äôe,
D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao,
J. Wang, K. Santhanam, L. J. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. S. Kim, N. Guha,
N.S.Chatterji,O.Khattab,P.Henderson,Q.Huang,R.Chi,S.M.Xie,S.Santurkar,S.Ganguli,
T. Hashimoto, T. F. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and
Y. Koreeda. Holistic evaluation of language models. Annals of the New York Academy of
Sciences, 2022.
[29]S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human false-
hoods. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 3214‚Äì3252, Dublin, Ireland, May 2022. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https:
//aclanthology.org/2022.acl-long.229 .
12

--- PAGE 13 ---
[30]S.Lin,J.Hilton,andO.Evans. Truthfulqa: Measuringhowmodelsmimichumanfalsehoods.
InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 3214‚Äì3252, 2022.
[31]S.Longpre,L.Hou,T.Vu,A.Webson,H.W.Chung,Y.Tay,D.Zhou,Q.V.Le,B.Zoph,J.Wei,
etal. Theflancollection: Designingdataandmethodsforeffectiveinstructiontuning. arXiv
preprint arXiv:2301.13688 , 2023.
[32]S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-Task Generalization via Natural
LanguageCrowdsourcingInstructions. In Annual Meeting of the Association for Computational
Linguistics (ACL), 2022.
[33]MosaicML. Introducingmpt-7b: Anewstandardforopen-source,commerciallyusablellms.
Blog post, 2023. URL https://www.mosaicml.com/blog/mpt-7b .
[34] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.
[35]L.Ouyang, J.Wu, X.Jiang,D. Almeida,C. L.Wainwright,P. Mishkin,C. Zhang,S.Agarwal,
K.Slama,A.Ray,etal. TrainingLanguageModelstoFollowInstructionswithHumanFeedback.
InAdvances in Neural Information Processing Systems (NeurIPS), 2022.
[36]B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint
arXiv:2304.03277 , 2023.
[37]S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training
trillionparametermodels. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis ,SC‚Äô20.IEEEPress,2020. ISBN9781728199986.
[38]J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable
training deep learning models with over 100 billion parameters. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2020.
[39]V.Sanh,A.Webson,C.Raffel,S.Bach,L.Sutawika,Z.Alyafeai,A.Chaffin,A.Stiegler,A.Raja,
M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani,
N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong,
H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A.
Fries,R.Teehan,T.L.Scao,S.Biderman,L.Gao,T.Wolf,andA.M.Rush. MultitaskPrompted
Training Enables Zero-Shot Task Generalization. In International Conference on Learning
Representations (ICLR), 2022.
[40]N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously
largeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer. In International Conference
on Learning Representations , 2017.
[41]A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro,
andetal. Beyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguage
models. arXiv preprint arXiv:2206.04615 , 2022.
[42]M. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,
E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve
them. arXiv preprint arXiv:2210.09261 , 2022.
[43]R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.
Stanford alpaca: An instruction-following llama model. GitHub repository, 2023. URL https:
//github.com/tatsu-lab/stanford_alpaca .
[44]H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozi√®re,N.Goyal,
E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 , 2023.
[45]H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023.
13

--- PAGE 14 ---
[46]P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui. Large language
models are not fair evaluators. arXiv preprint arXiv:2305.17926 , 2023.
[47]Y.Wang,Y.Kordi,S.Mishra,A.Liu,N.A.Smith,D.Khashabi,andH.Hajishirzi. Self-instruct:
Aligninglanguagemodelwithselfgeneratedinstructions. arXiv preprint arXiv:2212.10560 ,
2022.
[48]Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok,
A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-NaturalInstructions: Generalization via
Declarative Instructions on 1600+ Tasks. In EMNLP, 2022.
[49]J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.
Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning
Representations (ICLR), 2022.
[50]J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought
promptingelicitsreasoninginlargelanguagemodels. arXiv preprint arXiv:2201.11903 ,2022.
[51]O.Weller,N.Lourie,M.Gardner,andM.E.Peters. Learningfromtaskdescriptions. In Proceed-
ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,
pages1361‚Äì1375,Online,Nov.2020.AssociationforComputationalLinguistics. doi: 10.18653/
v1/2020.emnlp-main.105. URL https://aclanthology.org/2020.emnlp-main.105 .
[52]C.Xu,D.Guo,N.Duan,andJ.McAuley. Baize: Anopen-sourcechatmodelwithparameter-
efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196 , 2023.
[53]C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empow-
ering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 ,
2023.
[54] S.Zhang, S.Roller, N.Goyal,M. Artetxe,M. Chen,S.Chen, C.Dewan, M.Diab,X. Li,X. V.
Lin,etal. Opt: Openpre-trainedtransformerlanguagemodels. arXiv preprint arXiv:2205.01068 ,
2022.
[55]L. Zheng, Y. Sheng, W.-L. Chiang, H. Zhang, J. E. Gonzalez, and I. Stoica. Chatbot Arena:
Benchmarking LLMs in the Wild with Elo Ratings. Blog post, May 2023. URL https:
//lmsys.org/blog/2023-05-03-arena/ .
[56]C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less
is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.
14

--- PAGE 15 ---
Supplementary Material
A Limitations
Despite the comprehensiveness of our evaluations, we note that we did not exhaustively cover all
possible evaluations: for example, we do not explicitly evaluate models on their multi-turn dialogue
abilitiesnortheirsummarizationabilities. Instead,wefocusonacoresetofcapabilitieswebelieve
important, and cover broad open-ended tasks via our model and human preference-based evaluations.
We also note that we do not cover all possible instruction datasets and open models released re-
cently, due to the computational cost of doing this. Instead, we focus on a wide set of datasets we
believe are broadly representative of the type of open instruction datasets available (human-authored,
skill-targeted, GPT-distilled, etc), and focused on the strongest base model widely available when
performingexperiments. Futureworkcouldinvestigatewhethermorerecentstrongbasemodels(e.g.,
theFalconmodel[ 1]),orotherinstructiondatasets,performsignificantlybetterordifferentlyfrom
the models explored in this work.
Finally, we note that open-ended instruction-based evaluation is highly subjective and difficult due to
its extremely open-ended nature. There is likely no one answer that is definitively the best for any
given query, anddifferentannotators (whethertheybe humanormodel)will havedifferentbiases
and preferences. We also note that in the case of model-based evaluations, we primarily compare our
modeloutputsto Davinci-003generations,whichmayresultinoverlyrewardingmodelsthatavoid
shortcomingsof Davinci-003,ornotproperlyrewardingmodelsthatsharestrengthswith Davinci-003.
Despitenotbeingcompletelyexhaustiveinthiswork,webelievethatbycoveringabroadrangeof
models, it still serves as a useful and important contribution in showing what type of open resources
work,andwherefuturecommunityeffortsshouldgo(betterbasemodels,morediverseinstruction-
tuning datasets).
B Broader Impact
We believe that a rigorous evaluation of existing resources is broadly positive, exposing the strengths
anddeficienciesofcurrentlywidely-availableresources. Furthermore,asallresourcesusedarewidely
available, the harm posed by training these models is fairly small. We do note that training and
releasingespeciallylargeinstruction-tunedmodelswithoutwell-testedguidescarriesadegreeofrisk,
andsuchinitiallyreleaseourlargestmodelswithagatedsetup(requiringuserstoapplyforaccess
and be manually approved) to limit potential harms.
C Instruction Datasets Details
We provide a brief description of all the instruction datasets used (and licenses) below:
‚Ä¢SuperNI:AcollectionofdiverseNLPtaskswithinstructions,createdbyWangetal. [48]. The
dataset uses the Apache-2.0 license.
‚Ä¢CoT:Acollectionofdatasetsannotatedwithchain-of-thoughts[ 50]. WeusetheCoTmixturefrom
theFLANv2collection[ 9],splittingitoutasaseparatedataset. TheFLANmixtureisreleased
under the Apache-2.0 license, although the component datasets may not use this license.
‚Ä¢FlanV2: AcollectionofNLPtasksthatcombinesanumberofexistingNLPdatasetswithvarious
data augmentations, introduced by Chung et al. [9]. The mixture is released under the Apache-2.0
license, although the component datasets may not use this license.
‚Ä¢Dolly: A collectionofinstruction-followingsamplescreated byDatabricksemployees [ 12]. The
dataset is released under the Creative Commons Attribution-ShareAlike 3.0 Unported License.
‚Ä¢Open Assistant 1 : A crowdsourced human-annotated assistant-style conversation corpus, consist-
ing of a large number of sample conversations in a wide variety of languages [ 26]. The dataset is
released under the Apache-2.0 license.
‚Ä¢Self-Instruct : Adatasetofinstruction-followingsamplescreatedbypromptingGPT-3tocreate
new samples given some example instances [ 47]. The dataset is released under the Apache-2.0
license.
15

--- PAGE 16 ---
‚Ä¢UnnaturalInstructions : Adatasetofinstruction-followingsamplescreatedbyprompting Davinci-
002usingthemethodintroducedbyHonovichetal. [23]. ThedatasetisreleasedundertheMIT
license.
‚Ä¢Alpaca: Adatasetcreatedusingaself-instruct-stylemethodwith Davinci-003asthegeneration
model and some over improvements over self-instruct [ 43]. The dataset is released under a
Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license.
‚Ä¢Code-Alpaca : AdatasetcreatedusingtheAlpacamethod,butfocussingoncodegeneration[ 6].
The dataset is released under the Apache-2.0 license.
‚Ä¢GPT-4Alpaca : AdatasetcreatedusingtheAlpacadatasetasinputs,butreplacingtheexample
generationswithgenerationsfromGPT-4[ 36]. Weincludethistoseetheeffectofusingabetter
quality generation model. The dataset is released under the Apache-2.0 license.
‚Ä¢Baize: AdatasetcreatedbypromptChatGPTandlettingitconversewithitself[ 52]. Thedatasetis
released under the GNU General Public License v3.0.
‚Ä¢ShareGPT :Acollectionofuserinteractionswithvariouschatsystemspubliclyshared. Weusethe
‚Äòhtml-cleaned‚Äô variant available at https://huggingface.co/datasets/anon8231489123/
ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset . We then split
longconversations(over2048tokens)intomax-2048tokenchunks,followingtheVicunasetup
[8]. Wedonotdoanyfurtherfilteringofsamples. ThisdatasetisreleasedundertheApache-2.0
license.
We note that the SuperNI and CoT datasets are included in the FLAN V2 collection but only account
for a small portion of our subsampled FLAN V2 dataset.
We also note that we broadly use popular already publicly available instruction-tuning datasets,
andinthecaseofhuman-authoreddatasets,largelyusedatasetscreatedexplicitly(withparticipant
knowledge) for the purpose of training models (e.g., Dolly, Open Assistant 1). As instruction-tuning
data,mostdataisnotlikelytocontainpersonallyidentifyingdetails,althoughwenotethatwedidnot
make an effort to remove offensive content, so our models may produce toxic or harmful generations.
D Model Training Details and Compute
Wetrainallmodelsfortwoepochswithalearningrateof 2ùëí‚àí5(1ùëí‚àí5for30Band65Bmodels),with
no weight decay and a learning rate with linear decay and linear warmup for 3% of the total training
steps. Weuseamaximumsequencelengthof2048(1024for30Band65B),truncatingsampleswhere
necessary. During training, we make use of the DeepSpeed library [ 38] and ZeRO optimizer [ 37]
toallow forlarge-scalemodelfinetuning. Inallcases, wefullyfinetunemodels. Wetrainedmodels
primarily on the CSC LUMI GPU cluster, each node on which contains 4 AMD MI250x GPUs.
E Evaluation Setups
Weprovidefurtherdetailsontheevaluationsetupsusedbelow. Wealsonotethatwereleaseevaluation
code along with our training code to allow easy reproduction.
‚Ä¢MMLU: We use the official MMLU evaluation script and prompts available at https://github.
com/hendrycks/test , with modifications to allow for batch processing. We evaluate using 0
and 5 few-shot examples, following the original setup of MMLU.
‚Ä¢GSM: We evaluate models on the test set of GSM. Following Wei et al. [50], we evaluate with
andwithoutchain-of-thought(CoTvsDirect). Bothsettingsuse8few-shotin-contextexamples
(in the chain-of-thought setting, the few-shot examples are accompanied by chain-of-thoughts).
Because all answers in GSM are numbers, we extract the last number in the model response as the
finalanswer. Toallowforfasterevaluation,werandomlysampled200examplesfromthe1319
testing examples, which we find gives similar performance as the full-set evaluation.
‚Ä¢BBH:WefollowthesetupdescribedintheoriginalpaperSuzgunetal. [42],andevaluatewithand
without chain-of-thought (CoT vs Direct). The officially provided prompts, which have 3 few-shot
in-contextexamplesareusedforbothCoTandDirectsetups. FortheCoTsetup,weextractthefirst
word after the phrase ‚ÄòSo the answer is‚Äô, or the entire response if there is no such substring present.
16

--- PAGE 17 ---
‚Ä¢TydiQAWe follow the setup described in the PaLM 2 technical report [ 2] to evaluate models‚Äô
performanceinansweringmultilingualquestionsundertwosettings: 1)whenthegoldpassagethat
containstheanswerisgiven(GoldP/GP);2)whenthereisnocontextgiven(Closed-Book/CB).
One in-context example is used to familiarize the model with the answering format.
‚Ä¢Codex-Eval We use the HumanEval dataset in the Codex paper [ 7] for evaluating models‚Äô coding
ability. The dataset contains 164 programming problems, where models are prompted to complete
the Python function given its docstring. Following the original paper, we compute unbiased
estimates of pass@k to measure the functional correctness of models‚Äô outputs. We report both
pass@1andpass@10. Thepass@1resultswereobtainedbysamplingwithatemperatureof0.1
and the pass@10 results with a temperature of 0.8.
‚Ä¢ToxiGen We follow the setup in Touvron et al. [45], but use the original set of prompts from
Hartvigsenetal. [21],whicharedesignedtoelicittoxicgenerationsforcertaingroups. Wetake
only the prompts designed to produce toxic language (‚Äòhateful‚Äô prompts) and use 500 prompts
pergroup toreduceevaluationcosts. Forbaselanguage models,wepassin theoriginalToxiGen
promptsunchangedandgreedilydecodeuptothefirstnewline(oramaximumof512tokens). For
instruction-tunedmodels,weplacethepromptinthecorrespondingtemplate,andaskthemodelto
completetheprompt,untilthemodelgeneratesastoptoken(oramaximumof512tokens). We
passthegeneratedtextintoaroberta-largemodeltrainedtodetecttoxiccontentfinetunedaspartof
Hartvigsenetal. [21]5. Wethenreportthepercentageofgenerationsdeemedtoxicbytheclassifier.
‚Ä¢TruthfulQA FollowingTouvronetal. [45],wemainlyusethegenerationsettingofTrutufulQA
[30]. TheTrutufulQAdatasetcontains818questions,whichareusedtopromptthetestedmodelto
generateanswers. WeusethedefaultQApromptformatwith6in-contextQAexamples. Wefollow
theofficialscriptintheirofficialimplemention6todogreedydecodingandanswerpostprocessing.
We also follow their instruction to train two GPT-based classifiers for judging the truthfulness
and informativeness of the model response. We report the rate of the responses being truthful
(%Trutuful),informative(%Informative),andboth(%InformativeandTruthful)asourmetrics.
Following Touvron et al. [45], we only report the (% Informative and Truthful as our primary
metric in the main paper.
‚Ä¢AlpacaEval We use the package provided by Li et al. [27], following the default setup which asks
the evaluated model to generate responses for 805 prompts and employ GPT-4 to compare the
response with Davinci-003. We employ the ‚Äúalpaca_eval_gpt4_0314‚Äùannotator config insteadof
‚Äúalpaca_eval_gpt4‚Äù to make the results reproducible. We allow the evaluated model to generate up
to 8192 tokens, without specifying special stop sequences. The reported win-rate is the percentage
of model generations that GPT-4 reports as being preferred over the generations from Davinci-003.
For all the evaluations, we load models using the 8-bit mode [ 13] provided in the Huggingface
Transformerslibrary,whichwefindspeedsuptheinferencesignificantlyandhasnegligibleimpact
on the final performance. When doing generation, we use greedy decoding and a max length of 512
tokens, unless otherwise specified.
F Overview of All Automatic Evaluation Results
Table 8 presents a compilation of the results of all models trained as part of this work on all the
corecapabilityevaluationbenchmarks. Welistmultiplescenariosforallevaluationsettingsexcept
AlpacaEval,whichhasonesetting. Pleasereferto¬ßEforthemeaningsofthereportedmetrics. We
also calculate an average across benchmarks in Table 8. This is calculated by first calculating a
per-benchmark average by taking the average across scenarios. We then compute the overall average
with each benchmark weighted equally.
Additionally, for safety evaluation, we provide ToxiGen results broken down by group targeted in
Table9forallmodels,fromwhichwecanseesomegroupsarespeciallytargeted,evenafterinstruction
tuning. We all provide full TruthfulQA results in Table 10. The results are broken down into %
informative and % truthful - see Lin et al. [29] for details on these metrics.
5https://huggingface.co/tomh/toxigen_roberta
6https://github.com/sylinrl/TruthfulQA/
17

--- PAGE 18 ---
Table8: Anoverviewoftheperformanceofallmodelsfinetunedforthiswork,alongwithproprietary
models,onselectedbenchmarks. Tocalculatetheaverage,wecalculatetheaverageperbenchmark
and then take the average across these. See App. F for more details.
MMLU GSM BBH TydiQA Codex-Eval AlpacaEval Average
0-shot 5-shot Direct CoT Direct CoT GP CB P@1 P@10 v Davinci-003 -
Proprietary models ‚Üì
GPT-4 82.4 83.9 35.0 92.5 50.9 88.0 70.8 27.6 85.7 94.1 93.5 74.8
ChatGPT 67.9 69.9 32.5 76.0 49.0 66.1 51.9 20.0 72.2 88.4 83.6 63.4
LLaMa 65B finetuning experiments ‚Üì
Vanilla LLaMa 58.7 63.314.0 50.0 46.2 58.156.818.123.5 46.9 - -
ShareGPT 61.362.823.059.0 40.0 55.8 31.6 9.8 30.8 56.2 73.6 48.1
Human mix. 60.4 61.4 8.5 60.0 53.1 54.858.315.9 23.9 44.6 43.4 44.0
H+GPT mix (
 ) 59.2 60.8 10.0 59.0 48.4 54.4 56.6 13.3 29.2 49.4 61.8 47.0
LLaMa 30B finetuning experiments ‚Üì
Vanilla LLaMa 54.6 57.9 12.0 36.0 41.4 49.5 55.3 15.8 22.0 42.8 - -
ShareGPT 54.6 57.5 20.547.5 42.2 51.1 34.6 10.7 28.1 49.8 69.1 44.6
Human mix. 56.5 58.85.5 52.0 46.8 50.6 57.5 14.5 24.8 41.3 38.3 40.4
H+GPT mix (
 )57.758.4 6.0 53.0 47.1 51.9 51.9 13.0 27.2 48.9 62.3 44.9
LLaMa 13B finetuning experiments ‚Üì
Vanilla LLaMa 42.3 46.4 7.0 14.5 37.1 39.3 43.2 11.5 16.2 28.6 - -
SuperNI 49.7 50.3 2.5 4.0 9.4 4.5 50.29.6 8.2 12.9 4.2 20.0
CoT 44.2 45.2 12.540.0 38.7 41.9 47.8 9.1 12.8 23.7 6.0 27.3
Flan V2 50.651.2 3.0 20.0 41.7 40.8 47.2 11.4 9.0 16.8 3.2 24.8
Dolly 45.6 45.1 7.0 18.0 32.3 28.4 46.5 11.612.9 31.0 13.7 25.5
Open Assistant 1 43.3 36.7 5.0 15.0 35.9 39.6 33.4 10.3 16.1 31.9 58.1 32.0
Self-instruct 30.4 32.1 4.5 11.0 33.2 30.7 41.3 8.5 8.7 12.5 5.0 18.6
Unnat. Instruct. 46.4 45.7 5.5 8.0 37.9 33.7 41.0 8.5 14.4 23.9 8.4 23.5
Alpaca 45.0 46.9 7.0 9.5 36.0 36.6 31.1 7.9 14.6 29.9 21.9 25.7
Code-Alpaca 42.5 44.3 4.5 13.5 35.9 35.6 38.9 10.2 21.3 34.2 15.8 26.0
GPT4-Alpaca 46.9 47.1 9.0 16.5 38.2 38.8 23.5 6.2 15.1 36.6 63.1 33.7
Baize 43.7 41.6 5.0 10.0 37.2 38.7 33.6 7.2 15.1 28.7 21.9 25.4
ShareGPT 49.3 47.7 6.0 27.0 23.1 40.4 30.5 7.1 16.1 34.1 70.5 35.2
Human mix. 50.2 51.2 6.0 38.5 43.939.6 47.0 8.8 11.9 25.0 35.0 32.7
H+GPT mix (
 ) 49.3 51.94.540.540.743.345.6 9.2 21.235.9 56.5 37.9
LLaMa-2 13B finetuning experiments ‚Üì
Vanilla LLaMa-2 52.0 55.5 10.0 25.0 41.8 48.9 56.5 17.2 18.1 32.5 - -
H+GPT mix (
 ) 52.3 54.6 5.0 53.0 44.1 50.6 58.8 15.7 23.5 38.9 64.0 43.7
LLaMa 7B finetuning experiments ‚Üì
Vanilla LLaMa 31.5 33.8 5.0 10.0 32.2 33.0 38.4 9.0 11.0 20.5 - -
SuperNI 44.1 43.5 3.0 4.5 37.4 3.3 43.4 7.5 7.0 12.1 2.9 17.6
CoT 41.8 42.2 6.5 27.536.2 33.9 36.3 5.6 8.8 15.7 5.0 22.0
Flan V2 45.4 46.9 3.5 13.0 34.4 36.0 38.5 9.0 9.8 12.9 3.1 21.3
Dolly 38.1 35.0 4.5 5.5 28.3 23.8 39.8 9.711.4 22.5 10.9 20.1
Open Assistant 1 33.0 30.2 6.0 10.0 21.5 31.8 26.8 6.8 10.4 21.7 51.4 25.1
Self-instruct 35.6 32.7 3.5 7.0 31.5 29.4 34.5 7.1 6.2 11.8 4.0 17.3
Unnat. Instruct. 43.1 37.8 3.5 7.0 32.9 32.7 37.3 6.9 9.2 16.8 7.5 20.2
Alpaca 41.6 40.0 7.07.5 34.1 31.2 29.4 7.3 10.4 21.7 21.4 22.7
Code-Alpaca 34.3 33.7 6.5 7.0 31.1 30.6 35.8 9.5 16.6 28.2 15.3 22.0
GPT4-Alpaca 42.2 37.4 6.5 10.5 30.9 32.3 20.6 4.9 13.2 26.2 57.3 28.3
Baize 40.5 38.1 4.0 6.5 31.3 34.0 29.1 6.8 11.5 26.5 20.0 22.4
ShareGPT 44.5 39.5 6.0 9.5 9.7 34.1 22.8 7.2 12.3 21.2 62.4 27.6
Human mix 46.2 48.2 4.5 25.5 38.835.6 43.2 8.0 9.5 20.2 28.7 28.1
H+GPT mix (
 ) 44.8 47.1 7.025.0 38.5 38.5 43.5 8.018.6 29.1 48.6 33.1
LLaMa-2 7B finetuning experiments ‚Üì
Vanilla LLaMa-2 41.8 46.1 8.0 12.0 32.2 39.3 51.2 15.1 13.3 26.8 - -
H+GPT mix (
 ) 49.2 50.5 6.5 37.0 38.6 44.2 52.8 11.9 20.4 33.9 57.3 38.3
Non-LLaMa 7B finetuning experiments ‚Üì
OPT 6.7B 25.0 24.6 7.0 3.0 0.0 28.5 18.8 4.2 0.6 0.9 - -
+H+GPT mix 32.6 33.7 3.0 13.5 30.6 27.9 24.1 3.6 5.2 8.9 25.9 19.6
Pythia 6.9B 25.8 26.2 4.5 3.5 0.0 28.1 25.6 3.6 7.5 13.7 - -
+H+GPT mix 34.8 35.0 4.0 16.0 31.7 29.2 32.8 2.8 14.9 20.9 23.5 22.4
18

--- PAGE 19 ---
Table 9: ToxiGen resultsacross models. Wereportthe percentage of generations deemed toxic bya
separate classifier, broken down by the group the prompt is designed to produce toxic generations
about.
Asian Black Chinese Jewish Latino LGBTQ Ment. Mexican Mid. East Muslim Nat. Amer. Phys. Trans. Women Overall
Proprietary models ‚Üì
GPT-4 6.0 0.031.0 0.04.0 4.0 14.0 4.0 9.0 5.0 0.0 8.0 3.0 61.0 10.6
ChatGPT 2.0 16.0 33.0 2.0 11.0 27.0 40.0 17.0 16.0 35.0 0.0 78.0 27.0 84.0 27.7
LLaMa 65B models ‚Üì
LLaMa 66.4 99.4 66.2 97.0 86.8 83.6 96.0 90.6 96.0 92.2 100.0 78.6 64.2 78.6 85.4
ShareGPT 0.0 0.0 0.0 0.0 0.0 0.2 2.00.0 0.0 0.2 0.0 4.2 1.0 0.8 0.6
Human mix. 39.8 13.0 54.2 7.4 21.6 17.0 49.0 36.2 4.8 8.6 14.0 16.0 13.6 58.4 25.3
H+GPT mix (
 )0.0 0.0 9.20.0 0.0 9.0 25.0 4.6 3.2 1.8 0.0 18.8 9.6 26.2 7.7
LLaMa 30B models ‚Üì
LLaMa 71.2 98.2 72.8 97.4 66.6 79.6 98.6 92.8 96.0 92.0 100.0 86.4 58.4 90.4 85.7
ShareGPT 0.0 0.0 0.0 0.0 0.0 0.2 1.20.0 0.0 0.0 0.0 0.0 0.0 0.40.1
Human mix. 17.8 45.0 21.0 32.0 72.4 22.0 68.0 72.4 15.6 3.2 12.4 26.4 32.8 41.4 34.5
H+GPT mix (
 )0.0 0.0 4.40.01.2 3.0 8.4 0.8 0.6 2.8 0.0 2.2 1.4 17.4 3.0
LLaMa 13B models ‚Üì
LLaMa 39.2 90.6 81.6 85.8 64.6 76.6 98.8 89.0 97.0 97.0 100.0 90.0 67.8 78.6 82.6
SuperNI 56.6 97.2 88.8 87.2 95.8 74.6 45.6 96.6 87.4 39.6 78.2 76.2 79.2 79.2 77.3
CoT 13.8 54.0 37.0 42.8 62.4 59.8 25.0 71.0 32.0 43.6 51.0 21.0 58.8 42.2 43.9
Flan V2 39.8 70.6 39.4 46.0 81.8 59.6 89.0 55.8 55.2 33.2 85.8 56.6 76.0 70.6 61.4
Dolly 99.6 79.8 87.2 93.0 100.0 87.0 53.8 96.2 68.8 60.4 97.2 50.0 73.2 57.8 78.9
Open Assistant 1 0.8 0.00.80.0 0.0 27.0 11.4 2.8 1.2 1.2 0.6 5.8 20.4 0.4 5.2
Self-Instruct 98.4 99.6 57.8 95.2 89.8 86.6 97.4 96.0 95.4 76.8 100.0 78.8 80.0 97.8 89.3
Unnat. Instruct. 37.6 82.2 55.4 97.4 24.0 38.0 74.8 67.2 40.8 26.0 74.6 47.4 57.0 57.8 55.7
Alpaca 86.8 39.0 94.2 56.2 76.0 61.6 30.2 73.0 59.0 50.2 13.2 56.0 46.2 71.4 58.1
Code-Alpaca 100.0 81.6 98.0 100.0 100.0 96.4 77.8 95.8 87.8 90.6 100.0 75.0 93.6 92.0 92.0
GPT4-Alpaca 0.4 0.00.20.03.8 4.6 1.6 1.4 0.0 0.0 0.0 0.4 3.4 1.0 1.2
Baize 46.2 12.2 83.4 6.6 58.2 47.4 52.6 10.4 20.8 34.2 44.8 47.6 32.2 80.2 41.2
ShareGPT 0.0 0.0 5.40.0 0.0 3.2 5.4 0.0 1.6 2.6 0.0 1.6 6.2 9.4 2.5
Human mix. 70.8 92.4 74.4 84.6 92.4 63.2 94.8 71.4 79.8 49.8 98.6 61.2 62.0 80.8 76.9
H+GPT mix (
 )0.0 0.0 0.0 0.0 0.0 0.60.0 0.0 0.0 0.0 0.0 0.0 1.20.0 0.1
LLaMa-2 13B models ‚Üì
LLaMa-2 58.8 89.6 88.2 97.8 81.6 71.0 96.4 93.2 92.6 91.4 100.0 91.0 63.8 84.0 85.7
H+GPT mix (
 )0.016.4 3.8 3.8 44.6 22.8 23.0 39.4 5.8 9.0 49.6 14.8 6.4 22.8 18.7
LLaMa 7B models ‚Üì
LLaMa 43.6 94.8 85.4 91.2 96.6 75.4 98.8 91.2 95.0 89.8 100.0 92.8 63.6 77.0 85.4
SuperNI 99.4 98.2 91.8 89.8 92.4 77.0 65.4 93.8 85.0 87.6 87.2 75.8 80.2 70.0 85.3
CoT 77.4 89.0 58.2 55.8 87.8 51.4 68.8 68.2 60.8 57.6 53.8 46.8 43.0 64.0 63.0
Flan V2 54.0 68.6 89.2 92.2 54.4 75.0 80.0 87.8 88.2 83.6 96.6 68.8 69.2 77.6 77.5
Dolly 90.2 90.6 83.8 98.8 94.0 82.4 66.6 93.0 56.0 41.2 1.2 55.8 68.2 88.0 72.1
Open Assistant 1 8.0 17.6 53.8 95.2 12.2 40.8 33.6 55.6 27.2 22.6 35.4 45.0 29.2 72.0 39.2
Self-Instruct 100.0 94.8 73.4 88.4 88.0 89.6 75.4 95.8 91.2 76.4 98.6 87.8 86.8 99.4 89.0
Unnat. Instruct. 4.0 13.0 25.8 81.4 8.2 29.4 89.8 9.8 14.2 12.4 55.6 19.6 75.0 62.4 35.8
Alpaca 97.0 40.8 97.2 79.8 51.4 69.6 48.2 67.6 54.0 57.2 37.4 57.4 45.4 81.2 63.2
Code-Alpaca 98.6 80.2 99.2 100.0 91.6 88.8 60.8 99.4 83.0 69.8 66.8 79.6 72.8 90.0 84.3
GPT4-Alpaca 6.8 0.4 14.6 2.0 0.06.2 2.2 3.2 0.8 2.2 0.0 3.8 2.6 9.8 3.9
Baize 99.8 57.8 89.4 95.2 81.6 81.0 78.6 47.2 66.2 68.6 86.4 65.0 66.6 97.6 77.2
ShareGPT 0.0 0.0 12.0 0.00.8 5.4 1.0 0.4 0.6 3.6 0.4 21.6 5.6 26.0 5.5
Human mix. 20.4 74.6 54.4 61.6 53.4 40.4 63.0 68.0 55.2 44.6 50.4 38.8 24.4 76.0 51.8
H+GPT mix (
 ) 0.2 0.8 3.6 0.4 0.01.8 26.4 2.8 0.2 3.2 75.6 15.0 0.018.4 10.6
LLaMa-2 13B models ‚Üì
LLaMa-2 51.0 96.8 86.8 28.4 32.6 78.6 95.4 92.2 93.8 88.6 94.4 90.4 85.2 68.6 77.3
H+GPT mix (
 ) 21.8 59.0 71.0 18.4 23.2 15.4 74.2 60.8 39.2 3.6 45.2 21.0 14.6 90.8 39.9
Non-LLaMa 7B models ‚Üì
OPT 52.8 96.6 74.8 85.6 77.6 71.6 97.6 96.4 94.8 91.4 97.6 93.6 68.8 67.2 83.3
+ H+GPT mix 63.6 83.0 68.2 48.2 21.8 39.2 54.4 43.8 43.4 28.6 73.2 72.2 35.8 75.6 53.6
Pythia 82.2 99.6 70.6 75.0 85.6 65.8 97.6 93.8 94.2 84.4 98.6 88.4 67.2 54.2 82.7
+ H+GPT mix 37.4 72.4 94.6 58.4 54.6 36.8 78.8 47.2 55.4 43.8 39.4 68.4 37.2 72.4 56.9
19

--- PAGE 20 ---
Table10: TruthfulQAresultsacrossmodels. Wereportpercentageofanswersthatareinformative,
or truthful, or both.
% Informative % Truthful% Informative
and Truthful
Proprietary models ‚Üì
GPT-4 99.5 82.7 82.3
ChatGPT 96.0 79.2 75.2
LLaMa 65B models ‚Üì
Vanilla LLaMa 85.8 45.2 31.2
ShareGPT 86.8 76.6 63.5
Human mix 98.0 42.2 40.4
H+GPT mix (
 ) 90.5 58.3 48.7
LLaMa 30B models ‚Üì
Vanilla LLaMa 92.0 35.7 28.3
ShareGPT 71.0 81.4 52.5
Human mix 98.2 43.2 41.5
H+GPT mix (
 ) 92.8 53.2 46.0
LLaMa 13B models ‚Üì
Vanilla LLaMa 95.1 30.8 26.2
SuperNI 96.8 27.8 25.1
CoT 92.7 41.6 35.5
Flan V2 91.2 42.1 33.4
Dolly 98.8 34.1 32.9
Open Assistant 1 91.3 57.2 48.6
ShareGPT 91.2 68.5 60.0
Self-instruct 93.4 28.8 22.4
Unnat. Instruct. 84.6 46.9 31.7
Alpaca 99.9 39.9 39.8
Code-Alpaca 98.9 27.5 26.7
GPT4-Alpaca 87.5 69.0 56.7
Baize 87.9 56.1 43.9
Human mix. 98.4 33.3 32.1
H+GPT mix (
 ) 94.6 47.0 41.6
LLaMa-2 13B models ‚Üì
Vanilla LLaMa 2 99.0 32.1 31.1
H+GPT mix (
 ) 96.7 48.3 45.3
LLaMa 7B models ‚Üì
Vanilla LLaMa 96.7 26.4 23.6
SuperNI 98.0 28.4 26.7
CoT 93.5 40.3 35.1
Flan V2 96.1 36.1 33.2
Dolly 98.5 31.5 30.1
Open Assistant 1 92.0 48.5 40.9
ShareGPT 76.4 68.5 45.3
Self-instruct 96.5 25.5 22.4
Unnat. Instruct. 89.8 37.0 27.3
Alpaca 98.8 34.8 33.5
Code-Alpaca 99.1 25.9 25.1
GPT4-Alpaca 84.2 66.7 51.2
Baize 88.5 53.7 42.4
Human mix 97.7 36.2 34.1
H+GPT mix (
 ) 98.2 46.3 44.6
LLaMa-2 7B models ‚Üì
Vanilla LLaMa 2 93.0 33.4 26.7
H+GPT mix (
 ) 97.7 43.2 40.0
20

--- PAGE 21 ---
G Human Evaluation Details
G.1 Setup
Hereweprovidemoredetailsforthehumanevaluationdescribedin¬ß4.3. Ourevaluationcontains332
instructions, including 252 instructions from the Self-Instruct evaluation set [ 47] and 80 instructions
fromtheVicunaevaluationset[ 8]. Ourevaluationisconductedforthreepairsofmodels: 1) T√úLU
65B vs ChatGPT, 2) T√úLU65B vsT√úLU7B, 3)T√úLU65B v.s. a 65B LLAMAmodel trained on the
Human data mixture, using the same set of instructions for all three comparisons.
To ensure reliable evaluation, we recruited 18 expert annotators, which are researchers at AI2 or
studentsatUW,fortheannotation. AlltheseannotatorsarefluentEnglishspeakersandholdbachelor‚Äôs
degrees or above.
We design a website, shown in Figure 5, for our annotators to conduct the evaluation, and we will
releasethecodeforthiswebsite. Whendoingtheevaluation,annotatorsareinstructedtoreadcarefully
the prompt and outputs A and B from two models, and then answer three questions asking for the
acceptance of the outputs and their comparison in terms of helpfulness. They are encouraged to use
Google or any external tools that can help with the judgment. The model information is anonymized,
and their outputs are put in random order.
Figure 5: The website interface for our human evaluation (see App. G for details). Users need to log
in to the system, read the prompt and outputs fromtwo models (with model names anonymizedand
orderrandomized),thenanswerwhetheroutputAandoutputBareacceptableornotindividually,
and finally compare them in terms of helpfulness.
G.2 Inter-Annotator Agreement
We measure the agreement of our annotators on a subset of 119 examples (63 instances randomly
sampledfromtheChatGPT3vs T√úLU65Bcomparison,and59instancesrandomlysampledfrom
theT√úLU65BvsT√úLU7Bcomparison). Weassigntwoannotatorsforeachoftheseexamplesand
compute their agreement for both the acceptance evaluation and pairwise comparison evaluation.
21

--- PAGE 22 ---
Theannotatorsachieve anagreementof0.84forwhetheramodeloutputshouldbeacceptedornot.
For the pairwise comparison, following Zhou et al. [56], we report a tie-discounted accuracy, which
assigns one point if both annotators agreed, half a point if either annotator (but not both) labeled a tie,
and zero point otherwise. We also merged ‚Äúclearly better‚Äù and ‚Äúslightly better‚Äù together, so our final
options will be simply comparing which of A and B is better, or a tie. Our annotators achieved an
agreement of 0.72 for this pairwise comparison.
Although these numbers show reasonable agreement, we also note that there is a large extent of
subjectivity in human evaluation. This noise level also indicates that some prior work [ 8,55] that
uses a small number of examples for human evaluation might not be reliable enough. We suggest
thatthecommunityneedstofurtherimprovethereliabilityandscalabilityofhumanevaluationfor
instruction-following models.
H Further Investigation of Figure 2
To furtherinvestigate the degreeto which thenumber ofunique tokens isbeing used byGPT-4 as a
markerofquality, wecreatedadummyevaluatorthatcomparestwomodeloutputs, andassignsawin
tothe outputwith moreuniquetokens. Weplotthe win-ratecalculated usingthisdummyevaluator
against the win-rate calculated using GPT-4 in Figure 6.
We find that while the dummy evaluator generally over-estimates the win-rate, the trend is still
strikingly linear. Wenote thatthe ùëÖ2for thetrendline is.91, suggesting thatthe uniquetoken count
explains a large proportion of the variance in the win rates. Based on this, we believe that the number
of unique tokens is certainly a key preference that GPT-4 cares about in its evaluation, although it is
still not the only important feature.
Dummy Evaluator Winrate (%)
0.00%25.00%50.00%75.00%100.00%
0.00% 25.00% 50.00% 75.00%ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Figure6: Win-ratescoresofallmodelsjudgedbythedummyevaluatoragainstwin-rateofallmodels
using the GPT-4 evaluator.
I Model Licenses
Weprovidebriefinformationaboutthelicensesoftheunderlyingmodelswemakeuseofinthiswork
below.
‚Ä¢LLAMA: TheLLAMAmodelweightsarereleasedunderacustomlicensethatallowsusing
the model for non-commercial research purposes.
‚Ä¢LLAMA-2: The LL AMA-2 model weights arereleased under a custom license that allows
for commercial and research uses with some limitations (e.g., having less than 700 mil-
lion monthly active users if used in a commercial application), and explicitly allows for
redistribution of the weights.
22

--- PAGE 23 ---
‚Ä¢Pythia: The Pythia weights are released under the Apache-2.0 license.
‚Ä¢OPT: The OPT model weights are released under a custom license that allow only using the
model for non-commercial research purposes.
23
