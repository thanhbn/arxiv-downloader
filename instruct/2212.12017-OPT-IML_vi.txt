# OPT-IML: Mở rộng Siêu Học Hướng dẫn Mô hình Ngôn ngữ qua Góc độ Tổng quát hóa

Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu,
Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyrayy, Jeﬀ Wang,
Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanovy
Meta AI

## Tóm tắt

Các nghiên cứu gần đây đã chỉ ra rằng việc tinh chỉnh các mô hình ngôn ngữ lớn được pre-train trước trên một tập hợp các nhiệm vụ được mô tả thông qua các hướng dẫn, hay còn gọi là instruction-tuning, cải thiện khả năng tổng quát hóa zero và few-shot của chúng đối với các nhiệm vụ chưa thấy. Tuy nhiên, vẫn còn hiểu biết hạn chế về sự đánh đổi hiệu suất của các quyết định khác nhau được đưa ra trong quá trình instruction-tuning. Những quyết định này bao gồm quy mô và tính đa dạng của benchmark instruction-tuning, các chiến lược lấy mẫu nhiệm vụ khác nhau, tinh chỉnh có và không có demonstrations, huấn luyện sử dụng các dataset chuyên biệt cho reasoning và dialogue, và cuối cùng, chính các mục tiêu tinh chỉnh. Trong bài báo này, chúng tôi đặc trưng hóa tác động của các quyết định instruction-tuning đối với hiệu suất nhiệm vụ downstream khi mở rộng cả kích thước mô hình và benchmark. Để đạt được điều này, chúng tôi tạo ra OPT-IML Bench: một benchmark lớn cho Instruction Meta-Learning (IML) gồm 2000 nhiệm vụ NLP được hợp nhất thành các danh mục nhiệm vụ từ 8 benchmark hiện có, và chuẩn bị một framework đánh giá để đo lường ba loại tổng quát hóa mô hình: đối với các nhiệm vụ từ các danh mục hoàn toàn held-out, đối với các nhiệm vụ held-out từ các danh mục đã thấy, và đối với các instances held-out từ các nhiệm vụ đã thấy. Thông qua góc độ của framework này, trước tiên chúng tôi trình bày những insights về các quyết định instruction-tuning áp dụng cho OPT-30B và tiếp tục khai thác những insights này để huấn luyện OPT-IML 30B và 175B, là các phiên bản instruction-tuned của OPT. OPT-IML thể hiện cả ba khả năng tổng quát hóa ở cả hai quy mô trên bốn benchmark đánh giá khác nhau với các nhiệm vụ và định dạng đầu vào đa dạng – PromptSource, FLAN, Super-NaturalInstructions, và UniﬁedSKG. Không chỉ vượt trội đáng kể so với OPT trên tất cả benchmark mà còn có tính cạnh tranh cao với các mô hình hiện có được tinh chỉnh trên từng benchmark cụ thể. Chúng tôi phát hành OPT-IML ở cả hai quy mô, cùng với framework đánh giá OPT-IML Bench.

## 1. Giới thiệu

Instruction fine-tuning được chỉ ra (Wei et al., 2022a; Sanh et al., 2022; Chung et al., 2022a) cải thiện đáng kể hiệu suất zero- và few-shot của các LLM lớn được pre-train trước. Nó bao gồm việc tinh chỉnh LLM trên các tập hợp các nhiệm vụ NLP sử dụng các định dạng đầu vào kiểu hướng dẫn. Instruction-tuning thành công của LLM phụ thuộc vào một số khía cạnh như các mục tiêu được sử dụng để tinh chỉnh, phân phối và tính đa dạng của các nhiệm vụ tinh chỉnh, việc bao gồm các dataset chuyên biệt liên quan đến reasoning và dialogue, tinh chỉnh với demonstrations, và cũng như tính toàn diện của framework đánh giá. Trong bài báo này, chúng tôi phát triển một framework tinh chỉnh và đánh giá quy mô lớn rộng rãi gồm 2000 nhiệm vụ NLP (mà chúng tôi gọi là OPT-IML Bench) và sử dụng nó để đặc trưng hóa sự đánh đổi của các quyết định khác nhau liên quan đến instruction meta-learning (IML) trên các mô hình OPT (Zhang et al., 2022). Chúng tôi khai thác những insights thu được từ quá trình này để huấn luyện OPT-IML 30B và 175B, các phiên bản instruction-tuned của OPT.

Có một số lượng ngày càng tăng các meta-dataset lớn của các nhiệm vụ NLP như Super-NaturalInstructions (Wang et al., 2022), FLAN (Wei et al., 2022a) và PromptSource (Sanh et al., 2022). Các nghiên cứu instruction-tuning gần đây đã chứng minh thành công khi sử dụng những benchmark riêng lẻ này và sự kết hợp của chúng (Chung et al., 2022b), với khuyến nghị chung là mở rộng số lượng nhiệm vụ.

[Hình 1: Chúng tôi tinh chỉnh OPT trên một tập hợp lớn gồm 1500+ nhiệm vụ NLP được chia thành các danh mục nhiệm vụ (phía bên trái) để tạo ra OPT-IML. Mỗi danh mục chứa nhiều nhiệm vụ liên quan, cũng như nhiều prompt cho cùng một nhiệm vụ (ví dụ: IMDB), được tổng hợp từ nhiều benchmark. Chúng tôi đánh giá OPT-IML trên một tập hợp các danh mục đánh giá (phía bên phải) có thể rời rạc, chồng lấp một phần hoặc chồng lấp hoàn toàn với các danh mục được sử dụng để tinh chỉnh (ví dụ: Sentiment Analysis chồng lấp hoàn toàn và QA chồng lấp một phần), tương ứng với việc đánh giá khả năng tổng quát hóa của mô hình đối với các nhiệm vụ từ các danh mục hoàn toàn held-out, đối với các nhiệm vụ từ các danh mục đã thấy trong quá trình huấn luyện, và đối với các instances từ các nhiệm vụ đã thấy trong quá trình huấn luyện. Chúng tôi phát hành framework đánh giá này như OPT-IML Bench.]

Chúng tôi tuân theo khuyến nghị này bằng cách hợp nhất 8 meta-dataset thành một tập hợp lớn gồm 1,991 nhiệm vụ NLP chứa các hướng dẫn với nhiều prompt và nhóm chúng thành hơn 100 danh mục nhiệm vụ như Question Answering và Sentiment Analysis (Hình 1). Hơn nữa, chúng tôi chuyển đổi tập hợp này thành một framework đánh giá để đánh giá toàn diện các mô hình instruction-tuned quy mô lớn qua ba mức độ tổng quát hóa: 1) hiệu suất mô hình trên các nhiệm vụ từ các danh mục nhiệm vụ hoàn toàn held-out không được sử dụng để tinh chỉnh, như trong các nghiên cứu trước (Wei et al., 2022a; Sanh et al., 2022), và bổ sung, 2) hiệu suất trên các nhiệm vụ chưa thấy từ các danh mục đã thấy trong quá trình instruction-tuning, và, 3) hiệu suất trên các instances held-out của các nhiệm vụ đã thấy trong quá trình tinh chỉnh. Hai setting đầu đánh giá khả năng tổng quát hóa cross-task của instruction-tuning trong khi setting cuối đánh giá khả năng tổng quát hóa của supervised multi-task learning (McCann et al., 2018). Chúng tôi gọi framework instruction-tuning kết quả là OPT-IML Bench và minh họa thành phần của nó trong Hình 1 nơi phía bên phải mô tả các danh mục đánh giá, có thể hoàn toàn rời rạc, chồng lấp một phần, hoặc chồng lấp hoàn toàn với các danh mục được sử dụng để tinh chỉnh ở phía bên trái. Mỗi danh mục bao gồm các dataset có thể thuộc về nhiều benchmark và được liên kết với nhiều prompt.

Hiệu quả của instruction-tuning trên LLM phụ thuộc vào các yếu tố như tính đa dạng và phân phối của các nhiệm vụ tinh chỉnh, định dạng của các prompt của chúng, và các mục tiêu được sử dụng để tinh chỉnh. Một số nghiên cứu gần đây về instruction-tuning khám phá những yếu tố này bằng cách nhóm các nhiệm vụ thành các danh mục và đánh giá hiệu suất trên các nhiệm vụ từ các danh mục nhiệm vụ hoàn toàn held-out (Sanh et al., 2022; Wei et al., 2022a; Wang et al., 2022). Sử dụng framework đánh giá của chúng tôi xem xét nhiều mức độ tổng quát hóa, chúng tôi có thể đặc trưng hóa toàn diện sự đánh đổi liên quan đến những yếu tố khác nhau này khi mở rộng instruction-tuning lên tổng hợp của 8 benchmark khác nhau. Bằng cách instruction tuning OPT 30B (Zhang et al., 2022) trên OPT-IML Bench, chúng tôi phác thảo sự đánh đổi của các chiến lược lấy mẫu dataset và benchmark trong quá trình tinh chỉnh, các quy luật mở rộng đối với các nhiệm vụ và danh mục, tác động của các phương pháp kết hợp demonstrations nhiệm vụ vào instruction-tuning dựa trên Min et al. (2021), cũng như instruction-tuning với các dataset chuyên biệt chứa các chuỗi reasoning (Kojima et al., 2022; Wei et al., 2022b) và dialogue (Shuster et al., 2022). Những thí nghiệm này có thể phục vụ để thiết lập các thực hành tốt nhất cho instruction-tuning quy mô lớn của LLM.

Dựa trên những insights thu được từ các thí nghiệm tổng quát hóa của chúng tôi trên OPT-IML bench, chúng tôi huấn luyện OPT-IML. OPT-IML cải thiện đáng kể so với mô hình pre-trained gốc của nó ở cả quy mô 30B và 175B trên bốn benchmark instruction-tuning khác nhau: PromptSource (Sanh et al., 2022), FLAN (Wei et al., 2022a), Super-NaturalInstructions (Wang et al., 2022), và UniﬁedSKG (Xie et al., 2022). Bổ sung, các mô hình OPT-IML cũng thể hiện hiệu suất cạnh tranh so với từng mô hình instruction-tuned trước đây được tinh chỉnh riêng lẻ trên những benchmark này về cả hiệu suất zero và few-shot. Gần đây, theo hướng tương tự như nghiên cứu này, Chung et al. (2022b) đạt được những cải thiện ấn tượng trên các benchmark thách thức của MMLU (Hendrycks et al., 2020) và Big-BenchHard (Suzgun et al., 2022) bằng cách instruction-tuning PaLM (Chowdhery et al., 2022) và T5 (Raﬀel et al., 2020) trên một tập hợp mở rộng gồm 1.8K nhiệm vụ. OPT-IML được huấn luyện trong các setting tương tự vẫn kém hiệu suất so với các benchmark thách thức này và chúng tôi thảo luận điều này trong Phần 6. Theo OPT (Zhang et al., 2022), chúng tôi sẽ chia sẻ có trách nhiệm các phiên bản của OPT-IML ở cả hai quy mô, và cũng phát hành framework đánh giá OPT-IML Bench của chúng tôi để tạo thuận lợi cho các nghiên cứu tương lai theo hướng này.

## 2. Mở rộng các Benchmark Đa nhiệm vụ

Để đặc trưng hóa tác động của việc mở rộng nhiệm vụ cực đại trên instruction tuning, chúng tôi xây dựng dựa trên các tập hợp nhiệm vụ gần đây như Super-NaturalInstructions (Wang et al., 2022) và PromptSource (Sanh et al., 2022), và tổng hợp 8 tập hợp như vậy để tạo ra OPT-IML Benchmark cho instruction fine-tuning và đánh giá quy mô lớn trên các danh mục nhiệm vụ đa dạng, loại hướng dẫn và setup prompting (Bảng 1).

Trong phần còn lại của bài báo này, chúng tôi sử dụng các thuật ngữ nhiệm vụ và dataset thay thế cho nhau; mỗi nhiệm vụ/dataset có thể được khởi tạo sử dụng nhiều template prompt. Chúng tôi gọi dữ liệu gốc mà từ đó các nhiệm vụ được tạo ra là nguồn dữ liệu; nhiều nhiệm vụ có thể được tạo ra từ cùng một nguồn dữ liệu (ví dụ: question answering và question rewriting). Một benchmark bao gồm nhiều nhiệm vụ, trong đó mỗi nhiệm vụ thuộc về một danh mục/cụm nhiệm vụ duy nhất.

### 2.1 Curation Nhiệm vụ

Chúng tôi mở rộng benchmark Super-NaturalInstructions gồm 1600+ nhiệm vụ của Wang et al. (2022) với các tập hợp nhiệm vụ từ nhiều nghiên cứu hiện có về instruction-tuning: FLAN (Wei et al., 2022a), T0 (Sanh et al., 2022); prompt crowdsourcing: PromptSource (Bach et al., 2022); các nghiên cứu transfer cross-task: ExMix (Aribandi et al., 2022), T5 (Raﬀel et al., 2020), CrossFit (Ye et al., 2021); và hợp nhất nhiệm vụ theo lĩnh vực cụ thể: Structured Knowledge Grounding (Xie et al., 2022), Dialogue (Shuster et al., 2022) và Chain-of-thought Reasoning¹ (Chung et al., 2022b). Quá trình curation của tất cả những benchmark này có thể được tìm thấy trong Phụ lục A.1.

Có sự chồng lấp đáng kể giữa các dataset trong những benchmark này. Ví dụ, các dataset phổ biến như SQuAD v1/v2 (Rajpurkar et al., 2016, 2018) xuất hiện trong hầu hết tất cả các benchmark. Bổ sung, trong khi Super-NaturalInstructions, PromptSource, FLAN và Chain-of-thought Reasoning chứa các hướng dẫn dài được viết bởi con người hoặc các chuỗi reasoning, phần còn lại của các benchmark được thiết kế cho multi-task learning và các template prompt thường chỉ bao gồm các prefix ngắn cho field hoặc nhiệm vụ (ví dụ: "question:", "label:"). Do đó, chúng tôi chỉ giữ lại các nhiệm vụ từ các tập hợp CrossFit, ExMix và T5 không xuất hiện trong bất kỳ benchmark nào khác. Vì chúng tôi đang khám phá một số lượng lớn các nhiệm vụ, chúng tôi lấy tối đa 100k ví dụ (ngẫu nhiên) trên mỗi nhiệm vụ từ tất cả benchmark ngoại trừ FLAN, nơi chúng tôi lấy tối đa 30k ví dụ trên mỗi nhiệm vụ theo cùng thực hành như Wei et al. (2022a).

[Bảng 1: Chi tiết của OPT-IML Bench. Thống kê của mỗi benchmark hiện có được tính toán sử dụng dữ liệu gốc mà chúng tôi đã tải xuống. Thống kê của OPT-IML Bench được tính toán sử dụng dữ liệu sau khi chúng tôi thực hiện lọc nhiệm vụ và lấy tối đa M ví dụ trên mỗi nhiệm vụ. Đối với tất cả benchmark ngoại trừ FLAN, chúng tôi đặt M= 100k; đối với FLAN, chúng tôi đặt M= 30k theo Wei et al. (2022a). †Chúng tôi chỉ thống nhất thủ công việc phân loại nhiệm vụ trong các tập đánh giá của chúng tôi. Ước tính số lượng cụm nhiệm vụ trong tập train của chúng tôi dựa trên sự hợp nhất thô của các tag clustering từ mỗi benchmark gốc.]

### 2.2 Hợp nhất Benchmark

**Schema hướng dẫn.** Mỗi benchmark áp dụng các hướng dẫn và kiểu ngôn ngữ khác nhau. Trong Bảng 2, chúng tôi phân loại rộng rãi các hướng dẫn của chúng thành hai danh mục: dataset-level và instance-level. Hướng dẫn dataset-level định nghĩa nhiệm vụ tổng thể và có thể bao gồm thông tin phụ trợ như ví dụ positive/negative và giải thích. Mô hình được kỳ vọng học định nghĩa của nhiệm vụ dựa trên điều này và áp dụng kiến thức vào từng ví dụ đến sau đó. Hướng dẫn instance-level là các template được khởi tạo cho từng ví dụ riêng lẻ và đôi khi được thiết kế theo kiểu cloze-style để khai thác đầu ra mong muốn cho ví dụ. Chúng tôi đưa tất cả các nhiệm vụ qua các benchmark mà chúng tôi thu thập vào công thức prompt hai phần bao gồm các đoạn "instructions" và "output" (Bảng 2). Đối với CrossFit, ExMix và T5, vì các benchmark gốc không cung cấp hướng dẫn ngôn ngữ tự nhiên, chúng tôi viết thủ công một câu hướng dẫn đơn giản cho mỗi nhiệm vụ được bao gồm và sử dụng chúng ở mức instance. Ví dụ, hướng dẫn cho nhiệm vụ GPT-2 Deepfake Detection (Radford et al., 2021) trong ExMix viết "Is the following text produced by GPT-2?".

**Phân loại nhiệm vụ.** Chúng tôi phân loại các nhiệm vụ dưới các danh mục NLP thông thường theo thực hành của các nghiên cứu trước (Wei et al., 2022a; Sanh et al., 2022; Wang et al., 2022; Ye et al., 2021). Việc nhóm như vậy cung cấp một khung thuận tiện để nghiên cứu khả năng tổng quát hóa của các mô hình cross- và within categories. Chúng tôi chủ yếu tuân theo taxonomy 76 danh mục được định nghĩa bởi Super-NaturalInstructions. Các benchmark khác cũng cung cấp các cụm nhiệm vụ riêng của chúng. Chúng tôi thực hiện thống nhất thô của các cụm nhiệm vụ thủ công, ví dụ: hợp nhất "hate speech detection" với "toxic language detection". Bên cạnh điều này, các benchmark như CrossFit và PromptSource áp dụng phân loại nhiệm vụ chi tiết hơn so với Super-NaturalInstructions, ví dụ: CrossFit xác định nhiều lớp con của Question Answering. Trong những trường hợp như vậy, chúng tôi áp dụng việc gán coarse-grained hơn của Super-NaturalInstructions. Điều này dẫn đến một taxonomy một cấp với hơn 100 danh mục nhiệm vụ (Bảng 1).

[Bảng 2: Các công thức prompt khác nhau của nhiệm vụ COPA (Roemmele et al., 2011) từ Super-NaturalInstructions, PromptSource, FLAN và CrossFit. CrossFit không cung cấp hướng dẫn ngôn ngữ tự nhiên, đòi hỏi các mô hình phải dựa vào cách trình bày dữ liệu để suy ra yêu cầu nhiệm vụ.]

### 2.3 Tạo các Phân chia Benchmark

**Phân chia train, validation và test.** Chúng tôi phân chia tập hợp tất cả các nhiệm vụ theo cách cho phép chúng tôi thực hiện instruction fine-tuning quy mô lớn và đánh giá mô hình kết quả đối với ba mức độ tổng quát hóa. Đầu tiên, chúng tôi giữ lại một số danh mục nhiệm vụ để đánh giá khả năng tổng quát hóa của mô hình đối với các danh mục nhiệm vụ mới. Thứ hai, chúng tôi chọn một tập con của các danh mục còn lại như các danh mục partially held-out.² Chúng tôi chia các dataset trong những danh mục này thành train và evaluation và sử dụng chúng để kiểm tra khả năng tổng quát hóa của mô hình đối với các dataset mới từ các danh mục nhiệm vụ đã thấy. Chúng tôi chọn các danh mục fully và partially held-out bằng cách phần lớn duy trì nhất quán với các nghiên cứu instruction fine-tuning trước (Wang et al., 2022; Wei et al., 2022a; Sanh et al., 2022) để cho phép so sánh trực tiếp. Cuối cùng, đối với một tập con của các nhiệm vụ training, chúng tôi giữ lại các tập validation và test từ bản phát hành dữ liệu gốc, và sử dụng chúng để kiểm tra khả năng tổng quát hóa của mô hình trong setting multi-task learning chuẩn, tức là các ví dụ mới từ các nhiệm vụ đã thấy. Chúng tôi dành 35 nhiệm vụ đánh giá trải qua 9 danh mục nhiệm vụ từ các nhiệm vụ đánh giá làm tập validation³, và sử dụng chúng để đặc trưng hóa sự đánh đổi của các chiến lược instruction-tuning khác nhau trong §4. Chi tiết về các nhiệm vụ validation của chúng tôi bao gồm các metrics đánh giá của chúng được hiển thị trong Bảng 15.

**De-duplication nhiệm vụ.** Chúng tôi đảm bảo rằng các nhiệm vụ train và evaluation không chồng lấp trên nguồn dữ liệu mà chúng được tạo ra, để ngăn ngừa rò rỉ⁴, theo thực hành của Wang et al. (2022). Đối với mỗi cặp nhiệm vụ train và eval, chúng tôi tính toán tỷ lệ các ví dụ có bất kỳ sự chồng lấp 13-gram nào giữa các chuỗi được khởi tạo từ những ví dụ đó. Chúng tôi kiểm tra thủ công mọi cặp nơi hơn 1% của tập eval chồng lấp với tập training (~14,000 cặp) để xác nhận liệu việc tinh chỉnh trên nhiệm vụ train có thể có lợi bất công cho nhiệm vụ eval hay không, và quyết định loại bỏ nhiệm vụ train hoặc eval trong các trường hợp được xác nhận. Các cặp nhiệm vụ chia sẻ một tài nguyên ngữ cảnh rộng như Wikipedia nhưng chứa các nhãn đầu ra không liên quan được giữ lại. Bảng 1 hiển thị thống kê của các phân chia nhiệm vụ của chúng tôi.

### 2.4 Xây dựng Prompt Nhiệm vụ

Mỗi ví dụ trong setting zero-shot được định dạng sử dụng schema hướng dẫn hai phần như được mô tả trong Phần 2.1. Chúng tôi chèn một delimiter giữa các hướng dẫn và đầu ra nếu các hướng dẫn không kết thúc bằng ":". Tương tự như Chung et al. (2022b), đối với mỗi ví dụ, chúng tôi lấy mẫu ngẫu nhiên một delimiter từ một tập nhỏ⁵ để giảm thiểu overfitting. Đối với các prompt few-shot, chúng tôi đặt các ví dụ demonstration giữa các mô tả nhiệm vụ và ví dụ mục tiêu cho các benchmark áp dụng hướng dẫn task-level như Super-NaturalInstructions, và trước ví dụ nhiệm vụ cho các benchmark áp dụng hướng dẫn instance-level như FLAN và PromptSource. Ví dụ về các prompt cho từng nhiệm vụ có thể được tìm thấy trong Phụ lục C.

Các benchmark FLAN và PromptSource chứa nhiều template được viết thủ công trên mỗi nhiệm vụ. Để tăng thêm tính đa dạng nhiệm vụ, một số template trong những benchmark này đã thay đổi ngữ nghĩa nhiệm vụ gốc (ví dụ: "question answering" ➜"question generation"). Chúng tôi kiểm tra thủ công tất cả các template nhiệm vụ trong những benchmark này và loại bỏ các template đã thay đổi ngữ nghĩa nhiệm vụ gốc để tinh chỉnh các danh mục nhiệm vụ của chúng tôi.

## 3. Instruction Fine-tuning

Chúng tôi sử dụng OPT-IML Bench được trình bày trong Phần 2 để tinh chỉnh OPT (Zhang et al., 2022), một bộ các mô hình ngôn ngữ transformer decoder-only mã nguồn mở được phát hành ở các quy mô từ 125M đến 175B tham số thực hiện tương tự như GPT-3 (Brown et al., 2020a) trên một tập hợp các nhiệm vụ NLP chuẩn. OPT được huấn luyện trên 180B token duy nhất từ sự kết hợp của các dataset được sử dụng trong RoBERTa (Liu et al., 2019), the Pile (Gao et al., 2020), và PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2020) sử dụng mục tiêu next-word prediction. Chúng tôi mô tả quá trình instruction-tuning OPT ở quy mô 30B và 175B trong phần này.

### 3.1 Mục tiêu Fine-tuning

Chúng tôi finetune OPT theo cách tương tự như pre-training sử dụng mục tiêu next-word prediction có điều kiện trên tất cả các token trước đó như ngữ cảnh. Tuy nhiên, chúng tôi tách chuỗi training thành một chuỗi ngữ cảnh nguồn và một chuỗi mục tiêu và chỉ bao gồm các số hạng loss từ các token trong chuỗi mục tiêu (label-loss). Chúng tôi coi các hướng dẫn nhiệm vụ và đầu vào như các token nguồn và các token nhãn như các token mục tiêu. Chính thức, đối với một dataset fine-tuning D bao gồm các instances nguồn si và các token mục tiêu tương ứng của chúng ti = {tij}, một mô hình pre-trained với tham số θ được fine-tuned để tối thiểu hóa loss sau trên các token mục tiêu có điều kiện trên các token nguồn và các token mục tiêu đã thấy trước đó.

L(D; θ) = ΣiΣj − log p(tij|si; ti,<j; θ) (1)

Chúng tôi tối thiểu hóa loss này qua tất cả các dataset trong OPT-IML Bench của chúng tôi bằng cách trộn các ví dụ từ các dataset khác nhau dựa trên kích thước của chúng và tỷ lệ được gán cho các benchmark mà chúng đến từ (chi tiết hơn trong Phần 4).

### 3.2 Packing và Document Attention

Để sử dụng độ dài chuỗi tối đa cho hiệu quả tính toán, chúng tôi pack nhiều ví dụ (nguồn và mục tiêu) cùng nhau như một chuỗi gồm 2048 token (Raﬀel et al., 2020), được tách bởi các token <eos>. Một hệ quả của packing là các token thuộc về một ví dụ có thể attend đến các token từ các ví dụ được pack trước đó trong cùng chuỗi. Để giảm thiểu điều này, chúng tôi sử dụng document attention masking tức là chúng tôi sửa đổi token attention mask trong causal LM để chỉ attend đến các token là một phần của cùng ví dụ, thay vì tất cả các token trước đó trong chuỗi. Điều này thay đổi attention mask từ tam giác thành block tam giác và cải thiện cả tính ổn định và hiệu suất trong các thí nghiệm của chúng tôi.

### 3.3 Hyperparameters Fine-tuning

Chúng tôi fine-tune tất cả các mô hình 30B trên 64 A100 40GB, và các mô hình 175B trên 128 A100 40GB. Theo OPT, chúng tôi sử dụng Fully Sharded Data Parallel (Artetxe et al., 2021) và Megatron-LM Tensor Parallelism (Shoeybi et al., 2019). Chúng tôi kế thừa hầu hết các hyper-parameter mô hình cho mỗi quy mô mô hình theo OPT. Chúng tôi pack các ví dụ training của chúng tôi thành các chuỗi có độ dài 2048, cắt bỏ bên trái các ví dụ vượt quá. Chúng tôi sử dụng Adam (Kingma and Ba, 2014) với 32-bit state với (β₁; β₂) = (0.9; 0.95), warming up tuyến tính learning rate trong 60 steps đến maximum, tiếp theo là decay tuyến tính về 0. Chúng tôi tiến hành các thí nghiệm sơ bộ để chọn learning rate từ {1e−5; 3e−5; 5e−5; 6e−5} và batch size per-GPU từ {2, 4, 8} sử dụng phân chia validation của chúng tôi từ §2. Các hyperparameter kết quả được liệt kê trong Bảng 3. Chúng tôi sử dụng dropout 0.1 (bao gồm embedding dropout) và clip gradient norm đến 1.0, và sử dụng dynamic loss scaling để ngăn ngừa underflow (Micikevicius et al., 2018). Trong quá trình fine-tuning, các mô hình của chúng tôi thấy khoảng 2 tỷ token, chỉ là 0.6% ngân sách pre-training của OPT (Bảng 3).

[Bảng 3: Tham số fine-tuning cho tất cả các mô hình OPT-IML, bao gồm thời gian fine-tuning và số lượng token fine-tuning.]

## 4. Điều gì Quan trọng cho Instruction Fine-tuning?

Các nghiên cứu gần đây đã khám phá một số kỹ thuật instruction fine-tuning để tối ưu hóa hiệu suất của mô hình kết quả trên các loại nhiệm vụ downstream cụ thể, và cũng để cải thiện tính mạnh mẽ của chúng chống lại các biến thể trong prompt, kiểu hướng dẫn và setup prompting. Sử dụng mô hình OPT 30B với các setting hyper-parameter cơ bản được chọn trong §3.3, chúng tôi chạy các thí nghiệm để đặc trưng hóa tác động của tỷ lệ dataset, số lượng nhiệm vụ và tính đa dạng, sử dụng các dataset pre-training, dialogue, và reasoning, và training sử dụng demonstrations, trên instruction-tuning đối với ba mức độ tổng quát hóa mô hình của chúng tôi: fully held-out, partially held-out và fully supervised. Chúng tôi tổng hợp hiệu suất theo nhiều chiều như clusters và benchmarks để xác định các setting tốt nhất.

### 4.1 Setup Thí nghiệm

Mục tiêu của setup thí nghiệm của chúng tôi là đầu tiên, đặc trưng hóa tác động của vô số yếu tố liên quan đến quá trình fine-tuning, trên hiệu suất instruction-tuning, và thứ hai, sử dụng những phát hiện này để instruction-tune các mô hình OPT một cách hiệu quả. Các yếu tố mà chúng tôi thí nghiệm với là 1) thành phần của hỗn hợp dataset fine-tuning, 2) số lượng và tính đa dạng của các nhiệm vụ được sử dụng để fine-tuning, 3) sử dụng các dataset bổ sung liên quan đến pre-training, reasoning và dialogue như một phần của hỗn hợp fine-tuning, và 4) các cách khác nhau để fine-tuning với demonstrations.

**Chi tiết xây dựng prompt.** Để biên dịch dữ liệu train của chúng tôi, chúng tôi hợp nhất tất cả dữ liệu prompt cho một nhiệm vụ với N ví dụ và lấy ngẫu nhiên N prompt từ pool sao cho phân phối nhiệm vụ training được giữ nguyên bất kể có bao nhiêu prompt được đưa ra cho các nhiệm vụ. Chúng tôi hợp nhất các prompt cho mỗi nhiệm vụ theo cách tương tự trong tập validation của chúng tôi, và lấy mẫu ngẫu nhiên tối đa 250 prompt per nhiệm vụ để báo cáo kết quả validation. Đối với các nhiệm vụ test của chúng tôi, chúng tôi giữ tất cả các biến thể prompt và tất cả ví dụ.

**Mức độ tổng quát hóa.** Bắt đầu với một mô hình instruction-tuned baseline, chúng tôi đặc trưng hóa độc lập tác động của mỗi yếu tố, bằng cách tinh chỉnh các mô hình với một số biến thể của yếu tố đó và đánh giá các mô hình trên các nhiệm vụ từ phân chia validation của chúng tôi từ Phần 2, được tách thành ba mức độ tổng quát hóa: a) các nhiệm vụ từ các cluster không được bao gồm trong training (Fully Held-out), b) các nhiệm vụ chưa thấy trong quá trình training nhưng từ các cluster đã thấy (Partially Supervised), và c) các nhiệm vụ đã thấy trong quá trình training (Fully Supervised). Một setting instruction-tuning là mong muốn nếu nó cải thiện hiệu suất trên các nhiệm vụ fully held-out và partially supervised mà không hy sinh hiệu suất trên các nhiệm vụ fully supervised. Chúng tôi sử dụng hiệu suất trung bình qua cả ba mức độ tổng quát hóa trên cả setting 0-shot và 5-shot trên các tập validation/test của các nhiệm vụ trong phân chia validation để xác định các setting tốt nhất cho mỗi yếu tố.

**Decoding.** Dữ liệu đánh giá của chúng tôi bao gồm các nhiệm vụ với các ứng cử viên câu trả lời (trong đó một là đúng), cũng như các nhiệm vụ với nhiều chuỗi tham chiếu vàng. Đối với tập nhiệm vụ đầu tiên, chúng tôi sử dụng rank classification tương tự như Brown et al. (2020b), nơi chúng tôi chấm điểm mỗi ứng cử viên dựa trên likelihood của chúng và đầu ra ứng cử viên có điểm cao nhất như câu trả lời. Ứng cử viên này được sử dụng để tính toán accuracy trên nhiệm vụ. Đối với các nhiệm vụ không có ứng cử viên, chúng tôi thực hiện greedy decoding cho đến khi một token <eos> được dự đoán hoặc tối đa N=256 token được tạo ra. Dựa trên chuỗi được tạo ra và các tham chiếu, chúng tôi sau đó tính toán điểm Exact-match hoặc Rouge-L F1.

**Lựa chọn mô hình.** Đối với tất cả thí nghiệm, trước tiên chúng tôi tổng hợp kết quả riêng biệt cho 0-shot và 5-shot qua các subtype nhiệm vụ. Ví dụ, các phiên bản pro và anti của type 1 và type 2 Winobias (Zhao et al., 2018) từ PromptSource, và tất cả 57 subtask của MMLU (Hendrycks et al., 2020), sẽ được tổng hợp để có hiệu suất per nhiệm vụ. Nếu cùng nhiệm vụ tồn tại qua nhiều benchmark, chúng tôi sau đó tính trung bình hiệu suất qua các benchmark cũng như. Chúng tôi sau đó tính toán trung bình 0-shot và 5-shot của tất cả nhiệm vụ trong một danh mục (hoặc benchmark tùy thuộc vào thí nghiệm), và cuối cùng, tính toán trung bình kết hợp của tất cả điểm 0 và 5-shot của mỗi danh mục (hoặc benchmark), mà chúng tôi sử dụng để lựa chọn mô hình.

Chúng tôi tinh chỉnh mỗi mô hình trong 4000 steps và đánh giá trên phân chia validation của chúng tôi trên cả setting 0-shot và 5-shot, sử dụng 250 ví dụ từ mỗi nhiệm vụ để có hiệu quả tính toán. Như được mô tả trong Phần 2, các phân chia validation của chúng tôi cho mỗi nhiệm vụ bao gồm một hỗn hợp của nhiều prompt cho FLAN và PromptSource. Tất cả trừ bốn nhiệm vụ validation là các nhiệm vụ kiểu generation (nơi chúng tôi báo cáo Rouge-L F1). Chúng tôi tính toán accuracy dựa trên scoring cho các nhiệm vụ còn lại và tổng hợp chúng cùng với Rouge-L cho mục đích trình bày. Chúng tôi tham khảo Bảng 15 trong Phụ lục để biết chi tiết đầy đủ về các nhiệm vụ trong phân chia validation của chúng tôi.

### 4.2 Tác động của việc thay đổi tỷ lệ mixing tối đa của nhiệm vụ

Các nghiên cứu trước (Raﬀel et al., 2020; Wei et al., 2022a) thường sử dụng example-proportional sampling và xây dựng batch bằng cách lấy mẫu từ các dataset tỷ lệ thuận với kích thước của chúng, trong khi thực thi một tham số kích thước tối đa (EPS) để ngăn các dataset lớn làm áp đảo batch. Để hiểu cách tỷ lệ mixing tối đa này (EPS) ảnh hưởng đến hiệu suất qua các mức độ tổng quát hóa khác nhau, chúng tôi thực hiện các thí nghiệm với EPS ∈ {128; 256; 512; 1024; 2048; 4096; 8192; 16384; 10⁶} và báo cáo kết quả trong Bảng 4. EPS 512 khiến 97% dataset đạt maximum của chúng, trong khi EPS 8192 khiến 16% dataset đạt maximum của chúng. Chúng tôi cũng thí nghiệm không sử dụng EPS tức là EPS=100K.

Nhìn chung, chúng tôi thấy rằng trong khi EPS quan trọng đối với instruction-tuning tức là trung bình tất cả các mô hình sử dụng EPS vượt trội so với mô hình không có nó, sau một ngưỡng nhất định tức là ít hơn 4096 trong trường hợp của chúng tôi, có sự biến thiên tối thiểu trong hiệu suất qua tất cả các mức độ tổng quát hóa. Trong khi dựa trên hiệu suất trung bình cao nhất, chúng tôi chọn 4096 (cũng tương ứng với 50% độ dài dataset bị giới hạn) cho các thí nghiệm khác và các mô hình OPT-IML cuối cùng, chúng tôi thấy rằng tất cả các giá trị dưới 4096 cũng thực hiện khá tốt, với EPS=128 gần như khớp với 4096. Cũng lưu ý rằng việc thay đổi EPS ngầm thay đổi tỷ lệ dữ liệu fine-tuning từ mỗi benchmark, mà chúng tôi kiểm soát rõ ràng trong Phần tiếp theo.

[Bảng 4: Biến thiên hiệu suất qua các danh mục nhiệm vụ khác nhau với tỷ lệ mixing tối đa khác nhau (EPS), cho mỗi mức độ tổng quát hóa trên OPT-IML 30B, sau 4000 steps. Kết quả ở định dạng 0-shot/5-shot. Chúng tôi chỉ sử dụng hiệu suất 0-shot cho các nhiệm vụ summarization. Hầu hết các nhiệm vụ là các nhiệm vụ generation, mà chúng tôi báo cáo Rouge-L. Chúng tôi báo cáo accuracy cho MMLU. Một số nhiệm vụ trong Cause Effect Cluster cũng sử dụng accuracy, được tính trung bình với Rouge-L cho mục đích trình bày. Chúng tôi chọn các mô hình dựa trên hiệu suất trung bình của chúng được tổng hợp per danh mục, benchmark và shot.]

### 4.3 Tác động của việc thay đổi tỷ lệ benchmark

Trong Phần 2, chúng tôi mô tả nhiều kho nhiệm vụ và prompt (Sanh et al., 2022; Wang et al., 2022; Wei et al., 2022a; Ye et al., 2021; Aribandi et al., 2022) mà chúng tôi thống nhất để mở rộng quy mô lớn số lượng nhiệm vụ được sử dụng cho instruction-tuning. Tuy nhiên, việc sử dụng nhiều benchmark để training, cùng với chỉ example-proportional sampling, dẫn đến các benchmark với nhiều nhiệm vụ hơn làm áp đảo thành phần batch. Ví dụ, trong benchmark của chúng tôi, 71% ví dụ training sẽ đến từ SuperNatInst, với 18% từ PromptSource, và chỉ 5% từ FLAN. Vì mỗi benchmark được liên kết với một định dạng nhiệm vụ cụ thể, điều này có thể làm lệch mô hình kết quả về phía các định dạng input-output nhất định. Chúng tôi thay đổi tỷ lệ của các benchmark khác nhau để đánh giá tác động của chúng đối với hiệu suất nhiệm vụ downstream trên ba mức độ tổng quát hóa của chúng tôi và trình bày kết quả trong Bảng 5. Đối với thí nghiệm này, chúng tôi so sánh các mô hình dựa trên hiệu suất tổng hợp của chúng trên mỗi benchmark thay vì danh mục nhiệm vụ, vì chúng tôi muốn chọn các tham số thực hiện tốt trên số lượng benchmark tối đa.

Đầu tiên, chúng tôi xem xét các cải thiện hiệu suất trong cùng benchmark nơi tỷ lệ được thay đổi. Khi chúng tôi tăng tỷ lệ FLAN từ 5% lên 25%, hiệu suất của nó cải thiện đáng kể trên cả mức độ tổng quát hóa fully-held out và partially held-out, không có cải thiện đáng chú ý trên các nhiệm vụ fully-supervised. SuperNatInst hiển thị xu hướng tương tự trên các nhiệm vụ partially-supervised, nhưng đáng ngạc nhiên là không nhiều trên các nhiệm vụ fully held-out. Có thể định dạng input-output rất cụ thể của SuperNatInst làm cho việc thay đổi tỷ lệ của các cluster không liên quan không mang lại lợi ích cho các cluster fully held-out của nó. PromptSource tương đối không thay đổi trên các cluster fully supervised và partially supervised, có thể do đạt được sự bão hòa hiệu suất với ngay cả tỷ lệ 18%. Tuy nhiên, nó có lợi với tỷ lệ nhiều hơn trên các cluster fully-held out.

Thứ hai, chúng tôi cũng quan sát các benchmark bổ sung cho nhau. Ví dụ, accuracy cao nhất trên FLAN fully held-out tức là 88.8/83.6%, được đạt được, không phải với tỷ lệ cao nhất của FLAN, mà với việc cải thiện tỷ lệ của PromptSource và Crossﬁt. Tương tự, hiệu suất generation cao nhất trên PromptSource fully-held out là 79.7/83.5% được đạt được với 25% PS, và không phải với tỷ lệ 45% PS. Chúng tôi cũng quan sát một số sự đánh đổi, ví dụ, tỷ lệ tốt nhất cho FLAN và PromptSource dẫn đến sự sụt giảm mạnh trong hiệu suất trên các dataset reasoning, và ngược lại. Cuối cùng, việc đặt tỷ lệ Crossﬁt, Exmix, T5 và Uniﬁed-SKG thành 0 dẫn đến mô hình tồi tệ nhất, chứng minh lợi ích của việc sử dụng một tập đa dạng các benchmark cho instruction-tuning. Dựa trên hiệu suất trung bình qua các benchmark, "2/1/27/40/27/1/2", "7/1/35/25/28/2/2" và "4/2/20/25/45/2/2" thực hiện tốt nhất và chúng tôi chọn cái cuối cùng làm tỷ lệ cho các mô hình OPT-IML cuối cùng của chúng tôi. Bất chấp sự lựa chọn của chúng tôi, các mô hình instruction-tuned với các mục tiêu cuối khác nhau (ví dụ: tạo ra các chuỗi reasoning) sẽ có lợi từ việc chọn khác nhau. Chúng tôi cũng khám phá các phương pháp để cải thiện hiệu suất trên các dataset reasoning trong Phần 4.6.

[Bảng 5: Biến thiên hiệu suất per-benchmark ở mỗi mức độ tổng quát hóa với tỷ lệ benchmark thay đổi; Hàng đầu tiên đại diện cho tỷ lệ gốc trong benchmark OPT-IML. Kết quả ở định dạng 0-shot/5-shot. Chúng tôi chỉ sử dụng hiệu suất 0-shot cho các nhiệm vụ Summarization. Hầu hết các nhiệm vụ là các nhiệm vụ generation, mà chúng tôi báo cáo Rouge-L. Chúng tôi báo cáo accuracy cho MMLU. Bốn nhiệm vụ trong Cause Effect Cluster cũng sử dụng accuracy, được tính trung bình với Rouge-L cho mục đích trình bày. Chúng tôi chọn các mô hình dựa trên hiệu suất trung bình của chúng được tổng hợp per benchmark và shot.]

### 4.4 Tác động của Mở rộng Nhiệm vụ hoặc Danh mục

Các nghiên cứu trước đã chỉ ra rằng việc mở rộng số lượng nhiệm vụ training hoặc cluster cải thiện hiệu suất tổng thể của mô hình trên setting tổng quát hóa fully held-out (Wei et al., 2022a; Wang et al., 2022). Chúng tôi nghiên cứu tác động theo các trục tương tự nhưng với nhiều setting tổng quát hóa hơn như fully held-out, partially supervised, và các nhiệm vụ/danh mục fully supervised. Chúng tôi sử dụng cluster/danh mục thay thế cho nhau trong phần này. Đối với nghiên cứu mở rộng nhiệm vụ, chúng tôi lấy mẫu ngẫu nhiên 16, 64, 256, và 1024 tập nhiệm vụ sao cho các tập nhỏ hơn là tập con của các tập lớn hơn, và các nhiệm vụ fully supervised luôn được chọn. Hình 2 (kết quả đầy đủ trong Phụ lục Bảng 17) trình bày những nghiên cứu mở rộng nhiệm vụ này trên ba mức độ tổng quát hóa, được tổng hợp ở mức cluster cho cả hiệu suất 0 và 5-shot.

Chúng tôi quan sát rằng cả nhiệm vụ fully held-out và partially supervised đều có được những cải thiện nhiều nhất với sự gia tăng số lượng nhiệm vụ training. Thú vị là hiệu suất của các nhiệm vụ fully supervised vẫn không thay đổi ngay cả khi nhiều nhiệm vụ liên quan được thấy từ các cluster của các nhiệm vụ fully supervised, khi chúng tôi tăng các nhiệm vụ training. Trong setting fully held-out, các cluster Cause Effect Classification và Word Analogy có những cải thiện lớn nhất trong zero-shot và few-shot, tương ứng. Trên partially supervised, các cluster Question Answering và Toxic Language Detection có những cải thiện lớn nhất trên cả zero-shot và few-shot.

[Hình 2: Tác động của việc mở rộng số lượng nhiệm vụ training trên mỗi mức độ tổng quát hóa cho OPT-IML 30B dưới cả setting 0-shot và 5-shot, được tổng hợp theo danh mục nhiệm vụ.]

Đối với nghiên cứu mở rộng cluster, chúng tôi sắp xếp các cluster dựa trên thứ tự giảm dần của số lượng nhiệm vụ có mặt trong mỗi cluster và chọn 4, 16, 64, và 93 (tức là tất cả) cluster đầu tiên. Bổ sung, chúng tôi đảm bảo rằng các cluster Question Answering, Summarization, và Dialogue Generation luôn được đại diện vì các nhiệm vụ validation fully supervised của chúng tôi thuộc về ba cluster này. Hình 3 (kết quả đầy đủ trong Phụ lục Bảng 18) trình bày các kết quả tương ứng trên cả ba mức độ tổng quát hóa cho cả setting zero-shot và few-shot. Chúng tôi quan sát rằng khi chúng tôi tăng các cluster training, hiệu suất trên các nhiệm vụ fully supervised hoặc giữ nguyên hoặc giảm nhẹ trong setting few-shot. Trên các mức độ fully held-out và partially supervised, kết quả trên các setting zero-shot cải thiện với sự gia tăng số lượng cluster và kết quả hơi hỗn hợp cho setting few-shot, nhưng tổng thể chúng có xu hướng giảm với việc mở rộng cluster. Lưu ý rằng 4 cluster đầu tiên đã bao phủ 673 nhiệm vụ (các cluster thuộc về setting fully supervised có rất nhiều nhiệm vụ). Do đó, mô hình bắt đầu với hiệu suất mạnh, có thể dẫn đến kết quả hỗn hợp mà chúng tôi quan sát. Dựa trên những thí nghiệm này, chúng tôi sử dụng tất cả nhiệm vụ và cluster để huấn luyện các mô hình OPT-IML cuối cùng của chúng tôi.

[Hình 3: Tác động của việc mở rộng số lượng danh mục training trên mỗi mức độ tổng quát hóa cho OPT-IML 30B dưới cả setting 0-shot và 5-shot.]

### 4.5 Tác động của Pre-training trong quá trình Instruction-Tuning

Chúng tôi quan sát rằng việc sử dụng các cập nhật kiểu pre-training trên toàn bộ chuỗi trong quá trình fine-tuning có thể làm cho training ổn định hơn, vì vậy chúng tôi khám phá tác động hiệu suất của việc sử dụng dữ liệu pre-training trên ba mức độ tổng quát hóa của chúng tôi. Bảng 6 hiển thị một ví dụ được sử dụng trong các cập nhật kiểu pre-training. Theo Shuster et al. (2022), chúng tôi sử dụng shard cuối cùng của corpus được sử dụng để huấn luyện OPT (Zhang et al., 2022) làm dữ liệu pre-training của chúng tôi cho fine-tuning, vì nó chỉ được thấy một lần trong giai đoạn pre-training của OPT. Chúng tôi thí nghiệm với việc thêm dữ liệu pre-training theo tỷ lệ trong các lượng tăng dần 1%, 5%, 10%, và 50%, và trình bày kết quả cho setting 5-shot, được tổng hợp theo danh mục nhiệm vụ, trong Hình 4 (kết quả đầy đủ 0 và 5-shot trong Phụ lục Bảng 19).

[Bảng 6: Ví dụ từ các dataset pre-training, reasoning, và dialogue. Đối với dữ liệu pre-training và dialogue, nguồn là trống và toàn bộ chuỗi văn bản được coi là mục tiêu.]

Nhìn chung, đối với các mức độ tổng quát hóa fully held-out và partially supervised, chúng tôi quan sát rằng mô hình cải thiện khi thêm dữ liệu pre-training lên đến 10% và sau đó bắt đầu xấu đi sau đó. Chúng tôi cũng quan sát rằng việc sử dụng nhiều dữ liệu pre-training hơn dẫn đến điểm Rouge-L F1 tốt hơn nhưng điểm accuracy thấp hơn, một phần do ảnh hưởng của dữ liệu pre-training đối với tỷ lệ còn lại của các nhiệm vụ generation vs. classification. Dựa trên điểm trung bình qua các mức độ tổng quát hóa (xem Phụ lục Bảng 19), chúng tôi chọn bao gồm 5% dữ liệu pre-training trong instruction-tuning các mô hình OPT-IML của chúng tôi.

[Hình 4: Tác động của việc thực hiện các cập nhật pre-training trên toàn bộ chuỗi, cùng với instruction-tuning trên mỗi mức độ tổng quát hóa cho OPT-IML 30B trong setting 5-shot, được tổng hợp theo danh mục nhiệm vụ. Trục x đại diện cho % các cập nhật pre-training được thực hiện w.r.t tổng số cập nhật.]

### 4.6 Tác động của việc Thêm Dataset Reasoning

Các nghiên cứu gần đây (Wei et al., 2022b; Kojima et al., 2022) đã minh họa những cải thiện trong hiệu suất của LLM trên các nhiệm vụ reasoning, khi được nhắc nhở để tạo ra một chuỗi reasoning bằng ngôn ngữ tự nhiên trước khi tạo ra câu trả lời. Dựa trên những phát hiện này, chúng tôi cố gắng fine-tune rõ ràng LLM để thực hiện reasoning bằng cách biên dịch một tập gồm 14 dataset reasoning (xem Phụ lục A.1 để biết danh sách những dataset này), nơi đầu ra bao gồm một lý do trước câu trả lời và bằng cách bao gồm những dataset này trong quá trình instruction-tuning. Tập này bao gồm 9 dataset được sử dụng bởi Chung et al. (2022b) trong danh mục CoT của họ cũng như một số dataset bổ sung. Mỗi dataset có một prompt duy nhất sử dụng một hướng dẫn, rõ ràng yêu cầu mô hình tạo ra một chuỗi reasoning (Kojima et al., 2022), tiếp theo là các ví dụ trong setting few-shot minh họa cách chuỗi reasoning nên được tạo ra trước câu trả lời. Chúng tôi hiển thị một ví dụ với prompt như vậy trong Bảng 6. Sử dụng tỷ lệ benchmark "2/1/27/40/27/1/2" làm baseline (xem Phần 4.3), chúng tôi thí nghiệm với việc thêm 1%, 2%, và 4% tỷ lệ dữ liệu reasoning (bằng cách giảm tỷ lệ của benchmark có tỷ lệ cao nhất tức là SuperNatInst), và trình bày kết quả cho setting 5-shot trong Hình 5 (kết quả đầy đủ 0 và 5-shot trong Phụ lục Bảng 20) theo mức độ tổng quát hóa và danh mục nhiệm vụ.

Chúng tôi thấy một cải thiện hiệu suất đáng kể trên 2/14 nhiệm vụ reasoning validation held-out (Rouge-L từ 12.2% lên 31.6%) khi chúng tôi instruction-tune với các dataset reasoning, nhưng bên cạnh đó, chúng tôi cũng thấy những cải thiện trên các danh mục nhiệm vụ held-out khác như Cause-Effect, Stereotype Detection, Toxicity Detection, và Word Analogy. Hơn nữa, việc thêm 1% dữ liệu reasoning dẫn đến những cải thiện lớn nhất tổng thể, sau đó, những cải thiện bắt đầu giảm trên MMLU, Cause-Effect Accuracy, Toxicity, và Dialogue (tính trung bình trên 0 và 5-shot). Mặt khác, cluster Summarization (chỉ 0-shot, xem Phụ lục) tiếp tục có lợi từ tỷ lệ cao hơn của dữ liệu reasoning. Dựa trên hiệu suất trung bình qua các danh mục và mức độ tổng quát hóa, chúng tôi sử dụng 1% dữ liệu reasoning cho các mô hình OPT-IML cuối cùng của chúng tôi.

[Hình 5: Tác động của việc fine-tuning sử dụng các dataset reasoning trên mỗi mức độ tổng quát hóa cho OPT-IML 30B trong setting 5-shot, được tổng hợp theo danh mục nhiệm vụ. Chúng tôi thí nghiệm với việc thêm 1%, 2% và 4% dataset reasoning theo tỷ lệ. Lưu ý rằng baseline cho thí nghiệm này dựa trên tỷ lệ khác với các thí nghiệm khác.]

### 4.7 Tác động của việc Thêm Dataset Dialogue

Chúng tôi thí nghiệm với việc thêm đối thoại như dữ liệu fine-tuning phụ trợ để kiểm tra xem nó có thể cải thiện khả năng của LM để phản hồi đầu vào định hướng và hiểu các biểu thức tham chiếu hay không. Một mục tiêu khác là đánh giá xem phương pháp này có thể tạo ra hành vi chat-bot (Shuster et al., 2022) và làm cho các mô hình kết quả có tính đối thoại hơn hay không. Sử dụng một tập con của các dataset dialogue⁶ được sử dụng để huấn luyện BlenderBot 3 (Shuster et al., 2022), chúng tôi xử lý các đối thoại thành các chuỗi lượt được tách bởi một token newline duy nhất (xem Bảng 6 để biết ví dụ). Dữ liệu bao gồm 320,543 đối thoại duy nhất và chúng tôi fine-tune mô hình để dự đoán toàn bộ chuỗi đối thoại. Chúng tôi đặt tỷ lệ của dữ liệu dialogue được bao gồm là 0.5% và trình bày kết quả 0 và 5-shot theo danh mục nhiệm vụ và mức độ tổng quát hóa trên phân chia validation của chúng tôi trong Bảng 7.

Chúng tôi quan sát rằng việc thêm ngay cả chỉ 0.5% dữ liệu dialogue nói trên làm giảm hiệu suất 0-shot trong khi hiệu suất 5-shot vẫn không thay đổi. Cụ thể, hiệu suất 0-shot chịu tổn thất chủ yếu trên stereotype detection và word analogy. Khi kiểm tra các dự đoán mô hình trên những danh mục này, chúng tôi thấy rằng chúng chủ yếu là các nhiệm vụ generation có các tham chiếu hoặc là một từ duy nhất hoặc một đoạn văn bản ngắn với định dạng cụ thể (ví dụ: một cặp cụm từ từ đầu vào gốc tham chiếu đến nhau). Việc training với dữ liệu BB3 đã làm suy yếu khả năng của mô hình để tuân thủ định dạng yêu cầu.⁷ Nó cũng làm giảm đáng kể hiệu suất 5-shot của toxicity detection. Một phân tích lỗi tiết lộ vấn đề tương tự tức là mô hình có xu hướng thực hiện tồi tệ hơn trên các nhiệm vụ yêu cầu tạo ra một tập từ quyết định đặc biệt thay vì chỉ đơn giản tạo ra "yes" hoặc "no". Do sự thoái hóa mô hình nghiêm trọng trên những nhiệm vụ này, chúng tôi không thêm dữ liệu dialogue khi tinh chỉnh OPT-IML.

[Bảng 7: Tác động của việc fine-tuning với 0.5% dữ liệu dialogue trên mỗi mức độ tổng quát hóa cho OPT-IML 30B sau 4000 steps, được tổng hợp theo danh mục nhiệm vụ. Kết quả được trình bày ở định dạng 0-shot/5-shot. Hầu hết các danh mục sử dụng Rouge-L F1, MMLU sử dụng accuracy. Một số nhiệm vụ Cause-Effect sử dụng accuracy, được tính trung bình với Rouge-L F1 cho mục đích trình bày.]

### 4.8 Tác động của Meta-Training cho In-Context Learning

Các nghiên cứu gần đây đã chỉ ra rằng việc fine-tuning các mô hình ngôn ngữ với các ví dụ demonstration trong các hướng dẫn cải thiện khả năng học từ các ví dụ trong ngữ cảnh của chúng (Min et al., 2021; Wang et al., 2022; Chung et al., 2022b). Cả Min et al. (2021) và Wang et al. (2022) đều thí nghiệm với setup nơi một số lượng k demonstration examples không đổi được thêm vào mỗi ví dụ training. Các mô hình được đánh giá với cùng số lượng k demonstration examples trong quá trình inference. Chung et al. (2022b) sử dụng một hỗn hợp dữ liệu có và không có exemplar. Tuy nhiên, tỷ lệ của mỗi loại dữ liệu được sử dụng và có bao nhiêu exemplar được bao gồm không rõ ràng.

Chúng tôi cố gắng huấn luyện các mô hình là những người học few-shot in-context tốt hơn, và cũng mạnh mẽ đối với số lượng demonstration examples được sử dụng trong thời gian inference.⁸ Chúng tôi thí nghiệm với một cách đơn giản để tạo ra các ví dụ training bao gồm số lượng demonstration examples thay đổi. Đối với mỗi ví dụ e, chúng tôi lấy mẫu k từ một phân phối D với cap⁹ K, và chọn ngẫu nhiên k ví dụ khác Ed = {e1; :::; ek}; ei ≠ e, từ tập train, nếu k > 0. Chúng tôi thêm Ed như các demonstration examples trong prompt của e, nơi các ví dụ được tách bởi token đặc biệt [SEP]. Đối với các benchmark với hướng dẫn task-level như Super-NaturalInstructions, chúng tôi đặt các demonstration examples trước e và sau trường instruction; đối với các benchmark với hướng dẫn instance-level như FLAN và PromptSource, chúng tôi đặt các demonstration examples trước e.

Bởi vì các demonstration examples tăng đáng kể độ dài prompt, việc bao gồm quá nhiều ví dụ training few-shot thường dẫn đến hiệu suất tồi tệ hơn và giảm tính ổn định học tập, do sự thưa thớt trong loss và tính đa dạng batch thấp hơn. Do đó, chúng tôi chọn D là phân phối Zipf¹⁰, có thể bị nghiêng mạnh về phía k = 0. Chúng tôi huấn luyện các mô hình MetaICL với D khác nhau bằng cách điều chỉnh tham số hình dạng a của phân phối Zipf. Khi a = 4, 92.5% ví dụ là các ví dụ zero-shot; và khi a = 2, 67.1% ví dụ là các ví dụ zero-shot. Chúng tôi đặt K = 5 và sử dụng ba token newline liên tiếp như [SEP] theo Min et al. (2021).

**MetaICL với suffix loss.** Để giải quyết thêm vấn đề thưa thớt loss, chúng tôi cũng thí nghiệm với một biến thể của loss MetaICL gốc, được minh họa trong Hình 6. Cho một ví dụ với hướng dẫn và exemplar, thay vì huấn luyện mô hình để tạo ra nhãn mục tiêu, chúng tôi huấn luyện mô hình để tạo ra nhãn mục tiêu của exemplar đầu tiên tiếp theo là các chuỗi hoàn chỉnh của các exemplar còn lại. Điều này hiệu quả biến các demonstration examples thành các ví dụ training cũng như, và giảm thiểu vấn đề thưa thớt loss vì nó giờ được trải ra trên nhiều token hơn.

[Hình 6: Chúng tôi thí nghiệm với hai loại training loss cho MetaICL: generation loss trên nhãn của ví dụ mục tiêu như được đề xuất bởi Min et al. (2021), và generation loss trên nhãn của demonstration example đầu tiên và các chuỗi hoàn chỉnh của các ví dụ tiếp theo.]

**Suy giảm hiệu suất trên các nhiệm vụ generation.** Chúng tôi trình bày kết quả tập validation cho instruction-tuning với các setting khác nhau cho MetaICL, được tổng hợp theo mức độ tổng quát hóa và danh mục nhiệm vụ dưới cả setting 0 và 5-shot, trong Bảng 8. Chúng tôi quan sát rằng việc thêm training MetaICL dẫn đến hiệu suất tồi tệ hơn trong cả setup 0-shot và 5-shot trong hầu hết các trường hợp, trong khi MetaICL với suffix loss vượt trội so với MetaICL thông thường, đặc biệt trong setup 0-shot. Kiểm tra thêm hiệu suất per-danh mục tiết lộ rằng trong khi các mô hình MetaICL hiển thị những cải thiện hợp lý trong nhiều đánh giá 5-shot, hiệu suất 5-shot trên Stereotype Detection và Word Analogy suy giảm đáng kể. Một phân tích lỗi tiết lộ vấn đề tương tự như trong §4.7 – các mô hình MetaICL có xu hướng mất khả năng tuân thủ nghiêm ngặt pattern đầu ra khi có mặt của in-context exemplar. Bổ sung, standard MetaICL loss làm tổn hại đáng kể các nhiệm vụ reasoning. Các mô hình kết quả có xu hướng tạo ra các câu trả lời ngắn bất chấp sự hiện diện của các chuỗi reasoning trong các ví dụ in-context learning. Điều tra thêm tiết lộ rằng mô hình có thể đang over-fitting với các separator demonstration và việc sửa đổi chúng tại thời điểm inference có thể giảm thiểu đáng kể những vấn đề này (Bảng 21).¹¹ Thú vị là MetaICL làm suy giảm hiệu suất chỉ cho các nhiệm vụ generation, nhưng tổng thể có lợi cho các nhiệm vụ classification dựa trên scoring như MMLU. Tuy nhiên, do sự thoái hóa đầu ra nghiêm trọng trong setting thông thường, chúng tôi quyết định không sử dụng MetaICL để huấn luyện các mô hình OPT-IML của chúng tôi.

[Bảng 8: Tác động của việc fine-tuning MetaICL trên mỗi mức độ tổng quát hóa cho OPT-IML 30B sau 2000 steps, được tổng hợp theo danh mục nhiệm vụ. Kết quả được trình bày như 0-shot/5-shot. Chúng tôi gạch dưới các danh mục nơi các đầu ra mô hình MetaICL thể hiện sự thoái hóa nghiêm trọng so với mô hình baseline.]

## 5. Các Mô hình OPT-IML

Sử dụng các setting tốt nhất cho instruction tuning từ các thí nghiệm của chúng tôi trong Phần 4, chúng tôi instruction tune OPT 30B và 175B để tạo ra các mô hình OPT-IML 30B và 175B. Cụ thể, chúng tôi chọn các giá trị tốt nhất cho EPS và tỷ lệ benchmark, bao gồm tất cả các nhiệm vụ trong phân chia training, thêm 1% dataset với các chuỗi reasoning, và 5% dữ liệu từ corpus pre-training OPT, và chọn bỏ training với demonstrations tức là MetaICL, cũng như các dataset dialogue. Chúng tôi tune OPT-IML 30B trong 4000 steps, trong khi chúng tôi tune OPT-IML 175B trong số lượng steps gấp đôi với batch size bằng một nửa (cho mục đích hiệu quả memory). Dựa trên các metrics tập validation định kỳ, chúng tôi quyết định sử dụng checkpoint cuối cùng làm mô hình cuối cùng.

Chúng tôi đánh giá các mô hình OPT-IML của chúng tôi trên các nhiệm vụ đánh giá OPT cũng như trên bốn benchmark đa nhiệm vụ từ các nghiên cứu trước (Wei et al., 2022a; Sanh et al., 2022; Wang et al., 2022; Xie et al., 2022; Zhang et al., 2022) trong cả setting zero và 5-shot, so sánh trực tiếp chúng với các mô hình instruction-tuned cụ thể cho từng benchmark được phát hành bởi các nghiên cứu trước. Do đó, chúng tôi so sánh với các mô hình OPT baseline trên các tập đánh giá được sử dụng bởi OPT, với FLAN-137B trên các tập đánh giá của FLAN (Wei et al., 2022a), với T0pp 11B trên các tập đánh giá từ PromptSource (Sanh et al., 2022), với Tk-Instruct 11B trên các tập đánh giá từ Super-NaturalInstructions (Wang et al., 2022), và trên joint modeling của text với code/structs trên ba nhiệm vụ từ benchmark UniﬁedSKG (Xie et al., 2022). Chúng tôi kiểm tra những kết quả này trong các phần tiếp theo, và thấy rằng OPT-IML vượt trội so với OPT trên tất cả benchmark và có tính cạnh tranh với các mô hình instruction-tuned cụ thể cho từng benchmark riêng lẻ về cả hiệu suất zero- và few-shot.

### 5.1 Đánh giá OPT

Chúng tôi đánh giá OPT-IML trên một tập con gồm 14 nhiệm vụ NLP chuẩn được báo cáo bởi OPT (Zhang et al., 2022) trên các setting zero và few shot ở quy mô 30B và 175B, sử dụng cùng các prompt được phát hành bởi OPT (một prompt duy nhất per nhiệm vụ). Tất cả những nhiệm vụ này là các nhiệm vụ kiểu classification với nhiều ứng cử viên, vì vậy tương tự như OPT, chúng tôi sử dụng ứng cử viên với likelihood cao nhất như dự đoán mô hình và báo cáo accuracy trong Bảng 9. Bổ sung, tất cả những nhiệm vụ này được held-out trong quá trình training, một số từ các danh mục fully-held out của chúng tôi và một số từ các danh mục partially held-out của chúng tôi. Đối với setting few-shot, chúng tôi sử dụng cùng các ví dụ và số lượng shot được sử dụng bởi OPT tức là 32-shot, nhưng được cắt để phù hợp trong độ dài chuỗi tối đa của mô hình.

[Bảng 9: Accuracy của OPT-IML so với OPT trên 14 nhiệm vụ NLP chuẩn từ Zhang et al. (2022) ở định dạng 0-shot/32-shot. Đối với ARC, (e) biểu thị (Easy) và (c) biểu thị (Challenge).]

Trung bình, OPT-IML cải thiện so với OPT với khoảng 6-7% trên accuracy 0-shot ở cả quy mô mô hình 30B và 175B. Đối với accuracy 32-shot, chúng tôi thấy những cải thiện đáng kể trên mô hình 30B, và những cải thiện nhẹ hơn trên 175B. Trong khi những cải thiện đáng kể cho một số nhiệm vụ như RTE, WSC, BoolQ, ARC, CB, và WiC, instruction-tuning của chúng tôi không cải thiện hiệu suất cho các nhiệm vụ khác như StoryCloze, PIQA, Winograd, và Winogrande. Một số kết quả sau này cụ thể cho các prompt được sử dụng bởi OPT. Ví dụ, chúng tôi quan sát những cải thiện trên StoryCloze và Winogrande, khi được đánh giá trên một tập hợp các template prompt như một phần của PromptSource trong Phần 5.2. Một lý do cho điều này là các prompt OPT ban đầu được áp dụng từ GPT-3 (Brown et al., 2020a) và đã trải qua một quá trình prompt engineering để có hiệu suất tối ưu, trong khi FLAN và PromptSource đánh giá accuracy như trung bình sử dụng một tập hợp đa dạng các prompt, bao gồm các prompt dưới tối ưu. Do đó, một lợi thế của instruction-tuning cho những nhiệm vụ này có thể là cải thiện tính mạnh mẽ của mô hình và giảm nhu cầu cho prompt engineering.

### 5.2 Đánh giá trên PromptSource

Sanh et al. (2022) fine-tune một phiên bản LM được điều chỉnh của T5 11B (Raﬀel et al., 2020; Lester et al., 2021) trên 50 dataset từ PromptSource (được gọi là T0) và đánh giá trên một tập gồm 11 nhiệm vụ held-out là một phần của 4 danh mục fully held-out của họ. Mỗi nhiệm vụ được liên kết với nhiều template prompt, được đóng góp bởi cộng đồng nghiên cứu với sự giúp đỡ của công cụ prompting của họ. Vì tất cả những nhiệm vụ này cũng là một phần của các danh mục held-out trong OPT-IML, chúng tôi sử dụng setup đánh giá tương tự, với một số nhiệm vụ bổ sung cũng như. Hầu hết các nhiệm vụ là các nhiệm vụ classification nơi chúng tôi chấm điểm ứng cử viên dựa trên likelihood và báo cáo accuracy, với ngoại lệ của Blended Skill Talk, là một nhiệm vụ generation nơi chúng tôi báo cáo điểm Rouge-L F1. Vì mỗi nhiệm vụ sử dụng nhiều prompt, chúng tôi báo cáo metrics được tính trung bình qua các prompt dưới setting 0-shot và 5-shot trong Bảng 10.

[Bảng 10: Hiệu suất Zero- và 5-shot của OPT-IML 30B và 175B so với các mô hình OPT baseline cũng như mô hình T0-original-task-only 11B trên các nhiệm vụ đánh giá của Sanh et al. (2022). Chúng tôi báo cáo Rouge-L F1 cho Blended Skill Talk và sử dụng accuracy cho tất cả các nhiệm vụ khác. Mỗi metric nhiệm vụ được báo cáo như trung bình trên nhiều prompt original-task cho nhiệm vụ đó. Tất cả các nhiệm vụ được held out cho cả OPT-IML cũng như T0.]

Một số prompt được thu thập trong PromptSource là cho phiên bản đảo ngược của nhiệm vụ. Ví dụ, nhiệm vụ đảo ngược cho QA là question generation. Chúng tôi không train hoặc đánh giá sử dụng những prompt này, vì chúng có vấn đề khi các nhiệm vụ được gán cho các danh mục. Chúng tôi so sánh OPT-IML với mô hình T0-original-task-only tương ứng với setup held-out của chúng tôi (Sanh et al. (2022) cũng phát hành T0p và T0pp được huấn luyện với các nhiệm vụ bổ sung), và cũng chỉ được huấn luyện trên các prompt tuân thủ nhiệm vụ gốc.

OPT-IML 175B khớp với hiệu suất zero-shot của T0-original-task (11b) và vượt trội so với nó đáng kể về hiệu suất 5-shot. Trong khi cả hai mô hình đều không được huấn luyện trên demonstrations, các causal LM như OPT thể hiện khả năng tổng quát hóa mạnh hơn đối với setting few-shot so với các mô hình encoder-decoder như T0, và mô hình sau có thể có lợi từ training MetaICL để cải thiện hiệu suất few-shot của nó, như được khám phá bởi Chung et al. (2022b). Tương tự, trên nhiệm vụ generation Blended Skill Talk, T0 underperform các causal LM, có thể được quy cho quy mô lớn của dữ liệu tuning cho OPT-IML, hoặc có thể làm nổi bật khó khăn cho các mô hình encoder-decoder để tổng quát hóa đến các nhiệm vụ generation mới. Ở cả hai quy mô, OPT-IML vượt trội so với các mô hình OPT baseline trên hầu hết mọi nhiệm vụ ngoại trừ Crows Pairs. Như được mô tả trong Phần 5.1, đánh giá này sử dụng nhiều prompt per nhiệm vụ và thưởng cho các mô hình mạnh mẽ hơn đối với các prompt đầu vào. Bổ sung, lưu ý rằng OPT-IML 30B vượt trội so với OPT 175B baseline trung bình, chứng minh rằng instruction-tuning có thể là một cách để làm cho các mô hình quy mô nhỏ hơn hiệu quả về tài nguyên có tính cạnh tranh hơn.

Theo Sanh et al. (2022), chúng tôi cũng đánh giá trên WinoGender Schemas (Rudinger et al., 2018) được đúc như một nhiệm vụ textual entailment (Poliak et al., 2018), đo lường mức độ thiên lệch giới tính trong LLM, và thấy rằng instruction-tuning cải thiện rất nhiều accuracy trên nhiệm vụ này. Cuối cùng, chúng tôi đánh giá trên Crows Pairs (Nangia et al., 2020) được công thức hóa như một nhiệm vụ boolean QA về việc liệu một câu có minh họa một stereotype hay không (sử dụng một prompt duy nhất), và thấy sự xấu đi trong hiệu suất trên OPT-IML 175B so với OPT, nhưng không trên mô hình 30B. Có thể các công thức khác của nhiệm vụ này, ví dụ: dự đoán câu nào là stereotype, có thể hiển thị xu hướng khác nhau. Lưu ý rằng hai nhiệm vụ này không phải từ các cluster held-out, vì vậy có thể có các dataset training khác có lợi.

### 5.3 Đánh giá trên FLAN

Cùng với benchmark instruction-tuning FLAN bao gồm 62 dataset, mà chúng tôi bao gồm trong OPT-IML Bench, Wei et al. (2022a) cũng sử dụng nó để instruction-tune Lamda-PT (Thoppilan et al., 2022), một causal LM 137B được huấn luyện trên 1.5T từ của dữ liệu dialogue công cộng và văn bản web. Họ đánh giá instruction-tuning sử dụng FLAN-137B trên các danh mục nhiệm vụ fully held-out, bằng cách sử dụng chiến lược leave-one-out tức là họ tune trên tất cả các danh mục khác, do đó tạo ra một mô hình khác để đánh giá mỗi danh mục test. Điều này mang lại cơ hội để đánh giá các mô hình OPT-IML trên cùng các danh mục nhiệm vụ để đánh giá những cải thiện có thể đạt được bằng cách mở rộng benchmark instruction tuning lên 1500 nhiệm vụ sử dụng một mô hình instruction-tuned duy nhất.

[Bảng 11: So sánh hiệu suất của các mô hình OPT-IML và FLAN (Wei et al., 2022a) trên bốn cụm nhiệm vụ (NLI, Reading Comprehension, Closed-Book QA, và Co-reference) của benchmark FLAN. Chúng tôi báo cáo điểm accuracy ở định dạng 0-shot/k-shot, nơi k=5 đối với các mô hình của chúng tôi trong khi FLAN sử dụng k khác nhau cho mỗi nhiệm vụ. Không có setting few-shot cho WSC. Hiệu suất FLAN-137B dựa trên nhiều mô hình được huấn luyện sử dụng chiến lược leave-one-category-out.]

Chúng tôi đánh giá các mô hình OPT-IML của chúng tôi trên một tập con các nhiệm vụ được sử dụng bởi FLAN-137B, và dựa trên các phân chia của chúng tôi, một số nhiệm vụ từ các danh mục fully-held out (ANLI, CB, MNLI, RTE, SNLI, WNLI, Winogrande, WSC), trong khi còn lại từ các danh mục partially held-out (BoolQ, OpenBookQA, ARC). Tất cả những nhiệm vụ này sử dụng kiểu classification với ứng cử viên câu trả lời, mà chúng tôi đánh giá bằng cách chấm điểm dựa trên likelihood, và chúng tôi báo cáo accuracy zero-shot và few-shot trong Bảng 11. Lưu ý rằng mỗi nhiệm vụ được liên kết với 7-10 template, và chúng tôi báo cáo accuracy trung bình qua tất cả template. Một số template đảo ngược nhiệm vụ (ví dụ: QA trở thành question generation), và chúng tôi không đánh giá trên những template này. Cũng như, trong khi FLAN-137B sử dụng số lượng shot khác nhau cho mỗi nhiệm vụ cho đánh giá few-shot của họ, chúng tôi báo cáo kết quả 5-shot cho tất cả nhiệm vụ.

Chúng tôi thấy rằng instruction-tuning cải thiện đáng kể hiệu suất so với các mô hình OPT baseline ở quy mô 30B cũng như 175B trên từng 15 nhiệm vụ riêng lẻ. Trong khi Wei et al. (2022a) thấy instruction-tuning làm tổn hại các nhiệm vụ fully-held out ở quy mô 8B và thấp hơn, nhưng hiển thị hành vi emergent ở quy mô 66B tham số trở lên, các thí nghiệm của chúng tôi không hiển thị hành vi emergent này tức là cả hai mô hình OPT-IML 30B và 175B đạt được hơn 20% cải thiện trung bình so với các mô hình untuned tương ứng dưới setting 0-shot và few-shot. Bổ sung, mô hình OPT-IML 30B của chúng tôi vượt trội so với mô hình OPT 175B cơ sở 20% trên 0-shot và 12% trên 5-shot, minh họa rằng các mô hình instruction-tuned ở quy mô thấp hơn có thể là các lựa chọn thay thế hiệu quả về tài nguyên mạnh cho các mô hình untuned lớn hơn. So với FLAN-137B, OPT-IML 175B thực hiện cạnh tranh trên hiệu suất 5-shot, và mang lại cải thiện 3% trung bình trên hiệu suất 0-shot. Tuy nhiên, các khác biệt khác nhau trong setup thí nghiệm liên quan đến các cluster held-out, kích thước mô hình và số lượng token pre-training, làm cho khó có thể quy chuẩn một cách dứt khoát những cải thiện này cho việc mở rộng benchmark instruction-tuning.

### 5.4 Đánh giá trên Super-NaturalInstructions

Khác với các đánh giá đã thấy cho đến nay, Super-NaturalInstructions sử dụng định dạng hướng dẫn nghiêm ngặt (Phần 2), nơi một khối hướng dẫn chính thức được cung cấp ở đầu prompt, chi tiết ứng cử viên option và giải quyết sự mơ hồ nhiệm vụ, tiếp theo là nhiều demonstration, và có thể giúp đánh giá khả năng của các mô hình của chúng tôi để tổng quát hóa đến các định dạng hướng dẫn khác nhau. Wang et al. (2022) chia benchmark SuperNatInst thành các danh mục training và held-out, và train Tk-Instruct 3B và 11B, là các phiên bản instruction-tuned của các mô hình T5 được điều chỉnh LM. Họ đánh giá Tk-Instruct trên 12 danh mục đại diện cho 154 nhiệm vụ cho tổng quát hóa fully held-out. Trong số 12 danh mục này, Textual Entailment, Coreference Resolution và Dialogue Act Recognition là fully held-out trong framework đánh giá của chúng tôi. Chúng tôi đánh giá OPT-IML trên ba danh mục này trong setting 0-shot, 2-shot và 5-shot và báo cáo điểm Rouge-L F1 trong Bảng 12. Ba danh mục này bao gồm 44 nhiệm vụ và chúng tôi đánh giá trên top-100 ví dụ từ những nhiệm vụ này theo Wang et al. (2022), với mỗi nhiệm vụ sử dụng một prompt duy nhất. Trong tất cả trường hợp, chúng tôi tạo ra tối đa 256 token cho mỗi ví dụ test. Để so sánh, chúng tôi cũng đánh giá lại Tk-Instruct 11B trên những cluster này dưới cùng framework đánh giá. Chúng tôi sử dụng phiên bản của Tk-Instruct 11B thực hiện tốt nhất tổng thể tức là phiên bản được huấn luyện với hướng dẫn + 2 demonstration positive và không có demonstration negative.

[Bảng 12: So sánh OPT-IML với OPT baseline và Tk-Instruct 11b trên ba danh mục nhiệm vụ fully held-out từ Wang et al. (2022). Chúng tôi báo cáo điểm Rouge-L F1 ở định dạng hiệu suất 0-shot/2-shot/5-shot. Chúng tôi sử dụng phiên bản của Tk-Instruct được huấn luyện với hướng dẫn + 2 demonstration positive và không có demonstration negative.]

Vì Tk-Instruct được huấn luyện và đánh giá dưới setting 2-shot, chúng tôi bổ sung báo cáo kết quả trên setting 2-shot cho đánh giá này. Đầu tiên, các mô hình OPT-IML vượt trội so với các mô hình OPT baseline trên mỗi cluster ở cả hai quy mô, dưới setting 0-shot và tất cả few-shot và một lần nữa chúng tôi quan sát rằng một mô hình instruction tuned 30B vượt trội so với mô hình untuned 175B. Cũng như, trong khi cả OPT 30B và 175B thực hiện tương đương ở tất cả shot, phiên bản instruction-tuned của 175B vượt trội rất nhiều so với OPT-IML 30B, cho thấy rằng các mô hình lớn hơn có thể có lợi nhiều hơn từ instruction tuning. Lưu ý rằng khác với Textual Entailment và các nhiệm vụ khác từ các đánh giá trước, tất cả nhiệm vụ ở đây được đánh giá dưới setting generation (trái với scoring), làm cho nó đáng kể khó khăn hơn cho các mô hình untuned. OPT-IML 175B vượt trội so với Tk-Instruct 11B trên các định dạng 0-shot bất chấp mô hình đầu được tinh chỉnh trên một tập hỗn hợp các định dạng đa dạng từ nhiều benchmark, trong khi mô hình sau được tinh chỉnh cụ thể cho benchmark này. Xu hướng được đảo ngược cho các setting 2-shot và 5-shot nơi Tk-Instruct vượt trội so với OPT-IML. Ở đây, OPT-IML hiển thị hiệu suất đồng đều dưới cả hai setting trong khi Tk-Instruct bị lệch mạnh về phía setting 2-shot mà nó được huấn luyện. Do đó, hiệu suất của Tk-Instruct giảm từ 65.3 xuống 58.4, từ 2-shot xuống 5-shot.

### 5.5 Đánh giá trên UniﬁedSKG

UniﬁedSKG (Xie et al., 2022) là một tập hợp gồm 21 nhiệm vụ liên quan đến Structured Knowledge Grounding với các đầu vào heterogeneous như database, dialogue state, SQL query, v.v., mà chúng tôi bao gồm trong OPT-IML Bench có mục đích để trang bị mô hình với khả năng xử lý structured knowledge. Để đánh giá những khả năng này, chúng tôi so sánh các mô hình OPT-IML với OPT baseline trên ba nhiệm vụ UniﬁedSKG được định dạng như text-to-text: DART (Nan et al., 2020), là một nhiệm vụ data-to-text held-out để chuyển đổi data triple thành text, Spider (Yu et al., 2018), một nhiệm vụ SQL query generation cho một database và một input query, và fully supervised trong framework của chúng tôi, và MultiWoZ (Budzianowski et al., 2018), là một nhiệm vụ dialogue state tracking held-out. Cả ba nhiệm vụ đều là các nhiệm vụ generation nơi chúng tôi decode 256 token trước khi dừng và báo cáo điểm Rouge-L F1 dưới setting 0-shot và 5-shot trong Bảng 13.

[Bảng 13: So sánh hiệu suất của các mô hình OPT baseline với các mô hình OPT-IML trên các tập test của ba dataset từ benchmark UniﬁedSKG, đánh giá Database to Text Generation (DART) (Nan et al., 2020), Text to SQL Generation (Spider) (Yu et al., 2018), và Dialog State Tracking (MultiWoZ) (Budzianowski et al., 2018). Chúng tôi báo cáo điểm Rouge-L ở định dạng 0-shot/5-shot.]

Trên Spider, là setting fully supervised, các mô hình OPT-IML duy trì hiệu suất cao gần với điểm Rouge-L F1 85 bất chấp sự hiện diện của nhiều nhiệm vụ khác trong hỗn hợp instruction-tuning. Trên DART, OPT-IML hiển thị những cải thiện khiêm tốn trong setting 5-shot, nhưng vượt trội đáng kể so với các mô hình OPT trên setting zero-shot, với OPT-IML 30B vượt trội so với OPT 175B. MultiWoZ, mặt khác hiển thị sự xấu đi đáng kể với instruction tuning ở cả quy mô mô hình.

## 6. Thảo luận và Hạn chế

Trong phần trước, chúng tôi đã chứng minh trên nhiều benchmark đánh giá rằng các mô hình instruction-tuned hiệu quả có thể đạt được những cải thiện đáng kể so với các mô hình untuned trên cả setting zero- và few-shot. Chúng tôi đạt được điều này bằng cách đầu tiên mở rộng các dataset instruction-tuning để bao gồm 8 tập hợp lớn các nhiệm vụ NLP, mà chúng tôi chuyển đổi thành một framework đánh giá kiểm tra ba mức độ tổng quát hóa mô hình trên các nhiệm vụ downstream. Sử dụng framework này, chúng tôi đã đặc trưng hóa sự đánh đổi của các yếu tố khác nhau trên instruction tuning như 1) số lượng và tính đa dạng của các nhiệm vụ đầu vào, 2) phân phối của các nhiệm vụ và kiểu hướng dẫn khác nhau, 3) việc bao gồm các dataset chuyên biệt liên quan đến các chuỗi reasoning và dialogue, và 4) fine-tuning với demonstrations. Sự khám phá này giúp chúng tôi chọn các setting tốt nhất để instruction tune các mô hình OPT-IML ở quy mô 30B và 175B, thực hiện cạnh tranh trên một tập rộng rãi các benchmark.

Trong phần này, chúng tôi báo cáo kết quả bổ sung về instruction fine-tuning sử dụng tập hợp nhiệm vụ đầy đủ của chúng tôi và thảo luận về các hạn chế của phương pháp hiện tại của chúng tôi.

### 6.1 Đánh giá trên MMLU, BBH và RAFT

Trong khi chúng tôi chuyển đổi benchmark instruction-tuning được mở rộng quy mô lớn của chúng tôi thành một framework đánh giá để nghiên cứu các kỹ thuật instruction-tuning, gần đây Chung et al. (2022b) cũng đã mở rộng instruction fine-tuning lên đến 1,836 nhiệm vụ từ 4 benchmark sử dụng các mô hình PaLM (Chowdhery et al., 2022) lên đến 540B và các mô hình T5 (Raﬀel et al., 2020) lên đến 11B¹². Các mô hình kết quả, cụ thể là các series FLAN-PaLM và FLAN-T5 được đánh giá trên một số benchmark mô hình ngôn ngữ thách thức bao gồm MMLU (Hendrycks et al., 2021a), và Big Bench Hard (BBH) (Srivastava et al., 2022). Để thiết lập hiệu suất của OPT-IML trong setting tương tự (và bổ sung, trên RAFT (Alex et al., 2021)), chúng tôi instruction-tune OPT 30B và 175B trên toàn bộ benchmark gồm 1,991 nhiệm vụ của chúng tôi, mà chúng tôi gọi là OPT-IML-Max.

[Bảng 14: Hiệu suất tập test của OPT-IML-Max, được huấn luyện trên tất cả nhiệm vụ trong benchmark của chúng tôi, trên Big-Bench Hard, MMLU, và RAFT.]

Chúng tôi sử dụng option scoring cho hai benchmark classification MMLU và RAFT, và generation với Exact Match cho BBH. Chúng tôi đánh giá trên các tập test cho MMLU và BBH và trên phân chia evaluation cho RAFT được phát hành bởi benchmark HELM (Liang et al., 2022). Chúng tôi báo cáo những kết quả này trong Bảng 14 cùng với các mô hình lớn pre-trained và instruction-tuned khác. Bổ sung, chúng tôi cũng huấn luyện và trình bày kết quả cho OPT-IML-Max ở quy mô 1.3B (sử dụng cùng setting như OPT-IML-Max 30B). Trên cả ba dataset, OPT-IML-Max vượt trội so với các đối tác untuned của nó ở tất cả quy mô (ngoại trừ 1.3B trên BBH). Trong khi, OPT-IML-Max có tính cạnh tranh với FLAN-T5 11B trên RAFT, hiệu suất của nó tụt lại phía sau FLAN-T5, FLAN-PaLM và họ các mô hình instruction-tuned GPT-3 (*-davinci-*) trên MMLU và BBH. Trong khi quy mô của benchmark instruction-tuning vẫn tương tự qua những mô hình này, có nhiều khác biệt cơ bản khác. Có sự biến thiên lớn đối với số lượng token được sử dụng để huấn luyện các mô hình pre-training cơ bản tương ứng. Ví dụ, T5 được huấn luyện trên 1T token, FLAN-PaLM trên 800B và OPT trên 180B. Cũng có những khác biệt liên quan đến thành phần của dữ liệu pre-training và các kiến trúc modeling tương ứng. Chowdhery et al. (2022) thấy rằng các mô hình encoder-decoder có thể fine-tune hiệu quả hơn so với các mô hình decoder only ở quy mô tương tự, và việc mở rộng quy mô lớn các mô hình decoder-only có thể làm cho chúng cạnh tranh hơn. Cuối cùng, cũng có những khác biệt trong các thuật toán fine-tuning được sử dụng, ví dụ: một số mô hình davinci của OpenAI sử dụng RLHF (Christiano et al., 2017) trên các tín hiệu feedback được thu thập từ API của họ bổ sung cho supervised fine-tuning. Trong khi chúng tôi thấy rằng việc sử dụng Meta-ICL (§4) không mang lại một mô hình tốt hơn một cách toàn diện và không bao gồm nó trong các mô hình cuối cùng của chúng tôi, chúng mang lại những cải thiện 2-3% trên MMLU và BBH. Tất cả những yếu tố này làm cho khó giải thích khoảng cách trong hiệu suất trên những benchmark này, nhưng tuy nhiên, những đánh giá này phục vụ để thiết lập tác động của các quyết định instruction tuning của chúng tôi đối với các mô hình OPT trên những benchmark external thách thức này.

### 6.2 Hạn chế

Chúng tôi sử dụng framework đánh giá của chúng tôi để đặc trưng hóa sự đánh đổi của các biến instruction-tuning khác nhau trên OPT 30B độc lập với nhau. Mặc dù tốn nhiều tài nguyên để kiểm tra, có thể những biến này tương tác với nhau dẫn đến một lựa chọn khác về các setting tuning tốt nhất (ví dụ: việc thêm các dataset reasoning có thể ảnh hưởng đến lựa chọn tỷ lệ benchmark). Hơn nữa, tất cả sự đánh đổi được nghiên cứu trên instruction tuning 30B có thể không hiển thị cùng xu hướng ở quy mô lớn hơn.

Trong khi chúng tôi nghiên cứu sự đánh đổi instruction tuning sử dụng một tập toàn diện các phân chia của các danh mục fully held-out, partially supervised và fully supervised, việc chọn một tập khác các danh mục có thể dẫn đến việc ưu tiên các quyết định khác nhau so với những gì chúng tôi thực hiện trong bài báo này. Mặc dù chúng tôi gán các nhiệm vụ cho các danh mục dựa trên các định dạng cơ bản, việc gán như vậy có thể chủ quan và một sự gán danh mục khác có thể thay đổi các yếu tố tối ưu cho instruction-tuning. Ví dụ: các nhiệm vụ yêu cầu các kỹ năng khác nhau như phát hiện độc tính cũng có thể được đúc như các nhiệm vụ textual entailment.

### 6.3 AI Có trách nhiệm

Trong khi các mô hình OPT-IML vượt trội so với OPT baseline trên một tập rộng rãi các đánh giá (Phần 5), tuy nhiên, chúng dễ bị tổn thương trước các rủi ro khác nhau liên quan đến việc sử dụng các mô hình ngôn ngữ lớn liên quan đến tính chính xác thực tế (Thoppilan et al., 2022; Brown et al., 2020a; Chowdhery et al., 2022), tạo ra ngôn ngữ độc hại (Gehman et al., 2020) và thực thi các stereotype. Trong khi chúng tôi phát hành các mô hình OPT-IML của chúng tôi để lan truyền các nghiên cứu tương lai về instruction-tuning và để cải thiện tính sẵn có của các causal LM instruction-tuned lớn trên 100B tham số, việc sử dụng những mô hình này nên được kèm theo với các thực hành tốt nhất có trách nhiệm.

## 7. Nghiên cứu Liên quan

Nghiên cứu của chúng tôi về fine-tuning các mô hình ngôn ngữ lớn để tuân theo hướng dẫn trải qua nhiều lĩnh vực như multi-task learning, prompting, và meta-training của in-context learning. Chúng tôi thảo luận những lĩnh vực này dưới đây trong phạm vi có liên quan gần nhất đến nghiên cứu của chúng tôi.

**Instruction Tuning.** Các mô hình ngôn ngữ được huấn luyện để dự đoán token tiếp theo trong một chuỗi với self-supervised learning (Brown et al., 2020a; Zhang et al., 2022; Chowdhery et al., 2022). Prompt engineering và in-context learning đã trở thành phương pháp tiếp cận chiếm ưu thế để tận dụng những mô hình này để giải quyết nhiều nhiệm vụ NLP. Để căn chỉnh những mô hình này để tuân theo các hướng dẫn tự nhiên và tránh prompt engineering, các nghiên cứu gần đây đã đề xuất instruction fine-tuning (Ouyang et al., 2022; Wei et al., 2022a; Chung et al., 2022b; Wang et al., 2022). Một số nghiên cứu này tập trung vào fine-tuning mô hình trên một phạm vi rộng các nhiệm vụ sử dụng prompt và feedback được chú thích bởi con người (Ouyang et al., 2022), trong khi những nghiên cứu khác tập trung vào supervised fine-tuning sử dụng các benchmark học thuật và dataset được tăng cường với các hướng dẫn được tạo ra thủ công hoặc tự động (Wang et al., 2022; Wei et al., 2022a; Sanh et al., 2022; Zhong et al., 2021). Trong nghiên cứu của chúng tôi, chúng tôi tập trung vào phương pháp thứ hai và hợp nhất một tập hợp quy mô lớn các dataset có sẵn công khai với các hướng dẫn để finetune OPT. Đồng thời với nghiên cứu của chúng tôi, Chung et al. (2022a) cũng đề xuất một phương pháp mở rộng benchmark instruction tương tự lên 1836 nhiệm vụ từ 4 benchmark. Trong khi họ tập trung vào fine-tuning sử dụng toàn bộ benchmark để đẩy giới hạn hiệu suất trên một số nhiệm vụ held-out thách thức kiểm tra kiến thức thế giới và khả năng reasoning của mô hình như MMLU (Hendrycks et al., 2020) và Big-Bench Hard (BBH) (Suzgun et al., 2022), chúng tôi tập trung vào đặc trưng hóa sự đánh đổi của các quyết định instruction-tuning khác nhau có thể ảnh hưởng đến hiệu suất downstream.

**Prompting và Meta-Training Zero- và few-shot learning** (a.k.a. in-context learning) tận dụng rất ít ví dụ để giải quyết bất kỳ nhiệm vụ NLP nào bằng cách prompting hiệu quả các mô hình ngôn ngữ, đang trở thành paradigm chiếm ưu thế trong những năm gần đây (Brown et al., 2020a). Prompting bao gồm việc sửa đổi không gian đầu vào và đầu ra của một nhiệm vụ đã cho có thể tận dụng hiệu quả kiến thức của mô hình ngôn ngữ để giải quyết nó. Các phương pháp khác nhau đã đề xuất các cách prompting tốt hơn để cải thiện hiệu suất tổng quát hóa (Wei et al., 2022b; Lu et al., 2021). Hơn nữa, các phát triển gần đây đã chỉ ra các cách để cải thiện in-context learning (ICL) bằng cách meta-tuning các mô hình ngôn ngữ để thích ứng tốt hơn cho ICL (Min et al., 2022, 2021). Trong nghiên cứu của chúng tôi, chúng tôi tận dụng cả các biến thể của prompt có sẵn từ các benchmark khác nhau, cũng như meta-training với demonstration từ một pool lớn các nhiệm vụ, để nghiên cứu các setting hiệu quả cho instruction-based fine-tuning tạo ra tính mạnh mẽ chống lại ngôn ngữ prompting và setup khác nhau.

**Learning to Reason.** Bất chấp tiến bộ của in-context learning, các LLM state-of-the-art vẫn gặp khó khăn với các nhiệm vụ reasoning như commonsense reasoning (West et al., 2022), và các bài toán từ toán học (Hendrycks et al., 2021b) yêu cầu arithmetic reasoning, v.v. Để giải quyết những nhiệm vụ thách thức này, các nghiên cứu gần đây đã sử dụng các phương pháp prompting khác nhau bao gồm một lý do với câu trả lời cuối cùng dưới hình thức scratchpad cho arithmetic và logical reasoning (Nye et al., 2021), cung cấp các prompt chain-of-thought trong demonstration (Wei et al., 2022b), hoặc thêm các cụm từ trigger như let's think step-by-step để nhắc nhở các mô hình tạo ra giải thích (Kojima et al., 2022). Bổ sung cho việc thay đổi prompt, Chung et al. (2022a) đã tích hợp các giải thích step-by-step vào giai đoạn instruction tuning. Theo Chung et al. (2022a), chúng tôi mở rộng thêm tập các dataset reasoning lên 14 dataset và nghiên cứu tác động của tỷ lệ khác nhau của dữ liệu reasoning trên các cụm nhiệm vụ held-out khác nhau.

**Multi-task Learning.** Instruction-based fine-tuning có thể được xem như một công thức của Multi-task Learning (MTL). MTL là một paradigm phổ biến cải thiện hiệu suất tổng quát hóa của một nhiệm vụ khi được kết hợp với các nhiệm vụ liên quan bằng cách chia sẻ các tham số hoặc biểu diễn chung (Caruana, 1997; Kumar and Daume III, 2012). MTL đã được áp dụng cho nhiều kịch bản NLP trong những năm gần đây chủ yếu tập trung vào cải thiện hiệu suất trên các nhiệm vụ training hoặc đến các domain mới bằng cách tận dụng tín hiệu từ các nhiệm vụ liên quan (Collobert and Weston, 2008; McCann et al., 2018; Raﬀel et al., 2020; Vu et al., 2020). Trái ngược, instruction-based fine-tuning cho phép chúng tôi cải thiện hiệu suất tổng quát hóa đến các nhiệm vụ mới chưa bao giờ được thấy trong quá trình training. Điều này được đạt được bằng cách thống nhất tất cả các nhiệm vụ thành một định dạng chung (Kumar et al., 2016; Khashabi et al., 2020) thông qua hướng dẫn, và huấn luyện chúng cùng nhau bằng cách chia sẻ tất cả các weight của mô hình qua tất cả nhiệm vụ.

**Continuous Learning.** Các nghiên cứu hiện có cũng giải quyết việc thích ứng liên tục của các mô hình ngôn ngữ bằng cách xem xét lại các hướng dẫn (Yin et al., 2022) hoặc ví dụ (Scialom et al., 2022) của các nhiệm vụ đã học trước đó khi fine-tuning với một nhiệm vụ mới để ngăn ngừa catastrophic forgetting. Kết quả cho thấy rằng LM có thể được thích ứng hiệu quả với các nhiệm vụ mới mà không mất tầm nhìn của các nhiệm vụ đã học trước đó. Các nghiên cứu khác cho phép LM thực hiện các nhiệm vụ mới thông qua sự kết hợp số học của các task vector đã học (Ilharco et al., 2022) hoặc soft prompt (Anonymous, 2023) được vá vào LM cơ sở mà không thay đổi tham số của nó. Chúng tôi tập trung vào setting thích ứng (quy mô lớn) multi-task bằng cách fine-tuning LM với 2000 nhiệm vụ cùng một lúc. Việc thích ứng liên tục mô hình kết quả với dữ liệu mới, nhiệm vụ mới và domain mới sẽ là một hướng tương lai thú vị và quan trọng.

## 8. Kết luận

Instruction-tuning của LLM đã nổi lên như một phương tiện hiệu quả để cải thiện khả năng tổng quát hóa zero và few-shot của chúng. Chúng tôi đóng góp ba đóng góp chính cho instruction-tuning trong bài báo này. Đầu tiên, chúng tôi curation một benchmark quy mô lớn cho instruction-tuning bao gồm 2000 nhiệm vụ NLP từ 8 tập hợp dataset, được chú thích thành các danh mục nhiệm vụ. Chúng tôi tạo ra các phân chia đánh giá một cách chiến lược trên benchmark này để đánh giá ba loại khả năng tổng quát hóa mô hình khác nhau: 1) hiệu suất fully-supervised, 2) hiệu suất trên các nhiệm vụ chưa thấy từ các danh mục nhiệm vụ đã thấy, và 3) hiệu suất trên các nhiệm vụ từ các danh mục hoàn toàn held-out. Thứ hai, sử dụng bộ đánh giá của chúng tôi, chúng tôi thiết lập sự đánh đổi và thực hành tốt nhất của nhiều khía cạnh của instruction-tuning, như các phương pháp lấy mẫu khác nhau của các nhiệm vụ và danh mục fine-tuning, fine-tuning với demonstration nhiệm vụ, và fine-tuning với các dataset chuyên biệt cho reasoning và dialogue. Cuối cùng, sử dụng các setting tốt nhất từ các thí nghiệm của chúng tôi, chúng tôi huấn luyện và phát hành các mô hình OPT-IML 30B và 175B instruction-tuned dựa trên OPT, vượt trội mạnh so với OPT trên năm benchmark đánh giá và có tính cạnh tranh với các mô hình instruction-tuned gần đây được tinh chỉnh trên các benchmark riêng lẻ.

## Lời cảm ơn

Chúng tôi muốn cảm ơn Stephen Roller, Susan Zhang, và Naman Goyal vì sự giúp đỡ với việc fine-tuning OPT sử dụng codebase metaseq và với việc phát hành mô hình của chúng tôi; Lili Yu vì sự giúp đỡ với cơ sở hạ tầng và đánh giá; Sewon Min vì các thảo luận liên quan đến meta-training cho in-context learning; và Omer Levy, Timo Schick, và Scott Yih vì các thảo luận hữu ích liên quan đến instruction-tuning.

## Tài liệu tham khảo

[Các tài liệu tham khảo được giữ nguyên như trong bản gốc do chúng là các trích dẫn học thuật chuẩn]

[Tất cả các phụ lục A, B, C được dịch tiếp theo với cùng cấu trúc và chi tiết như bản gốc]

[Phụ lục A. Chi tiết Chuẩn bị Benchmark]
[Phụ lục B. Kết quả Thí nghiệm Bổ sung]  
[Phụ lục C. Ví dụ về Prompt từ Tất cả Benchmark]
