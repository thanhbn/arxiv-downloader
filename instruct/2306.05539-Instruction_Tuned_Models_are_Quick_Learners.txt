# 2306.05539.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2306.05539.pdf
# File size: 1249599 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Instruction Tuned Models are Quick Learners
Himanshu Gupta1}Saurabh Arjun Sawant1}Swaroop Mishra1|
Mutsumi Nakamura1Arindam Mitra2Santosh Mashetty1Chitta Baral1
1Arizona State University2Microsoft Research
{hgupta35, ssawan13, srmishr1, mutsumi, cbaral}@asu.edu
Abstract
Instruction tuning of language models has demonstrated the ability to enhance
model generalization to unseen tasks via in-context learning using a few exam-
ples. However, typical supervised learning still requires a plethora of downstream
training data for ﬁnetuning. Often in real-world situations, there is a scarcity of
data available for ﬁnetuning, falling somewhere between few shot inference and
fully supervised ﬁnetuning. In this work, we demonstrate the sample efﬁciency of
instruction tuned models over various tasks by estimating the minimal downstream
training data required by them to perform transfer learning and match the perfor-
mance of state-of-the-art (SOTA) supervised models. We conduct experiments
on 119 tasks from Super Natural Instructions (SuperNI) in both the single task
learning (STL) and multi task learning (MTL) settings. Our ﬁndings reveal that, in
the STL setting, instruction tuned models equipped with 25% of the downstream
train data surpass the SOTA performance on the downstream tasks. In the MTL
setting, an instruction tuned model trained on only 6% of downstream training data
achieve SOTA, while using 100% of the training data results in a 3.69% points
improvement (ROUGE-L 74.68) over the previous SOTA. We conduct an analysis
on T5 vs Tk-Instruct by developing several baselines to demonstrate that instruction
tuning aids in increasing both sample efﬁciency and transfer learning. Addition-
ally, we observe a consistent 4%performance increase in both settings when
pre-ﬁnetuning is performed with instructions. Finally, we conduct a categorical
study and ﬁnd that contrary to previous results, tasks in the question rewriting and
title generation categories suffer from instruction tuning.1
1 Introduction
Large language models (LLM) have achieved remarkable performances on several benchmark
evaluation suites such as SuperGLUE [39], BIG-Bench Hard (BBH) [38], and HELM [17]. Research
on LLMs has explored their abilities to follow instructions [43, 25, 42] and has developed specialized
models for the same (Flan, Instruct-GPT, Tk-Instruct, T0) [43, 27, 33]. Recent studies in the
instruction paradigm demonstrate the generalizability of models that are instruction tuned on training
tasks and evaluated using few shot inference [42, 43], as shown in the ﬁrst row of Fig. 1. Despite this,
SOTA performance is obtained by fully supervised ﬁnetuning on all available downstream training
data, as shown in the 4th row of Fig. 1. In real-world situations, there is usually a limited amount of
data available for ﬁnetuning, which is somewhere between few shot inference and fully supervised
ﬁnetuning. Given this context, we pose the question - if we use a small amount of the data from these
downstream tasks, how quickly could the model learn in the instruction paradigm?
1}Co-ﬁrst authors |Currently in Google Brain
Baseline approach, data splits and scripts are freely available at https://github.com/srsawant34/
efficient_instruction_learning
Preprint. Under review.arXiv:2306.05539v1  [cs.CL]  17 May 2023

--- PAGE 2 ---
Figure 1: Showcasing the difference between the few shot inference, fully supervised ﬁnetuning, and
our proposed analysis. The ﬁrst row represents conventional few shot inference using Tk-Instruct
which results in a score of 54.30. The fourth row indicates supervised SOTA that uses 100% of
downstream train data to ﬁnetune T5-3B to get a SOTA score of 70.99. Our ﬁndings demonstrate
the quick learning ability of the instruction tuned model. Using only 6% of downstream train data,
Tk-Instruct achieved a score of 70.40. Surpassing SOTA by 2 points with 25% of downstream train
data, our results highlight the MTL setting.
To answer this, we evaluate the minimal downstream training data required by instruction tuned
models to perform transfer learning and match the performance of supervised SOTA models. We
experiment on unseen tasks of Super Natural Instructions (SuperNI) [42], comprising of 119 tasks.
We experiment with single-task learning (STL), i.e. training 119 task-speciﬁc models, and multi-task
learning (MTL), where a single model is trained to solve all 119 tasks. We use Tk-Instruct 3B (T5-3B,
instruction tuned on 757 tasks of SuperNI) as the instruction tuned model [42] and use T5-3B [31] as
our non-instruction tuned model. We ﬁnd that in the STL setting, we achieve competitive results with
just 5.91% of the training data (68.34 ROUGE-L) and surpass the supervised SOTA score when using
only 25.33% of the entire dataset (71.71 ROUGE-L). In the MTL setting, when using 6% of the train
split, we match the SOTA performance (70.40 ROUGE-L), as shown in the 2nd row of Fig. 1. We
outperform SOTA by roughly 2% (73.14 ROUGE-L) when using 25% of the train split (3rd row of
Fig. 1.) and 3.69% when using 100% of the train split. To the best of our knowledge, we are ﬁrst to
explore the space of sample efﬁciency in instruction tuned large language models in both STL and
MTL setups. Details about the experimental setup are described in §4, and results are described in §5.
We analyze the impact of instructions by investigating sample efﬁciency across diverse ranges, by
developing multiple baselines to simulate low resource settings pertaining to training data availability.
Our ﬁndings highlight sample efﬁciency achieved through instruction tuning, reaching up to 75%,
even in limited training data. We delve into the impact of instruction tuning as an initial pre-ﬁnetuning
step. We develop two baselines (for both STL and MTL setups) employing pre-ﬁnetuning without
instructions. These baselines undergo further ﬁnetuning on the downstream training set. Our ﬁndings
demonstrate an increase in the performance of Tk-Instruct over the baselines by 3% and 5% in the
STL and MTL setups, respectively. This highlights the impact of instructions during pre-ﬁnetuning in
terms of facilitating transfer learning. We ﬁnally perform a category-wise analysis to investigate the
impact of instruction tuning on different task categories. Our ﬁndings reveal that tasks falling under
the textual entailment category demonstrate the most substantial improvements through instruction
tuning. On the other hand, tasks related to question rewriting and title generation exhibit challenges
and limitations when subjected to instruction tuning.
2

--- PAGE 3 ---
Contributions: (a) we show that an instruction tuned model using just 6% of downstream train
data matches the performance of a supervised SOTA model. (b) we ﬁnd that the instruction tuned
models perform up to 3% better than the SOTA when instruction tuned with 100% of the data. (c) to
investigate scenarios with signiﬁcantly limited downstream train data, we conduct a comprehensive
analysis by constructing multiple baselines. (d) we show the impact of our method on various
categories of tasks.
2 Related Work
Multi-task learning using LLMs (Language Models) has consistently shown performance beneﬁts
over task-speciﬁc learning [25, 50, 18, 2, 46, 3, 53, 5]. Instruction-based learning has emerged as
a promising paradigm in LLMs [22, 35, 27, 12, 9, 20, 34, 23, 1, 37], with recent studies exploring
various aspects such as dialogue generation [8], multimodality [45], chain of thought [44], distributed
training [11], and federated learning [51]. Moreover, the effectiveness of Prompts and Instructions has
been demonstrated in low-resource settings [16, 30], and different variants of prompting, including
Scratchpad [26], Majority V oting [41], Reframing [24], Least-to-Most Prompting [54], and Question
Decomposition [13, 29], have proven effective across various tasks. Instruction-based techniques
have also shown efﬁcacy in different applications, such as NER [40], program synthesis [14], style
transfer [32], tabular question answering [21], relation extraction [4], and biomedical applications
[28]. However, the majority of the existing works have primarily focused on zero/few-shot inference
scenarios [10, 6].
Our experiments are meant to simulate a real-world setting where few shot inference is not always
necessary and there are some samples available for training. While there have been several studies on
sample efﬁciency in other domains like reinforcement learning [48, 7, 52, 49, 47, 15], to the best
of our knowledge, no other work has explored the sample efﬁciency of instruction tuned models in
a generalized fashion. We also provide detailed analysis and task-speciﬁc insights with respect to
various instruction tuning methods.
3 Instruction Tuning
For each given task t, we assume that there are input and output instance pairs (Xt; Yt). Each sample
of the task is described by its instruction inst.
Single-task Learning (STL) Traditional supervised models learn a mapping function ( fM) between
input ( x) and output ( y) by using a training set of input/output pairs, (x; y)2(Xt
train; Yt
train ), for a
given task t. The model is then evaluated on the test set for the same task, (Xt
test; Xt
test). In the STL
setup, tmodels are trained for ttasks in an individual fashion.
Multi-task Learning (MTL) In this setup, the training data for all tasks are combined together. The
goal of MTL models is to learn a mapping function ( fM) between the input ( x) and output ( y), such
thatfM(x) =y, where (x; y)2(Xt
train; Yt
train )for all ttasks in a combined way. This model is
then evaluated on task-speciﬁc instances (x; y)2(Xt
test; Yt
test). In contrast to single-task models, a
single model is used to solve various tasks in this setup, which allows for generalization.
Modelling Instruction Tuning In this setup, the mapping function takes an instruction insttalong
with the input sample to give output as y;fM(inst; x ) =y. Instruction tuning can be achieved in
both Single-task and Multi-task setups. Deﬁnition : The term "Deﬁnition" pertains to the detailed
explanation of the task at hand along with speciﬁc instructions provided, enabling the model to
successful completion of the given task. Examples : "Examples" refer to the input/output pairs
associated with a particular instance of the task. In line with the approach introduced in SuperNI, we
incorporate two examples within the instruction prompts.
3.1 Proposed Analysis
We introduce two datasets for our task: xpre finetune andxtrain . These datasets are utilized as the
pre-ﬁnetuning and downstream training datasets, respectively. By pre-ﬁnetuning an LLM fMusing
instructions (inst; x pre finetune ), we get instruction tuned model finst.finstis now instruction
tuned on the downstream train data finst(inst; x train ) =y. For each experiment, different number
of downstream train samples are used. The instruction prompts change according to each downstream
3

--- PAGE 4 ---
Figure 2: Formulation of the proposed analysis. In the Single-task learning (STL) setting, finstis
instruction tuned individually on each downstream dataset. In the Multi-task learning (MTL) setup,
all downstream training tasks are combined together, and finstis instruction tuned on all of them. In
both setups, the number of input samples from the downstream train data is varied.
task. For STL setup, finstis individually instruction tuned on all tasks of the downstream train data;
tmodels are ﬁnetuned for ttasks. Each experiment will consist of tmodels instruction tuned with a
different number of training samples (Column 1 of Fig. 2). For MTL setup, one dataset is prepared
by combining all the tasks of the downstream train dataset together. finstis instruction tuned on the
combined dataset. Similar to the last setting, the experiment will have a different number of training
samples to highlight sample efﬁciency.
3.2 Baselines
To show a detailed analysis of instruction tuned modelling, pre-ﬁnetuning and cross-task generaliza-
tion, we develop different baselines across both setups.
3.2.1 STL baselines
We develop three baselines to compare the proposed modelling paradigm comprehensively. For the
ﬁrst baseline, we pre-ﬁnetune the model fMusing xpre finetune without instructions to get fM1.
fM1is now individually ﬁnetuned on all ttasks of xtrain using 5.91% of downstream train data to
getfSTL baseline 1. For the second baseline ( fSTL baseline 2), we individually ﬁnetune the fM
model on all ttasks with 25.33% of the downstream train set (1000 samples of each task) of xtrain .
We develop the third baseline ( fSTL baseline 3) by individually ﬁnetuning the model fMonttasks
ofxtrain and use 100% of the downstream training set. Third baseline serves as the supervised SOTA.
The rationale for all three baselines is two-fold: First, to compare the baselines with the proposed
model with fewer training samples. Second, the baseline is also used to compare the instruction tuned
model trained with the same number of samples to observe the relative improvement in performance.
Both advantages can be explained through the following example: fSTL baseline 1can highlight
the effect of pre-ﬁnetuning, demonstrate sample efﬁciency and can be compared for performance
improvement when the same number of samples are used.
3.2.2 MTL baselines
We develop two baselines to compare with the proposed MTL instruction tuned modelling paradigm.
For the ﬁrst baseline, we ﬁnetune fMon 25.33% of the downstream train set xtrain in MTL setup
to get fMTL baseline 1. For developing the second baseline, we pre-ﬁnetune the model fMusing
xpre finetune without instructions to get fM1.fM1model is ﬁnetuned in MTL setup on all ttasks
using 5.91% of the downstream train data (200 samples of each task) of xtrain . The rationale
for creating two MTL baselines is similar to STL baselines; to show sample efﬁciency, highlight
4

--- PAGE 5 ---
Answerability
ClassiﬁcationCoreference
ResolutionData
to TextQuestion
RewritingTextual
EntailmentTitle
GenerationOther
CategoriesGrand Total
and percentage
# of tasks 13 14 9 11 24 18 30 119 (100%)
100 1300 1370 826 949 2376 1784 2994 11.5K (3.09%)
200 2600 2441 1626 1849 4457 3484 5708 22.1K (5.91%)
1000 11529 8831 8026 9049 19019 16514 21988 94.9K (25.33%)
All 43871 35560 36815 41391 78802 78357 59949 374.7K (100%)
Table 1: Category wise statistics of the downstream train data used.We note that since all the tasks
have unequal samples, the total samples in each category will be different than # of Tasks*Number of
samples. Rows 100, 200, 1000, and all samples represent the sum of the number of samples chosen
during each experiment.
performance improvement when using the same number of samples, and showcase the effect of
instructions in pre-ﬁnetuning.
4 Experiments
4.1 Dataset
Statistics
# Total Tasks 119
# Total instances in train set 374745
# Total instances in Test Set 11810
# Total instances in pre-ﬁnetuning set 75700
Avg len of Train data w instructions 364.97
Avg len of Train data w/o instructions 89.03
Table 2: Statistics of the SuperNI dataset. Train
and Test set refers to Downstream data.We use seen tasks of SuperNI as the pre-
ﬁnetuning set consisting of 757 tasks with 100
samples for each task. We used the unseen task
set of SuperNI as the downstream train data,
consisting of 119 tasks that could be classiﬁed
into 11 categories. Their statistics are presented
in Table 2. The Dataset is classiﬁed into 11 cate-
gories of NLP tasks as shown in Table 3. For the
sake of clarity, we have clubbed seven categories
with fewer tasks into the others category. Table
1 gives detailed statistics across each category.
Since not all the tasks have exactly the same
number of samples, we choose the maximum
number available if the number of samples is
below the threshold. For example, task 1388 (Table 6 of Appendix §B) has a total of 191 samples.
We use 191 samples when ﬁnetuning with 200, 1000, and the entire dataset.
Category Count of Category
Textual Entailment 24
Title Generation 18
Coreference Resolution 14
Answerability Classiﬁcation 13
Question Rewriting 11
Data to Text 9
Word Analogy 8
Cause Effect Classiﬁcation 7
Dialogue Act Recognition 7
Keyword Tagging 5
Overlap Extraction 2
Grammar Error Correction 1
Grand Total 119
Table 3: Training sample statisticsWe refer the reader to Appendix §B, total num-
ber of training samples in each task (Table 6),
and the number of samples used for develop-
ment of baselines and proposed analysis (Table
7).
Dataset Split: Hundred samples per task have
been used for testing, and ten samples have been
used for validation. These samples are not a
part of the training set, and we have ensured
that there is no data leakage. Each task contains
the same number of samples for the test (100
samples) and validation set (10 samples).
4.2 Baselines
We use xpre finetune andxtrain in both STL
and MTL setups. For creating baselines, both
datasets are not equipped with instructions; they
contain just input and output for conventional
ﬁnetuning.
5

--- PAGE 6 ---
4.2.1 Single Task Learning Baselines
STL Baseline 1: A T5-3B model undergoes pre-ﬁnetuning using xpre finetune , which consists of
757 tasks from SuperNI. Subsequently, the model undergoes further ﬁnetuning on 200 samples per
task from the downstream train data of SuperNI ( xtrain ), resulting in 119 models.
STL Baseline 2: Each task from the downstream train data ( xtrain ) is used to ﬁnetune a T5-3B
model ( fM) with 1000 samples per task, resulting in 119 distinct models.
STL Baseline 3 (Supervised SOTA): A T5-3B model ( fM) is ﬁnetuned for each task using all
available samples from the downstream train data.
4.2.2 Multi Task Learning Baselines
MTL Baseline 1: Similar to STL Baseline 1, we preﬁnetune a T5-3B xpre finetune for each task to
get the model fM1.fM1is now ﬁnetuned on 200 samples per task from the downstream train data
(xtrain ) of SuperNI (in an MTL fashion).
MTL Baseline 2: T5-3B model is now ﬁnetuned on 1000 samples per task from the downstream
train data ( xtrain ) of SuperNI (in an MTL fashion).
4.3 Models and Evaluation Metrics
Models: We use Tk-Instruct 3B as the instruction tuned model. For STL setup 952 models (119x8)
were trained and 9 models were trained for MTL setup resulting in a total of 961models for our
analysis. All the models were trained on 6x Nvidia A100 40GB GPUs.
Evaluation metric: We consider all the tasks in the dataset as text generation problems and use the
ROUGE-L score [19] to evaluate the generated outputs.
5 Results
Figure 3: Results in the STL setup. and 3, re-
spectively. The horizontal dashed line is marked
on the graph to highlight the difference in train
data required between the proposed approach and
baselines. x-axis is in logarithmic scale.The results are presented in two parts, STL and
MTL results. Each section contains overall re-
sults, Category wise results, and a comparison
with the baselines that were deﬁned earlier.
5.1 Single Task Learning setup (STL)
Fig. 3 shows the rouge score of instruction tuned
models when training with different numbers of
samples. We see that there is an overall increas-
ing trend as the number of samples increases.
From the ﬁgure we observe that a max ROUGE-
L score of 72.04 is obtained when all samples
are used. Category wise scores of the baselines
are present in Table 4. Figure 4 shows the cate-
gory wise results of the instruction tuned models.
From the ﬁgure, we see that except for the An-
swerability Classiﬁcation category, all the cate-
gories have an increasing trend. Detailed results
regarding each category, baseline, and instruc-
tion tuning of the MTL setup can be found in
Table 12, Table 8, and Table 9 in Appendix §B.
50% efﬁcient w.r.t STL baseline 1: STL Base-
line 1 is denoted by the red point in Figure 3. The score with 100 samples is 65.93, and the score
with baseline 1 is 64.29.
Competitive performance using 6% data: STL Baseline 2 is denoted by the yellow point in Figure
3). The instruction tuned model uses roughly 23.33% of training samples when trained on 6% data
compared to STL baseline 2 which uses 25.33% data. The score with 200 samples/task is 68.34,
6

--- PAGE 7 ---
Answerability
ClassiﬁcationCoreference
ResolutionData to TextQuestion
RewritingTextual
EntailmentTitle
GenerationOther
Categories
STL Baseline 1 70.38 70.01 49.75 68.12 71.00 44.71 72.31
STL Baseline 2 78.77 68.61 50.89 71.00 77.78 46.78 75.62
STL Baseline 3 80.36 74.40 52.84 71.11 80.42 48.35 76.82
Table 4: STL category wise scores for all three baselines. We see that all the baselines follow a
linearly increasing trend as the number of samples increases with baselines 1, 2, and 3 (200, 1000, and
all samples used, respectively). However, little improvement is observed in the Question rewriting
category w.r.t baseline 2 and 3’s ROUGE-L score (71.00 and 71.11, respectively). Another deviation
from the standard trend was observed in the Coreference Resolution category where STL baseline 1
had a higher score as compared to STL baseline 2 (70.01 and 68.61 respectively).
Figure 4: Histogram showing category wise results of the proposed approach in STL setting. The
x-axis shows avg number of training samples. the y-axis shows the rouge-L scores. Most categories
follow a conventional trend of performance increase as the number of training samples increases.
This trend has an exception in two places. First: Answerability classiﬁcation score drops when all
samples are used after 1000 (75.15 to 74.38). Second: Coreference Resolution score drops when 200
samples are used after 100 (75.74 to 74.25).
while the score with baseline 2 is 68.91. In comparison with STL baseline 1 with the instruction
tuned model (both trained using the 6% data), an increase of 3%is observed.
Surpassing SOTA with 25% data: The instruction tuned model uses 25.33% of the data compared
to STL baseline 3 and gets a score of 71.71, compared to the 70.99 score of the baseline. Comparison
with baseline 2 (both trained using 25.33% of the data) yields an increment of 3%. When all samples
are used, there is a further increase of 1.04% (72.04 vs 70.99).
Category wise effect of instruction tuning: We observe that Answerability Classiﬁcation and Title
Generation categories scores decrease from instruction tuning as compared to baselines. The best
scores from instruction tuning are 75.15 and 44.55 respectively which are signiﬁcantly lower than the
best baseline scores of 80.36 and 48.35. The categories that beneﬁt from instruction tuning compared
to the baselines are Coreference Resolution (82.82 vs 74.40) and Data to Text (59.06 vs 52.84).
5.2 Multi task setup
Figure 5 shows the overall ROUGE-L score of instruction tuned models in the MTL setup. A max
ROUGE-L score of 74.68 is obtained when all samples are used, surpassing the SOTA of 70.99.
Figure 6 shows the category wise results of the instruction tuned models and Table 5 showcases the
baseline results. Detailed results regarding each category, baseline, and instruction tuning of the MTL
setup can be found in Table 13, Table 10, and Table 11 respectively in Appendix §B.
7

--- PAGE 8 ---
Answerability
ClassiﬁcationCoreference
ResolutionData to TextQuestion
RewritingTextual
EntailmentTitle
GenerationOther
Categories
MTL Baseline 1 75.35 75.87 55.85 75.64 65.95 62.19 45.20
MTL Baseline 2 76.89 81.93 54.73 79.80 67.89 67.18 42.19
Table 5: MTL baseline category-wise scores. All categories follow an increasing trend as conventional
thinking would suggest. The trend is however broken in the Data to Text category and other categories.
Figure 6: Histogram showing category wise results of the proposed approach in the MTL setting.
Similar to STL category wise scores, a linear trend is followed but has two exceptions. First: Data to
text score drops when all samples are used after 1000 (58.83 to 53.28). Second: Title Generation
score drops when all samples are used after 1000 (43.94 to 38.16)
50% efﬁcient compared to MTL Baseline 1: MTL baseline 1 was trained on roughly 6% of
downstream train samples. The score with 3% downstream data samples is 66.78, while the score
with baseline 1 is 65.63. If we compare the instruction tuned model trained 6% of downstream train
samples, there is an increase of roughly 5% points as it reaches a 70.40 rouge score.
Figure 5: Proposed model overall results in MTL
setup. The Red and Yellow dots represent MTL
Baselines 1 and 2, respectively. The score gap
between the proposed approach and the baseline
widens as compared to the STL setup.Surpassing SOTA with 6% train data:
MTL baseline two is denoted by the yellow
point in Figure 3, and was trained using 25%
of downstream train samples. The instruction
tuned model uses76% fewer samples when
trained on 6% of downstream train samples and
gets a score of 70.40, while the score with base-
line 2 is 68.10. The instruction tuned approach,
trained on the same samples as baseline 2, im-
proves by roughly 5% (73.14 vs. 68.10). When
all samples are used, a score of 74.68 is obtained,
surpassing SOTA by 3%.
Category wise effect of instruction tuning:
Contrasting results to STL settings are observed
as Question Rewriting and Title Generation cat-
egories experience a signiﬁcant drop (12% and
23% points respectively) as compared to the
best baseline scores. There is a signiﬁcant im-
provement observed in the Textual Entailment
category as the best score improves to 84.16
from 67.89 as compared to the baseline score.
8

--- PAGE 9 ---
5.3 Analysis
5.3.1 Category Wise Analysis
We analyze the performance across each category in both settings. In the STL setting, we ﬁnd that
the tasks belonging to the coreference resolution and data to text category have a high increase
in ROUGE-L score with instruction tuning as compared to baseline approaches (78.23 vs. 71.00
ROUGE-L in coreference resolution and 57.88 vs. 51.16 in Data to Text). Question rewriting
performed nearly the same (70.51 vs. 70.07 ROUGE-L) while answerability classiﬁcation and title
generation’s score decreased w.r.t baseline (74.10 vs. 76.50 ROUGE-L in answerability classiﬁcation
and 43.36 vs. 46.61 in title generation). In the MTL setup, similar ﬁndings are observed but across
different categories. We ﬁnd that the tasks belonging to the textual entailment category have the
highest increase with instruction tuning compared to the baseline (81.93 vs. 66.91 ROUGE-L).
Answerability classiﬁcation performed nearly the same (75.97 vs. 76.11 ROUGE-L) while question
rewriting and title generation’s score decreased w.r.t baseline (67.38 vs. 77.71 ROUGE-L in question
rewriting and 43.99 vs. 64.68 in title generation).
MTL consistently outperforming STL: We have performed multiple experiments across instruction
tuned modelling settings while keeping the number of training samples the same across different
settings. Across each training setup, there is an increase of 1-2% ROUGE-L in MTL setup as
compared to STL. Through both settings and all the experiments, it was evident that instruction tuned
models perform better in the multi-task setup as compared to the single-task setup.
Sample Efﬁciency: Instruction tuned models showcase sample efﬁciency across both MTL and STL
setups. Using multiple baselines, sample efﬁciency of roughly 50, 75, and 80% are achieved across
different spaces in both STL and MTL setups. We also see that when all samples are used in an
instruction tuned setting, the overall performance beats SOTA.
Effect of Instructions in pre-ﬁnetuning: STL Baseline 1 and MTL Baseline 1 were pre-ﬁnetuned
with 757 tasks of the SupperNI dataset but without instructions. They were later ﬁnetuned on 119
tasks downstream train data using 6% in STL and MTL fashion. Instructions have a signiﬁcant effect
in pretraining as the instruction tuned model outperformed these baselines by 4 and 5%, respectively,
when trained with the same number of samples.
6 Conclusion, Limitations and Future work
In this study, we have taken a signiﬁcant step forward in advancing the instruction paradigm by
incorporating a small portion of training data commonly available for downstream tasks. By instruc-
tion tuning models on the small-scale training sets of downstream tasks, we have observed notable
performance beneﬁts for the Tk-instruct model on SuperNI. These ﬁndings suggest that instruction
tuning can effectively assist a model in quickly learning a task even with limited data. While our
work presents promising results, there are several limitations that need to be acknowledged.
Firstly, our experiments were limited to using T5-3B and its instruction tuned variant. We were unable
to conduct experiments using T5-11B and its instruction tuned variant due to resource constraints.
Therefore, further investigation using larger models and datasets would provide a more comprehensive
understanding of the instruction tuning approach. Additionally, our experiments focused solely on
SuperNI, and we trained a total of 950+ models in the process. However, to achieve a more robust
evaluation, it is necessary to extend our investigation to larger benchmarks such as BigBench [36].
Nonetheless, evaluating these bigger benchmarks would require substantial time and resources.
In conclusion, our work represents an initial step towards making the instruction paradigm more real-
istic. Future research can build upon our ﬁndings to enhance performance across various benchmarks.
Addressing the limitations mentioned above and conducting experiments with more extensive re-
sources will contribute to a deeper understanding of instruction tuning and its potential for improving
model performance in downstream tasks.
References
[1] Ryan R. Anderson. “Vision Encoders in Visual Question Answering”. In: 2022.
9

--- PAGE 10 ---
[2] Jifan Chen, Yuhao Zhang, Lan Liu, Rui Dong, Xinchi Chen, Patrick Ng, William Yang Wang,
and Zhiheng Huang. “Improving Cross-task Generalization of Uniﬁed Table-to-text Models
with Compositional Task Conﬁgurations”. In: arXiv preprint arXiv:2212.08780 (2022).
[3] Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, and Weijie J Su. “Weighted Training
for Cross-Task Learning”. In: International Conference on Learning Representations . 2021.
[4] Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo
Si, and Huajun Chen. “Adaprompt: Adaptive prompt-based ﬁnetuning for relation extraction”.
In:arXiv e-prints (2021), arXiv–2104.
[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. “Scaling instruction-ﬁnetuned
language models”. In: arXiv preprint arXiv:2210.11416 (2022).
[6] Yuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang. “Learning Instructions with Unlabeled
Data for Zero-Shot Cross-Task Generalization”. In: arXiv preprint arXiv:2210.09175 (2022).
[7] Xu Guo, Boyang Albert Li, and Han Yu. “Improving the Sample Efﬁciency of Prompt Tuning
with Domain Adaptation”. In: ArXiv abs/2210.02952 (2022).
[8] Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskénazi, and Jeffrey P.
Bigham. “InstructDial: Improving Zero and Few-shot Generalization in Dialogue through
Instruction Tuning”. In: Conference on Empirical Methods in Natural Language Processing .
2022.
[9] Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. “Instruction Induction: From
Few Examples to Natural Language Task Descriptions”. In: arXiv preprint arXiv:2205.10782
(2022).
[10] Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and Matthew E. Peters.
“HINT: Hypernetwork Instruction Tuning for Efﬁcient Zero-Shot Generalisation”. In: ArXiv
abs/2212.10315 (2022).
[11] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae
Lee, Kyungjae Lee, and Minjoon Seo. “Exploring the Beneﬁts of Training Expert Language
Models over Instruction Tuning”. In: ArXiv abs/2302.03202 (2023).
[12] Weixi Kang, Sònia Pineda Hernández, Junxin Wang, and Antonio Malvaso. “Instruction-based
learning: A review”. In: Neuropsychologia (2022), p. 108142.
[13] Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. “Text
modular networks: Learning to decompose tasks in the language of existing models”. In: arXiv
preprint arXiv:2009.00751 (2020).
[14] Kirby Kuznia, Swaroop Mishra, Mihir Parmar, and Chitta Baral. “Less is more: Summary of
long instructions is better for program synthesis”. In: arXiv preprint arXiv:2203.08597 (2022).
[15] Gabriele Lagani, F. Falchi, Claudio Gennaro, and Giuseppe Amato. “Hebbian Semi-Supervised
Learning in a Sample Efﬁciency Setting”. In: Neural networks : the ofﬁcial journal of the
International Neural Network Society 143 (2021), pp. 719–731.
[16] Teven Le Scao and Alexander M Rush. “How many data points is a prompt worth?” In:
Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies . 2021, pp. 2627–2636.
[17] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. “Holistic evaluation of
language models”. In: arXiv preprint arXiv:2211.09110 (2022).
[18] Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren. “Unsupervised
cross-task generalization via retrieval augmentation”. In: arXiv preprint arXiv:2204.07937
(2022).
[19] Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries”. In: Text
Summarization Branches Out . Barcelona, Spain: Association for Computational Linguistics,
July 2004, pp. 74–81. URL:https://aclanthology.org/W04-1013 .
[20] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
Colin A Raffel. “Few-shot parameter-efﬁcient ﬁne-tuning is better and cheaper than in-context
learning”. In: Advances in Neural Information Processing Systems 35 (2022), pp. 1950–1965.
[21] Man Luo, Sharad Saxena, Swaroop Mishra, Mihir Parmar, and Chitta Baral. “BioTABQA:
Instruction Learning for Biomedical Table Question Answering”. In: arXiv preprint
arXiv:2207.02419 (2022).
10

--- PAGE 11 ---
[22] Itzik Malkiel and Lior Wolf. “Maximal Multiverse Learning for Promoting Cross-Task General-
ization of Fine-Tuned Language Models”. In: Proceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computational Linguistics: Main Volume . Online: Associa-
tion for Computational Linguistics, Apr. 2021, pp. 187–199. DOI:10.18653/v1/2021.eacl-
main.14 .URL:https://aclanthology.org/2021.eacl-main.14 .
[23] Rakesh Menon, Sayan Ghosh, and Shashank Srivastava. “CLUES: A Benchmark for Learning
Classiﬁers using Natural Language Explanations”. In: Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) . 2022, pp. 6523–
6546.
[24] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. “Re-
framing Instructional Prompts to GPTk’s Language”. In: Findings of the Association for
Computational Linguistics: ACL 2022 . Dublin, Ireland: Association for Computational Lin-
guistics, May 2022, pp. 589–612. DOI:10.18653/v1/2022.findings- acl.50 .URL:
https://aclanthology.org/2022.findings-acl.50 .
[25] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. “Cross-Task Gen-
eralization via Natural Language Crowdsourcing Instructions”. In: Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .
2022, pp. 3470–3487.
[26] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. “Show your
work: Scratchpads for intermediate computation with language models”. In: arXiv preprint
arXiv:2112.00114 (2021).
[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. “Training language models
to follow instructions with human feedback”. In: Advances in Neural Information Processing
Systems 35 (2022), pp. 27730–27744.
[28] Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral.
“In-BoXBART: Get Instructions into Biomedical Multi-Task Learning”. In: Findings of the
Association for Computational Linguistics: NAACL 2022 . Seattle, United States: Association
for Computational Linguistics, July 2022, pp. 112–128. DOI:10.18653/v1/2022.findings-
naacl.10 .URL:https://aclanthology.org/2022.findings-naacl.10 .
[29] Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. “Is a Question Decomposition
Unit All We Need?” In: arXiv preprint arXiv:2205.12538 (2022).
[30] Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, and Chitta Baral. “How Many Data
Samples is an Additional Instruction Worth?” In: arXiv preprint arXiv:2203.09161 (2022).
[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, Peter J Liu, et al. “Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer.” In: J. Mach. Learn. Res. 21.140 (2020), pp. 1–67.
[32] Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei.
“A recipe for arbitrary text style transfer with large language models”. In: arXiv preprint
arXiv:2109.03910 (2021).
[33] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, et al. “Multitask Prompted Training
Enables Zero-Shot Task Generalization”. In: International Conference on Learning Represen-
tations . 2021.
[34] Timo Schick and Hinrich Schütze. “True Few-Shot Learning with Prompts—A Real-World
Perspective”. In: Transactions of the Association for Computational Linguistics 10 (2022),
pp. 716–731.
[35] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. “Concept2robot:
Learning manipulation concepts from instructions and human demonstrations”. In: The Inter-
national Journal of Robotics Research 40.12-14 (2021), pp. 1419–1434.
[36] Aarohi Srivastava et al. “Beyond the Imitation Game: Quantifying and extrapolating the
capabilities of language models”. In: ArXiv abs/2206.04615 (2022).
[37] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang,
Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. “Selective annotation makes language
models better few-shot learners”. In: arXiv preprint arXiv:2209.01975 (2022).
11

--- PAGE 12 ---
[38] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung
Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. “Chal-
lenging BIG-Bench tasks and whether chain-of-thought can solve them”. In: arXiv preprint
arXiv:2210.09261 (2022).
[39] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel Bowman. “Superglue: A stickier benchmark for general-purpose
language understanding systems”. In: Advances in neural information processing systems 32
(2019).
[40] Liwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan, Sirui Wang, Wei Wu, and Weiran Xu.
“InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER”.
In:arXiv preprint arXiv:2203.03903 (2022).
[41] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. “Self-Instruct: Aligning Language Model with Self Generated
Instructions”. In: arXiv preprint arXiv:2212.10560 (2022).
[42] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap,
Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob
Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Morad-
shahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra,
Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. “Super-NaturalInstructions:
Generalization via Declarative Instructions on 1600+ NLP Tasks”. In: Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing . Abu Dhabi, United
Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 5085–5109. URL:
https://aclanthology.org/2022.emnlp-main.340 .
[43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. “Finetuned Language Models are Zero-Shot Learners”. In:
International Conference on Learning Representations . 2022. URL:https://openreview.
net/forum?id=gEZrGCozdqR .
[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai-hsin Chi, F. Xia, Quoc
Le, and Denny Zhou. “Chain of Thought Prompting Elicits Reasoning in Large Language
Models”. In: ArXiv abs/2201.11903 (2022).
[45] Zhiyang Xu, Ying Shen, and Lifu Huang. “MultiInstruct: Improving Multi-Modal Zero-Shot
Learning via Instruction Tuning”. In: ArXiv abs/2212.10773 (2022).
[46] Chenxiao Yang, Junwei Pan, Xiaofeng Gao, Tingyu Jiang, Dapeng Liu, and Guihai Chen.
“Cross-task knowledge distillation in multi-task recommendation”. In: Proceedings of the AAAI
Conference on Artiﬁcial Intelligence . V ol. 36. 2022, pp. 4318–4326.
[47] Shuo Yang, Yijun Dong, Rachel A. Ward, Inderjit S. Dhillon, Sujay Sanghavi, and Qi Lei. “Sam-
ple Efﬁciency of Data Augmentation Consistency Regularization”. In: ArXiv abs/2202.12230
(2022).
[48] Zhengyu Yang, Kan Ren, Xufang Luo, Minghuan Liu, Weiqing Liu, J. Bian, Weinan Zhang, and
Dongsheng Li. “Towards Applicable Reinforcement Learning: Improving the Generalization
and Sample Efﬁciency with Policy Ensemble”. In: International Joint Conference on Artiﬁcial
Intelligence . 2022.
[49] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus.
“Improving Sample Efﬁciency in Model-Free Reinforcement Learning from Images”. In: AAAI
Conference on Artiﬁcial Intelligence . 2019.
[50] Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. “CrossFit: A Few-shot Learning Challenge for
Cross-task Generalization in NLP”. In: Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing . 2021, pp. 7163–7189.
[51] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin Wang, and
Yiran Chen. “Towards Building the Federated GPT: Federated Instruction Tuning”. In: 2023.
[52] Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. “On the
Convergence and Sample Efﬁciency of Variance-Reduced Policy Gradient Method”. In: Neural
Information Processing Systems . 2021.
12

--- PAGE 13 ---
[53] Yichi Zhang and Joyce Chai. “Hierarchical Task Learning from Language Instructions with
Uniﬁed Transformers and Self-Monitoring”. In: Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021 . 2021, pp. 4202–4213.
[54] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. “Least-to-Most Prompting Enables
Complex Reasoning in Large Language Models”. In: arXiv preprint arXiv:2205.10625 (2022).
13

--- PAGE 14 ---
Appendix
A Task Descriptions
Answerability Classiﬁcation: Answerability classiﬁcation is a natural language processing (NLP)
task that involves determining whether a given text contains a question that can be answered. This
task can be useful in a variety of applications, such as chatbots or information retrieval systems,
where it is important to know whether a user’s input is a question that can be answered by the system.
To perform answerability classiﬁcation, an NLP model must ﬁrst be trained on a dataset of texts
labeled as either "answerable" or "unanswerable." The model can then be used to classify new texts
as either answerable or unanswerable based on their similarity to the texts in the training dataset.
Table 15 gives an example of this category.
Cause Effect Classiﬁcation: Cause-effect classiﬁcation is a natural language processing (NLP)
task that involves determining the causal relationships between events or actions described in text. This
task can be useful in a variety of applications, such as information extraction and text summarization,
where it is important to understand the underlying causes and effects of events described in text.
Example: Consider the following two sentences: "The car wouldn’t start because the battery was
dead." "The child was crying because he fell and skinned his knee." In the ﬁrst sentence, the cause is
"the battery was dead," and the effect is "the car wouldn’t start." In the second sentence, the cause is
"he fell and skinned his knee," and the effect is "the child was crying."
Coreference Resolution: Coreference resolution is a natural language processing (NLP) task that
involves identifying and linking mentions of the same real-world entities in text. This task is important
for understanding the meaning and context of text, as it allows a system to determine that multiple
mentions of a word or phrase in a document refer to the same entity. For example, consider the
following text: "John went to the store to buy some milk. He needed it for his cereal." In this text, the
pronouns "he" and "his" refer to the same person, "John." A coreference resolution system would
identify these pronouns as referring to the same entity and link them to the proper noun "John." Table
16 gives an example of this category.
Data-to-text: Data-to-text generation is a natural language processing (NLP) task that involves
automatically generating human-readable text from structured data. This task can be useful in a
variety of applications, such as automated report generation or data summarization, where it is
important to present data in a clear and concise manner. An example of data-to-text generation
is generating a weather report from data about the current temperature, humidity, and forecast for
a particular location. The data might include the following: Temperature: 75 degrees Fahrenheit
Humidity: 50% Forecast: sunny A data-to-text generation system could use this data to generate
the following text: "The current temperature is 75 degrees Fahrenheit and the humidity is 50%. The
forecast for today is sunny." Table 17 gives an example of this category.
Dialogue Act Recognition: Dialogue act recognition is a natural language processing (NLP) task
that involves identifying the purpose or intention behind a speaker’s words in a conversation. This
task can be useful in a variety of applications, such as chatbots or virtual assistants, where it is
important to understand the intent behind a user’s input in order to respond appropriately. An example
of dialogue act recognition is identifying the intent behind the following statement: "Can you pass
the salt?" The dialogue act in this statement might be classiﬁed as a request, as the speaker is asking
the listener to perform an action.
Grammar Error Correction: Grammar error correction is a natural language processing (NLP)
task that involves identifying and correcting grammatical errors in a given text. An example of a
sentence with a grammatical error that could be corrected as part of this task is: "I went to the stores
to buy some food and clothes." This sentence contains the grammatical error of using the wrong form
of the word "store." The correct form should be "store," which is singular, as in "I went to the store to
buy some food and clothes."
Keyword Tagging: Keyword tagging is the process of assigning speciﬁc keywords or labels to
a piece of text or document. This task is often used in natural language processing (NLP) to help
14

--- PAGE 15 ---
classify and organize large amounts of text data for various purposes, such as search engines, topic
modeling, and sentiment analysis. For example, consider a news article about recent political events
in the United States. Keyword tagging for this article might include labels such as "politics," "US
politics," "election," "government," and "political parties." These tags can help identify the main
themes and topics discussed in the article, making it easier for users to search for and ﬁnd similar
articles on the same topics.
Overlap Extraction: Overlap extraction is a natural language processing (NLP) task that involves
extracting overlapping text or data from multiple sources. This can be useful for a variety of purposes,
such as identifying common themes in different documents, comparing and contrast information, or
ﬁnding duplicates in a dataset. For example, consider a scenario where you have two news articles
discussing the same topic. You might use overlap extraction to identify the common themes or ideas
discussed in both articles, such as the main events, people involved, or key quotes. This could help
you understand the overall coverage of the topic and identify any discrepancies or differences in the
way it was presented by the two sources.
Question Rewriting: Question Rewriting is a natural language processing (NLP) task that involves
generating a new version of a given question that has the same meaning as the original, but is phrased
differently. For example, given the question "What is the capital of France?", a question rewriting
task might generate the following rephrased question: "Where is the seat of government for France
located?". Table 18 gives an example of this category.
Textual Entailment: Textual entailment is a natural language processing task that involves deter-
mining the relationship between two text passages. Speciﬁcally, it involves determining whether one
passage, called the "hypothesis," can be inferred from the other passage, called the "premise."
For example:
Premise: "The cat is sitting on the couch." Hypothesis: "There is a cat on the couch."
In this case, the hypothesis can be inferred from the premise, so the textual entailment relationship is
"entailment." Table 14 gives an example of this category.
Title Generation: Title generation is a natural language processing (NLP) task that involves
creating a title for a given text or topic. This task is often used in content creation and marketing,
where an eye-catching title is essential for attracting attention and engaging readers. For example, a
title generation task might involve creating a title for an article about the beneﬁts of meditation. Some
possible titles might be "5 Reasons Why Meditation is the Key to a Stress-Free Life," "Discover
the Surprising Beneﬁts of Meditation," or "Meditation: The Ultimate Tool for Relaxation and
Mindfulness." The goal of the title generation task is to generate a title that accurately reﬂects the
content of the article and is compelling enough to encourage readers to click and read more. Table 19
gives an example of this category.
Word Analogy: Word analogy is a natural language processing task that involves identifying
relationships between words based on their meanings and contexts. The goal is to ﬁnd a word that
is similar to another word in a speciﬁc way, based on the relationship between the two words. For
example, if the task is to ﬁnd a word that is similar to "man" in the same way that "woman" is similar
to "man," the correct answer would be "wife." The relationship between the words "man" and "wife"
is that they are both terms for a speciﬁc type of spouse, with "man" being the term for a husband and
"wife" being the term for a wife.
B Detailed Results
Hyperparameters: Train batch size: 1, Eval Batch size: 1, Gradient Accumulation Steps: 2, Max
source length: 1024, Max target length 128, generation max length: 128, learning rate: 5e-05, number
of epochs: 2, warmup steps: 0
Other results and datasets: Table 6 gives the task-wise statistics, the number of total samples, and
their category. Table 5 is the pivot table of the same, giving a category-wise count of the tasks. Table
15

--- PAGE 16 ---
7 gives the total number of samples used for different baselines and STL MTL methods. Table 8 gives
STL baseline scores. Table 9 gives STL Model results. Table 12 gives a category-wise average for the
same. Table 10 gives MTL baselines, and 11 gives the MTL results, and Table 13 gives category-wise
pivot for the same.
16

--- PAGE 17 ---
Task
NumberTask
CategoryTrain
SamplesTask
NumberTask
CategoryTrain
Samples
task020 Answerability Classiﬁcation 271 task201 Textual Entailment 6390
task033 Coreference Resolution 6390 task202 Textual Entailment 6388
task034 Question Rewriting 6390 task219 Title Generation 6390
task035 Question Rewriting 6394 task220 Title Generation 6389
task036 Keyword Tagging 812 task226 Answerability Classiﬁcation 368
task039 Overlap Extraction 6390 task232 Answerability Classiﬁcation 6390
task050 Answerability Classiﬁcation 5802 task233 Answerability Classiﬁcation 6389
task102 Data to Text 5298 task242 Answerability Classiﬁcation 5884
task1152 Word Analogy 94 task249 Coreference Resolution 574
task1153 Word Analogy 1908 task281 Overlap Extraction 1365
task1154 Word Analogy 694 task288 Title Generation 1823
task1155 Word Analogy 436 task290 Answerability Classiﬁcation 3372
task1156 Word Analogy 548 task304 Coreference Resolution 6389
task1157 Word Analogy 857 task329 Coreference Resolution 4339
task1158 Word Analogy 266 task330 Coreference Resolution 3849
task1159 Word Analogy 588 task349 Answerability Classiﬁcation 6388
task1161 Title Generation 6390 task362 Dialogue Act Recognition 6285
task1195 Question Rewriting 6384 task391 Cause Effect Classiﬁcation 2173
task121 Question Rewriting 17 task392 Cause Effect Classiﬁcation 2493
task133 Coreference Resolution 2745 task393 Cause Effect Classiﬁcation 100
task1342 Title Generation 6107 task401 Coreference Resolution 3636
task1344 Textual Entailment 2370 task402 Question Rewriting 2935
task1345 Question Rewriting 6390 task418 Title Generation 3229
task1356 Title Generation 6017 task442 Question Rewriting 1686
task1358 Title Generation 6017 task500 Title Generation 6390
task1385 Textual Entailment 880 task510 Title Generation 6389
task1386 Textual Entailment 883 task520 Answerability Classiﬁcation 890
task1387 Textual Entailment 1083 task569 Title Generation 6379
task1388 Textual Entailment 191 task602 Title Generation 84
task1390 Coreference Resolution 538 task613 Keyword Tagging 6390
task1391 Coreference Resolution 6381 task614 Cause Effect Classiﬁcation 6387
task1393 Cause Effect Classiﬁcation 386 task619 Title Generation 1149
task1394 Dialogue Act Recognition 114 task620 Keyword Tagging 1151
task1407 Data to Text 3019 task623 Keyword Tagging 290
task1409 Data to Text 6372 task640 Textual Entailment 100
task1439 Answerability Classiﬁcation 2420 task641 Textual Entailment 88
task1442 Answerability Classiﬁcation 1769 task642 Textual Entailment 190
task1516 Textual Entailment 599 task645 Keyword Tagging 1890
task1529 Textual Entailment 4950 task648 Coreference Resolution 171
task1531 Dialogue Act Recognition 387 task670 Question Rewriting 4639
task1533 Dialogue Act Recognition 6052 task671 Question Rewriting 4639
task1534 Dialogue Act Recognition 6051 task677 Data to Text 4400
task1540 Title Generation 2895 task738 Textual Entailment 6291
task1554 Textual Entailment 6385 task743 Title Generation 540
task1557 Grammar Error Correction 644 task760 Data to Text 26
task1562 Question Rewriting 32 task769 Title Generation 890
task1586 Title Generation 4890 task827 Cause Effect Classiﬁcation 886
task1598 Data to Text 6384 task828 Cause Effect Classiﬁcation 886
task1612 Textual Entailment 1690 task879 Dialogue Act Recognition 2195
task1615 Textual Entailment 1687 task880 Dialogue Act Recognition 1231
task1622 Question Rewriting 1885 task890 Textual Entailment 88
task1624 Answerability Classiﬁcation 1390 task891 Coreference Resolution 87
task1631 Data to Text 2880 task892 Coreference Resolution 83
task1640 Answerability Classiﬁcation 2538 task893 Coreference Resolution 100
task1659 Title Generation 6389 task935 Textual Entailment 6390
task1664 Coreference Resolution 278 task936 Textual Entailment 6390
task1728 Data to Text 6385 task937 Textual Entailment 6389
task190 Textual Entailment 6390 task957 Data to Text 2051
task199 Textual Entailment 6389 task970 Textual Entailment 2247
task200 Textual Entailment 4354
Table 6: Dataset Statistics which tell the max number of train samples available to each dataset
present.
17

--- PAGE 18 ---
Task
NumberTrain
Samples
in 10Train
Samples
in 100Train
Samples
in 200
STL Setup
Baseline 1
MLT Setup
Baseline 2Train
Samples
in 1000
STL Setup
Baseline 2
MLT Setup
Baseline 1Train
Samples
in All
STL Setup
Baseline 3Task
NumberTrain
Samples
in 10Train
Samples
in 100Train
Samples
in 200
STL Setup
Baseline 1
MLT Setup
Baseline 2Train
Samples
in 1000
STL Setup
Baseline 2
MLT Setup
Baseline 1Train
Samples
in All
STL Setup
Baseline 3
task020 10 100 200 271 271 task201 10 100 200 1000 6390
task033 10 100 200 1000 6390 task202 10 100 200 1000 6388
task034 10 100 200 1000 6390 task219 10 100 200 1000 6390
task035 10 100 200 1000 6394 task220 10 100 200 1000 6389
task036 10 100 200 812 812 task226 10 100 200 368 368
task039 10 100 200 1000 6390 task232 10 100 200 1000 6390
task050 10 100 200 1000 5802 task233 10 100 200 1000 6389
task102 10 100 200 1000 5298 task242 10 100 200 1000 5884
task1152 10 94 94 94 94 task249 10 100 200 574 574
task1153 10 100 200 1000 1908 task281 10 100 200 1000 1365
task1154 10 100 200 694 694 task288 10 100 200 1000 1823
task1155 10 100 200 436 436 task290 10 100 200 1000 3372
task1156 10 100 200 548 548 task304 10 100 200 1000 6389
task1157 10 100 200 857 857 task329 10 100 200 1000 4339
task1158 10 100 200 266 266 task330 10 100 200 1000 3849
task1159 10 100 200 588 588 task349 10 100 200 1000 6388
task1161 10 100 200 1000 6390 task362 10 100 200 1000 6285
task1195 10 100 200 1000 6384 task391 10 100 200 1000 2173
task121 10 17 17 17 17 task392 10 100 200 1000 2493
task133 10 100 200 1000 2745 task393 10 100 100 100 100
task1342 10 100 200 1000 6107 task401 10 100 200 1000 3636
task1344 10 100 200 1000 2370 task402 10 100 200 1000 2935
task1345 10 100 200 1000 6390 task418 10 100 200 1000 3229
task1356 10 100 200 1000 6017 task442 10 100 200 1000 1686
task1358 10 100 200 1000 6017 task500 10 100 200 1000 6390
task1385 10 100 200 880 880 task510 10 100 200 1000 6389
task1386 10 100 200 883 883 task520 10 100 200 890 890
task1387 10 100 200 1000 1083 task569 10 100 200 1000 6379
task1388 10 100 191 191 191 task602 10 84 84 84 84
task1390 10 100 200 538 538 task613 10 100 200 1000 6390
task1391 10 100 200 1000 6381 task614 10 100 200 1000 6387
task1393 10 100 200 386 386 task619 10 100 200 1000 1149
task1394 10 100 114 114 114 task620 10 100 200 1000 1151
task1407 10 100 200 1000 3019 task623 10 100 200 290 290
task1409 10 100 200 1000 6372 task640 10 100 100 100 100
task1439 10 100 200 1000 2420 task641 10 88 88 88 88
task1442 10 100 200 1000 1769 task642 10 100 190 190 190
task1516 10 100 200 599 599 task645 10 100 200 1000 1890
task1529 10 100 200 1000 4950 task648 10 100 171 171 171
task1531 10 100 200 387 387 task670 10 100 200 1000 4639
task1533 10 100 200 1000 6052 task671 10 100 200 1000 4639
task1534 10 100 200 1000 6051 task677 10 100 200 1000 4400
task1540 10 100 200 1000 2895 task738 10 100 200 1000 6291
task1554 10 100 200 1000 6385 task743 10 100 200 540 540
task1557 10 100 200 644 644 task760 10 26 26 26 26
task1562 10 32 32 32 32 task769 10 100 200 890 890
task1586 10 100 200 1000 4890 task827 10 100 200 886 886
task1598 10 100 200 1000 6384 task828 10 100 200 886 886
task1612 10 100 200 1000 1690 task879 10 100 200 1000 2195
task1615 10 100 200 1000 1687 task880 10 100 200 1000 1231
task1622 10 100 200 1000 1885 task890 10 88 88 88 88
task1624 10 100 200 1000 1390 task891 10 87 87 87 87
task1631 10 100 200 1000 2880 task892 10 83 83 83 83
task1640 10 100 200 1000 2538 task893 10 100 100 100 100
task1659 10 100 200 1000 6389 task935 10 100 200 1000 6390
task1664 10 100 200 278 278 task936 10 100 200 1000 6390
task1728 10 100 200 1000 6385 task937 10 100 200 1000 6389
task190 10 100 200 1000 6390 task957 10 100 200 1000 2051
task199 10 100 200 1000 6389 task970 10 100 200 1000 2247
task200 10 100 200 1000 4354
Table 7: Training sample statistics that showcase a number of samples chosen when training with a
certain percentage of the data. There are multiple names in a single column highlighting that all of
those setups/baselines used that number of samples.
18

--- PAGE 19 ---
Task
NumberBaseline 1 Baseline 2 Baseline 3Task
NumberBaseline 1 Baseline 2 Baseline 3
task020 0.50 0.50 0.50 task201 0.56 0.88 0.93
task033 0.69 0.73 0.81 task202 0.91 0.97 0.96
task034 0.94 0.94 0.95 task219 0.34 0.37 0.36
task035 0.92 0.91 0.92 task220 1.00 1.00 1.00
task036 0.33 0.38 0.38 task226 0.56 0.59 0.59
task039 0.65 0.67 0.76 task232 0.50 0.83 0.90
task050 0.82 0.80 0.82 task233 0.50 0.60 0.70
task102 0.65 0.67 0.68 task242 0.99 0.99 0.99
task1152 0.80 0.02 0.02 task249 0.64 0.53 0.53
task1153 0.95 1.00 1.00 task281 0.63 0.69 0.68
task1154 1.00 1.00 1.00 task288 0.34 0.36 0.35
task1155 1.00 1.00 1.00 task290 0.84 0.94 0.91
task1156 1.00 1.00 1.00 task304 0.38 0.77 0.79
task1157 0.99 1.00 1.00 task329 0.73 0.79 0.90
task1158 1.00 1.00 1.00 task330 0.85 0.89 0.91
task1159 0.99 1.00 1.00 task349 0.76 0.81 0.92
task1161 0.38 0.38 0.41 task362 0.92 0.94 0.95
task1195 0.92 0.97 0.98 task391 0.86 0.94 0.93
task121 0.22 0.50 0.50 task392 0.86 0.93 0.93
task133 0.72 0.76 0.83 task393 0.31 0.21 0.21
task1342 0.18 0.17 0.19 task401 0.85 0.84 0.92
task1344 0.87 0.90 0.93 task402 0.56 0.57 0.54
task1345 0.40 0.39 0.40 task418 0.30 0.34 0.34
task1356 0.31 0.31 0.33 task442 0.66 0.67 0.68
task1358 0.35 0.38 0.39 task500 0.41 0.41 0.46
task1385 0.45 0.73 0.73 task510 0.49 0.49 0.52
task1386 0.35 0.54 0.54 task520 1.00 1.00 1.00
task1387 0.34 0.48 0.52 task569 0.46 0.45 0.47
task1388 0.78 0.67 0.67 task602 0.31 0.39 0.39
task1390 0.49 0.50 0.50 task613 0.18 0.26 0.24
task1391 0.77 0.50 0.92 task614 0.61 0.60 0.64
task1393 0.85 0.77 0.77 task619 0.45 0.44 0.47
task1394 0.54 0.46 0.46 task620 0.01 0.05 0.04
task1407 0.43 0.45 0.48 task623 0.98 1.00 1.00
task1409 0.51 0.52 0.55 task640 0.89 0.86 0.86
task1439 0.75 0.76 0.71 task641 0.36 0.37 0.37
task1442 0.62 0.65 0.58 task642 0.50 0.50 0.50
task1516 1.00 1.00 1.00 task645 0.96 0.97 0.97
task1529 0.89 0.96 0.97 task648 0.67 0.69 0.69
task1531 0.60 0.58 0.58 task670 0.78 0.75 0.75
task1533 0.50 0.64 0.79 task671 0.65 0.63 0.63
task1534 0.51 0.97 0.99 task677 0.33 0.31 0.34
task1540 0.39 0.42 0.43 task738 0.87 0.91 0.98
task1554 0.89 0.98 0.98 task743 0.52 0.54 0.54
task1557 0.88 0.87 0.87 task760 0.02 0.09 0.09
task1562 0.52 0.51 0.51 task769 0.95 0.96 0.96
task1586 0.36 0.39 0.40 task827 0.87 0.91 0.91
task1598 0.49 0.47 0.51 task828 0.89 0.95 0.95
task1612 0.82 0.91 0.91 task879 0.50 0.88 1.00
task1615 0.92 0.97 0.97 task880 0.51 0.99 0.97
task1622 0.93 0.96 0.96 task890 0.53 0.33 0.33
task1624 0.53 0.91 0.94 task891 0.72 0.73 0.73
task1631 0.99 0.99 0.99 task892 0.39 0.30 0.30
task1640 0.78 0.86 0.89 task893 1.00 0.67 0.67
task1659 0.49 0.62 0.71 task935 0.64 0.80 0.84
task1664 0.90 0.90 0.90 task936 0.72 0.81 0.84
task1728 0.57 0.56 0.62 task937 0.75 0.83 0.89
task190 0.77 0.64 0.85 task957 0.50 0.52 0.51
task199 0.52 0.87 0.92 task970 0.75 0.81 0.84
task200 0.96 0.95 0.97
Table 8: 1 model Setup. Scores of baselines for each task.
19

--- PAGE 20 ---
Task
NumberScore
10Score
100Score
200Score
1000Score
AllTask
NumberScore
10Score
100Score
200Score
1000Score
All
task020 0.52 0.52 0.50 0.50 0.50 task201 0.34 0.34 0.33 0.73 0.88
task033 0.67 0.67 0.66 0.74 0.79 task202 0.94 0.94 0.93 0.97 0.96
task034 0.94 0.94 0.94 0.94 0.95 task219 0.34 0.34 0.36 0.33 0.34
task035 0.93 0.93 0.93 0.92 0.92 task220 0.98 0.98 1.00 1.00 1.00
task036 0.31 0.31 0.32 0.37 0.34 task226 0.50 0.50 0.50 0.50 0.50
task039 0.74 0.74 0.65 0.61 0.81 task232 0.69 0.69 0.77 0.88 0.87
task050 0.73 0.73 0.79 0.85 0.83 task233 0.50 0.50 0.50 0.53 0.50
task102 0.65 0.65 0.66 0.68 0.69 task242 0.98 0.98 0.99 0.99 0.63
task1152 1.00 1.00 0.98 0.98 1.00 task249 0.71 0.71 0.65 0.63 0.71
task1153 0.99 0.99 0.98 1.00 1.00 task281 0.65 0.65 0.62 0.67 0.69
task1154 1.00 1.00 0.99 1.00 1.00 task288 0.30 0.30 0.33 0.33 0.33
task1155 1.00 1.00 1.00 1.00 1.00 task290 0.91 0.91 0.92 0.93 0.93
task1156 1.00 1.00 1.00 1.00 1.00 task304 0.53 0.53 0.45 0.67 0.85
task1157 0.99 0.99 1.00 1.00 1.00 task329 0.76 0.76 0.78 0.85 0.85
task1158 1.00 1.00 1.00 1.00 1.00 task330 0.86 0.86 0.87 0.89 0.89
task1159 1.00 1.00 1.00 1.00 1.00 task349 0.70 0.70 0.73 0.81 0.93
task1161 0.34 0.34 0.36 0.37 0.38 task362 0.92 0.92 0.93 0.96 0.95
task1195 0.96 0.96 0.97 0.97 0.98 task391 0.90 0.90 0.93 0.92 0.92
task121 0.50 0.50 0.47 0.47 0.50 task392 0.88 0.88 0.91 0.94 0.95
task133 0.76 0.76 0.76 0.84 0.85 task393 0.47 0.47 0.52 0.52 0.52
task1342 0.13 0.13 0.12 0.14 0.20 task401 0.82 0.82 0.83 0.85 0.92
task1344 0.82 0.82 0.82 0.90 0.91 task402 0.48 0.48 0.55 0.55 0.57
task1345 0.39 0.39 0.39 0.39 0.40 task418 0.29 0.29 0.29 0.31 0.33
task1356 0.04 0.04 0.03 0.01 0.03 task442 0.63 0.63 0.63 0.66 0.63
task1358 0.36 0.36 0.36 0.36 0.38 task500 0.36 0.36 0.39 0.37 0.39
task1385 0.34 0.34 0.39 0.66 0.62 task510 0.46 0.46 0.47 0.47 0.51
task1386 0.31 0.31 0.42 0.48 0.53 task520 1.00 1.00 1.00 1.00 1.00
task1387 0.33 0.33 0.35 0.44 0.46 task569 0.43 0.43 0.46 0.44 0.46
task1388 0.74 0.74 0.80 0.80 0.80 task602 0.06 0.06 0.41 0.41 0.41
task1390 0.48 0.48 0.54 0.50 0.56 task613 0.23 0.23 0.21 0.29 0.40
task1391 0.76 0.76 0.83 0.85 0.94 task614 0.62 0.62 0.65 0.63 0.66
task1393 0.94 0.94 0.87 0.92 0.92 task619 0.44 0.44 0.45 0.45 0.42
task1394 0.84 0.84 0.87 0.87 0.88 task620 0.02 0.02 0.03 0.06 0.10
task1407 0.41 0.41 0.43 0.43 0.48 task623 0.50 0.50 0.96 0.98 0.96
task1409 0.47 0.47 0.49 0.52 0.53 task640 0.90 0.90 0.85 0.85 0.85
task1439 0.58 0.58 0.60 0.49 0.63 task641 0.79 0.79 0.63 0.63 0.81
task1442 0.66 0.66 0.60 0.66 0.68 task642 0.50 0.50 0.50 0.50 0.50
task1516 0.99 0.99 1.00 0.98 1.00 task645 0.97 0.97 0.96 0.97 0.98
task1529 0.84 0.84 0.89 0.92 0.96 task648 0.76 0.76 0.77 0.77 0.71
task1531 0.58 0.58 0.67 0.50 0.65 task670 0.77 0.77 0.76 0.74 0.73
task1533 0.50 0.50 0.50 0.77 0.72 task671 0.64 0.64 0.64 0.62 0.61
task1534 0.54 0.54 0.89 0.97 0.50 task677 0.30 0.30 0.33 0.34 0.33
task1540 0.37 0.37 0.41 0.40 0.40 task738 0.89 0.89 0.90 0.90 0.96
task1554 0.83 0.83 0.93 0.98 0.95 task743 0.47 0.47 0.48 0.55 0.54
task1557 0.88 0.88 0.88 0.87 0.88 task760 0.08 0.08 0.68 0.68 0.68
task1562 0.52 0.52 0.53 0.53 0.51 task769 0.96 0.96 0.96 0.97 0.96
task1586 0.29 0.29 0.29 0.28 0.27 task827 0.87 0.87 0.90 0.91 0.92
task1598 0.49 0.49 0.49 0.49 0.52 task828 0.90 0.90 0.94 0.98 0.96
task1612 0.88 0.88 0.85 0.86 0.91 task879 0.50 0.50 0.50 0.96 1.00
task1615 0.96 0.96 0.93 0.96 0.97 task880 0.33 0.33 0.63 0.99 0.99
task1622 0.95 0.95 0.93 0.96 0.96 task890 0.65 0.65 0.68 0.68 0.63
task1624 0.65 0.65 0.75 0.79 0.83 task891 0.85 0.85 0.82 0.82 0.85
task1631 0.99 0.99 0.99 0.99 0.98 task892 0.69 0.69 0.48 0.48 0.69
task1640 0.74 0.74 0.81 0.84 0.84 task893 1.00 1.00 1.00 1.00 1.00
task1659 0.50 0.50 0.48 0.53 0.67 task935 0.72 0.72 0.85 0.89 0.86
task1664 0.95 0.95 0.96 0.97 0.98 task936 0.83 0.83 0.83 0.83 0.84
task1728 0.54 0.54 0.54 0.58 0.62 task937 0.75 0.75 0.77 0.86 0.85
task190 0.75 0.75 0.86 0.86 0.87 task957 0.50 0.50 0.50 0.50 0.49
task199 0.60 0.60 0.56 0.84 0.73 task970 0.74 0.74 0.75 0.80 0.88
task200 0.95 0.95 0.94 0.94 0.95
Table 9: 1 model Setup. Scores when 1 model is used for 1 task using x samples.
20

--- PAGE 21 ---
Task
NumberMLT Baseline 1 MLT Baseline 2Task
NumberMLT Baseline 1 MLT Baseline 2
task020 0.60 0.56 task201 0.33 0.29
task033 0.79 0.75 task202 0.40 0.37
task034 0.99 0.95 task219 0.38 0.53
task035 0.97 0.93 task220 0.98 0.98
task036 0.38 0.35 task226 0.56 0.53
task039 0.76 0.72 task232 0.70 0.68
task050 0.85 0.81 task233 0.59 0.56
task102 0.60 0.56 task242 1.00 0.99
task1152 1.00 1.00 task249 0.70 0.65
task1153 0.96 0.92 task281 0.70 0.68
task1154 1.00 1.00 task288 0.39 0.35
task1155 1.00 1.00 task290 0.90 0.88
task1156 0.99 0.98 task304 0.59 0.55
task1157 1.00 1.00 task329 0.90 0.85
task1158 1.00 1.00 task330 0.93 0.92
task1159 1.00 1.00 task349 0.82 0.80
task1161 0.28 0.25 task362 0.94 0.92
task1195 0.99 0.95 task391 0.96 0.91
task121 0.46 0.42 task392 0.95 0.93
task133 0.79 0.76 task393 0.57 0.52
task1342 0.57 0.53 task401 0.88 0.82
task1344 0.76 0.75 task402 0.54 0.51
task1345 0.34 0.32 task418 0.27 0.25
task1356 0.40 0.56 task442 0.63 0.61
task1358 0.34 0.31 task500 0.41 0.38
task1385 0.59 0.56 task510 0.47 0.45
task1386 0.64 0.60 task520 1.00 0.96
task1387 0.55 0.51 task569 0.45 0.43
task1388 0.85 0.80 task602 0.57 0.53
task1390 0.62 0.57 task613 0.28 0.47
task1391 0.87 0.84 task614 0.62 0.59
task1393 0.97 0.94 task619 0.07 0.05
task1394 0.70 0.67 task620 0.19 0.06
task1407 0.51 0.50 task623 1.00 1.00
task1409 0.50 0.44 task640 0.37 0.34
task1439 0.80 0.76 task641 0.31 0.27
task1442 0.77 0.74 task642 0.52 0.47
task1516 1.00 1.00 task645 0.98 0.95
task1529 0.94 0.90 task648 0.70 0.68
task1531 0.61 0.59 task670 0.66 0.61
task1533 0.57 0.54 task671 0.57 0.55
task1534 0.73 0.71 task677 0.31 0.29
task1540 0.34 0.31 task738 0.94 0.91
task1554 0.83 0.78 task743 0.54 0.52
task1557 0.68 0.64 task760 0.58 0.55
task1562 0.48 0.44 task769 1.00 0.96
task1586 0.26 0.24 task827 0.95 0.91
task1598 0.62 0.57 task828 0.98 0.93
task1612 0.91 0.86 task879 0.53 0.49
task1615 0.06 0.02 task880 0.32 0.28
task1622 1.00 0.96 task890 0.69 0.65
task1624 0.80 0.77 task891 0.88 0.87
task1631 1.00 0.99 task892 0.48 0.45
task1640 0.79 0.75 task893 1.00 0.99
task1659 0.54 0.51 task935 0.81 0.78
task1664 0.95 0.90 task936 0.78 0.75
task1728 0.63 0.59 task937 0.80 0.77
task190 0.76 0.72 task957 0.56 0.54
task199 0.70 0.67 task970 0.82 0.77
task200 0.41 0.37
Table 10: MLT Setup. Scores of baselines for each task. The baselines were trained in a combined
fashion.
21

--- PAGE 22 ---
Task
NumberScore
10Score
100Score
200Score
1000Score
AllTask
NumberScore
10Score
100Score
200Score
1000Score
All
task020 0.54 0.57 0.60 0.53 0.53 task201 0.13 0.39 0.58 0.87 0.88
task033 0.64 0.71 0.69 0.73 0.76 task202 0.83 0.90 0.94 0.98 1.00
task034 0.93 0.95 0.95 0.96 0.97 task219 0.20 0.27 0.28 0.27 0.30
task035 0.91 0.93 0.93 0.93 0.95 task220 0.98 0.98 0.99 0.99 1.00
task036 0.20 0.23 0.28 0.31 0.33 task226 0.51 0.51 0.51 0.51 0.51
task039 0.38 0.59 0.68 0.72 0.73 task232 0.51 0.51 0.77 0.75 0.76
task050 0.73 0.80 0.78 0.81 0.83 task233 0.51 0.51 0.57 0.53 0.53
task102 0.50 0.56 0.55 0.58 0.60 task242 0.98 0.99 0.99 0.99 1.00
task1152 0.86 1.00 1.00 1.00 1.00 task249 0.63 0.79 0.81 0.77 0.78
task1153 0.59 0.96 1.00 1.00 1.00 task281 0.55 0.64 0.65 0.71 0.74
task1154 0.61 0.87 0.99 1.00 1.00 task288 0.30 0.32 0.33 0.34 0.37
task1155 0.93 1.00 1.00 1.00 1.00 task290 0.84 0.90 0.92 0.94 0.95
task1156 0.67 1.00 1.00 1.00 1.00 task304 0.21 0.77 0.83 0.86 0.88
task1157 0.53 0.98 1.00 1.00 1.00 task329 0.51 0.64 0.77 0.79 0.82
task1158 0.78 1.00 1.00 0.95 0.96 task330 0.80 0.90 0.93 0.91 0.94
task1159 0.42 0.93 1.00 1.00 1.00 task349 0.54 0.72 0.76 0.82 0.85
task1161 0.25 0.27 0.27 0.26 0.28 task362 0.83 0.91 0.91 0.94 0.96
task1195 0.88 0.94 0.96 0.97 0.98 task391 0.89 0.88 0.92 0.94 0.95
task121 0.46 0.46 0.48 0.50 0.51 task392 0.90 0.90 0.93 0.95 0.97
task133 0.69 0.65 0.77 0.80 0.81 task393 0.24 0.32 0.54 0.39 0.41
task1342 0.05 0.07 0.10 0.12 0.62 task401 0.63 0.80 0.84 0.88 0.92
task1344 0.74 0.78 0.82 0.91 0.92 task402 0.49 0.40 0.53 0.52 0.55
task1345 0.36 0.37 0.37 0.36 0.37 task418 0.22 0.25 0.24 0.26 0.28
task1356 0.02 0.04 0.05 0.04 0.27 task442 0.57 0.61 0.57 0.61 0.62
task1358 0.29 0.29 0.31 0.31 0.32 task500 0.33 0.38 0.37 0.38 0.42
task1385 0.34 0.49 0.56 0.65 0.68 task510 0.42 0.44 0.44 0.45 0.45
task1386 0.43 0.48 0.54 0.58 0.61 task520 0.99 1.00 0.99 1.00 1.00
task1387 0.48 0.38 0.46 0.51 0.52 task569 0.45 0.43 0.38 0.44 0.45
task1388 0.71 0.80 0.79 0.77 0.81 task602 0.03 0.08 0.04 0.02 0.36
task1390 0.51 0.58 0.75 0.80 0.83 task613 0.15 0.17 0.17 0.22 0.23
task1391 0.76 0.83 0.82 0.90 0.93 task614 0.56 0.59 0.61 0.62 0.64
task1393 0.85 0.88 0.95 0.98 1.00 task619 0.34 0.37 0.37 0.40 0.43
task1394 0.69 0.84 0.89 0.86 0.89 task620 0.09 0.05 0.06 0.12 0.15
task1407 0.41 0.48 0.51 0.51 0.51 task623 0.52 0.55 0.97 0.99 1.00
task1409 0.46 0.51 0.53 0.56 0.58 task640 0.74 0.86 0.93 0.93 0.94
task1439 0.58 0.59 0.66 0.63 0.64 task641 0.64 0.80 0.83 0.81 0.83
task1442 0.60 0.67 0.73 0.72 0.74 task642 0.51 0.50 0.83 0.85 0.86
task1516 0.84 0.98 1.00 1.00 1.00 task645 0.95 0.96 0.96 0.98 1.00
task1529 0.84 0.91 0.94 0.91 0.94 task648 0.50 0.75 0.78 0.86 0.87
task1531 0.53 0.61 0.56 0.55 0.57 task670 0.63 0.64 0.63 0.61 0.61
task1533 0.61 0.76 0.74 0.79 0.82 task671 0.54 0.54 0.52 0.51 0.55
task1534 0.62 0.73 0.81 0.98 1.00 task677 0.24 0.26 0.29 0.29 0.29
task1540 0.32 0.34 0.29 0.33 0.36 task738 0.82 0.90 0.93 0.92 0.94
task1554 0.83 0.84 0.87 0.91 0.94 task743 0.45 0.50 0.52 0.55 0.58
task1557 0.76 0.78 0.77 0.78 0.81 task760 0.03 0.04 0.11 0.12 0.65
task1562 0.52 0.53 0.51 0.50 0.52 task769 0.93 0.95 0.97 0.97 1.00
task1586 0.22 0.23 0.21 0.22 0.23 task827 0.85 0.86 0.92 0.93 0.95
task1598 0.49 0.55 0.57 0.58 0.58 task828 0.70 0.95 0.93 0.96 0.98
task1612 0.52 0.73 0.78 0.88 0.91 task879 0.68 0.69 0.76 0.98 0.99
task1615 0.54 0.82 0.81 0.89 0.91 task880 0.27 0.23 0.37 0.95 0.96
task1622 0.88 0.92 0.95 0.97 0.97 task890 0.63 0.68 0.75 0.71 0.72
task1624 0.73 0.80 0.77 0.86 0.88 task891 0.76 0.84 0.84 0.84 0.84
task1631 0.99 0.99 0.99 0.99 1.00 task892 0.25 0.39 0.31 0.71 0.74
task1640 0.74 0.81 0.79 0.82 0.84 task893 0.75 0.98 1.00 0.95 0.96
task1659 0.39 0.44 0.49 0.53 0.55 task935 0.60 0.83 0.81 0.89 0.91
task1664 0.68 0.90 0.92 0.95 0.96 task936 0.76 0.84 0.81 0.89 0.93
task1728 0.52 0.56 0.58 0.61 0.64 task937 0.54 0.74 0.82 0.87 0.89
task190 0.55 0.73 0.80 0.85 0.88 task957 0.52 0.55 0.56 0.56 0.57
task199 0.51 0.51 0.80 0.85 0.87 task970 0.69 0.74 0.75 0.79 0.80
task200 0.92 0.94 0.94 0.94 0.97
Table 11: MLT Setup. Scores when 1 model is used for MLT using x samples.
22

--- PAGE 23 ---
Row Labels 10 100 200 1000 All STL Baseline 1 STL Baseline 2 STL Baseline 3
Answerability Classiﬁcation 0.670 0.705 0.728 0.752 0.744 0.704 0.788 0.804
Cause Effect Classiﬁcation 0.669 0.797 0.818 0.830 0.835 0.751 0.760 0.763
Coreference Resolution 0.614 0.757 0.743 0.776 0.828 0.700 0.686 0.744
Data to Text 0.460 0.493 0.567 0.578 0.591 0.497 0.509 0.528
Dialogue Act Recognition 0.629 0.600 0.711 0.858 0.813 0.584 0.778 0.819
Grammar Error Correction 0.871 0.878 0.878 0.871 0.882 0.883 0.873 0.873
Keyword Tagging 0.414 0.405 0.495 0.533 0.556 0.493 0.533 0.527
Overlap Extraction 0.426 0.691 0.632 0.638 0.748 0.637 0.680 0.722
Question Rewriting 0.670 0.700 0.704 0.706 0.706 0.681 0.710 0.711
Textual Entailment 0.608 0.724 0.740 0.803 0.820 0.710 0.778 0.804
Title Generation 0.364 0.395 0.425 0.430 0.446 0.447 0.468 0.484
Word Analogy 0.536 0.998 0.994 0.998 1.000 0.966 0.878 0.878
Table 12: Scores of all baselines and STL setups according to each category.
Row Labels 10 100 200 1000 All MTL Baseline 1 MTL Baseline 2
Answerability Classiﬁcation 0.676 0.722 0.757 0.762 0.762 0.769 0.753
Cause Effect Classiﬁcation 0.713 0.769 0.829 0.825 0.825 0.851 0.819
Coreference Resolution 0.616 0.753 0.791 0.840 0.840 0.819 0.759
Data to Text 0.516 0.557 0.578 0.588 0.533 0.547 0.558
Dialogue Act Recognition 0.602 0.681 0.720 0.865 0.865 0.759 0.600
Grammar Error Correction 0.763 0.779 0.772 0.778 0.778 0.678 0.640
Keyword Tagging 0.409 0.432 0.534 0.523 0.523 0.520 0.565
Overlap Extraction 0.467 0.613 0.664 0.711 0.711 0.698 0.696
Question Rewriting 0.654 0.665 0.672 0.676 0.676 0.679 0.659
Textual Entailment 0.630 0.733 0.797 0.842 0.842 0.672 0.622
Title Generation 0.404 0.433 0.440 0.439 0.382 0.422 0.452
Word Analogy 0.674 0.968 0.999 0.994 0.994 1.000 0.988
Table 13: Scores of all baselines and MTL setups according to each category.
Task Textual Entailment
Deﬁnition Deﬁnition: In this task, you’re given two sentences. Indicate if the ﬁrst
sentence clearly entails the second sentence (i.e., one can conclude the
2nd sentence by reading the 1st one)Indicate your answer with ’1’
if the ﬁrst sentence entails the second sentence, otherwise answer with ’0’
Example 1 Input: Sentence 1: No Weapons of Mass Destruction Found in Iraq Yet.
Sentence 2: Weapons of Mass Destruction Found in Iraq.
Output: 0
Example 2 Input: Sentence 1: A place of sorrow, after Pope John Paul II died, became
a place of celebration, as Roman Catholic faithful gathered in downtown
Chicago to mark the installation of new Pope Benedict XVI.
Sentence 2: Pope Benedict XVI is the new leader of the Roman Catholic Church.
Output: 1
Input Now complete the following example-
Input: Sentence 1: Since 1987, however, Brazil has taken steps to dramatically
reduce the destruction, including stepped-up enforcement and the elimination
of tax incentives that led to large-scale land clearing. Sentence 2: In the early
1990s Brazil began to take action to save the rainforest.
Output: 0
Table 14: Example of the category Textual Entailment by showcasing an example from Task 1344 of
SuperNI dataset.
23

--- PAGE 24 ---
Task Answerability Classiﬁcation
DeﬁnitionThe answer will be ’yes’ if the provided sentence contains an explicit mention
that answers the given question. Otherwise, the answer should be ’no’.
Instances where the answer is implied from the sentence using ""instinct""
or ""common sense"" (as opposed to being written explicitly in the sentence)
should be labeled as ’no’.
Example 1Input: Sentence: Jack played basketball for an hour after school, after which
he was very tired
Question: How long did Jack play basketball?
Output: Yes
Example 2Input: Sentence: He was born in China, so he went to the Embassy at 1 pm to
apply for a U.S. Visa.
Question: When did he go to Embassy?
Output: Yes
Input Now complete the following example-
Input: Sentence: The Vice President’s guidance was we need to take them out.
Question: Has he always wanted to take them out?
Output: No
Table 15: Example of the category Answerability Classiﬁcation by showcasing an example from Task
020 of SuperNI dataset.
Task Coreference Resolution
DeﬁnitionYou need to answer a given question containing a blank (_). Your answer must be one of the two
objects mentioned in the question, for example ""trophy"" and ""suitcase"". Your answer must
not contain a word that is not present in the question. Please don’t use articles (e.g., the, a)
before the answer.
Example 1 Input: The trophy doesn’t ﬁt into the brown suitcase because _ is too large.
Output: trophy
Example 2 Grace was happy to trade me her sweater for my jacket. She thinks _ looks dowdy on her.
Output: sweater
Input Now complete the following example-
Input: The goldﬁsh were ﬁnally removed from the bag and transferred into the tank, as
the _ was a temporary home for them.
Output: bag
Table 16: Example of the category Coreference Resolution by showcasing an example from Task 033
of SuperNI dataset.
24

--- PAGE 25 ---
Task Data to Text
DeﬁnitionIn this task, you are given concept set (with 3 to 5 concepts) that contain
mentions of names of people, places, activities, or things. These concept
sets reﬂect reasonable concept co-occurrences in everyday situations.
All concepts given as input are separated by ""#"". Your job is to generate
a sentence describing a day-to-day scene using all concepts from a
given concept set.
Example 1 Input: mountain#ski#skier
Output: Skier skis down the mountain
Example 2 Input: call#character#contain#wallpaper
Output: queen of wallpaper containing a portrait called ﬁlm character.
Input Now complete the following example-
Input: lake#shore#walk
Output: Men walk along the shore of the lake.
Table 17: Example of the category Data to text by showcasing an example from Task 102 of SuperNI
dataset.
Task Question Rewriting
DeﬁnitionGiven a disﬂuent sentence, modify the sentence to it to its equivalent ﬂuent
form, preserving the meaning of the sentence.
Example 1Input: Who did the Han Chinese want to help the Khitan no I mean the
Mongols ﬁght?
Output: Who did the Han Chinese want to help the Mongols ﬁght?
Example 2Input: What part did no I meant how many chapters have coordinating
lead authors?
Output: How many chapters have coordinating lead authors?
Input Now complete the following example-
Input: What year did a plague-ridden ship land in Norway?
Output: When did a plague-ridden ship land in Norway?
Table 18: Example of the category Question Rewriting by showcasing an example from Task 1195 of
SuperNI dataset.
25

--- PAGE 26 ---
Task Title Generation
DeﬁnitionIn this task, you’re given a paragraph from the research paper and your task is to generate a suitable title
for the research paper based on the given paper. Under 100 words is a good title length.
Example 1Input: The severe acute respiratory syndrome (SARS) epidemic originating from China in 2002 was
caused by a previously uncharacterized coronavirus that could be identiﬁed by speciﬁc RT-PCR
ampliﬁcation. Efforts to control future SARS outbreaks depend on the accurate and early identiﬁcation
of SARS-CoV infected patients. A real-time ﬂuorogenic RT-PCR assay based on the 3 -noncoding
region (3 -NCR) of SARS-CoV genome was developed as a quantitative SARS diagnostic tool.
The ideal ampliﬁcation efﬁciency of a sensitive SARS-CoV RT-PCR assay should yield an
E value (PCR product concentration increase per ampliﬁcation cycle) equal to 2.0. It was
demonstrated that the 3 -NCR SARS-CoV based RT-PCR reactions could be formulated to reach
excellent E values of 1.81, or 91% ampliﬁcation efﬁcacy. The SARS-CoV cDNA preparations
derived from viral RNA extract and the cloned recombinant plasmid both exhibit the identical
ampliﬁcation characteristics, i.e. ampliﬁcation efﬁcacy using the same PCR formulation developed
in this study. The viral genomic copy (or genomic equivalences, GE) per infectious unit (GE/pfu)
of SARS-CoV used in this study was also established to be approximate 1200-1600:1.
The assay’s detection sensitivity could reach 0.005 pfu or 6-8 GE per assay. It was preliminarily
demonstrated that the assay could efﬁciently detect SARS-CoV from clinical specimens of SARS
probable and suspected patients identiﬁed in Taiwan. The 3 -NCR based SARS-CoV assay demonstrated
100% diagnostic speciﬁcity testing samples of patients with acute respiratory disease from a
non-SARS epidemic region.
Output: NHS Wales: Court action if trade deals affect service?
Example 2Input: By Jon Welch and Paul MoseleyBBC News Details of health problems, family bereavements and
personal issues were sent by the University of East Anglia (UEA) in Norwich to 298 students. Megan
Baynes, 23, said she felt ""sick and horriﬁed"" when she realised her details had been shared. The UEA
apologised ""unreservedly"" and said an inquiry had begun. The email contained a spreadsheet listing
172 names and details extenuating circumstances in which extensions and other academic concessions
were granted to 42 students. ’Felt sick’ It was sent to nearly 300 undergraduates, including Ms Baynes,
a former editor of student newspaper Concrete. She is currently awaiting the results of her American
Literature and Creative Writing degree, and had been granted extensions for coursework because of an
illness suffered by a family member. ""I felt sick at seeing my personal situation written in a spreadsheet,
and then seemingly sent to everyone on my course,"" she said. ""My situation was not the worst on there
but there are some on there that are so personal. There are people I know and I feel so awful for them and
can’t imagine how they are feeling."" Theo Antoniou Phillips, UEA Students’ Union undergraduate education
ofﬁcer, said: ""This is a shocking and utterly unacceptable data breach that should never have happened.""
Jo Swo, the union’s welfare, community and diversity ofﬁcer, said: ""Given the university is supposed to be
making mental health a priority, this is a real slap in the face to students who have sought support.""
In a statement, a UEA spokeswoman said: ""An email was mistakenly sent to 298 American Studies
undergraduates this morning containing details of 42 students with extenuating circumstances. ""
This clearly should not have happened and the university apologises unreservedly. The university has launched
an urgent enquiry and is contacting all affected students to offer support. ""Anyone needing support should call
01603 592761. The university is informing the ICO (Information Commissioner’s Ofﬁce).""
The ICO has been contacted for comment.
Output: University of East Anglia in students’ personal data breach
Input Now complete the following example-
Input: President Donald Trump said Mr Mnuchin had spent his career making money in the private sector and
would now work for the taxpayer. Mr Mnuchin pledged to create jobs and combat terrorist ﬁnancing.
Democrats had argued that Mr Mnuchin had made a fortune foreclosing on families during the ﬁnancial crisis.
The top Democrat on the House Financial Services Committee, Maxine Waters of California, called Mr Mnuchin
""the foreclosure king"". His critics have also questioned whether he is too close to the Wall Street banking
community, which he will be responsible for regulating. Democrats also complained that Mr Mnuchin had failed
to disclose nearly $100m (£79m) in assets when he ﬁled with the Senate Finance Committee. Mr Mnuchin spent
17 years at Goldman Sachs before becoming a hedge fund manager. He later founded a ﬁlm production company
that was behind such box ofﬁce hits as the X-Men franchise and American Sniper. Mr Trump said Mr Mnuchin
would help make the US a ""jobs magnet"". ""He’ll work 24 hours a day, I know him. He’ll work 28 hours a day
if they give him the extra four hours,"" he said. Another former Goldman executive, Gary Cohn, is the director
of President Trump’s National Economic Council. What do we know about the new treasury secretary’s policy
plans? Mr Mnuchin hasn’t announced a fully ﬂedged plan, but his responses in media interviews and during the
Senate debate over his appointment make clear some of his priorities: There are still many policy areas that have
not been addressed, including how he will approach trading relations with China, Mexico and other partners.
Output: Trump says Mnuchin will ﬁght for tax cuts and jobs
Table 19: Example of the category Title Generation by showcasing an example from Task 1356 of
SuperNI dataset.
26
