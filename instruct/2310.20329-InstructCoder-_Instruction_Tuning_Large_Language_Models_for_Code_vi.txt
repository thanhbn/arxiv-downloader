# InstructCoder: Điều chỉnh Hướng dẫn cho Mô hình Ngôn ngữ Lớn trong Chỉnh sửa Mã

Kaixin Li1*Qisheng Hu1*Xu Zhao1Hui Chen2Yuxi Xie1Tiedong Liu1
Qizhe Xie1†Junxian He3†
1Đại học Quốc gia Singapore2Đại học Công nghệ và Thiết kế Singapore
3Đại học Giao thông Thượng Hải
{likaixin,qishenghu,xu.zhao,xieyuxi,tiedong.liu}@u.nus.edu ,
hui_chen@mymail.sutd.edu.sg ,
junxianh@sjtu.edu.cn

## Tóm tắt

Chỉnh sửa mã bao gồm nhiều tác vụ thực tiễn mà các nhà phát triển phải đối mặt hàng ngày. Mặc dù có tính liên quan và tính hữu ích thực tiễn, việc chỉnh sửa mã tự động vẫn là một lĩnh vực ít được khám phá trong sự phát triển của các mô hình deep learning, một phần do tình trạng khan hiếm dữ liệu. Trong công việc này, chúng tôi khám phá việc sử dụng Mô hình Ngôn ngữ Lớn (LLMs) để chỉnh sửa mã dựa trên hướng dẫn của người dùng. Được đánh giá trên một benchmark mới được viết bằng tay dựa trên thực thi có tên EditEval, chúng tôi nhận thấy các mô hình hiện tại thường gặp khó khăn trong việc thực hiện các hướng dẫn. Theo hướng này, chúng tôi đóng góp InstructCoder, bộ dữ liệu điều chỉnh hướng dẫn đầu tiên được thiết kế để thích ứng LLMs cho chỉnh sửa mã đa mục đích, chứa các tác vụ chỉnh sửa mã có tính đa dạng cao như chèn comment, tối ưu hóa mã và tái cấu trúc mã. Nó bao gồm hơn 114.000 bộ ba hướng dẫn-đầu vào-đầu ra và bao phủ nhiều kịch bản chỉnh sửa mã khác biệt. Quá trình thu thập bắt đầu với dữ liệu commit đã được lọc từ các repository Python trên GitHub làm hạt giống. Tiếp theo, bộ dữ liệu được mở rộng có hệ thống thông qua một quá trình lặp, nơi cả các tác vụ hạt giống và được tạo ra đều được sử dụng để prompt ChatGPT để tạo thêm dữ liệu. Các phát hiện của chúng tôi tiết lộ rằng các LLM mã nguồn mở được fine-tune trên InstructCoder có thể tăng cường đáng kể độ chính xác của việc chỉnh sửa mã, thể hiện hiệu suất chỉnh sửa mã vượt trội tương đương với các LLM độc quyền tiên tiến.

Bộ dữ liệu và mã nguồn có sẵn tại https://github.com/qishenghu/CodeInstruct.

## 1 Giới thiệu

Các nhà phát triển thường tham gia vào chu trình viết và chỉnh sửa mã. Là một yếu tố quan trọng, chỉnh sửa mã chiếm một phần lớn trong quá trình này, bao gồm các tác vụ phụ đa dạng như tối ưu hóa mã, tái cấu trúc và sửa lỗi, mỗi tác vụ đều đặt ra những thách thức riêng biệt. Các công cụ chỉnh sửa mã tự động có thể tăng cường đáng kể năng suất của nhà phát triển bằng cách giảm bớt gánh nặng của các tác vụ đơn điệu. Tuy nhiên, nó vẫn là một lĩnh vực ít được khám phá, một phần do thiếu dữ liệu liên quan, cản trở tiến bộ đáng kể của các mô hình deep learning.

Được truyền cảm hứng từ những tiến bộ gần đây trong LLMs (Brown et al., 2020; Chowdhery et al., 2022; Ouyang et al., 2022; OpenAI, 2022; Touvron et al., 2023a; OpenAI, 2023) và Code LLMs (Nijkamp et al., 2023a; Chen et al., 2021a; Li et al., 2023a), chúng tôi khám phá khả năng thành thạo của LLMs trong các tác vụ chỉnh sửa mã dựa trên hướng dẫn của người dùng, ví dụ, "thêm một docstring cho hàm để làm rõ", "loại bỏ mã dư thừa", hoặc "tái cấu trúc thành các hàm có thể tái sử dụng". Những tác vụ này khác biệt rõ ràng với việc hoàn thành mã, vốn liên quan đến việc tạo mã để hoàn thành các đoạn mã hoặc comment đã cho.

Chỉnh sửa mã yêu cầu mô hình không chỉ hiểu mã hiện có mà còn thực hiện các sửa đổi phù hợp với các hướng dẫn đã cho, đồng thời tích hợp một cách liền mạch với ngữ cảnh. Ví dụ, việc loại bỏ mã dư thừa hoặc tái cấu trúc một hàm không nên ảnh hưởng đến giá trị trả về.

Để đánh giá có hệ thống LLMs cho việc chỉnh sửa mã, chúng tôi đã tạo ra một benchmark mới có tên EditEval. Nó chứa nhiều loại chỉnh sửa mã được chuyển đổi từ các commit Github và các bộ dữ liệu hiện có. Thú vị là, chúng tôi nhận thấy các mô hình mã nguồn mở cho kết quả không thỏa đáng, và thậm chí các LLM độc quyền tiên tiến nhất cũng gặp khó khăn trong việc giải quyết những tác vụ này.

Để giải quyết thách thức này, chúng tôi trình bày InstructCoder, một bộ dữ liệu đa dạng cho việc finetuning hướng dẫn, được thiết kế đặc biệt để cải thiện khả năng chỉnh sửa mã của LLMs. Cụ thể, chúng tôi đầu tiên thu thập và kiểm tra thủ công dữ liệu commit từ các repository công khai trên GitHub làm các tác vụ chỉnh sửa mã hạt giống. Sau đó, chúng tôi sử dụng dữ liệu hạt giống để prompt ChatGPT (OpenAI, 2022) tạo ra các hướng dẫn mới và các cặp đầu vào-đầu ra tương ứng. Quá trình này giống với các framework Self-Instruct (Wang et al., 2022a) và Alpaca (Taori et al., 2023). Bằng cách sáng tạo buộc các kịch bản hướng dẫn quá trình tạo, phương pháp của chúng tôi đảm bảo rằng các tác vụ trong InstructCoder đa dạng và liên quan đến các tình huống lập trình thực tế, tạo ra một bộ dữ liệu mạnh mẽ cho finetuning hướng dẫn trong lĩnh vực chỉnh sửa mã. Sau khi khử trùng lặp và xử lý hậu kỳ thích hợp, chúng tôi giữ lại hơn 114.000 mẫu trong bộ dữ liệu.

Các nghiên cứu thực nghiệm của chúng tôi tiết lộ rằng LLMs thể hiện những tiến bộ đáng chú ý trong khả năng chỉnh sửa mã sau khi fine-tuning với InstructCoder. Code LLaMA đạt được kết quả tốt nhất thông qua fine-tuning, đạt độ chính xác 57.22%, gần tương đương với ChatGPT. Các nghiên cứu sâu hơn cũng chỉ ra rằng trong khi việc pre-training của các mô hình là cơ bản, hiệu suất chỉnh sửa mã bị ảnh hưởng mạnh mẽ bởi chất lượng và khối lượng của dữ liệu instruction-tuning.

Tóm lại, các đóng góp của công việc này là (1) InstructCoder, bộ dữ liệu instruction-tuning đầu tiên có nhiều tác vụ chỉnh sửa mã đa dạng, và chứng minh hiệu quả của instruction-finetuning với InstructCoder; (2) EditEval, một benchmark mới được viết bằng tay dựa trên thực thi để đánh giá nghiêm ngặt việc chỉnh sửa mã đa mục đích; (3) Chúng tôi nhận thấy rằng các mô hình mã nguồn mở được instruction-tuned với InstructCoder có thể thể hiện hiệu suất chỉnh sửa mã mạnh mẽ tương đương với ChatGPT.

## 2 Công việc Liên quan

### 2.1 Bộ dữ liệu Finetuning Hướng dẫn

Các nghiên cứu trước đây đã kết luận rằng việc instruction finetuning LLMs trên một tập hợp đa dạng các tác vụ hướng dẫn có thể cải thiện thêm khả năng của LLMs trong việc tổng quát hóa tốt trên các tác vụ chưa thấy (Ouyang et al., 2022; Mishra et al., 2022; Wei et al., 2022; Chung et al., 2022; Wang et al., 2023c). Để hỗ trợ những tác vụ này, các bộ dữ liệu bao gồm một số lượng lớn đoạn mã với các chú thích tương ứng là cần thiết. Những hướng dẫn này có thể được tái công thức hóa từ các bộ dữ liệu hiện có (Aribandi et al., 2022; Wei et al., 2022; Mishra et al., 2022; Longpre et al., 2023), hoặc được viết bằng tay với nỗ lực crowdsourcing (Ouyang et al., 2022; Wang et al., 2022b). Việc tạo dữ liệu hướng dẫn bằng máy móc cũng đã được khám phá để giảm lao động con người (Wang et al., 2022a; Honovich et al., 2022; Taori et al., 2023; Xue et al., 2023). Mặc dù có mức độ nhiễu cao trong dữ liệu, hiệu quả của nó đã được xác định.

### 2.2 Tổng hợp Mã

Tạo mã là một lĩnh vực được nghiên cứu rộng rãi. Các mô hình ngôn ngữ được pre-trained trên các tập hợp mã lớn đã thể hiện khả năng mạnh mẽ trong nhiều tác vụ lập trình khác nhau. Một số LLM tổng quát có được khả năng tạo mã do sự pha trộn của mã trong corpus pre-training (ví dụ The Pile (Gao et al., 2020)), như GPT-3 (Brown et al., 2020), ChatGPT, GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023a), BLOOM (Scao et al., 2022), GPT-NeoX (Black et al., 2022), và Pythia (Biderman et al., 2023). Các LLM được đào tạo đặc biệt trên mã và được tối ưu hóa cho việc tạo mã cũng được nghiên cứu, ví dụ Codex (Chen et al., 2021a), CodeGen (Nijkamp et al., 2023b), CodeGeeX (Zheng et al., 2023) và StarCoder (Li et al., 2023a). Tất cả các mô hình này đều áp dụng kiến trúc transformer chỉ decoder nhưng khác nhau về kích thước và thiết kế mô hình cụ thể (ví dụ positional embedding, vị trí norm layer) cũng như việc lựa chọn và tiền xử lý corpus pre-training.

Mặt khác, tương đối ít tài liệu giải quyết mục tiêu chỉnh sửa mã. Các công việc trước đây tập trung vào một tập con của các tác vụ chỉnh sửa mã, như code infilling (Fried et al., 2023) và debugging (Just et al., 2014; Tarlow et al., 2020; Ding et al., 2020; Jimenez et al., 2023). Bộ dữ liệu PIE (Madaan et al., 2023) là một công việc đồng thời liên quan nhất đến của chúng tôi, tập trung vào việc tăng tốc các chương trình. Các công việc khác (Yin et al., 2018; Wei et al., 2023; Chakraborty et al., 2020) không thể chấp nhận ngôn ngữ tự nhiên làm ý định chỉnh sửa, khiến chúng ít thân thiện với người dùng hơn.

Tuy nhiên, các bộ dữ liệu được thiết kế đặc biệt cho chỉnh sửa mã đa mục đích vẫn vắng mặt. Để lấp đầy khoảng trống này, chúng tôi giới thiệu InstructCoder, một bộ dữ liệu mới nhằm thúc đẩy thêm khả năng chỉnh sửa mã với LLMs.

## 3 EditEval: Đánh giá các Mô hình Chỉnh sửa Mã

Như đã đề cập trước đó, chỉnh sửa mã khác biệt đáng kể với hoàn thành mã. Do đó, các bộ dữ liệu được sử dụng rộng rãi trong lĩnh vực hoàn thành mã, như MBPP (Austin et al., 2021) và HumanEval (Chen et al., 2021b), không đủ để đánh giá khả năng chỉnh sửa mã. Để đánh giá nghiêm ngặt khả năng chỉnh sửa mã, chúng tôi đã tuyển chọn một tập test gồm 194 tác vụ chỉnh sửa mã, được lấy từ ba nguồn chính: dữ liệu commit GitHub, MBPP, và HumanEval. Chúng tôi sử dụng mã đầu vào từ những nguồn này và tạo ra các hướng dẫn chỉnh sửa hợp lý. Đối với các nguồn GitHub, chúng tôi tạo thủ công các ngữ cảnh thực thi để mã có thể chạy được. Mỗi mẫu được đi kèm với một giải pháp chuẩn được viết bởi con người để đảm bảo hướng dẫn khả thi. Các chỉnh sửa mã được tạo ra được đánh giá nghiêm ngặt bằng các test case tự động để đánh giá tính đúng đắn của các chỉnh sửa. Một chỉnh sửa được coi là đúng chỉ khi nó vượt qua tất cả các test case. Phương pháp tự động này cung cấp một framework đánh giá mạnh mẽ và khách quan, cần thiết để đo điểm hiệu suất của mô hình trong các tình huống chỉnh sửa mã đa dạng. Phụ lục A trình bày một ví dụ về tập test.

Chúng tôi đã đo điểm một số mô hình instruction-tuned trên EditEval, và kết quả được liệt kê trong Bảng 1. Nhìn chung, kết quả tiết lộ tiềm năng cải thiện đáng kể trong chỉnh sửa mã. Alpaca và CodeAlpaca thể hiện độ chính xác dưới 20% với kích thước 7B và 13B, và nó chỉ tốt hơn ở 33B. Ở kích thước này, CodeAlpaca vượt qua Alpaca, đạt độ chính xác 35.56%. Chuyển sang các GPT, các mô hình độc quyền tiên tiến nhất cho đến thời điểm này, GPT-4 đạt hiệu suất tốt nhất ở 68.56%. Thậm chí ChatGPT cũng gặp khó khăn trong tác vụ này, chỉ đạt 57.73%. Sau khi kiểm tra kỹ hơn, chúng tôi nhận thấy thách thức của EditEval nằm ở yêu cầu cao đối với cả việc theo dõi hướng dẫn và hiểu mã. Mô hình phải nắm bắt được ngữ cảnh ngụ ý của mã đầu vào, sau đó hoàn thành việc chỉnh sửa trong ngữ cảnh của nó.

## 4 InstructCoder: Instruction-tuning Trao quyền cho Chỉnh sửa Mã

Trong phần này, chúng tôi giới thiệu cách chúng tôi tạo ra InstructCoder để tăng cường khả năng chỉnh sửa mã của LLMs thông qua instruction finetuning. Chúng tôi sử dụng một phương pháp dựa trên Self-Instruct (Wang et al., 2022a), mở rộng dữ liệu instruction finetuning bằng cách bootstrapping từ việc tạo mô hình ngôn ngữ. Phương pháp tạo dữ liệu với LLMs yêu cầu dữ liệu được gán nhãn bởi con người tối thiểu làm tác vụ hạt giống trong khi duy trì chất lượng và tính liên quan của các tác vụ trong bộ dữ liệu. Thông qua một quá trình lặp tạo hướng dẫn và tinh chỉnh chúng với khử trùng lặp, chúng tôi tạo ra một bộ dữ liệu với nhiều tác vụ chỉnh sửa mã. Hình 1 minh họa quy trình thu thập dữ liệu của InstructCoder.

### 4.1 Thu thập Dữ liệu Hạt giống

GitHub là một nền tảng lưu trữ mã có dịch vụ kiểm soát phiên bản tự nhiên ghi lại các chỉnh sửa mã với commits, có thể được chuyển đổi thành hướng dẫn. Các repository trên GitHub cung cấp dữ liệu đa dạng với chất lượng được tạo bởi con người. Tuy nhiên, dữ liệu không phù hợp cho việc sử dụng trực tiếp. Đầu tiên, các thông điệp commit chủ yếu ngắn gọn và mang tính kết quả, thiếu mô tả chi tiết. Hơn nữa, chúng có thể không chính xác hoặc thậm chí vắng mặt. Thứ hai, commits có thể rất lớn liên quan đến nhiều file, vượt quá phạm vi của công việc này. Theo hướng này, chúng tôi hướng sự chú ý của mình vào LLMs như một phương tiện để tạo dữ liệu, thay vì sử dụng trực tiếp dữ liệu đã thu thập.

Dữ liệu commit GitHub thô được tổng hợp bằng BigQuery. Để đảm bảo chất lượng cao và giải quyết các vấn đề cấp phép, chúng tôi tập trung vào các repository Python trên GitHub với hơn 100 sao và giấy phép dễ dãi. Tiêu chí lựa chọn của chúng tôi bị hạn chế vào các commit chỉ sửa đổi một khối mã trong một file Python duy nhất. Những commit này được xác định bằng git-diff.

Trong quá trình thu thập, chúng tôi gặp phải nhiều thông điệp commit không chính xác hoặc mang tính cảm xúc. Codex (Chen et al., 2021a) được sử dụng trong những trường hợp như vậy để làm rõ các thay đổi được thực hiện giữa các phiên bản và cải thiện các thông điệp commit, tạo ra các hướng dẫn chính xác và thông tin hơn. Tổng cộng 634 tác vụ đã được xử lý từ dữ liệu commit thông qua nỗ lực thủ công và được sử dụng cho quá trình self-instruct.

Ngoài dữ liệu commit GitHub, chúng tôi cũng tận dụng các mẫu được tạo chất lượng cao làm tác vụ hạt giống bổ sung. Với kiểm tra thủ công, một loạt 592 mẫu chất lượng cao đã được biên soạn làm tác vụ hạt giống bổ sung. Tập dữ liệu hạt giống này bao phủ một loạt các kịch bản chỉnh sửa mã và làm phong phú cơ sở mà InstructCoder được tạo ra, đảm bảo rằng các tác vụ có gốc rễ trong các trường hợp chỉnh sửa mã thực tế hợp lý.

### 4.2 Bootstrapping Hướng dẫn

Self-Instruct (Wang et al., 2022a) là một framework tự động hiệu quả cho việc tạo dữ liệu hướng dẫn. Nó hoạt động bằng cách bootstrapping lặp từ việc tạo của LLM, trình bày một cách để làm phong phú bộ dữ liệu hướng dẫn trong khi duy trì chất lượng tác vụ và tính liên quan từ một tập nhỏ các tác vụ hạt giống được đánh giá bởi con người. Chúng tôi tận dụng một phương pháp tương tự để tạo ra dữ liệu hướng dẫn chỉnh sửa mã đa dạng.

Trong mỗi lần lặp, bảy hướng dẫn tác vụ hạt giống và một hướng dẫn tác vụ được tạo bởi ChatGPT được lấy mẫu và kết hợp như một ngữ cảnh few-shot để prompt ChatGPT cho thêm hướng dẫn. Để tạo ra các hướng dẫn đa dạng và có thể áp dụng thực tế hơn, chúng tôi cũng tạo ra các tác vụ trên nhiều sub-domain bằng cách chỉ định ý định chỉnh sửa trong prompt được cung cấp. Các prompt liên quan được sử dụng có thể được tìm thấy trong Bảng 4 ở Phụ lục C.

### 4.3 Tạo Có điều kiện Kịch bản

Chúng tôi ban đầu nhận thấy nhiều mẫu được tạo ra chia sẻ các codebase tương tự mặc dù có các hướng dẫn và ví dụ few-shot khác nhau được cung cấp. Sự tương tự như vậy có thể làm giảm đáng kể giá trị của bộ dữ liệu. Phân tích thực nghiệm cho thấy vấn đề có thể được quy cho việc LLM tạo ra các codebase chung cho các đoạn input/output khi không đủ ngữ cảnh được cung cấp. Như một biện pháp đối phó, chúng tôi đề xuất giới thiệu các kịch bản chỉnh sửa mã cho việc tạo mã input/output.

Chúng tôi trình bày một số ví dụ trong Hình 9, 10, 11 ở Phụ lục D, nơi chúng tôi thường quan sát thấy các thể hiện được tạo với kịch bản thể hiện chất lượng cao hơn về ngữ cảnh phong phú hơn và cấu trúc mã so với những thể hiện không có.

Đối với mỗi hướng dẫn được tạo, chúng tôi đầu tiên prompt ChatGPT để tạo ra các sự kiện thực tế như các kịch bản "thế giới thực" nơi hướng dẫn chỉnh sửa có thể được thực hiện, và ngẫu nhiên chọn một cho việc tạo thể hiện trong bước tiếp theo. Tiếp theo, LLM được hướng dẫn tạo ra các mẫu tương ứng với hướng dẫn và kịch bản, đảm bảo các codebase và tên biến phù hợp. Prompt được sử dụng có thể được tìm thấy trong Bảng 4 ở Phụ lục C.

Bằng cách kết hợp tạo có điều kiện kịch bản, các mẫu kết quả thể hiện tính biến đổi tăng lên về codebase và đặt tên biến, do đó tăng cường tính đa dạng của InstructCoder.

### 4.4 Xử lý Hậu kỳ

Theo Self-Instruct (Wang et al., 2022a), khử trùng lặp được áp dụng trên các hướng dẫn được tạo để loại bỏ các hướng dẫn có điểm overlap ROUGE-L (Lin, 2004) lớn hơn 0.7 với các hướng dẫn hiện có. Đối với mã, chúng tôi sử dụng MinHash với chỉ mục Locality Sensitive Hashing (LSH) để loại bỏ các thể hiện có độ tương tự Jaccard lớn hơn 0.75. Cuối cùng, InstructCoder bao gồm hơn 114.000 tác vụ chỉnh sửa mã khác biệt.

Cho mục đích thực nghiệm, chúng tôi chỉ định 95% tác vụ cho training, trong khi 5% còn lại tạo thành tập validation của chúng tôi.

## 5 Phân tích Dữ liệu

Chúng tôi phân tích InstructCoder về mặt 1) tính đa dạng, 2) độ phức tạp, và 3) tính đúng đắn. Chúng tôi cung cấp phân tích phân phối và độ phức tạp của các thể hiện tác vụ. Cuối cùng, chúng tôi chứng minh thông qua điều tra của con người rằng dữ liệu của chúng tôi có độ tin cậy cao.

### 5.1 Tổng quan Thống kê

InstructCoder bao gồm hơn 114k hướng dẫn chỉnh sửa mã, mỗi hướng dẫn được ghép nối với một thể hiện input/output. Phân phối độ dài token của input/output có thể được xem trong Hình 4 và Bảng 5 ở Phụ lục E. Hầu hết dữ liệu nằm trong một phạm vi hợp lý về độ dài, trong khi một số giá trị cực trị phản ánh tính rộng của bộ dữ liệu của chúng tôi.

### 5.2 Tính Đa dạng Hướng dẫn

Để khám phá tính đa dạng của các tác vụ trong InstructCoder và khả năng áp dụng thực tế của chúng, chúng tôi trình bày các ý định hướng dẫn khác nhau tức là những gì các chỉnh sửa mã dự định hoàn thành, và các động từ hướng dẫn, tức là cách thức chỉnh sửa mã được hoàn thành.

**Ý định Hướng dẫn.** Chúng tôi yêu cầu ChatGPT phân loại các loại chỉnh sửa mã trong bộ dữ liệu của chúng tôi và xác định thủ công 27 thể loại thực nghiệm. Hình 2 cho thấy phân phối của các danh mục ý định chỉnh sửa mã trong InstructCoder, bao gồm thêm chức năng, tối ưu hóa mã, cải thiện khả năng đọc, v.v. Những mục tiêu này nhấn mạnh phạm vi rộng lớn của InstructCoder.

**Động từ Hướng dẫn.** Tính đa dạng của các động từ hướng dẫn cũng được miêu tả trong Hình 3a. Chúng tôi thể hiện 20 động từ gốc hàng đầu và 4 danh từ trực tiếp hàng đầu của chúng đều được xếp hạng theo tần suất. Trong khi một phần lớn các hướng dẫn có thể được nhóm thô như tạo tác (ví dụ "add", "implement", "create") và sửa đổi (ví dụ "modify", "replace", "change"), InstructCoder trình bày một phân phối long-tail với các động từ ít phổ biến khác ngoài top-20 chiếm 25.0% tỷ lệ phần trăm. Điều này chứng minh rằng bộ dữ liệu chứa một phổ rộng các hướng dẫn.

### 5.3 Tính Đa dạng Kịch bản

InstructCoder được thiết kế để bao phủ một loạt các kịch bản. Như đã thảo luận trong Phần 4.3, mỗi hướng dẫn được đi kèm với các kịch bản khác nhau nơi hướng dẫn chỉnh sửa có thể được thực hiện để cải thiện tính đa dạng. Một đám mây từ được cung cấp để hiển thị một số domain kịch bản trong bộ dữ liệu của chúng tôi, như được minh họa trong Hình 3b, với mỗi sector tham chiếu đến một domain khác nhau. Tính đa dạng của bộ dữ liệu được nhấn mạnh bởi sự hiện diện của một loạt các domain như xử lý hình ảnh, phát triển web và an ninh mạng.

### 5.4 Độ Phức tạp

Chúng tôi phản ánh độ phức tạp của một tác vụ chỉnh sửa mã bằng số lượng dòng khác biệt và tỷ lệ chỉnh sửa của chúng trong cặp input/output, được định nghĩa như:

ndiff=|I∪O\I∩O|, (1)
rdiff=ndiff/|I∪O|, (2)

trong đó I và O là các tập hợp mã input/output với các dòng đơn lẻ như các phần tử.

Chúng tôi đo số lượng dòng khác biệt của một thể hiện tác vụ chỉnh sửa mã bằng thư viện Python difflib. Chúng tôi nhận thấy rằng số lượng trung bình các dòng khác biệt trong InstructCoder là 11.9 và tỷ lệ chỉnh sửa trung bình là 0.52. Những giá trị này cho thấy một mức độ phức tạp khá chấp nhận được, chỉ ra rằng bộ dữ liệu không quá dễ cũng không quá khó. InstructCoder đạt được sự cân bằng về mặt độ phức tạp, làm cho nó phù hợp cho finetuning và đánh giá LLMs trong một loạt các tác vụ chỉnh sửa mã. Hình 12 ở Phụ lục E minh họa phân phối số lượng dòng khác biệt.

### 5.5 Tính Đúng đắn

Chúng tôi tiếp tục lấy mẫu ngẫu nhiên 200 thể hiện và mời các chú thích viên đánh giá các thể hiện dựa trên hai tiêu chí: tính hợp lệ của các hướng dẫn và tính đúng đắn của các đầu ra. Đánh giá tính hợp lệ tập trung vào việc xác định xem các hướng dẫn có thể hiện ý định chỉnh sửa rõ ràng và phù hợp hay không. Đánh giá tính đúng đắn kiểm tra xem các cặp input-output có phản ánh các thay đổi được chỉ định bởi các hướng dẫn hay không.

Kết quả trong Bảng 2 chỉ ra rằng hầu hết các hướng dẫn trong bộ dữ liệu InstructCoder đều hợp lệ. Một vài thể hiện thể hiện nhiễu và thỉnh thoảng thất bại trong việc theo dõi các hướng dẫn, nhưng tính đúng đắn cao được tìm thấy tổng thể. Trong số 200 thể hiện được đánh giá, 180 đã được giải quyết thành công, thể hiện chất lượng tổng thể và độ tin cậy của InstructCoder.

## 6 Thực nghiệm

### 6.1 Thiết lập

**Training.** Chúng tôi thực nghiệm với hai họ mô hình ngôn ngữ mã nguồn mở với các kích thước khác nhau: LLaMA (LLaMA, LLaMA-2 và Code LLaMA) (Touvron et al., 2023a,b; Roziere et al., 2023) và BLOOM (Scao et al., 2022).

LLaMA là một series các LLM với tham số từ 7 đến 65 tỷ. Chúng đã được pre-trained trên một corpus rộng lớn, trong đó khoảng 4.5% bao gồm mã. Series LLaMA-2 mở rộng họ với pre-training chuyên sâu hơn. Ngoài ra, Code LLaMAs được xây dựng trên LLaMA-2 và được đào tạo đặc biệt trên 500B token mã để tăng cường khả năng hiểu và tạo mã của nó. BLOOM là một LLM đa ngôn ngữ có khả năng tạo ra đầu ra giống con người trong 46 ngôn ngữ và 13 ngôn ngữ lập trình.

Một finetuning đầy đủ cập nhật tất cả các tham số trong một LLM có thể tốn kém về mặt tính toán. Thay vào đó, chúng tôi áp dụng LoRA (Hu et al., 2022), một phương pháp finetuning hiệu quả tham số tối ưu hóa một ma trận delta low-rank gần đúng của các lớp fully-connected. Bằng cách này chúng tôi có thể fine-tune một mô hình 33B trong một card GPU A100-80GB duy nhất. Trong các thực nghiệm của chúng tôi, LoRA được áp dụng cho các trọng số biến đổi query, key, value và output của kiến trúc Transformer (Vaswani et al., 2017). Tất cả các siêu tham số có thể được tìm thấy trong Bảng 6 ở Phụ lục F.

**Baselines.** Chúng tôi chọn ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023) và GPT-4 Turbo làm baselines mạnh mẽ. Các mô hình mã nguồn mở đã đề cập cùng với một mô hình LLaMA instruction-tuned được gọi là Alpaca (Taori et al., 2023) được bao gồm, và hiệu suất zero-shot của chúng được báo cáo.

Đồng thời với công việc của chúng tôi, CodeAlpaca là một bộ dữ liệu phổ biến được tạo ra với pipeline của Alpaca, khác ở chỗ dữ liệu hạt giống của nó được thay thế bằng các hướng dẫn dễ viết tay với các chương trình ngắn. Chúng tôi fine-tune các mô hình LLaMA với CodeAlpaca và Alpaca và so sánh kết quả.

## 7 Kết quả

### 7.1 Hiệu quả Finetuning với InstructCoder

Trong phần này, chúng tôi chứng minh giá trị của bộ dữ liệu InstructCoder của chúng tôi. Bảng 3 trình bày so sánh chi tiết về hiệu suất EditEval trên các mô hình được fine-tuned với InstructCoder và các mô hình baseline. Trong khi độ chính xác rất thấp được quan sát trong các mô hình plain mã nguồn mở, finetuning với InstructCoder tăng cường đáng kể độ chính xác, làm nổi bật hiệu quả của instruction fine-tuning hiệu quả với các cặp chỉnh sửa mã được tạo bởi máy móc.

Code LLaMA 13B tương đương với hiệu suất của ChatGPT và vượt qua các mô hình mã nguồn mở khác với tỷ lệ chính xác 57.22%. Mô hình LLaMA-33B đáng kể hơn cho thấy cải thiện đáng chú ý 35.56%, tuy nhiên nó vẫn thua Code LLaMA-7B, mô hình này hưởng lợi từ pre-training rộng lớn trên mã.

Như mong đợi, nền tảng pre-training của LLM ảnh hưởng đáng kể đến hiệu quả chỉnh sửa mã. LLaMA thể hiện độ chính xác cao hơn các mô hình BLOOM có kích thước tương tự. Trong số các LLaMA, những mô hình được pre-trained trên nhiều token hơn (series LLaMA-2) vượt trội hơn các phiên bản trước đó. Hơn nữa, Code LLaMAs vượt qua các mô hình LLaMA-2 như kết quả của pre-training rộng lớn của chúng đặc biệt trên dữ liệu coding. Mặc dù có khả năng khác nhau của các mô hình nền tảng, bộ dữ liệu của chúng tôi liên tục tăng cường hiệu suất.

### 7.2 Mở rộng Bộ dữ liệu

InstructCoder có quy mô nhỏ hơn đáng kể so với những gì LLMs thường được pre-trained. Để xác định tính đầy đủ của quy mô này, chúng tôi tiến hành một thực nghiệm trong đó chúng tôi fine-tuned các mô hình LLaMA sử dụng các tỷ lệ khác nhau (1%, 10%, và 100%) của bộ dữ liệu. Các tập con nhỏ hơn được đảm bảo được bao gồm trong các tập con lớn hơn.

Kết quả được hiển thị trong Hình 5. Xu hướng được xác định chứng minh một mối tương quan tích cực giữa độ chính xác của mô hình và quy mô của tập training.

Được fine-tuned chỉ với 1% dữ liệu, các mô hình trải qua một số lượng hạn chế các cập nhật tham số nhưng nhanh chóng thích ứng với các tác vụ, vượt qua điểm số chính xác zero-shot tương ứng của chúng với biên độ đáng kể. Điều này nhấn mạnh tầm quan trọng của instruction tuning. Khi khối lượng dữ liệu training tăng lên, chúng tôi quan sát cải thiện nhất quán trong độ chính xác của mô hình, tăng trưởng gần như tuyến tính so với thang đo logarithmic của số lượng mẫu. Quan trọng là, thực nghiệm của chúng tôi gợi ý thực nghiệm rằng các mô hình lớn hơn hiệu quả hơn với ngân sách tính toán training bị hạn chế.

### 7.3 Tỷ lệ Chỉnh sửa

Hình 6 mô tả độ chính xác của các mô hình LLaMA được fine-tuned như được đánh giá bởi GPT-4 trên năm mức tỷ lệ chỉnh sửa, sử dụng 2000 mẫu ngẫu nhiên từ tập validation. Đánh giá này, được biện minh trong Phụ lục H, liên quan đến việc prompting GPT-4 cho một đánh giá nhanh và tổng quát về các chỉnh sửa mã, cung cấp một góc nhìn thay thế cho đánh giá chỉnh sửa mã. Trong đánh giá này, các mô hình lớn hơn liên tục vượt trội hơn các đối tác nhỏ hơn của chúng. Đáng chú ý, độ chính xác giảm với tỷ lệ chỉnh sửa thấp hơn, có thể do các mô hình áp dụng shortcut sao chép đầu vào để giảm thiểu loss trong các kịch bản yêu cầu ít chỉnh sửa hơn. Xu hướng này, tuy nhiên, ít rõ rệt hơn trong các mô hình lớn hơn, thể hiện khả năng lớn hơn trong việc phân biệt sự khác biệt tinh tế trong các trường hợp tỷ lệ chỉnh sửa thấp.

## 8 Kết luận

Chúng tôi giới thiệu InstructCoder, bộ dữ liệu instruction-tuning đầu tiên cho các tác vụ chỉnh sửa mã đa mục đích. Nó bao gồm các thế hệ của LLMs, nơi các commit GitHub thực tế phục vụ như các tác vụ hạt giống để hướng dẫn quá trình tạo. Một phương pháp có điều kiện kịch bản được giới thiệu để đảm bảo cả tính đa dạng và chất lượng cao của dữ liệu. Các thực nghiệm của chúng tôi trên benchmark EditEval mới cho thấy rằng các mô hình mã nguồn mở có thể có được cải thiện lớn và thậm chí mang lại hiệu suất tương đương với các mô hình độc quyền thông qua parameter-efficient fine-tuning nhẹ về mặt tính toán với InstructCoder. Chúng tôi cũng tiết lộ rằng mô hình cơ sở LLM và quy mô của dữ liệu fine-tuning đều là những yếu tố sâu sắc của khả năng chỉnh sửa mã. Chúng tôi hy vọng bộ dữ liệu có thể mang lại lợi ích và truyền cảm hứng cho nhiều nghiên cứu hơn trong lĩnh vực này hướng tới việc xây dựng các mô hình coding mạnh mẽ hơn.

## Hạn chế

Phương pháp của chúng tôi không bao gồm các thay đổi mã liên quan đến ngữ cảnh multi-file, có thể hữu ích trong phát triển. Chúng tôi hy vọng khám phá thêm những khía cạnh này và kết hợp thêm các ngôn ngữ lập trình trong nghiên cứu tương lai của chúng tôi.

## Tài liệu Tham khảo

[Bản dịch sẽ giữ nguyên danh sách tài liệu tham khảo vì các tên tác giả, tên tạp chí và tên hội nghị thường được giữ nguyên bằng tiếng Anh trong các bài báo khoa học tiếng Việt]
