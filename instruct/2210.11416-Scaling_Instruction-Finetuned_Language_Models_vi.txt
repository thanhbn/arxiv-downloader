# 2210.11416.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2210.11416.pdf
# Kích thước tệp: 1557309 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mở rộng quy mô các Mô hình Ngôn ngữ được Tinh chỉnh theo Hướng dẫn
Hyung Won Chung∗Le Hou∗Shayne Longpre∗Barret Zoph†Yi Tay†
William Fedus†Yunxuan Li Xuezhi Wang Mostafa Dehghani Siddhartha Brahma
Albert Webson Shixiang Shane Gu Zhuyun Dai Mirac Suzgun Xinyun Chen
Aakanksha Chowdhery Alex Castro-Ros Marie Pellat Kevin Robinson
Dasha Valter Sharan Narang Gaurav Mishra Adams Yu Vincent Zhao
Yanping Huang Andrew Dai Hongkun Yu Slav Petrov Ed H. Chi
Jeﬀ Dean Jacob Devlin Adam Roberts Denny Zhou Quoc V. Le
Jason Wei
Google

Tóm tắt
Việc tinh chỉnh các mô hình ngôn ngữ trên một tập hợp các bộ dữ liệu được diễn đạt dưới dạng hướng dẫn đã được chứng minh là cải thiện hiệu suất mô hình và khả năng tổng quát hóa cho các nhiệm vụ chưa thấy. Trong bài báo này, chúng tôi khám phá việc tinh chỉnh theo hướng dẫn với sự tập trung đặc biệt vào (1) mở rộng số lượng nhiệm vụ, (2) mở rộng kích thước mô hình, và (3) tinh chỉnh trên dữ liệu chuỗi suy nghĩ. Chúng tôi thấy rằng việc tinh chỉnh theo hướng dẫn với các khía cạnh trên cải thiện đáng kể hiệu suất trên nhiều loại mô hình khác nhau (PaLM, T5, U-PaLM), thiết lập gợi ý (zero-shot, few-shot, CoT), và các benchmark đánh giá (MMLU, BBH, TyDiQA, MGSM, sinh văn bản mở, RealToxicityPrompts). Ví dụ, Flan-PaLM 540B được tinh chỉnh theo hướng dẫn trên 1.8K nhiệm vụ vượt trội hơn PaLM 540B với biên độ lớn (+9.4% trung bình). Flan-PaLM 540B đạt hiệu suất tối ưu trên một số benchmark, chẳng hạn như 75.2% trên MMLU five-shot. Chúng tôi cũng công bố các checkpoint Flan-T5,¹ đạt hiệu suất few-shot mạnh mẽ ngay cả khi so sánh với các mô hình lớn hơn nhiều, như PaLM 62B. Nhìn chung, tinh chỉnh theo hướng dẫn là một phương pháp tổng quát để cải thiện hiệu suất và khả năng sử dụng của các mô hình ngôn ngữ đã được tiền huấn luyện.

[Hình minh họa về quá trình tinh chỉnh và suy luận]

∗Đóng góp ngang nhau. Liên hệ: lehou@google.com .
†Người đóng góp cốt lõi.
¹Checkpoint công khai: https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints .

--- TRANG 2 ---
1 Giới thiệu
Một mục tiêu quan trọng của trí tuệ nhân tạo là phát triển các mô hình có thể tổng quát hóa cho các nhiệm vụ chưa thấy. Trong xử lý ngôn ngữ tự nhiên (NLP), các mô hình ngôn ngữ đã được tiền huấn luyện đã đạt được tiến bộ đáng kể hướng tới mục tiêu này, khi chúng có thể thực hiện các nhiệm vụ dựa trên mô tả bằng ngôn ngữ tự nhiên (Brown et al., 2020, và các nghiên cứu khác). Tiến bộ tiếp theo đã được thực hiện bằng cách tinh chỉnh các mô hình ngôn ngữ trên một tập hợp các nhiệm vụ được diễn đạt dưới dạng hướng dẫn, cho phép các mô hình phản ứng tốt hơn với hướng dẫn và giảm nhu cầu về các ví dụ few-shot (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2021, và các nghiên cứu khác).

Trong bài báo này, chúng tôi phát triển việc tinh chỉnh theo hướng dẫn theo nhiều cách. Đầu tiên, chúng tôi nghiên cứu tác động của việc mở rộng quy mô đối với tinh chỉnh theo hướng dẫn. Các thí nghiệm của chúng tôi cho thấy rằng tinh chỉnh theo hướng dẫn thực sự mở rộng tốt với số lượng nhiệm vụ và kích thước của mô hình. Các hành vi mở rộng tương ứng của chúng gợi ý rằng nghiên cứu trong tương lai nên mở rộng số lượng nhiệm vụ và kích thước mô hình thậm chí còn xa hơn. Thứ hai, chúng tôi nghiên cứu tác động của việc tinh chỉnh đối với khả năng thực hiện các nhiệm vụ suy luận của mô hình. Các thí nghiệm của chúng tôi cho thấy rằng trong khi các phương pháp tinh chỉnh theo hướng dẫn trước đây không bao gồm chuỗi suy nghĩ (CoT; Wei et al., 2022b) làm giảm nghiêm trọng hiệu suất trên các đánh giá CoT, việc chỉ thêm chín bộ dữ liệu CoT vào hỗn hợp tinh chỉnh cho phép hiệu suất tốt hơn trên tất cả các đánh giá.

Dựa trên những phát hiện này, chúng tôi huấn luyện Flan-PaLM bằng cách sử dụng mô hình 540B tham số, tăng số lượng nhiệm vụ tinh chỉnh lên 1.8K, và bao gồm dữ liệu CoT. Flan-PaLM vượt trội hơn PaLM, đạt được trạng thái tối ưu mới trên một số benchmark. Ví dụ, khả năng suy luận được cải thiện của Flan-PaLM cho phép nó tận dụng CoT và tính nhất quán bản thân (Wang et al., 2022c) để đạt 75.2% trên Massive Multi-task Language Understanding (MMLU; Hendrycks et al., 2020). Flan-PaLM cũng có khả năng đa ngôn ngữ được cải thiện so với PaLM, chẳng hạn như cải thiện tuyệt đối 14.9% trên TyDiQA one-shot (Clark et al., 2020) và 8.1% trên suy luận số học trong các ngôn ngữ ít được đại diện (Shi et al., 2022). Trong đánh giá của người chấm điểm con người, Flan-PaLM vượt trội đáng kể so với PaLM trên một tập hợp các câu hỏi sinh văn bản mở đầy thách thức, gợi ý khả năng sử dụng được cải thiện. Hơn nữa, chúng tôi thấy rằng tinh chỉnh theo hướng dẫn cũng cải thiện hiệu suất trên một số benchmark đánh giá AI có trách nhiệm.

Ngoài ra, chúng tôi cũng tinh chỉnh theo hướng dẫn các mô hình Flan-T5 (80M đến 11B). Các checkpoint này có khả năng zero-shot, few-shot, và CoT mạnh mẽ, vượt trội hơn các checkpoint công khai trước đây như T5 (Raﬀel et al., 2020). Ví dụ, Flan-T5 11B vượt trội hơn T5 11B với những cải thiện hai chữ số và thậm chí vượt trội hơn PaLM 62B trên một số nhiệm vụ BIG-Bench đầy thách thức (Srivastava et al., 2022). Nhìn chung, kết quả của chúng tôi nhấn mạnh cách tinh chỉnh theo hướng dẫn có thể cải thiện hiệu suất trên một loạt các mô hình, thiết lập gợi ý, và nhiệm vụ đánh giá.

[Bảng 1 với kết quả MMLU]

--- TRANG 3 ---
[Hình 2: Dữ liệu tinh chỉnh và nhiệm vụ được giữ lại]

2 Tinh chỉnh Flan
Chúng tôi tinh chỉnh theo hướng dẫn trên một tập hợp các nguồn dữ liệu (Hình 2) với nhiều loại mẫu hướng dẫn khác nhau (Hình 3). Chúng tôi gọi quy trình tinh chỉnh này là Flan (Finetuning language models; Wei et al., 2021) và thêm tiền tố "Flan" vào các mô hình đã tinh chỉnh kết quả (ví dụ, Flan-PaLM).² Chúng tôi cho thấy rằng Flan hoạt động trên nhiều kích thước và kiến trúc mô hình khác nhau (Bảng 2).

2.1 Dữ liệu Tinh chỉnh
Hỗn hợp nhiệm vụ. Tài liệu trước đây đã cho thấy rằng việc tăng số lượng nhiệm vụ trong tinh chỉnh với hướng dẫn cải thiện khả năng tổng quát hóa cho các nhiệm vụ chưa thấy (Wei et al., 2021; Sanh et al., 2021, và các nghiên cứu khác). Trong bài báo này, chúng tôi mở rộng lên 1,836 nhiệm vụ tinh chỉnh bằng cách kết hợp bốn hỗn hợp từ các nghiên cứu trước: Muﬃn, T0-SF, NIV2, và CoT, như được tóm tắt trong Hình 2. Muﬃn³ (80 nhiệm vụ) bao gồm 62 nhiệm vụ từ Wei et al. (2021) và 26 nhiệm vụ mới mà chúng tôi đã thêm trong công việc này, bao gồm dữ liệu đối thoại (Byrne et al., 2019; Anantha et al., 2021; Dai et al., 2022) và dữ liệu tổng hợp chương trình (Yasunaga and Liang, 2020; Li et al., 2022). T0-SF (193 nhiệm vụ) bao gồm các nhiệm vụ từ T0 (Sanh et al., 2021) không trùng lặp với dữ liệu được sử dụng trong Muﬃn (SF viết tắt của "sans Flan"). NIV2 (1554 nhiệm vụ) bao gồm các nhiệm vụ từ Wang et al. (2022c).⁴

²Chúng tôi sử dụng "Flan" để chỉ quy trình tinh chỉnh của chúng tôi. "FLAN" là một mô hình trong Wei et al. (2021).
³Multi-task finetuning with instructions.
⁴Chúng tôi đã loại bỏ 44 nhiệm vụ liên quan đến MMLU (Hendrycks et al., 2020), vì MMLU được sử dụng để đánh giá.

--- TRANG 4 ---
[Hình 3: Các kết hợp định dạng dữ liệu tinh chỉnh]

Hỗn hợp tinh chỉnh chuỗi suy nghĩ. Hỗn hợp dữ liệu tinh chỉnh thứ tư (suy luận) liên quan đến các chú thích CoT, mà chúng tôi sử dụng để khám phá xem việc tinh chỉnh trên các chú thích CoT có cải thiện hiệu suất trên các nhiệm vụ suy luận chưa thấy hay không. Chúng tôi tạo ra một hỗn hợp mới gồm chín bộ dữ liệu từ nghiên cứu trước đây mà những người chấm điểm con người đã viết thủ công các chú thích CoT cho một corpus huấn luyện. Chín bộ dữ liệu này bao gồm các nhiệm vụ như suy luận số học (Cobbe et al., 2021), suy luận đa bước (Geva et al., 2021), và suy luận ngôn ngữ tự nhiên (Camburu et al., 2020). Chúng tôi soạn thủ công mười mẫu hướng dẫn cho mỗi nhiệm vụ. Một thẻ dữ liệu được trình bày trong Phụ lục F.

Mẫu và định dạng. Đối với Muﬃn, T0-SF, và NIV2, chúng tôi sử dụng các mẫu hướng dẫn cho mỗi nhiệm vụ như được cung cấp bởi những người tạo ra các hỗn hợp. Đối với CoT, chúng tôi viết thủ công khoảng mười mẫu hướng dẫn cho mỗi trong số chín bộ dữ liệu. Để tạo các mẫu few-shot, chúng tôi viết nhiều ký hiệu phân cách ví dụ khác nhau (ví dụ, "Q:"/"A:") và áp dụng chúng ngẫu nhiên ở cấp độ ví dụ. Một ví dụ về định dạng cho cả có và không có ví dụ, cũng như có và không có CoT, được hiển thị trong Hình 3.

2.2 Quy trình Tinh chỉnh
Trong bài báo này, chúng tôi áp dụng tinh chỉnh theo hướng dẫn trên một loạt rộng các họ mô hình, bao gồm T5 (Raﬀel et al., 2020), PaLM (Chowdhery et al., 2022), và U-PaLM (Tay et al., 2022b). Các họ mô hình này trải dài trên một loạt kích thước, từ Flan-T5-small (80M tham số), đến PaLM và U-PaLM (540B tham số). Đối với mỗi mô hình, chúng tôi áp dụng cùng một quy trình huấn luyện, ngoại trừ một vài siêu tham số: tốc độ học, kích thước batch, dropout, và số bước tinh chỉnh. Chúng tôi sử dụng lịch trình tốc độ học không đổi và tinh chỉnh bằng bộ tối ưu hóa Adafactor (Shazeer and Stern, 2018). Chúng tôi sử dụng packing (Raﬀel et al., 2020) để kết hợp nhiều ví dụ huấn luyện thành một chuỗi duy nhất, phân tách đầu vào khỏi mục tiêu bằng token kết thúc chuỗi. Masking được áp dụng để ngăn các token chú ý đến nhau qua ranh giới ví dụ được đóng gói. Số bước tinh chỉnh, tốc độ học, kích thước batch, và dropout cho mỗi mô hình được cung cấp trong Phụ lục E. Đối với mỗi mô hình, chúng tôi sử dụng một checkpoint duy nhất cho tất cả các đánh giá; bước tối ưu được chọn dựa trên các đánh giá định kỳ (mỗi 2k đến 10k bước tùy thuộc vào kích thước mô hình) của các nhiệm vụ được giữ lại, và chúng tôi sử dụng cùng số bước checkpoint cho tất cả các lần chạy ablation cho một mô hình nhất định. Đáng chú ý, lượng tính toán được sử dụng để tinh chỉnh chỉ là một phần nhỏ so với tính toán huấn luyện, như được thể hiện trong Bảng 2. Ví dụ, chúng tôi chỉ sử dụng 0.2% tính toán tiền huấn luyện để tinh chỉnh theo hướng dẫn Flan-PaLM 540B (khoảng 512 chip TPU v4 trong 37 giờ). Chúng tôi sử dụng framework T5X dựa trên JAX (Bradbury et al., 2018; Roberts et al., 2022).

--- TRANG 5 ---
[Bảng 2: Chi phí tính toán tinh chỉnh so với tiền huấn luyện]

2.3 Giao thức Đánh giá
Benchmark đánh giá. Chúng tôi tập trung vào hiệu suất trên các nhiệm vụ được giữ lại không được bao gồm như một phần của dữ liệu tinh chỉnh. Chúng tôi quan tâm đến khả năng tổng thể của Flan-PaLM trên các nhiệm vụ kiến thức thế giới và suy luận. Do đó, chúng tôi đánh giá mô hình trên một loạt các benchmark khác nhau, bao gồm cả đa ngôn ngữ. Chúng tôi không sử dụng tập đánh giá từ Brown et al. (2020) vì hầu như tất cả các nhiệm vụ đó đều có tập huấn luyện được bao gồm trong hỗn hợp tinh chỉnh của chúng tôi. Thay vào đó, chúng tôi sử dụng các benchmark đầy thách thức sau, mà các mô hình ngôn ngữ hiện tại vẫn hoạt động dưới mức của những người chấm điểm chuyên gia con người. (1) MMLU (Hendrycks et al., 2020) bao gồm các câu hỏi thi từ 57 nhiệm vụ như toán học, lịch sử, luật pháp, và y học. (2) BBH bao gồm 23 nhiệm vụ đầy thách thức từ BIG-Bench (Srivastava et al., 2022) mà PaLM hoạt động dưới mức một người chấm điểm con người trung bình (Suzgun et al., 2022). (3) TyDiQA (Clark et al., 2020) là một benchmark hỏi đáp trên 8 ngôn ngữ đa dạng về mặt loại hình học. (4) MGSM (Shi et al., 2022) là một benchmark đa ngôn ngữ về các bài toán từ toán học từ Cobbe et al. (2021) được dịch thủ công sang 10 ngôn ngữ. Các benchmark này cũng được sử dụng trong bài báo PaLM (Chowdhery et al., 2022), không tìm thấy bất kỳ nhiễm dữ liệu có ý nghĩa nào với dữ liệu tiền huấn luyện, phù hợp với các phân tích nhiễm dữ liệu trong nghiên cứu trước đây (Brown et al., 2020; Wei et al., 2021; Du et al., 2022). Các đánh giá AI có trách nhiệm được thảo luận trong Phụ lục C.

Phương pháp và chỉ số đánh giá. Đối với MMLU và BBH, chúng tôi đánh giá cả khả năng dự đoán trực tiếp câu trả lời thông qua gợi ý trực tiếp, nơi mô hình đưa ra câu trả lời trực tiếp (Brown et al., 2020; Srivastava et al., 2022), cũng như thông qua gợi ý chuỗi suy nghĩ (CoT), nơi mô hình phải cung cấp một chuỗi suy luận trước khi đưa ra câu trả lời cuối cùng (Wei et al., 2022b). Đối với TyDiQA, chúng tôi chỉ đo điểm khớp chính xác của gợi ý trực tiếp, vì việc làm nổi bật phần của đoạn văn có câu trả lời đúng có thể không yêu cầu suy luận phức tạp. Đối với MGSM, chúng tôi chỉ đo độ chính xác của gợi ý CoT vì gợi ý trực tiếp có hiệu suất rất thấp. Đối với tất cả các benchmark, chúng tôi sử dụng các ví dụ few-shot đã cho, với số lượng ví dụ theo nghiên cứu trước đây: five-shot cho MMLU, three-shot cho BBH, one-shot cho TyDiQA, và 8-shot cho MGSM. Đối với một mô hình nhất định, chúng tôi cũng báo cáo một chỉ số "trung bình chuẩn hóa" duy nhất, theo "chỉ số ưa thích chuẩn hóa" trong BIG-Bench (Srivastava et al., 2022).⁵ Chỉ số trung bình chuẩn hóa của chúng tôi là macro-average trên sáu điểm chuẩn hóa: MMLU-Direct, MMLU-CoT, BBH-Direct, BBH-CoT, TyDiQA-Direct, và MGSM-CoT. Kết quả cho tất cả các nhiệm vụ trong mỗi benchmark được báo cáo trong Phụ lục D. Một số benchmark AI có trách nhiệm sử dụng các phương pháp bổ sung cho các nhiệm vụ sinh văn bản được mô tả trong Phụ lục C.

⁵Một chỉ số chuẩn hóa chia tỷ lệ một số đánh giá theo ranh giới dưới cụ thể của nhiệm vụ như baseline đoán ngẫu nhiên cho một câu hỏi trắc nghiệm. Ví dụ, nếu đoán ngẫu nhiên tạo ra 50% độ chính xác và độ chính xác tối đa 100%, thì độ chính xác thô 55% sẽ được chuẩn hóa thành 10%, và độ chính xác thô 45% sẽ được chuẩn hóa thành -10% vì nó tệ hơn ngẫu nhiên.

--- TRANG 6 ---
3 Mở rộng lên 540B tham số và 1.8K nhiệm vụ
Trước tiên chúng tôi kiểm tra tác động của việc mở rộng về mặt (1) kích thước mô hình và (2) số lượng nhiệm vụ tinh chỉnh đối với hiệu suất trên các nhiệm vụ được giữ lại. Chúng tôi mở rộng kích thước mô hình bằng cách thực hiện thí nghiệm trên ba kích thước mô hình PaLM: 8B, 62B, và 540B. Để mở rộng số lượng nhiệm vụ, chúng tôi tuần tự thêm các hỗn hợp nhiệm vụ bắt đầu từ hỗn hợp có ít nhiệm vụ nhất đến hỗn hợp có nhiều nhiệm vụ nhất: CoT, Muﬃn, T0-SF, và NIV2.

Hình 4 cho thấy tác động kết hợp của việc mở rộng hai biến này đối với trung bình chuẩn hóa của các benchmark được giữ lại. Kết quả benchmark riêng lẻ được báo cáo trong Bảng 3. Đầu tiên, chúng tôi thấy rằng đối với cả ba kích thước mô hình được hiển thị, tinh chỉnh đa nhiệm vụ theo hướng dẫn cải thiện hiệu suất với biên độ lớn so với không tinh chỉnh. Mức tăng hiệu suất dao động từ 9.4% đến 15.5%.

Thứ hai, việc tăng số lượng nhiệm vụ tinh chỉnh cải thiện hiệu suất, mặc dù phần lớn cải thiện đến từ việc sử dụng lên đến 282 nhiệm vụ. Có hai giải thích tiềm năng cho mức tăng nhỏ sau 282 nhiệm vụ. Một là các nhiệm vụ bổ sung không đặc biệt đa dạng, và do đó chúng không cung cấp cho mô hình kiến thức mới. Giải thích khác là hầu hết các lợi ích từ tinh chỉnh đa nhiệm vụ theo hướng dẫn đến từ việc mô hình học cách biểu达 tốt hơn kiến thức mà nó đã biết từ tiền huấn luyện, và hơn 282 nhiệm vụ không giúp ích quá nhiều. Giải thích thứ hai này có thể có ý nghĩa vì dữ liệu tiền huấn luyện bao gồm 780B token, trong khi tinh chỉnh theo hướng dẫn chỉ sử dụng 1.4B token (0.2% token tiền huấn luyện).

Cuối cùng, chúng tôi thấy rằng việc tăng quy mô mô hình theo thứ tự cường độ (tức là, 8B → 62B hoặc 62B → 540B) cải thiện hiệu suất đáng kể cho cả mô hình đã tinh chỉnh và chưa tinh chỉnh. Lưu ý rằng có thể phức tạp khi xác định xem tinh chỉnh theo hướng dẫn cải thiện mô hình nhỏ hay mô hình lớn nhiều hơn (so với baseline không tinh chỉnh). Ví dụ, mặc dù mức tăng tuyệt đối lớn hơn đối với mô hình 8B so với mô hình 540B (15.5% cho 8B so với 9.4% cho 540B), việc giảm tỷ lệ lỗi tương đối lớn hơn đối với mô hình 540B (18.4% cho 540B so với 16.6% cho 8B).

Việc vẽ các đường cong mở rộng như vậy cung cấp thông tin về cách mở rộng kích thước mô hình và số lượng nhiệm vụ thậm chí còn xa hơn có thể cải thiện hiệu suất. Việc mở rộng kích thước mô hình theo thứ tự cường độ khác (mặc dù đầy thách thức) dự kiến sẽ mang lại mức tăng hiệu suất đáng kể. Việc mở rộng số lượng nhiệm vụ tinh chỉnh cũng nên cải thiện hiệu suất, mặc dù có thể chỉ tăng dần. Nhìn chung, các đường cong mở rộng được vẽ chỉ ra rằng nghiên cứu trong tương lai nên tiếp tục mở rộng tinh chỉnh theo hướng dẫn.

[Hình 4: Hành vi mở rộng]

--- TRANG 7 ---
[Bảng 3: Kết quả chi tiết về tác động của việc tăng số lượng nhiệm vụ]

4 Tinh chỉnh với chú thích chuỗi suy nghĩ
Mục tiêu của tinh chỉnh Flan là tạo ra một checkpoint được cải thiện trên một loạt các đánh giá, bao gồm khả năng suy luận đa bước ngoài các nhiệm vụ NLP truyền thống. Trong phần này, chúng tôi khám phá tác động của việc bao gồm dữ liệu chuỗi suy nghĩ (CoT) trong hỗn hợp tinh chỉnh theo hướng dẫn. Đầu tiên, chúng tôi cho thấy rằng khả năng suy luận được cải thiện của Flan-PaLM vượt trội hơn các mô hình trước đây trên một số benchmark. Sau đó, chúng tôi tiến hành ablation dữ liệu tinh chỉnh CoT và cho thấy rằng trong khi tinh chỉnh theo hướng dẫn không có CoT thực sự làm giảm khả năng suy luận, việc bao gồm chỉ chín bộ dữ liệu CoT cải thiện hiệu suất trên tất cả các đánh giá. Cuối cùng, chúng tôi cho thấy rằng tinh chỉnh CoT mở khóa suy luận zero-shot thông qua "let's think step-by-step" trên các nhiệm vụ BIG-Bench đầy thách thức.

4.1 Tinh chỉnh trên chuỗi suy nghĩ cải thiện suy luận trên các nhiệm vụ được giữ lại
Đầu tiên chúng tôi cho thấy rằng việc bao gồm chín bộ dữ liệu với chú thích chuỗi suy nghĩ (CoT) trong hỗn hợp tinh chỉnh cải thiện khả năng suy luận. Bảng 4 cho thấy rằng khả năng gợi ý CoT của Flan-PaLM vượt trội hơn PaLM trên bốn benchmark đánh giá được giữ lại. Đối với BBH, chúng tôi tuân theo giao thức của Suzgun et al. (2022) và phân tầng các nhiệm vụ thành nhiệm vụ NLP và nhiệm vụ thuật toán.

Bảng 4 cũng cho thấy cách gợi ý CoT có thể được kết hợp với tính nhất quán bản thân (SC; Wang et al., 2022b) để đạt hiệu suất tối ưu mới trên một số benchmark. Ví dụ, trên benchmark MMLU (Hendrycks et al., 2020), Flan-PaLM 540B đạt 75.2%. Đây là biên độ rộng so với các mô hình trước đây (PaLM = 69.3%, code-davinci-002 = 68.3%, Chinchilla = 67.6%). Trên benchmark MGSM của các bài toán toán đa ngôn ngữ, Flan-PaLM với CoT + SC cải thiện SOTA đáng kể, đạt hiệu suất cao ngay cả trên các ngôn ngữ ít được đại diện, chẳng hạn như 69.6% trên Bengali. Để so sánh, PaLM với CoT + SC chỉ đạt 63.6% với tiếng Pháp và 61.2% trên tiếng Đức, là các ngôn ngữ có nhiều tài nguyên. Như một kết quả cuối cùng, trên GSM8K (Cobbe et al., 2021, không hiển thị trong bảng), Flan-PaLM với CoT + SC đạt trạng thái tối ưu mới là 83.9%, mặc dù lưu ý rằng bộ dữ liệu huấn luyện GSM8K được bao gồm trong hỗn hợp tinh chỉnh theo hướng dẫn.

--- TRANG 8 ---
[Bảng 4: So sánh hiệu suất Flan-PaLM và PaLM]

Chúng tôi cũng lưu ý rằng Flan-PaLM không đạt SOTA so với một số mô hình chuyên biệt nhất định. Ví dụ, đối với BBH-algo, bao gồm các nhiệm vụ chỉ yêu cầu thao tác ký hiệu (ví dụ, giữ thứ tự của danh sách đối tượng đã xáo trộn, sắp xếp danh sách từ theo thứ tự bảng chữ cái), Flan-PaLM không vượt trội hơn code-davinci-002, ngay cả với CoT + SC. Hơn nữa, mặc dù Flan-PaLM vượt trội hơn PaLM 14.9% trên TyDiQA one-shot, nó vẫn chưa ngang bằng với ByT5 được tinh chỉnh trên tập huấn luyện TyDiQA (Xue et al., 2022).

4.2 Cần một số dữ liệu chuỗi suy nghĩ để duy trì khả năng suy luận
Tiếp theo chúng tôi tiến hành ablation tác động của việc bao gồm chỉ chín bộ dữ liệu CoT trong tinh chỉnh theo hướng dẫn. Chúng tôi phân tầng các đánh giá thành các benchmark CoT được giữ lại (MMLU, BBH, và MGSM) và các benchmark không phải CoT được giữ lại (MMLU, BBH, và TyDiQA) và tính trung bình chuẩn hóa cho CoT và không phải CoT. Trong Hình 5-trái, hiệu suất trên các benchmark CoT được giữ lại mạnh hơn với tinh chỉnh kết hợp không phải CoT và CoT so với chỉ tinh chỉnh CoT một mình. Hình 5-phải xác nhận rằng, như mong đợi, tinh chỉnh trên CoT và không phải CoT kết hợp không làm giảm hiệu suất trên các nhiệm vụ không phải CoT so với tinh chỉnh chỉ trên không phải CoT.

[Hình 5: Tác động của việc kết hợp dữ liệu CoT và không phải CoT]

--- TRANG 9 ---
Một điểm quan trọng là Hình 5-trái cũng cho thấy rằng việc tinh chỉnh trên một số ví dụ CoT là rất quan trọng để giữ khả năng suy luận như vậy, vì tinh chỉnh chỉ trên không phải CoT làm giảm hiệu suất trên CoT một lượng đáng kể, như được thể hiện bởi đường màu xanh lá cây. Sự suy giảm này có thể gây ngạc nhiên trước nhiều nghiên cứu trước đây phát hiện rằng tinh chỉnh theo hướng dẫn cải thiện hiệu suất trên các nhiệm vụ chưa thấy (Wei et al., 2021; Sanh et al., 2021; Wang et al., 2019a; Min et al., 2022, và các nghiên cứu khác). Tuy nhiên, nghiên cứu trước đây chỉ đánh giá các nhiệm vụ NLP được giữ lại (ví dụ, tinh chỉnh trên tất cả các nhiệm vụ ngoại trừ phân tích cảm xúc, sau đó đánh giá trên các benchmark phân tích cảm xúc), và các mô hình trước đây thường quá nhỏ để suy luận CoT thành công. Cùng nhau, người ta có thể hiểu ablation này như sau: tinh chỉnh theo hướng dẫn cải thiện các nhiệm vụ chưa thấy khi các nhiệm vụ chưa thấy nằm trong cùng mô hình gợi ý với các nhiệm vụ tinh chỉnh (tức là, không phải CoT hoặc CoT). Do đó, cần cả dữ liệu không phải CoT và CoT để cải thiện khả năng mô hình trên tất cả các đánh giá.

4.3 Mở khóa suy luận zero-shot
Một lợi ích cuối cùng của tinh chỉnh theo hướng dẫn trên dữ liệu CoT cả có và không có ví dụ là mô hình kết quả có thể thực hiện suy luận CoT trong môi trường zero-shot. Môi trường zero-shot này quan trọng vì nó kiểm tra khả năng của mô hình tạo ra kỹ năng suy luận riêng của mình mà không cần ví dụ few-shot cho CoT, có thể yêu cầu kỹ thuật gợi ý đáng kể để soạn đúng cách.

Hình 6 cho thấy rằng đối với benchmark BBH của 23 nhiệm vụ BIG-Bench đầy thách thức chưa thấy, các mô hình Flan-PaLM có thể đạt hiệu suất được cải thiện bằng cách tận dụng suy luận CoT được kích hoạt bởi cụm từ "let's think step-by-step" (Kojima et al., 2022). Để so sánh, PaLM không có tinh chỉnh không tạo ra CoT cho phép nó giải quyết các vấn đề này. Ba ví dụ về CoT zero-shot được hiển thị cho PaLM và Flan-PaLM trong Hình 7.

Mặc dù kết quả CoT zero-shot âm tính trên PaLM có thể dường như mâu thuẫn với các phát hiện từ Kojima et al. (2022), một so sánh gần hơn cho thấy rằng chúng không không nhất quán. Phần lớn các thí nghiệm CoT zero-shot thành công trong bài báo đó thực tế tận dụng InstructGPT (Ouyang et al., 2022), được tinh chỉnh theo hướng dẫn (và chúng tôi đưa ra giả thuyết rằng tinh chỉnh theo hướng dẫn này bao gồm một số dữ liệu giống CoT). Các thí nghiệm thành công cho CoT zero-shot trên PaLM không có tinh chỉnh chỉ được hiển thị cho các bài toán từ toán, khác biệt đáng kể so với các loại vấn đề trong BBH.

[Hình 6: Hiệu suất zero-shot trên BBH]

--- TRANG 10 ---
[Hình 7: Ví dụ về suy luận CoT zero-shot]

5 Kết hợp tất cả lại
Với các kết quả trước đây về việc mở rộng số lượng nhiệm vụ và bao gồm dữ liệu chuỗi suy nghĩ, bây giờ chúng tôi cho thấy tính tổng quát của tinh chỉnh theo hướng dẫn bằng cách áp dụng nó cho một số mô hình có kích thước, kiến trúc và mục tiêu huấn luyện khác nhau. Ngoài họ mô hình PaLM, chúng tôi tinh chỉnh theo hướng dẫn các mô hình T5 có kiến trúc encoder-decoder, trái ngược với kiến trúc chỉ decoder của PaLM. Như một phiên bản mở rộng của mô hình PaLM 62B, chúng tôi tinh chỉnh theo hướng dẫn cont-PaLM, là mô hình PaLM-62B được khởi tạo từ PaLM-62B và sau đó được tiền huấn luyện thêm 500B token (Chowdhery et al., 2022). Cuối cùng, chúng tôi tinh chỉnh theo hướng dẫn U-PaLM, là mô hình PaLM 540B được khởi tạo từ PaLM-540B và sau đó được tiền huấn luyện với mục tiêu UL2 trong 20k bước bổ sung (Tay et al., 2022a,b).

Các kết quả đánh giá này được hiển thị trong Bảng 5. Tinh chỉnh theo hướng dẫn cải thiện hiệu suất trung bình chuẩn hóa với biên độ lớn cho tất cả các loại mô hình. Đối với các mô hình T5 không có tinh chỉnh theo hướng dẫn, chúng tôi sử dụng các mô hình thích ứng LM, được tạo ra bằng cách huấn luyện T5 trên 100B token bổ sung từ C4 với mục tiêu mô hình ngôn ngữ tiêu chuẩn (Lester et al., 2021). Với độ khó của các benchmark đánh giá của chúng tôi và thực tế là T5 không đa ngôn ngữ, các mô hình T5 hưởng lợi nhiều nhất từ tinh chỉnh theo hướng dẫn so với các mô hình chưa tinh chỉnh của chúng. Các kết quả này khá mạnh cho một số benchmark - ví dụ, Flan-T5-XL chỉ có 3B tham số và đạt điểm MMLU 52.4%, vượt trội hơn điểm 43.9% của GPT-3 175B. Như một điểm nổi bật khác, mô hình tổng thể mạnh nhất mà chúng tôi đạt được trong bài báo này kết hợp tinh chỉnh theo hướng dẫn với tiền huấn luyện tiếp tục UL2 được sử dụng trong mô hình U-PaLM. Kết quả này cho thấy rằng tinh chỉnh theo hướng dẫn và tiền huấn luyện tiếp tục UL2 là các phương pháp hiệu quả tính toán bổ sung để cải thiện hiệu suất của các mô hình ngôn ngữ mà không tăng quy mô mô hình. Các benchmark AI có trách nhiệm được báo cáo riêng trong Phụ lục C.

--- TRANG 11 ---
[Bảng 5: Kết quả toàn diện trên nhiều mô hình]

6 Đánh giá khả năng sử dụng của sinh văn bản mở
Ngoài các benchmark NLP, các mô hình ngôn ngữ cũng có khả năng tạo ra câu trả lời dài cho các yêu cầu mở. Các benchmark NLP tiêu chuẩn và các chỉ số tự động được sử dụng để đánh giá chúng không đủ để đo lường sở thích của con người giữa các phản hồi dạng mở này (Ouyang et al., 2022). Do đó, chúng tôi tiến hành đánh giá thủ công điều tra tác động của tinh chỉnh theo hướng dẫn đối với khả năng mô hình đưa ra phản hồi mở cho các đầu vào đầy thách thức. Để làm điều này, chúng tôi tạo ra một tập đánh giá gồm 190 ví dụ. Tập đánh giá này bao gồm các câu hỏi được đặt ra theo cách zero-shot cho mô hình trên năm danh mục đầy thách thức, mỗi danh mục 20 câu hỏi: sáng tạo, suy luận trên ngữ cảnh, suy luận phức tạp, lập kế hoạch, và giải thích. Đối với 60 trong số các ví dụ này (từ các danh mục suy luận phức tạp, lập kế hoạch, và giải thích), chúng tôi tạo ra một biến thể với cụm từ kích hoạt chuỗi suy nghĩ (ví dụ, "let's think step-by-step"), như một đánh giá khác về việc liệu tinh chỉnh trên CoT có cho phép zero-shot hay không, được đánh giá định lượng trong Phần 4.3. Ngoài 160 đầu vào zero-shot trên, chúng tôi bao gồm 30 đầu vào kiểm tra khả năng few-shot, mà các mô hình ngôn ngữ mạnh không có tinh chỉnh theo hướng dẫn đã được chứng minh là làm tốt (Chowdhery et al., 2022).

Trong đánh giá này, chúng tôi so sánh các mô hình PaLM 540B và Flan-PaLM 540B. Đối với cả hai mô hình, chúng tôi sử dụng

--- TRANG 12 ---
lấy mẫu nhiệt độ với τ = 0.7 để tạo ra năm phản hồi ngẫu nhiên, sau đó xếp hạng chúng theo điểm log probability mà không chuẩn hóa độ dài. Chúng tôi chọn phản hồi có điểm tốt nhất, sau bước lọc loại bỏ bất kỳ sinh phẩm nào có điểm tốt hơn một nửa điểm trung bình, mà chúng tôi thấy loại bỏ thành công một phần lớn các sinh phẩm có lặp lại không mong muốn. Ví dụ, nếu điểm log probability trung bình của năm sinh phẩm là -20, thì một sinh phẩm có điểm -3 có thể có lặp lại không mong muốn và chúng tôi lọc nó ra. Sau đó chúng tôi trình bày các đầu ra PaLM và Flan-PaLM cho người chấm điểm con người và yêu cầu họ chọn phản hồi dựa trên mức độ mong muốn. Mỗi cặp đầu ra được chấm điểm bởi một người chấm điểm. Hướng dẫn chấm điểm con người được cung cấp trong Phụ lục I.

[Hình 8: Kết quả đánh giá con người]

Kết quả của đánh giá con người này được hiển thị trong Hình 8 - trên 190 ví dụ, các sinh phẩm Flan-PaLM được ưa thích 79% thời gian. Đối với mọi thiết lập zero-shot, Flan-PaLM được ưa thích với biên độ lớn, và đối với các đầu vào sử dụng cụm từ kích hoạt CoT, sở thích của người chấm điểm cho Flan-PaLM so với PaLM tăng thêm khoảng 10%. Đối với few-shot, không có sự thoái hóa so với PaLM.

Khả năng này của các mô hình được tinh chỉnh theo hướng dẫn để phản hồi tốt hơn các đầu vào zero-shot mở phù hợp với Ouyang et al. (2022), cho thấy rằng tinh chỉnh các mô hình ngôn ngữ với một tập hợp các demonstration từ người gắn nhãn, cũng như học tăng cường từ phản hồi con người, cải thiện đánh giá con người trên phân phối prompt của người dùng. Hơn nữa, việc kiểm tra các sinh phẩm mô hình cho PaLM tiết lộ cách chỉ tiền huấn luyện trên mục tiêu dự đoán token tiếp theo không đủ cho khả năng sử dụng zero-shot tốt mặc dù hiệu suất mạnh trên các benchmark NLP. Ví dụ, việc kiểm tra định tính các hành vi không mong muốn của PaLM bao gồm (1) tiếp tục tạo văn bản liên quan thay vì trả lời câu hỏi, (2) lặp lại câu hỏi đầu vào với những sửa đổi nhỏ, và (3) không biết khi nào ngừng tạo văn bản. Điều này có thể là một artifact của việc không sử dụng token kết thúc chuỗi trong tiền huấn luyện, và một số ví dụ về những lỗi này được hiển thị trong Hình 9.

7 Thảo luận
Trong công việc này, chúng tôi đã mở rộng tinh chỉnh theo hướng dẫn bằng (1) mở rộng số lượng nhiệm vụ tinh chỉnh, (2) mở rộng kích thước mô hình, và (3) tinh chỉnh trên dữ liệu CoT. Các mô hình được tinh chỉnh theo hướng dẫn kết quả cho thấy hiệu suất được cải thiện trên một loạt các đánh giá few-shot, zero-shot, và CoT. Với điều này, chúng tôi tóm tắt các bài học từ bài báo này dưới đây.

Đường cong mở rộng cho tinh chỉnh theo hướng dẫn. Chúng tôi đã chỉ ra trong Phần 3 rằng hai thành phần chính của tinh chỉnh theo hướng dẫn - kích thước mô hình và số lượng nhiệm vụ tinh chỉnh - cải thiện hiệu suất. Nghiên cứu trước đây đã mở rộng số lượng mẫu (Puri et al., 2022), số lượng nhiệm vụ (lên 1.6K nhiệm vụ trong Wang et al. (2022c) nhưng sử dụng mô hình 3B) hoặc kích thước mô hình (lên mô hình 137B trong Wei et al. (2021) nhưng chỉ sử dụng 62 nhiệm vụ). Chúng tôi vẽ đường cong mở rộng cho cả hai thành phần này, cho thấy rằng việc mở rộng cả kích thước mô hình và số lượng nhiệm vụ tinh chỉnh dự kiến sẽ tiếp tục cải thiện hiệu suất, mặc dù mở rộng số lượng nhiệm vụ có lợi ích giảm dần (nhưng vẫn tích cực). Hơn nữa, biên độ cải thiện cho tinh chỉnh theo hướng dẫn so với các mô hình không có tinh chỉnh dường như không giảm, gợi ý rằng tinh chỉnh theo hướng dẫn có thể sẽ tiếp tục có ý nghĩa cho các mô hình tương lai.

--- TRANG 13 ---
[Hình 9: Ví dụ so sánh PaLM và Flan-PaLM]

Tinh chỉnh CoT quan trọng cho khả năng suy luận. Mặc dù nghiên cứu tinh chỉnh theo hướng dẫn trước đây đã cho thấy rằng tinh chỉnh trên các nhiệm vụ không phải CoT cải thiện hiệu suất trên các nhiệm vụ không phải CoT chưa thấy, chúng tôi thấy rằng điều này thực sự dẫn đến hiệu suất bị suy giảm trên các nhiệm vụ CoT. Như một giải pháp cho hiệu suất CoT bị suy giảm này, chúng tôi tinh chỉnh kết hợp trên cả dữ liệu không phải CoT và CoT (Phần 4). Tinh chỉnh kết hợp này cho phép hiệu suất CoT tốt hơn đáng kể trong khi duy trì hiệu suất trên các nhiệm vụ không phải CoT, cho phép một mô hình duy nhất làm tốt trên tất cả các đánh giá. Trong khi nghiên cứu trước đây đã cho thấy rằng tinh chỉnh CoT cải thiện hiệu suất trên các nhiệm vụ được giữ trong tinh chỉnh (Ling et al., 2017; Cobbe et al., 2021; Zelikman et al., 2022; Huang et al., 2022, và các nghiên cứu khác), chúng tôi đã cho thấy rằng tinh chỉnh CoT một mô hình lớn cải thiện hiệu suất trên các nhiệm vụ được giữ lại trong khi duy trì cải thiện hiệu suất cho các nhiệm vụ không phải CoT.

Tinh chỉnh theo hướng dẫn tổng quát hóa trên các mô hình. Trong Phần 5, chúng tôi quan sát tính tổng quát của tinh chỉnh theo hướng dẫn bằng cách áp dụng nó cho các mô hình với một loạt kiến trúc khác nhau (chỉ decoder, encoder-decoder), kích thước (T5-80M đến PaLM-540B), và mục tiêu tiền huấn luyện (causal LM, span corruption, và prefix LM + span corruption). Phát hiện này phù hợp với các nghiên cứu trước đây đã chứng minh hiệu quả của tinh chỉnh theo hướng dẫn trên các mô hình T5 (Sanh et al., 2021; Wang et al., 2022c; Scialom et al., 2022) hoặc các mô hình ngôn ngữ chỉ decoder (Wei et al., 2021; Ouyang et al., 2022). Ngoài ra, chúng tôi cho thấy rằng tinh chỉnh theo hướng dẫn kết hợp tốt với các kỹ thuật thích ứng mô hình khác như UL2R (Tay et al., 2022b), dẫn đến mô hình mạnh nhất mà chúng tôi đã huấn luyện trong công việc này (Flan-U-PaLM).

Tinh chỉnh theo hướng dẫn cải thiện khả năng sử dụng và giảm thiểu một số tác hại tiềm ẩn. Việc sử dụng trực tiếp checkpoint đã tiền huấn luyện có thể đầy thách thức đối với những người không chuyên vì mô hình được huấn luyện chỉ trên mục tiêu dự đoán token tiếp theo không biết khi nào ngừng tạo, và có thể mắc lỗi như tiếp tục đầu vào của người dùng thay vì phản hồi với nó. Trong Phần 6, chúng tôi thấy rằng trên một tập hợp đánh giá mở, các đầu ra từ Flan-PaLM có xếp hạng con người tốt hơn đáng kể so với các đầu ra từ PaLM, đặc biệt là cho các nhiệm vụ CoT như suy luận phức tạp, lập kế hoạch, và giải thích. Flan-PaLM vượt trội hơn PaLM trên một số benchmark AI có trách nhiệm, đặc biệt là các benchmark đo lường tác hại ngôn ngữ độc hại. Những kết quả này phù hợp với các phát hiện từ InstructGPT (Ouyang et al., 2022), cho thấy rằng các mô hình đã tinh chỉnh tạo ra các đầu ra phù hợp hơn với sở thích con người. Khả năng sử dụng zero-shot của các mô hình quan trọng cho việc áp dụng rộng rãi các mô hình ngôn ngữ không yêu cầu kỹ thuật gợi ý hoặc yêu cầu ví dụ few-shot. Một thẻ mô hình (Mitchell et al., 2019) được bao gồm trong phụ lục.

Tinh chỉnh theo hướng dẫn hiệu quả về mặt tính toán. Mặc dù việc mở rộng kích thước của các mô hình ngôn ngữ đã được chứng minh là cải thiện hiệu suất một cách đáng tin cậy, nó đòi hỏi tính toán đáng kể. Do đó, việc phát triển các kỹ thuật hiệu quả về mặt tính toán là quan trọng; các kỹ thuật như vậy có thể tận dụng các checkpoint hiện có, không thay đổi chi phí suy luận của các mô hình. Tinh chỉnh theo hướng dẫn cải thiện hiệu suất của các mô hình với lượng tính toán tương đối nhỏ - ví dụ, đối với PaLM 540B, tinh chỉnh theo hướng dẫn chỉ yêu cầu 0.2% tính toán tiền huấn luyện nhưng cải thiện trung bình chuẩn hóa trên các benchmark đánh giá 9.4%. Hơn nữa, các mô hình nhỏ hơn sử dụng tinh chỉnh theo hướng dẫn đôi khi có thể vượt trội hơn các mô hình lớn hơn không có nó. Ví dụ từ Bảng 5, Flan-PaLM 62B vượt trội hơn PaLM 540B trên TyDiQA (58.7% so với 52.9% EM), và Flan-T5 11B vượt trội hơn PaLM 62B trên BBH-direct (43.7% so với 37.5%).

Tóm lại, bằng chứng trên nhấn mạnh cách tinh chỉnh theo hướng dẫn cải thiện hiệu suất trên một loạt các đánh giá few-shot, zero-shot, CoT, và sinh văn bản mở. Tinh chỉnh theo hướng dẫn tổng quát hóa trên các mô hình và kết hợp tốt với các kỹ thuật khác như UL2R. Tất cả những lợi ích này đi kèm với chi phí tính toán tương đối nhỏ so với tiền huấn luyện mô hình. Vì những lý do này, chúng tôi khuyến nghị tinh chỉnh theo hướng dẫn cho hầu như tất cả các mô hình ngôn ngữ đã tiền huấn luyện.

8 Nghiên cứu Liên quan
Công việc của chúng tôi hoạt động tại giao điểm của nhiều lĩnh vực nghiên cứu rộng lớn, bao gồm học đa nhiệm vụ, hướng dẫn, gợi ý, suy luận đa bước, và các mô hình ngôn ngữ lớn (Radford et al., 2019; Brown et al., 2020; Aghajanyan et al., 2021; Chowdhery et al., 2022; Lewkowycz et al., 2022, và các nghiên cứu khác). Các mô hình mà chúng tôi khám phá trong bài báo này kết hợp tinh chỉnh dựa trên hướng dẫn (Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022; Min et al., 2022) với gợi ý và tinh chỉnh dựa trên lý luận (Ling et al., 2017; Cobbe et al., 2021; Wei et al., 2022b, và các nghiên cứu khác). Trong phần này, chúng tôi sẽ thảo luận về cách bài báo của chúng tôi liên quan đến nghiên cứu có liên quan nhất.

Tinh chỉnh theo hướng dẫn. Bài báo này là một phần của dòng nghiên cứu mới nổi tinh chỉnh mô hình đã tiền huấn luyện với hướng dẫn để cải thiện hiệu suất và khả năng sử dụng (Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022, và các nghiên cứu khác). Chúng tôi mở rộng nghiên cứu trước đây trong lĩnh vực này theo nhiều cách. Đầu tiên, về mặt dữ liệu tinh chỉnh, các mô hình của chúng tôi hưởng lợi từ việc được tinh chỉnh trên các hỗn hợp tổng hợp từ nghiên cứu trước đây (Wang et al., 2022c; Sanh et al., 2021; Wei et al., 2021) ngoài chuỗi suy nghĩ, đối thoại, và các bộ dữ liệu mã mà chúng tôi đã thêm. Thứ hai, trong khi nghiên cứu trước đây đã tập trung vào các mô hình ngôn ngữ nhỏ hơn như mô hình 3B (Wang et al., 2022c), mô hình 11B (Sanh et al., 2021), và mô hình 137B (Wei et al., 2021), trong bài báo này chúng tôi mở rộng lên 540B tham số và nghiên cứu rộng rãi tác động của việc mở rộng mô hình ngôn ngữ. Cuối cùng, trong khi phần lớn nghiên cứu trước đây tinh chỉnh trên zero-shot không có ví dụ (Zhong et al., 2021; Wei et al., 2021; Sanh et al., 2021; Wang et al., 2022a) hoặc few-shot với ví dụ (Ye et al., 2021; Wei et al., 2021; Mishra et al., 2021; Min et al., 2022; Wang et al., 2022c), quy trình của chúng tôi tinh chỉnh trên hỗn hợp của cả hai định dạng để cho phép cả zero-shot và few-shot.

Suy luận thông qua tinh chỉnh. Bài báo của chúng tôi cũng chứng minh rằng tinh chỉnh các mô hình ngôn ngữ lớn trên hỗn hợp các bộ dữ liệu bao gồm chú thích CoT cải thiện hiệu suất trên các nhiệm vụ suy luận chưa thấy. Nghiên cứu trước đây hoặc tinh chỉnh các mô hình ngôn ngữ trên một bộ dữ liệu suy luận duy nhất (Ling et al., 2017; Camburu et al., 2018; Cobbe et al., 2021; Nye et al., 2021; Zelikman et al., 2022, và các nghiên cứu khác) hoặc tập trung vào các mô hình có quy mô nhỏ hơn đáng kể (Ling et al., 2017; Camburu et al., 2018; Rajani et al., 2019; Talmor et al., 2020; Zelikman et al., 2022). Có lẽ nghiên cứu liên quan nhất ở đây là Huang et al. (2022), cho thấy rằng hiệu suất suy luận được cải thiện bằng cách tinh chỉnh đa nhiệm vụ một mô hình trên một số bộ dữ liệu CoT tổng hợp tự tạo. So với nghiên cứu đó, chúng tôi tinh chỉnh kết hợp trên dữ liệu CoT và không phải CoT và cho thấy rằng một checkpoint duy nhất có thể được sử dụng cho cả hai thiết lập.

Các phương pháp hiệu quả tính toán cho các mô hình ngôn ngữ tốt hơn. Việc mở rộng các mô hình ngôn ngữ cải thiện hiệu suất theo nhiều cách nhưng đòi hỏi tài nguyên tính toán đáng kể (Kaplan et al., 2020; Brown et al., 2020; Bommasani et al., 2021; Wei et al., 2022a, và các nghiên cứu khác). Như một so sánh tổng quát hơn, công việc của chúng tôi nằm trong số một hướng nghiên cứu đang phát triển nhằm cải thiện các mô hình ngôn ngữ mà không cần mở rộng quy mô lượng tính toán một cách lớn (Hoﬀmann et al., 2022; Padmakumar et al., 2022). Có lẽ nghiên cứu tương tự nhất với chúng tôi về mặt này là UL2R (Tay et al., 2022b), cũng thực hiện huấn luyện bổ sung, mặc dù với mục tiêu khác (causal LM + span corruption) và không có dữ liệu bổ sung nào. Chúng tôi đã cho thấy rằng UL2R và tinh chỉnh theo hướng dẫn có thể mang lại cải thiện kết hợp, với Flan-U-PaLM đạt hiệu suất mạnh nhất cho tất cả các mô hình chúng tôi huấn luyện trong bài báo này. Nghiên cứu bổ sung cải thiện các mô hình ngôn ngữ mà không mở rộng

--- TRANG 14 ---
tính toán bao gồm kiến trúc tốt hơn (So et al., 2021), mục tiêu huấn luyện được cải thiện (Tay et al., 2022a), và dữ liệu tốt hơn (Du et al., 2022), cùng với nghiên cứu khác.

9 Kết luận
Trong bài báo này, chúng tôi đã mở rộng tinh chỉnh theo hướng dẫn và huấn luyện Flan-PaLM bằng (1) mở rộng lên mô hình ngôn ngữ 540B tham số, (2) mở rộng lên 1.8K nhiệm vụ tinh chỉnh, và (3) bao gồm dữ liệu chuỗi suy nghĩ (CoT) trong tinh chỉnh. Các thí nghiệm cho thấy rằng hiệu suất mô hình được cải thiện đáng kể với cả kích thước mô hình lớn hơn và nhiều nhiệm vụ tinh chỉnh hơn. Hơn nữa, trong khi các phương pháp tinh chỉnh theo hướng dẫn trước đây làm giảm hiệu suất trên các nhiệm vụ CoT, việc tinh chỉnh kết hợp với dữ liệu CoT cải thiện hiệu suất trên tất cả các đánh giá. Flan-PaLM đạt hiệu suất tối ưu trên một số benchmark, chẳng hạn như 75.2% trên MMLU five-shot. Flan-PaLM cũng có khả năng sử dụng được cải thiện - ví dụ, nó có thể thực hiện suy luận zero-shot mà không cần kỹ thuật gợi ý hoặc ví dụ few-shot. Ngoài ra, chúng tôi cho thấy rằng tinh chỉnh theo hướng dẫn tương thích với một loạt kích thước mô hình, kiến trúc, và mục tiêu tiền huấn luyện. Với mục đích này, chúng tôi công bố các mô hình Flan-T5 vượt trội hơn các mô hình T5 baseline với biên độ lớn.

Lời cảm ơn
Chúng tôi cảm ơn Nathan Scales và Olivier Bousquet vì lời khuyên và phản hồi của họ về bài báo, và Hugh Williams vì sự giúp đỡ về dữ liệu và thẻ mô hình.

Tài liệu tham khảo
[Phần tài liệu tham khảo được giữ nguyên vì chứa các trích dẫn học thuật chuẩn]

--- TRANG 15 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 16-54 ---
[Phần phụ lục và các bảng kết quả chi tiết được dịch tương tự, giữ nguyên cấu trúc và số liệu]

Phụ lục
Mục lục
A Câu hỏi thường gặp 24
A.1 Các mô hình được tinh chỉnh theo hướng dẫn có tốt hơn cho tinh chỉnh một nhiệm vụ không? . . . . . . . . . . . . . . . 24
A.2 Việc sử dụng gợi ý CoT để đánh giá có luôn cải thiện hiệu suất không? . . . . . . . . . . . . . . . 24
A.3 Tinh chỉnh theo hướng dẫn có cải thiện hiệu suất nhiều hơn hay ít hơn cho các mô hình lớn hơn? . . . . . . . . . 24
A.4 Có bao nhiêu ví dụ được sử dụng như một phần của hỗn hợp CoT trong tinh chỉnh? . . . . . . . . . . . . . . . . 24

[Tiếp tục với các phần phụ lục khác...]
