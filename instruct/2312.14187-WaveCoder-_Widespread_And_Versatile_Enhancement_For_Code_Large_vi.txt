# 2312.14187.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2312.14187.pdf
# File size: 1466502 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
WaveCoder: Cải Tiến Rộng Rãi Và Đa Dụng Cho Các Mô Hình Ngôn Ngữ Lớn Về Code Thông Qua Tinh Chỉnh Hướng Dẫn
Zhaojian Yu1∗Xin Zhang2†Ning Shang2Yangyu Huang2Can Xu2
Yishujie Zhao1∗Wenxiang Hu2Qiufeng Yin2
1Đại học Thanh Hoa
2Microsoft
yzj23@mails.tsinghua.edu.cn ,xinzhang3@microsoft.com
https://github.com/microsoft/WaveCoder
Tóm tắt
Nghiên cứu gần đây chứng minh rằng, sau quá trình tinh chỉnh hướng dẫn, các Mô hình Ngôn ngữ Lớn về Code (Code LLMs) có thể đạt được khả năng ấn tượng để giải quyết một loạt rộng các tác vụ liên quan đến code. Tuy nhiên, các phương pháp tinh chỉnh hướng dẫn hiện tại cho Code LLMs chủ yếu tập trung vào tác vụ tạo code truyền thống, dẫn đến hiệu suất kém trong các tình huống đa tác vụ phức tạp. Trong bài báo này, chúng tôi tập trung vào nhiều tác vụ liên quan đến code và trình bày WaveCoder, một loạt Code LLMs được huấn luyện với dữ liệu hướng dẫn được Cải tiến Rộng rãi Và Đa dụng. Để cho phép các mô hình giải quyết các tác vụ liên quan đến code phức tạp, chúng tôi đề xuất một phương pháp để tạo ra các dữ liệu hướng dẫn đa dạng, chất lượng cao một cách ổn định từ tập dữ liệu code mã nguồn mở trong các tình huống đa tác vụ và thu được CodeSeaXDataset, một tập dữ liệu bao gồm 19,915 instance hướng dẫn trên 4 tác vụ liên quan đến code, nhằm cải thiện khả năng tổng quát hóa của Code LLM. Các thí nghiệm của chúng tôi chứng minh rằng các mô hình WaveCoder vượt trội đáng kể so với các mô hình mã nguồn mở khác về khả năng tổng quát hóa trên các tác vụ liên quan đến code khác nhau. Hơn nữa, WaveCoder-Ultra-6.7B thể hiện khả năng tổng quát hóa tối tân trên một loạt rộng các tác vụ liên quan đến code.

1 Giới thiệu
Gần đây, các Mô hình Ngôn ngữ Lớn (LLMs) như ChatGPT, GPT-4 (OpenAI, 2023), và Gemini1 đã đạt được mức hiệu suất chưa từng có trong một loạt rộng các tác vụ NLP. Các mô hình này sử dụng quy trình tiền huấn luyện tự giám sát, và tinh chỉnh có giám sát tiếp theo để thể hiện khả năng zero/few-shot đặc biệt, tuân theo hiệu quả các hướng dẫn của con người trên các tác vụ khác nhau.

Đối với các tác vụ liên quan đến code, một số nghiên cứu trước đó, bao gồm Codex (Chen et al., 2021), StarCoder (Li
∗Công việc được thực hiện trong thời gian thực tập tại Microsoft.
†Tác giả liên hệ.
1https://deepmind.google/technologies/
geminiet al., 2023a), CodeLLaMa (Roziere et al., 2023) và DeepseekCoder (Guo et al., 2024), đã thành công chứng minh rằng tiền huấn luyện trên corpus code có thể cải thiện đáng kể khả năng của mô hình trong việc giải quyết các vấn đề liên quan đến code. Sau quá trình tiền huấn luyện, tinh chỉnh hướng dẫn (Wei et al., 2022; Aribandi et al., 2022; Chung et al., 2022) đã cho thấy hiệu quả trong việc cải thiện chất lượng của các phản hồi LLM. Để cụ thể tăng cường hiệu suất của Code LLMs trên các tác vụ liên quan đến code thông qua tinh chỉnh hướng dẫn, nhiều phương pháp hiện tại để tạo dữ liệu hướng dẫn đã được thiết kế. Ví dụ, Code Alpaca (Chaudhary, 2023) sử dụng phương pháp self-instruct (Wang et al., 2023a) trong lĩnh vực coding, tận dụng khả năng few-shot của teacher LLM để tạo dữ liệu hướng dẫn. Tương tự, WizardCoder (Luo et al., 2024) áp dụng phương pháp evol-instruct (Xu et al., 2024) dựa trên Code Alpaca, thể hiện một phương pháp mới và hiệu quả cho việc tạo dữ liệu hướng dẫn.

Những ứng dụng này nhấn mạnh tiềm năng của việc sử dụng teacher LLMs để tạo ra nội dung hướng dẫn hiệu quả, từ đó mở ra con đường cho việc tạo dữ liệu hướng dẫn trong lĩnh vực code. Tuy nhiên, chất lượng của dữ liệu mà chúng tạo ra phụ thuộc rất nhiều vào hiệu suất của teacher LLM và các seed ban đầu hạn chế, điều này thường tạo ra một lượng lớn các instance hướng dẫn trùng lặp và giảm hiệu quả của tinh chỉnh hướng dẫn (Xu et al., 2022; Yan et al., 2023; Lee et al., 2022). Để thoát khỏi sự phụ thuộc vào teacher LLMs, Octopack (Muennighoff et al., 2024) xây dựng một tập dữ liệu hướng dẫn code tận dụng cấu trúc tự nhiên của Git commits. Tuy nhiên, việc đảm bảo chất lượng dữ liệu trong git messages là một thách thức đáng kể, và việc sàng lọc toàn diện dữ liệu thông qua các quy tắc lọc nhân tạo thường là một nhiệm vụ phức tạp. Ngoài ra, những nỗ lực này chủ yếu tập trung vào các tác vụ tạo code truyền thống và thiếu khả năng tạo ra các hướng dẫn chi tiết, cụ thể cho từng tác vụ trong các tình huống đa tác vụ.arXiv:2312.14187v5  [cs.CL]  7 Jun 2024

--- TRANG 2 ---
Quy tắc được định nghĩa thủ công
Không gian Embedding Code Tạo Coreset
Coreset Code ThôThu thập dữ liệu thôTạo Dữ liệu Hướng dẫn
Bộ Tạo LLM
Bộ Phân biệt LLM
Tạo Tệ Tạo Tốt zero/few shot Cơ sở dữ liệu Ví dụ Quy trình Huấn luyện Mô hình Ngôn ngữ Cơ sở
Tạo Tốt
CodeSeaXDataset Tinh chỉnh hướng dẫn WaveCoder Giải quyết vấn đề
Tập dữ liệu nền tảng Định nghĩa tác vụ:ØTạo codeØTóm tắt codeØ…
Quy tắc tác vụ:ØKiểm tra đầu vàoØKiểm tra việc tạoØ…
Đầu vào:…Tạo:
A
B Cài đặt Bộ tạo Cài đặt Bộ phân biệt
C
Định dạng lại việc tạo tốt thành dữ liệu hướng dẫn
Code Thô
A
B
C
Thu thập code thô Kcenter Greedy
D
D Quy trình huấn luyện

Hình 1: Tổng quan về cải tiến rộng rãi và đa dụng cho Code LLM. Phần B và C biểu thị Bộ Tạo dựa trên LLM và Bộ Phân biệt dựa trên LLM, trong đó bộ tạo có thể tận dụng các ví dụ khác nhau trong cơ sở dữ liệu ví dụ thông qua học trong ngữ cảnh.

Trong bài báo này, chúng tôi chủ yếu tập trung vào nhiều tác vụ liên quan đến code, nhằm tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng phù hợp với các yêu cầu tác vụ cụ thể. Giải quyết những thách thức nêu trên, chúng tôi tinh chỉnh dữ liệu hướng dẫn bằng cách phân loại các instance hướng dẫn thành bốn tác vụ liên quan đến code phổ quát trong CodeXGLUE (Lu et al., 2021): 1) Tóm tắt Code, 2) Tạo Code, 3) Dịch Code, 4) Sửa chữa Code và đề xuất một phương pháp tạo hướng dẫn được cải tiến rộng rãi và đa dụng có thể tận dụng tối đa dữ liệu code mã nguồn mở và tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng một cách ổn định trong các tình huống đa tác vụ. Thông qua chiến lược tạo này, chúng tôi thu được một tập dữ liệu gồm 19,915 instance hướng dẫn trên bốn tác vụ liên quan đến code, được gọi là CodeSeaXDataset.

Để xác thực phương pháp của chúng tôi, chúng tôi huấn luyện StarCoder (Li et al., 2023a), CodeLLaMa (Roziere et al., 2023), và DeepseekCoder (Guo et al., 2024) với tập dữ liệu CodeSeaXDataset ban đầu và có được WaveCoder. Sau đánh giá kỹ lưỡng trên các benchmark HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEvalPack (Muennighoff et al., 2024), kết quả thí nghiệm cho thấy WaveCoder của chúng tôi thể hiện khả năng tổng quát hóa xuất sắc dựa trên tinh chỉnh hướng dẫn được cải tiến rộng rãi và đa dụng. Hơn nữa, để khám phá thêm những cải tiến mang lại bởi chất lượng dữ liệu, chúng tôi sử dụng GPT-4 (OpenAI, 2023) để tái tạo phản hồi cho hướng dẫn trong CodeSeaXDataset. Được tinh chỉnh với tập dữ liệu CodeSeaXDataset 20K được cải tiến, chúng tôi thu được WaveCoder-Pro-6.7B đạt 72.0% pass@1 trên HumanEval (Chen et al., 2021) và vượt trội so với các Code LLM mã nguồn mở nhưng vẫn còn kém so với Code LLM SoTA. Kết hợp CodeSeaXDataset được cải tiến với WaveCoder-evol-codealpaca, tập dữ liệu Magicoder-evol-codealpaca2 đã được khử nhiễm, chúng tôi trình bày WaveCoder-Ultra-6.7B, với khả năng tổng quát hóa SoTA trên nhiều tác vụ liên quan đến code.

2 CodeSeaXDataset: Dữ liệu Hướng dẫn Bốn tác vụ Liên quan đến Code

2.1 Chi tiết Tác vụ
Với các tác vụ liên quan đến code từ CodeXGLUE (Lu et al., 2021), chúng tôi chọn bốn tác vụ phổ quát và phổ biến nhất từ ba tác vụ tạo sinh (code-to-text, text-to-code, và code-to-code) để khám phá thêm bao gồm Tóm tắt Code, Tạo Code, Dịch Code, và Sửa chữa Code. Mô tả chi tiết của các tác vụ này có thể tìm thấy dưới đây.

Tóm tắt Code (code-to-text). Tác vụ này nhằm tạo ra một bản tóm tắt ngắn gọn của một đoạn code cho trước. Code thô được sử dụng làm đầu vào và phản hồi của mô hình giáo viên được định dạng lại thành định dạng hướng dẫn.

2https://huggingface.co/datasets/
ise-uiuc/Magicoder-evol-codealpaca-110K

--- TRANG 3 ---
Bảng 1: Tỷ lệ dữ liệu được tạo trong giai đoạn tạo sinh.
Tác vụ Số lượng Tỷ lệ(%) Prompt
Tạo Code 11370 57.1 Triển khai các hàm thực hiện các phép toán cụ thể khi có đầu vào.
Tóm tắt Code 3165 15.8 Viết tài liệu rõ ràng và ngắn gọn cho đoạn code đã cho.
Sửa chữa Code 3144 15.8 Xác định và sửa lỗi trong đoạn code đã cho.
Dịch Code 2236 11.2 Viết lại đoạn code đã cho từ ngôn ngữ lập trình này sang ngôn ngữ lập trình khác.

Bảng 2: Tỷ lệ ngôn ngữ lập trình trong code thô.
Tác vụ Tỷ lệ(%)
Python 29.44
PHP 21.34
Go 19.68
Java 18.53
JavaScript 5.56
Khác (Ruby,C++,C#) 5.45

Tạo Code (text-to-code, code-to-code).
Trong tác vụ này, mô hình dự kiến sẽ tạo code dựa trên mô tả nhu cầu của người dùng. Do đó, mô hình giáo viên dự kiến sẽ tạo hướng dẫn và code giải pháp khi có code thô như một cặp hướng dẫn-giải pháp. Code giải pháp được tạo sau đó được coi là đầu ra.

Dịch Code (code-to-code). Tác vụ này liên quan đến việc chuyển đổi một ngôn ngữ lập trình thành ngôn ngữ khác. Prompt cụ thể cho tác vụ và code thô được đưa cho mô hình giáo viên, sau đó mô hình tạo hướng dẫn và code đã được dịch.

Sửa chữa Code (code-to-code). Mục đích của tác vụ này là cung cấp code đúng dựa trên các vấn đề tiềm ẩn trong code đã cho. Mô hình giáo viên dự kiến sẽ tạo giải pháp cho code không đúng, thường với code đúng và một số mô tả, sau đó được lấy làm đầu ra.

2.2 Tạo Hướng dẫn được Cải tiến Rộng rãi và Đa dụng

Trong các nghiên cứu trước đây (Zhou et al., 2023; Gupta et al., 2023), nhiều nhà nghiên cứu đã phát hiện ra rằng chất lượng và tính đa dạng của dữ liệu thường đóng vai trò quan trọng hơn trong quá trình tinh chỉnh hướng dẫn so với lượng dữ liệu. Việc cải thiện chất lượng và tính đa dạng của dữ liệu có liên quan trực tiếp đến hiệu suất của LLM được tinh chỉnh. Do đó, để đảm bảo chất lượng và tính đa dạng của dữ liệu của instance hướng dẫn, chúng tôi đề xuất một phương pháp tạo hướng dẫn được cải tiến rộng rãi và đa dụng bao gồm hai phần sau: 1) một phương pháp có thể giữ lại tính đa dạng của dữ liệu hướng dẫn bằng cách giữ lại tính đa dạng của code thô ở mức tối đa. 2) một khung Bộ Tạo-Bộ Phân biệt dựa trên LLM để tạo ra dữ liệu hướng dẫn chất lượng cao một cách ổn định.

2.2.1 Thu thập Code Thô
Để đảm bảo chất lượng và tính đa dạng của code thô, chúng tôi định nghĩa thủ công một số quy tắc lọc và sử dụng phương pháp cluster KCenterGreedy (Sener and Savarese, 2018; Chen et al., 2023) để có được bộ sưu tập code thô từ tập dữ liệu code mã nguồn mở. Trong công việc này, chúng tôi chọn CodeSearchNet3, chứa 2 triệu cặp <comment, code> từ các thư viện mã nguồn mở được lưu trữ trên GitHub, làm tập dữ liệu nền tảng và xử lý nó với các bước sau:

Quy tắc lọc được định nghĩa thủ công. Để chọn code chất lượng cao cho instruction-tuning, chúng tôi tạo các quy tắc sau để lọc tập dữ liệu nền tảng: i) Trong công việc này, chúng tôi đã lọc code để đảm bảo rằng độ dài của code yêu cầu không quá dài cũng không quá ngắn. ii) Theo Code Alpaca (Chaudhary, 2023), chúng tôi đã loại bỏ code thô chứa từ ngữ từ danh sách đen, có thể làm giảm hiệu suất của mô hình kết quả.

Phương pháp chọn Coreset. Để đảm bảo tính đa dạng của dữ liệu khi chọn mẫu code thô, chúng tôi sử dụng thuật toán KCenterGreedy (Sener and Savarese, 2018), đã được chứng minh hiệu quả trong việc thu được một tập hợp các mẫu cốt lõi của một phân phối, để chọn các mẫu đại diện từ tập dữ liệu code mã nguồn mở dựa trên các embedding code được mã hóa bởi cùng một mô hình embedding (roberta-large-v1 (Liu et al., 2019)).

Bằng cách kết hợp phương pháp như vậy vào tập dữ liệu code mã nguồn mở, tính đa dạng của dữ liệu được tạo không còn chỉ dựa vào khả năng của chính teacher LLM hoặc seed ban đầu. Hơn nữa, do ứng dụng của thuật toán KCenterGreedy, tính đa dạng của các ngôn ngữ cũng được giữ lại đáng kể, như được thể hiện trong Bảng 2.

3https://huggingface.co/datasets/code_
search_net

--- TRANG 4 ---
code mã nguồn mở
Tạo Chuyển đổi/Viết lại đoạn code đã cho từ ngôn ngữ lập trình này sang ngôn ngữ lập trình khác. Viết tài liệu rõ ràng và ngắn gọn cho đoạn code đã cho. Xác định và sửa lỗi trong đoạn code đã cho.

LM Mỗi trường hợp được tạo cần được cung cấp với các khóa sau:ØTên Tác vụØHướng dẫnØĐầu vàoØĐầu raĐây là một số yêu cầu bạn nên tuân theo:1. Đầu ra là một giải pháp cụ thể giải quyết Hướng dẫn và Đầu vào; do đó, Đầu ra phải có liên quan đến cả Hướng dẫn và Đầu vào.2. Hướng dẫn nên là một hoặc hai câu.3. Trong Đầu ra, chỉ nên chứa code. Không nên có giải thích nào được cung cấp bên ngoài code....Mỗi trường hợp được tạo cần được cung cấp với các khóa sau:ØTên Tác vụØHướng dẫnØĐầu vàoØĐầu raĐây là một số yêu cầu bạn nên tuân theo:1. Đầu ra là một giải pháp cụ thể giải quyết Hướng dẫn và Đầu vào; do đó, Đầu ra phải có liên quan đến cả Hướng dẫn và Đầu vào.2. Hướng dẫn nên là một hoặc hai câu.3. Trong Đầu ra, chỉ nên chứa code. Không nên có giải thích nào được cung cấp bên ngoài code....Mỗi trường hợp được tạo cần được cung cấp với các khóa sau:ØTên Tác vụØHướng dẫnØThông tinØGiải phápĐây là một số yêu cầu bạn nên tuân theo:1. Đầu ra là một giải pháp cụ thể giải quyết Hướng dẫn và Đầu vào; do đó, Đầu ra phải có liên quan đến cả Hướng dẫn và Đầu vào.2. Hướng dẫn nên là một hoặc hai câu.3. Trong Đầu ra, chỉ nên chứa code. Không nên có giải thích nào được cung cấp bên ngoài code....ØTên Tác vụØHướng dẫnØThông tinØGiải pháp

LM Phân tích:-bước 1: kiểm tra code:1. Đầu vào phải là code và không thể chỉ chứa bình luận. -bước 2: kiểm tra Đầu ra:1. Giải pháp: Giải pháp có liên quan đến hướng dẫn và thông tin. Giải pháp là giải quyết cụ thể cho hướng dẫn và thông tin. 2. Hướng dẫn: ngôn ngữ lập trình nên được chỉ định trong hướng dẫn. 3. Giải pháp: trong giải pháp, chỉ nên chứa code và bình luận trong code. Không nên có giải thích nào được cung cấp bên ngoài code. 4. Hướng dẫn: Nội dung của hướng dẫn nên có liên quan đến Đầu vào và nên là một bản tóm tắt của nội dung Đầu vào, không có thông tin không liên quan bổ sung nào. ...Được Tạo bởi Mô hìnhViết bởi Con người

A
B Giai đoạn Tạo sinh Giai đoạn Phân biệt

Hình 2: Tổng quan về khung Bộ Tạo-Bộ Phân biệt dựa trên LLM của chúng tôi. Trong phần A, đầu ra của Bộ Tạo bao gồm 4 khóa: Tên tác vụ, Hướng dẫn, Thông tin, Giải pháp. Tất cả các khóa sẽ được phân tích trong Giai đoạn Phân biệt và phân tích có thể được tái sử dụng làm ví dụ trong lượt tiếp theo.

2.2.2 Khung Bộ Tạo-Bộ Phân biệt dựa trên LLM
Sau quá trình thu thập code thô, tính đa dạng dữ liệu từ code thô đã được giữ lại, trong đó bước tiếp theo là tạo dữ liệu hướng dẫn cho tinh chỉnh có giám sát từ code thô. Để đảm bảo thêm chất lượng của dữ liệu hướng dẫn được tạo, như được thể hiện trong Hình 2, chúng tôi đề xuất một khung Bộ Tạo-Bộ Phân biệt dựa trên LLM trong đó bộ tạo có thể tận dụng một lượng lớn code mã nguồn mở không giám sát để tạo dữ liệu hướng dẫn có giám sát và bộ phân biệt có thể tạo phân tích cho từng thành phần trong dữ liệu hướng dẫn.

Giai đoạn Tạo sinh. Trong giai đoạn tạo sinh, chúng tôi sử dụng GPT-4 để tạo định nghĩa cho từng tác vụ liên quan đến code. Như được thể hiện trong Hình 2, theo định nghĩa tác vụ được tạo bởi mô hình, chúng tôi phát triển thủ công các yêu cầu tạo sinh cho từng tác vụ liên quan đến code. Tích hợp cả định nghĩa tác vụ và tất cả các yêu cầu liên quan vào prompt tạo sinh, chúng tôi lấy code thô làm đầu vào và chọn các ví dụ khác nhau từ cơ sở dữ liệu ví dụ để tạo dữ liệu hướng dẫn bằng GPT-3.5.

Giai đoạn Phân biệt. Trong quá trình khám phá quy trình tạo hướng dẫn, chúng tôi nhận thấy rằng chất lượng dữ liệu của các instance hướng dẫn không thể được đảm bảo chỉ thông qua giai đoạn tạo sinh. Để tăng cường khả năng kiểm soát của việc tạo dữ liệu và đảm bảo thêm chất lượng dữ liệu, chúng tôi sử dụng GPT-4 làm bộ phân biệt dựa trên LLM để tiếp tục phân tích và lọc dữ liệu hướng dẫn. Tiếp theo, được lấy cảm hứng từ Zero-shot-CoT (Kojima et al., 2022), chúng tôi thiết lập một loạt quy tắc, được minh họa trong Hình 4 và chia nhỏ chúng thành một số chủ đề phụ để đảm bảo độ chính xác phân biệt trong đó bộ phân biệt dựa trên LLM có thể phân tích việc tạo từng bước. Bằng cách áp dụng phương pháp này, các quy tắc phân biệt có thể được sửa đổi từng phần để giải quyết các vấn đề nhất định. Sau quá trình phân biệt, như được thể hiện trong Hình 1, mỗi instance hướng dẫn được phân loại là trường hợp tốt hoặc xấu và thông tin phân loại được chọn ngẫu nhiên trong việc tạo sinh sau đó làm ví dụ. Đối với việc tái sử dụng các instance hướng dẫn được phân loại này, khác với self-instruct (Wang et al., 2023a) chỉ sử dụng nhiệm vụ seed ban đầu làm ví dụ tốt, chúng tôi khai thác cả việc tạo tốt và việc tạo xấu làm ví dụ few-shot để bộ tạo có thể học từ lỗi trong các ví dụ xấu khác nhau. Do đó, khung này cung cấp một phương pháp toàn diện để tạo và đánh giá dữ liệu hướng dẫn, đảm bảo một tập dữ liệu huấn luyện chất lượng cao.

3 Thí nghiệm

3.1 Thiết lập
Khác với các nghiên cứu trước đây (Luo et al., 2024; Shen et al., 2023; Gunasekar et al., 2023) chủ yếu tập trung vào tác vụ tạo code, chúng tôi tạo khoảng 20K tập dữ liệu bao gồm 4 tác vụ liên quan đến code phổ biến để tăng cường khả năng tổng quát hóa của Code LLM. Để có được các mô hình WaveCoder, chúng tôi chọn StarCoder-15B, CodeLLaMa (7B và 13B), DeepseekCoder-6.7B làm mô hình cơ sở và tinh chỉnh tất cả các mô hình cơ sở trong 3 epoch sử dụng GPU NVIDIA A100-80GB. Đối với StarCoder-15B, CodeLLaMa-7B và CodeLLaMa-13B, chúng tôi đặt global batch size thành 256 sử dụng Tensor Parallel và đặt learning rate ban đầu ở 2e-5. Đối với DeepseekCoder-6.7B, chúng tôi đặt global batch size thành 512 sử dụng module Fully Sharded Data Parallel (FSDP) từ Pytorch và đặt learning rate ban đầu ở 5e-5.

--- TRANG 5 ---
Bảng 3: Kết quả pass@1 trên benchmark HumanEval và MBPP. Chúng tôi sử dụng điểm tự báo cáo khi có sẵn. Các từ viết tắt "CL", "SC", "DS" đề cập đến các mô hình cơ sở CodeLLaMa và StarCoder và DeepseekCoder, tương ứng. "WaveCoder-Pro-6.7B" và "WaveCoder-Ultra-6.7B" được chi tiết trong đoạn cuối của Phần 1. Do sự khác biệt trong chiến lược giải mã từ công việc đánh giá trước đây, chúng tôi đánh dấu kết quả của greedy decoding bằng màu xanh và n = 200 samples bằng màu đỏ. -: Không được báo cáo trong bài báo của họ.

Mô hình Tham số Mô hình Cơ sở Dữ liệu InsT HumanEval MBPP (500)
Mô hình Có bản quyền
GPT-4 - - - 85.4 / 67.0 -
ChatGPT - - - 73.2 / 48.1 52.2
Mô hình Mã nguồn Mở
StarCoder 15B - ✘ 33.6 43.3
OctoCoder 15B StarCoder 13K 46.2 43.5
WizardCoder 15B StarCoder 78K 57.3 51.8
WaveCoder-SC-15B
 15B StarCoder 20K 50.5 (+16.9) 51.0 (+7.4)
CodeLLaMa 7B - ✘ 33.5 41.4
CodeLLaMa-instruct 7B CodeLLaMa 14K 34.8 44.4
WaveCoder-CL-7B
 7B CodeLLaMa 20K 48.1 (+14.6) 47.2 (+5.8)
CodeLLaMa 13B - ✘ 36.0 47.0
CodeLLaMa-instruct 13B CodeLLaMa 14K 42.5 49.4
WaveCoder-CL-13B
 13B CodeLLaMa 20K 55.4 (+19.4) 49.6 (+2.6)
DeepseekCoder 6.7B - ✘ 49.4 60.6
Magicoder-DS 6.7B DeepseekCoder 75K 66.5 60.4
WaveCoder-DS-6.7B
 6.7B DeepseekCoder 20K 64.0 (+14.6) 62.8 (+2.2)
WaveCoder-Pro-6.7B
 6.7B DeepseekCoder 20K 72.0 (+22.6) 63.6 (+3.0)
Mô hình Mã nguồn Mở SoTA
DeepseekCoder-instruct∗6.7B DeepseekCoder - 73.8 62.8
Magicoder-S-DS 6.7B DeepseekCoder 185K 76.8 64.6
WaveCoder-Ultra-6.7B
 6.7B DeepseekCoder 130K 78.6 (+29.2) 64.4 (+3.8)

Benchmarks và Baselines. Để đảm bảo đánh giá toàn diện về khả năng tổng quát hóa của mô hình, chúng tôi chấm điểm mô hình của chúng tôi trên ba benchmark code trên các tác vụ liên quan đến code khác nhau: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) và HumanEvalPack (Muennighoff et al., 2024), như được minh họa trong Phụ lục D.

Mô hình Có bản quyền. Chúng tôi trình bày kết quả tự báo cáo từ một loạt LLMs SoTA, bao gồm ChatGPT (gpt-3.5-turbo), GPT-4. Nếu không được báo cáo, chúng tôi sử dụng kết quả từ Octopack (Muennighoff et al., 2024) hoặc tự đánh giá.

Mô hình Mã nguồn Mở. Để đảm bảo so sánh công bằng, chúng tôi chọn các mô hình đã được huấn luyện với lượng instance hướng dẫn tương tự cho phân tích so sánh của chúng tôi.

Mô hình Mã nguồn Mở SoTA. Chúng tôi so sánh WaveCoder-6.7B với Code LLM mã nguồn mở SoTA, bao gồm Magicoder-S-DS (Wei et al., 2023) và DeepseekCoder-instruct-6.7B (Wei et al., 2023) trên một loạt rộng các tác vụ liên quan đến code. Tất cả kết quả của các mô hình mã nguồn mở SoTA được trình bày từ EvalPlus. (Liu et al., 2023) Nếu không được báo cáo, chúng tôi tự đánh giá.

3.2 Kết quả
Đánh giá trên Tác vụ Tạo Code. HumanEval và MBPP là hai benchmark đại diện cho tác vụ tạo code, như được minh họa trong Phụ lục D. Bảng 3 hiển thị điểm pass@1 của các LLM khác nhau trên cả hai benchmark. Từ kết quả, chúng tôi có những quan sát sau:

1) WaveCoder-Pro-6.7B vượt trội so với các mô hình mã nguồn mở khác chỉ với 6.7B tham số và 20K dữ liệu hướng dẫn. Được huấn luyện với tập dữ liệu CodeSeaXDataset được cải tiến bởi GPT-4, WaveCoder-Pro-6.7B đạt 72.0% pass@1 trên HumanEval và 63.6% trên MBPP, vượt trội so với tất cả các mô hình mã nguồn mở nhưng vẫn kém so với các mô hình có bản quyền và các mô hình mã nguồn mở SoTA.

2) Dữ liệu hướng dẫn được tinh chỉnh và đa dạng có thể cải thiện đáng kể hiệu quả của tinh chỉnh hướng dẫn. Như được phác thảo trong Bảng 3, WaveCoder thể hiện hiệu suất đáng khen ngợi, sử dụng tập dữ liệu chỉ bao gồm khoảng 20K Dữ liệu Tinh chỉnh Hướng dẫn (InsT Data), điều này đặt nó ngang hàng với các đối thủ đương thời.

--- TRANG 6 ---
Bảng 4: Kết quả pass@1 trên benchmark HumanEvalFix. Chúng tôi sử dụng điểm tự báo cáo khi có sẵn. Do sự khác biệt trong chiến lược giải mã từ công việc đánh giá trước đây, chúng tôi đánh dấu kết quả của greedy decoding bằng màu xanh và n = 20 samples bằng màu đỏ.

Mô hình Python JavaScript Java Go C++ Rust Trung bình
GPT-4 47.0 48.2 50.0 50.6 47.6 43.3 47.8
StarCoder 8.7 15.7 13.3 20.1 15.6 6.7 13.4
OctoCoder 30.4 28.4 30.6 30.2 26.1 16.5 27.0
WizardCoder 31.8 29.5 30.7 30.4 18.7 13.0 25.7
WaveCoder-SC-15B
 39.3 35.1 34.8 36.2 30.2 22.5 33.0
CodeLLaMa-instruct-7B 28.0 23.2 23.2 18.3 0.1 0.1 15.5
CodeLLaMa-CodeAlpaca-7B 37.8 39.0 42.0 37.8 37.2 29.2 37.1
WaveCoder-CL-7B
 41.4 41.4 42.0 47.1 42.7 34.7 41.5
CodeLLaMa-instruct-13B 29.2 19.5 32.3 24.4 12.8 0.1 19.7
CodeLLaMa-CodeAlpaca-13B 42.7 43.9 50.0 45.7 39.6 37.2 43.2
WaveCoder-CL-13B
 48.8 48.2 50.6 51.8 45.1 40.2 47.4
DeepseekCoder-6.7B 29.9 29.2 39.0 29.2 25.0 21.9 29.0
Magicoder-DS 42.0 43.3 50.6 41.4 38.4 29.2 40.8
DeepseekCoder-CodeAlpaca-6.7B 49.4 51.8 45.1 48.8 44.5 31.7 45.2
WaveCoder-DS-6.7B
 57.9 52.4 57.3 47.5 45.1 36.0 49.4
WaveCoder-Pro-6.7B
 59.1 56.7 54.2 45.1 45.7 34.1 49.2
Deepseek-instruct-6.7B 56.1 58.5 57.3 49.4 45.1 36.6 50.5
Magicoder-S-DS 56.1 55.4 58.5 51.2 45.7 35.3 50.3
WaveCoder-Ultra-6.7B
 58.5 57.3 61.0 53.0 50.0 37.2 52.8

ing với các đối thủ đương thời. Mặc dù có sự thiếu hụt đáng chú ý trong các benchmark tạo code so với WizardCoder (50.5 vs 57.3) và Magicoder (64.0 vs 66.5), điều quan trọng là phải xem xét sự chênh lệch đáng kể về khối lượng dữ liệu huấn luyện. Hơn nữa, quan sát thấy rằng WaveCoder-pro-6.7B vượt trội đáng kể so với Magicoder-DS-6.7B (72.0 vs 66.5), chứng minh hiệu quả của chất lượng và tính đa dạng dữ liệu trong tinh chỉnh hướng dẫn.

Đánh giá trên Tác vụ Liên quan đến Code Khác. Chúng tôi chấm điểm WaveCoder với các Code LLM tối tân trên HumanEvalPack (Muennighoff et al., 2024) trong Bảng 4 và Bảng 5, làm nổi bật những quan sát quan trọng sau:

1) Các mô hình WaveCoder vượt trội so với tất cả các mô hình mã nguồn mở trên tác vụ liên quan đến code khác. Dựa trên Starcoder, WaveCoder-SC được đề xuất của chúng tôi đã thể hiện hiệu suất đặc biệt, vượt qua khả năng của cả WizardCoder và OctoCoder như được chứng minh bởi các benchmark HumanEvalFix (33.0 vs 25.7 vs 27.0) và HumanEvalExplain (30.8 vs 27.5 vs 24.5), điều này cũng được thể hiện trong các mô hình cơ sở khác. Đáng chú ý, WaveCoder-DS-6.7B đạt 49.4% điểm pass@1 trung bình trên HumanEvalFix và 41.3% trên HumanEvalExplain, vượt trội so với tất cả các mô hình mã nguồn mở và thể hiện khả năng tổng quát hóa mạnh mẽ trong các tình huống đa tác vụ.

2) Việc cải tiến trong tinh chỉnh và đa dạng hóa dữ liệu có thể tăng cường đáng kể hiệu quả của tinh chỉnh hướng dẫn trong các tình huống đa tác vụ. Việc tinh chỉnh dữ liệu như vậy, kết hợp với việc phân loại hướng dẫn thành bốn tác vụ liên quan đến code, đã đẩy các mô hình của chúng tôi đạt được khả năng tổng quát hóa chưa từng thấy trong các tác vụ liên quan đến code khác nhau. Đáng chú ý, mô hình WaveCoder-DS-6.7B của chúng tôi vượt trội so với GPT-4 (49.4 vs 47.8) trên HumanEvalFix, từ đó nhấn mạnh tiềm năng của các mô hình nhỏ hơn để đạt được hiệu suất gần ngang bằng với các mô hình có nhiều tham số khi được tối ưu hóa hiệu quả.

WaveCoder-Ultra-6.7B. Được lấy cảm hứng từ Magicoder-S-DS-6.7B (Wei et al., 2023), chúng tôi kết hợp CodeSeaXDataset với WaveCoder-evol-codealpaca thành một tập dữ liệu 130K. Được tinh chỉnh với sự kết hợp của hai tập dữ liệu này, chúng tôi thu được WaveCoder-Ultra-6.7B. Như được minh họa trong Bảng 3, 4, 5, WaveCoder-Ultra-6.7B có khả năng tổng quát hóa tối tân trên một loạt rộng các tác vụ liên quan đến code, điều này làm nổi bật tầm quan trọng của tập dữ liệu CodeSeaXDataset của chúng tôi một lần nữa và chứng minh tiềm năng của các tập dữ liệu lớn hơn.

4 Ablation và Phân tích

4.1 Ablation của Các Tác vụ Liên quan đến Code
Để khám phá mối quan hệ giữa các tác vụ khác nhau, chúng tôi tiến hành nghiên cứu ablation về loại tác vụ của dữ liệu hướng dẫn. Sử dụng DeepseekCoder-Base-

--- TRANG 7 ---
Bảng 5: Kết quả pass@1 trên benchmark HumanEvalExplain. Chúng tôi sử dụng điểm tự báo cáo khi có sẵn. Do sự khác biệt trong chiến lược giải mã từ công việc đánh giá trước đây, chúng tôi đánh dấu kết quả của greedy decoding bằng màu xanh và n = 20 samples bằng màu đỏ.

Mô hình Python JavaScript Java Go C++ Rust Trung bình
GPT-4 64.6 57.3 51.2 58.5 38.4 42.7 52.1
StarCoder 0.0 0.0 0.0 0.0 0.0 0.0 0.0
WizardCoder 32.5 33.0 27.4 26.7 28.2 16.9 27.5
OctoCoder 35.1 24.5 27.3 21.1 24.1 14.8 24.5
WaveCoder-SC-15B
 37.1 33.3 40.5 23.3 31.8 19.3 30.8
CodeLLaMa-instruct-7B 33.5 36.0 31.7 21.3 25.0 16.4 27.3
CodeLLaMa-CodeAlpaca-7B 34.7 24.4 37.8 23.2 28.6 19.5 28.0
WaveCoder-CL-7B
 41.4 31.7 39.0 25.0 34.1 23.2 32.4
CodeLLaMa-instruct-13B 40.2 26.8 37.2 22.5 28.0 14.6 28.2
CodeLLaMa-CodeAlpaca-13B 32.3 28.0 34.1 18.9 29.9 20.7 27.3
WaveCoder-CL-13B
 45.7 42.0 48.2 32.3 38.4 20.7 37.9
DeepseekCoder-6.7B 43.9 40.2 37.8 29.2 34.1 22.5 34.6
Deepseek-CodeAlpaca-6.7B 40.8 37.2 42.1 29.9 31.7 22.5 34.0
Magicoder-DS 55.5 36.6 49.4 36.0 39.6 27.4 40.7
WaveCoder-DS-6.7B
 48.2 47.5 49.4 32.3 48.2 22.0 41.3
WaveCoder-Pro-6.7B
 53.0 43.3 54.9 34.1 42.7 20.0 41.3
Magicoder-S-DS 60.3 46.3 54.3 38.4 48.1 29.2 46.1
Deepseek-instruct-6.7B 62.2 54.3 61.0 39.6 55.5 33.5 51.0
WaveCoder-Ultra-6.7B
 56.7 50.0 54.3 34.8 51.2 36.6 47.3

Bảng 6: Nghiên cứu ablation trên các tác vụ liên quan đến code khác nhau: CG (Tạo Code), CS (Tóm tắt Code), CT (Dịch Code), CR (Sửa chữa Code). WaveCoder-DS-6.7B sử dụng tất cả 4 tác vụ liên quan đến code.

Mô hình CG CS CT CR HumanEval HumanEval
Fix (Trung bình) HumanEval
Explain (Trung bình)
DeepseekCoder-Base-6.7B ✘ ✘ ✘ ✘ 49.4 29.0 34.6
WaveCoder-DS-6.7B
 ✔ ✔✔ ✔ 64.0 (+14.6) 49.4 (+20.4) 41.3 (+7.3)
-Không có Sửa chữa ✔ ✔ ✔ ✘ 60.9 (-3.1) 15.7 (-33.7) 41.2 (-0.1)
-Không có Tạo sinh ✘ ✔ ✔ ✔ 53.6 (-10.4) 47.4 (-2.0) 40.5 (-0.8)
-Không có Dịch ✔ ✔ ✘ ✔ 60.9 (-3.1) 49.3 (-0.1) 41.6 (+0.3)
-Không có Tóm tắt ✔ ✘ ✔ ✔ 61.5 (-2.5) 45.6 (-3.8) 28.4 (-12.9)

6.7B làm mô hình cơ sở và dữ liệu CodeSeaXDataset 20K ban đầu làm tập dữ liệu cơ sở, chúng tôi có những quan sát sau từ Bảng 6:

1) Dữ liệu hướng dẫn được tinh chỉnh có thể cải thiện đáng kể khả năng tổng quát hóa của các mô hình tiền huấn luyện mà không có sự đánh đổi. Như được thể hiện trong Bảng 6, kết hợp tất cả 4 tác vụ liên quan đến code vào dữ liệu huấn luyện, WaveCoder-DS-6.7B đạt hiệu suất tốt nhất trên benchmark của tất cả các tác vụ. Ví dụ, sự tham gia của tác vụ Sửa chữa Code mang lại cải thiện tuyệt đối trung bình đáng kể 33.7% cho HumanEvalFix mà không có sự suy giảm đáng kể nào trong các tác vụ khác, và thậm chí còn cải thiện 3.1% tuyệt đối cho benchmark HumanEval.

2) Các tác vụ khác nhau có thể thúc đẩy lẫn nhau để mô hình có thể thể hiện khả năng tổng quát hóa. Từ Bảng 6, chúng ta có thể quan sát thấy rằng bất kỳ sự kết hợp nào của ba tác vụ đều dẫn đến điểm số thấp hơn so với tất cả các tác vụ. Ví dụ, việc bổ sung tác vụ tóm tắt code mang lại cải thiện trung bình khiêm tốn nhưng đáng kể trên tất cả các benchmark. Hơn nữa, việc vắng mặt của bất kỳ tác vụ nào sẽ khiến điểm số của HumanEval giảm, điều này cũng phản ánh sự thúc đẩy lẫn nhau giữa các tác vụ khác nhau.

4.2 Thảo luận về Rò rỉ Dữ liệu
Trong phần này, chúng tôi khám phá khả năng rò rỉ thông qua ba tập dữ liệu hướng dẫn về code (tức là Code Alpaca, CodeSeaXDataset, Magicoder-evol-codealpaca). Để đảm bảo phân tích chính xác, chúng tôi sử dụng mô hình embedding SoTA GTE-Large (Li et al., 2023b) để mã hóa code chính tắc trong các benchmark kiểm tra và tất cả code trong tập huấn luyện. Tiếp theo, chúng tôi tìm người hàng xóm gần nhất trong tập huấn luyện cho mỗi câu hỏi trong benchmark kiểm tra. Như được minh họa trong Hình 3, CodeSeaXDataset có độ tương tự cosine trung bình thấp hơn

--- TRANG 8 ---
CodeSeaXDataset(Avg:0.88)CodeSeaXDataset
CodeSeaXCodeSeaXCodeSeaXDataset(Avg:0.877)CodeSeaXDataset

Hình 3: Thảo luận về rò rỉ dữ liệu trong các tập dữ liệu huấn luyện khác nhau. WaveCoder-evol-codealpaca chỉ tập dữ liệu Magicoder-evol-codealpaca đã được khử nhiễm dưới chiến lược của chúng tôi.

so với các tập dữ liệu khác. Hình 6 trong Phụ lục trình bày hai ví dụ về rò rỉ dữ liệu trong các tập huấn luyện này. Hơn nữa, chúng tôi phân tích tất cả các benchmark và nhận thấy vấn đề rò rỉ dữ liệu nghiêm trọng giữa HumanEval và tập dữ liệu Magicoder-evol-codealpaca. Do đó, chúng tôi khử nhiễm Magicoder-evol-codealpaca cho mỗi bài toán đánh giá trong HumanEval và thu được WaveCoder-evol-codealpaca. Như được minh họa trong Hình 3, WaveCoder-evol-codealpaca có độ tương tự thấp hơn so với Magicoder-evol-codealpaca.

5 Nghiên cứu Liên quan
Tinh chỉnh Hướng dẫn. Các nghiên cứu gần đây, như FLAN (Wei et al., 2022), ExT5 (Aribandi et al., 2022), và FLANT5 (Chung et al., 2022), đã nhấn mạnh hiệu quả của việc tích hợp các tác vụ đa dạng trong quy trình huấn luyện để tăng cường khả năng thích ứng của các mô hình tiền huấn luyện cho các tác vụ downstream. Cụ thể, Flan-PaLM 540B (Chung et al., 2022) với việc tinh chỉnh hướng dẫn trên 1.8K tác vụ đã chứng minh rằng tập dữ liệu hướng dẫn được cải tiến rộng rãi và đa dụng tăng cường đáng kể hiệu suất mô hình ngôn ngữ. InstructGPT (Ouyang et al., 2022), với việc kết hợp dữ liệu hướng dẫn cao cấp được tạo bởi các người chú thích con người, đã cho thấy triển vọng đáng kể trong việc căn chỉnh đầu ra mô hình với ý định của người dùng, thúc đẩy điều tra thêm về các cơ chế tinh chỉnh hướng dẫn. Ngoài ra, Stanford Alpaca (Taori et al., 2023) đã sáng tạo sử dụng dữ liệu hướng dẫn được tạo bởi GPT thông qua self-instruct (Wang et al., 2023a) cho quy trình tinh chỉnh hướng dẫn. WizardLM (Xu et al., 2024) đã xây dựng dựa trên những tiến bộ này bằng cách áp dụng phương pháp evol-instruct, cùng nhau làm sáng tỏ tác động biến đổi của tinh chỉnh hướng dẫn lên khả năng tổng thể của LLM.

Các Mô hình Ngôn ngữ Lớn về Code. Những tiến bộ gần đây trong tạo code đã được thúc đẩy bởi các Code LLM như CodeGen (Nijkamp et al., 2022), CodeT5 (Wang et al., 2021), StarCoder (Li et al., 2023a), CodeLLaMa (Roziere et al., 2023) và Deepseek-Coder (Guo et al., 2024), được hưởng lợi từ tiền huấn luyện rộng rãi trên corpus code mở rộng. Các nỗ lực để tăng cường thêm hiệu quả và khả năng giải quyết vấn đề đã dẫn đến việc phát triển các mô hình được tinh chỉnh hướng dẫn như InstructCodeT5+ (Wang et al., 2023b), WizardCoder (Luo et al., 2024), Pangu-coder2 (Shen et al., 2023), Tuy nhiên, tất cả dữ liệu hướng dẫn mà họ sử dụng đều từ Code Alpaca, không được tinh chỉnh đủ trong bối cảnh môi trường đa tác vụ, điều này thúc đẩy chúng tôi đề xuất các phương pháp mới để tạo dữ liệu hướng dẫn. Đồng thời, với việc phát hành nghiên cứu đương thời Magicoder (Wei et al., 2023), chúng tôi cung cấp một phân tích ngắn gọn trong Phần 3.

6 Kết luận
Bài báo này trình bày WaveCoder, một Code LLM được tinh chỉnh với dữ liệu hướng dẫn được cải tiến rộng rãi và đa dụng. Bằng cách cho phép các mô hình ngôn ngữ giải quyết hiệu quả các tác vụ liên quan đến code phức tạp, phương pháp của chúng tôi chứng minh tiềm năng của việc tích hợp nhiều tác vụ liên quan đến code vào tinh chỉnh hướng dẫn cho Code LLM và tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng cho các yêu cầu tác vụ cụ thể trong các tình huống đa tác vụ. WaveCoder đạt hiệu suất tổng quát hóa tối tân trên các tác vụ liên quan đến code khác nhau vượt trội so với các Code LLM mã nguồn mở hiện có. Hơn nữa, phân tích của chúng tôi về mối quan hệ của các tác vụ khác nhau cung cấp những hiểu biết có giá trị cho nghiên cứu tương lai, mở đường cho các tác vụ liên quan đến code rộng rãi hơn và tập dữ liệu lớn hơn.

--- TRANG 9 ---
Hạn chế
Chúng tôi trình bày WaveCoder và đề xuất một phương pháp tạo dữ liệu có thể tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng một cách ổn định từ tập dữ liệu mã nguồn mở trong tình huống đa tác vụ. Một hạn chế của công việc của chúng tôi là tập dữ liệu huấn luyện mà chúng tôi sử dụng chỉ bao gồm 19,915 hướng dẫn, điều này tạo ra những cải tiến hạn chế cho mô hình. Như được minh họa trong Phần 3, chúng tôi mở rộng tập dữ liệu huấn luyện thành một lượng lớn hơn và mô hình kết quả vẫn có cải thiện đáng kể. Do đó, công việc tương lai nên tập trung vào nhiều loại tác vụ liên quan đến code hơn và tập dữ liệu lớn hơn.

Tuyên bố Đạo đức
Chúng tôi xây dựng tập dữ liệu CodeSeaXDataset từ code mã nguồn mở. Đối với mỗi đoạn code chúng tôi sử dụng, chúng tôi cam kết tuân thủ các điều khoản của giấy phép của nó, bao gồm việc ghi nhận thích hợp đảm bảo rằng bất kỳ sửa đổi hoặc tác phẩm phái sinh nào cũng được chia sẻ dưới các điều khoản tương thích. Hơn nữa, chúng tôi nhận thấy vấn đề rò rỉ dữ liệu nghiêm trọng trong tập dữ liệu Magicoder-evol-codealpaca. Để đảm bảo so sánh công bằng, chúng tôi loại bỏ ba hàng xóm gần nhất của mỗi câu hỏi trong benchmark kiểm tra khỏi tập huấn luyện. Tuy nhiên, nếu tất cả các mẫu tương tự bị loại bỏ một cách tình cờ, tính toàn vẹn của dữ liệu sẽ bị hỏng, điều này có hại cho việc huấn luyện mô hình. Do đó, hiện tượng này nên được quy cho thực tế là các vấn đề trong các benchmark kiểm tra hiện tại là một số logic thuật toán cơ bản nhất. Để kết thúc, chúng tôi kêu gọi các benchmark kiểm tra toàn diện và phức tạp hơn cho Code LLM sẽ không dễ dàng gây ra vấn đề rò rỉ dữ liệu.

Tài liệu tham khảo
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,
Jai Gupta, Kai Hui, Sebastian Ruder, and Donald
Metzler. 2022. Ext5: Towards extreme multi-task
scaling for transfer learning. In International Confer-
ence on Learning Representations (ICLR) .

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .

Sahil Chaudhary. 2023. Code alpaca: An
instruction-following llama model for code genera-
tion. https://github.com/sahil280114/
codealpaca .

Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xi-
aomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo
Zhao. 2023. Maybe only 0.5% data is needed: A pre-
liminary exploration of low training data instruction
tuning. arXiv preprint arXiv:2305.09246 .

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .

Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all
you need. arXiv preprint arXiv:2306.11644 .

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y Wu, YK Li, et al. 2024. Deepseek-coder: When the
large language model meets programming–the rise of
code intelligence. arXiv preprint arXiv:2401.14196 .

Himanshu Gupta, Saurabh Arjun Sawant, Swaroop
Mishra, Mutsumi Nakamura, Arindam Mitra, San-
tosh Mashetty, and Chitta Baral. 2023. Instruction
tuned models are quick learners. arXiv preprint
arXiv:2306.05539 .

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations (ICLR) .

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances
in neural information processing systems (NIPS) ,
35:22199–22213.

Katherine Lee, Daphne Ippolito, Andrew Nystrom,
Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
and Nicholas Carlini. 2022. Deduplicating training
data makes language models better. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 8424–8445.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.
2023a. Starcoder: may the source be with you!
arXiv preprint arXiv:2305.06161 .

Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, and Meishan Zhang. 2023b. Towards
general text embeddings with multi-stage contrastive
learning. arXiv preprint arXiv:2308.03281 .

--- TRANG 10 ---
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and
LINGMING ZHANG. 2023. Is your code gener-
ated by chatGPT really correct? rigorous evalua-
tion of large language models for code generation.
InThirty-seventh Conference on Neural Information
Processing Systems .

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
Codexglue: A machine learning benchmark dataset
for code understanding and generation. In Thirty-
fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1) .

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder:
Empowering code large language models with evol-
instruct. International Conference on Learning Rep-
resentations (ICLR) .

Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam
Singh, Xiangru Tang, Leandro von Werra, and
Shayne Longpre. 2024. Octopack: Instruction tuning
code large language models. International Confer-
ence on Learning Representations (ICLR) .

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. 2022. Codegen: An open large language
model for code with multi-turn program synthesis.
arXiv preprint arXiv:2203.13474 .

OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems (NIPS) , 35:27730–
27744.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .

Ozan Sener and Silvio Savarese. 2018. Active learn-
ing for convolutional neural networks: A core-set
approach. In International Conference on Learning
Representations (ICLR) .

Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,
Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan
Ji, Jingyang Zhao, et al. 2023. Pangu-coder2: Boost-
ing large language models for code with ranking feed-
back. arXiv preprint arXiv:2307.14936 .

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/
stanford_alpaca .

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023a. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (ACL) (Volume 1: Long
Papers) , pages 13484–13508.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare,
Nghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi.
2023b. Codet5+: Open code large language mod-
els for code understanding and generation. arXiv
preprint .

Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH
Hoi. 2021. Codet5: Identifier-aware unified pre-
trained encoder-decoder models for code understand-
ing and generation. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 8696–8708.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations (ICLR) .

Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and
Lingming Zhang. 2023. Magicoder: Source code is
all you need. arXiv preprint arXiv:2312.02120 .

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2024. Wizardlm: Empowering large language
models to follow complex instructions. International
Conference on Learning Representations (ICLR) .

Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang
Li, and Jian Li. 2022. Learning to break the loop:
Analyzing and mitigating repetitions for neural text
generation. Advances in Neural Information Process-
ing Systems (NIPS) , 35:3082–3095.

Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu,
Yafu Li, and Yue Zhang. 2023. Understanding in-
context learning from repetitions. arXiv preprint
arXiv:2310.00297 .

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .

--- TRANG 11 ---
A Prompt
Theo Alpaca (Taori et al., 2023), chúng tôi đặt prompt tinh chỉnh như sau:

Prompt với Input:
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép với một đầu vào cung cấp thêm ngữ cảnh.
Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:{instruction}
### Đầu vào:{input}
### Phản hồi:

Prompt không có Input:
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép với một đầu vào cung cấp thêm ngữ cảnh.
Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:{instruction}
### Phản hồi:

B Ví dụ về khung Bộ Tạo-Bộ Phân biệt dựa trên LLM

Code Thô
#!/usr/bin/env python
# -*- coding: utf -8 -*-
"""
Server đơn giản thêm DocumentWordsProvider vào
worker CodeCompletion.
Trên Windows, script này được đóng băng bởi freeze_setup.py (cx_Freeze).
"""
from pyqode.core import backend
if __name__ == '__main__':
backend.CodeCompletionWorker.providers.append(backend.DocumentWordsProvider())
backend.serve_forever()

Tạo sinh
Task_name: Thêm DocumentWordsProvider vào worker CodeCompletion
Instruction: Viết code Python thêm DocumentWordsProvider vào worker CodeCompletion.
Information: Worker CodeCompletion từ module pyqode.core.backend. DocumentWordsProvider được sử dụng để cung cấp gợi ý hoàn thành từ cho tài liệu đã cho.
Solution:
from pyqode.core import backend
if __name__ == '__main__':
backend.CodeCompletionWorker.providers.append(backend.DocumentWordsProvider())
backend.serve_forever()

Note:
Script này được sử dụng để thêm DocumentWordsProvider vào worker CodeCompletion trong server đơn giản. Script được đóng băng bởi freeze_setup.py trên Windows.a

b Phân tích
- bước 1: kiểm tra Input:
1. Input phải là code và không thể chỉ chứa bình luận.
<trả lời: có, Input bao gồm cả code và bình luận, không chỉ bình luận >
- bước 2: kiểm tra Output:
1. Solution: Solution có liên quan đến instruction và information. Solution là giải quyết cụ thể cho instruction và information.
<trả lời: có>
2. Instruction: ngôn ngữ lập trình nên được chỉ định trong instruction.
<trả lời: có, instruction đề cập "Viết code Python," rõ ràng chỉ định ngôn ngữ lập trình là Python >
3. Solution: trong solution, chỉ nên chứa code và bình luận trong code. Không nên có giải thích nào được cung cấp bên ngoài code.
<trả lời: không, trong solution, hợp lý khi bao gồm code và bình luận trong code. Tuy nhiên, phần Note thuộc về giải thích được cung cấp bên ngoài code, điều này không được phép >
4. Instruction: Nội dung của instruction nên có liên quan đến Input và nên là một bản tóm tắt của nội dung Input, không có thông tin không liên quan bổ sung nào.
<trả lời: có, Instruction có liên quan đến Input >
-Trả lời tổng thể: không
- Lý do: Trong solution, hợp lý khi bao gồm code và bình luận trong code. Tuy nhiên, phần Note thuộc về giải thích được cung cấp bên ngoài code, điều này không được phép. Vì vậy trả lời tổng thể là không.c

Hình 4: Một ví dụ về khung Bộ Tạo-Bộ Phân biệt dựa trên LLM trên tác vụ tạo code. Bộ tạo tạo ra dữ liệu hướng dẫn dựa trên đầu vào (a). Tiếp theo, bộ phân biệt nhận đầu ra và tạo phân tích cho nó. Đầu ra (b) bao gồm bốn khóa, chúng tôi lấy information làm đầu vào và solution làm đầu ra trong tinh chỉnh hướng dẫn của chúng tôi. Phân tích (c) bao gồm lý do chi tiết cho từng quy tắc và trả lời tổng thể để kiểm tra xem mẫu có đáp ứng tất cả các yêu cầu hay không.

C So sánh với CodeAlpaca
Tập dữ liệu CodeAlpaca chứa 20K dữ liệu tuân theo hướng dẫn đa tác vụ được tạo bởi các kỹ thuật trong self-instruct (Taori et al., 2023). Để đảm bảo so sánh công bằng và đa chiều, chúng tôi lấy mẫu ngẫu nhiên 1K và 5K từ cả hai tập dữ liệu (CodeAlpaca và CodeSeaXDataset), đặt cùng một tập các tham số siêu huấn luyện (epoch = 3, learning rate = 1e-4, LoRA rank = 8) và sử dụng cùng prompt huấn luyện. Để ngăn ngừa overfitting, chúng tôi sử dụng Low-Rank Adaption (LoRA) (Hu et al., 2022) cho tinh chỉnh nếu kích thước của tập dữ liệu huấn luyện tuân theo hướng dẫn nhỏ hơn 5K và thực hiện tinh chỉnh đầy đủ trên toàn bộ tập dữ liệu 20K.

1) Sau khi được tinh chỉnh với 1K, 5K và 20K dữ liệu hướng dẫn tương ứng, hiệu suất của mô hình cơ sở cải thiện đáng kể trên HumanEval được thể hiện trong Hình 5. Lấy Starcoder làm mô hình cơ sở,

--- TRANG 12 ---
CodeSeaXDataset vượt trội so với CodeAlpaca (44.9% vs 41.7%, 45.7% vs 48.1% và 47.0% vs 50.5%) được thể hiện trong Hình 5 (a), điều này nhấn mạnh hiệu quả của phương pháp tinh chỉnh dữ liệu hướng dẫn của chúng tôi. Như được thể hiện trong Hình 5 (b), Kết quả của các mô hình cơ sở khác nhau trên CodeSeaXDataset vượt trội so với kết quả trên CodeAlpaca, điều này nhấn mạnh hiệu quả của tập dữ liệu CodeSeaXDataset trong việc tăng cường khả năng tuân theo hướng dẫn của mô hình cơ sở.

2) Theo Bảng 4 và Bảng 5, Tất cả các mô hình WaveCoder vượt trội đáng kể so với mô hình được tinh chỉnh với CodeAlpaca. Đáng chú ý, Điểm pass@1 của WaveCoder-CL-13B vượt trội so với CodeLLaMa-CodeAlpaca-13B đạt 10.6% cải thiện tuyệt đối trên HumanEvalExplain. Điều này nhấn mạnh hiệu quả của việc định nghĩa và phân loại các tác vụ liên quan đến code trong việc tăng cường khả năng tổng quát hóa của Code LLMs.

30354045505560
1K (LoRA) 5K (LoRA) 20K (Fully)41.741.745.745.74747
44.944.948.148.150.550.5

CodeAlpaca CodeOcean010203040506070
Starcoder CodeLLaMa-7B CodeLLaMa-13B DeepseekCoder-6.7B33.6 33.53649.447
3946.360.9
50.548.155.464

Mô hình Cơ sở CodeAlpaca CodeOceanpass@1
pass@1(a) HumanEval (Mô hình Cơ sở:Starcoder) (b) HumanEval (Các Mô hình Khác nhau)

Hình 5: So sánh với CodeAlpaca với kích thước tập dữ liệu khác nhau(a) và các mô hình cơ sở khác nhau(b). CodeSeaXDataset vượt trội so với CodeAlpaca trên HumanEval một cách đa chiều, phân tích chi tiết hơn được thể hiện trong Phần 3.3.

D Benchmark Đánh giá
HumanEval4, bao gồm 164 vấn đề lập trình Python được viết thủ công và trung bình 9.6 test case được phân bổ cho mỗi vấn đề hiện là benchmark được áp dụng rộng rãi nhất cho Code LLMs.

MBPP5 bao gồm khoảng 1,000 vấn đề lập trình Python được cộng đồng cung cấp, được thiết kế để có thể giải quyết bởi các lập trình viên cấp độ nhập môn, bao gồm các nguyên tắc cơ bản lập trình, chức năng thư viện tiêu chuẩn, và nhiều hơn nữa. Trong bài báo này, chúng tôi chọn tập dữ liệu kiểm tra 500 vấn đề để đánh giá cả suy luận few-shot của các mô hình được tinh chỉnh. Đối với những ai không có kết quả MBPP (500) được báo cáo hoặc không sử dụng, chúng tôi tái tạo cho họ bằng cách sử dụng bigcode-evaluation-harness6.

HumanEvalPack7 là sự mở rộng của HumanEval của OpenAI để bao gồm 6 ngôn ngữ tổng cộng trên 3 tác vụ. Trong bài báo này, chúng tôi chọn HumanEvalFix để đánh giá khả năng code to code đặc biệt trên tác vụ sửa chữa code và benchmark HumanEvalExplain để đánh giá khả năng code to text đặc biệt trên tác vụ tóm tắt code.

4https://huggingface.co/datasets/openai_humaneval
5https://huggingface.co/datasets/mbpp
6https://github.com/bigcode-project/bigcode-evaluation-harness
7https://huggingface.co/datasets/bigcode/humanevalpack

--- TRANG 13 ---
Khung Bộ Tạo-Bộ Phân biệt dựa trên LLM
Đầu vào:
Code Python tính diện tích hình tròn khi biết bán kính.

Đầu ra:
task_name: Tính Diện tích Hình tròn
instruction: Viết hàm Python tính diện tích hình tròn khi biết bán kính.
information: Công thức tính diện tích hình tròn là A = pi * r2, trong đó A là diện tích và r là bán kính.
solution:
import math
def area_of_circle(radius): return math.pi * radius ** 2

Phân tích: - Bước 1: Kiểm tra Instruction: Ngôn ngữ lập trình nên được chỉ định trong instruction. <trả lời: có, Instruction đề cập "Viết hàm Python," rõ ràng chỉ định ngôn ngữ lập trình là Python.>
- Bước 2: Kiểm tra Solution:
1. Solution có liên quan đến instruction và information. Solution là giải quyết cụ thể cho instruction và information. <trả lời: có, Solution được cung cấp tính diện tích hình tròn khi biết bán kính, có liên quan trực tiếp đến instruction và information được cung cấp.>
2. Trong solution, chỉ nên chứa code và bình luận trong code. Không nên có giải thích nào được cung cấp bên ngoài code. <trả lời: có, Solution chỉ chứa code mà không có giải thích nào bên ngoài code. Bình luận trong code không có mặt, nhưng code đơn giản và dễ hiểu.>
3. Code chứa logic thuật toán trong solution không nên thường xuyên bị chôn vùi bên trong các hàm phức tạp hoặc tài liệu kém, khiến chúng khó theo dõi hoặc học hỏi. <trả lời: có, Code chứa logic thuật toán trong solution không nên thường xuyên bị chôn vùi bên trong các hàm phức tạp hoặc tài liệu kém, khiến chúng khó theo dõi hoặc học hỏi.>
4. Code nên import các thư viện cần thiết. <trả lời: có, Code import thư viện math cần thiết, được yêu cầu để tính diện tích hình tròn sử dụng hằng số toán học pi.>
- Trả lời tổng thể: có
- Lý do: Tất cả các yêu cầu được đáp ứng theo các quy tắc đã cho. Ngôn ngữ lập trình được chỉ định, và solution có liên quan đến instruction và information. Code đơn giản, dễ theo dõi, và import thư viện cần thiết. Solution được cung cấp là một ví dụ tốt về code có cấu trúc tốt và ngắn gọn đáp ứng các đặc tả của tác vụ.

Bảng 7: Một ví dụ về khung Bộ Tạo-Bộ Phân biệt dựa trên LLM

--- TRANG 14 ---
HumanEval/1(code chính tắc):from typing import Listdef has_close_elements(numbers: List[float], threshold: float) -> bool:""" Kiểm tra xem trong danh sách số đã cho, có hai số nào gần nhau hơn ngưỡng đã cho không.>>> has_close_elements([1.0, 2.0, 3.0], 0.5)False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True"""for idx, elemin enumerate(numbers):for idx2, elem2 in enumerate(numbers):if idx!= idx2:distance = abs(elem-elem2)if distance < threshold:return Truereturn FalseĐây là hàm được viết bằng Python:```pythondef has_close_elements(list_of_numbers, threshold):for iin range(len(list_of_numbers)):for j in range(i+ 1, len(list_of_numbers)):if abs(list_of_numbers[i] -list_of_numbers[j]) < threshold:return Truereturn False```Độ tương tự Cosine: 94.3def closest_pair(arr, target): min_difference= 999999min_pair= (-1, -1)# Lặp qua tất cả các phần tử for iin range(len(arr)): # Lặp qua phần còn lại của các phần tử for j in range(i+ 1, len(arr)): diff = abs(arr[i] + arr[j] -target) # Kiểm tra sự khác biệt tối đa if diff < min_difference: min_difference= diff min_pair= (i, j) return min_pairĐộ tương tự Cosine: 87.5import mathdef find_closest_component(components):def _bounds(component):xmin, ymin, xmax, ymax= component["xmin"], component["ymin"],                   component["xmax"], component["ymax"]return math.sqrt(xmin**2 + ymin**2)return min(components, key=_bounds)Độ tương tự Cosine: 86.9Evol-codealpaca-v1
CodeAlpacaCodeOcean

HumanEval/101(code chính tắc):def make_a_pile(n):"""Cho số nguyên dương n, bạn phải tạo một đống gồm n mức đá.Mức đầu tiên có n viên đá.Số lượng đá ở mức tiếp theo là:-số lẻ tiếp theo nếu n là số lẻ.-số chẵn tiếp theo nếu n là số chẵn.Trả về số lượng đá trong mỗi mức trong một danh sách, trong đó phần tử tại chỉ số iđại diện cho số lượng đá trong mức (i+1).Ví dụ:>>> make_a_pile(3)[3, 5, 7]"""return [n + 2*ifor iin range(n)]def make_a_pile(n, pattern):if pattern == 'odd':p = [3]stone = 3for iin range(n-1):stone += 2p.append(stone)elifpattern == 'even':p = [3]stone = 3for iin range(n-1):stone += 2p.append(stone)return pprint(make_a_pile(3, 'odd'))print(make_a_pile(3, 'even'))Độ tương tự Cosine: 94.4def generate_list(n):"""Hàm này nhận số nguyên n và trả về danh sách có độ dài n các số từ 0 đến n-1."""return [ifor iin range(n)]Độ tương tự Cosine: 85.0def inventory_levels(arr)levels = Hash.new(0)arr.eachdo |type, quantity|levels[type] += quantityendreturn levelsendĐộ tương tự Cosine: 83.2Evol-codealpaca-v1
CodeAlpacaCodeOcean

Hình 6: Ví dụ về rò rỉ dữ liệu.
