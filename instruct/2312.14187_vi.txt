# 2312.14187.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2312.14187.pdf
# Kích thước tệp: 1466502 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
WaveCoder: Tăng Cường Rộng Rãi Và Đa Dạng Cho Các Mô Hình Ngôn Ngữ Lớn Về Mã Nguồn Bằng Điều Chỉnh Hướng Dẫn
Zhaojian Yu1∗Xin Zhang2†Ning Shang2Yangyu Huang2Can Xu2
Yishujie Zhao1∗Wenxiang Hu2Qiufeng Yin2
1Đại học Tsinghua
2Microsoft
yzj23@mails.tsinghua.edu.cn ,xinzhang3@microsoft.com
https://github.com/microsoft/WaveCoder
Tóm tắt
Nghiên cứu gần đây chứng minh rằng, sau khi điều chỉnh hướng dẫn, các Mô hình Ngôn ngữ Lớn về Mã nguồn (Code LLMs) có thể đạt được khả năng ấn tượng để giải quyết một loạt các nhiệm vụ liên quan đến mã nguồn. Tuy nhiên, các phương pháp điều chỉnh hướng dẫn hiện tại cho Code LLMs chủ yếu tập trung vào nhiệm vụ tạo mã truyền thống, dẫn đến hiệu suất kém trong các tình huống đa nhiệm vụ phức tạp. Trong bài báo này, chúng tôi tập trung vào nhiều nhiệm vụ liên quan đến mã nguồn và trình bày WaveCoder, một loạt Code LLMs được huấn luyện với dữ liệu hướng dẫn Tăng cường Rộng rãi Và Đa dạng. Để cho phép các mô hình giải quyết các nhiệm vụ phức tạp liên quan đến mã nguồn, chúng tôi đề xuất một phương pháp để tạo ra dữ liệu hướng dẫn đa dạng, chất lượng cao một cách ổn định từ bộ dữ liệu mã nguồn mở trong các tình huống đa nhiệm vụ và thu được CodeSeaXDataset, một bộ dữ liệu bao gồm 19.915 thể hiện hướng dẫn trên 4 nhiệm vụ liên quan đến mã nguồn, nhằm cải thiện khả năng tổng quát hóa của Code LLM. Các thí nghiệm của chúng tôi chứng minh rằng các mô hình WaveCoder vượt trội đáng kể so với các mô hình mã nguồn mở khác về khả năng tổng quát hóa trên các nhiệm vụ khác nhau liên quan đến mã nguồn. Hơn nữa, WaveCoder-Ultra-6.7B thể hiện khả năng tổng quát hóa tiên tiến nhất trên một loạt rộng các nhiệm vụ liên quan đến mã nguồn.

1 Giới thiệu
Gần đây, các Mô hình Ngôn ngữ Lớn (LLMs) như ChatGPT, GPT-4 (OpenAI, 2023), và Gemini1 đã đạt được mức hiệu suất chưa từng có trong một loạt rộng các nhiệm vụ NLP. Các mô hình này sử dụng quy trình tiền huấn luyện tự giám sát, và điều chỉnh có giám sát tiếp theo để thể hiện khả năng zero/few-shot đặc biệt, tuân theo hiệu quả các hướng dẫn của con người trên các nhiệm vụ khác nhau.

Đối với các nhiệm vụ liên quan đến mã nguồn, một số nghiên cứu trước đây, bao gồm Codex (Chen et al., 2021), StarCoder (Li

∗Công việc được thực hiện trong thời gian thực tập tại Microsoft.
†Tác giả liên hệ.
1https://deepmind.google/technologies/
geminiet al., 2023a), CodeLLaMa (Roziere et al., 2023) và DeepseekCoder (Guo et al., 2024), đã thành công chứng minh rằng tiền huấn luyện trên kho văn bản mã nguồn có thể cải thiện đáng kể khả năng của mô hình trong việc giải quyết các vấn đề liên quan đến mã nguồn. Sau quá trình tiền huấn luyện, điều chỉnh hướng dẫn (Wei et al., 2022; Aribandi et al., 2022; Chung et al., 2022) đã cho thấy hiệu quả trong việc cải thiện chất lượng phản hồi của LLM. Để đặc biệt tăng cường hiệu suất của Code LLMs trên các nhiệm vụ liên quan đến mã nguồn thông qua điều chỉnh hướng dẫn, nhiều phương pháp hiện có để tạo dữ liệu hướng dẫn đã được thiết kế. Ví dụ, Code Alpaca (Chaudhary, 2023) sử dụng phương pháp self-instruct (Wang et al., 2023a) trong lĩnh vực lập trình, tận dụng khả năng few-shot của teacher LLM để tạo dữ liệu hướng dẫn. Tương tự, WizardCoder (Luo et al., 2024) áp dụng phương pháp evol-instruct (Xu et al., 2024) dựa trên Code Alpaca, thể hiện một phương pháp mới và hiệu quả để tạo dữ liệu hướng dẫn. Các ứng dụng này nhấn mạnh tiềm năng của việc sử dụng teacher LLMs để sản xuất nội dung hướng dẫn hiệu quả, từ đó cung cấp một hướng để tạo dữ liệu hướng dẫn trong lĩnh vực mã nguồn.

Tuy nhiên, chất lượng dữ liệu mà chúng tạo ra phụ thuộc nhiều vào hiệu suất của teacher LLM và các hạt giống ban đầu hạn chế, thường tạo ra một lượng lớn các thể hiện hướng dẫn trùng lặp và giảm hiệu quả của điều chỉnh hướng dẫn (Xu et al., 2022; Yan et al., 2023; Lee et al., 2022). Để thoát khỏi sự phụ thuộc vào teacher LLMs, Octopack (Muennighoff et al., 2024) xây dựng một bộ dữ liệu hướng dẫn mã nguồn tận dụng cấu trúc tự nhiên của Git commits. Tuy nhiên, việc đảm bảo chất lượng dữ liệu trong thông điệp git là một thách thức đáng kể, và việc sàng lọc toàn diện dữ liệu thông qua các quy tắc lọc nhân tạo thường là một nhiệm vụ phức tạp. Ngoài ra, những nỗ lực này chủ yếu tập trung vào các nhiệm vụ tạo mã truyền thống và thiếu khả năng tạo ra hướng dẫn chi tiết, cụ thể theo nhiệm vụ trong các tình huống đa nhiệm vụ.arXiv:2312.14187v5  [cs.CL]  7 Jun 2024

--- TRANG 2 ---
Quy tắc được định nghĩa thủ công
Không gian Nhúng Mã nguồnTạo Tập lõi
Tập lõi Mã nguồn ThôThu thập dữ liệu thôTạo Dữ liệu Hướng dẫn
Bộ tạo LLM
Bộ phân biệt LLM
Tạo RaTạo TốtzéroÇít mẫuCơ sở dữ liệu Ví dụQuá trình HuấnnluyệnMô hình Ngôn ngữ Cơ sở
Tạo Tốt
CodeSeaXDatasetĐiều chỉnh hướng dẫnWaveCoderGiải quyết vấn đề
Bộ dữ liệu nền tảngĐịnh nghĩa nhiệm vụ:ØTạo mã nguồnØTóm tắt mã nguồnØ…
Quy tắc nhiệm vụ:ØKiểm tra đầu vàoØKiểm tra kết quả tạoØ…
Đầu vào:…Kết quả tạo:
A
BCài đặt bộ tạoCài đặt bộ phân biệt
C
Định dạng lại kết quả tạo tốt thành dữ liệu hướng dẫn
Mã nguồn Thô
A
B
C
Thu thập mã nguồn thôKcenterGreedy
D
DQuá trình huấn luyệnHình 1: Tổng quan về tăng cường rộng rãi và đa dạng cho Code LLM. Phần B và C chỉ ra Bộ tạo dựa trên LLM và Bộ phân biệt dựa trên LLM, trong đó bộ tạo có thể tận dụng các ví dụ khác nhau trong cơ sở dữ liệu ví dụ bằng học trong ngữ cảnh.

Trong bài báo này, chúng tôi chủ yếu tập trung vào nhiều nhiệm vụ liên quan đến mã nguồn, nhằm tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng phù hợp với các yêu cầu nhiệm vụ cụ thể. Để giải quyết các thách thức nêu trên, chúng tôi tinh chỉnh dữ liệu hướng dẫn bằng cách phân loại các thể hiện hướng dẫn thành bốn nhiệm vụ liên quan đến mã nguồn phổ quát trong CodeXGLUE (Lu et al., 2021): 1) Tóm tắt Mã nguồn, 2) Tạo Mã nguồn, 3) Dịch Mã nguồn, 4) Sửa chữa Mã nguồn và đề xuất một phương pháp tạo hướng dẫn tăng cường rộng rãi và đa dạng có thể tận dụng tối đa dữ liệu mã nguồn mở và tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng một cách ổn định trong các tình huống đa nhiệm vụ. Bằng chiến lược tạo này, chúng tôi thu được một bộ dữ liệu gồm 19.915 thể hiện hướng dẫn trên bốn nhiệm vụ liên quan đến mã nguồn, được gọi là CodeSeaXDataset.

Để xác nhận phương pháp của chúng tôi, chúng tôi huấn luyện StarCoder (Li et al., 2023a), CodeLLaMa (Roziere et al., 2023), và DeepseekCoder (Guo et al., 2024) với bộ dữ liệu CodeSeaXDataset ban đầu và có được WaveCoder. Sau một đánh giá kỹ lưỡng trên các chuẩn HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEvalPack (Muennighoff et al., 2024), kết quả thí nghiệm cho thấy WaveCoder của chúng tôi thể hiện khả năng tổng quát hóa xuất sắc dựa trên điều chỉnh hướng dẫn tăng cường rộng rãi và đa dạng. Hơn nữa, để khám phá thêm những cải tiến do chất lượng dữ liệu mang lại, chúng tôi sử dụng GPT-4 (OpenAI, 2023) để tái tạo phản hồi cho hướng dẫn trong CodeSeaXDataset. Được tinh chỉnh với bộ dữ liệu CodeSeaXDataset 20K được tăng cường, chúng tôi có được WaveCoder-Pro-6.7B đạt 72.0% pass@1 trên HumanEval (Chen et al., 2021) và vượt qua các Code LLMs mã nguồn mở nhưng vẫn đằng sau SoTA Code LLM. Kết hợp CodeSeaXDataset được tăng cường với WaveCoder-evol-codealpaca, bộ dữ liệu Magicoder-evol-codealpaca2 đã được khử nhiễm, chúng tôi trình bày WaveCoder-Ultra-6.7B, với khả năng tổng quát hóa SoTA trên nhiều nhiệm vụ liên quan đến mã nguồn.

2 CodeSeaXDataset: Dữ liệu Hướng dẫn Liên quan đến Mã nguồn Bốn nhiệm vụ
2.1 Chi tiết Nhiệm vụ
Dựa trên các nhiệm vụ liên quan đến mã nguồn từ CodeXGLUE (Lu et al., 2021), chúng tôi chọn bốn nhiệm vụ phổ quát và phổ biến nhất từ ba nhiệm vụ tạo sinh (code-to-text, text-to-code, và code-to-code) để khám phá thêm bao gồm Tóm tắt Mã nguồn, Tạo Mã nguồn, Dịch Mã nguồn, và Sửa chữa Mã nguồn. Mô tả chi tiết về các nhiệm vụ này có thể tìm thấy dưới đây.

Tóm tắt Mã nguồn (code-to-text). Nhiệm vụ này nhằm tạo một bản tóm tắt ngắn gọn về một đoạn mã đã cho. Mã nguồn thô được sử dụng làm đầu vào và phản hồi của mô hình teacher được tái cấu trúc thành định dạng hướng dẫn.

2https://huggingface.co/datasets/
ise-uiuc/Magicoder-evol-codealpaca-110K

--- TRANG 3 ---
Bảng 1: Tỷ lệ dữ liệu được tạo trong giai đoạn tạo.
Nhiệm vụ Số lượng Tỷ lệ(%) Lời nhắc
Tạo Mã nguồn 11370 57.1 Thực hiện các hàm thực hiện các phép toán cụ thể dựa trên đầu vào.
Tóm tắt Mã nguồn 3165 15.8 Viết tài liệu rõ ràng và súc tích cho mã nguồn đã cho.
Sửa chữa Mã nguồn 3144 15.8 Xác định và sửa lỗi trong mã nguồn đã cho.
Dịch Mã nguồn 2236 11.2 Viết lại mã nguồn đã cho từ một ngôn ngữ lập trình sang ngôn ngữ khác.

Bảng 2: Tỷ lệ ngôn ngữ lập trình trong mã nguồn thô.
Nhiệm vụ Tỷ lệ(%)
Python 29.44
PHP 21.34
Go 19.68
Java 18.53
JavaScript 5.56
Khác (Ruby,C++,C#) 5.45

Tạo Mã nguồn (text-to-code, code-to-code). Trong nhiệm vụ này, mô hình được mong đợi tạo mã dựa trên mô tả yêu cầu của người dùng. Do đó, mô hình teacher được mong đợi tạo hướng dẫn và mã giải pháp dựa trên mã nguồn thô như một cặp hướng dẫn-giải pháp. Mã giải pháp được tạo sau đó được coi là đầu ra.

Dịch Mã nguồn (code-to-code). Nhiệm vụ này liên quan đến việc chuyển đổi một ngôn ngữ lập trình sang ngôn ngữ khác. Lời nhắc cụ thể theo nhiệm vụ và mã nguồn thô được đưa cho mô hình teacher, sau đó mô hình tạo hướng dẫn và mã đã dịch.

Sửa chữa Mã nguồn (code-to-code). Mục đích của nhiệm vụ này là cung cấp mã đúng dựa trên các vấn đề tiềm ẩn trong mã đã cho. Mô hình teacher được mong đợi tạo giải pháp cho mã không chính xác, thường với mã đúng và một số mô tả, sau đó được lấy làm đầu ra.

2.2 Tạo Hướng dẫn Tăng cường Rộng rãi và Đa dạng

Trong các nghiên cứu trước đây (Zhou et al., 2023; Gupta et al., 2023), nhiều nhà nghiên cứu đã phát hiện ra rằng chất lượng và sự đa dạng của dữ liệu thường đóng vai trò quan trọng hơn trong quá trình điều chỉnh hướng dẫn so với lượng dữ liệu. Việc cải thiện chất lượng và sự đa dạng của dữ liệu có liên quan trực tiếp đến hiệu suất của LLM được tinh chỉnh. Do đó, để đảm bảo chất lượng và sự đa dạng của dữ liệu của thể hiện hướng dẫn, chúng tôi đề xuất một phương pháp tạo hướng dẫn tăng cường rộng rãi và đa dạng bao gồm hai phần sau: 1) một phương pháp có thể giữ lại sự đa dạng của dữ liệu hướng dẫn bằng cách giữ lại sự đa dạng của mã nguồn thô ở mức tối đa. 2) một khung Generator-Discriminator dựa trên LLM để tạo dữ liệu hướng dẫn chất lượng cao một cách ổn định.

2.2.1 Thu thập Mã nguồn Thô
Để đảm bảo chất lượng và sự đa dạng của mã nguồn thô, chúng tôi định nghĩa thủ công một số quy tắc lọc và sử dụng phương pháp phân cụm KCenterGreedy (Sener và Savarese, 2018; Chen et al., 2023) để có được bộ sưu tập mã nguồn thô từ bộ dữ liệu mã nguồn mở. Trong công việc này, chúng tôi chọn CodeSearchNet3, chứa 2 triệu cặp <comment, code> từ các thư viện mã nguồn mở được lưu trữ trên GitHub, làm bộ dữ liệu nền tảng của chúng tôi và xử lý nó với các bước sau:

Quy tắc lọc được định nghĩa thủ công. Để chọn mã chất lượng cao cho điều chỉnh hướng dẫn, chúng tôi đặt ra các quy tắc sau để lọc bộ dữ liệu nền tảng: i) Trong công việc này, chúng tôi đã lọc mã để đảm bảo rằng độ dài của mã yêu cầu không quá dài cũng không quá ngắn. ii) Theo Code Alpaca (Chaudhary, 2023), chúng tôi đã loại bỏ mã nguồn thô chứa các từ từ danh sách đen, có thể làm giảm hiệu suất của mô hình kết quả.

Phương pháp lựa chọn tập lõi. Để đảm bảo sự đa dạng của dữ liệu khi chọn các mẫu mã nguồn thô, chúng tôi sử dụng thuật toán KCenterGreedy (Sener và Savarese, 2018), đã được chứng minh hiệu quả trong việc thu được một tập hợp các mẫu lõi của một phân phối, để chọn các mẫu đại diện từ bộ dữ liệu mã nguồn mở dựa trên các nhúng mã được mã hóa bởi cùng một mô hình nhúng (roberta-large-v1 (Liu et al., 2019)).

Bằng cách kết hợp phương pháp như vậy vào bộ dữ liệu mã nguồn mở, sự đa dạng của dữ liệu được tạo không còn chỉ phụ thuộc vào khả năng của chính teacher LLM hoặc hạt giống ban đầu. Hơn nữa, do việc áp dụng thuật toán KCenterGreedy, sự đa dạng của các ngôn ngữ cũng được giữ lại đáng kể, như thể hiện trong Bảng 2.

3https://huggingface.co/datasets/code_
search_net

--- TRANG 4 ---
mã nguồn mở
Tạo raChuyển đổi/Viết lại mã nguồn đã cho từ một ngôn ngữ lập trình sang ngôn ngữ khác.Viết tài liệu rõ ràng và súc tích cho mã nguồn đã cho.Xác định và sửa lỗi trong mã nguồn đã cho.

LM Mỗi trường hợp được tạo cần được cung cấp các khóa sau:ØTên Nhiệm vụØHướng dẫnØĐầu vàoØĐầu raVới một số yêu cầu bạn nên tuân theo:1. Đầu ra là một giải pháp cụ thể giải quyết Hướng dẫn và Đầu vào; do đó, Đầu ra phải có liên quan đến cả Hướng dẫn và Đầu vào.2. Hướng dẫn nên là một hoặc hai câu.3. Trong Đầu ra, nó chỉ nên chứa mã. Không nên có giải thích nào được cung cấp bên ngoài mã....Mỗi trường hợp được tạo cần được cung cấp các khóa sau:ØTên Nhiệm vụØHướng dẫnØĐầu vàoØĐầu raVới một số yêu cầu bạn nên tuân theo:1. Đầu ra là một giải pháp cụ thể giải quyết Hướng dẫn và Đầu vào; do đó, Đầu ra phải có liên quan đến cả Hướng dẫn và Đầu vào.2. Hướng dẫn nên là một hoặc hai câu.3. Trong Đầu ra, nó chỉ nên chứa mã. Không nên có giải thích nào được cung cấp bên ngoài mã....Mỗi trường hợp được tạo cần được cung cấp các khóa sau:ØTên Nhiệm vụØHướng dẫnØThông tinØGiải phápVới một số yêu cầu bạn nên tuân theo:1. Đầu ra là một giải pháp cụ thể giải quyết Hướng dẫn và Đầu vào; do đó, Đầu ra phải có liên quan đến cả Hướng dẫn và Đầu vào.2. Hướng dẫn nên là một hoặc hai câu.3. Trong Đầu ra, nó chỉ nên chứa mã. Không nên có giải thích nào được cung cấp bên ngoài mã....ØTên Nhiệm vụØHướng dẫnØThông tinØGiải pháp

LM Phân tích:-bước 1: kiểm tra mã:1. Đầu vào nên là mã và không thể chỉ chứa các bình luận. -bước 2: kiểm tra Đầu ra:1. Giải pháp: Giải pháp có liên quan đến hướng dẫn và thông tin. Giải pháp là giải quyết cụ thể cho hướng dẫn và thông tin. 2. Hướng dẫn: ngôn ngữ lập trình nên được chỉ định trong hướng dẫn. 3. Giải pháp: trong giải pháp, nó chỉ nên chứa mã và các bình luận trong mã. Không nên có giải thích nào được cung cấp bên ngoài mã. 4. Hướng dẫn: Nội dung của hướng dẫn nên có liên quan đến Đầu vào và nên là một bản tóm tắt của nội dung Đầu vào, không có bất kỳ thông tin không liên quan nào khác. ...Được Mô hình TạoĐược Con người Viết

A
BGiai đoạn TạoGiai đoạn Phân biệtHình 2: Tổng quan về khung Generator-Discriminator dựa trên LLM của chúng tôi. Trong phần A, đầu ra của Generator bao gồm 4 khóa: Tên nhiệm vụ, Hướng dẫn, Thông tin, Giải pháp. Tất cả các khóa sẽ được phân tích trong Giai đoạn Phân biệt và phân tích có thể được tái sử dụng làm ví dụ trong lượt tiếp theo.

2.2.2 Khung Generator-Discriminator dựa trên LLM
Sau quá trình thu thập mã nguồn thô, sự đa dạng dữ liệu từ mã nguồn thô đã được giữ lại, nơi bước tiếp theo là tạo dữ liệu hướng dẫn cho điều chỉnh có giám sát từ mã nguồn thô. Để đảm bảo thêm chất lượng của dữ liệu hướng dẫn được tạo, như thể hiện trong Hình 2, chúng tôi đề xuất một khung Generator-Discriminator dựa trên LLM trong đó bộ tạo có thể tận dụng một lượng lớn mã nguồn mở không giám sát để tạo dữ liệu hướng dẫn có giám sát và bộ phân biệt có thể tạo phân tích cho từng thành phần trong dữ liệu hướng dẫn.

Giai đoạn Tạo. Trong giai đoạn tạo, chúng tôi sử dụng GPT-4 để tạo định nghĩa cho từng nhiệm vụ liên quan đến mã nguồn. Như thể hiện trong Hình 2, theo định nghĩa nhiệm vụ được tạo bởi mô hình, chúng tôi phát triển thủ công các yêu cầu tạo cho mỗi nhiệm vụ liên quan đến mã nguồn. Tích hợp cả định nghĩa nhiệm vụ và tất cả các yêu cầu liên quan vào lời nhắc tạo, chúng tôi lấy mã nguồn thô làm đầu vào và chọn các ví dụ khác nhau từ cơ sở dữ liệu ví dụ để tạo dữ liệu hướng dẫn bằng GPT-3.5.

Giai đoạn Phân biệt. Trong quá trình khám phá quy trình tạo hướng dẫn, chúng tôi nhận thấy rằng chất lượng dữ liệu của các thể hiện hướng dẫn không thể được đảm bảo chỉ thông qua giai đoạn tạo. Để tăng cường khả năng kiểm soát của việc tạo dữ liệu và đảm bảo thêm chất lượng dữ liệu, chúng tôi sử dụng GPT-4 làm bộ phân biệt dựa trên LLM để tiếp tục phân tích và lọc dữ liệu hướng dẫn. Tiếp theo, lấy cảm hứng từ Zero-shot-CoT (Kojima et al., 2022), chúng tôi thiết lập một loạt quy tắc, được minh họa trong Hình 4 và phân tích chúng thành một số chủ đề phụ để đảm bảo độ chính xác phân biệt, nơi bộ phân biệt dựa trên LLM có thể phân tích việc tạo từng bước. Bằng cách áp dụng phương pháp này, các quy tắc phân biệt có thể được sửa đổi một phần để giải quyết một số vấn đề nhất định. Sau quá trình phân biệt, như thể hiện trong Hình 1, mỗi thể hiện hướng dẫn được phân loại là trường hợp tốt hoặc xấu và thông tin phân loại sau đó được chọn ngẫu nhiên trong việc tạo tiếp theo làm ví dụ. Để tái sử dụng những thể hiện hướng dẫn được phân loại này, khác với self-instruct (Wang et al., 2023a) chỉ sử dụng nhiệm vụ hạt giống ban đầu làm ví dụ tốt, chúng tôi khai thác cả việc tạo tốt và việc tạo xấu làm ví dụ few-shot để bộ tạo có thể học từ sai lầm trong các ví dụ xấu khác nhau. Do đó, khung này cung cấp một phương pháp toàn diện để tạo và đánh giá dữ liệu hướng dẫn, đảm bảo một bộ dữ liệu huấn luyện chất lượng cao.

3 Thí nghiệm
3.1 Thiết lập
Khác với các nghiên cứu trước đây (Luo et al., 2024; Shen et al., 2023; Gunasekar et al., 2023) chủ yếu tập trung vào nhiệm vụ tạo mã, chúng tôi tạo khoảng 20K bộ dữ liệu bao gồm 4 nhiệm vụ phổ biến liên quan đến mã nguồn để tăng cường khả năng tổng quát hóa của Code LLM. Để có được các mô hình WaveCoder, Chúng tôi chọn StarCoder-15B, CodeLLaMa (7B và 13B), DeepseekCoder-6.7B làm mô hình cơ sở và tinh chỉnh tất cả các mô hình cơ sở trong 3 epochs sử dụng GPU NVIDIA A100-80GB. Đối với StarCoder-15B, CodeLLaMa-7B và CodeLLaMa-13B, chúng tôi đặt kích thước batch toàn cục là 256 sử dụng Tensor Parallel và đặt tốc độ học ban đầu ở 2e-5. Đối với DeepseekCoder-6.7B, chúng tôi đặt kích thước batch toàn cục là 512 sử dụng mô-đun Fully Sharded Data Parallel (FSDP) từ Pytorch và đặt tốc độ học ban đầu ở 5e-5.

--- TRANG 5 ---
Bảng 3: Kết quả pass@1 trên chuẩn HumanEval và MBPP. Chúng tôi sử dụng điểm tự báo cáo bất cứ khi nào có sẵn. Các từ viết tắt "CL", "SC", "DS" đề cập đến các mô hình cơ sở CodeLLaMa và StarCoder và DeepseekCoder, tương ứng. "WaveCoder-Pro-6.7B" và "WaveCoder-Ultra-6.7B" được chi tiết trong đoạn cuối của Phần 1. Do sự khác biệt trong chiến lược giải mã từ các công việc đánh giá trước đây, chúng tôi đánh dấu kết quả của greedy decoding bằng màu xanh và n = 200 mẫu bằng màu đỏ. -: Không được báo cáo trong bài báo của họ.

Mô hình Tham số Mô hình Cơ sở Dữ liệu InsT HumanEval MBPP (500)
Mô hình Độc quyền
GPT-4 - - - 85.4 / 67.0 -
ChatGPT - - - 73.2 / 48.1 52.2
Mô hình Mã nguồn Mở
StarCoder 15B - ✘ 33.6 43.3
OctoCoder 15B StarCoder 13K 46.2 43.5
WizardCoder 15B StarCoder 78K 57.3 51.8
WaveCoder-SC-15B
 15B StarCoder 20K 50.5 (+16.9) 51.0 (+7.4)
CodeLLaMa 7B - ✘ 33.5 41.4
CodeLLaMa-instruct 7B CodeLLaMa 14K 34.8 44.4
WaveCoder-CL-7B
 7B CodeLLaMa 20K 48.1 (+14.6) 47.2 (+5.8)
CodeLLaMa 13B - ✘ 36.0 47.0
CodeLLaMa-instruct 13B CodeLLaMa 14K 42.5 49.4
WaveCoder-CL-13B
 13B CodeLLaMa 20K 55.4 (+19.4) 49.6 (+2.6)
DeepseekCoder 6.7B - ✘ 49.4 60.6
Magicoder-DS 6.7B DeepseekCoder 75K 66.5 60.4
WaveCoder-DS-6.7B
 6.7B DeepseekCoder 20K 64.0 (+14.6) 62.8 (+2.2)
WaveCoder-Pro-6.7B
 6.7B DeepseekCoder 20K 72.0 (+22.6) 63.6 (+3.0)
Mô hình Mã nguồn Mở SoTA
DeepseekCoder-instruct∗6.7B DeepseekCoder - 73.8 62.8
Magicoder-S-DS 6.7B DeepseekCoder 185K 76.8 64.6
WaveCoder-Ultra-6.7B
 6.7B DeepseekCoder 130K 78.6 (+29.2) 64.4 (+3.8)

Chuẩn và Đường cơ sở. Để đảm bảo đánh giá kỹ lưỡng về khả năng tổng quát hóa của mô hình, chúng tôi chấm điểm mô hình của chúng tôi trên ba chuẩn mã trên các nhiệm vụ khác nhau liên quan đến mã: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) và HumanEvalPack (Muennighoff et al., 2024), như được minh họa trong Phụ lục D.

Mô hình Độc quyền. Chúng tôi trình bày các kết quả tự báo cáo từ một loạt các SoTA LLMs, bao gồm ChatGPT (gpt-3.5-turbo), GPT-4. Nếu không được báo cáo, chúng tôi sử dụng kết quả từ Octopack (Muennighoff et al., 2024) hoặc tự đánh giá.

Mô hình Mã nguồn Mở. Để đảm bảo so sánh công bằng, chúng tôi đã chọn các mô hình đã được huấn luyện với số lượng thể hiện hướng dẫn tương tự cho phân tích so sánh của chúng tôi.

Mô hình Mã nguồn Mở SoTA. Chúng tôi so sánh WaveCoder-6.7B với Code LLM mã nguồn mở SoTA, bao gồm Magicoder-S-DS (Wei et al., 2023) và DeepseekCoder-instruct-6.7B (Wei et al., 2023) trên một loạt rộng các nhiệm vụ liên quan đến mã. Tất cả kết quả của các mô hình mã nguồn mở SoTA được trình bày từ EvalPlus. (Liu et al., 2023) Nếu không được báo cáo, chúng tôi tự đánh giá.

3.2 Kết quả
Đánh giá trên Nhiệm vụ Tạo Mã. HumanEval và MBPP là hai chuẩn đại diện cho nhiệm vụ tạo mã, như được minh họa trong Phụ lục D. Bảng 3 cho thấy điểm pass@1 của các LLMs khác nhau trên cả hai chuẩn. Từ kết quả, Chúng tôi có những quan sát sau:

1) WaveCoder-Pro-6.7B vượt trội so với các mô hình mã nguồn mở khác chỉ với 6.7B tham số và 20K dữ liệu hướng dẫn. Được huấn luyện với bộ dữ liệu CodeSeaXDataset được tăng cường bởi GPT-4, WaveCoder-Pro-6.7B đạt 72.0% pass@1 trên HumanEval và 63.6% trên MBPP, vượt qua tất cả các mô hình mã nguồn mở nhưng vẫn đằng sau các mô hình độc quyền và các mô hình mã nguồn mở SoTA.

2) Dữ liệu hướng dẫn được tinh chỉnh và đa dạng có thể cải thiện đáng kể hiệu quả của điều chỉnh hướng dẫn. Như được mô tả trong Bảng 3, WaveCoder thể hiện hiệu suất đáng khen ngợi, sử dụng một bộ dữ liệu chỉ bao gồm khoảng 20K Dữ liệu Điều chỉnh Hướng dẫn (InsT Data), định vị nó trên một nền tảng bình đẳng

--- TRANG 6 ---
Bảng 4: Kết quả pass@1 trên chuẩn HumanEvalFix. Chúng tôi sử dụng điểm tự báo cáo bất cứ khi nào có sẵn. Do sự khác biệt trong chiến lược giải mã từ các công việc đánh giá trước đây, chúng tôi đánh dấu kết quả của greedy decoding bằng màu xanh và n = 20 mẫu bằng màu đỏ.

Mô hình Python JavaScript Java Go C++ Rust Trung bình
GPT-4 47.0 48.2 50.0 50.6 47.6 43.3 47.8
StarCoder 8.7 15.7 13.3 20.1 15.6 6.7 13.4
OctoCoder 30.4 28.4 30.6 30.2 26.1 16.5 27.0
WizardCoder 31.8 29.5 30.7 30.4 18.7 13.0 25.7
WaveCoder-SC-15B
 39.3 35.1 34.8 36.2 30.2 22.5 33.0
CodeLLaMa-instruct-7B 28.0 23.2 23.2 18.3 0.1 0.1 15.5
CodeLLaMa-CodeAlpaca-7B 37.8 39.0 42.0 37.8 37.2 29.2 37.1
WaveCoder-CL-7B
 41.4 41.4 42.0 47.1 42.7 34.7 41.5
CodeLLaMa-instruct-13B 29.2 19.5 32.3 24.4 12.8 0.1 19.7
CodeLLaMa-CodeAlpaca-13B 42.7 43.9 50.0 45.7 39.6 37.2 43.2
WaveCoder-CL-13B
 48.8 48.2 50.6 51.8 45.1 40.2 47.4
DeepseekCoder-6.7B 29.9 29.2 39.0 29.2 25.0 21.9 29.0
Magicoder-DS 42.0 43.3 50.6 41.4 38.4 29.2 40.8
DeepseekCoder-CodeAlpaca-6.7B 49.4 51.8 45.1 48.8 44.5 31.7 45.2
WaveCoder-DS-6.7B
 57.9 52.4 57.3 47.5 45.1 36.0 49.4
WaveCoder-Pro-6.7B
 59.1 56.7 54.2 45.1 45.7 34.1 49.2
Deepseek-instruct-6.7B 56.1 58.5 57.3 49.4 45.1 36.6 50.5
Magicoder-S-DS 56.1 55.4 58.5 51.2 45.7 35.3 50.3
WaveCoder-Ultra-6.7B
 58.5 57.3 61.0 53.0 50.0 37.2 52.8

với các đồng đại của nó. Mặc dù có một thiếu hụt rõ ràng trong các chuẩn tạo mã so với WizardCoder (50.5 vs 57.3) và Magicoder (64.0 vs 66.5), điều quan trọng là phải xem xét sự chênh lệch đáng kể về khối lượng dữ liệu huấn luyện. Hơn nữa, được quan sát rằng WaveCoder-pro-6.7B vượt trội đáng kể so với Magicoder-DS-6.7B (72.0 vs 66.5), chứng minh hiệu quả của chất lượng và sự đa dạng dữ liệu trong điều chỉnh hướng dẫn.

Đánh giá trên Nhiệm vụ Khác Liên quan đến Mã. Chúng tôi chấm điểm WaveCoder với các Code LLMs tiên tiến trên HumanEvalPack (Muennighoff et al., 2024) trong Bảng 4 và Bảng 5, làm nổi bật những quan sát đáng chú ý sau:

1) Các mô hình WaveCoder vượt trội so với tất cả các mô hình mã nguồn mở trên nhiệm vụ khác liên quan đến mã. Dựa trên Starcoder, WaveCoder-SC được đề xuất của chúng tôi đã thể hiện hiệu suất đặc biệt, vượt qua khả năng của cả WizardCoder và OctoCoder như được chứng minh bởi các chuẩn HumanEvalFix (33.0 vs 25.7 vs 27.0) và HumanEvalExplain (30.8 vs 27.5 vs 24.5), điều này cũng được thể hiện trong các mô hình cơ sở khác. Đáng chú ý, WaveCoder-DS-6.7B đạt 49.4% điểm pass@1 trung bình trên HumanEvalFix và 41.3% trên HumanEvalExplain, vượt qua tất cả các mô hình mã nguồn mở và thể hiện khả năng tổng quát hóa mạnh trong các tình huống đa nhiệm vụ.

2) Việc tăng cường trong tinh chỉnh và đa dạng hóa dữ liệu có thể tăng cường đáng kể hiệu quả của điều chỉnh hướng dẫn trong các tình huống đa nhiệm vụ. Việc tinh chỉnh dữ liệu như vậy, kết hợp với việc phân loại hướng dẫn thành bốn nhiệm vụ liên quan đến mã, đã đẩy các mô hình của chúng tôi đến khả năng tổng quát hóa chưa từng thấy trong các nhiệm vụ khác nhau liên quan đến mã. Đáng chú ý, mô hình WaveCoder-DS-6.7B của chúng tôi vượt trội so với GPT-4 (49.4 vs 47.8) trên HumanEvalFix, từ đó nhấn mạnh tiềm năng của các mô hình nhỏ hơn để đạt được sự tương đương gần với các mô hình có nhiều tham số khi được tối ưu hóa hiệu quả.

WaveCoder-Ultra-6.7B. Lấy cảm hứng từ Magicoder-S-DS-6.7B (Wei et al., 2023), chúng tôi kết hợp CodeSeaXDataset với WaveCoder-evol-codealpaca thành một bộ dữ liệu 130K. Được tinh chỉnh với sự kết hợp của hai bộ dữ liệu này, chúng tôi có được WaveCoder-Ultra-6.7B. Như được minh họa trong Bảng 3, 4, 5, WaveCoder-Ultra-6.7B có khả năng tổng quát hóa tiên tiến nhất trên một loạt rộng các nhiệm vụ liên quan đến mã, điều này làm nổi bật tầm quan trọng của bộ dữ liệu CodeSeaXDataset của chúng tôi một lần nữa và chứng minh tiềm năng của các bộ dữ liệu lớn hơn.

4 Nghiên cứu Ablation và Phân tích
4.1 Nghiên cứu Ablation về Các Nhiệm vụ Liên quan đến Mã
Để khám phá mối quan hệ giữa các nhiệm vụ khác nhau, chúng tôi tiến hành một nghiên cứu ablation về loại nhiệm vụ của dữ liệu hướng dẫn. Sử dụng DeepseekCoder-Base-

--- TRANG 7 ---
Bảng 5: Kết quả pass@1 trên chuẩn HumanEvalExplain. Chúng tôi sử dụng điểm tự báo cáo bất cứ khi nào có sẵn. Do sự khác biệt trong chiến lược giải mã từ các công việc đánh giá trước đây, chúng tôi đánh dấu kết quả của greedy decoding bằng màu xanh và n = 20 mẫu bằng màu đỏ.

Mô hình Python JavaScript Java Go C++ Rust Trung bình
GPT-4 64.6 57.3 51.2 58.5 38.4 42.7 52.1
StarCoder 0.0 0.0 0.0 0.0 0.0 0.0 0.0
WizardCoder 32.5 33.0 27.4 26.7 28.2 16.9 27.5
OctoCoder 35.1 24.5 27.3 21.1 24.1 14.8 24.5
WaveCoder-SC-15B
 37.1 33.3 40.5 23.3 31.8 19.3 30.8
CodeLLaMa-instruct-7B 33.5 36.0 31.7 21.3 25.0 16.4 27.3
CodeLLaMa-CodeAlpaca-7B 34.7 24.4 37.8 23.2 28.6 19.5 28.0
WaveCoder-CL-7B
 41.4 31.7 39.0 25.0 34.1 23.2 32.4
CodeLLaMa-instruct-13B 40.2 26.8 37.2 22.5 28.0 14.6 28.2
CodeLLaMa-CodeAlpaca-13B 32.3 28.0 34.1 18.9 29.9 20.7 27.3
WaveCoder-CL-13B
 45.7 42.0 48.2 32.3 38.4 20.7 37.9
DeepseekCoder-6.7B 43.9 40.2 37.8 29.2 34.1 22.5 34.6
Deepseek-CodeAlpaca-6.7B 40.8 37.2 42.1 29.9 31.7 22.5 34.0
Magicoder-DS 55.5 36.6 49.4 36.0 39.6 27.4 40.7
WaveCoder-DS-6.7B
 48.2 47.5 49.4 32.3 48.2 22.0 41.3
WaveCoder-Pro-6.7B
 53.0 43.3 54.9 34.1 42.7 20.0 41.3
Magicoder-S-DS 60.3 46.3 54.3 38.4 48.1 29.2 46.1
Deepseek-instruct-6.7B 62.2 54.3 61.0 39.6 55.5 33.5 51.0
WaveCoder-Ultra-6.7B
 56.7 50.0 54.3 34.8 51.2 36.6 47.3

Bảng 6: Nghiên cứu ablation trên các nhiệm vụ khác nhau liên quan đến mã: CG (Tạo Mã), CS (Tóm tắt Mã), CT (Dịch Mã), CR (Sửa chữa Mã). WaveCoder-DS-6.7B sử dụng tất cả 4 nhiệm vụ liên quan đến mã.

Mô hình CG CS CT CR HumanEvalHumanEval
Fix (Trung bình)HumanEval
Explain (Trung bình)
DeepseekCoder-Base-6.7B ✘ ✘ ✘ ✘ 49.4 29.0 34.6
WaveCoder-DS-6.7B
 ✔ ✔✔ ✔ 64.0 (+14.6) 49.4 (+20.4) 41.3 (+7.3)
-Không có Sửa chữa ✔ ✔ ✔ ✘ 60.9 (-3.1) 15.7 (-33.7) 41.2 (-0.1)
-Không có Tạo ✘ ✔ ✔ ✔ 53.6 (-10.4) 47.4 (-2.0) 40.5 (-0.8)
-Không có Dịch ✔ ✔ ✘ ✔ 60.9 (-3.1) 49.3 (-0.1) 41.6 (+0.3)
-Không có Tóm tắt ✔ ✘ ✔ ✔ 61.5 (-2.5) 45.6 (-3.8) 28.4 (-12.9)

6.7B làm mô hình cơ sở và dữ liệu CodeSeaXDataset 20K ban đầu làm bộ dữ liệu cơ sở, chúng tôi có những quan sát sau từ Bảng 6:

1) Dữ liệu hướng dẫn được tinh chỉnh có thể cải thiện đáng kể khả năng tổng quát hóa của các mô hình được tiền huấn luyện mà không có sự đánh đổi. Như thể hiện trong Bảng 6, kết hợp tất cả 4 nhiệm vụ liên quan đến mã vào dữ liệu huấn luyện, WaveCoder-DS-6.7B đạt hiệu suất tốt nhất trên chuẩn của tất cả các nhiệm vụ. Ví dụ, sự tham gia của nhiệm vụ Sửa chữa Mã mang lại cải thiện tuyệt đối đáng kể 33.7% cho HumanEvalFix mà không có bất kỳ sự giảm sút đáng kể nào trong các nhiệm vụ khác, và thậm chí cải thiện 3.1% tuyệt đối cho chuẩn HumanEval.

2) Các nhiệm vụ khác nhau có thể thúc đẩy lẫn nhau để mô hình có thể thể hiện khả năng tổng quát hóa. Từ Bảng 6, chúng ta có thể quan sát rằng bất kỳ sự kết hợp nào của ba nhiệm vụ đều dẫn đến điểm số thấp hơn so với tất cả các nhiệm vụ. Ví dụ, việc bổ sung nhiệm vụ tóm tắt mã cung cấp một cải thiện khiêm tốn nhưng đáng kể trung bình trên tất cả các chuẩn. Hơn nữa, việc vắng mặt của bất kỳ nhiệm vụ nào sẽ khiến điểm số của HumanEval giảm, điều này cũng phản ánh sự thúc đẩy lẫn nhau giữa các nhiệm vụ khác nhau.

4.2 Thảo luận về Rò rỉ Dữ liệu
Trong phần này, chúng tôi khám phá rò rỉ tiềm ẩn thông qua ba bộ dữ liệu hướng dẫn về mã (tức là Code Alpaca, CodeSeaXDataset, Magicoder-evol-codealpaca). Để đảm bảo phân tích chính xác, chúng tôi sử dụng mô hình nhúng SoTA GTE-Large (Li et al., 2023b) để mã hóa mã chuẩn trong các chuẩn kiểm tra và tất cả mã trong tập huấn luyện. Tiếp theo, chúng tôi tìm láng giềng gần nhất trong tập huấn luyện cho mỗi câu hỏi trong chuẩn kiểm tra. Như được minh họa trong Hình 3, CodeSeaXDataset có độ tương tự cosine trung bình thấp hơn

--- TRANG 8 ---
CodeSeaXDataset(Trung bình:0.88)CodeSeaXDataset
CodeSeaXCodeSeaXCodeSeaXDataset(Trung bình:0.877)CodeSeaXDatasetHình 3: Thảo luận về rò rỉ dữ liệu trong các bộ dữ liệu huấn luyện khác nhau. WaveCoder-evol-codealpaca chỉ ra bộ dữ liệu Magicoder-evol-codealpaca đã được khử nhiễm theo chiến lược của chúng tôi.

so với các bộ dữ liệu khác. Hình 6 trong Phụ lục trình bày hai ví dụ về rò rỉ dữ liệu trong các tập huấn luyện này. Hơn nữa, chúng tôi phân tích tất cả các chuẩn và nhận thấy vấn đề rò rỉ dữ liệu nghiêm trọng giữa HumanEval và bộ dữ liệu Magicoder-evol-codealpaca. Do đó, chúng tôi khử nhiễm Magicoder-evol-codealpaca cho mỗi vấn đề đánh giá trong HumanEval và có được WaveCoder-evol-codealpaca. Như được minh họa trong Hình 3, WaveCoder-evol-codealpaca có độ tương tự thấp hơn so với Magicoder-evol-codealpaca.

5 Nghiên cứu Liên quan
Điều chỉnh Hướng dẫn. Các nghiên cứu gần đây, như FLAN (Wei et al., 2022), ExT5 (Aribandi et al., 2022), và FLANT5 (Chung et al., 2022), đã nhấn mạnh hiệu quả của việc tích hợp các nhiệm vụ đa dạng trong quá trình huấn luyện để tăng cường khả năng thích ứng của các mô hình được tiền huấn luyện cho các nhiệm vụ xuôi dòng. Cụ thể, điều chỉnh hướng dẫn của Flan-PaLM 540B (Chung et al., 2022) trên 1.8K nhiệm vụ đã chứng minh rằng một bộ dữ liệu hướng dẫn tăng cường rộng rãi và đa dạng cải thiện đáng kể hiệu suất mô hình ngôn ngữ. InstructGPT (Ouyang et al., 2022), với việc kết hợp dữ liệu hướng dẫn cao cấp được tạo bởi các chú thích viên con người, đã cho thấy triển vọng đáng kể trong việc căn chỉnh đầu ra mô hình với ý định người dùng, thúc đẩy nghiên cứu thêm về các cơ chế điều chỉnh hướng dẫn. Ngoài ra, Stanford Alpaca (Taori et al., 2023) đã sáng tạo sử dụng dữ liệu hướng dẫn được tạo bởi GPT thông qua self-instruct (Wang et al., 2023a) cho quá trình điều chỉnh hướng dẫn. WizardLM (Xu et al., 2024) đã xây dựng dựa trên những tiến bộ này bằng cách áp dụng phương pháp evol-instruct, cùng nhau làm sáng tỏ tác động biến đổi của điều chỉnh hướng dẫn đối với khả năng tổng thể của LLM.

Mô hình Ngôn ngữ Lớn về Mã. Những tiến bộ gần đây trong tạo mã đã được thúc đẩy bởi các Code LLMs như CodeGen (Nijkamp et al., 2022), CodeT5 (Wang et al., 2021), StarCoder (Li et al., 2023a), CodeLLaMa (Roziere et al., 2023) và Deepseek-Coder (Guo et al., 2024), được hưởng lợi từ tiền huấn luyện mở rộng trên kho văn bản mã rộng lớn. Những nỗ lực để tăng cường hiệu quả và khả năng giải quyết vấn đề đã dẫn đến việc phát triển các mô hình được điều chỉnh hướng dẫn như InstructCodeT5+ (Wang et al., 2023b), WizardCoder (Luo et al., 2024), Pangu-coder2 (Shen et al., 2023), Tuy nhiên, tất cả dữ liệu hướng dẫn mà họ sử dụng đều từ Code Alpaca, không được tinh chỉnh đủ trong bối cảnh môi trường đa nhiệm vụ, điều này thúc đẩy chúng tôi đề xuất các phương pháp mới để tạo dữ liệu hướng dẫn. Đồng thời, với việc phát hành công việc đồng thời của chúng tôi Magicoder (Wei et al., 2023), chúng tôi cung cấp một phân tích súc tích trong Phần 3.

6 Kết luận
Bài báo này trình bày WaveCoder, một Code LLM được tinh chỉnh với dữ liệu hướng dẫn tăng cường rộng rãi và đa dạng. Bằng cách cho phép các mô hình ngôn ngữ giải quyết hiệu quả các nhiệm vụ phức tạp liên quan đến mã, phương pháp của chúng tôi chứng minh tiềm năng của việc tích hợp nhiều nhiệm vụ liên quan đến mã vào điều chỉnh hướng dẫn cho Code LLM và tạo dữ liệu hướng dẫn chất lượng cao và đa dạng cho các yêu cầu nhiệm vụ cụ thể trong các tình huống đa nhiệm vụ. WaveCoder đạt hiệu suất tổng quát hóa tiên tiến trên các nhiệm vụ khác nhau liên quan đến mã vượt qua các Code LLMs mã nguồn mở hiện có. Hơn nữa, phân tích của chúng tôi về mối quan hệ của các nhiệm vụ khác nhau cung cấp những hiểu biết có giá trị cho nghiên cứu tương lai, mở đường cho các nhiệm vụ liên quan đến mã rộng hơn và bộ dữ liệu lớn hơn.

--- TRANG 9 ---
Hạn chế
Chúng tôi trình bày WaveCoder và đề xuất một phương pháp tạo dữ liệu có thể tạo ra dữ liệu hướng dẫn chất lượng cao và đa dạng một cách ổn định từ bộ dữ liệu mã nguồn mở trong tình huống đa nhiệm vụ. Một hạn chế của công việc của chúng tôi là bộ dữ liệu huấn luyện mà chúng tôi sử dụng chỉ bao gồm 19.915 hướng dẫn, tạo ra những cải thiện hạn chế cho mô hình. Như được minh họa trong Phần 3, chúng tôi mở rộng bộ dữ liệu huấn luyện thành một lượng lớn hơn và mô hình kết quả vẫn có cải thiện đáng kể. Do đó, công việc tương lai nên tập trung vào nhiều loại nhiệm vụ liên quan đến mã hơn và bộ dữ liệu lớn hơn.

Tuyên bố Đạo đức
Chúng tôi xây dựng bộ dữ liệu CodeSeaXDataset từ mã nguồn mở. Đối với mỗi đoạn mã chúng tôi sử dụng, chúng tôi cam kết tuân thủ các điều khoản của giấy phép của nó, bao gồm việc ghi nhận thích hợp đảm bảo rằng bất kỳ sửa đổi hoặc tác phẩm phái sinh nào cũng được chia sẻ dưới các điều khoản tương thích. Hơn nữa, chúng tôi nhận thấy vấn đề rò rỉ dữ liệu nghiêm trọng trong bộ dữ liệu Magicoder-evol-codealpaca. Để đảm bảo so sánh công bằng, chúng tôi loại bỏ ba láng giềng gần nhất của mỗi câu hỏi trong chuẩn kiểm tra khỏi tập huấn luyện. Tuy nhiên, nếu tất cả các mẫu tương tự vô tình bị loại bỏ, tính toàn vẹn của dữ liệu sẽ bị tổn hại, điều này có hại cho việc huấn luyện mô hình. Do đó, hiện tượng này nên được quy cho thực tế là các vấn đề trong các chuẩn kiểm tra hiện tại là một số logic thuật toán cơ bản nhất. Để đạt được điều này, chúng tôi kêu gọi các chuẩn kiểm tra toàn diện và phức tạp hơn cho Code LLMs sẽ không dễ dàng gây ra vấn đề rò rỉ dữ liệu.

Tài liệu tham khảo
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,
Jai Gupta, Kai Hui, Sebastian Ruder, và Donald
Metzler. 2022. Ext5: Towards extreme multi-task
scaling for transfer learning. In International Confer-
ence on Learning Representations (ICLR) .

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .

Sahil Chaudhary. 2023. Code alpaca: An
instruction-following llama model for code genera-
tion. https://github.com/sahil280114/
codealpaca .

Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xi-
aomeng Hu, Xuetao Ma, Yifan Yanggong, và Junbo
Zhao. 2023. Maybe only 0.5% data is needed: A pre-
liminary exploration of low training data instruction
tuning. arXiv preprint arXiv:2305.09246 .

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .

Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all
you need. arXiv preprint arXiv:2306.11644 .

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y Wu, YK Li, et al. 2024. Deepseek-coder: When the
large language model meets programming–the rise of
code intelligence. arXiv preprint arXiv:2401.14196 .

Himanshu Gupta, Saurabh Arjun Sawant, Swaroop
Mishra, Mutsumi Nakamura, Arindam Mitra, San-
tosh Mashetty, và Chitta Baral. 2023. Instruction
tuned models are quick learners. arXiv preprint
arXiv:2306.05539 .

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations (ICLR) .

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, và Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances
in neural information processing systems (NIPS) ,
35:22199–22213.

Katherine Lee, Daphne Ippolito, Andrew Nystrom,
Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
và Nicholas Carlini. 2022. Deduplicating training
data makes language models better. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 8424–8445.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.
2023a. Starcoder: may the source be with you!
arXiv preprint arXiv:2305.06161 .

Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, và Meishan Zhang. 2023b. Towards
general text embeddings with multi-stage contrastive
learning. arXiv preprint arXiv:2308.03281 .

--- TRANG 10 ---
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, và
LINGMING ZHANG. 2023. Is your code gener-
ated by chatGPT really correct? rigorous evalua-
tion of large language models for code generation.
InThirty-seventh Conference on Neural Information
Processing Systems .

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, và Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
Codexglue: A machine learning benchmark dataset
for code understanding and generation. In Thirty-
fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1) .

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, và Daxin Jiang. 2024. Wizardcoder:
Empowering code large language models with evol-
instruct. International Conference on Learning Rep-
resentations (ICLR) .

Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam
Singh, Xiangru Tang, Leandro von Werra, và
Shayne Longpre. 2024. Octopack: Instruction tuning
code large language models. International Confer-
ence on Learning Representations (ICLR) .

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, và Caiming
Xiong. 2022. Codegen: An open large language
model for code with multi-turn program synthesis.
arXiv preprint arXiv:2203.13474 .

OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems (NIPS) , 35:27730–
27744.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .

Ozan Sener và Silvio Savarese. 2018. Active learn-
ing for convolutional neural networks: A core-set
approach. In International Conference on Learning
Representations (ICLR) .

Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,
Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan
Ji, Jingyang Zhao, et al. 2023. Pangu-coder2: Boost-
ing large language models for code with ranking feed-
back. arXiv preprint arXiv:2307.14936 .

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, và Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/
stanford_alpaca .

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh
Hajishirzi. 2023a. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (ACL) (Volume 1: Long
Papers) , pages 13484–13508.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare,
Nghi D.Q. Bui, Junnan Li, và Steven C. H. Hoi.
2023b. Codet5+: Open code large language mod-
els for code understanding and generation. arXiv
preprint .

Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH
Hoi. 2021. Codet5: Identifier-aware unified pre-
trained encoder-decoder models for code understand-
ing and generation. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 8696–8708.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, và Quoc V Le. 2022. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations (ICLR) .

Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, và
Lingming Zhang. 2023. Magicoder: Source code is
all you need. arXiv preprint arXiv:2312.02120 .

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin
Jiang. 2024. Wizardlm: Empowering large language
models to follow complex instructions. International
Conference on Learning Representations (ICLR) .

Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang
Li, và Jian Li. 2022. Learning to break the loop:
Analyzing and mitigating repetitions for neural text
generation. Advances in Neural Information Process-
ing Systems (NIPS) , 35:3082–3095.

Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu,
Yafu Li, và Yue Zhang. 2023. Understanding in-
context learning from repetitions. arXiv preprint
arXiv:2310.00297 .

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .

--- TRANG 11 ---
A Lời nhắc
Theo Alpaca (Taori et al., 2023), chúng tôi đặt lời nhắc tinh chỉnh như sau:

Lời nhắc với Đầu vào:
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với một đầu vào cung cấp ngữ cảnh thêm.
Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:{instruction}
### Đầu vào:{input}
### Phản hồi:

Lời nhắc không có Đầu vào:
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với một đầu vào cung cấp ngữ cảnh thêm.
Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:{instruction}
### Phản hồi:

B Ví dụ về khung Generator-Discriminator dựa trên LLM

Mã nguồn Thô
#!/usr/bin/env python
# -*- coding: utf -8 -*-
"""
Máy chủ đơn giản thêm DocumentWordsProvider vào
worker CodeCompletion.
Trên Windows, script này được đóng băng bởi freeze_setup.py (cx_Freeze).
"""
from pyqode.core import backend
if __name__ == '__main__':
 backend.CodeCompletionWorker.providers.append (backend.DocumentWordsProvider ())
 backend.serve_forever ()

Tạo
Task_name : Thêm DocumentWordsProvider vào worker CodeCompletion
Instruction: Viết mã Python thêm DocumentWordsProvider vào worker CodeCompletion.
Information: Worker CodeCompletion từ mô-đun pyqode.core.backend. DocumentWordsProvider được sử dụng để cung cấp gợi ý hoàn thành từ cho một tài liệu nhất định.
Solution:
from pyqode.core import backend
if __name__ == '__main__':
 backend.CodeCompletionWorker.providers.append (backend.DocumentWordsProvider ())
 backend.serve_forever ()

Note:
Script này được sử dụng để thêm DocumentWordsProvider vào worker CodeCompletion trong một máy chủ đơn giản. Script được đóng băng bởi freeze_setup.py trên Windows.a

bPhân tích
- bước 1: kiểm tra Đầu vào:
1. Đầu vào nên là mã và không thể chỉ chứa bình luận.
<answer: có, Đầu vào bao gồm cả mã và bình luận, không chỉ bình luận >
- bước 2: kiểm tra Đầu ra:
1. Solution: Solution có liên quan đến instruction và information. Solution là giải quyết cụ thể cho instruction và information.
<answer: có>
2. Instruction: ngôn ngữ lập trình nên được chỉ định trong instruction.
<answer: có, instruction chỉ định việc sử dụng Python >
3. Solution: trong solution, nó chỉ nên chứa mã và bình luận trong mã. Không nên có giải thích nào được cung cấp bên ngoài mã.
<answer: không, trong solution, việc bao gồm mã và bình luận trong mã là hợp lý. Tuy nhiên, phần Note thuộc về giải thích được cung cấp bên ngoài mã, điều này không được phép >
4. Instruction: Nội dung của instruction nên có liên quan đến Đầu vào và nên là bản tóm tắt của nội dung Đầu vào, không có bất kỳ thông tin không liên quan nào khác.
<answer: có, Instruction có liên quan đến Đầu vào >
-Câu trả lời Tổng thể: không
- Lý do: Trong solution, việc bao gồm mã và bình luận trong mã là hợp lý. Tuy nhiên, phần Note thuộc về giải thích được cung cấp bên ngoài mã, điều này không được phép. Vì vậy câu trả lời tổng thể là không.c

Hình 4: Một ví dụ về khung Generator-Discriminator dựa trên LLM trên nhiệm vụ tạo mã. Bộ tạo sản xuất dữ liệu hướng dẫn dựa trên đầu vào (a). Tiếp theo, bộ phân biệt chấp nhận đầu ra và tạo phân tích cho nó. Đầu ra (b) bao gồm bốn khóa, chúng tôi lấy thông tin làm đầu vào và solution làm đầu ra trong điều chỉnh hướng dẫn của chúng tôi. Phân tích (c) bao gồm lý do chi tiết cho mỗi quy tắc và câu trả lời tổng thể để kiểm tra xem mẫu có đáp ứng tất cả các yêu cầu không.

C So sánh với CodeAlpaca
Bộ dữ liệu CodeAlpaca chứa 20K dữ liệu tuân theo hướng dẫn đa nhiệm vụ được tạo bởi các kỹ thuật trong self-instruct (Taori et al., 2023). Để đảm bảo so sánh công bằng và đa chiều, chúng tôi lấy mẫu ngẫu nhiên 1K và 5K từ cả hai bộ dữ liệu (CodeAlpaca và CodeSeaXDataset), đặt cùng một bộ siêu tham số huấn luyện (epoch = 3, learning rate = 1e-4, LoRA rank = 8) và sử dụng cùng lời nhắc huấn luyện. Để ngăn chặn overfitting, chúng tôi sử dụng Low-Rank Adaption (LoRA) (Hu et al., 2022) để tinh chỉnh nếu kích thước của bộ dữ liệu huấn luyện tuân theo hướng dẫn nhỏ hơn 5K và thực hiện tinh chỉnh đầy đủ trên toàn bộ bộ dữ liệu 20K.

1) Sau khi được tinh chỉnh với 1K, 5K và 20K dữ liệu hướng dẫn tương ứng, hiệu suất của mô hình cơ sở cải thiện đáng kể trên HumanEval được thể hiện trong Hình 5. Lấy Starcoder làm mô hình cơ sở

--- TRANG 12 ---
mô hình, CodeSeaXDataset vượt qua CodeAlpaca (44.9% vs 41.7%, 45.7% vs 48.1% và 47.0% vs 50.5%) được thể hiện trong Hình 5 (a), điều này nhấn mạnh hiệu quả của phương pháp của chúng tôi trong việc tinh chỉnh dữ liệu hướng dẫn. Như thể hiện trong Hình 5 (b), Kết quả của các mô hình cơ sở khác nhau trên CodeSeaXDataset vượt qua kết quả trên CodeAlpaca, điều này nhấn mạnh hiệu quả của bộ dữ liệu CodeSeaXDataset trong việc tăng cường khả năng tuân theo hướng dẫn của mô hình cơ sở.

2) Theo Bảng 4 và Bảng 5, Tất cả các mô hình WaveCoder vượt trội đáng kể so với mô hình được tinh chỉnh với CodeAlpaca. Đáng chú ý, Điểm pass@1 của WaveCoder-CL-13B vượt qua CodeLLaMa-CodeAlpaca-13B đạt cải thiện tuyệt đối 10.6% trên HumanEvalExplain. Điều này nhấn mạnh hiệu quả của việc định nghĩa và phân loại các nhiệm vụ liên quan đến mã trong việc tăng cường khả năng tổng quát hóa của Code LLMs.

30354045505560
1K (LoRA) 5K (LoRA) 20K (Fully)41.741.745.745.74747
44.944.948.148.150.550.5
CodeAlpaca CodeOcean010203040506070
Starcoder CodeLLaMa-7B CodeLLaMa-13B DeepseekCoder-6.7B33.6 33.53649.447
3946.360.9
50.548.155.464
BaseModel CodeAlpaca CodeOceanpass@1
pass@1(a) HumanEval (Base Model:Starcoder) (b) HumanEval (Different Models)

Hình 5: So sánh với CodeAlpaca với kích thước bộ dữ liệu khác nhau(a) và các mô hình cơ sở khác nhau(b). CodeSeaXDataset vượt trội CodeAlpaca trên HumanEval đa chiều, phân tích chi tiết hơn được thể hiện trong Phần 3.3.

D Chuẩn Đánh giá
HumanEval4, bao gồm 164 vấn đề lập trình Python được viết thủ công và trung bình 9.6 test cases được phân bổ cho mỗi vấn đề, hiện là chuẩn được áp dụng rộng rãi nhất cho Code LLMs.

MBPP5 bao gồm khoảng 1.000 vấn đề lập trình Python từ đám đông, được thiết kế để có thể giải quyết bởi các lập trình viên cấp độ đầu vào, bao gồm các nguyên tắc cơ bản của lập trình, chức năng thư viện chuẩn, và v.v. Trong bài báo này, chúng tôi chọn bộ dữ liệu kiểm tra 500 vấn đề để đánh giá cả suy luận few-shot của các mô hình được tinh chỉnh. Đối với những kết quả MBPP (500) không được báo cáo hoặc không được sử dụng, chúng tôi đã tái tạo cho họ sử dụng bigcode-evaluation-harness6.

HumanEvalPack7 là một phần mở rộng của HumanEval của OpenAI để bao gồm tổng cộng 6 ngôn ngữ trên 3 nhiệm vụ. Trong bài báo này, chúng tôi chọn HumanEvalFix để đánh giá khả năng code to code đặc biệt trên nhiệm vụ sửa chữa mã và các chuẩn HumanEvalExplain để đánh giá khả năng code to text đặc biệt trên nhiệm vụ tóm tắt mã.

4https://huggingface.co/datasets/openai_humaneval
5https://huggingface.co/datasets/mbpp
6https://github.com/bigcode-project/bigcode-evaluation-harness
7https://huggingface.co/datasets/bigcode/humanevalpack

--- TRANG 13 ---
Khung Generator-Discriminator dựa trên LLM

Đầu vào:
Mã Python tính diện tích hình tròn dựa trên bán kính của nó.

Đầu ra:
task_name: Tính Diện tích Hình tròn
instruction: Viết một hàm Python tính diện tích hình tròn dựa trên bán kính của nó.
information: Công thức để tính diện tích hình tròn là A = pi * r2, trong đó A là diện tích và r là bán kính.
solution:
import math
def area_of_circle(radius): return math.pi * radius ** 2

Phân tích: - Bước 1: Kiểm tra Instruction: Ngôn ngữ lập trình nên được chỉ định trong instruction. <answer: có, Instruction đề cập đến "Viết một hàm Python," điều này chỉ định rõ ràng ngôn ngữ lập trình là Python.>
- Bước 2: Kiểm tra Solution:
1. Solution có liên quan đến instruction và information. Solution là giải quyết cụ thể cho instruction và information. <answer: có, Solution được cung cấp tính diện tích hình tròn dựa trên bán kính của nó, điều này có liên quan trực tiếp đến instruction và information được cung cấp.>
2. Trong solution, nó chỉ nên chứa mã và bình luận trong mã. Không nên có giải thích nào được cung cấp bên ngoài mã. <answer: có, Solution chỉ chứa mã mà không có giải thích nào bên ngoài mã. Bình luận trong mã không có mặt, nhưng mã đơn giản và dễ hiểu.>
3. Mã chứa logic thuật toán trong solution không nên thường xuyên bị chôn vùi trong các hàm phức tạp hoặc được tài liệu hóa kém, khiến chúng khó theo dõi hoặc học hỏi. <answer: có, Mã chứa logic thuật toán trong solution không nên thường xuyên bị chôn vùi trong các hàm phức tạp hoặc được tài liệu hóa kém, khiến chúng khó theo dõi hoặc học hỏi.>
4. Mã nên import các thư viện cần thiết. <answer: có, Mã import thư viện math cần thiết, được yêu cầu để tính diện tích hình tròn sử dụng hằng số toán học pi.>
- Câu trả lời tổng thể: có
- Lý do: Tất cả các yêu cầu đều được đáp ứng theo các quy tắc đã cho. Ngôn ngữ lập trình được chỉ định, và solution có liên quan đến instruction và information. Mã đơn giản, dễ theo dõi, và import thư viện cần thiết. Solution được cung cấp là một ví dụ tốt về mã có cấu trúc tốt và súc tích đáp ứng các đặc tả của nhiệm vụ.

Bảng 7: Một ví dụ về khung Generator-Discriminator dựa trên LLM

--- TRANG 14 ---
HumanEval/1(mã chuẩn):from typing import Listdef has_close_elements(numbers: List[float], threshold: float) -> bool:""" Kiểm tra xem trong danh sách số đã cho, có hai số nào gần nhau hơn ngưỡng đã cho không.>>> has_close_elements([1.0, 2.0, 3.0], 0.5)False>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)True"""for idx, elemin enumerate(numbers):for idx2, elem2 in enumerate(numbers):if idx!= idx2:distance = abs(elem-elem2)if distance < threshold:return Truereturn FalseĐây là hàm được viết bằng Python:```pythondef has_close_elements(list_of_numbers, threshold):for iin range(len(list_of_numbers)):for j in range(i+ 1, len(list_of_numbers)):if abs(list_of_numbers[i] -list_of_numbers[j]) < threshold:return Truereturn False```Độ tương tự Cosine: 94.3def closest_pair(arr, target): min_difference= 999999min_pair= (-1, -1)# Lặp qua tất cả các phần tử for iin range(len(arr)): # Lặp qua phần còn lại của các phần tử for j in range(i+ 1, len(arr)): diff = abs(arr[i] + arr[j] -target) # Kiểm tra sự khác biệt tối đa if diff < min_difference: min_difference= diff min_pair= (i, j) return min_pairĐộ tương tự Cosine: 87.5import mathdef find_closest_component(components):def _bounds(component):xmin, ymin, xmax, ymax= component["xmin"], component["ymin"],                   component["xmax"], component["ymax"]return math.sqrt(xmin**2 + ymin**2)return min(components, key=_bounds)Độ tương tự Cosine: 86.9Evol-codealpaca-v1
CodeAlpacaCodeOcean
HumanEval/101(mã chuẩn):def make_a_pile(n):"""Cho một số nguyên dương n, bạn phải tạo một đống gồm n   cấp độ đá.Cấp độ đầu tiên có n viên đá.Số lượng đá ở cấp độ tiếp theo là:-số lẻ tiếp theo nếu n là lẻ.-số chẵn tiếp theo nếu n là chẵn.Trả về số lượng đá ở mỗi cấp độ trong một danh sách, trong đó phần tử tại chỉ mục iđại diện cho số lượng đá ở cấp độ (i+1).Ví dụ:>>> make_a_pile(3)[3, 5, 7]"""return [n + 2*ifor iin range(n)]def make_a_pile(n, pattern):if pattern == 'odd':p = [3]stone = 3for iin range(n-1):stone += 2p.append(stone)elifpattern == 'even':p = [3]stone = 3for iin range(n-1):stone += 2p.append(stone)return pprint(make_a_pile(3, 'odd'))print(make_a_pile(3, 'even'))Độ tương tự Cosine: 94.4def generate_list(n):"""Hàm này nhận một số nguyên n và trả về một danh sách có độ dài n của các số từ 0 đến n-1."""return [ifor iin range(n)]Độ tương tự Cosine: 85.0def inventory_levels(arr)levels = Hash.new(0)arr.eachdo |type, quantity|levels[type] += quantityendreturn levelsendĐộ tương tự Cosine: 83.2Evol-codealpaca-v1
CodeAlpacaCodeOceanHình 6: Ví dụ về rò rỉ dữ liệu.
