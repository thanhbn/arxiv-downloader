# 2305.04032.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2305.04032.pdf
# Kích thước file: 1205674 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
ToolCoder: Dạy các mô hình tạo code sử dụng công cụ tìm kiếm API

Kechi Zhang
Phòng thí nghiệm chính về Công nghệ Phần mềm Tin cậy cao, Bộ Giáo dục (Đại học Bắc Kinh)
Bắc Kinh, Trung Quốc
zhangkechi@pku.edu.cn

Huangzhao Zhang
Phòng thí nghiệm chính về Công nghệ Phần mềm Tin cậy cao, Bộ Giáo dục (Đại học Bắc Kinh)
Bắc Kinh, Trung Quốc
zhang_hz@pku.edu.cn

Ge Li*
Phòng thí nghiệm chính về Công nghệ Phần mềm Tin cậy cao, Bộ Giáo dục (Đại học Bắc Kinh)
Bắc Kinh, Trung Quốc
lige@pku.edu.cn

Jia Li♂
Phòng thí nghiệm chính về Công nghệ Phần mềm Tin cậy cao, Bộ Giáo dục (Đại học Bắc Kinh)
Bắc Kinh, Trung Quốc
lijia@stu.pku.edu.cn

Zhuo Li
Phòng thí nghiệm chính về Công nghệ Phần mềm Tin cậy cao, Bộ Giáo dục (Đại học Bắc Kinh)
Bắc Kinh, Trung Quốc
lizhmq@pku.edu.cn

Zhi Jin*
Phòng thí nghiệm chính về Công nghệ Phần mềm Tin cậy cao, Bộ Giáo dục (Đại học Bắc Kinh)
Bắc Kinh, Trung Quốc
zhijin@pku.edu.cn

Tóm tắt—Việc tự động tạo mã nguồn từ các mô tả bằng ngôn ngữ tự nhiên đã trở thành một lĩnh vực nghiên cứu phát triển trong những năm gần đây. Tuy nhiên, các mô hình tạo code quy mô lớn hiện tại thường gặp khó khăn khi lựa chọn các API phù hợp cho các ngữ cảnh cụ thể. Những mô hình này có thể tạo ra các API không đáp ứng yêu cầu hoặc tham chiếu đến các API không tồn tại trong các thư viện bên thứ ba, đặc biệt là đối với các thư viện ít được biết đến hoặc riêng tư. Lấy cảm hứng từ quá trình các nhà phát triển con người sử dụng công cụ để tìm kiếm API, chúng tôi đề xuất ToolCoder, một phương pháp mới tích hợp các công cụ tìm kiếm API với các mô hình hiện có để hỗ trợ việc tạo code và lựa chọn API. Để dạy mô hình của chúng tôi sử dụng công cụ, chúng tôi giới thiệu một phương pháp chú thích dữ liệu tự động sử dụng ChatGPT để thêm thông tin sử dụng công cụ vào dữ liệu mã nguồn và tinh chỉnh các mô hình tạo code. Trong quá trình suy luận, chúng tôi tích hợp các công cụ tìm kiếm API vào quá trình tạo để mô hình của chúng tôi có thể tự động sử dụng công cụ tìm kiếm để nhận gợi ý khi lựa chọn API. Kết quả thí nghiệm của chúng tôi chứng minh rằng ToolCoder thể hiện hiệu suất và khả năng tổng quát hóa xuất sắc trên năm benchmark tạo code thư viện công khai và riêng tư, với cải thiện ít nhất 6,21% trên các chỉ số pass@1 trung bình và 9,64% cải thiện trên các chỉ số pass@10 trung bình so với các phương pháp tiên tiến nhất. Hơn nữa, chúng tôi chỉ ra rằng mô hình ToolCoder tương đối nhỏ của chúng tôi có thể so sánh được với một trong những mô hình tốt nhất hiện tại, GPT-3.5, làm nổi bật tiềm năng của việc kết hợp các công cụ lập trình vào quá trình tạo code.

I. GIỚI THIỆU

Việc tạo code tự động đã trở nên ngày càng quan trọng do nỗ lực đáng kể cần thiết để viết mã nguồn thủ công, đặc biệt là đối với phần mềm phức tạp. Các kỹ thuật học sâu, đặc biệt là các mô hình ngôn ngữ, đã cho thấy tiềm năng lớn trong việc tạo mã nguồn chất lượng cao từ các yêu cầu ngôn ngữ tự nhiên. Hiện tại, các mô hình tạo code được đào tạo trước được coi là giải pháp tiên tiến nhất cho các tác vụ tạo code khác nhau, chẳng hạn như các mô hình CodeX [3], ChatGPT [4, 16] và CodeGen [14].

Việc lựa chọn chính xác các giao diện lập trình ứng dụng (API) phù hợp là điều cần thiết để các mô hình được đào tạo trước tạo ra

*Tác giả liên hệ

definit_model(args):…...# inp_size(height, width, extra_dim)# out_size(height, width)out =np.?

# inp_size(height, width, extra_dim)# out_size(height, width)out = np.squeeze(inp)

definit_model(): # inp_size(height, width, extra_dim)# out_size(height, width)out =np.?

Hình 1. Một ví dụ minh họa về quá trình lập trình viên con người lựa chọn API phù hợp trong quá trình lập trình. Lập trình viên tóm tắt nhu cầu của họ thành một truy vấn (loại bỏ các mục một chiều) và sử dụng công cụ công cụ tìm kiếm hoặc công cụ tìm kiếm tài liệu để nhận gợi ý API phù hợp (np.squeeze).

code. Lựa chọn API là rất quan trọng để biểu đạt chính xác ngữ nghĩa chương trình và giải quyết các vấn đề một cách hiệu quả. Tuy nhiên, có quá nhiều thư viện bên thứ ba hiện có và các API của chúng, và các API mới liên tục được phát triển. Các mô hình hiện có thường gặp khó khăn trong việc lựa chọn API chính xác và sẽ tạo ra các API không tồn tại hoặc các API không đáp ứng yêu cầu. Ví dụ, theo các thí nghiệm sơ bộ của chúng tôi trên NumpyEval và PandasEval [26], một mô hình tạo code phổ biến CodeGen-2B tạo ra hơn 26% code chứa API không chính xác. Hơn nữa, vì lý do bảo mật và chức năng, các công ty công nghiệp thường xây dựng các thư viện riêng chỉ để sử dụng nội bộ. Đối với những thư viện riêng không có sẵn công khai, tỷ lệ lỗi sẽ tăng lên hơn 90%. Những thư viện công khai bên thứ ba hoặc thư viện riêng này cung cấp rất nhiều API mà các mô hình tạo code chưa từng thấy, dẫn đến việc mô hình không thể tạo code hướng API.

--- TRANG 2 ---

Vì vậy, việc khám phá các phương pháp để cải thiện các mô hình tạo code nhằm tạo ra mã nguồn chính xác sử dụng các API thư viện cụ thể theo miền hoặc riêng tư này là điều đáng để làm.

Để hỗ trợ các mô hình tạo code trong việc lựa chọn các API phù hợp trong quá trình tạo, chúng tôi lấy cảm hứng từ quan điểm của các lập trình viên con người. Trong hầu hết các tình huống lập trình, lập trình viên có thể sử dụng các công cụ tìm kiếm để nhận gợi ý từ các nguồn web hoặc tài liệu thư viện khi lựa chọn API. Hình 1 cho thấy một ví dụ về lập trình viên con người lựa chọn API phù hợp trong quá trình lập trình. Khi gặp phải tình huống sử dụng API, lập trình viên tóm tắt nhu cầu của họ thành một truy vấn và sử dụng các công cụ tìm kiếm API hiện có để tìm kiếm các API phù hợp, chẳng hạn như công cụ tìm kiếm Google hoặc công cụ tìm kiếm tài liệu cho các thư viện cụ thể. Sau đó, theo kết quả tìm kiếm, lập trình viên có thể chọn API phù hợp. Quá trình lập trình này sử dụng công cụ để truy xuất và xác định việc sử dụng API cải thiện hiệu quả lập trình, cải thiện độ chính xác và giảm rủi ro lỗi. Điều này thúc đẩy chúng tôi điều tra các phương pháp để dạy các mô hình tạo code sử dụng công cụ tìm kiếm để tìm các API phù hợp.

Trong bài báo này, chúng tôi đề xuất ToolCoder, một giải pháp chi phí thấp và hiệu quả tích hợp các công cụ tìm kiếm API vào các mô hình tạo code được đào tạo trước, bắt chước cách lập trình viên giải quyết vấn đề này. Để giúp các mô hình học cách sử dụng công cụ, chúng tôi đề xuất một phương pháp chú thích dữ liệu tự động với ChatGPT để thêm thông tin sử dụng công cụ vào dữ liệu mã nguồn và sử dụng bộ dữ liệu được chú thích để tinh chỉnh các mô hình tạo code. Cụ thể, phương pháp của chúng tôi sử dụng khả năng học trong ngữ cảnh của các mô hình lớn để chú thích một bộ dữ liệu tăng cường công cụ đặc biệt với chi phí thấp. Chúng tôi sử dụng tinh chỉnh hiệu quả tham số để cải thiện hiệu quả đào tạo. Trong quá trình suy luận, chúng tôi tích hợp các công cụ tìm kiếm API vào quá trình giải mã của mô hình, cho phép mô hình học cách sử dụng các công cụ bên ngoài một cách tự động.

Chúng tôi đánh giá rộng rãi ToolCoder được đề xuất với các chỉ số tỷ lệ pass [3]. ❶Chúng tôi đánh giá ToolCoder trên ba benchmark thư viện công khai. Mô hình của chúng tôi đạt được những cải thiện đáng kể so với các baseline tiên tiến nhất với ít nhất 10,11%, 3,26%, và 1,39% pass@1. ToolCoder tương đối nhỏ của chúng tôi thậm chí có thể so sánh được với một trong những mô hình ngôn ngữ tốt nhất hiện tại, GPT-3.5. ❷Chúng tôi tiếp tục đánh giá mô hình của chúng tôi trên hai benchmark thư viện riêng. Bằng cách chuyển sang công cụ tìm kiếm phù hợp, ToolCoder của chúng tôi có thể dễ dàng được chuyển giao cho các tình huống thư viện riêng này và đạt được cải thiện ổn định. Mô hình của chúng tôi thể hiện hiệu suất tổng quát hóa tốt hơn và nâng cao ít nhất 6,21% cải thiện trên các chỉ số pass@1 trung bình cho tất cả năm benchmark. ❸Chúng tôi cũng tiến hành nghiên cứu ablation để phân tích các cài đặt khác nhau trong các thí nghiệm của chúng tôi, bao gồm các cài đặt bộ dữ liệu, đào tạo và suy luận. Kết quả chứng minh hiệu quả của các thiết kế khác nhau trong phương pháp của chúng tôi.

Những đóng góp của chúng tôi trong bài báo này có thể được tóm tắt như sau:

• Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên kết hợp công cụ lập trình vào các mô hình tạo code. Kết quả của chúng tôi làm nổi bật tầm quan trọng của khả năng sử dụng công cụ của các mô hình.

Trường hợp 2: Pandas
df = pd.DataFrame({'A': [5, 6, 7], 'B': [7, 8, 9]}) 
# tổng tất cả các giá trị và trả về một giá trị số
sum_value = df.sum()
sum_value không phải là một giá trị số

Trường hợp 1: Numpy
a = np.arange(2*3*2).reshape((2,3,2)) 
count_value = a.count(2)
'numpy.ndarray' object has no attribute 'count' 

Trường hợp 3: Thư viện riêng (BeatNum)
import beatnum as bn
num_str = bn.numstr([0,33,4444522])
module 'beatnum' has no attribute 'numstr'

Hình 2. Các trường hợp thất bại của mô hình CodeGen-2B trong việc lựa chọn API, bao gồm tạo ra các API không tồn tại trên các thư viện công khai (Trường hợp 1), tạo ra các API không đủ điều kiện (Trường hợp 2), và thiếu kiến thức liên quan đến API trên các thư viện riêng (Trường hợp 3).

• Chúng tôi đề xuất một phương pháp tự động để chú thích các bộ dữ liệu trong kỹ thuật phần mềm. Khung chú thích chi phí thấp và hiệu quả này sử dụng ChatGPT mạnh mẽ để chú thích các bộ dữ liệu API với các bộ dữ liệu mã nguồn công khai, giảm nỗ lực thủ công cần thiết để tạo ra các bộ dữ liệu chú thích riêng. Phương pháp xây dựng bộ dữ liệu của chúng tôi cũng có thể dễ dàng được chuyển giao cho các tác vụ khác.

• Chúng tôi đề xuất ToolCoder, kết hợp khả năng sử dụng các công cụ tìm kiếm API vào các mô hình tạo code được đào tạo trước và cải thiện hiệu suất trên các tác vụ tạo code liên quan đến API. Phương pháp của chúng tôi vượt trội hơn các baseline hướng API hiện có trên nhiều benchmark tạo code liên quan đến API phổ biến.

II. CÁC VÍ DỤ ĐỘNG LỰC

Trong phần này, chúng tôi xem xét các hạn chế của các mô hình tạo code hiện tại khi lựa chọn một API phù hợp và cách các công cụ tìm kiếm hiện có có thể hỗ trợ trong việc lựa chọn API. Bằng cách khám phá những vấn đề này, chúng tôi hy vọng cung cấp bối cảnh cho nghiên cứu của chúng tôi và giải thích động lực của chúng tôi để đề xuất một phương pháp mới để giải quyết những thách thức này.

A. Những hạn chế của các mô hình tạo code hiện tại trong việc lựa chọn các API phù hợp

Các Giao diện Lập trình Ứng dụng (API) là cần thiết cho việc phát triển phần mềm hiện đại. API cho phép các nhà phát triển tích hợp mã và dịch vụ có sẵn trước đó vào các ứng dụng của họ. Sử dụng API có thể giảm đáng kể thời gian và công sức cần thiết để phát triển các hệ thống phần mềm phức tạp.

Tuy nhiên, việc lựa chọn API phù hợp vẫn là thách thức đối với các mô hình tạo code. Do sự gia tăng của các thư viện bên thứ ba và các API liên quan của chúng, các mô hình tạo code hiện có thường cần sự trợ giúp trong việc chọn API. Ở đây chúng tôi chọn mô hình phổ biến CodeGen-2B để kiểm tra hiệu suất của nó trong việc lựa chọn API. Hình 2 cho thấy ba ví dụ thất bại của code được tạo ra để lựa chọn API. ❶Trường hợp 1: Mô hình CodeGen-2B có nguy cơ tiềm ẩn tạo ra các API

--- TRANG 3 ---

BẢNG I
SO SÁNH HAI LOẠI CÔNG CỤ TÌM KIẾM CHO LỰA CHỌN API.

Công cụ Tìm kiếm Trực tuyến | Tìm kiếm Tài liệu
--- | ---
Nguồn Kiến thức | Cộng đồng Lập trình hoặc Trang web Hướng dẫn (StackOverFlow, datagy.io, v.v.) | Tài liệu Thư viện
Loại API | Thư viện công khai, đặc biệt là những thư viện nổi tiếng và được thảo luận rộng rãi | Bất kỳ API nào, bao gồm thư viện công khai và riêng tư
Ưu điểm | Thực tế và Chính xác, Nguồn phong phú, Cập nhật liên tục | Bao phủ rộng, Giải thích chi tiết, Ổn định
Ví dụ Công cụ | Google, Bing, DuckDuckGo | NumPydoc, Pandasdoc, Tài liệu riêng

không tồn tại, ngay cả trên một thư viện bên thứ ba phổ biến và thông dụng như NumPy. Không có API count trong thư viện Numpy, nhưng CodeGen2B vẫn tạo ra. ❷Trường hợp 2: Nó cũng có thể sử dụng sai API và tạo ra code không đủ điều kiện. df.sum() sẽ trả về kiểu Pandas Series chứ không phải giá trị số như yêu cầu. Điều này cho thấy rằng những mô hình tạo code hiện có này vẫn gặp thách thức trong việc chọn một API phù hợp để thực hiện một yêu cầu nhất định. Chúng tôi tiến hành một thí nghiệm thống kê để phân tích code được tạo ra trên hai benchmark, NumpyEval và PandasEval [27], và thấy rằng hơn 26% API được tạo ra có các vấn đề đã đề cập ở trên. Chúng tôi cũng tiến hành thí nghiệm trên các benchmark thư viện riêng như BeatNumEval [26]. Các thư viện riêng là phổ biến trong các tình huống code thực tế, và chúng không công khai vì lý do bảo mật và chức năng. Thí nghiệm cho thấy rằng đối với những API trong các thư viện riêng, tỷ lệ thất bại sẽ tăng lên hơn 90%. ❸Trường hợp 3: Chúng tôi thấy CodeGen-2B thiếu kiến thức tương ứng về riêng tư và bịa ra một API không thể hiểu được cho thư viện riêng BeatNum. Điều này cho thấy rằng các mô hình tạo code hiện có có những hạn chế trong việc tạo API và không được tối ưu hóa cụ thể để sử dụng API. Trong bài báo này, chúng tôi nhắm mục tiêu giải quyết những vấn đề lựa chọn API này.

B. Các công cụ tìm kiếm hiện có để hỗ trợ lựa chọn API

Lấy cảm hứng từ phương pháp mà lập trình viên sử dụng trong việc lựa chọn API, nghiên cứu của chúng tôi cho thấy rằng các công cụ tìm kiếm hiện có có thể cung cấp các đề xuất API. Ví dụ, trong Hình 1, nhà phát triển cần sử dụng thư viện numpy để loại bỏ chiều đơn bổ sung trong kích thước đầu vào. Nhà phát triển chuyển sang các công cụ công cụ tìm kiếm trực tuyến hoặc công cụ tìm kiếm tài liệu thư viện và nhận được gợi ý API phù hợp np.squeeze. Hai công cụ tìm kiếm này có thể đóng vai trò quan trọng trong việc lựa chọn API. Một so sánh của hai loại công cụ tìm kiếm này được đưa ra trong Bảng I. Chúng tôi sẽ phân tích việc sử dụng hai loại công cụ tìm kiếm này.

1) Công cụ Tìm kiếm Trực tuyến: Các công cụ tìm kiếm trực tuyến cung cấp thông tin phong phú về các tình huống sử dụng API khác nhau. Các lập trình viên con người chia sẻ kinh nghiệm của họ trong việc giải quyết các vấn đề lập trình khác nhau trên các trang web cộng đồng và hướng dẫn khác nhau như StackOverFlow¹ và datagy.io². Họ tổ chức và tóm tắt các gợi ý API được sử dụng cho các vấn đề khác nhau. Chính thức, những gợi ý API trực tuyến này thường được hiển thị dưới dạng chia sẻ kinh nghiệm lập trình hoặc hỏi đáp. Khi những người khác gặp phải các vấn đề tương tự, các công cụ tìm kiếm có thể sử dụng thông tin này một cách tốt, và lập trình viên chỉ cần cung cấp một truy vấn câu hỏi. Những công cụ tìm kiếm này coi các trang web cộng đồng này là nguồn tài nguyên kiến thức và có thể cung cấp các gợi ý API hữu ích, đặc biệt là đối với những thư viện công khai nổi tiếng và được thảo luận rộng rãi. Vì những kinh nghiệm lập trình trực tuyến này trên trang web được cập nhật liên tục và cung cấp nhiều tình huống thực tế, chúng ta thường có thể nhận được các gợi ý API chính xác hơn từ những công cụ tìm kiếm trực tuyến này. Những công cụ tìm kiếm thương mại hoàn thiện này như Google³, DuckDuckGo⁴ có thể cung cấp các phản hồi tìm kiếm chính xác, tức thời và nhanh chóng và được sử dụng rộng rãi bởi các lập trình viên con người.

2) Công cụ Tìm kiếm Tài liệu: Vì các thư viện công khai ít được biết đến hoặc thư viện riêng có ít thảo luận trên các trang web cộng đồng trực tuyến, các lập trình viên con người cũng chuyển sang tài liệu thư viện để tìm gợi ý API. Tài liệu thường có sẵn cho các thư viện công khai và thư viện riêng. Do đó, nó có thể cung cấp thông tin phong phú cho bất kỳ tình huống sử dụng API nào. Tài liệu cung cấp các giải thích chi tiết cho từng API, với sự hội tụ rộng trên thư viện tương ứng. Tài liệu chứa các giải thích chi tiết và chính xác về các tham số và cách sử dụng của API. Đó là nơi chi tiết nhất để học cách sử dụng API. Vì tài liệu không thay đổi thường xuyên, kết quả của nó ổn định hơn. Chính thức, thông tin API trong tài liệu thường được đưa ra theo cặp API và các bình luận tương ứng. Chúng ta có thể sử dụng BM25 [19] hoặc các điểm tương đồng ngữ nghĩa khác làm chỉ số tìm kiếm để tìm kiếm các bình luận đáp ứng yêu cầu và tìm API tương ứng làm gợi ý cuối cùng cho việc lập trình.

Những công cụ tìm kiếm đa dạng này rất hữu ích cho lập trình và lựa chọn API. Lấy cảm hứng từ quá trình tìm kiếm API của các nhà phát triển con người, chúng tôi nhắm mục tiêu kết hợp hai loại công cụ tìm kiếm này vào các mô hình tạo code. Bằng cách để mô hình tạo code học cách sử dụng những công cụ tìm kiếm trực tuyến hoặc công cụ tìm kiếm tài liệu này, các mô hình của chúng tôi có thể điều hướng hiệu quả lượng thông tin khổng lồ có sẵn để xác định các API liên quan nhất. Phương pháp này cho phép các mô hình của chúng tôi khớp API với các nhu cầu cụ thể một cách chính xác hơn.

III. CÔNG CỤ TÌM KIẾM API

Để trình bày tốt hơn phương pháp luận của mô hình chúng tôi, trước tiên chúng tôi cung cấp một giới thiệu ngắn gọn về công cụ tìm kiếm API trong phần này. Công cụ tìm kiếm API được đề xuất là một sự trừu tượng hóa của các công cụ tìm kiếm hiện có cho việc lựa chọn API và sẽ được sử dụng như một công cụ bên ngoài cho ToolCoder của chúng tôi.

¹https://stackoverflow.com/
²https://datagy.io/
³https://www.google.com/
⁴https://duckduckgo.com/

--- TRANG 4 ---

Theo các ví dụ động lực trong Phần II-B, chúng tôi phát triển công cụ tìm kiếm API cho các mô hình tạo code dựa trên hai loại nguồn này. ❶Đối với những thư viện công khai thường được sử dụng như numpy và pandas, chúng tôi sử dụng DuckDuckgo làm công cụ tìm kiếm vì nó cung cấp phương pháp thuận tiện và tự động hơn so với các công cụ tìm kiếm khác. Chúng tôi sử dụng công cụ tìm kiếm để tìm kiếm nội dung liên quan từ một số trang web cộng đồng trực tuyến và trích xuất các API được đề cập bằng string regex matching. Vì những nội dung này có phần giới thiệu phong phú hơn về API, có thể thu được các gợi ý API chính xác hơn từ công cụ tìm kiếm. ❷Đối với các API ít được biết đến hoặc những API trong thư viện riêng, chúng tôi sử dụng điểm BM25 làm chỉ số truy xuất để tìm kiếm từ tài liệu API tương ứng.

Sau đó chúng tôi trừu tượng hóa hai loại công cụ tìm kiếm thành một dạng thống nhất: chúng tôi sử dụng ký hiệu APISearch(query)→answer để biểu diễn lời gọi của công cụ tìm kiếm API, trong đó APISearch là tên hàm trừu tượng hóa các nguồn tìm kiếm API khác nhau, query biểu thị truy vấn tìm kiếm và answer chỉ ra câu trả lời trả về từ các công cụ tìm kiếm API có thể được tham chiếu để tạo code tiếp theo. Trong các thí nghiệm tiếp theo, chúng tôi tuần tự hóa các lời gọi công cụ tìm kiếm API cho đầu vào mô hình. Để phân biệt với văn bản thông thường, chúng tôi bao quanh các lời gọi công cụ bằng các token đặc biệt bằng cách bắt đầu với ⟨API⟩ và kết thúc với ⟨/API⟩.

Các ví dụ có thể được xem trong Hình 3. Chúng tôi đặt ⟨API⟩, ⟨/API⟩, và → làm các token đặc biệt trong từ vựng mô hình của chúng tôi.

IV. TOOLCODER

Trong phần này, chúng tôi trình bày phương pháp ToolCoder của chúng tôi để lựa chọn và sử dụng API trong thực hành lập trình. Mục tiêu của phương pháp chúng tôi là đào tạo một mô hình có thể lựa chọn và sử dụng hiệu quả các API phù hợp dựa trên code một phần hiện có. Để đạt được mục tiêu này, chúng tôi phân tách phương pháp của chúng tôi thành ba mô-đun, bao gồm chú thích dữ liệu, tinh chỉnh và suy luận. Ba mô-đun hoạt động trong một pipeline như được hiển thị trong Hình 3. Chúng tôi sẽ mô tả chi tiết trong các phần con sau.

A. Chú thích Dữ liệu Tự động

Để mô hình học cách sử dụng công cụ tìm kiếm API, trước tiên chúng ta cần một bộ dữ liệu bao gồm mã nguồn và các quy trình gọi công cụ liên quan. Như đã đề cập trong Phần III, chúng tôi trừu tượng hóa quy trình gọi tìm kiếm với ký hiệu ⟨API⟩APISearch(query)→answer⟨/API⟩. Tuy nhiên, những bộ dữ liệu như vậy không có sẵn ngay lập tức. Để giải quyết vấn đề này, chúng tôi đề xuất tự động tăng cường một bộ dữ liệu mã nguồn hiện có với ký hiệu gọi công cụ sử dụng ChatGPT (gpt-3.5-turbo)⁵, đã chứng minh khả năng học few-shot và thậm chí zero-shot xuất sắc trong nhiều tác vụ học ngôn ngữ khác nhau. Phương pháp chú thích chi phí thấp và hiệu quả này giảm nỗ lực thủ công cần thiết để tạo ra các bộ dữ liệu chú thích riêng. Quá trình chú thích dữ liệu của chúng tôi có thể được chia thành ba phần: ❶lựa chọn bộ dữ liệu cơ sở, ❷lựa chọn prompt, và ❸lọc và làm sạch.

Lựa chọn Bộ dữ liệu Cơ sở. Đối với bộ dữ liệu cơ sở, chúng tôi chọn sử dụng bộ dữ liệu được đào tạo trước phổ biến CodeSearchNet-Python [8]

⁵https://openai.com/

BẢNG II
THỐNG KÊ CỦA BỘ DỮ LIỆU CHÚ THÍCH.

Thống kê | 
--- | ---
Kích thước Bộ dữ liệu | 53,000
API Chú thích Trung bình | 3.2
Độ dài Trung bình (tính bằng từ) trước chú thích | 186.24
Độ dài Trung bình (tính bằng từ) sau chú thích | 211.49
Tỷ lệ một số thư viện bên thứ ba |
NumPy | 24%
Pandas | 13%
TorchData | 0%

làm bộ dữ liệu cơ sở. Đây là một bộ dữ liệu lập trình thực tế thu được từ GitHub mà không có bất kỳ chú thích bổ sung nào. Bộ dữ liệu này đã được sử dụng phổ biến bởi nhiều mô hình tạo code được đào tạo trước, vì vậy chúng tôi có thể đảm bảo càng nhiều càng tốt rằng việc đào tạo tiếp theo của chúng tôi sẽ không ảnh hưởng đến hiệu suất tổng quát hóa của mô hình về khả năng tạo và mô hình hóa ngôn ngữ. Chúng tôi sử dụng phương pháp lọc độ dài đơn giản và ngẫu nhiên chọn gần 60k mã nguồn cấp độ hàm từ bộ dữ liệu này làm bộ dữ liệu cơ sở cho phương pháp chú thích của chúng tôi.

Lựa chọn Prompt. Tương tự như [20], để giúp tạo ra bộ dữ liệu được chú thích, chúng tôi cần cung cấp một hướng dẫn chi tiết cho ChatGPT để chỉ định vai trò hệ thống của nó như một người chú thích dữ liệu, được hiển thị trong Hình 4. Để tạo điều kiện cho chất lượng của các bộ dữ liệu được tạo, chúng tôi viết thủ công ba cặp đầu vào-đầu ra do con người viết như một phần của prompt với ba thư viện bao gồm numpy, pandas, và matplotlib. Chúng tôi chọn ba thư viện này làm ví dụ trong prompt vì chúng tôi thành thạo về chúng, và chúng cũng được sử dụng phổ biến trong bộ dữ liệu cơ sở. Dựa trên prompt được chọn và bộ dữ liệu cơ sở của chúng tôi, chúng tôi sẽ yêu cầu ChatGPT chú thích bộ dữ liệu tăng cường công cụ. Chúng tôi tạo ra một dữ liệu được chú thích cho mỗi mẫu cơ sở. Quá trình chú thích tự động kéo dài bốn ngày.

Lọc và Làm sạch. Sau khi nhận được tất cả kết quả được tạo từ chatGPT, chúng tôi thực hiện một loạt các hoạt động lọc đơn giản trên kết quả để loại bỏ những mẫu dữ liệu bất thường. Chúng tôi lọc ra các lời gọi API Search lồng nhau, kiểm soát số lượng lời gọi API Search trong một mẫu ít hơn 5, và đảm bảo rằng ít nhất một là lời gọi API từ thư viện công khai. Chúng tôi lọc ra những mẫu khác với mã nguồn sau khi loại bỏ lời gọi API Search. Hơn nữa, đối với câu trả lời API được tạo trong lời gọi tìm kiếm, chúng tôi kiểm tra xem nó có được theo sau bởi API tương ứng trong code được tạo để đảm bảo rằng lời gọi tìm kiếm API có liên quan chặt chẽ với việc triển khai code cụ thể. Cuối cùng, chúng tôi đã làm sạch và thu được bộ dữ liệu cuối cùng là 53k, sẽ được sử dụng cho việc tinh chỉnh tiếp theo. Bảng II cho thấy thống kê của bộ dữ liệu được chú thích cuối cùng. Chúng tôi cũng đếm tỷ lệ một số API thư viện bên thứ ba trong bộ dữ liệu để tham khảo trong các thí nghiệm đánh giá tiếp theo. Trong phần bên trái của Hình 3, chúng tôi cũng đưa ra một mẫu ví dụ của bộ dữ liệu cuối cùng.

B. Tinh chỉnh Hiệu quả Tham số

Chúng tôi tận dụng bộ dữ liệu được chú thích để tinh chỉnh một mô hình ngôn ngữ được đào tạo trước để dạy mô hình tạo ra lời gọi công cụ tìm kiếm. Để giải quyết thách thức về tài nguyên tính toán hạn chế

--- TRANG 5 ---

Tạo Code
Mô hình Được đào tạo trước

⚙Tinh chỉnh Hiệu quả Tham số có thể được đào tạo chỉ với GPU cấp độ người tiêu dùng

ChatGPT
Trước Chú thích:
…if mean is not None:
samples=multivariate_normal(mean, matrix, N)…

Sau Chú thích:
…if mean is not None:
samples=<API>APISearch(Tạo ra các mẫu ngẫu nhiên từ một phân phối chuẩn đa biến.)-> multivariate_normal</API> multivariate_normal(mean, matrix, N)…

NL_input: Làm thế nào để tôi lấy giá trị tại hàng thứ n của một tên cột nhất định trong Pandas?

output = <API>APISearch(Chọn một hàng dữ liệu duy nhất từ DataFrame.)->pandas.DataFrame.iloc</API> df.iloc[n][column_name]

🔍Chọn một hàng dữ liệu duy nhất từ DataFrame
✅pandas.DataFrame.iloc

output = <API>APISearch(Chọn một hàng dữ liệu duy nhất từ DataFrame.)->

Bộ dữ liệu Đào tạo trước Công khai
(CodeSearchNet…)

Tinh chỉnh
Suy luận

🤖CodeGen-350M
🤖CodeGen-2B...

low-rank adaptation:
đóng băng hầu hết các tham số và chỉ tinh chỉnh với ít tham số

x
h
W
down
W
up
Trọng số Đã đóng băng Được đào tạo trước

Hình 3. Pipeline của phương pháp ToolCoder của chúng tôi. Pipeline có ba phần chính: (1) Tự động Chú thích Bộ dữ liệu Tăng cường Công cụ với ChatGPT, (2) Tinh chỉnh hiệu quả tham số mô hình tạo code được đào tạo trước hiện có với bộ dữ liệu được chú thích, và (3) Suy luận của mô hình được tinh chỉnh được tăng cường với các công cụ tìm kiếm API.

Nhiệm vụ của bạn là thêm các lời gọi đến Công cụ Tìm kiếm API vào một đoạn mã nguồn. Bạn có thể sử dụng Công cụ Tìm kiếm API để tra cứu các API bên thứ ba quan trọng từ tài liệu. Công cụ Tìm kiếm API sẽ giúp bạn lấy thông tin cần thiết để hoàn thành mã nguồn và lựa chọn API. Sử dụng định dạng: "<API>APISearch(query)->answer</API>". Trong định dạng, "query" là đầu vào tìm kiếm mô tả vai trò cụ thể của API cần thiết trong code này, và "answer" là đầu ra tìm kiếm API. Đây là một số ví dụ về lời gọi API:

Input: B = np.reshape(A, (-1, 2))
Output: B = <API>APISearch(Cung cấp một hình dạng mới cho một mảng mà không thay đổi dữ liệu của nó.)->np.reshape</API>np.reshape(A, (-1, 2))
(...hai cặp Input-Output khác...)

Input: {code}
Output:

Hình 4. Một prompt mẫu được sử dụng để tạo ra các bộ dữ liệu tăng cường API cho công cụ tìm kiếm API. Trong cài đặt của chúng tôi, Chúng tôi đã chọn tổng cộng ba cặp đầu vào-đầu ra do con người viết như một phần của prompt, sử dụng ba thư viện: numpy, pandas, và matplotlib.

và cải thiện hiệu quả đào tạo, chúng tôi đề xuất hạn chế số lượng tham số và lớp có thể đào tạo meta trong mô hình được đào tạo trước và áp dụng phương pháp tinh chỉnh hiệu quả tham số có thể thích ứng hiệu quả các mô hình được đào tạo trước với các loại tác vụ mới. Cụ thể, chúng tôi áp dụng LoRA [7] để giảm các tham số có thể đào tạo.

Low-Rank Adaptation (LoRA) là một phương pháp tinh chỉnh hiệu quả tham số dựa trên biểu diễn chiều thấp. Nó tiêm các ma trận rank thấp có thể đào tạo vào các lớp transformer để xấp xỉ các cập nhật trọng số. Đối với một ma trận trọng số được đào tạo trước W∈Rd×k, LoRA biểu diễn cập nhật của nó với một phân tách rank thấp W+δW =W+WdownWup, trong đó Wdown∈Rd×r, Wup∈Rr×k là các tham số có thể điều chỉnh. LoRA thường áp dụng cập nhật này cho các ma trận chiếu tuyến tính attention trong lớp con multi-head attention trong Transformer. Đối với một đầu vào cụ thể x cho chiếu tuyến tính trong multi-head attention, LoRA sửa đổi đầu ra chiếu là:

h←h+s·xWdownWup, (1)

trong đó s≥1 là một siêu tham số vô hướng có thể điều chỉnh. Minh họa của LoRA được hiển thị trong phần giữa của Hình 3.

Trong cài đặt đào tạo của chúng tôi, chúng tôi đóng băng hầu hết các tham số trong mô hình được đào tạo trước và chỉ áp dụng LoRA trên các chiếu query và value trong mô-đun attention cho mỗi lớp transformer. Kết quả là, chúng tôi chỉ cần đào tạo 0,18% tham số trong CodeGen-350M và 0,09% cho CodeGen-2B. Điều này làm cho việc tinh chỉnh hiệu quả các mô hình trên GPU cấp độ người tiêu dùng, chẳng hạn như Nvidia GeForce RTX 2080 (11GB RAM), trở nên khả thi.

Chiến lược tinh chỉnh hiệu quả tham số giảm đáng kể gánh nặng tính toán đào tạo trong các thí nghiệm của chúng tôi. Nó có thể đạt được kết quả có thể so sánh với đào tạo đầy đủ tham số với ít tài nguyên tính toán và thời gian hơn. Chúng tôi sẽ đưa ra phân tích chi tiết về thí nghiệm ablation trong Phần VI-C.

C. Suy luận được tăng cường với Công cụ

Sau khi đào tạo với bộ dữ liệu chú thích, mô hình có thể tạo ra các lời gọi tìm kiếm API trong quá trình tạo code. Mô tả pseudo-code của quá trình giải mã với quy trình công cụ tìm kiếm API có trong Thuật toán 1.

Trong quá trình suy luận, chúng tôi thực hiện giải mã thông thường cho đến khi mô hình tạo ra token ⟨API⟩, cho biết rằng nó tiếp theo mong đợi phản hồi cho một lời gọi API. Tại thời điểm này, chúng tôi tiếp tục quá trình giải mã và ghi lại các token được tạo tiếp theo để lấy truy vấn giữa APISearch( và )→. Sau đó chúng tôi ngắt quá trình giải mã và gọi công cụ tìm kiếm API để nhận phản hồi, và tiếp tục quá trình giải mã sau khi chèn cả phản hồi và token ⟨/API⟩.

--- TRANG 6 ---

Thuật toán 1 Suy luận với Công cụ Tìm kiếm API
1: procedure INFER WITH TOOL(model, input_nl, maxlen)
2:     Chuyển input_nl cho mô hình và nhận token được dự đoán
3:     output ← [token]
4:     i ← 0
5:     while i < maxlen do
6:         token ← token cuối cùng của output
7:         if token = ⟨API⟩ then
8:             query ← các token được tạo tiếp theo giữa APISearch( và )→
9:             response ← Gọi công cụ tìm kiếm API với query
10:            Thêm ⟨API⟩APISearch(query)→response⟨/API⟩ vào output
11:            i ← i + length của quá trình gọi
12:        else
13:            Chuyển token cho mô hình và nhận token được dự đoán
14:            Thêm token được dự đoán vào output
15:            i ← i + 1
16:        end if
17:    end while
18:    return output
19: end procedure

Như đã đề cập trong Phần III, chúng tôi áp dụng các nguồn tìm kiếm API khác nhau cho các loại sử dụng API khác nhau. Đối với những thư viện công khai thường được sử dụng, chúng tôi sử dụng DuckDuckGo, một công cụ tìm kiếm trang web trực tuyến phổ biến, để áp dụng tìm kiếm nội dung trong trang web trong một số trang web được chọn. Đối với những API thư viện ít được biết đến hoặc riêng tư, không có thông tin trực tuyến liên quan. Vì vậy chúng tôi sử dụng điểm BM25 làm chỉ số truy xuất để tìm kiếm từ các tài liệu API tương ứng. Chúng tôi đóng gói các giao diện tìm kiếm này để ToolCoder của chúng tôi có thể gọi các công cụ tìm kiếm với hiệu suất cao. Trong thí nghiệm của chúng tôi, chúng tôi kiểm soát độ trễ tìm kiếm trong vòng 0,6s để đảm bảo hiệu quả cao trong quá trình tạo code.

Sau khi toàn bộ quá trình suy luận kết thúc, chúng tôi sử dụng phương pháp khớp thông thường để loại bỏ phần tìm kiếm API từ code được tạo, tức là phần giữa ⟨API⟩ và ⟨/API⟩ để lấy code được tạo. Bằng cách sử dụng các công cụ tìm kiếm API theo cách này, chúng tôi có thể giải quyết hiệu quả thách thức lựa chọn các API phù hợp và giảm thời gian và công sức cần thiết cho các nhà phát triển để tìm các API phù hợp.

V. THIẾT LẬP THÍ NGHIỆM

Để đánh giá hiệu quả của phương pháp chúng tôi, chúng tôi thực hiện một nghiên cứu quy mô lớn để trả lời bốn câu hỏi nghiên cứu. Trong phần này, chúng tôi mô tả chi tiết về nghiên cứu của chúng tôi, bao gồm các bộ dữ liệu, chỉ số và baseline.

A. Câu hỏi Nghiên cứu

Nghiên cứu của chúng tôi nhằm trả lời bốn câu hỏi nghiên cứu. Trong RQ1, chúng tôi so sánh ToolCoder của chúng tôi với các mô hình tạo code SOTA trên ba benchmark thư viện công khai. Trong RQ2, chúng tôi tiến hành thí nghiệm trên hai benchmark thư viện riêng để cho thấy khả năng tổng quát hóa của mô hình được đề xuất trên những thư viện riêng đó. Trong RQ3, chúng tôi tiến hành nghiên cứu ablation để chứng minh những đóng góp của các mô-đun khác nhau. Trong RQ4, chúng tôi tiến hành một loạt các biện pháp chất lượng trên kết quả được tạo và phân tích hiệu quả và hạn chế của phương pháp chúng tôi thông qua các nghiên cứu trường hợp chi tiết.

RQ1. ToolCoder hoạt động như thế nào so với các baseline SOTA trong việc tạo code thư viện công khai? Để đánh giá hiệu suất của ToolCoder trong việc tạo code thư viện công khai, chúng tôi tiến hành thí nghiệm trên ba benchmark tạo code thư viện công khai, bao gồm numpy, pandas, và torchdata. Chúng tôi so sánh hiệu suất của ToolCoder với các baseline tạo code SOTA hiện có.

RQ2. ToolCoder hoạt động như thế nào trong việc tạo code thư viện riêng? Chúng tôi chọn hai benchmark thư viện riêng nơi các mô hình ngôn ngữ được đào tạo trước chưa bao giờ gặp phải bất kỳ API thư viện riêng nào, và không có thông tin liên quan nào có sẵn trực tuyến. Chúng tôi đánh giá hiệu suất của ToolCoder trên những thư viện riêng này để chứng minh khả năng tổng quát hóa và tính linh hoạt của nó.

RQ3. Những đóng góp của các mô-đun khác nhau trong phương pháp chúng tôi là gì? Pipeline phương pháp của chúng tôi bao gồm ba mô-đun: chú thích dữ liệu, tinh chỉnh và suy luận. Để phân tích hiệu quả của phương pháp chúng tôi, chúng tôi tiến hành nghiên cứu ablation bằng cách thay đổi các cài đặt trong pipeline của chúng tôi, bao gồm các cài đặt bộ dữ liệu, đào tạo và tìm kiếm suy luận.

RQ4. Chất lượng của code được tạo với ToolCoder như thế nào? Chúng tôi đánh giá chất lượng của code được tạo bằng ToolCoder bằng cách thực hiện phân tích nghiên cứu trường hợp. Ngoài ra, chúng tôi phân tích hiệu quả của phương pháp chúng tôi và giải thích tại sao mô hình của chúng tôi hoạt động.

B. Bộ dữ liệu

Các thí nghiệm của chúng tôi được tiến hành trên ba benchmark thư viện công khai, PandasEval, NumpyEval, và TorchDataEval, và hai benchmark thư viện riêng, bao gồm MonkeyEval và BeatNumEval. Chúng tôi chọn những benchmark này để đảm bảo phương pháp được đề xuất của chúng tôi có thể được sử dụng trong các tình huống lựa chọn API khác nhau.

1) Benchmark thư viện công khai: PandasEval [27] là một benchmark tạo phương thức hoặc khối cụ thể theo miền cho thư viện Pandas trong Python. PandasEval chứa 101 ví dụ kiểm tra. Mỗi ví dụ tương ứng với một vấn đề lập trình của Pandas, chứa code ngữ cảnh, thân phương thức mục tiêu (hoặc khối), và nhiều trường hợp kiểm tra. NumpyEval [27] gần như giống với PandasEval, ngoại trừ miền. NumpyEval cụ thể nhắm mục tiêu thư viện Numpy trong Python. Benchmark cũng chứa 101 ví dụ kiểm tra. TorchDataEval [26] dựa trên thư viện TorchData trong Python. TorchData là một thư viện mới được phát hành, có nhiều khả năng không được các mô hình được đào tạo trước nhìn thấy. Do đó, benchmark này được đề xuất để đánh giá mô hình chống lại thư viện chưa được nhìn thấy chứa 50 ví dụ kiểm tra. Trong các thí nghiệm của chúng tôi, bộ dữ liệu được chú thích của chúng tôi không chứa code API liên quan đến TorchData như được hiển thị trong Bảng II, và mô hình được đào tạo trước cơ sở của chúng tôi không chứa những dữ liệu này trong giai đoạn đào tạo trước, vì vậy benchmark này cũng có thể được sử dụng để chứng minh khả năng tổng quát hóa của phương pháp chúng tôi trên những API công khai nhưng chưa từng được nhìn thấy bởi mô hình tạo code.

2) Benchmark thư viện riêng: MonkeyEval [26], được sửa đổi từ PandasEval, được thiết kế để đánh giá mô hình tạo phương thức chống lại thư viện chưa được nhìn thấy. Thư viện Monkey được tạo ra bằng cách sửa đổi tất cả các từ khóa liên quan đến Pandas. ví dụ, "pandas" được chuyển đổi thành "monkey", "dataframe" được chuyển đổi thành "knowledgeframe", v.v.. Quá trình xây dựng thư viện đảm bảo rằng không có thông tin về tên API của những thư viện này bị rò rỉ trong các tài liệu trực tuyến hoặc bất kỳ bộ dữ liệu đào tạo nào. MonkeyEval chuyển đổi tất cả các ví dụ trong PandasEval, dẫn đến 101 ví dụ kiểm tra. BeatNumEval [26] được sửa đổi từ NumpyEval, theo cách tương tự như PandasEval thành MonkeyEval. BeatNumEval cũng có 101 ví dụ kiểm tra. Mô hình được đào tạo trước chưa nhìn thấy API trong MonkeyEval và BeatNumEval, và các nguồn tìm kiếm trực tuyến không thể cung cấp bất kỳ thông tin liên quan đến API nào. Vì vậy việc lựa chọn API trên những benchmark này sẽ chỉ dựa vào công cụ tìm kiếm API mà chúng tôi xây dựng trên tài liệu của những thư viện riêng này.

C. Chỉ số

Theo công việc trước đó, chúng tôi sử dụng chỉ số tỷ lệ pass pass@k [3] để đánh giá hiệu suất và tận dụng các unit test được cung cấp để xác định tính đúng đắn chức năng của các giải pháp code. Đối với mỗi vấn đề, chúng tôi gửi k giải pháp code để đánh giá. Nếu bất kỳ giải pháp code nào trong k giải pháp vượt qua tất cả các trường hợp kiểm tra ground truth, vấn đề được coi là đã giải quyết. Sau đó pass@k là phần trăm các vấn đề đã giải quyết. Trong các thí nghiệm của chúng tôi, chúng tôi đặt k={1,10}.

D. Baseline

Chúng tôi chọn sáu loạt mô hình tạo code gần đây làm baseline, bao gồm một trong những mô hình mạnh nhất, GPT-3.5. Những mô hình này có thể được chia thành hai loại: mô hình tổng quát và mô hình hướng API.

1) Mô hình Tổng quát: CodeT5 [2] là một mô hình encoder-decoder được đào tạo trước cho các tác vụ liên quan đến code. Nó sử dụng tác vụ đào tạo trước nhận biết định danh và đã đạt được kết quả SOTA trên nhiều benchmark tạo code tổng quát. Chúng tôi sử dụng CodeT5-base với 220M tham số trong các thí nghiệm của chúng tôi. PyCodeGPT [27] là một mô hình tạo code được đào tạo trước chỉ decoder với 110M tham số. Nó được khởi tạo với GPT-Neo và được đào tạo tiếp tục với một corpus code quy mô lớn trong Python. CodeGen [14] là một loạt mô hình tạo code được đào tạo trước chỉ decoder với các tham số thay đổi từ 350M đến 16B. Nó coi việc tạo code như một cuộc trò chuyện nhiều lượt giữa người dùng và hệ thống. CodeGen đã cho thấy khả năng mạnh mẽ trên nhiều tác vụ tạo code phức tạp. Do hạn chế tính toán, chúng tôi sử dụng các phiên bản 350M và 2B trong các thí nghiệm của chúng tôi. GPT-3.5 [4, 16] là một trong những mô hình tạo mạnh nhất từ OpenAI. Chúng tôi sử dụng mô hình "gpt-3.5-turbo" vì nó là mô hình hiệu quả chi phí và hiệu suất nhất trong gia đình GPT3.5. Như OpenAI tuyên bố, nó có thể được bổ sung với các khả năng ngôn ngữ tự nhiên và ngôn ngữ lập trình linh hoạt⁶.

2) Mô hình hướng API: CERT [27] là một phương pháp tạo được thiết kế cho code liên quan đến API. CERT chứa hai mô-đun: sketcher và generator, mỗi mô-đun được tinh chỉnh độc lập với PyCodeGPT. Nó đầu tiên dự đoán một sketch dựa trên mô tả NL và tạo ra code hoàn chỉnh dựa trên sketch. Đối với mỗi thư viện, CERT yêu cầu một trọng số được đào tạo đặc biệt để tạo. Chúng tôi sử dụng trọng số được phát hành như hai mô hình độc lập: CERT-numpy, CERT-pandas. CodeGenAPI [26] là một mô hình tạo code hướng API khác. Nó sử dụng pipeline hai giai đoạn để tạo code:

⁶https://platform.openai.com/docs/models/gpt-3-5

BẢNG III
TỶ LỆ PASS CỦA CÁC MÔ HÌNH TRÊN CÁC BENCHMARK THƯ VIỆN CÔNG KHAI

Mô hình | Para. | NumpyEval | | PandasEval | | TorchDataEval | |
--- | --- | --- | --- | --- | --- | --- | ---
| | pass@1 | pass@10 | pass@1 | pass@10 | pass@1 | pass@10
Mô hình Tổng quát | | | | | | |
CodeT5 | 220M | 0 | 0.1 | 0 | 0 | 0 | 0
PyCodeGPT | 110M | 18.04 | 38.61 | 12.75 | 37.62 | 3.80 | 14.00
CodeGen350M | 350M | 18.51 | 43.56 | 16.73 | 29.70 | 4.60 | 14.00
CodeGen2B | 2B | 29.10 | 53.46 | 30.69 | 42.57 | 7.00 | 18.00
GPT3.5 | - | 58.41 | 66.21 | 30.09 | 33.16 | 6.00 | 24.00
Hướng API | | | | | | |
CERT-numpy | 220M | 31.47 | 46.42 | 16.03 | 27.72 | 2.20 | 14.00
CERT-pandas | 220M | 18.81 | 33.66 | 28.42 | 48.04 | 2.80 | 6.00
CodeGenAPI | 350M | 16.55 | 29.48 | 13.58 | 34.95 | 7.19 | 16.93
CodeGenAPI-retrieval | 475M | 12.67 | 27.32 | 11.25 | 28.61 | 10.41 | 23.50
CodeGen-retrieval | 475M | 18.30 | 35.12 | 9.54 | 29.02 | 7.52 | 16.36
Của chúng tôi | | | | | | |
ToolCoder-OnlineTool350M | 350M | 35.64 | 50.50 | 22.77 | 37.62 | 7.40 | 20.00
2B | 2B | 41.58 | 55.44 | 31.68 | 47.52 | 11.80 | 24.00

cho một mô tả NL, CodeGenAPI đầu tiên sử dụng một mô hình retriever được khởi tạo với BERT [5] để tìm API từ tài liệu. Sau đó nó sử dụng một generator được khởi tạo với CodeGen-350M để tạo ra code hoàn chỉnh dựa trên API được truy xuất và mô tả vấn đề. Chúng tôi sử dụng ba cài đặt được phát hành trong bài báo của họ: CodeGenAPI, CodeGen-retrieval, và CodeGenAPI-retrieval. Cài đặt đầu tiên chỉ sử dụng generator được đào tạo mà không có truy xuất, và hai cài đặt sau sử dụng kết quả truy xuất top2 có hiệu suất tốt nhất để hỗ trợ tạo.

E. Chi tiết Triển khai

Đào tạo. Mô hình của chúng tôi được triển khai trong framework Pytorch, và chúng tôi thực hiện tất cả các thí nghiệm trên bốn GPU RTX 2080-11GB. Chúng tôi khởi tạo ToolCoder của chúng tôi bằng cách tận dụng các trọng số được đào tạo trước của CodeGen-350M và CodeGen-2B. Batch size đào tạo được đặt thành 8, và tổng số epoch đào tạo được đặt thành 10. Chúng tôi sử dụng validation loss để xác định checkpoint tốt nhất làm mô hình cuối cùng.

Công cụ. Khi triển khai công cụ tìm kiếm API, chúng tôi áp dụng tìm kiếm trực tuyến trong trang web trong datagy.io cũng như các trang web NumPy⁷, Pandas⁸ và TorchData⁹ sử dụng DuckDuckGo cho các benchmark thư viện công khai. Đối với các benchmark thư viện riêng, chúng tôi sử dụng tài liệu thư viện Monkey và BeatNum được cung cấp để thiết kế một công cụ tìm kiếm API dựa trên thuật toán BM25. Phản hồi của công cụ cho suy luận được coi là API được truy xuất đầu tiên.

Suy luận. Trong quá trình tạo mô hình, chúng tôi sử dụng temperature sampling với T= 0.8 và giới hạn ngân sách mẫu thành 10. Mỗi thí nghiệm được chạy ba lần với các seed ngẫu nhiên và sau đó lấy trung bình cho kết quả cuối cùng.

VI. KẾT QUẢ VÀ PHÂN TÍCH

A. RQ1: Kết quả cho Tạo Code API Thư viện Công khai

Để trả lời RQ1, chúng tôi đánh giá các baseline và ToolCoder của chúng tôi trên NumpyEval, PandasEval và TorchDataEval và kết quả được hiển thị trong Bảng III. ToolCoder-OnlineTool biểu thị

⁷https://numpy.org/doc/
⁸https://pandas.pydata.org/docs/
⁹https://pytorch.org/data/

--- TRANG 8 ---

BẢNG IV
TỶ LỆ PASS CỦA CÁC MÔ HÌNH TRÊN CÁC BENCHMARK THƯ VIỆN RIÊNG

Mô hình | Para. | MonkeyEval | | BeatNumEval | |
--- | --- | --- | --- | --- | ---
| | pass@1 | pass@10 | pass@1 | pass@10
Mô hình Tổng quát | | | | |
CodeT5 | 220M | 0 | 0 | 0 | 0
CodeGen350M | 350M | 0.95 | 4.90 | 5.15 | 11.96
CodeGen2B | 2B | 1.59 | 5.94 | 5.94 | 11.88
GPT3.5 | - | 2.47 | 8.91 | 6.68 | 17.82
Hướng API | | | | |
CodeGenAPI | 350M | 1.19 | 4.68 | 4.44 | 8.24
CodeGenAPI-retrieval | 475M | 3.41 | 8.33 | 5.90 | 11.79
CodeGen-retrieval | 475M | 2.46 | 6.35 | 6.65 | 13.68
Của chúng tôi | | | | |
ToolCoder-DocTool350M | 350M | 2.98 | 5.94 | 6.73 | 12.87
2B | 2B | 3.02 | 7.92 | 6.93 | 13.86

hiệu suất của mô hình chúng tôi với công cụ tìm kiếm trực tuyến để tạo code.

Chúng tôi nhận thấy rằng một số mô hình tạo code tổng quát, chẳng hạn như CodeT5, đã đạt được kết quả kém, điều này chứng minh rằng việc lựa chọn API thư viện công khai có những thách thức đặc biệt đối với các mô hình tạo code. Kết quả cho thấy ToolCoder đạt được kết quả tốt nhất trong số các baseline tạo code tổng quát và các baseline hướng API. Thậm chí so với mô hình cực kỳ lớn GPT3.5, mô hình của chúng tôi có thể đạt được hiệu suất có thể so sánh với những benchmark thư viện công khai này.

So với các baseline hướng API tiên tiến nhất, mô hình của chúng tôi đạt được 10,11%, 3,26%, và 1,39% cải thiện pass@1 so với baseline tốt nhất trên ba benchmark. Thậm chí khi chúng tôi kiểm soát các tham số mô hình của chúng tôi nhỏ hơn các baseline như ToolCoder-350M, mô hình của chúng tôi vẫn có thể đạt được hiệu suất tổng thể xuất sắc. Các mô hình hướng API hiện có chủ yếu tập trung vào đào tạo và suy luận trên một bộ dữ liệu code API thư viện, dẫn đến việc cùng một mô hình không đạt được kết quả tốt trên nhiều benchmark API, chẳng hạn như CERT-numpy và CERT-pandas. Mô hình của chúng tôi cho thấy khả năng tổng quát hóa mạnh hơn và có thể được áp dụng cho các thư viện API khác nhau. Mô hình của chúng tôi có thể đạt được kết quả xuất sắc ngay cả trên thư viện TorchData chưa được nhìn thấy. Mô hình của chúng tôi được đào tạo dựa trên các mô hình CodeGen. Hiệu suất của các mô hình ToolCoder của chúng tôi cao hơn đáng kể so với mô hình CodeGen cơ sở tương ứng, cho thấy rằng quá trình đào tạo và trợ lý công cụ của chúng tôi có thể giúp các mô hình học cách tạo code liên quan đến API tốt hơn.

B. RQ2: Kết quả cho Tạo Code API Thư viện Riêng

Để trả lời RQ2, chúng tôi đánh giá các baseline và ToolCoder của chúng tôi trên MonkeyEval và BeatNumEval. Kết quả được hiển thị trong Bảng IV. ToolCoder-DocTool biểu thị hiệu suất của mô hình chúng tôi với công cụ tìm kiếm tài liệu để tạo code vì những thư viện riêng này không có nguồn tài nguyên trực tuyến liên quan.

Những benchmark thư viện riêng này cực kỳ khó khăn đối với các mô hình tạo code tổng quát, điều mà chúng ta có thể thấy qua các điểm pass@1 và pass@10 nhỏ hơn. Với công cụ tìm kiếm tài liệu được tăng cường, ToolCoder của chúng tôi cho thấy khả năng tổng quát hóa ổn định trên hai benchmark mới này. Khi so sánh với các baseline hướng API tiên tiến nhất, mô hình của chúng tôi

BẢNG V
NGHIÊN CỨU ABLATION VỀ CÀI ĐẶT BỘ DỮ LIỆU. CHÚNG TÔI TIẾN HÀNH THÍ NGHIỆM TRÊN TOOLCODER-350M.

| Cài đặt Bộ dữ liệu | NumpyEval | | PandasEval | | TorchDataEval | |
| --- | --- | --- | --- | --- | --- | ---
| | pass@1 | pass@10 | pass@1 | pass@10 | pass@1 | pass@10
| ToolCoder-350M | 35.64 | 50.50 | 22.77 | 37.62 | 7.40 | 20.00
| bộ dữ liệu gốc | 19.40 | 39.60 | 19.92 | 38.61 | 6.00 | 14.00
| chú thích w/o query | 14.05 | 43.56 | 11.68 | 33.66 | 3.80 | 6.00
| CodeGen-350M | 18.51 | 43.56 | 16.73 | 29.70 | 4.60 | 14.00

cho thấy hiệu suất có thể so sánh. Kết hợp hiệu suất xuất sắc của phương pháp chúng tôi trên các benchmark thư viện công khai, pass@1 trung bình trên năm benchmark của hai loạt ToolCoder của chúng tôi là 15,10%, 19,00%. Đối với chỉ số pass@1 trung bình này, ToolCoder của chúng tôi vượt trội hơn baseline tốt nhất CodeGen-retrieval, chỉ là 8,89%, nâng cao ít nhất 6,21% cải thiện. Đối với pass@10 trung bình, mô hình của chúng tôi vượt trội hơn tất cả các baseline hướng API ít nhất 9,64%. Có thể tự tin rằng ToolCoder của chúng tôi cho thấy hiệu suất tổng thể tốt nhất trên các tình huống lựa chọn API khác nhau.

So với mô hình được đào tạo trước cơ sở CodeGen-350M và CodeGen-2B, mô hình của chúng tôi cải thiện rất nhiều. ToolCoder-350M vượt trội hơn CodeGen-350M cơ sở 2,03%, 1,58% trên pass@1 và 1,04%, 0,91% trên pass@10. ToolCoder-2B cũng đạt được cải thiện tương tự so với CodeGen-2B. Điều này cho thấy rằng các công cụ tìm kiếm tài liệu có thể giúp các mô hình tạo code lựa chọn các API phù hợp trong quá trình suy luận, do đó cải thiện chất lượng của code được tạo. So với mô hình mạnh nhất GPT3.5, ToolCoder của chúng tôi vẫn có thể đạt được kết quả tốt hơn trong một số cài đặt suy luận. Kết quả cho thấy rằng ToolCoder được đề xuất của chúng tôi có thể hỗ trợ quá trình lựa chọn API và tăng cường khả năng của mô hình tạo code.

C. RQ3: Nghiên cứu Ablation

Để trả lời RQ3, chúng tôi điều tra tác động của các mô-đun được thiết kế khác nhau trong pipeline của chúng tôi. Chúng tôi tiến hành các nghiên cứu ablation, bao gồm thay đổi các cài đặt bộ dữ liệu, đào tạo và suy luận trong các thí nghiệm của chúng tôi.

1) Cài đặt Bộ dữ liệu: Chúng tôi thực hiện các thí nghiệm ablation về việc xây dựng bộ dữ liệu trong Bảng V. Chúng tôi thay thế bộ dữ liệu đào tạo của chúng tôi bằng bộ dữ liệu gốc, chỉ chứa mã nguồn thông thường và không có chú thích, được gọi là bộ dữ liệu gốc. Chúng tôi cũng thêm một thí nghiệm để loại bỏ nội dung của truy vấn trong lời gọi tìm kiếm để dạng của nó trở thành APISearch()→answer. Trong quá trình suy luận, chúng tôi sử dụng mô tả câu hỏi để tìm kiếm API trực tiếp. Chúng tôi gọi ablation này là chú thích w/o query. Chúng tôi cũng thêm mô hình CodeGen-350M gốc để so sánh, không được đào tạo trên bộ dữ liệu mới.

Kết quả cho thấy rằng chú thích bộ dữ liệu của chúng tôi là điều cần thiết cho việc cải thiện. So với mô hình được đào tạo trên bộ dữ liệu gốc, ToolCoder-350M của chúng tôi cho thấy cải thiện ổn định trên hầu như tất cả các chỉ số. Bộ dữ liệu chú thích cho phép mô hình của chúng tôi sử dụng công cụ tìm kiếm bên ngoài để lựa chọn API và do đó cải thiện chất lượng của code được tạo. Kết quả cũng cho thấy rằng việc tạo ra truy vấn tìm kiếm là điều cần thiết. Khi chúng tôi loại bỏ

--- TRANG 9 ---

BẢNG VI
NGHIÊN CỨU ABLATION VỀ CÀI ĐẶT ĐÀO TẠO. CHÚNG TÔI TIẾN HÀNH THÍ NGHIỆM TRÊN TOOLCODER-350M.

| Cài đặt Đào tạo | Thời gian Đào tạo | Tham số Đào tạo | NumpyEval | | PandasEval | | TorchDataEval | |
| --- | --- | --- | --- | --- | --- | --- | --- | ---
| | | | pass@1 | pass@10 | pass@1 | pass@10 | pass@1 | pass@10
| ToolCoder-350M | 6h | 0.65M | 35.64 | 50.50 | 22.77 | 37.62 | 7.40 | 20.00
| full-training | 29h | 350M | 35.35 | 58.41 | 22.67 | 40.59 | 6.00 | 22.00

truy vấn tìm kiếm trong việc xây dựng dữ liệu và sử dụng mô tả vấn đề cho các công cụ tìm kiếm API, chúng tôi quan sát thấy sự giảm mạnh trong kết quả cuối cùng như chú thích w/o query trong Bảng V. Chúng tôi cho rằng điều này do mô tả vấn đề vẫn còn xa với việc sử dụng API cụ thể, vì vậy vẫn khó lựa chọn API phù hợp bằng cách sử dụng các công cụ tìm kiếm API hiện có. Chúng tôi cũng có thể xác nhận rằng chỉ tinh chỉnh trên bộ dữ liệu mã nguồn gốc không thể giúp mô hình học cách lựa chọn API. Chúng tôi so sánh CodeGen-350M với mô hình được đào tạo trên bộ dữ liệu gốc. Kết quả cho thấy rằng đào tạo bổ sung trên bộ dữ liệu code không cải thiện đáng kể hiệu suất của mô hình. Chìa khóa cho sự cải thiện của chúng tôi là chú thích công cụ API vào bộ dữ liệu code để dạy mô hình sử dụng các công cụ tìm kiếm API bên ngoài.

2) Cài đặt Đào tạo: Chúng tôi thực hiện các thí nghiệm ablation với ToolCoder-350M về cài đặt đào tạo trong Bảng VI. Các thí nghiệm của chúng tôi so sánh hiệu suất của hai phương pháp: đào tạo tham số đầy đủ, được gọi là full-training. Phương pháp được đề xuất của chúng tôi sử dụng LoRA để đào tạo hiệu quả tham số. Chúng tôi đánh giá hiệu suất của chúng trên các benchmark thư viện công khai và ghi lại chi phí đào tạo của chúng, bao gồm thời gian đào tạo và tham số, sử dụng 2*2080 GPU.

Kết quả cho thấy rằng chiến lược tinh chỉnh của chúng tôi hầu như không có penalty hiệu suất so với full-training thông thường. Trên các benchmark thư viện công khai, sự khác biệt giữa hai kết quả pass@1 nằm trong vòng 0,4%. Khoảng cách trong những kết quả này là có thể chấp nhận được, xem xét việc tiết kiệm chi phí đào tạo khổng lồ. Trong các cài đặt thí nghiệm của chúng tôi, chiến lược tinh chỉnh hiệu quả tham số của chúng tôi có thể giảm thời gian đào tạo từ 29h xuống 6h và các tham số đào tạo từ hơn 350M xuống 0,65M. Chúng tôi chỉ cần đào tạo 0,18% tham số trong CodeGen-350M và 0,09% cho CodeGen-2B. Điều này làm cho việc tinh chỉnh hiệu quả các mô hình trên GPU cấp độ người tiêu dùng, chẳng hạn như Nvidia GeForce RTX 2080 (11GB RAM), trở nên khả thi.

3) Cài đặt Suy luận: Chúng tôi thực hiện các thí nghiệm ablation về cài đặt suy luận trong Bảng VII. Chúng tôi thêm các thí nghiệm để vô hiệu hóa công cụ trong mô hình của chúng tôi. NoTool biểu thị rằng chúng tôi vô hiệu hóa công cụ cho suy luận và sử dụng mô hình được đào tạo của chúng tôi để trực tiếp tạo ra một API dựa trên truy vấn tìm kiếm và hoàn thành code. Chúng tôi so sánh với cài đặt suy luận gốc của chúng tôi trên các benchmark thư viện công khai và riêng.

Các thí nghiệm cho thấy rằng các công cụ bên ngoài của chúng tôi là điều cần thiết trong việc cải thiện hiệu suất. Trên các benchmark thư viện công khai, công cụ tìm kiếm trực tuyến cải thiện pass@1 1,88%, 2,57%, 0,4% cho ToolCoder-350M, và 2,87%, 0,29%, 4,3% cho ToolCoder-2B. Công cụ tìm kiếm trực tuyến có thể tìm kiếm các tình huống sử dụng API tương tự và cung cấp API chính xác

BẢNG VII
NGHIÊN CỨU ABLATION VỀ CÀI ĐẶT SUY LUẬN.

(a) Trên các benchmark thư viện công khai

| Cài đặt Suy luận | NumpyEval | | PandasEval | | TorchDataEval | |
| --- | --- | --- | --- | --- | --- | ---
| | pass@1 | pass@10 | pass@1 | pass@10 | pass@1 | pass@10
| OnlineTool-350M | 35.64 | 50.50 | 22.77 | 37.62 | 7.40 | 20.00
| NoTool-350M | 33.76 | 46.53 | 20.19 | 35.64 | 6.00 | 16.00
| OnlineTool-2B | 41.58 | 55.44 | 31.68 | 47.52 | 11.80 | 24.00
| NoTool-2B | 38.71 | 54.45 | 31.38 | 44.55 | 7.50 | 20.00

(b) Trên các benchmark thư viện riêng

| Cài đặt Suy luận | MonkeyEval | | BeatNumEval | |
| --- | --- | --- | --- | ---
| | pass@1 | pass@10 | pass@1 | pass@10
| OnlineTool-350M | 2.98 | 5.94 | 6.73 | 12.87
| NoTool-350M | 0.29 | 0.99 | 1.68 | 4.95
| OnlineTool-2B | 3.02 | 7.92 | 6.93 | 13.86
| NoTool-2B | 0.79 | 2.97 | 2.77 | 8.91

gợi ý. Khi xem xét các benchmark thư viện riêng, sự cải thiện còn đáng kể hơn. Chúng tôi thấy bản thân mô hình hoạt động kém trên các thư viện riêng. Tuy nhiên, với sự hỗ trợ của công cụ tìm kiếm tài liệu, mô hình của chúng tôi có thể chọn một API thư viện riêng phù hợp.

Một quan sát thú vị khác là NoTool cũng đạt được hiệu suất tương đối tốt trên các benchmark thư viện công khai. Chúng tôi tin rằng sự cải thiện đến từ quá trình chú thích bộ dữ liệu của chúng tôi. Quá trình gọi công cụ bổ sung trong bộ dữ liệu có thể được xem như một cách để suy nghĩ về và chọn API. Chuỗi suy nghĩ trong bộ dữ liệu chú thích có thể hỗ trợ mô hình tạo code hiểu rõ hơn các chức năng và tình huống ứng dụng của các API khác nhau, do đó trực tiếp cải thiện mô hình để lựa chọn API. Tuy nhiên, đối với các thư viện riêng, vì kiến thức về các thư viện riêng không được nhìn thấy bởi mô hình tạo code, dạng chú thích bộ dữ liệu này thách thức mang lại cải thiện cho mô hình. Với các công cụ tìm kiếm API phù hợp được tăng cường, ToolCoder của chúng tôi có thể lựa chọn API chính xác hơn và cải thiện thêm.

D. RQ4: Phân tích Định tính

Để trả lời RQ4, chúng tôi thực hiện phân tích nghiên cứu trường hợp để đánh giá chất lượng của code được tạo. Hình 5 biểu thị các đoạn code được tạo trên các benchmark thư viện công khai và riêng. Từ các ví dụ, chúng tôi thu được những phát hiện sau: ❶Truy vấn tìm kiếm được tạo cung cấp kế hoạch kỹ thuật chi tiết hơn cho giải pháp. Trường hợp NumpyEval yêu cầu tính tổng các giá trị trong dataframe, và truy vấn được tạo phân tích yêu cầu, tập trung đầu tiên vào việc tính tổng các mảng. Nó lấp đầy khoảng cách giữa yêu cầu và các API cụ thể. ❷Phản hồi của các công cụ tìm kiếm đều đóng vai trò quan trọng trong code được tạo. Công cụ tìm kiếm trực tuyến tìm thấy API phù hợp từ các trang web chính xác, và công cụ tìm kiếm tài liệu tìm thấy API phù hợp bằng cách tìm kiếm các bình luận API. ❸ToolCoder của chúng tôi cũng có thể thực hiện các sửa đổi cần thiết dựa trên phản hồi công cụ. Ví dụ, công cụ tìm kiếm trực tuyến trả về phản hồi là cumsum, không được định nghĩa trực tiếp trong code đầu vào.

--- TRANG 10 ---

NumpyEval/99

BeatNumEval/92

Input: import beatnum as bn 
master = bn.numset([1,2,3,4,5]) 
search = bn.numset([4,2,2,3]) 
# Tìm các chỉ số của một danh sách giá trị trong một numset beatnum

Output: out = <API>APISearch(Tìm các chỉ số nơi các phần tử nên được chèn để duy trì thứ tự.)->

🔍Tìm các chỉ số nơi các phần tử nên được chèn để duy trì thứ tự
✅Tài liệu BeatNum
find_sorted: tìm các chỉ số vào một numset được sắp xếp a sao cho nếu các phần tử tương ứng trong v được chèn trước các chỉ số thì thứ tự của a sẽ được bảo toàn

out = <API>APISearch(Tìm các chỉ số nơi các phần tử nên được chèn để duy trì thứ tự.)->bn.find_sorted</API>bn.find_sorted(master, search)

Input: import numpy as np 
import pandas as pd 
df = pd.DataFrame({'A': [5, 6, 7], 'B': [7, 8, 9]}) 
# Cách tốt nhất để tính tổng tất cả các giá trị trong một Pandas dataframe là gì? 
# kết quả là một giá trị số

Output: sum_value = <API>APISearch(Tính tổng các giá trị trong mảng đã cho.)->

🔍Tính tổng các giá trị trong mảng đã cho
✅https://datagy.io/numpy-cumsum/-> cumsum

sum_value = <API>APISearch(Tính tổng các giá trị trong mảng đã cho.)->cumsum</API>np.cumsum(df.values)[-1]

Hình 5. Nghiên cứu Trường hợp của ToolCoder-2B, với công cụ tìm kiếm trực tuyến trên NumpyEval và công cụ tìm kiếm tài liệu trên BeatNumEval.

ToolCoder của chúng tôi có thể thêm một số thành phần không có trong phản hồi và tạo ra API chính xác np.cumsum.

VII. CÁC MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ

Các mối đe dọa đối với tính hợp lệ nội bộ liên quan đến vai trò của kiến trúc mô hình và cài đặt siêu tham số. Trong các thí nghiệm của chúng tôi, chúng tôi thực hiện tìm kiếm lưới phạm vi nhỏ về cài đặt learning rate và batch size. Mô hình ToolCoder-350M của chúng tôi cố gắng giữ các siêu tham số giống như các mô hình baseline để so sánh công bằng.

Các mối đe dọa đối với tính hợp lệ ngoại bộ chủ yếu liên quan đến các tác vụ và bộ dữ liệu mà chúng tôi chọn trong bài báo này. Chúng tôi đối phó với điều này bằng cách đánh giá mô hình của chúng tôi trên năm benchmark khác nhau của hai loại API, bao gồm tạo code API thư viện công khai và riêng.

Các mối đe dọa đối với tính hợp lệ cấu trúc bao gồm các chỉ số đánh giá mà chúng tôi sử dụng trong công việc này. Chúng tôi sử dụng tỷ lệ pass để đánh giá tính đúng đắn của code được tạo một cách chính xác. Chỉ số này là đầy đủ cho các tác vụ tương ứng và đã được áp dụng bởi nhiều nghiên cứu trước đó.

VIII. CÔNG VIỆC LIÊN QUAN

A. Tạo Code

Tạo code nhằm mục đích tạo ra mã nguồn thỏa mãn một mô tả hoặc yêu cầu ngôn ngữ tự nhiên đã cho. Nó liên quan đến việc tự động tạo mã nguồn dựa trên các yêu cầu chức năng, chẳng hạn như mô tả ngôn ngữ tự nhiên [9] hoặc thuật toán pseudo code [10, 15, 25]. Gần đây, các mô hình ngôn ngữ được đào tạo trước đã cho thấy khả năng ấn tượng trong các tác vụ tạo code. Lu et al. [11] thích ứng mô hình GPT-2 [18] trên mã nguồn, tạo ra CodeGPT. Chen et al. [3] tinh chỉnh các mô hình GPT-3 [4] trên code để tạo ra CodeX và GitHub Copilot. OpenAI cũng tạo ra loạt mô hình GPT3.5, đã cho thấy khả năng tạo mạnh mẽ trong ngôn ngữ tự nhiên và ngôn ngữ lập trình. Cả CodeX và GPT3.5 đều không phải là mã nguồn mở, dẫn đến một số nỗ lực nhân bản CodeX trong công nghiệp và học thuật, tạo ra GPT-Neo [1], GPT-J [21], CodeParrot [22], PolyCoder [23], PyCodeGPT [27], InCoder [6], và CodeGen [14]. Trong các thí nghiệm của chúng tôi, chúng tôi chọn loạt mô hình CodeGen làm mô hình cơ sở để khám phá thêm.

Gần đây, một số công việc đã tập trung vào việc lựa chọn API trong quá trình tạo code. Như đã thảo luận trong Phần II-A, các mô hình tạo code hiện có vẫn gặp khó khăn với việc lựa chọn các API phù hợp cho một ngữ cảnh đã cho, đặc biệt là đối với các API riêng hoặc ít được biết đến. Công việc hiện có [26, 27, 29] đã đề xuất một số phương pháp tạo code hướng API. Chúng thường sử dụng pipeline hai giai đoạn, trong đó giai đoạn đầu tiên liên quan đến việc tìm kiếm hoặc tạo ra các API liên quan và sau đó sử dụng chúng để tạo code. Chúng tôi theo đuổi hướng nghiên cứu này và đề xuất tận dụng các mô hình được đào tạo trước và các công cụ tìm kiếm API để tự động hóa việc lựa chọn API trong thực hành lập trình. So sánh, phương pháp của chúng tôi có hai ưu điểm: ❶Phương pháp của chúng tôi cho thấy khả năng tổng quát hóa mạnh. Bằng cách thiết lập một công cụ tìm kiếm API phù hợp, phương pháp của chúng tôi có thể nhanh chóng thích ứng với bất kỳ tình huống tạo code liên quan đến API nào. ❷Phương pháp của chúng tôi không yêu cầu tạo đa giai đoạn. Thay vào đó, chúng tôi tích hợp công cụ tìm kiếm API vào quá trình giải mã, làm cho phương pháp của chúng tôi linh hoạt hơn và cho phép quá trình lựa chọn API gần với đoạn code cụ thể đang được tạo hơn.

B. Các Mô hình Ngôn ngữ Lớn Được Tăng cường Công cụ

Nghiên cứu gần đây trong mô hình ngôn ngữ đã khám phá việc sử dụng các công cụ bên ngoài để bổ sung kiến thức được lưu trữ trong các trọng số của mô hình [12]. Những công cụ bên ngoài này có thể bao gồm các mạng neural khác hoặc thậm chí chính mô hình ngôn ngữ, cho phép kết hợp các mô hình được đào tạo trước khác nhau trên các phương thức khác nhau, chẳng hạn như Socratic Model [28]. Ngoài ra, kiến thức ngôn ngữ tự nhiên có thể được truy xuất từ các nguồn bên ngoài, như đã được chứng minh bởi WebGPT [13] và ReAct [24] thông qua việc sử dụng các API tìm kiếm. Các phương pháp khác, chẳng hạn như Toolformer [20] và ART [17], tận dụng sự kết hợp của các công cụ tìm kiếm, công cụ hỏi đáp, công cụ dịch máy, máy tính và các công cụ khác để giải quyết các tác vụ NLP khác nhau. ChatGPT Plugins¹⁰ tiếp tục chứng minh tiềm năng cho các mô hình ngôn ngữ tích hợp với hàng nghìn đến hàng triệu công cụ. Tuy nhiên, việc kết hợp các công cụ lập trình vào các mô hình liên quan đến code chưa được khám phá. Bài báo của chúng tôi giải quyết khoảng trống này bằng cách trừu tượng hóa quá trình lập trình viên con người lựa chọn API thành một công cụ lập trình tăng cường các mô hình tạo code.

¹⁰https://openai.com/blog/chatgpt-plugins

--- TRANG 11 ---

IX. KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất ToolCoder, một phương pháp mới kết hợp các công cụ tìm kiếm API vào quá trình tạo code để hỗ trợ các mô hình trong việc lựa chọn các API phù hợp. Chúng tôi phân loại các công cụ tìm kiếm API thành hai loại, bao gồm các công cụ tìm kiếm trực tuyến và các công cụ tìm kiếm tài liệu, và trừu tượng hóa chúng thành một dạng thống nhất. Chúng tôi đề xuất một phương pháp chú thích bộ dữ liệu tự động để thêm thông tin sử dụng công cụ vào dữ liệu mã nguồn. Chiến lược hiệu quả tham số được sử dụng để tinh chỉnh mô hình. Trong quá trình suy luận, quá trình giải mã mô hình được tăng cường với các công cụ tìm kiếm API bên ngoài để lựa chọn API phù hợp. Các thí nghiệm trên các benchmark tạo code thư viện công khai và riêng cho thấy rằng ToolCoder của chúng tôi vượt trội hơn các phương pháp tiên tiến nhất, với ít nhất 6,21% cải thiện trên các chỉ số pass@1 trung bình. Các thí nghiệm của chúng tôi cũng chứng minh tiềm năng của việc kết hợp các công cụ lập trình vào quá trình tạo code, làm sáng tỏ hướng công việc tương lai này.

TÀI LIỆU THAM KHẢO

[1] Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Nếu bạn sử dụng phần mềm này, vui lòng trích dẫn nó bằng metadata này 58 (2021).

[2] Nghi Bui, Yue Wang, và Steven C. H. Hoi. 2022. Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5. Trong Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (Eds.). Association for Computational Linguistics, 812–823. https://aclanthology.org/2022.findings-emnlp.57

[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374

[4] Zekai Chen, Mariann Micsinai Balan, và Kevin Brown. 2023. Language Models are Few-shot Learners for Prognostic Prediction. CoRR abs/2302.12692 (2023). https://doi.org/10.48550/arXiv.2302.12692 arXiv:2302.12692

[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, và Thamar Solorio (Eds.). Association for Computational Linguistics, 4171–4186. https://doi.org/10.18653/v1/n19-1423

[6] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. 2022. InCoder: A Generative Model for Code Infilling and Synthesis. CoRR abs/2204.05999 (2022). https://doi.org/10.48550/arXiv.2204.05999 arXiv:2204.05999

[7] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id=nZeVKeeFYf9

[8] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http://arxiv.org/abs/1909.09436

[9] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, và Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 1643–1652. https://doi.org/10.18653/v1/d18-1192

[10] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, và Percy S Liang. 2019. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems 32 (2019).

[11] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. Trong Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren và Sai-Kit Yeung (Eds.).

[12] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, và Thomas Scialom. 2023. Augmented Language Models: a Survey. CoRR abs/2302.07842 (2023). https://doi.org/10.48550/arXiv.2302.07842 arXiv:2302.07842

[13] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, và John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. CoRR abs/2112.09332 (2021). arXiv:2112.09332 https://arxiv.org/abs/2112.09332

[14] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2022. A Conversational Paradigm for Program Synthesis. CoRR abs/2203.13474 (2022). https://doi.org/10.48550/arXiv.2203.13474 arXiv:2203.13474

[15] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, và Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation. Trong 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 574–584.

[16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, và Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR abs/2203.02155 (2022). https://doi.org/10.48550/arXiv.2203.02155 arXiv:2203.02155

[17] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, và Marco Túlio Ribeiro. 2023. ART: Automatic multi-step reasoning and tool-use for large language models. CoRR abs/2303.09014 (2023). https://doi.org/10.48550/arXiv.2303.09014 arXiv:2303.09014

[18] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.

[19] Stephen Robertson và Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends in Information Retrieval 3 (01 2009), 333–389. https://doi.org/10.1561/1500000019

[20] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. CoRR abs/2302.04761 (2023). https://doi.org/10.48550/arXiv.2302.04761 arXiv:2302.04761

[21] Ben Wang và Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.

[22] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, Online, 38–45. https://www.aclweb.org/anthology/2020.emnlp-demos.6

[23] Frank F. Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. Trong MAPS@PLDI 2022: 6th ACM SIGPLAN International Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022, Swarat Chaudhuri và Charles Sutton (Eds.). ACM, 1–10. https://doi.org/10.1145/3520312.3534862

[24] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, và Yuan Cao. 2022. ReAct: Synergizing Reasoning and Acting in Language Models. CoRR abs/2210.03629 (2022). https://doi.org/10.48550/arXiv.2210.03629 arXiv:2210.03629

[25] Pengcheng Yin và Graham Neubig. 2018. TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation. arXiv preprint arXiv:1810.02720 (2018).

[26] Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, và Jian-Guang Lou. 2022. When Language Model Meets Private Library. Trong Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (Eds.). Association for Computational Linguistics, 277–288. https://aclanthology.org/2022.findings-emnlp.21

[27] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, và Jian-Guang Lou. 2022. CERT: Continual Pre-training on Sketches for Library-oriented Code Generation. Trong Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, Luc De Raedt (Ed.). ijcai.org, 2369–2375. https://doi.org/10.24963/ijcai.2022/329

[28] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, và Pete Florence. 2022. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. CoRR abs/2204.00598 (2022). https://doi.org/10.48550/arXiv.2204.00598

--- TRANG 13 ---

arXiv:2204.00598

[29] Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, và Graham Neubig. 2023. DocPrompting: Generating Code by Retrieving the Docs. Trong The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=ZTCxT2t2Ru
