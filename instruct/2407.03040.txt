# 2407.03040.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2407.03040.pdf
# File size: 963625 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction
Tuning for Large Language Model
Xia HOU1, Qifeng Li1, Jian Yang2‚àó, Tongliang Li1, Linzheng Chai2,
Xianjie Wu2,Hangyuan Ji2,Zhoujun Li2,Jixuan Nie1,Jingbo Dun1,Wenfeng Song1,
1Computer School, Beijing Information Science & Technology University;
2State Key Laboratory of Complex & Critical Software Environment, Beihang University
{houxia, liqifeng, tonyliangli, niejixuan, dunjingbo, songwenfeng}@bistu.edu.cn;
{jiaya, challenging, wuxianjie, jhy_1, lizj}@buaa.edu.cn
Abstract
Instruction tuning as an effective technique
aligns the outputs of large language models
(LLMs) with human preference. But how to
generate the seasonal multi-turn dialogues from
raw documents for instruction tuning still re-
quires further exploration. In this paper, we
present a novel framework named R2S that
leverages the CoD‚ÄîChain of Dialogue logic to
guide large language models (LLMs) in gener-
ating knowledge-intensive multi-turn dialogues
for instruction tuning. By integrating raw doc-
uments from both open-source datasets and
domain-specific web-crawled documents into
a benchmark K-BENCH , we cover diverse ar-
eas such as Wikipedia (English), Science (Chi-
nese), and Artifacts (Chinese). Our approach
first decides the logic flow of the current di-
alogue and then prompts LLMs to produce
key phrases for sourcing relevant response con-
tent. This methodology enables the creation
of the GINSTRUCT instruction dataset, retain-
ing raw document knowledge within dialogue-
style interactions. Utilizing this dataset, we
fine-tune GLLM , a model designed to trans-
form raw documents into structured multi-turn
dialogues, thereby injecting comprehensive do-
main knowledge into the SFT model for en-
hanced instruction tuning. This work signifies
a stride towards refining the adaptability and
effectiveness of LLMs in processing and gen-
erating more accurate, contextually nuanced
responses across various fields.
1 Introduction
The evolution of large language models (LLMs),
including advancements seen in the generative
pre-trained Transformer (GPT) series by OpenAI,
marks a significant milestone in the field of natu-
ral language processing (NLP). A pivotal strategy
in their development has been instruction-tuning,
which leverages human-curated prompts, feedback,
‚àóCorresponding Author.
Large language models (LLMs), such as GPT3.5, GPT4, Llama, Lama2, Claude3, can understand, generate, and interact with human language. Document
LLM
Continue Pre-trainingDocuments
Dialogues
Fine-tuned LLM(a) Standard Domain-specific TrainingLarge language models, often abbreviated as LLMs, generate, and interact with human language. 
LLM
Supervised Fine-tuning (SFT)
Multi-turnDialogues
Fine-tuned LLM(b) Domain-specific Supervised Fine-tuning
What are large language models?Can you give examples of these models?Sure! Some examples include GPT3.5, GPT4, Llama, Lama2, and Claude3. 
Supervised Fine-tuningFigure 1: Comparison of standard domain-specific train-
ing with our proposed R2S.
and benchmark datasets to tailor LLMs‚Äô adaptabil-
ity to specific domains, such as complex reasoning
(Wei et al., 2022; Chai et al., 2024; Li et al., 2024)
and coding (Rozi√®re et al., 2023; Li et al., 2023a).
Instruction tuning (Wang et al., 2023) emerges as
an innovative approach, creating new tasks with
bespoke instructions, thus enhancing model perfor-
mance and cost-effectiveness. The diversity and
scope of instruction data are critical for the model‚Äôs
ability to generalize and excel in previously unseen
tasks, underpinning the continuous advancement
and specialization of LLMs in various fields.
Instruction tuning is a groundbreaking technique
designed to fine-tune these powerful LLMs for bet-
ter task-specific performance without intensive re-
training on massive datasets, which delicately re-
calibrates the response of LLMs by giving them
instructions or prompts that are carefully crafted to
elicit more accurate, contextually appropriate, or
nuanced outputs. Many recent works (Sun et al.,
2023; Luo et al., 2023) try to synthesize instruc-
tions, input, and output samples from an LLM and
then filter invalid or similar ones. However, these
methods focus on generating diverse single-turn
dialogues based on the query or response. In Fig-
ure 1, continuing pre-training with raw documents
and instruction tuning with human-annotated SFT
data can inject domain-specific knowledge, but thearXiv:2407.03040v1  [cs.CL]  3 Jul 2024

--- PAGE 2 ---
process requires two-stage training and large-scale
data. Therefore, How to produce reasonable multi-
turn dialogues for instruction turning to inject the
knowledge of raw documents into LLMs only with
instruction tuning.
In this paper, we propose a framework to con-
struct the Supervised fine-tuning dataset from the
Raw documents ( R2S ), which leverages the Chain
ofDialogue logic (CoD) to guide the LLM to cre-
ate the reasonable knowledge-intensive multi-turn
dialogues for instruction tuning. Specifically, we
collect the existing documents from open-source
dataset (Katyayan and Joshi, 2022; Yang et al.,
2023) and crawl the domain-specific documents
from websites (in the field of the artifacts) to cre-
ate a knowledge-intensive benchmark K-BENCH ,
comprised of three tasks, including Wikipedia (En-
glish), Science (Chinese), and Artifacts (Chinese).
Then, we introduce the chain of dialogue logic to
first decide the type (e.g. Opinion Exchange Q&A,
Informational Q&A, or Task-oriented Q&A) of the
current turn in multi-turn dialogue. subsequently,
we prompt the LLM to generate key phrases to
search relevant spans for response generation. In
this way, we successfully create the instruction
dataset GINSTRUCT , which can keep the knowl-
edge of the raw documents as much as possible in
the style of the dialogue. Based on the raw docu-
ments, GINSTRUCT , we can fine-tune GLLM based
on open-source LLMs, which aims at converting
raw documents into multi-turn dialogues. Finally,
we can use GLLM to generate the multi-turn SFT
data to inject the knowledge into the SFT model.
Extensive experiments of R2S are evaluated on
our created benchmark K-BENCH . The results
demonstrate that the synthetic instruction corpora
can effectively inject the knowledge of raw docu-
ments into the SFT model, notably getting excel-
lent performance under multiple evaluation metrics.
The fine-tuned generator GLLM further verifies the
effectiveness of CoD, leading to a reasonable and
natural multi-turn dialogue. The contributions in
this work are summarized as follows:
‚Ä¢We propose the chain of dialogue logic (CoD),
which ingeniously guides LLMs to produce
reasonable and natural knowledge-intensive
multi-turn dialogues for instruction tuning. It
allows the LLMs to generate dialogues that
are coherent and contextually relevant but also
embed rich, domain-specific knowledge into
these conversations.‚Ä¢We create a knowledge-intensive benchmark
(K-BENCH ) to facilitate the training and evalu-
ation of the proposed methods, this work con-
tributes a comprehensive knowledge-intensive
benchmark, K-BENCH , covering a diverse
range of topics‚Äîincluding Wikipedia (En-
glish), Science (Chinese), and Artifacts (Chi-
nese)‚Äî K-BENCH serves as a vital resource
for assessing the effectiveness of CoD and
the overall framework in handling complex,
knowledge-driven dialogue tasks.
‚Ä¢The work outlines the creation of GINSTRUCT ,
a synthetic instruction dataset that retains an
extensive amount of knowledge from the raw
documents in a dialogue format, which is used
to fine-tune an open-source LLM, referred to
asGLLM , which is specifically designed to
transform raw documents into cohesive multi-
turn dialogues. The experimental results from
evaluating R2S on the K-BENCH benchmark
demonstrate that this synthetic instruction ap-
proach is highly effective in enhancing the
SFT model, enabling it to excel across var-
ious performance metrics. The fine-tuned
generator GLLM is further proof of the ef-
ficiency of the CoD methodology, producing
dialogues that are both logical and rich in
domain-specific knowledge.
2 Problem Definition
Pre-training During pre-training, the LLM
learns to predict the next word in a document
based on the prefix context, enabling the LLM to
capture linguistic patterns, syntax, semantics, and
even some world knowledge embedded in the text.
Given the raw document x‚ààDd, the pre-training
can be described as:
P(x) =KY
k=1P(xk|x:k‚àí1) (1)
where xis the document with Ktokens.
Instruction Tuning Supervised fine-tuning
(SFT) is the process of taking a pre-trained model,
like GPT, and adapting it to a specific task by
training on a labeled dataset, which is typically
much shorter than the initial pre-training phase
since the model has already learned a wealth of
language features. Given the query and response
(q, a)‚ààDs, the single-turn instruction tuning can

--- PAGE 3 ---
Step1:Raw Documents
Document: Zigui County, is located in the west of Hubei Province, at the head of the damreservoir of the Three Gorges Project,......User: Hello, I am a local resident of Zigui County and would like to know what are the touristattractions in the county?" ,Dialogue Types: [Question-Answer]Search: [ZiguiCounty tourist attractions]Find: [Zigui County has a number of tourist attractions, such as Qu Yuan's hometown, QuYuan Temple, reading cave, face mirror, Sword Gorge.]Assistant:Hello, Zigui County has many famous tourist attractions, including Qu Yuan Guli, Qu Yuan Temple, Reading Cave, face mirror, Bingshu Sword Gorge, etc. These attractions have unique cultural and natural features.Chain of Dialogue Logic1stTurn
2nd Turn
Logic FlowUser: I think Sword Gorge is the most famous attraction fortourists" ,Dialogue Types: [Opinion-Rebuttal]Search: [Sword Gorge]Find: [face mirror, Sword Gorge.]Assistant: The most famous attraction is QuYuan Templenot Sword Gorge.  Step 2:Synthetic Multi-turn SFT
Generator LLM (gLLM)
‚Ä¶ 
Instruction TuningStep 3: Comprehensive Evaluation
Turn 1Turn ùëáInformation: 4.5Understanding: 5.0Helpfulness: 4.5Loyalty: 4.0Flexibility: 4.0‚Ä¶ Information: 4.5Understanding: 5.0Helpfulness: 4.5Loyalty: 4.0Flexibility: 4.0Consistency: 5.0    Coherence: 4.5    Interactivity: 4.5Turn 1~ùëÅOverall Dialogue Evaluation Criteria:1. Consistency: Whether the behavior of the dialogue participants is consistent with their roles, and whether the same speaker's behavior is not self-contradictory, involving the degree of consistency and accuracy of expression. Additionally, reducing dialogue content to maintain consistency is not best practice;‚Ä¶ Figure 2: Framework of R2S.
be described as:
P(a) =nY
i=1P(ai|a:i‚àí1, q;M) (2)
where qis the query, ais the response, a:i‚àí1is the
previous context from the 1-th to the i-1-th token.
Instruction Tuning with Raw Text If we only
have the dataset Ddwith a limited number of
raw documents, which are unable to support the
domain-specific pre-training and following SFT,
we can create muli-turn dialogues from Ddto in-
ject domain-specific knowledge into LLMs by in-
struction tuning. We create the multi-turn dialogue
{q(t), a(t)}T
t=1‚ààDsfromDdwith raw documents
to fine-tune the LLM Mas:
P({a(t)}T
t=1) =TY
t=1P(a(t)|a(:t‚àí1), q(:t);M) (3)
where a(t)andq(t)aret-th turn of the dialogures.
a(:t‚àí1)andq(t)are previous t‚àí1queries and t
answers.
3 Benchmark
3.1 Data Collection & Statisitcs
In Table 1, our created benchmark comprises three
distinct sections categorized by topics. we create
10,428English instruction instances and a test size
of1,041items from the documents of the Squad
V2.0 dataset. Then, the science dataset contains
9,540instruction instances and 955 test items from
10,000documents. The dataset in Artifacts utilizes
data extracted from a website crawler, compris-
ing6,152instructions, and 615test items fromEn Zh Zh
Type Wikipedia Science Artifacts
Source Squad V2 RefGPT Website Crawler
#Doc Size 10989 10000 6817
#Instruction Size 10428 9540 6152
#Test Size 1041 955 615
Max Doc Characters 4001 1526 1307
Avg Doc Characters 1265 272 584
Max Instruction Characters 14009 4866 5445
Avg Instruction Characters 7878 3656 3922
Table 1: Data statistics. ‚Äú#‚Äù denotes the number of
data examples. The maximum and average characters
refer to the maximum and average characters of a single
document or instruction dataset example.
6,872documents. This dataset reflects a diverse
collection strategy, spanning different languages
(English and Chinese), content types (General and
Artifacts), and sources (Squad V2, RefGPT, and a
website crawler), aiming to provide comprehensive
resources for domain-specific research.
3.2 Evaluation Metric
To evaluate the generated multi-turn dialogues, we
design the following metrics using the LLM (GPT-
4) with the score range ({1: ‚Äòvery bad‚Äô, 2: ‚Äòbad‚Äô, 3:
‚Äòneutral‚Äô, 4: ‚Äògood‚Äô, 4: ‚Äòvery good‚Äô }).
Informativeness (Info): Info is used to accu-
rately whether the query articulates their problem
or viewpoint, including the amount of key informa-
tion, word count, and precision of expression.
Understanding (US): US is used to evaluate the
relevance between queries and the corresponding
responses. For each turn of the dialogue, we calcu-
late the score of US and average them to get a final
score.

--- PAGE 4 ---
Usefulness (UF): UF is introduced to decide
whether the responses help the user solve their prob-
lem or confirm or correct their viewpoints (this may
be based on facts or external documents).
Fidelity (FD): FD judges where the response
involves factual document information and factual
knowledge used in the document.
Flexibility (FL): For the dialogue without the
specific knowledge, FLis used to judge whether
the response can correctly handle the query using
the common knowledge in LLMs.
Consistency (CS): Whether the behavior of con-
versation participants aligns with their roles and the
behavior of the same speaker is consistent, involv-
ing the consistency and accuracy of expression.
Cohesion (CO): : Whether transitions in the con-
versation, topic deepening, and switches between
topics are natural.
Interactivity (IA): Whether the user and assis-
tant express and share emotions during the ex-
change, including the accuracy of emotional ex-
pression and accurate understanding of emotions.
Coverage Rate (CR): For multi-round dialogs
constructed on the basis of documents, CRis used
to evaluate the proportion of content covered by
the dialogs over the content of the documents.
4 Framework of R2S
Figure 2 describes the overall framework of our
method.
4.1 Logic Definition
To create the multi-turn dialogue, we consider the
following characteristic of the multi-turn dialogue:
(1)Contextual Relevance : In multi-turn dialogues,
each round of conversation is related to the context
before and after it. Each answer is based on the
previous round‚Äôs question, and the next round‚Äôs
question may be based on the last answer. This
coherence is a critical characteristic of multi-turn
dialogues. (2) Continuity : In multi-turn dialogues,
the exchange between the user and the assistant is
continuous, not isolated single questions and an-
swers, but a series of questions and answers. (3)
Dialogue Depth : In multi-turn dialogues, users
may delve deeper into a topic with their inquiries
or follow up on the responses received with further
questions. This requires the assistant to have thecapability to manage complex dialogues and under-
stand the depth of the conversation. (4) Topic Tran-
sition : In multi-turn dialogues, users may switch
topics during the conversation. This demands the
assistant to be flexible in response and able to an-
swer on new topics. (5) Naturalness of Dialogue :
In multi-turn dialogues, the assistant‚Äôs responses
need to be as natural and fluid as possible, making
the user feel as though they are communicating
with a person, not a machine.
Inspired by these characteristics of the multi-
turn dialogue, we pre-define the six logic types
of each turn: (1) Question-Answer: This is the
most common logical sequence in dialogue, where
Character A asks a question and Character B pro-
vides an answer. For example, ‚ÄòHow old are you?‚Äô,
‚ÄòI‚Äôm 23 years old.‚Äô. (2) Question-Question: This
is a logical process where an intent to ask is com-
pleted. Character A asks a relatively vague ques-
tion, and if Character B needs to clarify the intent of
Character A‚Äôs question, Character B can continue
by asking another question. For example, ‚ÄòHow
old are you?‚Äô, ‚ÄòAre you asking about my age?‚Äô.
(3) Statement-Inquiry: Character A makes a state-
ment, and Character B asks for or requires more
information. For example, "A: I went to the mu-
seum today." "Oh, what interesting exhibitions did
you see?" (4) Statement-Explanation: Character A
states a fact, and then Character B explains or elab-
orates on the information related to that fact. For
example, ‚ÄòYao Ming won the CBA championship
in 2002.‚Äô, ‚ÄòB: He was also named CBA Rebound
King three times and Block King, and twice the
CBA Dunk King.‚Äô. (5) Opinion-Rebuttal: Charac-
ter A presents an opinion, and Character B counters
with facts or presents a different viewpoint. For ex-
ample, ‚ÄòA: I think this movie is really good.‚Äô, ‚ÄòNo,
this movie scored a 9.5 rating, it is a good movie.‚Äô.
(6) Opinion-Agreement: Character A expresses an
opinion, and Character B either agrees or disagrees
based on facts. For example, ‚ÄòA: That singer per-
formed terribly.‚Äô, ‚ÄòIndeed, the judges gave very low
scores.‚Äô.
Each link in the dialogue logical chain contains
the current turn‚Äôs dialogue type, progress, the log-
ical process of the dialogue participants, and the
purpose of the dialogue, achieving the effect of
guiding the generation of multi-turn dialogue.
4.2 Chain of Dialogue Logic
The chain of dialogue logic (DTC) encourages
large language models (LLMs) to mimic the human

--- PAGE 5 ---
Models LLM Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct GPT-3.5 3.15 3.60 3.60 3.60 3.15 3.57 3.57 3.12 59.18 2.96
CoT (Wei et al., 2022) GPT-3.5 2.93 3.55 3.55 4.06 3.70 3.58 3.64 3.88 46.04 3.46
CoD (our method) GPT-3.5 3.94 4.32 4.32 4.32 4.13 4.34 4.34 4.28 74.77 4.19
CoD (our method) GLLM (Llama-3-8B) 3.86 4.30 4.30 4.21 4.01 4.29 4.18 3.97 68.41 4.06
CoD (our method) GLLM (Qwen-2-7B) 3.89 4.26 4.26 4.27 4.08 4.30 4.24 4.17 67.54 4.09
Direct Qwen 3.57 4.23 4.23 4.23 4.20 4.12 4.30 4.02 70.62 4.04
CoT (Wei et al., 2022) Qwen 3.50 4.18 4.18 4.18 4.16 4.12 4.26 3.96 73.67 4.02
CoD (our method) Qwen 3.95 4.45 4.45 4.43 4.24 4.47 4.48 4.42 83.03 4.33
Direct Deepseek 3.53 4.20 4.20 4.20 4.17 4.13 4.26 4.03 60.79 3.97
CoT (Wei et al., 2022) Deepseek 3.64 4.28 4.28 4.29 4.18 4.25 4.34 4.11 70.02 4.09
CoD (our method) Deepseek 3.95 4.46 4.46 4.44 4.30 4.49 4.50 4.39 75.32 4.30
Table 2: Evaluation results of generated multi-turn SFT data in Artifacts.
Models LLM Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct GPT-3.5 3.24 3.66 3.66 3.67 3.24 3.65 3.65 2.98 21.18 3.20
CoT (Wei et al., 2022) GPT-3.5 2.95 3.67 3.67 4.03 3.72 3.67 3.70 3.46 19.50 3.42
CoD (our method) GPT-3.5 3.96 4.39 4.39 4.39 4.20 4.42 4.42 4.29 40.55 4.05
CoD (our method) GLLM (Llama-3-8B) 3.92 4.36 4.36 4.27 4.16 4.37 4.36 4.25 35.42 3.98
CoD (our method) GLLM (Qwen-2-7B) 3.93 4.34 4.34 4.29 4.19 4.40 4.38 4.25 36.74 3.99
Direct Qwen 3.37 4.16 4.16 4.17 4.13 4.07 4.16 3.91 33.07 3.75
CoT (Wei et al., 2022) Qwen 3.42 4.16 4.16 4.17 4.13 4.12 4.16 3.95 35.07 3.78
CoD (our method) Qwen 3.80 4.36 4.36 4.33 4.20 4.36 4.40 4.30 51.69 4.07
Direct Deepseek 3.53 4.11 4.11 4.16 4.11 4.10 4.18 4.02 21.71 3.71
CoT (Wei et al., 2022) Deepseek 3.51 4.19 4.19 4.20 4.13 4.16 4.22 4.01 34.24 3.81
CoD (our method) Deepseek 3.81 4.38 4.38 4.34 4.20 4.40 4.41 4.34 36.29 4.00
RefGPT (Yang et al., 2023) - 3.66 4.33 4.33 4.28 4.20 4.23 4.31 4.04 45.57 3.96
Table 3: Evaluation results of generated multi-turn SFT data in Science. The original dialogues from RefGPT (Yang
et al., 2023) are evaluated using the same merics and scoring methods for comparison.
thought process between rounds of dialogue, which
includes identifying the type of dialogue, searching
for relevant information, and discovering pertinent
details, aiming to guide the generation of dialogue
between turns. The design of the DTC format in
this paper is inspired by chain of thought (CoT)
and React. CoT is an improved prompting strategy
intended to enhance the performance of LLMs in
complex reasoning tasks such as arithmetic reason-
ing, common sense reasoning, and symbolic reason-
ing. React represents a new prompting paradigm
based on repeated thought-action observation cy-
cles until the current knowledge suffices to derive
an answer, employed to facilitate reasoning and
action within language models to tackle general
tasks. DTC benefits from the ‚ÄúThought‚Äù paradigm
of Dialogue-Search-Find, generating responses in
dialogue that are more accurate and richer com-
pared to CoT and React, as the Dialogue Types
component leads the type of dialogue response,
determining the direction for generating answers.
Meanwhile, the Search-Find component can intro-
duce factual knowledge into the dialogue, enhanc-
ing answer accuracy. Furthermore, unlike the sim-
ple prompt construction approach of input-output
pairs, DTC incorporates intermediate reasoningsteps, guiding the model towards the direction of
the final output.
4.3 GLLM
Given the synthetic multi-turn dialogue
{q(t), a(t)}T
t=1‚ààDsgenerated by the teacher LLM
(e.g. GPT-4), we can fine-tune the open-source
LLMs to obtain the generator GLLM based on
Qwen-2-7B and Llama-3-8B. To reduce the cost,
we can use the GLLM with fewer parameters to
inference more samples by adjusting the sampling
temperature and sampling. In our work, we create
the sampled dataset {q(t)
g, a(t)
g}T
t=1‚ààDsfrom
GLLM to augment the original dataset.
5 Experiments
We conduct three types of experiments to evaluate
the ability and effectiveness of our proposed R2S
framewor to inject document knowledge into multi-
turn dialogues:
‚Ä¢K-BENCH Evaluation : we assess the quality
ofK-BENCH by conducting a comprehensive
assessment involving both automated and hu-
man evaluation methods, subsequently

--- PAGE 6 ---
Models LLM Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct GPT-3.5 3.26 3.64 3.64 3.67 3.31 3.59 3.59 3.16 22.66 3.22
CoT (Wei et al., 2022) GPT-3.5 3.24 3.95 3.95 4.10 3.96 3.96 4.01 3.73 28.73 3.59
CoD (our method) GPT-3.5 3.87 4.30 4.30 4.34 4.11 4.35 4.35 4.30 73.03 4.17
CoD (our method) GLLM (Llama-3-8B) 3.86 4.28 4.28 4.25 4.07 4.32 4.32 4.01 66.28 4.07
CoD (our method) GLLM (Qwen-2-7B) 3.87 4.26 4.26 4.29 4.10 4.34 4.34 4.10 69.44 4.11
Direct Qwen 3.49 4.15 4.15 4.16 4.12 4.08 4.21 4.01 43.92 3.83
CoT (Wei et al., 2022) Qwen 3.58 4.25 4.25 4.26 4.10 4.24 4.29 4.06 60.29 4.00
CoD (our method) Qwen 3.75 4.36 4.36 4.20 4.16 4.36 4.38 4.31 68.01 4.14
Direct Deepseek 3.49 4.11 4.10 4.12 4.10 4.11 4.18 4.05 33.79 3.77
CoT (Wei et al., 2022) Deepseek 3.64 4.32 4.32 4.29 4.10 4.31 4.33 4.09 62.25 4.05
CoD (our method) Deepseek 3.69 4.33 4.33 4.27 4.17 4.31 4.34 4.29 65.13 4.10
Table 4: Evaluation results of generated multi-turn SFT data in Wikipedia.
‚Ä¢GLLM Evaluation : we examine how effec-
tively GLLM can generate multi-turn super-
vised fine-tuning (SFT) data with document
knowledge injected
‚Ä¢Fine-tuned LLM Evaluation : we aim to
ensure that the answers produced by LLMs
trained on the GLLM -generated multi-turn
dialogue data also exhibit a high degree of
coherence, contextual relevance, and factual
accuracy.
5.1 Experimental Setup
Base LLMs For constructing GINSTRUCT and
K-BENCH , we employ GPT-3.5 Turbo (OpenAI,
2022), Deepseek V2 Chat (DeepSeek-AI et al.,
2024), and Qwen-2-72B-Instruct (Bai et al., 2023).
Based on the pre-trained LLMs with different
model sizes, we utilize Llama-2-14B, Llama-3-8B1,
and Qwen-2-1.5B/7B2as the foundation models to
construct GLLM . For model-based evaluation, we
employ GPT-4 Turbo (GPT-4) (OpenAI, 2023) for
high standard.
Evaluation Benchmark We adopt both GIN-
STRUCT and K-BENCH created by GPT-3.5 for
training and evaluation, though more variants have
been created for experimental purposes. Data statis-
tics can be found in Table 1.
Implemetation Details Models from all experi-
ments are trained for 2 epochs with a cosine sched-
uler, starting at a learning rate of 2e-5 (3% warmup
steps). We use AdamW (Loshchilov and Hutter,
2017) as the optimizer and a batch size of 512 (max
length 4096 ). We adopt the evaluation metric intro-
duced in Section 3.2.
1https://github.com/meta-llama/llama
2https://github.com/QwenLM/Qwen25.2 Main Results
K-BENCH Evaluation To verify the quality of
ofK-BENCH and further assess the effectiveness
of the proposed CoD compared to CoT and direct-
instruction prompting approaches, we employ GPT-
4 Turbo as the judge to evaluate the K-BENCH .
GPT-4 Turbo was tasked with scoring each dia-
logue turn based on the evaluation metrics and scor-
ing standard described in 3.2. We adopt GPT-3.5
Turbo (GPT 3.5), Deepseek V2 Chat (Deepseek),
and Qwen-2-72B-Instruct (Qwen) as different vari-
ants of raw text (documents) to multi-turn SFT
data generators. Results of experiments on the
three different types of SFT data generated: arti-
facts, science, and wikipedia are shown in Table 2,
3, and 4 repectively. By examing the results, we
find that the LLMs with our proposed CoD design
outperforms other models on all metrics, except
for the coverage rate (CR) metric evaluated on the
original dialogues created by RefGPT (Yang et al.,
2023), using the same evaluation standards in Table
3. Yang et al. (2023) designed specific knowledge-
injecting methods to ensure the dialogue data they
construct include most of the facts from the doc-
uments, while our method not only ensures high
factual accuracy, but also exhibits a high degree of
coherence and contextual relevance. Moreover, the
SFT data generated by Qwen with the CoD design
still outperforms RefGPT on the CR metric, which
demonstrates the potential of our method reaching
high limits on different backbome LLMs. Since the
GINSTRUCT dataset proposed for training GLLM
is created using the same methods as creating K-
BENCH , the evaluation results also indicate that
theGINSTRUCT dataset exhibits high quality across
different metrics.
GLLM Evaluation In Table 2, 3, and 4, we also
present results of GLLM initialized on both Llama-
3-8B and Qwen-2-7B. In detail, we evaluate on

--- PAGE 7 ---
the SFT data generated by these two GLLM s us-
ing the same metrics and standards introduced in
‚ÄúK-BENCH Evaluation‚Äù. Evaluation results of the
SFT data generated by GPT-3.5 only slightly is
only slightly higher than the data generated by
GLLM , which indicates that the GLLM s intialized
on both LLMs obtained a high degree of data gen-
eration ability from GPT-3.5 with CoD. By lever-
aging the chain of dialogue logic and knowledge-
intensive documents, the SFT data produced by
GLLM surpasses traditional retrieval-based and
direct response methods, establishing new high
ground for knowledge-intensive instruction tuning
in large language models.
Fine-tuned LLM Evaluation To evaluate the ef-
fectiveness of LLMs fine-tuned on the SFT data
produced by GLLM , we conduct experiments com-
paring different generators, fine-tuned LLMs, and
model settings on 3 dataset sources seperately:
artifacts, RefGPT and SquadV2. We exam an-
swers produced by Qwen-2-7B and Llama3-8B
fine-tuned on multi-turn dialogue data generated
by GPT-3.5 and GLLM , respectively. To further
investigate the effectiveness of CoD, we compare
LLMs fine-tuned on the training data generated by
generators with and without CoD (refers as Direct
model in 5) guidance. Experiment results are pre-
sented in Table 5. We find that all CoD models
outperform direct models without CoD, demon-
strating the effectiveness our chain-of-logic design.
The LLMs trained on data produced by GLLM
generators only slighly underperform those trained
on data produced by GPT-3.5. This also matches
the results from the GLLM evaluation experiment,
where GLLM acquired most of the dialogue gener-
ation abilities from its upper-boundary GPT-3.5.
5.3 Analysis
Human Evaluation To ensure the robustness of
the GPT-4-based evaluation, we sampled 100 ex-
amples from K-BENCH created by GPT-3.5 and
conducted human evaluation. The human evalu-
ators were provided with the evaluation metrics
and detailed instructions to score each dialogue
turn from 1 to 5. The scores were aggregated to
provide an overall assessment of the dataset qual-
ity. The evaluators were encouraged to provide
qualitative feedback on the naturalness and factual
accuracy of the dialogues. Additionally, we per-
formed a correlation analysis between the scores
assigned by GPT-4 and human evaluators. Specifi-
Figure 3: Comparison of dialogues responses generated
by direct models and R2S
cally, we adopt Pearson and Spearman correlation
coefficients to measure the association between
the automated and human evaluations. The Pear-
son and Spearman correlation coefficients between
GPT-4 and human evaluations were 0.89 and 0.87,
respectively, indicating a strong alignment between
automated and human assessments.
The effectiveness of R2S Figure 3 is a case study
comparing dialogue responses generated by direct
models and R2S. By integrating the chain of dia-
logue logic, GLLM effectively maintains factual
accuracy and produces coherent, contextually rele-
vant dialogues that mimic human-like interactions.
Such approach not only improves the performance
of the generated dialogues but also provides a ro-
bust framework for instruction tuning of large lan-
guage models using raw text documents. Owing
to the CoD design: 1) the dialogues generated by
GLLM were significantly more informative and
detailed, providing users with comprehensive re-
sponses, 2) the understanding and coherence of the
dialogues were markedly improved, with responses
that logically followed the user‚Äôs queries, and 3)the
dialogues exhibited higher loyalty to the reference
documents, ensuring factual accuracy and reducing
the occurrence of hallucinations.
6 Related Work
Large Language Model Large language mod-
els (LLMs) (Touvron et al., 2023a,b; Frantar et al.,
2022; Bai et al., 2023; Du et al., 2021; Rozi√®re
et al., 2023), leveraging the Transformer architec-
ture, represent a significant leap in natural language
processing (NLP) (Li et al., 2022, 2023b; Qin et al.,
2024a). LLMs undergo rigorous training on exten-
sive textual datasets, enabling them to grasp a wide
range of linguistic nuances and contexts. LLMs fol-
low a two-stage process involving pre-training on

--- PAGE 8 ---
Models Dataset Fine-tuned LLM Generator Info (Q) UD (R) UF (R) FD (R) FL (R) CS CO IA CR Avg.
Direct Artifacts Qwen-2-7B GPT-3.5 3.90 4.30 4.28 3.89 4.06 4.45 4.45 4.44 67.61 4.12
Direct Artifacts Llama-3-8B GPT-3.5 3.38 4.09 4.05 3.67 3.97 4.19 4.16 3.80 59.47 3.80
CoD Artifacts Qwen-2-7B GPT-3.5 3.93 4.31 4.30 3.98 4.11 4.45 4.45 4.45 80.02 4.22
CoD Artifacts Llama-3-8B GPT-3.5 3.84 4.03 4.03 4.08 4.07 4.22 4.23 4.21 72.49 4.03
CoD Artifacts Qwen-2-7B GLLM 3.89 4.32 4.30 3.91 4.04 4.42 4.43 4.45 75.33 4.16
CoD Artifacts Llama-3-8B GLLM 3.75 3.98 3.98 3.89 4.01 4.19 4.21 4.19 64.57 3.93
Direct RefGPT Qwen-2-7B GPT-3.5 3.85 4.21 4.20 3.65 3.95 4.36 4.36 4.38 31.41 3.83
Direct RefGPT Llama-3-8B GPT-3.5 3.54 3.88 3.88 3.57 3.77 3.94 3.94 3.95 28.55 3.54
CoD RefGPT Qwen-2-7B GPT-3.5 3.87 4.24 4.24 3.76 4.04 4.39 4.40 4.40 57.23 4.02
CoD RefGPT Llama-3-8B GPT-3.5 3.60 3.97 3.97 3.69 3.87 3.99 4.00 3.99 52.84 3.74
CoD RefGPT Llama-3-8B GLLM 3.50 3.91 3.91 3.66 3.85 3.93 3.92 3.98 45.11 3.65
CoD RefGPT Qwen-2-7B GLLM 3.81 4.22 4.22 3.69 3.95 4.34 4.34 4.41 42.38 3.89
Direct SquadV2 Qwen-2-7B GPT-3.5 3.86 4.10 4.10 3.64 3.92 4.23 4.20 4.04 53.14 3.86
Direct SquadV2 Llama-3-8B GPT-3.5 3.80 3.56 3.56 3.26 3.40 3.91 3.82 3.76 46.32 3.48
CoD SquadV2 Qwen-2-7B GPT-3.5 3.87 4.10 4.10 3.67 3.95 4.23 4.20 4.07 61.84 3.92
CoD SquadV2 Llama-3-8B GPT-3.5 3.76 3.79 3.79 3.32 3.49 4.00 3.88 3.84 48.98 3.59
CoD SquadV2 Llama-3-8B GLLM 3.58 3.75 3.75 3.30 3.49 3.89 3.74 3.86 58.01 3.58
CoD SquadV2 Qwen-2-7B GLLM 3.80 4.01 4.01 3.74 3.94 4.27 4.17 4.12 51.73 3.84
Table 5: Evaluation results of different LLMs fine-tuned on the generated data. Direct denotes the generator is
trained with a direct instruction to generate dialogues, while CoD refers to training with the chain of dialogue logic
design. Fine-tuned LLM is the dialogue model trained on the generated data.
large-scale corpora followed by instruct tuning for
specific tasks, significantly improving performance
across downstream understanding and generation
challenges. Notably, the GPT series, starting from
GPT-1 and evolving through GPT-4, (Radford et al.,
2018; Black et al., 2022; Brown et al., 2020; Ope-
nAI, 2023) showcases a progressive increase in
model complexity and capacity, with GPT-3 com-
prising a staggering 175 billion parameters. The
introduction of instruction tuning further amplifies
the capabilities of LLMs, unlocking emergent abil-
ities for intricate reasoning tasks, such as math and
code. LLMs with instruction tuning garner atten-
tion from researchers and make significant impacts
across various industry scenarios.
Instruction Tuning LLMs refine their ability to
follow and understand user commands more accu-
rately fine-tuned on an instruction dataset (Ouyang
et al., 2022; Zan et al., 2023; Qin et al., 2024b), con-
sisting of various instructions and their correspond-
ing desired outputs. Early research in construct-
ing conversational datasets largely relies on manu-
ally annotated sets (e.g. QuAC (Choi et al., 2018)
and CoQA (Reddy et al., 2019)), but the limited
scale and high annotation costs restrict model per-
formance and generalizability. Simulation-based
approaches are adopted to generate synthetic dia-
logues through mimicking user-system interactions,
thus reducing dependence on manual annotations.
Recent advancements (Sun et al., 2023) empha-
size the capabilities of LLMs like GPT-4 in auto-
generating extensive, high-quality datasets such as
the SODA dataset (Kim et al., 2023), with 1100 mil-lion utterances and 3 billion tokens. These works
highlight LLMs‚Äô data generation prowess and the
importance of human intervention in enhancing
the accuracy of generated data, for instance, by
applying basic, safety, and commonsense filters
to GPT-3.5-generated dialogues (Chiang and Lee,
2023; Lotfi et al., 2023).
7 Conclusion
In this paper, we introduce R2S, a framework for
constructing a supervised fine-tuning (SFT) dataset
from raw documents utilizing the chain of dialogue
logic (CoD) to guide LLMs in creating knowledge-
intensive multi-turn dialogues for instruction tun-
ing. By aggregating existing documents from open-
source websites/datasets, we establish a compre-
hensive benchmark, K-BENCH , featuring topics in
Wikipedia (English), Science (Chinese), and Ar-
tifacts (Chinese). Utilizing CoD, we categorize
dialogue turns (e.g., Opinion Exchange Q&A and
Informational Q&A) and prompt the LLM to iden-
tify key phrases for generating relevant responses,
thus retaining raw document knowledge within
dialogue-style instruction datasets ( GINSTRUCT )
and enabling the fine-tuning of open-source LLMs
(GLLM ) for transforming raw documents into
multi-turn dialogues, further enriching the SFT
model with domain-specific knowledge. Extensive
experiments on K-BENCH validate the efficacy of
R2S, showing substantial improvements in perfor-
mance across various metrics. The SFT generator
GLLM confirms the effectiveness of CoD by creat-
ing reasonable and coherent multi-turn dialogues.

--- PAGE 9 ---
Limitations
Due to limited computing resources, the supervised
fine-tuning (SFT) dataset generated by GLLM is
evaluated on Qwen-2-7B and Llama-3-8B only,
while more backbone large language models should
be tested, ensuring the robustness of R2S to inject
document knowledge into SFT data. Besides, the
large-scale training scenario with more powerful
LLMs still requires further exploration.
Ethical Considerations
The dataset used for evaluation in this paper is
obtained from open data sources and has been man-
ually verified and screened to eliminate any data
with ethical risks and sensitive content. This en-
sures that the content is compliant with existing
regulations and laws.
References
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609 , abs/2309.16609.
Sid Black, Stella Biderman, Eric Hallahan, Quentin An-
thony, Leo Gao, Laurence Golding, Horace He, Con-
nor Leahy, Kyle McDonell, Jason Phang, et al. 2022.
GPT-NeoX-20B: An open-source autoregressive lan-
guage model. arXiv preprint arXiv:2204.06745 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877‚Äì1901.
Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,
Jiaheng Liu, Bing Wang, Xinnian Liang, Jiaqi Bai,
Tongliang Li, Qiyao Peng, and Zhoujun Li. 2024.
xcot: Cross-lingual instruction tuning for cross-
lingual chain-of-thought reasoning. arXiv preprint
arXiv:2401.07037 , abs/2401.07037.
David Cheng-Han Chiang and Hung-yi Lee. 2023. Can
large language models be an alternative to human
evaluations? In Proceedings of the 61st AnnualMeeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 15607‚Äì15631. Asso-
ciation for Computational Linguistics.
Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. Quac: Question answering in context.
InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, Brus-
sels, Belgium, October 31 - November 4, 2018 , pages
2174‚Äì2184. Association for Computational Linguis-
tics.
DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingx-
uan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr,
Chong Ruan, Damai Dai, Daya Guo, Dejian Yang,
Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli
Luo, Guangbo Hao, Guanting Chen, Guowei Li,
H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang,
Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li,
Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Ji-
aqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie
Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang
Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia,
Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,
Mingchuan Zhang, Minghua Zhang, Minghui Tang,
Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang,
Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du,
R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin
Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan
Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng
Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuip-
ing Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian
Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding
Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun
Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xi-
anzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,
Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiao-
tao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu,
Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou,
Xinyu Yang, Xuan Lu, Xuecheng Su, Y . Wu, Y . K.
Li, Y . X. Wei, Y . X. Zhu, Yanhong Xu, Yanping
Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui
Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang
Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao,
Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang,
Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng
Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang
You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli
Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie,
Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng
Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui
Gu, Zilin Li, and Ziwei Xie. 2024. Deepseek-v2: A
strong, economical, and efficient mixture-of-experts
language model. arXiv preprint arXiv:2405.04434 .
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021.
GLM: General language model pretraining with
autoregressive blank infilling. arXiv preprint
arXiv:2103.10360 .
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Dan Alistarh. 2022. GPTQ: Accurate post-training

--- PAGE 10 ---
quantization for generative pre-trained transformers.
arXiv preprint arXiv:2210.17323 .
Pragya Katyayan and Nisheeth Joshi. 2022. Design and
development of rule-based open-domain question-
answering system on squad v2.0 dataset. CoRR ,
abs/2204.09659.
Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,
Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,
Malihe Alikhani, Gunhee Kim, Maarten Sap, and
Yejin Choi. 2023. SODA: million-scale dialogue dis-
tillation with social commonsense contextualization.
InProceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023 , pages 12930‚Äì
12949. Association for Computational Linguistics.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou,
Marc Marone, Christopher Akiki, Jia Li, Jenny
Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue
Zhuo, Thomas Wang, Olivier Dehaene, Mishig
Davaadorj, Joel Lamy-Poirier, Jo√£o Monteiro, Oleh
Shliazhko, Nicolas Gontier, Nicholas Meade, Armel
Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi,
Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov,
Zhiruo Wang, Rudra Murthy V , Jason Stillerman,
Siva Sankalp Patel, Dmitry Abulkhanov, Marco
Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-
Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam
Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku-
nakov, Fedor Zhdanov, Manuel Romero, Tony Lee,
Nadav Timor, Jennifer Ding, Claire Schlesinger, Hai-
ley Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,
Alex Gu, Jennifer Robinson, Carolyn Jane Ander-
son, Brendan Dolan-Gavitt, Danish Contractor, Siva
Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jer-
nite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas
Wolf, Arjun Guha, Leandro von Werra, and Harm
de Vries. 2023a. StarCoder: May the source
be with you! arXiv preprint arXiv:2305.06161 ,
abs/2305.06161.
Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang,
Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou,
Hai-Tao Zheng, and Ying Shen. 2023b. Towards
real-world writing assistance: A chinese character
checking benchmark with faked and misspelled char-
acters. CoRR , abs/2311.11268.
Yinghui Li, Qingyu Zhou, Yangning Li, Zhongli Li,
Ruiyang Liu, Rongyi Sun, Zizhen Wang, Chao Li,
Yunbo Cao, and Hai-Tao Zheng. 2022. The past mis-
take is the future wisdom: Error-driven contrastive
probability optimization for chinese spell checking.
InFindings of the Association for Computational
Linguistics: ACL 2022, Dublin, Ireland, May 22-27,
2022 , pages 3202‚Äì3213. Association for Computa-
tional Linguistics.
Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong
Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, and
Philip S. Yu. 2024. When llms meet cunning ques-
tions: A fallacy understanding benchmark for large
language models. CoRR , abs/2402.11100.Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
Ehsan Lotfi, Maxime De Bruyn,
Jeska.buhmann@uantwerpen.be
Jeska.buhmann@uantwerpen.be, and Walter
Daelemans. 2023. Follow the knowledge: Structural
biases and artefacts in knowledge grounded dialog
datasets. In Proceedings of the Third DialDoc
Workshop on Document-grounded Dialogue and
Conversational Question Answering , pages 109‚Äì121,
Toronto, Canada. Association for Computational
Linguistics.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-
ardMath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
arXiv preprint arXiv:2308.09583 .
OpenAI. 2022. Introducing ChatGPT.
OpenAI. 2023. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS .
Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu,
Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che,
and Philip S. Yu. 2024a. Large language models meet
NLP: A survey. CoRR , abs/2405.12819.
Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen,
Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, and
Philip S. Yu. 2024b. Multilingual large language
model: A survey of resources, taxonomy and fron-
tiers. CoRR , abs/2404.04925.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training. OpenAI blog .
Siva Reddy, Danqi Chen, and Christopher D. Manning.
2019. Coqa: A conversational question answering
challenge. Trans. Assoc. Comput. Linguistics , 7:249‚Äì
266.
Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. 2023.
Code Llama: Open foundation models for code.
arXiv preprint arXiv:2308.12950 .
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. 2023. Principle-driven self-
alignment of language models from scratch with

--- PAGE 11 ---
minimal human supervision. arXiv preprint
arXiv:2305.03047 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. LLaMA: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aur√©lien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. abs/2307.09288.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023. Self-Instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 13484‚Äì13508. Association for Computational
Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. In Ad-
vances in Neural Information Processing Systems 35:
Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA,
USA, November 28 - December 9, 2022 .
Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang,
Zili Wang, Shusen Wang, and Hai Zhao. 2023. Re-
fgpt: Dialogue generation of gpt, by gpt, and for GPT.
InFindings of the Association for Computational Lin-
guistics: EMNLP 2023, Singapore, December 6-10,
2023 , pages 2511‚Äì2535. Association for Computa-
tional Linguistics.
Daoguang Zan, Ailun Yu, Bo Shen, Jiaxin Zhang, Tai-
hong Chen, Bing Geng, Bei Chen, Jichuan Ji, Yafen
Yao, Yongji Wang, and Qianxiang Wang. 2023. Canprogramming languages boost each other via in-
struction tuning? arXiv preprint arXiv:2308.16824 ,
abs/2308.16824.
