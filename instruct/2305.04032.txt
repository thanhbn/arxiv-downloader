# 2305.04032.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2305.04032.pdf
# File size: 1205674 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ToolCoder: Teach Code Generation Models to use
API search tools
Kechi Zhang
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
zhangkechi@pku.edu.cnHuangzhao Zhang
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
zhang hz@pku.edu.cnGe Li*
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
lige@pku.edu.cn
Jia Li‚ôÇ
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
lijia@stu.pku.edu.cnZhuo Li
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
lizhmq@pku.edu.cnZhi Jin*
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
zhijin@pku.edu.cn
Abstract ‚ÄîAutomatically generating source code from natural
language descriptions has been a growing field of research in
recent years. However, current large-scale code generation models
often encounter difficulties when selecting appropriate APIs for
specific contexts. These models may generate APIs that do not
meet requirements or refer to non-existent APIs in third-party
libraries, especially for lesser-known or private libraries. Inspired
by the process of human developers using tools to search APIs,
we propose ToolCoder, a novel approach that integrates API
search tools with existing models to assist in code generation
and API selection. To teach our model to use tools, we introduce
an automated data annotation method using ChatGPT to add
tool usage information into the source code data and fine-tune
code generation models. During inference, we integrate API
search tools into the generation process so that our model can
automatically use the search tool to get suggestions when selecting
an API. Our experimental results demonstrate that ToolCoder
exhibits excellent performance and generalization across five
public and private library code generation benchmarks, with at
least 6.21% improvement on average pass@1 metrics and 9.64%
improvement on average pass@10 metrics compared to state-
of-the-art methods. Furthermore, we show that our relatively
small ToolCoder model is comparable to one of the current
best models, GPT-3.5, highlighting the potential of incorporating
programming tools into the code generation process.
I. I NTRODUCTION
Automated code generation has become increasingly im-
portant due to the significant effort required to manually write
source code, especially for complex software. Deep learning
techniques, particularly language models, have shown great
promise in generating high-quality source code from natural
language requirements. Currently, pre-trained code generation
models are considered the state-of-the-art solution for various
code generation tasks, such as CodeX [3], ChatGPT [4, 16]
and CodeGen [14] models.
Accurately selecting appropriate application programming
interfaces (APIs) is essential for pre-trained models to generate
* Corresponding authors
definit_model(args):‚Ä¶...# inp_size(height, width, extra_dim)# out_size(height, width)out =np.?
# inp_size(height, width, extra_dim)# out_size(height, width)out = np.squeeze(inp)
definit_model(): # inp_size(height, width, extra_dim)# out_size(height, width)out =np.?Fig. 1. An illustrative example of the process of human programmers selecting
the proper API during coding. Programmers summarize their demands into a
query (remove single-dimensional entries) and use the search engine tool or
documentation search tool to get the proper API suggestion (np.squeeze) .
code. API selection is crucial for accurately expressing pro-
gram semantics and efficiently addressing problems. However,
there are too many existing third-party libraries and their
APIs, and new APIs are constantly being developed. Existing
models often find it challenging to select APIs accurately and
will generate non-existent APIs or APIs that do not meet
requirements. For example, according to our preliminary ex-
periments on NumpyEval and PandasEval [26], a popular code
generation model CodeGen-2B generates more than 26% code
that contains an incorrect API. Furthermore, for security and
functionality reasons, industrial companies often build private
libraries for internal use only. For these private libraries that
are not publicly available, the error rate will increase to more
than 90%. These third-party public libraries or private libraries
provide so many APIs that code generation models have never
seen, resulting in the model being unable to generate API-arXiv:2305.04032v5  [cs.SE]  11 Sep 2023

--- PAGE 2 ---
oriented code. Therefore, it is worth exploring methods to
improve code generation models to generate accurate source
codes using these domain-specific or private library APIs.
To assist code generation models in selecting appropriate
APIs during the generation process, we draw inspiration
from human programmers‚Äô perspectives. In most programming
scenarios, programmers can use search tools to get suggestions
from web sources or library documents when selecting an
API. Figure 1 shows an example of human programmers
selecting the proper API during coding. When encountering
an API usage scenario, programmers summarize their needs
into a query and use existing API search tools to search for
suitable APIs, such as Google search engines or documentation
search tools for specific libraries. Then, according to the search
results, programmers can choose the proper API. This pro-
gramming process using tools to retrieve and determine API
usage improves programming efficiency, improves accuracy,
and reduces the risk of errors. It motivates us to investigate
approaches to teach code generation models to use search tools
to find suitable APIs.
In this paper, we propose ToolCoder, a low-cost and efficient
solution that integrates API search tools into pre-trained code
generation models, mimicking how programmers solve this
problem. To help models learn to use tools, we propose an
automated data annotation method with ChatGPT to add tool
usage information into the source code data and use the anno-
tated dataset to fine-tune code generation models. Specifically,
our approach utilizes the in-context learning ability of large
models to annotate a special tool-augment dataset at a low
cost. We employ parameter-efficient fine-tuning to improve the
training efficiency. During inference, we integrate API search
tools into the decoding process of our model, allowing the
model to learn to use external tools autonomously.
We extensively evaluate our proposed ToolCoder with pass
rate metrics [3]. ‚ù∂We evaluate ToolCoder on three public
library benchmarks. Our model achieves significant improve-
ments over state-of-the-art baselines with at least 10.11%,
3.26%, and 1.39% pass@1. Our relatively small ToolCoder is
even comparable to one of the current best language models,
GPT-3.5. ‚ù∑We further evaluate our model on two private
library benchmarks. By switching to the appropriate search
tool, our ToolCoder can be easily transferred to these private
library scenarios and achieves stable improvement. Our model
exhibits better generalization performance and raises at least
6.21% improvement on the average pass@1 metrics for all five
benchmarks. ‚ù∏We also conduct an ablation study to analyze
the different settings in our experiments, including the dataset,
training, and inference settings. Results prove the effectiveness
of different designs in our approach.
Our contributions in this paper can be summarized as
follows:
‚Ä¢To the best of our knowledge, we are the first to incor-
porate a programming tool into code generation models.
Our results highlight the importance of models‚Äô ability to
use tools.
Case2: Pandasdf =pd.DataFrame({'A': [5, 6, 7], 'B': [7, 8, 9]}) # sum all valuesand return a numeric valuesum_value =df.sum()sum_value is not a numeric valueCase1: Numpya =np.arange(2*3*2).reshape((2,3,2)) count_value =a.count(2)'numpy.ndarray' object has no attribute 'count' 
Case3: Private library (BeatNum)import beatnumasbnnum_str =bn.numstr([0,33,4444522])module ‚Äòbeatnum' has no attribute 'numstr'Fig. 2. Failure Cases of CodeGen-2B model in selecting APIs, including gen-
erating non-existing APIs on public libraries ( Case 1 ), generating unqualified
APIs ( Case 2 ), and lack of API-related knowledge on private libraries ( Case
3).
‚Ä¢We propose an automatic method for annotating datasets
in software engineering. This low-cost and efficient an-
notation framework uses powerful ChatGPT to annotate
API datasets with public source code datasets, reducing
the manual effort required to create private annotated
datasets. Our dataset construction method can also be
easily transferred to other tasks.
‚Ä¢We propose ToolCoder, which incorporates the ability to
use API search tools into pre-trained code generation
models and improves the performance on API-related
code generation tasks. Our approach outperforms existing
API-oriented baselines on multiple popular API-related
code generation benchmarks.
II. M OTIVATING EXAMPLES
In this section, we examine the limitations of the current
code generation models when selecting a suitable API and how
existing search tools can aid in API selection. By exploring
these issues, we hope to provide context for our research
and explain our motivation for proposing a new approach to
address these challenges.
A. Limitations of current code generation models in selecting
suitable APIs
Application Programming Interfaces (APIs) are essential
to modern software development. APIs allow developers to
integrate pre-existing code and services into their applications.
Using APIs can significantly reduce the time and effort re-
quired to develop complex software systems.
However, selecting the proper API remains challenging for
code generation models. Due to the proliferation of third-party
libraries and their associated APIs, existing code generation
models often need help choosing APIs. Here we choose
the popular model CodeGen-2B to test its performance on
API selection. Figure 2 shows three failure examples of the
generated code for selecting APIs. ‚ù∂Case 1: The CodeGen-
2B model has the potential risk of generating APIs that do

--- PAGE 3 ---
TABLE I
COMPARISONS OF TWO TYPES OF SEARCH TOOLS FOR API SELECTION .
Online Search Engine Documentation Search
Knowledge
ResourcesProgramming Community
or Tutorial Websites
(StackOverFlow, datagy.io, etc.)Library Documentation
API TypePublic libraries,
especially those well-known
and widely-discussedAny APIs,
including public and
private libraries
AdvantagesPractical and Accurate
Rich sources
Keep updatingWide coverage
Detailed explanation
Stable
Example ToolsGoogle, Bing,
DuckDuckGoNumPydoc,
Pandasdoc,
Private documentations
not exist, even on such a popular and common third-party
library as NumPy. There is no count API in the Numpy
library, but CodeGen2B still generates. ‚ù∑Case 2: It may also
use the wrong API and generate unqualified code. df.sum()
will return with a Pandas Series type but not the required
numeric value. It shows that these existing code generation
models still have challenges choosing an appropriate API
to implement a given requirement. We conduct a statistical
experiment to analyze the generated code on two benchmarks,
NumpyEval and PandasEval [27], and find that more than 26%
of the APIs generated have the problems mentioned above.
We also conduct experiments on private library benchmarks
such as BeatNumEval [26]. Private libraries are widespread
in real code scenarios, and they are not public for security
and functional reasons. Experiments show that for those APIs
in private libraries, the failure rate will increase to more than
90%.‚ù∏Case 3: We find the CodeGen-2B lacks corresponding
private knowledge and concocts an incomprehensible API
for the private library BeatNum. It shows that existing code
generation models have limitations in API generation and are
not specifically optimized for using APIs. In this paper, we
aim to address these API selection issues.
B. Existing search tools to aid API selection
Drawing inspiration from the approach programmers em-
ploy in selecting an API, our study reveals that existing search
tools can provide API recommendations. For example, in
Figure 1, the developer needs to use the numpy library to
remove the extra single dimension in input size. The developer
turns to online search engine tools orlibrary documentation
search tools and gets the proper API suggestion np.squeeze .
These two search tools can play a significant role in selecting
APIs. A comparison of these two types of search tools is given
in Table I. We will analyze the use of these two types of search
tools.
1) Online Search Engine Tool: Online search engine tools
provide rich information on various API usage scenarios.
Human programmers share their experience in solving various
programming problems on various community and tutorialwebsites such as StackOverFlow1anddatagy.io2. They orga-
nize and summarize the API suggestions used for different
problems. Formally, these online API suggestions are usually
displayed in the form of programming experience sharing or
question and answer. When other people encounter similar
problems, search engines can use this information well, and
programmers only need to provide a question query. These
search engines regard these community websites as knowledge
resources and can provide helpful API suggestions, especially
for those public libraries that are well-known and widely
discussed. Since these online programming experiences on
the website are constantly updated and provide a variety
of practical scenarios, we can often get more accurate API
suggestions from these online search engine tools. These
mature commercial search engine tools such as Google3,
DuckDuckGo4can provide accurate, instant, and fast search
responses and are widely used by human programmers.
2) Documentation Search Tool: Since lesser-known public
libraries or private libraries have few discussions on the
online community websites, human programmers also turn to
library documentations for API suggestions. Documentations
are usually available for public libraries and private libraries.
Therefore, it can provide rich information for any API usage
scenario. Documentations provide detailed explanations for
each API, with a broad convergence over the corresponding
library. The documentation contains detailed and accurate
explanations of the parameters and usage of the API. It is
the most detailed place to learn how to use an API. Since
the documentation does not change frequently, its results are
more stable. Formally, API information in the documentation
is usually given in pairs of API and corresponding comments.
We can use BM25 [19] or other semantic similarity scores
as search metrics to search for comments that meet the
requirements and find the corresponding API as the final
suggestion for coding.
These various search tools are helpful for programming
and selecting an API. Inspired by the API search process
of human developers, we aim to incorporate these two types
of search tools into code generation models. By letting the
code generation model learn to use these online search engine
tools or documentation search tools, our models can effectively
navigate the vast amount of information available to identify
the most relevant APIs. This approach enables our models to
more accurately match APIs with specific needs.
III. API S EARCH TOOL
To better present the methodology of our model, we first
provide a brief introduction to the API search tool in this
section. The proposed API search tool is an abstraction of the
existing search tools for API selection and will be used as an
external tool for our ToolCoder.
1https://stackoverflow.com/
2https://datagy.io/
3https://www.google.com/
4https://duckduckgo.com/

--- PAGE 4 ---
Following the motivating examples in Section II-B, we
develop the API search tool for code generation models based
on these two categories of sources. ‚ù∂For those commonly
used public libraries such as numpy and pandas , we use
DuckDuckgo as the search engine because it provides a more
convenient and automated method compared to other search
engines. We use the search engine to search the relative
content from several online community websites and extract
the mentioned APIs with string regex matching. Since these
contents have a richer introduction to the API, more accurate
API suggestions can be obtained from the search engine. ‚ù∑
For lesser-known APIs or those in private libraries, we employ
theBM25 score as our retrieval metric to search from the
corresponding API documentation.
We then abstract the two types of search tools into a unified
form: we use the notation APISearch (query )‚Üíanswer to
represent the call of API search tool, where APISearch is
the function name that abstracts different API search sources,
query denotes the search query and answer indicates the
return answer from API search tools that can be referred to
for further code generation. In subsequent experiments, we
serialize API search tool calls for model input. To differentiate
from regular text, we surround the tool calls with special
tokens by starting with ‚ü®API‚ü©and ending with ‚ü®/API‚ü©.
Examples can be viewed in Figure 3. We set ‚ü®API‚ü©,‚ü®/API‚ü©,
and‚Üías special tokens in our model vocabulary.
IV. T OOLCODER
In this section, we present our approach ToolCoder for
selecting and using APIs in coding practices. The goal of our
approach is to train a model that can effectively select and use
appropriate APIs based on existing partial code. To achieve
this goal, we decompose our approach into three modules,
including data annotation, fine-tuning, and inference. The three
modules work in a pipeline as shown in Figure 3. We will
describe the details in the following subsections.
A. Automatic Data Annotation
In order for the model to learn to use the API search
tool, we first need a dataset that includes the source code
and associated tool call processes. As mentioned in Section
III, we abstract the search call process with the notation
‚ü®API‚ü©APISearch (query )‚Üíanswer ‚ü®/API‚ü©. However,
such datasets are not readily available. To address this issue,
we propose automatically augmenting an existing source code
dataset with the tool call notation using ChatGPT (gpt-3.5-
turbo)5, which has demonstrated excellent few-shot and even
zero-shot learning ability in many different language learning
tasks already. This low-cost and efficient annotation method
reduces the manual effort required to create private annotated
datasets. Our data annotation process can be divided into three
parts:‚ù∂base dataset selection, ‚ù∑prompt selection, and ‚ù∏
filter and clean.
Base Dataset Selection. For the base dataset, we choose to
use the popular pre-trained dataset CodeSearchNet-Python [8]
5https://openai.com/TABLE II
STATISTICS OF THE ANNOTATION DATASET .
Statistic
Dataset Size 53,000
Avg. Annotation API 3.2
Avg. Length (in words) before annotation 186.24
Avg. Length (in words) after annotation 211.49
Proportion of some third-party librariesNumPy 24%
Pandas 13%
TorchData 0%
as the base dataset. It is a real-world programming dataset
obtained from GitHub without any additional annotations. This
dataset is already commonly used by many pre-trained code
generation models, so we can assure as much as possible that
our subsequent training will not affect the model‚Äôs generaliza-
tion performance on language generation and modeling ability.
We use a simple length filtering method and randomly choose
nearly 60k function-level source code from this dataset as the
base dataset for our annotation method.
Prompt Selection. Similar to [20], to help generate the
annotated dataset, we need to provide a detailed instruction
for ChatGPT to specify its system role as a data annotator,
shown in Figure 4. To facilitate the quality of the generated
datasets, we manually write three human-written input-output
pairs as part of the prompt with three libraries including
numpy ,pandas , and matplotlib . We choose these three libraries
as the examples in the prompt because we are skilled in them,
and they are also commonly used in the base dataset. Based
on our selected prompt and base dataset, we will ask the
ChatGPT to annotate the tool-augmented dataset. We generate
one annotated data for each base sample. The automatic
annotation process lasted for four days.
Filter and Clean. After getting all the generated results from
chatGPT, we performed a series of simple filtering operations
on the results to remove those abnormal data samples. We
filter out nested API Search calls, control the number of API
Search calls in a sample of less than 5, and ensure that at
least one is an API call from a public library. We filter out
those samples that are different from the source code after
removing the API Search call. Furthermore, for the generated
API answer in the search call, we check whether it is followed
by the corresponding API in the generated code to ensure
that the API search call is closely related to the specific code
implementation. Finally, we cleaned and obtained the final data
set of 53k, which will be used for subsequent fine-tuning.
Table II shows the statistics of the final annotated dataset. We
also count the proportion of some third-party library APIs in
the dataset for reference in subsequent evaluation experiments.
In the left part of Figure 3, we also give an example sample
of the final dataset.
B. Parameter-efficient Fine-tuning
We leverage the annotated dataset to fine-tune a pre-trained
language model to teach the model to generate the search tool
call itself. To address the challenge of limited computational

--- PAGE 5 ---
CodeGenera)onPretrainedModel
‚öôParameter-eÔ¨Écient Fine-Tuning can be trained only with a consumer-level GPUChatGPT
Before Annota+on:‚Ä¶if mean is not None:samples=mulBvariate_normal(mean, matrix, N)‚Ä¶ A-er Annota+on:‚Ä¶if mean is not None:samples=<API>APISearch(Generates random samples from a mul9variate normal distribu9on.)-> mul9variate_normal</API> mulBvariate_normal(mean, matrix, N)‚Ä¶ NL_input:How do I get the value at an n-throw of a given column name in Pandas?
output = <API>APISearch(Selects a single row of data from a DataFrame.)->pandas.DataFrame.iloc</API> df.iloc[n][column_name]
üîçSelects a single row of data from a DataFrame
‚úÖpandas.DataFrame.iloc
output = <API>APISearch(Selects a single row of data from a DataFrame.)->
PublicPretraining Dataset(CodeSearchNet‚Ä¶)
Fine-TuningInference
ü§ñCodeGen-350M
ü§ñCodeGen-2B...
low-rank adapta-on:freeze most parameters and only Ô¨Ånetune with few of params
xhWdownWupFrozenPretrained WeightsFig. 3. The pipeline of our approach ToolCoder. The pipeline has three main parts: (1) Automatically Annotate Tool-augmented Dataset with ChatGPT, (2)
Parameter-efficient Fine-tune existing pre-trained code generation model with the annotated dataset, and (3) Inference of the fine-tuned model enhanced with
API search tools.
Your task is to add calls to a API Search Tool to a piece of source code. You can use an API Search Tool to lookup important third-party APIs from the document. The API Search Tool should help you get information required to complete the source code and select API. Use the format: "<API>APISearch(query)->answer</API>". In the format, "query" is the search input that describes the specific role of the API required in this code, and "answer" is the search output API. Here are some examples of API calls:Input:B = np.reshape(A, (-1, 2))Output: B = <API>APISearch(Gives a new shape to an array without changing its data.)->np.reshape</API>np.reshape(A, (-1, 2))(...another two Input-Output pairs...)Input:{code}Output:
Fig. 4. An exemplary prompt used to generate API-augmented datasets for
the API search tool. In our setting, We selected a total of three human-written
input-output pairs as part of the prompt, using three libraries: numpy ,pandas ,
andmatplotlib .
resources and improve the training efficiency, we propose
restricting the number of meta-trainable parameters and layers
in the pre-trained model and adopting a parameter-efficient
fine-tuning approach that can efficiently adapt pre-trained
models to new task types. In particular, we apply LoRA [7]
to reduce trainable parameters.
Low-Rank Adaptation (LoRA) is a low-dimensional
representation-based parameter-efficient tuning method. It in-
jects trainable low-rank matrices into transformer layers to
approximate the weight updates. For a pre-trained weight
matrix W‚ààRd√ók, LoRA represents its update with a low-
rank decomposition W+Œ¥W =W+WdownWup, where
Wdown‚ààRd√ór, Wup‚ààRr√ókare tunable parameters. LoRA
generally applies this update to the attention linear projection
matrices in the multi-head attention sub-layer in Transformer.For a specific input xto the linear projection in multi-head
attention, LoRA modifies the projection output has:
h‚Üêh+s¬∑xWdownWup, (1)
where s‚â•1is a tunable scalar hyperparameter. The illustra-
tion of LoRA is shown in the middle part of Figure 3.
In our training setting, we freeze most of the parameters in
the pre-trained model and only apply LoRA on the query and
value projections in the attention module for each transformer
layer. As a result, we only need to train 0.18% parameters
inCodeGen-350M and 0.09% for CodeGen-2B . It makes it
possible to efficiently fine-tune models on a consumer-level
GPU, such as Nvidia GeForce RTX 2080 (11GB RAM).
The parameter-efficient tuning strategy significantly reduces
the training computational burden in our experiments. It can
achieve results comparable to full-parameter training with less
computational resources and time. We will give a detailed
analysis of the ablation experiment in Section VI-C.
C. Inference enhanced with Tools
After training with the annotation dataset, the model can
generate the API search calls during the code generation
process. The pseudo-code description of the decoding process
with API search tool procedure is in Algorithm 1.
During inference, we perform regular decoding until the
model produces the ‚ü®API‚ü©token, indicating that it next
expects the response for an API call. At this point, we continue
the decoding process and record the following generated
tokens to get the query between APISearch (and )‚Üí. Then
we interrupt the decoding process and call the API search tool
to get a response, and continue the decoding process after
inserting both the response and the ‚ü®/API‚ü©token.

--- PAGE 6 ---
Algorithm 1 Inference with API Search Tool
1:procedure INFER WITHTOOL(model, input nl, maxlen)
2: Passinput nlto the model and get predicted token
3: output ‚Üê[token ]
4: i‚Üê0
5: whilei < maxlen do
6: token ‚Üêthe last token of output
7: iftoken =‚ü®API‚ü©then
8: query ‚Üêthe following generated tokens between APISearch( and)‚Üí
9: response ‚ÜêCall API search tool with query
10: Append ‚ü®API‚ü©APISearch(query) ‚Üíresponse ‚ü®/API‚ü©tooutput
11: i‚Üêi+length of the call process
12: else
13: Passtoken to the model and get predicted token
14: Append predicted token to output
15: i‚Üêi+ 1
16: end if
17: end while
18: return output
19: end procedure
As mentioned in Section III, we adopt different API search
sources for different types of API usage. For those commonly
used public libraries, we use the DuckDuckGo , a popular
online website search engine, to adopt content in-site search
in several selected websites. For those lesser-known or private
library APIs, there is no relevant online information. So we
employ the BM25 score as our retrieval metric to search
from the corresponding API documents. We encapsulate these
search interfaces so that our ToolCoder can call search tools
with high performance. In our experiment, we control the
search delay within 0.6s to ensure high efficiency during the
code generation process.
After the entire inference process is over, we use the regular
matching method to remove the API search part from the
generated code, that is, the part between ‚ü®API‚ü©and‚ü®/API‚ü©
to get the generated code. By using API search tools in this
way, we can effectively address the challenge of selecting
appropriate APIs and reduce the time and effort required for
developers to find suitable APIs.
V. E XPERIMENTAL SETUP
To assess the effectiveness of our approach, we perform
a large-scale study to answer four research questions. In this
section, we describe the details of our study, including datasets,
metrics, and baselines.
A. Research Question
Our study aims to answer four research questions. In RQ1,
we compare our ToolCoder to SOTA code generation models
on three public library benchmarks. In RQ2, we conduct ex-
periments on two private library benchmarks to show the gen-
eralization of our proposed model on those private libraries. In
RQ3, we conduct an ablation study to prove the contributions
of different modules. In RQ4, we conduct a series of quality
measures on the generated results and analyze the effectiveness
and limitations of our method through detailed case studies.
RQ1. How does ToolCoder perform compared to SOTA
baselines on public library code generation? To evaluate
ToolCoder‚Äôs performance on public library code generation,
we conduct experiments on three public library code genera-
tion benchmarks, including numpy ,pandas , and torchdata . Wecompare ToolCoder‚Äôs performance with existing SOTA code
generation baselines.
RQ2. How does ToolCoder perform on private library
code generation? We select two private library benchmarks
where the pre-trained language models have never encountered
any private library APIs, and there is no relevant information
available online. We evaluate ToolCoder‚Äôs performance on
these private libraries to demonstrate its generalization and
versatility.
RQ3. What are the contributions of different modules
in our approach? Our approach pipeline consists of three
modules: data annotation, fine-tuning, and inference. To ana-
lyze the effectiveness of our approach, we conduct an ablation
study by varying settings in our pipeline, including the dataset,
training, and inference search settings.
RQ4. How is the quality of our generated code with
ToolCoder? We evaluate the quality of generated code using
ToolCoder by performing a case study analysis. Additionally,
we analyze the effectiveness of our method and explain why
our model works.
B. Datasets
Our experiments are conducted on three public library
benchmarks, PandasEval, NumpyEval, and TorchDataEval,
and two private library benchmarks, including MonkeyEval
and BeatNumEval. We choose these benchmarks to ensure
our proposed method can be used in various API selection
scenarios.
1) Public library benchmarks: PandasEval [27] is a
domain-specific method or block generation benchmark for the
Pandas library in Python. PandasEval contains 101 test exam-
ples. Each example corresponds to a programming problem of
Pandas, containing the context code, the target method body
(or block), and multiple test cases. NumpyEval [27] is almost
the same as PandasEval, apart from the domain. NumpyEval
specifically targets the Numpy library in Python. The bench-
mark also contains 101 test examples. TorchDataEval [26]
is based on the TorchData library in Python. TorchData is a
newly released library, which is more likely to be unseen to
the pre-trained models. Therefore, this benchmark is proposed
to evaluate the model against the unseen library containing
50 test examples. In our experiments, our annotated dataset
does not contain API code related to TorchData as shown
in Table II, and our base pre-trained model does not contain
these data during the pre-training phase, so this benchmark
can also be used to demonstrate the generalization ability of
our method on those APIs that are public but never seen by
the code generation model.
2) Private library benchmarks: MonkeyEval [26], mod-
ified from PandasEval, is designed to evaluate the method
generation model against the unseen library. The Monkey li-
brary is crafted by modifying all Pandas-related keywords. e.g.,
‚Äúpandas‚Äù is converted to ‚Äúmonkey‚Äù, ‚Äúdataframe‚Äù is converted
to ‚Äúknowledgeframe‚Äù, etc.. The library construction process
ensures that no information about the API names of these
libraries is leaked in the online materials or any training

--- PAGE 7 ---
datasets. MonkeyEval converts all examples in PandasEval,
leading to 101 test examples. BeatNumEval [26] is mod-
ified from NumpyEval, in the same way as PandasEval to
MonkeyEval. BeatNumEval also has 101 test examples. The
pre-trained model has not seen the API in MonkeyEval and
BeatNumEval, and the online search resources cannot provide
any API-related information. So the API selection on these
benchmarks will only rely on the API search tool we built on
the documentation of these private libraries.
C. Metrics
Following the previous work, we use the metric pass rate
pass@k [3] for performance evaluation and take advantage of
the provided unit tests to determine the functional correctness
of code solutions. For each problem, we submit k code
solutions for evaluation. If any of the k code solutions passes
all ground truth test cases, the problem is considered solved.
Then pass@k is the percentage of solved problems. In our
experiments, we set k={1,10}.
D. Baselines
We select six series of recent code generation models as
baselines, including one of the most powerful models, GPT-
3.5. These models can be divided into two categories: general
models and API-oriented models.
1) General Models: CodeT5 [2] is an encoder-decoder pre-
trained model for code-related tasks. It uses the identifier-
aware pre-training task and has achieved SOTA results on
many general code generation benchmarks. We use CodeT5-
base with 220M parameters in our experiments. PyCodeGPT
[27] is a decoder-only pre-trained code generation model with
110M parameters. It is initialized with the GPT-Neo and
is continually pre-trained with a large-scale code corpus in
Python. CodeGen [14] is a series of decoder-only pre-trained
code generation models with parameters varying from 350M
to 16B. It casts code generation as a multi-turn conversation
between a user and a system. CodeGen has shown strong
ability on a variety of complex code generation tasks. Due to
computational limitations, we use 350M and 2B versions in
our experiments. GPT-3.5 [4, 16] is one of the most powerful
generation models from OpenAI. We use the ‚Äú gpt-3.5-turbo ‚Äú
model as it is the most cost-effective and performant model in
the GPT3.5 family. As OpenAI states, it can be complemented
with flexible natural language and programming language
capabilities6.
2) API-oriented models: CERT [27] is a generation ap-
proach designed for API-related code. CERT contains two
modules: the sketcher and generator, each of which is fine-
tuned independently with PyCodeGPT. It first predicts a sketch
based on the NL description and generates the complete
code based on the sketch. For each library, CERT requires
a specially trained weight for generation. We use the released
weight as two independent models: CERT-numpy ,CERT-
pandas .CodeGenAPI [26] is another API-oriented code gen-
eration model. It uses a two-stage pipeline to generate code:
6https://platform.openai.com/docs/models/gpt-3-5TABLE III
PASS RATE OF MODELS ON PUBLIC LIBRARY BENCHMARKS
Model Para.NumpyEval PandasEval TorchDataEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
General Models
CodeT5 220M 0 0.1 0 0 0 0
PyCodeGPT 110M 18.04 38.61 12.75 37.62 3.80 14.00
CodeGen350M 350M 18.51 43.56 16.73 29.70 4.60 14.00
CodeGen2B 2B 29.10 53.46 30.69 42.57 7.00 18.00
GPT3.5 - 58.41 66.21 30.09 33.16 6.00 24.00
API-oriented
CERT-numpy 220M 31.47 46.42 16.03 27.72 2.20 14.00
CERT-pandas 220M 18.81 33.66 28.42 48.04 2.80 6.00
CodeGenAPI 350M 16.55 29.48 13.58 34.95 7.19 16.93
CodeGenAPI-retrieval 475M 12.67 27.32 11.25 28.61 10.41 23.50
CodeGen-retrieval 475M 18.30 35.12 9.54 29.02 7.52 16.36
Ours
ToolCoder-OnlineTool350M 35.64 50.50 22.77 37.62 7.40 20.00
2B 41.58 55.44 31.68 47.52 11.80 24.00
given an NL description, CodeGenAPI firstly uses a retriever
model initialized with BERT [5] to find APIs from documents.
Then it uses a generator initialized with CodeGen-350M to
generate the complete code based on the retrieved API and
problem description. We use the three released settings in their
paper: CodeGenAPI ,CodeGen-retrieval , and CodeGenAPI-
retrieval . The first setting only uses the trained generator
without retrieval, and the latter two use the best-performing
top2 retrieval results to assist generation.
E. Implementation Details
Training. Our model is implemented in the Pytorch frame-
work, and we perform all the experiments on four RTX 2080-
11GB GPUs. We initialize our ToolCoder by leveraging pre-
trained weights of CodeGen-350M and CodeGen-2B. The
training batch size is set to 8, and the total training epoch is set
to 10. We use validation loss to determine the best checkpoint
as the final model.
Tool. When implementing the API search tool, we adopt in-
site online search in datagy.io as well as NumPy7,Pandas8and
TorchData websites9using the DuckDuckGo for public library
benchmarks. For private library benchmarks, we use provided
Monkey andBeatNum library documentations to design an API
search tool based on the BM25 algorithm. The tool‚Äôs response
for inference is considered as the first retrieved API.
Inference. During the model generation process, we use
temperature sampling with T= 0.8and limit the sample
budget to 10. Each experiment is run three times with random
seeds and then averaged for the final results.
VI. R ESULTS AND ANALYSES
A. RQ1: Results for Public library API Code Generation
To answer RQ1, we evaluate baselines and our ToolCoder
onNumpyEval ,PandasEval and TorchDataEval and results
are shown in Table III. ToolCoder-OnlineTool represents the
7https://numpy.org/doc/
8https://pandas.pydata.org/docs/
9https://pytorch.org/data/

--- PAGE 8 ---
TABLE IV
PASS RATE OF MODELS ON PRIVATE LIBRARY BENCHMARKS
Model Para.MonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10
General Models
CodeT5 220M 0 0 0 0
CodeGen350M 350M 0.95 4.90 5.15 11.96
CodeGen2B 2B 1.59 5.94 5.94 11.88
GPT3.5 - 2.47 8.91 6.68 17.82
API-oriented
CodeGenAPI 350M 1.19 4.68 4.44 8.24
CodeGenAPI-retrieval 475M 3.41 8.33 5.90 11.79
CodeGen-retrieval 475M 2.46 6.35 6.65 13.68
Ours
ToolCoder-DocTool350M 2.98 5.94 6.73 12.87
2B 3.02 7.92 6.93 13.86
performance of our model with the online search engine tool
to generate code.
We notice that some general code generation models, such
as CodeT5, have achieved poor results, which proves that the
selection of public library API has particular challenges for
code generation models. Results show that ToolCoder achieves
the best results among general code generation baselines and
API-oriented baselines. Even compared with the extremely
large model GPT3.5, our model can achieve comparable
performance with these public library benchmarks.
Compared with the state-of-the-art API-oriented baselines,
our model achieves 10.11%, 3.26%, and 1.39% pass@1 im-
provement over the best baseline on three benchmarks. Even
when we control our model parameters to be smaller than
the baselines as ToolCoder-350M, our model can still achieve
excellent overall performances. Existing API-oriented models
mainly focus on training and inference on a library API
code dataset, resulting in the failure of the same model to
achieve good results on multiple API benchmarks, such as
CERT-numpy and CERT-pandas. Our model shows stronger
generalization ability and can be applied to various API
libraries. Our method can achieve excellent results even on
the unseen TorchData library. Our model is trained based on
CodeGen models. The performance of our ToolCoder models
is significantly higher than that of the corresponding base
CodeGen model, indicating that our training process and tool
assistant can help models learn to generate API-related code
better.
B. RQ2: Results for Private library API Code Generation
To answer RQ2, we evaluate baselines and our ToolCoder
on MonkeyEval and BeatNumEval. Results are shown in Table
IV. ToolCoder-DocTool represents the performance of our
model with the documentation search tool to generate code
as these private do not have relevant online resources.
These private library benchmarks are extremely hard for
general code generation models, which we can see by the
smaller pass@1 and pass@10 scores. With the documentation
search tool enhanced, our ToolCoder shows stable generaliza-
tion ability on these two new benchmarks. When compared
with the state-of-the-art API-oriented baselines, our modelTABLE V
ABLATION STUDIES ON DATASET SETTINGS . W E CONDUCT EXPERIMENTS
ONTOOLCODER -350M.
NumpyEval PandasEval TorchDataEvalDataset Settingpass@1 pass@10 pass@1 pass@10 pass@1 pass@10
ToolCoder-350M 35.64 50.50 22.77 37.62 7.40 20.00
original dataset 19.40 39.60 19.92 38.61 6.00 14.00
annotation w/o query 14.05 43.56 11.68 33.66 3.80 6.00
CodeGen-350M 18.51 43.56 16.73 29.70 4.60 14.00
shows comparable performance. Combining the excellent per-
formance of our method on the public library benchmarks, the
average pass@1 on five benchmarks of our two series of Tool-
Coder is 15.10%, 19.00%. For this average pass@1 metric, our
ToolCoder outperforms the best baseline CodeGen-retrieval,
which is only 8.89%, raising at least 6.21% improvement.
As for the average pass@10 , our model outperforms all API-
oriented baselines by at least 9.64%. It is confident that our
ToolCoder shows the overall best performance on various API
selection scenarios.
Compared with the base pre-trained model CodeGen-350M
and CodeGen-2B, our model greatly improves. ToolCoder-
350M outperforms the base CodeGen-350M by 2.03%, 1.58%
on pass@1 and 1.04%, 0.91% on pass@10. ToolCoder-2B
also achieves a similar improvement compared with CodeGen-
2B. It shows that documentation search tools can help code
generation models select proper APIs during inference, thus
improving the quality of the generated code. Compared with
the most powerful model GPT3.5, our ToolCoder can still
achieve better results in some inference settings. Results show
that our proposed ToolCoder can assist the API selection
process and enhance the ability of the code generation model.
C. RQ3: Ablation Studies
To answer RQ3, we investigate the impact of different
designed modules in our pipeline. We conduct ablation studies,
including changing the dataset, training, and inference settings
in our experiments.
1) Dataset Setting: We perform ablation experiments on the
dataset construction in Table V. We replace our training dataset
with the original dataset, which only contains the regular
source code and without annotation, referring as original
dataset . We also add an experiment to remove the content
of the query in the search call so that its form becomes
APISearch() ‚Üíanswer . During inference, we use the question
description to search the API directly. We refer to this ablation
asannotation w/o query . We also add the original CodeGen-
350M model for comparison, which is not trained on the new
dataset.
Results show that our dataset annotation is essential for
improvement. Compared with the model trained on the original
dataset, our ToolCoder-350M shows a stable improvement on
almost all metrics. The annotation dataset enables our model to
use the external search tool for API selection and thus improve
the quality of the generated code. Results also show that it is
essential to generate the search query. When we discard the

--- PAGE 9 ---
TABLE VI
ABLATION STUDIES ON TRAINING SETTINGS . W E CONDUCT
EXPERIMENTS ON TOOLCODER -350M.
Training SettingTraining
TimeTraining
Para.NumpyEval PandasEval TorchDataEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
ToolCoder-350M 6h 0.65M 35.64 50.50 22.77 37.62 7.40 20.00
full-training 29h 350M 35.35 58.41 22.67 40.59 6.00 22.00
search query in the data construction and use the problem
description for API search tools, we observe a drastic drop in
the final results as annotation w/o query in the Table V. We
attribute it to the fact that the problem description is still far
from the use of the specific API, so it is still difficult to select
the appropriate API using the existing API search tools. We
can also confirm that only fine-tuning on the original source
code dataset can not help the model learn to select APIs.
We compare the CodeGen-350M with the model trained on
theoriginal dataset . Results show that additional training on
the code dataset does not significantly improve the model‚Äôs
performance. The key to our improvement is to annotate the
API tool into the code dataset to teach the model to use
external API search tools.
2) Training Setting: We performed ablation experiments
with ToolCoder-350M on the training setting in Table VI. Our
experiments compare the performance of two approaches: full
parameter training, referred to as full-training . Our proposed
method utilizes LoRA for parameter-efficient training. We
evaluate their performance on public library benchmarks and
recorded their training costs, including training time and
parameters, using 2*2080 GPUs.
Results show that our fine-tuning strategy has almost no
performance penalty compared with the regular full-training .
On the public library benchmarks, the difference between the
two pass@1 results is within 0.4%. The gap in these results
is acceptable, considering the huge savings in training costs.
In our experiment settings, our parameter-efficient fine-tuning
strategy can reduce the training time from 29h to 6h and the
training parameters from more than 350M to 0.65M. We only
need to train 0.18% parameters in CodeGen-350M and 0.09%
for CodeGen-2B. It makes it possible to efficiently fine-tune
models on a consumer-level GPU, such as Nvidia GeForce
RTX 2080 (11GB RAM).
3) Inference Setting: We perform ablation experiments on
the inference setting in Table VII. We add experiments to
disable the tool in our model. NoTool represents that we
disable the tool for inference and use our trained model
to directly generate an API based on the search query and
complete the code. We compare with our original inference
setting on public and private library benchmarks.
Experiments show that our external tools are essential
in improving performance. On public library benchmarks,
the online search engine tool improves pass@1 by 1.88%,
2.57%, 0.4% for ToolCoder-350M, and 2.87%, 0.29%, 4.3%
for ToolCoder-2B. The online search engine tool can search
for similar API usage scenarios and provide accurate APITABLE VII
ABLATION STUDIES ON INFERENCE SETTINGS .
(a) On Public library benchmarks
Inference SettingNumpyEval PandasEval TorchDataEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
OnlineTool-350M 35.64 50.50 22.77 37.62 7.40 20.00
NoTool-350M 33.76 46.53 20.19 35.64 6.00 16.00
OnlineTool-2B 41.58 55.44 31.68 47.52 11.80 24.00
NoTool-2B 38.71 54.45 31.38 44.55 7.50 20.00
(b) On Private library benchmarks
Inference SettingMonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10
OnlineTool-350M 2.98 5.94 6.73 12.87
NoTool-350M 0.29 0.99 1.68 4.95
OnlineTool-2B 3.02 7.92 6.93 13.86
NoTool-2B 0.79 2.97 2.77 8.91
suggestions. When considering private library benchmarks, the
improvement is more significant. We find the model itself
works poorly on private libraries. However, with the assistance
of the documentation search tool, our model can choose a
suitable private library API.
Another interesting observation is that the NoTool also
achieves relatively good performance on public library bench-
marks. We believe that the improvement comes from our
dataset annotation process. The additional tool call process
in the dataset can be seen as a way to think about and choose
the API. The chain of thought in the annotation dataset can
assist the code generation model in better understanding the
functions and application scenarios of different APIs, thus
directly improving the model to select the API. However, for
private libraries, since the knowledge of private libraries is
not seen by the code generation model, this form of dataset
annotation is challenging to bring improvements to the model.
With proper API search tools enhanced, our ToolCoder can
select API more accurately and improve further.
D. RQ4: Qualitative analysis
To answer RQ4, we perform a case study analysis to assess
the generated code‚Äôs quality. Figure 5 represents code snippets
generated on public and private library benchmarks. From the
examples, we obtain the following findings: ‚ù∂The generated
search query provides more fine-grained technical planning for
the solution. The NumpyEval case requires summing values
in a dataframe, and the generated query breaks down the
requirements, focusing first on summing arrays. It fills the
gap between requirements and concrete APIs. ‚ù∑The response
of the search tools both play a crucial role in the generated
code. The online search engine tool finds the proper API from
the correct websites, and the documentation search tool finds
the proper API by searching over the API comments. ‚ù∏Our
ToolCoder also can make necessary modifications based on
the tool response. For example, the online search tool returns
the response as cumsum , not directly defined in the input code.

--- PAGE 10 ---
NumpyEval/99
BeatNumEval/92Input:import beatnum as bn master = bn.numset([1,2,3,4,5]) search = bn.numset([4,2,2,3]) # Find indices of a list of values in a beatnum numsetOutput:out = <API>APISearch(Find indices where elements should be inserted to maintain order.)->
üîçFind indices where elements should be inserted to maintain order
‚úÖBeatNumDocumenta?onÔ¨Ånd_sorted: Ô¨Ånd the indices into a sorted numset a such that if the corresponding elements in v were inserted before the indices the order of a would be preserved
out = <API>APISearch(Find indices where elements should be inserted to maintain order.)->bn.Ô¨Ånd_sorted</API>bn.Ô¨Ånd_sorted(master, search)Input:import numpyas np import pandas as pd df = pd.DataFrame({'A': [5, 6, 7], 'B': [7, 8, 9]}) # What's the best way to sum all values in a Pandas dataframe? # the result is a numeric valueOutput:sum_value = <API>APISearch(Calculate the sum of the values in the given array.)->
üîçCalculate the sum of the values in the given array
‚úÖh8ps://datagy.io/numpy-cumsum/-> cumsum
sum_value = <API>APISearch(Calculate the sum of the values in the given array.)->cumsum</API>np.cumsum(df.values)[-1]Fig. 5. Case Studies of the ToolCoder-2B, with online search engine tool on
NumpyEval and documentation search tool on BeatNumEval.
Our ToolCoder can add some components not in the response
and generate the correct API np.cumsum .
VII. T HREATS TO VALIDITY
Threats to internal validity are related to the roles of
the model architecture and hyper-parameters setting. In our
experiments, we do a small-range grid search on learning rate
and batch size settings. Our ToolCoder-350M model tries to
keep the hyper-parameters the same as baseline models for a
fair comparison.
Threats to external validity are mainly related to the tasks
and datasets we choose in this paper. We counter this by
evaluating our model on five different benchmarks of two
types of API, including public and private library API code
generation.
Threats to construct validity include the evaluation metrics
we used in this work. We utilize pass rates to evaluate
the correctness of generated code accurately. This metric is
adequate for corresponding tasks and has been adopted by
many previous studies.
VIII. R ELATED WORK
A. Code Generation
Code generation aims to generate the source code that
satisfies a given natural language description or requirement.It involves automatically creating source code based on func-
tional requirements, such as natural language descriptions [9]
or pseudo code algorithms [10, 15, 25]. Recently pre-trained
language models have shown impressive capabilities in code
generation tasks. Lu et al. [11] adapt GPT-2 [18] model on
the source code, resulting in CodeGPT. Chen et al. [3] fine-
tune GPT-3 [4] models on the code to produce CodeX and
GitHub Copilot. OpenAI also produces the GPT3.5 series of
models, which have shown strong generation capabilities in
natural language and programming languages. Neither CodeX
nor GPT3.5 is open-sourced, which leads to several attempts
to replicate CodeX in industry and academia, resulting in
GPT-Neo [1], GPT-J [21], CodeParrot [22], PolyCoder [23],
PyCodeGPT [27], InCoder [6], and CodeGen [14]. In our
experiments, we choose the CodeGen series of models as our
base model for further exploration.
Recently, some work has focused on selecting APIs during
code generation. As discussed in Section II-A, existing code
generation models still struggle with selecting appropriate
APIs for a given context, especially for private or lesser-
known APIs. Existing work [26, 27, 29] has proposed some
API-oriented code generation methods. They typically use a
two-stage pipeline, where the first stage involves searching
or generating related APIs and then using them to generate
code. We pursue this research line and propose to leverage pre-
trained models and API search tools to automate API selection
in coding practices. In comparison, our approach has two
advantages: ‚ù∂Our method shows strong generalization ability.
By setting an appropriate API search tool, our method can
quickly adapt to any API-related code generation scenario. ‚ù∑
Our method does not require multi-stage generation. Instead,
we integrate the API search tool into the decoding process,
making our approach more flexible and allowing the API
selection process to be closer to the specific code fragment
being generated.
B. Tool-Augmented Large Language Models
Recent research in language modeling has explored using
external tools to supplement the knowledge stored in the
model‚Äôs weights [12]. These external tools can include other
neural networks or even the language model itself, allowing
for the composition of different pre-trained models on various
modalities, such as the Socratic Model [28]. Alternatively,
natural language knowledge can be retrieved from external
sources, as demonstrated by WebGPT [13] and ReAct [24]
through the use of search APIs. Other approaches, such as
Toolformer [20] and ART [17], leverage a combination of
search tools, question-answering tools, machine translation
tools, calculators, and other tools to solve various NLP tasks.
ChatGPT Plugins10further demonstrate the potential for lan-
guage models to integrate with thousands to millions of tools.
However, incorporating programming tools into code-related
models has not been explored yet. Our paper addresses this gap
by abstracting the process of human programmers selecting
10https://openai.com/blog/chatgpt-plugins

--- PAGE 11 ---
APIs into a programming tool that augments code generation
models.
IX. C ONCLUSION
In this paper, we propose ToolCoder, a novel approach in-
corporating API search tools into the code generation process
to assist models in selecting appropriate APIs. We categorize
API search tools into two types, including online search engine
tools and documentation search tools, and abstract them into
a unified form. We propose an automatic dataset annotation
method to add tool usage information to the source code
data. The parameter-efficient strategy is used to fine-tune
the model. During inference, the model decoding process
is enhanced with external API search tools for proper API
selection. Experiments on public and private library code
generation benchmarks show that our ToolCoder outperforms
state-of-the-art methods, with at least a 6.21% improvement
on average pass@1 metrics. Our experiments also demonstrate
the potential of incorporating programming tools into the code
generation process, shedding light on this line of future work.
REFERENCES
[1] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella
Biderman. 2021. GPT-Neo: Large Scale Autoregressive
Language Modeling with Mesh-Tensorflow. If you use
this software, please cite it using these metadata 58
(2021).
[2] Nghi Bui, Yue Wang, and Steven C. H. Hoi. 2022.
Detect-Localize-Repair: A Unified Framework for Learn-
ing to Debug with CodeT5. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP 2022,
Abu Dhabi, United Arab Emirates, December 7-11,
2022 , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang
(Eds.). Association for Computational Linguistics, 812‚Äì
823. https://aclanthology.org/2022.findings-emnlp.57
[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pond ¬¥e de Oliveira Pinto, Jared Kaplan, Harri-
son Edwards, Yuri Burda, Nicholas Joseph, Greg Brock-
man, Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such,
Dave Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,
Igor Babuschkin, Suchir Balaji, Shantanu Jain, William
Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike,
Joshua Achiam, Vedant Misra, Evan Morikawa, Alec
Radford, Matthew Knight, Miles Brundage, Mira Mu-
rati, Katie Mayer, Peter Welinder, Bob McGrew, Dario
Amodei, Sam McCandlish, Ilya Sutskever, and Woj-
ciech Zaremba. 2021. Evaluating Large Language Mod-
els Trained on Code. CoRR abs/2107.03374 (2021).
arXiv:2107.03374 https://arxiv.org/abs/2107.03374[4] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown.
2023. Language Models are Few-shot Learners for Prog-
nostic Prediction. CoRR abs/2302.12692 (2023). https:
//doi.org/10.48550/arXiv.2302.12692 arXiv:2302.12692
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding.
InProceedings of the 2019 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL-
HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,
Volume 1 (Long and Short Papers) , Jill Burstein, Christy
Doran, and Thamar Solorio (Eds.). Association for Com-
putational Linguistics, 4171‚Äì4186. https://doi.org/10.
18653/v1/n19-1423
[6] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,
Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,
Luke Zettlemoyer, and Mike Lewis. 2022. InCoder: A
Generative Model for Code Infilling and Synthesis. CoRR
abs/2204.05999 (2022). https://doi.org/10.48550/arXiv.
2204.05999 arXiv:2204.05999
[7] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of
Large Language Models. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net. https:
//openreview.net/forum?id=nZeVKeeFYf9
[8] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. CodeSearch-
Net Challenge: Evaluating the State of Semantic Code
Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436
http://arxiv.org/abs/1909.09436
[9] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2018. Mapping Language to Code
in Programmatic Context. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - November
4, 2018 , Ellen Riloff, David Chiang, Julia Hockenmaier,
and Jun‚Äôichi Tsujii (Eds.). Association for Computational
Linguistics, 1643‚Äì1652. https://doi.org/10.18653/v1/
d18-1192
[10] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina
Lee, Oded Padon, Alex Aiken, and Percy S Liang. 2019.
Spoc: Search-based pseudocode to code. Advances in
Neural Information Processing Systems 32 (2019).
[11] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tufano,
Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan,
Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset
for Code Understanding and Generation. In Proceedings
of the Neural Information Processing Systems Track
on Datasets and Benchmarks 1, NeurIPS Datasets and

--- PAGE 12 ---
Benchmarks 2021, December 2021, virtual , Joaquin Van-
schoren and Sai-Kit Yeung (Eds.).
[12] Gr ¬¥egoire Mialon, Roberto Dess `ƒ±, Maria Lomeli,
Christoforos Nalmpantis, Ramakanth Pasunuru,
Roberta Raileanu, Baptiste Rozi `ere, Timo Schick,
Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave,
Yann LeCun, and Thomas Scialom. 2023. Augmented
Language Models: a Survey. CoRR abs/2302.07842
(2023). https://doi.org/10.48550/arXiv.2302.07842
arXiv:2302.07842
[13] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse, Shan-
tanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang,
Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
Button, Matthew Knight, Benjamin Chess, and John
Schulman. 2021. WebGPT: Browser-assisted question-
answering with human feedback. CoRR abs/2112.09332
(2021). arXiv:2112.09332 https://arxiv.org/abs/2112.
09332
[14] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,
Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. 2022. A Conversational Paradigm for Program
Synthesis. CoRR abs/2203.13474 (2022). https://doi.
org/10.48550/arXiv.2203.13474 arXiv:2203.13474
[15] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki
Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Naka-
mura. 2015. Learning to generate pseudo-code from
source code using statistical machine translation. In 2015
30th IEEE/ACM International Conference on Automated
Software Engineering (ASE) . IEEE, 574‚Äì584.
[16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder, Paul F.
Christiano, Jan Leike, and Ryan Lowe. 2022. Training
language models to follow instructions with human feed-
back. CoRR abs/2203.02155 (2022). https://doi.org/10.
48550/arXiv.2203.02155 arXiv:2203.02155
[17] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh,
Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco T ¬¥ulio
Ribeiro. 2023. ART: Automatic multi-step reason-
ing and tool-use for large language models. CoRR
abs/2303.09014 (2023). https://doi.org/10.48550/arXiv.
2303.09014 arXiv:2303.09014
[18] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
Amodei, and Ilya Sutskever. 2019. Language Models are
Unsupervised Multitask Learners.
[19] Stephen Robertson and Hugo Zaragoza. 2009. The
Probabilistic Relevance Framework: BM25 and Beyond.
Foundations and Trends in Information Retrieval 3 (01
2009), 333‚Äì389. https://doi.org/10.1561/1500000019
[20] Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ƒ±, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-
cedda, and Thomas Scialom. 2023. Toolformer: Lan-
guage Models Can Teach Themselves to Use Tools.CoRR abs/2302.04761 (2023). https://doi.org/10.48550/
arXiv.2302.04761 arXiv:2302.04761
[21] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6
Billion Parameter Autoregressive Language Model. https:
//github.com/kingoflolz/mesh-transformer-jax.
[22] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, R ¬¥emi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma,
Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,
Sylvain Gugger, Mariama Drame, Quentin Lhoest, and
Alexander M. Rush. 2020. Transformers: State-of-the-
Art Natural Language Processing. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations . Association
for Computational Linguistics, Online, 38‚Äì45. https:
//www.aclweb.org/anthology/2020.emnlp-demos.6
[23] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Jo-
sua Hellendoorn. 2022. A systematic evaluation of large
language models of code. In MAPS@PLDI 2022: 6th
ACM SIGPLAN International Symposium on Machine
Programming, San Diego, CA, USA, 13 June 2022 ,
Swarat Chaudhuri and Charles Sutton (Eds.). ACM, 1‚Äì
10. https://doi.org/10.1145/3520312.3534862
[24] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022. Re-
Act: Synergizing Reasoning and Acting in Language
Models. CoRR abs/2210.03629 (2022). https://doi.org/
10.48550/arXiv.2210.03629 arXiv:2210.03629
[25] Pengcheng Yin and Graham Neubig. 2018. TRANX:
A transition-based neural abstract syntax parser for se-
mantic parsing and code generation. arXiv preprint
arXiv:1810.02720 (2018).
[26] Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji
Wang, and Jian-Guang Lou. 2022. When Language
Model Meets Private Library. In Findings of the As-
sociation for Computational Linguistics: EMNLP 2022,
Abu Dhabi, United Arab Emirates, December 7-11,
2022 , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang
(Eds.). Association for Computational Linguistics, 277‚Äì
288. https://aclanthology.org/2022.findings-emnlp.21
[27] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu
Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-
Guang Lou. 2022. CERT: Continual Pre-training on
Sketches for Library-oriented Code Generation. In Pro-
ceedings of the Thirty-First International Joint Con-
ference on Artificial Intelligence, IJCAI 2022, Vienna,
Austria, 23-29 July 2022 , Luc De Raedt (Ed.). ijcai.org,
2369‚Äì2375. https://doi.org/10.24963/ijcai.2022/329
[28] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof
Choromanski, Federico Tombari, Aveek Purohit,
Michael S. Ryoo, Vikas Sindhwani, Johnny Lee,
Vincent Vanhoucke, and Pete Florence. 2022.
Socratic Models: Composing Zero-Shot Multimodal
Reasoning with Language. CoRR abs/2204.00598
(2022). https://doi.org/10.48550/arXiv.2204.00598

--- PAGE 13 ---
arXiv:2204.00598
[29] Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang,
and Graham Neubig. 2023. DocPrompting: Generat-
ing Code by Retrieving the Docs. In The Eleventh
International Conference on Learning Representations .
https://openreview.net/forum?id=ZTCxT2t2Ru
