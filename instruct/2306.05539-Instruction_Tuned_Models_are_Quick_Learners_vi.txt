# Mô hình được Điều chỉnh Hướng dẫn là Người học Nhanh
Himanshu Gupta1}Saurabh Arjun Sawant1}Swaroop Mishra1|
Mutsumi Nakamura1Arindam Mitra2Santosh Mashetty1Chitta Baral1
1Đại học Bang Arizona2Microsoft Research
{hgupta35, ssawan13, srmishr1, mutsumi, cbaral}@asu.edu
Tóm tắt
Việc điều chỉnh hướng dẫn của các mô hình ngôn ngữ đã chứng minh khả năng tăng cường
khả năng tổng quát hóa mô hình cho các tác vụ chưa thấy thông qua học tập trong ngữ cảnh
sử dụng một vài ví dụ. Tuy nhiên, học có giám sát thông thường vẫn đòi hỏi rất nhiều dữ liệu
huấn luyện downstream để tinh chỉnh. Thường trong các tình huống thực tế, có sự khan hiếm
dữ liệu có sẵn để tinh chỉnh, nằm ở đâu đó giữa suy luận few shot và tinh chỉnh có giám sát
đầy đủ. Trong nghiên cứu này, chúng tôi chứng minh hiệu quả mẫu của các mô hình được
điều chỉnh hướng dẫn trên nhiều tác vụ khác nhau bằng cách ước tính lượng dữ liệu huấn
luyện downstream tối thiểu mà chúng cần để thực hiện học chuyển giao và đạt được hiệu
suất của các mô hình có giám sát hiện đại nhất (SOTA). Chúng tôi tiến hành thí nghiệm trên
119 tác vụ từ Super Natural Instructions (SuperNI) trong cả hai thiết lập học tác vụ đơn
(STL) và học đa tác vụ (MTL). Các phát hiện của chúng tôi tiết lộ rằng, trong thiết lập STL,
các mô hình được điều chỉnh hướng dẫn được trang bị 25% dữ liệu huấn luyện downstream
vượt qua hiệu suất SOTA trên các tác vụ downstream. Trong thiết lập MTL, một mô hình
được điều chỉnh hướng dẫn được huấn luyện chỉ trên 6% dữ liệu huấn luyện downstream
đạt được SOTA, trong khi sử dụng 100% dữ liệu huấn luyện dẫn đến cải thiện 3,69 điểm
phần trăm (ROUGE-L 74,68) so với SOTA trước đó. Chúng tôi tiến hành phân tích về T5
so với Tk-Instruct bằng cách phát triển một số đường cơ sở để chứng minh rằng điều chỉnh
hướng dẫn giúp tăng cả hiệu quả mẫu và học chuyển giao. Ngoài ra, chúng tôi quan sát
thấy sự tăng hiệu suất nhất quán 4% trong cả hai thiết lập khi tiền tinh chỉnh được thực hiện
với hướng dẫn. Cuối cùng, chúng tôi tiến hành nghiên cứu phân loại và phát hiện rằng trái
với các kết quả trước đó, các tác vụ trong các danh mục viết lại câu hỏi và tạo tiêu đề bị
thiệt hại từ điều chỉnh hướng dẫn.1

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã đạt được hiệu suất đáng kể trên một số bộ đánh giá chuẩn
như SuperGLUE [39], BIG-Bench Hard (BBH) [38], và HELM [17]. Nghiên cứu về LLM đã khám
phá khả năng tuân theo hướng dẫn của chúng [43, 25, 42] và đã phát triển các mô hình chuyên biệt
cho mục đích này (Flan, Instruct-GPT, Tk-Instruct, T0) [43, 27, 33]. Các nghiên cứu gần đây trong
mô hình hướng dẫn chứng minh khả năng tổng quát hóa của các mô hình được điều chỉnh hướng
dẫn trên các tác vụ huấn luyện và được đánh giá bằng suy luận few shot [42, 43], như được thể
hiện trong hàng đầu tiên của Hình 1. Mặc dù vậy, hiệu suất SOTA được đạt được bằng tinh chỉnh
có giám sát đầy đủ trên tất cả dữ liệu huấn luyện downstream có sẵn, như được thể hiện trong
hàng thứ 4 của Hình 1. Trong các tình huống thực tế, thường có một lượng dữ liệu hạn chế có sẵn
để tinh chỉnh, nằm ở đâu đó giữa suy luận few shot và tinh chỉnh có giám sát đầy đủ. Với bối cảnh
này, chúng tôi đặt ra câu hỏi - nếu chúng ta sử dụng một lượng nhỏ dữ liệu từ các tác vụ downstream
này, mô hình có thể học nhanh như thế nào trong mô hình hướng dẫn?

1}Đồng tác giả đầu tiên |Hiện tại ở Google Brain
Phương pháp cơ sở, chia tách dữ liệu và các script có sẵn miễn phí tại https://github.com/srsawant34/
efficient_instruction_learning
Bản thảo. Đang được xem xét.arXiv:2306.05539v1 [cs.CL] 17 May 2023

Hình 1: Thể hiện sự khác biệt giữa suy luận few shot, tinh chỉnh có giám sát đầy đủ, và phân tích
được đề xuất của chúng tôi. Hàng đầu tiên đại diện cho suy luận few shot thông thường sử dụng
Tk-Instruct cho kết quả điểm số 54,30. Hàng thứ tư chỉ ra SOTA có giám sát sử dụng 100% dữ
liệu huấn luyện downstream để tinh chỉnh T5-3B để có điểm SOTA là 70,99. Các phát hiện của
chúng tôi chứng minh khả năng học nhanh của mô hình được điều chỉnh hướng dẫn. Chỉ sử dụng
6% dữ liệu huấn luyện downstream, Tk-Instruct đạt điểm số 70,40. Vượt qua SOTA 2 điểm với
25% dữ liệu huấn luyện downstream, kết quả của chúng tôi làm nổi bật thiết lập MTL.

Để trả lời điều này, chúng tôi đánh giá lượng dữ liệu huấn luyện downstream tối thiểu cần thiết
cho các mô hình được điều chỉnh hướng dẫn để thực hiện học chuyển giao và phù hợp với hiệu
suất của các mô hình SOTA có giám sát. Chúng tôi thí nghiệm trên các tác vụ chưa thấy của Super
Natural Instructions (SuperNI) [42], bao gồm 119 tác vụ. Chúng tôi thí nghiệm với học tác vụ
đơn (STL), tức là huấn luyện 119 mô hình riêng biệt cho từng tác vụ, và học đa tác vụ (MTL), nơi
một mô hình đơn được huấn luyện để giải quyết tất cả 119 tác vụ. Chúng tôi sử dụng Tk-Instruct
3B (T5-3B, được điều chỉnh hướng dẫn trên 757 tác vụ của SuperNI) làm mô hình được điều chỉnh
hướng dẫn [42] và sử dụng T5-3B [31] làm mô hình không được điều chỉnh hướng dẫn. Chúng tôi
phát hiện rằng trong thiết lập STL, chúng tôi đạt được kết quả cạnh tranh chỉ với 5,91% dữ liệu
huấn luyện (68,34 ROUGE-L) và vượt qua điểm SOTA có giám sát khi chỉ sử dụng 25,33% toàn
bộ tập dữ liệu (71,71 ROUGE-L). Trong thiết lập MTL, khi sử dụng 6% tập huấn luyện, chúng
tôi phù hợp với hiệu suất SOTA (70,40 ROUGE-L), như được thể hiện trong hàng thứ 2 của Hình
1. Chúng tôi vượt trội hơn SOTA khoảng 2% (73,14 ROUGE-L) khi sử dụng 25% tập huấn luyện
(hàng thứ 3 của Hình 1.) và 3,69% khi sử dụng 100% tập huấn luyện. Theo hiểu biết của chúng
tôi, chúng tôi là những người đầu tiên khám phá không gian hiệu quả mẫu trong các mô hình ngôn
ngữ lớn được điều chỉnh hướng dẫn trong cả thiết lập STL và MTL. Chi tiết về thiết lập thí nghiệm
được mô tả trong §4, và kết quả được mô tả trong §5.

Chúng tôi phân tích tác động của hướng dẫn bằng cách điều tra hiệu quả mẫu trên các phạm vi
đa dạng, bằng cách phát triển nhiều đường cơ sở để mô phỏng các thiết lập ít tài nguyên liên quan
đến tính khả dụng của dữ liệu huấn luyện. Các phát hiện của chúng tôi làm nổi bật hiệu quả mẫu
đạt được thông qua điều chỉnh hướng dẫn, đạt đến 75%, ngay cả trong dữ liệu huấn luyện hạn chế.
Chúng tôi đi sâu vào tác động của điều chỉnh hướng dẫn như một bước tiền tinh chỉnh ban đầu.
Chúng tôi phát triển hai đường cơ sở (cho cả thiết lập STL và MTL) sử dụng tiền tinh chỉnh mà
không có hướng dẫn. Các đường cơ sở này trải qua tinh chỉnh thêm trên tập huấn luyện downstream.
Các phát hiện của chúng tôi chứng minh sự tăng hiệu suất của Tk-Instruct so với các đường cơ
sở 3% và 5% trong thiết lập STL và MTL tương ứng. Điều này làm nổi bật tác động của hướng
dẫn trong tiền tinh chỉnh về mặt tạo điều kiện cho học chuyển giao. Cuối cùng chúng tôi thực hiện
phân tích theo danh mục để điều tra tác động của điều chỉnh hướng dẫn trên các danh mục tác vụ
khác nhau. Các phát hiện của chúng tôi tiết lộ rằng các tác vụ thuộc danh mục suy luận văn bản
cho thấy những cải thiện đáng kể nhất thông qua điều chỉnh hướng dẫn. Mặt khác, các tác vụ liên
quan đến viết lại câu hỏi và tạo tiêu đề thể hiện thách thức và hạn chế khi chịu điều chỉnh hướng
dẫn.

Đóng góp: (a) chúng tôi cho thấy rằng một mô hình được điều chỉnh hướng dẫn chỉ sử dụng 6%
dữ liệu huấn luyện downstream phù hợp với hiệu suất của một mô hình SOTA có giám sát. (b) chúng
tôi phát hiện rằng các mô hình được điều chỉnh hướng dẫn hoạt động tốt hơn đến 3% so với SOTA
khi được điều chỉnh hướng dẫn với 100% dữ liệu. (c) để điều tra các tình huống với dữ liệu huấn
luyện downstream rất hạn chế, chúng tôi tiến hành phân tích toàn diện bằng cách xây dựng nhiều
đường cơ sở. (d) chúng tôi cho thấy tác động của phương pháp của chúng tôi trên các danh mục
tác vụ khác nhau.

2 Nghiên cứu Liên quan
Học đa tác vụ sử dụng LLM (Mô hình Ngôn ngữ) đã liên tục cho thấy lợi ích hiệu suất so với học
riêng biệt cho từng tác vụ [25, 50, 18, 2, 46, 3, 53, 5]. Học dựa trên hướng dẫn đã nổi lên như
một mô hình đầy hứa hẹn trong LLM [22, 35, 27, 12, 9, 20, 34, 23, 1, 37], với các nghiên cứu
gần đây khám phá nhiều khía cạnh khác nhau như tạo hội thoại [8], đa phương thức [45], chuỗi
suy nghĩ [44], huấn luyện phân tán [11], và học liên bang [51]. Hơn nữa, hiệu quả của Prompt và
Hướng dẫn đã được chứng minh trong các thiết lập ít tài nguyên [16, 30], và các biến thể khác
nhau của prompting, bao gồm Scratchpad [26], Majority Voting [41], Reframing [24], Least-to-Most
Prompting [54], và Question Decomposition [13, 29], đã chứng minh hiệu quả trên nhiều tác vụ
khác nhau. Các kỹ thuật dựa trên hướng dẫn cũng đã cho thấy hiệu quả trong các ứng dụng khác
nhau, như NER [40], tổng hợp chương trình [14], chuyển đổi phong cách [32], trả lời câu hỏi bảng
[21], trích xuất quan hệ [4], và ứng dụng y sinh [28]. Tuy nhiên, phần lớn các nghiên cứu hiện có
chủ yếu tập trung vào các tình huống suy luận zero/few-shot [10, 6].

Các thí nghiệm của chúng tôi nhằm mô phỏng một thiết lập thực tế nơi suy luận few shot không
phải lúc nào cũng cần thiết và có một số mẫu có sẵn để huấn luyện. Mặc dù đã có một số nghiên
cứu về hiệu quả mẫu trong các lĩnh vực khác như học tăng cường [48, 7, 52, 49, 47, 15], theo hiểu
biết của chúng tôi, không có nghiên cứu nào khác đã khám phá hiệu quả mẫu của các mô hình
được điều chỉnh hướng dẫn theo cách tổng quát. Chúng tôi cũng cung cấp phân tích chi tiết và
hiểu biết riêng biệt cho từng tác vụ đối với các phương pháp điều chỉnh hướng dẫn khác nhau.

3 Điều chỉnh Hướng dẫn
Đối với mỗi tác vụ t cho trước, chúng ta giả định rằng có các cặp thể hiện đầu vào và đầu ra (Xt; Yt).
Mỗi mẫu của tác vụ được mô tả bằng hướng dẫn inst của nó.

Học Tác vụ Đơn (STL) Các mô hình có giám sát truyền thống học một hàm ánh xạ (fM) giữa đầu
vào (x) và đầu ra (y) bằng cách sử dụng một tập huấn luyện các cặp đầu vào/đầu ra, (x; y)2(Xt
train; Yt
train), cho một tác vụ t cho trước. Mô hình sau đó được đánh giá trên tập kiểm tra cho cùng
tác vụ đó, (Xt
test; Xt
test). Trong thiết lập STL, t mô hình được huấn luyện cho t tác vụ theo cách
riêng biệt.

Học Đa tác vụ (MTL) Trong thiết lập này, dữ liệu huấn luyện cho tất cả các tác vụ được kết hợp
lại với nhau. Mục tiêu của các mô hình MTL là học một hàm ánh xạ (fM) giữa đầu vào (x) và đầu
ra (y), sao cho fM(x) = y, trong đó (x; y)2(Xt
train; Yt
train) cho tất cả t tác vụ theo cách kết hợp.
Mô hình này sau đó được đánh giá trên các thể hiện riêng biệt cho từng tác vụ (x; y)2(Xt
test; Yt
test).
Trái ngược với các mô hình tác vụ đơn, một mô hình đơn được sử dụng để giải quyết nhiều tác vụ
khác nhau trong thiết lập này, cho phép tổng quát hóa.

Mô hình hóa Điều chỉnh Hướng dẫn Trong thiết lập này, hàm ánh xạ nhận một hướng dẫn instt
cùng với mẫu đầu vào để đưa ra đầu ra là y; fM(inst; x) = y. Điều chỉnh hướng dẫn có thể được
thực hiện trong cả thiết lập Tác vụ đơn và Đa tác vụ. Định nghĩa: Thuật ngữ "Định nghĩa" liên
quan đến giải thích chi tiết của tác vụ hiện tại cùng với các hướng dẫn cụ thể được cung cấp, cho
phép mô hình hoàn thành thành công tác vụ đã cho. Ví dụ: "Ví dụ" đề cập đến các cặp đầu vào/
đầu ra liên quan đến một thể hiện cụ thể của tác vụ. Phù hợp với phương pháp được giới thiệu
trong SuperNI, chúng tôi kết hợp hai ví dụ trong các prompt hướng dẫn.

3.1 Phân tích Được đề xuất
Chúng tôi giới thiệu hai tập dữ liệu cho tác vụ của chúng tôi: xprefinetune và xtrain. Các tập dữ liệu
này được sử dụng làm tập dữ liệu tiền tinh chỉnh và huấn luyện downstream tương ứng. Bằng
cách tiền tinh chỉnh một LLM fM sử dụng hướng dẫn (inst; x prefinetune), chúng ta có mô hình
được điều chỉnh hướng dẫn finst. finst bây giờ được điều chỉnh hướng dẫn trên dữ liệu huấn luyện
downstream finst(inst; x train) = y. Đối với mỗi thí nghiệm, số lượng mẫu huấn luyện downstream
khác nhau được sử dụng. Các prompt hướng dẫn thay đổi theo từng tác vụ downstream.

Hình 2: Công thức của phân tích được đề xuất. Trong thiết lập Học tác vụ đơn (STL), finst được
điều chỉnh hướng dẫn riêng biệt trên từng tập dữ liệu downstream. Trong thiết lập Học đa tác vụ
(MTL), tất cả các tác vụ huấn luyện downstream được kết hợp lại với nhau, và finst được điều
chỉnh hướng dẫn trên tất cả chúng. Trong cả hai thiết lập, số lượng mẫu đầu vào từ dữ liệu huấn
luyện downstream được thay đổi.

tác vụ. Đối với thiết lập STL, finst được điều chỉnh hướng dẫn riêng biệt trên tất cả các tác vụ của
dữ liệu huấn luyện downstream; t mô hình được tinh chỉnh cho t tác vụ. Mỗi thí nghiệm sẽ bao
gồm t mô hình được điều chỉnh hướng dẫn với số lượng mẫu huấn luyện khác nhau (Cột 1 của
Hình 2). Đối với thiết lập MTL, một tập dữ liệu được chuẩn bị bằng cách kết hợp tất cả các tác vụ
của tập dữ liệu huấn luyện downstream lại với nhau. finst được điều chỉnh hướng dẫn trên tập dữ
liệu kết hợp. Tương tự như thiết lập cuối cùng, thí nghiệm sẽ có số lượng mẫu huấn luyện khác
nhau để làm nổi bật hiệu quả mẫu.

3.2 Đường cơ sở
Để thể hiện phân tích chi tiết về mô hình hóa được điều chỉnh hướng dẫn, tiền tinh chỉnh và tổng
quát hóa chéo tác vụ, chúng tôi phát triển các đường cơ sở khác nhau trên cả hai thiết lập.

3.2.1 Đường cơ sở STL
Chúng tôi phát triển ba đường cơ sở để so sánh mô hình được đề xuất một cách toàn diện. Đối với
đường cơ sở đầu tiên, chúng tôi tiền tinh chỉnh mô hình fM sử dụng xprefinetune mà không có
hướng dẫn để có fM1. fM1 bây giờ được tinh chỉnh riêng biệt trên tất cả t tác vụ của xtrain sử
dụng 5,91% dữ liệu huấn luyện downstream để có fSTLbaseline1. Đối với đường cơ sở thứ hai
(fSTLbaseline2), chúng tôi tinh chỉnh riêng biệt mô hình fM trên tất cả t tác vụ với 25,33% tập
huấn luyện downstream (1000 mẫu của mỗi tác vụ) của xtrain. Chúng tôi phát triển đường cơ sở
thứ ba (fSTLbaseline3) bằng cách tinh chỉnh riêng biệt mô hình fM trên t tác vụ của xtrain và
sử dụng 100% tập huấn luyện downstream. Đường cơ sở thứ ba phục vụ như SOTA có giám sát.
Lý do cho tất cả ba đường cơ sở có hai mặt: Thứ nhất, để so sánh các đường cơ sở với mô hình
được đề xuất với ít mẫu huấn luyện hơn. Thứ hai, đường cơ sở cũng được sử dụng để so sánh mô
hình được điều chỉnh hướng dẫn được huấn luyện với cùng số lượng mẫu để quan sát cải thiện
hiệu suất tương đối. Cả hai lợi thế đều có thể được giải thích thông qua ví dụ sau: fSTLbaseline1
có thể làm nổi bật hiệu ứng của tiền tinh chỉnh, chứng minh hiệu quả mẫu và có thể được so sánh
để cải thiện hiệu suất khi cùng số lượng mẫu được sử dụng.

3.2.2 Đường cơ sở MTL
Chúng tôi phát triển hai đường cơ sở để so sánh với mô hình được đề xuất MTL được điều chỉnh
hướng dẫn. Đối với đường cơ sở đầu tiên, chúng tôi tinh chỉnh fM trên 25,33% tập huấn luyện
downstream xtrain trong thiết lập MTL để có fMTLbaseline1. Để phát triển đường cơ sở thứ hai,
chúng tôi tiền tinh chỉnh mô hình fM sử dụng xprefinetune mà không có hướng dẫn để có fM1.
Mô hình fM1 được tinh chỉnh trong thiết lập MTL trên tất cả t tác vụ sử dụng 5,91% dữ liệu huấn
luyện downstream (200 mẫu của mỗi tác vụ) của xtrain. Lý do để tạo hai đường cơ sở MTL tương
tự như các đường cơ sở STL; để thể hiện hiệu quả mẫu, làm nổi bật

cải thiện hiệu suất khi sử dụng cùng số lượng mẫu, và thể hiện hiệu ứng của hướng dẫn trong
tiền tinh chỉnh.

4 Thí nghiệm
4.1 Tập dữ liệu
Thống kê
# Tổng số Tác vụ 119
# Tổng số thể hiện trong tập huấn luyện 374745
# Tổng số thể hiện trong Tập Kiểm tra 11810
# Tổng số thể hiện trong tập tiền tinh chỉnh 75700
Độ dài trung bình dữ liệu Huấn luyện với hướng dẫn 364,97
Độ dài trung bình dữ liệu Huấn luyện không có hướng dẫn 89,03
Bảng 2: Thống kê tập dữ liệu SuperNI. Tập Huấn luyện
và Kiểm tra đề cập đến dữ liệu Downstream.

Chúng tôi sử dụng các tác vụ đã thấy của SuperNI làm tập tiền
tinh chỉnh bao gồm 757 tác vụ với 100 mẫu cho mỗi tác vụ.
Chúng tôi sử dụng tập tác vụ chưa thấy của SuperNI làm dữ
liệu huấn luyện downstream, bao gồm 119 tác vụ có thể được
phân loại thành 11 danh mục. Thống kê của chúng được trình
bày trong Bảng 2. Tập dữ liệu được phân loại thành 11 danh
mục tác vụ NLP như được thể hiện trong Bảng 3. Để rõ ràng,
chúng tôi đã nhóm bảy danh mục với ít tác vụ hơn vào danh
mục khác. Bảng 1 đưa ra thống kê chi tiết trên mỗi danh mục.
Vì không phải tất cả các tác vụ đều có chính xác cùng số lượng
mẫu, chúng tôi chọn số lượng tối đa có sẵn nếu số lượng mẫu
dưới ngưỡng. Ví dụ, tác vụ 1388 (Bảng 6 của Phụ lục §B)
có tổng cộng 191 mẫu. Chúng tôi sử dụng 191 mẫu khi tinh
chỉnh với 200, 1000, và toàn bộ tập dữ liệu.

Danh mục Số lượng Danh mục
Suy luận Văn bản 24
Tạo Tiêu đề 18
Giải quyết Đồng tham chiếu 14
Phân loại Khả năng Trả lời 13
Viết lại Câu hỏi 11
Dữ liệu thành Văn bản 9
Tương tự Từ 8
Phân loại Nguyên nhân Hiệu ứng 7
Nhận dạng Hành vi Hội thoại 7
Gắn thẻ Từ khóa 5
Trích xuất Chồng lấp 2
Sửa lỗi Ngữ pháp 1
Tổng cộng 119
Bảng 3: Thống kê mẫu huấn luyện

Chúng tôi tham khảo độc giả đến Phụ lục §B, tổng số
mẫu huấn luyện trong mỗi tác vụ (Bảng 6), và số lượng
mẫu được sử dụng để phát triển đường cơ sở và phân
tích được đề xuất (Bảng 7).

Chia tách Tập dữ liệu: Một trăm mẫu mỗi tác vụ đã
được sử dụng để kiểm tra, và mười mẫu đã được sử dụng
để xác thực. Các mẫu này không phải là một phần của
tập huấn luyện, và chúng tôi đã đảm bảo rằng không có
rò rỉ dữ liệu. Mỗi tác vụ chứa cùng số lượng mẫu cho
kiểm tra (100 mẫu) và tập xác thực (10 mẫu).

4.2 Đường cơ sở
Chúng tôi sử dụng xprefinetune và xtrain trong cả thiết
lập STL và MTL. Để tạo đường cơ sở, cả hai tập dữ liệu
đều không được trang bị hướng dẫn; chúng chỉ chứa
đầu vào và đầu ra cho tinh chỉnh thông thường.

Answerability
ClassificationCoreference
ResolutionData
to TextQuestion
RewritingTextual
EntailmentTitle
GenerationOther
CategoriesGrand Total
and percentage
# of tasks 13 14 9 11 24 18 30 119 (100%)
100 1300 1370 826 949 2376 1784 2994 11.5K (3.09%)
200 2600 2441 1626 1849 4457 3484 5708 22.1K (5.91%)
1000 11529 8831 8026 9049 19019 16514 21988 94.9K (25.33%)
All 43871 35560 36815 41391 78802 78357 59949 374.7K (100%)
Bảng 1: Thống kê phân loại dữ liệu huấn luyện downstream được sử dụng. Chúng tôi lưu ý rằng
vì tất cả các tác vụ có số lượng mẫu không bằng nhau, tổng số mẫu trong mỗi danh mục sẽ khác
với # Tác vụ*Số lượng mẫu. Các hàng 100, 200, 1000, và tất cả mẫu đại diện cho tổng số lượng
mẫu được chọn trong mỗi thí nghiệm.

4.2.1 Đường cơ sở Học Tác vụ Đơn
STL Đường cơ sở 1: Một mô hình T5-3B trải qua tiền tinh chỉnh sử dụng xprefinetune, bao gồm
757 tác vụ từ SuperNI. Sau đó, mô hình trải qua tinh chỉnh thêm trên 200 mẫu mỗi tác vụ từ dữ
liệu huấn luyện downstream của SuperNI (xtrain), dẫn đến 119 mô hình.

STL Đường cơ sở 2: Mỗi tác vụ từ dữ liệu huấn luyện downstream (xtrain) được sử dụng để tinh
chỉnh mô hình T5-3B (fM) với 1000 mẫu mỗi tác vụ, dẫn đến 119 mô hình riêng biệt.

STL Đường cơ sở 3 (SOTA Có giám sát): Một mô hình T5-3B (fM) được tinh chỉnh cho mỗi tác
vụ sử dụng tất cả mẫu có sẵn từ dữ liệu huấn luyện downstream.

4.2.2 Đường cơ sở Học Đa tác vụ
MTL Đường cơ sở 1: Tương tự như STL Đường cơ sở 1, chúng tôi tiền tinh chỉnh T5-3B xprefinetune
cho mỗi tác vụ để có mô hình fM1. fM1 bây giờ được tinh chỉnh trên 200 mẫu mỗi tác vụ từ dữ
liệu huấn luyện downstream (xtrain) của SuperNI (theo kiểu MTL).

MTL Đường cơ sở 2: Mô hình T5-3B bây giờ được tinh chỉnh trên 1000 mẫu mỗi tác vụ từ dữ
liệu huấn luyện downstream (xtrain) của SuperNI (theo kiểu MTL).

4.3 Mô hình và Thước đo Đánh giá
Mô hình: Chúng tôi sử dụng Tk-Instruct 3B làm mô hình được điều chỉnh hướng dẫn. Cho thiết
lập STL, 952 mô hình (119x8) đã được huấn luyện và 9 mô hình đã được huấn luyện cho MTL
dẫn đến tổng cộng 961 mô hình cho phân tích của chúng tôi. Tất cả các mô hình đã được huấn
luyện trên 6x Nvidia A100 40GB GPU.

Thước đo đánh giá: Chúng tôi coi tất cả các tác vụ trong tập dữ liệu là các vấn đề tạo văn bản
và sử dụng điểm ROUGE-L [19] để đánh giá các đầu ra được tạo.

5 Kết quả

Hình 3: Kết quả trong thiết lập STL. và 3, tương ứng. Đường
ngang đứt nét được đánh dấu trên đồ thị để làm nổi bật sự
khác biệt trong dữ liệu huấn luyện cần thiết giữa phương
pháp được đề xuất và đường cơ sở. Trục x ở thang logarit.

Kết quả được trình bày trong hai phần, kết quả STL và
MTL. Mỗi phần chứa kết quả tổng thể, Kết quả theo Danh
mục, và so sánh với các đường cơ sở đã được định nghĩa
trước đó.

5.1 Thiết lập Học Tác vụ Đơn (STL)
Hình 3 cho thấy điểm rouge của các mô hình được điều
chỉnh hướng dẫn khi huấn luyện với số lượng mẫu khác
nhau. Chúng ta thấy rằng có xu hướng tăng tổng thể khi
số lượng mẫu tăng. Từ hình vẽ chúng ta quan sát thấy
rằng điểm ROUGE-L tối đa là 72,04 được đạt được khi
tất cả mẫu được sử dụng. Điểm số theo danh mục của
các đường cơ sở có mặt trong Bảng 4. Hình 4 cho thấy
kết quả theo danh mục của các mô hình được điều chỉnh
hướng dẫn. Từ hình vẽ, chúng ta thấy rằng ngoại trừ
danh mục Phân loại Khả năng Trả lời, tất cả các danh
mục đều có xu hướng tăng. Kết quả chi tiết về từng danh
mục, đường cơ sở, và điều chỉnh hướng dẫn của thiết lập
MTL có thể tìm thấy trong Bảng 12, Bảng 8, và Bảng 9
trong Phụ lục §B.

Hiệu quả 50% đối với STL đường cơ sở 1: STL Đường
cơ sở 1 được biểu thị bằng điểm màu đỏ trong Hình 3.
Điểm số với 100 mẫu là 65,93, và điểm số với đường cơ
sở 1 là 64,29.

Hiệu suất cạnh tranh sử dụng 6% dữ liệu: STL Đường cơ sở 2 được biểu thị bằng điểm màu
vàng trong Hình 3). Mô hình được điều chỉnh hướng dẫn sử dụng khoảng 23,33% mẫu huấn
luyện khi được huấn luyện trên 6% dữ liệu so với STL đường cơ sở 2 sử dụng 25,33% dữ liệu.
Điểm số với 200 mẫu/tác vụ là 68,34,

trong khi điểm số với đường cơ sở 2 là 68,91. So sánh với STL đường cơ sở 1 với mô hình được
điều chỉnh hướng dẫn (cả hai được huấn luyện sử dụng 6% dữ liệu), quan sát thấy sự tăng 3%.

Vượt qua SOTA với 25% dữ liệu: Mô hình được điều chỉnh hướng dẫn sử dụng 25,33% dữ liệu
so với STL đường cơ sở 3 và đạt điểm số 71,71, so với điểm số 70,99 của đường cơ sở. So sánh
với đường cơ sở 2 (cả hai được huấn luyện sử dụng 25,33% dữ liệu) cho thấy sự tăng 3%. Khi
tất cả mẫu được sử dụng, có sự tăng thêm 1,04% (72,04 so với 70,99).

Hiệu ứng theo danh mục của điều chỉnh hướng dẫn: Chúng tôi quan sát thấy rằng điểm số các
danh mục Phân loại Khả năng Trả lời và Tạo Tiêu đề giảm từ điều chỉnh hướng dẫn so với đường
cơ sở. Điểm số tốt nhất từ điều chỉnh hướng dẫn lần lượt là 75,15 và 44,55 thấp hơn đáng kể so
với điểm số đường cơ sở tốt nhất là 80,36 và 48,35. Các danh mục hưởng lợi từ điều chỉnh hướng
dẫn so với đường cơ sở là Giải quyết Đồng tham chiếu (82,82 so với 74,40) và Dữ liệu thành Văn
bản (59,06 so với 52,84).

Answerability
ClassificationCoreference
ResolutionData to TextQuestion
RewritingTextual
EntailmentTitle
GenerationOther
Categories
STL Baseline 1 70.38 70.01 49.75 68.12 71.00 44.71 72.31
STL Baseline 2 78.77 68.61 50.89 71.00 77.78 46.78 75.62
STL Baseline 3 80.36 74.40 52.84 71.11 80.42 48.35 76.82
Bảng 4: Điểm số theo danh mục STL cho tất cả ba đường cơ sở. Chúng ta thấy rằng tất cả các
đường cơ sở tuân theo xu hướng tăng tuyến tính khi số lượng mẫu tăng với đường cơ sở 1, 2, và
3 (200, 1000, và tất cả mẫu được sử dụng, tương ứng). Tuy nhiên, ít cải thiện được quan sát trong
danh mục Viết lại câu hỏi đối với điểm ROUGE-L của đường cơ sở 2 và 3 (71,00 và 71,11, tương
ứng). Một sự sai lệch khác từ xu hướng tiêu chuẩn được quan sát trong danh mục Giải quyết Đồng
tham chiếu nơi STL đường cơ sở 1 có điểm số cao hơn so với STL đường cơ sở 2 (70,01 và 68,61
tương ứng).

Hình 4: Biểu đồ cột thể hiện kết quả theo danh mục của phương pháp được đề xuất trong thiết
lập STL. Trục x thể hiện số lượng mẫu huấn luyện trung bình. trục y thể hiện điểm rouge-L. Hầu
hết các danh mục tuân theo xu hướng thông thường về tăng hiệu suất khi số lượng mẫu huấn
luyện tăng. Xu hướng này có ngoại lệ ở hai nơi. Thứ nhất: Điểm số Phân loại khả năng trả lời
giảm khi tất cả mẫu được sử dụng sau 1000 (75,15 xuống 74,38). Thứ hai: Điểm số Giải quyết
Đồng tham chiếu giảm khi 200 mẫu được sử dụng sau 100 (75,74 xuống 74,25).

5.2 Thiết lập đa tác vụ
Hình 5 thể hiện điểm ROUGE-L tổng thể của các mô hình được điều chỉnh hướng dẫn trong thiết
lập MTL. Điểm ROUGE-L tối đa là 74,68 được đạt được khi tất cả mẫu được sử dụng, vượt qua
SOTA là 70,99. Hình 6 thể hiện kết quả theo danh mục của các mô hình được điều chỉnh hướng
dẫn và Bảng 5 giới thiệu kết quả đường cơ sở. Kết quả chi tiết về từng danh mục, đường cơ sở,
và điều chỉnh hướng dẫn của thiết lập MTL có thể tìm thấy trong Bảng 13, Bảng 10, và Bảng 11
tương ứng trong Phụ lục §B.

Answerability
ClassificationCoreference
ResolutionData to TextQuestion
RewritingTextual
EntailmentTitle
GenerationOther
Categories
MTL Baseline 1 75.35 75.87 55.85 75.64 65.95 62.19 45.20
MTL Baseline 2 76.89 81.93 54.73 79.80 67.89 67.18 42.19
Bảng 5: Điểm số theo danh mục đường cơ sở MTL. Tất cả các danh mục tuân theo xu hướng tăng
như suy nghĩ thông thường sẽ gợi ý. Tuy nhiên, xu hướng bị phá vỡ trong danh mục Dữ liệu thành
Văn bản và các danh mục khác.

Hình 6: Biểu đồ cột thể hiện kết quả theo danh mục của phương pháp được đề xuất trong thiết
lập MTL. Tương tự như điểm số theo danh mục STL, xu hướng tuyến tính được tuân theo nhưng
có hai ngoại lệ. Thứ nhất: Điểm số Dữ liệu thành văn bản giảm khi tất cả mẫu được sử dụng sau
1000 (58,83 xuống 53,28). Thứ hai: Điểm số Tạo Tiêu đề giảm khi tất cả mẫu được sử dụng sau
1000 (43,94 xuống 38,16)

Hiệu quả 50% so với MTL Đường cơ sở 1: MTL đường cơ sở 1 được huấn luyện trên khoảng
6% mẫu huấn luyện downstream. Điểm số với 3% mẫu dữ liệu downstream là 66,78, trong khi
điểm số với đường cơ sở 1 là 65,63. Nếu chúng ta so sánh mô hình được điều chỉnh hướng dẫn
được huấn luyện 6% mẫu huấn luyện downstream, có sự tăng khoảng 5 điểm phần trăm khi đạt
điểm rouge 70,40.

Hình 5: Kết quả tổng thể mô hình được đề xuất trong thiết
lập MTL. Các điểm Đỏ và Vàng đại diện cho MTL Đường
cơ sở 1 và 2, tương ứng. Khoảng cách điểm số giữa phương
pháp được đề xuất và đường cơ sở mở rộng so với thiết lập
STL.

Vượt qua SOTA với 6% dữ liệu huấn luyện:
MTL đường cơ sở hai được biểu thị bằng điểm màu
vàng trong Hình 3, và được huấn luyện sử dụng 25%
mẫu huấn luyện downstream. Mô hình được điều chỉnh
hướng dẫn sử dụng ít hơn 76% mẫu khi được huấn
luyện trên 6% mẫu huấn luyện downstream và đạt
điểm số 70,40, trong khi điểm số với đường cơ sở 2
là 68,10. Phương pháp được điều chỉnh hướng dẫn,
được huấn luyện trên cùng mẫu như đường cơ sở 2,
cải thiện khoảng 5% (73,14 so với 68,10). Khi tất cả
mẫu được sử dụng, điểm số 74,68 được đạt được,
vượt qua SOTA 3%.

Hiệu ứng theo danh mục của điều chỉnh hướng dẫn:
Kết quả tương phản với thiết lập STL được quan sát
khi các danh mục Viết lại Câu hỏi và Tạo Tiêu đề trải
qua sự giảm đáng kể (12% và 23% điểm tương ứng)
so với điểm số đường cơ sở tốt nhất. Có sự cải thiện
đáng kể được quan sát trong danh mục Suy luận Văn
bản khi điểm số tốt nhất cải thiện lên 84,16 từ 67,89
so với điểm số đường cơ sở.

5.3 Phân tích
5.3.1 Phân tích Theo Danh mục
Chúng tôi phân tích hiệu suất trên từng danh mục trong cả hai thiết lập. Trong thiết lập STL, chúng
tôi phát hiện rằng các tác vụ thuộc danh mục giải quyết đồng tham chiếu và dữ liệu thành văn bản
có sự tăng cao trong điểm ROUGE-L với điều chỉnh hướng dẫn so với các phương pháp đường
cơ sở (78,23 so với 71,00 ROUGE-L trong giải quyết đồng tham chiếu và 57,88 so với 51,16 trong
Dữ liệu thành Văn bản). Viết lại câu hỏi thực hiện gần như tương tự (70,51 so với 70,07 ROUGE-L)
trong khi điểm số phân loại khả năng trả lời và tạo tiêu đề giảm đối với đường cơ sở (74,10 so với
76,50 ROUGE-L trong phân loại khả năng trả lời và 43,36 so với 46,61 trong tạo tiêu đề). Trong
thiết lập MTL, các phát hiện tương tự được quan sát nhưng trên các danh mục khác nhau. Chúng
tôi phát hiện rằng các tác vụ thuộc danh mục suy luận văn bản có sự tăng cao nhất với điều chỉnh
hướng dẫn so với đường cơ sở (81,93 so với 66,91 ROUGE-L). Phân loại khả năng trả lời thực
hiện gần như tương tự (75,97 so với 76,11 ROUGE-L) trong khi điểm số viết lại câu hỏi và tạo
tiêu đề giảm đối với đường cơ sở (67,38 so với 77,71 ROUGE-L trong viết lại câu hỏi và 43,99
so với 64,68 trong tạo tiêu đề).

MTL liên tục vượt trội hơn STL: Chúng tôi đã thực hiện nhiều thí nghiệm trên các thiết lập mô
hình hóa được điều chỉnh hướng dẫn trong khi giữ nguyên số lượng mẫu huấn luyện giống nhau
trên các thiết lập khác nhau. Trên mỗi thiết lập huấn luyện, có sự tăng 1-2% ROUGE-L trong
thiết lập MTL so với STL. Thông qua cả hai thiết lập và tất cả các thí nghiệm, rõ ràng rằng các
mô hình được điều chỉnh hướng dẫn hoạt động tốt hơn trong thiết lập đa tác vụ so với thiết lập
tác vụ đơn.

Hiệu quả Mẫu: Các mô hình được điều chỉnh hướng dẫn thể hiện hiệu quả mẫu trên cả thiết lập
MTL và STL. Sử dụng nhiều đường cơ sở, hiệu quả mẫu khoảng 50, 75, và 80% được đạt được
trên các không gian khác nhau trong cả thiết lập STL và MTL. Chúng tôi cũng thấy rằng khi tất
cả mẫu được sử dụng trong thiết lập được điều chỉnh hướng dẫn, hiệu suất tổng thể vượt qua
SOTA.

Hiệu ứng của Hướng dẫn trong tiền tinh chỉnh: STL Đường cơ sở 1 và MTL Đường cơ sở 1 được
tiền tinh chỉnh với 757 tác vụ của tập dữ liệu SupperNI nhưng không có hướng dẫn. Chúng sau
đó được tinh chỉnh trên 119 tác vụ dữ liệu huấn luyện downstream sử dụng 6% theo kiểu STL
và MTL. Hướng dẫn có hiệu ứng đáng kể trong tiền huấn luyện khi mô hình được điều chỉnh hướng
dẫn vượt trội hơn các đường cơ sở này 4 và 5%, tương ứng, khi được huấn luyện với cùng số
lượng mẫu.

6 Kết luận, Hạn chế và Nghiên cứu tương lai
Trong nghiên cứu này, chúng tôi đã thực hiện một bước tiến đáng kể trong việc thúc đẩy mô hình
hướng dẫn bằng cách kết hợp một phần nhỏ dữ liệu huấn luyện thường có sẵn cho các tác vụ
downstream. Bằng cách điều chỉnh hướng dẫn các mô hình trên các tập huấn luyện quy mô nhỏ
của các tác vụ downstream, chúng tôi đã quan sát thấy lợi ích hiệu suất đáng chú ý cho mô hình
Tk-instruct trên SuperNI. Các phát hiện này gợi ý rằng điều chỉnh hướng dẫn có thể hỗ trợ hiệu
quả một mô hình trong việc học nhanh một tác vụ ngay cả với dữ liệu hạn chế. Mặc dù nghiên
cứu của chúng tôi trình bày kết quả đầy hứa hẹn, có một số hạn chế cần được thừa nhận.

Thứ nhất, các thí nghiệm của chúng tôi bị hạn chế trong việc sử dụng T5-3B và biến thể được
điều chỉnh hướng dẫn của nó. Chúng tôi không thể tiến hành thí nghiệm sử dụng T5-11B và biến
thể được điều chỉnh hướng dẫn của nó do hạn chế tài nguyên. Do đó, điều tra thêm sử dụng các
mô hình và tập dữ liệu lớn hơn sẽ cung cấp hiểu biết toàn diện hơn về phương pháp điều chỉnh
hướng dẫn. Ngoài ra, các thí nghiệm của chúng tôi chỉ tập trung vào SuperNI, và chúng tôi đã
huấn luyện tổng cộng 950+ mô hình trong quá trình. Tuy nhiên, để đạt được đánh giá mạnh mẽ
hơn, cần thiết phải mở rộng điều tra của chúng tôi sang các benchmark lớn hơn như BigBench [36].
Tuy nhiên, đánh giá các benchmark lớn hơn này sẽ đòi hỏi thời gian và tài nguyên đáng kể.

Tóm lại, nghiên cứu của chúng tôi đại diện cho bước đầu tiên hướng tới việc làm cho mô hình
hướng dẫn thực tế hơn. Nghiên cứu tương lai có thể xây dựng dựa trên các phát hiện của chúng
tôi để tăng cường hiệu suất trên nhiều benchmark khác nhau. Giải quyết các hạn chế được đề cập
ở trên và tiến hành thí nghiệm với tài nguyên rộng lớn hơn sẽ góp phần vào hiểu biết sâu sắc hơn
về điều chỉnh hướng dẫn và tiềm năng của nó để cải thiện hiệu suất mô hình trong các tác vụ
downstream.

Tài liệu tham khảo
[1] Ryan R. Anderson. "Vision Encoders in Visual Question Answering". In: 2022.

[2] Jifan Chen, Yuhao Zhang, Lan Liu, Rui Dong, Xinchi Chen, Patrick Ng, William Yang Wang,
and Zhiheng Huang. "Improving Cross-task Generalization of Unified Table-to-text Models
with Compositional Task Configurations". In: arXiv preprint arXiv:2212.08780 (2022).

[3] Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, and Weijie J Su. "Weighted Training
for Cross-Task Learning". In: International Conference on Learning Representations. 2021.

[4] Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo
Si, and Huajun Chen. "Adaprompt: Adaptive prompt-based finetuning for relation extraction".
In: arXiv e-prints (2021), arXiv–2104.

[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. "Scaling instruction-finetuned
language models". In: arXiv preprint arXiv:2210.11416 (2022).

[6] Yuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang. "Learning Instructions with Unlabeled
Data for Zero-Shot Cross-Task Generalization". In: arXiv preprint arXiv:2210.09175 (2022).

[7] Xu Guo, Boyang Albert Li, and Han Yu. "Improving the Sample Efficiency of Prompt Tuning
with Domain Adaptation". In: ArXiv abs/2210.02952 (2022).

[8] Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskénazi, and Jeffrey P.
Bigham. "InstructDial: Improving Zero and Few-shot Generalization in Dialogue through
Instruction Tuning". In: Conference on Empirical Methods in Natural Language Processing.
2022.

[9] Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. "Instruction Induction: From
Few Examples to Natural Language Task Descriptions". In: arXiv preprint arXiv:2205.10782
(2022).

[10] Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and Matthew E. Peters.
"HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation". In: ArXiv
abs/2212.10315 (2022).

[11] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae
Lee, Kyungjae Lee, and Minjoon Seo. "Exploring the Benefits of Training Expert Language
Models over Instruction Tuning". In: ArXiv abs/2302.03202 (2023).

[12] Weixi Kang, Sònia Pineda Hernández, Junxin Wang, and Antonio Malvaso. "Instruction-based
learning: A review". In: Neuropsychologia (2022), p. 108142.

[13] Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. "Text
modular networks: Learning to decompose tasks in the language of existing models". In: arXiv
preprint arXiv:2009.00751 (2020).

[14] Kirby Kuznia, Swaroop Mishra, Mihir Parmar, and Chitta Baral. "Less is more: Summary of
long instructions is better for program synthesis". In: arXiv preprint arXiv:2203.08597 (2022).

[15] Gabriele Lagani, F. Falchi, Claudio Gennaro, and Giuseppe Amato. "Hebbian Semi-Supervised
Learning in a Sample Efficiency Setting". In: Neural networks : the official journal of the
International Neural Network Society 143 (2021), pp. 719–731.

[16] Teven Le Scao and Alexander M Rush. "How many data points is a prompt worth?" In:
Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. 2021, pp. 2627–2636.

[17] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. "Holistic evaluation of
language models". In: arXiv preprint arXiv:2211.09110 (2022).

[18] Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren. "Unsupervised
cross-task generalization via retrieval augmentation". In: arXiv preprint arXiv:2204.07937
(2022).

[19] Chin-Yew Lin. "ROUGE: A Package for Automatic Evaluation of Summaries". In: Text
Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics,
July 2004, pp. 74–81. URL: https://aclanthology.org/W04-1013.

[20] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
Colin A Raffel. "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
learning". In: Advances in Neural Information Processing Systems 35 (2022), pp. 1950–1965.

[21] Man Luo, Sharad Saxena, Swaroop Mishra, Mihir Parmar, and Chitta Baral. "BioTABQA:
Instruction Learning for Biomedical Table Question Answering". In: arXiv preprint
arXiv:2207.02419 (2022).

[22] Itzik Malkiel and Lior Wolf. "Maximal Multiverse Learning for Promoting Cross-Task General-
ization of Fine-Tuned Language Models". In: Proceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computational Linguistics: Main Volume. Online: Associa-
tion for Computational Linguistics, Apr. 2021, pp. 187–199. DOI: 10.18653/v1/2021.eacl-
main.14. URL: https://aclanthology.org/2021.eacl-main.14.

[23] Rakesh Menon, Sayan Ghosh, and Shashank Srivastava. "CLUES: A Benchmark for Learning
Classifiers using Natural Language Explanations". In: Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers). 2022, pp. 6523–
6546.

[24] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. "Re-
framing Instructional Prompts to GPTk's Language". In: Findings of the Association for
Computational Linguistics: ACL 2022. Dublin, Ireland: Association for Computational Lin-
guistics, May 2022, pp. 589–612. DOI: 10.18653/v1/2022.findings-acl.50. URL:
https://aclanthology.org/2022.findings-acl.50.

[25] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. "Cross-Task Gen-
eralization via Natural Language Crowdsourcing Instructions". In: Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
2022, pp. 3470–3487.

[26] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. "Show your
work: Scratchpads for intermediate computation with language models". In: arXiv preprint
arXiv:2112.00114 (2021).

[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. "Training language models
to follow instructions with human feedback". In: Advances in Neural Information Processing
Systems 35 (2022), pp. 27730–27744.

[28] Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral.
"In-BoXBART: Get Instructions into Biomedical Multi-Task Learning". In: Findings of the
Association for Computational Linguistics: NAACL 2022. Seattle, United States: Association
for Computational Linguistics, July 2022, pp. 112–128. DOI: 10.18653/v1/2022.findings-
naacl.10. URL: https://aclanthology.org/2022.findings-naacl.10.

[29] Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. "Is a Question Decomposition
Unit All We Need?" In: arXiv preprint arXiv:2205.12538 (2022).

[30] Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, and Chitta Baral. "How Many Data
Samples is an Additional Instruction Worth?" In: arXiv preprint arXiv:2203.09161 (2022).

[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, Peter J Liu, et al. "Exploring the limits of transfer learning with a unified
text-to-text transformer." In: J. Mach. Learn. Res. 21.140 (2020), pp. 1–67.

[32] Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei.
"A recipe for arbitrary text style transfer with large language models". In: arXiv preprint
arXiv:2109.03910 (2021).

[33] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. "Multitask Prompted Training
Enables Zero-Shot Task Generalization". In: International Conference on Learning Represen-
tations. 2021.

[34] Timo Schick and Hinrich Schütze. "True Few-Shot Learning with Prompts—A Real-World
Perspective". In: Transactions of the Association for Computational Linguistics 10 (2022),
pp. 716–731.

[35] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. "Concept2robot:
Learning manipulation concepts from instructions and human demonstrations". In: The Inter-
national Journal of Robotics Research 40.12-14 (2021), pp. 1419–1434.

[36] Aarohi Srivastava et al. "Beyond the Imitation Game: Quantifying and extrapolating the
capabilities of language models". In: ArXiv abs/2206.04615 (2022).

[37] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang,
Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. "Selective annotation makes language
models better few-shot learners". In: arXiv preprint arXiv:2209.01975 (2022).

[38] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung
Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. "Chal-
lenging BIG-Bench tasks and whether chain-of-thought can solve them". In: arXiv preprint
arXiv:2210.09261 (2022).

[39] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel Bowman. "Superglue: A stickier benchmark for general-purpose
language understanding systems". In: Advances in neural information processing systems 32
(2019).

[40] Liwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan, Sirui Wang, Wei Wu, and Weiran Xu.
"InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER".
In: arXiv preprint arXiv:2203.03903 (2022).

[41] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. "Self-Instruct: Aligning Language Model with Self Generated
Instructions". In: arXiv preprint arXiv:2212.10560 (2022).

[42] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap,
Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob
Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Morad-
shahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra,
Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. "Super-NaturalInstructions:
Generalization via Declarative Instructions on 1600+ NLP Tasks". In: Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, United
Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 5085–5109. URL:
https://aclanthology.org/2022.emnlp-main.340.

[43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. "Finetuned Language Models are Zero-Shot Learners". In:
International Conference on Learning Representations. 2022. URL: https://openreview.
net/forum?id=gEZrGCozdqR.

[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai-hsin Chi, F. Xia, Quoc
Le, and Denny Zhou. "Chain of Thought Prompting Elicits Reasoning in Large Language
Models". In: ArXiv abs/2201.11903 (2022).

[45] Zhiyang Xu, Ying Shen, and Lifu Huang. "MultiInstruct: Improving Multi-Modal Zero-Shot
Learning via Instruction Tuning". In: ArXiv abs/2212.10773 (2022).

[46] Chenxiao Yang, Junwei Pan, Xiaofeng Gao, Tingyu Jiang, Dapeng Liu, and Guihai Chen.
"Cross-task knowledge distillation in multi-task recommendation". In: Proceedings of the AAAI
Conference on Artificial Intelligence. Vol. 36. 2022, pp. 4318–4326.

[47] Shuo Yang, Yijun Dong, Rachel A. Ward, Inderjit S. Dhillon, Sujay Sanghavi, and Qi Lei. "Sam-
ple Efficiency of Data Augmentation Consistency Regularization". In: ArXiv abs/2202.12230
(2022).

[48] Zhengyu Yang, Kan Ren, Xufang Luo, Minghuan Liu, Weiqing Liu, J. Bian, Weinan Zhang, and
Dongsheng Li. "Towards Applicable Reinforcement Learning: Improving the Generalization
and Sample Efficiency with Policy Ensemble". In: International Joint Conference on Artificial
Intelligence. 2022.

[49] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus.
"Improving Sample Efficiency in Model-Free Reinforcement Learning from Images". In: AAAI
Conference on Artificial Intelligence. 2019.

[50] Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. "CrossFit: A Few-shot Learning Challenge for
Cross-task Generalization in NLP". In: Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. 2021, pp. 7163–7189.

[51] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin Wang, and
Yiran Chen. "Towards Building the Federated GPT: Federated Instruction Tuning". In: 2023.

[52] Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. "On the
Convergence and Sample Efficiency of Variance-Reduced Policy Gradient Method". In: Neural
Information Processing Systems. 2021.

[53] Yichi Zhang and Joyce Chai. "Hierarchical Task Learning from Language Instructions with
Unified Transformers and Self-Monitoring". In: Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021. 2021, pp. 4202–4213.

[54] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. "Least-to-Most Prompting Enables
Complex Reasoning in Large Language Models". In: arXiv preprint arXiv:2205.10625 (2022).

Phụ lục
A Mô tả Tác vụ
Phân loại Khả năng Trả lời: Phân loại khả năng trả lời là một tác vụ xử lý ngôn ngữ tự nhiên (NLP)
bao gồm việc xác định xem một văn bản cho trước có chứa một câu hỏi có thể được trả lời hay không.
Tác vụ này có thể hữu ích trong nhiều ứng dụng khác nhau, như chatbot hoặc hệ thống truy xuất
thông tin, nơi quan trọng là biết liệu đầu vào của người dùng có phải là một câu hỏi có thể được
trả lời bởi hệ thống hay không. Để thực hiện phân loại khả năng trả lời, một mô hình NLP trước
tiên phải được huấn luyện trên một tập dữ liệu văn bản được gắn nhãn là "có thể trả lời" hoặc
"không thể trả lời". Mô hình sau đó có thể được sử dụng để phân loại văn bản mới là có thể trả
lời hoặc không thể trả lời dựa trên sự tương tự của chúng với các văn bản trong tập dữ liệu huấn
luyện. Bảng 15 đưa ra một ví dụ về danh mục này.

Phân loại Nguyên nhân Hiệu ứng: Phân loại nguyên nhân-hiệu ứng là một tác vụ xử lý ngôn ngữ
tự nhiên (NLP) bao gồm việc xác định mối quan hệ nhân quả giữa các sự kiện hoặc hành động
được mô tả trong văn bản. Tác vụ này có thể hữu ích trong nhiều ứng dụng khác nhau, như trích
xuất thông tin và tóm tắt văn bản, nơi quan trọng là hiểu các nguyên nhân và hiệu ứng cơ bản của
các sự kiện được mô tả trong văn bản. Ví dụ: Xem xét hai câu sau: "Xe không khởi động được vì
ắc quy hết điện." "Đứa trẻ khóc vì nó ngã và trầy xước đầu gối." Trong câu đầu tiên, nguyên nhân
là "ắc quy hết điện," và hiệu ứng là "xe không khởi động được." Trong câu thứ hai, nguyên nhân
là "nó ngã và trầy xước đầu gối," và hiệu ứng là "đứa trẻ khóc."

Giải quyết Đồng tham chiếu: Giải quyết đồng tham chiếu là một tác vụ xử lý ngôn ngữ tự nhiên
(NLP) bao gồm việc xác định và liên kết các đề cập đến cùng các thực thể thế giới thực trong văn
bản. Tác vụ này quan trọng để hiểu ý nghĩa và ngữ cảnh của văn bản, vì nó cho phép hệ thống
xác định rằng nhiều đề cập đến một từ hoặc cụm từ trong tài liệu đề cập đến cùng một thực thể.
Ví dụ, xem xét văn bản sau: "John đi đến cửa hàng để mua sữa. Anh ấy cần nó cho ngũ cốc của
mình." Trong văn bản này, các đại từ "anh ấy" và "của mình" đề cập đến cùng một người, "John."
Một hệ thống giải quyết đồng tham chiếu sẽ xác định các đại từ này là đề cập đến cùng một thực
thể và liên kết chúng với danh từ riêng "John." Bảng 16 đưa ra một ví dụ về danh mục này.

Dữ liệu thành văn bản: Tạo dữ liệu thành văn bản là một tác vụ xử lý ngôn ngữ tự nhiên (NLP)
bao gồm việc tự động tạo văn bản có thể đọc được bởi con người từ dữ liệu có cấu trúc. Tác vụ
này có thể hữu ích trong nhiều ứng dụng khác nhau, như tạo báo cáo tự động hoặc tóm tắt dữ
liệu, nơi quan trọng là trình bày dữ liệu theo cách rõ ràng và súc tích. Một ví dụ về tạo dữ liệu
thành văn bản là tạo báo cáo thời tiết từ dữ liệu về nhiệt độ hiện tại, độ ẩm, và dự báo cho một
địa điểm cụ thể. Dữ liệu có thể bao gồm như sau: Nhiệt độ: 75 độ Fahrenheit Độ ẩm: 50% Dự
báo: nắng Một hệ thống tạo dữ liệu thành văn bản có thể sử dụng dữ liệu này để tạo văn bản sau:
"Nhiệt độ hiện tại là 75 độ Fahrenheit và độ ẩm là 50%. Dự báo cho hôm nay là nắng." Bảng 17
đưa ra một ví dụ về danh mục này.

Nhận dạng Hành vi Hội thoại: Nhận dạng hành vi hội thoại là một tác vụ xử lý ngôn ngữ tự nhiên
(NLP) bao gồm việc xác định mục đích hoặc ý định đằng sau lời nói của người nói trong cuộc trò
chuyện. Tác vụ này có thể hữu ích trong nhiều ứng dụng khác nhau, như chatbot hoặc trợ lý ảo,
nơi quan trọng là hiểu ý định đằng sau đầu vào của người dùng để phản hồi phù hợp. Một ví dụ
về nhận dạng hành vi hội thoại là xác định ý định đằng sau câu nói sau: "Bạn có thể chuyển muối
cho tôi không?" Hành vi hội thoại trong câu nói này có thể được phân loại là một yêu cầu, vì người
nói đang yêu cầu người nghe thực hiện một hành động.

Sửa lỗi Ngữ pháp: Sửa lỗi ngữ pháp là một tác vụ xử lý ngôn ngữ tự nhiên (NLP) bao gồm việc
xác định và sửa chữa các lỗi ngữ pháp trong một văn bản cho trước. Một ví dụ về một câu có lỗi
ngữ pháp có thể được sửa chữa như một phần của tác vụ này là: "Tôi đã đi đến các cửa hàng để
mua thức ăn và quần áo." Câu này chứa lỗi ngữ pháp là sử dụng sai dạng của từ "cửa hàng."
Dạng đúng nên là "cửa hàng," là số ít, như trong "Tôi đã đi đến cửa hàng để mua thức ăn và quần
áo."

Gắn thẻ Từ khóa: Gắn thẻ từ khóa là quá trình gán các từ khóa hoặc nhãn cụ thể cho một đoạn
văn bản hoặc tài liệu. Tác vụ này thường được sử dụng trong xử lý ngôn ngữ tự nhiên (NLP) để
giúp phân loại và tổ chức lượng lớn dữ liệu văn bản cho nhiều mục đích khác nhau, như công cụ
tìm kiếm, mô hình hóa chủ đề, và phân tích cảm xúc. Ví dụ, xem xét một bài báo về các sự kiện
chính trị gần đây ở Hoa Kỳ. Gắn thẻ từ khóa cho bài báo này có thể bao gồm các nhãn như "chính
trị," "chính trị Hoa Kỳ," "bầu cử," "chính phủ," và "các đảng chính trị." Các thẻ này có thể giúp
xác định các chủ đề và đề tài chính được thảo luận trong bài báo, làm cho việc tìm kiếm và tìm
các bài báo tương tự về cùng chủ đề dễ dàng hơn.

Trích xuất Chồng lấp: Trích xuất chồng lấp là một tác vụ xử lý ngôn ngữ tự nhiên (NLP) bao gồm
việc trích xuất văn bản hoặc dữ liệu chồng lấp từ nhiều nguồn. Điều này có thể hữu ích cho nhiều
mục đích khác nhau, như xác định các chủ đề chung trong các tài liệu khác nhau, so sánh và đối
chiếu thông tin, hoặc tìm bản sao trong tập dữ liệu. Ví dụ, xem xét một tình huống mà bạn có hai
bài báo thảo luận về cùng một chủ đề. Bạn có thể sử dụng trích xuất chồng lấp để xác định các
chủ đề hoặc ý tưởng chung được thảo luận trong cả hai bài báo, như các sự kiện chính, người liên
quan, hoặc trích dẫn quan trọng. Điều này có thể giúp bạn hiểu tổng thể về việc đưa tin về chủ
đề và xác định bất kỳ sự bất đồng hoặc khác biệt nào trong cách nó được trình bày bởi hai nguồn.

Viết lại Câu hỏi: Viết lại câu hỏi là một tác vụ xử lý ngôn ngữ tự nhiên (NLP) bao gồm việc tạo
một phiên bản mới của một câu hỏi cho trước có cùng ý nghĩa với câu gốc, nhưng được diễn đạt
khác. Ví dụ, với câu hỏi "Thủ đô của Pháp là gì?", một tác vụ viết lại câu hỏi có thể tạo ra câu
hỏi được diễn đạt lại sau: "Nơi đặt trụ sở chính phủ của Pháp ở đâu?". Bảng 18 đưa ra một ví
dụ về danh mục này.

Suy luận Văn bản: Suy luận văn bản là một tác vụ xử lý ngôn ngữ tự nhiên bao gồm việc xác định
mối quan hệ giữa hai đoạn văn bản. Cụ thể, nó bao gồm việc xác định xem một đoạn văn, được
gọi là "giả thuyết," có thể được suy ra từ đoạn văn khác, được gọi là "tiền đề," hay không. Ví dụ:
Tiền đề: "Con mèo đang ngồi trên ghế sofa." Giả thuyết: "Có một con mèo trên ghế sofa." Trong
trường hợp này, giả thuyết có thể được suy ra từ tiền đề, vì vậy mối quan hệ suy luận văn bản là
"kéo theo." Bảng 14 đưa ra một ví dụ về danh mục này.

Tạo Tiêu đề: Tạo tiêu đề là một tác vụ xử lý ngôn ngữ tự nhiên (NLP) bao gồm việc tạo tiêu đề
cho một văn bản hoặc chủ đề cho trước. Tác vụ này thường được sử dụng trong tạo nội dung và
tiếp thị, nơi một tiêu đề bắt mắt là thiết yếu để thu hút sự chú ý và thu hút độc giả. Ví dụ, một
tác vụ tạo tiêu đề có thể bao gồm việc tạo tiêu đề cho một bài báo về lợi ích của thiền. Một số
tiêu đề có thể là "5 Lý do Tại sao Thiền là Chìa khóa cho Cuộc sống Không Căng thẳng," "Khám
phá Lợi ích Đáng ngạc nhiên của Thiền," hoặc "Thiền: Công cụ Tối ưu cho Thư giãn và Chánh
niệm." Mục tiêu của tác vụ tạo tiêu đề là tạo ra một tiêu đề phản ánh chính xác nội dung của bài
báo và đủ hấp dẫn để khuyến khích độc giả nhấp và đọc thêm. Bảng 19 đưa ra một ví dụ về danh
mục này.

Tương tự Từ: Tương tự từ là một tác vụ xử lý ngôn ngữ tự nhiên bao gồm việc xác định mối quan
hệ giữa các từ dựa trên ý nghĩa và ngữ cảnh của chúng. Mục tiêu là tìm một từ tương tự với một
từ khác theo một cách cụ thể, dựa trên mối quan hệ giữa hai từ. Ví dụ, nếu tác vụ là tìm một từ
tương tự với "đàn ông" theo cách mà "phụ nữ" tương tự với "đàn ông," câu trả lời đúng sẽ là "vợ."
Mối quan hệ giữa các từ "đàn ông" và "vợ" là cả hai đều là thuật ngữ cho một loại vợ/chồng cụ
thể, với "đàn ông" là thuật ngữ cho chồng và "vợ" là thuật ngữ cho vợ.

B Kết quả Chi tiết
Siêu tham số: Kích thước batch huấn luyện: 1, Kích thước batch đánh giá: 1, Bước tích lũy gradient:
2, Độ dài nguồn tối đa: 1024, Độ dài đích tối đa 128, độ dài tối đa tạo: 128, tốc độ học: 5e-05,
số epoch: 2, bước khởi động: 0

Kết quả và tập dữ liệu khác: Bảng 6 đưa ra thống kê theo tác vụ, số lượng mẫu tổng cộng, và
danh mục của chúng. Bảng 5 là bảng pivot tương tự, đưa ra số lượng tác vụ theo danh mục. Bảng

7 đưa ra tổng số mẫu được sử dụng cho các đường cơ sở khác nhau và phương pháp STL MTL.
Bảng 8 đưa ra điểm số đường cơ sở STL. Bảng 9 đưa ra kết quả Mô hình STL. Bảng 12 đưa ra
trung bình theo danh mục cho tương tự. Bảng 10 đưa ra đường cơ sở MTL, và 11 đưa ra kết quả
MTL, và Bảng 13 đưa ra pivot theo danh mục cho tương tự.

[Tiếp tục với các bảng và dữ liệu chi tiết như trong văn bản gốc...]
