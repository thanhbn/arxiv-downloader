# AutoMix: Automatically Mixing Language Models

Pranjal Aggarwal♢∗Aman Madaan♣ ∗Ankit Anand‡Srividya Pranavi Potharaju†
Swaroop Mishra‡Pei Zhou△Aditya Gupta Dheeraj Rajagopal†Karthik Kappaganthu†
Yiming Yang♠Shyam Upadhyay†Manaal Faruqui†Mausam♢
♠Carnegie Mellon University ♣xAI †Google ‡Google DeepMind
♢IIT Delhi △University of Southern California
automix-models@googlegroups.com

## Abstract

Large language models (LLMs) are now available from cloud API providers in
various sizes and configurations. While this diversity offers a broad spectrum
of choices, effectively leveraging the options to optimize computational cost and
performance remains challenging. In this work, we present AutoMix , an approach
that strategically routes queries to larger LMs, based on the approximate correct-
ness of outputs from a smaller LM. Central to AutoMix are two key technical
contributions. First, it has a few-shot self-verification mechanism, which estimates
the reliability of its own outputs without requiring extensive training. Second,
given that self-verification can be noisy, it employs a POMDP based router that
can effectively select an appropriately sized model, based on answer confidence.
Experiments across five language models and five challenging datasets show that
AutoMix consistently surpasses strong baselines, reducing computational cost by
over 50% for comparable performance.

## 1 Introduction

The landscape of Large Language Models (LLMs) is rapidly evolving, with a wide array of models
now available in various sizes, capabilities, and computational requirements [Touvron et al., 2023,
OpenAI, 2023, Jiang et al., 2023a]. While larger models generally exhibit superior performance,
their substantial computational costs render them unaffordable for many simpler tasks. Moreover,
the vast array of available options makes it challenging for end-users to determine the optimal
model configuration for their specific needs. This challenge is further compounded by the intrinsic
complexity and variability of real-world tasks, ranging from simple (e.g., binary classification on
separable data) to complex (e.g., code generation) and potentially unsolvable tasks (e.g., certain
forms of multi-step reasoning). To address these issues and ensure that end-users can obtain the best
performance within their budget constraints, the development of model-switching techniques has
become increasingly important. These techniques involve dispatching queries to models of disparate
sizes and capabilities, allowing for a more efficient allocation of computational resources [Liu et al.,
2020, Zhou et al., 2020, Madaan and Yang, 2022, Geng et al., 2021, Schuster et al., 2022].

Contemporary model-switching strategies often rely on separate routing models trained for a fixed
set of tasks [Chen et al., 2023, Ding et al., 2024]. Moreover, modern LLMs are frequently accessible
only through black-box APIs, restricting direct model optimization due to the unavailability of
fine-tuning capabilities and weight access. This constraint, coupled with the expectation of access
to large amounts of task-specific data, creates a challenge that existing routing approaches fail to
address adequately. In response, we introduce AutoMix , a method that enables users to mixmodels
of various sizes and capabilities, assuming only access to black-box LLM APIs. As illustrated in
Figure 1,AutoMix consists of 3 steps designed within the constraints of black-box access: solution
generation (small model to generate initial answer), self-verification (same smaller model assesses
difficulty), and selective routing (routing to larger models when suggested by self-verification). At
a high level, this process mirrors human problem-solving, which inherently follows a multi-step
process: generate a solution, verify its validity, and refine it further based on verification outcomes.

An ideal router in AutoMix must incorporate several key characteristics. Firstly, it should be capable
of identifying the difficulty of a query based on confidence assessments of the smaller model's
answers. Secondly, it should be able to route to the appropriately sized model, for instance, easy
queries are routed to the small language model ( SLM ), hard queries to large language model ( LLM ),
and unlike previous works [Chen et al., 2023, Ramírez et al., 2024], unsolvable queries that no model
can solve should not be routed to any LM, thereby saving costs. Thirdly, it should be able to learn
from a small amount of data, as expected in real-world tasks. While self-verification seems a natural
choice to provide confidence assessments and learn from limited data, prior works have shown self-
verification is not particularly effective for reasoning tasks and is often noisy and ill-calibrated [Tyen
et al., 2023, Huang et al., 2023b] – a challenge that needs to be addressed. Finally, the router should
generalize to different scenarios in terms of the number of models with varying costs and capabilities.

To address these requirements, we propose two novel contributions in AutoMix . First, we formulate
self-verification as an entailment problem, where the consistency of the generated answer with the
context is evaluated to estimate confidence levels [Poliak, 2020, Dagan et al., 2022]. For example, an
answer discussing "apples" in a context focused on "tea" would be flagged as highly inconsistent.
Second, we introduce a Partially Observable MDP (POMDP) based router [Åström, 1965]. POMDPs
extend standard MDPs to decision problems where observations (self-verification probabilities) are
unreliable and provide a noisy estimate of states (question difficulty). Modeling the router as a
POMDP agent ensures all our requirements are met, as the POMDP can implicitly model various
difficulties and assess them based on noisy self-verification outputs. Furthermore, POMDPs provide
a principled formulation to learn robust routing policies in different scenarios with varying numbers
of models, different costs, and capabilities, while learning from as few as 50 examples.

We conduct extensive evaluations of AutoMix on five different dialogue and context-grounded
reasoning tasks, with five different models. Our results demonstrate that AutoMix consistently
outperforms baselines while reducing cost by over two times while achieving the same performance,
showcasing significant improvements and efficiency gains over existing model-switching strategies.

## 2 Background and Related Work

### Self-Verification 
The concept of self-verification in reasoning problems has been explored in various
works, such as Weng et al. [2023], Jiang et al. [2023b], Pan et al. [2023a]. These approaches typically
rely on the LLM's knowledge [Dhuliawala et al., 2023], a method that can pose challenges for
reasoning problems [Madaan et al., 2023, Huang et al., 2023b]. In contrast, AutoMix leverages
context for verification and introduces a POMDP router to mitigate potential noise from the verifier.
Another line of work collects a corpus of past mistakes made by models [Madaan et al., 2022], and
external knowledge bases for verification [Peng et al., 2023, Gao et al., 2023, Pan et al., 2023b].
In contrast, AutoMix uses the question's context to verify the answer. While previous works find
self-verification is unreliable to repair the model's output [Huang et al., 2023a], we demonstrate that
self-verification provides a valuable signal used for routing to an appropriate model.

### Mixing Models 
Several works have sought to optimize LLM inference cost through model switching,
employing specifically trained verifiers [Chen et al., 2023, Zhu et al., 2023, vSakota et al., 2023, Ding
et al., 2024]. AutoMix eliminates the need for expensive verifier training through few-shot SLM
model prompting and does not require upfront access to all input queries. The router, trained with as
few as 50 samples, outperforms specialized models. Our work is thus aligned with recent work that
aims at composing different models and external tools for inference time improvement of language
models [Khattab et al., 2023, Press et al., 2022, Yao et al., 2022, Zhou et al., 2022].

### Adaptive Computation 
Adaptive computation and model routing methods often preempt computa-
tion via intermediate representations [Liu et al., 2020, Zhou et al., 2020, Geng et al., 2021, Schuster
et al., 2022, Madaan and Yang, 2022]. Unlike these methods, AutoMix requires no architectural
changes and assumes only black-box access to APIs. While some black-box methods like Adaptive-
Consistency [Aggarwal et al., 2023] aim to optimize inference for reasoning tasks, they are limited to
a single LLM model, whereas AutoMix flexibly optimizes between two or more models.

### Background on Partially Observable Markov Decision Processes 
POMDPs extend Markov
Decision Processes (MDPs) to scenarios where an agent's state observation is partial or noisy. Defined
as a tuple ⟨S, A, T, R, Ω, O⟩,Sdenotes states, Aactions, and Ωpossible observations. The transition
function Tgives transition probabilities between states given an action, while the observation function
Pconnects actions and states to observations. The reward function Rassigns rewards to actions
in specific states. Agents in POMDPs maintain a belief ( b), a probability distribution over states.
This belief updates based on actions and observations. The objective in a POMDP is to find a policy
π:b7→a∈Athat maximizes the expected cumulative long-term reward. POMDPs have been
used in various domains, including robotics, automated navigation, crowdsourcing workflows, and
strategic planning [Kaelbling et al., 1998, Schwarting et al., 2018, Dai et al., 2010, Meuleau et al.,
2013]. However, AutoMix uses POMDPs in a novel way to route queries between LMs. Our noisy
self-verification outputs act as observations to the POMDP, which assesses question difficulty and
routes to the appropriate LM to maximize a reward function of cost and performance.

## 3 Problem Formulation

We address the problem of selecting the appropriate language model (LM) to maximize a user-defined
cost-quality tradeoff. We assume access to Ndistinct language models: LM 1, LM 2. . . LM N,
numbered in increasing order of number of parameters. Each model has an associated cost of Ci
and (unknown) performance of Pifor each input query. Our objective is to maximize the total
performance for any total cost while dynamically invoking any LM as appropriate for a given test
point. In experiments, we compare cost-quality curves of various methods and also evaluate them
using a newly proposed IBCmetric (See Section 5.1).

To test our approach, we consider context-grounded reasoning tasks, such as comprehension-based
QA and a variety of dialogue reasoning tasks, where given a context C(e.g., stories, newswire,
or dialogue history) and a question q, the model must generate an accurate and coherent answer
consistent with the provided context. Our choice of tasks is motivated by two key considerations:
(1) longer queries are more computationally demanding, underscoring the need for an approach
likeAutoMix to navigate the cost-accuracy tradeoff, and (2) the context allows for cross-checking
preliminary answers with available information using self-verification (described shortly). We assume
only black-box access is available to the LM APIs. For training any router models, we assume access
to a small training/validation dataset Dtrain consisting of <C, q,ˆy, yLMi>triplets, where ˆyis the
ground truth answer and yLMiis the answer generated by LMi.

## 4 AutoMix

At a high level, AutoMix consists of three steps: solution generation – using a small LM, say LMi
(initially i= 1), to generate an answer As; self-verification – using the same LMito assess As;
and selective routing – picking a larger LM, LMj(j > i ), when suggested by self-verification, else
returning Asas final answer. Figure 1 shows a representative example of the process for the two-LM
case. Next, we discuss our two technical contributions in detail.

### 4.1 Self-Verification

To assess the trustworthiness of As,AutoMix employs a few-shot verifier, V, which validates LMi's
output. Unlike existing works that perform verification by creating a new question [Weng et al., 2022,
Jiang et al., 2023b], we frame verification as an entailment task [Dagan et al., 2005, Poliak, 2020,
Dagan et al., 2022], to determine whether the answer generated by LMialigns with the provided
context. Specifically, the verifier gauges v=p(correct = 1 | As,C, q), with correct = 1indicating
thatAsis correct. To estimate the probability, we sample k >1times from the verifier ( LMi) with
high sampling temperature, and probability is then computed as1/k∑ki=11{correct = 1 }. We use the
same 4-shot verification prompt for all tasks and do not train a verifier. Figure 1, 2 shows the prompt
and an example of self-verification in action. We refer readers to Appendix B for all prompts used.

### 4.2 Router

Routing follows solution generation and self-verification. The router decides whether the output from
LMishould be accepted or the query be routed to some LMj(j > i ) for improved performance. The
router can also be interpreted as a meta-verifier , providing an extra layer of confidence assessment on
the few-shot verifier's evaluations. Specifically, Vascertains whether LMi's answer is entailed by the
context, making its decision without considering the inherent difficulty of the problem. For example,
when handling Unsolvable queries, invoking a larger LM will be resource-inefficient and would not
enhance performance. A good router can address this by not routing such a query further, and this
decision needs to be made using the verification probability and trends from the training data.

Addressing the notable challenges of self-correction in large language models [Madaan et al., 2023,
Huang et al., 2023b], AutoMix employs a non-LLM setup for routing and avoids escalating issues
such as hallucination and reasoning errors [Dziri et al., 2023]. The router can, in principle, adopt
various learning strategies, including supervised learning, reinforcement learning, and symbolic
reasoning. Subsequent sections provide details of two different routing strategies in AutoMix .

#### Thresholding 
In this simplistic routing approach for the two-model case ( N= 2), the decision to
route to LM 2is based on the probability vof the LM 1verifier and a threshold t. Ifv≥t, it returns
LM 1's answer, else routes the query to LM 2. Intuitively, a high probability indicates the verifier is
confident in its decision and can be trusted. Varying tcan help explore cost-performance tradeoffs.

#### POMDP-based Router 
A router should direct a query to larger LMs only when the performance
gap justifies the cost-quality tradeoff. Given the inherent uncertainty in the true state of system
performance, which remains unobserved, we formulate the router as a Partially Observable Markov
Decision Process (POMDP) [Åström, 1965]. POMDPs are particularly well-suited for scenarios
where observations, such as self-verification probabilities, may not be entirely reliable.

Recall that (Section 2) a POMDP is characterized by (S, A, T, R, Ω, O). In our application, the states
Srepresent the current selected LMiand performance metrics (e.g., accuracy or F-score) of various
LMs on a data point, denoted as S=⟨i, Perf LM 1, Perf LM 2, . . . , Perf LMN⟩. The actions include
either retaining the current LM's ( LMi) answer or routing to one of the larger LMs. Observations
Ω, in the form of the verifier, outputs vfrom LMi, enabling the POMDP to ascertain its belief
stateb: a probability distribution over S. The observation probabilities P(o|s), which indicate the
likelihood of observing o(verification output) given state s, are crucial for defining the POMDP
model. E.g., high verifier confidence might suggest that the current LM's performance Perf LMiis
high enough, reducing the necessity to switch to a more costly LM. The observation probabilities
are estimated directly on the train set, by computing the expectation of verification probabilities for
each state, i.e P(o|s) =∑sj,vj∈Dtrain1{sj=sandvj=o}/∑sj∈Dtrain1{sj=s}. However, since the states can be continuous,
we use non-parametric Gaussian kernel density estimation to estimate the observation probability
for a new state. Instead of directly estimating P(o|s), we first learn a joint distribution P(S, O)and
P(S), by drawing KDE at each training point. Then conditional probability P(o|s)is computed as:
P(o|s) =P(s,o)/P(s).

The objective of our POMDP is to optimize a reward function R=P−λ·C, where Pis overall
performance, Cis overall cost, λis a tuning parameter that balances two criteria, according to
user preferences. We utilize the AdaOps POMDP solver [Wu et al., 2021], which employs particle
filtering for representing belief, where each particle corresponds to a particular state. At inference,
the POMDP solver starts with an initial belief (uniform distribution), updates its belief state based
on verifier observations, and, based on the updated belief state b′, takes an action to maximize the
expected reward. We provide further details and the complete POMDP formulation in Appendix A.3.

## 5 Experiments

Through our experiments, we wish to answer the following research questions. How does AutoMix
compare with other model-switching strategies? How well does AutoMix perform when varying
(1) cost ratios between models, and (2) the amount of available training data? Following recent
work [Ding et al., 2024], we perform most experiments in the setting when N= 2, but also report
additional results in the three model cases (in Section 5.5). For N= 2, we use the terms SLM and
LLM to denote small ( LM 1) and large ( LM 2) language models, respectively.

### 5.1 A Metric for Cost-Performance Efficiency Analysis

#### Incremental Benefit Per Cost ( IBC)
In our approach to leveraging model performance, it is
essential to consider not only the raw accuracy of predictions but also the associated computational
or monetary costs. To that end, we introduce a metric to understand the performance of the models
in terms of cost. We introduce methods, denoted by M, to optimally integrate SLM andLLM . For
each method M, we associate a cost CMand performance PM. To quantify the utility of Mover
SLM, we define the metric Incremental Benefit Per Cost (IBC) as IBCM(Equation (1)).

IBCM=PM−PSLM/CM−CSLM, IBC BASE =PLLM−PSLM/CLLM−CSLM,∆IBC(M) =IBCM−IBC BASE/IBC BASE×100

The IBCmetric captures the efficiency of performance enhancement relative to the additional cost.
For comparative evaluation, we set a baseline IBC,IBC BASE, representing the benefit of always using
LLM over SLM . Finally, we compare methods using ∆IBC, which compares the IBCof a specific
method with IBC BASE. A positive IBClift suggests that Machieves performance increments more
cost-effectively than a standalone LLM , whereas a negative lift indicates reduced efficiency (Figure 3)

#### Geometric Interpretation 
On a Performance vs. Cost plot, consider the line segment joining
the data points of the small language model ( SLM ) and the large language model ( LLM ). This
segment's slope represents the rate of performance increase per unit of cost. The Incremental
Benefit per Cost ( IBC) for any method Mis the slope of the line from the SLM point to the point
representing M(Figure 3). A method Mthat lies above the SLM -LLM segment provides a steeper
slope, indicating a favorable IBC(and a positive ∆IBC). Conversely, if Mlies below the segment, it
suggests an unfavorable or negative IBC. Our primary objective is to develop methods that yield a
consistently positive IBC, maximizing performance enhancements for each additional unit of cost.

### 5.2 Setup

#### Models and Cost Calculation 
We use GPT-3.5,LLAMA 2-13 B, and MISTRAL -7B(instruct v0.2
version) [Touvron et al., 2023, Jiang et al., 2023a] as the smaller language models ( SLM s), and
GPT-4[OpenAI, 2023] as the larger language model ( LLM ). For each input query, we model fixed
costs for the models and verification, denoted by CSLM for smaller models, CLLM for the larger
model, and Cverfor the verification model. As the verification is performed by CSLMand the major
cost driver is the question context, which remains the same in both models, we set Cver=CSLM. At
inference, the total cost is computed as the sum of individual costs, C=CSLM+Cver+w·CLLM,
where windicates whether the larger model is invoked.

While the pricing of these APIs [Dehghani et al., 2021] is influenced by various complexities, our
focus on the black-box utilization of language models leads us to represent cost based on the actual
API price disparity between these models.For each of GPT-3.5,LLAMA 2-13 B,MISTRAL -7B, we
assign a relative cost of 1 unit for the SLM and 60, 100, 200 units for the LLM respectively. It's
important to note that the cost ratio between models can vary significantly depending on specific
deployment scenarios. For example, for a user with access to a single A6000 GPU, running LLAMA 2-
13Bmight incur virtually no cost, while utilizing GPT-4could be prohibitively expensive. We simulate
this scenario in Section 5.4. We refer readers to Appendix B for additional details.

#### Datasets 
We experiment with a diverse set of datasets: i) QASPER [Dasigi et al., 2021]: Question
answering over research papers; ii) QUALITY [Pang et al., 2022]: Multiple-choice questions (MCQ)
on long articles and stories; iii) COQA [Reddy et al., 2019]: Conversational comprehension requiring
coreference and pragmatic reasoning; iv) MUTUAL [Cui et al., 2020]: Multi-turn dialogue reasoning
(next response prediction); v) DIPLOMAT [Li et al., 2023]: Pragmatic identification and reasoning
questions on multi-turn dialogues. We use the F1 score for QASPER and COQA , and accuracy for
the remaining datasets. We use the default validation splits and utilize prompts from Shaham et al.
[2023] for QASPER and QUALITY , and adapt the QUALITY prompt for other datasets. We use identical
prompts for all models. We refer readers to Appendix B for more details.

#### Baselines 
We compare against FrugalGPT [Chen et al., 2023] and HybridLLM [Ding et al.,
2024], two state-of-the-art models, as our baselines. FrugalGPT uses a finetuned DistillBert
model [Sanh et al., 2019] as a router. If the router's confidence for a given question, context, and
SLM answer falls below a threshold, the query is routed to the LLM .HybridLLM uses trained
DeBERTa [He et al., 2021] as a router which directly chooses between SLM andLLM without
generating an SLM response. Further, training labels for HybridLLM 's router are generated based
on the probability of SLM outperforming LLM by a margin, computed by drawing multiple samples.
Note, that training each of these baselines along with AutoMix requires running inference on all
models, however, it is only a training time cost. However, at inference, unlike AutoMix we assign a
cost of 0 to the two baselines' routers due to their lower operational costs.

### 5.3 Main Results

Figure 4 illustrates the performance versus cost curves for various datasets and model-mixing
methods using MISTRAL -7BasSLM . Across all datasets, AutoMix -POMDP and AutoMix -
Threshold consistently outperform FrugalGPT andHybridLLM , staying above the SLM -LLM
curve and yielding better performance per unit cost. This is particularly notable as both FrugalGPT
andHybridLLM leverage domain-specific trained routers and do not incur any verification costs.
Comparable trends are observed with other SLMs LLAMA 2-13 Band GPT-3.5 in Figure 18, 19.

Table 1 presents a comparison of the average ∆IBCacross five equally sized cost regions for
each method, dataset, and three different SLM s.AutoMix -POMDP consistently outperforms
FrugalGPT andHybridLLM across all datasets and models, with performance equalling
FrugalGPT in the qasper dataset for the LLAMA 2-13 Bmodel. The gains vary across differ-
ent settings, with the best performance observed on the DIPLOMAT dataset, averaging 122% across
all models. Importantly, AutoMix -POMDP is the only method that yields positive gains across all
configurations and consistently matches or exceeds the performance of AutoMix -Threshold. This is
despite being tested on a variety of datasets and models representing different costs, task difficulty
(accuracy ranging from 30% to 90%), performance gap between the models (8% to ≈50%), different
answer types (MCQ, Open-ended). The results demonstrate that AutoMix , utilizing self-verification
and appropriate routing, can effectively mix different SLM s and GPT-4on a wide range of tasks
without access to model weights or domain-specific routing data. These substantial gains translate
directly into cost savings, underscoring the economic relevance of our approach.

### 5.4 Additional Experiments

#### Effect of Cost Ratio on AutoMix 
Our main experiments assumed different cost ratio ranging from
1:60 to 1:200 for different SLM s. Subsequently, we conduct an analysis to understand how alterations
in the cost ratio influence the ∆IBCvalues across different cost ratios. In Figure 5 (right), we vary
cost-ratio and report ∆IBCnormalized by the maximum value obtained on the dataset (a higher value
denotes higher gains). The results suggest that even for a cost ratio as low as 1:10, AutoMix begins
to deliver good gains for many datasets, which naturally improve as cost-ratio gets further skewed.

We supplement the analysis with cost-performance curves for different cost ratios in Appendix D.2.
We note that cost need not only be monetary cost, and could, in general, represent other considerations
such as latency or energy usage. These results demonstrate that AutoMix provides a novel method to
balance the cost-performance tradeoff across a wide range of cost scenarios and is robust to changes
without any modifications to the method.

#### AutoMix is Effective in Low-Resource Scenarios 
Figure 5 (left) demonstrates the performance
dynamics of AutoMix ,FrugalGPT andHybridLLM with varying validation set size. Notably,
our method significantly outperforms FrugalGPT andHybridLLM with limited data, despite
the latter's domain-specific training and zero verifier cost. E.g., with just 50 training examples,
AutoMix maintains 15% ∆IBCand beats the baselines by more than absolute 20%. This underscores
thatAutoMix is particularly advantageous in the real-world scenarios where training data is scarce.

#### When Does POMDP Routing Help? 
POMDP implicitly categorizes question difficulty into three
types: (a) easy queries solvable by the small model, (b) complex queries only larger models can
solve, and (c) potentially unsolvable queries for any model. It leverages self-verification probabili-
ties—which provide a noisy estimate of difficulty—to make non-linear decisions. For example, in
the QASPER dataset using MISTRAL -7BasSLM , POMDP identifies that lower confidence values
correspond to such cases, and instead of routing to the LLM as other methods do, returns the SLM
answer, saving cost. Furthermore, in setups with more than two models (as discussed in Section 5.5),
POMDP makes more nuanced decisions, for instance by combining information from small and
medium-sized verifiers, resulting in significantly superior performance.

#### Other Experiments 
We perform various other experiments, whose details are in appendix due
to space limitations. We first evaluate few-shot self-verification quantitatively and qualitatively and
observe that the self-verification can effectively use context to identify errors in answers generated by
SLM in many cases (see Appendix A).

WhileAutoMix is outperforms on diverse set of datasets, to further demonstrate our router's
generalizability, we evaluate out-of-domain generalization by training on one dataset and evaluat-
ing on held-out datasets. Results in Appendix D.3 show that AutoMix consistently outperforms
FrugalGPT andHybridLLM by significant margins. Overall, the task-agnostic design of POMDP
and these results highlight our method's generalizability.

We also study the latency overhead of AutoMix . At inference time, POMDP takes less than 1ms
for every query in the two-model setup. Moreover, network latency (usually approximately 10ms) is
much less than the average solution generation time by SLM and LLM (on the order of seconds).
Thus,AutoMix adds no additional computational or latency overhead compared to language model
inference. Finally, incorporating the POMDP router for end-users is convenient, as our library can be
incorporated in less than five lines of code.

### 5.5 Results of Automix with Three Models

We evaluate the performance of AutoMix ap-
plied to a three-model scenario ( N= 3). Specif-
ically, we use LLAMA 2-13 BasSLM ,LLAMA 2-
70BasMLM , and GPT-4asLLM . Results
are presented in Figure 6. AutoMix consis-
tently outperforms baselines across cost regions.
We first compare AutoMix toFrugalGPT ,
showing significant improvements across all
cost regions considered. We also compare
AutoMix to a baseline, UnionAutoMix ,
which selects between AutoMix SLM−MLM
andAutoMix MLM −LLM based on the user's
cost requirements. For instance, if the de-
sired average cost is less than that of the
MLM ,AutoMix SLM−MLM is employed;
otherwise, AutoMix MLM −LLM is utilized.
Further, we consider a baseline, Chained
AutoMix , by chaining AutoMix SLM−MLM
withAutoMix MLM −LLM . The query first
goes to the SLM , and anAutoMix SLM−MLM
decides between reporting the SLM answer or
routing to the MLM . In the latter case, a second
AutoMix MLM−LLMrepeats the procedure using the MLM andLLM models. Chained AutoMix
underperforms across the board, as it cannot directly route queries from the SLM to the LLM .
Additionally, whenever Chained AutoMix prompts the MLM , it invariably uses the costly verifier,
even when it might not be necessary. Further, in Figure 9, 10, we study two additional cases: 1.)
If the MLM underperforms the SLM , and its verifier is uninformative, then unlike FrugalGPT ,
AutoMix learns the irrelevance of MLM and routes directly from SLM toLLM as needed; 2.)
In scenarios where MLM underperforms the SLM , but its verifier provides useful information,
AutoMix leverages information from MLM 's verifier to outperform all baselines considered. The
experiments show the efficacy of AutoMix in providing optimum performance across diverse
situations without additional intervention. We refer readers to Appendix C for more details.

## 6 Conclusion

AutoMix integrates multiple black-box LLM APIs into a multi-step problem-solving framework,
optimizing the computational cost and performance trade-offs. Our work interweaves Good Old-
Fashioned Artificial Intelligence (GOFAI) approaches with LLMs, demonstrating that the incorpora-
tion of a POMDP can lead to effective routing between LLMs. Our work provides a novel method
to balance cost-performance tradeoffs across a wide variety of models over different desired cost
ranges, consistently outperforming the baselines by significant margins. AutoMix opens avenues
for several interesting research directions. While self-verification and correction are challenging for
LLMs in general, we find promising results using context-grounded few-shot verification coupled
with POMDPs, indicating that similar approaches may also help other scenarios. The work may be
extended to generation tasks, where the notion of performance is subjective.

## Limitations

While our empirical evidence across various models and datasets demonstrates the effectiveness of
AutoMix , its broader applicability might vary depending on the specific models and datasets used.
Furthermore, AutoMix is designed with a dialogue-related or context-grounded reasoning setup in
mind for effective self-verification, which does not include tasks like factual question-answering and
commonsense reasoning. In the future, as open-source models become more powerful and inference
costs decrease, it might be feasible to serve a strong model for all queries. However, there are likely
to still be availability trade-offs that could be managed using AutoMix .