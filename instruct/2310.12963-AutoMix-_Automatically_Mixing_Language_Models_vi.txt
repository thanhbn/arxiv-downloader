# 2310.12963.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2310.12963.pdf
# Kích thước tập tin: 1390729 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
AutoMix: Tự động Trộn các Mô hình Ngôn ngữ
Pranjal Aggarwal♢*Aman Madaan♣ *Ankit Anand‡Srividya Pranavi Potharaju†
Swaroop Mishra‡Pei Zhou△Aditya Gupta Dheeraj Rajagopal†Karthik Kappaganthu†
Yiming Yang♠Shyam Upadhyay†Manaal Faruqui†Mausam♢
♠Carnegie Mellon University ♣xAI †Google ‡Google DeepMind
♢IIT Delhi △University of Southern California
automix-models@googlegroups.com
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) hiện được cung cấp từ các nhà cung cấp API đám mây với nhiều kích thước và cấu hình khác nhau. Trong khi sự đa dạng này cung cấp một phổ rộng các lựa chọn, việc tận dụng hiệu quả các tùy chọn để tối ưu hóa chi phí tính toán và hiệu suất vẫn là một thách thức. Trong công trình này, chúng tôi trình bày AutoMix, một phương pháp điều hướng chiến lược các truy vấn đến các LM lớn hơn, dựa trên tính chính xác gần đúng của đầu ra từ một LM nhỏ hơn. Trọng tâm của AutoMix là hai đóng góp kỹ thuật chính. Đầu tiên, nó có một cơ chế tự kiểm chứng few-shot, ước tính độ tin cậy của đầu ra riêng của nó mà không cần đào tạo rộng rãi. Thứ hai, vì tự kiểm chứng có thể nhiễu, nó sử dụng một bộ định tuyến dựa trên POMDP có thể lựa chọn hiệu quả một mô hình có kích thước phù hợp, dựa trên độ tin cậy của câu trả lời. Các thí nghiệm trên năm mô hình ngôn ngữ và năm tập dữ liệu thách thức cho thấy AutoMix liên tục vượt trội các baseline mạnh, giảm chi phí tính toán hơn 50% cho hiệu suất tương đương.1
1 Giới thiệu
Bối cảnh các Mô hình Ngôn ngữ Lớn (LLM) đang phát triển nhanh chóng, với một loạt rộng các mô hình hiện có sẵn với nhiều kích thước, khả năng và yêu cầu tính toán khác nhau [Touvron et al., 2023, OpenAI, 2023, Jiang et al., 2023a]. Trong khi các mô hình lớn hơn thường thể hiện hiệu suất vượt trội, chi phí tính toán đáng kể của chúng khiến chúng trở nên không thể chi trả cho nhiều tác vụ đơn giản. Hơn nữa, loạt rộng các tùy chọn có sẵn khiến người dùng cuối khó xác định cấu hình mô hình tối ưu cho nhu cầu cụ thể của họ. Thách thức này còn được phức tạp hóa bởi độ phức tạp và tính biến đổi nội tại của các tác vụ thế giới thực, từ đơn giản (ví dụ: phân loại nhị phân trên dữ liệu có thể tách biệt) đến phức tạp (ví dụ: tạo mã) và các tác vụ có thể không giải được (ví dụ: một số dạng suy luận nhiều bước). Để giải quyết những vấn đề này và đảm bảo rằng người dùng cuối có thể đạt được hiệu suất tốt nhất trong giới hạn ngân sách của họ, việc phát triển các kỹ thuật chuyển đổi mô hình đã trở nên ngày càng quan trọng. Những kỹ thuật này bao gồm việc gửi các truy vấn đến các mô hình có kích thước và khả năng khác nhau, cho phép phân bổ tài nguyên tính toán hiệu quả hơn [Liu et al., 2020, Zhou et al., 2020, Madaan and Yang, 2022, Geng et al., 2021, Schuster et al., 2022].
Các chiến lược chuyển đổi mô hình đương đại thường dựa vào các mô hình định tuyến riêng biệt được đào tạo cho một tập hợp cố định các tác vụ [Chen et al., 2023, Ding et al., 2024]. Hơn nữa, các LLM hiện đại thường chỉ có thể truy cập được thông qua các API hộp đen, hạn chế tối ưu hóa mô hình trực tiếp do không có khả năng tinh chỉnh và truy cập trọng số. Ràng buộc này, kết hợp với kỳ vọng truy cập vào lượng lớn dữ liệu cụ thể cho tác vụ, tạo ra một thách thức mà các phương pháp định tuyến hiện có không giải quyết được một cách thỏa đáng. Để đáp ứng, chúng tôi giới thiệu AutoMix, một phương pháp cho phép người dùng trộn mô hình
**Đóng góp Ngang nhau. Công việc bắt đầu và một phần được thực hiện trong thời gian thực tập của Aman tại Google.
1Mã có sẵn tại github.com/automix-llm/automix
Hội nghị lần thứ 38 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2024).arXiv:2310.12963v5 [cs.CL] 19 Jan 2025

--- TRANG 2 ---
Định tuyến
LLM
Ngân sách: 
AutoMix
LM1/LLaMAPhát sinh
LM1Bộ kiểm chứng
Tự kiểm chứng
Bộ định tuyến 
POMDP
LM2/GPT-4Khó?Dễ hoặc
Không giải được?
Ngân sách: 
Độ chính xác: 
SLMĐộ chính xác: 
Auto mix tự động
tối ưu hóa sự cân bằng chi phí-hiệu suất
cho người dùng bằng cách trộn 
nhiều LM hộp đen
Hình 1: Ví dụ đại diện cho thiết lập 2 mô hình trong AutoMix. Thay vì chỉ dựa vào mô hình nhỏ (SLM) với hiệu suất thấp hoặc một mô hình lớn (LLM) với chi phí cao, AutoMix tự động trộn nhiều mô hình ngôn ngữ hộp đen, dựa trên sự cân bằng chi phí-chất lượng mong muốn của người dùng. AutoMix hoạt động trong quy trình 3 bước: 1.) phát sinh bởi một mô hình nhỏ (LM 1), 2.) tự kiểm chứng câu trả lời được tạo, 3.) sử dụng đánh giá độ tin cậy từ tự kiểm chứng để thực hiện định tuyến phù hợp đến một mô hình lớn hơn (LM 2). Đối với thiết lập N-mô hình, quy trình được lặp lại cho đến khi câu trả lời cuối cùng được báo cáo.
của nhiều kích thước và khả năng khác nhau, chỉ giả định truy cập vào các API LLM hộp đen. Như được minh họa trong Hình 1, AutoMix bao gồm 3 bước được thiết kế trong các ràng buộc của truy cập hộp đen: tạo giải pháp (mô hình nhỏ để tạo câu trả lời ban đầu), tự kiểm chứng (cùng mô hình nhỏ hơn đánh giá độ khó), và định tuyến có chọn lọc (định tuyến đến các mô hình lớn hơn khi được đề xuất bởi tự kiểm chứng). Ở mức độ cao, quy trình này phản ánh việc giải quyết vấn đề của con người, vốn dĩ tuân theo một quy trình nhiều bước: tạo ra một giải pháp, xác minh tính hợp lệ của nó, và cải thiện thêm dựa trên kết quả xác minh.
Một bộ định tuyến lý tưởng trong AutoMix phải kết hợp một số đặc điểm chính. Đầu tiên, nó phải có khả năng xác định độ khó của một truy vấn dựa trên đánh giá độ tin cậy của các câu trả lời của mô hình nhỏ hơn. Thứ hai, nó phải có thể định tuyến đến mô hình có kích thước phù hợp, ví dụ, các truy vấn dễ được định tuyến đến mô hình ngôn ngữ nhỏ (SLM), các truy vấn khó đến mô hình ngôn ngữ lớn (LLM), và khác với các công trình trước [Chen et al., 2023, Ramírez et al., 2024], các truy vấn không giải được mà không mô hình nào có thể giải quyết không nên được định tuyến đến bất kỳ LM nào, từ đó tiết kiệm chi phí. Thứ ba, nó phải có thể học từ một lượng nhỏ dữ liệu, như mong đợi trong các tác vụ thế giới thực. Trong khi tự kiểm chứng dường như là một lựa chọn tự nhiên để cung cấp đánh giá độ tin cậy và học từ dữ liệu hạn chế, các công trình trước đã cho thấy tự kiểm chứng không đặc biệt hiệu quả cho các tác vụ suy luận và thường nhiễu và không được hiệu chuẩn tốt [Tyen et al., 2023, Huang et al., 2023b] - một thách thức cần được giải quyết. Cuối cùng, bộ định tuyến phải tổng quát hóa cho các tình huống khác nhau về số lượng mô hình với chi phí và khả năng khác nhau.
Để giải quyết những yêu cầu này, chúng tôi đề xuất hai đóng góp mới trong AutoMix. Đầu tiên, chúng tôi xây dựng tự kiểm chứng như một vấn đề entailment, nơi tính nhất quán của câu trả lời được tạo với ngữ cảnh được đánh giá để ước tính mức độ tin cậy [Poliak, 2020, Dagan et al., 2022]. Ví dụ, một câu trả lời thảo luận về "táo" trong một ngữ cảnh tập trung vào "trà" sẽ được gắn cờ là có tính nhất quán rất cao. Thứ hai, chúng tôi giới thiệu một bộ định tuyến dựa trên Partially Observable MDP (POMDP) [Åström, 1965]. POMDP mở rộng MDP tiêu chuẩn đến các vấn đề quyết định nơi các quan sát (xác suất tự kiểm chứng) không đáng tin cậy và cung cấp ước tính nhiễu của các trạng thái (độ khó câu hỏi). Mô hình hóa bộ định tuyến như một tác nhân POMDP đảm bảo tất cả các yêu cầu của chúng tôi được đáp ứng, vì POMDP có thể mô hình ngầm nhiều độ khó khác nhau và đánh giá chúng dựa trên đầu ra tự kiểm chứng nhiễu. Hơn nữa, POMDP cung cấp một công thức có nguyên tắc để học các chính sách định tuyến mạnh mẽ trong các tình huống khác nhau với số lượng mô hình khác nhau, chi phí và khả năng khác nhau, trong khi học từ ít nhất 50 ví dụ.
Chúng tôi tiến hành đánh giá rộng rãi AutoMix trên năm tác vụ suy luận dựa trên đối thoại và ngữ cảnh khác nhau, với năm mô hình khác nhau. Kết quả của chúng tôi cho thấy AutoMix liên tục vượt trội các baseline trong khi giảm chi phí hơn hai lần khi đạt được cùng hiệu suất, thể hiện những cải thiện đáng kể và tăng hiệu quả so với các chiến lược chuyển đổi mô hình hiện có.
2 Bối cảnh và Công trình Liên quan
Tự kiểm chứng Khái niệm tự kiểm chứng trong các vấn đề suy luận đã được khám phá trong nhiều công trình khác nhau, chẳng hạn như Weng et al. [2023], Jiang et al. [2023b], Pan et al. [2023a]. Những phương pháp này thường
2

--- TRANG 3 ---
Ngữ cảnh: {context}
Câu hỏi: {question}
Câu trả lời được AI tạo: {generated_answer}
Hướng dẫn: Nhiệm vụ của bạn là đánh giá xem Câu trả lời được AI tạo có chính xác hay không, dựa trên ngữ cảnh và câu hỏi được cung cấp. Cung cấp phán đoán và lý luận cho từng trường hợp. Chọn giữa Chính xác hoặc Không chính xác.,→
,→
Đánh giá:"
Danh sách 1: Prompt Kiểm chứng. Quá trình kiểm chứng được đóng khung như một tác vụ entailment ngôn ngữ tự nhiên, nơi mô hình xác định tính hợp lệ của câu trả lời do mô hình tạo ra so với ngữ cảnh và câu hỏi. Chúng tôi sử dụng prompt few-shot chung cho tất cả các tác vụ.
dựa vào kiến thức của LLM [Dhuliawala et al., 2023], một phương pháp có thể đặt ra thách thức cho các vấn đề suy luận [Madaan et al., 2023, Huang et al., 2023b]. Ngược lại, AutoMix tận dụng ngữ cảnh để kiểm chứng và giới thiệu một bộ định tuyến POMDP để giảm thiểu tiếng ồn tiềm năng từ bộ kiểm chứng.
Một hướng công việc khác thu thập một kho các lỗi lầm trong quá khứ do các mô hình mắc phải [Madaan et al., 2022], và các cơ sở tri thức bên ngoài để kiểm chứng [Peng et al., 2023, Gao et al., 2023, Pan et al., 2023b].
Ngược lại, AutoMix sử dụng ngữ cảnh của câu hỏi để kiểm chứng câu trả lời. Trong khi các công trình trước thấy rằng tự kiểm chứng không đáng tin cậy để sửa chữa đầu ra của mô hình [Huang et al., 2023a], chúng tôi chứng minh rằng tự kiểm chứng cung cấp một tín hiệu có giá trị được sử dụng để định tuyến đến một mô hình phù hợp.
Trộn Mô hình Một số công trình đã tìm cách tối ưu hóa chi phí suy luận LLM thông qua chuyển đổi mô hình, sử dụng các bộ kiểm chứng được đào tạo riêng [Chen et al., 2023, Zhu et al., 2023, vSakota et al., 2023, Ding et al., 2024]. AutoMix loại bỏ nhu cầu đào tạo bộ kiểm chứng đắt đỏ thông qua prompting mô hình SLM few-shot và không yêu cầu truy cập trước tất cả các truy vấn đầu vào. Bộ định tuyến, được đào tạo với ít nhất 50 mẫu, vượt trội các mô hình chuyên biệt. Công việc của chúng tôi do đó phù hợp với công trình gần đây nhằm tổng hợp các mô hình khác nhau và công cụ bên ngoài để cải thiện thời gian suy luận của các mô hình ngôn ngữ [Khattab et al., 2023, Press et al., 2022, Yao et al., 2022, Zhou et al., 2022].
Tính toán Thích ứng Các phương pháp tính toán thích ứng và định tuyến mô hình thường ngăn chặn tính toán thông qua các biểu diễn trung gian [Liu et al., 2020, Zhou et al., 2020, Geng et al., 2021, Schuster et al., 2022, Madaan and Yang, 2022]. Khác với những phương pháp này, AutoMix không yêu cầu thay đổi kiến trúc và chỉ giả định truy cập hộp đen đến API. Trong khi một số phương pháp hộp đen như Adaptive-Consistency [Aggarwal et al., 2023] nhằm tối ưu hóa suy luận cho các tác vụ suy luận, chúng bị giới hạn ở một mô hình LLM duy nhất, trong khi AutoMix tối ưu hóa linh hoạt giữa hai hoặc nhiều mô hình.
Bối cảnh về Quy trình Quyết định Markov Quan sát Một phần POMDP mở rộng Quy trình Quyết định Markov (MDP) đến các tình huống nơi quan sát trạng thái của tác nhân là một phần hoặc nhiễu. Được định nghĩa là một bộ ⟨S, A, T, R, Ω, O⟩, S biểu thị các trạng thái, A hành động, và Ω các quan sát có thể. Hàm chuyển tiếp T cung cấp xác suất chuyển tiếp giữa các trạng thái cho một hành động, trong khi hàm quan sát P kết nối các hành động và trạng thái với các quan sát. Hàm phần thưởng R gán phần thưởng cho các hành động trong các trạng thái cụ thể. Các tác nhân trong POMDP duy trì một niềm tin (b), một phân phối xác suất trên các trạng thái. Niềm tin này cập nhật dựa trên các hành động và quan sát. Mục tiêu trong một POMDP là tìm một chính sách pi: b 7→ a trong A tối đa hóa phần thưởng tích lũy dài hạn mong đợi. POMDP đã được sử dụng trong nhiều lĩnh vực khác nhau, bao gồm robot, điều hướng tự động, quy trình làm việc crowdsourcing và lập kế hoạch chiến lược [Kaelbling et al., 1998, Schwarting et al., 2018, Dai et al., 2010, Meuleau et al., 2013]. Tuy nhiên, AutoMix sử dụng POMDP theo một cách mới để định tuyến các truy vấn giữa các LM. Các đầu ra tự kiểm chứng nhiễu của chúng tôi hoạt động như các quan sát đến POMDP, đánh giá độ khó câu hỏi và định tuyến đến LM phù hợp để tối đa hóa một hàm phần thưởng của chi phí và hiệu suất.
3 Xây dựng Vấn đề
Chúng tôi giải quyết vấn đề lựa chọn mô hình ngôn ngữ (LM) phù hợp để tối đa hóa sự cân bằng chi phí-chất lượng do người dùng định nghĩa. Chúng tôi giả định truy cập vào N mô hình ngôn ngữ khác biệt: LM 1, LM 2. . . LM N, được đánh số theo thứ tự tăng dần của số lượng tham số. Mỗi mô hình có chi phí liên quan Ci và hiệu suất (không biết) Pi cho mỗi truy vấn đầu vào. Mục tiêu của chúng tôi là tối đa hóa tổng hiệu suất cho bất kỳ tổng chi phí nào trong khi gọi động bất kỳ LM nào phù hợp cho một thử nghiệm đã cho
3

--- TRANG 4 ---
Ngữ cảnh
Quốc gia nào trồng nhiều trà nhất? Câu trả lời là Ấn Độ. Nó trồng gấp ba lần Trung Quốc. Quốc gia nào uống nhiều trà nhất? Không phải Trung Quốc hay Nhật Bản. Đó là Anh Quốc. [...] Một khách hàng đã đặt túi trà vào ấm. Sau đó ông chỉ đổ nước nóng lên trên. Và túi trà đã ra đời. Shen Nong là người đầu tiên uống trà. (Shen là một hoàng đế Trung Quốc.) Điều này là vào khoảng năm 2737 TCN. Shen có vấn đề tiêu hóa. Vì vậy ông uống vài cốc nước nóng mỗi ngày[...] Trà trở thành thức uống của Trung Quốc.
Câu hỏi
Shen Nong uống trà khi nào?Câu trả lời được tạo (bởi LLAMA2-13B)
Ông ấy uống nó vào năm 1990.
Đầu ra của Bộ kiểm chứng (bởi LLAMA2-13B)
Ngữ cảnh không đề cập rằng ông ấy uống trà vào năm 1990. Câu trả lời được AI tạo là Không chính xác.
Hình 2: Tự kiểm chứng Dựa trên Ngữ cảnh sử dụng LLAMA 2-13B trong Hành động. Ví dụ này thể hiện bộ kiểm chứng, sử dụng cùng mô hình với bộ tạo câu trả lời, xác định và từ chối một câu trả lời không chính xác - Ông ấy uống nó vào năm 1990 - bằng cách tận dụng hiệu quả ngữ cảnh.
điểm. Trong các thí nghiệm, chúng tôi so sánh các đường cong chi phí-chất lượng của nhiều phương pháp khác nhau và cũng đánh giá chúng bằng cách sử dụng một metric IBC mới được đề xuất (Xem Mục 5.1).
Để kiểm tra phương pháp của chúng tôi, chúng tôi xem xét các tác vụ suy luận dựa trên ngữ cảnh, chẳng hạn như QA dựa trên hiểu và nhiều tác vụ suy luận đối thoại khác nhau, nơi cho một ngữ cảnh C (ví dụ: câu chuyện, newswire, hoặc lịch sử đối thoại) và một câu hỏi q, mô hình phải tạo ra một câu trả lời chính xác và mạch lạc phù hợp với ngữ cảnh được cung cấp. Lựa chọn tác vụ của chúng tôi được thúc đẩy bởi hai cân nhắc chính: (1) các truy vấn dài hơn đòi hỏi tính toán nhiều hơn, nhấn mạnh nhu cầu cho một phương pháp như AutoMix để điều hướng sự cân bằng chi phí-độ chính xác, và (2) ngữ cảnh cho phép kiểm tra chéo các câu trả lời sơ bộ với thông tin có sẵn bằng cách sử dụng tự kiểm chứng (sẽ được mô tả ngắn gọn). Chúng tôi giả định chỉ có truy cập hộp đen đến các API LM. Để đào tạo bất kỳ mô hình định tuyến nào, chúng tôi giả định truy cập vào một tập dữ liệu đào tạo/xác thực nhỏ Dtrain bao gồm các bộ ba <C, q, ŷ, yLMi>, nơi ŷ là câu trả lời đúng và yLMi là câu trả lời được tạo bởi LMi.
4 AutoMix
Ở mức độ cao, AutoMix bao gồm ba bước: tạo giải pháp - sử dụng một LM nhỏ, giả sử LMi (ban đầu i = 1), để tạo ra một câu trả lời As; tự kiểm chứng - sử dụng cùng LMi để đánh giá As; và định tuyến có chọn lọc - chọn một LM lớn hơn, LMj (j > i), khi được đề xuất bởi tự kiểm chứng, ngược lại trả về As như câu trả lời cuối cùng. Hình 1 cho thấy một ví dụ đại diện của quy trình cho trường hợp hai-LM. Tiếp theo, chúng tôi thảo luận chi tiết hai đóng góp kỹ thuật của chúng tôi.
4.1 Tự kiểm chứng
Để đánh giá độ đáng tin cậy của As, AutoMix sử dụng một bộ kiểm chứng few-shot, V, xác thực đầu ra của LMi. Khác với các công trình hiện có thực hiện kiểm chứng bằng cách tạo ra một câu hỏi mới [Weng et al., 2022, Jiang et al., 2023b], chúng tôi đóng khung kiểm chứng như một tác vụ entailment [Dagan et al., 2005, Poliak, 2020, Dagan et al., 2022], để xác định liệu câu trả lời được tạo bởi LMi có phù hợp với ngữ cảnh được cung cấp hay không. Cụ thể, bộ kiểm chứng đo lường v = p(correct = 1 | As, C, q), với correct = 1 chỉ ra rằng As là chính xác. Để ước tính xác suất, chúng tôi lấy mẫu k > 1 lần từ bộ kiểm chứng (LMi) với nhiệt độ lấy mẫu cao, và xác suất sau đó được tính là 1/k ∑k i=1 1{correct = 1}. Chúng tôi sử dụng cùng prompt kiểm chứng 4-shot cho tất cả các tác vụ và không đào tạo bộ kiểm chứng. Hình 1, 2 cho thấy prompt và một ví dụ về tự kiểm chứng đang hoạt động. Chúng tôi giới thiệu độc giả đến Phụ lục B cho tất cả các prompt được sử dụng.
4

--- TRANG 5 ---
4.2 Bộ định tuyến
Định tuyến theo sau việc tạo giải pháp và tự kiểm chứng. Bộ định tuyến quyết định liệu đầu ra từ LMi có nên được chấp nhận hay truy vấn có nên được định tuyến đến một LMj nào đó (j > i) để cải thiện hiệu suất. Bộ định tuyến cũng có thể được hiểu như một meta-verifier, cung cấp một lớp đánh giá độ tin cậy bổ sung trên các đánh giá của bộ kiểm chứng few-shot. Cụ thể, V xác định liệu câu trả lời của LMi có được kéo theo bởi ngữ cảnh hay không, đưa ra quyết định của nó mà không xem xét độ khó nội tại của vấn đề. Ví dụ, khi xử lý các truy vấn Không giải được, việc gọi một LM lớn hơn sẽ không hiệu quả về tài nguyên và sẽ không tăng cường hiệu suất. Một bộ định tuyến tốt có thể giải quyết điều này bằng cách không định tuyến truy vấn đó xa hơn, và quyết định này cần được đưa ra bằng cách sử dụng xác suất kiểm chứng và xu hướng từ dữ liệu đào tạo.
Giải quyết những thách thức đáng chú ý của tự sửa chữa trong các mô hình ngôn ngữ lớn [Madaan et al., 2023, Huang et al., 2023b], AutoMix sử dụng một thiết lập không phải LLM cho định tuyến và tránh làm leo thang các vấn đề như ảo giác và lỗi suy luận [Dziri et al., 2023]. Bộ định tuyến có thể, về nguyên tắc, áp dụng nhiều chiến lược học khác nhau, bao gồm học có giám sát, học tăng cường và suy luận biểu tượng. Các phần tiếp theo cung cấp chi tiết về hai chiến lược định tuyến khác nhau trong AutoMix.
Ngưỡng Trong phương pháp định tuyến đơn giản này cho trường hợp hai mô hình (N = 2), quyết định định tuyến đến LM 2 dựa trên xác suất v của bộ kiểm chứng LM 1 và một ngưỡng t. Nếu v >= t, nó trả về câu trả lời của LM 1, ngược lại định tuyến truy vấn đến LM 2. Trực quan, một xác suất cao chỉ ra bộ kiểm chứng tự tin trong quyết định của nó và có thể được tin tưởng. Thay đổi t có thể giúp khám phá các sự cân bằng chi phí-hiệu suất.
Bộ định tuyến dựa trên POMDP Một bộ định tuyến nên hướng một truy vấn đến các LM lớn hơn chỉ khi khoảng cách hiệu suất biện minh cho sự cân bằng chi phí-chất lượng. Cho sự không chắc chắn nội tại trong trạng thái thực của hiệu suất hệ thống, vẫn không được quan sát, chúng tôi xây dựng bộ định tuyến như một Quy trình Quyết định Markov Quan sát Một phần (POMDP) [Åström, 1965]. POMDP đặc biệt phù hợp cho các tình huống nơi các quan sát, chẳng hạn như xác suất tự kiểm chứng, có thể không hoàn toàn đáng tin cậy.
Nhớ lại rằng (Mục 2) một POMDP được đặc trưng bởi (S, A, T, R, Ω, O). Trong ứng dụng của chúng tôi, các trạng thái S đại diện cho LMi hiện tại được chọn và các metric hiệu suất (ví dụ: độ chính xác hoặc F-score) của nhiều LM trên một điểm dữ liệu, được ký hiệu là S = ⟨i, Perf LM 1, Perf LM 2, . . . , Perf LMN⟩. Các hành động bao gồm việc giữ lại câu trả lời của LM hiện tại (LMi) hoặc định tuyến đến một trong các LM lớn hơn. Các quan sát Ω, dưới dạng các đầu ra của bộ kiểm chứng v từ LMi, cho phép POMDP xác định trạng thái niềm tin b: một phân phối xác suất trên S. Các xác suất quan sát P(o|s), chỉ ra khả năng quan sát o (đầu ra kiểm chứng) cho trạng thái s, là quan trọng để định nghĩa mô hình POMDP. Ví dụ: độ tin cậy cao của bộ kiểm chứng có thể gợi ý rằng hiệu suất của LM hiện tại Perf LMi đủ cao, giảm tính cần thiết chuyển sang một LM tốn kém hơn. Các xác suất quan sát được ước tính trực tiếp trên tập đào tạo, bằng cách tính kỳ vọng của xác suất kiểm chứng cho mỗi trạng thái, tức là P(o|s) = ∑sj,vj trong Dtrain 1{sj=s và vj=o} / ∑sj trong Dtrain 1{sj=s}. Tuy nhiên, vì các trạng thái có thể liên tục, chúng tôi sử dụng ước tính mật độ kernel Gaussian không tham số để ước tính xác suất quan sát cho một trạng thái mới. Thay vì ước tính trực tiếp P(o|s), chúng tôi đầu tiên học một phân phối kết hợp P(S, O) và P(S), bằng cách vẽ KDE tại mỗi điểm đào tạo. Sau đó xác suất có điều kiện P(o|s) được tính là: P(o|s) = P(s,o) / P(s).
Mục tiêu của POMDP của chúng tôi là tối ưu hóa một hàm phần thưởng R = P - lambda·C, nơi P là hiệu suất tổng thể, C là chi phí tổng thể, lambda là một tham số điều chỉnh cân bằng hai tiêu chí, theo sở thích của người dùng. Chúng tôi sử dụng bộ giải POMDP AdaOps [Wu et al., 2021], sử dụng lọc hạt để biểu diễn niềm tin, nơi mỗi hạt tương ứng với một trạng thái cụ thể. Tại suy luận, bộ giải POMDP bắt đầu với một niềm tin ban đầu (phân phối đồng nhất), cập nhật trạng thái niềm tin của nó dựa trên các quan sát của bộ kiểm chứng, và, dựa trên trạng thái niềm tin cập nhật b′, thực hiện một hành động để tối đa hóa phần thưởng mong đợi. Chúng tôi cung cấp thêm chi tiết và công thức POMDP hoàn chỉnh trong Phụ lục A.3.
5 Thí nghiệm
Thông qua các thí nghiệm của chúng tôi, chúng tôi muốn trả lời các câu hỏi nghiên cứu sau. AutoMix so sánh như thế nào với các chiến lược chuyển đổi mô hình khác? AutoMix hoạt động tốt như thế nào khi thay đổi (1) tỷ lệ chi phí giữa các mô hình, và (2) lượng dữ liệu đào tạo có sẵn? Theo công trình gần đây [Ding et al., 2024], chúng tôi thực hiện hầu hết các thí nghiệm trong thiết lập khi N = 2, nhưng cũng báo cáo
5

--- TRANG 6 ---
procedure ANSWER QUERY (C, q)
▷C: Ngữ cảnh, q: Câu hỏi, SLM /LLM:
Mô hình ngôn ngữ nhỏ/lớn
As←SOLVE (SLM,C, q)
v←SELF -VERIFY (As,C, q)
ifMETA -VERIFY (v,As,C, q)then
return As
else
Al←SOLVE (LLM,C, q)
return Al
end if
end procedure0 20 40 605055606570
Chi phíHiệu suấtSLM
LLM
Hình 3: Trái: Thuật toán AutoMix. Phải: Đường cong Hiệu suất vs. Chi phí. Độ dốc giữa SLM và LLM cung cấp một cách để đo Lợi ích Gia tăng trên mỗi Chi phí (IBC) cho các phương pháp trộn mô hình. Các phương pháp có độ dốc dốc hơn so với tham chiếu này khi được vẽ đồ thị so với SLM có IBC dương (vùng xanh lá), trong khi những phương pháp dưới tham chiếu có IBC âm (vùng đỏ).
kết quả bổ sung trong trường hợp ba mô hình (trong Mục 5.5). Đối với N = 2, chúng tôi sử dụng các thuật ngữ SLM và LLM để biểu thị các mô hình ngôn ngữ nhỏ (LM 1) và lớn (LM 2), tương ứng.
5.1 Một Metric cho Phân tích Hiệu quả Chi phí-Hiệu suất
Lợi ích Gia tăng trên mỗi Chi phí (IBC) Trong phương pháp của chúng tôi để tận dụng hiệu suất mô hình, điều quan trọng là không chỉ xem xét độ chính xác thô của các dự đoán mà còn cả chi phí tính toán hoặc tiền tệ liên quan. Để đạt được mục tiêu đó, chúng tôi giới thiệu một metric để hiểu hiệu suất của các mô hình về mặt chi phí. Chúng tôi giới thiệu các phương pháp, được ký hiệu bởi M, để tích hợp tối ưu SLM và LLM. Đối với mỗi phương pháp M, chúng tôi liên kết một chi phí CM và hiệu suất PM. Để định lượng tính hữu ích của M so với SLM, chúng tôi định nghĩa metric Lợi ích Gia tăng trên mỗi Chi phí (IBC) là IBCM (Phương trình (1)).
IBCM = (PM - PSLM) / (CM - CSLM), IBC BASE = (PLLM - PSLM) / (CLLM - CSLM), ΔIBC(M) = (IBCM - IBC BASE) / IBC BASE × 100
(1)
Metric IBC nắm bắt hiệu quả của việc tăng cường hiệu suất tương đối với chi phí bổ sung. Để đánh giá so sánh, chúng tôi đặt một IBC baseline, IBC BASE, đại diện cho lợi ích của việc luôn sử dụng LLM thay vì SLM. Cuối cùng, chúng tôi so sánh các phương pháp bằng cách sử dụng ΔIBC, so sánh IBC của một phương pháp cụ thể với IBC BASE. Một tăng IBC dương gợi ý rằng M đạt được tăng trưởng hiệu suất hiệu quả về chi phí hơn so với một LLM độc lập, trong khi một tăng âm chỉ ra hiệu quả giảm (Hình 3)
Giải thích Hình học Trên biểu đồ Hiệu suất vs. Chi phí, xem xét đoạn thẳng nối các điểm dữ liệu của mô hình ngôn ngữ nhỏ (SLM) và mô hình ngôn ngữ lớn (LLM). Độ dốc của đoạn này biểu thị tốc độ tăng hiệu suất trên mỗi đơn vị chi phí. Lợi ích Gia tăng trên mỗi Chi phí (IBC) cho bất kỳ phương pháp M nào là độ dốc của đường từ điểm SLM đến điểm đại diện cho M (Hình 3). Một phương pháp M nằm trên đoạn SLM-LLM cung cấp một độ dốc dốc hơn, chỉ ra một IBC thuận lợi (và một ΔIBC dương). Ngược lại, nếu M nằm dưới đoạn, nó gợi ý một IBC không thuận lợi hoặc âm. Mục tiêu chính của chúng tôi là phát triển các phương pháp mang lại IBC luôn dương, tối đa hóa các cải thiện hiệu suất cho mỗi đơn vị chi phí bổ sung.
5.2 Thiết lập
Mô hình và Tính toán Chi phí Chúng tôi sử dụng GPT-3.5, LLAMA 2-13B, và MISTRAL-7B (phiên bản instruct v0.2) [Touvron et al., 2023, Jiang et al., 2023a] làm các mô hình ngôn ngữ nhỏ hơn (SLM), và GPT-4 [OpenAI, 2023] làm mô hình ngôn ngữ lớn hơn (LLM). Đối với mỗi truy vấn đầu vào, chúng tôi mô hình hóa chi phí cố định cho các mô hình và kiểm chứng, được ký hiệu bởi CSLM cho các mô hình nhỏ hơn, CLLM cho mô hình lớn hơn, và Cver cho mô hình kiểm chứng. Vì kiểm chứng được thực hiện bởi CSLM và yếu tố chi phí chính là ngữ cảnh câu hỏi, vẫn giữ nguyên trong cả hai mô hình, chúng tôi đặt Cver = CSLM. Tại
6

--- TRANG 7 ---
0 50 100 150 2000.720.740.760.780.8DIPLOMAT
0 50 100 150 2000.550.650.750.850.95MUTUAL
0 50 100 150 2000.60.650.70.750.8QUALITY
0 50 100 150 2000.250.350.450.550.65COQA
0 50 100 150 2000.20.250.30.350.4QASPER
AutoMix +POMDP
AutoMix +Threshold
FrugalGPT [Chen et al., 2023]
HybridLLM [Ding et al., 2024]
LLAMA 2-13 B(SLM)
GPT-4 (LLM)
Random Mixing
Hình 4: Kết quả Chính: hiệu suất (trục y) vs. chi phí (trục x) cho các phương pháp khác nhau trên MISTRAL-7B/GPT-4 nhỏ và lớn. Meta-verifier dựa trên POMDP liên tục ở trên phép nội suy tuyến tính (trộn ngẫu nhiên) của SLM-LLM, biểu thị lợi ích gia tăng cao hơn trên mỗi đơn vị chi phí (IBC).
suy luận, tổng chi phí được tính là tổng các chi phí cá nhân, C = CSLM + Cver + w·CLLM, nơi w chỉ ra liệu mô hình lớn hơn có được gọi hay không.
Trong khi giá cả của các API này [Dehghani et al., 2021] bị ảnh hưởng bởi nhiều độ phức tạp khác nhau, việc tập trung vào việc sử dụng hộp đen của các mô hình ngôn ngữ dẫn chúng tôi đến việc biểu diễn chi phí dựa trên sự chênh lệch giá API thực tế giữa các mô hình này.2 Đối với mỗi GPT-3.5, LLAMA 2-13B, MISTRAL-7B, chúng tôi gán chi phí tương đối 1 đơn vị cho SLM và 60, 100, 200 đơn vị cho LLM tương ứng. Điều quan trọng cần lưu ý là tỷ lệ chi phí giữa các mô hình có thể thay đổi đáng kể tùy thuộc vào các tình huống triển khai cụ thể. Ví dụ, đối với người dùng có quyền truy cập vào một GPU A6000 duy nhất, chạy LLAMA 2-13B có thể không tốn chi phí gì, trong khi sử dụng GPT-4 có thể cực kỳ đắt đỏ. Chúng tôi mô phỏng tình huống này trong Mục 5.4. Chúng tôi giới thiệu độc giả đến Phụ lục B để biết thêm chi tiết.
Tập dữ liệu Chúng tôi thí nghiệm với một tập hợp đa dạng các tập dữ liệu: i) QASPER [Dasigi et al., 2021]: Hỏi đáp về các bài báo nghiên cứu; ii) QUALITY [Pang et al., 2022]: Câu hỏi trắc nghiệm (MCQ) về các bài viết và câu chuyện dài; iii) COQA [Reddy et al., 2019]: Hiểu đối thoại yêu cầu tham chiếu và suy luận thực dụng; iv) MUTUAL [Cui et al., 2020]: Suy luận đối thoại nhiều lượt (dự đoán phản hồi tiếp theo); v) DIPLOMAT [Li et al., 2023]: Các câu hỏi nhận dạng và suy luận thực dụng trên đối thoại nhiều lượt. Chúng tôi sử dụng điểm F1 cho QASPER và COQA, và độ chính xác cho các tập dữ liệu còn lại. Chúng tôi sử dụng các phần xác thực mặc định và sử dụng các prompt từ Shaham et al. [2023] cho QASPER và QUALITY, và điều chỉnh prompt QUALITY cho các tập dữ liệu khác. Chúng tôi sử dụng các prompt giống hệt nhau cho tất cả các mô hình. Chúng tôi giới thiệu độc giả đến Phụ lục B để biết thêm chi tiết.
Baseline Chúng tôi so sánh với FrugalGPT [Chen et al., 2023] và HybridLLM [Ding et al., 2024], hai mô hình hiện đại, làm baseline của chúng tôi. FrugalGPT sử dụng một mô hình DistillBert được tinh chỉnh [Sanh et al., 2019] làm bộ định tuyến. Nếu độ tin cậy của bộ định tuyến cho một câu hỏi, ngữ cảnh và câu trả lời SLM cho trước rơi xuống dưới một ngưỡng, truy vấn được định tuyến đến LLM. HybridLLM sử dụng DeBERTa được đào tạo [He et al., 2021] làm bộ định tuyến trực tiếp chọn giữa SLM và LLM mà không
2https://openai.com/pricing, https://together.ai/
7

--- TRANG 8 ---
Mô hình Phương pháp DIPLOMAT MUTUAL COQA QASPER QUALITY
MISTRAL-7BFrugalGPT 16.8 20.3 -16.7 7.7 8.1
HybridLLM 67.1 3.4 10.5 0.4 2.7
AutoMix + T 149.7 46.7 12.1 67.9 33.6
AutoMix + P 156.8 46.7 12.4 69.3 51.6
LLAMA 2-13 BFrugalGPT -7.0 -8.7 2.6 9.4 -4.7
HybridLLM 3.8 2.2 -0.5 7.2 6.5
AutoMix + T 50.1 11.8 86.5 -0.2 9.4
AutoMix + P 58.5 12.4 83.1 8.5 10.3
GPT-3.5FrugalGPT 30.1 11.1 37.0 87.2 10.1
HybridLLM 8.3 1.8 20.1 21.7 11.8
AutoMix + T 168.2 18.3 62.7 109.8 23.5
AutoMix + P 151.2 18.8 65.0 114.7 24.1
Bảng 1: Giá trị ΔIBC: AutoMix + T và AutoMix + P là các biến thể của phương pháp được đề xuất của chúng tôi với bộ định tuyến ngưỡng (T) và dựa trên POMDP, tương ứng. Số tốt nhất được in đậm, và tốt thứ hai được gạch dưới. AutoMix + POMDP thể hiện ΔIBC mạnh mẽ và nhất quán trên tất cả các tập dữ liệu, ngụ ý việc sử dụng tài nguyên tính toán khôn ngoan. Mặc dù đào tạo cụ thể theo miền và bộ kiểm chứng chi phí 0, FrugalGPT và HybridLLM kém hiệu suất hơn AutoMix trong hầu hết tất cả các tình huống.
tạo ra phản hồi SLM. Hơn nữa, nhãn đào tạo cho bộ định tuyến của HybridLLM được tạo ra dựa trên xác suất SLM vượt trội LLM bằng một lề, được tính bằng cách rút nhiều mẫu. Lưu ý, rằng đào tạo mỗi baseline này cùng với AutoMix yêu cầu chạy suy luận trên tất cả các mô hình, tuy nhiên, đó chỉ là chi phí thời gian đào tạo. Tuy nhiên, tại suy luận, khác với AutoMix, chúng tôi gán chi phí 0 cho các bộ định tuyến của hai baseline do chi phí vận hành thấp hơn của chúng.
5.3 Kết quả Chính
Hình 4 minh họa các đường cong hiệu suất versus chi phí cho nhiều tập dữ liệu và phương pháp trộn mô hình khác nhau sử dụng MISTRAL-7B như SLM. Trên tất cả các tập dữ liệu, AutoMix-POMDP và AutoMix-Threshold liên tục vượt trội FrugalGPT và HybridLLM, duy trì trên đường cong SLM-LLM và mang lại hiệu suất tốt hơn trên mỗi đơn vị chi phí. Điều này đặc biệt đáng chú ý vì cả FrugalGPT và HybridLLM đều tận dụng các bộ định tuyến được đào tạo cụ thể theo miền và không phát sinh chi phí kiểm chứng nào. Các xu hướng tương đương được quan sát với các SLM khác LLAMA 2-13B và GPT-3.5 trong Hình 18, 19.
Bảng 1 trình bày so sánh ΔIBC trung bình trên năm vùng chi phí có kích thước bằng nhau cho mỗi phương pháp, tập dữ liệu và ba SLM khác nhau. AutoMix-POMDP liên tục vượt trội FrugalGPT và HybridLLM trên tất cả các tập dữ liệu và mô hình, với hiệu suất bằng FrugalGPT trong tập dữ liệu qasper cho mô hình LLAMA 2-13B. Các tăng trưởng thay đổi trên các thiết lập khác nhau, với hiệu suất tốt nhất được quan sát trên tập dữ liệu DIPLOMAT, trung bình 122% trên tất cả các mô hình. Quan trọng, AutoMix-POMDP là phương pháp duy nhất mang lại tăng trưởng dương trên tất cả các cấu hình và liên tục bằng hoặc vượt hiệu suất của AutoMix-Threshold. Điều này mặc dù được kiểm tra trên nhiều tập dữ liệu và mô hình khác nhau đại diện cho các chi phí khác nhau, độ khó tác vụ (độ chính xác từ 30% đến 90%), khoảng cách hiệu suất giữa các mô hình (8% đến ~=50%), các loại câu trả lời khác nhau (MCQ, Mở). Kết quả cho thấy AutoMix, sử dụng tự kiểm chứng và định tuyến phù hợp, có thể hiệu quả trộn các SLM khác nhau và GPT-4 trên một loạt rộng các tác vụ mà không cần truy cập vào trọng số mô hình hoặc dữ liệu định tuyến cụ thể theo miền. Những tăng trưởng đáng kể này dịch trực tiếp thành tiết kiệm chi phí, nhấn mạnh sự liên quan kinh tế của phương pháp của chúng tôi.
5.4 Thí nghiệm Bổ sung
Ảnh hưởng của Tỷ lệ Chi phí lên AutoMix Các thí nghiệm chính của chúng tôi giả định tỷ lệ chi phí khác nhau từ 1:60 đến 1:200 cho các SLM khác nhau. Tiếp theo, chúng tôi tiến hành phân tích để hiểu làm thế nào các thay đổi trong tỷ lệ chi phí ảnh hưởng đến các giá trị ΔIBC trên các tỷ lệ chi phí khác nhau. Trong Hình 5 (phải), chúng tôi thay đổi tỷ lệ chi phí và báo cáo ΔIBC được chuẩn hóa bởi giá trị tối đa thu được trên tập dữ liệu (một giá trị cao hơn biểu thị tăng trưởng cao hơn). Kết quả gợi ý rằng ngay cả đối với tỷ lệ chi phí thấp như 1:10, AutoMix bắt đầu mang lại tăng trưởng tốt cho nhiều tập dữ liệu, tự nhiên cải thiện khi tỷ lệ chi phí trở nên lệch hơn.
8

--- TRANG 9 ---
/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000014/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000030/uni00000044/uni00000046/uni00000055/uni00000052/uni00000003/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000025/uni00000026/uni00000003/uni0000002f/uni0000004c/uni00000049/uni00000057
/uni00000024/uni00000058/uni00000057/uni00000052/uni00000050/uni0000004c/uni0000005b/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000033/uni00000032/uni00000030/uni00000027/uni00000033
/uni00000029/uni00000055/uni00000058/uni0000004a/uni00000044/uni0000004f
/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047/uni0000002f/uni0000002f/uni00000030
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000016
/uni00000026/uni00000052/uni00000056/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000025/uni00000026/uni00000003/uni0000002f/uni0000004c/uni00000049/uni00000057/uni00000027/uni0000004c/uni00000033/uni0000004f/uni00000052/uni00000050/uni00000044/uni00000057
/uni00000030/uni00000058/uni00000037/uni00000058/uni00000044/uni0000004f
/uni00000026/uni00000052/uni00000034/uni00000024
/uni00000034/uni00000024/uni00000036/uni00000033/uni00000028/uni00000035
/uni00000034/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005cHình 5: Trái: So sánh AutoMix với FrugalGPT và HybridLLM qua các kích thước dữ liệu đào tạo khác nhau cho thấy rằng mặc dù bộ kiểm chứng chi phí 0 và đào tạo cụ thể theo miền, các baseline kém hiệu suất hơn AutoMix. AutoMix xuất sắc trong các thiết lập dữ liệu hạn chế. Phải: ΔIBC được chuẩn hóa trên các tỷ lệ chi phí khác nhau. AutoMix cho thấy tăng trưởng hiệu suất mạnh mẽ ngay cả khi tỷ lệ chi phí thấp.
Chúng tôi bổ sung phân tích với các đường cong chi phí-hiệu suất cho các tỷ lệ chi phí khác nhau trong Phụ lục D.2. Chúng tôi lưu ý rằng chi phí không chỉ cần là chi phí tiền tệ, và có thể, nói chung, đại diện cho các cân nhắc khác như độ trễ hoặc sử dụng năng lượng. Những kết quả này chứng minh rằng AutoMix cung cấp một phương pháp mới để cân bằng sự cân bằng chi phí-hiệu suất trên một loạt rộng các tình huống chi phí và mạnh mẽ với những thay đổi mà không cần bất kỳ sửa đổi nào đối với phương pháp.
AutoMix Hiệu quả trong Các Tình huống Tài nguyên Hạn chế Hình 5 (trái) cho thấy động lực hiệu suất của AutoMix, FrugalGPT và HybridLLM với kích thước tập xác thực khác nhau. Đáng chú ý, phương pháp của chúng tôi vượt trội đáng kể FrugalGPT và HybridLLM với dữ liệu hạn chế, mặc dù phương pháp sau có đào tạo cụ thể theo miền và chi phí kiểm chứng bằng không. Ví dụ: chỉ với 50 ví dụ đào tạo, AutoMix duy trì 15% ΔIBC và đánh bại các baseline hơn 20% tuyệt đối. Điều này nhấn mạnh rằng AutoMix đặc biệt có lợi trong các tình huống thế giới thực nơi dữ liệu đào tạo khan hiếm.
Khi nào Định tuyến POMDP Giúp ích? POMDP ngầm phân loại độ khó câu hỏi thành ba loại: (a) các truy vấn dễ có thể giải quyết bởi mô hình nhỏ, (b) các truy vấn phức tạp chỉ các mô hình lớn hơn mới có thể giải quyết, và (c) các truy vấn có thể không giải được đối với bất kỳ mô hình nào. Nó tận dụng các xác suất tự kiểm chứng - cung cấp ước tính nhiễu về độ khó - để đưa ra quyết định phi tuyến. Ví dụ: trong tập dữ liệu QASPER sử dụng MISTRAL-7B như SLM, POMDP xác định rằng các giá trị độ tin cậy thấp hơn tương ứng với các trường hợp như vậy, và thay vì định tuyến đến LLM như các phương pháp khác làm, trả về câu trả lời SLM, tiết kiệm chi phí. Hơn nữa, trong các thiết lập với hơn hai mô hình (như được thảo luận trong Mục 5.5), POMDP đưa ra các quyết định tinh tế hơn, ví dụ bằng cách kết hợp thông tin từ các bộ kiểm chứng nhỏ và trung bình, dẫn đến hiệu suất vượt trội đáng kể.
Các Thí nghiệm Khác Chúng tôi thực hiện nhiều thí nghiệm khác, chi tiết được đưa vào phụ lục do hạn chế không gian. Chúng tôi đầu tiên đánh giá tự kiểm chứng few-shot định lượng và định tính và quan sát rằng tự kiểm chứng có thể hiệu quả sử dụng ngữ cảnh để xác định lỗi trong các câu trả lời được tạo bởi SLM trong nhiều trường hợp (xem Phụ lục A).
Trong khi AutoMix vượt trội trên tập hợp đa dạng các tập dữ liệu, để chứng minh thêm tính tổng quát hóa của bộ định tuyến của chúng tôi, chúng tôi đánh giá tổng quát hóa ngoài miền bằng cách đào tạo trên một tập dữ liệu và đánh giá trên các tập dữ liệu được giữ lại. Kết quả trong Phụ lục D.3 cho thấy AutoMix liên tục vượt trội FrugalGPT và HybridLLM với lề đáng kể. Nhìn chung, thiết kế không phụ thuộc tác vụ của POMDP và những kết quả này làm nổi bật tính tổng quát hóa của phương pháp của chúng tôi.
Chúng tôi cũng nghiên cứu chi phí độ trễ của AutoMix. Tại thời gian suy luận, POMDP mất ít hơn 1ms cho mỗi truy vấn trong thiết lập hai mô hình. Hơn nữa, độ trễ mạng (thường khoảng 10ms) ít hơn nhiều so với thời gian tạo giải pháp trung bình bởi SLM và LLM (theo thứ tự giây). Do đó, AutoMix không thêm chi phí tính toán hoặc độ trễ bổ sung so với suy luận mô hình ngôn ngữ. Cuối cùng, việc kết hợp bộ định tuyến POMDP cho người dùng cuối thuận tiện, vì thư viện của chúng tôi có thể được kết hợp trong ít hơn năm dòng mã.
9

--- TRANG 10 ---
5.5 Kết quả của Automix với Ba Mô hình
0 20 40 60 80 100
Chi phí Tính toán0.520.540.560.580.600.62Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
LLaMA-13B
LLaMA-70B
GPT-4
IBC13B/70B
IBC70B/GPT4
Hình 6: AutoMix với 3 mô hình: LLAMA 2-13B, GPT-4 và GPT-4. Phương pháp AutoMix cho thấy tăng IBC nhất quán cho cả vùng SLM-MLM và MLM-LLM. Hơn nữa, so với các baseline: FrugalGPT, chuỗi hai mô hình AutoMix hoặc sử dụng union của hai AutoMix, AutoMix 3 cung cấp cải thiện đáng kể.Chúng tôi đánh giá hiệu suất của AutoMix được áp dụng cho tình huống ba mô hình (N = 3). Cụ thể, chúng tôi sử dụng LLAMA 2-13B như SLM, LLAMA 2-70B như MLM, và GPT-4 như LLM. Kết quả được trình bày trong Hình 6. AutoMix liên tục vượt trội các baseline trên các vùng chi phí. Chúng tôi đầu tiên so sánh AutoMix với FrugalGPT, cho thấy cải thiện đáng kể trên tất cả các vùng chi phí được xem xét. Chúng tôi cũng so sánh AutoMix với một baseline, UnionAutoMix, chọn giữa AutoMix SLM−MLM và AutoMix MLM−LLM dựa trên yêu cầu chi phí của người dùng. Ví dụ: nếu chi phí trung bình mong muốn nhỏ hơn của MLM, AutoMix SLM−MLM được sử dụng; ngược lại, AutoMix MLM−LLM được sử dụng. Hơn nữa, chúng tôi xem xét một baseline, Chained AutoMix, bằng cách chuỗi AutoMix SLM−MLM với AutoMix MLM−LLM. Truy vấn đầu tiên đi đến SLM, và một AutoMix SLM−MLM quyết định giữa báo cáo câu trả lời SLM hoặc định tuyến đến MLM. Trong trường hợp sau, một AutoMix MLM−LLM thứ hai lặp lại quy trình sử dụng các mô hình MLM và LLM. Chained AutoMix kém hiệu suất trên toàn bộ, vì nó không thể định tuyến trực tiếp các truy vấn từ SLM đến LLM. Ngoài ra, bất cứ khi nào Chained AutoMix nhắc MLM, nó luôn sử dụng bộ kiểm chứng tốn kém, ngay cả khi nó có thể không cần thiết. Hơn nữa, trong Hình 9, 10, chúng tôi nghiên cứu hai trường hợp bổ sung: 1.) Nếu MLM kém hiệu suất hơn SLM, và bộ kiểm chứng của nó không cung cấp thông tin, thì khác với FrugalGPT, AutoMix học được sự không liên quan của MLM và định tuyến trực tiếp từ SLM đến LLM khi cần; 2.) Trong các tình huống nơi MLM kém hiệu suất hơn SLM, nhưng bộ kiểm chứng của nó cung cấp thông tin hữu ích, AutoMix tận dụng thông tin từ bộ kiểm chứng của MLM để vượt trội tất cả các baseline được xem xét. Các thí nghiệm cho thấy hiệu quả của AutoMix trong việc cung cấp hiệu suất tối ưu trên các tình huống đa dạng mà không cần can thiệp bổ sung. Chúng tôi giới thiệu độc giả đến Phụ lục C để biết thêm chi tiết.
6 Kết luận
AutoMix tích hợp nhiều API LLM hộp đen vào một khung giải quyết vấn đề nhiều bước, tối ưu hóa sự cân bằng chi phí tính toán và hiệu suất. Công việc của chúng tôi kết hợp các phương pháp Trí tuệ Nhân tạo Cũ Tốt (GOFAI) với LLM, chứng minh rằng việc kết hợp POMDP có thể dẫn đến định tuyến hiệu quả giữa các LLM. Công việc của chúng tôi cung cấp một phương pháp mới để cân bằng sự cân bằng chi phí-hiệu suất trên nhiều mô hình khác nhau trên các phạm vi chi phí mong muốn khác nhau, liên tục vượt trội các baseline với lề đáng kể. AutoMix mở ra các hướng cho một số hướng nghiên cứu thú vị. Trong khi tự kiểm chứng và sửa chữa là thách thức đối với LLM nói chung, chúng tôi thấy kết quả hứa hẹn sử dụng kiểm chứng dựa trên ngữ cảnh few-shot kết hợp với POMDP, chỉ ra rằng các phương pháp tương tự cũng có thể giúp các tình huống khác. Công việc có thể được mở rộng cho các tác vụ tạo, nơi khái niệm hiệu suất là chủ quan.
Hạn chế
Trong khi bằng chứng thực nghiệm của chúng tôi trên nhiều mô hình và tập dữ liệu khác nhau chứng minh hiệu quả của AutoMix, tính ứng dụng rộng hơn của nó có thể thay đổi tùy thuộc vào các mô hình và tập dữ liệu cụ thể được sử dụng. Hơn nữa, AutoMix được thiết kế với thiết lập suy luận dựa trên đối thoại hoặc ngữ cảnh trong đầu để tự kiểm chứng hiệu quả, không bao gồm các tác vụ như hỏi đáp sự kiện và suy luận thường thức. Trong tương lai, khi các mô hình mã nguồn mở trở nên mạnh mẽ hơn và chi phí suy luận giảm, có thể khả thi để phục vụ một mô hình mạnh cho tất cả các truy vấn. Tuy nhiên, có khả năng vẫn có các sự cân bằng về tính khả dụng có thể được quản lý bằng AutoMix.
10

--- TRANG 11 ---
7 Lời cảm ơn
Công trình này được hỗ trợ bởi grant IBM AI Horizons Network, các grant từ Google và Microsoft, một giải thưởng IBM SUR, và fellowship ghế Jai Gupta bởi IIT Delhi. Chúng tôi cảm ơn cơ sở HPC IIT Delhi cho tài nguyên tính toán của nó. Chúng tôi biết ơn Microsoft AFMR vì đã hỗ trợ công việc này. Chúng tôi cũng cảm ơn Kalpesh Krishna, Prakhar Gupta, Rahul Gupta, Siddharth Gopal, và Yang Song cho phản hồi có giá trị của họ.
Tài liệu tham khảo
Pranjal Aggarwal, Aman Madaan, Yiming Yang, và Mausam. Let's sample step by step: Adaptive-consistency for efficient reasoning and coding with LLMs. Trong Houda Bouamor, Juan Pino, và Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, trang 12375-12396, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.761. URL https://aclanthology.org/2023.emnlp-main.761.
Lingjiao Chen, Matei A. Zaharia, và James Y. Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. ArXiv, abs/2305.05176, 2023. URL https://api.semanticscholar.org/CorpusID:258564349.
Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, và Ming Zhou. Mutual: A dataset for multi-turn dialogue reasoning. ArXiv, abs/2004.04494, 2020. URL https://api.semanticscholar.org/CorpusID:215548215.
Ido Dagan, Oren Glickman, và Bernardo Magnini. The pascal recognising textual entailment challenge. Trong Machine learning challenges workshop, trang 177-190. Springer, 2005.
Ido Dagan, Dan Roth, Fabio Zanzotto, và Mark Sammons. Recognizing textual entailment: Models and applications. Springer Nature, 2022.
Peng Dai, Mausam, và Daniel S. Weld. Decision-theoretic control of crowd-sourced workflows. Proceedings of the AAAI Conference on Artificial Intelligence, 2010. URL https://api.semanticscholar.org/CorpusID:8730278.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, và Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 4599-4610, 2021.
Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, và Ashish Vaswani. The efficiency misnomer. Trong International Conference on Learning Representations, 2021.
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, và Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023.
Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks V. S. Lakshmanan, và Ahmed Hassan Awadallah. Hybrid LLM: Cost-efficient and quality-aware query routing. Trong The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=02f3mUtqnM.
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. Trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 16477-16508, 2023.
11

--- TRANG 12 ---
Shijie Geng, Peng Gao, Zuohui Fu, và Yongfeng Zhang. Romebert: Robust training of multi-exit bert. arXiv preprint arXiv:2101.09755, 2021.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, và Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention, 2021.
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, và Denny Zhou. Large language models cannot self-correct reasoning yet. ArXiv, abs/2310.01798, 2023a. URL https://api.semanticscholar.org/CorpusID:263609132.
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, và Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023b.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, và William El Sayed. Mistral 7b, 2023a.
Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, và James T Kwok. Backward reasoning in large language models for verification. arXiv preprint arXiv:2308.07758, 2023b.
Leslie Pack Kaelbling, Michael L. Littman, và Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1):99-134, 1998. ISSN 0004-3702. doi: https://doi.org/10.1016/S0004-3702(98)00023-X. URL https://www.sciencedirect.com/science/article/pii/S000437029800023X.
Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, và Ion Stoica. Efficient memory management for large language model serving with pagedattention. Trong Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.
Hengli Li, Songchun Zhu, và Zilong Zheng. Diplomat: A dialogue dataset for situated pragmatic reasoning. ArXiv, abs/2306.09030, 2023. URL https://api.semanticscholar.org/CorpusID:259164643.
Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, và Qi Ju. Fastbert: a self-distilling bert with adaptive inference time. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 6035-6044, 2020.
Aman Madaan và Yiming Yang. Flowgen: Fast and slow graph generation. arXiv preprint arXiv:2207.07656, 2022.
Aman Madaan, Niket Tandon, Peter Clark, và Yiming Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. Trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, trang 2833-2861, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.183.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
Nicolas Meuleau, Leonid Peshkin, Kee-Eung Kim, và Leslie Pack Kaelbling. Learning finite-state controllers for partially observable environments, 2013.
12

--- TRANG 13 ---
OpenAI. Gpt-4 technical report, 2023.
Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, và William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023a.
Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, và Preslav Nakov. Fact-checking complex claims with program-guided reasoning. arXiv preprint arXiv:2305.12744, 2023b.
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, et al. Quality: Question answering with long input texts, yes! Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 5336-5358, 2022.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.
Adam Poliak. A survey on recognizing textual entailment as an nlp evaluation. arXiv preprint arXiv:2010.03061, 2020.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, và Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.
Guillem Ramírez, Alexandra Birch, và Ivan Titov. Optimising calls to large language models with uncertainty-based two-tier selection, 2024.
Siva Reddy, Danqi Chen, và Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266, 2019.
Nils Reimers và Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.
Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, và Donald Metzler. Confident adaptive language modeling. arXiv preprint arXiv:2207.07061, 2022.
Wilko Schwarting, Javier Alonso-Mora, và Daniela Rus. Planning and decision-making for autonomous vehicles. Annu. Rev. Control. Robotics Auton. Syst., 1:187-210, 2018. URL https://api.semanticscholar.org/CorpusID:64853900.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, và Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. Trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, trang 12007-12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, và Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
13

--- TRANG 14 ---
Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, và Victor Cărbune. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516, 2023.
Marija vSakota, Maxime Peyrard, và Robert West. Fly-swat or cannon? cost-effective language model choice via meta-modeling. ArXiv, abs/2308.06077, 2023. URL https://api.semanticscholar.org/CorpusID:260866096.
Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, và Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022.
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, và Jun Zhao. Large language models are better reasoners with self-verification. CoRR, abs/2212.09561, 2023.
Chenyang Wu, Guoyu Yang, Zongzhang Zhang, Yang Yu, Dong Li, Wulong Liu, và Jianye HAO. Adaptive online packing-guided search for POMDPs. Trong A. Beygelzimer, Y. Dauphin, P. Liang, và J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=0zvTBoQb5PA.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, và Yuan Cao. React: Synergizing reasoning and acting in language models. Trong The Eleventh International Conference on Learning Representations, 2022.
Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, và Graham Neubig. Docprompting: Generating code by retrieving the docs. Trong The Eleventh International Conference on Learning Representations, 2022.
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, và Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33: 18330-18341, 2020.
Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark W. Barrett, Michael I. Jordan, và Jiantao Jiao. On optimal caching and model multiplexing for large model inference. ArXiv, abs/2306.02003, 2023. URL https://api.semanticscholar.org/CorpusID:259075212.
K.J Åström. Optimal control of markov processes with incomplete state information. Journal of Mathematical Analysis and Applications, 10(1):174-205, 1965. ISSN 0022-247X. doi: https://doi.org/10.1016/0022-247X(65)90154-X. URL https://www.sciencedirect.com/science/article/pii/0022247X6590154X.
14

--- TRANG 15 ---
A Tự kiểm chứng few-shot hiệu quả như thế nào?
A.1 Phân tích Định lượng
Một đóng góp đáng chú ý của công trình này là khái niệm tự kiểm chứng few-shot của các đầu ra. Tự kiểm chứng, đặc biệt cho các vấn đề suy luận, đặt ra những thách thức riêng; tuy nhiên, thiết lập của chúng tôi có một lợi thế độc đáo: khả năng sử dụng ngữ cảnh để xác thực câu trả lời. Ví dụ: mô hình có thể xác định các sai lệch sự kiện trong câu trả lời hoặc phân biệt những mâu thuẫn rõ ràng có thể không rõ ràng trong phản hồi ban đầu. Nhưng liệu lợi thế này có dịch thành tự kiểm chứng hiệu quả trong thực tế không? Như được mô tả trong Hình 12, cho tất cả trừ một lần tự kiểm chứng few-shot thành công trong việc xác định chính xác các ví dụ đúng bằng cách gán cho chúng xác suất cao hơn trên tất cả các tập dữ liệu khác. Cụ thể, trục y trên biểu đồ biểu thị độ chính xác của việc xác định các phản hồi đúng (được định nghĩa là F1 > 0.1). Đối với tất cả trừ QUALITY và QASPER xác suất kiểm chứng cao hơn, luôn có độ chính xác cao hơn. Ngay cả đối với QASPER, trừ 2/9 ngoại lệ, xu hướng thường đúng.
A.2 Phân tích Định tính
Các ví dụ đại diện từ phân tích định tính của chúng tôi được hiển thị trong Bảng 2, 3, và 4.
Thuộc tính Nội dung
Ngữ cảnh Quốc gia nào trồng nhiều trà nhất? ... Shen nhấp nó. Anh thích nó. Anh uống hết. Shen tự hào về thức uống mới của mình. Rút gọn để tóm tắt
Câu hỏi Anh ấy uống nó khi nào?
Câu trả lời được tạo Anh ấy uống nó vào năm 1990.
Câu trả lời Vàng khoảng năm 2737 TCN
Đầu ra Bộ kiểm chứng Ngữ cảnh không đề cập rằng anh ấy uống trà vào năm 1990. Quyết định Kiểm chứng: Câu trả lời được AI tạo là Không chính xác.
Bảng 2: Một ví dụ nơi câu trả lời được tạo có thể kiểm chứng là không chính xác, trích dẫn một ngày không được hỗ trợ (1990), và bộ kiểm chứng thành công nắm bắt sự khác biệt này.
Thuộc tính Nội dung
Ngữ cảnh Fort Lauderdale, Florida (CNN) - Chỉ nhấp một ngụm nước hoặc đi đến phòng tắm là cực kỳ đau đớn đối với Michael Brewer, 15 tuổi, ... Anh bị bỏng cấp độ hai và ba trên khoảng hai phần ba cơ thể, theo giám đốc liên kết của bệnh viện, Tiến sĩ Carl Schulman. Rút gọn để tóm tắt
Câu hỏi Bao lâu trước khi anh ấy hồi phục khỏi chấn thương của mình?
Câu trả lời được tạo anh ấy sẽ không bao giờ hồi phục khỏi chấn thương của mình.
Câu trả lời Vàng Anh ấy có một sự hồi phục suốt đời.
Đầu ra Bộ kiểm chứng Ngữ cảnh rõ ràng nói rằng thiếu niên đang hồi phục từ chấn thương của mình. Quyết định Kiểm chứng: Câu trả lời được AI tạo là Không chính xác.
Bảng 3: Ví dụ thể hiện một trường hợp thách thức cho bộ kiểm chứng. Câu trả lời được tạo là một cách diễn đạt lại của những gì được đề cập trong ngữ cảnh, tuy nhiên bộ kiểm chứng thất bại trong việc nắm bắt nó.
FrugalGPT vs. AutoMix ở các mức độ khả năng dữ liệu khác nhau Hình 7 chứng minh động lực hiệu suất của AutoMix và FrugalGPT với các kích thước dữ liệu xác thực khác nhau. Đáng chú ý, phương pháp của chúng tôi vượt trội đáng kể FrugalGPT với dữ liệu hạn chế, mặc dù phương pháp sau có đào tạo cụ thể theo miền và chi phí kiểm chứng bằng không. Tuy nhiên, khi dữ liệu đào tạo tăng, FrugalGPT thu hẹp
15

--- TRANG 16 ---
Thuộc tính Nội dung
Ngữ cảnh Cắt tỉa giữ cây chỉ cao ba hoặc bốn feet. Đây là chiều cao dễ dàng để hái trà. Trà xanh được hấp ngay sau khi lá được hái. Trà xanh _ những phẩm chất lành mạnh của nó. Ví dụ: nó có thể ngăn ngừa bệnh tim. Làm thế nào chúng ta có túi trà? Câu trả lời: bằng tai nạn. Rút gọn
Câu hỏi Họ đã làm gì với trà xanh sau khi hái?
Câu trả lời SLM hấp.
Câu trả lời Vàng cắt tỉa nó
Đầu ra Bộ kiểm chứng Ngữ cảnh đề cập rằng trà xanh được hấp sau khi được hái. Quyết định Kiểm chứng: Câu trả lời được AI tạo là Chính xác.
Bảng 4: Một trường hợp nơi bộ kiểm chứng coi câu trả lời là chính xác, và nhãn vàng không chính xác. Bộ kiểm chứng có thể suy luận chính xác rằng ngữ cảnh đề cập đến trà được hấp sau khi hái.
khoảng cách hiệu suất bằng cách tận dụng đào tạo cụ thể theo miền của nó. Mô hình này chỉ ra rằng AutoMix cung cấp một giải pháp đặc biệt có lợi trong các tình huống thế giới thực nơi dữ liệu có thể khan hiếm.
/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000014/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000030/uni00000044/uni00000046/uni00000055/uni00000052/uni00000003/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000025/uni00000026/uni00000003/uni0000002f/uni0000004c/uni00000049/uni00000057
/uni00000024/uni00000058/uni00000057/uni00000052/uni00000050/uni0000004c/uni0000005b/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000033/uni00000032/uni00000030/uni00000027/uni00000033
/uni00000029/uni00000055/uni00000058/uni0000004a/uni00000044/uni0000004f
/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047/uni0000002f/uni0000002f/uni00000030
Hình 7: So sánh AutoMix với FrugalGPT qua Kích thước Tập dữ liệu Đào tạo Khác nhau. Mặc dù bộ kiểm chứng chi phí bằng không và đào tạo cụ thể theo miền, FrugalGPT kém hiệu suất hơn AutoMix. AutoMix đặc biệt hữu ích cho các thiết lập dữ liệu hạn chế, với tăng trưởng cao hơn có thể thấy khi kích thước tập dữ liệu nhỏ hơn 1000.
A.3 Bộ định tuyến dựa trên POMDP trong AutoMix
Một bộ định tuyến nên hướng một truy vấn đến các Mô hình Ngôn ngữ (LM) lớn hơn chỉ khi khoảng cách hiệu suất biện minh cho sự cân bằng chi phí-chất lượng. Cho sự không chắc chắn nội tại trong trạng thái thực của hiệu suất hệ thống, vẫn không được quan sát, chúng tôi xây dựng bộ định tuyến như một Quy trình Quyết định Markov Quan sát Một phần (POMDP) [Åström, 1965]. POMDP đặc biệt phù hợp cho các tình huống nơi các quan sát, chẳng hạn như xác suất tự kiểm chứng, có thể không hoàn toàn đáng tin cậy.
Nhớ lại rằng (Mục 2) một POMDP được đặc trưng bởi bộ (S, A, T, R, Ω, O). Trong ứng dụng của chúng tôi, chúng tôi định nghĩa các thành phần này như sau:
16

--- TRANG 17 ---
Trạng thái (S): Không gian trạng thái đại diện cho LMi hiện tại được chọn (giai đoạn) và các metric hiệu suất (ví dụ: độ chính xác hoặc F-score) của nhiều LM trên một điểm dữ liệu. Chúng tôi ký hiệu điều này là S = ⟨i, Perf LM 1, Perf LM 2, . . . , Perf LMN⟩, nơi i là chỉ số của LM hiện tại và Perf LMj là hiệu suất của LM thứ j. Trạng thái cuối được biểu diễn bởi S = ⟨N, . . .⟩, nơi N là tổng số LM.
Hành động (A): Không gian hành động bao gồm việc giữ lại câu trả lời của LM hiện tại (a = LMi) hoặc định tuyến đến một trong các LM lớn hơn (được ký hiệu bởi Route j, j > i). Đối với tất cả LM trừ LMN, định tuyến đến LMj ngụ ý thực hiện cả tạo câu trả lời và tự kiểm chứng.
Hàm Chuyển tiếp (T): Các xác suất chuyển tiếp là xác định vì hiệu suất thực của các mô hình ngôn ngữ trên một câu hỏi là tĩnh, mặc dù không biết. Tuy nhiên, chiều đầu tiên trong vector trạng thái, đại diện cho LMi hiện tại, thay đổi thành j nếu hành động Route j được thực hiện hoặc thành N nếu câu trả lời của LM hiện tại được giữ lại. Vì trạng thái giai đoạn luôn tăng và được giới hạn bởi N, vấn đề của chúng tôi có một chân trời hữu hạn.
Hàm Phần thưởng (R): Mục tiêu của POMDP của chúng tôi là tối ưu hóa một hàm phần thưởng R = P - lambda·C, nơi P là hiệu suất tổng thể, C là chi phí tổng thể, và lambda là một tham số điều chỉnh cân bằng hai tiêu chí này theo sở thích của người dùng.
Quan sát (Ω): Các quan sát đến dưới dạng đầu ra bộ kiểm chứng v từ LMi, cho phép POMDP xác định trạng thái niềm tin b, là một phân phối xác suất trên S.
Hàm Quan sát (O): Các xác suất quan sát P(o|s) chỉ ra khả năng quan sát o (đầu ra kiểm chứng) cho trạng thái s. Những xác suất này rất quan trọng để định nghĩa mô hình POMDP. Ví dụ: độ tin cậy cao của bộ kiểm chứng có thể gợi ý rằng hiệu suất của LM hiện tại Perf LMi đủ cao, giảm tính cần thiết chuyển sang một LM tốn kém hơn.
Để ước tính các xác suất quan sát, chúng tôi đầu tiên xem xét trường hợp rời rạc. Trong tình huống này, chúng tôi có thể tính trực tiếp P(o|s) bằng cách sử dụng:
P(o|s) = ∑(sj,vj) trong D 1{sj=s và vj=o} / ∑sj trong D 1{sj=s}
nơi D đại diện cho tập hợp các cặp trạng thái-quan sát từ dữ liệu đào tạo.
Tuy nhiên, trong không gian trạng thái liên tục của chúng tôi, chúng tôi cần điều chỉnh phương pháp này. Chúng tôi sử dụng Ước tính Mật độ Kernel (KDE), một phương pháp không tham số để ước tính các hàm mật độ xác suất. Thay vì ước tính trực tiếp P(o|s), chúng tôi đầu tiên học một phân phối kết hợp P(S, O) bằng cách sử dụng KDE. Để làm điều này, chúng tôi vẽ một KDE tại mỗi điểm đào tạo:
ˆfh(x) = 1/(nhn) ∑i=1 đến n K((x-Xi)/h)
nơi K là hàm kernel Gaussian, h là băng thông, và Xi là các điểm mẫu.
Xác suất có điều kiện P(o|s) sau đó được tính là:
P(o|s) = P(s, o) / P(s)
nơi P(s) được tính tương tự như P(s, o) Những xác suất quan sát này được ước tính trực tiếp trên tập đào tạo bằng cách vẽ KDE cho mỗi điểm đào tạo.
Bằng cách tận dụng công thức POMDP này và mô hình quan sát dựa trên KDE, bộ định tuyến của chúng tôi có thể đưa ra quyết định thông tin về khi nào định tuyến các truy vấn đến các mô hình ngôn ngữ lớn hơn, hiệu quả cân bằng các cân nhắc hiệu suất và chi phí trong bối cảnh không chắc chắn.
A.3.1 Giải POMDP
Sau khi tất cả các thành phần khác nhau của POMDP của chúng tôi được định nghĩa như được hiển thị ở trên, việc tìm một chính sách tối ưu yêu cầu một bộ giải phù hợp. Một chính sách tối ưu trong bối cảnh này là một chiến lược pi*: B → A,
17

--- TRANG 18 ---
nơi B là tập hợp tất cả các trạng thái niềm tin có thể tối đa hóa phần thưởng tích lũy chiết khấu mong đợi. Tuy nhiên, công thức của chúng tôi trình bày những thách thức và cơ hội độc đáo mà các thuật toán POMDP truyền thống khó giải quyết hiệu quả. POMDP của chúng tôi mô hình một không gian trạng thái liên tục trong một không gian (N+ 1) chiều, nơi N là số lượng mô hình ngôn ngữ. Bản chất liên tục này đặt ra khó khăn cho nhiều thuật toán POMDP truyền thống, chẳng hạn như các phương pháp lặp giá trị dựa trên điểm, thường được thiết kế cho các không gian trạng thái rời rạc và có thể trở nên không thể xử lý về mặt tính toán trong các miền liên tục.
Tuy nhiên, công thức cụ thể của chúng tôi có một số đặc điểm mong muốn phân biệt nó với các POMDP truyền thống: 1. Chân trời hữu hạn và nhỏ: Vấn đề của chúng tôi có số lượng giai đoạn quyết định hạn chế (tối đa = N), giới hạn độ sâu của bất kỳ cây quyết định nào chúng tôi cần khám phá. 2. Số lượng quan sát hữu hạn và nhỏ: Các đầu ra bộ kiểm chứng, phục vụ như các quan sát của chúng tôi, là rời rạc và hạn chế số lượng (9), cho phép liệt kê tất cả các chuỗi quan sát có thể. 3. Cấu trúc Đồ thị Có hướng Không chu kỳ (DAG): Trong công thức của chúng tôi, một trạng thái cụ thể chỉ gặp một lần, và vấn đề tiến triển đơn hướng qua các giai đoạn.
Những đặc điểm này cho phép chúng tôi chi tiết một phương pháp giải pháp được điều chỉnh và hiệu quả hơn. Chúng tôi đầu tiên bắt đầu bằng cách trình bày một thuật toán đơn giản nhưng hiệu quả để giải quyết POMDP của chúng tôi, tận dụng những thuộc tính này để đạt được tốc độ và độ chính xác phù hợp. Phương pháp kết hợp lọc hạt để biểu diễn trạng thái niềm tin với một chiến lược tìm kiếm dựa trên cây, như được chi tiết trong phần tiếp theo.
Bộ giải POMDP Chúng tôi bắt đầu với mô tả của một bộ giải POMDP đơn giản để hiểu trực giác đằng sau cách giải quyết POMDP với công thức được đề xuất của chúng tôi. Do không gian trạng thái liên tục đa chiều, chúng tôi không thể biểu diễn tất cả các trạng thái niềm tin có thể một cách rõ ràng. Một phương pháp phổ biến để xấp xỉ các trạng thái niềm tin trong những trường hợp như vậy là sử dụng bộ lọc hạt.
Cụ thể, mỗi hạt đại diện cho một trạng thái có thể của hệ thống, và trạng thái niềm tin được biểu diễn như một tập hợp các hạt như vậy. Về mặt toán học, nếu chúng tôi ký hiệu tập hợp các hạt bởi {s(i)}M i=1, nơi M là số lượng hạt, trạng thái niềm tin b tại bất kỳ thời điểm nào được xấp xỉ là
b(s) ~= 1/M ∑i=1 đến M delta(s−s(i))
nơi delta là hàm delta Dirac.
Để giải quyết POMDP của chúng tôi, chúng tôi xây dựng một cây quyết định đại diện cho các chuỗi hành động và quan sát có thể. Cây này bao gồm hai loại nút: nút hành động, đại diện cho việc lựa chọn một mô hình ngôn ngữ, và nút quan sát, đại diện cho các đầu ra bộ kiểm chứng có thể sau một hành động. Sau mỗi nút quan sát, chúng tôi cập nhật các hạt niềm tin bằng cách sử dụng các xác suất quan sát, tinh chỉnh ước tính trạng thái của hệ thống.
Giả định rằng chúng tôi có thể khám phá và mở rộng tất cả các nút cho các cây nhỏ, chúng tôi cần tính giá trị của mỗi nút. Chúng tôi có thể tính phần thưởng trực tiếp từ mô hình phần thưởng cho các nút cuối. Đối với bất kỳ nút nào khác, giá trị V(b) được định nghĩa là phần thưởng mong đợi của hành động tốt nhất, có thể được tính đệ quy bằng cách sử dụng:
V(b) = max a [R(b, a) + gamma ∑o P(o|b, a)V(b′)]
nơi b′ là trạng thái niềm tin cập nhật sau khi quan sát o theo hành động a, và gamma là yếu tố chiết khấu được đặt thành 1 trong trường hợp của chúng tôi.
Một bước cuối cùng vẫn còn: cách cập nhật các trạng thái niềm tin. Điều này được thực hiện bằng cách sử dụng lọc hạt, như được mô tả tiếp theo. Trong lọc hạt, các trạng thái niềm tin được cập nhật bằng cách lấy mẫu từ trạng thái niềm tin hiện tại dựa trên các xác suất quan sát.
Về mặt toán học, các cập nhật lọc hạt không trọng số có thể được mô tả như sau. Cho một tập hợp các hạt {s(i) t}M i=1 đại diện cho trạng thái niềm tin tại thời điểm t, và một quan sát ot, tập hợp các hạt cập nhật {s(i) t+1}M i=1 được thu được bằng:
1. Đối với mỗi hạt s(i) t, lấy mẫu một trạng thái mới s(i) t+1 theo mô hình chuyển tiếp P(st+1| s(i) t, at).
18

--- TRANG 19 ---
2. Gán trọng số cho mỗi hạt mới bằng xác suất quan sát P(ot|s(i) t+1).
3. Lấy mẫu lại M hạt từ tập hợp có trọng số để tạo thành một tập hợp hạt không trọng số.
Thuật toán có thể được tóm tắt như sau:
Thuật toán 1 Cập nhật Lọc Hạt Không trọng số
1:Đầu vào: Các hạt hiện tại {s(i) t}M i=1, hành động at, quan sát ot
2:Đầu ra: Các hạt cập nhật {s(i) t+1}M i=1
3:foreach hạt s(i) t do
4: Lấy mẫu s(i) t+1∼P(st+1|s(i) t, at)
5: Tính trọng số wi=P(ot|s(i) t+1)
6:end for
7:Lấy mẫu lại M hạt từ {s(i) t+1, wi}M i=1 để thu được các hạt không trọng số {s(i) t+1}M i=1
Cho vấn đề chân trời nhỏ và hữu hạn của chúng tôi, chúng tôi đặt yếu tố chiết khấu là 1. Cuối cùng, chúng tôi có thể mô phỏng tất cả các tình huống có thể và học các chính sách xác định ngoại tuyến vì số lượng chuỗi quan sát hữu hạn. Do đó, POMDP được giảm thành một bảng tra cứu đơn giản tại suy luận, và chi phí suy luận là không đáng kể.
Sử dụng các bộ giải POMDP tốt hơn
Lưu ý rằng thiết kế bộ giải đơn giản được giới thiệu trong phần trước đủ và đủ nhanh cho mục đích của chúng tôi. Tuy nhiên, để xử lý các tình huống phức tạp hơn trong các công việc tương lai, chúng tôi sử dụng một bộ giải POMDP AdaOps tiên tiến và gần đây được giới thiệu [Wu et al., 2021]. AdaOps, viết tắt của Adaptive Online Packing-guided Search for POMDPs, cung cấp một số lợi thế, chẳng hạn như xử lý hiệu quả các không gian trạng thái lớn hơn và tăng khả năng mở rộng thông qua các kỹ thuật tìm kiếm trực tuyến thích ứng. Ngoài ra, nó kết hợp các chiến lược khám phá có hướng dẫn đóng gói để tập trung tài nguyên tính toán vào những phần hứa hẹn nhất của không gian niềm tin, dẫn đến ước tính chính sách chính xác hơn trong các môi trường phức tạp hoặc nơi tìm kiếm và không gian trạng thái lớn. Tuy nhiên, cho công thức đơn giản hơn của chúng tôi, cả AdaOps và thuật toán đơn giản sẽ xấp xỉ hội tụ đến cùng giải pháp khi số lượng hạt được sử dụng trong biểu diễn trạng thái niềm tin tăng.
B Chi tiết Bổ sung về Thiết lập Thí nghiệm
Để đánh giá, chúng tôi sử dụng các tập xác thực từ Shaham et al. [2022] cho QASPER và QUALITY, và sử dụng các prompt từ Shaham et al. [2023]. Đối với COQA, MUTUAL, và DIPLOMAT, chúng tôi sử dụng phần xác thực của nó và điều chỉnh prompt QUALITY. Để nhất quán, 1000 trường hợp được lấy mẫu từ tập xác thực của mỗi tập dữ liệu. Bất kể tập dữ liệu, các prompt đầu vào giống hệt nhau được gửi đến cả SLM và có thể LLM, đảm bảo chi phí xử lý đầu vào nhất quán. Độ dài đầu ra được cố định trong các tập dữ liệu trắc nghiệm như CNLI và QUALITY, và tính ngắn gọn của các phản hồi trong các tập dữ liệu khác cho phép chúng tôi giả định chi phí xử lý đầu ra đồng nhất. Chúng tôi sử dụng giải mã tham lam (nhiệt độ 0) và rút một mẫu duy nhất cho cả SLM và LLM. Để kiểm chứng, chúng tôi tạo tám mẫu cho mỗi câu hỏi (nhiệt độ = 1), có chi phí không đáng kể do ngữ cảnh lớn. Trong Hình 6, chúng tôi chuẩn hóa ΔIBC bằng một yếu tố tỷ lệ sao cho đối với tất cả các tập dữ liệu, tối đa được đặt thành 1.
Để chạy các thí nghiệm của chúng tôi, chúng tôi sử dụng các mô hình LLAMA 2-13B và GPT-4 từ huggingface3. Chúng tôi sử dụng vllm [Kwon et al., 2023] để lưu trữ các mô hình để suy luận.
Tỷ lệ Chi phí: Chúng tôi đã xem xét tỷ lệ chi phí 1:100 giữa GPT-4 và GPT-4, phản ánh sự chênh lệch giá API giữa các mô hình, đứng ở $0.225 cho LLAMA 2-13B so với $30 cho GPT-4 mỗi 1M token tại thời điểm viết. Ngoài ra, cho mục đích tự kiểm chứng, chúng tôi tạo 8 mẫu. Tuy nhiên, điều quan trọng cần lưu ý là chi phí tạo 8 mẫu là không đáng kể so với chi phí của một mẫu duy nhất, chủ yếu vì yếu tố chi phí chính là độ dài của ngữ cảnh
3Các mô hình có sẵn tại: https://huggingface.co/meta-llama/Llama-2-13b-hf và https://huggingface.co/meta-llama/Llama-2-70b-hf
19

--- TRANG 20 ---
(ví dụ: tạo nhỏ hơn 60 lần và 50 lần so với ngữ cảnh cơ sở cho QASPER và QUALITY, tương ứng). Do đó, việc gọi bộ kiểm chứng 8 lần được coi là tương đương về chi phí với việc gọi nó một lần. Hơn nữa, trong Mục 6, chúng tôi khám phá các tỷ lệ chi phí khác nhau và quan sát rằng, ngay cả với tỷ lệ thấp như 1:25, AutoMix bắt đầu mang lại tăng trưởng không tầm thường trên hầu hết các tập dữ liệu.
Tập dữ liệu Chúng tôi thí nghiệm với một tập hợp đa dạng các tập dữ liệu: i) QASPER [Dasigi et al., 2021]: Hỏi đáp về các bài báo nghiên cứu; ii) QUALITY [Pang et al., 2022]: Câu hỏi trắc nghiệm (MCQ) về các bài viết và câu chuyện dài; iii) COQA [Reddy et al., 2019]: Hiểu đối thoại yêu cầu tham chiếu và suy luận thực dụng; iv) MUTUAL [Cui et al., 2020]: Suy luận đối thoại nhiều lượt (dự đoán phản hồi tiếp theo); v) DIPLOMAT [Li et al., 2023]: Các câu hỏi nhận dạng và suy luận thực dụng trên đối thoại nhiều lượt. QUALITY, QASPER và MUTUAL được cấp phép dưới CC BY, trong khi DIPLOMAT được cấp phép dưới CC BY-NC-SA. COQA sử dụng nhiều giấy phép cho một số phần và được chi tiết trong https://stanfordnlp.github.io/coqa/. Chúng tôi sử dụng điểm F1 cho QASPER và COQA, và độ chính xác cho các tập dữ liệu còn lại. Để quản lý độ phức tạp đầu vào, chúng tôi giữ lại một tập con ngữ cảnh (tối đa 3500 token) được truy xuất bằng cách sử dụng câu hỏi như một khóa. Truy xuất được thực hiện với mô hình nhúng câu all-MiniLM-L6-v2 [Reimers and Gurevych, 2019].
Chúng tôi sử dụng các tập xác thực từ Shaham et al. [2022] cho QASPER và QUALITY, và sử dụng các prompt từ Shaham et al. [2023]. Đối với COQA, MUTUAL, và DIPLOMAT, chúng tôi sử dụng các phần xác thực của chúng và điều chỉnh prompt QUALITY. Bất kể tập dữ liệu, chúng tôi cung cấp các prompt đầu vào giống hệt nhau cho cả SLM và LLM để đảm bảo chi phí xử lý đầu vào nhất quán. Độ dài đầu ra được cố định trong các tập dữ liệu trắc nghiệm như QUALITY, và tính ngắn gọn của các phản hồi trong các tập dữ liệu khác cho phép chúng tôi giả định chi phí xử lý đầu ra đồng nhất. Chúng tôi sử dụng giải mã tham lam (nhiệt độ 0) và rút một mẫu duy nhất cho cả SLM và LLM. Để tự kiểm chứng, chúng tôi sử dụng nhiệt độ=0.7 và rút 8 mẫu.
C Mở rộng AutoMix thành Ba Mô hình
Một trường hợp sử dụng quan trọng của AutoMix là định tuyến giữa nhiều mô hình có quy mô khác nhau về chi phí và hiệu suất. Trong các phần trước, chúng tôi xem xét tình huống hai mô hình, nơi SLM nhỏ hơn 2 bậc so với LLM. Trong phần này, chúng tôi đánh giá mức độ hoạt động tốt của AutoMix nếu chúng tôi xem xét một mô hình ngôn ngữ cỡ trung thứ ba (MLM). Cụ thể, chúng tôi nhằm giải quyết một số câu hỏi: 1.) AutoMix có hoạt động tốt hơn các baseline trong tình huống n-mô hình không, 2.) AutoMix có thể tự động học bỏ qua các mô hình nếu chúng không hiệu quả không 3.) Bộ định tuyến POMDP trong AutoMix có lợi như thế nào, 4.) Các trường hợp thất bại của AutoMix trong tình huống 3-mô hình là gì?
Để trả lời những câu hỏi này, chúng tôi sử dụng LLAMA 2-13B/MISTRAL-7B/GPT-3.5 làm SLM, LLAMA 2-70B làm MLM, và GPT-4 làm LLM. Chúng tôi xem xét các baseline sau:
•FrugalGPT: FrugalGPT đề xuất sử dụng định tuyến kiểu cascade cho n mô hình, nơi truy vấn đầu tiên được gửi đến SLM, và nếu độ tin cậy dưới ngưỡng, nó được gửi đến MLM, và cứ thế. Các ngưỡng được điều chỉnh trên tập xác thực, để tối đa hóa hiệu suất trên các vùng chi phí khác nhau. Lưu ý rằng FrugalGPT đào tạo N−1 bộ kiểm chứng riêng biệt cho mỗi cặp trong một chuỗi mô hình. Tương tự như các phần trước, chúng tôi coi chi phí của bộ kiểm chứng là 0.
•Union AutoMix: UnionAutoMix là một baseline đơn giản, nơi chúng tôi chọn giữa các biến thể hai mô hình AutoMix SLM−MLM, AutoMix SLM−LLM và AutoMix MLM−LLM, tùy thuộc vào yêu cầu chi phí được xác định bởi người dùng cuối. Ví dụ: nếu chi phí trung bình mong muốn nhỏ hơn của MLM, AutoMix SLM−MLM có thể được sử dụng, trong khi AutoMix MLM−LLM hoặc AutoMix SLM−LLM được sử dụng cho các vùng chi phí vượt quá của MLM. Chính xác, cho mỗi vùng chi phí trong tập xác thực, biến thể hoạt động tốt nhất (ΔIBC cao nhất) được chọn: max cho tất cả vùng chi phí (PerfAutoMix SLM−MLM, PerfAutoMix MLM−LLM, PerfAutoMix SLM−LLM). Lưu ý, công thức dễ dàng tổng quát hóa cho N lớn hơn bằng cách xem xét tất cả các cặp có thể.
•Chained AutoMix: Chained AutoMix là một baseline nơi chúng tôi chuỗi các biến thể hai mô hình AutoMix SLM−MLM và AutoMix MLM−LLM. Truy vấn đầu tiên đi đến SLM, và một AutoMix SLM−MLM quyết định giữa báo cáo câu trả lời SLM hoặc định tuyến đến MLM. Trong trường hợp sau, một AutoMix MLM−LLM thứ hai lặp lại quy trình sử dụng các mô hình MLM và LLM.
20

--- TRANG 21 ---
C.1 Mở rộng IBC cho N=3 mô hình
Để mở rộng metric IBC cho >2 mô hình, điều quan trọng là xem xét vùng chi phí đang được xem xét. Ví dụ: đối với N= 3 xem xét hai trường hợp riêng biệt: 1) khi đường cong SLM-MLM-LLM lồi, và 2) khi đường cong lõm. Trong trường hợp lồi, việc chọn giữa MLM và SLM trong các vùng chi phí thấp là có lợi, trong khi có lợi khi chọn giữa MLM và LLM trong các vùng chi phí cao. Theo đó, đường cong IBC phù hợp (cạnh tranh hơn) được chọn để đánh giá tương ứng. Tuy nhiên, trong trường hợp thứ hai, khi các đường cong IBC lõm, sẽ thuận lợi hơn khi chọn giữa SLM và LLM và hoàn toàn bỏ qua MLM, vì về mặt hiệu suất gia tăng trên chi phí, nó luôn trình bày một bất lợi. Do đó, IBC SLM−LLM được chọn để đánh giá trong suốt. Lưu ý rằng đối với N mô hình chung, cho mỗi vùng chi phí, NC 2 tổ hợp cần được xem xét.
Với các baseline và metric đánh giá được định nghĩa rõ, chúng tôi tiếp theo nhằm trả lời từng trong bốn câu trả lời được đề cập ở trên:
C.2 Kết quả của Automix với Ba Mô hình
C.2.1 AutoMix có vượt trội các phương pháp baseline không?
0 20 40 60 80 100
Chi phí Tính toán0.150.200.250.300.35Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.720.740.760.780.80Hiệu suất (Điểm F1)GPT-4
Mistral-7B
LLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.250.300.350.400.450.500.550.60Hiệu suất (Điểm F1)GPT-4
GPT-3.5LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4
Hình 8: AutoMix vượt trội các baseline trong tình huống N= 3-mô hình. Các ví dụ đại diện cho thấy các tập dữ liệu QASPER, DIPLOMAT, COQA, nơi AutoMix liên tục vượt trội các baseline: FrugalGPT, UnionAutoMix, ChainedAutoMix trên tất cả các vùng chi phí được xem xét.
Hình 8 trình bày kết quả trên ba trường hợp đại diện tất cả từ các tập dữ liệu và SLM khác nhau. Trên tất cả các tập dữ liệu, AutoMix liên tục vượt trội các baseline, FrugalGPT, UnionAutoMix, và ChainedAutoMix, trên tất cả các vùng chi phí được xem xét. Vì FrugalGPT và Chained AutoMix là định tuyến kiểu cascade, chúng không thể định tuyến trực tiếp từ SLM đến LLM, và luôn phải gọi MLM trước, do đó thể hiện hiệu suất kém. Trong khi Union AutoMix có thể giảm thiểu vấn đề này, nó vẫn cần luôn gọi mô hình MLM và Verifier tốn kém của nó để đưa ra quyết định, trong khi AutoMix có thể sử dụng thông tin có sẵn từ bộ kiểm chứng SLM, để quyết định có định tuyến đến MLM hay LLM. Ngay cả trong các trường hợp khi định tuyến đến MLM diễn ra, POMDP trong AutoMix tận dụng thông tin từ cả bộ kiểm chứng SLM và MLM bằng cách cập nhật niềm tin của nó, cung cấp cho nó khả năng ra quyết định tinh tế hơn.
21

--- TRANG 22 ---
C.2.2 AutoMix có thể bỏ qua các mô hình phân tâm không?
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.600.650.700.750.80Hiệu suất (Điểm F1)GPT-4
Mistral-7B
LLaMA-70B
Hình 9: AutoMix tự động bỏ qua các mô hình MLM phân tâm kém hiệu suất hơn SLM. Tuy nhiên, FrugalGPT và Chained AutoMix khó phân biệt điều này, dẫn đến hiệu suất kém trên toàn bộ. Kết quả trên tập dữ liệu MUTUAL với MISTRAL-7B làm SLM.
Như một sự mạnh mẽ, chúng tôi bây giờ xem xét một tình huống, nơi MLM kém hiệu suất hơn SLM. Tình huống như vậy phổ biến trong thực tế vì các mô hình đến từ các gia đình khác nhau, được đào tạo trên các loại dữ liệu khác nhau, và có thể một mô hình tốt hơn mô hình khác mặc dù quy mô. Hình 9 xem xét hai/ba tình huống, nơi MLM kém hiệu suất hơn SLM. Lưu ý rằng trong những trường hợp như vậy, FrugalGPT hoạt động tệ hơn đáng kể vì nó luôn định tuyến đến MLM cho các vùng chi phí cao hơn, do đó lãng phí tài nguyên. Do đó FrugalGPT yêu cầu một cân nhắc bổ sung trên tập xác thực, để loại bỏ những mô hình phân tâm như vậy. Tuy nhiên, AutoMix có thể học được sự không liên quan của MLM, và học định tuyến trực tiếp từ SLM đến LLM, khi cần thiết. Hiệu suất của AutoMix tương tự như Union AutoMix trong những trường hợp như vậy vì MLM không bao giờ được sử dụng.
22

--- TRANG 23 ---
C.2.3 Có nên bỏ qua MLM Hiệu suất Thấp không?
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.730.740.750.760.770.780.790.80Hiệu suất (Điểm F1)GPT-4GPT-3.5
LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4
Hình 10: AutoMix có thể tận dụng ngay cả MLM hiệu suất tệ hơn bằng cách xác định các câu hỏi, nơi MLM tốt hơn SLM và sử dụng xác suất kiểm chứng MLM. Tất cả các baseline khác, không thể tận dụng thông tin này, và hoạt động tệ hơn AutoMix
Trong khi trong phần trước, chúng tôi đã thiết lập rằng MLM hiệu suất thấp nên được bỏ qua, điều này có thể không phải lúc nào cũng đúng. Điều này là do có thể có một số loại câu hỏi nơi MLM thực sự tốt hơn SLM, và nếu, thông qua xác suất tự kiểm chứng, có thể khai thác thông tin như vậy, nó có thể cải thiện thêm chất lượng chi phí của AutoMix. Hình 10 thể hiện một trường hợp như vậy. Cụ thể, AutoMix xác định rằng xác suất kiểm chứng thấp trên SLM ngụ ý rằng MLM có khả năng tốt hơn SLM. Do đó, nó định tuyến đến các mô hình MLM một cách phù hợp. Hơn nữa, thông tin kiểm chứng kết hợp từ SLM và MLM, cho phép AutoMix đưa ra quyết định tinh tế hơn, và do đó vượt trội tất cả các baseline khác được xem xét. Đến mức AutoMix có thể xác định các mô hình dẫn đến thống kê vượt trội ngay cả LLM thậm chí với khoảng một nửa chi phí. Thí nghiệm, cùng với những thí nghiệm trước, chứng minh hiệu quả và tính hữu ích của bộ định tuyến POMDP trong AutoMix, vì nó tự động có thể xử lý nhiều trường hợp khác nhau và liên tục cung cấp sự cân bằng chi phí-chất lượng tốt nhất.
C.2.4 AutoMix có luôn cung cấp tăng trưởng đáng kể không?
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.30.40.50.60.70.80.9Hiệu suất (Điểm F1)GPT-4
GPT-3.5LLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.300.350.400.450.500.550.60Hiệu suất (Điểm F1)GPT-4
Mistral-7BLLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
Mistral-7B
LLaMA-70B
GPT-4
IBC7B/70B
IBC70B/GPT4
Hình 11: AutoMix hoạt động tương tự như các baseline mà không có bất kỳ tăng trưởng có ý nghĩa thống kê nào trong hai ví dụ đại diện, do tự kiểm chứng tương đối kém.
23

--- TRANG 24 ---
Trong các phần phụ trước, chúng tôi xem xét các trường hợp, nơi AutoMix tốt hơn đáng kể so với FrugalGPT và thường là các biến thể khác của AutoMix. Tuy nhiên, trong phần này, chúng tôi xem xét một vài trường hợp, nơi AutoMix không cung cấp cải thiện đáng kể. Hình 11 đại diện cho hai trường hợp. Cụ thể, chúng tôi lưu ý rằng trong cả hai trường hợp, AutoMix tương tự như FrugalGPT. Hơn nữa, lợi thế so với các đường IBC là không đáng kể hoặc thậm chí âm. Chúng tôi phân tích định tính và thấy rằng tự kiểm chứng bởi MLM trong những tình huống như vậy không đặc biệt hữu ích, và do đó, POMDP không thể cung cấp tăng trưởng đáng kể. Chúng tôi tin rằng công việc tương lai nên giải quyết những thách thức như vậy bằng cách xem xét thông tin ngữ cảnh hoặc các hình thức kiểm chứng khác để cải thiện hiệu suất của các phương pháp của chúng tôi trong những trường hợp như vậy.
coqa diplomat_pir2 mutual qasper quality01020304050607080Tỷ lệ phần trăm Ví dụ Tốt (%)Tỷ lệ phần trăm Phản hồi Chính xác theo Xác suất Bộ kiểm chứng
0.00.20.40.60.81.0Xác suất Bộ kiểm chứng
Hình 12: Xác suất Bộ kiểm chứng và Tính Chính xác: Tỷ lệ phần trăm phản hồi chính xác trên các bin xác suất bộ kiểm chứng khác biệt cho LLAMA 2-13B làm SLM. Mỗi bin đại diện cho một phạm vi xác suất bộ kiểm chứng và độ chính xác tương ứng của các phản hồi trong phạm vi xác suất đó trên nhiều tập dữ liệu khác nhau. Đáng chú ý, đối với tất cả các tập dữ liệu, ngoại trừ QUALITY và QASPER, một điểm kiểm chứng cao hơn thường tương ứng với một tỷ lệ lớn hơn các ví dụ chính xác, chỉ ra rằng bộ kiểm chứng có thể, ở một mức độ nào đó, phân biệt độ tin cậy của các phản hồi được tạo bởi chính nó. Chúng tôi sử dụng một bộ định tuyến hoạt động như một meta-verifier để vượt qua những dự đoán nhiễu này.
Hình 13, 14, 15, 16, 17 cho thấy kết quả cho tình huống N= 3 cho AutoMix cùng với các baseline.
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.300.350.400.450.500.550.60Hiệu suất (Điểm F1)GPT-4
Mistral-7BLLaMA-70B
0 20 40 60 80 100
Chi phí Tính toán0.520.540.560.580.600.62Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.250.300.350.400.450.500.550.60Hiệu suất (Điểm F1)GPT-4
GPT-3.5LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4
Hình 13: AutoMix trong tình huống N= 3-mô hình. Tập dữ liệu: COQA.
24

--- TRANG 25 ---
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.220.240.260.280.300.320.340.36Hiệu suất (Điểm F1)GPT-4
Mistral-7B
LLaMA-70B
0 20 40 60 80 100
Chi phí Tính toán0.150.200.250.300.35Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.260.280.300.320.340.360.38Hiệu suất (Điểm F1)GPT-4
GPT-3.5
LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4Hình 14: AutoMix trong tình huống N= 3-mô hình. Tập dữ liệu: QASPER.
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.600.650.700.750.80Hiệu suất (Điểm F1)GPT-4
Mistral-7B
LLaMA-70B
0 20 40 60 80 100
Chi phí Tính toán0.500.550.600.650.700.750.80Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.600.650.700.750.80Hiệu suất (Điểm F1)GPT-4
GPT-3.5
LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4
Hình 15: AutoMix trong tình huống N= 3-mô hình. Tập dữ liệu: QUALITY.
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.550.600.650.700.750.800.850.90Hiệu suất (Điểm F1)GPT-4
Mistral-7B
LLaMA-70B
0 20 40 60 80 100
Chi phí Tính toán0.50.60.70.80.9Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.30.40.50.60.70.80.9Hiệu suất (Điểm F1)GPT-4
GPT-3.5LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4
Hình 16: AutoMix trong tình huống N= 3-mô hình. Tập dữ liệu: MUTUAL.
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.720.740.760.780.80Hiệu suất (Điểm F1)GPT-4
Mistral-7B
LLaMA-70B
0 20 40 60 80 100
Chi phí Tính toán0.6000.6250.6500.6750.7000.7250.7500.775Hiệu suất (Điểm F1)GPT-4
LLaMA-13BLLaMA-70B
0 25 50 75 100 125 150 175 200
Chi phí Tính toán0.730.740.750.760.770.780.790.80Hiệu suất (Điểm F1)GPT-4GPT-3.5
LLaMA-70BPhương pháp
Automix3
Chained Automix
Union Automix
Frugal
GPT-3.5
LLaMA-70B
GPT-4
IBCGPT3.5/70B
IBC70B/GPT4
Hình 17: AutoMix trong tình huống N= 3-mô hình. Tập dữ liệu: DIPLOMAT.
D Kết quả Bổ sung
D.1 Kết quả Chính
Hình 19, 18, 4 cho thấy kết quả của AutoMix cho 3 lựa chọn khác nhau của SLM. Lưu ý, cho 13/15 cấu hình, AutoMix-POMDP rõ ràng tốt hơn tất cả các baseline khác trong suốt các chi phí vận hành khác nhau. Ngay cả trong 2 trường hợp còn lại, AutoMix có tính cạnh tranh và gần như tương đương với các baseline. Chúng tôi thấy các xu hướng tương tự từ Mục 5.3 cho LLAMA 2-13B và GPT-3.5 cũng vậy.
D.2 AutoMix hiệu quả trên các Tỷ lệ Chi phí khác nhau
Trong phần này, chúng tôi so sánh thêm làm thế nào, AutoMix so sánh với các baseline cho các tỷ lệ chi phí khác nhau. Hình 20 chứng minh rằng ở tỷ lệ chi phí dựa trên API, AutoMix vượt trội tất cả các baseline. Ngay cả ở tỷ lệ 20:1 (một tình huống bi quan, không chắc cho AutoMix), AutoMix vượt trội các baseline, mặc dù với lề nhỏ hơn. Ngược lại, ở tỷ lệ 2000:1, AutoMix cho thấy cải thiện thậm chí cao hơn. Kết quả chứng minh tính mạnh mẽ của AutoMix đối với chi phí thay đổi liên tục của các mô hình và API.
25

--- TRANG 26 ---
0 50 1000.60.620.640.660.680.70.720.74DIPLOMAT
0 50 1000.50.60.70.80.9MUTUAL
0 50 1000.50.550.60.650.70.750.8QUALITY
0 50 1000.520.540.560.580.6COQA
0 50 1000.150.20.250.30.35QASPER
AutoMix +POMDP
AutoMix +Threshold
FrugalGPT [Chen et al., 2023]
HybridLLM [Ding et al., 2024]
LLAMA 2-13 B(SLM)
GPT-4 (LLM)
Random Mixing
Hình 18: Kết quả Chính: hiệu suất (trục y) vs. chi phí (trục x) cho các phương pháp khác nhau trên LLAMA2-13/GPT-4 nhỏ và lớn. Meta-verifier dựa trên POMDP liên tục ở trên phép nội suy tuyến tính của SLM-LLM, biểu thị lợi ích gia tăng cao hơn trên mỗi đơn vị chi phí (IBC).
D.3 Tổng quát hóa Ngoài Miền của AutoMix
Bộ định tuyến POMDP được giới thiệu trong AutoMix không giả định bất kỳ đặc điểm tác vụ hoặc tập dữ liệu cụ thể nào để hoạt động, vì nó chỉ dựa vào độ tin cậy tự kiểm chứng như đầu vào. Do đó, bộ định tuyến POMDP của chúng tôi có thể tổng quát hóa cho nhiều tập dữ liệu và tác vụ khác nhau. Để chứng minh thêm điều này, chúng tôi đánh giá tổng quát hóa ngoài miền bằng cách đào tạo trên một tập dữ liệu và đánh giá trên những tập dữ liệu khác. Cụ thể, chúng tôi đào tạo bộ định tuyến trên một trong năm tập dữ liệu và đánh giá nó trên bốn tập dữ liệu khác. Chúng tôi lặp lại thí nghiệm cho tất cả năm tập dữ liệu và ba SLM. Kết quả trong các thiết lập khác nhau, như được hiển thị trong Bảng 5, chỉ ra rằng bộ định tuyến POMDP của chúng tôi liên tục vượt trội cả FrugalGPT và HybridLLM. Thiết kế của bộ định tuyến POMDP, cùng với những kết quả này, làm nổi bật tính tổng quát hóa mạnh mẽ của phương pháp được đề xuất của chúng tôi.
Mistral-7b LLama-13b GPT-3.5
Automix 28.3 31.5 70.9
Frugal 12.5 0.0 14.3
Hybrid 2.4 -2.8 7.6
Bảng 5: Tổng quát hóa ngoài miền trên nhiều SLM khác nhau cho các phương pháp khác nhau. Điểm được tính trung bình trên năm tập dữ liệu. AutoMix vượt trội đáng kể các phương pháp khác.
E Prompt Few-Shot
E.1 Prompt Bộ kiểm chứng
26

--- TRANG 27 ---
0 10 20 30 40 50 60 700.750.760.770.780.790.8DIPLOMAT
0 10 20 30 40 50 60 700.250.50.750.95MUTUAL
0 50 1000.50.550.60.650.70.750.8QUALITY
0 10 20 30 40 50 60 700.250.350.450.550.65COQA
0 10 20 30 40 50 60 700.250.30.350.4QASPER
AutoMix +POMDP
AutoMix +Threshold
AutoMix +SelfConsistency
FrugalGPT [Chen et al., 2023]
HybridLLM [Ding et al., 2024]
LLAMA 2-13 B(SLM)
GPT-4 (LLM)
Random Mixing
Hình 19: Kết quả Chính: hiệu suất (trục y) vs. chi phí (trục x) cho các phương pháp khác nhau trên GPT-3.5/GPT-4 nhỏ và lớn. Bộ định tuyến dựa trên POMDP liên tục ở trên phép nội suy tuyến tính của SLM-LLM, biểu thị lợi ích gia tăng cao hơn trên mỗi đơn vị chi phí (IBC). AutoMix là phương pháp hoạt động tốt nhất liên tục trên tất cả các tập dữ liệu, thường với lề lớn.
Hình 20: Chúng tôi so sánh AutoMix với các baseline trên nhiều tỷ lệ chi phí LLM so với SLM khác nhau. Đối với tất cả các tỷ lệ chi phí được xem xét, AutoMix vượt trội các baseline với mức độ lề khác nhau.
Câu chuyện:
{các phần liên quan của câu chuyện}
{hướng dẫn}
Câu hỏi: {question}
Câu trả lời:
Danh sách 2: Prompt Tác vụ. Chúng tôi thí nghiệm với các tác vụ suy luận ngữ cảnh dài, yêu cầu trả lời câu hỏi từ câu chuyện, hợp đồng pháp lý, bài báo nghiên cứu và tiểu thuyết.
27

--- TRANG 28 ---
Ngữ cảnh: {context}
Câu hỏi: {question}
Câu trả lời được AI tạo: {generated_answer}
Hướng dẫn: Nhiệm vụ của bạn là đánh giá xem Câu trả lời được AI tạo có chính xác hay không, dựa trên ngữ cảnh và câu hỏi được cung cấp. Cung cấp phán đoán và lý luận cho từng trường hợp. Chọn giữa Chính xác hoặc Không chính xác.,→
,→
,→
Đánh giá:"'
Danh sách 3: Prompt Kiểm chứng. Quá trình kiểm chứng được đóng khung như một tác vụ entailment ngôn ngữ tự nhiên, nơi mô hình xác định tính hợp lệ của câu trả lời do mô hình tạo ra so với ngữ cảnh và câu hỏi.
28

--- TRANG 29 ---
Ngữ cảnh: Bản thảo, được phát hiện vào năm 1980 trong một tầng áp mái bụi bặm, hóa ra là một tác phẩm thất lạc của Shakespeare.\n ,→
Câu hỏi: Tác phẩm thất lạc của ai đã được phát hiện trong một tầng áp mái bụi bặm vào năm 1980?\n
Câu trả lời được AI tạo: Shakespeare\n
Hướng dẫn: Nhiệm vụ của bạn là đánh giá xem Câu trả lời được AI tạo có chính xác hay không, dựa trên ngữ cảnh và câu hỏi được cung cấp. Cung cấp phán đoán và lý luận cho từng trường hợp. Chọn giữa Chính xác hoặc Không chính xác.\n,→
,→
,→
Đánh giá: Ngữ cảnh cụ thể đề cập rằng một tác phẩm thất lạc của Shakespeare đã được phát hiện vào năm 1980 trong một tầng áp mái bụi bặm. ,→
Quyết định Kiểm chứng: Câu trả lời được AI tạo là Chính xác.
---
Ngữ cảnh: Sự kiện thiên thể, được biết đến như Mặt trăng Hồng, là độc nhất cho tháng Tư và có ý nghĩa văn hóa trong nhiều bộ lạc bản địa.\n,→
,→
Câu hỏi: Sự kiện thiên thể, Mặt trăng Hồng, xảy ra vào tháng nào?\n ,→
Câu trả lời được AI tạo: Tháng Bảy\n
Hướng dẫn: Nhiệm vụ của bạn là đánh giá xem Câu trả lời được AI tạo có chính xác hay không, dựa trên ngữ cảnh và câu hỏi được cung cấp. Cung cấp phán đoán và lý luận cho từng trường hợp. Chọn giữa Chính xác hoặc Không chính xác.\n,→
,→
,→
Đánh giá: Ngữ cảnh rõ ràng nói rằng Mặt trăng Hồng là độc nhất cho tháng Tư. ,→
Quyết định Kiểm chứng: Câu trả lời được AI tạo là Không chính xác.
---
{các ví dụ rút gọn}
Ngữ cảnh: {context}\n
Câu hỏi: {question}\n
Câu trả lời được AI tạo: {generated_answer}
Hướng dẫn: Nhiệm vụ của bạn là đánh giá xem Câu trả lời được AI tạo có chính xác hay không, dựa trên ngữ cảnh và câu hỏi được cung cấp. Cung cấp phán đoán và lý luận cho từng trường hợp. Chọn giữa Chính xác hoặc Không chính xác.,→
,→
,→
Đánh giá:
Danh sách 4: Prompt Bộ kiểm chứng Few-Shot: Prompt bộ kiểm chứng 3-shot để đánh giá tính chính xác của câu trả lời của SLM. Cùng prompt được sử dụng cho tất cả các tập dữ liệu.
29
