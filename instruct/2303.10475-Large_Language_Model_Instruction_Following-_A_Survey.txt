# 2303.10475.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2303.10475.pdf
# File size: 1973569 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Large Language Model Instruction Following: A Survey
of Progresses and Challenges
Renze Louâ™ Kai Zhangâ™¢and Wenpeng Yinâ™ 
â™ The Pennsylvania State Universityâ™¢The Ohio State University
{renze.lou, wenpeng}@psu.edu ;zhang.13253@osu.edu
Abstract
Task semantics can be expressed by a set of
input-output examples or a piece of textual in-
struction. Conventional machine learning ap-
proaches for natural language processing (NLP)
mainly rely on the availability of large-scale
sets of task-specific examples. Two issues arise:
first, collecting task-specific labeled examples
does not apply to scenarios where tasks may
be too complicated or costly to annotate, or
the system is required to handle a new task
immediately; second, this is not user-friendly
since end-users are probably more willing to
provide task description rather than a set of
examples before using the system. Therefore,
the community is paying increasing interest in
a new supervision-seeking paradigm for NLP:
learning to follow task instructions, i.e., instruc-
tion following . Despite its impressive progress,
there are some unsolved research equations that
the community struggles with. This survey pa-
per tries to summarize andprovide insights to
the current research on instruction following,
particularly, by answering the following ques-
tions: (i) What is task instruction, and what in-
struction types exist? (ii) How to model instruc-
tions? (iii) What are popular instruction follow-
ing datasets and evaluation metrics? (iv) What
factors influence and explain the instructionsâ€™
performance? (v) What challenges remain in
instruction following? To our knowledge, this
is the first comprehensive survey about instruc-
tion following.1
1 Introduction
One goal of AI is to build a system that can uni-
versally understand and solve new tasks. Labeled
examples (Figure 1 (a)), as the mainstream task
representation, are costly to obtain at scale or even
do not exist in some cases. Then, is there any
other task representation that can contribute to task
1The curated paper list can be found at: https://github.
com/RenzeLou/awesome-instruction-learning
A real masterpiece [â€¦]â†“Unseen Instances Generalization
What can I do if my paper is rejected?
Negative
(a). Example-driven Supervised LearningUnited States.
Negative
(b). Instruction-driven Supervised LearningClassify the review  as positive, negative, or neutral [â€¦]Identify all the named entities appearing in the utterance [â€¦]Translate the given English utterance into French script [â€¦]Donald Trump 
Je l'aime maiselle l'aime.
â€¦I didnâ€™t really enjoy that movie [â€¦]
These are all junks, [â€¦]â€¦The food is not bad, [â€¦]NeutralSentiment Analysis
A recent study published in the journal [â€¦]Summarization
Junk food makes people fat.Summarize the main topic of the given paragraph [â€¦]
Donald Trump served as the 45th president [â€¦]Entity Recognition
â€¦â€¦
â€¦
â€¦â€¦Correct the grammar of the following sentence [â€¦]What I can do if my paper was rejected?Grammar Correction
v
Ididnâ€™t really enjoy [â€¦]Negative
I love her, but she loves him.
PositiveWhat I can do if my paper was rejected?Grammar CorrectionSentiment Analysisâ€¦â€¦â†“ Unseen Tasks Generalization
Sentiment Analysis
â†“ Unseen Tasks Generalization
Machine TranslationI love her, but she loves him.
ðŸ”¥
ðŸ”¥
ðŸ”¥
ðŸ’§
Instruction is all I need!
ðŸ”¥
Task InstructionTrainingâ€¦â€¦TrainingMachine TranslationFigure 1: Two supervised learning paradigms: (a)
example-driven learning uses extensive labeled exam-
ples to represent the task semantics. The resulting sys-
tem can only generalize to unseen instances of the same
task; (b) instruction-driven learning tames model to fol-
low various task instructions. Besides unseen instances,
the final system can also generalize to unseen tasks.
comprehension? Textual instructions provide an-
other dimension of supervision for expressing the
task semantics, which often contains more abstract
and comprehensive knowledge of the target task
than individual labeled examples. As shown in
Figure 1 (b), with the availability of task instruc-
tions, systems can be quickly built to handle new
tasks. Such efficiency is highly desirable in real-
world applications, especially when task-specific
annotations are scarce. More importantly, instruc-
tion following leans toward human intelligence in
terms of learning new tasksâ€”a little child can well
solve a new mathematical task by learning from
its instruction and a few examples (Fennema et al.,
1996; Carpenter et al., 1996). As a result, this new
learning paradigm has recently attracted the main
attention of the machine learning and NLP commu-
nities (Wang et al., 2022b; Longpre et al., 2023).
When talking about â€œinstructionâ€, most of usarXiv:2303.10475v8  [cs.CL]  25 May 2024

--- PAGE 2 ---
will first think of â€œpromptâ€â€”using a brief template
to convert a task input into a new format (e.g., cloze
question) that caters to the language modeling ob-
jective of large language models (LLMs) (Brown
et al., 2020). Despite the prevalence of prompts
in text classification, machine translation, etc., we
argue that prompts are merely a special case of in-
structions. This paper takes a comprehensive and
broader view of instruction-driven NLP research.
Particularly, we try to answer the following ques-
tions: (i) what is task instruction, and what instruc-
tion types exist? (Â§ 4) (ii) given a task instruction,
how to encode it to assist the model generaliza-
tion on the target task? (Â§ 5) (iii) what are popular
instruction following datasets and the mainstream
evaluation metrics? (Â§ 6) (iv) what factors (e.g.,
model size, task numbers) impact the instruction-
driven systemsâ€™ performance? (Â§ 7) (v) what chal-
lenges exist in instruction following, and what are
future directions? (Â§ 8)
To our knowledge, this is the first paper that
surveys the instruction following. In contrast to
some existing surveys that focused on a specific in-
context instruction, such as prompts (Liu et al.,
2023a), input-by-output demonstrations (Dong
et al., 2023), or reasoning (Huang and Chang, 2023;
Qiao et al., 2023; Yu et al., 2023a), this work pro-
vides a more comprehensive overview of the in-
struction following. Our contributions are three-
fold:
â€¢Going beyond prompts, we analyze prompt
constraints via a user-centric lens, with a focus on
discerning the disparity between current instruction
following research and real-world needs.
â€¢Weinterpret different task instructions from
the unified perspective of indirect supervision , and
summarize their advantages, limitations, and scope
of applications;
â€¢We regard current ever-growing LLMs and in-
struction datasets as an effort of dual-track scaling;
additionally, we point out current notable research
issues and promising directions in the future.
2 Related Work
There are basically two topics that highly relate to
this paper, namely instruction following (2.1) and
Surveys on In-context Instructions (2.2).
2.1 Instruction Following
As illustrated in Figure 1, unlike traditional
example-driven supervised learning, the essenceof instruction following is to train the LLMs to un-
derstand various instructions and produce the cor-
responding responses. Since this capacity can be
extended to any unseen downstream tasks, instruc-
tion following has become an efficient learning
paradigm for solving few/zero-shot tasks (Radford
et al., 2019; Schick and SchÃ¼tze, 2021c; Yin et al.,
2022; Li et al., 2023a; Gupta et al., 2023; Sun et al.,
2024; Xie et al., 2024b, inter alia ). However, the
performance of instruction following highly relies
on both model and task scale: a larger LLM (or pre-
training with more tokens) tuned on more diverse
tasks can achieve significantly better few/zero-shot
performances on the downstream tasks (Chung
et al., 2022; Iyer et al., 2022; Wang et al., 2023c,
inter alia ). As scaling model size is unrealistic
for most of us, numerous recent studies worked on
collecting high-quality instruction-tuning datasets,
either employing human labors (Khashabi et al.,
2020; Ye et al., 2021; Sanh et al., 2022; Wang et al.,
2022b; Longpre et al., 2023; Conover et al., 2023;
KÃ¶pf et al., 2023) or distilling supervision from
the powerful LLMs (Wang et al., 2023d; Honovich
et al., 2023a; Taori et al., 2023; Peng et al., 2023;
Xu et al., 2023b; Geng et al., 2023; Chiang et al.,
2023; Xu et al., 2023a; KÃ¶ksal et al., 2023; Kim
et al., 2023; Ding et al., 2023; Yin et al., 2023; Lou
et al., 2024), e.g., utilizing ChatGPT or GPT-4 to
develop creative task instructions (OpenAI, 2022,
2023).
Despite the popularity, the current instruction fol-
lowing still suffers challenges and has considerable
room for evolution. This work not only surveys the
extensive existing literature on instruction follow-
ing but also goes beyond: we trace the development
of instruction following back to the early days of
semantic parsing based machine learning, and for-
mulate our story from an indirect supervision per-
spective. We hope this survey can systematically
introduce this popular yet challenging area.
2.2 Surveys on In-context Instructions
Several existing surveys (Dong et al., 2023; Huang
and Chang, 2023; Qiao et al., 2023; Yu et al.,
2023a) share similar motivations with us while fo-
cusing on merely some sub-area of instruction fol-
lowing, such as prompt, few-shot demonstrations,
chain-of-thoughts reasoning, etc. For example, Liu
et al. (2023a) provided a comprehensive overview
of prompt learning and LLMs, where the prompt
can be regarded as one specific type of textual in-

--- PAGE 3 ---
My car was smashed last night.
This expresses anger.
This is about health.This is about an accident.This movie is ____
It
was
a
real
masterpiece!
GoodWonderfulAccidentPositivePopularâ€¦Definition: Given a movie review, your task is to classify it as â€œpositiveâ€, â€œnegativeâ€, or â€œneutralâ€ based on the sentiment. Do not [...]    Now, complete the following instance:
Positive(a). NLI-orientedInstruction(b). LLM-orientedInstruction(c). Human-orientedInstructionExamples:Input: I didn't really enjoy that movie. Output: NegativeInput: I thought that it was really well made. Output: Positive[â€¦]optionalAnger
It
was
a
real
masterpiece!
InstructionInputOutput
EntailContradictTop-probPredictionsFigure 2: An illustration of three distinct categories of textual instructions.
struction (as categorized in Â§ 4). Some other stud-
ies surveying â€œsoft instructionâ€, namely parameter-
efficient fine-tuning methods (Lialin et al., 2023),
also differ from our scope of â€œtextual instructionâ€.
Notably, Zhang et al. (2023b) also proposed a sur-
vey on instruction tuning, however, they mostly
focused on the existing datasets and models; while
we present a more complete and consistent story of
the instruction following, including the instruction
categories, modeling strategies and so on, which
previous works have never introduced. To the best
of our knowledge, this is the first work that provides
a comprehensive and high-level story of instruction
following.
3 Preliminary
For instruction following, we target driving the sys-
tems to reach the corresponding output of the input
by following the instruction. Thus, we assume that
a dataset usually consists of three items:
â€¢Input (X): the input of an instance; it can be a
single piece of text (e.g., sentiment classification)
or a group of text pieces (e.g., textual entailment,
question answering, etc.).
â€¢Output (Y): the output of an instance; in clas-
sification problems, it can be one or multiple prede-
fined labels; in text generation tasks, it can be any
open-form text.
â€¢Template (T): a textual template that either
tries to express task intent or is used for bridging
X and Y.2T may not be an instruction yet.
In Â§ 4, we will elaborate that a task instruction I
2A plain template connecting XandY, e.g., â€œ The input
is [. . .] The output is [ . . .]â€, no task-specific semantics.is actually a combination of Twith XorY, or the
T on its own in some cases.
4 What is Task Instruction?â€”A Unified
Perspective from Indirect Supervision
This section first summarizes three main instruc-
tion types constructed by different combinations
ofT,X, and Y(as illustrated in Figure 2), then
presents our interpretation of them via an indirect
supervision perspective.
4.1 Three Types of Instructions
4.1.1 NLI-oriented Instructions (i.e., I=T+Y)
A conventional scheme to handle the classification
tasks is to convert the target labels into indices and
let models decide which indices the inputs belong
to. This paradigm only encodes the input semantics
while losing the label semantics. To let systems
recognize new labels without relying on massive
labeled examples, Yin et al. (2019) proposed con-
verting the target classification tasks into natural
language inference (NLI) by building a hypothesis
for each labelâ€”deriving the truth value of a label
is then converted into determining the truth value
of the hypothesis. As exemplified in Figure 2 (a),
this approach builds instructions ( I) by combining
a template ( T) with a label ( Y) to explain the task
semantics. Table 1 further provides more detailed
examples for NLI-oriented Instructions .
The advantages of NLI-oriented Instructions
learning are four-fold: (i) it keeps the label se-
mantics and makes it possible to encode the input-
output relations; (ii) it unifies various classification
problems into an NLI task; (iii) by making use of

--- PAGE 4 ---
Task NLI premise (i.e., input text) NLI hypothesis (i.e., instructions Y)
Entity
Typing[Donald Trump] entserved as the 45th president of the
United States from 2017 to 2021.(D) Donald Trump is a politician
(%) Donald Trump is a journalist
Entity
Relation[Donald Trump] ent1served as the 45th president of the
[United States] ent2from 2017 to 2021.(D) Donald Trump is citizen of United States
(%) Donald Trump is the CEO of United States
Event
Argument
ExtractionIn [1997] time , the [company] sub[hired] trigger [John D.
Idol] objto take over Bill Thang as the new chief executive.(D)John D. Idol was hired.
(D)John D. Idol was hired in 1997.
(%)Bill Thang was hired.
Event
RelationSalesforce and Slack Technologies have [entered] event 1
into a definitive agreement] under which Salesforce will
[acquire] event 2Slack.(D) Salesforce acquires Slack after it enters into the agreement with
Slack Tech.
(%) Salesforce acquires Slack because it enters into the agreement with
Slack Tech.
Stance
DetectionLast Tuesday, Bill said â€œanimals are equal to human beingsâ€
in his speech.(D) Bill supports that animals should have lawful rights.
(%) Bill opposes that animals should have lawful rights.
Table 1: NLI-oriented Instructions construct hypotheses to explain the labels (in bold ). â€œDâ€: correct; â€œ %â€:
incorrect.
Task Input X Template T (cloze question) Answer Output Y
Sentiment
AnalysisI would like to buy it again. [X]The product is .Great
Wonderful
. . .Positive
Entity
Tagging[Donald Trump] entserved as the 45th president of the
United States from 2017 to 2021.The entity in [X]is a class?Politician
President
. . .People
Relation
Tagging[Donald Trump] ent1served as the 45th president of the
[United States] ent2from 2017 to 2021.[X]entity 1is the of entity 2?Executive
Leader
. . .President
Textual
Entailment[X1]: Donald Trump served as the 45th president of the
United States from 2017 to 2021.
[X2]: Donald Trump is a citizen of United States.[X2]?, because [X1]Indeed
Sure
. . .Yes
TranslationDonald Trump served as the 45th president of the United
States from 2017 to 2021.Translate [X]to French: / Ã©tÃ© prÃ©sident ...
Table 2: LLM-oriented Instructions utilize templates to convert the origin inputs into fill-in-blank questions. In most
classification tasks, the intermediate answers may require further mapping (i.e., verbalizer).
the indirect supervision from existing NLI datasets,
a model trained on NLI tasks is expected to work
on other tasks in a zero-shot manner; (iv) it extends
the original close-set indices classification problem
into an open-domain label recognition paradigm.
Therefore, it has been widely used in a variety of
few/zero-shot classification tasks (Xu et al., 2023d),
such as classifying topics (Yin et al., 2019), sen-
timents (Zhong et al., 2021), stances (Xu et al.,
2022b), entity types (Li et al., 2022a), entity rela-
tions (Murty et al., 2020; Xia et al., 2021; Sainz
et al., 2021, 2022), etc.
4.1.2 LLM-oriented Instructions (i.e.,
prompts; I=T+X)
As shown in Figure 2 (b) and Table 2, the prompt is
a representative of the LLM-oriented Instructions ,
which is usually a brief utterance prepended with
the task input (prefix prompt), or a cloze-question
template (cloze prompt). It is basically designed
for querying the intermedia responses (that can
be further converted into the final outputs) fromthe LLM. Since the prompted input conforms to
the pre-training objectives of LLM (e.g., the cloze-
style input satisfies the masked language modeling
objective (Devlin et al., 2019a)), it helps get rid
of the reliance on the traditional supervised fine-
tuning and greatly alleviates the cost of human
annotations. Thus, prompt learning achieved im-
pressive results on a multitude of previous few/zero-
shot NLP tasks, like question answering (Radford
et al., 2019; Lin et al., 2022), machine transla-
tion (Li et al., 2022c), sentiment analysis (Wu and
Shi, 2022), textual entailment (Schick and SchÃ¼tze,
2021a,b), entity recognition (Cui et al., 2021; Wang
et al., 2022a), etc.
Despite the excellent performance of prompt
techniques, there are still two obvious shortcom-
ings with LLM-oriented Instructions in real-world
applications. (i) Not User-Friendly . As the prompt
is crafted for serving LLMs, it is encouraged to
design the prompt in a â€œmodelâ€™s languageâ€ (e.g.,
model-preferred incoherent words or internal em-
bedding). However, this LLM-oriented style is

--- PAGE 5 ---
Task Input X Template T + Few-shot Demonstrations Output Y
Sentiment
AnalysisI am extremely impressed
with its good performance. I
would like to buy it again!Task Definition :
In this task, you are given a product review, and you need to identify . . .
Demonstrations (optional) :
Input: These are junks, I am really regret... Output: Negative
Input: Wonderful bulb with good duration... Output: Positive
Test Instance :
Input: [X] Output:Positive
Named
Entity
ExtractionDonald Trump served as the
45th president of the United
States from 2017 to 2021.Task Definition :
Your task is to recognize the name of a person in the given sentence . . .
Demonstrations (optional) :
Input: Ousted WeWork founder Adam Neuman... Output: Adam Neuman
Input: Tim Cook became the CEO of Apple Inc since... Output: Tim Cook
Test Instance :
Input: [X] Output:Donald
Trump
Table 3: Two examples that illustrate the Human-oriented Instructions (w/ 2-shot demonstrations). Similar to the
LLM-oriented Instructions ,Human-oriented Instructions use task-level templates to convert the origin inputs into
blank questions. However, the templates here have sufficient task semantics (i.e., Task Definition ) and are sometimes
equipped with Demonstrations , while those in LLM-oriented Instructions usually do not.
hard to be understood by users and often violates
human intuitions (Gao et al., 2021; Li and Liang,
2021; Qin and Eisner, 2021; Khashabi et al., 2022).
Meanwhile, the performance of prompts highly de-
pends on the laborious prompt engineering (Bach
et al., 2022), but most end-users are not LLM
experts and usually lack sufficient knowledge to
tune an effective prompt. (ii) Applications Con-
straints . The prompt is usually short and simplistic,
whereas many tasks cannot be effectively formu-
lated with solely a brief prompt, making prompt
hard to deal with the diverse formats of real-world
NLP tasks (Chen et al., 2022b; Zhang et al., 2023a).
4.1.3 Human-oriented Instructions (i.e., I=T+
optional {Xi,Yi}k
i=1)
Human-oriented Instructions essentially denotes
the instructions used for crowd-sourcing on
the human-annotation platforms (e.g., Amazon
MTurk). Unlike LLM-oriented Instructions ,
Human-oriented Instructions (Figure 2 (c)) are
usually some human-readable, descriptive, and
paragraph-style information consisting of various
components, such as â€œ task title â€, â€œcategory â€,
â€œdefinition â€, and â€œ things to avoid â€, etc. (cf.
Mishra et al., 2022b) Thus, Human-oriented In-
structions are more user-friendly and can be ideally
applied to almost any complex NLP task. Table 3
further shows some representative task examples.
Accordingly, Human-oriented Instructions have
attracted much more attention in recent years (Hu
et al., 2022b; Gupta et al., 2022; Yin et al., 2022,
inter alia ). However, due to the complex nature,
Human-oriented Instructions are more challeng-ing to encode by vanilla LLMs. For example,
off-the-shelf GPT-2 was found to work poorly on
following MTurk instructions (Wolf et al., 2019;
Efrat and Levy, 2020). To tame the LLMs bet-
ter understand the Human-oriented Instructions ,
follow-up works began to collect large-scale in-
struction datasets (Mishra et al., 2022b; Wang et al.,
2022b). All the previous results showed that, af-
ter fine-tuning with various task instructions, the
text-to-text LLMs, like T5 (Raffel et al., 2020),
OPT (Zhang et al., 2022a) and Llama (Touvron
et al., 2023), achieved remarkable few/zero-shot
generalizations by following these complex instruc-
tions (Wang et al., 2023c; Ivison et al., 2023b).
4.2 An Indirect Supervision Perspective
Although the three types of instructions are very
different from each other, they are essentially seek-
ing the same thingâ€” indirect supervision â€”to cope
with target tasks that have limited annotations.
Specifically, NLI-oriented Instructions trans-
form target NLP problems into a source taskâ€”
NLIâ€”so that the rich supervision from existing
NLI datasets can act as indirect supervision for
those target problems. LLM-oriented Instruc-
tions reformat target problems into the source
taskâ€”language modeling, so that the rich generic-
purpose knowledge in those LLMs can be directly
utilized to get the output. Whether it is NLI-
oriented Instructions orLLM-oriented Instructions ,
both try to solve unseen tasks with a generaliz-
able system. However, both of them have lim-
ited application scope, e.g., they cannot efficiently
deal with some structured prediction tasks (Chen

--- PAGE 6 ---
Trait NLI-oriented LLM-oriented Human-oriented
Update LLM parameter? yes maybe yes
Require super large LLMs? no yes no
Require further label mapping (e.g., verbalizer)? yes yes no
End-user friendly? no no yes
Instruction granularity sentence-level (brief) sentence-level (brief) paragraph-level (complex)
Instruction scope output-wise input-wise task-wise
Task scope classification classification & generation classification & generation
Modeling objective NLI language modeling follow instructions
Source of indirect supervision NLI language modeling various Text-to-Text tasks
Table 4: Comparison of the three different instruction types in Â§ 4.
et al., 2022b; Zhang et al., 2023a). Instead of seek-
ing supervision from a single source task (NLI
or language modeling), Human-oriented Instruc-
tions learn indirect supervision from a large set
of training tasks, the resulting system, therefore,
can ideally generalize to any unseen textual tasks.
Table 4 further compares them from different di-
mensions.
5 How to Model Instructions?
Since both NLI-oriented Instructions andLLM-
oriented Instructions are associated with either
the input Xor the output Y, these types of in-
structions do not require specific system design
to encode them. NLI-oriented Instructions can be
handled by regular systems for the NLI task, and
LLM-oriented Instructions are mostly fed to auto-
regressive LLMs. In contrast, Human-oriented In-
structions are the most challenging type since it is
independent of any labeled instances.
Therefore, this section mainly presents several
mainstream modeling strategies for the Human-
oriented Instructions , as illustrated in Figure 3.
5.1 Semantic Parser
At the early stage of machine learning, to help the
systems understand natural language instructions, a
great number of works employed semantic parsing
to convert the instruction into the formal language
(logical formula), which can be more easily exe-
cuted by the systems (Goldwasser and Roth, 2011,
inter alia ). As exemplified in Figure 3 (a), a game
instruction â€œ Move any top card to an empty
free cell â€ can be processed into an executable
formula: â€œ card(x) âˆ§freecell(y) â€.
Previous research spent extensive efforts on this
strategy, among which most are used for human-
computer interaction tasks, e.g., playing soccer
games (Kuhlmann et al., 2004). To alleviate la-
borious human annotations, the follow-up works
ðŸ‚µðŸ‚«ðŸƒ
â€œMove any top card to an empty free cell.â€card (x) âˆ§ empty (y)Classify the review  as positive or negative.Instruction:Task input:It was a real masterpiece.Other resources:[â€¦]
â€œClassify the review as positive or negative.â€Positive
HyperNetworkIt was a real masterpiece.PositiveModel pluginInsert into the base model
â€œClassify the review as positive or negative.â€
Logical formula
SemanticparserHumaninstruction
Humaninstruction
Humaninstruction(a). Semantic Parser based Instruction Modeling
(b). Flatten-and-Concatenation based Instruction Modeling
(c). HyperNetwork based Instruction Modeling
â€œPlease give me a low-calorie meal plan â€¦â€Humaninstruction
Initial LMTuned LMSalad with two boiled eggs.Burger with two fried eggs.KL divergenceInstructionTask inputOther resources
Reward functionReward
LossParameter update(d). Reinforcement Learning Human FeedbackFigure 3: Four modeling strategies for instructions.
leveraged indirect or weak supervision from the
grounded environments (e.g., knowledge base) to
train the semantic parser (Kim and Mooney, 2012).
Limitations Semantic parser-based approaches
mainly apply to individual tasks rather than uni-
versal cross-task generalization, because building a
versatile semantic parser for all NLP tasks is over
challenging. By contrast, the approach introduced
in the next subsection aims at cross-task generaliza-
tion with limited supervision for the target tasks.
5.2 Flatten-and-Concatenation
In contrast to the semantic parser approach, which
considers the instructionsâ€™ structure and the target
problems, methods based on the neural networks
take more brutal treatment: as illustrated in Fig-
ure 3 (b)â€”instructions, regardless of their length,
structure, task types, etc., are flattened as a long

--- PAGE 7 ---
token sequence and concatenated with the input X
as a new input sequence for the models, which has
been widely adopted by the prior research (Wang
et al., 2022b; Wei et al., 2023, inter alia ). How-
ever, this naive strategy constantly results in un-
satisfactory performances when using vanilla mod-
els (Weller et al., 2020), leading to its reliance on
large-scale instruction fine-tuning, well-known as
â€œinstruction tuningâ€.
Limitations (i) Flattening and concatenating ev-
erything into a long sequence tends to ignore some
key information that humans can often capture in
the instruction (Mishra et al., 2022a; Jang et al.,
2022), such as negation (e.g., â€œ do not generate
outputs longer than 5 tokens â€), warning
(e.g., â€œ generate â€˜Dâ€™ if the question is
not answerable or youâ€™re not sure â€),
output constraints (e.g., â€œ your answer should
be in one of â€˜Aâ€™, â€˜Bâ€™, â€˜Câ€™, and â€˜Dâ€™ â€),
and so on. (ii) To let models understand the
instruction, a large number of training tasks have
to be prepared. This is similar to what happened in
the early years of deep learning in NLP: to improve
the performance of deep neural networks for a
particular task, we collect more labeled examples;
back to the instruction following, the systemâ€™s
comprehension of the instruction, unfortunately,
still exhibits a high degree of dependence on the
scale of training tasks (Chung et al., 2022).
5.3 HyperNetwork
Unlike the conventional modeling strategy that en-
codes the input sequence into the dense represen-
tation (i.e., language-to-representation), hypernet-
work follows a language-to-parameter paradigm:
as shown in Figure 3 (c), this scheme converts tex-
tual instruction into a block of model parameters
that can be further plugged into the underlying mod-
els (Ha et al., 2017; Houlsby et al., 2019; Jin et al.,
2020). As a result, hypernetwork-based instruction
modeling can better leverage the structured input
sequence by encoding the instruction and task input
separately (i.e., instruction-to-parameter, input-to-
representation), achieving stronger generalization
compared with the flatten-and-concatenation ap-
proach (Ye and Ren, 2021; Deb et al., 2022). It can
also significantly improve inference efficiency, as
concluded by recent works (Ivison et al., 2023a).
Limitations Despite the attractive attributes of
hypernetwork, its training instability and the re-
liance on architecture design (suiting the underly-ing models) are the stumbling blocks in real-world
applications (Brock et al., 2018; Ortiz et al., 2023).
5.4 Reinforcement Learning from Human
Feedback
The loss function for training LMs significantly
impacts the resulting LMsâ€™ instruction-following
performance (Tay et al., 2023). However, almost
all the aforementioned modeling strategies (except
the semantic-parser-based method) adopt a con-
ventional next token prediction loss (e.g. cross
entropy) to train the models, which tries to cap-
ture the human preference by simply comparing
the modelâ€™s generation text with the ground truth
reference. In order to directly optimize the LMs
with the supervision of human preference, recent
works utilized reinforcement learning from human
feedback (RLHF) to train the LMs (Stiennon et al.,
2020; Bai et al., 2022a; Ouyang et al., 2022).
Initial and tuned LM The first step of RLHF is
to obtain an initial LM, which is usually trained
with the flatten-and-concatenation-based modeling
strategy â€” concatenate instruction, input and all
other resources (if they exist) into one input se-
quence, and train the LM to generate the ground-
truth output (as we have introduced before). With
the initial LM as a starting point, we can copy it
to another independent parameter, which is the tar-
get LM will be continually updated in RLHF (i.e.,
tuned LM).
Prediction shift penalty As shown in Fig-
ure 3 (d), given the initial LM and its copy (the
target tuned LM), for each input sequence (e.g.,
instruction and task input), these two different LMs
will generate two outputs. After getting the gen-
eration texts of the initial and tuned LM, we can
further calculate the textual difference penalty be-
tween them:
y=Î¸(I, x)
yâˆ—=Î¸âˆ—(I, x)
rKL=KL(y, yâˆ—)
Here, the Î¸andÎ¸âˆ—represent the parameters of
initial and tuned LM, respectively. Iandxdenote
the instruction and task input; While yandyâˆ—are
the outputs of the initial and tuned LM. rKLis the fi-
nal reward (loss) of prediction shifting, and the KL
means the calculation of Kullbackâ€“Leibler (KL) di-
vergence. KL divergence is a widely adopted strat-

--- PAGE 8 ---
egy for measuring the textual difference, which can
be used as a part of the loss to penalize the tuned
LM on shifting the output substantially away from
the initial LMâ€™s generation. This prediction shift
penalty can prevent the tuned LM from fooling the
reward function to get a high reward but loses the
coherence of the generation text.
Reward function Besides the prediction shift
penalty, another part of the final reward comes
from the reward function. A reward function is
used for directly measuring how well the modelâ€™s
output aligns with human preference â€” the higher
rewards mean the output better aligns with human
preference.
A reward function is another model tuned on
human preference data, which is usually smaller
than the initial LM. It receives the instruction, task
input and modelsâ€™ outputs, and tries to predict a
reward scalar to reflect the alignment:
ralignment =rÎ¸(I, x, yâˆ—)
TherÎ¸is the reward model, which is usually a
regression model.
For training a reward model, previous works
collected a bunch of human preference data. For
example, Bai et al. (2022a) collected different out-
puts for each input instruction, and asked human
annotators to decide which output more aligned
with their preference. The training loss of reward
model can be formulated as follows:
l=âˆ’log 
Ïƒ 
rÎ¸ 
I, x, y+
âˆ’rÎ¸ 
I, x, yâˆ’
Where the Ïƒis the activation function that scales
the reward into (0,1],y+is the output preferred
by human, yâˆ’otherwise. By training on these
pairwise preference comparison data, the reward
model can directly learn to capture the human pref-
erence and make alignment reward estimation for
the RLHF.
The final training reward and inference To this
end, the final reward for RL update rule is:
r=ralignment âˆ’Î»rKL
TheÎ»here is the controlling factor.
After training with the above RL policy, the final
tuned LM can better align with human preference.
While the inference procedure of the tuned LM isactually similar to the aforementioned flatten-and-
concatenation modeling, where it receives instruc-
tion and input, and then generates the correspond-
ing output.
Limitations Compared with other modeling
strategies, RLHF requires much more expensive
human efforts because of collecting the preference
data, especially when the preference comparison
outputs are all written by humans (Ouyang et al.,
2022). Meanwhile, the performance of RLHF
highly relies on the quality of its human prefer-
ence annotations. More importantly, in some cases,
such as some open-ended creative writing tasks, dif-
ferent humans often hold high disagreement on the
preference decision due to the lack of ground-truth
output.
6 Instruction Following Dataset and
Evaluation
In this section, we shed light on an important
topic related to the instruction following, i.e., the
instruction-following datasets and the evaluation
settings for the instruction-tuned models.
6.1 Dataset
The essence of instruction following is to tame
the models by following various task instruc-
tions and responding with the corresponding de-
sired outputs. Therefore, the instruction-tuning
dataset (high-quality instruction-output pairs) is
the critical part (Wang et al., 2023c; Zhou et al.,
2023a). The current instruction-tuning datasets
can be divided into two categories according
to different annotation categories: 1) human-
annotated datasets (Â§ 6.1.1); 2) LLM-synthetic
datasets (Â§ 6.1.2).
We summarize the existing instruction-following
datasets in Table 6 of Appendix A for a better
overview.
6.1.1 Human-annotated datasets
The conventional way to create instruction-tuning
datasets is by employing human annotators, es-
pecially for those early-stage datasets, as shown
in Table 6. For example, PUBLIC POOL OF
PROMPTS (P3) (Sanh et al., 2022) and FLAN
(Wei et al., 2022a) collected multi-task instruction-
tuning datasets, where they employed human exper-
tise to design various prompts for each task. Mishra
et al. (2022b) proposed NATURAL INSTRUCTIONS ,
in which they collected more than 60 NLP tasks

--- PAGE 9 ---
with the corresponding human-written instructions;
Wang et al. (2022b) further extended this collection
into a 1.6k cross-lingual tasks scale contributed
by 88 NLP experts, namely SUPER -NATURAL IN-
STRUCTIONS .
Human-created datasets are mostly high-quality
(with minimum annotation errors) but require labo-
rious human efforts and expensive time consump-
tion. More importantly, humans suffer from limited
diversity â€” itâ€™s really challenging for humans to
brainstorm diverse and novel tasks; thus, the task
scale of human-annotated datasets is usually lim-
ited by human annotators (e.g., expertise level and
collaboration scheme of humans).
6.1.2 LLM-synthetic datasets
Since LLMs have shown their superior annotation
quality on various NLP tasks (He et al., 2023;
Pan et al., 2023), tons of recent works tried to
employ LLMs (e.g., ChatGPT and GPT-4) in-
stead of humans on instruction-tuning dataset cura-
tion. For instance, SELF-INSTURCT (Wang et al.,
2023d) and UNNATURAL INSTRUCTIONS (Hon-
ovich et al., 2023a) utilized human-annotated in-
structions as demonstrations to guide LLMs in de-
vising novel tasks and increasing task diversity.
WIZARD LM (Xu et al., 2023a) employed an in-
struction evolution paradigm to increase instruction
complexity. DYNOSAUR (Yin et al., 2023) repur-
posed existing input-output pairs in NLP datasets
to stimulate new instructions and reduce annota-
tion costs. MUFFIN (Lou et al., 2024) prompted
the LLMs to gather different task instructions for
the same input and obtained an impressive general-
ization capacity of the tuned smaller models. Be-
sides single-turn instruction-output datasets, some
works also collected multi-turn dialogue data from
ShareGPT3, where the instructions are created by
humans (users of OpenAI API), and the responses
are from LLMs (Geng et al., 2023; Chiang et al.,
2023).
Though these LLM-synthetic datasets contained
considerable noise (e.g., incoherent instructions
and hallucination outputs), the diverse task distribu-
tion and model-preferred output patterns still ben-
efit the smaller models on instruction-following,
achieving comparable even better generalization
performance compared with human-annotated
datasets (Wang et al., 2023d,c).
In a word, the choice between human-annotated
3https://sharegpt.com/and LLM-synthetic datasets can also be regarded as
a trade-off between data quality and diversity. Pre-
vious works have concluded that both factors affect
the performance of the resulting models (Chung
et al., 2022; Longpre et al., 2023) â€” mixing human
and machine data can lead to better results (Wang
et al., 2023d; Yin et al., 2023), while there is no
concrete conclusion about which factor outweighs
the other, which highly depends on the downstream
tasks and application situations.
6.2 Evaluation
6.2.1 Different evaluation schemes
How to evaluate an instruction-tuned model is also
a crucial topic. Most traditional NLP tasks usually
have concrete criteria on the task objective, while
for instruction following, the key objective is to
tame the model to follow instructions â€” how well
the model follows instructions is highly subjective
and depends on various preferences. Therefore,
different works tend to utilize various evaluation
strategies. In this section, we list several common
evaluation settings.
Automatic metrics When testing the modelâ€™s
instruction-following performance on an evalu-
ation dataset, if this dataset has â€œground-truthâ€
outputs, then a conventional criterion is to use
those automatic evaluation metrics, such as EXACT -
MATCH (Rajpurkar et al., 2016) and ROUGE (Lin,
2004), that have been widely used for evaluating
the generation models (Mishra et al., 2022b; Wang
et al., 2022b; Wei et al., 2022a; Sanh et al., 2022;
Lou and Yin, 2023; Yin et al., 2022). However,
this naive evaluation strategy suffers from several
drawbacks: 1) it has been widely committed that
the automatic generation metrics are not perfect
and have significant biases (e.g., BLUE score has
text length bias); 2) all of these metrics are used
for showing how well the modelâ€™s prediction aligns
with pre-annotated answers, however, most real-
world user tasks are highly open-ended, and there
are probably no official ground-truth labels to cal-
culate the metrics; 3) the essence of instruction
following is to follow userâ€™s instructions and pro-
vide desired responses that can well address userâ€™s
requirements, while automatic metrics more focus
on some superficial textual patterns and lacking the
reflection on how well the response satisfies the
instructions.

--- PAGE 10 ---
Human evaluation A more reliable evaluation
method is to employ humans to decide whether a
modelâ€™s response satisfies the instruction or not.
For example, given a task instruction and a cor-
responding model output, the human evaluator
should read the instruction and decide whether this
model output is acceptable or not (reporting an ac-
ceptance ratio for the target model) (Wang et al.,
2023d,c; Lou et al., 2024); or ask humans to com-
pare two modelsâ€™ outputs and decide which one
better satisfies the instruction (pairwise compar-
ison between two models) (Chiang et al., 2023;
Geng et al., 2023; Taori et al., 2023). Since instruc-
tions are mostly complicated and contain a lot of
explicit or implicit constraints, human evaluation is
more flexible and accurate than automatic metrics
in reflecting the instruction-following capacities of
different models.
However, human evaluation is also much more
expensive, slower than automatic evaluation, and
unreproducible. Thus, most of the works only con-
duct human evaluation on a small subset of the
whole evaluation benchmark. Meanwhile, it is
mostly based on human evaluatorsâ€™ personal pref-
erences and can result in high variance between
different evaluators.
Leading LLMs as evaluators To address the
aforementioned issues of human evaluation, recent
works also tried to use LLMs (e.g., GPT-4) rather
than humans to evaluate the modelsâ€™ instruction fol-
lowing capacity, such as VicunaEval (Chiang et al.,
2023) and AlpacaEval (Taori et al., 2023). Nev-
ertheless, although LLMs are cheaper and faster,
they were found to have serious preference bias on
some superficial textual patterns or hallucinations,
e.g., GPT-4 prefers the longer texts and those re-
sponses with diverse tokens (Wang et al., 2023c).
Meanwhile, only a final preference score is usually
insufficient for a comprehensive evaluation.
In order to improve reliability, instead of letting
LLMs simply provide a preference decision, other
works tend to ask LLMs to generate comprehensive
analyses besides the final decision, such as gener-
ating the error types, locations and explanations
before concluding with the final scores (Fernandes
et al., 2023; Xu et al., 2023e). Some other works
also predefined several explainable criterion ques-
tions for the various evaluation tasks (e.g., for an
instruction â€œ Please generate at least 25 sentences â€,
define a criterion â€œ is the modelâ€™s generation at least
25 sentences? â€), that can be further verified by hu-mans or LLMs easily (i.e., doing binary classifica-
tion on these predefined criteria) (Liu et al., 2023e;
Zhou et al., 2023b). Saha et al. (2023) also asked
LLMs to first generate the criteria questions au-
tomatically according to the instructions and then
evaluate the modelâ€™s response.
6.2.2 Two branches of evaluation
Despite the various evaluation choices in the in-
struction following, they can be summarized into
two branches from our view.
Task-centric evaluation Most evaluation
datasets in this branch are based on conventional
multi-task learning, where the evaluation tasks
are mostly traditional NLP tasks, such as natural
language inference (Wei et al., 2022a; Sanh et al.,
2022). This branch aims to test LLMsâ€™ instruction-
following and problem-solving capacity, and the
main criterion here is whether the models can
correctly solve the given textual task. Therefore,
most of the evaluation settings in this branch adopt
conventional automatic metrics to reflect the task
ground-truth label alignment. Representative
benchmarks are MMLU (Hendrycks et al., 2021),
BBH (Suzgun et al., 2023), SuperNI-Test (Wang
et al., 2022b), T0-Eval (Sanh et al., 2022),
InstructEval (Chia et al., 2023), etc.
Human-centric evaluation The evaluation in-
structions in this setting are user-oriented or
dialogue-like user queries, mainly used to test how
well the modelsâ€™ responses align with human pref-
erence, especially for the safety and usefulness
of the responses (e.g., harmlessness and honesty).
Unlike the task-centric evaluation, human-centric
evaluation cares less about the ground-truth labels
since most user tasks are open-ended. Thus, this
evaluation setting is more subjective and requires
more high-level human or LLM efforts. Represen-
tative benchmarks are AlpacaFarm (Dubois et al.,
2023), VicunaEval (Chiang et al., 2023), HHH (Bai
et al., 2022b), etc.
To our knowledge, since instruction following
is a pretty wide topic that can be related to various
downstream tasks and real-world scenarios, there
is still a lack of a comprehensive evaluation setting
that can be applied to all of the target scenarios. A
more practical choice is to adopt different evalua-
tion settings according to the objectives of different
works (i.e., task-centric or human-centric).

--- PAGE 11 ---
Recipes for Instruction Following
Model-related Factors (Â§ 7.1)
â€¢ Instruction-tuned LLMs >Vanilla LLMs.
â€¢Instruction following tames LLMs to be more safe,
robust, and user-friendly.
â€¢Larger LLMs benefit more from instruction follow-
ing.
Instruction-related Factors (Â§ 7.2)
â€¢Rewriting your instruction with several epochs be-
fore it works.
â€¢Keep instruction paradigm consistent during training
and testing (e.g., abstractiveness).
â€¢Design multiple instructions for one task in different
wordings and perspectives.
â€¢Feeling exhausted about promoting diversity? Resort
to the LLMs!
â€¢ Few-shot demonstrations are useful in most cases.
Demonstration-related Factors (Â§ 7.3)
â€¢The choice of your few-shot examples matters a lot!
â€¢ Sort your examples in a decent order.
â€¢Enhance your examples with step-by-step reasoning
explanation.
â€¢Let your model exploit the input-output mapping
from the examples.
Model-Instruction Alignment (Â§ 7.4)
â€¢Better design your instructions in a modelâ€™s language
(e.g., conforming to the pertaining objectives).
Data-wise Factors (Â§ 7.5)
â€¢ Try to tune LLMs on more diverse tasks.
Table 5: The takeaways. We summarize some high-level
suggestions for successful instruction following.
7 Factors that Influence Instruction
Following Performance
Instruction following is proven to be effective in
a lot of few/zero-shot NLP tasks, but how to ex-
plain the impressive performance of instruction?
And which aspects make a successful instruction
following procedure? We categorize the factors
affecting instruction following performance into
five dimensions: model ,instruction ,demonstration ,
model-instruction interaction , and dataset . Table 5
displays a roadmap for this section, where we also
conclude the takeaways to make it easy to refer to.
7.1 Model-related Factors
7.1.1 Update model or not
As shown in Figure 1 (b), to drive LLMs to under-
stand and follow task instructions more smoothly,
a widely-adopted practice is fine-tuning LLMs
on multi-task datasets, where each task input is
equipped with a task instruction. This procedure isalso well-known as â€œinstruction tuningâ€. A lot of
works demonstrated that instruction-tuned LLMs
could better follow the instructions of unseen tasks
compared with frozen LLMs (Wei et al., 2022a;
Sanh et al., 2022).
Besides the performance gains on unseen tasks,
instruction tuning has many other benefits, such as
learning faster on the downstream tasks (Longpre
et al., 2023; Gupta et al., 2023), being more robust
to the tiny instruction perturbations (e.g., paraphras-
ing) (Weller et al., 2020; Sanh et al., 2022; Gu et al.,
2023), becoming more user-friendly (Chung et al.,
2022), and being better at following soft instruc-
tions (Wei et al., 2022a), etc.
7.1.2 Model scale
Recent works demonstrated that the model scale
significantly impacts the generalization perfor-
mance of instruction following (Chung et al., 2022;
Longpre et al., 2023; Wang et al., 2023c, inter
alia). As shown in Figure 4, the generalization
performances of each model consistently increase
when scaling up the model size. More interestingly,
when the model scale is large enough, even vanilla
LLMs can significantly outperform smaller LLMs
tuned on extensive tasks (see Flan-PaLM; vanilla
540B >8B + 1836 tasks), which probably implies
that the benefits of scaling up the model size can
outweigh dataset scaling.
However, the super-large model scale is usually
unaffordable for most research groups, and it also
leads to huge carbon emissions, making it unreal-
istic in most real-world scenarios (Strubell et al.,
2019; Schick and SchÃ¼tze, 2021c). Accordingly,
recent works began to investigate a more efficient
way to address the model scale problem, e.g., the
parameter-efficient fine-tuning (Hu et al., 2022a;
Liu et al., 2022a; Lialin et al., 2023; Jang et al.,
2023).
7.2 Instruction-related Factors
7.2.1 Instruction engineering
A common problem in instruction following is that
the pre-trained models are usually sensitive to some
subtle modifications in the instruction (Weller et al.,
2020; Efrat and Levy, 2020; Bach et al., 2022;
Mishra et al., 2022a; Gu et al., 2023) â€” even a
minor edition on instruction, such as paraphrasing
or word replacement, can lead to huge performance
variance. Therefore, modifying the wording of in-
struction before usage, namely instruction engineer-
ing, is critical for the modelsâ€™ performance.

--- PAGE 12 ---
(a) Flan-T5(b) Flan-PaLMFigure 4: The scaling trends of instruction following, including scaling model size and task numbers. We report
the cross-task generalization performances of two widely-adopted instruction-tuned LLMs, namely Flan-T5 and
Flan-PaLM (Chung et al., 2022), where the source scores mainly come from (Wei et al., 2022a; Chung et al., 2022;
Longpre et al., 2023). It is worth noting that different papers may utilize distinct evaluation benchmarks with
various metrics. To clearly summarize the scaling trends, instead of simply copying the original scores, we report
thenormalized performances in each figure (thatâ€™s why the highest performance of each figure can reach 100%).
One straightforward solution is to manually
rewrite the instruction, i.e., human instruction en-
gineering. When humans perform instruction engi-
neering, the criteria of rewriting are mostly human
intuition. For example, Mishra et al. (2022a) con-
ducted error cases analysis on GPTâ€™s instruction-
following outputs. Accordingly, they designed sev-
eral empirical rules on instruction writing and â€œre-
framedâ€ the instructions. All of these proposed
rules are based on human intuition, e.g., itemiz-
ing instructions and task decomposition. In or-
der to avoid the preference bias introduced by a
small group of humans, Bach et al. (2022) pro-
posed community-driven instruction engineering,
where they collected instructions created by various
NLP experts with different writing styles, diversi-
fying the choices of instructions. However, human
instruction engineering is time-consuming and ex-
pensive. Moreover, the human intuition on instruc-
tion designing might be subjective and sometimes
sub-optimal for the models.
To this end, automatic instruction engineering
tries to let the model figure out the better instruc-
tions automatically. Prasad et al. (2023) proposed
an edition-based method to automatically modify
the instruction. For each iteration, they edited the
instruction in phrase level to generate multiple can-didates, and then employed the target model to pre-
dict the scores of the different candidates by using
a small labeled set (i.e., calculating the ground-
truth entropy and accuracy). In doing this, Prasad
et al. (2023) achieved better performance compared
with those manually-reframed instructions (Mishra
et al., 2022a). Besides using the ground-truth score,
Gonen et al. (2023) utilized the modelâ€™s prediction
likelihood as feedback to select instruction can-
didates, which even doesnâ€™t require any labeled
instances. Deng et al. (2022) further proposed a re-
inforcement learning framework to conduct instruc-
tion engineering. Despite the superior performance,
the obvious drawback of automatic instruction en-
gineering is the poor explainability, where the re-
sulting instructions mostly violate human intuition
(e.g., some task-irrelevant sentences) (Khashabi
et al., 2022; Prasad et al., 2023), which is similar
to the soft instruction.
In a word, instruction engineering is a trade-off
procedure â€” lower explainability is the tax of bet-
ter performances. Meanwhile, instruction engineer-
ing is a highly empirical subject, and there are
no gold-standard rules/methods on it â€” different
models and tasks might require totally different in-
struction designing. Hence, we highly recommend
the community release the accompanying instruc-

--- PAGE 13 ---
tion manuals when releasing their instruction-tuned
models, thus ensuring stable and expected model
behaviours (e.g., OpenAIâ€™s cooking book4).
7.2.2 Instruction consistency
This factor considers the instructions across the
training tasks and test tasks . Keeping the instruc-
tion paradigm (e.g., abstractiveness) consistent is
crucial in instruction following. Wei et al. (2022a)
first investigated the performance impact of chang-
ing the instruction paradigm. They found that
LLMs tuned on short instructions (i.e., task names)
cannot generalize to longer sentence-style instruc-
tions ( short Ì¸â‡’long ). Similarly, Gu et al. (2023)
observed the performance dropping when chang-
ing paragraph-style instructions to shorter sentence-
style instructions at the test phase ( longÌ¸â‡’short ),
further indicating the importance of instruction con-
sistency.
Besides discrete instruction, maintaining the in-
struction paradigm is also critical for soft instruc-
tion, i.e., keeping the same-size prefix embedding
when testing on unseen tasks (Xu et al., 2022a).
Interestingly, similar results were also found in the
few-shot demonstrations (i.e., in-context learning),
where the combination of input-output pairs or the
number of demonstrations cannot be changed dur-
ing training and evaluation (Min et al., 2022a,b;
Iyer et al., 2022). These phenomena raise a con-
cern: although instruction-tuned LLMs are robust
to tiny perturbations of instructions, they are vul-
nerable when facing more significant alterations,
which is far behind human-level generalization .
7.2.3 Instruction diversity
To further improve the robustness of LLMs, espe-
cially when facing significant alterations of instruc-
tion paradigms, people try to promote instruction
diversity during the training phase â€”for the same
training task, writing multiple instructions in dif-
ferent textual expressions (e.g., different wordings
and lengths), then training LLMs on the mixture
of diverse instructions. Notably, Sanh et al. (2022)
showed that adopting instructions with diverse writ-
ing styles not only improved the model generaliza-
tion but also compensated for the limited model
scale to some extent.
Nevertheless, manually crafting instructions
with diversity is expensive and usually hard to
4https://platform.openai.com/
docs/guides/prompt-engineering/
six-strategies-for-getting-better-resultsachieve due to the human annotation bias (Huynh
et al., 2021; Parmar et al., 2023). Owing to the ex-
cellent annotation quality of LLMs (He et al., 2023;
Pan et al., 2023), a considerable number of works
began to employ models to compose innovative
instructions (Zhang et al., 2020, 2021; Honovich
et al., 2023b). Although the model-generated in-
structions have been proven to contain more noise,
benefiting from the diverse syntax structures (Ki-
taev and Klein, 2018), these instructions could
still show complementary effects with the human-
written instructions (Wang et al., 2023d). More
interestingly, Lou et al. (2024) proposed a new
instruction-following dataset paradigm, where they
employed LLMs to synthesize diverse task instruc-
tions for each input. Benefiting from this paradigm,
the tuned LMs were forced to focus more on the
instruction than the task input, achieving promising
instruction-following performance. All of these
results may imply the profitability of instruction
diversity, even at the expense of the correctness of
instructions .
7.2.4 Add demonstrations or not
Demonstrations, i.e., a couple of input-output ex-
amples, have been shown to be critical for the
expressiveness of task instructions. For example,
existing works found that adding a few positive
demonstrations in the textual instructions could re-
sult in a significant performance improvement on
the unseen tasks (Yin et al., 2022; Deb et al., 2022),
especially for the tasks occupying complex output
space (Mishra et al., 2022b; Wang et al., 2022b).
Surprisingly, Gu et al. (2023) further found that
models highly relied on few-shot demonstrations
and even abandoned other useful resources (e.g.,
detailed task definition) when demonstrations were
available. This prominence is perhaps because the
LLMs prefer to exploit the more superficial pat-
terns of the demonstrations rather than the other
complex textual expressions (Min et al., 2022b).
In other words, at present, a comprehensive frame-
work for accurately encoding pure instructions in
the absence of demonstrations or task scaling re-
mains elusive (Lou and Yin, 2023).
7.3 Demonstration-related Factors
Since few-shot demonstrations can impact the
modelâ€™s instruction following performance a lot,
recent studies investigated different factors in the
demonstrations, which can further enhance the
modelâ€™s demonstration learning efficiency.

--- PAGE 14 ---
7.3.1 The selection of demonstrations
Given an unlabeled test instance (i.e., input-only
instance waiting for the answer from the model),
and a pool of labeled training instances (i.e., input-
output pairs), how to select the better demonstra-
tions from this pool for the test instance is a funda-
mental question for in-context learning.
Liu et al. (2022b) proposed an unsupervised
demonstration selection strategy, where they uti-
lized kNN (kNearest Neighbors) to retrieve the
demonstrations with the closed embedding distance
as the test instance. The key step in the clustering-
based selection methods is the distance metrics,
such as L2 distance, cosine-similarity or mutual
information (Sorensen et al., 2022). In addition to
the clustering-based methods, another branch of
methods used the output score of models as the
selection criterion (Gonen et al., 2023; Wu et al.,
2023; Li and Qiu, 2023). For example, Nguyen
and Wong (2023) tried to select a subset Afrom
the training pool as the demonstrations by mea-
suring the modelâ€™s average performance variance
between Aand the complement set A.
Beyond the above unsupervised or weak-
supervised selection strategies, some other works
also employed supervised methods. Wang et al.
(2023a) regarded the LMs as implicit topic models,
where the LMs can generate meaningful concept
representation based on the few-shot demonstra-
tions. By training the topic models, they selected
demonstrations that could maximize the likelihood
of the given concept. Meanwhile, Zhang et al.
(2022b) regarded the demonstration selection as
a Markov decision process (Bellman, 1957) and
proposed a reinforcement learning model via Q-
learning (Jang et al., 2019).
7.3.2 The order of demonstrations
Even with the same set of demonstrations, the dif-
ference in the example order can also impact the
modelâ€™s in-context learning performance. Zhao
et al. (2021) emphasized that the GPT-3 is sensitive
to the order of the demonstrations, and they conjec-
tured that this sensitivity potentially comes from
Recency Bias â€” the tendency to repeat answers
that appear towards the end of the prompt. Lu et al.
(2022) further conducted comprehensive experi-
ments and found that, along with GPT-3, various
models suffer from order sensitivity.
To this end, recent works proposed several meth-
ods to sort a â€œsuitableâ€ example order for the
LMs. For example, based on the Recency Bias ,Liu et al. (2022b) calculated the embedding sim-
ilarity between the demonstrations and the target
input, those more similar examples were put closer
(right more) to the input. Lu et al. (2022) proposed
several entropy-based metrics to search for the best
demonstration order.
7.3.3 Reasoning steps augmentation
Beyond the standard input-by-output demonstra-
tions, augmenting in-context examples with reason-
ing steps is found helpful for the modelâ€™s perfor-
mance, especially for the super-large models.
Wei et al. (2022b) proposed chain-of-thoughts
(CoT), where they inserted some human-written in-
termediate reasoning steps (i.e., rationale) between
input and output of in-context demonstration. By
doing so, when predicting the target output, the
models can generate intermediate reasoning steps
as well, thus enhancing the performance on rea-
soning tasks (e.g., math word problems) and the
explainability of LMs. In addition to the human-
written CoT, Xu et al. (2023c) also found that the
CoT synthesized by larger models can assist the
smaller models. Based on the promising results
of adopting CoT, more advanced variations were
proposed for more accurate reasoning, such as
program-of-thoughts (PoT) (Chen et al., 2022a),
tree-of-thoughts (ToT) (Yao et al., 2023), graph-of-
thoughts (GoT) (Besta et al., 2024), and CoT with
self-consistency decoding augmentation (Wang
et al., 2023b).
However, similar to the demonstration sensitiv-
ity, different CoT writing styles can also result in
performance variance. Therefore, in contrast to
the human-craft CoT (i.e., few-shot CoT), Zhang
et al. (2022c) proposed Auto-CoT (i.e., zero-shot
CoT), where they added a â€œLetâ€™s think step by
stepâ€ into the prompt and let the models gener-
ate CoTs themselves. Afterwards, more and more
variations of Auto-CoT were proposed to address
more complicated reasoning tasks. For example,
Self-Ask (Press et al., 2023) asked the model to
first generate several questions regarding the input
and then answer these questions by the model itself
â€” these self-generated contexts were further used as
the reasoning rationales to help answer the original
input. Similarly, Least-to-Most (Zhou et al., 2022)
asked the model to decompose an origin complex
input into several sub-questions and answer them
subsequently, which can be used as the rationales
as well.

--- PAGE 15 ---
7.3.4 Emphasizing input-output mappings
For in-context learning, the model usually cannot
directly â€œlearnâ€ the input-output mapping from the
given examples because there is no parameter up-
date for the models. Therefore, one issue of in-
context learning is that, when conducting instruc-
tion following, the demonstrations are not necessar-
ily needed for the model to solve the task (i.e., even
without the few-shot demonstrations, the model
can still make predictions). Min et al. (2022b) also
found that the model is more likely to â€œcopyâ€ the
output candidate from the demonstrations, instead
of truly learning the underlying mapping.
To this end, Wei et al. (2023) proposed symbol
tuning. Different from conventional instruction fol-
lowing, which tunes the models to follow input-by-
output demonstrations to complete the target input,
symbol tuning uses some unrelated symbols to re-
place the origin outputs of the demonstrations. For
example, the origin output space of the demonstra-
tions might be â€œpositiveâ€ and â€œnegativeâ€; symbol
tuning uses â€œFooâ€ and â€œBarâ€ instead. After losing
the semantics of the output spaces, there are no
prior label biases (Zhao et al., 2021) for the mod-
els to rely on to make the final prediction, so the
models are forced to figure out the input-output
mapping in the context.
7.4 Model-Instruction Alignment
This factor refers to making the procedure of in-
struction following better conform to the prefer-
ence of LLMs. One aspect is the training ob-
jective. Since the current instruction following
paradigm mainly employs the LLMs as the sys-
tem backbone, one of the potential explanations for
why LLM-oriented Instructions (i.e., prompt) can
work is that prompt aligns well with the pretrain-
ing objectiveâ€”language modelingâ€”and activates
the task-specific knowledge of the LLMs. Some
existing works demonstrated the importance of con-
forming to the pretraining objective of LLMs when
doing instruction following (Schick and SchÃ¼tze,
2021c; Tay et al., 2023), such as recalling language
modeling objectives in fine-tuning phase (Iyer et al.,
2022). Another aspect of model preference align-
ment is the way of designing instructions: that
is, converting the instructions into model-oriented
styles (Deng et al., 2022). For example, using soft
instructions (i.e., continuous embedding) instead of
human-understandable discrete instructions (Lester
et al., 2021; Liu et al., 2021; Ye et al., 2022a). Itis consistent with the empirical guidelines estab-
lished in the field of prompt engineering, which em-
phasize the significance of model-oriented prompt
design.5Despite the performance profits, it is
still controversial whether it is worthwhile to con-
vert the original human-oriented instructions into a
LLM-oriented style, because it always impairs the
interpretability of instructions and is highly con-
trary to human intuition (Khashabi et al., 2022;
Webson and Pavlick, 2022; Prasad et al., 2023).
7.5 Data-wise Factor: Task Scale
The task scale often refers to the number of differ-
ent training task categories in the dataset. Since
â€œdata-wise factorâ€ also includes the scale of training
instances, Wang et al. (2022b) investigated the im-
pact of both task and instance scales. They found
that instance scale (fixed task number, increasing
the number of instances per task) can only bring a
limited performance boost, while task scale is the
key factor for instruction following, in line with
the observations of other works (Wei et al., 2022a;
Chung et al., 2022). As illustrated in Figure 4, the
same-size model with more tuning tasks usually
gains better performance. However, the perfor-
mance improvement of scaling up tasks is unstable,
especially when the model size is too small (e.g.,
0.08B Flan-T5). This phenomenon aligns with the
discussion in Â§ 7.1, we can draw a similar conclu-
sion here: the profits of the task scale are highly
governed by the model scale.
7.6 Main Takeaway: Dual-Track Scaling
Among all the factors discussed in this section,
scaling is arguably the core factor that leads to the
success of instruction following. Prior to LLM-
based instruction following, scaling was mainly
for deep learning models: from single-layer neu-
ral nets to multi-layer perceptions, from convo-
lutional/recurrent neural networks to deep-layer
transformers (Hochreiter and Schmidhuber, 1997;
LeCun et al., 1998; Vaswani et al., 2017; Devlin
et al., 2019b). Along with the pretraining of mas-
sive raw text data, the ever-increasing models are
expected to have encoded a vast amount of generic-
purpose knowledge (Zhou et al., 2023a). In the
era of instruction following, where the commu-
nity is more interested in cross-task generalization,
merely scaling LLMs seems not enough. Thus, re-
searchers take a parallel scaling: to collect more
5Using prefix prompts for the auto-regressive LMs, while
using cloze prompts for the masked LMs (Liu et al., 2023a).

--- PAGE 16 ---
and more training tasks and labeled examples for
each. We interpret this as a dual-track scaling .
Overall, this dual-track scaling jointly seeks super-
vision to solve new tasksâ€”the supervision either
comes from LLMsâ€™ pretraining or substantial train-
ing tasks. Despite its progress, some notable chal-
lenges remain in this area, which we will discuss
in the next section.
8 Challenges and Future Directions
Despite all the aforementioned benefits of instruc-
tion, tons of under-explored challenges remain in
this area. In this section, we list several chal-
lenges related to the instruction following, which
are worthwhile for future research to investigate.
8.1 The Tax of Instruction Alignment
The instruction following aims at taming the mod-
els to better assist humans in real-world tasks;
therefore, in addition to pursuing ultimate perfor-
mance, inference-time safety is also a crucial aspect
for the instruction-tuned models (i.e., instruction
alignment). Ouyang et al. (2022) defined â€œalign-
mentâ€ with three criteria â€” Helpful ,Honest , and
Harmless (HHH), which has been widely consid-
ered by the previous instruction tuning models and
datasets (Bai et al., 2022b; Yin et al., 2023; Wang
et al., 2023d; Lou et al., 2024). However, align-
ment can also bring â€œtaxâ€ to the instruction-tuned
models. For example, Bekbayev et al. (2023) found
that the well-aligned answers provided in the in-
struction following datasets can drop the modelâ€™s
performance a lot on various task benchmarks. This
implies a trade-off between performance and safety
for instruction following, which requires careful
consideration.
8.2 Learning Negated Information
Negation is the common linguistic property and has
been found to be crucial for various NLP tasks, e.g.,
NLI (Naik et al., 2018; Kassner and SchÃ¼tze, 2020).
Specific to instruction following, negation denotes
anythings-to-avoid information of in-context in-
structions, including negated requirements (e.g.,
â€œavoid using stop words â€) and negative demon-
strations (i.e., some wrong examples). Although
humans can learn a lot from the negation (Dudschig
and Kaup, 2018), existing works found LLMs often
fail to follow the negated instructions; some nega-
tions can even drop modelsâ€™ performance (Li et al.,
2022b; Jang et al., 2022; Mishra et al., 2022a).Since negation has increasingly become a chal-
lenge in instruction following, we provide several
hints to inspire future work. One potential solu-
tion is unlikelihood training (Hosseini et al., 2021;
Ye et al., 2022b), which trains the LLMs to min-
imize the ground truth probability when negated
instructions are conditioned. Besides, Yin et al.
(2022) proposed pretraining the LMs on the neg-
ative demonstrations with maximizing likelihood
objective to exploit the useful information in the
negation. Some other methods, such as contrast-
consistent projection (Burns et al., 2023) and n-
gram representations (Sun and Lu, 2022), also pro-
vided insights into tackling this problem.
8.3 Adversarial Instruction Attacks
Though most of the instruction-tuned LMs can
align well with human preferences and provide
harmless responses, recent works found that they
could easily be attacked â€” the modelâ€™s response
can be manipulated by using simple prompting
strategies. Kang et al. (2023) designed several
prompts to trigger the LLMs to generate malicious
content. For example, instead of directly provid-
ing malicious instruction with obviously harmful
intentions, they split the instruction into several
pieces (each piece itself doesnâ€™t trigger the LLMsâ€™
defence mechanism). In doing this, those powerful
preference-aligned LLMs, such as ChatGPT and
InstructGPT, were successfully fooled and gener-
ated harmful content. Li et al. (2023b) also found
that the retrieval-augmented generation models can
be easily attacked by injecting adversarial ques-
tions into the retrieved context. Besides attacking
the instruction-tuned LLMs, Wan et al. (2023) con-
cluded that LLMs can also be attacked during in-
struction following. Based on the clean instances,
they automatically created a few poisoned exam-
ples to train the LLMs and found that the resulting
LLMs could be manipulated by using some trigger
words.
Since instruction-tuned LLMs have been applied
to various real-world scenarios, such as web agents
and search engines (Deng et al., 2023; Xie et al.,
2024a), the safety of LLMsâ€™ generations is becom-
ing more urgent. Simply conducting preference
alignment or content filtering seems to be insuf-
ficient, especially for those super-strong LLMs.
Thus, developing efficient defence methods is nec-
essary for the current instruction-tuned models.
Meanwhile, further deep analyses of LLMsâ€™ vulner-

--- PAGE 17 ---
ability are also critical, potentially providing more
insights into the defence.
8.4 Explainability of Instruction Following
As we have mentioned in Â§ 7, to achieve a promis-
ing cross-task performance, one of the critical
factors is to convert the Human-oriented Instruc-
tions intoLLM-oriented Instructions , i.e., making
the instructions conform to the modelâ€™s preference.
Numerous previous works have verified the effec-
tiveness of catering to the modelâ€™s preference in
designing instructions, e.g., using the modelâ€™s per-
plexity in choosing appropriate instructions (Gonen
et al., 2023). Despite the performance gains, the
resulting instructions consistently violate human
intuitions and show worrying reliability, such as
some semantically incoherent, task-irrelevant, or
even misleading instructions (Khashabi et al., 2022;
Prasad et al., 2023). These results prove the con-
flict between performance profits and the human
interpretability of instructions, which is tricky to
trade-off.
Although Mishra et al. (2022a) demonstrated
that it is possible to maintain both the faithfulness
and effectiveness of instructions, manual rewriting
requires laborious human efforts. Therefore, one
of the future trends is to investigate how to auto-
matically rephrase the instructions, in a way that
matches both human and model preferences.
8.5 Learning to Follow Instruction rather
than Merely Generating Y
Multi-task instruction following is becoming a fun-
damental practice in the current instruction follow-
ing paradigm. However, there are two issues in
such a learning paradigm: (i) it relies on training
on massive labeled examples to learn the instruc-
tions, which is still expensive and unrealistic for
using large-scale LLMs; (ii) although the ultimate
goal of instruction following is learning to follow
instructions by observing various training tasks, the
current training objective is still the conventional
maximum likelihood of reference outputs. This
implicit instruction following objective can lead to
sub-optimal optimization (i.e., LLMs can learn to
generate YforXwithout really understanding the
meaning of instructions I).
To this end, one desired future direction is to
evolve a new learning objective to help LLMs ex-
plicitly learn to follow instructions, which might al-
leviate the reliance on large-scale labeled instances.
Moreover, a more ambitious and challenging ideais to drive the system to follow instructions without
additional tuning on the labeled examples of any
specific tasks (Ye et al., 2023), which is somehow
similar to semantic parser-based paradigm (Â§ 5).
8.6 Multi-Lingual Instruction Following
Intuitively, the instruction following is language-
agnostic capacity for the language models, which
means that it is also possible for the multi-lingual
language models to follow the same semantic in-
structions with different languages. For exam-
ple, Kew et al. (2023) found LLMs tuned with
more than three languages exhibit stronger instruc-
tion following capacity, implying the benefits of
multi-lingual instruction tuning. Unfortunately,
most of the current open-sourced instruction fol-
lowing datasets and foundation models are English-
centric (as shown in Table 6). Therefore, the re-
lease of high-quality multi-lingual instruction tun-
ing datasets (with pair translation) should be valu-
able for future research, as also mentioned by Peng
et al. (2023).
9 Instruction-related Applications
In addition to the main body of our paper, we also
survey some popular instruction-related application
directions to inspire future board-wide utilization
for instruction following.
9.1 Human-Computer Interaction
Textual instructions can be naturally regarded as a
human-computer interaction method. Numerous
previous works employed natural language instruc-
tions to guide the computer to perform various real-
world tasks.
For the non-NLP (multi-modal) tasks, most fo-
cused on environment-grounded language learn-
ing, i.e., driving the agent to associate natural
language instructions with the environments and
make corresponding reactions, such as selecting
mentioned objects from an image/video (Matuszek
et al., 2012; Krishnamurthy and Kollar, 2013; Puig
et al., 2018), following navigational instructions
to move the agent (Tellex et al., 2011; Kim and
Mooney, 2012; Chen, 2012; Artzi and Zettlemoyer,
2013; Bisk et al., 2016), plotting corresponding
traces on a map (V ogel and Jurafsky, 2010; Chen
and Mooney, 2011), playing soccer/card games
based on given rules (Kuhlmann et al., 2004; Eisen-
stein et al., 2009; Branavan et al., 2011; BabeÂ¸ s-
Vroman et al., 2012; Goldwasser and Roth, 2011),

--- PAGE 18 ---
generating real-time sports broadcast (Chen and
Mooney, 2008; Liang et al., 2009), controlling
software (Branavan et al., 2010), and querying ex-
ternal databases (Clarke et al., 2010), etc. Mean-
while, instructions are also widely adapted to help
communicate with the system in solving NLP
tasks, e.g., following instructions to manipulate
strings (Gaddy and Klein, 2019), classifying emails
based on the given explanations (Srivastava et al.,
2017, 2018), and text-to-code generation (Acqua-
viva et al., 2022).
Recently, a growing body of research tended to
design the human-computer communication proce-
dure in an iterative andmodular manner (Dwivedi-
Yu et al., 2022; Chakrabarty et al., 2022). For
example, Li et al. (2020) built a system to help
the users tackle daily missions (e.g., ordering cof-
fee or requesting Uber). Benefiting from a user-
friendly graphical interface, the system can iter-
atively ask questions about the tasks, and users
can continually refine their instructions to avoid
unclear descriptions or vague concepts. As it is
usually hard for non-expert users to write sufficient
instructions in one shot, adapting an iterative and
modular paradigm in designing instruction-based
AI systems can help guide the users to enrich the
task instruction step by step. Thus, this paradigm
efficiently relieves the thinking demands of users
and leads to a more user-oriented system (Mishra
and Nouri, 2023). Due to its practical values, we
emphasize the importance of this branch of work
in this paper.
9.2 Data and Feature Augmentation
Task instructions are regarded as indirect supervi-
sion resources where sometimes superficial and
assertive rules are embedded. These rules are also
known as labeling functions that can be directly
applied for annotations.6Therefore, some exist-
ing works also employed the instruction as a dis-
tant supervision to perform data or feature aug-
mentation (Srivastava et al., 2018; Hancock et al.,
2018; Ye et al., 2020). For instance, Srivastava
et al. (2017) used a semantic parser to convert nat-
ural language explanations into logical forms, and
applied them on all instances in the dataset to gen-
erate additional binary features. Wang et al. (2020)
utilized the label explanations to annotate the raw
corpus automatically and trained the classifier on
6For example, if â€œa very fair priceâ€ is sentiment-positive,
every sentence with a similar adj-noun collocation as â€œfair
priceâ€ will be positive as well.the resulting noisy data.
Besides the straightforward augmentation, Su
et al. (2023) further used the task instruction to en-
rich the model representation and achieved strong
cross-task generalization. Specifically, they trained
an embedding model (a single encoder) on the di-
verse instruction datasets with contrastive learning,
and then used this model to produce task-specific
representations based on the instruction for the
downstream unseen tasks.
9.3 Generalist Language Models
According to the definition of Artificial General In-
telligence (AGI), the â€œgeneralist modelâ€ is usually
a system that can be competent for different tasks
and scalable in changeable contexts, which shall
go far beyond the initial anticipations of its cre-
ators (Wang and Goertzel, 2007; Goertzel, 2014).
While specific to the NLP domain, a generalist lan-
guage model is supposed to be an excellent multi-
task assistant, that is skilled in handling a variety
of real-world NLP tasks and different languages, in
a completely zero/few-shot manner (Arivazhagan
et al., 2019; Pratap et al., 2020; Wei et al., 2022a).
As numerous existing works demonstrated the in-
credible power of using instructions in cross-task
generalization (Wei et al., 2022a; Sanh et al., 2022;
Mishra et al., 2022b; Wang et al., 2022b; Chung
et al., 2022, inter alia ), the instruction is likely to
become a breakthrough in achieving this ultimate
goal.
Notably, the recent remarkable applications of
instructions, namely InstructGPT, ChatGPT, and
GPT-4, also indicated a big step towards building
generalist language models. For example, during
the pretraining of LLama-2, Touvron et al. (2023)
utilized the idea of context distilling to inculcate
instructions within LLMs, thus addressing the in-
consistency issue of instruction following in the
multi-turn dialogue situation. OpenAI GPT-series
adopt RLHF to align the modelâ€™s preference with
human instructions, where feedback supervision
plays a big role. Although the answer to â€œ Is instruc-
tion or human feedback, that contributes more to
the performance of ChatGPT? â€ remains ambiguous
and needs further investigation, we introduce some
recent works highlighting the critical role of instruc-
tion following. For example, Chung et al. (2022)
conducted extensive experiments to evaluate the
human-preference alignments of PaLM (Chowdh-
ery et al., 2023). They found that, even without any

--- PAGE 19 ---
human feedback, the instruction following signifi-
cantly reduced the toxicity in the open-ended gen-
erations of PaLM, such as gender and occupation
bias. In addition, some other works also solely em-
ployed creative instructions instead of human feed-
back and achieved notable cross-task results (Bai
et al., 2022b; Honovich et al., 2023a; Wang et al.,
2023d). Furthermore, as the knowledge conflict
problem of LLMs has a significant impact on the
applications of instruction-tuned models (Xie et al.,
2024a), in order to make the LLMs more general-
ist and useful in the real world, recent works also
utilized the idea of the instruction following to en-
hance the retrieval-augmented language models,
and vice versa, improve the instructions by adopt-
ing retrieved knowledge (Lin et al., 2023).
10 Conclusion
This survey summarizes the existing literature on
instruction following, providing a comprehensive
overview of the field, including instruction tax-
onomies, modeling strategies, and key aspects of in-
struction utilization. It also addresses unique chal-
lenges and offers hints for future research. Unlike
previous works, we go beyond the limited scope of
modern instruction followingâ€”we trace the studies
of instruction following back to the early stage of
machine learning, and explore textual instruction
as an indirect supervision for LLMs. To our knowl-
edge, this is the first extensive survey on instruc-
tion following. Overall, we aim to offer valuable
insights and inspire further in-depth research in this
area.
References
Samuel Acquaviva, Yewen Pu, Marta Kryven,
Theodoros Sechopoulos, Catherine Wong,
Gabrielle E. Ecanow, Maxwell I. Nye, Michael Henry
Tessler, and Josh Tenenbaum. 2022. Communicating
natural programs to humans and machines. In
Advances in Neural Information Processing Systems
35: Annual Conference on Neural Information
Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9,
2022 .
Yuvanesh Anand, Zach Nussbaum, Brandon Duder-
stadt, Benjamin Schmidt, and Andriy Mulyar. 2023.
Gpt4all: Training an assistant-style chatbot with large
scale data distillation from gpt-3.5-turbo. https:
//github.com/nomic-ai/gpt4all .
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and
Donald Metzler. 2022. Ext5: Towards extreme multi-
task scaling for transfer learning. In The Tenth In-
ternational Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 . Open-
Review.net.
Naveen Arivazhagan, Ankur Bapna, Orhan Firat,
Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,
Mia Xu Chen, Yuan Cao, George F. Foster, Colin
Cherry, Wolfgang Macherey, Zhifeng Chen, and
Yonghui Wu. 2019. Massively multilingual neural
machine translation in the wild: Findings and chal-
lenges. CoRR , abs/1907.05019.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics , 1:49â€“62.
Monica BabeÂ¸ s-Vroman, James MacGlashan, Ruoyuan
Gao, Kevin Winner, Richard Adjogah, Marie des-
Jardins, Michael Littman, and Smaranda Muresan.
2012. Learning to interpret natural language instruc-
tions. In Proceedings of the Second Workshop on
Semantic Interpretation in an Actionable Context ,
pages 1â€“6, MontrÃ©al, Canada. Association for Com-
putational Linguistics.
Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert
Webson, Colin Raffel, Nihal V . Nayak, Abheesht
Sharma, Taewoon Kim, M Saiful Bari, Thibault
Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,
Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gun-
jan Chhablani, Han Wang, Jason Fries, Maged Al-
shaibani, Shanya Sharma, Urmish Thakker, Khalid
Almubarak, Xiangru Tang, Dragomir Radev, Mike
Tian-jian Jiang, and Alexander Rush. 2022. Prompt-
Source: An integrated development environment and
repository for natural language prompts. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics: System Demonstra-
tions , pages 93â€“104, Dublin, Ireland. Association for
Computational Linguistics.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom B.
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Benjamin Mann, and Jared Kaplan. 2022a. Train-
ing a helpful and harmless assistant with rein-
forcement learning from human feedback. CoRR ,
abs/2204.05862.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,

--- PAGE 20 ---
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosiute, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, NoemÃ­ Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022b. Constitutional AI: harmless-
ness from AI feedback. CoRR , abs/2212.08073.
Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, and
James Yamazaki. 2023. The poison of alignment.
ArXiv preprint , abs/2308.13449.
Richard Bellman. 1957. A markovian decision process.
Journal of mathematics and mechanics , pages 679â€“
684.
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gersten-
berger, Michal Podstawski, Lukas Gianinazzi, Joanna
Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-
otr Nyczyk, et al. 2024. Graph of thoughts: Solving
elaborate problems with large language models. In
Proceedings of the AAAI Conference on Artificial
Intelligence , pages 17682â€“17690.
Yonatan Bisk, Deniz Yuret, and Daniel Marcu. 2016.
Natural language communication with robots. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 751â€“761, San Diego, California. Association
for Computational Linguistics.
S.R.K. Branavan, David Silver, and Regina Barzilay.
2011. Learning to win by reading manuals in a
Monte-Carlo framework. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies , pages
268â€“277, Portland, Oregon, USA. Association for
Computational Linguistics.
S.R.K. Branavan, Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 1268â€“
1277, Uppsala, Sweden. Association for Computa-
tional Linguistics.
Andrew Brock, Theodore Lim, James M. Ritchie, and
Nick Weston. 2018. SMASH: one-shot model archi-
tecture search through hypernetworks. In 6th Inter-
national Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings . OpenRe-
view.net.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Collin Burns, Haotian Ye, Dan Klein, and Jacob Stein-
hardt. 2023. Discovering latent knowledge in lan-
guage models without supervision. In The Eleventh
International Conference on Learning Representa-
tions .
Thomas P Carpenter, Elizabeth Fennema, and Megan L
Franke. 1996. Cognitively guided instruction: A
knowledge base for reform in primary mathematics
instruction. The elementary school journal .
Tuhin Chakrabarty, Vishakh Padmakumar, and He He.
2022. Help me write a poem: Instruction tuning as a
vehicle for collaborative poetry writing. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 6848â€“6863,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
David Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
430â€“439, Jeju Island, Korea. Association for Compu-
tational Linguistics.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language ac-
quisition. In Machine Learning, Proceedings of the
Twenty-Fifth International Conference (ICML 2008),
Helsinki, Finland, June 5-9, 2008 , volume 307 of
ACM International Conference Proceeding Series ,
pages 128â€“135. ACM.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelli-
gence, AAAI 2011, San Francisco, California, USA,
August 7-11, 2011 . AAAI Press.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022a. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. ArXiv preprint ,
abs/2211.12588.
Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,
Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and
Huajun Chen. 2022b. Knowprompt: Knowledge-
aware Prompt-tuning with Synergistic Optimization
for Relation Extraction. In Proceedings of the ACM
Web Conference 2022 , pages 2778â€“2788.

--- PAGE 21 ---
Yew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-
janya Poria. 2023. Instructeval: Towards holistic
evaluation of instruction-tuned large language mod-
els.ArXiv preprint , abs/2306.04757.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2023. Palm: Scaling language mod-
eling with pathways. J. Mach. Learn. Res. , 24:240:1â€“
240:113.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling Instruction-Finetuned Language Mod-
els.ArXiv preprint , abs/2210.11416.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the worldâ€™s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning , pages 18â€“27, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Mike Conover, Matt Hayes, Matt Mathur, Xiangrui
Meng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick
Wendell, and Patrick Zaharia. 2023. Hello dolly: De-
mocratizing the magic of chatgpt with open models.
Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.
2021. Template-based named entity recognition us-
ing BART. In Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021 , pages
1835â€“1845, Online. Association for Computational
Linguistics.
Budhaditya Deb, Ahmed Hassan Awadallah, and Guo-
qing Zheng. 2022. Boosting natural language gener-
ation from instructions with meta-learning. In Pro-ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6792â€“
6808, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-
han Wang, Han Guo, Tianmin Shu, Meng Song, Eric
Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing
discrete text prompts with reinforcement learning.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
3369â€“3391, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,
Samual Stevens, Boshi Wang, Huan Sun, and Yu Su.
2023. Mind2web: Towards a generalist agent for the
web. In Advances in Neural Information Processing
Systems 36: Annual Conference on Neural Informa-
tion Processing Systems 2023, NeurIPS 2023, New
Orleans, LA, USA, December 10 - 16, 2023 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019a. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171â€“4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019b. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171â€“4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,
Shengding Hu, Zhiyuan Liu, Maosong Sun, and
Bowen Zhou. 2023. Enhancing chat language models
by scaling high-quality instructional conversations.
InProceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023 , pages 3029â€“
3051. Association for Computational Linguistics.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, et al. 2023.
A Survey on In-Context Learning. ArXiv preprint ,
abs/2301.00234.
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi
Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,
Percy Liang, and Tatsunori B. Hashimoto. 2023. Al-
pacafarm: A simulation framework for methods that
learn from human feedback. In Advances in Neural
Information Processing Systems 36: Annual Confer-
ence on Neural Information Processing Systems 2023,
NeurIPS 2023, New Orleans, LA, USA, December 10
- 16, 2023 .

--- PAGE 22 ---
Carolin Dudschig and Barbara Kaup. 2018. How does
â€œnot leftâ€ become â€œrightâ€? electrophysiological evi-
dence for a dynamic conflict-bound negation process-
ing account. Journal of Experimental Psychology:
Human Perception and Performance , 44(5):716.
Jane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria
Lomeli, Patrick Lewis, Gautier Izacard, Edouard
Grave, Sebastian Riedel, and Fabio Petroni. 2022.
Editeval: An instruction-based benchmark for text
improvements. ArXiv preprint , abs/2209.13331.
Avia Efrat and Omer Levy. 2020. The turking test: Can
language models understand instructions? ArXiv
preprint , abs/2010.11982.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing , pages 958â€“967, Singapore.
Association for Computational Linguistics.
Elizabeth Fennema, Thomas P Carpenter, Megan L
Franke, Linda Levi, Victoria R Jacobs, and Susan B
Empson. 1996. A longitudinal study of learning to
use childrenâ€™s thinking in mathematics instruction.
Journal for research in mathematics education .
Patrick Fernandes, Daniel Deutsch, Mara Finkelstein,
Parker Riley, AndrÃ© FT Martins, Graham Neubig,
Ankush Garg, Jonathan H Clark, Markus Freitag,
and Orhan Firat. 2023. The devil is in the errors:
Leveraging large language models for fine-grained
machine translation evaluation. In Proceedings of the
Eighth Conference on Machine Translation , pages
1066â€“1083.
David Gaddy and Dan Klein. 2019. Pre-learning en-
vironment representations for data-efficient neural
instruction following. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics , pages 1946â€“1956, Florence, Italy. Asso-
ciation for Computational Linguistics.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816â€“3830, Online. Association for Computa-
tional Linguistics.
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-
lace, Pieter Abbeel, Sergey Levine, and Dawn Song.
2023. Koala: A dialogue model for academic re-
search. Blog post.
Ben Goertzel. 2014. Artificial General Intelligence:
Concept, State of The Art, and Future Prospects.
Journal of Artificial General Intelligence , 5(1):1.
Dan Goldwasser and Dan Roth. 2011. Learning from
natural instructions. In IJCAI 2011, Proceedings of
the 22nd International Joint Conference on ArtificialIntelligence, Barcelona, Catalonia, Spain, July 16-22,
2011 , pages 1794â€“1800. IJCAI/AAAI.
Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,
and Luke Zettlemoyer. 2023. Demystifying prompts
in language models via perplexity estimation. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023, Singapore, December 6-10,
2023 , pages 10136â€“10148. Association for Computa-
tional Linguistics.
Jiasheng Gu, Hongyu Zhao, Hanzi Xu, Liangyu Nie,
Hongyuan Mei, and Wenpeng Yin. 2023. Robust-
ness of learning from task instructions. In Findings
of the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
13935â€“13948. Association for Computational Lin-
guistics.
Himanshu Gupta, Saurabh Arjun Sawant, Swaroop
Mishra, Mutsumi Nakamura, Arindam Mitra, San-
tosh Mashetty, and Chitta Baral. 2023. Instruction
tuned models are quick learners. ArXiv preprint ,
abs/2306.05539.
Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri,
Maxine Eskenazi, and Jeffrey Bigham. 2022. In-
structDial: Improving zero and few-shot general-
ization in dialogue through instruction tuning. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages 505â€“
525, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
David Ha, Andrew M. Dai, and Quoc V . Le. 2017.
Hypernetworks. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings . OpenReview.net.
Braden Hancock, Paroma Varma, Stephanie Wang, Mar-
tin Bringmann, Percy Liang, and Christopher RÃ©.
2018. Training classifiers with natural language ex-
planations. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1884â€“1895, Mel-
bourne, Australia. Association for Computational
Linguistics.
Xingwei He, Zhenghao Lin, Yeyun Gong, A Jin, Hang
Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,
Weizhu Chen, et al. 2023. Annollm: Making large
language models to be better crowdsourced annota-
tors. ArXiv preprint , abs/2303.16854.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2021. Measuring massive multitask language
understanding. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021 . OpenReview.net.
Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
short-term memory. Neural computation , 9(8):1735â€“
1780.

--- PAGE 23 ---
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2023a. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), ACL 2023, Toronto, Canada, July 9-14,
2023 , pages 14409â€“14428. Association for Computa-
tional Linguistics.
Or Honovich, Uri Shaham, Samuel R. Bowman, and
Omer Levy. 2023b. Instruction induction: From few
examples to natural language task descriptions. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), ACL 2023, Toronto, Canada, July 9-14,
2023 , pages 1935â€“1952. Association for Computa-
tional Linguistics.
Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R De-
von Hjelm, Alessandro Sordoni, and Aaron Courville.
2021. Understanding by understanding not: Model-
ing negation in language models. In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1301â€“1312,
Online. Association for Computational Linguistics.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In Pro-
ceedings of the 36th International Conference on Ma-
chine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA , volume 97 of Proceedings
of Machine Learning Research , pages 2790â€“2799.
PMLR.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022a. Lora: Low-rank adaptation of
large language models. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,
Noah A. Smith, and Mari Ostendorf. 2022b. In-
context learning for few-shot dialogue state tracking.
InFindings of the Association for Computational
Linguistics: EMNLP 2022 , pages 2627â€“2643, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Jie Huang and Kevin Chen-Chuan Chang. 2023. To-
wards reasoning in large language models: A survey.
InFindings of the Association for Computational
Linguistics: ACL 2023, Toronto, Canada, July 9-14,
2023 , pages 1049â€“1065. Association for Computa-
tional Linguistics.
Jessica Huynh, Jeffrey Bigham, and Maxine Eskenazi.
2021. A Survey of NLP-related Crowdsourcing Hits:
What Works and What Does Not. ArXiv preprint ,
abs/2111.05241.
Hamish Ivison, Akshita Bhagia, Yizhong Wang, Han-
naneh Hajishirzi, and Matthew E. Peters. 2023a.HINT: hypernetwork instruction tuning for efficient
zero- and few-shot generalisation. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
11272â€“11288. Association for Computational Lin-
guistics.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Nathan Lambert, Matthew Peters, Pradeep Dasigi,
Joel Jang, David Wadden, Noah A Smith, Iz Belt-
agy, et al. 2023b. Camels in a changing climate:
Enhancing lm adaptation with tulu 2. ArXiv preprint ,
abs/2311.10702.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, DÃ¡niel Simig, Ping Yu, Kurt Shus-
ter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.
2022. OPT-IML: Scaling Language Model Instruc-
tion Meta Learning through the Lens of Generaliza-
tion. ArXiv preprint , abs/2212.12017.
Beakcheol Jang, Myeonghwi Kim, Gaspard Harerimana,
and Jong Wook Kim. 2019. Q-learning algorithms: A
comprehensive classification and applications. IEEE
access , 7:133653â€“133667.
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung
Kim, Lajanugen Logeswaran, Moontae Lee, Kyung-
jae Lee, and Minjoon Seo. 2023. Exploring the bene-
fits of training expert language models over instruc-
tion tuning. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , volume 202 of Proceedings of Machine
Learning Research , pages 14702â€“14729. PMLR.
Joel Jang, Seonghyeon Ye, and Minjoon Seo. 2022. Can
large language models truly understand prompts? A
case study with negated prompts. In Transfer Learn-
ing for Natural Language Processing Workshop, 03
December 2022, New Orleans, Louisiana, USA , vol-
ume 203 of Proceedings of Machine Learning Re-
search , pages 52â€“62. PMLR.
Tian Jin, Zhun Liu, Shengjia Yan, Alexandre Eichen-
berger, and Louis-Philippe Morency. 2020. Lan-
guage to network: Conditional parameter adaptation
with natural language descriptions. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 6994â€“7007, On-
line. Association for Computational Linguistics.
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin,
Matei Zaharia, and Tatsunori Hashimoto. 2023. Ex-
ploiting programmatic behavior of llms: Dual-use
through standard security attacks. ArXiv preprint ,
abs/2302.05733.
Nora Kassner and Hinrich SchÃ¼tze. 2020. Negated and
misprimed probes for pretrained language models:
Birds can talk, but cannot fly. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 7811â€“7818, Online. Asso-
ciation for Computational Linguistics.

--- PAGE 24 ---
Tannon Kew, Florian Schottmann, and Rico Sennrich.
2023. Turning english-centric llms into polyglots:
How much multilinguality is needed? ArXiv preprint ,
abs/2312.12683.
Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui
Qin, Kyle Richardson, Sean Welleck, Hannaneh Ha-
jishirzi, Tushar Khot, Ashish Sabharwal, Sameer
Singh, and Yejin Choi. 2022. Prompt wayward-
ness: The curious case of discretized interpretation
of continuous prompts. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 3631â€“3643, Seattle,
United States. Association for Computational Lin-
guistics.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-
mat boundaries with a single QA system. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020 , pages 1896â€“1907, Online. Association
for Computational Linguistics.
Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised PCFG induction for grounded language learn-
ing with highly ambiguous supervision. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning , pages 433â€“444,
Jeju Island, Korea. Association for Computational
Linguistics.
Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang,
Seonghyeon Ye, Jamin Shin, and Minjoon Seo. 2023.
The cot collection: Improving zero-shot and few-shot
learning of language models via chain-of-thought
fine-tuning. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2023, Singapore, December 6-10, 2023 ,
pages 12685â€“12708. Association for Computational
Linguistics.
Nikita Kitaev and Dan Klein. 2018. Constituency pars-
ing with a self-attentive encoder. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 2676â€“2686, Melbourne, Australia. Association
for Computational Linguistics.
Abdullatif KÃ¶ksal, Timo Schick, Anna Korhonen, and
Hinrich SchÃ¼tze. 2023. Longform: Optimizing in-
struction tuning for long text generation with corpus
extraction. ArXiv preprint , abs/2304.08460.
Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte,
Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,
Abdullah Barhoum, Duc Nguyen, Oliver Stan-
ley, RichÃ¡rd Nagyfi, Shahul ES, Sameer Suri,
David Glushkov, Arnav Dantuluri, Andrew Maguire,
Christoph Schuhmann, Huu Nguyen, and Alexan-
der Mattick. 2023. Openassistant conversations -
democratizing large language model alignment. In
Advances in Neural Information Processing Systems36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 .
Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly
learning to parse and perceive: Connecting natural
language to the physical world. Transactions of the
Association for Computational Linguistics , 1:193â€“
206.
Gregory Kuhlmann, Peter Stone, Raymond Mooney,
and Jude Shavlik. 2004. Guiding a reinforcement
learner with natural language advice: Initial results
in robocup soccer. In The AAAI-2004 workshop on
supervisory control of learning and adaptive systems .
San Jose, CA.
Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE ,
86(11):2278â€“2324.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045â€“3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Bangzheng Li, Wenpeng Yin, and Muhao Chen. 2022a.
Ultra-fine entity typing with indirect supervision
from natural language inference. Transactions of the
Association for Computational Linguistics , 10:607â€“
622.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei
Liu. 2023a. Mimic-it: Multi-modal in-context in-
struction tuning. ArXiv preprint , abs/2306.05425.
Judith Yue Li, Aren Jansen, Qingqing Huang, Ravi
Ganti, Joonseok Lee, and Dima Kuzmin. 2022b.
Maqa: A multimodal qa benchmark for negation.
InNeurIPS 2022 Workshop on Synthetic Data for
Empowering ML Research .
Toby Jia-Jun Li, Tom Mitchell, and Brad Myers. 2020.
Interactive task learning from GUI-grounded natural
language instructions and demonstrations. In Pro-
ceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics: System Demon-
strations , pages 215â€“223, Online. Association for
Computational Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582â€“
4597, Online. Association for Computational Lin-
guistics.
Xiaonan Li and Xipeng Qiu. 2023. Finding supporting
examples for in-context learning. ArXiv preprint ,
abs/2302.13539.

--- PAGE 25 ---
Yafu Li, Yongjing Yin, Jing Li, and Yue Zhang. 2022c.
Prompt-driven neural machine translation. In Find-
ings of the Association for Computational Linguis-
tics: ACL 2022 , pages 2579â€“2590, Dublin, Ireland.
Association for Computational Linguistics.
Zekun Li, Baolin Peng, Pengcheng He, and Xifeng
Yan. 2023b. Do you really follow me? adversarial
instructions for evaluating the robustness of large
language models. ArXiv preprint , abs/2308.10819.
Vladislav Lialin, Vijeta Deshpande, and Anna
Rumshisky. 2023. Scaling down to scale up: A guide
to parameter-efficient fine-tuning. ArXiv preprint ,
abs/2303.15647.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP , pages 91â€“99, Suntec, Sin-
gapore. Association for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74â€“81, Barcelona, Spain.
Association for Computational Linguistics.
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,
Maria Lomeli, Rich James, Pedro Rodriguez, Jacob
Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.
Ra-dit: Retrieval-augmented dual instruction tuning.
ArXiv preprint , abs/2310.01352.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian Oâ€™Horo, Jeff Wang, Luke Zettle-
moyer, Zornitsa Kozareva, Mona T. Diab, Veselin
Stoyanov, and Xian Li. 2022. Few-shot learning with
multilingual generative language models. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2022, Abu
Dhabi, United Arab Emirates, December 7-11, 2022 ,
pages 9019â€“9052. Association for Computational
Linguistics.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin Raffel.
2022a. Few-shot parameter-efficient fine-tuning is
better and cheaper than in-context learning. In Ad-
vances in Neural Information Processing Systems 35:
Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA,
USA, November 28 - December 9, 2022 .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022b. What
makes good in-context examples for GPT-3? In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures ,
pages 100â€“114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023a. Pre-
train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing.
ACM Computing Surveys , 55(9):1â€“35.
Qian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and
Min Lin. 2023b. From zero to hero: Examining the
power of symbolic tasks in instruction tuning. ArXiv
preprint , abs/2304.07995.
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and
Junxian He. 2023c. What makes good data for
alignment? a comprehensive study of automatic
data selection in instruction tuning. ArXiv preprint ,
abs/2312.15685.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
understands, too. ArXiv preprint , abs/2103.10385.
Yilun Liu, Shimin Tao, Xiaofeng Zhao, Ming Zhu, Wen-
bing Ma, Junhao Zhu, Chang Su, Yutai Hou, Miao
Zhang, Min Zhang, et al. 2023d. Automatic instruc-
tion optimization for open-source llm instruction tun-
ing. ArXiv preprint , abs/2311.13246.
Yixin Liu, Alexander R Fabbri, Jiawen Chen, Yilun
Zhao, Simeng Han, Shafiq Joty, Pengfei Liu,
Dragomir Radev, Chien-Sheng Wu, and Arman Co-
han. 2023e. Benchmarking generation and evalu-
ation capabilities of large language models for in-
struction controllable summarization. ArXiv preprint ,
abs/2311.09184.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le,
Barret Zoph, Jason Wei, and Adam Roberts. 2023.
The flan collection: Designing data and methods for
effective instruction tuning. In International Con-
ference on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA , volume 202 of
Proceedings of Machine Learning Research , pages
22631â€“22648. PMLR.
Renze Lou and Wenpeng Yin. 2023. Forget demon-
strations, focus on learning from textual instructions.
ArXiv preprint , abs/2308.03795.
Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Jan-
ice Ahn, Hanzi Xu, Yu su, and Wenpeng Yin. 2024.
MUFFIN: Curating multi-faceted instructions for im-
proving instruction following. In The Twelfth Inter-
national Conference on Learning Representations .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8086â€“8098, Dublin, Ireland. Association for Compu-
tational Linguistics.

--- PAGE 26 ---
Cynthia Matuszek, Nicholas FitzGerald, Luke S. Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the 29th Interna-
tional Conference on Machine Learning, ICML 2012,
Edinburgh, Scotland, UK, June 26 - July 1, 2012 .
icml.cc / Omnipress.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022a. MetaICL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2791â€“2809, Seattle, United States.
Association for Computational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022b. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11048â€“11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin
Choi, and Hannaneh Hajishirzi. 2022a. Reframing
instructional prompts to GPTkâ€™s language. In Find-
ings of the Association for Computational Linguistics:
ACL 2022 , pages 589â€“612, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022b. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470â€“3487, Dublin, Ireland.
Association for Computational Linguistics.
Swaroop Mishra and Elnaz Nouri. 2023. HELP ME
THINK: A simple prompting strategy for non-experts
to create customized content with models. In Find-
ings of the Association for Computational Linguis-
tics: ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 11834â€“11890. Association for Computational
Linguistics.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-
ley Schoelkopf, Xiangru Tang, Dragomir Radev,
Alham Fikri Aji, Khalid Almubarak, Samuel Al-
banie, Zaid Alyafeai, Albert Webson, Edward Raff,
and Colin Raffel. 2023. Crosslingual generaliza-
tion through multitask finetuning. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
15991â€“16111. Association for Computational Lin-
guistics.
Shikhar Murty, Pang Wei Koh, and Percy Liang. 2020.
ExpBERT: Representation engineering with natural
language explanations. In Proceedings of the 58thAnnual Meeting of the Association for Computational
Linguistics , pages 2106â€“2113, Online. Association
for Computational Linguistics.
Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
InProceedings of the 27th International Conference
on Computational Linguistics , pages 2340â€“2353,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.
Tai Nguyen and Eric Wong. 2023. In-context ex-
ample selection with influences. ArXiv preprint ,
abs/2302.11042.
OpenAI. 2022. Chatgpt.
OpenAI. 2023. Gpt-4 technical report. ArXiv preprint ,
abs/2303.08774.
Jose Javier Gonzalez Ortiz, John Guttag, and Adrian
Dalca. 2023. Non-proportional parametrizations
for stable hypernetwork learning. ArXiv preprint ,
abs/2304.07645.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In Advances in Neural
Information Processing Systems 35: Annual Confer-
ence on Neural Information Processing Systems 2022,
NeurIPS 2022, New Orleans, LA, USA, November 28
- December 9, 2022 .
Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel
Li, Steven Basart, Thomas Woodside, Hanlin Zhang,
Scott Emmons, and Dan Hendrycks. 2023. Do the
rewards justify the means? measuring trade-offs be-
tween rewards and ethical behavior in the machiavelli
benchmark. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , volume 202 of Proceedings of Machine
Learning Research , pages 26837â€“26867. PMLR.
Mihir Parmar, Swaroop Mishra, Mor Geva, and Chitta
Baral. 2023. Donâ€™t blame the annotator: Bias al-
ready starts in the annotation instructions. In Pro-
ceedings of the 17th Conference of the European
Chapter of the Association for Computational Lin-
guistics , pages 1779â€“1789, Dubrovnik, Croatia. As-
sociation for Computational Linguistics.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. ArXiv preprint , abs/2304.03277.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit
Bansal. 2023. GrIPS: Gradient-free, edit-based in-
struction search for prompting large language models.
InProceedings of the 17th Conference of the Euro-
pean Chapter of the Association for Computational

--- PAGE 27 ---
Linguistics , pages 3845â€“3864, Dubrovnik, Croatia.
Association for Computational Linguistics.
Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni
Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, and
Ronan Collobert. 2020. Massively multilingual ASR:
50 languages, 1 model, 1 billion parameters. In In-
terspeech 2020, 21st Annual Conference of the Inter-
national Speech Communication Association, Virtual
Event, Shanghai, China, 25-29 October 2020 , pages
4751â€“4755. ISCA.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2023. Measuring
and narrowing the compositionality gap in language
models. In The 2023 Conference on Empirical Meth-
ods in Natural Language Processing .
Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li,
Tingwu Wang, Sanja Fidler, and Antonio Torralba.
2018. Virtualhome: Simulating household activities
via programs. In 2018 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2018,
Salt Lake City, UT, USA, June 18-22, 2018 , pages
8494â€“8502. IEEE Computer Society.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,
Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,
and Huajun Chen. 2023. Reasoning with language
model prompting: A survey. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, July 9-14, 2023 , pages 5368â€“
5393. Association for Computational Linguistics.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying LMs with mixtures of soft prompts.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 5203â€“5212, Online. Association for Computa-
tional Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
Models are Unsupervised Multitask Learners. Ope-
nAI blog .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1â€“140:67.
Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop
Kang. 2023. Coedit: Text editing by task-specific
instruction tuning. In Findings of the Association for
Computational Linguistics: EMNLP 2023, Singapore,
December 6-10, 2023 , pages 5274â€“5291. Association
for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings ofthe 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383â€“2392, Austin,
Texas. Association for Computational Linguistics.
Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit
Bansal, Jason Weston, and Xian Li. 2023. Branch-
solve-merge improves large language model evalua-
tion and generation. ArXiv preprint , abs/2310.15123.
Oscar Sainz, Itziar Gonzalez-Dios, Oier Lopez de La-
calle, Bonan Min, and Eneko Agirre. 2022. Tex-
tual entailment for event argument extraction: Zero-
and few-shot with multi-source learning. In Find-
ings of the Association for Computational Linguis-
tics: NAACL 2022 , pages 2439â€“2455, Seattle, United
States. Association for Computational Linguistics.
Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka,
Ander Barrena, and Eneko Agirre. 2021. Label ver-
balization and entailment for effective zero and few-
shot relation extraction. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1199â€“1212, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault FÃ©vry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M. Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.
Timo Schick and Hinrich SchÃ¼tze. 2021a. Exploiting
cloze-questions for few-shot text classification and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume ,
pages 255â€“269, Online. Association for Computa-
tional Linguistics.
Timo Schick and Hinrich SchÃ¼tze. 2021b. Few-shot
text generation with natural language instructions. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pages 390â€“
402, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Timo Schick and Hinrich SchÃ¼tze. 2021c. Itâ€™s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2339â€“2352, Online. Association
for Computational Linguistics.

--- PAGE 28 ---
Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei,
Zhenzhong Lan, and Yue Zhang. 2023. Dynamics
of instruction tuning: Each ability of large language
models has its own growth pace. ArXiv preprint ,
abs/2310.19651.
Taylor Sorensen, Joshua Robinson, Christopher Ryt-
ting, Alexander Shaw, Kyle Rogers, Alexia Delorey,
Mahmoud Khalil, Nancy Fulda, and David Wingate.
2022. An information-theoretic approach to prompt
engineering without ground truth labels. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 819â€“862, Dublin, Ireland. Association
for Computational Linguistics.
Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2017. Joint concept learning and semantic parsing
from natural language explanations. In Proceedings
of the 2017 Conference on Empirical Methods in Nat-
ural Language Processing , pages 1527â€“1536, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2018. Zero-shot learning of classifiers from natural
language quantification. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 306â€“316,
Melbourne, Australia. Association for Computational
Linguistics.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F. Christiano. 2020. Learn-
ing to summarize with human feedback. In Advances
in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual .
Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for
deep learning in NLP. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 3645â€“3650, Florence, Italy. Asso-
ciation for Computational Linguistics.
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,
Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.
Smith, Luke Zettlemoyer, and Tao Yu. 2023. One
embedder, any task: Instruction-finetuned text em-
beddings. In Findings of the Association for Com-
putational Linguistics: ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 1102â€“1121. Association for
Computational Linguistics.
Lin Sun, Kai Zhang, Qingyuan Li, and Renze Lou. 2024.
UMIE: unified multimodal information extraction
with instruction tuning. In Thirty-Eighth AAAI Con-
ference on Artificial Intelligence, AAAI 2024, Thirty-
Sixth Conference on Innovative Applications of Ar-
tificial Intelligence, IAAI 2024, Fourteenth Sympo-
sium on Educational Advances in Artificial Intelli-
gence, EAAI 2014, February 20-27, 2024, Vancouver,
Canada , pages 19062â€“19070. AAAI Press.Xiaobing Sun and Wei Lu. 2022. Implicit n-grams in-
duced by recurrence. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1624â€“1639, Seattle,
United States. Association for Computational Lin-
guistics.
Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V . Le, Ed H. Chi,
Denny Zhou, and Jason Wei. 2023. Challenging
big-bench tasks and whether chain-of-thought can
solve them. In Findings of the Association for Com-
putational Linguistics: ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 13003â€“13051. Association for
Computational Linguistics.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Gar-
cia, Jason Wei, Xuezhi Wang, Hyung Won Chung,
Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,
Denny Zhou, Neil Houlsby, and Donald Metzler.
2023. UL2: unifying language learning paradigms.
InThe Eleventh International Conference on Learn-
ing Representations, ICLR 2023, Kigali, Rwanda,
May 1-5, 2023 . OpenReview.net.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R Walter, Ashis Gopal Banerjee, Seth Teller,
and Nicholas Roy. 2011. Approaching the symbol
grounding problem with probabilistic graphical mod-
els.AI magazine , 32(4):64â€“76.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,
Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. ArXiv preprint ,
abs/2302.13971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998â€“6008.
Adam V ogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics , pages 806â€“814, Uppsala, Sweden.
Association for Computational Linguistics.
Alexander Wan, Eric Wallace, Sheng Shen, and Dan
Klein. 2023. Poisoning language models during in-
struction tuning. In International Conference on Ma-
chine Learning, ICML 2023, 23-29 July 2023, Hon-
olulu, Hawaii, USA , volume 202 of Proceedings of

--- PAGE 29 ---
Machine Learning Research , pages 35413â€“35425.
PMLR.
Liwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan,
Sirui Wang, Wei Wu, and Weiran Xu. 2022a. Instruc-
tionNER: A Multi-Task Instruction-Based Genera-
tive Framework for Few-Shot NER. ArXiv preprint ,
abs/2203.03903.
Pei Wang and Ben Goertzel. 2007. Introduction: As-
pects of Artificial General Intelligence. In Proceed-
ings of the 2007 conference on Advances in Artificial
General Intelligence: Concepts, Architectures and
Algorithms: Proceedings of the AGI Workshop 2006 ,
pages 1â€“16.
Xinyi Wang, Wanrong Zhu, and William Yang Wang.
2023a. Large language models are implicitly
topic models: Explaining and finding good demon-
strations for in-context learning. ArXiv preprint ,
abs/2301.11916.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .
Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-
hery, and Denny Zhou. 2023b. Self-consistency
improves chain of thought reasoning in language
models. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Chandu, David Wad-
den, Kelsey MacMillan, Noah A. Smith, Iz Beltagy,
and Hannaneh Hajishirzi. 2023c. How far can camels
go? exploring the state of instruction tuning on open
resources. In Advances in Neural Information Pro-
cessing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023,
New Orleans, LA, USA, December 10 - 16, 2023 .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023d. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 13484â€“13508. Association for Computational
Linguistics.
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al.
2022b. Benchmarking Generalization via In-context
Instructions on 1,600+ Language Tasks. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 5085â€“5109.
Ziqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan,
Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xi-
ang Ren. 2020. Learning from explanations with
neural execution tree. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.Albert Webson and Ellie Pavlick. 2022. Do prompt-
based models really understand the meaning of their
prompts? In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2300â€“2344, Seattle, United States.
Association for Computational Linguistics.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022a. Finetuned
language models are zero-shot learners. In The Tenth
International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022 .
OpenReview.net.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022b. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems
35: Annual Conference on Neural Information Pro-
cessing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022 .
Jerry W. Wei, Le Hou, Andrew K. Lampinen, Xiangn-
ing Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng
Lu, Denny Zhou, Tengyu Ma, and Quoc V . Le. 2023.
Symbol tuning improves in-context learning in lan-
guage models. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2023, Singapore, December 6-10,
2023 , pages 968â€“979. Association for Computational
Linguistics.
Orion Weller, Nicholas Lourie, Matt Gardner, and
Matthew E. Peters. 2020. Learning from task de-
scriptions. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 1361â€“1375, Online. Association for
Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, RÃ©mi Louf, Morgan Fun-
towicz, et al. 2019. HuggingFaceâ€™s Transformers:
State-of-the-Art Natural Language Processing. ArXiv
preprint , abs/1910.03771.
Hui Wu and Xiaodong Shi. 2022. Adversarial soft
prompt tuning for cross-domain sentiment analysis.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 2438â€“2447, Dublin, Ireland.
Association for Computational Linguistics.
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-
mad Abdul-Mageed, and Alham Fikri Aji. 2024.
Lamini-lm: A diverse herd of distilled models from
large-scale instructions. In Proceedings of the 18th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, EACL 2024 -
Volume 1: Long Papers, St. Julianâ€™s, Malta, March
17-22, 2024 , pages 944â€“964. Association for Com-
putational Linguistics.

--- PAGE 30 ---
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2023. Self-adaptive in-context learn-
ing: An information compression perspective for in-
context example selection and ordering. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
1423â€“1436. Association for Computational Linguis-
tics.
Congying Xia, Wenpeng Yin, Yihao Feng, and Philip
Yu. 2021. Incremental few-shot text classification
with multi-round new classes: Formulation, dataset
and system. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 1351â€“1360, Online. Association
for Computational Linguistics.
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and
Yu Su. 2024a. Adaptive chameleon or stubborn sloth:
Revealing the behavior of large language models in
knowledge conflicts. In The Twelfth International
Conference on Learning Representations .
Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu,
Renze Lou, Yuandong Tian, Yanghua Xiao, and
Yu Su. 2024b. Travelplanner: A benchmark for real-
world planning with language agents. arXiv preprint
arXiv:2402.01622 .
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023a. Wizardlm: Empowering large lan-
guage models to follow complex instructions. ArXiv
preprint , abs/2304.12244.
Canwen Xu, Daya Guo, Nan Duan, and Julian J.
McAuley. 2023b. Baize: An open-source chat model
with parameter-efficient tuning on self-chat data. In
Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023 , pages 6268â€“
6278. Association for Computational Linguistics.
Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu,
Chenguang Zhu, and Julian McAuley. 2023c. Small
models are valuable plug-ins for large language mod-
els.ArXiv preprint , abs/2305.08848.
Haike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng, and
Zhilin Yang. 2023d. A universal discriminator for
zero-shot generalization. In Proceedings of the 61st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2023,
Toronto, Canada, July 9-14, 2023 , pages 10559â€“
10575. Association for Computational Linguistics.
Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang
Yanggang, Haiyu Li, and Zhilin Yang. 2022a. Zero-
Prompt: Scaling prompt-based pretraining to 1,000
tasks improves zero-shot generalization. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2022 , pages 4235â€“4252, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.Hanzi Xu, Slobodan Vucetic, and Wenpeng Yin. 2022b.
OpenStance: Real-world zero-shot stance detection.
InProceedings of the 26th Conference on Computa-
tional Natural Language Learning (CoNLL) , pages
314â€“324, Abu Dhabi, United Arab Emirates (Hybrid).
Association for Computational Linguistics.
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao
Song, Markus Freitag, William Wang, and Lei Li.
2023e. INSTRUCTSCORE: Towards explainable
text generation evaluation with automatic feedback.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5967â€“5994. Association for Computational Linguis-
tics.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2023. Tree of thoughts: Deliberate problem solving
with large language models. In Advances in Neural
Information Processing Systems 36: Annual Confer-
ence on Neural Information Processing Systems 2023,
NeurIPS 2023, New Orleans, LA, USA, December 10
- 16, 2023 .
Qinyuan Ye, Xiao Huang, Elizabeth Boschee, and Xiang
Ren. 2020. Teaching machine comprehension with
compositional explanations. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP 2020 ,
pages 1599â€“1615, Online. Association for Computa-
tional Linguistics.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
CrossFit: A few-shot learning challenge for cross-
task generalization in NLP. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7163â€“7189, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Qinyuan Ye and Xiang Ren. 2021. Learning to gener-
ate task-specific adapters from task description. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 2: Short Papers) , pages 646â€“653,
Online. Association for Computational Linguistics.
Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang,
Hyeongu Yun, Yireun Kim, and Minjoon Seo. 2023.
In-context instruction learning. ArXiv preprint ,
abs/2302.14691.
Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo,
and Minjoon Seo. 2022a. Retrieval of soft prompt en-
hances zero-shot task generalization. ArXiv preprint ,
abs/2210.03029.
Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo
Shin, and Minjoon Seo. 2022b. Guess the Instruc-
tion! Making Language Models Stronger Zero-Shot
Learners. ArXiv preprint , abs/2210.02969.
Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal,
Jiawei Han, and Kai-Wei Chang. 2023. Dynosaur:

--- PAGE 31 ---
A dynamic growth paradigm for instruction-tuning
data curation. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2023, Singapore, December 6-10, 2023 ,
pages 4031â€“4047. Association for Computational
Linguistics.
Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Bench-
marking zero-shot text classification: Datasets, eval-
uation and entailment approach. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3914â€“3923, Hong Kong,
China. Association for Computational Linguistics.
Wenpeng Yin, Jia Li, and Caiming Xiong. 2022. Con-
TinTin: Continual learning from task instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3062â€“3072, Dublin, Ireland.
Association for Computational Linguistics.
Fei Yu, Hongbo Zhang, and Benyou Wang. 2023a. Na-
ture language reasoning, a survey. ArXiv preprint ,
abs/2303.14725.
Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang,
Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng
Yin. 2023b. Wavecoder: Widespread and versatile
enhanced instruction tuning with refined data genera-
tion. ArXiv preprint , abs/2312.14187.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An
open bilingual pre-trained model. In The Eleventh In-
ternational Conference on Learning Representations .
Hongming Zhang, Muhao Chen, Haoyu Wang, Yangqiu
Song, and Dan Roth. 2020. Analogous process struc-
ture induction for sub-event sequence prediction. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 1541â€“1550, Online. Association for Computa-
tional Linguistics.
Kai Zhang, Bernal Jimenez Gutierrez, and Yu Su. 2023a.
Aligning instruction tasks unlocks large language
models as zero-shot relation extractors. In Findings
of the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
794â€“812. Association for Computational Linguistics.
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-
wei Zhang, Fei Wu, et al. 2023b. Instruction tuning
for large language models: A survey. ArXiv preprint ,
abs/2308.10792.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022a. Opt: Open pre-trained transformer language
models. ArXiv preprint , abs/2205.01068.Yi Zhang, Sujay Kumar Jauhar, Julia Kiseleva, Ryen
White, and Dan Roth. 2021. Learning to decom-
pose and organize complex tasks. In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 2726â€“2735,
Online. Association for Computational Linguistics.
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Ac-
tive example selection for in-context learning. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 9134â€“
9148, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022c. Automatic chain of thought prompt-
ing in large language models. In The Eleventh Inter-
national Conference on Learning Representations .
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Vir-
tual Event , volume 139 of Proceedings of Machine
Learning Research , pages 12697â€“12706. PMLR.
Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021. Adapting language models for zero-shot learn-
ing by meta-tuning on dataset and prompt collections.
InFindings of the Association for Computational
Linguistics: EMNLP 2021 , pages 2856â€“2878, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,
Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping
Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023a. LIMA:
less is more for alignment. In Advances in Neural
Information Processing Systems 36: Annual Confer-
ence on Neural Information Processing Systems 2023,
NeurIPS 2023, New Orleans, LA, USA, December 10
- 16, 2023 .
Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc V Le, et al. 2022.
Least-to-most prompting enables complex reasoning
in large language models. In The Eleventh Interna-
tional Conference on Learning Representations .
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,
and Le Hou. 2023b. Instruction-following evalu-
ation for large language models. ArXiv preprint ,
abs/2311.07911.
Appendix A. Instruction-following Datasets

--- PAGE 32 ---
Datasets Release TimeScaleLanguage Annotator
# of Tasks # of Instances (k)
UnifiedQA (Khashabi et al., 2020) 05/2020 46 750 monolingual
 Human
CrossFit (Ye et al., 2021) 04/2021 159 71,000 monolingual
 Human
Natural Instructions (Mishra et al., 2022b) 04/2021 61 620 monolingual
 Human
Flan 2021 (Wei et al., 2022a) 09/2021 62 4,400 monolingual
 Human
P3(Sanh et al., 2022) 10/2021 62 12,000 monolingual
 Human
MetaICL (Min et al., 2022a) 10/2021 142 3,500 monolingual
 Human
ExMix (Aribandi et al., 2022) 11/2021 107 500 monolingual
 Human
Super-Natural Instructions (Wang et al., 2022b) 04/2022 1,613 5,000 multilingual
 Human
GLM (Zeng et al., 2022) 10/2022 77 12,000 bilingual
 Human
Flan 2022 (Longpre et al., 2023) 10/2022 1,836 15,000 multilingual
 Human
xP3(Muennighoff et al., 2023) 11/2022 71 81,000 multilingual
 Human
Unnatural Instructions (Honovich et al., 2023a) 12/2022 117 64 monolingual
 InstructGPT
Self-Instruct (Wang et al., 2023d) 12/2022 / 82 monolingual
 GPT-3
OPT-IML (Iyer et al., 2022) 12/2022 2,207 18,000 multilingual
 Human
Alpaca (Taori et al., 2023) 03/2023 / 52 monolingual
 InstructGPT
Baize (Xu et al., 2023b) 04/2023 / 100 monolingual
 ChatGPT
Koala (Geng et al., 2023) 04/2023 / /monolingual
Human
ChatGPT
GPT4All (Anand et al., 2023) 04/2023 / 808 monolingual
Human
ChatGPT
Alpaca-gpt4 (Peng et al., 2023) 04/2023 / 113 bilingual
 GPT-4
Vicuna (Chiang et al., 2023) 04/2023 / 76 monolingual
Human
ChatGPT
Dolly (Conover et al., 2023) 04/2023 / 15 monolingual
 Human
Oasst (KÃ¶pf et al., 2023) 04/2023 / 84 multilingual
 Human
LongForm (KÃ¶ksal et al., 2023) 04/2023 / 27 monolingual
Human
InstructGPT
Symbolic-Instruct (Liu et al., 2023b) 04/2023 / 796 monolingual
 Human
LaMini (Wu et al., 2024) 04/2023 / 2,580 monolingual
 ChatGPT
WizardLM (Xu et al., 2023a) 04/2023 / 196 monolingual
 ChatGPT
COEDIT (Raheja et al., 2023) 05/2023 / 82 monolingual
 Human
UltraChat (Ding et al., 2023) 05/2023 / 1,500 monolingual
 ChatGPT
CoT Collection (Kim et al., 2023) 05/2023 1,060 1,880 monolingual
 Codex
Dynosaur (Yin et al., 2023) 05/2023 5,740 801 monolingual
 ChatGPT
MUFFIN (Lou et al., 2024) 10/2023 / 68 monolingual
Human
ChatGPT
GPT-4
Dynamics-of-Instruction (Song et al., 2023) 10/2023 / 40 monolingual
 Human
CoachLM (Liu et al., 2023d) 11/2023 / 2monolingual
 Human
DEITA (Liu et al., 2023c) 12/2023 / 10 monolingual
 ChatGPT
WaveCoder (Yu et al., 2023b) 12/2023 4 20 monolingual
ChatGPT
GPT-4
Table 6: Instruction-tuning datasets summarization. Due to the diverse user tasks, some datasets didnâ€™t report the
task scale.
