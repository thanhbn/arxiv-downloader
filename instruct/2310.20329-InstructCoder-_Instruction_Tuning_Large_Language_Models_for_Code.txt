# 2310.20329.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2310.20329.pdf
# File size: 7663032 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
InstructCoder: Instruction Tuning Large Language Models for Code
Editing
Kaixin Li1*Qisheng Hu1*Xu Zhao1Hui Chen2Yuxi Xie1Tiedong Liu1
Qizhe Xie1†Junxian He3†
1National University of Singapore2Singapore University of Technology and Design
3Shanghai Jiao Tong University
{likaixin,qishenghu,xu.zhao,xieyuxi,tiedong.liu}@u.nus.edu ,
hui_chen@mymail.sutd.edu.sg ,
junxianh@sjtu.edu.cn
Abstract
Code editing encompasses a variety of prag-
matic tasks that developers deal with daily.
Despite its relevance and practical usefulness,
automatic code editing remains an underex-
plored area in the evolution of deep learning
models, partly due to data scarcity. In this
work, we explore the use of Large Language
Models (LLMs) to edit code based on user in-
structions. Evaluated on a novel human-written
execution-based benchmark dubbed EditEval ,
we found current models often struggle to ful-
fill the instructions. In light of this, we con-
tribute InstructCoder , the first instruction-
tuning dataset designed to adapt LLMs for
general-purpose code editing, containing high-
diversity code-editing tasks such as comment
insertion, code optimization, and code refac-
toring. It consists of over 114,000 instruction-
input-output triplets and covers multiple dis-
tinct code editing scenarios. The collection
process starts with filtered commit data sourced
from GitHub Python repositories as seeds. Sub-
sequently, the dataset is systematically ex-
panded through an iterative process, where
both seed and generated tasks are used to
prompt ChatGPT for more data. Our findings
reveal that open-source LLMs fine-tuned on
InstructCoder can significantly enhance the ac-
curacy of code edits, exhibiting superior code-
editing performance matching advanced pro-
prietary LLMs.
The dataset and the source code are avail-
able at https://github.com/qishenghu/
CodeInstruct .
1 Introduction
Developers typically engage in a cyclic routine of
writing and revising code. As a crucial element,
*Equal contribution. Ordering is determined by dice
rolling.
†Equal advising. Ordering is determined by dice rolling.code editing takes up a great portion of this process,
encapsulating diverse sub-tasks such as code opti-
mization, refactoring, and bug fixing, each posing
distinct challenges. Automated code editing tools
could substantially boost developer productivity by
alleviating the burden of monotonous tasks. How-
ever, it remains an under-explored area, partly due
to the lack of relevant data, hampering substantial
progress by deep learning models.
Inspired by the recent advancements in
LLMs (Brown et al., 2020; Chowdhery et al., 2022;
Ouyang et al., 2022; OpenAI, 2022; Touvron et al.,
2023a; OpenAI, 2023) and Code LLMs (Nijkamp
et al., 2023a; Chen et al., 2021a; Li et al., 2023a),
we explore the proficiency of LLMs in code edit-
ing tasks based on user instructions, for instance,
“add a docstring to the function for clarity”, “re-
move redundant code”, or “refactor it into reusable
functions”. These tasks are distinctly different
from code completion, which involves generating
code to complete given code snippets or comments.
Code editing requires the model to not only under-
stand the existing code but also execute modifica-
tions that are in line with the given instructions,
while seamlessly integrating with the context. For
example, removing redundant code or refactoring
a function should not affect the return value.
To systematically evaluate LLMs for code
editing, we created a novel benchmark named
EditEval . It contains various types of code edits
adapted from Github commits and existing datasets.
Intriguingly, we found that open-source models
yield unsatisfactory results, and even the most ad-
vanced proprietary LLMs struggle to solve these
tasks.
In addressing this challenge, we present Instruct-
Coder, a diverse dataset for instruction finetuning,
particularly designed to improve the code editingarXiv:2310.20329v3  [cs.CL]  28 Feb 2024

--- PAGE 2 ---
Figure 1: Data collection pipeline of InstructCoder (left) and a qualitative example from the dataset (right, best
viewed with zoom). Initial seed tasks are selected from GitHub commits, and inspire ChatGPT to generate
new instructions. Plausible scenarios where the filtered instructions may be used are then generated. Finally,
corresponding code input and output are obtained conditioned on both the instruction and scenario. High-quality
samples are manually selected and recurrently added to the task pool for further generation.
abilities of LLMs. Specifically, we first collect and
manually scrutinize commit data from public repos-
itories on GitHub as the seed code editing tasks.
Then, we utilize the seed data to prompt Chat-
GPT (OpenAI, 2022) to generate new instructions
and input-output pairs respectively. This process
resembles the Self-Instruct (Wang et al., 2022a)
and Alpaca (Taori et al., 2023) frameworks. By
innovatively forcing scenarios to guide the gener-
ation process, our approach ensures that the tasks
in InstructCoder are diverse and relevant to real-
world programming situations, resulting in a ro-
bust dataset for instruction finetuning in the code
editing domain. After proper deduplication and
postprocessing, we retain over 114,000 samples in
the dataset.
Our empirical studies reveal that LLMs display
notable gains in code editing abilities after fine-
tuning with InstructCoder. Code LLaMA achieves
the best results through fine-tuning, attaining an ac-
curacy of 57.22%, closely matching ChatGPT. Fur-
ther studies also signify that while the pre-training
of the models is fundamental, the code editing per-
formance is highly influenced by the quality and
volume of the instruction-tuning data.In summary, the contributions of this work
are (1) InstructCoder , the first instruction-tuning
dataset featuring a wide range of diverse code
editing tasks, and demonstrate the effectiveness
of instruction-finetuning with InstructCoder; (2)
EditEval , a novel human-written execution-based
benchmark for the rigorous evaluation of general-
purpose code editing; (3) We find that open-
source models instruction-tuned with Instruct-
Coder can demonstrate strong code editing per-
formance matching ChatGPT.
2 Related Work
2.1 Instruction Finetuning Datasets
Previous studies have concluded that instruction
finetuning LLMs on a diverse collection of in-
structional tasks can further improve the ability of
LLMs to generalize well on unseen tasks (Ouyang
et al., 2022; Mishra et al., 2022; Wei et al., 2022;
Chung et al., 2022; Wang et al., 2023c). To support
these tasks, datasets consisting of a large number
of code snippets with corresponding annotations
are necessary. These instruction can be reformu-
lated from existing datasets (Aribandi et al., 2022;
Wei et al., 2022; Mishra et al., 2022; Longpre et al.,
2

--- PAGE 3 ---
Add new features Fix bugs Improve code readability Error handling General Code Refactor Add logging/monitoring Implement cache mechanisms Add comments/docstring Create unit tests Enhance performance Optimize complexity Implement multithreading Update library dependencies Ensure code security Internationalize code Adhere to coding standards Apply design patterns Reduce code duplication Integrate APIs/services Remove dead code Adapt code for accessibility Modularize code Migrate to new framework Complete incomplete code Address memory leaks Code Linting02k4k6k8k10k12k14k
2k4k6k8k10k12k14kNumber
CategoryNumberFigure 2: Distribution of code edit intent categories.
(a) The top 20 most common root verbs with each top 4 noun
objects in the instructions. Instructions with other infrequent
root verbs take up 25%.
(b) Wordcloud of scenario domains. Each sector with a dif-
ferent color corresponds to a different scenario domain. Each
domain is a cluster of similar scenarios.
Figure 3: Visualizations of InstructCoder data. Best viewed in zoom.
2023), or human-written with crowd-sourcing ef-
forts (Ouyang et al., 2022; Wang et al., 2022b).
Machine generation of instruction data has also
been explored to reduce human labour (Wang et al.,
2022a; Honovich et al., 2022; Taori et al., 2023;
Xue et al., 2023). Despite the presence of elevated
noise levels within the data, its effectiveness has
been identified.
2.2 Code Synthesis
Code generation is an extensively studied area.
Language models pre-trained on large collections
of code have demonstrated strong abilities in a va-
riety of programming tasks. Some general LLMsgain code generation abilities due to the mixture of
code in the pre-training corpus (e.g. The Pile (Gao
et al., 2020)), such as GPT-3 (Brown et al., 2020),
ChatGPT, GPT-4 (OpenAI, 2023), LLaMA (Tou-
vron et al., 2023a), BLOOM (Scao et al., 2022),
GPT-NeoX (Black et al., 2022), and Pythia (Bider-
man et al., 2023). LLMs specifically trained on
code and optimized for code generation are also
studied, e.g. Codex (Chen et al., 2021a), Code-
Gen (Nijkamp et al., 2023b), CodeGeeX (Zheng
et al., 2023) and StarCoder (Li et al., 2023a). These
models all adopt the decoder-only transformer ar-
chitecture but differ in size and specific model de-
sign (e.g. positional embedding, norm layer place-
3

--- PAGE 4 ---
Model Accuracy (%)
ChatGPT (gpt-3.5-turbo-0613) 57.73
GPT-4 (gpt-4-0613) 68.56
GPT-4 Turbo (gpt-4-1106-preview) 66.49
7B 13B 33B
Alpaca 12.37 19.59 30.93
LLaMA+CodeAlpaca 18.56 18.56 35.56
Table 1: Results of several instruction-tuned models
evaluated on EditEval.
ment) as well as the selection and preprocessing of
the pre-training corpus.
On the other hand, relatively little literature ad-
dresses the objective of code editing. Previous
works focus on a subset of code editing tasks,
such as code infilling (Fried et al., 2023) and de-
bugging (Just et al., 2014; Tarlow et al., 2020;
Ding et al., 2020; Jimenez et al., 2023). The
PIE (Madaan et al., 2023) dataset is a concur-
rent work most relevant to ours, which focuses
on speeding up programs. Other works (Yin et al.,
2018; Wei et al., 2023; Chakraborty et al., 2020)
can not accept natural language as edit intentions,
rendering them less user-friendly.
Nevertheless, datasets particularly tailored for
general-purpose code editing are absent. To fill this
gap, we introduce InstructCoder, a novel dataset
aimed at further advancing the capabilities of code
editing with LLMs.
3 EditEval: Evaluating Code Editing
Models
As aforementioned, code editing is significantly
different from code completion. Consequently,
widely utilized datasets in the realm of code com-
pletion, such as MBPP (Austin et al., 2021) and
HumanEval (Chen et al., 2021b), fall short in eval-
uating code editing capabilities. To rigorously eval-
uate the code editing capabilities, we curated a test
set of 194 code editing tasks, derived from three
key sources: GitHub commit data, MBPP, and Hu-
manEval. We harness the input code from these
sources and create plausible edit instructions. For
GitHub sources, we manually create execution con-
texts so that the code is runnable. Each sample is
accompanied by a canonical solution written by
humans to ensure the instruction is viable. The
generated code edits are strictly assessed using au-
tomated test cases to evaluate the correctness of theedits. An edit is considered correct only if it passes
all the test cases. This automated method provides
a robust and objective evaluation framework, es-
sential for benchmarking the model’s performance
in diverse code editing situations. Appendix A
showcases an example of the test set.
We benchmarked several instruction-tuned mod-
els on EditEval, and the results are listed in Table
1. Generally, the results reveal significant poten-
tial for improvement in code editing. Alpaca and
CodeAlpaca exhibit accuracies below 20% with
7B and 13B sizes, and it only gets better at 33B.
At this size, CodeAlpaca beats Alpaca, achiev-
ing 35.56% accuracy. Turning to the GPTs, the
most advanced proprietary models up to this point,
GPT-4 achieves the best performance at 68.56%.
Even ChatGPT struggles at this task, scoring only
57.73%. Upon closer examination, we found the
challenge of EditEval lies in the high demand for
both instruction following and code understanding.
The model has to have a grasp of the implicated
context of the input code, and then accomplish the
edit within its context.
4 InstructCoder: Instruction-tuning
Empowers Code Editing
In this section, we introduce how we create In-
structCoder to boost the code editing abilities of
LLMs via instruction finetuning. We employed a
method based on Self-Instruct (Wang et al., 2022a),
which expands instruction finetuning data by boot-
strapping off language model generation. The
methodology of generating data with LLMs re-
quires minimal human-labeled data as seed tasks
while maintaining the quality and relevance of the
tasks in the dataset. Through an iterative process
of generating instructions and refining them with
deduplication, we create a dataset of a wide range
of code-editing tasks. Figure 1 illustrates the data
collection pipeline of InstructCoder.
4.1 Seed Data Collection
GitHub is a code hosting platform whose version
control service naturally records code edits with
commits, which can be converted to instructions.
The repositories on GitHub provide diverse data
with human-generated quality. However, the data
4

--- PAGE 5 ---
is not suitable for direct utilization1. First, commit
messages are mostly brief and resultant, missing
detailed descriptions. Furthermore, they can be
imprecise or even absent. Second, commits can
be huge involving multiple files, which is beyond
the scope of this work. In light of this, we direct
our attention towards LLMs as a means to generate
data, instead of the direct utilization of collected
data.
Raw GitHub commit data were collated using
BigQuery2. To ensure high quality and address
licensing issues, we focused on Python reposito-
ries on GitHub with over 100 stars and permissive
licenses. Our selection criteria was restricted to
commits modifying only one code block within a
single Python file. These commits were identified
by git-diff3.
During the collection process, we came across
many imprecise or emotionally charged commit
messages. Codex (Chen et al., 2021a) was em-
ployed in such cases to clarify the changes made
between versions and improve the commit mes-
sages, resulting in more precise and informative
instructions. A total of 634 tasks were processed
from the commit data through manual efforts and
were used for the self-instruct process.
In addition to GitHub commit data, we also
leverage high-quality generated samples as addi-
tional seed tasks. With manual inspection, a batch
of 592 high-quality samples was compiled as ad-
ditional seed tasks. This set of seed data covers a
wide range of code-editing scenarios and enriches
the basis on which InstructCoder is created, ensur-
ing that the tasks are rooted in plausible real-world
code-editing cases4.
4.2 Instruction Bootstrapping
Self-Instruct (Wang et al., 2022a) is as an effec-
tive automated framework for instruction data gen-
eration. It works by iterative bootstrapping off
LLM’s generation, presenting a way to enrich the
instructional dataset while maintaining task quality
1Initial attempts to utilize real-world GitHub commit data
for model fine-tuning yielded suboptimal results. Please refer
to Appendix B for a detailed discussion.
2https://cloud.google.com/bigquery
3https://git-scm.com/docs/git-diff
4Incorporating additional seeds also allows for modulating
the distribution of generated data, facilitating customization
for specific requirements.and relevance from a small set of human-evaluated
seed tasks. We leveraged a similar approach to
generate diverse code editing instructional data.
In each iteration, seven seed task instructions and
one ChatGPT-generated task instruction are sam-
pled and combined as a few-shot context to prompt
ChatGPT for more instructions. To generate more
diverse and practically applicable instructions, we
also generated tasks across multiple sub-domains
by specifying the editing intent in the prompt pro-
vided. Relevant prompts used can be found in
Table 4 in Appendix C.
4.3 Scenario-conditional Generation
We originally found many generated samples share
similar codebases despite different instructions and
few-shot examples provided. Such similarity could
largely diminish the dataset’s value. Empirical
analysis suggests the issue could be attributed to
LLM generating general codebases for input/out-
put snippets when insufficient context is provided.
As a countermeasure, we propose to introduce code
editing scenarios for input/output code generation.
We present some examples in Figure 9,10,11 in
Appendix D, where we generally observe that in-
stances generated with scenario demonstrate higher
quality in terms of richer context and code structure
compared to those without.
For each generated instruction, we first
prompted ChatGPT to generate practical events as
“real-world” scenarios where the editing instruction
could be performed, and randomly select one for
instance generation in the next step. Subsequently,
the LLM was instructed to generate samples that
correspond with the instruction and scenario, en-
suring the codebases and variable names are appro-
priate. The prompt used can be found in Table 4 in
Appendix C.
By incorporating scenario-conditional genera-
tion, the resulting samples exhibit increased vari-
ability regarding codebases and variable naming,
thus augmenting the diversity of InstructCoder.
4.4 Postprocessing
Following Self-Instruct (Wang et al., 2022a), dedu-
plication was applied on the generated instructions
to remove instructions that have a ROUGE-L (Lin,
2004) overlap score larger than 0.7 with the ex-
isting instructions. For the code, we employed
5

--- PAGE 6 ---
Figure 4: Token length distribution of InstructCoder
MinHash with Locality Sensitive Hashing (LSH)
indexing to remove instances with a Jaccard simi-
larity greater than 0.75. Ultimately, InstructCoder
comprises over 114,000 distinct code editing tasks.
For experimental purposes, we designated 95%
of the tasks for training, while the remaining 5%
formed our validation set.
5 Data Analysis
We analyze InstructCoder in terms of 1) diversity,
2) complexity, and 3) correctness. We provide
distribution and complexity analyses of the task
instances. Finally, we demonstrate through human
investigation that our data is highly reliable.
5.1 Statistic Overview
InstructCoder comprises over 114k code editing
instructions, each paired with an input/output in-
stance. The token length distribution of input/out-
put can be viewed in Figure 4 and Table 5 in Ap-
pendix E. Most of the data falls within a reasonable
range in terms of length, while some extreme val-
ues reflect the breadth of our dataset.
5.2 Instruction Diversity
To explore the diversity of tasks in InstructCoder
and their practical applicability, we present various
instruction intents i.e. what the code edits intend
to accomplish, and instruction verbs, i.e. how the
code edit is accomplished.
Instruction Intents. We asked ChatGPT to clas-
sify the types of code edits in our dataset and manu-
ally identified 27 empirical genres. Figure 2 shows
the distribution of the code edit intent categories
in InstructCoder, which include adding functional-
ities, optimizing code, improving readability, etc.These objectives underscore the extensive range of
InstructCoder.
Instruction Verbs. The diversity of instruction
verbs is also portrayed in Figure 3a. We demon-
strate the top 20 root verbs and their top 4 direct
nouns both ranked by frequency. While a great
portion of the instructions can be roughly clustered
ascreation (e.g. “add”, “implement”, “creat”) and
modification (e.g. “modify”, “replace”, “change”),
InstructCoder presents a long-tail distribution with
less common verbs other than the top-20 taking
up 25.0% percentage. This demonstrates that the
dataset contains a wide spectrum of instructions.
5.3 Scenario Diversity
InstructCoder is designed to cover a wide range
of scenarios. As discussed in Section 4.3, each in-
struction was accompanied by different scenarios
where the editing instruction could be performed
to improve diversity. A word cloud is provided to
show some of the scenario domains in our dataset,
as illustrated in Figure 3b, with each sector refer-
ring to a different domain. The diversity of the
dataset is emphasized by the presence of a wide
range of domains such as image processing, web
development, and cybersecurity.
5.4 Complexity
We reflect the complexity of a code edit task using
the number of differing lines and their edit ratio in
the input/output pair, which are defined as:
ndiff=|I∪O\I∩O|, (1)
rdiff=ndiff
|I∪O|, (2)
where IandOare sets of input/output code with
single lines as elements.
We measure the differing lines of a code-editing
task instance using the Python library difflib .5We
found that the average number of differing lines in
InstructCoder is 11.9 and the average edit ratio is
0.52. These values suggest a fairly acceptable level
of complexity, indicating that the dataset is neither
too easy nor too hard. InstructCoder strikes a bal-
ance in terms of complexity, making it well-suited
for finetuning and evaluating LLMs in a wide range
5https://docs.python.org/3/library/difflib.
html
6

--- PAGE 7 ---
Question Pass
Determine if the instruction is valid. 97%
Is the output an acceptable edited code
response to the instruction and input?90%
Table 2: Quality check questions and results on a ran-
domly sampled subset with 200 data points.
of code editing tasks. Figure 12 in Appendix E il-
lustrates the distribution of the number of differing
lines.
5.5 Correctness
We further randomly sampled 200 instances and
invite annotators to evaluate the instances based on
two criteria: the validity of the instructions and the
correctness of the outputs. The validity assessment
focused on determining if the instructions exhibit
clear and appropriate editing intents. The correct-
ness evaluation examines if the input-output pairs
reflect the changes specified by the instructions.
The results in Table 2 indicate that most instruc-
tions in the InstructCoder dataset are valid. A few
instances exhibited noise and occasional failure to
follow the instructions, but high correctness was
found overall. Out of the 200 evaluated instances,
180 were successfully solved, showcasing the over-
all quality and reliability of InstructCoder.
6 Experiments
6.1 Setup
Training. We experiment with two families
of open-source language models with various
sizes: LLaMA (LLaMA, LLaMA-2 and Code
LLaMA) (Touvron et al., 2023a,b; Roziere et al.,
2023) and BLOOM (Scao et al., 2022).
LLaMA is a series of LLMs with parameters
ranging from 7 to 65 billion. They have been pre-
trained on a vast corpus, of which approximately
4.5% comprises code. The LLaMA-2 series ex-
tends the family with more intensive pre-training.
Additionally, Code LLaMAs are built on LLaMA-2
and specifically trained on 500B tokens of code to
enhance its code understanding and generation ca-
pabilities. BLOOM is a multilingual LLM capable
of generating human-like outputs in 46 languages
and 13 programming languages.
A full finetuning updating all the parameters
in an LLM can be computationally expensive.Model SizeAccuracy (%)∆Accw/o ft w/ ft
ChatGPT (gpt-3.5-turbo-0613) - 57.73 - -
BLOOM3B 0.52 15.46 + 14.94
7B 1.03 19.59 + 18.56
LLaMA-17B 2.57 26.80 + 24.23
13B 6.19 28.35 + 22.16
33B 6.19 41.75 + 35.56
LLaMA-27B 4.12 27.32 + 23.20
13B 14.95 34.54 + 19.59
Code LLaMA7B 29.90 45.88 + 15.98
13B 28.86 57.22 + 28.36
Table 3: Models finetuned with InstructCoder signifi-
cantly improve in code edit accuracy on EditEval, re-
gardless of the model family or model size.
Instead, we adopt LoRA (Hu et al., 2022), a
parameter-efficient finetuning method that opti-
mizes an approximated low-rank delta matrix of
the fully-connected layers. In this way we could
fine-tune a 33B model in a single A100-80GB GPU
card. In our experiments, LoRA is applied to the
query, key, value, and output transform weights
of the Transformer architecture (Vaswani et al.,
2017). All hyperparameters can be found in Table
6 in Appendix F.
Baselines. We select ChatGPT (OpenAI, 2022),
GPT-4 (OpenAI, 2023) and GPT-4 Turbo as strong
baselines. The aforementioned open-source mod-
els along with an instruction-tuned LLaMA model
called Alpaca (Taori et al., 2023) are included, and
their zero-shot performance is reported.
Concurrent to our work, CodeAlpaca6is a popu-
lar dataset generated with the pipeline of Alpaca,
differing in that its seed data is replaced by hand-
written easy instructions with short programs. We
fine-tune LLaMA models with CodeAlpaca and
Alpaca and compare the results.
7 Results
7.1 Finetuning Efficacy with InstructCoder
In this section, we demonstrate the value of our
InstructCoder dataset. Table 3 presents a detailed
comparison of EditEval performance across mod-
els fine-tuned with InstructCoder and baseline mod-
els. While very low accuracies are observed in
open-source plain models, finetuning with Instruct-
Coder significantly boost the accuracy, highlight-
6https://github.com/sahil280114/codealpaca
7

--- PAGE 8 ---
103104105
Size of Training Set15202530354045Code Edit Accuracy %
14.6918.0426.8022.6823.2028.3530.9331.9641.75LLaMA 33B
LLaMA 13B
LLaMA 7BFigure 5: Data scaling performance of InstructCoder
on LLaMA evaluated on EditEval, using 1%, 10% and
100% training data.
ing the effectiveness of efficient instruction fine-
tuning with machine-generated code edit pairs.
Code LLaMA 13B matches ChatGPT’s perfor-
mance and surpasses other open-source models
with a 57.22% accuracy rate. The more substan-
tial LLaMA-33B model shows a notable 35.56%
improvement, yet it falls behind Code LLaMA-7B,
which benefits from extensive pre-training on code.
For qualitative results, see Appendix G.
As expected, the pre-training foundation of
LLM significantly influences code-editing effi-
cacy. LLaMA demonstrated higher accuracies than
BLOOM models of similar sizes. Among LLa-
MAs, those pre-trained on more tokens (LLaMA-2
series) outperformed earlier versions. Furthermore,
Code LLaMAs exceed LLaMA-2 models as a re-
sult of their extensive pre-training specifically on
coding data. Despite the varying capabilities of
the foundational models, our dataset consistently
enhances performance.
7.2 Dataset Scaling
InstructCoder has a scale considerably smaller than
what LLMs are typically pre-trained on. To ascer-
tain the sufficiency of this scale, we conducted
an experiment wherein we fine-tuned the LLaMA
models using varying proportions (1%, 10%, and
100%) of the dataset. The smaller subsets are guar-
anteed to be encompassed within the larger subsets.
The results are shown in Figure 5. The identified
trend demonstrates a positive correlation between
the model’s accuracy and the scale of the training
set.
Fine-tuned with merely 1% of the data, the mod-
els experience a limited number of parameter up-
dates but quickly adapt to the tasks, surpassing
(0, 0.2) [0.2, 0.4) [0.4, 0.6) [0.6, 0.8) [0.8, 1.0)
Edit Ratio Range405060708090100Code Edit Accuracy %GPT-4 Evaluation of Each Edit Ratio Range
7b
13b
33bFigure 6: GPT-4 evaluation results at different edit ra-
tios on 2000 validation samples.
their respective zero-shot accuracy scores by signif-
icant margins. This underscores the significance of
instruction tuning. As the volume of training data
increases, we observe consistent improvements in
model accuracy, approximately growing linearly
with respect to the logarithmic scale of the number
of samples. Crucially, our experiment empirically
suggests that larger models are more effective with
a constrained training compute budget.
7.3 Edit Ratio
Figure 6 depicts the accuracy of fine-tuned LLaMA
models as evaluated by GPT-4 across five edit ra-
tio levels, using 2000 random samples from the
validation set. This evaluation, justified in Ap-
pendix H, involves prompting GPT-4 for a quick
and general assessment of code edits, offering an
alternative perspective to code edit evaluation. In
this assessment, larger models consistently outper-
form their smaller counterparts. Notably, accuracy
decreases with lower edit ratios, potentially due to
the models adopting the shortcut of copying inputs
to minimize loss in scenarios requiring fewer edits.
This trend, however, is less pronounced in larger
models, which show a greater ability to discern
subtle differences in cases of low edit ratios.
8 Conclusion
We introduce InstructCoder, the first instruction-
tuning dataset for general-purpose code-editing
tasks. It comprises generations of LLMs, where
real GitHub commits serve as seed tasks to guide
the generation process. A scenario-conditional
approach is introduced to ensure both diversity
and high quality of the data. Our experiments
on the novel EditEval benchmark show that open-
8

--- PAGE 9 ---
source models can gain huge improvements and
even yield performance matching proprietary mod-
els through computationally lightweight parameter-
efficient fine-tuning with InstructCoder. We also
reveal that the LLM base model and the scale of
fine-tuning data are both profound factors of code-
editing ability. We hope the dataset can benefit and
inspire more research in this area towards building
more powerful coding models.
Limitations
Our approach did not encompass code changes in-
volving multi-file contexts, which might be useful
in development. We hope to explore these aspects
further and incorporate additional programming
languages in our future research.
References
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,
Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and
Donald Metzler. 2022. Ext5: Towards extreme multi-
task scaling for transfer learning. In The Tenth In-
ternational Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 . Open-
Review.net.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
2021. Program synthesis with large language models.
arXiv preprint arXiv:2108.07732 .
Stella Biderman, Hailey Schoelkopf, Quentin An-
thony, Herbie Bradley, Kyle O’Brien, Eric Halla-
han, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al. 2023.
Pythia: A suite for analyzing large language mod-
els across training and scaling. arXiv preprint
arXiv:2304.01373 .
Sidney Black, Stella Biderman, Eric Hallahan,
Quentin Gregory Anthony, Leo Gao, Laurence Gold-
ing, Horace He, Connor Leahy, Kyle McDonell,
Jason Phang, Michael Martin Pieler, USVSN Sai
Prashanth, Shivanshu Purohit, Laria Reynolds,
Jonathan Tow, Ben Wang, and Samuel Weinbach.
2022. GPT-neox-20b: An open-source autoregres-
sive language model. In Challenges & Perspectives
in Creating Large Language Models .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shotlearners. Advances in neural information processing
systems , 33:1877–1901.
Saikat Chakraborty, Yangruibo Ding, Miltiadis Allama-
nis, and Baishakhi Ray. 2020. Codit: Code editing
with tree-based neural models. IEEE Transactions
on Software Engineering , 48(4):1385–1399.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021a. Evaluating large lan-
guage models trained on code. arXiv preprint
arXiv:2107.03374 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021b. Evaluating large lan-
guage models trained on code. arXiv preprint
arXiv:2107.03374 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac
Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sha-
ran Narang, Gaurav Mishra, Adams Yu, Vincent Y .
Zhao, Yanping Huang, Andrew M. Dai, Hongkun
Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,
Adam Roberts, Denny Zhou, Quoc V . Le, and Jason
Wei. 2022. Scaling instruction-finetuned language
models. CoRR , abs/2210.11416.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and psychological mea-
surement , 20(1):37–46.
Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu,
and Vincent J Hellendoorn. 2020. Patching as trans-
lation: the data and the metaphor. In Proceedings
of the 35th IEEE/ACM International Conference on
Automated Software Engineering , pages 275–286.
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,
Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih,
Luke Zettlemoyer, and Mike Lewis. 2023. Incoder:
A generative model for code infilling and synthe-
sis. In The Eleventh International Conference on
Learning Representations .
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2023. Gptscore: Evaluate as you desire. arXiv
preprint arXiv:2302.04166 .
9

--- PAGE 10 ---
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de Rosa, Olli Saarikivi, et al. 2023. Textbooks are
all you need. arXiv preprint arXiv:2306.11644 .
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2022. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. arXiv
preprint arXiv:2212.09689 .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Carlos E. Jimenez, John Yang, Alexander Wettig,
Shunyu Yao, Kexin Pei, Ofir Press, and Karthik
Narasimhan. 2023. Swe-bench: Can language mod-
els resolve real-world github issues?
René Just, Darioush Jalali, and Michael D Ernst. 2014.
Defects4j: A database of existing faults to enable
controlled testing studies for java programs. In Pro-
ceedings of the 2014 international symposium on
software testing and analysis , pages 437–440.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou,
Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
et al. 2023a. Starcoder: may the source be with you!
arXiv preprint arXiv:2305.06161 .
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
2023b. Textbooks are all you need ii: phi-1.5 techni-
cal report. arXiv preprint arXiv:2309.05463 .
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023. Gpteval:
Nlg evaluation using gpt-4 with better human align-
ment. arXiv preprint arXiv:2303.16634 .
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 .
Aman Madaan, Alexander Shypula, Uri Alon, Milad
Hashemi, Parthasarathy Ranganathan, Yiming Yang,
Graham Neubig, and Amir Yazdanbakhsh. 2023.Learning performance-improving code edits. arXiv
preprint arXiv:2302.07867 .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470–3487, Dublin, Ireland.
Association for Computational Linguistics.
Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-
vio Savarese, and Yingbo Zhou. 2023a. Codegen2:
Lessons for training llms on programming and natu-
ral languages. arXiv preprint arXiv:2305.02309 .
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,
Huan Wang, Yingbo Zhou, Silvio Savarese, and
Caiming Xiong. 2023b. Codegen: An open large
language model for code with multi-turn program
synthesis. In The Eleventh International Conference
on Learning Representations .
OpenAI. 2022. Introducing ChatGPT. https://
openai.com/blog/chatgpt .
OpenAI. 2023. Gpt-4 technical report. https://
arxiv.org/pdf/2303.08774 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al.
2023. Code llama: Open foundation models for
code. arXiv preprint arXiv:2308.12950 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin
Chen, Pierre-Antoine Manzagol, Charles Sutton, and
Edward Aftandilian. 2020. Learning to fix build er-
rors with graph2diff neural networks. In Proceedings
of the IEEE/ACM 42nd international conference on
software engineering workshops , pages 19–20.
10

--- PAGE 11 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023a. Llama: Open and ef-
ficient foundation language models. arXiv preprint
arXiv:2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems , 30.
Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang
Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.
2023a. Is chatgpt a good nlg evaluator? a preliminary
study. arXiv preprint arXiv:2303.04048 .
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,
Rangan Majumder, and Furu Wei. 2023b. Improving
text embeddings with large language models. arXiv
preprint arXiv:2401.00368 .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022a. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560 .
Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
Anjana Arunkumar, David Stap, Eshaan Pathak,
Giannis Karamanolakis, Haizhi Lai, Ishan Puro-
hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,
Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,
Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,
Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,
Shailaja Keyur Sampat, Siddhartha Mishra, Sujan
Reddy A, Sumanta Patro, Tanay Dixit, and Xudong
Shen. 2022b. Super-NaturalInstructions: General-
ization via declarative instructions on 1600+ NLP
tasks. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 5085–5109, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Zhen Wang, Rameswar Panda, Leonid Karlinsky, Roge-
rio Feris, Huan Sun, and Yoon Kim. 2023c. Multi-
task prompt tuning enables parameter-efficient trans-
fer learning. In The Eleventh International Confer-
ence on Learning Representations .Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned
language models are zero-shot learners. In The Tenth
International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022 .
OpenReview.net.
Jiayi Wei, Greg Durrett, and Isil Dillig. 2023. Coeditor:
Leveraging contextual changes for multi-round code
auto-editing. arXiv preprint arXiv:2305.18584 .
Fuzhao Xue, Kabir Jain, Mahir Hitesh Shah, Zangwei
Zheng, and Yang You. 2023. Instruction in the wild:
A user-based instruction dataset. https://github.
com/XueFuzhao/InstructionWild .
Pengcheng Yin, Graham Neubig, Miltiadis Allama-
nis, Marc Brockschmidt, and Alexander L Gaunt.
2018. Learning to represent edits. arXiv preprint
arXiv:1810.13337 .
Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan
Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang,
Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023.
Codegeex: A pre-trained model for code generation
with multilingual evaluations on humaneval-x.
Terry Yue Zhuo. 2023. Large language models are
state-of-the-art evaluators of code generation. arXiv
preprint arXiv:2304.14317 .
11

--- PAGE 12 ---
A An Example Test of EditEval
An example of EditEval the test set is showcased below. To accomplish the task, the model must not only
adhere to the user’s instructions but also comprehend the input code in the context provided.
Figure 7: An example instance of EditEval.
12

--- PAGE 13 ---
B Comparing Machine-Generated Data and Real-World Data
LLaMA-1-7B LLaMA-1-13B
Model Size0510152025AccuracyBase Model
Github Commits
Alpaca
InstructCoder
Figure 8: EditEval accuracies of instruction fine-tuned LLaMA-1 models (7B and 13B) with GitHub commits and
other datasets. InstructCoder significantly outperformed GitHub commits, and the lead is more pronounced with a
larger base model, indicating the effectiveness of InstructCoder. Conversely, fine-tuning with raw GitHub commits
yields poor results, and is the worst among all three data sources on LLaMA-1 13B.
Given the substantial repository of code and commit data available on GitHub, a natural idea is to utilize
these real-world data to fine-tune a model to perform code editing. However, as discussed in Section 4.1,
these data from GitHub can be extremely noisy, especially in the commit messages, rendering them a
sub-optimal choice for instruction-tuning. On the other hand, machine-generated data is increasingly
recognized for its utility, as evidenced by various studies that achieves enhanced results with this type
of data (Gunasekar et al., 2023; Li et al., 2023b; Wang et al., 2023b). This approach provides better
controllability over the distribution of the generated contents and facilitates the collection of diverse data,
including those under-represented or difficult to mine and clean from real-world data.
The experiment results in Figure 8 corroborate the usage of machine-generated data. We further
collected GitHub commits matching the size of InstructCoder, and used the same hyperparameters for
instruction fine-tuning. As can be seen in the results, InstructCoder significantly outperformed raw
GitHub commits, and the lead is more profound with a larger base model, demonstrating the effectiveness
of InstructCoder. On the other hand, fine-tuning with GitHub commits yields poor results, and is the worst
among all three data sources on LLaMA-1 13B. The observation suggests that using machine-generated
data for instruction fine-tuning is superior in terms of training code editing models.
13

--- PAGE 14 ---
C Prompts
The prompts used in our data collection and experiments are listed in Table 4.
Stage Prompt
Instruction Generation Given the existing instructions, please generate a list of
diverse Python code editing instructions. The new instructions
should address diverse editing tasks. Please ensure that the
instructions are clear and diverse. Include any relevant
variable names in the instructions.
Scenario Generation Given a Python code editing task, please come up with 10
diverse scenarios with concise descriptions of where this
task could be performed or come from.
Instance Generation Given Python code editing task instructions and their
scenarios where the task instruction could be used, you need
to come up with examples for the following code editing tasks.
You need to generate an input and output code pair and make
sure your variable names are suitable for the scenario. The
input code is related to the task instruction, but must NOT
meet the task requirements. The output code fulfills the task
requirements based on the input code.
GPT4 Evaluation Given a code editing instruction, please determine if the
output is an acceptable edited code response to the instruction
and input. Give "Yes" or "No".
Table 4: Prompts used in this work.
14

--- PAGE 15 ---
D Qualitative Examples of Scenario-Conditional Generation
Three comparisons are presented, each showing instances that were generated with or without the
inclusion of a scenario.
Figure 9: Example instance #1 generated without scenario (Left) and with scenario (Right)
Figure 10: Example instance #2 generated without scenario (Left) and with scenario (Right)
15

--- PAGE 16 ---
Figure 11: Example instance #3 generated without scenario (Left) and with scenario (Right)
16

--- PAGE 17 ---
E Additional statistics of InstructCoder
Token Length Instruction Input Output
mean 21.85 172.03 248.43
25% 17 99 138
50% 21 147 213
75% 26 218 321
min 3 10 10
max 116 1019 1024
Table 5: Token length statistics using the LLaMA (Touvron et al., 2023a) tokenizer.
Figure 12: Edit rows distribution of InstructCoder. Numbers greater than 40 are aggregated as the last bin.
17

--- PAGE 18 ---
F Hyperparameters
The hyperparameters used in all finetuning experiments are listed in Table 6. For all inferences, we utilize
greedy decoding. For OpenAI’s GPTs, we achieve this by setting its temperature to 0.
Hyperparameter Value
learning rate 0.0003
batch size 128
epochs 3
max sentence length 1024
lora rank 16
lora dropout 0.05
lora modules key, query, value, output
Table 6: Hyperparameters used for finetuning language models.
G Qualitative Examples Generated by Finetuned LLaMA-33B
We demonstrate some qualitative example responses generated by finetuned LLaMA-33B.
Figure 13: Qualitative examples generated by finetuned LLaMA-33B
18

--- PAGE 19 ---
H Alignment of GPT-4 Evaluation and Human Evaluation
Due to the extremely demanding nature of creating automated tests, we seek to investigate the viability
of using GPT-4 as an automatic evaluator to lessen the extensive human effort involved. Using LLMs
as generation evaluators has been demonstrated effective in NLG tasks (Liu et al., 2023; Wang et al.,
2023a; Fu et al., 2023), and especially in code generation (Zhuo, 2023). To further validate this idea, we
collected an additional 134 commits data testing purposes and processed them in the same manner as the
seed tasks. Both GPT-4 evaluation and human evaluation are conducted on this dataset to assess their
alignment.
Human evaluation. Each sample is annotated by three examiners, and the average accuracy is recorded.
We developed an annotation tool to ensure the impartiality of evaluation (see Figure 14 for the user
interface). Generations of different models are shuffled and the anonymity of the models is guaranteed.
The edit is annotated as correct if it correctly reflects the instruction demands and wrong if it fails to
follow the instruction.
GPT-4 evaluation. We ask GPT-4 to evaluate if the code edit is an acceptable response to the input and
collect the correct rate. The prompts for GPT-4 evaluation can be found in C.
Results. We carry out the experiments on the code edits generated by ChatGPT and LLaMA of three
sizes fine-tuned with InstructCoder. While we found that the human annotators are always slightly
stricter than the GPT-4 evaluator, the overall Cohen’s Kappa value of the GPT-4 evaluations and human
evaluations reaches 0.665, which is substantial according to Cohen (1960). This renders GPT-4 evaluation
as a convenient and effective method for evaluating the correctness of code edit tasks.
19

--- PAGE 20 ---
Figure 14: A screenshot of our human scoring annotation tool.
20

--- PAGE 21 ---
I Data Filtering Process
The detailed process of filtering the dataset is listed below:
•We selected GitHub repos with over 100 stars to ensure the overall quality. We only utilized repos
with permissive licenses (MIT, Apache-2.0, GPL-3.0, GPL-2.0, BSD-2.0, BSD-3.0, LGPL-2.1,
LGPL-3.0, AGPL-3.0).
•We kept commits in which only one single .py file was changed. Using git-diff, we identified and
preserved commits where only one code block was changed.
• We discarded commits with single-word or empty commit messages.
• We removed commits with over 100 edited rows.
Manual:
• We discarded rare commits containing inappropriate language.
• We discarded commits where the change in the source code does not match the commit message.
• We filtered out project-specific adjustments that lack sufficient context.
•We utilized Codex (Chen et al., 2021a) to rewrite ambiguous commit messages, enhancing the clarity
of the intended code edits.
21
