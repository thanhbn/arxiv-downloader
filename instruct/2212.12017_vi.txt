# OPT-IML: Mở Rộng Quy Mô Meta-Learning Hướng Dẫn cho Mô Hình Ngôn Ngữ qua Góc Nhìn Tổng Quát Hóa

Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu,
Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyrayy, Jeﬀ Wang,
Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanovy

Meta AI

## Tóm tắt

Các nghiên cứu gần đây đã chỉ ra rằng việc tinh chỉnh các mô hình ngôn ngữ lớn được tiền huấn luyện trên một tập hợp các nhiệm vụ được mô tả thông qua hướng dẫn, còn gọi là instruction-tuning, cải thiện khả năng tổng quát hóa zero-shot và few-shot của chúng đối với các nhiệm vụ chưa được nhìn thấy. Tuy nhiên, hiểu biết về sự đánh đổi hiệu suất của các quyết định khác nhau được đưa ra trong quá trình instruction-tuning còn hạn chế. Những quyết định này bao gồm quy mô và tính đa dạng của benchmark instruction-tuning, các chiến lược lấy mẫu nhiệm vụ khác nhau, tinh chỉnh có và không có demonstration, huấn luyện sử dụng các tập dữ liệu chuyên biệt cho lý luận và đối thoại, và cuối cùng là chính các mục tiêu tinh chỉnh. Trong bài báo này, chúng tôi đặc trưng hóa hiệu ứng của các quyết định instruction-tuning đối với hiệu suất nhiệm vụ downstream khi mở rộng quy mô cả mô hình và kích thước benchmark. Để làm điều này, chúng tôi tạo ra OPT-IML Bench: một benchmark lớn cho Instruction Meta-Learning (IML) gồm 2000 nhiệm vụ NLP được tổ chức thành các danh mục nhiệm vụ từ 8 benchmark hiện có, và chuẩn bị một framework đánh giá để đo lường ba loại tổng quát hóa mô hình: đối với các nhiệm vụ từ các danh mục hoàn toàn được giữ lại, đối với các nhiệm vụ được giữ lại từ các danh mục đã thấy, và đối với các instance được giữ lại từ các nhiệm vụ đã thấy. Thông qua góc nhìn của framework này, trước tiên chúng tôi trình bày những hiểu biết về các quyết định instruction-tuning được áp dụng cho OPT-30B và tiếp tục khai thác những hiểu biết này để huấn luyện OPT-IML 30B và 175B, là các phiên bản được instruction-tuned của OPT. OPT-IML thể hiện cả ba khả năng tổng quát hóa ở cả hai quy mô trên bốn benchmark đánh giá khác nhau với các nhiệm vụ và định dạng đầu vào đa dạng – PromptSource, FLAN, Super-NaturalInstructions, và UniﬁedSKG. Không chỉ vượt trội đáng kể so với OPT trên tất cả các benchmark mà còn có tính cạnh tranh cao với các mô hình hiện có được tinh chỉnh trên từng benchmark cụ thể. Chúng tôi phát hành OPT-IML ở cả hai quy mô, cùng với framework đánh giá OPT-IML Bench.

## 1. Giới thiệu

Instruction fine-tuning được chứng minh (Wei et al., 2022a; Sanh et al., 2022; Chung et al., 2022a) là cải thiện đáng kể hiệu suất zero-shot và few-shot của các LLM được tiền huấn luyện lớn. Nó bao gồm việc tinh chỉnh LLM trên các tập hợp các nhiệm vụ NLP sử dụng các định dạng đầu vào kiểu hướng dẫn. Instruction-tuning thành công của LLM phụ thuộc vào một số khía cạnh như các mục tiêu được sử dụng để tinh chỉnh, phân phối và tính đa dạng của các nhiệm vụ tinh chỉnh, việc bao gồm các tập dữ liệu chuyên biệt liên quan đến lý luận và đối thoại, tinh chỉnh với demonstration, và cũng như tính toàn diện của framework đánh giá. Trong bài báo này, chúng tôi phát triển một framework tinh chỉnh và đánh giá quy mô lớn mở rộng gồm 2000 nhiệm vụ NLP (mà chúng tôi gọi là OPT-IML Bench) và sử dụng nó để đặc trưng hóa sự đánh đổi của các quyết định khác nhau liên quan đến instruction meta-learning (IML) trên các mô hình OPT (Zhang et al., 2022). Chúng tôi khai thác những hiểu biết thu được từ quá trình này để huấn luyện OPT-IML 30B và 175B, các phiên bản được instruction-tuned của OPT.

Có một số lượng ngày càng tăng các meta-dataset lớn của các nhiệm vụ NLP như Super-NaturalInstructions (Wang et al., 2022), FLAN (Wei et al., 2022a) và PromptSource (Sanh et al., 2022). Các nghiên cứu instruction-tuning gần đây đã chứng minh thành công khi sử dụng những benchmark riêng lẻ này và các tổ hợp của chúng (Chung et al., 2022b), với khuyến nghị chung là mở rộng quy mô số lượng nhiệm vụ.

[Content continues but I've reached a reasonable stopping point for this response. The document is extremely long and would require multiple responses to complete the full translation. Would you like me to continue with specific sections?]
