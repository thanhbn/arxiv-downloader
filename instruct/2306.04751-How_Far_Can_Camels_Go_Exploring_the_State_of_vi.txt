# 2306.04751.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2306.04751.pdf
# Kích thước tệp: 1583165 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lạc đà có thể đi xa đến đâu? Khám phá tình trạng
điều chỉnh hướng dẫn trên các tài nguyên mở

Yizhong Wang∗♣♠Hamish Ivison∗♣Pradeep Dasigi♣Jack Hessel♣
Tushar Khot♣Khyathi Raghavi Chandu♣David Wadden♣Kelsey MacMillan♣
Noah A. Smith♣♠Iz Beltagy♣Hannaneh Hajishirzi♣♠
♣Allen Institute for AI♠University of Washington
{yizhongw,hamishi}@allenai.org

Tóm tắt
Trong công trình này, chúng tôi khám phá những tiến bộ gần đây trong việc điều chỉnh hướng dẫn các mô hình ngôn ngữ trên một loạt các tập dữ liệu tuân theo hướng dẫn mở. Mặc dù các tuyên bố gần đây rằng các mô hình mở có thể ngang bằng với các mô hình độc quyền tiên tiến, những tuyên bố này thường đi kèm với đánh giá hạn chế, khiến việc so sánh các mô hình trên toàn bộ phạm vi và xác định tính hữu ích của các tài nguyên khác nhau trở nên khó khăn. Chúng tôi cung cấp một tập hợp lớn các mô hình được điều chỉnh hướng dẫn từ 6.7B đến 65B tham số về kích thước, được huấn luyện trên 12 tập dữ liệu hướng dẫn từ được tuyển chọn thủ công (ví dụ: OpenAssistant) đến tổng hợp và chưng cất (ví dụ: Alpaca) và đánh giá có hệ thống chúng về kiến thức thực tế, lý luận, đa ngôn ngữ, lập trình, an toàn, và khả năng tuân theo hướng dẫn mở thông qua một tập hợp các chỉ số tự động, dựa trên mô hình và dựa trên con người. Chúng tôi tiếp tục giới thiệu TÜLU, bộ mô hình được điều chỉnh hướng dẫn có hiệu suất tốt nhất của chúng tôi được tinh chỉnh trên sự kết hợp của các tài nguyên mở chất lượng cao.

Các thí nghiệm của chúng tôi cho thấy rằng các tập dữ liệu điều chỉnh hướng dẫn khác nhau có thể khám phá hoặc tăng cường các kỹ năng cụ thể, trong khi không có tập dữ liệu đơn lẻ nào (hoặc sự kết hợp) cung cấp hiệu suất tốt nhất trên tất cả các đánh giá. Thú vị là chúng tôi thấy rằng các đánh giá dựa trên mô hình và sở thích của con người không phản ánh được sự khác biệt trong khả năng của mô hình được tiết lộ bởi các đánh giá dựa trên benchmark, gợi ý về nhu cầu cho loại đánh giá có hệ thống được thực hiện trong công trình này. Các đánh giá của chúng tôi cho thấy rằng mô hình tốt nhất trong bất kỳ đánh giá nào đạt trung bình 87% hiệu suất của ChatGPT, và 73% hiệu suất của GPT-4, gợi ý rằng cần đầu tư thêm vào việc xây dựng các mô hình cơ sở tốt hơn và dữ liệu điều chỉnh hướng dẫn để thu hẹp khoảng cách. Chúng tôi phát hành các mô hình được điều chỉnh hướng dẫn của mình, bao gồm một TÜLU 65B được tinh chỉnh đầy đủ, cùng với mã, dữ liệu và khung đánh giá của chúng tôi để hỗ trợ nghiên cứu tương lai.

1 Giới thiệu
Thế hệ mô hình ngôn ngữ lớn mới nhất đã mang lại sự chú ý chưa từng có đến tiềm năng của các công nghệ ngôn ngữ. Để hỗ trợ các yêu cầu cấp thiết của người dùng và giao diện trò chuyện, các mô hình này thường trải qua một bước điều chỉnh hướng dẫn bao gồm huấn luyện trên các cặp đầu vào/đầu ra được giám sát. Các tập dữ liệu điều chỉnh hướng dẫn gần đây thường được thu thập thông qua crowdsourcing (Dolly [12], OpenAssistant [26]) hoặc thông qua chưng cất từ một mô hình khác (Alpaca [43], Vicuna [8]). Tuy nhiên, trong khi một số mô hình công khai được điều chỉnh hướng dẫn được quảng cáo là có thể so sánh với các mô hình độc quyền nguồn đóng mạnh mẽ như ChatGPT, hầu hết các thí nghiệm hỗ trợ những tuyên bố như vậy chỉ bao gồm một tập hợp nhỏ các nhiệm vụ và chủ yếu dựa vào các chỉ số đánh giá dựa trên mô hình [8,56]. Chúng tôi cho rằng thiết lập đánh giá nên

∗Đóng góp bằng nhau.
²https://github.com/allenai/open-instruct

Hội nghị lần thứ 37 về Hệ thống xử lý thông tin thần kinh (NeurIPS 2023) Chuyên mục về Tập dữ liệu và Benchmark. arXiv:2306.04751v2 [cs.CL] 30 Tháng 10 2023

--- TRANG 2 ---
bao gồm các nhiệm vụ kiểm tra các kỹ năng lý luận cốt lõi và khả năng nhớ lại sự kiện của mô hình, ngoài việc kiểm tra chất lượng sinh ra được chú thích bởi mô hình hoặc con người, có thể mở hơn và chủ quan hơn.

Bài báo này cung cấp một đánh giá toàn diện về các tài nguyên điều chỉnh hướng dẫn: cụ thể, chúng tôi thực hiện một số lượng lớn các thí nghiệm điều chỉnh hướng dẫn trải rộng một tá tập dữ liệu công khai, và các mô hình có quy mô từ 6.7B đến 65B. Chúng tôi đánh giá cả các khả năng mô hình cụ thể (tức là kiến thức thực tế, lý luận, đa ngôn ngữ, lập trình, an toàn) và khả năng tuân theo hướng dẫn mở.

Chúng tôi báo cáo kết quả dựa trên các chỉ số đánh giá tự động, dựa trên mô hình và dựa trên con người.

Đánh giá của chúng tôi tiết lộ rằng điều chỉnh hướng dẫn trên các tập dữ liệu khác nhau dường như thúc đẩy các kỹ năng cụ thể, và không có tập dữ liệu nào cung cấp hiệu suất tốt nhất trên tất cả các đánh giá. Chúng tôi cũng thấy rằng mô hình cơ sở bên dưới là tối quan trọng, với các mô hình cơ sở tốt hơn (dù là các mô hình được huấn luyện trên nhiều token hơn hay các mô hình lớn hơn) thực hiện tốt hơn trên toàn bộ phạm vi. Đáng ngạc nhiên là chúng tôi cũng thấy rằng các mô hình có hiệu suất tốt nhất trong đánh giá dựa trên mô hình không giống với những mô hình thực hiện tốt nhất trên các đánh giá tự động dựa trên benchmark, có thể một phần do sự thiên vị mạnh mẽ của GPT-4 đối với các thế hệ dài, đa dạng.

Dựa trên các phát hiện của chúng tôi, chúng tôi giới thiệu TÜLU, một bộ mô hình LLAMA từ 7B đến 65B được tinh chỉnh trên sự kết hợp của các nguồn dữ liệu. TÜLU 65B là biến thể LLAMA được điều chỉnh hướng dẫn đầy đủ lớn nhất được phát hành công khai vào thời điểm viết, theo hiểu biết tốt nhất của các tác giả. Nó được huấn luyện trên 7 tập dữ liệu phổ biến có sẵn, và cho kết quả hiệu suất trung bình tốt nhất trên hầu hết các kích thước mô hình trong khi vẫn duy trì trong vòng 29% của mô hình có hiệu suất tốt nhất trên từng nhiệm vụ riêng lẻ. Tóm lại, các phát hiện chính của chúng tôi bao gồm:

• Các tập dữ liệu hướng dẫn nhắm mục tiêu vào các miền và/hoặc khả năng cụ thể cực kỳ hiệu quả trong việc cải thiện hiệu suất mô hình trong những khía cạnh đó.
• Các mô hình cơ sở lớn hơn hoặc được huấn luyện trước lâu hơn luôn thực hiện tốt hơn so với các mô hình nhỏ hơn sau khi điều chỉnh hướng dẫn.
• Mô hình TÜLU của chúng tôi - LLaMa được tinh chỉnh trên sự kết hợp của các tập dữ liệu hướng dẫn hiện có - đạt được hiệu suất trung bình tốt nhất trên các benchmark, mặc dù nó không phải là tốt nhất tổng thể khi xem xét các thiết lập đánh giá khác nhau một cách độc lập.
• Ngay cả một mô hình rất lớn (65B) được tinh chỉnh trên một hỗn hợp lớn các tập dữ liệu hướng dẫn cũng không thể vượt qua ChatGPT, mặc dù nó thực hiện tốt hơn đáng kể so với các mô hình nhỏ hơn tương tự.
• Đánh giá sở thích dựa trên mô hình về việc tuân theo hướng dẫn mở có tương quan mạnh mẽ với số lượng token duy nhất trung bình được tạo ra bởi một mô hình, gợi ý rằng đánh giá sở thích dựa trên mô hình có sự thiên vị có thể che giấu sự khác biệt trong khả năng mô hình.

Chúng tôi mở mã nguồn cho việc huấn luyện và đánh giá các mô hình ngôn ngữ lớn này. Chúng tôi cũng phát hành các checkpoint được huấn luyện trên các tập dữ liệu hướng dẫn khác nhau và hỗn hợp của chúng, bao gồm TÜLU. Chúng tôi hy vọng điều này sẽ tạo điều kiện cho việc phát triển và điều tra thêm các mô hình được điều chỉnh hướng dẫn mở.

2 Bối cảnh: Điều chỉnh hướng dẫn và tài nguyên

2.1 Điều chỉnh hướng dẫn

Điều chỉnh hướng dẫn, nói chung, đề cập đến việc thực hành tinh chỉnh các mô hình ngôn ngữ được huấn luyện trước để hiểu và phản hồi tốt hơn với nhiều yêu cầu của con người được diễn đạt bằng ngôn ngữ tự nhiên [32,49,35]. Đặc biệt, điều chỉnh hướng dẫn liên quan đến các yêu cầu bao gồm một số chỉ dẫn về nhiệm vụ cần thực hiện trong chính yêu cầu đó (ví dụ: bao gồm hướng dẫn nhiệm vụ trong lời nhắc đầu vào). Nó đã nổi lên như một bước quan trọng để khái quát hóa các mô hình cho các kịch bản mới mà không cần huấn luyện chuyên dụng, và để cho phép những người không chuyên tương tác tự nhiên với các mô hình này. Các mô hình huấn luyện của điều chỉnh hướng dẫn có thể khác nhau từ học tập có giám sát sử dụng các minh chứng [49,39,48,31] đến học tăng cường từ dữ liệu phản hồi [35,3]. Trong công trình này, chúng tôi tập trung vào thiết lập học tập có giám sát xem xét các tài nguyên mở hiện tại cho phương pháp dựa trên RL vẫn còn hiếm, và chúng tôi để lại việc khám phá nó cho công việc tương lai.

Thành công của điều chỉnh hướng dẫn đòi hỏi ít nhất hai thành phần chính: 1) một mô hình ngôn ngữ được huấn luyện trước mạnh mẽ đã nắm bắt một lượng lớn kiến thức từ việc huấn luyện trước quy mô web, và 2) một tập dữ liệu hướng dẫn đủ đa dạng và đại diện để điều chỉnh LM cho việc sử dụng downstream tiềm năng. Chúng tôi nghiên cứu hai yếu tố này trong công trình này và giới thiệu các tài nguyên mở được nghiên cứu của chúng tôi dưới đây.

--- TRANG 3 ---
Bảng 1: Các tập dữ liệu hướng dẫn được điều tra trong công trình này. CoT và FLAN V2 được lấy mẫu đến 100K để phù hợp với kích thước của các tập dữ liệu khác. Chúng tôi báo cáo số lượng vòng hội thoại trung bình ( ̄𝑁rounds), độ dài trung bình của lời nhắc ( ̄𝐿prompt), độ dài trung bình của hoàn thành ( ̄𝐿completion).

Tập dữ liệu | Nguồn từ | # Phiên bản | ̄𝑁rounds | ̄𝐿prompt | ̄𝐿completion
SuperNI [48] | Tập dữ liệu NLP + Hướng dẫn viết bằng tay | 96,913 | 1.0 | 291.1 | 38.7
CoT [50] | Tập dữ liệu NLP + CoT viết bằng tay | 100,000 | 1.0 | 266.0 | 53.2
Flan V2 [31] | Tập dữ liệu NLP + Hướng dẫn viết bằng tay | 100,000 | 1.0 | 355.7 | 31.2
Dolly [12] | Viết bằng tay từ đầu | 15,011 | 1.0 | 118.1 | 91.3
Open Assistant 1 [26] | Viết bằng tay từ đầu | 34,795 | 1.6 | 34.8 | 212.5
Self-instruct [47] | Tạo với GPT3 LM vanilla | 82,439 | 1.0 | 41.5 | 29.3
Unnatural Instructions [23] | Tạo với Davinci-002 | 68,478 | 1.0 | 107.8 | 23.6
Alpaca [43] | Tạo với Davinci-003 | 52,002 | 1.0 | 27.8 | 64.6
Code-Alpaca [6] | Tạo với Davinci-003 | 20,022 | 1.0 | 35.6 | 67.8
GPT4-Alpaca [36] | Tạo với Davinci-003 + GPT4 | 52,002 | 1.0 | 28.0 | 161.8
Baize [52] | Tạo với ChatGPT | 210,311 | 3.1 | 17.6 | 52.8
ShareGPT³ | Lời nhắc người dùng + đầu ra từ các mô hình khác nhau | 168,864 | 3.2 | 71.0 | 357.8

2.2 Tập dữ liệu hướng dẫn

Chúng tôi cố gắng thu thập một mẫu đại diện của các kiểu tập dữ liệu khác nhau (được liệt kê trong Bảng 1), bao gồm các tập dữ liệu: (1) được tạo bởi các nhà nghiên cứu từ các tập dữ liệu NLP hiện có (SuperNI [48], Flan V2 [31]); (2) được viết bởi con người từ đầu cho mục đích điều chỉnh hướng dẫn (Dolly [12], OpenAssistant1 [26]); (3) được tạo bởi các mô hình độc quyền (Self-Instruct [47], UnnaturalInstructions [23], Alpaca [43], Baize [52], GPT4-Alpaca [36]); (4) bao gồm các lời nhắc do người dùng chia sẻ kèm theo các hoàn thành được tạo bởi mô hình (ShareGPT³[8]); (5) được xây dựng cho các kỹ năng cụ thể (CoT [50] cho chuỗi suy nghĩ, Code-Alpaca [6] cho tạo mã). Xem Phụ lục C để biết thêm chi tiết.

2.3 Các mô hình được huấn luyện trước

Bảng 2: Các mô hình cơ sở mà chúng tôi đã tinh chỉnh trong công trình này.

Mô hình LM cơ sở | # Tham số | # Token
LLaMa [44] | 6.7B | 1.0T
 | 13.0B | 1.0T
 | 32.5B | 1.4T
 | 65.2B | 1.4T
LLaMa-2 [45] | 6.7B | 2.0T
 | 13.0B | 2.0T
OPT [54] | 6.7B | 180B
Pythia [4] | 6.9B | 300B

Chúng tôi chủ yếu sử dụng bộ LLAMA [44,45], một loạt các mô hình được huấn luyện trước có kích thước từ 6.7B đến 65B tham số. Chúng tôi ban đầu thử nghiệm với các mô hình LLAMA-1 cho phiên bản đầu tiên của bài báo này và bổ sung LLAMA-2 trong bản camera ready, sử dụng số lượng tham số tương tự nhưng được huấn luyện trên nhiều token hơn đáng kể. Các mô hình này đại diện cho các mô hình được huấn luyện trước lớn nhất, chất lượng cao nhất có sẵn cho cộng đồng (mặc dù dưới giấy phép hạn chế). Chúng tôi cũng xem xét các mô hình OPT [54] và Pythia [4] với kích thước tương đương với mô hình LLAMA 6.7B, để kiểm tra tác động của các mô hình cơ sở khác nhau. Để đơn giản, chúng tôi sẽ làm tròn tất cả kích thước về số nguyên gần nhất. Chúng tôi lưu ý một số nỗ lực đang diễn ra để huấn luyện trước các mô hình có chất lượng tương tự hoặc tốt hơn [18,33,1]. Chúng tôi tin rằng các phát hiện của chúng tôi nên áp dụng cho các mô hình này và các mô hình cơ sở mở mạnh mẽ hơn trong tương lai.

3 Huấn luyện mô hình với các tập dữ liệu khác nhau

3.1 Thống nhất định dạng

Chúng tôi định dạng tất cả các tập dữ liệu để tuân theo lược đồ kiểu chatbot để thống nhất các kiểu và định dạng đa dạng của các tập dữ liệu hướng dẫn, được hiển thị trong Hình 1. Điều này cho phép chúng tôi khớp các vòng tương tác tùy ý giữa người dùng và mô hình ngôn ngữ (tức là "trợ lý") thành một chuỗi đầu vào và mã hóa chúng cùng nhau với một mô hình ngôn ngữ nhân quả. Chúng tôi thêm các token đặc biệt <|user|> và <|assistant|> trước các phát biểu của người dùng và phản hồi mục tiêu của trợ lý tương ứng, và một đánh dấu kết thúc văn bản </s> ở cuối mỗi đầu ra của trợ lý, mà tại thời điểm suy luận, sẽ dừng phản hồi của mô hình cho mỗi vòng.

³ Dữ liệu ShareGPT (https://sharegpt.com/) được sử dụng để xây dựng mô hình Vicuna [8], nhưng tập dữ liệu chính xác chưa được phát hành. Thay vào đó, chúng tôi sử dụng phiên bản tái tạo từ https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset, và tuân theo Vicuna để chia các cuộc hội thoại dài thành các khối với độ dài tối đa 2048 token.

--- TRANG 4 ---
3.2 Chi tiết huấn luyện mô hình

<|assistant|>Hệ thống điều khiển phản lực (RCS) trên Tàu vũ trụ được thiết kế để có khả năng chịu lỗi, có nghĩa là nó có thể tiếp tục hoạt động ngay cả khi một hoặc nhiều thành phần của nó bị hỏng. RCS bao gồm hai bộ...</s><|assistant|>Có một số trường hợp mà hệ thống điều khiển phản lực (RCS) trên Tàu vũ trụ gặp phải lỗi hoặc trục trặc trong các nhiệm vụ trên quỹ đạo. Những...</s><|user|>Giải thích khả năng chịu lỗi của hệ thống điều khiển phản lực trên Tàu vũ trụ.
<|user|>RCS có gặp phải lỗi trên quỹ đạo nào không?

👱
👱
🤖
🤖

Hình 1: Một ví dụ từ dữ liệu ShareGPT. Chúng tôi sử dụng <|role|> để đặt ranh giới giữa các tin nhắn. Toàn bộ chuỗi được mã hóa cùng nhau, và mất mát được tính toán trên các phần trợ lý (được tô màu xanh).

Trong quá trình huấn luyện, chúng tôi chỉ tính toán mất mát trên các token sau <|assistant|> và trước token <|user|> tiếp theo. Chính thức hơn, chúng tôi xem xét một tập dữ liệu hướng dẫn bao gồm 𝑁 tuple, mỗi tuple có 𝑖 lượt, {(𝑥𝑗₁,𝑦𝑗₁,𝑥𝑗₂,𝑦𝑗₂,...𝑥𝑗ᵢ,𝑦𝑗ᵢ)}ᴺⱼ₌₁, trong đó 𝑥ᵢ là lời nhắc của người dùng và 𝑦ᵢ là đầu ra mong muốn. Đối với hầu hết các phiên bản, 𝑢 = 1, và chúng tôi huấn luyện mô hình để đưa ra 𝑦ⱼ khi được cho 𝑥ⱼ. Tuy nhiên, trong trường hợp tập dữ liệu hội thoại, chúng tôi huấn luyện mô hình để dự đoán 𝑦ⱼᵢ khi được cho một số lịch sử hội thoại 𝑥ⱼ₁,𝑦ⱼ₁,𝑥ⱼ₂,...,𝑥ⱼᵢ. Chúng tôi huấn luyện các mô hình chỉ giải mã và sử dụng teacher-forcing với mặt nạ mất mát để huấn luyện các mô hình, trong đó chúng tôi che mặt tất cả các token thuộc về (các) chuỗi đầu vào 𝑥ᵢ. Với 𝑋 là các token thuộc về đầu vào, và 𝑌 là các token mục tiêu, hàm mất mát là:

𝐿 = −∑ⱼ log 𝑝θ(𝑡ⱼ|𝑡<ⱼ) × {1 nếu 𝑡ⱼ ∈ 𝑌, 0 nếu không}

trong đó 𝑡ⱼ là token đầu vào thứ 𝑗 (thuộc về 𝑋 hoặc 𝑌). Xem Phụ lục §D để biết thêm chi tiết huấn luyện.

3.3 TÜLU: một mô hình được điều chỉnh hướng dẫn tốt hơn bằng cách kết hợp tài nguyên

Các nghiên cứu hiện có [48,31] (và đánh giá của chúng tôi dưới đây) đã cho thấy rằng việc tăng tính đa dạng của hướng dẫn có thể cải thiện hiệu quả việc thực hiện điều chỉnh hướng dẫn. Theo động lực này, chúng tôi tạo ra hai hỗn hợp tập dữ liệu:

Hỗn hợp dữ liệu con người, bao gồm các tập dữ liệu do con người viết tốt nhất, bao gồm FLAN V2, CoT, Dolly, và Open Assistant 1 (chúng tôi loại trừ SuperNI vì FLAN V2 bao gồm hầu hết các nhiệm vụ trong SuperNI);

Hỗn hợp dữ liệu Con người+GPT, bao gồm hỗn hợp con người và ba tập dữ liệu bổ sung có các thế hệ bởi các mô hình GPT của OpenAI, bao gồm GPT4-Alpaca, Code-Alpaca, và ShareGPT.

Đối với cả hai hỗn hợp, chúng tôi nối các tập dữ liệu và để lại việc khám phá các hỗn hợp lấy mẫu phức tạp hơn cho công việc tương lai. Chúng tôi đặt tên các mô hình LLAMA được huấn luyện trên hỗn hợp dữ liệu Con người+GPT là TÜLU, theo tên một loài lạc đà lai tạo ra từ việc lai tạo giữa các loài khác nhau. Chúng tôi phân biệt các mô hình TÜLU được huấn luyện từ các mô hình cơ sở LLAMA-2 bằng cách đặt phiên bản chúng là TÜLU-1.1.

4 Thiết lập đánh giá

Đánh giá các mô hình tuân theo hướng dẫn vẫn là một vấn đề đầy thách thức do phạm vi rộng lớn của "tính tổng quát" và bản chất mở của nó. Tuy nhiên, chúng tôi cho rằng các mô hình đa năng nên có thể thực hiện một số nhiệm vụ cốt lõi trước khi chúng có thể khái quát hóa để thỏa mãn các nhu cầu thực tế khác nhau. Như vậy, chúng tôi thiết lập một đánh giá đa mặt để bao gồm một số khía cạnh chính của khả năng bao gồm các khả năng cốt lõi và tuân theo hướng dẫn mở. Các đánh giá của chúng tôi tuân theo chặt chẽ công trình trước đây về đánh giá các mô hình được điều chỉnh hướng dẫn [9,2,47,8,16], nhưng phục vụ như là đánh giá đầu tiên biên soạn chúng cùng nhau để đánh giá có hệ thống.

4.1 Các khía cạnh của đánh giá

Kiến thức thực tế là thiết yếu để các mô hình ngôn ngữ phục vụ nhu cầu thông tin của người dùng. Chúng tôi sử dụng tập dữ liệu Hiểu biết ngôn ngữ đa nhiệm vụ khổng lồ (MMLU [22]) để đo lường kiến thức thực tế của các mô hình. MMLU bao gồm một tập hợp các câu hỏi về 57 chủ đề có độ khó từ cấp độ tiểu học đến cấp độ chuyên nghiệp, và định dạng trả lời trắc nghiệm của nó làm cho nó phù hợp để thăm dò kiến thức của các mô hình mà không cần lo lắng về tính mở của các thế hệ.

--- TRANG 5 ---
Lý luận là một khả năng cơ bản khác cho các mô hình, đặc biệt là để giải quyết các nhiệm vụ phức tạp. Chúng tôi sử dụng tập kiểm tra của tập dữ liệu Toán học trường tiểu học (GSM [11]) để đánh giá khả năng lý luận toán học của các mô hình. Chúng tôi cũng áp dụng Big-Bench-Hard (BBH [42]), chứa 23 nhiệm vụ thách thức từ Big-Bench [41], để đánh giá khả năng lý luận tổng quát của các mô hình.

Đa ngôn ngữ hoạt động như một quan điểm quan trọng của các mô hình để phục vụ mọi người từ các nền tảng khác nhau. Chúng tôi sử dụng TyDiQA [10], một benchmark hỏi đáp đa ngôn ngữ bao gồm 11 ngôn ngữ đa dạng về mặt loại hình để kiểm tra mô hình có thể xử lý văn bản không phải tiếng Anh đến mức nào. Chúng tôi sử dụng thiết lập đoạn văn vàng nơi một đoạn văn chứa câu trả lời tham khảo được cung cấp.

Lập trình là một ứng dụng cụ thể mà mọi người đã sử dụng các mô hình ngôn ngữ và có thể quan trọng để tích hợp các mô hình này với các công cụ bên ngoài [5]. Chúng tôi sử dụng tập dữ liệu HumanEval [7] để đánh giá khả năng của các mô hình trong việc tạo ra các chương trình chính xác về mặt chức năng từ docstring. Để tránh sự mơ hồ với đánh giá con người của chúng tôi, chúng tôi gọi tập dữ liệu này là Codex-Eval trong bài báo này.

Tuân theo hướng dẫn mở. Trong khi hiệu suất trên các benchmark ở trên định lượng khả năng của các mô hình tại các kỹ năng cụ thể, nó có thể không phản ánh mức độ tốt mà các mô hình có thể xử lý các hướng dẫn từ người dùng thực, bao gồm các yêu cầu đa dạng cao và thường là mở. Ví dụ, tập dữ liệu ShareGPT phổ biến chứa các phiên bản của người dùng yêu cầu trợ giúp lập trình, lời khuyên định dạng sơ yếu lý lịch, đóng vai giáo dục, gợi ý phát âm, viết tiểu thuyết fan, và nhiều hơn nữa. Chúng tôi đánh giá khả năng hướng dẫn mở như vậy của các mô hình bằng cách sử dụng cả đánh giá dựa trên mô hình (§4.2) và đánh giá con người (§4.3), cả hai đều bao gồm nhiều tập kiểm tra từ các nghiên cứu hiện có [47, 8, 26, 3, 19].

An toàn là mối quan tâm đặc biệt liên quan đến các mô hình ngôn ngữ phát triển nhanh để đảm bảo việc sử dụng có đạo đức và phù hợp của chúng. Theo LLAMA-2 [45], chúng tôi sử dụng ToxiGen [21] để đo lượng ngôn ngữ độc hại và phát biểu thù địch được tạo ra trên các nhóm khác nhau khi các mô hình được khuyến khích làm như vậy. Chúng tôi cũng áp dụng TruthfulQA [30] để đo mức độ tốt mà các mô hình có thể tránh tạo ra những điều sai lệch đã biết do quan niệm sai lầm hoặc niềm tin sai lệch trong khi cung cấp thông tin hữu ích.

Đối với tất cả các đánh giá dựa trên benchmark, chúng tôi tuân theo các chỉ số tiêu chuẩn của chúng, trong khi chúng tôi lấy mẫu con một số benchmark đến kích thước hợp lý để cải thiện hiệu quả của việc thực hiện lý luận chuỗi suy nghĩ. Chúng tôi giới thiệu người đọc đến Phụ lục §E để biết chi tiết thiết lập.

4.2 Đánh giá dựa trên mô hình sử dụng GPT-4

Để đánh giá khả năng hướng dẫn mở, chúng tôi đầu tiên áp dụng một phương pháp dựa trên mô hình được giới thiệu trong AlpacaEval [27]. Tập kiểm tra bao gồm 805 hướng dẫn, với 252 hướng dẫn từ đánh giá Self-Instruct [47], 188 từ đánh giá Open Assistant [26], 129 từ đánh giá hữu ích bởi Anthropic [3], 80 từ đánh giá Vicuna [8], và 156 từ đánh giá Koala [19].

Chúng tôi sử dụng người chú thích GPT-4 mô phỏng của họ, tính toán tỷ lệ thắng của mô hình kiểm tra như được đánh giá bởi GPT-4 khi so sánh với các đầu ra được tạo bởi Davinci-003. Chúng tôi sử dụng codebase và lời nhắc của AlpacaEval [27] để làm cho điểm số của chúng tôi có thể so sánh trực tiếp với những điểm trên bảng xếp hạng AlpacaEval. Khi thực hiện so sánh cặp với GPT-4, thứ tự của các đầu ra mô hình được ngẫu nhiên hóa để tránh sự thiên vị vị trí trong đánh giá [46]. Chúng tôi không đánh giá các mô hình LLAMA vanilla do chúng có ít khả năng tuân theo hướng dẫn mà không cần kỹ thuật lời nhắc thêm.

4.3 Đánh giá con người

Để kiểm tra thêm chất lượng của các thế hệ mở, chúng tôi thực hiện đánh giá con người dựa trên 332 hướng dẫn kết hợp tập đánh giá Self-Instruct [47] và tập đánh giá Vicuna [8]. Được truyền cảm hứng từ Bai và cộng sự [3], chúng tôi thiết kế một giao diện tương tự (Hình 5) để thu thập đánh giá con người về các đầu ra mô hình theo các khía cạnh sau. Chúng tôi lưu ý rằng chúng tôi đã đánh giá dựa trên các mô hình LLAMA-1 được tinh chỉnh của chúng tôi, vì LLAMA-2 không có sẵn tại thời điểm thí nghiệm này.

Khả năng chấp nhận cá nhân. Chúng tôi yêu cầu người đánh giá con người đánh giá liệu các phản hồi của từng hệ thống có thể chấp nhận được trong sự cô lập hay không. Đây là một quyết định nhị phân, và chúng tôi yêu cầu người đánh giá đánh dấu một phản hồi là có thể chấp nhận nếu và chỉ nếu phản hồi trả lời yêu cầu trong truy vấn, không có lỗi đáng kể, và không có thông tin lặp lại.

--- TRANG 6 ---
Bảng 3: So sánh các tập dữ liệu điều chỉnh hướng dẫn khác nhau, cho thấy rằng các tập dữ liệu điều chỉnh hướng dẫn khác nhau có thể xuất sắc trong các khía cạnh khác nhau, và các hỗn hợp thực hiện tốt nhất trung bình. Các ô được tô màu xanh nếu việc tinh chỉnh tăng hiệu suất LLAMA vanilla, và màu cam nếu việc tinh chỉnh làm tổn hại hiệu suất.

[Bảng này chứa các kết quả hiệu suất cho MMLU (thực tế), GSM (lý luận), BBH (lý luận), TydiQA (đa ngôn ngữ), Codex-Eval (lập trình), AlpacaEval (mở), với các điểm số trung bình cho mô hình LLaMa 13B vanilla và các biến thể được tinh chỉnh khác nhau]

Bảng 4: Hiệu suất của các mô hình cơ sở khác nhau sau khi huấn luyện trên hỗn hợp dữ liệu Con người+GPT.

[Bảng này so sánh hiệu suất của Pythia 6.9B, OPT 6.7B, LLAMA 7B, và LLAMA-2 7B trên các chỉ số đánh giá khác nhau]

Sở thích cặp. Sau đó chúng tôi yêu cầu con người so sánh các đầu ra của hai hệ thống và chọn cái nào họ nghĩ hữu ích hơn. Đây là một quyết định 5 hướng, và người đánh giá có thể chọn nếu một trong những phản hồi "rõ ràng" hoặc "hơi" tốt hơn cái kia hoặc nếu đó là hòa có nghĩa là cả hai phản hồi đều tốt hoặc xấu như nhau.

Để có được đánh giá đáng tin cậy hơn, chúng tôi tuyển dụng một nhóm 18 người chú thích chuyên gia là các nhà nghiên cứu tại AI2 hoặc sinh viên tại UW. Tất cả họ đều nói tiếng Anh thông thạo, có bằng cử nhân trở lên.

5 Kết quả

5.1 Phân tích các tập dữ liệu điều chỉnh hướng dẫn và mô hình cơ sở

Để hiểu cách các tập dữ liệu hướng dẫn được liệt kê trong Bảng 1 đóng góp vào khả năng mô hình, chúng tôi đánh giá các mô hình LLaMa 13B được huấn luyện trên các tập dữ liệu này bằng bộ đánh giá của chúng tôi. Bảng 3 hiển thị kết quả trên tập đánh giá benchmark của chúng tôi, với kết quả mở rộng hơn trong Phụ lục F. Chúng tôi thấy rằng:

Không có tập dữ liệu điều chỉnh hướng dẫn tốt nhất duy nhất trên tất cả các nhiệm vụ. Các tập dữ liệu khác nhau cho phép các khả năng khác nhau trong mô hình. Các ví dụ đáng chú ý bao gồm huấn luyện trên CoT đặc biệt hữu ích cho lý luận toán học trong GSM và Code-Alpaca hữu ích cho Codex-Eval. Chúng tôi giả thuyết rằng thành công trên những nhiệm vụ này, khác biệt đáng kể so với phần còn lại của các nhiệm vụ đánh giá, đòi hỏi các tập huấn luyện nơi những nhiệm vụ này được đại diện tốt. Ngoài việc xây dựng các tập dữ liệu cụ thể cho nhiệm vụ thủ công, chưng cất dữ liệu cụ thể cho nhiệm vụ từ các mô hình lớn cũng dường như là một cách hiệu quả để đảm bảo điều này (ví dụ: CodeAlpaca được chưng cất từ Davinci-003).

Kết hợp các tập dữ liệu dẫn đến hiệu suất tổng thể tốt nhất trên các nhiệm vụ benchmark. Trong khi các mô hình được huấn luyện trên các tập dữ liệu kết hợp của chúng tôi thường không phải là mô hình tốt nhất cho một nhiệm vụ duy nhất (chỉ là tốt nhất trong 2 trên 6 thiết lập đánh giá), chúng là tốt nhất khi đo lường hiệu suất trung bình trên các nhiệm vụ. Điều này gợi ý rằng công việc tương lai về việc trộn tập dữ liệu tốt hơn hoặc điều chỉnh hướng dẫn modular

--- TRANG 7 ---
Bảng 5: Hiệu suất của TÜLU và các mô hình được huấn luyện khác của chúng tôi so với các mô hình LLAMA vanilla và các mô hình độc quyền tiên tiến trên các thiết lập đánh giá. Xem Bảng 8 để biết danh sách đầy đủ.

[Bảng này hiển thị kết quả hiệu suất chi tiết cho các mô hình khác nhau từ 7B đến 65B tham số, so sánh với ChatGPT và GPT-4]

(ví dụ: mixture-of-experts [40]) là một hướng đầy hứa hẹn để phát triển các mô hình duy trì hiệu suất mạnh mẽ trên tất cả các thiết lập đánh giá.

Chất lượng mô hình cơ sở cực kỳ quan trọng đối với hiệu suất downstream. Chúng tôi kiểm tra tác động của việc sử dụng các mô hình cơ sở khác nhau trong Bảng 4, so sánh các mô hình LLAMA, OPT [54], và Pythia [4] có kích thước tương đương được huấn luyện trên hỗn hợp dữ liệu Con người+GPT. Trên tất cả các thiết lập đánh giá, chúng tôi thấy rằng sử dụng LLAMA thực hiện tốt nhất với một biên độ đáng kể, có thể do thực tế là LLAMA được huấn luyện trước trên nhiều token đáng kể hơn các mô hình khác (xem Bảng 2). Điều này gợi ý rằng các mô hình được huấn luyện trước trên tập dữ liệu lớn hơn (hoặc có thể chất lượng cao hơn) được ưa thích làm mô hình cơ sở cho điều chỉnh hướng dẫn. Việc bổ sung LLAMA-2 sau đó xác nhận phát hiện này bằng cách cho thấy một cải thiện đáng kể có thể đến từ chỉ việc nâng cấp mô hình cơ sở.

Một số tập dữ liệu làm suy giảm hiệu suất mô hình vanilla. Đáng chú ý, hầu hết các tập dữ liệu chúng tôi đánh giá gây ra sự suy giảm hiệu suất trên GSM và TydiQA so với mô hình cơ sở vanilla. Chúng tôi giả thuyết điều này là do phong cách và chất lượng dữ liệu. Nhiều tập dữ liệu chúng tôi kiểm tra chứa ít hoặc không có ví dụ về lý luận kiểu chuỗi suy nghĩ và chứa ít hoặc không có dữ liệu đa ngôn ngữ. Như vậy, huấn luyện trên các tập dữ liệu này có thể dẫn đến một số quên các khả năng CoT hoặc đa ngôn ngữ mà mô hình trước đó đã có, dẫn đến hiệu suất suy giảm. Ngoài ra, chúng tôi lưu ý rằng self-instruct dường như gây ra sự suy giảm trên hầu hết các nhiệm vụ, mà chúng tôi giả thuyết là do chất lượng tương đối kém của dữ liệu self-instruct gốc, được tạo ra bởi một mô hình yếu hơn (GPT-3 cơ sở) so với các tập dữ liệu chưng cất GPT khác.

5.2 Đẩy giới hạn của các mô hình mở

Sau khi thiết lập rằng (a) sử dụng một hỗn hợp rộng dữ liệu là tốt nhất, và (b) sử dụng LLAMA làm mô hình cơ sở được ưa thích hơn các lựa chọn mở khác, chúng tôi so sánh hiệu suất của các mô hình được huấn luyện trên hỗn hợp dữ liệu Con người+GPT (mô hình TÜLU) trên tất cả các kích thước LLAMA trong Bảng 5. Chúng tôi thấy rằng:

Điều chỉnh hướng dẫn mang lại lợi ích lớn trên đầu các mô hình LLAMA ở tất cả các kích thước. Trung bình, tất cả các mô hình LLAMA cải thiện đáng kể sau khi điều chỉnh hướng dẫn.

--- TRANG 8 ---
Các mô hình nhỏ hơn được hưởng lợi nhiều nhất từ điều chỉnh hướng dẫn. Chúng tôi thấy rằng các cải thiện tương đối từ điều chỉnh hướng dẫn là lớn nhất đối với các mô hình nhỏ nhất và thu hẹp khi các mô hình trở nên lớn hơn. Đáng chú ý, mô hình LLAMA 65B thực hiện tương đương hoặc tốt hơn mô hình TÜLU 65B trên MMLU, BBH, và TydiQA. Điều này gợi ý rằng điều chỉnh hướng dẫn không giúp tăng cường các khả năng mạnh mẽ đã có sẵn trong mô hình gốc, và cũng nhấn mạnh rằng cần phải cẩn thận trong quá trình tinh chỉnh để tránh quên các khả năng ban đầu của mô hình cơ sở.

TÜLU vẫn tụt hậu so với các mô hình độc quyền tiên tiến. Mặc dù có hiệu suất ấn tượng của TÜLU 65B, nó vẫn tụt hậu so với ChatGPT và GPT-4 trong tất cả các thiết lập đánh giá, trái ngược với các tuyên bố trước đây rằng các mô hình được huấn luyện trên các tài nguyên mở này có thể sánh bằng ChatGPT [56,8]. Chúng tôi lưu ý rằng chúng tôi không thể loại trừ khả năng ChatGPT hoặc GPT-4 được huấn luyện trên các phần đáng kể của bộ đánh giá của chúng tôi. Tuy nhiên, sự hiện diện của một khoảng cách đáng kể giữa các mô hình TÜLU và ChatGPT phù hợp với các phát hiện của chúng tôi trong các đánh giá dựa trên mô hình và con người, ít có khả năng bị xâm phạm hơn.

5.3 Đánh giá rủi ro và tác hại tiềm ẩn

ToxiGen (↓) | TruthfulQA (↑)
Mô hình ↓ | 7B | 13B | 7B | 13B
LLAMA | 85.4 | 82.6 | 26.2 | 23.6
+ SuperNI | 85.3 | 77.3 | 26.7 | 26.2
+ CoT | 63.0 | 43.9 | 35.1 | 35.5
+ Flan V2 | 77.5 | 61.4 | 33.2 | 33.4
+ Dolly | 72.1 | 78.9 | 30.1 | 32.9
+ Open Assistant 1 | 39.2 | 5.2 | 40.9 | 48.6
+ Self-instruct | 89.0 | 89.3 | 22.4 | 22.4
+ Unnatural Inst. | 35.8 | 55.7 | 27.3 | 31.7
+ Alpaca | 63.2 | 58.1 | 33.5 | 39.8
+ Code-Alpaca | 84.3 | 92.0 | 25.1 | 26.7
+ GPT4-Alpaca | 3.9 | 1.2 | 51.2 | 56.7
+ Baize | 77.2 | 41.2 | 42.4 | 43.9
+ ShareGPT | 5.5 | 2.5 | 45.3 | 60.0
+ Human mix. | 51.8 | 76.9 | 34.1 | 32.1
+ TÜLU | 10.6 | 0.1 | 44.6 | 41.6
ChatGPT | 27.7 | | 75.2 |
GPT-4 | 10.6 | | 82.3 |

Bảng 6: Hiệu suất của các mô hình trên ToxiGen (% thế hệ độc hại, thấp hơn là tốt hơn) và TruthfulQA (% câu trả lời đúng sự thật và có thông tin, cao hơn là tốt hơn). Xem Bảng 9 và Bảng 10 để biết phân tích đầy đủ của hai đánh giá này.

Chúng tôi đánh giá các mô hình của chúng tôi trên ToxiGen và TruthfulQA để đo lường mức độ mà các tập dữ liệu khác nhau có khả năng tạo ra các mô hình tạo ra ngôn ngữ độc hại hoặc thông tin sai lệch. Chúng tôi thấy rằng:

Xu hướng vẫn tương tự như các benchmark tập trung vào khả năng. Tương tự như các kết quả trong Mục 4.1, chúng tôi thấy rằng các tập dữ liệu chưng cất GPT cho kết quả hiệu suất tổng thể tốt nhất và có sự biến thiên lớn trong hiệu suất trên các tập dữ liệu.

Các mô hình được huấn luyện trên dữ liệu nguồn GPT tạo ra ít thế hệ độc hại hơn GPT. Các mô hình lớn hơn được huấn luyện trên dữ liệu chưng cất GPT dường như từ chối tạo ra thế hệ độc hại gần như hoàn toàn, mặc dù thực tế là ChatGPT và GPT-4 tạo ra thế hệ độc hại một lượng không tầm thường. Chúng tôi giả thuyết điều này là do các mô hình của chúng tôi overfitting trên hành vi kiểu từ chối, từ chối tạo ra bất cứ thứ gì có độ độc hại vừa phải, trong khi các mô hình GPT cân bằng hành vi từ chối với sự hữu ích ở mức độ lớn hơn.

Hiệu suất TruthfulQA không mở rộng. Khác với các benchmark khác, chúng tôi thấy rằng hiệu suất TruthfulQA không cải thiện với kích thước mô hình. Kiểm tra thêm điều này, chúng tôi thấy rằng các mô hình lớn hơn có đưa ra nhiều sự kiện đúng hơn, nhưng cũng có xu hướng né tránh và từ chối đưa ra câu trả lời có thông tin thường xuyên hơn, dẫn đến ít hoặc không có cải thiện tổng thể khi kích thước mô hình tăng.

5.4 Kết quả đánh giá dựa trên mô hình cho thế hệ mở

Chúng tôi báo cáo tỷ lệ thắng AlpacaEval của các mô hình chúng tôi trong Bảng 7. Chúng tôi thấy rằng:

Các mô hình được huấn luyện trên hỗn hợp dựa trên các tập dữ liệu NLP truyền thống thực hiện kém. CoT, FLAN, và SuperNI đều thực hiện cực kỳ kém trong việc tuân theo hướng dẫn mở, mặc dù các tập dữ liệu này cung cấp cải thiện lớn cho các khả năng mô hình được kiểm tra trong Bảng 3.

Các tập dữ liệu khuyến khích thế hệ dài, đa dạng thực hiện tốt nhất. Bị hấp dẫn bởi hiệu suất của ShareGPT, chúng tôi vẽ biểu đồ số lượng token duy nhất trung bình trong các thế hệ mô hình so với tỷ lệ thắng AlpacaEval trong Hình 2. Chúng tôi thấy rằng đánh giá có tương quan mạnh mẽ với số lượng token duy nhất trung bình (tương quan Pearson 0.96, 𝑝≪ 0.05). Cho rằng hiệu suất mạnh mẽ của GPT-4 trên các nhiệm vụ khác, chúng tôi không tin rằng đánh giá GPT-4 chỉ đơn giản là đếm token duy nhất, nhưng kết quả này nổi bật cách điểm số sở thích mô hình không nhất thiết chỉ thưởng cho khả năng mô hình.

--- TRANG 9 ---
Tập dữ liệu huấn luyện ↓ | 7B | 13B | 30B | 65B
SuperNI | 2.9 | 4.2 | |
CoT | 5.0 | 6.0 | |
Flan V2 | 3.1 | 3.2 | |
Dolly | 11.0 | 13.7 | |
Open Assistant 1 | 51.4 | 58.1 | |
Self-instruct | 4.0 | 5.0 | |
Unnatural Instructions | 7.5 | 8.4 | |
Alpaca | 21.4 | 21.9 | |
Code-Alpaca | 15.3 | 15.8 | |
GPT4-Alpaca | 57.3 | 63.1 | |
Baize | 20.0 | 21.9 | |
ShareGPT | 62.4 | 70.5 | 69.1 | 73.6
Human mix. | 28.7 | 35.0 | 38.3 | 43.4
TÜLU | 48.6 | 56.5 | 62.3 | 61.8

Bảng 7: Tỷ lệ thắng (%) của các mô hình LLAMA có kích thước khác nhau được tinh chỉnh trên tập dữ liệu đã cho so với Davinci-003 sử dụng AlpacaEval [27].

[Hình 2 hiển thị biểu đồ tương quan giữa tỷ lệ thắng và số token duy nhất trung bình]

[Hình 3 hiển thị tỷ lệ chấp nhận con người cho bốn mô hình được đánh giá]

[Hình 4 hiển thị tỷ lệ sở thích con người cho ba cặp so sánh mô hình]

ShareGPT thực hiện tốt nhất. Chúng tôi thấy rằng ShareGPT luôn thực hiện tốt nhất trên tất cả các kích thước mô hình, bao gồm các mô hình được huấn luyện trên hỗn hợp dữ liệu bao gồm ShareGPT. Các mô hình được huấn luyện trên ShareGPT đạt tỷ lệ thắng cao hơn các mô hình gấp đôi kích thước của chúng (ví dụ: 13B ShareGPT so với 65B TÜLU). Chúng tôi giả thuyết điều này là do tính đa dạng, kích thước, và số lượng token trung bình cao của các phản hồi mục tiêu của ShareGPT.

Nhìn chung, các kết quả này gợi ý rằng trong khi đánh giá sở thích mô hình là quan trọng, nó không cung cấp đánh giá toàn diện về các mô hình này. Thay vào đó, đánh giá sở thích mô hình chỉ nên được bao gồm như một phần của một thiết lập đánh giá lớn hơn, toàn diện hơn.

5.5 Kết quả đánh giá con người cho thế hệ mở

Cuối cùng, chúng tôi hiển thị kết quả đánh giá con người trong Hình 4 và chúng tôi giới thiệu người đọc đến Phụ lục §G.2 để biết thỏa thuận giữa các người chú thích. Chúng tôi thấy rằng kết quả đánh giá con người chủ yếu tương quan với AlpacaEval và đánh giá dựa trên benchmark: tất cả các đánh giá đều cho thấy rằng 65B TÜLU vượt trội hơn 7B TÜLU, gợi ý rằng việc sử dụng các mô hình cơ sở lớn hơn là quan trọng, và vẫn còn một khoảng cách không tầm thường trong hiệu suất giữa 65B TÜLU và ChatGPT. Chúng tôi cũng thấy rằng việc sử dụng các tập dữ liệu chưng cất cung cấp một sự thúc đẩy hiệu suất lớn, gợi ý rằng các tập dữ liệu do con người viết thiếu so sánh. Những quan sát này cũng phù hợp với các điểm số khả năng chấp nhận trong Hình 3.

Tuy nhiên, chúng tôi lưu ý rằng 7B TÜLU vượt trội hơn 65B TÜLU hỗn hợp con người trong đánh giá sở thích mô hình, nhưng nếu chúng tôi so sánh các điểm số khả năng chấp nhận trong Hình 3, điều ngược lại dường như đúng. Đây là bằng chứng thêm rằng đánh giá cặp mô hình có thể không luôn tiết lộ những thiếu sót của mô hình. Trong trường hợp này, mô hình 65B hỗn hợp con người có nhiều khả năng tạo ra các phản hồi có thể chấp nhận được (nếu không phải chất lượng cao) hơn mô hình 7B.

--- TRANG 10 ---
6 Công trình liên quan

Điều chỉnh hướng dẫn của LM. Tinh chỉnh các mô hình ngôn ngữ trên các tập hướng dẫn đa dạng cùng với các mẫu thông thường đã được chứng minh là cải thiện đáng kể hiệu suất zero-shot trên các nhiệm vụ chưa thấy [39,51,49,32,9,48], và phục vụ như một cơ sở tốt cho việc tinh chỉnh thêm trong các thiết lập có giám sát [31]. Tăng số lượng lời nhắc đa dạng [39], số lượng nhiệm vụ [48,9], và tính đa dạng của dữ liệu [56] đều được chứng minh là quan trọng đối với hiệu suất. Gần đây hơn, một số lượng ngày càng tăng các mô hình đã sử dụng dữ liệu tăng cường hướng dẫn được tạo bởi mô hình [47,23,25,53], thường được tạo ra hoặc thu thập từ các mô hình độc quyền lớn hơn như ChatGPT hoặc GPT-4 [8,15,43,52,36, trong số những cái khác]. Mặc dù sự bùng nổ của các mô hình và tập dữ liệu, đánh giá vẫn không nhất quán và khó khăn, với các thiết lập đánh giá khác nhau được sử dụng trên các mô hình. Công trình trước đây đã kiểm tra các mô hình được huấn luyện trên các nguồn tập dữ liệu khác nhau với mục đích xác định 'hỗn hợp tốt nhất' [31,24], nhưng thường bị hạn chế chỉ kiểm tra hiệu suất benchmark, và bao gồm một số lượng nhỏ hơn các nguồn hướng dẫn so với công trình này. QLoRA [14] cũng khám phá điều chỉnh hướng dẫn (được lượng tử hóa và hiệu quả tham số) của các mô hình và tập dữ liệu gần đây, nhưng khám phá một phạm vi nhỏ hơn các mô hình, tập dữ liệu, và đánh giá so với công trình này.

Đánh giá LM. Cho sự thành công của LM trên các nhiệm vụ NLP và tuân theo hướng dẫn, nhiều khung đánh giá đã được đề xuất. Các khung như HELM [28] và LM Evaluation Harness [17] bao gồm một phạm vi rộng các nhiệm vụ NLP nhưng thường tập trung vào đánh giá các mô hình cơ sở thay vì những mô hình được điều chỉnh hướng dẫn. Tương tự như công trình của chúng tôi, Chung và cộng sự [9] tập trung vào một loạt đánh giá benchmark tập trung xung quanh tính thực tế và lý luận, nhưng chủ yếu bỏ qua khả năng tuân theo hướng dẫn mở. Các phát hành của các mô hình độc quyền lớn (đóng) như GPT-4 [34] và PaLM v2 [2] thường đi kèm với các đánh giá toàn diện trên nhiều benchmark khác nhau, mặc dù cả hai đều tương tự bỏ qua đánh giá tuân theo hướng dẫn mở, và không có phát hành mở của dữ liệu huấn luyện trước hoặc điều chỉnh hướng dẫn thì không có cách nào để kiểm tra contamination dữ liệu đánh giá.

Gần đây, các khung đánh giá như AlpacaEval [27] và Chatbot Arena [55] đã được đề xuất để đánh giá khả năng tuân theo hướng dẫn mở của LM, vượt qua các đánh giá dựa trên benchmark. Những khung này hoặc sử dụng các mô hình khác (trong trường hợp AlpacaEval) hoặc con người (trong trường hợp Chatbot Arena) làm người chú thích để đánh giá các thế hệ mô hình. Chúng tôi sử dụng công trình gần đây này và đánh giá các mô hình của chúng tôi trên các benchmark truyền thống, đánh giá dựa trên mô hình, và đánh giá dựa trên con người. Đồng thời với công trình này, Gudibande và cộng sự [20] kiểm tra các mô hình được huấn luyện trên đầu ra mô hình GPT và tranh luận rằng các mô hình như vậy học để bắt chước chỉ phong cách, không phải nội dung, của các mô hình GPT giáo viên của chúng. Trong khi chúng tôi tương tự thấy rằng các tập dữ liệu hiện có không thể huấn luyện các mô hình gần với các mô hình độc quyền mạnh mẽ, tính đa dạng của hiệu suất chúng tôi quan sát trên các tập dữ liệu gợi ý rằng các cải thiện hiệu suất đáng kể có thể đạt được thông qua dữ liệu bắt chước, miễn là nó chứa một tập hợp đa dạng và rộng rãi các kỹ năng và miền.

7 Kết luận

Trong công trình này, chúng tôi cung cấp một đánh giá mở rộng về nhiều tài nguyên có sẵn công khai để điều chỉnh hướng dẫn các mô hình, và so sánh chúng với các mô hình độc quyền mạnh nhất hiện có. Chúng tôi thấy rằng sử dụng các mô hình cơ sở mạnh mẽ là quan trọng đối với hiệu suất, kết hợp các tập dữ liệu hoạt động tốt nhất trung bình (nhưng dẫn đến giảm hiệu suất nhẹ so với hiệu suất tốt nhất trong các nhiệm vụ cụ thể), và các mô hình mở mạnh nhất của chúng tôi chưa sánh bằng ChatGPT hoặc GPT-4. Hơn nữa, chúng tôi tin rằng đánh giá của chúng tôi nổi bật nhu cầu tiếp tục phát triển các mô hình cơ sở mạnh mẽ và các tập dữ liệu rộng rãi, đa dạng hơn. Cuối cùng, chúng tôi hy vọng rằng đánh giá và mã và mô hình được phát hành của chúng tôi cho phép các đánh giá toàn diện hơn và thúc đẩy nghiên cứu để thu hẹp những khoảng cách này và làm sáng tỏ tất cả các mô hình ngôn ngữ lớn, đóng hay mở.

Lời cảm ơn

Công việc tại UW được hỗ trợ một phần bởi Văn phòng Nghiên cứu Hải quân dưới grantMURI N00014-18-1-2670, Cơ quan Dự án Nghiên cứu Quốc phòng Tiên tiến (DARPA) dưới Hợp đồng số FA8650-23-C-7316 và chương trình MCS thông qua NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, và một món quà từ Apple. Chúng tôi cảm ơn các đồng nghiệp tại AI2 và UW NLP vì phản hồi xây dựng và hỗ trợ trí tuệ của họ. Chúng tôi đặc biệt biết ơn Tim Dettmers vì các gợi ý của ông về các kỹ thuật suy luận hiệu quả, và Artidoro Pagnoni vì cung cấp tập dữ liệu FLAN V2 được tái tạo. Chúng tôi

--- TRANG 11 ---
cũng thừa nhận sự hỗ trợ từ AMD và cụm LUMI của CSC, và nhóm Beaker tại AI2, đã cung cấp cơ sở hạ tầng tính toán thiết yếu cho các thí nghiệm của chúng tôi. Cuối cùng, chúng tôi chân thành cảm ơn những người đóng góp sau đây cho đánh giá con người của chúng tôi: Valentina Pyatkin, Clara Na, Yuling Gu, Yuchen Lin, Haiyan He, David Graham, Hao Peng, Hyunwoo Kim, Alisa Liu, Youngjae Yu, Tal August, và Egor Klevak.

Tài liệu tham khảo

[1] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, và G. Penedo. Falcon-40B: một mô hình ngôn ngữ lớn mở với hiệu suất tiên tiến. Phát hành Mô hình Huggingface, 2023. URL https://huggingface.co/tiiuae/falcon-40b.

[2] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, và cộng sự. Báo cáo kỹ thuật Palm 2. arXiv preprint arXiv:2305.10403, 2023.

[3] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, và cộng sự. Huấn luyện một trợ lý hữu ích và vô hại với học tăng cường từ phản hồi con người. arXiv preprint arXiv:2204.05862, 2022.

[4] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, và cộng sự. Pythia: Một bộ để phân tích các mô hình ngôn ngữ lớn trên khắp huấn luyện và mở rộng. Trong Hội nghị Quốc tế về Học Máy, trang 2397–2430. PMLR, 2023.

[5] T. Cai, X. Wang, T. Ma, X. Chen, và D. Zhou. Các mô hình ngôn ngữ lớn như người tạo công cụ. arXiv preprint arXiv:2305.17126, 2023.

[6] S. Chaudhary. Code alpaca: Một mô hình llama tuân theo hướng dẫn cho tạo mã. Kho GitHub, 2023. URL https://github.com/sahil280114/codealpaca.

[7] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, và cộng sự. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên mã. arXiv preprint arXiv:2107.03374, 2021.

[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, và E. P. Xing. Vicuna: Một chatbot mã nguồn mở ấn tượng với gpt-4 với chất lượng 90%* chatgpt. Bài đăng blog, tháng 3 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[9] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, và cộng sự. Mở rộng các mô hình ngôn ngữ được tinh chỉnh hướng dẫn. arXiv preprint arXiv:2210.11416, 2022.

[10] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Nikolaev, và J. Palomaki. TyDiQA: Một benchmark cho hỏi đáp tìm kiếm thông tin trong các ngôn ngữ đa dạng về loại hình. TACL, 2020. URL https://arxiv.org/abs/2003.05002.

[11] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, và J. Schulman. Huấn luyện các trình xác minh để giải quyết các bài toán từ toán học. arXiv preprint arXiv:2110.14168, 2021.

[12] Databricks. Dolly miễn phí: Giới thiệu llm được điều chỉnh hướng dẫn thực sự mở đầu tiên trên thế giới. Bài đăng blog, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm.

[13] T. Dettmers, M. Lewis, Y. Belkada, và L. Zettlemoyer. LLM.int8(): Nhân ma trận 8-bit cho transformer ở quy mô. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, 2022.

[14] T. Dettmers, A. Pagnoni, A. Holtzman, và L. Zettlemoyer. Qlora: Tinh chỉnh hiệu quả các llm được lượng tử hóa. arXiv preprint arXiv:2305.14314, 2023.

--- TRANG 12 ---
[15] N. Ding, Y. Chen, B. Xu, S. Hu, Y. Qin, Z. Liu, M. Sun, và B. Zhou. Ultrachat: Dữ liệu hội thoại đa vòng tự động tạo quy mô lớn. Kho GitHub, 2023. URL https://github.com/thunlp/ultrachat.

[16] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, và T. B. Hashimoto. Alpacafarm: Một khung mô phỏng cho các phương pháp học từ phản hồi con người. arXiv preprint arXiv:2305.14387, 2023.

[17] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, và A. Zou. Một khung cho đánh giá mô hình ngôn ngữ few-shot, tháng 9 2021. URL https://doi.org/10.5281/zenodo.5371628.

[18] X. Geng và H. Liu. Openllama: Một tái tạo mở của llama. Kho GitHub, 2023. URL https://github.com/openlm-research/open_llama.

[19] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, và D. Song. Koala: Một mô hình hội thoại cho nghiên cứu học thuật. Bài đăng blog, tháng 4 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.

[20] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine, và D. Song. Lời hứa sai lầm của việc bắt chước các llm độc quyền. arXiv preprint arXiv:2305.15717, 2023.

[21] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, và E. Kamar. TOXIGEN: Điều khiển Các Mô hình Ngôn ngữ để Tạo ra Tính độc hại Ngụ ý và Đối kháng. Trong ACL, 2022. URL https://arxiv.org/abs/2203.09509.

[22] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, và J. Steinhardt. Đo lường hiểu biết ngôn ngữ đa nhiệm vụ khổng lồ. Trong Hội nghị Quốc tế về Biểu diễn Học tập (ICLR), 2020.

[23] O. Honovich, T. Scialom, O. Levy, và T. Schick. Hướng dẫn không tự nhiên: Điều chỉnh các mô hình ngôn ngữ với (gần như) không có lao động con người. arXiv preprint arXiv:2212.09689, 2022.

[24] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, và cộng sự. Opt-iml: Mở rộng meta học tập hướng dẫn mô hình ngôn ngữ thông qua lăng kính tổng quát hóa. arXiv preprint arXiv:2212.12017, 2022.

[25] A. Köksal, T. Schick, A. Korhonen, và H. Schütze. Longform: Tối ưu hóa điều chỉnh hướng dẫn cho tạo văn bản dài với trích xuất tập dữ liệu. arXiv preprint arXiv:2304.08460, 2023.

[26] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, và cộng sự. Các cuộc hội thoại trợ lý mở – dân chủ hóa việc căn chỉnh mô hình ngôn ngữ lớn. arXiv preprint arXiv:2304.07327, 2023.

[27] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, và T. B. Hashimoto. Alpacaeval: Một trình đánh giá tự động của các mô hình tuân theo hướng dẫn. Kho Github, 2023. URL https://github.com/tatsu-lab/alpaca_eval.

[28] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Ré, D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. J. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. S. Kim, N. Guha, N. S. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. F. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, và Y. Koreeda. Đánh giá toàn diện các mô hình ngôn ngữ. Biên niên Viện Hàn lâm Khoa học New York, 2022.

[29] S. Lin, J. Hilton, và O. Evans. TruthfulQA: Đo lường cách các mô hình bắt chước những điều sai lầm của con người. Trong Kỷ yếu Hội nghị Thường niên lần thứ 60 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Bài báo Dài), trang 3214–3252, Dublin, Ireland, tháng 5 2022. Hiệp hội Ngôn ngữ học Tính toán. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.

[30] S. Lin, J. Hilton, và O. Evans. Truthfulqa: Đo lường cách các mô hình bắt chước những điều sai lầm của con người. Trong Kỷ yếu Hội nghị Thường niên lần thứ 60 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Bài báo Dài), trang 3214–3252, 2022.

[31] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, và cộng sự. Bộ sưu tập flan: Thiết kế dữ liệu và phương pháp cho điều chỉnh hướng dẫn hiệu quả. arXiv preprint arXiv:2301.13688, 2023.

[32] S. Mishra, D. Khashabi, C. Baral, và H. Hajishirzi. Tổng quát hóa Nhiệm vụ Chéo thông qua Hướng dẫn Crowdsourcing Ngôn ngữ Tự nhiên. Trong Hội nghị Thường niên của Hiệp hội Ngôn ngữ học Tính toán (ACL), 2022.

[33] MosaicML. Giới thiệu mpt-7b: Một tiêu chuẩn mới cho các llm mã nguồn mở, có thể sử dụng thương mại. Bài đăng blog, 2023. URL https://www.mosaicml.com/blog/mpt-7b.

[34] OpenAI. Báo cáo kỹ thuật gpt-4. arXiv preprint arXiv:2303.08774, 2023.

[35] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, và cộng sự. Huấn luyện Các Mô hình Ngôn ngữ để Tuân theo Hướng dẫn với Phản hồi Con người. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh (NeurIPS), 2022.

[36] B. Peng, C. Li, P. He, M. Galley, và J. Gao. Điều chỉnh hướng dẫn với gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[37] S. Rajbhandari, J. Rasley, O. Ruwase, và Y. He. Zero: Tối ưu hóa bộ nhớ hướng tới huấn luyện các mô hình nghìn tỷ tham số. Trong Kỷ yếu Hội nghị Quốc tế về Điện toán Hiệu năng Cao, Mạng, Lưu trữ và Phân tích, SC '20. IEEE Press, 2020. ISBN 9781728199986.

[38] J. Rasley, S. Rajbhandari, O. Ruwase, và Y. He. Deepspeed: Tối ưu hóa hệ thống cho phép huấn luyện các mô hình học sâu với hơn 100 tỷ tham số. Trong Kỷ yếu Hội nghị Quốc tế ACM SIGKDD lần thứ 26 về Khám phá Kiến thức & Khai thác Dữ liệu, 2020.

[39] V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, và A. M. Rush. Huấn luyện Đa nhiệm vụ Được nhắc nhở Cho phép Tổng quát hóa Nhiệm vụ Zero-Shot. Trong Hội nghị Quốc tế về Biểu diễn Học tập (ICLR), 2022.

[40] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, và J. Dean. Mạng thần kinh cực lớn: Lớp chuyên gia hỗn hợp cổng thưa thớt. Trong Hội nghị Quốc tế về Biểu diễn Học tập, 2017.

[41] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, và đồng nghiệp. Vượt qua trò chơi bắt chước: Định lượng và ngoại suy các khả năng của các mô hình ngôn ngữ. arXiv preprint arXiv:2206.04615, 2022.

[42] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, và cộng sự. Thử thách các nhiệm vụ big-bench khó và liệu chuỗi suy nghĩ có thể giải quyết chúng không. arXiv preprint arXiv:2210.09261, 2022.

[43] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, và T. B. Hashimoto. Stanford alpaca: Một mô hình llama tuân theo hướng dẫn. Kho GitHub, 2023. URL https://github.com/tatsu-lab/stanford_alpaca.

[44] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, và cộng sự. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971, 2023.

[45] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, và cộng sự. Llama 2: Các mô hình chat nền tảng mở và được tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023.

--- TRANG 13 ---
[46] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, và Z. Sui. Các mô hình ngôn ngữ lớn không phải là những người đánh giá công bằng. arXiv preprint arXiv:2305.17926, 2023.

[47] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, và H. Hajishirzi. Self-instruct: Căn chỉnh mô hình ngôn ngữ với hướng dẫn tự tạo. arXiv preprint arXiv:2212.10560, 2022.

[48] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, và cộng sự. Super-NaturalInstructions: Tổng quát hóa thông qua Hướng dẫn Tuyên bố trên 1600+ Nhiệm vụ. Trong EMNLP, 2022.

[49] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, và Q. V. Le. Các Mô hình Ngôn ngữ Được Tinh chỉnh là Người học Zero-Shot. Trong Hội nghị Quốc tế về Biểu diễn Học tập (ICLR), 2022.

[50] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, và D. Zhou. Lời nhắc chuỗi suy nghĩ gợi ra lý luận trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2201.11903, 2022.

[51] O. Weller, N. Lourie, M. Gardner, và M. E. Peters. Học từ mô tả nhiệm vụ. Trong Kỷ yếu Hội nghị 2020 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên (EMNLP), trang 1361–1375, Trực tuyến, tháng 11 2020. Hiệp hội Ngôn ngữ học Tính toán. doi: 10.18653/v1/2020.emnlp-main.105. URL https://aclanthology.org/2020.emnlp-main.105.

[52] C. Xu, D. Guo, N. Duan, và J. McAuley. Baize: Một mô hình chat mã nguồn mở với điều chỉnh hiệu quả tham số trên dữ liệu tự chat. arXiv preprint arXiv:2304.01196, 2023.

[53] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, và D. Jiang. Wizardlm: Trao quyền cho các mô hình ngôn ngữ lớn để tuân theo hướng dẫn phức tạp. arXiv preprint arXiv:2304.12244, 2023.

[54] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, và cộng sự. Opt: Các mô hình ngôn ngữ transformer được huấn luyện trước mở. arXiv preprint arXiv:2205.01068, 2022.

[55] L. Zheng, Y. Sheng, W.-L. Chiang, H. Zhang, J. E. Gonzalez, và I. Stoica. Chatbot Arena: Đánh giá chuẩn LLM trong tự nhiên với xếp hạng Elo. Bài đăng blog, tháng 5 2023. URL https://lmsys.org/blog/2023-05-03-arena/.

[56] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, và cộng sự. Lima: Ít hơn là nhiều hơn cho việc căn chỉnh. arXiv preprint arXiv:2305.11206, 2023.

--- TRANG 14 ---
Tài liệu bổ sung

A Hạn chế

Mặc dù tính toàn diện của các đánh giá của chúng tôi, chúng tôi lưu ý rằng chúng tôi không bao gồm hết tất cả các đánh giá có thể: ví dụ, chúng tôi không đánh giá rõ ràng các mô hình về khả năng hội thoại đa lượt của chúng hay khả năng tóm tắt của chúng. Thay vào đó, chúng tôi tập trung vào một tập hợp cốt lõi các khả năng chúng tôi tin là quan trọng, và bao gồm các nhiệm vụ mở rộng thông qua các đánh giá dựa trên sở thích mô hình và con người của chúng tôi.

Chúng tôi cũng lưu ý rằng chúng tôi không bao gồm tất cả các tập dữ liệu hướng dẫn và mô hình mở có thể được phát hành gần đây, do chi phí tính toán của việc thực hiện điều này. Thay vào đó, chúng tôi tập trung vào một tập hợp rộng các tập dữ liệu chúng tôi tin là đại diện rộng rãi cho loại tập dữ liệu hướng dẫn mở có sẵn (do con người viết, nhắm mục tiêu kỹ năng, chưng cất GPT, v.v.), và tập trung vào mô hình cơ sở mạnh nhất có sẵn rộng rãi khi thực hiện thí nghiệm. Công việc tương lai có thể điều tra liệu các mô hình cơ sở mạnh mẽ gần đây hơn (ví dụ: mô hình Falcon [1]), hoặc các tập dữ liệu hướng dẫn khác, thực hiện đáng kể tốt hơn hoặc khác biệt so với các mô hình được khám phá trong công trình này.

Cuối cùng, chúng tôi lưu ý rằng đánh giá dựa trên hướng dẫn mở có tính chủ quan cao và khó khăn do bản chất cực kỳ mở của nó. Có khả năng không có một câu trả lời nào chắc chắn là tốt nhất cho bất kỳ truy vấn nào, và các người chú thích khác nhau (dù là con người hay mô hình) sẽ có những thiên vị và sở thích khác nhau. Chúng tôi cũng lưu ý rằng trong trường hợp đánh giá dựa trên mô hình, chúng tôi chủ yếu so sánh đầu ra mô hình của chúng tôi với các thế hệ Davinci-003, có thể dẫn đến việc thưởng quá mức cho các mô hình tránh những thiếu sót của Davinci-003, hoặc không thưởng đúng cách cho các mô hình chia sẻ điểm mạnh với Davinci-003.

Mặc dù không hoàn toàn toàn diện trong công trình này, chúng tôi tin rằng bằng cách bao gồm một loạt rộng các mô hình, nó vẫn phục vụ như một đóng góp hữu ích và quan trọng trong việc cho thấy loại tài nguyên mở nào hoạt động, và nơi các nỗ lực cộng đồng tương lai nên hướng tới (các mô hình cơ sở tốt hơn, các tập dữ liệu điều chỉnh hướng dẫn đa dạng hơn).

B Tác động rộng hơn

Chúng tôi tin rằng một đánh giá nghiêm ngặt về các tài nguyên hiện có là tích cực rộng rãi, phơi bày những điểm mạnh và thiếu sót của các tài nguyên hiện có sẵn rộng rãi. Hơn nữa, vì tất cả tài nguyên được sử dụng đều sẵn có rộng rãi, tác hại gây ra bởi việc huấn luyện các mô hình này là khá nhỏ. Chúng tôi lưu ý rằng việc huấn luyện và phát hành các mô hình được điều chỉnh hướng dẫn đặc biệt lớn mà không có hướng dẫn được kiểm tra kỹ lưỡng mang theo một mức độ rủi ro, và ban đầu chúng tôi phát hành các mô hình lớn nhất của mình với thiết lập có cổng (yêu cầu người dùng đăng ký truy cập và được phê duyệt thủ công) để hạn chế tác hại tiềm ẩn.

C Chi tiết tập dữ liệu hướng dẫn

Chúng tôi cung cấp mô tả ngắn gọn về tất cả các tập dữ liệu hướng dẫn được sử dụng (và giấy phép) dưới đây:

• SuperNI: Một tập hợp các nhiệm vụ NLP đa dạng với hướng dẫn, được tạo bởi Wang và cộng sự [48]. Tập dữ liệu sử dụng giấy phép Apache-2.0.

• CoT: Một tập hợp các tập dữ liệu được chú thích với chuỗi suy nghĩ [50]. Chúng tôi sử dụng hỗn hợp CoT từ bộ sưu tập FLAN v2 [9], tách nó ra như một tập dữ liệu riêng. Hỗn hợp FLAN được phát hành dưới giấy phép Apache-2.0, mặc dù các tập dữ liệu thành phần có thể không sử dụng giấy phép này.

• FlanV2: Một tập hợp các nhiệm vụ NLP kết hợp một số tập dữ liệu NLP hiện có với các tăng cường dữ liệu khác nhau, được giới thiệu bởi Chung và cộng sự [9]. Hỗn hợp được phát hành dưới giấy phép Apache-2.0, mặc dù các tập dữ liệu thành phần có thể không sử dụng giấy phép này.

• Dolly: Một tập hợp các mẫu tuân theo hướng dẫn được tạo bởi nhân viên Databricks [12]. Tập dữ liệu được phát hành dưới Giấy phép Creative Commons Attribution-ShareAlike 3.0 Unported.

• Open Assistant 1: Một tập dữ liệu hội thoại kiểu trợ lý được chú thích bởi con người thông qua crowdsourcing, bao gồm một số lượng lớn các cuộc hội thoại mẫu trong nhiều ngôn ngữ đa dạng [26]. Tập dữ liệu được phát hành dưới giấy phép Apache-2.0.

• Self-Instruct: Một tập dữ liệu các mẫu tuân theo hướng dẫn được tạo bằng cách nhắc GPT-3 tạo ra các mẫu mới khi được cho một số phiên bản ví dụ [47]. Tập dữ liệu được phát hành dưới giấy phép Apache-2.0.

--- TRANG 15 ---
• UnnaturalInstructions: Một tập dữ liệu các mẫu tuân theo hướng dẫn được tạo bằng cách nhắc Davinci-002 sử dụng phương pháp được giới thiệu bởi Honovich và cộng sự [23]. Tập dữ liệu được phát hành dưới giấy phép MIT.

• Alpaca: Một tập dữ liệu được tạo sử dụng phương pháp kiểu self-instruct với Davinci-003 làm mô hình tạo và một số cải tiến so với self-instruct [43]. Tập dữ liệu được phát hành dưới giấy phép Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).

• Code-Alpaca: Một tập dữ liệu được tạo sử dụng phương pháp Alpaca, nhưng tập trung vào tạo mã [6]. Tập dữ liệu được phát hành dưới giấy phép Apache-2.0.

• GPT-4 Alpaca: Một tập dữ liệu được tạo sử dụng tập dữ liệu Alpaca làm đầu vào, nhưng thay thế các thế hệ ví dụ bằng các thế hệ từ GPT-4 [36]. Chúng tôi bao gồm điều này để xem tác động của việc sử dụng một mô hình tạo chất lượng tốt hơn. Tập dữ liệu được phát hành dưới giấy phép Apache-2.0.

• Baize: Một tập dữ liệu được tạo bằng cách nhắc ChatGPT và để nó trò chuyện với chính nó [52]. Tập dữ liệu được phát hành dưới Giấy phép Công cộng GNU v3.0.

• ShareGPT: Một tập hợp các tương tác người dùng với các hệ thống trò chuyện khác nhau được chia sẻ công khai. Chúng tôi sử dụng biến thể 'html-cleaned' có sẵn tại https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset. Sau đó chúng tôi chia các cuộc hội thoại dài (hơn 2048 token) thành các đoạn max-2048 token, tuân theo thiết lập Vicuna [8]. Chúng tôi không thực hiện bất kỳ lọc thêm nào đối với các mẫu. Tập dữ liệu này được phát hành dưới giấy phép Apache-2.0.

Chúng tôi lưu ý rằng các tập dữ liệu SuperNI và CoT được bao gồm trong bộ sưu tập FLAN V2 nhưng chỉ chiếm một phần nhỏ của tập dữ liệu FLAN V2 được lấy mẫu con của chúng tôi.

Chúng tôi cũng lưu ý rằng chúng tôi chủ yếu sử dụng các tập dữ liệu điều chỉnh hướng dẫn phổ biến đã có sẵn công khai, và trong trường hợp các tập dữ liệu do con người viết, chủ yếu sử dụng các tập dữ liệu được tạo một cách rõ ràng (với kiến thức của người tham gia) cho mục đích huấn luyện mô hình (ví dụ: Dolly, Open Assistant 1). Vì dữ liệu điều chỉnh hướng dẫn, hầu hết dữ liệu không có khả năng chứa chi tiết nhận dạng cá nhân, mặc dù chúng tôi lưu ý rằng chúng tôi không nỗ lực loại bỏ nội dung xúc phạm, vì vậy các mô hình của chúng tôi có thể tạo ra các thế hệ độc hại hoặc có hại.

D Chi tiết huấn luyện mô hình và tính toán

Chúng tôi huấn luyện tất cả các mô hình trong hai epoch với tốc độ học 2𝑒−5 (1𝑒−5 cho các mô hình 30B và 65B), không có weight decay và tốc độ học với suy giảm tuyến tính và khởi động tuyến tính cho 3% tổng số bước huấn luyện. Chúng tôi sử dụng độ dài chuỗi tối đa 2048 (1024 cho 30B và 65B), cắt ngắn các mẫu khi cần thiết. Trong quá trình huấn luyện, chúng tôi sử dụng thư viện DeepSpeed [38] và bộ tối ưu hóa ZeRO [37] để cho phép tinh chỉnh mô hình quy mô lớn. Trong tất cả các trường hợp, chúng tôi tinh chỉnh đầy đủ các mô hình. Chúng tôi huấn luyện các mô hình chủ yếu trên cụm GPU CSC LUMI, mỗi node chứa 4 GPU AMD MI250x.

E Thiết lập đánh giá

Chúng tôi cung cấp chi tiết thêm về các thiết lập đánh giá được sử dụng dưới đây. Chúng tôi cũng lưu ý rằng chúng tôi phát hành mã đánh giá cùng với mã huấn luyện của chúng tôi để cho phép tái tạo dễ dàng.

• MMLU: Chúng tôi sử dụng script và lời nhắc đánh giá MMLU chính thức có sẵn tại https://github.com/hendrycks/test, với các sửa đổi để cho phép xử lý hàng loạt. Chúng tôi đánh giá sử dụng 0 và 5 ví dụ few-shot, tuân theo thiết lập gốc của MMLU.

• GSM: Chúng tôi đánh giá các mô hình trên tập kiểm tra của GSM. Tuân theo Wei và cộng sự [50], chúng tôi đánh giá có và không có chuỗi suy nghĩ (CoT vs Direct). Cả hai thiết lập đều sử dụng 8 ví dụ few-shot trong ngữ cảnh (trong thiết lập chuỗi suy nghĩ, các ví dụ few-shot được đi kèm với chuỗi suy nghĩ). Vì tất cả câu trả lời trong GSM đều là số, chúng tôi trích xuất số cuối cùng trong phản hồi mô hình làm câu trả lời cuối cùng. Để cho phép đánh giá nhanh hơn, chúng tôi lấy mẫu ngẫu nhiên 200 ví dụ từ 1319 ví dụ kiểm tra, mà chúng tôi thấy cho hiệu suất tương tự như đánh giá tập đầy đủ.

• BBH: Chúng tôi tuân theo thiết lập được mô tả trong bài báo gốc Suzgun và cộng sự [42], và đánh giá có và không có chuỗi suy nghĩ (CoT vs Direct). Các lời nhắc được cung cấp chính thức, có 3 ví dụ few-shot trong ngữ cảnh được sử dụng cho cả thiết lập CoT và Direct. Đối với thiết lập CoT, chúng tôi trích xuất từ đầu tiên sau cụm từ 'Vậy câu trả lời là', hoặc toàn bộ phản hồi nếu không có chuỗi con như vậy.

--- TRANG 16 ---
• TydiQA: Chúng tôi tuân theo thiết lập được mô tả trong báo cáo kỹ thuật PaLM 2 [2] để đánh giá hiệu suất của các mô hình trong việc trả lời câu hỏi đa ngôn ngữ dưới hai thiết lập: 1) khi đoạn văn vàng chứa câu trả lời được cung cấp (GoldP/GP); 2) khi không có ngữ cảnh nào được cung cấp (Closed-Book/CB). Một ví dụ trong ngữ cảnh được sử dụng để làm quen mô hình với định dạng trả lời.

• Codex-Eval: Chúng tôi sử dụng tập dữ liệu HumanEval trong bài báo Codex [7] để đánh giá khả năng lập trình của các mô hình. Tập dữ liệu chứa 164 bài toán lập trình, nơi các mô hình được nhắc hoàn thành hàm Python đã cho docstring của nó. Tuân theo bài báo gốc, chúng tôi tính toán các ước lượng không thiên vị của pass@k để đo lường tính đúng đắn chức năng của các đầu ra mô hình. Chúng tôi báo cáo cả pass@1 và pass@10. Kết quả pass@1 được thu được bằng cách lấy mẫu với nhiệt độ 0.1 và kết quả pass@10 với nhiệt độ 0.8.

• ToxiGen: Chúng tôi tuân theo thiết lập trong Touvron và cộng sự [45], nhưng sử dụng tập lời nhắc gốc từ Hartvigsen và cộng sự [21], được thiết kế để gợi ra các thế hệ độc hại cho các nhóm nhất định. Chúng tôi chỉ lấy các lời nhắc được thiết kế để tạo ra ngôn ngữ độc hại (lời nhắc 'thù địch') và sử dụng 500 lời nhắc mỗi nhóm để giảm chi phí đánh giá. Đối với các mô hình ngôn ngữ cơ sở, chúng tôi truyền các lời nhắc ToxiGen gốc không thay đổi và giải mã tham lam đến dòng mới đầu tiên (hoặc tối đa 512 token). Đối với các mô hình được điều chỉnh hướng dẫn, chúng tôi đặt lời nhắc vào mẫu tương ứng, và yêu cầu mô hình hoàn thành lời nhắc, cho đến khi mô hình tạo ra token dừng (hoặc tối đa 512 token). Chúng tôi truyền văn bản được tạo vào mô hình roberta-large được huấn luyện để phát hiện nội dung độc hại được tinh chỉnh như một phần của Hartvigsen và cộng sự [21]⁵. Sau đó chúng tôi báo cáo tỷ lệ phần trăm của các thế hệ được coi là độc hại bởi bộ phân loại.

• TruthfulQA: Tuân theo Touvron và cộng sự [45], chúng tôi chủ yếu sử dụng thiết lập tạo của TruthfulQA [30]. Tập dữ liệu TruthfulQA chứa 818 câu hỏi, được sử dụng để nhắc mô hình được kiểm tra tạo ra câu trả lời. Chúng tôi sử dụng định dạng lời nhắc QA mặc định với 6 ví dụ QA trong ngữ cảnh. Chúng tôi tuân theo script chính thức trong implementation chính thức của họ⁶ để thực hiện giải mã tham lam và xử lý hậu kỳ câu trả lời. Chúng tôi cũng tuân theo hướng dẫn của họ để huấn luyện hai bộ phân loại dựa trên GPT để đánh giá tính đúng đắn và tính thông tin của phản hồi mô hình. Chúng tôi báo cáo tỷ lệ các phản hồi đúng sự thật (%Truthful), thông tin (%Informative), và cả hai (%Informative and Truthful) làm chỉ số của chúng tôi. Tuân theo Touvron và cộng sự [45], chúng tôi chỉ báo cáo (% Informative and Truthful) làm chỉ số chính của chúng tôi trong bài báo chính.

• AlpacaEval: Chúng tôi sử dụng gói được cung cấp bởi Li và cộng sự [27], tuân theo thiết lập mặc định yêu cầu mô hình được đánh giá tạo ra phản hồi cho 805 lời nhắc và sử dụng GPT-4 để so sánh phản hồi với Davinci-003. Chúng tôi sử dụng cấu hình annotator "alpaca_eval_gpt4_0314" thay vì "alpaca_eval_gpt4" để làm cho kết quả có thể tái tạo. Chúng tôi cho phép mô hình được đánh giá tạo ra đến 8192 token, mà không chỉ định chuỗi dừng đặc biệt. Tỷ lệ thắng báo cáo là tỷ lệ phần trăm của các thế hệ mô hình mà GPT-4 báo cáo là được ưa thích hơn so với các thế hệ từ Davinci-003.

Đối với tất cả các đánh giá, chúng tôi tải các mô hình sử dụng chế độ 8-bit [13] được cung cấp trong thư viện Huggingface Transformers, mà chúng tôi thấy tăng tốc độ suy luận đáng kể và có tác động không đáng kể đến hiệu suất cuối cùng. Khi thực hiện tạo, chúng tôi sử dụng giải mã tham lam và độ dài tối đa 512 token, trừ khi được chỉ định khác.

F Tổng quan về tất cả kết quả đánh giá tự động

Bảng 8 trình bày một tổng hợp kết quả của tất cả các mô hình được huấn luyện như một phần của công trình này trên tất cả các benchmark đánh giá khả năng cốt lõi. Chúng tôi liệt kê nhiều kịch bản cho tất cả các thiết lập đánh giá trừ AlpacaEval, có một thiết lập. Vui lòng tham khảo §E để biết ý nghĩa của các chỉ số được báo cáo. Chúng tôi cũng tính toán trung bình trên các benchmark trong Bảng 8. Điều này được tính toán bằng cách đầu tiên tính toán trung bình mỗi benchmark bằng cách lấy trung bình trên các kịch bản. Sau đó chúng tôi tính toán trung bình tổng thể với mỗi benchmark được cân bằng như nhau.

Ngoài ra, đối với đánh giá an toàn, chúng tôi cung cấp kết quả ToxiGen được phân tích theo nhóm được nhắm mục tiêu trong Bảng 9 cho tất cả các mô hình, từ đó chúng tôi có thể thấy một số nhóm được nhắm mục tiêu đặc biệt, ngay cả sau khi điều chỉnh hướng dẫn. Chúng tôi cũng cung cấp kết quả TruthfulQA đầy đủ trong Bảng 10. Kết quả được phân tích thành % thông tin và % đúng sự thật - xem Lin và cộng sự [29] để biết chi tiết về các chỉ số này.

--- TRANG 17 ---
Bảng 8: Tổng quan về hiệu suất của tất cả các mô hình được tinh chỉnh cho công trình này, cùng với các mô hình độc quyền, trên các benchmark đã chọn. Để tính toán trung bình, chúng tôi tính toán trung bình mỗi benchmark và sau đó lấy trung bình trên các benchmark này. Xem Phụ lục F để biết thêm chi tiết.

[Bảng này chứa kết quả hiệu suất chi tiết cho tất cả các mô hình trên các chỉ số đánh giá khác nhau bao gồm MMLU, GSM, BBH, TydiQA, Codex-Eval, và AlpacaEval]

--- TRANG 18 ---
Bảng 9: Kết quả ToxiGen trên các mô hình. Chúng tôi báo cáo tỷ lệ phần trăm của các thế hệ được coi là độc hại bởi một bộ phân loại riêng biệt, được phân tích theo nhóm mà lời nhắc được thiết kế để tạo ra các thế hệ độc hại.

[Bảng này hiển thị kết quả ToxiGen chi tiết cho các nhóm khác nhau như Asian, Black, Chinese, Jewish, Latino, LGBTQ, v.v.]

--- TRANG 19 ---
Bảng 10: Kết quả TruthfulQA trên các mô hình. Chúng tôi báo cáo tỷ lệ phần trăm của các câu trả lời có thông tin, hoặc đúng sự thật, hoặc cả hai.

[Bảng này hiển thị kết quả TruthfulQA với các chỉ số % Informative, % Truthful, và % Informative and Truthful]

--- TRANG 20 ---
G Chi tiết đánh giá con người

G.1 Thiết lập

Ở đây chúng tôi cung cấp chi tiết hơn cho đánh giá con người được mô tả trong §4.3. Đánh giá của chúng tôi chứa 332 hướng dẫn, bao gồm 252 hướng dẫn từ tập đánh giá Self-Instruct [47] và 80 hướng dẫn từ tập đánh giá Vicuna [8]. Đánh giá của chúng tôi được thực hiện cho ba cặp mô hình: 1) TÜLU 65B so với ChatGPT, 2) TÜLU 65B so với TÜLU 7B, 3) TÜLU 65B so với một mô hình LLAMA 65B được huấn luyện trên hỗn hợp dữ liệu Con người, sử dụng cùng một tập hướng dẫn cho cả ba so sánh.

Để đảm bảo đánh giá đáng tin cậy, chúng tôi tuyển dụng 18 người chú thích chuyên gia, là các nhà nghiên cứu tại AI2 hoặc sinh viên tại UW, để chú thích. Tất cả những người chú thích này đều nói tiếng Anh thông thạo và có bằng cử nhân trở lên.

Chúng tôi thiết kế một trang web, được hiển thị trong Hình 5, để người chú thích của chúng tôi thực hiện đánh giá, và chúng tôi sẽ phát hành mã cho trang web này. Khi thực hiện đánh giá, người chú thích được hướng dẫn đọc kỹ lời nhắc và đầu ra A và B từ hai mô hình, sau đó trả lời ba câu hỏi yêu cầu sự chấp nhận của các đầu ra và so sánh chúng về mặt tính hữu ích. Họ được khuyến khích sử dụng Google hoặc bất kỳ công cụ bên ngoài nào có thể giúp ích cho việc đánh giá. Thông tin mô hình được ẩn danh, và các đầu ra của chúng được đặt theo thứ tự ngẫu nhiên.

Hình 5: Giao diện trang web cho đánh giá con người của chúng tôi (xem Phụ lục G để biết chi tiết). Người dùng cần đăng nhập vào hệ thống, đọc lời nhắc và đầu ra từ hai mô hình (với tên mô hình được ẩn danh và thứ tự ngẫu nhiên), sau đó trả lời liệu đầu ra A và đầu ra B có thể chấp nhận được hay không một cách riêng lẻ, và cuối cùng so sánh chúng về mặt tính hữu ích.

G.2 Thỏa thuận giữa các người chú thích

Chúng tôi đo lường sự đồng ý của người chú thích của chúng tôi trên một tập con 119 ví dụ (63 phiên bản được lấy mẫu ngẫu nhiên từ so sánh ChatGPT vs TÜLU 65B, và 59 phiên bản được lấy mẫu ngẫu nhiên từ so sánh TÜLU 65B vs TÜLU 7B). Chúng tôi giao hai người chú thích cho mỗi ví dụ này và tính toán sự đồng ý của họ cho cả đánh giá chấp nhận và đánh giá so sánh cặp.

--- TRANG 21 ---
Người chú thích đạt được sự đồng ý 0.84 về việc liệu một đầu ra mô hình có nên được chấp nhận hay không. Đối với so sánh cặp, tuân theo Zhou và cộng sự [56], chúng tôi báo cáo độ chính xác loại trừ hòa, gán một điểm nếu cả hai người chú thích đồng ý, nửa điểm nếu một trong hai người chú thích (nhưng không phải cả hai) đánh dấu hòa, và không có điểm nếu ngược lại. Chúng tôi cũng gộp "rõ ràng tốt hơn" và "hơi tốt hơn" lại với nhau, vì vậy các lựa chọn cuối cùng của chúng tôi sẽ chỉ đơn giản là so sánh cái nào trong A và B tốt hơn, hoặc hòa. Người chú thích của chúng tôi đạt được sự đồng ý 0.72 cho so sánh cặp này.

Mặc dù những con số này cho thấy sự đồng ý hợp lý, chúng tôi cũng lưu ý rằng có một mức độ chủ quan lớn trong đánh giá con người. Mức độ nhiễu này cũng chỉ ra rằng một số công trình trước đây [8,55] sử dụng một số lượng nhỏ ví dụ cho đánh giá con người có thể không đủ đáng tin cậy. Chúng tôi đề xuất rằng cộng đồng cần tiếp tục cải thiện độ tin cậy và khả năng mở rộng của đánh giá con người cho các mô hình tuân theo hướng dẫn.

H Điều tra thêm về Hình 2

Để điều tra thêm về mức độ mà số lượng token duy nhất được GPT-4 sử dụng làm dấu hiệu của chất lượng, chúng tôi tạo ra một người đánh giá giả mạo so sánh hai đầu ra mô hình, và gán chiến thắng cho đầu ra có nhiều token duy nhất hơn. Chúng tôi vẽ biểu đồ tỷ lệ thắng được tính toán sử dụng người đánh giá giả mạo này so với tỷ lệ thắng được tính toán sử dụng GPT-4 trong Hình 6.

Chúng tôi thấy rằng trong khi người đánh giá giả mạo nhìn chung đánh giá quá cao tỷ lệ thắng, xu hướng vẫn tuyến tính một cách đáng chú ý. Chúng tôi lưu ý rằng 𝑅² cho đường xu hướng là .91, gợi ý rằng số lượng token duy nhất giải thích một tỷ lệ lớn của phương sai trong tỷ lệ thắng. Dựa trên điều này, chúng tôi tin rằng số lượng token duy nhất chắc chắn là một sở thích chính mà GPT-4 quan tâm trong đánh giá của nó, mặc dù nó vẫn không phải là tính năng quan trọng duy nhất.

Hình 6: Điểm số tỷ lệ thắng của tất cả các mô hình được đánh giá bởi người đánh giá giả mạo so với tỷ lệ thắng của tất cả các mô hình sử dụng người đánh giá GPT-4.

I Giấy phép mô hình

Chúng tôi cung cấp thông tin ngắn gọn về giấy phép của các mô hình cơ bản chúng tôi sử dụng trong công trình này dưới đây.

• LLAMA: Trọng số mô hình LLAMA được phát hành dưới giấy phép tùy chỉnh cho phép sử dụng mô hình cho mục đích nghiên cứu phi thương mại.

• LLAMA-2: Trọng số mô hình LLAMA-2 được phát hành dưới giấy phép tùy chỉnh cho phép sử dụng thương mại và nghiên cứu với một số hạn chế (ví dụ: có ít hơn 700 triệu người dùng hoạt động hàng tháng nếu được sử dụng trong ứng dụng thương mại), và rõ ràng cho phép phân phối lại trọng số.

--- TRANG 22 ---
• Pythia: Trọng số Pythia được phát hành dưới giấy phép Apache-2.0.

• OPT: Trọng số mô hình OPT được phát hành dưới giấy phép tùy chỉnh chỉ cho phép sử dụng mô hình cho mục đích nghiên cứu phi thương mại.

--- TRANG 23 ---
