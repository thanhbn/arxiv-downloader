# 2308.10792.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2308.10792.pdf
# Kích thước tệp: 5626976 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Điều chỉnh Hướng dẫn cho Mô hình Ngôn ngữ Lớn: Một Khảo sát
Shengyu Zhang♠, Linfeng Dong♠, Xiaoya Li♣, Sen Zhang♠
Xiaofei Sun♠, Shuhe Wang♣, Jiwei Li♠♣, Runyi Hu♠
Tianwei Zhang▲, Fei Wu♠và Guoyin Wang♦

Tóm tắt
Bài báo này khảo sát các nghiên cứu trong lĩnh vực điều chỉnh hướng dẫn (IT) đang phát triển nhanh chóng, cũng có thể được gọi là tinh chỉnh có giám sát (SFT)1, một kỹ thuật quan trọng để nâng cao khả năng và tính kiểm soát của các mô hình ngôn ngữ lớn (LLMs). Điều chỉnh hướng dẫn đề cập đến quá trình đào tạo thêm LLMs trên một tập dữ liệu bao gồm các cặp (HƯỚNG DẪN, ĐẦU RA) theo cách có giám sát, điều này thu hẹp khoảng cách giữa mục tiêu dự đoán từ tiếp theo của LLMs và mục tiêu của người dùng là có LLMs tuân theo hướng dẫn của con người. Trong công trình này, chúng tôi thực hiện một đánh giá có hệ thống của tài liệu, bao gồm phương pháp luận chung của SFT, việc xây dựng tập dữ liệu SFT, đào tạo các mô hình SFT, và ứng dụng cho các phương thức, lĩnh vực và ứng dụng khác nhau, cùng với phân tích về các khía cạnh ảnh hưởng đến kết quả của SFT (ví dụ: tạo ra đầu ra hướng dẫn, kích thước của tập dữ liệu hướng dẫn, v.v.). Chúng tôi cũng xem xét các bẫy tiềm ẩn của SFT cùng với những chỉ trích về nó, cùng với những nỗ lực chỉ ra những thiếu sót hiện tại của các chiến lược hiện có và đề xuất một số hướng nghiên cứu có thể mang lại thành quả.

1 Giới thiệu
Lĩnh vực mô hình ngôn ngữ lớn (LLMs) đã chứng kiến những tiến bộ đáng kể trong những năm gần đây. Các LLMs như GPT-3 (Brown et al., 2020b), PaLM (Chowdhery et al., 2022), và LLaMA (Touvron et al., 2023a) đã thể hiện khả năng ấn tượng trên một loạt rộng các nhiệm vụ ngôn ngữ tự nhiên (Zhao et al., 2021; Wang et al., 2022b, 2023b; Wan et al., 2023; Sun et al., 2023c; Wei et al., 2023a; Li et al., 2023a; Gao et al., 2023a; Yao et al., 2023; Yang et al., 2022a; Qian et al., 2022; Lee et al., 2022; Yang et al., 2022b; Gao et al., 2023b; Ning et al., 2023; Liu et al., 2021b; Wiegreffe et al., 2021; Sun et al., 2023b,a; Adlakha et al., 2023; Chen et al., 2023b). Một trong những vấn đề chính với LLMs là sự không khớp giữa mục tiêu đào tạo và mục tiêu của người dùng: LLMs thường được đào tạo để tối thiểu hóa lỗi dự đoán từ theo ngữ cảnh trên các kho dữ liệu lớn; trong khi người dùng muốn mô hình "tuân theo hướng dẫn của họ một cách hữu ích và an toàn" (Radford et al., 2019; Brown et al., 2020a; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022)

Để giải quyết sự không khớp này, điều chỉnh hướng dẫn (IT), cũng có thể được gọi là tinh chỉnh có giám sát (SFT), được đề xuất, phục vụ như một kỹ thuật hiệu quả để nâng cao khả năng và tính kiểm soát của các mô hình ngôn ngữ lớn. Nó bao gồm việc đào tạo thêm LLMs sử dụng các cặp (HƯỚNG DẪN, ĐẦU RA), trong đó HƯỚNG DẪN biểu thị hướng dẫn của con người cho mô hình, và ĐẦU RA biểu thị đầu ra mong muốn tuân theo HƯỚNG DẪN. Lợi ích của SFT có ba khía cạnh: (1) Tinh chỉnh LLM trên tập dữ liệu hướng dẫn thu hẹp khoảng cách giữa mục tiêu dự đoán từ tiếp theo của LLMs và mục tiêu tuân theo hướng dẫn của người dùng; (2) SFT cho phép hành vi mô hình có thể kiểm soát và dự đoán được hơn so với LLMs tiêu chuẩn. Các hướng dẫn phục vụ để ràng buộc đầu ra của mô hình để phù hợp với các đặc điểm phản hồi mong muốn hoặc kiến thức lĩnh vực, cung cấp một kênh cho con người can thiệp vào hành vi của mô hình; và (3) SFT có hiệu quả tính toán và có thể giúp LLMs nhanh chóng thích ứng với một lĩnh vực cụ thể mà không cần đào tạo lại rộng rãi hoặc thay đổi kiến trúc.

Mặc dù hiệu quả, SFT cũng đặt ra thách thức: (1) Tạo ra các hướng dẫn chất lượng cao mà đúng cách bao phủ các hành vi mục tiêu mong muốn không đơn giản: các tập dữ liệu hướng dẫn hiện có thường bị hạn chế về số lượng, tính đa dạng và sáng tạo; (2) đã có mối quan ngại ngày càng tăng rằng SFT chỉ cải thiện trên các nhiệm vụ được hỗ trợ mạnh trong tập dữ liệu đào tạo SFT (Gudibande et al., 2023); và (3) đã có sự chỉ trích mạnh mẽ rằng SFT chỉ nắm bắt các mẫu và phong cách ở mức bề mặt (ví dụ: định dạng đầu ra) thay vì hiểu và học nhiệm vụ (Kung và Peng, 2023). Cải thiện việc tuân theo hướng dẫn và xử lý các phản hồi mô hình không lường trước vẫn là những vấn đề nghiên cứu mở. Những thách thức này làm nổi bật tầm quan trọng của các điều tra, phân tích và tóm tắt sâu hơn trong lĩnh vực này, để tối ưu hóa quá trình tinh chỉnh và hiểu rõ hơn về hành vi của các LLMs được điều chỉnh hướng dẫn.

Trong tài liệu, đã có sự quan tâm nghiên cứu ngày càng tăng trong phân tích và thảo luận về LLMs, bao gồm các phương pháp tiền đào tạo (Zhao et al., 2023), khả năng lập luận (Huang và Chang, 2022), ứng dụng downstream (Yang et al., 2023a; Sun et al., 2023b), nhưng hiếm khi về chủ đề điều chỉnh hướng dẫn LLM. Khảo sát này cố gắng lấp đầy khoảng trống này, tổ chức kiến thức mới nhất về lĩnh vực đang phát triển nhanh chóng này.

Cụ thể,
• Phần 2 trình bày phương pháp luận chung được sử dụng trong điều chỉnh hướng dẫn.
• Phần 3 phác thảo quá trình xây dựng các tập dữ liệu SFT đại diện thường được sử dụng.
• Phần 4 trình bày các mô hình được điều chỉnh hướng dẫn đại diện.
• Phần 5 xem xét các kỹ thuật và tập dữ liệu đa phương thức cho điều chỉnh hướng dẫn, bao gồm hình ảnh, lời nói và video.
• Phần 6 xem xét các nỗ lực thích ứng LLMs cho các lĩnh vực và ứng dụng khác nhau sử dụng chiến lược SFT.
• Phần 7 xem xét các khám phá để làm cho điều chỉnh hướng dẫn hiệu quả hơn, giảm chi phí tính toán và thời gian liên quan đến việc thích ứng các mô hình lớn.
• Phần 8 trình bày đánh giá các mô hình SFT, phân tích về chúng, cùng với chỉ trích đối với chúng.

2 Phương pháp luận

Trong phần này, chúng tôi mô tả quy trình chung được sử dụng trong điều chỉnh hướng dẫn.

2.1 Xây dựng Tập dữ liệu Hướng dẫn

Mỗi thể hiện trong tập dữ liệu hướng dẫn bao gồm ba yếu tố: một hướng dẫn, là một chuỗi văn bản ngôn ngữ tự nhiên để chỉ định nhiệm vụ (ví dụ: viết một lá thư cảm ơn cho XX về XX, viết một blog về chủ đề XX, v.v.); một đầu vào tùy chọn cung cấp thông tin bổ sung cho ngữ cảnh; và một đầu ra dự kiến dựa trên hướng dẫn và đầu vào.

Có hai phương pháp chung để xây dựng tập dữ liệu hướng dẫn:

• Tích hợp dữ liệu từ các tập dữ liệu ngôn ngữ tự nhiên có chú thích. Trong phương pháp này, các cặp (hướng dẫn, đầu ra) được thu thập từ các tập dữ liệu ngôn ngữ tự nhiên có chú thích hiện có bằng cách sử dụng các mẫu để chuyển đổi các cặp văn bản-nhãn thành các cặp (hướng dẫn, đầu ra). Các tập dữ liệu như Flan (Longpre et al., 2023) và P3 (Sanh et al., 2021) được xây dựng dựa trên chiến lược tích hợp dữ liệu.

• Tạo đầu ra sử dụng LLMs: Một cách thay thế để nhanh chóng thu thập các đầu ra mong muốn cho các hướng dẫn đã cho là sử dụng LLMs như GPT-3.5-Turbo hoặc GPT4 thay vì thu thập thủ công các đầu ra. Hướng dẫn có thể đến từ hai nguồn: (1) thu thập thủ công; hoặc (2) mở rộng dựa trên một số lượng nhỏ hướng dẫn hạt giống viết tay sử dụng LLMs. Tiếp theo, các hướng dẫn được thu thập được đưa vào LLMs để có được đầu ra. Các tập dữ liệu như InstructWild (Xue et al., 2023) và Self-Instruct (Wang et al., 2022c) được tạo theo phương pháp này.

Đối với các tập dữ liệu SFT hội thoại nhiều lượt, chúng ta có thể có các mô hình ngôn ngữ lớn tự đóng vai các vai trò khác nhau (người dùng và trợ lý AI) để tạo ra các tin nhắn trong định dạng hội thoại (Xu et al., 2023b).

2.2 Điều chỉnh Hướng dẫn / Tinh chỉnh có Giám sát

Dựa trên tập dữ liệu SFT được thu thập, một mô hình đã được tiền đào tạo có thể được tinh chỉnh trực tiếp theo cách hoàn toàn có giám sát, trong đó với hướng dẫn và đầu vào đã cho, mô hình được đào tạo bằng cách dự đoán từng token trong đầu ra một cách tuần tự.

3 Tập dữ liệu

Trong phần này, chúng tôi chi tiết các tập dữ liệu điều chỉnh hướng dẫn trong cộng đồng, phân loại chúng thành ba lớp: (1) Dữ liệu Thủ công, (2) Dữ liệu Tổng hợp qua Chưng cất, và (3) Dữ liệu Tổng hợp qua Tự cải thiện. Dưới đây, chúng tôi mô tả một số tập dữ liệu được sử dụng rộng rãi, và đối với các tập dữ liệu đã thu thập đầy đủ, chúng tôi đặt chúng trong Phụ lục A.

3.1 Dữ liệu Thủ công

Dữ liệu thủ công bao gồm các tập dữ liệu được chú thích thủ công hoặc được lấy trực tiếp từ internet. Việc tạo ra các tập dữ liệu này thường không liên quan đến các kỹ thuật học máy, chỉ dựa vào thu thập và xác minh thủ công, dẫn đến các tập dữ liệu nhỏ hơn nói chung. Dưới đây là một số tập dữ liệu thủ công được sử dụng rộng rãi:

3.1.1 Natural Instructions

Natural Instructions (Mishra et al., 2021) là một tập dữ liệu hướng dẫn tiếng Anh thủ công bao gồm 193K thể hiện, đến từ 61 nhiệm vụ NLP riêng biệt. Tập dữ liệu bao gồm "hướng dẫn" và "thể hiện". Mỗi thể hiện trong "hướng dẫn" là một mô tả nhiệm vụ bao gồm 7 thành phần: tiêu đề, định nghĩa, những điều cần tránh nhấn mạnh/cảnh báo, gợi ý, ví dụ tích cực và ví dụ tiêu cực. Hình con (a) trong Hình 2 đưa ra một ví dụ về "hướng dẫn". "Thể hiện" bao gồm các cặp ("đầu vào", "đầu ra"), là dữ liệu đầu vào và kết quả văn bản tuân theo hướng dẫn đã cho một cách chính xác. Hình con (b) trong Hình 2 đưa ra một ví dụ về các thể hiện.

Dữ liệu đến từ các tập dữ liệu NLP hiện có của 61 nhiệm vụ. Các tác giả thu thập "hướng dẫn" bằng cách tham khảo tệp hướng dẫn chú thích tập dữ liệu. Tiếp theo, các tác giả xây dựng "thể hiện" bằng cách thống nhất các thể hiện dữ liệu trên tất cả các tập dữ liệu NLP thành các cặp ("đầu vào", "đầu ra").

3.1.2 P3

P3 (Public Pool of Prompts) (Sanh et al., 2021) là một tập dữ liệu điều chỉnh hướng dẫn được xây dựng bằng cách tích hợp 170 tập dữ liệu NLP tiếng Anh và 2,052 gợi ý tiếng Anh. Gợi ý, đôi khi được gọi là mẫu nhiệm vụ, là các hàm ánh xạ một thể hiện dữ liệu trong nhiệm vụ NLP thông thường (ví dụ: trả lời câu hỏi, phân loại văn bản) thành một cặp đầu vào-đầu ra ngôn ngữ tự nhiên.

Mỗi thể hiện trong P3 có ba thành phần: "inputs", "answer_choices", và "targets". "Inputs" là một chuỗi văn bản mô tả nhiệm vụ bằng ngôn ngữ tự nhiên (ví dụ: "Nếu anh ấy thích Mary là đúng, thì có đúng rằng anh ấy thích con mèo của Mary không?"). "Answer choices" là một danh sách chuỗi văn bản là các phản hồi có thể áp dụng cho nhiệm vụ đã cho (ví dụ: ["có", "không", "không xác định"]). "Targets" là một chuỗi văn bản là phản hồi chính xác cho "inputs" đã cho (ví dụ: "có"). Các tác giả xây dựng PromptSource, một công cụ để tạo gợi ý chất lượng cao một cách hợp tác và một kho lưu trữ để cung cấp mã nguồn mở cho các gợi ý chất lượng cao. Tập dữ liệu P3 được xây dựng bằng cách lấy mẫu ngẫu nhiên một gợi ý từ nhiều gợi ý trong PromptSource và ánh xạ mỗi thể hiện thành một bộ ba ("inputs", "answer choices", "targets").

3.1.3 xP3

xP3 (Crosslingual Public Pool of Prompts) (Muennighoff et al., 2022) là một tập dữ liệu hướng dẫn đa ngôn ngữ bao gồm 16 nhiệm vụ ngôn ngữ tự nhiên đa dạng trong 46 ngôn ngữ. Mỗi thể hiện trong tập dữ liệu có hai thành phần: "inputs" và "targets". "Inputs" là một mô tả nhiệm vụ bằng ngôn ngữ tự nhiên. "Targets" là kết quả văn bản tuân theo hướng dẫn "inputs" một cách chính xác.

Dữ liệu gốc trong xP3 đến từ ba nguồn: tập dữ liệu hướng dẫn tiếng Anh P3, 4 nhiệm vụ tiếng Anh chưa thấy trong P3 (ví dụ: dịch thuật, tổng hợp chương trình), và 30 tập dữ liệu NLP đa ngôn ngữ. Các tác giả xây dựng tập dữ liệu xP3 bằng cách lấy mẫu các mẫu nhiệm vụ viết bởi con người từ PromptSource và sau đó điền vào các mẫu để chuyển đổi các nhiệm vụ NLP đa dạng thành một định dạng thống nhất. Ví dụ, một mẫu nhiệm vụ cho nhiệm vụ suy luận ngôn ngữ tự nhiên như sau: "Nếu Tiền đề là đúng, thì có đúng rằng Giả thuyết?" ; "có", "có thể", "không" tương ứng với các nhãn nhiệm vụ gốc "kéo theo (0)", "trung tính (1)" và "mâu thuẫn (2)".

3.1.4 Flan 2021

Flan 2021 (Longpre et al., 2023) là một tập dữ liệu hướng dẫn tiếng Anh được xây dựng bằng cách chuyển đổi 62 điểm chuẩn NLP được sử dụng rộng rãi (ví dụ: SST-2, SNLI, AG News, MultiRC) thành các cặp đầu vào-đầu ra ngôn ngữ. Mỗi thể hiện trong Flan 2021 có các thành phần "input" và "target". "Input" là một chuỗi văn bản mô tả một nhiệm vụ qua một hướng dẫn ngôn ngữ tự nhiên (ví dụ: "xác định cảm xúc của câu 'Anh ấy thích con mèo.' là tích cực hay tiêu cực?"). "Target" là một kết quả văn bản thực hiện hướng dẫn "input" một cách chính xác (ví dụ: "tích cực"). Các tác giả chuyển đổi các tập dữ liệu NLP thông thường thành các cặp đầu vào-mục tiêu bằng cách: Bước 1: soạn thảo thủ công các mẫu hướng dẫn và mục tiêu; Bước 2: điền các mẫu với các thể hiện dữ liệu từ tập dữ liệu.

3.1.5 LIMA

LIMA (Zhou et al., 2023a) là một tập dữ liệu hướng dẫn tiếng Anh bao gồm một tập huấn luyện với 1K thể hiện dữ liệu và một tập kiểm tra với 300 thể hiện. Tập huấn luyện chứa 1K cặp ("hướng dẫn", "phản hồi"). Đối với dữ liệu huấn luyện, 75% được lấy mẫu từ ba trang web câu hỏi & trả lời cộng đồng (tức là Stack Exchange, wikiHow, và Pushshift Reddit Dataset (Baumgartner et al., 2020)); 20% được viết thủ công bởi một nhóm tác giả (được gọi là Nhóm A) được truyền cảm hứng từ sở thích của họ; 5% được lấy mẫu từ tập dữ liệu Super-Natural Instructions (Wang et al., 2022d). Về tập hợp lệ, các tác giả lấy mẫu 50 thể hiện từ tập viết bởi tác giả Nhóm A. Tập kiểm tra chứa 300 ví dụ, với 76,7% được viết bởi một nhóm khác (Nhóm B) của các tác giả và 23,3% được lấy mẫu từ Pushshift Reddit Dataset (Baumgartner et al., 2020), là một bộ sưu tập câu hỏi & trả lời trong cộng đồng Reddit.

3.1.6 Super-Natural Instructions

Super Natural Instructions (Wang et al., 2022f) là một bộ sưu tập hướng dẫn đa ngôn ngữ bao gồm 1,616 nhiệm vụ NLP và 5M thể hiện nhiệm vụ, bao phủ 76 loại nhiệm vụ riêng biệt (ví dụ: phân loại văn bản, trích xuất thông tin, viết lại văn bản, sáng tác văn bản, v.v.) và 55 ngôn ngữ. Mỗi nhiệm vụ trong tập dữ liệu bao gồm một "hướng dẫn" và "thể hiện nhiệm vụ". Cụ thể, "hướng dẫn" có ba thành phần: một "định nghĩa" mô tả nhiệm vụ bằng ngôn ngữ tự nhiên; "ví dụ tích cực" là các mẫu đầu vào và đầu ra chính xác, cùng với một giải thích ngắn cho mỗi; và "ví dụ tiêu cực" là các mẫu đầu vào và đầu ra không mong muốn, cùng với một giải thích ngắn cho mỗi, như được hiển thị trong Hình 2 (a). "Thể hiện nhiệm vụ" là các thể hiện dữ liệu bao gồm đầu vào văn bản và một danh sách các đầu ra văn bản chấp nhận được, như được hiển thị trong Hình 2 (b). Dữ liệu gốc trong Super Natural Instructions đến từ ba nguồn: (1) các tập dữ liệu NLP công khai hiện có (ví dụ: CommonsenseQA); (2) các chú thích trung gian có thể áp dụng được tạo ra thông qua một quá trình crowdsourcing (ví dụ: kết quả diễn giải cho một câu hỏi đã cho trong quá trình crowdsourcing tập dữ liệu QA); (3) các nhiệm vụ tổng hợp được chuyển đổi từ các nhiệm vụ ký hiệu và được diễn giải lại trong một vài câu (ví dụ: các phép toán đại số như so sánh số).

3.1.7 Dolly

Dolly (Conover et al., 2023a) là một tập dữ liệu hướng dẫn tiếng Anh với 15,000 thể hiện dữ liệu được tạo bởi con người được thiết kế để cho phép LLMs tương tác với người dùng giống như ChatGPT. Tập dữ liệu được thiết kế để mô phỏng một loạt rộng các hành vi của con người, bao gồm 7 loại cụ thể: Q&A mở, Q&A đóng, trích xuất thông tin từ Wikipedia, tóm tắt thông tin từ Wikipedia, động não, phân loại và viết sáng tạo. Các ví dụ của mỗi loại nhiệm vụ trong tập dữ liệu được hiển thị trong Bảng 1.

3.1.8 OpenAssistant Conversations

OpenAssistant Conversations (Köpf et al., 2023) là một kho văn bản hội thoại kiểu trợ lý đa ngôn ngữ thủ công bao gồm 161,443 tin nhắn (tức là 91,829 gợi ý người dùng, 69,614 phản hồi trợ lý) từ 66,497 cây hội thoại trong 35 ngôn ngữ, cùng với 461,292 đánh giá chất lượng có chú thích của con người. Mỗi thể hiện trong tập dữ liệu là một cây hội thoại (CT). Cụ thể, mỗi nút trong cây hội thoại biểu thị một tin nhắn được tạo bởi các vai trò (tức là người gợi ý, trợ lý) trong hội thoại. Nút gốc của CT đại diện cho một gợi ý ban đầu từ người gợi ý, trong khi các nút khác biểu thị phản hồi từ người gợi ý hoặc trợ lý. Một đường dẫn từ gốc đến bất kỳ nút nào trong CT đại diện cho một hội thoại hợp lệ giữa người gợi ý và trợ lý theo lượt và được gọi là một luồng. Hình 4 hiển thị một ví dụ về cây hội thoại bao gồm 12 tin nhắn trong 6 luồng.

Các tác giả đầu tiên thu thập các cây hội thoại dựa trên quy trình năm bước:
Bước 1. gợi ý: người đóng góp thực hiện vai trò người gợi ý và tạo ra các gợi ý ban đầu;
Bước 2. gán nhãn gợi ý: người đóng góp đánh giá điểm cho các gợi ý ban đầu từ bước 1, và các tác giả chọn các gợi ý chất lượng cao làm nút gốc với chiến lược lấy mẫu cân bằng;
Bước 3. mở rộng nút cây: người đóng góp thêm tin nhắn phản hồi với vai trò người gợi ý hoặc trợ lý;
Bước 4. gán nhãn phản hồi: người đóng góp chỉ định điểm cho phản hồi nút hiện có;
Bước 5. xếp hạng: người đóng góp xếp hạng phản hồi trợ lý tham khảo hướng dẫn người đóng góp.

Máy trạng thái cây quản lý và theo dõi trạng thái (ví dụ: trạng thái ban đầu, trạng thái phát triển, trạng thái kết thúc) trong suốt quá trình tạo hội thoại. Sau đó, tập dữ liệu OpenAssistant Conversations được xây dựng bằng cách lọc ra các cây hội thoại xúc phạm và không phù hợp.

--- TRANG 6 ---

Loại Hướng dẫn | Ví dụ
---|---
Q&A Mở | Tại sao mọi người thích phim hài?
Q&A Đóng | Giao phối ngoài hay giao phối trong có lợi cho con cái hơn?
Trích xuất Thông tin | John Moses Browning là ai?
Tóm tắt Thông tin | Vui lòng tóm tắt Linkedin làm gì.
Động não | Đưa cho tôi một số ý tưởng để quản lý sếp của tôi.
Phân loại | Xác định loài động vật nào còn sống hoặc đã tuyệt chủng: Palaeophis, Rùa Khổng lồ
Viết sáng tạo | Viết một truyện ngắn về một người phát hiện ra một căn phòng ẩn trong nhà của họ.

Bảng 1: Các ví dụ về hướng dẫn trong Dolly V1 (Conover et al., 2023a).

3.2 Dữ liệu Tổng hợp qua Chưng cất

Dữ liệu tổng hợp được tạo ra thông qua các mô hình đã được tiền đào tạo, thay vì được lấy trực tiếp từ internet hoặc được chú thích bởi các chú thích viên con người. So với dữ liệu điều chỉnh hướng dẫn được chú thích thủ công, dữ liệu tổng hợp thường có hai ưu điểm: (1) Tạo dữ liệu tổng hợp cụ thể cho nhiệm vụ nhanh hơn và hiệu quả về chi phí hơn so với tạo dữ liệu điều chỉnh hướng dẫn được chú thích thủ công; (2) Chất lượng và tính đa dạng của dữ liệu tổng hợp vượt qua những gì các chú thích viên con người có thể tạo ra, dẫn đến hiệu suất tinh chỉnh được nâng cao và LLMs tổng quát hóa rộng hơn.

Dưới đây, chúng tôi đầu tiên tập trung vào phương pháp dữ liệu tổng hợp được sử dụng rộng rãi: Chưng cất, và trong Phần 3.3 chúng tôi tiếp tục với phương pháp dữ liệu tổng hợp khác: Tự cải thiện.

Thông thường, chưng cất bao gồm việc truyền đạt kiến thức và khả năng nhận thức từ một mô hình giáo viên có khả năng cao đến một mô hình học sinh ít phức tạp hơn, nhưng hiệu quả hơn về mặt tính toán, với mục tiêu nâng cao cả chất lượng phản hồi và hiệu quả tính toán. Trong bối cảnh tạo dữ liệu tổng hợp, quá trình này bao gồm thu thập các truy vấn từ các LLMs được tinh chỉnh (ví dụ: ChatGPT (OpenAI, 2022)) và sử dụng các truy vấn này làm cơ sở để tinh chỉnh các LLMs tiếp theo. Các minh họa được hiển thị trong Hình 5, nơi Taori et al. (2023a) đang cố gắng chuyển giao kiến thức mạnh mẽ của GPT-3 (Brown et al., 2020a) đến một mô hình ngôn ngữ nhỏ hơn LLaMA-7B (Touvron et al., 2023a).

Với khả năng của chưng cất trong việc bắt chước hiệu suất của các LLMs mạnh mẽ hiện có, ngày càng nhiều nhà nghiên cứu tập trung vào việc khám phá các truy vấn phức tạp hơn để khai thác khả năng của các LLMs hiện tại, chẳng hạn như:

Alpaca. Alpaca (Taori et al., 2023a), một chuỗi LLMs được giới thiệu bởi nhóm Stanford NLP, nổi bật vì ứng dụng chưng cất của nó. Cụ thể, bằng cách được tinh chỉnh trên 52K mảnh dữ liệu chưng cất được tạo bởi GPT-3 (Brown et al., 2020a), mô hình LLaMA-7B (Touvron et al., 2023a) nhỏ hơn đạt được hiệu suất tương đương hoặc thậm chí vượt qua GPT-3 (Brown et al., 2020a).

WizardLM / Evol-Instruct. Thay vì truy vấn đơn giản từ mô hình dòng GPT, WizardLM (Xu et al., 2023a) tập trung vào cách thu được các hướng dẫn và phản hồi đa dạng và chất lượng cao từ GPT-3 (Brown et al., 2020a). Để thực hiện điều này, WizardLM (Xu et al., 2023a) đầu tiên xây dựng một hệ thống năm cấp của các gợi ý truy vấn, tăng dần độ phức tạp của việc tạo dữ liệu. Sau đó, WizardLM (Xu et al., 2023a) mở rộng phạm vi chủ đề gợi ý truy vấn thông qua việc mở rộng thủ công, do đó tăng cường tính đa dạng của dữ liệu được tạo ra. Cuối cùng, bằng cách tinh chỉnh LLM mã nguồn mở LLaMA (Touvron et al., 2023b), WizardLM (Xu et al., 2023a) đạt được hơn 90% năng lực của ChatGPT (OpenAI, 2022) trên 17 trong số 29 kỹ năng.

Orca và Orca-2. Orca (Mukherjee et al., 2023) và Orca-2 (Mitra et al., 2023) đại diện cho hai tập dữ liệu chưng cất mở rộng được thiết kế để hướng dẫn các mô hình ngôn ngữ nhỏ hơn trong lập luận logic. Orca (Mukherjee et al., 2023), chẳng hạn, bao gồm vô số chỉ thị lập luận, chẳng hạn như "hãy suy nghĩ từng bước" và "biện minh phản hồi của bạn," để minh họa các con đường lập luận của LLMs (ví dụ: ChatGPT (OpenAI, 2022)) trong việc tạo ra câu trả lời của họ. Dựa trên khái niệm này, Orca (Mukherjee et al., 2023) biên soạn 1M phản hồi từ GPT-4 (OpenAI, 2023), trong khi Orca-2 (Mitra et al., 2023) tiếp tục tích lũy 817K phản hồi từ GPT-4 (OpenAI, 2023). Bộ sưu tập rộng lớn này tạo điều kiện cho việc tinh chỉnh các mô hình ngôn ngữ nhỏ hơn, cho phép chúng đạt được hoặc thậm chí vượt qua hiệu suất của các mô hình lớn gấp 5 đến 10 lần.

Baize. Baize (Conover et al., 2023b) là một kho văn bản tiếng Anh cho các cuộc hội thoại nhiều lượt, bao gồm 111.5K thể hiện, được tạo ra với ChatGPT. Mỗi cuộc trao đổi bao gồm một gợi ý từ người dùng và một phản hồi từ trợ lý. Để tạo ra tập dữ liệu Baize, các tác giả đề xuất tự trò chuyện, nơi ChatGPT đóng vai trò của người dùng và trợ lý AI theo lượt và tạo ra các tin nhắn trong định dạng hội thoại. Cụ thể, các tác giả đầu tiên tạo ra một mẫu nhiệm vụ xác định vai trò và nhiệm vụ cho ChatGPT (như được hiển thị trong Bảng 2). Tiếp theo, họ lấy mẫu các câu hỏi (ví dụ: "Làm thế nào để sửa tài khoản Google Play Store không hoạt động?") từ các tập dữ liệu Quora và Stack Overflow làm hạt giống hội thoại (ví dụ: chủ đề). Sau đó, họ gợi ý ChatGPT với mẫu và hạt giống được lấy mẫu. ChatGPT liên tục tạo ra tin nhắn cho cả hai bên cho đến khi đạt được điểm dừng tự nhiên.

Các Tập dữ liệu Chưng cất Cụ thể cho Nhiệm vụ. Ngoài các tập dữ liệu trên, có nhiều tập dữ liệu trong lĩnh vực chung, chẳng hạn như: ShareGPT2, WildChat (Zhao et al., 2024), Vicuna (Zheng et al., 2024), Unnatural Instructions (Honovich et al., 2022). Ngoài ra, có những nỗ lực nhằm sử dụng chưng cất để tạo ra các tập dữ liệu cụ thể cho nhiệm vụ bắt chước các năng lực của LLMs trong các lĩnh vực cụ thể. Ví dụ, đối với tạo mã, có WizardCoder (Luo et al., 2023), Magicoder (Wei et al., 2023b) và WaveCoder (Yu et al., 2023), đối với lập luận và viết, có Phi-1 (Gunasekar et al., 2023) và Phi-1.5 (Li et al., 2023i), và đối với xếp hạng, có Nectar (Zhu et al., 2023a).

3.3 Dữ liệu Tổng hợp qua Tự cải thiện

Khái niệm tự cải thiện được Wang et al. (2022c) tiếp tục: cải thiện khả năng tuân theo hướng dẫn của một LLM đã được tiền đào tạo (không được tinh chỉnh) (ví dụ: GPT-3 gốc (Brown et al., 2020b)) bằng cách bootstrap từ các thế hệ của chính nó. Hình 6 minh họa toàn bộ quá trình tự cải thiện với bốn bước:

Bước 1: Wang et al. (2022c) bắt đầu bằng cách thu thập thủ công 175 nhiệm vụ viết bởi con người, mỗi nhiệm vụ bao gồm một hướng dẫn và một phản hồi mong đợi, sau đó được thêm vào nhóm nhiệm vụ làm dữ liệu hạt giống.

Bước 2: Để tạo hướng dẫn, Wang et al. (2022c) lấy mẫu ngẫu nhiên 8 hướng dẫn hạt giống từ nhóm nhiệm vụ đã xây dựng để phục vụ như một gợi ý few-shot, hướng dẫn GPT-3 gốc tạo ra các hướng dẫn mới thông qua học trong ngữ cảnh.

Bước 3: Đối với mỗi hướng dẫn được tạo ra, nếu hướng dẫn là nhiệm vụ đầu ra trước (ví dụ: Viết), GPT-3 gốc sẽ trực tiếp tạo ra phản hồi tương ứng. Ngược lại, nếu hướng dẫn liên quan đến nhiệm vụ đầu vào trước (ví dụ: Đọc hiểu), GPT-3 gốc sẽ đầu tiên tạo ra ngữ cảnh cần thiết làm đầu vào trước khi tạo ra phản hồi tương ứng.

Bước 4: Các ví dụ định dạng (hướng dẫn, phản hồi) được tạo ra được lọc theo một loạt quy tắc hoặc mô hình.

Theo quy trình trên, Wang et al. (2022c) thu thập các tập dữ liệu Self-Instruct bao gồm 52K hướng dẫn, và đánh giá thêm cho thấy GPT-3 (Brown et al., 2020a) với Self-Instruct vượt trội hơn các tập dữ liệu của các đối tác với biên độ lớn, chỉ để lại khoảng cách tuyệt đối 5% so với InstructGPT (Ouyang et al., 2022).

Quá trình tự cải thiện được phác thảo dựa vào việc tạo dữ liệu tổng hợp trực tiếp từ chính mô hình, đòi hỏi một LLM mạnh mẽ làm xương sống nền tảng. Không có LLM mạnh mẽ, chu kỳ tự cải thiện này có thể hạn chế việc học vào các khả năng ban đầu của mô hình và có thể khuếch đại bất kỳ thiên kiến và lỗi nào hiện có. Mặc dù có những rủi ro này, vẫn có công việc hiệu quả trong lĩnh vực tự cải thiện:

3.3.1 SPIN

SPIN (Chen et al., 2024b), viết tắt của Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models, đại diện cho một phương pháp tự cải thiện chuyên biệt tập trung xung quanh cơ chế tự chơi. Trong thiết lập này, người tham gia chính (mô hình ngôn ngữ) trải qua tinh chỉnh để phân biệt các phản hồi từ người tham gia đối lập (mô hình ngôn ngữ từ lần lặp trước) và phân phối dữ liệu mong muốn. Quá trình này lặp đi lặp lại điều chỉnh mô hình ngôn ngữ để khớp chặt chẽ với phân phối dữ liệu mục tiêu.

Cụ thể, hãy tưởng tượng một lần lặp hiện tại của LLM là pθt, được sử dụng để tạo ra phản hồi y′ cho một gợi ý x đã cho từ tập dữ liệu với các hướng dẫn có nhãn của con người. Mục tiêu sau đó trở thành phát triển một LLM mới pθt+1 có khả năng phân biệt giữa y′, phản hồi được tạo bởi, và y, phản hồi được tạo bởi con người. Động lực này giống như một trò chơi hai người chơi trong đó người chơi chính, LLM mới hơn pθt+1 nhằm xác định sự khác biệt giữa các phản hồi của đối thủ pθt và những phản hồi được tạo bởi con người. Ngược lại, đối thủ, hoặc LLM cũ hơn pθt cố gắng tạo ra các phản hồi bắt chước chặt chẽ những phản hồi được tìm thấy trong tập dữ liệu điều chỉnh hướng dẫn có nhãn của con người. Bằng cách tinh chỉnh pθt cũ hơn để ưu tiên các phản hồi giống con người hơn các phản hồi của chính nó, một LLM mới pθt+1 được tạo ra, phù hợp chặt chẽ hơn với phân phối dữ liệu có nhãn của con người. Trong các lần lặp tiếp theo, LLM được cải thiện mới này pθt+1 đảm nhận vai trò đối thủ trong việc tạo phản hồi. Mục tiêu cuối cùng của cơ chế tự chơi này là để LLM phát triển cho đến khi nó đạt đến điểm mà pθ∗=phuman, giai đoạn mà phiên bản LLM tiên tiến nhất không thể phân biệt giữa các phản hồi được tạo bởi người tiền nhiệm và những phản hồi được tạo bởi con người. SPIN (Chen et al., 2024b) phục vụ như một phương pháp tự cải thiện biến thể cho phép các mô hình ngôn ngữ tự cải thiện mà không cần dữ liệu con người bổ sung hoặc phản hồi từ các mô hình ngôn ngữ mạnh mẽ hơn. Kết quả thực nghiệm cho thấy SPIN (Chen et al., 2024b) nâng cao đáng kể hiệu suất của các mô hình ngôn ngữ trên một loạt điểm chuẩn, vượt trội thậm chí những mô hình được đào tạo sử dụng dữ liệu con người bổ sung hoặc phản hồi từ các hệ thống AI bên ngoài.

3.3.2 Instruction Back-translation

Instruction back-translation (Li et al., 2023g), viết tắt của Self Alignment with Instruction Backtranslation, là một phương pháp chuyên biệt khác dựa trên tự cải thiện. Trái ngược với phương pháp của Wang et al. (2022c), bao gồm việc tạo phản hồi cho các hướng dẫn do con người cung cấp, Li et al. (2023g) áp dụng chiến lược ngược bằng cách tạo hướng dẫn cho các văn bản do con người thu thập được tìm thấy trực tuyến. Để đạt được mục tiêu này, Li et al. (2023g) tuân theo quy trình năm bước:

Bước 1: Thu thập (1) văn bản không có nhãn từ Clueweb (Overwijk et al., 2022), dưới giả định rằng các văn bản này có thể được liên kết với các hướng dẫn chất lượng cao, và (2) 3,200 mảnh dữ liệu định dạng (hướng dẫn, phản hồi) viết bởi con người để phục vụ làm dữ liệu hạt giống.

Bước 2: Một mô hình dịch ngược, có xương sống bởi LLaMA (Touvron et al., 2023b), được đào tạo trên dữ liệu hạt giống đã thu thập, lấy phản hồi làm đầu vào và tạo ra hướng dẫn làm đầu ra. Mô hình này sau đó được sử dụng để rút ra các hướng dẫn từ các văn bản không có nhãn đã thu thập.

Bước 3: Các văn bản không có nhãn đã thu thập được đưa vào mô hình dịch ngược đã được đào tạo, dẫn đến lượng lớn dữ liệu định dạng (hướng dẫn, phản hồi) thô.

Bước 4: Một mô hình đánh giá, có xương sống bởi LLaMA (Touvron et al., 2023b), được đào tạo trên dữ liệu hạt giống đã thu thập. Mô hình này xử lý hướng dẫn làm đầu vào và tạo ra phản hồi tương ứng làm đầu ra, sau đó được sử dụng để đánh giá mỗi cặp (hướng dẫn, phản hồi) được chú thích trong bước 3.

Bước 5: Lọc các cặp (hướng dẫn, phản hồi) chất lượng thấp, và sử dụng dữ liệu còn lại để tinh chỉnh LLMs.

Theo năm bước được phác thảo, Li et al. (2023g) tạo ra 502K mảnh dữ liệu tổng hợp. Mô hình LLaMA (Touvron et al., 2023b), được tinh chỉnh với tập dữ liệu được chú thích này, vượt qua tất cả các mô hình dựa trên LLaMA khác trên bảng xếp hạng Alpaca mà không phụ thuộc vào dữ liệu chưng cất, thể hiện một quá trình tự cải thiện hiệu quả cao.

4 LLMs Được Điều chỉnh Hướng dẫn

Trong phần này, chúng tôi chi tiết các mô hình LLM được sử dụng rộng rãi trong cộng đồng được đào tạo thông qua điều chỉnh hướng dẫn.

4.1 InstructionGPT

InstructGPT (176B) (Ouyang et al., 2022) được khởi tạo với GPT-3 (176B) (Brown et al., 2020b) và sau đó được tinh chỉnh trên các hướng dẫn của con người. Quy trình tinh chỉnh bao gồm ba bước sau: (1) tinh chỉnh có giám sát (SFT) trên tập dữ liệu hướng dẫn được lọc bởi con người, được thu thập từ hồ sơ lịch sử API Playground; (2) đào tạo một mô hình phần thưởng để dự đoán sở thích của con người dựa trên tập dữ liệu được chú thích, được xây dựng thông qua lao động con người bằng cách lấy mẫu nhiều phản hồi cho một hướng dẫn và xếp hạng chúng từ tốt nhất đến tệ nhất; (3) tối ưu hóa thêm mô hình từ Bước 1 với các hướng dẫn mới và mô hình phần thưởng đã được đào tạo trong bước (2). Các tham số được cập nhật sử dụng phương pháp tối ưu hóa chính sách gần đúng (PPO) (Schulman et al., 2017), một phương pháp học tăng cường gradient chính sách. Các bước (2) và (3) được thay phiên nhiều lần cho đến khi hiệu suất mô hình không cải thiện đáng kể.

Nhìn chung, InstructGPT vượt trội hơn GPT-3. Đối với đánh giá tự động, InstructGPT vượt trội hơn GPT-3 10% trên tập dữ liệu TruthfulQA (Lin et al., 2021) về tính trung thực và 7% trên RealToxicityPrompts (Gehman et al., 2020) về độc tính. Trên các tập dữ liệu NLP (tức là WSC), InstructGPT đạt được hiệu suất tương đương với GPT-3. Đối với đánh giá của con người, về bốn khía cạnh khác nhau, bao gồm tuân theo hướng dẫn chính xác, tuân theo ràng buộc rõ ràng, ít ảo giác hơn và tạo ra phản hồi phù hợp, InstructGPT vượt trội hơn GPT-3 +10%, +20%, -20%, và +10%, tương ứng.

4.2 BLOOMZ

BLOOMZ (176B) (Muennighoff et al., 2022) được khởi tạo với BLOOM (176B) (Scao et al., 2022), và sau đó được tinh chỉnh trên tập dữ liệu hướng dẫn xP3 (Muennighoff et al., 2022), một bộ sưu tập các tập dữ liệu hướng dẫn của con người trong 46 ngôn ngữ, đến từ hai nguồn: (1) P3, là một bộ sưu tập các cặp (hướng dẫn tiếng Anh, phản hồi tiếng Anh); và (2) một tập (hướng dẫn tiếng Anh, phản hồi đa ngôn ngữ) được chuyển đổi từ các tập dữ liệu NLP đa ngôn ngữ (ví dụ: điểm chuẩn tiếng Trung) bằng cách điền các mẫu nhiệm vụ với các hướng dẫn tiếng Anh được xác định trước.

Đối với đánh giá tự động, BLOOMZ hoạt động tốt hơn BLOOM trong thiết lập zero-shot lần lượt +10.4%, 20.5%, và 9.8% trên các tập dữ liệu giải quyết tham chiếu, hoàn thành câu và suy luận ngôn ngữ tự nhiên. Đối với điểm chuẩn HumanEval (Chen et al., 2021b), BLOOMZ vượt trội hơn BLOOM 10% về chỉ số Pass@100. Đối với các nhiệm vụ tạo sinh, BLOOMZ nhận được cải thiện BLEU +9% so với BLOOM trên điểm chuẩn lm-evaluation-harness.

4.3 Flan-T5

Flan-T5 (11B) là một mô hình ngôn ngữ lớn được khởi tạo với T5 (11B) (Raffel et al., 2019), và sau đó được tinh chỉnh trên tập dữ liệu FLAN (Longpre et al., 2023). Tập dữ liệu FLAN là một bộ sưu tập các cặp (hướng dẫn, cặp) được xây dựng từ 62 tập dữ liệu của 12 nhiệm vụ NLP (ví dụ: suy luận ngôn ngữ tự nhiên, lập luận thông thường, tạo ra cách diễn đạt) bằng cách điền các mẫu với các hướng dẫn khác nhau dưới một định dạng nhiệm vụ thống nhất.

Trong quá trình tinh chỉnh, FLAN-T5 áp dụng khung JAX-based T5X và chọn mô hình tốt nhất được đánh giá trên các nhiệm vụ giữ lại mỗi 2k bước. So với giai đoạn tiền đào tạo của T5, tinh chỉnh tốn 0.2% tài nguyên tính toán (khoảng 128 chip TPU v4 trong 37 giờ).

Đối với đánh giá, FLAN-T5 (11B) vượt trội hơn T5 (11B), và đạt được kết quả tương đương với các mô hình lớn hơn, bao gồm PaLM (60B) (Chowdhery et al., 2022) trong thiết lập few-shot. FLAN-T5 vượt trội hơn T5 lần lượt +18.9%, +12.3%, +4.1%, +5.8%, +2.1%, và +8% trên MMLU (Hendrycks et al., 2020b), BBH (Suzgun et al., 2022b), TyDiQA (Clark et al., 2020), MGSM (Shi et al., 2022), tạo sinh mở và RealToxicityPrompts (Gehman et al., 2020). Trong thiết lập few-shot, FLAN-T5 vượt trội hơn PaLM +1.4% và +1.2% trên các tập dữ liệu BBH và TyDiQA.

4.4 Alpaca

Alpaca (7B) (Taori et al., 2023a) là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh LLaMA (7B) (Touvron et al., 2023a) trên tập dữ liệu hướng dẫn đã xây dựng được tạo bởi InstructGPT (175B, text-davinci-003) (Ouyang et al., 2022). Quá trình tinh chỉnh mất khoảng 3 giờ trên thiết bị 8-card 80GB A100 với đào tạo độ chính xác hỗn hợp và song song dữ liệu được chia sẻ đầy đủ.

Alpaca (7B) đạt được hiệu suất tương đương với InstructGPT (175B, text-davinci-003) về đánh giá của con người. Cụ thể, Alpaca vượt trội hơn InstructGPT trên tập dữ liệu self-instruct, giành được 90 chiến thắng so với 89 chiến thắng.

4.5 Vicuna

Vicuna (13B) (Chiang et al., 2023) là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh LLaMA (13B) (Touvron et al., 2023a) trên tập dữ liệu hội thoại được tạo bởi ChatGPT.

Các tác giả thu thập các cuộc hội thoại ChatGPT được chia sẻ bởi người dùng từ ShareGPT.com, và nhận được 70K bản ghi hội thoại sau khi lọc ra các mẫu chất lượng thấp. LLaMA (13B) được tinh chỉnh trên tập dữ liệu hội thoại đã xây dựng sử dụng hàm mất mát được sửa đổi phù hợp với các cuộc hội thoại nhiều lượt. Để hiểu rõ hơn về ngữ cảnh dài qua đối thoại nhiều lượt, các tác giả mở rộng độ dài ngữ cảnh tối đa từ 512 lên 2048. Đối với đào tạo, các tác giả áp dụng các kỹ thuật gradient checkpointing và flash attention (Dao et al., 2022) để giảm chi phí bộ nhớ GPU trong quá trình tinh chỉnh. Quá trình tinh chỉnh mất 24 giờ trên thiết bị 8×80GB A100 với song song dữ liệu được chia sẻ đầy đủ.

Các tác giả xây dựng một tập kiểm tra được sử dụng độc quyền để đo hiệu suất của chatbots. Họ thu thập một tập kiểm tra bao gồm 8 danh mục câu hỏi, chẳng hạn như bài toán Fermi, kịch bản nhập vai, nhiệm vụ mã hóa/toán học, v.v., và sau đó yêu cầu GPT-4 (OpenAI, 2023) đánh giá phản hồi của các mô hình xem xét tính hữu ích, liên quan, chính xác và chi tiết. Trên tập kiểm tra đã xây dựng, Vicuna (13B) vượt trội hơn Alpaca (13B) (Taori et al., 2023a) và LLaMA (13B) trong 90% câu hỏi kiểm tra, và tạo ra phản hồi có đánh giá bằng hoặc tốt hơn so với ChatGPT trong 45% câu hỏi.

4.6 GPT-4-LLM

GPT-4-LLM (7B) (Peng et al., 2023) là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh LLaMA (7B) (Touvron et al., 2023a) trên tập dữ liệu hướng dẫn được tạo bởi GPT-4 (OpenAI, 2023). GPT-4-LLM được khởi tạo với LLaMA, sau đó được tinh chỉnh trong hai bước sau: (1) tinh chỉnh có giám sát trên tập dữ liệu hướng dẫn đã xây dựng. Các tác giả sử dụng các hướng dẫn từ Alpaca (Taori et al., 2023a), và sau đó thu thập phản hồi sử dụng GPT-4. LLaMA được tinh chỉnh trên tập dữ liệu được tạo bởi GPT-4. Quá trình tinh chỉnh mất khoảng ba giờ trên máy 8*80GB A100 với độ chính xác hỗn hợp và song song dữ liệu được chia sẻ đầy đủ. (2) tối ưu hóa mô hình bước-1 sử dụng phương pháp tối ưu hóa chính sách gần đúng (PPO) (Schulman et al., 2017), các tác giả đầu tiên xây dựng một tập dữ liệu so sánh bằng cách thu thập phản hồi từ GPT-4, InstructGPT (Ouyang et al., 2022), và OPT-IML (Iyer et al., 2022) cho một bộ sưu tập hướng dẫn và sau đó yêu cầu GPT-4 đánh giá mỗi phản hồi từ 1 đến 10. Sử dụng các đánh giá, một mô hình phần thưởng được đào tạo dựa trên OPT (Zhang et al., 2022a). Mô hình được tinh chỉnh từ Bước 1 được tối ưu hóa bằng cách sử dụng mô hình phần thưởng để tính toán gradient chính sách.

Đối với đánh giá, GPT-4-LLM (7B) vượt trội không chỉ mô hình cơ sở Alpaca (7B), mà còn các mô hình lớn hơn bao gồm Alpaca (13B) và LLAMA (13B). Đối với đánh giá tự động, GPT-4-LLM (7B) vượt trội hơn Alpaca lần lượt 0.2, 0.5, và 0.7 trên các tập dữ liệu User-Oriented-Instructions-252 (Wang et al., 2022c), Vicuna-Instructions (Chiang et al., 2023), và Unnatural Instructions (Honovich et al., 2022). Đối với đánh giá của con người, về các khía cạnh bao gồm tính hữu ích, trung thực và vô hại, GPT-4-LLM vượt trội hơn Alpaca lần lượt 11.7, 20.9, và 28.6.

4.7 Claude

Claude là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh mô hình ngôn ngữ đã được tiền đào tạo trên tập dữ liệu hướng dẫn, nhằm tạo ra các phản hồi hữu ích và vô hại. Quá trình tinh chỉnh bao gồm hai giai đoạn: (1) tinh chỉnh có giám sát trên tập dữ liệu hướng dẫn. Các tác giả tạo ra một tập dữ liệu hướng dẫn bằng cách thu thập 52K hướng dẫn khác nhau, được ghép nối với các phản hồi được tạo bởi GPT-4. Quá trình tinh chỉnh mất khoảng tám giờ trên máy 8-card 80GB A100 với độ chính xác hỗn hợp và song song dữ liệu được chia sẻ đầy đủ. (2) tối ưu hóa mô hình bước-1 với phương pháp tối ưu hóa chính sách gần đúng (Schulman et al., 2017). Các tác giả đầu tiên xây dựng một tập dữ liệu so sánh bằng cách thu thập phản hồi từ nhiều mô hình ngôn ngữ lớn (ví dụ: GPT-3 (Brown et al., 2020b)) cho bộ sưu tập hướng dẫn đã cho và sau đó yêu cầu GPT-4 (OpenAI, 2023) đánh giá mỗi phản hồi. Sử dụng các đánh giá, một mô hình phần thưởng được đào tạo. Sau đó, mô hình được tinh chỉnh từ Bước 1 được tối ưu hóa sử dụng mô hình phần thưởng với phương pháp tối ưu hóa chính sách gần đúng.

Claude tạo ra các phản hồi hữu ích và vô hại hơn so với mô hình xương sống. Đối với đánh giá tự động, Claude vượt trội hơn GPT-3 7% trên RealToxicityPrompts (Gehman et al., 2020) về độc tính. Đối với đánh giá của con người, về bốn khía cạnh khác nhau, bao gồm tuân theo hướng dẫn chính xác, tuân theo ràng buộc rõ ràng, ít ảo giác hơn và tạo ra phản hồi phù hợp, Claude vượt trội hơn GPT-3 (Brown et al., 2020b) lần lượt +10%, +20%, -20%, và +10%.

4.8 WizardLM

WizardLM (7B) (Xu et al., 2023a) là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh LLaMA (7B) (Touvron et al., 2023a) trên tập dữ liệu hướng dẫn Evol-Instruct được tạo bởi ChatGPT (chi tiết xem Phần 3.2). Nó được tinh chỉnh trên một tập con (với 70K) của Evol-Instruct để cho phép so sánh công bằng với Vicuna (Chiang et al., 2023). Quá trình tinh chỉnh mất khoảng 70 giờ trên 3 epochs dựa trên 8 GPU V100 với kỹ thuật Deepspeed Zero-3 (Rasley et al., 2020). Trong quá trình suy luận, độ dài tạo tối đa là 2048.

Để đánh giá hiệu suất của LLMs trên các hướng dẫn phức tạp, các tác giả thu thập 218 hướng dẫn do con người tạo ra từ các kịch bản thực tế (ví dụ: dự án mã nguồn mở, nền tảng và diễn đàn), được gọi là tập kiểm tra Evol-Instruct.

Đánh giá được thực hiện trên tập kiểm tra Evol-Instruct và tập kiểm tra của Vicuna. Đối với đánh giá của con người, WizardLM vượt trội hơn Alpaca (7B) (Taori et al., 2023a) và Vicuna (7B) với biên độ lớn, và tạo ra phản hồi bằng hoặc tốt hơn trên 67% mẫu kiểm tra so với ChatGPT. Đánh giá tự động được thực hiện bằng cách yêu cầu GPT-4 đánh giá phản hồi của LLMs. Cụ thể, WizardLM nhận được cải thiện hiệu suất so với Alpaca lần lượt +6.2%, +5.3% trên tập kiểm tra Evol-Instruct và tập kiểm tra của Vicuna. WizardLM đạt được vượt trội hơn Vicuna +5.8 trên tập kiểm tra Evol-Instruct và +1.7% trên tập kiểm tra của Vicuna.

4.9 ChatGLM2

ChatGLM2 (6B) (Du et al., 2022) là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh GLM (6B) (Du et al., 2022) trên tập dữ liệu song ngữ chứa cả hướng dẫn tiếng Anh và tiếng Trung. Tập dữ liệu hướng dẫn song ngữ chứa 1.4T tokens, với tỷ lệ 1:1 giữa tiếng Trung và tiếng Anh. Các hướng dẫn trong tập dữ liệu được lấy mẫu từ các nhiệm vụ trả lời câu hỏi và hoàn thành đối thoại. ChatGLM được khởi tạo với GLM, sau đó được đào tạo bằng chiến lược tinh chỉnh ba bước, tương tự như InstructGPT (Ouyang et al., 2022). Để mô hình hóa thông tin ngữ cảnh tốt hơn qua các cuộc hội thoại nhiều lượt, các tác giả mở rộng độ dài ngữ cảnh tối đa từ 1024 lên 32K. Để giảm chi phí bộ nhớ GPU trong giai đoạn tinh chỉnh, các tác giả sử dụng các chiến lược multi-query attention và causal mask. Trong quá trình suy luận, ChatGLM2 yêu cầu 13GB bộ nhớ GPU với FP16 và hỗ trợ các cuộc hội thoại lên đến 8K độ dài với 6GB bộ nhớ GPU sử dụng kỹ thuật quantization mô hình INT4.

Đánh giá được thực hiện trên bốn điểm chuẩn tiếng Anh và tiếng Trung, bao gồm MMLU (tiếng Anh) (Hendrycks et al., 2020b), C-Eval (tiếng Trung) (Huang et al., 2023), GSM8K (Toán) (Cobbe et al., 2021), và BBH (tiếng Anh) (Suzgun et al., 2022b). ChatGLM2 (6B) vượt trội hơn GLM (6B) và mô hình cơ sở ChatGLM (6B) trên tất cả các điểm chuẩn. Cụ thể, ChatGLM2 vượt trội hơn GLM lần lượt +3.1 trên MMLU, +5.0 trên C-Eval, +8.6 trên GSM8K, và +2.2 trên BBH. ChatGLM2 đạt được hiệu suất tốt hơn so với ChatGLM lần lượt +2.1, +1.2, +0.4, +0.8 trên MMLU, C-Eval, GSM8K và BBH.

4.10 LIMA

LIMA (65B) (Zhou et al., 2023a) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh LLaMA (65B) (Touvron et al., 2023a) trên tập dữ liệu hướng dẫn, được xây dựng dựa trên giả thuyết căn chỉnh bề mặt được đề xuất.

Giả thuyết căn chỉnh bề mặt đề cập đến ý tưởng rằng kiến thức và khả năng của mô hình gần như được thu thập trong giai đoạn tiền đào tạo, trong khi đào tạo căn chỉnh (ví dụ: điều chỉnh hướng dẫn) dạy các mô hình tạo ra phản hồi dưới các định dạng được người dùng ưa thích. Dựa trên giả thuyết căn chỉnh bề mặt, các tác giả tuyên bố rằng các mô hình ngôn ngữ lớn có thể tạo ra phản hồi được người dùng hài lòng bằng cách tinh chỉnh nó trên một phần nhỏ dữ liệu hướng dẫn. Do đó, các tác giả xây dựng các tập huấn luyện/hợp lệ/kiểm tra hướng dẫn để xác minh giả thuyết này.

Đánh giá được thực hiện trên tập kiểm tra đã xây dựng. Đối với đánh giá của con người, LIMA vượt trội hơn InstructGPT và Alpaca lần lượt 17% và 19%. Ngoài ra, LIMA đạt được kết quả tương đương với BARD, Claude, và GPT-4. Đối với đánh giá tự động, được thực hiện bằng cách yêu cầu GPT-4 đánh giá phản hồi và điểm đánh giá cao hơn biểu thị hiệu suất tốt hơn, LIMA vượt trội hơn InstructGPT và Alpaca lần lượt 20% và 36%, đạt được kết quả tương đương với BARD, trong khi kém hiệu suất hơn Claude và GPT-4. Kết quả thực nghiệm xác minh giả thuyết căn chỉnh bề mặt được đề xuất.

4.11 Khác

OPT-IML (175B) (Iyer et al., 2022) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh mô hình OPT (175B) (Zhang et al., 2022a) trên tập dữ liệu Instruction Meta-Learning (IML) đã xây dựng, bao gồm hơn 1500 nhiệm vụ NLP từ 8 điểm chuẩn có sẵn công khai như PromptSource (Bach et al., 2022), FLAN (Longpre et al., 2023), và Super-NaturalInstructions (Wang et al., 2022e). Sau khi tinh chỉnh, OPT-IML vượt trội hơn OPT trên tất cả các điểm chuẩn.

Dolly 2.0 (12B) (Conover et al., 2023a) được khởi tạo với mô hình ngôn ngữ đã được tiền đào tạo Pythia (12B) (Biderman et al., 2023), và được tinh chỉnh trên tập dữ liệu hướng dẫn databricks-dolly-15k, chứa 7 danh mục nhiệm vụ NLP như phân loại văn bản và trích xuất thông tin. Sau khi tinh chỉnh, Dolly 2.0 (12B) vượt trội hơn Pythia (12B) trên điểm chuẩn EleutherAI LLM Evaluation Harness (Gao et al., 2021) với biên độ lớn, và đạt được hiệu suất tương đương với GPT-NEOX (20B) (Black et al., 2022), có gấp đôi tham số so với Dolly 2.0 (12B).

Falcon-Instruct (40B) (Almazrouei et al., 2023a) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh Falcon (40B) (Almazrouei et al., 2023b) trên tập dữ liệu đối thoại tiếng Anh, chứa 150 triệu tokens từ tập dữ liệu Baize (Xu et al., 2023c), với thêm 5% dữ liệu từ tập dữ liệu RefinedWeb (Penedo et al., 2023). Để giảm sử dụng bộ nhớ, các tác giả sử dụng các kỹ thuật flash attention (Dao et al., 2022) và multi-query. Đối với đánh giá, Falcon-Instruct (40B) đạt được hiệu suất tốt hơn trên Open LLM Leaderboard (Beeching et al., 2023) so với mô hình cơ sở Falcon (40B), và vượt trội hơn Guanaco (65B), có nhiều tham số mô hình hơn.

Guanaco (7B) (JosephusCheung, 2021) là một mô hình ngôn ngữ đối thoại nhiều lượt được đào tạo bằng cách tinh chỉnh LLaMA (7B) (Touvron et al., 2023a) trên tập dữ liệu đối thoại đa ngôn ngữ đã xây dựng. Tập dữ liệu đối thoại đa ngôn ngữ đến từ hai nguồn: Alpaca (Taori et al., 2023a), chứa 52K cặp dữ liệu hướng dẫn tiếng Anh; và dữ liệu đối thoại đa ngôn ngữ (ví dụ: tiếng Trung giản thể, tiếng Trung phồn thể, tiếng Nhật, tiếng Đức), chứa 534K+ cuộc hội thoại nhiều lượt. Sau khi tinh chỉnh, Guanaco có thể tạo ra phản hồi cụ thể theo vai trò và phản hồi liên tục về một chủ đề nhất định trong các cuộc hội thoại nhiều lượt.

Minotaur (15B) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh Starcoder Plus (15B) (Li et al., 2023f) trên các tập dữ liệu hướng dẫn mã nguồn mở bao gồm WizardLM (Xu et al., 2023a) và GPTeacher-General-Instruct. Đối với suy luận mô hình, Minotaur hỗ trợ độ dài ngữ cảnh tối đa là 18K tokens.

Nous-Hermes (13B) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh LLaMA (13B) (Touvron et al., 2023a) trên tập dữ liệu hướng dẫn, chứa hơn 300k hướng dẫn, được lấy mẫu từ GPTeacher, CodeAlpaca (Chaudhary, 2023), GPT-4-LLM (Peng et al., 2023), Unnatural Instructions (Honovich et al., 2022), và các tập con BiologyPhysicsChemistry trong Camel-AI (Li et al., 2023c). Phản hồi được tạo bởi GPT-4. Đối với đánh giá, Nous-Hermes (13B) đạt được hiệu suất tương đương với GPT-3.5-turbo trên nhiều nhiệm vụ như ARC challenge (Clark et al., 2018) và BoolQ (Clark et al., 2019).

TÜLU (6.7B) (Wang et al., 2023d) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh OPT (6.7B) (Zhang et al., 2022a) trên tập dữ liệu hướng dẫn hỗn hợp, chứa FLAN V2 (Longpre et al., 2023), CoT (Wei et al., 2022), Dolly (Conover et al., 2023a), Open Assistant-1, GPT4-Alpaca, Code-Alpaca (Chaudhary, 2023), và ShareGPT. Sau khi tinh chỉnh, TÜLU (6.7B) đạt trung bình 83% hiệu suất của ChatGPT và 68% hiệu suất của GPT-4.

YuLan-Chat (13B) (YuLan-Chat-Team, 2023) là một mô hình ngôn ngữ được đào tạo bằng cách tinh chỉnh LLaMA (13B) (Touvron et al., 2023a) trên tập dữ liệu song ngữ đã xây dựng, chứa 250,000 cặp hướng dẫn tiếng Trung-tiếng Anh. Sau khi tinh chỉnh, YuLan-Chat-13B đạt được kết quả tương đương với mô hình mã nguồn mở tiên tiến ChatGLM (6B) (Du et al., 2022), và vượt trội hơn Vicuna (13B) (Chiang et al., 2023) trên tập dữ liệu English BBH3K (BBH3K là một tập con của điểm chuẩn BBH (Srivastava et al., 2022a)).

MOSS (16B) là một mô hình ngôn ngữ đối thoại song ngữ, nhằm tham gia vào các cuộc hội thoại nhiều lượt và sử dụng các plugin khác nhau, được đào tạo bằng cách tinh chỉnh trên các hướng dẫn đối thoại. Sau khi tinh chỉnh, MOSS vượt trội hơn mô hình xương sống và tạo ra các phản hồi phù hợp tốt hơn với sở thích của con người.

Airoboros (13B) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh LLAMA (13B) (Touvron et al., 2023a) trên tập dữ liệu Self-instruct (Wang et al., 2022c). Sau khi tinh chỉnh, Airoboros vượt trội đáng kể hơn LLAMA (13B) (Touvron et al., 2023a) trên tất cả các điểm chuẩn và đạt được kết quả có tính so sánh cao với các mô hình được tinh chỉnh cụ thể cho các điểm chuẩn nhất định.

UltraLM (13B) (Ding et al., 2023a) là một mô hình ngôn ngữ lớn được đào tạo bằng cách tinh chỉnh LLAMA (13B) (Touvron et al., 2023a). Đối với đánh giá, UltraLM (13B) vượt trội hơn Dolly (12B) (Conover et al., 2023a) và đạt được tỷ lệ thắng lên đến 98%. Ngoài ra, nó vượt qua các mô hình mã nguồn mở tốt nhất trước đó (tức là Vicuna (Chiang et al., 2023) và WizardLM (Xu et al., 2023a)) với tỷ lệ thắng lần lượt là 9% và 28%.

5 Điều chỉnh Hướng dẫn Đa phương thức

5.1 Tập dữ liệu Đa phương thức

MULTIINSTRUCT (Xu et al., 2022) là một tập dữ liệu điều chỉnh hướng dẫn đa phương thức bao gồm 62 nhiệm vụ đa phương thức đa dạng trong định dạng seq-to-seq thống nhất. Tập dữ liệu này bao phủ 10 danh mục rộng và các nhiệm vụ của nó được bắt nguồn từ 21 tập dữ liệu mã nguồn mở hiện có. Mỗi nhiệm vụ được trang bị 5 hướng dẫn được viết bởi chuyên gia. Đối với các nhiệm vụ hiện có, các tác giả sử dụng các cặp đầu vào/đầu ra từ các tập dữ liệu mã nguồn mở có sẵn của họ để tạo ra các thể hiện. Trong khi đối với mỗi nhiệm vụ mới, các tác giả tạo ra từ 5k đến 5M thể hiện bằng cách trích xuất thông tin cần thiết từ các thể hiện của các nhiệm vụ hiện có hoặc tái cấu trúc chúng. Tập dữ liệu MULTIINSTRUCT đã chứng minh hiệu quả của nó trong việc nâng cao các kỹ thuật học chuyển giao khác nhau. Ví dụ, tinh chỉnh mô hình OFA (930M) (Wang et al., 2022a) sử dụng các chiến lược học chuyển giao khác nhau như Mixed Instruction Tuning và Sequential Instruction Tuning trên MULTIINSTRUCT cải thiện hiệu suất zero-shot trên tất cả các nhiệm vụ chưa thấy. Trên nhiệm vụ VQA thông thường, OFA được tinh chỉnh trên MULTIINSTRUCT đạt được 50.60 trên RougeL và 31.17 trên độ chính xác, trong khi OFA gốc đạt được 14.97 trên RougeL và 0.40 trên độ chính xác.

PMC-VQA (Zhang et al., 2023c) là một tập dữ liệu trả lời câu hỏi hình ảnh y tế quy mô lớn bao gồm 227k cặp hình ảnh-câu hỏi của 149k hình ảnh, bao phủ các phương thức hoặc bệnh khác nhau. Tập dữ liệu có thể được sử dụng cho cả nhiệm vụ mở và nhiều lựa chọn. Quy trình tạo ra tập dữ liệu PMC-VQA bao gồm thu thập các cặp hình ảnh-chú thích từ tập dữ liệu PMC-OA (Lin et al., 2023b), sử dụng ChatGPT để tạo ra các cặp câu hỏi-trả lời, và xác minh thủ công một tập con của tập dữ liệu để đảm bảo chất lượng. Các tác giả đề xuất mô hình dựa trên tạo sinh MedVInT cho hiểu biết hình ảnh y tế bằng cách căn chỉnh thông tin hình ảnh với mô hình ngôn ngữ lớn. MedVInT được tiền đào tạo trên PMC-VQA đạt được hiệu suất tiên tiến và vượt trội hơn các mô hình hiện có trên các điểm chuẩn VQA-RAD (Lau et al., 2018) và SLAKE (Liu et al., 2021a), với 81.6% độ chính xác trên VQA-RAD và 88.0% độ chính xác trên SLAKE.

LAMM (Yin et al., 2023) là một tập dữ liệu điều chỉnh hướng dẫn đa phương thức toàn diện cho hiểu biết hình ảnh 2D và đám mây điểm 3D. LAMM chứa 186K cặp phản hồi hướng dẫn ngôn ngữ-hình ảnh, và 10K cặp phản hồi hướng dẫn ngôn ngữ-đám mây điểm. Các tác giả thu thập hình ảnh và đám mây điểm từ các tập dữ liệu có sẵn công khai và sử dụng các phương pháp GPT-API và tự hướng dẫn để tạo ra các hướng dẫn và phản hồi dựa trên các nhãn gốc từ các tập dữ liệu này. LAMM-Dataset bao gồm các cặp dữ liệu cho trả lời câu hỏi kiến thức thông thường bằng cách kết hợp hệ thống nhãn đồ thị kiến thức phân cấp từ tập dữ liệu Bamboo (Zhang et al., 2022b) và mô tả Wikipedia tương ứng. Các tác giả cũng đề xuất LAMM-Benchmark, đánh giá các mô hình ngôn ngữ đa phương thức hiện có (MLLM) trên các nhiệm vụ thị giác máy tính khác nhau. Nó bao gồm 9 nhiệm vụ hình ảnh thông thường và 3 nhiệm vụ đám mây điểm thông thường, và LAMM-Framework, một khung đào tạo MLLM chính thức phân biệt các khối encoder, projector và LLM finetuning cho các phương thức khác nhau để tránh xung đột phương thức.

Vision-Flan (Xu et al., 2024) là tập dữ liệu điều chỉnh hướng dẫn hình ảnh lớn nhất có sẵn công khai do con người chú thích, bao gồm 1,664,261 thể hiện và hơn 200 nhiệm vụ ngôn ngữ-thị giác đa dạng được bắt nguồn từ 101 tập dữ liệu thị giác máy tính mã nguồn mở. Mỗi nhiệm vụ được đi kèm với các hướng dẫn được viết bởi chuyên gia và các mẫu được tạo tỉ mỉ cho đầu vào và đầu ra. Tập dữ liệu bao phủ một phổ rộng các nhiệm vụ, bao gồm chú thích hình ảnh, trả lời câu hỏi hình ảnh và hiểu biết hình ảnh. Được thiết kế để nâng cao nghiên cứu và ứng dụng trong các lĩnh vực mô hình ngôn ngữ-thị giác, Vision-Flan nhằm mở rộng các chân trời tương tác và hiểu biết giữa các phương thức hình ảnh và ngôn ngữ. Nó cung cấp cho các nhà nghiên cứu và phát triển một tài nguyên có giá trị để đẩy ranh giới của các mô hình ngôn ngữ-thị giác và đổi mới các thuật toán trên một loạt đa dạng các lĩnh vực.

ALLaVA (Chen et al., 2024a) đại diện cho một tập dữ liệu mã nguồn mở, mở rộng được thiết kế riêng cho việc tinh chỉnh các mô hình trả lời câu hỏi hình ảnh, có đặc điểm là 1.4M mục bao gồm chú thích chi tiết, hướng dẫn phức tạp và câu trả lời toàn diện được tạo bởi GPT-4V (Yang et al., 2023b). Để tạo ra chú thích chất lượng cao và câu trả lời câu hỏi hình ảnh, Chen et al. (2024a) giới thiệu phương pháp chưng cất cả chú thích và cặp QA cho một hình ảnh trong một tương tác duy nhất. Quá trình này bao gồm việc ban đầu trình bày GPT-4V (Yang et al., 2023b) với một hình ảnh, sau đó gợi ý nó tạo ra cả chú thích chi tiết và cặp trả lời câu hỏi hình ảnh. Phương pháp kết hợp dữ liệu hình ảnh bổ sung này cho phép mô hình phát triển hiểu biết tinh tế hơn về cả yếu tố hình ảnh và văn bản, nâng cao khả năng cung cấp câu trả lời chính xác và phù hợp với ngữ cảnh. Hơn nữa, phương pháp này có tiềm năng giảm sự xuất hiện của ảo giác bằng cách cung cấp cho mô hình nhiều thông tin ngữ cảnh hơn (dữ liệu hình ảnh).

ShareGPT4V (Chen et al., 2023a) là một bộ sưu tập các cặp hình ảnh-văn bản mô tả chi tiết, bao gồm hai thành phần: 100K chú thích được tạo bởi GPT4-Vision (Yang et al., 2023b) từ nhiều hình ảnh khác nhau, và 1.2M chú thích được phát triển sử dụng mô hình đã được tiền đào tạo của họ, được đào tạo trên tập 100K chú thích chất lượng cao ban đầu. Những chú thích này bao phủ toàn diện các khía cạnh như kiến thức toàn cầu, thuộc tính đối tượng, mối quan hệ không gian và đánh giá thẩm mỹ. Sử dụng tập dữ liệu này, mô hình ShareGPT4V-7B, sau khi được tinh chỉnh, vượt qua các LMMs quy mô 7B cạnh tranh trên tất cả 11 bài kiểm tra điểm chuẩn. Đáng chú ý, nó đạt được điểm tích lũy đáng kể là 1943.8 trên điểm chuẩn MME, vượt trội hơn mô hình Qwen-VL-Chat-7B (Bai et al., 2023) đứng thứ hai, được đào tạo với 1.4 tỷ mẫu, 95.6 điểm.

5.2 Mô hình Điều chỉnh Hướng dẫn Đa phương thức

InstructPix2Pix (983M) (Brooks et al., 2022) là một mô hình khuếch tán có điều kiện được đào tạo bằng cách tinh chỉnh Stable Diffusion (983M) (Rombach et al., 2022) trên tập dữ liệu đa phương thức đã xây dựng chứa hơn 450K hướng dẫn chỉnh sửa văn bản và hình ảnh tương ứng trước và sau khi chỉnh sửa. Các tác giả kết hợp khả năng của hai mô hình đã được tiền đào tạo quy mô lớn, một mô hình ngôn ngữ GPT-3 (Brown et al., 2020b) và một mô hình văn bản-thành-hình ảnh Stable Diffusion (Rombach et al., 2022), để tạo ra tập dữ liệu đào tạo. GPT-3 được tinh chỉnh để tạo ra các chỉnh sửa văn bản dựa trên các gợi ý hình ảnh, trong khi Stable Diffusion được sử dụng để chuyển đổi các chỉnh sửa văn bản được tạo ra thành các chỉnh sửa hình ảnh thực tế. InstructPix2Pix sau đó được đào tạo trên tập dữ liệu được tạo ra này sử dụng mục tiêu khuếch tán ẩn. Hình 7 hiển thị quá trình tạo ra tập dữ liệu chỉnh sửa hình ảnh và đào tạo mô hình khuếch tán trên tập dữ liệu đó. Các tác giả so sánh phương pháp được đề xuất một cách định tính với các công trình trước đó như SDEdit (Meng et al., 2022) và Text2Live (Bar-Tal et al., 2022), làm nổi bật khả năng của mô hình trong việc tuân theo hướng dẫn chỉnh sửa hình ảnh thay vì mô tả về hình ảnh hoặc lớp chỉnh sửa. Các tác giả cũng trình bày so sánh định lượng với SDEdit (Meng et al., 2022) sử dụng các chỉ số đo tính nhất quán hình ảnh và chất lượng chỉnh sửa.

LLaVA (13B) (Liu et al., 2023b) là một mô hình đa phương thức lớn được phát triển bằng cách kết nối bộ mã hóa hình ảnh của CLIP (400M) (Radford et al., 2021) với bộ giải mã ngôn ngữ LLaMA (7B) (Touvron et al., 2023a). LLaVA được tinh chỉnh sử dụng tập dữ liệu ngôn ngữ-thị giác hướng dẫn được tạo ra bao gồm 158K mẫu tuân theo hướng dẫn ngôn ngữ-hình ảnh độc đáo. Quá trình thu thập dữ liệu bao gồm tạo ra các gợi ý hội thoại, mô tả chi tiết và lập luận phức tạp. GPT-4 được sử dụng để chuyển đổi các cặp hình ảnh-văn bản thành định dạng tuân theo hướng dẫn phù hợp cho tập dữ liệu này. Các đặc điểm hình ảnh như chú thích và hộp giới hạn được sử dụng để mã hóa hình ảnh. LLaVA đạt được 85.1% điểm tương đối so với GPT-4 trên tập dữ liệu tuân theo hướng dẫn đa phương thức tổng hợp. Khi được tinh chỉnh trên Science QA, sự kết hợp của LLaVA và GPT-4 đạt được độ chính xác tiên tiến mới là 92.53%.

Video-LLaMA (Zhang et al., 2023b) là một khung đa phương thức nâng cao các mô hình ngôn ngữ lớn với khả năng hiểu cả nội dung hình ảnh và âm thanh trong video. Kiến trúc của Video-LLaMA bao gồm hai nhánh mã hóa: Nhánh Ngôn ngữ-Thị giác (VL) và Nhánh Âm thanh-Ngôn ngữ (AL), và một bộ giải mã ngôn ngữ (Vicuna (7B/13B) (Chiang et al., 2023), LLaMA (7B) (Touvron et al., 2023a), v.v.). Nhánh VL bao gồm một bộ mã hóa hình ảnh đóng băng đã được tiền đào tạo (thành phần thị giác đã được tiền đào tạo của BLIP-2 (Li et al., 2023d), bao gồm ViT-G/14 và Q-former đã được tiền đào tạo), một lớp embedding vị trí, một video Q-former và một lớp tuyến tính. Nhánh AL bao gồm một bộ mã hóa âm thanh đã được tiền đào tạo (ImageBind (Girdhar et al., 2023)) và một Audio Q-former. Hình 8 hiển thị kiến trúc tổng thể của Video-LLaMA với Nhánh Ngôn ngữ-Thị giác và Nhánh Âm thanh-Ngôn ngữ. Nhánh VL được đào tạo trên tập dữ liệu chú thích video Webvid-2M (Bain et al., 2021) với nhiệm vụ tạo sinh video-thành-văn bản, và được tinh chỉnh trên dữ liệu điều chỉnh hướng dẫn từ MiniGPT-4 (Zhu et al., 2023b), LLaVA (Liu et al., 2023b) và VideoChat (Li et al., 2023e). Nhánh AL được đào tạo trên dữ liệu chú thích hướng dẫn video/hình ảnh để kết nối đầu ra của ImageBind với bộ giải mã ngôn ngữ. Sau khi tinh chỉnh, Video-LLaMA có thể cảm nhận và hiểu nội dung video, thể hiện khả năng tích hợp thông tin âm thanh và hình ảnh, hiểu hình ảnh tĩnh, nhận biết các khái niệm kiến thức thông thường và nắm bắt động lực thời gian trong video.

InstructBLIP (1.2B) (Dai et al., 2023) là một khung điều chỉnh hướng dẫn ngôn ngữ-thị giác được khởi tạo với mô hình BLIP-2 (Li et al., 2023d) đã được tiền đào tạo bao gồm một bộ mã hóa hình ảnh, một LLM (FlanT5 (3B/11B) (Chung et al., 2022) hoặc Vicuna (7B/13B) (Chiang et al., 2023)), và một Query Transformer (Q-Former) để kết nối hai. Như được hiển thị trong Hình 9, Q-Former trích xuất các đặc điểm hình ảnh nhận thức hướng dẫn từ các embedding đầu ra của bộ mã hóa hình ảnh đóng băng, và đưa các đặc điểm hình ảnh làm đầu vào gợi ý mềm cho LLM đóng băng. Các tác giả đánh giá mô hình InstructBLIP được đề xuất trên nhiều nhiệm vụ ngôn ngữ-thị giác, bao gồm phân loại hình ảnh, chú thích hình ảnh, trả lời câu hỏi hình ảnh và lập luận hình ảnh. Họ sử dụng 26 tập dữ liệu có sẵn công khai, chia chúng thành 13 tập dữ liệu held-in và 13 held-out để đào tạo và đánh giá. Các tác giả chứng minh rằng InstructBLIP đạt được hiệu suất zero-shot tiên tiến trên một loạt rộng các nhiệm vụ ngôn ngữ-thị giác. InstructBLIP đạt được cải thiện tương đối trung bình 15.0% khi so sánh với BLIP-2, InstructBLIP nhỏ nhất (4B) vượt trội hơn Flamingo (80B) (Alayrac et al., 2022) trên tất cả sáu tập dữ liệu đánh giá được chia sẻ với cải thiện tương đối trung bình 24.8%.

Otter (Li et al., 2023b) là một mô hình đa phương thức được đào tạo bằng cách tinh chỉnh OpenFlamingo (9B) (Awadalla et al., 2023), với các bộ mã hóa ngôn ngữ và thị giác được đóng băng và chỉ tinh chỉnh module Perceiver resampler, các lớp cross-attention và embedding đầu vào/đầu ra. Các tác giả tổ chức các nhiệm vụ đa phương thức đa dạng bao phủ 11 danh mục và xây dựng tập dữ liệu điều chỉnh hướng dẫn trong ngữ cảnh đa phương thức MIMIC-IT của 2.8M cặp phản hồi hướng dẫn đa phương thức, bao gồm các bộ ba hình ảnh-hướng dẫn-trả lời, trong đó hướng dẫn-trả lời được điều chỉnh theo hình ảnh. Mỗi mẫu dữ liệu cũng bao gồm ngữ cảnh, chứa một loạt các bộ ba hình ảnh-hướng dẫn-trả lời có liên quan ngữ cảnh với bộ ba được truy vấn. Otter thể hiện khả năng tuân theo hướng dẫn của người dùng chính xác hơn và cung cấp mô tả chi tiết hơn về hình ảnh so với OpenFlamingo (Awadalla et al., 2023).

MultiModal-GPT (Gong et al., 2023) là một mô hình điều chỉnh hướng dẫn đa phương thức có khả năng tuân theo các hướng dẫn đa dạng, tạo ra chú thích chi tiết, đếm các đối tượng cụ thể và giải quyết các câu hỏi chung. MultiModal-GPT được đào tạo bằng cách tinh chỉnh OpenFlamingo (9B) (Awadalla et al., 2023) trên các dữ liệu hướng dẫn hình ảnh được tạo ra khác nhau với các tập dữ liệu mở, bao gồm VQA, Chú thích Hình ảnh, Lập luận Hình ảnh, OCR Văn bản và Đối thoại Hình ảnh. Các thí nghiệm chứng minh khả năng thành thạo của MultiModal-GPT trong việc duy trì các cuộc đối thoại liên tục với con người.

6 Điều chỉnh Hướng dẫn Cụ thể theo Lĩnh vực

Trong phần này, chúng tôi mô tả điều chỉnh hướng dẫn trong các lĩnh vực và ứng dụng khác nhau.

6.1 Đối thoại

InstructDial (Gupta et al., 2022) là một khung điều chỉnh hướng dẫn được thiết kế cho đối thoại. Nó chứa một bộ sưu tập 48 nhiệm vụ đối thoại trong định dạng văn bản-thành-văn bản nhất quán được tạo ra từ 59 tập dữ liệu đối thoại. Mỗi thể hiện nhiệm vụ bao gồm mô tả nhiệm vụ, đầu vào thể hiện, ràng buộc, hướng dẫn và đầu ra. Để đảm bảo tuân theo hướng dẫn, khung giới thiệu hai meta-task: (1) nhiệm vụ lựa chọn hướng dẫn, nơi mô hình chọn hướng dẫn tương ứng với cặp đầu vào-đầu ra đã cho; và (2) nhiệm vụ nhị phân hướng dẫn, nơi mô hình dự đoán "có" hoặc "không" nếu một hướng dẫn dẫn đến đầu ra đã cho từ một đầu vào. Hai mô hình cơ sở T0-3B (Sanh et al., 2021) (phiên bản 3B tham số của T5 (Lester et al., 2021)) và BART0 (Lin et al., 2022) (406M tham số dựa trên Bart-large (Lewis et al., 2019)) được tinh chỉnh trên các nhiệm vụ từ InstructDial. InstructDial đạt được kết quả ấn tượng trên các tập dữ liệu và nhiệm vụ đối thoại chưa thấy, bao gồm đánh giá đối thoại và phát hiện ý định. Hơn nữa, nó mang lại kết quả thậm chí tốt hơn khi áp dụng cho thiết lập few-shot.

6.2 Phân loại Ý định và Gắn thẻ Slot

LINGUIST (Rosenbaum et al., 2022) tinh chỉnh AlexaTM 5B (Soltan et al., 2022), một mô hình đa ngôn ngữ 5 tỷ tham số, trên tập dữ liệu hướng dẫn cho các nhiệm vụ phân loại ý định và gắn thẻ slot. Mỗi hướng dẫn bao gồm năm khối: (i) ngôn ngữ của đầu ra được tạo ra, (ii) ý định, (iii) loại slot và giá trị để bao gồm trong đầu ra (ví dụ: số 3 trong [3, tuyết] tương ứng với loại slot, và tuyết là giá trị được sử dụng cho slot đó), (iv) ánh xạ từ nhãn loại slot đến số, và (v) lên đến 10 ví dụ để hướng dẫn định dạng của đầu ra. LINGUIST cho thấy cải thiện đáng kể so với các phương pháp tiên tiến trong thiết lập ý định mới 10-shot sử dụng tập dữ liệu SNIPS (Coucke et al., 2018). Trong thiết lập cross-lingual zero-shot của tập dữ liệu mATIS++ (Xu et al., 2020), LINGUIST vượt qua một baseline mạnh của Machine Translation với Slot Alignment trên 6 ngôn ngữ trong khi duy trì hiệu suất phân loại ý định.

6.3 Trích xuất Thông tin

InstructUIE (Wang et al., 2023c) là một khung trích xuất thông tin thống nhất (IE) dựa trên điều chỉnh hướng dẫn, chuyển đổi các nhiệm vụ IE thành định dạng seq2seq và giải quyết chúng bằng cách tinh chỉnh FlanT5 11B (Chung et al., 2022) trên tập dữ liệu SFT đã xây dựng. Hình 10 hiển thị kiến trúc tổng thể của InstructUIE. Nó giới thiệu IE INSTRUCTIONS, một điểm chuẩn của 32 tập dữ liệu trích xuất thông tin đa dạng trong định dạng văn bản-thành-văn bản thống nhất với các hướng dẫn được viết bởi chuyên gia. Mỗi thể hiện nhiệm vụ được mô tả bởi bốn thuộc tính: hướng dẫn nhiệm vụ, tùy chọn, văn bản và đầu ra. Hướng dẫn nhiệm vụ chứa thông tin như loại thông tin cần được trích xuất, định dạng cấu trúc đầu ra và các ràng buộc hoặc quy tắc bổ sung cần được tuân theo trong quá trình trích xuất. Tùy chọn đề cập đến các ràng buộc nhãn đầu ra của một nhiệm vụ. Văn bản đề cập đến câu đầu vào. Đầu ra là câu được thu được bằng cách chuyển đổi các thẻ gốc của mẫu (ví dụ: "thẻ thực thể: khoảng thực thể" cho NER). Trong thiết lập có giám sát, InstructUIE hoạt động tương đương với BERT (Devlin et al., 2018) và vượt trội hơn các phương pháp tiên tiến và GPT3.5 (Brown et al., 2020a) trong thiết lập zero-shot.

6.4 Phân tích Cảm xúc Dựa trên Khía cạnh

Varia et al. (2022) đề xuất một khung điều chỉnh hướng dẫn thống nhất để giải quyết nhiệm vụ Phân tích Cảm xúc Dựa trên Khía cạnh (ABSA) dựa trên mô hình T5 (220M) (Raffel et al., 2019) được tinh chỉnh. Khung giải quyết nhiều tiểu nhiệm vụ được phân tách bao gồm bốn yếu tố của ABSA, cụ thể là Thuật ngữ Khía cạnh, Danh mục Khía cạnh, Thuật ngữ Ý kiến và Cảm xúc. Nó coi các tiểu nhiệm vụ này như một sự kết hợp của năm nhiệm vụ Trả lời Câu hỏi (QA) bằng cách chuyển đổi mỗi câu trong kho văn bản sử dụng các mẫu hướng dẫn được cung cấp cho mỗi nhiệm vụ. Ví dụ, một trong những mẫu hướng dẫn được sử dụng là "Các thuật ngữ khía cạnh trong văn bản là gì: $TEXT?". Khung thể hiện cải thiện đáng kể (8.29 F1 trung bình) so với các phương pháp tiên tiến trong kịch bản học few-shot và vẫn tương đương trong kịch bản tinh chỉnh đầy đủ.

6.5 Viết

Zhang et al. (2023d) đề xuất Writing-Alpaca-7B tinh chỉnh LLaMa-7B (Peng et al., 2023) trên tập dữ liệu hướng dẫn viết để cung cấp hỗ trợ viết. Tập dữ liệu hướng dẫn được đề xuất là một phần mở rộng của điểm chuẩn EDITEVAL (Dwivedi-Yu et al., 2022) dựa trên dữ liệu hướng dẫn, với nhiệm vụ Cập nhật được loại bỏ và nhiệm vụ về tính đúng ngữ pháp được giới thiệu. Sơ đồ hướng dẫn tuân thủ nghiêm ngặt sơ đồ trong dự án Stanford Alpaca (Taori et al., 2023a), bao gồm một lời nói đầu phổ quát, một trường hướng dẫn để hướng dẫn hoàn thành nhiệm vụ, một trường đầu vào cung cấp văn bản cần được chỉnh sửa và một trường phản hồi yêu cầu các mô hình điền vào. Writing-Alpaca-7B cải thiện hiệu suất của LLaMa trên tất cả các nhiệm vụ viết và vượt trội hơn các LLMs có sẵn lớn hơn khác.

CoEdIT (Raheja et al., 2023) tinh chỉnh FLANT5 (Chung et al., 2022) (770M tham số, 3B tham số và 11B tham số) trên tập dữ liệu hướng dẫn để chỉnh sửa văn bản nhằm cung cấp hỗ trợ viết. Tập dữ liệu hướng dẫn bao gồm khoảng 82K cặp <hướng dẫn: nguồn, mục tiêu>. Như được hiển thị trong Hình 11, mô hình nhận hướng dẫn từ người dùng chỉ định các đặc điểm của văn bản mong muốn, chẳng hạn như "Làm cho câu đơn giản hơn", và đầu ra văn bản đã chỉnh sửa. CoEdIT đạt được hiệu suất tiên tiến trên một số nhiệm vụ chỉnh sửa văn bản, bao gồm sửa lỗi ngữ pháp, đơn giản hóa văn bản, chỉnh sửa văn bản lặp đi lặp lại và ba nhiệm vụ chỉnh sửa phong cách: chuyển đổi phong cách trang trọng, trung lập hóa và diễn giải. Hơn nữa, nó có thể tổng quát hóa tốt cho các nhiệm vụ mới, liền kề không được thấy trong quá trình tinh chỉnh.

CoPoet (Chakrabarty et al., 2022) là một công cụ viết thơ hợp tác sử dụng một mô hình ngôn ngữ lớn (ví dụ: mô hình T5-3B, T5-11B và T0-3B) được đào tạo trên một bộ sưu tập đa dạng các hướng dẫn để viết thơ. Mỗi mẫu trong tập dữ liệu hướng dẫn bao gồm cặp <hướng dẫn, dòng_thơ>. Có ba loại hướng dẫn chính: Tiếp tục, Ràng buộc Từ vựng và Kỹ thuật Tu từ. CoPoet được hướng dẫn bởi các hướng dẫn của người dùng chỉ định các thuộc tính mong muốn của thơ, chẳng hạn như viết một câu về "tình yêu" hoặc kết thúc một câu bằng "bay". Hệ thống không chỉ cạnh tranh với các LLMs có sẵn công khai được đào tạo trên hướng dẫn, chẳng hạn như InstructGPT (Ouyang et al., 2022), mà còn có khả năng thỏa mãn các hướng dẫn tổng hợp chưa thấy.

6.6 Y tế

Radiology-GPT (Liu et al., 2023c) là một mô hình Alpaca-7B (Taori et al., 2023a) được tinh chỉnh cho X-quang, sử dụng phương pháp điều chỉnh hướng dẫn trên tập dữ liệu rộng lớn về kiến thức lĩnh vực X-quang. Báo cáo X-quang thường bao gồm hai phần tương ứng: "Findings" và "Impression". Phần "Findings" chứa các quan sát chi tiết từ hình ảnh X-quang, trong khi phần "Impression" tóm tắt các diễn giải được rút ra từ những quan sát đó. Radiology-GPT cung cấp một hướng dẫn ngắn gọn cho văn bản "Findings": "Rút ra ấn tượng từ phát hiện trong báo cáo X-quang". Văn bản "Impression" từ cùng một báo cáo phục vụ như đầu ra mục tiêu. So với các mô hình ngôn ngữ chung như StableLM (Islamovic), Dolly (Conover et al., 2023a) và LLaMA (Touvron et al., 2023a), Radiology-GPT thể hiện khả năng thích ứng đáng kể trong chẩn đoán X-quang, nghiên cứu và giao tiếp.

ChatDoctor (Li et al., 2023j) dựa trên mô hình LLaMa-7B (Touvron et al., 2023a) được tinh chỉnh, sử dụng tập dữ liệu hướng dẫn alpaca (Taori et al., 2023a) và tập dữ liệu đối thoại bác sĩ-bệnh nhân HealthCareMagic100k. Và các mẫu gợi ý được thiết kế để truy xuất cơ sở dữ liệu kiến thức bên ngoài, chẳng hạn như Cơ sở dữ liệu Bệnh và truy xuất Wikipedia, trong các cuộc hội thoại bác sĩ-bệnh nhân để có được đầu ra chính xác hơn từ mô hình. ChatDoctor cải thiện đáng kể khả năng của mô hình trong việc hiểu nhu cầu bệnh nhân và cung cấp lời khuyên có thông tin. Bằng cách trang bị cho mô hình khả năng tự truy xuất thông tin từ các nguồn trực tuyến và ngoại tuyến đáng tin cậy, độ chính xác của phản hồi được cải thiện đáng kể.

ChatGLM-Med (Wang et al., 2023a) được tinh chỉnh trên tập dữ liệu hướng dẫn y tế tiếng Trung dựa trên mô hình ChatGLM-6B (Du et al., 2022). Tập dữ liệu hướng dẫn bao gồm các cặp câu hỏi và trả lời liên quan đến y tế, được tạo ra sử dụng API GPT 3.5 và Đồ thị Kiến thức Y tế. Mô hình này cải thiện hiệu suất trả lời câu hỏi của ChatGLM (Du et al., 2022) trong lĩnh vực y tế.

6.7 Số học

Goat (Liu và Low, 2023) là một mô hình LLaMA-7B (Touvron et al., 2023a) được tinh chỉnh dựa trên hướng dẫn, nhằm giải quyết các bài toán số học. Nó thể hiện các bài toán số học dưới dạng trả lời câu hỏi ngôn ngữ tự nhiên, chẳng hạn như "8914/64 bằng bao nhiêu?", bằng cách tạo ra hàng trăm mẫu hướng dẫn sử dụng ChatGPT (OpenAI, 2022). Mô hình áp dụng các kỹ thuật khác nhau để nâng cao khả năng thích ứng với các định dạng câu hỏi đa dạng, chẳng hạn như loại bỏ ngẫu nhiên khoảng trắng giữa các số và ký hiệu trong biểu thức số học và thay thế "*" bằng "x" hoặc "nhân". Mô hình Goat đạt được hiệu suất tiên tiến trên tiểu nhiệm vụ số học BIG-bench (Srivastava et al., 2022a). Cụ thể, Goat-7B zero-shot tương đương hoặc vượt qua độ chính xác đạt được bởi PaLM-540B few-shot (Chowdhery et al., 2022).

6.8 Mã

WizardCoder (Luo et al., 2023) sử dụng StarCoder 15B (Li et al., 2023f) làm nền tảng với điều chỉnh hướng dẫn phức tạp, bằng cách áp dụng phương pháp Evol-Instruct (Xu et al., 2023a) cho lĩnh vực mã. Tập dữ liệu đào tạo được tạo ra thông qua việc áp dụng lặp đi lặp lại kỹ thuật Evol-Instruct trên tập dữ liệu Code Alpaca (Taori et al., 2023b), bao gồm các thuộc tính sau cho mỗi mẫu: hướng dẫn, đầu vào và đầu ra mong đợi. Ví dụ, khi hướng dẫn là "Sửa đổi truy vấn SQL sau để chọn các yếu tố riêng biệt", đầu vào là truy vấn SQL, và đầu ra mong đợi là câu trả lời được tạo ra. WizardCoder vượt trội hơn tất cả các LLMs Mã mã nguồn mở khác và thậm chí vượt qua các LLMs đóng lớn nhất, Claude của Anthropic và Bard của Google, trên HumanEval và HumanEval+.

7 Kỹ thuật Tinh chỉnh Hiệu quả

Các kỹ thuật tinh chỉnh hiệu quả nhằm thích ứng LLMs cho các nhiệm vụ downstream bằng cách tối ưu hóa một phần nhỏ tham số theo nhiều cách, tức là dựa trên bổ sung, dựa trên đặc tả và dựa trên tham số hóa lại. Các phương pháp dựa trên bổ sung giới thiệu các tham số hoặc module có thể đào tạo bổ sung không có trong mô hình gốc. Các phương pháp đại diện bao gồm điều chỉnh adapter (Houlsby et al., 2019) và điều chỉnh dựa trên gợi ý (Schick và Schütze, 2021). Các phương pháp dựa trên đặc tả chỉ định các tham số mô hình cố hữu nhất định để được điều chỉnh trong khi đóng băng các tham số khác. Ví dụ, BitFit (Zaken et al., 2022) điều chỉnh các thuật ngữ bias của mô hình đã được tiền đào tạo. Các phương pháp tham số hóa lại chuyển đổi trọng số mô hình thành các dạng hiệu quả hơn về tham số để điều chỉnh. Giả thuyết chính là việc thích ứng mô hình có thứ hạng thấp, vì vậy trọng số có thể được tham số hóa lại thành các yếu tố thứ hạng thấp hoặc một không gian con chiều thấp (ví dụ: LoRA (Hu et al., 2021)). Điều chỉnh gợi ý nội tại tìm ra một không gian con chiều thấp được chia sẻ bởi các gợi ý điều chỉnh trên các nhiệm vụ đa dạng.

7.1 LoRA

Thích ứng Thứ hạng Thấp (LoRA) (Hu et al., 2021) cho phép thích ứng hiệu quả của LLMs sử dụng các cập nhật thứ hạng thấp. LoRA sử dụng DeepSpeed (Rasley et al., 2020) làm xương sống đào tạo. Insight chính của LoRA là sự thay đổi thực tế trong trọng số LLMs cần thiết cho việc thích ứng nhiệm vụ mới nằm trong một không gian con chiều thấp. Cụ thể, đối với ma trận trọng số đã được tiền đào tạo W0, các tác giả mô hình hóa ma trận trọng số thích ứng như W0+∆W, trong đó ∆W là một cập nhật thứ hạng thấp. ∆W được tham số hóa như ∆W=BA, trong đó A và B là các ma trận có thể đào tạo nhỏ hơn nhiều. Thứ hạng r của ∆W được chọn nhỏ hơn nhiều so với các chiều của W0. Trực giác là thay vì đào tạo trực tiếp tất cả W0, các tác giả đào tạo A và B chiều thấp, gián tiếp đào tạo W0 trong một không gian con thứ hạng thấp của các hướng quan trọng cho nhiệm vụ downstream. Điều này dẫn đến ít tham số có thể đào tạo hơn nhiều so với tinh chỉnh đầy đủ. Đối với GPT-3, LoRA giảm số lượng tham số có thể đào tạo 10,000 lần và sử dụng bộ nhớ 3 lần so với tinh chỉnh đầy đủ.

7.2 HINT

HINT (Ivison et al., 2022) kết hợp lợi ích tổng quát hóa của điều chỉnh hướng dẫn với tinh chỉnh hiệu quả theo yêu cầu, tránh xử lý lặp đi lặp lại các hướng dẫn dài. Bản chất của HINT nằm trong các hypernetwork, tạo ra các module hiệu quả tham số cho việc thích ứng LLMs dựa trên hướng dẫn ngôn ngữ tự nhiên và các ví dụ few-shot. Hypernetwork được áp dụng chuyển đổi hướng dẫn và các ví dụ few-shot thành hướng dẫn được mã hóa và tạo ra các tham số adapter và prefix sử dụng bộ mã hóa văn bản đã được tiền đào tạo và bộ tạo tham số dựa trên cross-attention. Sau đó, các adapter và prefix được tạo ra được chèn vào mô hình xương sống như các module điều chỉnh hiệu quả. Trong quá trình suy luận, hypernetwork thực hiện suy luận chỉ một lần mỗi nhiệm vụ để tạo ra các module thích ứng. Lợi ích là HINT có thể kết hợp các hướng dẫn dài và few-shot bổ sung mà không tăng tính toán, không giống như các phương pháp tinh chỉnh thông thường hoặc nối đầu vào.

7.3 Qlora

QLORA (Dettmers et al., 2023) bao gồm quantization tối ưu và tối ưu hóa bộ nhớ, nhằm cung cấp tinh chỉnh LLMs hiệu quả và hiệu quả. QLORA bao gồm Quantization NormalFloat 4-bit (NF4), là một sơ đồ quantization được tối ưu hóa cho phân phối bình thường điển hình của trọng số LLM. Bằng cách quantization dựa trên các quantile của phân phối bình thường, NF4 cung cấp hiệu suất tốt hơn so với quantization số nguyên hoặc float 4-bit tiêu chuẩn. Để giảm thêm bộ nhớ, các hằng số quantization tự chúng được quantization thành 8 bit. Mức quantization thứ hai này tiết kiệm thêm 0.37 bit mỗi tham số trung bình. QLORA tận dụng tính năng bộ nhớ thống nhất của NVIDIA để chuyển các trạng thái optimizer sang RAM CPU khi bộ nhớ GPU bị vượt quá, tránh hết bộ nhớ trong quá trình đào tạo. QLORA cho phép đào tạo LLM 65B tham số trên một GPU 48GB duy nhất mà không có suy giảm so với tinh chỉnh 16-bit đầy đủ. QLORA hoạt động bằng cách đóng băng LLM cơ sở được quantization 4-bit, sau đó lan truyền ngược qua nó vào một tập nhỏ trọng số adapter thứ hạng thấp 16-bit được học.

7.4 LOMO

Tối ưu hóa Bộ nhớ Thấp (LOMO) (Lv et al., 2023) cho phép tinh chỉnh tham số đầy đủ của LLMs sử dụng tài nguyên tính toán hạn chế thông qua sự kết hợp của tính toán gradient và cập nhật. Bản chất là kết hợp tính toán gradient và cập nhật tham số thành một bước trong quá trình lan truyền ngược, do đó tránh lưu trữ các tensor gradient đầy đủ. Đầu tiên, phân tích lý thuyết được cung cấp trong LOMO về lý do tại sao SGD có thể hoạt động tốt cho việc tinh chỉnh các mô hình đã được tiền đào tạo lớn mặc dù có những thách thức của nó trên các mô hình nhỏ hơn. Ngoài ra, LOMO cập nhật mỗi tensor tham số ngay lập tức sau khi tính toán gradient của nó trong lan truyền ngược. Lưu trữ gradient của một tham số tại một thời điểm giảm bộ nhớ gradient xuống O(1). LOMO sử dụng cắt giá trị gradient, tính toán norm gradient riêng biệt và tỷ lệ mất mát động để ổn định đào tạo. Việc tích hợp các phương pháp activation checkpointing và tối ưu hóa ZeRO tiết kiệm bộ nhớ.

7.5 Delta-tuning

Delta-tuning (Ding et al., 2023b) cung cấp các góc nhìn tối ưu hóa và điều khiển tối ưu cho phân tích lý thuyết. Một cách trực quan, delta-tuning thực hiện tối ưu hóa không gian con bằng cách hạn chế điều chỉnh cho một đa tạp chiều thấp. Các tham số được điều chỉnh hoạt động như các bộ điều khiển tối ưu hướng dẫn hành vi mô hình trên các nhiệm vụ downstream.

8 Đánh giá, Phân tích và Chỉ trích

8.1 Đánh giá Đóng

Được chấp nhận rộng rãi trong số các nhà nghiên cứu rằng các mô hình mục đích chung phải thể hiện thành thạo trong một số nhiệm vụ cốt lõi nhất định trước khi chúng có thể tổng quát hóa hiệu quả để đáp ứng các nhu cầu thực tế đa dạng. Đánh giá đóng giúp đạt được mục tiêu này, thường liên quan đến các câu hỏi trắc nghiệm để đánh giá hiệu suất của LLMs. Dưới đây là 6 đánh giá đóng được sử dụng rộng rãi:

(1) MMLU. Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020a) bao gồm 14079 câu hỏi bao phủ 57 nhiệm vụ bao gồm toán học tiểu học, lịch sử Hoa Kỳ, khoa học máy tính, luật, và nhiều hơn nữa. Phạm vi rộng của các chủ đề và các câu hỏi phức tạp làm cho MMLU phù hợp để kiểm tra khả năng hiểu ngôn ngữ và ra quyết định của mô hình.

(2) MATH và (3) GSM8K. MATH (Hendrycks et al., 2021) và GSM8K (Cobbe et al., 2021) là hai tập dữ liệu toán học riêng biệt được sử dụng để đánh giá các khía cạnh khác nhau của khả năng mô hình. Tập dữ liệu MATH (Hendrycks et al., 2021) bao gồm 12,500 bài toán toán học phức tạp cấp độ cạnh tranh, được thiết kế chủ yếu để đánh giá khả năng của các mô hình trong việc giải quyết các câu hỏi toán học thách thức và tiên tiến thường gặp ở cấp độ đại học. Ngược lại, tập dữ liệu GSM8K (Cobbe et al., 2021) chứa 8,500 bài toán toán học tiểu học chất lượng cao, nhằm kiểm tra khả năng lập luận toán học cơ bản của các mô hình.

(4) BBH. BBH, viết tắt của BIG-Bench Hard (Suzgun et al., 2022a), là một tập con của tập dữ liệu BIG-Bench (Srivastava et al., 2022b) bao gồm 23 nhiệm vụ thách thức. Những nhiệm vụ này được chọn vì chúng luôn chứng minh quá khó khăn cho các mô hình ngôn ngữ lớn hiện tại để xử lý hiệu quả. Yêu cầu lập luận phức tạp, nhiều bước, tập dữ liệu BBH được sử dụng chủ yếu để đánh giá khả năng lập luận chung của các mô hình, kiểm tra khả năng điều hướng và giải quyết các vấn đề phức tạp của chúng.

(5) HumanEval (Coding). HumanEval (Chen et al., 2021a) bao gồm 164 bài toán lập trình, bao gồm hiểu ngôn ngữ, thuật toán và toán học đơn giản, với một số tương đương với các câu hỏi phỏng vấn phần mềm đơn giản. Mục đích chính của tập dữ liệu này là đánh giá khả năng của các mô hình trong việc tạo ra các chương trình chính xác dựa trên các docstring được cung cấp.

(6) IFEval. IFEval (Zhou et al., 2023b) bao gồm 500 gợi ý, mỗi gợi ý chứa các hướng dẫn cụ thể như "viết một bài báo với hơn 800 từ" hoặc "bao quanh phản hồi của bạn trong dấu ngoặc kép". Tập dữ liệu này được sử dụng để kiểm tra khả năng của các mô hình ngôn ngữ lớn trong việc tuân theo chính xác các hướng dẫn đã cho.

8.2 Đánh giá HELM

HELM (Liang et al., 2022) là một đánh giá toàn diện của các Mô hình Ngôn ngữ (LMs) để cải thiện tính minh bạch của các mô hình ngôn ngữ, cung cấp hiểu biết toàn diện hơn về khả năng, rủi ro và hạn chế của các mô hình ngôn ngữ. Cụ thể, khác với các phương pháp đánh giá khác, HELM cho rằng một đánh giá toàn diện của các mô hình ngôn ngữ nên tập trung vào ba yếu tố sau:

(1) Bao phủ rộng. Trong quá trình phát triển, các mô hình ngôn ngữ có thể được thích ứng cho các nhiệm vụ NLP khác nhau (ví dụ: gắn nhãn chuỗi và trả lời câu hỏi), do đó, việc đánh giá các mô hình ngôn ngữ cần được thực hiện trong một loạt rộng các kịch bản. Để bao gồm tất cả các kịch bản tiềm năng, HELM đề xuất một phân loại từ trên xuống, bắt đầu bằng cách biên soạn tất cả các nhiệm vụ hiện có trong một hội nghị NLP lớn (ACL2022) thành một không gian nhiệm vụ và chia mỗi nhiệm vụ thành dạng kịch bản (ví dụ: ngôn ngữ) và chỉ số (ví dụ: độ chính xác). Sau đó khi đối mặt với một nhiệm vụ cụ thể, phân loại sẽ chọn một hoặc nhiều kịch bản và chỉ số trong không gian nhiệm vụ để bao phủ nó. Bằng cách phân tích cấu trúc của mỗi nhiệm vụ, HELM làm rõ nội dung đánh giá (kịch bản nhiệm vụ và chỉ số) và cải thiện bao phủ kịch bản của các mô hình ngôn ngữ từ 17.9% lên 96.0%.

(2) Đo lường đa chỉ số. Để cho phép con người cân nhắc các mô hình ngôn ngữ từ các góc độ khác nhau, HELM đề xuất đo lường đa chỉ số. HELM đã bao phủ 16 kịch bản khác nhau và 7 chỉ số. Để đảm bảo kết quả của đo lường đa chỉ số chuyên sâu, HELM đã đo 98 trong số 112 kịch bản cốt lõi có thể (87.5%).

(3) Tiêu chuẩn hóa. Sự gia tăng quy mô và độ phức tạp đào tạo của các mô hình ngôn ngữ đã cản trở nghiêm trọng hiểu biết của con người về cấu trúc của mỗi mô hình ngôn ngữ. Để thiết lập hiểu biết thống nhất về các mô hình ngôn ngữ hiện có, HELM đánh giá 30 mô hình ngôn ngữ nổi tiếng, bao phủ các tổ chức như Google (UL2(Tay et al., 2022)), OpenAI (GPT-3(Brown et al., 2020b)), và EleutherAI (GPT-NeoX(Black et al., 2022)). Thú vị thay, HELM chỉ ra rằng các LMs như T5 (Raffel et al., 2019) và Anthropic-LMv4-s3 (Bai et al., 2022a) đã không được so sánh trực tiếp trong công trình ban đầu, trong khi các LLMs như GPT-3 và YaLM vẫn khác với các báo cáo tương ứng của chúng sau nhiều đánh giá.

8.3 LLM Như một Thẩm phán

LLM như một thẩm phán đề cập đến một tập hợp các phương pháp sử dụng các LLMs mạnh mẽ, đặc biệt là GPT-4 (OpenAI, 2023), để đánh giá đầu ra của các LLMs khác. Có ba lý do chính cho phương pháp này: (1) Hiệu quả – Việc xem xét thủ công nhiều đầu ra LLM có thể tốn nhiều lao động, trong khi GPT-4 có thể đánh giá các phản hồi quy mô lớn một cách nhanh chóng, tiết kiệm cả thời gian và công sức; (2) Điểm chuẩn Đáng tin cậy – Là một trong những mô hình tiên tiến nhất có sẵn, GPT-4 cung cấp một điểm chuẩn đáng tin cậy, cho phép các nhà nghiên cứu so sánh hiệu suất của các LLMs khác nhau với một tiêu chuẩn cao; và (3) Khả năng Nâng cao – Với khả năng hiểu và lập luận được cải thiện so với các mô hình trước, GPT-4 phù hợp hơn để phân tích các khía cạnh tinh tế của việc tạo ngôn ngữ và xử lý các đầu ra phức tạp từ các LLMs khác. Dưới đây, chúng tôi chi tiết 4 điểm chuẩn thẩm phán được chấp nhận thông thường:

(1) AlpacaEval. AlpacaEval (Li et al., 2023h) là một chỉ số đánh giá tự động tận dụng LLMs, bao gồm 805 hướng dẫn được chọn để phản ánh các tương tác người dùng điển hình từ demo web Alpaca. Cụ thể, đối với mỗi hướng dẫn, cả mô hình cơ sở b (hiện tại là GPT-4 turbo (OpenAI, 2023)) và mô hình được đánh giá m đều tạo ra phản hồi. Một đánh giá viên dựa trên GPT-4 turbo sau đó thực hiện so sánh trực tiếp của các phản hồi này, xác định xác suất ưu tiên mô hình được đánh giá. Tỷ lệ thắng được tính như xác suất mong đợi rằng đánh giá viên ưu tiên phản hồi của mô hình được đánh giá trên 805 hướng dẫn, phục vụ như một chỉ số chính để đánh giá hiệu suất của chatbot LM được đánh giá.

(2) Length-Controlled AlpacaEval. Length-Controlled AlpacaEval (Dubois et al., 2024) là một biến thể của chỉ số đánh giá AlpacaEval (Li et al., 2023h), được thiết kế để tối thiểu hóa thiên kiến độ dài, vì AlpacaEval gốc có xu hướng ưu tiên các mô hình tạo ra phản hồi dài hơn. Để đạt được mục tiêu này, Dubois et al. (2024) đầu tiên fit một mô hình tuyến tính tổng quát để dự đoán sở thích của chú thích viên (GPT-4) dựa trên ba yếu tố: (1) hướng dẫn, (2) mô hình được sử dụng, và (3) sự khác biệt độ dài giữa đầu ra cơ sở và mô hình. Sau đó, bằng cách điều kiện sự khác biệt độ dài về 0, Dubois et al. (2024) có thể thu được sở thích được kiểm soát độ dài. Ý tưởng này, dự đoán kết quả trong khi điều kiện về sự khác biệt độ dài (trung gian), là một kỹ thuật phổ biến trong suy luận thống kê, và bằng cách giới thiệu nó, Length-Controlled AlpacaEval tăng tương quan Spearman với Chatbot Arena của LMSYS từ 0.94 lên 0.98.

(3) MT-Bench. Hiện tại, các đánh giá đóng chỉ đo khả năng cốt lõi của LLMs trên một tập hạn chế các nhiệm vụ, chẳng hạn như MMLU (Hendrycks et al., 2020a) cho các quyết định trắc nghiệm, mà không đánh giá đầy đủ sự căn chỉnh của nó với sở thích con người trong các nhiệm vụ mở, chẳng hạn như khả năng tuân theo hướng dẫn trong các cuộc đối thoại nhiều lượt một cách chính xác. Để giảm nhẹ vấn đề này, Zheng et al. (2023) giới thiệu MT-Bench, bao gồm 80 câu hỏi nhiều lượt chất lượng cao được thiết kế để đánh giá khả năng của LLMs trong các cuộc hội thoại nhiều lượt và tuân theo hướng dẫn, với các đánh giá được thực hiện sử dụng GPT-4. MT-Bench được chế tác tỉ mỉ để bao phủ tám nhiệm vụ thông thường: viết, nhập vai, trích xuất, lập luận, toán, mã hóa, kiến thức I (STEM), và kiến thức II (nhân văn/khoa học xã hội). Đối với căn chỉnh, GPT-4 đạt được hơn 80% thỏa thuận, tương đương với mức độ thỏa thuận giữa con người, làm cho nó trở thành lựa chọn đáng tin cậy hơn cho một điểm chuẩn công khai.

(4) WildBench. Mặc dù các đánh giá trên hiệu quả, chúng có những hạn chế đáng chú ý trong thành phần nhiệm vụ và bao phủ kỹ năng. Ví dụ, MT-Bench (Hendrycks et al., 2020a) chỉ bao gồm 80 hướng dẫn kiểm tra, trong khi AlpacaEval (Li et al., 2023h) có nhiều nhiệm vụ đơn giản, chẳng hạn như "Thủ đô của Úc là gì?" Để giải quyết vấn đề này, Lin et al. (2024) giới thiệu WildBench, bao gồm 1,024 hướng dẫn kiểm tra được tuyển chọn cẩn thận từ các nhật ký cuộc hội thoại người-chatbot rộng lớn. WildBench rút ra trực tiếp từ các tương tác người dùng thực tế, có nhiều nhiệm vụ thách thức, chẳng hạn như mã hóa và giải quyết bài toán toán học. Những nhiệm vụ này thường xuyên đòi hỏi tư duy phản biện, làm cho WildBench khó khăn hơn đáng kể so với các điểm chuẩn khác. WildBench sử dụng hai chỉ số: WB-Reward cho so sánh cặp và WB-Score cho đánh giá cá nhân. Cả hai chỉ số đều cho thấy sự căn chỉnh mạnh mẽ với các đánh giá của con người, với tương quan Pearson là 0.98 cho WB-Reward và 0.95 cho WB-Score khi so sánh với các đánh giá được bình chọn bởi con người.

8.4 Điều chỉnh Hướng dẫn Tài nguyên Thấp

Gupta et al. (2023) cố gắng ước tính dữ liệu đào tạo downstream tối thiểu được yêu cầu bởi các mô hình SFT để khớp với các mô hình có giám sát SOTA trên các nhiệm vụ khác nhau. Gupta et al. (2023) thực hiện các thí nghiệm trên 119 nhiệm vụ từ Super Natural Instructions (SuperNI) trong cả thiết lập học một nhiệm vụ (STL) và học đa nhiệm vụ (MTL). Kết quả cho thấy rằng trong thiết lập STL, các mô hình SFT chỉ với 25% dữ liệu đào tạo downstream vượt trội hơn các mô hình SOTA trên những nhiệm vụ đó, trong khi trong thiết lập MTL, chỉ 6% dữ liệu đào tạo downstream có thể dẫn các mô hình SFT đạt được hiệu suất SOTA. Những phát hiện này cho thấy điều chỉnh hướng dẫn có thể hỗ trợ hiệu quả một mô hình trong việc học nhanh chóng một nhiệm vụ ngay cả với dữ liệu hạn chế.

Tuy nhiên, do hạn chế tài nguyên, Gupta et al. (2023) đã không thực hiện các thí nghiệm trên LLMs, như T5-11B. Vì vậy, để có hiểu biết toàn diện hơn về các mô hình SFT, cần có điều tra sâu hơn sử dụng các mô hình ngôn ngữ và tập dữ liệu lớn hơn.

8.5 Tập dữ liệu Hướng dẫn Nhỏ hơn

SFT yêu cầu một lượng lớn dữ liệu hướng dẫn chuyên biệt để đào tạo. Zhou et al. (2023a) đưa ra giả thuyết rằng LLM đã được tiền đào tạo chỉ cần học phong cách hoặc định dạng để tương tác với người dùng và đề xuất LIMA đạt được hiệu suất mạnh mẽ bằng cách tinh chỉnh LLM chỉ trên 1,000 ví dụ huấn luyện được chọn lọc cẩn thận.

Cụ thể, LIMA đầu tiên tuyển chọn thủ công 1,000 minh chứng với các gợi ý và phản hồi chất lượng cao. Sau đó 1,000 minh chứng được sử dụng để tinh chỉnh LLaMa 65B-parameter đã được tiền đào tạo (Touvron et al., 2023b). Bằng cách so sánh, trên hơn 300 nhiệm vụ thách thức, LIMA vượt trội hơn GPT-davinci003 (Brown et al., 2020b), được tinh chỉnh trên 5,200 ví dụ bằng điều chỉnh phản hồi con người. Hơn nữa, chỉ với một nửa số lượng minh chứng, LIMA đạt được kết quả tương đương với GPT-4 (OpenAI, 2023), Claude (Bai et al., 2022b), và Bard. Trên tất cả, LIMA chứng minh rằng kiến thức và khả năng mạnh mẽ của LLMs có thể được tiếp xúc với người dùng chỉ với một vài hướng dẫn được tuyển chọn cẩn thận để tinh chỉnh.

8.6 Đánh giá Tập dữ liệu Điều chỉnh Hướng dẫn

Hiệu suất của mô hình SFT phụ thuộc rất nhiều vào các tập dữ liệu SFT. Tuy nhiên, thiếu đánh giá cho các tập dữ liệu SFT này từ các khía cạnh mở và chủ quan.

Để giải quyết vấn đề này, Wang et al. (2023d) thực hiện đánh giá tập dữ liệu bằng cách tinh chỉnh mô hình LLaMa (Touvron et al., 2023b) trên nhiều tập dữ liệu SFT mở và đo các mô hình được tinh chỉnh khác nhau thông qua cả đánh giá tự động và con người. Một mô hình bổ sung được đào tạo trên sự kết hợp của các tập dữ liệu SFT. Đối với kết quả, Wang et al. (2023d) cho thấy không có một tập dữ liệu SFT tốt nhất duy nhất trên tất cả các nhiệm vụ, trong khi bằng cách kết hợp thủ công các tập dữ liệu có thể đạt được hiệu suất tổng thể tốt nhất. Bên cạnh đó, Wang et al. (2023d) chỉ ra rằng mặc dù SFT có thể mang lại lợi ích lớn cho LLMs ở tất cả các kích thước, các mô hình nhỏ hơn và các mô hình có chất lượng cơ sở cao hưởng lợi nhiều nhất từ SFT. Đối với đánh giá con người, Wang et al. (2023d) một mô hình lớn hơn có nhiều khả năng nhận được điểm chấp nhận cao hơn.

8.7 Căn chỉnh Bề mặt

Mặc dù có những cải thiện ấn tượng trong hiệu suất của điều chỉnh hướng dẫn, thiếu rõ ràng về kiến thức cụ thể mà các mô hình thu được thông qua điều chỉnh hướng dẫn, đặt ra câu hỏi về: Điều chỉnh hướng dẫn có chỉ học Sao chép Mẫu không? hoặc Chính xác thì điều chỉnh căn chỉnh chuyển đổi LLM cơ sở như thế nào?

Để trả lời những câu hỏi này, Kung và Peng (2023) đào sâu vào phân tích cách các mô hình sử dụng hướng dẫn trong SFT bằng cách so sánh việc điều chỉnh khi được cung cấp với các hướng dẫn bị thay đổi so với các hướng dẫn gốc.

Cụ thể, Kung và Peng (2023) tạo ra các định nghĩa nhiệm vụ đơn giản hóa loại bỏ tất cả các thành phần ngữ nghĩa, chỉ để lại thông tin đầu ra. Ngoài ra, Kung và Peng (2023) cũng kết hợp các ví dụ lừa dối chứa ánh xạ đầu vào-đầu ra không chính xác. Đáng ngạc nhiên, các thí nghiệm cho thấy các mô hình được đào tạo trên các định nghĩa nhiệm vụ đơn giản hóa hoặc ví dụ lừa dối này có thể đạt được hiệu suất tương đương với những mô hình được đào tạo trên các hướng dẫn và ví dụ gốc. Hơn nữa, bài báo cũng giới thiệu một baseline cho nhiệm vụ phân loại với zero-shot, đạt được hiệu suất tương tự như SFT trong thiết lập tài nguyên thấp.

Tương tự như các phát hiện của Kung và Peng (2023), một số nghiên cứu tiếp theo (Zhou et al., 2023a; Lin et al., 2023a) đã đi đến cùng kết luận: những cải thiện hiệu suất được quan sát trong các mô hình SFT hiện tại thường do căn chỉnh bề mặt. Điều này có nghĩa là các mô hình xuất sắc trong việc nhận biết căn chỉnh bề mặt, chẳng hạn như làm chủ các định dạng đầu ra và đưa ra các phỏng đoán có giáo dục, thay vì thực sự hiểu và học các nhiệm vụ cơ bản.

8.8 Bắt chước LLMs Độc quyền

Bắt chước LLMs là một phương pháp thu thập đầu ra từ một mô hình mạnh hơn, chẳng hạn như một hệ thống độc quyền như ChatGPT, và sử dụng những đầu ra này để tinh chỉnh một LLM mã nguồn mở. Thông qua cách này, một LLM mã nguồn mở có thể có được khả năng cạnh tranh với bất kỳ mô hình độc quyền nào.

Gudibande et al. (2023) thực hiện một số thí nghiệm để phân tích một cách phê phán hiệu quả của việc bắt chước mô hình. Cụ thể, Gudibande et al. (2023) đầu tiên thu thập các tập dữ liệu từ đầu ra của ChatGPT trên các nhiệm vụ rộng. Sau đó các tập dữ liệu này được sử dụng để tinh chỉnh một loạt mô hình bao phủ kích thước từ 1.5B đến 13B, mô hình cơ sở GPT-2 và LLaMA, và số lượng dữ liệu từ 0.3M tokens đến 150M tokens.

Đối với đánh giá, Gudibande et al. (2023) chứng minh rằng trên các nhiệm vụ với tập dữ liệu được hỗ trợ, các mô hình bắt chước tốt hơn nhiều so với trước đây, và đầu ra của chúng xuất hiện tương tự như của ChatGPT. Trong khi đó trên các nhiệm vụ không có tập dữ liệu bắt chước, các mô hình bắt chước không có cải thiện hoặc thậm chí giảm độ chính xác.

Do đó, Gudibande et al. (2023) chỉ ra rằng đó là hiện tượng mà các mô hình bắt chước thành thạo trong việc bắt chước phong cách của ChatGPT (ví dụ: trôi chảy, tự tin và có cấu trúc tốt) làm cho các nhà nghiên cứu có ảo tưởng về khả năng chung của các mô hình bắt chước. Vì vậy, Gudibande et al. (2023) đề xuất rằng thay vì bắt chước các mô hình độc quyền, các nhà nghiên cứu nên tập trung vào việc cải thiện chất lượng của các mô hình cơ sở và ví dụ hướng dẫn.

9 Kết luận

Công trình này khảo sát những tiến bộ gần đây trong lĩnh vực điều chỉnh hướng dẫn đang phát triển nhanh chóng, cũng có thể được gọi là tinh chỉnh có giám sát (SFT). Chúng tôi thực hiện một đánh giá có hệ thống của tài liệu, bao gồm phương pháp luận chung của SFT, việc xây dựng tập dữ liệu SFT, đào tạo các mô hình SFT, các ứng dụng của SFT cho các phương thức, lĩnh vực và ứng dụng khác nhau. Chúng tôi cũng xem xét phân tích về các mô hình SFT để khám phá cả ưu điểm và bẫy tiềm ẩn của chúng. Chúng tôi hy vọng công trình này sẽ hoạt động như một kích thích để thúc đẩy các nỗ lực tiếp theo nhằm giải quyết những thiếu sót của các mô hình SFT hiện tại.

[Phần tài liệu tham khảo và phụ lục được lược bỏ để tiết kiệm không gian]
