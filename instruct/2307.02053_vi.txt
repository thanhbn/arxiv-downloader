# FLACUNA: Giải Phóng Sức Mạnh Giải Quyết Vấn Đề của VICUNA bằng Việc Tinh Chỉnh F LAN

Deepanway Ghosal‡, Yew Ken Chia‡, Navonil Majumder†, Soujanya Poria‡
‡DeCLaRe Lab, Singapore University of Technology and Design, Singapore
{deepanway_ghosal, yewken_chia}@mymail.sutd.edu.sg
{navonil_majumder,sporia}@sutd.edu.sg
MÃ NGUỒN:https://github.com/declare-lab/flacuna
MÔ HÌNH:https://huggingface.co/declare-lab/flacuna-13b-v1.0
FLAN-MINI:https://huggingface.co/declare-lab/flan-mini

## Tóm tắt

Gần đây, việc phát hành INSTRUCT EVAL [Chia et al., 2023] đã cung cấp những hiểu biết có giá trị về hiệu suất của các mô hình ngôn ngữ lớn (LLMs) sử dụng kiến trúc encoder-decoder hoặc decoder-only. Thú vị là, mặc dù được giới thiệu cách đây bốn năm, các LLM dựa trên T5, như FLAN-T5, vẫn tiếp tục vượt trội so với các LLM dựa trên decoder mới nhất, như LLAMA và VICUNA, trong các nhiệm vụ yêu cầu kỹ năng giải quyết vấn đề tổng quát. Sự khác biệt về hiệu suất này có thể được quy cho ba yếu tố chính: (1) Dữ liệu tiền huấn luyện, (2) Kiến trúc backbone, và (3) Tập dữ liệu hướng dẫn. Trong báo cáo kỹ thuật này, trọng tâm chính của chúng tôi là nghiên cứu tác động của yếu tố thứ ba bằng cách tận dụng VICUNA, một mô hình ngôn ngữ lớn dựa trên LLAMA, đã trải qua quá trình tinh chỉnh trên các cuộc hội thoại ChatGPT. Để đạt được mục tiêu này, chúng tôi đã tinh chỉnh VICUNA bằng cách sử dụng một bộ sưu tập tập dữ liệu hướng dẫn tùy chỉnh có tên FLAN-MINI. Bộ sưu tập này bao gồm một tập con của tập dữ liệu hướng dẫn quy mô lớn được gọi là FLAN, cũng như các tập dữ liệu liên quan đến mã code và tập dữ liệu hội thoại được rút ra từ ChatGPT/GPT-4. Tập dữ liệu này bao gồm một số lượng lớn các nhiệm vụ

Bản thảo. Đang trong quá trình đánh giá.arXiv:2307.02053v1 [cs.CL] 5 Jul 2023

--- TRANG 2 ---

đòi hỏi kỹ năng giải quyết vấn đề. Các phát hiện thực nghiệm của chúng tôi chỉ ra rõ ràng rằng khả năng giải quyết vấn đề được tăng cường của mô hình FLACUNA được thu được thông qua việc tinh chỉnh VICUNA trên tập dữ liệu FLAN, dẫn đến những cải thiện đáng kể trên nhiều tập dữ liệu chuẩn trong INSTRUCT EVAL. FLACUNA được công khai tại https://huggingface.co/declare-lab/flacuna-13b-v1.0.

## 1 Giới thiệu

ChatGPT và người kế nhiệm GPT-4 đã vượt qua các mô hình tiên tiến trước đó của chúng trong phần lớn các nhiệm vụ và tập dữ liệu đánh giá chuẩn. Tuy nhiên, để bảo vệ quyền riêng tư, việc chạy natively một mô hình có kích thước 175B+ như GPT-3 vượt quá khả năng của hầu hết các tổ chức, chứ đừng nói đến các cá nhân. Điều này đã thúc đẩy nhiều nhà nghiên cứu tinh chỉnh các LLM có kích thước có thể quản lý được — từ 7B đến 30B trên một tập đa dạng các ví dụ hướng dẫn được tạo ra bởi ChatGPT hoặc GPT-4. Điều này đã sinh ra các LLM như Alpaca [Taori et al., 2023] và VICUNA [Chiang et al., 2023] là các checkpoint được tinh chỉnh của LLaMA [Touvron et al., 2023]. Những mô hình này đã đạt được hiệu suất gần với mức độ ChatGPT trên một số nhiệm vụ đánh giá chuẩn cụ thể, nhưng tổng thể khả năng tổng quát hóa vẫn còn khó nắm bắt. Các công trình gần đây như INSTRUCT EVAL [Chia et al., 2023] gợi ý mạnh mẽ rằng các tập dữ liệu tinh chỉnh quyết định hiệu suất cụ thể cho từng nhiệm vụ. Chẳng hạn, đã quan sát thấy rằng FLAN-T5— một checkpoint T5 được tinh chỉnh trên tập dữ liệu hướng dẫn FLAN Collection — vượt trội so với VICUNA và Alpaca trong các nhiệm vụ liên quan đến kỹ năng lý luận và giải quyết vấn đề mạnh. Điều này đã thúc đẩy chúng tôi tinh chỉnh VICUNA trên tập dữ liệu FLAN-MINI Collection, dự đoán sự cải thiện trên các nhiệm vụ đòi hỏi lý luận cao trong INSTRUCT EVAL [Chia et al., 2023].

Để đạt được mục tiêu này, trước tiên chúng tôi lấy mẫu một tập dữ liệu hướng dẫn có kích thước 1M từ tập dữ liệu FLAN Collection có kích thước 15M [Longpre et al., 2023] và kết hợp nó với một số tập dữ liệu khác bao gồm các nhiệm vụ lập trình và các cuộc hội thoại được chưng cất từ ChatGPT/GPT-4. Tập dữ liệu nhỏ hơn kết quả, FLAN-MINI, sau đó được chuyển đổi thành định dạng hội thoại của VICUNA. Để đảm bảo chi phí tính toán hợp lý cho quá trình tinh chỉnh, chúng tôi cải tiến adapter LoRA [Hu et al., 2021] vào decoder-transformer LLaMA [Touvron et al., 2023] của VICUNA. Sau quá trình tinh chỉnh LoRA hiệu quả về tham số của checkpoint VICUNA trên FLAN-MINI, chúng tôi thu được FLACUNA. Như mong đợi, FLACUNA vượt trội so với VICUNA với một biên độ đáng kể trên hầu hết các tập dữ liệu chuẩn, đặc biệt là cho các nhiệm vụ đòi hỏi lý luận cao. Tuy nhiên, hiệu suất của FLACUNA vẫn còn thấp hơn FLAN-T5 trên các benchmark lý luận tương tự. Điều này có thể được quy cho tập dữ liệu hướng dẫn nhỏ hơn 15 lần có thể chứa ít mẫu đa dạng hơn. Hơn nữa, việc tinh chỉnh đầy đủ VICUNA có thể thu hẹp khoảng cách với FLAN-T5.

Công trình này tổng thể có những đóng góp sau:
1. Cải thiện khả năng giải quyết vấn đề của VICUNA thông qua tinh chỉnh hiệu quả về tham số trên FLAN-MINI.
2. Giới thiệu một tập dữ liệu tinh chỉnh hướng dẫn, FLAN-MINI, bao gồm một tập đa dạng các nhiệm vụ và mẫu template.

## 2 Chi tiết Huấn luyện

**Chuẩn bị FLAN-MINI Collection.** Cho trước kích thước khổng lồ của FLAN Collection [Longpre et al., 2023], chúng tôi đã chọn làm việc với một tập con được lựa chọn cẩn thận duy trì mức độ đa dạng nhiệm vụ cao trong khi giảm kích thước tập dữ liệu tổng thể. Trong Bảng 1, chúng tôi trình bày các nhiệm vụ cụ thể được bao gồm trong tập con FLAN của chúng tôi, cùng với kích thước tập dữ liệu tương ứng của chúng. Vì việc phát hành công khai của FLAN Collection không bao gồm các nhiệm vụ lập trình, chúng tôi bổ sung bộ sưu tập với các tập dữ liệu code hiện có. Cụ thể, chúng tôi bao gồm CodeContests [Li et al., 2022a], APPS [Hendrycks et al., 2021a] và CodeSearchNet [Husain et al., 2019a]. Theo pipeline xử lý dữ liệu của FLAN Collection, chúng tôi lấy mẫu một số lượng cố định các ví dụ từ mỗi tập dữ liệu, trong đó mỗi ví dụ được tăng cường ngẫu nhiên với các template prompt khác nhau. Cụ thể, các ví dụ được xử lý với một nhóm các template prompt được chế tạo thủ công và có thể được sử dụng như các ví dụ zero-shot hoặc được nhóm lại với các demonstration few-shot [Longpre et al., 2023].

**Duy trì Khả năng Trò chuyện của VICUNA.** VICUNA đã thể hiện khả năng trò chuyện đáng chú ý, đạt được 90% hiệu suất của ChatGPT. Điều này cho thấy tiềm năng đáng kể của nó như một lựa chọn mã nguồn mở thay thế cho các mô hình ngôn ngữ lớn (LLM) mã nguồn đóng như ChatGPT. Để đảm bảo

--- TRANG 3 ---

| Tên Tập dữ liệu | Nguồn | Kích thước Tập dữ liệu |
|---|---|---|
| Flan2021 | FLAN | 388K |
| Public Pool of Prompts | FLAN | 320K |
| Natural instructions v2 | FLAN | 200K |
| CoT | FLAN | 100K |
| Code Search | Husain et al. [2019b] | 100K |
| Code Contest | Li et al. [2022b] | 50K |
| Apps | Hendrycks et al. [2021b] | 50K |
| GPT4-Alpaca | GPT-4 | 52K |
| Code-Alpaca | ChatGPT | 20K |
| ShareGPT | ChatGPT | 60K |
| Tổng cộng | - | 1.34M |

Bảng 1: FLAN-MINI Collection, được sử dụng để huấn luyện FLACUNA.

rằng FLACUNA giữ lại kiến thức đã học và khả năng trò chuyện của VICUNA, chúng tôi đã kết hợp các tập dữ liệu ChatGPT khác nhau, bao gồm Alpaca [Taori et al., 2023], Code Alpaca [Chaudhary, 2023], và ShareGPT [Chiang et al., 2023], vào bộ sưu tập FLAN của chúng tôi. Trong ba tập dữ liệu này, VICUNA ban đầu được tinh chỉnh bằng cách sử dụng tập dữ liệu ShareGPT. Bộ sưu tập cuối cùng sau đó được sử dụng để huấn luyện FLACUNA.

**Kiến trúc.** Chúng tôi sử dụng LORA trong mô hình VICUNA để tinh chỉnh trên bộ sưu tập FLAN-MINI. Chúng tôi chèn các adapter rank thấp vào tất cả các lớp projection query và value, dẫn đến tổng số tham số có thể huấn luyện là 6.55M, chỉ chiếm khoảng 0.05% số tham số của mô hình VICUNA 13B gốc. Độ dài chuỗi đầu vào tối đa được đặt là 1280, và việc huấn luyện hiệu quả được tạo điều kiện bằng cách sử dụng độ chính xác bf16.

**Chi tiết Siêu tham số.** FLACUNA được huấn luyện trên 4 × A6000 GPU trong 1 epoch. Chúng tôi sử dụng 16 bước tích lũy gradient với kích thước batch trên mỗi thiết bị là 2, dẫn đến tổng kích thước batch là 128. Chúng tôi sử dụng 3000 bước warm-up và tốc độ học 2e-5.

## 3 Nhiệm vụ Đánh giá và Kết quả

### 3.1 Đánh giá Giải quyết Vấn đề

Để đánh giá năng lực giải quyết vấn đề của các mô hình ngôn ngữ lớn (LLM) được hướng dẫn, INSTRUCT EVAL sử dụng một loạt các benchmark bao gồm các kỳ thi thực tế khám phá các chủ đề đa dạng. Những benchmark này bao gồm các hướng dẫn phức tạp, bài toán số học, thách thức lập trình, và các nhiệm vụ lý luận nhân quả. Để vượt trội trong các benchmark này, các mô hình cần thể hiện sự hiểu biết sâu sắc về thế giới, chứng minh khả năng lý luận multi-hop, thể hiện sự sáng tạo, và sử dụng vô số kỹ năng nhận thức khác.

**Kiến thức Thế giới.** Benchmark Massive Multitask Language Understanding (MMLU), được giới thiệu trong công trình của Hendrycks et al. [2021c], phục vụ như một công cụ đánh giá để đo lường năng lực giải quyết vấn đề và kiến thức thế giới của các mô hình ngôn ngữ trên các chủ đề khác nhau. Nó cung cấp các đánh giá trong cả setting zero-shot và few-shot, trình bày một kịch bản đánh giá thách thức và giống con người hơn. Benchmark MMLU bao gồm một phạm vi toàn diện gồm 57 chủ đề trải dài STEM, nhân văn, khoa học xã hội, và các lĩnh vực khác. Mức độ khó của các nhiệm vụ trong benchmark thay đổi từ cấp độ cơ bản đến cấp độ chuyên nghiệp nâng cao, cung cấp một đánh giá toàn diện về khả năng của mô hình trong giải quyết vấn đề và hiểu biết lĩnh vực.

**Hướng dẫn Phức tạp.** Tập con được biết đến với tên BIG-Bench Hard (BBH) bao gồm 23 nhiệm vụ đòi hỏi cao được lựa chọn cẩn thận từ benchmark BIG-Bench [Srivastava et al., 2022] để nhắm mục tiêu cụ thể các nhiệm vụ được coi là vượt quá khả năng hiện tại của các mô hình ngôn ngữ [Suzgun et al., 2022]. BBH trình bày cho các mô hình những hướng dẫn phức tạp đòi hỏi kỹ năng nâng cao trong điều hướng, suy luận logic, và phát hiện ngụy biện.

--- TRANG 4 ---

**Hiểu biết và Số học.** Discrete Reasoning Over Paragraphs (DROP) là một nhiệm vụ đọc hiểu với trọng tâm toán học. Nó thách thức các hệ thống tham gia vào lý luận rời rạc bằng cách phân tích các đoạn văn được trích xuất từ các bài viết Wikipedia. Để vượt trội trong nhiệm vụ DROP, một hệ thống cần điều hướng khéo léo các tham chiếu trong một câu hỏi và xác định các phần phù hợp của đoạn văn được cung cấp. Ngoài ra, hệ thống phải chứng minh thành thạo trong việc thực hiện các phép toán rời rạc như cộng, đếm, hoặc sắp xếp.

**Lập trình.** HumanEval phục vụ như một benchmark giải quyết vấn đề được thiết kế đặc biệt để đánh giá hiệu suất của các mô hình ngôn ngữ lớn được huấn luyện trên code [Chen et al., 2021]. Benchmark bao gồm 164 bài toán lập trình độc đáo, bao gồm các lĩnh vực như hiểu biết ngôn ngữ, thuật toán, và toán học cơ bản. Một số bài toán được bao gồm trong HumanEval có tính chất tương tự như các câu hỏi phỏng vấn phần mềm đơn giản. Trong quá trình đánh giá, các mô hình được đánh giá dựa trên tính đúng đắn chức năng của các chương trình code chúng tạo ra, với tiêu chí về tính đúng đắn được xác định bởi các docstring đã cho. HumanEval cung cấp một khung đánh giá toàn diện để đánh giá khả năng giải quyết vấn đề của các mô hình ngôn ngữ trong bối cảnh tập trung vào code.

**Nhân quả.** Benchmark Counterfactual Reasoning Assessment (CRASS) là một tập dữ liệu mới và công cụ đánh giá được phát triển đặc biệt để đánh giá khả năng lý luận nhân quả của các mô hình ngôn ngữ lớn. Bằng cách sử dụng các kịch bản phản thực tế, CRASS kiểm tra khả năng của mô hình trong việc xác định và lựa chọn các giải thích nhân quả phù hợp. Benchmark này cung cấp một khung đánh giá độc đáo và nghiêm ngặt để đo lường khả năng lý luận nhân quả của các mô hình ngôn ngữ.

### 3.2 Căn chỉnh với Giá trị Con người

Lưu ý tầm quan trọng của việc căn chỉnh các LLM với giá trị con người, INSTRUCT EVAL kết hợp benchmark Helpful, Honest, and Harmless (HHH) [Askell et al., 2021]. Benchmark này trình bày các cuộc đối thoại hấp dẫn giữa con người và các trợ lý hội thoại, thách thức mô hình phân biệt và cung cấp phản hồi phù hợp nhất. Nó bao gồm một mảng đa dạng gồm 61 mẫu liên quan đến tính trung thực, 59 mẫu liên quan đến tính hữu ích, và 58 mẫu liên quan đến tính vô hại, cùng với 43 trường hợp độc đáo thuộc danh mục "khác". Việc bao gồm danh mục "khác" tính đến các ví dụ thể hiện các giá trị không được bao gồm rõ ràng bởi tính trung thực, tính hữu ích, hoặc tính vô hại.

### 3.3 Thí nghiệm Viết

Đối với thí nghiệm viết, chúng tôi sử dụng tập dữ liệu IMPACT, có sẵn trong INSTRUCT EVAL. Tập dữ liệu toàn diện này bao gồm 50 prompt trên các danh mục riêng biệt, cụ thể là thông tin, chuyên nghiệp, tranh luận, và sáng tạo. Tiếp theo đó, ChatGPT được giao trách nhiệm chấm điểm phản hồi của các mô hình về mặt tính liên quan (Rel.) và tính mạch lạc (Coh.) trên thang điểm từ 1 đến 5. Để biết thông tin toàn diện hơn về đánh giá này, chúng tôi giới thiệu độc giả đến Chia et al. [2023].

### 3.4 Kết quả

**Baseline So sánh.** Như baseline, chúng tôi đã chọn VICUNA [Zheng et al., 2023] và STABLE VICUNA¹.

**Giải quyết Vấn đề Few-shot.** Chúng tôi trình bày kết quả của FLACUNA trên năm tập dữ liệu (xem Bảng 2) từ benchmark INSTRUCT EVAL, tập trung vào các nhiệm vụ giải quyết vấn đề. Trong 4 trong 5 nhiệm vụ, FLACUNA vượt trội so với VICUNA, cho thấy cải thiện hiệu suất trung bình 5.6 điểm so với backbone LLaMA. Tuy nhiên, nó hoạt động hơi kém hơn trên các nhiệm vụ giải quyết vấn đề liên quan đến code trong tập dữ liệu HumanEval, với biên độ 0.6 điểm. Tổng thể, sự cải thiện trong FLACUNA so với VICUNA là 5.1 điểm trung bình trên năm nhiệm vụ.

Trong số năm tập dữ liệu giải quyết vấn đề, một trong số chúng, DROP, được phân loại là tập dữ liệu held-in. Nó là một phần của bộ sưu tập FLAN của chúng tôi và được sử dụng để huấn luyện FLACUNA. Kết quả là, chúng tôi quan sát được một sự cải thiện hiệu suất đáng kể 11 điểm so với VICUNA. Các tập dữ liệu còn lại được coi là held out.

¹https://huggingface.co/CarperAI/stable-vicuna-13b-delta

--- TRANG 5 ---

| Mô hình | Kích thước | MMLU (5-shot) | BBH (3-shot) | DROP ⋆(3-shot) | CRASS (3-shot) | HumanEval (0-shot) | Avg. |
|---|---|---|---|---|---|---|---|
|  |  | Perf. | ∆ | Perf. | ∆ | Perf. | ∆ | Perf. | ∆ | Perf. | ∆ | Perf. | ∆ |
| GPT-4 | - | 86.4 | - | - | - | 80.9 | - | - | - | 67.0 | - | - | - |
| ChatGPT | - | 70.0 | - | 49.5 | - | 64.1 | - | 90.5 | - | 48.1 | - | 64.5 | - |
| Flan-UL2 | 20B | 55.0 | - | 44.7 | - | 64.3 | - | 94.2 | - | 0.0 | - | 51.6 | - |
| Alpaca-Lora | 30B | 58.4 | +0.6 | 41.3 | +2.0 | 45.1 | -0.3 | 79.2 | +10.6 | 18.9 | +4.9 | 48.6 | +3.6 |
| OpenAssistant | 30B | 56.9 | -0.9 | 39.2 | -0.1 | 46.0 | +0.6 | 67.2 | +1.4 | 23.1 | +9.1 | 46.5 | +1.5 |
| OPT-IML | 30B | 38.6 | +11.3 | 31.3 | +3.0 | 47.5 | +28.0 | 67.2 | +32.5 | 9.1 | +7.9 | 38.7 | +16.5 |
| Flan-T5 | 11B | 54.5 | +29.3 | 43.9 | +13.6 | 67.2 | +49.7 | 88.3 | +54.7 | 0.0 | +0.0 | 50.8 | +29.5 |
| Flan-Alpaca | 11B | 50.9 | +25.7 | 23.3 | -7.0 | 62.3 | +44.8 | 90.2 | +56.6 | 0.0 | +0.0 | 45.3 | +24.0 |
| Dolly V2 | 12B | 25.6 | -1.3 | 29.7 | +0.2 | 16.6 | -0.5 | 35.8 | +1.1 | 8.5 | -0.6 | 23.2 | -0.7 |
| Flan-T5 | 3B | 49.2 | +25.9 | 40.2 | +15.9 | 56.3 | +43.7 | 91.2 | +60.2 | 0.0 | +0.0 | 47.4 | +29.2 |
| ChatGLM | 6B | 36.1 | - | 31.3 | - | 44.2 | - | 51.1 | - | 3.1 | - | 33.2 | - |
| Mosaic-Chat | 7B | 37.1 | +1.9 | 32.0 | +1.1 | 20.2 | -7.4 | 47.5 | +13.6 | 17.7 | +7.4 | 30.9 | +3.3 |
| STABLE VICUNA | 13B | 49.2 | +3.0 | 37.5 | +0.4 | 34.3 | -1.0 | 67.5 | +8.7 | 15.9 | +2.5 | 40.9 | +2.7 |
| VICUNA | 13B | 50.6 | +4.5 | 37.6 | +0.5 | 32.6 | -3.0 | 60.9 | +2.1 | 11.6 | -1.8 | 38.7 | +0.5 |
| FLACUNA | 13B | 51.1 | +5.0 | 39.3 | +2.2 | 43.6 | +8.0 | 74.1 | +15.3 | 11.0 | -2.4 | 43.8 | +5.6 |

Bảng 2: Kết quả đánh giá cho các benchmark giải quyết vấn đề. Chúng tôi ký hiệu hiệu suất gốc trên các benchmark là Perf., trong khi ∆ ký hiệu sự thay đổi hiệu suất so với các LLM nền tảng tương ứng. ⋆ chỉ ra rằng DROP là một tập dữ liệu held-in.

| Mô hình | Kích thước | MMLU (0-shot) | BBH (0-shot) | CRASS (0-shot) |
|---|---|---|---|---|
| Flan-UL2 | 20B | 54.4 | 34.9 | - |
| OpenAssistant | 30B | 52.0 | 33.4 | - |
| OPT IML | 30B | 41.3 | 17.4 | - |
| TK-Instruct | 11B | 39.4 | 17.1 | - |
| Flan-T5-XXL | 11B | 54.1 | 39.5 | - |
| Dolly V2 | 12B | 25.4 | 22.3 | - |
| STABLE VICUNA | 13B | 47.5 | 18.5 | 64.2 |
| VICUNA | 13B | 48.3 | 28.3 | 65.7 |
| FLACUNA | 13B | 49.4 | 32.5 | 67.9 |

Bảng 3: Đánh giá giải quyết vấn đề 0-shot của FLACUNA và các mô hình baseline khác.

**Giải quyết Vấn đề 0-shot.** Chúng tôi đã tiến hành đánh giá hiệu suất 0-shot của FLACUNA và so sánh nó với cả VICUNA và STABLE VICUNA. Kết quả được trình bày trong Bảng 3 cho thấy một bước nhảy hiệu suất đáng chú ý của FLACUNA so với các đối thủ cạnh tranh. Sự cải thiện này có thể được quy cho việc huấn luyện FLACUNA trên tập dữ liệu hướng dẫn FLAN chất lượng cao.

**Đánh giá HHH.** Chúng tôi đã tiến hành đánh giá thêm bằng cách sử dụng tập dữ liệu đánh giá HHH của BBH (xem Bảng 4), trong đó FLACUNA thể hiện một sự cải thiện ấn tượng 11% so với VICUNA. Đáng chú ý, bộ sưu tập tập dữ liệu hướng dẫn của chúng tôi nhằm mục đích tăng cường khả năng giải quyết vấn đề của VICUNA, nhưng nó cũng có tác động tích cực đến hiệu suất HHH của nó. Quan sát này phù hợp với kinh nghiệm của FLAN-T5, đã đạt được cải thiện hiệu suất 24.2% so với backbone T5 của nó sau khi tinh chỉnh trên FLAN.

**Đánh giá Viết.** Trong khi FLACUNA chủ yếu vượt trội trong các nhiệm vụ giải quyết vấn đề, chúng tôi đã nỗ lực duy trì khả năng viết và trò chuyện ấn tượng của VICUNA. Để đạt được điều này, chúng tôi đã kết hợp các tập dữ liệu hội thoại được tạo ra bởi GPT-4, như GPT-4-Alpaca và ShareGPT, vào bộ sưu tập FLAN-MINI. Tuy nhiên, bất chấp những nỗ lực này, chúng tôi quan sát thấy một số vấn đề trong hiệu suất viết của FLACUNA. Trong một số trường hợp, nó tạo ra các đoạn code để đáp lại các prompt không liên quan đến lập trình. Chúng tôi quy hành vi này cho sự mất cân bằng dữ liệu đáng kể, trong đó tập dữ liệu hội thoại chỉ chiếm 8.2% của toàn bộ hỗn hợp dữ liệu. Các kỹ thuật kỹ thuật prompt có thể giúp khắc phục những vấn đề như vậy.

Chúng tôi phát hiện ra rằng FLACUNA tạo ra các phản hồi có chất lượng hợp lý khi được cung cấp template sau: "Một cuộc trò chuyện giữa một người dùng tò mò và một trợ lý trí tuệ nhân tạo. Trợ lý đưa ra các câu trả lời hữu ích, chi tiết và lịch sự cho các câu hỏi của người dùng. USER: định nghĩa của nhiệm vụ./n/n câu hỏi/n Output:

--- TRANG 6 ---

| Mô hình | Kích thước | Harmlessness | Helpfulness | Honesty | Other | Avg. | ∆Avg. |
|---|---|---|---|---|---|---|---|
| ChatGPT | - | 90.7 | 91.2 | 78.1 | 86.3 | 86.6 | - |
| Flan-Alpaca | 11B | 74.2 | 81.4 | 77.4 | 83.4 | 79.1 | +26.6 |
| Flan-T5 | 11B | 75.9 | 75.3 | 75.1 | 79.6 | 76.7 | +24.2 |
| Tk-Instruct | 11B | 70.1 | 54.8 | 62.3 | 76.0 | 65.8 | +13.3 |
| T5 | 11B | 46.4 | 54.8 | 58.1 | 50.7 | 52.5 | - |
| Alpaca | 13B | 49.7 | 51.2 | 51.8 | 45.5 | 49.5 | -12.3 |
| LLaMA | 13B | 57.2 | 61.0 | 57.0 | 72.0 | 61.8 | - |
| Dolly V2 | 12B | 51.7 | 59.9 | 47.0 | 58.1 | 54.2 | +9.1 |
| Pythia | 12B | 41.3 | 46.1 | 43.6 | 49.3 | 45.1 | - |
| STABLE VICUNA | 13B | 61.7 | 67.2 | 57.1 | 79.1 | 66.3 | +4.5 |
| VICUNA | 13B | 62.0 | 66.1 | 52.4 | 74.4 | 63.7 | +1.9 |
| FLACUNA | 13B | 72.4 | 71.2 | 70.5 | 83.7 | 74.5 | +12.6 |

Bảng 4: Kết quả đánh giá cho việc căn chỉnh với giá trị con người trên benchmark honesty, helpfulness, và harmlessness (HHH). Avg. ký hiệu hiệu suất trung bình, trong khi ∆Avg. ký hiệu cải thiện trung bình so với mô hình nền tảng tương ứng.

| Mô hình | Kích thước | Informative |  | Professional |  | Argumentative |  | Creative |  | Avg. |  |
|---|---|---|---|---|---|---|---|---|---|---|---|
|  |  | Rel. | Coh. | Rel. | Coh. | Rel. | Coh. | Rel. | Coh. | Rel. | Coh. |
| ChatGPT | - | 3.34 | 3.98 | 3.88 | 3.96 | 3.96 | 3.82 | 3.92 | 3.94 | 3.78 | 3.93 |
| Flan-Alpaca | 11B | 3.56 | 3.46 | 3.54 | 3.70 | 3.22 | 3.28 | 3.70 | 3.40 | 3.51 | 3.46 |
| Flan-T5 | 11B | 2.64 | 3.24 | 2.62 | 3.22 | 2.54 | 3.40 | 2.50 | 2.72 | 2.58 | 3.15 |
| Dolly-V2 | 12B | 3.54 | 3.64 | 2.96 | 3.74 | 3.66 | 3.20 | 3.02 | 3.18 | 3.30 | 3.44 |
| STABLE VICUNA | 13B | 3.54 | 3.64 | 2.96 | 3.74 | 3.30 | 3.20 | 3.02 | 3.18 | 3.21 | 3.44 |
| VICUNA | 13B | 3.60 | 3.96 | 3.74 | 3.82 | 3.82 | 3.56 | 3.82 | 3.92 | 3.75 | 3.82 |
| FLACUNA | 13B | 3.02 | 3.42 | 3.48 | 3.52 | 3.38 | 3.02 | 3.92 | 3.80 | 3.45 | 3.44 |

Bảng 5: Kết quả đánh giá cho các nhiệm vụ dựa trên viết.

ASSISTANT:". Phát hiện này gợi ý rằng với các prompt phù hợp, chúng tôi có thể cải thiện hiệu suất trò chuyện của FLACUNA.

Tuy nhiên, khi kiểm tra cẩn thận các mẫu được tạo ra, rõ ràng là FLACUNA không vượt trội so với VICUNA như một trợ lý viết. Quan sát này được củng cố bởi việc đánh giá các phản hồi được tạo ra cho các prompt trong tập dữ liệu IMPACT bằng ChatGPT, như được miêu tả trong Bảng 5. ChatGPT liên tục xếp hạng phản hồi của VICUNA cao hơn đáng kể so với những phản hồi của FLACUNA.

Hiệu suất kém của FLACUNA trong các kịch bản dựa trên viết có thể được quy cho một số yếu tố. Thứ nhất, sự khan hiếm không cân xứng của các tập dữ liệu hội thoại trong FLAN có thể đã góp phần vào kết quả này. Ngoài ra, các phương pháp tinh chỉnh hiệu quả về tham số như LORA có thể hạn chế hiệu quả của mô hình trong việc học cả khả năng giải quyết vấn đề và khả năng viết tổng quát. Do đó, chúng tôi có thể khám phá các phương pháp huấn luyện hiệu quả khác cho các LLM trong tương lai [Lv et al., 2023].

Một ví dụ về prompt và phản hồi của FLACUNA được hiển thị dưới đây.

Prompt: Một cuộc trò chuyện giữa một người dùng tò mò và một trợ lý trí tuệ nhân tạo. Trợ lý đưa ra các câu trả lời hữu ích, chi tiết và lịch sự cho các câu hỏi của người dùng. USER: Bạn được giao nhiệm vụ chứng minh kỹ năng viết của mình trong bối cảnh chuyên nghiệp hoặc công việc cho câu hỏi sau đây. Viết một hướng dẫn sử dụng cho một thiết bị gia dụng, cung cấp hướng dẫn từng bước cho việc vận hành, bảo trì và khắc phục sự cố của nó, sử dụng ngôn ngữ rõ ràng và đơn giản. Output: ASSISTANT:

Phản hồi của FLACUNA:

--- TRANG 7 ---

1. Giới thiệu
Chào mừng bạn đến với hướng dẫn sử dụng cho thiết bị gia dụng XYZ. Thiết bị này được thiết kế để làm cho cuộc sống của bạn dễ dàng và tiện lợi hơn. Với thiết bị này, bạn có thể chuẩn bị bữa ăn của mình một cách nhanh chóng và hiệu quả.

2. Vận hành
Để vận hành thiết bị, hãy làm theo các bước sau:
a. Bật thiết bị bằng cách nhấn nút nguồn. b. Chọn chế độ nấu ăn mong muốn bằng cách sử dụng bộ chọn chế độ. c. Đặt thức ăn vào bên trong thiết bị. d. Đóng nắp và nhấn nút bắt đầu. e. Chờ thiết bị hoàn thành quá trình nấu ăn.

3. Bảo trì
Để giữ cho thiết bị của bạn trong tình trạng hoạt động tốt, hãy làm theo các bước sau:
a. Làm sạch thiết bị sau mỗi lần sử dụng. b. Kiểm tra thiết bị để tìm bất kỳ dấu hiệu hư hỏng hoặc hao mòn nào. c. Nếu bạn nhận thấy bất kỳ vấn đề nào, hãy liên hệ với nhà sản xuất hoặc kỹ thuật viên sửa chữa được chứng nhận.

4. Khắc phục sự cố
Nếu bạn gặp bất kỳ vấn đề nào với thiết bị của mình, hãy thử các bước khắc phục sự cố sau:
a. Kiểm tra dây nguồn và phích cắm để đảm bảo chúng được kết nối chắc chắn. b. Kiểm tra cầu chì để đảm bảo nó không bị cháy. c. Kiểm tra hướng dẫn sử dụng thiết bị để tìm mẹo khắc phục sự cố. d. Nếu vấn đề vẫn tiếp tục, hãy liên hệ với nhà sản xuất hoặc kỹ thuật viên sửa chữa được chứng nhận.

5. Biện pháp An toàn
Luôn tuân thủ các biện pháp an toàn này khi sử dụng thiết bị:
a. Giữ thiết bị sạch sẽ và không có mảnh vụn. b. Không sử dụng thiết bị nếu nó bị hư hỏng hoặc có dấu hiệu hao mòn. c. Không để thiết bị không có người trông coi khi nó đang hoạt động. d. Luôn sử dụng thiết bị theo hướng dẫn của nhà sản xuất.

6. Kết luận
Chúc mừng bạn với thiết bị gia dụng XYZ mới của mình! Với việc sử dụng và bảo trì đúng cách, thiết bị của bạn sẽ cung cấp cho bạn nhiều năm dịch vụ đáng tin cậy. Nếu bạn có bất kỳ câu hỏi hoặc mối quan tâm nào, vui lòng tham khảo hướng dẫn sử dụng thiết bị hoặc liên hệ với nhà sản xuất.

## 4 Hạn chế và Công việc Tương lai

Bất chấp những tiến bộ đầy hứa hẹn của FLACUNA so với VICUNA, chúng tôi đã xác định một số vấn đề cần được giải quyết:

• Nếu FLACUNA được yêu cầu cung cấp câu trả lời mô tả cho các câu hỏi như "Trình bày các lập luận ủng hộ hoặc phản đối việc hạ thấp tuổi uống rượu," FLACUNA tạo ra các đoạn code thay thế. Hành vi này có thể được quy cho sự hiểu biết không hoàn hảo về hướng dẫn hoặc xu hướng ảo giác.

• FLACUNA vẫn còn thua kém đáng kể so với FLAN-T5 về khả năng giải quyết vấn đề.

• Đáng ngạc nhiên, FLACUNA thể hiện hiệu suất kém hơn so với cả LLAMA và VICUNA trên các vấn đề liên quan đến lập trình. Kết quả này là bất ngờ, xem xét rằng chúng tôi đã kết hợp nhiều tập dữ liệu giải quyết vấn đề lập trình vào bộ sưu tập tinh chỉnh hướng dẫn của chúng tôi.

• FLACUNA được huấn luyện với độ dài chuỗi đầu vào tối đa là 1280 điều này hạn chế khả năng hiểu các chuỗi đầu vào dài hơn.

Để giải quyết những hạn chế và vấn đề đã biết này, chúng tôi có thể khám phá các bước sau:

• Dựa trên các nghiên cứu trước đây, đã quan sát thấy rằng LoRA hoạt động tốt hơn với các mô hình lớn hơn [Chia et al., 2023], như những mô hình có 30B hoặc 65B tham số, và vượt trội trong các setting cụ thể cho nhiệm vụ. Do đó, trong công việc tương lai, chúng tôi có thể tăng cường FLACUNA bằng cách tinh chỉnh đầy đủ VICUNA, không có

--- TRANG 8 ---

LoRA, đặc biệt trên bộ sưu tập FLAN. Một công việc tương lai khác là huấn luyện FLACUNA trên độ dài token dài hơn.

• Chúng tôi có thể kết hợp bộ sưu tập FLAN gốc vào quá trình huấn luyện, vì nó lớn hơn mười lăm lần so với tập dữ liệu hướng dẫn chúng tôi đã sử dụng trong nghiên cứu này. FLAN-T5 đã trải qua huấn luyện trên bộ sưu tập rộng lớn này, điều này dẫn đến hiệu suất giải quyết vấn đề đáng chú ý.

• Hiệu suất trò chuyện hoặc viết của FLACUNA có thể được cải thiện bằng cách kết hợp các tập dữ liệu hội thoại lớn hơn trong FLAN-MINI và sau đó huấn luyện FLACUNA trên đó.

## Tài liệu Tham khảo

Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic evaluation of instruction-tuned large language models, 2023.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/stanford_alpaca.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://vicuna.lmsys.org.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. Science, 378(6624):1092–1097, dec 2022a. doi: 10.1126/science.abq1158. URL https://doi.org/10.1126%2Fscience.abq1158.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. ArXiv, abs/2105.09938, 2021a.

Hamel Husain, Hongqi Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. ArXiv, abs/1909.09436, 2019a.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019b.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022b.

--- TRANG 9 ---

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021b.

Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/codealpaca, 2023.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021c. URL https://openreview.net/forum?id=d7KBjmI3GmQ.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita

--- TRANG 10 ---

Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramón Risco Delgado, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022.

Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv, abs/2210.09261, 2022.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-tuning for large language models with limited resources, 2023.
