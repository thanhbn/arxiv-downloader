# 2308.12711.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2308.12711.pdf
# File size: 1904748 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Harnessing the Power of David against Goliath: Exploring Instruction Data
Generation without Using Closed-Source Models
Yue Wang1,2, Xinrui Wang1, Juntao Li1∗, Jinxiong Chang2,
Qishen Zhang2,Zhongyi Liu2,Guannan Zhang2,Min Zhang1
1School of Computer Science and Technology, Soochow University
2Ant Group
ywangnlp@stu.suda.edu.cn
Abstract
Instruction tuning is instrumental in enabling
Large Language Models (LLMs) to follow user
instructions to complete various open-domain
tasks. The success of instruction tuning de-
pends on the availability of high-quality instruc-
tion data. Owing to the exorbitant cost and
substandard quality of human annotation, re-
cent works have been deeply engaged in the ex-
ploration of the utilization of powerful closed-
source models to generate instruction data auto-
matically. However, these methods carry poten-
tial risks arising from the usage requirements of
powerful closed-source models, which strictly
forbid the utilization of their outputs to develop
machine learning models. To deal with this
problem, in this work, we explore alternative
approaches to generate high-quality instruction
data that do not rely on closed-source models.
Our exploration includes an investigation of
various existing instruction generation meth-
ods, culminating in the integration of the most
efficient variant with two novel strategies to
enhance the quality further. Evaluation results
from two benchmarks and the GPT-4 model
demonstrate the effectiveness of our generated
instruction data, which can outperform Alpaca,
a method reliant on closed-source models. We
hope that more progress can be achieved in
generating high-quality instruction data with-
out using closed-source models.
1 Introduction
Instruction tuning has received a wide range of
attention from the Natural Language Process-
ing (NLP) community (Mishra et al., 2022; Wei
et al., 2021; Sanh et al., 2021; Chung et al., 2022;
Ouyang et al., 2022; Wang et al., 2022a). Through
the utilization of various types of instruction data,
LLMs can fully exploit the knowledge obtained
in the pre-training stage, leading to the superior
∗Corresponding author. Work is done during the intern-
ship of Yue Wang at Ant Group.
Figure 1: Comparison of different instruction data gen-
eration methods. We focus on generating high-quality
instruction cheaply without using closed-source models.
performance of generalization capability across un-
seen tasks. This ability to transcend task-specific
limitations illustrates the potential of LLMs for
Artificial General Intelligence (AGI).
The quality of data is paramount to the success
of instruction tuning (Zhou et al., 2023; Chen et al.,
2023). Existing instruction datasets fall into two
categories: those created through manual annota-
tion and those generated by models. The former
method involves the manual conversion of existing
NLP datasets into an instruction format, but this
approach is constrained by high cost and variable
quality (Wang et al., 2022a; Honovich et al., 2022).
The latter method existing methods, on the other
hand, typically use powerful closed-source models
to generate instruction datasets (Wang et al., 2022a;
Honovich et al., 2022; Taori et al., 2023; Köksal
et al., 2023; Yin et al., 2023). Unfortunately, the us-
age requirements of powerful closed-source models
usually restrict using their outputs for developing
machine learning models, e.g., OpenAI1, Google2
and Anthropic3, introducing potential risks to ex-
1https://openai.com/policies/terms-of-use
2https://policies.google.com/terms/
generative-ai
3https://vault.pactsafe.io/s/
9f502c93-cb5c-4571-b205-1e479da61794/legal.html#
termsarXiv:2308.12711v1  [cs.CL]  24 Aug 2023

--- PAGE 2 ---
isting model generation methods. Therefore, the
development of a novel method to automatically
generate high-quality instruction data without rely-
ing on closed-source models becomes imperative.
However, fulfilling this demand is challenging due
to the need for high-quality instruction data that
meets multiple criteria including instruction diver-
sity, output validity, and tight alignment between
instruction and output. Due to the limited capabili-
ties of open-source models, it is more challenging
to address these problems.
To address this problem, in this work, we first
explore different variants of the existing instruction
data generation methods when do not use closed-
source models. We then introduce a novel frame-
work for instruction data generation that does not
rely on closed-source models, as shown in Figure 2.
Our framework consists of three main components:
the training of the instruction generation model,
the generation of the instruction data, and the fil-
tering of generated instructions. Specifically, we
first explore the effectiveness of different variants
of existing instruction generation methods without
using closed-source models. Subsequently, to en-
hance the alignment between generated instructions
and selected outputs, we propose a novel instruc-
tion filtering strategy to select the most appropri-
ate instruction from a pool of generated candidate
instructions. Moreover, to improve the diversity
of instruction, we introduce a novel extract-then-
generate strategy, which extracts varied segments
from the existing passages to produce more diverse
instructions. Experimental results on two instruc-
tion tuning benchmarks confirm the effectiveness
of our generated data, which outperform total nine
instruction datasets, including in-domain datasets
and those generated by closed-source models. Fur-
ther evaluations based on GPT-4 demonstrate our
data can slightly exceed the performance of Alpaca.
In conclusion, the contributions of this work are
as follows:
•We first explore the potential of generating
instruction data without using closed-source
models by studying different variants of exist-
ing instruction data generation methods;
•We propose a novel instruction filtering strat-
egy to ensure the alignment between the gen-
erated instructions and outputs. To improve
the diversity of instructions, we also propose
an extract-then-generate strategy;Dataset Number Avg. Ins. Len. Avg. Out. Len. Task Type
SUPERNI 11,810 186.62 5.86 Classification&Generation
LongForm 2,045 129.94 300.86 Long Text Generation
Table 1: The statistics of the SUPERNI and LongForm
test sets. Avg. Ins. Len. denotes the average length
of instructions. Avg. Out. Len. denotes the average
length of golden outputs.
•Benchmark and GPT-4 evaluation results con-
firm the effectiveness of open-source mod-
els in generating high-quality instruction data,
delivering results on par with closed-source
models generated datasets.
2Study on Adapting Existing Instruction
Generation Approaches
In this section, we first introduce the experimental
settings and then explore some variants of existing
instruction generation methods when do not use
closed-source models.
2.1 Implementation Details
We implement all models with the open-source
toolkit Transformers4(Wolf et al., 2020). In the
fine-tuning stage, we follow Taori et al. (2023) use
AdamW (Loshchilov and Hutter, 2019) optimizer
and set the learning rate to 2e-5, batch size to 128,
learning rate warmup ratio to 0.03, and perform
training for 3 epochs. We utilize LLaMA-7B (Tou-
vron et al., 2023) as the backbone model and update
all parameters during the fine-tuning stage. All the
fine-tuning experiments are performed with 80GB
NVIDIA A100 GPUs. During the instruction gener-
ation stage, we employ Nucleus Sampling (Holtz-
man et al., 2019) to generate diverse instruction
data. Specifically, we set the Top-p to 0.9, the
Top-k to 40, and the temperature to 0.7.
2.2 Automatic Evaluation Benchmarks
To evaluate different instruction methods automati-
cally and comprehensively, we select two popular
benchmarks with different characteristics: SUPER-
NATURALINSTRUCTIONS (SUPERNI) (Wang
et al., 2022b) and LongForm (Köksal et al., 2023).
We show the statistic of two benchmarks in Ta-
ble 1. SUPERNI is a large-scale instruction tuning
benchmark, encompassing 1,616 NLP tasks. To
ensure consistency, we use the official test set split-
ting (Wang et al., 2022b). The test set consists
of 15,310 instances, categorized into 12 task cate-
gories and 154 tasks in total. These tasks include
4https://github.com/huggingface/transformers

--- PAGE 3 ---
Figure 2: An illustration of our instruction data generation framework, which does not use closed-source models
throughout the process. Specifically, in Step 1 , we reverse the instruction and output of original instruction data to
train an instruction generation model; in Step 2 , we use the fragments collected from the existing corpus as the
output and use the instruction generation model to generate candidate instructions for these selected outputs; in
Step 3 , we propose a novel instruction filtering strategy to select the most appropriate instruction; in Step 4 , we use
the filtered instruction data to train the final instruction following model.
a wide range of classification and generation tasks
at the word, sentence, and document levels, with
a maximum of 100 instances per task. LongForm
is a long text generation instruction benchmark,
which consists of 2,045 examples and is generated
from OpenAI ‘text-davinci-003’ model according
to English Wikipedia or C4 corpus, or natural in-
struction corpus (Stack Exchange (Gao et al., 2020)
and WikiHow (Koupaee and Wang, 2018)) or exist-
ing NLP tasks (BigBench (Srivastava et al., 2022),
Enron (Cohen, 2015), BEA-2019 shared task on
grammatical error correction (Bryant et al., 2019)).
For the automatic metrics, we use the original eval-
uation metrics (Wang et al., 2022b; Köksal et al.,
2023), i.e., ROUGE-L (Lin, 2004) score for SU-
PERNI and METEOR (Banerjee and Lavie, 2005)
score for LongForm. We show the statistic of two
benchmarks in Table 1. We can find that due to
the different task types, the two benchmarks differ
significantly in the average lengths.
2.3 Instruction Dataset Baselines
To conduct a comprehensive comparison, we
compare various instruction datasets: (1) Self-
Instruct (Wang et al., 2022a) employs vanilla
GPT3 (Brown et al., 2020) to generate instruc-
tion data automatically; (2) Alpaca (Taori et al.,
2023) introduces refinements to the pipeline of
Self-Instruct to generate more high-quality data;
(3) LLaMA-GPT4 (Peng et al., 2023) deftly lever-ages the formidable power of GPT-4 (OpenAI,
2023) in order to generate responses for the instruc-
tions of Alpaca adeptly; (4) Evol-Instruct (Xu
et al., 2023) put forth some prompt engineering
strategies aimed at enhancing the complexity of
instructions; (5) Dromedary (Sun et al., 2023)
propose elaborate prompt engineer strategies to
generate instruction data with the use of LLaMa-
65B (Touvron et al., 2023); (6) DYNASAUR (Yin
et al., 2023) leverages the power of LLMs to
generate instruction data by harnessing the meta-
data and data fields of existing NLP datasets;
(7) Dolly (Databricks, 2023) stands as a remark-
able open-source dataset, meticulously composed
of human-written instructions that cater to a wide
range of general-purpose tasks. We also compare
with the training split of (8) SUPERNI (Wang et al.,
2022b) and (9) LongForm (Köksal et al., 2023))
to compare with in-domain datasets.
2.4 Results
In Table 2, we find that almost all existing instruc-
tion datasets cannot achieve good performance on
both benchmarks. We think this phenomenon re-
sults from the significantly different statistics of
the two benchmarks, which need models to master
different abilities. Besides, both SUPERNI and
LongForm only bring significant improvements on
the corresponding test set, while bringing little im-
provements on the other test set. This phenomenon

--- PAGE 4 ---
Method Potential Risks Data Size SUPERNI LongForm
LLaMA-7B (Touvron et al., 2023) % - 9.88 13.2
+SELF-INSTRUCT (Wang et al., 2022a) ! 82K 37.58 10.5
+Alpaca (Taori et al., 2023) ! 52K 38.62 14.06
+LLaMA-GPT4 (Peng et al., 2023) ! 52K 30.22 15.78
+Evol-Instruct (Xu et al., 2023) ! 70K 21.36 16.9
+DYNASAUR (Yin et al., 2023) ! 52K 41.06 12.60
+LongForm (Köksal et al., 2023) ! 24K 18.18 18.55
+Dolly (Databricks, 2023) % 15K 20.10 13.5
+Dromedary (Sun et al., 2023) % 52K 11.56 18.70
+SUPERNI (Wang et al., 2022b) % 75K 44.66 7.7
+SUPERNI (Wang et al., 2022b)+LongForm (Köksal et al., 2023) ! 28K 35.83 14.01
+Ours (Generated from Unsupervised Corpus) % 14K 8.82 21.00
+Ours (Generated from Supervised Corpus) % 14K 45.00 9.77
+Ours (Generated from Unsupervised +Supervised Corpus) % 28K 44.88 18.84
Table 2: The performance of our generated instruction data and all instruction dataset baselines on SUPERNI
and LongForm benchmarks. Bold andUnderline indicate the best and the second best performance respectively.
Potential Risks denotes whether use closed-source models to generate instructions, which have potential risks
due to the terms of use. Following the original work (Wang et al., 2022b; Köksal et al., 2023), we report the
ROUGE-L (Lin, 2004) score for SUPERNI and METEOR (Banerjee and Lavie, 2005) score for LongForm.
further confirms our hypothesis that it requires dif-
ferent skills to deal with SUPERNI and LongForm.
2.5 Exploring the Potential of Existing
Instruction Generation Methods
Due to the usage requirements, it is forbidden to
use closed-source models to generate instruction
data. Therefore, we explore the potential of exist-
ing instruction generation methods when do not
use closed-sourced models.
Instrution Data Generation Methods We first
try to adapt existing instruction data generation
methods. Specifically, we study two typical meth-
ods: Alpaca (Touvron et al., 2023) and Long-
Form (Köksal et al., 2023). Alpaca generates in-
struction data in a step-by-step manner, starting
with the generation of instructions, followed by the
generation of inputs based on these instructions,
and ultimately producing outputs by seamlessly
combining the instructions with the corresponding
inputs. LongForm, on the other hand, first col-
lects passages from the existing corpus and uses
closed-source LLMs to generate the instruction for
these selected passages. In our preliminary experi-
ments, we observe that both Alpaca and LongForm
encounter challenges in generating high-quality in-
struction data with in-context learning when using
LLaMa-7B as backbone models. We attribute this
difficulty to the discernible ability gap between
LLaMa-7B and ChatGPT. To address this prob-
lem, we leverage existing instruction data to traina specialized instruction data generator. Specifi-
cally, we transform the existing instruction data
into the instruction generation templates of Alpaca
and LongForm to use training data to update the
parameters of LLaMa-7B. The templates used are
shown in the Appendix. We ensure that the data
used to train the instruction data generator is not
collected from the outputs of closed-source models.
However, despite updating the parameters to de-
velop a specific instruction data generator, Alpaca
still has difficulty generating high-quality data. Af-
ter rule-based data processing, the generation speed
of Alpaca is limited to a mere 30 instances per hour
on 1x 80GB NVIDIA A100 GPU. Therefore, in the
following experiments, we follow the approach of
LongForm to generate instruction data.
Training Strategy We explore the effect of dif-
ferent training strategies of the instruction gener-
ator, including instruction formats, training data
source and size. Specifically, we first study whether
the open-source instruction following model can
generate high-quality instruction data. We show
the details of instruction generation and the fol-
lowing formats in the Appendix. We also se-
lect three training data sources: (1) Seed Instruc-
tions : is introduce by Self-Instruct (Wang et al.,
2022a), which contains 175 human written instruc-
tion data; (2) Dolly (Databricks, 2023) is a human-
written general-purpose instruction dataset; (3) SU-
PERNI (Wang et al., 2022b).

--- PAGE 5 ---
Instruction Generation ModelSUPERNI LongFormTraining Strategies Training Data Source Training Data Size
Automatic Generation with Using Open-Source Models
Instruction Generation Seed Instruction 175 9.61 18.01
Instruction Generation Dolly 14,563 10.00 17.01
Instruction Generation SUPERNI 13,500 5.95 16.24
Instruction Generation Dolly+SUPERNI 28,063 10.45 16.59
Instruction Following Dolly+SUPERNI 28,063 5.89 16.79
Automatic Generation with Using Closed-Source Models
- - - 10.28 16.15
Table 3: The performance of our generated instruc-
tion data on SUPERNI and LongForm benchmarks. We
generate instructions for 13,500 unsupervised passages
collected from Wikipedia and C4 corpus. Bold and
Underline indicate the best and the second best perfor-
mance respectively. For comparison, we also report
the performance of using the OpenAI ‘text-davinci-003’
model to generate instructions for the same passages.
Instruction Generation ModelGenerated Data Size SUPERNI LongFormTraining Data Source Training Data Size
Automatic Generation without Instruction Filtering Strategy
Seed Instruction 175 13,500 18.36 4.00
Dolly 14,563 13,500 23.92 7.26
SUPERNI 175 13,500 17.62 5.56
SUPERNI 13,500 13,500 40.34 9.50
SUPERNI 13,500 73,256 37.97 8.63
Automatic Generation with Instruction Filtering Strategy
SUPERNI 13,500 13,500 45.00 9.77
SUPERNI 13,500 73,256 41.69 9.06
Manual Annotation
- - 13,500 44.03 8.05
- - 75,317 44.66 7.74
Table 4: The performance of our generated instruc-
tion data on SUPERNI and LongForm benchmarks We
generate instructions for supervised outputs collected
from SUPERNI. Bold andUnderline indicate the best
and the second best performance respectively. For com-
parison, we also report the performance of the original
SUPERNI manual annotation.
Inference Strategy We also explore the effect of
the inference data sources. Specifically, we collect
fragments from both unsupervised and supervised
corpus. For unsupervised corpus, we follow Köksal
et al. (2023) to sample 9,000 passages from the
Wikipedia corpus and 4,500 passages from the C4
corpus. For supervised corpus, we sample data
from SUPERNI (Wang et al., 2022b).
Results and Analysis We report the results of
different training and inference strategies in Ta-
ble 3 and 4. From the results in Table 3, we can
find that with only 175 instruction data, LLaMA-
7B can generate better instructions than OpenAI
‘text-davinci-003’ model for unsupervised corpus.
Table 4 shows that the instruction data generator
has difficulty in generating high-quality instruction
data for supervised corpus. We argue that the statis-
tics of different corpus cause this phenomenon. The
outputs of the supervised corpus are usually very
short, especially for answer choice tasks, whereFiltering Filtering Model SUPERNI LongForm
% - 7.61 18.01
! LLaMa-7B 8.77 21.00
! +Dolly 8.34 20.85
! +SUPERNI 8.50 20.91
! +Seed Instrcution 8.82 21.12
! +All 8.85 20.97
Table 5: The performance of our different filtering
strategies on SUPERNI and LongForm benchmarks.
We use the instruction generation model trained with
seed instructions to generate candidate instructions for
13,500 unsupervised passages collected from Wikipedia
and C4 corpus. Bold andUnderline indicate the best
and the second best performance respectively.
the answer is only one character, ‘A’ or ’B’, which
only provides limited information to the model and
is difficult to generate diverse high-quality data.
Therefore, only with the help of a filtering strategy,
combined with 13,500 training data, can achieve
comparable performance to manual annotation. In
the next section, we will introduce the instruction
filtering strategy in detail.
3 Exploring Novel Strategies
3.1 Instruction Filtering
Strategy In order to balance the diversity of in-
struction and the alignment between instruction
and output, we propose a novel Instruction Filter-
ing Strategy. The motivation of this strategy is that
the most appropriate instruction can infer the output
as the max possible. Specifically, we first gener-
ate multiple candidate instructions for one output.
Then we use the instruction following model to
calculate the perplexity (PPL) of the output given
the candidate instruction. Formally, we select the
instruction that: arg minIppl(O|I), where Ide-
notes the candidate instruction and Odenotes the
passages selected as outputs.
Results We first study the effect of different fil-
tering models. From the results in Table 5, we can
find that using the instruction filtering strategy has
a significant effect on the performance, while the
effect of different filtering models is not huge. We
think these results come from the fact that discrim-
ination is simple than generative. Therefore, to
simplify the progress, in all the following experi-
ments, we choose to use the original LLaMA-7B
model as the filtering model.

--- PAGE 6 ---
Figure 3: The performance of different extraction strate-
gies on the SUPERNI benchmark. We use different
extraction strategies to extract fragments from 13,500
unsupervised passages collected from Wikipedia and C4
corpus. Then we use the instruction generation model
trained with seed instructions to generate candidate in-
structions with these fragments.
3.2 Extract-then-Generate
Strategy From the above results, we can find
that the selected outputs have a large effect on the
quality of the generated instruction. Due to the
single distribution of the outputs we use before that
Wikipedia and C4 are too long and SUPERNI is too
short, we try to diversify the distribution of outputs.
Because the output of SUPERNI is collected from
existing NLP tasks, which means that they have
been extracted manually, we try to extract parts
from the Wikipedia and C4 passages. We propose
three strategies to extract passages:
•Keywords : We use the toolkit Yake5(Cam-
pos et al., 2018a,b, 2020) to extract the most
unique keywords from the original passage;
•Random Sentence : We extract one sentence
from the original passage randomly;
•LLM Extraction : We try to utilize LLMs
to extract the most valuable fragments from
the original passage. Specifically, we use the
LLaMA-7B-Dolly model to extract fragments.
We show the templates used for LLM Extrac-
tion in the Appendix.
Results We report the results of all the extraction
strategies in Figure 3. We can find that compared
5https://github.com/LIAAD/yake
Figure 4: Relative response quality against ChatGPT,
which is assessed by GPT-4.
to the original passages, the LLM extraction strat-
egy brings significant improvements to SUPERNI
and outperforms the other two methods. There is
a large amount of data in the unsupervised corpus,
and how to efficiently use unsupervised data is still
under-explored. We hope the extract-then-generate
strategy can shed light on generating diverse in-
struction data from unsupervised data.
4 Putting It All Together
In this section, we combine all the above strategies
together to form our final instruction data frame-
work. Figure 2 shows the illustration of our frame-
work. We conduct a comprehensive evaluation to
confirm the effectiveness of our framework.
4.1 Main Results
To compare with various baselines, we report the
performance of our framework in Table 2. We can
find that our generated instruction data can out-
perform all the baselines on SUPERNI and Long-
Form. Specifically, only the instruction data gener-
ated from an unsupervised corpus can achieve the
best performance on LongForm, while the instruc-
tion data generated from a supervised corpus can
achieve the best performance on SUPERNI. Com-
bining these two types of data can achieve a bal-
anced result on LongForm and SUPERNI, which
can outperform the same amount of the mixing of
original in-domain data significantly.
4.2 GPT-4 Evaluation
To further confirm the effectiveness of our gener-
ated data, we follow Chiang et al. (2023) to lever-
age GPT-4 (OpenAI, 2023) to conduct an auto-
matic evaluation. Specifically, GPT-4 automati-
cally scores the response quality of different mod-
els from 1 to 10 on unseen Vicuna-Instructions.

--- PAGE 7 ---
Method SUPERNI LongForm
T5-XL (Raffel et al., 2020) 0.89 4.81
+Alpaca (Taori et al., 2023) 1.77 2.50
+Dolly (Databricks, 2023) 0.96 6.62
+DYNASAUR (Yin et al., 2023) 1.30 2.09
+Dromedary (Sun et al., 2023) 0.84 4.17
+SUPERNI (Wang et al., 2022b) 1.22 0.79
+LongForm (Köksal et al., 2023) 0.94 10.22
+Ours (Wikipedia&C4 Source) 1.14 8.42
+Ours (SUPERNI Source) 1.93 3.55
+Ours (Wikipedia&C4+SUPERNI Source) 0.98 4.62
Flan-T5-XL (Chung et al., 2022) 36.54 5.11
+Alpaca (Taori et al., 2023) 22.21 10.38
+Dolly (Databricks, 2023) 31.21 6.34
+DYNASAUR (Yin et al., 2023) 35.25 5.43
+Dromedary (Sun et al., 2023) 7.83 11.64
+SUPERNI (Wang et al., 2022b) 29.15 3.63
+LongForm (Köksal et al., 2023) 40.37 7.05
+Ours (Wikipedia&C4 Source) 34.66 7.50
+Ours (SUPERNI Source) 37.70 4.83
+Ours (Wikipedia&C4+SUPERNI Source) 33.78 5.74
Table 6: The performance of our generated instruction
data and all instruction dataset baselines on SUPERNI
benchmarks based on T5-XL and Flan-T5-XL. Bold
andUnderline indicate the best and the second best
performance respectively. Following the original usage
of the automatic metric of benchmarks (Wang et al.,
2022b; Köksal et al., 2023), we report the ROUGE-
L (Lin, 2004) score for SUPERNI.
Because we do not generate code and math instruc-
tion data, we remove evaluation instructions related
to code and math and finally keep 70 instructions to
test. The evaluation results are reported in Figure 4,
which use that our generated data can outperform
Alpaca slightly. All the evaluation results show
the potential of generating high-quality instruction
data without using closed-source models.
4.3 Instruction Diversity Analysis
We also study the diversity of generated instruction
data. Following Wang et al. (2022a), we use the
Berkeley Neural Parser (Kitaev and Klein, 2018;
Kitaev et al., 2019) to extract the top 20 most com-
mon root verbs and their top 4 direct noun objects
of our generated instruction data, which are shown
in Figure 5. From the results, we can observe that
the extract-then-generate strategy can improve the
diversity of the generated instruction data of the
original passages in the unsupervised corpus.
4.4 Other Backbone Models
To confirm further the effectiveness of our gener-
ated data, we also conduct experiments on T5 (Raf-
fel et al., 2020) and Flan-T5 (Chung et al., 2022),
which is reported in Table 6. Without instruc-
tion tuning on FLAN collections, T5-XL fails onSUPERNI. Flan-T5-XL can achieve strong perfor-
mance on SUPERNI while having a similar result
to T5-XL on LongForm. We speculate that this
phenomenon results from there being a large num-
ber of classification tasks collected in the FLAN
collections, and the outputs of classification tasks
are usually too short, which may do little to help
improve the long text generation ability. Despite
the weak ability of T5-XL and Flan-T5-XL, we can
find that our generated instruction data can achieve
comparable and even better performance than the
instruction datasets using closed-source models.
5 Related Work
In recent years, instruction tuning has gained sig-
nificant attention, which can enable LLMs to gen-
eralize to unseen tasks. In this section, we divide
existing instruction datasets into two groups: man-
ual annotation instruction datasets and model gen-
eration instruction datasets. In this section, we
introduce these two types of methods.
Manual Annotation Instruction Datasets For
the purpose of generalizing LLMs to unseen tasks,
instruct tuning needs a large number of instruction
data with various task types. The most straightfor-
ward way to achieve this goal is to collect from
existing NLP datasets. Therefore, early instruc-
tion datasets are collected from large-scale exist-
ing NLP task datasets and transformed into in-
struction formats with manual written templates,
e.g., Natural Instructions (Mishra et al., 2022),
Flan 2021 (Wei et al., 2021), and T0 (Sanh et al.,
2021). With the scaling law of instruction data
that more task types and data can improve the per-
formance of instruction tuning (Xu et al., 2022),
subsequent works collect more data from existing
datasets to expand tasks to the thousands, e.g., Ze-
roPrompt (Xu et al., 2022), Super-Natural Instruc-
tions (Wang et al., 2022b) and OPT-IML (Iyer et al.,
2022). To further enrich data sources, xP3 (Muen-
nighoff et al., 2022) adds multilingual instruction
tuning; Flan Collection (Chung et al., 2022; Long-
pre et al., 2023) collects chain-of-thought data and
samples some data to be organized in the format
of in-context learning. Gu et al. (2022) leverages
human-written rules and templates to convert un-
supervised data into the instruction data format to
further raise the number of data resources. In order
to align with human requirements in realistic sce-
narios rather than collecting instruction data from
existing NLP datasets, some recent works focus

--- PAGE 8 ---
(a) SUPERNI
 (b) Wikipedia&C4
 (c) Extracted Fragments
Figure 5: The top 20 most common root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the
instruction generated from: (a) SUPERNI ;(b) Wikipedia&C4 ;(c) Fragments Extracted from Wikipedia&C4 .
on collecting instruction data from realistic scenar-
ios (Ouyang et al., 2022; Databricks, 2023).
Model Generation Instruction Datasets Lim-
ited by the high cost of manual annotation, it is un-
affordable to collect high-quality instruction data
manually. Therefore, model generation models fo-
cus on generating instruction data automatically.
Existing methods have explored the effectiveness
of employing large language models to generate
high-quality instruction data. Specifically, by pro-
viding multiple seed instruction-tuning tasks as
prompts, Self-Instruct (Wang et al., 2022a), Un-
natural Instruction (Honovich et al., 2022), and
Alpaca (Taori et al., 2023) can follow the format
of the given seed tasks to generate instruction data
automatically. Subsequently, there is a line of work
to improve the quality of generated data further.
Specifically, Xu et al. (2023) focuses on improv-
ing the task complexity of instruction data; Jiang
et al. (2023) proposes an adversarial distillation
framework to utilize the feedback of student mod-
els; Ding et al. (2023) utilizes two separate LLM
APIs to generate conversational instruction data.
Besides, relying solely on LLMs to generate both
instruction and outputs may generate low-quality
data (Köksal et al., 2023; Yang et al., 2023). To
improve the quality of generated instruction data,
LongForm (Köksal et al., 2023) is the pioneering
work leveraging LLMs to generate the instruction
based on an existing corpus. Dynasaur (Yin et al.,
2023) uses LLMs to generate instruction data with
the help of the meta-information of existing NLP
datasets. RefGPT (Yang et al., 2023) handles the
hallucination problem by constraining the LLMs to
rely on the provided reference instead of generating
dialogues based solely on their own knowledge.6 Conclusion and Future Work
Instruction tuning has gained more and more at-
tention, which uses high-quality instruction data to
enable LLMs to generalize to various tasks. Lim-
ited by the high cost, manual annotation instruction
datasets lack a large amount of high-quality instruc-
tion data. Therefore, recent works focus on lever-
aging powerful closed-source models to generate
instruction data automatically, which have achieved
huge success. However, the usage requirements of
closed-source models are usually forbidden to use
their outputs to develop machine learning models,
which causes these methods potential risks.
In this work, we first verify the potential of gen-
erating high-quality instruction datasets without
using closed-source models by studying different
strategies. Subsequently, we introduce a novel in-
struction data generation framework, consisting of
the best variants of existing methods and our novel
proposed strategies. Experimental results confirm
the effectiveness of our data, which can outperform
Alpaca in both benchmark and GPT-4 evaluation.
Despite the promising results achieved in this
work, it is still a large performance gap between
closed-source models and open-source models.
Therefore, we hope this work can prompt the devel-
opment of open-source instruction tuning models
to narrow the gap with closed-source models con-
tinuously. Besides, this work is a preliminary study
to explore the potential of generating high-quality
instruction datasets without using closed-source
models. In future work, we will try to generate
instructions with more diverse and complex task
types, such as math, code, etc. We will also gener-
alize our framework to support more languages.

--- PAGE 9 ---
Limitations
Although our instruction framework can generate
high-quality instruction data, it still has the follow-
ing limitations:
•Hallucinations : Even though our generated
data utilize the existing corpus as outputs to
ensure the validity of outputs, due to the char-
acteristic of the used backbone LLMs, it may
still have the hallucination problem;
•Instruction Diversity : Despite our proposed
strategy, the diversity of generated instruction
can be further improved. Through the diver-
sity analysis, the generated instruction data is
still far from satisfactory.
Ethics Statement
The instruction data are generated automatically
from LLMs, which do not represent the viewpoints
of the authors. Due to social bias and lack of profes-
sional knowledge, the generated data may contain
misleading and toxic information, which needs to
be addressed before being applied to realistic sce-
narios. To promote the development of instruction
generation without using closed-source models, we
will release our codes and generated data.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of
the acl workshop on intrinsic and extrinsic evaluation
measures for machine translation and/or summariza-
tion, pages 65–72.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Christopher Bryant, Mariano Felice, Øistein E Ander-
sen, and Ted Briscoe. 2019. The bea-2019 shared
task on grammatical error correction. In Proceed-
ings of the Fourteenth Workshop on Innovative Use
of NLP for Building Educational Applications , pages
52–75.
Ricardo Campos, Vítor Mangaravite, Arian Pasquali,
Alípio Jorge, Célia Nunes, and Adam Jatowt. 2020.
Yake! keyword extraction from single documents
using multiple local features. Information Sciences ,
509:257–289.Ricardo Campos, Vítor Mangaravite, Arian Pasquali,
Alípio Mário Jorge, Célia Nunes, and Adam Jatowt.
2018a. A text feature based automatic keyword ex-
traction method for single documents. In European
conference on information retrieval , pages 684–691.
Springer.
Ricardo Campos, Vítor Mangaravite, Arian Pasquali,
Alípio Mário Jorge, Célia Nunes, and Adam Jatowt.
2018b. Yake! collection-independent automatic key-
word extractor. In European Conference on Informa-
tion Retrieval , pages 806–810. Springer.
Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xi-
aomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo
Zhao. 2023. Maybe only 0.5% data is needed: A pre-
liminary exploration of low training data instruction
tuning. arXiv preprint arXiv:2305.09246 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
William W. Cohen. 2015. Enron email dataset. https:
//www.cs.cmu.edu/~enron/ .
Databricks. 2023. Databricks’ dolly, a large
language model trained on the databricks ma-
chine learning platform. https://github.com/
databrickslabs/dolly .
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi
Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,
and Bowen Zhou. 2023. Enhancing chat language
models by scaling high-quality instructional conver-
sations. arXiv preprint arXiv:2305.14233 .
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Yuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang.
2022. Learning instructions with unlabeled data for
zero-shot cross-task generalization. arXiv preprint
arXiv:2210.09175 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text de-
generation. In International Conference on Learning
Representations .
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2022. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. arXiv
preprint arXiv:2212.09689 .

--- PAGE 10 ---
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-
ter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.
2022. Opt-iml: Scaling language model instruc-
tion meta learning through the lens of generalization.
arXiv preprint arXiv:2212.12017 .
Yuxin Jiang, Chunkit Chan, Mingyang Chen, and
Wei Wang. 2023. Lion: Adversarial distillation of
closed-source large language model. arXiv preprint
arXiv:2305.12870 .
Nikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-
lingual constituency parsing with self-attention and
pre-training. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 3499–3505.
Nikita Kitaev and Dan Klein. 2018. Constituency pars-
ing with a self-attentive encoder. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 2676–2686.
Abdullatif Köksal, Timo Schick, Anna Korhonen, and
Hinrich Schütze. 2023. Longform: Optimizing in-
struction tuning for long text generation with corpus
extraction. arXiv preprint arXiv:2304.08460 .
Mahnaz Koupaee and William Yang Wang. 2018. Wiki-
how: A large scale text summarization dataset. arXiv
preprint arXiv:1810.09305 .
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 .
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470–3487, Dublin, Ireland.
Association for Computational Linguistics.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, et al. 2022. Crosslingual generaliza-
tion through multitask finetuning. arXiv preprint
arXiv:2211.01786 .
OpenAI. 2023. Gpt-4 technical report.Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. 2023. Principle-driven self-
alignment of language models from scratch with
minimal human supervision. arXiv preprint
arXiv:2305.03047 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022a. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560 .
Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran, An-
jana Arunkumar, David Stap, et al. 2022b. Super-
naturalinstructions: Generalization via declarative
instructions on 1600+ nlp tasks. In Proceedings of

--- PAGE 11 ---
the 2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 5085–5109.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2020. Transformers: State-of-the-art natural
language processing. In Proceedings of the 2020 con-
ference on empirical methods in natural language
processing: system demonstrations , pages 38–45.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023. Wizardlm: Empowering large language
models to follow complex instructions.
Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-
gang Wang, Haiyu Li, and Zhilin Yang. 2022. Zero-
prompt: Scaling prompt-based pretraining to 1,000
tasks improves zero-shot generalization. arXiv
preprint arXiv:2201.06910 .
Dongjie Yang, Ruifeng Yuan, YuanTao Fan, YiFei Yang,
Zili Wang, Shushen Wang, and Hai Zhao. 2023. Re-
fgpt: Reference-> truthful & customized dialogues
generation by gpts and for gpts. arXiv preprint
arXiv:2305.14994 .
Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal,
Jiawei Han, and Kai-Wei Chang. 2023. Dynosaur: A
dynamic growth paradigm for instruction-tuning data
curation. arXiv preprint arXiv:2305.14327 .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .

--- PAGE 12 ---
A The Diversity of SUPERNI and
LongForm
To compare the diversity of instructions, we also
visualize the statistics of SUPERNI and LongForm
test sets, which is shown in Figure 6.
B The Templates of Different Strayegies
We show the instruction following prompt of Al-
paca in Table 7, the instruction data generation
prompt of Alpaca in Table 8, the instruction data
generation prompt of LongForm in Table 9 and the
LLM extraction prompt in Table 10.

--- PAGE 13 ---
(a) SUPERNI test set.
 (b) LongForm test set.
Figure 6: The top 20 most common root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the
instruction generated from: (a) SUPERNI test set ;(b) LongForm test set .
Instruction with Input
Below is an instruction that describes a task, paired with an input that provides further context.Write a response
that appropriately completes the request.
### Instruction:
{{ instruction }}
### Input:
{{input}}
### Response:
Instruction without Input
Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{{ instruction }}
### Response:
Table 7: The instruction following prompt of Alpaca.

--- PAGE 14 ---
Instruction Date Generation Meta Prompt
You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a
GPT model and we will evaluate the GPT model for completing the instructions.
Here are the requirements:
1. Try not to repeat the verb for each instruction to maximize diversity.
2. The language used for the instruction also should be diverse. For example, you should combine questions
with imperative instrucitons.
3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended
generation, classification, editing, etc.
4. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to
create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a
reminder because it cannot perform any action.
5. The instructions should be in English.
6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.
7. You should generate an appropriate input to the instruction. The input field should contain a specific example
provided for the instruction. It should involve realistic data and should not contain simple placeholders. The
input should provide substantial content to make the instruction challenging but should ideally not exceed 100
words.
8. Not all instructions require input. For example, when a instruction asks about some general information,
"what is the highest peak in the world", it is not necssary to provide a specific context. In this case, we simply
put "<noinput>" in the input field.
9. The output should be an appropriate response to the instruction and the input. Make sure the output is less
than 100 words.
List of 20 tasks:
1. Instruction:
{instruction_example1}
1. Input:
{input_example1}
1. Output:
{output_example1}
2. Instruction:
{instruction_example2}
2. Input:
{input_example2}
2. Output:
{output_example2}
3. Instruction:
{instruction_example3}
3. Input:
{input_example3}
3. Output:
{output_example3}
4. Instruction:
Table 8: The instruction data generation prompt of Alpaca.

--- PAGE 15 ---
Template for the Instruction Style
Instruction: X
Output: <corpus_example>
What kind of instruction could this be the answer to?
X:
Template for the Informal Chatbot Style
You are a chatbot. A user sent you an informal message and your reply is as follows.
Message: X
Reply: <corpus_example>
What is the informal message X?
X:
Template for the Search Engine/query Style
You are a search engine. A person queried something in detail and the most relevant document about the query is
as follows.
Query: X
Document: <corpus_example>
What is the detailed query X?
X:
Table 9: The instruction data generation prompt of LongForm. <corpus_example> represents the selected passages.
Below is an instruction that describes a task, paired with an input that provides further context. Write a response
that appropriately completes the request.
### Instruction:
Select key segments from the given article below.
### Input:
{{input}}
### Response:
Table 10: The LLM extraction prompt.
