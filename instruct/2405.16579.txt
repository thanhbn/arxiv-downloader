# 2405.16579.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/instruct/2405.16579.pdf
# File size: 1336132 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Automatically Generating Numerous Context-Driven
SFT Data for LLMs across Diverse Granularity
Shanghaoran Quan
Beihang University
shrquan@buaa.edu.cn
Abstract
Constructing high-quality query-response pairs from custom corpus is crucial for
supervised fine-tuning (SFT) large language models (LLMs) in many applications,
like creating domain-specific AI assistants or roleplaying agents. However, sourc-
ing this data through human annotation is costly, and existing automated methods
often fail to capture the diverse range of contextual granularity and tend to produce
homogeneous data. To tackle these issues, we introduce a novel method named
AUGCON, capable of automatically generating context-driven SFT data across
multiple levels of granularity with high diversity, quality and fidelity. AUGCON
begins by generating queries using the Context-Split-Tree (CST), an innovative
approach for recursively deriving queries and splitting context to cover full granu-
larity. Then, we train a scorer through contrastive learning to collaborate with CST
to rank and refine queries. Finally, a synergistic integration of self-alignment and
self-improving is introduced to obtain high-fidelity responses.
Extensive experiments are conducted incorporating both human and automatic
evaluations, encompassing a test scenario and four widely-used benchmarks in
English and Chinese. The results highlight the significant advantages of AUGCON
in producing high diversity, quality, and fidelity SFT data against several state-of-
the-art methods. All of our code, dataset, and fine-tuned model will be available at:
https://github.com/quanshr/AugCon .
1 Introduction
With the rise of impressive capabilities of large language models (LLMs), a variety of custom
LLM-based AI assistants have been introduced [ 15,13,89,27,46]. By incorporating specialized
knowledge into LLMs, these custom models have been shown to outperform their general-purpose
counterparts in their respective areas. These models can be developed through two strategies:
building them from scratch [ 93,53,36,19] or adapting existing general LLMs through supervised
fine-tuning (SFT) [ 74,97,59,43], with the latter approach often favored for its efficiency and the
foundational advantages offered by the general LLMs [31, 18, 29, 81].
Directly supervised fine-tuning on the raw, custom corpus, also known as domain-adaptive pre-
training (DAPT) [ 26], has proven beneficial [ 9,37] but revealed to be insufficient and may impair
prompting ability on domain-specific tasks [ 54,63]. To better leverage the privatized knowledge and
customize the outputs of LLMs, supervised fine-tuning using custom query-response pairs has become
common practice [ 72,12,10,22,92]. However, sourcing these pairs through human annotation is
very costly and can’t generate at scale. Recent studies have explored automated methods for creating
these pairs from custom corpus. AdaptLLM [ 15], for instance, has used regex-based patterns to
generate query-response pairs, but this approach tends to produce a limited variety of SFT data, which
may not significantly enhance prompting capabilities and risks overfitting due to the narrow range of
query types predefined. ETRC [ 31] and Context-Instruct [ 84] improved this by employing delicately
Preprint. Under review.arXiv:2405.16579v1  [cs.CL]  26 May 2024

--- PAGE 2 ---
Corpus
Context0.96
0.88
0.75
0.65
QueryResponse
0.93
Scorer
+ −>
LLMSelf-
Alignment
Self-
ImprovingRecursively Deriving Queries via 
Context -Split -TreeTraining Scorer to Rank Queries 
and Filtering Obtaining High -Fidelity Responses
Diverse
GranularityFigure 1: An overview of the proposed A UGCON.
designed prompts to generate queries from context using an LLM. However, those existing methods
using the same workflow repeatedly on the same context tend to produce redundant queries without
adequately covering the entire context at various levels of granularity. To automatically construct
synthetic custom SFT data incorporating a wide range of contextual granularity (queries range from
detailed questions to macro topics) with high diversity (queries need to be diversified to cover as
much as possible the provided corpus), quality (responses are correct and efficient in answering the
queries), and fidelity (data needs to follow human values and conform to predetermined tone and
formats) still remain challenges.
To address these challenges, we propose AUGCON, which automatically generates multi-granularity
context-driven SFT data for LLMs at scale with high diversity, quality, and fidelity. AUGCON
performs the following three essential steps:
1.Recursively Deriving Queries via Context-Split-Tree: Considering that it is difficult for any
predetermined prompts to generate multi-granularity queries from the same context, we propose a
novel method called Context-Split-Tree (CST). Starting from a context (which is a continuous text
chunk extracted from the corpus), we use an LLM to derive a query from it. At the same time,
we ask the LLM to split this context into two contexts that are as independent as possible. Each
context will recursively continue to derive queries and splits until it cannot be further divided.
At the end, we will obtain a binary tree rooted in the initial context, and each node represents a
context and contains a query that matches the granularity of it.
2.Training Scorer to Rank Queries and Filtering: To further ensure the quality and diversity
of the queries, we use contrastive learning to train a scorer to evaluate the query by taking the
obtained queries as positive examples and manipulating the prompt in Step 1 ( e.g., by using
suboptimal instruction or attaching fewer few-shot examples) to generate negative examples. Then,
we sort the derived queries under the same context using the scorer and only retain the queries that
get high scores and the diversity evaluated by ROUGE-L reaching a specific threshold. To ensure
high quality and high diversity of queries while reaching the certain quantity requirements, the
filtering stage will be iterated with CST until the requirements are met.
3.Obtaining High-Fidelity Responses: Inspired by the significant impact principles [ 77,76] have
on LLMs, we employ a principle-driven self-alignment approach to guide the LLM in producing
high-fidelity responses to filtered queries and their respective contexts. To enhance the quality
of the generated answers further, we apply random search and conduct the LLM to self-evaluate
its responses and discover the best in-context learning (ICL) examples from those annotated by
humans. Ultimately, all context, ICL examples, and principles are discarded, leaving only the
query-response pairs to supervised fine-tune the LLM.
The entire process only requires a handful of few-shot CST examples, alignment principles, and
query response examples. We can also achieve impressive results by just utilizing the open-source
model, which will later be fine-tuned with synthetic data, eliminating the necessity of distilling more
powerful LLMs like ChatGPT.
2

--- PAGE 3 ---
To assess the efficacy of our approach, we meticulously construct a test scenario and carefully
assemble a dataset consisting of high-quality Chinese magazine articles centered around daily topics,
along with corresponding test queries. Human evaluation demonstrates that our method excels
in generating queries of superior quality and in enhancing the performance of fine-tuned models.
Additionally, automatic evaluations conducted on four popularly used English benchmarks with
relevant metrics further highlight the significant advantages our method holds in capturing contextual
knowledge when compared to other state-of-the-art context-driven SFT data generation approaches.
Specifically, the contributions of our work lie on:
•We propose AUGCON, which can automatically generate multi-granularity context-driven
SFT data from the corpus for LLMs at scale with high diversity, quality, and fidelity,
providing the solution to a problem worth studying.
•Our ideas of deriving queries via CST, training the scorer using contrastive learning to
collaborate with the generation process to refine data, and synergistic integrating self-
alignment and self-improving to obtain high-fidelity responses, are very novel and may
inspire further works.
•Extensive experiments incorporating both human and automatic evaluations, encompassing
the test scenario and widely-used benchmarks in English and Chinese compared with other
state-of-the-art methods demonstrate the effectiveness and advantages of A UGCON.
•To boost the academy and for others to generate high-diversity SFT data on their own
corpus without effort, we open-source all of our code, dataset, and fine-tuned model at:
https://github.com/quanshr/AugCon .
2 Our Method: A UGCON
In this section, we delve into the details of our proposed AUGCON. A more comprehensive expla-
nation is presented in Appendix C given the space limitation. Additionally, to facilitate a direct
understanding of how each step functions collectively, a case demonstration is provided in Appendix F.
2.1 Preliminary
We have a raw custom corpus C={C1, C2, . . . , C n}with each context Cirepresents a continuous
text chunk extracted from corpus C, the instruct prompt ICSTand few-shot examples ECSTfor Context-
Split-Tree and IRandERfor answering the queries, and several response principles Prepresenting
the human demands on responses when answering questions1. The ERare context-query-response
triplets and will follow the response principles, represented as ER∼ P.
Our task is to generate numerous SFT query response pairs D={(qi,j, ri,j)}that each pair derives
from either whole or part of context Ci. The derived triplet (C, q, r )should also follow the response
principles P, and the generated Dis expected to have high diversity, quality, and fidelity.
2.2 Recursively Deriving Queries via Context-Split-Tree
This step is to derive context-query pairs (C, q)from the given corpus C. Previous approaches applied
regex-based or predetermined prompts for query generation, which often led to queries that were
relatively monotonous in structure and granularity. We believe that this type of approach did not
fully exploit the context, leading to queries incapable of effectively provoking the model’s capability
to comprehend and differentiate between various levels of detail within the context, resulting in
suboptimal outcomes.
To address this issue, we propose a very novel and effective method called Context-Split-Tree (CST),
with the pseudocode shown in Algorithm 1. CST starts with an entire context C, with each attached
with the instruct prompt ICSTand few-shot examples ECSTto call an LLM to generate a query q
deriving from the entire context. At the same time, we ask the LLM to semantically divide the context
into two child contexts C1andC2, and the instruct prompt is designed with hints to let the LLM
polish the two split contexts to make them as independent as possible and collectively encompass the
1In this work, we use query-response and question-answer interchangeably.
3

--- PAGE 4 ---
entirety of the original context. Each child context will continue to recursively derive query and split
until reaching a point where one of its split child context lengths is not less than itself or the length
falls below a predetermined threshold λ. At this point, we consider it to have been split into the
minimum granularity and cannot be further divided. Upon the completion of this recursive process,
a binary tree structure is formed, with the initial context at the root, and each node representing a
context along with its corresponding query tailored to its specific granularity. We collect data from
all nodes as the outcome of this step. The detailed prompt templates and several case demonstrations
are attached in Appendix C.1.2 and F, respectively.
Algorithm 1 Context Split Tree
Input: A corpus C, CST prompt instruction ICST, CST few-shot examples ECST
Output: Query dataset Data comprises of split context and derived query pairs
1:function CONTEXT SPLIT TREE(C, Data )
2: iflen(C)< λ then
3: return ▷Below the minimum granularity
4: end if
5: Call LLM to get C1, C2, q←LLM( ICST, ECST, C)
6: Append (C, q)toData
7: iflen(C1)≥len(C)orlen(C2)≥len(C)orROUGE-L[P] <0.7
then
8: return ▷The signs of hallucinations
9: end if
10: CONTEXT SPLIT TREE(C1, Data ) ▷Recursive calling
11: CONTEXT SPLIT TREE(C2, Data )
12:end function
13:
14:Initialize Data←empty list
15:foreach extracted context C∈ Cdo ▷Extraction method is in Appx C.1
16: CONTEXT SPLIT TREE(C, Data )
17:end for
18:return Data
The minimum length threshold λand the initial context length lare like the lower bound and upper
bound to control the granularity distribution of generated questions. One can easily adjust the overall
average granularity of generated queries by adjusting the length threshold. Similarly, if we seek to
address more global questions, we can do it by simply increasing the initial context length, as long as
the model’s context window permits. One beneficial property of CST is that the number of questions
ultimately generated will maintain a linear relationship with the length of the initial text provided
(proof can be found in Appendix C.1.1). This ensures that adjusting the length of the segmented
contexts in the corpus does not lead to significant fluctuations in the total number of queries obtained,
but rather merely shifts the distribution of query granularity. By employing CST, we can produce
queries that span across different levels of details in the context, and these queries naturally have little
redundancy or repetition, enabling more efficient use of the context information and stimulating the
model’s capability to comprehend and grasp the context in different granularities. Moreover, another
benefit of CST is that the derived queries just match the split context, making the later generated
response to these queries more accurate and pertinent with less unrelated information.
2.3 Training Scorer to Rank Queries and Filtering
To further enhance the quality and diversity of the generated data, we introduce an effective ranking
and filtering strategy collaborating with CST. Previous works have attempted to filter training data
via heuristic algorithms, such as filtering out queries that are too long or too short [ 83]. Other
works that are more relevant to us attempt to train scorers to judge the complexity and quality of
question-response pairs [ 49], but they need to have a step of distillation on stronger LLM APIs like
ChatGPT, and their training methods are less effective. For example, they put a series of responses
and ask for direct ranking, suffering from the positional bias [47] in LLMs, or ask LLMs to directly
assign a scalar score to a response, which is unstable. In this work, we apply contrastive learning to
4

--- PAGE 5 ---
train a scorer to judge the degree of adherence to instruct prompt and few-shot examples, which is
data-efficient and can achieve effective performance without the need for stronger LLMs.
The structure of our scorer is obtained by adding a linear head after the base model to map the last
hidden state to a one-dimensional space. We take context-query pairs as inputs, applying scorer Scto
yield a scalar score s=Sc(C, q). We use the context query pairs obtained from Step 1 as positive
samples: q+=LLM (ICST, ECST, C), and obtain negative samples by manipulating the instruct
prompt (use suboptimal instructions): q−=LLM (ICST−, ECST, C), few-shot examples (reduce ICL
examples count): q−=LLM (ICST, ECST−, C) or both of them: q−=LLM (ICST−, ECST−, C). The
details of constructing positive-negative pairs are presented in Appendix C.2.1. Note that we do
not generate all corresponding negative examples for positive data for training scorer, but rather
randomly select a very small number of samples ( e.g.only500pairs for each negative types in our
implementation) to form the training set DSc. Then, the loss function of scorer can be represented as:
L=−E(C,q+,q−)∼DSc[log(σ(Sc(C, q+)−Sc(C, q−))))] (1)
We use the trained scorer applied on all the context query pairs obtained in Step 1 to get their scores.
For each root context, we rank all queries from its CST in descending order of scores. Then, we start
with an empty set and add one training query each time, only if the current query has a ROUGE-L
precision score of less than 0.7compared to any previously added queries. We will stop adding as
the count reaches the limit. Each context will form such a set, and ultimately, we consolidate and
retain the training data from all the sets. Through this approach, we can obtain diverse data and easily
control the quantity, for it makes it possible to apply multi-times CST in the same context and filter
the repeated one. The details of how the filtering pipeline cooperates with CST to improve the quality
and diversity of queries are expatiated in Appendix C.2.2.
2.4 Obtaining High-Fidelity Responses
Inspired by the significant impact principles [ 77,76] have on LLMs, this principle-driven self-
alignment step begins by appending the context and a set of helpful, ethical, and reliable principles to
the LLM. These principles are meticulously crafted to ensure the LLM’s outputs are closely aligned
with human preferences or mimic certain response tones. Before initiating the response generation,
we deploy a self-improving pipeline that makes the LLM self-evaluate its response and sift through
the entire set of human-annotated Q&A pairs ER, where random search is applied to find the most
fitting few-shot examples to help LLM generate high-fidelity responses under the predetermined
principles, denoted as ER′. The detailed implementation is shown in Appendix C.3.
The innovative synergistic integration of the principle-driven methodology with self-improving effec-
tively improves the fidelity of generated responses. Following this, we execute LLM (IR, ER′, C, q )
to elicit each response r, ensuring that each response is not only high in quality but also in alignment
with our established principles. Notably, due to the precise matching of each query with its context’s
granularity within the CST framework, the LLM can effortlessly provide accurate and pertinent
responses to the queries.
After obtaining all generated data, we prune all context, instruction, and response principles and
only retain synthetic query response pairs as SFT data. This approach allows the fine-tuned LLM to
potentially learn the methods and nuances of responding to queries in a manner that naturally aligns
with human expectations, enabling the LLM to directly generate responses that are well-aligned with
reliable principles and optimal ICL exemplars across a wide range of queries. It’s important to note
that the fine-tuned LLM can generate high-quality responses without the need to explicitly reference
the principles set and ICL exemplars.
3 Evaluations
To validate the effectiveness of the proposed method, we apply human evaluation on a test scenario
in Section 3.2 and conduct automatic evaluations on four popular and widely used benchmarks in
Section 3.3. The set of contexts, base language models, and quantity of retained query-response
pairs are maintained the same (if applicable) on both the baselines and our method to ensure a fair
comparison. To provide a more thorough evaluation of our method, we present extensive experiments
evaluated from various perspectives in Appendix G.
5

--- PAGE 6 ---
3.1 Baselines
To demonstrate the advantages of our method, we meticulously collect the following relevant baselines
from a wide range of research, with the implementation details presented in Appendix D.
(1)Chat Model [7,79] applies instruction tuning and alignment tuning after pre-training. We utilize
it both as the basic baseline and as the fundamental model for calling and fine-tuning across all other
baselines and our methods for fair comparison.
(2)DAPT [26] continuously pre-trains directly on the raw custom corpus to adapt and grasp domain-
specific knowledge.
(3)AdaptLLM [15] builds SFT samples by converting the raw corpora into reading comprehension
tasks via regex-based mining patterns. Tasks they design include summarization, word-to-text, natural
language inference, commonsense reasoning, and paragraph detection.
(4)ETRC [31] derives question-answer pairs from extracted contexts with an LLM and augments data
by ensembling contexts and their corresponding question-answer pairs with a length-based clustering
algorithm. their corresponding question-answer pairs with a length-based clustering algorithm.
(5)Context-Instruct [84] is a context-driven instruction generation method that contains three parts:
1) partition text into manageable segments, 2) use an LLM to generate question, response, and
confidence score triplets based on the segments, and 3) apply confidence-score-based filtering and
deduplication to ensure data quality and diversity.
3.2 Human Evaluation
To ensure the efficacy and reliability of our methodology, we build a test scenario and conduct
a comprehensive human evaluation. This evaluation compares our constructed data and the fine-
tuned model against various baselines, aiming to provide a rigorous assessment of performance
enhancement and validation of our techniques. The human evaluation protocol is designed to provide
a nuanced understanding of the improvements our method offers, ensuring that the enhancements are
not just statistically significant, but also meaningful and perceptible to the users.
Specifically, we meticulously curate a corpus dataset, referred to as the DailyM dataset, which
consists of 1,000articles carefully selected from a variety of high-quality Chinese magazines closely
related to daily life. These articles extensively cover issues of widespread public concern such as
basic livelihood, politics, economics, and law, with each article containing approximately 4,000
Chinese characters. Then, we test how well our method and baselines build an AI chat assistant
specialized in this daily concern corpus. We apply our method on DailyM to generated SFT data
called DailyM-SFT and use these data to fine-tune Qwen1.5-32B-Chat [ 7] to get fine-tuned model
Qwen-DailyM-32B. To further test our method, we conduct annotators to write a total of 1,000
queries they are interested in related to these articles, forming the DailyM test set. To facilitate
further research and development within the research community, we plan to make our DailyM , the
constructed SFT data DailyM-SFT , and the fine-tuned model Qwen-DailyM-32B all open-sourced.
3.2.1 Metrics
In our comprehensive evaluation framework, we assess both the generated queries and the outputs
under the DailyM test set of the fine-tuned models. This dual approach ensures a holistic understand-
ing of the method’s performance, encompassing the generation of realistic, diverse queries and the
quality of the responses provided by the fine-tuned models. The evaluation metrics have been tailored
to address the specific characteristics and objectives users are concerned about in real scenarios.
For generated queries:
1.Realism : This metric evaluates how closely the generated queries resemble those that would be
posed by users, and whether they curious or willing to ask this question. Evaluators will consider
the naturalness and authenticity of the queries, scoring them on a scale from 1 (completely
artificial) to 5 (indistinguishable from human-created).
2.Diversity : Reflecting on the range of topics and the variety of the generated queries. A score from
1 (very monotonous) to 5 (highly diverse) will be assigned, with higher scores indicating a wide
spectrum of query types and topics, including various levels of granularity.
6

--- PAGE 7 ---
For fine-tuned models’ outputs:
1.Relevance : This assesses how relevant the model’s responses are to the test queries. It is crucial
that the responses accurately address the queries’ intent, providing meaningful and contextually
appropriate information. Scores will range from 1 to 5, with 5 being the most relevant.
2.Accuracy : This metric measures the factual correctness of the responses, with a score from 1
(many hallucinations) to 5 (completely accurate). Accuracy is paramount, and evaluators are
provided with relevant context and external tools like search engines to support their judgments.
3.Satisfaction : Reflecting the degree of satisfaction, this general metric allows evaluators to rate
their total satisfaction with the responses, serving as an overall assessment. The scoring will be
from 1 (highly dissatisfied) to 5 (highly satisfied).
For both generated queries and model outputs, evaluators are provided with detailed scoring rubrics
and examples to promote consistency in evaluation. The queries and outputs will be reviewed
by multiple independent evaluators to ensure a balanced and objective assessment, with average
scores calculated for each metric to determine the overall performance. This comprehensive human
evaluation metrics approach is designed to rigorously assess the effectiveness and applicability of our
method in generating multi-granularity and provoking queries and producing high quality and fidelity
responses. We provide the detailed evaluation guidance in Appendix E.
3.2.2 Results
For all our baselines and the proposed AUGCON, we employ Qwen1.5-32B-Chat [ 7], a popular open-
source LLM proficient in Chinese, as the fundamental model for calling and conducting fine-tuning
for evaluations. For methods such as AdaptLLM, ETRC, Context-Instruct, and our AUGCONwhich
generate query-response pairs based on context, we adhere to a standard where every 35Chinese
characters derive one query-response pair to ensure a fair comparison. We limit the number of
generated entries to the same in the comparison because we find that all methods spend much more
time on final fine-tuning process compared to the previous generation. Meanwhile, we also provide
a GPT-4 judge in Appendix G.2, a computation experiment in Appendix G.3, and a training phase
experiment in Appendix G.4 for a more comprehensive comparison.
Realism Diversity Relevance Accuracy Satisfaction12345ScoreChat
DAPT
AdaptLLM
ETRC
Con-Instr
AugCon
Figure 2: The results of human evaluation on DailyM . Query metrics are not applicable for the base
chat model and DAPT so we don’t show them.
Figure 2 presents the results of a human evaluation on the DailyM test set. The results demonstrate
thatAUGCONconsistently surpasses the baseline methods across all evaluation metrics. Specifically,
the superior performance in terms of query realism and diversity underscores our method’s ability
to produce human-like and high-diversity queries. Since our CST and filtering process effectively
gain multi-granularity queries that are more effective in covering all granularity levels of context,
the derived data will extract more useful knowledge from the corpus. Furthermore, the impressive
performance in judging relevance, accuracy, and satisfaction in responses from fine-tuned models
further validates that our method’s high-quality and diverse queries, coupled with high-fidelity
responses, can indeed enhance the performance of subsequently fine-tuned models and achieve higher
satisfaction scores from humans. This suggests that AUGCONis particularly adept at constructing
high-quality supervised fine-tuning data for LLMs from a given corpus.
7

--- PAGE 8 ---
3.3 Automatic Evaluation
To objectively assess the impact of our approach, we conduct automatic evaluations on four widely
used benchmarks, employing relevant metrics to facilitate a direct comparison between models fine-
tuned with our generated data and established baselines. This concise evaluation method provides
clear, objective insights into the efficacy of our data construction paradigm, highlighting the potential
advantages of our A UGCONover existing methods.
3.3.1 Benchmarks
To automatically evaluate our method and baselines, we meticulously collect a range of short-form
and long-form question-answering benchmarks that have corpus or contexts for reference (we re-
compile all reference contexts as the corpus). All baselines and our AUGCONare applied on the
same provided corpus and test on the test set. All the public links to these benchmarks are listed in
Appendix Table 3.
(1)SQuAD1.1 [67] is a reading comprehension dataset, consisting of questions posed by crowdwork-
ers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
from the corresponding reading passage. SQuAD1.1 contains 100,000+ question-answer pairs on
500+ articles.
(2)TriviaQA [33] includes 95K question-answer pairs authored by trivia enthusiasts and indepen-
dently gathered evidence documents, six per question on average, that provide high quality distant
supervision for answering the questions.
(3)DROP [21] is a crowdsourced, adversarially-created, 96K-question benchmark, in which a system
must resolve references in a question, perhaps to multiple input positions, and perform discrete
operations over them.
(4)WebGLM-QA [50] is the data used to train the WebGLM generator module and an LLM
bootstrapped quoted and long-formed QA dataset via in-context learning and corresponding strategies
to clean and refine, with 45K high-quality filtered and 83K unfiltered samples.
3.3.2 Metrics
For datasets featuring short-form responses (applied to the SQuAD1.1, TriviaQA, and DROP datasets),
we measure the model’s performance using accuracy (Acc). A response is considered correct if and
only if it matches any of the possible answers. For datasets with long-form responses (applied to the
WebGLM-QA dataset), we employ BERTScore (BS) [ 99] (we use Roberta-Large [ 52] for calculation)
to evaluate the semantic similarity between the generated outputs and the reference responses.
3.3.3 Results
Short-form (Acc) Long-form (BS)
Method SQuAD1.1 TriviaQA DROP WebGLM-QA
Llama3-c 70B 0.212±0.004 0.723±0.003 0.220±0.004 0.837±0.002
DAPT 0.258±0.004 0.767±0.003 0.266±0.004 0.851±0.002
AdaptLLM 0.273±0.003 0.791±0.004 0.284±0.003 0.842±0.001
ETRC 0.301±0.004 0.812±0.003 0.326±0.004 0.903±0.001
Context-Instruct 0.314±0.003 0.825±0.003 0.334±0.003 0.885±0.001
AUGCON(Ours) 0.336±0.004 0.849±0.003 0.350±0.003 0.924±0.002
Table 1: The results of automatic evaluation on four benchmarks. We run 10 times for each test and
report the mean value and standard deviation, with the best results shown in bold.
We employ Llama3-70B-Instruct [ 4] as the fundamental model for calling and conducting fine-tuning
for automatic evaluations for all our baselines and the proposed AUGCON. The detailed results
are shown in Table 1. The results illustrate that our proposed method consistently outperforms the
established baselines across all four datasets. Specifically, when analyzing short-form datasets, it
becomes evident that the data generated by AUGCONsurpasses the comparative methods in extracting
pivotal information and knowledge from the corpus, thus improving the question-answering accuracy
8

--- PAGE 9 ---
of fine-tuned models. Meanwhile, the exceptional performance of AUGCONon datasets emphasizing
long-form responses showcases its proficiency in generating high-fidelity query-response pairs. This
capability directly contributes to enhancing the effectiveness of chat models, enabling them to deliver
more relevant, engaging, and contextually appropriate responses based on the given corpus. This,
in turn, significantly improves the overall user experience by ensuring that interactions are not only
informative but also closely aligned with the user’s specific curiosities and requirements.
Furthermore, the consistency of AUGCONin achieving top results across all four datasets, each
with unique query patterns and focuses, speaks volumes about its versatility and adaptability. Such
consistent performance across varied datasets underscores the robust generalization ability of our
method, making it a highly effective tool for a broad spectrum of corpus types and catering to diverse
user interests and inquiries. We also provide ablation studies and further analysis in Appendix G.1.
4 Related work
Synthetic Data for Language Models Due to the challenges of data scarcity [ 6], privacy con-
cerns [ 1], and the sheer cost of data collection and annotation [ 24], synthetic data has emerged as a
promising solution to build large, diverse, and high-quality datasets at scale [ 48]. One benefit of syn-
thetic data is it can be tailored to specific requirements [ 15,31,50], with practical applications having
been employed in various domains. WizardMath [ 58] leverages a series of operations to increase the
complexity of questions and answers using GPT-3.5, while Reflexion [ 75] employs external or inter-
nally simulated linguistic feedback to improve the code reasoning capabilities of language models.
Similarly, Toolformer [ 71] learns to decide which APIs to call and what arguments to pass by training
on template-generated data. In addition, synthesized data has been proven effective in mitigating hal-
lucination [ 86,87,32,78] and aligning with shared human preferences and values [ 8,77,76,65,35].
While the generation of context-driven synthetic data has proven to be a powerful substitute for
manual annotation, the challenge of ensuring high-quality synthetic data, which encompasses the
complexity of queries [ 49,40,45], the diversity of semantics [ 17,85,80,57,55], and the scale of the
synthetic datasets [95, 25, 42], has been a consistent pursuit.
Context-Driven Synthetic Data Numerous studies have developed techniques for creating syn-
thetic data informed by contextual cues. UltraChat [ 17] leverages user-specified topics and supple-
ments these with existing textual material to craft instructional conversations aimed at enhancing
chatbot performance. SPIN [ 14], on the other hand, autonomously generates training data from its
previous iterations, employing this approach to progressively refine its capabilities. RECOST [ 98]
selects top-tier instructional content by incorporating external knowledge to assess synthesized
examples using an in-context relative predictive entropy measure. Additionally, various methods
have been devised to extract character profiles and personas from collected books or scripts for
the purpose of producing roleplaying dialogues [ 73,101,84,41], and several initiatives focus
on mining domain-specific data from specialized corpora to construct domain-specific language
models [ 15,31,23,16,96]. While alternative approaches employ retrieval augmented generation
(RAG) [ 68,11] or integrate auxiliary knowledge in vast context windows [ 91,5], issues like entity
susceptibility [ 20], high inference computational demand [ 44,28], and alignment difficulties with
formats and preferences [ 64,60,3] highlight the crucial role of context-driven SFT in effectively
incorporating corpus knowledge internally.
5 Conclusion
In this work, we propose AUGCON, a highly innovative and effective method to build custom AI
assistants from the corpus by deriving SFT query-response pairs with diverse granularity. AUGCON
starts with query generation through the Context-Split-Tree (CST), an innovative approach for
recursively deriving queries and splitting context to cover full granularity. We then employ contrastive
learning to develop a scorer that works with CST to rank and refine queries. Finally, we introduce a
synergistic integration of self-alignment and self-improving to obtain high-fidelity responses. We
conduct extensive experiments on Qwen1.5-32B-Chat and Llama3-70B-Instruct models. The human
evaluation on a test scenario and automatic evaluation on four benchmarks demonstrate the significant
advantages of our method in producing high diversity, quality, and fidelity context-driven SFT data
and improving the performance of custom fine-tuned models against existing methods.
9

--- PAGE 10 ---
References
[1]Nazmiye Ceren Abay, Yan Zhou, Murat Kantarcioglu, Bhavani Thuraisingham, and Latanya Sweeney.
Privacy preserving synthetic data release using deep learning. In Machine Learning and Knowledge
Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14,
2018, Proceedings, Part I 18 , pages 510–526. Springer, 2019.
[2]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 , 2023.
[3]Angus Addlesee, Weronika Siei ´nska, Nancie Gunson, Daniel Hernández Garcia, Christian Dondrup, and
Oliver Lemon. Multi-party goal tracking with llms: Comparing pre-training, fine-tuning, and prompt
engineering. arXiv preprint arXiv:2308.15231 , 2023.
[4] AI@Meta. Llama 3 model card. 2024.
[5]Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.
Training-free long-context scaling of large language models. arXiv preprint arXiv:2402.17463 , 2024.
[6]Rohit Babbar and Bernhard Schölkopf. Data scarcity, robustness and extreme multi-label classification.
Machine Learning , 108(8):1329–1351, 2019.
[7]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han,
Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.
[8]Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from
ai feedback. arXiv preprint arXiv:2212.08073 , 2022.
[9]Markus Bayer, Philipp Kuehn, Ramin Shanehsaz, and Christian Reuter. Cysecbert: A domain-adapted
language model for the cybersecurity domain. ACM Transactions on Privacy and Security , 27(2):1–20,
2024.
[10] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai
Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism.
arXiv preprint arXiv:2401.02954 , 2024.
[11] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving
language models by retrieving from trillions of tokens. In International conference on machine learning ,
pages 2206–2240. PMLR, 2022.
[12] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for time-series
forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469 , 2023.
[13] Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong
Xu, Xiang Bai, Xuanjing Huang, et al. Disc-finllm: A chinese financial large language model based on
multiple experts fine-tuning. arXiv preprint arXiv:2310.15205 , 2023.
[14] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts
weak language models to strong language models. arXiv preprint arXiv:2401.01335 , 2024.
[15] Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehen-
sion. arXiv preprint arXiv:2309.09530 , 2023.
[16] Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Yi Xu, Luoyi Fu, Weinan
Zhang, Xinbing Wang, Chenghu Zhou, et al. K2: A foundation language model for geoscience knowledge
understanding and utilization. In Proceedings of the 17th ACM International Conference on Web Search
and Data Mining , pages 161–170, 2024.
[17] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,
and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations.
arXiv preprint arXiv:2305.14233 , 2023.
[18] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang,
Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by
supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492 , 2023.
[19] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and
Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation.
Advances in neural information processing systems , 32, 2019.
[20] Kevin Du, Vésteinn Snæbjarnarson, Niklas Stoehr, Jennifer C White, Aaron Schein, and Ryan Cotterell.
Context versus prior knowledge in language models. arXiv preprint arXiv:2404.04633 , 2024.
10

--- PAGE 11 ---
[21] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop:
A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the
2019 Conference on NAACL , 2019.
[22] Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli,
Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold Milstein, et al. An interactive agent
foundation model. arXiv preprint arXiv:2402.05929 , 2024.
[23] Nathan C Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor W
Coley, and Vijay Gadepally. Neural scaling of deep chemical models. Nature Machine Intelligence ,
5(11):1297–1305, 2023.
[24] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd workers for text-
annotation tasks. Proceedings of the National Academy of Sciences , 120(30):e2305016120, 2023.
[25] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,
Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need.
arXiv preprint arXiv:2306.11644 , 2023.
[26] Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and
Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. arXiv preprint
arXiv:2004.10964 , 2020.
[27] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexan-
der Löser, Daniel Truhn, and Keno K Bressem. Medalpaca–an open-source collection of medical
conversational ai models and training data. arXiv preprint arXiv:2304.08247 , 2023.
[28] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling
in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713 , 2022.
[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 ,
2021.
[30] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and
Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798 ,
2023.
[31] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng,
Feng Sun, Qi Zhang, Deqing Wang, et al. Improving domain adaptation through extended-text reading
comprehension. arXiv preprint arXiv:2401.07284 , 2024.
[32] Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam
Mitra, Ahmed Awadallah, and Ece Kamar. Teaching language models to hallucinate less with synthetic
tasks. arXiv preprint arXiv:2310.06827 , 2023.
[33] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.
[34] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vard-
hamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling
declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 , 2023.
[35] Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Min-
joon Seo. Aligning large language models through synthetic feedback. arXiv preprint arXiv:2305.13735 ,
2023.
[36] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason
Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In
International Conference on Machine Learning , pages 17506–17533. PMLR, 2023.
[37] Jan-David Krieger, Timo Spinde, Terry Ruas, Juhi Kulshrestha, and Bela Gipp. A domain-adaptive
pre-training approach for language bias detection in news. In Proceedings of the 22nd ACM/IEEE joint
conference on digital libraries , pages 1–7, 2022.
[38] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving
with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles , pages
611–626, 2023.
[39] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. Datasets: A community library
for natural language processing. arXiv preprint arXiv:2109.02846 , 2021.
[40] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and
Houwen Peng. Common 7b language models already possess strong math capabilities. arXiv preprint
arXiv:2403.04706 , 2024.
11

--- PAGE 12 ---
[41] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Commu-
nicative agents for" mind" exploration of large language model society. Advances in Neural Information
Processing Systems , 36, 2024.
[42] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 , 2023.
[43] Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. Data-
efficient fine-tuning for llm-based recommendation. arXiv preprint arXiv:2401.17197 , 2024.
[44] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A
Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances
in Neural Information Processing Systems , 35:1950–1965, 2022.
[45] Haoxiong Liu and Andrew Chi-Chih Yao. Augmenting math word problems via iterative question
composing. arXiv preprint arXiv:2401.09003 , 2024.
[46] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah
Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, et al. Chipnemo: Domain-adapted
llms for chip design. arXiv preprint arXiv:2311.00176 , 2023.
[47] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for
Computational Linguistics , 12:157–173, 2024.
[48] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng,
Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models.
arXiv preprint arXiv:2404.07503 , 2024.
[49] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a
comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685 ,
2023.
[50] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie
Tang. Webglm: Towards an efficient web-enhanced question answering system with human preferences.
InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages
4549–4560, 2023.
[51] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan
Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models.
arXiv preprint arXiv:2311.18743 , 2023.
[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692 , 2019.
[53] Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, and Xiaoming Zhai. Context matters: A strategy to
pre-train language model for science education. In International Conference on Artificial Intelligence in
Education , pages 666–674. Springer, 2023.
[54] Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu,
Cheng Chen, Sekeun Kim, et al. Tailoring large language models to radiology: A preliminary approach
to llm adaptation for a highly specialized domain. In International Workshop on Machine Learning in
Medical Imaging , pages 464–473. Springer, 2023.
[55] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa:
Building gpt-4 level conversational qa models. arXiv preprint arXiv:2401.10225 , 2024.
[56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
[57] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, and Chang Zhou. # instag:
Instruction tagging for diversity and complexity analysis. arXiv preprint arXiv:2308.07074 , 2023.
[58] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large
language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583 , 2023.
[59] Ling Luo, Jinzhong Ning, Yingwen Zhao, Zhijun Wang, Zeyuan Ding, Peng Chen, Weiru Fu, Qinyu Han,
Guangtao Xu, Yunzhi Qiu, et al. Taiyi: a bilingual fine-tuned large language model for diverse biomedical
tasks. Journal of the American Medical Informatics Association , page ocae037, 2024.
[60] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Few-shot fine-
tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938 ,
2023.
12

--- PAGE 13 ---
[61] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus,
Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language models. Advances
in Neural Information Processing Systems , 36, 2024.
[62] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in neural information processing systems , 35:27730–27744, 2022.
[63] Soumen Pal, Manojit Bhattacharya, Sang-Soo Lee, and Chiranjib Chakraborty. A domain-specific next-
generation large language model (llm) or chatgpt is required for biomedical engineering and research.
Annals of Biomedical Engineering , 52(3):451–454, 2024.
[64] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-
tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint
arXiv:2310.03693 , 2023.
[65] Shanghaoran Quan. Dmoerm: Recipes of mixture-of-experts for effective reward modeling. arXiv
preprint arXiv:2403.01197 , 2024.
[66] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations
toward training trillion parameter models. In SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis , pages 1–16. IEEE, 2020.
[67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016.
[68] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and
Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for
Computational Linguistics , 11:1316–1331, 2023.
[69] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations
enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 3505–3506, 2020.
[70] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.
Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 ,
2024.
[71] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke
Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves
to use tools. Advances in Neural Information Processing Systems , 36, 2024.
[72] Omar Shaikh, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal:
Simulating conflict to teach conflict resolution. In Proceedings of the CHI Conference on Human Factors
in Computing Systems , pages 1–20, 2024.
[73] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing.
arXiv preprint arXiv:2310.10158 , 2023.
[74] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke
Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document
boundaries. arXiv preprint arXiv:2310.10638 , 2023.
[75] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:
Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems ,
36, 2024.
[76] Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. Salmon: Self-alignment with principle-following reward models. arXiv preprint
arXiv:2310.05910 , 2023.
[77] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human
supervision. Advances in Neural Information Processing Systems , 36, 2024.
[78] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. Fine-tuning
language models for factuality. arXiv preprint arXiv:2311.08401 , 2023.
[79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
[80] Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, and Dianhui Chu. A survey on data selection for
llm instruction tuning. arXiv preprint arXiv:2402.05123 , 2024.
[81] Rui Wang, Yixue Hao, Long Hu, Jincai Chen, Min Chen, and Di Wu. Self-supervised learning with
data-efficient supervised fine-tuning for crowd counting. IEEE Transactions on Multimedia , 2023.
13

--- PAGE 14 ---
[82] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv
preprint arXiv:2203.11171 , 2022.
[83] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In
Proceedings of the 61st Annual Meeting Of The Association For Computational Linguistics , 2023.
[84] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,
Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting, and
enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746 , 2023.
[85] Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Lifeng Shang, Xin Jiang, and
Qun Liu. Data management for large language models: A survey. arXiv preprint arXiv:2312.01700 ,
2023.
[86] Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu,
Denny Zhou, Tengyu Ma, et al. Symbol tuning improves in-context learning in language models. arXiv
preprint arXiv:2305.08298 , 2023.
[87] Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy
in large language models. arXiv preprint arXiv:2308.03958 , 2023.
[88] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art
natural language processing. arXiv preprint arXiv:1910.03771 , 2019.
[89] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan
Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance.
arXiv preprint arXiv:2303.17564 , 2023.
[90] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting
influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333 , 2024.
[91] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation
models. arXiv preprint arXiv:2309.16039 , 2023.
[92] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint
arXiv:2304.12244 , 2023.
[93] Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, and Yang
Gao. Mindllm: Pre-training lightweight large language model from scratch, evaluations and domain
applications. arXiv preprint arXiv:2310.15777 , 2023.
[94] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng
Yin. Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation.
arXiv preprint arXiv:2312.14187 , 2023.
[95] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scal-
ing relationship on learning mathematical reasoning with large language models. arXiv preprint
arXiv:2308.01825 , 2023.
[96] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao
Xiao, Song Yun, Wei Lin, et al. Disc-lawllm: Fine-tuning large language models for intelligent legal
services. arXiv preprint arXiv:2309.11325 , 2023.
[97] Salah Zaiem, Robin Algayres, Titouan Parcollet, Slim Essid, and Mirco Ravanelli. Fine-tuning strategies
for faster inference using speech self-supervised models: a comparative study. In 2023 IEEE International
Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW) , pages 1–5. IEEE, 2023.
[98] Qi Zhang, Yiming Zhang, Haobo Wang, and Junbo Zhao. Recost: External knowledge guided data-
efficient instruction tuning. arXiv preprint arXiv:2402.17355 , 2024.
[99] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.
[100] Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn. Machine translation with large language
models: Prompting, few-shot learning, and fine-tuning with qlora. In Proceedings of the Eighth Conference
on Machine Translation , pages 468–481, 2023.
[101] Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng,
Jiaming Yang, Xiyao Xiao, et al. Characterglm: Customizing chinese conversational ai characters with
large language models. arXiv preprint arXiv:2311.16832 , 2023.
14

--- PAGE 15 ---
Appendix
A Limitations 16
B Boarder Impacts 16
C Method Details 17
C.1 Recursively Deriving Queries via Context-Split-Tree . . . . . . . . . . . . . . . . 17
C.1.1 Proof of Linear Relationship . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.1.2 Prompt Template & Few-Shot Examples . . . . . . . . . . . . . . . . . . . 18
C.2 Training Scorer to Rank Queries and Filtering . . . . . . . . . . . . . . . . . . . . . 21
C.2.1 Training Scorer via Contrastive Learning . . . . . . . . . . . . . . . . . . . 21
C.2.2 Collaborating Scorer with CST to Filter Queries . . . . . . . . . . . . . . 22
C.3 Obtaining High-Fidelity Responses . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D Implementation Details 23
D.1 Assets Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E Human Evaluation Guidance 24
F Case Demonstration 26
G Additional Experiments 29
G.1 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
G.2 GPT-4 Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
G.3 Computation Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
G.4 Training Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
G.5 Granularity Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
15

--- PAGE 16 ---
A Limitations
While our research introduces an innovative approach to automatically generating multi-granularity
SFT data for LLMs based on context, several limitations should be acknowledged. These limitations
highlight areas for future research and potential improvements in our methodology.
Contextual Depth and Complexity Our method relies heavily on the context provided for SFT
data construction. However, the complexity and the depth of the context can vary significantly,
potentially impacting the quality and the relevance of the generated SFT data. In instances where the
context is too narrow or lacks depth, the model may produce SFT data that is not sufficiently diverse
or representative.
Model Bias and Sensitivity Like all machine learning models, our approach is subject to the biases
inherent in the training data. Despite efforts to mitigate these biases, there may still be underlying
biases that affect the SFT data generation process. Additionally, our method’s sensitivity to nuanced
linguistic and cultural differences may not be fully addressed, possibly leading to the generation of
data that might not be entirely appropriate or sensitive to all contexts.
Scalability and Computational Resources While our method is designed to automate the con-
struction of SFT data for LLMs, the scalability of this approach can be constrained by available
computational resources. The processing power required to analyze complex contexts and gener-
ate high-quality SFT data can be substantial, which may limit the applicability of our method in
resource-constrained environments.
Generalization Across Different Languages and Domains Our initial experiments and results are
promising but are primarily focused on specific languages and domains. The ability of our method
to generalize across different languages, dialects, and specialized domains has not been thoroughly
tested. This limitation suggests that the effectiveness of our approach may vary significantly when
applied to less common languages or highly specialized fields.
Evaluation Metrics and Benchmarks The evaluation of the automatically constructed SFT data’s
quality relies on metrics that may not fully capture the nuances of semantic fidelity and context
appropriateness. The development of more refined evaluation benchmarks and metrics that can better
assess the quality and utility of SFT data in training LLMs remains an area for future work.
Conclusion While our method presents a novel approach to enhancing the training of large language
models through contextually generated SFT data with multi-granularity, these limitations underscore
the importance of continued research and development. Addressing these challenges will be crucial
for improving the efficacy and applicability of automated SFT data construction methods in the field
of large language model.
B Boarder Impacts
The advent of large language models has significantly advanced the capabilities of artificial intelli-
gence in understanding and generating human-like text. Our research presents a novel methodology
for automatically generating SFT query response data pairs based on context with multi-granularity, a
development that holds considerable implications for the field of AI and its applications across various
domains. This section explores the broader impact of our work, encompassing both its potential
benefits and challenges.
Enhancing Model Performance and Efficiency The automated construction of SFT datasets
tailored to the context at hand has the potential to significantly improve the performance of LLMs. By
providing high-quality, targeted training data, models can achieve better understanding and generate
more accurate outputs, thereby increasing their utility in a wide range of applications. Furthermore,
this method reduces the need for extensive manual dataset preparation, leading to more efficient
model development.
16

--- PAGE 17 ---
Advancing Domain-Specific Applications Our method stands to greatly enhance the development
of domain-specific applications, such as specialized question-answering assistants. By enabling
the automatic generation of SFT datasets tailored to specific contexts, our approach facilitates the
creation of LLMs that are not only more accurate but also more relevant for specialized fields such
as medicine, law, and engineering. This could lead to significant improvements in professional
assistance systems, offering experts timely and accurate information and potentially accelerating
decision-making processes in critical situations.
Democratizing AI Development By automating the construction of SFT datasets, our methodology
could democratize access to high-quality AI development. Smaller organizations or groups with
limited resources might find it easier to develop powerful, context-specific AI tools without the
need for extensive manual dataset curation. This democratization could accelerate innovation and
competition, leading to a broader range of AI applications and services available to the public.
Educational Implications Our approach could also have profound implications for education.
Customized LLMs can be developed to provide students with personalized learning assistants. These
AI tutors could adapt to each student’s learning style and pace, offering explanations, supplementary
information, or exercises based on the context of the student’s needs. Such personalized education
could enhance learning outcomes and make education more accessible and effective for diverse
learners.
Ethical and Societal Considerations However, the broader deployment of context-specific LLMs
also raises important ethical and societal considerations. The accuracy and fairness of these models
depend on the quality and diversity of the SFT data. There is a risk that biases present in the training
data could be amplified, leading to unfair or harmful outcomes. It is crucial to develop methodologies
for monitoring and mitigating bias in these models.
Moreover, the widespread use of powerful, domain-specific LLMs could have unforeseen impacts on
employment, particularly in sectors where decision-making or informational roles are automated by
AI. While these technologies can augment human capabilities, there is also a need for policies that
address potential displacements and ensure that the benefits of AI advancements are broadly shared
across society.
Conclusion The automatic construction of context-specific SFT datasets for LLMs as proposed
in our research has the potential to significantly impact various sectors positively. However, it is
imperative to navigate the ethical, societal, and environmental challenges associated with these
advancements. By addressing these issues proactively, we can ensure that the benefits of AI are
realized equitably and sustainably across society.
C Method Details
In this section, we delve into the detailed framework of our methods, providing a thorough exam-
ination of each step in practice. Accompanied by comprehensive prompt templates and detailed
implementation specifics, we aim to offer a clear understanding of how our approach functions in
action.
C.1 Recursively Deriving Queries via Context-Split-Tree
The Context-Split-Tree construction process prepares by dividing the corpus into short, consecutive
text contexts, each with a limit of 500words. Gain inspiration from retrieval augmentation meth-
ods [ 70], if a sentence surpasses the 500-word threshold, we move the entire sentence to the next
context, rather than cutting it mid-sentence. This approach maintains the contextual integrity and
semantic consistency of the text in each context. After obtaining the extracted contexts, for each
context, we construct a correspondence CST in the manner described previously in Section 2.2. The
specific templates and few-shot examples used are detailed in Appendix C.1.2.
17

--- PAGE 18 ---
C.1.1 Proof of Linear Relationship
A commendable property is that with the initial text length lprovided, we can achieve multi-
granularity effects through a linear quantity of generated questions. To be more precise, the number
of questions generated will have a linear relationship with the number of minimum sentence units
contained in the initial context. This allows us to generate different distributions of query granularity
by simply adjusting the minimum sentence length λand the initial context length without worrying
about significant fluctuations in the computation consumption or the overall number of queries
obtained. We will prove this property in the following.
We represent context as a collection of sentences C={S1, S2, . . . , S n}. We assume that during the
CST process, these sentences are the smallest units and will not be split internally or increased in
quantity (they may be polished but do not change the essential semantics). Formally, we have the
following assumption.
Assumption: Given a context C={S1, S2, . . . , S n}(n >1), using LLM for a split will definitely
yield one question qand two child context C1andC2that satisfy that there exists an integer 1≤i < n
such that C1={S1, . . . , S i}andC2={Si+1, . . . , S n}. Specifically, when the context degrades to
a single sentence, calling the LLM will generate a question and terminate.
Then, based on the preceding assumptions, we have the following proposition.
Proposition: For any context C={S1, S2, . . . , S n}containing nsentences, where nis an arbitrary
positive integer, applying CST to it will ultimately generate 2n−1questions.
Proof: We will prove the proposition using the Second Principle of Mathematical Induction .
1.First, for n= 1, calling the LLM will generate a question and then terminate, which is consistent
with the proposition.
2.Secondly, assume the conclusion holds for all n≤k(k≥1). When n=k+ 1, according to the
assumption, calling the LLM with C={S1, . . . , S k+1}will generate a question q, along with
two sub-contexts C1andC2, where there exists 1≤i < k + 1such that C1={S1, . . . , S i}, and
C2={Si+1, . . . , S k+1}. The numbers of sentences in C1andC2areiandk+ 1−iand both
strictly less than k+ 1. By assumption, C1will eventually generate 2i−1questions, and C2
will generate 2(k+ 1−i)−1 = 2( k−i) + 1 questions. Therefore, in total, Cwill generate
1 + (2 i−1) + (2( k−i) + 1) = 2 k+ 1 = 2 n−1questions by the end. Thus, the proposition
also holds for n=k+ 1.
3.Finally, combining 1 and 2 along with the Second Principle of Mathematical Induction , it can be
concluded that the proposition holds.
Therefore, a context containing nsentences will ultimately generate 2n−1questions, which
establishes a linear relationship between the number of sentences in the context and the number of
questions generated. □
C.1.2 Prompt Template & Few-Shot Examples
Our method is applicable across various languages, for which we provide the utilized prompt templates
in both English and Chinese.
For English version:
Prompt Template for Context-Split-Tree with Instruction in English
Given an entire context as the Context, generate a Question about the entire context that users
might be interested in, which answer should be able to be derived directly from the Context.
Then, divide the entire context into two sub-contexts Context 1 and Context 2 based on their
semantic content, making necessary adjustments within each sub-contexts to ensure they are
independently coherent.
—
Provide in the following form:
18

--- PAGE 19 ---
Context: The entire context
Question: Regarding the entire context
Context 1: Sub-context 1
Context 2: Sub-context 2
—
Context: Trying to rebound from their divisional home loss to the Buccaneers, the Panthers
flew to the Louisiana Superdome for a Week 5 divisional duel with the winless New Orleans
Saints. With QB Jake Delhomme out and done for the year with a right elbow injury, QB
David Carr was given the start. In the first quarter, Carolina took the early lead with kicker
John Kasay getting a 23-yard field goal. The Saints responded with kicker Olindo Mare
getting a 25-yard field goal. In the second quarter, the Panthers went back into the lead with
Kasay nailing a 35-yard field goal. New Orleans would respond with Mare kicking a 28-yard
field goal. In the third quarter, Carolina trailed as Saints FB Mike Karney got a 2-yard TD
run for the only score of the period. In the fourth quarter, the Panthers tied the game with
Carr completing a 17-yard TD pass to WR Steve Smith. Afterwards, Carolina sealed the win
in the final seconds with Kasay nailing a 52-yard field goal as time ran out.
Question: How did the Carolina Panthers secure their victory against the New Orleans Saints
in their Week 5 divisional duel?
Context 1: Trying to rebound from their divisional home loss to the Buccaneers, the Panthers
flew to the Louisiana Superdome for a Week 5 divisional duel with the winless New Orleans
Saints. With QB Jake Delhomme out for the year with a right elbow injury, QB David Carr
was given the start. In the first quarter, Carolina took the early lead with kicker John Kasay
getting a 23-yard field goal. The Saints responded with kicker Olindo Mare getting a 25-yard
field goal. In the second quarter, the Panthers went back into the lead with Kasay nailing a
35-yard field goal, followed by New Orleans’ response with Mare kicking a 28-yard field
goal.
Context 2: As the game progressed into the third quarter, the Panthers found themselves
trailing after Saints FB Mike Karney got a 2-yard TD run, marking the only score of the
period. However, in the fourth quarter, the Panthers managed to tie the game thanks to QB
David Carr completing a 17-yard TD pass to WR Steve Smith. The climax of the match came
in the final seconds with John Kasay nailing a 52-yard field goal as time ran out, securing a
dramatic victory for Carolina against the New Orleans Saints.
—
Context: As a cell grows, its volume increases more quickly than its surface area. If a cell
was to get very large, the small surface area would not allow enough nutrients to enter the cell
quickly enough for the cell’s needs. However, large cells have a way of dealing with some
size challenges. Big cells, such as some white blood cells, often grow more nuclei so that they
can supply enough proteins and RNA for the cell’s requirements. Large, metabolically active
cells often have lots of cell protrusions, resulting in many folds throughout the membrane.
These folds increase the surface area available for transport of materials into or out of the cell.
Such cell types are found lining your small intestine, where they absorb nutrients from your
food through protrusions called microvilli.
Question: How do large cells adapt to the challenge of having a volume that increases more
quickly than their surface area to meet their metabolic needs?
Context 1: As a cell grows, its volume increases more quickly than its surface area. If a cell
was to get very large, the small surface area would not allow enough nutrients to enter the
cell quickly enough for the cell’s needs.
Context 2: Large cells have a way of dealing with their size challenges. Big cells, such as
some white blood cells, often grow more nuclei so that they can supply enough proteins
and RNA for the cell’s requirements. Large, metabolically active cells often have lots of
cell protrusions, resulting in many folds throughout the membrane. These folds increase
the surface area available for transport of materials into or out of the cell. Such cell types
are found lining your small intestine, where they absorb nutrients from your food through
protrusions called microvilli.
—
19

--- PAGE 20 ---
Context: Philip Arnold Heseltine is best known as a composer of songs and other vocal music;
he also achieved notoriety in his lifetime through his unconventional and often scandalous
lifestyle.
Question: Why is Philip Arnold Heseltine’s reputation mixed?
Context 1: Philip Arnold Heseltine is best known as a composer of songs and other vocal
music.
Context 2: Philip Arnold Heseltine also achieved notoriety in his lifetime through his
unconventional and often scandalous lifestyle.
—
Context: {Context}
Question:
In the prompt template, the {Context} in blue color will be replaced with the given context to derive
query and split during usage. After the LLM provides a response, we employ regular expressions
to parse out the fields for Question, Context 1, and Context 2. Should the parsing fail, we will
attempt the process up to three more times; otherwise, the context will be discarded. However, in
our experiment, we find that almost all contexts can be successfully parsed in the first response. To
observe this template in a more specific way, we have provided a detailed case demonstration in
Appendix F.
For Chinese version:
Prompt Template for Context-Split-Tree with Instruction and Few-Shot in Chinese
给定整个段落Context，生成一个关于整个段落的用户可能关心的问题Question，问
题答案要出自段落，然后将整个段落按语义划分为两个子段落，两子段落内进行一
些必要的微调使得每一段相互独立。
—
以下面的形式给出：
Context:整个段落
Question: 关于整个段落的问题
Context 1: 子段落1
Context 2: 子段落2
—
Context: 2020 年11月11日，丁真去舅舅家吃饭时，偶然遇到了摄影师胡波。胡波本来
想拍丁真的弟弟尼玛，但是没有遇到，所以胡波临时决定改拍丁真。然后胡波于当
日将录制丁真的7秒短视频上传至抖音平台，丁真的短视频随即高热度传播，视频播
放量过千万，点赞数百万。丁真珍珠随后在新浪微博上成为热门话题，相关话题阅
读量达到五十亿。
Question: 丁真是怎么火起来的？
Context 1: 2020 年11月11日，丁真去舅舅家吃饭时，偶然遇到了摄影师胡波。胡波本
来想拍丁真的弟弟尼玛，但是没有遇到，所以胡波临时决定改拍丁真。
Context 2: 摄影师胡波将录制丁真的7秒短视频上传至抖音平台，丁真的短视频随即
高热度传播，视频播放量过千万，点赞数百万。丁真珍珠随后在新浪微博上成为热
门话题，相关话题阅读量达到五十亿。
—
Context:当今世界，国际竞争越来越多地表现为知识产权的竞争。企业遭遇的海外
知识产权纠纷日益激烈。我国已连续10年成为美国337调查的最大目标国，仅2012年
就遭受337调查13起，数十家企业涉案。大力发展知识产权服务业，可增强市场主体
的创新能力，促进品牌全球化，优化技术贸易结构，为企业实施“走出去”战略保驾
护航。同时，大力发展版权等知识产权服务，有利于保护优秀创意成果，提升版权
产业对国民经济的贡献率，促进文化产业和创意经济发展。而且，大力发展农产品
地理标志和农作物种业等知识产权服务，可优化现代农业和林业的产业布局，拓宽
农民增收渠道，促进城乡经济社会发展一体化。
Question:发展知识产权服务业到底有啥用？
Context 1:当今世界，国际竞争越来越多地表现为知识产权的竞争。企业遭遇的海外
知识产权纠纷日益激烈。我国已连续10年成为美国337调查的最大目标国，仅2012年
就遭受337调查13起，数十家企业涉案。大力发展知识产权服务业，可增强市场主体
20

--- PAGE 21 ---
的创新能力，促进品牌全球化，优化技术贸易结构，为企业实施“走出去”战略保驾
护航。
Context 2: 大力发展版权等知识产权服务，有利于保护优秀创意成果，提升版权产业
对国民经济的贡献率，促进文化产业和创意经济发展。而且，大力发展农产品地理
标志和农作物种业等知识产权服务，可优化现代农业和林业的产业布局，拓宽农民
增收渠道，促进城乡经济社会发展一体化。
—
Context:当前，高校所广泛采用的增量法对高校预算所具有的严肃性、权威性产生
着一定的制约作用，因此高校应当以避免对预算进行频繁变更为出发点，采用零基
预算、绩效预算等方式，做好中长期预算规划以及预算考评工作；另一方面高校有
必要针对财务构建完善的激励机制与约束机制。在对预算执行情况开展绩效考评的
基础上，高校不仅有必要将预算考评结果作为制定下一阶段预算规划的重要依据，
而且需要构建起奖励制度与问责制度，通过鼓励良好的预算执行行为、追求预算执
行不力现象当中的责任，对高校各个单位的预算执行行为进行激励与约束，从而确
保财务预算的执行能够始终处于合理的范围之内。
Question: 现在高校是怎么保证财务预算处于合理范围内的？
Context 1:当前，高校所广泛采用的增量法对高校预算所具有的严肃性、权威性产生
着一定的制约作用，因此高校应当以避免对预算进行频繁变更为出发点，采用零基
预算、绩效预算等方式，做好中长期预算规划以及预算考评工作。
Context 2: 现在高校有必要针对财务构建完善的激励机制与约束机制。在对预算执行
情况开展绩效考评的基础上，高校不仅有必要将预算考评结果作为制定下一阶段预
算规划的重要依据，而且需要构建起奖励制度与问责制度，通过鼓励良好的预算执
行行为、追求预算执行不力现象当中的责任，对高校各个单位的预算执行行为进行
激励与约束，从而确保财务预算的执行能够始终处于合理的范围之内。
—
Context: {Context}
Question:
Note that the three few-shot examples in the prompt templates are not entirely fixed but can be
adapted to different corpus by selecting suitable examples to stimulate the model to generate a
question distribution that is close to the distribution of real user questions about that domain corpus,
which can improve the quality of the synthetic data and the effectiveness of the final fine-tuned model.
C.2 Training Scorer to Rank Queries and Filtering
C.2.1 Training Scorer via Contrastive Learning
In our approach to enhancing data quality and diversity, we focus on the innovative construction of
positive and negative samples for training our scorer. By employing contrastive learning, we train the
scorer that efficiently evaluates the degree of adherence to instruct prompts and few-shot examples,
surpassing previous methods that rely on heuristic algorithms or direct scoring, which are prone to
positional bias and instability.
Positive samples are straightforwardly generated by employing the LLM to create context-query
pairs that adhere to well-designed instruct prompts and few-shot examples shown in Appendix C.1.2,
which are also the queries generated through Step 1. These serve as exemplars reflecting the desired
output. On the other hand, the creation of negative samples involves intentional manipulation of
instruct prompts or few-shot examples, or both. More specifically, we manipulate the instruction
by simplifying the instruct prompt to “Given a context, generate a question and split context into
two sub-contexts” . To manipulate few-shot examples, we degrading it to one-shot by retaining only
one example. We also combine both approaches to simultaneously manipulate the instruction and
few-shot examples to generate more negative samples. These manipulations aim to deviate from the
optimal query generation, thus producing examples that diverge from the model’s training objective.
For each type, we randomly select 500positive samples generated from Step 1 to construct negative
samples, forming 1,500positive-negative sample pairs for scorer training in total.
We construct the scorer’s structure as outlined in Section 2.3, initializing it with parameters from
our base LLM to serve as the training warm-up. The scorer’s parameter set is a duplicated version,
ensuring that modifications do not impact the original base LLM. For efficient training, we employ
21

--- PAGE 22 ---
QLoRA with 4-bit quantization and the ranks of 32, significantly reducing GPU memory requirements
and speeding up the training process. A more detailed breakdown of the hyperparameter configuration
is provided in Table 5.
C.2.2 Collaborating Scorer with CST to Filter Queries
The scorer is designed to work in cooperation with CST to enhance the quality and diversity of the
generated questions while also meeting the quantitative requirements. For a given context, we first
use CST to generate a series of potential questions. Then, each question is scored by the scorer, with
the scores used to rank them from highest to lowest. We sequentially add questions to a candidate
set, but only if the current question’s ROUGE-L F1 similarity to any question already in the set is
less than 0.7. This process continues until the number of questions in the candidate set reaches the
desired quantity N.
If the initial round of CST and scoring does not yield the required number of questions, we initiate
another round of CST to expand on the initial questions, followed by repeating the scoring and
selection process until the target quantity is achieved. The scorer’s role in scoring and filtering is
like to effectively condense the output from CST, ensuring that even with a smaller set of questions,
both high quality and diversity are maintained. The pseudocode of filtering process is shown in
Algorithm 2.
Algorithm 2 Scorer collaborates with CST to filter queries
Input: A Context C, Required number of maintained queries N
Output: Query dataset Data comprises exactly Nqueries with high quality and
diversity
1:function FILTER (QAll)
2: Initialize QCand←empty list
3: SortQAllby score descending
4: foreach (q, s)∈QAlldo
5: ifAll ROUGE-L[F1] with QCand <0.7then
6: Append qtoQCand ▷Append if diversity reach the threshold
7: iflen(QCand) =Nthen return QCand ▷Enough quantity
8: end if
9: end if
10: end for
11: return QCand
12:end function
13:
14:Initialize Data←empty list
15:Initialize QAll←empty list ▷Store all query-score pairs
16:while len(Data )< N do ▷Iterate until enough queries obtained
17: Initialize QNew←empty list
18: CONTEXT SPLIT TREE(C, Q New) ▷Call CST to get new queries
19: foreachq∈QNew do
20: Apeend (q, Sc(C, q))toQAll ▷Score each new query
21: end for
22: SetData←FILTER (QAll)
23:end while
24:return Data
C.3 Obtaining High-Fidelity Responses
Our design is motivated by the significant influence principles have on guiding LLMs, aiming to
achieve high-fidelity responses through a principle-driven self-alignment step. These principles are
anticipated to enhance the LLM’s ability to produce high-fidelity, realistic, and helpful answers, and
the specific principles vary depending on the task and remain exploratory. They may also include
rules for directing the LLM to generate responses in a particular tone. This could be particularly
valuable when creating SFT data for a custom LLM assistant or for role-playing. Furthermore, the
22

--- PAGE 23 ---
existence of principles serves as a method for aligning with human preferences, offering a viable
alternative to the cumbersome process of reinforcement learning from human feedback (RLHF) [ 62].
Different from previous approaches, we innovate to integrate a self-improving pipeline to further
increase fidelity. Instead of manually selecting a few-shot examples from annotated examples, we
divide the annotated examples into training and testing sets. We then conduct a random search that
iteratively selects a subset from the training set and allows the LLM to self-evaluate the output scores
in the test set. This process is iterated 16times by default, and the subset that achieves the highest
scores in the test set is used as the few-shot ICL examples. We implement this pipeline through the
DSPy [ 34] framework, significantly reducing coding effort. This self-improving process works well
with principle-driven self-alignment, as it aids in identifying the optimal ICL examples that guide the
LLM to generate helpful, realistic, and reliable answers in line with alignment principles, markedly
enhancing the quality and fidelity of the responses and, consequently, the responses by the fine-tuned
models.
Finally, we prune all contexts, principles, and ICL examples to retain only the query-response pairs for
supervised fine-tuning of the LLM. While several studies [ 98,94,90] try to further execute filtering
on generated answers, we leave it as a future work as it is not such crucial for our method. Actually,
simply generating additional iterations on the same query and retaining self-consistent [ 82] responses
may further improve some degrees of reasoning accuracy for short-form responses. However, this
might not be a good deal when also taking the computing costs into consideration since letting LLMs
improve and correct their own responses is not an easy thing [ 30]. We believe that since each question
we obtain in CST precisely matches the granularity of its context, it will be easy for LLM to provide
accurate and pertinent answers to the questions.
D Implementation Details
All experiments are implemented on a single node with eight Nvidia A100 80G GPUs and 160Intel
Xeon Gold 6248 CPUs. To speed up the generation of LLM calls, we use the vLLM [ 38] inference
engine for acceleration, and make concurrent requests with a concurrency of 8threads. In order to
reduce GPU memory usage and accelerate training speed, we use the DeepSpeed [ 69] distributed
training framework accelerating with ZeRO-2 [ 66], where the AdamW [ 56] optimizer is applied for
gradient descent. We employ QLoRA with 4-bit quantization and the ranks of 32, which has been
demonstrated to be able to achieve satisfactory results in previous works [ 31,100]. We train for 4
epochs in total, for it has been shown as the maximum number of iterations that negligible affect the
training loss [ 61]. On the DailyM dataset, our AUGCONgenerates about 120K pieces of data in 184
A100 GPU hours and required another 272A100 GPU hours for supervised fine-tuning. On four
benchmarks, the generation throughput is about 340pairs per A100 GPU hour and the total running
times vary from 120to448A100 GPU hours depending on corpus size. More detailed settings on
hyperparameters can be found in Appendix D.2.
D.1 Assets Use
All the pre-trained LLM and open-source datasets we use in experiments can be respectively found
on Huggingface transformers [ 88] and datasets [ 39], and we have checked that they are available for
research purposes and have been properly cited and correctly adhered to open-source licenses. We list
the public links of the used LLMs in Table 2 and the used datasets in Table 3. We will also make our
DailyM dataset open-sourced at https://github.com/quanshr/AugCon to boost the academy.
Table 2: Public links to the used LLMs.
LLM Link
Qwen1.5-c 32B[7] https://huggingface.co/Qwen/Qwen1.5-32B-Chat
Llama3-c 70B[4] https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct
23

--- PAGE 24 ---
Table 3: Public links to the used datasets.
Dataset Link
SQuAD1.1 [67] https://huggingface.co/datasets/rajpurkar/squad
TriviaQA [33] https://huggingface.co/datasets/mandarjoshi/trivia_qa
DROP [21] https://huggingface.co/datasets/ucinlp/drop
WebGLM-QA [50] https://huggingface.co/datasets/THUDM/webglm-qa
D.2 Hyperparameters
We present the generation hyperparameters in Table 4 and the fine-tuning configurations in Table 5.
Table 4: Generation Hyperparameters
Parameter Value
Max instruction length 4096
Max new tokens 4096
Top-k 50
Top-p 1.0
Temperature (for query) 0.85
Temperature (for response) 0.2
Table 5: Training Configurations
Parameter Value
Epoch 4
Learning rate 5e-5
Mini batch size 4
Warmup steps 50
Weight decay 0.01
Compute dtype bfloat16
Quantization dtype nf4
Lora rank 32
Lora alpha 32
Lora dropout 0.05
Lora bias none
E Human Evaluation Guidance
To establish a robust assessment framework for both generated queries and model outputs, we
have devised an extensive human evaluation guideline shown in Table 6. Each score will also be
accompanied by several corresponding examples, ensuring a consistent and objective evaluation
process. This guideline emphasizes key metrics, including realism, diversity, relevance, accuracy,
and satisfaction. By following this guide, evaluators can thoroughly assess the effectiveness of our
method, guaranteeing the generation of high-quality, multi-granularity queries and responses. Our
approach strives for comprehensive evaluations, aided by detailed scoring rubrics and examples to
enable balanced decision-making.
24

--- PAGE 25 ---
Table 6: The guidance for human evaluation.
Score Realism
5 The query is indistinguishable from those a human might ask. It is natural, authentic,
and precisely the type of question a curious user would pose.
4 The query closely resembles real user inquiries, with minor differences. It maintains a
high level of realism and naturalness.
3 The query shows moderate realism, differing somewhat from typical user questions. It
still appears natural and understanding.
2 The query has noticeable deviations from real user questions, affecting its realism. It
shows signs of artificiality but remains understandable.
1 The query is clearly artificial, lacking realism and naturalness. It differs significantly
from how a real user would ask.
Score Diversity
5 The queries exhibit exceptional diversity, covering a wide range of topics and varying
greatly in their nature and specificity.
4 The queries show good diversity, exploring multiple topics and presenting different
types of questions. They maintain a solid variety, even if not exhaustive.
3 The queries present moderate diversity, touching upon several topics but with some
repetitiveness or predictability in their nature.
2 The queries show limited diversity, often sticking to a narrow range of topics or lacking
variety in their structure and content.
1 The queries lack diversity, being highly repetitive, monotonous, and showing minimal
to no variation in topics or approach.
Score Relevance
5 The response is highly relevant, precisely addressing the query’s intent and providing
contextually appropriate information.
4 The response is mostly relevant, with minor deviations that do not significantly affect
its overall alignment with the query.
3 The response shows moderate relevance, partially addressing the query but with some
noticeable gaps or misalignments.
2 The response has limited relevance, straying significantly from the core of the query or
providing only partially related information.
1 The response is irrelevant, failing to address the query’s intent or providing information
that is completely off-topic.
Score Accuracy
5 The response is completely accurate, with no factual errors or hallucinations. All
information provided is verifiable and aligns with external sources.
4 The response contains minor inaccuracies or minor hallucinations, but the overall
information conveyed is mostly correct and reliable.
3 The response shows moderate accuracy, with some noticeable factual errors or
hallucinations that don’t significantly alter the main message.
2 The response has significant inaccuracies or hallucinations, affecting the overall
reliability and correctness of the information provided.
1 The response is highly inaccurate, containing multiple factual errors or severe
hallucinations that render the information untrustworthy.
Score Satisfaction
5 The evaluator is highly satisfied with the responses. They fully meet expectations,
leaving no room for improvement.
4 The evaluator is mostly satisfied. The responses are generally good, with only minor
shortcomings or areas for improvement.
3 The evaluator feels moderately satisfied. The responses have notable strengths but also
some weaknesses that need addressing.
2 The evaluator is somewhat dissatisfied. The responses show significant room for
improvement and may not fully meet expectations.
1 The evaluator is highly dissatisfied. The responses fail to meet expectations on multiple
levels, requiring substantial improvement.
25

--- PAGE 26 ---
F Case Demonstration
In this section, we select a specific context as a case demonstration to inspect the entire process around
it, from generating questions by constructing Context-Split-Tree, to scoring queries and filtering
them, and finally to generating answers by attaching self-alignment principles and optimal few-shot
examples. This demonstration is designed to provide a clearer and more intuitive understanding of
our algorithm’s workflow.
Initially, we input the starting context as the root node of the whole Context-Split-Tree. With the
assistance of the LLM, we derive an overall question from this context and simultaneously split it into
two child contexts. We have omitted the specific instruction in the text boxes below, as it has already
been presented in detail in Appendix C.1.2. The Context in boxes is the input while the Question,
Context 1, and Context 2 are parsed outputs:
Node 1 →Node 2, Node 3
Context: The profits of the contemporary global value chains (GVC) form a V-shape, also
known as the “smile curve”. At one end of this curve are research and development (R&D)
and design, and at the other end are services and marketing, with processing and production
situated in the middle. Typically, the profit margin for industries at both ends ranges from
20% to 25%, whereas the profit margin for the production processes in the middle stands at
merely 5%.
Question: Why do entrepreneurs worldwide strive to move up the value chain?
Context 1: The profits of the contemporary global value chains (GVC) form a V-shape, also
known as the “smile curve”. At one end of this curve are research and development (R&D)
and design, and at the other end are services and marketing, with processing and production
situated in the middle.
Context 2: Typically, the profit margin for industries at both ends of the global value chains
ranges from 20% to 25%, whereas the profit margin for the production processes in the
middle stands at merely 5%.
The child context undergoes recursive splitting and derives questions with corresponding granularity:
Node 2 →Node 4, Node 5
Context: The profits of the contemporary global value chains (GVC) form a V-shape, also
known as the “smile curve”. At one end of this curve are research and development (R&D)
and design, and at the other end are services and marketing, with processing and production
situated in the middle.
Question: What are the key components of the contemporary global value chains?
Context 1: The profits of the contemporary global value chains (GVC) form a V-shape, also
known as the “smile curve”.
Context 2: At one end of the smile curve are research and development (R&D) and design,
and at the other end are services and marketing, with processing and production situated in
the middle.
We use depth-first-search (DFS) so this is the time for Node 4. Actually, using breadth-first-search
(BFS) will ultimately achieve the same effect. Node 4 derives a question but terminates afterward
due to its unsuccessful attempt to split into two viable child contexts:
Node 4 Ends
Context: The profits of the contemporary global value chains (GVC) form a V-shape, also
known as the “smile curve”.
Question: What does the global value curve look like?
Context 1: The profits of the contemporary global value chains (GVC) form a V-shape, also
known as the “smile curve”.
Context 2:
26

--- PAGE 27 ---
The child Context 1 split from Node 5 will not further derive questions or split because it falls below
the minimum length threshold. However, one can adjust this threshold to a lower value to derive
more detailed questions:
Node 5 →Node 6
Context: At one end of the smile curve are research and development (R&D) and design,
and at the other end are services and marketing, with processing and production situated in
the middle.
Question: What is the structure of the smile curve?
Context 1: At one end of the smile curve are research and development (R&D) and design.
Context 2: The other end of the smile curve are services and marketing, with processing and
production situated in the middle.
Node 6 derives a question and terminates because both of its child contexts are too short:
Node 6 Ends
Context: The other end of the smile curve are services and marketing, with processing and
production situated in the middle.
Question: What lies in the middle of the smile curve?
Context 1: The other end of the smile curve are services and marketing.
Context 2: The processing and production are situated in the middle.
After Node 2 has completed its recursion, it is now Node 3’s turn to proceed:
Node 3 →Node 7, Node 8
Context: Typically, the profit margin for industries at both ends of the global value chains
ranges from 20% to 25%, whereas the profit margin for the production processes in the
middle stands at merely 5%.
Question: Which type of industry has the lowest profit margin?
Context 1: Typically, the profit margin for industries at both ends of the global value chains
ranges from 20% to 25%.
Context 2: Whereas the profit margin for the production processes in the middle stands at
merely 5%.
Node 7 and Node 8 terminate after deriving one detailed question each, as they have reached the
minimum granularity and cannot split properly:
Node 7 Ends
Context: Typically, the profit margin for industries at both ends of the global value chains
ranges from 20% to 25%.
Question: How high can the profit margin go for industries at two ends of the global value
chains?
Context 1: Typically, the profit margin for industries at both ends of the global value chains
ranges from 20% to 25%.
Context 2:
Node 8 Ends
Context: Whereas the profit margin for the production processes in the middle stands at
merely 5%.
Question: What is the profit margin for the production processes?
Context 1: Whereas the profit margin for the production processes in the middle stands at
merely 5%.
Context 2:
27

--- PAGE 28 ---
Then, the recursion comes to an end, and this entire process ultimately results in the formation of the
Context-Split-Tree depicted in Figure 3. Each node within this tree contains a context and a question
that align with the corresponding granularity.
1
2 3
4 5
67 8 Minimum Granularity
Figure 3: The schematic of the constructed CST in this case. Each node contains a context and a
corresponding question, with the node size indicating different levels of granularity.
We collect context question pairs from all nodes into a list and employ the trained scorer to evaluate
each item. The items are then sorted based on their scores, from highest to lowest. Next, we
sequentially examine each item and choose those queries whose ROUGE-L scores with all previously
selected queries are below 0.7. Due to the low ROUGE-L scores among query pairs in that case, the
resulting selected set shown in Table 7 primarily comprises the preceding few items. However, if
we aim to derive a greater number of questions from the context, such as 10 questions, an additional
round of CST becomes necessary. Following this, all queries generated across both CST sessions
undergo a collective ranking and filtering process. During this step, the ROUGE-L metric proves
useful for eliminating queries that have lower scores and are similar to previously selected ones.
At this point, in addition to our results, we also provide the generated query list using the Context-
Instruct method for a direct comparison. The Context-Instruct method produces question-confidence-
answer triplets, where the confidence level can be either high or low and is used for filtering purposes.
The results are presented in Table 8, with the responses omitted to conserve space. Putting Table 7 and
Table 8 together, we can perceive that the queries generated by Context-Instruct exhibit a noticeable
lack of diversity and granularity compared to our queries. This discrepancy provides an intuitive
explanation for our method’s superior performance in generating multi-granularity queries and
ultimately producing better results than other methods.
query score select
Why do entrepreneurs worldwide strive to move up the value chain? 0.95 ✓
What are the key components of the contemporary global value chains? 0.91 ✓
How high can the profit margin go for industries at two ends of the global value
chains?0.88 ✓
What does the global value curve look like? 0.83 ✓
Which type of industry has the lowest profit margin? 0.74 ✗
What is the structure of the smile curve? 0.67 ✗
What is the profit margin for the production processes? 0.64 ✗
What lies in the middle of the smile curve? 0.59 ✗
Table 7: The results of ranking and filtering the queries.
Returning to our workflow, after filtering the queries, we leverage the alignment principles and
a select few-shot examples (detailed in Appendix 5) to assist our LLM in providing high-fidelity
answers to these questions. Thanks to our Context-Split-Tree process providing the specific window
of context required to answer the questions, obtaining these answers is relatively easy and results
in more accurate and context-relevant responses. Once we have obtained high-fidelity answers, we
remove the alignment principles, few-shot examples, and context, retaining only the query-response
28

--- PAGE 29 ---
query confidence select
What is the shape of the profits of global value chains? High ✓
What is the “smile curve” also known as? High ✓
What is the primary function of the industries at one end of the “smile
curve”?High ✓
Which activities have a profit margin ranging from 20% to 25%? High ✓
What is the typical profit margin for industries in the middle of the “smile
curve”?High ✗
What is situated at both ends of the “smile curve”? Low ✗
Do companies typically earn higher profits from production processes than
from R&D?Low ✗
Is the “smile curve” a linear representation of profits in global value chains? Low ✗
Table 8: The generated questions using Context-Instruct.
pairs to supervised fine-tune our LLM. The final obtained SFT data for this case are presented in the
following text tables.
SFT Data 1
Question: Why do entrepreneurs worldwide strive to move up the value chain?
Answer: Entrepreneurs worldwide strive to move up the value chain because the profit
margins are significantly higher at the ends of the curve, ranging from 20% to 25%, compared
to the middle, which has a profit margin of only 5%. By moving up the value chain,
entrepreneurs can increase their profit margins and gain a competitive advantage in the
market.
SFT Data 2
Question: What are the key components of the contemporary global value chains?
Answer: The key components of the contemporary global value chains are:
1. Research and Development (R&D) and Design (at one end of the curve)
2. Processing and Production (at the other end of the curve)
3. Services and Marketing (in the middle of the curve)
SFT Data 3
Question: How high can the profit margin go for industries at two ends of the global value
chains?
Answer: The profit margin for industries at both ends of the global value chains can go up to
25%.
SFT Data 4
Question: What does the global value curve look like?
Answer: It looks like a V-shape, also known as the “smile curve”.
G Additional Experiments
G.1 Ablation Study
In this section, we detail the ablation experiments conducted to assess the indispensability and impact
of the three essential steps in our proposed method. These steps are integral to our approach, designed
to recursively derive queries, rank and filter them for quality and diversity, and finally, generate
high-fidelity responses. Through these experiments, we aim to delineate the contribution of each step
29

--- PAGE 30 ---
towards the overall effectiveness of our method. In this study, we develop the following four distinct
variations of our method, with each one specifically tailored to concentrate on a fundamental step:
1.AUGCONw/o
CST1 omits the use of the Context-Split-Tree for iteratively splitting and generating
queries for given contexts. Instead, AUGCONw/o
CSTemploys a technique where few-shot examples
are used to iteratively derive queries from the extracted context until the desired number of queries
is obtained (we set the desired number to be the same with all generated queries of AUGCON
without filtering). The purpose of this modification is to assess the efficacy of CST in deriving
multi-granularity queries. Additionally, this variant facilitates an examination of how the exclusion
of CST impacts the diversity of the generated queries and the overall performance of the final
fine-tuned model.
2.AUGCONw/o
CST2also omits the use of the Context-Split-Tree for iteratively splitting and generating
queries for given contexts. Different from AUGCONw/o
CST1,AUGCONw/o
CST2 splits the contexts in a
heuristic way that each time splits it in the middle (we will let the whole sentence in the middle in
the first sub context to maintain semantic integrity) until reaching the minimum granularity. And
then use all split contexts to iteratively derive queries until the quantity is enough. This variant
is designed to further assess the efficacy of CST in deriving multi-granularity queries with the
comparison with a heuristic context segmentation method.
3.AUGCONw/o
filtereliminates the scoring and filtering process to evaluate its effects on the overall
quality and diversity of the generated queries. If the number of queries generated in Step 1 exceeds
the predetermined limit, we just proceed by randomly selecting a sufficient number of queries to
meet the quota. This variant enables us to assess the effects of bypassing our established quality
and diversity control mechanisms.
4.AUGCONw/o
fidelityobtains the answers to the queries without adhering to self-alignment or employing
the self-improving. Instead, AUGCONw/o
fidelity utilizes fixed predetermined few-shot examples along
with a straightforward prompt design devoid of guiding principles. This variant allows us to
evaluate the efficacy of our response generation methodology in enhancing the overall quality and
relevance of responses.
We implement the four variants on TriviaQA (short-form) and WebGLM-QA (long-form) datasets
and conduct a comparison with our A UGCON. The results are shown in Table 9.
Short-form (Acc) Long-form (BS)
Variant TriviaQA WebGLM-QA
AUGCONw/o
CST1 0.793±0.003 0.912±0.001
AUGCONw/o
CST2 0.826±0.003 0.910±0.001
AUGCONw/o
filter 0.828±0.003 0.915±0.001
AUGCONw/o
fidelity 0.833±0.004 0.907±0.002
AUGCON 0.849±0.003 0.924±0.002
Table 9: The results of ablation study.
Our analysis has led to three key insights. Firstly, when compared to our AUGCON, all variants
yield suboptimal outcomes. This highlights the critical nature of each step within our methodology,
underscoring the fact that they are all crucial and collectively contribute to achieving superior
performance.
Secondly, within the context of short-form datasets, it was observed that the variants that undergo
modifications in the CST process perform the poorest. This finding suggests that the CST process
plays a vital role in encompassing a comprehensive scope of granularity, thereby enabling the
extraction of a broader spectrum of knowledge.
Thirdly, with regard to long-form datasets, the variant AUGCONw/o
fidelity demonstrates the lowest level
of performance. This outcome underlines the significance of self-alignment and self-enhancement
mechanisms in generating responses of high quality and fidelity.
30

--- PAGE 31 ---
G.2 GPT-4 Judge
To conduct a more comprehensive evaluation, we utilized GPT-4 [ 2] (we use gpt-4-0125-preview) to
judge as the referee on the baseline method and our approach. By using GPT-4 to compare the outputs
of the model fine-tuned on our generated data with the baseline methods, we can better understand
the advantages of our model and avoid biases stemming from manual preferences. We provide the
query in the DailyM test set as inputs and get the outputs of the model fine-tuned using our method
and baseline method on the DailyM dataset. To facilitate a nuanced evaluation, we categorized the
queries into three levels of granularity: detail question ( e.g.,how deep does the abortion needle
penetrate? ), concept question ( e.g.,what is the difference between emergencies and crises? ), and
macro question ( e.g.,what impact does faith have on us? ). We present the detailed guidance for
categorizing in Table 11.
For the reliability of the results, we extract relevant references to the questions in the dataset corpus
to assist GPT-4 in making decisions. Then, we ask GPT-4 to compare the outputs generated by the
two models, with the template shown in below text box. To mitigate the potential impact of position
bias of LLMs, we implement a robust evaluation strategy that for each pair of outputs, we swap
their positions and queried GPT-4 twice. In cases where the two responses were not consistent, we
continued to inquire until we obtained a unanimous answer. The comparison results are shown in
Figure 4.
GPT-4 Judge Template
Given a question and relevant reference, decide which one is better between answer 1 and
answer 2.
Question: {Question}
Reference: {Reference}
Answer 1: {Answer1}
Answer 2: {Answer2}
Your decision:
Chat DAPT AdaptLLM ETRC Con-Instr20253035404550Winning Rate (%)
(a) Detail question
Chat DAPT AdaptLLM ETRC Con-Instr20253035404550Winning Rate (%) (b) Concept question
Chat DAPT AdaptLLM ETRC Con-Instr20253035404550Winning Rate (%) (c) Macro question
Figure 4: The results of GPT-4 judge on three levels of questions.
The performance comparisons are depicted in Figure 4. Note that since we employ a decisive, non-tie
judgment, for a method to be considered superior, it must surpass the 50% winning threshold. Through
this rigorous comparison, it becomes apparent that our proposed method significantly outperforms
the baseline methods across all three categories of question granularity. Notably, the winning rates
for the baseline methods generally fall below 40% in most instances, underscoring the effectiveness
of our approach.
Among the baseline methods evaluated, ETRC and Context-Instruct exhibit relatively better per-
formance in the domain of concept questions, with winning rates slightly exceeding 40%. This
performance is commendable when compared to other baseline methods. However, it’s important
to acknowledge that even these higher-performing baselines do not meet the 50% threshold and
exhibit weaknesses across the other two question granularities. This observation highlights a critical
advantage of our method: its ability to maintain a balanced focus across different types of questions.
Consequently, our approach not only excels in one particular area but also enhances performance
across all three types of queries simultaneously.
31

--- PAGE 32 ---
The results from this comparison clearly illustrate the superior capability of our method in handling
a diverse range of question complexities and types. This balanced focus is pivotal for developing
systems that can adapt to varied informational needs, thereby improving the overall query response
performance. Our findings suggest that our method could significantly contribute to advancements in
improving the performance of fine-tuned models by offering a more versatile and effective approach
to extracting multi-granularity query-response pairs from context.
G.3 Computation Experiment
In previous experiments, since we find that all methods spend much more time on final fine-tuning
compared to the previous generation, we maintained that each method produced an equivalent
number of query response pairs for the given contexts to balance computing resources. Nonetheless,
when examining methods like ETRC, Context-Instruct, and our AUGCONthat need LLM calls, we
observed variations in the computational time required to generate each query response data pair.
In this evaluation, we control the LLM computation to be the same, all gave 80A100 GPU hours
for running the workflow to generate data from the DailyM dataset and then compare the fine-tuned
model using GPT-4 judge on DailyM test set. The results are shown in Table 10.
Winning Rate
AUGCON 64.5% 35.5% ETRC
AUGCON 60.3% 39.7% Con-Instr
Table 10: The winning rates in computation experiment.
In our comparative analysis, we discovered that our AUGCONconsistently achieved higher winning
rates when set against two other LLM-based, context-driven query extraction methodologies. This
finding is particularly significant as it validates the efficacy of our method in yielding positive
outcomes even with a reduced quantity of query-response pairs. The cornerstone of this enhanced
performance lies in the superior quality and diversity of the data pairs generated by our method.
Unlike conventional approaches that may produce voluminous but redundant data, our method
focuses on creating data pairs that are both essential and varied. This strategic approach to data
generation ensures that AUGCONoperates efficiently, making the most of every data pair to contribute
meaningfully to the fine-tuning of LLMs. As a result, AUGCONstands out as a highly data-efficient
method, capable of fine-tuning LLMs with less generated data to achieve better performance. This
advantage is especially crucial in scenarios where access to large amounts of the corpus or the
computation resource is constrained. By eliminating redundancy and emphasizing the importance of
quality and diversity, AUGCONpaves the way for more effective and efficient context-based SFT data
generation processes, ultimately leading to enhanced model performance even with limited sources.
G.4 Training Phase
We present the loss curve training on the generated DailyM-SFT in Figure 5. From the graph, it is
evident that the training loss decreases gradually in general throughout the training phase, with a
notable sharp decline at the outset. An interesting observation is that the training loss appears to
plateau within epochs from Epoch 2 onwards, yet we observe sudden drops in loss at the boundaries
between two consecutive epochs. This pattern strongly signals that our training dataset is characterized
by extremely low similarity and exceptionally high diversity, meaning that training on one segment of
data does not have an impact on the loss associated with another segment. Conversely, if the dataset
were more homogeneous, for instance, by including duplicates of certain data points, we would likely
observe some decrease in the loss within epochs, just like the drop in two consecutive epochs in
our curve. This phenomenon also indicates that the knowledge encapsulated in our responses is
remarkably diverse too, covering a broad spectrum of information in the extracted domain corpus
with minimal data usage, which is important for improving the performance of fine-tuned LLMs
according to existing research [25, 42].
We conduct a comprehensive human evaluation at each checkpoint during model training, and the
overall satisfaction scores are presented in Figure 5. Our results show that, on the DailyM test set,
the evaluation scores exhibit a rapid and steady increase as training progresses. Furthermore, to
32

--- PAGE 33 ---
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Epoch0.751.001.251.501.752.002.252.502.75Train PPL
3.43.63.84.04.24.4
Score
DailyM
AlignBenchFigure 5: The training loss and human evaluation results during training phase.
verify that our training does not compromise the model’s general conversational abilities, we also
employ AlignBench [ 51], a widely used benchmark that assesses the chat models abilities across
multiple dimensions, including language understanding, question answering, writing, reasoning,
and so on. Notably, thanks to the wide range of high-quality daily-focused articles provided by our
DailyM corpus, our model trained on the derived DailyM-SFT not only preserves its original general
abilities but also demonstrates a slight improvement, which also highlights the advantages of our
methodology.
G.5 Granularity Comparison
In exploring the landscape of multi-granularity question generation, our methodology introduces a
nuanced approach to constructing questions across different levels of granularity. We categorize ques-
tions into three distinct types based on their scope and depth: detail questions, concept questions, and
macro questions, with the detailed guidance for categorizing shown in Table 11. This categorization
allows us to systematically address the varying levels of knowledge Q&A intends and users’ interests.
(a) A UGCON
37.8%
(Detail)35.3%
(Concept)26.9%
(Macro) (b) Context-Instruct
17.9%
(Detail)
63.4%
(Concept)18.7%
(Macro)
Figure 6: The proportion of three levels of granularity questions generated by AUGCONand Context-
Instruct.
The proportions of three types of questions are displayed in Figure 6. Our approach yields a balanced
distribution of question types, with 37.8% detail questions, 35.3% concept questions, and 26.9%
macro questions. This distribution is indicative of our method’s capacity to generate a diverse array of
questions, catering to a broad spectrum of informational needs and cognitive processes. The balance
ensures that the LLM is not only able to obtain specific information (detail questions) but is also able
33

--- PAGE 34 ---
to grasp broader concepts (concept questions) and understand overarching themes or ideas (macro
questions).
In contrast, when compared to the Context-Instruct method, which produces questions with a
distribution of 17.9% detail questions, 63.4% concept questions, and 18.7% macro questions, the
advantages of our approach become evident. The Context-Instruct method exhibits a pronounced
emphasis on concept questions, which, while valuable, suggests a potential oversight of the importance
of detail and macro questions. Such distribution may limit the method’s effectiveness in scenarios
where a more detailed understanding or a broader overview is essential.
Additionally, combined with the previous analysis of the CST process, our method is superior in
constructing a multi-granularity question set that is not only balanced but also versatile. By ensuring a
more equitable distribution of question types, our approach enhances the comprehensiveness and depth
of information exploration. It facilitates a more holistic understanding of content, accommodating a
wider range of learning styles and information-seeking behaviors. This versatility is paramount in
improving the fine-tuned LLMs’ ability to tackle a wider array of questions and applications.
34

--- PAGE 35 ---
Table 11: The guidance to categorize each granularity of questions.
Category Description and Examples
Detail
QuestionsDescription: Detail questions ask for specific information or facts about a partic-
ular aspect of a broader topic. These questions are often precise and seek exact
answers.
Tips for Identification :
•Look for questions asking for “how,” “what,” “where,” “when,” or “who” in
a specific context.
•These questions usually focus on a narrow aspect rather than the whole topic.
Examples:
• How deep does the abortion needle penetrate?
• What is the boiling point of mercury?
• Who was the first person to climb Mount Everest?
Concept
QuestionsDescription: Concept questions explore the understanding, differences, or defini-
tions of ideas, theories, or terminologies. They are more about the “why” or “what
is” than about specific details.
Tips for Identification :
• These questions often ask for explanations, comparisons, or definitions.
• They seek to clarify concepts or understand the distinctions between them.
Examples:
• What is the difference between emergencies and crises?
• How do you differentiate between classical and operant conditioning?
• What does the term ’biodiversity’ encompass?
Macro
QuestionsDescription: Macro questions address broad themes, trends, or impacts. They
are expansive and consider the bigger picture, often relating to societal, global, or
philosophical inquiries.
Tips for Identification :
•Look for questions that ask about “impact,” “influence,” “role,” or “impor-
tance.”
• These questions are overarching and not limited to specific instances.
Examples:
• What impact does faith have on us?
• How does climate change affect global agriculture?
• What is the role of technology in shaping modern education?
35

--- PAGE 36 ---
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations of the work in Appendix A.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
36

--- PAGE 37 ---
Justification: We have a proof on linear relationship and we provide the full set of assump-
tions and a complete and correct proof in Appendix C.1.1.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We disclose our implementation details including all crucial hyperparameters,
training configurations, and the used models and datasets in Appendix D, and our method
is elaborated in detail in Section 2 and Appendix C attached with specific prompts, which
will be sufficient to reproduce the main experimental results of the paper to the extent that it
affects the main claims and/or conclusions of the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
37

--- PAGE 38 ---
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide the links to the used models and datasets in Appendix D.1 and
provide our code of AUGCON, proposed dataset DailyM , generated SFT dataset DailyM-SFT
and fine-tuned model Qwen-DailyM-32B with sufficient instructions to faithfully reproduce
the main experimental results in our GitHub repository https://github.com/quanshr/
AugCon .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All training and testing details of our work can be found in Appendix D and
Section 3
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report error bars suitably and correctly defined in Table 1.
Guidelines:
• The answer NA means that the paper does not include experiments.
38

--- PAGE 39 ---
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide sufficient information on the computer resources (type of com-
pute workers, memory, time of execution) needed to reproduce the experiments in Ap-
pendix D and G.3.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
39

--- PAGE 40 ---
Justification: We discuss both potential positive societal impacts and negative societal
impacts of the work in Appendix B.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our released model is fine-tuned from the existing open-source model with
SFT data derived from high-quality magazines closely related to daily life, which contains
no harmful or misleading information and does not have such risks of misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We conform to the license and terms of all used assets, as mentioned in
Appendix D.1.
Guidelines:
• The answer NA means that the paper does not use existing assets.
40

--- PAGE 41 ---
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide the construction details of the assets in Section 3.2, and the
documentations are provided alongside the assets at https://github.com/quanshr/
AugCon .
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [Yes]
Justification: We include the detailed human evaluation metrics in Section 3.2.1 and the full
text of instructions in Appendix E.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
41

--- PAGE 42 ---
Answer: [NA]
Justification: Not human subjects research.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42
