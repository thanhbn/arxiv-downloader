# Auto-Instruct: Tự Động Tạo và Xếp Hạng Hướng Dẫn
cho Các Mô Hình Ngôn Ngữ Hộp Đen

Zhihan Zhang♠∗, Shuohang Wang♢, Wenhao Yu♠, Yichong Xu♢, Dan Iter♢,
Qingkai Zeng♠, Yang Liu♢, Chenguang Zhu♢, Meng Jiang♠
♠University of Notre Dame
♢Microsoft Azure AI
zzhang23@nd.edu

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) có thể thực hiện nhiều loại nhiệm vụ khác nhau bằng cách tuân theo các hướng dẫn bằng ngôn ngữ tự nhiên, mà không cần thiết phải tinh chỉnh cụ thể cho từng nhiệm vụ. Thật không may, hiệu suất của LLMs bị ảnh hưởng rất lớn bởi chất lượng của các hướng dẫn này, và việc viết thủ công các hướng dẫn hiệu quả cho mỗi nhiệm vụ là một quá trình tốn công sức và mang tính chủ quan. Trong bài báo này, chúng tôi giới thiệu Auto-Instruct, một phương pháp mới để tự động cải thiện chất lượng của các hướng dẫn được cung cấp cho LLMs. Phương pháp của chúng tôi tận dụng khả năng sinh tự nhiên của LLMs để tạo ra các hướng dẫn ứng cử viên đa dạng cho một nhiệm vụ nhất định, sau đó xếp hạng chúng bằng một mô hình tính điểm được huấn luyện trên 575 nhiệm vụ NLP hiện có đa dạng. Trong các thí nghiệm trên 118 nhiệm vụ ngoài miền, Auto-Instruct vượt trội hơn cả hướng dẫn do con người viết và các phương pháp cơ sở hiện có của hướng dẫn do LLM tạo ra. Hơn nữa, phương pháp của chúng tôi thể hiện khả năng tổng quát hóa đáng chú ý ngay cả với các LLMs khác không được đưa vào quá trình huấn luyện của nó.¹

## 1 Giới thiệu
Các mô hình ngôn ngữ lớn được điều chỉnh theo hướng dẫn (LLMs) đã trở nên rất phổ biến như các giải pháp cho vô số nhiệm vụ NLP, nhờ vào khả năng thông thạo của chúng trong việc diễn giải các hướng dẫn bằng ngôn ngữ tự nhiên (Wei et al., 2021; Chung et al., 2022; Ouyang et al., 2022; Taori et al., 2023). Khi việc tinh chỉnh LLMs thường trở nên không khả thi, các hướng dẫn đóng vai trò ngày càng quan trọng trong việc thúc đẩy những LLMs hộp đen như vậy. Đặc biệt trong bối cảnh few-shot thực sự² (Perez et al., 2021) nơi người dùng nhằm mục đích giải quyết một nhiệm vụ mới chỉ với một mô tả nhiệm vụ cơ bản và một vài dữ liệu

∗Công việc này được thực hiện khi Zhihan đang thực tập tại Microsoft Azure AI.
¹Mô hình và mã nguồn có sẵn tại https://github.com/ytyz1307zzh/Auto-Instruct .
²Một kịch bản trong đó không có dữ liệu huấn luyện hoặc xác thực bổ sung nào có sẵn để điều chỉnh siêu tham số và lựa chọn gợi ý, ngoài các ví dụ few-shot (Perez et al., 2021).

[Hình 1: Pipeline Auto-Instruct. Đầu tiên chúng tôi gợi ý LLM để tạo ra một tập hợp đa dạng các hướng dẫn ứng cử viên với các phong cách khác nhau, sau đó huấn luyện một mô hình để xếp hạng và chọn hướng dẫn hiệu quả nhất cho một ví dụ nhất định. Cuối cùng, hướng dẫn được chọn được sử dụng để gợi ý LLM suy luận đầu ra cho ví dụ này.]

ví dụ trong tầm tay, một hướng dẫn được tạo ra khéo léo là rất cần thiết để cho phép LLM nắm bắt ánh xạ đầu vào-đầu ra cần thiết để hoàn thành nhiệm vụ.

Mặc dù tầm quan trọng của các hướng dẫn, cách tiếp cận phổ biến khi sử dụng LLM hộp đen trên một nhiệm vụ mới vẫn là kỹ thuật gợi ý thủ công (White et al., 2023; Mishra et al., 2023). Tuy nhiên, cách tiếp cận như vậy không chỉ tốn thời gian mà còn có xu hướng tạo ra các hướng dẫn chưa tối ưu.

Trước bối cảnh này, các nỗ lực đã được thực hiện để trao quyền cho LLMs tự động tạo ra các hướng dẫn (Honovich et al., 2022; Zhou et al., 2022; Singh et al., 2022). Những cách tiếp cận này cung cấp cho LLM một số ít ví dụ và gợi ý nó tạo ra một hướng dẫn dựa trên những minh chứng này. Trong khi những phương pháp như vậy thể hiện khả năng của LLM trong việc tạo ra các hướng dẫn mạch lạc (Honovich et al., 2022), việc chỉ tạo ra một hướng dẫn duy nhất

không thể đảm bảo hiệu suất đáng tin cậy cho các ví dụ chưa thấy trong nhiệm vụ đã cho. Như một giải pháp đơn giản, các tập xác thực đã được sử dụng để đánh giá hiệu quả của một tập hợp các hướng dẫn được lấy mẫu (Zhou et al., 2022; Singh et al., 2022), nhưng điều này không thực tế đối với nhiều nhiệm vụ được định nghĩa trong bối cảnh few-shot thực sự (Suzgun et al., 2022). Bên cạnh đó, những cách tiếp cận này chủ yếu được thử nghiệm trên các nhiệm vụ đơn giản nơi các hướng dẫn cơ bản đã đủ, chẳng hạn như các phép toán số học hoặc phân loại cảm xúc. Các nhiệm vụ phức tạp hơn trong các benchmark NLP (Wang et al., 2022), đòi hỏi kỹ thuật hướng dẫn cẩn thận, vẫn phần lớn chưa được kiểm tra cho một giải pháp tự động.

Để giải quyết những thách thức nói trên, chúng tôi đề xuất Auto-Instruct, một cách tiếp cận mới để tự động tạo ra và xếp hạng các hướng dẫn cho LLMs hộp đen trên các nhiệm vụ NLP khác nhau, trong bối cảnh few-shot thực sự. Đối với mỗi nhiệm vụ downstream, đầu tiên chúng tôi gợi ý LLM để lấy mẫu nhiều hướng dẫn ứng cử viên, dựa trên một hướng dẫn seed cơ bản và các minh chứng few-shot. Chúng tôi thu thập một tập ứng cử viên đa dạng bằng cách chỉ định phong cách mong đợi của mỗi hướng dẫn. Nhận thức được hiệu suất biến đổi của LLMs trên các hướng dẫn khác nhau, cùng với việc thiếu dữ liệu xác thực để lựa chọn hướng dẫn từ trước, chúng tôi huấn luyện một mô hình tính điểm để xếp hạng và chọn hướng dẫn phù hợp nhất cho mỗi ví dụ kiểm tra downstream. Để đảm bảo khả năng tổng quát hóa cần thiết trong bối cảnh few-shot, mô hình được huấn luyện trên 575 nhiệm vụ NLP hiện có trước khi được triển khai cho các nhiệm vụ kiểm tra ngoài miền. Cuối cùng, hướng dẫn được chọn được sử dụng để gợi ý LLM cho suy luận downstream.

Trong các thí nghiệm với text-davinci-003 của OpenAI, Auto-Instruct mang lại hiệu suất đáng kể trên 118 nhiệm vụ ngoài miền từ Super Natural Instructions (SuperNI; Wang et al., 2022) và Big Bench Hard (BBH; Suzgun et al., 2022). Thể hiện khả năng tổng quát hóa mạnh mẽ trong các kịch bản ngoài miền, Auto-Instruct vượt trội hơn các hướng dẫn seed do con người viết, cách tiếp cận tạo hướng dẫn tiên tiến nhất iPrompt (Singh et al., 2022), và các phương pháp cơ sở khác nhau của việc gợi ý LLM để lựa chọn hướng dẫn. Hơn nữa, Auto-Instruct thể hiện hiệu suất ấn tượng trong bối cảnh zero-shot và trong tổng quát hóa sang các LLMs khác (tức là, ChatGPT và GPT-4). Nghiên cứu của chúng tôi nhấn mạnh rằng việc tự động tạo ra và xếp hạng các hướng dẫn là một cách tiếp cận đầy hứa hẹn để tận dụng hiệu quả sức mạnh của các LLMs hộp đen.

## 2 Công trình liên quan
Việc lựa chọn hướng dẫn đóng vai trò then chốt trong việc sử dụng hiệu quả LLMs. Để đạt được điều này, một loạt các cách tiếp cận đã được triển khai, với tối ưu hóa tham số và sinh bằng LLM nổi bật như những phương pháp quan trọng. Tối ưu hóa tham số chủ yếu liên quan đến việc sử dụng các tham số để điều chỉnh hướng dẫn (Shin et al., 2020; Shi et al., 2022; Deng et al., 2022). Ví dụ, Shin et al. (2020) sử dụng tìm kiếm dựa trên gradient trên một độ dài xác định trước của các token rời rạc làm hướng dẫn. Shi et al. (2022) cải thiện thêm cách tiếp cận này bằng cách bảo tồn tính dễ đọc của các token được lấy mẫu thông qua một ràng buộc độ phức tạp. Như một cách tiếp cận linh hoạt hơn, Deng et al. (2022) tối ưu hóa việc tạo hướng dẫn thông qua học tăng cường, với phần thưởng được tính dựa trên đầu ra LLM. Tuy nhiên, những chiến lược này yêu cầu truy cập vào các tham số LLM hoặc một tập huấn luyện để tối ưu hóa, khiến chúng ít áp dụng được hơn cho các LLMs hộp đen chỉ với một số lượng hạn chế các ví dụ có sẵn. Hơn nữa, các hướng dẫn được tạo ra bởi những phương pháp này thường thiếu tính trôi chảy hoặc thậm chí trở thành vô nghĩa, do đó ảnh hưởng đến khả năng diễn giải của chúng.

Ngược lại, luồng sinh bằng LLM lựa chọn hướng dẫn bằng cách gợi ý trực tiếp LLM (Honovich et al., 2022; Zhou et al., 2022; Singh et al., 2022). Ví dụ, Honovich et al. (2022) là một trong những người đầu tiên tiết lộ rằng LLMs có thể viết một hướng dẫn cho một nhiệm vụ nhất định sau khi quan sát chỉ một vài minh chứng, và Zhou et al. (2022) cải thiện chất lượng của các hướng dẫn được tạo ra bằng cách chọn hướng dẫn có hiệu suất tốt nhất trên dữ liệu xác thực. iPrompt (Singh et al., 2022) là phương pháp có khả năng nhất cho đến nay với quá trình tạo và xác thực lặp đi lặp lại để lựa chọn hướng dẫn. Tuy nhiên, những cách tiếp cận này vẫn cần thiết một tập xác thực để xếp hạng hướng dẫn, và các hướng dẫn chúng tạo ra thường có hiệu suất kém hơn so với những hướng dẫn do con người viết.

Bên cạnh việc lựa chọn hướng dẫn, các nhà nghiên cứu cũng đã khám phá các hướng trực giao khác của việc cải thiện gợi ý LLM, chẳng hạn như việc lựa chọn các minh chứng trong ngữ cảnh. Một số công trình tập trung vào việc xác định các minh chứng phù hợp nhất từ các ví dụ huấn luyện (Rubin et al., 2022; Lu et al., 2022a; Wang et al., 2023a) và thứ tự tối ưu của chúng (Lu et al., 2022b) trong gợi ý few-shot. Các nghiên cứu khác kiểm tra việc kỹ thuật và lựa chọn các chuỗi lý luận được ghép nối với các minh chứng few-shot trên các nhiệm vụ lý luận nhiều bước (Wei et al., 2022; Zhang et al., 2022b; Ye and Durrett, 2023; Liang et al., 2023b). Chúng tôi dành việc khám phá tích hợp những kỹ thuật trực giao này với cách tiếp cận của chúng tôi để tối ưu hóa toàn diện toàn bộ gợi ý LLM cho công việc tương lai.

## 3 Công thức hóa vấn đề
Trong công việc này, chúng tôi tập trung vào bối cảnh few-shot thực sự nơi người dùng nhằm mục đích giải quyết một nhiệm vụ mới với một LLM hộp đen. Trong khi dễ dàng để đưa ra một số ít ví dụ và một mô tả cơ bản, người dùng có thể không có hiểu biết sâu sắc về loại hướng dẫn nào sẽ hiệu quả cho các ví dụ chưa thấy. Do đó, với các ví dụ few-shot làm minh chứng và mô tả cơ bản làm hướng dẫn seed, mục tiêu của chúng tôi là tự động hóa quá trình tạo ra một hướng dẫn hiệu quả hơn cho nhiệm vụ đã cho.

Chúng tôi công thức hóa vấn đề theo các thực hành thông thường của học trong ngữ cảnh (Dong et al., 2023). Trong bối cảnh few-shot nói trên, gợi ý để truy vấn một LLM hộp đen bao gồm một hướng dẫn I, đầu vào kiểm tra x, và một vài cặp đầu vào-đầu ra làm minh chứng {xᵈᵢ, yᵈᵢ}ⁿᵢ₌₁. LLM được mong đợi tạo ra một đầu ra y∼P(·|I,{xᵈᵢ, yᵈᵢ}ⁿᵢ₌₁, x). Công việc này nhằm mục đích tự động tìm ra một hướng dẫn vượt trội I' dựa trên hướng dẫn seed do con người viết Iₛ, do đó tránh được nhu cầu kỹ thuật thủ công đáng kể. Bên cạnh đó, chúng tôi cũng khám phá bối cảnh zero-shot nơi không có minh chứng nào được cung cấp cho LLM.

Mặc dù hướng dẫn có thể có nhiều cách tích hợp với các minh chứng và đầu vào kiểm tra, để giảm độ phức tạp của vấn đề, chúng tôi định dạng toàn bộ gợi ý theo thứ tự (I, xᵈ₁, yᵈ₁,···, xᵈₙ, yᵈₙ, x). Điều này phù hợp với quy ước giải quyết vấn đề nơi nhiệm vụ được phác thảo trước, tiếp theo là việc cung cấp các ví dụ dữ liệu, và đầu vào kiểm tra cuối cùng được cung cấp. Trong thực tế, chúng tôi sử dụng n = 3 cho tất cả các nhiệm vụ.

## 4 Auto-Instruct
Auto-Instruct bao gồm hai bước: tạo hướng dẫn và xếp hạng hướng dẫn. Đầu tiên chúng tôi gợi ý LLM hộp đen để tạo ra một tập hợp đa dạng các hướng dẫn ứng cử viên (§4.1) cho mỗi nhiệm vụ downstream. Tiếp theo, chúng tôi huấn luyện một mô hình tính điểm để xếp hạng tất cả các hướng dẫn ứng cử viên cho mỗi ví dụ kiểm tra nhất định, vì các ví dụ khác nhau có thể hưởng lợi từ các hướng dẫn khác nhau (§4.2). Sau đó, hướng dẫn được xếp hạng cao nhất được chọn để gợi ý LLM hộp đen trên ví dụ kiểm tra cụ thể đó cho suy luận downstream.

[Hình 2: Meta-prompt hướng dẫn LLM tạo ra hướng dẫn từng bước cho nhiệm vụ đã cho. Các meta-prompt khác được thể hiện trong Phụ lục E.]

### 4.1 Tạo hướng dẫn
Như đã đề cập trong §3, chúng tôi tận dụng một mô tả nhiệm vụ cơ bản do con người viết làm hướng dẫn seed Iₛ và gợi ý LLM hộp đen để tạo ra một số hướng dẫn ứng cử viên {Iᶜⱼ}ᵐⱼ₌₁. Cụ thể, trong bối cảnh few-shot, chúng tôi gợi ý LLM để tạo ra các hướng dẫn ứng cử viên Iᶜ∼P(·|Iₛ,{xᵈᵢ, yᵈᵢ}ⁿᵢ₌₁) dựa trên hướng dẫn seed và các minh chứng few-shot. Các cách tiếp cận trước đây (Zhou et al., 2022; Singh et al., 2022) chỉ sử dụng một meta-prompt³ duy nhất và thu thập các hướng dẫn ứng cử viên thông qua lấy mẫu token. Thông thường, những hướng dẫn được lấy mẫu như vậy chỉ thể hiện những biến đổi nhỏ trong cách diễn đạt thay vì sự đa dạng nội dung đáng kể. Hơn nữa, chất lượng của chúng dựa vào việc lựa chọn tùy ý của meta-prompt, điều này chuyển tính không đáng tin cậy của kỹ thuật hướng dẫn thủ công sang kỹ thuật meta-prompt thủ công.

Trong cách tiếp cận cải tiến của chúng tôi, chúng tôi tuyển chọn một tập hợp các meta-prompt để kích thích LLM lấy mẫu các hướng dẫn ứng cử viên đa dạng bằng cách định nghĩa các phong cách yêu cầu khác nhau của hướng dẫn. Những meta-prompt này bao gồm:

1. Viết một hướng dẫn về cách giải quyết nhiệm vụ sau trong một câu.
2. Viết một hướng dẫn về cách giải quyết nhiệm vụ sau trong một đoạn văn.
3. Viết một hướng dẫn từng bước về cách giải quyết nhiệm vụ sau.
4. Viết một hướng dẫn về cách giải quyết nhiệm vụ sau. Hướng dẫn phải bao gồm các giải thích của các ví dụ đã cho.

Cùng với 4 meta-prompt này, chúng tôi cũng đưa vào các hướng dẫn do con người viết từ các nhiệm vụ NLP hiện có làm minh chứng để hướng dẫn việc tạo ra

³Gợi ý cho LLM để tạo ra hướng dẫn.

[Hình 3: Biểu đồ hộp thể hiện mức độ biến đổi hiệu suất LLM với các hướng dẫn khác nhau, được thử nghiệm trên text-davinci-003 của OpenAI. Hiệu suất được đánh giá bằng ROUGE-L trên SuperNI và Độ chính xác trên BBH. Mỗi giá trị đại diện cho độ lệch chuẩn của hiệu suất LLM trên tất cả các hướng dẫn được tạo ra trên một nhiệm vụ duy nhất.]

hướng dẫn. Một cách trực quan, chúng tôi gợi ý LLM bắt chước phong cách của các hướng dẫn do con người viết trong những nhiệm vụ minh chứng này. Chúng tôi lấy nguồn các nhiệm vụ minh chứng với hướng dẫn của chúng từ các nhiệm vụ huấn luyện của chúng tôi trong SuperNI, nhóm chúng thành 3 cụm dựa trên độ dài của hướng dẫn của chúng, để hướng dẫn LLM tạo ra các hướng dẫn có độ chi tiết khác nhau. Hình 2 cung cấp một ví dụ về meta-prompt #3. Các meta-prompt khác được chi tiết trong Phụ lục E.

Dựa trên 7 meta-prompt khác biệt này (tức là, 4 meta-prompt cụ thể theo phong cách + 3 nhóm nhiệm vụ minh chứng), chúng tôi lấy mẫu 3 hướng dẫn dưới mỗi meta-prompt thông qua lấy mẫu nucleus (Holtzman et al., 2020). Bao gồm hướng dẫn seed gốc, chúng tôi thu thập tổng cộng 22 hướng dẫn ứng cử viên cho mỗi nhiệm vụ. Kết quả là, chúng tôi tạo ra một tập hợp các hướng dẫn ứng cử viên đa dạng và toàn diện, do đó giảm tính ngẫu nhiên mang lại bởi các sắc thái của các meta-prompt khác nhau.

Trong bối cảnh zero-shot, do không có minh chứng, LLM được gợi ý để tạo ra hướng dẫn ứng cử viên Iᶜ∼P(·|Iₛ) chỉ dựa trên hướng dẫn seed. Bên cạnh đó, meta-prompt giải thích ví dụ được loại bỏ. Như chúng tôi chứng minh trong §5.4.5, ngay cả khi không có sự hỗ trợ của các minh chứng, các meta-prompt cụ thể theo phong cách của chúng tôi vẫn cho phép LLM tạo ra các hướng dẫn có thông tin.

#### 4.1.1 Tính không ổn định dưới các hướng dẫn khác nhau
Trong khi LLMs có khả năng tạo ra các hướng dẫn có ý nghĩa, việc dựa vào một hướng dẫn được tạo ra duy nhất có thể sẽ dẫn đến hiệu suất chưa tối ưu do độ nhạy cảm của LLM đối với cách diễn đạt của các hướng dẫn. Tính không ổn định này đặc biệt rõ ràng trong bối cảnh zero-shot do thiếu các minh chứng để hỗ trợ dự đoán. Trong Hình 3, chúng tôi tính toán độ lệch chuẩn của hiệu suất LLM sử dụng các hướng dẫn khác nhau, sau khi đã đánh giá tất cả các hướng dẫn cho mỗi nhiệm vụ downstream. Điều này cho thấy sự biến động hiệu suất mong đợi khi thay thế một hướng dẫn bằng một hướng dẫn khác. Độ lệch chuẩn trung vị trên tất cả các nhiệm vụ là 3.1 và 4.2 điểm trong ROUGE-L cho bối cảnh few-shot và zero-shot tương ứng trên SuperNI, và các phần tư thứ ba là 5.7 và 6.9 điểm tương ứng. Việc lựa chọn hướng dẫn thậm chí gây ra sự biến động hiệu suất hai chữ số trên nhiều nhiệm vụ. Do đó, việc phát triển một phương pháp để xếp hạng và chọn hướng dẫn trở thành một công việc thiết yếu.

### 4.2 Xếp hạng hướng dẫn
Trong bối cảnh few-shot thực sự, các minh chứng không đủ để phản ánh hiệu quả của các hướng dẫn ứng cử viên do kích thước mẫu nhỏ. Để vượt qua hạn chế này, chúng tôi huấn luyện một mô hình xếp hạng hướng dẫn có thể tổng quát hóa trên nhiều nhiệm vụ NLP, và sau đó áp dụng nó cho mỗi ví dụ kiểm tra trong các nhiệm vụ ngoài miền. Một cách trực quan, mô hình này được huấn luyện để xếp hạng các hướng dẫn theo hiệu suất downstream của chúng trên LLM, tức là, gán điểm số cao hơn cho các hướng dẫn hiệu quả hơn.

#### 4.2.1 Mô hình
Nhờ vào khả năng tổng quát hóa đã được chứng minh của họ mô hình T5 (Raffel et al., 2020; Sanh et al., 2022), chúng tôi bắt đầu từ mô hình FLAN-T5-Large được điều chỉnh theo hướng dẫn (Chung et al., 2022) và huấn luyện nó với mục tiêu xếp hạng hướng dẫn của chúng tôi. Cho một ví dụ cụ thể (x, y) trong đó x là đầu vào và y là đầu ra chính xác, cũng như một hướng dẫn ứng cử viên tùy ý Iᶜ, mô hình dự đoán một điểm số Q_T5(Iᶜ, x) như một ước tính về hiệu quả của hướng dẫn trên ví dụ. Tận dụng bản chất tuân theo hướng dẫn của FLAN-T5, chúng tôi đưa ra gợi ý sau cho mô hình xếp hạng:

Ví dụ: x. Đầu vào: Iᶜ. Đây có phải là một hướng dẫn tốt để giải quyết ví dụ không?

Q_T5(Iᶜ, x) sau đó được tính như logit của token "yes" tại vị trí bắt đầu của bộ giải mã. Bổ sung, chúng tôi thu được hiệu suất downstream của Iᶜ bằng cách tính toán điểm số ROUGE-L giữa đầu ra được dự đoán của LLM ŷ (khi Iᶜ được sử dụng làm hướng dẫn) so với đầu ra chính xác y, được ký hiệu là r(y,ŷ). Mô hình sau đó được huấn luyện với một hàm mất mát theo danh sách để căn chỉnh các điểm số Q_T5(Iᶜ, x) của tất cả các hướng dẫn ứng cử viên với hiệu suất downstream tương ứng r(y,ŷ) của chúng, trong khi xem xét tính ưu việt tương đối giữa các hướng dẫn khác nhau. Cụ thể, đầu tiên chúng tôi chuẩn hóa cả danh sách các điểm số được dự đoán Q_T5(Iᶜ, x) và danh sách hiệu suất downstream r(y,ŷ) bằng cách áp dụng softmax trên tất cả các hướng dẫn ứng cử viên, sau đó tính toán KL-divergence giữa hai phân phối được chuẩn hóa này làm hàm mất mát huấn luyện:

L = 1/|B| ∑_(x,y)∈B KL(σ(r(y,ŷ)) || σ(Q_T5(Iᶜ, x))),

trong đó ŷ∼P_LLM(·|Iᶜ,{xᵈᵢ, yᵈᵢ}ⁿᵢ₌₁, x).

Lưu ý rằng B là một batch các ví dụ và σ là hàm softmax. Trong quá trình kiểm tra, cho một ví dụ kiểm tra cụ thể, trong số tất cả các hướng dẫn ứng cử viên, chúng tôi chọn Iᶜ đạt được điểm số cao nhất Q_T5(Iᶜ, x) làm hướng dẫn cuối cùng, và gợi ý LLM với nó để thu được đầu ra mong muốn.

#### 4.2.2 Dữ liệu huấn luyện
Để huấn luyện một mô hình xếp hạng như vậy với khả năng tổng quát hóa cho các nhiệm vụ ngoài miền, chúng tôi phân loại các nhiệm vụ trong benchmark SuperNI theo loại nhiệm vụ của chúng (ví dụ, QA, phân tích cảm xúc, v.v.) và nhóm những danh mục này thành các tập huấn luyện và kiểm tra. Chúng tôi loại trừ các nhiệm vụ liên quan đến các ngôn ngữ không phải tiếng Anh hoặc những nhiệm vụ có đầu vào quá dài. Để tránh rò rỉ dữ liệu, chúng tôi cũng loại trừ các nhiệm vụ từ dữ liệu huấn luyện được lấy nguồn từ cùng một tập dữ liệu với bất kỳ nhiệm vụ kiểm tra nào. Điều này mang lại 575 nhiệm vụ cho huấn luyện và 91 cho kiểm tra.

Chúng tôi lấy mẫu tối đa 400 ví dụ từ mỗi nhiệm vụ huấn luyện, dẫn đến tổng cộng 122k ví dụ. Các phương pháp tiền xử lý và lọc dữ liệu bổ sung được sử dụng để tăng tốc quá trình huấn luyện có thể được tìm thấy trong Phụ lục A.

## 5 Thí nghiệm

### 5.1 Cài đặt
Để đánh giá cách tiếp cận của chúng tôi trong bối cảnh few-shot thực sự, chúng tôi thử nghiệm nó trên nhiều nhiệm vụ ngoài miền — 91 từ SuperNI (Wang et al., 2022) và 27 từ BBH (Suzgun et al., 2022), nơi không có sự chồng lấp giữa các danh mục nhiệm vụ trong huấn luyện và kiểm tra. Tập kiểm tra SuperNI bao gồm cả các nhiệm vụ phân loại và sinh, ví dụ, phân loại thường thức, trích xuất thông tin, v.v.⁴ BBH trình bày một tập hợp đa dạng các nhiệm vụ trải rộng từ QA thường thức đến các vấn đề toán học. ROUGE-L⁵ trung bình và độ chính xác khớp chính xác được sử dụng để đánh giá trên SuperNI và BBH, tương ứng. Các thí nghiệm chính của chúng tôi được tiến hành sử dụng text-davinci-003 của OpenAI để tạo hướng dẫn và suy luận downstream. Chúng tôi cũng khám phá các hướng dẫn được tạo ra bởi ChatGPT (gpt-3.5-turbo) hoặc GPT-4 (OpenAI, 2023) trong §5.4.1.

Trong bối cảnh zero-shot, mô hình xếp hạng được huấn luyện riêng biệt trên dữ liệu nơi điểm số ROUGE downstream của các hướng dẫn ứng cử viên cũng được thu được dưới gợi ý zero-shot. Đối với các nhiệm vụ phân loại zero-shot, chúng tôi thêm các hướng dẫn định dạng bổ sung vào hướng dẫn seed để thu hẹp các tùy chọn trả lời trong cả việc tạo hướng dẫn và suy luận downstream. Các cài đặt thí nghiệm bổ sung có thể được tìm thấy trong Phụ lục B.

### 5.2 Phương pháp cơ sở
Như các phương pháp cơ sở trong thí nghiệm của chúng tôi, đầu tiên chúng tôi xem xét ba cách tiếp cận thay thế chỉ dựa trên việc gợi ý LLM:

(1) Cross-Validation. Chúng tôi tận dụng các minh chứng 3-shot làm dữ liệu xác thực để xếp hạng các hướng dẫn, với mỗi hướng dẫn hoạt động như ví dụ kiểm tra lặp đi lặp lại trong khi hai hướng dẫn khác phục vụ như minh chứng. Điểm số ROUGE-L (hoặc độ chính xác cho BBH) được sử dụng làm tiêu chí xếp hạng chính, và log-probability của đầu ra chính xác được so sánh như tiêu chí phụ. Hướng dẫn được chọn bởi các minh chứng sau đó được áp dụng trên tất cả các ví dụ kiểm tra trong cùng một nhiệm vụ.

(2) LM Selection. Chúng tôi gợi ý trực tiếp LLM để chọn một hướng dẫn bằng cách liệt kê tất cả các hướng dẫn ứng cử viên trong một gợi ý duy nhất. Chúng tôi đánh số các hướng dẫn và yêu cầu LLM tạo ra số của hướng dẫn mà nó cho là phù hợp nhất cho mỗi ví dụ kiểm tra.

(3) On-the-fly Generation. Như một biến thể đơn giản hóa không có xếp hạng hướng dẫn, mô hình được yêu cầu tạo ra trực tiếp một hướng dẫn cho mỗi ví dụ kiểm tra. Hướng dẫn được tạo ra sau đó được sử dụng để gợi ý LLM cho cùng ví dụ đó.

Hơn nữa, chúng tôi xem xét iPrompt (Singh et al., 2022), cách tiếp cận tiên tiến hiện có trong việc tối ưu hóa hướng dẫn với LLMs. iPrompt tạo ra các hướng dẫn một cách lặp đi lặp lại cho đến khi nó không thể tìm thấy một hướng dẫn nào có hiệu suất tốt hơn trên một tập xác thực. Để đánh giá

⁴Danh sách đầy đủ các nhiệm vụ kiểm tra SuperNI có trong Phụ lục G.
⁵Các tác giả gốc của SuperNI đã thấy ROUGE-L có tương quan tích cực với độ chính xác trên các nhiệm vụ phân loại, vì vậy ROUGE-L trung bình được áp dụng để đơn giản.

[Bảng 1: Kết quả trên SuperNI (91 nhiệm vụ) và BBH (27 nhiệm vụ) dưới bối cảnh few-shot và zero-shot tương ứng. Chúng tôi báo cáo ROUGE-L trên SuperNI và độ chính xác trên BBH. Các phương pháp với * áp dụng cùng một hướng dẫn cho một nhiệm vụ nhất định, trong khi các phương pháp với † có thể chọn các hướng dẫn khác nhau cho các ví dụ khác nhau. iPrompt tạo ra và xếp hạng các hướng dẫn ứng cử viên một cách lặp đi lặp lại, trong khi các phương pháp khác áp dụng pipeline tạo-sau-đó-xếp-hạng. Chúng tôi lưu ý rằng iPrompt, iPrompt+ và Cross-Validation không áp dụng được trong bối cảnh zero-shot do cần các ví dụ xác thực. Kết quả chi tiết trên SuperNI của các danh mục nhiệm vụ khác nhau có thể được tìm thấy tại Phụ lục D.1.]

iPrompt trong bối cảnh few-shot thực sự, chúng tôi tiến hành xác thực của nó trên các minh chứng 3-shot. Bên cạnh đó, vì iPrompt gốc tạo ra các hướng dẫn dựa trên các ví dụ mà không có bất kỳ mô tả nhiệm vụ nào, để so sánh công bằng, chúng tôi triển khai một phương pháp cơ sở iPrompt+ sử dụng meta-prompt tương tự như của chúng tôi với hướng dẫn seed (Xem Phụ lục C để biết chi tiết). Ngoài ra, chúng tôi đánh giá hiệu suất của việc không sử dụng bất kỳ hướng dẫn nào (Empty Instruction), sử dụng trực tiếp hướng dẫn seed do con người viết (Human Instruction) hoặc chọn ngẫu nhiên một hướng dẫn từ các ứng cử viên được tạo ra (Random Selection) trên mỗi nhiệm vụ.

### 5.3 Kết quả
Kết quả tổng thể của SuperNI và BBH được thể hiện trong Bảng 1, nơi điểm số được tính trung bình trên tất cả các nhiệm vụ. Auto-Instruct thể hiện tính nhất quán và khả năng tổng quát hóa đáng chú ý trong các kịch bản ngoài miền, vượt trội hơn tất cả các phương pháp cơ sở trên các benchmark và cài đặt khác nhau. Các phát hiện chính được nêu dưới đây.

LLM thể hiện khả năng cạnh tranh trong việc tạo ra các hướng dẫn hiệu quả, tuy nhiên việc xếp hạng vẫn cần thiết. Phù hợp với công việc trước đây (Zhou et al., 2022; Singh et al., 2022), LLM có thể tạo ra các hướng dẫn hiệu quả cho các nhiệm vụ khác nhau. Các meta-prompt cụ thể theo phong cách của chúng tôi cho phép LLM tạo ra một tập hợp đa dạng các hướng dẫn để phục vụ cho các kịch bản khác nhau nơi các nhiệm vụ khác nhau có thể ưa thích các phong cách hướng dẫn khác nhau. Trong bối cảnh few-shot, các hướng dẫn do LLM tạo ra đã vượt trội hơn các hướng dẫn do con người viết trung bình, như được chỉ ra bởi điểm số lựa chọn ngẫu nhiên. Mặc dù con người có thể có kiến thức trước về một số ví dụ khi họ viết hướng dẫn, LLM, không được cung cấp bất kỳ minh chứng nào trong bối cảnh zero-shot, tạo ra các hướng dẫn có chất lượng tương đương với những hướng dẫn do con người viết. Tuy nhiên, cả việc lựa chọn ngẫu nhiên và việc tạo ra trực tiếp một hướng dẫn duy nhất (tức là, tạo ra tức thì) đều không cải thiện đáng kể so với phương pháp cơ sở do con người viết. Điều này phù hợp với tính không ổn định của hiệu suất LLM trên các hướng dẫn khác nhau như đã thảo luận trong Hình 3, điều này cho thấy việc xếp hạng hướng dẫn thêm vẫn cần thiết.

Việc đơn giản gợi ý LLM hoặc sử dụng dữ liệu xác thực không đáng tin cậy trong bối cảnh ít tài nguyên. Mặc dù mang lại sự tiện lợi của việc không cần huấn luyện bất kỳ mô hình nào, cả việc gợi ý trực tiếp LLM (LM selection) và sử dụng các minh chứng few-shot để xác thực (iPrompt và cross-validation) đều thất bại trong việc mang lại kết quả cải thiện nhất quán so với lựa chọn ngẫu nhiên. Điều này nhấn mạnh rằng (1) bản thân LLM thiếu manh mối về hiệu suất downstream mong đợi của các hướng dẫn khác nhau; (2) khối lượng dữ liệu xác thực phải đủ lớn để ước tính hiệu quả hiệu suất của các hướng dẫn trên dữ liệu kiểm tra, điều này mang lại chi phí cao trong nhiều kịch bản thực tế.

Mô hình xếp hạng hướng dẫn được huấn luyện của chúng tôi là cách tiếp cận hiệu quả nhất để chọn hướng dẫn cho đến nay. Mặc dù dữ liệu và hướng dẫn cho các nhiệm vụ ngoài miền không được mô hình xếp hạng nhìn thấy, nó thể hiện khả năng tổng quát hóa đầy hứa hẹn trong việc chọn các hướng dẫn hiệu quả nhờ vào việc huấn luyện trên hàng trăm nhiệm vụ khác nhau. Ví dụ, trên benchmark SuperNI, nó vượt trội hơn lựa chọn ngẫu nhiên 4% và 8% trong bối cảnh few-shot và zero-shot tương ứng. Bên cạnh đó, pipeline hoàn chỉnh của chúng tôi mang lại cải thiện tương đối 6% so với các hướng dẫn gốc do con người viết trong cả bối cảnh few-shot và zero-shot, cho thấy rằng các hướng dẫn do con người viết vẫn cần cải thiện trong nhiều ngữ cảnh.

### 5.4 Phân tích
Trong phần này, chúng tôi đi sâu hơn vào hiệu suất của cách tiếp cận của chúng tôi bằng cách phân tích việc sử dụng các LLMs khác để tạo hướng dẫn, hiệu suất trên các nhiệm vụ đã thấy, kích thước của dữ liệu huấn luyện, và các nghiên cứu trường hợp. Phân tích bổ sung về so sánh giữa Auto-Instruct và ensemble nhiều đáp án có trong Phụ lục D. Những phân tích này được tiến hành trong bối cảnh few-shot trừ khi được nêu khác.

#### 5.4.1 Tổng quát hóa sang các LLMs khác
Để kiểm tra thêm khả năng tổng quát hóa của cách tiếp cận của chúng tôi, chúng tôi chuyển Auto-Instruct sang các LLMs khác bằng cách sử dụng ChatGPT (gpt-3.5-turbo) và GPT-4 làm mô hình suy luận downstream. Như Bảng 2 gợi ý, các hướng dẫn được chọn bởi Auto-Instruct trên text-davinci-003 vẫn hiệu quả nếu được chuyển sang ChatGPT và

[Bảng 2: Kết quả SuperNI của việc chuyển Auto-Instruct sang ChatGPT và GPT-4, sử dụng hoặc (1) hướng dẫn được tạo ra bởi text-davinci-003, hoặc (2) hướng dẫn được tạo ra bởi cùng mô hình với suy luận downstream (tức là, ChatGPT hoặc GPT-4). Mô hình xếp hạng hướng dẫn vẫn là mô hình được huấn luyện trên hướng dẫn text-davinci-003.]

GPT-4. Hơn nữa, mô hình xếp hạng hướng dẫn của chúng tôi có thể xếp hạng các hướng dẫn được tạo ra bởi ChatGPT hoặc GPT-4 trong cả kịch bản few-shot và zero-shot, mặc dù không nhìn thấy bất kỳ hướng dẫn nào được tạo ra bởi những LLMs này trong quá trình huấn luyện. Kết quả cải thiện cũng có thể được thấy khi chuyển Auto-Instruct sang LLaMA-2-chat (Touvron et al., 2023), một LLM mã nguồn mở gần đây, như được thể hiện trong Phụ lục D.2. Như một kết luận, mặc dù có sự khác biệt trong cách diễn đạt trên các hướng dẫn được tạo ra bởi các LLMs khác nhau, mẫu cơ bản xác định hiệu quả hướng dẫn có thể chuyển đổi được, mặc dù cải thiện lớn nhất vẫn được thấy trong các thí nghiệm cùng-LLM. Đủ để nói, mô hình xếp hạng hướng dẫn được huấn luyện của chúng tôi có thể được áp dụng trực tiếp để chọn hướng dẫn cho các LLMs khác mà không cần huấn luyện lại.

#### 5.4.2 Đánh giá xếp hạng hướng dẫn
Để điều tra hiệu quả của mô hình xếp hạng hướng dẫn, chúng tôi so sánh nó với các phương pháp cơ sở lựa chọn hướng dẫn khác bằng cách gán nhãn bạc cho các hướng dẫn ứng cử viên, với kết quả được chi tiết trong Bảng 3. Đầu tiên, chúng tôi sử dụng hiệu suất downstream thực tế của các hướng dẫn ứng cử viên làm nhãn bạc. Mô hình xếp hạng của chúng tôi có khả năng phân biệt các hướng dẫn tốt hơn, như được thể hiện bởi độ chính xác rõ ràng cao hơn trong việc chọn các hướng dẫn top-1 hoặc top-5 trong số tất cả 22 ứng cử viên. Thứ hai, chúng tôi đánh giá tần suất hướng dẫn được chọn cải thiện hiệu suất downstream so với hướng dẫn trống hoặc hướng dẫn do con người viết. Một lần nữa, các hướng dẫn từ mô hình xếp hạng của chúng tôi tạo ra những cải thiện đáng kể nhất, vượt trội hơn các hướng dẫn do con người viết trong 7% trường hợp nhiều hơn so với lựa chọn ngẫu nhiên. Việc cải thiện hiệu suất nhất quán trên tất cả các đánh giá nhãn bạc còn khẳng định thêm tính ưu việt của mô hình chúng tôi so với các phương pháp xếp hạng thay thế dựa trên cross-validation hoặc LM selection.

[Bảng 3: Đánh giá xếp hạng hướng dẫn trên nhãn bạc. Trái: chúng tôi đánh giá tỷ lệ phần trăm các trường hợp nơi hướng dẫn được chọn là tốt nhất (top-1) hoặc nằm trong top-5 ứng cử viên, theo hiệu suất downstream thực tế. Chúng tôi lưu ý rằng có thể có nhiều hướng dẫn chia sẻ điểm số tốt nhất. Phải: chúng tôi kiểm tra tỷ lệ phần trăm các hướng dẫn được chọn vượt trội hơn hướng dẫn trống hoặc hướng dẫn do con người viết.]

#### 5.4.3 Auto-Instruct trên các nhiệm vụ đã thấy
Bên cạnh bối cảnh ngoài miền, chúng tôi khám phá một bối cảnh trong miền nơi chúng tôi chọn các ví dụ bổ sung từ các nhiệm vụ được thấy trong quá trình huấn luyện, để kiểm tra thêm năng lực của mô hình xếp hạng hướng dẫn. Để so sánh công bằng khả năng xếp hạng của mô hình trên các nhiệm vụ khác nhau, chúng tôi thí nghiệm với các ví dụ nhạy cảm với hướng dẫn, được định nghĩa là các ví dụ nơi không phải tất cả các hướng dẫn ứng cử viên đều mang lại cùng điểm số ROUGE. Chúng tôi lấy mẫu 100 ví dụ bổ sung từ mỗi trong 100 nhiệm vụ được thấy trong huấn luyện nhưng không được bao gồm trong tập dev. Như được trình bày trong Bảng 4, mô hình thể hiện khả năng xếp hạng tăng cường trên các nhiệm vụ đã thấy do tiếp xúc trước với các hướng dẫn trong quá trình huấn luyện. Điều này cho thấy rằng cách tiếp cận của chúng tôi hữu ích trong cả hoàn cảnh giàu dữ liệu và nghèo dữ liệu.

[Bảng 4: Kết quả trên dữ liệu kiểm tra nhạy cảm với hướng dẫn của cả nhiệm vụ đã thấy (100 nhiệm vụ thấy trong huấn luyện) và nhiệm vụ chưa thấy (giống như Bảng 1) từ SuperNI. Chúng tôi báo cáo thêm tỷ lệ cải thiện tương đối so với phương pháp cơ sở lựa chọn ngẫu nhiên vì hiệu suất vanilla không ở cùng thang đo.]

#### 5.4.4 Hiệu quả của việc có thêm nhiệm vụ huấn luyện
Để phân tích hiệu quả của việc huấn luyện đa nhiệm vụ quy mô lớn đối với khả năng tổng quát hóa ngoài miền, chúng tôi thao tác số lượng nhiệm vụ huấn luyện của mô hình xếp hạng hướng dẫn. Cụ thể, chúng tôi loại trừ các nhiệm vụ từ tập huấn luyện theo danh mục của chúng, tức là, tất cả các nhiệm vụ từ các danh mục được chọn bị loại bỏ. Như được thể hiện trong Hình 4, việc tăng số lượng nhiệm vụ huấn luyện từ các danh mục bổ sung là một yếu tố đóng góp chính cho hiệu suất vượt trội của mô hình chúng tôi so với phương pháp cơ sở lựa chọn ngẫu nhiên. Vì hiệu suất chưa đạt đến điểm cao nguyên khi tất cả các nhiệm vụ được bao gồm, có thể mong đợi những cải thiện hiệu suất thêm nếu có nhiều nhiệm vụ huấn luyện hơn.

[Hình 4: Kết quả của việc sử dụng số lượng nhiệm vụ huấn luyện khác nhau. 0% có nghĩa là sử dụng trực tiếp checkpoint FLAN-T5 đã được huấn luyện trước trong xếp hạng hướng dẫn, thể hiện hiệu suất tương tự như lựa chọn hướng dẫn ngẫu nhiên.]

#### 5.4.5 Phân tích các hướng dẫn được chọn
Hình 6 minh họa cách các hướng dẫn được chọn của chúng tôi cải thiện các hướng dẫn gốc do con người viết. Như được chỉ ra bởi điểm số độ tương tự trung bình, Auto-Instruct có thể cung cấp các hướng dẫn tương tự hơn với các hướng dẫn tối ưu trong số các ứng cử viên. Như được chứng minh bởi biểu đồ phân tán, trong các kịch bản nơi hướng dẫn được chọn vượt trội hơn hướng dẫn con người, embedding của nó thường lệch đáng kể từ embedding của hướng dẫn con người nhưng vẫn gần với hướng dẫn tối ưu. Những kết quả này gợi ý rằng hướng dẫn được chọn tinh chỉnh hướng dẫn seed do con người viết bằng cách tiến bộ hướng tới giải pháp lý tưởng, trong khi khoảng cách embedding giữa các hướng dẫn được chọn và seed làm cho sự cải thiện như vậy khó đạt được bằng kỹ thuật thủ công thuần túy.

Ngoài ra, chúng tôi cung cấp một nghiên cứu trường hợp trong Hình 5 trong bối cảnh zero-shot nơi LLM không thể tham khảo bất kỳ minh chứng nào. Tuy nhiên, LLM vẫn tạo ra các ví dụ bổ sung sử dụng kiến thức thu được từ việc tiền huấn luyện rộng rãi của nó. Những ví dụ bổ sung này có thể hoạt động như minh chứng để tạo ra một bối cảnh suy luận "2-shot", dẫn đến một dự đoán chính xác không thể đạt được thông qua suy luận zero-shot gốc. Ngược lại, chúng tôi cũng trình bày một ví dụ nơi hướng dẫn do LLM tạo ra bao gồm các mô tả ảo tưởng làm biến dạng ý nghĩa gốc của hướng dẫn seed. Sự không khớp giữa hướng dẫn này và ví dụ kiểm tra dẫn đến việc nó bị từ chối bởi mô hình xếp hạng. Độc giả có thể tìm thấy thêm các nghiên cứu trường hợp trong Phụ lục F.

[Hình 5: Trong trường hợp này, Auto-Instruct chọn một hướng dẫn "chuyển đổi" suy luận zero-shot thành suy luận "2-shot" bằng cách cung cấp các ví dụ bổ sung (tô đỏ), trong khi loại bỏ một hướng dẫn bao gồm ảo tưởng trong mô tả nhiệm vụ (tô xanh). Hướng dẫn con người cũng được bao gồm trong các ứng cử viên xếp hạng.]

[Hình 6: Trên: Embedding hướng dẫn của 5 nhiệm vụ SuperNI nơi hướng dẫn được chọn của Auto-Instruct hoạt động tốt hơn hướng dẫn con người, được trực quan hóa bằng T-SNE. "Best" đề cập đến hướng dẫn có điểm số ROUGE cao nhất. Dưới: Độ tương tự cosine trung bình giữa embedding hướng dẫn trên tất cả các nhiệm vụ SuperNI. Hai mô hình embedding là text-embedding-ada-002 từ OpenAI và all-mpnet-base-v2 từ Sentence-Transformers.]

## 6 Kết luận
Trong công việc này, chúng tôi giới thiệu Auto-Instruct, một cách tiếp cận tự động để tạo ra, xếp hạng và chọn hướng dẫn, cung cấp một giải pháp cho chi phí cao và tính chủ quan liên quan đến các hướng dẫn do con người kỹ thuật. Cách tiếp cận của chúng tôi bắt đầu bằng việc gợi ý LLM để tạo ra một tập hợp đa dạng các hướng dẫn ứng cử viên. Tiếp theo, một mô hình xếp hạng hướng dẫn được huấn luyện trên hàng trăm nhiệm vụ được sử dụng để xếp hạng các hướng dẫn ứng cử viên và chọn hướng dẫn hiệu quả nhất để giải quyết một ví dụ cụ thể. Kết quả thí nghiệm chứng minh rằng cách tiếp cận của chúng tôi cung cấp các hướng dẫn tốt hơn cả hướng dẫn do con người viết và những hướng dẫn được tạo ra bởi các cách tiếp cận tạo hướng dẫn trước đây, như được thử nghiệm trên 118 nhiệm vụ ngoài miền.

## Hạn chế
Theo hiểu biết của chúng tôi, công việc này có các hạn chế sau:

• Do chi phí đáng kể liên quan đến các mô hình OpenAI, và khả năng hạn chế của giao diện API của chúng, chúng tôi chỉ ghi điểm các hướng dẫn ứng cử viên trên một số lượng vừa phải các nhiệm vụ như được mô tả trong §4.2.2. Cho các kết quả trong Hình 4, chúng tôi mong đợi rằng mô hình có thể thể hiện khả năng tổng quát hóa cải thiện nếu có nhiều dữ liệu huấn luyện với các hướng dẫn được gắn nhãn hơn.

• Phạm vi của nghiên cứu này bị giới hạn trong việc tạo ra các hướng dẫn bằng tiếng Anh; các nhiệm vụ bằng ngôn ngữ không phải tiếng Anh không phải là một phần của dữ liệu huấn luyện của chúng tôi. Kết quả là, mô hình có thể không hoạt động thỏa đáng cho các nhiệm vụ không phải tiếng Anh. Điều tra thêm về việc tạo ra các hướng dẫn đa ngôn ngữ được để lại cho công việc tương lai.

• Mặc dù sử dụng một loạt các meta-prompt, điều này giảm đáng kể sự phụ thuộc vào kỹ thuật prompt, cách diễn đạt của những meta-prompt này vẫn có thể ảnh hưởng đến chất lượng của các hướng dẫn được tạo ra. Chúng tôi để lại việc khám phá tự động đa dạng hóa các hướng dẫn được tạo ra như công việc tương lai.

## Lời cảm ơn
Công việc này được hỗ trợ bởi NSF IIS-2119531, IIS-2137396, IIS-2142827, IIS-2234058, CCF-1901059, và ONR N00014-22-1-2507. Chúng tôi cảm ơn Canwen Xu (University of California San Diego) vì những gợi ý có giá trị của anh ấy trong quá trình viết bài báo.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ được giữ nguyên như trong bản gốc]

## Phụ lục A Tiền xử lý dữ liệu huấn luyện
Như được chi tiết trong §4.2, mô hình xếp hạng hướng dẫn được huấn luyện để xếp hạng các hướng dẫn ứng cử viên theo hiệu suất downstream của chúng trên LLM. Hiệu suất downstream của một hướng dẫn Iᶜ đề cập đến mức độ đầu ra được dự đoán của LLM ŷ khớp với đầu ra chính xác y khi sử dụng Iᶜ để gợi ý LLM, được định lượng bởi điểm số ROUGE-L r(y,ŷ).

Để tính toán điểm số này, chúng tôi ghép nối mỗi ví dụ huấn luyện với tất cả 22 hướng dẫn ứng cử viên của nhiệm vụ tương ứng (được tạo ra bằng phương pháp trong §4.1), và thu thập đầu ra được dự đoán của LLM cho ví dụ được gợi ý bởi mỗi hướng dẫn ứng cử viên. Sau khi tính toán điểm số ROUGE-L so với chính xác, chúng tôi loại bỏ các ví dụ nơi các hướng dẫn ứng cử viên không thể xếp hạng một cách khác biệt – trong các trường hợp nơi phạm vi hiệu suất downstream trên các hướng dẫn khác nhau ít hơn 10 điểm trong ROUGE-L.

Để tăng tốc quá trình huấn luyện, chúng tôi lấy mẫu 8 hướng dẫn ứng cử viên từ tổng pool của 22 cho mỗi ví dụ, và huấn luyện mô hình để xếp hạng 8 hướng dẫn này. Tuy nhiên, trong một số nhiệm vụ, một số hướng dẫn có thể vượt trội đáng kể so với các hướng dẫn khác. Lấy mẫu đồng đều 8 hướng dẫn ứng cử viên có thể dẫn đến những hướng dẫn "đặc biệt" như vậy được ưa chuộng quá nhiều lần trong việc huấn luyện mô hình xếp hạng. Để giải quyết điều này, chúng tôi tỷ lệ nghịch đảo tỷ lệ lấy mẫu của mỗi hướng dẫn với độ phổ biến của nó (tức là, số trường hợp nơi hướng dẫn này vượt trội hơn tất cả các hướng dẫn khác). Cuối cùng, chúng tôi lấy mẫu tối đa 400 ví dụ từ mỗi nhiệm vụ huấn luyện, dẫn đến tổng cộng 122k ví dụ huấn luyện.

## Phụ lục B Cài đặt thí nghiệm chi tiết
Mô hình xếp hạng hướng dẫn được khởi tạo với FLAN-T5-Large (780M tham số; Chung et al., 2022), và được huấn luyện sử dụng Adafactor (Shazeer and Stern, 2018) với tỷ lệ học 5e-5, kích thước batch 128 và tỷ lệ dropout 0.1. Chúng tôi sử dụng một tập dev trong miền bao gồm tổng cộng 5k ví dụ chưa thấy từ 100 nhiệm vụ huấn luyện để chọn checkpoint tốt nhất trong 5 epoch. Hiệu suất xác thực trên tập dev là 67.66 trong ROUGE-L, trong khi lựa chọn ngẫu nhiên chỉ đạt được điểm số 54.28. Khi sử dụng các mô hình OpenAI, để tạo hướng dẫn, chúng tôi đặt độ dài hướng dẫn tối đa là 300 token, và chúng tôi sử dụng nhiệt độ 1.0 và top_p 0.75 cho lấy mẫu token; để suy luận downstream, chúng tôi đặt cả hai thành 0 cho đầu ra tất định. Việc tạo ra tất cả các hướng dẫn ứng cử viên cho 91 nhiệm vụ kiểm tra SuperNI tốn của chúng tôi tổng cộng 18 USD, theo giá của OpenAI (0.02 USD cho 1k token cho text-davinci-003). Trong các thí nghiệm text-davinci-003, điểm số lựa chọn ngẫu nhiên được tính như điểm số trung bình trên tất cả các hướng dẫn trên mỗi ví dụ, bao gồm hướng dẫn seed do con người viết. Trong các hướng dẫn ChatGPT và GPT-4, do khả năng hạn chế của giao diện API của chúng, chúng tôi lấy mẫu ngẫu nhiên một hướng dẫn cho mỗi ví dụ và kiểm tra hiệu suất của nó.

## Phụ lục C Phương pháp cơ sở iPrompt
Trong phần này, chúng tôi phác thảo các điều chỉnh được thực hiện cho phương pháp iPrompt⁸ (Singh et al., 2022) cho cài đặt của chúng tôi. Chúng tôi chủ yếu giải quyết hai sự khác biệt giữa triển khai gốc của nó và thiết lập của chúng tôi: (1) iPrompt gốc tạo ra các hướng dẫn sử dụng GPT-J (Wang and Komatsuzaki, 2021), và (2) nó sử dụng một tập xác thực để ghi điểm và chọn hướng dẫn. Để giải quyết (1), chúng tôi sử dụng text-davinci-003 để tạo hướng dẫn, giống hệt với mô hình được sử dụng cho suy luận downstream. Đối với (2), chúng tôi tiến hành xác thực hướng dẫn của nó trên các minh chứng 3-shot. Do chi phí của việc yêu cầu lặp đi lặp lại API OpenAI, chúng tôi kết hợp một tiêu chí dừng sớm dừng quá trình nếu hiệu suất xác thực⁹ không cải thiện trong 10 lần lặp. Thực tế, hầu như tất cả các nhiệm vụ dừng trước 30 lần lặp. Sau đó, chúng tôi chọn hướng dẫn có hiệu suất xác thực tốt nhất để đánh giá trên các ví dụ kiểm tra.

Theo codebase gốc, chúng tôi sử dụng meta-prompt được thể hiện trong Hình 7 để tạo hướng dẫn với iPrompt. Vì meta-prompt này không sử dụng bất kỳ mô tả nhiệm vụ nào, để so sánh công bằng, chúng tôi triển khai một phương pháp cơ sở iPrompt+ với meta-prompt tương tự như phương pháp của chúng tôi sử dụng hướng dẫn seed, như được thể hiện trong Hình 8. Độc giả có thể tham khảo bài báo gốc (Singh et al., 2022) để biết chi tiết kỹ thuật của iPrompt.

[Hình 7: Meta-prompt tạo hướng dẫn với iPrompt.]
[Hình 8: Meta-prompt tạo hướng dẫn với iPrompt+, tương tự như của chúng tôi trong Hình 10.]

## Phụ lục D Kết quả thí nghiệm bổ sung
Trong phần này, chúng tôi trình bày thêm kết quả thí nghiệm ngoài những kết quả được phân tích trong Phần 5. Tất cả các thí nghiệm trong phần này được tiến hành trong bối cảnh few-shot trừ khi được nêu khác.

### D.1 Kết quả SuperNI theo danh mục nhiệm vụ
Ở đây, chúng tôi trình bày kết quả thí nghiệm chi tiết trên 8 danh mục khác nhau của các nhiệm vụ kiểm tra SuperNI (xem Phụ lục G để biết danh sách các nhiệm vụ kiểm tra). Như được thể hiện trong Hình 9, Auto-Instruct vượt trội hơn các hướng dẫn do con người viết và ngẫu nhiên bất kể nó được đánh giá trên các nhiệm vụ phân loại, trích xuất hay sinh, với ngoại lệ duy nhất là phân loại khả năng trả lời. Đáng chú ý, Auto-Instruct vượt trội hơn hướng dẫn gốc do con người viết 10%, 9% và 8% trên phân loại thường thức (nhiệm vụ phân loại), tương tự từ (nhiệm vụ sinh ngắn) và tạo đối thoại (nhiệm vụ sinh dài), tương ứng.

[Hình 9: Hiệu suất few-shot của các hướng dẫn được chọn bởi Auto-Instruct (được ký hiệu là "Selected") trên tất cả 8 danh mục của các nhiệm vụ kiểm tra SuperNI, so với các hướng dẫn do con người viết và được chọn ngẫu nhiên.]

### D.2 Tổng quát hóa sang các LLMs khác
Ngoài Phần 5.4.1, chúng tôi đánh giá thêm khả năng tổng quát hóa của Auto-Instruct sang các LLMs mã nguồn mở. Như được chứng minh trong Bảng 5, các hướng dẫn được chọn bởi Auto-Instruct tăng cường hiệu suất của LLaMA-2-chat (Touvron et al., 2023). Điều này một lần nữa nhấn mạnh khả năng của Auto-Instruct để tổng quát hóa trên các LLMs khác nhau mà không cần huấn luyện lại mô hình xếp hạng hướng dẫn. Đáng chú ý rằng chúng tôi sử dụng các hướng dẫn được tạo ra bởi text-davinci-003 trong những thí nghiệm này, vì cả phiên bản 7B và 13B của LLaMA-2-chat thể hiện khả năng yếu hơn trong việc tuân theo các meta-prompt của chúng tôi để tạo hướng dẫn, tương phản với các mô hình GPT mega-size. Chúng tôi để lại nghiên cứu về tạo hướng dẫn với các LLMs mã nguồn mở gần đây như công việc tương lai.

[Bảng 5: Kết quả SuperNI của việc chuyển Auto-Instruct sang LLaMA-2-chat-7B, sử dụng các hướng dẫn được tạo ra bởi text-davinci-003. Mô hình xếp hạng hướng dẫn vẫn là mô hình được huấn luyện trên hướng dẫn text-davinci-003.]

### D.3 So sánh với ensemble đáp án
Vì Auto-Instruct bao gồm lấy mẫu nhiều hướng dẫn ứng cử viên trước khi chọn hướng dẫn tốt nhất, chúng tôi so sánh nó với một cách tiếp cận lấy mẫu khác, tức là, lấy mẫu và ensemble nhiều đáp án. Sử dụng hướng dẫn gốc do con người viết, chúng tôi lấy mẫu phản hồi 10 lần với nucleus sampling (Holtzman et al., 2020), mà không lấy mẫu nhiều hướng dẫn. Sau đó, chúng tôi ensemble tất cả 10 phản hồi bằng cách tận dụng xác suất LM của mỗi phản hồi duy nhất trước khi chọn phản hồi có khả năng cao nhất, tương tự như ý tưởng của self-consistency (Wang et al., 2023b). Kết quả, được thể hiện trong Bảng 6, cho thấy rằng cách tiếp cận ensemble đáp án chỉ mang lại cải thiện nhỏ trên SuperNI, không thể so sánh với việc cải thiện hiệu suất đạt được với Auto-Instruct.

[Bảng 6: Kết quả của ensemble đa đáp án được gợi ý bởi các hướng dẫn do con người viết trên các nhiệm vụ kiểm tra SuperNI.]

## Phụ lục E Meta-prompt cho tạo hướng dẫn
Trong phần này, chúng tôi liệt kê tất cả các meta-prompt được sử dụng trong quá trình tạo hướng dẫn, như được phác thảo trong §4.1. Đối với bối cảnh zero-shot, chúng tôi bỏ qua trường "Examples" trong meta-prompt để LLM diễn đạt lại hướng dẫn seed. Bên cạnh đó, meta-prompt với giải thích cho các minh chứng không áp dụng được trong bối cảnh zero-shot. Meta-prompt sử dụng các nhiệm vụ khác làm minh chứng (Hình 10e) được tích hợp với ba nhóm nhiệm vụ minh chứng, mỗi nhóm khác nhau về độ dài hướng dẫn trung bình. Do đó, LLM được gợi ý để tạo ra các hướng dẫn có độ chi tiết tương tự như các nhiệm vụ minh chứng. Các nhiệm vụ minh chứng được lấy mẫu từ SuperNI. Trong SuperNI, mỗi nhiệm vụ được ghép nối với một tóm tắt nhiệm vụ ngắn gọn và một định nghĩa nhiệm vụ chi tiết thường dài hơn nhiều. Đối với mỗi nhiệm vụ minh chứng, chúng tôi sử dụng tóm tắt nhiệm vụ làm hướng dẫn seed và định nghĩa nhiệm vụ làm hướng dẫn mục tiêu. Chúng tôi kiềm chế không sử dụng định nghĩa nhiệm vụ trong các nhiệm vụ kiểm tra vì (1) một số định nghĩa nhiệm vụ quá dài để phù hợp trong mô hình T5 cùng với đầu vào (2) chúng tôi thấy trong thực tế rằng LLM có xu hướng lặp lại định nghĩa nhiệm vụ ở mức độ lớn nếu nó được sử dụng làm hướng dẫn seed. Mặc dù Auto-Instruct chưa bao giờ thấy định nghĩa nhiệm vụ dài hơn nhiều của các nhiệm vụ kiểm tra, hướng dẫn được chọn của chúng tôi vẫn hoạt động tốt hơn so với việc sử dụng định nghĩa nhiệm vụ làm hướng dẫn, với điểm số trung bình 62.41 trên SuperNI trong bối cảnh few-shot. Chúng tôi để lại việc khám phá tích hợp các hướng dẫn phức tạp hơn như công việc tương lai.

[Hình 10: Meta-prompt mà chúng tôi sử dụng để chỉ định các phong cách mong muốn khác nhau của hướng dẫn trong quá trình tạo hướng dẫn.]

## Phụ lục F Nghiên cứu trường hợp bổ sung
Trong phần này, chúng tôi cung cấp thêm 3 trường hợp (2 few-shot và 1 zero-shot) nơi Auto-Instruct cải thiện các hướng dẫn gốc do con người viết. Những nghiên cứu trường hợp này được thể hiện trong Hình 11, 12, và 13. Vui lòng tham khảo các chú thích tương ứng để biết giải thích trường hợp chi tiết.

[Hình 11-13: Các nghiên cứu trường hợp bổ sung]

## Phụ lục G Tất cả nhiệm vụ kiểm tra
Trong Bảng 7, chúng tôi liệt kê tất cả 91 nhiệm vụ kiểm tra SuperNI được sử dụng trong các thí nghiệm ngoài miền của chúng tôi. Vì kích thước của các nhiệm vụ không cân bằng trên SuperNI, để đánh giá hiệu quả, chúng tôi lấy mẫu ngẫu nhiên 200 trường hợp cho mỗi nhiệm vụ, tạo ra tổng cộng 18,200 ví dụ kiểm tra.

[Bảng 7: Tất cả các nhiệm vụ kiểm tra SuperNI, được nhóm thành các danh mục khác nhau.]
