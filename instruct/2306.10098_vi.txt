# Tối ưu hóa Chỉ dẫn Khả vi cho Khái quát hóa Đa nhiệm
Masaru Isonuma1,2Junichiro Mori1,3Ichiro Sakata1
1Đại học Tokyo2Đại học Edinburgh3RIKEN
{isonuma, isakata}@ipr-ctr.t.u-tokyo.ac.jp mori@mi.u-tokyo.ac.jp

## Tóm tắt
Điều chỉnh chỉ dẫn đã thu hút nhiều sự chú ý để đạt được khả năng khái quát hóa trên nhiều loại nhiệm vụ khác nhau. Mặc dù nhiều loại chỉ dẫn khác nhau đã được tạo ra thủ công cho việc điều chỉnh chỉ dẫn, vẫn chưa rõ loại chỉ dẫn nào là tối ưu để có được khả năng khái quát hóa đa nhiệm. Công trình này trình bày tối ưu hóa chỉ dẫn, tối ưu hóa các chỉ dẫn huấn luyện đối với khả năng khái quát hóa. Thay vì điều chỉnh chỉ dẫn thủ công, chúng tôi giới thiệu các chỉ dẫn có thể học và tối ưu hóa chúng bằng thuật toán giảm gradient bằng cách tận dụng tối ưu hóa hai cấp. Kết quả thực nghiệm cho thấy chỉ dẫn được học tăng cường tính đa dạng của chỉ dẫn và cải thiện khả năng khái quát hóa so với việc chỉ sử dụng các chỉ dẫn được tạo thủ công.

## 1 Giới thiệu
Gần đây, tiến bộ đáng kể đã được thực hiện trong việc phát triển các mô hình có thể khái quát hóa cho các nhiệm vụ tùy ý bằng cách tuân theo các mô tả ngôn ngữ tự nhiên (Brown et al., 2020; Ouyang et al., 2022). Điều chỉnh chỉ dẫn đã trở thành một khu vực quan tâm như một kỹ thuật huấn luyện để có được khả năng khái quát hóa như vậy (Wei et al., 2022; Sanh et al., 2022; Mishra et al., 2022). Bằng cách tinh chỉnh các mô hình ngôn ngữ được huấn luyện trước trên nhiều nhiệm vụ khác nhau với các chỉ dẫn của chúng, các mô hình có thể khái quát hóa cho các nhiệm vụ tùy ý chưa được thấy trong quá trình huấn luyện. Nhiều nghiên cứu trước đây đã chứng kiến hiệu quả của việc điều chỉnh chỉ dẫn (Chung et al., 2022; Wang et al., 2022; Lampinen et al., 2022).

Nhiều chỉ dẫn khác nhau đã được tạo ra cho việc điều chỉnh chỉ dẫn, chẳng hạn như tên nhiệm vụ, định nghĩa nhiệm vụ, các ví dụ tích cực/tiêu cực của một nhiệm vụ, giải thích tại sao mỗi ví dụ tích cực/tiêu cực là đúng/sai, v.v. Tuy nhiên, Mishra et al. (2022); Wang et al. (2022) đã chỉ ra rằng định nghĩa và các ví dụ tích cực của nhiệm vụ là đủ cho việc điều chỉnh chỉ dẫn, và tác dụng của việc thêm các loại chỉ dẫn khác là không đáng kể hoặc đôi khi có tác động tiêu cực đến hiệu suất khái quát hóa.

Tìm kiếm một chỉ dẫn tối ưu cho khái quát hóa đa nhiệm là một vấn đề quan trọng đối với việc điều chỉnh chỉ dẫn, trong khi nó đòi hỏi rất nhiều nỗ lực của con người (hơn 100 nhà nghiên cứu đã tham gia vào các nghiên cứu trước đây). Hơn nữa, các chỉ dẫn có thể hiểu được bởi con người không nhất thiết là tối ưu để có được khả năng khái quát hóa đa nhiệm.

Dựa trên bối cảnh này, chúng tôi đề xuất tối ưu hóa chỉ dẫn, giới thiệu các chỉ dẫn có thể học và tối ưu hóa chúng đối với khả năng khái quát hóa đa nhiệm. Như được thể hiện trong Hình 1, một mô hình θ được tối ưu hóa để tối đa hóa hiệu suất trên các nhiệm vụ meta-train bằng cách tuân theo các chỉ dẫn có thể học. Ngược lại, các chỉ dẫn có thể học ϕ được huấn luyện để tối đa hóa hiệu suất meta-test của mô hình được huấn luyện θ∗(ϕ). Tối ưu hóa này được gọi là tối ưu hóa hai cấp và thường được sử dụng trong tối ưu hóa siêu tham số (Franceschi et al., 2017; Lorraine et al., 2020), meta-learning (Finn et al., 2017; Franceschi et al., 2018), và tìm kiếm kiến trúc mạng nơ-ron (Liu et al., 2018; Zhang et al., 2021). Chúng tôi coi các chỉ dẫn huấn luyện là một loại siêu tham số đặc biệt và tối ưu hóa chúng bằng thuật toán giảm gradient bằng cách nới lỏng không gian tìm kiếm để trở thành liên tục.

Để tạo ra các chỉ dẫn có thể học, chúng tôi đề xuất hai phương pháp: nhúng chỉ dẫn, tạo ra các embedding của chỉ dẫn, và trích xuất chỉ dẫn, chọn một ví dụ nhiệm vụ tối ưu. Gần đây, kỹ thuật prompt engineering đã thu hút sự chú ý để tìm kiếm prompt tối ưu để thực hiện một nhiệm vụ (Liu et al., 2022b). Một số công trình nghiên cứu các prompt liên tục thực hiện prompting trong không gian embedding của token (Li and Liang, 2021; Lester et al., 2021), trong khi những công trình khác truy xuất các ví dụ tối ưu như một prompt thử nghiệm cho in-context learning (Liu et al., 2022a; Rubin et al., 2022). Nhúng chỉ dẫn và trích xuất chỉ dẫn của chúng tôi tuân theo ý tưởng của các prompt liên tục và bộ truy xuất prompt, tương ứng. Trong khi công trình trước đây tối ưu hóa prompt để giải quyết một nhiệm vụ cá nhân trong thử nghiệm, nghiên cứu của chúng tôi khác biệt về mục tiêu và mục đích của tối ưu hóa. Chúng tôi tối ưu hóa các prompt huấn luyện để tối đa hóa khả năng khái quát hóa đa nhiệm của mô hình được huấn luyện.

Trong thực nghiệm, chúng tôi xác nhận rằng trích xuất chỉ dẫn đã trích xuất thành công chỉ dẫn phù hợp, cung cấp bằng chứng về khái niệm. Về so sánh với điều chỉnh chỉ dẫn, nhúng chỉ dẫn tăng cường tính đa dạng của chỉ dẫn và cải thiện khả năng khái quát hóa so với việc chỉ sử dụng các chỉ dẫn được tạo thủ công. Ngược lại, trích xuất chỉ dẫn không đóng góp vào việc tăng hiệu suất, điều này cho thấy việc sử dụng cùng một ví dụ nhiệm vụ trên các trường hợp là bất ngờ có lợi cho khái quát hóa đa nhiệm. Nghiên cứu này cung cấp cơ sở để khám phá các chỉ dẫn tối ưu cho điều chỉnh chỉ dẫn.

## 2 Kiến thức nền tảng
Điều chỉnh chỉ dẫn huấn luyện một mô hình θ để tối thiểu hóa loss huấn luyện được định nghĩa trong Phương trình (1):

θ∗=argmin θL(θ)
=argmin θ∑ t∈Ttrain ∑i=1^Nt −logpθ(y_t^(i)|[It;X_t^(i)])   (1)

trong đó X_t^(i) và It biểu thị ma trận embedding của đầu vào thứ i và chỉ dẫn của nhiệm vụ t, tương ứng. y_t^(i) là một chuỗi token đại diện cho nhãn lớp hoặc văn bản tham chiếu. Điều chỉnh chỉ dẫn coi tất cả các nhiệm vụ như việc tạo văn bản có điều kiện cho việc nối chỉ dẫn và đầu vào nhiệm vụ [It;Xt]. Bằng cách thêm chỉ dẫn vào trước đầu vào nhiệm vụ, mô hình được huấn luyện θ∗ có thể khái quát hóa cho nhiều nhiệm vụ chưa thấy khác nhau t ∉ Ttrain.

Các chỉ dẫn huấn luyện tối ưu đã được tìm kiếm bằng cách tạo ra thủ công nhiều loại chỉ dẫn khác nhau cho điều chỉnh chỉ dẫn (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022). Tuy nhiên, Mishra et al. (2022); Wang et al. (2022) đã chỉ ra rằng định nghĩa nhiệm vụ và các ví dụ nhiệm vụ là đủ cho điều chỉnh chỉ dẫn, trong khi việc thêm các loại chỉ dẫn khác là không đáng kể hoặc đôi khi ảnh hưởng tiêu cực đến hiệu suất khái quát hóa. Quan sát này thúc đẩy chúng tôi tự động tối ưu hóa các chỉ dẫn huấn luyện, thay vì điều chỉnh chúng thủ công. Chúng tôi giới thiệu các chỉ dẫn có thể học và tối ưu hóa chúng bằng thuật toán giảm gradient bằng cách tận dụng tối ưu hóa hai cấp. Phần tiếp theo cung cấp chi tiết về tối ưu hóa chỉ dẫn.

## 3 Tối ưu hóa Chỉ dẫn
Tối ưu hóa chỉ dẫn chia các nhiệm vụ huấn luyện Ttrain thành hai tập: các nhiệm vụ meta-train Tmeta−train và các nhiệm vụ meta-test Tmeta−test. Sau đó, một mô hình θ được huấn luyện để tối thiểu hóa loss trong trên các nhiệm vụ meta-train bằng cách tuân theo các chỉ dẫn có thể học Iϕ trong Phương trình (2).

θ∗(ϕ) = argmin θLin(θ,ϕ)
=argmin θ∑ t∈Tmeta−train ∑i=1^Nt −logpθ(y_t^(i)|[Iϕ;X_t^(i)])   (2)

trong đó ϕ là một tham số cho các chỉ dẫn có thể học. Iϕ được xây dựng bằng cách sử dụng nhúng chỉ dẫn (Phần 3.1) hoặc trích xuất chỉ dẫn (Phần 3.2), sẽ được giải thích sau.

Nếu chỉ dẫn có thể học Iϕ được tạo ngẫu nhiên, mô hình được huấn luyện θ∗(ϕ) sẽ hoạt động kém trên các nhiệm vụ chưa thấy. Do đó, chúng tôi tối ưu hóa ϕ sao cho mô hình được huấn luyện θ∗(ϕ) đạt được hiệu suất cao trên các nhiệm vụ meta-test, không được hiển thị trong quá trình huấn luyện. ϕ được cập nhật để tối thiểu hóa loss ngoài trong Phương trình (3).

ϕ∗= argmin ϕLout(θ∗(ϕ))
=argmin ϕ∑ t∈Tmeta−test ∑i=1^Nt −logpθ∗(y_t^(i)|[It;X_t^(i)])   (3)

Tối ưu hóa này được gọi là tối ưu hóa hai cấp và thường được sử dụng trong tối ưu hóa siêu tham số. Lưu ý rằng chúng tôi sử dụng chỉ dẫn được tạo thủ công It để đo hiệu suất meta-test vì chúng tôi muốn phát triển một mô hình có thể chấp nhận các chỉ dẫn do con người tạo ra tùy ý.

### 3.1 Nhúng Chỉ dẫn
Phần này trình bày một phương pháp để tạo ra các chỉ dẫn có thể học Iϕ. Như được thể hiện trong Hình 2 (trái), nhúng chỉ dẫn thay thế các chỉ dẫn được tạo thủ công bằng các embedding của chỉ dẫn có thể học hoặc thêm chúng vào trước các chỉ dẫn được tạo thủ công. Chúng tôi xem xét hai loại tham số hóa của chỉ dẫn có thể học sau đây:

**Tham số hóa Trực tiếp (DP)** Chúng tôi tham số hóa chỉ dẫn có thể học Iϕ bằng cách chuẩn bị một ma trận có thể học cho mỗi nhiệm vụ: Iϕ=Wt ∈ R^l×d trong đó l biểu thị độ dài tùy ý của một chỉ dẫn có thể học, và d là chiều của các embedding trong mô hình θ. Mặc dù tham số hóa này rất đơn giản, kích thước của tham số ϕ (|Ttrain|×l×d) tăng khi có nhiều nhiệm vụ huấn luyện. Hơn nữa, vì mỗi ma trận có thể học Wt chỉ được cập nhật khi nhiệm vụ t được sử dụng để tính loss meta-train, các ma trận được cập nhật không thường xuyên khi số lượng nhiệm vụ huấn luyện lớn. Do đó, chúng tôi đề xuất một phương pháp tham số hóa khác có thể mở rộng cho số lượng lớn các nhiệm vụ huấn luyện.

**Chuyển đổi Trường hợp (IC)** Một phương pháp tham số hóa khác là chuyển đổi một trường hợp nhiệm vụ z_t^(i) thành Iϕ như được thể hiện trong Phương trình (4) và (5).

h_t^(i) = avgpool(z_t^(i) Vϕ)   (4)
Iϕ = Wϕh_t^(i)   (5)

trong đó trường hợp nhiệm vụ z_t^(i) là một chuỗi token được định nghĩa là "Input: x_t^(i) Output: y_t^(i)", trong đó x_t^(i) và y_t^(i) đại diện cho đầu vào và đầu ra thứ i của một nhiệm vụ t, tương ứng. Vϕ ∈ R^v×d' là một ma trận embedding từ trong đó v biểu thị kích thước từ vựng, và avgpool biểu thị thao tác average-pooling trên các token được nhúng. h_t^(i) ∈ R^d' biểu thị một biểu diễn tiềm ẩn của z_t^(i), và Wϕ ∈ R^l×d×d' là một tensor có thể học để chuyển đổi biểu diễn tiềm ẩn thành một chỉ dẫn^1. Chúng tôi giả định rằng Vϕ và Wϕ được tối ưu hóa để tạo ra một chỉ dẫn tối ưu cho một trường hợp nhiệm vụ. Vì các tham số được chia sẻ trên tất cả các nhiệm vụ huấn luyện, tham số hóa này có thể mở rộng cho số lượng lớn các nhiệm vụ huấn luyện.

### 3.2 Trích xuất Chỉ dẫn
Chúng tôi xem xét một loại chỉ dẫn khác có nhiều ứng viên để sử dụng. Một ví dụ nhiệm vụ là một ví dụ vì mỗi trường hợp nhiệm vụ j ∈ {1, . . . , N^t} trong tập huấn luyện có thể được sử dụng như một ví dụ nhiệm vụ. Trong khi điều chỉnh chỉ dẫn chọn ngẫu nhiên một ví dụ nhiệm vụ làm chỉ dẫn, một ví dụ nhiệm vụ tối ưu sẽ tồn tại cho khái quát hóa đa nhiệm. Chúng tôi khám phá cách chọn ví dụ nhiệm vụ tối ưu để tối đa hóa hiệu suất trên các nhiệm vụ chưa thấy. Một phác thảo của trích xuất chỉ dẫn được thể hiện trong Hình 2 (phải).

Chúng tôi tham số hóa xác suất pϕ(z_t^(j)), trong đó trường hợp thứ j được chọn làm ví dụ của nhiệm vụ t. Tương tự như nhúng chỉ dẫn, chúng tôi xem xét hai tham số hóa sau đây:

**Tham số hóa Trực tiếp (DP)** Chúng tôi tham số hóa logits của pϕ(z_t^(j)) bằng cách sử dụng một vector có thể học vt ∈ R^Nt cho mỗi nhiệm vụ t. Logits được chuyển đổi thành xác suất sử dụng hàm softmax trong Phương trình (6).

pϕ(z_t^(j)) = exp(v_t^(j)) / ∑_{j=1}^{Nt} exp(v_t^(j))   (6)

Tham số hóa này đơn giản nhưng không thể mở rộng khi số lượng nhiệm vụ huấn luyện lớn.

**Chuyển đổi Trường hợp (IC)** Trong khi tham số hóa trực tiếp tham số hóa pϕ(z_t^(j)) bất kể trường hợp nhiệm vụ (tức là đầu vào và đầu ra nhiệm vụ), chuyển đổi trường hợp xem xét xác suất có điều kiện cho một trường hợp nhiệm vụ. Cụ thể, chuyển đổi trường hợp tham số hóa xác suất trong đó z_t^(j) được chọn làm ví dụ của trường hợp z_t^(i) trong Phương trình (7).

pϕ(z_t^(j)|z_t^(i)) = exp(h_t^(j) Wϕ h_t^(i)) / ∑_{j=1}^{Nt} exp(h_t^(j) Wϕ h_t^(i))   (7)

trong đó Wϕ ∈ R^{d'×d'} biểu thị một ma trận có thể học, và h_t^(j) ∈ R^{d'} là một biểu diễn tiềm ẩn của trường hợp nhiệm vụ z_t^(j) thu được bởi Phương trình (4). Tham số hóa này giả định rằng Vϕ và Wϕ được tối ưu hóa để chọn một ví dụ tối ưu cho một trường hợp nhiệm vụ. Vì các tham số ϕ được chia sẻ trên tất cả các nhiệm vụ huấn luyện, tham số hóa này cũng có thể mở rộng cho số lượng lớn các nhiệm vụ huấn luyện.

Sau đó, một trường hợp có xác suất cao nhất được trích xuất làm chỉ dẫn như được thể hiện trong Phương trình (8) và (9).

zt = argmax_j pϕ(z_t^(j))   (8)
Iϕ = zt Vθ   (9)

trong đó Vθ ∈ R^{v×d} là ma trận embedding từ của mô hình θ. Vì thao tác argmax không khả vi, chúng tôi sử dụng ước lượng straight-through (Bengio et al., 2013) để xấp xỉ gradient trong lượt truyền ngược^2. Vì tính toán xác suất của tất cả các trường hợp đòi hỏi chi phí tính toán cao khi số lượng trường hợp đáng kể, chúng tôi đặt một giá trị không đổi là Nt=N và lấy mẫu ngẫu nhiên N trường hợp từ tất cả các trường hợp huấn luyện.

### 3.3 Giải quyết Hiệu quả Tối ưu hóa Hai cấp
Việc giải quyết trực tiếp tối ưu hóa hai cấp đòi hỏi chi phí tính toán đáng kể vì nó bao gồm một công thức lồng nhau. Như được thể hiện trong Thuật toán 1, việc xấp xỉ tối ưu hóa trong trong Phương trình (2) bằng K bước gradient giảm đáng kể chi phí tính toán, trong đó K đủ lớn để đạt đến các điểm tối ưu của vòng lặp trong (Franceschi et al., 2017; Shaban et al., 2019).

**Thuật toán 1** Tối ưu hóa Hai cấp
```
while not converged do
    for k = 1, . . . , K do
        θ^(k) ← θ^(k−1) − η∇θLin(θ,ϕ)|θ=θ^(k−1)
    end for
    ϕ ← ϕ − η∇ϕLout(θ^(K))
end while
```

Tính toán hypergradient ∇ϕLout(θ^(K)) vẫn đòi hỏi không gian bộ nhớ lớn O(K|θ|+|ϕ|) vì nó cần lưu trữ gradient K bước (Franceschi et al., 2017), và mô hình ngôn ngữ θ chứa rất nhiều tham số. Sử dụng định lý hàm ẩn trong Phương trình (10) và (11), hypergradient có thể được tính toán mà không cần lưu trữ các gradient trung gian (Bengio, 2000; Lorraine et al., 2020).

∇ϕLout(θ^(K)(ϕ)) = ∂Lout(θ^(K))/∂θ^(K) ∂θ^(K)(ϕ)/∂ϕ   (10)

∂θ^(K)(ϕ)/∂ϕ = −[∂Lin(θ,ϕ)/∂θ∂θ]^{−1} ∂Lin(θ,ϕ)/∂θ∂ϕ |_{θ^(K),ϕ}   (11)

Tuy nhiên, việc tính toán nghịch đảo của ma trận Hessian trong Phương trình (11) là không thực tế vì việc nghịch đảo chính xác Hessian thường đòi hỏi chi phí tính toán O(|θ|^3). Do đó chúng tôi xấp xỉ nghịch đảo-Hessian bằng cách sử dụng xấp xỉ Neumann, được giới thiệu trong tối ưu hóa siêu tham số (Lorraine et al., 2020; Zhang et al., 2021). Nghịch đảo của ma trận Hessian có thể được xấp xỉ như được thể hiện trong Phương trình (12).

[∂Lin(θ,ϕ)/∂θ∂θ]^{−1} = lim_{M→∞} γ ∑_{m=0}^M [E − γ∂Lin(θ,ϕ)/∂θ∂θ]^m   (12)

trong đó E biểu thị một ma trận đơn vị. γ ∈ R đủ nhỏ để thỏa mãn ∥E−γ∂Lin(θ,ϕ)/∂θ∂θ∥<1 trong chuẩn toán tử. Do đó, chi phí tính toán của hypergradient giảm đáng kể xuống O(|θ|+|ϕ|) như được thể hiện trong Lorraine et al. (2020).

## 4 Thực nghiệm

### 4.1 Thiết lập Thực nghiệm^3

**Tập dữ liệu** Trong thực nghiệm này, chúng tôi sử dụng SUPER-NATURAL INSTRUCTIONS (SUP-NATINST; Wang et al., 2022) làm chuẩn để đo khái quát hóa đa nhiệm. SUP-NATINST bao gồm hơn 1.600 nhiệm vụ đa dạng và các chỉ dẫn của chúng trên nhiều ngôn ngữ. Chúng tôi sử dụng các nhiệm vụ tiếng Anh và các chỉ dẫn của chúng, dẫn đến tổng cộng 876 nhiệm vụ. Chúng tôi sử dụng cùng tập phân chia thử nghiệm của các nhiệm vụ (12 loại; 119 nhiệm vụ) và 100 trường hợp cho mỗi nhiệm vụ như Wang et al. (2022). 60 loại nhiệm vụ còn lại (757 nhiệm vụ) được sử dụng cho meta-train, meta-test và validation. Tập validation bao gồm 10 trường hợp trên tất cả 757 nhiệm vụ, được sử dụng để xác định các siêu tham số bao gồm phân chia meta-train/test. Dựa trên hiệu suất validation, chúng tôi chia 60 loại nhiệm vụ thành 50 và 10 loại, được sử dụng cho tập meta-train và meta-test, tương ứng. Chúng tôi sử dụng 100 trường hợp của mỗi nhiệm vụ cho tập meta-train/test. Bảng 1 tóm tắt các thống kê cho mỗi phân chia. Các loại nhiệm vụ trong mỗi phân chia được liệt kê trong Phụ lục A.1.

**Bảng 1:** Thống kê của tập dữ liệu.
| Phân chia | Meta-train | Meta-test | Valid | Test |
|-----------|------------|-----------|-------|------|
| # nhiệm vụ | 715 | 42 | 757 | 119 |
| # loại nhiệm vụ | 50 | 10 | 60 | 12 |
| # trường hợp/nhiệm vụ | 100 | 100 | 10 | 100 |

**Đánh giá & Baseline** Chúng tôi đánh giá khái quát hóa đa nhiệm trong hai thiết lập: thiết lập zero-shot sử dụng định nghĩa nhiệm vụ làm chỉ dẫn thử nghiệm, và thiết lập one-shot sử dụng một ví dụ nhiệm vụ (n=1) làm chỉ dẫn thử nghiệm. Chúng tôi áp dụng ROUGE-L (Lin, 2004) để đánh giá tất cả các nhiệm vụ. Wang et al. (2022) chỉ ra rằng kết quả đánh giá của con người khá phù hợp với ROUGE-L trên nhiều nhiệm vụ khác nhau.

Đối với các chỉ dẫn huấn luyện baseline, chúng tôi sử dụng các chỉ dẫn được tạo thủ công (ví dụ: định nghĩa nhiệm vụ), các ví dụ được chọn ngẫu nhiên cho mỗi nhiệm vụ hoặc mỗi trường hợp. Các chỉ dẫn có thể học được tạo ra bởi nhúng chỉ dẫn hoặc các ví dụ tối ưu được chọn bởi trích xuất chỉ dẫn được so sánh.

**Chi tiết Triển khai** Trong thực nghiệm của chúng tôi, chúng tôi sử dụng T5 được huấn luyện trước (Raffel et al., 2020) làm mô hình θ. Cụ thể, chúng tôi sử dụng phiên bản LM-adapted của T5-base gốc (220M)^4, được huấn luyện thêm với mục tiêu mô hình hóa ngôn ngữ (Lester et al., 2021). Các siêu tham số của mô hình θ được điều chỉnh dựa trên hiệu suất validation của điều chỉnh chỉ dẫn (baseline), và cùng các siêu tham số được sử dụng cho tối ưu hóa chỉ dẫn. Các siêu tham số của chỉ dẫn có thể học ϕ được xác định đối với hiệu suất validation của tối ưu hóa chỉ dẫn. Chi tiết thêm được cung cấp trong Phụ lục A.2.

### 4.2 Bằng chứng Khái niệm
Trước khi chuyển sang so sánh với điều chỉnh chỉ dẫn, chúng tôi chỉ ra rằng trích xuất chỉ dẫn của chúng tôi tối ưu hóa thành công chỉ dẫn huấn luyện. Chúng tôi huấn luyện các mô hình với hai loại chỉ dẫn huấn luyện: một trong số đó là một ví dụ nhiệm vụ, và cái khác là một văn bản trống. Sau đó, chúng tôi đánh giá chúng trên tập thử nghiệm, trong đó một ví dụ nhiệm vụ được sử dụng làm chỉ dẫn thử nghiệm. Như được thể hiện trong Hình 3 (trái), mô hình được huấn luyện với một ví dụ nhiệm vụ đạt được gần 40% ROUGE-L (đen), trong khi mô hình được huấn luyện với văn bản trống giảm đáng kể xuống khoảng 20% ROUGE-L (xám).

Theo những kết quả sơ bộ này, chúng tôi xác minh rằng trích xuất chỉ dẫn của chúng tôi chọn một cách thích hợp một ví dụ nhiệm vụ từ hai chỉ dẫn huấn luyện và có được khả năng khái quát hóa đủ. Hình 3 (trái) cho thấy trích xuất chỉ dẫn của chúng tôi đạt được hiệu suất cạnh tranh với mô hình được huấn luyện với một ví dụ nhiệm vụ. Cụ thể, chuyển đổi trường hợp (IC; xanh dương) hội tụ nhanh hơn so với tham số hóa trực tiếp (DP; xanh dương nhạt). Hình 3 (phải) trình bày tỷ lệ phần trăm của các trường hợp huấn luyện trong đó một ví dụ nhiệm vụ được chọn làm chỉ dẫn huấn luyện. Về DP, tỷ lệ phần trăm tăng một cách mượt mà, trong khi nó bão hòa ở khoảng 50%. Ngược lại, IC đạt gần 100%, mặc dù sự gia tăng hơi không ổn định. Những kết quả này chỉ ra rằng trích xuất chỉ dẫn của chúng tôi chọn thành công một chỉ dẫn huấn luyện thích hợp. Lưu ý rằng thời gian huấn luyện của tối ưu hóa chỉ dẫn là hợp lý so với điều chỉnh chỉ dẫn, như được thể hiện trong Phụ lục A.3.

### 4.3 Kết quả Chính
Ở đây, chúng tôi kiểm tra hiệu quả của tối ưu hóa chỉ dẫn bằng cách so sánh nó với các baseline. Trong Bảng 2 và 3, chúng tôi hiển thị hiệu suất trung bình trên 8 seed ngẫu nhiên khác nhau và khoảng tin cậy 95% đối với phân phối t.

Bảng 2 hiển thị ROUGE-L trung bình trên tất cả các nhiệm vụ thử nghiệm trong đó định nghĩa nhiệm vụ được sử dụng làm chỉ dẫn thử nghiệm, trong khi thay đổi chỉ dẫn huấn luyện. Như baseline của chỉ dẫn huấn luyện, chúng tôi sử dụng các định nghĩa nhiệm vụ được tạo thủ công được nối với các ví dụ tích cực/tiêu cực và giải thích về mỗi ví dụ tích cực/tiêu cực. Khi chỉ sử dụng các chỉ dẫn có thể học được tạo ra bởi nhúng chỉ dẫn, hiệu suất tệ hơn đáng kể so với các baseline. Hiệu suất kém này cho thấy rằng các chỉ dẫn được học không thể thay thế cho các chỉ dẫn được tạo thủ công. Tuy nhiên, việc nối chỉ dẫn có thể học với định nghĩa nhiệm vụ dẫn đến tăng hiệu suất, trong khi việc thêm các chỉ dẫn khác (ví dụ tích cực/tiêu cực và giải thích) có tác động tiêu cực. Như sẽ được chi tiết hóa trong Phần 5.1, việc thêm các chỉ dẫn có thể học cải thiện tính đa dạng của chỉ dẫn và đạt được hiệu suất khái quát hóa cao hơn.

**Bảng 2:** Đánh giá zero-shot trong đó định nghĩa nhiệm vụ được sử dụng làm chỉ dẫn thử nghiệm, trong khi chỉ dẫn huấn luyện được thay đổi như trên. Def.: định nghĩa nhiệm vụ; Pos.: ví dụ tích cực (n=1), Neg.: ví dụ tiêu cực (n=1); Expl.: giải thích tại sao mỗi ví dụ tích cực/tiêu cực là đúng/sai. DP và IC đại diện cho tham số hóa trực tiếp và chuyển đổi trường hợp, tương ứng.

| Chỉ dẫn Huấn luyện | ROUGE-L |
|--------------------|---------|
| Def. | 33.82 ± 0.47 |
| Def. + Pos. | 27.74 ± 0.41 |
| Def. + Pos. + Neg. | 27.91 ± 0.66 |
| Def. + Pos. + Neg. + Expl. | 29.07 ± 0.31 |
| Instruction Embedder (DP) | 11.79 ± 0.27 |
| Instruction Embedder (IC) | 11.99 ± 0.22 |
| Def. + Instruction Embedder (DP) | 34.79 ± 0.33 |
| Def. + Instruction Embedder (IC) | 34.97 ± 0.46 |

Trong Bảng 3, chúng tôi hiển thị kết quả trong đó một ví dụ nhiệm vụ được sử dụng làm chỉ dẫn thử nghiệm. Thật không may, trích xuất chỉ dẫn của chúng tôi hoạt động kém hơn các ví dụ được chọn ngẫu nhiên cho mỗi nhiệm vụ (tức là, cùng một ví dụ được sử dụng cho mỗi trường hợp). Để điều tra lý do cho hiệu suất tệ hơn, chúng tôi thêm một baseline khác, chọn ngẫu nhiên một ví dụ cho mỗi trường hợp (tức là, các ví dụ khác nhau được sử dụng cho mỗi trường hợp). Bất ngờ, các ví dụ ngẫu nhiên cho kết quả ROUGE-L tệ hơn đáng kể khi chúng được chọn cho mỗi trường hợp. Kết quả này chỉ ra rằng việc sử dụng cùng một ví dụ trên tất cả các trường hợp của mỗi nhiệm vụ là có lợi cho khái quát hóa đa nhiệm. Vì trích xuất chỉ dẫn (DP và IC) cập nhật ví dụ tối ưu trong quá trình tối ưu hóa, nó hoạt động tệ hơn so với các ví dụ được chọn ngẫu nhiên cho mỗi nhiệm vụ. Đặc biệt, vì IC thay đổi ví dụ tối ưu cho mỗi trường hợp, nó dẫn đến hiệu suất thấp hơn.

**Bảng 3:** Đánh giá one-shot trong đó một ví dụ nhiệm vụ được sử dụng làm chỉ dẫn thử nghiệm trong khi chỉ dẫn huấn luyện được thay đổi như trên. Random Exemplar biểu thị các ví dụ được chọn ngẫu nhiên cho mỗi nhiệm vụ hoặc mỗi trường hợp (n=1). DP và IC đại diện cho tham số hóa trực tiếp và chuyển đổi trường hợp, tương ứng.

| Chỉ dẫn Huấn luyện | ROUGE-L |
|--------------------|---------|
| Random Exemplar (each task) | 39.59 ± 0.14 |
| Random Exemplar (each instance) | 37.19 ± 0.25 |
| Instruction Extractor (DP) | 37.85 ± 0.67 |
| Instruction Extractor (IC) | 37.15 ± 0.52 |

Kết quả đánh giá của mỗi loại nhiệm vụ thử nghiệm được thể hiện trong Phụ lục A.4.

## 5 Thảo luận

### 5.1 Phân tích Chỉ dẫn Được học
Chúng tôi thảo luận về cách chỉ dẫn được học đóng góp vào việc cải thiện khái quát hóa đa nhiệm. Vì nhúng chỉ dẫn trực tiếp tạo ra các embedding chỉ dẫn trong không gian liên tục, chỉ dẫn được học khó diễn giải. Theo Lester et al. (2021), chúng tôi tính toán các láng giềng gần nhất của mỗi token trong chỉ dẫn được học từ từ vựng của mô hình θ; tuy nhiên, chúng tôi không thể tìm thấy các mẫu rõ ràng cho các token gần nhất. Do đó, chúng tôi tính toán các embedding của các chỉ dẫn được học và trực quan hóa chúng ở không gian hai chiều bằng cách sử dụng t-SNE (Van der Maaten and Hinton, 2008). Các embedding được thu được bằng average pooling trên các trạng thái ẩn cuối cùng được mã hóa bởi bộ mã hóa T5.

Trong Hình 4, chúng tôi hiển thị các embedding của 20 loại nhiệm vụ hàng đầu đối với số lượng nhiệm vụ trong tập meta-train. Các embedding của định nghĩa nhiệm vụ (trái) được nhóm chặt chẽ theo loại nhiệm vụ, và các nhiệm vụ huấn luyện không bao phủ một số không gian. Mặt khác, các embedding của chỉ dẫn được học (phải) được nhóm một cách thô sơ, và một số loại nhiệm vụ được phân tán trên không gian embedding (ví dụ: phân tích cảm xúc và phát hiện ngôn ngữ độc hại). Vì các chỉ dẫn được học tăng cường tính đa dạng của chỉ dẫn và bao phủ không gian embedding rộng hơn, mô hình được huấn luyện có thể khái quát hóa cho nhiều loại chỉ dẫn rộng hơn. Do đó, các chỉ dẫn được học cải thiện hiệu suất khái quát hóa trên các nhiệm vụ chưa thấy.

Hình 5 hiển thị hiệu suất khái quát hóa liên quan đến độ dài của chỉ dẫn có thể học được thêm vào trước định nghĩa nhiệm vụ. Hiệu suất của mô hình bão hòa khi độ dài là 2^6 = 64. Khi chỉ dẫn dài hơn 64, hiệu suất giảm đáng kể. Vì tối ưu hóa hai cấp có xu hướng không ổn định đối với các siêu tham số quy mô lớn, độ dài chỉ dẫn lớn dẫn đến hiệu suất khái quát hóa thấp.

### 5.2 Phân tích Phân chia Meta-train/test
Chúng tôi nghiên cứu cách phân chia meta-train/test ảnh hưởng đến hiệu suất khái quát hóa của mô hình được huấn luyện.

**Số lượng Nhiệm vụ Meta-train/test** Hình 6 hiển thị hiệu suất với số lượng khác nhau của các loại nhiệm vụ trong phân chia meta-train/test: 1/59, 10/50, 20/40, 30/30, 40/20, 50/10, và 59/1. Trong mỗi phân chia, các nhiệm vụ meta-train/test được chọn ngẫu nhiên. Mô hình được huấn luyện đạt được hiệu suất khái quát hóa tốt nhất khi số lượng loại trong meta-test là 10. Hiệu suất tệ đi khi số lượng nhiệm vụ meta-test tăng, trong khi số lượng nhiệm vụ meta-train giảm tương ứng.

**Đa dạng vs. Không đa dạng** Chúng tôi kiểm tra xem các nhiệm vụ meta-test nên đa dạng hay không đa dạng. Nếu các nhiệm vụ meta-test đa dạng, hiệu suất khái quát hóa sẽ được cải thiện vì chỉ dẫn được huấn luyện để đạt được hiệu suất cao hơn trên các nhiệm vụ khác nhau. Tuy nhiên, nó cũng tăng nguy cơ rằng một số nhiệm vụ meta-test tương tự như các nhiệm vụ meta-train, điều này sẽ ảnh hưởng tiêu cực đến hiệu suất trên các nhiệm vụ chưa thấy. Không rõ ràng liệu các nhiệm vụ meta-test nên đa dạng hay không đa dạng.

Để trả lời câu hỏi này, chúng tôi chuẩn bị hai loại phân chia meta-test. Một bao gồm các nhiệm vụ được chọn ngẫu nhiên, trong khi cái khác bao gồm các nhiệm vụ được nhóm bằng k-means clustering. Chúng tôi chuẩn bị 16 phân chia ngẫu nhiên khác nhau, trong khi k-means chia các nhiệm vụ thành 16 nhóm dựa trên các embedding của định nghĩa nhiệm vụ. Sau đó, đối với cả phân chia ngẫu nhiên và k-means, phân chia tốt nhất cho tập validation được chọn từ 16 phân chia. Kết quả thực nghiệm cho thấy mô hình được huấn luyện trên phân chia ngẫu nhiên đạt được 36.1 ROUGE-L, trong khi của k-means ghi điểm 35.0 ROUGE-L trên tập thử nghiệm. Mặc dù biên độ không đáng kể, chúng tôi xác nhận rằng các nhiệm vụ meta-test đa dạng có lợi hơn cho khái quát hóa đa nhiệm.

## 6 Công trình Liên quan

**Điều chỉnh Chỉ dẫn** Điều chỉnh chỉ dẫn đã thu hút sự chú ý đáng kể để đạt được các mô hình có thể khái quát hóa trên nhiều nhiệm vụ khác nhau (Wei et al., 2022; Sanh et al., 2022; Mishra et al., 2022). Bằng cách thêm vào trước một vài ví dụ (Min et al., 2022b; Chen et al., 2022) hoặc các chỉ dẫn dựa trên văn bản (Wei et al., 2022; Sanh et al., 2022; Mishra et al., 2022) vào học đa nhiệm, mô hình được huấn luyện có thể khái quát hóa cho các nhiệm vụ chưa thấy trong quá trình huấn luyện. Tiến bộ thêm đã được thực hiện bằng cách mở rộng số lượng nhiệm vụ (Wang et al., 2022; Chung et al., 2022), mở rộng kích thước mô hình (Chung et al., 2022; Scao et al., 2022), và cải thiện chiến lược huấn luyện (Lang et al., 2022; Min et al., 2022a; Ye et al., 2023). Ngược lại, công trình của chúng tôi là nghiên cứu đầu tiên tối ưu hóa các chỉ dẫn huấn luyện để cải thiện khả năng khái quát hóa đa nhiệm.

Mặc dù SUPER-NATURAL INSTRUCTIONS (Wang et al., 2022) được sử dụng làm chuẩn để đo khái quát hóa đa nhiệm trong nghiên cứu của chúng tôi, tối ưu hóa chỉ dẫn của chúng tôi có thể được áp dụng cho các chuẩn đa nhiệm khác, chẳng hạn như CROSSFIT (Ye et al., 2021) và PromptSource (Bach et al., 2022).

**Kỹ thuật Prompt Engineering** NLP dựa trên chỉ dẫn gần đây đã phát triển kỹ thuật prompt engineering, tìm kiếm prompt phù hợp nhất để thực hiện một nhiệm vụ (Liu et al., 2022b). Trong khi có nhiều nghiên cứu tìm kiếm prompt tối ưu trong không gian token rời rạc (Shin et al., 2020; Schick and Schütze, 2021; Gao et al., 2021), một số công trình nghiên cứu các prompt liên tục thực hiện prompting trong không gian embedding của token (Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021). Các nghiên cứu khác truy xuất các ví dụ thích hợp như một prompt thử nghiệm cho in-context learning và đạt được hiệu suất tốt hơn so với các ví dụ được chọn ngẫu nhiên (Das et al., 2021; Liu et al., 2022a; Rubin et al., 2022). Trong khi các phương pháp nói trên tối ưu hóa prompt để thực hiện một nhiệm vụ cá nhân trong thử nghiệm, nghiên cứu của chúng tôi khác biệt về mục tiêu và mục đích của tối ưu hóa; chúng tôi tối ưu hóa các prompt huấn luyện để tối đa hóa hiệu suất khái quát hóa của mô hình được huấn luyện.

**Tối ưu hóa Hai cấp** Tối ưu hóa hai cấp đã được sử dụng để tối ưu hóa siêu tham số (Franceschi et al., 2017; Lorraine et al., 2020), trọng số mô hình ban đầu (Finn et al., 2017; Franceschi et al., 2018), và kiến trúc mô hình (Liu et al., 2018; Zhang et al., 2021). Chúng tôi tối ưu hóa các chỉ dẫn huấn luyện bằng cách coi chúng là một loại siêu tham số đặc biệt. Các chỉ dẫn có thể học được xây dựng bởi nhiều siêu tham số, điều này làm cho tối ưu hóa hai cấp khó khăn về mặt chi phí tính toán và ổn định. Các nghiên cứu gần đây (Rajeswaran et al., 2019; Lorraine et al., 2020; Zhang et al., 2021) giảm đáng kể chi phí tính toán và cải thiện ổn định bằng cách kết hợp định lý hàm ẩn với các xấp xỉ nghịch đảo Hessian hiệu quả. Chúng tôi tận dụng ý tưởng này cho tối ưu hóa chỉ dẫn, đạt được tối ưu hóa chỉ dẫn với chi phí tính toán hợp lý và ổn định.

## 7 Kết luận
Nghiên cứu này trình bày tối ưu hóa chỉ dẫn, tối ưu hóa các chỉ dẫn huấn luyện liên quan đến khả năng khái quát hóa. Kết quả thực nghiệm cho thấy trích xuất chỉ dẫn của chúng tôi trích xuất thành công chỉ dẫn phù hợp, cung cấp bằng chứng về khái niệm. Về so sánh với điều chỉnh chỉ dẫn, nhúng chỉ dẫn tăng cường tính đa dạng của chỉ dẫn và cải thiện khả năng khái quát hóa hơn so với việc chỉ sử dụng các chỉ dẫn được tạo thủ công. Ngược lại, trích xuất chỉ dẫn không đóng góp vào việc tăng hiệu suất vì việc sử dụng cùng một ví dụ nhiệm vụ trên các trường hợp là bất ngờ có lợi cho khái quát hóa đa nhiệm. Nghiên cứu này cung cấp cơ sở để khám phá các chỉ dẫn tối ưu cho điều chỉnh chỉ dẫn.

## Hạn chế
Nghiên cứu của chúng tôi sử dụng T5-base (220M) do khả năng tài nguyên tính toán của chúng tôi (Tesla V100 32GB). Do đó, không rõ liệu phương pháp của chúng tôi cũng hiệu quả cho các mô hình lớn hơn, chẳng hạn như T5-XL/XXL. Lester et al. (2021) lập luận rằng các prompt liên tục đặc biệt hiệu quả cho các mô hình T5 lớn. Theo kết quả của họ, nhúng chỉ dẫn của chúng tôi cũng được kỳ vọng sẽ hiệu quả cho các mô hình lớn hơn.

Như được thể hiện trong Hình 3, tối ưu hóa chỉ dẫn hơi không ổn định để hội tụ. Một số nghiên cứu đã giải quyết sự hội tụ không ổn định của tối ưu hóa hai cấp bằng chuẩn hóa L2, dừng sớm (Zela et al., 2019), hoặc nhiễu loạn của siêu tham số (Chen and Hsieh, 2020). Những phương pháp này có thể hiệu quả trong việc ổn định tối ưu hóa chỉ dẫn.

## Tuyên bố Đạo đức
Nghiên cứu của chúng tôi tuân thủ Chính sách Đạo đức ACL. Chúng tôi đã sử dụng S2ORC (Lo et al., 2020, CC BY-NC 4.0), PyTorch (Paszke et al., 2019, giấy phép kiểu BSD) và HuggingFace Transformers (Wolf et al., 2020, Apache-2.0) như các tạo phẩm khoa học. Nghiên cứu của chúng tôi được thực hiện dưới các giấy phép và điều khoản của các tạo phẩm khoa học. Mô hình của chúng tôi được huấn luyện trên một tập hợp các tập dữ liệu có sẵn công khai (Wang et al., 2022), trong đó phân phối dữ liệu không mong muốn, chẳng hạn như thông tin sai lệch, thiên vị, hoặc nội dung xúc phạm, có thể có mặt. Những rủi ro tiềm ẩn như vậy cần được nhận thức.

## Lời cảm ơn
Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh về phản hồi có giá trị của họ. Công trình này được hỗ trợ bởi JST ACT-X JPMJAX1904, JST CREST JPMJCR21D1, NEDO JPNP20006, và JSPS KAKENHI 23K16940, Nhật Bản.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch theo thứ tự và định dạng gốc...]

## A Phụ lục

### A.1 Phân chia Nhiệm vụ
Các loại nhiệm vụ được sử dụng trong phân chia meta-train/meta-test/test được liệt kê trong Bảng 4. Chúng tôi chuẩn bị 16 phân chia ngẫu nhiên của meta-train/test và sử dụng cái đạt được hiệu suất validation tốt nhất.

### A.2 Chi tiết Triển khai
Chúng tôi huấn luyện mô hình θ trong ba epoch sử dụng Adam (Kingma and Ba, 2014) với tốc độ học 1.0×10^-5 với suy giảm tuyến tính, bước khởi động 8000, và kích thước batch 2. Độ dài đầu vào và đầu ra tối đa được đặt là 1024 và 128, tương ứng.

Các chỉ dẫn có thể học ϕ được huấn luyện sử dụng Adam với kích thước batch 8. Tốc độ học được đặt là 1.0×10^-5 cho nhúng chỉ dẫn (DP), 1.0×10^-6 cho nhúng chỉ dẫn (IC), 5.0×10^-5 cho trích xuất chỉ dẫn (DP), 1.0×10^-5 cho trích xuất chỉ dẫn (IC) với suy giảm tuyến tính. Độ dài của chỉ dẫn có thể học là l=64, số bước tối ưu hóa trong là K=20 trong Thuật toán 1, các siêu tham số cho xấp xỉ Neumann là M=1 và γ=1.0×10^-5 trong Phương trình (12). Độ dài đầu vào tối đa trong Phương trình (4) là 128, và chúng tôi lấy mẫu ngẫu nhiên N=32 trường hợp cho các ứng viên của trích xuất chỉ dẫn.

Mã của chúng tôi được triển khai với Python v3.8.13, PyTorch v1.12.0 (Paszke et al., 2019), và transformers v4.18.0 (Wolf et al., 2020). Mã của chúng tôi dựa trên script được công bố bởi Wang et al. (2022)^5. ROUGE-L được tính toán sử dụng gói Python được phân phối bởi Google^6.

### A.3 Thời gian Tính toán
Các thực nghiệm của chúng tôi được thực hiện với một Tesla V100 (32GB) duy nhất. Mỗi lần chạy huấn luyện mất khoảng 8 giờ cho tối ưu hóa chỉ dẫn, trong khi mất 5 giờ cho điều chỉnh chỉ dẫn, không có validation. Tuy nhiên, thời gian huấn luyện của tối ưu hóa chỉ dẫn phụ thuộc vào số bước huấn luyện trong K. Nó giảm xuống 6 giờ khi K=100, trong khi hiệu suất giảm nhẹ.

### A.4 Kết quả Thực nghiệm cho Mỗi Nhiệm vụ Thử nghiệm
Bảng 5 và Bảng 6 hiển thị đánh giá zero-shot và one-shot cho mỗi loại nhiệm vụ thử nghiệm, tương ứng. Chúng tôi hiển thị hiệu suất trung bình trên 8 seed ngẫu nhiên khác nhau và khoảng tin cậy 95% đối với phân phối t.

[Các bảng chi tiết được dịch theo cấu trúc và nội dung gốc...]

^1 Chúng tôi đã thử sử dụng bộ mã hóa T5 để thu được h_t^(i); tuy nhiên, nó làm cho tối ưu hóa hai cấp không ổn định do số lượng lớn tham số.

^2 Chúng tôi cũng đã thử tính toán Iϕ sử dụng kỳ vọng của z_t^(j): Iϕ=E_{pϕ}[z_t^(j) Vθ] thay vì thao tác argmax; tuy nhiên, nó hoạt động kém đáng kể.

^3 Mã có sẵn tại https://github.com/misonuma/instopt.

^4 https://huggingface.co/google/t5-base-lm-adapt

^5 https://github.com/yizhongw/Tk-Instruct

^6 https://pypi.org/project/rouge-score/
