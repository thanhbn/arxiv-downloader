# 2306.08568.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/instruct/2306.08568.pdf
# Kích thước tệp: 1273887 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
WizardCoder : TRAO QUYỀN CHO CÁC MÔ HÌNH NGÔN NGỮ LỚN VỀ MÃ VỚI EVOL-INSTRUCT
Ziyang Luo2∗Can Xu1∗Pu Zhao1Qingfeng Sun1Xiubo Geng1
Wenxiang Hu1Chongyang Tao2Jing Ma2†Qingwei Lin1Daxin Jiang1†
1Microsoft‡
2Đại học Baptist Hong Kong
{cszyluo, majing }@comp.hkbu.edu.hk, {caxu,puzhao }@microsoft.com
{qins,xigeng,wenxh,chongyang.tao,qlin,djiang }@microsoft.com
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn về Mã (Code LLMs), chẳng hạn như StarCoder, đã thể hiện hiệu suất đáng chú ý trong các nhiệm vụ liên quan đến mã khác nhau. Tuy nhiên, khác với các đối tác của chúng trong lĩnh vực mô hình hóa ngôn ngữ tổng quát, kỹ thuật tinh chỉnh hướng dẫn vẫn còn tương đối ít được nghiên cứu trong lĩnh vực này. Trong bài báo này, chúng tôi trình bày Code Evol-Instruct, một phương pháp mới mà áp dụng phương pháp Evol-Instruct vào lĩnh vực mã, tăng cường Code LLMs để tạo ra các mô hình mới WizardCoder. Thông qua các thí nghiệm toàn diện trên năm điểm chuẩn sinh mã nổi bật, cụ thể là HumanEval, HumanEval+, MBPP, DS-1000, và MultiPL-E, các mô hình của chúng tôi thể hiện hiệu suất xuất sắc. Chúng luôn vượt trội hơn tất cả các Code LLMs mã nguồn mở khác với biên độ đáng kể. Đáng chú ý, WizardCoder 15B thậm chí còn vượt qua các LLMs mã nguồn đóng nổi tiếng, bao gồm Claude của Anthropic và Bard của Google, trên các điểm chuẩn HumanEval và HumanEval+. Ngoài ra, WizardCoder 34B không chỉ đạt được điểm HumanEval có thể so sánh với GPT3.5 (ChatGPT) mà còn vượt qua nó trên điểm chuẩn HumanEval+. Hơn nữa, khám phá sơ bộ của chúng tôi làm nổi bật vai trò then chốt của độ phức tạp hướng dẫn trong việc đạt được hiệu suất lập trình đặc biệt.

1 GIỚI THIỆU
Gần đây, Các Mô hình Ngôn ngữ Lớn (LLMs) (Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Anil et al., 2023; Hoffmann et al., 2022; Rae et al., 2021; Zeng et al., 2022; Zhang et al., 2022; Touvron et al., 2023a) đã thu hút sự chú ý to lớn và thể hiện thành công ấn tượng. Đáng chú ý, GPT3.5 (ChatGPT) của OpenAI nổi bật như một ví dụ tiêu biểu. Những mô hình này, thông qua việc tiền huấn luyện rộng rãi trên dữ liệu internet khổng lồ và tinh chỉnh với dữ liệu hướng dẫn chi tiết (Ouyang et al., 2022), đã đạt được hiệu suất zero-shot tối tân (SOTA) trên các nhiệm vụ NLP đa dạng. Xu hướng này cũng mở rộng đến lĩnh vực hiểu và sinh mã, nơi mà một số lượng lớn Code LLMs đã xuất hiện (Chen et al., 2021a; Li et al., 2022; Fried et al., 2022; Nijkamp et al., 2023b; Zheng et al., 2023; Wang et al., 2021; 2023; Li et al., 2023a; Nijkamp et al., 2023a; Rozì`ere et al., 2023). Những mô hình này, được tiền huấn luyện trên dữ liệu mã đáng kể, xuất sắc trong các nhiệm vụ liên quan đến mã khác nhau, liên tục mang lại hiệu suất ấn tượng.

Trái với hầu hết các Code LLMs trước đây chủ yếu tập trung vào quá trình tiền huấn luyện, đã có sự khám phá hạn chế về việc điều chỉnh hướng dẫn chi tiết trong lĩnh vực mã. Việc giới thiệu điều chỉnh hướng dẫn ban đầu được thiết kế để tăng cường khả năng tổng quát hóa của LMs trên các nhiệm vụ khác nhau thông qua huấn luyện đa nhiệm vụ (Raffel et al., 2020; Wei et al., 2022; Chung et al., 2022; Aribandi et al., 2022; Sanh et al., 2022; Xu et al., 2022; Khashabi et al., 2020). InstructGPT của OpenAI (Ouyang et al., 2022), ví dụ, liên quan đến việc yêu cầu các chú thích viên con người cung cấp hướng dẫn rõ ràng để đảm bảo phù hợp với ý định của người dùng. Tương tự, các công trình gần đây như Alpaca (Taori et al., 2023)

∗Đóng góp ngang nhau. Công việc được thực hiện trong thời gian thực tập tại Microsoft.
†Tác giả liên hệ.
‡Bài báo này đã được chấp nhận tại ICLR 2024. Vui lòng trích dẫn phiên bản ICLR.
1arXiv:2306.08568v2  [cs.CL]  27 May 2025

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Phát triển một chương trình Python tạo ra một mật khẩu ngẫu nhiên có độ dài 8 ký tự.Hướng dẫn
Đây là một chương trình ví dụ tạo ra một mật khẩu ngẫu nhiên có độ dài 8 sử dụng module `random` có sẵn của Python:
```python
import random
import string
def generate_password(length):
    chars = string.ascii_letters + string.digits +
string.punctuation
    password = ''.join(random.choice(chars) for i in
range(length))     
    return password
password = generate_password(8)
print(password)
```Phản hồi
Dễ
Nhiệm vụPhát triển một chương trình Python tạo ra một mật khẩu ngẫu nhiên có độ dài 12 ký tự, bao gồm ít nhất một chữ cái viết hoa, một chữ cái viết thường, một chữ số, và một ký tự đặc biệt từ danh sách (!@#$%^&*).
Đây là một đoạn mã ví dụ thực hiện điều này:
```python
import random
import string
def generate_password(length):
    chars = string.ascii_uppercase + string.ascii_lowercase +
string.digits + '!@#$%^&*'
    password = ''
    password += random.choice(string.ascii_uppercase)
    password += random.choice(string.ascii_lowercase)
    password += random.choice(string.digits)
    password += random.choice('!@#$%^&*')
    for i in range(length - 4):
        password += random.choice(chars)
    password_list = list(password)
    random.shuffle(password_list)
    password = ''.join(password_list)
    return password
password = generate_password(12)
print(password)
```Hướng dẫn
Phản hồi
Khó hơn
Nhiệm vụ1. Thêm các ràng buộc và yêu cầu mới vào vấn đề gốc, thêm khoảng 10 từ bổ sung.
2. Thay thế một yêu cầu thường được sử dụng trong nhiệm vụ lập trình bằng một yêu cầu ít phổ biến và cụ thể hơn.
3. ...Phương pháp heuristic
Code
Evol-Instruct

Hình 1: Minh họa cho Code Evol-Instruct mới của chúng tôi và hiệu suất pass@1 vượt trội của WizardCoder 34B, vượt trội hơn SOTA mã nguồn mở (CodeLlama-34B-Series tính đến ngày trước 24 tháng 8, 2023) với biên độ lớn trong 9 ngôn ngữ lập trình khác nhau. Điểm Python là trung bình giữa HumanEval và MBPP.

đã sử dụng phương pháp self-instruct (Wang et al., 2022), nơi GPT3.5 (ChatGPT) tạo ra dữ liệu hướng dẫn. Vicuna (Chiang et al., 2023) sử dụng các cuộc trò chuyện được người dùng chia sẻ thu thập từ ShareGPT.com. WizardLM (Xu et al., 2023) giới thiệu phương pháp Evol-Instruct, bao gồm việc phát triển dữ liệu hướng dẫn tổng quát hiện có để tạo ra các tập dữ liệu phức tạp và đa dạng hơn. Lấy cảm hứng từ những công trình trước đây này trong lĩnh vực tổng quát, công việc của chúng tôi, Code Evol-Instruct, được thiết kế đặc biệt cho các đặc điểm riêng biệt của lĩnh vực lập trình.

Trong nghiên cứu này, chúng tôi mục tiêu tăng cường khả năng của các Code LLMs mã nguồn mở SOTA (tức là StarCoder và CodeLlama), bằng cách giới thiệu Code Evol-Instruct mới của chúng tôi. Động lực của phương pháp điều chỉnh hướng dẫn chi tiết này trong lĩnh vực mã là tự động tăng độ phức tạp của dữ liệu hướng dẫn mã, để tận dụng tối đa khả năng lập trình nội tại của Code LLMs. Code Evol-Instruct của chúng tôi kết hợp một số phương pháp mới, bao gồm heuristics được thiết kế riêng cho các tính năng nhiệm vụ lập trình, heuristics mẫu đối kháng, yêu cầu về độ phức tạp thời gian/không gian, và điều khiển dừng phát triển. Toàn bộ quá trình bao gồm hai bước: ban đầu, chúng tôi áp dụng Code Evol-Instruct để phát triển dữ liệu hướng dẫn mã cơ bản, cụ thể là Code Alpaca (Chaudhary, 2023). Tiếp theo, chúng tôi tinh chỉnh StarCoder và CodeLlama sử dụng tập huấn luyện tuân theo hướng dẫn mã mới được tạo ra, tạo ra các mô hình WizardCoder của chúng tôi.

Hình 1 và kết quả thực nghiệm thu được từ năm điểm chuẩn sinh mã, cụ thể là HumanEval (Chen et al., 2021b), HumanEval+ (Liu et al., 2023), MBPP (Austin et al., 2021), DS-100 (Lai et al., 2022), và MultiPL-E (Cassano et al., 2022), chứng minh rằng các mô hình WizardCoder của chúng tôi vượt trội hơn tất cả các Code LLMs mã nguồn mở khác (trước 24 tháng 8, 2023), đạt được hiệu suất tối tân (SOTA). Đáng chú ý, WizardCoder 15B của chúng tôi thậm chí còn vượt qua Claude nổi tiếng của Anthropic và Bard của Google về tỷ lệ pass trên HumanEval và HumanEval+. Hơn nữa, WizardCoder 34B không chỉ đạt được điểm HumanEval có thể so sánh với GPT3.5 (ChatGPT) mà còn vượt qua nó trên điểm chuẩn HumanEval+. Ngoài ra, các nghiên cứu sơ bộ của chúng tôi chỉ ra rằng độ phức tạp của hướng dẫn là chìa khóa để đạt được hiệu suất lập trình đặc biệt.

Những đóng góp của công việc này có thể được tóm tắt như sau:
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
• Chúng tôi giới thiệu Code Evol-Instruct, một phương pháp tinh chỉnh hướng dẫn mới cho mã, nâng cao hiệu suất của các Code LLMs mã nguồn mở với biên độ lớn.
• Chúng tôi phát triển các mô hình WizardCoder, vượt trội hơn tất cả các Code LLMs mã nguồn mở khác với biên độ đáng kể trong các nhiệm vụ lập trình. Đáng chú ý, phiên bản 15B thậm chí còn vượt trội hơn các LLMs mã nguồn đóng nổi tiếng, như Claude và Bard. Phiên bản 34B đạt được điểm HumanEval có thể so sánh với GPT3.5 (ChatGPT) và vượt qua nó trên điểm chuẩn HumanEval+.
• Chúng tôi tiến hành một nghiên cứu sơ bộ làm nổi bật vai trò then chốt của độ phức tạp hướng dẫn trong việc đạt được hiệu suất lập trình đặc biệt.

2 CÔNG TRÌNH LIÊN QUAN
Mô hình Ngôn ngữ Lớn. Gần đây, LLMs đã thể hiện những thành tựu đáng chú ý trên một phổ rộng các nhiệm vụ. Các công ty công nghệ hàng đầu đã có những bước tiến đáng kể trong việc phát triển các LLMs có trình độ cao. Bao gồm GPT3&4 của OpenAI (Brown et al., 2020; OpenAI, 2023), PaLM của Google (Chowdhery et al., 2022; Anil et al., 2023), và Bard1, Chinchilla của DeepMind (Hoffmann et al., 2022), và Gopher (Rae et al., 2021), cũng như Claude của Anthropic2. Tuy nhiên, điều quan trọng cần lưu ý là những mô hình này là mã nguồn đóng và chỉ có thể được truy cập thông qua các API cụ thể hoặc có thể không truy cập được.

Cộng đồng AI đã chứng kiến việc phát hành một số LLMs mã nguồn mở, nơi các trọng số mô hình được công khai. EleutherAI đã đóng góp GPT-NeoX-20B (Black et al., 2022) và GPT-J-6B (Wang & Komatsuzaki, 2021). Google đã phát hành UL2-20B (Tay et al., 2022). Đại học Tsinghua đã giới thiệu GLM-130B (Zeng et al., 2022). Meta đã phát hành OPT (Zhang et al., 2022) và LLaMA1&2 (Touvron et al., 2023a;b). Đáng chú ý rằng trong khi những mô hình mã nguồn mở này đã có những đóng góp có giá trị, chúng thường không thể hiện cùng mức độ hiệu suất như các đối tác mã nguồn đóng.

Mô hình Ngôn ngữ Lớn cho Mã. Các nghiên cứu gần đây đã giới thiệu một số lượng đáng kể LLMs cho các nhiệm vụ liên quan đến mã để giải quyết các thách thức của việc hiểu và sinh mã. OpenAI đã tiết lộ Codex (Chen et al., 2021a) và Code-Davinci (Microsoft, 2023). Google đã đề xuất PaLM-Coder (Chowdhery et al., 2022). Chúng hoạt động xuất sắc trên các điểm chuẩn hoàn thành mã phổ biến, như HumanEval (Chen et al., 2021b) và MBPP (Austin et al., 2021). Tuy nhiên, những mô hình này là mã nguồn đóng.

Mặt khác, có một số Code LLMs mã nguồn mở có sẵn. Salesforce đã giới thiệu CodeGen1&2 (Nijkamp et al., 2023b;a), CodeT5 (Wang et al., 2021), và CodeT5+ (Wang et al., 2023). Đại học Tsinghua đã đóng góp CodeGeeX (Zheng et al., 2023), và Dự án BigCode đã phát triển StarCoder (Li et al., 2023a). Meta đã phát hành CodeLlama-Series (Rozì`ere et al., 2023), đạt được hiệu suất SOTA mã nguồn mở trên một số điểm chuẩn. Mô hình liên quan chặt chẽ, CodeLlama-Instruct, tinh chỉnh hiệu suất của nó thông qua phương pháp self-instruct. Những mô hình này đã thể hiện những tiến bộ đáng chú ý trong các nhiệm vụ liên quan đến mã. Tuy nhiên, khi so sánh với các mô hình mã nguồn đóng SOTA, chúng vẫn tụt hậu đáng kể. Trái với các mô hình nêu trên, công việc của chúng tôi chứng minh rằng việc huấn luyện thêm Code LLMs với Code Evol-Instruct của chúng tôi có thể tăng cường hiệu suất đáng kể.

Tinh chỉnh Hướng dẫn. Mục tiêu chính của tinh chỉnh hướng dẫn trong giai đoạn đầu là tăng cường khả năng tổng quát hóa liên nhiệm vụ của LMs. Điều này đạt được bằng cách tinh chỉnh LMs với một kho ngữ liệu lớn các nhiệm vụ NLP công khai. T5 (Raffel et al., 2020) là một trong những mô hình đầu tiên khám phá phương pháp này, huấn luyện trên nhiều nhiệm vụ text-to-text có giám sát. Các công trình tiếp theo như FLAN (Wei et al., 2022), ExT5 (Aribandi et al., 2022), T0 (Sanh et al., 2022), và UnifiedQA (Khashabi et al., 2020) đã mở rộng thêm phạm vi nhiệm vụ để củng cố khả năng tổng quát hóa tổng thể của LMs. Đáng chú ý, ZeroPrompt (Xu et al., 2022) và FLAN-T5 (Chung et al., 2022) đã đẩy ranh giới bằng cách kết hợp hàng nghìn nhiệm vụ trong pipeline huấn luyện của họ. Qua những nghiên cứu này, một phát hiện nhất quán nổi lên: tinh chỉnh LMs với các hướng dẫn nhiệm vụ NLP đa dạng mang lại cải thiện hiệu suất đáng kể khi áp dụng cho các nhiệm vụ mới.

1https://bard.google.com/
2https://www.anthropic.com/index/introducing-claude
3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Trong khi tinh chỉnh LMs với các nhiệm vụ NLP đa dạng đã cho thấy kết quả đầy hứa hẹn, nó thường không đạt được sự phù hợp với ý định của người dùng thực tế. OpenAI đã theo đuổi một cách tiếp cận khác bằng cách yêu cầu các chú thích viên con người cung cấp một kho ngữ liệu lớn các hướng dẫn của con người, bao gồm các hình thức đa dạng và một phạm vi rộng các loại nhiệm vụ. Dựa trên tập dữ liệu này, OpenAI đã huấn luyện mô hình GPT3 (Brown et al., 2020) để tạo ra InstructGPT (Ouyang et al., 2022), phù hợp hơn với đầu vào của người dùng. Dòng phát triển này thậm chí đã dẫn đến công trình ấn tượng được biết đến là GPT3.5 (ChatGPT). Tuy nhiên, điều quan trọng cần lưu ý là tập dữ liệu và trọng số mô hình liên quan đến những tiến bộ này không được công khai. Alpaca (Taori et al., 2023) đi theo một con đường khác bằng cách áp dụng phương pháp self-instruct (Wang et al., 2022), tận dụng GPT3.5 (ChatGPT) để tạo dữ liệu huấn luyện. Vicuna (Chiang et al., 2023) sử dụng các cuộc trò chuyện được người dùng chia sẻ thu thập từ ShareGPT.com để huấn luyện mô hình của mình. WizardLM (Xu et al., 2023) giới thiệu phương pháp Evol-Instruct, bao gồm việc phát triển dữ liệu hướng dẫn tổng quát hiện có để tạo ra các tập dữ liệu phức tạp và đa dạng hơn. Lấy cảm hứng từ ý tưởng này, công việc của chúng tôi, Code Evol-Instruct, phù hợp với các đặc điểm riêng biệt của lĩnh vực lập trình, là phương pháp tinh chỉnh hướng dẫn đầu tiên được thiết kế rõ ràng để tăng cường Code LLMs.

3 WIZARD CODER : CODE LLM MÃ NGUỒN MỞ SOTA
Trong phần này, chúng tôi trình bày chi tiết về phương pháp luận của WizardCoder. Như được minh họa trong Hình 1, trước tiên chúng tôi áp dụng Code Evol-Instruct để lặp lại việc phát triển tập dữ liệu Code Alpaca. Tiếp theo, chúng tôi tinh chỉnh các Code LLMs được tiền huấn luyện với dữ liệu đã phát triển.

3.1 CODE EVOL-INSTRUCT
Lấy cảm hứng từ phương pháp Evol-Instruct được đề xuất bởi WizardLM Xu et al. (2023), công việc này cố gắng tự động tăng cường độ phức tạp của hướng dẫn mã, từ đó cải thiện hiệu quả tinh chỉnh của Code LLMs. Khác với lĩnh vực tổng quát, các phương pháp của chúng tôi được thiết kế tỉ mỉ để phù hợp với các đặc điểm cụ thể của lĩnh vực lập trình. Quá trình tiến hóa giới thiệu các tính năng sau:

1. Heuristics phù hợp với các tính năng nhiệm vụ lập trình trên các nền tảng như LeetCode, tăng cường tính phức tạp một cách chiến lược của các nhiệm vụ lập trình để nâng cao khả năng của mô hình.
2. Giới thiệu mã lỗi như một mẫu đối kháng, lấy cảm hứng từ nghiên cứu trước đây về tấn công các mô hình mã được tiền huấn luyện Yang et al. (2022); Jha & Reddy (2022), thêm một phương pháp mới và hiệu quả để leo thang độ phức tạp nhiệm vụ.
3. Giới thiệu một heuristic nhấn mạnh độ phức tạp thời gian và không gian tận dụng insights từ các nghiên cứu trước đây Madaan et al. (2023), cung cấp một con đường có giá trị để cải thiện độ phức tạp nhiệm vụ.

Vậy, mẫu prompt tiến hóa mã như sau:

Prompt cho Code Evol-Instruct
Xin hãy tăng độ khó của câu hỏi kiểm tra lập trình đã cho một chút.
Bạn có thể tăng độ khó bằng cách sử dụng, nhưng không giới hạn ở, các phương pháp sau:
{method }
{question }

Ở đây, {question} đại diện cho hướng dẫn mã hiện tại đang chờ tiến hóa, và {method} là loại tiến hóa. Năm loại chúng tôi sử dụng được liệt kê như sau:
4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Phương pháp Heuristic Tiến hóa Mã
Thêm các ràng buộc và yêu cầu mới vào vấn đề gốc, thêm khoảng 10 từ bổ sung.
Thay thế một yêu cầu thường được sử dụng trong nhiệm vụ lập trình bằng một yêu cầu ít phổ biến và cụ thể hơn.
Nếu vấn đề gốc có thể được giải quyết chỉ với một vài bước logic, xin hãy thêm nhiều bước lý luận hơn.
Cung cấp một đoạn mã lỗi như một tham khảo để tăng sự misdirection.
Đề xuất các yêu cầu về độ phức tạp thời gian hoặc không gian cao hơn, nhưng xin hãy kiềm chế không làm điều này quá thường xuyên.

3.2 HUẤN LUYỆN WizardCoder
Chúng tôi sử dụng quy trình sau để huấn luyện WizardCoder. Ban đầu, chúng tôi sử dụng StarCoder 15B (Li et al., 2023a) và CodeLlama-34B-Python (Rozì`ere et al., 2023) làm nền tảng và tiến hành tinh chỉnh chúng sử dụng tập huấn luyện tuân theo hướng dẫn mã, được phát triển thông qua Code Evol-Instruct. Định dạng prompt cho tinh chỉnh được nêu như sau:

Prompt cho Định dạng Tinh chỉnh
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:
{instruction }
### Phản hồi:

Để xây dựng tập dữ liệu huấn luyện, chúng tôi khởi tạo nó với tập dữ liệu tuân theo hướng dẫn được gọi là Code Alpaca3. Chúng tôi lặp lại việc sử dụng kỹ thuật Code Evol-Instruct trên tập dữ liệu này bao gồm khoảng 20k mẫu để tạo ra dữ liệu đã phát triển. Sau mỗi vòng phát triển dữ liệu, chúng tôi hợp nhất dữ liệu đã phát triển từ tất cả các vòng trước đó với tập dữ liệu gốc để tinh chỉnh Code LLMs. Một tập dev bên ngoài phục vụ như Evol Stop được kiểm soát. Nếu hiệu suất giảm, chúng tôi dừng tiến hóa. Trong Phụ lục C, chúng tôi nêu phương pháp được sử dụng để ngăn chặn rò rỉ dữ liệu. Ngoài ra, Phụ lục D trình bày một số ví dụ đã phát triển để tham khảo.

4 THỰC NGHIỆM
Phần này bắt đầu bằng việc cung cấp một cái nhìn tổng quan toàn diện về các mô hình baseline trong thí nghiệm của chúng tôi. Tiếp theo, chúng tôi trình bày hiệu suất của các mô hình của chúng tôi trên năm điểm chuẩn sinh mã: HumanEval (Chen et al., 2021b), HumanEval+ (Liu et al., 2023), MBPP (Austin et al., 2021), DS-1000 (Lai et al., 2022) và MultiPL-E (Cassano et al., 2022).

4.1 BASELINES
Mô hình Mã nguồn Đóng. Nhiều công ty công nghệ đã thành công trong việc phát triển các LLMs có trình độ cao trong khi chọn không công khai phát hành chúng. Những mô hình này được gọi là mô hình mã nguồn đóng. Cho nghiên cứu của chúng tôi, chúng tôi kết hợp một số lượng đáng kể những mô hình này làm baseline. Cụ thể, các baseline của chúng tôi bao gồm: (i) GPT3.5(ChatGPT)&GPT4 của OpenAI (OpenAI, 2023), Code-Davinci-002 (Microsoft, 2023), Code-Cushman-001 (Microsoft, 2023), và

3https://github.com/sahil280114/codealpaca
5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 2: Tỷ lệ phần trăm pass rates trên HumanEval và HumanEval+ với một lần thử duy nhất (greedy decoding), theo bảng xếp hạng EvalPlus (Liu et al., 2023).

Codex (Chen et al., 2021a); (ii) Bard, PaLM 2 của Google (Anil et al., 2023), PaLM (Chowdhery et al., 2022), và LaMDA (Thoppilan et al., 2022); (iii) AlphaCode của Google DeepMind (Li et al., 2022);(iv) Claude của Anthropic; (v) PanguCoder2 của Huawei (Shen et al., 2023); và (vi) Unnatural-CodeLlama-34B của Meta (Rozì`ere et al., 2023).

Mô hình Mã nguồn Mở. Một số LLMs mã nguồn mở (OSS) đã được cung cấp cho cộng đồng AI, mặc dù hiệu suất của chúng thường tụt hậu so với các mô hình mã nguồn đóng rất nhiều. Như một phần của nghiên cứu, chúng tôi kết hợp một số lượng đáng kể những mô hình mã nguồn mở này làm baseline. Các baseline của chúng tôi bao gồm những mô hình sau: InCoderFried et al. (2022), StarCoder và StarCoder-Plus (Li et al., 2023a), LLaMa1&2 (Touvron et al., 2023a;b), CodeGen (Nijkamp et al., 2023b), CodeGeeX (Zheng et al., 2023), CodeT5+(Wang et al., 2023), và CodeLlama (Rozì`ere et al., 2023). Ngoài ra, chúng tôi cũng bao gồm một số mô hình với tinh chỉnh hướng dẫn, bao gồm CodeLlama-Instruct (Rozì`ere et al., 2023), OctoCoder (Muennighoff et al., 2023), InstructCodeT5+ (Wang et al., 2023), Instruct-Codegen-16B,4Guanaco-65B (Dettmers et al., 2023), Falcon-40B-Instruct (Penedo et al., 2023) và Vicuna-13B (Chiang et al., 2023). Chi tiết thêm có thể được tìm thấy trong Phụ lục B.

4.2 CHI TIẾT TRIỂN KHAI
StarCoder và CodeLlama-34B-Python phục vụ làm mô hình nền tảng cơ bản của chúng tôi. gpt3.5-turbo của OpenAI được sử dụng để phát triển tập dữ liệu và tạo phản hồi. Tập dữ liệu đã phát triển bao gồm khoảng 78k mẫu. Để tinh chỉnh các mô hình cơ bản, chúng tôi sử dụng các cấu hình cụ thể, bao gồm batch size là 512, sequence length là 2048, 200 bước tinh chỉnh, 30 bước warmup, learning rate là 2e-5, một Cosine learning rate scheduler, và fp16 mixed precision.

4.3 ĐÁNH GIÁ TRÊN HUMAN EVAL, HUMAN EVAL+,VÀ MBPP
HumanEval (Chen et al., 2021b), HumanEval+ (Liu et al., 2023), và MBPP (Austin et al., 2021) là những điểm chuẩn chính trong lĩnh vực Code LLM, có các vấn đề lập trình Python đa dạng được xác thực bằng test cases. HumanEval bao gồm 164 vấn đề với trung bình 9.6 test cases mỗi vấn đề. HumanEval+ mở rộng test cases đáng kể đến trung bình 774.8 mỗi vấn đề. Ngược lại, MBPP cung cấp 500 vấn đề lập trình test với ba automated test cases mỗi vấn đề.5

So sánh với Mô hình Mã nguồn Đóng. Theo cùng thiết lập của bảng xếp hạng EvalPlus (Liu et al., 2023). Trong Hình 2, chúng tôi so sánh các mô hình WizardCoder của chúng tôi với các mô hình mã nguồn đóng, như GPT4, Claude, và Bard trên bảng xếp hạng này. Đáng chú ý, tất cả các mô hình tạo mã

4https://huggingface.co/sahil2801/instruct-codegen-16B
5Để so sánh công bằng, chúng tôi trình bày kết quả cho GPT3.5(ChatGPT)&GPT4 sử dụng Eval-Plus với các API mới nhất của OpenAI (Liu et al., 2023) (Hình 2) và báo cáo của OpenAI (OpenAI, 2023) (Bảng 1). Chi tiết định dạng prompt có trong Phụ lục A.
6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 1: Kết quả pass@1(%) trên HumanEval và MBPP. Chúng tôi tuân theo các công trình trước đây (Chen et al., 2021b) để tạo ra n=200 mẫu để ước tính điểm pass@1 của các mô hình WizardCoder với cùng một tập hợp hyperparameters: temperate=0.2, và top p=0.95. *: kết quả tái tạo của chúng tôi.

Model Params HumanEval MBPP
Mô hình mã nguồn đóng
LaMDA (Thoppilan et al., 2022) 137B 14.0 -
AlphaCode (Li et al., 2022) 1.1B 17.1 -
PaLM (Chowdhery et al., 2022) 540B 26.2 36.8
PaLM-Coder (Chowdhery et al., 2022) 540B 36.0 47.0
PaLM 2-S (Anil et al., 2023) Unknown 37.6 50.0
Codex (Chen et al., 2021a) 2.5B 21.4 -
Codex (Chen et al., 2021a) 12B 28.8 -
Code-Cushman-001 (Microsoft, 2023) Unknown 33.5 45.9
Code-Davinci-002 (Microsoft, 2023) Unknown 47.0 58.1
GPT-3.5 (ChatGPT) (OpenAI, 2023) Unknown 48.1 52.2
PanguCoder2 (Shen et al., 2023) 15B 61.6 -
Unnatural-CodeLlama (Rozì`ere et al., 2023) 34B 62.2 61.2
GPT-4 (OpenAI, 2023) Unknown 67.0 -
Mô hình mã nguồn mở
Llama (Touvron et al., 2023a) 65B 23.7 37.7
Llama2 (Touvron et al., 2023b) 70B 29.9 45.0
CodeGen-Mono (Nijkamp et al., 2023b) 16B 29.3 35.3
CodeGeeX (Zheng et al., 2023) 13B 22.9 24.4
StarCoder (Li et al., 2023a) 15B 33.6 43.6∗
CodeT5+ (Wang et al., 2023) 16B 30.9 -
InstructCodeT5+ (Wang et al., 2023) 16B 35.0 -
OctoCoder (Muennighoff et al., 2023) 15B 46.2 -
CodeLlama (Rozì`ere et al., 2023) 34B 48.8 55.0
CodeLlama-Python (Rozì`ere et al., 2023) 34B 53.7 56.2
CodeLlama-Instruct (Rozì`ere et al., 2023) 34B 41.5 57.0
WizardCoder 15B 57.3 51.8
WizardCoder 34B 71.5 61.2

giải pháp cho mỗi vấn đề sử dụng một lần thử duy nhất, và tỷ lệ pass rate phần trăm được báo cáo. Để duy trì tính nhất quán, chúng tôi sử dụng cùng thiết lập thực nghiệm bằng cách tạo ra các câu trả lời sử dụng greedy decoding và đánh giá các mô hình WizardCoder sử dụng các mã đánh giá được cung cấp.

Như được mô tả trong Hình 2, WizardCoder 34B của chúng tôi đạt vị trí thứ hai trong điểm chuẩn này, vượt qua GPT3.5 (ChatGPT, 64.6 vs. 63.4) trên HumanEval+. Phiên bản 15B của chúng tôi vượt trội hơn Claude-Plus (59.8 vs. 53.0) và Bard (59.8 vs. 44.5). Hơn nữa, các mô hình WizardCoder của chúng tôi thể hiện ưu thế đáng chú ý so với các LLMs mã nguồn mở khác trải qua tinh chỉnh hướng dẫn.

So sánh với Mô hình Mã nguồn Mở. Trong Bảng 1, chúng tôi tiến hành so sánh toàn diện WizardCoder của chúng tôi với các mô hình mã nguồn mở khác trên các điểm chuẩn HumanEval và MBPP. Trái với kết quả được trình bày trong Hình 2, chúng tôi tuân thủ phương pháp được nêu trong các nghiên cứu trước đây Chen et al. (2021b) bằng cách tạo ra n mẫu cho mỗi vấn đề để ước tính điểm pass@1. Các phát hiện được trình bày trong Bảng 1 rõ ràng chứng minh rằng WizardCoder của chúng tôi thể hiện lợi thế hiệu suất đáng kể so với tất cả các mô hình mã nguồn mở.

4.4 ĐÁNH GIÁ TRÊN LẬP TRÌNH ĐA NGÔN NGỮ
Chúng tôi bao gồm kết quả đánh giá toàn diện trên 8 ngôn ngữ lập trình riêng biệt trên các điểm chuẩn MultiPL-E. Những ngôn ngữ này bao gồm Java, JavaScript, C++, PHP, R, Julia, Swift, và Rust. Kết quả thực nghiệm, như được trình bày trong Bảng 2, rõ ràng thể hiện hiệu suất vượt trội của các mô hình WizardCoder của chúng tôi trên tất cả các ngôn ngữ lập trình được đánh giá, vượt qua các SOTA Code LLMs mã nguồn mở. Điều này nhấn mạnh hiệu quả của phương pháp Code Evol-Instruct của chúng tôi.

4.5 ĐÁNH GIÁ TRÊN DS-1000
Điểm chuẩn DS-1000 Lai et al. (2022) bao gồm 1k workflow khoa học dữ liệu riêng biệt trải rộng 7 thư viện. Nó đánh giá hiệu suất của sinh mã so với test cases và hỗ trợ hai chế độ đánh giá: completion và insertion. Trong thí nghiệm của chúng tôi, chúng tôi chỉ báo cáo điểm insertion cho

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
các mô hình hỗ trợ. Trong Bảng 3, chúng tôi trình bày kết quả pass@1 (n=40) cho mỗi thư viện, cùng với điểm tổng thể.6 Dựa trên những kết quả này, kết luận của chúng tôi là WizardCoder thể hiện ưu thế đáng kể so với tất cả các mô hình khác khi giải quyết các vấn đề khoa học dữ liệu trên điểm chuẩn DS-1000.

Bảng 2: Kết quả pass@1(%) trên 8 ngôn ngữ lập trình khác nhau trên các điểm chuẩn MultiPL-E (Cassano et al., 2022). Tất cả các mô hình được đánh giá với cùng một tập hợp hyperparameters: temperature=0.2, top p=0.95, max length=512, và n=50.

Model Params Java Js CPP PHP R Julia Swift Rust
CodeGen-Multi 16B 22.2 19.2 21.0 8.4 6.5 0 1.3 4.2
CodeGeeX 13B 19.1 16.9 16.9 13.5 3.9 0.3 7.3 7.9
Code-Cushman-001 - 31.9 31.3 30.6 29.0 11.0 1.5 22.1 25.2
StarCoderBase 15B 28.5 31.7 30.6 26.8 10.2 21.1 16.7 24.5
StarCoder 15B 30.2 30.8 31.6 26.1 15.5 23.0 22.7 21.8
CodeLlama 34B 40.2 41.7 41.4 40.4 22.7 31.4 35.3 38.7
CodeLlama-Python 34B 39.5 44.7 39.1 39.8 22.4 31.4 34.3 39.7
CodeLlama-Instruct 34B 41.5 45.9 41.5 37.0 24.3 32.7 37.6 39.3
WizardCoder 15B 35.8 41.9 39.0 39.3 33.5 34.0 33.7 27.1
WizardCoder 34B 44.9 55.3 47.2 47.2 39.8 41.5 44.3 46.2

Bảng 3: Hiệu suất của WizardCoder 15B và các mô hình baseline trên DS-1000. Tất cả các mô hình được đánh giá với cùng một tập hợp hyperparameters: temperature=0.2, top p=0.5, max length=1024. Điểm là độ chính xác pass@1 trung bình trên 40 mẫu. Nhiệm vụ Matplotlib (plt) không có ngữ cảnh phù hợp, vì vậy điểm insertion và completion giống hệt nhau.

Format Model plt np pd py scp sk tf All
# of problems: 155 220 291 68 106 115 45 1,000
Completion InCoder-6B 28.3 4.4 3.1 4.4 2.8 2.8 3.8 7.4
Completion CodeGen-mono 31.7 10.9 3.4 7.0 9.0 10.8 15.2 11.7
Completion Code-Cushman-001 40.7 21.8 7.9 12.4 11.3 18.0 12.2 18.1
Completion StarCoder 51.7 29.7 11.4 21.4 20.2 29.5 24.5 26.0
Completion WizardCoder 55.2 33.6 16.7 26.2 24.2 24.9 26.7 29.2
Insertion InCoder-6B 28.3 4.6 2.9 4.4 2.8 3.1 7.8 7.5
Insertion StarCoder 51.7 30.8 10.3 21.0 20.2 27.4 20.0 25.4
Insertion WizardCoder 55.2 35.1 20.4 30.4 28.9 32.3 37.8 32.8

5 PHÂN TÍCH

Bảng 4: Các mô hình thực thi tiến hóa khác nhau.
Base Model Evol Model Pass@1
StarCoder-15B GPT-4 62.2
StarCoder-15B GPT-3.5 59.8
StarCoder-15B CodeLlama 55.5
CodeLlama-34B GPT-4 73.8
CodeLlama-34B GPT-3.5 73.2
CodeLlama-34B CodeLlama-34B 70.1

Mô hình Tiến hóa và Vòng. Trong Bảng 4, GPT-4 thay thế GPT-3.5 cho các vòng tiến hóa, tăng điểm HumanEval Pass@1 lên 73.8 (34B) và 62.2 (15B). Sử dụng OSS CodeLlama-Instruct-34B cũng chứng minh hiệu quả, mang lại điểm 70.1 (34B) và 55.5 (15B). Mặc dù hiệu suất lập trình vượt trội của GPT-4 (88.4 vs. 73.2), lợi ích trong các vòng tiến hóa không tỷ lệ thuận (73.8 vs. 73.2). Ngược lại, hiệu suất yếu hơn của CodeLlama thu hẹp khi sử dụng Code Evol-Instruct (73.2 vs. 70.1), làm nổi bật vai trò quan trọng của nó. Chi tiết thí nghiệm thêm được liệt kê trong Phụ lục E. Ngoài ra, Hình 3 trình bày kết quả cho các vòng tiến hóa dữ liệu khác nhau. Tất cả các mô hình được tinh chỉnh với 200 bước. Do kích thước hạn chế của tập dev của MBPP, chúng tôi hợp nhất tập huấn luyện và tập dev, tạo thành tập dev MBPP-400. Các thí nghiệm tiết lộ rằng điểm pass@1 cao nhất trên cả tập dev MBPP-400 và HumanEval đạt được sau ba vòng tiến hóa.

6 Cho rằng điểm chuẩn này và các mã đánh giá của nó không được thiết kế cho các mô hình được tinh chỉnh hướng dẫn, chúng tôi gặp những thách thức đáng kể trong việc căn chỉnh mô hình 34B của chúng tôi với framework này. Hơn nữa, mô hình cơ bản Codellama-34B không hỗ trợ chèn mã. Do đó, chúng tôi chỉ bao gồm kết quả mô hình 15B của chúng tôi.
8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
(a) Hiệu suất Pass@1 trên tập dev MBPP-400.
 (b) Hiệu suất Pass@1 trên HumanEval.
Hình 3: Tác động của số lượng vòng tiến hóa dữ liệu.

Độ phức tạp và Số lượng. Trong khi hiệu suất nâng cao nhờ phương pháp Code Evol-Instruct của chúng tôi đã rõ ràng trong các thí nghiệm trước đây, vẫn còn một câu hỏi mở về việc liệu lợi ích hiệu suất này là kết quả của việc tăng số lượng mẫu hay token. Trong quá trình tiến hóa, mỗi vòng bao gồm thêm mẫu, và việc giới thiệu các hướng dẫn phức tạp hơn chắc chắn dẫn đến tăng token trong dữ liệu huấn luyện. Để giải quyết câu hỏi này, chúng tôi tinh chỉnh các mô hình chỉ sử dụng dữ liệu vòng cụ thể riêng biệt từ đầu với số lượng mẫu tương tự (phần trên) hoặc token (phần dưới) trong Bảng 5.

Bảng 5: Phân tích về việc liệu lợi ích hiệu suất có đến từ nhiều token hơn.
Evol #Samples Pass@1
Round 0 20.0k 45.7
Round 1 18.8k 56.1
Round 2 19.7k 53.0
Round 3 19.3k 54.3
Round 4 19.0k 51.2
Evol #Tokens Pass@1
Round 0 2.3M 44.5
Round 1 2.3M 51.8
Round 2 2.3M 52.4
Round 3 2.3M 50.0
Round 4 2.3M 49.4

Khi mỗi vòng chứa cùng số lượng mẫu hoặc token, các mô hình được huấn luyện với dữ liệu gốc vẫn tụt hậu so với các vòng đã tiến hóa. Hơn nữa, việc kết hợp dữ liệu từ các vòng khác nhau dẫn đến hiệu suất tốt nhất. Những kết quả này cho thấy nguồn chính của lợi ích thực sự do phương pháp Code Evol-Instruct của chúng tôi, chứ không chỉ là tăng mẫu hoặc token.

Độ phức tạp và Tương đồng. Ngoài phân tích số lượng, chúng tôi cũng điều tra xem liệu tiến hóa có dẫn đến việc bao gồm dữ liệu tương đồng hơn với tập test. Để giải quyết điều này, chúng tôi thực hiện phân tích tập test HumanEval. Chúng tôi sử dụng các mẫu test như truy vấn để truy xuất mẫu top-1 từ dữ liệu huấn luyện của mỗi vòng tiến hóa, sử dụng mô hình embeddings SOTA, gte-large (Li et al., 2023b). Ngoài ra, chúng tôi sử dụng GPT4 để cung cấp điểm tương đồng trung bình giữa tập test và các mẫu top-1 được truy xuất. Chi tiết được hiển thị trong Phụ lục C.

Hình 4: Điểm tương đồng trung bình giữa các mẫu HumanEval và dữ liệu top-1 được truy xuất, từ 1 (hoàn toàn khác biệt) đến 10 (giống hệt).

Hình 4 minh họa rằng quá trình tiến hóa không mang lại điểm tương đồng cao hơn. Hơn nữa, điểm tương đồng trên tất cả các vòng vẫn tương đối thấp. Những phát hiện này chỉ ra rằng nguồn chính của lợi ích hiệu suất là việc giới thiệu dữ liệu phức tạp hơn.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Bài báo này giới thiệu các mô hình WizardCoder, các Code LLMs được tinh chỉnh Code Evol-Instruct. Kết quả thực nghiệm chứng minh rằng các mô hình WizardCoder đạt được hiệu suất SOTA vượt qua tất cả các Code LLMs mã nguồn mở hiện có trên năm điểm chuẩn sinh mã được công nhận rộng rãi: HumanEval, HumanEval+, MBPP, DS-1000 và MultiPL-E. Đáng chú ý, mô hình WizardCoder 15B vượt qua một số LLMs mã nguồn đóng nổi tiếng, như Claude và Bard. Ngoài ra, WizardCoder 34B đạt được điểm HumanEval có thể so sánh với GPT3.5 (ChatGPT) và vượt qua nó trên điểm chuẩn HumanEval+. Hơn nữa, phân tích của chúng tôi nhấn mạnh vai trò then chốt của độ phức tạp hướng dẫn trong việc nâng cao hiệu suất. Đối với công việc tương lai, như được mô tả trong Hình 2, mô hình của chúng tôi vẫn tụt hậu đáng kể so với LLM SOTA, GPT4. Do đó, công việc tương lai sẽ tăng cường thêm hiệu suất của mô hình.
9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
LỜI CẢM ơN
Công việc này được hỗ trợ một phần bởi Quỹ Khoa học Trẻ Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62206233) và Hong Kong RGC ECS (Số 22200722).

TÀI LIỆU THAM KHẢO
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hern ́andez ́Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl ́ement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D ́ıaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, và et al. Palm 2 technical report. CoRR , abs/2305.10403, 2023. doi: 10.48550/arXiv.2305.10403. URL https://doi.org/10.48550/arXiv.2305.10403 .

Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, và Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL https://openreview.net/forum?id= Vzh1BFUCiIX .

Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le, và Charles Sutton. Program synthesis with large language models. CoRR , abs/2108.07732, 2021. URL https://arxiv.org/abs/ 2108.07732 .

Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, và Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model. CoRR , abs/2204.06745, 2022. doi: 10.48550/arXiv. 2204.06745. URL https://doi.org/10.48550/arXiv.2204.06745 .

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. Trong Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, và Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .

Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, và Abhinav Jangda. A scalable and extensible approach to benchmarking nl2code for 18 programming languages. CoRR , abs/2208.08227, 2022. doi: 10.48550/arXiv.2208. 08227. URL https://doi.org/10.48550/arXiv.2208.08227 .

Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https: //github.com/sahil280114/codealpaca , 2023.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond ́e de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021a. URL https://arxiv. org/abs/2107.03374 .

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond ́e de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021b. URL https://arxiv. org/abs/2107.03374 .

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //vicuna.lmsys.org .

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR , abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311 .

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, và Jason Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022. doi: 10.48550/arXiv.2210.11416. URL https://doi.org/10.48550/arXiv.2210.11416 .

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. CoRR , abs/2305.14314, 2023. doi: 10.48550/arXiv.2305.14314. URL https: //doi.org/10.48550/arXiv.2305.14314 .

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. Incoder: A generative model for code infilling and synthesis. CoRR , abs/2204.05999, 2022. doi: 10.48550/arXiv.2204.05999. URL https: //doi.org/10.48550/arXiv.2204.05999 .
11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, và Laurent Sifre. Training compute-optimal large language models. CoRR , abs/2203.15556, 2022. doi: 10.48550/arXiv.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556 .

Akshita Jha và Chandan K. Reddy. Codeattack: Code-based adversarial attacks for pre-trained programming language models. Trong AAAI Conference on Artificial Intelligence , 2022. URL https://api.semanticscholar.org/CorpusID:249240370 .

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, và Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. Trong Trevor Cohn, Yulan He, và Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 , volume EMNLP 2020 của Findings of ACL , pp. 1896– 1907. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.171. URLhttps://doi.org/10.18653/v1/2020.findings-emnlp.171 .

Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida I. Wang, và Tao Yu. DS-1000: A natural and reliable benchmark for data science code generation. CoRR , abs/2211.11501, 2022. doi: 10.48550/arXiv.2211.11501. URL https://doi.org/10.48550/arXiv.2211.11501 .

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 , 2023a.

Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R ́emi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. Competition-level code generation with alphacode. CoRR , abs/2203.07814, 2022. doi: 10.48550/arXiv.2203.07814. URLhttps://doi.org/10.48550/arXiv.2203.07814 .

Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, và Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. CoRR , abs/2308.03281, 2023b. doi: 10.48550/arXiv.2308.03281. URL https://doi.org/10.48550/arXiv.2308.03281 .

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, và Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. CoRR , abs/2305.01210, 2023. doi: 10.48550/arXiv.2305.01210. URL https://doi.org/10. 48550/arXiv.2305.01210 .

Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, và Amir Yazdanbakhsh. Learning performance-improving code edits. ArXiv , abs/2302.07867, 2023. URL https://api.semanticscholar.org/CorpusID: 256868633 .

Microsoft. Azure openai service models. https://learn.microsoft.com/en-us/ azure/cognitive-services/openai/concepts/models , 2023.

Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, và Shayne Longpre. Octopack: Instruction tuning code large language models. CoRR , abs/2308.07124, 2023. doi: 10.48550/arXiv.2308.07124. URL https://doi.org/10.48550/arXiv.2308.07124 .

Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, và Yingbo Zhou. Codegen2: Lessons for training llms on programming and natural languages. CoRR , abs/2305.02309, 2023a. doi: 10.48550/arXiv.2305.02309. URL https://doi.org/10.48550/arXiv. 2305.02309 .
12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. Trong The Eleventh International Conference on Learning Representations , 2023b. URL https://openreview.net/forum?id=iaYcJKpY2B_ .

OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URLhttps://doi.org/10.48550/arXiv.2303.08774 .

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, và Ryan Lowe. Training language models to follow instructions with human feedback. Trong NeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html .

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. CoRR , abs/2306.01116, 2023. doi: 10.48550/arXiv.2306.01116. URL https: //doi.org/10.48550/arXiv.2306.01116 .

Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, và Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. CoRR , abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446 .

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21:140:1–140:67, 2020. URL http://jmlr.org/ papers/v21/20-074.html .

Baptiste Rozi ̀ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J ́er ́emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D ́efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, và Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR , abs/2308.12950, 2023. doi: 10.48550/arXiv.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950 .

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F ́evry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, và Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URLhttps://openreview.net/forum?id=9Vrb9D0WI4 .
13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, và Qianxiang Wang. Pangu-coder2: Boosting large language models for code with ranking feedback. CoRR , abs/2307.14936, 2023. doi: 10.48550/arXiv.2307.14936. URL https://doi.org/10.48550/arXiv.2307.14936 .

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca , 2023.

Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, và Donald Metzler. Unifying language learning paradigms. CoRR , abs/2205.05131, 2022. doi: 10.48550/arXiv.2205.05131. URL https://doi.org/10. 48550/arXiv.2205.05131 .

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, và Quoc Le. Lamda: Language models for dialog applications. CoRR , abs/2201.08239, 2022. URL https://arxiv.org/abs/2201. 08239 .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ́ee Lacroix, Baptiste Rozi ̀ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur ́elien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language models. CoRR , abs/2302.13971, 2023a. doi: 10.48550/arXiv.2302.13971. URL https://doi. org/10.48550/arXiv.2302.13971 .

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur ́elien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR , abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/ 10.48550/arXiv.2307.09288 .

Ben Wang và Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 , 2022.

Yue Wang, Weishi Wang, Shafiq R. Joty, và Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , pp. 8696–8708. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.685. URL https://doi.org/10. 18653/v1/2021.emnlp-main.685 .
14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, và Steven C. H. Hoi. Codet5+: Open code large language models for code understanding and generation. CoRR , abs/2305.07922, 2023. doi: 10.48550/arXiv.2305.07922. URL https://doi.org/10. 48550/arXiv.2305.07922 .

Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V . Le. Finetuned language models are zero-shot learners. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL https://openreview.net/forum?id= gEZrGCozdqR .

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 , 2023.

Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, và Zhilin Yang. Zeroprompt: Scaling prompt-based pretraining to 1, 000 tasks improves zero-shot generalization. Trong Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7- 11, 2022 , pp. 4235–4252. Association for Computational Linguistics, 2022. URL https:// aclanthology.org/2022.findings-emnlp.312 .

Zhou Yang, Jieke Shi, Junda He, và David Lo. Natural attack for pre-trained models of code. 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE) , pp. 1482–1493, 2022. URLhttps://api.semanticscholar.org/CorpusID:246210250 .

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, và Jie Tang. GLM-130B: an open bilingual pre-trained model. CoRR , abs/2210.02414, 2022. doi: 10.48550/arXiv.2210.02414. URL https://doi.org/10. 48550/arXiv.2210.02414 .

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. OPT: open pre-trained transformer language models. CoRR , abs/2205.01068, 2022. doi: 10.48550/ arXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068 .

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, và Jie Tang. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. CoRR , abs/2303.17568, 2023. doi: 10.48550/arXiv.2303.17568. URL https://doi.org/10.48550/arXiv.2303.17568 .
15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
A ĐỊNH DẠNG PROMPT
Trong phần này, chúng tôi bao gồm prompt để đánh giá trên các nhiệm vụ khác nhau.

Zero-Shot Prompt để Đánh giá trên HumanEval và HumanEval+
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:
Tạo một script Python cho vấn đề này:
{Question }
### Phản hồi:

Three-Shot Prompt để Đánh giá trên MBPP
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:
Tạo một script Python cho vấn đề này:
{Question }
{Test Example 1 }
{Test Example 2 }
{Test Example 3 }
### Phản hồi:

Zero-Shot Prompt để Đánh giá trên DS-1000 (Completion)
Dưới đây là một hướng dẫn mô tả một nhiệm vụ, được ghép nối với một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành yêu cầu một cách thích hợp.
### Hướng dẫn:
{Question }
Hoàn thành mã Python trong "...".
### Phản hồi:

Trong trường hợp DS-1000 (Insertion), việc tuân thủ các đặc tả của điểm chuẩn đòi hỏi việc sử dụng ký hiệu chèn đặc biệt của StarCoder. Do đó, chúng tôi thấy cần thiết phải căn chỉnh với cùng định dạng prompt được sử dụng bởi StarCoder cho điểm chuẩn cụ thể này.

Đối với điểm chuẩn MultiPL-E, chúng tôi nhận ra sự cần thiết phải căn chỉnh với các mã đánh giá được cung cấp bởi bigcode-evaluation-harness.7 Do đó, chúng tôi chọn áp dụng cùng định dạng prompt được sử dụng bởi StarCoder.

B CHI TIẾT BASELINES
Chúng tôi bao gồm một số lượng lớn mô hình làm baseline. Đối với GPT3.5 (ChatGPT)&GPT4. kết quả của chúng được lấy từ báo cáo của GPT4 và EvalPlus. Kết quả của Code-Davinci-002, Code-Cushman-001, Codex, PaLM, PaLM 2, LaMDA, AlpahaCode, Incoder, StarCoder, LLaMa, CodeGen, CodeGeeX, CodeT5+, và InstructCodeT5+ là từ bài báo của StarCoder hoặc CodeT5+. Kết quả của Bard được đánh giá với API của Google. Kết quả của Claude được đánh giá với API của Anthropic. Kết quả

7https://github.com/bigcode-project/bigcode-evaluation-harness
16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
của Instruct-Codegen-16B, Guanaco-65B, Falcon-40B-Instruct, và Vicuna-13B được đánh giá với các checkpoint mã nguồn mở. Kết quả của CodeLlama-Series là từ bài báo CodeLlama. Kết quả của OctoCoder là từ bài báo của nó. Kết quả của PanguCoder2 cũng từ bài báo của nó.

Điểm MBPP của StarCoder khác với trong bài báo gốc của nó. Thông qua liên lạc cá nhân, chúng tôi được thông báo rằng StarCoder được đánh giá sử dụng phiên bản được làm sạch và nhỏ hơn của MBPP, chỉ bao gồm 397 vấn đề, ít hơn đáng kể so với các điểm chuẩn MBPP gốc (500). Do đó, chúng tôi tiến hành đánh giá lại StarCoder sử dụng MBPP gốc.

C KIỂM TRA TƯƠNG ĐỒNG VÀ LỌC DỮ LIỆU
Các định dạng prompt để tính điểm tương đồng như sau:

System Prompt để Kiểm tra Tương đồng
Nhiệm vụ của bạn là đánh giá sự tương đồng của hai nhiệm vụ lập trình đã cho. Vui lòng xem xét cẩn thận hai nhiệm vụ lập trình, chú ý đến sự chồng chéo trong tên hàm, cấu trúc mã, chủ đề, và nội dung. Khi bạn đã xem xét cẩn thận cả hai nhiệm vụ lập trình, hãy cung cấp điểm tương đồng giữa hai nhiệm vụ lập trình này. Điểm nên trong khoảng từ 1 đến 10 (1: nhiệm vụ lập trình hoàn toàn khác biệt; 10: nhiệm vụ lập trình giống hệt). Bạn chỉ cần cung cấp điểm của mình. Định dạng phản hồi là:
Score: '...'

User Input để Kiểm tra Tương đồng
# Task1
{task1 }
# Task2
{task2 }

Để ngăn chặn triệt để rò rỉ dữ liệu từ các tập dữ liệu test vào tập dữ liệu huấn luyện, chúng tôi thực hiện một bước lọc dữ liệu bổ sung. Sử dụng mô hình embeddings SOTA, gte-large, chúng tôi coi tất cả các mẫu test như truy vấn để trích xuất 5 mẫu hàng đầu từ dữ liệu huấn luyện. Sau đó, GPT-4 được sử dụng để đánh giá sự tương đồng giữa các mẫu được truy xuất và mẫu test. Nhiệm vụ cho GPT-4 được đơn giản hóa thành quyết định nhị phân—hoặc "có" hoặc "không" chỉ ra một kết quả khớp. Trong trường hợp kết quả khớp tích cực, mẫu được loại trừ khỏi dữ liệu huấn luyện.

D VÍ DỤ EVOL
Trong phần này, chúng tôi trình bày một số ví dụ đã phát triển để làm rõ ảnh hưởng được tạo ra bởi Code Evol-Instruct của chúng tôi.

Ví dụ 1:
• Vòng 0: Viết một truy vấn MongoDB để chọn tất cả các tài liệu trong một collection nơi trường 'category' là 'clothes'.
• Vòng 1: Viết một truy vấn MongoDB để chọn tất cả các tài liệu trong một collection nơi trường 'category' là 'clothes' và trường 'brand' không bằng 'Nike'.
• Vòng 2: Viết một truy vấn MongoDB để chọn tất cả các tài liệu trong một collection nơi trường 'category' là 'clothes' và trường 'brand' không bằng 'Nike', và trường 'price' lớn hơn hoặc bằng 100 và nhỏ hơn hoặc bằng 500.
• Vòng 3: Viết một truy vấn MongoDB để chọn tất cả các tài liệu trong một collection nơi trường 'category' là 'clothes' và trường 'brand' không bằng 'Nike', và trường 'price' lớn hơn hoặc bằng 100 và nhỏ hơn hoặc bằng 500, và trường 'color' là 'red' hoặc 'blue'. Ngoài ra, sắp xếp các tài liệu theo thứ tự giảm dần theo trường 'date added' và giới hạn kết quả là 10 tài liệu đầu tiên.
17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Ví dụ 2:
• Vòng 0: Sử dụng Tensorflow, thiết lập một mô hình để dự đoán phân loại của một chữ số viết tay.
• Vòng 1: Sử dụng Tensorflow, thiết lập một mô hình để dự đoán phân loại của một chữ số viết tay với ràng buộc là mô hình phải được huấn luyện chỉ sử dụng 1000 mẫu từ tập dữ liệu MNIST.
• Vòng 2: Sử dụng TensorFlow, thiết lập một mô hình để dự đoán phân loại của một chữ số viết tay với ràng buộc là mô hình phải được huấn luyện chỉ sử dụng 500 mẫu từ tập dữ liệu MNIST. Ngoài ra, triển khai transfer learning để kết hợp các tính năng học được từ một mô hình được tiền huấn luyện.
• Vòng 3: Sử dụng TensorFlow và tập dữ liệu MNIST, phát triển một mạng nơ-ron tích chập (CNN) để phân loại chính xác các chữ số viết tay. Tuy nhiên, trong phiên bản mới này của nhiệm vụ, bạn phải giới hạn huấn luyện tối đa 500 hình ảnh. Hơn nữa, kết hợp transfer learning bằng cách sử dụng một mô hình được tiền huấn luyện để trích xuất tính năng từ dữ liệu và cải thiện độ chính xác của mô hình.

Ví dụ 3:
• Vòng 0: Tạo một class trong Java có thể lưu trữ danh sách học sinh và các khóa học họ đang đăng ký.
• Vòng 1: Tạo một chương trình Java quản lý cơ sở dữ liệu học sinh với các khóa học, bao gồm các tùy chọn để thêm, xóa và tìm kiếm học sinh, cũng như các khóa học cho mỗi học sinh. Chương trình cũng nên có thể hiển thị thống kê về số lượng học sinh và khóa học.
• Vòng 2: Phát triển một chương trình Java quản lý cơ sở dữ liệu học sinh chứa các khóa học và điểm số. Chương trình của bạn nên cho phép người dùng thêm, xóa và tìm kiếm học sinh, cũng như thêm hoặc bỏ các khóa học cho mỗi học sinh. Hơn nữa, nó nên hiển thị thống kê về số lượng học sinh và khóa học, và tính toán điểm trung bình cho mỗi học sinh. Đảm bảo rằng chương trình của bạn triển khai các cơ chế xử lý lỗi thích hợp.
• Vòng 3: Phát triển một chương trình Java để quản lý hệ thống thư viện nơi người dùng có thể check-in, check-out và tìm kiếm sách trong thư viện. Hệ thống nên cho phép người dùng thêm sách mới và xóa sách hiện có khỏi thư viện. Chương trình cũng nên cung cấp thống kê về số lượng sách được check-in và check-out, và tính toán thời gian đọc trung bình cho mỗi cuốn sách. Đảm bảo rằng chương trình của bạn triển khai các cơ chế xử lý lỗi thích hợp và thực thi các ràng buộc toàn vẹn dữ liệu như không cho phép người dùng check-out những cuốn sách đã được check-out.

E SỬ DỤNG CÁC MÔ HÌNH THỰC THI TIẾN HÓA KHÁC NHAU
Chúng tôi khám phá việc sử dụng các mô hình mã nguồn mở (OSS) CodeLlama-Instruct-34B để tạo ra các hướng dẫn đã phát triển. Tuy nhiên, nó thể hiện hiệu suất lập trình tương đối thấp trong việc tạo phản hồi. Để giải quyết điều này, chúng tôi tinh chỉnh nó sử dụng tập dữ liệu code-alpaca của chúng tôi và sử dụng mô hình này để tạo phản hồi.
18
