# 2304.03894.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2304.03894.pdf
# File size: 2121463 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AMULTIFIDELITY APPROACH TO CONTINUAL LEARNING FOR
PHYSICAL SYSTEMS
Amanda Howard, Yucheng Fu, and Panos Stinis
Advanced Computing, Mathematics and Data Division
Pacific Northwest National Laboratory
Richland, WA 99354
amanda.howard@pnnl.gov
February 13, 2024
ABSTRACT
We introduce a novel continual learning method based on multifidelity deep neural networks. This
method learns the correlation between the output of previously trained models and the desired output
of the model on the current training dataset, limiting catastrophic forgetting. On its own the multi-
fidelity continual learning method shows robust results that limit forgetting across several datasets.
Additionally, we show that the multifidelity method can be combined with existing continual learning
methods, including replay and memory aware synapses, to further limit catastrophic forgetting. The
proposed continual learning method is especially suited for physical problems where the data satisfy
the same physical laws on each domain, or for physics-informed neural networks, because in these
cases we expect there to be a strong correlation between the output of the previous model and the
model on the current training domain.
Model A
yA
x1
...
xN
Model B
yB
FB
nlFB
lLinear network
Nonlinear network
Figure 1: Graphical abstract
1 Introduction
In many real world applications of machine learning data is received sequentially or in discrete datasets. When used as
training data new information received about the system requires completely retraining a given neural network. MucharXiv:2304.03894v2  [math.NA]  9 Feb 2024

--- PAGE 2 ---
APREPRINT - FEBRUARY 13, 2024
recent work has focused on how to instead incorporate the newly received training data into the machine learning model
without requiring retraining with the full dataset and without forgetting the previously learned model. This process is
referred to as continual learning [ 1]. One key goal in continual learning is to limit catastrophic forgetting, or abruptly
and completely forgetting the previously trained data.
Many methods have been proposed to limit forgetting in continual learning. In replay (rehearsal), a subset of the
training set from previously trained regions are used in training subsequent models, so the method can limit forgetting
by reevaluating on the previous regions [ 2]. However, replay requires access to the previously used training data
sets. This both requires large storage capabilities for large datasets, and also physical access to the previous dataset.
However, data privacy can limit access to prior datasets, so replay may not be a feasible option. An alternative to replay
are regularization methods, where a regularizer is used to assign weights to each parameter in the neural network,
representing the parameter’s importance. Then, a penalty is applied to prevent the parameters with the largest weights
from changing. Multiple methods have been proposed for how to calculate the importance weights. Among the top
choices are Synaptic Intelligence [ 3], elastic weight consolidation (EWC) [ 4], and memory aware synapses (MAS)
[5]. Subsequent work has shown that MAS performs among the best in multiple use cases, and is more robust to the
choice of hyperparameters, so here we use MAS [ 6,7]. Finally, a third category of continual learning methods includes
those that employ task-specific modules [ 8], ensembles [ 9], adapters [ 10], reservoir computing based architectures [ 11],
slow-fast weights [12, 13] and more.
In recent years, a huge research focus has been on scientific machine learning methods for physical systems [ 14,15,16],
for example fluid mechanics and rheology [ 17,18,19,20], metamaterial development [ 21,22,23], high speed flows
[24], and power systems [ 25,26,27]. In particular, physics-informed neural networks, or PINNs [ 28], allow for
accurately representing differential operators through automatic differentiation, allowing for finding the solution to
PDEs without explicit mesh generation. Work on continual learning for PINNs is limited. While as a first attempt
PINNs can be trained on the entire domain because the issues of data acquisition and privacy do not apply, many
systems have been identified for which it is not possible to train a PINN for the entire desired time domain. For example,
even the simple examples used in this work, a pendulum and the Allen-Cahn equation, cannot be trained by a PINN for
long times. Recent work has looked at improving the training of PINNs for such systems, including applications of the
neural tangent kernel [ 29], but more work remains to be done. The closest work we are aware of for continual learning
with PINNS is the backward-compatible PINNs in [ 30] and incremental PINNs (iPINNs) in [ 31]. Backward-compatible
PINNs train NPINNs on a sequence of Ntime domains, and in each new domain enforce that the output from the
current PINN satisfies the PINN loss function in the current domain and the output of the previous model on all previous
domains. We note that this work is distinct from the replay approach taken with PINNs in this work, both in the
single fidelity and multifidelity cases, because we enforce that the Nth neural network satisfies the residual in all prior
domains, not the output from the previous model. In iPINNs, PINNs are trained to satisfy a series of different equations
through a subnetwork for each equation, rather than the same equation over a long time.
We will introduce the multifidelity continual learning method in Sec. 2. We will then show the performance of the
method on physics-informed problems in Sec. 3 and on data-informed problems in Sec. 4.
2 Multifidelity continual learning method
We assume that we have a domain Ω, which we divide into Nsubdomains Ω =∪N
i=0Ωi. We will learn sequential
models on each subdomain Ωi, with the goal that the ith model can provide accurate predictions on the domain ∪i
j=0Ωj.
That is, the ith model does not forget the information learned on earlier domains used in training. We will focus on
applications to physical systems, where we either have data available or knowledge of the physical laws the system
obeys. We will begin this section with a brief overview of physics-informed neural networks (PINNs), then discuss
the multifidelity continual learning method (MFCL), and conclude with a description of methods we use to limit
catastrophic forgetting.
2.1 Physics-informed neural networks
In this section we give a brief introduction to single-fidelity and multifidelity physics-informed neural networks (PINNs),
which were introduced in [ 28] and have been covered in depth for many relevant applications [ 32,14]. PINNs are
generally used, in these applications, for initial-boundary valued problems.
st+Ox[s] =0,x∈Ω, t∈[0, T] (1)
s(x, t) =g(x, t)x∈∂Ω, t∈[0, T] (2)
s(x,0) =u(x)x∈Ω (3)
2

--- PAGE 3 ---
APREPRINT - FEBRUARY 13, 2024
where Ω∈RNis an open, bounded domain with boundary ∂Ω,ganduare given functions, and xandtare the spatial
and temporal coordinates, respectively. Oxis a general differential operator with respect to x. We wish to find an
approximation to s(x, t)by a (series) of deep neural networks with parameters γ, denoted by sγ(x, t). The neural
network is trained by minimizing the loss function
L(γ) =λbcLbc(γ) +λicLic(γ) +λrLr(γ) +λdataLdata(γ) (4)
where the subscripts bc,ic,r, and data denote the terms corresponding to the boundary conditions, initial conditions,
and residual, and any provided data, respectively. We take Nbc,Nic, and Nrto be the batch sizes of the boundary,
initial, and residual data point, and denote the training data by
(xi
bc, ti
bc),g(xi
bc, ti
bc)	Nbc
i=0,
(xi
ic),u(xi
bc)	Nic
i=0, and
(xi
r, ti
r)	Nr
i=0. The boundary and initial collocation points are randomly sampled uniformly in their respective domains.
The selection of the Nrresidual points will be discussed in Sec. ??. If data representing the solution sis available,
we can also consider an additional dataset
(xi
data, ti
data),s(xi
data, ti
data)	Ndata
i=0. This term is included to capture the
data-based training we will cover in Sec. 4.
The individual loss terms are given by the mean square errors,
Lbc(γ) =1
NbcNbcX
i=0sγ(xi
bc, ti
bc)−g(xi
bc, ti
bc)2(5)
Lic(γ) =1
NicNicX
i=0sγ(xi
ic,0)−u(xi
ic)2(6)
Lr(γ) =1
NrNrX
i=0rγ(xi
r, ti
r)2(7)
Ldata(γ) =1
NdataNdataX
i=0sγ(xi
data, ti
data)−s(xi
data, ti
data)2(8)
where
rγ(x, t) =∂
∂tsγ(x, t) +Ox[sγ(xi
data, ti
data)]. (9)
The weighting parameters λbc,λic,λr, and λdata are chosen before training by the user.
Multifidelity PINNs, as used in this work, are inspired by [ 33]. We assume we have a low fidelity model in the form of
a deep neural network that approximates a given dataset or differential operator with low accuracy. We want to train
two additional neural networks to learn the linear and nonlinear correlations between the low fidelity approximation and
a high fidelity approximation or high fidelity data. We denote these neural networks as NN lfor the linear correlation
andNN nlfor the nonlinear correlation. The output is then sγ(x, t) =NN nl(x, t;γ) +NN l(x, t;γ), where γis all
trainable parameters of the linear and nonlinear networks. The loss function includes an additional term,
LMF(γ) =λbcLbc(γ) +λicLic(γ) +λrLr(γ) +λdataLdata(γ) +λX
(γnl,ij)2, (10)
where {γnl,ij}is the set of all weights and biases of the nonlinear network NN nl. No activation function is used in
NN lto result in learning a linear correlation between the previous prediction and the high fidelity model.
2.2 Multifidelity continual learning
In the MFCL method, we exploit correlations between the previously trained models on prior domains and the expected
model on the current domain. Explicitly, we use the prior model NN i−1as a low fidelity model for domain Ωi. Then,
we learn the correlation between NN i−1on domain Ωiand the data or physics given on the domain . By learning a
general combination of linear and nonlinear terms, we can capture complex correlations. Because the method learns
only the correlation between the previous model and the new model, we can in general use smaller networks in each
subdomain. The procedure requires two initial steps:
1.Train a (single-fidelity) DNN or PINN on Ω1, denoted by NN∗(x, t;γ∗). This network will approximate the
solution in a single domain.
2.Train a multifidelity DNN or PINN in Ω1, which takes as input the single fidelity model NN∗(x, t;γ∗)as a
low fidelity approximation. This initial multifidelity network is denoted by NN 1(x, t;γ1)
3

--- PAGE 4 ---
APREPRINT - FEBRUARY 13, 2024
Then, for each additional domain Ωi, we train a multifidelity DNN or PINN in Ωi, denoted by NN i(x, t;γi), which
takes as input the previous multifidelity model NN i−1(x, t;γi−1)as a low fidelity approximation. The goal is for
NN i(x, t;γi)to provide an accurate solution on ∪i
j=1Ωi, even when data from Ωj,j < i , is not used in training the
multifidelity network NN i. A diagram of the method is given in Fig. 2.
NN i−1(x, t)
.........x1
...
xN
tLinear network
Nonlinear network NN i(x, t)
Fi
nlFi
l
∂y∂x∂t
∂2
x
∂2
yO[NN i(x, t)]
Figure 2: Diagram of the MF-CL method on domain Ωi. The output from the previously trained neural network,
NN i−1(x, t;γi−1), is used as input to the linear and nonlinear subnets for a point (x, t)∈Ωi,x∈RN. The output
neural network is the sum of the linear and nonlinear subnetworks.
As we will show, the MF-CL method provides more accurate results with less forgetting than single fidelity training on
its own, however, the method can be improved by a few methods that have been previously developed both for reducing
forgetting in continual learning and for selecting collocation points for training PINNs. These methods are discussed
below.
2.3 Memory aware synapses
Memory aware synapses (MAS) is a continual learning method that attempts to limit forgetting in continual learning by
assigning an importance weight to each neuron in the neural network. Then, a penalty term is added to the loss function
to prevent large deviations in the values of important weights when the next networks are trained. The importance
weights are found by measuring how sensitive the output of neural net NN nis to changes in the network parameters
[5]. For each weight and bias γijin the neural network we calculate the importance weight parameter
Ωn
ij=1
NNX
k=1∂ 
ℓ2
2NN n(xk;γ)
∂γij(11)
where ℓ2
2denotes the squared ℓ2norm of the output of the neural network NN napplied at xk. The loss function in eq.
10 is then modified to read:
LMF,MAS (γn) =λbcLbc(γn) +λicLic(γn) +λrLr(γn) +λdataLdata(γn) +λX
i,j(γn
nl,ij)2
+λMASX
i,jΩn−1
ij 
γn
ij−γn−1
ij2(12)
When applying MAS to multifidelity neural networks, we calculate the MAS terms separately:
Ωn,nl
ij=1
NNX
k=1∂
ℓ2
2NNnl
n(xk;γ)
∂γnl
ij,Ωn,l
ij=1
NNX
k=1∂
ℓ2
2NNl
n(xk;γ)
∂γl
ij(13)
where nldenotes the nonlinear network and ldenotes the linear network. In this way, roughly, the importance in the
weights in calculating the linear and nonlinear terms is found separately, instead of determining the importance in the
overall output of the sum of the networks. The parameter λMAS is kept the same for the linear and nonlinear parts.
4

--- PAGE 5 ---
APREPRINT - FEBRUARY 13, 2024
2.4 Replay
In replay, a selection of points in the previously trained domains, ∪n−1
i=1Ωiare selected at each iteration and the residual
loss,Lr(γn)is evaluated at the points. In this way, the multididelity training still satisfies the PDE across the earlier
trained domains. For PINNs, the replay approach only requires knowledge of the geometry of ∪n−1
i=1Ωi, and not the
value of the output of the model on this domain.
2.5 Transfer learning
In all cases in this work, the values of the trainable parameters in each subsequent network NN i,i≥2, is initialized
from the final values of the trainable parameters in the previous network, NN i−1. In notation, γ0
i=γi−1. This
approach allows for faster training because the network is not initialized randomly. We note that some previous work
has found less forgetting by initializing each subsequent network randomly [ 34], and leave the exploration of this option
for future work.
3 Physics-informed training
In this section, we give examples of applying the multifidelity continual learning for physics-informed neural networks
in cases where PINNs fail to train. We show that using continual learning in time can improve the accuracy of training a
PINN for long-time integration problems, where a single PINN is not sufficient. All hyperparameters used in training
are given in Appendix 8.
3.1 Pendulum dynamics
In this section, we consider the gravity pendulum with damping from [ 29]. The system is governed by an ODE for
t∈[0, T]
ds1
dt=s2, (14)
ds2
dt=−b
ms2−g
Lsin(s1). (15)
The initial conditions are give by s1(0) = s2(0) = 1 . We take m=L= 1,b= 0.05, and g= 9.81, and we take
T= 10 .
Figure 3: Results from training a single PINN to satisfy Eqs. 14 and 15 (solid lines) compared with the exact solution
(dotted line) for s1(left) and s2(right). The results decay to zero quickly and the learned solution does not agree well
with the exact solution.
We first consider a single PINN trained in t∈[0,10]in Fig. 3. The solution quickly goes to zero, showing that a
single PINN cannot capture the longtime dynamics of even this simple system. Similar results were shown in [ 29]. We
will note that there are recent advances that have been developed for improving the training of PINNs for long-time
integration problems [ 29,35,36]. In this section, we will explore how continual learning can also allow for accurate
solutions over long times by dividing the time domains into subdomains.
We divide the domain into five subdomains, Ωi= [2( i−1),2i]and train on each domain using both traditional single
fidelity continual learning and MF-CL, and SF-CL and MF-CL approaches augmented by replay and MAS. For each
5

--- PAGE 6 ---
APREPRINT - FEBRUARY 13, 2024
Single fidelity Multifidelity
Applied alone 16.774 2.147
Replay 0.041 0.079
MAS 1.390 1.146
Table 1: RMSE of the final output NN 5on the full domain for the pendulum problem. For the MAS cases, the network
is trained for six values of λMAS , and the case with the lowest RMSE is shown in the table above. The replay results
haveN= 100 neurons in each hidden layer, see Table 2 for cases with varying neurons in each hidden layer.
case, we calculate the root mean square error (RMSE) of the final output NN 5on the full domain, Ω = [0 ,10]by
RMSE =vuut1
NNX
j=1[NN 5(tj)−s(tj)]2, (16)
where sdenotes the exact solution. If forgetting is limited, the final solution should have a small RMSE on the full
domain.
(a) Single fidelity
(b) Multifidelity
Figure 4: Results from training the single fidelity (a) and multifidelity (b) alone to satisfy Eqs. 14 and 15 compared with
the exact solution (dash-dotted line) for s1(left) and s2(right). Of particular importance is the final network, NN 5
(blue solid line), which is trained on Ω5= [8,10]. While the multifidelity results in (b) have significant errors, the
are substantially better than the single fidelity results in (a). In the single fidelity training, each network NN iis only
accurate on the subdomain Ωi, and extrapolation outside Ωipresents significant difficulties.
It is clear from Table 1 that replay performs the best in both cases, and significantly better than any other approach. It is
no surprise that the SF applied alone case has a large RMSE, as it does not have any incorporation of techniques to limit
forgetting. This case is shown in Fig. 4a.
Fig. 5 gives the best MAS results for each of the sets of hyperparameters considered, with λMAS = 100 for single
fidelity and λMAS = 0.001for multifidelity. As is unsurprising given the smaller RMSE, the multifidelity outperforms
the single fidelity training with MAS.
As shown in Fig. 6, the SF-replay case does appear to outperform the MF-replay case. However, it is interesting to look
at the RMSE as we change the network size in Table 2. While the MF-replay case is robust to changes in the network
size, the single fidelity case only achieves a small RMSE with a very specific architecture.
6

--- PAGE 7 ---
APREPRINT - FEBRUARY 13, 2024
(a) Single fidelity
(b) Multifidelity
Figure 5: Results from training the single fidelity (a) and multifidelity (b) with MAS to satisfy Eqs. 14 and 15 compared
with the exact solution (dash-dotted line) for s1(left) and s2(right). Of particular importance is the final network,
NN 5(blue solid line), which is trained on Ω5= [8,10]. These simulations plotted here have the smallest RMSEs of
NN 5onΩof any of the sets of hyperparameters tested. In the single fidelity case, MAS appears to cause restrictions
in training that are too strict, and later networks NN iare no longer accurate on their respective domains Ωi. For the
multifidelity training, the solutions are accurate across a wider portion of the full domain, and the RMSE is decreased
compared with multifidelity training alone.
(a) Single fidelity
(b) Multifidelity
Figure 6: Results from training the single fidelity (a) and multifidelity (b) with Replay to satisfy Eqs. 14 and 15
compared with the exact solution (dash-dotted line) for s1(left) and s2(right). Both cases show very limited forgetting.
7

--- PAGE 8 ---
APREPRINT - FEBRUARY 13, 2024
N Single fidelity Multifidelity
25 3.130 0.044
50 0.107 0.027
100 0.041 0.079
150 0.055 0.054
200 0.091 0.038
Table 2: RMSE of the final output NN 5on the full domain for the pendulum problem. The SF case has five hidden
layers with Nneurons each. In the MF case, each nonlinear network has five hidden layers with Nneurons. The
multifidelity linear network has one hidden layer with 20 neurons.
3.2 Allen-Cahn equation
The Allen-Cahn equation is given by
ut−c2
1uxx+ 5u3−5u= 0, t∈(0,1], x∈[−1,1] (17)
u(x,0) = x2cos(πx), x∈[−1,1] (18)
u(x, t) =u(−x, t), t∈[0,1], x=−1, x= 1 (19)
ux(x, t) =ux(−x, t), t∈[0,1], x=−1, x= 1 (20)
We take c2
1= 0.0001 . The Allen-Cahn equation is notoriously difficult for PINNs to solve by direct application [ 37,38],
see Fig. 7. Modifications of PINNs have successfully been able to solve the Allen-Cahn equation, including by using a
discrete Runge-Kutta neural network [ 28], adaptive sampling of the collocation points [ 37], and backward compatible
PINNs [ 30]. In this section we show that we can accurately learn the solution to the Allen-Cahn equation by applying
the multifidelity continual learning framework.
Figure 7: Results from training a single PINN training for the Allen-Cahn equation. The bottom figures are taken at
t= 0.25(left) and t= 0.75(right). While the PINN trains well until about 0.3, the solution degrades with increasing t.
We divide the domain into four subdomains, Ωi= [2( i−1),2i], and report the relative RMSE of NN 4on the full
domain Ω.When the multifidelity and single fidelity methods are trained alone, in Fig. 8, they have approximately
equal relative RMSEs. MAS and replay both improve the results, in Figs. 9 and 10, respectively. A summary of the
results is given in Table 3.
8

--- PAGE 9 ---
APREPRINT - FEBRUARY 13, 2024
Figure 8: NN 4results from training a single fidelity and multifidelity PINN training alone for the Allen-Cahn equation.
The bottom figures are taken at t= 0.25(left) and t= 0.75(right). The multifidelity results have errors about half as
large as those of the single fidelity results.
Single fidelity Multifidelity
No CL 0.126 0.128
Replay 0.087 0.084
MAS 0.146 0.056
Table 3: Relative RMSE of the final output NN 4on the full domain for the Allen-Cahn equation. For the MAS cases,
the network is trained for seven values of λMAS , and the case with the lowest RMSE is shown in the table above.
4 Data-informed training
4.1 Batteries
This is a case where if an additional dataset is added, it not is clear a priori which subdomain it lies in. Therefore, it is
essential that the final model can predict the current accurately for the entire domain without forgetting.
For testing, a vanadium redox-flow battery (VRFB) system was selected to generate datasets. The left image in Fig. 11
shows a typical configuration of a VRFB, which consists of electrodes, current collectors and a membrane separator. The
negative and positive side have a storage tank each to store the redox couple of V2+/V3+andV4+/V5+, respectively.
We applied the MFCL method for the problem of identifying the applied charge current from a given charge voltage
curve. To generate the VRFB charge curve dataset, a highly computationally efficient 2-D analytical model was utilized
[39,40]. This model fully resolves the coupled physics of active species transport, electrochemical reaction kinetics,
and fluid dynamics within the battery cell, thereby providing a faithful representation of the VRFB system. Further
details on the model and its parameters can be found in [ 39]. Typical charge curves are visualized in the right plot of
Fig. 11 for five selected current levels. For a given charge current, the battery voltage ( E) is calculated at different
state-of-charge (SOC) values to form the charge curve which is used as input data. The applied charge current Iwhich
gives rise to the charge curve is the output quantity we want to predict.
We divide the data set in five sets by charge current and train with and without MAS in the single fidelity case. The
subdomains are Ω1= [0.1,2),Ω2= [2,4),Ω3= [4,6),Ω4= [6,8), and Ω5= [8,9]. The errors are calculated by
9

--- PAGE 10 ---
APREPRINT - FEBRUARY 13, 2024
Figure 9: NN 4results from training a single fidelity and multifidelity PINN training with MAS for the Allen-Cahn
equation. The bottom figures are taken at t= 0.25(left) and t= 0.75(right). These results represent the best MAS
results from all sets of hyperparameters considered. The multifidelity results have errors about a quarter as large as
those of the single fidelity results.
the RMSE of the output of NN 5on a test test selected from Ω =∪5
i=1Ωi. We test two network architectures, a wide
network which has two hidden layers with 80 neurons each, and a deeper and narrower network which has three hidden
layers with 40 neurons each. We first train with the single fidelity and multifdelity approaches alone, see Fig. 12. The
multifidelity continual learning results show less forgetting than those from the single fidelity continual learning.
We then consider the impact of adding MAS. We consider the narrow and wide networks with and without MAS scaling,
for a total of four cases. The multifidelity MAS results show significant improvement, see Fig. 13. In Fig. 14, we
compare the performance across the value of the MAS hyperparameter λMAS . We see that the single fidelity approach
performance is robust, since it is insensitive to the value of λMAS.However, it is not very accurate. On the other hand,
the multifidelity approach can be substantially more accurate than the single fidelity approach for most values of λMAS.
Overall, the multifidelity results significantly outperform the single fidelity results.
4.2 Energy consumption
To provide a second example of data-informed continual learning, we consider the city-scale daily energy consumption
dataset from [ 41]. The dataset consists of daily energy usage for three metropolitan areas, New York, Sacramento, and
Los Angeles, along with daily weather data. Three years of data are used as a test set, with an additional year as a test
set.
Energy usage depends strongly on the weather, with air conditioner usage in the warmer months and heating in the
winter months. Therefore, to provide different tasks to the continual learning training, we divide the three years of
training data by quarter. Task 1 has training data from January to March, Task 2 has training data from April to June,
Task 3 has training data from July to September, and Task 4 has training data from October to December. The test set
for all tasks is to predict the energy usage from July 2018 to June 2019. An illustration of the testing and training data
divided into tasks is given in Fig. 15.
We train both single fidelity and multifidelity networks with and without MAS. We consider a range of λ∈[0.001,100].
We also compare with training a network without continual learning. In this case, a single fidelity DNN receives allof
the training data from all four tasks, to try and predict the energy usage from July 2018 to June 2019. This case serves
10

--- PAGE 11 ---
APREPRINT - FEBRUARY 13, 2024
Figure 10: NN 4results from training a single fidelity and multifidelity PINN training with replay for the Allen-Cahn
equation. The bottom figures are taken at t= 0.25(left) and t= 0.75(right).
Figure 11: The VRFB system used for battery data generation (left). Sample charge curve distribution at different
charge current (right).
as a benchmark for the reasonable level of error we can expect from our model using continual learning. The results are
shown in Table 4. We note that in all cases, the multifidelity continual learning approach outperforms the single fidelity
continual learning. Including MAS does improve the results, as shown in Fig. 16. The continual learning methods do
perform worse than the case with no continual learning, which is expected because they never have access to all the
training data simultaneously. A comparison of the RMSE for each value of λMAS tested is given in Fig. 17. We note
11

--- PAGE 12 ---
APREPRINT - FEBRUARY 13, 2024
(a) Single fidelity
(b) Multifidelity
Figure 12: Results from the single fidelity (a) and multifidelity (b) training alone for the battery test case. The left
column has the network outputs of each task on all the tasks, and the right column shows the RMSE of each task tested
on each other task. The results in this figure use the narrow architecture, with three hidden layers and 40 neurons per
layer.
City No CL Single fidelity Multifidelity Single fidelity, MAS Multifidelity, MAS
New York 16.6289 95.8546 36.8522 38.2728 30.5356
Sacramento 2.6952 8.9388 4.4678 5.4253 4.7614
Los Angeles 5.5859 15.1774 9.8254 8.0617 7.0310
Table 4: RMSE (GWh) of the final output NN 4on the full test domain for the energy consumption case. For the MAS
cases, the network is trained for seven values of λMAS , and the case with the lowest RMSE is shown in the table above.
that overall, the multifidelity approach with MAS is more robust than the single fidelity training with MAS, resulting in
a smaller RMSE across a range of λMAS.
.
5 Discussion and future work
We have introduced a novel continual learning method based on multifidelity deep neural networks. The premise of the
method is the existence of correlations between the output of previously trained models and the desired output of the
model on the current training dataset. The discovery and use of these correlations can limit catastrophic forgetting. On
its own, the multifidelity continual learning method has shown robustness and limited forgetting across several datasets
12

--- PAGE 13 ---
APREPRINT - FEBRUARY 13, 2024
(a) Single fidelity
(b) Multifidelity
Figure 13: Results from the single fidelity (a) and multifidelity (b) training with MAS for the battery test case. The
single fidelity case struggles to train accurately, while multifidelity has very limited forgetting. The left column has the
network outputs of each task on all the tasks, and the right column shows the RMSE of each task tested on each other
task. The results shown represent the best output from the MAS hyperparameters tested. For the single fidelity case, the
results are from the narrow network with λMAS = 100 , and for the multifidelity case, the results are from the wide
network with λMAS = 0.001.
for physics-informed and data-driven training examples. Additionally, it can be combined with existing continual
learning methods, including replay and memory aware synapses (MAS), to further limit catastrophic forgetting.
The proposed continual learning method is especially suited for physical problems where the data satisfy the same
physical laws on each domain, or for a physics-informed neural network, because in these cases we expect there to be a
strong correlation between the output of the previous model and the model on the current training domain. As a result
of exploiting the correlation between data in the various domains instead of training from scratch for each domain, the
method can afford to continue learning in new domains using smaller networks. Specifically, its training accuracy is
more robust to the size of the network employed in the new domain. This can lead to computational savings during
both training and inference. The approach is particularly suited for situations where privacy concerns can limit access
to prior datasets. It can also offer new possibilities in the area of federated learning by allowing the design of new
algorithms for processing sensor data in a distributed fashion. These topics are under investigation and results will be
reported in a future publication.
13

--- PAGE 14 ---
APREPRINT - FEBRUARY 13, 2024
Figure 14: Comparison of the RMSE with MAS for the redox flow battery test case. The RMSE is lower for almost all
the multifidelity test cases in comparison with the single fidelity test cases.
Figure 15: Illustration of the datasets used in the energy consumption example.
6 Data availability
All code and data needed to reproduce these results will be made available at https://github.com/pnnl/
Multifidelity_continual_learning/ .
7 Acknowledgements
This research was supported by the Energy Storage Materials Initiative (ESMI), under the Laboratory Directed Research
and Development (LDRD) Program at Pacific Northwest National Laboratory (PNNL). The computational work was
performed using PNNL Institutional Computing at Pacific Northwest National Laboratory. PNNL is a multi-program
national laboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute under Contract
No. DE-AC05-76RL01830.
References
[1]German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning
with neural networks: A review. Neural networks , 113:54–71, 2019.
[2]Eli Verwimp, Matthias De Lange, and Tinne Tuytelaars. Rehearsal revealed: The limits and merits of revisiting
samples in continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pages 9385–9394, 2021.
[3]Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In
International conference on machine learning , pages 3987–3995. PMLR, 2017.
14

--- PAGE 15 ---
APREPRINT - FEBRUARY 13, 2024
Figure 16: Results of the energy consumption problem. (Left) results without MAS. (Right) results with MAS.
Figure 17: Comparison of the RMSEs generated by training with MAS for the energy consumption problem.
[4]James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017.
[5]Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) ,
pages 139–154, 2018.
[6]Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and
Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on
pattern analysis and machine intelligence , 44(7):3366–3385, 2021.
[7]Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenarios: A
categorization and case for strong baselines. arXiv preprint arXiv:1810.12488 , 2018.
[8]Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu,
Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671 , 2016.
[9]Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and
lifelong learning. arXiv preprint arXiv:2002.06715 , 2020.
15

--- PAGE 16 ---
APREPRINT - FEBRUARY 13, 2024
[10] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vuli ´c, Sebastian Ruder, Kyunghyun Cho,
and Iryna Gurevych. Adapterhub: A framework for adapting transformers. arXiv preprint arXiv:2007.07779 ,
2020.
[11] Leonard Bereska and Efstratios Gavves. Continual learning of dynamical systems with competitive federated
reservoir computing. In Conference on Lifelong Learning Agents , pages 335–350. PMLR, 2022.
[12] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International conference on machine learning , pages
2554–2563. PMLR, 2017.
[13] Max Vladymyrov, Andrey Zhmoginov, and Mark Sandler. Continual few-shot learning using hypertransformers.
arXiv preprint arXiv:2301.04584 , 2023.
[14] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-
informed machine learning. Nature Reviews Physics , 3(6):422–440, 2021.
[15] Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Manish Parashar,
Abani Patra, James Sethian, Stefan Wild, et al. Workshop report on basic research needs for scientific ma-
chine learning: Core technologies for artificial intelligence. Technical report, USDOE Office of Science (SC),
Washington, DC (United States), 2019.
[16] Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco
Piccialli. Scientific machine learning through physics–informed neural networks: Where we are and what’s next.
Journal of Scientific Computing , 92(3):88, 2022.
[17] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-
informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics ,
426:109951, 2021.
[18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and
pressure fields from flow visualizations. Science , 367(6481):1026–1030, 2020.
[19] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural
networks (pinns) for fluid mechanics: A review. Acta Mechanica Sinica , 37(12):1727–1738, 2021.
[20] Archis S Joglekar and Alexander G R Thomas. Machine learning of hidden variables in multiscale fluid simulation.
Machine Learning: Science and Technology , 4(3):035049, sep 2023.
[21] Dehao Liu and Yan Wang. Multi-fidelity physics-constrained neural network and its application in materials
modeling. Journal of Mechanical Design , 141(12), 2019.
[22] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse
problems in nano-optics and metamaterials. Optics express , 28(8):11618–11633, 2020.
[23] Zhiwei Fang and Justin Zhan. Deep physical informed neural networks for metamaterial design. IEEE Access ,
8:24506–24513, 2019.
[24] Zhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed
flows. Computer Methods in Applied Mechanics and Engineering , 360:112789, 2020.
[25] George S Misyris, Andreas Venzke, and Spyros Chatzivasileiadis. Physics-informed neural networks for power
systems. In 2020 IEEE Power & Energy Society General Meeting (PESGM) , pages 1–5. IEEE, 2020.
[26] Bin Huang and Jianhui Wang. Applications of physics-informed neural networks in power systems-a review.
IEEE Transactions on Power Systems , 38(1):572–588, 2022.
[27] Christian Moya and Guang Lin. Dae-pinn: a physics-informed neural network model for simulating differential
algebraic equations with application to power networks. Neural Computing and Applications , 35(5):3789–3804,
2023.
[28] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of
Computational Physics , 378:686–707, 2019.
[29] Sifan Wang and Paris Perdikaris. Long-time integration of parametric evolution equations with physics-informed
deeponets. Journal of Computational Physics , 475:111855, 2023.
[30] Revanth Mattey and Susanta Ghosh. A novel sequential method to train physics informed neural networks for
allen cahn and cahn hilliard equations. Computer Methods in Applied Mechanics and Engineering , 390:114474,
2022.
[31] Aleksandr Dekhovich, Marcel HF Sluiter, David MJ Tax, and Miguel A Bessa. ipinns: Incremental learning for
physics-informed neural networks. arXiv preprint arXiv:2304.04854 , 2023.
16

--- PAGE 17 ---
APREPRINT - FEBRUARY 13, 2024
[32] Majid Rasht-Behesht, Christian Huber, Khemraj Shukla, and George Em Karniadakis. Physics-informed neural
networks (pinns) for wave propagation and full waveform inversions. Journal of Geophysical Research: Solid
Earth , 127(5):e2021JB023120, 2022.
[33] Xuhui Meng and George Em Karniadakis. A composite neural network that learns from multi-fidelity data:
Application to function approximation and inverse pde problems. Journal of Computational Physics , 401:109020,
2020.
[34] Frederik Benzing. Unifying regularisation methods for continual learning. arXiv preprint arXiv:2006.06357 ,
2020.
[35] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel
perspective. Journal of Computational Physics , 449:110768, 2022.
[36] Xuhui Meng, Zhen Li, Dongkun Zhang, and George Em Karniadakis. Ppinn: Parareal physics-informed neural
network for time-dependent pdes. Computer Methods in Applied Mechanics and Engineering , 370:113250, 2020.
[37] Colby L Wight and Jia Zhao. Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics informed
neural networks. arXiv preprint arXiv:2007.04542 , 2020.
[38] Franz M Rohrhofer, Stefan Posch, Clemens Gößnitzer, and Bernhard C Geiger. On the role of fixed points of
dynamical systems in training physics-informed neural networks. arXiv preprint arXiv:2203.13648 , 2022.
[39] Yunxiang Chen, Jie Bao, Zhijie Xu, Peiyuan Gao, Litao Yan, Soowhan Kim, and Wei Wang. A two-dimensional an-
alytical unit cell model for redox flow battery evaluation and optimization. Journal of Power Sources , 506:230192,
2021.
[40] Yunxiang Chen, Zhijie Xu, Chao Wang, Jie Bao, Brian Koeppel, Litao Yan, Peiyuan Gao, and Wei Wang.
Analytical modeling for redox flow battery design. Journal of Power Sources , 482:228817, 2021.
[41] Zhe Wang, Tianzhen Hong, Han Li, and Mary Ann Piette. Predicting city-scale daily electricity consumption
using data-driven models. Advances in Applied Energy , 2:100025, 2021.
17

--- PAGE 18 ---
APREPRINT - FEBRUARY 13, 2024
8 Appendix
In this section we report the training parameters used to train the results reported above.
18

--- PAGE 19 ---
APREPRINT - FEBRUARY 13, 2024Sec. 3.1 Sec. 3.2 Sec. 4.1 Sec. 4.2
SFNN 1learning rate (1e-3, 2000, 0.95) (1e-4, 2000, 0.99) (1e-3, 2000, 0.9) (1e-3, 2000, 0.99)
SFNN i,i >1learning rate (1e-3, 2000, 0.95) (1e-4, 2000, 0.99) (1e-3, 2000, 0.95) (1e-3, 2000, 0.99)
SF activation function swish tanh relu tanh
SF architecture [1, 100, 100, 100, 100, 100, 2] [2, 200, 200, 200, 200, 200, 1] [784, 80, 80, 1] or [5, 100, 100, 100, 1]
[784, 40, 40, 40, 1]
SF batch size 100 500 10 100
SF boundary condition batch size 1 100 – –
SF iterations 50,000 100,000 100,000 100,000
MFNN∗learning rate (1e-3, 2000, 0.99) (1e-4, 2000, 0.99) (1e-3, 2000, 0.9) (1e-3, 2000, 0.99)
MFNN∗activation function swish tanhtanh relu tanh
MFNN∗architecture [1, 200, 200, 200, 1] [2, 200, 200, 200, 200, 200, 1] [784, 40, 40, 40, 1] [5, 100, 100, 100, 1]
MFNN∗iterations 50,000 50,000 100,000 100,000
MF batch size 200 1000 10 100
MF boundary condition batch size 1 100 – –
MFNN ilearning rate (1e-3, 2000, 0.99) (5e-4, 2000, 0.99) (1e-3, 2000, 0.95) (1e-3, 2000, 0.99)
MFNN iactivation function swish tanh relu tanh
MFNN inonlinear architecture [3, 100, 100, 100, 100, 100, 2] [3, 200, 200, 200, 200, 200, 1] [785, 80, 80, 1] or [6, 100, 100, 100, 1]
[785, 40, 40, 40, 1]
MFNN ilinear architecture [2, 20, 2] [1, 20, 1] [1, 1] [1, 5, 1]
MFNN 1iterations 50,000 50,000 100,000 100,000
MFNN i,i >1iterations 100,000 100,000 100,000 100,000
MF MAS samples 1000 3000 400 1201
SF MAS samples 1000 3000 400 1201
No CL learning rate (1e-3, 2000, 0.95) (1e-4, 2000, 0.99) – (1e-3, 2000, 0.99)
No CL activation function swish tanh – relu
No CL architecture [1, 200, 200, 200, 1] [2, 200, 200, 200, 200, 200, 1] – [5, 40, 40, 40, 1]
No CL batch size 200 500 – 100
No CL boundary condition batch size 1 100 – –
No CL iterations 50,000 100,000 – 100,000
λbc – 1 – –
λic 1 100 – –
λr 10 1 – –
λdata 0 0 1 1
λ 1e-4 1e-5 1e-4 1e-5
Table 5: Hyperparameters for training the results in this paper. SF refers to the single fidelity results. MF refers to the multifidelity results. For the multifidelity
results, NN∗denotes the first single fidelity network in the multifidelity framework, and NN idenotes the multifidelity neural networks. For the learning rate,
the triplet (a, b, c )denotes the exponential_decay function in Jax with learning rate a, decay steps b, and decay rate c.relu is the rectified linear unit (ReLU)
activation function
19
