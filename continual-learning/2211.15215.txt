# 2211.15215.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2211.15215.pdf
# File size: 1915920 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Progressive Learning without Forgetting
Tao Feng1, Hangjie Yuan2, Mang Wang3, Ziyuan Huang4, Ang Bian1, Jianzhou Zhang1
1Sichuan University2Zhejiang University3ByteDance Inc.4National University of Singapore
fengtao.hi@gmail.com, hj.yuan@zju.edu.cn, wangmang@bytedance.com
ziyuan.huang@u.nus.edu, bian@scu.edu.cn, zhangjz@scu.edu.cn
Abstract
Learning from changing tasks and sequential experience
without forgetting the obtained knowledge is a challenging
problem for artiﬁcial neural networks. In this work, we fo-
cus on two challenging problems in the paradigm of Con-
tinual Learning (CL) without involving any old data: (i)the
accumulation of catastrophic forgetting caused by the grad-
ually fading knowledge space from which the model learns
the previous knowledge; (ii)the uncontrolled tug-of-war dy-
namics to balance the stability and plasticity during the
learning of new tasks. In order to tackle these problems, we
present Progressive Learning without Forgetting (PLwF)
and a credit assignment regime in the optimizer. PLwF
densely introduces model functions from previous tasks to
construct a knowledge space such that it contains the most
reliable knowledge on each task and the distribution in-
formation of different tasks, while credit assignment con-
trols the tug-of-war dynamics by removing gradient conﬂict
through projection. Extensive ablative experiments demon-
strate the effectiveness of PLwF and credit assignment. In
comparison with other CL methods, we report notably bet-
ter results even without relying on any raw data.
1. Introduction
Continual Learning (CL) remains a long-standing chal-
lenge for Artiﬁcial Neural Networks (ANNs) [10, 19, 54].
During CL, the model is prone to suffer from catastrophic
forgetting, where the deep learner only performs well on
the most recent tasks while no longer recalls the knowledge
learned in earlier tasks. A naive strategy for avoiding catas-
trophic forgetting would be training a new model on data
for all the existing tasks. However, this is impractical due
to data inaccessibility. Hence, a line of work [9, 18, 34] fo-
cuses on constructing special subspace of old tasks instead
of adopting data to mitigate forgetting.
Another popular solution [1,15,25,56] is to impose regu-
larization on the deep learner in the battle against forgetting.
Under this paradigm, there exists two key problems. First,
Stage TStage 2 Stage 1Stage TStage 2 Stage 1(a)  Untrustworthy function (b) Recalled function
Accuracy (%)010203040
Task12345
UntrustworthyRecalled
(c) The improvement on forgettingFigure 1. (a)The illustration of the accumulation of forgotten
knowledge caused by gradually fading knowledge space when
learning in a new stage. Dotted lines denote the use of previous
knowledge space in new tasks. The degree of opacity denotes the
reliability of knowledge in current tasks. (c)The ﬁgure indicat-
ing results for the ﬁrst 5 tasks of our PLwF compared to a vanilla
method (LwF [30]).
although the passing of knowledge of previous tasks on to
the model during the learning of a new task through reg-
ularization [30] can reduce the amount of knowledge that
is lost, it can hardly preserve all the knowledge of the old
tasks. Hence, during CL over a long sequence of tasks, the
model becomes less and less certain of its knowledge over
early tasks (as in Figure 1c, Teal column), i.e.,the reliability
of a certain model as the knowledge container of early tasks
gradually dwindles [6]. This means the amount of forgot-
ten knowledge is accumulating (as in Figure 1a), which we
term as the accumulation of catastrophic forgetting . Sec-
ond, when learning a new task, the model is required to bal-
ance the learning of the coming task and the ﬁght against
forgetting. From the perceptive of gradient-based optimiza-
tion [17,27], this creates a tug-of-war game [19], where the
gradients for achieving two objectives conﬂict and impede
the learning of both aspects. In this work, we focus on mit-
igating these two problems.
In a sense, overcoming forgetting by regularization can
be understood as the process of function matching [5] be-
tween the current model function and the previous model
function(s) that contain knowledge on the old tasks [30].
Hence, the essential reason for the accumulation of catas-
trophic forgetting is the fading reliability of the learnable
knowledge space constructed by the previous model func-
tion [6]. In light of this, we propose to densely exploit the
model functions produced in previous tasks to construct thearXiv:2211.15215v1  [cs.CV]  28 Nov 2022

--- PAGE 2 ---
knowledge space. Since this space is based on an ample
number of past experiences [4, 16], the learning process is
more progressive and hence we dub our method Progressive
Learning without Forgetting (PLwF). This has two merits:
(i)the knowledge space now contains the most precise and
fresh knowledge over all the previous tasks, which means
the reliability of the knowledge space over the old tasks will
not fade with increasingly more new tasks; (ii)the full dis-
tribution over labels of different tasks can now be inferred
from the knowledge space that contains all previous model
functions, which is proven effective for learning classiﬁca-
tion models by the strategy of label smoothing [36, 57].
Since the tug-of-war game arises due to the conﬂicting
gradients of learning the new task and overcoming forget-
ting the early tasks [19], we resort to assigning different
credits to the gradients contributed by these two parts such
that the combined gradient can update the model without
hindering the optimization of either objective. For the credit
assignment, we take inspiration from [19, 53] and remove
the conﬂicting part in one of the gradients by projecting it
to the orthogonal direction of the other gradient. Since we
have densely introduced all the previous model functions in
the knowledge space, we enumerate all the possible con-
ﬂict pairs and repeat the conﬂict removal operation in our
credit assignment algorithm. Our approach is also partially
inspired by the biological studies [20,49] which suggest that
during the learning process of new knowledge, the human
brain reduces the rate of synaptic plasticity to avoid the dis-
turbances caused by the new knowledge [10], so as to pre-
serve the learned knowledge.
The contribution of this paper can be summarized as fol-
lows. (i)In order to overcome the problem of fading reli-
ability of the knowledge space, we propose PLwF , where
the previous model functions are densely introduced to the
knowledge space. (ii)We establish the credit assignment
regime in optimization to reconstruct the tug-of-war dynam-
ics in CL, which better balances learning and overcoming
forgetting by determining the degree of stability-plasticity
of individual parameters. (iii)We perform extensive exper-
iments which show the effectiveness of PLwF and credit
assignments. Under the paradigm of CL without involving
any old data, our method achieves notable performance im-
provement on CIFAR-10, CIFAR-100 and Tiny-ImageNet
compared with state-of-the-art methods.
2. Related Work
Regularization-based Continual Learning. These
methods attempt to realize consolidation of the previously
acquired knowledge by extending additional regularization
terms. For example, both EWC [25] and EWC++ [42]
adopts second-order derivatives to measure the sensitivity of
each task parameter and penalize changes in important pa-
rameters speciﬁc to previous tasks Likewise, IMM [28] esti-mates Gaussian posteriors for the task parameters. MAS [1]
redeﬁnes the parameter importance measure as unsuper-
vised settings. SI [56] computes path integrals on the op-
timization trajectory. Besides, R-walk [7], as a general-
ized version of [25] and [56], introduces episode memory.
Moreover, LwF [30] utilizes knowledge distillation to retain
the learned knowledge from previous tasks, which signi-
ﬁes that probability functions outputted by the current task
are constrained. EBLL [46] promotes [30] and encourages
the maintenance of low-dimensional important feature rep-
resentations of previous tasks. However, plain thoughts of
distillation [5] do not mean that no information loss occurs.
Different from those methods described above, this work
reformulate long-sequence learning tasks as a progressive
matching problem for functions to minimize forgetting.
Gradient-based Continual Learning. This category
forces the current task gradient to stay aligned with the
gradient from previously learned tasks to achieve forget-
ting minimization. For instance, both GEM [34] and A-
GEM [9] restrict the update direction of the current task gra-
dient by calculating gradients depending on previous sam-
ples of episode memory. GPM [41] stores the bases from
a gradient subspace of old samples into the memory in a
form of gradient projection memory and updates the gradi-
ent in the orthogonal directions. Similarly, OGD [14] takes
gradient steps in the orthogonal direction of new task and
past task gradients to minimize forgetting. FS-DGPM [12]
further raises ﬂattening sharpness to improve the gradient
projection memory. RGO [32] adopts an iteratively updated
optimizer to modify the gradient, thus providing the model
with the capability of continuous learning. Regarding these
successful gradient solutions, a core element is that a gradi-
ent subspace of the previous tasks can be constructed to sat-
isfy the memory mechanism. Unlike previous methods that
require optimization-based gradient projection along with
raw data, credit assignment is a heuristic method to remove
gradient conﬂict without old data.
Other approaches. (i) Expansion-based methods. Un-
der this category, DEN [52] and HAT [43] dynamically ex-
tend extra components to reduce the interference of new and
old tasks. PNN [40] is immune to forgetting and can lever-
age prior knowledge via lateral connections to previously
learned features. RCL [47] utilizes reinforcement learning
to ﬁnd an optimum structure for sequential tasks. By ex-
tending particular task parameters, APD [51] is able to min-
imize the increase in network complexity. (ii) Rehearsal-
based methods . This category attempts to alleviate the for-
getting via using subsets of previous task examples as mem-
ory cells, such as iCaRL [38] and GSS [2]. Based on differ-
ent sampling strategies, they establish limited budgets in a
memory buffer for rehearsal. And GEM families [9,34] pro-
poses episode memory as the buffer. To get rid of the need
to directly store raw data, recently a series of works elabo-

--- PAGE 3 ---
(b) Ours
(a) Existing workCurrent stage
Previous stagesCurrent knowledge flow
Previous knowledge flowf1=(x;/uni03B8)f2=(x;/uni03B8)f3=(x;/uni03B8)f4=(x;/uni03B8)f1=(x;/uni03B8)f2=(x;/uni03B8)f3=(x;/uni03B8)f4=(x;/uni03B8)
 x
 xFrozenFigure 2. Comparison between the existing work and the proposed method. ( Note that both methods do not get access to data from
previous tasks. ) (a) Existing regularization-based approaches that only leverages the model function in the last task as the knowledge space
for overcoming forgetting. (b) Our PLwF approach that constructs the knowledge space by densely introducing the model functions in the
previous tasks. We use single-headed layout , all tasks share the ﬁnal classiﬁer layer and inference is performed without task identity .
rately construct special subspace of old tasks as the mem-
ory [12, 31, 41] and have shown remarkable performance.
3. Preliminaries
Continual Learning. We consider the CL problem is
comprised of Ttasks, denoted by t= 1;:::;T , withN
examples sampled for each task. We assume that training
instancext, observed in task t, is an i.i.d sample from dis-
tribution Pxt. The key of CL is how the targets ytis chosen,
whereyt2Ytis the targets space. We assume the targets
fortth task isfy1;y2;:::;ytg. Our goal is to learn a pre-
diction function f(;) :RD7 !R, such thatf(;)not
only learns towards ground-truth targets of the current stage
yt, but also targetsfy1;y2;:::;yt 1greceived from the early
tasks. Formally, at tth task, we seek to minimize the follow-
ing objective:
min
XT
t=1ExPxt[L(f(x;);yt)] (1)
Learning without Forgetting. In LwF [30], for each new
task, the goal is to learn a function that maps input xto the
corresponding label yt,i.e.ft(xt;t;#t) =yt, wheret
and#tdenotes parameters of the feature encoder and pre-
dictor. Formally, the optimization objective is deﬁned as:
min
t;#tExPxt[LCE(ft(x;t;#t);yt)] (2)
whereLCEdenotes the Cross-Entropy loss.
For the old task, LwF forces the output probabilities for
each image to be close to the recorded output from the last
task. The KL Divergence is used to encourage the outputs of
one network to approximate the outputs of another network.
This procedure is also referred as function matching [5].Formally, the optimization is deﬁned as
min
t;#tExPxt[LKL(ft(x;t;#t);ft 1(x;t 1;#t 1))]
(3)
Wheret 1and#t 1denotes the optimal set of parame-
ters in the last task and ft 1(x;t 1;#t 1)2Y 1Y 2
:::Y (t 1). As shown in Figure 2a, given its alignment
with the prediction function of the last task, LwF provides
a knowledge space closely correlated to the current task.
Nevertheless, limited by catastrophic forgetting, the last
knowledge space has suffered the evaporation of knowledge
from previous tasks. This results in a deep learner that does
not recall well the knowledge learned in earlier tasks.
4. Progressive Learning without Forgetting
In the protocol of regularization-based methods [29], de-
termining how to progressively cumulate experience in long
task sequences is not intuitive due to the lack of raw data.
To tackle this problem, we propose PLwF.
In CL consisting of Ttasks, for naturally accumulating
experience as shown in Figure 2b, we need to learn a func-
tionft(;t;#t)shared by the majority of parameters across
different tasks. Therefore, the function space Ftcovering
more knowledge is the core. This space carries rich training
signals thanks to the distribution information of labels over
all the previous functions, which is deﬁned as follows.
Deﬁnition 1. Suppose there exists a prediction function
f()for each task, then we deﬁne a function set Ft=
ff1(;1;#1);f2(;2;#2);:::ft 1(;t 1;#t 1)g
where each function f()2 Fthas an encoder and a
predictor#for a given task and is frozen during tth task.
Intuitively, when tis larger than 2, the set beneﬁts form
richer prior knowledge thanks to functions of early tasks.
We describe the set as a space with less vanishing knowl-
edge for previous tasks. When optimizing for PLwF, for the

--- PAGE 4 ---
new task, we follow LwF [42] to learn towards the label yt
as formulated in Equation 3. For the old tasks, we formulate
the optimization objective at tth task as:
min
t;#tXt 1
i=1ExPxt[LKL(ft(x;t;#t);fi(x;i;#i))]
(4)
where function ft(;t;#t)pushes its output probability to
be close to the recorded output from every function in Ft.
Similarly, we use KL Divergence to encourage matching
of probability distributions among all functions. Through
PLwF, we encourage the functions to progressively encode
the similarities between the distribution information over la-
bels in long sequence of tasks. Merely learning from the
outputs of the last task (Figure 2a) can lead to a loss of
knowledge of the earlier tasks, while PLwF (Figure 2b) re-
tains knowledge of earlier tasks, thus reducing the space of
knowledge vanishing.
Relaxation of PLwF. InDeﬁnition 1 , we deﬁne the func-
tion set to contain all previous functions, as illustrated in
Figure 2b. However, this appears to be a strong assumption
to build on. Thus, we propose a relaxation of PLwF to ﬁt
in more scenarios. To be more speciﬁc, a relaxed function
set can be ~FtFt, which includes fewer functions from
previous stages. When applying Equation 4, we would en-
courage the output probability of ft(;t;#t)to be close to
the recorded output from every function in ~Ft. Note that
under this deﬁnition, LwF [42] appears to be a special case
of the relaxed PLwF.
5. Credit Assignment in PLwF
The concept of credit assignment was proposed by [19]
in CL and reﬂects how different parameters are responsi-
ble for expected network behaviors. The standard gradient
method takes a small step along the descending direction to
update the network. In such a process, the gradient inde-
pendently determines whether a change in each parameter
reduces the loss. At this point, credit assignment is entirely
subject to plasticity. Changing settings, such as extending
regularization term and using it as a gradient proxy, may
help strengthen stability, but naive credit assignment can
cause a gradient update with forced tug-of-war dynamics.
Therefore, it is of value to recreate reﬁned tug-of-war dy-
namics through proper credit assignment.
Since we densely introduced all the previous model func-
tions through PLwF, the credit assignment regime needs to
handle more complex tug-of-war dynamics. Suppose an
essence of optimization problem in the PLwF comes from
the tug-of-war of gradients. In an SGD [59] optimizer, pa-
rameters are updated as follows:
i:=i 1 X
trLt() (5)Table 1. Gradient assignment matrix example following the nota-
tions in Deﬁnition 4 .denotes credit assignment measure. Pink
denotesga;gb<0.
g1g2 ...gt 1gt
g11;11;2 ...1;t 11;t
g2HH2;12;2 ...2;t 12;t
... ... ... ... ... ...
gt 1t 1;1XXXt 1;2...t 1;t 1t 1;t
gtt;1t;2 ...XXXt;t 1t;t
However, such a solution is unable to develop reliable credit
assignments in learning process. To this end, we recreate a
regime by modifying the gradient in PLwF, thereby mini-
mizing forgetting. We allow positive interactions between
the gradients of different tasks to overcome the stability-
plasticity dilemma. Inspired by [19, 53], we deﬁne the fol-
lowing conditions to explore credit assignment in PLwF.
Deﬁnition 2. The credit assignment measure is given
by the cosine similarity ga;gb=hga;gbi
kgakkgbk, wheregaand
gbare gradients from two arbitrary tasks.
Deﬁnition 3. If credit assignment measure between two
tasksga;gb<0, then the tug-of-war dynamics exists, oth-
erwise it does not.
To better determine the degree of stability-plasticity of
each parameter and avoid negative changes of gradients, we
deﬁne a gradient assignment matrix Min each iteration.
Deﬁnition 4. Suppose there exists a gradient set G=
fg1;g2;:::; gt;8t2Tg. For two arbitrary elements ga,gb
in the set, there is a measure ga;gbwith the symmetry con-
dition satisﬁed. Then there exists a matrix Mof sizeTT
as the gradient assignment matrix, reﬂecting the credit as-
signment of two arbitrary elements.
The setGcomes from the optimization process of PLwF.
If considering symmetry, ga;gb=gb;ga. Thereby, we
focus on the entries in the upper or lower triangle of Mand
the entries on the diagonal of M. Throughout the entire
process, Deﬁnition 4 investigates the assignment conditions
of each gradient across all tasks. Particularly, the matrix M
could capture the tug-and-war dynamics in each iteration.
To generate reliable credit assignment regime in the op-
timization process, the following steps are performed: (i)
Map the credit assignment measure ga;gbin the setGand
calculate the credit assignment matrix M.(ii)Extract the
pairs of the gradients located above (or below) the main di-
agonal with ga;gb<0(Pink in Table 1) in the matrix M
as a subsetH.(iii)For each pair (ga;gb)of the gradients in
H, replacegaby its projection onto the normal plane of gb:
ga=ga gagb
kgbk2gb.
The above procedure is executed for every iteration dur-
ing optimization. This reduces the degree of disturbance
of gradient applied in per batch of each task towards other
tasks in the batch, thereby reducing the tug-of-war dynam-

--- PAGE 5 ---
1 2 3 4 5
Tasks020406080100Accuracy (%) 
Plain SGD
Single-shot
PLwF
PLwF w/ Credit(a) 5-Split CIFAR-10
1 2 3 4 5 6 7 8 910
Tasks020406080100Accuracy (%) 
Plain SGD
Single-shot
PLwF
PLwF w/ Credit (b) 10-Split CIFAR-100
1234567891011121314151617181920
Tasks020406080100Accuracy (%) 
Plain SGD
Single-shot
PLwF
PLwF w/ Credit (c) 20-Split CIFAR-100
Figure 3. Performance variation on the ﬁrst task when trained over 5 tasks, 10 tasks and 20 tasks on CIFAR-10 and CIFAR-100.
ics. After this, the parameter is updated as:
i:=i 1 X
t(rLt())credit(6)
Where the credit term indicates the regime operator for the
gradient. Through the simple reset, our solution replaces the
original gradients with updated ones and passes them to the
respective optimizer. The experimental results validate the
hypothesis of recreating a careful credit assignment regime
and the improvement in CL capacity intuitively demon-
strates the alleviation of the stability-plasticity dilemma.
6. Experimental Setup
Datasets. Following typical research on class incre-
mental learning [11, 26, 56], we evaluate the performance
of PLwF on CIFAR-10, CIFAR-100 and Tiny-ImageNet.
Speciﬁcally, we split CIFAR-10 into 5 tasks with 2 classes
per task, split CIFAT-100 into 10/20 tasks with 10/5 classes
per task and split Tiny-ImageNet into 10 tasks with 20
classes per task. Specially, we construct short task se-
quences 2-Split CIFAR-10, 2-Split CIFAR-100, and 2-Split
Tiny-ImageNet for empirical analysis of credit assignment,
in which they divide 10, 100, and 200 classes of CIFAR-
10 and CIFAR-100, and Tiny-ImageNet into 2 tasks with
5, 50, and 100 classes per task. For a fair comparison with
different methods, we use all the classes in the same order
to perform experiments.
Methods for Comparison. PLwF strictly follows the
regularization method, we do not store any old samples
from the raw data when learning new classes. Therefore,
we ﬁrst compare our method with several regularization-
based methods: EWC [25], EWC++ [42], MAS [1], and
SI [56]. Additionally, we also compare several rehearsal-
based methods: ER [39], GEM [34], A-GEM [9], FDR [3],
GSS [2], HAL [8], and PODNET [13]. The buffer size
for all rehearsal-based methods is set to 500 following [6].
For the compared methods, we follow the open-source
implementations [6, 23] with accompanying best hyper-
parameters to perform CL.
Architectures and Training Details. Similar to [6, 38],
we employ the ResNet-18 [21] in CIFAR-10 experiments.
In CIFAR-100 and Tiny-ImageNet experiments, we use the
ResNet-32 similar to [58]. To ensure a fair comparison, allmodels are trained with the vanilla-SGD [59] optimizer. For
experiments on CIFAR-10 and CIFAR-100, the network is
trained with the batch size of 128 while the batch size is set
to 32 in Tiny-ImageNet. For all datasets, we train the ini-
tial task with 200 epochs and the remaining task with 250
epochs. All the experiments are performed on 1 NVIDIA
TITAN GPU. Finally, we report average accuracy over all
tasks and the last-task accuracy, the latter of which indicates
the degree of forgetting. For all experiments, we evaluate
and compare our method in single-headed layout where
all tasks share the ﬁnal classiﬁer layer and inference is per-
formed without task identity .
7. Results and Discussions
Controlling Forgetting. As learning continues, the per-
formance of the initial task tends to undergo the most thor-
ough forgetting. To illustrate how PLwF ameliorates forget-
ting, we show the changes in performance on the ﬁrst task as
learning continues on 5-Split CIFAR-10, 10-Split CIFAR-
100 and 20-Split CIFAR-100 in Figure 3. The same trend is
observed in all datasets: (i)Plain-SGD demonstrates utter
catastrophic forgetting due to the standard learning proto-
col, and its performance on the ﬁrst task collapses to zero
when learning the second task; (ii)Speciﬁcally, the single-
shot setting represents a naive learning method which only
adopts the function at the last task as a basic learning space.
This setting beneﬁts from knowledge transferred from the
last task, preserving some performance; (iii)In comparison,
PLwF shows a considerably gentler slope of the forgetting
curve, which shows intransigence towards earlier tasks; (iv)
After credit assignment mediates the tug-of-war dynamics,
forgetting is further controlled, which is reﬂected in the gen-
tler slope of the forgetting curve than PLwF. To sum up, we
control forgetting by using functions before they become
untrustworthy, and by removing gradient conﬂict.
Main Results. Table 2 reports results on all datasets,
which sheds light on the following observations: (i)Fair
comparison with regularization-based methods: In this
setup, any raw data from the past stages are prohibited, the
proposed method achieves state-of-the-art performance on
all datasets. When compared to EWC [25], EWC++ [42],
MAS [1], and SI [56], the gap appears unbridgeable. From
our perspective, this category of methods has its root in the

--- PAGE 6 ---
Table 2. Evaluation results (%) on different datasets. ‘Reg.’ and ‘Reh.’ indicates the regularization-based and rehearsal-based methods.
‘Avg’ and ‘Last’ indicates the average accuracy over all tasks, the last-task accuracy. ‘-’ indicates experiments cannot be performed due to
intractable training time ( e.g.GEM on Tiny-ImageNet).
Techniques 5-Split CIFAR-10 10-Split CIFAR-100 20-Split CIFAR-100 10-Split Tiny-ImageNet
MethodReg. Reh. Avg Last Avg Last Avg Last Avg Last
EWC [25] X 41.00 11.24 30.73 13.98 18.25 6.17 25.36 12.40
EWC++ [42] X 42.02 18.67 25.27 8.93 13.39 3.17 20.81 7.06
MAS [1] X 44.25 22.40 34.42 15.88 18.85 6.98 26.57 10.01
SI [56] X 44.15 19.60 25.78 9.24 16.22 4.71 20.13 7.03
GEM [34] X X 50.12 30.92 28.57 12.25 19.87 9.09 - -
A-GEM [9] X X 47.38 19.68 25.81 9.32 16.16 4.75 22.58 7.99
ER [39] X 62.54 45.7 39.31 19.61 25.43 9.38 26.60 10.19
FDR [3] X 54.34 31.34 40.51 23.07 29.20 15.63 29.67 10.89
GSS [2] X 68.63 45.07 32.90 12.81 22.85 7.19 26.30 9.29
HAL [8] X 62.63 45.05 30.15 12.80 25.46 11.90 18.75 5.99
PODNET [13] X 67.87 45.20 44.18 21.44 28.49 11.51 29.16 13.6
Ours X 66.36 51.15 44.72 28.79 30.65 15.09 30.95 17.02
Accuracy (%)153045607590
Number of Tasks12345678910
20273478910
Strong (100%)
Weak (30%)
Weak (50%)
(a) 10-Split CIFAR-100.
Accuracy (%)51525354555657585
Number of Tasks1234567891011121314151617181920
10162214151617181920
Strong (100%)
Weak (30%)
Weak (50%) (b) 20-Split CIFAR-100
Figure 4. The average accuracy measured by the end of each task
for the relaxed PLwF on Split CIFAR-100.
important parameters from the earlier tasks, whose relia-
bility dwindles at subsequent stages of learning, thereby
resulting in limited performance. (ii)Comparison with
rehearsal-based methods: The proposed method presents
an outstanding performance towards all previous meth-
ods, despite no raw data from any past tasks being stored.
GEM [34] and A-GEM [9] resort to gradients likewise,
but their episode memory present a less satisfactory effect
when evaluating the distribution information of classes. (iii)
Particularly, GSS [2] is slightly better than our method on
CIFAR-10 on Avg, which is attributed to its efﬁcient buffer
strategy. However, our method takes a 6.08% lead on the
Last task. Moreover, on Split CIFAR-100 and Split Tiny-
ImageNet with greater difﬁculty, our method signiﬁcantly
exceeds GSS. GSS presents a decreased performance when
evaluating on challenging tasks. To sum up, the perfor-
mance of the proposed method surpasses the most advanced
method under the protocol of regularization-based methods.
Besides, it also presents better or comparable performance
than rehearsal-based methods even without relying on any
raw data. Remarkably, our method performs more stably on
more complex datasets due to the distribution information
over labels from all the previous functions.
Relaxing PLwF. As indicated in Section 4, we show amore relaxed version of PLwF by adopting a subset of pre-
vious functions ~Ft. To validate the impact of the relaxed
PLwF, we set up the following experiments: (i)Strong as-
sumptions (to use all previous functions in Equation 4). (ii)
Weak assumptions (to use the (t 1)th function and the
ﬁrst 30% or 50% functions from earlier stages in Equation
4). The results in Figure 4 reveal that when adopting the
(t 1)th and the ﬁrst 50% functions, PLwF suffers a per-
formance drop by only 0.54% (Avg) and 0.69% (Avg) on
10 steps and 20 steps setting while reducing computational
overhead by about 40% compared to strong assumptions;
when adopting the (t 1)th and the ﬁrst 30% functions,
PLwF suffers a performance drop by only 1.73% (Avg) and
1.18% (Avg) on 10 steps and 20 steps setting while reducing
computational overhead by about 60% compared to strong
assumptions. Similar ideas [35] are adopted in [35] to boost
the sampling efﬁciency of diffusion models by up to 256
times. This performance-speed trade-off brought by adap-
tively changing the matching complexity could potentially
beneﬁt the application of PLwF.
Model Efﬁciency. In this subsection, we assess the per-
formance of PLwF, PLwF w/ credit and typical CL methods
in terms of model efﬁciency. All experiments are executed
on a server with one NVIDIA TITAN Graphic Card. Fig-
ure 5 reports the training time and the increasing ratio of
training 20 steps over training 10 steps on CIFAR-100. We
draw the following observations about the training time: (i)
In both benchmarks, plain PLwF has a comparable running
time to FDR (Figure 5a). (ii)The computational overhead
of our method is signiﬁcantly lower than rehearsal-based
methods like GSS and GEM. We draw the following ob-
servations about the increasing ratio: (i)As the number of
tasks increases from 10 to 20, the increasing ratio of PLwF
in computation is less than PODNET (135.33%) and GEM

--- PAGE 7 ---
Table 3. Ablation study (%) on different datasets. ‘Avg.’ and ‘Last’ indicates average accuracy over all tasks, last-task accuracy.
Method5-Split CIFAR-10 10-Split CIFAR-100 10-Split Tiny-ImageNet
Avg Last Avg Last Avg Last
Single-shot 57.56 39.51 33.04 15.94 23.36 8.43
vanilla PLwF 64.38(+9.82) 48.07(+8.56) 42.94(+9.9) 25.79(+9.85) 27.69(+7.59) 15.12(+8.59)
PLwF w/ Credit 66.36 (+1.98) 51.15 (+3.08) 44.72 (+1.78) 28.79 (+3.00) 30.95 (+3.26) 17.02 (+1.90)
Table 4. The effects (%) of credit assignment on a short task sequences (2-steps).
2-Split CIFAR-10 2-Split CIFAR-100 2-Split Tiny-ImageNet
MethodAvg Last Avg Last Avg Last
w/o Credit 80.53 70.59 61.55 46.84 33.68 25.82
w/ Credit 81.52 (+0.99) 72.58 (+1.99) 62.21 (+0.66) 48.15 (+1.31) 34.42 (+0.74) 27.17 (+1.35)Table 5. Credit assignment on EWC.
Method5-Split CIFAR-10
Avg Last
EWC 41.00 11.24
w/ Credit 43.98 (+2.98) 19.74 (+8.50)
EWCLwFPODNETFDRPLwFPLwF w/ CreditGSSGEMTime (h)0306090120150
10 Tasks20 Tasks
(a) Training time
EWCLwFPODNETFDRPLwFPLwF w/CreditGSSGEMGrowth ratio (%)0%40%80%120%160%
71.55%
18.98%
3.92%
52.53%
39.21%
135.33%
3.86%
6.96% (b) Increasing ratio
Figure 5. Model efﬁciency on CIFAR-100 (10 steps and 20 steps).
(71.55%) (Figure 5b). (ii)PLwF w/ credit imposes trivial
time overhead (3.92%) even if trained on 20 tasks. These
observations reveal the practicality of PLwF.
Limitations. PLwF progressively transfers knowledge
from previous functions to current stage, while this can re-
sult in the growth of computation when we have increas-
ing number of tasks. We provide brief discussions here on
how to potentially reduce computation, and more can be
found in the Appendix. First, vanilla function matching in-
volves cumbersome repetitive forwarding, and our method
exacerbates the process due to the increase in the number of
matching functions. This suggests that growth can be ame-
liorated by reducing repeated forwarding, e.g.Fast Knowl-
edge Distillation [45] that can speed up 2 4x without com-
promising accuracy in a single-shot matching. In our case,
we could largely speed up the training due to a large num-
ber of tasks. Textbrewer [50] provides a plug-and-play sup-
port for this solution to enable a fast instantiation. Second,
Prune, then Distill [37] reduces computational overhead in
the matching process by pruning models, which can even
further improve the performance in a low-cost manner. In-
depth explorations are left as future work.
8. Empirical Analysis of Method
An Intuitive Explanation of PLwF. To examine the
beneﬁt of adapting previous functions, we experiment on a
special case of the relaxed PLwF — adopting the ﬁrst func-
tion of all previous functions. More experiments on other
choices of functions are detailed in the Appendix. Figure
6 reveals the following observations: (i)The results on the
1 2 3 4 5 6 7 8 910
Tasks020406080100Accuracy (%) 
Single-shot
w/ Progressive(a) Performance variation on the
1st task for 10-step.
1 2 3 4 5 6 7 8 9
Tasks0102030405060Accuracy (%)(b) Results of all tasks at 10th
step.
Figure 6. The effects of adopting the ﬁrst function in PLwF when
trained over 10 tasks on CIFAR-100.
Accuracy (%)010203040
Tasks12345
UntrustworthyRecalled
(a) Random order 1
Accuracy (%)01020304050
Tasks12345
UntrustworthyRecalled (b) Random order 2
Figure 7. The examination of order-agnostic behaviour on 10-Split
CIFAR-100. ‘Recalled’ indicates improved performance of our
method in ﬁrst 5 tasks. ‘Untrustworthy’ indicates performance of
vanilla method (LwF [30] in ﬁrst 5 tasks).
ﬁrst ten classes remains decent throughout ten incremen-
tal steps (the orange line in Figure 6(a)). (ii)At the 10th
step, the result of the ﬁrst ten classes is substantially higher
than other ten classes (the orange column in Figure 6(b)).
We conclude that a deep learner can recall the knowledge
learned in an early task by adopting the corresponding func-
tion, which makes the current function more trustworthy.
Support for PLwF InDeﬁnition 1 , we propose that there
is a space where knowledge disappears less. Firstly, we ab-
late the inﬂuences of the PLwF hypothesis. Table 3 reveals
that the expansion in learning space signiﬁcantly improves
the task performance. To further examine such a hypothe-
sis, different sampling strategies are used to introduce vari-
ous function spaces in which the degree of knowledge dis-
appearance varies. In Figure 8, we present results using
different sampling strategies. Speciﬁcally, regular sampling
represents sampling from the function Fby the set of in-
tervals 2;3;4. Random sampling represents that 2, 3 and 4

--- PAGE 8 ---
Table 6. Results (%) of order-agnostic behavior on 10-Split CIFAR-100, averaged across 4 random seeds. ‘-’ is similar to Table 2
Method EWC EWC++ MAS SI GEM A-GEM ER FDR GSS HAL PODNET Ours
Avg 30.77 24.03 32.99 25.87 - 25.63 39.95 40.77 32.98 29.88 45.22 46.69
Last 15.11 7.09 15.25 9.29 - 9.30 20.13 23.56 12.93 12.79 21.08 27.77
5-Split CIFAR-10 10-Split CIFAR-100
Datasets3540455055606570Average Accuracy (%)62.57
42.4760.73
39.5458.83
37.92intervals w/ 2
intervals w/ 3
intervals w/ 4
(a) Regular sampling
5-Split CIFAR-10 10-Split CIFAR-100
Datasets3540455055606570Average Accuracy (%)66.57
44.1764.9
43.0163.64
42.134 functions
3 functions
2 functions (b) Irregular sampling
Figure 8. Empirical analysis of PLwF using (a) regular sampling
with ﬁxed step sizes and (b) irregular sampling to random sample
2/3/4 previous model functions into the knowledge space.
functions are randomly selected from the function F. In-
tuitively, the function space Frefers to the better knowl-
edge that can be learned when the function of each task is
used. This is proved from the best performance shown in
Table 2 that the function space in this case has the most sig-
niﬁcant CL ability with the least forgetting. As shown in
Figure 8, as the space size got reduced under different sam-
pling strategies, the result of CL showed a downward trend,
reﬂecting the disappearance of more knowledge. These
phenomena support our hypothesis. That means the space
where knowledge disappears less can facilitate PLwF, since
it covers a priori knowledge of more accurate label distri-
butions. In contrast, the improvement in learning ability at
each task signiﬁes that using the PLwF training model is a
good initialization for the subsequent learning. Therefore, a
benign promotion is formed to maximize the expectation of
accumulating experience. More details about the impact of
learning space are given in the Appendix.
Support for Credit Assignment Hypothesis. To
test the credit assignment hypothesis proposed in the
method section, the experiments are conducted on the 5-
Split CIFAR-10, 10-Split CIFAR-100, and 10-Split Tiny-
ImageNet. As shown in Table 3, on the 5-Step and 10-
Step benchmarks, the performance of CL after the incor-
poration of credit assignment obtains an increase of 1.98%,
1.78%, and 3.26% on Avg and 3.08%, 3.00%, and 1.90%
on Last. This indicates that the optimizer is changing to a
direction more favorable to the continual learner, and the
credit assignment successfully mediates stability and plas-
ticity. Moreover, it also shows that the tug-of-war dynamics
in each iteration between different tasks can be well cap-
tured by the gradient assignment matrix to improve the re-
liability of the credit assignment. Furthermore, a short se-
quence of tasks containing only 2 steps, the most funda-
mental puzzle for the credit assignment regime, is specially
designated to observe stability-plasticity. In this case, there
is only one tug-of-war in which the regime must work toTable 7. The beneﬁt of credit assignment on different optimizers,
which is evaluated on 5-Split CIFAR-10.
Optimizer Avg Last
Adamw/o Credit 65.77 49.25
w/ Credit 66.78 (+1.01) 50.16 (+0.91)
Adadeltaw/o Credit 61.76 41.79
w/ Credit 62.00 (+0.24) 42.28 (+0.49)
RMSpropw/o Credit 47.06 24.80
w/ Credit 48.82 (+1.76) 27.86 (+3.06)
improve upon. Therefore, it’s the most essential scenario
for the credit assignment. As shown in Table 4, the perfor-
mance is tangibly improved on each setting, robustly medi-
ating stability and plasticity. In summary, the performance
of our method on tasks of sequences with different lengths
provides good empirical evidence for the credit assignment
assumption that mediates the tug-of-war dynamics.
The Order-agnostic Behaviour. To scrutinize the effect
of changing class orders, we deﬁne several random orders
based on different random seeds to split CIFAR-100, which
yields more dynamic and agnostic class distributions. Ta-
ble 6 reveals that our method still performs well without
involving any old data. Moreover, as shown in Figure 7, in
this case, our method remains effective in controlling for-
getting (Red column). Overall, the impact of the task order
seems insigniﬁcant.
Generality of Credit Assignment. We analyze the gen-
erality of credit assignment from the use of different op-
timzers and from its usefulness on previous CL methods.
Firstly, we analyze the performance of credit assignments
on other optimizers (Adam [24], Adadetla [55], and RM-
Sprop) apart from SGD. We obtain three observations from
Table 7: (i)the tug-of-war dynamics occurs commonly in
gradient-based optimization methods; (ii)the credit assign-
ment regime shows tangible improvement for continuous
learners by alleviating tug-of-war dynamics; (iii)the credit
assignment regime can work together with widely-adopted
gradient-based optimizers; Secondly, we adapt credit as-
signment to previous methods like EWC [25]. Remarkably,
as shown in Table 5, we observe an incredible improvement
by equipping the credit assignment on EWC (+2.98% on
Avg, +8.50% on Last). Combining these observations, we
conclude that recreating carefully the tug-of-war dynamics
is valuable for CL and helps us to better understand CL.

--- PAGE 9 ---
9. Conclusion
In this paper, we propose PLwF, which densely intro-
duces previous functions, creating a learning space with less
vanishing knowledge. Building on the newly-constructed
space, we further minimize forgetting by establishing the
credit assignment regime to recreate the tug-of-war dynam-
ics when learning new tasks. We show PLwF retains faithful
knowledge while requiring neither old samples nor elabo-
rate construction of the old task subspace.
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 139–154, 2018. 1, 2, 5, 6
[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. Advances in neural information processing sys-
tems, 32, 2019. 2, 5, 6
[3] Ari S. Benjamin, David Rolnick, and Konrad P. K ¨ording.
Measuring and regularizing networks in function space.
InInternational Conference on Learning Representations ,
2019. 5, 6
[4] Marcus K Benna and Stefano Fusi. Computational principles
of synaptic memory consolidation. Nature neuroscience ,
19(12):1697–1706, 2016. 2
[5] Lucas Beyer, Xiaohua Zhai, Am ´elie Royer, Larisa Markeeva,
Rohan Anil, and Alexander Kolesnikov. Knowledge distil-
lation: A good teacher is patient and consistent. Proceed-
ings of the IEEE conference on Computer Vision and Pattern
Recognition , 2022. 1, 2, 3
[6] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for gen-
eral continual learning: a strong, simple baseline. Advances
in neural information processing systems , 33:15920–15930,
2020. 1, 5
[7] Arslan Chaudhry, Puneet Kumar Dokania, Thalaiyasingam
Ajanthan, and Philip H. S. Torr. Riemannian walk for in-
cremental learning: Understanding forgetting and intransi-
gence. In Vittorio Ferrari, Martial Hebert, Cristian Smin-
chisescu, and Yair Weiss, editors, Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV) , volume
11215, pages 556–572, 2018. 2
[8] Arslan Chaudhry, Albert Gordo, Puneet K. Dokania, Philip
H. S. Torr, and David Lopez-Paz. Using hindsight to anchor
past knowledge in continual learning. In AAAI , pages 6993–
7001, 2021. 5, 6
[9] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-
gem. arXiv preprint arXiv:1812.00420 , 2018. 1, 2, 5, 6
[10] Joseph Cichon and Wen-Biao Gan. Branch-speciﬁc den-
dritic ca2+ spikes cause persistent synaptic plasticity. Na-
ture, 520(7546):180–185, 2015. 1, 2
[11] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and TinneTuytelaars. A continual learning survey: Defying forgetting
in classiﬁcation tasks. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence , 2021. 5
[12] Danruo Deng, Guangyong Chen, Jianye Hao, Qiong Wang,
and Pheng-Ann Heng. Flattening sharpness for dynamic gra-
dient projection memory beneﬁts continual learning. Ad-
vances in Neural Information Processing Systems , 34, 2021.
2, 3
[13] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs distilla-
tion for small-tasks incremental learning. In ECCV , 2020. 5,
6
[14] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
Orthogonal gradient descent for continual learning. In Inter-
national Conference on Artiﬁcial Intelligence and Statistics ,
pages 3762–3773. PMLR, 2020. 2
[15] Tao Feng, Mang Wang, and Hangjie Yuan. Overcoming
catastrophic forgetting in incremental object detection via
elastic response distillation. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR . IEEE,
2022. 1
[16] Stefano Fusi, Patrick J Drew, and Larry F Abbott. Cascade
models of synaptically stored memories. Neuron , 45(4):599–
611, 2005. 2
[17] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep
learning . MIT press, 2016. 1
[18] Yunhui Guo, Mingrui Liu, Tianbao Yang, and Tajana Ros-
ing. Improved schemes for episodic memory-based lifelong
learning. Advances in Neural Information Processing Sys-
tems, 33:1023–1035, 2020. 1
[19] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan
Pascanu. Embracing change: Continual learning in deep
neural networks. Trends in cognitive sciences , 24(12):1028–
1040, 2020. 1, 2, 4
[20] Akiko Hayashi-Takagi, Sho Yagishita, Mayumi Nakamura,
Fukutoshi Shirai, Yi I Wu, Amanda L Loshbaugh, Brian
Kuhlman, Klaus M Hahn, and Haruo Kasai. Labelling and
optical erasure of synaptic memory traces in the motor cor-
tex.Nature , 525(7569):333–338, 2015. 2
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 5
[22] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
Distilling the knowledge in a neural network. CoRR ,
abs/1503.02531, 2015. 14
[23] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and
Zsolt Kira. Re-evaluating continual learning scenarios: A
categorization and case for strong baselines. In NeurIPS
Continual learning Workshop , 2018. 5
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations , 2015. 8
[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-

--- PAGE 10 ---
ral networks. Proceedings of the national academy of sci-
ences , 114(13):3521–3526, 2017. 1, 2, 5, 6, 8, 12, 15
[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Technical report , 2009.
5
[27] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep
learning. nature , 521(7553):436–444, 2015. 1
[28] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha,
and Byoung-Tak Zhang. Overcoming catastrophic forgetting
by incremental moment matching. In Advances in Neural
Information Processing Systems , pages 4652–4662, 2017. 2
[29] Timoth ´ee Lesort, Andrei Stoian, and David Filliat. Reg-
ularization shortcomings for continual learning. CoRR ,
abs/1912.03049, 2019. 3
[30] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Trans. Pattern Anal. Mach. Intell. , 40(12):2935–2947,
2018. 1, 2, 3, 7
[31] Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang. Trgp:
Trust region gradient projection for continual learning. arXiv
preprint arXiv:2202.02931 , 2022. 3
[32] Hao Liu and Huaping Liu. Continual learning with recursive
gradient optimization. International Conference on Learning
Representations , 2022. 2
[33] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, De-
cebal Constantin Mocanu, Zhangyang Wang, and Mykola
Pechenizkiy. The unreasonable effectiveness of random
pruning: Return of the most naive baseline for sparse train-
ing. ICLR , 2022. 14
[34] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. Advances in neu-
ral information processing systems , 30, 2017. 1, 2, 5, 6
[35] Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Er-
mon, Jonathan Ho, and Tim Salimans. On distillation of
guided diffusion models. CoRR , 2022. 6
[36] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hinton.
When does label smoothing help? Advances in Neural In-
formation Processing Systems , 32, 2019. 2
[37] Jinhyuk Park and Albert No. Prune your model before distill
it.ECCV , 2022. 7, 14
[38] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer
and representation learning. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition , pages
2001–2010, 2017. 2, 5, 12, 15
[39] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-
licrap, and Gregory Wayne. Experience replay for continual
learning. In Advances in Neural Information Processing Sys-
tems, volume 32, 2019. 5, 6
[40] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
CoRR , abs/1606.04671, 2016. 2
[41] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient pro-
jection memory for continual learning. In International Con-
ference on Learning Representations , 2020. 2, 3
[42] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina,
Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pas-
canu, and Raia Hadsell. Progress & compress: A scalableframework for continual learning. In International Confer-
ence on Machine Learning , volume 80, pages 4535–4544,
2018. 2, 4, 5, 6
[43] Joan Serr `a, Didac Suris, Marius Miron, and Alexandros
Karatzoglou. Overcoming catastrophic forgetting with hard
attention to the task. In Proceedings of the 35th International
Conference on Machine Learning , volume 80 of Proceedings
of Machine Learning Research , pages 4555–4564, 2018. 2
[44] Zhiqiang Shen and Marios Savvides. MEAL V2: boosting
vanilla resnet-50 to 80%+ top-1 accuracy on imagenet with-
out tricks. CoRR , abs/2009.08453, 2020. 14
[45] Zhiqiang Shen and Eric Xing. A fast knowledge distillation
framework for visual recognition. ECCV , 2022. 7, 14
[46] Amal Rannen Triki, Rahaf Aljundi, Matthew B. Blaschko,
and Tinne Tuytelaars. Encoder based lifelong learning. In
IEEE International Conference on Computer Vision, ICCV
2017 , pages 1329–1337, 2017. 2
[47] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In
Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen
Grauman, Nicol `o Cesa-Bianchi, and Roman Garnett, editors,
Advances in Neural Information Processing Systems , pages
907–916, 2018. 2
[48] Shipeng Yan, Jiangwei Xie, and Xuming He. DER: dy-
namically expandable representation for class incremental
learning. In IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR , pages 3014–3023. Computer Vi-
sion Foundation / IEEE, 2021. 15
[49] Guang Yang, Cora Sau Wan Lai, Joseph Cichon, Lei Ma,
Wei Li, and Wen-Biao Gan. Sleep promotes branch-
speciﬁc formation of dendritic spines after learning. Science ,
344(6188):1173–1178, 2014. 2
[50] Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che,
Ting Liu, Shijin Wang, and Guoping Hu. TextBrewer:
An Open-Source Knowledge Distillation Toolkit for Natu-
ral Language Processing. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics:
System Demonstrations , 2020. 7
[51] Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju
Hwang. Scalable and order-robust continual learning with
additive parameter decomposition. In International Confer-
ence on Learning Representations , 2020. 2
[52] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
Hwang. Lifelong learning with dynamically expandable net-
works. In International Conference on Learning Represen-
tations , 2018. 2
[53] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,
Karol Hausman, and Chelsea Finn. Gradient surgery for
multi-task learning. Advances in Neural Information Pro-
cessing Systems , 33:5824–5836, 2020. 2, 4
[54] Hangjie Yuan, Jianwen Jiang, Samuel Albanie, Tao Feng,
Ziyuan Huang, Dong Ni, and Mingqian Tang. RLIP: rela-
tional language-image pre-training for human-object inter-
action detection. 2022. 1
[55] Matthew D. Zeiler. ADADELTA: an adaptive learning rate
method. CoRR , abs/1212.5701, 2012. 8
[56] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International

--- PAGE 11 ---
Conference on Machine Learning , pages 3987–3995. PMLR,
2017. 1, 2, 5, 6
[57] Jie Zheng, Andrea GP Schjetnan, Mar Yebra, Bernard A
Gomes, Clayton P Mosher, Suneil K Kalia, Tauﬁk A
Valiante, Adam N Mamelak, Gabriel Kreiman, and Ueli
Rutishauser. Neurons detect cognitive boundaries to struc-
ture episodic memories in humans. Nature neuroscience ,
25(3):358–368, 2022. 2
[58] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan
Zhan. Pycil: A python toolbox for class-incremental learn-
ing. arXiv preprint arXiv:2112.12533 , 2021. 5
[59] Martin Zinkevich. Online convex programming and general-
ized inﬁnitesimal gradient ascent. In Machine Learning, Pro-
ceedings of the Twentieth International Conference (ICML) ,
2003. 4, 5, 12

--- PAGE 12 ---
Supplementary Material
In this supplementary material, we ﬁrst provide more
observations about the credibility of the functions based
on different methods (Appendix A.1). Next, we provide
more intuitive explanations of PLwF from different previ-
ous stages (Appendix A.2). Then, we present detailed dis-
cussion about limitations (Appendix A.3). And we provide
more versions of relaxed PLwF (Appendix A.4). Moreover,
we analyze the GPU occupation of PLwF (Appendix A.5).
Then, we provide additional experiments and discuss the in-
ﬂuence brought by the order-agnostic behaviour and more
details about main results (Appendix A.6 and A.8). Next,
we present more discussions about the generality of credit
assignment (Appendix A.7). Finally, we discuss trendy di-
rections in Raw-Data-Free methods (Appendix A.9). Code
will be publicly available upon publication.
A.1 More observations about the credibility of the
functions
In this subsection, we conduct more extensive experi-
ments on EWC [25] and iCaRL [38], aiming to prove that
the problem of fading credibility of a given function is pre-
vailing in regularization-based and memory-based methods.
This observation motivates PLwF.
Figure 9 shows how a single function can lose credibility.
We observe that even though EWC [25] and iCaRL [38] pre-
vent forgetting to different extent based on different meth-
ods, they still show the same trends: as incremental learn-
ing proceeds, the function forgets more and more about the
stages they should be responsible for . As shown in Fig-
ure 9, although EWC and iCaRL retain more superior per-
formance (Orange line) compared to Plain-SGD [59] (Blue
line), forgetting is still accumulating. Compared to iCaRL,
EWC is the worse one. This indicates that the function be-
comes untrustworthy for the ﬁrst task as the sequence of
tasks increases. (Although we exemplify our idea using the
ﬁrst task, other tasks can show similar trends.)
Figure 10 reveals the relationship between the reliability
of the function at the early task and at the current task. As
shown in Figure 10, we observe a trend that more perfor-
mance is retained for the function closer to the current task
(task 10 in Figure 10 (a) and Figure 10 (b)), e.g. the per-
formance of function from task 9 (nearest to the task 10) is
the least forgotten. Conversely, less performance is retained
for the function further away from the current task, e.g.the
performance of function from task 1 (farthest to the task 10)
is the most forgotten. This indicates that the function of the
early task gradually loses its reliability.
Based on these observations, we can conclude that
changes of the knowledge space caused by the fading re-
liability of the function is an issue that needs to be carefully
considered during CL. This provides more support for the
proposed PLwF method.
1 2 3 4 5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Plain SGD
EWC(a) EWC
1 2 3 4 5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Plain SGD
iCaRL
(b) iCaRL
Figure 9. Performance variation of EWC and iCaRL on the
ﬁrst task when trained over 10 tasks on CIFAR-100. Plain SGD
demonstrates utterly forgetting due to the standard learning proto-
col.
1 2 3 4 5 6 7 8 9 10
Task020406080100Accuracy (%)EWC
(a) EWC
1 2 3 4 5 6 7 8 9 10
Task020406080100Accuracy (%)iCaRL
(b) iCaRL
Figure 10. Performance variation of EWC and iCaRL on each task
when trained over 10 tasks on CIFAR-100.
A.2 More intuitive explanations of PLwF
As shown in Figure 11, to examine the beneﬁt of adapt-
ing previous functions, we observe the effect of matching
functions from different distances in PLwF. We speciﬁcally
use only one function from the early 5 tasks as a match-
ing function to perform CL. Figure 11 reveals the following
observations: (i.) In Figure 11 (a, c, e, g, i), when one
function from early task is applied as a matching function,
the accumulation of forgetting at that task decreases sig-
niﬁcantly as learning continues (Orange line). This indi-
cates that deep learner recalls the knowledge learned in that

--- PAGE 13 ---
1 2 3 4 5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Single-shot
w/ Progressive(a) Performance variation on the 1st task if
only matching the function from the 1st task.
1 2 3 4 5 6 7 8 9
Task020406080100Accuracy (%)(b) Results of all tasks using PLwF if only
matching the function from the 1st task.
2 3 4 5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Single-shot
w/ Progressive
(c) Performance variation on the 2nd task if
only matching the function from the 2nd task.
1 2 3 4 5 6 7 8 9
Task020406080100Accuracy (%)(d) Results of all tasks using PLwF when
matching the function from the 2nd task.
3 4 5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Single-shot
w/ Progressive
(e) Performance variation on the 3rd task if
only matching the function from the 3rd task.
1 2 3 4 5 6 7 8 9
Task020406080100Accuracy (%)(f) Results of all tasks using PLwF when
matching the function from the 3rd task.
4 5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Single-shot
w/ Progressive
(g) Performance variation on the 4th task if
only matching the function from the 4th task.
1 2 3 4 5 6 7 8 9
Task020406080100Accuracy (%)(h) Results of all tasks using PLwF when
matching the function from the 4th task.
5 6 7 8 9 10
Task020406080100Accuracy on the 1st Task (%) 
Single-shot
w/ Progressive
(i) Performance variation on the 5th task if only
matching the function from the 5th task.
1 2 3 4 5 6 7 8 9
Task020406080100Accuracy (%)(j) Results of all tasks using PLwF when
matching the function from the 5th task.
Figure 11. The effects of function from different distances in PLwF when trained over 10 tasks on CIFAR-100.
early tasks, which makes current function more trustworthy.
(ii.) Compared to other tasks whose corresponding match-
ing functions are not adopted, the task whose corresponding
matching function is adopted is signiﬁcantly less forgotten
(Orange column in Figure 11 (b, d, f, h, j)). This indicates
that a function from early tasks without knowledge fadingis essential.
In summary, these experiments provide more support for
the reliability of the functions from early tasks. Building
on these, we conjecture that: the absence of a function at
a speciﬁc stage makes CL biased against this old task . In
contrast, provided absent functions from the early tasks, a

--- PAGE 14 ---
deep learner could recall the knowledge learned.
A.3 More discussion about limitations
In the main paper, we discuss the limitations of the pro-
posed method. As a complement, we provide detailed dis-
cussions and evidence on two method options for reduc-
ing computational complexity to ensure the applicability of
PLwF.
[Method A] Fast Knowledge Distillation [45] (FKD).
In the following paragraphs, we aim to investigate a possi-
ble approach to boost the training efﬁciency of PLwF with
the assistance of FKD .
PLwF adopts vanilla KD [22] as a default method to
transfer knowledge from previous functions to the current
model. The main drawback of a vanilla KD framework is
that it consumes the majority of the computational over-
head on forwarding through the giant teacher model. To
be more speciﬁc, the parameters of the teacher model is
frozen, making repetitive forwarding on the teacher model
redundant in training. While FKD, to some extent, solve
the problem. FKD generates one probability vector as the
soft label for each training image, then reuse them circu-
larly for different training epochs. Efﬁciently reduce repeti-
tive forward computations to speed up 25× without com-
promising accuracy . For example, Table 8 reveals that by
employing the same hyper-parameters and teacher network
(Resnet-50), FKD [45] achieves similar results to a base-
line KD method [44] while greatly accelerating the training.
(Results are borrowed from [45].)
Table 8. Results on FKD with ResNet-50. ~represents the train-
ing using cosine lr and 1.5x epochs.
Method Network Top-1 Top-5 Speed-up
Baseline KD [44] ResNet-50 80.67 95.09 1.0
w/ FKD ResNet-50 80.70 95.13 0.3x
w/~FKD ResNet-50 80.91 95.39 0.5x
With regard to the combination of PLwF and FKD, since
PLwF densely introduces previous functions into the cur-
rent stage of incremental learning, we could largely speed
up the training due to the large number of functions ( i.e.
teacher models). We expect that contributions will follow.
[Method B] Prune, then Distill [37]. Another intuitive
solution to reduce computational overhead is to boost the
inference speed of model functions. Prune, then Distill
provides a good support for this solution. We analyze the
feasibility in terms of both inference speed and accuracy.
Inference speed & Accuracy: The desirable result of
Method B is to reduce the computational overhead while en-
suring uncompromised accuracy. In Table 9, FKD [37] has
demonstrated pruning models would not negatively impact
the inference speed and accuracy. Furthermore, FKD [37]even helps student models achieve better performance. (Re-
sults in Table 9 are borrowed from [37].
Table 9. The effect of Prune, then Distill. Teacher “None” indi-
cates the student is trained without a teacher, while the pruning
ratio “None” means the distillation from the unpruned teacher.
Teacher Pruning ratio Accuracy Student Accuracy
None - - ResNet18 57.75 ± 0.24
ResNet18 None 57.75 ResNet18 57.97 ± 0.10
ResNet18 36% 57.66 ResNet18 59.39 ± 0.21
ResNet18 59% 57.58 ResNet18 58.99 ± 0.26
ResNet18 79% 57.32 ResNet18 59.33 ± 0.18
In addition, we could utilize more superior pruning
methods to boost the performance. For example, [33] pro-
posed that a randomly pruned subnetwork of ResNet can
outperform a dense ResNet. To sum up, in our case, the
problem of high computational overhead would be decently
resolved by Prune, then Distill.
A.4 Discussion the relaxed PLwF
We provide several relaxed versions of PLwF by adopt-
ing a subset of previous functions ~F:(i)Scheme 1 (Fig-
ure 15 and Table 13): Use the ( t 1)th function and
ﬁrst 30%/50% functions from earlier stages in Equation 4.
(ii)Scheme 2 : (Figure 16 and Table 14): Solely use ﬁrst
30%/50% functions from earlier stages in Equation 4. (iii)
Scheme 3 : (Figure 17 and Table 15): Use functions sampled
with a stage interval of 2, 3 and 4 in Equation 4. (iv)Scheme
4: (Figure 18 and Table 16): Use 2, 3 and 4 randomly se-
lected functions in Equation 4.
All of the above schemes substantially reduce the com-
putational overhead, which enhances the application of the
proposed method. Among them, Scheme 1 considers that
previous functions have different inﬂuences on the opti-
mization of the current task, for example, more recent func-
tions suffer from slighter forgetting, and earlier functions
suffer from severer forgetting. Therefore, we could sam-
ple more recent functions to achieve decent performance.
In contrast, Scheme 2 abandons more recent functions and
thus, appears a less cost-effective strategy. While Scheme
3andScheme 4 are affected by different sampling strate-
gies, thus a good sampling strategy needs to be considered
in practice. In summary, by proper sampling functions (re-
laxed PLwF), we could maintain the computational over-
head of PLwF to be constant, while ensuring decent perfor-
mance. Such a scheme potentially beneﬁts the application
of PLwF.
A.5 GPU occupation of PLwF and Relaxed PLwF
In this subsection, we present the GPU occupation of
PLwF and Relaxed PLwF. In Figure 12a, we speciﬁcally

--- PAGE 15 ---
compare to dynamically expandable representation method
(DER [48]). We observe that the GPU occupancy and
growth rate are signiﬁcantly lower than DER, even though
PLwF densely introduced the previous functions. More-
over, as shown in Figure 12b, we present the detailed GPU
utilization for scheme 1 (Figure 15 and Table 13 ). Fig-
ure 12b reveals that we could maintain the computational
overhead of PLwF to be constant.
GPU occupation (Mb)010203040
Task12345678910
LwF
PLwF
DER
(a) PLwF
GPU occupation (Mb)05101520
Task12345678910
LwF
Relaxed PLwF
Relaxed PLwF
(b) Relaxed PLwF
Figure 12. GPU occupation of PLwF and Relaxed PLwF on Split
CIFAR-100. (b) Relaxed PLwF (green) indicates the GPU occu-
pancy of Scheme 1 (30%) and Relaxed PLwF (gray) indicates the
GPU occupancy of Scheme 1 (50%).
A.6 More details about main results
To better understand the actual advantage of the pro-
posed method, we present the average results over sev-
eral stages of learning on 10-Split CIFAR-100 and 20-Split
CIFAT-100. As shown in Figure 13, PLwF consistently out-
performs all the methods by signiﬁcant margins across all
settings.
A.7 More discussion about the generality of credit
assignment
InGenerality of Credit Assignment of the main pa-
per, we present the results of adding credit assignment on
EWC [25], and brings tangible improvements. Further, we
observe the impact of credit assignment on iCaRL [38]. In
Table 10, we present the results of adding credit assignment
on iCaRL, which improves the performance of iCaRL by
1.14% on Avg and 2.08% on Last. In Table 12, we present
the performance variation of iCaRL on the ﬁrst task, which
shows forgetting is further controlled. This is a valuable
Accuracy (%)0255075100
Number of Tasks12345678910
EWC
EWC++
MAS
SI
PLwF
Naive Rehearsal
GEM(a) 10-Split CIFAR-100.
Accuracy (%)0255075100
Number of Tasks1234567891011121314151617181920
EWC
EWC++
MAS
SI
PLwF
Naive Rehearsal
GEM
(b) 20-Split CIFAR-100.
Figure 13. Incremental learning with 5 and 10 classes at a time on
Split CIFAR-100.
phenomenon since the tug-of-way dynamics commonly ex-
ists in previous methods, and the credit assignment can
recreate this conﬂict. We expect the credit assignment will
inspire future work to focus on this problem and contribu-
tions will follow.
Table 10. Credit assignment on iCaRL.
Method5-Split CIFAR10
Avg Last
iCaRL 79.21 69.00
w/ Credit 80.35 (+1.14) 71.08 (+2.08)
A.8 Details about the order-agnostic behaviour
To observe the inﬂuence of different class orders, we set
different random seeds to split CIFAR-100 dataset, which
yields a more dynamic and agnostic class distributions. As
shown in Table 11, the proposed method outperforms other
methods using three different random seeds. This further
validates the robustness of our method. Moreover, as shown
in Figure 14(a, b, c), in this case, our method remains effec-
tive in controlling forgetting (Orange line). These observa-
tions provide additional support for our method.
A.9 Trendy directions in raw-data-free methods
In some applications, storing raw data is not feasible due
to privacy and security concerns, and this requires CL meth-
ods to maintain reasonable performance without any raw
data. The regularization-based method is one line of this
approach.

--- PAGE 16 ---
Table 11. Evaluation results (%) of order-agnostic behavior on different seeds.
Random Method EWC EWC++ MAS SI GEM A-GEM ER FDR GSS HAL PODNET Ours
Order 1Avg 29.82 23.68 33.82 26.08 - 25.93 40.89 40.20 33.85 28.63 46.95 49.37
Last 13.72 7.18 15.97 9.24 - 9.68 21.27 22.45 13.50 12.12 22.25 29.57
Order 2Avg 31.10 23.85 32.38 25.90 - 24.89 39.88 40.99 33.10 30.73 44.24 47.14
Last 16.88 6.69 14.80 9.32 - 9.29 20.02 23.55 12.88 13.05 19.80 27.20
Order 3Avg 31.44 23.30 31.32 25.70 - 25.87 39.72 41.38 32.08 30.00 45.50 45.54
Last 15.85 5.54 14.34 9.37 - 8.89 19.62 25.18 12.53 13.19 20.83 25.50
Table 12. Performance variation (%) of iCaRL on 1st task when trained over 1 task to 5 tasks on 5-Split CIFAR-10.
Method Task 1 Task 2 Task 3 Task 4 Task 5
iCaRL 97.60 85.90 84.80 82.90 54.75
w/ Credit 97.60 (+0) 87.30 (+1.4) 86.05 (+1.25) 83.50 (+0.6) 58.95 (+4.2)
1 2 3 4 5 6 7 8 910
Tasks020406080100Accuracy (%) 
Single-shot
Ours
(a) Random order 1
1 2 3 4 5 6 7 8 910
Tasks020406080100Accuracy (%) 
Single-shot
Ours
(b) Random order 2
1 2 3 4 5 6 7 8 910
Tasks020406080100Accuracy (%) 
Single-shot
Ours
(c) Random order 3
Figure 14. Performance variation of the ﬁrst task on different seeds
when trained over 10 tasks on Split CIFAR-100.
PLwF points out that the regularization-based method is
limited by the incomplete learnable space due to the fad-
ing function credibility. In contrast, the rehearsal-based
methods suffers from a slighter credibility crisis of the
function due to direct exposure to raw data. PLwF pro-
vides a potential direction for regularization-based meth-
ods: Finding a knowledge container with the most precise
and fresh knowledge over the previous tasks. Although
rehearsal-based methods generally achieve better perfor-
mance than regularization-based methods, PLwF paves a
way for regularization-based methods (raw-data-free meth-
ods) to thrive again.

--- PAGE 17 ---
Accuracy (%)153045607590
Number of Tasks12345678910
20273478910
Strong (100%)
Weak (30% + (t-1))
Weak (50% + (t-1))Figure 15. The ﬁrst 30%/50% and ( t 1)th functions fromFTable 13. The ﬁrst 30%/50% and ( t 1)th functions fromF
Equation 4 Avg Last Cost
Strong (100%) 42.94 25.79 -
Weak (50% + ( t 1)) 42.40 (#0.54) 23.37 (#2.42)#40%
Weak (30% + ( t 1)) 41.21 (#1.73) 21.07 (#4.72)#60%
Accuracy (%)153045607590
Number of Tasks12345678910
18263478910
Strong (100%)
Weak (only 30%)
Weak (only 50%) 
Figure 16. The ﬁrst 30%/50% functions from FTable 14. The ﬁrst 30%/50% functions from F
Equation 4 Avg Last Cost
Strong (100%) 42.94 25.79 -
Weak (50%) 41.55 (#1.39) 21.60 (#4.00)#50%
Weak (30%) 38.94 (#4.00) 18.76 (#7.03)#70%
Accuracy (%)153045607590
Number of Tasks12345678910
16253478910
Strong (100%)
Shortcut w/2
Shortcut w/3
Shortcut w/4
Figure 17. The shortcut with interval 2, 3, 4 from FTable 15. The shortcut with interval 2, 3, 4 from F
Equation 4 Avg Last Cost
Strong (100%) 42.94 25.79 -
Shortcut w/2 39.34 (#3.6) 23.10 (#2.78)#50%
Shortcut w/3 36.41 (#6.53) 18.14 (#7.65)#70%
Shortcut w/4 35.24 (#7.70) 17.78 (#8.01)#80%
Accuracy (%)153045607590
Number of Tasks12345678910
16253478910
Strong (100%)
Random w/2
Random w/3
Random w/4
Figure 18. Random selection of 2, 3, 4 functions from FTable 16. Random selection of 2, 3, 4 functions from F
Equation 4 Avg Last Cost
Strong (100%) 42.94 25.79 -
Random w/4 41.86 (#1.08) 22.44 (#3.35)#60%
Random w/3 39.82 (#3.12) 19.41 (#6.38)#70%
Random w/2 38.72 (#4.22) 17.43 (#8.36)#80%
