# 2110.03215.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2110.03215.pdf
# Kích thước tệp: 1951536 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
HƯỚNG TỚI VIỆC HỌC KIẾN THỨC LIÊN TỤC CỦA
CÁC MÔ HÌNH NGÔN NGỮ

Joel Jang1Seonghyeon Ye1Sohee Yang1Joongbo Shin2
Janghoon Han2Gyeonghun Kim2Stanley Jungkyu Choi2Minjoon Seo1
1KAIST AI2LG AI Research
fjoeljang,vano1205,sohee.yang,minjoon g@kaist.ac.kr
fjb.shin,janghoon.han,ghkayne.kim,stanleyjk.choi g@lgresearch.ai

TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LM) được biết đến với khả năng mã hóa kiến thức thế giới trong các tham số của chúng khi được tiền huấn luyện trên một lượng lớn kho dữ liệu web, điều này thường được sử dụng để thực hiện các tác vụ phụ thuộc kiến thức như trả lời câu hỏi, kiểm tra sự thật và đối thoại mở. Trong các tình huống thực tế, kiến thức thế giới được lưu trữ trong các LM có thể nhanh chóng trở nên lỗi thời khi thế giới thay đổi, nhưng việc tránh quên thảm khốc và có được kiến thức mới một cách đáng tin cậy trong khi bảo tồn kiến thức bất biến là không đơn giản. Để thúc đẩy cộng đồng hướng tới việc duy trì tốt hơn các LM luôn thay đổi, chúng tôi xây dựng một vấn đề học liên tục (CL) mới gọi là Học Kiến thức Liên tục (CKL). Chúng tôi xây dựng một bộ đánh giá và chỉ số mới để định lượng việc duy trì kiến thức thế giới bất biến theo thời gian, việc cập nhật kiến thức lỗi thời và việc thu thập kiến thức mới. Chúng tôi áp dụng các phương pháp gần đây có thể áp dụng từ tài liệu để tạo ra nhiều đường cơ sở mạnh. Thông qua các thí nghiệm mở rộng, chúng tôi thấy rằng CKL thể hiện những thách thức độc đáo không được giải quyết trong các thiết lập CL trước đây, nơi mà việc mở rộng tham số là cần thiết để duy trì và học kiến thức một cách đáng tin cậy đồng thời. Bằng cách làm nổi bật các nguyên nhân quan trọng của việc quên kiến thức, chúng tôi cho thấy CKL là một vấn đề thách thức và quan trọng giúp chúng ta hiểu rõ hơn và huấn luyện các LM luôn thay đổi. Các bộ dữ liệu đánh giá, điểm kiểm tra mô hình và mã để tái tạo kết quả của chúng tôi có sẵn tại URL này.

1 GIỚI THIỆU
Các nghiên cứu gần đây đã cho thấy rằng các Mô hình Ngôn ngữ lớn (LM), như T5 (Raffel et al., 2019) và GPT-3 (Brown et al., 2020), có khả năng lưu trữ một lượng kiến thức thế giới khổng lồ trong các tham số của chúng khi được tiền huấn luyện trên một kho dữ liệu văn bản rộng lớn (Petroni et al., 2019). Các LM được tiền huấn luyện này đã cho thấy tiềm năng phục vụ như các cơ sở kiến thức khi được thăm dò kiến thức thế giới mà không cần bất kỳ tinh chỉnh nào thông qua tác vụ Phân tích Mô hình Ngôn ngữ (LAMA) (Petroni et al., 2019), tác vụ này yêu cầu thăm dó các LM cho kiến thức thế giới theo cách zero-shot thông qua việc điền vào chỗ trống, và các kết quả đầy hứa hẹn sử dụng kiến thức thế giới được mã hóa khi được tinh chỉnh trên các Tác vụ Ngôn ngữ Chuyên sâu Kiến thức khác nhau (KILT) (Petroni et al., 2021), ví dụ như trả lời câu hỏi, đối thoại mở có kiến thức.

Trong khi kiến thức thế giới được lưu trữ trong các LM có nhiều trường hợp sử dụng đa dạng, nó có thể nhanh chóng trở nên lỗi thời khi thế giới thay đổi nhanh chóng, và các LM cần thường xuyên làm mới kiến thức thế giới nội bộ của chúng cho phù hợp. Ví dụ, không thể thăm dò thông tin mới như " đã giành chiến thắng trong Bầu cử Tổng thống Mỹ 2020 " từ T5 ban đầu (Raffel et al., 2019) được tiền huấn luyện trên kho dữ liệu web C4 từ tháng 4 năm 2019.1 Ngoài ra, thông tin có thể đã từng được coi là chính xác có thể không còn hợp lệ nữa vì thông tin đã được cập nhật. Chẳng hạn, câu trả lời cho " Cristiano Ronaldo chơi cho đội bóng đá nào? " đã thay đổi từ Juventus thành Manchester United vào tháng 9 năm 2021. Trong khi đó, thông tin bất biến theo thời gian được học từ kho dữ liệu gốc như " Barack Obama sinh ra ở Honolulu, Hawaii " không nên bị thay đổi trong các LM.

1T5 ban đầu được tiền huấn luyện trên bộ dữ liệu C4 (khoảng 750 GB), là một bản sao được làm sạch của Common Crawl được trích xuất từ web vào tháng 4 năm 2019.

--- TRANG 2 ---
Tiền Huấn luyện LM (tiếp tục)
GPT-2
T5LM ngẫu nhiênTiền Huấn luyện LMLM được Tiền Huấn luyệnLM được Tiền Huấn luyện Liên tục
GPT-2
T5
GPT-2
T5!!!"LAMA BẤT BIẾNLAMA CẬP NHẬTLAMA MỚIBộ Đánh giá CKL

Hình 1: Tổng quan về bộ đánh giá HỌC KIẾN THỨC LIÊN TỤC. LAMA BẤT BIẾN được sử dụng để đo lường kiến thức thế giới bất biến theo thời gian có được từ D0. LAMA CẬP NHẬT được sử dụng để đo lường việc cập nhật kiến thức thế giới từ D0→D1. LAMA MỚI được sử dụng để đo lường kiến thức thế giới mới có được từ D1.

Mặc dù tầm quan trọng của nó, thách thức làm mới kiến thức thế giới nội bộ được lưu trữ trong các tham số của LM là không đơn giản và chỉ được khám phá trong các thiết lập khá cụ thể. Ví dụ, các nghiên cứu gần đây đã đề xuất sửa đổi kiến thức mục tiêu cụ thể như các sự kiện riêng lẻ (De Cao et al., 2021; Zhu et al., 2020; Dai et al., 2021). Dhingra et al. (2021) đã giải quyết các LM như các cơ sở kiến thức thời gian bằng cách mô hình hóa chung văn bản với dấu thời gian của nó. Nhưng vấn đề làm mới kiến thức thế giới của các LM theo cách tổng quát và có thể mở rộng hơn, chẳng hạn như thông qua tiền huấn luyện liên tục trên một kho dữ liệu với kiến thức mới, chưa được xây dựng chính thức hoặc khám phá bởi các nghiên cứu trước đây.

Hơn nữa, cộng đồng thiếu một bộ đánh giá có thể được sử dụng để nghiên cứu có hệ thống cách kiến thức nội bộ của các LM thay đổi thông qua việc huấn luyện trên thông tin mới. Cuối cùng, các phương pháp để làm mới kiến thức của các LM một cách hiệu quả ở quy mô lớn vẫn chưa được khám phá kỹ lưỡng.

Trong nghiên cứu này, chúng tôi đề xuất một công thức học liên tục (CL) mới có tên HỌC KIẾN THỨC LIÊN TỤC (CKL), nơi chúng tôi cố gắng làm mới kiến thức thế giới nội bộ của các LM thông qua tiền huấn luyện liên tục trên các kho dữ liệu mới. Chúng tôi phân loại có hệ thống kiến thức thế giới thành ba danh mục chính và tạo ra các bộ dữ liệu đánh giá để đo lường từng loại trong quá trình CKL: (1) LAMA BẤT BIẾN cho kiến thức thế giới bất biến theo thời gian trong các LM không nên bị quên hoặc thay đổi, (2) LAMA CẬP NHẬT cho kiến thức thế giới lỗi thời cần được cập nhật trong các LM, và (3) LAMA MỚI cho kiến thức thế giới mới nên được tiêm vào các LM. Chúng tôi cũng đề xuất một chỉ số mới có tên FUAR (TỈ LỆ QUÊN / (CẬP NHẬT + THU THẬP)) có thể đo lường sự đánh đổi giữa quên, cập nhật và thu thập kiến thức. Cuối cùng, trong khi người ta có thể nghĩ đến việc triển khai các phương pháp CL đương đại cho bộ đánh giá này, chúng tôi cho thấy CKL có những khác biệt không đơn giản với các công thức CL truyền thống và yêu cầu các phương pháp cụ thể cho CKL. Chúng tôi tìm thấy và so sánh các kiến trúc mô hình và phương pháp huấn luyện (Chen et al., 2020; He et al., 2021; Hu et al., 2021; Wang et al., 2021b) từ tài liệu đã cho thấy tiềm năng giảm thiểu việc quên kiến thức có được trong quá trình tiền huấn luyện, thiết lập chúng như các đường cơ sở cho bộ đánh giá CKL.

Tóm lại, trong khi thách thức làm mới kiến thức thế giới nội bộ của các LM là thiết yếu trong các tình huống thực tế, nó vẫn chưa được xây dựng hoặc khám phá rộng rãi. Do đó, trong bài báo này:

• Chúng tôi đề xuất một công thức CL mới gọi là HỌC KIẾN THỨC LIÊN TỤC (CKL) và xây dựng một bộ đánh giá mới để đo lường lượng quên và lượng kiến thức thế giới có được bằng cách tiếp tục tiền huấn luyện trên một kho dữ liệu mô hình ngôn ngữ mới mà chúng tôi xây dựng, chứa kiến thức mới.

• Chúng tôi khám phá các kiến trúc LM và phương pháp huấn luyện là các đường cơ sở tự nhiên cho CKL trong tài liệu, ký hiệu chúng là các phương pháp CKL, và thực hiện các thí nghiệm mở rộng trên bộ đánh giá CKL của chúng tôi. Chúng tôi phân loại chúng thành các phương pháp điều chỉnh, luyện tập lại và mở rộng tham số, giống như trong tài liệu CL truyền thống, và so sánh hiệu quả của từng loại phương pháp bằng cách sử dụng một chỉ số mới có tên FUAR mà chúng tôi đề xuất để đo lường sự đánh đổi giữa kiến thức bị quên và kiến thức được cập nhật hoặc thu thập.

• Hướng tới việc tạo ra một LM luôn thay đổi, chúng tôi thực hiện phân tích mở rộng trong bộ đánh giá CKL và làm nổi bật những thách thức và phát hiện quan trọng: các phương pháp mở rộng tham số có hạn chế về việc không hiệu quả bộ nhớ mặc dù thực hiện tốt nhất trong hầu hết các thí nghiệm của chúng tôi và việc nhìn thấy cùng một dữ liệu lặp đi lặp lại trong quá trình tiếp tục tiền huấn luyện là nguyên nhân quan trọng của việc quên. Ngoài ra, chúng tôi cho thấy những kết quả thú vị cần khám phá thêm: tỷ lệ học có thể được thay đổi để cân bằng việc quên và học kiến thức mới, CKL có thể giúp thực hiện các tác vụ chuyên sâu kiến thức trước đó sau khi thu thập kiến thức thế giới mới, và các phương pháp CKL có thể chuyển giao qua các kiến trúc LM mặc dù cho thấy xu hướng khác nhau trong hiệu suất.

Một tổng quan về bộ đánh giá CKL được đề xuất được hiển thị trong Hình 1.

2 NGHIÊN CỨU LIÊN QUAN

Các Mô hình Ngôn ngữ (LM) sử dụng kiến thức từ các nguồn bên ngoài, như Tạo sinh Tăng cường Truy xuất (RAG) (Lewis et al., 2020a) và Blender Bot 2.0 (Xu et al., 2021; Komeili et al., 2021), đối phó với thế giới thay đổi bằng cách cập nhật các nguồn bên ngoài trong quá trình suy luận hoặc tìm kiếm internet để truy xuất thông tin gần đây. Tuy nhiên, các nghiên cứu gần đây đã cho thấy rằng những mô hình tăng cường bộ nhớ này gặp phải vấn đề ảo giác, có nghĩa là chúng trình bày thông tin sai như thể nó đúng, mặc dù được cung cấp kiến thức cập nhật trong quá trình suy luận (Zhang & Choi, 2021), điều này trở nên tồi tệ hơn khi kích thước của LM tăng (Longpre et al., 2021), khiến việc làm mới các tham số ẩn trở nên quan trọng hơn nữa.

Để làm mới kiến thức nội bộ của các LM, người ta có thể xem xét tiền huấn luyện các LM từ đầu với một kho dữ liệu văn bản được cập nhật mới có quy mô tương tự như kho dữ liệu được sử dụng trong quá trình tiền huấn luyện ban đầu, chẳng hạn như một bản sao gần đây của toàn bộ Wikipedia. Tuy nhiên, phương pháp này đòi hỏi tính toán cao và cũng có hại cho môi trường (Patterson et al., 2021). Một phương pháp thay thế khác là tiếp tục quá trình tiền huấn luyện trên một kho dữ liệu nhỏ hơn nhiều chứa kiến thức thế giới mới, nhưng phương pháp như vậy được biết là gặp phải vấn đề quên thảm khốc (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017), nơi các mô hình quên kiến thức đã học trước đó khi chúng thu thập kiến thức mới.

Lazaridou et al. (2021); Jin et al. (2021) đề xuất triển khai các phương pháp Học Liên tục (CL) trước đây (Sun et al., 2020; d'Autume et al., 2019) để giải quyết vấn đề này. Tuy nhiên, điều quan trọng cần lưu ý là có những khác biệt không đơn giản giữa CL truyền thống và công thức Học Kiến thức Liên tục (CKL) được đề xuất làm cho việc áp dụng các phương pháp CL truyền thống không đầy đủ.

Trong CL truyền thống, các phương pháp có thể được phân loại chủ yếu thành các phương pháp điều chỉnh, luyện tập lại và mở rộng tham số. (1) Trong khi các phương pháp điều chỉnh (Kirkpatrick et al., 2017) yêu cầu xác định các tham số quan trọng được sử dụng cho các tác vụ trước đó, chính xác cách thức và nơi kiến thức được lưu trữ trong các tham số của LM hiện đang cực kỳ khó khăn để xác định và định vị (Vig et al., 2020; De Cao et al., 2021). (2) Trong khi các phương pháp luyện tập lại trước đây (Lopez-Paz & Ranzato, 2017) xem xét việc học tất cả các luồng tác vụ cùng một lúc (học đa tác vụ) như ranh giới hiệu suất trên và tái tạo thiết lập như vậy với các mẫu được lưu trữ trong bộ nhớ tình huống, một vài mẫu từ kho dữ liệu tiền huấn luyện không thể đại diện cho kiến thức thế giới tổng thể từ kho dữ liệu. Hơn nữa, nếu các LM được tiền huấn luyện trên một sự nối tiếp được xáo trộn của luồng kho dữ liệu, không có gì đảm bảo rằng các LM sẽ thu thập thông tin chính xác, gần đây từ các kho dữ liệu gần đây, đặc biệt trong các trường hợp mà các kho dữ liệu trước đó lớn hơn nhiều so với các kho dữ liệu sau này, điều này được chỉ ra bởi các thí nghiệm trong Phần 5.1. (3) Cuối cùng, các phương pháp mở rộng tham số trước đây (Rusu et al., 2016; Yoon et al., 2018) tập trung vào việc học một luồng các tác vụ khác nhau thông qua giám sát mạnh, trong khi trong CKL, trọng tâm là liên tục cập nhật kiến thức thế giới từ một luồng kho dữ liệu thông qua tự giám sát.

Do những khác biệt cơ bản này, thay vì các phương pháp CL đương đại được đề cập ở trên, chúng tôi khám phá các phương pháp từ tài liệu phù hợp với CKL (Chen et al., 2020; He et al., 2021; Hu et al., 2021; Wang et al., 2021b), sửa đổi và điều chỉnh từng phương pháp theo nhu cầu của chúng tôi như các phương pháp CKL. Cuối cùng, trong khi đã được chỉ ra rằng một số công thức CL truyền thống có thể có ít tầm quan trọng thực tế trong các tình huống thực tế bởi Prabhu et al. (2020), CKL gần gũi hơn nhiều với động lực ban đầu đằng sau CL, đó là "đặc điểm cơ bản của trí tuệ tự nhiên là khả năng học liên tục kiến thức mới trong khi cập nhật thông tin về những kiến thức cũ" (Prabhu et al., 2020). Chi tiết về các nghiên cứu liên quan về các phương pháp CL truyền thống và cách các phương pháp CKL giải quyết những khác biệt cơ bản được cung cấp trong Phụ lục A.

3 HỌC KIẾN THỨC LIÊN TỤC (CKL)

Trong phần này, chúng tôi giải thích việc xây dựng tác vụ, quá trình xây dựng dữ liệu và chỉ số được đề xuất để đo lường sự đánh đổi giữa quên kiến thức thế giới trước đó và cập nhật và học kiến thức thế giới mới.

--- TRANG 3 ---

3.1 XÂY DỰNG TÁC VỤ

Khi xem tác vụ làm mới kiến thức nội bộ của các LM như một trong những công thức CL, tiền huấn luyện trên kho dữ liệu gốc có thể được coi là một tác vụ trước đó, và tiếp tục tiền huấn luyện trên kho dữ liệu mới có thể được coi là tác vụ hiện tại, mục tiêu chính trở thành duy trì kiến thức thế giới bất biến theo thời gian có được thông qua tiền huấn luyện ban đầu trong khi học hiệu quả kiến thức mới và cập nhật thông qua tiếp tục tiền huấn luyện. Trong suốt bài báo, chúng tôi để D0 đề cập đến kho dữ liệu được sử dụng cho tiền huấn luyện ban đầu và để D1 biểu thị kho dữ liệu mới được sử dụng cho tiếp tục tiền huấn luyện.

Kho Dữ liệu Văn bản Mới cho Mô hình Ngôn ngữ Để các LM làm mới kiến thức nội bộ của chúng, chúng cần được tiền huấn luyện liên tục trên một kho dữ liệu văn bản mới D1 có thông tin cập nhật và mới. D1 lý tưởng nên nhỏ hơn nhiều so với D0, vì một D1 lớn bằng kích thước của D0 sẽ dẫn đến chi phí tính toán khổng lồ tương tự như tiền huấn luyện các LM từ đầu. Để xây dựng D1, chúng tôi thu thập các bài báo tin tức được xuất bản gần đây từ web tạo ra CC-RECENT NEWS.2

Thăm dò các LM cho Kiến thức Thế giới Tác vụ được sử dụng rộng rãi nhất để thăm dò các LM cho kiến thức thế giới là tác vụ Phân tích Mô hình Ngôn ngữ (LAMA) (Petroni et al., 2019), bao gồm các câu điền chỗ trống được tạo ra từ một tập hợp các nguồn kiến thức sử dụng các mẫu được định nghĩa thủ công. Chúng tôi định nghĩa rằng một LM biết một sự kiện nếu nó có thể dự đoán thành công theo cách zero-shot thực thể bị che trong câu điền chỗ trống, như " Dante sinh ra ở " là Florence. Trong khi có thể có các lựa chọn thay thế khác để đo lường kiến thức thế giới được mã hóa trong các LM3, chúng tôi xây dựng các bộ dữ liệu chính của chúng tôi như các tác vụ LAMA, đồng thời cũng cung cấp thêm các cặp câu hỏi tương ứng với các câu điền chỗ trống cho những người muốn kiểm tra trên CBQA.

Đo lường Duy trì Kiến thức Thế giới Bất biến theo Thời gian Chúng tôi định nghĩa kiến thức thế giới bất biến theo thời gian là thông tin có mặt trong D0 không có khả năng xung đột với thông tin từ D1. Ví dụ, nếu thông tin về nơi sinh của Barack Obama có mặt trong D0, không có khả năng D1 chứa thông tin mâu thuẫn với sự kiện đó. Ngoài ra, chúng tôi phân loại các trường hợp mà dấu thời gian được cố định như " Cristiano Ronaldo chơi cho vào năm 2010. " là bất biến theo thời gian. Những trường hợp bất biến theo thời gian này không nên bị thay đổi khi các LM được tiền huấn luyện liên tục trên D1. Để đo lường bao nhiều thông tin bất biến theo thời gian bị mất do quên thảm khốc trong quá trình tiếp tục tiền huấn luyện, chúng tôi tạo ra LAMA BẤT BIẾN, một tập con của LAMA (Petroni et al., 2019), bao gồm chỉ các câu điền chỗ trống bất biến theo thời gian được chi tiết trong Phụ lục B.1.

Đo lường Cập nhật Kiến thức Thế giới Lỗi thời Trong nghiên cứu này, chúng tôi định nghĩa kiến thức thế giới lỗi thời là thông tin xung đột giữa D0 và D1. Ví dụ, Tổng thống Mỹ có thể là Barack Obama trong D0 và Joe Biden trong D1. Trong trường hợp này, LM nên cập nhật kiến thức nội bộ của nó là Joe Biden là tổng thống Mỹ. Nếu một LM được tiền huấn luyện trên cả D0 và D1 đồng thời, không có gì đảm bảo rằng LM sẽ thu thập thông tin chính xác, gần đây từ D1, đặc biệt trong các trường hợp mà D0 lớn hơn nhiều so với D1, đây là một trong những khác biệt lớn nhất giữa thiết lập CKL và CL truyền thống. Để đo lường việc cập nhật thông tin lỗi thời, chúng tôi xây dựng LAMA CẬP NHẬT được tạo thành từ các câu điền chỗ trống mà câu trả lời có thể được tìm thấy trong cả D0 và D1, nhưng xung đột.

Đo lường Thu thập Kiến thức Thế giới Mới Chúng tôi định nghĩa kiến thức thế giới mới là thông tin có mặt trong D1, nhưng không có trong D0. Để đo lường kiến thức mới được thu thập thông qua tiếp tục tiền huấn luyện trên D1, chúng tôi xây dựng LAMA MỚI được tạo thành từ các câu điền chỗ trống chi tiết yêu cầu kiến thức mới từ D1 để trả lời đúng. Chúng tôi cung cấp hai bộ dữ liệu để đo lường kiến thức thế giới mới: LAMA MỚI, cho đó mỗi trường hợp được xác minh rằng câu trả lời không tồn tại trong D0, nhưng chỉ trong D1, và LAMA MỚI-DỄ cho đó mỗi trường hợp không hoàn toàn tuân thủ định nghĩa nghiêm ngặt của chúng tôi về kiến thức thế giới mới do quá trình tạo ra của nó, nhưng được sử dụng để đo lường tổng quát kiến thức mới được thu thập từ tiếp tục tiền huấn luyện trên D1 ở quy mô lớn hơn.

2CC-RECENT NEWS bao gồm 221.779 bài báo (168M token), được ước tính nhỏ hơn khoảng 750 lần so với C4, một phiên bản được làm sạch của bộ dữ liệu Common Crawl tháng 4 năm 2019 (https://commoncrawl.org/) được sử dụng để tiền huấn luyện ban đầu LM T5 (Raffel et al., 2019).

3Trả lời câu hỏi sách đóng (CBQA) (Roberts et al., 2020) cũng có thể được coi là một tác vụ đo lường kiến thức thế giới của các LM thông qua tinh chỉnh, nhưng đã được chỉ ra rằng phần lớn hiệu suất tăng của nó là do sự chồng chéo test-train (Lewis et al., 2020b; Wang et al., 2021a) trong các bộ dữ liệu.

--- TRANG 4 ---

LAMA MỚI-DỄ có thể được coi là dễ hơn vì mỗi trường hợp được xây dựng để tương tự với phân phối dữ liệu được nhìn thấy trong quá trình tiếp tục tiền huấn luyện.

Xây dựng Bộ dữ liệu Dữ liệu để tiếp tục tiền huấn luyện, CC-RECENT NEWS, được xây dựng bằng cách sử dụng news-please (Hamborg et al., 2017). LAMA BẤT BIẾN được xây dựng bằng cách chọn thủ công 28 quan hệ bất biến theo thời gian từ T-Rex (Elsahar et al., 2018). Đối với LAMA CẬP NHẬT và LAMA MỚI, chúng tôi sử dụng Amazon Mechanical Turk (mturk)4 để thu thập đám đông các Tác vụ Thông minh Con người (HIT). Quá trình này yêu cầu chọn các câu hỏi có thể trả lời từ danh sách các câu hỏi được tạo bởi mô hình được giới thiệu trong Lewis et al. (2021) và chuyển đổi chúng thành các câu điền chỗ trống. Chúng tôi cũng đã thuê riêng 11 chuyên gia để xác minh tính đúng đắn và tìm kiếm cơ sở dữ liệu C4 để phân loại từng trường hợp theo định nghĩa của chúng tôi về cập nhật và mới. LAMA MỚI-DỄ được xây dựng ở quy mô lớn hơn thông qua quá trình mturk hai giai đoạn nơi các câu được chọn từ các bài báo chứa thông tin mới được khử ngữ cảnh và diễn giải5 trước khi bị che, xác minh và chuyển đổi thành các câu hỏi tương ứng. Thống kê bộ dữ liệu được xây dựng có trong Bảng 1. Chi tiết quan trọng về quy trình xây dựng dữ liệu, ví dụ và thống kê chi tiết hơn được cung cấp trong Phụ lục B.

Bảng 1: Thống kê bộ dữ liệu. Độ dài đầu vào và câu trả lời là độ dài token trung bình tương ứng.

Bộ dữ liệu | Kích thước | Độ dài Đầu vào | Độ dài Câu trả lời | Bộ dữ liệu | Kích thước | Độ dài Đầu vào | Độ dài Câu trả lời
LAMA BẤT BIẾN | 17474 | 11.9 | 1.3 | LAMA MỚI | 797 | 14.7 | 8.7
LAMA CẬP NHẬT | 924 | 13.7 | 9.4 | LAMA MỚI-DỄ | 11177 | 44.4 | 6.1

3.2 CHỈ SỐ KẾT HỢP CHO CKL

Chúng tôi đề xuất một chỉ số mới, FUAR (TỈ LỆ QUÊN / (CẬP NHẬT + THU THẬP)), có thể so sánh hiệu quả của từng phương pháp CKL bằng cách sử dụng sự đánh đổi giữa kiến thức bất biến theo thời gian bị quên và kiến thức mới hoặc được cập nhật. FUAR đại diện tương đối cho số lượng trường hợp kiến thức bất biến theo thời gian bị quên để học một trường hợp kiến thức mới hoặc cập nhật.

Chúng tôi đầu tiên định nghĩa FUAR cho trường hợp tổng quát nơi có thể có nhiều kho dữ liệu được sử dụng để huấn luyện một LM luôn thay đổi.

Để T là một tác vụ tùy ý và (Di)n i=0 là một chuỗi kho dữ liệu được sử dụng cho tiền huấn luyện LM, nơi D0 là kho dữ liệu tiền huấn luyện ban đầu. Chúng tôi định nghĩa Gap(T;Da;Db) = Score(T) của LMa - Score(T) của LMb, nơi LMa đại diện cho LM sau khi được tiền huấn luyện trên Da. Sau đó, chúng tôi ký hiệu TF = (TFi)n-1 i=0 như một chuỗi tác vụ từ (Di)n-1 i=0 đo lường việc quên kiến thức bất biến từ mỗi kho dữ liệu tương ứng. Nếu không có tác vụ như vậy từ kho dữ liệu Di, giá trị của TFi được đặt thành n.d., có nghĩa là không được định nghĩa. Tương tự, chúng tôi ký hiệu TUn và TAn như các tác vụ từ Dn đo lường việc cập nhật và thu thập kiến thức mới, tương ứng. Chúng tôi định nghĩa FUAR như sau:

[Công thức toán học phức tạp]

Lựa chọn các tác vụ đánh giá TF, TUn và TAn có thể khác nhau theo từng thiết lập thí nghiệm. Giá trị FUAR là 1.0 đại diện cho một tình huống đánh đổi bằng nhau nơi một trường hợp kiến thức bất biến theo thời gian của TF bị quên trung bình để có được một trường hợp kiến thức mới hoặc cập nhật của TUn và TAn. Hai thuật ngữ trong mẫu số được cộng lại vì kiến thức mới thu được và kiến thức cập nhật loại trừ lẫn nhau theo định nghĩa. Khi giá trị nhỏ hơn 1, có nghĩa là mô hình thu được nhiều kiến thức mới hoặc cập nhật hơn lượng kiến thức bị quên, vì vậy các phương pháp thể hiện giá trị FUAR thấp có thể được coi là phù hợp cho CKL. Nếu giá trị bằng không, thì đó là trường hợp không có việc quên xảy ra và là ranh giới trên cho hiệu suất. Nếu mẫu số là 0, chúng tôi ký hiệu trường hợp là không có lợi ích và coi nó là trường hợp tồi tệ nhất có thể.6

4 THIẾT LẬP THÍ NGHIỆM

Chúng tôi thực hiện các thí nghiệm mở rộng với một mô hình mã hóa-giải mã, T5 (Raffel et al., 2019), một LM lớn (737M tham số) ban đầu được tiền huấn luyện trên bản sao tháng 4 năm 2019 của C4 và bản sao tháng 5 năm 2020 của Wikipedia (do đó D0 trong các thí nghiệm của chúng tôi) với che giấu khoảng nổi bật (SSM). Chi tiết về cấu hình tiền huấn luyện, tiếp tục tiền huấn luyện và đánh giá có trong Phụ lục C. Chúng tôi thiết lập các phương pháp sau đây như các đường cơ sở cho bộ đánh giá CKL và phân loại chúng thành các phương pháp điều chỉnh, luyện tập lại và mở rộng tham số. Các siêu tham số cụ thể được sử dụng để triển khai từng phương pháp được chi tiết trong Phụ lục D.

Ban đầu đề cập đến thiết lập nơi chúng tôi đánh giá LM trước bất kỳ tiếp tục tiền huấn luyện nào. Hiệu suất của mô hình này có thể được coi là ranh giới trên cho LAMA BẤT BIẾN và ranh giới dưới trên LAMA CẬP NHẬT và LAMA MỚI.

Vanilla là một thiết lập cụ thể của tiếp tục tiền huấn luyện (Gururangan et al., 2020), nơi miền là kiến thức mới, và LM được tiền huấn luyện thêm mà không có bất kỳ chiến lược huấn luyện nào.

RecAdam (Chen et al., 2020) thuộc về danh mục các phương pháp điều chỉnh. Nó đặt một giả định độc lập mạnh hơn giữa các tham số mô hình so với phương pháp điều chỉnh truyền thống (EWC (Kirkpatrick et al., 2017)) và không truy cập kho dữ liệu tiền huấn luyện ban đầu để điều chỉnh trọng số mô hình trong quá trình tiếp tục tiền huấn luyện. Bộ tối ưu hóa được ủ nhiệt để ít điều chỉnh hơn được áp dụng khi quá trình huấn luyện tiến triển.

Mix-Review (He et al., 2021) thuộc về danh mục các phương pháp luyện tập lại, giả định truy cập vào kho dữ liệu tiền huấn luyện ban đầu và trộn các tập con ngẫu nhiên của dữ liệu tiền huấn luyện ban đầu trong quá trình tiếp tục tiền huấn luyện, tùy thuộc vào tỷ lệ trộn tại bước thời gian hiện tại. Khi quá trình huấn luyện tiến triển, tỷ lệ trộn giảm về 0, giảm lượng dữ liệu gốc được trộn tại mỗi lần lặp.

LoRA (Hu et al., 2021) thuộc về danh mục các phương pháp mở rộng tham số. Nó đóng băng các tham số gốc của LM và thêm các ma trận phân tích hạng có thể huấn luyện vào mỗi lớp được cập nhật trong quá trình tiếp tục tiền huấn luyện. Hu et al. (2021) đã triển khai phương pháp này với các mô hình chỉ giải mã (GPT-2 (Radford et al., 2019) & GPT-3 (Brown et al., 2020)) trong khi chúng tôi áp dụng nó cho một mô hình mã hóa-giải mã, ký hiệu là T5-LoRA.

K-Adapter (Wang et al., 2021b) là một phương pháp mở rộng tham số khác đóng băng các tham số gốc của LM trong khi thêm k số lớp mới, gọi là adapter, được cập nhật trong quá trình tiếp tục tiền huấn luyện. Wang et al. (2021b) đã cho thấy việc tiêm thành công kiến thức thực tế và ngôn ngữ học cho các mô hình chỉ mã hóa, BERT (Devlin et al., 2019) & RoBERTa (Liu et al., 2019), trong khi chúng tôi cũng áp dụng nó cho một mô hình mã hóa-giải mã, T5, và mô hình chỉ giải mã, GPT-2.

Modular là một phương pháp mở rộng tham số được đề xuất mới đặc biệt cho các mô hình mã hóa-giải mã đóng băng bộ mã hóa gốc, được tiền huấn luyện trong khi thêm một bộ mã hóa mới, được khởi tạo ngẫu nhiên được cập nhật trong quá trình tiếp tục tiền huấn luyện. Đối với bộ mã hóa mới được thêm, chúng tôi thay đổi kích thước thành T5-small trong khi giữ kích thước của bộ mã hóa và giải mã gốc là T5-large.

5 KẾT QUẢ THÍ NGHIỆM

Trong phần này, chúng tôi đầu tiên hiển thị kết quả thí nghiệm chính cho Bộ đánh giá CKL. Sau đó, vì nhiều bước học kiến thức liên tục, tức là CKL được cần thiết để huấn luyện một LM thay đổi thật sự, chúng tôi khám phá ảnh hưởng của nhiều giai đoạn CKL cũng như cách epoch, kích thước kho dữ liệu và tổng số bước huấn luyện ảnh hưởng đến CKL. Chúng tôi khám phá thêm cách tỷ lệ học ảnh hưởng đến CKL trong Phụ lục E, cách tiếp tục tiền huấn luyện trên D1 ảnh hưởng đến hiệu suất của các tác vụ KILT yêu cầu kiến thức từ D0 trong Phụ lục F, cách các phương pháp CKL chuyển giao qua các kiến trúc LM trong Phụ lục G, và cách các đầu ra dự đoán thay đổi trong quá trình CKL trong Phụ lục H.

5.1 KẾT QUẢ CHÍNH

Bảng 2 hiển thị kết quả thí nghiệm chính của chúng tôi trên bộ đánh giá CKL. Trong khi chỉ có khớp chính xác (EM) được báo cáo trong Bảng 2, chúng tôi báo cáo điểm F1 cũng như độ chính xác trung bình tại k (P@k, k=1,5,10,20,50,100) trong Phụ lục J. Các mô hình T5 ban đầu được tiền huấn luyện trên C4 (khoảng 1 nghìn tỷ cập nhật token) và Wikipedia, được coi là D0.7, và sau đó được tiền huấn luyện liên tục trên CC-RecentNews (kho dữ liệu D1) trong 4 epoch (25k bước huấn luyện toàn cục, khoảng 673 triệu cập nhật token) sử dụng từng phương pháp CKL. Mỗi IL, UL, NL, NLE đứng cho LAMA BẤT BIẾN, LAMA CẬP NHẬT, LAMA MỚI và LAMA MỚI-DỄ, tương ứng. Mô tả chi tiết về thiết lập cho thí nghiệm này được bao gồm trong chú thích.

Bảng 2: Hiệu suất thăm dò zero-shot trên bộ đánh giá CKL. Kết quả tốt nhất cho từng tác vụ và chỉ số được hiển thị in đậm, và kết quả tốt thứ hai được gạch dưới.

[Bảng với các kết quả hiệu suất cho các phương pháp khác nhau]

Chúng tôi đầu tiên thấy rằng tất cả các phương pháp CKL ngoại trừ T5-MixReview đều hiệu quả hơn trong việc quên ít kiến thức bất biến theo thời gian trong khi cập nhật và thu thập kiến thức mới so với việc sử dụng phương pháp ngây thơ của T5-Vanilla như được hiển thị bởi FUAR. Kết quả này cũng làm nổi bật sự khác biệt chính giữa CKL và CL; trong khi các phương pháp luyện tập lại cho thấy hiệu suất mạnh trong các thiết lập CL truyền thống (Prabhu et al., 2020; Bang et al., 2021), trong CKL, nó cho thấy hiệu suất tồi tệ nhất vì việc cập nhật kiến thức lỗi thời và thu thập kiến thức mới bị cản trở nghiêm trọng như được hiển thị trong hiệu suất của UL và NL trong khi không cho thấy việc giảm thiểu cạnh tranh của việc quên như được hiển thị trong hiệu suất của IL so với các phương pháp CKL khác. Trong số các phương pháp CKL khác, chúng tôi quan sát một xu hướng khá nhất quán rằng các phương pháp mở rộng tham số đạt được kết quả tốt hơn. Kết quả tốt nhất và tốt thứ hai trên tất cả UL, NL và NLE đều từ các phương pháp mở rộng tham số. Trong khi đó, mặc dù UL và NL được xây dựng theo cùng quy trình, có sự khác biệt lớn giữa điểm EM của UL và NL. Chúng tôi phân tích nguồn gốc của sự khác biệt này trong Phụ lục I.

Hình 9 hiển thị cách điểm EM của từng tác vụ thay đổi khi T5-Kadapters, phương pháp CKL với hiệu suất mạnh nhất, và T5-Vanilla được tiền huấn luyện liên tục trên D1. Trong tất cả các tác vụ, hiệu suất của T5-Initial có thể được coi là ranh giới trên cho IL và ranh giới dưới cho UL, NL, NLE. Tương ứng với các quan sát chính của chúng tôi, CKL cho phép duy trì đáng kể kiến thức thế giới bất biến theo thời gian trong khi cải thiện việc cập nhật và thu thập kiến thức thế giới mới so với T5-Vanilla, giảm thiểu sự đánh đổi tổng thể.

5.2 KHÁM PHÁ NHIỀU GIAI ĐOẠN CKL

Để hiển thị tiềm năng tạo ra một LM thay đổi thật sự, chúng tôi khám phá ảnh hưởng của nhiều giai đoạn CKL bằng cách tạo ra CC-RECENT NEWS-SMALL, ký hiệu là SMALL, là một biến thể nhỏ của CC-RECENT NEWS bao gồm 10% được lấy mẫu ngẫu nhiên từ kho dữ liệu gốc. Sau đó chúng tôi chia CC-RECENT NEWS-SMALL thành hai phần khác nhau theo ngày xuất bản của mỗi bài báo để mô phỏng một thiết lập nơi nhiều giai đoạn CKL được cần thiết, ký hiệu là SMALL-P1 (05.2020 - 11.2020)) và SMALL-P2 (11.2020 - 04.2021). NLE8 cũng được chia thành hai bộ dữ liệu khác nhau, nhỏ hơn, NLE P1 và NLE P2, mỗi bộ bao gồm các trường hợp được xây dựng từ các bài báo trong SMALL-P1 và SMALL-P2, tương ứng. Chúng tôi so sánh cách các phương pháp CKL cho T5 thực hiện trên IL, NLE P1 và NLE P2 khi được tiền huấn luyện liên tục hoàn toàn trên SMALL trong 5k bước (8 epoch), và khi được tiền huấn luyện tuần tự trên SMALL-P1 và sau đó trên SMALL-P2 trong 2.5k bước (8 epoch) mỗi cái. Trong tình huống SMALL-P1→SMALL-P2, có hai giai đoạn CKL nơi D0 là C4 và Wikipedia, D1 là SMALL-P1, và D2 là SMALL-P2. Phần còn lại của cấu hình được đặt giống hệt với các thí nghiệm chính.

Bảng 3: Hiệu suất thăm dò zero-shot sau khi các mô hình T5 được tiền huấn luyện liên tục trên các tập con khác nhau của CC-RECENT NEWS. NLE và IL đứng cho NewLAMA-Easy và InvariantLAMA, tương ứng. Có ba tình huống theo kho dữ liệu được sử dụng cho tiếp tục tiền huấn luyện, được giải thích trong văn bản của Phần 5.2. FUAR của ba tình huống được tính khác nhau, và các tác vụ tương ứng được hiển thị trong bảng như các tham số của FUAR: TF, TUn và TAn. Trong thiết lập này, TF bao gồm chỉ một tác vụ duy nhất TF0 (IL) đo lường thông tin bất biến theo thời gian bị mất từ chỉ D0. Đối với SMALL, chúng tôi tính toán khoảng cách trên NLE bằng cách sử dụng tổng có trọng số của các khoảng cách trên NLE P1 và NLE P2 với trọng số đồng nhất.

[Bảng 3 với các kết quả hiệu suất cho các tình huống khác nhau]

So sánh hiệu suất trên IL của hai tình huống, SMALL và SMALL-P1→SMALL-P2, kết quả cho thấy các LM dễ bị quên nhiều hơn khi chúng trải qua nhiều giai đoạn CKL, mặc dù có cùng số bước huấn luyện. Một trong những lý do có thể là do lịch trình tỷ lệ học, được khởi tạo ở đầu mỗi giai đoạn.

Hơn nữa, mặc dù cho thấy hiệu suất tốt nhất tổng thể, nhược điểm của các phương pháp mở rộng tham số cũng được làm nổi bật trong thiết lập SMALL-P1→SMALL-P2; chúng yêu cầu các tham số mới được thêm vào mỗi giai đoạn cập nhật. Ví dụ, số lượng tổng tham số của T5-Modular tăng 36M trong mỗi vòng của giai đoạn tiếp tục tiền huấn luyện. Tương tự, xem xét một số lượng lớn các giai đoạn CKL giới thiệu các vấn đề mới cần được nghiên cứu thêm. Xem xét rằng các LM nên được cập nhật thường xuyên với một lượng nhỏ dữ liệu trong các tình huống thực tế để thu được kiến thức thế giới cập nhật về thế giới luôn thay đổi theo cách hiệu quả tính toán, cần nhiều nghiên cứu hơn để giảm thiểu lượng quên theo sau số lượng giai đoạn cập nhật lớn hơn.

Ảnh hưởng của Epoch, Kích thước Kho dữ liệu và Tổng số Bước Huấn luyện trong CKL đối với Việc Quên

Hình 3 hiển thị kết quả của T5-Vanilla và T5-Kadapters trong quá trình tiếp tục tiền huấn luyện trong các tình huống khác nhau từ Bảng 2 và 3, nơi mỗi điểm trong đồ thị đại diện cho hiệu suất của IL sau mỗi epoch. So sánh MAIN (4 epoch) và SMALL (8 epoch) trong Hình 3 (a) T5-Vanilla, chúng ta có thể thấy rằng nhiều quên hơn xảy ra trong SMALL, mặc dù được huấn luyện ít hơn năm lần số bước huấn luyện toàn cục. Hiện tượng này được làm nổi bật thêm khi so sánh kết quả từ SMALL-P1 (8 epoch) cho thấy lượng quên nhiều nhất mặc dù được huấn luyện ít hơn mười lần số bước huấn luyện toàn cục. Trong khi sự giảm tổng thể được giảm thiểu nhiều trong Hình 3 (b) T5-Kadapters, chúng ta quan sát cùng xu hướng giữa mỗi tình huống cho thấy việc quan sát cùng dữ liệu lặp đi lặp lại trong quá trình tiếp tục tiền huấn luyện quan trọng như thế nào trong việc gây ra quên.

Kết quả phù hợp với các phát hiện từ Lee et al. (2021) gợi ý rằng các LM nên được tiền huấn luyện chỉ với một vài epoch trên dữ liệu ít trùng lặp hơn để đạt hiệu quả. Chúng tôi thêm trực giác bổ sung vào các phát hiện của họ và đưa ra giả thuyết rằng sự không hiệu quả của tiền huấn luyện từ dữ liệu trùng lặp có thể đã được gây ra bởi việc quên kiến thức đuôi dài khá trong kho dữ liệu tiền huấn luyện.

6 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất HỌC KIẾN THỨC LIÊN TỤC (CKL), nơi chúng tôi thiết lập các bộ dữ liệu và chỉ số đánh giá, và khám phá các phương pháp hướng tới học kiến thức liên tục của một LM luôn thay đổi. Chúng tôi thấy rằng các phương pháp mở rộng tham số cho thấy hiệu suất mạnh nhất trong tất cả các thiết lập thí nghiệm, tuy nhiên có sự không hiệu quả bộ nhớ nghiêm trọng và việc nhìn thấy cùng dữ liệu thường xuyên là nguyên nhân quan trọng của việc quên. Chúng tôi cũng thảo luận một số kết quả thú vị khác mà chúng tôi để lại việc khám phá thêm cho các nghiên cứu tương lai. Cuối cùng, chúng tôi đề xuất cộng đồng khám phá CKL để thiết kế tốt hơn một LM luôn thay đổi.

--- TRANG 10 ---

LỜI CẢM ơN

Các tác giả muốn cảm ơn Sang-Woo Lee, Jinheon Baek, Miyoung Ko, Hyunji Lee và Eunbi Choi vì những thảo luận hữu ích. Nghiên cứu này được hỗ trợ bởi Viện Lập kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2019-0-00075, Chương trình Trường Đại học Trí tuệ Nhân tạo (KAIST)).

TÀI LIỆU THAM KHẢO

Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, và Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. Trong CVPR, 2021.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Trong NeurIPS, 2020.

Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, và Xiangzhan Yu. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. Trong EMNLP, 2020.

Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, và Michael Collins. Decontextualization: Making sentences stand-alone. TACL, 9:447–461, 2021.

Damai Dai, Li Dong, Y. Hao, Zhifang Sui, và Furu Wei. Knowledge neurons in pretrained transformers. ArXiv, abs/2104.08696, 2021.

Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, và Dani Yogatama. Episodic memory in lifelong language learning. Trong NeurIPS, 2019.

Nicola De Cao, Wilker Aziz, và Ivan Titov. Editing factual knowledge in language models. Trong EMNLP, 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. Trong NAACL, 2019.

Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, và William W Cohen. Time-aware language models as temporal knowledge bases. arXiv preprint arXiv:2106.15110, 2021.

Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, và Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. Trong ICLR, 2019.

Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Elena Simperl, và Frederique Laforest. T-rex: A large scale alignment of natural language with knowledge base triples. Trong LREC, 2018.

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, và Michael Auli. Eli5: Long form question answering. Trong ACL, 2019.

Zhaochen Guo và Denilson Barbosa. Robust named entity disambiguation with random walks. Semantic Web, 9(4):459–479, 2018.

Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, và Noah A Smith. Don't stop pretraining: adapt language models to domains and tasks. Trong ACL, 2020.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. Trong ICML, 2020.

Felix Hamborg, Norman Meuschke, Corinna Breitinger, và Bela Gipp. news-please: A generic news crawler and extractor. Trong 15th International Symposium of Information Science (ISI 2017), pp. 218–223, 2017.

--- TRANG 11 ---

Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, và Fuchun Peng. Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models. Trong EACL, 2021.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, và Gerhard Weikum. Robust disambiguation of named entities in text. Trong EMNLP, 2011.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, và Xiang Ren. Lifelong pretraining: Continually adapting language models to emerging corpora. arXiv preprint arXiv:2110.08534, 2021.

Mandar Joshi, Eunsol Choi, Daniel S Weld, và Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Trong ACL, 2017.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.

Mojtaba Komeili, Kurt Shuster, và Jason Weston. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566, 2021.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. TACL, 7:453–466, 2019.

Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Sebastian Ruder, Dani Yogatama, et al. Pitfalls of static language modelling. arXiv preprint arXiv:2102.01951, 2021.

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, và Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.

Omer Levy, Minjoon Seo, Eunsol Choi, và Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. Trong CoNLL, 2017.

Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Trong NeurIPS, 2020a.

Patrick Lewis, Pontus Stenetorp, và Sebastian Riedel. Question and answer test-train overlap in open-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020b.

Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, và Sebastian Riedel. Paq: 65 million probably-asked questions and what you can do with them. Trong EACL, 2021.

Yanyang Li, Ye Lin, Tong Xiao, và Jingbo Zhu. An efficient transformer decoder with compressed sub-layers. arXiv preprint arXiv:2101.00542, 2021.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, và Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052, 2021.

--- TRANG 12 ---

David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Trong NeurIPS, 2017.

Michael McCloskey và Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.

David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, và Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, và Sebastian Riedel. Language models as knowledge bases? Trong EMNLP, 2019.

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowledge intensive language tasks. Trong NAACL, 2021.

Nina Poerner, Ulli Waltinger, và Hinrich Schütze. E-bert: Efficient-yet-effective entity embeddings for bert. Trong Findings of EMNLP, 2019.

Ameya Prabhu, Philip HS Torr, và Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. Trong ECCV, 2020.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

Adam Roberts, Colin Raffel, và Noam Shazeer. How much knowledge can you pack into the parameters of a language model? Trong EMNLP, 2020.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, và Sameer Singh. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. Trong EMNLP, 2020.

Fan-Keng Sun, Cheng-Hao Ho, và Hung-Yi Lee. Lamol: Language modeling for lifelong language learning. Trong ICLR, 2020.

James Thorne, Andreas Vlachos, Christos Christodoulopoulos, và Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. Trong NAACL, 2018.

Jörg Tiedemann và Santhosh Thottingal. OPUS-MT — Building open translation services for the World. Trong EAMT, Lisbon, Portugal, 2020.

Pat Verga, Haitian Sun, Livio Baldini Soares, và William W Cohen. Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge. Trong NAACL, 2021.

Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, và Stuart Shieber. Causal mediation analysis for interpreting neural nlp: The case of gender bias. Trong NeurIPS, 2020.

Cunxiang Wang, Pai Liu, và Yue Zhang. Can generative pre-trained language models serve as knowledge bases for closed-book qa? Trong ACL, 2021a.

Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang, Ming Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. Trong Findings of ACL, 2021b.

--- TRANG 13 ---

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander M. Rush. Transformers: State-of-the-art natural language processing. Trong EMNLP System Demonstrations, 2020.

Jing Xu, Arthur Szlam, và Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. arXiv preprint arXiv:2107.07567, 2021.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, và Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Trong EMNLP, 2018.

Jaehong Yoon, Eunho Yang, Jeongtae Lee, và Sung Ju Hwang. Lifelong learning with dynamically expandable networks. Trong ICLR, 2018.

Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, và Yejin Choi. Defending against neural fake news. Trong NeurIPS, 2019.

Michael J.Q. Zhang và Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. EMNLP, 2021.

Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, và Sanjiv Kumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363, 2020.

[Nội dung tiếp tục với các phụ lục và bảng biểu khác...]
