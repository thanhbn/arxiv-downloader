# 2303.05911.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2303.05911.pdf
# File size: 6992829 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Lifelong Machine Learning Potentials
Marco Eckhoff∗and Markus Reiher†
ETH Zürich, Departement Chemie und Angewandte Biowissenschaften,
Vladimir-Prelog-Weg 2, 8093 Zürich, Switzerland.
(Dated: May 11, 2023)
Machine learning potentials (MLPs) trained on accurate quantum chemical data can retain the
high accuracy, while inflicting little computational demands. On the downside, they need to be
trained for each individual system. In recent years, a vast number of MLPs has been trained from
scratch because learning additional data typically requires to train again on all data to not forget
previously acquired knowledge. Additionally, most common structural descriptors of MLPs cannot
represent efficiently a large number of different chemical elements. In this work, we tackle these
problems by introducing element-embracing atom-centered symmetry functions (eeACSFs) which
combine structural properties and element information from the periodic table. These eeACSFs are
akeyforourdevelopmentofalifelongmachinelearningpotential(lMLP).Uncertaintyquantification
can be exploited to transgress a fixed, pre-trained MLP to arrive at a continuously adapting lMLP,
because a predefined level of accuracy can be ensured. To extend the applicability of an lMLP to
new systems, we apply continual learning strategies to enable autonomous and on-the-fly training
on a continuous stream of new data. For the training of deep neural networks, we propose the
continual resilient (CoRe) optimizer and incremental learning strategies relying on rehearsal of data,
regularization of parameters, and the architecture of the model.
Keywords: Lifelong Machine Learning, Continual Resilient (CoRe) Optimizer, Element-Embracing Atom-
Centered Symmetry Functions, High-Dimensional Neural Network Potential, Uncertainty Quantification
1. INTRODUCTION
For the prediction and understanding of the properties
and reactivity of atomistic systems, the knowledge of the
potential energy surface is inevitable. The potential en-
ergysurfaceisgivenbytheelectronicenergyasafunction
of the nuclear positions and can be obtained from elec-
tronic structure methods such as density functional the-
ory (DFT) or approximate wave-function theories [1, 2].
These methods are generally applicable and achieve ac-
curateresultsformanysystems, butevenstate-of-the-art
approaches still lead to high computational demands in
extended simulations [3–5]. Instead of explicitly calculat-
ing the electronic energy for each nuclear conformation in
Born-Oppenheimer approximation, empirical force fields
rely on approximate (simple) analytical expressions of
the potential energy surface. Hence, they avoid the ex-
plicit integration of quantum mechanical equations and
thereby enabling efficient atomistic simulations. How-
ever, force field expressions are of limited accuracy and
they are typically not universal for all chemical systems
[6–9].
By contrast, machine learning potentials (MLPs) [10–
15] can preserve the high accuracy of electronic struc-
ture methods, but at low computational cost comparable
to that of force fields. MLPs are not based on physical
approximations, but rely on very flexible mathematical
expressions to obtain an analytical potential energy sur-
face. The parameters of these expressions are trained
∗marco.eckhoff@phys.chem.ethz.ch
†mreiher@ethz.chon electronic structure reference data including chemi-
cal structures and their respective energies and atomic
forces. While first-generation MLPs were only appli-
cable to low-dimensional systems [16], the introduction
of second-generation MLPs in form of high-dimensional
neural network potentials (HDNNPs) [17–19] has facili-
tated atomistic simulations of systems with tens of thou-
sands of atoms on nanosecond timescales with an accu-
racy similar to that of first-principles methods. In re-
cent years, various other types of MLPs have been pro-
posed such as neural network potentials [20–23], Gaus-
sian approximation potentials [24], moment tensor po-
tentials [25], and many more. Second-generation MLPs
rely on the locality of the major part of atomic inter-
actions [19]. However, third-generation MLPs add the
description of long-range interactions [24, 26] and fourth-
generation MLPs even non-local interactions [27, 28] be-
yond the applied cutoff sphere for atomic interactions.
Reference atomistic structures need to represent suf-
ficiently well the conformation space to be explored in
subsequent simulations. The reason is that MLPs are
designed for interpolation of learned atomic interactions
but the extrapolation capability far beyond the trained
atomic environment space is limited [12, 18]. For ex-
ample, an MLP trained on water will fail for gaseous
dihydrogen and dioxygen. A systematic construction ap-
proach is not feasible for most systems because of the
enormous conformation space that is typically accessi-
ble for an atomistic system, especially when one con-
siders various large-amplitude motions or chemical re-
actions. To obtain reasonable reference conformations,
iterative active learning protocols have been devised [29–
33]. In such protocols, preliminary MLPs are validated
in atomistic simulations to identify important, but yetarXiv:2303.05911v2  [cs.LG]  4 Jun 2023

--- PAGE 2 ---
2
unknown conformations. These conformations are recal-
culated with the reference method and then added to the
reference data to train an improved MLP. However, it is
difficult to ensure that a reference data set is complete
even for a specific application because some conforma-
tions may occur only very rarely. Also, different scientific
targets and purposes may, for the same system, highlight
different regions of its conformation space and, therefore,
require different reference conformations.
CurrentMLPslearninisolation, i.e., amodelistrained
on predefined data and is subsequently applied in simula-
tions without exploiting for future MLPs the knowledge
already learned. Despite the general functional form,
MLPs can therefore only be applied reliably to a spe-
cific conformation space—which can span various chem-
ical formulae and configurations—and the transferabil-
itysignificantlybeyondtheirlearnedatomicenvironment
space is limited [8, 9]. As a consequence, a vast num-
ber of single-purpose MLPs has been constructed [34–
42]. Recent work has attempted to overcome this limita-
tion by leveraging very large and broad training data sets
[43, 44], but future applications may still require different
reference data.
To achieve a general-purpose solution, MLPs need to
be adaptable to learn new chemical systems based on al-
ready acquired knowledge. Obviously, the training data
sets of present-day MLPs can be simply extended to in-
clude additional systems in active learning procedures.
However, the training process of new or extended data
sets is often started from scratch with randomly initial-
ized MLP parameters. Even if previously obtained pa-
rameters are used, the training typically does not dif-
ferentiate between new and old data points. Hence, all
data points need to be trained simultaneously from a
stationary batch of data every time new data is added.
Consequently, learning larger data sets becomes increas-
ingly inefficient as the amount of repeated training of old
data grows. This trend is opposite to how the human
brain operates which efficiently learns additional knowl-
edge based on already acquired expertise [45–47].
This ability of incremental learning is referred to as
lifelong learning or continual learning in the field of ma-
chine learning [46, 47] and it is also desirable for MLPs.
Lifelong machine learning means that the model contin-
ually acquires and fine-tunes knowledge. For MLPs it
will be a single-incremental-task scenario because new
chemicalstructuresaresequentiallyaddedbuttheenergy
needs to be distinguished among all encountered struc-
tures [48]. Lifelong learning is a challenge for machine
learning because incremental learning from a continuous
stream of information typically results in catastrophic
forgetting and interference [49, 50]. The MLP param-
eters are adapted to the new data, while the memory
about the old data fades away.
In principle, building on prior knowledge could even
reduce the necessary amount of training data and more
complex tasks can be learned more efficiently. According
to complementary learning systems theory [47], learningis based on episodic memory and generalization. While
the episodic memory learns arbitrary information fast,
the generalization is a slow learning process to structure
the knowledge. Therefore, the episodic memory is similar
to the training data set, while the MLP parameters are
trained to generalize the data. Consequently, the param-
eters need to be plastic to integrate new knowledge, but,
at the same time, they need to be stable to avoid for-
getting [45–47]. In recent years, several algorithms have
been developed to reach this stability-plasticity balance
and to mitigate catastrophic forgetting; examples are
ExStream [51], gradient episodic memory [52], synaptic
intelligence [53], AR1 [48], progressive neural networks
[54], and growing dual-memory [55]. These approaches
rely on rehearsal of selected training data, regularization
of machine learning parameters, and/or the architecture
of the machine learning model.
The situation becomes more complex for a general-
purposeoruniversalMLPasitwouldbefurtherhindered
by the unfavorable scaling in computational cost with
the number of chemical elements. This feature plagues
most common MLP descriptors [9, 56, 57] such as atom-
centered symmetry functions (ACSFs) [58], smooth over-
lap of atomic positions [59], and many more [60, 61]—
with exception of graph representations [62–64]. The
reason is a rapid increase in the number of structural de-
scriptorssincetheseareconstructedindependentlyofone
anotherforeachelementcombination. Somestudieshave
attempted to overcome this problem by combining ele-
ment and structure information in the structural descrip-
tor through the inclusion of a single additional element-
dependent weight [65–67]. Despite some successful ap-
plications, employing, for example, the atomic number
as in weighted ACSFs [67] is restricted to elements of
similar atomic numbers. Otherwise, the contributions of
light atoms, such as H, are obscured by contributions of
heavy atoms, such as iodine (e.g., factor 53·53in angular
contributions).
In this work, we exploit additional weight terms based
on different properties for separate ACSFs that can re-
duce bias toward specific elements and increase the dis-
criminabilityofthedescriptorrepresentationfordifferent
conformations. Chemical intuition is based on regulari-
ties in molecular structure and on trends in the periodic
table. To exploit these trends, we use information about
the period and the group [68], or more precisely, the po-
sition in the s-, p-, d-, and f-block instead of the atomic
number. For instance, all halogens lead to a similar type
of bonding, while the bond length increases in higher pe-
riods.
Furthermore, we propose that uncertainty quantifica-
tioncanenabletheapplicationofadaptableMLPparam-
eters. Adaptable parameters require that the reliability
of an MLP is probed on the fly to avoid full validation
of each MLP training extension. As long as the error
of the MLP is below a tolerance in production calcula-
tions, little variations with the learning process will not
significantly affect the results. Moreover, MLPs with un-

--- PAGE 3 ---
3
certainty quantification are able to report warnings in
case of unknown conformations, even if these are within
the general training space. Therefore, the uncertainty
quantification can be employed to estimate the transfer-
ability of the MLP, with low uncertainty meaning high
transferability. To obtain uncertainty quantification, for
example, for HDNNPs, some studies have applied an en-
semble approach [69–73]. A small ensemble of HDNNPs
is then trained differently and independently on the same
reference data. The deviations between the predictions
of the individual HDNNPs can be employed as a proxy
to the prediction error due to the high flexibility of the
neural networks. Moreover, taking the average of the en-
semble improves the accuracy of the representation. We
note that such uncertainty measures should accompany
any sort of modeling approach in atomistic simulations
[74], although their implementation has only started re-
cently.
Consequently, this work introduces element-embracing
atom-centered symmetry functions (eeACSFs) to over-
come the unfavorable scaling with the number of ele-
ments up to an arbitrary number. In combination with
uncertainty quantification and continual learning strate-
gies, the concept of a lifelong machine learning potential
(lMLP) is proposed, which can be trained in a rolling
fashion by a continuous stream of new data. To perform
this challenging training process, we present the new con-
tinual resilient (CoRe) optimizer which is an adaptive
method for stochastic first-order iterative optimization.
CoRe is applied in combination with our lifelong train-
ing strategies which provide an adaptive selection of em-
ployed training data, including reduction of the data set
size and removal of doubtful data. We demonstrate the
performance of our lMLP concept for a data set including
42differentS N2reactionsandtherebytendifferentchem-
ical elements. Although our work on the lMLP concept
rests on a second-generation HDNNP representation, we
emphasize that the concept can also be applied for a dif-
ferent base model. Furthermore, the CoRe optimizer and
the lifelong adaptive data selection can be employed in
training of machine learning models beyond lMLPs and
contribute to the development of lifelong machine learn-
ing to mitigate catastrophic forgetting.
This work is organized as follows: In Section 2 we sum-
marize the HDNNP method and introduce eeACSFs, the
CoRe optimizer, lifelong adaptive data selection, uncer-
tainty quantification, and lMLPs. After presenting the
computational details in Section 3, Section 4 starts with
a description of the reference data. Section 4 continues
with a performance assessment of the eeACSF represen-
tation, a comparison between the results of CoRe and
those of other optimizers, performance tests of the life-
long training strategies, and a validation of the uncer-
tainty quantification in potential energy surface predic-
tions. This work ends with a conclusion in Section 5.2. METHODS
2.1. High-Dimensional Neural Network Potential
with Standardization
For a system containing Nelemelements and Nm
atom
atoms of element m, the second-generation HDNNP en-
ergy [17–19] is given by a sum of atomic energy contri-
butions Em
atom ,nof every atom n:
E=NelemX
m=1Nm
atomX
n=1Em
atom ,n. (1)
Each atomic energy contribution is obtained from a feed-
forward neural network,
Em
atom ,n=bm,3
1+n2X
λ=1am,23
λ1·f2(
bm,2
λ+n1X
κ=1am,12
κλ
·f1"
bm,1
κ+nGX
i=1am,01
iκ·αm
i 
Gm
n,i−βm
i!#)
.(2)
The weights aandbare trained individually for each ele-
ment maccordingtoenergiesandatomicforcesofatrain-
ing data set containing multiple chemical structures. We
note that the number of hidden layers can differ from the
case shown, which is for two hidden layers with n1andn2
neuronseach. Avectoroflocalstructuraldescriptors Gm
n
ofdimension nG, whichisexplainedinthenextsection, is
given as inputto this atomicneural network. Wepropose
the activation function f(x) = 1 .59223·tanh( x), which is
discussed in Section S1.1 of the Supporting Information
alongwithatailoredweightinitializationscheme[40,75].
The convergence advantage and accuracy increase of this
activation function and weight initialization compared to
a hyperbolic tangent and a non-tailored weight initializa-
tion is shown in Table S3 and Figures S1 (a), S1 (b), and
S2 in the Supporting Information.
In contrast to the original HDNNP method [17] the
trainable weights αm
iandβm
iare introduced for stan-
dardization of the structural descriptor input Gm
n,i. For
this purpose, the structural descriptor is shifted by βm
i,
which is initialized by the mean of the descriptor values
Gm
n,iin the initial training data set for all atoms nof the
respective element m. The result is scaled by αm
i, which
is initialized by the inverse standard deviation of the re-
spective descriptor values in the initial training data. In
this way, the weights am,01
iκof the input layer are mul-
tiplied with values, which are centered around zero and
show a standard deviation of one for the initial train-
ing data. This standardization can improve the train-
ing performance and is adjustable in case of additional
training data. We note that the weight initialization can
be restricted to values inside a certain interval to avoid
numerical issues. The weight pair am,01
iκandαm
imay
be combined, but we treat them separately for technical
reasons during the optimization.

--- PAGE 4 ---
4
2.2. Element-Embracing Atom-Centered
Symmetry Functions
HDNNPs usually employ vectors of many-body atom-
centered symmetry functions [58] to represent the local
atomic environments. These descriptors fulfill the trans-
lational, rotational, and permutational invariances of the
potential energy surface. They depend on distances and
angles of all neighboring atoms which are inside a cut-
off sphere of radius Rc. This radius needs to be suf-
ficiently large to account for all relevant interactions.
Since no connectivities are used, the HDNNP is able to
describe chemical reactions. The dimensionality of the
input vector does not depend on the individual atomic
environment—a requirement to obtain generally applica-
ble atomic neural networks. Conventional ACSF vectors
are constructed in such a way that each ACSF represents
only all interactions between a specific chemical element
pair or triple. This construction, however, leads to an
unfavorable scaling in the number of descriptors with re-
spect to the number of elements.
Forthisreason, weintroduceelement-embracingatom-
centered symmetry functions (eeACSFs) which explicitly
depend on the element information Hfrom the periodic
table (see next paragraph). Similar to ACSFs, there are
two types of eeACSFs. The radial eeACSFs,
Grad
n,i="
1
Hrad
max,iNatomX
j̸=nHrad
i,jexp 
−ηrad
i·R2
nj
·fc(Rnj)#1
2
,(3)
are a function of the distances Rnjbetween the cen-
tral atom nand the neighboring atoms j. The angular
eeACSFs,
Gang
n,i=(
2−ζi
Hang
max,iNatomX
j̸=nNatomX
k̸=n,jHang
i,jk[1 +λicos (θnjk)]ζi
·exp
−ηang
i 
R2
nj+R2
nk
·fc(Rnj)·fc(Rnk))1
2
,
(4)
depend in addition on the angle θnjkbetween atom nand
the two neighbors jandk. Different values for the pa-
rameters ηrad
i≥0,ηang
i≥0,λi=±1, and ξi≥1of each
eeACSF ieventually produce a structural fingerprint of
the atomic environment of atom n. The cutoff function,
fc(Rnj) =

exp
1−
1−R2
nj
R2c−1
forRnj< R c
0 otherwise,
(5)
damps the contributions smoothly to zero beyond the
cutoff radius Rc. The advantage of this cutoff functionis that also the derivatives of all orders are zero at Rc,
which is beneficial for the calculation of forces, normal
modes, and so forth. In contrast to ACSFs, a square root
is applied to eeACSFs to mitigate a too strong effect of
the number of neighbors on the eeACSF value. Such a
strong effect can be observed for training data including
very different molecule sizes and particle densities, and it
can decrease parametrization performance due to a too
broad range of input values.
As element descriptors we propose nfor the period
number of the element in the periodic table, mfor the
group number in the s- and p-block (main group 1 to 8),
anddfor the group number in the d-block. Main group
elements obey d= 0and d-block elements m= 0. A
special case is helium with m= 8. The values of these
descriptors for the neighboring atom jare used in the
element-dependent term Hrad
i,jof the radial eeACSFs,
Hrad
i,j∈ {1, nj, mj, dj,nj:=X−nj,
mj:= 9−mj,dj:= 11−dj	
.(6)
For the construction of radial eeACSFs without element
dependence, Hrad
i,j= 1can be applied. In this work, the
maximum period is set to X−1 = 5, i.e., elements up
to xenon are considered. The descriptors n,m, and d
are employed to balance contributions of light and heavy
elementsaswellasofelementswithfewandmanyvalence
electrons. For main group elements d= 0is used and for
d-block elements m= 0. For elements of higher periods,
the group in the f-block can be implemented in the same
way as for the d-block and X−1can be set to 7. To keep
the contribution of each interaction to a value between 0
and 1, the radial eeACSF is divided by Hrad
max,iwhich is
the maximum possible value of Hrad
i,j.
The element-dependent terms Hang
i,jkof the angular
eeACSFs are calculated as linear combinations,
Hang
i,jk=Hrad
i,j+γiHrad
i,k+Ci, (7)
with γi=±1and
Ci=(
0 for γi= 1∨Hrad
i,j=Hrad
i,k= 0
1 otherwise.(8)
Asaconsequence, theelement-dependentprefactorofthe
angular eeACSF is defined as
Hang
max,i=(
2Hrad
max,iforγi= 1
Hrad
max,iotherwise. (9)
In conclusion, for systems with/without d-block ele-
ments five/seven different Hrad
i,jand nine/eleven Hang
i,jk
terms are required. In combination with typically around
fiveηrad
i, two ηang
i, two λi, and three ζiparameter val-
ues, the descriptor vector will consist of 25/35 radial and
108/132 angular eeACSFs independent of the number of
elements. By contrast, the number of radial ACSFs is
proportional to the number of elements and the number

--- PAGE 5 ---
5
of angular ACSFs scales withPNelem
m=1m. For example,
in the case of four elements, 20 radial and 120 angular
ACSFs are obtained for the same number of parameter
values. Hence, at around four elements is the break-even
point of computational cost, since the additional effort
for determining Hrad
i,jandHang
i,jkfrom the listed element-
dependent values is small. We note that the number of
different parameters values can affect the resolution of
the representation. The computational bottleneck in the
descriptor calculation during MLP applications is the de-
termination of the derivatives as a function of the atomic
positions to obtain the atomic forces. Since no additional
position-dependent properties are included in eeACSFs
compared to ACSFs, the computational cost per eeACSF
is similar to that of ACSFs.
2.3. Training of High-Dimensional Neural Network
Potentials
To optimize the weights of the atomic neural networks
with respect to potential energies Eref,rand Cartesian
atomic force components Fref,r
α,nof reference conforma-
tions r, a loss function is defined:
Lt=q2
NconfNconfX
r=1Er,t−Eref,r
Nr
atom2
+ 
3NconfX
r=1Nr
atom!−1
·NconfX
r=1Nr
atomX
n=1X
α=x,y,z 
Fr,t
α,n−Fref,r
α,n2.
(10)
The atomic force component is the negative gradient of
the energy with respect to the Cartesian coordinate αr
n=
xr
n, yr
n, zr
nof atom nof conformation r,
Fr
α,n=−∂Er
∂αrn. (11)
A set of Nconfconformations is trained simultaneously
in each training epoch tto accelerate the optimization
and reduce overfitting of single data points. Since the
HDNNP prediction of a conformation can be dependent
on the atomic neural networks of different chemical el-
ements, these networks are also trained simultaneously.
To balance the contributions of energies and forces to the
loss function, the hyperparameter qis used. This hyper-
parameter needs to be chosen with care as it can signif-
icantly affect the training performance. To optimize the
weights, in each training epoch the gradient of the loss
function with respect to the weights,
wξ:=wm,µν
χ,κλ∈ {am,µν
κλ, bm,ν
λ, αm
κ, βm
κ},(12)
is calculated, where ξis a unique index of each weight.2.4. Continual Resilient (CoRe) Optimizer
To improve the training process, we developed the con-
tinual resilient (CoRe) optimizer which aims to combine
the robustness of resilient backpropagation (RPROP)
[76, 77] with the performance of the Adam optimizer [78].
Moreover, we introduce adaptive decay rates of moving
averages of the loss function gradients, plasticity factors
of the weights obtained from an importance score, and
weight decays bounding the weight values to increase the
convergence speed and final accuracy beyond state-of-
the-art optimizers.
CoRe is a first-order gradient-based optimizer. It em-
ploys individual adaptive learning rates for each weight
wξ, which depend on optimization history. The algo-
rithm is intended for stochastic iterative optimizations,
i.e., subsamples of the batch of training data are used
in each training epoch. Thus, computational efficiency
benefits of stochastic gradient decent (SGD) [79] can be
exploited.
In the spirit of the Adam optimizer, exponential mov-
ing averages of the gradient,
gτ
ξ=βτ
1·gτ−1
ξ+ (1−βτ
1)∂Lt
∂wt
ξ, (13)
and the squared gradient,
hτ
ξ=β2·hτ−1
ξ+ (1−β2) 
∂Lt
∂wt
ξ!2
,(14)
with decay rates βτ
1, β2∈[0,1)are activated in the com-
putation of the individual adaptive learning rates of each
weight wt
ξ.τis the optimization step counter for each
weight, which can be different from the training epoch
t. For example, for HDNNPs a training data subsample
of a training epoch may not contain every chemical el-
ement of the entire training data set leading to weight
updates only for some of the atomic neural networks in
the respective training epoch. In this way, the HDNNP
model already provides an architectural strategy to mit-
igate catastrophic forgetting. We note that Equations
(13) and (14) represent the case of minimization, while
for maximization the sign of the loss function derivative
with respect to the weight must be inverted.
In contrast to the Adam optimizer, β1is a function of
τ,
βτ
1=βb
1+ 
βa
1−βb
1
exp"
−τ−1
βc
12#
.(15)
The hyperparameters βa
1, βb
1∈[0,1)define theinitial and
final values of β1. These values are interconverted by a
Gaussian with hyperparameter βc
1>0. A larger β1value
increases the dependence on previous gradient informa-
tion, that is, on the optimization history of all recent
trainingdatasubsamples. Thereby, theoptimizationper-
formance with respect to the entire training data set can

--- PAGE 6 ---
6
be improved. A smaller β1value yields a higher depen-
dence on the current gradient of the subsample, decreas-
inginsomewaythemomentofinertiaoftheoptimization
process. The latter is beneficial for rapidly and strongly
changing gradients occurring in fast convergence at the
beginning of an optimization, which is started from ran-
domly initialized weights. In conclusion, Equation (15)
can be employed to increase β1in the course of the opti-
mization to improve the training performance.
One contribution of the weight update magnitudes of
CoRe is obtained, in analogy to the Adam optimizer,
according to
uτ
ξ=gτ
ξ
1−(βτ
1)τ

hτ
ξ
1−(β2)τ1
2
+ϵ

−1
.(16)
The moving averages gτ
ξandhτ
ξget bias corrected by
1−(βτ
1)τand1−(β2)τ, respectively, to counteract their
initialization bias toward zero ( g0
ξ, h0
ξ= 0). The divi-
sion of the two bias-corrected moving averages makes the
weight update magnitudes invariant to gradient rescal-
ing. A form of step size annealing is obtained, because a
decreasing gradient value leads to a decrease of the up-
date uτ
ξ. Therefore, for well-behaving optimizations the
absolute value of uτ
ξtypically decreases from ±1in the
first optimization step τ= 1towards zero. Higher values
of the hyperparameter β2promote this annealing. The
hyperparameter ϵ⪆0is added for numerical stability.
As a second contribution to the weight update, the
plasticity factor,
Pτ
ξ=

0forτ > t hist
∧Sτ−1
ξtop-nm,µν
frozen ,χinSm,µν,τ −1
χ
1otherwise,(17)
isintroducedtomitigateforgettingofoldinformation. In
the initial training phase, this factor equals one. When
τ > t hist, with hyperparameter thist>0, the plasticity
factor can freeze the values of some weights by setting
Pτ
ξto zero. The selection of these weights depends on
a score value Sτ−1
ξ, which ranks the importance of the
weights with regard to previously predicted loss func-
tion decrease, as will be explained in Equation (20). The
score value Sτ−1
ξis compared to all other score values
Sm,µν,τ −1
χ of the same group. For HDNNPs, a group is
composed by the weights within the atomic neural net-
work of the same element m, with the same weight type
χ, and the same layer assignment µν. The weights be-
longing to the nfrozenhighest score values in their respec-
tive group are frozen for the optimization step τ. The
hyperparameter nm,µν
frozen ,χ≥0can be set individually for
each group of weights. In this way, a regularization is es-
tablished for weights with highest estimated importance.
A further contribution of the step size adjustment isadapted from RPROP,
sτ
ξ=

min
η+·sτ−1
ξ, smax
forgτ−1
ξ·gτ
ξ·Pτ
ξ>0
max
η−·sτ−1
ξ, smin
forgτ−1
ξ·gτ
ξ·Pτ
ξ<0
sτ−1
ξforgτ−1
ξ·gτ
ξ·Pτ
ξ= 0.
(18)
Ingeneral, thestepsize sτ
ξdoesnotdependonthemagni-
tudeofthegradientbutonlyonitssign, yieldingarobust
optimization. Every time gτ
ξchanges its sign compared
togτ−1
ξ, the previous optimization step probably jumped
over a local minimum. Then, the previous step size sτ−1
ξ
wastoolargeandneedstobedecreased. Ifthesignsof gτ
ξ
andgτ−1
ξare the same, sτ−1
ξcan be increased to speed up
convergence. The decrease factor η−and increase factor
η+, with 0< η−≤1≤η+, are applied to obtain the cur-
rent step size sτ
ξ, whereby sτ
ξis bounded by minimal and
maximalstepsizes smin, smax>0. IfPτ
ξ,gτ
ξ, and/or gτ−1
ξ
are zero, the step size update is omitted. The step size s0
ξ
needs to be initialized determining the first optimization
step size s1
ξ. The value can be chosen in reasonable pro-
portion to the initial weight values and experience has
shown that the precise choice of this parameter is rather
noncritical due to the fast adaption. We note that the
gradientisnotresetto gτ
ξ= 0forgτ−1
ξ·gτ
ξ<0,incontrast
to some RPROP variants which include a backtracking
weight step [76, 80]. As a consequence, the history of gτ
ξ
can be retained.
To decrease the risk of overfitting, the weight update,
wt
ξ= 
1−dm,µν
χ·uτ
ξ·Pτ
ξ·sτ
ξ
wt−1
ξ−uτ
ξ·Pτ
ξ·sτ
ξ,
(19)
includes a weight decay with weight decay hyperparam-
eterdm,µν
χ∈[0,(smax)−1). The weight wt−1
ξis reduced
by the fraction obtained from the product of dm,µν
χand
the absolute current weight update |uτ
ξ|·Pτ
ξ·sτ
ξ. We note
that the sign of the weight update—equal to the sign
ofgτ
ξ—is only encoded in uτ
ξ. The inverse weight decay
hyperparameter is the maximal absolute weight value in
well-behaving optimizations, i.e., uτ
ξ≤ ±1, preventing
strong increases or decreases of weights. To obtain the
new weight wt
ξ, the current weight update uτ
ξ·Pτ
ξ·sτ
ξis
subtracted from the previous weight wt−1
ξ. Hence, the
previous weight is changed in the opposite direction than
the sign of gτ
ξwith an individually adapted learning rate.
The score value,
Sτ
ξ=

Sτ−1
ξ+ (thist)−1gτ
ξ·uτ
ξ·Pτ
ξ·sτ
ξforτ≤thisth
1−(thist)−1i
Sτ−1
ξ
+ (thist)−1gτ
ξ·uτ
ξ·Pτ
ξ·sτ
ξotherwise,
(20)
accounts for weight specific contributions to previous loss
function decreases. It is inspired from the synaptic intel-
ligence method [53]. The loss function decrease is esti-

--- PAGE 7 ---
7
mated by the product of the moving average of the gradi-
entgτ
ξand the weight update uτ
ξ·Pτ
ξ·sτ
ξfor each step τ.
Forinfinitesimallysmallchangesintheoppositedirection
ofthegradient,theproductofgradientandchangeequals
the respective loss function decrease. For larger updates
in optimizations, as in the present case, the loss function
decrease is typically overestimated by this product, but
still reasonable. In addition, the gradient will be noisy
if it is calculated for a subsample of the entire training
data set. The sign inversion of the update is omitted in
the calculation of the score value. Consequently, a higher
positive score value represents a larger loss function de-
crease. The score value is initialized as S0
ξ= 0. For each
τ≤thist,gτ
ξ·uτ
ξ·Pτ
ξ·sτ
ξis summed with equal contri-
bution (thist)−1to build an initial history. Afterwards,
Sτ
ξis calculated as an exponential moving average with
decay parameter 1−(thist)−1. The score value identi-
fies the most important weights for the accurate predic-
tion of previous training data. These weights can then
be restricted by the plasticity factor (Equation (17)), to
balance the stability-plasticity ratio.
As a variant, CoRe can also use the sign of the moving
average of the gradient gτ
ξas update factor uτ
ξ, i.e., uτ
ξ=
sgn(gτ
ξ). Furthermore, CoRe can be employed without
plasticity factors, i.e., nm,µν
frozen ,χ= 0. The Adam optimizer
is a special case of CoRe for the hyperparameter settings
βa
1=βb
1,nm,µν
frozen ,χ= 0,η+, η−= 1, and dm,µν
χ = 0.
RPROP without the backtracking weight step is another
special case for the hyperparameter settings βa
1, βb
1, β2=
0,nm,µν
frozen ,χ= 0, and dm,µν
χ = 0.
General recommendations of the hyperparameter val-
ues can be provided for β2= 0.999,ϵ= 10−8,η−= 0.5,
η+= 1.2,smin= 10−6, and smax= 1, whicharemostlyin
agreement with the Adam optimizer and RPROP. Typ-
ically, s0
ξ= 10−3is a good choice for the initial learn-
ing rate. The weight decay dm,µν
χdepends on the de-
sired range of weight values. For example, weights asso-
ciated with the output neuron in atomic neural networks
should not be restricted to allow for an arbitrary atomic
energy value, while weights connecting input and hid-
den layers or hidden and hidden layers applying hyper-
bolic tangents as activation functions can be restricted
by using dm,µν
χ = 0.1.βc
1andthistdepend on the con-
vergence speed and the total number of epochs. For
example, βc
1= 500andthist= 500were employed in
this work, while the total number of epochs were 1500,
2000, and 2500. In addition, nm,µν
frozen ,χwas set to 1%of
the number of weights am,µν
κλandbm,ν
λin their respective
group, except for those weights associated with the out-
put neuron. βa
1andβb
1need to be smaller for rapidly
and strongly changing gradients. For example, βa
1= 0.45
andβb
1= 0.7,0.725were used in this work. We note
that for highly uncorrelated training data an increase of
the subsample size can be beneficial to reduce gradient
fluctuations.2.5. Lifelong Adaptive Data Selection
Stochasticoptimization, thatisgradientcalculationon
a subsample of the training data in each epoch, can re-
tain the computational demand on a manageable level
for large data sets. However, random subsamples may
lead to an inefficient approach because various data are
typically represented at different levels of accuracy. This
issue will be even more severe if additional training data
are added during the training process in lifelong machine
learning. Moreover, redundant and incorrect data need
to be removed during the training process to be able
to learn autonomously from a continuous stream of new
data. Still, rehearsal of representative old data is very
important to avoid catastrophic forgetting.
To find a solution for these issues, we developed an
algorithm for lifelong adaptive data selection. The first
key ingredient of the algorithm is an adaptive selection
factor Sr
histfor each training conformation rdepending
on the conformation’s loss function contribution and the
traininghistory. Thesecondkeyingredientisabalancing
scheme for the stochastic choice of good and bad repre-
sented training data. The algorithm intends to exclude
redundant and incorrect data and to improve the train-
ing performance by balancing adjustment to new or bad
represented data and rehearsal of good represented data.
Algorithm 1: Choice of the training data subsample to
be fitted in a training epoch in the lifelong adaptive data
selection algorithm. All vector operations are element-
wise. Assignmentstatementsincludingconditionschange
only vector entries for which the condition is true.
Nfit←min
Nfit,len 
S>0
hist
Ngood←int (pgood·Nfit)
Nbad←Nfit−Ngood
Lmax
old←max
L\NaN
old
Pbad←ShistLold
Lmax
old
Pbad←max (Shist) if Pr
bad= NaN
Pbad←Pbad
sum (Pbad)
Dfit←random_choice ( D,Pbad, Nbad)
Pgood←S\fit
hist"
1−L\fit
old
Lmax
old#
Pmin
good←min 
P>0
good,1
·ϵ′
Pgood←Pmin
good ifL\fit
old,r=Lmax
old
Pgood←Pmin
good ifPr
good= NaN
Pgood←Pgood
sum (Pgood)
Dfit←Dfit∪random_choice
D\fit,Pgood, Ngood
The lifelong adaptive data selection algorithm can be
subdivided into a part carried out before the loss func-
tion calculation and a part after this calculation. In the
first part (Algorithm 1) the training data subsample is
chosen for which the loss function and the respective gra-
dients are calculated. This subsample is indicated by the

--- PAGE 8 ---
8
index “fit” as these data are used for fitting the weights.
The number of conformations to be fitted per epoch Nfit
is a training hyperparameter and can be chosen based
on the data set size and the correlation between the data
points. Since conformations can be excluded by an adap-
tive selection factor Sr
hist(Algorithm 2), Nfitcannot be
larger than the number of conformations with Sr
hist>0,
i.e.,len 
S>0
hist
. The training data subsample is split
into a set of Ngoodalready well represented conforma-
tions and Nbadnew or insufficiently represented confor-
mations. The fraction of good data is defined by pgood
(Algorithm 2), while Ngoodhas to be an integer. pgoodis
initialized as 0.
To determine the probabilities PbadandPgoodfor con-
formationstobechosenasbadorgooddata, respectively,
the contributions of the conformations to the loss func-
tion are employed. The last calculated contributions Lold
(or Not a Number (NaN) if no contribution was calcu-
lated for a conformation so far) are divided by the max-
imal, non-NaN contribution Lmax
old. These quotients are
multipliedbytheconformationspecificadaptiveselection
factors Shistto yield the probabilities Pbad.Shistis ini-
tialized as unity vector. All NaN components of Pbadare
set to the maximum of Shist. For normalization Pbadis
divided by the sum of its components. A random choice
ofNbadconformations with probabilities Pbadis selected
from the training data set Dto be part of the training
data subsample.
To obtain Ngoodgood data from the remaining data
setD\fit, the probability Pgoodis determined. Pgoodis
proportional to the difference between a unity vector 1
and the quotient of L\fit
oldandLmax
old. The minimum non-
zero probability of Pgoodis determined and multiplied
byϵ′⪆0to obtain a very low probability Pmin
goodfor non-
excluded but unfavored conformations. Pgoodentries are
set to Pmin
goodif the loss function contribution of the corre-
sponding conformation is equal to the maximal contribu-
tion or if the Pgoodentry is NaN. Subsequently, Pgoodis
divided by the sum of its contributions. The good data
subsample is a random choice of Ngooddata from D\fit
with probabilities Pgood. The union of the selected bad
and good data is then employed as training data subsam-
ple to calculate the loss function.
To update the adaptive selection factors Shist, the new
loss function contributions Lfit
newof the conformations
in the training data subsample are used (Algorithm 2).
Therefore, a weighted sum of energy loss Lfit
Eand force
lossLfit
Fis employed, which is divided by the number
of atoms of the respective conformations Nfit
atom.Lrel,
which is the quotient of Lfit
newand the total loss Lfit
newof
the training epoch as defined in Equation (10), is com-
pared to different thresholds T1
F<1< T2
F< T3
F< TX
to update Shistand the exclusion strike counter X. The
latter is introduced to exclude prediction outliers. Un-
der the assumption that the model is reasonable, outliers
are presumably incorrect data points. Therefore, X—
initialized as zero vector—counts how many consecutive
evaluations Lr
relis greater than the threshold TX. IfXrAlgorithm 2: Update of the adaptive selection factors
for each conformation in the training data subsample in
the lifelong adaptive data selection algorithm. All vector
operations are element-wise. Assignment statements in-
cluding conditions change only vector entries for which
the condition is true.
Lfit
new←q2Lfit
E+1
3Lfit
F
Nfit
atom
Lrel←Lfit
new
Lfit
new
X←Xr+ 1 if Lr
rel> TX
X←0 ifLr
rel≤TX
Shist←max (1 , Sr
hist) if Lr
rel≥T1
F
Shist←min ( Sr
hist,1) if Lr
rel≤T2
F
Shist←Sr
hist·F−−ifLr
rel< T1
F∧Lr
new≤Lr
old
Shist←Sr
hist·F−ifLr
rel< T1
F∧Lr
new> Lr
old
Shist←Sr
hist·F+ifT2
F< Lr
rel≤T3
F∧Lr
new> Lr
old
Shist←Sr
hist·F++ifLr
rel> T3
F∧Lr
new> Lr
old
Shist←0 ifXr≥NX
Shist←0 ifSr
hist< Smin
hist∨Sr
hist> Smax
hist
pgood←cliph
pgood+ sgn
Lfit
new−Lfit
old
p±,0, pmax
goodi
Lfit
old←Lfit
new
Lfit
old←Lfit
new
reaches the hyperparameter NX, the adaptive selection
factor Sr
histis set to zero and the respective conformation
is thus excluded from further training.
The adaptive selection factors Shistare further modi-
fied depending on the following conditions: If Lr
rel≥T1
F
andSr
hist<1,Sr
histis set to one. If Lr
rel≤T2
Fand
Sr
hist>1,Sr
histis also changed to one. In this way, Sr
hist
of well represented data ( Lr
rel< T1
F) can be lower than
one leading to lowering of the probability to be chosen for
the training data subsample. On the other side, Sr
histof
badrepresenteddata( Lr
rel> T2
F)canbegreaterthanone
increasing the selection probability. For conformations
with T1
F≤Lr
rel≤T2
F,Sr
histis resetted to one, because
Lr
relis around the average loss contribution. In addition,
Shistcanbemodifiedbydecreasefactors F−−andF−and
increase factors F+andF++. In this way, Sr
histtracks the
training history to assess the training importance of the
associated conformation r. The decrease and increase
factors are initialized as F−(−)= (Smin)(NF−(−))−1
and
F+(+) = (Smax)(NF+(+))−1
, respectively. Consequently,
the hyperparameters NF−−,NF−,NF+, and NF++define
how many consecutive applications of the associated de-
crease and increase factor lead to an adaptive selection
factor below and above the hyperparameters Sminand
Smax, respectively. For well represented data Sr
histis de-
creased by F−−if the loss function contribution stayed
constant or decreased compared to the previously cal-
culated one. Otherwise Sr
histof well represented data is
less strongly decreased by F−. For T2
F≤Lr
rel≤T3
Fand
Lr
new> Lr
old,Sr
histisincreasedby F+, whileitisincreased
by the larger factor F++forLr
rel≥T3
FandLr
new> Lr
old.
Afterwards, all Sr
histbelow Sminare set to zero for data

--- PAGE 9 ---
9
set reduction because the associated conformations are
steadily well represented by the model. Thus, these data
are presumably redundant and can be excluded since re-
hearsal of other data and/or other learning strategies are
sufficient to not forget these conformations. All Sr
hist
above Smaxare also set to zero because the model was
not able for many optimization steps to represent these
conformations accurately. Therefore, under the assump-
tion that the model is reasonable, these conformations
are not consistent to the major part of the other data
and can be removed from the training data. In this way,
the training performance improves for the other data.
For adaptive balancing of learning bad represented
data and not forgetting good represented data, the frac-
tion of good data pgoodis modified according to the
change of the total loss Lfit
new−Lfit
old.pgoodis increased by
p±if the loss change is positive, while it is decreased by
p±for negative loss change. Loldis initialized as infinity
andp±is defined as pmax
good·(Np)−1, with the maximal
pgoodvalue pmax
goodand the total number of possible pgood
values Np. Lastly, the new loss function contributions
Lfit
newoverwrite the old loss function contributions Lfit
old
for the conformations of the training data subsample and
the new total loss value Lfit
newreplaces the old total loss
value Lfit
old.
2.6. Uncertainty Quantification
An lMLP requires to predict the uncertainties of the
energy ∆Eand the atomic force components ∆Fα,nin
addition to their values for a chemical structure. The
reason is that a prerequisite of an lMLP is that it can
produce results at every training stage. Since the pre-
dictions at different training stages can vary, the lMLP
needs to provide a measure of the prediction accuracy.
Then, the predictions can be compared within their ac-
curacy in which they should agree. In this way, a re-
liable method is obtained which still can adapt to new
information over time. We note that uncertainty quan-
tification also significantly advances other computational
chemistrymethodsbecauseoverinterpretationoffeatures
beyond the method’s accuracy can be prevented.
The uncertainty of machine learning models can be
separated into contributions from noise in the reference
data, biasofthemodel, andmodelvariancecontributions
[81]. The handling of noisy training data is addressed by
the lifelong adaptive data selection, while noise of the
test data does not influence the model performance but
onlytheperformanceevaluation. Forcorrectlyconverged
electronic structure calculations the noise is determined
by the convergence thresholds and thus small for MLP
reference data. Model bias is caused by limitations in
the model architecture, representation by the descrip-
tors, and reference data set size. Therefore, the model
architecture needs to be optimized for different appli-
cations and elaborate descriptors need to be tested tokeep this error contribution small. Lifelong learning ad-
dresses the remaining part of incomplete reference data.
Model variance error can be reliably reduced by ensemble
(or committee) models and its size can be predicted by
statistics. The respective uncertainty estimation is based
on the huge flexibility of the mathematical expressions
underlying machine learning models like artificial neu-
ral networks. For well trained conformation space the
ensemble member predictions presumably agree, while
the predictions are arbitrary for unknown conformation
space. Therefore, they can spread largely for an ensem-
ble of independently and differently trained MLPs, i.e.,
using, for example, different initial weights.
Consequently, ensembles of MLPs have been used for a
straightforward uncertainty quantification of model vari-
ance contributions [69–73, 82]. The final energy predic-
tionEis obtained by the mean of the ensemble of NMLP
MLP energies Ep,
E=1
NMLPNMLPX
p=1Ep. (21)
Analogously, the atomic force components Fα,nare cal-
culated as mean of the negative energy gradient with re-
spect to the Cartesian coordinates αnof an MLP ensem-
ble,
Fα,n=−1
NMLPNMLPX
p=1∂Ep
∂αn. (22)
The energy uncertainties ∆Ecan then be obtained from
the sample standard deviation of the ensemble member
energy predictions,
∆E= max
RMSE( E),
"
c2
NMLP−1NMLPX
p=1 
Ep−E2#1
2

,(23)
whiletheminimaluncertaintyisdefinedbytherootmean
square error RMSE( E)of the reference data. The atomic
force component uncertainties ∆Fα,ncan be analogously
calculated.
The scaling factor cis introduced to adjust the sample
standarddeviationprovidingacertainconfidenceinterval
that the uncertainty quantification is equal to or larger
than the actual error with respect to the reference data.
However, this uncertainty quantification misses model
bias contributions and hence underestimates the error
in cases where the lack of training data is the primary
error source. Despite that, as also the model variance
increases in these cases, this uncertainty quantification
can still signal whether the error will be high even if the
predicted uncertainty value is not accurate. For applica-
tions of MLPs this information is sufficient because small
uncertainties can be well predicted and the occurrence of
large errors requires in any way an improvement of the

--- PAGE 10 ---
10
calculation which can be achieved by lifelong machine
learning.
2.7. Perspective of Lifelong Machine Learning
Potentials
On the long-term perspective an lMLP which learns
moreandmoresystemsovertimewhilethetrainingbene-
fits from prior learned knowledge of chemical interactions
is the grand goal. The lMLP learns then in the same
way as chemistry evolves, i.e., by systematically expand-
ing prior knowledge. As this work is a proof of concept,
the presented lifelong training strategies are naturally
not in such an advanced state yet to reach this grand
goal. In the short-term perspective with the presented
algorithms, lMLPs will presumably be trained on data
of related systems because the benefit is the largest and
the difficulty is the lowest in this case. When the life-
long training strategies as well as the MLP models are
developed to a more elaborate state, more and more in-
formation can be combined in one lMLP.
This work presents a comprehensive basis of ingredi-
ents for lMLPs. Future works need, for example, to
develop algorithms for the continual expansion of the
model architectures to avoid information capacity lim-
itations of the underlying deep learning representations.
For instance, neural network growth can be established
by using differently sized architectures of the lMLP en-
semble members. By tracking their relative performance
the requirement to grow can be identified and indi-
vidual ensemble members can be adapted. Moreover,
coarse-grained structural identifiers like the bin and hash
method [83] can be applied to augment the selection and
exclusion criteria of training data used for rehearsal. In
addition, they can be employed for fast checks whether
a certain conformation is represented by similar ones in
the training data. From this information unknown and
badly represented conformations can be distinguished to
avoid redundant reference calculations.
Current MLP models are often limited to the repre-
sentation of a single electronic state. Thus, for example,
onlyaspecifictotalchargeandspinmultiplicitycanoften
be represented by a single MLP. However, recent works
target these limitations for more general MLP models
[28, 33, 84] which can then be used as base model for
lMLPs. A current restriction of MLPs is the require-
ment of consistent training data, i.e., all training data
have to be calculated by the same method, basis set, etc.
Therefore, MLP models need to be developed which, for
example, treat different reference methods in a similar
fashion as different electronic states to establish lMLPs
learning from data of different reference methods.
Further, open, free, and accessible reference data of
published work is obviously highly important. To con-
tinue the training of previous work, not only the final
weight values but also the associated CoRe optimizer
properties of the weights τ,gτ
ξ,hτ
ξ,sτ
ξ, and Sτ
ξand thelifelongadaptivedataselectionproperties Lr
old,Sr
hist, and
Xrof every still required training conformation as well
as the values Loldandpgoodneed to be saved and made
available. Moreover, autonomous workflows need to be
set up in future work for a user-friendly development of
anlMLPwithdirectinterfacestoelectronicstructureand
atomistic simulation software.
3. COMPUTATIONAL DETAILS
In the HDNNP models of this work, each atomic neu-
ral network consisted of an input layer with nG= 153
(eeACSFs) or 156(ACSFs) neurons, three hidden layers
with n1= 102,n2= 61, and n3= 44neurons, and a sin-
gle output neuron. The weight initialization is described
in Section S1.1 of the Supporting Information. In pre-
dictions, the ensemble size NMLP= 10was applied with
the error scaling factor c= 2. We note that the ensemble
size was not optimized for individual cases, but it can be
in future applications depending on the improvement of
value and uncertainty prediction versus additional com-
putational demand.
For the training of each HDNNP, we split the reference
data set randomly into 90%training conformations and
10%test conformations. In each training epoch, 10%of
all training conformations were used for fitting reference
data sets A and B (Table 2) and 4.07%were employed
for fitting reference data set C. The latter yields the same
number of trained conformations per epoch for reference
data set C compared to B. This counting includes con-
formations with an adaptive selection factor of Sr
hist= 0
and conformations which were first added at a late train-
ing epoch. To represent the molecular structures of the
reference conformations, eeACSF vectors were equipped
with the parameters given in Table 1. A larger than
usual cutoff radius of Rc= 12Å was applied to avoid
issues with electrostatic interactions, which are not in
the focus of this work. However, future studies can eas-
ily adopt more elaborate schemes for electrostatics like
fourth-generation HDNNPs [28].
The supervised training was performed on the total
DFT energy minus the sum of the atomic DFT energies
of the neutral free atoms in their lowest spin state (see
Table S1 in the Supporting Information). All DFT data
of SN2 reactions were obtained for a total charge of −1e,
with ebeing the elementary charge, and a spin multi-
plicity of 1. The DFT calculations were performed with
the quantum chemistry program ORCA (version 5.0.3)
[85, 86]. The PBE exchange-correlation functional [87]
was chosen with the def2-TZVP basis set [88] and use of
the RI-J approximation. In addition, D3 semi-classical
dispersion corrections [89] with Becke-Johnson damping
[90, 91] were applied. To balance the training on the
reference DFT energies and forces, the loss function hy-
perparameter was set to q= 10.9.
For SGD optimizations the best found learning rate of
0.00075 was applied. RPROP optimizations were per-

--- PAGE 11 ---
11
Table 1: All combinations of the listed parameters
were applied for radial and angular eeACSFs,
respectively. The cutoff radius was set to Rc= 12Å for
all eeACSFs. γ=±1was used except for Hang= 1,
where only γ= 1was applied.
Radial eeACSFs
Hrad1,n,m,n,m
ηrad/Å−20,0.010702,0.023348,0.044203,
0.066118,0.104168,0.180285,
0.370959,1.115414
Angular eeACSFs
Hang1,n,m,n,m
ηang/Å−20.011238,0.090144
λ −1,1
ξ 1,2.409421,9.996864
formed with the hyperparameters recommended in Ref-
erence 76. An exception was the initial learning rate,
for which 0.001 was found to be the optimal choice in
this work. For the Adam optimizer the same hyperpa-
rameters were used as in Reference 78 since no general
improvement by hyperparameter variations was found.
The hyperparameters of the CoRe optimizer were:
βa
1= 0.45,βb
1= 0.7,0.725,βc
1= 500,β2= 0.999,
ϵ= 10−8,η−= 0.5,η+= 1.2,smin= 10−6,smax= 1,
s0
ξ= 10−3, and thist= 500.nm,µν
frozen ,χwas set to 1%of
the number of weights am,µν
κλandbm,ν
λin their respective
group. Exceptions were those weights associated with
the output neuron, for which nm,µν max
frozen ,χwas 0. For the
weights αandβ,nm,µν
frozen ,χ= 0was applied. The weight
decay hyperparameter dm,µν
χwas0.01forχ=α, β, 0 for
ν=νmax, and 0.1otherwise. Training was performed for
1500, 2000, and 2500 epochs.
The hyperparameters of the lifelong adaptive data se-
lection were: Smin
hist= 0.1,Smax
hist= 100,p
T1
F= 0.9,p
T2
F= 1.2,p
T3
F= 2.0,NF−−= 30,NF−= 100,
NF+= 500,NF++= 150,√TX= 7.5,NX= 5,
pmax
good=2
3,Np= 20, and ϵ′= 10−6.
The lMLP software was written in Python and ex-
ploits the scientific computing package NumPy (version
1.21.6) [92] and the machine learning framework Py-
Torch (version 1.12.1) [93]. It is available on Zenodo
(DOI: 10.5281/zenodo.7912832) alongside the reference
data sets, generated output of this work, and scripts to
analyze and plot this output.
For a fair and reliable comparison of the different MLP
trainings, we performed for each setting 20 trainings with
different random numbers used in the splitting of train-
ing and test data, weight initialization, and training data
selection process. For each setting, the best ten MLPs
were employed in the analysis to reduce the dependence
on the explicit random numbers. In this way, the effect
of outliers was minimized that can originate from an un-favorable initial parameter choice, which can affect all
optimizers. Thus, the provided means and standard de-
viationsrepresenttheperformanceforwellworkingcases.
As we employed different splittings of training and test
data in the training of ensemble members in this work,
theensemblepredictionofthereferencedatamixestrain-
ing and test data predictions. While this approach effi-
ciently uses all reference data for training, unbiased val-
idation of the ensemble needs to be performed on addi-
tional data.
4. RESULTS AND DISCUSSION
4.1. Reference Data
For the comparison of the performance of ACSFs and
eeACSFs, areferencedatasetAwasconstructedcontain-
ingtendifferentgas-phaseS N2reactions. Thesereactions
were represented by 2026 reference conformations and
theirrespectiveenergiesandatomicforcecomponentsob-
tained from the reference method. The reference confor-
mationsweredifferentchemicalstructureswhichsampled
theconformationspaceoftheS N2reactionsincludingthe
leaving groups X−=Cl−, I−and nucleophiles Y−=Cl−,
HCC−, I−for central methyl carbon atoms and tertiary
tert-butyl carbon atoms (Figure 1). We note that steric
hindrance leads to high energy barriers in S N2 reactions
of tertiary carbon atoms. This set of S N2 reactions in-
cludedonlyfourdifferentelementssothattheapplication
of ACSFs was still feasible in terms of computational de-
mand. The combinatorial growth of the ACSF vector
with the number of elements hampers the application to
reference data including more elements.
11
The lMLP software was written in Python and exploits
the scientiﬁc computing package NumPy (version 1.21.6)
[91] and the machine learning framework PyTorch (ver-
sion 1.12.1) [92]. It will be available on Zenodo alongside
the reference data sets, generated output of this work,
and scripts to analyze and plot this output.
For a fair and reliable comparison of the diﬀerent MLP
trainings, we performed for each setting 20 trainings with
diﬀerent random numbers used in the splitting of train-
ing and test data, weight initialization, and training data
selection process. For each setting, the best ten MLPs
were employed in the analysis to reduce the dependence
on the explicit random numbers. In this way, the eﬀect
of outliers was minimized that can originate from an un-
favorable initial parameter choice, which can aﬀect all
optimizers. Thus, the provided means and standard de-
viations represent the performance for well working cases.
As we employed diﬀerent splittings of training and test
data in the training of ensemble members in this work,
the ensemble prediction of the reference data mixes train-
ing and test data predictions. While this approach eﬃ-
ciently uses all reference data for training, unbiased val-
idation of the ensemble needs to be performed on addi-
tional data.
4. RESULTS AND DISCUSSION
4.1. Reference Data
For the comparison of the performance of ACSFs and
eeACSFs, a reference data set A was constructed contain-
ing ten diﬀerent gas-phase S N2 reactions. These reactions
were represented by 2026 reference conformations and
their respective energies and atomic force components ob-
tained from the reference method. The reference confor-
mations were diﬀerent chemical structures which sampled
the conformation space of the S N2 reactions including the
leaving groups X−= Cl−, I−and nucleophiles Y−= Cl−,
HCC−, I−for central methyl carbon atoms and tertiary
tert-butyl carbon atoms (Figure 1). We note that steric
hindrance leads to high energy barriers in S N2 reactions
of tertiary carbon atoms. This set of S N2 reactions in-
cluded only four diﬀerent elements so that the application
of ACSFs was still feasible in terms of computational de-
mand. The combinatorial growth of the ACSF vector
with the number of elements hampers the application to
reference data including more elements.
Figure 1:S N2 reaction at a methyl carbon atom with
leaving group X and nucleophile Y.By contrast, the size of the eeACSF vector stays con-
stant with the number of elements enabling training of
reference data containing an arbitrary number of ele-
ments. To test eeACSFs, a reference data set B was
compiled consisting of 8600 conformations including 42
diﬀerent S N2 reactions with leaving groups X−= Cl−, I−,
nucleophiles Y−= Br−, Cl−, F−, H2N−, H3CO−, HCC−,
HO−, HS−, HSe−, I−, NC−, and central methyl andtert-
butyl carbon atoms.
Both reference data sets A and B contained conforma-
tions obtained in constrained DFT optimizations. The
distances carbon-leaving group and nucleophile-carbon
were set in the range from1.05to5.25Å by an irreg-
ular sampling approach. A preliminary grid of distance
values, which was more dense for smaller distances, was
deﬁned with individual minimal distances for each S N2
reaction system. To obtain theﬁnal distance values, ran-
dom changes on the grid values were applied with the
constraint that the values cannot get smaller or larger
than adjacent grid values..
To add the representation of structural distortions,
12551 conformations of the S N2 reaction systems with
central methyl carbon atoms were generated by random
atomic displacements. For each conformation obtained in
constrained DFT optimizations, three new conformations
were generated by randomly displacing all atomic posi-
tions inside atom-centered spheres with the same radii
of0.05,0.1, and0.15Å. If some interatomic distances
turned out to be too small, the process was restarted
with another set of random displacements. These con-
formations together with those of reference data set B
formed reference data set C. We emphasize that these
reference data were only employed in performance evalu-
ations of lifelong learning and can be insuﬃcient to carry
out atomistic simulations because relevant reference con-
formations can be missing.
The only preprocessing of the reference data sets was
a restriction to maximal absolute atomic force compo-
nents of15 eVÅ−1to exclude those conformations which
only occur under extreme conditions. The energy and
atomic force component ranges and standard deviations
of the diﬀerent data sets are compiled in Table 2 and are
referred to in performance comparisons in the following
sections. The mean energy ranges and standard devia-
tions for the individual S N2 reaction systems are provided
in Table S3 in the Supporting Information.
4.2. Element-Embracing Atom-Centered
Symmetry Functions
For four elements the sizes of the optimized ACSF
vector (156) and eeACSF vector (153) are similar (see
Table S2 in the Supporting Information for parameters
of the ACSFs). Therefore, this number of elements is
the turning point at which the eeACSF representation
becomes computationally advantageous. Table 3 shows
that the representation by ACSFs and eeACSFs of ref-
Figure 1: SN2 reaction at a methyl carbon atom with
leaving group X and nucleophile Y.
By contrast, the size of the eeACSF vector stays con-
stant with the number of elements enabling training of
reference data containing an arbitrary number of ele-
ments. To test eeACSFs, a reference data set B was
compiled consisting of 8600 conformations including 42
differentS N2reactionswithleavinggroupsX−=Cl−, I−,
nucleophilesY−=Br−, Cl−, F−, H2N−, H3CO−, HCC−,
HO−, HS−, HSe−, I−, NC−, andcentralmethyland tert-
butyl carbon atoms.
Both reference data sets A and B contained conforma-
tions obtained in constrained DFT optimizations. The
distances carbon-leaving group and nucleophile-carbon
were set in the range from 1.05to5.25Å by an irreg-
ular sampling approach. A preliminary grid of distance

--- PAGE 12 ---
12
values, which was more dense for smaller distances, was
defined with individual minimal distances for each S N2
reaction system. To obtain the final distance values, ran-
dom changes on the grid values were applied with the
constraint that the values cannot get smaller or larger
than adjacent grid values..
To add the representation of structural distortions,
12551 conformations of the S N2 reaction systems with
central methyl carbon atoms were generated by random
atomicdisplacements. Foreachconformationobtainedin
constrainedDFToptimizations, threenewconformations
were generated by randomly displacing all atomic posi-
tions inside atom-centered spheres with the same radii
of0.05,0.1, and 0.15Å. If some interatomic distances
turned out to be too small, the process was restarted
with another set of random displacements. These con-
formations together with those of reference data set B
formed reference data set C. We emphasize that these
reference data were only employed in performance evalu-
ations of lifelong learning and can be insufficient to carry
out atomistic simulations because relevant reference con-
formations can be missing.
The only preprocessing of the reference data sets was
a restriction to maximal absolute atomic force compo-
nents of 15 eVÅ−1to exclude those conformations which
only occur under extreme conditions. The energy and
atomic force component ranges and standard deviations
of the different data sets are compiled in Table 2 and are
referred to in performance comparisons in the following
sections. The mean energy ranges and standard devia-
tionsfortheindividualS N2reactionsystemsareprovided
in Table S4 in the Supporting Information.
Table 2: Number of S N2 reactions NSN2, conformations
Nconf, atoms in total Ntotal
atom, and elements Nelemfor the
different reference data sets A, B, and C. Additionally,
the energy range Eref
rangeand standard deviation Eref
std
and the atomic force component range Fref
α,n,rangeand
standard deviation Fref
α,n,stdare provided.
Reference data set A B C
NSN2 10 42 42
Nconf 2026 8600 21151
Ntotal
atom 22983 100117 190065
Nelem 4 10 10
Eref
range/meV atom−11715.3 1855.0 2018.0
Eref
std/meV atom−1382.6 373.8 391.5
Fref
α,n,range/meVÅ−129883 29984 29998
Fref
α,n,std/meVÅ−11098 1066 17064.2. Element-Embracing Atom-Centered
Symmetry Functions
For four elements the sizes of the optimized ACSF
vector (156) and eeACSF vector (153) are similar (see
Table S2 in the Supporting Information for parameters
of the ACSFs). Therefore, this number of elements is
the turning point at which the eeACSF representation
becomes computationally advantageous. Table 3 shows
that the representation by ACSFs and eeACSFs of ref-
erence data set A yields HDNNPs with similar RMSEs
for the test data justifying the structural representation
by eeACSFs (see Figures S3 (a) and (b) in the Support-
ing Information for the prediction error distribution of
the ensemble). The accuracy of HDNNPs using ACSFs
is on average slightly better. This trend is expected
due to the full separation of contributions from differ-
ent element combinations, while these contributions are
mixed in eeACSFs. However, the eeACSF representa-
tion is less prone to overfitting according to the given
training and test RMSEs. The reason may be that ev-
ery eeACSF value depends on all neighbor atoms inside
the cutoff radius, while an ACSF value depends only on
certain neighbor atoms. The latter may adjust better
to very specific environments but worsens generalization
and transferability. The Figures S4 (a) and (b) and S5 in
the Supporting Information show that the convergence
and training process is similar for ACSF and eeACSF
representations.
Table 3: RMSE values of individual HDNNPs, i.e.,
before ensembling, and the ensemble trained on
reference data set A using ACSF or eeACSFs. The
CoRe optimizer and lifelong adaptive data selection
were applied for 2000 epochs.
Individual HDNNPs ACSF eeACSF
RMSE( Etrain)/meV atom−12.0±0.2 2.5±0.3
RMSE( Etest)/meV atom−12.3±0.2 2.8±0.3
RMSE( Ftrain
α,n)/meVÅ−159±3 73 ±3
RMSE( Ftest
α,n)/meVÅ−187±9 92 ±8
Ensemble
RMSE( E)/meV atom−10.9 1.3
RMSE( Fα,n)/meVÅ−134 44
To train the ten element containing reference data
set B, the size of the ACSF vector would be 750
to obtain the same resolution by the parameters
ηrad,ηang,λ, and ξ. The resulting high com-
putational demand can be prevented by applying
eeACSFs with a constant vector size of 153. The
accuracy of individual HDNNPs is RMSE( Etrain) =
(3.9±0.4) meV atom−1,RMSE( Etest) = (4 .5±
0.6) meV atom−1,RMSE( Ftrain
α,n) = (99 ±7) meVÅ−1,
andRMSE( Ftest
α,n) = (116 ±4) meVÅ−1. The higher

--- PAGE 13 ---
13
accuracy of the results in Table 3 is a reason of the
fewerandlesscomplexreferencedatatobetrained, while
the model architecture and training hyperparameters re-
mained unchanged. However, especially the HDNNP en-
semble accuracy of RMSE( E) = 2 .6 meV atom−1and
RMSE( Fα,n) = 64 meV Å−1(see Figures S6 (a) and
(b) in the Supporting Information for the prediction er-
ror distribution) is comparable to other state-of-the-art
MLPs trained for less elements [19] and evidences that
eeACSFs are able to represent the different local atomic
environments including various neighbor elements. Fur-
ther, the significantly improved accuracy of the HDNNP
ensemble compared to that of individual HDNNPs sup-
ports the use of an ensemble beyond the access of uncer-
tainty quantification. For performance comparisons of
HDNNPs with other MLPs we refer to the References 42
and 64.
The energy RMSE is slightly larger than usual due
to the relatively broad energy range to be trained (Ta-
ble 2). Moreover, the mean energy range for the
individual S N2 reaction systems is also broad with
747 meV atom−1for those with central methyl carbon
atoms and 438 meV atom−1for those with central tert-
butyl carbon atoms. The atomic force component RMSE
is somewhat lower than usual despite the broad range be-
cause a significant fraction of forces is close to zero.
4.3. Continual Resilient (CoRe) Optimizer
Accurate MLPs can only be obtained if the optimizer
can efficiently and reliably find apposite weight values in
the high-dimensional parameter space. Training with a
fixed learning rate as in SGD yields HDNNPs of poor
accuracy. Most RMSE values of SGD results for refer-
ence data set B listed in Table 4 are almost an order of
magnitude larger than those of CoRe results highlighting
the importance of the optimizer. We found RPROP and
the Adam optimizer to be the best performing optimizers
available in PyTorch 1.12.1 for the given machine learn-
ing model and reference data. We note that RPROP is
intended for batch learning on all data at once, while we
perform stochastic optimization with RPROP. RPROP
converges fast and smooth, but plateaus at not satisfying
accuracy (Figures 2 (a) and (b)). By contrast, the Adam
optimizer requires more steps to reach the accuracy of
RPROP, but it is able to reach lower RMSE values in
the end. However, the convergence is noisy and therefore
hampers continual applications.
Our CoRe optimizer converges even faster than
RPROP and reaches a better final accuracy than the
Adamoptimizer(Figures2(a)and(b)). Theconvergence
is still almost as smooth as that of RPROP. Hence, CoRe
combines and improves the benefits of both, RPROP and
Adam. We note that these trends also hold for a random
training data selection (see Figures S7 (a) and (b) in the
Supporting Information) instead of the lifelong adaptive
data selection underlying the results in Figures 2 (a) and
13
Table 4:RMSE values of individual HDNNPs and the ensemble trained on reference data set B using the
optimizers SGD, RPROP, Adam, and CoRe. The optimizers and lifelong adaptive data selection were applied for
1500 (RPROP) and 2000 (SGD, Adam, CoRe) epochs.
Individual HDNNPs SGD RPROP Adam CoRe
RMSE(Etrain)/meV atom−137±7 9.5±0.7 6.1±1.5 3.9±0.4
RMSE(Etest)/meV atom−137±7 10.0±0.7 6.4±1.5 4.5±0.6
RMSE(Ftrain
α,n)/meVÅ−1564±17 191±5 119±8 99±7
RMSE(Ftest
α,n)/meVÅ−1557±11 205±8 127±8 116±4
Ensemble
RMSE( E)/meV atom−133 6.8 3.9 2.6
RMSE( Fα,n)/meVÅ−1529 131 95 64
(a)
(b)
Figure 2:Convergence of the optimizers Adam,
RPROP, and CoRe for training reference data set B.
The test set RMSE values of(a)energiesEtestand(b)
atomic force componentsFtest
α,nare shown as a function
of the training epochn epoch. RMSE values of individual
HDNNPs are represented by dots, while their mean,
which is unequal to the ensemble RMSE value, is shown
by a solid line. Lifelong adaptive data selection was
applied in the optimizations.
training data selection (see Figures S5 (a) and (b) in theSupporting Information) instead of the lifelong adaptive
data selection underlying the results in Figures 2 (a) and
(b).
4.4. Lifelong Adaptive Data Selection
Continual data set reduction is essential for lifelong
machine learning to keep the amount of training data
for rehearsal on a manageable level during incremental
learning of new data. Figure 3 reveals how the training
data set is narrowed duringﬁtting. The training data
reduction sets in after about 600 epochs because for data
exclusion the adaptive selection factorSr
histneeds to be
decreased belowSmin
histor increased aboveSmax
histby a spec-
iﬁed number of consecutive applications of the decrease
or increase factors, respectively (Algorithm 2). The opti-
mization with RPROP plateaus already after about 600
epochs. Therefore, the data exclusion is too fast in the
subsequent epochs, since the loss contribution of most
conformations does not change much, biasing the impor-
tance evaluation by the adaptive selection factors. By
contrast, theﬂuctuations in the convergence behavior of
the optimizations with the Adam optimizer lead to a slow
reduction of the training data. The reason for this is that
the data importance measures also undergo theﬂuctua-
tions which hamper to overcome the exclusion thresholds.
The training data reduction of optimizations using CoRe
is in between RPROP and Adam yielding a more bal-
anced process. The number of excluded conformations
per epoch also reduces for advanced training stages in
later epochs making the training more stable (see also
Figure S3 in the Supporting Information). By contrast,
the optimizations with RPROP become unstable after
about 1500 epochs due to the too rapid and strong data
reduction.
In the training of reference data set B using the CoRe
optimizer, the lifelong adaptive data selection assigned on
average(5.5±0.3)·103training conformations to be re-
dundant after 2000 epochs. Therefore, the training data
was reduced to29%of the initial amount. Figures 2 (a)Figure 2: Convergence of the optimizers Adam,
RPROP, and CoRe for training reference data set B.
The test set RMSE values of (a)energies Etestand(b)
atomic force components Ftest
α,nare shown as a function
of the training epoch nepoch. RMSE values of individual
HDNNPs are represented by dots, while their mean,
which is unequal to the ensemble RMSE value, is shown
by a solid line. Lifelong adaptive data selection was
applied in the optimizations.
(b).
4.4. Lifelong Adaptive Data Selection
Continual data set reduction is essential for lifelong
machine learning to keep the amount of training data
for rehearsal on a manageable level during incremental
learning of new data. Figure 3 reveals how the training
data set is narrowed during fitting. The training data
reduction sets in after about 600 epochs because for data
exclusion the adaptive selection factor Sr
histneeds to be
decreased below Smin
histor increased above Smax
histby a spec-
ified number of consecutive applications of the decrease
or increase factors, respectively (Algorithm 2). The opti-
mization with RPROP plateaus already after about 600
epochs. Therefore, the data exclusion is too fast in the
subsequent epochs, since the loss contribution of most

--- PAGE 14 ---
14
Table 4: RMSE values of individual HDNNPs and the ensemble trained on reference data set B using the
optimizers SGD, RPROP, Adam, and CoRe. The optimizers and lifelong adaptive data selection were applied for
1500 (RPROP) and 2000 (SGD, Adam, CoRe) epochs.
Individual HDNNPs SGD RPROP Adam CoRe
RMSE( Etrain)/meV atom−137±7 9 .5±0.7 6.1±1.5 3.9±0.4
RMSE( Etest)/meV atom−137±7 10 .0±0.7 6.4±1.5 4.5±0.6
RMSE( Ftrain
α,n)/meVÅ−1564±17 191 ±5 119 ±8 99±7
RMSE( Ftest
α,n)/meVÅ−1557±11 205 ±8 127 ±8 116 ±4
Ensemble
RMSE( E)/meV atom−133 6 .8 3 .9 2 .6
RMSE( Fα,n)/meVÅ−1529 131 95 64
conformations does not change much, biasing the impor-
tance evaluation by the adaptive selection factors. By
contrast, the fluctuations in the convergence behavior of
theoptimizationswiththeAdamoptimizerleadtoaslow
reduction of the training data. The reason for this is that
the data importance measures also undergo the fluctua-
tionswhichhampertoovercometheexclusionthresholds.
The training data reduction of optimizations using CoRe
is in between RPROP and Adam yielding a more bal-
anced process. The number of excluded conformations
per epoch also reduces for advanced training stages in
later epochs making the training more stable (see also
Figure S5 in the Supporting Information). By contrast,
the optimizations with RPROP become unstable after
about 1500 epochs due to the too rapid and strong data
reduction.
14
Figure 3:Training data set reduction of the optimizers
Adam, RPROP, and CoRe for training reference data
set B. The number of considered training conformations
Ntrainis shown as a function of the training epoch
nepoch. The values ofNtrainof individual HDNNPs are
represented by dots, while their mean is shown by a
solid line. The black dashed line represents the number
of training conformations which was used forﬁtting in
each epoch.
and (b) show that this data reduction does not lead to a
decline of the accuracy, which would be expected if train-
ing is performed on an non-representative subset of the
training conformations. Despite the strong reduction of
the amount of data, the lifelong adaptive data selection
signiﬁcantly improves the training accuracy compared to
random data selection based on all training data (see
Table S4 and Figures S5 (a) and (b) in the Supporting
Information). This trend is observed for all optimizers.
For the CoRe optimizer lifelong adaptive data selection
yields an improvement of the ensemble RMSE values by
31%for the energies and48%for the atomic force com-
ponents compared to random data selection.
On average38±11conformations were excluded from
training because the model was not able to represent
these conformations with a high accuracy. In this way,
hindrance of the training process by these conformations
can be avoided. Since the same 24 conformations were ex-
cluded in more than half of the training processes, these
conformations are likely to be doubtful. Conformations,
which are excluded only by a few individual HDNNPs
of the ensemble, can still be predicted by the ensemble
average (see Figure S4 (a) and (b) in the Supporting
Information). In this way, the individual training pro-
cesses can be improved, while the generalization of the
ensemble prediction is still provided. Arising uncertainty
for certain conformations due to their exclusion in some
HDNNP trainings is covered by the uncertainty quan-
tiﬁcation. Therefore, this approach does not aﬀect the
reliability of the method.4.5. Lifelong Machine Learning Potentials
Three frequently occurring example cases are explored,
in which lifelong learning can be beneﬁcial in comparison
to iterative cycles of data set expansion and construct-
ing new MLPs trained on all data. These cases represent
training data completion of a sparsely sampled confor-
mation space, expansion of the represented conformation
space for the same chemical systems, and learning addi-
tional chemical systems. Lifelong learning can add an
arbitrary number of new data points in each training
epoch and does not have to be applied in a block-wise
scheme as used in conventional active learning for MLPs.
In this work, however, we added new data only in a single
training epoch for a clear characterization of the resulting
eﬀects.
A sparsely sampled conformation space was obtained
for the initial training epochs when a high random frac-
tion of the training data wasﬁrst available at a late
epoch. Figures 4 (a) and (b) show that the proposed
lifelong learning strategies can handle this case very well
yielding an almost constantﬁnal lMLP accuracy with
respect to the late data fractionp late. The number of
epochs, after which the late data fraction was added,
was lower for highp latebecause otherwise the small frac-
tion of initial data would be overﬁtted. Figures S6 (b)
and S7 in the Supporting Information show that the
additional data were added at those epochs where the
accuracy of the test atomic force components plateaus
or even increases. The mean ensemble accuracy for
plate∈[0.5,0.8]isRMSE( E) = 2.6 meV atom−1and
RMSE( Fα,n) = 68 meVÅ−1and hence very similar to
that forp late= 0(Table 4). In addition to theﬂexibil-
ity gained in the training process, the lifelong learning
approach requires less training data to be handled in the
initial epochs compared to training on all data. Figure S7
in the Supporting Information reveals that the number
of excluded conformations by the lifelong adaptive data
selection is similar after 1500 epochs for diﬀerent values
ofp late.
To examine the performance for the case of the expan-
sion of the represented conformation space for the same
chemical systems, an lMLP wasﬁrst trained on reference
data set B for 1250 epochs. Subsequently, the additional
structurally distorted conformations of reference data set
C were added and training was continued for another
1250 epochs. Table 5 reveals that lifelong learning yields
RMSE values for individual HDNNPs which are about
13%higher than those of batch learning on all training
data of reference data set C (see Figures S8 (a) and (b)
and S9 in the Supporting Information for the training
process). However, most of this lost accuracy is regained
by the ensemble model which eﬃciently reduces the in-
creased model variance (see Table 5 and Figures S10 (a)
and (b) in the Supporting Information for the prediction
error distribution). Hence, the lMLP concept is able to
extend the represented conformation space, while it re-
tains the accuracy.
Figure 3: Training data set reduction of the optimizers
Adam, RPROP, and CoRe for training reference data
set B. The number of considered training conformations
Ntrainis shown as a function of the training epoch
nepoch. The values of Ntrainof individual HDNNPs are
represented by dots, while their mean is shown by a
solid line. The black dashed line represents the number
of training conformations which was used for fitting in
each epoch.
In the training of reference data set B using the CoReoptimizer,thelifelongadaptivedataselectionassignedon
average (5.5±0.3)·103training conformations to be re-
dundant after 2000 epochs. Therefore, the training data
was reduced to 29%of the initial amount. Figures 2 (a)
and (b) show that this data reduction does not lead to
a decline of the accuracy, which would be expected if
training is performed on a non-representative subset of
the training conformations. Despite the strong reduction
oftheamountofdata, thelifelongadaptivedataselection
significantly improves the training accuracy compared to
random data selection based on all training data (see
Table S5 and Figures S7 (a) and (b) in the Supporting
Information). This trend is observed for all optimizers.
For the CoRe optimizer lifelong adaptive data selection
yields an improvement of the ensemble RMSE values by
31%for the energies and 48%for the atomic force com-
ponents compared to random data selection.
On average 38±11conformations were excluded from
training because the model was not able to represent
these conformations with a high accuracy. In this way,
hindrance of the training process by these conformations
canbeavoided. Sincethesame24conformationswereex-
cluded in more than half of the training processes, these
conformations are likely to be doubtful. Conformations,
which are excluded only by a few individual HDNNPs
of the ensemble, can still be predicted by the ensemble
average (see Figures S6 (a) and (b) in the Supporting
Information). In this way, the individual training pro-
cesses can be improved, while the generalization of the
ensemble prediction is still provided. Arising uncertainty
for certain conformations due to their exclusion in some
HDNNP trainings is covered by the uncertainty quan-
tification. Therefore, this approach does not affect the
reliability of the method.
4.5. Lifelong Machine Learning Potentials
Threefrequentlyoccurringexamplecasesareexplored,
in which lifelong learning can be beneficial in comparison
to iterative cycles of data set expansion and construct-

--- PAGE 15 ---
15
ing new MLPs trained on all data. These cases represent
training data completion of a sparsely sampled confor-
mation space, expansion of the represented conformation
space for the same chemical systems, and learning addi-
tional chemical systems. Lifelong learning can add an
arbitrary number of new data points in each training
epoch and does not have to be applied in a block-wise
scheme as used in conventional active learning for MLPs.
In this work, however, we added new data only in a single
trainingepochforaclearcharacterizationoftheresulting
effects.
15
(a)
(b)
Figure 4:Final accuracy of lMLPs for which a fraction
ofp latetraining data of reference data set B wasﬁrst
available at a late training epoch. These data were
either chosen randomly or a certain block was used.
More detailed information about the procedure is
provided in the main text. The test RMSE values of
(a)energiesEtestand(b)atomic force components
Ftest
α,nare shown as a function of the late data fraction
plate. RMSE values of individual HDNNPs are
represented by dots, their mean by a solid line, and
their range by a lighter colored band. The CoRe
optimizer (βb
1= 0.7for “Block” withp late>0and
βb
1= 0.725otherwise) and lifelong adaptive data
selection were applied for 1500 epochs.
The higher RMSE values obtained in the training of
reference data set C compared to B result from the even
larger energy range and broader atomic force component
distribution (see Table 2 and Table S3 in the Supporting
Information), while the model architecture and training
hyperparameters remained unchanged. Still, especially
the ensemble atomic force component RMSE is similar to
other state-of-the-art HDNNPs trained for less elements
[18].
To investigate the eﬃciency for learning additional
chemical systems, the system-sorted reference data set
B is split into two blocks, whereby the fraction of the
second block isp late. The conformations are alphabet-Table 5:RMSE values of individual HDNNPs and the
ensemble trained on reference data set C using batch
learning on all training data at once and lifelong
learning. In lifelong learning, only the conformations of
reference data set B were trained for the initial 1250
epochs and then the additional conformations of
reference data set C were added. The CoRe optimizer
withβb
1= 0.7and lifelong adaptive data selection were
applied for 2500 epochs.
Individual HDNNPs Batch Lifelong
learning learning
RMSE(Etrain)/meV atom−16.1±0.2 6.9±0.6
RMSE(Etest)/meV atom−16.8±0.2 7.8±0.5
RMSE(Ftrain
α,n)/meVÅ−1168±3 185±12
RMSE(Ftest
α,n)/meVÅ−1182±4 205±10
Ensemble
RMSE( E)/meV atom−14.3 4.5
RMSE( Fα,n)/meVÅ−1121 122
ically sorted in the order central carbon atom, leaving
group, and nucleophile. Learning additional S N2 reac-
tions for centraltert-butyl carbon atoms, i.e., adding nu-
cleophiles and leaving groups at a late epoch which are
only known for central methyl carbon atoms, yields a
similar accuracy as learning on all data from the begin-
ning (p late<0.5in Figures 4 (a) and (b)). Forp late≥0.5
only reactions with central methyl carbon atoms are con-
tained in the initial training data leading to an increase
of theﬁnal test RMSE values. We emphasize that for
plate≥0.8also some elements are missing in the initial
training data. Still, the accuracy is better than that ob-
tained using RPROP and for the energies it is similar to
the Adam results (Table 4).
Similar to the aforementioned case, ensembling can
eﬃciently reduce the model variance introduced by in-
cremental learning and hence is an important tool
for lifelong machine learning. The ensemble accuracy
isRMSE( E) = 3.1 meV atom−1andRMSE( Fα,n) =
78 meVÅ−1forp late∈[0.5,0.8]and therefore about21%
larger than forp late= 0. However, in these cases of
learning additional chemical systems the lifelong learn-
ing strategies require further development to be on par
with the accuracy of training on all data.
As this work is a proof of concept for lMLPs, more and
diﬀerent chemical systems need to be explored in future
work toﬁne-tune and improve the lifelong learning strate-
gies. Still, we showed that lifelong learning can reach the
same accuracy as training on all data, while theﬂexibility
of the training process was signiﬁcantly increased.
Figure 4: Final accuracy of lMLPs for which a fraction
ofplatetraining data of reference data set B was first
available at a late training epoch. These data were
either chosen randomly or a certain block was used.
More detailed information about the procedure is
provided in the main text. The test RMSE values of
(a)energies Etestand(b)atomic force components
Ftest
α,nare shown as a function of the late data fraction
plate. RMSE values of individual HDNNPs are
represented by dots, their mean by a solid line, and
their range by a lighter colored band. The CoRe
optimizer ( βb
1= 0.7for “Block” with plate>0and
βb
1= 0.725otherwise) and lifelong adaptive data
selection were applied for 1500 epochs.
A sparsely sampled conformation space was obtained
for the initial training epochs when a high random frac-tion of the training data was first available at a late
epoch. Figures 4 (a) and (b) show that the proposed
lifelong learning strategies can handle this case very well
yielding an almost constant final lMLP accuracy with
respect to the late data fraction plate. The number of
epochs, after which the late data fraction was added,
was lower for high platebecause otherwise the small frac-
tion of initial data would be overfitted. Figures S8 (b)
and S9 in the Supporting Information show that the
additional data were added at those epochs where the
accuracy of the test atomic force components plateaus
or even increases. The mean ensemble accuracy for
plate∈[0.5,0.8]isRMSE( E) = 2 .6 meV atom−1and
RMSE( Fα,n) = 68 meV Å−1and hence very similar to
that for plate= 0(Table 4). In addition to the flexibil-
ity gained in the training process, the lifelong learning
approach requires less training data to be handled in the
initialepochscomparedtotrainingonalldata. FigureS9
in the Supporting Information reveals that the number
of excluded conformations by the lifelong adaptive data
selection is similar after 1500 epochs for different values
ofplate.
Table 5: RMSE values of individual HDNNPs and the
ensemble trained on reference data set C using learning
on a stationary batch of all training data and lifelong
learning. In lifelong learning, only the conformations of
reference data set B were trained for the initial 1250
epochs and then the additional conformations of
reference data set C were added. The CoRe optimizer
with βb
1= 0.7and lifelong adaptive data selection were
applied for 2500 epochs.
Individual HDNNPs Stationary Lifelong
data learning
RMSE( Etrain)/meV atom−16.1±0.2 6 .9±0.6
RMSE( Etest)/meV atom−16.8±0.2 7 .8±0.5
RMSE( Ftrain
α,n)/meVÅ−1168±3 185 ±12
RMSE( Ftest
α,n)/meVÅ−1182±4 205 ±10
Ensemble
RMSE( E)/meV atom−14.3 4 .5
RMSE( Fα,n)/meVÅ−1121 122
To examine the performance for the case of the expan-
sion of the represented conformation space for the same
chemical systems, an lMLP was first trained on reference
data set B for 1250 epochs. Subsequently, the additional
structurally distorted conformations of reference data set
C were added and training was continued for another
1250 epochs. Table 5 reveals that lifelong learning yields
RMSE values for individual HDNNPs which are about
13%higher than those of learning on a stationary batch
of all training data of reference data set C (see Figures
S10 (a) and (b) and S11 in the Supporting Information
forthetrainingprocess). However, mostofthislostaccu-

--- PAGE 16 ---
16
racy is regained by the ensemble model which efficiently
reduces the increased model variance (see Table 5 and
Figures S12 (a) and (b) in the Supporting Information
for the prediction error distribution). Hence, the lMLP
concept is able to extend the represented conformation
space, while it retains the accuracy.
The higher RMSE values obtained in the training of
reference data set C compared to B result from the even
larger energy range and broader atomic force component
distribution (see Table 2 and Table S4 in the Supporting
Information), while the model architecture and training
hyperparameters remained unchanged. Still, especially
the ensemble atomic force component RMSE is similar to
other state-of-the-art HDNNPs trained for less elements
[19].
To investigate the efficiency for learning additional
chemical systems, the system-sorted reference data set
B is split into two blocks, whereby the fraction of the
second block is plate. The conformations are alphabet-
ically sorted in the order central carbon atom, leaving
group, and nucleophile. Learning additional S N2 reac-
tions for central tert-butyl carbon atoms, i.e., adding nu-
cleophiles and leaving groups at a late epoch which are
only known for central methyl carbon atoms, yields a
similar accuracy as learning on all data from the begin-
ning ( plate<0.5in Figures 4 (a) and (b)). For plate≥0.5
only reactions with central methyl carbon atoms are con-
tained in the initial training data leading to an increase
of the final test RMSE values. We emphasize that for
plate≥0.8also some elements are missing in the initial
training data. Still, the accuracy is better than that ob-
tained using RPROP and for the energies it is similar to
the Adam results (Table 4).
Similar to the aforementioned case, ensembling can
efficiently reduce the model variance introduced by in-
cremental learning and hence is an important tool
for lifelong machine learning. The ensemble accuracy
isRMSE( E) = 3 .1 meV atom−1and RMSE( Fα,n) =
78 meVÅ−1forplate∈[0.5,0.8]and therefore about 21%
larger than for plate= 0. However, in these cases of
learning additional chemical systems the lifelong learn-
ing strategies require further development to be on par
with the accuracy of training on all data.
As this work is a proof of concept for lMLPs, more and
different chemical systems need to be explored in future
worktofine-tuneandimprovethelifelonglearningstrate-
gies. Still, we showed that lifelong learning can reach the
sameaccuracyastrainingonalldata, whiletheflexibility
of the training process was significantly increased.
4.6. Ensemble Prediction and Uncertainty
Quantification
To validate the ensemble prediction and examine the
uncertainty quantification, the lMLP trained by the
CoRe optimizer on reference data set B was applied on
conformations obtained from constrained DFT optimiza-tions on a dense grid of rCl−CandrBr−Cdistances for
the S N2 reaction Br–+ CH3ClBrCH3+ Cl–. We
emphasize that reference data set B uses a sparser and
irregularsamplingofthisreactionandthevalidationcon-
formations are unlikely to be in the reference data set B.
Hence, the smoothness of the lMLP potential energy sur-
face can be validated. The validation set contained 1062
conformations with maximal atomic force components of
16 eVÅ−1.
16
4.6. Ensemble Prediction and Uncertainty
Quantiﬁcation
To validate the ensemble prediction and examine the
uncertainty quantiﬁcation, the lMLP trained by the
CoRe optimizer on reference data set B was applied on
conformations obtained from constrained DFT optimiza-
tions on a dense grid ofr Cl−C andr Br−C distances for
the S N2 reaction Br–+ CH 3Cl BrCH 3+ Cl–. We
emphasize that reference data set B uses a sparser and
irregular sampling of this reaction and the validation con-
formations are unlikely to be in the reference data set B.
Hence, the smoothness of the lMLP potential energy sur-
face can be validated. The validation set contained 1062
conformations with maximal atomic force components of
16 eVÅ−1.
Figure 5:Potential energy surface of the S N2 reaction
Br–+ CH 3Cl BrCH 3+ Cl–. The lMLP ensemble
prediction energy Eis referenced to the minimum DFT
reference energyEref
minof the given conformation space
spanned by DFT optimized structures with constrained
distancesr Cl−C andr Br−C. The color represents the
error of Ewith respect to the DFT reference energy
Eref. Black dots show the explicit evaluations of the
lMLP. The colors between black dots are interpolated.
Figure 5 reveals that most sections of the represented
potential energy surface are predicted within chemical ac-
curacy, i.e.,1 kcal mol−1= 4.184 kJ mol−1, with respect
to the DFT reference energies. For this six atom sys-
tem an error of less than7.2 meV atom−1is therefore
required. Larger errors are only observed for high energy
conformations, which we expected because the maximal
atomic force component can be1 eVÅ−1higher than that
of the training data. The small errors with smooth dis-
tributions in the trained conformation space prove the
smoothness of the lMLP.
The reliability of the uncertainty quantiﬁcation is ad-
dressed in Figure 6 (a) and (b). The uncertainty quantiﬁ-
cation is equal to or larger than the absolute error with
respect to the DFT reference energies for1.4%of the val-
(a)
(b)
Figure 6:Absolute values of the errors with respect to
the DFT reference and the uncertainty quantiﬁcation
for the ensemble prediction of(a)energies|∆ E|and
(b)atomic force components|∆ Fα,n|of the validation
data. The orderxsorts the validation conformations
according to their uncertainty quantiﬁcation.
idation data with uncertainties∆ E≤10 meV atom−1.
For the atomic force components the fraction is0.3%
for∆Fα,n≤250 meVÅ−1. Hence, for most conforma-
tions the magnitude of the error is reliably predicted. For
large errors the uncertainty quantiﬁcation is expected to
underestimate the errors (see Section 2 2.6) so that the
above mentioned fractions increase to34%for∆ E >
10 meV atom−1and7%for∆ Fα,n>250 meVÅ−1. Still,
a reliable identiﬁcation of large errors is provided.
5. CONCLUSIONS
This work introduces the concept of lMLPs which can
ﬁne-tune and extend their representation in a rolling fash-
ion. Hence, the lMLP concept unites MLP model eﬃ-
ciency and accuracy withﬂexibility. For an lMLP, a uni-
versal and computationally eﬃcient atomic structure rep-
resentation, an MLP model, uncertainty quantiﬁcation,
and lifelong learning strategies need to be combined.
Therefore, we introduced eeACSF vectors for the struc-
Figure 5: Potential energy surface of the S N2 reaction
Br–+ CH3ClBrCH3+ Cl–. The lMLP ensemble
prediction energy Eis referenced to the minimum DFT
reference energy Eref
minof the given conformation space
spanned by DFT optimized structures with constrained
distances rCl−CandrBr−C. The color represents the
error of Ewith respect to the DFT reference energy
Eref. Black dots show the explicit evaluations of the
lMLP. The colors between black dots are interpolated.
Figure 5 reveals that most sections of the represented
potentialenergysurfacearepredictedwithinchemicalac-
curacy, i.e., 1 kcal mol−1= 4.184 kJ mol−1, with respect
to the DFT reference energies. For this six atom sys-
tem an error of less than 7.2 meV atom−1is therefore
required. Larger errors are only observed for high energy
conformations, which we expected because the maximal
atomicforcecomponentcanbe 1 eVÅ−1higherthanthat
of the training data. The small errors with smooth dis-
tributions in the trained conformation space prove the
smoothness of the lMLP.
The reliability of the uncertainty quantification is ad-
dressedinFigure6(a)and(b). Theuncertaintyquantifi-
cation is equal to or larger than the absolute error with
respect to the DFT reference energies for 98.6%of the
validationdatawithuncertainties ∆E≤10 meV atom−1.
For the atomic force components the fraction is 99.7%
for∆Fα,n≤250 meV Å−1. Hence, for most conforma-
tions the magnitude of the error is reliably predicted. For
large errors the uncertainty quantification is expected to
underestimate the errors (see Section 2.6) so that the
above mentioned fractions decrease to 66%for∆E >

--- PAGE 17 ---
17
16
4.6. Ensemble Prediction and Uncertainty
Quantiﬁcation
To validate the ensemble prediction and examine the
uncertainty quantiﬁcation, the lMLP trained by the
CoRe optimizer on reference data set B was applied on
conformations obtained from constrained DFT optimiza-
tions on a dense grid ofr Cl−C andr Br−C distances for
the S N2 reaction Br–+ CH 3Cl BrCH 3+ Cl–. We
emphasize that reference data set B uses a sparser and
irregular sampling of this reaction and the validation con-
formations are unlikely to be in the reference data set B.
Hence, the smoothness of the lMLP potential energy sur-
face can be validated. The validation set contained 1062
conformations with maximal atomic force components of
16 eVÅ−1.
Figure 5:Potential energy surface of the S N2 reaction
Br–+ CH 3Cl BrCH 3+ Cl–. The lMLP ensemble
prediction energy Eis referenced to the minimum DFT
reference energyEref
minof the given conformation space
spanned by DFT optimized structures with constrained
distancesr Cl−C andr Br−C. The color represents the
error of Ewith respect to the DFT reference energy
Eref. Black dots show the explicit evaluations of the
lMLP. The colors between black dots are interpolated.
Figure 5 reveals that most sections of the represented
potential energy surface are predicted within chemical ac-
curacy, i.e.,1 kcal mol−1= 4.184 kJ mol−1, with respect
to the DFT reference energies. For this six atom sys-
tem an error of less than7.2 meV atom−1is therefore
required. Larger errors are only observed for high energy
conformations, which we expected because the maximal
atomic force component can be1 eVÅ−1higher than that
of the training data. The small errors with smooth dis-
tributions in the trained conformation space prove the
smoothness of the lMLP.
The reliability of the uncertainty quantiﬁcation is ad-
dressed in Figure 6 (a) and (b). The uncertainty quantiﬁ-
cation is equal to or larger than the absolute error with
respect to the DFT reference energies for1.4%of the val-
(a)
(b)
Figure 6:Absolute values of the errors with respect to
the DFT reference and the uncertainty quantiﬁcation
for the ensemble prediction of(a)energies|∆ E|and
(b)atomic force components|∆ Fα,n|of the validation
data. The orderxsorts the validation conformations
according to their uncertainty quantiﬁcation.
idation data with uncertainties∆ E≤10 meV atom−1.
For the atomic force components the fraction is0.3%
for∆Fα,n≤250 meVÅ−1. Hence, for most conforma-
tions the magnitude of the error is reliably predicted. For
large errors the uncertainty quantiﬁcation is expected to
underestimate the errors (see Section 2 2.6) so that the
above mentioned fractions increase to34%for∆ E >
10 meV atom−1and7%for∆ Fα,n>250 meVÅ−1. Still,
a reliable identiﬁcation of large errors is provided.
5. CONCLUSIONS
This work introduces the concept of lMLPs which can
ﬁne-tune and extend their representation in a rolling fash-
ion. Hence, the lMLP concept unites MLP model eﬃ-
ciency and accuracy withﬂexibility. For an lMLP, a uni-
versal and computationally eﬃcient atomic structure rep-
resentation, an MLP model, uncertainty quantiﬁcation,
and lifelong learning strategies need to be combined.
Therefore, we introduced eeACSF vectors for the struc-
Figure 6: Absolute values of the errors with respect to
the DFT reference and the uncertainty quantification
for the ensemble prediction of (a)energies |∆E|and
(b)atomic force components |∆Fα,n|of the validation
data. The order xsorts the validation conformations
according to their uncertainty quantification.
10 meV atom−1and 93%for∆Fα,n>250 meV Å−1.
Still, a reliable identification of large errors is provided.
5. CONCLUSIONS
This work introduces the concept of lMLPs which can
fine-tuneandextendtheirrepresentationinarollingfash-
ion. Hence, the lMLP concept unites MLP model effi-
ciency and accuracy with flexibility. For an lMLP, a uni-
versalandcomputationallyefficientatomicstructurerep-
resentation, an MLP model, uncertainty quantification,
and lifelong learning strategies need to be combined.
Therefore,weintroducedeeACSFvectorsforthestruc-
tural representation, which are size-independent with re-
spect to the number of chemical elements in contrast to
many other common MLP descriptors. Their represen-tation performance of an S N2 reference data set is sim-
ilar to that of ACSF vectors at the break-even point of
computational cost, which is at about four different el-
ements. For several more elements eeACSF vectors be-
come the only computationally reasonable option due to
the combinatorial growth of ACSF vectors. An ensemble
HDNNP model using eeACSFs can predict an S N2 ref-
erence data set with ten different elements with a state-
of-the-art accuracy of previous HDNNPs trained on less
elements. Further, an ensemble of HDNNPs is a reliable
way to quantify the uncertainty due to model variance
and to identify conformations with high uncertainty in
predictions. Additionally, ensembling increases the ac-
curacy yielding potential energy surfaces with chemical
accuracy for the S N2 reactions.
As a basis of our lifelong learning strategies, we in-
troduced the CoRe optimizer which can combine and
improve the fast convergence of RPROP and the high
final accuracy of the Adam optimizer. In the training of
this work, the CoRe optimizer significantly improves the
HDNNP ensemble accuracy by about 33%for energies
and forces compared to the Adam optimizer. Applying
lifelong adaptive data selection further improves the ac-
curacy and enables to narrow the training data set and
exclude doubtful data during the training process. The
CoRe optimizer and lifelong adaptive data selection can
also improve training of machine learning models beyond
lMLPs.
Finally, an lMLP can adapt to additional data which
can be continuously added at any point in the training
process. In this way, improvements of lMLPs are possi-
ble without learning again on all previous data and still
a reliable method is obtained due to uncertainty quan-
tification. In learning cases which are obtained during
active learning or in the extension of the conformation
space for the same reaction systems, the training accu-
racy is similar to that of learning on a stationary batch
of all data. Even adding new reaction systems can be
performed by the presented algorithms with only mod-
erate accuracy loss. The benefit of lifelong learning is
the enhanced flexibility of the training process enabling
rolling explorations of chemical reactivity and training
continuation of previous lMLPs. Moreover, adaptability
of lMLPs is especially advantageous for large reference
data sets where training on all data at once is computa-
tionally very demanding. We emphasize that the lMLP
concept can also be applied for other MLP approaches
beyond HDNNPs in future work.
ACKNOWLEDGEMENT
This work was supported by an ETH Zurich Postdoc-
toral Fellowship.
[1] C. J. Cramer, Essentials of Computational Chemistry:
Theories and Models (Wiley, 2013).[2] F. Jensen, Introduction to Computational Chemistry
(Wiley, 2017).

--- PAGE 18 ---
18
[3] K. Burke, Perspective on density functional theory, J.
Chem. Phys. 136, 150901 (2012).
[4] C. Riplinger, P. Pinski, U. Becker, E. F. Valeev, and
F. Neese, Sparse maps—a systematic infrastructure for
reduced-scaling electronic structure methods. II. Linear
scaling domain based pair natural orbital coupled cluster
theory, J. Chem. Phys. 144, 024109 (2016).
[5] S. Das, P. Motamarri, V. Gavini, B. Turcksin, Y. W.
Li, and B. Leback, Fast, scalable and accurate finite-
element based ab initio calculations using mixed preci-
sion computing: 46 PFLOPS simulation of a metallic
dislocation system., in International Conference for High
Performance Computing, Networking, Storage and Anal-
ysis (SC19) (Denver, CO, USA, 2019).
[6] A. C. T. van Duin, S. Dasgupta, F. Lorant, and W. A.
Goddard, ReaxFF: A reactive force field for hydrocar-
bons, J. Phys. Chem. A 105, 9396 (2001).
[7] S. Piana, K. Lindorff-Larsen, and D. E. Shaw, How ro-
bust are protein folding simulations with respect to force
field parameterization?, Biophys. J. 100, L47 (2011).
[8] P. Friederich, F. Häse, J. Proppe, and A. Aspuru-Guzik,
Machine-learned potentials for next-generation matter
simulations, Nat. Mater. 20, 750 (2021).
[9] J. Behler and G. Csányi, Machine learning potentials for
extended systems - a perspective, Eur. Phys. J. B 94, 142
(2021).
[10] J. Behler, Perspective: Machine learning potentials
for atomistic simulations, J. Chem. Phys. 145, 170901
(2016).
[11] A. P. Bartók, S. De, C. Poelking, N. Bernstein, J. R.
Kermode, G. Csányi, and M. Ceriotti, Machine learning
unifiesthemodelingofmaterialsandmolecules,Sci.Adv.
3, e1701816 (2017).
[12] V. L. Deringer, M. A. Caro, and G. Csányi, Machine
learning interatomic potentials as emerging tools for ma-
terials science, Adv. Mater. 31, 1902765 (2019).
[13] F. Noé, A. Tkatchenko, K.-R. Müller, and C. Clementi,
Machine learning for molecular simulation, Ann. Rev.
Phys. Chem. 71, 361 (2020).
[14] J. Westermayr, M. Gastegger, K. T. Schütt, and R. J.
Maurer, Perspective on integrating machine learning into
computational chemistry and materials science, J. Chem.
Phys.154, 230903 (2021).
[15] S. Käser, L. I. Vazquez-Salazar, M. Meuwly, and
K. Töpfer, Neural network potentials for chemistry: con-
cepts, applications and prospects, Digital Discovery 2, 28
(2023).
[16] T. B. Blank, S. D. Brown, A. W. Calhoun, and D. J.
Doren, Neural network models of potential energy sur-
faces, J. Chem. Phys. 103, 4129 (1995).
[17] J. Behler and M. Parrinello, Generalized neural-network
representation of high-dimensional potential-energy sur-
faces, Phys. Rev. Lett. 98, 146401 (2007).
[18] J. Behler, First principles neural network potentials for
reactive simulations of large molecular and condensed
systems, Angew. Chem. Int. Ed. 56, 12828 (2017).
[19] J. Behler, Four generations of high-dimensional neural
network potentials, Chem. Rev. 121, 10037 (2021).
[20] K. T. Schüett, F. Arbabzadah, S. Chmiela, K. R. Müller,
and A. Tkatchenko, Quantum-chemical insights from
deep tensor neural networks, Nat. Commun. 8, 13890
(2017).
[21] O. T. Unke and M. Meuwly, PhysNet: A neural network
for predicting energies, forces, dipole moments, and par-tial charges, J. Chem. Theory Comput. 15, 3678 (2019).
[22] O. T. Unke, S. Chmiela, M. Gastegger, K. T. Schütt,
H. E. Sauceda, and K.-R. Müller, SpookyNet: Learning
force fields with electronic degrees of freedom and nonlo-
cal effects, Nat. Commun. 12, 7273 (2021).
[23] I. Batatia, D. P. Kovács, G. N. C. Simm, C. Ortner,
and G. Csányi, MACE: Higher order equivariant message
passing neural networks for fast and accurate force fields,
arXiv:2206.07697 [stat.ML] (2022).
[24] A. P. Bartók, M. C. Payne, R. Kondor, and G. Csányi,
Gaussian approximation potentials: The accuracy of
quantum mechanics, without the electrons, Phys. Rev.
Lett.104, 136403 (2010).
[25] A. V. Shapeev, Moment tensor potentials: a class of sys-
tematicallyimprovableinteratomicpotentials,Multiscale
Model. Simul. 14, 1153 (2016).
[26] N. Artrith, T. Morawietz, and J. Behler, High-
dimensional neural-network potentials for multicompo-
nent systems: Applications to zinc oxide, Phys. Rev. B
83, 153101 (2011).
[27] S. A. Ghasemi, A. Hofstetter, S. Saha, and S. Goedecker,
Interatomic potentials for ionic systems with density
functional accuracy based on charge densities obtained
by a neural network, Phys. Rev. B 92, 045131 (2015).
[28] T. W. Ko, J. A. Finkler, S. Goedecker, and J. Behler, A
fourth-generation high-dimensional neural network po-
tential with accurate electrostatics including non-local
charge transfer, Nat. Commun. 12, 398 (2021).
[29] N. Artrith and J. Behler, High-dimensional neural net-
work potentials for metal surfaces: A prototype study for
copper, Phys. Rev. B 85, 045439 (2012).
[30] J. Behler, Constructing high-dimensional neural network
potentials: A tutorial review, Int. J. Quantum Chem.
115, 1032 (2015).
[31] E. V. Podryabinkin and A. V. Shapeev, Active learning
of linearly parametrized interatomic potential, Comput.
Mater. Sci. 140, 171 (2017).
[32] N. Bernstein, G. Csányi, and V. L. Deringer, De novo
exploration and self-guided learning of potential-energy
surfaces, npj Comput. Mater. 5, 99 (2019).
[33] M. Eckhoff and J. Behler, High-dimensional neural
network potentials for magnetic systems using spin-
dependent atom-centered symmetry functions, npj Com-
put. Mater. 7, 170 (2021).
[34] T. Morawietz, A. Singraber, C. Dellago, and J. Behler,
How van der Waals interactions determine the unique
properties of water, Proc. Natl. Acad. Sci. 113, 8368
(2016).
[35] M. Hellström and J. Behler, Concentration-dependent
proton transfer mechanisms in aqueous NaOH solutions:
From acceptor-driven to donor-driven and back, J. Phys.
Chem. Lett. 7, 3302 (2016).
[36] B. Cheng, E. A. Engel, J. Behler, C. Dellago, and M. Ce-
riotti, Ab initio thermodynamics of liquid and solid wa-
ter, Proc. Natl. Acad. Sci. 116, 1110 (2019).
[37] J.Westermayr, M.Gastegger, M.F.S.J.Menger, S.Mai,
L. González, and P. Marquetand, Machine learning en-
ables long time scale molecular photodynamics simula-
tions, Chem. Sci. 10, 8100 (2019).
[38] S. Amabilino, L. A. Bratholm, S. J. Bennie, A. C.
Vaucher, M. Reiher, and D. R. Glowacki, Training neu-
ral nets to learn reactive potential energy surfaces using
interactive quantum chemistry in virtual reality, J. Phys.
Chem. A123, 4486 (2019).

--- PAGE 19 ---
19
[39] M. Eckhoff and J. Behler, From molecular fragments to
the bulk: Development of a neural network potential for
MOF-5, J. Chem. Theory Comput. 15, 3793 (2019).
[40] M.Eckhoff, F.Schönewald, M.Risch, C.A.Volkert, P.E.
Blöchl, and J. Behler, Closing the gap between theory
andexperimentforlithiummanganeseoxidespinelsusing
a high-dimensional neural network potential, Phys. Rev.
B102, 174102 (2020).
[41] M. Eckhoff and J. Behler, Insights into lithium man-
ganese oxide-water interfaces using machine learning po-
tentials, J. Chem. Phys. 155, 244703 (2021).
[42] Y. Zuo, C. Chen, X. Li, Z. Deng, Y. Chen, J. Behler,
G.Csányi, A.V.Shapeev, A.P.Thompson, M.A.Wood,
and S. P. Ong, Performance and cost assessment of ma-
chine learning interatomic potentials, J. Phys. Chem. A
124, 731 (2020).
[43] J.S.Smith, O.Isayev,andA.E.Roitberg,ANI-1: Anex-
tensible neural network potential with DFT accuracy at
force field computational cost, Chem. Sci. 8, 3192 (2017).
[44] S. Zhang, M. Z. Makoś, R. B. Jadrich, E. Kraka, K. M.
Barros, B. T. Nebgen, S. Tretiak, O. Isayev, N. Lubbers,
R. A. Messerly, and J. S. Smith, Exploring the frontiers
of chemistry with a general reactive machine learning po-
tential, 10.26434/chemrxiv-2022-15ct6-v2 (2022).
[45] D. Hassabis, D. Kumaran, C. Summerfield, and
M. Botvinick, Neuroscience-inspired artificial intelli-
gence, Neuron 95, 245 (2017).
[46] Z. Chen and B. Liu, Lifelong Machine Learning (Morgan
& Claypool Publishers, 2018).
[47] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and
S. Wermter, Continual lifelong learning with neural net-
works: A review, Neural Netw. 113, 54 (2019).
[48] D. Maltoni and V. Lomonaco, Continuous learning in
single-incremental-task scenarios, Neural Netw. 116, 56
(2019).
[49] S. Grossberg, Adaptive resonance theory: How a brain
learns to consciously attend, learn, and recognize a
changing world, Neural Netw. 37, 1 (2013).
[50] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville,
and Y. Bengio, An empirical investigation of catas-
trophic forgetting in gradient-based neural networks,
arXiv:1312.6211v3 [stat.ML] (2015).
[51] T. L. Hayes, N. D. Cahill, and C. Kanan, Memory effi-
cientexperiencereplayforstreaminglearning,in Interna-
tional Conference on Robotics and Automation (ICRA)
(Montreal, Canada, 2019) pp. 9769–9776.
[52] D. Lopez-Paz and M. A. Ranzato, Gradient episodic
memory for continual learning, in 31stConference on
Neural Information Processing Systems (NIPS) (Long
Beach, CA, USA, 2017) pp. 6470–6479.
[53] F. Zenke, B. Poole, and S. Ganguli, Continual learn-
ing through synaptic intelligence, in 34thInternational
Conference on Machine Learning (ICML) (Sydney, Aus-
tralia, 2017) pp. 3987–3995.
[54] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,
J.Kirkpatrick, K.Kavukcuoglu, R.Pascanu,andR.Had-
sell, Progressive neural networks, arXiv:1606.04671v4
[cs.LG] (2022).
[55] G. I. Parisi, J. Tani, C. Weber, and S. Wermter, Lifelong
learning of spatiotemporal representations with dual-
memory recurrent self-organization, Front. Neurorobot.
12, 78 (2018).
[56] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger,
I. Poltavsky, K. T. Schütt, A. Tkatchenko, and K.-R.Müller, Machine learning force fields, Chem. Rev. 121,
10142 (2021).
[57] H. J. Kulik, T. Hammerschmidt, J. Schmidt, S. Botti,
M. A. L. Marques, M. Boley, M. Scheffler, M. Todor-
ović, P. Rinke, C. Oses, A. Smolyanyuk, S. Curtarolo,
A. Tkatchenko, A. P. Bartók, S. Manzhos, M. Ihara,
T. Carrington, J. Behler, O. Isayev, M. Veit, A. Grisafi,
J. Nigam, M. Ceriotti, K. T. Schütt, J. Westermayr,
M. Gastegger, R. J. Maurer, B. Kalita, K. Burke, R. Na-
gai, R. Akashi, O. Sugino, J. Hermann, F. Noé, S. Pilati,
C. Draxl, M. Kuban, S. Rigamonti, M. Scheidgen, M. Es-
ters, D. Hicks, C. Toher, P. V. Balachandran, I. Tam-
blyn, S. Whitelam, C. Bellinger, and L. M. Ghiringhelli,
Roadmap on machine learning in electronic structure,
Electron. Struct. 4, 023004 (2022).
[58] J. Behler, Atom-centered symmetry functions for con-
structing high-dimensional neural network potentials, J.
Chem. Phys. 134, 074106 (2011).
[59] A. P. Bartók, R. Kondor, and G. Csányi, On representing
chemical environments, Phys. Rev. B 87, 184115 (2013).
[60] F. Musil, A. Grisafi, A. P. Bartók, C. Ortner, G. Csányi,
and M. Ceriotti, Physics-inspired structural representa-
tions for molecules and materials, Chem. Rev. 121, 9759
(2021).
[61] S. Gugler and M. Reiher, Quantum chemical roots
of machine-learning molecular similarity descriptors, J.
Chem. Theory Comput. 18, 6670 (2022).
[62] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and
G. Monfardini, The graph neural network model, IEEE
Trans. Neural Netw. 20, 61 (2009).
[63] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals,
and G. E. Dahl, Neural message passing for quantum
chemistry, in 34thInternational Conference on Machine
Learning (ICML) (Sydney, Australia, 2017) pp. 1263–
1272.
[64] C. Chen and S. P. Ong, A universal graph deep learning
interatomic potential for the periodic table, Nat. Com-
put. Sci.2, 718 (2022).
[65] N. Artrith, A. Urban, and G. Ceder, Efficient and accu-
rate machine-learning interpolation of atomic energies in
compositionswithmanyspecies,Phys.Rev.B 96,014112
(2017).
[66] S. Rostami, M. Amsler, and S. A. Ghasemi, Optimized
symmetry functions for machine-learning interatomic po-
tentials of multicomponent systems, J. Chem. Phys. 149,
124106 (2018).
[67] M. Gastegger, L. Schwiedrzik, M. Bittermann,
F. Berzsenyi, and P. Marquetand, wACSF–weighted
atom-centered symmetry functions as descriptors in ma-
chine learning potentials, J. Chem. Phys. 148, 241709
(2018).
[68] F. A. Faber, A. S. Christensen, B. Huang, and O. A. von
Lilienfeld, Alchemical and structural distribution based
representation for universal quantum machine learning,
J. Chem. Phys. 148, 241717 (2018).
[69] A. A. Peterson, R. Christensen, and A. Khorshidi, Ad-
dressing uncertainty in atomistic machine learning, Phys.
Chem. Chem. Phys. 19, 10978 (2017).
[70] J. S. Smith, B. Nebgen, N. Lubbers, O. Isayev, and A. E.
Roitberg, Less is more: Sampling chemical space with
active learning, J. Chem. Phys. 148, 241733 (2018).
[71] J. S. Smith, B. T. Nebgen, R. Zubatyuk, N. Lubbers,
C. Devereux, K. Barros, S. Tretiak, O. Isayev, and A. E.
Roitberg, Approaching coupled cluster accuracy with a

--- PAGE 20 ---
20
general-purpose neural network potential through trans-
fer learning, Nat. Commun. 10, 2903 (2019).
[72] C. Devereux, J. S. Smith, K. K. Huddleston, K. Barros,
R. Zubatyuk, O. Isayev, and A. E. Roitberg, Extending
the applicability of the ANI deep learning molecular po-
tential to sulfur and halogens, J. Chem. Theory Comput.
16, 4192 (2020).
[73] G. Imbalzano, Y. Zhuang, V. Kapil, K. Rossi, E. A. En-
gel, F. Grasselli, and M. Ceriotti, Uncertainty estimation
for molecular dynamics and sampling, J. Chem. Phys.
154, 074102 (2021).
[74] M.Reiher,Molecule-specificuncertaintyquantificationin
quantum chemical studies, Isr. J. Chem. 62, e202100101
(2022).
[75] M. Eckhoff, K. N. Lausch, P. E. Blöchl, and J. Behler,
Predicting oxidation and spin states by high-dimensional
neural networks: Applications to lithium manganese ox-
ide spinels, J. Chem. Phys. 153, 164107 (2020).
[76] M. Riedmiller and H. Braun, A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm, in International Conference on Neural Networks
(ICNN) (San Francisco, CA, USA, 1993) pp. 586–591.
[77] M. Riedmiller, Advanced supervised learning in multi-
layer perceptrons—from backpropagation to adaptive
learning algorithms, Comput. Stand. Interfaces 16, 265
(1994).
[78] D. P. Kingma and J. Ba, Adam: A method for stochas-
tic optimization, in 3rdInternational Conference on
Learning Representations (ICLR) (San Diego, CA, USA,
2015).
[79] H. Robbins and S. Monro, A stochastic approximation
method, Ann. Math. Stat. 22, 400 (1951).
[80] C. Igel and M. Hüsken, Improving the rprop learning
algorithm, in Second International ICSC Symposium on
Neural Computation (2000) pp. 115–121.
[81] E. Heid, C. J. McGill, F. H. Vermeire, and W. H. Green,
Characterizing uncertainty in machine learning for chem-
istry, 10.26434/chemrxiv-2023-00vcg-v2 (2023).
[82] F. Musil, M. J. Willatt, M. A. Langovoy, and M. Ceri-
otti, Fast and accurate uncertainty estimation in chemi-
cal machine learning, J. Chem. Theory Comput. 15, 906
(2019).
[83] M. L. Paleico and J. Behler, A bin and hash method
for analyzing reference data and descriptors in machinelearningpotentials,Mach.Learn.: Sci.Technol. 2,037001
(2021).
[84] J. Westermayr and P. Marquetand, Machine learning for
electronically excited states of molecules, Chem. Rev.
121, 9873 (2021).
[85] F. Neese, The ORCA program system, WIREs Comput.
Mol. Sci.2, 73 (2022).
[86] F. Neese, Software update: The ORCA program
system—version5.0,WIREsComput.Mol.Sci. 12,e1606
(2022).
[87] J. P. Perdew, K. Burke, and M. Ernzerhof, Generalized
gradient approximation made simple, Phys. Rev. Lett.
77, 3865 (1996).
[88] F. Weigend and R. Ahlrichs, Balanced basis sets of split
valence, triple zeta valence and quadruple zeta valence
quality for H to Rn: Design and assessment of accuracy,
Phys. Chem. Chem. Phys. 7, 3297 (2005).
[89] S. Grimme, J. Antony, S. Ehrlich, and H. Krieg, A con-
sistent and accurate ab initio parametrization of density
functional dispersion correction (DFT-D) for the 94 ele-
ments H-Pu, J. Chem. Phys. 132, 154104 (2010).
[90] E. R. Johnson and A. D. Becke, A post-Hartree-Fock
model of intermolecular interactions, J. Chem. Phys.
123, 024101 (2005).
[91] S. Grimme, S. Ehrlich, and L. Goerigk, Effect of the
damping function in dispersion corrected density func-
tional theory, J. Comput. Chem. 32, 1456 (2011).
[92] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gom-
mers, P. Virtanen, D. Cournapeau, E. Wieser, J. Tay-
lor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer,
M. H. van Kerkwijk, M. Brett, A. Haldane, J. Fer-
nández del Río, M. Wiebe, P. Peterson, P. Gérard-
Marchant, K.Sheppard, T.Reddy, W.Weckesser, H.Ab-
basi, C. Gohlke, and T. E. Oliphant, Array programming
with NumPy, Nature 585, 357 (2020).
[93] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Rai-
son, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala, PyTorch: An imperative style,
high-performance deep learning library, in 33rdInterna-
tional Conference on Neural Information Processing Sys-
tems (NIPS) (Vancouver, Canada, 2019) pp. 8026–8037.
