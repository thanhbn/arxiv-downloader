# Học Tập Tiến Bộ mà Không Quên

Tao Feng¹, Hangjie Yuan², Mang Wang³, Ziyuan Huang⁴, Ang Bian¹, Jianzhou Zhang¹
¹Đại học Tứ Xuyên ²Đại học Chiết Giang ³ByteDance Inc. ⁴Đại học Quốc gia Singapore
fengtao.hi@gmail.com, hj.yuan@zju.edu.cn, wangmang@bytedance.com
ziyuan.huang@u.nus.edu, bian@scu.edu.cn, zhangjz@scu.edu.cn

## Tóm tắt

Học từ các nhiệm vụ thay đổi và kinh nghiệm tuần tự mà không quên kiến thức đã thu được là một vấn đề thách thức đối với các mạng nơ-ron nhân tạo. Trong công trình này, chúng tôi tập trung vào hai vấn đề thách thức trong mô hình Học Liên tục (CL) mà không liên quan đến bất kỳ dữ liệu cũ nào: (i) sự tích lũy của hiện tượng quên thảm khốc do không gian kiến thức từ đó mô hình học kiến thức trước đó đang dần mờ nhạt; (ii) động lực kéo co không được kiểm soát để cân bằng tính ổn định và tính dẻo trong quá trình học các nhiệm vụ mới. Để giải quyết những vấn đề này, chúng tôi trình bày Học Tập Tiến Bộ mà Không Quên (PLwF) và một chế độ phân bổ tín dụng trong bộ tối ưu hóa. PLwF đưa vào dày đặc các hàm mô hình từ các nhiệm vụ trước để xây dựng một không gian kiến thức sao cho nó chứa kiến thức đáng tin cậy nhất về mỗi nhiệm vụ và thông tin phân phối của các nhiệm vụ khác nhau, trong khi phân bổ tín dụng kiểm soát động lực kéo co bằng cách loại bỏ xung đột gradient thông qua phép chiếu. Các thí nghiệm khử tách rộng rãi chứng minh hiệu quả của PLwF và phân bổ tín dụng. So với các phương pháp CL khác, chúng tôi báo cáo kết quả tốt hơn đáng kể ngay cả khi không dựa vào bất kỳ dữ liệu thô nào.

## 1. Giới thiệu

Học Liên tục (CL) vẫn là một thách thức lâu dài đối với Mạng Nơ-ron Nhân tạo (ANNs). Trong CL, mô hình dễ bị hiện tượng quên thảm khốc, nơi mà bộ học sâu chỉ hoạt động tốt trên các nhiệm vụ gần đây nhất trong khi không còn nhớ được kiến thức đã học trong các nhiệm vụ trước đó. Một chiến lược ngây thơ để tránh quên thảm khốc sẽ là huấn luyện một mô hình mới trên dữ liệu cho tất cả các nhiệm vụ hiện có. Tuy nhiên, điều này không thực tế do không thể truy cập dữ liệu. Do đó, một dòng công trình tập trung vào việc xây dựng không gian con đặc biệt của các nhiệm vụ cũ thay vì sử dụng dữ liệu để giảm thiểu hiện tượng quên.

Một giải pháp phổ biến khác là áp đặt điều chuẩn hóa lên bộ học sâu trong cuộc chiến chống lại hiện tượng quên. Trong mô hình này, tồn tại hai vấn đề chính. Đầu tiên, mặc dù việc truyền kiến thức của các nhiệm vụ trước đó cho mô hình trong quá trình học một nhiệm vụ mới thông qua điều chuẩn hóa có thể giảm lượng kiến thức bị mất, nhưng nó khó có thể bảo tồn toàn bộ kiến thức của các nhiệm vụ cũ. Do đó, trong CL trên một chuỗi dài các nhiệm vụ, mô hình trở nên ngày càng ít chắc chắn về kiến thức của mình đối với các nhiệm vụ sớm, tức là độ tin cậy của một mô hình nhất định như là container kiến thức của các nhiệm vụ sớm dần dần suy giảm. Điều này có nghĩa là lượng kiến thức bị quên đang tích lũy, mà chúng tôi gọi là sự tích lũy của hiện tượng quên thảm khốc. Thứ hai, khi học một nhiệm vụ mới, mô hình được yêu cầu cân bằng việc học nhiệm vụ sắp tới và cuộc chiến chống lại hiện tượng quên. Từ góc độ tối ưu hóa dựa trên gradient, điều này tạo ra một trò chơi kéo co, nơi các gradient để đạt được hai mục tiêu xung đột và cản trở việc học cả hai khía cạnh. Trong công trình này, chúng tôi tập trung vào việc giảm thiểu hai vấn đề này.

Theo một nghĩa nào đó, việc khắc phục hiện tượng quên bằng điều chuẩn hóa có thể được hiểu như quá trình khớp hàm giữa hàm mô hình hiện tại và (các) hàm mô hình trước đó chứa kiến thức về các nhiệm vụ cũ. Do đó, lý do cơ bản cho sự tích lũy của hiện tượng quên thảm khốc là độ tin cậy đang mờ nhạt của không gian kiến thức có thể học được do hàm mô hình trước đó xây dựng. Dựa trên điều này, chúng tôi đề xuất khai thác dày đặc các hàm mô hình được tạo ra trong các nhiệm vụ trước để xây dựng không gian kiến thức. Vì không gian này dựa trên một số lượng lớn kinh nghiệm trong quá khứ, quá trình học tập có tính tiến bộ hơn và do đó chúng tôi gọi phương pháp của mình là Học Tập Tiến Bộ mà Không Quên (PLwF). Điều này có hai ưu điểm: (i) không gian kiến thức bây giờ chứa kiến thức chính xác và tươi mới nhất về tất cả các nhiệm vụ trước đó, có nghĩa là độ tin cậy của không gian kiến thức đối với các nhiệm vụ cũ sẽ không mờ nhạt với ngày càng nhiều nhiệm vụ mới; (ii) phân phối đầy đủ trên các nhãn của các nhiệm vụ khác nhau bây giờ có thể được suy ra từ không gian kiến thức chứa tất cả các hàm mô hình trước đó, điều này được chứng minh là hiệu quả cho việc học các mô hình phân loại bằng chiến lược làm mượt nhãn.

Vì trò chơi kéo co phát sinh do các gradient xung đột của việc học nhiệm vụ mới và khắc phục hiện tượng quên các nhiệm vụ sớm, chúng tôi tìm đến việc phân bổ các tín dụng khác nhau cho các gradient được đóng góp bởi hai phần này sao cho gradient kết hợp có thể cập nhật mô hình mà không cản trở việc tối ưu hóa của một trong hai mục tiêu. Đối với phân bổ tín dụng, chúng tôi lấy cảm hứng từ các nghiên cứu và loại bỏ phần xung đột trong một trong các gradient bằng cách chiếu nó lên hướng trực giao của gradient khác. Vì chúng tôi đã đưa vào dày đặc tất cả các hàm mô hình trước đó trong không gian kiến thức, chúng tôi liệt kê tất cả các cặp xung đột có thể và lặp lại thao tác loại bỏ xung đột trong thuật toán phân bổ tín dụng của chúng tôi. Cách tiếp cận của chúng tôi cũng được lấy cảm hứng một phần từ các nghiên cứu sinh học cho rằng trong quá trình học kiến thức mới, não người giảm tốc độ dẻo synaptic để tránh những rối loạn do kiến thức mới gây ra, từ đó bảo tồn kiến thức đã học.

Đóng góp của bài báo này có thể được tóm tắt như sau. (i) Để khắc phục vấn đề độ tin cậy đang mờ nhạt của không gian kiến thức, chúng tôi đề xuất PLwF, nơi các hàm mô hình trước đó được đưa vào dày đặc vào không gian kiến thức. (ii) Chúng tôi thiết lập chế độ phân bổ tín dụng trong tối ưu hóa để tái tạo động lực kéo co trong CL, điều này cân bằng tốt hơn việc học và khắc phục hiện tượng quên bằng cách xác định mức độ ổn định-dẻo của các tham số riêng lẻ. (iii) Chúng tôi thực hiện các thí nghiệm rộng rãi cho thấy hiệu quả của PLwF và phân bổ tín dụng. Trong mô hình CL mà không liên quan đến bất kỳ dữ liệu cũ nào, phương pháp của chúng tôi đạt được cải thiện hiệu suất đáng kể trên CIFAR-10, CIFAR-100 và Tiny-ImageNet so với các phương pháp tiên tiến.

## 2. Công trình liên quan

**Học Liên tục dựa trên Điều chuẩn hóa.** Các phương pháp này cố gắng thực hiện củng cố kiến thức đã thu được trước đó bằng cách mở rộng các số hạng điều chuẩn hóa bổ sung. Ví dụ, cả EWC và EWC++ đều sử dụng đạo hàm bậc hai để đo độ nhạy cảm của mỗi tham số nhiệm vụ và phạt những thay đổi trong các tham số quan trọng cụ thể cho các nhiệm vụ trước đó. Tương tự, IMM ước tính posterior Gaussian cho các tham số nhiệm vụ. MAS định nghĩa lại thước đo tầm quan trọng tham số như các cài đặt không giám sát. SI tính toán tích phân đường dẫn trên quỹ đạo tối ưu hóa. Ngoài ra, R-walk, như một phiên bản tổng quát của các nghiên cứu trước, đưa ra bộ nhớ episode. Hơn nữa, LwF sử dụng chưng cất kiến thức để giữ lại kiến thức đã học từ các nhiệm vụ trước đó, điều này có nghĩa là các hàm xác suất được xuất ra bởi nhiệm vụ hiện tại bị ràng buộc. EBLL thúc đẩy và khuyến khích việc duy trì các biểu diễn đặc trưng quan trọng chiều thấp của các nhiệm vụ trước đó. Tuy nhiên, những suy nghĩ đơn giản về chưng cất không có nghĩa là không có mất mát thông tin. Khác với những phương pháp được mô tả ở trên, công trình này tái công thức hóa các nhiệm vụ học chuỗi dài như một vấn đề khớp tiến bộ cho các hàm để giảm thiểu hiện tượng quên.

**Học Liên tục dựa trên Gradient.** Danh mục này buộc gradient nhiệm vụ hiện tại phải luôn thẳng hàng với gradient từ các nhiệm vụ đã học trước đó để đạt được việc giảm thiểu quên. Ví dụ, cả GEM và A-GEM đều hạn chế hướng cập nhật của gradient nhiệm vụ hiện tại bằng cách tính toán gradient dựa trên các mẫu trước đó của bộ nhớ episode. GPM lưu trữ các cơ sở từ không gian con gradient của các mẫu cũ vào bộ nhớ dưới dạng bộ nhớ chiếu gradient và cập nhật gradient theo các hướng trực giao. Tương tự, OGD thực hiện các bước gradient theo hướng trực giao của gradient nhiệm vụ mới và nhiệm vụ quá khứ để giảm thiểu hiện tượng quên. FS-DGPM tiếp tục nâng cao độ sắc nét làm phẳng để cải thiện bộ nhớ chiếu gradient. RGO sử dụng bộ tối ưu hóa được cập nhật lặp đi lặp lại để sửa đổi gradient, từ đó cung cấp cho mô hình khả năng học liên tục. Liên quan đến những giải pháp gradient thành công này, một yếu tố cốt lõi là có thể xây dựng không gian con gradient của các nhiệm vụ trước đó để thỏa mãn cơ chế bộ nhớ. Không giống như các phương pháp trước đó yêu cầu chiếu gradient dựa trên tối ưu hóa cùng với dữ liệu thô, phân bổ tín dụng là một phương pháp heuristic để loại bỏ xung đột gradient mà không cần dữ liệu cũ.

**Các cách tiếp cận khác.** (i) Phương pháp dựa trên mở rộng. Trong danh mục này, DEN và HAT mở rộng động các thành phần bổ sung để giảm sự can thiệp của các nhiệm vụ mới và cũ. PNN không bị quên và có thể tận dụng kiến thức trước thông qua các kết nối bên với các đặc trưng đã học trước đó. RCL sử dụng học tăng cường để tìm cấu trúc tối ưu cho các nhiệm vụ tuần tự. Bằng cách mở rộng các tham số nhiệm vụ cụ thể, APD có thể giảm thiểu sự gia tăng độ phức tạp mạng. (ii) Phương pháp dựa trên tái diễn. Danh mục này cố gắng giảm thiểu hiện tượng quên thông qua việc sử dụng các tập con của các ví dụ nhiệm vụ trước đó như các tế bào bộ nhớ, chẳng hạn như iCaRL và GSS. Dựa trên các chiến lược lấy mẫu khác nhau, họ thiết lập ngân sách hạn chế trong bộ đệm bộ nhớ để tái diễn. Và các họ GEM đề xuất bộ nhớ episode như bộ đệm. Để thoát khỏi nhu cầu lưu trữ trực tiếp dữ liệu thô, gần đây một loạt công trình đã xây dựng một cách tinh vi không gian con đặc biệt của các nhiệm vụ cũ như bộ nhớ và đã cho thấy hiệu suất đáng chú ý.

## 3. Kiến thức cơ bản

**Học Liên tục.** Chúng tôi xem xét vấn đề CL bao gồm T nhiệm vụ, ký hiệu là t = 1,...,T, với N ví dụ được lấy mẫu cho mỗi nhiệm vụ. Chúng tôi giả định rằng instance huấn luyện x^t, được quan sát trong nhiệm vụ t, là một mẫu i.i.d từ phân phối P_x^t. Chìa khóa của CL là cách target y^t được chọn, trong đó y^t ∈ Y^t là không gian target. Chúng tôi giả định target cho nhiệm vụ thứ t là {y^1, y^2, ..., y^t}. Mục tiêu của chúng tôi là học một hàm dự đoán f(·; ·): R^D → R, sao cho f(·; ·) không chỉ học hướng tới ground-truth target của giai đoạn hiện tại y^t, mà còn target {y^1, y^2, ..., y^{t-1}} nhận được từ các nhiệm vụ sớm. Chính thức, tại nhiệm vụ thứ t, chúng tôi tìm cách giảm thiểu mục tiêu sau:

min ∑_{t=1}^T E_{x~P_x^t}[L(f(x; ·), y^t)]     (1)

**Learning without Forgetting.** Trong LwF, đối với mỗi nhiệm vụ mới, mục tiêu là học một hàm ánh xạ input x tới nhãn tương ứng y^t, tức là f^t(x^t; φ^t, θ^t) = y^t, trong đó φ^t và θ^t ký hiệu các tham số của bộ mã hóa đặc trưng và bộ dự đoán. Chính thức, mục tiêu tối ưu hóa được định nghĩa là:

min_{φ^t,θ^t} E_{x~P_x^t}[L_CE(f^t(x; φ^t, θ^t), y^t)]     (2)

trong đó L_CE ký hiệu mất mát Cross-Entropy.

Đối với nhiệm vụ cũ, LwF buộc xác suất đầu ra cho mỗi hình ảnh phải gần với đầu ra được ghi lại từ nhiệm vụ cuối cùng. KL Divergence được sử dụng để khuyến khích đầu ra của một mạng xấp xỉ đầu ra của mạng khác. Thủ tục này cũng được gọi là khớp hàm. Chính thức, việc tối ưu hóa được định nghĩa là:

min_{φ^t,θ^t} E_{x~P_x^t}[L_KL(f^t(x; φ^t, θ^t), f^{t-1}(x; φ^{t-1}, θ^{t-1}))]     (3)

Trong đó φ^{t-1} và θ^{t-1} ký hiệu tập tham số tối ưu trong nhiệm vụ cuối cùng và f^{t-1}(x; φ^{t-1}, θ^{t-1}) ∈ Y^1 × Y^2 × ... × Y^{t-1}. Như được thể hiện trong Hình 2a, với sự thẳng hàng của nó với hàm dự đoán của nhiệm vụ cuối cùng, LwF cung cấp một không gian kiến thức có tương quan chặt chẽ với nhiệm vụ hiện tại. Tuy nhiên, bị hạn chế bởi hiện tượng quên thảm khốc, không gian kiến thức cuối cùng đã phải chịu sự bay hơi kiến thức từ các nhiệm vụ trước đó. Điều này dẫn đến một bộ học sâu không nhớ tốt kiến thức đã học trong các nhiệm vụ trước đó.

## 4. Học Tập Tiến Bộ mà Không Quên

Trong giao thức của các phương pháp dựa trên điều chuẩn hóa, việc xác định cách tích lũy kinh nghiệm một cách tiến bộ trong các chuỗi nhiệm vụ dài không trực quan do thiếu dữ liệu thô. Để giải quyết vấn đề này, chúng tôi đề xuất PLwF.

Trong CL bao gồm T nhiệm vụ, để tích lũy kinh nghiệm một cách tự nhiên như được thể hiện trong Hình 2b, chúng tôi cần học một hàm f^t(·; φ^t, θ^t) được chia sẻ bởi phần lớn các tham số trên các nhiệm vụ khác nhau. Do đó, không gian hàm F^t bao phủ nhiều kiến thức hơn là cốt lõi. Không gian này mang theo các tín hiệu huấn luyện phong phú nhờ thông tin phân phối của các nhãn trên tất cả các hàm trước đó, được định nghĩa như sau.

**Định nghĩa 1.** Giả sử tồn tại một hàm dự đoán f(·) cho mỗi nhiệm vụ, thì chúng tôi định nghĩa một tập hàm F^t = {f^1(·; φ^1, θ^1), f^2(·; φ^2, θ^2), ..., f^{t-1}(·; φ^{t-1}, θ^{t-1})} trong đó mỗi hàm f(·) ∈ F^t có một bộ mã hóa φ và một bộ dự đoán θ cho một nhiệm vụ nhất định và được đóng băng trong nhiệm vụ thứ t.

Trực quan, khi t lớn hơn 2, tập hợp này hưởng lợi từ kiến thức tiên nghiệm phong phú hơn nhờ các hàm của các nhiệm vụ sớm. Chúng tôi mô tả tập hợp này như một không gian với ít kiến thức biến mất hơn cho các nhiệm vụ trước đó. Khi tối ưu hóa cho PLwF, đối với nhiệm vụ mới, chúng tôi theo LwF để học hướng tới nhãn y^t như được công thức hóa trong Phương trình 3. Đối với các nhiệm vụ cũ, chúng tôi công thức hóa mục tiêu tối ưu hóa tại nhiệm vụ thứ t như:

min_{φ^t,θ^t} ∑_{i=1}^{t-1} E_{x~P_x^t}[L_KL(f^t(x; φ^t, θ^t), f^i(x; φ^i, θ^i))]     (4)

trong đó hàm f^t(·; φ^t, θ^t) đẩy xác suất đầu ra của nó gần với đầu ra được ghi lại từ mọi hàm trong F^t. Tương tự, chúng tôi sử dụng KL Divergence để khuyến khích khớp các phân phối xác suất giữa tất cả các hàm. Thông qua PLwF, chúng tôi khuyến khích các hàm mã hóa một cách tiến bộ những điểm tương đồng giữa thông tin phân phối trên các nhãn trong chuỗi dài các nhiệm vụ. Việc chỉ học từ đầu ra của nhiệm vụ cuối cùng (Hình 2a) có thể dẫn đến mất kiến thức của các nhiệm vụ trước đó, trong khi PLwF (Hình 2b) giữ lại kiến thức của các nhiệm vụ trước đó, do đó giảm không gian kiến thức biến mất.

**Sự nới lỏng của PLwF.** Trong Định nghĩa 1, chúng tôi định nghĩa tập hàm chứa tất cả các hàm trước đó, như được minh họa trong Hình 2b. Tuy nhiên, điều này dường như là một giả định mạnh để xây dựng. Do đó, chúng tôi đề xuất một sự nới lỏng của PLwF để phù hợp với nhiều kịch bản hơn. Cụ thể hơn, một tập hàm nới lỏng có thể là F̃^t ⊆ F^t, bao gồm ít hàm hơn từ các giai đoạn trước. Khi áp dụng Phương trình 4, chúng tôi sẽ khuyến khích xác suất đầu ra của f^t(·; φ^t, θ^t) gần với đầu ra được ghi lại từ mọi hàm trong F̃^t. Lưu ý rằng dưới định nghĩa này, LwF xuất hiện như một trường hợp đặc biệt của PLwF nới lỏng.

## 5. Phân bổ Tín dụng trong PLwF

Khái niệm phân bổ tín dụng được đề xuất bởi các nghiên cứu trong CL và phản ánh cách các tham số khác nhau chịu trách nhiệm cho các hành vi mạng mong đợi. Phương pháp gradient chuẩn thực hiện một bước nhỏ theo hướng giảm để cập nhật mạng. Trong quá trình như vậy, gradient độc lập xác định liệu thay đổi trong mỗi tham số có giảm mất mát hay không. Tại thời điểm này, phân bổ tín dụng hoàn toàn phụ thuộc vào tính dẻo. Thay đổi cài đặt, chẳng hạn như mở rộng số hạng điều chuẩn hóa và sử dụng nó như một proxy gradient, có thể giúp tăng cường tính ổn định, nhưng phân bổ tín dụng ngây thơ có thể gây ra cập nhật gradient với động lực kéo co bị ép buộc. Do đó, việc tái tạo động lực kéo co được tinh chỉnh thông qua phân bổ tín dụng thích hợp là có giá trị.

Vì chúng tôi đã đưa vào dày đặc tất cả các hàm mô hình trước đó thông qua PLwF, chế độ phân bổ tín dụng cần xử lý động lực kéo co phức tạp hơn. Giả sử bản chất của vấn đề tối ưu hóa trong PLwF xuất phát từ kéo co của các gradient. Trong bộ tối ưu hóa SGD, các tham số được cập nhật như sau:

θ_i := θ_i - η ∑_t ∇L_t(θ)     (5)

Tuy nhiên, một giải pháp như vậy không thể phát triển được các phân bổ tín dụng đáng tin cậy trong quá trình học. Để đạt được điều này, chúng tôi tái tạo một chế độ bằng cách sửa đổi gradient trong PLwF, từ đó giảm thiểu hiện tượng quên. Chúng tôi cho phép các tương tác tích cực giữa các gradient của các nhiệm vụ khác nhau để khắc phục dylà độ ổn định-dẻo. Lấy cảm hứng từ các nghiên cứu, chúng tôi định nghĩa các điều kiện sau để khám phá phân bổ tín dụng trong PLwF.

**Định nghĩa 2.** Thước đo phân bổ tín dụng được cho bởi độ tương tự cosine ⟨g_a, g_b⟩ = ⟨g_a, g_b⟩/(||g_a|| ||g_b||), trong đó g_a và g_b là gradient từ hai nhiệm vụ tùy ý.

**Định nghĩa 3.** Nếu thước đo phân bổ tín dụng giữa hai nhiệm vụ ⟨g_a, g_b⟩ < 0, thì động lực kéo co tồn tại, ngược lại thì không.

Để xác định tốt hơn mức độ ổn định-dẻo của mỗi tham số và tránh những thay đổi tiêu cực của gradient, chúng tôi định nghĩa một ma trận phân bổ gradient M trong mỗi lần lặp.

**Định nghĩa 4.** Giả sử tồn tại một tập gradient G = {g_1, g_2, ..., g_t, ∀t ∈ T}. Đối với hai phần tử tùy ý g_a, g_b trong tập hợp, có một thước đo ⟨g_a, g_b⟩ với điều kiện đối xứng được thỏa mãn. Thì tồn tại một ma trận M kích thước T×T như là ma trận phân bổ gradient, phản ánh phân bổ tín dụng của hai phần tử tùy ý.

Tập G xuất phát từ quá trình tối ưu hóa của PLwF. Nếu xem xét tính đối xứng, ⟨g_a, g_b⟩ = ⟨g_b, g_a⟩. Do đó, chúng tôi tập trung vào các mục trong tam giác trên hoặc dưới của M và các mục trên đường chéo của M. Trong suốt toàn bộ quá trình, Định nghĩa 4 điều tra các điều kiện phân bổ của mỗi gradient trên tất cả các nhiệm vụ. Đặc biệt, ma trận M có thể nắm bắt động lực kéo co trong mỗi lần lặp.

Để tạo ra chế độ phân bổ tín dụng đáng tin cậy trong quá trình tối ưu hóa, các bước sau được thực hiện: (i) Ánh xạ thước đo phân bổ tín dụng ⟨g_a, g_b⟩ trong tập G và tính toán ma trận phân bổ tín dụng M. (ii) Trích xuất các cặp gradient nằm phía trên (hoặc dưới) đường chéo chính với ⟨g_a, g_b⟩ < 0 (Màu hồng trong Bảng 1) trong ma trận M như một tập con H. (iii) Đối với mỗi cặp (g_a, g_b) của các gradient trong H, thay thế g_a bằng phép chiếu của nó lên mặt phẳng pháp tuyến của g_b: g_a = g_a - (g_a^T g_b)/(||g_b||^2) g_b.

Thủ tục trên được thực thi cho mọi lần lặp trong quá trình tối ưu hóa. Điều này giảm mức độ nhiễu loạn của gradient được áp dụng trong mỗi batch của mỗi nhiệm vụ đối với các nhiệm vụ khác trong batch, từ đó giảm động lực kéo co. Sau đó, tham số được cập nhật như:

θ_i := θ_i - η ∑_t (∇L_t(θ))_credit     (6)

Trong đó số hạng credit chỉ ra toán tử chế độ cho gradient. Thông qua việc đặt lại đơn giản, giải pháp của chúng tôi thay thế các gradient gốc bằng những gradient đã được cập nhật và chuyển chúng đến bộ tối ưu hóa tương ứng. Kết quả thí nghiệm xác nhận giả thuyết về việc tái tạo chế độ phân bổ tín dụng cẩn thận và sự cải thiện trong khả năng CL một cách trực quan chứng minh việc giảm thiểu dylà ổn định-dẻo.

## 6. Thiết lập Thí nghiệm

**Tập dữ liệu.** Theo nghiên cứu điển hình về học tăng dần lớp, chúng tôi đánh giá hiệu suất của PLwF trên CIFAR-10, CIFAR-100 và Tiny-ImageNet. Cụ thể, chúng tôi chia CIFAR-10 thành 5 nhiệm vụ với 2 lớp mỗi nhiệm vụ, chia CIFAR-100 thành 10/20 nhiệm vụ với 10/5 lớp mỗi nhiệm vụ và chia Tiny-ImageNet thành 10 nhiệm vụ với 20 lớp mỗi nhiệm vụ. Đặc biệt, chúng tôi xây dựng các chuỗi nhiệm vụ ngắn 2-Split CIFAR-10, 2-Split CIFAR-100, và 2-Split Tiny-ImageNet để phân tích thực nghiệm về phân bổ tín dụng, trong đó chúng chia 10, 100, và 200 lớp của CIFAR-10 và CIFAR-100, và Tiny-ImageNet thành 2 nhiệm vụ với 5, 50, và 100 lớp mỗi nhiệm vụ. Để so sánh công bằng với các phương pháp khác nhau, chúng tôi sử dụng tất cả các lớp theo cùng thứ tự để thực hiện thí nghiệm.

**Phương pháp để So sánh.** PLwF tuân thủ nghiêm ngặt phương pháp điều chuẩn hóa, chúng tôi không lưu trữ bất kỳ mẫu cũ nào từ dữ liệu thô khi học các lớp mới. Do đó, chúng tôi đầu tiên so sánh phương pháp của mình với một số phương pháp dựa trên điều chuẩn hóa: EWC, EWC++, MAS, và SI. Ngoài ra, chúng tôi cũng so sánh một số phương pháp dựa trên tái diễn: ER, GEM, A-GEM, FDR, GSS, HAL, và PODNET. Kích thước bộ đệm cho tất cả các phương pháp dựa trên tái diễn được đặt thành 500 theo nghiên cứu trước. Đối với các phương pháp được so sánh, chúng tôi tuân theo các triển khai mã nguồn mở với các siêu tham số tốt nhất đi kèm để thực hiện CL.

**Kiến trúc và Chi tiết Huấn luyện.** Tương tự như các nghiên cứu trước, chúng tôi sử dụng ResNet-18 trong các thí nghiệm CIFAR-10. Trong các thí nghiệm CIFAR-100 và Tiny-ImageNet, chúng tôi sử dụng ResNet-32 tương tự như nghiên cứu khác. Để đảm bảo so sánh công bằng, tất cả các mô hình được huấn luyện với bộ tối ưu hóa vanilla-SGD. Đối với các thí nghiệm trên CIFAR-10 và CIFAR-100, mạng được huấn luyện với kích thước batch 128 trong khi kích thước batch được đặt thành 32 trong Tiny-ImageNet. Đối với tất cả các tập dữ liệu, chúng tôi huấn luyện nhiệm vụ ban đầu với 200 epoch và các nhiệm vụ còn lại với 250 epoch. Tất cả các thí nghiệm được thực hiện trên 1 GPU NVIDIA TITAN. Cuối cùng, chúng tôi báo cáo độ chính xác trung bình trên tất cả các nhiệm vụ và độ chính xác nhiệm vụ cuối, cái sau chỉ ra mức độ quên. Đối với tất cả các thí nghiệm, chúng tôi đánh giá và so sánh phương pháp của mình trong bố cục đầu đơn nơi tất cả các nhiệm vụ chia sẻ lớp phân loại cuối cùng và suy luận được thực hiện mà không cần nhận dạng nhiệm vụ.

## 7. Kết quả và Thảo luận

**Kiểm soát Hiện tượng Quên.** Khi việc học tiếp tục, hiệu suất của nhiệm vụ ban đầu có xu hướng trải qua hiện tượng quên triệt để nhất. Để minh họa cách PLwF cải thiện hiện tượng quên, chúng tôi cho thấy những thay đổi trong hiệu suất trên nhiệm vụ đầu tiên khi việc học tiếp tục trên 5-Split CIFAR-10, 10-Split CIFAR-100 và 20-Split CIFAR-100 trong Hình 3. Cùng một xu hướng được quan sát trong tất cả các tập dữ liệu: (i) Plain-SGD chứng minh hiện tượng quên thảm khốc hoàn toàn do giao thức học chuẩn, và hiệu suất của nó trên nhiệm vụ đầu tiên sụp đổ về không khi học nhiệm vụ thứ hai; (ii) Cụ thể, cài đặt single-shot đại diện cho một phương pháp học ngây thơ chỉ sử dụng hàm tại nhiệm vụ cuối cùng như một không gian học cơ bản. Cài đặt này hưởng lợi từ kiến thức được chuyển từ nhiệm vụ cuối cùng, bảo tồn một số hiệu suất; (iii) So sánh, PLwF cho thấy độ dốc nhẹ nhàng hơn đáng kể của đường cong quên, điều này cho thấy sự bất khuất đối với các nhiệm vụ trước đó; (iv) Sau khi phân bổ tín dụng điều hòa động lực kéo co, hiện tượng quên được kiểm soát thêm, điều này được phản ánh trong độ dốc nhẹ nhàng hơn của đường cong quên so với PLwF. Tóm lại, chúng tôi kiểm soát hiện tượng quên bằng cách sử dụng các hàm trước khi chúng trở nên không đáng tin cậy, và bằng cách loại bỏ xung đột gradient.

**Kết quả Chính.** Bảng 2 báo cáo kết quả trên tất cả các tập dữ liệu, điều này làm sáng tỏ các quan sát sau: (i) So sánh công bằng với các phương pháp dựa trên điều chuẩn hóa: Trong thiết lập này, bất kỳ dữ liệu thô nào từ các giai đoạn quá khứ đều bị cấm, phương pháp đề xuất đạt được hiệu suất tiên tiến trên tất cả các tập dữ liệu. Khi so sánh với EWC, EWC++, MAS, và SI, khoảng cách xuất hiện không thể vượt qua. Từ quan điểm của chúng tôi, danh mục phương pháp này có nguồn gốc từ các tham số quan trọng từ các nhiệm vụ trước đó, mà độ tin cậy của chúng suy giảm tại các giai đoạn học tiếp theo, từ đó dẫn đến hiệu suất hạn chế. (ii) So sánh với các phương pháp dựa trên tái diễn: Phương pháp đề xuất trình bày hiệu suất nổi bật đối với tất cả các phương pháp trước đó, mặc dù không có dữ liệu thô nào từ bất kỳ nhiệm vụ nào trong quá khứ được lưu trữ. GEM và A-GEM cũng sử dụng gradient tương tự, nhưng bộ nhớ episode của chúng trình bày hiệu quả kém thỏa đáng hơn khi đánh giá thông tin phân phối của các lớp. (iii) Đặc biệt, GSS hơi tốt hơn phương pháp của chúng tôi trên CIFAR-10 về Avg, điều này được quy cho chiến lược bộ đệm hiệu quả của nó. Tuy nhiên, phương pháp của chúng tôi dẫn trước 6.08% trên nhiệm vụ Last. Hơn nữa, trên Split CIFAR-100 và Split Tiny-ImageNet với độ khó lớn hơn, phương pháp của chúng tôi vượt trội đáng kể so với GSS. GSS trình bày hiệu suất giảm khi đánh giá trên các nhiệm vụ thách thức. Tóm lại, hiệu suất của phương pháp đề xuất vượt qua phương pháp tiên tiến nhất dưới giao thức của các phương pháp dựa trên điều chuẩn hóa. Ngoài ra, nó cũng trình bày hiệu suất tốt hơn hoặc tương đương với các phương pháp dựa trên tái diễn ngay cả khi không dựa vào bất kỳ dữ liệu thô nào. Đáng chú ý, phương pháp của chúng tôi hoạt động ổn định hơn trên các tập dữ liệu phức tạp hơn do thông tin phân phối trên các nhãn từ tất cả các hàm trước đó.

**Nới lỏng PLwF.** Như được chỉ ra trong Phần 4, chúng tôi cho thấy một phiên bản nới lỏng hơn của PLwF bằng cách sử dụng một tập con của các hàm trước đó F̃^t. Để xác nhận tác động của PLwF nới lỏng, chúng tôi thiết lập các thí nghiệm sau: (i) Giả định mạnh (sử dụng tất cả các hàm trước đó trong Phương trình 4). (ii) Giả định yếu (sử dụng hàm thứ (t-1) và 30% hoặc 50% đầu tiên các hàm từ các giai đoạn trước đó trong Phương trình 4). Kết quả trong Hình 4 tiết lộ rằng khi sử dụng hàm thứ (t-1) và 50% đầu tiên các hàm, PLwF chỉ giảm hiệu suất 0.54% (Avg) và 0.69% (Avg) trên cài đặt 10 bước và 20 bước trong khi giảm chi phí tính toán khoảng 40% so với giả định mạnh; khi sử dụng hàm thứ (t-1) và 30% đầu tiên các hàm, PLwF chỉ giảm hiệu suất 1.73% (Avg) và 1.18% (Avg) trên cài đặt 10 bước và 20 bước trong khi giảm chi phí tính toán khoảng 60% so với giả định mạnh. Những ý tưởng tương tự được sử dụng để tăng hiệu quả lấy mẫu của các mô hình khuếch tán lên đến 256 lần. Sự đánh đổi hiệu suất-tốc độ này được mang lại bởi việc thay đổi độ phức tạp khớp một cách thích ứng có thể có lợi cho việc ứng dụng PLwF.

**Hiệu quả Mô hình.** Trong phần này, chúng tôi đánh giá hiệu suất của PLwF, PLwF w/ credit và các phương pháp CL điển hình về hiệu quả mô hình. Tất cả các thí nghiệm được thực thi trên một server với một Card Đồ họa NVIDIA TITAN. Hình 5 báo cáo thời gian huấn luyện và tỷ lệ tăng của việc huấn luyện 20 bước so với huấn luyện 10 bước trên CIFAR-100. Chúng tôi rút ra những quan sát sau về thời gian huấn luyện: (i) Trong cả hai benchmark, PLwF đơn giản có thời gian chạy tương đương với FDR (Hình 5a). (ii) Chi phí tính toán của phương pháp chúng tôi thấp hơn đáng kể so với các phương pháp dựa trên tái diễn như GSS và GEM. Chúng tôi rút ra những quan sát sau về tỷ lệ tăng: (i) Khi số lượng nhiệm vụ tăng từ 10 lên 20, tỷ lệ tăng của PLwF trong tính toán ít hơn PODNET (135.33%) và GEM (71.55%) (Hình 5b). (ii) PLwF w/ credit áp đặt chi phí thời gian tầm thường (3.92%) ngay cả khi được huấn luyện trên 20 nhiệm vụ. Những quan sát này tiết lộ tính thực tế của PLwF.

**Hạn chế.** PLwF chuyển kiến thức một cách tiến bộ từ các hàm trước đó đến giai đoạn hiện tại, trong khi điều này có thể dẫn đến sự tăng trưởng tính toán khi chúng ta có số lượng nhiệm vụ tăng lên. Chúng tôi cung cấp các thảo luận ngắn gọn ở đây về cách có thể giảm tính toán, và nhiều hơn có thể được tìm thấy trong Phụ lục. Đầu tiên, khớp hàm vanilla liên quan đến forwarding lặp đi lặp lại cồng kềnh, và phương pháp của chúng tôi làm trầm trọng thêm quá trình do sự gia tăng số lượng hàm khớp. Điều này cho thấy rằng sự tăng trưởng có thể được cải thiện bằng cách giảm forwarding lặp lại, ví dụ Fast Knowledge Distillation có thể tăng tốc 2-4x mà không ảnh hưởng đến độ chính xác trong khớp single-shot. Trong trường hợp của chúng tôi, chúng tôi có thể tăng tốc đáng kể việc huấn luyện do số lượng lớn các nhiệm vụ. Textbrewer cung cấp hỗ trợ plug-and-play cho giải pháp này để cho phép khởi tạo nhanh. Thứ hai, Prune, then Distill giảm chi phí tính toán trong quá trình khớp bằng cách cắt tỉa các mô hình, điều này thậm chí có thể cải thiện thêm hiệu suất theo cách chi phí thấp. Các khám phá sâu hơn được để lại như công việc tương lai.

## 8. Phân tích Thực nghiệm của Phương pháp

**Giải thích Trực quan về PLwF.** Để kiểm tra lợi ích của việc thích ứng các hàm trước đó, chúng tôi thí nghiệm trên một trường hợp đặc biệt của PLwF nới lỏng — sử dụng hàm đầu tiên của tất cả các hàm trước đó. Thêm thí nghiệm về các lựa chọn hàm khác được chi tiết trong Phụ lục. Hình 6 tiết lộ những quan sát sau: (i) Kết quả trên 10 lớp đầu tiên vẫn tốt trong suốt mười bước tăng dần (đường màu cam trong Hình 6(a)). (ii) Tại bước thứ 10, kết quả của 10 lớp đầu tiên cao hơn đáng kể so với 10 lớp khác (cột màu cam trong Hình 6(b)). Chúng tôi kết luận rằng một bộ học sâu có thể nhớ lại kiến thức đã học trong một nhiệm vụ sớm bằng cách sử dụng hàm tương ứng, điều này làm cho hàm hiện tại đáng tin cậy hơn.

**Hỗ trợ cho PLwF.** Trong Định nghĩa 1, chúng tôi đề xuất rằng có một không gian nơi kiến thức biến mất ít hơn. Đầu tiên, chúng tôi khử tách ảnh hưởng của giả thuyết PLwF. Bảng 3 tiết lộ rằng việc mở rộng không gian học cải thiện đáng kể hiệu suất nhiệm vụ. Để kiểm tra thêm giả thuyết như vậy, các chiến lược lấy mẫu khác nhau được sử dụng để đưa ra các không gian hàm khác nhau trong đó mức độ biến mất kiến thức khác nhau. Trong Hình 8, chúng tôi trình bày kết quả sử dụng các chiến lược lấy mẫu khác nhau. Cụ thể, lấy mẫu đều đặn đại diện cho lấy mẫu từ hàm F theo tập hợp các khoảng 2, 3, 4. Lấy mẫu ngẫu nhiên đại diện cho việc 2, 3 và 4 hàm được chọn ngẫu nhiên từ hàm F. Trực quan, không gian hàm F đề cập đến kiến thức tốt hơn có thể được học khi hàm của mỗi nhiệm vụ được sử dụng. Điều này được chứng minh từ hiệu suất tốt nhất được thể hiện trong Bảng 2 rằng không gian hàm trong trường hợp này có khả năng CL đáng kể nhất với ít quên nhất. Như được thể hiện trong Hình 8, khi kích thước không gian giảm dưới các chiến lược lấy mẫu khác nhau, kết quả của CL cho thấy xu hướng giảm, phản ánh sự biến mất của nhiều kiến thức hơn. Những hiện tượng này hỗ trợ giả thuyết của chúng tôi. Điều đó có nghĩa là không gian nơi kiến thức biến mất ít hơn có thể tạo thuận lợi cho PLwF, vì nó bao phủ kiến thức tiên nghiệm của các phân phối nhãn chính xác hơn. Ngược lại, sự cải thiện trong khả năng học tại mỗi nhiệm vụ có nghĩa là việc sử dụng mô hình huấn luyện PLwF là một khởi tạo tốt cho việc học tiếp theo. Do đó, một sự thúc đẩy lành tính được hình thành để tối đa hóa kỳ vọng tích lũy kinh nghiệm. Thêm chi tiết về tác động của không gian học được đưa ra trong Phụ lục.

**Hỗ trợ cho Giả thuyết Phân bổ Tín dụng.** Để kiểm tra giả thuyết phân bổ tín dụng được đề xuất trong phần phương pháp, các thí nghiệm được tiến hành trên 5-Split CIFAR-10, 10-Split CIFAR-100, và 10-Split Tiny-ImageNet. Như được thể hiện trong Bảng 3, trên các benchmark 5-Step và 10-Step, hiệu suất của CL sau khi kết hợp phân bổ tín dụng có được sự gia tăng 1.98%, 1.78%, và 3.26% trên Avg và 3.08%, 3.00%, và 1.90% trên Last. Điều này chỉ ra rằng bộ tối ưu hóa đang thay đổi theo hướng thuận lợi hơn cho bộ học liên tục, và phân bổ tín dụng thành công điều hòa tính ổn định và dẻo. Hơn nữa, nó cũng cho thấy rằng động lực kéo co trong mỗi lần lặp giữa các nhiệm vụ khác nhau có thể được nắm bắt tốt bởi ma trận phân bổ gradient để cải thiện độ tin cậy của phân bổ tín dụng. Ngoài ra, một chuỗi ngắn các nhiệm vụ chỉ chứa 2 bước, câu đố cơ bản nhất cho chế độ phân bổ tín dụng, được thiết kế đặc biệt để quan sát tính ổn định-dẻo. Trong trường hợp này, chỉ có một kéo co mà chế độ phải hoạt động để cải thiện. Do đó, đây là kịch bản cơ bản nhất cho phân bổ tín dụng. Như được thể hiện trong Bảng 4, hiệu suất được cải thiện rõ rệt trên mỗi cài đặt, điều hòa mạnh mẽ tính ổn định và dẻo. Tóm lại, hiệu suất của phương pháp chúng tôi trên các nhiệm vụ của các chuỗi với độ dài khác nhau cung cấp bằng chứng thực nghiệm tốt cho giả định phân bổ tín dụng rằng điều hòa động lực kéo co.

**Hành vi Không phụ thuộc Thứ tự.** Để xem xét kỹ hiệu ứng của việc thay đổi thứ tự lớp, chúng tôi định nghĩa một số thứ tự ngẫu nhiên dựa trên các seed ngẫu nhiên khác nhau để chia CIFAR-100, điều này mang lại các phân phối lớp động và không biết trước hơn. Bảng 6 tiết lộ rằng phương pháp của chúng tôi vẫn hoạt động tốt mà không liên quan đến bất kỳ dữ liệu cũ nào. Hơn nữa, như được thể hiện trong Hình 7, trong trường hợp này, phương pháp của chúng tôi vẫn hiệu quả trong việc kiểm soát hiện tượng quên (Cột đỏ). Nhìn chung, tác động của thứ tự nhiệm vụ dường như không đáng kể.

**Tính Tổng quát của Phân bổ Tín dụng.** Chúng tôi phân tích tính tổng quát của phân bổ tín dụng từ việc sử dụng các bộ tối ưu hóa khác nhau và từ tính hữu ích của nó trên các phương pháp CL trước đó. Đầu tiên, chúng tôi phân tích hiệu suất của phân bổ tín dụng trên các bộ tối ưu hóa khác (Adam, Adadelta, và RMSprop) ngoài SGD. Chúng tôi có được ba quan sát từ Bảng 7: (i) động lực kéo co xảy ra phổ biến trong các phương pháp tối ưu hóa dựa trên gradient; (ii) chế độ phân bổ tín dụng cho thấy cải thiện rõ rệt cho các bộ học liên tục bằng cách giảm thiểu động lực kéo co; (iii) chế độ phân bổ tín dụng có thể hoạt động cùng với các bộ tối ưu hóa dựa trên gradient được sử dụng rộng rãi; Thứ hai, chúng tôi thích ứng phân bổ tín dụng với các phương pháp trước đó như EWC. Đáng chú ý, như được thể hiện trong Bảng 5, chúng tôi quan sát sự cải thiện đáng kinh ngạc bằng cách trang bị phân bổ tín dụng trên EWC (+2.98% trên Avg, +8.50% trên Last). Kết hợp những quan sát này, chúng tôi kết luận rằng việc tái tạo cẩn thận động lực kéo co là có giá trị cho CL và giúp chúng ta hiểu rõ hơn về CL.

## 9. Kết luận

Trong bài báo này, chúng tôi đề xuất PLwF, đưa vào dày đặc các hàm trước đó, tạo ra một không gian học với ít kiến thức biến mất hơn. Dựa trên không gian mới được xây dựng, chúng tôi tiếp tục giảm thiểu hiện tượng quên bằng cách thiết lập chế độ phân bổ tín dụng để tái tạo động lực kéo co khi học các nhiệm vụ mới. Chúng tôi cho thấy PLwF giữ lại kiến thức trung thực mà không yêu cầu các mẫu cũ hay xây dựng tinh vi không gian con nhiệm vụ cũ.

## Tài liệu tham khảo

[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), pages 139–154, 2018.

[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019.

[3] Ari S. Benjamin, David Rolnick, and Konrad P. Kording. Measuring and regularizing networks in function space. In International Conference on Learning Representations, 2019.

[4] Marcus K Benna and Stefano Fusi. Computational principles of synaptic memory consolidation. Nature neuroscience, 19(12):1697–1706, 2016.

[5] Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2022.

[6] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920–15930, 2020.

[7] Arslan Chaudhry, Puneet Kumar Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Proceedings of the European Conference on Computer Vision (ECCV), volume 11215, pages 556–572, 2018.

[8] Arslan Chaudhry, Albert Gordo, Puneet K. Dokania, Philip H. S. Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. In AAAI, pages 6993–7001, 2021.

[9] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.

[10] Joseph Cichon and Wen-Biao Gan. Branch-specific dendritic ca2+ spikes cause persistent synaptic plasticity. Nature, 520(7546):180–185, 2015.

[11] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

[12] Danruo Deng, Guangyong Chen, Jianye Hao, Qiong Wang, and Pheng-Ann Heng. Flattening sharpness for dynamic gradient projection memory benefits continual learning. Advances in Neural Information Processing Systems, 34, 2021.

[13] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In ECCV, 2020.

[14] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Statistics, pages 3762–3773. PMLR, 2020.

[15] Tao Feng, Mang Wang, and Hangjie Yuan. Overcoming catastrophic forgetting in incremental object detection via elastic response distillation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR. IEEE, 2022.

[16] Stefano Fusi, Patrick J Drew, and Larry F Abbott. Cascade models of synaptically stored memories. Neuron, 45(4):599–611, 2005.

[17] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.

[18] Yunhui Guo, Mingrui Liu, Tianbao Yang, and Tajana Rosing. Improved schemes for episodic memory-based lifelong learning. Advances in Neural Information Processing Systems, 33:1023–1035, 2020.

[19] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028–1040, 2020.

[20] Akiko Hayashi-Takagi, Sho Yagishita, Mayumi Nakamura, Fukutoshi Shirai, Yi I Wu, Amanda L Loshbaugh, Brian Kuhlman, Klaus M Hahn, and Haruo Kasai. Labelling and optical erasure of synaptic memory traces in the motor cortex. Nature, 525(7569):333–338, 2015.

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[22] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.

[23] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. In NeurIPS Continual learning Workshop, 2018.

[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.

[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.

[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, 2009.

[27] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.

[28] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, pages 4652–4662, 2017.

[29] Timothée Lesort, Andrei Stoian, and David Filliat. Regularization shortcomings for continual learning. CoRR, abs/1912.03049, 2019.

[30] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018.

[31] Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang. Trgp: Trust region gradient projection for continual learning. arXiv preprint arXiv:2202.02931, 2022.

[32] Hao Liu and Huaping Liu. Continual learning with recursive gradient optimization. International Conference on Learning Representations, 2022.

[33] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. ICLR, 2022.

[34] David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.

[35] Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. CoRR, 2022.

[36] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in Neural Information Processing Systems, 32, 2019.

[37] Jinhyuk Park and Albert No. Prune your model before distill it. ECCV, 2022.

[38] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017.

[39] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In Advances in Neural Information Processing Systems, volume 32, 2019.

[40] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.

[41] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning. In International Conference on Learning Representations, 2020.

[42] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning, volume 80, pages 4535–4544, 2018.

[43] Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4555–4564, 2018.

[44] Zhiqiang Shen and Marios Savvides. MEAL V2: boosting vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without tricks. CoRR, abs/2009.08453, 2020.

[45] Zhiqiang Shen and Eric Xing. A fast knowledge distillation framework for visual recognition. ECCV, 2022.

[46] Amal Rannen Triki, Rahaf Aljundi, Matthew B. Blaschko, and Tinne Tuytelaars. Encoder based lifelong learning. In IEEE International Conference on Computer Vision, ICCV 2017, pages 1329–1337, 2017.

[47] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems, pages 907–916, 2018.

[48] Shipeng Yan, Jiangwei Xie, and Xuming He. DER: dynamically expandable representation for class incremental learning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 3014–3023. Computer Vision Foundation / IEEE, 2021.

[49] Guang Yang, Cora Sau Wan Lai, Joseph Cichon, Lei Ma, Wei Li, and Wen-Biao Gan. Sleep promotes branch-specific formation of dendritic spines after learning. Science, 344(6188):1173–1178, 2014.

[50] Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting Liu, Shijin Wang, and Guoping Hu. TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2020.

[51] Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust continual learning with additive parameter decomposition. In International Conference on Learning Representations, 2020.

[52] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In International Conference on Learning Representations, 2018.

[53] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824–5836, 2020.

[54] Hangjie Yuan, Jianwen Jiang, Samuel Albanie, Tao Feng, Ziyuan Huang, Dong Ni, and Mingqian Tang. RLIP: relational language-image pre-training for human-object interaction detection. 2022.

[55] Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701, 2012.

[56] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning, pages 3987–3995. PMLR, 2017.

[57] Jie Zheng, Andrea GP Schjetnan, Mar Yebra, Bernard A Gomes, Clayton P Mosher, Suneil K Kalia, Taufik A Valiante, Adam N Mamelak, Gabriel Kreiman, and Ueli Rutishauser. Neurons detect cognitive boundaries to structure episodic memories in humans. Nature neuroscience, 25(3):358–368, 2022.

[58] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan Zhan. Pycil: A python toolbox for class-incremental learning. arXiv preprint arXiv:2112.12533, 2021.

[59] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Machine Learning, Proceedings of the Twentieth International Conference (ICML), 2003.

## Tài liệu Bổ sung

Trong tài liệu bổ sung này, đầu tiên chúng tôi cung cấp thêm quan sát về độ tin cậy của các hàm dựa trên các phương pháp khác nhau (Phụ lục A.1). Tiếp theo, chúng tôi cung cấp thêm giải thích trực quan về PLwF từ các giai đoạn trước khác nhau (Phụ lục A.2). Sau đó, chúng tôi trình bày thảo luận chi tiết về hạn chế (Phụ lục A.3). Và chúng tôi cung cấp thêm phiên bản của PLwF nới lỏng (Phụ lục A.4). Hơn nữa, chúng tôi phân tích việc sử dụng GPU của PLwF (Phụ lục A.5). Sau đó, chúng tôi cung cấp thêm thí nghiệm và thảo luận về ảnh hưởng mang lại bởi hành vi không phụ thuộc thứ tự và thêm chi tiết về kết quả chính (Phụ lục A.6 và A.8). Tiếp theo, chúng tôi trình bày thêm thảo luận về tính tổng quát của phân bổ tín dụng (Phụ lục A.7). Cuối cùng, chúng tôi thảo luận về các hướng xu hướng trong các phương pháp Raw-Data-Free (Phụ lục A.9). Code sẽ được công khai sau khi xuất bản.

## A.1 Thêm quan sát về độ tin cậy của các hàm

Trong phần này, chúng tôi tiến hành thêm thí nghiệm rộng rãi trên EWC và iCaRL, nhằm chứng minh rằng vấn đề độ tin cậy đang mờ nhạt của một hàm nhất định là phổ biến trong các phương pháp dựa trên điều chuẩn hóa và dựa trên bộ nhớ. Quan sát này thúc đẩy PLwF.

Hình 9 cho thấy cách một hàm đơn có thể mất độ tin cậy. Chúng tôi quan sát rằng mặc dù EWC và iCaRL ngăn chặn hiện tượng quên ở mức độ khác nhau dựa trên các phương pháp khác nhau, chúng vẫn cho thấy cùng xu hướng: khi học tăng dần tiến triển, hàm quên ngày càng nhiều về các giai đoạn mà chúng phải chịu trách nhiệm. Như được thể hiện trong Hình 9, mặc dù EWC và iCaRL giữ lại hiệu suất tốt hơn (Đường màu cam) so với Plain-SGD (Đường màu xanh), hiện tượng quên vẫn đang tích lũy. So với iCaRL, EWC là phương pháp tệ hơn. Điều này chỉ ra rằng hàm trở nên không đáng tin cậy cho nhiệm vụ đầu tiên khi chuỗi các nhiệm vụ tăng lên. (Mặc dù chúng tôi minh họa ý tưởng của mình bằng nhiệm vụ đầu tiên, các nhiệm vụ khác có thể cho thấy xu hướng tương tự.)

Hình 10 tiết lộ mối quan hệ giữa độ tin cậy của hàm tại nhiệm vụ sớm và tại nhiệm vụ hiện tại. Như được thể hiện trong Hình 10, chúng tôi quan sát xu hướng rằng hiệu suất được giữ lại nhiều hơn cho hàm gần với nhiệm vụ hiện tại hơn (nhiệm vụ 10 trong Hình 10 (a) và Hình 10 (b)), ví dụ hiệu suất của hàm từ nhiệm vụ 9 (gần nhất với nhiệm vụ 10) bị quên ít nhất. Ngược lại, ít hiệu suất được giữ lại cho hàm xa hơn từ nhiệm vụ hiện tại, ví dụ hiệu suất của hàm từ nhiệm vụ 1 (xa nhất từ nhiệm vụ 10) bị quên nhiều nhất. Điều này chỉ ra rằng hàm của nhiệm vụ sớm dần dần mất độ tin cậy của nó.

Dựa trên những quan sát này, chúng tôi có thể kết luận rằng những thay đổi của không gian kiến thức do độ tin cậy đang mờ nhạt của hàm là một vấn đề cần được xem xét cẩn thận trong CL. Điều này cung cấp thêm hỗ trợ cho phương pháp PLwF được đề xuất.

## A.2 Thêm giải thích trực quan về PLwF

Như được thể hiện trong Hình 11, để kiểm tra lợi ích của việc thích ứng các hàm trước đó, chúng tôi quan sát hiệu ứng của việc khớp các hàm từ các khoảng cách khác nhau trong PLwF. Chúng tôi cụ thể sử dụng chỉ một hàm từ 5 nhiệm vụ đầu làm hàm khớp để thực hiện CL. Hình 11 tiết lộ những quan sát sau: (i.) Trong Hình 11 (a, c, e, g, i), khi một hàm từ nhiệm vụ sớm được áp dụng như một hàm khớp, sự tích lũy của hiện tượng quên tại nhiệm vụ đó giảm đáng kể khi việc học tiếp tục (Đường màu cam). Điều này chỉ ra rằng bộ học sâu nhớ lại kiến thức đã học trong các nhiệm vụ sớm đó, điều này làm cho hàm hiện tại đáng tin cậy hơn. (ii.) So với các nhiệm vụ khác mà các hàm khớp tương ứng không được sử dụng, nhiệm vụ mà hàm khớp tương ứng được sử dụng bị quên ít hơn đáng kể (Cột màu cam trong Hình 11 (b, d, f, h, j)). Điều này chỉ ra rằng một hàm từ các nhiệm vụ sớm mà không có sự mờ nhạt kiến thức là cần thiết.

Tóm lại, những thí nghiệm này cung cấp thêm hỗ trợ cho độ tin cậy của các hàm từ các nhiệm vụ sớm. Dựa trên những điều này, chúng tôi suy đoán rằng: sự vắng mặt của một hàm tại một giai đoạn cụ thể làm cho CL thiên vị chống lại nhiệm vụ cũ này. Ngược lại, được cung cấp các hàm vắng mặt từ các nhiệm vụ sớm, một bộ học sâu có thể nhớ lại kiến thức đã học.

## A.3 Thêm thảo luận về hạn chế

Trong bài báo chính, chúng tôi thảo luận về hạn chế của phương pháp đề xuất. Như một bổ sung, chúng tôi cung cấp thảo luận chi tiết và bằng chứng về hai tùy chọn phương pháp để giảm độ phức tạp tính toán nhằm đảm bảo tính ứng dụng của PLwF.

**[Phương pháp A] Fast Knowledge Distillation (FKD).** Trong các đoạn văn sau, chúng tôi nhằm điều tra một cách tiếp cận có thể để tăng hiệu quả huấn luyện của PLwF với sự hỗ trợ của FKD.

PLwF sử dụng KD vanilla như một phương pháp mặc định để chuyển kiến thức từ các hàm trước đó đến mô hình hiện tại. Nhược điểm chính của khung KD vanilla là nó tiêu thụ phần lớn chi phí tính toán trên forwarding thông qua mô hình giáo viên khổng lồ. Cụ thể hơn, các tham số của mô hình giáo viên bị đóng băng, làm cho forwarding lặp đi lặp lại trên mô hình giáo viên trở nên dư thừa trong huấn luyện. Trong khi FKD, ở một mức độ nào đó, giải quyết vấn đề. FKD tạo ra một vector xác suất như nhãn mềm cho mỗi hình ảnh huấn luyện, sau đó tái sử dụng chúng một cách tuần hoàn cho các epoch huấn luyện khác nhau. Hiệu quả giảm các tính toán forward lặp lại để tăng tốc 25× mà không ảnh hưởng đến độ chính xác. Ví dụ, Bảng 8 tiết lộ rằng bằng cách sử dụng cùng các siêu tham số và mạng giáo viên (Resnet-50), FKD đạt được kết quả tương tự với phương pháp KD cơ bản trong khi tăng tốc đáng kể việc huấn luyện.

Liên quan đến sự kết hợp của PLwF và FKD, vì PLwF đưa vào dày đặc các hàm trước đó vào giai đoạn hiện tại của học tăng dần, chúng tôi có thể tăng tốc đáng kể việc huấn luyện do số lượng lớn các hàm (tức là các mô hình giáo viên). Chúng tôi mong đợi rằng các đóng góp sẽ theo sau.

**[Phương pháp B] Prune, then Distill.** Một giải pháp trực quan khác để giảm chi phí tính toán là tăng tốc độ suy luận của các hàm mô hình. Prune, then Distill cung cấp hỗ trợ tốt cho giải pháp này. Chúng tôi phân tích tính khả thi về cả tốc độ suy luận và độ chính xác.

Tốc độ suy luận & Độ chính xác: Kết quả mong muốn của Phương pháp B là giảm chi phí tính toán trong khi đảm bảo độ chính xác không bị ảnh hưởng. Trong Bảng 9, FKD đã chứng minh việc cắt tỉa các mô hình sẽ không tác động tiêu cực đến tốc độ suy luận và độ chính xác. Hơn nữa, FKD thậm chí giúp các mô hình học sinh đạt hiệu suất tốt hơn.

Ngoài ra, chúng tôi có thể sử dụng các phương pháp cắt tỉa vượt trội hơn để tăng hiệu suất. Ví dụ, đã đề xuất rằng một mạng con được cắt tỉa ngẫu nhiên của ResNet có thể vượt trội hơn ResNet dày đặc. Tóm lại, trong trường hợp của chúng tôi, vấn đề chi phí tính toán cao sẽ được giải quyết tốt bởi Prune, then Distill.

## A.4 Thảo luận về PLwF nới lỏng

Chúng tôi cung cấp một số phiên bản nới lỏng của PLwF bằng cách sử dụng một tập con của các hàm trước đó F̃: (i) Sơ đồ 1 (Hình 15 và Bảng 13): Sử dụng hàm thứ (t-1) và 30%/50% đầu tiên các hàm từ các giai đoạn trước đó trong Phương trình 4. (ii) Sơ đồ 2: (Hình 16 và Bảng 14): Chỉ sử dụng 30%/50% đầu tiên các hàm từ các giai đoạn trước đó trong Phương trình 4. (iii) Sơ đồ 3: (Hình 17 và Bảng 15): Sử dụng các hàm được lấy mẫu với khoảng giai đoạn 2, 3 và 4 trong Phương trình 4. (iv) Sơ đồ 4: (Hình 18 và Bảng 16): Sử dụng 2, 3 và 4 hàm được chọn ngẫu nhiên trong Phương trình 4.

Tất cả các sơ đồ trên đều giảm đáng kể chi phí tính toán, điều này tăng cường ứng dụng của phương pháp đề xuất. Trong số chúng, Sơ đồ 1 xem xét rằng các hàm trước đó có ảnh hưởng khác nhau đến việc tối ưu hóa của nhiệm vụ hiện tại, ví dụ, các hàm gần đây hơn chịu ít quên hơn, và các hàm trước đó chịu nhiều quên hơn. Do đó, chúng tôi có thể lấy mẫu nhiều hàm gần đây hơn để đạt được hiệu suất tốt. Ngược lại, Sơ đồ 2 từ bỏ các hàm gần đây hơn và do đó, xuất hiện như một chiến lược kém hiệu quả về chi phí. Trong khi Sơ đồ 3 và Sơ đồ 4 bị ảnh hưởng bởi các chiến lược lấy mẫu khác nhau, do đó một chiến lược lấy mẫu tốt cần được xem xét trong thực tế. Tóm lại, bằng cách lấy mẫu các hàm thích hợp (PLwF nới lỏng), chúng tôi có thể duy trì chi phí tính toán của PLwF ở mức không đổi, trong khi đảm bảo hiệu suất tốt. Một sơ đồ như vậy có lợi cho việc ứng dụng PLwF.

## A.5 Sử dụng GPU của PLwF và PLwF Nới lỏng

Trong phần này, chúng tôi trình bày việc sử dụng GPU của PLwF và PLwF Nới lỏng. Trong Hình 12a, chúng tôi cụ thể so sánh với phương pháp biểu diễn có thể mở rộng động (DER). Chúng tôi quan sát rằng việc sử dụng GPU và tốc độ tăng trưởng thấp hơn đáng kể so với DER, mặc dù PLwF đưa vào dày đặc các hàm trước đó. Hơn nữa, như được thể hiện trong Hình 12b, chúng tôi trình bày việc sử dụng GPU chi tiết cho sơ đồ 1 (Hình 15 và Bảng 13). Hình 12b tiết lộ rằng chúng tôi có thể duy trì chi phí tính toán của PLwF ở mức không đổi.

## A.6 Thêm chi tiết về kết quả chính

Để hiểu rõ hơn về lợi thế thực sự của phương pháp đề xuất, chúng tôi trình bày kết quả trung bình qua một số giai đoạn học trên 10-Split CIFAR-100 và 20-Split CIFAR-100. Như được thể hiện trong Hình 13, PLwF liên tục vượt trội hơn tất cả các phương pháp với khoảng cách đáng kể trên tất cả các cài đặt.

## A.7 Thêm thảo luận về tính tổng quát của phân bổ tín dụng

Trong Tính tổng quát của Phân bổ Tín dụng của bài báo chính, chúng tôi trình bày kết quả của việc thêm phân bổ tín dụng trên EWC, và mang lại cải thiện rõ rệt. Hơn nữa, chúng tôi quan sát tác động của phân bổ tín dụng trên iCaRL. Trong Bảng 10, chúng tôi trình bày kết quả của việc thêm phân bổ tín dụng trên iCaRL, điều này cải thiện hiệu suất của iCaRL 1.14% trên Avg và 2.08% trên Last. Trong Bảng 12, chúng tôi trình bày biến thể hiệu suất của iCaRL trên nhiệm vụ đầu tiên, điều này cho thấy hiện tượng quên được kiểm soát thêm. Đây là một hiện tượng có giá trị vì động lực kéo co thường tồn tại trong các phương pháp trước đó, và phân bổ tín dụng có thể tái tạo xung đột này. Chúng tôi mong đợi phân bổ tín dụng sẽ truyền cảm hứng cho công việc tương lai tập trung vào vấn đề này và các đóng góp sẽ theo sau.

## A.8 Chi tiết về hành vi không phụ thuộc thứ tự

Để quan sát ảnh hưởng của các thứ tự lớp khác nhau, chúng tôi đặt các seed ngẫu nhiên khác nhau để chia tập dữ liệu CIFAR-100, điều này mang lại các phân phối lớp động và không biết trước hơn. Như được thể hiện trong Bảng 11, phương pháp đề xuất vượt trội hơn các phương pháp khác sử dụng ba seed ngẫu nhiên khác nhau. Điều này xác nhận thêm tính mạnh mẽ của phương pháp chúng tôi. Hơn nữa, như được thể hiện trong Hình 14(a, b, c), trong trường hợp này, phương pháp của chúng tôi vẫn hiệu quả trong việc kiểm soát hiện tượng quên (Đường màu cam). Những quan sát này cung cấp hỗ trợ bổ sung cho phương pháp của chúng tôi.

## A.9 Hướng xu hướng trong các phương pháp raw-data-free

Trong một số ứng dụng, việc lưu trữ dữ liệu thô không khả thi do các mối quan ngại về quyền riêng tư và bảo mật, và điều này đòi hỏi các phương pháp CL duy trì hiệu suất hợp lý mà không có bất kỳ dữ liệu thô nào. Phương pháp dựa trên điều chuẩn hóa là một hướng của cách tiếp cận này.

PLwF chỉ ra rằng phương pháp dựa trên điều chuẩn hóa bị hạn chế bởi không gian có thể học không đầy đủ do độ tin cậy hàm đang mờ nhạt. Ngược lại, các phương pháp dựa trên tái diễn chịu khủng hoảng độ tin cậy nhẹ hơn của hàm do tiếp xúc trực tiếp với dữ liệu thô. PLwF cung cấp một hướng tiềm năng cho các phương pháp dựa trên điều chuẩn hóa: Tìm một container kiến thức với kiến thức chính xác và tươi mới nhất về các nhiệm vụ trước đó. Mặc dù các phương pháp dựa trên tái diễn thường đạt được hiệu suất tốt hơn so với các phương pháp dựa trên điều chuẩn hóa, PLwF mở đường cho các phương pháp dựa trên điều chuẩn hóa (phương pháp raw-data-free) phát triển trở lại.
