# 1903.04476.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/1903.04476.pdf
# File size: 393145 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Continual Learning via Neural Pruning
Siavash Golkar
New York University
golkar@nyu.eduMichael Kagan
SLAC National Accelerator Laboratory
makagan@slac.stanford.edu
Kyunghyun Cho
New York University
Facebook AI Research
CIFAR Azrieli Global Scholar
kyunghyun.cho@nyu.edu
Abstract
We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed
at lifelong learning in Ô¨Åxed capacity models based on neuronal model sparsiÔ¨Åcation.
In this method, subsequent tasks are trained using the inactive neurons and Ô¨Ålters of
the sparsiÔ¨Åed network and cause zero deterioration to the performance of previous
tasks. In order to deal with the possible compromise between model sparsity and
performance, we formalize and incorporate the concept of graceful forgetting: the
idea that it is preferable to suffer a small amount of forgetting in a controlled manner
if it helps regain network capacity and prevents uncontrolled loss of performance
during the training of future tasks. CLNP also provides simple continual learning
diagnostic tools in terms of the number of free neurons left for the training of future
tasks as well as the number of neurons that are being reused. In particular, we see
in experiments that CLNP veriÔ¨Åes and automatically takes advantage of the fact
that the features of earlier layers are more transferable. We show empirically that
CLNP leads to signiÔ¨Åcantly improved results over current weight elasticity based
methods.
1 Introduction
Continual learning, the ability of models to learn to solve new tasks beyond what has previously been
trained, has garnered much attention from the machine learning community in recent years. This is
driven in part by the practical advantages promised by continual learning schemes such as improved
performance on subsequent tasks as well as a more efÔ¨Åcient use of resources in machines with memory
constraints. There is also great interest in continual learning from a more long term perspective, in
that any approach towards artiÔ¨Åcial general intelligence needs to be able to continuously build on top
of prior experiences.
As it stands today, the main obstacle in the path of effective continual learning is the problem of
catastrophic forgetting: machines trained on new problems forget about the tasks that they were
previously trained on. There are multiple approaches in the literature which seek to alleviate this
problem, ranging from employing networks with many submodules [ 2,12,15] to constraining the
weights of the network that are deemed important for previous tasks [ 6,8,20]. These approaches
either require specialized genetic algorithm training schemes or still suffer catastrophic forgetting,
albeit at a smaller rate. In particular, to the best of our knowledge, there is no form of guarantee
regarding the performance of previous tasks among the Ô¨Åxed capacity models which use standard
SGD training schemes.
Preprint. Work in progress.arXiv:1903.04476v1  [cs.LG]  11 Mar 2019

--- PAGE 2 ---
In this work we introduce a simple Ô¨Åxed capacity continual learning scheme which can be trained using
standard gradient descent methods and by construction suffers zerodeterioration on previously learned
problems during the training of new tasks. In short, we take advantage of the over-parametrization of
neural networks by using an activation based neural pruning sparsiÔ¨Åcation scheme to train models
which only use a fraction of their width. We then train subsequent tasks utilizing the unused capacity
of the model. By cutting off certain connection in the network, we make sure that new tasks can take
advantage of previously learned features but cause no interference in the pathways of the previously
learned tasks.
Main contributions
We introduce Continual Learning via Neural Pruning (CLNP), a simple and intuitive lifelong
learning method with the following properties:
‚ÄìGiven a network with activation based neuron sparsity trained on some previous
tasks, CLNP trains new tasks utilizing the unused weights of the network in a manner
which takes advantage of the features learned by the previous tasks while causing zero
catastrophic forgetting.
‚ÄìCLNP provides simple diagnostics in the form of the number of remaining and reused
neurons and Ô¨Ålters when training each task. In particular, in experiments we see that
CLNP veriÔ¨Åes and automatically takes advantage of the fact that the features of earlier
layers are more transferable.
We expand on the idea of graceful forgetting, the notion that it is preferable to suffer some
small amount of forgetting in a controlled manner if it helps regain network capacity and
prevents uncontrolled loss of performance during the training of future tasks. We use
this idea to control the compromise between network sparsity and model accuracy in our
approach.
We show empirically that using an activation based neural pruning sparsiÔ¨Åcation scheme,
we signiÔ¨Åcantly outperform previous approaches based on weight elasticity on a number
of benchmarks. We also demonstrate in one example that using a slightly more advanced
variation of our sparsiÔ¨Åcation method, the network suffers virtually no loss of performance,
either from catastrophic forgetting or from sparsiÔ¨Åcation.
The remainder of the paper is organized as follows. In Sec. 2, we provide the methodology of our
approach. We Ô¨Årst discuss the generalities of training non-destructively in sparsiÔ¨Åed regions of
the network in Sec. 2.1. We then discuss our sparsiÔ¨Åcation scheme as well as the idea of graceful
forgetting as a compromise between sparsity and model performance in Sec. 2.2. We provide an
empirical test of our methodology in Sec. 3, Ô¨Årst on ten tasks derived from the MNIST dataset in
Sec. 3.1 and then on a problem derived from CIFAR-10 and CIFAR-100 in Sec. 3.2. In both cases we
show signiÔ¨Åcant improvement over comparable prior work.
Related work
Lifelong learning. Previous work addressing the problem of catastrophic forgetting generally fall
under two categories. In the Ô¨Årst category, the model is comprised of many individual modules at
each layer and forgetting is prevented either by routing the data through different modules [ 2] or by
successively adding new modules for each new task [ 12,15]. This approach often (but not always)
has the advantage of suffering zero forgetting, however, the structure of these networks is specialized.
In the case of [12, 15], the model is not Ô¨Åxed capacity and in the case of [2] training is done using a
tournament selection genetic algorithm. In the second category of approaches to lifelong learning the
structure of the network as well as the training scheme are standard, and forgetting is addressed by
constraining important weights from changing [ 6,8,20]. These approaches, generally referred to
as weight elasticity methods, have the advantage of having simpler training schemes but still suffer
catastrophic forgetting, albeit at a smaller rate than unconstrained training.
The approach we take in this paper falls under the second category where the network structure
and training scheme are simple and forgetting is prevented by constraining certain weights from
changing. However, it shares the main advantage of the Ô¨Årst category approaches in that we suffer
zero catastrophic forgetting during the training of subsequent tasks. In particular, our method can
2

--- PAGE 3 ---
be thought of as a second category adaptation and simpliÔ¨Åcation of the path based approach of [ 2]
using activation based sparsiÔ¨Åcation and the idea of graceful forgetting. Since our method is directly
comparable to other second category approaches, we will provide quantitative comparisons with
other methods in this category.
Network superposition. The method we put forward in this paper can be thought of as a non-
destructive way of superposing multiple instances of the same architecture using sparsiÔ¨Åcation.
There are previous works in this direction [ 16,17], however these are more along the lines of neural
architecture search and are not aimed at preventing forgetting. The most relevant work in this direction
is [1], which can be thought of as a Fourier space implementation of our approach. However, in
comparison to [ 1] where the superposition is approximate, our approach is exact and we have precise
control over the relationship between the different models being superposed in terms of transfer
learning. We will provide quantitative comparisons in our experiments.
SparsiÔ¨Åcation. SparsiÔ¨Åcation of neural networks has a long history [ 11,13,18]. While sparsi-
Ô¨Åcation is a crucial tool that we use, it is not in itself a focus of this work. For accessibility, we
use a simple neuron/Ô¨Ålter based sparsiÔ¨Åcation scheme which can be thought of as a single iteration
variation of [7] without Ô¨Åne-tuning. For a recent review of sparsiÔ¨Åcation methods see [4].
Parameter based sparsity (sparsity in the weight matrices) was used in [ 12] as a tool to reduce (but
not eliminate) catastrophic forgetting and increase efÔ¨Åciency in a dynamically growing network. In
contrast, our method is centered around activation based sparsity (the sparsity in the number of used
Ô¨Ålters/neurons), suffers zero catastrophic forgetting and is Ô¨Åxed capacity. Our method also bears
some resemblance to [3], a forbear of clustering based methods put forward in the 90s.
2 Methodology
The core idea of our method is to take advantage of the fact that neural networks are vastly over-
parametrized [ 14]. A manifestation of this over-parametrization is through the practice of sparsiÔ¨Åca-
tion, i.e. the compression of neural network with relatively little loss of performance [ 4,11,18]. As
an example, Luo et al . [13] show that VGG-16 can be compressed by more than 16 times, leaving
more than 90% of the connections unused. In this section we Ô¨Årst show that given an activation based
sparse network, we can leverage the unused capacity of the model to develop a continual learning
scheme which suffers no catastrophic forgetting. We then discuss the idea of graceful forgetting to
address the tension between sparsiÔ¨Åcation and model performance in the context of lifelong learning.
It is important to differentiate activation based neuronal sparsity from parameter based weight sparsity.
The former implies only a subset of the neurons or Ô¨Ålters of each layer are active whereas the latter
means that many of the weights are zero but all neurons are assumed to be active at all layers. In
the remainder of the paper, when we mention sparsity, we are referring to activation based neuronal
sparsity.
In what follows we will discuss sparsity for fully connected layers by looking at the individual
neurons. The same argument goes through identically for individual channels of convolutional layers.
2.1 Generalities
Let us assume that we have a trained network which is sparse in the sense that only a subset of the
neurons of the network are active. In effect, networks with this form of sparsity can be thought of as
narrower networks embedded inside the original structure. There are many approaches that aim to
train such sparse networks with little loss of performance (see for example Refs. [ 7,13]). We will
discuss our particular sparsiÔ¨Åcation method in detail in Sec. 2.2.
Fig. 1 shows a cartoon of a network with activation based neuronal sparsity, where the active and
inactive neurons are respectively denoted by blue and grey nodes. Based on the connectivity structure,
the weights of the network can also be split into three classes. First we have the active weights Wact
which connect active nodes to active nodes. These are denoted in blue in Fig. 1. Next we have the
weights which connect any node to inactive nodes, we call these the free weights Wfree, denoted in
grey in Fig. 1. Finally we have the weights which connect the inactive nodes to the active nodes,
3

--- PAGE 4 ---
we call these the interference weights Wint, denoted in red dashed lines in Fig. 1. A more precise
deÔ¨Ånition of the active and inactive neurons and weights is given in Sec. 2.2.
InputOutput
Figure 1: The partition of an network
with neuronal sparsity into active, inac-
tive and interference parts.The crux of our approach is the simple observation that if all
the interference weights Wintare set to zero, the free weights
Wfreecan be changed arbitrarily without causing any change
whatsoever to the output of the network. We can therefore
utilize these weights to train new tasks without causing any
harm to the performance of the previous tasks.
Note that we can further split the free weights into two groups.
First, the weights which connect active nodes to inactive nodes.
These are the weights that take advantage of previously learned
features and are therefore responsible for transfer learning
throughout the network. We also have the weights that connect
inactive nodes to inactive nodes. These weights can form com-
pletely new pathways to the input and train new features. A
complimentary diagnostic for the amount of transfer learning
taking place is the number of new active neurons at each layer
after the training of subsequent tasks. Given that an efÔ¨Åcient
sparse training scheme would not need to relearn the features that are already present in the network,
the number of new neurons grown at each stage of training is an indicator of the sufÔ¨Åciency of the
already learned features for the purposes of the new task. We will see more of this point in Sec. 3.
Output architecture. In order to fully Ô¨Çesh out a continual learning scheme, we need to specify
the connectivity structure of the output nodes. There are two intuitive routes that we can take. We
demonstrate these in Fig. 2, where in the middle we have the same structure as in Fig. 1, but with
the interference weights put to zero. In order to train a new task, one option is to use a new output
layer (i.e. a new head) while saving the previous output layer. This option, demonstrated in Fig. 2 on
the left, is known as the multi-head approach and is standard in continual learning. Because each
new output layer comes with its own sets of weights which connect to the Ô¨Ånal hidden layer neurons,
this method is not a fully Ô¨Åxed capacity method. Note that in our approach to continual learning,
training a multi-head network with a fully depleted core structure, i.e. a network where are no more
free neurons left, is equivalent to Ô¨Ånal layer transfer learning.
InputOutput
InputHead 1 Head 2
InputOutputOutput
Figure 2: The output structure of multi-task learning networks. The sparsiÔ¨Åed network trained on task 1 is
presented in the center after having the interference weights severed. The multi-head and single-head expansion
of this network trained on task 2 are presented on the left and right respectively. In both cases, the output of the
model on task 1 remains unchanged during training task 2, i.e. the green connections do not affect the ouput of
the blue sub-network.
In scenarios where the output layer of the different tasks are structurally compatible, for example
when all tasks are classiÔ¨Åcation on the same number of classes, we can use a single-head approach.
Demonstrated in Fig. 2 on the right, in this approach we use the same output layer for all tasks, but
for each task, we mask out the neurons of the Ô¨Ånal hidden layer that were trained on other tasks.
4

--- PAGE 5 ---
In the case of Fig. 2, only green nodes in the Ô¨Ånal hidden layer are connected to the output for the
second task and only blue nodes for the Ô¨Årst task. This is equivalent to a dynamic partitioning of the
Ô¨Ånal hidden layer into multiple unequal sized parts, one part for each task. In practice this is done
using a multiplicative masking operation with a task dependent mask, denoted in Fig. 2 by dashed
lines after the Ô¨Ånal hidden layer. This structure is truly Ô¨Åxed capacity, but is more restrictive to train
than its multi-head counterpart. Note that since an unconstrained single-head structure would cause
a large amount of interference between tasks, until now it has not been a viable option for weight
elasticity based approaches to continual learning.
2.2 Methodology details
In what follows we will assume that the we are using RectiÔ¨Åer Linear Units (ReLU [ 5]). While we
have only tested our methodology with ReLU networks, we expect it to work similarly with other
activations.
SparsiÔ¨Åcation. So far in this section we have shown that given a sparse network trained on a
number of tasks, we can train the network on new tasks without suffering any catastrophic forgetting.
We now discuss the speciÔ¨Åc sparsiÔ¨Åcation scheme that we use throughout this paper, which is similar
in spirit to the network trimming approach put forward in Ref. [7].
Our sparsiÔ¨Åcation method is comprised of two parts. First, during the training of each task, we
add anL1weight regulator to promote sparsity in the network and to regulate the magnitude of the
weights of the network. The coefÔ¨Åcient of of this regulator is a hyperparameter of our approach.
Also, since different layers have different weight distributions, we can gain more control over the
amount of sparsity in each layer by choosing a different for each layer. The second part of our
sparsiÔ¨Åcation scheme is post-training neuron pruning based on the average activity of each neuron.
Note that the most efÔ¨Åcient sparsiÔ¨Åcation algorithms include a third part which involves adjusting the
surviving weights of the network after pruning. This step is referred to as Ô¨Åne-tuning and is done by
retraining the network for a few epochs while only updating the weights which survive sparsiÔ¨Åcation.
This causes the model to regain some of its lost performance because of pruning. To achieve a yet
higher level of sparsity, one can iterate the pruning and Ô¨Åne-tuning steps multiple times. In this paper,
we only perform one iteration of pruning for simplicity. We also skip the Ô¨Åne tuning step, unless
otherwise speciÔ¨Åed.
In Sec. 2.1, we partitioned the network into active and inactive parts. A precise deÔ¨Ånition of these
different partitions is as follows. Given network N, comprised of Llayers, we denote the neurons of
each layer as Nlwithl= 1L. Let us also assume that the network Nhas been trained on dataset
S. In order to Ô¨Ånd the active and inactive neurons of the network, we compute the average activity
over the entire dataset Sfor each individual neuron. We identify the active neurons Nact
l, i.e. the blue
nodes in Fig. 1, as those whose average activation exceeds some threshold parameter :
Nact
l=fNljES 
Nl
>g:
The inactive neurons are taken as the complement Ninact
l=NlnNact
l. The threshold value is a
post-training hyperparameter of our approach. Similar to the L1weight regulator hyperparameter ,
can take different values for the different layers. Furthermore, if = 0,Ninact
lwould be given
by the neurons in the network which are completely dead and the function being computed by the
network is entirely captured in Nact
l. We can therefore view Nact
las a compression of the network into
a sub-network of smaller width. Based on their connectivity structure, the weights of each layer are
again divided into active, free and interference parts, respectively corresponding to the blue, grey
and red lines in Fig. 1. The overall algorithm of our approach in its multi-head incarnation is given
in Alg. 1 (Here, we use notation W 
A!B
to denotes the subset of weights in Wwhich connect
neuronsAto neuronsB).
Graceful forgetting. While sparsity is crucial in our approach for the training of later tasks, care
needs to be taken so as not to overly sparsify and thereby reduce the model‚Äôs performance. In practice,
model sparsity has the same relationship with generalization as other regularization schemes. As
sparsity increases, initially the generalization performance of the model improves. However, as we
push our sparsity knobs (i.e. the L1regulator and activity threshold) higher and make the network
sparser, eventually both training and validation accuracy will suffer and the network fails to Ô¨Åt the
5

--- PAGE 6 ---
Algorithm 1: Continual Learning via Neural Pruning (CLNP) - Multi-head
Data: datasetsS=fSig, network with l‚Äôth layer neurons Nl
Nact
l;Wint ;;
Wfree  W 
Nl!Nl+1
;
forSi2Sdo
Wint   0;
Re-initialize Wfree;
Place new head N(i)
L;
Train onSiupdating only Wfree;
Nact
l  Nact
l[fNljESi 
jNlj
>g;
Ninact
l  NlnN(i)
l;
Wint  W 
Ninact
l!N(1)
l+1
;
Wfree  W 
Nl!Ninact
l+1
;
data properly. This means that in choosing these hyperparameters, we have to make a compromise
between model performance and remaining network capacity for future tasks.
This brings us to a subject which is often overlooked in lifelong learning literature generally referred
to as graceful forgetting. This is the general notion that it would be preferable to sacriÔ¨Åce a tiny
bit of accuracy in a controlled manner, if it reduces catastrophic forgetting of this task and helps in
the training of future tasks. We believe any successful Ô¨Åxed capacity continual learning algorithm
needs to implement some form of graceful forgetting scheme. In our approach, graceful forgetting is
implemented through the sparsity vs. performance compromise. In other words, after the training
of each task, we sparsify the model up to some acceptable level of performance loss in a controlled
manner. We then move on to subsequent tasks knowing that the model no longer suffers any further
deterioration from training future tasks. This has to be contrasted with other weight elasticity
approaches which use soft constraints on the weights of the network and cannot guarantee future
performance of previously trained tasks. We will see in the next section that using the exact same
network structure, our approach leads to noticeably improved results over existing methods.
Explicitly, the choice of sparsity hyperparameters is made based on this idea of graceful forgetting as
follows. As is standard practice, we split the dataset for each task into training, validation and test
sets. We scan over a range of hyperparameters (i.e. , theL1weight regulator and , the learning
rate) using grid search and note the value of the best validation accuracy across all hyperparameters.
We then pick the models which achieve validation accuracy within a margin of m%of this best
validation accuracy. The margin parameter mcontrols how much we are willing to compromise on
accuracy to regain capacity and in experiments we take it to be generally in the range of 0:05% to
2%depending on the task. We sparsify the picked models using the highest activation threshold 
such that the model remains within this margin of the best validation accuracy. We Ô¨Ånally pick the
hyperparameters which give the highest sparsity among these models. In this way, we efÔ¨Åciently Ô¨Ånd
the hyperparameters which afford the highest sparsity model with validation accuracy within m%of
its highest value.
After pruning away the unused weights and neurons of the model with the hyperparameters chosen as
above, we report the test accuracy of the sparsiÔ¨Åed network. It should be noted that this algorithm
for training and hyperparameter grid search does not incur any signiÔ¨Åcant additional computational
burden over standard practice. The hyperparameter search is performed in standard fashion, and
the additional steps of selecting networks within the acceptable margin, scanning the threshold, and
selecting the highest sparsity network only require evaluation and do not include any additional
network training.
3 Experiments
We evaluated our approach for continual learning on the permuted MNIST [ 10], and split versions of
CIFAR-10 and CIFAR-100 [9] and compare to previous results.
6

--- PAGE 7 ---
3.1 permuted MNIST
In this experiment, we look at the performance of our approach on ten tasks derived from the MNIST
dataset via ten random permutations of the pixels. To compare with previous work, we choose the
same structure and hyperparameters as in Ref. [ 20]: an MLP with two hidden layers, each with
2000 neurons and ReLU activation and a softmax multi-class cross-entropy loss trained with Adam
optimizer and batch size 256. We make a small modiÔ¨Åcation to the structure of the network: as
opposed to Refs. [ 1,20] which use a multi-head structure, we employ a single-head network. This
makes our network a truly Ô¨Åxed capacity structure and renders the continual learning task more
challenging.
Method Accuracy (%)
Single Task SGD 98:480:05
Kirkpatrick et al. [8] 97.0
Zenke et al. [20] 97.2
Cheung et al. [1] 97.6
CLNP (ours) 98:420:04
Table 1: Comparison of test accuracy aver-
aged over 10 permuted MNIST tasks.Just as in Ref. [ 20], we do a grid search over the hy-
perparameters on the Ô¨Årst task using a heldout val-
idation set. For the remaining tasks, we settle on
learning rate of 0:002 andL1weight regularization
= 10 7;10 5;10 6respectively for the Ô¨Årst, second
and Ô¨Ånal layers. Finally, when sparsifying after train-
ing each task, we allow for graceful forgetting with a
small margin of m= 0:05%. We run the experiment
5 times and report the meand and standard deviation of
the test accuracy of the network in Tab. 1. With test
error within 0:05% of single task SGD training, CLNP
virtually eliminates catastrophic forgetting in this exam-
ple and noticeably outperforms previous methods while
employing a more restrictive Ô¨Åxed capacity single head
architecture.
First layer Second layer010203040Used layer capacity (%)
12345678910
Figure 3: The percentage of the hidden layer
neurons used for each task on the permuted
MNIST experiment.A nice feature of our sparsity based approach is that we
have a clear understanding of how much of the network
has been taken up by previous tasks and how much is
left free. We also have a good indication of how many
of the features of the previously learned tasks are being
reused. Fig. 3 shows what percentage of the neurons of
the hidden layer are used after each task averaged over
the 5 runs. We see that the number of used neurons of
the Ô¨Årst layer does not greatly increase after the Ô¨Årst task
is trained, implying a signiÔ¨Åcant amount of reusage of
the features of this layer. The number of used neurons in
the second layer grows linearly with each task. This is
as expected, since in a single-head structure none of the
neurons of the Ô¨Ånal hidden layer are reused in order to
prevent interference (see Fig. 2).
Finally note that after training all 10 tasks, the neurons of the two hidden layers of the network are
only 18% and 40% utilized, leaving a lot of capacity free for future tasks. In our experiments, we
can train a total of about 25 random permutation tasks with the same test accuracy of 98.4% before
the capacity of the Ô¨Ånal hidden layer is fully depleted. This is in contrast to previous work where
the average accuracy over all tasks continues to decrease as more tasks are trained (see for example
Fig. 4 in Ref. [20]).
3.2 Split CIFAR-10/CIFAR-100
In this experiment, we train a model sequentially, Ô¨Årst on CIFAR-10 (task 1) and then on CIFAR-100
split into 10 different tasks, each with 10 classes (tasks 2-11). We use two different models for this
experiment, a smaller multi-head network used in Ref. [ 20] and a wider single-head network for
demonstration purposes, respectively given in Tab. 2 and Tab. 3.
In order to provide a direct comparison to previous results on this dataset we adopt the model and
the training scheme of Ref. [ 20]. SpeciÔ¨Åcally, we train sequentially on only the Ô¨Årst 6 tasks of this
problem using Adam optimizer with learning rate 0.001. We choose L1weight regulator coefÔ¨Åcient
= 510 5. We also use two different graceful forgetting schemes deÔ¨Åned via validation accuracy
7

--- PAGE 8 ---
Task 1 Task 2 Task 3 Task 4 Task 5 Task 650556065707580Validation accuracy (%)
Zenke et al.
CLNP m=2%
CLNP m=4% + FTCLNP m=1%
CLNP m=1% (depleted)
From scratch(a) Validation accuracy comparison
Conv 1 Conv 2 Conv 3 Conv 4 Dense 1020406080100Used layer capacity (%)
123456 (b) Average network capacity usage per task
Figure 4: CIFAR-10 and split CIFAR-100 results on multi-head network.
acceptability margins of m= 1% andm= 2% . We perform the experiment 5 times and report the
validation accuracy and its standard deviation on the 6 tasks.
Layer Chan. Ker. Str. Pad. Dropout
3232input 3
Conv 1 32 33 1 1
Conv 2 32 33 1 1
MaxPool 33 2 1 0.25
Conv 3 64 33 1 1
Conv 4 64 33 1 1
MaxPool 33 2 1 0.25
Dense 1 512 0.5
Task 1: Dense 10
: Dense 10
Task 6: Dense 10
Table 2: Split CIFAR narrow multi-head
model. Convolutional layers and Dense 1 are
followed by ReLU activation.The results of the experiment are shown in Fig. 4a. We
see that again we outperform the previous results by a
noticeable margin. Note, however, that the more ambi-
tiousm= 1% scheme which only allowed for an initial
graceful forgetting of less than 1%, runs out of capacity
after the fourth task is trained. As mentioned in Sec. 2,
a multi-head network which runs out of capacity on the
last hidden layer is trained just like Ô¨Ånal layer transfer
learning, i.e. the previous weights are all Ô¨Åxed and a new
head is trained. We notice that after the model capacity
is depleted, the performance of the m= 1% scheme
plummets, showing the necessity for new neurons to be
trained in the core of the network. The more moderate
forgetting scheme m= 2% , however, maintains high
performance throughout all tasks and does not run out of
capacity until Ô¨Ånal task is trained.
Note that even though we outperform previous methods, the narrow structure of this network is
ill-suited for our purposes. An ideal network for our method would have an evenly over-parametrized
capacity at all layers of the structure but with this network, 95% of the parameters are concentrated in
the dense layer which follows the convolutions. Furthermore, the use of 3 dropout layers is not the
most conducive for structural sparsity. We therefore expect the network to Ô¨Åll up very quickly during
training. Fig. 4b shows the network capacity usage per task for the moderate m= 1% forgetting
scheme averaged over the 5 runs. Notice that the Ô¨Årst task alone almost Ô¨Ålls up the entirety of the Ô¨Årst
and second convolutional layer capacities, leaving little room for new convolutional channels to be
learned in future tasks. The fact that even with this undesirable structure we still outperform previous
methods is surprising and is an idication that a very large amount of transfer learning is taking place.
In what follows, we look at two variations on our approach to this problem: Ô¨Årst with a slightly more
advanced sparsiÔ¨Åcation scheme and second with a single-head network of much wider width.
CLNP + Ô¨Åne tuning. In sparsiÔ¨Åcation literature, following the pruning of the redundant weights
and neurons, it is common practice to Ô¨Åne-tune the remaining weights of the network and iterate on
this process until a desired sparsity is achieved. In this paper we have chosen to perform a single
iteration of this process and also omit the Ô¨Åne-tuning stage for the sake of simplicity and also so that
our results are directly comparable to previous work. We now look at the potential of compression
based continual learning approaches given a slightly more advanced variation of our sparsiÔ¨Åcation
scheme. Explicitly, after training on any task, we still perform a single iteration of neuron pruning
but this time with a larger graceful forgetting margin of m= 4% and then we Ô¨Åne-tune the remaining
weights of the network by retraining on the same task for 20 epochs with learning rate = 10 4and
L1weight regulator = 510 5. The results of this method are given in Fig. 4a under ‚ÄòCLNP m=
4% + FT‚Äô. We see that here there is virtually no catastrophic forgetting on the Ô¨Årst task (if anything
8

--- PAGE 9 ---
the model performs even better after pruning and retraining as has been reported in previous sparsity
literature [ 7,11]). The remaining tasks also get a signiÔ¨Åcant boost from this improved sparsiÔ¨Åcation
method.
Layer Chan. Ker. Str. Pad.
3232input 3
Conv 1 128 33 1 1
Conv 2 256 33 2 0
Conv 3 512 33 1 0
Conv 4 1024 33 2 1
Conv 5 2048 33 1 0
Conv 6 10 33 1 0
AvgPool
Table 3: Split CIFAR wide single-head
model. Convolutions 1-5 are followed by
BatchNorm then ReLU activation.Wide single-head network. To gain further insight
into the interplay of transfer learning and network width
in networks with activation based neuronal sparsity, we
also trained a much wider fully convolutional network
on the same task (Tab. 3). This time we use a single-head
structure making the task more challenging because of
the Ô¨Åxed total capacity of the network. We also use a
slightly different training scheme, employing a heldout
validation set comprised of 10% of the training samples
to Ô¨Ånd optimal hyperparameters for each task and for
early stopping. We run the experiment 5 times and report
the mean and standard deviation of the test accuracy after
all tasks have been trained.
The wider network in this experiment has the capacity to train 10 tasks (i.e. CIFAR-10 plus 9 out of
the 10 CIFAR-100 tasks) before depleting its Ô¨Ånal hidden layer. Unlike multi-head structures where a
depleted Ô¨Ånal hidden layer leads to naive transfer learning on new tasks, for single-head structures, a
depleted Ô¨Ånal hidden layer simply has no free connections to the output and the network is entirely
Ô¨Åxed. Fig. 5a shows the results of this experiment.1For comparison we have also provid ed the
results of multi-task training and training each task individually from scratch. In order to not bias
our choices of graceful forgetting margins, the multi-task and individual training computations were
performed only after CLNP results were completed. Details of the multi-task training is given in the
supplementary materials section.
Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10606570758085Test accuracy (%)
Simultaneous training
CLNP m=2%
CLNP m=2% (Depleted)
From scratch
(a) Test accuracy comparison
Conv 1 Conv 2 Conv 3 Conv 4 Conv 5020406080100Used layer capacity (%)
12345678910(b) Sample network capacity usage per task
Figure 5: CIFAR-10 and split CIFAR-100 results on wide single-head network.
The average neuron capacity usage per task is given by Fig. 5b. There are a number of interesting
features in this graph. First note that similar to the single-head MNIST usage graph, tasks number 1-7
each roughly takes up the same number of neurons of the Ô¨Ånal hidden layer. This is again as expected
since the neurons of this layer are connected to the output layer and are not reused in order to avoid
interference. By the time we get to task 8, more than 85% of the capacity of this layer is depleted.
This directly affects the performance of the network which can be seen in terms of reduced test
accuracy both in comparison to the results from multi-task training and individual training from
scratch.
Perhaps the most interesting observation in the training of the wide network is in the number of new
channels learned at each layer for each consecutive tasks. Notice that the Ô¨Årst convolutional layer
trains new channels only for task 1 and 2. The second and third convolutional layers, grow new
1The only comparable single-head experiment on this dataset that we are aware of was done by Cheung et al .
[1]who also use a single-head 6-layer convolutional network. Using superposition techniques they Ô¨Ånd that
the Ô¨Årst task suffers about a 10% catastrophic forgetting (from 72% down to63% after training 4 new
tasks, accuracy on other tasks not reported). In comparison our method suffers less than a 2% drop via graceful
forgetting and remains unchanged during the 9 subsequent tasks.
9

--- PAGE 10 ---
channels up to task 3 and task 5 respectively. The fourth layer keeps training new channels up to the
last task. The fact that the Ô¨Årst layer grows no new channels after the second task implies that the
features learned during the training of the Ô¨Årst two tasks are fully utilized and deemed sufÔ¨Åcient for
the training of the subsequent tasks. The fact that this sufÔ¨Åciency happens after training more tasks
for layers 2 and 3 is a veriÔ¨Åcation of the fact that features learned in lower layers are more general
and thus more transferable in comparison with the features of the higher layers which are known to
specialize [ 19]. This observation implies that models which hope to be effective at continual learning
need to be wider in the higher layers to accommodate this lack of transferability of the features at
these scales.
4 Conclusion
In this work we have introduced a simple and intuitive lifelong learning method which leverages
the over-paremetrization of neural networks to train new tasks in the inactive neurons/Ô¨Ålters of
the network without suffering any catastrophic forgetting in the previously trained tasks. We also
implemented a controlled way of graceful forgetting by sacriÔ¨Åcing a little bit of accuracy at the end
of the training of each task in order to regain network capacity for training new tasks. We showed
empirically that this method leads to noticeably improved results compared to previous approaches.
Our methodology also comes with simple diagnostics regarding the number of free neurons left for the
training of new tasks. Model capacity usage graphs are also informative regarding the transferability
and sufÔ¨Åciency of the features of different layers. Using such graphs, we can verify the notion that
the features learned in earlier layers are more transferable. We can also leverage these diagnostic
tools to pinpoint any layers that run out of capacity prematurely, and resolve thse bottlenecks in the
network by simply increasing the number of neurons in these layers when moving on to the next
task. In this way, our method can efÔ¨Åciently expand to accomodate more tasks and compensate for
sub-optimal network width choices.
It is important to note that our algorithm is crucially dependent on the sparsiÔ¨Åcation method used.
In this work we employed a neuron activity sparsiÔ¨Åcation scheme based on a single iteration neural
pruning. We also demonstrated that using a slightly more advanced sparsiÔ¨Åcation scheme, i.e. adding
a Ô¨Åne tuning step after pruning can lead to even better results. This shows that our results, regarding
the number of tasks that can be learned or the accuracy that can be kept in a Ô¨Åxed network structure,
can only improve with more efÔ¨Åcient sparsiÔ¨Åcation methods. We hope that this observation opens
the path for a new class of neuronal sparsity focused lifelong learning methods as an alternative to
current weight elasticity based approaches.
Acknowledgments
We would like to thank Kyle Cranmer and Johann Brehmer for interesting discussions and input. SG
is supported by the James Arthur Postdoctoral Fellowship. MK is supported by the US Department of
Energy (DOE) under grant DE-AC02-76SF00515 and by the SLAC Panofsky Fellowship. This work
was partly supported by NVidia (Project: "NVIDIA - NYU Autonomous Driving Collaboration").
A Multi-task training on CIFAR-10/100
Multi-task learning, the simultaneous training of multiple tasks at the same time, is generally a
complicated problem. Extra care has to be taken especially when the different tasks have different
difÔ¨Åculties or different dataset sizes. In the mixed CIFAR-10/100 problem, the dataset for the Ô¨Årst
task is ten times larger than the other tasks. An important practical choice which affects the relative
performance of the tasks is how to perform minibatch training. In particular during one epoch of
training, if we draw from all tasks indiscriminately, the samples of the smaller datasets run out when
only 10% of the larger dataset is seen. In this case we can either start a new epoch i.e. cut the larger
dataset short or we can keep training on the larger dataset until it runs out. In practice, this choice
amounts to either sacriÔ¨Åcing the performance of the Ô¨Årst task in favor of the other tasks or vice versa.
For the purposes of comparison to lifelong learning, we take a compromise approach. When training
all tasks at the same time, we take each batch to comprise of 50 samples from task 1 and 10 samples
from each of the other tasks, the total batch size adding to 140. We then start a new epoch when the
the smaller datasets run out. In this way, a new epoch is started when only 50% of the large dataset
10

--- PAGE 11 ---
of task 1 is seen. Note that the relative number of samples in each task biases the network towards
one task or another, therefore picking an even higher number of samples of the Ô¨Årst task would again
lead to sacriÔ¨Åcing the accuracy on the other tasks. We made the choice of the 5 to 1 ratio purely as a
middle ground. It is possible that other choices can lead to better overall performance.
In order to adapt the single-head model of Tab. 3 for multi-task training, we partition the neurons of
the Ô¨Ånal hidden layer into 10 equal parts which form the ‚Äúheads‚Äù of the 10 different tasks. We chose
to train only on 10 out of the 11 tasks in order to provide a fair comparison since our continuous
learning algorithm depletes the network after 10 tasks. We train using 120 epochs using Adam
optimizer with learning rate 0.001 and learning rate schedule with milestones at 50 and 90 epochs
and= 0:05. We use the same heldout validation set as continuous learning and individual learning
for early stopping. We run the training 5 times and report the mean and standard deviation of the test
accuracy in Fig. 5a.
References
[1]Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. 2019.
Superposition of many models into one. (Feb 2019). arXiv:cs.LG/1902.05522
[2]C. Fernando, D. Banarse, C. Blundell, Y . Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra.
2017. PathNet: Evolution Channels Gradient Descent in Super Neural Networks. (Jan. 2017).
arXiv:1701.08734
[3]Robert M. French. 1991. Using Semi-Distributed Representations to Overcome Catastrophic
Forgetting in Connectionist Networks.
[4]Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The State of Sparsity in Deep Neural
Networks. (Feb 2019). arXiv:cs.LG/1902.09574
[5]Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectiÔ¨Åer neural networks.
InProceedings of the fourteenth international conference on artiÔ¨Åcial intelligence and statistics .
315‚Äì323.
[6]Lu Hou and James T. Kwok. 2018. Power Law in SparsiÔ¨Åed Deep Neural Networks. CoRR
(2018). arXiv:1805.01891 http://arxiv.org/abs/1805.01891
[7]Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. 2016. Network Trimming: A
Data-Driven Neuron Pruning Approach towards EfÔ¨Åcient Deep Architectures. (Jul 2016).
arXiv:cs.NE/1607.03250
[8]J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Mi-
lan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran,
and R. Hadsell. 2016. Overcoming catastrophic forgetting in neural networks. (Dec. 2016).
arXiv:cs.LG/1612.00796
[9]Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2009. CIFAR-10 (Canadian Institute for
Advanced Research). (2009). http://www.cs.toronto.edu/~kriz/cifar.html
[10] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/
[11] Yann LeCun, John S. Denker, and Sara A. Solla. 1989. Optimal Brain Damage. In NIPS .
[12] Jeongtae Lee, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. 2017. Lifelong Learning with
Dynamically Expandable Networks. CoRR (2017). arXiv:1708.01547 http://arxiv.org/
abs/1708.01547
[13] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. 2017. ThiNet: A Filter Level Pruning Method for
Deep Neural Network Compression. (Jul 2017). arXiv:cs.CV/1707.06342
[14] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. 2018.
Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks.
CoRR abs/1805.12076 (2018). arXiv:1805.12076 http://arxiv.org/abs/1805.12076
11

--- PAGE 12 ---
[15] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pas-
canu, and R. Hadsell. 2016. Progressive Neural Networks. (June 2016). arXiv:cs.LG/1606.04671
[16] S. Saxena and J. Verbeek. 2016. Convolutional Neural Fabrics. (June 2016).
arXiv:cs.CV/1606.02492
[17] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. 2017.
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. (Jan.
2017). arXiv:cs.LG/1701.06538
[18] Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2015. Sparsifying Neural Network Connections
for Face Recognition. CoRR abs/1512.01891 (2015). arXiv:1512.01891 http://arxiv.org/
abs/1512.01891
[19] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable
are features in deep neural networks? In Advances in Neural Information Processing
Systems 27 , Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-
berger (Eds.). Curran Associates, Inc., 3320‚Äì3328. http://papers.nips.cc/paper/
5347-how-transferable-are-features-in-deep-neural-networks.pdf
[20] F. Zenke, B. Poole, and S. Ganguli. 2017. Continual learning with intelligent synapses. Pro-
ceedings of International Conference on Machine Learning (ICML) (2017).
12
