# 2106.09563.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2106.09563.pdf
# File size: 916008 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
ONANYTIME LEARNING AT MACROSCALE
Lucas Caccia
McGill University, Mila
Facebook AI ResearchJing Xu
Facebook AI ResearchMyle Ott
Facebook AI Research
Marc’Aurelio Ranzatoy
Facebook AI ResearchLudovic Denoyerz
Facebook AI Research
ABSTRACT
In many practical applications of machine learning data arrives sequentially over time in large chunks.
Practitioners have then to decide how to allocate their computational budget in order to obtain the
best performance at any point in time. Online learning theory for convex optimization suggests that
the best strategy is to use data as soon as it arrives. However, this might not be the best strategy when
using deep non-linear networks, particularly when these perform multiple passes over each chunk
of data rendering the overall distribution non i.i.d.. In this paper, we formalize this learning setting
in the simplest scenario in which each data chunk is drawn from the same underlying distribution,
and make a ﬁrst attempt at empirically answering the following questions: How long should the
learner wait before training on the newly arrived chunks? What architecture should the learner adopt?
Should the learner increase capacity over time as more data is observed? We probe this learning
setting using convolutional neural networks trained on classic computer vision benchmarks as well
as a large transformer model trained on a large-scale language modeling task. Code is available at
www.github.com/facebookresearch/ALMA .
1 I NTRODUCTION
In many practical applications of machine learning, data is not static but arrives sequentially in large chunks (or mega-
batches). For instance, deployed language modeling systems need to be updated every few months to accommodate
new snapshots of the Common Crawl dataset1. Similarly, visual object recognition systems need to be updated as new
labeled data is gathered thanks to users interacting with the system. Moreover, as computing clusters are equipped with
more memory and compute, machine learning practitioners would like to train bigger and bigger models on the ever
increasing amount of data, since bigger models are often more accurate. In this setting, they face a dilemma: How to
maximize the performance of the system at any point in time while satisfying a certain computational budget?
This question has certainly been studied before, most notably in the online learning literature (Cesa-Bianchi & Lugosi,
2006). For instance, in a contextual bandit setting the learner observes one example at the time and receives a reward
after making a prediction. Of course, this can be extended to the case where the input is not just a single example but a
set of examples (hereinafter referred to as mega-batch).
While prior works on online learning set a sound theoretical framework, there are some subtle issues that make it not
quite applicable to the practical setting described above. First, computation is seldom explicitly taken into account,
while in practice algorithms that are too computationally intensive cannot be considered at scale. Second, the vast
majority of these works assumes linearity of predictors and convexity of optimization problems, whereby the order of
examples does not change the optimum solution. Instead, in many practical applications (like language modeling) we
are interested in using deep neural networks which are highly non-linear and which map to non-convex optimization
problems. The lack of linearity hinders theoretical analysis, and it has profound practical implications. For instance,
according to online learning theory the best case scenario is achieved when there are no delays (Joulani et al., 2016;
* Authors contributed equally
yNow at DeepMind
zNow at Ubisoft
1https://commoncrawl.org/the-data/
1arXiv:2106.09563v5  [cs.LG]  2 Aug 2022

--- PAGE 2 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Online Learning  
[linear]
∞    ALMA  
     [non-linear]Stationarity
Waiting  
TimeConcept Drift  
[linear]Supervised  
Continual Learning  
[non-linear]
Batch Supervised  
[non-linear]
1  
[lots of small chunks]Medium  
[few large chunks] [1 huge chunk]  No Drift With Drift
Anytime Learning  
[non-linear]
0 20 40 60 80 100
Time
(ticks correspond to training data chunks)0.20.40.60.8Error rate
Performance tradeoff when waiting for more data
T ardy Large-Scale Tuning
Fine-Tuning on Every 10 chunks
Fine-Tuning on Every chunk
Figure 1: Left: ALMA compared to other learning frameworks. In ALMA, mega-batches of data are drawn from the same
distribution (no drift) and arrive sequentially, but the learner can decide how long to wait before training on them. In the limit, if the
learner waits till the end of the stream then learning reduces to standard batch supervised learning. Right: Examples of CIFAR 10
learning curves varying how long to wait before updating the model. Waiting for a small number of mega-batches before updating
the parameters results in lower anytime error rate (smaller area under the learning curve).
Flaspohler et al., 2021), meaning that examples and their error signal are best to be consumed right away without any
staleness in the model parameters. To use the language of the practitioner training a language model, this means that
according to convex online learning the best strategy is to train one mega-batch at the time. This however might not be
a good strategy.
Consider what would happen if the deep neural network does multiple passes over each mega-batch before processing
the next, and compare its performance to the one of a learner that waits for all the mega-batches to arrive before shufﬂing
all data and applying the same stochastic gradient descent optimization algorithm as shown on the right part of Fig. 1.
The latter setting is the standard procedure used in supervised learning (green curve): the learning algorithm optimizes
a ﬁxed objective (i.e the empirical risk over the entire training dataset) that is known to produce good predictors. While
this predictor obtains the best ﬁnal performance, it also attains the worst anytime performance since its predictions
were random throughout the learning experience. In the former setting, by updating after each new mega-batch (purple
curve), we can expect to maintain a good predictor all along the training experience, overcoming the problem described
previously. However in this case, the learner is facing a changing learning objective, since each new mega-batch
deﬁnes a slightly different empirical risk (Jothimurugesan et al., 2018). While we can expect this effect to be negligible
when using linear models which eventually will converge to the same global optimum when all mega-batches are
available, this is not the case when using non-linear predictors like deep neural networks. In that case, the sequence
of optimization problems generated by the sequence of mega-batches may lead the learner to a completely different
(local) optimum than the supervised learning setting, and thus to a completely different predictor. There is thus an open
question about how different models behave when performing sequential learning over a stream of mega-batches.
In this paper, we empirically analyze several deep learning models (§4) under the assumption that data comes as a
sequence of mega-batches, all drawn from the same distribution for simplicity. Since we are interested in models
that attain good performance at any point in time and since we evaluate only after learning on each mega-batch but
not during the learning of each individual mega-batch, we dub this learning setting Anytime Learning at MAcroscale
(ALMA) (§3).
Through extensive empirical analysis (§5) we provide supporting evidence that waiting for a few mega-batches before
updating the model is often the best strategy, although how long to wait depends on several factors such as the time
horizon and model size relative to the amount of data in each mega-batch. Second, bigger models are more statistically
efﬁcient and generalize better. Third, none of the approaches we tried for growing the architecture were more effective
than simpler alternatives which used ﬁxed architectures, like ensembling. Overall, this study provides clear directions
of future research, and also a platform for benchmarking new approaches against well tuned baselines (code available in
supplementary material).
2 R ELATED WORK
ALMA relates to several other learning frameworks as illustrated on the left of Figure 1. i) It shares the same
assumptions of classical batch supervised learning (Vapnik, 1998) at the level of each mega-batch. However, it overall
violates the assumptions of i.i.d. observations, because data points come in a stream of mega-batches and because
the learner typically makes several passes over each mega-batch. Moreover, in ALMA the learner can choose how
long to wait before training. In this sense, batch supervised learning can be thought of as an extreme case of ALMA
(single mega-batch because learner waited till the end of the stream to train). ii) As mentioned in the previous section,
2

--- PAGE 3 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
ALMA relates to online learning (Bottou, 1998) because data comes sequentially and because in both cases we measure
performance in terms of regret (although in §3.1 our cumulative error lacks a reference oracle since this is not known in
our setting). However, in ALMA we are also explicit about the computational budget used by the model and aim at
striking a good trade-off between regret and computational cost. In our current work, we restrict ALMA to stationary
distributions, while online learning is more general and encompasses also non-stationary distributions. Finally and
most importantly, in ALMA we focus on non-linear predictors while typical literature on online learning considers
linear predictors. iii) Similarly, ALMA relates to concept drift (Lu et al., 2018) because of the sequential nature of
the observations. However, literature on concept drift often focuses on linear predictors. iv) ALMA can be seen as
a degenerate case of supervised continual learning, where the task distribution is stationary. However, in supervised
continual learning there is often a focus on attaining a predictor that represents the entire distribution of tasks by the end
of learning, while in ALMA we measure cumulative error like in prequential learning. v) ALMA relates more broadly to
transfer learning (Pan & Yang, 2010), as the problem of adapting to a new batch of data can be interpreted as leveraging
knowledge acquired on previous batches to more effciently learn from the new batch of data. vi) Finally, ALMA relates
to anytime learning (Grefenstette & Ramsey, 1992; Ramsey & Grefenstette, 1994), which has been recently applied to
compare various autoML frameworks (Liu et al., 2020). However, unlike traditional anytime learning, in this work
we are not interested in assessing the anytime learning ability at the level of each mega-batch, but only at a coarser
granularity, at the level of the entire stream of mega-batches. Lastly, we note that while anytime learning operates in a
similar setting as online learning (see Fig. 1), it is often used with non-linear predictors in a supervised learning setting.
To the best of our knowledge, the most relevant prior work is by Sahoo et al. (2018) which considers a setting similar to
ours, except that their stream is composed by individual examples and in their setting there is no concept of waiting
time nor revisiting data points several times. However, they also benchmark against methods that increase capacity over
time, although their analysis was limited to fully connected networks.
3 L EARNING SETTING
In anytime learning at macroscale (ALMA), we assume that there exists an underlying data distribution p(x;y)with
inputx2RDand desired label y2f1;:::;Cg. For the sake of simplicity of exposition, in this work we restrict
ourselves to classiﬁcation problems, but similar arguments can be made for regression, for instance. The key property
of ALMA is that data is presented to the learner as a stream SBofBconsecutive batches of examples. Let Dibe a
collection of N0i.i.d. samples randomly drawn from p(x;y), fori2f1;:::;Bg. The stream is then deﬁned as
the ordered sequence SB=fD1;:::;DBg. We refer to each dataset Diasmega-batch , as it is composed by a large
number of examples.
Typically a learner m:RD!f 1;:::;Cgupdates its parameters by processing a mini-batch ofnNexamples
at the time from each mega-batch Diin such a way to minimize its objective function. Since the data is observed
as a stream of mega-batches, the learner cannot have access to future mega-batches, and cross-validation of model
hyper-parameters can only be performed using a subset of the current mega-batch. In other words, the learner can only
do one pass over the stream. However, the learner typically does multiple passes over the current mega-batch if this
improves its generalization ability. In fact, the learner might make several passes over the current and some previous
mega-batches, although replaying too much might eventually deplete its computational budget.
Either way, since the learner makes several passes over each mega-batch, the overall data distribution observed by the
learner by the end of the stream is not i.i.d., even though mega-batches are drawn from the same underlying distribution
p(x;y)and samples drawn from each mega-batch are i.i.d.. For instance, in the limit case where each mega-batch
consists of a single example from a set of nexamples and a learner performing kpasses over each mega-batch, the
stream will consist of a sequence of examples (in a certain order) each replicated ktimes, which is different from
drawing uniformly at random knexamples from the original set of nexamples. This implies a trade-off between
ﬁtting the current data well versus generalizing well by the end of the stream.
In ALMA, the learner has an additional hyper-parameter compared to other learning frameworks: It can decide how long
to wait before updating its parameters. We measure such waiting time in terms of number of consecutive mega-batches.
For instance, a model with a waiting time equal to k, aggregates kconsecutive mega-batches before updating its
parameters. This will sacriﬁce a bit its performance during the waiting period, but might ultimately yield better
generalization since the model can better shufﬂe the data and get closer to the ideal i.i.d. data distribution required by
stochastic gradient descent optimization.
3

--- PAGE 4 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Algorithm 1 Training in the ALMA setting
1:procedure TRAIN (m;w , replay, grow) .m is the model, wis the waiting time
2:t 1
3:D ;
4: whilet<B do .For each stage
5: ifreplay then .Acquirewmega-batches
6:D D[D t[:::[Dt+w 1
7: else
8:D D t[:::[Dt+w 1
9:t t+w
10: ifgrow then
11: m:grow () .Grow the model if the model is a growing model
12:m:train (D) .Fine-tune or retrain from scratch mon the collected dataset
3.1 M ETRICS
We evaluate learners in the ALMA setting across three axes, namely: error rate, memory and computation. Let tbe the
time at which the t-th mega-batch arrives; this data can be used by the model to update its parameters or it is simply
aggregated to previous mega-batches for later use.
We compute the error rate of model mat timet(after the arrival and potential update over the t-th mega-batch) and
compute the area under the curve obtained varying tfrom 0till the total number of mega-batches B; the resulting
cumulative error rate (CER) is:
CER =BX
t=11
jDTsjX
(x;y)2DTsjm(x;t)6=yj (1)
wherem(x;t)is the model at time tequipped with parameters t,DTsis the test set (common for all mega-batches in
the stream),jDTsjis the number of examples in the test set, and jm(x;t)6=yjis one if the model prediction does not
match the ground truth label and zero otherwise. The outer sum computes the discrete integral of the error rate over
time. CER is going to be small only when the error rate is small throughout the entire stream. CER is instead large for a
tardy model that waits till the very last mega-batch to update the model, even though eventually this may obtain a very
low ﬁnal error rate.
Similarly, we compute the cumulative memory usage and compute as:
Mem =BX
t=0jtj;Comp =BX
t=0O(m(;t)) (2)
wherejtjis the number of free parameters of the model at time t, andO(m(;t))is the number of ﬂops used by the
model to process the t-th mega-batch.
Notice that the above metrics are measured by the environment as training progresses, and will be used in our empirical
assessment (§5). However, the learner does not have access to the test set. The learner has only access to the validation
set of the current mega-batch, and can only use that to select its own hyper-parameters.
4 L EARNING ALGORITHMS
In this section, we describe the methods we tested in the ALMA setting. They generally follow the learning procedure
shown in Algorithm 1. At a high level, we consider two families of models, those with a monolithic architecture and
those with a modular architecture (e.g. ensembling). The latter are amenable to grow over time by adding new modules
to the existing set. We will start by describing ﬁxed architectures (§4.1) and then conclude with growing architectures
(§4.2). We also evaluate models in the setting where they can replay previous mega-batches.
4.1 F IXED ARCHITECTURES
The ﬁrst family of methods trains models with a ﬁxed architecture. These models are sequentially trained over new
mega-batches and exhibit a ﬁxed memory footprint. We consider three models:
4

--- PAGE 5 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Single Model ( SM): This is a standard multi-layer neural network (e.g., fully connected neural network or trans-
former) trained by stochastic gradient descent and initialized from the parameters of the model trained on the previous
mega-batch, unless otherwise speciﬁed.
Ensemble of Models ( Ens): The second approach is the simplest modular approach, consisting of an ensemble of
Nneural networks with the same architecture but different random initialization seed, each being trained independently
on the same data. The output of the overall model at test time is the average probability distribution produced by each
component2. The advantage of Ensis that training and inference can be trivially parallelized, enabling to scale up model
parameters very easily. The disadvantange is that inference requires Ntimes more compute than what is required by
each component.
Uniform Mixture of Models ( UMix ): A potential drawback of Ensis that evaluation and training are inconsistent,
meaning that training and testing use different model predictors. UMix addresses this by training a model whose
prediction is the average (in logit space) of the predictions produced by Nnetworks. While this requires synchronization
during training, now both training and evaluation use the same model.
4.2 G ROWING ARCHITECTURES
In the previous section, the number of parameters and the architecture of the model are ﬁxed throughout the model’s
lifetime. However, as more data is observed, it is interesting to consider dynamic architectures that grow over time,
because these may save compute and memory during the earlier stages of learning while providing more predictive
power during the later stages. We consider three growing approaches:
Growing Ensemble ( gEns ): Like the Ensmodel, gEns is also a combination of neural networks trained independently.
While Ensconsiders a ﬁxed number of networks that are, at each stage, trained over the new chunck of data, gEns
replaces this step by a growing step where knew neural networks are added. In our implementation, only these kneural
networks are trained over the new data, while the other neural networks (trained on previous mega-batches) are kept
ﬁxed. Therefore, when starting with a single component and until the next growing step, the cost of training gEns is
equal to SMfor the same model architecture. Unless otherwise speciﬁed, we use k= 1for the experiments in the paper.
Growing Mixture of Experts ( gMoE ):A hierarchical mixture of experts models (MoE) is an architecture where
at layerlthe output representation zlis:zl=Pk
j=1g(jjzl 1)h(zl 1jj), wheregis the gating or routing function
andh(jj)is thej-th expert. Compared to Ens, MoE has exponentially many more components albeit with a lot of
parameter sharing. Another advantage is that when selecting only one (or a few) experts, the computational cost is
independent of the number of experts, assuming the cost of gating is negligible compared to the cost or executing the
experts. The main issue is that MoE are notoriously harder to train (Eigen et al., 2014; Denoyer & Gallinari, 2015;
Lepikhin et al., 2020). In this work, we consider a growing version of MoE, which we denote with gMoE , whereby
experts are added gradually over time. This has a tree structured gating function where leaves correspond to experts.
At each layer, we calculate each expert’s contribution to the total loss by summing the losses of the examples routed
through that expert. We then "split" the expert responsible for the largest contribution to the loss. The split is performed
by adding an expert with the same parameters, and turning the corresponding leaf node of the gate into a binary internal
node with a child leaf for the old and new expert. This process guarantees that right before and right after a growth step
the loss is the same. See Appendix A for further details.
Fireﬂy (Wu et al., 2020) ( FF):FFis a method which progressively grows neural networks, jointly optimizing both
the model architecture and parameters. Growth includes both a width expansion by adding new hidden units (or feature
maps) as well as a depth expansion by adding new layers. Importantly, this is an example of non-modular method
unlike EnsorgMoE , which is potentially more expressive but also more inefﬁcient at inference time because there is no
structured sparsity that can be leveraged to speed up computation.
5 E XPERIMENTS
In this section we ﬁrst describe how standard benchmarks can be repurposed for ALMA, we then provide the details of
the models we tested, and we ﬁnally conclude with an analysis of the results we obtained, aiming to understand which
method attains the best trade-off between time, accuracy, compute and memory usage.
2Classical bagging approaches and majority vote strategies have been also explored without signiﬁcant difference.
5

--- PAGE 6 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Datasets We consider a variety of datasets. The ﬁrst dataset is CIFAR 10 (Krizhevsky, 2009) that has a training set
with 50;000images of size 32x32pixels belonging to 10classes such as bird, car, horse, ship, truck, etc. The second
dataset is MNIST (LeCun et al., 1998), which consists of a training set with 60;000quasi-binary handwritten digits of
size 28x28pixels, and a test set with 10;000examples. The third dataset, used for our large-scale language modeling
evaluation, is a portion of the collection of English language text introduced in Liu et al. (2019), consisting of Books,
Wikipedia and Common Crawl. We consider 4 (large) mega-batches for training and one additional mega-batch for
evaluation, each consisting of approximately 440M words; we also hold out a validation set with approximately 0.5M
words of Common Crawl for model selection. We use a byte-pair encoding (BPE) (Sennrich et al., 2016) vocabulary
with 50;000units, following Radford et al. (2019). This dataset is fairly representative of what practitioners might face
when maintaining a deployed system with new data arriving every few months.
Given a dataset like any of the above, we construct a benchmark for ALMA evaluation as follows: 1) we randomly
partition the training set into Bmega-batches with equal number of training examples ( B= 100 for CIFAR,B= 500
for MNIST and B= 4for the text dataset), 2) from each mega-batch we extract 10% of the data to build the mega-batch
validation set (except for the large scale language modeling dataset where we use the provided validation set), and 3)
we create a learning experience by doing one pass over the sequence of mega-batches. For each mega-batch, the learner
can query as many mini-batches as desired. The learner can also decide not to train on the data of a mega-batch right
away but instead towait and accumulate data across a few consecutive mega-batches. While the learner observes data,
it is also tested on the test set. This is not used for validation purposes, but only for ﬁnal reporting as shown in §6.
Models We evaluate the six approaches presented in §4, and for each of them we consider various waiting times,
a version with and without replay, and at least four model sizes. For methods with expanding architectures, we try
different conﬁgurations of hyper-parameters controlling when to grow, and how much to grow. For simplicity, we
limit expansion phases to occur in between megabatches. Next, we describe in details the architecture used on each
dataset. Further experimental details to aide reproducibility are reported in Appendix B. On MNIST the backbone
architecture of SMis a three layer fully connected neural network with ReLU units. We considered various hidden units
size, ranging from 4 to 64 (which we refer to as [small] and [large], respectively), which let us simulate the regime of
big data relative to the size of the network and explore how to grow architectures without worrying about overﬁtting.
Similarly, the components of Ens,gEns andUMix areSMnetworks of the same size as stated above; gMoE also starts
off as SMand adds modules (at the ﬁrst two layers) that have the same size as the original layer of SM.
On CIFAR 10, the methods and notations are the same as in MNIST. The only difference is that the backbone architecture
is a scaled down version of a VGG19 convolutional neural network (Simonyan & Zisserman, 2015) as in (Wu et al.,
2020), where the number of intermediate feature maps is the same for each layer, ranging from 4to64. On this dataset,
we also consider FFstarting off from the same VGG19 backbone.
For the language modeling task SMis a Switch Transformer (Fedus et al., 2021), which is a hard mixture of experts
model with an additional load balancing loss term and hard capacity constraint applied during training to prevent uneven
expert utilization. Following Fedus et al. (2021), we ﬁx the weight of the balancing loss term to 0:01and use a capacity
factor of 1, ensuring relatively uniform expert utilization. We train the model using Adam (Kingma & Ba, 2015) and
tune the learning rate and dropout on the validation set. In the growing setting we copy the expert weights and gating
network weights corresponding to the top- kexperts incurring the largest loss, where kis typically between 2 and 4.
This growing procedure preserves a ﬂat mixture and adds multiple experts at the time. While this approach performs
slightly worse than the one described in §4.2, it is easier to implement at scale. We consider two model sizes: a base
model with 6 layers and model dimension of 512, for a total of 40M shared parameters and 6M additional parameters
per expert; and a large model with 12 layers and model dimension of 768, for a total of 96M shared parameters and 28M
additional parameters per expert. We use an input sequence length of 512 tokens and we do not use replay given the
large mega-batch sizes. During each mega-batch, we train all language models for exactly 120000 gradient steps (results
in Fig. 5) unless otherwise speciﬁed (e.g. Tab. 1). This makes it easier to compare models for the same computational
budget at the mega-batch level.
6 R ESULTS
6.1 V ISUAL RECOGNITION
Since conclusions are rather similar, we focus our analysis on the more challenging CIFAR 10 dataset, and report results
also on MNIST in Appendix C.
Smallest waiting time might not be optimal: We begin our analysis in the setting without replay, shown in Fig. 2.
We ﬁrst observe that an intermediate waiting time (in this case equal to 10) strikes the best trade-off between Cumulative
6

--- PAGE 7 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
101102103
Training TFLOPS500100015002000250030003500Cumulative Error RateSM
Ens
UMix
gEns
gMoE
FF
wait 100
wait 10
wait 2
105106107108109
Number of Params500100015002000250030003500
Cumulative Error Rate
Figure 2: CIFAR 10 results: Cumulative error rate versus cumulative ﬂops and number of parameters without replay. For the same
model type we vary the size of the backbone architecture and the waiting time.
0 20 40 60 80 100
number of mega-batches0.40.60.8Error Rate
Small Models (4 channels)
SM
Ens
wait 100
wait 10
wait 2
0 20 40 60 80 100
number of mega-batches0.20.40.60.8
Error Rate
Large Models (64 channels)
0 100 200 300 400 500
number of mega-batches0.20.40.60.8Error Rate
Small Models (H=4)
wait 10
wait 1
wait 10 
is better
wait 1  
is better
0 100 200 300 400 500
number of mega-batches0.20.40.60.8
Error Rate
Large Models (H=64)
Figure 3: Error rate over time of small models (left) and large models (right) on CIFAR 10 (top) and MNIST (bottom).
Error Rate (CER) and both training cost (left) and memory cost (right). As shown in Fig. 3-top, where the test error rate
is plotted as a function of the number of mega-batches received, greedy methods using waiting time equal to 2 achieve a
lower error rate only during the very beginning of the stream, but are outperformed later on. Tardy predictors waiting
for all 100 mega-batches before training obtain the best ﬁnal accuracy, but have no predictive capabilities throughout
the ﬁrst 99 mega-batches. Instead, methods with an intermediate waiting time (shown in orange) can quickly deliver a
reasonable predictor early in the stream, and obtain a ﬁnal error rate that is very close to the lower bound obtained by
tardy methods. Thus, a waiting time of 10 yields the lowest area under the curve (or CER) on CIFAR 10.
On MNIST however, an intermediate waiting time is best only for small models, as shown in Fig. 3-bottom. Very
greedy models do not converge as well in this setting, which leads to a signiﬁcant penalty in terms of CER. However,
bigger networks converge very fast in just a few megabatches, making smaller waiting times more desirable. Therefore,
the optimal waiting time depends on several factors such as the model size, the time horizon, how difﬁcult the task is
and how quickly the model learns. In such non-convex setting, it is certainly not necessarily true that learning on the
data as soon as it becomes available attains always the best trade-off between error rate and compute.
Larger models are more statistically efﬁcient: Second, we observe that bigger models ( SMandEns) not only
generalize better but they are also statistically more efﬁcient: on the small Ensobtained almost 40% error rate by the
end of its learning experience (Fig. 3-top left), which is worse than the error rate obtained by the large Ensjust after
having observed one tenth of the entire stream. The statistical efﬁciency of large models does not apply only to large
transformers (Kaplan et al., 2020), but also to fully connected (we obtained similar results on MNIST, see Fig. 3-bottom)
and convolutional models.
Growing does not improve: If we focus our attention on the three approaches with growing architectures, namely
gMoE ,gEns , and FF, we ﬁnd that there is no clear winner among them. When comparing across a ﬁxed computational
7

--- PAGE 8 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
budget (Fig. 2 left), gEns overall performs better than gMoE andFF. However, when we ﬁx the memory budget instead
(Fig. 2 right), gEns is, on average the worst performing method.
Next, we investigate the efﬁciency of growing, since in principle, we would expect that adapting model capacity with the
amount of data should strike a better trade-off between accuracy and memory/compute usage. For a ﬁxed computation
or memory budget, it is always better to start with a larger model , rather than growing it over time. Indeed, we ﬁnd
that on both graphs of Fig. 2, SMalmost always outperforms gMoE andFF, a trend that is especially visible for higher
budgets of TFLOPS and parameters. In other words, a gMoE orFFthat starts small and ﬁnishes big will typically be
outperformed by a SMmodel of average size.
Finally, Ensis more efﬁcient than gEns in terms of memory, but vice versa in terms of training compute. However,
should we look at the inference cost of both methods, we would ﬁnd that Ensoutperforms its growing counterpart,
whose inference cost grows over time while it is ﬁxed for Ens. Once again, the best strategy is to pick the largest
ﬁxed-capacity model for a given computational budget. Notice that these conclusions apply to the methods considered
in this study, and improving approaches that dynamically adapt their architecture over time is clearly a worthwhile
avenue of future research.
Operating point matters: We proceed by contrasting UMix andEns, where the former averages predictions during
training between different components, while the latter trains each component independently. In all our experiments
when working with smaller models, UMix has a slight edge on both memory and compute fronts; however as the size of
each component gets bigger the trend reverses, and Ensoutperfoms UMix . We surmise that smaller models suffer the
most from the inherent inefﬁciency of ensembling which forces each component to learn the same set of features. When
capacity is limited, it is better to coordinate learning among the components instead. Overall, this ﬁnding highlights
how conclusions about which model works best really depends on the operating point. Only when we consider the full
spectrum of model sizes, can we conclude which approach works best.
Ens strikes the best trade-off: More generally, Ens is the best performing method for larger models across all
our experiments, including the language models reported in §6.2. This is a remarkable ﬁnding given the simplicty of
the approach and how easy it is to parallelize their training process. Ensembling makes it very easy to increase model
capacity early on, and it is so far the best way to utilize compute at scale, a possible indication of the inefﬁciency of
training large models using alternative approaches, which highlights yet another worthwhile avenue of future research.
101102103104
Training TFLOPS10001250150017502000225025002750Cumulative Error Rate
Impact of replay for different methods on CIFAR
SM
Ens
UMix
gEns
gMoE
FF
wait 10
Figure 4: Impact of replay on the CIFAR-10 dataset with a
wait time of 10. For each method we show a line from the
result without replay (left) and with replay (right).Replaying past mega-batches does not improve: We
now consider the same approaches as before but with
models trained on all megabatches seen so far. Therefore,
at the very last training step, models are trained on the
entire dataset (concatenation of all megabatches). In Fig.
4 we report the results when the waiting time is equal to
10. In all cases, replaying data gives better results at the
expense of an increase in compute. Except for gEns , these
gains are roughly the same for all methods, as all seg-
ments are parallel to each other. gEns gains less as the last
component which is trained on the full dataset has dispro-
portionate inﬂuence in the model average which includes
components trained on fewer megabatches. However, this
last component essentially coincides with SMtrained on
the full dataset. Hence the two methods converge to the
same performance when using replay. We provide addi-
tional results with replay in Appendix C.2, which shows
that there are beneﬁts from replaying only at higher computational budgets where also the optimal waiting time reduces
to1.
More importantly, we observe that replay does not yield a signiﬁcantly better trade-off between CER and compute.
For the same computational budget, methods using replay attain similar CER of methods that do not use replay. Other
factors such as the size of the backbone architecture or the waiting time matter more.
6.2 L ANGUAGE MODELING EXPERIMENTS
For the large-scale language modeling experiments, we consider two model sizes (base and large, see §5), with an
inference cost per input of 42 and 126 GFLOPS, respectively. The number of experts is set to 4, 8 and 12 for SM, and it
does not affect the inference cost since only one expert per input is selected regardless of the total number of experts.
8

--- PAGE 9 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
5e6 1e7 2e7 4e7 8e7
Cumulative TFLOPS2022242628Average PPL
4
8
12SM
Ens
gEns
gMoE
small
large
1e8 2e8 4e8 8e8
Number of Params
4 6 8 10 12 14 16
Number of Experts
Figure 5: Language modeling trade-offs: average perplexity (PPL) versus cumulative compute, number of parameters and number
of experts. Numbers in red refer to the number of experts in the corresponding SMruns.
Due to the computational burden of these experiments (in total more than 200 GPU days), we limit our analysis to four
mega-batches. Nevertheless, this scale (of model and data) and type of application are rather representative of a typical
ALMA setting. Please refer to Tab. 2 in Appendix D for a comprehensive analysis, as here we will only highlight the
major ﬁndings.
The main results are presented in Fig. 5. Each line is a trajectory with four points, one for each mega-batch in the
stream, as we report average as opposed to cumulative perplexity. For a given model size and for a given computational
budget, there are three SMmodels, one for each number of experts we consider, namely 4, 8 and 12.
Larger models are more efﬁcient: In agreement with our results on computer vision tasks, we observe that bigger
models tend to generalize better and are more sample efﬁcient. For instance, the large model after a single mega-batch
outperforms all base models, including base models after four mega-batches which have seen four times more data.
This ﬁnding is consistent across all methods tried for this experiment.
Growing does not improve: Once again, there is no clear winner among growing methods. For larger models, gEns
outperforms gMoE for the same compute, and perform similarly for base models. However, for all model sizes, gMoE is
more memory efﬁcient, therefore the optimal approach among them will depend on both compute and memory budget.
More importantly, we observe that models with ﬁxed capacity are more compute and memory efﬁcient than models
that grow over time. Looking at the average perplexity as a function of the number of experts, we see that methods
which start with a small number of experts and later grow are outperformed by similar ﬁxed architecture which have an
intermediate number of experts. This highlights the importance of having more capacity at the start of training, rather
than at the end.
Ensembles perform the best: Third, Ensthrives in the larger capacity setting. Looking at the orange markers in
the graph, we see that for equal computation budget, Ensmethods outperform all other methods, which is consistent
with the computer vision results. In the base setting instead, versions of SM(see the lowest blue points) strike a better
tradeoff in both compute and memory.
Learning sequentially is harder: We argued initially that once the learner makes several passes over each megabatch,
the data distribution cannot be considered i.i.d. anymore, relative to the empirical distribution of the union of all
megabatches. It is however unclear how much this issue has a practical impact in the performance of the model. In
order to assess this we run one last experiment using our best performing approach, namely Ens. We compare a model
trained onkmega-batches sequentially with the same model trained all at once on the aggregation of the same k
mega-batches. Since both approaches have the same computation budget, the same architecture and are fed with the
same data, we can disentangle the effect of the non-i.i.d nature of the data in ALMA. The results shown in Tab. 1
conﬁrm that ALMA’s sequential (seq.) training is indeed more challenging. Across all four conﬁgurations, models
incur a drop in performance when compared to regular i.i.d training, and even more so when the model is larger. This
gap offers another opportunity of future research on ways to make sequential training more effective when using deep
non-linear neural networks.
7 C ONCLUSIONS
In the abstract we promised the reader to provide an empircal answer to several questions:
9

--- PAGE 10 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Method PPL k= 3, iid PPLk= 3, seq. PPL k= 4, iid PPLk= 4, seq.
Small Ens4@2 24.30 24.57 24.13 24.35
BigEns4@2 18.04 19.14 17.88 18.92
Table 1: Ablation on the effect of learning sequentially (seq.) as opposed to learning with fully i.i.d. data, for the same
amount of data and compute. The model is an ensemble with 2 components each of which with 4 experts per block.
1)How long should thelearner wait before training onthenewly arrived mega-batches? There is no single answer to
this question. We have seen that on CIFAR 10 but also on MNIST when using smaller architectures and when using
replay with smaller compute budgets, an intermediate waiting time strike the best trade-off. However, there is no known
formula for deriving the waiting time, as it depends on several factors such as the time horizon, the initial performance
of the model and how quickly a model learns, to name a few. The ﬁrm conclusion is that greedily updating the model
as soon as data becomes available, as advocated by literature on convex online learning, might not always be the best
strategy when using deep neural networks, In practice, also waiting too long, to the point that the learner does not even
have time to perform a single pass over the aggregated mega-batches, might be suboptimal.
2)What architecture should thelearner adopt? Our study indicates that, among all methods we tested, ensembling
strikes the best trade-off in general. Ensembling is simple and easily parallelizable, and it offers a straightforward way
to increase capacity. Starting off with a larger model, for instance via ensembling, is an excellent way to obtain good
anytime performance.
3)Should thelearner increase capacity over time asmore data isobserved? The answer is negative, currently. It is
better to start off with the largest architecture ﬁtting into memory and keeping that ﬁxed. A cynical interpretation of this
conclusion could make the reader believe that growing the architecture size should not be a topic of interest. However,
as data is added over time so is computation and memory. It is often the case that researchers working on large-scale
learning instantiate (rightly so) the biggest possible model to train on their task, but few months later they can manage
to launch even bigger models thanks to compute and engineering advances. How can the larger model leverage what has
been learned from the previously trained model? Is there a modeling choice that strikes a better trade-off than retraining
from scratch? More generally, what are good approaches to extract information from a new batch of data to integrate it
into an existing model? We believe these are great avenues of future research, and that our ALMA framework (learning
and evaluation protocol, codebase, baselines) provides a good abstraction of the practical setting, and a sound tool to
pursue such investigation.
8 R EPRODUCIBILITY STATEMENT
We have made several efforts to ensure that the results provided in the paper are fully reproducible. We ﬁrst provide
an easy-to-use codebase from which all the computer vision results in this paper are generated. In this codebase, one
can ﬁnd the exact hyperparameters used for each method in the provided conﬁgurations. We have attached a readme
to the code in order to guide users running our code. For the LM experiments, as stated in the appendix we use the
fairseq (Ott et al., 2019) and provide the required information to replicate our results.
9 A CKNOWLEDGEMENTS
We would like to thank Csaba Szepesvari for discussing how ALMA relates to online learning, Jörg Bornschein for
general discussion and for pointing out at missing experimental results, and Thang Doan for giving feedback on earlier
drafts.
REFERENCES
Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. CoRR, abs/1308.3432, 2013.
Léon Bottou. Online algorithms and stochastic approximations. In David Saad (ed.), Online Learning andNeural
Networks. Cambridge University Press, Cambridge, UK, 1998.
Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, andgames. Cambridge university press, 2006.
10

--- PAGE 11 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Ludovic Denoyer and Patrick Gallinari. Deep sequential neural networks. EWRL, 2015.
David Eigen, Ilya Sutskever, and Marc’Aurelio Ranzato. Learning factored representations in a deep mixture of experts.
ICLR, 2014.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple
and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Genevieve Flaspohler, Francesco Orabona, Judah Cohen, Soukayna Mouatadid, Miruna Oprescu, Paulo Orenstein, and
Lester Mackey. Online learning with optimism and delay. In International Conference onMachine Learning, 2021.
Jürgen Fritsch, Michael Finke, and Alex Waibel. Adaptively growing hierarchical mixtures of experts. In Advances in
Neural Information Processing Systems, 1996.
John J. Grefenstette and Connie Loggia Ramsey. Approach to anytime learning. In Proceedings oftheNinth
International Conference onMachine Learning, 1992.
Ellango Jothimurugesan, Ashraf Tahmasbi, Phillip B. Gibbons, and Srikanta Tirthapura. Variance-reduced stochastic
gradient descent on streaming data. In Neural Information Processing Systems, 2018.
Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Delay-tolerant online convex optimization: Uniﬁed analysis and
adaptive-gradient algorithms. In Association fortheAdvancement ofArtiﬁcial Intelligence, 2016.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann
LeCun (eds.), 3rdInternational Conference onLearning Representations, ICLR 2015, SanDiego, CA, USA, May
7-9,2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980 .
Alex Krizhevsky. Learning multiple layers of features from tiny images. University ofToronto, technical report , 2009.
Yann LeCun, Leon Bottou, and and Patrick Haffner Yoshua Bengio. Gradient-based learning applied to document
recognition. Proceedings oftheIEEE, 86(11):2278–2324, 1998.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding.
CoRR, abs/2006.16668, 2020.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 ,
2019.
Zhengying Liu, Zhen Xu, Shangeth Rajaa, Meysam Madadi, Julio C. S. Jacques Junior, Sergio Escalera, Adrien Pavao,
Sebastien Treguer, Wei-Wei Tu, and Isabelle Guyon. Towards automated deep learning: Analysis of the autodl
challenge series 2019. In Hugo Jair Escalante and Raia Hadsell (eds.), Proceedings oftheNeurIPS 2019 Competition
andDemonstration Track, volume 123 of Proceedings ofMachine Learning Research, pp. 242–252. PMLR, 2020.
Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept drift: A review.
IEEE Transactions onKnowledge andData Engineering, 31(12):2346–2363, 2018.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.
fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038, 2019.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. TKDE, 2010.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Connie Loggia Ramsey and John J. Grefenstette. Case-based anytime learning. In AAAI Technical Report WS-94-01 ,
1994.
Doyen Sahoo, Quang Pham, Jing Lu, and Steven C.H. Hoi. Online deep learning: Learning deep neural networks on
the ﬂy. In International Joint Conferences onArtiﬁcial Intelligence Organization, 2018.
11

--- PAGE 12 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In
Proceedings ofthe54th Annual Meeting oftheAssociation forComputational Linguistics (V olume 1:Long Papers) ,
pp. 1715–1725, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In
International Conference onLearning Representations, 2015.
Vladimir Vapnik. Statistical learning theory. Wiley New York, 1998.
Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Fireﬂy neural architecture descent: a general approach for growing
neural networks. In Advances inNeural Information Processing Systems, 2020.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
12

--- PAGE 13 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
APPENDIX
A G ROWING MIXTURES OF EXPERTS
Growing Mixture of Experts ( gMoE ):A mixture of expert ( MoE ) is a sequence of non-linear functions, each of
which is potentially a mixture of experts (omitting the dependence on parameters):
m(x) =fl(fl 1(:::f1(x):::));withfi(z) =kX
j=1gi(jjz)hi(zjj)
wheregiis the gating function at the i-th layer which outputs a categorical distribution over the number of experts, and
hi(jj)is thejexpert at layer i. The gating function can be “soft” in which case it outputs non-zero weights for each
expert via a softmax, or “hard” version in which case only one expert is selected through a multinomial sampling (and
learned through the straight-through estimator in this paper (Bengio et al., 2013)). At test time in the “hard” case, we
select the expert with the largest probability. The interest of mixtures of experts is they have a high expressivity, and
experts can be easily added to increase the capacity of the model. The gMoE model is the growing version where, at
each stage as illustrated in Fig. 6, new experts are added at each layer (Fritsch et al., 1996).
Two expertsThree experts after splitting
new expert modulenew gate
Figure 6: Illustration of a growth step in a tree structured mixture
of experts. A network is composed of several layers like this. The
blue squares are experts (e.g VGG layers). The red elements corre-
sponds to the gatings which, given an input compute a score for
each expert. When splitting an expert (right), the gating structure
is updated by creating a child gate, and an additional expert is
added to the mixture.The key design considerations are: when to grow, what
to grow and how to grow. Here, we will refer to our
default setting which favors simplicity, unless otherwise
speciﬁed.
A growth step is triggered at each stage, ensuring a linear
growth over time. We grow by adding one expert at each
layer, making sure that all experts within a layer have
the same architecture albeit with different parameters. In
order to grow, we look at which expert has associated the
largest cumulative loss; we call such expert the losing
expert. The cumulative loss is deﬁned as the sum of
the losses of examples on the validation set that have
been routed through a particular expert; each expert has
associated a cumulative loss value. The rationale is to
identify at each layer the expert responsible for the largest
contribution to the total loss.
To avoid drop in the loss function and to keep its differen-
tiability when splitting an expert, we propose a tree-based
approach where the losing expert is split into two experts
with exactly the same parameters as illustrated in Fig. 6: Two children leaves are derived and we instantiate a new
gating for the children which decides whether an input example routed to the old expert, should now go to the right or
left expert child. The parameters of the new gate are initialized at random while the parameters of the new experts are
exact copies of the ones of the losing expert that we split.
More formally, if sis the losing expert then the term gi(sjz)hi(zjs)is replaced by:
2X
k=1gi(sjz)gi(kjz;s)hi(zjs;k) (3)
wheregi(kjz;s)is the newly introduced gate, and zis the input of the gating and experts.
Over time, the gating function learns to partition its input space into a binary tree (if we start from a single expert), and
the gating value of an expert is the product of the gating probabilities on the path from root to the leaf expert. Both
the gating tree structure and the particular initialization scheme guarantee that the growth step is smooth and fully
differentiable, in particular, the loss before and after the growth step is the same.
If we consider each path in the MoE model to be a different model, then with Llayer ofkMoE components, there are
kLmany possible paths through the MoE model, hence the number of paths grows exponentially with the number of
layers. You can think of this as an ensemble with exponentially many components, but this is still tractable because
components share parameters.
13

--- PAGE 14 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
Algorithm 2 gMoE
1:k: number of mega-batches to aggregate
2:D=;
3:function TRAIN (Di,i)
4:D+=Di
5: ifimodk== 0 then
6: ExtractDV ALandDTRfromD
7: whilemis not converged: do
8: (x;y)DTR.In practice, sample mini-batches.
9: m.update(x;y)
10:D=;
11: m.grow(DV AL) .Growth step can be done at a different rate too.
12:function GROW (DV AL)
13: foreach layer in the network do
14: Letibe the losing expert on DV AL, i.e. the expert incurring the largest cumulative loss.
15: Turn corresponding gating output in an internal node and derive 2 gate children
16: Initialize the new experts by copying the parameters from the old parent expert.
17: Initialize the new gating between the two siblings at random.
B H YPER -PARAMETER SETTINGS
B.1 C OMPUTER VISION EXPERIMENTS
For each megabatch received, we keep 10% of the data to perform cross-validation. All experiments are run on a
single 16GB Quadro GP100 GPU. We apply data normalization for each dataset considered. A training minibatch size
of 128 is used. UMix andEnsmodels have N= 5in all experiments. for gEns , we train one model n= 1at every
mega-batch, so the total number of models depends on the amount of mega-batches. For Fireﬂy we use a growth rate of
0:25, meaning that at every growth phase, we add approximately a quarter of the initial number of parameters.
B.1.1 MNIST
Models are trained for 100 epochs, and we report results with soft gating. We use the AdaDelta (Zeiler (2012)) optimizer
with default learning rate of 1. We use a MLP with 2 hidden layers of varying width (e.g. 4,8 or 32 neurons).
B.1.2 CIFAR-10
Models are also trained for 100 epochs with a learning rate of 0.01. We use Stochastic Gradient Descent with momentum
value of 0.9 and weight decay of 110 4. During training, we apply random horizontal ﬂips and select random
image crops with padding of 4 pixels. For the architecture, we use the same reduced VGG with batch normalization as
prescribed in Wu et al. (2020). All layers are initialized with the same number of channels (e.g. 4, 8, or 32 channels).
For the Fireﬂy experiments, we keep all the Fireﬂy-speciﬁc hyperparameters to the default values suggested in the
author’s public codebase. We make one exception to this, namely we adapt the growth ratio to result in linear (rather
than exponential) growth.
B.2 L ANGUAGE MODELING EXPERIMENTS
All the language models are trained using fairseq (Ott et al., 2019) with a maximum of eight 32GB GPUs (NVIDIA
V100), optimized with Adam (Kingma & Ba, 2015) using 1= 0:9,2= 0:98,=1e-8. The learning rate is warmed
up over the ﬁrst several hundred updates (between 500 and 4000) and then linearly decayed to 0 over the remaining
updates, with a peak value tuned between 2e-4 and 5e-3. Models are trained up to 120,000 updates with local batch size
of 8 sequences per GPU, with gradient accumulation as needed to achieve a total batch size of 192 sequences; each
sequence has 512 tokens. We ﬁx the Switch Transformer balancing loss term to 0:01and use a capacity factor of 1,
following Fedus et al. (2021).
14

--- PAGE 15 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
C D ETAILED COMPUTER VISION RESULTS
C.1 MNIST
We show equivalent ﬁgures to the ones presented for CIFAR (e.g. Fig 2 and 3). We note for a given waiting time,
different models rank similarly as in the CIFAR results. The main difference with the other computer vision dataset is
on the optimal waiting time. As we saw in Fig. 3, on MNIST a predictor obtains good performance using very few
mega-batches, making small waiting time competitive. Nevertheless, we do see that in terms of ﬁnal error rate, a small
waiting time underperforms, especially for small models.
101
100101
Training TFLOPS20040060080010001200Cumulative Error RateSM
Ens
UMix
gEns
gMoE
wait 500
wait 50
wait 25
wait 10
wait 1
106107108109
Number of Params20040060080010001200
Cumulative Error Rate
0 100 200 300 400 500
number of mega-batches0.10.20.3Error Rate
Small Models (4 hidden units)
SM
Ens
wait 500
wait 25
wait 10
wait 1
0 100 200 300 400 500
number of mega-batches0.10.20.3
Error Rate
Large Models (64 hidden units)
Figure 7: (top) Cumulative error rate versus cumulative ﬂops and number of parameters without replay. For the same model type we
vary the size of the backbone architecture and the waiting time. (bottom) Anytime Error Rate for the same methods on MNIST
15

--- PAGE 16 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
C.2 CIFAR-10
For this dataset, we provide more results when using replay across a variety of methods and waiting times. We note
in this setting, as the computational budget increases, the optimal waiting time decreases. This is because as more
mega-batches is received, the training distribution gets closer to the ideal i.i.d scenario. It can therefore bypass the
optimization issues faced when training for multiple iterations on a small dataset. Again, we emphasize that this is not
the case when using a small waiting time and no replay.
101102103104
Training TFLOPS10001500200025003000Cumulative Error Rate
Impact of replay for SM on CIFAR
SM
wait 10
wait 2
101102103104
Training TFLOPS10001250150017502000225025002750Cumulative Error RateImpact of replay for UMix on CIFAR
UMix
wait 10
wait 2
101102103104
Training TFLOPS10001500200025003000Cumulative Error RateImpact of replay for Ens on CIFAR
Ens
wait 10
wait 2
101102103104
Training TFLOPS10001500200025003000Cumulative Error Rate
Impact of replay for gEns on CIFAR
gEns
wait 10
wait 2
101102103104
Training TFLOPS1000150020002500Cumulative Error RateImpact of replay for gMoE on CIFAR
gMoE
wait 10
wait 2
101102103104
Training TFLOPS10001500200025003000Cumulative Error RateImpact of replay for FF on CIFAR
FF
wait 10
wait 2
Figure 8: Impact of Replay across different methods and waiting times for CIFAR-10.
16

--- PAGE 17 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
D F ULL LM R ESULTS
Below we present the full quantitative results for our language modeling experiments.
Base model perplexity Large model perplexity
setting # experts jjt0t1t2t3 jjt0t1t2t3
SM_w1 4 65M 28.57 27.45 26.91 26.53 210M 22.47 21.62 20.84 20.54
SM_w1 8 91M 26.29 25.29 24.74 24.40 323M 21.52 20.38 19.64 19.22
SM_w1 12 116M 25.63 24.70 24.17 23.78 436M 21.63 20.26 19.44 18.98
SM_w3 8 91M * * 25.18 323M * * 19.29
SM_w3 (3x steps) 8 91M * * 24.21 323M * * 18.48
SM_w4 12 116M * * * 24.41 436M * * * 19.01
SM_w4 (4x steps) 12 116M * * * 22.87 436M * * * 17.70
Ens_w14@2 130M 26.20 25.12 24.57 24.35 420M 20.32 19.55 19.14 18.92
4@4 260M 25.03 24.03 23.45 23.29 840M 19.27 18.52 18.22 18.07
Ens_w3 4@2 130M * * 25.52 420M * * 19.11
Ens_w3 (3x steps) 4@2 130M * * 24.30 420M * * 18.04
Ens_w4 4@2 130M * * * 25.49 420M * * * 19.03
Ens_w4 (4x steps) 4@2 130M * * * 24.13 420M * * * 17.84
gEns _w14@1 65M 28.57 210M 22.47
4@2 130M 26.27 420M 20.25
4@3 195M 25.41 630M 19.49
4@4 260M 25.01 840M 19.18
gMoE _w14 65M 28.57 210M 22.47
6 78M 26.46 266M 21.22
8 91M 25.66 323M 20.39
12 116M 25.28 436M 20.15
Table 2: Large scale language modeling results. For EnsandgEns , 4@3 means 3 components in the ensemble, each of
which has 4 experts per block, for instance.
17

--- PAGE 18 ---
Published at 1st Conference on Lifelong Learning Agents, 2022
E E XTENDED FIGURE 1
In this section, we add runs with replay to Fig. 1. We note that runs with replay are not directly comparable to runs
without replay, because they have a higher computational cost. Indeed, runs that ﬁne-tune every 10 chunks cost 4.5x the
cost of non-replay runs, and runs ﬁne-tuning every chunk cost 122.5x .
0 20 40 60 80 100
Time
(ticks correspond to training data chunks)0.20.30.40.50.60.70.80.9Error rate
Performance tradeoff when waiting for more data
No Replay
T ardy Large-Scale Tuning
Fine-Tuning on Every 10 chunks
Fine-Tuning on Every chunk
With Replay
Fine-Tuning on Every 10 chunks
Fine-Tuning on Every chunk
0 20 40 60 80 100
Time
(ticks correspond to training data chunks)0.20.30.40.50.60.70.80.9Error rate
Performance tradeoff when waiting for more data
No Replay
T ardy Large-Scale Tuning
Fine-Tuning on Every 10 chunks
Fine-Tuning on Every chunk
With Replay
Fine-Tuning on Every 10 chunks
Fine-Tuning on Every chunk
Figure 9: Fixed architecture runs (top) and growing ensemble runs (bottom)
18
