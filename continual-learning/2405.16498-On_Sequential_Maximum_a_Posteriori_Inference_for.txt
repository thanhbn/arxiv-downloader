# 2405.16498.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2405.16498.pdf
# File size: 1026350 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
On Sequential Maximum a Posteriori Inference for
Continual Learning
Menghao Waiyan William Zhu
Tsinghua Shenzhen International Graduate School
Shenzhen, China
zhumh22@mails.tsinghua.edu.cnErcan Engin Kuruo ˘glu
Tsinghua Shenzhen International Graduate School
Shenzhen, China
kuruoglu@sz.tsinghua.edu.cn
Abstract —We formulate sequential maximum a posteriori
inference as a recursion of loss functions and reduce the problem
of continual learning to approximating the previous loss function.
We then propose two coreset-free methods: autodiff quadratic
consolidation, which uses an accurate and full quadratic ap-
proximation, and neural consolidation, which uses a neural
network approximation. These methods are not scalable with
respect to the neural network size, and we study them for
classification tasks in combination with a fixed pre-trained feature
extractor. We also introduce simple but challenging classical
task sequences based on Iris and Wine datasets. We find that
neural consolidation performs well in the classical task sequences,
where the input dimension is small, while autodiff quadratic
consolidation performs consistently well in image task sequences
with a fixed pre-trained feature extractor, achieving comparable
performance to joint maximum a posteriori training in many
cases.
Index Terms —Bayesian inference, class-incremental learning,
continual learning, domain-incremental learning, neural net-
works
I. I NTRODUCTION
When a neural network (including a generalized linear
model, which is essentially a neural network with no hidden
layers) is trained on a task and fine-tuned on a new task, it
loses predictive performance on the old task. This is known
as catastrophic forgetting [1] and can be prevented by training
jointly on all tasks, but the previous data may not be accessible
due to computational or privacy constraints. Thus, we would
like to learn from a sequence of tasks with limited or no access
to the previous data. This is known as continual learning or
incremental learning or lifelong learning.
For classification tasks, three types of continual learning
settings are commonly studied [2]:
1) Task-incremental learning, in which task IDs are pro-
vided and the classes change between tasks
2) Domain-incremental learning, in which task IDs are not
provided and the classes remain the same between tasks
but the input data distribution changes between tasks
3) Class-incremental learning, in which task IDs are not
provided and the classes change between tasks
For example, Split MNIST is a sequence of five tasks created
from the MNIST dataset, in which the first task consists of
images of zeros and ones, the second task consists of images
of twos and threes and so on. In the task-incremental setting,
the task IDs 1-5 are provided, and the neural network onlyhas to decide between two classes for each task ID. Typically,
a multi-headed neural network with five heads (one head for
each task) is used in this setting. In the domain-incremental
setting, the task is classification of even and odd digits without
access to the task ID. In the class-incremental setting, the task
is classification of all ten digits without access to the task ID.
In these settings, a single-headed neural network is typically
used.
Task-incremental learning has been criticized as task IDs
make the problem of continual learning easier [3]. In fact,
if there is only one class per task, then the task ID could
be used to make a perfect prediction. Moreover, in practice,
it is unlikely that task IDs are accessible. For example, in
Split MNIST, we would like to classify all ten digits in the
end rather than just tell which of the two digits in each task.
In accordance with the desiderata proposed in [3], we focus
on domain- and class-incremental learning with single-headed
neural networks on task sequences with more than two similar
tasks and no access to the previous data.
For a single-headed neural network with fixed architecture,
the learning problem can be formulated as Bayesian inference
on the neural network parameters. Then, sequential Bayesian
inference provides an eltextegant approach to continual learn-
ing. In particular, continual learning can be done by using the
previous posterior distribution as the current prior distribution.
If full Bayesian prediction is used, then the approach is known
as a Bayesian neural network. In our work, we focus on
maximum a posteriori (MAP) prediction, which uses only
the maximum point of the posterior distribution. By defining
the loss function as the negative log joint probability density
function (PDF), this leads to a recursive formulation of loss
functions, and the problem is reduced to approximating the
previous loss function.
When adult humans learn to recognize new objects, they
have already learned similar objects before, and they do not
need to continually learn low-level features such as edges,
corners and shapes. This suggests that the low-level layers of a
neural network should be fixed after learning on a similar task,
and then the neural network can be used as a feature extractor.
For example, a neural network pre-trained on handwritten
letters can be used as a feature extractor for continual learning
on handwritten digits.
Our goals are to investigate the continual learning perfor-arXiv:2405.16498v4  [cs.LG]  10 Mar 2025

--- PAGE 2 ---
mance of a full quadratic approximation and a neural network
approximation of the previous loss functions and the effect
of using a pre-trained feature extractor for sequential MAP
inference.
II. S EQUENTIAL MAXIMUM A POSTERIORI INFERENCE
We describe our probabilistic model for continual learning
and how sequential MAP inference based on it leads to a
recursion of loss functions. Then, we propose two methods for
approximating the previous loss function: autodiff quadratic
consolidation (AQC), which uses an accurate and full quadratic
approximation, and neural consolidation (NC), which uses
a neural network approximation. Finally, we discuss their
limitations.
A. Probabilistic Model
θ
y1
x1y2
x2. . .
. . .yt
xt
Fig. 1: Bayesian network for continual learning. θis the
collection of parameters of the neural network, x1:tare the
inputs and y1:tare the outputs.
Letθbe the collection of parameters of the neural network
andx1:tandy1:tbe the inputs and outputs from time 1to
t, respectively. x1:tare assumed to be independent, and y1:t
are assumed to be conditionally independent given θandx1:t.
These assumptions are depicted in Fig. 1.
(x,y)1:tare not necessarily identically distributed. How-
ever, it is assumed that tasks are similar, i.e. the form of
the likelihood function lt(yt|θ, xt)is the same for all tasks.
For example, in multi-class classification, it is the categorical
likelihood function for all tasks.
After observing (x,y)1:t= (x, y)1:tat time t, the posterior
PDF is
pt(θ|x1:t, y1:t) =1
ztpt−1(θ|x1:t−1, y1:t−1)lt(yt|θ, xt)(1)
where zt=R
Θpt−1(θ|x1:t−1, y1:t−1)lt(yt|θ, xt)dθis a nor-
malization term which does not depend on θ. MAP prediction
uses the maximum of the posterior PDF to make a prediction
f(x;θ∗
t), where fis the neural network function, xis the input
andθ∗
tis the MAP estimate at time t.
Since multiplying by a constant does not affect the min-
imum, the loss function Ltat time tcan be defined as
the negative log joint PDF −lnjt(θ, y1:t|x1:t)at time t.
Then, the minimum of the loss function is equivalent to theMAP estimate, and we have a recursion of loss functions for
t= 1,2, . . .:
Lt(θ) =Lt−1(θ) +lt(θ) (2)
where ltis the negative log likelihood (NLL) at time t. For
binary classification, where the likelihood is assumed to be
Bernoulli, ltis the binary or Bernoulli cross entropy, while
for multi-class classification, where the likelihood is assumed
to be categorical. ltis the categorical cross entropy. L0is the
negative log (un-normalized) prior at time 1, e.g.,1
2∥θ∥2for
the standard Gaussian prior.
In Eq. (2), Lt−1depends on the previous data (x,y)1:t−1
andltdepends on the current data (x,y)t. Forgetting happens
when we minimize only ltbut ignore Lt−1as in fine-tuning. In
joint MAP training, all the data are used, so Ltis effectively
minimized. If there is no access to the previous data, then
Lt−1must be approximated. We investigate two ways to
approximate it, namely quadratic approximation and neural
network approximation.
B. Autodiff Quadratic Consolidation
A quadratic approximation of Lt−1corresponds to a
Laplace approximation of the posterior PDF at time t−1.
The quadratic approximation is a second-order Taylor series
approximation around θ∗
t−1, where the gradient is zero, so the
linear term disappears. Moreover, the constant term does not
affect the PDF. Thus, the approximation consists of a single
quadratic term of the form1
2(θ−θ∗
t−1)TH(lt−1)(θ∗
t−1)(θ−
θ∗
t−1), where H(lt−1)(θ∗
t−1)is the Hessian matrix of the NLL
of task t−1atθ∗
t−1. [4] shows that successive quadratic
approximation results in addition of the Hessian matrices of
the NLL of the previous tasks at the corresponding minima.
Thus, the approximate loss function is
ˆLt(θ) =λ
2(θ−θ∗
t−1)T t−1X
i=0H(li)(θ∗
i)!
(θ−θ∗
t−1) +lt(θ)
(3)
where λis a positive real number introduced as a hyperpa-
rameter.
The Hessian matrix is the transpose of the Jacobian matrix
of the gradient: H(f)(x) = ( J(∇f)(x))Tfor any twice
differentiable point xof a function f. In most cases, the NLL
lt−1is twice continuously differentiable, so the Hessian matrix
at its minimum is symmetric positive definite. Then, it can be
implemented as J(∇lt−1)(θ∗
t−1), the Jacobian matrix of its
gradient at its minimum, by using automatic differentiation.
Since the Hessian operator is linear, the Hessian matrix of
the batch negative log likelihood is equal to the sum of the
Hessian matrices of the mini-batch negative log likelihood:
H(lt−1)(θ∗
t−1) =H
bX
j=1lt−1,j
(θ∗
t−1) =bX
j=1H(lt−1,j)(θ∗
t−1)
(4)
where lt−1,jis the mini-batch negative log likelihood of the
j-th mini-batch at time t−1.

--- PAGE 3 ---
The above trick allows working with a large dataset by
dividing it into small mini-batches. However, if the neural
network is large, i.e. θhas a large amount of parameters, then
the computation may become intractable.
The training for each task thus consists of three steps:
1) If it is the first task, then the loss function is set to
1
2∥θ∥2+l1(θ)assuming a standard Gaussian prior; oth-
erwise, it is updated as in Eq. (3) with the MAP estimate
θ∗
t−1and the Hessian matrix Ht−1=Pt−1
i=0H(li)(θ∗
i)
of the previous task.
2) Training is done on the loss function using mini-batch
gradient descent, and the regularization term is scaled by
dividing by the number of mini-batches in the dataset.
3) The Hessian matrix for the current task is computed
and added to that of the previous task Ht=Ht−1+
H(lt)(θ∗
t), and the current MAP estimate θ∗
tand Hessian
matrix Htare stored in order to be used to update the
next loss function.
This method is referred to as Autodiff Quadratic Consoli-
dation (AQC).
C. Neural Consolidation
A neural network approximation uses a consolidator neu-
ral network κwith parameters ϕ∗
t−1. The approximate loss
function is
ˆLt(θ) =λκ(θ;ϕ∗
t−1) +lt(θ) (5)
where λis a positive real number introduced as a hyperpa-
rameter and ϕ∗
t−1is the collection of trained parameters of
the consolidator neural network at time t−1.
The consolidator neural network is trained by minimizing
anL2-regularized Huber loss function to fit it to the previous
loss function with a sample generated uniformly within a ball
of radius raround θ∗
t−1at each gradient descent step. If n
points are generated, then the consolidator loss function is
Lt−1,κ(ϕ) =1
2β∥ϕ∥2
2+nX
i=1ht−1,i(ϕ) (6)
where βis a positive real number introduced as a hyperpa-
rameter and ht−1,i(ϕ)is the Huber loss function with respect
toˆLt−1(θi)andκ(θi;ϕ). If the dataset is very large, ˆLt−1can
be computed on the sample in mini-batches and added.
The training for each task thus consists of three steps:
1) If it is the first task, then the loss function is set
to1
2∥θ∥2+l1(θ)assuming a standard Gaussian prior;
otherwise, it is updated as in Eq. (5) with the parameters
ϕ∗
t−1of the consolidator neural network of the previous
task.
2) Training is done on the loss function using mini-batch
gradient descent, and the regularization term is scaled by
dividing by the number of mini-batches in the dataset.
3) The consolidator neural network is trained by perform-
ing gradient descent on Eq. (6) with a sample of npoints
ofθgenerated uniformly within a ball of radius raround
θ∗
tat each step as described above, and the parametersϕ∗
tof the consolidator neural network are stored in order
to be used to update the next loss function.
This method is referred to as Neural Consolidation (NC).
D. Limitations
As in all single-headed approaches, the total number of
classes must be known in advance. The main limitation of
AQC and NC is that they are not scalable with respect to
the neural network size although they can be used when the
datasets are large. This limitation may be overcome by using
a fixed feature extractor pre-trained on a similar task and
performing continual learning with one dense layer on the
features. Finally, both continual learning methods are sensitive
to hyperparameters, so a validation dataset sequence should be
used to perform hyperparameter tuning.
III. R ELATED WORK
There are several continual learning methods which are
based on sequential MAP inference and use quadratic approx-
imation of the previous loss function. Elastic weight consol-
idation (EWC) approximates the Hessian matrix by using a
diagonal approximation of the empirical Fisher information
matrix (eFIM) [5]. The original EWC adds a quadratic term to
the objective for every task. [6] proposes a corrected objective
with a single quadratic term for which the eFIM can be
cumulatively added. There is another variant called EWC++
[7], which performs a convex combination of the previous and
current eFIMs instead of adding them. Synaptic intelligence
(SI) performs a diagonal approximation of the Hessian matrix
by using the change in loss during gradient descent [8]. Online
structured Laplace approximation uses Kronecker factorization
to perform a block-diagonal approximation of the Hessian
matrix, in which the diagonal blocks of the matrix correspond
to a layer of the neural network [9].
Another class of methods that are not based on sequential
MAP inference but are based on sequential Bayesian infer-
ence is sequential variational inference. It approximates the
posterior distribution with a variational distribution, which
is a simple parametric distribution, typically a Gaussian or
a Gaussian mixture distribution, by minimizing an objective
called the variational free energy or the negative evidence
lower bound with respect to the parameters of the variational
distribution. It uses the whole posterior distribution rather than
a point from it to make predictions. Gaussian variational con-
tinual learning (G-VCL) [10] and Gaussian mixture variational
continual learning (GM-VCL) [11] approximate the posterior
distribution over the parameters with a Gaussian distribution
and a Gaussian mixture distribution, respectively. Gaussian
sequential function space variational inference (G-SFSVI) [12]
approximates the posterior distribution over the outputs (before
the final activation function) of a selected number of inputs
called inducing points with a Gaussian distribution.
Pre-training for initialization and pre-training for feature
extraction have both been empirically shown to improve
continual learning. In the former, the pre-trained parameters
are used as the initial parameters for continual learning [13],

--- PAGE 4 ---
[14]. In the latter, the pre-trained neural network is used as a
feature extractor [15]–[17].
We investigate the continual learning performance of AQC,
which is the most accurate form of quadratic approximation
of the previous loss function, and NC, which is a neural
network approximation of the previous loss function. In image
classification tasks, we use a fixed feature extractor pre-trained
on a similar task and perform continual learning with one
dense layer on the features.
IV. E XPERIMENTS
Experiments are performed on classical task sequences
as well as image task sequences. In each experiment, the
final average accuracy of AQC and NC are compared with
reference methods, sequential variational inference methods
and sequential MAP inference methods. Each task sequence
has a training dataset sequence, validation dataset sequence
and testing dataset sequence. The validation dataset sequence
is used to perform hyperparameter tuning via grid search.
Descriptions of data used and methods compared as well as
adiscussion of results are provided below, and more details are
provided in Appendix A.
A. Data
The task sequences for continual learning as well as the
tasks for pre-training are listed below. The classical task
sequences that we introduce here might seem simple, but they
are challenging task sequences for continual learning. In all
task sequences, CI indicates that it is for class-incremental
learning, while DI indicates that it is for domain-incremental
learning.
•Classical Task Sequences
– CI Split Iris : Iris is a task for classification of 3
species of flowers based on 4 features. It is split into
3 tasks for learning 1 species at a time. CI Split 2D
Iris is a task sequence for visualization derived from
it by selecting two features ”petal length” and ”petal
width”.
– CI Split Wine : Wine is a task for classification of 3
classes of wine based on 13 features. It is split into
3 tasks for learning 1 class at a time.
•Image task sequences
– CI Split MNIST : MNIST is a task for classification
of 10 classes of digits based on grayscale images of
handwritten digits. It is split into 5 tasks for learning
2 classes at a time.
– CI Split CIFAR-10 : CIFAR-10 is a task for clas-
sification of 10 classes of natural objects based on
images. It is split into 5 tasks for learning 2 classes
at a time.
– CI Split HAM-8 : HAM10000 is a task for clas-
sification of 8 classes of skin conditions based on
dermatoscopic images. It is renamed to HAM-8
based on the number of classes and split into 4 tasks
for learning 2 classes at a time.– DI Split MNIST : MNIST is split into 5 tasks with
2 classes at a time, but each task is of binary
classification of even and odd digits.
– DI Split CIFAR-8 : CIFAR-10 has 6 types of animal
and 4 types of vehicle, so 2 types of animal (”bird”
and ”frog”) are removed to make CIFAR-8, which is
then split into 4 tasks with 2 classes at a time, but
each task is of binary classification of vehicles and
animals.
– DI Split HAM-6 : HAM-8 has 4 types of benign skin
condition, 1 type of indeterminate skin condition and
3 types of malignant skin condition, so 1 type of
benign skin condition (”vascular lesion”) and 1 type
of indeterminate skin condition (”actinic keratosis”)
are removed to make HAM-6, which is then split
into 3 tasks with 2 classes at a time, but each task is
of binary classification of benign and malignant skin
conditions.
•Pre-training Tasks
– EMNIST Letters : EMNIST Letters is a task for
classification of 26 classes of letters based on
grayscale images of handwritten letters. It has no
classes in common with MNIST and is used for pre-
training for CI Split MNIST and DI Split MNIST.
– CIFAR-100 : CIFAR-100 is a task for classification
of 100 classes of natural objects based on images.
It has no classes in common with CIFAR-10 and is
used for pre-training for CI Split CIFAR-10 and DI
Split CIFAR-10.
– BCN-12 : BCN20000 is a task for classification of 8
classes of skin conditions based on dermatoscopic
images. It is renamed to BCN-12 based on the
number of classes. Most classes are in common
with HAM-8, but the images are from different
populations. It is used for pre-training for CI Split
MNIST and DI Split MNIST.
B. Methods
We compare AQC and NC with reference methods, se-
quential variational inference methods and sequential MAP
inference methods. Joint MAP training and fine-tuning serve
as the best and worst reference methods, respectively. In
the former, all the previous data are used together with the
current data to train the neural network, while in the latter,
the previously trained neural network is simply fine-tuned to
the current task. The variational inference methods compared
are G-VCL [10], GM-VCL [11] and G-SFSVI [12], and the
sequential MAP inference methods compared are EWC with
Husz ´ar’s corrected penalty [4], [6] and SI [8]. These methods
are described in Section III. To make fair comparisons, only
coreset-free methods, i.e. methods do not store any previous
data, are considered.
Each method runs through each training dataset sequence,
its hyperparameters are selected based on the final average
accuracy on the validation dataset sequence, and it is evaluated
based on the final average accuracy on the testing dataset

--- PAGE 5 ---
(a) Softmax regression
(b) Fully connected neural network
Fig. 2: Visualizations of prediction probabilities for the methods on CI Split 2D Iris. The x-axis is the petal length (cm) and
the y-axis is the petal width (cm). The pseudocolor plot shows the prediction probabilities, where the 3 class probabilities are
mapped to the red, green and blue values, respectively, and the dots show the observed data. NC performs the best and is
better with softmax regression.
sequence. The final average accuracy on a dataset sequence is
defined as the average of all the accuracies on all the datasets
in the sequence.
C. Results
Since AQC relies on the very accurate automatic differentia-
tion for Hessian computation, we expect that it has better final
average accuracy than EWC and SI, and we are interested in
observing how much better it performs. Since neural networks
are powerful function approximators, and the previous loss
functions in our experiments are not quadratic, we expect that
NC has better final average than the quadratic approximation
methods. We are also interested in how much a pre-trained
feature extractor helps in sequential MAP inference.
Visualizations of the prediction probabilities for the meth-
ods on CI Split 2D Iris for softmax regression and a fully
connected neural network are shown in Fig. 2. We find that
AQC performs better than EWC and SI, and NC performs the
best, but it does better for softmax regression.
The testing final average accuracies for the methods on
classical and image task sequences are shown in Table I.
We find that AQC performs better than EWC and SI, and
NC performs the best in most classical task sequences. We
also find that NC performs better with softmax regression
than with a fully connected neural network probably becausethe loss function in the former is convex and is easier to
fit. In image task sequences, where a feature extractor is
used, AQC consistently performs the best and has performance
comparable to joint MAP training in some task sequences.
However, we find that NC does not perform as well as AQC,
but is better than EWC and SI in some task sequences.
It is notable that EWC performs as poorly as fine-tuning
in class-incremental learning on the whole neural network
from scratch [2], but using a pre-trained feature extractor
significantly improves it. Moreover, in DI Split CIFAR-8,
using a pre-trained feature extractor alone significantly reduces
forgetting, and even fine-tuning performs quite well.
A possible reason that NC does not perform well in image
task sequences is that the dimension of the feature space
is high (64 in CI Split MNIST and DI Split MNIST and
512 in other task sequences), and random sampling becomes
inefficient in high dimensions.
D. Data Availability
All the datasets used in this work are publicly available. Iris
and Wine are available from the scikit-learn package
[18], which is released under the 3-clause BSD license.
MNIST, EMNIST, CIFAR-10 and CIFAR-100 are available
from the pytorch package [19], which is also released
under the 3-clause BSD license. HAM10000 [20] is released

--- PAGE 6 ---
TABLE I: Testing final average accuracy for the methods on classical and image task sequences. For classical task sequences,
results for softmax regression (SR) and a fully connected neural network (FCNN) are shown. Pre-training on a similar task
is used for image task sequences. NC performs the best in most classical task sequences while AQC performs the best in all
image task sequences with a fixed pre-trained feature extractor.
(a) Classical task sequences
MethodCI Split Iris CI Split Wine
SR FCNN SR FCNN
Joint MAP training 96.6667 100.0000 91.1111 91.1111
Fine-tuning 33.3333 33.3333 33.3333 33.3333
G-VCL 33.3333 33.3333 33.3333 33.3333
GM-VCL 33.3333 33.3333 33.3333 33.3333
G-SFSVI 66.6667 33.3333 33.3333 33.3333
EWC 63.3333 33.3333 46.6667 33.3333
SI 33.3333 33.3333 33.3333 33.3333
AQC 66.6667 63.3333 49.6032 33.3333
NC 93.3333 63.3333 62.6984 48.2540
(b) Image task sequences
Method CI Split MNIST CI Split CIFAR-10 CI Split HAM-8 DI Split MNIST DI Split CIFAR-8 DI Split HAM-6
Joint MAP training 95.1077 76.2500 43.0531 93.4338 96.6250 68.3237
Fine-tuning 19.8382 19.0400 21.9512 64.4789 89.6250 63.8068
G-VCL 25.8657 19.0100 21.6463 80.1092 92.9000 63.3071
GM-VCL 25.2141 18.9800 21.0366 77.3996 92.6000 63.6112
G-SFSVI 51.1918 32.8000 34.6563 72.5471 91.2375 63.3381
EWC 73.3744 31.9400 30.8636 82.9540 94.6625 63.8068
SI 30.2364 27.0500 21.6463 79.4063 95.0750 64.0020
AQC 92.5394 61.3100 41.6660 90.9951 96.2000 66.1442
NC 77.5334 40.2500 24.1040 89.9168 93.8000 62.2400
by the Hospital Clinic in Barcelona under CC BY-NC, and
BCN20000 [21] is released by ViDIR Group, Department of
Dermatology, Medical University of Vienna, also under CC
BY-NC.
E. Code Availability
Documented and reproducible code
is available under an MIT Licence at
https://github.com/blackblitz/bcl .
V. C ONCLUSION
We formulated continual learning based on sequential max-
imum a posteriori inference as a recursion of loss functions
and reduced the problem to function approximation. We
then proposed two coreset-free methods based on it: autod-
iff quadratic consolidation and neural consolidation, which
use a full quadratic approximation and a neural network
approximation, respectively, to approximate the previous loss
function. Moreover, we showed empirically that pre-training
the neural network on a similar task could significantly reduce
forgetting with sequential maximum a posteriori inference
methods. Neural consolidation performs the best in classical
task sequences, where the input dimension is small. Autodiff
quadratic consolidation consistently performs very well in
image task sequences with pre-training on a similar task,
achieving performance comparable to joint maximum a pos-
teriori training in many cases. In the future, we may considerspecial neural network architectures for neural consolidation
as well as more applications in medical image classification,
document image understanding [22] and materials science
[23].
ACKNOWLEDGEMENTS
We thank Pengcheng Hao, Professor Yang Li from Tsinghua
Shenzhen International Graduate School and anonymous re-
viewers for their feedback.
REFERENCES
[1] M. McCloskey and N. J. Cohen, “Catastrophic interfer-
ence in connectionist networks: The sequential learning
problem,” Psychology of Learning and Motivation -
Advances in Research and Theory , vol. 24, C 1989.
[2] G. M. van de Ven, T. Tuytelaars, and A. S. Tolias,
“Three types of incremental learning,” Nature Machine
Intelligence , vol. 4, no. 12, pp. 1185–1197, 2022.
[3] S. Farquhar and Y . Gal, Towards robust evaluations of
continual learning , 2019. arXiv: 1805.09733.
[4] F. Husz ´ar,On quadratic penalties in elastic weight
consolidation , 2017. arXiv: 1712.03847.

--- PAGE 7 ---
[5] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G.
Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,
A. Grabska-Barwinska, D. Hassabis, C. Clopath, D.
Kumaran, and R. Hadsell, “Overcoming catastrophic
forgetting in neural networks,” Proceedings of the Na-
tional Academy of Sciences of the United States of
America , vol. 114, no. 13, pp. 3521–3526, 2017.
[6] F. Husz ´ar, “Note on the quadratic penalties in elas-
tic weight consolidation,” Proceedings of the National
Academy of Sciences of the United States of America ,
vol. 115, no. 11, E2496–E2497, 2018.
[7] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S.
Torr, “Riemannian walk for incremental learning: Un-
derstanding forgetting and intransigence,” in Computer
Vision – ECCV 2018 , V . Ferrari, M. Hebert, C. Smin-
chisescu, and Y . Weiss, Eds., Cham: Springer Interna-
tional Publishing, 2018, pp. 556–572.
[8] F. Zenke, B. Poole, and S. Ganguli, “Continual learning
through synaptic intelligence,” in Proceedings of the
34th International Conference on Machine Learning ,
D. Precup and Y . W. Teh, Eds., ser. Proceedings of
Machine Learning Research, vol. 70, PMLR, 2017,
pp. 3987–3995.
[9] H. Ritter, A. Botev, and D. Barber, “Online structured
Laplace approximations for overcoming catastrophic
forgetting,” in Advances in Neural Information Pro-
cessing Systems , S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.,
vol. 31, Curran Associates, Inc., 2018.
[10] C. V . Nguyen, Y . Li, T. D. Bui, and R. E. Turner, “Vari-
ational continual learning,” in International Conference
on Learning Representations , 2018.
[11] H. Phan, A. P. Tuan, S. Nguyen, N. V . Linh, and
K. Than, “Reducing catastrophic forgetting in neural
networks via Gaussian mixture approximation,” in Ad-
vances in Knowledge Discovery and Data Mining , J.
Gama, T. Li, Y . Yu, E. Chen, Y . Zheng, and F. Teng,
Eds., Cham: Springer International Publishing, 2022,
pp. 106–117.
[12] T. G. J. Rudner, F. B. Smith, Q. Feng, Y . W. Teh, and Y .
Gal, “Continual learning via sequential function-space
variational inference,” in Proceedings of the 39th Inter-
national Conference on Machine Learning , K. Chaud-
huri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and
S. Sabato, Eds., ser. Proceedings of Machine Learning
Research, vol. 162, PMLR, 2022, pp. 18 871–18 887.
[13] K.-Y . Lee, Y . Zhong, and Y .-X. Wang, “Do pre-
trained models benefit equally in continual learning?”
InProceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision (WACV) , 2023,
pp. 6485–6493.
[14] S. V . Mehta, D. Patil, S. Chandar, and E. Strubell,
“An empirical investigation of the role of pre-training
in lifelong learning,” Journal of Machine Learning
Research , vol. 24, no. 214, pp. 1–50, 2023.[15] W. Hu, Q. Qin, M. Wang, J. Ma, and B. Liu, “Continual
learning by using information of each class holistically,”
vol. 35, pp. 7797–7805, 2021.
[16] X. Li, H. Li, and L. Ma, “Continual learning of medical
image classification based on feature replay,” in 2022
16th IEEE International Conference on Signal Process-
ing (ICSP) , vol. 1, 2022, pp. 426–430.
[17] Y . Yang, Z. Cui, J. Xu, C. Zhong, W.-S. Zheng, and
R. Wang, “Continual learning with Bayesian model
based on a fixed pre-trained feature extractor,” Visual
Intelligence , vol. 1, no. 1, p. 5, 2023.
[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D.
Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay,
“Scikit-learn: Machine learning in Python,” Journal of
Machine Learning Research , vol. 12, pp. 2825–2830,
2011.
[19] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M.
V oznesensky, B. Bao, P. Bell, D. Berard, E. Burovski,
G. Chauhan, A. Chourdia, W. Constable, A. Desmaison,
Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind,
B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M.
Lazos, M. Lezcano, Y . Liang, J. Liang, Y . Lu, C. Luk,
B. Maher, Y . Pan, C. Puhrsch, M. Reso, M. Saroufim,
M. Y . Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X.
Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A.
Mathews, G. Chanan, P. Wu, and S. Chintala, “PyTorch
2: Faster Machine Learning Through Dynamic Python
Bytecode Transformation and Graph Compilation,” in
29th ACM International Conference on Architectural
Support for Programming Languages and Operating
Systems, Volume 2 (ASPLOS ’24) , ACM, 2024.
[20] P. Tschandl, C. Rosendahl, and H. Kittler, “The
HAM10000 dataset, a large collection of multi-source
dermatoscopic images of common pigmented skin le-
sions,” Scientific data , vol. 5, no. 1, pp. 1–9, 2018.
[21] C. Hern ´andez-P ´erez, M. Combalia, S. Podlipnik, N. C.
Codella, V . Rotemberg, A. C. Halpern, O. Reiter, C.
Carrera, A. Barreiro, B. Helba, et al. , “BCN20000: Der-
moscopic lesions in the wild,” Scientific Data , vol. 11,
no. 1, p. 641, 2024.
[22] E. E. Kuruo ˘glu and A. S. Taylor, “Using annotations
for summarizing a document image and itemizing the
summary based on similar annotations,” US7712028B2,
2010.
[23] F. Saffarimiandoab, R. Mattesini, W. Fu, E. E. Ku-
ruoglu, and X. Zhang, “Insights on features’ contribu-
tion to desalination dynamics and capacity of capacitive
deionization through machine learning study,” Desali-
nation , vol. 515, p. 115 197, 2021.

--- PAGE 8 ---
APPENDIX A
EXPERIMENT DETAILS
A. Data Preparation
For task sequences based on Iris and Wine, the dataset
is split into training and testing datasets with 20% testing
size, and then the training dataset into training and validation
datasets with 20% validation size, so the training, validation
and testing proportions are 64%, 16% and 20%, respectively.
Finally, each dataset is split by class into a dataset sequence.
For EMNIST Letters, CIFAR-100 and task sequences based
on MNIST and CIFAR-10, training and testing datasets are
available from PyTorch, so the training dataset is split into
training and validation datasets with 20% validation size. Each
dataset is then split by class into a dataset sequence.
For BCN-12 and HAM-8, the 640×450images are resized
to32×32with Lanczos interpolation. For all image data, the
pixel values are divided by 255 so that they take values be-
tween 0 and 1. Data augmentation (e.g. flipping and cropping)
is not performed.
B. Neural Network Architectures
The fully connected neural network used for CI Split 2D
Iris and CI Split Iris has 1 hidden layer of 4 nodes, while that
used for CI Split Wine has 1 hidden layer of 16 nodes. All
the hidden nodes use swish activation.
The pre-trained neural network for both CI Split MNIST
and DI Split MNIST has 2 convolutional layers and 2 dense
layers, totaling 4 layers. Each convolutional layer has 32 3×3
filters and is followed by group normalization with 32 groups,
swish activation and average pooling with a size of 2×2. The
hidden dense layer has 64 nodes with swish activation. Thus,
the feature dimension is 64.
The pre-trained neural network for CI Split CIFAR-10, CI
Split HAM-8 DI Split CIFAR-8 and DI Split HAM-6 has
17 convolutional layers and 1 dense layer, totaling 18 layers.
Each convolutional layer is followed by group normalization
with 32 groups and swish activation. The 2nd to the 17th
convolutional layers are arranged into 8 residual blocks, each
with 2 convolutional layers, and every 2 residual blocks are
followed by average pooling with a size of 2×2. The numbers
of filters for the 17 convolutional layers are 32, 64, 64, 64,
64, 128, 128, 128, 128, 256, 256, 256, 256, 512, 512, 512 and
512, respectively, and the filter sizes are all 3×3. Thus, the
feature dimension is 512.
In all experiments, the consolidator neural network used
in NC is a fully connected neural network with 2 hidden
layers, each with 256 nodes. All the hidden nodes use swish
activation.
C. Training
In all experiments, the prior PDF at time 1 is a standard
Gaussian PDF (of an appropriate dimension), and an Adam
optimizer is used with a one-cycle learning rate schedule.
The neural network parameters are initialized by using the
Lecun normal initializer for the weights and setting to zero
for the biases. For pre-training tasks and class-incremental tasksequences, each task is of multi-class classification, so cate-
gorical cross entropy is used, while for domain-incremental
task sequences, each task is of binary classification, so binary
or Bernoulli cross entropy is used. BCN-12 is a task with
severe class imbalance, so for pre-training on BCN-12, in-
stead of the standard cross entropy, a weighted cross entropy
−Pk
i=1m
nipilnqiis used, where piis the true label indicator
andqiis the predicted probability, niis the frequency of the
i-th class and m= min {n1, n2, . . . , n k}.
For CI Split 2D Iris and CI Split Iris, training for each task
is performed for 100 epochs with a base learning rate of 0.1
and a batch size of 16. For CI Split Wine, training for each
task is performed similarly but with a base learning rate of
0.01.
For CI Split MNIST and DI Split MNIST, pre-training is
performed on EMNIST Letters for 20 epochs with a base
learning rate of 0.01 and a batch size of 64, and training for
each task is performed similarly. For CI Split MNIST and DI
Split CIFAR-8, pre-training is performed on CIFAR-100 for
20 epochs with a base learning rate of 0.001 and a batch size
of 64, and training for each task is performed similarly but
with a base learning rate of 0.01. For CI Split HAM-8 and
DI Split HAM-6, pre-training is performed on BCN-12 for 20
epochs with a base learning rate of 0.0001 and a batch size of
64, and training for each task is performed similarly but with
a base learning rate of 0.001.
For G-SFSVI, the inducing points are randomly generated
from a uniform distribution in a hyperrectangle the boundaries
of which are determined by the minimum and maximum
values of the training input data across all tasks in the task
sequence. For image task sequences with pre-training, the
boundaries for each feature component are set to -1 and 6.
D. Hyperparameter Tuning
In EWC, SI, AQC and NC, there is a hyperparameter λthat
determines the regularization strength. SI has an extra damping
hyperparameter ξand NC has an extra radius hyperparameter
r. Hyperparameter tuning is performed based on the validation
final average accuracy via grid search among the following
values:
•EWC: λ∈ {1,10,100,1000,10000}
•SI:λ∈ {1,10,100,1000,10000}, ξ∈ {0.1,1.0,10}
•AQC: λ∈ {1,10,100,1000,10000}
•NC:λ∈ {1,10,100,1000,10000}, r∈ {1,10,100}
