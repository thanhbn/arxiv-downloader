# 2209.08660.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2209.08660.pdf
# Kích thước tệp: 857691 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2209.08660v2  [cs.LG]  20 Nov 2023Xuất bản tại Transactions on Machine Learning Research (09 /2023)
Học Thời Điểm Học: Lập Lịch Phát Lại trong
Học Tập Liên Tục
Marcus Klasson marcus.klasson@aalto.fi
Đại học Aalto
Hedvig Kjellström hedvig@kth.se
Viện Công nghệ Hoàng gia KTH
Cheng Zhang cheng.zhang@microsoft.com
Microsoft Research
Đánh giá trên OpenReview: https: // openreview. net/ forum? id= Q4aAITDgdP
Tóm tắt
Các phương pháp phát lại được biết đến là thành công trong việc giảm thiểu quên thảm khốc trong các kịch bản học tập liên tục mặc dù chỉ có quyền truy cập hạn chế vào dữ liệu lịch sử. Tuy nhiên, việc lưu trữ dữ liệu lịch sử rẻ trong nhiều bối cảnh thực tế, nhưng việc phát lại toàn bộ dữ liệu lịch sử thường bị cấm do ràng buộc về thời gian xử lý. Trong những bối cảnh như vậy, chúng tôi đề xuất rằng các hệ thống học tập liên tục nên học thời điểm học và lập lịch những nhiệm vụ nào cần phát lại tại các bước thời gian khác nhau. Trước tiên chúng tôi chứng minh lợi ích của đề xuất này bằng cách sử dụng tìm kiếm cây Monte Carlo để tìm lịch phát lại phù hợp, và cho thấy rằng các lịch phát lại được tìm thấy có thể vượt trội hơn các chính sách lập lịch cố định khi kết hợp với các phương pháp phát lại khác nhau trong các cài đặt học tập liên tục khác nhau. Ngoài ra, chúng tôi đề xuất một khung cho việc học các chính sách lập lịch phát lại với học tăng cường. Chúng tôi cho thấy rằng các chính sách đã học có thể tổng quát hóa tốt hơn trong các kịch bản học tập liên tục mới so với việc phát lại đồng đều tất cả các nhiệm vụ đã thấy, mà không tốn thêm chi phí tính toán. Nghiên cứu của chúng tôi tiết lộ tầm quan trọng của việc học thời điểm học trong học tập liên tục, điều này đưa nghiên cứu hiện tại gần hơn với nhu cầu thực tế.

1 Giới thiệu
Nhiều tổ chức triển khai các hệ thống học máy nhận được lượng lớn dữ liệu hàng ngày ( Bailis et al. , 2017;Hazelwood et al. ,2018). Mặc dù tất cả dữ liệu lịch sử được lưu trữ trong đám mây trong thực tế, việc đào tạo lại các hệ thống học máy hàng ngày là quá tốn kém cả về thời gian và chi phí. Trong bối cảnh này, các hệ thống thường cần liên tục thích ứng với các nhiệm vụ mới trong khi duy trì các khả năng đã học trước đó. Các phương pháp học tập liên tục (CL) ( Delange et al. ,2021;Parisi et al. ,2019) giải quyết thách thức này, trong đó, đặc biệt, các phương pháp phát lại ( Chaudhry et al. ,2019;Hayes et al. ,2020) đã được chứng minh là hiệu quả trong việc đạt được hiệu suất dự đoán tuyệt vời. Các phương pháp phát lại giảm thiểu quên thảm khốc bằng cách xem xét lại một tập hợp nhỏ các mẫu, điều này khả thi để xử lý so với kích thước của dữ liệu lịch sử. Trong tài liệu CL truyền thống, bộ nhớ phát lại bị hạn chế do giả định rằng dữ liệu lịch sử không có sẵn. Trong các bối cảnh thực tế nơi dữ liệu lịch sử luôn có sẵn, yêu cầu về bộ nhớ nhỏ vẫn tồn tại do các vấn đề về thời gian xử lý và chi phí.

Nghiên cứu gần đây về CL dựa trên phát lại đã tập trung vào chất lượng của các mẫu bộ nhớ ( Aljundi et al. ,2019b ; Chaudhry et al. ,2019;Nguyen et al. ,2018;Rebuffi et al. ,2017;Yoon et al. ,2022) hoặc nén dữ liệu để tăng dung lượng bộ nhớ ( Hayes et al. ,2020;Pellegrini et al. ,2020). Hầu hết các phương pháp trước đây phân bổ không gian lưu trữ bộ nhớ bằng nhau cho các mẫu từ các nhiệm vụ cũ, và phát lại toàn bộ bộ nhớ để giảm thiểu quên thảm khốc. Tuy nhiên, trong các cài đặt học tập suốt đời, chiến lược đơn giản này sẽ không hiệu quả vì bộ nhớ phải lưu trữ một số lượng lớn các nhiệm vụ. Hơn nữa, chính sách lựa chọn đồng đều thường được sử dụng của các mẫu để phát lại bỏ qua thời điểm của những nhiệm vụ nào cần học lại. Điều này trái ngược với việc học của con người nơi các phương pháp giáo dục tập trung vào lập lịch học các nhiệm vụ mới và ôn tập kiến thức đã học trước đó. Ví dụ, lặp lại có khoảng cách ( Dempster ,1989;Ebbinghaus ,2013;Landauer & Bjork ,1977), nơi khoảng thời gian giữa các lần ôn tập tăng dần, đã được chứng minh là tăng cường khả năng ghi nhớ so với ôn tập đồng đều.

Chúng tôi lập luận rằng việc tìm lịch phù hợp về những nhiệm vụ nào cần phát lại trong các cài đặt bộ nhớ cố định là quan trọng đối với CL. Để chứng minh tuyên bố của chúng tôi, chúng tôi thực hiện một thí nghiệm đơn giản trên tập dữ liệu Split MNIST ( Zenke et al. , 2017) nơi mỗi nhiệm vụ bao gồm việc học các chữ số 0/1, 2/3, v.v. xuất hiện theo trình tự. Ở đây, bộ nhớ phát lại chứa 10 mẫu từ nhiệm vụ 1 và chỉ có thể được phát lại khi học một trong các nhiệm vụ. Hình 1 cho thấy cách hiệu suất nhiệm vụ tiến triển theo thời gian khi bộ nhớ này được phát lại tại các nhiệm vụ khác nhau. Trong ví dụ này, hiệu suất trung bình tốt nhất đạt được khi bộ nhớ được sử dụng khi học nhiệm vụ 5. Lưu ý rằng việc chọn các điểm thời gian khác nhau để phát lại cùng một bộ nhớ dẫn đến hiệu suất phân loại khác nhau đáng chú ý. Những kết quả này cho thấy rằng việc lập lịch thời điểm áp dụng phát lại có thể ảnh hưởng đáng kể đến hiệu suất cuối cùng của hệ thống CL.

Trong bài báo này, chúng tôi đề xuất học thời điểm học cho các hệ thống CL, trong đó chúng tôi học các lịch phát lại về những nhiệm vụ nào cần phát lại tại các thời điểm khác nhau được lấy cảm hứng từ việc học của con người ( Dempster ,1989). Để chứng minh lợi ích của lập lịch phát lại, chúng tôi thực hiện các thí nghiệm trong môi trường CL lý tưởng nơi nhiều lần thử được cho phép để cho phép tìm kiếm lịch phát lại tối ưu. Chúng tôi sử dụng tìm kiếm cây Monte Carlo (MCTS) ( Coulom ,2006) để tìm các lịch phát lại phù hợp, được đánh giá bằng cách đo hiệu suất nhiệm vụ của mạng được đào tạo trong kịch bản CL trong đó các mẫu phát lại theo lịch được sử dụng để giảm quên thảm khốc. Hơn nữa, vì việc sử dụng MCTS trong các cài đặt CL thực tế là không khả thi, chúng tôi đề xuất một khung sử dụng học tăng cường (RL) ( Sutton & Barto ,2018) để học các chính sách lập lịch phát lại. Mục tiêu của chúng tôi là học một chính sách tổng quát đã khám phá ngầm các mối quan hệ nhiệm vụ trong quá trình đào tạo, sao cho chính sách có thể được áp dụng để giảm thiểu quên thảm khốc trong các kịch bản CL mới mà không cần đào tạo thêm tại thời điểm kiểm tra. Chúng tôi đánh giá chính sách đã học bằng cách so sánh khả năng lập lịch các nhiệm vụ phát lại của nó với các chính sách lập lịch cố định, chẳng hạn như phát lại đồng đều tất cả các nhiệm vụ. Tóm lại, những đóng góp của chúng tôi là:

• Chúng tôi đề xuất một cài đặt CL mới nơi dữ liệu lịch sử có sẵn trong khi thời gian xử lý bị hạn chế, nhằm điều chỉnh nghiên cứu CL hiện tại gần hơn với nhu cầu thực tế (Phần 3.1). Trong cài đặt này, chúng tôi giới thiệu lập lịch phát lại nơi chúng tôi học thời điểm của những nhiệm vụ nào cần phát lại (Phần 3.2).

• Để chứng minh lợi ích của lập lịch phát lại, chúng tôi áp dụng MCTS trong môi trường CL lý tưởng nơi MCTS tìm kiếm trên một tập hợp hữu hạn các bộ nhớ phát lại tại mỗi nhiệm vụ (Phần 3.2). Chúng tôi cho thấy rằng các lịch phát lại được tìm thấy có thể giảm thiểu quên thảm khốc một cách hiệu quả trên nhiều chuẩn mực cho các phương pháp lựa chọn bộ nhớ và phát lại khác nhau trong các kịch bản CL khác nhau (Phần 4.1).

• Là bước đầu tiên hướng tới việc cho phép lập lịch phát lại cho các cài đặt CL thực tế, chúng tôi đề xuất một khung dựa trên RL để học các chính sách giảm thiểu quên thảm khốc trên các môi trường CL khác nhau (Phần 3.3). Chúng tôi cho thấy rằng các chính sách đã học có thể vượt trội hơn việc phát lại đồng đều tất cả các nhiệm vụ trong các kịch bản CL với thứ tự nhiệm vụ mới và tập dữ liệu chưa thấy trong quá trình đào tạo (Phần 4.2).

--- TRANG 2 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)
1 2 3 4 5708090100
NhiệmvụĐộchínhxác(%)ACC: 89.66%
1 2 3 4 5708090100
NhiệmvụACC: 93.85%
1 2 3 4 5708090100
NhiệmvụACC: 93.17%
1 2 3 4 5708090100
NhiệmvụACC: 94.49%Nhiệm vụ 1
Nhiệm vụ 2
Nhiệm vụ 3
Nhiệm vụ 4
Nhiệm vụ 5
Phát lại
Hình 1: Độ chính xác nhiệm vụ trên Split MNIST ( Zenke et al. ,2017) khi chỉ phát lại 10 mẫu của lớp 0/1 tại một bước thời gian duy nhất. Đường thẳng đứng màu đen chỉ ra khi phát lại được sử dụng. ACC biểu thị độ chính xác trung bình trên tất cả các nhiệm vụ sau khi học Nhiệm vụ 5. Kết quả được tính trung bình trên 5 hạt giống. Những kết quả này cho thấy rằng thời điểm phát lại nhiệm vụ trước đó là quan trọng đối với hiệu suất cuối cùng.

để phát lại bỏ qua thời điểm của những nhiệm vụ nào cần học lại. Điều này trái ngược với việc học của con người nơi các phương pháp giáo dục tập trung vào lập lịch học các nhiệm vụ mới và ôn tập kiến thức đã học trước đó. Ví dụ, lặp lại có khoảng cách ( Dempster ,1989;Ebbinghaus ,2013;Landauer & Bjork ,1977), nơi khoảng thời gian giữa các lần ôn tập tăng dần, đã được chứng minh là tăng cường khả năng ghi nhớ bộ nhớ so với ôn tập đồng đều.

Chúng tôi lập luận rằng việc tìm lịch phù hợp về những nhiệm vụ nào cần phát lại trong các cài đặt bộ nhớ cố định là quan trọng đối với CL. Để chứng minh tuyên bố của chúng tôi, chúng tôi thực hiện một thí nghiệm đơn giản trên tập dữ liệu Split MNIST ( Zenke et al. , 2017) nơi mỗi nhiệm vụ bao gồm việc học các chữ số 0/1, 2/3, v.v. xuất hiện theo trình tự. Ở đây, bộ nhớ phát lại chứa 10 mẫu từ nhiệm vụ 1 và chỉ có thể được phát lại khi học một trong các nhiệm vụ. Hình 1 cho thấy cách hiệu suất nhiệm vụ tiến triển theo thời gian khi bộ nhớ này được phát lại tại các nhiệm vụ khác nhau. Trong ví dụ này, hiệu suất trung bình tốt nhất đạt được khi bộ nhớ được sử dụng khi học nhiệm vụ 5. Lưu ý rằng việc chọn các điểm thời gian khác nhau để phát lại cùng một bộ nhớ dẫn đến hiệu suất phân loại khác nhau đáng chú ý. Những kết quả này cho thấy rằng việc lập lịch thời điểm áp dụng phát lại có thể ảnh hưởng đáng kể đến hiệu suất cuối cùng của hệ thống CL.

Trong bài báo này, chúng tôi đề xuất học thời điểm học cho các hệ thống CL, trong đó chúng tôi học các lịch phát lại về những nhiệm vụ nào cần phát lại tại các thời điểm khác nhau được lấy cảm hứng từ việc học của con người ( Dempster ,1989). Để chứng minh lợi ích của lập lịch phát lại, chúng tôi thực hiện các thí nghiệm trong môi trường CL lý tưởng nơi nhiều lần thử được cho phép để cho phép tìm kiếm lịch phát lại tối ưu. Chúng tôi sử dụng tìm kiếm cây Monte Carlo (MCTS) ( Coulom ,2006) để tìm các lịch phát lại phù hợp, được đánh giá bằng cách đo hiệu suất nhiệm vụ của mạng được đào tạo trong kịch bản CL trong đó các mẫu phát lại theo lịch được sử dụng để giảm quên thảm khốc. Hơn nữa, vì việc sử dụng MCTS trong các cài đặt CL thực tế là không khả thi, chúng tôi đề xuất một khung sử dụng học tăng cường (RL) ( Sutton & Barto ,2018) để học các chính sách lập lịch phát lại. Mục tiêu của chúng tôi là học một chính sách tổng quát đã khám phá ngầm các mối quan hệ nhiệm vụ trong quá trình đào tạo, sao cho chính sách có thể được áp dụng để giảm thiểu quên thảm khốc trong các kịch bản CL mới mà không cần đào tạo thêm tại thời điểm kiểm tra. Chúng tôi đánh giá chính sách đã học bằng cách so sánh khả năng lập lịch các nhiệm vụ phát lại của nó với các chính sách lập lịch cố định, chẳng hạn như phát lại đồng đều tất cả các nhiệm vụ. Tóm lại, những đóng góp của chúng tôi là:

• Chúng tôi đề xuất một cài đặt CL mới nơi dữ liệu lịch sử có sẵn trong khi thời gian xử lý bị hạn chế, nhằm điều chỉnh nghiên cứu CL hiện tại gần hơn với nhu cầu thực tế (Phần 3.1). Trong cài đặt này, chúng tôi giới thiệu lập lịch phát lại nơi chúng tôi học thời điểm của những nhiệm vụ nào cần phát lại (Phần 3.2).

• Để chứng minh lợi ích của lập lịch phát lại, chúng tôi áp dụng MCTS trong môi trường CL lý tưởng nơi MCTS tìm kiếm trên một tập hợp hữu hạn các bộ nhớ phát lại tại mỗi nhiệm vụ (Phần 3.2). Chúng tôi cho thấy rằng các lịch phát lại được tìm thấy có thể giảm thiểu quên thảm khốc một cách hiệu quả trên nhiều chuẩn mực cho các phương pháp lựa chọn bộ nhớ và phát lại khác nhau trong các kịch bản CL khác nhau (Phần 4.1).

• Là bước đầu tiên hướng tới việc cho phép lập lịch phát lại cho các cài đặt CL thực tế, chúng tôi đề xuất một khung dựa trên RL để học các chính sách giảm thiểu quên thảm khốc trên các môi trường CL khác nhau (Phần 3.3). Chúng tôi cho thấy rằng các chính sách đã học có thể vượt trội hơn việc phát lại đồng đều tất cả các nhiệm vụ trong các kịch bản CL với thứ tự nhiệm vụ mới và tập dữ liệu chưa thấy trong quá trình đào tạo (Phần 4.2).

--- TRANG 3 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

2 Công trình liên quan

Trong phần này, chúng tôi đưa ra một cái nhìn tổng quan ngắn gọn về các phương pháp khác nhau trong CL, đặc biệt là các phương pháp phát lại cũng như các kỹ thuật lặp lại có khoảng cách cho CL của con người và tổng quát hóa trong RL.

Học tập liên tục. CL truyền thống có thể được chia thành ba lĩnh vực chính, cụ thể là các phương pháp dựa trên chính quy hóa, dựa trên kiến trúc, và dựa trên phát lại. Các phương pháp dựa trên chính quy hóa bảo vệ các tham số ảnh hưởng đến hiệu suất trên các nhiệm vụ đã biết khỏi những thay đổi lớn và sử dụng các tham số khác để học các nhiệm vụ mới ( Adel et al. ,2019;Kirkpatrick et al. ,2017;Li & Hoiem ,2017;Nguyen et al. ,2018;Schwarz et al. ,2018; Zenke et al. ,2017). Các phương pháp dựa trên kiến trúc giảm thiểu quên thảm khốc bằng cách duy trì các tham số đặc thù cho nhiệm vụ ( Mallya & Lazebnik ,2018;Rusu et al. ,2016;Serra et al. ,2018;Xu & Zhu ,2018;Yoon et al. , 2020;2018). Các phương pháp phát lại trộn các mẫu từ các nhiệm vụ cũ với tập dữ liệu hiện tại để giảm thiểu quên thảm khốc, nơi các mẫu phát lại được lưu trữ trong bộ nhớ ngoài ( Aljundi et al. ,2019a ;b; Chaudhry et al. ,2019;Chrysakis & Moens ,2020;Hayes et al. ,2019;2020;Iscen et al. ,2020;Isele & Cosgun , 2018;Liu et al. ,2021;Rebuffi et al. ,2017;Rolnick et al. ,2019;Verwimp et al. ,2021) hoặc được tạo ra bằng mô hình sinh ( Shin et al. ,2017;van de Ven & Tolias ,2018). Các phương pháp dựa trên chính quy hóa và kiến trúc động đã được kết hợp với các phương pháp dựa trên phát lại để vượt qua những hạn chế của chúng ( Buzzega et al. ,2020;Chaudhry et al. ,2018a ;b;2021;Douillard et al. ,2020;Ebrahimi et al. , 2020;Joseph & Balasubramanian ,2020;Lopez-Paz & Ranzato ,2017;Mirzadeh et al. ,2021;Pan et al. ,2020; Pellegrini et al. ,2020;Riemer et al. ,2019;von Oswald et al. ,2020). Công trình của chúng tôi liên quan nhất đến các phương pháp dựa trên phát lại với bộ nhớ ngoài mà chúng tôi dành nhiều thời gian hơn để mô tả trong đoạn tiếp theo.

Học tập liên tục dựa trên phát lại. Một giả định chính trong CL truyền thống là chỉ một lượng hạn chế dữ liệu đến có thể được lưu trữ trong bộ nhớ để sử dụng lại trong tương lai nhằm giảm thiểu quên thảm khốc của các nhiệm vụ cũ. Điều này đã dẫn đến nhiều nỗ lực nghiên cứu trong CL dựa trên phát lại hoặc bộ nhớ tập trung vào việc lựa chọn các mẫu chất lượng cao để giữ trong bộ nhớ ( Aljundi et al. ,2019b ;Bang et al. ,2021;Borsos et al. ,2020; Chaudhry et al. ,2019;Chrysakis & Moens ,2020;Hayes et al. ,2019;Isele & Cosgun ,2018;Jin et al. ,2021; Nguyen et al. ,2018;Rebuffi et al. ,2017;Sun et al. ,2022;Yoon et al. ,2022). Một lựa chọn thay thế cho việc phát triển các chiến lược lựa chọn là nén dữ liệu hình ảnh thô thành các biểu diễn đặc trưng để tăng số lượng mẫu bộ nhớ có thể được lưu trữ để phát lại ( Hayes et al. ,2020;Iscen et al. ,2020;Pellegrini et al. ,2020). Gần đây hơn, các phương pháp lựa chọn mẫu nào cần truy xuất từ bộ nhớ sẽ can thiệp nhiều nhất với một cập nhật tham số được dự đoán đã được đề xuất để giảm thiểu quên thảm khốc ( Aljundi et al. , 2019a ;Shim et al. ,2021). Trong bài báo này, chúng tôi tập trung vào việc lựa chọn thời điểm phát lại các nhiệm vụ cũ, điều này hầu như bị bỏ qua trong tài liệu. Phương pháp lập lịch phát lại của chúng tôi khác với các công trình được đề cập ở trên vì chúng tôi tập trung vào việc học lựa chọn những nhiệm vụ nào cần phát lại. Tuy nhiên, lập lịch của chúng tôi có thể được kết hợp với bất kỳ phương pháp dựa trên phát lại nào, bao gồm bất kỳ chiến lược lựa chọn và truy xuất bộ nhớ nào.

Độc lập và đồng thời với công trình của chúng tôi ở đây, ( Prabhu et al. ,2023b ;a) cũng tập trung vào cài đặt trong CL mà không có ràng buộc lưu trữ. Cụ thể, Prabhu et al. (2023b ) lưu mọi mẫu đến với các đặc trưng được đào tạo trước và sử dụng bộ phân loại kNN trong không gian đặc trưng để thực hiện thích ứng nhanh mà không quên dữ liệu cũ. Prabhu et al. (2023a ) nghiên cứu cách các phương pháp CL hoạt động dưới các ràng buộc khác nhau về tính toán và cho thấy rằng việc lấy mẫu đồng đều từ dữ liệu lịch sử vượt trội hơn các phương pháp trước đó. Trong công trình này, chúng tôi lập luận rằng việc sử dụng bộ nhớ phát lại nhỏ là bổ sung để giảm lượng tính toán cho các phương pháp CL. Để giảm thiểu quên thảm khốc một cách hiệu quả với bộ nhớ phát lại nhỏ, được thúc đẩy từ việc học của con người, chúng tôi nghiên cứu những nhiệm vụ nào cần lựa chọn để phát lại tại các thời điểm khác nhau trong CL.

Học tập liên tục của con người. Con người là các hệ thống CL theo nghĩa học các nhiệm vụ và khái niệm theo trình tự. Thời điểm học và ôn tập là thiết yếu để con người ghi nhớ tốt hơn ( Dempster , 1989;Dunlosky et al. ,2013;Willis ,2007). Một ví dụ về kỹ thuật là lặp lại có khoảng cách nơi khoảng thời gian giữa các lần ôn tập được tăng dần để cải thiện khả năng ghi nhớ bộ nhớ dài hạn ( Dempster ,1989; Ebbinghaus ,2013), điều này đã được chứng minh là cải thiện khả năng ghi nhớ bộ nhớ tốt hơn thời gian ôn tập cách đều nhau ( Hawley et al. ,2008;Landauer & Bjork ,1977). Một số công trình trong CL với mạng nơ-ron được lấy cảm hứng từ các kỹ thuật học của con người, bao gồm lặp lại có khoảng cách ( Amiri et al. ,2017;Feng et al. ,2019; Smolen et al. ,2016), cơ chế ngủ ( Ball et al. ,2020;Mallya & Lazebnik ,2018;Schwarz et al. ,2018), và tái kích hoạt bộ nhớ ( Hayes et al. ,2020;van de Ven et al. ,2020). Lập lịch phát lại cũng được lấy cảm hứng từ lặp lại có khoảng cách, nơi chúng tôi học các lịch về những nhiệm vụ nào cần phát lại tại các thời điểm khác nhau.

--- TRANG 4 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Tổng quát hóa trong RL. Tổng quát hóa là một chủ đề nghiên cứu tích cực trong RL ( Kirk et al. ,2023) vì các agent RL có xu hướng overfitting với môi trường đào tạo của chúng ( Henderson et al. ,2018;Zhang et al. ,2018a ;b). Mục tiêu thường là chuyển giao các chính sách đã học sang môi trường với các nhiệm vụ mới ( Finn et al. ,2017;Higgins et al. ,2017;Kessler et al. , 2022) và không gian hành động ( Chandak et al. ,2019;2020;Jain et al. ,2020). Một số phương pháp nhằm cải thiện khả năng tổng quát hóa bằng cách tạo ra dữ liệu đào tạo đa dạng hơn ( Cobbe et al. ,2019;Tobin et al. ,2017; Wang et al. ,2020;Zhang et al. ,2018a ), sử dụng chính quy hóa mạng hoặc thiên kiến quy nạp ( Farebrother et al. , 2018;Igl et al. ,2019;Zambaldi et al. ,2018), hoặc học các mô hình động lực ( Ball et al. ,2021;Nagabandi et al. , 2019). Trong bài báo này, chúng tôi học các chính sách sử dụng RL để lựa chọn những nhiệm vụ nào mà mạng CL nên phát lại bằng cách tạo ra dữ liệu đào tạo từ các môi trường CL khác nhau. Việc sử dụng RL để quản lý bộ nhớ trong CL đã được nghiên cứu trước đó trong RMM ( Liu et al. ,2021) nơi chính sách đã học lựa chọn phân vùng bộ nhớ giữa các nhiệm vụ cũ và mới. Khung RL của chúng tôi có những điểm tương đồng với RMM ở chỗ mục tiêu của chúng tôi là học chính sách có thể chuyển giao mà không cần chi phí đào tạo thêm tại thời điểm kiểm tra, chúng tôi giả định số lượng nhiệm vụ được biết, và chính sách được học từ các môi trường CL được tạo ra khác với môi trường kiểm tra. Sự khác biệt chính nằm ở cài đặt CL nơi chúng tôi giả định rằng dữ liệu lịch sử có thể truy cập để phát lại bất cứ lúc nào. Hơn nữa, trong khi RMM giữ một phân vùng cho mọi nhiệm vụ đã thấy trong suốt kịch bản CL, phương pháp của chúng tôi lựa chọn những nhiệm vụ nào cần phát lại tại mỗi nhiệm vụ.

3 Phương pháp

Ở đây, chúng tôi mô tả cài đặt vấn đề mới của chúng tôi về CL nơi dữ liệu lịch sử có sẵn trong khi thời gian xử lý bị hạn chế khi học các nhiệm vụ mới. Trong Phần 3.1 và 3.2, chúng tôi trình bày cài đặt vấn đề được xem xét, cũng như ý tưởng của chúng tôi về việc học các lịch về những nhiệm vụ nào cần phát lại tại các bước thời gian khác nhau để giảm thiểu quên thảm khốc. Phần 3.2 cũng mô tả cách chúng tôi sử dụng MCTS ( Coulom ,2006) để nghiên cứu lợi ích của lập lịch phát lại trong CL. Trong Phần 3.3, chúng tôi trình bày một khung dựa trên RL để học các chính sách lập lịch phát lại có thể tổng quát hóa sang các kịch bản CL khác nhau.

3.1 Cài đặt vấn đề

Chúng tôi tập trung vào một cài đặt hơi mới trong CL, nơi chúng tôi giả định rằng tất cả dữ liệu lịch sử có sẵn để giảm thiểu quên thảm khốc vì việc lưu trữ dữ liệu rẻ. Tuy nhiên, vì khối lượng dữ liệu này thường rất lớn, việc đào tạo lại trên tất cả dữ liệu lịch sử bất cứ khi nào hệ thống CL phải thích ứng với các nhiệm vụ mới là không thực tế. Do đó, chúng tôi giả định có những ràng buộc về thời gian xử lý hạn chế hệ thống chỉ lấy mẫu một bộ nhớ phát lại nhỏ từ dữ liệu lịch sử chỉ một lần khi thích ứng với các nhiệm vụ mới. Thách thức trở thành cách lựa chọn những nhiệm vụ cũ nào để điền vào bộ nhớ phát lại, sao cho hệ thống CL đạt được độ chính xác tốt nhất có thể và giảm thiểu quên trên tất cả các nhiệm vụ.

Ký hiệu của cài đặt vấn đề của chúng tôi giống với cài đặt CL truyền thống cho phân loại hình ảnh. Chúng tôi để mạng fφ, được tham số hóa bởi φ, học T nhiệm vụ tuần tự từ các tập dữ liệu D1, . . . , DT xuất hiện từng cái một. Tập dữ liệu thứ t Dt={(x(i)t, y(i)t)}Nti=1 bao gồm Nt mẫu nơi x(i)t và y(i)t là điểm dữ liệu và nhãn lớp thứ i tương ứng. Hơn nữa, mỗi tập dữ liệu được chia thành tập huấn luyện, xác thực và kiểm tra, tức là, Dt={D(train)t ,D(val)t,D(test)t}. Mục tiêu tại nhiệm vụ t là giảm thiểu tổn thất ℓ(fφ(xt), yt) nơi ℓ(·) là tổn thất entropy chéo trong trường hợp của chúng tôi.

Chúng tôi giả định rằng dữ liệu lịch sử từ các nhiệm vụ cũ có thể truy cập tại bất kỳ nhiệm vụ t nào. Tuy nhiên, do ràng buộc thời gian xử lý, chúng tôi chỉ có thể sử dụng một bộ nhớ phát lại nhỏ M gồm M mẫu lịch sử để phát lại khi học một nhiệm vụ mới. Thách thức sau đó trở thành cách lựa chọn M mẫu phát lại để giữ lại kiến thức của các nhiệm vụ cũ một cách hiệu quả. Chúng tôi tập trung vào việc lựa chọn các mẫu ở cấp nhiệm vụ bằng cách quyết định tỷ lệ nhiệm vụ ( p1, . . . , pt−1) của các mẫu cần lấy từ mỗi nhiệm vụ, nơi pi≥0 là tỷ lệ của M mẫu từ nhiệm vụ i để đặt trong M và ∑t−1i=1pi= 1. Để đơn giản hóa việc lựa chọn những nhiệm vụ nào cần phát lại, chúng tôi giả định rằng số lượng nhiệm vụ T cần học được biết và xây dựng một tập rời rạc các tỷ lệ nhiệm vụ có thể có để lựa chọn cho việc xây dựng M.

3.2 Lập lịch phát lại trong học tập liên tục

Trong phần này, chúng tôi mô tả thiết lập của chúng tôi để cho phép lập lịch cho việc lựa chọn bộ nhớ phát lại tại các bước thời gian khác nhau. Chúng tôi định nghĩa một lịch phát lại là một chuỗi S= (p1, . . . , pT−1), nơi các tỷ lệ nhiệm vụ

--- TRANG 5 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

pi= (p1, . . . , pT−1) với 1 ≤i≤T−1 được sử dụng để xác định có bao nhiêu mẫu từ các nhiệm vụ đã thấy để điền vào bộ nhớ phát lại tại nhiệm vụ i. Chúng tôi xây dựng một không gian hành động với số lượng rời rạc các lựa chọn tỷ lệ nhiệm vụ có thể được chọn tại mỗi nhiệm vụ: Tại nhiệm vụ t, chúng tôi có t−1 nhiệm vụ lịch sử mà chúng tôi có thể chọn mẫu từ đó. Chúng tôi tạo t−1 thùng bt= [b1, . . . , bt−1] và lấy mẫu một chỉ số nhiệm vụ cho mỗi thùng bi∈ {1, . . . , t−1}. Các thùng được coi là có thể hoán đổi và chúng tôi chỉ giữ các lựa chọn duy nhất. Ví dụ, tại nhiệm vụ 3, chúng tôi đã thấy nhiệm vụ 1 và 2, vì vậy các lựa chọn duy nhất của vector là [1,1],[1,2],[2,2], nơi [1,1] chỉ ra rằng tất cả mẫu bộ nhớ đều từ nhiệm vụ 1, [1,2] chỉ ra rằng một nửa bộ nhớ từ nhiệm vụ 1 và nửa còn lại từ nhiệm vụ 2, v.v. Chúng tôi đếm số lần xuất hiện của mỗi chỉ số nhiệm vụ trong bt và chia cho t−1 để có được tỷ lệ nhiệm vụ, tức là, pt=bincount(bt)/(t−1). Chúng tôi làm tròn số mẫu phát lại từ nhiệm vụ i, tức là, pi·M, lên hoặc xuống tương ứng để giữ kích thước bộ nhớ M cố định khi điền bộ nhớ. Từ đặc tả này, chúng tôi có thể xây dựng một cây các lịch phát lại khác nhau để đánh giá với mạng.

Nhiệm vụ 1
∅
Nhiệm vụ 2
Nhiệm vụ 3
Nhiệm vụ 4
· · ·
 · · ·
 · · ·
 · · ·
Nhiệm vụ 5
· · ·
 · · · · · · · · ·
 · · · · · · · · ·
 · · ·

Hình 2: Không gian hành động hình cây của các bộ nhớ phát lại có thể có với kích thước M= 8 tại mỗi nhiệm vụ từ phương pháp rời rạc hóa được mô tả trong Phần 3.2 cho Split MNIST.

Hình 2 cho thấy một ví dụ về cây lịch phát lại với Split MNIST nơi kích thước bộ nhớ là M= 8. Mỗi cấp tương ứng với một nhiệm vụ CL, và chúng tôi cho thấy một số ví dụ về các bộ nhớ phát lại có thể có trong cây có thể được đánh giá tại mỗi nhiệm vụ. Một lịch phát lại được biểu diễn như một đường duyệt qua các thành phần bộ nhớ phát lại khác nhau từ nhiệm vụ 1 đến nhiệm vụ 5. Tại nhiệm vụ 1, bộ nhớ M1=∅ trống, trong khi M2 được điền với các mẫu từ nhiệm vụ 1 tại nhiệm vụ 2. Bộ nhớ M3 có thể được cấu tạo với các mẫu từ nhiệm vụ 1 hoặc 2, hoặc điền đều M3 với các mẫu từ cả hai nhiệm vụ. Tất cả các đường dẫn có thể trong cây đều là các lịch phát lại hợp lệ. Chúng tôi cho thấy ba ví dụ về các lịch có thể trong Hình 2 để minh họa: đường dẫn màu xanh biểu thị một lịch phát lại nơi chỉ các mẫu nhiệm vụ 1 được phát lại. Đường dẫn màu đỏ biểu thị việc sử dụng bộ nhớ với các nhiệm vụ được phân phối đều, và đường dẫn màu tím biểu thị một lịch nơi bộ nhớ chỉ được điền với các mẫu từ nhiệm vụ trước đó nhất.

Tìm kiếm cây Monte Carlo cho lịch phát lại. Không gian hành động hình cây của tỷ lệ nhiệm vụ tăng nhanh với số lượng nhiệm vụ. Điều này làm phức tạp việc nghiên cứu lập lịch phát lại trong các tập dữ liệu có khoảng thời gian nhiệm vụ dài, nơi không gian hành động quá lớn để sử dụng tìm kiếm toàn diện. Chúng tôi đề xuất sử dụng MCTS vì nó đã thành công trong các ứng dụng với không gian hành động lớn ( Browne et al. ,2012;Chaudhry & Lee ,2018;Silver et al. ,2016). Chúng tôi áp dụng MCTS trong một cài đặt lý tưởng với nhiều lần rollout được phép để chứng minh rằng lập lịch phát lại có thể thiết yếu cho hiệu suất CL cuối cùng, nơi MCTS tập trung tìm kiếm theo hướng có kết quả hứa hẹn trong môi trường CL.

Mỗi thành phần bộ nhớ phát lại trong không gian hành động tương ứng với một nút mà MCTS có thể thăm, như có thể thấy trong Hình 2. Tại cấp t, nút vt liên quan đến một tỷ lệ nhiệm vụ pt được sử dụng để truy xuất thành phần bộ nhớ phát lại từ dữ liệu lịch sử tại nhiệm vụ t. Một lần rollout MCTS tương ứng với việc duyệt qua tất cả các cấp cây 1 , ..., T để chọn lịch phát lại S để sử dụng trong quá trình đào tạo CL. Mỗi tỷ lệ nhiệm vụ pt từ mỗi nút được thăm được lưu trữ trong S trong quá trình rollout. Khi đạt tới cấp T, chúng tôi bắt đầu đào tạo CL và sử dụng S để xây dựng bộ nhớ phát lại tại mỗi nhiệm vụ. Tiếp theo, chúng tôi phác thảo ngắn gọn các bước MCTS để thực hiện tìm kiếm (chi tiết trong Phụ lục A.1):

•Lựa chọn. Trong một lần rollout, nút hiện tại vt hoặc di chuyển ngẫu nhiên đến một nút con chưa thăm, hoặc chọn nút con tiếp theo vt+1 bằng cách đánh giá Upper Confidence Tree (UCT) ( Kocsis & Szepesvári ,2006) với hàm từ Chaudhry & Lee (2018):

UCT(vt, vt+1) = max(q(vt+1)) +C√2 log(n(vt))/n(vt+1), (1)

--- TRANG 6 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

nơi q(·) là hàm phần thưởng, C là hằng số khám phá, và n(·) là số lần thăm nút.

•Mở rộng. Bất cứ khi nào nút hiện tại vt có các nút con chưa thăm, cây tìm kiếm được mở rộng với một trong các nút con chưa thăm vt+1 được chọn bằng lấy mẫu đồng đều.

•Mô phỏng và phần thưởng. Sau khi mở rộng, các nút tiếp theo được chọn ngẫu nhiên cho đến khi đạt nút terminal vT. Các tỷ lệ nhiệm vụ từ các nút rollout được thăm tạo thành lịch phát lại S. Sau khi đào tạo mạng sử dụng S để phát lại, chúng tôi tính phần thưởng cho rollout được cho bởi r=1T∑Ti=1A(val)T,i, nơi A(val)T,i là độ chính xác xác thực của nhiệm vụ i tại nhiệm vụ T.

•Lan truyền ngược. Phần thưởng r được lan truyền ngược từ nút được mở rộng vt đến gốc v1, nơi hàm phần thưởng q(·) và số lần thăm n(·) được cập nhật tại mỗi nút.

3.3 Khung học chính sách cho lập lịch phát lại

Trong phần này, chúng tôi trình bày một khung dựa trên RL để học các chính sách lập lịch phát lại. Chúng tôi tập trung vào việc học một chính sách tổng quát có thể được áp dụng trong bất kỳ kịch bản CL nào để giảm thiểu quên thảm khốc nhằm tránh đào tạo lại chính sách cho mọi tập dữ liệu CL mới. Trực giác của chúng tôi là có thể tồn tại các mẫu tổng quát liên quan đến lập lịch phát lại, ví dụ, rằng các nhiệm vụ khó hơn hoặc đã bị quên nên được phát lại thường xuyên hơn. Chúng tôi nhằm khám phá ngầm các đặc tính nhiệm vụ như vậy bằng cách sử dụng hiệu suất nhiệm vụ của mạng CL làm trạng thái cho chính sách để lựa chọn những nhiệm vụ nào cần phát lại. Biểu diễn các trạng thái với hiệu suất nhiệm vụ cũng cho phép chuyển giao chính sách đã học để giảm quên trong các môi trường CL chưa thấy. Tiếp theo, chúng tôi trình bày phương pháp mô hình hóa của chúng tôi để học các chính sách lập lịch phát lại sử dụng RL.

Môi trường CL. Chúng tôi mô hình hóa các môi trường CL như Quá trình quyết định Markov ( Bellman ,1957) (MDP) nơi mỗi MDP được biểu diễn như một tuple Ei= (Si,A, Pi, Ri, µi, γ) bao gồm không gian trạng thái Si, không gian hành động A, xác suất chuyển trạng thái Pi(s′|s, a), hàm phần thưởng Ri(s, a), phân phối trạng thái ban đầu µi(s1), và hệ số chiết khấu γ. Mỗi môi trường Ei chứa một mạng fφ và các tập dữ liệu nhiệm vụ D1:T nơi tập dữ liệu thứ t được học tại bước thời gian t. Lưu ý rằng mỗi tập dữ liệu nhiệm vụ được chia thành tập huấn luyện, xác thực và kiểm tra.

•Trạng thái: Trạng thái st được định nghĩa là độ chính xác nhiệm vụ At,1:t được đánh giá tại nhiệm vụ t, sao cho st= [At,1, ..., At,t,0, ...,0] nơi zero-padding được sử dụng trên các nhiệm vụ tương lai. Chúng tôi có được các trạng thái bằng cách đánh giá bộ phân loại trên các tập dữ liệu xác thực để tránh overfitting chính sách với dữ liệu huấn luyện.

•Hành động: Chúng tôi sử dụng cùng không gian hành động như cho MCTS (xem Phần 3.2), sao cho at∈ A tương ứng với một tỷ lệ nhiệm vụ pt được sử dụng để lấy mẫu bộ nhớ phát lại Mt.

•Phần thưởng: Chúng tôi sử dụng phần thưởng dày đặc được định nghĩa là độ chính xác xác thực trung bình tại nhiệm vụ t, tức là, rt=1t∑ti=1A(val)t,i, để dễ dàng khám phá trong không gian hành động. Điều này tương tự như các công trình trước đó kết hợp RL với CL ( Liu et al. ,2021;Xu & Zhu ,2018), nơi mục tiêu cho agent là tối đa hóa độ chính xác xác thực nhiệm vụ trong một tập.

Phân phối chuyển trạng thái Pi(s′|s, a) biểu thị động lực của môi trường, phụ thuộc vào khởi tạo của fφ và thứ tự nhiệm vụ trong D1:T.

Đào tạo và đánh giá. Chính sách tương tác với các môi trường CL bằng cách lựa chọn những nhiệm vụ nào mà mạng fφ nên phát lại để giảm thiểu quên thảm khốc. Trạng thái st được lấy bằng cách đánh giá fφ trên các tập xác thực D(val)1:t sau khi học nhiệm vụ t. Hành động at được chọn dưới chính sách πθ(a|st), được tham số hóa bởi θ, được chuyển đổi thành tỷ lệ nhiệm vụ pt để lấy mẫu bộ nhớ phát lại Mt từ các tập dữ liệu lịch sử. Mạng fφ được đào tạo trên nhiệm vụ t+1 trong khi phát lại Mt, và chúng tôi có được phần thưởng rt+1 và trạng thái tiếp theo st+1 bằng cách đánh giá fφ trên các tập xác thực D(val)1:t+1. Các chuyển tiếp được thu thập ( st, at, rt+1, st+1) được sử dụng để cập nhật chính sách, và một tập mới bắt đầu sau khi fφ đã học nhiệm vụ cuối cùng T. Chúng tôi để chính sách tương tác với nhiều môi trường đào tạo E(train)={Ei}Ki=1 được lấy mẫu từ một phân phối môi trường CL, tức là, Ei∼p(E). Để tạo ra các môi trường CL đa dạng, chúng tôi để mỗi Ei có các khởi tạo mạng khác nhau của fφ và thứ tự nhiệm vụ trong các tập dữ liệu. Mục tiêu của chúng tôi là học một chính sách lập lịch phát lại tổng quát có thể được áp dụng trong các môi trường CL mới để giảm thiểu quên thảm khốc. Do đó, trong Phần 4.2, chúng tôi đánh giá chính sách trong các môi trường CL với thứ tự nhiệm vụ mới hoặc tập dữ liệu chưa thấy trong quá trình đào tạo. Chính sách được áp dụng chỉ cho một tập CL duy nhất mà không đào tạo thêm trong môi trường kiểm tra.

--- TRANG 7 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

4 Thí nghiệm

Trong phần này, chúng tôi trình bày kết quả thí nghiệm để cho thấy tầm quan trọng của lập lịch phát lại trong CL. Trước tiên, chúng tôi chứng minh lợi ích với lập lịch phát lại bằng cách sử dụng MCTS để tìm lịch phát lại trong Phần 4.1. Sau đó, chúng tôi đánh giá khung dựa trên RL của chúng tôi sử dụng DQN ( Mnih et al. ,2013) và A2C ( Mnih et al. , 2016) để học các chính sách tổng quát hóa sang các kịch bản CL mới trong Phần 4.2. Chi tiết đầy đủ về cài đặt thí nghiệm và kết quả bổ sung có trong Phụ lục C và D. Mã nguồn được công khai dưới giấy phép MIT¹.

4.1 Kết quả về lập lịch phát lại với tìm kiếm cây Monte Carlo

Trong phần này, chúng tôi cho thấy lợi ích của lập lịch phát lại trong các môi trường CL đơn lẻ sử dụng MCTS. Chúng tôi thực hiện đánh giá toàn diện nơi chúng tôi áp dụng MCTS với các phương pháp lựa chọn bộ nhớ và phát lại khác nhau, thay đổi kích thước bộ nhớ trong các cài đặt CL khác nhau ( Van de Ven & Tolias ,2019), và cho thấy hiệu quả tiềm năng của lập lịch phát lại trong cài đặt bộ nhớ nhỏ.

Thiết lập thí nghiệm. Chúng tôi tiến hành thí nghiệm trên một số tập dữ liệu chuẩn mực CL: Split MNIST ( LeCun et al. ,1998;Zenke et al. ,2017), Split FashionMNIST ( Xiao et al. ,2017), Split notMNIST ( Bulatov ,2011), Permuted MNIST ( Goodfellow et al. ,2013), và Split CIFAR-100 ( Krizhevsky & Hinton ,2009), và Split miniImagenet ( Vinyals et al. ,2016). Chúng tôi sử dụng MLP 2 lớp với 256 đơn vị ẩn cho Split MNIST, Split FashionMNIST, Split notMNIST, và Permuted MNIST. Chúng tôi áp dụng ConvNet từ Schwarz et al. (2018);Vinyals et al. (2016) cho Split CIFAR-100, và ResNet-18 thu gọn từ Lopez-Paz & Ranzato (2017) cho Split miniImagenet. Chúng tôi sử dụng các lớp đầu ra đa đầu và giả định nhãn nhiệm vụ có sẵn tại thời điểm kiểm tra trừ khi được nêu khác, ngoại trừ Permuted MNIST nơi lớp đầu ra đơn đầu được sử dụng. Chúng tôi đo hiệu suất CL sử dụng ACC là độ chính xác kiểm tra trung bình trên các nhiệm vụ và BWT để quên ( Lopez-Paz & Ranzato ,2017), tức là,

ACC =1T∑Ti=1AT,i∈[0,1],BWT =1T−1∑T−1i=1AT,i−Ai,i∈[−1,1], (2)

nơi At,i là độ chính xác kiểm tra cho nhiệm vụ i sau khi học nhiệm vụ t. Chúng tôi so sánh MCTS với các baseline:

•Ngẫu nhiên. Chính sách ngẫu nhiên lựa chọn ngẫu nhiên tỷ lệ nhiệm vụ từ không gian hành động về cách cấu trúc bộ nhớ phát lại tại mỗi nhiệm vụ.

•Lịch nhiệm vụ bằng nhau (ETS). Chính sách lựa chọn tỷ lệ nhiệm vụ bằng nhau sao cho bộ nhớ phát lại nhằm điền bộ nhớ với số lượng mẫu bằng nhau từ mỗi nhiệm vụ đã thấy.

•Heuristic Global Drop (Heur-GD). Chính sách heuristic phát lại các nhiệm vụ có độ chính xác xác thực dưới một ngưỡng nhất định tỷ lệ với độ chính xác xác thực tốt nhất đạt được trên nhiệm vụ.

Heur-GD dựa trên trực giác rằng các nhiệm vụ đã quên nên được phát lại. Bộ nhớ phát lại được điền với M/k mẫu mỗi nhiệm vụ, nơi k là số nhiệm vụ được chọn, nhưng bỏ qua phát lại nếu k= 0. MCTS và Heur-GD ngẫu nhiên lấy mẫu 15% dữ liệu huấn luyện của mỗi nhiệm vụ để sử dụng cho xác thực. Đối với MCTS, kết quả được báo cáo được đánh giá trên tập kiểm tra bằng cách sử dụng lịch phát lại được chọn từ các tập xác thực. Bộ nhớ phát lại Mt có kích thước M được lấy mẫu trước khi học nhiệm vụ t và được phát lại cho mỗi batch trong suốt quá trình học nhiệm vụ t. Kích thước bộ nhớ được đặt thành M= 10 cho Split MNIST, Split FashionMNIST, và Split notMNIST, và M= 100 cho Permuted MNIST, Split CIFAR-100, và Split miniImagenet, trừ khi được nêu khác.

Chúng tôi báo cáo trung bình và độ lệch chuẩn sử dụng 5 hạt giống trên tất cả các tập dữ liệu. Khi báo cáo số liệu, chúng tôi chỉ ra bằng màu đỏ, cam và vàng phương pháp lập lịch tốt nhất thứ 1, 2 và 3 dựa trên giá trị trung bình của nó trên số liệu cho mỗi tập dữ liệu. Phụ lục C chứa chi tiết đầy đủ về cài đặt thí nghiệm và kết quả bổ sung bao gồm các kiểm định t của Welch để đánh giá ý nghĩa thống kê giữa MCTS và các baseline.

Thay đổi kích thước bộ nhớ. Chúng tôi cho thấy rằng phương pháp của chúng tôi có thể cải thiện hiệu suất CL trên các kích thước bộ nhớ khác nhau trong các kịch bản CL khác nhau. Hình 3a cho thấy kết quả trong các kịch bản Học tăng dần theo nhiệm vụ và miền (IL), nơi chúng tôi quan sát rằng MCTS thường có được độ chính xác nhiệm vụ tốt hơn ETS, đặc biệt cho kích thước bộ nhớ nhỏ. Cả MCTS và ETS đều hoạt động tốt hơn Heur-GD khi M tăng, điều này cho thấy rằng Heur-GD yêu cầu điều chỉnh cẩn thận các ngưỡng xác thực. Chúng tôi cung cấp kết quả từ các kiểm định t thống kê trong Phụ lục C.7 để hỗ trợ quan sát của chúng tôi. Chúng tôi cũng thực hiện thí nghiệm trong kịch bản Class-IL

¹Mã: https://github.com/marcusklasson/replay_scheduling

--- TRANG 8 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

8 24 80 120 200 400 800 94 95 96 97 98 99 ACC (%)Split MNIST
8 24 80 120 200 400 800 90 92 94 96 98 Split FashionMNIST
8 24 80 120 200 400 800 90 92 94 96 Split notMNIST
90 270 450 900 2250 70 75 80 85 90
Kích thước bộ nhớ M ACC (%)Permuted MNIST
95 285 475 950 1900 50 55 60 65 70 75 80
Kích thước bộ nhớ M Split CIFAR-100
95 285 475 950 1900 50 55 60 65
Kích thước bộ nhớ M Split miniImagenet

(a) Task/Domain-IL

10 20 40 100 200 30 40 50 60 70 80 ACC (%)Split MNIST
Ngẫu nhiên
ETS
Heur-GD
MCTS (Của chúng tôi) 10 20 40 100 200 30 40 50 60 70 Split FashionMNIST
10 20 40 100 200 30 40 50 60 70 80 Split notMNIST
100 200 400 800 1600 5 10 15
Kích thước bộ nhớ M ACC (%)Split CIFAR-100
100 200 400 800 1600 5 10 15
Kích thước bộ nhớ M Split miniImagenet

(b) Class-IL

Hình 3: So sánh hiệu suất với ACC trên các kích thước bộ nhớ khác nhau cho các phương pháp, nơi (a) cho thấy kết quả trong cài đặt Học tăng dần theo nhiệm vụ và miền (IL), và (b) trong cài đặt Class-IL. Kết quả đã được tính trung bình trên 5 hạt giống. Kết quả cho thấy rằng lập lịch phát lại có thể vượt trội hơn các baseline trên cả tập dữ liệu nhỏ và lớn trên các lựa chọn backbone khác nhau trong các cài đặt CL khác nhau.

nơi nhãn nhiệm vụ vắng mặt. Ở đây, bộ nhớ phát lại luôn được điền với ít nhất 1 mẫu/lớp để tránh quên hoàn toàn các nhiệm vụ không được phát lại. Mỗi phương pháp lập lịch sau đó chọn những nhiệm vụ nào cần phát lại trong số Mrest=M−t·nc mẫu còn lại, nơi nc là số lớp mỗi nhiệm vụ. Hình 3b cho thấy rằng ETS tiếp cận MCTS khi M tăng trên các tập dữ liệu 5-nhiệm vụ. Tuy nhiên, trên Split CIFAR-100 và Split miniImagenet thách thức hơn, MCTS vượt trội hơn ETS rõ ràng khi M tăng. Những kết quả này cho thấy rằng việc chọn lịch phát lại phù hợp là thiết yếu trong các kịch bản CL khác nhau với cả tập dữ liệu nhỏ và lớn trên các lựa chọn backbone khác nhau.

2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
Nhiệm vụ hiện tại Nhiệm vụ được phát lại

Hình 4: Lịch phát lại từ MCTS trên Split CIFAR-100 được hiển thị như biểu đồ bong bóng. Tỷ lệ nhiệm vụ thay đổi động theo thời gian điều này sẽ khó thay thế bằng heuristics.

Trực quan hóa lịch phát lại. Chúng tôi trực quan hóa một lịch phát lại từ Split CIFAR-100 với kích thước bộ nhớ M= 100 để có cái nhìn sâu sắc về hành vi của chính sách lập lịch từ MCTS. Hình 4 cho thấy biểu đồ bong bóng của các tỷ lệ nhiệm vụ được chọn được sử dụng để điền bộ nhớ phát lại tại mỗi nhiệm vụ. Mỗi màu vòng tròn tương ứng với một nhiệm vụ phát lại, và kích thước của nó biểu thị tỷ lệ mẫu phát lại tại nhiệm vụ hiện tại. Tổng các điểm trong tất cả các vòng tròn tại mỗi cột được cố định tại tất cả các nhiệm vụ hiện tại. Tỷ lệ nhiệm vụ thay đổi động theo thời gian theo cách phi tuyến phức tạp điều này sẽ khó thay thế bằng phương pháp heuristic. Hơn nữa, chúng tôi có thể quan sát lập lịch kiểu lặp lại có khoảng cách trên nhiều nhiệm vụ, ví dụ, nhiệm vụ 1-3 được phát lại với tỷ lệ tương tự tại các nhiệm vụ ban đầu nhưng cuối cùng bắt đầu thay đổi khoảng thời gian giữa các lần phát lại. Ngoài ra, nhiệm vụ 4 và 6 cần ít phát lại hơn trong giai đoạn đầu, điều này có thể là do chúng đơn giản hơn hoặc có tương quan với các nhiệm vụ khác. Phụ lục C.3 cho thấy một trực quan hóa tương tự cho Split MNIST.

Lập lịch với các phương pháp lựa chọn bộ nhớ khác nhau. Chúng tôi cho thấy rằng phương pháp của chúng tôi có thể được kết hợp với bất kỳ phương pháp lựa chọn bộ nhớ nào để lưu trữ mẫu phát lại. Ngoài lấy mẫu đồng đều, chúng tôi áp dụng các phương pháp lựa chọn bộ nhớ khác nhau thường được sử dụng trong tài liệu CL, cụ thể là phân cụm k-means và Mean-of-Features (MoF) ( Rebuffi et al. ,2017). Bảng 1 cho thấy kết quả trên tất cả các tập dữ liệu. Chúng tôi chỉ ra bằng màu đỏ, cam và vàng phương pháp hoạt động tốt nhất thứ 1, 2 và 3 dựa trên giá trị trung bình của nó trên số liệu. Trong Phụ lục C.4, chúng tôi cung cấp kết quả bổ sung từ lựa chọn k-center ( Nguyen et al. ,2018) và kiểm định t, và cũng cho thấy rằng chuyển giao tiến cho các nhiệm vụ mới là không đáng kể cho mỗi phương pháp khi sử dụng lựa chọn đồng đều trong thí nghiệm này. Chúng tôi lưu ý rằng việc sử dụng lịch phát lại từ MCTS vượt trội hơn các baseline khi sử dụng các phương pháp lựa chọn thay thế, nơi MoF hoạt động tốt nhất trên hầu hết các tập dữ liệu.

--- TRANG 9 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Bảng 1: So sánh hiệu suất giữa MCTS (Của chúng tôi) và các baseline với các phương pháp lựa chọn bộ nhớ khác nhau, cụ thể là lấy mẫu đồng đều, k-means, và Mean-of-Features (MoF). Chúng tôi báo cáo trung bình và độ lệch chuẩn của ACC và BWT được tính trung bình trên 5 hạt giống. MCTS hoạt động tốt hơn hoặc ngang bằng với các baseline trên hầu hết các tập dữ liệu và phương pháp lựa chọn, nơi MoF cho kết quả tốt nhất nói chung.

Split MNIST Split FashionMNIST Split notMNIST
Bộ nhớ Lịch ACC (%) BWT (%) ACC (%) BWT (%) ACC (%) BWT (%)
Offline Joint 99.75 ±0.06 0.01 ±0.06 99.34 ±0.08 -0.01 ±0.14 96.12 ±0.57 -0.21 ±0.71
Đồng đều Ngẫu nhiên 94.91 ±2.52 -6.13 ±3.16 95.89 ±2.03 -4.33 ±2.55 91.84 ±1.48 -5.37 ±2.12
ETS 94.02 ±4.25 -7.22 ±5.33 95.81 ±3.53 -4.45 ±4.34 91.01 ±1.39 -6.16 ±1.82
Heur-GD 96.02 ±2.32 -4.64 ±2.90 97.09 ±0.62 -2.82 ±0.84 91.26 ±3.99 -6.06 ±4.70
MCTS (Của chúng tôi) 97.93 ±0.56 -2.27 ±0.71 98.27 ±0.17 -1.29 ±0.20 94.64 ±0.39 -1.47 ±0.79
k-means Ngẫu nhiên 92.65 ±1.38 -8.96 ±1.74 93.11 ±2.75 -7.76 ±3.42 93.11 ±1.01 -3.78 ±1.43
ETS 92.89 ±3.53 -8.66 ±4.42 96.47 ±0.85 -3.55 ±1.07 93.80 ±0.82 -2.84 ±0.81
Heur-GD 96.28 ±1.68 -4.32 ±2.11 95.78 ±1.50 -4.46 ±1.87 91.75 ±0.94 -5.60 ±2.07
MCTS (Của chúng tôi) 98.20 ±0.16 -1.94 ±0.22 98.48 ±0.26 -1.04 ±0.31 93.61 ±0.71 -3.11 ±0.55
MoF Ngẫu nhiên 96.96 ±1.34 -3.57 ±1.69 96.39 ±1.69 -3.66 ±2.17 93.09 ±1.40 -3.70 ±1.76
ETS 97.04 ±1.23 -3.46 ±1.50 96.48 ±1.33 -3.55 ±1.73 92.64 ±0.87 -4.57 ±1.59
Heur-GD 96.46 ±2.41 -4.09 ±3.01 95.84 ±0.89 -4.39 ±1.15 93.24 ±0.77 -3.48 ±1.37
MCTS (Của chúng tôi) 98.37 ±0.24 -1.70 ±0.28 97.84 ±0.32 -1.81 ±0.39 94.62 ±0.42 -1.80 ±0.56

Permuted MNIST Split CIFAR-100 Split miniImagenet
Bộ nhớ Lịch ACC (%) BWT (%) ACC (%) BWT (%) ACC (%) BWT (%)
Offline Joint 95.34 ±0.13 0.17 ±0.18 84.73 ±0.81 -1.06 ±0.81 74.03 ±0.83 9.70 ±0.68
Đồng đều Ngẫu nhiên 72.59 ±1.52 -25.71 ±1.76 53.76 ±1.80 -35.11 ±1.93 49.89 ±1.03 -14.79 ±1.14
ETS 71.09 ±2.31 -27.39 ±2.59 47.70 ±2.16 -41.69 ±2.37 46.97 ±1.24 -18.32 ±1.34
Heur-GD 76.68 ±2.13 -20.82 ±2.41 57.31 ±1.21 -30.76 ±1.45 49.66 ±1.10 -12.04 ±0.59
MCTS (Của chúng tôi) 76.34 ±0.98 -21.21 ±1.16 56.60 ±1.13 -31.39 ±1.11 50.20 ±0.72 -13.46 ±1.22
k-means Ngẫu nhiên 71.91 ±1.24 -26.45 ±1.34 53.20 ±1.44 -35.77 ±1.31 49.96 ±1.46 -14.81 ±1.18
ETS 69.40 ±1.32 -29.23 ±1.47 47.51 ±1.14 -41.77 ±1.30 45.82 ±0.92 -19.53 ±1.10
Heur-GD 75.57 ±1.18 -22.11 ±1.22 54.31 ±3.94 -33.80 ±4.24 49.25 ±1.00 -12.92 ±1.22
MCTS (Của chúng tôi) 77.74 ±0.80 -19.66 ±0.95 56.95 ±0.92 -30.92 ±0.83 50.47 ±0.85 -13.31 ±1.24
MoF Ngẫu nhiên 78.80 ±1.07 -18.79 ±1.16 62.35 ±1.24 -26.33 ±1.25 56.02 ±1.11 -7.99 ±1.13
ETS 77.62 ±1.12 -20.10 ±1.26 60.43 ±1.17 -28.22 ±1.26 56.12 ±1.12 -8.93 ±0.83
Heur-GD 77.27 ±1.45 -20.15 ±1.63 55.60 ±2.70 -32.57 ±2.77 52.30 ±0.59 -9.61 ±0.67
MCTS (Của chúng tôi) 81.58 ±0.75 -15.41 ±0.86 64.22 ±0.65 -23.48 ±1.02 57.70 ±0.51 -5.31 ±0.55

Bảng 2: So sánh hiệu suất giữa các phương pháp lập lịch MCTS (Của chúng tôi), Ngẫu nhiên, ETS, và Heuristic kết hợp với các phương pháp phát lại HAL, MER, và DER. Chúng tôi báo cáo trung bình và độ lệch chuẩn của ACC và BWT được tính trung bình trên 5 hạt giống. ∗ biểu thị kết quả nơi một số hạt giống không hội tụ. Áp dụng MCTS cho mỗi phương pháp có thể vượt trội hơn cùng phương pháp sử dụng lịch baseline.

Split MNIST Split CIFAR-100 Split miniImagenet
Phương pháp Lịch ACC (%) BWT (%) ACC (%) BWT (%) ACC (%) BWT (%)
HAL Ngẫu nhiên 96.32 ±1.77 -3.90 ±2.28 35.90 ±2.47 -17.37 ±3.76 40.86 ±1.86 -5.12 ±2.23
ETS 97.21 ±1.25 -2.80 ±1.59 34.90 ±2.02 -18.92 ±0.91 38.13 ±1.18 -8.19 ±1.73
Heur-GD 97.69 ±0.19 -2.22 ±0.24 35.07 ±1.29 -24.76 ±2.41 39.51 ±1.49 -5.65 ±0.77
MCTS (Của chúng tôi) 97.96 ±0.15 -1.85 ±0.18 40.22 ±1.57 -12.77 ±1.30 41.39 ±1.15 -3.69 ±1.86
MER Ngẫu nhiên 93.00 ±3.22 -7.96 ±4.15 42.68 ±0.86 -35.56 ±1.39 32.86 ±0.95 -7.71 ±0.45
ETS 92.97 ±1.73 -8.52 ±2.15 43.38 ±1.81 -34.84 ±1.98 33.58 ±1.53 -6.80 ±1.46
Heur-GD 94.30 ±2.79 -6.46 ±3.50 40.90 ±1.70 -44.10 ±2.03 34.22 ±1.93 -7.57 ±1.63
MCTS (Của chúng tôi) 96.44 ±0.72 -4.14 ±0.94 44.29 ±0.69 -32.73 ±0.88 32.74 ±1.29 -5.77 ±1.04
DER Ngẫu nhiên 95.91 ±2.18 -4.40 ±2.46 56.17 ±1.30 -29.03 ±1.38 35.13 ±4.11 -10.85 ±2.92
ETS 98.17 ±0.35 -2.00 ±0.42 52.58 ±1.49 -32.93 ±2.04 35.50 ±2.84 -10.94 ±2.21
Heur-GD 94.57 ±1.71 -6.08 ±2.09 55.75 ±1.08 -31.27 ±1.02 43.62 ±0.88 -8.18 ±1.16
MCTS (Của chúng tôi) 99.02 ±0.10 -0.91 ±0.13 58.99 ±0.98 -24.95 ±0.64 43.46 ±0.95 -9.32 ±1.37

Áp dụng lập lịch cho các phương pháp phát lại khác nhau. Trong thí nghiệm này, chúng tôi cho thấy rằng lập lịch phát lại có thể được kết hợp với bất kỳ phương pháp phát lại nào để tăng cường hiệu suất CL. Chúng tôi kết hợp MCTS với Hindsight Anchor Learning (HAL) ( Chaudhry et al. ,2021), Meta-Experience Replay (MER) ( Riemer et al. ,2019), Dark Experience Replay (DER) ( Buzzega et al. ,2020). Bảng 2 cho thấy so sánh hiệu suất giữa lập lịch MCTS của chúng tôi với việc sử dụng lịch Ngẫu nhiên, ETS, và Heuristic cho mỗi phương pháp. Chúng tôi chỉ ra bằng màu đỏ, cam và vàng phương pháp phát lại hoạt động tốt nhất thứ 1, 2 và 3 dựa trên giá trị trung bình của nó

--- TRANG 10 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

trên số liệu. Trong Phụ lục C.5, chúng tôi cung cấp các siêu tham số cho mỗi phương pháp phát lại và kết quả từ các kiểm định t thống kê. Kết quả xác nhận rằng lập lịch phát lại quan trọng đối với hiệu suất cuối cùng với cùng ràng buộc bộ nhớ và nó có thể có lợi cho bất kỳ khung CL hiện có nào.

Hiệu quả của lập lịch phát lại. Chúng tôi minh họa hiệu quả của lập lịch phát lại trong cài đặt nơi chỉ 1 mẫu/lớp có sẵn từ dữ liệu lịch sử để phát lại. Chúng tôi xem xét kịch bản nơi kích thước bộ nhớ phát lại nhỏ hơn số lượng lớp. Kích thước bộ nhớ phát lại cho MCTS được đặt thành M= 2 cho các tập dữ liệu 5-nhiệm vụ, sao cho chỉ 2 mẫu có thể được chọn để phát lại từ các nhiệm vụ đã thấy. Đối với các tập dữ liệu CL lớn hơn, chúng tôi đặt M= 50. Sau đó chúng tôi so sánh với các baseline CL tiết kiệm bộ nhớ A-GEM ( Chaudhry et al. ,2018b ) và ER-Ring ( Chaudhry et al. ,2019), cũng như lựa chọn bộ nhớ đồng đều. Chúng tôi để các baseline này tăng kích thước bộ nhớ của chúng, sao cho 1 mẫu/lớp được lưu trữ và sử dụng để phát lại. Bảng 3 cho thấy hiệu suất giữa MCTS và các baseline, nơi chúng tôi chỉ ra bằng màu đỏ, cam và vàng phương pháp hoạt động tốt nhất thứ 1, 2 và 3 dựa trên giá trị trung bình của nó trên số liệu. Mặc dù sử dụng ít mẫu hơn đáng kể để phát lại, lịch MCTS hoạt động chủ yếu ngang bằng với các baseline và vượt trội hơn chúng trên Permuted MNIST. Trong Phụ lục C.6, chúng tôi cho thấy rằng MCTS không khác biệt thống kê so với các baseline trên hầu hết các tập dữ liệu theo các kiểm định t thống kê. Những kết quả này cho thấy rằng lập lịch phát lại là một hướng nghiên cứu quan trọng trong CL, vì việc lưu trữ mọi lớp đã thấy trong bộ nhớ có thể không hiệu quả trong các cài đặt có số lượng lớn các nhiệm vụ.

Bảng 3: So sánh hiệu suất trong cài đặt bộ nhớ nơi chỉ 1 mẫu/lớp có sẵn từ dữ liệu lịch sử để phát lại. Các baseline phát lại tất cả mẫu có sẵn, trong khi MCTS chọn 2 mẫu cho Split MNIST và 50 mẫu cho Permuted MNIST và Split miniImagenet. MCTS hoạt động ngang bằng với các baseline tốt nhất trên Split MNIST và miniImagenet, và hoạt động tốt nhất trên Permuted MNIST.

Split MNIST Permuted MNIST Split miniImagenet
Phương pháp ACC (%) BWT (%) ACC (%) BWT (%) ACC (%) BWT (%)
Ngẫu nhiên 92.56 ±2.90 -8.97 ±3.62 70.02 ±1.76 -28.22 ±1.92 48.85 ±1.38 -14.55 ±1.86
A-GEM 94.97 ±1.50 -6.03 ±1.87 64.71 ±1.78 -34.41 ±2.05 32.06 ±1.83 -30.81 ±1.79
ER-Ring 94.94 ±1.56 -6.07 ±1.92 69.73 ±1.13 -28.87 ±1.29 49.82 ±1.69 -14.38 ±1.57
Đồng đều 95.77 ±1.12 -5.02 ±1.39 69.85 ±1.01 -28.74 ±1.17 50.56 ±1.07 -13.52 ±1.34
MCTS (Của chúng tôi) 96.07 ±1.60 -4.59 ±2.01 72.52 ±0.54 -25.43 ±0.65 50.70 ±0.54 -12.60 ±1.13

Thí nghiệm bổ sung. Chúng tôi tiếp tục chứng minh lợi ích của việc phát lại các nhiệm vụ cụ thể trong CL bằng cách i) cho thấy rằng các lịch phát lại được tìm thấy bởi MCTS có thể giảm thiểu quên thảm khốc một cách hiệu quả với các mẫu phát lại khác nhau từ các nhiệm vụ được lập lịch (xem Phụ lục C.8), và ii) so sánh các lịch MCTS với ETS kết hợp với Heur-GD để gán tỷ lệ cao hơn cho các nhiệm vụ khó (xem Phụ lục C.9).

4.2 Tổng quát hóa chính sách sang các kịch bản học tập liên tục mới

Trong phần này, chúng tôi đánh giá mức độ tốt của các chính sách lập lịch phát lại đã học có thể giảm thiểu quên thảm khốc trong các môi trường CL mới. Chúng tôi sử dụng DQN và A2C để học chính sách và đánh giá khả năng tổng quát hóa của chúng trong các môi trường CL với thứ tự nhiệm vụ mới và tập dữ liệu chưa thấy trong quá trình đào tạo.

Thiết lập thí nghiệm. Chúng tôi tiến hành thí nghiệm trên các tập dữ liệu 5-nhiệm vụ Split MNIST, Split FashionMNIST, Split notMNIST, và Split CIFAR-10 ( Krizhevsky & Hinton ,2009). Cài đặt CL nói chung giống như trong Phần 4.1. Chúng tôi đánh giá tất cả các phương pháp trên 10 môi trường kiểm tra khác nhau, và đánh giá khả năng tổng quát hóa bằng cách xếp hạng tất cả các phương pháp bằng cách so sánh ACC đo được của chúng trên mỗi hạt giống trong mỗi môi trường kiểm tra, vì hiệu suất giữa các môi trường có thể thay đổi đáng kể (chi tiết trong Phụ lục D.2). Chính sách được áp dụng chỉ cho một lần duyệt qua các nhiệm vụ CL tại thời điểm kiểm tra. Chúng tôi thêm hai baseline:

•Heuristic Local Drop (Heur-LD). Chính sách heuristic phát lại các nhiệm vụ có độ chính xác xác thực dưới một ngưỡng tỷ lệ với độ chính xác xác thực trước đó đã đạt được trên nhiệm vụ.

•Heuristic Accuracy Threshold (Heur-AT). Chính sách heuristic phát lại các nhiệm vụ có độ chính xác xác thực dưới một ngưỡng cố định.

Kích thước bộ nhớ được đặt thành M= 10, và chúng tôi tính trung bình kết quả trên 5 hạt giống. Trong kết quả, chúng tôi chỉ ra bằng màu đỏ, cam và vàng phương pháp lập lịch hoạt động tốt nhất thứ 1, 2 và 3 dựa trên giá trị trung bình của nó cho mỗi số liệu. Trong Phụ lục D, chúng tôi cung cấp chi tiết đầy đủ về cài đặt thí nghiệm và kết quả bổ sung

--- TRANG 11 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Bảng 4: So sánh hiệu suất được đo bằng xếp hạng trung bình (Rank) và ACC (%) giữa các chính sách lập lịch trong thí nghiệm tổng quát hóa Thứ tự nhiệm vụ mới. Kết quả được tính trung bình trên 10 môi trường kiểm tra. Các chính sách đã học của chúng tôi sử dụng DQN và A2C hoạt động cạnh tranh với các chính sách cố định (Ngẫu nhiên và ETS) và các heuristics (Heur) trên các tập dữ liệu 5-nhiệm vụ.

S-MNIST S-FashionMNIST S-notMNIST S-CIFAR-10
Lịch Rank ( ↓) ACC ( ↑) Rank ( ↓) ACC ( ↑) Rank ( ↓) ACC ( ↑) Rank ( ↓) ACC ( ↑)
Ngẫu nhiên 3.98 91.8 ±4.7 3.68 93.9 ±3.8 3.68 91.7 ±2.8 4.91 81.6 ±6.1
ETS 3.82 91.8 ±5.0 4.74 93.5 ±3.0 4.44 90.4 ±3.1 5.38 81.0 ±5.9
Heur-GD 4.53 91.3 ±4.3 4.33 91.8 ±7.9 3.44 92.3 ±1.5 4.03 83.2 ±5.0
Heur-LD 4.67 91.0 ±4.1 3.63 93.8 ±5.0 3.96 91.7 ±2.0 3.63 83.5 ±4.2
Heur-AT 4.38 91.5 ±4.0 4.16 92.9 ±4.9 5.50 89.7 ±2.9 3.43 83.7 ±4.3
DQN (Của chúng tôi) 3.46 93.0 ±2.7 3.78 94.2 ±3.8 3.51 91.9 ±2.5 3.83 83.0 ±4.3
A2C (Của chúng tôi) 3.16 93.1 ±3.7 3.68 94.8 ±3.3 3.47 92.1 ±1.8 2.79 83.9 ±3.8

1 2 3 4 5 70 80 90 100
Nhiệm vụ Độ chính xác (%) ACC: 89.75%
2 3 4 5 1 2 3 4
Nhiệm vụ hiện tại Nhiệm vụ được phát lại Lịch phát lại

(a) A2C

1 2 3 4 5 70 80 90 100
Nhiệm vụ Độ chính xác (%) ACC: 88.32%
T1
T2
T3
T4
T5
2 3 4 5 1 2 3 4
Nhiệm vụ hiện tại Nhiệm vụ được phát lại Lịch phát lại

(b) ETS

Hình 5: Độ chính xác nhiệm vụ và lịch phát lại cho A2C và ETS cho một môi trường Split CIFAR-10. Lịch phát lại được hiển thị như biểu đồ bong bóng và từ 1 hạt giống. Chính sách đã học bởi A2C có thể linh hoạt hơn ETS xem xét phát lại các nhiệm vụ đã quên, chẳng hạn như Nhiệm vụ 2, để tăng cường hiệu suất CL.

bao gồm các kiểm định t của Welch để đánh giá ý nghĩa thống kê giữa khung RL của chúng tôi và các baseline. Chúng tôi cũng trình bày kết quả baseline với ETS kết hợp với các heuristics trong Phụ lục D.5 do hạn chế không gian.

Tổng quát hóa sang thứ tự nhiệm vụ mới. Chúng tôi cho thấy rằng các chính sách lập lịch phát lại đã học có thể tổng quát hóa sang các môi trường CL với thứ tự nhiệm vụ chưa thấy trước đó. Các môi trường đào tạo và kiểm tra được tạo ra với thứ tự nhiệm vụ duy nhất của các tập dữ liệu CL. Bảng 4 cho thấy xếp hạng trung bình và ACC cho DQN, A2C, và các baseline khi được áp dụng trong 10 môi trường kiểm tra. Các chính sách đã học của chúng tôi có được xếp hạng trung bình tốt nhất trên hầu hết các tập dữ liệu, nơi A2C hoạt động tốt hơn DQN nói chung. Để cung cấp thêm cái nhìn sâu sắc, Hình 5 cho thấy tiến triển độ chính xác nhiệm vụ và lịch phát lại tương ứng từ A2C và ETS từ một môi trường kiểm tra Split CIFAR-10. trong Hình 5. Lịch phát lại được hiển thị với biểu đồ bong bóng cho thấy tỷ lệ nhiệm vụ được chọn để sử dụng để cấu tạo bộ nhớ phát lại tại mỗi nhiệm vụ. Trong Hình 5a, chúng tôi quan sát rằng A2C quyết định phát lại nhiệm vụ 2 nhiều hơn nhiệm vụ 1 khi hiệu suất trên nhiệm vụ 2 giảm, điều này dẫn đến số liệu ACC hơi tốt hơn mà A2C đạt được so với ETS. Những kết quả này cho thấy rằng chính sách đã học có thể linh hoạt xem xét phát lại các nhiệm vụ đã quên để tăng cường hiệu suất CL.

Bảng 5: Xếp hạng và ACC giữa các chính sách lập lịch trong thí nghiệm Tập dữ liệu mới. Các chính sách của chúng tôi tổng quát hóa tốt trên S-notMNIST, nhưng bị vượt trội trên S-FashionMNIST.

S-notMNIST S-FashionMNIST
Lịch Rank ( ↓) ACC ( ↑) Rank ( ↓) ACC ( ↑)
Ngẫu nhiên 3.94 91.4 ±2.7 4.05 92.6 ±4.7
ETS 4.06 91.5 ±1.7 3.84 92.8 ±3.7
Heur-GD 4.61 89.8 ±4.8 3.08 94.1 ±2.8
Heur-LD 4.96 89.1 ±4.7 4.96 90.9 ±4.1
Heur-AT 4.29 90.2 ±4.9 3.83 91.9 ±7.0
DQN (Của chúng tôi) 3.40 91.7 ±3.2 4.12 92.2 ±5.3
A2C (Của chúng tôi) 2.74 92.6 ±1.6 4.12 92.1 ±5.5

Tổng quát hóa sang tập dữ liệu mới. Chúng tôi cho thấy rằng các chính sách lập lịch phát lại đã học có khả năng tổng quát hóa sang các môi trường CL với tập dữ liệu mới chưa thấy trong các môi trường đào tạo. Chúng tôi thực hiện hai bộ thí nghiệm với các môi trường đào tạo và kiểm tra khác nhau để đánh giá chính sách đã học:

1. S-notMNIST: đào tạo với các môi trường được tạo ra với Split MNIST và FashionMNIST và kiểm tra trên các môi trường được tạo ra với Split notMNIST.

2. S-FashionMNIST: đào tạo với các môi trường được tạo ra với Split MNIST và notMNIST và kiểm tra trên các môi trường được tạo ra với Split FashionMNIST.

Bảng 5 cho thấy xếp hạng trung bình và ACC cho DQN, A2C, và các baseline khi tổng quát hóa sang các môi trường kiểm tra với tập dữ liệu mới. Chúng tôi quan sát rằng cả A2C và DQN đều thành công tổng quát hóa sang Split

--- TRANG 12 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

notMNIST so với các baseline. Tuy nhiên, các chính sách RL đã học gặp khó khăn trong việc tổng quát hóa sang các môi trường Split FashionMNIST, điều này có thể do sự biến đổi cao trong động lực chuyển trạng thái, tức là, độ chính xác nhiệm vụ, giữa các môi trường đào tạo và kiểm tra. Điều này cho thấy rằng việc học các chính sách lập lịch phát lại sử dụng RL kế thừa các thách thức chung với tổng quát hóa trong RL, chẳng hạn như tính mạnh mẽ đối với sự thay đổi miền. Có thể, hiệu suất có thể được cải thiện bằng cách tạo ra nhiều môi trường đào tạo hơn cho agent để thể hiện nhiều biến thể của các kịch bản CL hơn, hoặc bằng cách sử dụng các phương pháp RL tiên tiến khác, ví dụ, các kỹ thuật chính quy hóa ( Igl et al. ,2019) hoặc tinh chỉnh trực tuyến ( Nair et al. ,2020), có thể tổng quát hóa tốt hơn.

5 Kết luận

Chúng tôi đề xuất học thời điểm học, tức là, trong bối cảnh CL thực tế, học các lịch về những nhiệm vụ nào cần phát lại tại các thời điểm khác nhau. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên xem xét thời điểm học trong CL được lấy cảm hứng từ các kỹ thuật học của con người. Chúng tôi chứng minh lợi ích với lập lịch phát lại trong CL bằng cách cho thấy trên một số chuẩn mực CL rằng các lịch phát lại được tìm thấy với MCTS có thể vượt trội hơn việc phát lại tất cả các nhiệm vụ đồng đều hoặc dựa vào các quy tắc lập lịch heuristic. Hơn nữa, chúng tôi đề xuất một khung dựa trên RL để học các chính sách lập lịch như một bước hướng tới việc cho phép lập lịch phát lại trong các cài đặt CL thực tế. Các chính sách đã học bất khả tri đối với tập dữ liệu CL, và có thể được áp dụng để giảm quên thảm khốc trong các kịch bản CL mới mà không cần đào tạo thêm. Phương pháp lập lịch phát lại của chúng tôi đưa nghiên cứu hiện tại gần hơn với việc giải quyết các thách thức CL thực tế nơi số lượng nhiệm vụ vượt quá kích thước bộ nhớ phát lại.

Hạn chế và công việc tương lai. Tổng quát hóa trong RL là một chủ đề nghiên cứu thách thức tự thân. Với phương pháp hiện tại, lượng lớn dữ liệu đa dạng và thời gian đào tạo được yêu cầu để cho phép chính sách đã học tổng quát hóa tốt. Điều này có thể tốn kém vì việc tạo ra các môi trường CL đắt đỏ do mỗi chuyển trạng thái liên quan đến việc đào tạo mạng trên một nhiệm vụ CL. Hơn nữa, chúng tôi hiện đang xem xét một không gian hành động rời rạc khó xây dựng, đặc biệt trong các kịch bản CL quy mô lớn. Do đó, trong công việc tương lai, chúng tôi sẽ khám phá các phương pháp RL tiên tiến hơn có thể xử lý các hành động liên tục và tổng quát hóa tốt.

Cuối cùng, chúng tôi nghiên cứu lập lịch phát lại dưới cài đặt CL dựa trên nhiệm vụ nơi các thay đổi nhiệm vụ được biết. Hơn nữa, chúng tôi giả định rằng số lượng nhiệm vụ cần học được biết trước và các tập xác thực có thể truy cập tại thời điểm kiểm tra. Mở rộng lập lịch phát lại sang các cài đặt CL không có nhiệm vụ và trực tuyến vẫn là một câu hỏi nghiên cứu mở, nơi đánh giá liên tục ( De Lange et al. ,2023) độ chính xác lớp có thể cần thiết để lựa chọn bộ nhớ phát lại hiệu quả. Chúng tôi hy vọng rằng lợi ích đã chứng minh của chúng tôi với lập lịch phát lại trong CL sẽ khuyến khích nhiều nghiên cứu hơn theo hướng này.

Tuyên bố tác động rộng

Mối quan ngại xung quanh các vấn đề riêng tư đã được nêu ra trong tài liệu CL trong các cài đặt nơi dữ liệu lịch sử được lưu trữ để phát lại các nhiệm vụ theo thời gian. Như đã thảo luận bởi Prabhu et al. (2023a ), các mạng nơ-ron sâu đã được chứng minh là có khả năng tái tạo dữ liệu đào tạo của chúng ( Haim et al. ,2022), có nghĩa là việc loại bỏ quyền truy cập vào dữ liệu lịch sử không thể giải quyết các vấn đề riêng tư tiềm ẩn trong CL một mình. Tuy nhiên, lập lịch phát lại có thể được kết hợp với bất kỳ phương pháp phát lại bảo vệ riêng tư nào, ví dụ, các phương pháp phát lại các đặc trưng nén ( Hayes et al. ,2020) hoặc dữ liệu tổng hợp được tạo ra ( Shin et al. ,2017) thay vì dữ liệu thô. Hơn nữa, mặc dù việc học các mối quan hệ nhiệm vụ có thể có lợi, khung RL được đề xuất của chúng tôi học một chính sách bất khả tri tập dữ liệu để lựa chọn những nhiệm vụ nào cần phát lại chỉ từ các số liệu hiệu suất CL.

Một phương pháp lập lịch có thể mở rộng trong CL dựa trên phát lại sẽ hữu ích để giảm tính toán trong các ứng dụng thực tế nhưng hiện đang thiếu. Vì việc lưu trữ dữ liệu lịch sử nói chung có thể chi trả được, các phương pháp lập lịch để lựa chọn bộ nhớ phát lại nhỏ giảm thiểu quên thảm khốc một cách hiệu quả là thiết yếu cho các hệ thống CL. Phát lại tất cả các nhiệm vụ trước đó một cách đồng đều có thể không hiệu quả, điều này đã được chứng minh cho việc học của con người ( Hawley et al. ,2008), và việc tạo ra các quy tắc lập lịch heuristic có thể khó để làm cho chúng tổng quát hóa sang các kịch bản mới. Do đó, trong bài báo này, chúng tôi đề xuất rằng các hệ thống CL cần học thời điểm học từ các nhiệm vụ cũ, trong đó chúng tôi học một chính sách lập lịch để lựa chọn những nhiệm vụ nào cần phát lại tại các thời điểm khác nhau. Vì việc duy trì hiệu suất chấp nhận được trên một số lượng lớn nhiệm vụ có thể khó khăn trong các cài đặt hạn chế tính toán, chúng tôi tin rằng lập lịch phát lại là một hướng nghiên cứu quan trọng để cho phép các ứng dụng CL thực tế.

--- TRANG 13 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Đóng góp của tác giả

CZ đưa ra ý tưởng, và MK và CZ đóng góp vào việc chính thức hóa phương pháp luận. MK thực hiện tất cả các thí nghiệm, tạo ra các hình ảnh trực quan, và viết hầu hết văn bản. Tất cả tác giả tham gia thảo luận kết quả và đóng góp vào việc viết bản thảo.

Lời cảm ơn

Nghiên cứu này được tài trợ bởi Quỹ Promobilia (cấp nr F-16500) trong khi tác giả đầu tiên đang tại KTH, và nó được hoàn thành trong khi được tài trợ bởi Trung tâm Trí tuệ nhân tạo Phần Lan (FCAI). Chúng tôi muốn cảm ơn (theo thứ tự bảng chữ cái) Arno Solin, Christian Pek, Sofia Broomé, Ruibo Tu, và Truls Nyberg vì phản hồi về bản thảo. Chúng tôi cũng cảm ơn Sam Devlin vì các cuộc thảo luận sớm về phần học tăng cường của bài báo. Cuối cùng, chúng tôi cảm ơn các nhà đánh giá ẩn danh vì những gợi ý và phản hồi hữu ích của họ.

Tài liệu tham khảo

Tameem Adel, Han Zhao, và Richard E Turner. Continual learning with adaptive weights (claw). Trong International Conference on Learning Representations, 2019.

Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, và Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. Advances in neural information processing systems, 32, 2019a.

Rahaf Aljundi, Min Lin, Baptiste Goujaud, và Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019b.

Hadi Amiri, Timothy Miller, và Guergana Savova. Repeat before forgetting: Spaced repetition for efficient and effective training of neural networks. Trong Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2401–2410, 2017.

Peter Bailis, Edward Gan, Samuel Madden, Deepak Narayanan, Kexin Rong, và Sahaana Suri. MacroBase: Prioritizing attention in fast data. Trong Proceedings of the 2017 ACM International Conference on Management of Data, pp. 541–556, 2017.

Philip J Ball, Yingzhen Li, Angus Lamb, và Cheng Zhang. A study on efficiency in continual learning inspired by human learning. NeurIPS 2020 Workshop on BabyMind, 2020.

Philip J Ball, Cong Lu, Jack Parker-Holder, và Stephen Roberts. Augmented world models facilitate zero-shot dynamics generalization from a single offline environment. Trong International Conference on Machine Learning, pp. 619–629. PMLR, 2021.

Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, và Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8218–8227, 2021.

Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):679–684, 1957.

Zalán Borsos, Mojmir Mutny, và Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. Advances in neural information processing systems, 33:14879–14890, 2020.

Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, và Simon Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1–43, 2012.

Yaroslav Bulatov. The notMNIST dataset. http://yaroslavvb.com/upload/notMNIST/, 2011.

Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, và Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920–15930, 2020.

--- TRANG 14 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, và Philip Thomas. Learning action representations for reinforcement learning. Trong International conference on machine learning, pp. 941–950. PMLR, 2019.

Yash Chandak, Georgios Theocharous, Chris Nota, và Philip Thomas. Lifelong learning with a changing action set. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pp. 3373–3380, 2020.

Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, và Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. Trong Proceedings of the European Conference on Computer Vision (ECCV), pp. 532–547, 2018a.

Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, và Mohamed Elhoseiny. Efficient lifelong learning with a-gem. Trong International Conference on Learning Representations, 2018b.

Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, và Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.

Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip Torr, và David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pp. 6993–7001, 2021.

Muhammad Umar Chaudhry và Jee-Hyong Lee. Feature selection for high dimensional data using monte carlo tree search. IEEE Access, 6:76036–76048, 2018.

Aristotelis Chrysakis và Marie-Francine Moens. Online continual learning from imbalanced data. Trong International Conference on Machine Learning, pp. 1952–1961. PMLR, 2020.

Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, và John Schulman. Quantifying generalization in reinforcement learning. Trong International Conference on Machine Learning, pp. 1282–1289. PMLR, 2019.

Cédric Colas, Olivier Sigaud, và Pierre-Yves Oudeyer. A hitchhiker's guide to statistical comparisons of reinforcement learning algorithms. Trong ICLR Worskhop on Reproducibility, 2019.

Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. Trong International conference on computers and games, pp. 72–83. Springer, 2006.

Matthias De Lange, Gido M van de Ven, và Tinne Tuytelaars. Continual evaluation for lifelong learning: Identifying the stability gap. Trong The Eleventh International Conference on Learning Representations, 2023.

Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, và Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

Frank N Dempster. Spacing effects and their implications for theory and practice. Educational Psychology Review, 1(4):309–330, 1989.

Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, và Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.

Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, và Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. Trong Computer Vision – ECCV 2020, pp. 86–102. Springer International Publishing, 2020.

John Dunlosky, Katherine A. Rawson, Elizabeth J. Marsh, Mitchell J. Nathan, và Daniel T. Willingham. Improving students' learning with effective learning techniques: Promising directions from cognitive and educational psychology. Psychological Science in the Public Interest, 14(1):4–58, 2013. ISSN 15291006. URL http://www.jstor.org/stable/23484712.

--- TRANG 15 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Hermann Ebbinghaus. Memory: A contribution to experimental psychology. Annals of neurosciences, 20(4):155, 2013.

Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, và Marcus Rohrbach. Adversarial continual learning. Trong Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16, pp. 386–402. Springer, 2020.

Jesse Farebrother, Marlos C Machado, và Michael Bowling. Generalization and regularization in dqn. arXiv preprint arXiv:1810.00123, 2018.

Kanyin Feng, Xiao Zhao, Jing Liu, Ying Cai, Zhifang Ye, Chuansheng Chen, và Gui Xue. Spaced learning enhances episodic memory by increasing neural pattern similarity across repetitions. Journal of Neuroscience, 39(27):5351–5360, 2019.

Chelsea Finn, Pieter Abbeel, và Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. Trong International conference on machine learning, pp. 1126–1135. PMLR, 2017.

Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, và Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.

Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, và Michal Irani. Reconstructing training data from trained neural networks. Advances in Neural Information Processing Systems, 35:22911–22924, 2022.

Karri S Hawley, Katie E Cherry, Emily O Boudreaux, và Erin M Jackson. A comparison of adjusted spaced retrieval versus a uniform expanded retrieval schedule for learning a name–face association in older adults with probable alzheimer's disease. Journal of Clinical and Experimental Neuropsychology, 30(6):639–649, 2008.

Tyler L Hayes, Nathan D Cahill, và Christopher Kanan. Memory efficient experience replay for streaming learning. Trong 2019 International Conference on Robotics and Automation (ICRA), pp. 9769–9776. IEEE, 2019.

Tyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, và Christopher Kanan. Remind your neural network to prevent catastrophic forgetting. Trong European Conference on Computer Vision, pp. 466–483. Springer, 2020.

Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, et al. Applied machine learning at facebook: A datacenter infrastructure perspective. Trong 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 620–629. IEEE, 2018.

Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, và David Meger. Deep reinforcement learning that matters. Trong Proceedings of the AAAI conference on artificial intelligence, 2018.

Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, và Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. Trong International Conference on Machine Learning, pp. 1480–1490. PMLR, 2017.

Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin, và Katja Hofmann. Generalization in reinforcement learning with selective noise injection and information bottleneck. Advances in neural information processing systems, 32, 2019.

Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, và Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. Trong International Conference on Learning Representations, 2021.

Sergey Ioffe và Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Trong International conference on machine learning, pp. 448–456. PMLR, 2015.

--- TRANG 16 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, và Cordelia Schmid. Memory-efficient incremental learning through feature adaptation. Trong European Conference on Computer Vision, pp. 699–715. Springer, 2020.

David Isele và Akansel Cosgun. Selective experience replay for lifelong learning. Trong Proceedings of the AAAI Conference on Artificial Intelligence, 2018.

Ayush Jain, Andrew Szot, và Joseph Lim. Generalization to new actions in reinforcement learning. Trong International Conference on Machine Learning, pp. 4661–4672. PMLR, 2020.

Xisen Jin, Arka Sadhu, Junyi Du, và Xiang Ren. Gradient-based editing of memory examples for online task-free continual learning. Advances in Neural Information Processing Systems, 34:29193–29205, 2021.

KJ Joseph và Vineeth N Balasubramanian. Meta-consolidation for continual learning. Advances in Neural Information Processing Systems, 33:14374–14386, 2020.

Samuel Kessler, Jack Parker-Holder, Philip Ball, Stefan Zohren, và Stephen J Roberts. Same state, different task: Continual reinforcement learning without interference. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pp. 7143–7151, 2022.

Diederik P Kingma và Jimmy Ba. Adam: A method for stochastic optimization. Trong International Conference on Learning Representations, 2015.

Robert Kirk, Amy Zhang, Edward Grefenstette, và Tim Rocktäschel. A survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76:201–264, 2023.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.

Levente Kocsis và Csaba Szepesvári. Bandit based monte-carlo planning. Trong European conference on machine learning, pp. 282–293. Springer, 2006.

Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.

Alex Krizhevsky và Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.

T. Landauer và Robert Bjork. Optimum rehearsal patterns and name learning. Practical aspects of memory, 1, 11 1977.

Yann LeCun, Léon Bottou, Yoshua Bengio, và Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Zhizhong Li và Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017.

Yaoyao Liu, Bernt Schiele, và Qianru Sun. Rmm: Reinforced memory management for class-incremental learning. Advances in Neural Information Processing Systems, 34:3478–3490, 2021.

David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.

Arun Mallya và Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7765–7773, 2018.

Seyed Iman Mirzadeh và Hassan Ghasemzadeh. Cl-gym: Full-featured pytorch library for continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 3621–3627, June 2021.

--- TRANG 17 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, và Hassan Ghasemzadeh. Linear mode connectivity in multitask and continual learning. Trong International Conference on Learning Representations, 2021.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, và Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, và Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. Trong International conference on machine learning, pp. 1928–1937. PMLR, 2016.

Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, và Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. Trong International Conference on Learning Representations, 2019.

Ashvin Nair, Abhishek Gupta, Murtaza Dalal, và Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

Cuong V Nguyen, Yingzhen Li, Thang D Bui, và Richard E Turner. Variational continual learning. Trong International Conference on Learning Representations, 2018.

Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, và Mohammad Emtiyaz E Khan. Continual deep learning by functional regularisation of memorable past. Advances in Neural Information Processing Systems, 33:4453–4464, 2020.

German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, và Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

Lorenzo Pellegrini, Gabriele Graffieti, Vincenzo Lomonaco, và Davide Maltoni. Latent replay for real-time continual learning. Trong 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 10203–10209. IEEE, 2020.

Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet K Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, và Adel Bibi. Computationally budgeted continual learning: What does matter? Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3698–3707, 2023a.

Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, và Ozan Sener. Online continual learning without the storage constraint. arXiv preprint arXiv:2305.09253, 2023b.

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental classifier and representation learning. Trong Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.

Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, và Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. Trong International Conference on Learning Representations, 2019.

David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, và Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

--- TRANG 18 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, và Raia Hadsell. Progress & compress: A scalable framework for continual learning. Trong International Conference on Machine Learning, pp. 4528–4537. PMLR, 2018.

Joan Serra, Didac Suris, Marius Miron, và Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. Trong International Conference on Machine Learning, pp. 4548–4557. PMLR, 2018.

Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, và Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. Trong Proceedings of the AAAI Conference on Artificial Intelligence, pp. 9630–9638, 2021.

Hanul Shin, Jung Kwon Lee, Jaehong Kim, và Jiwon Kim. Continual learning with deep generative replay. Advances in neural information processing systems, 30, 2017.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

Paul Smolen, Yili Zhang, và John H Byrne. The right time to learn: mechanisms and optimization of spaced learning. Nature Reviews Neuroscience, 17(2):77, 2016.

Shengyang Sun, Daniele Calandriello, Huiyi Hu, Ang Li, và Michalis Titsias. Information-theoretic online memory selection for continual learning. Trong International Conference on Learning Representations, 2022.

Richard S Sutton và Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, và Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. Trong 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23–30. IEEE, 2017.

Gido M van de Ven và Andreas S Tolias. Generative replay with feedback connections as a general strategy for continual learning. arXiv preprint arXiv:1809.10635, 2018.

Gido M Van de Ven và Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.

Gido M van de Ven, Hava T Siegelmann, và Andreas S Tolias. Brain-inspired replay for continual learning with artificial neural networks. Nature communications, 11(1):1–14, 2020.

Eli Verwimp, Matthias De Lange, và Tinne Tuytelaars. Rehearsal revealed: The limits and merits of revisiting samples in continual learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9385–9394, 2021.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.

Johannes von Oswald, Christian Henning, João Sacramento, và Benjamin F Grewe. Continual learning with hypernetworks. Trong International Conference on Learning Representations, 2020.

Kaixin Wang, Bingyi Kang, Jie Shao, và Jiashi Feng. Improving generalization in reinforcement learning with mixture regularization. Advances in Neural Information Processing Systems, 33:7968–7978, 2020.

Judy Willis. Review of research: Brain-based teaching strategies for improving students' memory, learning, and test-taking success. Childhood Education, 83(5):310–315, 2007.

Han Xiao, Kashif Rasul, và Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

Ju Xu và Zhanxing Zhu. Reinforced continual learning. Advances in Neural Information Processing Systems, 31, 2018.

--- TRANG 19 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Jaehong Yoon, Eunho Yang, Jeongtae Lee, và Sung Ju Hwang. Lifelong learning with dynamically expandable networks. Trong International Conference on Learning Representations, 2018.

Jaehong Yoon, Saehoon Kim, Eunho Yang, và Sung Ju Hwang. Scalable and order-robust continual learning with additive parameter decomposition. Trong International Conference on Learning Representations, 2020.

Jaehong Yoon, Divyam Madaan, Eunho Yang, và Sung Ju Hwang. Online coreset selection for rehearsal-based continual learning. Trong International Conference on Learning Representations, 2022.

Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Deep reinforcement learning with relational inductive biases. Trong International conference on learning representations, 2018.

Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence. Trong International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.

Amy Zhang, Nicolas Ballas, và Joelle Pineau. A dissection of overfitting and generalization in continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018a.

Chiyuan Zhang, Oriol Vinyals, Remi Munos, và Samy Bengio. A study on overfitting in deep reinforcement learning. arXiv preprint arXiv:1804.06893, 2018b.

Phụ lục

Tài liệu bổ sung này được cấu trúc như sau:

• Phụ lục A: Thông tin bổ sung về phương pháp luận của MCTS để tìm lịch phát lại và khung dựa trên RL của chúng tôi để học chính sách.

• Phụ lục B: Thông tin bổ sung về các baseline lập lịch heuristic và siêu tham số.

• Phụ lục C: Cài đặt thí nghiệm bổ sung và kết quả cho Phần 4.1.

• Phụ lục D: Cài đặt thí nghiệm bổ sung và kết quả cho Phần 4.2.

• Mã nguồn được công khai tại https://github.com/marcusklasson/replay_scheduling.

A Phương pháp luận bổ sung

Trong phần này, chúng tôi cung cấp mã giả cho MCTS để tìm kiếm lịch phát lại trong các môi trường CL đơn lẻ trong Phần A.1 cũng như mã giả cho khung dựa trên RL để học các chính sách lập lịch phát lại trong Phần A.2.

A.1 Thuật toán tìm kiếm cây Monte Carlo cho lập lịch phát lại

Chúng tôi cung cấp mã giả trong Thuật toán 1 phác thảo các bước cho phương pháp của chúng tôi sử dụng tìm kiếm cây Monte Carlo (MCTS) để tìm lịch phát lại được mô tả trong bài báo chính (Phần 3.2). Quy trình MCTS chọn hành động về tỷ lệ nhiệm vụ nào để điền bộ nhớ phát lại tại mỗi nhiệm vụ, nơi các tỷ lệ nhiệm vụ được chọn được lưu trữ trong lịch phát lại S. Lịch sau đó được chuyển đến EvaluateReplaySchedule(·) nơi phần học tập liên tục thực hiện đào tạo với bộ nhớ phát lại được điền theo lịch. Phần thưởng cho lịch S là độ chính xác xác thực trung bình trên tất cả các nhiệm vụ sau khi học nhiệm vụ T, tức là, ACC, được lan truyền ngược qua cây để cập nhật thống kê của các nút được chọn. Lịch Sbest mang lại điểm ACC tốt nhất được trả về để sử dụng cho đánh giá trên các tập kiểm tra được giữ lại.

Hàm GetReplayMemory(·) là chính sách để truy xuất bộ nhớ phát lại M từ dữ liệu lịch sử được cho tỷ lệ nhiệm vụ p. Số lượng mẫu mỗi nhiệm vụ được xác định bởi tỷ lệ nhiệm vụ được làm tròn lên hoặc xuống tương ứng để điền M với M mẫu phát lại tổng cộng. Hàm GetTaskProportion(·) đơn giản trả về tỷ lệ nhiệm vụ liên quan đến nút đã cho.

--- TRANG 20 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Thuật toán 1 Tìm kiếm cây Monte Carlo cho lập lịch phát lại
Yêu cầu: Các nút cây v1:T, Tập dữ liệu D1:T, Tỷ lệ học η
Yêu cầu: Kích thước bộ nhớ phát lại M
1: ACCbest←0, Sbest←()
2: while trong ngân sách tính toán do
3:    S←()
4:    vt, S←TreePolicy(v1, S)
5:    vT, S←DefaultPolicy(vt, S)
6:    ACC←EvaluateReplaySchedule(D1:T, S, M)
7:    Backpropagate(vt, ACC)
8:    if ACC > ACCbest then
9:       ACCbest←ACC
10:      Sbest←S
11: return ACCbest, Sbest
12: function TreePolicy(vt, S)
13:    while vt là non-terminal do
14:       if vt chưa được mở rộng đầy đủ then
15:          return Expansion(vt, S)
16:       else
17:          vt←BestChild(vt)
18:          S.append(pt), nơi pt←GetTaskProportion(vt)
19:    return vt, S
20: function Expansion(vt, S)
21:    Lấy mẫu vt+1 đồng đều trong các con chưa thăm của vt
22:    S.append(pt+1), nơi pt+1←GetTaskProportion(vt+1)
23:    Thêm con mới vt+1 vào nút vt
24:    return vt+1, S
25: function BestChild(vt)
26:    vt+1= arg max_{vt+1∈children của vt} max(Q(vt+1)) + C√(2 log(N(vt))/N(vt+1))
27:    return vt+1
28: function DefaultPolicy(vt, S)
29:    while vt là non-terminal do
30:       Lấy mẫu vt+1 đồng đều trong các con của vt
31:       S.append(pt+1), nơi pt+1←GetTaskProportion(vt+1)
32:       Cập nhật vt←vt+1
33:    return vt, S
34: function EvaluateReplaySchedule(D1:T, S, M)
35:    Khởi tạo mạng nơ-ron fθ
36:    for t= 1, . . . , T do
37:       p←S[t−1]
38:       M← GetReplayMemory(D^(train)_{1:t−1}, p, M)
39:       for B∼D^(train)_t do
40:          θ←SGD(B∪M, θ, η)
41:    A^(val)_{1:T}←EvaluateAccuracy(fθ, D^(val)_{1:T})
42:    ACC←(1/T)∑^T_{i=1} A^(val)_{T,i}
43:    return ACC
44: function Backpropagate(vt, R)
45:    while vt không phải là gốc do
46:       N(vt)←N(vt) + 1
47:       Q(vt)←R
48:       vt←cha của vt

--- TRANG 21 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Các bước sau được thực hiện trong một lần rollout MCTS (hoặc lần lặp):

1. Lựa chọn bao gồm việc chọn một nút chưa thăm ngẫu nhiên, hoặc chọn nút tiếp theo bằng cách đánh giá điểm UCT (xem Phương trình 1) nếu tất cả các con đã được thăm. Trong Thuật toán 1, TreePolicy(·) nối thêm tỷ lệ nhiệm vụ pt vào lịch phát lại S tại mỗi nút được chọn.

2. Mở rộng bao gồm việc mở rộng cây tìm kiếm với một trong các nút con chưa thăm vt+1 được chọn bằng lấy mẫu đồng đều. Expansion(·) trong Thuật toán 1 nối thêm tỷ lệ nhiệm vụ pt vào lịch phát lại S của nút được mở rộng.

3. Mô phỏng bao gồm việc chọn các nút tiếp theo ngẫu nhiên cho đến khi đạt nút terminal vT. Trong Thuật toán 1, DefaultPolicy(·) nối thêm tỷ lệ nhiệm vụ pt vào lịch phát lại S tại mỗi nút được chọn ngẫu nhiên cho đến khi đạt nút terminal.

4. Phần thưởng Phần thưởng cho rollout được cho bởi ACC của các tập xác thực cho mỗi nhiệm vụ. Trong Thuật toán 1, EvaluateReplaySchedule(·) bao gồm việc học các nhiệm vụ t= 1, . . . , T tuần tự và sử dụng lịch phát lại để lấy mẫu bộ nhớ phát lại để sử dụng cho việc giảm thiểu quên thảm khốc khi học một nhiệm vụ mới. Phần thưởng r cho rollout được tính sau khi nhiệm vụ T đã được học.

5. Lan truyền ngược bao gồm việc cập nhật hàm phần thưởng q(·) và số lần thăm n(·) từ nút mở rộng lên nút gốc. Xem Backpropagate(·) trong Thuật toán 1.

A.2 Thuật toán khung RL

Chúng tôi cung cấp mã giả cho khung dựa trên RL để học chính sách lập lịch phát lại với DQN ( Mnih et al. ,2013) hoặc A2C ( Mnih et al. ,2016) trong Thuật toán 2. Quy trình thu thập kinh nghiệm từ tất cả các môi trường đào tạo trong E^(train) tại mỗi bước thời gian t. Các tập dữ liệu và bộ phân loại đặc thù cho mỗi môi trường Ei∈ E^(train). Tại t= 1, chúng tôi có được trạng thái ban đầu s^(i)_1 bằng cách đánh giá bộ phân loại trên tập xác thực D^(val)_1 sau khi đào tạo bộ phân loại trên nhiệm vụ 1. Tiếp theo, chúng tôi có được bộ nhớ phát lại để giảm thiểu quên thảm khốc khi học nhiệm vụ tiếp theo t+ 1 bằng cách 1) thực hiện hành động a^(i)_t dưới chính sách πθ, 2) chuyển đổi hành động a^(i)_t thành tỷ lệ nhiệm vụ pt, và 3) lấy mẫu bộ nhớ phát lại Mt từ các tập dữ liệu lịch sử được cho tỷ lệ được chọn. Sau đó chúng tôi có được phần thưởng rt và trạng thái tiếp theo st+1 bằng cách đánh giá bộ phân loại trên các tập xác thực D^(val)_{1:t+1} sau khi học nhiệm vụ t+ 1. Kinh nghiệm được thu thập từ mỗi bước thời gian được lưu trữ trong bộ đệm kinh nghiệm B cho cả DQN và A2C. Trong UpdatePolicy(·), chúng tôi phác thảo các bước để cập nhật tham số chính sách θ với DQN hoặc A2C.

--- TRANG 22 ---
Xuất bản tại Transactions on Machine Learning Research (09 /2023)

Thuật toán 2 Khung RL để học chính sách lập lịch phát lại
Yêu cầu: E^(train): Môi trường đào tạo, θ: Tham số chính sách, γ: Hệ số chiết khấu
Yêu cầu: η: Tỷ lệ học, nepisodes: Số tập, M: Kích thước bộ nhớ phát lại
Yêu cầu: nsteps: Số bước cho A2C
1: B={} ⊲ Khởi tạo bộ đệm kinh nghiệm
2: for i= 1, . . . , nepisodes do
3:    for t= 1, . . . , T −1 do
4:       for Ei∈ E^(train) do
5:          D1:t+1=GetDatasets(Ei, t) ⊲ Lấy tập dữ liệu từ môi trường Ei
6:          f^(i)_φ=GetClassifier(Ei) ⊲ Lấy bộ phân loại từ môi trường Ei
7:          if t== 1 then
8:             Train(f^(i)_φ, D^(train)_t) ⊲ Đào tạo bộ phân loại f^(i)_φ trên nhiệm vụ 1
9:             A^(val)_{1:t}=Eval(f^(i)_φ, D^(val)_{1:t}) ⊲ Đánh giá bộ phân loại f^(i)_φ trên nhiệm vụ 1
10:            s^(i)_t=A^(val)_{1:t}= [A^(val)_{1,1}, 0, ..., 0] ⊲ Lấy trạng thái ban đầu
11:         a^(i)_t∼πθ(a, s^(i)_t) ⊲ Thực hiện hành động dưới chính sách πθ
12:         pt=GetTaskProportion(a^(i)_t)
13:         Mt∼GetReplayMemory(D^(train)_{1:t}, pt, M)
14:         Train(f^(i)_φ, D^(train)_{t+1} ∪ Mt) ⊲ Đào tạo bộ phân loại f^(i)_φ
15:         A^(val)_{1:t+1}=Eval(f^(i)_φ, D^(val)_{1:t+1}) ⊲ Đánh giá bộ phân loại f^(i)_φ
16:         s^(i)_{t+1}=A^(val)_{1:t+1}= [A^(val)_{t+1,1}, ..., A^(val)_{t+1,t+1}, 0, ..., 0] ⊲ Lấy trạng thái tiếp theo
17:         r^(i)_t=(1/(t+1))∑^{t+1}_{j=1} A^(val)_{1:t+1} ⊲ Tính phần thưởng
18:         B=B ∪ {(s^(i)_t, a^(i)_t, r^(i)_t, s^(i)_{t+1})} ⊲ Lưu trữ chuyển tiếp trong bộ đệm
19:      if thời gian cập nhật chính sách then
20:         θ, B=UpdatePolicy(θ, B, γ, η, nsteps) ⊲ Cập nhật chính sách với kinh nghiệm
21: return θ ⊲ Trả về chính sách
22: function UpdatePolicy(θ, B, γ, η, nsteps)
23:    if DQN then
24:       (sj, aj, rj, s′j)∼ B ⊲ Lấy mẫu mini-batch từ bộ đệm
25:       yj={rj nếu s′j là terminal; rj+γ max_a Qθ−(s′j, a) nếu ngược lại} ⊲ Tính yj với mạng đích θ−
26:       θ=θ−η∇θ(yj−Qθ(sj, aj))^2 ⊲ Cập nhật Q-function
27:    else if A2C then
28:       st=B[nsteps] ⊲ Lấy trạng thái cuối trong bộ đệm
29:       R={0 nếu st là terminal; Vθv(st) nếu ngược lại} ⊲ Bootstrap từ trạng thái cuối
30:       for j=nsteps −1, ..., 0 do
31:          sj, aj, rj=B[j] ⊲ Lấy trạng thái, hành động, và phần thưởng tại bước j
32:          R=rj+γR
33:          θ=θ−η∇θ log πθ(aj, sj)(R−Vθv(sj)) ⊲ Cập nhật chính sách
34:          θv=θv−η∇θv(R−Vθv(sj))^2 ⊲ Cập nhật hàm giá trị
35:       B={} ⊲ Đặt lại bộ đệm kinh nghiệm
36:    return θ, B
