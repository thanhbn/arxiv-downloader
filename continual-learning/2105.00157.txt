# 2105.00157.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2105.00157.pdf
# File size: 4108919 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Deep Learning Framework for Lifelong Machine Learning
Charles X. Ling CHARLES .LING @UWO .CA
Tanner Bohn TBOHN @UWO .CA
Western University
1151 Richmond St.
London ON, N6A 3K7, Canada
Abstract
Humans can learn a variety of concepts and skills incrementally over the course of their lives while
exhibiting many desirable properties, such as continual learning without forgetting, forward transfer
and backward transfer of knowledge, and learning a new concept or task with only a few examples.
Several lines of machine learning research, such as lifelong machine learning, few-shot learning,
and transfer learning attempt to capture these properties. However, most previous approaches can
only demonstrate subsets of these properties, often by different complex mechanisms. In this work,
we propose a simple yet powerful uniﬁed deep learning framework that supports almost all of these
properties and approaches through onecentral mechanism. Experiments on toy examples support
our claims. We also draw connections between many peculiarities of human learning (such as
memory loss and “rain man”) and our framework.
As academics, we often lack resources required to build and train, deep neural networks with
billions of parameters on hundreds of TPUs. Thus, while our framework is still conceptual, and
our experiment results are surely not SOTA, we hope that this uniﬁed lifelong learning framework
inspires new work towards large-scale experiments and understanding human learning in general.
This paper is summarized in two short YouTube videos: https://youtu.be/gCuUyGETbTU
(part 1) and https://youtu.be/XsaGI01b-1o (part 2).
Keywords: Lifelong machine learning, Multi-task learning, Deep learning, Human-inspired
learning, Weight consolidation, Neural networks
1. Introduction
The past decade has seen signiﬁcant growth in the capabilities of artiﬁcial intelligence and
machine learning (ML). Deep learning in particular has archived great successes in medical image
recognition and diagnostics (Litjens et al., 2017; Shen, Wu, and Suk, 2017), tasks on natural
language processing (Radford et al., 2019; Devlin et al., 2019), difﬁcult games (Silver et al., 2017),
and even farming (Kamilaris and Prenafeta-Bold ´u, 2018). However, deep learning models almost
always need thousands or millions of training samples, given at once to train the models, to perform
well. This is in a sharp contrast with human learning, which normally learns a new concept with a
small number of samples, and continues to learn over a lifetime. Other major weaknesses in current
deep learning, when compared to human learning, include difﬁculty in leveraging previous learned
knowledge to better learn new concepts (and vice versa) and learning many tasks sequentially
without forgetting previous ones.
Several lines of research in supervised learning exist to overcome these weaknesses. Multi-
task learning (Caruana, 1997) considers how to learn multiple concepts at the same time such that
they help each other to be learned better. The related ﬁeld of transfer learning (Pan and Yang,
2009) assumes that some concepts have been previously learned and we would like to transfer their
1arXiv:2105.00157v1  [cs.AI]  1 May 2021

--- PAGE 2 ---
knowledge to assist learning new concepts. Few-shot learning (Fei-Fei, Fergus, and Perona, 2006)
aims to learn tasks with a small number of labeled data. Lifelong machine learning (LML) (Thrun,
1998; Thrun., 1995), also known as continual (Parisi et al., 2019) or sequential learning (McCloskey
and Cohen, 1989), considers how to learn and transfer skills across long sequences of tasks.
However, most previous LML approaches only demonstrate subsets of these human-like
properties by different complex mechanisms. It is our belief that human lifelong concept learning
with labeled examples likely uses a single (or small set of) mechanism(s). This is because humans
learn many concepts in their lives, and the process is continuous without sharp boundary . For
example, learning a new concept (such as a new shape) or updating a learned concept (such as
receiving more data of a previously learned shape) may subtly inﬂuence concepts learned before and
after. Using LML terminology, this subtle inﬂuence is due to forward and backward transfer, non-
forgetting of some previous concepts, and graceful forgetting of others. These inﬂuences appear
to combine seamlessly in humans, providing effective lifelong learning capabilities. Thus, we
wish to ﬁnd a learning framework with one central mechanism that can seamlessly implement such
inﬂuences1, and demonstrate many human-like lifelong learning properties.
continual learning 
non-forgetting 
forward transfer 
backward transfer confusion reduction 
graceful forgetting multi-task learning 
curriculum learning 
few-shot learning 
pre-training 
transfer learning 
convolutional networks 
LML properties Close connections with 
many ML settings human-like learning 
(a) (b) (c)which supports 
additional mechanisms Consolidation 
mechanism 
Figure 1: An overview of our uniﬁed framework: by combining a central consolidation mechanism
with additional tools, we are able to exhibit many lifelong learning properties and demonstrate
close connections with other ML settings. The set of desirable LML properties in (a) is discussed in
Section 2.2. A description of the consolidation mechanism and its combination with the additional
mechanisms shown in (b) is discussed in Section 3. The learning settings listed in (c) where our
framework can be applied is discussed in Section 5. The connection between our framework and
human learning in particular is expanded upon in Section 5.5.
In this paper, we propose such a uniﬁed framework with one central mechanism with the
support of other mechanisms such as network expansion and rehearsal. A high-level overview
of the framework is depicted in Figure 1. We demonstrate its uniﬁed characteristic by discussing
how it can illustrate many desirable LML properties such as non-forgetting, forward and backward
transfer, and graceful forgetting (Section 3). To support our framework, we include proof-of-concept
experimental results for our LML properties in Section 4. We also discuss connections to other ML
1. More speciﬁcally, we are aiming to describe a higher-level algorithmic mechanism, rather than provide a model of
speciﬁc neural processes associated with human learning.
2

--- PAGE 3 ---
settings, and draw parallels with many peculiarities seen in human learning, such as memory loss
and “rain man” in Section 5.5. We hope that this perspective can shed new light on human learning.
Our paper is different from most deep learning papers that aim to achieve state-of-the-art results.
Instead, our paper shows the generality of a new framework that can exhibit many properties, apply
to many ML settings, and encompass previous works. We hope it inspires more researchers to
engage in the various related topics towards better understanding of ML, human general intelligence,
and human learning.
2. Lifelong Learning and its Properties
In this section we will ﬁrst describe our LML setting and its relation to other learning settings. We
will then discuss a broad set of important LML properties. Lastly, we will provide a comparison of
common approaches to LML.
2.1 Lifelong Learning Setting
In our lifelong setting, we mainly consider the task-incremental classiﬁcation tasks, where batches
of data for new tasks arrive sequentially. That is, a sequence of (T0;D0);(T1;D1);:::are given,
whereDiis the labeled training data of task Ti. Classiﬁcation models for (T0;T1;:::;T j)must be
learned and functional before (Tj+1;Dj+1)arrives. This models the incremental process of human
lifelong learning.
As a toy running example for this paper, we assume that the sequence of the classiﬁcation tasks
is to learn to classify hand-written numbers and letters, starting with T0=“0”, thenT1=“1”, then
T2=“2”, and so on, as seen in Figure 2. It will be clear soon why we use hand-written letters “O”
and “Z” in our running example.
Task 0 Task 1 Task 2 Task 3 Task O Task Z 
Positives 
Negatives 
Figure 2: The sequence of binary classiﬁcation tasks used in our paper as a running example. Here
we assume that the same group of negative characters is shared across tasks2.
Our LML setting is distinct from most supervised learning and multi-task learning settings in
two important ways:
1. The data for task Tjis only available when learning Tj(and not before). Similarly, data for
tasksTj+1and so on are not yet available, but classiﬁers for Tasks T0toTjmust be built.
That is, one cannot wait until all data is available to build classiﬁers, as in the “batch mode”
of learning.
2. Also worth considering for this toy example is where negative examples of each task include some positive samples
of the tasks that came before (since this data will have been made available).
3

--- PAGE 4 ---
2. When building a new classiﬁer for Tj, the LML algorithm should refrain, as much as possible,
from using data from previous tasks; otherwise, when learning the last task using data of all
previous tasks, it becomes the traditional “batch mode” again.
We believe that our lifelong learning setting closely matches how humans learn concepts in sequence
in their lifetime.
Our LML setting to learn one concept at a time can be easily extended to learning multi-class
classiﬁers in sequence. For example, the ﬁrst task can learn to classify different mammals (given
data of mammals), and then learn to classify various birds (given data of birds), and so on. This is
illustrated in Figure 3. Another direction of extension is “anytime” LML, in which data of any task
might be received at any time, and the relevant classiﬁer must be updated with minimal use of data
and interference on other learned classiﬁers.
Task 1 
(classify mammals) Task 2 
(classify birds) 
Figure 3: An example sequence of multi-class tasks, ﬁrst learning to classify various mammals and
then birds.
2.2 Lifelong Learning Properties
Here we discuss at a high level several properties a LML approach would ideally exhibit. We will
use the running example from Figure 2 of learning a sequence of binary classiﬁers for “0”, “1”, “2”,
“3”, “O”, and “Z”.
Continual learning and deploying As mentioned earlier, before starting to learn a new task, Tj,
our LML approach should be able to perform well on all earlier tasks, T<j. The data,Dj, forTj,
is only available when learning Tj. While learning Tj, LML should refrain from using the previous
task data,D<j. This is in contrast to standard multi-task (batch) learning, where all data of all tasks
are used for training at the same time. This continual learning condition ensures that the model
is 1) useful, since each task must be learned to an acceptable performance level whenever data is
available, 2) ﬂexible, in that new tasks can be continually accommodated, 3) efﬁcient, in that tasks
are learned with high computational and data efﬁciency, and 4) human-like, in that humans seem to
learn in a similar continuous fashion over their lifetime. See Section 3.2 for details.
Non-forgetting This is the ability to avoid catastrophic forgetting (McCloskey and Cohen, 1989),
where learning Tjcauses a dramatic loss in performance on T<j. Ideally, learning Tjwhen using
only the data of Tjwould not negatively affect T<j. For example, learning the classiﬁer for T1of
“1” should not cause performance on the previous task of “0” to degrade, when the classiﬁer for
task “0” is not trained at that time. See Section 3.2 for details.
4

--- PAGE 5 ---
Forward transfer This is the ability to learn new tasks, Tj, easier and better following earlier
learned similar tasks, T<j. This is also known as knowledge transfer (Pan and Yang, 2009).
Achieving sufﬁcient positive forward transfer also enables few-shot learning of later concepts.
For example, ﬁrst learning to classify “0” should allow the later task of “O” to be learned faster, as
they are very visually similar. On the other hand, transferring between non-similar tasks may lead
to negative transfer, compromising the performance of the new task. See Section 3.3 for details.
Confusion reduction Classiﬁcation algorithms often ﬁnd the minimal set of discriminating
features necessary for learning the tasks at hand. As training of each new task is performed
individually with its training data, the testing accuracy can be high when testing on its own test data.
However, when tested on data of all tasks, confusions can happen. For example, when learning the
sequence of tasks of “0”, “1”, “2”, “3”, “O”, and “Z”, the two tasks of “0” and “O” may be confused
(images of “0” may be predicted as “O”, and vice versa). Similarly for “2” and “Z”. In such cases,
we will need to resolve confusion between pairs of similar tasks.
In human lifelong learning, this type of confusion may happen too. For example, when meeting
new people, if the ﬁrst two people are visually distinct (such as very tall vs. very short) we can rely
only on this feature to tell them apart. However, if more people arrive and they are similar to the
ﬁrst two people, we may initially confuse them and must ﬁnd ﬁner details to reduce confusion, or
to uniquely distinguish them. In the extreme case where we encounter identical twins, signiﬁcant
effort may be required to learn the necessary details (by re-using their facial image data) to resolve
confusion. See Section 3.4 for details.
Graceful forgetting A valuable property opposite to non-forgetting is graceful forgetting (Aljundi
et al., 2018), often seen in humans. In our framework, learning new tasks requires additional model
capacity, and when sufﬁcient expansion is not possible, the model can turn to graceful forgetting of
unimportant tasks to free up capacity for new tasks. See Section 3.2 and 3.5 for details.
Backward transfer This is knowledge transfer from TjtoT<j, the opposite direction as forward
transfer. When learning a task, Tj, it may in turn help to improve the performance of T<j. This
is like a “review” before a ﬁnal exam after materials of all chapters have been taught and learned.
Later materials can often help better understand earlier materials. See Section 3.6 for details.
Human-like learning As a new type of evaluation criteria for LML, we can consider how well
an approach is predictive of human learning behaviour. If a LML approach or framework is also
able to provide explanatory power and match peculiarities of human learning (such as confusion or
knowledge transfer in certain scenarios), it would have value in ﬁelds outside of ML. We discuss
such connections between our framework and human learning in Section 5.5.
2.3 Comparison of Different Lifelong Learning Approaches
The mechanisms used in previous work to perform LML tend to fall into three categories, and they
can often only demonstrate subsets of LML properties as previously discussed. The ﬁrst mechanism,
replay, commonly works by storing previous task data and training on it alongside new task data
(Rebufﬁ et al., 2017; Isele and Cosgun, 2018; Chaudhry et al., 2019; Wu et al., 2019). As a result of
its data and computation inefﬁciency, we consider it generally not to be very a human-like learning
mechanism. An exception to this is generative replay, where the storing of previous task samples
is replaced by a deep generative model trained to produce samples for interleaving with new task
5

--- PAGE 6 ---
samples. This approach, which requires a constant memory overhead is inspired by the generative
ability of the primate hippocampus (Shin et al., 2017; Kamra, Gupta, and Liu, 2017).
The second mechanism is regularization. This mechanism works by restricting weight changes
(making them less “ﬂexible”) via a loss function so that learning new tasks does not signiﬁcantly
affect previous task performance (Kirkpatrick et al., 2016; Zenke, Poole, and Ganguli, 2017;
Chaudhry et al., 2018; Ritter, Botev, and Barber, 2018; Li and Hoiem, 2017; Zhang et al., 2020).
We use this mechanism as an essential component in our uniﬁed framework. Compared to previous
approaches, we propose to use regularization more strategically. Instead of simply controlling
weight ﬂexibility for non-forgetting, we leverage it to also encourage forward and backward transfer
(Section 3.3 and 3.6), reduce confusion (Section 3.4), and perform graceful forgetting (Section 3.5).
The third mechanism, dynamic architecture, commonly works by adding new weights for each
task and only allowing those to be tuned (Rusu et al., 2016; Yoon et al., 2018; Xu and Zhu, 2018).
This is often done without requiring previous task data and completely reduces forgetting while also
allowing previous task knowledge to speed up learning of the new task. While this mechanism is
necessary for LML of an arbitrarily long sequence of tasks (any ﬁxed-size network will eventually
reach maximum capacity), it should be used sparingly to avoid unnecessary computational costs. In
Sections 3.2, 3.3, and 3.4, we describe how dynamic architectures can be efﬁciently used to help
achieve many LML properties by combining it with our central mechanism.
3. A Uniﬁed Framework for Lifelong Learning
In this section we describe how our uniﬁed framework works. We start by introducing the central
mechanism and in the rest of the section, discuss how to use this mechanism and combine it with
additional mechanisms to achieve the several desirable LML properties described in Section 2.2.
While not restricted to a particular neural network type, we primarily consider our framework as
applied to deep neural networks, which have become popular in recent years, and are an attractive
type of ML model due to their ability to automatically learn abstract features from data.
3.1 A Central Consolidation Mechanism
We propose a LML framework which situates a consolidation policy as the central mechanism. The
consolidation policy works through a hyperparameter, bbb, which controls the ﬂexibility of the model
parameters to be learned. When using neural networks (as considered in this paper), bbbcontrols how
ﬂexibly the weights can be modiﬁed during gradient descent (or by any weight updating algorithm).
In regression, bbbcontrols how much coefﬁcients can be modiﬁed. In rule-based learning, bbbmay
control or regulate how much rule conditions can be added or deleted, or how much rule conﬁdences
can be updated, and so on. In this paper, we will mainly focus on deep neural networks.
As deep learning essentially works through the minimization of a properly deﬁned loss function,
our consolidation mechanism essentially works as a regularization term in the loss function. More
speciﬁcally, if each network weight, i, is associated with a consolidation value of bbbi0, the new
loss function, Lnewwhile learning a given task of index tis deﬁned as follows:
Lnew() =Lt() +X
ibbbi(i target
i )2(1)
6

--- PAGE 7 ---
Here,target
i is the target value for a weight to be changed to. Ltis a standard loss (such as
cross-entropy) on task t. This loss has the following behaviour: a large bbbicauses changing iaway
fromtarget
i to be strongly penalized during training. When bbbi=1, we refer to these weights as
“frozen” , and simply ﬁx them during training. In this case we can consider ito be masked during
backpropagation and completely prevented from changing to improve efﬁciency. In contrast, bbbi= 0
indicates that the weight is free to change, i.e. it is “unfrozen” .
3.2 Continual Learning of New Tasks Without Forgetting
In both lifelong and human learning, we desire to learn new tasks after learning previous tasks. In
humans, this is supported by the ability to continually grow new connections between neurons and
remove old connections (Cunha, Brambilla, and Thomas, 2010). When this ability is compromised,
so is our ability to learn new things. Similarly, in our conceptual framework we consider learning
new tasks through the strategic and ﬂexible use of network expansion.
The pseudo-code in Algorithm 1 describes how to learn a new task, Tj, in a deep neural network
in our conceptual framework after previous tasks, T0;:::;T j 1, have been learned.
Algorithm 1: Continual Learning without Forgetting
// Given that tasks T0;:::;T j 1have been learned
1Recruit free units for Tjand unfreeze new weights // Blue links in Figure 4
2Freeze weights of previous tasks // Red links in Figure 4 for
non-forgetting
3Initialize weights from earlier units to newly recruited units as described in Section 3.3
// Green links in Figure 4 for forward transfer
4Train the new task Tjto minimize Eq. 1 // only on the data of new task Tj
As seen in Figure 4, the role of red weights is to be frozen, blue weights are randomly initialized
and free to tune, and green weights are selectively initialized and consolidated to encourage forward
transfer (Section 3.3). This method of expansion differs from Progressive Neural Networks (Rusu et
al., 2016) in that rather than connecting the green links to an adapter and adding the adapter output
to the input of the new layer, all inputs to the new layer are simply concatenated. When learning
each new classiﬁcation task, minimization on the loss function (Equation 1) is applied only on that
classiﬁer with data for that task.
An important question to ask when learning a new task, Tj, is that of how many new neurons
need to be added. This is a difﬁcult question, inﬂuenced by many factors, including how complex the
new taskTjis, and whether the previously learned tasks can help in learning Tj(positive transfer;
see Section 3.3). In Section 4.1 we will propose a simple and effective strategy for the network
expansion, and show that indeed, when learning a sequence of tasks with the consolidation values
controlled as described here, previously learned tasks will not be affected.
On the other hand, in Section 4.4, we will show that if bbbis decreased for some previous task, the
performance on the task will degrade. In this case, graceful forgetting has happened.
7

--- PAGE 8 ---
(a) (b)
min L  on “1” (c)
min L  on “3” 0 1 3Negatives Negatives Negatives
b=∞ b=0 b=0Figure 4: Continual learning with network expansion and without forgetting. In (a), T0is being
trained. All blue weights are randomly initialized and free to tune ( bbb= 0). In (b),T1is being
learned without forgetting T0, by freezing weights for T0(red links;bbb=1). Note that the loss
function is minimized only for the “1” classiﬁer and data for “0” is not needed. Here, green links
are for forward transfer (see Section 3.3), and blue links are free to tune. In (c), T3is learned without
forgetting of the previous tasks. The loss function is minimized only for the “3” classiﬁer.
3.3 Forward Transfer
While continual learning without forgetting ensures past task performances are maintained , previous
tasks do not beneﬁt the learning of new tasks, a concept prominent in multi-task and transfer learning
(Pan and Yang, 2009; Zhang and Yang, 2017), and appears in LML as “forward transfer”.
Forward transfer in deep neural networks is easily supported in our LML framework. Figure 5
illustrates several cases to consider with different levels of forward transfer. When a new task (such
as “O”) arrives, we ﬁrst check if it is similar enough to a previous learned task (such as “0”, “1”,
...). Similarity can be estimated many different ways, such as a measure of the similarity of the data,
how well the learned classiﬁer performs on the new task, and so on. In Section 4.2 we explain how
we judge if two classiﬁcation tasks are similar in more detail. Then one or several previous similar
tasks would be chosen to forward-transfer their learned knowledge to the new one. Again, many
possibilities exist. Here we describe a simple strategy, reﬂected in Figure 5. For the output layer
weights of the new task, copy the weights of the output layer from the most similar previous task.
And for the intermediate layers, only randomly initialize the green weights when the task similarity
is above some threshold.
Our forward transfer mechanism is intended to have the effect that if a new task is very similar
to a previous task, positive transfer will occur, allowing the new task to be learned with less training
data. This achieves few-shot learning. As we will show in Section 4.2, after learning “0”, “1”,
“2”, “3”, and if “O” is to be learned next, since “O” is highly similar to “0”, output weights of “0”
would be copied to learn “O”. We will show that “O” is learned to a low testing error with many
8

--- PAGE 9 ---
New task is very 
similar to first task 
Copy & tune 
(a)
min L  on “O” 0 ONew task is similar to 
first task 
Copy & 
tune
(b)
min L  on “Z” 
2 ZNew task is very different 
from previous tasks 
(c)
min L  on “3” 
0 3Figure 5: Adding proper forward transfer in continual learning without forgetting in our LML
framework. Three special cases of forward transfer are considered here. In (a), when the new
task (such as “O”) is very similar to a previous task (such as “0”), no new nodes may be needed
(i.e., no network expansion as in Section 3.2). The output-layer weights of the new task can be
copied from the previous similar task during weight initialization. Note that those initial weights
can still be ﬁne-tuned in minimizing the loss of the new task (“O” here). In (b), if the new task (such
as “Z”) is similar enough (according to some threshold, see later) to a previous task (such as “2”),
positive transfer is expected from the previous task to the new task. Here, the expansion amount
for the new task can be smaller, if the similarity is high. All green and blue weights will be tuned
when minimizing the loss of the new task (“Z” here). In (c), if the new task is very different from
any previous tasks, the transfer links are disconnected to prevent possible negative transfer. When
positive transfer happens, we expect that the training requirements would be reduced signiﬁcantly,
i.e., few-shot learning would occur (see experiments in Section 4.2).
fewer examples. We also show that if forward transfer is incorrectly “forced” from dissimilar tasks
when learning “O” and “Z”, the predictive error is signiﬁcantly higher. By observing differences
in predictive error across transfer strategies, we can distinguish between and quantify positive and
negative transfer.
3.4 Confusion Reduction
As discussed in Section 2.2, in our LML framework, tasks are learned in sequence, and data for
the new tasks are only available when learning the new tasks. In this case, “confusion” can happen
between two similar tasks (such as “0” and “O”, “2” and “Z”). It is interesting to note that the more
similar the two tasks are, the more forward transfer would help in learning the new task (as seen
in Section 4.2), yet at the same time, the more confusion occurs between them. In general, small
confusion can happen between any pair or subset of classiﬁcation tasks, and when the confusion is
above some threshold, we need ways to reduce it3.
3. Note that we assume classes are mutually exclusive in this paper. In the case of multi-label learning, certain
“confusions” may be desired, and need not be resolved. Also, when learning and evaluating each task individually,
it is also called “multi-head” evaluation. When evaluation is done across all classiﬁers learned so far, it is often
9

--- PAGE 10 ---
We propose to resolve such confusion in a pairwise manner. To resolve confusion between Ti
(such as “0”) and Tj(such as “O”), some training data of both TiandTjwould be needed, whether
stored or generated. Here, confusion is measured by the sum of errors evaluated on TiandTjwhen
data of both tasks is presented as input. The training is to minimize the error on these tasks together,
while weights for other tasks are frozen.
More speciﬁcally, whenever confusion between TiandTjis greater than some threshold,
2[0;1], we propose a two-step strategy to reduce confusion. First, using the existing network,
simultaneously ﬁne-tune the last-layer weights of Tiand weights of Tjon samples of the confused
tasks. This step is shown in Figure 6a. When the confusion is small, this step alone may reduce
the confusion to below . If the current network capacity is not sufﬁcient to resolve confusion, then
we can move onto the second step, where we expand the model by some amount, and all the new
weights can now be learned. This step is reﬂected in Figure 6b. In both cases, only those weights
associated with the confused tasks are tuned, leaving other tasks unaffected.
First step in reducing confusion 
with existing network 
(a)
min L  on “2” + “Z” (b)
min L  on “2” + “Z” 2 Z 2 Z Second step in reducing confusion 
with expanded network 
Figure 6: The two steps of the confusion-reduction process. In (a) we ﬁrst try to reduce confusion
using the existing network capacity, freezing the necessary weights to maintain performance on
other tasks. In (b), where confusion cannot be reduce by ﬁne-tuning alone, we expand the network
and all newly added weights can be learned.
In experiments described in Section 4.3, we demonstrate that both steps of this process
contribute to decreasing confusion between highly similar tasks.
3.5 Graceful Forgetting
As we learn more and more tasks in our LML framework, the expanding network may reach some
size limitation. Eventually, we may reach a limit where the network cannot offer free units to learn a
new task well without signiﬁcantly forgetting already-learned tasks. In such cases, we can consider
“graceful” forgetting (Aljundi et al., 2018) of previous tasks.
called “single-head” evaluation (Chaudhry et al., 2018). Single-head evaluation is much harder, thus in our LML
framework, confusion between classiﬁers must be reduced.
10

--- PAGE 11 ---
For example, consider the case where after learning T0;T1;T2, there are very few free units to
learnT3. If we freeze all previous task weights with bbb=1and train on data of T3, we will ﬁnd that
T3cannot be learned well even after many epochs of training (see Figure 7a and Section 4.4). In this
case, we can set the bbbvalues for some less important tasks, say T0, to be small or 0, and then train
with samples of T3again, as in Figure 7b. In this case we expect to see that the error on T3decreases,
while sacriﬁcing performance of T0gradually and gracefully. As T0also indirectly affects T1and
T2(even though their bbb=1), their predictive accuracy may also be slightly decreased.
One might wonder why T0would be chosen to be gracefully forgotten. This choice is likely
to be domain and situation speciﬁc. For example, if the T0classiﬁer has not been needed to make
predictions for a long period of time, its “importance” may be lower than other classiﬁers, and its
bbbvalues could be reduced gradually to allow graceful forgetting. This would be especially useful
when there are no more free neurons for learning new tasks. In Section 5.5, we will discuss parallels
with human leaning, which seems to exhibit a similar phenomenon. For example, a friend who you
have not met and thought about for a long time may tend be to forgotten gradually over years.
Try learn task 3 with little 
available capacity but fail 
(a)
min L  on “3” 1 0 3 Unfreeze old task (0) and 
continue training task 3 
(b)
min L  on “3” 1 0 3 
Figure 7: Graceful forgetting of an old task when learning a new task without free network capacity.
In (a) when minimizing the loss on “3”, it cannot be reduced further. In (b), task “0” is chosen to
be forgotten, and its weights (blue links) are unfrozen ( bbbis set small or 0). Training on task “3”
continues to achieve a much smaller loss as previous tasks are gracefully forgotten.
3.6 Backward Transfer
In Section 3.3 we discussed forward transfer, where previous tasks help to learn the new task. Can
new tasks similarly help to improve performance on previous tasks to achieve positive backward
transfer? Our framework may be able to achieve such backward transfer by initializing backward
transfer links (between sufﬁciently similar tasks) and ﬁne-tuning on the desired tasks. In Figure 8,
the links supporting backward transfer from “O” to “0” are labelled. In Section 4.5 we evaluate this
approach to backward transfer. Similar to forward transfer, we ﬁnd that when tasks are sufﬁciently
11

--- PAGE 12 ---
similar, backward transfer works well. This appears similar to human learning, where learning
related concepts reinforce each other.
Although forward and backward transfer, as well as confusion reduction are performed with
pairs of tasks, it is possible to extend it to operate on larger subsets of tasks, or even all tasks
at once, to perform an “overall reﬁnement”. Such overall reﬁnement is obviously similar to the
more resource-intensive batch multi-task learning setting, where we assume that we can train on
all tasks at once. However, in our LML setting, after already taking care to achieve forward
transfer and confusion reduction separately and more efﬁciently, this reﬁnement process may be
done infrequently and quickly.
Newly initialized 
backward transfer 
links Backward transfer refinement 
of all previous tasks 
0 1
min L  on all tasks before “O” O
Figure 8: Consolidation of weights during backward transfer from “O” to previous tasks (“0”, “1”,
...). Since the earlier task has already been trained, this process would require little training time to
converge (as only backward links are learned from scratch).
4. Experimental Veriﬁcation
In this section we will present experimental results demonstrating the capabilities of our proposed
LML framework. These experiments mainly use the running example in Figure 2, and thus we
consider them as proof-of-concept experiments.
Task sequence We will use the binary classiﬁcation task sequence illustrated in Figure 2, with
samples taken from the balanced EMNIST dataset (Cohen et al., 2017). This task sequence is a
minimal case allowing for proof-of-concept experiments where we can be sure that there is a) clear
room for forward transfer (e.g. from “0” to “O” or “2” to “Z”) and b) clear cases of confusion (e.g.
between “0” and “O”). In a more complex task sequence it would be difﬁcult to verify whether the
proposed mechanisms work as intended. All results will be averaged across 15 random seeds. After
ﬁrst establishing feasibility with these toy experiments, it is our future work to verify the proposed
LML algorithms on more complex task sequences. In addition to using simpliﬁed experiments,
we only set bbbvalues to1or0in our experiments, equivalent to masking selected weights during
gradient descent. It would be our future work to set bbbto intermediate values in order to observe
more graded behaviors of our LML algorithms.
Architecture and training We will use a network architecture with two hidden layers with
ReLU activation. The Adam optimizer (Kingma and Ba, 2014) will be used, with the default
hyperparameters provided by Keras (Chollet and others, 2015). We will use a batch size of 64
12

--- PAGE 13 ---
and training for 10 epochs for each task, unless otherwise stated. Further details will be given for
each experiment.
The outline of this section is as follows. In Section 4.1, we will evaluate our framework on
continual learning without forgetting. In Section 4.2, we will evaluate it at accelerating task learning
with forward transfer. In Section 4.3, we will evaluate its ability to reduce confusion between tasks.
In Section 4.4, we will evaluate its ability to gracefully forget tasks to allow for new-task learning.
Finally, in Section 4.5, we will evaluate its ability to support backward transfer.
4.1 Continual Learning of New Tasks Without Forgetting
To evaluate the ability of our proposed framework to continually learn new tasks without forgetting,
we will observe the test AUC (area under the receiver operating characteristic) of each task as more
tasks are learned. AUC is a suitable metric for evaluating the performance on these binary tasks as
a result of the imbalanced data (for each task, the ratio of positive to negative samples is 1:4). When
the AUC on a task remains constant after it is initially learned, it indicates that forgetting has been
avoided. For this experiment, we will use a constant network expansion rate of 25 units per hidden
layer per task. All forward transfer links (green links in Figure 4 will be enabled and randomly
initialized. Additionally, we will use 100 positive samples for each task (and 400 negatives – 100
per character type in the negative class).
Observations From Figure 9, we can clearly see that enabling the non-forgetting mechanism by
freezing learned task weights indeed prevents forgetting, as indicated by the dashed lines remaining
at a constant AUC. While the initial performance of later tasks might be lower than without freezing,
eventually freezing pays off. For example, while the learned task “2” performance is lower when
non-forgetting is enabled (because there are fewer tunable weights in the network), eventually the
performance of the unfrozen model drops even lower (once task “O” is learned).
last learned tasktest AUC
949698100
Task 0 Task 1 Task 2 Task 3 Task O Task ZTask 0
Task 1
Task 2
Task 0 (frozen)
Task 1 (frozen)
Task 2 (frozen)
Figure 9: Evaluating the non-forgetting ability of the proposed framework. Solid lines indicate
the performance histories of each task when no freezing is used. Dashed lines correspond to
performance with freezing. To avoid clutter, only the histories of the ﬁrst three tasks are shown.
13

--- PAGE 14 ---
4.2 Forward Transfer
To evaluate the ability of our framework to support forward transfer, we will observe the test AUC
of tasks using a variety of initialization and weight copying strategies. For this experiment, we will
enable non-forgetting and also use difﬁculty-based expansion (up to a maximum expansion rate of
25 per layer per task).
Similarity and difﬁculty-based expansion method To accommodate a new task, Tj, we extend
the neural network width by an amount, Nj, proportional to the estimated difﬁculty of the task. To
computeNj, we ﬁrst compute the maximum similarity to previous tasks. To compute the similarity
between two tasks, sim(Ti;Tj), we feed positive samples of the new task, Tj, into the existing
network, and average the probabilities output by model for Ti. When the similarity between Tjand
any previous task is high (i.e. the new samples are similar to those of a previous task), proportionally
fewer nodes are added. That is, Nj=Nmax(1 max i=1;:::j 1sim(Ti;Tj)). In these experiments,
Nmax= 25 . In the extreme case where a new task is identical (or very similar) to a previous one,
no new nodes (aside from the output) may need to be added.
For the ﬁrst four tasks, we will use 100 positive samples for each task, but only 10 for the last
two tasks, so that the differences between the various forward transfer strategies can be highlighted.
The four strategies we compare are as follows:
• A LLRANDOM INIT: This strategy simply randomly initializes all forward transfer links (as in
the previous experiment).
• O NESIMILAR : This strategy ﬁrst computes the similarity between all previous tasks and the
new one. When the similarity is above = 0:5, then the corresponding forward transfer links
in the intermediate layers are randomly initialized. Additionally, when the single most similar
previous task has a similarity >, then the output layer weights are copied from that task, as
reﬂected in Figure 5.
• O NERANDOM : This strategy is similar to the previous one, except that one random previous
task is chosen to copy weights from, independent of the similarity.
• O NEWORST : This strategy initializes only the intermediate layer forward transfer links from
the least similar task, and copies output weights only from the least similar task.
Observations From Figure 10, we can see that on the last two few-shot tasks, the O NESIMILAR
strategy works best by far, achieving comparable results to tasks with 10X as much data. This is
of course possible via the clear similarity between “O” and “0” and between “Z” and “2”. The
ONERANDOM strategy, which works similar to the O NESIMILAR strategy for the intermediate
layers, performs second best, suggesting that while the weight copying from the most similar task
has the largest beneﬁt, initializing transfer links from similar tasks is also helpful.
In Figure 11, we can see what happens when, instead of only copying weights when a previous
task is similar enough (i.e. O NESIMILAR ), we “force” weights to be copied even if the most
similar task is not very similar. We call this strategy ONEALWAYS . After ﬁrst learning “0”, “1”,
“2”, and “3” with 100 positive samples, we see that for many options for the ﬁfth task (again
with only 10 positive samples), the achieved performance is better than the baseline strategy of
ALLRANDOM INIT. There are however, several characters where this strategy hurts performance
(e.g. “I”, “J”, “X”), which provide clear cases of negative transfer.
14

--- PAGE 15 ---
test AUC
0255075100
Task 0 (100) Task 1 (100) Task 2 (100) Task 3 (100) Task O (10) Task Z (10)AllRandomInit OneSimilar OneRandom OneWorstFigure 10: Evaluating the proposed forward transfer mechanism of our framework (shown here as
ONESIMILAR ). The four strategies shown in the ﬁgure are described in the text. The ﬁrst four
tasks have 100 positive samples each, while the last two have only 10, requiring few-shot learning.
Our proposed weight initialization and copying mechanism (red), performs the best. It allows for
achieving much higher test AUCs than alternative strategies.
next char (after 0, 1, 2, 3)forward transfer benefit-40-200204060
ABCDEFGH IJKLMNOTUVW XYZ
Figure 11: The test AUC difference between using the O NEALWAYS and A LLRANDOM INIT
strategies for a range of characters. Positive values indicate that O NEALWAYS performs better.
The characters here are learned after “0”, “1”, “2”, “3”. Note that “P”, “Q”, “R”, and “S” are
skipped, as they are included in the negative class.
4.3 Confusion Reduction
This experiment will evaluate the ability of our framework to reduce confusion between tasks, using
the two-stage bbb-setting and expansion strategy illustrated in Figure 6. We will observe the maximum
confusion of the last two tasks (“O” and “Z”) through the stages of confusion reduction – lower
values indicate better confusion reduction. Confusion between a pair of tasks is the percentage of
the time that positive samples from either task are mis-classiﬁed as the other task. For “O”, the
maximum confusion almost exclusively refers to confusion with “0”, and for “Z”, it is with “2”.
For this experiment, we will now use the O NESIMILAR forward transfer strategy with 100
positive samples per task. Additionally, we will use a confusion threshold of = 0:1, so that each
15

--- PAGE 16 ---
stage of confusion reduction will run if the confusion is not yet below 10%. We will also evaluate
confusion expansion amounts (second stage) of both 5 and 10 per layer.
maximum confusion
0%10%20%30%40%50%
Task O Task Zinitial pre_expansion post_expansion (5) post_expansion (10)
Figure 12: Evaluating the confusion-reduction effectiveness of the proposed framework. We report
here the maximum confusion between “O” and previous tasks (“0”, “1”, “2”, “3”) and between “Z”
and previous tasks. Most often, confusion happens between “O” and “0” and between “Z” and “2”.
Observations From Figure 12, we can see that both stages of the confusion reduction mechanism
(ﬁrst tuning, i.e. pre-expansion and then last-resort expansion with tuning, i.e. post-expansion)
contribute to reducing confusion. We also see that using a larger confusion-expansion amount
contributes to further reduction of confusion.
4.4 Graceful Forgetting
To evaluate the ability of our framework to perform graceful forgetting, we will try to learn a new
task, “3”, with a very small number of new unfrozen nodes (after ﬁrst learning “0”, “1”, “2”),
and observe the test AUC of all tasks over the course of training this task. Successful graceful
forgetting should result in the performance of the new task substantially increasing while previous
tasks experience low rates of forgetting.
For this experiment, we will return to using a constant expansion of 25 nodes per task per
layer, except for task 3, where only one new node per layer will be added. We will also enable
non-forgetting (except where graceful forgetting is performed), and randomly initialize all forward
transfer links. We will train task “3” for 10 epochs before performing the graceful forgetting, at
which point we train it for another 10 epochs.
Observations From Figures 13 and 14, we can see that graceful forgetting has a large positive
effect on the learning of the fourth task, where it is only task “0” being forgotten (as in Figure 13)
or “0”, “1” and “2” being forgotten (as in Figure 14). When all three previous tasks are unfrozen,
task “3” appears to improve faster as we may expect. In both cases, the forgetting experienced by
the previous tasks is slow.
16

--- PAGE 17 ---
Figure 13: Graceful forgetting with forgetting of
the ﬁrst task at the 10 epoch mark. We ﬁnd that
performance on task “3” quickly increases after
task 0 is unfrozen, with most of the forgetting
occurring in task 0.
Figure 14: Graceful forgetting with forgetting
of the ﬁrst three tasks at the 10 epoch mark.
We ﬁnd that the learning of task “3” is faster
following unfreezing of previous tasks, with
more distributed forgetting.
4.5 Backward Transfer
This experiment will evaluate the ability of our framework to achieve backward transfer, where
knowledge from newer tasks is used to allow older tasks to be learned better. This is done by
initializing backward transfer links and performing ﬁne-tuning of tasks.
To evaluate the ability of our framework to achieve backward transfer of knowledge, we will
ﬁrst learn a task “0” with few samples, and learn either a second similar task (“O”) or a second
dissimilar task (“Z”). After learning the second task, we will initialize the backward transfer links
(labelled in Figure 8) and ﬁne-tune “0”. For this experiment we found it sufﬁcient to set bbb= 0 for
only the weights (both new and old) in the output layer for “0”. We will observe the test AUC of
task “0” over the course of ﬁne-tuning. Effective backward transfer would result in the performance
of task “0” increasing faster during tuning when the second task contains relevant knowledge (“O”).
For this experiment, we will use a constant expansion of 25 nodes per task per layer, with non-
forgetting and random initialization of all forward transfer links (and backward transfer links when
applicable). We will train the ﬁrst task with only 10 positive samples and the second task with 50.
Fewer samples for the ﬁrst task will allow differences between backward transfer performance for
different task sequences to be emphasized.
Observations From Figure 15, we can see that when no backward transfer links are enabled (blue
line), there is almost no performance increase during tuning. This is expected since all weights
being tuned have already been trained on the data for “0”. In the cases where backward transfer
links are enabled (only for the output layer in our experiments), we see that the performance does
drop at ﬁrst. This is a result of the randomly initialized weights negatively inﬂuencing the classiﬁer.
Once ﬁne-tuning begins however, when the transfer links are from a similar task (“O” – red line), the
performance of “0” increases to an overall higher level than when the transfer is from a dissimilar
17

--- PAGE 18 ---
Task 0 fine-tuning epochsTask 0 test AUC
5060708090100
0 2 4 6 8 10no backward transfer
backward transfer from O
backward transfer from ZFigure 15: Exploring the backward transfer ability of the framework. We see that when the second
task is similar to the ﬁrst (“O” is more similar to “0” than “Z”), the beneﬁt from tuning with
backward transfer links enabled is greater.
task (“Z” – yellow line). The performance of “0” does increase in both cases however, indicating
that greater representational capacity, not only useful features, contributes to the increase.
5. Discussions
In this section we will discuss how our uniﬁed framework can be applied to learning settings beyond
fully connected neural networks or simple task sequences. Several of these learning settings are
closely related to LML in their setup and scope: multi-task learning, curriculum learning, few-shot
learning, and convolutional networks. We will provide a brief discussion of their connections to our
framework. This section concludes with a discussion on parallels with human learning.
5.1 Multi-Task and Curriculum Learning
Special cases of LML are multi-task learning (Caruana, 1997) and curriculum learning (Bengio et
al., 2009). In multi-task learning, all (Ti;Di)are provided together, allowing the model to be trained
on all tasks at the same time. In curriculum learning, all data is similarly made available, but the
problem is to identify the optimal order in which to train on data for the most efﬁcient and effective
learning. An example of an intuitive type of curriculum is to learn tasks from “easy” to “hard”
(Elman, 1993), similar to the way humans often learn new concepts.
5.2 Few-shot Learning and Bongard Problems
Lifelong and curriculum learning have great potential to achieve few-shot learning of difﬁcult
concepts. By learning relevant easier concepts earlier with LML, later harder ones could be learned
with many fewer examples via forward transfer of knowledge.
Humans can often discover the underlying rules and patterns of complex concepts with only a
few examples. We can use Bongard Problems (BPs) (Bongard, 1967) to illustrate this4. A particular
BP asks a human to recognize the underlying rule of classiﬁcation with only 6 examples on the left
4. A collection of BPs can be found here: http://www.foundalis.com/res/bps/bpidx.htm .
18

--- PAGE 19 ---
(a) (b) Figure 16: Two examples of BPs. (a) BP #5 is simple, where the rule is polygonal shapes on the
left and curvilinear shapes on the right. (b) BP #99 is quite hard, where the rule is that larger shapes
formed by connecting similar small shapes overlap vs. the larger shapes do not overlap. Such
highly abstract rules are common among BPs. For difﬁcult BPs, we ﬁnd that for both humans and
ML models, adding more training samples often does not help. Instead, it takes a “stroke of insight”
or sufﬁciently similar previously acquired knowledge to solve it.
(say they are positive) and 6 examples on the right (negative samples). Two such BPs are shown in
Figure 16, where (a) is a simple BP and (b) is a difﬁcult BP for humans. To solve BPs, humans often
rely on highly abstract rules learned earlier in their lives (or in easier BPs). Some form of LML of
many visual abstract concepts and shapes must likely have happened in humans so that they can
solve BPs well with only 12 examples in total.
We can outline a possible few-shot learning process to learn such BPs in our LML framework,
in a curriculum-like fashion. First, we can train (or pre-train) on simpler tasks to recognize simpler
shapes. Next, harder and more complex visual shapes and BPs would be trained. This idea of
using curriculum learning to “working up to” more difﬁcult BPs is reﬂected in Figure 17. In this
illustrative example, we feed previous class labels (the output nodes) into nodes of new tasks to
provide opportunity for forward transfer of knowledge. With this approach, BPs could be solved
with fewer examples, as shown by Yun, Bohn, and Ling (2020). Note that in this illustrative
example, the networks of subsequent tasks can have different and increasing depths in order to
solve more difﬁcult problems after easier problems are learned.
To solve difﬁcult BPs with a few examples (such as BP #99 in Figure 16b), forward transfer
of knowledge is crucial. If a person cannot solve BP #99 (or other hard BPs) in several minutes,
normally more training data would not help much. Often “a ﬂash of inspiration” or “Aha!” moment
would suddenly occur and the problem is solved. This is because a person has learned probably
thousands or tens of thousands of concepts and abstract relations in his/her life, and the correct
knowledge is suddenly combined to solve the hard BP.
5.3 Pre-Training Methods
Pre-training is a ubiquitous process in deep learning (Devlin et al., 2019; Girshick et al., 2014; Wang
and Gupta, 2015). In computer vision (CV), state-of-the-art approaches often pre-train on a non-
target task for which there is abundant data (Mahajan et al., 2018), such as the ImageNet dataset
(Russakovsky et al., 2015). Pre-trained CV models such as VGG-16 (Simonyan and Zisserman,
2014) have been publicly released, allowing anyone to ﬁne-tune their model to achieve otherwise
infeasible performance on a target task. By reducing the need for target task data, pre-trained CV
19

--- PAGE 20 ---
Task 1:  Stroke types (e.g. 
straight and curved) 
Task 2: 
Basic shapes 
Task k: 
Bongard problem 5 
Task >k: 
More difficult BPs 1 2
kFigure 17: An illustrative example of curriculum learning to work up to solving Bongard problems
with a small number of positive and negative examples. After learning to differentiate between basic
strokes types, shapes, and possibly other early tasks, these representations including the output and
hidden layers can be transferred to Tjto possibly solving BPs such as BP#5 (sharp angles vs. smooth
curves) with a few examples. This process can continue to allow us to solve a more complex visual
reasoning problem with a few samples. Note that networks of subsequent tasks can have increasing
depths in order to solve harder BPs after easier ones have been learned.
models have become a common approach to few-shot learning. (Ramalho, Sousbie, and Peluchetti,
2019). In natural language processing, pre-trained language models have become popular (Conneau
et al., 2017; Peters et al., 2018; Devlin et al., 2019). For example, BERT (Bidirectional Encoder
Representations from Transformers) has shown that state-of-the-art performance can be attained on
several different NLP tasks with one pre-trained model (Devlin et al., 2019).
With pre-training, often a large neural network is ﬁrst trained on one or more tasks with very
large labeled training datasets. The trained neural network would then be frozen, and a smaller
network stacked on top of the large network and trained for improved performance on target tasks.
From the perspective of our framework, the consolidation values of the smaller networks would be
considered unfrozen for training.
Pre-training can thus be viewed as certain weight consolidation policies in our LML framework.
The main difference is that most of our consolidation algorithms in the last two sections apply to
weights per tasks, or column-wise in our ﬁgures, while for pre-training, consolidation is layer-wise.
An interesting research topic would thus be to consider designing consolidation policies which
perform both layer- and column-wise consolidation based on the task at hand.
A natural way to ﬁne-tune the consolidation policy when tuning on the target task is to consider
how much to unfreeze each layer. Instead of simply tuning the output layer or tuning the entire
network, we can interpolate between these two policies, to allow gradual unfreezing of layers in the
network to best adapt to the target task without losing the generalization beneﬁts of pre-training.
See (Erhan et al., 2010) for an example of such directions.
20

--- PAGE 21 ---
5.4 Convolutional Networks
We can consider the application of our framework to convolutional neural networks (CNNs), not
only fully connected feed-forward networks. Training on image tasks is often performed with
resource-intensive batch learning, while our framework would allow for increased efﬁciency while
being careful to maintain the high performance associated with batch learning.
To adapt the consolidation mechanism to CNNs, each bbbican correspond to a ﬁlter rather than a
single weight (as in a densely connected layer). A ﬁlter is essentially a set of weights which takes
the representation from the previous layer (feature maps), and transforms it into a new feature map.
Largebbbon a ﬁlter means that it cannot be easily modiﬁed during learning. Next, we consider how
to achieve the various LML properties in CNNs:
mammals 
(a) (b)birds 0 1input 
image 
feature 
maps feature 
maps 
feature 
maps feature 
maps concat 
feature 
maps concat 
flatten feature 
maps 
flatten 
Figure 18: A simpliﬁed visualization of how our framework can be applied to convolutional
networks. In (a) is a diagram of our consolidation mechanism and expansion being applied for
learning two tasks with a fully connected network. In (b) is the corresponding CNN with the same
high-level topology as (a) being applied to e.g. mammals for T0and birds for T1. Links with unﬁlled
arrow heads indicate ﬁlter weights, and links with ﬁlled arrow heads indicate weights between fully
connected layers. Dotted (red) indicates large bbbvalues, solid (blue) indicates bbbvalues of 0, and
dashed (green) indicates transfer links whose consolidation may depend on task similarity.
Continual learning of new tasks without forgetting To extend the network for new tasks, we
now add columns of convolutional ﬁlters, as reﬂected in Figure 18b (solid links). For previous tasks
not to be forgotten, their bbb=1(see the red dotted links of Figure 18). For more difﬁcult tasks, we
can add a greater number of ﬁlters (analogous to a greater number of nodes).
21

--- PAGE 22 ---
Forward transfer The two techniques for encouraging forward transfer also extend to CNNs.
First, instead of copying weights from previous tasks when similar to the new task, we can now
copy over ﬁlter values. This can be done using similar ideas to those laid out in Section 3.3. Second,
we can encourage forward transfer and avoid negative transfer through disconnecting the forward
transfer links; essentially these ﬁlters are initialized to zero and the bbb=1, preventing negative
transfer of knowledge from the previous task.
5.5 Speculations on Parallels with Human Learning
“Rain man” 
Great rote memory but less transfer 
Large free 
capacity 
Memory loss of recent events 
Alzheimer’s disease 
Little free capacity. 
Aggressive neural 
pruning 
Sleep deprived, tired brain 
min L  on “2” + “Z” No “replay” to 
reduce confusion. 
Poor lifelong 
learning algorithms 
2 Z(a)
(b)
(c)
Figure 19: Illustrations of parallels
between human learning behaviours
and those that can be exhibited by our
uniﬁed LML framework. Orange dotted
links represent intermediate bbbvalues.As our framework focuses on controlling the ﬂexibility
of individual network weights, it is natural to ask how
our approach lines up with the mammalian brain whose
lifelong concept learning behaviours we are attempting
to capture. We will discuss brieﬂy how certain behaviours
can be exhibited by our framework with speciﬁc network
sizes, training, and consolidation policies, and how it may
translate to what happens in the human brain at a similar
level of abstraction.
Resourceful and versatile Ideally, human learning
allows us to acquire a lot of knowledge, yet be ﬂexible
enough to adapt to new experiences and make meaningful
connections between experiences. This is analogous
to our framework when everything works perfectly,
including but not limited to: the network having an ample
supply of free units for new tasks (Section 3.2), using
previous knowledge to learn new tasks (Section 3.3),
not forgetting previous tasks while learning new ones
(Section 3.2), and using new knowledge to reﬁne old
skills (Section 3.6).
“Rain man” Ifbbbvalues for previous tasks are very
large to achieve non-forgetting (Section 3.2), and no
connections are made between previous and new tasks,
knowledge transfer (Section 3.3 and 3.6) will likely not
happen. This is reminiscent of Kim Peek, who was able
to remember vast amounts of information, but performed
poorly at abstraction-related tasks that require connecting
unrelated skills and information and was the inspiration
for the main character of the movie Rain Man (Treffert
and Christensen, 2005). See Figure 19a for an illustration
of our framework used to model similar behaviour.
Memory loss As shown in Section 3.5, when availabil-
ity of free units is limited, or worse, if units of the neural
network are pruned away, graceful forgetting can be used
to learn new tasks. In certain brain diseases, some “tasks” could be forgotten ﬁrst. For example, an
22

--- PAGE 23 ---
early stage of Alzheimer’s disease (a highly complex neurological disease not fully understood) is
usually characterized by good memory on events years ago but poor on recent events (Tierney et al.,
1996). This can be modeled by large bbbvalues for old tasks and small values for recent ones (shown
as orange links in Figure 19b). Aggressive neuron pruning may also be happening (similar to node
pruning in deep neural networks) to degrade the performance of tasks.
Sleep deprived The human brain is suspected of performing important memory-related processes
during sleep, and sleep deprivation is detrimental to memory performance (Walker, 2010; Killgore,
2010). Confusion reduction and backward transfer are important stages of our proposed approach
which use rehearsal (functionally similar to memory replay), where the model is exposed to samples
from past tasks in order to perform ﬁne-tuning to achieve various properties (Section 3.4 and 3.6).
Without these rehearsal-employing steps, the model may be less able to distinguish between samples
of similar classes. Additionally, the ability to identify connections between newer tasks and older
ones will be lost, so that potentially useful newly acquired skills cannot beneﬁt older tasks. In
addition, when the brain is “tired” and not well-rested, this may be analogous to poor optimization
of policies in our framework (and in all ML algorithms). This is reﬂected in Figure 19c through
the orange transfer and new task weights, which would reduce the ability for the new task to be
learned well or efﬁciently make use of previous task knowledge. If optimization is not thorough in
Equation 1, performance would be poor in all aspects of LML. See Figure 19c for an illustration.
6. Conclusions
In this work, we presented a conceptual uniﬁed framework for LML using one central mechanism
based on consolidation. We discussed how our approach can capture many important properties of
lifelong learning of concepts, including non-forgetting, forward and backward transfer, confusion
reduction, and so on, under one roof. Proof-of-concept results were reported to support the
feasibility of these properties. Rather than aiming for state-of-the-art results, this paper proposed
research directions to help inform future LML research, including new algorithms and theoretical
results. Last, we noted several similarities between our models with different training and
consolidation policies and certain behaviors in human learning.
Acknowledgments
We are thankful for the constructive discussion and comments from lab members and colleagues
on the many drafts of this work. We also acknowledge the support of the Natural Sciences
and Engineering Research Council of Canada (NSERC) through the Discovery Grants Program.
NSERC invests annually over $1 billion in people, discovery and innovation.
23

--- PAGE 24 ---
References
Aljundi, R.; Babiloni, F.; Elhoseiny, M.; Rohrbach, M.; and Tuytelaars, T. 2018. Memory aware
synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer
Vision (ECCV) , 139–154.
Bengio, Y .; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In Proceedings
of the 26th annual international conference on machine learning , 41–48. ACM.
Bongard, M. M. 1967. The problem of recognition. Fizmatgiz, Moscow .
Caruana, R. 1997. Multitask learning. Machine learning 28(1):41–75.
Chaudhry, A.; Dokania, P. K.; Ajanthan, T.; and Torr, P. H. S. 2018. Riemannian Walk for
Incremental Learning: Understanding Forgetting and Intransigence. CoRR abs/1801.10112.
Chaudhry, A.; Rohrbach, M.; Elhoseiny, M.; Ajanthan, T.; Dokania, P. K.; Torr, P. H.; and Ranzato,
M. 2019. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486 .
Chollet, F., et al. 2015. Keras.
Cohen, G.; Afshar, S.; Tapson, J.; and van Schaik, A. 2017. EMNIST: an extension of MNIST to
handwritten letters. CoRR abs/1702.05373.
Conneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Bordes, A. 2017. Supervised Learning of
Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the
2017 Conference on Empirical Methods in Natural Language Processing , 670–680. Copenhagen,
Denmark: Association for Computational Linguistics.
Cunha, C.; Brambilla, R.; and Thomas, K. L. 2010. A simple role for BDNF in learning and
memory? Frontiers in molecular neuroscience 3:1.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers) , 4171–4186. Minneapolis,
Minnesota: Association for Computational Linguistics.
Elman, J. L. 1993. Learning and development in neural networks: The importance of starting small.
Cognition 48(1):71–99.
Erhan, D.; Bengio, Y .; Courville, A.; Manzagol, P.-A.; Vincent, P.; and Bengio, S. 2010. Why
does unsupervised pre-training help deep learning? Journal of Machine Learning Research
11(Feb):625–660.
Fei-Fei, L.; Fergus, R.; and Perona, P. 2006. One-shot learning of object categories. IEEE
transactions on pattern analysis and machine intelligence 28(4):594–611.
Girshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition , 580–587.
24

--- PAGE 25 ---
Isele, D., and Cosgun, A. 2018. Selective experience replay for lifelong learning. In Thirty-second
AAAI conference on artiﬁcial intelligence .
Kamilaris, A., and Prenafeta-Bold ´u, F. X. 2018. Deep learning in agriculture: A survey. Computers
and electronics in agriculture 147:70–90.
Kamra, N.; Gupta, U.; and Liu, Y . 2017. Deep generative dual memory network for continual
learning. arXiv preprint arXiv:1710.10368 .
Killgore, W. D. 2010. Effects of sleep deprivation on cognition. In Progress in brain research ,
volume 185. Elsevier. 105–129.
Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Desjardins, G.; Rusu, A. A.; Milan, K.;
Quan, J.; Ramalho, T.; Grabska-Barwinska, A.; Hassabis, D.; Clopath, C.; Kumaran, D.; and
Hadsell, R. 2016. Overcoming catastrophic forgetting in neural networks. cite arxiv:1612.00796.
Li, Z., and Hoiem, D. 2017. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence 40(12):2935–2947.
Litjens, G.; Kooi, T.; Bejnordi, B. E.; Setio, A. A. A.; Ciompi, F.; Ghafoorian, M.; Van Der Laak,
J. A.; Van Ginneken, B.; and S ´anchez, C. I. 2017. A survey on deep learning in medical image
analysis. Medical image analysis 42:60–88.
Mahajan, D.; Girshick, R.; Ramanathan, V .; He, K.; Paluri, M.; Li, Y .; Bharambe, A.; and van der
Maaten, L. 2018. Exploring the limits of weakly supervised pretraining. In Proceedings of the
European Conference on Computer Vision (ECCV) , 181–196.
McCloskey, M., and Cohen, N. J. 1989. Catastrophic interference in connectionist networks: The
sequential learning problem. The psychology of learning and motivation 109–165.
Pan, S. J., and Yang, Q. 2009. A survey on transfer learning. IEEE Transactions on knowledge and
data engineering 22(10):1345–1359.
Parisi, G. I.; Kemker, R.; Part, J. L.; Kanan, C.; and Wermter, S. 2019. Continual lifelong learning
with neural networks: A review. Neural Networks .
Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018.
Deep contextualized word representations. arXiv preprint arXiv:1802.05365 .
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models
are unsupervised multitask learners. OpenAI Blog 1(8):9.
Ramalho, T.; Sousbie, T.; and Peluchetti, S. 2019. An empirical study of pretrained representations
for few-shot classiﬁcation. arXiv preprint arXiv:1910.01319 .
Rebufﬁ, S.-A.; Kolesnikov, A.; Sperl, G.; and Lampert, C. H. 2017. iCaRL: Incremental Classiﬁer
and Representation Learning. In CVPR , 5533–5542. IEEE Computer Society.
25

--- PAGE 26 ---
Ritter, H.; Botev, A.; and Barber, D. 2018. Online structured laplace approximations for overcoming
catastrophic forgetting. In Advances in Neural Information Processing Systems , 3738–3748.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.;
Khosla, A.; Bernstein, M.; et al. 2015. Imagenet large scale visual recognition challenge.
International journal of computer vision 115(3):211–252.
Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.; Kirkpatrick, J.; Kavukcuoglu, K.;
Pascanu, R.; and Hadsell, R. 2016. Progressive Neural Networks. CoRR abs/1606.04671.
Shen, D.; Wu, G.; and Suk, H.-I. 2017. Deep learning in medical image analysis. Annual review of
biomedical engineering 19:221–248.
Shin, H.; Lee, J. K.; Kim, J.; and Kim, J. 2017. Continual learning with deep generative replay. In
Advances in Neural Information Processing Systems , 2990–2999.
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez, A.; Hubert, T.; Baker,
L.; Lai, M.; Bolton, A.; et al. 2017. Mastering the game of go without human knowledge. Nature
550(7676):354–359.
Simonyan, K., and Zisserman, A. 2014. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 .
Thrun., S. 1995. A lifelong learning perspective for mobile robot control. Intelligent Robots and
Systems .
Thrun, S. 1998. Lifelong learning algorithms. In Learning to learn . Springer. 181–209.
Tierney, M.; Szalai, J.; Snow, W.; Fisher, R.; Nores, A.; Nadon, G.; Dunn, E.; and George-
Hyslop, P. S. 1996. Prediction of probable Alzheimer’s disease in memory-impaired patients: A
prospective longitudinal study. Neurology 46(3):661–665.
Treffert, D. A., and Christensen, D. D. 2005. Inside the mind of a savant. Scientiﬁc American
293(6):108–113.
Walker, M. P. 2010. Sleep, memory and emotion. In Progress in brain research , volume 185.
Elsevier. 49–68.
Wang, X., and Gupta, A. 2015. Unsupervised learning of visual representations using videos. In
Proceedings of the IEEE International Conference on Computer Vision , 2794–2802.
Wu, Y .; Chen, Y .; Wang, L.; Ye, Y .; Liu, Z.; Guo, Y .; and Fu, Y . 2019. Large Scale Incremental
Learning.
Xu, J., and Zhu, Z. 2018. Reinforced continual learning. In Advances in Neural Information
Processing Systems , 899–908.
Yoon, J.; Yang, E.; Lee, J.; and Hwang, S. J. 2018. Lifelong Learning with Dynamically Expandable
Networks. In ICLR (Poster) . OpenReview.net.
26

--- PAGE 27 ---
Yun, X.; Bohn, T.; and Ling, C. 2020. A Deeper Look at Bongard Problems. In Canadian
Conference on Artiﬁcial Intelligence , 528–539. Springer.
Zenke, F.; Poole, B.; and Ganguli, S. 2017. Continual Learning Through Synaptic Intelligence.
In Precup, D., and Teh, Y . W., eds., ICML , volume 70 of Proceedings of Machine Learning
Research , 3987–3995. PMLR.
Zhang, Y ., and Yang, Q. 2017. A survey on multi-task learning. arXiv preprint arXiv:1707.08114 .
Zhang, J.; Zhang, J.; Ghosh, S.; Li, D.; Tasci, S.; Heck, L.; Zhang, H.; and Kuo, C.-C. J. 2020.
Class-incremental learning via deep model consolidation. In The IEEE Winter Conference on
Applications of Computer Vision , 1131–1140.
27
