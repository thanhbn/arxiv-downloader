# Học Liên Tục với Trọng Số Thích Ứng (CLAW)

Tameem Adel
Khoa Kỹ thuật, Đại học Cambridge
tah47@cam.ac.uk

Han Zhao
Đại học Carnegie Mellon
han.zhao@cs.cmu.edu

Richard E. Turner
Khoa Kỹ thuật, Đại học Cambridge
Microsoft Research
ret26@cam.ac.uk

## TÓM TẮT

Các phương pháp học liên tục nhằm học thành công một tập hợp các nhiệm vụ liên quan được trình bày theo cách trực tuyến. Gần đây, một số khung làm việc đã được phát triển để cho phép học sâu được triển khai trong kịch bản học tập này. Một quyết định mô hình hóa quan trọng là mức độ kiến trúc nên được chia sẻ giữa các nhiệm vụ. Một mặt, mô hình hóa riêng biệt từng nhiệm vụ tránh được quên thảm khốc nhưng không hỗ trợ học chuyển giao và dẫn đến các mô hình lớn. Mặt khác, việc xác định cứng nhắc một thành phần chia sẻ và một phần đặc thù nhiệm vụ cho phép chuyển giao nhiệm vụ và giới hạn kích thước mô hình, nhưng dễ bị quên thảm khốc và hạn chế hình thức chuyển giao nhiệm vụ có thể xảy ra. Lý tưởng nhất, mạng nên xác định một cách thích ứng những phần nào của mạng để chia sẻ theo cách hướng dữ liệu. Ở đây chúng tôi giới thiệu một phương pháp như vậy được gọi là Học Liên Tục với Trọng Số Thích Ứng (CLAW), dựa trên mô hình hóa xác suất và suy luận biến phân. Các thí nghiệm cho thấy CLAW đạt được hiệu suất tiên tiến trên sáu tiêu chuẩn về hiệu suất học liên tục tổng thể, được đo bằng độ chính xác phân loại, và về việc giải quyết quên thảm khốc.

## 1. GIỚI THIỆU

Học liên tục (CL), đôi khi được gọi là học suốt đời hoặc học tăng dần, đề cập đến một khung trực tuyến nơi kiến thức thu được từ việc học các nhiệm vụ trong quá khứ được giữ lại và tích lũy để có thể được tái sử dụng trong hiện tại và tương lai. Dữ liệu thuộc về các nhiệm vụ khác nhau có thể không i.i.d. Một người học liên tục phải có khả năng học một nhiệm vụ mới, quan trọng là không quên các nhiệm vụ trước đó. Ngoài ra, các khung CL nên liên tục thích ứng với bất kỳ dịch chuyển miền nào xảy ra giữa các nhiệm vụ. Các cập nhật học tập phải có tính tăng dần - tức là, mô hình được cập nhật tại mỗi nhiệm vụ chỉ sử dụng dữ liệu mới và mô hình cũ, không có quyền truy cập vào tất cả dữ liệu trước đó (từ các nhiệm vụ sớm hơn) - do các ràng buộc về tốc độ, bảo mật và quyền riêng tư. Phải tìm được sự cân bằng giữa việc thích ứng với nhiệm vụ mới và duy trì ổn định để bảo tồn kiến thức từ các nhiệm vụ trước đó.

Việc thích ứng quá mức có thể dẫn đến quên cách thực hiện các nhiệm vụ sớm hơn một cách vô tình. Thật vậy, quên thảm khốc là một trong những bệnh lý chính trong học liên tục.

Nhiều phương pháp học liên tục sử dụng một kiến trúc được chia a priori thành (i) một phần toàn cầu phát triển chậm; và (ii) một phần cục bộ, đặc thù nhiệm vụ, phát triển nhanh. Đây là một cách để cho phép chuyển giao đa nhiệm vụ trong khi giảm thiểu quên thảm khốc, điều này đã được chứng minh là hiệu quả, mặc dù có những hạn chế. Việc xác định a priori các phần toàn cầu chia sẻ và cục bộ đặc thù nhiệm vụ trong kiến trúc hạn chế tính linh hoạt. Khi các nhiệm vụ phức tạp và không đồng nhất hơn được xem xét, người ta muốn có một phương pháp linh hoạt hơn, hướng dữ liệu để xác định lượng chia sẻ phù hợp giữa các nhiệm vụ. Ở đây, chúng tôi hướng đến việc tự động hóa quá trình thích ứng kiến trúc để mỗi neuron của mạng có thể được giữ nguyên, tức là hoạt động như toàn cầu, hoặc được thích ứng với nhiệm vụ mới một cách cục bộ. Khung suy luận biến phân được đề xuất của chúng tôi đủ linh hoạt để học phạm vi mà trong đó các tham số thích ứng có thể thay đổi. Chúng tôi giới thiệu cho mỗi neuron một tham số nhị phân kiểm soát có thích ứng hay không, và hai tham số để kiểm soát mức độ thích ứng. Tất cả các tham số được học thông qua suy luận biến phân. Chúng tôi giới thiệu khung của mình như một mở rộng của thuật toán học liên tục biến phân, có bản chất biến phân và Bayes tuần tự giúp thuận tiện cho quy trình mô hình hóa và thích ứng kiến trúc của chúng tôi. Các ý tưởng mô hình hóa của chúng tôi cũng có thể được áp dụng cho các khung học liên tục khác, xem Phụ lục để thảo luận ngắn gọn.

Chúng tôi nhấn mạnh các đóng góp sau: (1) Một khung mô hình hóa linh hoạt tự động hóa việc thích ứng các phần cục bộ và toàn cầu của kiến trúc liên tục (đa nhiệm vụ). Điều này tối ưu hóa sự cân bằng giữa việc giảm thiểu quên thảm khốc và cải thiện chuyển giao nhiệm vụ. (2) Một thuật toán suy luận biến phân xác suất hỗ trợ cập nhật tăng dần với các tham số được học thích ứng. (3) Khả năng kết hợp các phương pháp mô hình hóa và suy luận của chúng tôi mà không cần bất kỳ tăng cường đáng kể nào về kiến trúc (không cần neuron mới). (4) Kết quả tiên tiến trong sáu thí nghiệm trên năm bộ dữ liệu, chứng minh hiệu quả của khung của chúng tôi về độ chính xác tổng thể và giảm quên thảm khốc.

## 2. NỀN TẢNG VỀ HỌC LIÊN TỤC BIẾN PHÂN (VCL)

Trong bài báo này, chúng tôi sử dụng Học Liên Tục Biến Phân (VCL, Nguyen et al., 2018) làm khung học liên tục cơ bản. Tuy nhiên, các phương pháp của chúng tôi áp dụng cho các khung khác, xem Phụ lục (Mục A.1). VCL là một khung Bayes biến phân nơi hậu nghiệm của các tham số mô hình được học và cập nhật liên tục từ một chuỗi T bộ dữ liệu. Cụ thể hơn, biểu thị p(y|θ,x) là phân phối xác suất được trả về bởi một bộ phân loại phân biệt với đầu vào x, đầu ra y và tham số θ. Với Dt, chúng ta xấp xỉ hậu nghiệm không thể tính được p(θ|D1:t) sau khi quan sát t bộ dữ liệu đầu tiên thông qua một phân phối biến phân có thể tính được qt như:

qt(θ) ∝ 1/Zt qt-1(θ)p(Dt|θ), (1)

trong đó q0 là tiên nghiệm p, p(Dt|θ) = ∏n=1^Nt p(yt^(n)|θ,xt^(n)), và Zt là hằng số chuẩn hóa không phụ thuộc vào θ mà chỉ phụ thuộc vào dữ liệu D. Khung này cho phép hậu nghiệm xấp xỉ qt(θ) được cập nhật tăng dần từ hậu nghiệm xấp xỉ trước đó qt-1(θ) theo cách trực tuyến.

Trong VCL, việc xấp xỉ trong (1) được thực hiện bằng cách tối thiểu hóa divergence KL sau đây trên một họ Q các phân phối có thể tính được:

qt(θ) = argmin[q∈Q] KL[q(θ) || 1/Zt qt-1(θ)p(Dt|θ)]. (2)

Khung này có thể được tăng cường để giảm thiểu thêm quên thảm khốc bằng cách sử dụng coreset (Nguyen et al., 2018), tức là một tập hợp đại diện của dữ liệu từ các nhiệm vụ đã quan sát trước đó có thể phục vụ như bộ nhớ và có thể được xem lại trước khi đưa ra quyết định. Như đã thảo luận trong Công trình Liên quan, điều này dẫn đến chi phí phụ của bộ nhớ và tối ưu hóa (chọn các điểm dữ liệu đại diện nhất). Công trình trước đây về VCL đã xem xét các mô hình đơn giản mà không có xây dựng hoặc thích ứng kiến trúc tự động.

## 3. PHƯƠNG PHÁP CLAW CỦA CHÚNG TÔI

Trong các phương pháp CL trước đó, các phần của kiến trúc mạng được chia sẻ giữa các nhiệm vụ đã học được chỉ định a priori. Để giảm bớt tính cứng nhắc này và cân bằng hiệu quả giữa thích ứng và ổn định, chúng tôi đề xuất một mô hình liên tục, đa nhiệm vụ trong đó việc thích ứng kiến trúc được hướng dẫn bởi dữ liệu bằng cách học neuron nào cần được thích ứng cũng như khả năng thích ứng tối đa cho mỗi neuron. Tất cả các tham số mô hình (bao gồm những tham số được sử dụng để thích ứng) được ước tính thông qua một thuật toán suy luận biến phân hiệu quả học tăng dần từ dữ liệu của các nhiệm vụ liên tiếp, không cần lưu trữ (cũng không tạo ra) dữ liệu từ các nhiệm vụ trước đó và không mở rộng kích thước mạng.

### 3.1 MÔ HÌNH HÓA

Với các tham số mô hình θ, mục tiêu biến phân tổng thể mà chúng ta hướng đến tối đa hóa tại nhiệm vụ với chỉ số t tương đương với khả năng biên trực tuyến sau:

L(θ) = -KL[qt(θ)||qt-1(θ)] + ∑n=1^Nt Eqt(θ)[log p(yt^(n)|xt^(n);θ)]. (3)

Chúng tôi đề xuất một khung nơi kiến trúc, có tham số là θ, được thích ứng một cách linh hoạt dựa trên các nhiệm vụ có sẵn, thông qua một quy trình học tập sẽ được mô tả bên dưới. Với mỗi nhiệm vụ, chúng tôi tự động hóa việc thích ứng các đóng góp của neuron. Cả quyết định thích ứng (tức là có thích ứng hay không) và mức độ thích ứng tối đa được phép cho mỗi neuron đều được học. Chúng tôi đề cập đến biến thích ứng nhị phân là ξ. Có một biến khác s được học theo cách đa nhiệm vụ để kiểm soát mức độ thích ứng tối đa, sao cho biểu thức b = s/(1+e^(-a)) - 1 giới hạn mức độ khác biệt của trọng số đặc thù nhiệm vụ so với trọng số toàn cầu, trong trường hợp neuron tương ứng sẽ được thích ứng. Tham số a mô tả thích ứng không ràng buộc, như được mô tả sau này.

Chúng tôi minh họa mô hình được đề xuất để thực hiện việc thích ứng này bằng cách học các đóng góp xác suất của các neuron khác nhau trong kiến trúc mạng trên cơ sở từng nhiệm vụ. Chúng tôi tiếp theo với các chi tiết suy luận. Các bước của mô hình hóa được đề xuất được liệt kê như sau:

Đối với một nhiệm vụ T, bộ phân loại mà chúng tôi đang mô hình hóa xuất ra: ∑n=1^NT log p(y^(n)|x^(n);wT).

Trọng số đặc thù nhiệm vụ wT có thể được biểu thị theo các trọng số toàn cầu tương ứng như sau:
wT = (1 + bT ⊙ ξT) ⊙ w. (4)

Ký hiệu ⊙ biểu thị phép nhân theo từng phần tử (Hadamard).

Đối với mỗi nhiệm vụ T và mỗi neuron j tại tầng i, ξT[i,j] là một biến nhị phân chỉ ra liệu trọng số tương ứng có được thích ứng (ξT[i,j] = 1) hay không được thích ứng (ξT[i,j] = 0). Ban đầu giả sử rằng xác suất thích ứng ξT[i,j] tuân theo phân phối Bernoulli với xác suất pi,j, ξT[i,j] ~ Bernoulli(pi,j). Vì Bernoulli này không đơn giản để tối ưu hóa, và để áp dụng một quy trình suy luận có thể mở rộng dựa trên các biến tiềm ẩn liên tục, chúng tôi thay thế Bernoulli này bằng một Gaussian có cùng trung bình và phương sai từ đó chúng tôi rút ξT[i,j]. Vì mục đích đạt được độ trung thực cao hơn so với những gì được cấp bởi một Gaussian tiêu chuẩn, chúng tôi dựa suy luận của mình trên một ước tính Gaussian biến phân. Mặc dù trong bối cảnh khác với học liên tục và với các bộ ước tính khác, ý tưởng thay thế Bernoulli bằng Gaussian tương đương đã được chứng minh là hiệu quả với dropout.

Việc xấp xỉ phân phối Bernoulli bằng phân phối Gaussian tương ứng được thực hiện bằng cách khớp trung bình và phương sai. Trung bình và phương sai của phân phối Bernoulli lần lượt là pi,j, pi,j(1-pi,j). Một phân phối Gaussian với cùng trung bình và phương sai được sử dụng để khớp với ξT[i,j].

ξT[i,j] ~ N(pi,j; pi,j(1-pi,j)). (5)

Biến bT kiểm soát cường độ của việc thích ứng và nó giới hạn phạm vi thích ứng thông qua:
1 + bT = s/(1 + e^(-aT)). (6)

Để thích ứng tối đa là s. Biến aT là một giá trị thích ứng không ràng buộc, tương tự như trong (Swietojanski & Renals, 2014). Việc cộng thêm 1 là để tạo điều kiện sử dụng phân phối xác suất trong khi vẫn giữ một phạm vi thích ứng cho phép sự suy giảm hoặc khuếch đại đóng góp của mỗi neuron.

Trước khi đối mặt với bộ dữ liệu đầu tiên và nhiệm vụ học t = 1, tiên nghiệm trên trọng số q0(w) = p(w) được chọn là tiên nghiệm thang log, có thể được biểu thị như: p(log|w|) ∝ c, trong đó c là một hằng số. Tiên nghiệm thang log có thể được mô tả thay thế như:

p(|w|) ∝ 1/|w|. (7)

Ở mức độ cao, việc thích ứng đóng góp neuron có thể được xem như một tổng quát hóa của cơ chế chú ý trong bối cảnh học liên tục. Việc áp dụng quy trình thích ứng này cho đầu vào dẫn đến một cơ chế chú ý. Tuy nhiên, phương pháp của chúng tôi tổng quát hơn vì chúng tôi không chỉ áp dụng nó cho tầng rất dưới cùng (tức là đầu vào), mà xuyên suốt toàn bộ mạng. Tiếp theo chúng tôi chỉ ra cách cơ chế suy luận biến phân của chúng tôi cho phép chúng tôi học các tham số thích ứng.

### 3.2 SUY LUẬN

Chúng tôi mô tả các chi tiết liên quan đến cơ chế suy luận biến phân được đề xuất. Các tham số thích ứng được bao gồm trong các tham số biến phân.

Các tham số mô hình (phiên bản chưa thích ứng) bao gồm các vector trọng số w. Để tự động hóa thích ứng, chúng tôi thực hiện suy luận trên pi,j, vốn sẽ là một siêu tham số của tiên nghiệm. Nhân w với (1 + b ⊙ ξ) trong đó ξ được phân phối theo (5), thì từ (4) với biến nhiễu ngẫu nhiên ε ~ N(0;1):

wT[i,j] = w[i,j] * (1 + b[i,j] * (p[i,j] + b[i,j] * √(p[i,j](1-p[i,j])) * ε)),

q(w[i,j]|ξ[i,j]) ~ N(w[i,j](1 + b[i,j]p[i,j]); b²[i,j]w²[i,j]p[i,j](1-p[i,j])). (8)

Từ (7) và (8), divergence KL tương ứng giữa hậu nghiệm biến phân của w, q(w|ξ) và tiên nghiệm p(w) như sau. Các chỉ số dưới được loại bỏ khi q lần lượt được sử dụng như một chỉ số dưới để cải thiện khả năng đọc. Các tham số biến phân là w[i,j] và p[i,j].

KL[q(w[i,j]|ξ[i,j])||p(w[i,j])] = Eq(w|ξ)[log[q(w[i,j]|ξ[i,j])/p(w[i,j])]] =
Eq(w|ξ)[log q(w[i,j]|ξ[i,j])] - Eq(w|ξ)[log p(w[i,j])] = -H(q(w[i,j]|ξ[i,j])) - Eq(w|ξ)[log p(w[i,j])] (9)

= 0.5(1 + log(2π) + log(b²[i,j]p[i,j](1-p[i,j]))) - Eq(w|ξ)[log 1/|w|] (10)

= log b[i,j] - 0.5 log p[i,j] - 0.5 log(1-p[i,j]) + c + Eq(w|ξ)[log|w|], (11)

trong đó việc chuyển từ (9) sang (10) là do tính toán entropy của Gaussian q(w[i,j]|ξ[i,j]) được định nghĩa trong (8). Việc chuyển từ (10) sang (11) là do sử dụng tiên nghiệm thang log, tương tự như Phụ lục C trong (Kingma et al., 2015) và Mục 4.2 trong (Molchanov et al., 2017). Eq(w|ξ)[log|w|] được tính toán thông qua một xấp xỉ chính xác tương tự như phương trình (14) trong (Molchanov et al., 2017), với các giá trị k1, k2 và k3 hơi khác. Đây là một xấp xỉ rất gần thông qua việc tính toán trước số học Eq(w|ξ)[log|w|] sử dụng một đa thức bậc ba.

Đây là dạng của divergence KL giữa hậu nghiệm xấp xỉ sau nhiệm vụ đầu tiên và tiên nghiệm. Sau đó, dễ thấy cách divergence KL này áp dụng cho các nhiệm vụ tiếp theo theo cách tương tự như (2), nhưng có tính đến dạng hậu nghiệm mới và tiên nghiệm ban đầu.

Biểu thức divergence KL được rút ra trong (11) cần được tối thiểu hóa. Bằng cách tối thiểu hóa (11) đối với p[i,j] và sau đó sử dụng các mẫu từ các phân phối tương ứng để gán giá trị cho ξ[i,j], các đóng góp thích ứng của mỗi neuron j tại mỗi tầng i của mạng được học cho mỗi nhiệm vụ. Các giá trị của p[i,j] được ràng buộc giữa 0 và 1 trong quá trình huấn luyện thông qua gradient descent có chiếu.

#### 3.2.1 HỌC CÁC GIÁ TRỊ THÍCH ỨNG TỐI ĐA

Sử dụng (6) để biểu thị giá trị của b[i,j], và bỏ qua số hạng không đổi trong đó vì nó không ảnh hưởng đến việc tối ưu hóa, divergence KL trong (11) tương đương với:

KL[q(w[i,j]|ξ[i,j])||p(w[i,j])] ≈ log s[i,j] + log(1 + e^(-a[i,j])) - 0.5 log p[i,j] - 0.5 log(1-p[i,j]) + c + Eq(w|ξ)[log|w|]. (12)

Các giá trị của a[i,j] được học bằng cách tối thiểu hóa (12) đối với a[i,j]. Phần này giải thích cách học biến thích ứng tối đa s[i,j]. Các giá trị của s[i,j] tối đa của hàm logistic được định nghĩa trong (6) được học từ nhiều nhiệm vụ. Đối với mỗi neuron j tại tầng i, có một giá trị tổng quát s[i,j] và một giá trị khác đặc thù cho mỗi nhiệm vụ t, được gọi là s[i,j,t]. Điều này tương tự như quy trình meta-learning được đề xuất trong (Finn et al., 2017). Quy trình sau để học s được thực hiện cho mỗi nhiệm vụ t sao cho: (i) việc tối ưu hóa được thực hiện để học một giá trị đặc thù nhiệm vụ s[i,j,t] được hưởng lợi từ việc khởi tạo ấm với giá trị tổng quát s[i,j] thay vì một điều kiện ban đầu ngẫu nhiên; và sau đó (ii) thông tin mới thu được từ nhiệm vụ hiện tại t được phản ánh trở lại để cập nhật giá trị tổng quát s[i,j].

Đầu tiên chia mẫu Nt thành hai nửa. Đối với nửa đầu, khởi hành từ giá trị tổng quát của s[i,j] như một điều kiện ban đầu, và sử dụng các ví dụ dữ liệu được gán từ nhiệm vụ t để học các giá trị đặc thù nhiệm vụ s[i,j,t] cho nhiệm vụ hiện tại t. Đối với neuron j tại tầng i, đề cập đến số hạng thứ hai trong (3), ∑n=1^Nt Eqt(θ)[log p(y^(n)|x^(n);θ)] như ft(x;y;s[i,j]). Tập hợp các tham số θ chứa s cũng như các tham số khác, nhưng chúng tôi tập trung vào s trong ký hiệu f vì quy trình sau được phát triển để tối ưu hóa s. Ngoài ra, đề cập đến mất mát của hàm (phân loại) f là Err(f) = CE(f(x;θ)||y), trong đó CE là cross-entropy:

s[i,j,t] = s[i,j] - α1/Ntr ∇s[i,j] ∑d=1^(Nt/2) Err(ft(xd;yd;s[i,j])). (13)

Bây giờ sử dụng nửa thứ hai của dữ liệu từ nhiệm vụ t để cập nhật giá trị tổng quát đã học s[i,j]:

s[i,j] = s[i,j] - α2/Ntr ∇s[i,j] ∑d=1+Nt/2^Nt Err(ft(xd;yd;s[i,j,t])). (14)

Trong đó α1 và α2 là các tham số kích thước bước.

Khi kiểm tra trên các mẫu từ nhiệm vụ t sau khi đã đối mặt với các nhiệm vụ tương lai t+1; t+2; ..., giá trị của s[i,j] được sử dụng là s[i,j,t] đã học. Chỉ có một giá trị cho mỗi neuron, vì vậy chi phí phụ từ việc lưu trữ các giá trị như vậy là không đáng kể. Các bước chính của thuật toán được liệt kê trong Thuật toán 1.

Tại nhiệm vụ t, độ phức tạp thuật toán của một cập nhật kết hợp duy nhất của các tham số dựa trên các số hạng cộng trong (12) là O(MELD²), trong đó L là số tầng trong mạng, D là số neuron (lớn nhất) trong một tầng duy nhất, E là số mẫu được lấy từ biến nhiễu ngẫu nhiên ε, và M là kích thước minibatch. Mỗi ε được thu được bằng cách lấy một mẫu từ p tương ứng, để ε không dẫn đến chi phí phụ về độ phức tạp.

### Thuật toán 1 Học Liên Tục với Trọng Số Thích Ứng (CLAW)

**Đầu vào:** Một chuỗi T bộ dữ liệu, {x^(n)_t, y^(n)_t}^Nt_n=1, trong đó t = 1;2;...;T và Nt là kích thước của bộ dữ liệu liên kết với nhiệm vụ thứ t.

**Đầu ra:** qt(θ), trong đó θ là các tham số mô hình.

Khởi tạo tất cả p(|w[i,j]|) với tiên nghiệm thang log, như trong (7).

**for** t = 1 ... T **do**
    Tiết lộ bộ dữ liệu {x^(n)_t, y^(n)_t}^Nt_n=1 cho nhiệm vụ hiện tại t.
    **for** i = 1 ... #tầng **do**
        **for** j = 1 ... #neuron tại tầng i **do**
            Tính toán p[i,j] sử dụng stochastic gradient descent trên (11).
            Tính toán s[i,j,t] sử dụng (13).
            Cập nhật giá trị tổng quát tương ứng s[i,j] sử dụng (14).
        **end for**
    **end for**
**end for**

## 4. THÍ NGHIỆM

Các thí nghiệm của chúng tôi chủ yếu nhằm đánh giá những điều sau: (i) hiệu suất tổng thể của CLAW được giới thiệu, được mô tả bởi độ chính xác phân loại trung bình trên tất cả các nhiệm vụ; (ii) mức độ mà quên thảm khốc có thể được giảm thiểu khi triển khai CLAW; và (iii) mức độ chuyển giao tích cực về phía trước đạt được. Các thí nghiệm chứng minh hiệu quả của CLAW trong việc đạt được kết quả học liên tục tiên tiến được đo bằng độ chính xác phân loại và bằng việc giảm quên thảm khốc đạt được. Chúng tôi cũng thực hiện các nghiên cứu loại bỏ trong Mục D trong Phụ lục thể hiện sự liên quan của mỗi tham số thích ứng được đề xuất.

Chúng tôi thực hiện sáu thí nghiệm trên năm bộ dữ liệu. Các bộ dữ liệu được sử dụng là: MNIST, notMNIST, Fashion-MNIST, Omniglot và CIFAR-100. Chúng tôi so sánh kết quả thu được bởi CLAW với sáu thuật toán học liên tục tiên tiến khác nhau: thuật toán VCL (dạng gốc và một với coreset), thuật toán elastic weight consolidation (EWC), thuật toán progress and compress (P&C), thuật toán reinforced continual learning (RCL), thuật toán được gọi là functional regularisation for continual learning (FRCL) sử dụng Gaussian processes và thuật toán learn-to-grow (LTG).

### 4.1 ĐỘ CHÍNH XÁC PHÂN LOẠI TỔNG THỂ

Chỉ số chính của chúng tôi là độ chính xác phân loại quan trọng nhất. Chúng tôi xem xét sáu thí nghiệm học liên tục, dựa trên các bộ dữ liệu MNIST, notMNIST, Fashion-MNIST, Omniglot và CIFAR-100. CLAW được giới thiệu được so sánh với hai phiên bản VCL: VCL không có coreset và VCL với coreset 200 điểm được lắp ráp bởi phương pháp K-center, EWC, P&C, RCL, FRCL (phiên bản TR của nó) và LTG. Tất cả các giá trị độ chính xác phân loại được báo cáo phản ánh độ chính xác phân loại trung bình trên tất cả các nhiệm vụ mà người học đã huấn luyện cho đến nay. Cụ thể hơn, giả sử rằng người học liên tục vừa hoàn thành việc huấn luyện trên một nhiệm vụ t, thì độ chính xác phân loại được báo cáo tại thời điểm t là giá trị độ chính xác trung bình thu được từ việc kiểm tra trên các tập có kích thước bằng nhau, mỗi tập thuộc về một trong các nhiệm vụ 1, 2, ..., t. Đối với tất cả các thí nghiệm phân loại, thống kê được báo cáo là trung bình của mười lần lặp lại. Ý nghĩa thống kê và sai số chuẩn của độ chính xác phân loại trung bình thu được sau khi hoàn thành hai nhiệm vụ cuối cùng của mỗi thí nghiệm được hiển thị trong Mục E trong Phụ lục.

Như có thể thấy trong Hình 1, CLAW đạt được độ chính xác phân loại tiên tiến trong tất cả sáu thí nghiệm. Kích thước minibatch là 128 cho Split MNIST và 256 cho tất cả các thí nghiệm khác. Mô tả chi tiết hơn về kết quả của mọi thí nghiệm được đưa ra tiếp theo:

**Permuted MNIST** Sử dụng MNIST, Permuted MNIST là một tiêu chuẩn học liên tục tiêu chuẩn. Đối với mỗi nhiệm vụ t, bộ dữ liệu tương ứng được hình thành bằng cách thực hiện một quá trình hoán vị ngẫu nhiên cố định trên các hình ảnh MNIST có nhãn. Việc hoán vị ngẫu nhiên này là duy nhất cho mỗi nhiệm vụ, tức là nó khác nhau cho mỗi nhiệm vụ. Đối với siêu tham số của EWC, kiểm soát đóng góp tổng thể từ dữ liệu trước đó, chúng tôi thử nghiệm với hai giá trị, λ = 1 và λ = 100. Chúng tôi báo cáo sau này vì nó luôn vượt trội hơn EWC với λ = 1 trong thí nghiệm này. EWC với λ = 100 cũng đã tạo ra kết quả phân loại EWC tốt nhất trước đây. Trong thí nghiệm này, các mạng đầy đủ kết nối một đầu với hai tầng ẩn được sử dụng. Có 100 đơn vị ẩn trong mỗi tầng, với kích hoạt ReLU. Adam là bộ tối ưu hóa được sử dụng trong 6 thí nghiệm với α = 0.001, β1 = 0.9 và β2 = 0.999. Chi tiết thí nghiệm thêm được đưa ra trong Mục C trong Phụ lục. Kết quả của độ chính xác phân loại tích lũy, trung bình trên các nhiệm vụ, trên một tập kiểm tra được hiển thị trong Hình 1a. Sau 10 nhiệm vụ, CLAW đạt được kết quả phân loại cao hơn đáng kể (kiểm tra Phụ lục) so với tất cả các đối thủ cạnh tranh.

**Split MNIST** Trong thí nghiệm dựa trên MNIST này, năm nhiệm vụ phân loại nhị phân được xử lý theo trình tự sau: 0/1, 2/3, 4/5, 6/7, và 8/9. Kiến trúc được sử dụng bao gồm các mạng đầy đủ kết nối đa đầu với hai tầng ẩn, mỗi tầng gồm 256 đơn vị ẩn với kích hoạt ReLU. Như có thể thấy trong Hình 1b, CLAW đạt được độ chính xác phân loại cao nhất.

**Split notMNIST** Nó chứa 400,000 hình ảnh huấn luyện, và các lớp là 10 ký tự, từ A đến J. Mỗi hình ảnh bao gồm một ký tự, và có các kiểu phông chữ khác nhau. Năm nhiệm vụ phân loại nhị phân là: A/F, B/G, C/H, D/I, và E/J. Các mạng được sử dụng ở đây chứa bốn tầng ẩn, mỗi tầng chứa 150 đơn vị ẩn với kích hoạt ReLU. CLAW đạt được cải thiện rõ ràng về độ chính xác phân loại so với các đối thủ cạnh tranh (Hình 1c).

**Split Fashion-MNIST** Fashion-MNIST là một bộ dữ liệu có kích thước giống như MNIST nhưng dựa trên 10 lớp khác nhau (và thách thức hơn). Năm nhiệm vụ phân loại nhị phân ở đây là: T-shirt/Trouser, Pullover/Dress, Coat/Sandals, Shirt/Sneaker, và Bag/Ankle boots. Kiến trúc được sử dụng giống như trong Split notMNIST. Trong hầu hết các nhiệm vụ học liên tục (bao gồm cả những nhiệm vụ quan trọng hơn, sau này) CLAW đạt được cải thiện phân loại rõ ràng (Hình 1d).

**Omniglot** Đây là một nhiệm vụ học tuần tự của các ký tự viết tay của 50 bảng chữ cái (tổng cộng hơn 1,600 ký tự với 20 ví dụ mỗi ký tự) thuộc bộ dữ liệu Omniglot. Chúng tôi tuân theo cùng cách thức mà nhiệm vụ này đã được sử dụng trong học liên tục trước đây; các ký tự viết tay từ mỗi bảng chữ cái tạo thành một nhiệm vụ riêng biệt. Do đó chúng tôi có 50 nhiệm vụ, điều này cũng cho phép đánh giá khả năng mở rộng của các khung so sánh. Mô hình được sử dụng là một CNN. Để xử lý các convolution trong CLAW, chúng tôi đã sử dụng ý tưởng được đề xuất và được gọi là local reparameterisation trick bởi Kingma et al., trong đó một tham số toàn cầu duy nhất được sử dụng cho mỗi kích hoạt neuron trong phân phối biến phân, thay vì sử dụng tham số cho mọi phần tử trọng số thành phần. Chi tiết thêm về CNN được sử dụng được đưa ra trong Mục C. CLAW thích ứng tự động đạt được độ chính xác phân loại tốt hơn (Hình 1e).

**CIFAR-100** Bộ dữ liệu này bao gồm 60,000 hình ảnh màu có kích thước 32×32. Nó chứa 100 lớp, với 600 hình ảnh cho mỗi lớp. Chúng tôi sử dụng phiên bản split CIFAR-100. Tương tự như Lopez-Paz & Ranzato (2017), chúng tôi thực hiện thí nghiệm 20 nhiệm vụ với một tập con rời rạc của năm lớp cho mỗi nhiệm vụ. CLAW đạt được độ chính xác phân loại cao hơn đáng kể (Hình 1f) - cũng cao hơn so với trạng thái nghệ thuật trước đó trên CIFAR-100 bởi Kemker & Kanan (2018). Chi tiết của CNN được sử dụng trong Mục C.

Một kết luận có thể rút ra từ Hình 1(a-f) là CLAW liên tục đạt được kết quả tiên tiến (trong tất cả 6 thí nghiệm). Cũng có thể thấy rằng CLAW mở rộng tốt. Ví dụ, sự khác biệt giữa CLAW và đối thủ cạnh tranh tốt nhất quan trọng hơn với Split notMNIST so với hai thí nghiệm đầu tiên, dựa trên MNIST nhỏ hơn và ít thách thức hơn. Ngoài ra, CLAW đạt được kết quả tốt với Omniglot và CIFAR-100.

### 4.2 QUÊN THẢM KHỐC

Để đánh giá quên thảm khốc, chúng tôi cho thấy độ chính xác trên nhiệm vụ ban đầu thay đổi như thế nào trong suốt quá trình huấn luyện trên các nhiệm vụ còn lại. Vì Omniglot (và CIFAR-100) chứa số lượng nhiệm vụ lớn hơn: 50 (20) nhiệm vụ, tức là 49 (19) nhiệm vụ còn lại sau nhiệm vụ ban đầu, cài đặt này phù hợp hơn cho Omniglot và CIFAR-100. Tuy nhiên chúng tôi vẫn hiển thị kết quả cho Split MNIST, Split notMNIST, Split Fashion-MNIST, Omniglot và CIFAR-100. Như có thể thấy trong Hình 2, CLAW (đôi khi cùng với) đạt được mức độ duy trì hiệu suất tiên tiến. Trong số các đối thủ cạnh tranh, P&C và LTG cũng đạt được mức độ duy trì hiệu suất cao.

Một kết luận thực nghiệm có thể rút ra từ thí nghiệm này và thí nghiệm trước đó, là CLAW đạt được kết quả học liên tục tổng thể tốt hơn, một phần nhờ cách nó giải quyết quên thảm khốc. Ý tưởng thích ứng kiến trúc bằng cách thích ứng đóng góp của các neuron của mỗi tầng cũng có vẻ hoạt động tốt với các bộ dữ liệu như Omniglot và CIFAR-100, đưa ra hướng cho công việc tương lai gần nơi CLAW có thể được mở rộng cho các lĩnh vực ứng dụng khác dựa trên CNN.

### 4.3 CHUYỂN GIAO TÍCH CỰC VỀ PHÍA TRƯỚC

Mục đích của thí nghiệm này là đánh giá tác động của việc học các nhiệm vụ trước đó đối với nhiệm vụ hiện tại. Nói cách khác, chúng tôi muốn đánh giá liệu một thuật toán có tránh được chuyển giao tiêu cực hay không, bằng cách đánh giá hiệu suất tương đối đạt được trên một nhiệm vụ duy nhất sau khi học một số lượng nhiệm vụ trước đó khác nhau. Từ Hình 3, chúng ta có thể thấy rằng CLAW đạt được kết quả tiên tiến trong 4 trên 5 thí nghiệm (ngang bằng trong thí nghiệm thứ năm) về việc tránh chuyển giao tiêu cực.

## 5. CÔNG TRÌNH LIÊN QUAN

Chúng tôi thảo luận ngắn gọn ba phương pháp liên quan đến học liên tục: (a) dựa trên regularisation, (b) dựa trên kiến trúc và (c) dựa trên bộ nhớ. Chúng tôi cung cấp chi tiết thêm về công trình liên quan trong Mục A trong Phụ lục. (a) Một phương pháp bổ sung cho CLAW là phương pháp dựa trên regularisation để cân bằng khả năng thích ứng với quên thảm khốc: một mức độ ổn định được giữ thông qua việc bảo vệ các tham số ảnh hưởng lớn đến dự đoán khỏi những thay đổi radical, trong khi cho phép phần còn lại của các tham số thay đổi không hạn chế. Thuật toán elastic weight consolidation (EWC) bởi Kirkpatrick et al. (2017) là một ví dụ tinh túy, nơi một hình phạt bậc hai được áp đặt lên sự khác biệt giữa các giá trị tham số của nhiệm vụ cũ và mới. Một hạn chế là mức độ cao của việc điều chỉnh thủ công được yêu cầu. (b) Phương pháp dựa trên kiến trúc nhằm xử lý các vấn đề ổn định và thích ứng bằng một sự phân chia cố định của kiến trúc thành các phần toàn cầu và cục bộ. (c) Phương pháp dựa trên bộ nhớ dựa vào bộ nhớ episodic để lưu trữ dữ liệu (hoặc pseudo-data) từ các nhiệm vụ trước đó. Các hạn chế bao gồm chi phí phụ cho các nhiệm vụ như lưu trữ dữ liệu, replay, và tối ưu hóa để chọn (hoặc tạo ra) các điểm. CLAW cũng có thể được xem như một sự kết hợp của phương pháp dựa trên regularisation (cơ chế suy luận biến phân) và phương pháp mô hình hóa tự động hóa quá trình xây dựng kiến trúc theo cách hướng dữ liệu, tránh chi phí phụ từ việc lưu trữ hoặc tạo ra các điểm dữ liệu từ các nhiệm vụ trước đó. CLAW cũng trực giao với (và đơn giản để kết hợp với, nếu cần) các phương pháp dựa trên bộ nhớ.

## 6. KẾT LUẬN

Chúng tôi đã giới thiệu một khung học liên tục học cách thích ứng kiến trúc của nó từ các nhiệm vụ và dữ liệu có sẵn, dựa trên suy luận biến phân. Thay vì chia cứng nhắc kiến trúc thành các phần chia sẻ và đặc thù nhiệm vụ, phương pháp của chúng tôi thích ứng đóng góp của mỗi neuron. Chúng tôi đạt được điều đó mà không cần mở rộng kiến trúc với các tầng mới hoặc neuron mới. Kết quả của sáu thí nghiệm khác nhau trên năm bộ dữ liệu chứng minh hiệu suất thực nghiệm mạnh mẽ của khung được giới thiệu, về độ chính xác học liên tục tổng thể trung bình và chuyển giao về phía trước, và cũng về việc giảm thiểu hiệu quả quên thảm khốc.
