# 2304.05362.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2304.05362.pdf
# File size: 1485759 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MASIL: Towards Maximum Separable Class Representation for Few Shot Class
Incremental Learning
Anant Khandelwal
Applied Scientist, Amazon
Abstract
Few Shot Class Incremental Learning (FSCIL) with few
examples per class for each incremental session is the re-
alistic setting of continual learning since obtaining large
number of annotated samples is not feasible and cost ef-
fective. We present the framework MASIL as a step to-
wards learning the maximal separable classiﬁer. It ad-
dresses the common problem i.e forgetting of old classes
and over-ﬁtting to novel classes by learning the classiﬁer
weights to be maximally separable between classes form-
ing a simplex Equiangular Tight Frame. We propose the
idea of concept factorization explaining the collapsed fea-
tures for base session classes in terms of concept basis and
use these to induce classiﬁer simplex for few shot classes.
We further adds ﬁne tuning to reduce any error occurred
during factorization and train the classiﬁer jointly on base
and novel classes without retaining any base class samples
in memory. Experimental results on miniImageNet, CIFAR-
100 and CUB-200 demonstrate that MASIL outperforms all
the benchmarks.
1. Introduction
The success of Convolutional Neural Networks (CNN)
in wide range of computer vision tasks [27,40,43,48,51,60]
relies on the fact that the training requires large scale im-
age datasets [14] and the train and test distributions are al-
most identical [41]. However, when deploying them in real
world environments it requires that these models to quickly
adapt to changing streams of data and hence can recognize
the novel classes emerged over a period of time. But the
underlying bottleneck for this adaptation is that CNN re-
quires large amount of data to be collected for each of the
novel classes, this takes lot of human effort to annotate them
which is infeasible. However, annotating only a few sam-
ples seems reasonable, we term this ability to adapt to novel
classes (with only few examples) without forgetting the old
classes as the few shot class incremental learning (FSCIL).
Fine-tuning the pre-trained network with limited number of
training examples of only novel classes cause the model toforget old classes ( catastrophic forgetting ) and overﬁtting
on recent novel classes [17, 22, 31, 68]. Large amount of
studies has been conducted to solve the problem of catas-
trophic forgetting [25]. This includes approaches based on:
constraining the weight changes [2,16,37,44,78], retaining
the samples from previous data in a memory [2,6,13,54,63]
data augmentation [72,77,84,86], dynamic expansion based
architectures (DEA) which expands the network for each
new incoming task id while the weights of the base net-
work are frozen for learning keeping both old and new in-
formation [18, 20, 24, 33, 45, 73]. All these approaches are
broadly categorised into two main themes i.e. multi-task
and multi-class. Multi-task approaches like DEA requires
resolving the task id during inference, which is typically
unavailable. Multi-class scenario refers to learning a sin-
gle classiﬁer with a aim to recognize the base and novel
classes in a single task. In this paper we study the FSCIL
problem under multi-class scenario, since it is more realis-
tic and practical. Recent approaches [1, 28, 80] have pro-
posed to learn the backbone network as feature extractor
using data of base classes, and then use this frozen fea-
ture extractor to learn the classiﬁer prototypes for novel
classes incrementally. But this does not guarantee the maxi-
mum separability between the classiﬁer prototypes for base
and novel classes and hence can lead to the confusion be-
tween the old and new classes resulting in limited perfor-
mance. Other approaches [1, 9, 28], which uses custom
loss functions and regularizers to learn the classiﬁer pro-
totypes for novel classes along with preventing forgetting
on base classes, are also limited by performance because
of misalignment between ﬁxed features of base classes and
classiﬁer. Recent work NC-FSCIL [75] proposed the use
of neural collapse to learn the maximally separable classi-
ﬁer. They proposed to learn the two layer classiﬁer with
pre-ﬁxed simplex weights for base and each incremental
sessions. The two layer classiﬁer training with pre-ﬁxed
weights for few shot novel classes will not be able to gen-
eralize well and hence results in overﬁtted class representa-
tion. Opposed to them we proposed to learn itself the max-
imum separable class representation using Neural Collapse
properties, but we used concept factorization on backbonearXiv:2304.05362v1  [cs.CV]  8 Apr 2023

--- PAGE 2 ---
network to be able to represent any class in general and
hence obtain the generalized classiﬁer for novel few shot
classes.
In this work, we address this problem of misalignment
between ﬁxed features of backbone network (feature extrac-
tor) and classiﬁer prototypes to prevent forgetting of base
class. Towards that we attempt to learn the maximal sepa-
rable classiﬁer to avoid confusion between base and novel
classes in each incremental session. Our work is inspired
from two main studies: 1) Neural Collapse (NC) for imbal-
anced data [12, 53] and, 2) Concept Factorization [19, 35].
Neural collapse is the phenomenon where the network when
trained beyond zero error towards zero loss, results in col-
lapsing the last layer features of backbone network to form
an Equiangular Tight Frame (ETF). The vertices of this
frame denotes the feature vector representing the class and
aligned with classiﬁer prototype of the corresponding class
[53]. This guarantees a maximal separable classiﬁer since
ETF is a geometric structure forming a simplex where the
within class variance is minimized (because of collapse to
a single vector) and between class variance is maximized
lying at equal angles from each other. However, with pre-
ﬁxed simplex for base classes, the feature extractor is easy
to train since the sufﬁcient data is available for each of the
class in base session ( t= 0) resulting in collapse but for
any incremental session t1with few labelled samples
learning the collapsed features for novel classes is challeng-
ing since with few samples (as much as 5 samples for a
class) the ﬁxed feature extractor is not able to align well
with the novel class prototype. To resolve that we introduce
the mechanism of concept factorization, where we dissect
the collapsed feature extractor on base session to identify
the concept basis in the input images. Once the concept
basis (” concept bank ”) is identiﬁed from base session, we
recognize them as the building block from which the in-
cremental session classiﬁer simplex is induced and hence
the new set of coefﬁcients can be learnt for inducing the
simplex with novel classes. This is additionally ﬁne-tuned
along with base class simplex to further align this with few
shot instances to reduce any irreducible error occurred dur-
ing calculating optimal coefﬁcients for the ” concept bank ”.
This has been illustrated in Fig.1. To summarize, our main
contributions are as follows:
• We introduce a novel framework MASIL as an attempt
to learn the maximal separable classiﬁer for FSCIL.
• We identiﬁed the mechanism where the base session
collapsed features (obtained as per Neural Collapse
properties) can further be dissected in terms of ” con-
cept bank ”, which forms the basis for building classi-
ﬁer prototype of novel classes encountered during in-
cremental session.
• Evaluation on three popular FSCIL benchmarksdatasets demonstrating state-of-the-art performance.
Extensive ablation study has been done to analyze the
importance of loss function introduced using Neural
collapse properties and the advantage of simplex ﬁne-
tuning to reduce the irreducible error.
2. Related Work
2.1. Few Shot Learning
The idea of few shot learning (FSL) is to adapt the model
on novel classes (with only few labelled instances) with-
out caring for the performance on base classes. Most of
the works uses meta-learning [21, 64–66] or metric learn-
ing [64, 66, 69]. Recently, the approaches [23, 59] have
demonstrated the use of meta learning to recognize the base
and novel classes both, by sampling ”fake” few shot classi-
ﬁcation task from base classes to learn a classiﬁer for novel
classes. Finally, the learned classiﬁer weights are combined
to jointly recognize the base and novel classes. Some of the
works [59] regard this as sort of incremental learning. Con-
trastively, FSCIL setting is much more realistic where the
base dataset is not accessible during the incremental stage
and we have to adapt the model for novel classes without
catastrophic forgetting [17,68]. Metric learning approaches
focus on learning a strong backbone network for learning
transferable features across the tasks, on top which the simi-
larity function (like k-nearest neighbours in [69], non linear
distance metric in [66] ) is learnt to demonstrate the abil-
ity to classify the novel classes with transferable features.
However, this requires to train the as much similarity func-
tion as the number of incremental sessions in FSCIL but the
aim of FSCIL is to train one uniﬁed classiﬁer for the base
and novel classes. We will discuss in the next section how
existing works have dealt the problem of FSCIL different
from FSL.
2.2. Few Shot Class Incremental Learning
Class Incremental Learning (CIL) : We start by ﬁrst
discussing the idea of Class Incremental Learning (CIL),
it aims to learn a classiﬁer that manages to continuously
update itself to recognize all the novel classes without for-
getting the base classes [7, 44, 58]. To overcome this for-
getting mechanism CIL studies has been categorized into
three broad categories: regularization based [16, 37, 44],
rehearsal based [2, 6, 13, 54], and knowledge distillation
[30, 58, 71]. Regularization based methods constraint the
weight changes on the novel classes thereby keeping the
information previously learnt for base classes, this causes
these methods to suffer for generalization on novel classes
because of small allowed change in weights. Rehearsal
based methods in which the model is continually be re-
freshed using old data reserve so that it maintains to learn
the novel classes along with old classes. This is limited

--- PAGE 3 ---
Figure 1. MASIL : Illustrating the concept basis obtained from concept factorization of features after feature extractor and their relation
with input images. After base session training, classiﬁer simplex for novel classes are induced from these concept basis. Implicit memory
(not shown) storing the feature mean of classes seen till current session. This is used to jointly updating the weights of classiﬁer for both
base and novel classes during Simplex Finetuning.
by the amount of old data it can retain in memory, and the
how the instances from old data to be selected for maximal
information with minimum memory requirements. These
methods are not scalable to large number of classes because
of the limited memory. For example, iCaRL [58] learns
the nearest neighbour classiﬁer for novel classes while
maintaining the memory of exemplars from base session.
Knowledge Distillation based methods requires the use of
large teacher model to guide the learning of small student
model [29,74]. It works by distilling the previously learned
information to new model with novel classes, so as to avoid
forgetting of base classes. Recent studies [16, 18, 30] con-
ducted the distillation on feature level rather than on output
logit level at the classiﬁer. However, these solutions suffer
from a problem of distinguishing between base and novel
classes leading to limitation in the performance.
Few Shot Class Incremental Learning (FSCIL) : Com-
pared to CIL setting, FSCIL aims to learns the novel classes
(along with base classes) with few labelled instances [17,
68], which is much more realistic and hard, since learning
from few instances of novel classes causes over-ﬁtting on
novel classes [64,66]. In order to do this, some studies have
focused to align base and incremental session using aug-
mentation [56], searching for ﬂat minima [62]. However,
for avoiding over-ﬁtting to novel classes it is required that
the classiﬁer prototypes for novel classes should be maxi-
mally separable from base classes. Adjusting prototypes for
base classes is not feasible since that requires the use of base
session data. However, these studies [80, 85] have focused
on evolving prototypes for novel classes. Large number of
existing works have focused on building the custom lossand regularizer [1, 28, 30, 34, 50, 59, 67, 74]. However, the
same disadvantages we discussed in CIL for regularization
and custom loss functions applies in FSCIL as well. In this
work we focused on the optimal evolution of prototypes for
novel classes which is derived from the same function us-
ing which base class prototypes have been developed and
ensuring the maximal separability between old and novel
classes.
3. Problem Statement and Context
In this section we will introduce the problem deﬁnition
of Few Shot Class Incremental Learning in Section 3.1 and
context in subsequent sections.
3.1. Few Shot Class Incremental Learning
Formally, we deﬁne Few Shot Class Incremental Learn-
ing (FSCIL) as the stream of labelled data in time sequence
asD0,D1, ....., whereDt=f(xt
j;yt
j)gj=jDtj
j=1 .Ctbe the
number of classes in training set Dt, where8(i;j)Ci\Cj=
;. Speciﬁcally, we consider D0as the base session with
large label spaceC0with each class c2C 0have sufﬁcient
training images. For t >0each of the incremental session
Dthave only few labelled images for each novel classes.
FSCIL is deﬁned as the time step incremental training of
model onDt8t>0with no access to any of the previ-
ous labelled set from D0toDt 1. Fort>0Dtwe denote
the setting as C classes with K training examples per class
as C-way K-Shot FSCIL where Ct\C0
t=;8t6=t0. After
each incremental session training with Dt, model is eval-
uated to recognize all the training classes encountered so
far i.e.[i=t
i=0Ci. Hence, FSCIL not only aims to recognize

--- PAGE 4 ---
novel classes but to avoid forgetting of the old classes and
the setting of learning of novel classes is highly imbalanced
and suffers from data scarcity problem as well. This makes
FSCIL setting more suited for real world applications.
Initialization : AssumingC0as the number of base
classes and we have total Tincremental session and each
session haskclasses, so there will be total K=C0+Tk
classes. To be able to perform FSCIL, we denote the model
trained on base session consists of backbone feature extrac-
torf(;f)and classiﬁer parameters W2RKd, where W
is a MLP classiﬁer consisting of L layers denoted as W=
W1W2:::::WL. For inputX2Rnwe denote the features
obtained from feature extractor as H=f(X;f)2RdN,
whereNis the total number of training instances. Simi-
lar to [12] we also consider last layer features Has freely
optimization variables. The optimization objective is then
deﬁned as follows:
min
W;HL(W;H) =1
2NjjWH Yjj2
F+W
2jjWjj2
F+
H
2jjHjj2
F(1)
where Y2RKN, is the class label for each of the train-
ing instance in Ninstances and W,Hare regularization
hyperparameters.
3.2. Neural Collapse
In recent works [12, 53] which have studied the practice
of training the DNN beyond zero error towards zero loss.
This reveals the geometric structure in the form of simplex
equiangular tight frame, formed by the last layer features
along with classiﬁer weights. This has been demonstrated
on the balanced data and models with various popular ar-
chitectures. Neural Collapse as deﬁned in [53] consists of
the following four properties:
• (NC1)Variability Collapse : Last layer features of
the backbone network for a particular class collapse
to within-class mean.
• (NC2)Convergence : results in optimal class-means
which are equally and maximally pairwise separated
forming a simplex Equiangular Tight Frame (ETF).
• (NC3)Classiﬁer Convergence : Optimal class means
forming ETF are aligned to the corresponding classi-
ﬁer weights uptio rescaling.
• (NC4)Simpliﬁcation to nearest class center When
(NC1)-(NC3) holds, the model prediction using logits
respects nearest class centers.
In addition to the balanced data, this [12] have derived the
geometrical analysis for the imbalanced data given as:Deﬁnition 1. Let (W*,H*) be the global optimizer of
equation 1, r=min(K;d)andW=UWSWVT
Wbe the
SVD factorization of W. Then the following holds for the
imbalanced data:
•(NC1) leads to collapse of features within the same
classH=HY, where H= [h
1;h
2;::::h
K]2
RdK
•(NC3) leads to alignment between classi-
ﬁer weights and corresponding class mean as
w
k=q
nkH
Wh
k8k2[K], wherenkis the number
of instances of class k.
•(NC2) leads to optimal class means equally and max-
imally separated forming simplex Equiangular Tight
Frame (ETF) WW>=diagfs2
kgk=K
k=1, wheresk
are the singular values of W
Another approach i.e. Deep Simplex Classiﬁer [8], pro-
posed the optimization problem as minimization of features
obtained from feature extractor to the vertices of simplex
as:
min
hi2HT1
nnX
i=1jjhi syijj2(2)
wheresyiis vertex of simplex and is treated as the class
center for class yi.
4. MASIL
Overall framework of our proposed method is illustrated
in Fig. 1. FSCIL aims to learn the classiﬁer weights WKd
which works for all classes irrespective of whether they be-
longs to the base classes during t= 0 or few shot classes
duringt > 0. Traditionally, this has been achieved by
ﬁrst learning the classiﬁer weights for C0base classes and
then learn the weights for novel classes W(t)2Rkdwith
the regularized constraint in the loss function that the old
weightsW2RC0+(t 1)kbe preserved with little or no
updates. However, this leads to misalignment between the
classiﬁer prototypes of old and novel classes causing old
new confusion(ONC) [31] and catastrophic forgetting [25].
This causes drop in performance of FSCIL classiﬁer as the
number of incremental session grows resulting in poor gen-
eralizability even in recognizing the base classes. To miti-
gate this, in this work we adopted the properties obtained
from Neural Collapse to learn maximally separable clas-
siﬁer along with concept factorization to learn classiﬁer
weights (organized as simplex) for novel classes with few
samples. We restricted the feature extractor from updates
during incremental session training and rely on concept
factorization of the activations obtained for base classes
to obtain the basis of concepts called ” concept bank ”, us-
ing which we can represent maximally separable classiﬁer

--- PAGE 5 ---
weights i.e. simplex for few shot classes. To represent the
classiﬁer simplex using ” concept bank ” it requires to solve
only for the coefﬁcient matrix which can be done by just
solving the Non Negative Least Squares (NNLS).
4.1. Concept Factorization
The idea of concept factorization relates to the phenom-
ena of neural collapse, where it learns to maximally sep-
arate the classes by forming the simplex at class level on
both levels of class features and classiﬁer weights. In or-
der to achieve this it merge the activations (during forward
pass) of the same class until they all converge to the one
hot class vector at the logits layer as depicted in equation 2.
This allows the class wise feature vectors which are concen-
trated at higher layers to be recursively broken into multiple
concepts moving from highest layer to lower layers trac-
ing back to the input images where it can be explained with
regions as concepts, combination of which makes it possi-
ble to be able to classify it to particular class. We adopted
NMF (Non Negative Matrix Factorization) as in [19] of ac-
tivations obtained at the output of feature extractor given
as:
min
P0;Q01
2jjA PQTjj2
F (3)
where,jj:jjFis the Frobenius norm, the activations A2
Rndobtained from crop of images Xi=(xi)Xi2
Xnpwithis a crop function. We take random crops
(governed by ) of images, this results in unique concepts
across the categories to be able to build the bank of unique
concept vectors called ” concept bank ”. Activations at the
last layer of feature extractor after global pooling for these
random crops is given as A=f(X;f)2Rnd. NMF
is simply the factorization of concept activations Ainto the
”concept bank ”Q2Rvd(where it follows low rank fac-
torizationvmin(n;p)) and coefﬁcients P2Rnv
denote the importance of each of the concepts in explain-
ing the activations A. Once the ” concept bank ” is pre-
computed, we can obtain the coefﬁcients P(x)for any
inputxusing NNLS (Non-Negative Least Squares) i.e.
min
P01
2jjf(x;f) P(x)QTjj2
F. Relating activation fac-
torization in equation 3 and neural collapse in equation 2,
implies that the activations when collapses to the mean fea-
tures vector for each class forming the class simplex vector
which is composed of concept basis vectors and the corre-
sponding coefﬁcients, combining for all classes which gives
the overall basis called ” concept bank ”.
4.2. NMF Layer
During NMF factorization of equation 3, we keep the
feature extractor f(:;f)frozen. We approached the NMF
problem solution using ADMM (Alternating Direction
Method of Multipliers) [5] since NMF is non-convex, but
however it can be made convex by ﬁxing the value of eitherof the two factors (P;Q)which requires alternating update
of either of two factors ﬁxing one at a time, which is equiva-
lent to solving a Non-Negative Least Squares (NNLS) prob-
lem making it convex. This alternating update mechanism
called as ADMM , formulated as:
Pt+1=arg min
P01
2jjA PQtTjj2
F (4)
Qt+1=arg min
Q01
2jjA PtQTjj2
F (5)
It ensures global or local minimum since each of the
NNLS problem obeys Karush–Kuhn–Tucker (KKT) opti-
mality conditions [36, 42]. Using these conditions form-
ing the implicit function [26] makes the implicit differenti-
ation [3, 26, 38] allows to compute the gradients (@P
@A;@Q
@A),
but however we have to relate the concepts with the input
image regions we require to compute (@P
@X;@Q
@X). This can
be calculated as:
@P
@X=@A
@X@P
@A;@Q
@X=@A
@X@Q
@A(6)
Computation of@A
@Xis fairly straight word using Pytorch .
More details on implementation of combining gradients
from implicit differentiation in Jax[4,32] and gradient from
Pytorch computation is detailed in Section A. Once we pre-
compute the ”concept bank” for base classes using equation
5 and 6, we ﬁxed the Qand allows only to compute optimal
coefﬁcientsP(x)for any input xusing NNLS
min
P01
2jjf(x;f) P(x)QTjj2
F (7)
which give the optimal representation of activation for any
inputxin terms concept basis vectors.
4.3. Classiﬁer Simplex Representation
Equation 2 is optimizing the feature representation for
each class resulting in collapsed representation for class yi
assyi. Similarly, equation 1 results in simplex represen-
tation for each class i.e. wyi2W. So if we consider the
normalized simplex representation on a unit hypersphere [8]
of each class then:
wT
yisyi= 18yi2[j=t
j=0Cj (8)
which results in the modiﬁed loss function of equation 2 to:
min
hi1
jDjjX
(xi;yi)2DjjjwT
yihi 1jj2
F (9)
s.t.wT
yisyi= 1 which is same as in equation 1 and hence
follow the neural collapse properties. Moreover, optimizing
equation 2, results in the collapsed feature representation

--- PAGE 6 ---
for all instances belonging to that class. Additionally, equa-
tion 3 computes the best approximation of collapsed feature
representation HPQT. For any input (xi;yi)belongs
toDj; j > 0, then optimal hiobtained from equation 7 is
given as:
hi=P(xi)QT(10)
From (NC1), the collapsed feature representation of each
class converge to a unique vector e.g. for class yithe fea-
ture representation of all instances is denoted as Hyi2H,
(NC1) implies covarianceP
Hyi  !0. i.e. the features
collapse to their corresponding class means i.e. h
yi=Pnyi
i=1hi, wherenyiis the number of instances for class
yi, and as per loss in equation 2, this is minimum when
syi=h
yi, then from equation 8 and 10:
^ wyi=1
jDjj0
@X
(xi;yi)2DjP(xi)1
AQT8j >0 (11)
where the coefﬁcients P(xi)for each instance of class yi
are calculated using NNLS as per equation 7, additionally,
yi2 Cj;j > 0are the few shot classes and classiﬁer
weights are the optimal simplex representation for few shot
classes. For base session classes ( j= 0) the classiﬁer sim-
plex representation is simply ^ wyi=sT
yi. Since we imple-
mented the classiﬁer using MLP with L= 2layers, for each
layer the simplex representation is ^ wl;yi= (^ wyi)1=L.
4.4. Simplex Finetuning
In Section 4.3 we described the optimal simplex rep-
resentation for each class belongs to the few shot class
yi2Cj;j > 0. But, however due to the the inherent irre-
ducible error to NNLS, we approach the optimal represen-
tation of simplex for few shot class by further ﬁne-tuning
the classiﬁer weights (keeping the feature extractor frozen)
initialized using simplex representation as obtained in equa-
tion 11. To avoid deviating the weights to much from opti-
mal simplex representation we add a constraint to the loss
in equation 9 as:
min
wyiL(wyi) =1
jDjjX
(xi;yi)2DjjjwT
yihi 1jj2
F+
jjwyi ^ wyijj2
F;2[0;1](12)
whereyi2Cj;j > 0and feature extractor is frozen and
hence optimizing for the best wyi. Since base session train-
ing (i.e.j= 0 and datasetD0) is governed using the loss
function of equation 2 and hence results in collapsed rep-
resentation of features at the terminal layer for each class
yi2C 0. For the simplex representation for each class in C0
to remain maximally separable with the ones obtained for
few shot class we utilized the collapsed representation offeatures for each class to further ﬁne tune the simplex repre-
sentation, but without keeping the image instances in mem-
ory we memorized the collapsed representation (which is
the mean representation of instance features for each class)
inMgiven as:
Myi=1
nyinyiX
i=1hi;8Myi2M (13)
where,nyiis the number of instances of class yi. The up-
dated loss function during ﬁne tuning stage include the base
session classes and few shot class is given as:
min
wyiL(wyi) =1
jDjjX
(xi;yi)2DjjjwT
yihi 1jj2
F+
1
jMjX
(Myi;yi)2MjjwT
yiMyi 1jj2
F+
jjwyi ^ wyijj2
F;2[0;1](14)
where the constraint is now valid for base session classes
as well along with few shot classes with the fact that sim-
plex representation for each class should not deviate much
(depends on the contributing factor ) from the optimal sim-
plex representation. In each incremental session we train
our classiﬁer network using this loss function after deriving
the simplex representation for each few shot classes from
equation 11.
5. Experiments
We prove the effectiveness of MASIL on three well
known FSCIL benchmark datasets (as in ALICE [56]) de-
scribed in Section 5.1 along with FSCIL setting and com-
pared its performance with the state-of-the-art methods
(Section 5.2). Training details and hyper parameters are dis-
cussed in Appendix A.
5.1. Dataset Details
•CIFAR-100 [39] consists of 100 classes in total with
color images of size 3232. Each class consists of 500
images for training and 100 images for testing. The
base session ( t= 0) consists of 60 classes and the rest
40 classes contributed for 8 incremental session with
5-way 5-shot setting (i.e. 5 images for each of the 5
classes) for 1t8.
•miniImageNet [61] is a variant of ImageNet [15] with
color images of size 8484. It also consits of same
number of classes as CIFAR-100 and same number of
images in train and test, resulting in the same conﬁgu-
ration for base and incremental sessions.
•CUB-200 [70] consists of 11,788 images (size 224
224) in total spanning across 200 classes. There are

--- PAGE 7 ---
MethodsSession Accuracy (%) ( ") Average
Acc. (")Relative
Improvement0 1 2 3 4 5 6 7 8
iCaRL [58] 61.31 46.32 42.94 37.63 30.49 24.00 20.89 18.80 17.21 33.29 +41.65
NCM [30] 61.31 47.80 39.30 31.90 25.70 21.40 18.70 17.20 14.17 30.83 +44.69
D-Cosine [69] 70.37 65.45 61.41 58.00 54.81 51.89 49.10 47.27 45.63 55.99 +13.23
TOPIC [68] 61.31 50.09 45.17 41.16 37.48 35.52 32.19 29.46 24.42 39.64 +34.44
IDLVQ [9] 64.77 59.87 55.93 52.62 49.88 47.55 44.83 43.14 41.84 51.16 +17.02
Self-promoted [85] 61.45 63.80 59.53 55.53 52.50 52.50 46.69 43.79 41.92 52.76 +16.94
CEC [80] 72.00 66.83 62.97 59.43 56.70 53.73 51.19 49.24 47.63 57.75 +11.23
LIMIT [83] 72.32 68.47 64.30 60.78 57.95 55.07 52.70 50.72 49.19 59.06 +9.67
Regularizer [1] 80.37 74.68 69.39 65.51 62.38 59.03 56.36 53.95 51.73 63.71 +7.13
MetaFSCIL [11] 72.04 67.94 63.77 60.29 57.58 55.16 52.90 50.79 49.19 58.85 +9.67
C-FSCIL [28] 76.40 71.14 66.46 63.29 60.42 57.46 54.78 53.11 51.41 61.61 +7.45
Data-free Replay [47] 71.84 67.12 63.21 59.77 57.01 53.95 51.55 49.52 48.21 58.02 +10.65
ALICE [56] 80.60 70.60 67.40 64.50 62.50 60.00 57.80 56.80 55.70 63.99 +3.16
SSFE-Net [52] 72.06 66.17 62.25 59.74 56.36 53.85 51.96 49.55 47.73 57.74 +11.13
NC-FSCIL [75] 84.02 76.80 72.00 67.83 66.35 64.04 61.46 59.54 58.31 67.82 +0.55
MASIL(Ours) 85.15 77.00 72.20 67.92 66.60 64.2 61.50 59.60 58.86 68.11
Table 1. Performance comparison on miniImageNet with ResNet-18 as backbone architecture under 5-way 5-shot FSCIL setting. Table
denotes the accuracy in each session, average accuracy across sessions and ”Relative Improvement” denotes the improvement of our
method in the last session. Methods above separating line are CIL methods for FSCIL as in [68] and [80]
MethodsSession Accuracy (%) ( ") Average
Acc. (")Relative
Improvement0 1 2 3 4 5 6 7 8
iCaRL [58] 64.10 53.28 41.69 34.13 27.93 25.06 20.41 15.48 13.73 32.87 +42.42
NCM [30] 64.10 53.05 43.96 36.97 31.61 26.73 21.23 16.78 13.54 34.22 +42.61
D-Cosine [69] 74.55 67.43 63.63 59.55 56.11 53.80 51.68 49.67 47.68 58.23 +8.47
TOPIC [68] 64.10 55.88 47.07 45.16 40.11 36.38 33.96 31.55 29.37 42.62 26.78
Self-promoted [9] 64.10 65.86 61.36 57.45 53.69 50.75 48.58 45.66 43.25 54.52 +12.9
CEC [80] 73.07 68.88 65.26 61.19 58.09 55.57 53.22 51.34 49.14 59.53 +7.01
DSN [11] 73.00 68.83 64.82 62.64 59.36 56.96 54.04 51.57 50.00 60.14 +6.15
LIMIT [83] 73.81 72.09 67.87 63.89 60.70 57.77 55.67 53.52 51.23 61.84 +4.92
MetaFSCIL [1] 74.50 70.10 66.84 62.77 59.48 56.52 54.36 52.56 49.97 60.79 +6.18
C-FSCIL [28] 77.47 72.40 67.47 63.25 59.84 56.95 54.42 52.47 50.47 61.64 +5.68
Data-free Replay [47] 74.40 70.20 66.54 62.51 59.71 56.58 54.52 52.39 50.14 60.78 +6.01
ALICE [56] 79.00 70.50 67.10 63.40 61.20 59.20 58.10 56.30 54.10 63.21 +2.05
NC-FSCIL [75] 82.52 76.82 73.34 69.68 66.19 62.85 60.96 59.02 56.11 67.50 +1.12
MASIL(Ours) 82.55 76.98 73.44 69.75 66.48 62.98 61.4 59.81 57.23 67.84
Table 2. Performance comparison on CIFAR-100 with ResNet-18 as backbone architecture under 5-way 5-shot FSCIL setting. Table
denotes the accuracy in each session, average accuracy across sessions and ”Relative Improvement” denotes the improvement of our
method in the last session. Methods above separating line are CIL methods for FSCIL as in [68] and [80]
5,994 images in train and 5,794 images in test. Base
session (t= 0) consists of 100 classes and rest 100
classes contributed towards 10 incremental session
(1t10) with 10-way 5-shot setting (5 images
for 10 classes each).
5.2. Benchmark Evaluation
Performance comparison on miniImageNet, CIFAR-100
and CUB-200 is demonstrated in Table 1, 2 and 5 (given
in Appendix B due to space limitation) respectively. Our
method MASIL outperforms in all the methods in the last
session with relative improvement of +3.16%, +2.05% and
+0.14% on miniImageNet, CIFAR-100, CUB-200 respec-
tively as compared to strongest baseline ALICE [56]. Addi-
tionally, our method outperforms all the methods in all the
sessions (except on CUB-200 session 2). Moreover, on av-erage accuracy our method outperforms atleast by +1.79%
as compared to strongest baseline, collectively is an indica-
tor that our model helps in mitigating the forgetting issue in
a realistic setting of continual learning namely FSCIL .
5.3. Ablation Studies
We consider variations to base model (backbone network
i.e. ResNet-18 with classiﬁer and memory as introduced in
equation 13) to validate the 1) effects of loss introduced in
equation 9 (ETF) as compared to cross-entropy (CE) loss
with and without neural collapse induced simplex classiﬁer
prototypes, 2) effects of few shot simplex induced with con-
cept factorization and 3) effects of simplex ﬁne tuning. To
validate the ﬁrst effect there are two models. The ﬁrst model
(Learnable + CE) uses a classiﬁer with learning weights
from CE loss, which is the most common practice. Second
model (NC+ CE) uses the CE loss with classiﬁer weights

--- PAGE 8 ---
ModelsminiImageNet CIFAR-100 CUB-200
Final (") Average (")Final (") Average (")Final (") Average (")
Learnable + CE 50.04 61.30 52.13 62.68 50.38 59.58
NC+ CE 56.66 68.23 54.42 64.00 56.83 65.51
NC+ ETF Loss 58.31 67.82 56.11 67.50 59.44 67.28
NC+ ETF Loss + CF 58.72 68.04 56.13 67.51 59.72 67.45
MASIL (Ours) 58.86 68.11 57.23 67.84 60.24 67.54
Table 3. Ablation Studies on three datasets investigating the effects of Simplex based loss, Concept Factorization and Simplex Fine Tuning
Figure 2. Average cosine similarities between different classes at each session for Train (Left) and Test (Right) on miniImageNet. Calcu-
lation of cosine similarity is done for all the classes encountered so far after the model gets trained in current session.
as per the neural collapse properties but uses the CE loss
instead of the loss in equation 9. To validate the second ef-
fect (third model i.e. NC+ ETF Loss) we did not initialize
the classiﬁer weights for few shot classes and train them as
per the loss in equation 9 with memory of base classes as in
equation 13. For third effect i.e. fourth model ( NC+ ETF
Loss + CF) we reported the performance without ﬁne tuning
and just use the classiﬁer weights calculated from concept
factorization (CF) as in equation 11. Finally, we reported
performance of MASIL to compare among all of them. As
shown in Table 3, adopting the loss function in equation 9
is deﬁnitely helps in mitigating performance drop as com-
pared to CE loss even with classiﬁer weights is assumed
to be forming simplex, and it further mitigates using the
weight initialized with CF and further with ﬁne tuning. It
indicates the success of CF along with neural collapse to-
wards optimal solution for FSCIL.
5.4. Analysing Classiﬁer Weights
We further analysed the classiﬁer weights alignment
with respect to the mean feature (collapsed feature) of each
class. We used the classiﬁer weights and mean feature from
each of the models described in ablation studies to validate
the effect of MASIL in learning the maximal separable clas-
siﬁer, where the separable property between classes is mea-sured by cosine similarity. Speciﬁcally, we plotted the aver-
age cosine similarities between mean feature and the clas-
siﬁer weights of different classes i.e. Avgk6=k0fhkw0
kg
for both train and test datasets. We have illustrated this
for miniImageNet in Fig. 2. Clearly, on both the train and
test the similarity between different classes goes on increas-
ing for the ”Learnable + CE” model. While using the loss
in equation 9 (as per Neural Collapse) have no increasing
trend. Incorporating concept factorization and simplex ﬁne
tuning (in MASIL ) further reduces the similarities as the ses-
sion grows on and hence mitigate the effect of forgetting and
conﬁrming the maximum separability with MASIL .
6. Conclusion
In this paper we propose the novel framework MASIL as
an step towards learning the maximum separable classiﬁer
in a competitive setting of continual learning i.e. FSCIL.
We propose to induce the simplex from concept factoriza-
tion helps in few shot cases. We introduced novel loss func-
tion where the base and novel classes can be learnt together
during ﬁne tuning to further mitigate forgetting and over-
ﬁtting. In experiments MASIL outperforms all the bench-
marks with sufﬁcient margin on three datasets proving its
efﬁciency.

--- PAGE 9 ---
References
[1] Afra Feyza Aky ¨urek, Ekin Aky ¨urek, Derry Wijaya, and Ja-
cob Andreas. Subspace regularizers for few-shot class incre-
mental learning. arXiv preprint arXiv:2110.07059 , 2021. 1,
3, 7
[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 139–154, 2018. 1, 2
[3] Bradley M Bell and James V Burke. Algorithmic differenti-
ation of implicit functions and optimal values. In Advances
in Automatic Differentiation , pages 67–77. Springer, 2008. 5
[4] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy
Frostig, Stephan Hoyer, Felipe Llinares-L ´opez, Fabian Pe-
dregosa, and Jean-Philippe Vert. Efﬁcient and modular
implicit differentiation. arXiv preprint arXiv:2105.15183 ,
2021. 5, 12
[5] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
Jonathan Eckstein, et al. Distributed optimization and sta-
tistical learning via the alternating direction method of mul-
tipliers. Foundations and Trends® in Machine learning ,
3(1):1–122, 2011. 5, 12
[6] Francisco M Castro, Manuel J Mar ´ın-Jim ´enez, Nicol ´as Guil,
Cordelia Schmid, and Karteek Alahari. End-to-end incre-
mental learning. In Proceedings of the European conference
on computer vision (ECCV) , pages 233–248, 2018. 1, 2, 13
[7] Gert Cauwenberghs and Tomaso Poggio. Incremental and
decremental support vector machine learning. Advances in
neural information processing systems , 13, 2000. 2
[8] Hakan Cevikalp and Hasan Saribas. Deep simplex classi-
ﬁer for maximizing the margin in both euclidean and angular
spaces. arXiv preprint arXiv:2212.11747 , 2022. 4, 5
[9] Kuilin Chen and Chi-Guhn Lee. Incremental few-shot learn-
ing via vector quantization in deep embedded space. In In-
ternational Conference on Learning Representations , 2020.
1, 7, 13
[10] Ali Cheraghian, Shaﬁn Rahman, Sameera Ramasinghe,
Pengfei Fang, Christian Simon, Lars Petersson, and
Mehrtash Harandi. Synthesized feature based few-shot class-
incremental learning on a mixture of subspaces. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 8661–8670, 2021. 13
[11] Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao
Yu, and Jin Tang. Metafscil: A meta-learning approach
for few-shot class incremental learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14166–14175, 2022. 7, 13
[12] Hien Dang, Tan Nguyen, Tho Tran, Hung Tran, and Nhat
Ho. Neural collapse in deep linear network: From balanced
to imbalanced data. arXiv preprint arXiv:2301.00437 , 2023.
2, 4
[13] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and
Tinne Tuytelaars. Continual learning: A comparative study
on how to defy forgetting in classiﬁcation tasks. arXiv
preprint arXiv:1909.08383 , 2(6):2, 2019. 1, 2[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 1
[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 248–255, 2009. 6
[16] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,
Ziyan Wu, and Rama Chellappa. Learning without mem-
orizing. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 5138–5146,
2019. 1, 2, 3
[17] Songlin Dong, Xiaopeng Hong, Xiaoyu Tao, Xinyuan
Chang, Xing Wei, and Yihong Gong. Few-shot class-
incremental learning via relation knowledge distillation. In
Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence , volume 35, pages 1255–1263, 2021. 1, 2, 3
[18] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs distilla-
tion for small-tasks incremental learning. In European Con-
ference on Computer Vision , pages 86–102. Springer, 2020.
1, 3
[19] Thomas Fel, Agustin Picard, Louis Bethune, Thibaut
Boissin, David Vigouroux, Julien Colin, R ´emi Cad `ene, and
Thomas Serre. Craft: Concept recursive activation factor-
ization for explainability. arXiv preprint arXiv:2211.10154 ,
2022. 2, 5, 12
[20] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori
Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and
Daan Wierstra. Pathnet: Evolution channels gradient descent
in super neural networks. arXiv preprint arXiv:1701.08734 ,
2017. 1
[21] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
InInternational conference on machine learning , pages
1126–1135. PMLR, 2017. 2
[22] Robert M French. Catastrophic forgetting in connectionist
networks. Trends in cognitive sciences , 3(4):128–135, 1999.
1
[23] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4367–4375, 2018. 2
[24] Siavash Golkar, Michael Kagan, and Kyunghyun Cho.
Continual learning via neural pruning. arXiv preprint
arXiv:1903.04476 , 2019. 1
[25] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville,
and Yoshua Bengio. An empirical investigation of catas-
trophic forgetting in gradient-based neural networks. arXiv
preprint arXiv:1312.6211 , 2013. 1, 4
[26] Andreas Griewank and Andrea Walther. Evaluating deriva-
tives: principles and techniques of algorithmic differentia-
tion. SIAM, 2008. 5
[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 1, 12

--- PAGE 10 ---
[28] Michael Hersche, Geethan Karunaratne, Giovanni Cheru-
bini, Luca Benini, Abu Sebastian, and Abbas Rahimi. Con-
strained few-shot class-incremental learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 9057–9067, 2022. 1, 3, 7
[29] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2(7), 2015. 3
[30] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a uniﬁed classiﬁer incrementally via
rebalancing. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 831–839,
2019. 2, 3, 7, 13
[31] Bingchen Huang, Zhineng Chen, Peng Zhou, Jiayin Chen,
and Zuxuan Wu. Resolving task confusion in dynamic ex-
pansion architectures for class incremental learning. arXiv
preprint arXiv:2212.14284 , 2022. 1, 4
[32] Kejun Huang, Nicholas D Sidiropoulos, and Athanasios P
Liavas. A ﬂexible and efﬁcient algorithmic framework for
constrained matrix and tensor factorization. IEEE Transac-
tions on Signal Processing , 64(19):5052–5065, 2016. 5, 12
[33] Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung
Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting,
picking and growing for unforgetting continual learning. Ad-
vances in Neural Information Processing Systems , 32, 2019.
1
[34] KJ Joseph, Salman Khan, Fahad Shahbaz Khan,
Rao Muhammad Anwer, and Vineeth N Balasubrama-
nian. Energy-based latent aligner for incremental learning.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7452–7461, 2022. 3
[35] Mahdi M Kalayeh, Haroon Idrees, and Mubarak Shah. Nmf-
knn: Image annotation using weighted multi-view non-
negative matrix factorization. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pages 184–191, 2014. 2
[36] William Karush. Minima of functions of several variables
with inequalities as side conditions. In Traces and Emer-
gence of Nonlinear Programming , pages 217–245. Springer,
2014. 5
[37] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the national academy of sci-
ences , 114(13):3521–3526, 2017. 1, 2
[38] Steven George Krantz and Harold R Parks. The implicit func-
tion theorem: history, theory, and applications . Springer Sci-
ence & Business Media, 2002. 5
[39] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 6
[40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-
berger, editors, Advances in Neural Information Processing
Systems , volume 25. Curran Associates, Inc., 2012. 1[41] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. Communications of the ACM , 60(6):84–90, 2017. 1
[42] Harold W Kuhn and Albert W Tucker. Nonlinear program-
ming. In Traces and emergence of nonlinear programming ,
pages 247–258. Springer, 2014. 5
[43] Diangang Li, Xing Wei, Xiaopeng Hong, and Yihong Gong.
Infrared-visible cross-modal person re-identiﬁcation with an
x modality. In Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence , volume 34, pages 4610–4617, 2020. 1
[44] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-
gence , 40(12):2935–2947, 2017. 1, 2
[45] Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang,
and Wei-Shi Zheng. Preserving earlier knowledge in contin-
ual learning with the help of all previous feature extractors.
arXiv preprint arXiv:2104.13614 , 2021. 1
[46] Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Ming-
sheng Long, and Han Hu. Negative margin matters: Under-
standing margin in few-shot classiﬁcation. In European con-
ference on computer vision , pages 438–455. Springer, 2020.
13
[47] Huan Liu, Li Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu,
Jun Chen, and Jin Tang. Few-shot class-incremental learn-
ing via entropy-regularized data-free replay. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Is-
rael, October 23–27, 2022, Proceedings, Part XXIV , pages
146–162. Springer, 2022. 7, 13
[48] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. Sphereface: Deep hypersphere embedding
for face recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 212–220,
2017. 1
[49] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-
tic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 , 2016. 12
[50] Bin Lu, Xiaoying Gan, Lina Yang, Weinan Zhang, Luoyi
Fu, and Xinbing Wang. Geometer: Graph few-shot class-
incremental learning via prototype representation. arXiv
preprint arXiv:2205.13954 , 2022. 3
[51] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong.
Bayesian loss for crowd count estimation with point super-
vision. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 6142–6151, 2019. 1
[52] Zicheng Pan, Xiaohan Yu, Miaohua Zhang, and Yongsheng
Gao. Ssfe-net: Self-supervised feature enhancement for
ultra-ﬁne-grained few-shot class incremental learning. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 6275–6284, 2023. 7, 13
[53] Vardan Papyan, XY Han, and David L Donoho. Prevalence
of neural collapse during the terminal phase of deep learning
training. Proceedings of the National Academy of Sciences ,
117(40):24652–24663, 2020. 2, 4
[54] German I Parisi, Ronald Kemker, Jose L Part, Christopher
Kanan, and Stefan Wermter. Continual lifelong learning with
neural networks: A review. Neural Networks , 113:54–71,
2019. 1, 2

--- PAGE 11 ---
[55] Fabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gramfort,
Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
et al. Scikit-learn: Machine learning in python. the Journal
of machine Learning research , 12:2825–2830, 2011. 12
[56] Can Peng, Kun Zhao, Tianren Wang, Meng Li, and Brian C
Lovell. Few-shot class-incremental learning from an open-
set perspective. In European Conference on Computer Vi-
sion, pages 382–397. Springer, 2022. 3, 6, 7, 12, 13
[57] F. Pernici, M. Bruni, C. Baecchi, F. Turchini, and A. Del
Bimbo. Class-incremental learning with pre-allocated ﬁxed
classiﬁers. In 2020 25th International Conference on Pattern
Recognition (ICPR) , pages 6259–6266, Los Alamitos, CA,
USA, jan 2021. IEEE Computer Society. 13
[58] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer
and representation learning. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition , pages
2001–2010, 2017. 2, 3, 7, 13
[59] Mengye Ren, Renjie Liao, Ethan Fetaya, and Richard Zemel.
Incremental few-shot learning with attention attractor net-
works. Advances in Neural Information Processing Systems ,
32, 2019. 2, 3
[60] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems , 28, 2015. 1
[61] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115(3):211–252, 2015. 6
[62] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming
Zhan, and Xiao-Ming Wu. Overcoming catastrophic for-
getting in incremental few-shot learning by ﬁnding ﬂat min-
ima. Advances in Neural Information Processing Systems ,
34:6747–6761, 2021. 3
[63] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. Advances in
neural information processing systems , 30, 2017. 1
[64] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
networks for few-shot learning. Advances in neural informa-
tion processing systems , 30, 2017. 2, 3
[65] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele.
Meta-transfer learning for few-shot learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 403–412, 2019. 2
[66] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS
Torr, and Timothy M Hospedales. Learning to compare: Re-
lation network for few-shot learning. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1199–1208, 2018. 2, 3
[67] Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei,
and Yihong Gong. Topology-preserving class-incremental
learning. In European Conference on Computer Vision ,
pages 254–270. Springer, 2020. 3[68] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin
Dong, Xing Wei, and Yihong Gong. Few-shot class-
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
12183–12192, 2020. 1, 2, 3, 7, 12, 13
[69] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
Wierstra, et al. Matching networks for one shot learning. Ad-
vances in neural information processing systems , 29, 2016.
2, 7, 13
[70] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 6
[71] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
cremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
374–382, 2019. 2
[72] Ye Xiang, Ying Fu, Pan Ji, and Hua Huang. Incremental
learning using conditional adversarial networks. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 6619–6628, 2019. 1
[73] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-
ically expandable representation for class incremental learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 3014–3023,
2021. 1
[74] Dongbao Yang, Yu Zhou, Wei Shi, Dayan Wu, and Weiping
Wang. Rd-iod: Two-level residual-distillation-based triple-
network for incremental object detection. ACM Transactions
on Multimedia Computing, Communications, and Applica-
tions (TOMM) , 18(1):1–23, 2022. 3
[75] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip
Torr, and Dacheng Tao. Neural collapse inspired feature-
classiﬁer alignment for few-shot class incremental learning.
arXiv preprint arXiv:2302.03004 , 2023. 1, 7, 13
[76] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong
Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz.
Dreaming to distill: Data-free knowledge transfer via deep-
inversion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8715–
8724, 2020. 13
[77] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,
Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de
Weijer. Semantic drift compensation for class-incremental
learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6982–
6991, 2020. 1
[78] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning , pages 3987–3995. PMLR,
2017. 1
[79] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.
Deepemd: Few-shot image classiﬁcation with differentiable
earth mover’s distance and structured classiﬁers. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 12203–12213, 2020. 13
[80] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan,
and Yinghui Xu. Few-shot incremental learning with contin-

--- PAGE 12 ---
ually evolved classiﬁers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12455–12464, 2021. 1, 3, 7, 12, 13
[81] Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu,
and Xi Li. Mgsvf: Multi-grained slow vs. fast framework for
few-shot class-incremental learning. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2021. 13
[82] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shil-
iang Pu, and De-Chuan Zhan. Forward compatible few-shot
class-incremental learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 9046–9056, 2022. 13
[83] Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Few-shot
class-incremental learning by sampling multi-phase tasks.
arXiv preprint arXiv:2203.17030 , 2022. 7, 13
[84] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-
Lin Liu. Prototype augmentation and self-supervision for
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5871–5880, 2021. 1
[85] Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, and Zheng-Jun
Zha. Self-promoted prototype reﬁnement for few-shot class-
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6801–6810, 2021. 3, 7, 13
[86] Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-
Jun Zha. Self-sustaining representation expansion for non-
exemplar class-incremental learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9296–9305, 2022. 1A. Implementation Details
Backbone Architecture : Existing works in FSCIL have
leveraged ResNet-18, ResNet-12, ResNet-20 [27] as the
backbone network for feature extractor. Following AL-
ICE [56], we use ResNet-18 as the backbone network for
feature extractor on top which the two layer MLP for pro-
jecting the features as the classiﬁcation layer is trained for
base and incremental sessions.
Concept Factorization : We used low rank factorization
variablev= 64 for CIFAR and miniImageNet, and v= 72
for CUB-200. For function, it corresponds to randomly
choosing 10 cropped patches of size 1818on CIFAR-
100 and miniImageNet and patch size of 6464for CUB-
200. We didn’t use the scikit-learn implementation [55] of
NMF, we leverage the work of [19, 32], which uses Jax[4,
55] implementation of ADMM [5] using Jaxopt library. We
convert the Jaxarray to tensor array to be able to combine
with the tensor array obtained from Pytorch on the gradient
@A
@Xand compute the gradient with respect to input images
i.e.@P
@X;@Q
@X.
DatasetBase Session Incremental Session
EpcohsLearning
RateIterationsLearning
Rate
CIFAR-100 200 0.25 20-80 0.25
miniImageNet 500 0.25 120-190 0.025
CUB-200 80 0.025 80-150 0.05
Table 4. Training Details for Base and Incremental Session
Training Details : Data augmentation strategies like ran-
dom crop, horizontal ﬂip, rotation, brightness variation,
cutout, resizing, ﬂipping and color jittering were all applied
following the recent works [56, 68, 80]. Additionally, we
adopted the standard data pre-processing as in [56]. With
a variation of number of epochs and iterations for base and
incremental session across three datasets we keep the batch
size of 512 for base session and 64 for incremental ses-
sion during simplex ﬁnetuning as explained in Section 4.4.
Dataset wise epochs/iterations and learning rates for base
and incremental session is given in Table 4. Additionally,
we use SGDR [49] with momentum as optimizer which uses
cosine annealing strategy to reduce learning rate. Our code
will be publicly available upon acceptance.
B. Additional Results
We continued summarising results for CUB-200 com-
paring various methods with MASIL . Although improve-
ment in last session is very small +0.14% as compared to
strongest baseline ALICE [56]. But we are consistently
better in average accuracy and session wise accuracy (ex-
cept only two sessions 2 and 8). On average accuracy we

--- PAGE 13 ---
MethodsSession Accuracy (%) ( ") Average
Acc. (")Relative
Improvement0 1 2 3 4 5 6 7 8 9 10
iCaRL [58] 68.68 52.65 48.61 44.16 36.62 29.52 27.83 26.26 24.01 23.89 21.16 36.67 +39.08
EEIL [6] 68.68 53.63 47.91 44.20 36.30 27.46 25.93 24.70 23.95 24.13 22.11 36.27 +38.13
NCM [30] 68.68 57.12 44.21 28.78 26.71 25.66 24.62 21.52 20.12 20.06 19.87 32.49 +40.37
Fixed classiﬁer [57] 68.47 51.00 45.42 40.76 35.90 33.18 27.23 24.24 21.18 17.34 16.20 34.63 +44.04
D-NegCosine [46] 74.96 70.57 66.62 61.32 60.09 56.06 55.03 52.78 51.50 50.08 48.47 58.86 +11.77
D-DeepEMD [79] 75.35 70.69 66.68 62.34 59.76 56.54 54.61 52.52 50.73 49.20 47.60 58.73 +12.64
D-Cosine [69] 75.52 70.95 66.46 61.20 60.86 56.88 55.40 53.49 51.94 50.93 49.31 59.36 +10.93
DeepInv [76] 75.90 70.21 65.36 60.14 58.79 55.88 53.21 51.27 49.38 47.11 45.67 57.54 +14.57
TOPIC [68] 68.68 62.49 54.81 49.99 45.25 41.40 38.35 35.36 32.22 28.31 26.28 43.92 +33.96
IDLVQ [9] 77.37 74.72 70.28 67.13 65.34 63.52 62.10 61.54 59.04 58.68 57.81 65.23 +2.43
SPPR [85] 68.68 61.85 57.43 52.68 50.19 46.88 44.65 43.07 40.17 39.63 37.33 49.32 +22.91
[10] 68.78 59.37 59.32 54.96 52.58 49.81 48.09 46.32 44.33 43.43 43.23 51.84 +17.01
CEC [80] 75.85 71.94 68.50 63.50 62.43 58.27 57.73 55.81 54.83 53.52 52.28 61.33 +7.96
LIMIT [83] 76.32 74.18 72.68 69.19 68.79 65.64 63.57 62.69 61.47 60.44 58.45 66.67 +1.79
MgSvF [81] 72.29 70.53 67.00 64.92 62.67 61.89 59.63 59.15 57.73 55.92 54.33 62.37 +5.91
MetaFSCIL [11] 75.9 72.41 68.78 64.78 62.96 59.99 58.3 56.85 54.78 53.82 52.64 61.93 +7.6
FACT [82] 75.90 73.23 70.84 66.13 65.56 62.15 61.74 59.83 58.41 57.89 56.94 64.42 +3.3
Data-free replay [47] 75.90 72.14 68.64 63.76 62.58 59.11 57.82 55.89 54.92 53.58 52.39 61.52 +7.85
ALICE [56] 77.40 72.70 70.60 67.20 65.90 63.40 62.90 61.90 60.50 60.60 60.10 65.75 +0.14
SSFE-Net [52] 76.38 72.11 68.82 64.77 63.59 60.56 59.84 58.93 57.33 56.23 54.28 62.98 +5.96
NC-FSCIL [75] 80.45 75.98 72.30 70.28 68.17 65.16 64.43 63.25 60.66 60.01 59.44 67.28 +0.8
MASIL(Ours) 80.50 76.02 72.25 70.30 68.85 65.72 64.45 63.28 60.80 60.60 60.24 67.54
Table 5. Performance comparison on CUB-200 with ResNet-18 as backbone architecture under 10-way 5-shot FSCIL setting. Table denotes
the accuracy in each session, average accuracy across sessions and ”Relative Improvement” denotes the improvement of our method in the
last session. Methods above separating line are CIL methods for FSCIL as in [68] and [80]
outperform ALICE [56] by +1.79% as shown in the Table
5. To further analyze the underlying reason for performance
improvement because of ﬁne tuning of classiﬁer weights ob-
tained from concept basis, we calculated the average cosine
similarity of the concept basis ciwith allcj, wherej6=i
for all the three datasets as given in the Table 6. Formally it
is calculated as:
1
K(K 1)KX
i=1KX
j=1;j6=icicj (15)
Dataset Average Cosine Similarity
miniImageNet -5.22e-4
CIFAR-100 -8.78e-3
CUB-200 -4.54e-4
Table 6. Calculated cosine similarity among concept basis for each
of the three benchmark datasets
These entries are almost close to zeros resulting in the con-
cept basis which are non-overlapping and non-repetitive and
hence can induce the unique classiﬁer weights correspond
to novel classes, that can be represented in terms of their
combination.
