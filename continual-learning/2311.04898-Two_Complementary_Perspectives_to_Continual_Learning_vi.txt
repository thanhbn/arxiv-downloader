Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., và Tuytelaars, T. (2018). Memory aware synapses: Học gì (không) quên. Trong Proceedings of the European Conference on Computer Vision (ECCV), trang 139–154.

Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Lin, M., Charlin, L., và Tuytelaars, T. (2019a). Học liên tục trực tuyến với phục hồi can thiệp tối đa. Advances in Neural Information Processing Systems, 32.

Aljundi, R., Lin, M., Goujaud, B., và Bengio, Y. (2019b). Lựa chọn mẫu dựa trên gradient cho học liên tục trực tuyến. Advances in Neural Information Processing Systems, 32.

Buzzega, P., Boschini, M., Porrello, A., Abati, D., và Calderara, S. (2020). Kinh nghiệm tối cho học liên tục tổng quát: một baseline mạnh mẽ, đơn giản. Advances in Neural Information Processing Systems, 33.

Caccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau, J., và Belilovsky, E. (2022). Cái nhìn sâu sắc mới về việc giảm thay đổi biểu diễn đột ngột trong học liên tục trực tuyến. Trong International Conference on Learning Representations (ICLR).

Chaudhry, A., Ranzato, M., Rohrbach, M., và Elhoseiny, M. (2019a). Học suốt đời hiệu quả với a-GEM. Trong International Conference on Learning Representations (ICLR).

Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., và Ranzato, M. (2019b). Về bộ nhớ episo nhỏ trong học liên tục. arXiv preprint arXiv:1902.10486.

De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., và Tuytelaars, T. (2022). Một khảo sát học liên tục: Thách thức việc quên trong các nhiệm vụ phân loại. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385.

De Lange, M., van de Ven, G. M., và Tuytelaars, T. (2023). Đánh giá liên tục cho học suốt đời: Xác định khoảng cách ổn định. Trong International Conference on Learning Representations (ICLR).

Deng, D., Chen, G., Hao, J., Wang, Q., và Heng, P.-A. (2021). Làm phẳng độ sắc nét cho chiếu gradient động mang lại lợi ích cho học liên tục. Advances in Neural Information Processing Systems, 34.

Dhar, P., Singh, R. V., Peng, K.-C., Wu, Z., và Chellappa, R. (2019). Học mà không ghi nhớ. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 5138–5146.

Draxler, F., Veschgini, K., Salmhofer, M., và Hamprecht, F. (2018). Về cơ bản không có rào cản trong cảnh quan năng lượng mạng neural. Trong International Conference on Machine Learning (ICML), trang 1309–1318.

Elenter, J., NaderiAlizadeh, N., Javidi, T., và Ribeiro, A. (2023). Học liên tục primal-dual: Ổn định và tính dẻo thông qua nhân tử Lagrange. arXiv preprint arXiv:2310.00154.

Farajtabar, M., Azizan, N., Mott, A., và Li, A. (2020). Gradient descent trực giao cho học liên tục. Trong International Conference on Artificial Intelligence and Statistics (AISTATS), trang 3762–3773.

Farquhar, S. và Gal, Y. (2019). Một cái nhìn Bayesian thống nhất về học liên tục. arXiv preprint arXiv:1902.06494.

Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P., và Wilson, A. G. (2018). Bề mặt mất mát, kết nối mode, và ensemble nhanh của dnns. Advances in Neural Information Processing Systems, 31.

Guo, Y., Hu, W., Zhao, D., và Liu, B. (2022). Chiếu trực giao thích ứng cho học liên tục batch và online. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 36, trang 6783–6791.

Hadsell, R., Rao, D., Rusu, A. A., và Pascanu, R. (2020). Đón nhận thay đổi: Học liên tục trong mạng neural sâu. Trends in Cognitive Sciences, 24(12):1028–1040.

He, K., Zhang, X., Ren, S., và Sun, J. (2016). Học residual sâu cho nhận dạng hình ảnh. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 770–778.

He, X. (2018). Học liên tục bằng chính quy hóa conceptor. Trong Continual Learning Workshop (NeurIPS 2018).

He, X. và Jaeger, H. (2018). Vượt qua can thiệp thảm khốc sử dụng backpropagation hỗ trợ bởi conceptor. Trong International Conference on Learning Representations (ICLR).

Henning, C., Cervera, M., D'Angelo, F., Von Oswald, J., Traber, R., Ehret, B., Kobayashi, S., Grewe, B. F., và Sacramento, J. (2021). Meta-replay posterior cho học liên tục. Advances in Neural Information Processing Systems, 34.

Hinton, G., Vinyals, O., và Dean, J. (2014). Chưng cất kiến thức trong một mạng neural. NIPS 2014 Deep Learning Workshop.

Ioffe, S. và Szegedy, C. (2015). Batch normalization: Tăng tốc huấn luyện mạng sâu bằng cách giảm sự thay đổi covariate nội bộ. Trong International Conference on Machine Learning (ICML), trang 448–456.

Kamath, S., Soutif-Cormerais, A., Van De Weijer, J., và Raducanu, B. (2024). Phạm vi mở rộng của khoảng cách ổn định: Tiết lộ sự hiện diện của nó trong học tăng dần chung của các nhiệm vụ đồng nhất. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, trang 4182–4186.

Kao, T.-C., Jensen, K., van de Ven, G., Bernacchia, A., và Hennequin, G. (2021). Học liên tục tự nhiên: thành công là một hành trình, không (chỉ) là một đích đến. Advances in Neural Information Processing Systems, 34.

Kim, G., Xiao, C., Konishi, T., Ke, Z., và Liu, B. (2022). Một nghiên cứu lý thuyết về giải quyết học liên tục. Advances in Neural Information Processing Systems, 35.

Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Vượt qua quên lãng thảm khốc trong mạng neural. Proceedings of the National Academy of Sciences, 114(13):3521–3526.

Kolouri, S., Ketz, N. A., Soltoggio, A., và Pilly, P. K. (2020). Consolidation synaptic Cramer được cắt để bảo tồn các biểu diễn được học sâu. Trong International Conference on Learning Representations (ICLR).

Krizhevsky, A., Hinton, G., et al. (2009). Học nhiều lớp tính năng từ hình ảnh nhỏ. Technical Report, University of Toronto, Canada.

LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et al. (1998). Học dựa trên gradient được áp dụng cho nhận dạng tài liệu. Proceedings of the IEEE, 86(11):2278–2324.

Lee, K., Lee, K., Shin, J., và Lee, H. (2019). Vượt qua quên lãng thảm khốc với dữ liệu không gán nhãn trong tự nhiên. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), trang 312–321.

Lesort, T., Lomonaco, V., Stoian, A., Maltoni, D., Filliat, D., và Díaz-Rodríguez, N. (2020). Học liên tục cho robot: Định nghĩa, khung, chiến lược học, cơ hội và thách thức. Information Fusion, 58:52–68.

Li, Z. và Hoiem, D. (2017). Học mà không quên. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947.

Lin, S., Yang, L., Fan, D., và Zhang, J. (2022). TRGP: Chiếu gradient vùng tin cậy cho học liên tục. Trong International Conference on Learning Representations (ICLR).

Lin, Z., Shi, J., Pathak, D., và Ramanan, D. (2021). Điểm chuẩn CLEAR: Học liên tục trên hình ảnh thế giới thực. Trong Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).

Lopez-Paz, D. và Ranzato, M. (2017). Bộ nhớ episo gradient cho học liên tục. Advances in Neural Information Processing Systems, 30.

Masse, N. Y., Grant, G. D., và Freedman, D. J. (2018). Giảm thiểu quên lãng thảm khốc sử dụng gating phụ thuộc ngữ cảnh và ổn định synaptic. Proceedings of the National Academy of Sciences, 115(44):E10467–E10475.

McCloskey, M. và Cohen, N. J. (1989). Can thiệp thảm khốc trong mạng kết nối: Vấn đề học tuần tự. Trong Psychology of Learning and Motivation, tập 24, trang 109–165. Elsevier.

Mirzadeh, S. I., Farajtabar, M., Gorur, D., Pascanu, R., và Ghasemzadeh, H. (2021). Kết nối mode tuyến tính trong học đa nhiệm vụ và liên tục. Trong International Conference on Learning Representations (ICLR).

Mundt, M., Hong, Y., Pliushch, I., và Ramesh, V. (2023). Một cái nhìn toàn diện về học liên tục với mạng neural sâu: Các bài học bị lãng quên và cầu nối đến học tích cực và thế giới mở. Neural Networks, 160:306–336.

Nguyen, C. V., Li, Y., Bui, T. D., và Turner, R. E. (2018). Học liên tục biến phân. Trong International Conference on Learning Representations (ICLR).

Prabhu, A., Al Kader Hammoud, H. A., Dokania, P. K., Torr, P. H., Lim, S.-N., Ghanem, B., và Bibi, A. (2023). Học liên tục có ngân sách tính toán: Điều gì quan trọng? Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 3698–3707.

Ratcliff, R. (1990). Các mô hình kết nối của bộ nhớ nhận dạng: ràng buộc áp đặt bởi các hàm học và quên. Psychological Review, 97(2):285.

Rebuffi, S.-A., Kolesnikov, A., Sperl, G., và Lampert, C. H. (2017). ICARL: Học phân loại và biểu diễn tăng dần. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 2001–2010.

Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., và Tesauro, G. (2018). Học để học mà không quên bằng cách tối đa hóa chuyển giao và tối thiểu hóa can thiệp. Trong International Conference on Learning Representations (ICLR).

Robins, A. (1995). Quên lãng thảm khốc, diễn tập và pseudo-diễn tập. Connection Science, 7(2):123–146.

Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., và Wayne, G. (2019). Phát lại kinh nghiệm cho học liên tục. Advances in Neural Information Processing Systems, 32.

Rudner, T. G., Smith, F. B., Feng, Q., Teh, Y. W., và Gal, Y. (2022). Học liên tục thông qua suy luận biến phân không gian hàm tuần tự. Trong International Conference on Machine Learning, trang 18871–18887.

Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., và Hadsell, R. (2016). Mạng neural tiến bộ. arXiv preprint arXiv:1606.04671.

Saha, G., Garg, I., và Roy, K. (2021). Bộ nhớ chiếu gradient cho học liên tục. Trong International Conference on Learning Representations (ICLR).

Schulman, J., Levine, S., Abbeel, P., Jordan, M., và Moritz, P. (2015). Tối ưu hóa chính sách vùng tin cậy. Trong International Conference on Machine Learning (ICML), trang 1889–1897.

Serra, J., Suris, D., Miron, M., và Karatzoglou, A. (2018). Vượt qua quên lãng thảm khốc với chú ý cứng đến nhiệm vụ. Trong International Conference on Machine Learning (ICML), trang 4548–4557.

Shin, H., Lee, J. K., Kim, J., và Kim, J. (2017). Học liên tục với phát lại sinh sâu. Advances in Neural Information Processing Systems, 30.

Titsias, M. K., Schwarz, J., de G. Matthews, A. G., Pascanu, R., và Teh, Y. W. (2020). Chính quy hóa chức năng cho học liên tục với quá trình Gaussian. Trong International Conference on Learning Representations (ICLR).

van de Ven, G. M., Li, Z., và Tolias, A. S. (2021). Học tăng dần theo lớp với bộ phân loại sinh. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W), trang 3611–3620.

van de Ven, G. M., Siegelmann, H. T., và Tolias, A. S. (2020). Phát lại lấy cảm hứng từ não cho học liên tục với mạng neural nhân tạo. Nature Communications, 11:4069.

van de Ven, G. M., Tuytelaars, T., và Tolias, A. S. (2022). Ba kiểu học tăng dần. Nature Machine Intelligence, 4(12):1185–1197.

Verwimp, E., De Lange, M., và Tuytelaars, T. (2021). Diễn tập được tiết lộ: Giới hạn và giá trị của việc xem lại mẫu trong học liên tục. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), trang 9385–9394.

Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Mạng khớp nối cho học một lần. Advances in Neural Information Processing Systems, 29.

Wang, L., Zhang, X., Yang, K., Yu, L., Li, C., Hong, L., Zhang, S., Li, Z., Zhong, Y., và Zhu, J. (2022). Phát lại bộ nhớ với nén dữ liệu cho học liên tục. Trong International Conference on Learning Representations (ICLR).

Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., và Fu, Y. (2019). Học tăng dần quy mô lớn. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 374–382.

Yan, S., Xie, J., và He, X. (2021). DER: Biểu diễn mở rộng động cho học tăng dần theo lớp. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 3014–3023.

Yoo, J., Liu, Y., Wood, F., và Pleiss, G. (2024). Phát lại cận kề theo lớp: Một phương pháp điểm cận kề cho học liên tục trực tuyến. arXiv preprint arXiv:2402.09542.

Yoon, J., Yang, E., Lee, J., và Hwang, S. J. (2018). Học suốt đời với mạng mở rộng động. Trong International Conference on Learning Representations (ICLR).

Zaj ˛ ac, M., Tuytelaars, T., và van de Ven, G. M. (2024). Phân loại dựa trên lỗi dự đoán cho học tăng dần theo lớp. Trong International Conference on Learning Representations (ICLR).

Zeng, G., Chen, Y., Cui, B., và Yu, S. (2019). Học liên tục xử lý phụ thuộc ngữ cảnh trong mạng neural. Nature Machine Intelligence, 1(8):364–372.

Zenke, F., Poole, B., và Ganguli, S. (2017). Học liên tục thông qua trí thông minh synaptic. Trong International Conference on Machine Learning (ICML), trang 3987–3995.

--- TRANG 16 ---
Hai Góc Nhìn Bổ Sung cho Học Liên Tục

PHỤ LỤC

Phụ lục chứa thông tin bổ sung cho nội dung được trình bày trong phần chính. Phụ lục A và B mở rộng về metric độ chính xác tối thiểu trung bình được sử dụng trong văn bản chính để định lượng khoảng cách ổn định, và cung cấp chi tiết về các cơ chế học liên tục BiC và DER. Phụ lục C chi tiết các nét tinh tế của triển khai phương pháp kết hợp ER + GEM của chúng tôi. Phụ lục D chứa các thí nghiệm bổ sung ngoài giao thức thí nghiệm gốc của chúng tôi phân tích ảnh hưởng của siêu tham số γ của GEM. Cuối cùng, trong Phụ lục E, chúng tôi tiết lộ kết quả cho tất cả các thí nghiệm bao gồm những thí nghiệm chưa được đưa vào văn bản chính.

Mã được ghi chép để tái tạo tất cả các thí nghiệm có sẵn công khai tại https://github.com/TimmHess/TwoComplementaryPerspectivesCL.

A METRIC ĐỘ CHÍNH XÁC

Độ chính xác phân loại là metric cơ sở chúng tôi sử dụng trong suốt công trình này. Cho một tập dữ liệu D và mô hình f, độ chính xác phân loại A(D, f) là tỷ lệ phần trăm mẫu trong D được phân loại đúng bởi f.

Độ chính xác trung bình cuối cùng là độ chính xác phân loại được tính trung bình qua tất cả các nhiệm vụ ở cuối quá trình huấn luyện. Chính thức:

avg-ACC = 1/T ∑(t=1 to T) A(D̂t, fw_final), (3)

trong đó D̂t, ∀t∈[1, ..., T] là tập đánh giá của mỗi nhiệm vụ, và fw_final là mô hình cuối cùng sau khi kết thúc huấn luyện trên tất cả T nhiệm vụ. Đây là một metric phổ biến để biểu thị chất lượng của mô hình được học liên tục.

Độ chính xác tối thiểu trung bình biểu thị trung bình của các độ chính xác phân loại thấp nhất được quan sát cho mỗi nhiệm vụ, được đo từ ngay sau khi hoàn thành huấn luyện trên nhiệm vụ đó cho đến cuối tất cả việc học trong chuỗi nhiệm vụ:

min-ACC (Tt) = min(|Tt|<n≤|TT|) A(D̂t, fw_n), (4)

avg-min-ACC = 1/(T-1) ∑(t=1 to T-1) min-ACC (Tt), (5)

trong đó fw_n chỉ ra mô hình sau lần lặp huấn luyện thứ n. Bằng cách lạm dụng ký hiệu nhẹ, n=|Tt| đề cập đến lần lặp huấn luyện cuối cùng của nhiệm vụ Tt, và n=|TT| đánh dấu lần lặp huấn luyện cuối cùng của toàn bộ chuỗi nhiệm vụ T. Metric này là một thước đo trường hợp xấu nhất về mức độ duy trì độ chính xác phân loại của mô hình tại bất kỳ điểm nào trong suốt quá trình huấn luyện liên tục. Để làm cho việc tính toán A(D̂t, fw_n) theo lần lặp trở nên khả thi về mặt tính toán, chúng tôi sử dụng một tập đánh giá rút gọn có kích thước 1000 mỗi nhiệm vụ. Tập rút gọn được lấy mẫu đồng nhất từ mỗi tập đánh giá tương ứng, một lần cho mỗi lần chạy. Điều này tương tự như De Lange et al. (2023) và đã chỉ ra thực nghiệm rằng nó xấp xỉ gần với việc sử dụng tập đánh giá đầy đủ.

B CHI TIẾT PHƯƠNG PHÁP

BiC (Bias-Correction) được đề xuất bởi Wu et al. (2019), những người được thúc đẩy bởi quan sát rằng huấn luyện liên tục tăng dần theo lớp khiến cho bộ phân loại được huấn luyện liên tục trở nên thiên vị về tập lớp được quan sát gần đây nhất. Phương pháp này được thiết kế cụ thể cho các thiết lập học tăng dần theo lớp trong đó mỗi nhiệm vụ được quan sát Tt giới thiệu một tập các lớp không chồng chéo m, sao cho dữ liệu tương ứng Xt^m = {(xi, yi), ∀yi∈[n+ 1, ..., n +m]}. Ở đây, xi, yi biểu thị cặp ví dụ và nhãn, và n là số lớp đã được quan sát. Để điều chỉnh thiên vị, một tập (nhỏ) dữ liệu xác thực cho mỗi nhiệm vụ được lưu trữ. Những dữ liệu này được lấy từ tập huấn luyện và loại trừ khỏi việc huấn luyện mô hình. Chúng chỉ được sử dụng để điều chỉnh thiên vị của bộ phân loại trong một giai đoạn huấn luyện riêng biệt. Bản thân việc điều chỉnh thiên vị được thực hiện bởi một lớp tuyến tính bao gồm hai tham số, α và β, và được gọi là lớp điều chỉnh thiên vị. Các logit được tạo ra bởi mô hình cho các lớp đã quan sát trước đó được giữ nguyên, nhưng thiên vị trong các logit được tạo ra cho m lớp mới được quan sát (n+ 1, ..., n +m) được điều chỉnh bởi lớp điều chỉnh thiên vị:

qk = {
    ok                    1≤k≤n
    αok+β                n + 1≤k≤n+m
}, (6)

trong đó ok biểu thị logit đầu ra cho lớp thứ k. Các tham số điều chỉnh thiên vị (α, β) được chia sẻ cho tất cả các lớp mới và được tối ưu hóa thông qua mất mát phân loại cross-entropy:

Lb = -∑(k=1 to n+m) log[pk(qk)], (7)

với pk(.) chỉ ra xác suất đầu ra, tức là softmax của các logit.

Bên cạnh việc điều chỉnh thiên vị, BiC sử dụng augmentation dữ liệu, một cơ chế phát lại, và một cơ chế chưng cất để huấn luyện liên tục mô hình. Augmentation dữ liệu bao gồm cắt ngẫu nhiên với tỷ lệ từ 0.2 đến 1.0 và lật ngang ngẫu nhiên với xác suất p = 0.5. Những augmentation này cũng được áp dụng cho các mẫu được lưu trong bộ đệm trong quá trình phát lại. Cơ chế phát lại tổng quát được thảo luận trong Mục 4 của phần chính của bài báo này. Ở đây, chúng tôi đơn giản hóa nó thành việc phân bổ một bộ đệm phụ trợ X̂ có kích thước M cho phép xen kẽ huấn luyện với các exemplar từ n lớp đã quan sát trước đó. Một bổ sung đáng chú ý là bộ đệm này chứa cả exemplar phát lại để huấn luyện và để xác thực, với phần sau đã bao gồm các mẫu từ nhiệm vụ hiện tại. Wu et al. (2019) thấy rằng tỷ lệ phân bổ 9 : 1 cho huấn luyện/xác thực là đủ. Mất mát huấn luyện cross-entropy xen kẽ phát lại được công thức hóa như:

Lc = ∑((x,y)∈X̂n∪Xt^m) ∑(k=1 to n+m) -δy=k log(pk(x)). (8)

Mất mát chưng cất chính quy hóa bổ sung được công thức hóa như:

Ld = ∑(x∈X̂n∪Xt^m) ∑(k=1 to n) -π̂k(x) log[πk(x)], (9)

π̂k = e^(ôn_k(x)/T) / ∑(j=1 to n) e^(ôn_j(x)/T), πk(x) = e^(on+m_k(x)/T) / ∑(j=1 to n) e^(on+1_j(x)/T),

với ôn biểu thị các logit từ mô hình cũ, trước đó và T là vô hướng nhiệt độ. Lưu ý rằng việc điều chỉnh thiên vị trước đó được áp dụng trong ôn. Cuối cùng, cả hai mất mát được kết hợp thành mất mát huấn luyện tổng:

L = λLd + (1−λ)Lc, (10)

với vô hướng cân bằng λ = n/(n+m), với n và m là số lớp cũ và mới tương ứng.

DER (Dark Experience Replay) là một phương pháp phát lại kinh nghiệm mở rộng công thức phát lại tiêu chuẩn (c.f. Eq. 8) bằng một thành phần chính quy hóa dựa trên chưng cất (Hinton et al., 2014):

LDER = Lc + αE(x,ok)∼X̂[DKL(pk(ok)||f(x))], (11)

với siêu tham số giảm mất mát α. Bộ đệm phát lại phụ trợ được định nghĩa như:

X̂ = {(xi, oi), 0≤i < M},

chứa M cặp (xi, oi) của các exemplar nhiệm vụ trước đó xi cùng với các logit đầu ra của mô hình oi (tại thời điểm thêm chúng vào bộ đệm), thay vì các target yi.

Hơn nữa, để tránh mất thông tin trong hàm softmax khi so sánh đầu ra mô hình f(x) với oi, các tác giả chọn xấp xỉ phân kỳ KL (DKL) bằng khoảng cách Euclidean. Với điều đó, mất mát cuối cùng trở thành:

LDER = Lc + αE(x,z)∼X̂[||z−h(x)||²₂]. (12)

Như đối với BiC, augmentation dữ liệu bằng cắt ngẫu nhiên với tỷ lệ từ 0.2 đến 1.0 và lật ngang ngẫu nhiên với xác suất p = 0.5 được áp dụng cho tất cả dữ liệu được chuyển tiếp.

C CHI TIẾT TRIỂN KHAI

Việc triển khai thực tế của việc kết hợp phát lại kinh nghiệm (ER) và cơ chế chiếu gradient của GEM hoặc A-GEM bao gồm nhiều khía cạnh có lợi từ việc làm rõ bổ sung. Trong mục này chúng tôi thảo luận về việc tính toán các gradient tham chiếu và xử lý batch normalization. Để đi kèm cuộc thảo luận này, chúng tôi cung cấp mã giả chi tiết cho các triển khai ER + A-GEM (Thuật toán 2) và ER + GEM (Thuật toán 3), bổ sung cho mã giả cấp cao hơn cho ER + A-GEM trong văn bản chính.

Thuật toán 2 ER + AGEM (chi tiết)
Yêu cầu: tham số w, hàm mất mát ℓ, tốc độ học λ, luồng dữ liệu {D1, ..., D T}
1: M← {}
2: cho t= 1, ..., T thực hiện
3:   cho (x, y)∈Dt thực hiện
4:     #1. Lấy mẫu từ bộ đệm bộ nhớ
5:     (x̃k,ỹk)←SAMPLE (M)
6:     
7:     #2. Tính gradient của hàm mất mát chung (xấp xỉ)
8:     [zx, zx̃]←fw([x,x̃])
9:     g← ∇ wℓ(zx, y)
10:    gold← ∇ wℓ(zx̃,ỹ)
11:    gjoint←1/t g+ (1−1/t)gold
12:    
13:    #3. Tính gradient tham chiếu
14:    gref←gold
15:    
16:    #4. Chiếu gradient
17:    ḡ←PROJECT_AGEM (gjoint, gref)
18:    
19:    #5. Cập nhật tham số mô hình
20:    w←OPTIMIZER_STEP (w, λ, ḡ)
21:  kết thúc cho
22:  M←UPDATE_BUFFER (M, D t)
23: kết thúc cho

Tính toán gradient tham chiếu: Trong cơ chế GEM và A-GEM, các gradient tham chiếu thông báo cho việc chiếu gradient bằng cách chỉ ra hướng gradient làm giảm mất mát trên các nhiệm vụ đã học trước đó. Khi kết hợp ER với A-GEM, chúng tôi lấy các gradient tham chiếu giống như các gradient được sử dụng trong việc tối ưu hóa hàm mất mát chung xấp xỉ (tức là, gref=gold). Việc tính toán các gradient tham chiếu trên một mini-batch được lấy mẫu riêng biệt từ bộ đệm bộ nhớ có thể có tác động có lợi cho huấn luyện, nhưng sẽ đi kèm với chi phí tính toán tăng lên của việc tăng gấp đôi kích thước mini-batch phát lại một cách hiệu quả. Khi kết hợp ER với GEM, việc thu được các gradient tham chiếu phức tạp hơn vì cần một gradient tham chiếu riêng biệt cho mỗi nhiệm vụ đã học trước đó. Trong công thức gốc của GEM bởi Lopez-Paz và Ranzato (2017), các gradient tham chiếu được tính toán đối với toàn bộ bộ đệm bộ nhớ. Điều này nhanh chóng trở nên rất tốn kém về mặt tính toán nếu lượng lớn dữ liệu được lưu trữ. Như một biện pháp giảm thiểu, thay vì sử dụng toàn bộ bộ đệm của mỗi nhiệm vụ để tính toán gradient tham chiếu, chúng tôi lấy mẫu một mini-batch

Thuật toán 3 ER + GEM (chi tiết)
Yêu cầu: tham số w, hàm mất mát ℓ, tốc độ học λ, siêu tham số γ, luồng dữ liệu {D1, ..., D T}
1: Mt← {}, ∀t= 1, ..., T
2: cho t= 1, ..., T thực hiện
3:   cho (x, y)∈Dt thực hiện
4:     #1. Lấy mẫu từ bộ đệm bộ nhớ
5:     (x̃k,ỹk)←SAMPLE (Mk) cho tất cả k < t
6:     M← {(x̃k,ỹk)} cho tất cả k < t
7:     (x̃k<t,ỹk<t)←SAMPLE (M)
8:     
9:     #2. Tính gradient của hàm mất mát chung (xấp xỉ)
10:    [zx, zx̃k<t]←fw([x,x̃k<t])
11:    g← ∇ wℓ(zx, y)
12:    gold← ∇ wℓ(zx̃k<t,ỹk<t)
13:    gjoint←1/t g+ (1−1/t)gold
14:    
15:    #3. Tính gradient tham chiếu
16:    FREEZE_BATCH_NORM (fw)
17:    grefk← ∇ wℓ(fw(x̃k),ỹk) cho tất cả k < t
18:    UNFREEZE_BATCH_NORM (fw)
19:    
20:    #4. Chiếu gradient
21:    ḡ←PROJECT_GEM (gjoint,[gref1, ..., g refk], γ)
22:    
23:    #5. Cập nhật tham số mô hình
24:    w←OPTIMIZER_STEP (w, λ, ḡ)
25:  kết thúc cho
26:  Mt←FILL_BUFFER (Dt)
27: kết thúc cho

mỗi nhiệm vụ đã quan sát trước đó. Cụ thể, khi huấn luyện trên nhiệm vụ t, chúng tôi lấy mẫu k=t−1 mini-batch, một từ mỗi bộ đệm bộ nhớ cụ thể cho nhiệm vụ Mk, xem Thuật toán 3 dòng 5. Tất cả các mini-batch được lấy mẫu có cùng kích thước với mini-batch hiện tại được quan sát bởi mô hình. Để xấp xỉ gần quan hệ 'cùng-mini-batch' giữa gradient phát lại và các gradient tham chiếu, sau đó chúng tôi thu được mini-batch phát lại bằng cách lấy mẫu đồng nhất từ k mini-batch của các gradient tham chiếu, xem Thuật toán 3 dòng 6-7.

Batch norm: Khi triển khai ER + A-GEM hoặc ER + GEM, một khía cạnh khác cần xem xét cẩn thận là việc sử dụng batch normalization (Ioffe và Szegedy, 2015), được bao gồm trong kiến trúc ResNet-18 rút gọn mà chúng tôi sử dụng cho tất cả các điểm chuẩn ngoại trừ Rotated MNIST. Khi huấn luyện với batch norm, các thống kê normalization được tính toán tương đối với từng mini-batch riêng lẻ được chuyển tiếp qua mô hình. Điều này có nghĩa là khi dữ liệu hiện tại và dữ liệu phát lại được chuyển tiếp qua mô hình trong các mini-batch khác nhau, chúng sử dụng các thống kê normalization khác nhau, điều này có thể gây ra sự bất ổn trong quá trình huấn luyện. Đối với phát lại tiêu chuẩn, cũng như đối với ER + A-GEM, chúng tôi có thể giảm thiểu sự bất ổn tiềm năng này bằng cách chuyển tiếp dữ liệu hiện tại và được phát lại cùng nhau, xem Thuật toán 2 dòng 8. Tuy nhiên, khi sử dụng phát lại kết hợp với GEM, nó trở nên phức tạp hơn, vì nhiều dữ liệu từ bộ đệm bộ nhớ được chuyển tiếp qua mô hình hơn so với yêu cầu để xấp xỉ hàm mất mát chung. Việc chuyển tiếp tất cả dữ liệu cùng nhau có vẻ không mong muốn vì nó sẽ thiên vị các thống kê normalization quá nhiều về phía dữ liệu từ các nhiệm vụ trước đó (và việc làm như vậy cũng có thể không thực tế vì kích thước mini-batch có thể trở nên quá lớn để chuyển tiếp tất cả dữ liệu cùng nhau). Thay vào đó, các triển khai phổ biến của GEM thường chuyển tiếp dữ liệu của mỗi nhiệm vụ trước đó riêng biệt, có nghĩa là mỗi gradient tham chiếu được tính toán với một normalization tùy chỉnh, cụ thể cho nhiệm vụ. Một normalization khác nhau như vậy cho mỗi gradient tham chiếu có thể gây ra sự bất ổn trong quá trình huấn luyện. Để cố gắng giảm thiểu điều này, khi tính toán các gradient tham chiếu cho GEM, chúng tôi đóng băng các lớp batch-norm, xem Thuật toán 3 dòng 16-18.

D NGHIÊN CỨU BỔ SUNG VỀ QUY TRÌNH TỐI ƯU HÓA CỦA GEM

Ở đây, chúng tôi xem xét chi tiết quy trình tối ưu hóa của GEM; cụ thể, bước chiếu gradient của nó (tức là, dòng 21 trong Thuật toán 3). Trong khi thực hiện các thí nghiệm cho bài báo này, chúng tôi nhận ra rằng quy trình tối ưu hóa của GEM không được định nghĩa một cách rõ ràng và nó có một siêu tham số có ảnh hưởng. Tác động của siêu tham số này, không có trong quy trình tối ưu hóa của A-GEM, có thể giải thích phần lớn sự khác biệt về hiệu suất mà chúng tôi quan sát giữa việc sử dụng quy trình tối ưu hóa của GEM so với A-GEM (xem mục 6 trong văn bản chính).

Động lực đằng sau cơ chế tối ưu hóa của GEM là chỉ cho phép những cập nhật gradient không làm tăng mất mát trên bất kỳ nhiệm vụ trước đó nào. Về mặt toán học, Lopez-Paz và Ranzato (2017) công thức hóa cơ chế tối ưu hóa của GEM như sau. Khi huấn luyện trên nhiệm vụ t, gradient ḡ dựa trên đó bước tối ưu hóa được thực hiện được cho bởi nghiệm của:

minimize ḡ 1/2||g−ḡ||²₂ (13)
subject to ⟨ḡ, gk⟩ ≥0,∀k < t, (14)

trong đó g là gradient của mất mát được tối ưu hóa và gk là gradient tham chiếu được tính toán trên dữ liệu được lưu trữ cho nhiệm vụ thứ k. Phương trình (13) và (14) định nghĩa một chương trình bậc hai (QP) trong p biến, với p là số tham số có thể huấn luyện của mạng neural. Để giải QP này, GEM sử dụng bài toán đối ngẫu, được cho bởi:

minimize v 1/2v^T GG^T + g^T G^T v (15)
subject to v≥0, (16)

với G= (g1, ..., g t−1). Bài toán đối ngẫu này là một QP chỉ trong t−1 biến, và do đó có thể được giải hiệu quả hơn. Sau khi giải bài toán đối ngẫu này, ḡ có thể được phục hồi như:

ḡ=G^T v*+g, (17)

trong đó v* là nghiệm của bài toán đối ngẫu.

Tuy nhiên đây không phải là toàn bộ câu chuyện. Lopez-Paz và Ranzato (2017) tiếp tục giới thiệu một siêu tham số γ, vì trong thực tế họ thấy rằng 'thêm một hằng số nhỏ γ≥0 vào v* thiên vị việc chiếu gradient về phía các cập nhật ưu tiên chuyển giao ngược có lợi' (trang 4). Dựa trên mô tả này, người đọc có thể kỳ vọng Phương trình (17) thay đổi thành g̃=G^T(v*+γ) +g, nhưng trong triển khai mã chính thức của GEM, γ thay vào đó được thêm vào phía bên phải của ràng buộc bất đẳng thức của bài toán đối ngẫu (tức là, bất đẳng thức trong Phương trình (16) thay đổi thành v≥γ).

Hơn nữa, việc đặt γ >0 giới thiệu một tinh tế khác, vì nó làm cho nghiệm ḡ của bài toán đối ngẫu luôn khác với g, ngay cả khi ràng buộc ⟨g, gk⟩ ≥0 được thỏa mãn cho từng nhiệm vụ trước đó k. Tuy nhiên, trong triển khai mã chính thức của GEM, ḡ vẫn được đặt thành g nếu ⟨g, gk⟩ ≥0 cho tất cả k < t. Nói cách khác, siêu tham số γ chỉ được sử dụng nếu ⟨g, gk⟩<0 cho ít nhất một nhiệm vụ trước đó k. Điều này do đó giới thiệu một sự gián đoạn (xem Hình D.1 để có đánh giá thực nghiệm về tác động của điều này).

Hình D.1: Khi nào thực hiện chiếu GEM. Được minh họa là sự khác biệt cho ER + GEM giữa việc thực hiện thao tác chiếu GEM tại mỗi lần lặp so với chỉ khi độ tương tự cosine của gradient hiện tại với ít nhất một gradient tham chiếu ≤0.

Cách mà siêu tham số γ được xử lý trong triển khai mã chính thức của GEM thường được các triển khai có sẵn công khai khác của GEM tiếp nhận. Đối với các thí nghiệm đã đăng ký trước được báo cáo trong văn bản chính, chúng tôi cũng tuân theo triển khai mã chính thức của GEM, và chúng tôi sử dụng γ= 0.5, vì đây là giá trị mà Lopez-Paz và Ranzato (2017) sử dụng cho tất cả các thí nghiệm chính của họ.

Trong Phụ lục này chúng tôi báo cáo các thí nghiệm bổ sung khám phá tác động của siêu tham số γ. Đầu tiên, chúng tôi lưu ý rằng tác động của γ có thể được hiểu như việc mở rộng ảnh hưởng của các gradient tham chiếu gk đối với ḡ. Lý do cho điều này là γ có xu hướng tăng v*, và ḡ liên quan đến v* thông qua ḡ=G^T v*+g. Hình D.2 đánh giá thực nghiệm tác động của việc thay đổi γ đối với hiệu suất của ER + GEM trên các phiên bản offline của Rotated MNIST và Domain CIFAR-100. Đối với các thí nghiệm này, sự gián đoạn về siêu tham số γ được loại bỏ (tức là, bài toán đối ngẫu được giải tại mỗi lần lặp, chúng tôi không kiểm tra trước xem ⟨g, gk⟩<0 cho ít nhất một k < t). Đối với Rotated MNIST, chúng tôi thấy rằng việc tăng γ dẫn đến cả việc giảm khoảng cách ổn định và tăng hiệu suất cuối cùng. Đối với Domain CIFAR-100, chúng tôi thấy rằng γ càng thấp, sự sụp đổ xuất hiện càng muộn, và với γ≤0.05 chúng tôi không còn quan sát thấy sự sụp đổ.

E KẾT QUẢ BỔ SUNG

Trong mục này chúng tôi cung cấp các kết quả còn lại mà chúng tôi thu được từ giao thức thí nghiệm nhưng không đưa vào văn bản chính để tránh lộn xộn. Một tổng quan toàn diện về tất cả dữ liệu có thể được tìm thấy trong Bảng E.1 cho Rotated MNIST, Bảng E.2 cho Domain CIFAR-100, Bảng E.3 cho Split CIFAR-100, và Bảng E.4 cho Mini-ImageNet. Ngoài ra, chúng tôi đi kèm cái nhìn dạng bảng bằng các biểu đồ bổ sung để đánh giá định tính (Hình E.1 và E.2).

[Các bảng và hình chi tiết được liệt kê tiếp tục với tất cả dữ liệu thí nghiệm đầy đủ...]
