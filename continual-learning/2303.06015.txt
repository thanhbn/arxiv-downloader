# 2303.06015.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2303.06015.pdf
# File size: 456539 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Dynamic Y-KD: A Hybrid Approach to Continual Instance Segmentation
Mathieu Pag ´e Fortin, Brahim Chaib-draa
Laval University, Canada
mathieu.page-fortin.1@ulaval.ca, brahim.chaib-draa@ift.ulaval.ca
Abstract
Despite the success of deep learning models on instance seg-
mentation, current methods still suffer from catastrophic for-
getting in continual learning scenarios. In this paper, our
contributions for continual instance segmentation are three-
fold. First, we propose the Y-knowledge distillation (Y-KD ),
a technique that shares a common feature extractor between
the teacher and student networks. As the teacher is also up-
dated with new data in Y-KD , the increased plasticity results
in new modules that are specialized on new classes. Sec-
ond, our Y-KD approach is supported by a dynamic architec-
ture method that trains task-specific modules with a unique
instance segmentation head, thereby significantly reducing
forgetting. Third, we complete our approach by leveraging
checkpoint averaging as a simple method to manually bal-
ance the trade-off between performance on the various sets
of classes, thus increasing control over the model’s behavior
without any additional cost. These contributions are united in
our model that we name the Dynamic Y-KD network .
We perform extensive experiments on several single-step and
multi-steps incremental learning scenarios, and we show that
our approach outperforms previous methods both on past
and new classes. For instance, compared to recent work, our
method obtains +2.1%mAP on old classes in 15-1,+7.6%
mAP on new classes in 19-1 and reaches 91.5%of the mAP
obtained by joint-training on all classes in 15-5.
Introduction
Instance segmentation, the task of detecting and segmenting
each object individually in images, is a fundamental prob-
lem of computer vision that has many applications. Several
approaches based on deep learning have been proposed in
the last few years (Gu, Bai, and Kong 2022). However, it is
generally assumed that the training dataset is fixed, such that
training can be done in one step. This scenario faces limita-
tions when deployed in real-world applications where the
environment can change or use cases can evolve to include
new sets of classes (Lesort et al. 2020). Incrementing deep
learning models to introduce new object categories is a chal-
lenging task because these methods are prone to catastrophic
forgetting (McCloskey and Cohen 1989); they become bi-
ased towards novel classes while previous knowledge is dis-
carded.
The challenge of catastrophic forgetting is encapsulated
by the stability-plasticity dilemma (Wu, Gong, and Li 2021;
Figure 1: Overview of the differences between standard KD (top
row) and our Dynamic Y-KD network (bottom row). During train-
ing, our approach only uses the previous head Ht−1while the fea-
ture extractor is shared between the teacher and student networks.
This increases plasticity, allowing to train task-specific feature ex-
tractors while preserving a generic head Ht. At inference, our Dy-
namic Y-KD network benefits from more stability by using special-
ized modules in parallel with a generic instance segmentation head.
Grossberg 1982): a learning model must balance the preser-
vation of past knowledge (stability) with the flexibility to ac-
quire new knowledge (plasticity). However, these two abili-
ties generally conflict with one another. For instance, as gra-
dient descent updates the weights of neural networks to learn
new categories, this high plasticity also induces the replace-
ment and suppression of previous knowledge (De Lange
et al. 2021).
Continual learning (CL) is thus gaining more attention as
it aims to bring deep learning methods to succeed even on
non-stationary datasets. Previous work mostly studied CL
for classification (De Lange et al. 2021), and to a lesser
extent semantic segmentation (Cermelli et al. 2020; Douil-
lard et al. 2021) and object detection (Menezes et al. 2023;
Wang et al. 2021). To our knowledge, the works of Gu,
Deng, and Wei (2021) and Cermelli et al. (2022) are the
only ones to propose continual instance segmentation (CIS)
approaches. They both rely exclusively on knowledge dis-arXiv:2303.06015v3  [cs.CV]  11 Sep 2023

--- PAGE 2 ---
tillation (KD) (Hinton, Vinyals, and Dean 2015), a popular
regularization-based strategy that uses the model from the
previous step as a teacher network to distill its knowledge
in the new model, thereby reducing forgetting (see Fig. 1,
top row). However, the main drawback of KD is that per-
formances are generally limited (Menezes et al. 2023). KD
constrains the model to increase stability but this comes at
the cost of reduced plasticity, making new classes harder to
learn optimally.
To address this, we propose in this paper a new KD strat-
egy in which the teacher and student networks share a com-
mon trainable feature extractor, coupled with a dynamic
architecture model that grows new task-specific modules.
These two design choices are motivated by a preliminary
study that highlights two key properties of Mask R-CNN
trained in incremental scenarios, namely 1) the stability of
feature extractors and 2) the compatibility of the head with
previous feature extractors (see Fig. 2). We name our hybrid
approach the Dynamic Y-KD network .
Before learning new classes, the model from the previ-
ous step is duplicated and we only freeze the teacher in-
stance segmentation head. Training images are then fed to
the shared feature extractor and the resulting feature maps
are sent in parallel 1) to the new head for training and 2)
to the previous head for KD, thus forming a Y-shaped ar-
chitecture (see Fig. 1) that we name the Y-knowledge distil-
lation (Y-KD ). As the feature extractor of the teacher net-
work is constantly updated, the student network benefits
from more plasticity. This increased plasticity allows the
growth of new feature extractor modules that are specialized
on novel classes.
During inference, the specialized modules are used with a
unique instance segmentation head. Thus, by growing task-
specific feature extraction branches to accommodate new
categories, our model is able to learn new classes more ef-
ficiently, and by using specialized modules whose weights
are frozen during incremental steps, forgetting of previ-
ous classes is significantly reduced. Notably, our results on
Pascal-VOC (Everingham et al. 2009) and our ablation study
show that the components of the Dynamic Y-KD network en-
hance forward transfer (Menezes et al. 2023).
Moreover, if we measure performances of CL methods
by their mean average precision (mAP) ratio with a non-CL
equivalent (i.e. joint-training) (Menezes et al. 2023), our ap-
proach obtains, on old classes, 97.8%and89.0%compared
to94.7%and83.7%obtained by MMA (Cermelli et al.
2022) on 19-1 and15-1, respectively. On new classes in 15-
5and10-2, our approach obtains 86.2%and83.1%of the
joint-training mAP, compared to 78.9%and77.5%obtained
with MMA, respectively.
Finally, inspired by our preliminary study that highlights
the compatibility of incremented heads with previous fea-
ture extractors, we repurpose the use of checkpoint averag-
ing (Huang et al. 2017; Gao et al. 2022) to provide control
over the performance trade-off on different sets of classes
in CL. Our results show that we can thereby easily adjust
the model to either perform better on some sets of classes or
others. This offers a simple control mechanism and can be
a useful tool in the development of real-world applicationswhere some classes are more important than others.
In summary, our contributions are as follows:
• We highlight two intriguing properties of Mask R-CNN
regarding 1) the stability of feature extractors, and 2) the
compatibility of instance segmentation heads with previ-
ous feature extractors. To our knowledge, we are the first
to make these observations.
• We exploit these two observations to propose 1) the Y-
KD, a new KD strategy that increases plasticity by using
a shared feature extractor, and 2) a dynamic architecture
that develops new task-specific feature extractors that are
used with a common head at inference.
• Our Dynamic Y-KD network significantly outperforms
previous methods on various incremental scenarios of
Pascal-VOC both on new and old classes. Furthermore,
we isolate the contributions of each component in an ab-
lation study.
• We propose checkpoint averaging, a zero-cost mecha-
nism to control the trade-off between performances on
old, intermediary and new classes after training.
Related Work
Instance Segmentation
Instance segmentation is an important problem in computer
vision that aims to produce a unique segmentation mask
of objects that belong to a predefined set of classes. One
of the most widely adopted approaches is the “detect then
segment” strategy, which has been popularized by Mask R-
CNN (He et al. 2017). Recent work on instance segmenta-
tion has explored alternative approaches such as one-stage
methods (Bolya et al. 2019; Wang et al. 2020), and more
complex techniques (Chen et al. 2019; Fang et al. 2021;
Cheng et al. 2022). However, few work addressed catas-
trophic forgetting when these methods face CL situations.
In this paper, we build upon Mask R-CNN as we propose
a dynamic architecture that grows new modules of special-
ized feature extraction before the RPN to address the limi-
tations of existing methods and improve the performance of
instance segmentation in CL scenarios.
Continual Learning
CL studies solutions to enable the incrementation of mod-
els with novel classes without losing previously acquired
knowledge. The main families of CL strategies are generally
categorized into 1) replay-based (Rebuffi et al. 2017; Mara-
cani et al. 2021; Shieh et al. 2020; Maracani et al. 2021; Ver-
wimp, De Lange, and Tuytelaars 2021), 2) regularization-
based (Cermelli et al. 2020, 2022; Liu et al. 2020; Kirk-
patrick et al. 2017) and 3) dynamic architecture-based meth-
ods, also called parameter isolation-based (Rusu et al. 2016;
Aljundi, Chakravarty, and Tuytelaars 2017; Li et al. 2018;
Zhang et al. 2021; Douillard et al. 2022). In the follow-
ing sections, we focus on regularization-based and dynamic
architecture-based methods since we propose a hybrid strat-
egy between these two approaches to build our Dynamic Y-
KD network .

--- PAGE 3 ---
Regularization-based Methods. Since catastrophic for-
getting results from a drift in the model’s parameters,
this can be mitigated by applying specific regularization
losses. One of the most widely used regularization-based ap-
proaches is knowledge distillation (KD) (Hinton, Vinyals,
and Dean 2015), which leverages the outputs of a previous
model to guide the new model in producing similar activa-
tions for previous categories.
As examples, ILOD (Shmelkov, Schmid, and Alahari
2017) applied a L2loss on the predicted logits of old
classes and bounding boxes to prevent the new model from
overly shifting its outputs towards new classes. In Faster
ILOD (Peng, Zhao, and Lovell 2020), an additional distil-
lation term is applied on the features of the RPN of Faster-
RCNN (Girshick 2015) for more stability. One of the first
work on CIS has been proposed in (Gu, Deng, and Wei
2021), in which KD is performed by two teacher networks to
increment YOLACT (Bolya et al. 2019). In MiB (Cermelli
et al. 2020), the authors adapted the KD and cross-entropy
losses to account for the background shift in continual se-
mantic segmentation. In MMA (Cermelli et al. 2022), the
authors then extended these ideas to the tasks of continual
object detection and CIS with Faster R-CNN and Mask R-
CNN respectively.
In this work, we also leverage KD losses with Mask
R-CNN. However, contrarily to previous work where the
teacher network is completely frozen, our method differs as
the feature extractor used for KD is shared with the learn-
ing model, and is therefore continuously updated during the
learning process. This approach enhances the model’s plas-
ticity and forward transfer capabilities, as evidenced by our
improved results on new classes and our ablation study (see
Table 4 lines 2 vs 5 ).
Dynamic Architecture-based Methods. These methods,
also named parameter isolation, freeze some parts of the
network (Li et al. 2018) and grow new branches to learn
new tasks (Zhang et al. 2021). One of the drawbacks of this
strategy is that it generally increases the memory footprint
at each step. Some work such as (Zhang et al. 2021) adopt
model pruning to reduce the number of weights while limit-
ing performance loss. In our work, we reduce model growth
by showing empirically that a unique instance segmentation
head can be used with small specialized feature extractors.
Different strategies such as regularization and dynamic ar-
chitectures each have their pros and cons. Our hybrid ap-
proach seeks to combine the strengths of both while mitigat-
ing their drawbacks. We thereby differ from previous work
as we combine KD during training with a dynamic architec-
ture approach to improve learning of new classes and reduce
forgetting.
Checkpoint Averaging
Averaging the weights from checkpoints saved at different
epochs has been shown to improve generalization by acting
similarly to ensemble methods (Huang et al. 2017; Vaswani
et al. 2017; Gao et al. 2022). In this work, we first show
that this simple trick can also be leveraged in CL by aver-
aging the weights between the instance segmentation headstrained after any incremental step iandjto reduce forget-
ting of classes C0:iwhile preserving similar or slightly infe-
rior results on new classes Cj. This offers a new mechanism
to manually control the trade-off between performances on
old and new classes without requiring retraining or incurring
any additional cost.
Continual Instance Segmentation
Problem Formulation
In CIS, we aim to increment a model fθt−1, parameterized
byθt−1, to a model fθtthat can detect and segment in-
stances of new classes Ctas well as old classes C0:t−1. At
each step twe are given a training dataset Dtcomposed
of images Xtand ground-truth annotations Ytthat indi-
cate the bounding boxes, segmentation masks and seman-
tic classes. Following the experimental setup established in
previous work (Cermelli et al. 2022), we consider that the
annotations Ytare only available for current classes Ct,
whereas objects of previous categories appearing in Dtare
unlabelled.
Mask R-CNN for CIS
In the context of CIS, Mask R-CNN (He et al. 2017) is made
of a feature extractor Fθtparameterized by θtat each step t,
a region proposal network (RPN) that proposes regions of
interests (RoIs), and two parallel heads: 1) a box head for
classification and regression of bounding boxes coordinates
of each RoI, and 2) a segmentation head for the segmen-
tation of each RoI. For simplicity, we summarize Mask R-
CNN in three modules: 1) a backbone Bthat is frozen during
all steps, 2) a set of task-specific modules of feature extrac-
tion defined by {Fθi}t
i=0that learn class-specific features
from the outputs of B, and 3) a head Hθtthat comprises the
RPN, the box head and the segmentation head (see Fig. 1).
Knowledge Distillation. One of the main challenges of
CL is to preserve past knowledge while learning new
classes. Previous work (Shmelkov, Schmid, and Alahari
2017; Peng, Zhao, and Lovell 2020; Cermelli et al. 2022)
showed the benefits of knowledge distillation (KD) to pre-
vent the new network from significantly diverging while
learning new classes. Generally, the KD loss has the follow-
ing form:
Lkd=−1
R·CRX
i=1X
c∈C1:t−1ˆYt−1
i,clogˆYt
i,c, (1)
where ˆYt
i,cis the score for class cgiven by the model fθt
for the i-th output. In the context of RoI classification, this
KD loss would encourage the new model to produce similar
scores of past classes for each of the NRoIs, i.e. R=N.
On the other hand, since segmentation is a pixel-wise clas-
sification, the number of outputs is then R=NHW ,
where HandWis the height and width of the segmenta-
tion masks, respectively.
As highlighted in prior work (Cermelli et al. 2020, 2022),
the conventional KD loss overlooks the background shift,
wherein new classes were previously learned as background

--- PAGE 4 ---
(a) CKA betwen feature maps given by the backbone
att= 0andt= 5.
(b) mAP@0.5 obtained with the new and old feature ex-
tactors, given the new instance segmentation head.
Figure 2: Preliminary experiments using Mask R-CNN with KD losses similar to (Cermelli et al. 2022) in a 15-1 scenario on Pascal-VOC.
by the model. To address this, the KD loss should be adapted
to incorporate the scores of these new classes into the back-
ground class before proceeding with distillation. The unbi-
ased KD loss (Cermelli et al. 2022) thus becomes:
Lunkd =−1
R·CRX
i=1h
ˆYt−1
i,bglog(ˆYt
bg+X
c∈CtˆYt
i,c)+
X
c′∈C0:t−1\bgˆYt−1
i,c′logˆYt
i,c′i
. (2)
In this way, when the previous model gives high scores
for the background class, the new model is encouraged to
predict either background or any of the new classes, which
is the desired behaviour.
Dynamic Y-KD: a Hybrid Approach
In this section, we formulate our proposed Dynamic Y-KD
network . We begin by summarizing key observations that
were made in preliminary experiments using Mask R-CNN
with standard knowledge distillation (KD) losses. From
these observations, we motivate our Y-KD and dynamic ar-
chitecture strategies. We then proceed with a formulation
of our hybrid method that synergistically leverages both
techniques. Finally, we introduce checkpoint averaging as
a mechanism to control the performance of CL models.
Motivation
Stability of the Feature Extractor (FE). In preliminary
experiments on CIS using Mask R-CNN with standard KD
losses, we noticed that the FE remains very stable even after
several incremental steps. More specifically, we compared
the representations produced by the base FE with the rep-
resentations of the new FE that has been incremented with
five novel classes after five incremental steps ( 15-1). We
show in Figure 2a the Centered Kernel Alignment (CKA)
scores (Kornblith et al. 2019) of each class separately.
Surprisingly, we found that the CKA scores were very
high (i.e. >0.94), even for classes that have not been seen
by the base model. This shows that the FE is only slightlyfine-tuned to learn task-specific features during incremental
steps.
Compatibility of the head with previous FEs. Then, we
hypothesized that if the FE is stable, it should be possible to
reuse the old FE with the new instance segmentation head.
We compare in Figure 2b the mAP@0.5 of a model that uses
either the new or the base FE with the same new head for in-
ference. Interestingly, the base FE with the new head obtains
better results on the old classes, showing the compatibility
of the incremented head with a previous iteration of the FE.
This highlights the compositionality of Mask R-CNN in CL:
modules from different incremental steps are still compat-
ible and can be effectively combined to give models with
different properties. For instance, Figure 2b shows that us-
ing the FE from t= 0 with the head at t= 5 produces a
model that is better on base classes but worst on new ones.
This motivates the idea of developing task-specific FEs to
preserve discriminative features of each set of classes.
Our Model
Y-KD : Training Specialized Modules with a Generic
Head. The stability of the FE suggests two aspects: 1) al-
lowing the FE more plasticity may lead to improved results
on new classes, and 2) it might not be necessary to freeze the
teacher FE during KD if the FE is already stable.
We have explored these two hypotheses by proposing a
KD strategy that aims to develop specialized FEs to better
represent new classes while allowing the teacher FE to be
updated. This is accomplished by using a common feature
extractor Ft
θwhich is connected in parallel to the previous
headHt−1and the new head Ht, thus forming a Y-shaped
architecture (see Fig. 1) that we name the Y-knowledge dis-
tillation (Y-KD ).
In most previous works, a frozen copy of the whole
teacher network is kept and used during training to distill its
outputs to the student network. In our approach, Y-KD con-
sists of sharing the same trainable feature extractor between
the teacher and student networks to increase plasticity dur-
ing incremental learning. Y-KD is thus performed by pass-
ing the images in the shared backbone and feature extractor,

--- PAGE 5 ---
which gives the feature maps ˆXtas follows:
ˆXt=Ft
θ(B(Xt)). (3)
The feature maps ˆXtare then sent to the teacher and student
heads separately to produce their respective outputs:
ˆYt−1=Ht−1(ˆXt),
ˆYt=Ht(ˆXt), (4)
where ˆY:= (p, r, s, ω, m )which are respectively the class
logits p, the regression scores rof box coordinates, the ob-
jectness score sand box coordinates ωgiven by the RPN,
and the segmentation mask m. KD is then performed be-
tween the outputs of the teacher and student heads:
Lunkd(ˆYt−1,ˆYt) =λ1Lbox
unkd(pt−1, pt, rt−1, rt)+
λ2LRPN
kd(st−1, st, ωt−1, ωt)+
λ3Lmask
kd(mt−1, mt), (5)
whereLbox
unkd ,LRPN
kd andLmask
kd are distillation losses (Cer-
melli et al. 2022) applied on the box head, the RPN and the
mask head, respectively.
The total loss is then the following:
L=Lmask(ˆYt, Yt) +Lunkd(ˆYt−1,ˆYt), (6)
where Lmask is the supervised loss to train Mask R-CNN.
For more details on the specific implementation of these
losses, we refer the reader to the supplementary material.
With these distillation losses and by using a shared FE, the
behaviour of the teacher network is made dynamic since its
FE is also trained on novel images, but it still encourages the
student head to preserve previous knowledge. This increases
plasticity by allowing the student network to better learn the
new classes while keeping the ability of the head to detect
and segment previous categories.
Dynamic Architecture. The second observation of our
preliminary experiments, which highlighted the compatibil-
ity of the head with previous FEs, suggests that using task-
specific FEs with a unique head would be a promising op-
tion for CIS. On the one hand, isolating parameters of task-
specific FEs would reduce forgetting (as shown in Fig. 2b),
and since a unique head would be used for inference, the
growth in parameters would be minimal.
Therefore, we now propose our dynamic architecture-
based method. At inference, we plug all specialized FEs to
the same backbone and instance segmentation head in the
following way (see Fig. 1). The backbone Bextracts gen-
eral features from the input image X, and these features
are sent to the task-specific modules F0, F1, ..., Ftin paral-
lel to produce their corresponding feature maps. These fea-
ture maps are then given to the most recent head Htfor
instance segmentation to produce their corresponding pre-
dictions ˆY0,ˆY1, ...,ˆYt. All predictions are then merged by
only keeping the outputs that correspond to the domain of
expertise of each sub-network as follows:
ˆYt= [ˆYi
c∈Ci],∀i= 0, ..., t (7)
This filtering and merging step is necessary because we
use a common generic head that can segment all classes from
any feature maps.Memory and Computational Costs. A common draw-
back of dynamic architecture-based strategies is that they
generally increase the memory and computational costs as
the model grows (Lesort et al. 2020). Our approach does not
make exception, as it linearly increases these costs by adding
a specialized module for each task. However during training,
we exclusively use the previous and new heads Ht−1andHt
with a shared backbone to perform our Y-KD . Heads from
earlier steps are discarded and previous specialized modules
are not used during training (see Fig. 1 bottom-left), such
that the memory and computational costs are constant.
Furthermore, since a large part of the backbone is frozen,
the number of weights added at each incremental step by the
growth in task-specific FEs only accounts for 8.2Mparame-
ters, which represents8.2M
35.3M= 23.3%of the original model
when using ResNet-50. Future work should address this lim-
itation, e.g., with pruning or quantization methods (Zhang
et al. 2021). In this paper, we focused on developing the first
dynamic architecture for CIS.
Checkpoint Averaging to Mitigate Forgetting
In CL, the trade-off between performances on previous or
new classes can only be indirectly controlled by choos-
ing hyper-parameters before training (Lesort et al. 2020;
De Lange et al. 2021). However, this is a tedious approach as
it requires retraining the model with different combinations
until a satisfactory trade-off is reached.
To alleviate this problem, we propose a simple tool to
manually control the trade-off between performances on old
and new classes by leveraging checkpoint averaging (Huang
et al. 2017; Gao et al. 2022). We can average the weights
of heads that have been obtained after different incremen-
tal tasks to improve the ability of the model to segment in-
stances of previous sets of classes. Given the parameters θi
andθjof heads HiandHjthat have learned classes C0:i
andC0:jrespectively, with i < j , we can create a new head
Ht
θmthat mixes their parameters as follows:
Ht
θm:=wiθi+wjθj, (8)
where wi, wj∈[0,1]are factors to balance the contri-
bution of each set of parameters. By doing so, we can re-
cover performances on classes C0:iif forgetting is judged
to be substantial. In return, a small drop in performance on
classes Cjshould be expected. Nonetheless, this offers a
simple zero-cost mechanism to gain control over forgetting,
which can be a useful tool to define a performance balance,
for instance when some classes are more critical then others.
Experiments
Experimental Setup
Following previous work on CIS (Cermelli et al. 2022;
Zhang et al. 2021), we opted to assess our approach us-
ing diverse continual learning scenarios derived from the
Pascal-VOC dataset (Everingham et al. 2009). Given the in-
creased complexity presented by the class-incremental sce-
nario compared to conventional setups and the current state
of continual instance segmentation methods, i.e. (Zhang
et al. 2021; Cermelli et al. 2022), Pascal-VOC provides

--- PAGE 6 ---
19-1 15-5 10-10
Method 1-19 20 1-20 1-15 16-20 1-20 1-10 11-20 1-20
Fine-tuning 3.9 45.1 5.9 3.3 31.6 10.3 2.7 32.3 17.5
ILOD 36.5 38.9 36.6 37.2 31.1 35.7 36.1 25.9 31.0
Faster ILOD 36.9 37.1 36.9 37.7 30.7 35.9 37.0 25.8 31.4
MMA 37.2 38.1 37.3 37.1 31.4 35.7 37.2 28.7 33.0
Ours 38.5 45.7 38.8 37.3 34.3 36.5 37.4 29.8 33.6
Joint Training 39.3 50.0 39.9 39.9 39.8 39.9 40.0 39.7 39.9
Table 1: mAP@(0.5, 0.95)% results of single-step incremental instance segmentation on Pascal-VOC 2012.
15-1 10-2 10-5
Method 1-15 16-19 20 1-20 1-10 11-18 19-20 1-20 1-10 11-15 16-20 1-20
Fine-tuning - - - - 0.5 0.4 40.1 4.4 1.2 0.5 31.4 8.6
ILOD 30.9 19.7 39.9 29.1 30.5 17.6 39.6 26.2 35.9 25.8 29.2 31.7
Faster ILOD 32.3 19.7 35.8 30.0 30.5 17.8 38.5 26.2 36.1 26.1 29.1 31.9
MMA 33.4 21.2 35.0 31.1 32.3 21.1 41.4 28.8 35.7 28.0 31.6 32.7
Ours 35.5 19.7 43.9 32.8 33.5 20.0 44.4 29.2 35.7 25.2 33.4 32.5
Joint Training 39.9 37.2 50.0 39.9 40.0 36.3 53.4 39.9 40.0 39.7 39.8 39.9
Table 2: mAP@(0.5, 0.95)% results of multi-step incremental instance segmentation on Pascal-VOC 2012.
a more manageable benchmark than complex datasets that
pose substantial challenges even in standard, non-continual
learning contexts.
Pascal-VOC is composed of 20semantic classes, which
we divide in distinct sets to simulate incremental learning
scenarios. Each scenario is defined as N-k, where Nis the
number of base classes in the first step, and kis the number
of classes added in the following incremental steps to reach
the total of 20classes.
Metrics. We evaluate the performance of the models using
the mean average precision (mAP), averaged over 10 thresh-
olds ranging from 0.5 to 0.95, i.e., mAP@ {0.5:0.95 }. More
specifically, we separately report 1) the mAP for base classes
to show the ability to preserve past knowledge (i.e. stability);
2) the mAP for new classes to evaluate the capacity to be in-
cremented with new categories (i.e. plasticity); 3) the mAP
on all classes to show the global performance; and 4) for
multi-steps incremental learning, we also report the mAP of
intermediary classes (e.g. classes 16-19 in the 15-1 scenario)
separately since results on them are influenced both by plas-
ticity and stability. In our analyses of the results, we also
use the ratio between the mAP of a given CL method and
its non-CL equivalent (i.e. the joint training method) to give
an idea of the level of performance that CL methods can
achieve compared to a non-CL upper-bound.
Baselines. Since only very few methods have been pro-
posed for CIS, we compare our approach with MMA (Cer-
melli et al. 2022) as well as adaptations of ILOD (Shmelkov,
Schmid, and Alahari 2017) and Faster ILOD (Peng, Zhao,
and Lovell 2020) that have been presented in (Cermelli et al.
2022). We also consider lower and upper bounds, repre-
sented by a basic fine-tuning approach that does not incorpo-
rate any CL mechanism, and joint-training that trains on all
classes simultaneously. We ran all experiments by extending
the framework implemented by (Cermelli et al. 2022).Results
Single-step Incremental Learning. The results for
single-step incremental learning scenarios are shown in Ta-
ble 1. We can see that that the increased plasticity of fine-
tuning, due to the absence of regularization losses, allows to
obtain better results than most other methods on new classes.
However, despite its superior plasticity, fine-tuning is far
from achieving the same results than joint training on the
new classes.
On the other hand, our approach obtains significantly
higher results on new classes while preserving similar or
better mAP on base classes. On new classes, our approach
obtains +7.6%in19-1,+2.9%in15-5 and+1.1%in10-10
compared to MMA. Interestingly, our approach even out-
performs fine-tuning on new classes in two of the three sce-
narios. This is especially the case in 15-5 where we obtain
34.3%on classes 16-20 whereas fine-tuning, the second best
approach, obtains 31.6%. This shows the ability of our Y-
KDstrategy to enhance forward transfer by training feature
extractors that are specialized on new classes. This contri-
bution of Y-KD is also highlighted by our ablation study be-
low (i.e. compare lines 2-5 in Table 4)
In addition to giving better mAP on new classes, our ap-
proach also reduces forgetting compared to other methods,
bringing the mAP on all classes ( 1-20) closer to the ones
of joint training in all three scenarios. Notably, compared to
joint training, our method obtains mAP ratios of38.8%
39.9%=
97.2%on classes 1-20 in the 19-1 scenario,36.5%
39.9%= 91.5%
in15-5, and33.6%
39.9%= 84.2%in10-10 .
Multi-steps Incremental Learning. We now show the re-
sults for multi-steps incremental learning scenarios in Ta-
ble 2. We can observe that our approach performs well in
these more complicated situations. Our method stands out
even more on base classes in the 15-1 and10-2 scenarios,

--- PAGE 7 ---
Task w4 w5Base Int. New All
15-10 1 35.5 19.7 43.9 32.8
0.25 0.75 35.4 23.9 42.7 33.5
0.5 0.5 35.6 22.7 39.7 33.2
10-20 1 33.5 20.0 44.4 29.2
0.25 0.75 33.8 20.2 42.9 29.3
0.5 0.5 34.2 20.4 40.0 29.2
Table 3: mAP@0.5-0.95% results for checkpoint averaging using
different weights w4andw5.
confirming the compatibility of previous FEs with an in-
cremented head. Indeed, we outperform MMA by +2.1%
and+1.2%on base classes in these scenarios, respectively.
In10-5, all approaches including ours obtain similar mAP
ranging from 35.7−36.1%. On the last classes (e.g. classes
16-20 in the 10-5 scenario), our approach strongly out-
performs previous work, as it obtains +4.0%,+3.0%and
+1.8%compared to the second best approaches in 15-1,10-
2and10-5, respectively.
With respect to intermediary classes, the heightened plas-
ticity in our method seems to come with a trade-off: it makes
recently acquired knowledge more prone to forgetting. How-
ever, our method consistently outperforms others when eval-
uating performance across all classes ( 1-20). For instance,
even if our method obtains slightly inferior results on classes
16-19 and11-18 in the 15-1 and10-2 scenarios respectively,
theDynamic Y-KD network obtains a better average on all
classes ( 1-20), outperforming MMA by +1.7%and+0.4%.
We now discuss how the checkpoint averaging trick can fur-
ther address the limitation of our approach regarding inter-
mediary classes.
Checkpoint Averaging. To ensure fairness, we did not use
this trick while comparing methods in Tables 1-2. We now
show how our last contribution can be a viable tool to man-
age the compromise on different sets of classes, mitigat-
ing the drawback of our approach on intermediary classes.
Specifically, we average the weights of the heads obtained
after the fourth and fifth incremental learning steps accord-
ing to Equation 8. The parameters θmof the new head used
at inference thus becomes an average between the parame-
ters of the heads Hθ4andHθ5, weighted by w4andw5.
In Table 3, we show the results on base, intermediary
and new classes in 15-1 and10-2 scenarios by varying the
weights w4andw5. We can see that although a small drop is
observed on new classes, fusing the weights from the fourth
incremental step allows to recover performances on inter-
mediary and base classes. For instance, in 15-1, the decrease
of mAP on new classes from 43.9%to42.7%is compen-
sated by an increase of +4.2%on intermediary classes (i.e.
16-19 ) with w4= 0.25, which now outperforms previous
methods (see Table 2). Similarly, in 10-2, a slightly bet-
ter mAP on all classes of 29.3%can be obtained by us-
ing(w4= 0.25, w5= 0.75)as forgetting of base and
intermediary classes is reduced by fusing past knowledge.
Thereby, our proposed checkpoint averaging trick allows to
manually create a new model that exhibits different perfor-
mances on the various sets of classes without requiring anyY-KD KD FE0FE11-15 16-20 1-20
1 21.7 52.9 29.5
2 67.2 53.1 63.7
3 60.8 57.8 60.0
4 67.4 37.2 59.8
5 67.4 57.8 65.0
Table 4: mAP@0.5% results of the ablation study in the 15-5 sce-
nario.
training or additional computational costs.
Ablation Study. Finally, we perform an ablation study to
highlight the importance of the two aspects of our Dynamic
Y-KD network , namely that 1) Y-KD using a shared FE dur-
ing training improves results on new classes and 2) using a
dynamic architecture reduces forgetting.
The results of the ablation study in a 15-5 scenario are
shown in Table 4. From line 1 , we can see that a purely
architectural-based strategy that grows new modules to ac-
commodate new tasks does not work for CIS, as catastrophic
forgetting still happens. Without KD, FE0cannot remain
compatible with the incremented head, such that it performs
poorly on previous classes. While standard KD ( line 2 ) of-
fers reasonable performances on old and new classes, we
can see that new classes can be better learned using our Y-
KDstrategy ( line 3 ). However, the increased plasticity from
using a shared backbone in our Y-KD strategy comes at a
cost of decreased stability, as shown by the fact that the
mAP@0.5 drops to 60.8%on classes 1-15. Better results on
these previous classes can be obtained using FE0(line 4 ), as
the mAP@0.5 rises to 67.4%. But since FE0has not learned
task-specific features of classes 16-20 , it cannot perform as
well on new classes. Therefore, the best of both worlds is
obtained by using both FE0and FE1(line 5 ), which corre-
sponds to our Dynamic Y-KD network , as it performs better
on new classes (57.8% vs53.1%) and even slightly better
on previous classes than standard KD ( 67.4% vs67.2%).
Conclusion
In preliminary experiments on continual instance segmen-
tation using Mask R-CNN with knowledge distillation, we
made two observations regarding the stability of feature
extractors and the compatibility of instance segmentation
heads with previous backbones. We leveraged these two ob-
servations by proposing the Y-KD and the use of a dynamic
architecture to form the Dynamic Y-KD network . Our ap-
proach increases plasticity and allows to train feature ex-
tractors that are specialized on new classes, while preserv-
ing a generic head that is compatible with all previous task-
specific feature extractors for better stability.
Our results on several single-step and multi-steps incre-
mental learning scenarios showed that our approach reduces
forgetting of previous classes as well as improving mAP on
new classes, thus outperforming previous methods in most
setups. Additionally, we proposed a zero-cost trick based on
checkpoint averaging to manually adjust the trade-off be-
tween the performances on the various sets of classes.

--- PAGE 8 ---
References
Aljundi, R.; Chakravarty, P.; and Tuytelaars, T. 2017. Expert
gate: Lifelong learning with a network of experts. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 3366–3375.
Bolya, D.; Zhou, C.; Xiao, F.; and Lee, Y . J. 2019. Yolact:
Real-time instance segmentation. In Proceedings of the
IEEE/CVF international conference on computer vision ,
9157–9166.
Cermelli, F.; Geraci, A.; Fontanel, D.; and Caputo, B. 2022.
Modeling Missing Annotations for Incremental Learning in
Object Detection. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 3700–
3710.
Cermelli, F.; Mancini, M.; Bulo, S. R.; Ricci, E.; and Caputo,
B. 2020. Modeling the background for incremental learning
in semantic segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
9233–9242.
Chen, K.; Pang, J.; Wang, J.; Xiong, Y .; Li, X.; Sun, S.;
Feng, W.; Liu, Z.; Shi, J.; Ouyang, W.; et al. 2019. Hy-
brid task cascade for instance segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , 4974–4983.
Cheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Gird-
har, R. 2022. Masked-attention mask transformer for univer-
sal image segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
1290–1299.
De Lange, M.; Aljundi, R.; Masana, M.; Parisot, S.; Jia, X.;
Leonardis, A.; Slabaugh, G.; and Tuytelaars, T. 2021. A con-
tinual learning survey: Defying forgetting in classification
tasks. IEEE transactions on pattern analysis and machine
intelligence , 44(7): 3366–3385.
Douillard, A.; Chen, Y .; Dapogny, A.; and Cord, M. 2021.
Plop: Learning without forgetting for continual semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 4040–4050.
Douillard, A.; Ram ´e, A.; Couairon, G.; and Cord, M. 2022.
Dytox: Transformers for continual learning with dynamic
token expansion. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 9285–
9295.
Everingham, M.; Van Gool, L.; Williams, C. K.; Winn, J.;
and Zisserman, A. 2009. The pascal visual object classes
(voc) challenge. International journal of computer vision ,
88: 303–308.
Fang, Y .; Yang, S.; Wang, X.; Li, Y .; Fang, C.; Shan, Y .;
Feng, B.; and Liu, W. 2021. Instances as queries. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , 6910–6919.
Gao, Y .; Herold, C.; Yang, Z.; and Ney, H. 2022. Revisiting
Checkpoint Averaging for Neural Machine Translation. In
Findings of the Association for Computational Linguistics:
AACL-IJCNLP 2022 , 188–196.
Girshick, R. 2015. Fast r-cnn. In Proceedings of the IEEE
international conference on computer vision , 1440–1448.Grossberg, S. 1982. Studies of mind and brain: Neural prin-
ciples of learning, perception, development, cognition, and
motor control. Boston studies in the philosophy of science .
Gu, W.; Bai, S.; and Kong, L. 2022. A review on 2D instance
segmentation based on deep neural networks. Image and
Vision Computing , 104401.
Gu, Y .; Deng, C.; and Wei, K. 2021. Class-incremental in-
stance segmentation via multi-teacher networks. In Proceed-
ings of the AAAI Conference on Artificial Intelligence , vol-
ume 35, 1478–1486.
He, K.; Gkioxari, G.; Doll ´ar, P.; and Girshick, R. 2017. Mask
r-cnn. In Proceedings of the IEEE international conference
on computer vision , 2961–2969.
Hinton, G.; Vinyals, O.; and Dean, J. 2015. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 .
Huang, G.; Li, Y .; Pleiss, G.; Liu, Z.; Hopcroft, J. E.; and
Weinberger, K. Q. 2017. Snapshot ensembles: Train 1, get
m for free. arXiv preprint arXiv:1704.00109 .
Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-
jardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.;
Grabska-Barwinska, A.; et al. 2017. Overcoming catas-
trophic forgetting in neural networks. Proceedings of the
national academy of sciences , 114(13): 3521–3526.
Kornblith, S.; Norouzi, M.; Lee, H.; and Hinton, G. 2019.
Similarity of neural network representations revisited. In In-
ternational Conference on Machine Learning , 3519–3529.
PMLR.
Lesort, T.; Lomonaco, V .; Stoian, A.; Maltoni, D.; Filliat,
D.; and D ´ıaz-Rodr ´ıguez, N. 2020. Continual learning for
robotics: Definition, framework, learning strategies, oppor-
tunities and challenges. Information fusion , 58: 52–68.
Li, W.; Wu, Q.; Xu, L.; and Shang, C. 2018. Incremental
learning of single-stage detectors with mining memory neu-
rons. In 2018 IEEE 4th International Conference on Com-
puter and Communications (ICCC) , 1981–1985. IEEE.
Liu, L.; Kuang, Z.; Chen, Y .; Xue, J.-H.; Yang, W.; and
Zhang, W. 2020. Incdet: In defense of elastic weight consol-
idation for incremental object detection. IEEE transactions
on neural networks and learning systems , 32(6): 2306–2319.
Maracani, A.; Michieli, U.; Toldo, M.; and Zanuttigh, P.
2021. Recall: Replay-based continual learning in seman-
tic segmentation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 7026–7035.
McCloskey, M.; and Cohen, N. J. 1989. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. In Psychology of learning and motivation , vol-
ume 24, 109–165. Elsevier.
Menezes, A. G.; de Moura, G.; Alves, C.; and de Carvalho,
A. C. 2023. Continual object detection: a review of defini-
tions, strategies, and challenges. Neural Networks .
Peng, C.; Zhao, K.; and Lovell, B. C. 2020. Faster ilod: In-
cremental learning for object detectors based on faster rcnn.
Pattern recognition letters , 140: 109–115.

--- PAGE 9 ---
Rebuffi, S.-A.; Kolesnikov, A.; Sperl, G.; and Lampert, C. H.
2017. icarl: Incremental classifier and representation learn-
ing. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition , 2001–2010.
Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.;
Kirkpatrick, J.; Kavukcuoglu, K.; Pascanu, R.; and Had-
sell, R. 2016. Progressive neural networks. arXiv preprint
arXiv:1606.04671 .
Shieh, J.-L.; Haq, Q. M. u.; Haq, M. A.; Karam, S.; Chondro,
P.; Gao, D.-Q.; and Ruan, S.-J. 2020. Continual learning
strategy in one-stage object detection framework based on
experience replay for autonomous driving vehicle. Sensors ,
20(23): 6777.
Shmelkov, K.; Schmid, C.; and Alahari, K. 2017. Incremen-
tal learning of object detectors without catastrophic forget-
ting. In Proceedings of the IEEE international conference
on computer vision , 3400–3409.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems , 30.
Verwimp, E.; De Lange, M.; and Tuytelaars, T. 2021. Re-
hearsal revealed: The limits and merits of revisiting samples
in continual learning. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , 9385–9394.
Wang, J.; Wang, X.; Shang-Guan, Y .; and Gupta, A. 2021.
Wanderlust: Online continual object detection in the real
world. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , 10829–10838.
Wang, X.; Zhang, R.; Kong, T.; Li, L.; and Shen, C.
2020. Solov2: Dynamic and fast instance segmentation.
Advances in Neural information processing systems , 33:
17721–17732.
Wu, G.; Gong, S.; and Li, P. 2021. Striking a balance be-
tween stability and plasticity for class-incremental learning.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , 1124–1133.
Zhang, N.; Sun, Z.; Zhang, K.; and Xiao, L. 2021. Incre-
mental learning of object detection with output merging of
compact expert detectors. In 2021 4th International Con-
ference on Intelligent Autonomous Systems (ICoIAS) , 1–7.
IEEE.
