# BiRT: Phát Lại Sinh Học Cảm Hứng trong Vision Transformers cho Học Liên Tục

Kishaan Jeeveswaran1 Prashant Bhat1 2 Bahram Zonooz* 1 2 Elahe Arani* 1 2

Tóm tắt
Khả năng của mạng nơ-ron sâu để liên tục học và thích ứng với một chuỗi các nhiệm vụ vẫn là thách thức do quên lãng thảm khốc các nhiệm vụ đã học trước đó. Mặt khác, con người có khả năng đáng chú ý để thu thập, tiếp thu và chuyển giao kiến thức qua các nhiệm vụ trong suốt cuộc đời mà không quên lãng thảm khốc. Tính linh hoạt của não bộ có thể được quy cho việc diễn tập các trải nghiệm trừu tượng thông qua hệ thống học tập bổ sung. Tuy nhiên, diễn tập biểu diễn trong vision transformers thiếu tính đa dạng, dẫn đến overfitting và do đó, hiệu suất giảm đáng kể so với diễn tập hình ảnh thô. Vì vậy, chúng tôi đề xuất BiRT, một phương pháp học liên tục dựa trên diễn tập biểu diễn mới sử dụng vision transformers. Cụ thể, chúng tôi giới thiệu nhiễu xây dựng ở các giai đoạn khác nhau của vision transformer và thực thi tính nhất quán trong dự đoán đối với trung bình di động theo hàm mũ của mô hình hoạt động. Phương pháp của chúng tôi cung cấp cải thiện hiệu suất nhất quán so với diễn tập hình ảnh thô và diễn tập biểu diễn vanilla trên nhiều benchmark CL thử thách, đồng thời hiệu quả về bộ nhớ và mạnh mẽ trước các hỏng hóc tự nhiên và adversarial.

1. Giới thiệu
Các hệ thống tính toán hoạt động trong thế giới thực thường tiếp xúc với một chuỗi nhiều nhiệm vụ với các luồng dữ liệu không ổn định. Tương tự như các sinh vật, điều mong muốn là các hệ thống nhân tạo này có thể học liên tục để hành động và thích ứng thành công với các tình huống mới trong thế giới thực. Tuy nhiên, mạng nơ-ron sâu được thiết kế vốn để huấn luyện trên dữ liệu tĩnh, độc lập và phân phối giống hệt nhau (i.i.d.). Bản chất tuần tự của học liên tục (CL) vi phạm giả định mạnh này, dẫn đến quên lãng thảm khốc các nhiệm vụ cũ. Quên lãng thảm khốc thường dẫn đến giảm nhanh hiệu suất của các nhiệm vụ cũ và trong trường hợp tồi tệ nhất, thông tin đã thu thập trước đó bị ghi đè hoàn toàn bởi thông tin mới (Parisi et al., 2019).

Các phương pháp dựa trên diễn tập, lưu trữ và phát lại các mẫu nhiệm vụ trước đó, đã khá thành công trong việc giảm thiểu quên lãng thảm khốc trong CL. Bằng chứng gần đây cho thấy diễn tập thậm chí có thể không thể tránh khỏi trong một số tình huống CL nhất định (Farquhar and Gal, 2018). Tuy nhiên, phát lại pixel thô từ các trải nghiệm quá khứ không nhất quán với các cơ chế sinh lý thần kinh trong não (Kudithipudi et al., 2022; Hayes et al., 2019). Hơn nữa, việc phát lại pixel thô không hiệu quả về bộ nhớ và làm nảy sinh mối quan ngại về quyền riêng tư và bảo mật dữ liệu (Mai et al., 2022). Đặt cạnh nhau diễn tập trải nghiệm sinh học và nhân tạo, diễn tập biểu diễn là một giải pháp thay thế hấp dẫn để giải quyết các vấn đề liên quan đến diễn tập hình ảnh thô trong CL. Diễn tập biểu diễn, có thể là tạo sinh (van de Ven et al., 2020; Lao et al., 2020) hoặc bằng cách lưu trữ (Hayes et al., 2020; Caccia et al., 2020; Iscen et al., 2020), đòi hỏi phát lại các đặc trưng tiềm ẩn của các lớp trung gian của DNN để giảm thiểu quên lãng thảm khốc. Trong các phương pháp tạo sinh, bản thân generator cũng lớn như mô hình CL và dễ bị quên lãng thảm khốc. Ngoài ra, các mô hình tạo sinh khó huấn luyện và gặp phải sự sụp đổ mode. Tuy nhiên, mặc dù lưu trữ biểu diễn hiệu quả về bộ nhớ và tính toán, việc chọn lớp lý tưởng cho diễn tập vẫn là một câu hỏi mở. Hơn nữa, các biểu diễn được lưu trữ trong bộ nhớ bị giới hạn thiếu tính đa dạng, dẫn đến overfitting.

Ngược lại, não người học, lưu trữ và ghi nhớ trải nghiệm mà không quên lãng thảm khốc các nhiệm vụ trước đó. Tính linh hoạt của não có thể được quy cho việc diễn tập các trải nghiệm trừu tượng thông qua nhiều hệ thống bộ nhớ (Hassabis et al., 2017) và một tập hợp phong phú các nguyên tắc xử lý sinh lý thần kinh (Parisi et al., 2019). Ngoài ra, não chứa các nhiễu loạn ngẫu nhiên của tín hiệu, được gọi là nhiễu, góp phần vào sự biến đổi tế bào và hành vi từ thử nghiệm này sang thử nghiệm khác (Faisal et al., 2008). Mặc dù nhiễu đôi khi được coi là phiền toái, nhiễu tạo thành một thành phần đáng chú ý của chiến lược tính toán của não. Não khai thác nhiễu để thực hiện các nhiệm vụ, chẳng hạn như suy luận xác suất thông qua lấy mẫu, tạo điều kiện cho việc học và thích ứng trong môi trường động (Maass, 2014). Như trường hợp trong não, chúng tôi giả thuyết rằng nhiễu có thể là một công cụ có giá trị trong việc cải thiện khái quát hóa trong diễn tập biểu diễn trong vision transformers.

Vì vậy, chúng tôi đề xuất BiRT, một phương pháp học liên tục dựa trên diễn tập biểu diễn mới sử dụng vision transformers, các kiến trúc được tạo thành từ các module self-attention lấy cảm hứng từ sự chú ý thị giác của con người (Lindsay, 2020). Cụ thể, phương pháp của chúng tôi bao gồm hai hệ thống học tập bổ sung: một mô hình hoạt động và bộ nhớ ngữ nghĩa, một trung bình di động theo hàm mũ của mô hình hoạt động. Để giảm overfitting và mang lại sự đa dạng trong diễn tập biểu diễn, BiRT giới thiệu các nhiễu có thể kiểm soát ở nhiều giai đoạn khác nhau của vision transformer và thực thi tính nhất quán trong dự đoán đối với bộ nhớ ngữ nghĩa. Khi bộ nhớ ngữ nghĩa củng cố thông tin ngữ nghĩa, việc điều chỉnh tính nhất quán trong sự hiện diện của nhiễu có ý nghĩa thúc đẩy khái quát hóa đồng thời giảm thiểu hiệu quả overfitting. BiRT cung cấp cải thiện hiệu suất nhất quán so với diễn tập hình ảnh thô và diễn tập biểu diễn vanilla trên nhiều tình huống và số liệu CL đồng thời mạnh mẽ trước các hỏng hóc tự nhiên và adversarial (Hình 1).

2. Các Nghiên Cứu Liên Quan
Học Liên Tục: DNN thường được thiết kế để thích ứng tăng dần với các luồng dữ liệu i.i.d. tĩnh được hiển thị riêng biệt và theo thứ tự ngẫu nhiên (Parisi et al., 2019). Do đó, học tuần tự trên dữ liệu không i.i.d. gây ra quên lãng thảm khốc các nhiệm vụ trước đó và overfitting nhiệm vụ hiện tại. Các phương pháp giải quyết quên lãng thảm khốc có thể được chia rộng thành ba loại: các phương pháp dựa trên điều chỉnh (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017) phạt những thay đổi trong các tham số quan trọng liên quan đến các nhiệm vụ trước đó, các phương pháp cô lập tham số (Rusu et al., 2016; Aljundi et al., 2017; Fernando et al., 2017) phân bổ một tập hợp tham số riêng biệt cho các nhiệm vụ riêng biệt, và các phương pháp dựa trên diễn tập (Ratcliff, 1990; Rebuffi et al., 2017; Lopez-Paz and Ranzato, 2017; Bhat et al., 2023) lưu trữ các mẫu nhiệm vụ cũ và phát lại chúng cùng với các mẫu nhiệm vụ hiện tại. Trong số các phương pháp khác nhau để giảm thiểu quên lãng thảm khốc, diễn tập trải nghiệm khá thành công trong nhiều tình huống CL (Parisi et al., 2019).

Các phương pháp dựa trên diễn tập phát lại pixel thô từ các trải nghiệm quá khứ, không nhất quán với cách con người học liên tục (Kudithipudi et al., 2022). Hơn nữa, việc phát lại pixel thô có thể có những hậu quả khác, bao gồm dung lượng bộ nhớ lớn, quyền riêng tư dữ liệu và mối quan ngại bảo mật (Mai et al., 2022). Do đó, một số nghiên cứu (Pellegrini et al., 2020; Iscen et al., 2020; Caccia et al., 2020) bắt chước diễn tập biểu diễn trừu tượng trong não bằng cách lưu trữ và phát lại các biểu diễn từ các lớp trung gian trong DNN. Diễn tập biểu diễn có thể được thực hiện bằng cách sử dụng các mô hình tạo sinh (van de Ven et al., 2020; Lao et al., 2020) hoặc bằng cách lưu trữ các biểu diễn nhiệm vụ trước đó trong buffer (Hayes et al., 2020; Iscen et al., 2020). Trong khi các mô hình tạo sinh tự chúng dễ bị quên và sụp đổ mode, việc lưu trữ biểu diễn trong buffer bộ nhớ bị giới hạn thiếu tính đa dạng do không có sẵn các cơ chế tăng cường phù hợp. Mặc dù việc phát lại biểu diễn cấp cao có thể giảm thiểu chi phí bộ nhớ và mối quan ngại về quyền riêng tư, việc phát lại biểu diễn lặp đi lặp lại dẫn đến overfitting.

Transformers cho CL: Kiến trúc Transformer (Vaswani et al., 2017) lần đầu tiên được phát triển cho dịch máy và sau đó mở rộng cho các nhiệm vụ thị giác máy tính (Dosovitskiy et al., 2020; Touvron et al., 2021; Jeeveswaran. et al., 2022) bằng cách coi các patch hình ảnh như là thay thế cho tokens. Mặc dù thành công trong một số benchmark, vision transformers chưa được xem xét rộng rãi cho học liên tục. Yu et al. (2021) nghiên cứu transformers trong setting học tăng dần lớp và chỉ ra một số vấn đề trong việc áp dụng một cách ngây thơ transformers trong CL. DyTox (Douillard et al., 2021) đề xuất một kiến trúc mở rộng động sử dụng các task tokens riêng biệt để mô hình hóa ngữ cảnh của các lớp khác nhau trong CL. LVT (Wang et al., 2022a) đề xuất một khóa ngoài và một bias attention để ổn định bản đồ attention giữa các nhiệm vụ và sử dụng cấu trúc classifier kép để tránh can thiệp thảm khốc khi học các nhiệm vụ mới. Pelosin et al. (2022) đề xuất một loss điều chỉnh bất đối xứng trên các bản đồ attention được gộp đối với mô hình được học trên nhiệm vụ trước đó để học liên tục trong phương pháp không có exemplar. Một số nghiên cứu đồng thời khác (Ermis et al., 2022; Wang et al., 2022c;b) khai thác mô hình được huấn luyện trước và kết hợp việc học các tham số chung và cụ thể cho nhiệm vụ. Không giống như những nghiên cứu này, chúng tôi không sử dụng các mô hình được huấn luyện trước và phát lại các biểu diễn trung gian thay vì đầu vào hình ảnh thô. Chúng tôi tìm cách cải thiện hiệu suất của vision transformers dưới diễn tập biểu diễn trong CL. Vì nhiễu đóng vai trò xây dựng trong não, chúng tôi bắt chước sự phổ biến của nhiễu trong não và sự biến đổi từ thử nghiệm này sang thử nghiệm khác bằng cách tiêm nhiễu vào phương pháp đề xuất của chúng tôi.

3. Phương Pháp Đề Xuất
Paradigm CL thường bao gồm T nhiệm vụ tuần tự, với dữ liệu dần dần trở nên có sẵn theo thời gian. Trong mỗi nhiệm vụ t ∈ {1; 2; ::; T}, các mẫu và nhãn tương ứng (xi; yi)^N_i=1 được rút ra từ phân phối cụ thể cho nhiệm vụ Dt. Mô hình học liên tục f được tối ưu hóa tuần tự trên từng nhiệm vụ một lần, và suy luận được thực hiện trên tất cả các nhiệm vụ đã thấy cho đến nay. CL đặc biệt thách thức đối với vision transformers do dữ liệu huấn luyện hạn chế cho mỗi nhiệm vụ (Raghu et al., 2021; Touvron et al., 2021) ngoài vấn đề quên lãng thảm khốc. Bằng cách bắt chước sự liên kết của các trải nghiệm quá khứ và hiện tại trong não, diễn tập trải nghiệm (ER) giải quyết một phần vấn đề quên lãng thảm khốc. Do đó, mục tiêu học của ER như sau:

Ler = λE_(xi;yi)∼Dt[Lce(f(xi); yi)]
+ E_(xj;yj)∼Dm[Lce(f(xj); yj)]; (1)

trong đó λ đại diện cho một tham số cân bằng, Dm là bộ nhớ tình tiết, và Lce là cross-entropy loss. Để giảm thêm quên lãng thảm khốc, chúng tôi sử dụng một hệ thống học tập bổ sung dựa trên diễn tập biểu diễn trừu tượng, cấp cao. Để thúc đẩy sự đa dạng và khái quát hóa trong diễn tập biểu diễn, chúng tôi giới thiệu các nhiễu có thể kiểm soát ở các giai đoạn khác nhau của vision transformer và thực thi tính nhất quán trong dự đoán đối với bộ nhớ ngữ nghĩa. Trong các phần sau, chúng tôi mô tả chi tiết các thành phần khác nhau của BiRT.

3.1. Củng Cố Kiến Thức thông qua hệ thống học tập bổ sung
Lý thuyết hệ thống học tập bổ sung (CLS) cho rằng hippocampus và neocortex đòi hỏi các tính chất bổ sung cần thiết để nắm bắt các tương tác phức tạp trong não (McNaughton and O'Reilly, 1995). Lấy cảm hứng từ lý thuyết CLS, chúng tôi đề xuất một hệ thống học tập dựa trên transformer bộ nhớ kép thu thập và tiếp thu kiến thức trong thời gian ngắn và dài. Mô hình hoạt động gặp phải các nhiệm vụ mới và củng cố kiến thức trong thời gian ngắn. Sau đó chúng tôi dần dần tập hợp các trọng số của mô hình hoạt động vào bộ nhớ ngữ nghĩa trong các giai đoạn không hoạt động xen kẽ. Theo Arani et al. (2021), chúng tôi thiết kế bộ nhớ ngữ nghĩa như một trung bình di động theo hàm mũ của mô hình hoạt động như sau:

θs = αθs + (1 − α)θw (2)

trong đó θw và θs là các trọng số của mô hình hoạt động và bộ nhớ ngữ nghĩa, tương ứng, và α là một tham số suy giảm. Khi mô hình hoạt động tập trung vào chuyên môn hóa nhiệm vụ hiện tại, bản sao của mô hình hoạt động tại mỗi bước huấn luyện có thể được coi là một chuyên gia về một nhiệm vụ cụ thể. Do đó, việc tập hợp các trọng số trong suốt quá trình huấn luyện CL có thể được coi là một ensemble của các mô hình chuyên gia củng cố kiến thức qua các nhiệm vụ, dẫn đến ranh giới quyết định mượt mà hơn.

3.2. Bộ Nhớ Tình Tiết
Phù hợp với diễn tập trải nghiệm trong não (Ji and Wilson, 2007), chúng tôi đề xuất một diễn tập biểu diễn trừu tượng, cấp cao cho vision transformers. Mô hình hoạt động bao gồm hai hàm lồng nhau: g(·) và fw(·). Một vài lớp đầu tiên của encoder, g(·), xử lý đầu vào hình ảnh thô, và đầu ra cùng với nhãn ground truth được lưu trữ trong bộ nhớ tình tiết Dm. Để đảm bảo tính nhất quán trong các biểu diễn trung gian, g(·) có thể được khởi tạo bằng cách sử dụng các trọng số được huấn luyện trước và cố định trước khi bắt đầu huấn luyện CL hoặc cố định sau khi học một số nhiệm vụ. Mặt khác, fw(·), các lớp sau của transformer, xử lý các biểu diễn trừu tượng cấp cao, và vẫn có thể học được trong suốt quá trình huấn luyện CL. Trong các giai đoạn không hoạt động xen kẽ, đối tác ổn định bộ nhớ ngữ nghĩa fs(·) được cập nhật theo Eq. 2.

Bộ nhớ tình tiết được điền vào ranh giới nhiệm vụ bằng cách sử dụng iCaRL herding (Rebuffi et al., 2017). Các biểu diễn rj = g(xj), được lưu trữ trong bộ nhớ tình tiết, được xen kẽ với các biểu diễn nhiệm vụ hiện tại và được xử lý đồng bộ bởi fw(·) và fs(·). Mục tiêu học cho diễn tập biểu diễn do đó có thể được thu được bằng cách thích ứng Eq. 1 như sau:

Lrepr = λE_(xi;yi)∼Dt[Lce(fw(g(xi)); yi)]
+ E_(rj;yj)∼Dm[Lce(fw(rj); yj)] (3)

3.3. Nhiễu và Biến Đổi Từ Thử Nghiệm Này Sang Thử Nghiệm Khác
Nhiễu phổ biến ở mọi cấp độ của hệ thống thần kinh và gần đây đã được chứng minh đóng vai trò xây dựng trong não (Faisal et al., 2008; McDonnell and Ward, 2011). Biến đổi từ thử nghiệm này sang thử nghiệm khác, một hiện tượng phổ biến trong các hệ thống sinh học trong đó phản ứng thần kinh với cùng một kích thích khác nhau qua các thử nghiệm, thường là kết quả của nhiễu (Faisal et al., 2008). Biến đổi từ thử nghiệm này sang thử nghiệm khác đã được chứng minh là một trong những thành phần chính của cơ chế tính toán trong não (Maass, 2014). Hơn nữa, việc tiêm nhiễu vào pipeline học tập mạng nơ-ron đã được chứng minh dẫn đến hội tụ nhanh hơn đến tối ưu toàn cục (Zhou et al., 2019), khái quát hóa tốt hơn (Srivastava et al., 2014), và chưng cất kiến thức hiệu quả.

Để mô phỏng nhiễu và biến đổi từ thử nghiệm này sang thử nghiệm khác, chúng tôi ngẫu nhiên tiêm nhiễu xây dựng vào các thành phần khác nhau của thiết lập CL của chúng tôi. Trong các phần sau, chúng tôi mô tả chi tiết cách chúng tôi tận dụng nhiễu trong quá trình huấn luyện CL.

3.3.1. NHIỄU BIỂU DIỄN ~M
Trong quá trình huấn luyện CL, mô hình hoạt động gặp phải dữ liệu cụ thể nhiệm vụ Dt được đưa vào g(·) trước, sau đó các biểu diễn đầu ra của g(·) được xen kẽ với các biểu diễn của các mẫu nhiệm vụ trước đó từ bộ nhớ tình tiết Dm. Chúng tôi cập nhật Dm tại ranh giới nhiệm vụ bằng cách sử dụng iCaRL herding. Các biểu diễn xen kẽ sau đó được xử lý bởi cả fw(·) và fs(·). Tương tự như việc phát lại các mẫu mới trong não (Liu et al., 2019), chúng tôi kết hợp tuyến tính các biểu diễn được lấy mẫu từ bộ nhớ tình tiết bằng cách sử dụng manifold mixup (Verma et al., 2019):

~r = λri + (1 − λ)rj
~y = λyi + (1 − λ)yj; (4)

trong đó ri, rj là các biểu diễn được lưu trữ của hai mẫu khác nhau và yi, yj là các nhãn tương ứng. Ở đây, hệ số trộn λ được rút ra từ phân phối Beta. Khi manifold mixup nội suy các biểu diễn của các mẫu thuộc các lớp/nhiệm vụ khác nhau, nó mang lại sự đa dạng cho diễn tập trải nghiệm, do đó giảm overfitting.

3.3.2. NHIỄU ATTENTION ~A
Khi chúng tôi sử dụng vision transformer như kiến trúc lựa chọn, self-attention tạo thành thành phần cốt lõi của BiRT. Mô hình hoạt động fw(·) trong BiRT bao gồm một số lớp multi-head self-attention ánh xạ một query và một tập hợp các cặp key-value thành một đầu ra. Chúng tôi tiêm nhiễu vào scaled dot-product attention tại mỗi lớp của fw(·) trong khi phát lại biểu diễn như sau:

Attention(Q; K; V) = (softmax(QK^T/√dk + ε))V (5)

trong đó Q, K và V là các ma trận query, key và value, và ε ∼ N(0; σ²) là nhiễu Gaussian trắng. Bằng cách ngẫu nhiên tiêm nhiễu vào self-attention, chúng tôi ngăn chặn BiRT chú ý đến các đặc trưng cụ thể của mẫu, do đó có thể giảm thiểu overfitting.

3.3.3. NHIỄU GIÁM SÁT ~T VÀ ~S
Bây giờ chúng tôi chuyển sự tập trung của mình đến các tín hiệu giám sát để giảm thêm overfitting trong CL. Do over-parameterization, mô hình CL có xu hướng overfit trên số lượng hạn chế các mẫu từ buffer. Do đó, chúng tôi giới thiệu một nhiễu nhãn tổng hợp (~T) trong đó một phần trăm nhỏ các mẫu được gán lại một lớp ngẫu nhiên. BiRT tận dụng thực tế rằng nhiễu nhãn thưa thớt, có nghĩa là chỉ một phần nhỏ các nhãn bị hỏng trong khi phần còn lại vẫn nguyên vẹn trong thế giới thực (Liu et al., 2022). Ngoài ra, các tác động có hại của nhiễu nhãn vốn có đối với khái quát hóa có thể được giảm thiểu bằng cách sử dụng nhiễu nhãn có thể kiểm soát bổ sung (Chen et al., 2021).

Trong các giai đoạn không hoạt động xen kẽ, kiến thức trong mô hình hoạt động được củng cố vào bộ nhớ ngữ nghĩa thông qua Eq. 2. Do đó, kiến thức về các nhiệm vụ trước đó được mã hóa trong các trọng số bộ nhớ ngữ nghĩa trong quỹ đạo học của mô hình hoạt động (Hinton et al., 2015). Sau đó, để truy xuất kiến thức cấu trúc được mã hóa trong bộ nhớ ngữ nghĩa, chúng tôi điều chỉnh hàm được học bởi mô hình hoạt động bằng cách thực thi tính nhất quán trong các dự đoán của nó đối với bộ nhớ ngữ nghĩa:

Lcr = λ₁E_xi∼Dt[||fw(g(xi)) − fs(g(xi))||p]
+ λ₂E_rj∼Dm[||fw(rj) − fs(rj)||p]; (6)

trong đó λ₁ và λ₂ là các trọng số cân bằng. Để bắt chước biến đổi từ thử nghiệm này sang thử nghiệm khác trong não, chúng tôi tiêm nhiễu vào logits của bộ nhớ ngữ nghĩa (~S) trước khi áp dụng điều chỉnh tính nhất quán như sau:

fs(rj) ← fs(rj) + ε (7)

trong đó ε ∼ N(0; σ²) là nhiễu Gaussian trắng, Lcr đại diện cho khoảng cách Minkowski mong đợi giữa các cặp dự đoán tương ứng và p = 2. Điều chỉnh tính nhất quán cho phép mô hình hoạt động truy xuất kiến thức cấu trúc từ bộ nhớ ngữ nghĩa từ các nhiệm vụ trước đó. Do đó, mô hình hoạt động thích ứng ranh giới quyết định với các nhiệm vụ mới mà không quên lãng thảm khốc các nhiệm vụ trước đó.

Do đó, mục tiêu học cuối cối cho mô hình hoạt động như sau:

L = Lrepr + βLcr (8)

trong đó β là một tham số cân bằng. Phương pháp đề xuất của chúng tôi được minh họa trong Hình 2 và được chi tiết trong Thuật toán 1.

Lưu ý rằng những nhiễu này được áp dụng ngẫu nhiên, và do đó, một biểu diễn duy nhất có thể có nhiều nhiễu liên quan đến nó. Mặc dù nhiễu thường được xử lý như một phiền toái, BiRT giới thiệu nhiễu có thể kiểm soát ở các giai đoạn khác nhau của vision transformer để thúc đẩy khái quát hóa mạnh mẽ trong CL.

4. Kết Quả Thực Nghiệm
Chúng tôi sử dụng thư viện continuum (Douillard and Lesort, 2021) để triển khai các tình huống CL khác nhau và xây dựng phương pháp của chúng tôi trên phương pháp DyTox (Douillard et al., 2021), baseline chính trong tất cả các thực nghiệm của chúng tôi. Chúng tôi báo cáo độ chính xác cuối cùng (Last), độ chính xác trung bình (Avg), chuyển giao tiến (FWT), chuyển giao lùi (BWT) và quên lãng. Thông tin thêm về thiết lập thực nghiệm, tập dữ liệu và số liệu có thể được tìm thấy trong Phụ lục A.

Bảng 1 trình bày so sánh phương pháp của chúng tôi với các benchmark CL tiêu chuẩn với các kích thước buffer khác nhau, trung bình qua ba random seeds. Chúng ta có thể thực hiện các quan sát sau từ Bảng 1: (i) Qua các setting CL và kích thước buffer khác nhau, BiRT cho thấy cải thiện hiệu suất nhất quán so với DyTox qua tất cả các số liệu. (ii) BiRT cho phép củng cố thông tin phong phú về các nhiệm vụ trước đó tốt hơn ngay cả dưới các chế độ buffer thấp, ví dụ cho CIFAR-100, cải thiện tuyệt đối về Last Acc là 7,28% cho kích thước buffer 1000 trong khi lên đến 15,66% cho kích thước buffer 500. (iii) BWT và FWT làm sáng tỏ ảnh hưởng của việc học một nhiệm vụ mới t đến hiệu suất của các nhiệm vụ trước và sau, tương ứng. BiRT cho thấy BWT âm nhỏ hơn và FWT dương cao hơn qua tất cả các tập dữ liệu CL, dẫn đến ít quên lãng hơn và tạo điều kiện tiến tốt hơn. (iv) TinyImageNet là một trong những tập dữ liệu thử thách cho CL được xem xét trong công việc này. Dưới các chế độ buffer thấp, số lượng mẫu trên mỗi lớp sẽ bị hạn chế nghiêm trọng do số lượng lớp lớn trên mỗi nhiệm vụ. BiRT liên tục vượt trội DyTox qua tất cả các kích thước buffer trên TinyImageNet.

Bảng 2 tiếp tục chứng minh so sánh phương pháp của chúng tôi với các phương pháp dựa trên transformer không có exemplar (ATT-asym và FUNC-asym (Pelosin et al., 2022); trung bình qua 3 seeds) và dựa trên diễn tập (DyToX và LVT; trung bình qua 5 thứ tự lớp). Mặc dù ban đầu không được thiết kế cho tình huống không có exemplar, BiRT cho thấy cải thiện đáng kể so với các phương pháp không có diễn tập. Tiến triển từ tình huống không có exemplar, BiRT cho thấy cải thiện hiệu suất hơn nữa khi được cung cấp diễn tập trải nghiệm. Chúng tôi cũng so sánh các phương pháp CL với số lượng nhiệm vụ khác nhau trong CIFAR-100 với kích thước buffer hạn chế. BiRT củng cố các đặc trưng có thể khái quát hóa thay vì các đặc trưng phân biệt cụ thể cho các mẫu được đệm, do đó thể hiện hiệu suất vượt trội qua tất cả kích thước buffer và chuỗi nhiệm vụ.

Củng cố giả thuyết trước đó của chúng tôi, các nhiễu có thể kiểm soát được giới thiệu trong BiRT đóng vai trò xây dựng trong việc thúc đẩy khái quát hóa và do đó giảm overfitting trong CL. Ngoài việc giảm bớt mối quan ngại về quyền riêng tư, việc thay thế diễn tập hình ảnh thô bằng diễn tập biểu diễn giảm dung lượng bộ nhớ mà không ảnh hưởng đến hiệu suất.

5. Phân Tích Mô Hình
Thiên Vị Gần Đây Nhiệm Vụ: Học tuần tự nhiều nhiệm vụ khiến dự đoán classifier nghiêng về các nhiệm vụ gần đây, dẫn đến thiên vị gần đây nhiệm vụ (Masana et al., 2020). Một hậu quả trực tiếp của thiên vị gần đây nhiệm vụ là norm classifier cao hơn cho các lớp gần đây trong khi thấp hơn cho các lớp cũ hơn, có nghĩa là các lớp cũ hơn ít có khả năng được chọn để dự đoán (Hou et al., 2019). Theo phân tích trong (Bhat et al., 2022), Hình 4 (phải) cho thấy xác suất được chuẩn hóa rằng tất cả các lớp trong mỗi nhiệm vụ được dự đoán ở cuối huấn luyện. Các xác suất trong BiRT được phân phối đều hơn so với DyTox, dẫn đến thiên vị gần đây thấp hơn. Chúng tôi lập luận rằng các nhiễu giám sát được đề xuất trong BiRT điều chỉnh ngầm classifier hướng tới xác suất dự đoán được phân phối đều hơn.

Tình Trạng Khó Xử Ổn Định-Dẻo Dai: Mức độ mà mô hình CL đủ dẻo dai để thu thập thông tin mới trong khi đủ ổn định để không can thiệp thảm khốc với kiến thức được củng cố được gọi là tình trạng khó xử ổn định-dẻo dai (Parisi et al., 2019). Quên lãng thảm khốc là hậu quả trực tiếp của tình trạng khó xử này khi tính dẻo dai của mô hình CL vượt qua tính ổn định của nó. Để điều tra mức độ phương pháp của chúng tôi xử lý tình trạng khó xử ổn định-dẻo dai, chúng tôi vẽ biểu đồ hiệu suất theo nhiệm vụ ở cuối mỗi nhiệm vụ trong Hình 3 cho tập test CIFAR-100. Theo Sarfraz et al. (2022), chúng tôi cũng trực quan hóa một thước đo cân bằng chính thức trong Hình 4 (trái). Cả mô hình hoạt động và bộ nhớ ngữ nghĩa đều thể hiện tính ổn định cao hơn, trong khi DyTox dẻo dai hơn. Do đó, DyTox dễ bị quên lãng hơn, trong khi BiRT hiển thị cân bằng ổn định-dẻo dai tốt hơn so với baseline.

Phân Tích Bản Đồ Attention: Khi học tiến triển qua một chuỗi các nhiệm vụ, một mô hình CL giữ được sự tập trung vào các vùng nổi bật trải qua ít quên lãng thảm khốc hơn. Do đó, sẽ có lợi khi nghiên cứu sự biến đổi trong các vùng nổi bật của hình ảnh trong quỹ đạo học. Hình 6 cho thấy so sánh các bản đồ saliency cho các mẫu của nhiệm vụ đầu tiên sau khi huấn luyện trên nhiệm vụ đầu tiên và cuối cùng, tương ứng. Như có thể thấy, BiRT giữ được sự chú ý đến các vùng quan trọng trong những hình ảnh này tốt hơn DyTox. Chúng tôi cho rằng nhiễu attention được đề xuất trong BiRT giúp tập trung vào các đặc trưng toàn lớp thay vì các đặc trưng cụ thể mẫu, do đó giữ được sự chú ý đến các vùng quan trọng trong hình ảnh test. Giải thích thêm và trực quan hóa mở rộng được cung cấp trong Phụ lục M.

Phân Tích Tính Mạnh Mẽ: Các mô hình học liên tục chủ yếu được đánh giá về độ chính xác trên các nhiệm vụ đã thấy và số liệu quên lãng. Tuy nhiên, cộng đồng nghiên cứu đã phần lớn bỏ qua tính dễ bị tổn thương của các mô hình được học liên tục đối với các cuộc tấn công adversarial và dữ liệu bị hỏng trong tự nhiên (Khan et al., 2022). Hình 5 minh họa tính mạnh mẽ của BiRT đối với cuộc tấn công adversarial với cường độ khác nhau (Kim, 2020) và một số hỏng hóc tự nhiên (Hendrycks and Dietterich, 2019). Ngoài ra, chúng tôi đánh giá tính mạnh mẽ của BiRT mà không có bất kỳ nhiễu nào trong quỹ đạo học để làm sáng tỏ lợi ích của việc cảm ứng nhiễu một cách xây dựng trong pipeline của các mô hình học liên tục. BiRT mạnh mẽ đối với các cuộc tấn công adversarial, cũng như dữ liệu bị hỏng, và học với nhiễu dẫn đến cải thiện tính mạnh mẽ. Điều này rõ ràng từ hiệu suất dưới các nhiễu nghiêm trọng như 'contrast', 'fog', 'motion blur' và hiệu suất trung bình qua các setting khác nhau trong đó học với nhiễu giúp mô hình phục hồi từ hiệu suất kém hơn.

Điều này làm cho nó phù hợp cho các ứng dụng quan trọng về an toàn, chẳng hạn như xe tự lái, nơi hậu quả của việc mô hình thất bại có thể nghiêm trọng.

6. Nghiên Cứu Loại Bỏ
Bảng 3 cung cấp cái nhìn tổng quan về hiệu ứng của các thành phần khác nhau được sử dụng trong BiRT. Không giống như DyTox, chúng tôi sử dụng trung bình di động theo hàm mũ như bộ nhớ ngữ nghĩa, dẫn đến bước nhảy lớn nhất về độ chính xác. BiRT đòi hỏi nhiễu biểu diễn, attention và giám sát để thúc đẩy khái quát hóa mạnh mẽ trong CL và đa dạng hóa các biểu diễn được đệm. Như có thể thấy, cả ba thành phần của BiRT đều đóng vai trò xây dựng trong việc xây dựng một học liên tục thành công. Nhiễu giám sát, nhiễu biểu diễn và nhiễu attention mang lại cải thiện hiệu suất 0,54%, 3,30% và 3,41%, tương ứng, so với BiRT không có bất kỳ nhiễu nào. Ngoài ra, so với diễn tập biểu diễn vanilla, sự kết hợp đúng của các nhiễu có thể kiểm soát trong BiRT giảm đáng kể overfitting và cải thiện hiệu suất lên đến 9% (Avg tương đối). Do đó, điều cần thiết là phải có nhiễu có thể kiểm soát để cải thiện thêm diễn tập biểu diễn trong CL.

7. Kết Luận và Hướng Nghiên Cứu Tương Lai
Chúng tôi đã đề xuất BiRT, một phương pháp học liên tục dựa trên diễn tập biểu diễn mới dựa trên vision transformers. Cụ thể, chúng tôi giới thiệu các nhiễu có thể kiểm soát ở nhiều giai đoạn khác nhau của vision transformer và thực thi tính nhất quán trong dự đoán đối với trung bình di động theo hàm mũ của mô hình hoạt động. Kết quả thực nghiệm của chúng tôi cho thấy BiRT vượt trội hơn diễn tập hình ảnh thô và diễn tập biểu diễn vanilla trong khi hiệu quả về bộ nhớ và mạnh mẽ trước các hỏng hóc tự nhiên và adversarial. Hơn nữa, cải thiện thậm chí còn rõ rệt hơn dưới các chế độ buffer thấp và chuỗi nhiệm vụ dài hơn. Củng cố giả thuyết trước đó của chúng tôi, các nhiễu có thể kiểm soát được giới thiệu trong BiRT đóng vai trò xây dựng trong việc thúc đẩy khái quát hóa và do đó giảm overfitting trong CL. Mở rộng công việc của chúng tôi cho các setting thực tế hơn như CL tổng quát nơi ranh giới nhiệm vụ không được biết tại thời gian huấn luyện, và khám phá các kiến trúc transformer hiệu quả khác là một số hướng nghiên cứu hữu ích cho công việc này.
