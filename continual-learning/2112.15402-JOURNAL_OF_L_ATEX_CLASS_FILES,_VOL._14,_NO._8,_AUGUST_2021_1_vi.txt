# 2112.15402.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2112.15402.pdf
# Kích thước tệp: 4476716 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 1
Phát lại Trải nghiệm Quan hệ: Học liên tục
bằng cách Điều chỉnh Thích ứng Mối quan hệ theo Nhiệm vụ
Quanziang Wang, Renzhen Wang, Yuexiang Li, Dong Wei, Hong Wang,
Kai Ma, Yefeng Zheng, Fellow, IEEE , Deyu Meng, Member, IEEE
Tóm tắt —Học liên tục là một mô hình học máy đầy hứa hẹn để học các nhiệm vụ mới trong khi giữ lại kiến thức đã học trước đó trên dữ liệu đào tạo dạng luồng. Cho đến nay, các phương pháp dựa trên rehearsal, giữ một phần nhỏ dữ liệu từ các nhiệm vụ cũ như bộ đệm bộ nhớ, đã cho thấy hiệu suất tốt trong việc giảm thiểu quên thảm khốc cho kiến thức đã học trước đó. Tuy nhiên, hầu hết các phương pháp này thường đối xử với mỗi nhiệm vụ mới một cách bình đẳng, điều này có thể không xem xét đầy đủ mối quan hệ hoặc sự tương tự giữa các nhiệm vụ cũ và mới. Hơn nữa, các phương pháp này thường bỏ qua tầm quan trọng của mẫu trong quá trình đào tạo liên tục và dẫn đến hiệu suất không tối ưu trên một số nhiệm vụ nhất định. Để giải quyết vấn đề thách thức này, chúng tôi đề xuất Phát lại Trải nghiệm Quan hệ (RER), một khung học hai cấp, để điều chỉnh thích ứng mối quan hệ theo nhiệm vụ và tầm quan trọng mẫu trong mỗi nhiệm vụ để đạt được sự cân bằng tốt hơn giữa 'ổn định' và 'dẻo dai'. Như vậy, phương pháp đề xuất có khả năng tích lũy kiến thức mới trong khi củng cố kiến thức cũ đã học trước đó trong quá trình học liên tục. Các thí nghiệm mở rộng được thực hiện trên ba bộ dữ liệu có sẵn công khai ( tức là CIFAR-10, CIFAR-100, và Tiny ImageNet) cho thấy phương pháp đề xuất có thể cải thiện hiệu suất của tất cả các baseline một cách nhất quán và vượt qua các phương pháp hiện đại hiện tại.
Từ khóa chỉ mục —Học liên tục, tình trạng khó xử ổn định-dẻo dai, tối ưu hóa hai cấp.

I. GIỚI THIỆU
MẠNG nơ-ron sâu được đào tạo ngoại tuyến đã đạt được kết quả xuất sắc trong nhiều nhiệm vụ thị giác máy tính, chẳng hạn như phân loại [1]–[3], phân đoạn ngữ nghĩa [4]–[6], và phát hiện đối tượng [7]–[9]. Tuy nhiên, những mô hình này thường gặp khó khăn trong các tình huống học liên tục (CL) nơi kiến thức được tích lũy dần dần từ dữ liệu được tạo ra bởi một phân phối không ổn định. Thực tế, việc không thể của các mô hình là do chúng có xu hướng ghi đè kiến thức đã học trước đó bất cứ khi nào có nhiệm vụ mới. Điều này được gọi là quên thảm khốc [10] và là một thách thức chính trong học liên tục.

Gần đây, nhiều phương pháp CL khác nhau đã được đề xuất để giảm nhẹ vấn đề quên thảm khốc. Các thuật toán hiện có cho CL có thể được chia thô thành ba loại [11]:
1) Các phương pháp dựa trên mô hình [12]–[16], sửa đổi động kiến trúc của mô hình để xử lý nhiệm vụ mới và sử dụng cấu trúc mô hình cụ thể cho các nhiệm vụ khác nhau trong giai đoạn kiểm tra; 2) Các phương pháp dựa trên chính quy hóa [17]–[21], đề xuất các ràng buộc để duy trì các tham số mô hình

Quanziang Wang, Renzhen Wang, và Deyu Meng thuộc Trường Toán học và Thống kê, Đại học Giao thông Xi'an, Xi'an 710049, P.R. China.
Yuexiang Li, Dong Wei, Hong Wang, Kai Ma, và Yefeng Zheng thuộc Phòng thí nghiệm Tencent Jarvis, Shenzhen 518052, P.R. China.

Hình 1. Minh họa hai yếu tố chính ảnh hưởng đến ổn định và dẻo dai trong học liên tục: (i) Mối quan hệ giữa các nhiệm vụ mới và cũ thay đổi động. Ví dụ, lớp 'bird' của nhiệm vụ 2 có liên quan ngữ nghĩa với 'airplane' của nhiệm vụ 1 do khoảng cách gần hơn giữa hai cụm này trong không gian tiềm ẩn. (ii) Tầm quan trọng của các điểm dữ liệu trong mỗi lớp khác nhau đối với việc đào tạo mô hình. Các mẫu có viền đen, nằm xung quanh ranh giới phân loại, khó khăn/có lợi hơn cho việc phân loại so với các mẫu khác.

tương đối quan trọng cho các nhiệm vụ đã học trước đó; 3) Các phương pháp dựa trên rehearsal [22]–[30], lưu trữ một tập con nhỏ các ví dụ từ các nhiệm vụ đã học trước đó trong bộ đệm bộ nhớ và phát lại chúng trong quá trình học các nhiệm vụ mới. Trong nghiên cứu này, chúng tôi chủ yếu tập trung vào khám phá tiềm năng của các phương pháp dựa trên rehearsal, do hiệu suất tương đối ổn định và hiệu quả của chúng trong việc giảm thiểu quên thảm khốc.

Do thiếu dữ liệu từ các nhiệm vụ cũ, ngoại trừ bộ đệm bộ nhớ nhỏ, các phương pháp dựa trên rehearsal yêu cầu tìm kiếm sự cân bằng thích hợp giữa việc học thông tin mới và giữ lại kiến thức trước đó, được gọi là tình trạng khó xử 'ổn định-dẻo dai' [31]. Nhiều phương pháp dựa trên rehearsal, chẳng hạn như Experience Replay (ER) [22], [23] và DER++ [26], chủ yếu tập trung vào việc tận dụng thông tin lưu trữ từ các nhiệm vụ trước đó để giảm thiểu việc quên mô hình trong quá trình đào tạo các nhiệm vụ mới. Tuy nhiên, các phương pháp này đã chú ý hạn chế đến việc mô hình hóa mối quan hệ phức tạp giữa các nhiệm vụ. Ví dụ, hàm mất mát của ER là tổng của các mất mát của các mẫu nhiệm vụ mới đến và những mẫu nhiệm vụ cũ được lưu trữ trong bộ đệm bộ nhớ: Lnew+λLold, trong đó λ là một siêu tham số và thường được điều chỉnh thủ công bằng tìm kiếm lưới ở đầu quá trình đào tạo để cân bằng sự đánh đổi giữa ổn định và dẻo dai. Siêu tham số đánh đổi này thường cần được xác định trước cẩn thận để đảm bảo xu hướng đào tạo hợp lý trong toàn bộ quá trình CL.

Thực tế, như được hiển thị trong Hình 1, dưới các cài đặt của CL, luồng dữ liệu của các nhiệm vụ mới có xu hướng gây ra sự thay đổi liên tục của phân phối dữ liệu. Do đó, mối quan hệ giữa các nhiệm vụ cũ và mới thay đổi động trong suốt

arXiv:2112.15402v3  [cs.LG]  3 Aug 2023

--- TRANG 2 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 2

quá trình học tập. Ví dụ, kiến thức được trích xuất từ các nhiệm vụ trước đó liên quan đến mèo có thể bị làm phiền bởi nhiệm vụ mới về nhận dạng chó, điều này có thể dẫn đến việc quên nghiêm trọng về phân loại mèo và hiệu suất không tối ưu cho mô hình đã học. Ngược lại, một số lớp không liên quan ngữ nghĩa ( ví dụ, ô tô) có ít tác động đến mèo trong các nhiệm vụ cũ. Rõ ràng, mô hình liên tục nên điều chỉnh trọng tâm của nó vào các nhiệm vụ cũ ( tức là 'ổn định') hoặc các nhiệm vụ mới ( tức là 'dẻo dai') theo thời gian thực theo mối quan hệ thay đổi động của chúng. Tuy nhiên, khó khăn cho các phương pháp hiện có để mô hình hóa mối quan hệ phức tạp theo nhiệm vụ như vậy bằng cách chỉ đơn giản điều chỉnh và cố định các siêu tham số ở đầu quá trình đào tạo mạng. Hơn nữa, các nghiên cứu trước đó không xem xét rộng rãi một yếu tố nội tại khác, tức là tầm quan trọng mẫu trong cùng một lớp về việc học biểu diễn đặc trưng. Cụ thể, các mẫu dễ học chứa kiến thức đại diện hơn cho mỗi lớp, trong khi những mẫu 'khó', nằm xung quanh ranh giới quyết định để phân loại, có thể đóng góp vào việc học của bộ phân loại [32] (tham khảo Hình 1). Từ góc độ này, việc mô hình hóa mối quan hệ giữa các nhiệm vụ mới và cũ và tầm quan trọng mẫu trong cùng một lớp là điều cần thiết.

Để đạt được mục tiêu này, chúng tôi đề xuất Phát lại Trải nghiệm Quan hệ (RER), một khung học hai cấp để kết hợp mối quan hệ nhiệm vụ phức tạp và tầm quan trọng mẫu đã đề cập ở trên. Cụ thể, các vấn đề tối ưu hóa vòng lặp trong nhằm giải quyết 'dẻo dai' bằng cách tận dụng các ví dụ từ nhiệm vụ mới đến, trong khi vấn đề tối ưu hóa vòng lặp ngoài của RER nhằm giải quyết vấn đề 'ổn định' bằng cách giảm thiểu rủi ro thực nghiệm của một batch dữ liệu cân bằng (bao gồm các mẫu từ các nhiệm vụ cũ được lưu trữ trong bộ đệm bộ nhớ và những mẫu từ các nhiệm vụ mới). Để tận dụng đầy đủ các mẫu đào tạo bằng cách xem xét mối quan hệ liên nhiệm vụ và tầm quan trọng mẫu, chúng tôi thiết kế một hàm trọng số rõ ràng được tham số hóa bởi một mạng nơ-ron nhẹ (được gọi là Relation Replay Net, hoặc RRN), ánh xạ thông tin trừu tượng từng cặp của các mẫu từ các nhiệm vụ mới và cũ thành các trọng số mất mát tương ứng của chúng. Chúng tôi chứng minh về mặt lý thuyết rằng RRN được cập nhật dựa trên sự tương tự của gradient trung bình giữa các lớp trong tối ưu hóa vòng lặp ngoài, cho thấy rằng RER đề xuất của chúng tôi có thể mô hình hóa ngầm mối quan hệ theo nhiệm vụ. Chúng tôi tiến hành các thí nghiệm mở rộng trên các bộ dữ liệu chuẩn khác nhau, và kết quả xác minh hiệu quả của phương pháp đề xuất trong việc cải thiện nhất quán các baseline khác nhau cho các phương pháp học liên tục dựa trên rehearsal. Tóm lại, đóng góp của chúng tôi chủ yếu gồm bốn phần:

1) Phương pháp đề xuất xem xét hai yếu tố, tức là mối quan hệ nhiệm vụ trong toàn bộ quá trình học liên tục và tầm quan trọng mẫu trong mỗi lớp, để phân công trọng số mẫu. Thiết kế như vậy tạo điều kiện cho mô hình giải quyết tình trạng khó xử 'ổn định-dẻo dai' gây khó khăn cho mô hình học liên tục.

2) Chúng tôi chứng minh về mặt lý thuyết rằng phương pháp đề xuất có thể mô hình hóa ngầm mối quan hệ nhiệm vụ. Cụ thể, công thức cập nhật của Relation Replay Net phụ thuộc vào sự tương tự giữa gradient của mỗi mẫu đào tạo và gradient trung bình của mỗi lớp được lưu trữ trong bộ đệm bộ nhớ.

3) Theo hiểu biết của chúng tôi, trong vấn đề học liên tục, chúng tôi là người đầu tiên đề xuất một chiến lược trọng số mẫu tự động để phân công thích ứng một trọng số hợp lý cho mỗi mẫu từ các nhiệm vụ mới và cũ, linh hoạt hơn các phương pháp hiện có dựa trên điều chỉnh thủ công.

4) Phương pháp đề xuất có thể được áp dụng cho các baseline học liên tục dựa trên rehearsal khác nhau và cải thiện hiệu suất của chúng một cách nhất quán dưới các cài đặt khác nhau.

Bài báo này được tổ chức như sau. Phần II cung cấp một đánh giá về một số công trình liên quan và Phần III giới thiệu ngắn gọn về cài đặt và các ký hiệu cần thiết cho học liên tục. Phần IV trình bày chi tiết phương pháp RER đề xuất. Phần V sau đó cung cấp các thí nghiệm và phân tích phương pháp của chúng tôi. Bài báo cuối cùng được kết luận trong Phần VI.

II. CÔNG TRÌNH LIÊN QUAN

A. Học Liên tục

Các Phương pháp dựa trên Rehearsal. Cơ chế chính nằm dưới các phương pháp dựa trên rehearsal [22]–[29], [33] là sử dụng thông tin của dữ liệu từ các nhiệm vụ cũ để ngăn chặn việc quên trong khi đào tạo các nhiệm vụ mới. Cụ thể, các phương pháp này thường liên quan đến việc lưu một phần các mẫu dữ liệu từ các nhiệm vụ cũ như một bộ đệm bộ nhớ, sau đó được sử dụng cùng với các mẫu dữ liệu mới đến để đào tạo mô hình. Ví dụ, GEM [25] công thức hóa một vấn đề lập trình bậc hai để thực thi tính trực giao giữa hướng tối ưu hóa cho một nhiệm vụ mới và những hướng được lưu trữ trước đó trong bộ đệm bộ nhớ trong quá trình đào tạo. A-GEM [34] nới lỏng các ràng buộc trong GEM bằng cách chỉ hạn chế tích vô hướng của các gradient mẫu mới và cũ là không âm, nhằm cải thiện hiệu quả tính toán của thuật toán. iCaRL [24] lưu trữ các mẫu đại diện nhất, tức là nằm xung quanh tâm lớp trong không gian tiềm ẩn như bộ đệm bộ nhớ, và sử dụng bộ phân loại Nearest Class Mean (NCM) để giảm thiểu tác động của những thay đổi biểu diễn đặc trưng. Rainbow Memory [28] xây dựng bộ đệm bộ nhớ bằng cách lấy mẫu các mẫu đại diện hơn, được xác định bởi độ tin cậy dự đoán của dữ liệu sau các biến đổi khác nhau. Ngoài các nhãn sự thật cơ bản, DER++ [26] cũng lưu các xác suất dữ liệu được tạo ra bởi mô hình từ các epoch trước đó trong bộ đệm bộ nhớ như các nhãn mềm, được sử dụng để chưng cất nhằm ngăn chặn thêm việc quên. Mặt khác, các phương pháp dựa trên rehearsal thường dẫn đến các vấn đề mất cân bằng lớp, vì chúng dựa vào một bộ đệm bộ nhớ nhỏ để lưu trữ một số lượng hạn chế các ví dụ từ các nhiệm vụ cũ. Để giải quyết vấn đề này, LUCIR [33] chuẩn hóa các logit dự đoán và áp đặt các ràng buộc trên các đặc trưng để sửa chữa vấn đề mất cân bằng giữa các mẫu mới và cũ. ER-ACE [29] tính toán hàm mất mát của các nhiệm vụ mới và cũ một cách độc lập để giảm nhẹ sự can thiệp lẫn nhau giữa chúng.

Các Phương pháp Học Liên tục Khác. Các phương pháp dựa trên mô hình [12]–[16], [35] nhằm trao cho mô hình khả năng thích ứng với các nhiệm vụ mới bằng cách tự động sửa đổi kiến trúc của nó. Mặc dù các phương pháp này thể hiện hiệu suất ấn tượng trong nhiều thí nghiệm mô phỏng khác nhau, việc đào tạo và tối ưu hóa của chúng được biết là thách thức và đòi hỏi tính toán cao. Ví dụ, PNN [12] lưu tất cả các mạng của các nhiệm vụ trước đó để tránh quên, điều này thường chiếm một bộ đệm bộ nhớ lớn và cần đào tạo một mạng khác cho nhiệm vụ mới. Các phương pháp dựa trên chính quy hóa

--- TRANG 3 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 3

[17]–[21], [36] thiết kế các ràng buộc khác nhau để ngăn chặn việc thay đổi các tham số quan trọng của các nhiệm vụ trước đó trong quá trình đào tạo các nhiệm vụ mới. Thông thường, EWC [17] hạn chế việc cập nhật các tham số quan trọng, được chọn bởi ma trận Fisher từ góc độ Bayesian. SI [19] xác định các tham số quan trọng bằng hiệu ứng của sự thay đổi tham số lên mất mát. LwF [18] lưu các dự đoán mô hình trước đó của các mẫu nhiệm vụ mới như các nhãn mềm để chưng cất kiến thức được trích xuất. Mặc dù đơn giản và hiệu quả, các phương pháp này cũng đối mặt với những thách thức như điều chỉnh siêu tham số và độ nhạy với việc lựa chọn các tham số chính quy hóa. Bên cạnh đó, học liên tục cũng được áp dụng trên nhiều vấn đề thực tế, chẳng hạn như phân đoạn ngữ nghĩa [37]–[39], học few-shot [40]–[42], và phát hiện cảm xúc [43], [44], v.v. Để biết thông tin chi tiết hơn, chúng tôi khuyến nghị tham khảo [11], [45].

B. Chiến lược Trọng số Mẫu

Về mặt trọng số mẫu, phương pháp của chúng tôi có liên quan chặt chẽ với L2RW [46] và Meta Weight Net (MW-Net) [47]. Các phương pháp này liên quan đến việc đào tạo một mạng phân loại trên bộ dữ liệu nhãn nhiễu và được tối ưu hóa thông qua một chiến lược meta-learning để giảm thiểu tác động của các nhãn nhiễu. Quá trình meta-learning này được hướng dẫn bởi một bộ dữ liệu sạch nhỏ, được gọi là tập meta. Tuy nhiên, những thách thức của CL khác với vấn đề nhãn nhiễu, và đặc biệt luồng dữ liệu không ổn định và việc quên thảm khốc nghiêm trọng khiến việc áp dụng trực tiếp các phương pháp trọng số mẫu này vào CL trở nên khó khăn. Để giảm thiểu vấn đề này, chúng tôi đề xuất một chiến lược trọng số mẫu theo cặp mới để mô hình hóa mối quan hệ nhiệm vụ và nó không yêu cầu bất kỳ dữ liệu chất lượng cao bổ sung nào như tập meta giống như L2RW hoặc MW-Net do hạn chế của các cài đặt học liên tục. Theo hiểu biết của chúng tôi, đây nên là công trình đầu tiên sử dụng chiến lược trọng số mẫu tự động cho vấn đề học liên tục.

III. KIẾN THỨC CƠ BẢN

Trong phần này, chúng tôi giới thiệu ngắn gọn về các cài đặt của vấn đề học liên tục và hai baseline chính dựa trên rehearsal. Trong học liên tục, mô hình f được yêu cầu học một luồng các nhiệm vụ T={D1,D2,···} và đối với mỗi nhiệm vụ Dt, chỉ một vài mẫu có thể được lưu trữ trong bộ đệm bộ nhớ M, trong đó kích thước bộ đệm được ký hiệu là M. Thông thường, mô hình f:X → RCt là một mạng nơ-ron phân loại được tham số hóa bởi θ, trong đó Ct biểu thị tổng số lớp trên các nhiệm vụ từ 1 đến t. Trong nghiên cứu này, chúng tôi chủ yếu tập trung vào các cài đặt Class Incremental (Class-IL) và Task Incremental (Task-IL) của học liên tục. Cụ thể, trong Task-IL, ranh giới nhiệm vụ rõ ràng, tức là ID nhiệm vụ t được biết trong quá trình đào tạo và kiểm tra. Do đó, mô hình có thể đưa dữ liệu đầu vào vào bộ phân loại cụ thể cho nhiệm vụ tương ứng bằng cách sử dụng các mặt nạ [26] dựa trên ID nhiệm vụ của nó. Trái lại, ID nhiệm vụ không thể tiếp cận được trong Class-IL, điều này khó khăn hơn.

A. Experience Replay (ER)

ER là một phương pháp dựa trên rehearsal cơ bản nhằm giảm thiểu việc quên các nhiệm vụ đã học trước đó trong khi học những nhiệm vụ mới bằng cách trả lời các mẫu được lưu trữ trong bộ đệm bộ nhớ. Cụ thể, trong quá trình đào tạo mỗi nhiệm vụ, nó lấy mẫu một batch dữ liệu BD={(xD
i, yD
i)}B
i=1 từ nhiệm vụ thứ t hiện tại Dt và một batch dữ liệu khác BM={(xM
i, yM
i)}B
i=1 từ bộ đệm bộ nhớ M với cùng kích thước batch B, trong đó xi và yi đại diện cho một ví dụ và nhãn tương ứng của nó, tương ứng. Hàm mất mát của mô hình có thể được công thức hóa như:

Ltr(θ) =1
BBX
i=1λDLCE(xD
i;θ) +λMLCE(xM
i;θ),(1)

trong đó hàm mất mát LCE là mất mát cross-entropy (CE). Trong Eq. (1), các siêu tham số λD và λM đại diện cho các trọng số được gán cho các hàm mất mát cho nhiệm vụ mới và bộ đệm bộ nhớ, tương ứng. Thông thường, một cài đặt phổ biến là λD= 1 và λM=λ. Giá trị của λ là một siêu tham số quan trọng phải được đặt trước thủ công trước khi đào tạo. Một giá trị λ lớn hơn có thể tăng cường tính dẻo dai của mô hình để cải thiện hiệu suất trên nhiệm vụ mới, trong khi một giá trị λ nhỏ hơn có thể nhấn mạnh tính ổn định và ngăn chặn việc quên các nhiệm vụ trước đó. Tuy nhiên, việc đặt một trọng số cố định cho λ không phải lúc nào cũng tối ưu cho tất cả các bộ dữ liệu, và việc điều chỉnh nó cho mỗi nhiệm vụ đến đặt ra một thách thức đáng kể.

B. Dark Experience Replay++ (DER++)

DER++ [26], một phần mở rộng của ER, thêm một mất mát chưng cất kiến thức bổ sung LKD giữa các xác suất dự đoán của mô hình hiện tại và những mô hình trước đó cho các mẫu trong bộ đệm bộ nhớ để giảm thiểu thêm việc quên so với ER. Chính thức, mục tiêu đào tạo là

Ltr(θ)=1
BBX
i=1λDLCE(xD
i;θ)+λMLCE(xM
i;θ)+γMLKD(xM
i;θ),
(2)

trong đó trọng số cho nhiệm vụ mới nhất quán với ER (λD= 1), và các trọng số cho hàm mất mát của bộ đệm bộ nhớ [λM, γM] là hai siêu tham số thường được đặt thủ công bằng tìm kiếm lưới. Lưu ý rằng các siêu tham số này có liên quan với nhau và đóng vai trò quan trọng trong việc xác định sự đánh đổi 'ổn định-dẻo dai' của mô hình. Tuy nhiên, tương tự như ER, hai tham số này cũng được yêu cầu phải được đặt trước và cố định trong toàn bộ quá trình học liên tục. Điều này đặt ra một thách thức về cách tự động cân bằng 'ổn định' và 'dẻo dai' trong quá trình đào tạo trên các tình huống học liên tục khác nhau.

IV. PHÁT LẠI TRẢI NGHIỆM QUAN HỆ

Hầu hết các công trình hiện có giảm nhẹ vấn đề quên mô hình bằng cách lưu trữ nhiều thông tin hơn về các nhiệm vụ cũ, điều này có thể dẫn đến nhu cầu tăng về lưu trữ dữ liệu. Trong nghiên cứu này, chúng tôi nhằm tăng cường học liên tục từ một góc độ khác—gán trọng số thích hợp cho các mẫu đào tạo dựa trên các yếu tố sau: 1) mối quan hệ nhiệm vụ giữa các nhiệm vụ mới và cũ, và 2) tầm quan trọng mẫu trong mỗi nhiệm vụ cho việc đào tạo mạng. Để tính đến yếu tố đầu tiên, chúng tôi xem xét rằng một mẫu từ bộ đệm bộ nhớ chứa thông tin tương tự hoặc liên quan ngữ nghĩa với các mẫu nhiệm vụ mới dễ bị can thiệp hơn, có thể dẫn đến nhiều việc quên hơn. Trong tình huống này, mô hình nên

--- TRANG 4 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 4

ℒ𝑡𝑡𝑡𝑡ℬ𝑡𝑡𝑡𝑡;𝜃𝜃𝑘𝑘,𝜙𝜙𝑘𝑘
ℒ𝑏𝑏𝑏𝑏ℬ𝑏𝑏𝑏𝑏;𝜃𝜃𝜙𝜙Vòng lặp trong
Vòng lặp ngoài𝜙𝜙𝑘𝑘+1=𝜙𝜙𝑘𝑘−𝜂𝜂𝜙𝜙𝛻𝛻𝜙𝜙ℒ𝑏𝑏𝑏𝑏ℬ𝑏𝑏𝑏𝑏;𝜃𝜃𝑘𝑘+1𝜙𝜙𝜃𝜃𝑘𝑘+1𝜙𝜙=𝜃𝜃𝑘𝑘−𝜂𝜂𝜃𝜃𝛻𝛻𝜃𝜃ℒ𝑡𝑡𝑡𝑡(ℬ𝑡𝑡𝑡𝑡;𝜃𝜃𝑘𝑘,𝜙𝜙𝑘𝑘)𝑥𝑥𝑀𝑀𝑥𝑥𝐷𝐷𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝐷𝐷)
𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝑀𝑀)𝑧𝑧𝐷𝐷
𝑧𝑧𝑀𝑀𝜆𝜆𝐷𝐷𝜙𝜙𝑘𝑘
𝜆𝜆𝑀𝑀𝜙𝜙𝑘𝑘
𝑧𝑧𝑏𝑏𝑏𝑏𝑥𝑥𝑏𝑏𝑏𝑏
lan truyền ngược lan truyền tiến truyền tham sốRRN
ℎ⋅;𝜙𝜙𝑘𝑘Minh họa chính
Mạng Chính
𝑓𝑓⋅;𝜃𝜃𝑘𝑘
Mạng Chính 
Đã Cập nhật
𝑓𝑓⋅;𝜃𝜃𝑘𝑘

Hình 2. Minh họa chính của Phát lại Trải nghiệm Quan hệ đề xuất tại lần lặp thứ k. Trong vòng lặp trong, Mạng Chính được đào tạo trên batch dữ liệu ghép cặp Btr. Trong vòng lặp trong, Relation Replay Net (RRN) tạo ra các trọng số của batch mẫu đào tạo ghép cặp Btr, và Mạng Chính được cập nhật bằng SGD dựa trên mất mát đào tạo có trọng số. Trong vòng lặp ngoài, cho các tham số đã cập nhật của Mạng Chính, RRN được đào tạo với batch dữ liệu được lấy mẫu từ bộ đệm bộ nhớ, là một bộ dữ liệu tương đối cân bằng.

có lẽ chú ý nhiều hơn đến mẫu bộ đệm bộ nhớ này (tức là gán trọng số mẫu lớn hơn cho mẫu cũ này hoặc giảm trọng số của các mẫu nhiệm vụ mới) để giảm nhẹ vấn đề quên. Ngược lại, mô hình nên gán trọng số lớn hơn một cách thích hợp cho các mẫu nhiệm vụ mới để học kiến thức mới tốt hơn mà không làm phiền quá nhiều các nhiệm vụ cũ. Đối với yếu tố thứ hai, các điểm dữ liệu từ cùng một lớp cũng có xu hướng đóng góp khác nhau vào việc đào tạo mô hình. Các mẫu dễ học cung cấp thông tin đặc trưng đại diện hơn của các lớp của chúng, trong khi các mẫu 'khó' có lợi hơn cho việc tinh chỉnh ranh giới phân loại.

Như đã đề cập ở trên, các trọng số mất mát Λ, điều khiển sự cân bằng giữa 'ổn định' và 'dẻo dai' trong quá trình đào tạo trên các tình huống học liên tục khác nhau, nên được gán động cho các mẫu khác nhau thay vì được điều chỉnh thủ công và cố định [26]. Để đạt được mục tiêu này, chúng tôi đề xuất một Relation Replay Net (RRN) trích xuất kiến thức tương tác của các mẫu mới và cũ và tạo ra các trọng số tương ứng Λ một cách động. Nó tạo điều kiện cho mạng phân loại chính (được gọi là Main Net) đạt được sự đánh đổi tốt hơn giữa 'ổn định' và 'dẻo dai'. Một cách trực quan, RRN phụ thuộc vào trạng thái đào tạo của Main Net, và các trọng số mẫu được tạo ra bởi RRN lần lượt ảnh hưởng đến việc đào tạo Main Net. Do đó, thay vì đào tạo end-to-end ngây thơ, chúng tôi áp dụng một khung học hai cấp để tối ưu hóa cùng nhau RRN và Main Net, và tính ưu việt của tối ưu hóa hai cấp sẽ được chứng minh trong Phần V-C.

Để đơn giản, chúng tôi ban đầu áp dụng phương pháp đề xuất vào ER (gọi là Relational Experience Replay, hoặc RER), và việc triển khai dựa trên nhiều baseline hơn có thể được tìm thấy trong Phụ lục II-A.

A. Tổng quan

Khung RER, như được miêu tả trong Hình 2, bao gồm hai thành phần chính: Main Net f(·;θ) chịu trách nhiệm cho học liên tục, có thể là bất kỳ kiến trúc backbone thường được sử dụng nào, và RRN h(·;ϕ) được sử dụng để ước tính trọng số mẫu.

Một mặt, một mô hình CL phải có khả năng thích ứng với các nhiệm vụ mới khác nhau dựa trên kiến thức học được từ các nhiệm vụ cũ, tức là mối quan hệ nhiệm vụ sẽ ảnh hưởng đến 'dẻo dai' như đã đề cập ở trên. Hơn nữa, mỗi mẫu nhiệm vụ mới mang một mức độ ảnh hưởng khác nhau đến tính dẻo dai của mô hình, làm nổi bật tầm quan trọng của trọng số mẫu trong việc tinh chỉnh sự chú ý của mô hình. Để mô hình hóa 'dẻo dai' động này, chúng tôi gán trọng số cho các mẫu đào tạo theo cặp bao gồm các mẫu nhiệm vụ mới và cũ để đào tạo Main Net, trong đó các trọng số được tạo ra bởi RRN dựa trên mối quan hệ của chúng.

Cụ thể, chúng tôi kết hợp batch nhiệm vụ mới BD và batch bộ nhớ BM để xây dựng theo cặp batch mẫu đào tạo Btr={(xD
i, xM
i)}B
i=1, trong đó chúng tôi bỏ qua các nhãn để thuận tiện cho ký hiệu. Sau đó, hàm mất mát có trọng số cho Main Net f công thức hóa vấn đề tối ưu hóa vòng lặp trong của khung học hai cấp, đó là:

θ∗(ϕ) = arg min
θLtr(Btr;θ, ϕ)
≜1
BBX
i=1λD
i(ϕ)Ltr(xD
i;θ) +λM
i(ϕ)Ltr(xM
i;θ),(3)

trong đó Ltr là mất mát CE dựa trên ER, và các trọng số mẫu Λ ={λD
i, λM
i}B
i=1 của các cặp dữ liệu này được tạo ra bởi RRN tự động.

Mặt khác, mô hình nên có thể giảm nhẹ vấn đề quên trong khi học các nhiệm vụ mới. RRN đề xuất nên chú ý nhiều hơn đến 'ổn định' để ngăn Main Net tập trung quá nhiều vào các nhiệm vụ mới. Do đó chúng tôi công thức hóa vấn đề tối ưu hóa vòng lặp ngoài trên dữ liệu bộ đệm bộ nhớ, làm cho Main Net được trả về bằng cách tối ưu hóa vòng lặp trong trong Eq. (3) hoạt động như một sự củng cố ổn định của kiến thức từ các nhiệm vụ đã học1, tức là,

ϕ∗= arg min
ϕLbf(Bbf;θ∗(ϕ))
≜1
BBX
i=1Lbf(xi;θ∗(ϕ)),(4)

trong đó Lbf(x;θ) có thể được áp dụng đơn giản như mất mát CE, và Bbf là một batch khác được lấy mẫu từ bộ đệm bộ nhớ M, khác với batch đào tạo vòng lặp trong BM. Bằng cách tạo ra động các trọng số cụ thể cho các mẫu khác nhau, RRN có thể giúp Main Net cân bằng tốt hơn sự đánh đổi giữa các nhiệm vụ mới và cũ, qua đó tạo điều kiện cho quá trình học liên tục.

Hơn nữa, thiết kế của RRN nên xem xét các yếu tố sau: 1) RRN nên tính đến thông tin tương tác giữa các nhiệm vụ mới và cũ; 2) đầu vào của RRN nên 'phong phú thông tin' để đảm bảo rằng RRN có thể trích xuất kiến thức hữu ích để tạo ra các trọng số có ý nghĩa. Với mục đích này, chúng tôi đề xuất ghép cặp mỗi mẫu nhiệm vụ mới với một mẫu tương ứng từ bộ đệm bộ nhớ và sử dụng thông tin trừu tượng tương ứng của chúng làm đầu vào cho RRN. Cụ thể, đối với mỗi cặp mẫu xD
i từ nhiệm vụ mới và xM
i từ bộ đệm bộ nhớ, chúng tôi lấy các mất mát LD,M=
Ltr(xD
i), Ltr(xM
i)
, và chuẩn logit ||zD,M||=
∥zD
i∥2,∥zM
i∥2
 làm đầu vào của RRN, trong đó z=f(x;θ) là các logit dự đoán. Bằng cách tính đến cả thông tin nhãn và đặc trưng, RRN có thể nắm bắt các tương đồng ngữ nghĩa và phân phối giữa các mẫu được ghép cặp và tạo ra các trọng số thích hợp cho Main Net để cân bằng tầm quan trọng của các nhiệm vụ mới và cũ trong quá trình đào tạo.

Do đó, các trọng số mẫu ghép cặp được tạo ra bởi RRN có thể được viết như:


λD
i(ϕ), λM
i(ϕ)
=h
LD,M,∥zD,M∥2;ϕ
. (5)

Tóm lại, RER đề xuất tạo thành một khung học hai cấp để mô hình hóa đồng thời 'dẻo dai' và 'ổn định' trong các vấn đề tối ưu hóa vòng lặp trong và ngoài, tương ứng. RRN, được hướng dẫn bởi một tập M tương đối cân bằng, tự động tạo ra các trọng số mẫu để tinh chỉnh hướng tối ưu hóa của Main Net. Do đó, nó có thể tạo điều kiện cho việc tối ưu hóa Main Net, dẫn đến sự đánh đổi tốt hơn giữa các nhiệm vụ mới và cũ. Trong giai đoạn suy luận, chúng ta có thể trực tiếp dự đoán các hình ảnh kiểm tra bằng Main Net mà không cần qua RRN. Lưu ý rằng phương pháp của chúng tôi có thể dễ dàng thích ứng với các baseline dựa trên rehearsal khác, chẳng hạn như ER-ACE [29] và DER++ [26] bằng một số sửa đổi đơn giản (vui lòng tham khảo Phần V-B và Phụ lục II-A).

B. Quy trình Tối ưu hóa

Do khó tìm ra các giải pháp dạng đóng, việc tối ưu hóa θ và ϕ như được hiển thị trong Eq. (3) và Eq. (4) phụ thuộc vào hai vòng lặp lồng nhau, điều này tốn kém về mặt tính toán. Xem xét hiệu quả tính toán và quy mô lớn của

1Lưu ý rằng tham số ϕ của Relation Replay Net được coi là một siêu tham số của Main Net.

Thuật toán 1 Thuật toán đào tạo Relational Experience Replay
Đầu vào : dữ liệu nhiệm vụ mới Dt, bộ đệm bộ nhớ M
Đầu ra : các tham số Main Net và Relation Replay Net (RRN) {θ, ϕ}
1:while Dt̸=∅ do
2: while k < Iter max do
3: Lấy mẫu một batch nhiệm vụ mới BD∈ Dt và một batch bộ đệm BM∈ M
4: Xây dựng một batch đào tạo ghép cặp Btr← BD và BM
5: Tính toán mất mát vòng lặp trong bằng Eq. (3)
6: Cập nhật θk bằng Eq. (6)
7: Lấy mẫu một batch bộ đệm khác Bbf∈ M
8: Tính toán mất mát vòng lặp ngoài bằng Eq. (4)
9: Cập nhật ϕk bằng Eq. (7)
10: k+ +.
11: end while
12:end while

dữ liệu cần xử lý, chúng tôi áp dụng một chiến lược tối ưu hóa dựa trên gradient trực tuyến thay thế để giải quyết khung học hai cấp được đề xuất.

Cập nhật θ: Tham khảo Eq (3), cho tham số ϕk của RRN tại bước lặp k, chúng tôi tối ưu hóa tham số θ của Main Net bằng gradient descent một bước:

θk+1(ϕ) =θk−ηθ▽θLtr(Btr;θk, ϕk), (6)

trong đó ηθ là tốc độ học vòng lặp trong. Lưu ý rằng tham số cập nhật θk+1(ϕ) thực sự là một hàm của ϕ.

Cập nhật ϕ: Với tham số Main Net θ, chúng ta có thể tối ưu hóa tham số RRN ϕ bằng Eq. (4) cho θk+1(ϕ) bằng công thức sau:

ϕk+1=ϕk−ηϕ▽ϕLbf(Bbf;θk+1(ϕk)), (7)

trong đó ηϕ là tốc độ học vòng lặp ngoài. Chi tiết hơn về tính toán gradient có thể được tìm thấy trong Phụ lục I.

C. Phân tích Lý thuyết

Theo Eqs. (6) và (7), chúng ta có mệnh đề sau để tiết lộ thêm cách phương pháp đề xuất mô hình hóa mối quan hệ theo nhiệm vụ.

Mệnh đề 1. Đặt gbf(x) =∂Lbf(x;θ)
∂θ
θk và gtr(x) =
∂Ltr(x;θ)
∂θ
θk biểu thị các gradient của mẫu bộ đệm và mẫu đào tạo đối với tham số θ, tương ứng. Khi đó công thức cập nhật của ϕ được trình bày trong Eq. (7) có thể được công thức hóa lại như

θk+1(ϕ) =θk+ηθηϕ
BBX
j=1G(j)·∂hj(ϕ)
∂ϕ
ϕk, (8)

trong đó gradient ∂hj(ϕ)
∂ϕ=
∂λD
j(ϕ)
∂ϕ,∂λM
j(ϕ)
∂ϕT
, và hệ số G(j) là

G(j) =1
BCtX
c=1 BcX
i=1gbf(xi)!

gtr(xD
j), gtr(xM
j)
.(9)

--- TRANG 5 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 5

⋮
⋮⋮𝑧𝑧𝐷𝐷
𝑧𝑧𝑀𝑀𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝐷𝐷)
𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝑀𝑀)
𝑧𝑧𝐷𝐷2
𝑧𝑧𝑀𝑀2𝜆𝜆𝐷𝐷
𝜆𝜆𝑀𝑀RRN
ℎ⋅;𝜙𝜙Relation replay net (RRN)
𝜆𝜆𝐷𝐷
𝜆𝜆𝑀𝑀RRN
ℎ⋅;𝜙𝜙
⋮⋮
⋮𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝐷𝐷)
𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝑀𝑀)𝑧𝑧𝐷𝐷2
𝑧𝑧𝑀𝑀2
𝜆𝜆𝐷𝐷
𝜆𝜆𝑀𝑀RRN
ℎ⋅;𝜙𝜙
⋮⋮
⋮𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝐷𝐷)
𝐿𝐿𝑡𝑡𝑡𝑡(𝑥𝑥𝑀𝑀)𝑧𝑧𝐷𝐷2
𝑧𝑧𝑀𝑀2

Hình 3. Kiến trúc của Relation Replay Net (RRN).

Trong Mệnh đề 1, chúng tôi chứng minh rằng mối quan hệ theo nhiệm vụ trong RRN có thể được mô hình hóa ngầm thông qua quá trình cập nhật SGD. Điều này đạt được bằng cách tính toán hệ số G(j), đại diện cho tương đồng tích vô hướng trung bình giữa gradient của các mẫu bộ đệm và gradient của các mẫu đào tạo trong mỗi lớp. Biện pháp này hiệu quả nắm bắt mối quan hệ giữa nhiệm vụ mới và các lớp đã thấy trước đó từ góc độ gradient. Chứng minh có thể được tìm thấy trong Phụ lục I.

D. Chi tiết Triển khai

Kiến trúc của Relation Replay Net. Chúng tôi đề xuất một mạng nơ-ron hai lớp ẩn như RRN. Một cách trực quan, lớp đầu tiên bao gồm hai lớp tuyến tính riêng biệt để trích xuất thông tin tương tác của các giá trị mất mát và chuẩn logit cho các mẫu đào tạo ghép cặp, tương ứng. Lớp thứ hai là một lớp tuyến tính hợp nhất thông tin được trích xuất để tạo ra trọng số cho các cặp mẫu tự động. Hình 3 minh họa kiến trúc của RRN trong đó chúng tôi đặt số đơn vị ẩn là 16 để hiệu quả tính toán.

Chi tiết Đào tạo. Khi đào tạo một nhiệm vụ mới, RRN nhằm tạo ra các trọng số mẫu hợp lý cho cả các mẫu nhiệm vụ mới và các mẫu bộ đệm để cải thiện sự đánh đổi giữa 'ổn định' và 'dẻo dai', điều này cần một giai đoạn khởi động để khám phá mối quan hệ giữa các nhiệm vụ mới và cũ. Cụ thể, trong giai đoạn khởi động (độ dài giai đoạn được ký hiệu là Iter warm ), chúng tôi cập nhật RRN bằng Eq. (7) và cập nhật Main Net thông qua các trọng số cố định đặt trước như các phương pháp trước đó. Sau giai đoạn khởi động, chúng tôi chuyển sang sử dụng các trọng số mẫu được tạo ra bởi RRN để hướng dẫn việc đào tạo Main Net thay vì các trọng số cố định trước. Ngoài ra, để giảm gánh nặng tính toán, chúng tôi chỉ cập nhật RRN một lần trong khi cập nhật Main Net cho nhiều bước (số bước được ký hiệu là Interval ). Các lựa chọn cụ thể của Iter warm và Interval được trình bày trong Phần V-D.

V. KẾT QUẢ THÍ NGHIỆM

Để xác thực hiệu quả của phương pháp chúng tôi, chúng tôi tiến hành các thí nghiệm mở rộng trên ba bộ dữ liệu có sẵn công khai, tức là CIFAR-10 , CIFAR-100 [48] và Tiny ImageNet [49], và trình bày phân tích ablation kỹ lưỡng để hiểu rõ phương pháp của chúng tôi trong phần này.

A. Cài đặt Thí nghiệm

Đối với CIFAR-10, chúng tôi chia 10 lớp thành năm nhiệm vụ và mỗi nhiệm vụ là một vấn đề phân loại nhị phân. Cả bộ dữ liệu CIFAR-100 và Tiny ImageNet đều được chia thành 10 nhiệm vụ, trong đó mỗi nhiệm vụ là một vấn đề phân loại 1-của-10 và 1-của-20, tương ứng. Để đảm bảo đánh giá mạnh mẽ và đáng tin cậy, khung được đào tạo trong 50 epoch trên mỗi nhiệm vụ trên tất cả các bộ dữ liệu theo DER++ [26].

Như đã đề cập ở trên, nghiên cứu này chủ yếu tập trung vào các cấu hình của Class-IL và Task-IL. Các cài đặt thí nghiệm chi tiết nhất quán với DER++ [26] để so sánh công bằng. Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá mô hình bằng hai số liệu thường được sử dụng, tức là Average Accuracy (ACC) và Backward Transfer (BWT) [25], [26]. Chúng tôi lặp lại mỗi thí nghiệm năm lần để giảm tính ngẫu nhiên của việc đào tạo mạng. Tương tự như [26], tổng kích thước của bộ đệm bộ nhớ vẫn không đổi trong suốt toàn bộ quá trình đào tạo.

Phương pháp đề xuất được triển khai với nền tảng PyTorch [50]. Backbone của Main Net là ResNet-18 được sử dụng rộng rãi [2]. Trong vòng lặp trong, Main Net được tối ưu hóa bằng SGD với tốc độ học ban đầu là 0.03 cho tất cả các bộ dữ liệu, và trong vòng lặp ngoài, Adam [51] được áp dụng để tối ưu hóa RRN, trong đó tốc độ học ban đầu được đặt là 0.001 với weight decay là 10−4.

B. So sánh với Các Phương pháp Hiện đại

Như đã đề cập trước đó, phương pháp đề xuất của chúng tôi nhằm tăng cường các baseline dựa trên rehearsal chung bằng cách tính đến tình trạng khó xử 'ổn định-dẻo dai', liên quan đến mối quan hệ giữa các nhiệm vụ mới và cũ, và tầm quan trọng cụ thể của các mẫu khác nhau. Chúng tôi tích hợp phương pháp này vào ba baseline đại diện: ER, ER-ACE [29], và DER++ [26], được gọi là Relational-ER (RER), Relational-ER-ACE (RER-ACE), và Relational-DER (RDER), tương ứng. Chi tiết triển khai có thể được tìm thấy trong Phụ lục II-A.

Bảng I trình bày kết quả so sánh giữa thuật toán đề xuất của chúng tôi được áp dụng trên ba baseline và nhiều phương pháp hiện đại khác về số liệu ACC và BWT. Các phương pháp so sánh này bao gồm ba phương pháp dựa trên chính quy hóa: oEWC [20], SI [19], và LwF [18], và bốn phương pháp dựa trên rehearsal: GEM [25], A-GEM [34], iCaRL [24], và GSS [52]. Bên cạnh đó, chúng tôi cũng cung cấp một phương pháp giới hạn trên và một phương pháp giới hạn dưới để tham khảo tốt hơn, trong đó phương pháp trước được đào tạo trên tất cả dữ liệu từ các nhiệm vụ cũ và mới cùng nhau và phương pháp sau được đào tạo trực tiếp trên nhiệm vụ mới mà không có bất kỳ chiến lược nào để ngăn chặn việc quên mô hình.

Đối với CIFAR-10, có thể quan sát thấy rằng phương pháp đề xuất của chúng tôi có thể được thích ứng với các baseline dựa trên rehearsal khác nhau và đạt được cải thiện hiệu suất nhất quán. Ví dụ, RDER đề xuất đạt được đến 3.08% cải thiện hiệu suất tuyệt đối so với baseline DER++ với bộ đệm bộ nhớ 200 dưới Class-IL. Bên cạnh đó, phương pháp của chúng tôi đạt được độ chính xác phân loại cao hơn đáng kể so với tất cả các phương pháp hiện đại so sánh dưới các cài đặt khác nhau. Mặt khác, phương pháp của chúng tôi cũng có thể giảm đáng kể BWT của các baseline, cho thấy rằng phương pháp đề xuất có thể

--- TRANG 6 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 6

BẢNG I
SO SÁNH VỚI BA BASELINE KHÁC NHAU VÀ CÁC PHƯƠNG PHÁP HIỆN ĐẠI KHÁC TRÊN CIFAR-10 VÀ TINY-IMAGE NET. KẾT QUẢ CỦA PHƯƠNG PHÁP CHÚNG TÔI ĐƯỢC HIỂN THỊ TRONG CÁC Ô MÀU XÁM VÀ KẾT QUẢ TỐT HỚN ĐƯỢC TRÌNH BÀY IN ĐẬM.

Kích thước Bộ đệm Phương phápCIFAR-10 Tiny-ImageNet
Class-IL Task-IL Class-IL Task-IL
ACC BWT ACC BWT ACC BWT ACC BWT
- Giới hạn trên 92.20 ±0.15 - 98.31 ±0.12 - 59.99 ±0.19 - 82.04 ±0.10 -
- Giới hạn dưới 19.62 ±0.05 -96.39 ±0.12 61.02 ±3.33 -46.24 ±2.12 7.92 ±0.26 -76.73 ±0.08 18.31 ±0.68 -64.97 ±1.70
- oEWC 19.49 ±0.12 -91.64 ±3.07 68.29 ±3.92 -29.13 ±4.11 7.58 ±0.10 -73.91 ±0.79 19.20 ±0.31 -59.86 ±0.42
- SI 19.48 ±0.17 -95.78 ±0.64 68.05 ±5.91 -38.76 ±0.89 6.58 ±0.31 -67.91 ±0.96 36.32 ±0.13 -53.26 ±0.75
- LwF 19.61 ±0.05 -96.69 ±0.25 63.29 ±2.35 -32.56 ±0.56 8.46 ±0.22 -76.74 ±0.44 15.85 ±0.58 -67.79 ±0.23
200GEM 25.54 ±0.76 -82.61 ±1.60 90.44 ±0.94 - 9.27 ±2.07 - - - -
A-GEM 20.04 ±0.34 -95.73 ±0.20 83.88 ±1.49 -16.39 ±0.80 8.07 ±0.08 -77.02 ±0.22 22.77 ±0.03 -56.61 ±0.32
iCaRL 49.02 ±3.20 -28.72 ±0.49 88.99 ±2.13 - 1.01 ±4.15 7.53 ±0.79 -22.70 ±0.44 28.19 ±1.47 -10.36 ±0.31
GSS 39.07 ±5.59 -75.25 ±4.07 88.80 ±2.89 - 8.56 ±1.78 - - - -
ER 55.84 ±0.71 -47.77 ±1.39 92.41 ±0.59 - 5.73 ±0.38 8.67 ±0.25 -77.29 ±0.26 39.28 ±0.83 -42.05 ±0.29
RER 58.59 ±0.74 -44.50 ±0.80 92.85 ±0.36 - 5.40 ±0.65 9.35 ±0.21 -76.67 ±0.38 40.83 ±0.58 -41.19 ±0.55
ER-ACE 63.02 ±1.29 -20.35 ±1.76 92.59 ±0.36 - 5.33 ±0.41 11.67 ±0.29 -49.03 ±1.61 42.08 ±0.35 -37.71 ±1.10
RER-ACE 63.52 ±0.71 -20.11 ±5.66 92.63 ±0.58 - 5.43 ±0.67 12.18 ±0.41 -48.91 ±2.59 44.11 ±0.62 -36.62 ±1.17
DER++ 62.30 ±1.07 -35.83 ±1.34 90.74 ±1.01 - 7.45 ±1.07 12.26 ±0.31 -68.37 ±1.38 40.47 ±1.53 -40.41 ±1.29
RDER 65.38 ±0.42 -34.16 ±1.90 91.67 ±0.80 - 6.81 ±1.19 13.96 ±0.64 -67.02 ±1.24 40.87 ±0.92 -39.87 ±1.31
500GEM 26.20 ±1.26 -74.31 ±4.62 92.16 ±0.69 - 9.12 ±0.21 - - - -
A-GEM 22.67 ±0.57 -94.01 ±1.16 89.48 ±1.45 -14.26 ±4.18 8.06 ±0.04 -77.06 ±0.41 25.33 ±0.49 -55.68 ±1.01
iCaRL 47.55 ±3.95 -25.71 ±1.10 88.22 ±2.62 - 1.06 ±4.21 9.38 ±1.53 -20.89 ±0.23 31.55 ±3.27 - 7.30 ±0.79
GSS 49.73 ±4.78 -62.88 ±2.67 91.02 ±1.57 - 7.73 ±3.99 - - - -
ER 69.01 ±0.37 -33.02 ±2.62 94.28 ±0.27 - 3.09 ±1.61 10.40 ±0.16 -74.36 ±0.58 48.82 ±0.34 -31.06 ±1.53
RER 69.22 ±1.96 -29.79 ±2.87 94.50 ±0.41 - 3.40 ±0.33 11.50 ±0.47 -74.13 ±0.72 51.28 ±0.93 -30.29 ±1.24
ER-ACE 71.26 ±0.66 -13.37 ±1.06 94.31 ±0.23 - 3.19 ±0.39 19.59 ±0.13 -47.56 ±0.68 50.99 ±0.45 -29.32 ±0.46
RER-ACE 71.29 ±1.15 -12.53 ±2.41 94.25 ±0.23 - 3.16 ±0.80 20.41 ±0.66 -42.22 ±1.09 54.62 ±0.87 -25.15 ±0.98
DER++ 72.11 ±1.41 -23.40 ±1.32 94.21 ±0.32 - 3.98 ±0.60 19.29 ±1.14 -60.58 ±0.46 51.39 ±0.91 -26.90 ±0.52
RDER 73.99 ±1.03 -22.86 ±1.76 94.04 ±0.43 - 3.82 ±0.59 20.06 ±1.18 -56.16 ±1.38 52.56 ±0.69 -25.02 ±0.24
5120GEM 25.26 ±3.46 -75.27 ±4.41 95.55 ±0.02 - 6.91 ±2.33 - - - -
A-GEM 21.99 ±2.29 -84.49 ±3.08 90.10 ±2.09 - 9.89 ±0.40 7.96 ±0.13 -76.01 ±0.52 26.22 ±0.65 -55.61 ±0.84
iCaRL 55.07 ±1.55 -24.94 ±0.14 92.23 ±0.84 - 0.99 ±1.41 14.08 ±1.92 -16.00 ±0.28 40.83 ±3.11 - 2.60 ±0.35
GSS 67.27 ±4.27 -58.11 ±9.12 94.19 ±1.15 - 6.38 ±1.71 - - - -
ER 83.30 ±0.50 -13.79 ±1.40 96.95 ±0.15 - 0.98 ±0.36 28.52 ±0.37 -52.54 ±1.45 68.46 ±0.40 -10.13 ±0.20
RER 83.53 ±0.55 -12.13 ±1.29 96.98 ±0.17 - 0.79 ±0.24 33.86 ±0.64 -45.56 ±2.13 69.31 ±0.41 -10.68 ±0.25
ER-ACE 82.98 ±0.38 - 3.99 ±0.61 96.76 ±0.08 - 0.63 ±0.30 37.02 ±0.17 -33.29 ±1.03 68.69 ±0.19 - 9.88 ±0.42
RER-ACE 83.74 ±0.79 - 4.05 ±1.81 96.80 ±0.20 - 0.57 ±0.27 36.97 ±0.94 -33.79 ±3.07 69.05 ±0.73 - 9.51 ±0.79
DER++ 84.50 ±0.63 - 9.79 ±0.34 95.91 ±0.57 - 1.57 ±0.25 37.88 ±0.37 -30.62 ±1.78 68.05 ±0.53 - 8.80 ±0.24
RDER 85.56 ±0.38 - 8.81 ±0.71 96.21 ±0.22 - 1.42 ±0.09 39.67 ±0.96 -29.37 ±1.53 68.82 ±0.54 - 8.03 ±0.46

BẢNG II
SO SÁNH ACC VỚI BA BASELINE KHÁC NHAU TRÊN CIFAR-100. KẾT QUẢ CỦA PHƯƠNG PHÁP CHÚNG TÔI ĐƯỢC HIỂN THỊ TRONG CÁC Ô MÀU XÁM VÀ KẾT QUẢ TỐT HƠN ĐƯỢC TRÌNH BÀY IN ĐẬM.

Cài đặt Kích thước Bộ đệm ER RER ER-ACE RER-ACE DER++ RDER
100 11.31 ±0.21 13.94 ±0.89 18.59 ±1.08 20.20 ±0.86 14.98 ±0.65 20.79 ±1.05
200 14.78 ±0.40 16.40 ±0.53 25.14 ±1.83 26.64 ±0.29 24.17 ±1.37 30.65 ±0.76
500 23.10 ±0.32 26.97 ±0.75 36.02 ±0.84 36.06 ±1.14 35.19 ±1.30 39.50 ±1.54Class-IL
5120 51.43 ±1.01 54.08 ±0.63 53.93 ±2.04 54.38 ±1.08 55.58 ±1.86 60.07 ±0.23
100 58.64 ±1.31 59.77 ±0.03 59.91 ±1.01 61.10 ±0.93 58.32 ±1.27 59.07 ±0.73
200 66.31 ±0.76 66.83 ±0.97 64.81 ±3.14 67.42 ±0.60 66.47 ±0.57 68.60 ±0.51
500 73.10 ±0.99 73.99 ±0.51 74.13 ±0.84 74.54 ±1.37 74.10 ±1.62 75.59 ±0.85Task-IL
5120 86.16 ±0.47 85.35 ±0.34 84.69 ±1.32 85.15 ±0.41 86.23 ±2.18 86.54 ±0.31

hiệu quả giảm việc quên mô hình trong khi cải thiện độ chính xác phân loại, tức là đạt được sự cân bằng tốt hơn giữa 'ổn định' và 'dẻo dai'.

Đối với bộ dữ liệu Tiny-ImageNet, phương pháp của chúng tôi cũng nhất quán đạt được kết quả tốt nhất dưới hầu hết tất cả các cài đặt. Mặc dù iCaRL đạt được số liệu BWT tốt, nó thiếu về mặt ACC, tiết lộ rằng một mô hình chú ý nhiều đến việc tránh quên có thể tác động tiêu cực đến hiệu suất phân loại. Trái lại, phương pháp của chúng tôi cải thiện cả ACC và BWT so với các baseline tương ứng, làm nổi bật rằng phương pháp của chúng tôi có thể đạt được sự đánh đổi tốt hơn giữa các nhiệm vụ mới và cũ. Đáng chú ý, kết quả của GEM và GSS không được báo cáo trong Bảng I vì chi phí tính toán quá mức không thể chấp nhận được.

Hơn nữa, chúng tôi xác thực hiệu quả của phương pháp chúng tôi trên CIFAR-100 trong Bảng II. Rõ ràng, phương pháp của chúng tôi có thể đạt được cải thiện đáng kể cho tất cả ba baseline này trên các kích thước bộ đệm khác nhau. Ví dụ, trong cài đặt Class-IL với kích thước bộ đệm 100, phương pháp của chúng tôi áp dụng cho ER, ER-ACE, và DER có thể cải thiện ACC của chúng lần lượt 2.63%, 1.61%, và 5.81%. Những kết quả này tiếp tục chứng minh khả năng thích ứng mạnh mẽ của phương pháp chúng tôi với nhiều bộ dữ liệu và các cài đặt đa dạng.

--- TRANG 7 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 7

(a) CIFAR-10 ER/RER
 (b) CIFAR-10 ER-ACE/RER-ACE
 (c) CIFAR-10 DER++/RDER

(d) CIFAR-100 ER/RER
 (e) Tiny-ImageNet ER/RER

Hình 4. Độ chính xác phân loại (%) của mỗi nhiệm vụ trong suốt toàn bộ quá trình đào tạo. Các đường chấm-liền đại diện cho các phương pháp so sánh ER, ER-ACE, hoặc DER++ và các đường tam giác-gạch ngang đại diện cho các phương pháp của chúng tôi RER, RER-ACE, hoặc RDER.

C. Thảo luận

Để khám phá thêm phương pháp đề xuất, chúng tôi tiến hành thêm các thí nghiệm và phân tích trong phần này.

Phương pháp của chúng tôi có giảm thiểu việc quên mô hình không? Để phân tích tốt hơn vấn đề quên cho các nhiệm vụ cũ, chúng tôi trực quan hóa sự thay đổi độ chính xác cho mỗi nhiệm vụ trong quá trình học liên tục trong Hình 4 và Phụ lục II-C. Kết quả trực quan hóa cho thấy phương pháp của chúng tôi giảm thiểu tốt hơn việc quên cho các nhiệm vụ trước đó, tức là củng cố kiến thức cho các nhiệm vụ cũ tốt hơn so với các baseline tương ứng. Lưu ý rằng hiệu suất của RDER thấp hơn một chút so với DER++ cho một số nhiệm vụ mới (được hiển thị trong Hình 4c). Điều này là do phương pháp của chúng tôi nhằm cải thiện khả năng tổng quát hóa của tất cả các nhiệm vụ, không chỉ chú ý đến các nhiệm vụ mới với nhiều dữ liệu hơn.

Tương tự nhiệm vụ ảnh hưởng như thế nào đến các mô hình học liên tục? Trong phần này, chúng tôi nghiên cứu thêm tác động của tương tự nhiệm vụ đến việc tạo ra trọng số mẫu. Từ CIFAR-10, chúng tôi chọn hai lớp thuộc về danh mục 'object', cụ thể là 'ship' và 'truck', làm nhiệm vụ 1. Đối với nhiệm vụ 2, chúng tôi xem xét hai thiết lập riêng biệt: Thiết lập 1 , bao gồm 'airplane' và 'automobile', và Thiết lập 2 , bao gồm 'cat' và 'horse'. Đáng chú ý, Thiết lập 1 là một nhiệm vụ tương đối liên quan ngữ nghĩa vì các lớp của nó cũng thuộc về danh mục 'object'. Ngược lại, cả hai lớp trong Thiết lập 2 đều thuộc về danh mục 'animals', làm cho nó trở thành một nhiệm vụ tương đối không liên quan ngữ nghĩa với tương tự thấp hơn với nhiệm vụ 1. Ở đây chúng tôi tập trung vào baseline ER và RER đề xuất của chúng tôi, vì ER sử dụng cùng một hàm mất mát ( tức là mất mát CE) cho cả các nhiệm vụ mới và cũ, và các trọng số mẫu tương ứng trực tiếp phản ánh đóng góp tương ứng của chúng vào việc đào tạo Main Net2.

Để giảm thiểu tác động của các yếu tố ngẫu nhiên, chúng tôi đào tạo ER/RER 10 lần và báo cáo ACC dưới Thiết lập 1 và Thiết lập 2 trong Hình 7(a). ACC của Thiết lập 1 thấp hơn đáng kể so với Thiết lập 2 cho cả ER và RER, ngụ ý rằng kiến thức được trích xuất từ nhiệm vụ mới trong Thiết lập 1 tương tự với nhiệm vụ cũ, điều này có thể can thiệp vào việc phân loại của nhiệm vụ cũ. Hơn nữa, RER đề xuất của chúng tôi thể hiện khoảng cách hiệu suất nhỏ hơn giữa Thiết lập 2 và Thiết lập 1 so với ER, cho thấy rằng phương pháp của chúng tôi có thể hiệu quả trích xuất mối quan hệ nhiệm vụ và giảm thiểu sự can thiệp từ các nhiệm vụ mới và dẫn đến cải thiện hiệu suất tổng thể.

Hơn nữa, vào cuối nhiệm vụ 2 trong RER, chúng tôi tính toán các trọng số trung bình cho các mẫu xác thực từ các nhiệm vụ cũ và mới, tương ứng. Hình 7(b) hiển thị kết quả cho 10 lần chạy khác nhau của RER dưới Thiết lập 1 (được đại diện bởi các điểm rải màu 'dark orange') và Thiết lập 2 (được đại diện bởi các điểm rải màu 'dodgerblue'). Rõ ràng rằng dưới Thiết lập 1 , RRN có xu hướng tạo ra các trọng số lớn hơn cho các mẫu nhiệm vụ cũ và các trọng số nhỏ hơn cho các mẫu nhiệm vụ mới. Điều này gợi ý rằng đối với một nhiệm vụ mới tương tự, nhiệm vụ cũ quan trọng hơn trong việc cải thiện việc quên vì các nhiệm vụ tương tự có thể tạo ra các đặc trưng tương tự

2Ở đây chúng tôi loại trừ DER++ và ER-ACE khỏi phân tích của chúng tôi vì chúng áp dụng các hàm mất mát khác nhau và có thể đưa vào các nguồn can thiệp bổ sung.

--- TRANG 8 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 8

Phương pháp Thiết lập 1 Thiết lập 2 Khoảng cách
ER 81.38 ±1.67 88.29 ±0.89 6.91
RER 84.32 ±0.81 89.41 ±1.60 5.09

Hình 5. *
(a) ACC của ER và RER dưới Thiết lập 1 (nhiệm vụ liên quan ngữ nghĩa) và Thiết lập 2 (nhiệm vụ không liên quan ngữ nghĩa). Khoảng cách trong cột cuối cho thấy sự khác biệt giữa ACC của Thiết lập 1 và Thiết lập 2.

Hình 6. *
(b) Trực quan hóa các trọng số mẫu trung bình của nhiệm vụ mới (trục ngang) và nhiệm vụ cũ (trục dọc) của Thiết lập 1 và Thiết lập 2.

Hình 7. Trực quan hóa hiệu ứng tương tự nhiệm vụ.

sẽ can thiệp lẫn nhau. Mặt khác, dưới Thiết lập 2 , các trọng số mẫu trung bình của các nhiệm vụ cũ và mới phân tán hơn giữa 0 và 1, cho thấy rằng các nhiệm vụ mới không tương tự (tức là không liên quan ngữ nghĩa) có thể không can thiệp mạnh mẽ với các nhiệm vụ cũ và các trọng số mẫu cho mỗi nhiệm vụ nên được xác định theo thời gian thực trong suốt toàn bộ quá trình đào tạo.

Tại sao chúng ta cần tối ưu hóa hai cấp? Để đánh giá hiệu quả của mô hình tối ưu hóa hai cấp, chúng tôi tiến hành một nghiên cứu ablation bằng đào tạo end-to-end Main Net và RRN cùng nhau dựa trên DER++ (được gọi là Vanilla). Cụ thể, chúng tôi sử dụng cùng một RRN để tạo ra các trọng số mẫu ghép cặp và cập nhật Main Net và RRN bằng một bước lan truyền ngược thông qua Ltr+Lbf. Trong Bảng III, chúng tôi quan sát thấy rằng các trọng số mẫu được tạo ra bởi phương pháp Vanilla dường như không có thông tin, và thậm chí làm hỏng hiệu suất của phương pháp baseline (DER++), cho thấy rằng mô hình đào tạo end-to-end không thể tạo ra các trọng số có ý nghĩa để tăng cường khả năng tổng quát hóa của Main Net. Trái lại, phương pháp RDER của chúng tôi nhất quán vượt trội so với cả DER++ và phương pháp Vanilla, qua đó chứng minh hiệu quả của khung tối ưu hóa hai cấp đề xuất để đạt được sự đánh đổi tốt hơn giữa 'ổn định' và 'dẻo dai'.

Tại sao sử dụng bộ đệm bộ nhớ để đào tạo Relation Replay Net trong vòng lặp ngoài? Trong Eq. (4) của tối ưu hóa vòng lặp ngoài, chúng tôi sử dụng bộ đệm bộ nhớ M để đào tạo RRN. Tuy nhiên, những lo ngại phát sinh về khả năng overfitting khi sử dụng bộ đệm bộ nhớ để hướng dẫn RRN. Để giải quyết điều này, chúng tôi điều tra hai phương pháp để xây dựng tập đào tạo vòng lặp ngoài

BẢNG III
ACC TRÊN BỘ DỮ LIỆU CIFAR-10 CỦA ĐÀO TẠO VANILLA VÀ KHUNG TỐI ƯU HÓA HAI CẤP ĐỀ XUẤT CỦA CHÚNG TÔI.

Kích thước
Bộ nhớPhương pháp Class-IL Task-IL
200DER++ [26] 62.30 ±1.07 90.74 ±1.01
RDER (của chúng tôi) 65.38±0.42 91.67±0.80
Vanilla 62.85 ±2.90 91.20 ±1.88
500DER++ [26] 72.11 ±1.41 94.21 ±0.32
RDER (của chúng tôi) 73.99±1.03 94.04±0.43
Vanilla 72.28 ±0.93 93.42 ±0.75
5120DER++ [26] 84.50 ±0.63 95.91 ±0.57
RDER (của chúng tôi) 85.56±0.38 96.21±0.22
Vanilla 82.26 ±2.45 95.3 ±0.72

BẢNG IV
ACC TRÊN BỘ DỮ LIỆU CIFAR-10 CỦA CÁC CÁCH KHÁC NHAU ĐỂ XÂY DỰNG TẬP ĐÀO TẠO TRONG VÒNG LẶP NGOÀI.

Kích thước
Bộ nhớPhương pháp Class-IL Task-IL
200DER++ [26] 62.30 ±1.07 90.74 ±1.01
RDER (của chúng tôi) 65.38±0.42 91.67±0.80
Split RDER 62.53 ±0.66 91.36 ±0.77
500DER++ [26] 73.11 ±1.41 94.21 ±0.32
RDER (của chúng tôi) 73.99±1.03 94.04±0.43
Split RDER 72.57 ±0.73 93.77 ±0.33
5120DER++ [26] 84.50 ±0.63 95.91 ±0.57
RDER (của chúng tôi) 85.56±0.38 96.21±0.22
Split RDER 85.35 ±0.24 96.19 ±0.46

: 1) Chia bộ đệm bộ nhớ M thành hai tập, được sử dụng để đào tạo Main Net trong vòng lặp trong và RRN trong vòng lặp ngoài, tương ứng [53]. 2) Hoặc, đào tạo RRN trong vòng lặp ngoài sử dụng toàn bộ bộ đệm bộ nhớ M, như chúng tôi đề xuất.

Chúng tôi trình bày so sánh giữa hai phương pháp dựa trên DER++ và báo cáo kết quả trong Bảng IV, trong đó hai phương pháp được ký hiệu là 'Split RDER' và 'RDER', tương ứng. Split RDER chia bộ đệm bộ nhớ thành hai tập cho đào tạo vòng lặp ngoài và trong với tỷ lệ 20%−80%, theo [53], trong khi phương pháp RDER không liên quan đến việc chia như vậy. Kết quả trong Bảng IV cho thấy Split RDER thể hiện cải thiện nhẹ so với baseline DER++, nhưng hiệu suất của nó vẫn thấp hơn RDER đề xuất của chúng tôi. Những phát hiện này gợi ý rằng phương pháp đầu tiên có thể dẫn đến khả năng tổng quát hóa tốt hơn của RRN trong khi giảm số lượng mẫu bộ nhớ được sử dụng để đào tạo Main Net. Tuy nhiên, việc giảm này trong các mẫu đào tạo có thể tạo ra sự mất cân bằng nghiêm trọng hơn giữa các nhiệm vụ mới và cũ, qua đó giảm hiệu suất tổng thể, đặc biệt khi kích thước bộ đệm bị hạn chế.

D. Nghiên cứu Ablation

Trong phần này, trước tiên chúng tôi xác thực hiệu ứng của các kích thước bộ đệm nhỏ trên phương pháp của chúng tôi dựa trên ba baseline. Và sau đó chúng tôi tiến hành một nghiên cứu ablation chi tiết dựa trên RDER để đánh giá ảnh hưởng gây ra bởi hai siêu tham số quan trọng Iter warm và Interval trên CIFAR-10 với các kích thước bộ đệm khác nhau.

Hiệu ứng của Kích thước Bộ đệm Nhỏ. Để nghiên cứu thêm tác động của các kích thước bộ đệm nhỏ đến phương pháp của chúng tôi, chúng tôi đánh giá phương pháp của chúng tôi áp dụng cho ba baseline trên CIFAR-10 trong Bảng V.

--- TRANG 9 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 9

BẢNG V
SO SÁNH ACC TRÊN CIFAR-10 VỚI CÁC KÍCH THƯỚC BỘ ĐỆM NHỎ. KẾT QUẢ CỦA PHƯƠNG PHÁP CHÚNG TÔI ĐƯỢC HIỂN THỊ TRONG CÁC Ô MÀU XÁM VÀ KẾT QUẢ TỐT HƠN ĐƯỢC TRÌNH BÀY IN ĐẬM.

Cài đặt Kích thước Bộ đệm ER RER ER-ACE RER-ACE DER++ RDER
50 36.47 ±2.92 37.43 ±1.27 42.16 ±1.78 41.95 ±1.07 46.70 ±4.14 49.75 ±1.92Class-IL100 46.75 ±1.58 50.85 ±1.14 56.96 ±2.39 57.98 ±3.02 53.82 ±1.21 56.36 ±1.54
50 88.13 ±1.16 88.07 ±0.80 87.09 ±1.05 86.45 ±1.05 84.00 ±0.84 85.81 ±1.99Task-IL100 90.27 ±1.05 90.56 ±0.41 90.28 ±0.38 91.01 ±0.04 87.48 ±1.43 89.16 ±1.37

Cụ thể, RDER tăng cường DER++ khoảng +3.05% với kích thước bộ đệm 50 dưới cài đặt Class-IL, chứng minh rằng phương pháp của chúng tôi có thể khám phá thêm thông tin trong bộ đệm bộ nhớ để tăng cường hiệu suất tổng thể của Main Net.

Tác động của Giai đoạn Khởi động ( Iter warm ).Bảng VI trình bày kết quả đánh giá cho các Iter warm khác nhau dưới ba cài đặt khác nhau 1
3,1
2,2
3
×Iter max. Hiệu suất của RDER giảm đáng kể khi sử dụng giá trị Iter warm nhỏ (tức là 17), gợi ý rằng RRN yêu cầu số lượng bước khởi động đầy đủ để tạo ra các trọng số mẫu có ý nghĩa để nắm bắt chính xác mối quan hệ theo nhiệm vụ và tầm quan trọng mẫu trong mỗi nhiệm vụ. Mặt khác, có thể quan sát thấy một sự giảm nhẹ trong hiệu suất dưới Iter warm = 33, vì các trọng số đặt trước có thể dẫn đường sai trong giai đoạn khởi động. Một giai đoạn khởi động quá dài cũng dẫn đến các lần lặp không đủ cho việc đào tạo Main Net được hướng dẫn bởi các trọng số được tạo ra. Do đó, chúng tôi khuyến nghị đặt Iter warm là một nửa số epoch lặp cho mỗi nhiệm vụ, điều này hoạt động tốt nhất trong Bảng VI.

Tác động của Khoảng cách Cập nhật Relation Replay Net (Interval ).Ở đây chúng tôi thay đổi giá trị của Interval để điều tra tác động của nó đến khung của chúng tôi. Như được hiển thị trong Bảng VII, cài đặt Interval = 5 mang lại độ chính xác phân loại cao nhất trên một loạt các kích thước bộ đệm bộ nhớ. Tuy nhiên, Interval nhỏ, chẳng hạn như Interval = 1 , thường dẫn đến giảm hiệu suất do sự xen kẽ thường xuyên giữa việc cập nhật Main Net và RRN, dẫn đến dao động trong quá trình đào tạo. Ngoài ra, việc cập nhật RRN thường xuyên tốn kém về mặt tính toán, có thể cản trở sự hội tụ của toàn bộ khung. Ngược lại, Interval lớn, chẳng hạn như Interval = 10 , có thể dẫn đến tính toán nhanh hơn, nhưng chúng tôi quan sát thấy một sự giảm đáng kể trong hiệu suất do việc đào tạo RRN không đầy đủ, có thể dẫn đến việc tạo ra các trọng số mẫu không tối ưu. Để tạo ra sự cân bằng giữa độ chính xác và hiệu quả tính toán, chúng tôi đề xuất một công thức thực nghiệm Interval = #epoch/ 10, trong đó #epoch đại diện cho số epoch lặp cho mỗi nhiệm vụ.

VI. KẾT LUẬN

Trong bài báo này, chúng tôi tập trung vào tình trạng khó xử 'ổn định-dẻo dai' trong học liên tục và cố gắng điều chỉnh thích ứng mối quan hệ giữa các nhiệm vụ và mẫu khác nhau. Để đạt được mục tiêu này, chúng tôi đề xuất một khung học liên tục mới, Relational Experience Replay, điều chỉnh theo cặp các trọng số mẫu của các mẫu từ các nhiệm vụ mới và bộ đệm bộ nhớ. Các trọng số mẫu được tạo ra bởi Relation Replay Net có thể tạo điều kiện cho việc tối ưu hóa Main Net để đạt được sự đánh đổi tốt hơn

BẢNG VI
ACC TRÊN CIFAR-10 VỚI CHIỀU DÀI KHÁC NHAU CỦA GIAI ĐOẠN KHỞI ĐỘNG (Iterwarm ).

Kích thước
Bộ nhớIterwarm Class-IL Task-IL
5017 48.02 ±0.70 85.13 ±1.27
25 49.75 ±1.92 85.81 ±1.99
33 47.41 ±0.91 85.12 ±1.35
20017 64.53 ±1.13 91.84 ±0.29
25 65.38 ±0.42 91.67 ±0.80
33 64.84 ±1.08 92.88 ±0.42
50017 72.17 ±1.11 93.57 ±0.17
25 73.99 ±1.03 94.04 ±0.43
33 73.03 ±1.49 93.85 ±0.44

BẢNG VII
ACC TRÊN CIFAR-10 VỚI CÁC GIÁ TRỊ KHÁC NHAU CỦA KHOẢNG CÁCH CẬP NHẬT RELATION REPLAY NET (Interval ).

Kích thước
Bộ nhớInterval Class-IL Task-IL
501 48.12 ±0.51 84.44 ±1.48
5 49.75 ±1.92 85.81 ±1.99
10 46.21 ±1.00 85.18 ±1.65
2001 64.27 ±1.46 91.57 ±0.59
5 65.38 ±0.42 91.67 ±0.80
10 64.66 ±0.29 91.71 ±0.62
5001 71.23 ±0.99 93.15 ±0.68
5 73.99 ±1.03 94.04 ±0.43
10 72.11 ±0.46 93.62 ±0.23

giữa 'ổn định' và 'dẻo dai'. Phương pháp đề xuất có thể dễ dàng tích hợp với nhiều phương pháp dựa trên rehearsal để đạt được cải thiện đáng kể. Chúng tôi xác minh về mặt lý thuyết và thực nghiệm rằng các trọng số mẫu được tạo ra có thể trích xuất mối quan hệ giữa các nhiệm vụ mới và cũ để tự động điều chỉnh việc đào tạo Main Net và tăng cường hiệu suất tổng thể. Chúng tôi hy vọng rằng phương pháp của chúng tôi có thể cung cấp nhiều hiểu biết hơn về tình trạng khó xử 'ổn định-dẻo dai' và thúc đẩy sự phát triển của lĩnh vực học liên tục.

PHỤ LỤC I
CHI TIẾT TÍNH TOÁN VỀ CẬP NHẬT RELATION REPLAY NET

Trong phần này, chúng tôi cung cấp tính toán chi tiết về các đạo hàm của RRN. Tham khảo lại Phần IV-B, gradient descent của các tham số RRN được đưa ra trong Eq. (7) và lặp lại ở đây như

ϕk+1=ϕk−ηϕ▽ϕLbf(Bbf;θ(ϕ)), (10)

trong đó θ(ϕ) là các tham số cập nhật một bước được tạo ra bởi Eq. (6).

--- TRANG 10 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 10

Ở đây chúng ta có thể sử dụng quy tắc chuỗi để tính toán đạo hàm của ϕ như sau.

▽ϕLbf(Bbf;θk(ϕ))
=1
BBX
i=1▽ϕLbf(xi;θk(ϕ))
=1
BBX
i=1∂Lbf(xi;θ)
∂θ
θk∂θ(ϕ)
∂ϕ
ϕk,(11)

trong đó số hạng thứ hai có thể được biểu diễn như

∂θ(ϕ)
∂ϕ
ϕk=−ηθ
BBX
j=1
▽θLtr(xD
j;θk)·∂λD
j(ϕ)
∂ϕ
ϕk
+▽θLtr(xM
j;θk)·∂λM
j(ϕ)
∂ϕ
ϕk
.(12)

Sau đó thay thế Eq. (12) vào Eq. (11) và trao đổi thứ tự của hai tổng, chúng ta có thể có

▽ϕLbf(Bbf;θk(ϕ))
=−ηθ
BBX
j=1
1
BBX
i=1∂Lbf(xi;θ)
∂θ
θk·∂Ltr(xD
j;θ)
∂θ
θk·∂λD
j(ϕ)
∂ϕ
ϕk
+1
BBX
i=1∂Lbf(xi;θ)
∂θ
θk·∂Ltr(xM
j;θ)
∂θ
θk·∂λM
j(ϕ)
∂ϕ
ϕk

=−ηθ
BBX
j=1
GD(j)·∂λD
j(ϕ)
∂ϕ
ϕk+GM(j)·∂λM
j(ϕ)
∂ϕ
ϕk

=−ηθ
BBX
j=1G(j)∂h(ϕ)
∂ϕ
ϕk,
(13)

trong đó số hạng cuối cùng trong Eq. (13) là đạo hàm của các đầu ra RRN hj(ϕ) của cặp mẫu đào tạo thứ j đối với các tham số mạng ϕ, đó là

∂h(ϕ)
∂ϕ=
∂λD
j(ϕ)
∂ϕ
ϕk
∂λM
j(ϕ)
∂ϕ
ϕk
, (14)

và các hệ số G(j) =
GD(j)GM(j)
, trong đó

GD(j) =1
BBX
i=1∂Lbf(xi;θ)
∂θ
θk·∂Ltr(xD
j;θ)
∂θ
θk
≜1
BBX
i=1gbf(xi)·gtr(xD
j),
GM(j) =1
BBX
i=1∂Lbf(xi;θ)
∂θ
θk·∂Ltr(xM
j;θ)
∂θ
θk
≜1
BBX
i=1gbf(xi)·gtr(xM
j),(15)

tương ứng. Ký hiệu gradient của mất mát meta của mẫu thứ i của Dbf là gbf(xi) =∂Lbf(xi;θ)
∂θ
θk, và gradient của mất mát đào tạo trên cặp mẫu thứ j của Dtr là gtr(xD
j) =∂Ltr(xD
j;θ)
∂θ
θk và gtr(xM
j) =∂Ltr(xM
j;θ)
∂θ
θk. Rõ ràng, hệ số G(j) đại diện cho sự tương tự giữa gradient của mất mát đào tạo
gtr(xM
j)gtr(xD
j)
 và trung bình của gradient của mất mát meta gbf(xi). Hơn nữa, chúng ta có thể công thức hóa lại gradient trung bình của mất mát meta theo lớp. Hệ số G(j) có thể được biểu diễn như:

GD(j) =1
BCtX
c=1 BcX
i=1gbf(xi)!
gtr(xD
j),
GM(j) =1
BCtX
c=1 BcX
i=1gbf(xi)!
gtr(xM
j),(16)

trong đó Ct là số lượng tất cả các lớp đã thấy, Bc là số lượng mẫu của mỗi lớp trong một batch, và PCt
c=1Bc=B. Công thức này cho thấy rằng các hệ số G(j) mô hình hóa ngầm mối quan hệ giữa kiến thức được trích xuất từ mỗi mẫu đào tạo của nhiệm vụ mới và kiến thức từ các mẫu meta trung bình.

Sau đó số hạng đạo hàm đầu tiên trong Eq. (13) có thể được biểu diễn như:

▽ϕLbf(Dbf;θ(ϕ)) =−ηθ
BBX
i=1G(j)·∂h(ϕ)
∂ϕ
ϕk. (17)

Do đó, gradient của các tham số RRN ϕ có thể được tính toán bằng Eq. (8), có thể dễ dàng thực hiện trong PyTorch [50] với việc tự động phân biệt.

PHỤ LỤC II
CHI TIẾT THÍ NGHIỆM THÊM

Trong phần này, trước tiên chúng tôi minh họa một số chi tiết thí nghiệm và sau đó trình bày một số kết quả bổ sung để chứng minh thêm hiệu quả của phương pháp chúng tôi.

A. Cách áp dụng Phương pháp Đề xuất cho Các Baseline Khác?

Trong phần chính của bài báo, chúng tôi lấy ER làm ví dụ để minh họa cách phương pháp đề xuất của chúng tôi giúp các mô hình học liên tục dựa trên rehearsal giải quyết tình trạng khó xử 'ổn định-dẻo dai'. Ở đây chúng tôi trình bày cách áp dụng phương pháp đề xuất của chúng tôi cho các baseline khác, tức là ER-ACE [29], và DER++ [26].

a) Relational-ER-ACE (RER-ACE): ER-ACE [29], đại diện cho 'Experience Replay with Asymmetric Cross-Entropy', kết hợp các mất mát của các mẫu nhiệm vụ mới và các mẫu bộ đệm bộ nhớ như

Ltr(θ) =1
BBX
i=1λDLCE(xD
i;θ, Ccurr)
+λMLCE(xM
i;θ, Ccurr∪Ccurr),(18)

trong đó các siêu tham số ΛD và ΛM được đặt trước là 1 trong [29]. LCE(D;C) được định nghĩa như:

LCE(x;θ, C) =−logzc(x)P
c′∈Czc′(x), (19)

trong đó mẫu x thuộc về lớp thứ c và zc(x) là phần tử thứ c của đầu ra mạng phân loại chính f(x;θ). Rõ ràng, việc áp dụng phương pháp đề xuất của chúng tôi vào ER-ACE là đơn giản, trong đó RRN vẫn tạo ra các trọng số [ΛD,ΛM] cho mỗi cặp mẫu đào tạo. Tối ưu hóa vòng lặp ngoài giống như Eq. (4) và các trọng số mẫu được tạo ra được áp dụng cho tối ưu hóa vòng lặp trong Eq. (18).

--- TRANG 11 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 11

b) Relational-DER (RDER): Hàm mất mát của DER++ được hiển thị trong Eq. (2), liên quan đến ba siêu tham số [λD, λM, γM]. Một cách trực quan, hàm mất mát vòng lặp ngoài cho RRN trong Eq. (4) có thể được công thức hóa lại như:

Lbf(x;θ) =Lbf
CE(x;θ) +Lbf
KD(x;θ), (20)

là tổng của mất mát CE và mất mát chưng cất.

B. Các Siêu tham số Khác

Trong giai đoạn khởi động ( tức là các epoch trước Iter warm ), chúng tôi sử dụng các trọng số đặt trước trong tối ưu hóa vòng lặp trong giống như các phương pháp trước đó. Cụ thể, đối với RER và RER-ACE, trọng số cho mất mát CE của các mẫu nhiệm vụ mới và cũ được đặt trước lần lượt là 1 và 0.5. Và đối với RDER, các trọng số đặt trước lần lượt là 1, 0.5, và 0.2 cho mất mát CE của các mẫu nhiệm vụ mới LCE(Dt), mất mát CE của các mẫu bộ đệm bộ nhớ LCE(Mt), và mất mát chưng cất của các mẫu bộ đệm bộ nhớ LKD(Mt).

C. Kết quả Trực quan hóa Bổ sung

Ở đây chúng tôi hiển thị độ chính xác phân loại của mỗi nhiệm vụ của ER-ACE/RER-ACE và DER++/RDER trên CIFAR-100 trong Hình 8(a) và Hình 8(c), và trên Tiny ImageNet trong Hình 8(b) và Hình 8(d). Tương tự như Hình 8, phương pháp của chúng tôi rõ ràng cải thiện độ chính xác của các nhiệm vụ trước đó. Bên cạnh đó, để cân bằng 'ổn định' và 'dẻo dai', RDER đạt được hiệu suất tổng thể cao hơn mặc dù có thể không vượt trội so với DER++ trên một số nhiệm vụ mới trong Hình 8(c). Tất cả những kết quả này tiếp tục chứng minh hiệu quả của phương pháp chúng tôi, có thể dễ dàng thích ứng với nhiều phương pháp dựa trên rehearsal.

TÀI LIỆU THAM KHẢO
[1] K. Simonyan và A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556 , 2014.
[2] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong IEEE Conference on Computer Vision and Pattern Recognition , 2016, pp. 770–778.
[3] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, và K. Keutzer, "DenseNet: Implementing efficient ConvNet descriptor pyramids," arXiv preprint arXiv:1404.1869 , 2014.
[4] H. Zhao, J. Shi, X. Qi, X. Wang, và J. Jia, "Pyramid scene parsing network," trong Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp. 2881–2890.
[5] K. He, G. Gkioxari, P. Doll ́ar, và R. Girshick, "Mask R-CNN," trong Proceedings of the IEEE International Conference on Computer Vision , 2017, pp. 2961–2969.
[6] A. Douillard, Y . Chen, A. Dapogny, và M. Cord, "PLOP: Learning without forgetting for continual semantic segmentation," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4040–4050.

--- TRANG 12 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 12

(a) CIFAR-100 ER-ACE/RER-ACE
 (b) Tiny-ImageNet ER-ACE/RER-ACE

(c) CIFAR-100 DER++/RDER
 (d) Tiny-ImageNet DER++/RDER

Hình 8. Độ chính xác phân loại (%) của mỗi nhiệm vụ trong toàn bộ quá trình đào tạo. Các đường chấm-liền đại diện cho các phương pháp so sánh ER, ER-ACE, hoặc DER++ và các đường tam giác-gạch ngang đại diện cho các phương pháp của chúng tôi RER, RER-ACE, hoặc RDER.

[7] S. Ren, K. He, R. Girshick, và J. Sun, "Faster R-CNN: Towards real-time object detection with region proposal networks," Advances in Neural Information Processing Systems , vol. 28, 2015.
[8] J. Redmon và A. Farhadi, "YOLOv3: An incremental improvement," arXiv preprint arXiv:1804.02767 , 2018.
[9] A. Bochkovskiy, C.-Y . Wang, và H.-Y . M. Liao, "YOLOv4: Optimal speed and accuracy of object detection," arXiv preprint arXiv:2004.10934 , 2020.
[10] M. McCloskey và N. J. Cohen, "Catastrophic interference in connectionist networks: The sequential learning problem," trong Psychology of Learning and Motivation . Elsevier, 1989, vol. 24, pp. 109–165.
[11] M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, và T. Tuytelaars, "A continual learning survey: Defying forgetting in classification tasks," IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 44, no. 7, pp. 3366–3385, 2021.
[12] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, và R. Hadsell, "Progressive neural networks," arXiv preprint arXiv:1606.04671 , 2016.
[13] C. Fernando, D. Banarse, C. Blundell, Y . Zwols, D. Ha, A. A. Rusu, A. Pritzel, và D. Wierstra, "PathNet: Evolution channels gradient descent in super neural networks," arXiv preprint arXiv:1701.08734 , 2017.
[14] J. Serra, D. Suris, M. Miron, và A. Karatzoglou, "Overcoming catastrophic forgetting with hard attention to the task," trong International Conference on Machine Learning , 2018, pp. 4548–4557.
[15] D. Abati, J. Tomczak, T. Blankevoort, S. Calderara, R. Cucchiara, và B. E. Bejnordi, "Conditional channel gated networks for task-aware continual learning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 3931–3940.
[16] S. Yan, J. Xie, và X. He, "DER: Dynamically expandable representation for class incremental learning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 3014–3023.
[17] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al. , "Overcoming catastrophic forgetting in neural networks," National Academy of Sciences , vol. 114, no. 13, pp. 3521–3526, 2017.
[18] Z. Li và D. Hoiem, "Learning without forgetting," IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 40, no. 12, pp. 2935–2947, 2017.
[19] F. Zenke, B. Poole, và S. Ganguli, "Continual learning through synaptic intelligence," trong International Conference on Machine Learning , 2017, pp. 3987–3995.
[20] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y . W. Teh, R. Pascanu, và R. Hadsell, "Progress & compress: A scalable framework for continual learning," trong International Conference on Machine Learning , 2018, pp. 4528–4537.
[21] H. Yin, P. Yang, và P. Li, "Mitigating forgetting in online continual learning with neuron calibration," Advances in Neural Information Processing Systems , vol. 34, pp. 10 260–10 272, 2021.
[22] R. Ratcliff, "Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions." Psychological Review , vol. 97, no. 2, pp. 285–308, 1990.
[23] A. Robins, "Catastrophic forgetting, rehearsal and pseudo rehearsal," Connection Science , vol. 7, no. 2, pp. 123–146, 1995.
[24] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, và C. H. Lampert, "iCaRL: Incremental classifier and representation learning," trong IEEE Conference on Computer Vision and Pattern Recognition , 2017, pp. 2001–2010.
[25] D. Lopez-Paz và M. Ranzato, "Gradient episodic memory for continual learning," trong Advances in Neural Information Processing Systems , vol. 30, 2017, pp. 6467–6476.
[26] P. Buzzega, M. Boschini, A. Porrello, D. Abati, và S. Calderara, "Dark experience for general continual learning: a strong, simple baseline," trong Advances in Neural Information Processing Systems , vol. 33, 2020, pp. 15 920–15 930.
[27] A. Chaudhry, A. Gordo, P. Dokania, P. Torr, và D. Lopez-Paz, "Using hindsight to anchor past knowledge in continual learning," trong AAAI Conference on Artificial Intelligence , vol. 35, no. 8, 2021, pp. 6993–7001.
[28] J. Bang, H. Kim, Y . Yoo, J.-W. Ha, và J. Choi, "Rainbow memory: Continual learning with a memory of diverse samples," trong IEEE Conference on Computer Vision and Pattern Recognition , 2021, pp. 8218–8227.
[29] L. Caccia, R. Aljundi, N. Asadi, T. Tuytelaars, J. Pineau, và E. Belilovsky, "New insights on reducing abrupt representation change in online continual learning," trong International Conference on Learning Representations , 2022.

--- TRANG 13 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 13

[30] H. Ahn, J. Kwak, S. Lim, H. Bang, H. Kim, và T. Moon, "SS-IL: Separated softmax for incremental learning," trong Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 844–853.
[31] G. A. Carpenter và S. Grossberg, "A massively parallel architecture for a self-organizing neural pattern recognition machine," Computer Vision, Graphics, and Image Processing , vol. 37, no. 1, pp. 54–115, 1987.
[32] B. Zhou, Q. Cui, X.-S. Wei, và Z.-M. Chen, "BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition," trong IEEE Conference on Computer Vision and Pattern Recognition , 2020.
[33] S. Hou, X. Pan, C. C. Loy, Z. Wang, và D. Lin, "Learning a unified classifier incrementally via rebalancing," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 831–839.
[34] A. Chaudhry, M. Ranzato, M. Rohrbach, và M. Elhoseiny, "Efficient lifelong learning with A-GEM," arXiv preprint arXiv:1812.00420 , 2018.
[35] A. Mallya và S. Lazebnik, "PackNet: Adding multiple tasks to a single network by iterative pruning," trong IEEE Conference on Computer Vision and Pattern Recognition , 2018, pp. 7765–7773.
[36] C. V . Nguyen, Y . Li, T. D. Bui, và R. E. Turner, "Variational continual learning," trong International Conference on Learning Representations , 2018.
[37] A. Maracani, U. Michieli, M. Toldo, và P. Zanuttigh, "Recall: Replay-based continual learning in semantic segmentation," trong Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 7026–7035.
[38] C.-B. Zhang, J.-W. Xiao, X. Liu, Y .-C. Chen, và M.-M. Cheng, "Representation compensation networks for continual semantic segmentation," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022, pp. 7053–7064.
[39] G. Yang, E. Fini, D. Xu, P. Rota, M. Ding, T. Hao, X. Alameda-Pineda, và E. Ricci, "Continual attentive fusion for incremental learning in semantic segmentation," IEEE Transactions on Multimedia , 2022.
[40] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, và Y . Gong, "Few-shot class-incremental learning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 12 183–12 192.
[41] D.-W. Zhou, F.-Y . Wang, H.-J. Ye, L. Ma, S. Pu, và D.-C. Zhan, "Forward compatible few-shot class-incremental learning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022, pp. 9046–9056.
[42] Y . Cui, W. Deng, X. Xu, Z. Liu, Z. Liu, M. Pietik ̈ainen, và L. Liu, "Uncertainty-guided semi-supervised few-shot class-incremental learning with knowledge distillation," IEEE Transactions on Multimedia , 2022.
[43] S. Thuseethan, S. Rajasegarar, và J. Yearwood, "Deep continual learning for emerging emotion recognition," IEEE Transactions on Multimedia , vol. 24, pp. 4367–4380, 2021.
[44] W. Nie, R. Chang, M. Ren, Y . Su, và A. Liu, "I-gcn: Incremental graph convolution network for conversation emotion detection," IEEE Transactions on Multimedia , vol. 24, pp. 4471–4481, 2021.
[45] D.-W. Zhou, Q.-W. Wang, Z.-H. Qi, H.-J. Ye, D.-C. Zhan, và Z. Liu, "Deep class-incremental learning: A survey," arXiv preprint arXiv:2302.03648 , 2023.
[46] M. Ren, W. Zeng, B. Yang, và R. Urtasun, "Learning to reweight examples for robust deep learning," trong International Conference on Machine Learning . PMLR, 2018, pp. 4334–4343.
[47] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, và D. Meng, "Meta-weight net: Learning an explicit mapping for sample weighting," arXiv preprint arXiv:1902.07379 , 2019.
[48] A. Krizhevsky, G. Hinton et al. , "Learning multiple layers of features from tiny images," 2009.
[49] Stanford, "Tiny imagenet challenge (CS231n)," http://tiny-imagenet. herokuapp.com/, 2015.
[50] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, và A. Lerer, "Automatic differentiation in pytorch," 2017.
[51] D. P. Kingma và J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980 , 2014.
[52] R. Aljundi, M. Lin, B. Goujaud, và Y . Bengio, "Gradient based sample selection for online continual learning," trong Advances in Neural Information Processing Systems , vol. 32, 2019, pp. 11 816–11 825.
[53] Q. Pham, C. Liu, D. Sahoo, và H. Steven, "Contextual transformation networks for online continual learning," trong International Conference on Learning Representations , 2020.
