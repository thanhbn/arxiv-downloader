Về Hiệu Quả của Chính Quy Hóa Đẳng Biến cho Học Tập Liên Tục Trực Tuyến Mạnh Mẽ

Lorenzo Bonicelli¹, Matteo Boschini¹, Emanuele Frascaroli¹, Angelo Porrello¹, Matteo Pennisi², Giovanni Bellitto², Simone Palazzo², Concetto Spampinato², Simone Calderara¹

¹AImageLab - Đại học Modena và Reggio Emilia
²PeRCeiVe Lab - Đại học Catania

Tóm tắt
Con người có thể học theo cách gia tăng, trong khi các mạng nơ-ron lại quên thông tin đã thu được trước đó một cách thảm khốc. Các phương pháp Học Tập Liên Tục (CL) tìm cách thu hẹp khoảng cách này bằng cách tạo điều kiện cho việc chuyển giao kiến thức đến cả các nhiệm vụ trước đó (chuyển giao ngược) và các nhiệm vụ tương lai (chuyển giao tiến) trong quá trình huấn luyện. Nghiên cứu gần đây đã chỉ ra rằng tự giám sát có thể tạo ra các mô hình đa năng có thể tổng quát hóa tốt đến các nhiệm vụ downstream đa dạng. Tuy nhiên, học tự giám sát tương phản (CSSL), một kỹ thuật tự giám sát phổ biến, có hiệu quả hạn chế trong CL trực tuyến (OCL). OCL chỉ cho phép một lần lặp của tập dữ liệu đầu vào, và hiệu quả mẫu thấp của CSSL cản trở việc sử dụng nó trên luồng dữ liệu đầu vào.

Trong nghiên cứu này, chúng tôi đề xuất Học Tập Liên Tục thông qua Chính Quy Hóa Đẳng Biến (CLER), một phương pháp OCL tận dụng các nhiệm vụ đẳng biến để tự giám sát, tránh các hạn chế của CSSL. Phương pháp của chúng tôi đại diện cho nỗ lực đầu tiên kết hợp kiến thức đẳng biến với CL và có thể dễ dàng tích hợp với các phương pháp OCL hiện có. Các phân tích sâu rộng làm sáng tỏ cách các nhiệm vụ pretext đẳng biến ảnh hưởng đến luồng thông tin của mạng và tác động của nó đến động lực CL.

1. Giới thiệu

Khi xử lý các phân phối đầu vào không ổn định, Mạng Nơ-ron Nhân Tạo (ANN) cho thấy xu hướng thiên vị đối với dữ liệu huấn luyện đến và do đó quên kiến thức đã thu được trước đó một cách thảm khốc [39]. Học Tập Liên Tục (CL) là một lĩnh vực học máy phát triển nhanh chóng nhằm thiết kế các phương pháp để chống lại hiệu ứng này [42, 17]. Dựa trên phân tách tham số [38, 48], chính quy hóa [31, 33] hoặc replay [47, 8, 9] - các phương pháp CL cho phép các hệ thống học máy thích ứng liên tục trong khi vẫn hiệu quả trên dữ liệu cũ. Để đánh giá ưu điểm của các nghiên cứu này, một loạt các thiết lập thực nghiệm đã được đề xuất trong những năm gần đây; trong số đó, chúng tôi tập trung vào kịch bản Học Tập Liên Tục Trực Tuyến (OCL) đầy thách thức [2, 12, 9] do tính ứng dụng của nó đối với các vấn đề thực tế: vì nó chỉ cho phép một lần xử lý dữ liệu huấn luyện, nó thể hiện giả định thực tế rằng một người học CL trong thực tế sẽ hiếm khi tiếp xúc với cùng một đầu vào hai lần.

Được thúc đẩy bởi thành công của Học Tự Giám Sát Tương Phản (CSSL) [15, 51, 5], một số phương pháp CL gần đây xoay quanh học biểu diễn tự giám sát [43, 10, 22, 36]. Thật vậy, vì các biểu diễn tự giám sát thường được thừa nhận là bất khả tri và dễ dàng chuyển giao đến các nhiệm vụ downstream đa dạng [14], việc khai thác chúng có vẻ đặc biệt hứa hẹn trong kịch bản trực tuyến, nơi việc học một biểu diễn chung trên các nhiệm vụ cũng quan trọng như việc ngăn chặn quên lãng. Hơn nữa, chúng tôi cho rằng việc liên kết các lớp đến với các biểu diễn đa mục đích khuyến khích sự xuất hiện của một cơ sở kiến thức ngang và có thể chia sẻ, ít bị quên lãng hơn.

Tuy nhiên, chúng tôi nhận thấy rằng mô hình CSSL không phải là viên đạn bạc: thật vậy, các phương pháp tương phản được đặc trưng bởi hiệu quả mẫu thấp vì sự hội tụ của chúng đòi hỏi một lượng lớn tài nguyên. Do đó, các phương pháp CL cần số epoch huấn luyện cao hơn khi được trang bị chính quy hóa tương phản [10], điều này xung đột với các ràng buộc của OCL. Hơn nữa, chúng thường tập trung học biểu diễn của mình vào một bộ đệm bộ nhớ nhỏ [43], điều này đòi hỏi rủi ro cao về overfitting [6].

Nghiên cứu này giải quyết những hạn chế này, tiết lộ lợi ích của các nhiệm vụ tự giám sát đẳng biến (ví dụ: dự đoán xoay, ghép hình jigsaw, ...) cho kịch bản OCL. Để cung cấp cái nhìn sâu sắc, Hình 1 xem xét một người học đơn giản dựa trên Tinh chỉnh (tức là, không có biện pháp chống quên lãng) và báo cáo hiệu suất của nó trong kịch bản trực tuyến chỉ cho phép một epoch cho mỗi nhiệm vụ: khi làm như vậy, chúng tôi so sánh các hiệu ứng của mục tiêu phụ dựa trên học tự giám sát đẳng biến (trong trường hợp này, dự đoán xoay bốn chiều) hoặc trên Barlow Twins [51], một phương pháp dựa trên CSSL gần đây cũng đã cho thấy ưu điểm của nó trong CL [43]. Chúng tôi quan sát thấy rằng cả hai nhiệm vụ học biểu diễn đều cho phép giảm can thiệp giữa các đặc trưng được học bởi SSL, được hỗ trợ bởi sự liên kết gradient thuận lợi hơn giữa các nhiệm vụ hiện tại và tiếp theo (Hình 1a). Đáng ngạc nhiên, Hình 1b cho thấy rằng chỉ có mô hình hỗ trợ xoay mới có lợi ích đáng kể về độ chính xác nhiệm vụ cá nhân cho mục tiêu dựa trên CSSL. Chúng tôi phỏng đoán rằng số lượng bước huấn luyện hạn chế trong CL trực tuyến không đủ để các phương pháp tương phản (như Barlow Twins) tạo ra các biểu diễn hiệu quả cho nhiệm vụ downstream.

Để giải quyết các hạn chế CSSL nói trên trong thiết lập OCL, chúng tôi đề xuất Học Tập Liên Tục thông qua Chính Quy Hóa Đẳng Biến (CLER), một bộ chính quy hóa OCL mới được xây dựng dựa trên các nhiệm vụ pretext đẳng biến - theo hiểu biết của chúng tôi, đây là nỗ lực đầu tiên khai thác thông tin đẳng biến trong CL. Chúng tôi chứng minh rằng đề xuất của chúng tôi có thể dễ dàng kết hợp với các phương pháp CL hiện đại hàng đầu, dẫn đến cải thiện hiệu suất tổng quát. Thông qua các thí nghiệm bổ sung, chúng tôi làm nổi bật các thuộc tính cấu trúc và dự đoán được CLER cung cấp và vẽ ra so sánh chi tiết với các lựa chọn thay thế dựa trên CSSL.

2. Nghiên cứu liên quan

Học Tập Liên Tục (Trực Tuyến) là một lĩnh vực học máy nghiên cứu việc huấn luyện trên các chuỗi nhiệm vụ không i.i.d., với mục tiêu giữ lại càng nhiều kiến thức càng tốt từ các nhiệm vụ cũ và giảm thiểu quên lãng thảm khốc [39]. Tài liệu hiện có cung cấp các kỹ thuật khác nhau để giải quyết vấn đề này: các phương pháp dựa trên chính quy hóa [31, 33] được thiết kế để kiểm soát các cập nhật tham số nhằm ngăn chặn các sửa đổi phá hoại đối với các đặc trưng quan trọng cho các nhiệm vụ trước đó; các phương pháp dựa trên phân tách [38, 48] xác định các tập con tham số liên quan đến nhiệm vụ và ngăn chặn sự thay đổi của chúng bằng cách kết hợp đóng băng tham số, mở rộng mô hình và cổng đặc trưng; các phương pháp dựa trên replay [47, 46, 8, 9] lưu trữ các ví dụ từ quá khứ trong bộ đệm bộ nhớ, với mục tiêu định kỳ làm mới kiến thức cũ. Mặc dù đơn giản, phương pháp sau thường được coi là giải pháp hiệu quả nhất đến nay [21, 50, 13].

Các phương pháp này thường được đánh giá trong một thiết lập huấn luyện thoải mái, nơi nhiệm vụ hiện tại có thể được trải nghiệm qua nhiều epoch. Trong các ứng dụng thực tế, yêu cầu này hiếm khi được thỏa mãn; Học Tập Liên Tục Trực Tuyến (OCL) [37, 35, 3] là một kịch bản đầy thách thức và thực tế bổ sung điều kiện rằng mỗi mẫu của luồng chỉ có thể được nhìn thấy một lần. Các nghiên cứu nhắm mục tiêu OCL thường đều thuộc họ dựa trên replay [35, 13]¹. Trong số các đề xuất gần đây, MIR [2] và GSS [3] đề xuất các quy trình lựa chọn mẫu replay cải tiến, ER-AML/ER-ACE [9] khuyến khích cân bằng trong việc học thông qua các hàm loss được thiết kế cẩn thận, CoPE [18] học bằng cách khai thác các tóm tắt lớp phát triển chậm.

Học Biểu Diễn Tự Giám Sát trong CL. Học Tự Giám Sát nhằm học các biểu diễn hữu ích trực tiếp từ dữ liệu, tức là không cần chú thích thủ công. Các nghiên cứu SSL gần đây cho thấy rằng những phương pháp này có thể học các biểu diễn mạnh mẽ có thể đạt được hoặc thậm chí vượt qua những biểu diễn của học có giám sát [14, 15, 51]. Trong bối cảnh CL, các phương pháp SSL thường được huấn luyện để khuyến khích mạng backbone bất biến với các biến đổi đã cho [10, 22, 43, 36, 30]. Co2L [10] học các biểu diễn cho các nhiệm vụ mới với một quy trình học tương phản có giám sát được sửa đổi [29], nơi các mẫu nhiệm vụ hiện tại được sử dụng làm neo và các phần tử trong bộ đệm được sử dụng làm mẫu âm - tất cả điều này trong khi bảo tồn kiến thức quá khứ thông qua chưng cất. Tuy nhiên, việc áp dụng các phương pháp SSL trong CL không đơn giản: SSL hưởng lợi từ kích thước batch lớn và yêu cầu một số bước huấn luyện để hội tụ [14]; điều này đại diện cho một giới hạn cho Co2L, vì số lượng mẫu âm bị giới hạn bởi kích thước bộ đệm nhỏ. DualNet [43] tách riêng việc học biểu diễn khỏi mục tiêu CL thông qua hai mạng bổ sung: một mạng chậm khai thác các mẫu bộ đệm để học một biểu diễn tổng thể, trong khi một mạng nhanh học tuần tự từ luồng đầu vào, sử dụng các đặc trưng từ mạng chậm để hướng dẫn quá trình.

Học Tự Giám Sát Pretext và Xoay. Khác với CSSL, [25] sử dụng nhiệm vụ pretext dự đoán xoay bốn chiều để cung cấp tín hiệu học tập mạnh mẽ cho việc học biểu diễn. Trong [24], nhiệm vụ pretext xoay được áp dụng trong bối cảnh học few-shot; tương tự, [16] ghép nối dự đoán xoay với các phương pháp SSL hiện có, dẫn đến cải thiện hiệu suất nhất quán. Gần đây, các tác giả của [1] đã nghiên cứu vai trò của bất biến và đẳng biến trong SSL, cho thấy rằng một số biến đổi (ví dụ: xoay bốn chiều, ghép hình jigsaw) có thể hiệu quả khi được sử dụng để khuyến khích đẳng biến, nhưng có thể dẫn đến các hiệu ứng phá hoại khi thực thi bất biến.

¹Tất cả các nghiên cứu OCL đương đại chỉ xem xét các phương pháp replay, do ưu thế hiệu suất rõ ràng của chúng so với tất cả các lựa chọn thay thế [37, 9].

3. Phương pháp

3.1. Học Tập Liên Tục Trực Tuyến

Trong Học Tập Liên Tục Trực Tuyến (OCL) [3, 12], một DNN đơn f được huấn luyện trên một chuỗi các nhiệm vụ phân loại T₁;...;T_T. Mỗi nhiệm vụ bao gồm các phân phối đầu vào và đầu ra rời rạc (T_i = (X_i, Y_i), với Y_i ∩ Y_j = ∅ cho i ≠ j) và mỗi cặp ví dụ-nhãn chỉ có thể được hiển thị cho mô hình một lần. Tại nhiệm vụ T_c, CL nhằm tối ưu hóa f trên tất cả T nhiệm vụ, trong khi chỉ có quyền truy cập vào dữ liệu từ chính T_c:

L = Σᵢ₌₁ᶜ⁻¹ R_i + R_c + Σⱼ₌ₒ₊₁ᵀ R_j

trong đó R_i = E_(x,y)∈T_i ℓ(f(x), y) biểu thị rủi ro thực nghiệm liên quan đến dữ liệu của nhiệm vụ T_i.

Trong Phương trình 1, số hạng 1 (ổn định) yêu cầu f duy trì hiệu quả dự đoán trên dữ liệu đã gặp trước đó, trong khi số hạng 3 (tính dẻo) gợi ý rằng mô hình nên chuẩn bị để phù hợp với các phân phối dữ liệu mới trong các nhiệm vụ sau này. Chỉ có 2 có thể được theo đuổi trực tiếp bằng cách huấn luyện trên dữ liệu; thay vào đó, 1 và 3 được đạt được bằng các số hạng loss phụ. Các phương pháp CL cố gắng cân bằng ba số hạng, thường được hiểu là can thiệp lẫn nhau [46, 4, 34].

3.2. OCL thông qua Chính Quy Hóa Đẳng Biến

Các mục tiêu 1 và 3 từ Phương trình 1 đặc trưng cho những thách thức chính khi thiết kế một mô hình CL. Tuy nhiên, cả hai đều có thể được giải quyết bằng cách học một biểu diễn có thể được chia sẻ trên nhiều nhiệm vụ. Để đạt được điều này, chúng tôi trang bị cho người học trực tuyến một mục tiêu SSL phụ. Các nghiên cứu trong tài liệu hiện tại theo đuổi mục tiêu này thông qua các số hạng loss CSSL [10, 43]; thay vào đó, chúng tôi theo các hiểu biết được trình bày trong Phần 1 và chọn một nhiệm vụ pretext đẳng biến [16], được định nghĩa như sau.

Cho A = {A_i}ᵢ₌₁ᴷ là một họ các biến đổi đầu vào A_i: X → X (ví dụ: xoay, ghép hình jigsaw), chúng tôi biến đổi mỗi exemplar đầu vào với A_k được chọn ngẫu nhiên và yêu cầu mô hình đang huấn luyện nhận dạng biến đổi bằng cách dự đoán nhãn đúng k ∈ Y_A = {1,...,K}. Với mục đích này, chúng tôi viết lại f là h ∘ g, trong đó g là phần đầu của mạng, dành riêng cho việc trích xuất đặc trưng, và h bao gồm phần sau của mô hình, bao gồm đầu phân loại đa lớp cuối cùng cho nhiệm vụ CL. Tiếp theo, chúng tôi giới thiệu h': một mạng con riêng biệt theo cùng cấu trúc như h, cuối cùng chiếu biểu diễn g(·) lên tập Y_A.

Chúng tôi coi việc lựa chọn A là một siêu tham số. Trong các thí nghiệm của mình, chúng tôi khám phá hai loại biến đổi khác nhau: tập hợp 4 phép xoay ảnh không méo {Rot 0°, Rot 90°, Rot 180°, Rot 270°} [24, 25], và 24 hoán vị của các miếng được tạo ra bởi ghép hình jigsaw 2×2 [41].

Phương pháp kết quả, được gọi là CLER, bao gồm một số hạng chính quy hóa L_r có thể được áp dụng dễ dàng trên một mạng backbone như được hiển thị trong Hình 2. Cho x ∈ B_in là một mẫu đến từ batch đầu vào, chúng tôi định nghĩa L_r là:

L_r = λ_r E_{x∈B_in} Σ_{k∈Y_A} CE(h'(g(A_k(x))), k)

trong đó CE là loss cross-entropy và λ_r là một siêu tham số vô hướng để kiểm soát cường độ của chính quy hóa. Chúng tôi nhấn mạnh rằng không gian nhãn Y_A của nhiệm vụ pretext vẫn không đổi theo thời gian. Do đó, mục tiêu của CLER có thể được so sánh với các vấn đề phân loại nơi chỉ có phân phối sinh dữ liệu là đối tượng thay đổi (học Domain-Incremental [50]).

Đẳng biến & bất biến. Một hàm f được gọi là đẳng biến w.r.t. A nếu tồn tại một ánh xạ M_A sao cho:
f(T(x)) = M_A(f(x)), ∀x ∈ X

Trong khi mục tiêu học trong Phương trình 2 thúc đẩy độ nhạy với tập hợp các biến đổi đã chọn, việc giải quyết nhiệm vụ CL buộc mô hình trở nên bất biến w.r.t. các tăng cường dữ liệu được sử dụng. Để tránh chồng chéo giữa hai mục tiêu, chúng tôi tính Phương trình 2 chỉ trên các đầu vào không được tăng cường.

4. Thí nghiệm

4.1. Thiết lập thí nghiệm

Bộ test chuẩn. Chúng tôi xây dựng các bộ test chuẩn OCL bằng cách lấy các tập dữ liệu phân loại ảnh và chia các lớp của chúng đều thành một loạt các nhiệm vụ rời rạc. Trong kịch bản học trực tuyến, người học sau đó sẽ trải nghiệm mỗi nhiệm vụ chỉ một lần (epoch đơn). Để biết thêm chi tiết về các thí nghiệm, chúng tôi giới thiệu độc giả đến tài liệu bổ sung.

• Seq. CIFAR-100 [52, 45, 13] được thu được bằng cách chia 100 lớp ban đầu của CIFAR-100 [32] thành 10 nhiệm vụ liên tiếp. Đối với mỗi lớp, tập huấn luyện và test bao gồm 500 và 100 ảnh RGB 32×32 tương ứng.

• Seq. mini ImageNet [13, 20, 19] là một tập dữ liệu đầy thách thức bao gồm tổng cộng 100 lớp từ tập dữ liệu ImageNet phổ biến và một chuỗi nhiệm vụ dài hơn. Trong khi số lượng mẫu giống như trong Seq. CIFAR-100, các ảnh được thay đổi kích thước thành 84×84 và chia thành 20 nhiệm vụ 5-way.

Giao thức đánh giá. Chúng tôi chủ yếu tập trung đánh giá của mình vào thiết lập Class-Incremental trực tuyến (oCIL), nơi mô hình được yêu cầu dần dần học cách giải quyết tất cả các nhiệm vụ, không có thông tin về định danh nhiệm vụ (Task-ID). Khác với thiết lập Task-Incremental trực tuyến (oTIL), nơi Task-ID có sẵn trong quá trình suy luận, oCIL buộc người học xây dựng một bộ phân loại đầu đơn. Chúng tôi trình bày kết quả rộng rãi trong cả thiết lập oCIL và oTIL.

Các phương pháp baseline. Chúng tôi báo cáo kết quả của CLER trên một lựa chọn các phương pháp hiện đại hàng đầu (SOTA) khả thi cho thiết lập oCIL.

• Experience Replay with Asymmetric Cross-Entropy (ER-ACE) [9]. Bắt đầu từ baseline store-and-replay phổ biến (Experience Replay [44, 47]), các tác giả đề xuất một sự thay đổi nhằm ngăn chặn sự mất cân bằng do tối ưu hóa đồng thời dữ liệu hiện tại và quá khứ.

• eXtended Dark Experience Replay (X-DER) [7] là một mô hình kết hợp replay với tự chưng cất, trong khi áp dụng các lựa chọn thiết kế cẩn thận để pha trộn hài hòa các hàm dự đoán được học tại các thời điểm khác nhau.

• Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams (CoPE) [18] đề xuất một bộ phân loại dựa trên các prototype lớp, có kế hoạch cập nhật cẩn thận cho phép học gia tăng trong khi tránh những gián đoạn đột ngột trong không gian tiềm ẩn.

• DualNet [43] là một kiến trúc backbone kép tách riêng vấn đề phân loại gia tăng khỏi vấn đề học một biểu diễn có thể chuyển giao tổng thể. Nhiệm vụ sau được giao cho một trong các backbone (slow learner), được huấn luyện với một số hạng loss CSSL trên dữ liệu i.i.d. đến từ bộ đệm replay; backbone kia (fast learner) thay vào đó được giao nhiệm vụ phù hợp với các nhiệm vụ CL trong khi tận dụng các biểu diễn được tạo ra bởi slow learner.

Tất cả các mô hình được huấn luyện trong một epoch với SGD, với kích thước batch cố định là 10 cả trên luồng đầu vào và bộ đệm replay. Chúng tôi đánh giá tất cả các mô hình với hai kích thước khác nhau cho bộ đệm bộ nhớ: 500 và 2000 cho Seq. CIFAR-100 và 2000 và 8000 cho Seq. miniImageNet. Đối với các phương pháp này, đầu vào B_in trong Phương trình 2 là sự nối các ảnh đến từ cả luồng và bộ đệm.

Để so sánh tốt hơn hiệu ứng của CLER, chúng tôi cũng bao gồm kết quả của một mô hình được huấn luyện chung trên tất cả các lớp trong một epoch (Joint-online) và 30 và 50 epoch tương ứng trên Seq. CIFAR-100 và Seq. miniImageNet (Joint-offline). Ngoài ra, chúng tôi bao gồm kết quả của một mô hình được huấn luyện trên chuỗi nhiệm vụ không có biện pháp chống quên lãng (Finetune).

Kiến trúc. Chúng tôi dựa vào ResNet18 [27] làm backbone trong tất cả các thí nghiệm. Đối với DualNet, chúng tôi sử dụng mô hình này làm slow learner và - phù hợp với [43] - xây dựng fast learner như một mạng feed-forward với cùng số lượng lớp tích chập như các khối residual trong slow learner.

Bất kể phương pháp CL cơ bản, chúng tôi định nghĩa bộ trích xuất đặc trưng g và các đầu phân loại h và h' bằng cách chia backbone ResNet tại khối residual thứ hai từ cuối; cụ thể, h và h' bao gồm khối residual cuối cùng, tiếp theo là một phép chiếu tuyến tính lên các tập hợp lớp tương ứng Y = ⋃ᵢ₌₁ᵀ Yᵢ và Y_A.

Các chỉ số. Như một chỉ báo chính về hiệu suất của mô hình khi kết thúc OCL, chúng tôi báo cáo Độ Chính Xác Trung Bình Cuối Cùng (A_F). Cho aᵢʲ là độ chính xác của mô hình khi kết thúc nhiệm vụ j được tính trên tập test của nhiệm vụ Tᵢ, A_F được tính là:

A_F = (1/T) Σᵢ₌₁ᵀ aᵢᵀ

Để đánh giá thêm về việc học khi các nhiệm vụ tiến triển, chúng tôi báo cáo Quên Lãng Trung Bình Điều Chỉnh Cuối Cùng (F̄_F), được định nghĩa như sau:

F̄_F = (1/(T-1)) Σᵢ₌₁ᵀ⁻¹ (a*ᵢ - aᵢᵀ) / a*ᵢ⁺

trong đó a*ᵢ = max_{t∈{i,...,T-1}} aᵢᵗ, ∀i ∈ {1,...,T-1}

F̄_F là một thước đo mới được phát triển từ chỉ số Quên Lãng được sử dụng rộng rãi [11] để tạo điều kiện so sánh giữa các phương pháp có hiệu suất không đồng đều. Cụ thể, trong khi Quên Lãng ban đầu được giới hạn trên bởi độ chính xác của mô hình, F̄_F thay đổi trong [0, 100]. F̄_F = 100 biểu thị một phương pháp không giữ lại độ chính xác nào trên các nhiệm vụ trước đó (ví dụ: Finetune) và F̄_F = 0 một phương pháp không có giảm hiệu suất trên các nhiệm vụ quá khứ.

Chúng tôi lặp lại mỗi thí nghiệm 10 lần và báo cáo trung bình A_F và F̄_F, và độ lệch chuẩn của số trước. Vui lòng tham khảo tài liệu bổ sung cho độ lệch chuẩn và ý nghĩa thống kê.

4.2. So sánh với Hiện Đại Hàng Đầu

Chúng tôi bao gồm kết quả đánh giá của mình trên Seq. CIFAR-100 và Seq. miniImageNet cho oCIL và oTIL trong Bảng 1 và 2 tương ứng. Đối với mỗi thí nghiệm, chúng tôi báo cáo người thực hiện tốt nhất trong số các nhiệm vụ pretext jigsaw 2×2 và xoay². Bằng chứng chúng tôi trình bày mạnh mẽ hỗ trợ các tuyên bố ban đầu của chúng tôi, với CLER cải thiện các phương pháp SOTA trong tất cả các bộ test chuẩn. Cụ thể, chúng tôi chứng kiến một cải thiện toàn diện về A_F, trong khi F̄_F cho thấy khả năng chống lại quên lãng mạnh hơn.

Thú vị là, hiệu ứng của chính quy hóa của chúng tôi được duy trì bất kể lựa chọn kích thước bộ đệm, với cải thiện oCIL trung bình 3,59 và 3,40 trên Seq. CIFAR-100 và 3,12 và 3,46 trên Seq. miniImageNet. Chúng tôi thấy ngoại lệ đáng chú ý duy nhất là trong trường hợp DualNet trên Seq. CIFAR-100. Thật vậy, ngay cả khi không có chính quy hóa của chúng tôi, F_AA thấp hơn và quên lãng cao hơn so với các baseline khác cho thấy rằng mô hình không thể hưởng lợi từ bộ đệm bộ nhớ. Điều này có thể do thực tế rằng slow learner chỉ được huấn luyện với mục tiêu CSSL trên các mẫu từ bộ đệm, điều này hạn chế chất lượng biểu diễn của nó khi bộ đệm có kích thước vừa phải. Tuy nhiên, kết quả của nó trên Seq. miniImageNet đầy thách thức, khi kết hợp với CLER, cho thấy rằng hiệu ứng như vậy có thể được giảm thiểu bằng cách tận dụng SSL đẳng biến, cho phép fast learner phát triển các biểu diễn tốt hơn trong OCL.

5. Phân tích mô hình

Trong phần còn lại, chúng tôi phân tích các đóng góp khác nhau của CLER và thu thập thêm hiểu biết về hiệu ứng tổng thể của nó đối với các nhiệm vụ CL. Theo hiểu biết của chúng tôi, nghiên cứu của chúng tôi là đầu tiên xem xét hiệu ứng của các nhiệm vụ pretext dựa trên đẳng biến trong một thiết lập gia tăng.

5.1. Hiệu ứng của CLER trên Backbone

Để phân tích sâu về các hiệu ứng gây ra trên backbone, chúng tôi xem xét ER-ACE có và không có CLER và tiến hành ba thí nghiệm bổ sung, lấy cảm hứng từ tài liệu Network Pruning [40]. Mục đích của chúng tôi ở đây là tiết lộ cách thông tin được mang bởi các đặc trưng đã học phân phối trên các tham số của backbone.

Tầm quan trọng và dư thừa. Đầu tiên, chúng tôi định lượng đóng góp của mỗi tham số đối với loss tổng thể sau khi huấn luyện trên Seq. CIFAR-100 bằng cách tính thước đo tầm quan trọng Î⁽¹⁾ₘ được đề xuất trong [40]. Trong Hình 3a, chúng tôi tập trung vào các lớp tích chập và báo cáo tỷ lệ các tham số có điểm tầm quan trọng cao hơn trung bình của lớp để cung cấp đánh giá nhỏ gọn mỗi lớp.

Ngoài ra, chúng tôi thực hiện cắt tỉa Geometric Median [28] trên mô hình, do đó loại bỏ những bộ lọc F_d dư thừa nhất - tức là trung bình giống nhất với tất cả những bộ lọc khác trong cùng một lớp. Trong Hình 3b, chúng tôi báo cáo độ tương tự trong lớp trung bình ḡ cho các kernel bị loại bỏ:

ḡ(F_d) = (1/F) Σⱼ₌₁ᶠ ||F_d - Fⱼ||

với F là tổng số bộ lọc trong lớp được xem xét.

Kết quả của chúng tôi tiết lộ rằng CLER đẩy mô hình phù hợp với nhiệm vụ đã học với các cấu hình tham số dày đặc (Î⁽¹⁾ₘ cao hơn trong Hình 3a) cũng giống nhau hơn (ḡ thấp hơn trong Hình 3b). Chúng tôi phỏng đoán rằng điều này có thể liên quan đến sự gia tăng hiệu suất được báo cáo trong Phần 4.2: vì kiến thức của một nhiệm vụ cụ thể không dựa vào chỉ một vài tham số mà thay vào đó xuất hiện phân phối hơn, ít có khả năng các cập nhật trọng số tiếp theo sẽ hoàn toàn xóa kiến thức đã thu được trước đó. Hơn nữa, tỷ lệ tham số quan trọng cao hơn, kết hợp với dư thừa cao hơn, cho thấy rằng những bộ lọc quan trọng bị xóa bởi quên lãng có thể được khôi phục khi cần thiết, bằng cách đơn giản tận dụng các nhóm tham số dư thừa.

Phục hồi. Để hỗ trợ trực giác của chúng tôi, chúng tôi đã tiến hành một đánh giá bổ sung thăm dò động lực học tập với CLER. Sau khi huấn luyện trên nhiệm vụ thứ 6 của Seq. CIFAR-100, chúng tôi ngẫu nhiên loại bỏ một phần các bộ lọc tích chập trong các mô hình của mình và huấn luyện lại chỉ sử dụng loss cross-entropy trên một vài batch từ cùng nhiệm vụ, báo cáo độ chính xác sau mỗi batch trong Hình 3c. Thú vị là, tầm quan trọng phân phối được tạo ra bởi mục tiêu huấn luyện của chúng tôi dẫn đến giảm độ chính xác ban đầu cao hơn cho CLER. Tuy nhiên, phương pháp được đề xuất của chúng tôi nhanh chóng phục hồi hiệu suất của nó, đạt được độ chính xác mục tiêu trước khi loại bỏ trong ít bước hơn w.r.t. baseline.

5.2. Bất biến & Đẳng biến

Trong khi trong các phần trước chúng tôi đã khám phá vai trò của đẳng biến như một bộ chính quy hóa cho OCL, bây giờ chúng tôi muốn đặc trưng tốt hơn các nhiệm vụ pretext khác nhau, cũng như so sánh với một mục tiêu CSSL dựa trên bất biến.

Xoay vs Jigsaw. Kết quả được trình bày cho đến nay mô tả một lợi thế rõ ràng của nhiệm vụ pretext ghép hình jigsaw, điều này có thể gợi ý rằng lợi ích hiệu suất không liên quan cụ thể đến đẳng biến mà đến trường hợp trước. Để giải quyết mối quan tâm như vậy, trong Hình 4 chúng tôi trình bày kết quả chi tiết cho đánh giá Phần 4.2 trên thiết lập oCIL cả với xoay bốn chiều và ghép hình jigsaw. Kết quả của chúng tôi mô tả một lợi thế rõ ràng của cả hai nhiệm vụ pretext đẳng biến w.r.t. phương pháp baseline. Hơn nữa, hiệu suất tương tự đạt được bởi hai (đặc biệt trên bộ test chuẩn Seq. miniImageNet đầy thách thức) chứng minh thêm giả định ban đầu của chúng tôi về hiệu quả của các phương pháp SSL dựa trên đẳng biến trong CL.

So sánh với các phương pháp CSSL. Phân tích ban đầu của chúng tôi cho thấy rằng việc thực thi đẳng biến đối với một tập hợp các biến đổi đầu vào một cách hiệu quả cho phép CLER học một biểu diễn mạnh mẽ chống lại quên lãng, bằng cách phân tán đóng góp của mỗi đặc trưng trên tất cả các tham số có thể học. Điều này trái ngược với tài liệu CL hiện tại, thay vào đó dựa vào các nhiệm vụ CSSL [10, 43] để học một biểu diễn bất biến đối với tăng cường dữ liệu mạnh và các biến đổi đầu vào.

Để chứng minh thêm đóng góp của chúng tôi, trong Bảng 3 chúng tôi so sánh đề xuất của chúng tôi về một số hạng loss đẳng biến với một số hạng thúc đẩy bất biến bằng một mục tiêu CSSL. Đối với số hạng sau, chúng tôi lấy cảm hứng từ [43] và chọn Barlow Twins. Kết quả của chúng tôi cho thấy hiệu ứng chính quy hóa vượt trội cho CLER, với CSSL thậm chí còn làm tổn hại hiệu suất trong một số kịch bản. Điều này cho thấy rằng số ít lần lặp huấn luyện được cho phép trong OCL không cho phép CSSL chuyển giao kiến thức hữu ích, do đó cuối cùng cản trở việc học gia tăng.

Khả năng áp dụng cho thiết lập multi-epoch. Trong khi chúng tôi tập trung đánh giá của mình vào OCL, chúng tôi nhận thấy rằng phương pháp được đề xuất của chúng tôi cũng có thể chứng minh có lợi trong một môi trường ít nghiêm ngặt hơn cho phép nhiều lần lặp. Thiết lập như vậy mô phỏng một kịch bản độ trễ thấp thực tế, nơi mong muốn là một thuật toán có khả năng thích ứng nhanh chóng với luồng dữ liệu thay đổi trong khi giữ lại kiến thức từ quá khứ. Kết quả của đánh giá này trên bộ test chuẩn Seq. CIFAR-100 được tóm tắt trong Bảng 4. Do hạn chế không gian, chúng tôi chỉ bao gồm kết quả về kịch bản Class-Incremental.

Không ngạc nhiên, khi số epoch tăng, mô hình có thể bắt đầu tận dụng đầy đủ kiến thức đến từ luồng. Tuy nhiên, vì các nhiệm vụ CSSL thường yêu cầu một số lượng lớn lần lặp để hội tụ, CLER của chúng tôi vẫn là lựa chọn tốt hơn cho nhiệm vụ ngăn chặn quên lãng trong khi tăng cường biểu diễn của mô hình cơ sở.

5.3. Lợi thế của CLER có thực sự gắn liền với OCL không?

Hiệu suất được nâng cao nhất quán của các phương pháp baseline khi kết hợp với CLER có thể làm dấy lên nghi ngờ rằng chính quy hóa SSL nói chung là hiệu quả và không đặc biệt liên quan đến Học Tập Liên Tục per se. Để làm sáng tỏ điểm này, chúng tôi áp dụng cả chính quy hóa CSSL và CLER trên giới hạn trên Joint multi-epoch (Joint-offline) và báo cáo kết quả trong Bảng 5; test đơn giản này rõ ràng cho thấy rằng - nếu đủ epoch được cho phép và phương pháp đạt được hội tụ đầy đủ - sự hiện diện của các số hạng SSL bổ sung không ảnh hưởng đáng kể đến độ chính xác đạt được.

Để bổ sung cho kết quả này, chúng tôi cũng áp dụng kỹ thuật được đề xuất trên đỉnh của huấn luyện Joint một epoch. Trong bối cảnh này, CLER chứng minh hiệu quả và hơn thế nữa so với CSSL. Phù hợp với những gì được hiển thị trong Hình 1, kết quả này xác nhận rằng SSL tạo điều kiện cho sự hội tụ của người học khi chỉ có ít điểm dữ liệu và rằng phương pháp đẳng biến của CLER hiệu quả mẫu hơn các phương pháp CSSL điển hình.

Tóm lại, chúng tôi tóm tắt rằng chính quy hóa tự giám sát không hiệu quả trong thiết lập multi-epoch không liên tục (Bảng 5 trên); nó trở nên liên quan trong thiết lập epoch đơn (Bảng 5 dưới) hoặc liên tục (Bảng 4). Do hiệu quả mẫu được nâng cao, phương pháp đẳng biến được theo đuổi bởi CLER đặc biệt hiệu quả khi ít epoch hơn được thực hiện. Vì lý do này, ứng dụng của nó là lý tưởng cho thiết lập OCL.

5.4. Khả năng áp dụng cho Học Tập Liên Tục Không Dữ Liệu

Các đối thủ SOTA mà chúng tôi xác thực CLER trong Phần 4 thuộc họ các phương pháp CL dựa trên rehearsal. Những phương pháp này đại diện cho phương pháp được ưa thích nhất trong kịch bản oCIL đầy thách thức, trong đó hiệu suất của các lớp phương pháp khác bị tổn hại nghiêm trọng [37, 9, 26, 53]. Tuy nhiên, một dòng nghiên cứu rất gần đây đưa ra chỉ trích về việc áp dụng replay, trích dẫn các vấn đề riêng tư tiềm ẩn [49, 23]. Thay vào đó, họ tập trung vào thiết lập Học Class-Incremental Không Dữ Liệu (DFCIL) được gọi là, tức là Học Class-Incremental multi-epoch không có bộ đệm bộ nhớ.

Để cung cấp một bức tranh rõ ràng về tính linh hoạt của đề xuất của chúng tôi, chúng tôi tiếp tục giới thiệu ứng dụng của nó trên đỉnh của hai phương pháp DFCIL: Relation-Guided Representation Learning dựa trên đảo ngược mô hình (R-DFCIL) [23] và Multi-Class Learning without Forgetting dựa trên chưng cất (LWF.MC) [45]. Kết quả trong Bảng 6 minh họa rằng CLER mang lại cải thiện hiệu suất ổn định ngay cả trong DFCIL, điều này tiết lộ rằng hiệu quả của nó không phụ thuộc vào tính khả dụng của dữ liệu replay.

6. Kết luận

Chúng tôi trình bày Học Tập Liên Tục thông qua Chính Quy Hóa Đẳng Biến (CLER), một phương pháp mới cho Học Tập Liên Tục Trực Tuyến (OCL) khuyến khích các biểu diễn nhạy cảm với một tập hợp các biến đổi đầu vào. Phương pháp của chúng tôi giới thiệu một kỹ thuật chính quy hóa dựa trên các nhiệm vụ pretext SSL đẳng biến (giải ghép hình jigsaw và dự đoán xoay bốn chiều). Bằng phương tiện thí nghiệm, chúng tôi chỉ ra rằng việc áp dụng CLER cho các phương pháp hiện đại hàng đầu liên tục dẫn đến hiệu suất tốt hơn. Hơn nữa, chúng tôi cung cấp một phân tích sâu về hiệu ứng của CLER trên các tham số của mạng backbone và so sánh nó với các phương pháp Học Tự Giám Sát Tương Phản khác.

Kết quả mạnh mẽ của chúng tôi với các lựa chọn khác nhau của các nhiệm vụ pretext đẳng biến hỗ trợ thêm giả thuyết ban đầu của chúng tôi, đặt nền tảng cho các mô hình OCL tốt hơn dựa trên các ràng buộc đẳng biến. Chúng tôi để lại phân tích này cho nghiên cứu tương lai.
