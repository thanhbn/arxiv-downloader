# 2301.01828.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2301.01828.pdf
# File size: 2391475 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Citation: Kessler, S.; Cobb, A.; Rudner,
T.G.J.; Zohren, S.; Roberts, S.J. On
Sequential Bayesian Inference for
Continual Learning. Entropy 2023 ,1, 0.
https://doi.org/
Academic Editors: Irad E. Ben-Gal
and Amichai Painsky
Received: 1 May 2023
Revised: 24 May 2023
Accepted: 28 May 2023
Published:
Copyright: ©2023 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
entropy
Article
On Sequential Bayesian Inference for Continual Learning
Samuel Kessler1,*, Adam Cobb3, Tim G. J. Rudner2, Stefan Zohren1and Stephen J. Roberts1
1University of Oxford, Department of Engineering Science, Oxford OX2 6ED, UK;
skessler@robots.ox.ac.uk (S.K.); zohren@robots.ox.ac.uk (S.Z.); sjrob@robots.ox.ac.uk (S.J.R.)
2University of Oxford, Department of Computer Science, Oxford OX1 3QG, UK; tim.rudner@cs.ox.ac.uk
(T.G.J.R.)
3SRI International, Arlington, VA 22209, US; adam.cobb@sri.com
*Correspondence: skessler@robots.ox.ac.uk
Abstract: Sequential Bayesian inference can be used for continual learning to prevent catastrophic for-
getting of past tasks and provide an informative prior when learning new tasks. We revisit sequential
Bayesian inference and assess whether using the previous task’s posterior as a prior for a new task
can prevent catastrophic forgetting in Bayesian neural networks. Our first contribution is to perform
sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior
for new tasks by approximating the posterior via fitting a density estimator on Hamiltonian Monte
Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating
the difficulty in performing sequential Bayesian inference in neural networks. Furthermore, we
study simple analytical examples of sequential Bayesian inference and CL and highlight the issue
of model misspecification which can lead to sub-optimal continual learning performance despite
exact inference. Furthermore, we discuss how task data imbalances can cause forgetting. From these
limitations, we argue that we need probabilistic models of the continual learning generative process
rather than relying on sequential Bayesian inference over Bayesian neural network weights. Our final
contribution is to propose a simple baseline called Prototypical Bayesian Continual Learning , which
is competitive with the best performing Bayesian continual learning methods on class incremental
continual learning computer vision benchmarks.
Keywords: continual learning; lifelong learning; sequential Bayesian inference; Bayesian deep
learning; Bayesian neural networks
1. Introduction
The goal of continual learning (CL) is to find a predictor that learns to solve a sequence
of new tasks without losing the ability to solve previously learned tasks. One key challenge
of CL with neural networks (NNs) is that model parameters from previously learned tasks
are “overwritten” during gradient-based learning of new tasks, which leads to catastrophic
forgetting of previously learned abilities [ 1,2]. One approach to CL hinges on using recursive
applications of Bayes’ Theorem; using the weight posterior in a Bayesian neural network
(BNN) as the prior for a new task [ 3]. However, obtaining a full posterior over NN weights
is computationally demanding and we often need to resort to approximations, such as
the Laplace method [ 4] or variational inference [ 5,6] to obtain a neural network weight
posterior.
When performing Bayesian CL, sequential Bayesian inference is performed with an
approximate BNN posterior, not the true posterior [ 7–12]. If we consider the performance
of sequential Bayesian inference with a variational approximation over a BNN weight
posterior then we barely observe an improvement over simply learning new tasks with
stochastic gradient descent (SGD). We develop this statement further in section 2.2. So if we
had access to the true BNN weight posterior, would this be enough to prevent forgetting
by sequential Bayesian inference?
Entropy 2023 ,1, 0. https://doi.org/10.3390/e1010000 https://www.mdpi.com/journal/entropyarXiv:2301.01828v3  [cs.LG]  7 Jan 2025

--- PAGE 2 ---
Entropy 2023 ,1, 0 2 of 28
Our contributions in this chapter are to revisit Bayesian CL. 1) Experimentally, we
perform sequential Bayesian inference using the true Bayesian NN weight posterior. We
do this by using the gold standard of Bayesian inference methods, Hamiltonian Monte
Carlo (HMC) [ 13]. We use density estimation over HMC samples and use this approximate
posterior density as a prior for the next task within the HMC sampling process. Surprisingly
our HMC method for CL yields no noticeable benefits over an approximate inference
method (VCL [ 9]) despite using samples from the true posterior. 2) As a result we consider
a simple analytical example and highlight that exact inference with a misspecified model
can still cause forgetting. 3) We show mathematically that under certain assumptions task
data imbalances cause forgetting in Bayesian NNs. 4) We propose a new probabilistic model
for CL and show that by explicitly modeling the generative process of the data, we can
achieve good performance, avoiding the need to rely on recursive Bayesian inference over
NN weights to prevent forgetting. Our proposed model, Prototypical Bayesian Continual
Learning (ProtoCL), is conceptually simple, scalable, and competitive with state-of-the-art
Bayesian CL methods in the class-incremental learning setting.
2. Background
2.1. The Continual Learning Problem
Continual learning (CL) is a learning setting whereby a model must learn to make
predictions over a set of tasks sequentially while maintaining performance across all
previously learned tasks. In CL, the model is sequentially shown Ttasks, denoted Ttfor
t=1,. . .,T. Each task, Tt, is comprised of a dataset Dt={(xi,yi)}Nt
i=1, which a model
needs to learn to make predictions with. More generally, tasks are denoted by distinct tuples
comprised of the conditional and marginal data distributions, {pt(y|x),pt(x)}. After task
Tt, the model will lose access to the training dataset but its performance will be continually
evaluated on all tasks Tifori≤t. For a thorough overview of different continual learning
scenarios, see appendix A.
2.2. Bayesian Continual Learning
We consider a setting in which task data arrives sequentially at timesteps, t=
1, 2, . . .,T. At the first timestep, t=1, that is, for task T1, the model receives the first
dataset D1and learns the conditional distribution p(yi|xi,θ)for all (xi,yi)∈ D 1(iindexes
a datapoint in D1). We denote the parameters θas having a prior distribution p(θ)forT1.
The posterior predictive distribution for a test point x∗
1∈ D 1is hence:
p(y∗
1|x∗
1,D1) =Z
p(y∗
1|x∗
1,θ)p(θ|D1)dθ. (1)
We note that computing this posterior predictive distribution requires p(θ|D1). For t=2,
a CL model is required to fit p(yi|xi,θ)for(xi,yi)∈ D 1∪ D 2. The posterior predictive
distribution for a new test point x∗
2∈ D 1∪ D 2point is:
p(y∗
2|x∗
2,D1,D2) =Z
p(y∗
2|x∗
2,θ)p(θ|D1,D2)dθ. (2)
The posterior must thus be updated to reflect this new conditional distribution. We can use
repeated application of Bayes’ rule to calculate the posterior distributions p(θ|D1,. . .,DT)
as:
p(θ|D1, . . . ,DT−1,DT) =p(DT|θ)p(θ|D1, . . . ,DT−1)
p(DT|D1, . . . ,DT−1). (3)
In the CL setting, we lose access to previous training datasets; however, using repeated
applications of Bayes’ rule Equation (3) allows us to sequentially incorporate information

--- PAGE 3 ---
Entropy 2023 ,1, 0 3 of 28
from past tasks in the parameters θ. At t=1, we have access to D1and the posterior over
parameters is:
logp(θ|D1) =logp(D1|θ) +logp(θ)−logp(D1). (4)
Att=2, we require p(θ|D1,D2)to calculate the posterior predictive distribution in Equa-
tion (2). However, we have lost access to D1. According to Bayes’ rule, the posterior may
be written as:
logp(θ|D1,D2) =logp(D2|θ) +logp(θ|D1)−logp(D2|D1), (5)
where we used the conditional independence of D2andD1given θ. We note that the
likelihood p(D2|θ)is only dependent upon the current task dataset, D2, and that the prior
p(θ|D1)encodes parameter knowledge from the previous task. Hence, we can use the
posterior evaluated at tas a prior for learning a new task at t+1. From Equation (3), we
require that our model with parameters θis a sufficient statistic of D1, i.e., p(D2|θ,D1) =
p(D2|θ), making the likelihood conditionally independent of D1given θ. This observation
motivates the use of high-capacity predictors, such as Bayesian neural networks, that are
flexible enough to learn from D1.
Continual Learning Example: Split-MNIST
For the MNIST dataset [ 14] we know that if we were to train a BNN we would achieve
good performance by inferring the posterior p(θ|D)appendix B and integrating out the
posterior to infer the posterior predictive distribution over a test point eq. (1). So if we were
to split the dataset MNIST into 5two-class classification tasks then we should be able to
recursively recover the multi-task posterior p(θ|D) = p(θ|D1. . .,D5)using eq. (3). This
problem is called Split-MNIST [ 15], where the first task involves the classification of the
digits{0, 1}then the second task classification of the digits {2, 3}and so on.
We can define three different CL settings [ 16–18]. When we allow the CL agent to
make predictions with a task identifier tthe scenario is referred to as task-incremental .
The identifier tcould be used to select different heads Section 2.1, for instance. This
scenario is not compatible with sequential Bayesian inference outlined in Equation (3) since
no task identifier is required for making predictions. Domain-incremental learning is another
scenario that does not have access to tduring evaluation and requires the CL agent to
perform classification to the same output space for each task; for example, for Split-MNIST
the output space is {0, 1}for all tasks, so this amounts to classifying between even and odd
digits. Domain incremental learning is compatible with sequential Bayesian inference with
a Bernoulli likelihood. The third scenario is class-incremental learning which also does not
have access to tbut the agent needs to classify each example to its corresponding class.
For Split-MNIST, for example, the output space is {0,. . ., 9}for each task. Class-incremental
learning is compatible with sequential Bayesian inference with a categorical likelihood.
2.3. Variational Continual Learning
Variational CL (VCL; Nguyen et al. [9]) simplifies the Bayesian inference problem
in Equation (3) into a sequence of approximate Bayesian updates on the distribution over
random neural network weights θ. To do so, VCL uses the variational posterior from
previous tasks as a prior for new tasks. In this way, learning to solve the first task entails
finding a variational distribution q1(θ|D1)that maximizes a corresponding variational
objective. For the subsequent task, the prior is chosen to be q1(θ|D1), and the goal becomes
to learn a variational distribution q2(θ|D2)that maximizes a corresponding variational
objective under this prior. Denoting the recursive posterior inferred from multiple datasets
byqt(θ|D1:t), we can express the variational CL objective for the t-th task as:
L(θ,Dt) =DKL[qt(θ)||qt−1(θ|D1:t−1)]−Eqt[logp(Dt|θ)]. (6)

--- PAGE 4 ---
Entropy 2023 ,1, 0 4 of 28
When applying VCL to the problem of Split-MNIST Figure 1, we can see that single-
headed VCL barely performs better than SGD when remembering past tasks. Multi-
headed VCL performs better, despite not being a requirement from sequential Bayesian
inference Equation (3). Therefore, why does single-head VCL not improve over SGD if we
can recursively build up an approximate posterior using Equation (3)? We hypothesize
that it could be due to using a variational approximation of the posterior and so we are not
actually strictly performing the Bayesian CL process described in Section 2.2. We test this
hypothesis in the next section by propagating the true BNN posterior to verify whether we
can recursively obtain the true multi-task posterior and so improve on single-head VCL
and prevent catastrophic forgetting.
1 2 3 4 5
Tasks0.200.400.600.801.00Accuracy
Task 1 (0 vs. 1)
1 2 3 4 5
Tasks
Task 2 (2 vs. 3)
1 2 3 4 5
Tasks
Task 3 (4 vs. 5)
1 2 3 4 5
Tasks
Task 4 (6 vs. 7)
1 2 3 4 5
Tasks
Task 5 (8 vs. 9)
SGD
VCL SH
VCL MH
Figure 1. Accuracy on Split-MNIST for various CL methods with a two-layer BNN, all accuracies are
an average and standard deviation over 10runs with different random seeds. We compare an NN
trained with SGD (single-headed) with VCL. We consider single-headed (SH) and multi-head (MH)
VCL variants, i.e. domain and task incremental learning respectively.
3. Bayesian Continual Learning with Hamiltonian Monte Carlo
To perform inference over BNN weights we use the HMC algorithm [ 13]. We then use
these samples and learn a density estimator that can be used as a prior for a new task (we
considered Sequential Monte Carlo, but it is unable to scale to the dimensions required for
the NNs we consider [ 19]. HMC on the other hand has recently been successfully scaled
to relatively small BNNs of the size considered in this paper [ 20] and ResNet models but
at large computational cost [ 21]). HMC is considered the gold standard in approximate
inference and is guaranteed to asymptotically produce samples from the true posterior (in
the NeurIPS 2021 Bayesian Deep Learning Competition (https://izmailovpavel.github.io/
neurips_bdl_competition), the goal was to find an approximate inference method that is as
“close” as possible to the posterior samples from HMC). We use posterior samples of θfrom
HMC and then fit a density estimator over these samples, to use as a prior for a new task.
This allows us to use a multi-modal posterior distribution over θrather than a diagonal
Gaussian variational posterior such as in VCL. More concretely, to propagate the posterior
p(θ|D1)we use a density estimator, defined ˆp(θ|D1), to fit a probability density on HMC
samples as a posterior. For the next task T2we can use ˆp(θ|D1)as a prior for a new HMC
sampling chain and so on (see Figure 2). The density estimator priors need to satisfy two
key conditions for use within HMC sampling. Firstly, that they are a probability density
function. Secondly, that they are differentiable with respect to the input samples.
...
Figure 2. Illustration of the posterior propagation process; priors in blue are in the top row and
posterior samples on the bottom row. This is a two-step process where we first perform HMC with
an isotropic Gaussian prior for T1then perform density estimation on the HMC samples from the
posterior to obtain ˆp1(θ|D1). This posterior can then be used as a prior for the new task T2and so on.

--- PAGE 5 ---
Entropy 2023 ,1, 0 5 of 28
We use a toy dataset (Figure 3) with two classes and inputs x∈R2[22]. Each task is a
binary classification problem where the decision boundary extends from left to right for
each new task. We train a two-layer BNN, with a hidden state size of 10. We use Gaussian
Mixture Models (GMM) as a density estimator for approximating the posterior with HMC
samples. We also tried Normalizing Flows which should be more flexible [ 23]; however,
these did not work robustly for HMC sampling (RealNVP was very sensitive to the choice
of random seed, the samples from the learned distribution did not give accurate predictions
for the current task and led to numerical instabilities when used as a prior within HMC
sampling). To the best of our knowledge, we are the first to incorporate flexible priors into
the sampling methods such as HMC.
0 1 2
x10.5
0.00.51.0x2Task 1
Task 2Task 3
Task 4Task 5
1 2 3 4 5
Tasks0.000.501.00Accuracy
Task 1
1 2 3 4 5
Tasks0.600.801.00
Task 2
1 2 3 4 5
Tasks0.600.801.00
Task 3
1 2 3 4 5
Tasks0.600.801.00
Task 4
1 2 3 4 5
Tasks0.801.00
Task 5HMC MH VCL SH VCL SGD MT SGD/HMC
Figure 3. On the left is the toy dataset of 5distinct 2-way classification tasks that involve classifying
circles and squares [ 22]. Moreover, continual learning binary classification test accuracies over 10
seeds. The pink solid line is a multi-task (MT) baseline accuracy using SGD/HMC with the same
model as for the CL experiments. This is a domain incremental learning scenario, entirely consistent
with sequential Bayesian inference appendix B.
Training a BNN with HMC on the same multi-task dataset obtains a test accuracy of 1.0.
Thus, the final posterior is suitable for continual learning under Equation (3) and we should
be able to recursively arrive at the multi-task posterior with recursive Bayesian inference.
This means that if we were to sequentially build up the posterior eq. (3) then we should
expect an accuracy of 1.0as well since the multi-task posterior predictive performance
is an upper bound to the performance of the sequential Bayesian inference for continual
learning appendix B.
The results from Figure 3 demonstrate that using HMC with an approximate multi-
modal posterior fails to prevent forgetting and is less effective than using multi-head
VCL. In fact, multi-head VCL clearly outperforms HMC, indicating that the source of
the knowledge retention is not through the propagation of the posterior but through the
task-specific heads. For T2, we use ˆp(θ|D1)instead of p(θ|D1)as a prior and this will
bias the HMC sampling for all subsequent tasks. In the next paragraph, we detail the
measures taken to ensure that our HMC chains have converged so we can assume that
we are sampling from the true posterior. Moreover, we assess the fidelity of the GMM
density estimator with respect to the HMC samples. We also repeated these experiments
with another toy dataset of five binary classification tasks where we observed similar
results Appendix C.
For HMC, we ensure that we are sampling from the posterior by assessing chain
convergence and effective sample sizes (Figure A6). The effective sample size measures the
autocorrelation in the chain. The effective sample sizes for the HMC chains for our BNNs
are similar to the literature [ 20]. Moreover, we ensure that the GMM approximate posterior
is multi-modal and has a more complex posterior in comparison to VCL, and that the
GMM samples produce equivalent results to HMC samples for the current task (Figure A5).
See Appendix D for details.
The 2-d benchmarks we consider in this section are from previous works and are
domain-incremental continual learning problems. The domain incremental setting is
also simpler [ 18] than the class-incremental setting and thus a good starting point when
attempting to perform exact sequential Bayesian inference. Despite this, we are not able to
perform sequential Bayesian inference in BNNs despite using HMC, which is considered

--- PAGE 6 ---
Entropy 2023 ,1, 0 6 of 28
1.0
 0.8
 0.6
 0.4
 0.2
 0.0
x0.25
0.000.250.500.751.001.25yT ask 1
1.0
 0.5
 0.0 0.5 1.0
xT ask 2
T ask 1 data T ask 2 data Posterior PredictivePosterior Predictive Distributions
Figure 4. Posterior predictive distributions for a Bayesian linear regression model. Left, data comes
from a single task that the Bayesian linear model can fit well. Right, a new dataset is obtained from
a different part of the domain, and our sequentially updated Bayesian linear regression models
(correctly under Bayesian inference) a global solution to both of these datasets which is sub-optimal
for both.
the gold standard of Bayesian deep learning. HMC and density estimation with a GMM
produces richer, more accurate, and multi-modal posteriors. Despite this, we are still not
able to sequentially build up the multi-task posterior or obtain much better results than an
isotropic Gaussian posterior such as single-head VCL. The weak point of this method is the
density estimation, the GMM removes probability mass over areas of the BNN weight space
posterior, which is important for the new task. This demonstrates just how difficult a task
it is to model BNN weight posteriors. In the next section, we study a different analytical
example of sequential Bayesian inference and look at how model misspecification and task
data imbalances can cause forgetting in Bayesian CL.
4. Bayesian Continual Learning and Model Misspecification
We now consider a simple analytical example where we can perform the sequential
Bayesian inference eq. (3) in closed form using conjugacy. We consider a Bayesian linear
regression model with Gaussian likelihoods for 2continual learning tasks. This simple
example highlights that forgetting Equation (A3) may occur under certain conditions
despite correct inference.
We use a Gaussian likelihood of the form p(D|θ) =N(y;f(X;θ),β−1)such that
y=f(X;θ) +ϵwhere ϵ∼ N (0,β−1)and f(X;θ) =θX⊤. We put a Gaussian prior
over the parameters θsuch that p(θ) =N(θ;m0,Σ0)for our first task. Via conjugacy of
the Gaussian prior and likelihood, the posterior is also Gaussian, p(θ|D) =N(θ;m,Σ)
where m=Σ(Σ−1
0m0+βX⊤y)andΣ−1=Σ−1
0+βX⊤Xfor task 2and onwards. By
using sequential Bayesian inference we can have closed-form update equations for our
parameters.
From the form of the linear regression posterior, our model can only model linear data.
So, if we have data that is linear and drawn from a second task, from a distinct part of the
domain, then the model correctly models a linear model, which is the Bayes solution of
the multi-task problem appendix B. For example, the task 1dataset is generated according
toy=x+1+ϵ, where ϵ=N(0,β−1)forx∈[−1, 0). From fig. 4 we can see that our
Bayesian linear regression accurately models this first dataset. Now, if we sequentially
model a second dataset, with data drawn from y=−x+1+ϵ, where ϵ=N(0,β−1)for
x∈[0, 1]. The model regresses to both of these datasets: the continual learning regression

--- PAGE 7 ---
Entropy 2023 ,1, 0 7 of 28
from using the posterior from task 1as a prior for task 2is the same as if we were to regress
to the multi-task dataset of both tasks 1 and 2 (see fig. 4 on the right)1.
However, as we can see from fig. 4 (right), this is a suboptimal continual learning
solution, since we want the average performance eq. (A1) to be high for all tasks. More
specifically the performance after learning task 2isP2=p2,2+p2,1where pi,jis the perfor-
mance for task jafter learning task iand j≤i. Higher is better for the performance measure
pi,j, for regression pi,jcould be the log-likelihood. We see however, that performance is
low for the exact sequential Bayesian linear regression continual learning model fig. 4. As
with all continual learning benchmarks, we require our model to perform equally well
on both tasks. In this case, we can specify a better model which is not just a Bayesian
linear regression model, but a mixture of linear regressors [ 24].Despite performing exact
inference a misspecified model can forget . We get forgetting of task 1after learning task 2, since
the forgetting metric eq. (A3), is f2
1=p1,1−p2,1>0since p1,1>p2,1which indicates
forgetting, as can be seen from fig. 4.
In the case of HMC, we verified that our Bayesian neural network had perfect perfor-
mance on all tasks beforehand . In section 3 we had a well-specified model but struggled with
exact sequential Bayesian inference eq. (3). With this Bayesian linear regressions scenario
we are performing exact inference, however, we have a misspecified model and so unable
to obtain good performance on both tasks appendix B. It is important to disentangle model
misspecification and exact inference and highlight that model misspecification is a caveat
that has not been highlighted in the CL literature as far as we are aware. Furthermore, we
can only ensure that our models are well specified if we have access to data from all tasks a
priori. So in the scenario of online continual learning [25–27] we cannot know if our model
will perform well on all past and future tasks without making assumptions on the task
distributions.
5. Sequential Bayesian Inference and Imbalanced Task Data
Neural Networks are complex models with a broad hypothesis space and hence are a
suitably well-specified model when tackling continual learning problems [ 28]. However,
we struggle to fit the posterior samples from HMC to perform sequential Bayesian inference
in Section 3.
We continue to use Bayesian filtering and assume a Bayesian NN where the posterior
is Gaussian with a full covariance. By modeling the entire covariance, we enable modeling
of how each individual weight varies with respect to all others. We do this by interpreting
online learning in Bayesian NNs as filtering [ 29]. Our treatment is similar to Aitchison [30],
who derives an optimizer by leveraging Bayesian filtering. We consider inference in the
graphical model depicted in Figure 5. The aim is to infer the optimal BNN weights, θ∗
tat
tgiven a single observation and the BNN weight prior. The previous BNN weights are
used as a prior for inferring the posterior BNN parameters. We consider the online setting,
where a single data point (xt,yt)is observed at a time.
1A previous version of this section had a similar example where data for both tasks was generated from the same
domains p1(x) =p2(x), but the conditional distributions mapped to different outputs p1(y|x)̸=p2(y|x). In
this previous example, the tasks were conflicting and so the Bayesian multitask posterior was also sub-optimal
for continual learning. Now, in this 2task linear regression setup, the domains for both tasks are distinct and
so the tasks are not conflicting.

--- PAGE 8 ---
Entropy 2023 ,1, 0 8 of 28
θ∗
t−1θ∗
t−2 θ∗
tθ∗
t+1yt−1 yt−2 yt yt+1
xt−1 xt−2 xt xt+1. . . . . .
Figure 5. Graphical model for filtering. Grey and white nodes and latent variables are observed, re-
spectively.
Instead of modeling the full covariance, we instead consider each parameter θias a
function of all the other parameters θ−it. We also assume that the values of the weights
are close to those of the previous timestep [ 31]. To obtain the updated equations for BNN
parameters given a new observation and prior, we make two simplifying assumptions
as follows.
Assumption 1. For a Bayesian neural network with output f(xt;θ)and likelihood L(xt,yt;θ),
the derivative evaluated at θtiszt=∂L(xt,yt;θ)/∂θ|θ=θtand the Hessian is H. We assume a
quadratic loss for a data point (xt,yt)of the form:
L(xt,yt;θ) =Lt(θ) =−1
2θ⊤Hθ+z⊤
tθ, (7)
the result of a second-order Taylor expansion. The Hessian is assumed to be constant with respect to
(xt,yt)(but not with respect to θ).
To construct the dynamical equation for θ, consider the gradient for the i-th weight
while all other parameters are set to their current estimate at the optimal value for the θ∗
it:
θ∗
it=−1
HiiH⊤
−iiθ−it, (8)
since zit=0at a mode. The equation above shows us that the dynamics of the optimal
weight θ∗
itis dependent on all the other current values of the parameters θ−it. The dynamics
ofθ−itare a complex stochastic process dependent on many different variables such as the
dataset, model architecture, learning rate schedule, etc.
Assumption 2. Since reasoning about the dynamics of θ−itis intractable, we assume that at the
next timestep, the optimal weights are close to the previous timesteps with a discretized Ornstein–
Uhlenbeck process for the weights θ−itwith reversion speed ϑ∈R+and noise variance η2
−i:
p(θ−i,t+1|θ−i,t) =N((1−ϑ)θ−it,η2
−i), (9)
this implies that the dynamics for the optimal weight are defined by
p(θ∗
i,t+1|θ∗
i,t) =N((1−ϑ)θ∗
it,η2), (10)
where η2=η2
−iH⊤
−iiH−ii.
In simple terms, in Assumption 2, we assume a parsimonious model of the dynamics,
and that the next value of θ−i,tis close to their previous value according to a Gaussian,
similarly to Aitchison [30].

--- PAGE 9 ---
Entropy 2023 ,1, 0 9 of 28
Lemma 1. Under Assumptions 1 and 2 the dynamics and likelihood are Gaussian. Thus, we are
able to infer the posterior distribution over the optimal weights using Bayesian updates and by
linearizing the BNN the update equations for the posterior of the mean and variance of the BNN for
a new data point are:
µt,post=σ2
t,post 
µt,prior
σ2
t,prior(η2)+yt
σ2g(xt)!
and1
σ2
t,post=g(xt)2
σ2+1
σ2
t,prior(η2), (11)
where we drop the notation for the i-th parameter, the posterior is N(θ∗
t;µt,post,σ2
t,post)andg(xt) =
∂f(xt;θ∗
it)
∂θ∗
itandσ2
t,prioris a function of η2.
See Appendix G for the derivation of Lemma 1. From Equation (11), we can notice
that the posterior mean depends linearly on the prior and a data-dependent term and
so will behave similarly to our previous example in Section 4. Under Assumption 1 and
Assumption 2, if there is a data imbalance between tasks in Equation (11), then the data-
dependent term will dominate the prior term if there is more data for the current task.
In Section 3, we showed that it is very difficult with current machine learning tools to
perform sequential Bayesian inference for simple CL problems with small Bayesian NNs.
When we disentangle Bayesian inference and model misspecification, we show showed
that misspecified models can forget despite exact Bayesian inference. The only way to
ensure that our model is well specified is to show that the multi-task posterior produces
reasonable posterior predictive distributions p(y|x,D) =R
p(y|x,D,θ)p(θ|D)dθfor one’s
application. Additionally, in this section, we have shown that if there is a task dataset size
imbalance, then we can obtain forgetting under certain assumptions.
6. Related Work
There has been a recent resurgence in the field of CL [ 32] given the advent of deep
learning. Methods that approximate sequential Bayesian inference Equation (3) have been
seminal in CL’s revival and have used a diagonal Laplace approximation [ 3,7]. The diago-
nal Laplace approximation has been enhanced by modeling covariances between neural
network weights in the same layer [ 8]. Instead of the Laplace approximation, we can use
a variational approximation for sequential Bayesian inference, named VCL [ 9,33]. The
variational Gaussian variance of each Bayesian NN parameter can be used to pre-condition
the learning rates of each weight and create a mask per task by using pruning [ 10]. Using
richer priors has also been explored [ 11,34–37]. For example, one can learn a scaling of the
Gaussian NN weight parameters for each task by learning a new variational adaptation
parameter which can strengthen the contribution of a specific neuron [ 38]. The online
Laplace approximation can be seen as a special case of VCL where the KL-divergence
term Equation (6) is tempered and the temperature tends to 0[12]. Gaussian processes
have also been applied to CL problems leveraging inducing points to retain previous task
functions [39,40].
Bayesian methods that regularize weights have not matched up to the performance of
experience replay-based CL methods [ 41] in terms of accuracy on CL image classification
benchmarks. Instead of regularizing high-dimensional weight spaces, regularizing task
functions is a more direct approach to combat forgetting [ 42]. Bayesian NN weights can
also be generated by a hypernetwork, where the hypernetwork needs only simple CL
techniques to prevent forgetting [ 43]. In particular, one can leverage the duality between
the Laplace approximation and Gaussian processes to develop a functional regularization
approach to Bayesian CL [44] or using function-space variational inference [45,46].
In the next section, we propose a simple Bayesian continual learning baseline that
models the data-generating continual learning process and performs exact sequential
Bayesian inference in a low-dimensional embedding space. Previous work has explored
modeling the data-generating process by inferring the joint distribution of inputs and
targets p(x,y)and learning a generative model to replay data to prevent forgetting [ 47],

--- PAGE 10 ---
Entropy 2023 ,1, 0 10 of 28
and by learning a generative model per class and evaluating the likelihood of the inputs
given each class p(x|y)[48].
7. Prototypical Bayesian Continual Learning
We have shown that sequential Bayes over NN parameters is very difficult (section 3),
and is only suitable for situations where the multi-task posterior is suitable for all tasks.
We now show that a more fruitful approach is to model the generative CL problem with
a generative classifier where each class is represented by a prototype and classification is
distance based to the prototype. This is simple and scalable. In particular, we represent
classes by prototypes [ 49,50] and maintain prototypes with a replay buffer to prevent
catastrophic forgetting. We refer to this framework as Prototypical Bayesian Continual
Learning, or ProtoCL for short. This approach can be viewed as a probabilistic variant
of iCarl [ 50], which creates embedding functions for different classes which are simply
class means and predictions are made by nearest neighbors. ProtoCL also bears similarities
to the few-shot learning model Probabilistic Clustering for Online Classification [ 51] and
MetaQDA [ 52], developed for few-shot image classification. MetaQDA uses Normal-
inverse Wishart conjugate priors for Gaussian quadratic discriminant analysis (QDA),
ProtoCL has different design choices as follows.
enc
 encEmbedding
SpaceEmbedding
Space
Figure 6. Overview of ProtoCL.
Model. ProtoCL models the generative CL process. We consider classes j∈ {1,. . .,J},
generated from a categorical distribution with a Dirichlet prior:
yi,t∼Cat(p1:J),p1:J∼Dir(αt). (12)
Images are embedded into an embedding space by an encoder, z=f(x;w)with parameters
w. The per-class embeddings are Gaussian whose with isotropic variance. The prototype
mean has a prior which is also Gaussian with and diagonal covariance:
zit|yit∼ N(¯zyt,Σϵ), ¯zyt∼ N(µyt,Λ−1
yt). (13)
See fig. 6 for an overview of the model. To alleviate forgetting in CL, ProtoCL uses
a coreset of past task data to continue to embed past classes distinctly as prototypes. The
posterior distribution over class probabilities {pj}J
j=1and class embeddings {¯zyj}J
j=1is
denoted in short hand as p(θ)with parameters ηt={αt,µ1:J,t,Λ−1
1:J,t}. We model the Gaus-
sians with a diagonal covariance. ProtoCL models each class prototype but does not use
task-specific NN parameters or modules like multi-head VCL. ProtoCL uses a probabilistic
model over an embedding space which allows it to use powerful embedding functions

--- PAGE 11 ---
Entropy 2023 ,1, 0 11 of 28
Algorithm 1 ProtoCL continual learning
1:Input: task datasets T1:T, initialize embedding function: f(·;w), coreset: M=∅.
2:forT1toTTdo
3: foreach batch in Ti∪ M do
4: Optimize f(·;w)by maximizing the posterior predictive p(z,y)eq. (17)
5: Obtain posterior over θby updating η, eqs. (14) to (16).
6: end for
7: Add random subset from TitoM.
8:end for
f(·;w)without having to parameterize them probabilistically and so this approach will be
more scalable than VCL, for instance.
Inference. As the Dirichlet prior is conjugate with the Categorical distribution and likewise
the Gaussian over prototypes with a Gaussian prior over the prototype mean, we can
calculate posteriors in closed form and update the parameters ηtas new data is observed
without using gradient-based updates. We optimize the model by maximizing the posterior
predictive distribution and use a softmax over class probabilities to perform predictions.
We perform gradient-based learning of the NN embedding function f(·;w)and update
the parameters, ηtat each iteration of gradient descent as well, see algorithm 1.
Sequential updates. We can obtain our parameter updates for the Dirichlet posterior by
Categorical-Dirichlet conjugacy:
αt+1,j=αt,j+Nt
∑
i=1I(yi
t=j), (14)
where Ntare the number of points seen during the update at time step t. Also, due to
Gaussian-Gaussian conjugacy, the posterior for the Gaussian prototypes is governed by:
Λyt+1=Λyt+NyΣ−1
ϵ (15)
Λyt+1µyt+1=NyΣ−1
ϵ¯zyt+Λytµyt,∀yt∈Ct, (16)
where Nyare the number of samples of class yand ¯zyt= (1/Ny)∑Ny
i=1zyi, see appendix F
for the detailed derivation.
Objective. We optimize the posterior predictive distribution of the prototypes and classes:
p(z,y) =Z
p(z,y|θt;ηt)p(θt;ηt)dθt=p(y)Nt
∏
i=1N(zit|yit;µyt,t,Σϵ+Λ−1
yt,t). (17)
Where the p(y) =αy/∑J
j=1αj, see appendix F.3 for the detailed derivation. This objec-
tive can then be optimized using gradient-based optimization for learning the prototype
embedding function z=f(x;w).
Predictions. To make a prediction for a test point x∗the class with the maximum (log)-
posterior predictive is chosen, where the posterior predictive is:
p(y∗=j|x∗,x1:t,y1:t) =p(y∗=j|z∗,θt) =p(y∗=j,z∗|θt)
∑ip(y=i,z∗|θt), (18)
see appendix F.4 for further details.
Preventing forgetting. We use coresets to retain the class prototypes. Coresets are ran-
domly sampled data from previous tasks which are then stored together in a replay buffer
and added to the next task training set. At the end of learning a task Tt, we retain a subset
Mt⊂ D tand augment each new task dataset to ensure that posterior parameters ηtand
prototypes are able to retain previous task information. With no coreset the average accu-
racy over Split-MNIST is 33.25±0.15, fig. 7 dramatically below 93.73±1.05 from table 1.

--- PAGE 12 ---
Entropy 2023 ,1, 0 12 of 28
0250 500 7501000 1500 2000
mem. size405060708090Accuracy
Figure 7. Split-MNIST average test accuracy over 5tasks for different memory sizes. On the x-axis
we show the size of the entire memory buffer shared by all 5tasks. Accuracies are over a mean and
standard deviation over 5 different runs with different random seeds.
So the main mechanism for preventing forgetting is the replay buffer which enables the net-
work to maintain a prototype per class, rather than the sequential Bayesian inference in the
prototype embedding space similarly to functional Bayesian regularization methods [53].
Class-incremental learning. In this CL setting we do not tell the CL agent which task it is
being evaluated on with a task identifier t. So we cannot use the task identifier to select
a specific head to use for classifying a test point, for example. Also, we require the CL
agent to identify each class, {0,. . ., 9}for Split-MNIST and Split-CIFAR10 for example,
and not just {0, 1}as in domain-incremental learning. Class-incremental learning is more
general, realistic, and harder a problem setting and thus important to focus on rather than
other settings, despite domain-incremental learning also being compatible with sequential
Bayesian inference as described in eq. (3).
Implementation. For Split-MNIST and Split-FMNIST the baselines and ProtoCL all use
two-layer NNs with a hidden state size of 200. For Split-CIFAR10 and Split-CIFAR100,
the baselines and ProtoCL use a four-layer convolution neural network with two fully
connected layers of size 512similarly to Pan et al. [22]. For ProtoCL and all baselines
that rely on replay, we fix the size of the coreset to 200points per task. For all ProtoCL
models, we allow the prior Dirichlet parameters to be learned and set their initial value to
0.7found by a random search over MNIST with ProtoCL. An important hyperparameter
for ProtoCL is the embedding dimension of the Gaussian prototypes for Split-MNIST and
Split-FMNIST this was set to 128while for the larger vision datasets, this was set to 32
found using grid-search.
Table 1. Mean accuracies across all tasks over CL vision benchmarks for class incremental learning [17].
All results are averages and standard errors over 10seeds.∗Uses the predictive entropy to make a
decision about which head for class incremental learning.
Method Coreset Split-MNIST Split-FMNIST
VCL [9] ✗ 33.01±0.08 32.77 ±1.25
+coreset ✓ 52.98±18.56 61.12 ±16.96
HIBNN∗[11] ✗ 85.50±3.20 43.70 ±20.21
FROMP [22] ✓ 84.40±0.00 68.54 ±0.00
S-FSVI [46] ✓ 92.94±0.17 80.55±0.41
ProtoCL ( ours ) ✓ 93.73±1.05 82.73 ±1.70

--- PAGE 13 ---
Entropy 2023 ,1, 0 13 of 28
Table 2. Mean accuracies across all tasks over CL vision benchmarks for class incremental learning [17].
All results are averages and standard errors over 10seeds.∗Uses the predictive entropy to make a
decision about which head for class incremental learning. Training times have been benchmarked
using an Nvidia RTX3090 GPU.
Method Training time (sec) (↓) Split CIFAR-10 (acc) (↑)
FROMP [22] 1425 ±28 48.92 ±10.86
S-FSVI [46] 44434 ±91 50.85 ±3.87
ProtoCL ( ours ) 384±6 55.81 ±2.10
Split CIFAR-100 (acc)
S-FSVI [46] 37355 ±1135 20.04 ±2.37
ProtoCL ( ours ) 1425±28 23.96 ±1.34
Results. ProtoCL produces good results on CL benchmarks on par or better than S-FSVI [ 46]
which is state-of-the-art among Bayesian CL methods while being a lot more efficient to
train and without requiring expensive variational inference. ProtoCL can flexibly scale to
larger CL vision benchmarks producing better results than S-FSVI. Code to reproduce all
experiments can be found here https://github.com/skezle/bayes_cl_posterior. All our
experiments are in the more realistic class incremental learning setting, which is a harder
setting than those reported in most CL papers, so the results in table 1 are lower for certain
baselines than in the respective papers. We use 200data points per task, see fig. 7 for a
sensitivity analysis of the performance over the Split-MNIST benchmark as a function of
core size for ProtoCL.
The stated aim of ProtoCL is not to provide a novel state-of-the-art method for CL,
but rather to propose a simple baseline that takes an alternative route than weight-space
sequential Bayesian inference. We can achieve strong results that mitigate forgetting,
namely by modeling the generative CL process and using sequential Bayesian inference
over a few parameters in the class prototype embedding space. We argue that modeling
the generative CL process is a fruitful direction for further research rather than attempting
sequential Bayesian inference over the weights of a BNN. ProtoCL scales to 10tasks of
Split-CIFAR100 which to the best of our knowledge, is the most number of tasks and classes
which has been considered by previous Bayesian continual learning methods.
8. Discussion and Conclusions
In this paper, we revisited the use of sequential Bayesian inference for CL. We can use
sequential Bayes to recursively build up the multi-task posterior Equation (3). Previous
methods have relied on approximate inference and see little benefit over SGD. We test the
hypothesis of whether this poor performance is due to the approximate inference scheme
by using HMC in two simple CL problems. HMC asymptotically samples from the true
posterior, and we use a density estimator over HMC samples to use as a prior for a new
task within the HMC sampling process. We perform many checks for HMC convergence.
This density is multi-modal and accurate with respect to the current task but is not able to
improve over using an approximate posterior. This demonstrates just how challenging it is
to work with BNN weight posteriors. The source of error comes from the density estimation
step. We then look at an analytical example of sequential Bayesian inference where we
perform exact inference; however, due to model misspecification, we observe forgetting.
The only way to ensure a well-specified model is to assess the multi-task performance
over all tasks a priori. This might not be possible in online CL settings. We then model
an analytical example over Bayesian NNs and, under certain assumptions, show that
if there are task data imbalances then this will cause forgetting; data imbalances are a
common problem in many areas of machine learning in addition to continual learning [ 54].
Sequential Bayesian inference is also not exempt from the effects of task data imbalances.
Because of these results, we argue against performing weight space sequential Bayesian

--- PAGE 14 ---
Entropy 2023 ,1, 0 14 of 28
inference and instead model the generative CL problem. We introduce a simple baseline
called ProtoCL. ProtoCL does not require complex variational optimization and achieves
competitive results to the state-of-the-art in the realistic setting of class incremental learning.
This conclusion should not be a surprise since the latest Bayesian CL papers have
all relied on multi-head architectures or inducing points/coresets to prevent forgetting,
rather than better weight-space inference schemes. Our observations are in line with recent
theory from [ 55], which states that optimal CL requires perfect memory. Although the
results were shown with deterministic NNs the same results follow for BNN with a single
set of parameters. Future research efforts should focus on more functional approaches to
sequential Bayesian inference, in which previous task functions are remembered [ 22,45,53].
This shifts the problem of remembering previous task functions to a coreset similar to
sparse variational-inference Gaussian Processes [56,57].
Author Contributions: S.K lead the research including conceptualization, performing the experiments
and writing the paper. S.J.R helped with conceptualization. A.C helped helped with the development
of the ideas and the implementation of HMC with a density estimator as a prior. T.G.J.R ran the
S-FSVI baselines for the class incremental continual learning experiments. T.G.J.R, A.C and S.J.R
helped to write the paper.
Funding: S.K. acknowledges funding from the Oxford-Man Institute of Quantitative Finance. T.G.J.R.
acknowledges funding from the Rhodes Trust, Qualcomm, and the Engineering and Physical Sciences
Research Council (EPSRC). This material is based upon work supported by the United States Air
Force and DARPA under Contract No. FA8750-20-C-0002. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the author(s) and do not necessarily reflect
the views of the United States Air Force and DARPA.
Institutional Review Board Statement: Not applicable.
Data Availability Statement: All data is publically available, code to reproduce all experiments can
be found here https://github.com/skezle/bayes_cl_posterior.
Acknowledgments: We would like to thank Sebastian Farquhar, Laurence Aitchison, Jeremias
Knoblauch, and Chris Holmes for discussions. We would also like to thank Philip Ball for his help
with writing the paper.
Conflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design
of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or
in the decision to publish the results.

--- PAGE 15 ---
Entropy 2023 ,1, 0 15 of 28
Abbreviations
The following abbreviations are used in this manuscript:
CL Continual Learning
NN Neural Network
BNN Bayesian Neural Network
HMC Hamiltonian Monte Carlo
VCL Variational Continual Learning
SGD Stochastic Gradient Descent
SH Single Head
MH Multi-head
GMM Gaussian Mixture Model
ProtoCL Prototypical Bayesian Continual Learning

--- PAGE 16 ---
Entropy 2023 ,1, 0 16 of 28
Appendix
Table of Contents
A Continual Learning Experimental Scenarios . . . . . . . . . . . . . . . . . . . . . 16
B Continual Learning Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C The Toy Gaussians Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D HMC Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
E Density Estimation Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
F Prototypical Bayesian Continual Learning . . . . . . . . . . . . . . . . . . . . . . 20
F.1 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
F.2 Sequential Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.3 ProtoCL Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.4 Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
F.5 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
G Sequential Bayesian Estimation as Bayesian Neural Network Optimization . . 24
H. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Appendix A. Continual Learning Experimental Scenarios
To evaluate different continual learning methods, we need to define the commonly
used continual learning scenarios that are used in the literature and throughout this paper.
Previous work has introduced models for continual learning where the NN architec-
tures used a feature extractor shared among all continual learning tasks, but a bespoke
feature to output linear layer is trained for each task and then frozen [ 15]. Alternatively,
other works have sought methods that do not require this manual selection of different
feature to output linear heads and proposed a single feature to output linear layer which
is shared among all tasks in continual learning [ 58]. In this section, we will categorize
and systematically interpret the different continual learning scenarios, similar to previous
important work [16,17].
In terms of notation, a task Ttcan be characterized by the conditional and marginal
data distributions pt(y|x)and pt(x)and a task identifier twe denote samples from the
distributions y∼pt(y)and x∼pt(x).
Task incremental learning. This first scenario and generally the easiest scenario for
continual learning. Each task is a subset of classes in the dataset where the input domains
are disjoint p1(x)̸=p2(x)while the output spaces are shared among all tasks p1(y) =p2(y)
a task identifier is also available t. For example, for Split CIFAR10 all tasks are a binary
classification problems, and the classes are all mapped to {0, 1}for each task. The task
identifier can be used to select a different linear layer per task [ 3,9,15] — described as a
multi-head network. The task identifier can be used during training and for evaluation.
Domain incremental learning. In this scenario the domain explicitly increases, since
p1(x)̸=p2(x)but the learner is required to retain knowledge about previous domains.
The output spaces remain shared by all tasks p1(y) =p2(y). In contrast to task incremental
learning, no task identifier is available to the continual learning agent.
Class incremental learning. In this scenario the domain increases p1(x)̸=p2(x)as
the number of tasks increases, while the number of classes seen increases as the number of
tasks increases, so p1(y)̸=p2(y). Additionally, no task identifier is available to the agent.
An example is illustrated in fig. A1.
Multi-head versus single-head networks. A common design choice not exclusively
used in continual learning is to have an output linear layer per task, or a linear head per
task, h, map to outputs h:Z → Y , where y∈ Y. Such that a continual learning agent

--- PAGE 17 ---
Entropy 2023 ,1, 0 17 of 28
010011Task Incremental Learning
01None01None
01None23NoneDomain Incremental LearningClass Incremental Learning
Figure A1. Three continual learning scenarios. Example datapoints from 2tasks from the Split
CIFAR10 benchmark. The first task is binary classification of airplanes versus automobiles and the
second task is binary classification of birds versus cats. In each row we have a different task, in
each sub-row in each task the exact class ywhich needs to be predicted is enumerated, and the
task identifier, t, is shown. The support of the discrete class labels is defined as supp(P(y)) ={y∈
{0, . . . , 9 }:P(y)>0}.
uses a separate head per task {hi}T
i=1, these methods are called multi-headed while those
that use one head are called single-headed. Note that the multi-head networks are only
compatible with task-incremental learning since they require knowledge of a task identifier, t,
to select a new head during training and the head that corresponds to a specific task during
evaluation. On the other hand, the single-headed network doesn’t require knowledge of the
task identifier during training and evaluation and so is compatible with domain-incremental
and class-incremental learning.
Appendix B. Continual Learning Metrics
I will define the metrics which are used to measure the performance of supervised
continual learning agents and which are used in the experimental setups in this paper.
Average Performance. Letpk,jbe the performance of the continual learning agent,
higher performance is better, such as the accuracy on the test set jafter training incre-
mentally on tasks 1,. . .,kand j≤k. Then the average performance at task kis defined
as:
Pk=1
kk
∑
i=1pk,i. (A1)
The higher the average performance Pkover all tasks 1,. . .,jthe better the continual learner
is at learning all tasks T:jseen so far. For classification the performance pk,jis the accuracy
ak,jand for a canonical benchmark like Split MNIST which has 5tasks in total then we
measure the performance at the end of training on the last task i.e. k=5:
P5=1
55
∑
i=1a5,i (A2)
which averages the performance over the test sets j=1,. . ., 5. This is the main metric
which is used throughout the paper as it simultaneously measures how well a continual
learner is able to perform a specific task and how well it retains knowledge and prevents
catastrophic forgetting.

--- PAGE 18 ---
Entropy 2023 ,1, 0 18 of 28
Average Forgetting. This is defined as the difference between the performance after a
task is trained and the current performance. This difference defines the performance gap
and therefore how much the model has forgotten how to perform a task. The forgetting for
the task jafter learning ktasks 1, . . . , kand j<kis defined as:
fk
j=pj,j−pk,j,∀j<k. (A3)
The average forgetting can only be defined for the previous k−1 tasks as:
Fk=1
k−1k−1
∑
j=1fk
j. (A4)
AnFkclose to zero implies little forgetting. A negative forgetting implies that the perfor-
mance improves throughout continual learning and the learner can transfer knowledge
from future tasks when being evaluated on previous tasks. This is a very desirable and yet
rare property of a continual learner. A positive Fkindicates forgetting and the performance
on task jdegrades after learning task k. A note of caution: we can get no forgetting while
our learner has not learned anything at all: and pk,j=0,∀jimplies no forgetting Fk=0
which is also undesirable.
Performance upper bounds An upper bound to performance is how well the continual
learning model can do in comparison to a multi-task model which can learn from all tasks
at once. A different upper bound on performance is the single task performance; the
performance of training a single different machine learning model on each individual task,
this model won’t benefit from transfer available to the multi-task model.
Multi-task performance as an upper bound is natural for the computer vision continual
learning benchmarks since the tasks are constructed by subsetting classes. In sequential
Bayesian inference, the upper bound on performance is the multi-task posterior which
the sequential posterior builds up through sequential Bayesian updates eq. (5). As we
discuss in section 4 the Bayesian multi-task performance can sub-optimal for the continual
learning problem we want to solve and the continual learning solution can outperform the
multi-task solution.
Appendix C. The Toy Gaussians Dataset
See Figure A2 for a visualization of the toy Gaussians dataset, which we use as a simple
CL problem. This is used for evaluating our method for propagating the true posterior by
using HMC for posterior inference and then using a density estimator on HMC samples as
a prior for a new task. We construct 5,2-way classification problems for CL. Each 2-way
task involves classifying adjacent circles and squares Figure A2. With a 2layer network
with 10neurons we obtain a test accuracy of 1.0for the multi-task learning of all 5tasks
together. Hence, according to Equation (3) a BNN with the same size should be able to
learn all 5binary classification tasks continually by sequentially building up the posterior.
Appendix D. HMC Implementation Details
We set the prior for T1, top1(θ) =N(0,τ−1I)with τ=10. We burn-in the HMC
chain for 1000 steps and sample for 10, 000 more steps and run 20different chains to obtain
samples from our posterior, which we then pass to our density estimator. We use a step
size of 0.001 and trajectory length of L=20, see Appendix E for further implementation
details of the density estimation procedure. For the GMM, we optimize for the number of
components by using a holdout set of HMC samples.
Appendix E. Density Estimation Diagnostics
We provide plots to show that the HMC chains indeed sample from the posterior
have converged in Figures A4 and A6. We run 20HMC sampling chains and randomly
select one chain to plot for each seed (of 10). We run HMC over 10seeds and aggregate the

--- PAGE 19 ---
Entropy 2023 ,1, 0 19 of 28
results Figures 3 and A2. The posteriors p(θ|D1),. . .are approximated with a GMM and
used as a prior for the second task and so forth.
We provide empirical evidence to show that the density estimators have fit to HMC
samples of the posterior in Figures A3 and A5, where we show the number of components
of the GMM density estimator, which we use as a prior for a new task, are all multi-modal
posteriors. We show the BNN accuracy when sampling BNN weights from our GMM all
recover the accuracy of the converged HMC samples. The effective sample size (ESS) of the
20chains is a measure of how correlated the samples are (higher is better). The reported
ESS values for our experiments are in line with previous work which uses HMC for BNN
inference [20].
2
 1
 0 1 2
x12
1
012x2Task 1
Task 2Task 3
Task 4Task 5
1 2 3 4 5
Tasks0.000.501.00Accuracy
Task 1
1 2 3 4 5
Tasks0.000.501.00
Task 2
1 2 3 4 5
Tasks0.600.801.00
Task 3
1 2 3 4 5
Tasks0.400.600.801.00
Task 4
1 2 3 4 5
Tasks0.800.901.00
Task 5
HMC
MH VCL
SH VCL
SGD
MT SGD/HMC
Figure A2. Continual learning binary classification accuracies from the toy Gaussian dataset similar
to [43] using 10random seeds. The pink solid line is a multi-task (MT) baseline test accuracy using
SGD/HMC.
1 2 3 4 5
Task01020304050ESS
1 2 3 4 5
Task0.00.20.40.60.81.0Acc. sampled from 
 GMM posterior
1 2 3 4 5
Task0510152025# GMM components 
 in posterior
Figure A3. Diagnostics from using a GMM prior fit to samples of the posterior generated from HMC,
all results are for 10random seeds. Left, effective sample sizes (ESS) of the resulting HMC chains of
the posterior, all are greater than those reported in other works using HMC for BNNs [ 20].Middle,
the accuracy of the BNN when using samples from the GMM density estimator instead of the samples
from HMC. Right, The optimal number of components of each GMM posterior fitted with a holdout
set of HMC samples by maximizing the likelihood.
0.000.250.500.751.00AccuracyTask 1 Task 2 Task 3 Task 4 Task 5
0.00.20.40.6nll
Iteration number
Figure A4. Convergence plots from a one randomly sampled HMC chain (of 20) for each task over
10different runs (seeds) for 5tasks from the toy Gaussian dataset similar to Henning et al. [43]
(visualized in Figure A2). We use a GMM density estimator as the prior conditioned on the previous
task data.

--- PAGE 20 ---
Entropy 2023 ,1, 0 20 of 28
12345
Task01020304050ESS
12345
Task0.00.20.40.60.81.0Task Acc.
12345
Task0.00.20.40.60.81.0Acc. sampled from 
 GMM posterior
12345
Task0510152025# GMM components 
 in posterior
Figure A5. Diagnostics from using a GMM to fit samples of the posterior HMC samples, all results
are for 10random seeds on the toy dataset from Pan et al. [22] (and visualized in Figure 3). Left,
effective sample sizes (ESS) of the resulting HMC chains of the posterior, all are greater than those
reported in other works using HMC for BNNs [ 20].Middle left . the current task accuracy from
HMC sampling. Middle right , the accuracy of the BNN when using samples from the GMM density
estimator instead of the converged HMC samples. Right , The optimal number of components of each
GMM posterior fitted with a holdout set of HMC samples by maximizing the likelihood.
0.51.0AccuracyTask 1 Task 2 Task 3 Task 4 Task 5
0.51.0NLL
HMC Iteration
Figure A6. Convergence plots from a randomly sampled HMC chain (of 20) for each task over 10
different seeds for 5tasks from the toy dataset from [ 22] (see Figure 3 for a visualization of the data).
We use a GMM density estimator as a prior.
Appendix F. Prototypical Bayesian Continual Learning
ProtoCL models the generative process of CL where new tasks are comprised of new
classes j∈ {1,. . .,J}of a total of Jand can be modeled by using a categorical distribution
with a Dirichlet prior:
yi,t∼Cat(p1:J),p1:J∼Dir(αt). (A5)
We learn a joint embedding space for our data with a NN, z=f(x;w)with parameters
w. The embedding space for each class is Gaussian whose mean has a prior which is also
Gaussian:
zit|yit∼ N(¯zyt,Σϵ), ¯zyt∼ N(µyt,Λ−1
yt). (A6)
By ensuring that we have an embedding per class and using a memory of past
data, we ensure that the embedding does not drift. The posterior parameters are ηt=
{αt,µ1:J,t,Λ−1
1:J,t}.
Appendix F.1. Inference
As the Dirichlet prior is conjugate with the categorical distribution and so is the
Gaussian distribution with a Gaussian prior over the mean of the embedding, then we can
calculate posteriors in closed form and update our parameters as we see new data online
without using gradient-based updates. We perform gradient-based learning of the NN
embedding function f(·;w)with parameters w. We optimize the model by maximizing
the log-predictive posterior of the data and use the softmax over class probabilities to
perform predictions. The posterior over class probabilities {pj}J
j=1and class embeddings
{¯zyj}J
j=1is denoted as p(θ)for short hand and has parameters are ηt={αt,µ1:J,t,Λ−1
1:J,t}
are updated in closed form at each iteration of gradient descent.

--- PAGE 21 ---
Entropy 2023 ,1, 0 21 of 28
Appendix F.2. Sequential Updates
We can obtain our posterior:
p(θt|Dt)∝p(Dt|θt)p(θt) (A7)
=Nt
∏
i=1p(zi
t|yi
t; ¯zyt,Σϵ,yt)p(yi
t|p1:J)p(pi:J;αt)p(¯zyt;µyt,t,Λ−1
yt,t) (A8)
=N(µt+1,Σt+1)Dir(αt+1), (A9)
where Ntis the number of data points seen during update t. Concentrating on the
Categorical-Dirichlet conjugacy:
Dir(αt+1)∝p(p1:J;αt)Nt
∏
i=1p(yi
t;pi:J) (A10)
∝J
∏
j=1pαj−1
jNt
∏
i=1J
∏
j=1pI(yi
t=j)
j(A11)
=J
∏
j=1pαj−1+∑Nt
i=1I(yi
t=j)
j. (A12)
Thus:
αt+1,j=αt,j+Nt
∑
i=1I(yi
t=j). (A13)
Moreover, due to Gaussian-Gaussian conjugacy, then the posterior for the Gaussian proto-
type of the embedding for each class is:
N(µt+1,Λt+1)∝Nt
∏
i=1N(zi
t|yi
t; ¯zyt,Σϵ)N(¯zyt;µyt,t,Λ−1
yt) (A14)
=∏
yt∈{1,..., J}N(zyt|yt; ¯zyt,1
NytΣϵ)N(¯zyt;µyt+1,Λ−1
yt) (A15)
=∏
yt∈{1,..., J}N(¯zyt;µt+1,Λ−1
yt+1), (A16)
where Nytis the number of points of class ytfrom the set of all classes C={1,. . .,J}.
The update equations for the mean and variance of the posterior are:
Λyt+1=Λyt+NytΣ−1
ϵ,∀yt∈Ct (A17)
Λyt+1µyt+1=NytΣ−1
ϵ¯zyt+Λytµyt,∀yt∈Ct. (A18)
Appendix F.3. ProtoCL Objective
The posterior predictive distribution we want to optimize is:
p(z,y) =Z
p(z,y|θ;η)p(θ;η)dθ, (A19)

--- PAGE 22 ---
Entropy 2023 ,1, 0 22 of 28
where p(θ)denotes the distributions over class probabilities {pj}J
j=1and mean embeddings
{¯zyj}J
j=1,
p(z,y) =ZNt
∏
i=1p(zit|yit; ¯zyt,Σϵ)p(yit|p1:J)p(p1:J;αt)p(¯zyt;µyt,t,Λ−1
yt,t)dp1:Jd¯zyt (A20)
=ZNt
∏
i=1p(zit|yit;zyt,Σϵ)p(¯zyt;µyt,t,Λ−1
yt,t)d¯zytZNt
∏
i=1p(yit|p1:J)p(p1:J;αt)dp1:J
| {z }
∏ip(yi)=p(y)(A21)
=p(y)Nt
∏
i=1Z−1
iZ
N(¯zyit;c,C)d¯zyt (A22)
=p(y)Nt
∏
i=1N(zit|yit;µyt,t,Σϵ+Λ−1
yt,t). (A23)
where in Equation (A22) we use §8.1.8 in [59]. The term p(y)is:
p(y) =Z
p(y|p1:J)p(p1:J;αt)dp1:J (A24)
=Z
pyΓ(∑J
j=1αj)
∏J
j=1Γ(αj)J
∏
j=1pαj−1
jdp1:J (A25)
=Γ(∑J
j=1αj)
∏J
j=1Γ(αj)Z J
∏
j=1pI(y=j)+αj−1
jdp1:J (A26)
=Γ(∑J
j=1αj)
∏J
j=1Γ(αj)∏J
j=1Γ(I(y=j) +αj)
Γ(1+∑J
j=1αj)(A27)
=Γ(∑J
j=1αj)
∏J
j=1Γ(αj)∏J
j=1Γ(I(y=j) +αj)
∑J
j=1αjΓ(∑J
j=1αj)(A28)
=∏J
j=1,j̸=yΓ(αj)
∏J
j=1Γ(αj)Γ(1+αy)
∑J
j=1αj(A29)
=∏J
j=1,j̸=yΓ(αj)
∏J
j=1Γ(αj)αyΓ(αy)
∑J
j=1αj(A30)
=αy
∑J
j=1αj, (A31)
where we use the identity Γ(n+1) =nΓ(n).

--- PAGE 23 ---
Entropy 2023 ,1, 0 23 of 28
0250 500 7501000 1500 2000
mem. size405060708090Accuracy
Figure A7. Split-MNIST average test accuracy over five tasks for different memory sizes. On the
x-axis, we show the size of the entire memory buffer shared by all five tasks. Accuracies are over a
mean and standard deviation over five different runs with different random seeds.
0.00.10.20.30.40.5Dirichlet pclass prob: 1 class prob: 2 class prob: 3 class prob: 4 class prob: 5
12345
Task0.00.10.20.30.40.5Dirichlet pclass prob: 6
12345
Taskclass prob: 7
12345
Taskclass prob: 8
12345
Taskclass prob: 9
12345
Taskclass prob: 10
Figure A8. The evolution of the Dirichlet parameters αtfor each class in Split-MNIST tasks for
ProtoCL. All αjare shown over 10seeds with ±1standard error. By the end of training, all classes are
roughly equally likely, as we have trained on equal amounts of all classes.
Appendix F.4. Predictions
To make a prediction for a test point x∗:
p(y∗=j|x∗,x1:t,y1:t) =p(y∗=j|z∗,θt) (A32)
=p(z∗|y∗=j,θt)p(y∗=j|θt)
∑ip(z∗|y∗=i,ηt)p(y∗=i|θt)(A33)
=p(y∗=j,z∗|θt)
∑ip(y=i,z∗|θt), (A34)
where θtare sufficient statistics for (x1:t,y1:t).
Preventing forgetting. As we wish to retain the task-specific prototypes, at the end of
learning a task Ttwe take a small subset of the data as a memory to ensure that posterior
parameters and prototypes do not drift, see Algorithm 1.

--- PAGE 24 ---
Entropy 2023 ,1, 0 24 of 28
Appendix F.5. Experimental Setup
The prototype variance, Σϵis set to a diagonal matrix with the variances of each
prototype set to 0.05. The prototype prior precisions, Λyt, are also diagonals and initialized
randomly and exponentiated to ensure a positive semi-definite covariance for the sequential
updates. The parameters αj∀jare set to 0.78, which was found by random search over
the validation set on MNIST. We also allow αjto be learned in the gradient update step in
addition to the sequential update step (lines 4 and 5 Algorithm 1), see Figure A8 to see the
evolution of the αjor all classes jover the course of learning Split-MNIST.
For the Split-MNIST and Split-FMNIST benchmarks, we use an NN with two layers of
size 200and trained for 50epochs with an Adam optimizer. We perform a grid-search over
learning rates, dropout rates, and weight decay coefficients. The embedding dimension
is set to 128. For the Split-CIFAR10 and Split-CIFAR100 benchmarks, we use the same
network as Pan et al. [22], which consists of four convolution layers and two linear layers.
We train the networks for 80epochs for each task with the Adam optimizer with a learning
rate of 1×10−3. The embedding dimension is set to 32. All experiments are run on a single
GPU NVIDIA RTX 3090.
Appendix G. Sequential Bayesian Estimation as Bayesian Neural Network Optimization
We shall consider inference in the graphical model depicted in Figure A9. The aim is
to infer the optimal BNN weights, θ∗
tat time tgiven observations and the previous BNN
weights. We assume a Gaussian posterior over weights with full covariance; hence, we
model interactions between all weights. We shall consider the online setting where we see
one data point (xt,yt)at a time and we will make no assumption as to whether the data
comes from the same task or different tasks over the course of learning.
θ∗
t−1θ∗
t−2 θ∗
tθ∗
t+1yt−1 yt−2 yt yt+1
xt−1 xt−2 xt xt+1. . . . . .
Figure A9. Graphical model of under which we perform inference in Section 5. Grey nodes are
observed and white are latent variables.
We set up the problem of sequential Bayesian inference as a filtering problem and we
leverage the work of Aitchison [30], which casts NN optimization as Bayesian sequential
inference. We make the reasonable assumption that the distribution over weights is a
Gaussian with full covariance. Since reasoning about the full covariance matrix of a BNN
is intractable, we instead consider the i-th parameter and reason about the dynamics
of the optimal estimates θ∗
itas a function of all the other parameters θ−it. Each weight
is functionally dependent on all others. If we had access to the full covariance of the
parameters, then we could reason about the unknown optimal weights θ∗
itgiven the values
of all the other weights θ−it. However, since we do not have access to the full covariance,
another approach is to reason about the dynamics of θ∗
itgiven the dynamics of θ−itand
assume that the values of the weights are close to those of the previous timestep [ 31] and
so we cast the problem as a dynamical system.
Consider a quadratic loss of the form:
L(xt,yt;θ) =Lt(θ) =−1
2θ⊤Hθ+z⊤
tθ, (A35)

--- PAGE 25 ---
Entropy 2023 ,1, 0 25 of 28
which we can arrive at by simple Taylor expansion, where His the Hessian which is
assumed to be constant across data points but not across the parameters θ. If the BNN
output takes the form f(xt;θ), then the derivative evaluated at θtiszt=∂L(xt,yt;θ)
∂θ|θ=θt.
To construct the dynamical equations for our weights, consider the gradient for a single
datapoint:
∂Lt(θ)
∂θ=−Hθ+zt. (A36)
If we consider the gradient for the i-th weight while all other parameters are set to their
current estimate:
∂L(θi,θ−i)
∂θi=−Hiiθit−H⊤
−iiθ−it+zti. (A37)
When the gradient is set to zero we recover the optimal value for θit, denoted as θ∗
it:
θ∗
it=−1
HiiH⊤
−iiθ−it. (A38)
since zti=0at the modes. The equation above shows us that the dynamics of the optimal
weight θ∗
itis dependent on all the other current values of the parameters θ−it. That is,
the dynamics of θ∗
itare governed by the dynamics of the weights θ−it. The dynamics of θ−it
are a complex stochastic process dependent on many different variables. Since reasoning
about the dynamics is intractable, we instead assume a discretized Ornstein–Uhlenbeck
process for the weights θ−itwith reversion speed ϑ∈R+and noise variance η2
−i:
p(θ−i,t+1|θ−i,t) =N((1−ϑ)θ−it,η2
−i), (A39)
this implies that the dynamics for the optimal weight are defined by
p(θ∗
i,t+1|θ∗
i,t) =N((1−ϑ)θ∗
it,η2), (A40)
where η2=η2
−iH⊤
−iiH−ii. This same assumption is made in Aitchison [30]. This assumes a
parsimonious model of the dynamics. Together with our likelihood:
p(yt|xt;θ∗
t) =N(yt;f(xt;θ∗
t),σ2) (A41)
where f(·,θ)is a neural network prediction with weights θ, we can now define a linear
dynamical system for the optimal weight θ∗
iby linearizing the Bayesian NN [ 31] and by
using the transition dynamics in Equation (A40). Thus, we are able to infer the posterior
distribution over the optimal weights using Kalman filter-like updates [ 60]. As the dynam-
ics and likelihood are Gaussian, then the prior and posterior are also Gaussian, for ease of
notation we drop the index isuch that θ∗
it=θ∗
t:
p(θ∗
t|(x,y)t−1, . . . ,(x,y)1) =N(µt,prior ,σ2
t,prior) (A42)
p(θ∗
t|(x,y)t, . . . ,(x,y)1) =N(µt,post,σ2
t,post) (A43)
By using the transition dynamics and the prior we can obtain closed-form updates:
p(θ∗
t|(x,y)t−1, . . . ,(x,y)1) =Z
p(θ∗
t|θ∗
t−1)p(θ∗
t−1|(x,y)t−1, . . . ,(x,y)1)dθ∗
t−1 (A44)
N(θ∗
t;µt,prior ,σ2
t,prior) =Z
N(θ∗
t;(1−ϑ)θ∗
t−1,η2)N(θ∗
t−1;µt−1,post ,σ2
t−1,post)dθ∗
t−1. (A45)

--- PAGE 26 ---
Entropy 2023 ,1, 0 26 of 28
Integrating out θ∗
t−1we can obtain updates for the prior for the next timestep as follows:
µt,prior= (1−ϑ)µt−1,post (A46)
σ2
t,prior=η2+ (1−ϑ)−2σ2
t−1,post . (A47)
The updates for obtaining our posterior parameters: µt,post andσ2
t,post, comes from applying
Bayes’ theorem:
logN(θ∗
t;µt,post,σ2
t,post)∝logN(yt;f(xt;θ∗
t),σ2) +logN(θ∗
t;µt,prior ,σ2
t,prior), (A48)
by linearizing our Bayesian NN such that f(xt,θ0)≈f(xt,θ0) +∂f(xt;θ∗
t)
∂θ∗
t(θ∗
t−θ0)and by
substituting into Equation (A48) we obtain our update equation for the posterior of the
mean of our BNN parameters:
−1
2σ2
t,post(θ∗
t−µt,post)2=−1
2σ2(y−g(xt)θ∗
t)2−1
2σ2
t,prior(θ∗
t−µt,prior)2(A49)
µt,post=σ2
t,post 
µt,prior
σ2
t,prior+y
σ2g(xt)!
, (A50)
where g(xt) =∂f(xt;θ∗
t)
∂θ∗
t, and the update equation for the variance of the Gaussian posterior
is:
1
σ2
t,post=g(xt)2
σ2+1
σ2
t,prior. (A51)
From our updated equations, Equation (A50) and Equation (A51), we can notice that
the posterior mean depends linearly on the prior and an additional data dependent term.
These equations are similar to the filtering example in Section 4. Therefore, under certain
assumptions above, a BNN should behave similarly. If there exists a task data imbalance,
then the data term will dominate the prior term in Equation (A50) and could lead to
forgetting of previous tasks.
1. McCloskey, M.; Cohen, N.J. Catastrophic interference in connectionist networks: The sequential
learning problem. In Psychology of learning and motivation ; Elsevier, 1989; Vol. 24, pp. 109–165.
2. French, R.M. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 1999 ,
3, 128–135.
3. Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Desjardins, G.; Rusu, A.A.; Milan,
K.; Quan, J.; Ramalho, T.; Grabska-Barwinska, A.; et al. Overcoming catastrophic forget-
ting in neural networks. Proceedings of the National Academy of Sciences 2017 ,114, 3521–3526,
[https://www.pnas.org/content/114/13/3521.full.pdf]. https://doi.org/10.1073/pnas.1611
835114.
4. MacKay, D.J. A practical Bayesian framework for backpropagation networks. Neural computation
1992 ,4, 448–472.
5. Graves, A. Practical variational inference for neural networks. Advances in neural information
processing systems 2011 ,24.
6. Blundell, C.; Cornebise, J.; Kavukcuoglu, K.; Wierstra, D. Weight uncertainty in neural network.
In Proceedings of the International Conference on Machine Learning. PMLR, 2015, pp. 1613–
1622.
7. Schwarz, J.; Czarnecki, W.; Luketina, J.; Grabska-Barwinska, A.; Teh, Y.W.; Pascanu, R.; Hadsell,
R. Progress & compress: A scalable framework for continual learning. In Proceedings of the
International Conference on Machine Learning. PMLR, 2018, pp. 4528–4537.
8. Ritter, H.; Botev, A.; Barber, D. Online structured laplace approximations for overcoming
catastrophic forgetting. Advances in Neural Information Processing Systems 2018 ,31.

--- PAGE 27 ---
Entropy 2023 ,1, 0 27 of 28
9. Nguyen, C.V .; Li, Y.; Bui, T.D.; Turner, R.E. Variational Continual Learning. In Proceedings of
the International Conference on Learning Representations, 2018.
10. Ebrahimi, S.; Elhoseiny, M.; Darrell, T.; Rohrbach, M. Uncertainty-Guided Continual Learning
in Bayesian Neural Networks. In Proceedings of the Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, 2019, pp. 75–78.
11. Kessler, S.; Nguyen, V .; Zohren, S.; Roberts, S.J. Hierarchical indian buffet neural networks for
bayesian continual learning. In Proceedings of the Uncertainty in Artificial Intelligence. PMLR,
2021, pp. 749–759.
12. Loo, N.; Swaroop, S.; Turner, R.E. Generalized Variational Continual Learning. In Proceedings
of the International Conference on Learning Representations, 2020.
13. Neal, R.M.; et al. MCMC using Hamiltonian dynamics. Handbook of Markov chain Monte Carlo
2011 ,2, 2.
14. LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P . Gradient-based learning applied to document
recognition. Proceedings of the IEEE 1998 ,86, 2278–2324.
15. Zenke, F.; Poole, B.; Ganguli, S. Continual learning through synaptic intelligence. In Proceedings
of the International Conference on Machine Learning. PMLR, 2017, pp. 3987–3995.
16. Hsu, Y.C.; Liu, Y.C.; Ramasamy, A.; Kira, Z. Re-evaluating continual learning scenarios: A
categorization and case for strong baselines. arXiv preprint arXiv:1810.12488 2018 .
17. Van de Ven, G.M.; Tolias, A.S. Three scenarios for continual learning. arXiv preprint
arXiv:1904.07734 2019 .
18. van de Ven, G.M.; Tuytelaars, T.; Tolias, A.S. Three types of incremental learning. Nature
Machine Intelligence 2022 , pp. 1–13.
19. Chopin, N.; Papaspiliopoulos, O.; et al. An introduction to sequential Monte Carlo ; Vol. 4, Springer,
2020.
20. Cobb, A.D.; Jalaian, B. Scaling Hamiltonian Monte Carlo Inference for Bayesian Neural Net-
works with Symmetric Splitting. Uncertainty in Artificial Intelligence 2021 .
21. Izmailov, P .; Vikram, S.; Hoffman, M.D.; Wilson, A.G.G. What are Bayesian neural network
posteriors really like? In Proceedings of the International conference on machine learning.
PMLR, 2021, pp. 4629–4640.
22. Pan, P .; Swaroop, S.; Immer, A.; Eschenhagen, R.; Turner, R.; Khan, M.E.E. Continual deep learn-
ing by functional regularisation of memorable past. Advances in Neural Information Processing
Systems 2020 ,33, 4453–4464.
23. Dinh, L.; Sohl-Dickstein, J.; Bengio, S. Density estimation using real NVP. arXiv preprint
arXiv:1605.08803 2016 .
24. Bishop, C.M. Pattern recognition and machine learning ; springer, 2006.
25. Aljundi, R.; Lin, M.; Goujaud, B.; Bengio, Y. Gradient based sample selection for online continual
learning. Advances in neural information processing systems 2019 ,32.
26. Aljundi, R.; Kelchtermans, K.; Tuytelaars, T. Task-free continual learning. In Proceedings of the
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019,
pp. 11254–11263.
27. De Lange, M.; Aljundi, R.; Masana, M.; Parisot, S.; Jia, X.; Leonardis, A.; Slabaugh, G.; Tuytelaars,
T. A continual learning survey: Defying forgetting in classification tasks. arXiv preprint
arXiv:1909.08383 2019 .
28. Wilson, A.G.; Izmailov, P . Bayesian deep learning and a probabilistic perspective of generaliza-
tion. Advances in neural information processing systems 2020 ,33, 4697–4708.
29. Ciftcioglu, Ö.; Türkcan, E. Adaptive training of feedforward neural networks by Kalman
filtering 1995 .
30. Aitchison, L. Bayesian filtering unifies adaptive and non-adaptive neural network optimization
methods. Advances in Neural Information Processing Systems 2020 ,33, 18173–18182.
31. Jacot, A.; Gabriel, F.; Hongler, C. Neural tangent kernel: Convergence and generalization in
neural networks. Advances in neural information processing systems 2018 ,31.
32. Thrun, S.; Mitchell, T.M. Lifelong robot learning. Robotics and autonomous systems 1995 ,15, 25–46.
33. Zeno, C.; Golan, I.; Hoffer, E.; Soudry, D. Task agnostic continual learning using online
variational bayes. arXiv preprint arXiv:1803.10123 2018 .
34. Ahn, H.; Cha, S.; Lee, D.; Moon, T. Uncertainty-based continual learning with adaptive
regularization. Advances in neural information processing systems 2019 ,32.
35. Farquhar, S.; Osborne, M.A.; Gal, Y. Radial bayesian neural networks: Beyond discrete support
in large-scale bayesian deep learning. In Proceedings of the International Conference on
Artificial Intelligence and Statistics. PMLR, 2020, pp. 1352–1362.

--- PAGE 28 ---
Entropy 2023 ,1, 0 28 of 28
36. Mehta, N.; Liang, K.; Verma, V .K.; Carin, L. Continual learning using a Bayesian nonparametric
dictionary of weight factors. In Proceedings of the International Conference on Artificial
Intelligence and Statistics. PMLR, 2021, pp. 100–108.
37. Kumar, A.; Chatterjee, S.; Rai, P . Bayesian structural adaptation for continual learning. In
Proceedings of the International Conference on Machine Learning. PMLR, 2021, pp. 5850–5860.
38. Adel, T.; Zhao, H.; Turner, R.E. Continual learning with adaptive weights (claw). arXiv preprint
arXiv:1911.09514 2019 .
39. Titsias, M.K.; Schwarz, J.; Matthews, A.G.d.G.; Pascanu, R.; Teh, Y.W. Functional Regularisation
for Continual Learning with Gaussian Processes. In Proceedings of the ICLR, 2020.
40. Kapoor, S.; Karaletsos, T.; Bui, T.D. Variational auto-regressive Gaussian processes for continual
learning. In Proceedings of the International Conference on Machine Learning. PMLR, 2021, pp.
5290–5300.
41. Buzzega, P .; Boschini, M.; Porrello, A.; Abati, D.; Calderara, S. Dark experience for general
continual learning: a strong, simple baseline. Advances in neural information processing systems
2020 ,33, 15920–15930.
42. Benjamin, A.; Rolnick, D.; Kording, K. Measuring and regularizing networks in function space.
In Proceedings of the International Conference on Learning Representations, 2018.
43. Henning, C.; Cervera, M.; D’Angelo, F.; Von Oswald, J.; Traber, R.; Ehret, B.; Kobayashi, S.;
Grewe, B.F.; Sacramento, J. Posterior meta-replay for continual learning. Advances in Neural
Information Processing Systems 2021 ,34, 14135–14149.
44. Swaroop, S.; Nguyen, C.V .; Bui, T.D.; Turner, R.E. Improving and understanding variational
continual learning. arXiv preprint arXiv:1905.02099 2019 .
45. Rudner, T.G.J.; Chen, Z.; Teh, Y.W.; Gal, Y. Tractabe Function-Space Variational Inference in
Bayesian Neural Networks. 2022.
46. Rudner, T.G.J.; Smith, F.B.; Feng, Q.; Teh, Y.W.; Gal, Y. Continual Learning via Sequential
Function-Space Variational Inference. In Proceedings of the Proceedings of the 38th International
Conference on Machine Learning. PMLR, 2022, Proceedings of Machine Learning Research.
47. Lavda, F.; Ramapuram, J.; Gregorova, M.; Kalousis, A. Continual classification learning using
generative models. arXiv preprint arXiv:1810.10612 2018 .
48. van de Ven, G.M.; Li, Z.; Tolias, A.S. Class-incremental learning with generative classifiers. In
Proceedings of the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2021, pp. 3611–3620.
49. Snell, J.; Swersky, K.; Zemel, R. Prototypical networks for few-shot learning. Advances in neural
information processing systems 2017 ,30.
50. Rebuffi, S.A.; Kolesnikov, A.; Sperl, G.; Lampert, C.H. ICARL: Incremental classifier and
representation learning. In Proceedings of the Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, 2017, pp. 2001–2010.
51. Harrison, J.; Sharma, A.; Finn, C.; Pavone, M. Continuous meta-learning without tasks. Advances
in neural information processing systems 2020 ,33, 17571–17581.
52. Zhang, X.; Meng, D.; Gouk, H.; Hospedales, T.M. Shallow bayesian meta learning for real-
world few-shot recognition. In Proceedings of the Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021, pp. 651–660.
53. Titsias, M.K.; Schwarz, J.; Matthews, A.G.d.G.; Pascanu, R.; Teh, Y.W. Functional Regularisation
for Continual Learning. International Conference on Learning Representations 2020 .
54. Chrysakis, A.; Moens, M.F. Online continual learning from imbalanced data. In Proceedings of
the International Conference on Machine Learning. PMLR, 2020, pp. 1952–1961.
55. Knoblauch, J.; Husain, H.; Diethe, T. Optimal continual learning has perfect memory and is
NP-hard. In Proceedings of the International Conference on Machine Learning. PMLR, 2020,
pp. 5327–5337.
56. Titsias, M. Variational learning of inducing variables in sparse Gaussian processes. In Proceed-
ings of the Artificial intelligence and statistics. PMLR, 2009, pp. 567–574.
57. Hensman, J.; Fusi, N.; Lawrence, N.D. Gaussian processes for big data. arXiv preprint
arXiv:1309.6835 2013 .
58. Farquhar, S.; Gal, Y. Towards robust evaluations of continual learning. arXiv preprint
arXiv:1805.09733 2018 .
59. Petersen, K.B.; Pedersen, M.S.; et al. The matrix cookbook. Technical University of Denmark 2008 ,
7, 510.
60. Kalman, R.E. A new approach to linear filtering and prediction problems 1960 .
