# 2110.08534.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2110.08534.pdf
# File size: 935885 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Lifelong Pretraining : Continually Adapting Language Models
to Emerging Corpora
Xisen Jiny1Dejiao Zhang2Henghui Zhu2Wei Xiao2
Shang-Wen Liz2Xiaokai Wei2Andrew Arnold2Xiang Ren1
1University of Southern California2AWS AI Labs
{xisenjin,xiangren}@usc.edu
{dejiaoz, henghui, weixiaow, shangwenl, xiaokaiw, anarnld}
@amazon.com
Abstract
Pretrained language models (PTLMs) are typi-
cally learned over a large, static corpus and fur-
ther Ô¨Åne-tuned for various downstream tasks.
However, when deployed in the real world, a
PTLM-based model must deal with data dis-
tributions that deviate from what the PTLM
was initially trained on. In this paper, we
study a lifelong language model pretraining
challenge where a PTLM is continually up-
dated so as to adapt to emerging data. Over
a domain-incremental research paper stream
and a chronologically-ordered tweet stream,
we incrementally pretrain a PTLM with dif-
ferent continual learning algorithms, and keep
track of the downstream task performance (af-
ter Ô¨Åne-tuning). We evaluate PTLM‚Äôs ability
to adapt to new corpora while retaining learned
knowledge in earlier corpora. Our experiments
show distillation-based approaches to be most
effective in retaining downstream performance
in earlier domains. The algorithms also im-
prove knowledge transfer, allowing models to
achieve better downstream performance over
the latest data, and improve temporal gen-
eralization when distribution gaps exist be-
tween training and evaluation because of time.
We believe our problem formulation, methods,
and analysis will inspire future studies towards
continual pretraining of language models.
1 Introduction
Pretrained language models (PTLMs) have
achieved remarkable performance on benchmark
datasets for a range of NLP tasks (Liu et al., 2019b;
Brown et al., 2020). However, when deployed in
the wild, NLP systems must deal with emerging
data that have constantly shifting data distribution,
different from the text corpora they were initially
pretrained on ‚Äî for example, when new data do-
mains are introduced (upper part of Fig. 1) (Gu-
rurangan et al., 2020), or when the language uses
yWork done during an internship at AWS AI Labs.
zWork done while at Amazon.
Domain-Incremental Research Paper StreamKnowledgeretention
All domainsChronologically-Ordered Tweet Stream
2014201620182020Adaptation & Temporal generalization
Latest data
Bio-MedicalComputer ScienceMaterials SciencePhysicsFigure 1: Two data streams created for studying life-
long language model pre-training. We focus on evalu-
ating knowledge retention on the domain-incremental
research papers stream; we focus on adaptation to the
latest data and temporal generalization on the chrono-
logically ordered tweet stream.
and vocabulary change over time (lower part of
Fig. 1) (Lazaridou et al., 2021). Fine-tuning from a
static and possibly ‚Äúoutdated" PTLM may limit the
model performance on downstream tasks, as the
PTLM may no longer provide an effective model
initialization (Beltagy et al., 2019; M√ºller et al.,
2020). Here we look to understand whether con-
tinuously adapting a PTLM to emerging data can
yield gains on various downstream tasks, and how
to achieve better downstream performance for such
lifelong PTLM adaptation.
A number of recent works make attempts on
adapting PTLMs to a new data domain. Gururan-
gan et al. (2020); Yao et al. (2021) adapt language
models to corpora of different genres and topics
and observe performance improvement in domain-
speciÔ¨Åc downstream tasks. Arumae et al. (2020)
further show that by regularizing the parameters
of PTLMs, the downstream tasks performance on
the general domain can be preserved. Another line
of works focuses on temporal domain shift (Hom-
baiah et al., 2021), which analyzes the effect of
pretraining over up-to-date data to the downstreamarXiv:2110.08534v3  [cs.CL]  19 Jul 2022

--- PAGE 2 ---
tasks. R√∂ttger and Pierrehumbert (2021) further
study vocabulary composition approaches for im-
proving adaptation to up-to-date corpora. However,
these work focus their study on adapting PTLM
to a single new domain; while in practice, cor-
pora from distinct domains and time stamps may
emerge sequentially. Whether one can maintain
a single, up-to-date PTLM remains an open prob-
lem. Related to this, Lazaridou et al. (2021) study
adaptation of PTLMs over temporal data streams,
but solely focus on language modeling instead of
Ô¨Åne-tuning performance. It is also important to un-
derstand multiple aspects of the utility of lifelong
PTLM pretraining, such as knowledge retention
over all the seen data, and study what methods can
improve the utility of PTLMs in such a continual
pretraining process.
In this paper, we formulate a Lifelong Language
Model Pretraining task to simulate practical sce-
narios of maintaining and adapting a PTLM over
emerging corpora, create a testbed (along with
pretraining data streams and downstream tasks)
for studying continual pretraining algorithms, and
present a systematic evaluation protocol for measur-
ing the progress made on this challenging problem
(see Figure 2 for an illustration). We consider two
types of text corpus sequences when constructing
pretraining data streams, each of which simulates a
representative use case and that has slightly differ-
ent focuses on the evaluation: continuously learn-
ing a single model that is applicable to both old and
new domains; and improving the model‚Äôs ability to
handle latest data. SpeciÔ¨Åcally, we construct 1) a
domain-incremental text stream that consists of aca-
demic papers published in four research Ô¨Åelds, and
2) a temporal tweet stream that consists of tweets
collected from four different years. By conducting
systematic experiments on these two data streams,
we look to answer a series of analysis questions:
1) whether continual pretraining retains Ô¨Åne-tuning
performance over earlier corpora compared to tra-
ditional ofÔ¨Çine pretraining, 2) whether pretraining
improves downstream performance on the latest
data, and 3) whether pretraining improves temporal
generalization where training and evaluation have
distribution gaps because of time.
To address the research questions above, we con-
duct a systematic evaluation of existing continual
learning (CL) algorithms, spanning over model-
expansion based, memory-based, and distillation-
based approaches. Our results show distillation-based approaches are most effective in knowledge
retention in the research paper stream, while si-
multaneously improve adaptation to latest data and
temporal generalization in the tweet stream. We
believe our problem formulation, evaluation setup,
methods and analysis can inspire more future work
on continual pretraining of language models.
2 Problem Formulation
Here we present the problem formulation for life-
long pretraining of PTLM, provide details about the
data stream construction process and downstream
tasks, and introduce the evaluation protocol.
2.1 Lifelong Pretraining of PTLMs
We consider the scenario where one needs to de-
ploy and/or maintain NLP models over a sequence
ofTdata domains. At each time step tthe model
visits an unlabeled text corpus Dtfrom a domain
with a data distribution P(Dt). The data distribu-
tionP(Dt)evolves as the time step t, forming a
data stream D1::T=fD1;D2;:::D Tg. In practice,
the data domain shift can refer to the topic change
of the text content (from computer science research
papers to biomedical papers), or temporal evolution
of the text (from past to recent tweets). The task of
lifelong pretraining of PTLM looks to continuously
adapt a language model fas the model visits (unla-
beled) text corpus Dtfrom the data stream D1::T,
in order to provide a good model initialization for
Ô¨Åne-tuning on downstream tasks from the same do-
main. With slight abuse in notations, we also use
Dtto directly refer to a data domain.
Here, we assume a language model fis updated
sequentially over each pretraining corpora Dt, with-
out accessing the full earlier corpora fDigi<tin the
data stream D1::T. This aims to capture practical
constraints such as privacy restriction for storing
earlier data, or computation budget for training
over all the text corpora in D1::T. We useftto
denote the language model right after updating on
the domainDt. In our study, fis a RoBERTa-base
transformer (Liu et al., 2019b) and the model ( f0)
is initialized with pretrained RoBERTa weights.
The utility of the PTLMs fftgis evaluated based
on their Ô¨Åne-tuned model performance on various
downstream tasks. After updating on a domain
Di, the model fican be Ô¨Åne-tuned over down-
stream tasks from visited domains Dtwhereti.
We note the set of downstream tasks related to do-
mainDtasSt=fSj
tgNt
j=1, assuming the number

--- PAGE 3 ---
Continual Pretraining‚Ä¶ùê∑!ùê∑"ùê∑#ùëì#
ùëì!Knowledge RetentionTrain
ùëÜ"
OldOldTest
‚Ä¶ùê∑$Fine-Tuning & EvaluationTrain
LatestLatestTestAdaptation toLatestDataTemporal GeneralizationùëÜ"Train
ùëÜ"
ùëÜ!OldLatestTestùëÜ!ùëÜ!Figure 2: Training, evaluation setups, and metrics of life-
long language model pretraining. The model sequentially
visits each corpus, and is Ô¨Åne-tuned on downstream datasets
related to the domains of pretraining. We evaluate knowl-
edge retention and adaptation to new data with downstream
Ô¨Åne-tuning performance on old and latest domains respec-
tively. Besides, we evaluate temporal generalization where
training/test examples are drawn from different time steps.
of downstream tasks is Nt. Note that in the Ô¨Åne-
tuning stage, model fthas no access to any of the
pretraining corpus D1::T.
2.2 Data Streams & Downstream Datasets
We construct data streams to simulate two repre-
sentative scenarios of data domain shifts in practice
(also see Fig. 1): one domain-incremental stream to
simulate the sequential changes of research paper
areas; and one chronologically-ordered stream to
simulate tweets emerging over time.
Domain-incremental Paper Stream. This pa-
per stream consists of the full text of research pa-
pers published in four research areas: biomedical,
computer science, material science, and physics,
Ô¨Åltered from the S2ORC dataset1, which are pre-
sented sequentially to the model. For each domain,
we evaluate downstream performance over two
datasets. The downstream tasks span over vari-
ous tasks such as relation extraction and named
entity recognition, and are summarized in Table 1.
We detail these datasets in Appendix D.
Chronologically-ordered Tweet Stream. This
tweet data stream consists of tweets from the
year 2014, 2016, 2018 and 2020, collected
by the Archive Team2and preprocessed follow-
ing Nguyen et al. (2020). These four tweet corpora
are presented sequentially to the language model
following the chronological order of the tweet year.
1We use the 20200705v1 version of the S2ORC dataset at https://
github.com/allenai/s2orc
2https://archive.org/details/twitterstreamDomains Downstream Datasets Metrics
Bio-Medicine Chemprot (Vindahl, 2016) Micro-F1
RCT-Sample (Dernoncourt and Lee, 2017) Micro-F1
Comp. Science ACL-ARC (Jurgens et al., 2018) Macro-F1
SciERC (Luan et al., 2018) Macro-F1
Mat. Science Synthesis (Mysore et al., 2019) Macro-F1
MNER (Olivetti et al., 2020) Micro-F1
Physics Keyphrase (Augenstein et al., 2017) Macro-F1
Hyponym (Augenstein et al., 2017) Macro-F1
Table 1: Summary of downstream datasets relevant to
each domain in the research paper stream.
For downstream tasks, we hold out 1M tweets from
each year‚Äôs corpus to construct multi-label hash-
tag prediction datasets (Gong and Zhang, 2016)
and single-label emoji prediction datasets (Barbieri
et al., 2018). On two datasets, we report label rank-
ing average precision scores (a multi-label version
of MRR) of models (Azeemi and Waheed, 2021)
and Macro-F1 respectively. The detailed dataset
construction process is included in Appendix D.
2.3 Evaluation Protocol
We consider three key aspects for evaluating the
utility of the language models fftgthat are con-
tinuously updated over the data stream D1::T, also
illustrated in Figure 2: 1) knowledge retention and
transfer over the pretraining corpora seen earlier;
2) adaptation to the latest data domain, and 3) tem-
poral generalization when training and evaluation
data are from different time steps.
Knowledge Retention. A key utility of contin-
ual language model pretraining is to obtain a sin-
gle model applicable to all domains. We focus
on the evaluation of the ability with the domain-
incremental paper stream, because for the tweet
stream, the practical need of performance over out-
dated data is limited. Knowledge retention is mea-
sured with the downstream task performance from
earlier or the current domains that the pretrained
model has visited. More formally, for each pre-
trained model checkpoint in ffig, we Ô¨Åne-tune fi
over downstream tasks fStgwheretiand eval-
uate the corresponding test set performance. It is
important that the models do not suffer from catas-
trophic forgetting (Robins, 1995), i.e.,signiÔ¨Åcantly
reduced helpfulness when fiis Ô¨Åne-tuned for down-
stream tasks Stfrom earlier domains with t<i .
Adaption to Latest Data Domain. In certain
scenarios, performance of downstream models over
the latest data domain should be emphasized. For
example, classiÔ¨Åers in the tweet domain are usually

--- PAGE 4 ---
trained and evaluated with up-to-date data for prac-
tical deployment. Formally, we focus on the down-
stream task performance of models Ô¨Åne-tuned from
the Ô¨Ånal pretrained model checkpoint fT, where
the downstream tasks STare also from the latest
domain. To succeed in these metrics, it is crucial
for the model to transfer knowledge from earlier
domains to the latest domain.
Temporal Generalization Ability. We consider
another practical Ô¨Åne-tuning scenario in the tweet
stream where the model is trained on outdated
data and evaluated on the latest data (Rijhwani
and Preotiuc-Pietro, 2020; Huang and Paul, 2018),
referred to as the temporal generalization ability.
Formally, we Ô¨Åne-tune the Ô¨Ånal pretrained model
checkpointfTover the training set of downstream
tasksStfrom an earlier time step ( t < T ), and
evaluate on the test set of the downstream tasks ST
from the latest time step T.
3 Methods
Lifelong language model pretraining introduces
novel challenges because of the large training sets
and more comprehensive evaluation protocols com-
pared to classiÔ¨Åcation tasks. We establish several
strong baselines, and evaluate the performance of
continual learning algorithms from different cate-
gories spanning over model-expansion, memory-
based, and distillation-based approaches, We illus-
trate the approaches in Figure 3.
3.1 Simple Baselines
We consider several simple baselines which contin-
ual learning algorithms will be compared against.
RoBERTa-base (f0) corresponds to not pre-
training on any of the domain-speciÔ¨Åc corpora.
By separately pretraining f0on each corpus
D1;D2;:::D T, we obtain TTask-Specific
pretrained models. We also pretrain f0sequentially
overD1::T, which we refer to as sequential
pretraining . While it allows knowledge trans-
fer between domains compared to domain-speciÔ¨Åc
models, without any continual learning algorithms,
sequential pretraining is prone to catastrophic for-
getting (Robins, 1995). Finally, we randomly
shufÔ¨Çe corpora from all domains D1::Tbefore
pretraining, noted as Multi-Task Learning
(MTL) . MTL corresponds to an ofÔ¨Çine training
paradigm that models new corpora by re-training
over all corpora seen before. The drawback is that
it requires storing full data from earlier domains,
Memory ReplayDistillation + CLAdapters ùëì!ùëî"#$!ùëî"#%!ùëî"!ùëì!&%Layer ùëòLayer ùëò+1Replay memoryùë•'ùë•(ùëì"MemoryStreamReplay memoryùëì"#%{ùë•(,ùë•'}ùëì"ùë¶"#%ùë¶"‚Ñé"#%‚Ñé"Logit DistillationRep. Distillationùêµ"#%ùêµ"Contrast&SEEDDistillation~~~Similarity matrixFigure 3: Comparison of adapter, memory re-
play, and distillation-based continual learning algo-
rithms. Details of the methods are introduced in Sec. 3.
and that it can be extremely costly to repetitively
retrain over earlier data if new data keeps emerging.
3.2 Model-expansion and
Regularization-based Methods
We Ô¨Årst introduce model-expansion based ap-
proaches, which add small trainable modules ( e.g.,
multi-layer perceptron) to the model per new do-
main while keeping other parts of the model frozen.
TheAdapter approach is a representative ap-
proach that learns a set of ‚Äúadapter‚Äù layers gt=
fgk
tgK
k=1for each domain Dtand each of the K
transformer layers (Houlsby et al., 2019). We also
experiment with a simple Layer Expansion
approach, which learns separate top two layers of
the transformer and the prediction head for each
domain. We also involve a regularization-based
continual learning baseline, online EWC (Schwarz
et al., 2018), which directly penalize change of
model parameters.
3.3 Memory Replay Methods
We also experiment with Experience Replay
(ER) (Chaudhry et al., 2019), which alleviates for-
getting by storing a subset of earlier examples and
periodically re-training (replaying) over them. We
maintain a Ô¨Åxed-size memory M(100kexamples
by default) and populate the memory Meach time
pretraining on a domain DtÔ¨Ånishes with examples
in the current domain. We ensure Malways con-
tains a balanced sample of examples from all seen
domainsD1::t. We replay a mini-batch of examples
from the memory every 10 training steps.
3.4 Distillation-based CL Methods
While knowledge distillation (KD) (Hinton et al.,
2015) techniques have been studied intensively for
pretrained language models (Sun et al., 2019), ap-
plying them to continual learning has been under-

--- PAGE 5 ---
explored outside image classiÔ¨Åcation tasks (Li and
Hoiem, 2018; RebufÔ¨Å et al., 2017; Hou et al., 2018).
Distillation based CL approaches store one previ-
ous model checkpoint of the model (noted as ft 1)
and regularize the differences between ft 1and
the current model ft. We adapt several existing
knowledge distillation techniques to PTLMs and
utilize them for continual learning. We note, while
individual distillation techniques are not original,
their adaptation to CL algorithms can be novel.
We perform distillation with examples from the
current domain Dtand a replay memory M(sim-
ilar to ER). Despite the potential gap between Dt
and the training data of ft 1, the approach allows
utilizing more data for distillation. Formally, each
time the model receives a mini-batch of stream
examples xsor a draws mini-batch of memory ex-
amples xmfromM(both noted as x), we collect
certain outputs of the model ( e.g., output logits or
intermediate representations) with ft 1andft. We
compute a distillation loss `KD(x;ft 1;ft)that pe-
nalizes the differences between the model outputs,
and jointly optimize it with the masked language
modeling loss `MLM. The Ô¨Ånal objective is written
as`=`MLM+`KD, whereis a hyperparameter
to weight the distillation loss.
Logit Distillation. In logit distillation (Hinton
et al., 2015), we collect the output logits of ftand
ft 1, noted as ytandyt 1respectively. The dis-
tillation loss is computed as DKL(yt;yt 1), where
DKLis the Kullback‚ÄìLeibler divergence function.
Representation Distillation. We also consider
minimizing the representational deviation of sen-
tences between previous and current models (Sun
et al., 2019; Jiao et al., 2020). We extract the rep-
resentation of each word of two models, noted
ash1:N
t 1andh1:N
t, before the masked language
modeling prediction head, where Nis the length
of the sentence. Then, we compute MSE loss
jjh1:N
t 1 h1:N
tjj2
2as the distillation loss.
Contrastive Distillation. In addition to output
logits and hidden representations, we further look
intorepresentational similarity within a batch of
examples as additional knowledge to distill. The
approach is adapted from (Cha et al., 2021), which
is originally studied for supervised image classiÔ¨Å-
cation tasks. We brieÔ¨Çy introduce the adapted algo-
rithm and leave the details in Appendix E. During
continual pretraining, in addition to the language
model pretraining objective, we add an unsuper-vised contrastive learning objective, namely the
SimCSE (Gao et al., 2021) objective to encourage
sentence representations to reÔ¨Çect semantic simi-
larities between sentences. Then, we compute the
intra-batch representational similarity matrices of
sentence representations ( i.e.between each pair of
examples in the mini-batch) with ft 1andft, noted
asBt 1andBt, and minimize the cross entropy
loss`distill= 1
NPN
i=1PN
j=1Bt 1
ijlogBt
ij
Self-Supervised Distillation (SEED). SEED
distillation proposed by (Fang et al., 2021) has a
similar spirit as the contrastive distillation. The
only difference is that it distills representational
similarity between the batch and a large set of
other examples . We leave the details of the algo-
rithm in Appendix E. We further combine SEED
Distillationwith logit distillation and refer to the
approach as SEED-Logit Distillation.
4 Results
We summarize our Ô¨Åndings over the created data
streams. We ask whether lifelong pretraining and
continual learning algorthms are effective base on
our evaluation protocol proposed in Sec. 2.3.
4.1 Experiment Settings
We use the RoBERTa-base model (Liu et al.,
2019b), initialized with RoBERTa-base weights
throughout the experiments. We set the maximal
sequence length to 128 and an effective training
batch size of 2,048. On the research paper stream,
models are trained for 8 ksteps in the Ô¨Årst domain
and 4ksteps in the subsequent domains. On the
Tweet stream, we train the models for 4 ksteps in
each domain. These correspond to less than a single
pass of data in each domain. See Appendix A for
detailed setups.
4.2 Domain Incremental Data Stream
As we introduced in Sec. 2.2, in the domain incre-
mental research paper stream, we expect a model
ftto perform well on all downstream tasks S1::t
from domains D1::t. In Table 2, we report the per-
formance of models on all downstream tasks S1::T
Ô¨Åne-tuned from the Ô¨Ånal pretraining checkpoint,
fT. We visualize more complete change of down-
stream task performance over different time steps
of pretraining ( i.e.,,f1;f2;f3;f4) in Fig. 4. We
also report the log perplexity of masked language
modeling (MLM) in Table 2 as additional informa-

--- PAGE 6 ---
Task D1- Biomedical D2- Computer Science D3- Materials Science D4- Physics
Dataset Chemprot RCT-Sample MLM ACL-ARC SciERC MLM MNER Synthesis MLM Keyphrase Hyponym MLM
Roberta-base 82.03 0:7 78.070:7 1.993 64.32 2:879.071:62.153 83.150:391.250:62.117 66.21 1:067.594:52.278
Sequential Pretraining 82.09 0:5 79.600:5 1.654 72.73 2:981.430:81.807 83.990:392.101:01.590 67.571:074.684:41.381
ER 82.73 0:3 79.980:3 1.737 72.50 1:081.641:11.857 83.990:492.650:41.621 66.11 1:172.824:31.391
Online EWC 81.83 0:2 78.840:5 1.655 71.81 2:680.790:51.803 83.430:491.890:51.571 66.70 0:672.986:01.388
Adapter 83.30 0:4 80.410:4 1.417 69.32 3:580.221:51.633 83.910:391.690:61.522 66.23 1:469.654:51.554
Layer Expansion 83.74 0:3 81.100:5 1.210 65.172:979.350:81.756 82.480:492.331:01.389 65.70 1:173.343:71.534
Logit-KD 83.39 0:4 81.210:1 1.392 73.703:481.920:81.699 83.960:392.201:01.425 64.751:171.293:61.460
Rep-KD 82.34 0:3 79.590:5 1.684 71.17 2:578.781:11.810 84.130:392.020:81.585 65.96 1:673.935:51.389
Contrast-KD 82.29 0:5 79.920:4 1.722 71.15 1:180.491:61.856 83.260:492.620:71.612 65.95 1:772.263:11.428
SEED-KD 82.78 0:3 80.380:4 1.720 69.98 2:481.610:71.829 82.990:492.350:71.609 65.35 1:074.794:11.401
SEED-Logit-KD 83.720:4 81.050:2 1.391 69.90 4:583.030:61.703 83.280:592.871:01.428 65.96 1:571.925:51.460
Task-SpeciÔ¨Åc LM 83.74 0:3 81.100:5 1.210 72.20 2:681.241:71.629 84.020:291.560:41.418 65.95 1:169.434:51.426
MTL 82.91 1:6 80.670:4 1.289 69.46 1:881.120:81.616 83.920:392.660:61.355 65.37 1:673.315:21.418
Table 2: Results on the Research Paper stream. We report log perplexity of MLM and the performance of downstream
models Ô¨Åne-tuned from the Ô¨Ånal checkpoint of the pretrained model ( t= 4). Performance of the best performing CL algorithm
is marked bold.
jMj, k Chemprot RCT ACL-ARC SciERC MLM- D1;2
100k;10 82.73 79.98 72.50 81.64 1.737/1.857
100k;100 82.06 78.64 71.97 81.62 1.599/1.789
10M;10 82.87 79.98 71.80 81.63 1.438/1.732
Table 3: Downstream task and MLM performance of fT
under different memory sizes jMjand the frequency of replay
k(replaying every ksteps of training) in ER.
tion. With these results, we address the research
questions below.
Does lifelong pretraining help retain knowledge
across different domain corpora? We Ô¨Årst ex-
amine whether task-speciÔ¨Åc or lifelong pretraining
improves performance over domain-speciÔ¨Åc down-
stream tasks. Comparing Task-SpeciÔ¨Åc LMs with
RoBERTa-base in Table 2, we notice consistent per-
formance improvements, especially on Biomedical
and Computer Science domains ( D1;D2). We also
see Sequential Pretraining could consistently out-
perform RoBERTa-base. However, the comparison
between Sequential Pretraining and Task SpeciÔ¨Åc
LMs are mixed: on D1;D2;D3, Sequential Pre-
training could outperform Task-SpeciÔ¨Åc LMs only
except MNER; while on the earliest biomedical
domain (D1), Sequential Pretraining achieves sub-
stantially lower performance. From Figure 4, we
see the performance of Sequential Pretraining on
Chemprot and RCT (from D1) drops signiÔ¨Åcantly
fromt= 1to4. The results imply lifelong pretrain-
ing allows later domains to beneÔ¨Åt from knowledge
transfer from earlier domains, but the performance
on earlier domains is limited because of forgetting.
Does continual learning algorithms help retain
knowledge in sequential pretraining? Next, we
compare different kinds of CL algorithms and in-
vestigate the effect of CL algorithms in alleviating
forgetting and improving knowledge transfer. Ta-ble 2 shows that Online-EWC slightly improves
MLM perplexity compared to Sequential PT, but
brings no improvement to the Ô¨Åne-tuning perfor-
mance. We hypothesize that regularization directly
in the parameter space as in Online-EWC is not
effective when the parameter space is very high
dimensional. Adapter improves downstream task
F1 scores on the bio-medical domain ( D1) by 1.2%
and 0.8%, but does not outperform Sequential Pre-
training in other domains (similarly for Simple
Layer Expansion approach), likely because a great
portion of the model is kept frozen.
In contrast, the memory-replay based approach
(ER) allows training the full parameters of the
model and has been shown to be highly effective
in continual learning of classiÔ¨Åcation tasks (Wang
et al., 2019; Chaudhry et al., 2019). However, we
surprisingly Ô¨Ånd that ER could hardly improve over
Sequential Pretraining except D1. A similar pattern
can be found in the MLM perplexity. We hypothe-
size that the positive effect of example replay has
diminished because of the overÔ¨Åtting to the mem-
ory examples. Table 3 summarizes the effect of
tuning hyperpameters in ER. When we reduce the
frequency of replay (from every 10 steps to 100
steps), the MLM performance improves, which im-
plies reduced overÔ¨Åtting; however, the performance
of downstream task performance does not improve.
When we increase the size of the memory jMjfrom
100kto10M, the MLM perplexity also improves;
still, there are still no improvements in downstream
tasks. It may imply ER itself is not an effective
approach for continual pretraining.
Unlike ER, distillation approaches utilize richer
information such as output logits or representation
similarity to preserve past knowledge. We Ô¨Ånd
either Logit KD or SEED-Logit KD to be most

--- PAGE 7 ---
Sequential ER Adapter Logit
KDRep
KDContrast
KDSEED-Logit
KD81828384
Roberta-base
T ask-Specific
MTL(a)Chemprot
Sequential ER Adapter Logit
KDRep
KDContrast
KDSEED-Logit
KD7879808182
Roberta-base
T ask-Specific
MTL (b)RCT-Sample
Sequential ER Adapter Logit
KDRep
KDContrast
KDSEED-Logit
KD60657075
Roberta-base
T ask-Specific
MTL
(c)ACL-ARC
Sequential ER Adapter Logit
KDRep
KDContrast
KDSEED-Logit
KD78808284
Roberta-base
T ask-Specific
MTL (d)SciERC
Figure 4: Performance evolution of downstream models. Models are Ô¨Åne-tuned from checkpoints of lifelong pretrained LMs
at different time steps t. For Chemprot and RCT-Sample from D1, we use t2 f1;2;3;4g; while for ACL-ARC and SciERC
fromD2,t2 f2;3;4g. Methods achieving the best performance at the end of training ( t= 4) is highlighted.
100 200 500
#. Downstream Training Instances020406080100Macro F129.540.163.9
36.349.665.4
30.248.066.7
29.746.464.9
42.052.970.1
RoBERT a-base
Sequential-PT
MTL
ER
Logit-KD
Figure 5: Performance of downstream models with vari-
ous number of training examples , exempliÔ¨Åed with SciERC.
The models are Ô¨Åne-tuned from the Ô¨Ånal pretrained model ( f4).
effective depending on the task, while Rep-KD
and Contrastive-KD are less effective. The best
performing distillation approach improves F1 over
Sequential Pretraining on downstream tasks from
D1,D2at least by 1.0%. However, performance on
D3;D4, which come later in the data stream, does
not improve over Sequential Pretraining, possibly
because the distillation loss term makes the model
rigid in obtaining new knowledge.
What is the gap between lifelong pretraining
and multi-task learning across all the domains?
Multi-Task Learning refers to the ofÔ¨Çine training
paradigm, which retrain PTLMs over all corpora
(D1::t) each time a new corpus Dtbecomes avail-
able. We examine whether lifelong pretraining is
comparable to multi-task pretraining in terms of
performance. From Table 2 and Figure 4, we see
Sequential Pretraining in general underperforms
MTL except for the Ô¨Ånal domain. However, certain
CL approaches, such as Logit-Distillation, could
improve over MTL on all downstream tasks from
the Ô¨Årst and the second domain. We speculate thereason is that continual learning naturally provides
a curriculum (Xu et al., 2020; Shi et al., 2015) to
models where each individual task is easier to learn.
The results have a positive implication that lifelong
pretraining is not only more computationally efÔ¨Å-
cient and requires less storage of past data, but may
also improve the performance of pretraining.
Does lifelong pretraining make models more data
efÔ¨Åcient? In Table 5, we further examine the per-
formance of Ô¨Ånal pretrained models under different
amounts of training examples. We include full
results in Appendix B. We Ô¨Ånd in general, perfor-
mance improvements are more signiÔ¨Åcant in the
low-resource setup.
Computational Costs. We quantify computa-
tional costs of different CL algorithms with the
number of forward and backward passes in Table 4
and present additional experiments with controlled
computational costs in Appendix F. We Ô¨Ånd ad-
ditional computational cost is necessary for per-
formance improvement of distillation-based CL.
However, it is not possible to trade performance
simply by investing more computation budget with
arbitrary CL algorithms. We leave detailed discus-
sions in Appendix F.
4.3 Temporal Data Stream
We conduct analysis on pretraining PTLM on
chronologically-ordered tweet corpora, to under-
stand whether lifelong pretraining helps adaptation
to the latest data and improves temporal generaliza-
tion ability. The results are summarized in Table 5.

--- PAGE 8 ---
Method #. of Forward #. of Backward #. Total #. Total ( k=10) Wall Time 4k
Main results
Sequential PT b b 2b 2b 4:0104sec.
ER (1 + 1=k)b (1 + 1=k)b (2 + 2=k)b 2:2b 4:2104sec.
Logit-Distill (2 + 2=k)b (1 + 1=k)b (3 + 3=k)b 3:3b 6:9104sec.
SEED-Logit-Distill (3 + 3=k)b (2 + 2=k)b (5 + 5=k)b 5:5b 9:7104sec.
Additional Controlled Experiments
Sequential PT b0=1:2b 1:2b 1:2b 2:4b 2:4b 4:4104sec.
ERk=5 1:2b 1:2b 2:4b 2:4b 4:4104sec.
Sparse Logit-KD 1:3b 1:1b 2:4b 2:4b 4:4104sec.
Sparse SEED-Logit-KD ncontrast 1:3b 1:1b 2:4b 2:4b 4:8104sec.
Table 4: Number of forward and backward passes over PTLMs and wall clock time of different approaches. The
number of forward and backwards passes are computed over visits of bbatches from the training data stream,
wherekis the frequency of replay. The wall clock time is calculated over 4 ksteps of training (which is the
number of training steps of a single domain in the Research Paper stream) excluding the Ô¨Årst domain, as no
replay or distillation happens while learning the Ô¨Årst domain. In the additional controlled experiments (described
in Appendix. F), we control the total number of forward and backward passes of different approaches.
Years 2018 (D3)2020 (D4)2014 (D1)
!2020 (D4)2016 (D2)
!2020 (D4)
Hashtag Prediction
RoBERTa-base 48.08 1:0 56.420:2 39.312:7 42.232:7
Sequential PT 56.79 0:5 59.850:4 44.001:1 49.871:8
ER 56.93 0:1 59.561:7 43.310:2 50.720:6
Logit-KD 58.210:5 60.520:2 44.260:9 50.920:8
Contrast-KD 57.94 0:4 59.540:3 45.220:1 52.141:1
SEED-KD 56.87 0:2 59.710:2 43.390:4 49.621:0
SEED-Logit-KD 57.75 0:460.740:6 45.350:6 51.560:7
Task-SpeciÔ¨Åc (2014) 56.16 0:6 59.590:3 44.340:6 49.260:7
Task-SpeciÔ¨Åc (Latest) 56.61 0:4 59.870:6 43.440:5 49.411:1
MTL 57.89 0:4 59.950:3 44.040:3 50.370:3
Emoji Prediction
RoBERTa-base 25.71 0:1 24.420:2 12.020:4 13.240:2
Sequential PT 29.30 0:1 27.690:1 14.200:2 16.081:4
ER 29.50 0:1 27.750:1 14.360:4 16.820:3
Logit-KD 29.77 0:1 27.800:1 14.200:3 16.281:1
Contrast-KD 29.48 0:2 27.720:3 14.420:3 17.520:1
SEED-KD 30.120:1 27.660:1 14.360:1 16.970:4
SEED-Logit-KD 29.98 0:127.840:2 14.360:1 16.970:3
Task-SpeciÔ¨Åc (2014) 28.94 0:0 26.980:2 13.390:2 15.140:2
Task-SpeciÔ¨Åc (Latest) 29.06 0:2 27.190:1 13.000:2 14.480:3
MTL 29.52 0:2 27.470:0 14.070:2 16.640:2
Table 5: Results on temporal data stream. We show Ô¨Åne-
tuning performance over years 2018 and 2020 ( D3,D4) and
the Temporal generalization from 2014 or 2016 to 2020 data
(D1!D4,D2!D4) on Twitter Hashtag and Emoji predic-
tion datasets. Models are Ô¨Åne-tuned from the Ô¨Ånal pre-trained
model fT. We include full results on other years ( D1,D2,
D3!D4) in Appendix C.
Will LMs be outdated? We compare the perfor-
mance of Task-SpeciÔ¨Åc (2014) to the Task-SpeciÔ¨Åc
models pretrained on the year of downstream
datasets (noted as Task-SpeciÔ¨Åc (Latest)) and no-
tice consistent improvements in downstream tasks
in 2018 and 2020 (Ô¨Årst two columns in Table 5).
Sequential Pretraining could also outperform the
Task-SpeciÔ¨Åc (2014) model. It veriÔ¨Åes that lan-
guage models may get outdated over time, but the
issue can be addressed by task-speciÔ¨Åc or lifelong
pretraining over the latest corpora.Does lifelong pretraining help improve the down-
stream model‚Äôs performance on latest data? We
show that downstream model‚Äôs performance over
later data (D3;D4) can be improved over Task-
SpeciÔ¨Åc models when continual learning algo-
rithms are applied. From the Ô¨Årst two columns
of Table 5, we see Logit-KD and SEED-KD im-
prove Hashtag prediction score over data of years
2018 and 2020. SEED-Logit KD further improves
prediction F1 on Emoji prediction. Note that these
Ô¨Åndings are in contrast to the research paper stream,
where CL algorithms do not improve performance
in the latest domain D4. The reason can be the
higher similarity between domains in the tweet cor-
pora making the knowledge transfer easier, which
is further discussed in Appendix I.
Does lifelong pretraining improve temporal gen-
eralization? Temporal generalization evaluates
downstream performance over latest test data when
Ô¨Åne-tuned over outdated training data. We show
lifelong pretraining brings clear improvement to
temporal generalization. From Table 5, we see
even Sequential Pretraining could improve over
the model pretrained merely on the year 2020 data
(Task-SpeciÔ¨Åc (2020)) consistently. We Ô¨Ånd per-
formance further improves with CL algorithms ap-
plied. SEED-Logit-KD performs best in general
on crossyear hashtag prediction tasks. In crossyear
emoji prediction, we Ô¨Ånd Contrast-KD and SEED-
KD perform best. We also Ô¨Ånd that SEED-Logit-
KD could slightly outperform Logit-KD.
5 Related Works
Domain and Temporal Adaptation of Language
Models. Gururangan et al. (2020) study adapta-

--- PAGE 9 ---
tion of PTLMs to domain-speciÔ¨Åc corpora. Aru-
mae et al. (2020) study algorithms to mitigate for-
getting in original PTLMs, but does not investigate
forgetting that happens over a sequence of domains.
Maronikolakis and Sch√ºtze (2021); R√∂ttger and
Pierrehumbert (2021); Luu et al. (2021) proposes
sequential pretraining over domains or emerging
data, but did not investigate CL algorithms. Sev-
eral recent studies have demonstrated the neces-
sity of adapting LMs over time (Lazaridou et al.,
2021) while speciÔ¨Åcally focusing on factual knowl-
edge (Dhingra et al., 2021; Jang et al., 2021).
Continual Learning Algorithms in NLP. Con-
tinual learning in NLP has mainly been studied for
classiÔ¨Åcation tasks. An effective approach is to
utilize a number of stored past examples (de Mas-
son d‚ÄôAutume et al., 2019; Wang et al., 2020), or
pseudo examples ( e.g., the ones generated with a
PTLM (Sun et al., 2020; Kanwatchara et al., 2021)).
Recent extensions of the algorithm (Chuang et al.,
2020) perform knowledge distillation with gener-
ated pseudo examples. Other lines of works fo-
cus on regularization over the sentence representa-
tions (Wang et al., 2019; Huang et al., 2021; Liu
et al., 2019a) or directly merging models in the
parameter space (Matena and Raffel, 2021). Model
expansion-based approaches (Liu et al., 2019a;
Pfeiffer et al., 2021), including learning domain
speciÔ¨Åc expert models (Gururangan et al., 2021),
are also actively studied. Wu et al. (2022) present a
comparative study of algorithms in the context of
continual Ô¨Åne-tuning over NLP tasks.
6 Conclusion
In this paper, we formulated the lifelong language
model pretraining problem and constructed two
data streams associated with downstream datasets.
We evaluated knowledge retention, adaptation to
the latest data, and temporal generalization ability
of continually pretrained language models. Our
experiments show distillation-based approaches
being most effective in these evaluation setups.
A limitation of the work is that it has not been
fully addressed whether there exists a variant of
distillation-based CL approach that consistently
outperforms Logit-KD. Based on the current obser-
vation, we conclude the performance of different
KD approaches for CL is highly task-dependent. It
asks for more future works into continual learning
algorithms within the proposed problem setup.References
Kristjan Arumae, Qing Sun, and Parminder Bhatia.
2020. An empirical investigation towards efÔ¨Å-
cient multi-domain language model pre-training. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 4854‚Äì4864, Online. Association for Computa-
tional Linguistics.
Isabelle Augenstein, Mrinal Das, Sebastian Riedel,
Lakshmi Vikraman, and Andrew McCallum. 2017.
SemEval 2017 task 10: ScienceIE - extracting
keyphrases and relations from scientiÔ¨Åc publica-
tions. In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017) ,
pages 546‚Äì555, Vancouver, Canada. Association for
Computational Linguistics.
Abdul Hameed Azeemi and Adeel Waheed. 2021.
Covid-19 tweets analysis through transformer lan-
guage models. ArXiv , abs/2103.00199.
Francesco Barbieri, Jose Camacho-Collados,
Francesco Ronzano, Luis Espinosa-Anke, Miguel
Ballesteros, Valerio Basile, Viviana Patti, and
Horacio Saggion. 2018. SemEval 2018 task 2:
Multilingual emoji prediction. In Proceedings
of The 12th International Workshop on Semantic
Evaluation , pages 24‚Äì33, New Orleans, Louisiana.
Association for Computational Linguistics.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-
ERT: A pretrained language model for scientiÔ¨Åc text.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3615‚Äì
3620, Hong Kong, China. Association for Computa-
tional Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual .
Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. 2021.
Co2l: Contrastive continual learning. ArXiv ,
abs/2106.14413.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-
seiny, Thalaiyasingam Ajanthan, Puneet K Dokania,
Philip HS Torr, and Marc‚ÄôAurelio Ranzato. 2019.
On tiny episodic memories in continual learning.
arXiv preprint arXiv:1902.10486 .

--- PAGE 10 ---
Yung-Sung Chuang, Shang-Yu Su, and Yun-Nung
Chen. 2020. Lifelong language knowledge distil-
lation. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2914‚Äì2924, Online. Associa-
tion for Computational Linguistics.
Cyprien de Masson d‚ÄôAutume, Sebastian Ruder, Ling-
peng Kong, and Dani Yogatama. 2019. Episodic
memory in lifelong language learning. In Advances
in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada , pages 13122‚Äì13131.
Franck Dernoncourt and Ji Young Lee. 2017. PubMed
200k RCT: a dataset for sequential sentence clas-
siÔ¨Åcation in medical abstracts. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers) ,
pages 308‚Äì313, Taipei, Taiwan. Asian Federation of
Natural Language Processing.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171‚Äì4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisen-
schlos, D. Gillick, Jacob Eisenstein, and William W.
Cohen. 2021. Time-aware language models as tem-
poral knowledge bases. ArXiv , abs/2106.15110.
Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, L. Zhang,
Yezhou Yang, and Zicheng Liu. 2021. Seed:
Self-supervised distillation for visual representation.
ArXiv , abs/2101.04731.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. ArXiv , abs/2104.08821.
Yuyun Gong and Qi Zhang. 2016. Hashtag recom-
mendation using attention-based convolutional neu-
ral network. In Proceedings of the Twenty-Fifth
International Joint Conference on ArtiÔ¨Åcial Intelli-
gence, IJCAI 2016, New York, NY, USA, 9-15 July
2016 , pages 2782‚Äì2788. IJCAI/AAAI Press.
Suchin Gururangan, Michael Lewis, Ari Holtzman,
Noah A. Smith, and Luke Zettlemoyer. 2021. Demix
layers: Disentangling domains for modular lan-
guage modeling. ArXiv , abs/2108.05036.
Suchin Gururangan, Ana Marasovi ¬¥c, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don‚Äôt stop pretraining:
Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages8342‚Äì8360, Online. Association for Computational
Linguistics.
Geoffrey E. Hinton, Oriol Vinyals, and J. Dean. 2015.
Distilling the knowledge in a neural network. ArXiv ,
abs/1503.02531.
Spurthi Amba Hombaiah, Tao Chen, Mingyang Zhang,
Michael Bendersky, and Marc-Alexander Najork.
2021. Dynamic language models for continuously
evolving content. Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery &
Data Mining .
Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang,
and Dahua Lin. 2018. Lifelong learning via progres-
sive distillation and retrospection. In ECCV .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efÔ¨Åcient transfer learning for NLP.
InProceedings of the 36th International Confer-
ence on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , volume 97 of
Proceedings of Machine Learning Research , pages
2790‚Äì2799. PMLR.
Xiaolei Huang and Michael J. Paul. 2018. Examining
temporality in document classiÔ¨Åcation. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers) , pages 694‚Äì699, Melbourne, Australia. Asso-
ciation for Computational Linguistics.
Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi
Wang, and Diyi Yang. 2021. Continual learning for
text classiÔ¨Åcation with information disentanglement
based regularization. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2736‚Äì2746, Online.
Association for Computational Linguistics.
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,
Janghoon Han, Gyeonghun Kim, Stanley Jungkyu
Choi, and Minjoon Seo. 2021. Towards contin-
ual knowledge learning of language models. arXiv
preprint arXiv:2110.03215 .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2020. TinyBERT: Distilling BERT for natural lan-
guage understanding. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
4163‚Äì4174, Online. Association for Computational
Linguistics.
David Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-
Farland, and Dan Jurafsky. 2018. Measuring the evo-
lution of a scientiÔ¨Åc Ô¨Åeld through citation frames.
Transactions of the Association for Computational
Linguistics , 6:391‚Äì406.

--- PAGE 11 ---
Kasidis Kanwatchara, Thanapapas Horsuwan, Piyawat
Lertvittayakumjorn, Boonserm Kijsirikul, and Peer-
apon Vateekul. 2021. Rational LAMOL: A
rationale-based lifelong learning framework. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 2942‚Äì
2953, Online. Association for Computational Lin-
guistics.
Angeliki Lazaridou, A. Kuncoro, E. Gribovskaya, De-
vang Agrawal, Adam Liska, Tayfun Terzi, Mai
Gimenez, Cyprien de Masson d‚ÄôAutume, Sebastian
Ruder, Dani Yogatama, Kris Cao, Tom√°s Kocisk√Ω,
Susannah Young, and P. Blunsom. 2021. Assessing
temporal generalization in neural language models.
NeurIPS .
Zhizhong Li and Derek Hoiem. 2018. Learning with-
out forgetting. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence , 40:2935‚Äì2947.
Tianlin Liu, Lyle Ungar, and Jo√£o Sedoc. 2019a. Con-
tinual learning for sentence representations using
conceptors. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 3274‚Äì3279, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv , abs/1907.11692.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-
ney, and Daniel Weld. 2020. S2ORC: The semantic
scholar open research corpus. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 4969‚Äì4983, Online. As-
sociation for Computational Linguistics.
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh
Hajishirzi. 2018. Multi-task identiÔ¨Åcation of enti-
ties, relations, and coreference for scientiÔ¨Åc knowl-
edge graph construction. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 3219‚Äì3232, Brussels, Bel-
gium. Association for Computational Linguistics.
Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-
ishma Mandyam, and Noah A. Smith. 2021. Time
waits for no one! analysis and challenges of tempo-
ral misalignment.
Antonis Maronikolakis and Hinrich Sch√ºtze. 2021.
Multidomain pretrained language models for green
NLP. In Proceedings of the Second Workshop
on Domain Adaptation for NLP , pages 1‚Äì8, Kyiv,
Ukraine. Association for Computational Linguistics.Michael Matena and Colin Raffel. 2021. Merging
models with Ô¨Åsher-weighted averaging. ArXiv ,
abs/2111.09832.
Martin M√ºller, Marcel Salath√©, and Per Egil Kummer-
vold. 2020. Covid-twitter-bert: A natural language
processing model to analyse covid-19 content on
twitter. ArXiv , abs/2005.07503.
Sheshera Mysore, Zachary Jensen, Edward Kim, Kevin
Huang, Haw-Shiuan Chang, Emma Strubell, Jef-
frey Flanigan, Andrew McCallum, and Elsa Olivetti.
2019. The materials science procedural text corpus:
Annotating materials synthesis procedures with shal-
low semantic structures. In Proceedings of the 13th
Linguistic Annotation Workshop , pages 56‚Äì64, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.
2020. BERTweet: A pre-trained language model
for English tweets. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations , pages 9‚Äì
14, Online. Association for Computational Linguis-
tics.
Elsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga
Kononova, Gerbrand Ceder, Thomas Yong-Jin Han,
and Anna M Hiszpanski. 2020. Data-driven mate-
rials research enabled by natural language process-
ing and information extraction. Applied Physics Re-
views , 7(4):041317.
Jonas Pfeiffer, Aishwarya Kamath, Andreas R√ºckl√©,
Kyunghyun Cho, and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume ,
pages 487‚Äì503, Online. Association for Computa-
tional Linguistics.
Sylvestre-Alvise RebufÔ¨Å, Alexander Kolesnikov,
Georg Sperl, and Christoph H. Lampert. 2017. icarl:
Incremental classiÔ¨Åer and representation learning.
In2017 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2017, Honolulu, HI,
USA, July 21-26, 2017 , pages 5533‚Äì5542. IEEE
Computer Society.
Shruti Rijhwani and Daniel Preotiuc-Pietro. 2020.
Temporally-informed analysis of named entity
recognition. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 7605‚Äì7617, Online. Association for
Computational Linguistics.
Anthony V . Robins. 1995. Catastrophic forgetting, re-
hearsal and pseudorehearsal. Connect. Sci. , 7:123‚Äì
146.
Paul R√∂ttger and J. Pierrehumbert. 2021. Temporal
adaptation of bert and performance on downstream
document classiÔ¨Åcation: Insights from social media.
Findings of EMNLP .

--- PAGE 12 ---
Jonathan Schwarz, Wojciech Czarnecki, Je-
lena Luketina, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell.
2018. Progress & compress: A scalable framework
for continual learning. In Proceedings of the 35th
International Conference on Machine Learning,
ICML 2018, Stockholmsm√§ssan, Stockholm, Swe-
den, July 10-15, 2018 , volume 80 of Proceedings
of Machine Learning Research , pages 4535‚Äì4544.
PMLR.
Yangyang Shi, Martha Larson, and Catholijn M. Jonker.
2015. Recurrent neural network language model
adaptation with curriculum learning. Comput.
Speech Lang. , 33:136‚Äì154.
Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee.
2020. LAMOL: language modeling for lifelong lan-
guage learning. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.
Patient knowledge distillation for BERT model com-
pression. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
4323‚Äì4332, Hong Kong, China. Association for
Computational Linguistics.
Jens Vindahl. 2016. Chemprot-3.0: a global chemical
biology diseases mapping.
Hong Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo,
Shiyu Chang, and William Yang Wang. 2019. Sen-
tence embedding alignment for lifelong relation ex-
traction. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
796‚Äì806, Minneapolis, Minnesota. Association for
Computational Linguistics.
Zirui Wang, Sanket Vaibhav Mehta, Barnabas Poczos,
and Jaime Carbonell. 2020. EfÔ¨Åcient meta lifelong-
learning with limited memory. In Proceedings of
the 2020 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP) , pages 535‚Äì548,
Online. Association for Computational Linguistics.
Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang
Li, Guilin Qi, and Gholamreza Haffari. 2022. Pre-
trained language model in continual learning: A
comparative study. In International Conference on
Learning Representations .
Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan
Wang, Hongtao Xie, and Yongdong Zhang. 2020.
Curriculum learning for natural language under-
standing. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 6095‚Äì6104, Online. Association for Computa-
tional Linguistics.Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong,
and Furu Wei. 2021. Adapt-and-distill: Developing
small, fast and effective pretrained language mod-
els for domains. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 460‚Äì470, Online. Association for Computa-
tional Linguistics.

--- PAGE 13 ---
100 200 500
#. Downstream Training Instances020406080100Micro F147.155.070.7
49.858.570.4
51.158.871.5
53.559.469.3
53.463.674.3
RoBERT a-base
Sequential-PT
MTL
ER
Logit-KD(a)Chemprot
100 200 500
#. Downstream Training Instances020406080100Macro F129.540.163.9
36.349.665.4
30.248.066.7
29.746.464.9
42.052.970.1
RoBERT a-base
Sequential-PT
MTL
ER
Logit-KD
(b)SciERC
Figure 6: Performance of downstream models with various
number of training examples. The models are Ô¨Åne-tuned from
the Ô¨Ånal pretrained model ( f4).
A Detailed Experiment Settings
We use a linearly decreasing learning rate initial-
ized with 5e-4 on the research paper stream and
3e-4 on the tweet stream. On the research paper
stream, we train the model for 8,000 steps in the
Ô¨Årst task, and 4,000 steps in the subsequent tasks.
On the tweet stream, we train the model for 8,000
steps in all tasks. We hold out 128,000 sentences
from each corpus to evaluate MLM performance.
As the size of pretraining corpora is large, during
training, each training example is visited only once.
We use the masked language modeling perplex-
ity over held-out validation sets of the pretraining
corpora as the metrics for hyperparameter tuning.
Common hyperparameters such as learning rate
and batch sizes are tuned with Task-speciÔ¨Åc models
with the Ô¨Årst task. Hyperparameters that are spe-
ciÔ¨Åc to continual learning algorithms, such as the
scale of the distillation loss, is tuned using the Ô¨Årst
two domains in the stream according to the MLM
performance over validation sets. The weight of
the distillation term is set as 1.0 for logit dis-
tillation and 0.1 for other distillation algorithms.
By default, we replay or perform distillation with
a mini-batch of examples from the replay mem-
ory every 10 training steps in ER and Distillation-
based CL approaches. We use the huggingface
transformers library https://github.com/
huggingface/transformers for implemen-
tation.Task 2014 2016 2018 2020
Hashtag Prediction
RoBERTa-base 56.65 0:645.502:148.081:056.420:2
Sequential PT 59.00 0:154.280:356.790:559.850:4
ER 59.00 0:154.900:256.930:159.561:7
Adapter 58.76 0:752.551:554.341:759.011:0
Logit-KD 60.93 0:555.960:258.210:560.520:2
Rep-KD 60.47 0:151.772:655.791:459.800:2
Contrast-KD 60.72 0:655.850:057.940:459.540:3
SEED-KD 58.82 0:454.550:556.870:259.710:2
SEED-Logit-KD 61.28 0:255.590:557.750:460.740:6
Task-SpeciÔ¨Åc (2014) 61.62 0:355.380:656.160:659.590:3
Task-SpeciÔ¨Åc (Latest) 59.91 0:355.471:056.610:459.870:6
MTL 60.51 0:355.161:657.890:459.950:3
Emoji Prediction
RoBERTa-base 28.73 0:226.860:225.710:124.420:2
Sequential PT 32.69 0:230.550:329.300:127.690:1
ER 32.88 0:230.520:229.500:127.750:1
Adapter 32.15 0:229.850:028.720:026.800:3
Logit-KD 33.08 0:330.880:129.770:127.800:1
Rep-KD 32.71 0:230.510:229.450:127.270:2
Contrast-KD 32.90 0:131.010:129.480:227.720:3
SEED-KD 32.91 0:130.840:330.120:127.660:1
SEED-Logit-KD 33.28 0:131.170:129.980:127.840:2
Task-SpeciÔ¨Åc (2014) 33.37 0:230.540:328.940:026.980:2
Task-SpeciÔ¨Åc (Latest) 32.31 0:029.830:529.060:227.190:1
MTL 32.78 0:130.540:029.520:227.470:0
Table 6: Full performance on Twitter Hashtag prediction and
Emoji prediction, Ô¨Åne-tuned from the pre-trained model in the
Ô¨Ånal time step.
B Low-Resource Fine-Tuning
Figure 6 summarizes the performance of Ô¨Åne-tuned
models from the Ô¨Ånal model checkpoint ( t= 4)
using different amount of downstream training ex-
amples. We see on Chemprot and SciERC, the ben-
eÔ¨Åt of Sequential Pretraining over RoBERTa-base
is more signiÔ¨Åcant in low-resource Ô¨Åne-tuning se-
tups. Whenever Seqential Pretraining outperforms
RoBERTa-base, we notice Logit-KD could further
improve over Sequential Pretraining.
C Full Results over the Tweet Stream
Tables 6 and 7 summarize full results over the
Tweet stream. Compared to the table 5 in the main
text, we add downstream performance over data
from years 2014 and 2016 ( D1,D2), and temporal
generalization from year 2014 to 2020 ( D1!D4).
D Dataset Details
The research paper stream consists of full text
of 6.6M, 12.1M, 7.8M, and 7.5M research pa-
pers from the S2ORC (Lo et al., 2020) dataset.
We evaluate downstream Ô¨Åne-tuning performance
on two in-domain datasets for each research area:
Chemprot relation exaction dataset (Vindahl, 2016)
and RCT abstract sentence role labeling dataset
(Dernoncourt and Lee, 2017) for the bio-medical

--- PAGE 14 ---
Task 2014 !2020 2016!2020 2018!2020
Crossyear Hashtag Prediction
RoBERTa-base 39.31 2:7 42.232:7 37.192:1
Sequential PT 44.00 1:1 49.871:8 46.630:9
ER 43.31 0:2 50.720:6 46.270:4
Adapter 42.61 0:5 48.001:6 42.630:9
Logit-KD 44.26 0:9 50.920:8 46.841:0
Rep-KD 42.48 0:2 50.381:5 42.230:2
Contrast-KD 45.22 0:1 52.141:1 47.470:8
SEED-KD 43.39 0:4 49.621:0 46.370:8
SEED-Logit-KD 45.35 0:6 51.560:7 47.740:3
Task-SpeciÔ¨Åc (2014) 44.34 0:6 49.260:7 45.090:7
Task-SpeciÔ¨Åc (2020) 43.44 0:5 49.411:1 44.340:4
-4x steps 44.340:6 51.780:7 44.690:7
MTL 44.04 0:3 50.370:3 44.310:0
Crossyear Emoji Prediction
RoBERTa-base 12.02 0:4 13.240:2 18.670:1
Sequential PT 14.20 0:2 16.081:4 21.060:9
ER 14.36 0:4 16.820:3 21.570:1
Adapter 13.53 0:2 15.680:3 20.640:1
Logit-KD 14.20 0:3 16.281:1 21.291:0
Rep-KD 13.89 0:1 16.030:3 20.860:2
Contrast-KD 14.42 0:3 17.520:1 21.430:1
SEED-KD 14.36 0:1 16.970:4 21.880:3
SEED-Logit-KD 14.36 0:1 16.970:3 21.620:1
Task-SpeciÔ¨Åc (2014) 13.39 0:2 15.140:2 20.790:3
Task-SpeciÔ¨Åc (2020) 13.00 0:2 14.480:3 19.300:2
-4x steps 12.900:4 14.850:3 19.830:2
MTL 14.07 0:2 16.640:2 20.940:7
Table 7: Temporal generalization performance on Twitter
Hashtag prediction datasets Ô¨Åne-tuned from the Ô¨Ånal pre-
trained model. Year 1 !Year 2 indicates the hashtag pre-
diction model is Ô¨Åne-tuned on data in year Year 1 , and
evaluated on test data in Year 2 .
domain; ACL-ARC citation intent classiÔ¨Åcation
dataset (Jurgens et al., 2018) and SciERC rela-
tion extraction dataset (Luan et al., 2018) for the
computer science domain; relation extraction over
Synthesis procedures (Mysore et al., 2019) and
named entity recognition over material science
papers (MNER) (Olivetti et al., 2020) for mate-
rial science domain; keyphrase classiÔ¨Åcation and
hyponym classiÔ¨Åcation after Ô¨Åltering out physics
papers for the physics domain (Augenstein et al.,
2017). We report micro-averaged F1 on Chemprot,
RCT, MNER datasets following the evaluation
metrics in the original work, and report macro-
averaged F1 on all other datasets. We use the of-
Ô¨Åcial data splits for all datasets except for RCT,
where we employ a low-resource training setup
following Gururangan et al. (2020).
The pretraining corpora for the tweet stream con-
sist of 25M tweets in each year. For downstream
tasks, we use a separate set of 1M tweets from
each year to construct multi-label hashtag predic-
tion (Gong and Zhang, 2016) datasets and single-
label emoji prediction datasets (Barbieri et al.,
2018). We replace user names to special tokens.
For Hashtag prediction, the label space consists oftweets containing 200 most frequent hashtags in
each year. We independently sample 500 tweets
per label (hashtag) as training, validation and test
sets, which results 10 kexamples in each of the
data splits. For emoji prediction, we construct 20-
way single-label emoji prediction datasets for each
year following Barbieri et al. (2018) with the 1M
held out tweets. We sample 5,000 tweets per emoji
in each split, resulting in balanced datasets of the
same size as the hashtag prediction datasets.
E Details of Continual Learning
Algorithms
E.1 Contrastive Distillation
During continual pretraining, in addition to the
language model pretraining objective, we add a un-
supervised contrastive learning objective, namely
the SimCSE (Gao et al., 2021) objective, so that
the similarity in the sentence representation better
reÔ¨Çects the semantic similarity in the sentence. We
use thel2-normalized representation of the start-
of-sequence token at the Ô¨Ånal layer as the sentence
representation, noted as h. Then, we distill the
intra-batch representational similarity from the pre-
vious model ft 1to the current model ft. Given a
mini-batch of Nexamples x, we compute the rep-
resentational dot-product similarity matrix between
normalized sentence representations hbetween
each pair of examples with ft 1andft, noted as
Bt 1andBt, where each element Bijis,
Bij=exp(hihj=)P
k=1::Nexp(hihk=)(1)
whereis a temperature hyperparameter. We spec-
ify a temperature t= 0:05for the teacher model
ft 1and a temperature sfor the student model
ft= 0:01. We compute the cross-entropy between
Bt 1andBtas the distillation loss,
`distill= 1
NNX
i=1NX
j=1Bt 1
ijlogBt
ij (2)
E.2 SEED Distillation
SEED distillation proposed by (Fang et al., 2021)
has a similar spirit as the contrastive distillation
with differences in the examples used for com-
puting similarity matrices computes. The algo-
rithm distills representational similarity between
the batch and a large set of other examples , main-
tained in an example queue Q. As the number

--- PAGE 15 ---
of target examples Kcan be much larger than
the batch size, it allows distillation of richer in-
formation by regularizing similarities. During pre-
training, the method maintains a Ô¨Åxed-size queue
Qto cache examples from the current domain
Dt. Given a mini-batch of training examples x,
it computes cosine similarity between each pair
of examples within the batch xandQwithft 1
andft, resulting in two similarity matrices Bt 1,
Py2RjBjjQj. Similar to the contrastive distil-
lation, the distillation loss is the cross-entropy be-
tween two similarity matrices Bt 1andBtcom-
puted in the same way as Eq. 2.
F Analysis and Controlled Experiments
of Computational Costs
Computational cost is a crucial matter for online
continual learning systems. In this section, we ana-
lyze the computational costs of continual learning
algorithms and perform controlled experiments of
computational costs.
We quantify computational costs with the total
number of forward (Cf) and backward ( Cb)com-
putations (C=Cf+Cb) over the PTLMs, which is
easy to control; in practice, we Ô¨Ånd the wall clock
time of training was approximately linear to C. We
summarize the number of forward and backward
passes and the wall clock time of training in Table 4.
In the visit of bbatches from the training stream,
Sequential PT performs bforward and backward
passes respectively over the PTLM, resulting in
C= 2b. Experience replay further replays 1 batch
of examples every ksteps over the training stream,
which results in C= (2 + 2=k)b. In our main
experiments, ris set to 10 (Sec. 3.3). Logit-Distill
and Rep-Distill require one additional forward pass
over a frozen PTLM to compute the target of dis-
tillation, resulting in C= (3 + 3=k)b. Distilla-
tion algorithms that perform contrastive learning
with SimCSE ( i.e.SEED-Distill and SEED-Logit-
Distill) additionally require one forward and back-
ward pass using the same batch of examples with
different dropout masks. Therefore, for SEED-
Logit-Distill, C= (5 + 5=k)b.
To control the number of forward and backward
passes, we present approaches to compensate the
lower computation costs compared to Distillation
algorithms and one approach to shrink the com-
putational cost of distillation algorithms: (1) for
Sequential PT, we train the models for 1.2 times
more steps so that C= 2:4b, noted as SequentialPTb0=1:2b; (2) for ER, we increase the replay fre-
quencykto 5 from the default setup 10, so that
C= 2:4b. We also decrease the cost of Logit-KD
and SEED-Logit-KD by reducing the frequency
of distillation from every 1 batch to every r0=10
steps, while still replaying and distilling knowledge
over 1 batch of memory examples every 10 train-
ing steps. This results in Cf= (1 + 2=k+ 1=k0)b
andCb= (1 + 1=k)b, whereC= 2:4bwhen
bothrandr0are 10. The approach is referred to
as Sparse Logit-KD. Finally, for SEED-Logit-KD,
we remove the SimCSE loss from training and per-
form sparse distillation similar to Sparse-Logit-KD,
which also results in C= 2:4b.
The performance of the models is presented in
Table 9. We notice that at the end of pretraining, in-
creasing the number of training steps in Sequential
PT by 1.2 times does not lead to performance boost
on the latest domain ( D4), while the performance
over tasks from earlier domains (Chemprot, ACL-
ARC, SciERC) slightly dropped, possibly due to
increased forgetting. For ER, we notice replay-
ing only slightly more frequently (ER k=5) than
the default setup ( k=10) greatly increased the per-
plexity of MLM, implying signiÔ¨Åcantly increased
overÔ¨Åtting to the memory; while the performance
differences of downstream tasks compared to the
default ER is mixed. When we decrease the replay
frequency of distillation, the performance on Logit-
KD and SEED-KD also decreased and does not
outperform ER.
The results show additional computation costs
can be necessary for continual learning algorithms
such as Logit-KD and SEED-Logit-KD. However,
the results also show that there is no simple trade-
off between computational cost and performance.
We have seen that it is not always beneÔ¨Åcial to in-
crease the number of training steps over the emerg-
ing data, as it increases forgetting in earlier do-
mains. Similarly, increasing the frequency of re-
play may lead to signiÔ¨Åcant overÔ¨Åtting to the re-
play memory. Investigating into more effective
continual learning algorithms, despite increased
computation costs, allows us to obtain performance
improvement that cannot be simply traded with
more computation with arbitrary continual learning
algorithms. We leave more thorough studies into
this topic as future work.

--- PAGE 16 ---
Sequential ER Logit
KDSEED-Logit
KD8384858687
RoBERT a-large
Domain-Specific(a)Chemprot
Sequential ER Logit
KDSEED-Logit
KD7880828486
RoBERT a-large
Domain-Specific (b)RCT-Sample
Sequential ER Logit
KDSEED-Logit
KD60657075
RoBERT a-large
Domain-Specific
(c)ACL-ARC
Sequential ER Logit
KDSEED-Logit
KD80828486
RoBERT a-large
Domain-Specific (d)SciERC
Figure 7: Performance evolution of downstream models with RoBERTa-large as the base model. Models are Ô¨Åne-tuned from
checkpoints of lifelong pretrained LMs at different time steps t. For Chemprot and RCT-Sample from D1, we use t2 f1;2;3;4g;
while for ACL-ARC and SciERC from D2,t2 f2;3;4g. Methods achieving the best performance at the end of training ( t= 4)
is highlighted.
Domain Biomedical Computer Science
Dataset Chemprot RCT-Sample ACL-ARC SciERC
RoBERTa-large 84.39 0:7 80.760:7 72.203:283.020:8
Naive 85.43 0:6 81.100:5 73.442:082.880:7
ER 85.42 0:2 81.300:4 71.512:583.220:5
Logit-KD 86.180:7 81.930:7 72.102:083.230:6
SEED-Logit-KD 85.98 0:4 81.340:4 74.023:083.890:6
Task-SpeciÔ¨Åc 85.99 0:3 82.020:6 76.071:082.910:9
Table 8: Results on the Research Paper stream with
RoBERTa-large as the base model.
G Experiments with RoBERTa-large
We present additional experiments on RoBERTa-
large. Figure 7 and Table 8 summarizes the re-
sults of selected continual learning algorithms and
baselines. On Chemprot, RCT-Sample, ACL-ARC
and SciERC, either SEED-Logit-KD or Logit-KD
achieves best performance with the Ô¨Ånal pretrained
model checkpoint. We notice that sometimes cer-
tain continual learning algorithms (ER, Logit-KD)
achieves lower F1 at the initial time step ( e.g.,t=2
in Figure 7(c)). In these cases, we hypothesize
continual learning algorithms may hurt model‚Äôs
performance in capturing new knowledge, despite
its potential to reduce forgetting.
H Experiments with BERT on Tweet
Stream After 2019
In this section, we present an additional set of exper-
iments on BERT-base (Devlin et al., 2019) model,
which is originally pretrained with Wikipedia arti-
cles before 2019, with Tweets only after 2019. Thetraining corpora D1::4consist of tweets from the
Ô¨Årst half of 2019, the second half of 2019, the Ô¨Årst
half of 2020, and the second half of 2020 respec-
tively. We accordingly construct hashtag prediction
and cross-year hashtag prediction datasets. The
performance of downstream tasks Ô¨Åne-tuned from
the Ô¨Ånal pretrained model is presented in Table 10.
We see Sequential PT clearly outperforms BERT-
base which is not continually pretrained, and that
Logit-KD generally improves hashtag prediction
performance compared to Sequential PT except on
the Ô¨Årst half of 2019. We hypothesize the small
temporal gap between D1::4makes improvements
less signiÔ¨Åcant than our main experiment setup.
We present temporal generalization performance
in cross-year hashtag prediction tasks in Table 11.
Similarly, Logit-KD improves over Sequential PT
in two out of three cross-year hashtag prediction
setups.
I Analysis of Data Streams
In this section, we provide further analysis about
the created research paper stream and the tweet
stream. We measure cosine distances dvof vocab-
ulary distributions between each pair of different
domains (D1::4)and summarize the results in Fig-
ure 8. The results indicate that the Tweet stream has
a magnitude smaller vocabulary distribution gap
between domains, which is in the scale of 1e 5,
compared to the research paper stream, which is
in the scale of 1e 2. On the Tweet stream, we see
the differences of vocabulary distributions align

--- PAGE 17 ---
Task D1- Biomedical D2- Computer Science D3- Materials Science D4- Physics
Dataset Chemprot RCT-Sample MLM ACL-ARC SciERC MLM MNER Synthesis MLM Keyphrase Hyponym MLM
Sequential Pretraining 82.09 0:5 79.600:5 1.654 72.73 2:981.430:81.807 83.990:392.101:01.590 67.57 1:074.684:41.381
Sequential Pretraining b0=1:2b81.680:5 79.800:4 1.656 70.57 3:080.891:21.793 83.650:392.160:71.578 67.611:475.034:11.379
ER 82.73 0:3 79.980:3 1.737 72.50 1:081.641:11.857 83.990:492.650:41.621 66.11 1:172.824:31.391
ERk=5 83.000:1 79.790:4 1.913 69.85 2:682.301:22.049 84.030:291.600:61.721 65.55 0:475.643:21.418
Logit-KD-Sparse 82.80 0:4 79.800:5 1.476 73.31 2:081.190:81.744 83.840:492.290:71.472 66.650:777.277:11.385
SEED-KD-Sparse 82.51 0:4 79.520:5 1.474 73.70 3:481.920:81.741 83.960:392.201:01.480 64.75 1:171.293:61.381
Table 9: Performance of distillation algorithms in the setup of controlled computational costs.
Task 2019-1 2019-2 2020-1 2020-2
Hashtag Prediction
BERT-base 46.38 0:448.050:841.671:069.000:5
Sequential PT 50.46 0:152.700:746.491:071.630:7
ER 49.90 0:452.330:646.840:371.670:4
Logit-KD 50.19 0:953.700:447.640:472.440:5
SEED-Logit-KD 50.790:852.840:546.040:472.240:6
Table 10: Hashtag prediction performance of continually
pretrained BERT models over tweets after 2019.
Task 2019-1 !2019-2 2019-1!2020-1 2019-1!2020-2
Hashtag Prediction
BERT-base 40.19 0:3 41.000:6 40.850:8
Sequential PT 43.30 0:7 48.602:1 44.070:8
ER 42.96 0:9 46.071:6 44.260:7
Logit-KD 43.35 1:6 46.910:5 45.030:2
SEED-Logit-KD 43.560:4 45.770:7 43.760:5
Table 11: Temporal generalization performance of Hash-
tag prediction models Ô¨Åne-tuned from continually pretrained
BERT models over tweets after 2019.
with the temporal gap between domains. On the
research paper stream, we Ô¨Ånd some domains to
be more similar than others. For example, Bio-
medical (D1) and Material Science domains (D3)
have larger similarity in their vocabulary distribu-
tions, which explains general downstream perfor-
mance increase on D1after the model is pretrained
onD3(Fig. 4 (a,b)).
The differences in vocabulary distribution ex-
plain inconsistency in results between two data
streams, speciÔ¨Åcally, whether lifelong pretraining
improves downstream model performance on the
latest domain, as we mentioned in Sec. 4.3. Other
than this, our main Ô¨Åndings, such as the effect of
distillation-based CL algorithms on reducing for-
getting, are consistent over two datasets with such
signiÔ¨Åcant differences in their changes of vocab-
ulary distribution. We believe it implies the con-
clusions in this paper should be reliable in diverse
data streams.
J Ethic Risks
We would like to note that, in practice, continu-
ally pretrained models over real-world data streams
BioMedCS
MaterialsPhysicsBioMed
CS
Materials
Physics0.00e+00 5.35e-02 1.28e-02 6.58e-02
0.00e+00 2.65e-02 1.17e-02
0.00e+00 2.97e-02
0.00e+00Research Paper Stream(a)Research Paper Stream
2014 2016 2018 20202014
2016
2018
20200.00e+00 9.92e-06 1.61e-05 2.03e-05
0.00e+00 9.40e-06 2.54e-05
0.00e+00 6.83e-06
0.00e+00T weet Stream
(b)Tweet Stream
Figure 8: Cosine distance of vocabulary distributions
between each pair of datasets in two data streams.

--- PAGE 18 ---
would require identiÔ¨Åcation and removal of biased
contents from pretraining corpora, which may af-
fect the prediction of downstream models. As
PTLMs are continuously updated, the bias in earlier
pretraining may have a profound negative impact.
In future works, it is preferable to develop algo-
rithms to ‚Äúforget‚Äù certain biased knowledge from
language models. We further note that any data
released in this paper, especially the tweet stream,
should only be used for research purposes.
