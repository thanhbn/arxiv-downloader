# 2305.04769.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2305.04769.pdf
# File size: 7435914 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Kishaan Jeeveswaran1Prashant Bhat1 2Bahram Zonooz* 1 2Elahe Arani* 1 2
Abstract
The ability of deep neural networks to continu-
ally learn and adapt to a sequence of tasks has
remained challenging due to catastrophic forget-
ting of previously learned tasks. Humans, on the
other hand, have a remarkable ability to acquire,
assimilate, and transfer knowledge across tasks
throughout their lifetime without catastrophic for-
getting. The versatility of the brain can be at-
tributed to the rehearsal of abstract experiences
through a complementary learning system. How-
ever, representation rehearsal in vision transform-
ers lacks diversity, resulting in overﬁtting and con-
sequently, performance drops signiﬁcantly com-
pared to raw image rehearsal. Therefore, we pro-
pose BiRT, a novel representation rehearsal-based
continual learning approach using vision trans-
formers. Speciﬁcally, we introduce constructive
noises at various stages of the vision transformer
and enforce consistency in predictions with re-
spect to an exponential moving average of the
working model. Our method provides consistent
performance gain over raw image and vanilla rep-
resentation rehearsal on several challenging CL
benchmarks, while being memory efﬁcient and
robust to natural and adversarial corruptions.1
1. Introduction
Computational systems operating in the real world are nor-
mally exposed to a sequence of multiple tasks with non-
stationary data streams. Similar to biological organisms, it
is desirable for these artiﬁcial systems to be able to learn
on a continual basis to successfully act and adapt to new
scenarios in the real world. However, deep neural networks
*Equal contribution1Advanced Research Lab, NavInfo Europe,
Netherlands2Dep. of Mathematics and Computer Science, Eind-
hoven University of Technology, Netherlands. Correspondence to:
<kishaan96@gmail.com >,<p.s.bhat@tue.nl, b.zonooz@tue.nl,
e.arani@tue.nl >.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).
1Code available at github.com/NeurAI-Lab/BiRT.
5 Tasks10 Tasks20 Tasks
Trade-of f
PGD Attack
FWTNat Robustness0102030405060DyTox
 BiRT
Figure 1. Overall performance of our proposed method, BiRT, vs.
DyTox trained continually on CIFAR-100 with 500 buffer size on
different metrics; Top-1 accuracy is reported for all metrics. There-
fore, a CL method with full coverage of the octagon has all the
ideal features: highest accuracy (on varying task sequences), natu-
ral/adversarial robustness, forward transfer, and stability-plasticity
trade-off.
(DNNs) are inherently designed for training on stationary,
independent, and identically distributed (i.i.d.) data. The
sequential nature of continual learning (CL) violates this
strong assumption, leading to catastrophic forgetting of
older tasks. Catastrophic forgetting often leads to a rapid de-
cline in the performance of old tasks and, in the worst case,
the previously acquired information is completely overwrit-
ten by the new one (Parisi et al., 2019).
Rehearsal-based approaches, which store and replay previ-
ous task samples, have been fairly successful in mitigating
catastrophic forgetting in CL. Recent evidence suggests that
replay might even be unavoidable in certain CL scenarios
(Farquhar and Gal, 2018). However, replaying raw pixels
from past experiences is not consistent with neurophysio-
logical mechanisms in the brain (Kudithipudi et al., 2022;
Hayes et al., 2019). Furthermore, the replay of raw pixels
is memory inefﬁcient and raises data privacy and security
concerns (Mai et al., 2022). Juxtaposing biological and
artiﬁcial experience rehearsal, representation rehearsal is a
lucrative alternative to address the problems associated with
raw image rehearsal in CL. Representation rehearsal, either
generative (van de Ven et al., 2020; Lao et al., 2020) or byarXiv:2305.04769v1  [cs.CV]  8 May 2023

--- PAGE 2 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Classifier
Episodic MemoryPatch
EmbeddingSelf-Attention Block
Input Encoder     
Input ImageWorking Model      
Classifier
Semantic Memory      
Figure 2. BiRT employs a bio-inspired non-veridical experience replay in a dual memory system based on vision transformers. The
semantic memory, fs, gradually assimilates learned knowledge from working model, fw, by taking an exponential moving average over
its weights. The semantic memory interacts with the episodic memory which stores the learned representations of the previous tasks ( r).
To effectively replay these abstract high-level representations, we inject constructive noise by mixing up representations ( ~M), adding
noise to the internal attention maps ( ~A), and emulating trial-to-trial variability through adding noise to the outputs of semantic memory
(~S) and to the targets ( ~T). To retrieve the knowledge, the consolidated knowledge from semantic memory is enforced to the working
model in the functional space via a consistency regularization.
storing (Hayes et al., 2020; Caccia et al., 2020; Iscen et al.,
2020), entails replaying the latent features of the intermedi-
ate layers of DNNs to mitigate catastrophic forgetting. In
generative methods, the generator itself is as large as the
CL model and is prone to catastrophic forgetting. Addi-
tionally, generative models are difﬁcult to train and suffer
mode collapse. However, although storing representations
is memory and computation efﬁcient, choosing an ideal
layer for rehearsal remains an open question. Furthermore,
stored representations in a bounded memory lack diversity,
resulting in overﬁtting.
In contrast, the human brain learns, stores, and remem-
bers experiences without catastrophically forgetting previ-
ous tasks. The versatility of the brain can be attributed to the
rehearsal of abstract experiences through multiple memory
systems (Hassabis et al., 2017) and a rich set of neuro-
physiological processing principles (Parisi et al., 2019). In
addition, the brain harbors random disturbances of signals,
termed noise, that contribute to cellular and behavioral trial-
to-trial variability (Faisal et al., 2008). Although noise is
sometimes considered a nuisance, noise forms a notable
component of the computational strategy of the brain. The
brain exploits noise to perform tasks, such as probabilis-
tic inference through sampling, that facilitate learning and
adaptation in dynamic environments (Maass, 2014). As is
the case in the brain, we hypothesize that noise can be a
valuable tool in improving generalization in representation
rehearsal in vision transformers.
To this end, we propose BiRT, a novel representation
rehearsal-based continual learning method based on vision
transformers, architectures composed of self-attention mod-ules inspired by human visual attention (Lindsay, 2020).
Speciﬁcally, our method consists of two complementary
learning systems: a working model and semantic mem-
ory, an exponential moving average of the working model.
To reduce overﬁtting and bring diversity in representation
rehearsal, BiRT introduces various controllable noises at
various stages of the vision transformer and enforces con-
sistency in predictions with respect to semantic memory.
As semantic memory consolidates semantic information,
consistency regularization in the presence of meaningful
noise promotes generalization while effectively reducing
overﬁtting. BiRT provides a consistent performance gain
over the raw image and the vanilla representation rehearsal
on several CL scenarios and metrics while being robust to
natural and adversarial corruptions (Figure 1).
2. Related Work
Continual Learning : DNNs are typically designed to in-
crementally adapt to stationary i.i.d. data streams shown in
isolation and random order (Parisi et al., 2019). Therefore,
sequential learning over non-i.i.d. data causes catastrophic
forgetting of previous tasks and overﬁtting of the current
task. Approaches to address catastrophic forgetting can be
broadly divided into three categories: regularization-based
approaches (Kirkpatrick et al., 2017; Zenke et al., 2017;
Li and Hoiem, 2017) penalize changes in important pa-
rameters pertaining to previous tasks, parameter isolation
methods (Rusu et al., 2016; Aljundi et al., 2017; Fernando
et al., 2017) allocate a distinct set of parameters for distinct
tasks, and rehearsal-based approaches (Ratcliff, 1990; Re-
bufﬁ et al., 2017; Lopez-Paz and Ranzato, 2017; Bhat et al.,

--- PAGE 3 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
2023) store old task samples and replay them alongside
current task samples. Among different approaches to mit-
igate catastrophic forgetting, experience rehearsal is fairly
successful in multiple CL scenarios (Parisi et al., 2019).
Rehearsal-based approaches replay raw pixels from past ex-
periences, inconsistent with how humans continually learn
(Kudithipudi et al., 2022). Furthermore, the replay of raw
pixels can have other ramiﬁcations, including a large mem-
ory footprint, data privacy, and security concerns (Mai et al.,
2022). Therefore, several works (Pellegrini et al., 2020;
Iscen et al., 2020; Caccia et al., 2020) mimic abstract rep-
resentation rehearsal in the brain by storing and replaying
representations from intermediate layers in DNNs. Repre-
sentation rehearsal can be done by employing generative
models (van de Ven et al., 2020; Lao et al., 2020) or by
storing previous task representations in the buffer (Hayes
et al., 2020; Iscen et al., 2020). While generative mod-
els themselves are prone to forgetting and mode collapse,
storing representations in a bounded memory buffer lacks
diversity due to the unavailability of proper augmentation
mechanisms. Although high-level representation replay can
potentially mitigate memory overhead and privacy concerns,
replaying representations over and over again leads to over-
ﬁtting.
Transformers for CL: Transformer architectures (Vaswani
et al., 2017) were ﬁrst developed for machine translation and
later expanded to computer vision tasks (Dosovitskiy et al.,
2020; Touvron et al., 2021; Jeeveswaran. et al., 2022) by
considering image patches as replacements for tokens. De-
spite their success in several benchmarks, vision transform-
ers have not been widely considered for continual learning.
Yu et al. (2021) studied transformers in a class-incremental
learning setting and pointed out several problems in naively
applying transformers in CL. DyTox (Douillard et al., 2021)
proposed a dynamically expanding architecture using sepa-
rate task tokens to model the context of different classes in
CL. LVT (Wang et al., 2022a) proposed an external key and
an attention bias to stabilize the attention map between tasks
and used a dual classiﬁer structure to avoid catastrophic
interference while learning new tasks. Pelosin et al. (2022)
proposed an asymmetric regularization loss on pooled atten-
tion maps with respect to the model learned on the previous
task to continually learn in an exemplar-free approach. Sev-
eral other concurrent works (Ermis et al., 2022; Wang et al.,
2022c;b) harnessed the pre-trained model and incorporated
the learning of generic and task-speciﬁc parameters. Unlike
these works, we do not use pre-trained models and replay
intermediate representations instead of raw image inputs.
We seek to improve the performance of vision transformers
under representation rehearsal in CL. As noise plays a con-
structive role in the brain, we mimic the prevalence of noise
in the brain and the consequent trial-to-trial variability byinjecting noise into our proposed method.
3. Proposed Method
The CL paradigm normally consists of Tsequential tasks,
with the data gradually becoming available over time. Dur-
ing each task t2f1;2;::;Tg, the samples and the corre-
sponding labels (xi;yi)N
i=1are drawn from the task-speciﬁc
distributionDt. The continual learning model fis opti-
mized sequentially on one task at a time, and inference is
carried out on all the tasks seen so far. CL is especially
challenging for vision transformers due to the limited train-
ing data for every task (Raghu et al., 2021; Touvron et al.,
2021) in addition to the issue of catastrophic forgetting. By
mimicking the association of past and present experiences
in the brain, experience rehearsal (ER) partially addresses
the problem of catastrophic forgetting. Thus, the learning
objective of ER is as follows:
Ler, E
(xi;yi)Dt[Lce(f(xi);yi) ]
+ E
(xj;yj)Dm[Lce(f(xj);yj) ];(1)
whererepresents a balancing parameter, Dmis episodic
memory, andLceis cross-entropy loss. To further reduce
catastrophic forgetting, we employ a complementary learn-
ing system based on abstract, high-level representation re-
hearsal. To promote diversity and generalization in repre-
sentation rehearsal, we introduce various controllable noises
at different stages of the vision transformer and enforce con-
sistency in predictions with respect to the semantic memory.
In the following sections, we describe in detail different
components of BiRT.
3.1. Knowledge Consolidation through complementary
learning system
Complementary learning system (CLS) theory posits that
the hippocampus and neocortex entail complementary prop-
erties necessary to capture complex interactions in the brain
(McNaughton and O’Reilly, 1995). Inspired by CLS the-
ory, we propose a dual memory transformer-based learning
system that acquires and assimilates knowledge over short
and long periods of time. The working model encounters
new tasks and consolidates knowledge over short periods
of time. We then gradually aggregate the weights of the
working model into semantic memory during intermittent
stages of inactivity. Following Arani et al. (2021), we design
the semantic memory as an exponential moving average of
the working model as follows:
s=s+ (1 )w (2)
wherewandsare the weights of the working model and
semantic memory, respectively, and is a decay parameter.

--- PAGE 4 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
As the working model focuses on specializing on the current
task, the copy of the working model at each training step
can be considered as an expert on a particular task. There-
fore, the aggregation of weights throughout CL training can
be deemed as an ensemble of expert models that consoli-
date knowledge across tasks, resulting in smoother decision
boundaries.
3.2. Episodic Memory
In line with experience rehearsal in the brain (Ji and Wilson,
2007), we propose an abstract, high-level representation
rehearsal for vision transformers. The working model com-
prises two nested functions: g(:)andfw(:). The ﬁrst few
layers of the encoder, g(:), process the raw image input,
and the output along with the ground truth label is stored in
episodic memoryDm. To ensure consistency in intermedi-
ate representations, g(:)can be initialized using pre-trained
weights and ﬁxed before starting CL training or ﬁxed after
learning some tasks. On the other hand, fw(:), the later
layers of the transformer, process abstract high-level repre-
sentations, and remain learnable throughout the CL training.
During intermittent stages of inactivity, the stable counter-
part semantic memory fs(:)is updated according to Eq.
2.
The episodic memory is populated at the task boundary us-
ing iCaRL herding (Rebufﬁ et al., 2017). Representations
rj=g(xj), stored in episodic memory, are interleaved
with current task representations and are processed syn-
chronously by fw(:)andfs(:). The learning objective for
representation rehearsal can thus be obtained by adapting
Eq. 1 as follows:
Lrepr, E
(xi;yi)Dt[Lce(fw(g(xi));yi) ]
+ E
(rj;yj)Dm[Lce(fw(rj);yj) ](3)
3.3. Noise and Trial-to-Trial Variability
Noise is prevalent at every level of the nervous system and
has recently been shown to play a constructive role in the
brain (Faisal et al., 2008; McDonnell and Ward, 2011). Trial-
to-trial variability, a common phenomenon in biological
systems in which the neural response to the same stimuli
differs across trials, is often the result of noise (Faisal et al.,
2008). Trial-to-trial variability has been shown to be one of
the key components of the computational mechanism in the
brain (Maass, 2014). Furthermore, injecting noise into the
neural network learning pipeline has been shown to result
in faster convergence to the global optimum (Zhou et al.,
2019), better generalization (Srivastava et al., 2014), and
effective knowledge distillation.
To simulate noise and trial-to-trial variability, we stochas-Algorithm 1 BiRT Algorithm
input: Data streamsDt, bufferDm, working model fw,
hyperparameters ,t,m,a,s
for all taskst2f1;2;::;Tgdo
forepochse2f1;2;::;Egdo
sample a mini-batch (x;y)Dt
x=augment (x)
ifDm6=;then
sample a mini-batch (r;y)Dm
a;b;c;d;eU(0;1)
~y ~T(y) ifa<t
(~r;~y) ~M(r;y) ifb<m I(Eq. 4)
~A ~A(A) ifc<a I(Eq. 5)
fs(r) ~S(fs(r);)ifd<s I(Eq. 7)
end if
Compute outputs of fw(:)andfs(:)
ComputeL=Lrepr+Lcr I(Eqs. 3, 6, 8)
w w+rwL
s s+ (1 )wife<eandt>1
end for
iftask-end = True then
ift = 1 then
Freezeg(:)
s=copy(w)
end if
Dm (r;y)
end if
end for
Return: working model w, and semantic memory s
tically inject constructive noise into various components
of our CL setup. In the following sections, we describe in
detail how exactly we leverage noise during CL training.
3.3.1. R EPRESENTATION NOISE ~M
During CL training, the working model encounters task-
speciﬁc dataDtthat are ﬁrst fed into g(:), and then the
output representations of g(:)are interleaved with the repre-
sentations of previous task samples from episodic memory
Dm. We updateDmat the task boundary using iCaRL
herding. The interleaved representations are then processed
by bothfw(:)andfs(:). Analogous to the replay of novel
samples in the brain (Liu et al., 2019), we linearly com-
bine representations sampled from episodic memory using
a manifold mixup (Verma et al., 2019):
~r=ri+ (1 )rj
~y=yi+ (1 )yj;(4)
whereri;rjare stored representations of two different sam-
ples andyi;yjare the corresponding labels. Here, the mix-
ing coefﬁcient is drawn from a Beta distribution. As
manifold mixup interpolates representations of samples be-

--- PAGE 5 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Table 1. Results on multiple datasets learned with 10 tasks with varying buffer sizes, averaged over multiple class orders. BiRT achieves
consistent improvements over DyTox in different metrics, i.e. accuracy, forgetting, BWT, and FWT. The last accuracy determines the
performance on past tasks after learning the last task, and the average accuracy shows the average of the last accuracy after learning every
task.
BUFFER SIZE 500 1000 2000
JOINT DYTOX BIRT D YTOX BIRT D YTOX BIRT
CIFAR-100LAST ACC" 74.99 0.22 34.54 1.82 50.20 0.67 43.92 0.84 51.20 1.46 52.34 0.46 53.01 0.57
AVGACC" 58.35 1.54 63.82 1.80 63.67 1.31 64.56 2.31 68.42 1.13 66.70 0.36
BWT" -39.79 1.16 -15.62 0.29 -32.05 0.33 -15.25 0.66 -24.44 0.65 -16.30 1.31
FWT" 41.51 1.61 56.14 1.52 50.04 1.17 57.04 2.2 57.77 0.77 59.74 1.30
FORGETTING# 53.87 1.95 17.45 0.61 43.64 0.71 17.70 1.42 33.92 0.79 19.00 1.98
TINYIMAGE NETLAST ACC" 58.46 0.60 23.95 0.71 32.60 .018 33.25 1.28 38.41 0.33 37.34 0.22 40.49 0.52
AVGACC" 42.53 1.74 44.57 2.84 48.74 1.29 49.26 2.34 51.30 2.17 51.15 0.34
BWT" -40.46 0.41 -13.38 0.98 -31.12 1.19 -17.34 0.51 -27.68 0.77 -17.85 0.37
FWT" 27.84 1.02 37.87 1.91 36.60 0.34 41.97 1.54 40.39 1.16 43.93 1.54
FORGETTING# 52.32 0.94 14.57 2.00 40.07 2.12 18.85 0.22 35.56 1.29 19.48 0.21
IMAGE NET-100LAST ACC" 79.05 0.16 39.03 1.57 51.05 0.24 50.62 1.04 52.89 0.96 58.54 0.42 59.52 1.39
AVGACC" 60.52 1.56 65.51 0.30 68.14 1.38 67.33 0.57 71.67 1.71 70.51 1.87
BWT" -38.15 0.48 -14.42 0.06 -26.87 0.72 -12.90 0.31 -21.10 0.78 -16.53 0.84
FWT" 44.94 1.69 58.27 0.30 56.86 1.46 60.78 0.86 62.85 1.54 63.40 2.01
FORGETTING# 51.71 0.91 16.10 0.42 37.93 11.23 14.83 0.67 28.68 1.41 19.79 0.61
longing to different classes / tasks, it brings diversity for the
experience-rehearsal, thereby reducing overﬁtting.
3.3.2. A TTENTION NOISE ~A
As we employ vision transformer as our architecture of
choice, self-attention forms the core component of BiRT.
The working model fw(:)in BiRT consists of several multi-
head self-attention layers that map a query and a set of
key-value pairs to an output. We inject noise into the scaled
dot-product attention at each layer of fw(:)while replaying
the representation as follows:
Attention(Q;K;V ) = (softmaxQKT
pdk
+)V(5)
whereQ,KandVare query, key and value matrices, and
N(0;2)is a white Gaussian noise. By stochastically
injecting noise into self-attention, we discourage BiRT from
attending to sample speciﬁc features, thereby potentially
mitigating overﬁtting.
3.3.3. S UPERVISION NOISE ~TAND ~S
We now shift our focus toward the supervision signals to fur-
ther reduce overﬁtting in CL. Due to over-parameterization,
the CL model tends to overﬁt on the limited number of sam-
ples from the buffer. Therefore, we introduce a synthetic
label noise ( ~T) wherein a small percentage of the samples
are re-assigned a random class. BiRT takes advantage of the
fact that label noise is sparse, meaning that only a fraction
of the labels are corrupted while the rest are intact in the
real world (Liu et al., 2022). In addition, the harmful effects
of inherent label noise on generalization can be mitigated by
using additional controllable label noise (Chen et al., 2021).During intermittent stages of inactivity, the knowledge in
the working model is consolidated into semantic memory
through Eq. 2. Therefore, knowledge of previous tasks is
encoded in semantic memory weights during the learning
trajectory of the working model (Hinton et al., 2015). Then,
to retrieve the structural knowledge encoded in the semantic
memory, we regularize the function learned by the work-
ing model by enforcing consistency in its predictions with
respect to the semantic memory:
Lcr,1E
xiDtkfw(g(xi)) fs(g(xi))kp
+2E
rjDmkfw(rj) fs(rj)kp;(6)
where1and2are balancing weights. To mimic trial-to-
trial variability in the brain, we inject noise into the logits
of semantic memory ( ~S) before applying consistency regu-
larization as follows:
fs(rj) fs(rj) + (7)
whereN (0;2)is a white Gaussian noise, Lcrrep-
resents the expected Minkowski distance between the cor-
responding pairs of predictions and p= 2. Consistency
regularization enables the working model to retrieve struc-
tural knowledge from the semantic memory from previous
tasks. Consequently, the working model adapts the decision
boundary to new tasks without catastrophically forgetting
previous tasks.
Thus, the ﬁnal learning objective for the working model is
as follows:
L,Lrepr+Lcr (8)
whereis a balancing parameter. Our proposed approach is
illustrated in Figure 2 and is detailed in Algorithm 1.

--- PAGE 6 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Note that these noises are applied stochastically, and there-
fore, a single representation can have multiple noises asso-
ciated with it. Although noise is generally treated as a nui-
sance, BiRT introduces controllable noise at various stages
of the vision transformer to promote robust generalization
in CL.
4. Experimental Results
We use the continuum library (Douillard and Lesort, 2021)
to implement different CL scenarios and build our approach
on top of DyTox (Douillard et al., 2021) method, the main
baseline in all our experiments. We report the last accuracy
(Last), average accuracy (Avg), forward transfer (FWT),
backward transfer (BWT) and forgetting. More information
on experimental setup, datasets, and metrics can be found
in Appendix A.
Table 1 presents the comparison of our method with stan-
dard CL benchmarks with different buffer sizes, averaged
across three random seeds. We can make the following ob-
servations from Table 1: (i) Across CL settings and different
buffer sizes, BiRT shows consistent performance improve-
ment over DyTox across all metrics. (ii) BiRT enables the
consolidation of rich information about the previous tasks
better even under low buffer regimes, e.g. for CIFAR-100,
the absolute improvement in terms of Last Acc is 7:28% for
buffer size 1000 while it is as much as 15:66% for buffer
size 500. (iii) BWT and FWT elucidate the inﬂuence of
learning a new task ton the performance of previous and
subsequent tasks, respectively. BiRT shows a smaller nega-
tive BWT and a higher positive FWT across all CL datasets,
resulting in less forgetting and better forward facilitation.
(iv) TinyImageNet is one of the challenging datasets for
CL considered in this work. Under low buffer regimes, the
number of samples per class will be severely limited due
to the large number of classes per task. BiRT consistently
outperforms DyTox across all buffer sizes on TinyImageNet.
Table 2 further demonstrates the comparison of our
method with transformer-based exemplar-free (ATT-asym
and FUNC-asym (Pelosin et al., 2022); averaged over 3
seeds) and rehearsal-based (DyToX and LVT; averaged
over 5 class orderings) approaches. Although originally
not designed for the exemplar-free scenario, BiRT shows
a signiﬁcant improvement over the rehearsal-free methods.
Progressing from the exemplar-free scenario, BiRT shows
a further improvement in performance when provided with
experience rehearsal. We also compare CL methods with
different numbers of tasks in CIFAR-100 with limited buffer
sizes. BiRT consolidates generalizable features rather than
discriminative features speciﬁc to buffered samples, thereby
exhibiting superior performance across all buffer sizes and
task sequences.Reinforcing our earlier hypothesis, the controllable noises
introduced in BiRT play a constructive role in promoting
generalization and consequently reducing overﬁtting in CL.
In addition to allaying privacy concerns, replacing raw im-
age rehearsal with representation rehearsal reduces the mem-
ory footprint without compromising performance.
5. Model Analysis
Task Recency Bias: Sequential learning of multiple tasks
causes classiﬁer predictions to tilt toward recent tasks, re-
sulting in a task recency bias (Masana et al., 2020). One
direct consequence of task recency bias is that the classi-
ﬁer norm is higher for recent classes while lower for older
classes, which means that older classes are less likely to
be picked for prediction (Hou et al., 2019). Following the
analysis in (Bhat et al., 2022), Figure 4 (right) shows the
normalized probability that all classes in each task are pre-
dicted at the end of training. The probabilities in BiRT are
more evenly distributed than in DyTox, resulting in a lower
recency bias. We argue that supervision noises proposed
in BiRT implicitly regularize the classiﬁer towards more
evenly distributed prediction probabilities.
Stability-Plasticity Dilemma: The extent to which the CL
model is plastic enough to acquire new information while
stable enough not to catastrophically interfere with con-
solidated knowledge is referred to as stability-plasticity
dilemma (Parisi et al., 2019). Catastrophic forgetting is
a direct consequence of this dilemma when the plasticity
of the CL model overtakes its stability. To investigate how
well our method handles the stability-plasticity dilemma,
we plot the task-wise performance at the end of each task in
Figure 3 for the CIFAR-100 test set. Following Sarfraz et al.
(2022), we also visualize a formal trade-off measure in Fig-
ure 4 (left). Both the working model and semantic memory
exhibit higher stability, while DyTox is more plastic. There-
fore, DyTox is more prone to forgetting, whereas BiRT
displays a better stability-plasticity trade-off compared to
the baseline.
Attention Map Analysis: As learning progresses through
a sequence of tasks, a CL model that retains its focus on
salient regions undergoes less catastrophic forgetting. There-
fore, it would be beneﬁcial to study the variation in the
salient regions of the image during the learning trajectory.
Figure 6 shows a comparison of saliency maps for samples
of the ﬁrst task after training on the ﬁrst and last task, re-
spectively. As can be seen, BiRT retains the attention to
important regions in these images better than DyTox. We
contend that the attention noise proposed in BiRT helps
focus on class-wide features rather than sample speciﬁc fea-
tures, thereby retaining attention to important regions in test
images. More explanation and extended visualizations are
provided in Appendix M.

--- PAGE 7 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Table 2. Results on CIFAR-100 learned with 5, 10, and 20 tasks with varying buffer sizes. BiRT achieves consistent improvements over
the state-of-the-art on average accuracy and last accuracy.
METHODSBUFFER
SIZE#P5STEPS 10STEPS 20STEPS
AVG LAST AVG LAST AVG LAST
ATT- ASYM - 16.87 - - 25.58 0.01 16.31 0.00 - -
FUNC- ASYM - 16.87 - - 25.95 0.00 16.21 0.01 - -
BIRT - 10.73 - - 56.40 1.57 42.59 0.84 - -
DYTOX 10.73 56.98 0.61 41.50 1.00 48.31 1.23 23.92 1.11 38.10 1.72 14.27 0.94
LVT 200 8.9 - 39.68 1.36 - 35.41 1.28 - 20.63 1.14
BIRT 10.73 67.15 0.95 54.15 0.94 61.01 1.58 45.59 1.54 48.03 0.97 29.10 1.88
DYTOX 10.73 63.85 0.99 52.99 0.53 58.35 1.54 34.54 1.82 49.98 1.32 24.86 0.81
LVT 500 8.9 - 44.73 1.19 - 43.51 1.06 - 26.75 1.29
BIRT 10.73 68.40 1.56 55.65 0.99 63.82 1.80 50.20 0.67 50.34 1.64 30.22 1.63
T1 T2 T3 T4 T5 T6 T7 T8 T9T10After T1
After T2
After T3
After T4
After T5
After T6
After T7
After T8
After T9
After T1092.0
79.1 85.7
70.3 68.1 83.7
59.6 62.7 59.1 82.2
47.0 52.6 42.8 64.0 85.4
46.2 42.4 29.7 43.9 74.2 90.7
42.3 34.1 26.4 37.5 62.6 74.9 79.0
37.7 28.4 19.3 29.9 39.2 61.9 51.7 89.4
33.4 22.0 16.0 26.4 29.0 46.4 38.6 63.6 87.5
27.6 18.6 7.520.0 23.6 38.2 26.0 42.4 63.6 84.5DyT ox
T1 T2 T3 T4 T5 T6 T7 T8 T9T1092.5
80.9 80.6
74.0 69.9 70.3
70.9 65.4 60.0 65.8
67.2 63.3 54.2 59.8 66.4
63.4 56.2 51.2 55.1 62.5 73.2
64.4 52.6 47.4 53.1 61.5 67.2 50.9
62.3 50.1 44.2 48.5 56.9 66.2 47.7 59.3
60.5 46.6 43.5 48.4 52.0 66.0 45.4 54.6 56.8
57.8 46.0 42.6 46.5 50.9 62.3 43.9 52.0 52.7 48.3BiRT (Working Model)
T1 T2 T3 T4 T5 T6 T7 T8 T9T1086.9
80.9 79.5
75.3 70.5 70.0
70.5 65.6 59.1 64.2
67.0 62.7 55.2 59.7 65.5
64.2 56.5 51.7 54.4 61.7 73.5
64.2 51.3 47.7 53.9 61.5 68.2 52.1
62.3 50.5 44.8 50.1 55.9 67.1 47.8 59.1
60.6 47.9 44.4 47.7 52.3 64.7 44.4 54.7 57.0
57.8 45.5 42.0 46.6 50.9 61.6 43.9 53.2 51.8 48.2BiRT (Semantic Memory)
Figure 3. Comparison of task-wise performance after learning each task on CIFAR-100 with a buffer size of 500 learned for 10 tasks. The
working model achieves better accuracy for the seen tasks after learning 10 tasks compared to DyTox. The semantic memory retains the
performance of older tasks better than DyTox and the working model.
Robustness Analysis: Continual learning models are
mostly evaluated on accuracy on seen tasks and forgetting
metrics. However, the research community has largely ne-
glected the susceptibility of continually learned models to
adversarial attacks and corrupted data in the wild (Khan
et al., 2022). Figure 5 illustrates the robustness of BiRT on
adversarial attack of varying strengths (Kim, 2020) and sev-
eral natural corruptions (Hendrycks and Dietterich, 2019).
In addition, we evaluate the robustness of BiRT without
any noise in the learning trajectory in order to elucidate the
beneﬁts of constructively inducing noise in the pipeline of
continually learning models. BiRT is robust to adversarial
attacks, as well as corrupted data, and learning with noise
results in improved robustness. This is evident from the
performance under severe noises such as ‘contrast’ ,‘fog’ ,
‘motion blur ’ and the average performance across different
settings wherein learning with noise helps the model recover
from the inferior performance.
This makes it well-suited for safety-critical applications,
such as autonomous vehicles, where the consequences of amodel failure can be severe.
6. Ablation Study
Table 3 provides an overview of the effect of the different
components used in BiRT. Unlike DyTox, we employ an
exponential moving average as semantic memory, resulting
in the biggest jump in accuracy. BiRT entails representa-
tion, attention, and supervision noises to promote robust
generalization in CL and diversify the buffered representa-
tions. As can be seen, all three components of BiRT play a
constructive role in building a successful continual learner.
Supervision noise, representation noise, and attention noise
bring performance improvements of 0.54%, 3.30%, and
3.41%, respectively, over BiRT without any noise. In addi-
tion, compared to vanilla representation rehearsal, the right
combination of controllable noises in BiRT greatly reduces
overﬁtting and improves performance by as much as 9%
(relative Avg). Therefore, it is quintessential to have con-
trollable noise to further improve representation rehearsal
in CL.

--- PAGE 8 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
DyT ox BiRT304050607080 Accuracy (%)Plasticity
Stability
Trade-off
DyT ox BiRT0.00.10.20.30.40.5Accuracy (%)T ask 1
T ask 10
Figure 4. Comparison of CL methods in the stability-plasticity trade-off (left) and the task-recency bias (right) on C-100 (buffer size 500).
Figure 5. Robustness of CL methods to adversarial attacks (left) and 19 different natural corruptions (right) on C-100 (buffer size 500).
DyT oxT ask 1
 T ask 10
 T ask 1
 T ask 10
BiRT
Figure 6. Comparison of attention maps on the validation set of the
ﬁrst task of ImageNet-100 trained for 10 tasks with buffer size 500
(red regions correspond to regions with higher attention). BiRT
retains the knowledge of salient regions in the image better than
DyTox, leading to better predictions and less forgetting.
7. Conclusions and Future Work
We proposed BiRT, a novel representation rehearsal-based
continual learning approach based on vision transformers.
Speciﬁcally, we introduce controllable noises at various
stages of the vision transformer and enforce consistency
in predictions with respect to an exponential moving aver-
age of the working model. Our empirical results show thatTable 3. Ablations of the different key components of BiRT. The
average and last accuracies are reported on CIFAR100 for the
buffer size of 500 learned for 10 tasks.
SUPERVISION
NOISEREPRES .
NOISEATTENTION
NOISELAST
ACCAVG
ACC
3 3 3 50.20 0.67 63.82 1.80
7 3 3 49.63 0.30 63.67 1.55
7 7 3 49.30 0.91 63.29 1.71
3 7 7 49.19 0.46 62.58 1.44
7 3 7 46.43 0.41 61.83 0.23
7 7 7 45.89 1.25 59.58 0.58
DYTOX 34.54 1.82 58.35 1.54
BiRT outperforms raw image rehearsal and vanilla repre-
sentation rehearsal while being memory efﬁcient and robust
to natural and adversarial corruptions. Furthermore, the
improvement is even more pronounced under low buffer
regimes and longer task sequences. Reinforcing our earlier
hypothesis, the controllable noises introduced in BiRT play
a constructive role in promoting generalization and conse-
quently reducing overﬁtting in CL. Extending our work to
more realistic settings such as general CL where task bound-
aries are not known at training time, and exploring other
efﬁcient transformer architectures are some of the useful
research directions for this work.

--- PAGE 9 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
References
Aljundi, R., Chakravarty, P., and Tuytelaars, T. (2017). Ex-
pert gate: Lifelong learning with a network of experts. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 3366–3375.
Arani, E., Sarfraz, F., and Zonooz, B. (2021). Learning fast,
learning slow: A general continual learning method based
on complementary learning system. In International Con-
ference on Learning Representations .
Bhat, P. S., Zonooz, B., and Arani, E. (2022). Consistency
is the key to further mitigating catastrophic forgetting in
continual learning. In Conference on Lifelong Learning
Agents , pages 1195–1212. PMLR.
Bhat, P. S., Zonooz, B., and Arani, E. (2023). Task-aware
information routing from common representation space
in lifelong learning. In The Eleventh International Con-
ference on Learning Representations .
Caccia, L., Belilovsky, E., Caccia, M., and Pineau, J. (2020).
Online learned continual compression with adaptive quan-
tization modules. In International conference on machine
learning , pages 1240–1250. PMLR.
Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H.
(2018). Riemannian walk for incremental learning: Un-
derstanding forgetting and intransigence. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 532–547.
Chen, P., Chen, G., Ye, J., Heng, P.-A., et al. (2021). Noise
against noise: stochastic label noise helps combat inher-
ent label noise. In International Conference on Learning
Representations .
d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S.,
Biroli, G., and Sagun, L. (2021). Convit: Improving vi-
sion transformers with soft convolutional inductive biases.
CoRR , abs/2103.10697.
De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X.,
Leonardis, A., Slabaugh, G., and Tuytelaars, T. (2021).
A continual learning survey: Defying forgetting in classi-
ﬁcation tasks. IEEE transactions on pattern analysis and
machine intelligence , 44(7):3366–3385.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. (2009). Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision
and pattern recognition , pages 248–255. Ieee.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M., Heigold, G., Gelly, S., et al. (2020). An image is
worth 16x16 words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929 .Douillard, A. and Lesort, T. (2021). Continuum: Simple
management of complex continual learning scenarios.
Douillard, A., Ram ´e, A., Couairon, G., and Cord, M. (2021).
Dytox: Transformers for continual learning with dynamic
token expansion. arXiv preprint arXiv:2111.11326 .
Ebrahimi, S., Petryk, S., Gokul, A., Gan, W., Gonzalez,
J. E., Rohrbach, M., and Darrell, T. (2021). Remembering
for the right reasons: Explanations reduce catastrophic
forgetting. Applied AI Letters , 2(4):e44.
Ermis, B., Zappella, G., Wistuba, M., Rawal, A., and Ar-
chambeau, C. (2022). Continual learning with trans-
formers for image classiﬁcation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3774–3781.
Faisal, A. A., Selen, L. P., and Wolpert, D. M. (2008). Noise
in the nervous system. Nature reviews neuroscience ,
9(4):292–303.
Farquhar, S. and Gal, Y . (2018). Towards robust evaluations
of continual learning. arXiv preprint arXiv:1805.09733 .
Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha,
D., Rusu, A. A., Pritzel, A., and Wierstra, D. (2017).
Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734 .
Gao, Q., Zhao, C., Ghanem, B., and Zhang, J. (2022). R-
dfcil: Relation-guided representation learning for data-
free class incremental learning. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XXIII , pages
423–439. Springer.
Hassabis, D., Kumaran, D., Summerﬁeld, C., and Botvinick,
M. (2017). Neuroscience-inspired artiﬁcial intelligence.
Neuron , 95(2):245–258.
Hayes, T. L., Cahill, N. D., and Kanan, C. (2019). Memory
efﬁcient experience replay for streaming learning. In 2019
International Conference on Robotics and Automation
(ICRA) , pages 9769–9776. IEEE.
Hayes, T. L., Kaﬂe, K., Shrestha, R., Acharya, M., and
Kanan, C. (2020). Remind your neural network to pre-
vent catastrophic forgetting. In European Conference on
Computer Vision , pages 466–483. Springer.
Hendrycks, D. and Dietterich, T. (2019). Benchmarking
neural network robustness to common corruptions and
perturbations. arXiv preprint arXiv:1903.12261 .
Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the
knowledge in a neural network.

--- PAGE 10 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Hou, S., Pan, X., Loy, C. C., Wang, Z., and Lin, D. (2019).
Learning a uniﬁed classiﬁer incrementally via rebalanc-
ing. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 831–
839.
Iscen, A., Zhang, J., Lazebnik, S., and Schmid, C. (2020).
Memory-efﬁcient incremental learning through feature
adaptation. In European conference on computer vision ,
pages 699–715. Springer.
Jeeveswaran., K., Kathiresan., S., Varma., A., Magdy., O.,
Zonooz., B., and Arani., E. (2022). A comprehensive
study of vision transformers on dense prediction tasks. In
Proceedings of the 17th International Joint Conference
on Computer Vision, Imaging and Computer Graphics
Theory and Applications - Volume 4: VISAPP , , pages
213–223. INSTICC, SciTePress.
Ji, D. and Wilson, M. A. (2007). Coordinated memory
replay in the visual cortex and hippocampus during sleep.
Nature neuroscience , 10(1):100–107.
Kemker, R. and Kanan, C. (2017). Fearnet: Brain-
inspired model for incremental learning. arXiv preprint
arXiv:1711.10563 .
Khan, H., Shah, P. M., Zaidi, S. F. A., et al. (2022). Suscep-
tibility of continual learning against adversarial attacks.
arXiv preprint arXiv:2207.05225 .
Kim, H. (2020). Torchattacks: A pytorch repository for
adversarial attacks. arXiv preprint arXiv:2010.01950 .
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des-
jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho,
T., Grabska-Barwinska, A., et al. (2017). Overcoming
catastrophic forgetting in neural networks. Proceedings
of the national academy of sciences , 114(13):3521–3526.
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple
layers of features from tiny images.
Kudithipudi, D., Aguilar-Simon, M., Babb, J., Bazhenov,
M., Blackiston, D., Bongard, J., Brna, A. P.,
Chakravarthi Raja, S., Cheney, N., Clune, J., et al. (2022).
Biological underpinnings for lifelong learning machines.
Nature Machine Intelligence , 4(3):196–210.
Kumaran, D., Hassabis, D., and McClelland, J. L. (2016).
What learning systems do intelligent agents need? com-
plementary learning systems theory updated. Trends in
cognitive sciences , 20(7):512–534.
Lao, Q., Jiang, X., Havaei, M., and Bengio, Y . (2020).
Continuous domain adaptation with variational domain-
agnostic feature replay. arXiv preprint arXiv:2003.04382 .Le, Y . and Yang, X. (2015). Tiny imagenet visual recogni-
tion challenge. CS 231N , 7(7):3.
Li, Z. and Hoiem, D. (2017). Learning without forgetting.
IEEE transactions on pattern analysis and machine intel-
ligence , 40(12):2935–2947.
Lindsay, G. W. (2020). Attention in psychology, neuro-
science, and machine learning. Frontiers in computa-
tional neuroscience , page 29.
Liu, S., Zhu, Z., Qu, Q., and You, C. (2022). Robust training
under label noise by over-parameterization. In Interna-
tional Conference on Machine Learning , pages 14153–
14172. PMLR.
Liu, Y ., Dolan, R. J., Kurth-Nelson, Z., and Behrens, T. E.
(2019). Human replay spontaneously reorganizes experi-
ence. Cell, 178(3):640–652.
Lopez-Paz, D. and Ranzato, M. (2017). Gradient episodic
memory for continual learning. Advances in neural infor-
mation processing systems , 30.
Maass, W. (2014). Noise as a resource for computation and
learning in networks of spiking neurons. Proceedings of
the IEEE , 102(5):860–880.
Mai, Z., Li, R., Jeong, J., Quispe, D., Kim, H., and Sanner, S.
(2022). Online continual learning in image classiﬁcation:
An empirical survey. Neurocomputing , 469:28–51.
Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov,
A. D., and van de Weijer, J. (2020). Class-incremental
learning: survey and performance evaluation on image
classiﬁcation. arXiv preprint arXiv:2010.15277 .
McDonnell, M. D. and Ward, L. M. (2011). The beneﬁts of
noise in neural systems: bridging theory and experiment.
Nature Reviews Neuroscience , 12(7):415–425.
McNaughton, B. L. and O’Reilly, R. C. (1995). Why there
are complementary learning systems in the hippocampus
and neocortex: Insights from the successes and failures
of.Psychological Review , 102(3):419–457.
Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter,
S. (2019). Continual lifelong learning with neural net-
works: A review. Neural Networks , 113:54–71.
Pellegrini, L., Grafﬁeti, G., Lomonaco, V ., and Maltoni, D.
(2020). Latent replay for real-time continual learning. In
2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) , pages 10203–10209. IEEE.
Pelosin, F., Jha, S., Torsello, A., Raducanu, B., and van de
Weijer, J. (2022). Towards exemplar-free continual learn-
ing in vision transformers: an account of attention, func-
tional and weight regularization. In Proceedings of the

--- PAGE 11 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3820–3829.
Peng, J., Tang, B., Jiang, H., Li, Z., Lei, Y ., Lin, T., and
Li, H. (2021). Overcoming long-term catastrophic for-
getting through adversarial neural pruning and synaptic
consolidation. IEEE Transactions on Neural Networks
and Learning Systems .
Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and
Dosovitskiy, A. (2021). Do vision transformers see like
convolutional neural networks? Advances in Neural
Information Processing Systems , 34.
Ratcliff, R. (1990). Connectionist models of recognition
memory: constraints imposed by learning and forgetting
functions. Psychological review , 97(2):285.
Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H.
(2017). icarl: Incremental classiﬁer and representation
learning. In Proceedings of the IEEE conference on Com-
puter Vision and Pattern Recognition , pages 2001–2010.
Robins, A. (1995). Catastrophic forgetting, rehearsal and
pseudorehearsal. Connection Science , 7(2):123–146.
Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer,
H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and
Hadsell, R. (2016). Progressive neural networks. arXiv
preprint arXiv:1606.04671 .
Sarfraz, F., Arani, E., and Zonooz, B. (2022). Synergy
between synaptic consolidation and experience replay for
general continual learning. In Conference on Lifelong
Learning Agents , pages 920–936. PMLR.
Smith, J., Hsu, Y .-C., Balloch, J., Shen, Y ., Jin, H., and
Kira, Z. (2021). Always be dreaming: A new approach
for data-free class-incremental learning. In Proceedings
of the IEEE/CVF International Conference on Computer
Vision , pages 9374–9384.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. (2014). Dropout: a simple way to
prevent neural networks from overﬁtting. The journal of
machine learning research , 15(1):1929–1958.
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and J ´egou, H. (2021). Training data-efﬁcient image
transformers & distillation through attention. In Interna-
tional Conference on Machine Learning , pages 10347–
10357. PMLR.
van de Ven, G. M., Siegelmann, H. T., and Tolias, A. S.
(2020). Brain-inspired replay for continual learning
with artiﬁcial neural networks. Nature communications ,
11(1):1–14.Van de Ven, G. M. and Tolias, A. S. (2019). Three scenarios
for continual learning. arXiv preprint arXiv:1904.07734 .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017).
Attention is all you need. In Advances in neural informa-
tion processing systems , pages 5998–6008.
Verma, V ., Lamb, A., Beckham, C., Najaﬁ, A., Mitliagkas, I.,
Lopez-Paz, D., and Bengio, Y . (2019). Manifold mixup:
Better representations by interpolating hidden states. In
International Conference on Machine Learning , pages
6438–6447. PMLR.
Wang, Z., Liu, L., Duan, Y ., Kong, Y ., and Tao, D. (2022a).
Continual learning with lifelong vision transformer. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 171–181.
Wang, Z., Zhang, Z., Ebrahimi, S., Sun, R., Zhang, H., Lee,
C.-Y ., Ren, X., Su, G., Perot, V ., Dy, J., et al. (2022b).
Dualprompt: Complementary prompting for rehearsal-
free continual learning. arXiv preprint arXiv:2204.04799 .
Wang, Z., Zhang, Z., Lee, C.-Y ., Zhang, H., Sun, R., Ren, X.,
Su, G., Perot, V ., Dy, J., and Pﬁster, T. (2022c). Learning
to prompt for continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 139–149.
Yin, H., Molchanov, P., Alvarez, J. M., Li, Z., Mallya, A.,
Hoiem, D., Jha, N. K., and Kautz, J. (2020). Dreaming to
distill: Data-free knowledge transfer via deepinversion. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8715–8724.
Yoon, J., Kim, S., Yang, E., and Hwang, S. J. (2019).
Scalable and order-robust continual learning with
additive parameter decomposition. arXiv preprint
arXiv:1902.09432 .
Yu, P., Chen, Y ., Jin, Y ., and Liu, Z. (2021). Improving vi-
sion transformers for incremental learning. arXiv preprint
arXiv:2112.06103 .
Zenke, F., Poole, B., and Ganguli, S. (2017). Continual
learning through synaptic intelligence. In International
Conference on Machine Learning , pages 3987–3995.
PMLR.
Zhou, M., Liu, T., Li, Y ., Lin, D., Zhou, E., and Zhao, T.
(2019). Toward understanding the importance of noise in
training neural networks. In International Conference on
Machine Learning , pages 7594–7602. PMLR.

--- PAGE 12 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
A. Experimental setup, datasets and metrics
We use the continuum library (Douillard and Lesort, 2021) to implement different CL scenarios and build our approach
on top of DyTox (Douillard et al., 2021) framework, which is the main baseline in all our experiments. We use a network
that consists of 5 self-attention blocks and a task-attention block. All blocks have 12 attention heads and an embedding
dimension of 384. We train models with a learning rate of 5e 4, a batch size of 128, and a weight decay of 1e 6. All models,
including the baseline, are trained for 500 epochs per task in CIFAR-100 (Krizhevsky et al., 2009), TinyImageNet (Le and
Yang, 2015), and ImageNet-100 (Deng et al., 2009). During the patch embedding process, we utilize patch sizes of 4 for
CIFAR-100, 8 for TinyImageNet, and 16 for ImageNet-100. After each task, the model is ﬁne-tuned on a balanced dataset
with a learning rate of 5e 5for 20 epochs. All models are trained on a single NVIDIA V100 GPU, and all evaluations are
performed on a single NVIDIA RTX 2080 Ti GPU.
We focus mainly on the class-incremental learning setting (Class-IL) (Van de Ven and Tolias, 2019), where the task ID is not
known at the test time. In every task, samples belonging to a new set of classes disjoint from the previous tasks’ classes are
learned by the model. Following Douillard et al. (2021) and De Lange et al. (2021), we evaluate our approach on CIFAR-100,
ImageNet-100, and TinyImageNet. CIFAR-100 consists of 50,000 training images and 10,000 test images of size 32 32
belonging to 100 classes. ImageNet-100 consists of 129k train and 5,000 validation images of size 224 224 belonging to
100 classes. TinyImageNet consists of 100,000 training images and 10,000 test images of size 96 96 belonging to 200
classes.
Except for the analysis of longer task sequences, all other experiments are carried out in the Class-IL setting with 10 tasks.
In the case of CIFAR-100, 100 classes are divided into 10 tasks, with 10 classes in each task. Similarly, 20 classes per task
are learned on TinyImageNet and 10 classes per task on ImageNet-100. The order in which classes are learned can affect the
performance of a CL model (Yoon et al., 2019). We use “class order 1” from (Douillard et al., 2021) for CIFAR-100 and
ImageNet-100, and the sequential class order from 1 to 200 for TinyImageNet-200.
Although the performance of task-incremental learning (Task-IL) can be evaluated in our proposed approach, we exclude
them in our analysis because it simpliﬁes the CL scenario by assuming the availability of task id at the test time, which
translates into choosing the right prediction head during inference.
A.1. Evaluation Metrics
To evaluate the performance of different models under different settings, we select ﬁve different metrics widely used in the
CL literature. We formalize each metric below.
1.Last Accuracy (Douillard et al., 2021) deﬁnes the ﬁnal performance of the CL model on the validation set of all the
tasks seen so far. Concretely, given that tasks are sampled from a set t21;2:::;T , whereTis the total number of tasks
andak;jis the accuracy of a CL model on the validation set of the task kafter learning task j, last accuracy Alastis as
follows:
Alast=1
TTX
k=1ak;T (9)
2.Average Accuracy (Rebufﬁ et al., 2017) deﬁnes the average performance of the learned CL model on the validation
set of all tasks seen so far after learning each task. Given that Kis the number of tasks seen so far and Tis the total
number of tasks, the average accuracy Aavgis as follows:
Aavg=1
TTX
j=11
KKX
k=1ak;j (10)
3.Backward Transfer (BWT) (Lopez-Paz and Ranzato, 2017) deﬁnes the inﬂuence of the learning task ton previously
seen tasksk<t . Positive BWT implies that the learning task tincreased performance on previous tasks, while negative
BWT indicates that the learning task taffected the performance of the model on previous tasks. Formally, BWT is as
follows:
BWT =1
T 1T 1X
j=1aT;j aj;j (11)

--- PAGE 13 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
1 2 3 4 5
Number of Learned T asks2030405060708090100Accuracy (%)
5 T asksDyT ox
BiRT
1 2 3 4 5 6 7 8 9 10
Number of Learned T asks2030405060708090100
10 T asksDyT ox
BiRT
2 4 6 8 10 12 14 16 18 20
Number of Learned T asks2030405060708090100
20 T asksDyT ox
BiRT
Figure 7. Comparison of performance across learning longer sequence of tasks on CIFAR-100.
4.Forward Transfer (FWT) (Lopez-Paz and Ranzato, 2017) deﬁnes the inﬂuence of learning the task ton future tasks
k >t . Positive FWT implies that learning the task tincreased performance in future tasks and vice versa. Positive
FWT occurs when the model learns generalizable features that can help it learn future tasks. Formally, given that ^ajis
the accuracy of the task jat random initialization, the FWT is as follows:
FWT =1
T 1TX
j=2aj 1;j ^aj (12)
5.Forgetting (Chaudhry et al., 2018) quantiﬁes the forgetting of previously learned tasks given the current state of
the model. It is deﬁned as the difference between the maximum accuracy of the model in previously learned tasks
throughout the learning process and the current accuracy of the task. Concretely, forgetting for the task kis as follows:
Forgetting = max
l21;2;:::;k 1ak;l ak;T (13)
B. Quantitative results for ﬁgures
To facilitate comparisons with BiRT, we provide quantitative results for the ﬁgures in the main text.
B.1. Effect of Longer Sequences
Given a limited buffer size, catastrophic forgetting worsens with increasing number of tasks, since the number of representa-
tive samples per task/class will be more limited (Peng et al., 2021). To perform better, it is quintessential for the CL model
to consolidate generalizable features rather than discriminative features speciﬁc to buffered samples. Figure 7 presents
the performance of CL models in sequences of 5, 10, and 20 tasks on CIFAR-100 with a buffer size of 500. Even as the
number of tasks increases, BiRT maintains a substantial improvement over DyTox across all task sequences. As is the case
with low-buffer regimes, BiRT consolidates the past task information better than the baseline, thereby further mitigating
catastrophic forgetting.
B.2. Stability-plasticity dilemma
Figure 4 (left) shows that BiRT achieves better stability, while DyTox is more plastic. We concluded that DyTox is more
prone to forgetting, while BiRT exhibits a better stability-plasticity trade-off. We provide the numerical values for the same
in Table 4. Note that the semantic memory of BiRT achieves a slightly higher stability-plasticity trade-off compared to the
working model of BiRT (which is not clear in the illustration).
C. Working Principle of DyTox
As mentioned in Section A, we build our proposed approach on top of DyTox framework (Douillard et al., 2021), an
architecture expansion approach to continual learning with Transformers as the working model. DyTox uses the information

--- PAGE 14 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Table 4. Quantitative results for the stability-plasticity trade-off in CIFAR-100 for 10 tasks with buffer size 500.
PLASTICITY STABILITY TRADE -OFF
DYTOX 86.06 29.74 44.16
BIRT - W ORKING MODEL 66.42 50.52 57.38
BIRT - S EMANTIC MEMORY 66.08 50.37 57.16
T1 T2 T3 T4 T5 T6 T7 T8 T9T10After T1
After T2
After T3
After T4
After T5
After T6
After T7
After T8
After T9
After T1064.0
39.5 71.4
24.7 53.5 72.0
18.4 38.7 48.4 68.6
17.2 29.9 43.1 49.1 75.5
11.5 22.8 30.7 35.0 55.1 78.8
9.618.1 24.4 25.6 38.1 57.7 66.5
7.914.4 14.2 18.0 30.2 41.5 45.1 77.6
5.110.4 10.7 13.4 19.2 26.1 24.6 55.2 80.0
3.4 7.4 6.610.9 15.0 17.5 15.5 36.5 50.0 78.7DyT ox
T1 T2 T3 T4 T5 T6 T7 T8 T9T1063.1
50.6 57.4
44.1 50.3 47.1
38.6 43.3 43.4 45.8
37.8 38.4 41.1 41.3 46.9
35.2 36.8 39.6 39.0 43.6 43.6
35.4 35.3 36.5 37.4 40.3 42.8 25.9
33.3 31.0 33.8 34.8 40.0 37.8 24.9 41.0
30.0 28.6 32.5 33.0 38.2 33.1 24.9 41.1 36.6
30.1 28.0 31.4 33.5 35.0 33.9 24.0 41.4 37.4 30.0BiRT (Working Model)
T1 T2 T3 T4 T5 T6 T7 T8 T9T1086.9
50.9 58.0
44.9 49.1 47.1
40.4 43.3 43.3 45.2
38.3 39.7 41.2 41.6 46.4
35.7 37.2 39.8 39.3 42.2 44.1
36.3 34.8 37.4 36.2 40.7 43.6 26.1
33.7 31.1 34.4 33.6 39.6 37.4 24.6 41.7
30.7 29.8 32.2 32.8 37.4 34.4 25.1 42.1 36.8
29.8 27.7 31.5 32.4 36.2 35.3 23.9 39.1 37.3 31.2BiRT (Semantic Memory)
Figure 8. Comparison of task-wise performance after learning every task on TinyImageNet with a buffer size of 500 learned for 10 tasks.
The working model achieves better accuracy for the seen tasks after learning 10 tasks compared to DyTox. Semantic memory retains the
performance of older tasks better than the baseline and working model.
about the task id during the training time to learn task-speciﬁc classiﬁers and task tokens. However, no task oracle is used
during inference.
DyTox architecture consists of 5 blocks of Self-Attention Blocks (SABs, implemented using ConVit (d’Ascoli et al., 2021))
as an encoder to process the input image after the tokenization process. The features predicted by the encoder are then
combined with a task token (which is speciﬁc to that task) and fed into a Task-Attention Block (TAB), in which the task
token attends to the features and extracts the task-speciﬁc information. A task-speciﬁc classiﬁer projects the processed
task token to the number of classes in the task. Thus, the task token and classiﬁer are expanded with respect to every task,
while the SAB and TAB blocks are shared between tasks. Furthermore, it employs the copy of the working model at the
task boundary as a teacher model to distill the information about past tasks into the working model. DyTox freezes the task
tokens and classiﬁer heads of previously learned tasks in order to retain the performance on old tasks.
D. Model Analysis on Other Datasets
We analyze task-wise probability (in Figure 3), stability-plasticity trade-off, and task-recency bias (in Figure 4) on the
CIFAR-100 dataset learned for 10 tasks with buffer size 500 in the main text. Here, we show additional results on other
datasets (TinyImageNet and ImageNet-100).
Figure 8 illustrates the task-wise accuracy of BiRT vs. DyTox in TinyImageNet. It is evident that BiRT (Semantic Memory)
retains more knowledge about past tasks, which in turn helps BiRT (Working Model) achieve better overall performance
compared to DyTox. The stability-plasticity trade-off shown in Figure 9 corroborates this conclusion by showing that both
the working model and the semantic memory of BiRT have better stability and trade-off values compared to the baseline.
Given that TinyImageNet is one of the challenging benchmarks used in our study, we can see a very high task recency bias
in DyTox in Figure 9, suggesting that the model is more likely to predict classes from the last few tasks for samples during
inference. The skew toward recent tasks is more pronounced in the TinyImageNet data set compared to CIFAR-100. On the
other hand, we can see a more balanced distribution of prediction probabilities in the working model and semantic memory
of BiRT.

--- PAGE 15 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
DyT ox BiRT10203040506070 Accuracy (%)Plasticity
Stability
Trade-off
DyT ox BiRT0.00.10.20.30.40.50.6Accuracy (%)T ask 1
T ask 10
Figure 9. Comparison of the stability-plasticity trade-off (left) and the task-recency bias (right) trained for 10 tasks on TinyImageNet with
buffer size 500.
Table 5. Hyperparameters used in BiRT for different datasets and tasks.
DATASET #OFTASKS BUFFER SIZE  e12
CIFAR-1005200 0.0005 0.001 0.05 0.01
500 0.005 0.003 0.05 0.01
10200 0.001 0.003 0.05 0.001
500 0.001 0.003 0.05 0.001
1000 0.0005 0.0008 0.05 0.01
2000 0.0002 0.0015 0.05 0.01
20200 0.005 0.001 0.05 0.08
500 0.0005 0.003 0.05 0.1
TINYIMAGE NET 10500 0.001 0.003 0.05 0.01
1000 0.01 0.0008 0.01 0.001
2000 0.0001 0.008 0.01 0.0008
IMAGE NET-100 10500 0.0001 0.003 0.05 0.001
1000 0.0001 0.003 0.05 0.001
2000 0.01 0.005 0.01 0.001
Table 6. Robustness of BiRT under individual noise in CIFAR-100 dataset.
BIRT W/ONOISE SUPERVISION NOISE REPRESENTATION NOISE ATTENTION NOISE
LAST ACC 45.89 49.64 46.43 49.06
ADVACC(=4) 36.52 37.95 36.64 38.39
ADVACC(=8) 26.17 26.26 25.64 27.04
NATCORACC 21.82 24.33 21.07 24.42
E. Hyperparameters for the Empirical Results
We provide the hyperparameters that we used in our proposed approach for different datasets and tasks in Table 5. Two main
hyperparameters in our approach are the decay parameter that is used to gradually assimilate knowledge into the semantic
memory of the working model with frequency ein Eq. 2 and the weighting parameters 1and2in Eq. 6 used to enforce
consistency between the working model and the knowledge consolidated in the semantic memory with respect to images
from the current task and representations from the buffer memory.
F. Robustness Analysis with Individual Noise
In order to elucidate the improvements in robustness of BiRT brought about by different noises, we conducted more
experiments to ablate the same. As shown in Table 6, overall, every noise proposed in this paper contributes to improving
the generalization of stored representations, enabling effective CL in vision transformers. Every noise makes the model less
susceptible to adversarial attacks and more robust to natural corruption on the data.

--- PAGE 16 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Table 7. Comparison of performance across different noise strengths on CIFAR-100 dataset with buffer size 500.
SUPERVISION NOISE REPRESENTATION NOISE ATTENTION NOISE
STRENGTH (P) L AST ACC STRENGTH (P) L AST ACC STRENGTH (P) L AST ACC
0.2 50.90 0.2 51.45 0.2 49.93
0.7 49.85 0.7 49.27 0.8 49.82
Table 8. Comparison of training time taken to learn one task in CIFAR-100 dataset with buffer size 500.
CIFAR-100 T INYIMANGE NET IMAGE NET-100
DYTOX44MINS2HOURS 10MINS11HOURS 22MINS
BIRT45MINS2HOURS 6MINS10HOURS 52MINS
Table 9. Comparison between the working model and the semantic memory of BiRT for different datasets and buffer sizes.
DATASETBUFFER
SIZEWORKING
MODELSEMANTIC
MEMORY
CIFAR-100500 50.20 0.67 50.11 0.75
1000 51.20 1.46 51.17 1.41
TINYIMAGE NET500 32.60 0.18 32.58 0.24
1000 38.42 0.34 38.24 0.37
IMAGE NET-100500 51.06 0.24 50.80 0.56
1000 52.21 0.00 51.69 0.00
G. Sensitivity Analysis to Noise
We control the strength and amount of noise added at different stages of the training process, based on the percentage of
samples to which noise is added in each batch. We conducted additional experiments on CIFAR-100 with 10 tasks and a
buffer size of 500, varying the percentage of samples to which each noise type is added. The results are shown in Table 7.
‘p’ denotes the percentage of samples to which the corresponding noise is added during the replay of the representation in
each batch (batch size = 128). It is evident that different levels of noise change the last accuracy; however, the performance
at different levels of noise reveals that BiRT is not very sensitive to hyperparameters.
H. Training Time Analysis
We conducted an experiment to compare the training time of different CL models considered in this work. The training time
on an NVIDIA RTX 2080 Ti for various datasets with buffer size 500 to learn a single task (500 epochs) in CIFAR-100
dataset is enumerated in Table 8. As can be seen, both DyTox and BiRT entail similar training times, indicating that the
proposed noise-based approach in BiRT does not increase the training time. In fact, our proposed approach improves
generalization performance to a large extent with minimal/no additional computational cost.
I. Analysis on Working Model and Semantic Memory
We compare the performance between DyTox and the BiRT working model in Table 1. However, stochastically assimilating
the knowledge learned in the working model into the semantic memory throughout the learning process and at the end of
tasks results in a generalized working model with lesser forgetting. We show the last accuracy of the working model and
semantic memory for different datasets and buffer sizes in Table 9.
J. Quantitative Results for Model Analysis
Figure 4 in the main text illustrates the stability-plasticity trade-off between DyTox and BiRT. We provide the quantitative
results for the same in Table 10. DyTox is more prone to forgetting, whereas BiRT displays a better stability-plasticity
trade-off compared to the baseline. We evaluated the robustness of DyTox, BiRT without noise, and BiRT across different
strengths of adversarial attacks and natural corruptions. Qualitative results are presented in Figure 5 in the main text. Table
11 and 12 enumerate the quantitative results of the same.

--- PAGE 17 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Table 10. Quantitative results for the stability-plasticity analysis of different CL models.
PLASTICITY STABILITY TRADE -OFF
DYTOX 73.30 18.08 29.01
BIRT 43.73 32.75 37.45
Table 11. Quantitative results of different CL models to different levels of adversarial attacks. Noise in BiRT improves its robustness
against adversarial attacks across different epsilon values.
AVERAGE 0 0.25 0.5 1 2 4 8 16 32
DYTOX 23.59 35.29 33.35 31.43 29.30 28.26 27.88 19.16 7.03 0.67
BIRT W/ONOISE 31.50 47.45 44.74 41.77 38.92 37.54 36.53 26.18 9.54 0.90
BIRT 33.37 50.27 47.32 44.23 41.30 39.79 38.95 27.37 10.12 1.03
Table 12. Quantitative results of different CL models to different levels of natural corruption. Noise in BiRT improves its robustness
against natural corruption across different strengths.
AVERAGE BRIGHT . CONTRAST DEFOCUS ELASTIC FOG FROST G BLUR G NOISE GLASS
DYTOX 21.06 26.66 12.38 22.64 21.32 18.57 24.92 21.21 20.99 17.08
BIRT W/ONOISE 21.82 28.71 10.99 22.19 21.34 16.07 27.44 20.40 23.41 18.32
BIRT 25.81 32.59 14.19 26.49 25.53 20.04 32.46 24.34 27.17 22.22
IMPULSE JPEG MOTION PIXELATE SATURATE SHOT SNOW SPATTER SPECKLE ZOOM
DYTOX 18.51 22.69 19.56 24.54 21.60 22.00 21.39 22.43 21.11 20.57
BIRT W/ONOISE 19.83 25.23 18.22 24.25 21.36 24.36 25.05 24.57 23.30 19.71
BIRT 22.86 29.71 21.84 28.71 24.68 28.32 29.58 28.67 27.09 23.98
K. Extended Related Works
In addition to the CL methods discussed in the Related Works section in the main text, there is another line of work that
pursues the ‘Deep Inversion’ technique to synthesize replay images for old tasks. Deep inversion works by inverting a neural
network’s feature extractor to generate synthetic input data that is similar to the original input data. In the context of class
incremental learning, deep inversion can be used to generate synthetic data for the new classes that the model needs to learn
without requiring access to any real data for those classes (Yin et al., 2020; Gao et al., 2022; Smith et al., 2021). Though this
approach alleviates any privacy issues and is more memory-efﬁcient, the model responsible for generating the synthetic data
might undergo catastrophic forgetting and this can be exacerbated in long-task sequences.
The theory of a complementary learning system (CLS) posits that the ability to continually acquire and assimilate knowledge
over time in the brain is mediated by multiple memory systems (Hassabis et al., 2017; Kumaran et al., 2016). Inspired by
CLS theory, CLS-ER (Arani et al., 2021) proposed a dual memory method that maintains multiple semantic memories that
interact with episodic memory. On the other hand, FearNet (Kemker and Kanan, 2017) utilizes a brain-inspired dual-memory
system coupled with pseudo rehearsal (Robins, 1995) in order to efﬁciently learn new tasks.
L. Limitations
BiRT is a novel continual learning approach that can be applied to various tasks. However, the effectiveness of different
levels of noise in BiRT varies in terms of generalization and robustness. The impact of hyperparameters on the effectiveness
of different types of noise can also affect accuracy to some extent. However, our empirical results reveal that BiRT is not
very sensitive to hyperparameters. BiRT may not be well-suited for datasets with small images (e.g., 32 x 32) since the
representations stored in the buffer for such datasets may require more memory compared to storing images. Nonetheless,
since real-world datasets typically contain high-resolution images (as in ImageNet-100 and TinyImageNet), BiRT can enable
efﬁcient CL in most cases. BiRT does not raise privacy concerns as we do not store personal data, and there are no known
bias and fairness issues since we do not use any pretrained weights.

--- PAGE 18 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Figure 10. Comparison of attention maps with respect to the class token on the validation set of the ﬁrst task of ImageNet-100 trained
for 10 tasks with buffer size 500. The attention maps are plotted after learning the ﬁrst, fourth, seventh, and last tasks (red regions
correspond to regions with higher attention). BiRT retains the knowledge of salient regions in the image better than DyTox, leading to
better predictions and less forgetting.
M. Attention Map Analysis
A CL model that is able to preserve the salient regions learned in the ﬁrst task (when those samples were trained) as learning
progresses through the subsequent tasks would provide less catastrophic forgetting (Ebrahimi et al., 2021). The [CLS] token
in Vision Transformers, which is utilized to infer the class of a sample (Dosovitskiy et al., 2020), attends to the salient
regions of an image in order to extract rich features pertaining to the task learned by the model. Therefore, it would be
beneﬁcial to study the drift in the regions that the model considers to be salient in the image as learning progresses.
Concretely, we study the attention maps calculated by the last Class-Attention block in BiRT for samples in the validation
set of the ﬁrst task as the learning progresses from the ﬁrst task to the last task. We overlay the attention map as a heatmap
(interpolated to the image size) on the image. Figures 10 and 11 show that the BiRT working model preserves the attention
map learned in the ﬁrst task better than DyTox as the training progresses.

--- PAGE 19 ---
BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning
Figure 11. Comparison of attention maps with respect to the class token on the validation set of the ﬁrst task of ImageNet-100 trained
for 10 tasks with buffer size 500. The attention maps are plotted after learning the ﬁrst, fourth, seventh, and last tasks (red regions
correspond to regions with higher attention). BiRT retains the knowledge of salient regions in the image better than DyTox, leading to
better predictions and less forgetting.
