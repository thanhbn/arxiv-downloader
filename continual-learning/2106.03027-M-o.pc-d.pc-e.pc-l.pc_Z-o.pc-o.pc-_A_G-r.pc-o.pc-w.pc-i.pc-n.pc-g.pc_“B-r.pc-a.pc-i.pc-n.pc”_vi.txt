# 2106.03027.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2106.03027.pdf
# Kích thước tệp: 1552661 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Model Zoo: Một "Bộ não" Phát triển
Học tập Liên tục
Rahul Ramesh
Đại học Pennsylvania
rahulram@seas.upenn.eduPratik Chaudhari
Đại học Pennsylvania
pratikac@seas.upenn.edu

Tóm tắt
Bài báo này lập luận rằng các phương pháp học tập liên tục có thể được hưởng lợi bằng cách chia nhỏ khả năng của người học trên nhiều mô hình. Chúng tôi sử dụng lý thuyết học thống kê và phân tích thực nghiệm để chỉ ra cách nhiều nhiệm vụ có thể tương tác với nhau theo cách không tầm thường khi một mô hình đơn được huấn luyện trên chúng. Lỗi tổng quát hóa trên một nhiệm vụ cụ thể có thể cải thiện khi nó được huấn luyện với các nhiệm vụ có tính hiệp đồng, nhưng cũng có thể xấu đi khi được huấn luyện với các nhiệm vụ cạnh tranh. Lý thuyết này thúc đẩy phương pháp của chúng tôi có tên Model Zoo, được lấy cảm hứng từ tài liệu boosting, phát triển một tập hợp các mô hình nhỏ, mỗi mô hình được huấn luyện trong một giai đoạn của học tập liên tục. Chúng tôi chứng minh rằng Model Zoo đạt được những cải tiến lớn về độ chính xác trên nhiều bài toán benchmark học tập liên tục. Mã nguồn có sẵn tại https://github.com/grasp-lyrl/modelzoo_continual.

1 Giới thiệu
Một người học liên tục tìm cách tận dụng dữ liệu từ các nhiệm vụ trước để học các nhiệm vụ mới được chỉ ra cho nó trong tương lai, và ngược lại, tận dụng dữ liệu từ các nhiệm vụ mới này để cải thiện độ chính xác của nó trên các nhiệm vụ trước. Có lý khi hiệu suất của một người học như vậy sẽ phụ thuộc vào mức độ liên quan của các tập nhiệm vụ này. Nếu hai tập nhiệm vụ không tương tự, việc học trên các nhiệm vụ trước khó có thể có lợi cho các nhiệm vụ tương lai—thậm chí có thể có hại. Và tương tự, các nhiệm vụ mới có thể khiến người học "quên" và dẫn đến sự suy giảm độ chính xác trên các nhiệm vụ trước. Mục tiêu của chúng tôi trong bài báo này là mô hình hóa mối liên quan giữa các nhiệm vụ và phát triển các phương pháp mới cho học tập liên tục dẫn đến việc chuyển giao tiến-lùi tốt bằng cách tính đến sự tương đồng và khác biệt giữa các nhiệm vụ. Các đóng góp của chúng tôi như sau.

1. Phân tích lý thuyết Chúng tôi đặc trưng khi nhiều nhiệm vụ có thể được học bằng một mô hình đơn và, tương tự, khi làm như vậy có hại cho độ chính xác của một nhiệm vụ cụ thể. Ý tưởng kỹ thuật chính ở đây là định nghĩa một khái niệm về mối liên quan giữa các nhiệm vụ. Chúng tôi đầu tiên chỉ ra cách nếu các đầu vào của các nhiệm vụ khác nhau là các biến đổi "đơn giản" của nhau (và tương tự cho các đầu ra), thì người ta có thể học một bộ tạo đặc trưng chia sẻ tổng quát hóa tốt hơn trên mọi nhiệm vụ, so với việc huấn luyện nhiệm vụ đó một cách riêng lẻ. Các nhiệm vụ như vậy có liên quan chặt chẽ với nhau và do đó có lợi khi khớp một mô hình đơn trên tất cả chúng. Chúng tôi chỉ ra rằng nếu các nhiệm vụ không có liên quan chặt chẽ như vậy, đặc biệt nếu mô hình tối ưu cho một nhiệm vụ dự đoán kém trên nhiệm vụ khác, thì việc khớp một mô hình đơn trên các nhiệm vụ như vậy có thể tệ hơn so với việc huấn luyện mỗi nhiệm vụ một cách riêng lẻ. Các nhiệm vụ như vậy cạnh tranh với nhau để có khả năng cố định trong mô hình đơn. Chúng tôi cũng nghiên cứu thực nghiệm sự cạnh tranh này bằng bộ dữ liệu CIFAR-100.

2. Phát triển thuật toán Phân tích trên gợi ý rằng một người học liên tục có thể được hưởng lợi từ việc chia nhỏ khả năng học của nó trên các tập nhiệm vụ có tính hiệp đồng. Chúng tôi phát triển một người học liên tục như vậy gọi là Model Zoo. Tại mỗi giai đoạn, một mô hình đa nhiệm vụ nhỏ được khớp với nhiệm vụ hiện tại và một số nhiệm vụ trước được thêm vào Model Zoo. Phương pháp này được lấy cảm hứng lỏng lẻo từ AdaBoost ở chỗ nó chọn các nhiệm vụ có hiệu suất kém trong các vòng trước và do đó có thể được hưởng lợi nhiều nhất từ việc được huấn luyện với nhiệm vụ hiện tại. Tại thời điểm suy luận, với nhiệm vụ đã cho, chúng tôi lấy trung bình các dự đoán từ tất cả các mô hình trong tập hợp đã được huấn luyện trên nhiệm vụ đó.

3. Kết quả thực nghiệm Chúng tôi đánh giá toàn diện Model Zoo trên các bài toán benchmark học tập liên tục tăng dần nhiệm vụ hiện có và chỉ ra so sánh với các phương pháp hiện có. Có sự đa dạng rộng rãi trong các thiết lập bài toán được sử dụng bởi các phương pháp hiện có, ví dụ, một số phát lại dữ liệu từ các nhiệm vụ trước (như Model Zoo được thiết kế để làm), một số chỉ phát lại một tập con dữ liệu, một số chỉ huấn luyện cho một epoch trong mỗi

--- TRANG 2 ---
5 10 15 20
Số Nhiệm vụ0255075100Độ chính xác Trung bình (%)
SGD (47)
EWC (48)
AGEM (52)ER (55)
Stable-SGD (49)
TAG (58)Isolated-small-Single Epoch (66)
Model Zoo-small-Single Epoch (81)
Isolated-Single Epoch (40)Model Zoo- Single Epoch (64)
Isolated-Multi epoch (86)
Model Zoo-Multi Epoch (97)So sánh các Phương pháp trên Mini-Imagenet
5 10 15 20
Số Nhiệm vụ0255075100Độ chính xác Nhiệm vụ (%)
IsolatedMulti Epoch Single Epoch
Task1 Task5 Task9 Task13 Task17Độ chính xác Nhiệm vụ Cá nhân trên Split-miniImagenet

Hình 1: Trái: Các phương pháp học tập liên tục hiện có hoạt động tốt như thế nào trong thiết lập một epoch? Chúng tôi theo dõi độ chính xác trung bình (trên tất cả các nhiệm vụ đã thấy cho đến giai đoạn hiện tại) trên bộ dữ liệu Split-miniImagenet. Tất cả các phương pháp trong biểu đồ này (trừ khi được chỉ định khác) được đánh giá trong thiết lập một epoch (Lopez-Paz và Ranzato, 2017), tức là mỗi nhiệm vụ mới chỉ được phép huấn luyện 1 epoch. Chúng tôi so sánh phương pháp Model Zoo của chúng tôi và các biến thể của nó (tất cả đều in đậm) với các phương pháp học tập liên tục hiện có được thiết kế cho thiết lập một epoch (các đường mờ, xem Bảng 1 để biết tham chiếu). Isolated đề cập đến một thực hiện rất đơn giản của Model Zoo trong đó một mô hình riêng biệt được khớp tại mỗi giai đoạn mà không có bất kỳ học tập liên tục nào, hoặc chia sẻ dữ liệu giữa các nhiệm vụ; Isolated-small hoặc Model Zoo-small đề cập đến việc sử dụng một mạng sâu rất nhỏ với 0.12M trọng số. Một số phát hiện đáng ngạc nhiên được thấy ở đây. (i) Isolated-small (màu đen) vượt trội hơn các phương pháp hiện có hơn 10%, trong khi có thời gian huấn luyện nhanh hơn, thời gian suy luận, kích thước mô hình tương đương và không thực hiện bất kỳ phát lại dữ liệu nào. Điều này cho thấy các phương pháp hiện có không đủ tận dụng dữ liệu từ nhiều nhiệm vụ. Điều này cũng cho thấy tiện ích của các phương pháp đơn giản như Isolated để thực hiện đánh giá học tập liên tục thực tế và thực tế hơn. (ii) Trong khi mô hình lớn hơn với 3.6M trọng số mỗi vòng, Isolated-SingleEpoch (màu xanh hoàng gia), hoạt động kém, độ chính xác của nó tốt hơn các phương pháp hiện có (Isolated-MultiEpoch) khi được huấn luyện cho nhiều epoch. Điều này cho thấy các phương pháp có thể bị thiếu huấn luyện nghiêm trọng trong thiết lập một epoch và đây có thể không phải là thiết lập thích hợp để xây dựng các phương pháp học tập liên tục; điều này cũng được nhận thấy bởi Lopez-Paz và Ranzato (2017). (iii) Model Zoo và Model Zoo-small phát lại tất cả dữ liệu từ các nhiệm vụ trước (A-GEM cũng phát lại 10% dữ liệu), đạt được khoảng 10% cải thiện so với các đối tác Isolated của chúng trong cả thiết lập một epoch và nhiều epoch; Model Zoo có khả năng cải thiện để giải quyết mỗi nhiệm vụ bằng cách tận dụng các nhiệm vụ khác. Điều này cho thấy việc phát lại dữ liệu từ các nhiệm vụ trước là có lợi (Robins, 1995), ngay cả khi phát lại có thể không phù hợp với một số công thức phong cách của học tập liên tục trong tài liệu (Farquhar và Gal, 2019a; Kaushik et al., 2021). Không làm như vậy làm tổn hại đáng kể đến việc chuyển giao tiến và lùi, và độ chính xác nhiệm vụ trung bình.

Phải: Thiết lập một epoch có cho thấy chuyển giao tiến-lùi không? Sự tiến hóa của độ chính xác nhiệm vụ cá nhân của Model Zoo (thiết lập nhiều epoch in đậm và thiết lập một epoch chấm), trên bộ dữ liệu Split-miniImagenet (chỉ 5 nhiệm vụ được vẽ ở đây, xem Hình A6 cho phiên bản đầy đủ). Các dấu X biểu thị độ chính xác của Isolated. Độ chính xác của các nhiệm vụ cải thiện với mỗi giai đoạn cho thấy chuyển giao lùi. Ngoài ra, các dấu X thường dưới độ chính xác ban đầu của nhiệm vụ trong quá trình học tập liên tục, cho thấy chuyển giao tiến. Trong khi cả Model Zoo một epoch và nhiều epoch đều cho thấy chuyển giao tiến-lùi tốt, độ chính xác của các nhiệm vụ cho trường hợp trước kém hơn khoảng 25% so với trường hợp sau; các biểu đồ tương ứng cho các phương pháp khác có trong Phụ lục B.6. Điều này cho thấy chúng ta cũng nên chú ý đến việc thiếu huấn luyện và độ chính xác mỗi nhiệm vụ trong học tập liên tục.

giai đoạn, một số sử dụng kiến trúc cực nhỏ, v.v. Chúng tôi so sánh Model Zoo với các phương pháp hiện có trong một số thiết lập này. Model Zoo đạt được độ chính xác tốt hơn các phương pháp hiện có trên các benchmark được đánh giá. Cải thiện trong độ chính xác trung bình mỗi nhiệm vụ khá lớn trong một số trường hợp, ví dụ, 30% cho Split-miniImagenet. Chúng tôi cũng chỉ ra rằng Model Zoo thể hiện chuyển giao tiến và lùi mạnh mẽ.

4. Cái nhìn phê phán về học tập liên tục Chúng tôi thấy rằng ngay cả một người học Isolated, tức là người huấn luyện một mô hình (nhỏ) trên các nhiệm vụ từ mỗi giai đoạn và không thực hiện bất kỳ học tập liên tục nào, vượt trội đáng kể so với hầu hết các phương pháp học tập liên tục hiện có trên các bài toán benchmark được đánh giá, ví dụ, hơn 8% trong Hình 1 và Bảng 1 và ??. Hiệu suất mạnh mẽ này đáng ngạc nhiên vì đây là một người học rất đơn giản có thời gian huấn luyện/suy luận tốt hơn, không phát lại dữ liệu, và số lượng trọng số tương đương với các phương pháp hiện có.

--- TRANG 3 ---
2 Phân tích lý thuyết về cách học từ nhiều nhiệm vụ

Trong phần này, chúng tôi (i) công thức hóa bài toán học từ nhiều nhiệm vụ, (ii) thảo luận về một mô hình đơn giản làm nổi bật khi việc huấn luyện một mô hình trên nhiều nhiệm vụ là có lợi, và (iii) chỉ ra các kết quả mới về cách khả năng cố định của mô hình gây ra cạnh tranh giữa các nhiệm vụ.

2.1 Công thức Bài toán

Một nhiệm vụ học có giám sát được định nghĩa là một phân phối xác suất kết hợp P(x;y) của các đầu vào x∈X và nhãn y∈Y. Người học có quyền truy cập vào m mẫu i.i.d S={xi;yi}i=1;:::;m từ nhiệm vụ. Một giả thuyết là một hàm h:X→Y với h∈H là không gian giả thuyết. Người học có thể chọn một giả thuyết tối thiểu hóa rủi ro thực nghiệm

êS(h) = 1/m Σi=1m 1{h(xi)≠yi}

với hy vọng đạt được rủi ro dân số nhỏ

eP(h) = P(h(x)≠y):

Các kết quả PAC-learning cổ điển (Vapnik, 1998) gợi ý rằng với xác suất ít nhất 1−δ trên các lần rút S, đồng nhất cho bất kỳ h∈H, chúng ta có eP(h)≤êS(h)+ε nếu

m = O(D log 1/δ/ε²)                                                   (1)

trong đó D=VC(H) là chiều VC của không gian giả thuyết H. Chúng tôi định nghĩa "rủi ro dư thừa" của một giả thuyết là

EP(h) = eP(h) − inf h∈H eP(h):

Trong thiết lập học tập liên tục, một nhiệm vụ mới được chỉ ra cho người học tại mỗi giai đoạn (hoặc vòng). Do đó sau n giai đoạn, người học được trình bày với n nhiệm vụ P:= (P1;:::;Pn), với các tập huấn luyện tương ứng S:= (S1;:::;Sn), mỗi tập có m mẫu, và người học chọn n giả thuyết h= (h1;:::;hn)∈Hn, mỗi hi∈H. Nếu nó tìm kiếm rủi ro dân số trung bình nhỏ

eP(h) = 1/n Σi=1n ePi(hi);

nó có thể làm như vậy bằng cách tối thiểu hóa rủi ro thực nghiệm trung bình

êS(h) = 1/n Σi=1n êSi(hi):

Như Baxter (2000) chỉ ra, dưới các điều kiện rất chung, nếu

m = O(1/ε² dH(n) 1/n log 1/δ);                                       (2)

thì chúng ta có eP(h)≤êS(h)+ε cho bất kỳ h∈Hn. Lượng dH(n) ở đây là một chiều VC tổng quát cho họ các không gian giả thuyết Hn, phụ thuộc vào phân phối kết hợp của các nhiệm vụ. Càng lớn số lượng nhiệm vụ n, càng nhỏ dH(n) (Ben-David và Borbely, 2008). Việc (2) có phải là một cải tiến so với việc huấn luyện nhiệm vụ một cách riêng lẻ như trong (1) phụ thuộc vào lớp giả thuyết H và mức độ liên quan của các nhiệm vụ P1;:::;Pn thông qua lượng dH(n). Điều quan trọng nhất cần lưu ý ở đây là theo các tính toán này, nếu người ta muốn đạt được rủi ro dân số trung bình nhỏ trên các nhiệm vụ, việc huấn luyện nhiều nhiệm vụ cùng nhau không thể tệ hơn:

dH(n)≤VC(H):

Kết quả này là động lực cho các phương pháp huấn luyện nhiều nhiệm vụ cùng nhau.

--- TRANG 4 ---
2.2 Kiểm soát rủi ro dư thừa của một nhiệm vụ cụ thể cho các nhiệm vụ hiệp đồng

Một mục tiêu quan trọng của học tập liên tục là có rủi ro thấp trên tất cả các nhiệm vụ. Đây là một yêu cầu mạnh mẽ hơn so với (2) bó buộc rủi ro dân số trung bình trên tất cả các nhiệm vụ.

Giả sử tồn tại một họ F các hàm fi:X→X ánh xạ các đầu vào của một nhiệm vụ sang các đầu vào của nhiệm vụ khác, tức là bất kỳ nhiệm vụ nào có thể được viết là

Pj(A) = f[Pi](A) = Pi({(f(x);y) : (x;y)∈A})

cho một số hàm f∈F cho bất kỳ tập A nào. Chúng ta có thể giả định mà không mất tính tổng quát rằng F hoạt động như một nhóm trên không gian giả thuyết và H đóng dưới tác động của nó. Nói đơn giản, điều này đòi hỏi rằng cho h∈H phù hợp cho nhiệm vụ P, chúng ta có thể thu được một giả thuyết mới hf phù hợp cho nhiệm vụ khác f[P]. Thay vì tìm kiếm trên toàn bộ không gian Hn như trong §2.1, bây giờ chúng ta chỉ cần tìm một giả thuyết h∈H sao cho quỹ đạo của nó

[h]F = {h' : ∃f∈F với h'=hf}

chứa các giả thuyết có rủi ro thực nghiệm thấp trên mỗi nhiệm vụ trong n nhiệm vụ. Về mặt khái niệm, bước này học thiên hướng quy nạp (Baxter, 2000; Thrun và Pratt, 2012). Độ phức tạp mẫu để làm như vậy chính xác là (2). Từ trong quỹ đạo này, chúng ta có thể chọn một giả thuyết có rủi ro thực nghiệm thấp cho một nhiệm vụ được chọn P1. Độ phức tạp mẫu của bước thứ hai này là

|S1| = O(1/ε² (dmax log 1/δ))                                        (3)

trong đó dmax = suph∈H VC([h]F). Bằng hội tụ đồng nhất, như Ben-David và Schuller (2003) chỉ ra, thủ tục hai bước này đảm bảo rủi ro dư thừa thấp cho mọi nhiệm vụ P1;:::;Pn. Chúng ta có

suph∈H VC([h]F) = dmax ≤ dH(n+1) ≤ dH(n) ≤ D = VC(H): (4)

Tổng độ phức tạp mẫu thuận lợi so với học nhiệm vụ một cách riêng lẻ nếu cả dH(n) và dmax đều nhỏ. Ví dụ, nếu F hữu hạn và n≥log |F|/D, chúng ta có dH(n)≤2 log |F| cho thấy chúng ta nhận được lợi ích thống kê từ việc học với nhiều nhiệm vụ nếu D≫log |F|.

Nhận xét 1 (Dữ liệu từ các nhiệm vụ khác có thể không cải thiện độ chính xác ngay cả khi chúng hiệp đồng). Hãy đưa ra một số quan sát sử dụng phân tích trên. (i) Từ (4), số mẫu mỗi nhiệm vụ m giảm với n; đây là lợi ích của sự liên quan mạnh mẽ giữa các nhiệm vụ và như chúng ta thấy tiếp theo, điều này không xảy ra trong trường hợp chung. (ii) Số lượng nhiệm vụ tỷ lệ về cơ bản tuyến tính với D, cho thấy người ta nên sử dụng một mô hình nhỏ nếu chúng ta có ít nhiệm vụ. (iii) Nhưng chúng ta không thể luôn sử dụng một mô hình nhỏ. Nếu các nhiệm vụ đa dạng và liên quan bởi các biến đổi phức tạp với |F| lớn, chúng ta cần một không gian giả thuyết lớn để học chúng cùng nhau. Nếu |F| lớn và H không phù hợp, chiều VC dmax lớn bằng chính D; trong trường hợp này lại không có lợi ích thống kê từ việc huấn luyện nhiều nhiệm vụ cùng nhau, nhưng cũng không có sự suy giảm.

2.3 Cạnh tranh nhiệm vụ xảy ra cho các không gian giả thuyết với khả năng hạn chế

Có thể có các thiết lập mà việc khớp một mô hình trên nhiều nhiệm vụ có thể không đủ. Để nghiên cứu điều này, chúng tôi xem xét một khái niệm yếu hơn về mối liên quan. Chúng tôi nói rằng hai nhiệm vụ Pi;Pj là εij-liên quan nếu

c E^{1/εij}_{Pi}(h) ≥ EPj(h;h*i); cho tất cả h∈H:                    (5)

Ở đây EP(h;h') := eP(h) − eP(h') và h*i = argminh∈H ePi(h) là giả thuyết tốt nhất cho nhiệm vụ Pi; chúng tôi đặt c≥1 là một hệ số độc lập với i;j. Càng nhỏ εij, càng hữu ích các mẫu từ Pi để học Pj. Định nghĩa gợi ý rằng tất cả các giả thuyết h có rủi ro dư thừa thấp trên Pi cũng có rủi ro dư thừa thấp trên Pj đến một số hạng cộng ePj(h*j) và hiệu ứng này trở nên mạnh hơn khi εij→1+. Lưu ý rằng định nghĩa về mối liên quan không đối xứng. Hanneke và Kpotufe (2020) gọi đây là số mũ chuyển giao. Để có một số trực giác, chúng ta có thể kết nối định nghĩa này với một bất đẳng thức tam giác nhất định giữa các nhiệm vụ được phát triển bởi Crammer et al. (2008): trong thiết lập có thể thực hiện được khi ePi(h*i) = 0, cho c;εij = 1, chúng ta có thể viết (5) như

ePi(h) + ePj(h*i) ≥ ePj(h)

đây là kiểu như một tam giác với các đỉnh tại h;h*i và h*j với các số hạng như ePi(h) đại diện cho độ dài của cạnh giữa h và h*i. Định nghĩa này do đó mô hình hóa một tập các nhiệm vụ và không gian giả thuyết không quá bệnh lý, ePj(h) không thể tệ hơn nhiều so với tổng của hai cạnh khác. Bây giờ chúng ta có thể chỉ ra định lý sau bó buộc rủi ro dư thừa EP1(h) cho một giả thuyết h được huấn luyện sử dụng dữ liệu từ nhiều nhiệm vụ. Xem Phụ lục C cho chứng minh.

--- TRANG 5 ---
Định lý 2 (Cạnh tranh nhiệm vụ). Giả sử chúng ta muốn tìm một giả thuyết tốt cho nhiệm vụ P1 và có quyền truy cập vào n nhiệm vụ P1;:::;Pn trong đó mỗi cặp Pi;Pj là εij-liên quan. Sắp xếp các nhiệm vụ theo thứ tự tăng dần của εi1, tức là mức độ liên quan của chúng với P1. Gọi thứ tự này là P(1);P(2);:::;P(n) với ε(1)≤ε(2)≤:::≤ε(n) và P(1)≡P1 và ε(1) = 1. Gọi ĥk là giả thuyết tối thiểu hóa rủi ro thực nghiệm trung bình của k nhiệm vụ đầu tiên. Khi đó, với xác suất ít nhất 1−δ trên các lần rút dữ liệu huấn luyện,

EP1(ĥk) ≤ 1/k Σi=1k EP1(h*(i)) + c/k êS(ĥ) + c' √(D log 1/δ)/(km)^{1/2} 1/εmax    (6)

trong đó εmax(k) = max{ε(1);:::;ε(k)} và c;c' là các hằng số.

Lưu ý rằng số hạng đầu tiên tăng với số lượng nhiệm vụ k vì chúng ta chọn các nhiệm vụ với εi1 thấp hơn ngày càng khác biệt với P1. Số hạng thứ hai thường giảm với k. Rủi ro thực nghiệm êS(ĥ) thường nhỏ; trong các thí nghiệm của chúng tôi với mạng sâu, chúng tôi đạt được về cơ bản lỗi huấn luyện bằng không trên tất cả. Tăng số lượng nhiệm vụ k, tăng số lượng mẫu hiệu quả km, do đó giảm số hạng thứ hai tổng thể. Đồng thời, các mẫu mới này ngày càng kém hiệu quả hơn vì εmax(k) tăng với k.

Nhận xét 3 (Chọn kích thước của không gian giả thuyết). Số hạng thứ nhất và thứ hai đặc trưng cho sự hiệp đồng và cạnh tranh giữa các nhiệm vụ và việc cân bằng chúng là chìa khóa cho hiệu suất tốt trên một nhiệm vụ đã cho. Tăng kích thước của không gian giả thuyết giảm số hạng đầu tiên vì nó cho phép một giả thuyết đơn đồng ý dễ dàng hơn trên hai phân phối riêng biệt Pi và Pj. Tuy nhiên, điều này đến với chi phí tăng số hạng thứ hai tăng với kích thước của không gian giả thuyết.

Nhận xét 4 (Tập các nhiệm vụ hiệp đồng có thể khác nhau cho các nhiệm vụ khác nhau). Vế phải trong (6) được tối thiểu hóa cho một lựa chọn k (trong đó 1≤k≤n) cân bằng số hạng thứ nhất và thứ hai. k tối ưu có thể thay đổi với nhiệm vụ, ví dụ, một k tối ưu nhỏ cho thấy sự bất hòa nhiệm vụ, trong đó nhiệm vụ cụ thể, chẳng hạn P1 nên được huấn luyện với một tập cụ thể của các nhiệm vụ khác. Ngay cả đối với các bộ dữ liệu điển hình như CIFAR-100, việc hiểu tập lý tưởng của các nhiệm vụ để huấn luyện cùng là rất phức tạp; Hình 2 nghiên cứu điều này thực nghiệm.

Nhận xét 5 (Học tập liên tục đặc biệt thách thức do cạnh tranh nhiệm vụ). Định lý 2 cho thấy rằng không chỉ người học được chỉ ra các nhiệm vụ tuần tự, mà nó cũng có thể phải làm việc chống lại sự cạnh tranh giữa nhiệm vụ hiện tại và biểu diễn đã học trên một nhiệm vụ trước. Nó không có quyền truy cập vào các nhiệm vụ hiệp đồng từ tương lai khi học trên nhiệm vụ hiện tại. Và hơn nữa, trong các thiết lập không có phát lại dữ liệu, người học không thể được hưởng lợi từ các nhiệm vụ hiệp đồng trước một cách rõ ràng, ngoài biểu diễn mà nó đã học. Điều này gợi ý rằng người ta phải cẩn thận hơn nữa về cách biểu diễn trong học tập liên tục nên được cập nhật.

[Hình 2: Ma trận cạnh tranh giữa các nhiệm vụ cho thấy tương tác phức tạp giữa số lượng nhiệm vụ được huấn luyện cùng nhau và độ chính xác trên các nhiệm vụ cố định từ CIFAR100]

--- TRANG 6 ---
3 Model Zoo: Một người học liên tục phát triển khả năng học của nó

Định lý 2 có thể được coi như một "định lý không có bữa trưa miễn phí". Nó cho thấy người ta không nên luôn mong đợi rủi ro dư thừa được cải thiện bằng cách kết hợp dữ liệu từ các nhiệm vụ khác nhau. Định lý này cũng gợi ý một cách để giải quyết vấn đề thông qua Nhận xét 3 và 4. Nếu chúng ta học các mô hình nhỏ trên các nhiệm vụ hiệp đồng, chúng ta có thể hy vọng có mỗi nhiệm vụ được hưởng lợi từ sự hiệp đồng mà không bị suy giảm độ chính xác do cạnh tranh nhiệm vụ với các nhiệm vụ bất hòa. Model Zoo là một phương pháp đơn giản được thiết kế cho mục đích này.

Hãy giả sử rằng các nhiệm vụ P1;:::;Pn được chỉ ra tuần tự cho người học liên tục. Chúng tôi giả định rằng tất cả các nhiệm vụ có cùng miền đầu vào X nhưng có thể có các miền đầu ra khác nhau Y1;:::;Yn. Tại mỗi "giai đoạn" k, Model Zoo được thiết kế để huấn luyện sử dụng nhiệm vụ hiện tại Pk và một tập con của các nhiệm vụ trước. Ví dụ, tại giai đoạn k = 2, chúng tôi huấn luyện một mô hình với một bộ tạo đặc trưng h và các bộ phân loại cụ thể cho nhiệm vụ để thu được các mô hình g1∘h:X→Y1 và g2∘h:X→Y2. Mô hình này có thể phân loại đầu vào từ cả hai nhiệm vụ và đưa ra một vector xác suất pgih(y|x);∀y∈Yi tùy thuộc vào nhiệm vụ. Chúng tôi giả định rằng danh tính của nhiệm vụ được biết tại thời điểm kiểm tra.

[Hình 3: Sơ đồ minh họa ý tưởng Model Zoo với các mô hình được huấn luyện trên các tập nhiệm vụ khác nhau]

Gọi tập các nhiệm vụ được xem xét tại giai đoạn k được ký hiệu bởi Pk={P^{ω1_k};:::;P^{ωb_k}} trong đó bk là một siêu tham số và ωi_k∈{1;:::;k}. Huấn luyện trên Pk sẽ bao gồm, như ví dụ trên, huấn luyện một mô hình với một bộ tạo đặc trưng hk và các bộ phân loại cụ thể cho nhiệm vụ gk;ωi_k cho mỗi nhiệm vụ được chọn trong vòng đó. Các mô hình như vậy, một mô hình được huấn luyện trong mỗi vòng, cùng nhau tạo thành "Model Zoo". Sau k vòng, dữ liệu từ, chẳng hạn, Pi với i≤k có thể được dự đoán bằng cách sử dụng trung bình của các xác suất lớp được đưa ra bởi tất cả các mô hình đã được khớp trên nhiệm vụ đó, tức là,

pk,i(y|x) ∝ Σ_{l=1}^k 1{Pi∈Pl} gl,i∘hl(x):                                   (7)

Biểu thức này cũng được sử dụng để dự đoán tại thời điểm kiểm tra.

Chọn các nhiệm vụ để huấn luyện cùng cho mỗi vòng sử dụng boosting Về nguyên tắc, chúng ta có thể sử dụng các số mũ chuyển giao εij để chọn các nhiệm vụ hiệp đồng, nhưng việc tính toán các số mũ chuyển giao về cơ bản khó khăn như việc huấn luyện trên tất cả các nhiệm vụ, một người học liên tục không có quyền truy cập vào tất cả các nhiệm vụ trước. Do đó chúng tôi phát triển một cách tự động để chọn các nhiệm vụ trong mỗi vòng. Chúng tôi lấy cảm hứng từ boosting (Schapire và Freund, 2013) cho mục đích này. Nhớ lại thuật toán AdaBoost xây dựng một tập hợp các học viên yếu (chúng có thể là bất kỳ người học nào về nguyên tắc Mason et al. (1999)), mỗi học viên được khớp trên dữ liệu huấn luyện được tái trọng số lặp đi lặp lại (Breiman, 1998). Chúng tôi nghĩ về các mô hình đã học tại mỗi giai đoạn của học tập liên tục trong Model Zoo như các "học viên yếu" và mỗi vòng boosting như tương đương với mỗi giai đoạn của học tập liên tục. Gọi wk∈Rn là một vector trọng số cụ thể cho nhiệm vụ được chuẩn hóa. Sau giai đoạn k

wk,i ∝ exp(-1/m Σ_{(x,y)∈Si} log pk,i(y|x)):                                  (8)

cho mỗi nhiệm vụ Pi với i≤k; cho i > k, wk,i = 0. Các nhiệm vụ cho vòng tiếp theo Pk+1 được rút từ một phân phối đa thức với trọng số wk. Do đó, các nhiệm vụ có rủi ro thực nghiệm thấp dưới Model Zoo hiện tại nhận được trọng số thấp cho vòng boosting tiếp theo. Giống như AdaBoost đẩy lỗi huấn luyện trên tất cả các mẫu xuống không một cách cấp số nhân (Schapire và Freund, 2013) bằng cách tập trung lặp đi lặp lại vào các mẫu khó phân loại, Model Zoo đạt được rủi ro thực nghiệm thấp trên tất cả các nhiệm vụ khi thêm nhiều mô hình.

Tính năng chính của Model Zoo là nó tự động chia khả năng trên các tập nhiệm vụ. Ngay cả khi các nhiệm vụ cạnh tranh được chọn trong một vòng, có thể dẫn đến rủi ro dư thừa cao trên một số nhiệm vụ, nó sẽ được chọn lại trong các vòng tương lai nếu nó có lỗi lớn dưới tập hợp. Nói theo cách thông tục, tập hợp trong Model Zoo đại diện cho một "bộ não" phát triển khả năng học liên tục khi nhiều nhiệm vụ được chỉ ra cho nó.

Nhận xét 6 (Các giả định trong công thức của Model Zoo). Chúng tôi giả định rằng, cả tại thời điểm huấn luyện và thời điểm kiểm tra, danh tính của nhiệm vụ được người học liên tục biết. Dữ liệu từ các nhiệm vụ trước cũng được

--- TRANG 7 ---
lưu trữ với danh tính nhiệm vụ. Đây được gọi là thiết lập tăng dần nhiệm vụ trong tài liệu (Van de Ven và Tolias, 2019). Công trình gần đây trong học tập liên tục cũng nghiên cứu các thiết lập trong đó danh tính nhiệm vụ như vậy không được biết, ví dụ, (Kaushik et al., 2021), Model Zoo không được thiết kế để xử lý các thiết lập như vậy.

4 Xác thực Thực nghiệm

4.1 Thiết lập

Bộ dữ liệu Chúng tôi đánh giá trên Rotated-MNIST (Lopez-Paz và Ranzato, 2017), Split-MNIST (Zenke et al., 2017), Permuted-MNIST (Kirkpatrick et al., 2017), Split-CIFAR10 (Zenke et al., 2017), Split-CIFAR100∗ (Zenke et al., 2017), Coarse-CIFAR100 (Rosenbaum et al., 2017; Yoon et al., 2019; Shanahan et al., 2021) và Split-miniImagenet (Vinyals et al., 2016; Chaudhry et al., 2019b). Split-MNIST, Split-CIFAR10, Split-CIFAR100 và Split-miniImagenet sử dụng các nhóm nhãn liên tiếp (lần lượt là 2, 2, 5 và 10) để tạo thành các nhiệm vụ. Coarse-CIFAR100 là một biến thể của CIFAR100 trong đó mỗi siêu lớp được coi là một nhiệm vụ khác nhau (Yoon et al., 2019; 2021; Shanahan et al., 2021). Nghiên cứu của chúng tôi trong Hình 2 đã phát hiện rằng Coarse-CIFAR100 là một bộ dữ liệu khó cho học tập liên tục, có lẽ vì sự khác biệt ngữ nghĩa giữa các siêu lớp khác nhau.

Kiến trúc mạng nơ-ron và phương pháp huấn luyện Chúng tôi sử dụng mạng residual rộng nhỏ của Zagoruyko và Komodakis (2016) (WRN-16-4 với 3.6M trọng số) với các bộ phân loại cụ thể cho nhiệm vụ (một lớp kết nối đầy đủ). Chúng tôi cũng sử dụng một mạng nhỏ hơn nữa (0.12M trọng số) với 3 lớp tích chập (kích thước kernel 3 và 80 bộ lọc) xen kẽ với các lớp max-pooling, ReLU, batch-norm, với các lớp bộ phân loại cụ thể cho nhiệm vụ. Stochastic gradient descent (SGD) với momentum Nesterov và tỷ lệ học cosine-annealed được sử dụng để huấn luyện tất cả các mô hình với độ chính xác hỗn hợp. Ray Tune (Liaw et al., 2018) được sử dụng để tối ưu hóa siêu tham số sử dụng một mô hình học đa nhiệm vụ trên tất cả các nhiệm vụ từ Coarse CIFAR-100. Khi chúng tôi thực hiện phát lại đầy đủ, Model Zoo lấy mẫu b = min(k;5) nhiệm vụ tại giai đoạn thứ k; đối với các bài toán có n = 5 nhiệm vụ, chúng tôi đặt b = 2; lưu ý rằng b = 1 cho thấy không có phát lại dữ liệu. Tất cả các siêu tham số được giữ cố định cho tất cả các bộ dữ liệu và tất cả các thí nghiệm (xem §4.2).

Xem Phụ lục A để biết thêm chi tiết.

4.2 Đánh giá các phương pháp học tập liên tục

Có sự đa dạng rộng rãi của các công thức bài toán trong tài liệu học tập liên tục (Farquhar và Gal, 2019a; Prabhu et al., 2020; Vogelstein et al., 2020; Lopez-Paz và Ranzato, 2017; Van de Ven và Tolias, 2019). Các công thức khác nhau về việc liệu chúng cho phép phát lại dữ liệu từ các nhiệm vụ trước, số epoch mà người học được phép huấn luyện mỗi nhiệm vụ, và khả năng của mô hình được khớp. Chúng tôi tiếp theo giải thích các công thức khác nhau này, lý do đằng sau chúng, và cách chúng tôi thực hiện Model Zoo để phù hợp với mỗi thiết lập này.

(i) Công thức nghiêm ngặt, ví dụ, Kirkpatrick et al. (2017); Kaushik et al. (2021), không cho phép bất kỳ phát lại dữ liệu nào. Đối với công thức nghiêm ngặt của Model Zoo, chúng tôi đơn giản đặt wk,i = 0 cho tất cả i≠k trong (8). Tại mỗi giai đoạn, một mô hình đơn được huấn luyện trên nhiệm vụ hiện tại và được thêm vào zoo—chúng tôi gọi người học khá đơn giản này là Isolated. Từ quan điểm thực tế, một công thức như vậy áp đặt ràng buộc về lượng tài nguyên tính toán (tính toán và/hoặc bộ nhớ) có sẵn trong quá trình huấn luyện.

(ii) Người ta có thể phát lại dữ liệu ở các mức độ khác nhau, ví dụ, tất cả (Nguyen et al., 2017; Guo et al., 2020b), hoặc một tập con của nó (Chaudhry et al., 2019a). Giống như AdaBoost, Model Zoo được thiết kế cơ bản để cho phép phát lại đầy đủ các nhiệm vụ trước. Tuy nhiên, chúng tôi có thể dễ dàng thực hiện nó với phát lại hạn chế bằng cách chỉ sử dụng một tập con dữ liệu để tính toán cập nhật gradient và cũng là độ chính xác trên các nhiệm vụ trước trong giai đoạn thứ k. Chúng tôi sử dụng danh pháp Model Zoo (10% replay) để cho thấy chỉ 10% dữ liệu từ các nhiệm vụ trước được sử dụng; các thuật toán như A-GEM (Chaudhry et al., 2019a) cũng sử dụng 10% dữ liệu trước trên các bộ dữ liệu CIFAR100. Xem Phụ lục A.4 để biết chi tiết thực hiện. Lưu ý rằng Model Zoo mà không có bất kỳ phát lại dữ liệu nào đơn giản là

--- TRANG 8 ---
Isolated. Hãy để chúng tôi nhấn mạnh rằng qua tất cả các thiết lập bài toán này, Model Zoo vẫn là một người học liên tục hợp pháp vì nó có quyền truy cập vào mỗi nhiệm vụ tuần tự và có ngân sách tính toán cố định (b nhiệm vụ) tại mỗi giai đoạn. Đối với một người học đa nhiệm vụ, độ phức tạp tính toán tỷ lệ với số lượng nhiệm vụ.

(iii) Để áp đặt ràng buộc nghiêm ngặt về độ phức tạp tính toán của mỗi giai đoạn một số công trình, ví dụ, Chaudhry et al. (2019a), huấn luyện mỗi nhiệm vụ cho một epoch đơn. Do đó chúng tôi chỉ ra kết quả sử dụng cả Model Zoo (single epoch) (trong đó chúng tôi phát lại dữ liệu trước cho 1 epoch) và Isolated (single epoch) (không phát lại). Ngay cả khi lý do đằng sau việc sử dụng mỗi dữ liệu chỉ một lần là có cơ sở, một epoch đơn khá không đủ để huấn luyện các mạng sâu hiện đại; nếu người ta nghĩ về các cân nhắc sinh học, các thuật toán descent cục bộ như stochastic gradient descent (SGD) khá khác biệt so với các mạch lặp trong não sinh học (Kietzmann et al., 2019). Chúng tôi cũng chạy các phương pháp epoch đơn sử dụng một mô hình rất nhỏ (0.12M trọng số); đây là Model Zoo/Isolated-small (single epoch).

(iv) Multi-Head huấn luyện một mô hình đơn trên tất cả các nhiệm vụ để tối thiểu hóa rủi ro thực nghiệm trung bình với các bộ phân loại cụ thể cho nhiệm vụ; các mini-batch chứa mẫu từ các nhiệm vụ khác nhau. Vì Multi-Head được huấn luyện trên tất cả các nhiệm vụ cùng nhau, nó không phải là một người học liên tục, nhưng độ chính xác của nó được mong đợi là một giới hạn trên của độ chính xác của các phương pháp học tập liên tục.

Tiêu chí đánh giá Chúng tôi so sánh các thuật toán về độ chính xác validation được lấy trung bình trên tất cả các nhiệm vụ vào cuối tất cả các giai đoạn, chuyển giao tiến trung bình mỗi nhiệm vụ (độ chính xác trên một nhiệm vụ mới khi nó được thấy lần đầu, càng lớn số này càng nhiều chuyển giao tiến), quên trung bình mỗi nhiệm vụ (khoảng cách trong độ chính xác tối đa của một nhiệm vụ trong quá trình học tập liên tục và độ chính xác của nó ở cuối, càng lớn số này càng nhiều quên và càng tệ chuyển giao lùi), thời gian huấn luyện và suy luận, và bộ nhớ. Hãy để chúng tôi lưu ý rằng chuyển giao tiến đôi khi cũng được gọi là "độ chính xác học" (Riemer et al., 2018), và một thước đo khác của chuyển giao lùi là khoảng cách giữa độ chính xác ở cuối huấn luyện và độ chính xác ban đầu của nhiệm vụ.

4.3 Kết quả

Bảng 1 cho thấy độ chính xác validation của các phương pháp học tập liên tục khác nhau trên các bài toán benchmark tiêu chuẩn. Có nhiều quan sát đáng chú ý ở đây.

(i) Độ chính xác của các phương pháp hiện có so với trong Bảng 1 (xem ?? cũng như) kém hơn Isolated. Điều này đáng ngạc nhiên vì Isolated có thể được coi là người học liên tục đơn giản nhất có thể—người mở khóa khả năng mới tại mỗi giai đoạn và không phát lại dữ liệu. Điều này cho thấy các phương pháp hiện có có thể đang thất bại trong việc đạt được chuyển giao tiến hoặc lùi so với việc đơn giản huấn luyện nhiệm vụ một cách riêng lẻ; Bảng 2 điều tra điều này sâu hơn.

(ii) Ngược lại, Model Zoo (cả ba biến thể: small, small với 10% phát lại dữ liệu và phương pháp tiêu chuẩn) có độ chính xác tốt hơn so với cả các phương pháp hiện có cũng như Isolated. Điều này cho thấy tiện ích của việc chia khả năng của người học trên nhiều nhiệm vụ.

(iii) Model Zoo khớp với độ chính xác của người học đa nhiệm vụ trong hàng cuối của Bảng 1 có quyền truy cập vào tất cả các nhiệm vụ trước. Đáng ngạc nhiên, Model Zoo hoạt động tốt hơn Multi-Head mặc dù được huấn luyện theo cách liên tục, đặc biệt trên các bài toán khó hơn như Coarse-CIFAR100 và Split-miniImagenet. Đây là một minh chứng trực tiếp về hiệu quả của Model Zoo trong việc giảm thiểu cạnh tranh nhiệm vụ: cơ chế chia khả năng không chỉ tránh quên thảm khốc, mà nó cũng có thể tận dụng dữ liệu từ các nhiệm vụ khác ngay cả khi chúng được chỉ ra tuần tự.

Bảng 2 cho thấy so sánh các phương pháp được phát triển trong bài báo này với các phương pháp hiện có trên Split-CIFAR100 về các thước đo cụ thể cho học tập liên tục. Chúng tôi thấy:

(i) Không có sự khác biệt đáng kể trong hiệu suất chuyển giao tiến trong thiết lập epoch đơn; các biến thể lớn hơn của Isolated và Model Zoo không hoạt động tốt ở đây vì một epoch đơn không đủ để huấn luyện các mạng sâu hiện đại. Nhưng Model Zoo và các biến thể cho thấy ít quên hơn, nó về cơ bản bằng không. Điều này cho thấy mặc dù các phương pháp hiện có được thiết kế để tránh quên (thiết lập epoch đơn hỗ trợ điều này trực tiếp), chẳng hạn, A-GEM, hoặc EWC, chúng vẫn quên. Quên có thể được giảm thiểu bởi cơ chế chia khả năng trong Model Zoo. Độ chính xác mỗi nhiệm vụ của các phương pháp hiện có cũng khá thấp so với các biến thể Model Zoo.

--- TRANG 9 ---
[Bảng 1: Độ chính xác trung bình mỗi nhiệm vụ (%) vào cuối tất cả các giai đoạn - bảng lớn với kết quả so sánh giữa các phương pháp khác nhau]

--- TRANG 10 ---
[Bảng 2: So sánh các thước đo đánh giá học tập liên tục trên Split-CIFAR100]

[Hình 4: Các nghiên cứu loại bỏ cho thấy độ chính xác trung bình mỗi nhiệm vụ khi thay đổi kích thước phát lại dữ liệu, số nhiệm vụ trước được lấy mẫu, và so sánh với ensemble]

(ii) Nếu các phương pháp của chúng tôi được thực hiện trong thiết lập nhiều epoch, thì chuyển giao tiến đặc biệt tốt và gần như tốt bằng độ chính xác trung bình của nhiệm vụ. Đáng ngạc nhiên, điều này không đến với chi phí quên, mà lại về cơ bản bằng không.

(iii) Ngay cả khi Model Zoo và các biến thể của nó được thực hiện với các mô hình rất nhỏ (0.12M trọng số/giai đoạn, tức là 2.42M trọng số/20 giai đoạn), độ chính xác vẫn tốt hơn (Bảng 1). Điều này gợi ý rằng Model Zoo là một phương pháp hiệu quả và khả thi cho học tập liên tục. Thực tế, ngay cả mô hình lớn hơn được sử dụng trong Model Zoo là WRN-16-4 với 3.6M trọng số và do đó chúng tôi có thể huấn luyện nhiều mô hình trên cùng một GPU dễ dàng; đây là lý do tại sao thời gian huấn luyện của Model Zoo gần giống với Model Zoo-small.

(iv) Sự đơn giản của Model Zoo và các biến thể của nó dẫn đến thời gian huấn luyện nhỏ hơn nhiều và thời gian suy luận tương đương so với các phương pháp hiện có.

5 Công trình Liên quan

Công trình lý thuyết về học từ nhiều nhiệm vụ Các công trình như Baxter (2000); Maurer (2006), hoặc các công trình gần đây như Du et al. (2020); Tripuraneni et al. (2020) nghiên cứu một bộ tạo đặc trưng chia sẻ với các bộ phân loại cụ thể cho nhiệm vụ, và chỉ ra rằng độ phức tạp mẫu của việc học một nhiệm vụ cải thiện nếu

--- TRANG 11 ---
các bộ phân loại cụ thể cho nhiệm vụ thực sự đủ đa dạng. Cũng được đánh giá cao rằng một bộ tạo đặc trưng chia sẻ như vậy có thể không tồn tại cho các nhiệm vụ khác biệt. Vì vậy một quan điểm khác về vấn đề có thể được tìm thấy trong Crammer et al. (2008); Ben-David et al. (2010); Ben-David và Borbely (2008) cho thấy việc học các nhiệm vụ đa dạng đòi hỏi một bộ tạo đặc trưng lớn hơn và do đó nhiều mẫu hơn; chúng tôi thảo luận điều này trong §2.2. Chúng tôi xây dựng dựa trên Hanneke và Kpotufe (2019; 2020) để xây dựng số mũ chuyển giao trong §2; công trình của họ chỉ ra rằng ngay cả trong các thiết lập rất thuận lợi, ví dụ, khi tất cả các nhiệm vụ có cùng bộ phân loại tối ưu, việc có quyền truy cập vào một số lượng lớn các nhiệm vụ có thể không giúp ích. Model Zoo bị ảnh hưởng mạnh mẽ từ các kết quả này và chúng tôi nghĩ về nó về cơ bản như một cách để vượt qua chúng.

Có một số công cụ thuật toán để ước tính mối liên quan nhiệm vụ, ví dụ, (Evgeniou et al., 2005; Cavallanti et al., 2010; Kumar và Daume III, 2012), và mặc dù các phương pháp như vậy phổ biến trong học chuyển giao (Pentina và Lampert, 2015; Jaakkola và Haussler, 1999), người ta không thể áp dụng chúng trong học tập liên tục vì chúng ta không biết các nhiệm vụ trước. Như §2 chỉ ra, mối liên quan nhiệm vụ rất quan trọng đối với việc học tốt. Vì vậy, lấy cảm hứng từ AdaBoost (Schapire và Freund, 2013), Model Zoo sử dụng một chỉ báo đơn giản về những nhiệm vụ trước nào có thể được hưởng lợi từ những nhiệm vụ tương lai, đó là những nhiệm vụ có độ chính xác thấp dưới tập hợp hiện tại.

Quên thảm khốc đã là trọng tâm của một số kỹ thuật học tập liên tục, ví dụ, những kỹ thuật dựa trên bộ nhớ episodic (Lopez-Paz và Ranzato, 2017; Chaudhry et al., 2019a; Farajtabar et al., 2020; Guo et al., 2020a), phát lại dữ liệu (Robins, 1995; Shin et al., 2017; Lee et al., 2017), kiến trúc mới (Serra et al., 2018), dựa trên phát lại tạo sinh (Mocanu et al., 2016; Shin et al., 2017; Liu et al., 2020; Ven et al., 2020), dựa trên ensemble (Aljundi et al., 2017; Wen et al., 2020) và các phương pháp chọn các hướng dư thừa cục bộ trong không gian trọng số (Kirkpatrick et al., 2017; Aljundi et al., 2018; Mallya et al., 2018; Zenke et al., 2017; Chaudhry et al., 2018). Các phương pháp biến phân, ví dụ, (Nguyen et al., 2017; Farquhar và Gal, 2019b), cập nhật tuần tự một posterior trên các trọng số và có nền tảng elegant trong các phương pháp Bayesian nhưng việc thực hiện chúng cho các bộ dữ liệu lớn vẫn là một thách thức.

Mặc dù hoạt động tích cực, một giải pháp hiệu quả cho quên vẫn phần lớn chưa được biết đến.

Model Zoo chấp nhận thực tế rằng quên là một hiện tượng cơ bản của việc học nhiều nhiệm vụ và do đó việc chia khả năng có thể là cần thiết; kết quả của chúng tôi cho thấy phương pháp này hiệu quả trong việc giải quyết quên. Phương pháp này cũng cải thiện đáng kể các thước đo chính khác, ví dụ, chuyển giao tiến-lùi và độ phức tạp tính toán của huấn luyện và suy luận đã nhận được sự chú ý hạn chế (Díaz-Rodríguez et al., 2018). Hãy để chúng tôi lưu ý rằng Model Zoo được thiết kế cho thiết lập học tập liên tục tăng dần nhiệm vụ (Van de Ven và Tolias, 2019).

Chia sẻ/cô lập tham số Một bộ tạo đặc trưng chia sẻ đơn (tức là chia sẻ tham số cứng) là một kiến trúc phổ biến (Kirkpatrick et al., 2017; Lopez-Paz và Ranzato, 2017; Rebuffi et al., 2017a; Nguyen et al., 2017; Mirzadeh et al., 2020b; Chaudhry et al., 2019b). Đã được công nhận rằng điều này không đủ; điều này đã dẫn đến các phương pháp cho chia sẻ tham số mềm hoặc thiết kế hoặc học các kiến trúc định tuyến chuyên biệt (Rosenbaum et al., 2017; Sun et al., 2019; Fernando et al., 2017; Devin et al., 2017; Misra et al., 2016; Vandenhende et al., 2019). Model Zoo là một thực hiện rất đơn giản của cô lập tham số, hoặc phát triển (Rusu et al., 2016; Mallya và Lazebnik, 2018; Xu và Zhu, 2018). Model Zoo huấn luyện trên một giai đoạn và không bao giờ cập nhật mô hình lại nhưng độ chính xác của nó đóng vai trò trong việc xác định liệu một mô hình mới có nên được sử dụng cho nhiệm vụ trước đó hay không. Để mở rộng sự tương tự, giống như các kiến trúc chia sẻ tham số mềm sử dụng, chẳng hạn xung đột gradient (Aljundi et al., 2018) hoặc attention (Serra et al., 2018), để xác định synapses nào để chia sẻ, Model Zoo sử dụng loss huấn luyện của tập hợp để quyết định nhiệm vụ nào mô hình mới nên được huấn luyện.

6 Thảo luận

Học tập liên tục là một vấn đề quan trọng khi các hệ thống học sâu chuyển từ mô hình truyền thống có một mô hình cố định thực hiện suy luận trên các truy vấn người dùng sang các thiết lập mà chúng ta muốn cập nhật mô hình để xử lý các loại truy vấn mới. Các yêu cầu chính của một hệ thống như vậy rõ ràng: nó phải hiển thị độ chính xác cao mỗi nhiệm vụ và chuyển giao tiến-lùi mạnh mẽ. Bài báo này tìm cách phát triển một người học liên tục như vậy và điều tra vấn đề thông qua lăng kính của mối liên quan nhiệm vụ. Nó lập luận rằng người học phải chia khả năng của mình trên các tập nhiệm vụ để giảm thiểu cạnh tranh giữa các nhiệm vụ và được hưởng lợi từ sự hiệp đồng giữa chúng. Chúng tôi phát triển Model Zoo, là một thuật toán học tập liên tục lấy cảm hứng từ AdaBoost, phát triển một tập hợp các mô hình, mỗi mô hình được huấn luyện trên dữ liệu từ giai đoạn hiện tại cùng với một tập con của các nhiệm vụ trước. Chúng tôi chỉ ra rằng trên nhiều bộ dữ liệu, công thức bài toán và tiêu chí đánh giá khác nhau, Model Zoo và các biến thể của nó vượt trội hơn các phương pháp học tập liên tục hiện có. Chúng tôi cũng chỉ ra rằng một phương pháp baseline đơn giản, trong đó một mô hình nhỏ riêng biệt được huấn luyện độc lập trong mỗi giai đoạn, vượt trội hơn một số phương pháp liên tục hiện có. Phụ lục D thảo luận thêm về các kết quả này.

Tài liệu tham khảo

[Danh sách các tài liệu tham khảo được dịch sang tiếng Việt...]

--- TRANG 12-29 ---
[Tiếp tục dịch các phần còn lại của tài liệu bao gồm các phụ lục, bảng, hình và chứng minh...]
