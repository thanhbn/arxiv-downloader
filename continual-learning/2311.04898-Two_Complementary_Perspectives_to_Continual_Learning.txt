# 2311.04898.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2311.04898.pdf
# File size: 19273030 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Two Complementary Perspectives to Continual Learning:
Ask Not Only What to Optimize, But Also How
Timm Hess Tinne Tuytelaars Gido M. van de Ven
KU Leuven KU Leuven KU Leuven
Abstract
Recent years have seen considerable progress in
the continual training of deep neural networks,
predominantly thanks to approaches that add re-
play or regularization terms to the loss function
to approximate the joint loss over all tasks so far.
However, we show that even with a perfect ap-
proximation to the joint loss, these approaches
still suffer from temporary but substantial forget-
ting when starting to train on a new task. Moti-
vated by this ‘stability gap’, we propose that con-
tinual learning strategies should focus not only
on the optimization objective, but also on the way
this objective is optimized. While there is some
continual learning work that alters the optimiza-
tion trajectory (e.g., using gradient projection
techniques), this line of research is positioned as
alternative to improving the optimization objec-
tive, while we argue it should be complementary.
In search of empirical support for our proposi-
tion, we perform a series of pre-registered exper-
iments combining replay-approximated joint ob-
jectives with gradient projection-based optimiza-
tion routines. However, this first experimental at-
tempt fails to show clear and consistent benefits.
Nevertheless, our conceptual arguments, as well
as some of our empirical results, demonstrate the
distinctive importance of the optimization trajec-
tory in continual learning, thereby opening up a
new direction for continual learning research.
1 INTRODUCTION
Learning continually from a stream of non-stationary data
is challenging for deep neural networks. When these net-
works are trained on something new, their default be-
Proceedings of the 1stContinualAI Unconference, 2023, Virtual.
PMLR V olume 249. Copyright 2023 by the author(s).haviour is to quickly forget most of what was learned be-
fore (McCloskey and Cohen, 1989; Ratcliff, 1990). Con-
siderable progress has been made in recent years towards
overcoming such ‘catastrophic forgetting’, for a large part
thanks to methods using replay (Robins, 1995; Rolnick
et al., 2019) and regularization (Kirkpatrick et al., 2017;
Li and Hoiem, 2017). These methods work by adding extra
terms to the loss function, and they can be interpreted as
attempts to approximate the joint loss over all tasks so far.
Recently a peculiar property of replay and regularization
methods was pointed out. These approaches tend to suf-
fer from substantial forgetting when starting to learn a new
task, although this forgetting is often temporary and fol-
lowed by a phase of performance recovery (De Lange et al.,
2023). We postulate that avoiding this ‘stability gap’ is im-
portant, both because the transient drops in performance
themselves can be problematic (e.g., for safety-critical ap-
plications) and because doing so might lead to more effi-
cient and better performing algorithms, as constantly hav-
ing to re-learn past tasks seems wasteful. Importantly, how-
ever, we demonstrate that the stability gap cannot be over-
come by merely improving replay or regularization. This
motivates us to propose that, instead, continual learning
needs an additional perspective: rather than focusing only
onwhat to optimize (i.e., the optimization objective), the
field should also think about how to optimize (i.e., the op-
timization trajectory).
There are existing works that explore modifying the opti-
mization trajectory as a mechanism for continual learning,
but so far this line of research has been positioned as alter-
native to improving the optimization objective, rather than
as complementary. A prime example is Gradient Episodic
Memory (GEM; Lopez-Paz and Ranzato, 2017), which al-
ters the optimization process by projecting gradients to en-
courage parameter updates that do not strongly interfere
with old tasks. Crucially, GEM applies this optimization
routine to optimizing the loss on the new task, while ac-
cording to our proposal such modified optimization rou-
tines should be used to optimize approximations of the joint
loss. As a first evaluation of the merits of our proposition,
in this work we use GEM’s gradient projection-based op-
timization routine to instead optimize replay-approximatedarXiv:2311.04898v2  [cs.LG]  21 Jun 2024

--- PAGE 2 ---
Two Complementary Perspectives to Continual Learning
versions of the joint loss. In a series of pre-registered exper-
iments, using both domain- and class-incremental learning
benchmarks, we test whether this combined approach re-
duces the stability gap, and whether this in turn leads to
higher learning efficiency and better final performance.
The remainder of the paper is organized as follows. In sec-
tion 2 we develop our main proposition. In section 3 we
review existing optimization-based methods for continual
learning. In section 4 we propose experiments of which
we hope that they can provide proof-of-concept demonstra-
tions for our main proposition, and in section 5 we describe
the detailed pre-registered experimental protocol. In sec-
tion 6 we present the results, and we end with a discussion
and outlook in section 7.
2 TWO PERSPECTIVES TO
CONTINUAL LEARNING
In this section we use conceptual arguments and prelimi-
nary data to develop the proposition that continual learning
should focus not only on what to optimize, but also on how.
We first describe the current dominant approach to contin-
ual learning (subsection 2.1), we then point out a funda-
mental issue with this approach (subsection 2.2), and we fi-
nally propose a complementary approach and explain why
it could address this issue (subsection 2.3).
To help us reason about the different approaches that con-
tinual learning methods could take, in this section we con-
sider the following continual learning problem. Assume a
model fw, parameterized by w, that has learned a set of
weights bwoldfor an initial task1, or a set of tasks, by opti-
mizing a loss function ℓoldon training data Dold∼ D old.
We then wish to continue training the same model on a
new task, by optimizing a loss function ℓnewon training
dataDnew∼ D new, in such a way that the model main-
tains (or possibly improves) its performance on the previ-
ously learned task(s). As has been thoroughly described
in the continual learning literature, if the model is trained
on the new task in the standard way (i.e., optimize the new
lossℓnewwith stochastic gradient descent), the typical result
is catastrophic forgetting and a solution bwnewthat is good
for the new task but no longer for the old one(s).
2.1 The Standard Approach to Continual Learning:
Improving the Loss Function
To mitigate catastrophic forgetting, continual learning re-
search from the past few years has typically focused on
making changes to the loss function that is optimized. In
particular, rather than optimizing the loss on the new task,
many continual learning methods can be interpreted as op-
1The term ‘task’ is used here in a rather general way; it loosely
refers to a combination of a data distribution and a loss function.timizing an approximate version of the joint loss:
eℓjoint=ℓnew+eℓold, (1)
witheℓoldthe method’s proxy for the loss on the old tasks.
A straight-forward example of this approach is ‘experience
replay’, which approximates ℓoldby revisiting a subset of
previously observed examples that are stored in an aux-
iliary memory buffer. In continual learning experiments,
typically limits are imposed on the buffer’s storage capacity
and/or on the computational budget for training the model
(Lesort et al., 2020; Wang et al., 2022; Prabhu et al., 2023).
Both constraints prevent full replay of all previously ob-
served data, meaning that ℓoldcan only be approximated.
A wide range of studies aims to improve the quality of this
approximation, for example by modifying the way samples
are selected to be stored in the buffer (Rebuffi et al., 2017;
Chaudhry et al., 2019b; Aljundi et al., 2019b; Lin et al.,
2021; Mundt et al., 2023), or by adaptively selecting which
samples from the buffer to replay (Riemer et al., 2018;
Aljundi et al., 2019a). As an alternative to storing past
samples explicitly, generative models can be learned to ap-
proximate the input distributions of previous tasks (Robins,
1995; Shin et al., 2017; van de Ven et al., 2020).
Another popular class of methods for continual learning
is based on regularization. As proxy for the loss on the
old tasks, these methods add regularizing terms to the
loss that impose penalties either for changes to the net-
work’s weights (‘parameter regularization’; Kirkpatrick
et al., 2017; Zenke et al., 2017; Aljundi et al., 2018) or for
changes to the network’s input-output mapping (‘functional
regularization’; Li and Hoiem, 2017; Dhar et al., 2019; Lee
et al., 2019; Titsias et al., 2020). That these methods can
be interpreted as attempts to approximate the joint loss can
be shown by taking either a Bayesian perspective (Nguyen
et al., 2018; Farquhar and Gal, 2019; Kao et al., 2021; Rud-
ner et al., 2022) or a geometric perspective (Kolouri et al.,
2020).
In summary, both replay- and regularization-based methods
for continual learning operate by changing the loss func-
tion that is optimized, often with the aim of creating an ap-
proximate version of the joint loss. When developing new
continual learning methods of this kind, the challenge is to
design better objective functions.
2.2 The Stability Gap: A Challenge for the Standard
Approach to Continual Learning
It was recently pointed out that even when replay- or
regularization-based methods are considered to perform
well (in the sense that they obtain good performance on
both old and new tasks after finishing training on the new
task), these methods still suffer from substantial, albeit of-
ten temporary, forgetting during the initial phase of training
on a new task (De Lange et al., 2023). Until recently, this

--- PAGE 3 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
Figure 1: The stability gap occurs even with incremen-
tal joint training (or ‘full replay’). Shown is the test ac-
curacy on the first task while the network is incrementally
trained on all five tasks of Domain CIFAR. During the n-th
task, the network is trained jointly on all training data from
the first ntasks. Even with this ideal approximation to ℓjoint,
performance severely drops upon encountering a new task.
Displayed are the means over five repetitions, shaded areas
are±1standard error of the mean. Vertical dashed lines
indicate task switches.
phenomenon – referred to as the stability gap – had not
been observed, or been paid little attention to, due to the
common evaluation setup in continual learning that only
measures performance after training on the new data has
converged. Nevertheless, the stability gap is undesirable,
especially for safety-critical applications in which sudden
drops in performance can be highly problematic. But also
from the perspective of computational efficiency, or even if
only the final learning outcome is of interest, avoiding the
stability gap might be beneficial, as preventing forgetting
seems easier and more efficient than having to re-learn later
on (see Figure 4 of Van de Ven et al. (2020) for empirical
support for this intuition).
Why does the stability gap happen? One possibility is that
the stability gap is due to imprecision in the approximations
of the joint loss made by replay and regularization. If this
were the case, it could be interpreted as good news for the
standard approach to continual learning, as it would imply
that by continuing to improve the quality of replay or reg-
ularization the stability gap could be overcome. However,
this is not the case, as in preliminary experiments we find
that the stability gap is consistently observed even with in-
cremental joint training (Figure 1). This indicates that with
better approximations to the joint loss alone, the stability
gap cannot be solved.
2.3 Proposed Complementary Approach:
Improving the Optimization Trajectory
The above observations relating to the stability gap suggest
that the standard approach to continual learning of focusing
on the loss function is not sufficient. We believe that con-
tinual learning would benefit from an additional perspec-
tive: rather than concentrating only on improving the op-timization objective (i.e., what loss function to optimize),
we argue that continual learning should also focus on im-
proving the optimization trajectory (i.e., how to optimize
that loss function).
To help explain why we believe that focusing on the opti-
mization trajectory can yield benefits, we revisit the con-
tinual learning problem discussed at the start of section 2,
which is schematically illustrated in Figure 2. Starting
point is a model fwthat has already learned a solution bwold
by optimizing loss ℓold. Continuing to train this model by
optimizing loss ℓnewwould result in catastrophic forgetting.
Instead, as discussed, the standard approach in continual
learning is to optimize eℓjoint=ℓnew+eℓold, an approxima-
tion to the joint loss, rather than ℓnew. Optimizing a suit-
ably approximated version of the joint loss results in a so-
lutionbwjointthat is good for both the old and the new tasks.
However, if this loss is optimized with standard stochastic
gradient descent, the trajectory that is taken from bwoldto
bwjointgoes through a region in parameter space where the
loss on the old tasks is high. The corresponding transient
drop in performance on the old tasks is the stability gap.
A first possibility that must be dealt with is that the sta-
bility gap is unavoidable, in the sense that there simply is
no path from bwoldtobwjointthat does not traverse a region
where the performance on old tasks is poor. Although this
is theoretically possible, this option seems unlikely given
recent work on mode connectivity in deep neural networks
(Draxler et al., 2018; Garipov et al., 2018) showing that
Figure 2: Schematic of the stability gap, and how adjust-
ing the optimization trajectory could avoid it. When,
starting from a solution for the old tasks ( bwold), a proxy
of the joint loss ( eℓjoint) is optimized with standard stochas-
tic gradient descent, the optimization trajectory first passes
through a region in parameter space with high loss on the
old tasks before converging to a solution that is good for all
tasks (bwjoint). Work on mode connectivity suggests that a
low-loss path between bwoldandbwjointexists as well (dashed
arrow), indicating that it should be possible to overcome the
stability gap with a different optimization routine. Green
shading indicates areas of low loss on the old tasks.

--- PAGE 4 ---
Two Complementary Perspectives to Continual Learning
different local optima found by stochastic gradient descent
are often connected by simple paths of non-increasing loss.
In particular, Mirzadeh et al. (2021) showed that when op-
timizing a neural network using stochastic gradient descent
on the joint loss while starting from a single task solution,
the resulting joint solution is connected to the single task
solution by a linear manifold of low loss on the single task.
The same holds when, starting from a single task solution,
the replay-approximated joint loss is optimized rather than
the joint loss itself (Verwimp et al., 2021).
The above work on mode connectivity thus suggests that by
changing the optimization routine it should be possible to
avoid the stability gap. Besides that reducing the stability
gap is important for safety-critical applications, we believe
that it could bring other benefits for continual learning as
well. For example, getting rid of the repeated re-learning
cycles that characterize the stability gap could increase the
learning efficiency. Moreover, if the loss function is non-
convex, as is the case with deep neural networks, changing
the way the loss is optimized could also lead to different,
and hopefully better, final learning outcomes. The possi-
bility of improved final performance is supported by the
observation from Caccia et al. (2022) that the abrupt for-
getting after task switches is not always recovered later on.
This leads us to the following three hypotheses:
Main hypothesis
Better optimization routines for continual learning can:
(H1) reduce the stability gap.
Secondary hypotheses
Reducing the stability gap can:
(H2) increase learning efficiency;
(H3) improve the final learning outcome.
How to Improve Optimization for Continual Learning?
In the above we have made an argument that continual
learning should focus on improving its optimization rou-
tines, but we have not yet discussed howthis could be done.
To avoid the stability gap, an optimization routine is needed
that is less greedy than standard stochastic gradient descent
and that favors parameter updates that do not substantially
increase the loss on old tasks. Interestingly, as we review in
section 3, optimization routines based on gradient projec-
tion have been explored in the continual learning literature
that already have these properties. Importantly, however,
currently these gradient projection-based optimization rou-
tines are not used in the way envisioned by us, as they are
used to optimize the loss on the new task rather than an
approximated version of the joint loss.
2.4 Another Way to Avoid the Stability Gap?
An alternative approach that can circumvent the stability
gap in continual learning is to use certain parts of the net-work only for specific tasks. This approach is employed by
network expansion methods (Rusu et al., 2016; Yoon et al.,
2018; Yan et al., 2021) and parameter isolation methods
(Serra et al., 2018; Masse et al., 2018). To see why this ap-
proach can help to avoid the stability gap, consider the ex-
treme case of using a separate sub-network for each task. In
this case there is no forgetting at all, and thus also no stabil-
ity gap. However, the use of task-specific components has
some important disadvantages. Firstly, if task identity is
not always provided, as is the case with domain- and class-
incremental learning (van de Ven et al., 2022), it might not
be clear which parts of the network should be used. This is-
sue could be addressed by inferring task identity, for exam-
ple using generative models or other out-of-distribution de-
tection techniques (van de Ven et al., 2021; Henning et al.,
2021; Kim et al., 2022; Zaj ˛ ac et al., 2024), but such task
inference can be challenging. Secondly, having separate
parts of the network per task limits the potential of positive
transfer between tasks, which is an important desideratum
for continual learning (Hadsell et al., 2020).
3 GRADIENT PROJECTION-BASED
OPTIMIZATION
A tool that has been explored in the continual learning lit-
erature for modifying the way a given loss function ℓ(w)
is optimized is ‘gradient projection’. With gradient projec-
tion, rather than basing the parameter updates on the origi-
nal gradient g=∇wℓ(w), they are based on a projected
version ¯gof that gradient. Important from the perspec-
tive of our paper, gradient projection does not alter the loss
function that is optimized (e.g., the loss landscape and its
local minima remain unchanged), it only changes the way
the loss function is optimized (Kao et al., 2021). In this
section we review two current lines of continual learning
studies that make use of gradient projection.
3.1 Orthogonal Gradient Projection
Orthogonal gradient projection methods aim to avoid inter-
ference between tasks by confining the training of each new
task to previously unused subspaces. To restrict parameter
updates to directions that do not interfere with the perfor-
mance on old tasks, the gradient of the loss on the new task
is projected to the orthogonal complement of the ‘gradient
subspaces’ of old tasks. Various ways to construct these
gradient subspaces have been proposed. Several studies
use the subspaces spanned by all layer-wise inputs of old
tasks, which they characterize using conceptors (He and
Jaeger, 2018) or by iteratively accumulating projector ma-
trices using a recursive least squares algorithm (Zeng et al.,
2019; Guo et al., 2022). To reduce the memory and com-
putational costs, Saha et al. (2021) approximate the input
subspaces of each task using singular value decomposition
and its k-rank approximation. Farajtabar et al. (2020) in-

--- PAGE 5 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
stead use the span of a set of stored gradient directions of
old tasks as the gradient subspace to protect.
Constraining sequential optimization to orthogonal sub-
spaces can mitigate forgetting effectively, but it restricts
the learning of new tasks to successively smaller subspaces
and it eliminates the potential to further improve the model
with respect to old tasks. Orthogonal gradient projection
methods especially struggle or break down when the in-
put spaces of different tasks substantially overlap with each
other (He, 2018).
Recent advancements in this line of work therefore focus
on relaxing the constraints of the orthogonal projection
framework to enable knowledge transfer between tasks.
Deng et al. (2021) dynamically scale gradients and search
for flatter minima, and Lin et al. (2022) use the idea of
‘trust regions’ (Schulman et al., 2015) to selectively relax
constraints for protected subspaces of old tasks most re-
lated to the new task. As an alternative, Kao et al. (2021)
take a Bayesian perspective and transform the gradients us-
ing the inverse of a Kronecker-factored approximation to
the Fisher information matrix, and additionally protect pre-
vious knowledge by parameter-based regularization.
3.2 Gradient Episodic Memories
Another class of gradient projection-based optimization
methods for continual learning, which also does not en-
force strict orthogonality of future updates, is based on Gra-
dient Episodic Memory (GEM; Lopez-Paz and Ranzato,
2017). The projection mechanism of this approach is mo-
tivated by a constrained optimization problem, where the
goal is to optimize ℓnewwithout increasing ℓold:
minwℓnew(w),such that ℓold(w)≤ℓold(bwold).(2)
To determine whether a parameter update based on g=
∇wℓnew(w)might increase ℓold, the gradient(s) for the old
task(s) are estimated using examples from a replay buffer:
gold=∇weℓold(w). If the directions of gandgoldalign
(in the sense that their angle does not exceed 90◦), it is
conjectured that a parameter update based on gis un-
likely to increase ℓold, and gis left unchanged. If the
angle between gandgoldexceeds 90◦,gis projected to
¯g=g−gTgold
gT
oldgoldgold, which is the closest gradient to g(in
l2-norm) with a 90◦angle to gold. Because in our contin-
ual learning example it is the case that ℓoldencompasses
all past tasks, this description actually corresponds to Av-
eraged GEM (A-GEM; Chaudhry et al., 2019a), a compu-
tationally more efficient version of GEM. The original for-
mulation of GEM enforces ¯gto align with the gradient of
each individual past task (Lopez-Paz and Ranzato, 2017),
see Appendix D for details.
Given that GEM and A-GEM explicitly aim to prevent in-
creases of the loss on old tasks, these methods might beable to avoid the stability gap. Empirically, however, this is
not the case, as GEM suffers from considerably larger sta-
bility gaps than experience replay (De Lange et al., 2023).
Moreover, also in terms of final performance, experience
replay consistently outperforms both GEM and A-GEM
(De Lange et al., 2022; van de Ven et al., 2022).
We expect that the disappointing performance of GEM is
due to its choice of objective function: GEM optimizes the
loss on the new task (i.e., ℓnew) rather than an approxima-
tion to the joint loss (i.e., eℓjoint). In other words, we believe
that GEM under-utilizes its replay buffer by solely delin-
eating gradient constraints but not actively optimizing the
replay-approximated joint loss. When GEM was proposed,
it was assumed that directly optimizing a joint loss approx-
imated with a relatively small replay buffer could not work
well due to overfitting (Lopez-Paz and Ranzato, 2017), but
recent work indicates such overfitting is not as detrimental
as thought (Chaudhry et al., 2019b; Verwimp et al., 2021).
Nevertheless, as far as we are aware, changing GEM’s ob-
jective function has not been explored.
4 PROOF-OF-CONCEPT EXPERIMENTS
As discussed in the last section, the gradient projection-
based optimization routine of GEM encourages parameter
updates that do not strongly interfere with old tasks with-
out imposing overly strict constraints that would fully seg-
regate tasks. Yet, so far this optimization routine has not
been used to optimize proxies of the joint loss. This makes
GEM a convenient tool for a first set of proof-of-concept
experiments to evaluate the merits of our proposition that
continual learning should consider both what andhow to
optimize. We plan to combine GEM’s optimization routine
both with a basic version of experience replay that explic-
itly approximates the joint loss (subsection 4.1) and with
state-of-the-art replay-based methods (subsection 4.2).
4.1 Experience Replay with Gradient
Projection-based Optimization
In a first set of experiments we test whether, when the opti-
mization objective is a standard replay-approximated ver-
sion of the joint loss, using GEM’s gradient projection-
based optimization routine provides the benefits hypothe-
sized in subsection 2.3.
Approximating the Joint Loss To approximate the joint
loss we use a basic version of Experience Replay (ER). In
our implementation of ER, at the end of each task new
examples are added to the memory buffer using class-
balanced sampling from the training set, and in each train-
ing iteration uniform sampling from the buffer is used to
choose which samples to replay. To approximate the joint
loss as closely as possible, when training on the n-th task,
we balance the loss on the current data and the loss on the

--- PAGE 6 ---
Two Complementary Perspectives to Continual Learning
replayed data using eℓjoint=1
nℓnew+ (1−1
n)eℓold. In each it-
eration, the total number of replayed samples from all past
tasks combined is always equal to b, which is the size of
the mini-batch from the current task.
In addition to approximating the joint loss with ER, we also
run experiments using the joint loss itself. For this, all train-
ing data from past tasks are stored, and in each iteration we
usebsamples from each past task to compute ℓold. This can
be thought of as ‘full replay’.
Optimization Trajectory To try to improve the sub-
optimal optimization trajectory that is taken by standard
ER, we use the gradient projection-based optimization rou-
tines of GEM and A-GEM. Importantly, we only use the
optimization routines of GEM and A-GEM, not their opti-
mization objectives. As optimization objective we instead
use the replay-approximated joint loss eℓjoint. To achieve
this, in the description of GEM in the first paragraph of
subsection 3.2, we only need to replace all mentions of ℓnew
witheℓjoint. To further illustrate our proposed combination
approach, pseudocode for ER + A-GEM is provided in Al-
gorithm 1.
Algorithm 1 ER + A-GEM
Require: parameters w, loss function ℓ, learning rate λ,
data stream {D1, ..., D T}
M← {}
fort= 1, ..., T do
for(x, y)∈Dtdo
g← ∇ wℓ(fw(x), y)
(˜x,˜y)←SAMPLE (M)
gold← ∇ wℓ(fw(˜x),˜y)
gjoint←1
tg+ (1−1
t)gold
¯g←PROJECT_AGEM (gjoint, gold)
w←OPTIMIZER_STEP (w, λ, ¯g)
end for
M←UPDATE_BUFFER (M, D t)
end for
function PROJECT_AGEM (g, g ref)
ifgTgref≥0then
return g
else
return g−gTgref
gT
refgrefgref
end if
end function
Approaches to Compare The main experimental com-
parison of interest is between standard ER and our pro-
posed combination approach ER + GEM, as this allows
testing whether, when doing continual learning by optimiz-
ing a proxy of the joint loss, benefits can be gained by
changing the way this objective is optimized. Addition-
ally, to probe the individual contributions of the optimiza-
tion objective and the optimization routine, we also includeTable 1: Overview of the approaches to compare in our
proof-of-concept experiment, illustrated with ER and GEM
as base methods. GP: gradient projection.
Approximate GP-based
Method joint loss optimization
Finetuning ✗ ✗
ER ✓✓✓ ✗
GEM ✗ ✓✓✓
ER + GEM ✓✓✓ ✓ ✓✓
GEM itself and continual finetuning in our experimental
comparison. See Table 1 for an overview of the approaches
we compare. In this table ER can be replaced by ‘full re-
play’, and GEM can be replaced by A-GEM. We run ex-
periments with all combinations of these base methods.
4.2 Improving State-of-the-art
Next we ask whether the use of gradient projection-based
optimization could improve the performance of state-of-
the-art replay-based methods. To test this, we run the meth-
ods Dark Experience Replay (DER; Buzzega et al., 2020)
and Bias Correction (BiC; Wu et al., 2019) both with and
without using the optimization routine of A-GEM. For
these experiments we only consider A-GEM, as it is not
straight-forward to combine DER with the original version
of GEM. We implement DER and BiC according to their
original papers. For completeness, details for both meth-
ods are provided in Appendix B. As BiC is a method that
is specialized for class-incremental learning, it is included
only with the class-incremental learning benchmarks.
5 EXPERIMENTAL PROTOCOL
5.1 Setup
We consider a task-aware supervised continual learning
setting, with a task sequence T={T1, ..., T T}ofTdis-
joint classification tasks Tt. A fixed capacity neural net-
work model fwis incrementally trained on these tasks
with a cross-entropy classification loss. When training on
taskTt, the model has only access to the training data Dt=
{Xt, Yt}of that task and the data in the memory buffer (see
below), and the goal is to learn a model with strong perfor-
mance on all tasks T≤tencountered so far. The model may
be evaluated after any parameter update.
Benchmarks We conduct our study on four benchmarks,
covering the domain- and class-incremental learning sce-
narios (van de Ven et al., 2022). As class-incremental
learning benchmarks we use Split CIFAR-100, which is

--- PAGE 7 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
based on the CIFAR-100 dataset (Krizhevsky et al., 2009),
and Split Mini-Imagenet, which is based on Mini-Imagenet
(Vinyals et al., 2016). Both original datasets contain
50,000 RGB images of 100 classes; CIFAR-100 in res-
olution 32x32, Mini-Imagenet in resolution 84x84. For
Split CIFAR-100 the classes are divided into ten tasks with
ten classes each, for Split Mini-Imagenet the classes are
divided into twenty tasks with five classes each. In both
cases, the classes are divided over the tasks randomly, and
for each random seed a different division is used. We also
use the CIFAR-100 dataset to construct Domain CIFAR, a
domain-incremental learning benchmark. For this bench-
mark, each of the twenty super-classes of CIFAR-100
is split across five tasks, such that every task contains
one member of each super-class (i.e., there are twenty
classes per task, one from each super-class). The goal
in each task is to predict to which super-class a sample
belongs. The other domain-incremental learning bench-
mark is Rotated MNIST. Each task consists of the entire
MNIST dataset (LeCun et al., 1998) with a certain static
rotation applied. We construct three tasks with rotations
{0◦,80◦,160◦}, as De Lange et al. (2023) found these to
provoke the largest stability gaps without inducing ambi-
guity between digits 6and9(which would happen with
rotations close to 180◦).
Architectures For the Rotated MNIST benchmark, we
use a fully-connected neural network with two hidden lay-
ers of 400 ReLUs each, followed by a softmax output
layer. For the other benchmarks, following Lopez-Paz and
Ranzato (2017), we use a reduced ResNet-18 architecture.
Compared to a standard ResNet-18 (He et al., 2016), this
architecture has three times less channels in each layer and
replaces the 7×7kernel with stride of 2 in the initial convo-
lutional layer by a 3×3kernel with stride of 1. The latter
prevents an early stark information reduction for images
with small resolution. All benchmarks are trained with a
single-headed final layer that is shared between all tasks.
Memory Buffer The memory buffer can store up to
100 samples of each class. For the domain-incremental
learning benchmarks this means 100 samples of each class
per task (e.g., with Rotated MNIST, for each digit the buffer
can store 100 examples with rotation 0◦, 100 examples with
rotation 80◦and 100 examples with rotation 160◦). Excep-
tions to this are the experiments with ‘full replay’, in which
all training data are stored.
Offline & Online All experiments are run in both an ‘of-
fline version’ and an ‘online version’. In the offline version,
multiple passes over the data are allowed, and the number
of training iterations is set relatively high to encourage near
convergence for each task. In the online version only a sin-
gle epoch per task is allowed (i.e., each sample is seen just
once, with the exception if it is replayed from memory).Training Hyperparameters All models are trained us-
ing an SGD optimizer with momentum 0.9 and no weight-
decay. When gradient projection is used, this optimizer
acts on the projected gradients. Except for whitening (with
mean and standard deviation of the respective full training
sets), no data augmentations are used. Exceptions to this
are the experiments with DER and BiC, for which we use
the data augmentations described in the original papers that
proposed these methods (Buzzega et al., 2020; Wu et al.,
2019). In the offline experiments, we train with mini-batch
size 128 for around five epochs (Rotated MNIST) or ten
epochs (Domain CIFAR, Split CIFAR-100 and Split Mini-
Imagenet) per task. To be exact, we use 2000 iterations
per task for Rotated MNIST, 800 for Domain CIFAR, 400
for Split CIFAR-100, and 200 for Split Mini-Imagenet. For
each experiment in the offline setting, we sweep a set of
static learning rates {0.1,0.01,0.001}. For each experi-
ment in the online setting, we sweep both a set of mini-
batch sizes {10,64,128}and a set of static learning rates
{0.1,0.01,0.001}. In the online setting, the number of it-
erations per task is determined by the selected mini-batch
size and the number of training samples.
5.2 Evaluation
We track the performance of all methods throughout train-
ing using ‘continual evaluation’ (De Lange et al., 2023).
In particular, after every training iteration we evaluate for
each task the accuracy of the model on a hold-out test set.
Testing the Hypotheses To quantitatively compare the
stability gap of different approaches (i.e., to evaluate H1),
we use the ‘average minimum accuracy’ metric defined by
De Lange et al. (2023). For completeness, details of this
metric are provided in Appendix A. To qualitatively com-
pare the stability gaps, we plot per-task accuracy curves
with per-iteration resolution (e.g., as in Figure 1). To com-
pare the learning efficiency of different approaches (i.e., to
evaluate H2), we use the final average accuracy of the on-
line experiments. To compare the final learning outcomes
of different approaches (i.e., to evaluate H3), we use the
final average accuracy of the offline experiments.
Computational Complexity To provide insight into the
computational complexity of the considered methods, we
report for each method its empirical training time on the
online version of Split CIFAR-100. For this evaluation all
methods are run on identical hardware.
Standard Errors Each experiment is run five times, with
a different random seed and different division of the classes
over tasks for each run. For each metric, both the mean over
these runs and the standard error of the mean are reported.

--- PAGE 8 ---
Two Complementary Perspectives to Continual Learning
6 RESULTS
In Table 2 we report the quantitative results for our main ex-
perimental comparisons across all benchmarks. The aver-
age minimum accuracy (MIN) indicates the worst-case ac-
curacy throughout training, which we use as proxy for the
stability gap. The final average accuracy (ACC) reflects the
performance of the continual learning model at the end of
training. The results indicate that combining standard ER,
or incremental joint training, with the optimization mech-
anism of A-GEM does not significantly change either the
stability gap or the final performance on any of the tested
benchmarks. On the other hand, combining ER or incre-
mental joint training with GEM’s optimization mechanism
induces clear effects, although these are not always bene-
ficial. For Rotated MNIST, using the optimization mech-
anism of GEM on top of ER reduces the stability gap, as
reflected by an increase of the average minimum accuracy,
and improves final performance. These positive effects of
GEM’s optimization mechanism persist even when used on
top of incremental joint training (see also Table E.1 in the
Appendix). However, for the other benchmarks, using the
optimization mechanism of GEM does not yield these ben-
efits and instead frequently impairs performance.In the following, we first examine the stability gap for a se-
lection of benchmarks in more detail (subsection 6.1-6.3).
Then, we look at the results for extending recent state-of-
the-art replay-based methods with A-GEM’s optimization
mechanism (subsection 6.4). Finally, we evaluate the ad-
ditional computational overhead induced by the GEM and
A-GEM gradient projection mechanisms (subsection 6.5).
6.1 Rotated MNIST
Our results in Table 2 hint that the domain-incremental
learning problem of Rotated MNIST provides a case where
replay can be improved by altering the optimization trajec-
tory with GEM’s gradient projection mechanism. In Fig-
ure 3, we depict the per-iteration accuracy on the first task
while incrementally training on all the tasks, with detailed
views at the task switches to highlight the stability gap. We
observe that when training on the third task, ER + GEM
modestly reduces the stability gap compared to ER. Re-
garding the final performance, ER + GEM shows a clear
improvement relative to ER. For incremental joint training,
the effects of using GEM’s optimization routine are more
modest, which might be related to the relatively low num-
ber of gradients being projected, as indicated in the bottom
Table 2: Main quantitative results. For all benchmarks we report the final average accuracy (A VG) and average minimum
accuracy (MIN) for standard ER and incremental joint training (or ‘full replay’) – both by themselves and in combination
with the optimization mechanism of GEM and A-GEM. For each benchmark, this table reports the results obtained with the
learning rate (LR) and mini-batch size (BS) that resulted in the highest final accuracy for standard ER and incremental joint
training. Bold values mark the highest performance within each ‘comparison group’ (ER, ER + GEM and ER + A-GEM are
one such group; Joint, Joint + GEM and Joint + A-GEM another). Reported is the mean ±standard error over five random
seeds. The full quantitative results for all pre-registered experiments are provided in Tables E.1-E.4 in Appendix E.
ERER +
GEMER +
A-GEMJointJoint +
GEMJoint +
A-GEM
Rotated
MNISTOffline
LR 0.1MIN 83.1±0.584.1±0.4 84.1±0.4 84.1±0.482.5±0.786.7±0.8 86.7±0.8 86.7±0.887.5±0.9 87.5±0.9 87.5±0.986.6±0.6 86.6±0.6 86.6±0.6
A VG 91.9±0.193.7±0.1 93.7±0.1 93.7±0.191.8±0.297.5±0.097.8±0.0 97.8±0.0 97.8±0.097.5±0.0
Online
LR 0.01; BS 10MIN 86.8±0.389.1±0.4 89.1±0.4 89.1±0.487.1±0.492.3±0.492.8±0.3 92.8±0.3 92.8±0.392.4±0.1
A VG 92.7±0.194.2±0.1 94.2±0.1 94.2±0.192.8±0.296.8±0.0 96.8±0.0 96.8±0.096.8±0.1 96.8±0.1 96.8±0.196.8±0.1 96.8±0.1 96.8±0.1
Domain
CIFAR-100Offline
LR 0.1MIN 33.2±1.3 33.2±1.3 33.2±1.3 7.2±2.634.0±0.9 34.0±0.9 34.0±0.942.9±0.7 42.9±0.7 42.9±0.742.2±1.1 42.2±1.1 42.2±1.142.7±0.8 42.7±0.8 42.7±0.8
A VG 48.6±0.5 48.6±0.5 48.6±0.523.9±1.848.6±0.5 48.6±0.5 48.6±0.552.4±0.8 52.4±0.8 52.4±0.852.6±0.6 52.6±0.6 52.6±0.652.0±0.7 52.0±0.7 52.0±0.7
Online
LR 0.01; BS 10MIN 29.2±0.7 29.2±0.7 29.2±0.7 4.3±0.129.2±0.7 29.2±0.7 29.2±0.735.5±1.1 35.5±1.1 35.5±1.135.6±1.3 35.6±1.3 35.6±1.335.1±0.9 35.1±0.9 35.1±0.9
A VG 38.3±0.8 38.3±0.8 38.3±0.819.8±1.438.3±0.8 38.3±0.8 38.3±0.849.8±1.0 49.8±1.0 49.8±1.049.4±1.5 49.4±1.5 49.4±1.549.7±1.2 49.7±1.2 49.7±1.2
Split
CIFAR-100Offline
LR 0.1MIN 12.4±0.3 12.4±0.3 12.4±0.3 0.0±0.012.1±0.3 12.1±0.3 12.1±0.322.5±0.2 2.8±1.923.1±0.4 23.1±0.4 23.1±0.4
A VG 22.8±0.4 22.8±0.4 22.8±0.4 7.1±1.922.4±0.6 22.4±0.6 22.4±0.632.2±0.5 32.2±0.5 32.2±0.516.5±6.732.5±0.4 32.5±0.4 32.5±0.4
Online
LR 0.01; BS 10MIN 10.6±0.3 10.6±0.3 10.6±0.3 0.0±0.010.7±0.4 10.7±0.4 10.7±0.427.0±0.5 27.0±0.5 27.0±0.527.2±0.5 27.2±0.5 27.2±0.527.2±0.6 27.2±0.6 27.2±0.6
A VG 20.7±0.3 20.7±0.3 20.7±0.3 8.5±0.920.5±0.4 20.5±0.4 20.5±0.441.2±0.6 41.2±0.6 41.2±0.641.1±0.4 41.1±0.4 41.1±0.441.4±0.2 41.4±0.2 41.4±0.2
Split Mini-
ImageNetOffline
LR 0.1MIN 8.6±0.5 8.6±0.5 8.6±0.5 0.0±0.0 8.3±0.4 8.3±0.4 8.3±0.416.5±0.5 16.5±0.5 16.5±0.5 0.0±0.016.4±0.4 16.4±0.4 16.4±0.4
A VG 16.9±0.9 16.9±0.9 16.9±0.9 4.7±1.616.9±0.5 16.9±0.5 16.9±0.528.1±0.6 28.1±0.6 28.1±0.6 3.3±0.428.5±0.7 28.5±0.7 28.5±0.7
Online
LR 0.01; BS 64MIN 3.3±0.2 3.3±0.2 3.3±0.2 0.0±0.0 3.2±0.2 3.2±0.2 3.2±0.2 9.1±0.2 9.1±0.2 9.1±0.2 0.0±0.0 8.9±0.3 8.9±0.3 8.9±0.3
A VG 13.9±0.5 13.9±0.5 13.9±0.5 5.8±0.314.6±0.3 14.6±0.3 14.6±0.327.9±0.8 27.9±0.8 27.9±0.8 5.5±0.528.3±1.3 28.3±1.3 28.3±1.3

--- PAGE 9 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
part of the figure. The stability gap is almost unaltered, but
we still observe a statistically significant improvement of
the average final performance. In contrast, using A-GEM’s
optimization routine does not yield any improvements on
top of ER or incremental joint training. In Appendix D, we
investigate a hyperparameter of GEM that can explain part
of this difference between ER + GEM and ER + A-GEM .
6.2 Offline Natural Image Benchmarks
In the offline versions of the continual learning benchmarks
based on CIFAR-100 and Mini-Imagenet, the benefits we
observed with ER + GEM on Rotated MNIST do not recre-
ate. Instead, ER + GEM substantially deteriorates the per-
formance on these benchmarks. As before, using A-GEM’s
optimization routine does not seem to have any real impact.
Detailed views for Domain CIFAR-100 are depicted in Fig-
ure 4. The performance of ER + GEM decreases incremen-
tally due to a collapse originating from the combination of
ER with GEM’s gradient projection. For Joint + GEM, a
collapse does not arise but there is also no benefit over
Joint, which may be related to the low number of gradi-
ent projections. For Split CIFAR-100, detailed in Figure 5,
we find slightly varied behavior. With ER + GEM, there
is a small reduction of the stability gap after the first task
switch, however, seemingly at the expense of model’s abil-
ity to recover lost performance. For Joint + GEM, collapse
is visible from task 6 onwards, but already during earliertask switches, such as when starting training on task 5,
there is an increased instability that is enlarging the sta-
bility gap instead of closing it. The results for Split Mini-
ImageNet, provided in Figure E.1 in Appendix E, are com-
parable to those of Split CIFAR-100.
6.3 Online Benchmarks
After running the experiments that we had proposed for the
online versions, we found them to be less insightful than
we had anticipated. On the one hand, for Rotated MNIST,
due to its relatively large training set and rapid convergence
of the model, the results for the online version are largely
similar to those for the offline version, and therefore do not
provide additional insights. On the other hand, for the nat-
ural image benchmarks, we found that for the settings that
we had chosen for the online versions, there are no clear
stability gaps with standard ER or with incremental joint
training. This is illustrated for Domain CIFAR-100 in Fig-
ure 6. It can further be seen that there are very few gradient
projections, which might be related to the absence of a sta-
bility gap in these training conditions. For ER + A-GEM
there is not a single projection, meaning it fully reduces
to standard ER, while for ER + GEM there are projections
during the training of later tasks, which seem mostly dis-
ruptive to the learning as indicated by the increased noise
and subsequent performance collapse. We find similar re-
sults for the other natural image benchmarks. For Split
CIFAR-100, shown in Figure E.2 in the Appendix, there
Figure 3: Stability gaps for the first task of offline Rotated MNIST. The left side shows standard ER, the right side
incremental joint training (or ‘full replay’) – both by themselves and in combination with the optimization mechanism of
GEM and A-GEM. The middle panels show the test accuracy on the first task while the model is incrementally trained
for all tasks of the benchmark. The top panels show zoomed-in views of the first 50 training iterations after a task switch,
allowing a more detailed qualitative comparison of the stability gap. These plots show the mean ±standard error (shaded
area) over five runs with different random seeds. The bottom panel shows for every iteration the proportion of runs where
the gradient was projected, with 0indicating that at this iteration there was no run in which a gradient was projected
and1indicating that there was a gradient projection in every run.

--- PAGE 10 ---
Two Complementary Perspectives to Continual Learning
Figure 4: Stability gaps for the first task of offline Domain CIFAR-100. The left side shows standard ER, the right side
incremental joint training (or ‘full replay’) – both by themselves and in combination with the optimization mechanism of
GEM and A-GEM. The middle panels show the test accuracy on the first task while the model is incrementally trained
for all tasks of the benchmark. The top panels show zoomed-in views of the first 50 training iterations after a task switch,
allowing a more detailed qualitative comparison of the stability gap. These plots show the mean ±standard error (shaded
area) over five runs with different random seeds. The bottom panel shows for every iteration the proportion of runs where
the gradient was projected, with 0indicating that at this iteration there was no run in which a gradient was projected
and1indicating that there was a gradient projection in every run.
Figure 5: Stability gaps for the first task of offline Split CIFAR-100. The left side shows standard ER, the right side
incremental joint training (or ‘full replay’) – both by themselves and in combination with the optimization mechanism of
GEM and A-GEM. The middle panels show the test accuracy on the first task while the model is incrementally trained
for all tasks of the benchmark. The top panels show zoomed-in views of the first 50 training iterations after a task switch,
allowing a more detailed qualitative comparison of the stability gap. These plots show the mean ±standard error (shaded
area) over five runs with different random seeds. The bottom panel shows for every iteration the proportion of runs where
the gradient was projected, with 0indicating that at this iteration there was no run in which a gradient was projected
and1indicating that there was a gradient projection in every run.

--- PAGE 11 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
Figure 6: Stability gaps for first task of online Domain CIFAR-100. The left side shows standard ER, the right side
incremental joint training (or ‘full replay’) – both by themselves and in combination with the optimization mechanism of
GEM and A-GEM. The middle panels show the test accuracy on the first task while the model is incrementally trained
for all tasks of the benchmark. The top panels show zoomed-in views of the first 50 training iterations after a task switch,
allowing a more detailed qualitative comparison of the stability gap. These plots show the mean ±standard error (shaded
area) over five runs with different random seeds. The bottom panel shows for every iteration the proportion of runs where
the gradient was projected, with 0indicating that at this iteration there was no run in which a gradient was projected
and1indicating that there was a gradient projection in every run.
is only a small stability gap and a few A-GEM projections
after the first task switch, but after that not anymore.
6.4 Combining DER and BiC with A-GEM
In Table 3, we report the results for combining the recent
state-of-the-art replay-based methods DER and BiC with
the gradient projection-based optimization mechanism of
A-GEM. Similar as in our main experiments in Table 2, we
find no consistent improvements by using A-GEM’s opti-
mization mechanism on top of either DER or BiC.6.5 Computational Complexity
In Table 4, we evaluate the computational complexity of
ER + GEM and ER + A-GEM relative to standard ER, by
comparing their empirical training time on the online ver-
sion of Split CIFAR-100. For a fair comparison, all meth-
ods are run on the same hardware and we attempted to use
comparable implementations. We observe that ER + GEM
takes the longest time to train. This can be explained be-
cause ER + GEM needs to compute a reference gradient
for each previous task and solve the quadratic program
Table 3: Using A-GEM on top of state-of-the-art methods. For the benchmarks in the offline setting, we report the final
average accuracy (A VG) and average minimum accuracy (MIN) for DER and BiC – both by themselves and in combination
with A-GEM’s optimization mechanism. This table shows the results for the learning rate (LR) with the best final average
accuracy for the standard versions of DER and BiC. Reported is the mean ±standard error over five random seeds.
DERDER +
A-GEMBiCBiC +
A-GEM
Rotated
MNISTLR0.01MIN 82.6±0.583.0±0.4 - -
A VG 87.3±0.287.1±0.2 - -
Domain
CIFAR-100LR0.01MIN 44.4±0.744.8±1.0 - -
A VG 59.7±0.560.7±0.4 - -
Split
CIFAR-100LR0.1MIN 1.4±0.1 1.6±0.222.3±0.923.1±1.1
A VG 26.0±0.925.6±1.338.3±1.038.4±0.9
Split Mini-
ImageNetLR0.01MIN 0.1±0.0 0.2±0.113.3±0.313.5±0.2
A VG 7.4±0.5 7.6±0.725.9±0.726.2±0.5

--- PAGE 12 ---
Two Complementary Perspectives to Continual Learning
Table 4: Computational complexity. The empirical train-
ing time on online Split CIFAR-100 is shown. Reported
is the mean ±standard error over five runs with learning
rate0.01and mini-batch size 10.
Time [s]
ER 59.24±0.66
ER + A-GEM 61.52±0.60
ER + GEM 252.25±1.21
integrated in its gradient projection mechanism. The dif-
ference in training time between ER + A-GEM and stan-
dard ER is relatively small. The overhead introduced by
ER + A-GEM compared to standard ER is governed by the
line¯g←PROJECT_AGEM (gjoint, gold)in Algorithm 1: the
only additional computations are the dot-product gTgrefand
the gradient projection g−gTgref
gT
refgref. Extra computations to
calculate the reference gradients, which are required for
A-GEM’s projection, are not needed because those are al-
ready available from the ER mechanism.
7 DISCUSSION
Most of the recent progress in continual learning has been
achieved by the addition of replay or regularization terms
to the loss function, which is done typically with the aim
of approximating the loss that one would like to optimize
(e.g., the joint loss over all tasks so far). However, in
this work we have shown that even if current replay- or
regularization-based methods would manage to perfectly
approximate the desired loss, they would still suffer from
the stability gap. This insight highlights that the current
approach to continual learning is not sufficient. This has
motivated us to argue that rather than concentrating only on
improving the optimization objective (i.e., what loss func-
tion to optimize), continual learning should also focus on
improving the optimization trajectory (i.e., how to optimize
that loss function). Our proposition thus delineates two
complementary perspectives to continual learning, which
we believe to be a useful framework for developing new
continual learning methods.
In search of a proof-of-concept demonstration for our
proposition, we proposed a series of pre-registered exper-
iments combining established continual learning methods
that approximate the joint loss by experience replay, with
existing continual learning methods that change the op-
timization trajectory. These existing optimization-based
continual learning methods, namely GEM and A-GEM,
had previously only been used to optimize the loss on the
new task, and not – as we propose – to optimize an ap-
proximated version of the joint loss. Unfortunately, these
pre-registered experiments did not show clear and consis-tent benefits of this combined approach. In particular, alter-
ing the optimization trajectory with A-GEM did not show
any substantial impact on the results compared to standard
replay. On the other hand, using GEM’s optimization on
top of standard replay had positive effects on the Rotated
MNIST benchmark, but also a disruptive effect on the per-
formance on other benchmarks composed of natural im-
ages.
While our pre-registered experiments were not able to
confirm our hypotheses or provide convincing proof-of-
concept demonstrations, we emphasize that the contribu-
tions of this paper extend beyond those empirical results.
Indeed, taking inspiration from the stability gap (De Lange
et al., 2023), the main contribution of this paper has been to
develop the conceptual proposition that continual learning
should ask not only what to optimize, but also how. Fol-
lowing the release of the pre-registered proposal of this pa-
per, our proposition has started to inspire follow-up work.
Building on our work, Kamath et al. (2024) demonstrate
that the stability gap can occur even with incremental learn-
ing of the same task (i.e., when there is no distribution
shift). In other recent work, Yoo et al. (2024) provide sup-
porting evidence for our proposition by showing that in an
online continual learning setup, the performance of vari-
ous established replay methods can be improved by using
a proximal point method-inspired optimization routine.
Although we have provided compelling arguments that the
way in which optimization is done in continual learning can
be improved, how to do this largely remains an open ques-
tion that demands further exploration. Promising directions
of future work include the development of more principled
methods for gradient projection to bridge the gap between
theoretical guarantees and practical performance. Beyond
gradient projection, other constrained optimization mecha-
nisms could be explored, such as using Lagrange multipli-
ers (e.g., building upon Elenter et al., 2023) or trust region
methods (e.g., building upon Kao et al., 2021). Addition-
ally, such mechanisms may also incorporate second order
optimization, intermediate objectives and dynamic learning
rates to allow for a smooth learning process.
Acknowledgements
We thank Matthias De Lange, Eli Verwimp and anony-
mous reviewers for useful comments. This project has
been supported by funding from the European Union under
the Horizon 2020 research and innovation program (ERC
project KeepOnLearning, grant agreement No. 101021347)
and under Horizon Europe (Marie Skłodowska-Curie fel-
lowship, grant agreement No. 101067759).
References
Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and
Tuytelaars, T. (2018). Memory aware synapses: Learn-

--- PAGE 13 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
ing what (not) to forget. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 139–
154.
Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Lin, M.,
Charlin, L., and Tuytelaars, T. (2019a). Online continual
learning with maximal interfered retrieval. Advances in
Neural Information Processing Systems , 32.
Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . (2019b).
Gradient based sample selection for online continual
learning. Advances in Neural Information Processing
Systems , 32.
Buzzega, P., Boschini, M., Porrello, A., Abati, D., and
Calderara, S. (2020). Dark experience for general con-
tinual learning: a strong, simple baseline. Advances in
Neural Information Processing Systems , 33.
Caccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau,
J., and Belilovsky, E. (2022). New insights on reducing
abrupt representation change in online continual learn-
ing. In International Conference on Learning Represen-
tations (ICLR) .
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny,
M. (2019a). Efficient lifelong learning with a-GEM. In
International Conference on Learning Representations
(ICLR) .
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T.,
Dokania, P. K., Torr, P. H., and Ranzato, M. (2019b).
On tiny episodic memories in continual learning. arXiv
preprint arXiv:1902.10486 .
De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X.,
Leonardis, A., Slabaugh, G., and Tuytelaars, T. (2022).
A continual learning survey: Defying forgetting in clas-
sification tasks. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 44(7):3366–3385.
De Lange, M., van de Ven, G. M., and Tuytelaars, T.
(2023). Continual evaluation for lifelong learning: Iden-
tifying the stability gap. In International Conference on
Learning Representations (ICLR) .
Deng, D., Chen, G., Hao, J., Wang, Q., and Heng, P.-A.
(2021). Flattening sharpness for dynamic gradient pro-
jection memory benefits continual learning. Advances in
Neural Information Processing Systems , 34.
Dhar, P., Singh, R. V ., Peng, K.-C., Wu, Z., and Chellappa,
R. (2019). Learning without memorizing. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 5138–5146.
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht,
F. (2018). Essentially no barriers in neural network en-
ergy landscape. In International Conference on Machine
Learning (ICML) , pages 1309–1318.
Elenter, J., NaderiAlizadeh, N., Javidi, T., and Ribeiro, A.
(2023). Primal-dual continual learning: Stability andplasticity through lagrange multipliers. arXiv preprint
arXiv:2310.00154 .
Farajtabar, M., Azizan, N., Mott, A., and Li, A. (2020).
Orthogonal gradient descent for continual learning. In
International Conference on Artificial Intelligence and
Statistics (AISTATS) , pages 3762–3773.
Farquhar, S. and Gal, Y . (2019). A unifying bayesian view
of continual learning. arXiv preprint arXiv:1902.06494 .
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P.,
and Wilson, A. G. (2018). Loss surfaces, mode connec-
tivity, and fast ensembling of dnns. Advances in Neural
Information Processing Systems , 31.
Guo, Y ., Hu, W., Zhao, D., and Liu, B. (2022). Adaptive or-
thogonal projection for batch and online continual learn-
ing. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 36, pages 6783–6791.
Hadsell, R., Rao, D., Rusu, A. A., and Pascanu, R. (2020).
Embracing change: Continual learning in deep neural
networks. Trends in Cognitive Sciences , 24(12):1028–
1040.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep resid-
ual learning for image recognition. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 770–778.
He, X. (2018). Continual learning by conceptor regulariza-
tion. In Continual Learning Workshop (NeurIPS 2018) .
He, X. and Jaeger, H. (2018). Overcoming catastrophic
interference using conceptor-aided backpropagation. In
International Conference on Learning Representations
(ICLR) .
Henning, C., Cervera, M., D’Angelo, F., V on Oswald, J.,
Traber, R., Ehret, B., Kobayashi, S., Grewe, B. F., and
Sacramento, J. (2021). Posterior meta-replay for contin-
ual learning. Advances in Neural Information Processing
Systems , 34.
Hinton, G., Vinyals, O., and Dean, J. (2014). Distilling
the knowledge in a neural network. NIPS 2014 Deep
Learning Workshop .
Ioffe, S. and Szegedy, C. (2015). Batch normalization: Ac-
celerating deep network training by reducing internal co-
variate shift. In International Conference on Machine
Learning (ICML) , pages 448–456.
Kamath, S., Soutif-Cormerais, A., Van De Weijer, J., and
Raducanu, B. (2024). The expanding scope of the sta-
bility gap: Unveiling its presence in joint incremental
learning of homogeneous tasks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops , pages 4182–4186.
Kao, T.-C., Jensen, K., van de Ven, G., Bernacchia, A.,
and Hennequin, G. (2021). Natural continual learning:
success is a journey, not (just) a destination. Advances
in Neural Information Processing Systems , 34.

--- PAGE 14 ---
Two Complementary Perspectives to Continual Learning
Kim, G., Xiao, C., Konishi, T., Ke, Z., and Liu, B. (2022).
A theoretical study on solving continual learning. Ad-
vances in Neural Information Processing Systems , 35.
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J.,
Desjardins, G., Rusu, A. A., Milan, K., Quan, J.,
Ramalho, T., Grabska-Barwinska, A., et al. (2017).
Overcoming catastrophic forgetting in neural networks.
Proceedings of the National Academy of Sciences ,
114(13):3521–3526.
Kolouri, S., Ketz, N. A., Soltoggio, A., and Pilly, P. K.
(2020). Sliced cramer synaptic consolidation for pre-
serving deeply learned representations. In International
Conference on Learning Representations (ICLR) .
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple
layers of features from tiny images. Techinal Report ,
University of Toronto, Canada.
LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE , 86(11):2278–2324.
Lee, K., Lee, K., Shin, J., and Lee, H. (2019). Overcoming
catastrophic forgetting with unlabeled data in the wild.
InProceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 312–321.
Lesort, T., Lomonaco, V ., Stoian, A., Maltoni, D., Filliat,
D., and Díaz-Rodríguez, N. (2020). Continual learn-
ing for robotics: Definition, framework, learning strate-
gies, opportunities and challenges. Information Fusion ,
58:52–68.
Li, Z. and Hoiem, D. (2017). Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine In-
telligence , 40(12):2935–2947.
Lin, S., Yang, L., Fan, D., and Zhang, J. (2022). TRGP:
Trust region gradient projection for continual learning.
InInternational Conference on Learning Representa-
tions (ICLR) .
Lin, Z., Shi, J., Pathak, D., and Ramanan, D. (2021). The
CLEAR benchmark: Continual LEArning on real-world
imagery. In Thirty-fifth Conference on Neural Informa-
tion Processing Systems Datasets and Benchmarks Track
(Round 2) .
Lopez-Paz, D. and Ranzato, M. (2017). Gradient episodic
memory for continual learning. Advances in Neural In-
formation Processing Systems , 30.
Masse, N. Y ., Grant, G. D., and Freedman, D. J.
(2018). Alleviating catastrophic forgetting using
context-dependent gating and synaptic stabilization.
Proceedings of the National Academy of Sciences ,
115(44):E10467–E10475.
McCloskey, M. and Cohen, N. J. (1989). Catastrophic
interference in connectionist networks: The sequential
learning problem. In Psychology of Learning and Moti-
vation , volume 24, pages 109–165. Elsevier.Mirzadeh, S. I., Farajtabar, M., Gorur, D., Pascanu, R., and
Ghasemzadeh, H. (2021). Linear mode connectivity in
multitask and continual learning. In International Con-
ference on Learning Representations (ICLR) .
Mundt, M., Hong, Y ., Pliushch, I., and Ramesh, V . (2023).
A wholistic view of continual learning with deep neural
networks: Forgotten lessons and the bridge to active and
open world learning. Neural Networks , 160:306–336.
Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. (2018).
Variational continual learning. In International Confer-
ence on Learning Representations (ICLR) .
Prabhu, A., Al Kader Hammoud, H. A., Dokania, P. K.,
Torr, P. H., Lim, S.-N., Ghanem, B., and Bibi, A.
(2023). Computationally budgeted continual learning:
What does matter? In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 3698–3707.
Ratcliff, R. (1990). Connectionist models of recognition
memory: constraints imposed by learning and forgetting
functions. Psychological Review , 97(2):285.
Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert,
C. H. (2017). ICARL: Incremental classifier and rep-
resentation learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR) , pages 2001–2010.
Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu,
Y ., and Tesauro, G. (2018). Learning to learn without
forgetting by maximizing transfer and minimizing inter-
ference. In International Conference on Learning Rep-
resentations (ICLR) .
Robins, A. (1995). Catastrophic forgetting, rehearsal and
pseudorehearsal. Connection Science , 7(2):123–146.
Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., and
Wayne, G. (2019). Experience replay for continual
learning. Advances in Neural Information Processing
Systems , 32.
Rudner, T. G., Smith, F. B., Feng, Q., Teh, Y . W., and Gal,
Y . (2022). Continual learning via sequential function-
space variational inference. In International Conference
on Machine Learning , pages 18871–18887.
Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer,
H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and
Hadsell, R. (2016). Progressive neural networks. arXiv
preprint arXiv:1606.04671 .
Saha, G., Garg, I., and Roy, K. (2021). Gradient projection
memory for continual learning. In International Confer-
ence on Learning Representations (ICLR) .
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and
Moritz, P. (2015). Trust region policy optimization. In
International Conference on Machine Learning (ICML) ,
pages 1889–1897.

--- PAGE 15 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
Serra, J., Suris, D., Miron, M., and Karatzoglou, A. (2018).
Overcoming catastrophic forgetting with hard attention
to the task. In International Conference on Machine
Learning (ICML) , pages 4548–4557.
Shin, H., Lee, J. K., Kim, J., and Kim, J. (2017). Contin-
ual learning with deep generative replay. Advances in
Neural Information Processing Systems , 30.
Titsias, M. K., Schwarz, J., de G. Matthews, A. G., Pas-
canu, R., and Teh, Y . W. (2020). Functional regularisa-
tion for continual learning with gaussian processes. In
International Conference on Learning Representations
(ICLR) .
van de Ven, G. M., Li, Z., and Tolias, A. S. (2021).
Class-incremental learning with generative classifiers. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops (CVPR-W) ,
pages 3611–3620.
van de Ven, G. M., Siegelmann, H. T., and Tolias, A. S.
(2020). Brain-inspired replay for continual learning
with artificial neural networks. Nature Communications ,
11:4069.
van de Ven, G. M., Tuytelaars, T., and Tolias, A. S. (2022).
Three types of incremental learning. Nature Machine
Intelligence , 4(12):1185–1197.
Verwimp, E., De Lange, M., and Tuytelaars, T. (2021).
Rehearsal revealed: The limits and merits of revisiting
samples in continual learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 9385–9394.
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.
(2016). Matching networks for one shot learning. Ad-
vances in Neural Information Processing Systems , 29.
Wang, L., Zhang, X., Yang, K., Yu, L., Li, C., Hong, L.,
Zhang, S., Li, Z., Zhong, Y ., and Zhu, J. (2022). Memory
replay with data compression for continual learning. In
International Conference on Learning Representations
(ICLR) .
Wu, Y ., Chen, Y ., Wang, L., Ye, Y ., Liu, Z., Guo, Y ., and Fu,
Y . (2019). Large scale incremental learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 374–382.
Yan, S., Xie, J., and He, X. (2021). DER: Dynamically ex-
pandable representation for class incremental learning.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages
3014–3023.
Yoo, J., Liu, Y ., Wood, F., and Pleiss, G. (2024). Layerwise
proximal replay: A proximal point method for online
continual learning. arXiv preprint arXiv:2402.09542 .
Yoon, J., Yang, E., Lee, J., and Hwang, S. J. (2018). Life-
long learning with dynamically expandable networks. InInternational Conference on Learning Representations
(ICLR) .
Zaj ˛ ac, M., Tuytelaars, T., and van de Ven, G. M.
(2024). Prediction error-based classification for class-
incremental learning. In International Conference on
Learning Representations (ICLR) .
Zeng, G., Chen, Y ., Cui, B., and Yu, S. (2019). Continual
learning of context-dependent processing in neural net-
works. Nature Machine Intelligence , 1(8):364–372.
Zenke, F., Poole, B., and Ganguli, S. (2017). Continual
learning through synaptic intelligence. In International
Conference on Machine Learning (ICML) , pages 3987–
3995.

--- PAGE 16 ---
Two Complementary Perspectives to Continual Learning
APPENDIX
The Appendix contains additional information for the con-
tent presented in the main body. Appendices A and B ex-
pand upon the average minimum accuracy metric utilized
in the main text to quantify the stability gap, and provide
the details on the BiC and DER continual learning mecha-
nisms. Appendix C details the nuances of our ER + GEM
combination approach implementation. Appendix D con-
tains additional experiments beyond our original experi-
mental protocol that analyze the influence of GEM’s hy-
perparameter γ. Finally, in Appendix E, we disclose the
results for all experiments including those not already fea-
tured in the main text.
Documented code to reproduce all experiments is pub-
licly available at https://github.com/TimmHess/
TwoComplementaryPerspectivesCL .
A ACCURACY METRICS
Classification accuracy is the base metric we use through-
out this work. Given a data set Dand model f, the classifi-
cation accuracy A(D, f)is the percentage of samples in D
that is correctly classified by f.
Final average accuracy is the classification accuracy av-
eraged over all tasks at the end of training. Formally:
avg-ACC =1
TTX
t=1A(ˆDt, fwfinal), (3)
where ˆDt,∀t∈[1, ..., T ]is the evaluation set of each task,
andfwfinalis the final model after concluding training on the
allTtasks. This is a common metric to express the quality
of the continually learned model.
Average minimum accuracy denotes the average of the
lowest classification accuracies observed for each task, as
measured from directly after finishing training on that task
until the end of all learning in the task sequence:
min-ACC (Tt) = min
|Tt|<n≤|TT|A(ˆDt, fwn), (4)
avg-min-ACC =1
T−1T−1X
tmin-ACC (Tt),(5)
where fwnindicates the model after the nthtraining iter-
ation. By slight abuse of notation, n=|Tt|refers to the
last training iteration of task Tt, and n=|TT|marks the
final training iteration of the entire task sequence T. This
metric is a worst case measure of how well the model’s
classification accuracy is maintained at any point through-
out continual training. To render per-iteration calculation
ofA(ˆDt, fwn)computationally feasible, we resort to a re-
duced evaluation set of size 1000 per task. The reducedset is sampled uniformly from each evaluation set respec-
tively, once for every run. This is similar to De Lange et al.
(2023) and has empirically shown to closely approximate
using the full evaluation set.
B METHOD DETAILS
BiC (Bias-Correction) was proposed by Wu et al.
(2019), who were motivated by the observation that
class-incremental continual training causes the continually
trained classifier to become biased towards the most re-
cently observed set of classes. The approach is specifically
designed for class-incremental learning settings where
each observed task Ttintroduces a set of non-overlapping
classes m, such that the corresponding data Xm
t=
{(xi, yi),∀yi∈[n+ 1, ..., n +m]}. Here, xi, yidenote
the example and label pair, and nis the number of already
observed classes. To correct the bias, a (small) set of val-
idation data for each task is stored. These data are taken
from the training set and excluded from training the model.
They are only used for correcting the classifier’s bias in a
separate training stage. The bias-correction itself is real-
ized by a linear layer that consists of two parameters, α
andβ, and is called the bias-correction layer. The logits
produced by the model for previously observed classes are
kept unaltered, but the bias in the logits produced for the m
newly observed classes ( n+ 1, ..., n +m) is corrected by
the bias-correction layer:
qk=(
ok 1≤k≤n
αok+β n + 1≤k≤n+m,(6)
where okdenotes the output logits for the kthclass. The
bias-correction parameters ( α, β) are shared for all new
classes and optimized via a cross-entropy classification
loss:
Lb=−n+mX
k=1log[pk(qk)], (7)
withpk(.)indicating the output probability, i.e. softmax of
the logits.
Next to the bias-correction, BiC uses data augmentation,
a replay mechanism, and a distillation mechanism to con-
tinually train the model. The data-augmentation comprises
random cropping with scales ranging 0.2to1.0and ran-
dom horizontal flipping with chance of p= 0.5. These
augmentations are also applied to buffered samples during
replay. The general replay mechanism is discussed in Sec-
tion 4 of the main body of this paper. Here, we simplify
it to allocating an auxiliary buffer ˆXof size Mthat allows
to interleave training with exemplars from previously ob-
served nclasses. A notable addition is that this buffer holds
both, the replay exemplars for training and for validation,
with the latter already including samples from the current

--- PAGE 17 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
task. Wu et al. (2019) found a allocation ratio of 9 : 1 for
training/validation to be sufficient. The replay interleaved
cross-entropy training loss is formulated as:
Lc=X
(x,y)∈ˆXn∪Xm
tn+mX
k=1−δy=klog(pk(x)). (8)
The additional regularizing distillation loss is formulated
as:
Ld=X
x∈ˆXn∪XmnX
k=1−ˆπk(x) log[πk(x)], (9)
ˆπk=eˆon
k(x)/T
Pn
j=1eˆon
j(x)/T, πk(x) =eon+m
k(x)/T
Pn
j=1eon+1
j(x)/T,
with ˆondenoting the logits from the previous, old, model
andTthe temperature scalar. Note that the previous bias-
correction is applied in ˆon. Ultimately, both losses are com-
bined to the total training loss:
L=λLd+ (1−λ)Lc, (10)
with balancing scalar λ=n
n+m, with nandmbeing the
number of old and new classes respectively.
DER (Dark Experience Replay) is an experience re-
play approach that extents the standard replay formula-
tion ( c.f.Eq. 8) by a regularization term based on distil-
lation (Hinton et al., 2014):
LDER=Lc+αE(x,ok)∼ˆX[DKL(pk(ok)||f(x))],(11)
with loss discounting hyperparameter α. The auxiliary re-
play buffer is defined as:
ˆX={(xi, oi),0≤i < M },
containing Mpairs (xi, oi)of previous tasks exemplars xi
along with the models output logits oi(at the time of adding
them to the buffer), instead of targets yi.
Further, to avoid information loss in the softmax-function
when comparing model output f(x)tooi, the authors chose
to approximate the KL divergence (D KLby the Euclidean
distance. With that, the final loss becomes:
LDER=Lc+αE(x,z)∼ˆX[||z−h(x)||2
2]. (12)
As for BiC, data augmentation by random cropping with
scales ranging 0.2to1.0and random horizontal flipping
with chance of p= 0.5are applied to all forwarded data.
C IMPLEMENTATION DETAILS
The practical implementation of combining experience
replay (ER) and the gradient projection mechanism of
GEM or A-GEM includes multiple aspects that benefitAlgorithm 2 ER + AGEM (detailed)
Require: parameters w, loss function ℓ, learning rate λ,
data stream {D1, ..., D T}
1:M← {}
2:fort= 1, ..., T do
3: for(x, y)∈Dtdo
4: #1. Sample from memory buffer
5: (˜xk,˜yk)←SAMPLE (M)
6:
7: #2. Compute gradients of (approx.) joint loss
8: [zx, z˜x]←fw([x,˜x])
9: g← ∇ wℓ(zx, y)
10: gold← ∇ wℓ(z˜x,˜y)
11: gjoint←1
tg+ (1−1
t)gold
12:
13: #3. Compute reference gradients
14: gref←gold
15:
16: #4. Gradient projection
17: ¯g←PROJECT_AGEM (gjoint, gref)
18:
19: #5. Update model parameters
20: w←OPTIMIZER_STEP (w, λ, ¯g)
21: end for
22: M←UPDATE_BUFFER (M, D t)
23:end for
from additional clarification. In this section we discuss
the calculation of the reference gradients and the han-
dling of batch normalization. To accompany this discus-
sion, we provide detailed pseudocodes for our implementa-
tions of ER + A-GEM (Algorithm 2) and ER + GEM (Algo-
rithm 3), complementing the higher-level pseudocode for
ER + A-GEM in the main text.
Calculation of reference gradients: In the GEM and
A-GEM mechanism, reference gradients inform the gradi-
ent projection by indicating the gradient direction that de-
creases the loss on previously learned tasks. When com-
bining ER with A-GEM, we take the reference gradients to
be the same as the gradients that are used in optimizing the
approximate joint loss (i.e., gref=gold). Computing the ref-
erence gradients on a separately sampled mini-batch from
the memory buffer may have beneficial effects for training,
but would come at an increased computational cost of ef-
fectively doubling the replay mini-batch size. When com-
bining ER with GEM, obtaining the reference gradients is
more complex because a separate reference gradient is re-
quired for each previously learned tasks. In the original
formulation of GEM by Lopez-Paz and Ranzato (2017),
the reference gradients are calculated with respect to the
entire memory buffer. This quickly becomes computation-
ally very costly if large amounts of data are stored. As
a mitigation, rather than using each task’s entire buffer to
compute the reference gradient, we sample one mini-batch

--- PAGE 18 ---
Two Complementary Perspectives to Continual Learning
Algorithm 3 ER + GEM (detailed)
Require: parameters w, loss function ℓ, learning rate λ,
hyperparameter γ, data stream {D1, ..., D T}
1:Mt← {} ,∀t= 1, ..., T
2:fort= 1, ..., T do
3: for(x, y)∈Dtdo
4: #1. Sample from memory buffer
5: (˜xk,˜yk)←SAMPLE (Mk)for all k < t
6: M← {(˜xk,˜yk)}for all k < t
7: (˜xk<t,˜yk<t)←SAMPLE (M)
8:
9: #2. Compute gradients of (approx.) joint loss
10: [zx, z˜xk<t]←fw([x,˜xk<t])
11: g← ∇ wℓ(zx, y)
12: gold← ∇ wℓ(z˜xk<t,˜yk<t)
13: gjoint←1
tg+ (1−1
t)gold
14:
15: #3. Compute reference gradients
16: FREEZE_BATCH_NORM (fw)
17: grefk← ∇ wℓ(fw(˜xk),˜yk)for all k < t
18: UNFREEZE_BATCH_NORM (fw)
19:
20: #4. Gradient projection
21: ¯g←PROJECT_GEM (gjoint,[gref1, ..., g refk], γ)
22:
23: #5. Update model parameters
24: w←OPTIMIZER_STEP (w, λ, ¯g)
25: end for
26: Mt←FILL_BUFFER (Dt)
27:end for
per previously observed task. In particular, when training
on task t, we sample k=t−1mini-batches, one from each
task-specific memory buffer Mk, see Algorithm 3 line 5.
All sampled mini-batches are of the same size as the mini-
batch currently observed by the model. In order to closely
approximate the ‘same-mini-batch’ relation between the re-
play gradient and the reference gradients, we then obtain
the replay mini-batch by uniformly sampling from the k
mini-batches of the reference gradients, see Algorithm 3
lines 6-7.
Batch norm: When implementing ER + A-GEM or
ER + GEM, another aspect requiring careful consideration
is the use of batch normalization (Ioffe and Szegedy, 2015),
which is included in the reduced ResNet-18 architecture
that we use for all benchmarks except Rotated MNIST.
When training with batch norm, the normalization statis-
tics are computed relative to each individual mini-batch
that is forwarded through the model. This means that when
the current data and the replay data are forwarded through
the model in different mini-batches, they use different nor-
malization statistics, which might induce instability in the
training. For standard replay, as well as for ER + A-GEM,
we can mitigate this potential instability by forwarding thecurrent and replayed data together, see Algorithm 2 line 8.
However, when using replay in combination with GEM, it
becomes more complex, because more data from the mem-
ory buffer is forwarded through the model than required
for approximating the joint loss. Forwarding all data to-
gether seems undesirable as it would bias the normalization
statistics too much toward the data from previous tasks (and
doing so might also be impractical as the mini-batch size
might become too large for forwarding all data together).
Instead, common implementations of GEM typically for-
ward the data of each past task separately, which means
that each reference gradient is computed with a custom,
task-specific normalization. Such a different normalization
for each reference gradient might induce instability in the
training. To try to mitigate this, when computing the refer-
ence gradients for GEM, we freeze the batch-norm layers,
see Algorithm 3 line 16-18.
D ADDITIONAL STUDY OF THE
OPTIMIZATION ROUTINE OF GEM
Here, we take a detailed look at the optimization rou-
tine of GEM; in particular, at its gradient projection step
(i.e., line 21 in Algorithm 3). While performing the experi-
ments for this paper, we realized that the optimization rou-
tine of GEM is not unambiguously defined and that it has
an influential hyperparameter. The effect of this hyperpa-
rameter, which is not present in the optimization routine of
A-GEM, can explain for a large part the difference in per-
formances that we observed between using the optimiza-
tion routine of GEM versus that of A-GEM (see section 6
in the main text).
The motivation behind the optimization mechanism of
GEM is to allow only such gradient updates that do not
increase the loss on any previous task. Mathematically,
Lopez-Paz and Ranzato (2017) formulated GEM’s opti-
mization mechanism as follows. When training on task t,
the gradient ¯gbased upon which the optimization step is
taken is given by the solution to:
minimize ¯g1
2||g−¯g||2
2 (13)
subject to ⟨¯g, gk⟩ ≥0,∀k < t, (14)
where gis the gradient of the loss being optimized and gk
is the reference gradient computed on stored data for the
kthtask. Equations (13) and (14) define a quadratic pro-
gram (QP) in pvariables, with pthe number of trainable
parameters of the neural network. To solve this QP, GEM
uses the dual problem, which is given by:
minimize v1
2vTGGT+gTGTv (15)
subject to v≥0, (16)

--- PAGE 19 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
withG= (g1, ..., g t−1).2This dual problem is a QP in only
t−1variables, and can therefore be solved more efficiently.
After solving this dual problem, ¯gcan be recovered as:
¯g=GTv∗+g, (17)
where v∗is the solution to the dual problem.
This is however not the full story. Lopez-Paz and Ranzato
(2017) further introduced a hyperparameter γ, because in
practice they found that ‘adding a small constant γ≥0to
v∗biased the gradient projection to updates that favored
beneficial backward transfer’ (p. 4). Based on this descrip-
tion, the reader might expect Equation (17) to change to
˜g=GT(v∗+γ) +g, but in the official code implemen-
tation of GEM,3γis instead added to the right-hand side
of the inequality constraint of the dual problem (i.e., the
inequality in Equation (16) changes to v≥γ).
Furthermore, setting γ >0introduces another subtlety, be-
cause it makes that the solution ¯gto the dual problem is
always different from g, even if the constraint ⟨g, gk⟩ ≥0
is satisfied for each past task k. Nevertheless, in the of-
ficial code implementation of GEM, ¯gis still set to gif
⟨g, gk⟩ ≥0for all k < t . In other words, the hyperparame-
terγis used only if ⟨g, gk⟩<0for at least one past task k.
This thus introduces a discontinuity (see Figure D.1 for an
empirical evaluation of the effect of this).
Figure D.1: When to perform the GEM projection. Il-
lustrated is the difference for ER + GEM between perform-
ing the GEM projection operation at every iteration versus
only when the cosine-similarity of the current gradient with
at least one reference gradient is ≤0.
2In the published version of Lopez-Paz and Ranzato (2017),
Gis erroneously defined as −(g1, ..., g t−1). In September 2022,
the authors corrected this in the ArXiv version.
3https://github.com/facebookresearch/
GradientEpisodicMemoryThe way that hyperparameter γis treated in GEM’s offi-
cial code implementation is typically taken over by other
publicly available implementations of GEM. For the pre-
registered experiments reported in the main text, we fol-
lowed the official code implementation of GEM as well,
and we used γ= 0.5, as this is the value that Lopez-Paz
and Ranzato (2017) used for all their main experiments.
In this Appendix we report additional experiments that ex-
plore the impact of hyperparameter γ. First, we note that
the effect of γcan be interpreted as enlarging the influence
of the reference gradients gkon¯g. The reason for this is
thatγtends to increase v∗, and ¯gis related to v∗through
¯g=GTv∗+g. Figure D.2 empirically evaluates the impact
of varying γon the performance of ER + GEM on the of-
fline versions of Rotated MNIST and Domain CIFAR-100.
For these experiments, the discontinuity regarding hyper-
parameter γis removed (i.e., the dual problem is solved at
every iteration, we do not first check whether ⟨g, gk⟩<0
for at least one k < t ). For Rotated MNIST, we find that
increasing γleads to both a reduction in the stability gap
and an increase in final performance. For Domain CIFAR-
100, we find that the lower γ, the later the collapse appears,
and with γ≤0.05we no longer observe a collapse.
E ADDITIONAL RESULTS
In this section we provide the remaining results we ob-
tained from our experimental protocol but did not include
in the main text to avoid clutter. An extensive overview of
all data can be found in the Table E.1 for Rotated MNIST,
Table E.2 for Domain CIFAR-100, Table E.3 for Split
CIFAR-100, and Table E.4 for Mini-ImageNet. Also, we
accompany the tabular view by additional plots for qualita-
tive assessment (Figures E.1 and E.2).

--- PAGE 20 ---
Two Complementary Perspectives to Continual Learning
ER + GEM ER
γ 0.0 0 .05 0 .1 0 .5 0 .8 1 .0 −
MIN 84.4±0.785.6±0.586.8±0.589.1±0.289.5±0.490.0±0.2 90.0±0.2 90.0±0.283.1±0.5
A VG 93.6±0.193.7±0.293.8±0.293.9±0.194.1±0.194.1±0.2 94.1±0.2 94.1±0.291.9±0.1ER + GEM ER
γ 0.0 0 .05 0 .1 0 .5 0 .8 1 .0 −
MIN 33.0±0.6 33.0±0.6 33.0±0.633.6±1.5 33.6±1.5 33.6±1.5 8.4±1.5 9.0±1.010.0±1.110.7±1.833.2±1.3 33.2±1.3 33.2±1.3
A VG 48.1±0.7 48.1±0.7 48.1±0.747.2±0.729.0±4.921.3±1.222.9±0.921.3±2.348.6±0.5 48.6±0.5 48.6±0.5
Figure D.2: Influence of GEM’s hyperparameter γγγon Rotated MNIST (left) and Domain CIFAR-100 (right). The
middle panels show the test accuracy on the first task while the model is incrementally trained on all tasks. The top panels
show zoomed in views of the first 50iterations after a task switch, allowing a more detailed qualitative comparison of the
stability gaps. The bottom panels display tables that quantitatively compare average minimum accuracy (MIN) and final
average accuracy (A VG).

--- PAGE 21 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de Ven
Figure E.1: Stability gaps for first task of offline Split Mini-ImageNet. The left side shows standard ER, the right side
incremental joint training (or ‘full replay’) – both by themselves and in combination with the optimization mechanism of
GEM and A-GEM. The middle panels show the test accuracy on the first task while the model is incrementally trained
for all tasks of the benchmark. The top panels show zoomed-in views of the first 50 training iterations after a task switch,
allowing a more detailed qualitative comparison of the stability gap. These plots show the mean ±standard error (shaded
area) over five runs with different random seeds. The bottom panel shows for every iteration the proportion of runs where
the gradient was projected, with 0indicating that at this iteration there was no run in which a gradient was projected
and1indicating that there was a gradient projection in every run.
Figure E.2: Stability gaps for the first task of online Split CIFAR-100. The left side shows standard ER, the right side
incremental joint training (or ‘full replay’) – both by themselves and in combination with the optimization mechanism of
GEM and A-GEM. The middle panels show the test accuracy on the first task while the model is incrementally trained
for all tasks of the benchmark. The top panels show zoomed-in views of the first 50 training iterations after a task switch,
allowing a more detailed qualitative comparison of the stability gap. These plots show the mean ±standard error (shaded
area) over five runs with different random seeds. The bottom panel shows for every iteration the proportion of runs where
the gradient was projected, with 0indicating that at this iteration there was no run in which a gradient was projected
and1indicating that there was a gradient projection in every run.

--- PAGE 22 ---
Two Complementary Perspectives to Continual LearningTable E.1: Final average accuracy (A VG) and average minimum accuracy (MIN) for all hyperparameter settings of online and offline Rotated MNIST. Runs
marked with gray background are used for comparisons in the main body. Results reported as mean ±standard error over 5 runs with different random seeds.
Offline
LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
0.1MIN 20.9±0.843.6±3.733.0±1.3 83.1±0.584.1±0.482.5±0.7 42.3±3.930.3±8.1 86.7±0.887.5±0.986.6±0.6
A VG 52.8±0.691.8±0.265.2±0.8 91.9±0.193.7±0.191.8±0.2 54.8±4.544.8±9.9 97.5±0.097.8±0.097.5±0.0
0.01MIN 30.2±0.455.9±3.039.5±1.2 82.8±0.684.4±0.282.8±0.7 82.6±0.583.0±0.4 86.2±0.587.5±0.586.9±0.5
A VG 56.2±0.292.4±0.171.1±0.5 91.7±0.193.4±0.191.8±0.1 87.3±0.287.1±0.2 96.6±0.097.0±0.096.5±0.0
0.001MIN 31.2±0.584.6±0.447.7±0.7 84.9±0.386.5±0.484.9±0.3 60.2±1.160.2±1.1 88.0±0.388.1±0.388.0±0.3
A VG 53.2±0.390.8±0.165.4±0.4 87.6±0.289.4±0.187.6±0.2 68.6±0.468.6±0.4 92.0±0.192.1±0.092.0±0.0
Online
BS LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
100.1MIN 9.8±0.0 9.8±0.0 9.8±0.0 9.8±0.0 9.8±0.0 9.8±0.0 7.6±0.4 8.0±0.2 9.8±0.0 9.8±0.0 9.8±0.0
A VG 10.1±0.410.1±0.410.1±0.4 10.1±0.410.1±0.410.1±0.4 10.3±0.310.3±0.3 10.1±0.410.1±0.410.1±0.4
0.01MIN 23.7±0.779.5±2.636.3±1.2 86.8±0.389.1±0.487.1±0.4 43.8±4.546.7±3.5 92.3±0.492.8±0.392.4±0.1
A VG 53.8±0.692.9±0.164.6±1.1 92.7±0.194.2±0.192.8±0.2 57.9±6.160.9±2.7 96.8±0.096.8±0.196.8±0.1
0.001MIN 31.4±0.382.5±0.547.2±0.7 86.3±0.388.0±0.286.3±0.3 69.8±0.769.1±0.9 90.9±0.491.5±0.391.0±0.3
A VG 55.8±0.392.4±0.169.3±0.2 90.8±0.392.3±0.190.8±0.3 78.3±0.378.3±0.3 95.1±0.195.4±0.095.1±0.1
640.1MIN 16.1±1.254.3±2.623.4±1.1 84.2±0.784.2±1.084.3±0.6 7.3±0.3 7.9±0.3 87.5±0.788.4±0.886.1±0.5
A VG 48.0±0.686.9±0.856.3±0.7 91.2±0.293.0±0.391.0±0.1 9.9±0.1 9.9±0.1 96.2±0.196.2±0.196.1±0.0
0.01MIN 28.9±0.370.0±2.040.6±0.8 83.9±0.484.6±0.583.9±0.3 72.5±0.472.1±0.5 87.6±0.688.8±0.687.3±0.5
A VG 54.3±0.391.9±0.266.5±0.7 91.2±0.292.7±0.191.3±0.1 80.5±0.380.4±0.3 95.6±0.195.8±0.195.5±0.1
0.001MIN 30.0±0.379.0±0.339.6±0.6 81.6±0.382.4±0.381.6±0.3 42.6±0.642.8±0.6 83.9±0.383.9±0.484.1±0.3
A VG 51.4±0.287.9±0.258.6±0.5 83.7±0.384.9±0.283.8±0.3 52.1±0.452.2±0.4 86.5±0.186.6±0.186.5±0.1
1280.1MIN 20.4±0.739.0±3.927.4±0.8 83.2±0.683.6±0.683.5±0.6 59.8±1.260.2±0.6 87.1±0.588.8±0.487.2±0.8
A VG 51.6±0.391.4±0.157.9±0.5 91.6±0.293.4±0.191.5±0.1 71.3±0.472.2±0.5 96.5±0.096.8±0.196.5±0.1
0.01MIN 28.7±0.561.8±2.338.2±0.9 83.4±0.683.5±0.583.3±0.6 66.7±0.666.7±0.6 86.9±0.487.2±0.587.0±0.4
A VG 53.7±0.490.7±0.362.8±0.4 90.0±0.191.6±0.190.0±0.1 76.1±0.375.9±0.2 94.4±0.194.7±0.094.4±0.0
0.001MIN 33.7±0.476.1±0.441.3±0.6 77.6±0.278.1±0.277.6±0.2 29.6±0.730.0±0.7 79.1±0.279.2±0.279.1±0.2
A VG 52.8±0.283.5±0.258.0±0.2 77.1±0.177.9±0.177.1±0.1 37.9±0.438.1±0.4 78.8±0.178.8±0.178.7±0.1

--- PAGE 23 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de VenTable E.2: Final average accuracy (A VG) and average minimum accuracy (MIN) for all hyperparameter settings of online and offline Domain CIFAR-100 .
Runs marked with gray background are used for comparisons in the main body. Results reported as mean ±standard error over 5 runs with different random seeds.
Offline
LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
MIN 12.2±1.13.5±0.3 12.2±0.5 33.2±1.37.2±2.6 34.0±0.9 46.0±1.046.3±1.0 42.9±0.742.2±1.142.7±0.80.1A VG 35.9±0.717.6±3.136.1±0.9 48.6±0.523.9±1.848.6±0.5 59.6±1.060.3±0.6 52.4±0.852.6±0.652.0±0.7
0.01MIN 11.6±0.85.1±0.5 12.9±1.2 32.6±0.63.8±0.4 30.8±0.6 44.4±0.744.8±1.0 41.0±0.819.8±6.940.2±0.6
A VG 36.9±0.618.0±1.237.2±0.5 46.2±0.316.4±1.546.2±0.2 59.7±0.560.7±0.4 50.2±0.242.2±6.950.1±0.3
0.001MIN 16.9±0.64.4±0.5 16.1±0.4 29.0±0.43.5±0.4 29.4±0.4 34.4±0.434.4±0.3 36.8±0.236.5±0.436.6±0.4
A VG 32.2±0.420.4±1.232.5±0.3 35.0±0.213.7±2.535.6±0.3 46.2±0.346.0±0.3 39.6±0.139.4±0.239.3±0.4
Online
BS LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
100.1MIN 11.9±0.6 6.6±1.312.1±0.425.6±0.9 5.6±1.625.6±0.915.4±0.714.8±0.629.1±0.730.7±1.030.6±1.0
A VG 23.8±0.820.9±1.825.1±0.635.1±0.820.7±4.735.1±0.826.0±0.825.5±0.843.2±0.545.3±0.945.0±1.0
0.01MIN 14.6±0.414.0±0.615.1±0.529.2±0.7 4.3±0.129.2±0.720.3±0.320.1±0.435.5±1.135.6±1.335.1±0.9
A VG 27.6±0.621.1±0.828.0±0.738.3±0.819.8±1.438.3±0.830.7±0.330.6±0.749.8±1.049.4±1.549.7±1.2
0.001MIN 16.3±0.615.3±0.817.1±0.630.7±0.7 4.0±0.130.7±0.723.7±0.223.8±0.338.1±0.837.9±0.837.9±0.7
A VG 28.9±0.623.8±1.229.9±0.438.0±0.526.7±3.338.0±0.534.0±0.533.5±0.548.3±0.448.1±0.649.0±0.4
640.1MIN 13.3±0.4 3.9±0.213.7±0.525.7±0.621.2±4.325.7±0.617.3±0.517.3±0.531.0±0.831.6±1.130.9±1.2
A VG 24.2±0.412.1±2.024.9±0.434.7±1.229.3±4.534.7±1.226.3±0.726.4±0.545.4±1.144.2±1.344.9±1.2
MIN 14.6±0.3 5.5±0.415.4±0.528.0±0.327.7±1.028.0±0.321.5±0.521.7±0.437.1±1.037.0±0.837.0±0.90.01A VG 27.2±0.319.5±1.928.2±0.735.9±0.435.1±0.535.9±0.431.5±0.632.2±0.848.9±0.249.5±0.648.9±0.1
0.001MIN 16.1±0.4 8.6±0.916.2±0.325.3±0.3 6.1±2.725.3±0.319.6±0.320.0±0.227.8±0.322.6±3.127.8±0.4
A VG 24.4±0.523.4±2.524.8±0.631.5±0.213.9±4.431.5±0.225.9±0.526.1±0.634.4±0.234.5±0.234.5±0.2
1280.1MIN 11.8±0.5 3.5±0.211.8±0.724.7±0.620.5±4.224.7±0.617.6±0.417.6±0.529.3±1.029.8±0.829.9±0.8
A VG 23.4±1.012.0±0.523.5±0.734.0±0.728.6±4.834.0±0.725.1±0.525.7±0.842.0±0.843.4±1.044.3±1.0
0.01MIN 13.8±0.6 4.1±0.213.1±0.827.2±0.726.7±0.527.2±0.720.2±0.320.3±0.434.1±0.534.1±0.534.2±0.5
A VG 25.9±0.612.5±1.826.0±0.934.7±0.435.3±0.834.7±0.430.2±0.430.1±0.345.1±0.545.0±0.645.0±0.7
0.001MIN 15.1±0.7 6.6±1.015.5±0.522.5±0.5 9.5±3.322.5±0.518.3±0.518.4±0.524.0±0.522.5±0.823.4±0.5
A VG 22.3±0.410.3±2.022.7±0.526.9±0.218.1±3.726.9±0.223.7±0.223.7±0.328.1±0.226.6±1.528.2±0.2

--- PAGE 24 ---
Two Complementary Perspectives to Continual LearningTable E.3: Final average accuracy (A VG) and average minimum accuracy (MIN) for all hyperparameter settings of online and offline Split-CIFAR100. Runs
marked with gray background are used for comparisons in the main body. Results reported as mean ±standard error over 5 runs with different random seeds.
Offline
LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMBiC∗ BiC∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
MIN 0.0±0.0 0.0±0.0 0.0±0.0 12.4±0.3 0.0±0.012.1±0.3 1.4±0.1 1.6±0.2 22.3±0.923.1±1.122.5±0.2 2.8±1.923.1±0.40.1A VG 6.5±0.4 1.5±0.2 6.4±0.3 22.8±0.4 7.1±1.922.4±0.6 26.0±0.925.6±1.3 38.3±1.038.4±0.932.2±0.516.5±6.732.5±0.4
0.01MIN 0.0±0.0 0.0±0.0 0.0±0.0 10.8±0.6 0.0±0.010.9±0.7 1.0±0.3 1.0±0.2 27.4±0.427.8±0.521.6±0.7 3.8±0.721.3±0.8
A VG 6.6±0.4 3.4±0.5 6.4±0.4 18.5±0.6 7.6±1.619.6±0.2 22.1±1.022.6±0.7 34.3±0.534.4±0.825.8±0.422.6±3.625.8±0.4
0.001MIN 0.0±0.0 0.0±0.0 0.0±0.0 8.3±0.5 2.0±1.9 8.3±0.4 0.1±0.0 0.1±0.0 10.5±0.410.7±0.616.7±0.613.8±3.516.5±0.6
A VG 6.6±0.3 3.7±0.2 6.7±0.3 12.5±0.310.2±1.711.7±0.3 9.1±0.2 9.2±0.2 16.7±0.816.9±0.720.6±0.417.2±3.920.3±0.5
Online
BS LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMBiC∗ BiC∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
100.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 9.0±0.4 0.0±0.0 9.1±0.3 0.0±0.0 0.0±0.0 7.5±0.4 8.6±0.2 22.1±0.522.9±0.622.8±0.4
A VG 5.0±0.4 1.3±0.2 5.0±0.4 19.3±0.4 6.9±0.919.2±0.3 5.5±0.3 5.4±0.3 15.9±0.517.0±0.6 38.4±1.238.6±0.638.7±0.4
MIN 0.0±0.0 0.0±0.0 0.0±0.0 10.6±0.3 0.0±0.010.7±0.4 0.0±0.0 0.0±0.0 11.1±0.411.1±0.4 27.0±0.527.2±0.527.2±0.60.01A VG 5.1±0.3 2.1±0.1 5.0±0.3 20.7±0.3 8.5±0.920.5±0.4 5.8±0.3 5.9±0.2 18.4±0.418.4±0.4 41.2±0.641.1±0.441.4±0.2
0.001MIN 0.0±0.0 0.1±0.1 0.0±0.0 10.0±0.2 0.0±0.010.0±0.2 0.0±0.0 0.0±0.0 7.9±0.3 7.9±0.3 21.2±0.210.5±4.621.4±0.3
A VG 6.0±0.3 3.0±0.3 5.8±0.3 18.8±0.212.2±1.318.8±0.2 6.4±0.4 6.4±0.4 12.6±0.412.6±0.3 32.6±0.326.5±6.232.8±0.3
640.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 5.7±0.5 0.0±0.0 5.6±0.4 0.0±0.0 0.0±0.0 4.5±0.6 4.5±0.6 12.2±0.711.1±0.912.1±0.4
A VG 2.2±0.3 2.4±0.8 2.4±0.5 15.2±0.5 5.5±0.315.4±0.4 3.8±0.3 4.0±0.3 15.2±0.415.2±0.4 29.1±1.029.2±0.928.8±1.0
0.01MIN 0.0±0.0 0.0±0.0 0.0±0.0 8.0±0.4 3.3±2.1 8.3±0.3 0.0±0.0 0.0±0.0 7.9±0.3 7.9±0.3 17.6±0.418.5±0.218.3±0.4
A VG 1.9±0.2 5.3±1.4 1.9±0.2 17.3±0.7 8.7±4.017.0±0.8 4.0±0.3 3.6±0.3 15.1±0.415.1±0.4 33.8±0.233.7±0.434.3±0.3
0.001MIN 0.0±0.0 0.0±0.0 0.0±0.0 6.3±0.2 0.0±0.0 6.3±0.3 0.0±0.0 0.0±0.0 1.8±0.3 1.8±0.3 8.2±0.3 1.3±0.4 8.3±0.3
A VG 4.1±0.2 6.9±1.5 4.2±0.3 13.0±0.5 5.9±1.413.0±0.5 4.1±0.3 4.1±0.4 7.0±0.4 7.0±0.4 16.0±0.4 8.4±2.415.7±0.4
1280.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 1.5±0.2 0.0±0.0 1.6±0.3 0.3±0.2 0.0±0.0 2.8±0.2 2.8±0.2 3.6±0.6 0.9±0.8 4.5±0.4
A VG 1.1±0.1 1.5±0.2 1.7±0.3 9.1±0.6 4.1±0.7 9.1±0.2 1.4±0.2 1.6±0.4 12.0±0.412.0±0.4 14.4±0.9 7.4±2.115.0±0.8
0.01MIN 0.3±0.2 0.0±0.0 0.4±0.2 6.7±0.5 1.4±1.4 6.6±0.4 0.0±0.0 0.3±0.2 5.0±0.3 5.0±0.3 13.2±0.5 5.3±3.313.4±0.6
A VG 1.3±0.1 5.3±0.5 1.6±0.2 15.2±0.2 6.6±2.815.3±0.4 2.1±0.1 2.6±0.2 12.1±0.412.1±0.4 25.7±1.011.8±5.725.4±0.8
0.001MIN 1.0±0.3 0.0±0.0 1.0±0.2 3.0±0.2 0.0±0.0 3.0±0.2 1.1±0.3 1.0±0.2 0.9±0.1 0.9±0.1 3.2±0.4 1.1±0.2 3.3±0.3
A VG 3.4±0.2 4.8±1.4 3.4±0.3 10.2±0.3 3.6±1.310.2±0.3 3.2±0.3 3.3±0.3 4.9±0.3 4.9±0.3 11.6±0.4 4.5±0.311.6±0.4

--- PAGE 25 ---
Timm Hess, Tinne Tuytelaars, Gido M. van de VenTable E.4: Final average accuracy (A VG) and minimum average accuracy (MIN) for all hyperparameter settings of online and offline Split Mini-ImageNet.
Runs marked with gray background are used for comparisons in the main body. Results reported as mean ±standard error over 5 runs with different random seeds.
Offline
LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMBiC∗ BiC∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
0.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 8.6±0.5 0.0±0.0 8.3±0.4 0.0±0.0 0.0±0.0 7.8±0.9 8.1±0.816.5±0.5 0.0±0.016.4±0.4
A VG 3.2±0.1 1.0±0.0 3.2±0.116.9±0.9 4.7±1.616.9±0.5 4.3±0.2 4.2±0.820.3±0.620.4±1.228.1±0.6 3.3±0.428.5±0.7
0.01MIN 0.0±0.0 0.0±0.0 0.0±0.0 4.1±0.5 0.0±0.0 4.0±0.7 0.1±0.0 0.2±0.113.3±0.313.5±0.2 8.9±1.1 0.0±0.0 8.5±0.7
A VG 3.1±0.1 1.9±0.1 3.1±0.113.5±0.6 6.4±1.914.1±0.5 7.4±0.5 7.6±0.725.9±0.726.2±0.519.1±1.2 4.6±0.518.5±0.9
0.001MIN 0.0±0.0 0.0±0.0 0.0±0.0 4.0±0.3 0.0±0.0 3.8±0.3 0.0±0.0 0.0±0.0 6.8±0.6 6.7±0.4 9.6±0.8 0.0±0.0 9.6±0.7
A VG 3.2±0.1 3.6±0.6 3.4±0.110.1±0.4 5.2±0.8 9.9±0.2 4.2±0.2 4.5±0.214.4±0.414.2±0.316.7±0.7 6.4±1.116.9±0.7
Online
BS LRFinal
ACCFinetune GEM AGEM ERER
+ GEMER
+ AGEMDER∗ DER∗
+ AGEMBiC∗ BiC∗
+ AGEMJointJoint
+ GEMJoint
+ AGEM
100.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 0.3±0.1 0.0±0.0 0.3±0.2 0.8±0.2 1.1±0.0 0.6±0.2 0.1±0.1 0.2±0.10.1±0.0 1.0±0.3
A VG 1.0±0.0 1.6±0.5 1.1±0.1 6.6±0.5 1.8±0.5 6.4±0.6 1.0±0.0 1.0±0.0 8.3±0.8 3.7±0.3 3.9±0.4 3.4±0.4 8.0±1.1
0.01MIN 0.1±0.1 0.0±0.0 0.3±0.2 1.0±0.2 0.0±0.0 0.9±0.3 0.4±0.2 0.2±0.2 1.6±0.2 0.3±0.1 0.5±0.2 0.0±0.0 2.0±0.2
A VG 1.1±0.1 3.0±0.5 1.3±0.110.8±0.4 5.5±0.610.9±0.7 1.5±0.2 1.7±0.316.2±0.6 7.8±0.5 8.0±0.6 4.8±0.616.7±0.9
0.001MIN 0.2±0.1 0.0±0.0 0.2±0.1 1.4±0.3 0.0±0.0 1.5±0.3 0.4±0.1 0.6±0.2 1.3±0.2 0.0±0.0 0.0±0.0 0.0±0.0 1.5±0.3
A VG 1.2±0.2 4.0±0.8 1.2±0.111.0±0.3 5.2±0.911.1±0.4 1.8±0.2 2.5±0.212.3±0.3 4.5±0.3 4.5±0.3 5.4±0.912.3±0.3
640.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 0.7±0.2 0.0±0.0 1.0±0.2 0.4±0.3 0.4±0.3 0.2±0.1 0.2±0.1 3.1±0.4 0.0±0.0 3.1±0.6
A VG 1.0±0.0 1.0±0.1 1.3±0.1 8.3±0.3 1.7±0.2 8.6±0.6 1.1±0.1 1.1±0.1 5.0±0.5 5.3±0.416.9±0.8 3.5±0.516.8±1.4
0.01MIN 0.0±0.0 0.0±0.0 0.0±0.0 3.3±0.2 0.0±0.0 3.2±0.2 0.1±0.1 0.1±0.0 2.0±0.3 1.9±0.4 9.1±0.2 0.0±0.0 8.9±0.3
A VG 1.2±0.1 2.7±0.9 1.4±0.213.9±0.5 5.8±0.314.6±0.3 1.6±0.3 1.8±0.211.1±0.511.0±0.727.9±0.8 5.5±0.528.3±1.3
0.001MIN 0.2±0.1 0.0±0.0 0.3±0.3 3.7±0.5 0.0±0.0 3.8±0.5 0.9±0.1 0.9±0.3 0.8±0.1 0.8±0.2 4.1±1.5 0.0±0.0 5.9±0.5
A VG 1.4±0.1 8.3±0.8 1.7±0.213.6±0.2 6.6±0.913.7±0.3 2.3±0.2 1.8±0.1 6.3±0.3 6.4±0.312.4±3.6 3.6±1.117.9±0.5
1280.1MIN 0.0±0.0 0.0±0.0 0.0±0.0 0.3±0.1 0.0±0.0 0.3±0.2 0.8±0.2 1.1±0.0 0.1±0.1 0.2±0.1 0.6±0.2 0.1±0.0 1.0±0.3
A VG 1.0±0.0 1.6±0.5 1.1±0.1 6.6±0.5 1.8±0.5 6.4±0.6 1.0±0.0 1.0±0.0 3.7±0.3 3.9±0.4 8.3±0.8 3.4±0.4 8.0±1.1
0.01MIN 0.1±0.1 0.0±0.0 0.3±0.2 1.0±0.2 0.0±0.0 0.9±0.3 0.4±0.2 0.2±0.2 0.3±0.1 0.5±0.2 1.6±0.2 0.0±0.0 2.0±0.2
A VG 1.1±0.1 3.0±0.5 1.3±0.110.8±0.4 5.5±0.610.9±0.7 1.5±0.2 1.7±0.3 7.8±0.5 8.0±0.616.2±0.6 4.8±0.616.7±0.9
0.001MIN 0.2±0.1 0.0±0.0 0.2±0.1 1.4±0.3 0.0±0.0 1.5±0.3 0.4±0.1 0.6±0.2 0.0±0.0 0.0±0.0 1.3±0.2 0.0±0.0 1.5±0.3
A VG 1.2±0.2 4.0±0.8 1.2±0.111.0±0.3 5.2±0.911.1±0.4 1.8±0.2 2.5±0.2 4.5±0.3 4.5±0.312.3±0.3 5.4±0.912.3±0.3
