# 2303.14962.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2303.14962.pdf
# File size: 2962790 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PREPRINT, MARCH 2023 1
Forget-free Continual Learning with
Soft-Winning SubNetworks
Haeyong Kang, Jaehong Y oon, Sultan Rizky Madjid, Sung Ju Hwang, and Chang D. Y oo
Abstract ‚ÄîInspired by Regularized Lottery Ticket Hypothesis (RLTH) , which states that competitive smooth (non-binary) subnetworks
exist within a dense network in continual learning tasks, we investigate two proposed architecture-based continual learning methods
which sequentially learn and select adaptive binary- (WSN) and non-binary Soft-Subnetworks (SoftNet) for each task. WSN and SoftNet
jointly learn the regularized model weights and task-adaptive non-binary masks of subnetworks associated with each task whilst
attempting to select a small set of weights to be activated (winning ticket) by reusing weights of the prior subnetworks. Our proposed
WSN and SoftNet are inherently immune to catastrophic forgetting as each selected subnetwork model does not infringe upon other
subnetworks in Task Incremental Learning (TIL). In TIL, binary masks spawned per winning ticket are encoded into one N-bit binary digit
mask, then compressed using Huffman coding for a sub-linear increase in network capacity to the number of tasks. Surprisingly, in the
inference step, SoftNet generated by injecting small noises to the backgrounds of acquired WSN (holding the foregrounds of WSN)
provides excellent forward transfer power for future tasks in TIL. SoftNet shows its effectiveness over WSN in regularizing parameters to
tackle the overÔ¨Åtting, to a few examples in Few-shot Class Incremental Learning (FSCIL).
Index Terms ‚ÄîContinual Learning (CL), Task Incremental Learning (TIL), Few-shot Class Incremental Learning (FSCIL), Regularized
Lottery Ticket Hypothesis (RLTH), Wining SubNetworks (WSN), Soft-Subnetwork (SoftNet)
F
1 I NTRODUCTION
CONTINUAL Learning (CL), also known as Lifelong
Learning [ 69], [61], [82], [23], is a learning paradigm
where a series of tasks are learned sequentially. The principle
objective of continual learning is to replicate human cogni-
tion, characterized by the ability to learn new concepts or
skills incrementally throughout one‚Äôs lifespan. An optimal
continual learning system could facilitate a positive forward
and backward transfer, leveraging the knowledge gained
from previous tasks to solve new ones while also updating
its understanding of previous tasks with the new knowledge.
However, building successful continual learning systems is
challenging due to the occurrence of catastrophic forgetting
orcatastrophic interference [53], a phenomenon where the
performance of the model on previous tasks signiÔ¨Åcantly
deteriorates when it learns new tasks. This can make it
challenging to retain the knowledge acquired from previous
tasks, ultimately leading to a decline in overall performance.
To tackle the catastrophic forgetting problem in continual
learning, numerous approaches have been proposed, which
can be broadly classiÔ¨Åed as follows: (1) Regularization-
based methods [38], [7], [34], [70], [55] aim to keep the
learned information of past tasks during continual training
aided by sophisticatedly designed regularization terms, (2)
Rehearsal-based methods [59], [60], [8], [9], [62] utilize a set
of real or synthesized data from the previous tasks and revisit
them, and (3) Architecture-based methods [49], [64], [44],
[72], [36], [37] propose to minimize the inter-task interference
via newly designed architectural components.
Despite the remarkable success of recent works on
rehearsal- and architecture-based continual learning, a ma-
Email: haeyong.kang@kaist.ac.kr
Corresponding Author.jority of recent methods request external memory as new
tasks arrive, making the model difÔ¨Åcult to scale to larger and
more complex tasks. Rehearsal-based CL requires additional
storage to store the replay buffer or generative models,
and architecture-based methods leverage additional model
capacity to account for new tasks. These trends lead to an
essential question: how can we build a memory-efÔ¨Åcient
CL model that does not exceed the backbone network‚Äôs
capacity or even requires a much smaller capacity? Several
studies have shown that deep neural networks are over-
parameterized [ 17], [21], [43] and thus removing redun-
dant/unnecessary weights can achieve on-par or even better
performance than the original dense network. More recently,
Lottery Ticket Hypothesis (LTH) [ 18] demonstrates the
existence of sparse subnetworks, named winning tickets , that
preserve the performance of a dense network. However,
searching for optimal winning tickets during continual
learning with iterative pruning methods requires repetitive
pruning and retraining for each arriving task, which could
be more practical. Furthermore, leveraged by Regularized
Lottery Ticket Hypothesis (RLTH) [ 37], subnetworks exempli-
Ô¨Åed that it could overÔ¨Åt a few task data, potentially limiting
their effectiveness on new tasks or datasets.
To tackle the issues of external replay buffer, capacity,
and over-Ô¨Åtting, we suggest a novel regularized CL method
which Ô¨Ånds the high-performing Regularized Winning Sub-
Network referred to as Soft-Subnetwork ( SoftNet ) [37] given
tasks without the need for retraining and rewinding. As a
baseline of SoftNet, non-regularized Winning Subnetworks
referred to as WSN [36] are restated, as shown in Figure 1 (d).
Also, we set previous pruning-based CL approaches [ 49], [72]
(see Figure 1 (a)) to baselines of architectures, which obtain
task-speciÔ¨Åc subnetworks given a pre-trained backbone
network. Our WSN incrementally learns model weights andarXiv:2303.14962v1  [cs.LG]  27 Mar 2023

--- PAGE 2 ---
PREPRINT, MARCH 2023 2
t-1 t
t-1 t
t-1 t
t-1 t
(a) Fixed Backbone
(Piggyback, SupSup)(b) Biased Transfer
(PackNet, CLNP)(c) Selective Reuse Expansion
beyond Dense Network (APD)(d) Selective Reuse Expansion
within Network (WSN)
Fig. 1: Concept Comparison: (a) Piggyback [ 49], and SupSup [ 72] Ô¨Ånd the optimal binary mask on a Ô¨Åxed backbone network
a given task (b) PackNet [ 50] and CLNP [ 19] forces the model to reuse all features and weights from previous subnetworks
which causes bias in the transfer of knowledge (c) APD [ 78] selectively reuse and dynamically expand the dense network (d)
Our WSN selectively reuse and dynamically expand subnetworks within a dense network. Green edges are reused weights.
task-adaptive binary masks (the subnetworks) within the
neural network. To allow the forward transfer when a model
learns on a new task, we reuse the learned subnetwork
weights for the previous tasks, however selectively, as
opposed to using all the weights [ 50] (see Figure 1 (b)), that
may lead to biased transfer. Further, the WSN eliminates the
threat of catastrophic forgetting during continual learning by
freezing the subnetwork weights for the previous tasks and
does not suffer from the negative transfer, unlike [ 80] (see
Figure 1 (c)), whose subnetwork weights for the previous
tasks can be updated when training on the new tasks.
LTH often leverages the magnitudes of the weights
as a pruning criterion to Ô¨Ånd the optimal subnetworks.
However, in CL, relying only on the weight magnitude may
be ineffective since the weights are shared across classes,
and thus training on the new tasks will change the weights
trained for previous tasks (reused weights). The update
of reused weights will trigger an avalanche effect where
weights selected to be part of the subnetworks for later tasks
will always be better in the eyes of the learner, which will
result in the catastrophic forgetting of the knowledge for
the prior tasks. Thus, in CL, the learner must train on the
new tasks without changing the reused weights. To Ô¨Ånd
the optimal subnetworks, we decouple the information of
the learning parameter and the network structure into two
separate learnable parameters, namely, weights and weight
scores . The weight scores are binary masks with the same
shapes as the weights. Now, subnetworks are found by
selecting the weights with the top- kpercent weight ranking
scores. More importantly, decoupling the weights and the
structure allows us to Ô¨Ånd the optimal subnetwork online
without iterative retraining, pruning, and rewinding. There is
one more thing to consider to Ô¨Ånd the optimal subnetworks
induced by binary masks in CL. According to CL tasks, the
subnetworks tend to overÔ¨Åt the few samples, i.e., Few-Shot
Class Incremental Learning (FSCIL). Therefore, we could Ô¨Ånd
regularized subnetworks yielded by smooth (soft) masks. To
this end, the proposed methods are designed to jointly learn
the weights and the structure of the optimal regularized
subnetworks, whose overall size is smaller than a dense
network.
Our contributions can be summarized as follows:Inspired by Regularized Lottery Ticket Hypothesis (RLTH),
we propose novel forget-free continual learning methods
referred to as WSN and SoftNet, which learn a compact
subnetwork for each task while keeping the weights
selected by the previous tasks intact.
Our proposed WSN and SoftNet do not perform explicit
pruning for learning the subnetwork. Our methods elim-
inate catastrophic forgetting and enable forward transfer
from previous tasks to new ones in Task Incremental
Learning (TIL).
Our WSN obtains compact subnetworks using Huffman
coding with a sub-linear increase in the network capacity,
outperforming existing continual learning methods regard-
ing accuracy-capacity trade-off and forward / backward
transfer in TIL.
Our SoftNet trains two different types of subnetworks
for solving the FSCIL problem, alleviating the continual
learner from forgetting previous sessions and overÔ¨Åtting
simultaneously, outperforming strong baselines on public
benchmark tasks.
2 R ELATED WORKS
Continual Learning. Continual learning [ 53], [69], [41],
[45], also known as lifelong learning, is the challenge of
learning a sequence of tasks continuously while utilizing
and preserving previously learned knowledge to improve
performance on new tasks. Several major approaches
have been proposed to tackle the challenges of continual
learning, such as catastrophic forgetting. One such approach
isregularization-based methods [38], [7], [34], [70], [55],
which aim to reduce catastrophic forgetting by imposing
regularization constraints that inhibit changes to the
weights or nodes associated with past tasks. Rehearsal-based
approaches [59], [8], [9], [62], [16] store small data summaries
to the past tasks and replay them during training to
retain the acquired knowledge. Some methods in this
line of work [ 66], [2] accommodate the generative model
to construct the pseudo-rehearsals for previous tasks.
Architecture-based approaches [49], [64], [44], [72], [36], [37] use
the additional capacity to expand [ 75], [80] or isolate [ 61]
model parameters, preserving learned knowledge and
preventing forgetting. Both rehearsal and architecture-based

--- PAGE 3 ---
PREPRINT, MARCH 2023 3
methods have shown remarkable efÔ¨Åcacy in suppressing
catastrophic forgetting but require additional capacity for
the task-adaptive parameters [72] or the replay buffers.
Pruning-based Continual Learning. While most works
aim to increase the performance of continual learners by
adding memory, some researchers have focused on building
memory and computationally efÔ¨Åcient continual learners by
using pruning-based constraints. CLNP [ 19] is one example
of a method that selects important neurons for a given task
using`1regularization to induce sparsity and freezes them
to maintain performance. Neurons that are not selected
are reinitialized for future task training. Another method,
Piggyback [ 49], trains task-speciÔ¨Åc binary masks on the
weights given a pre-trained model. However, this method
does not allow for knowledge transfer among tasks, and its
performance highly depends on the quality of the backbone
model. HAT [ 64] proposes task-speciÔ¨Åc learnable attention
vectors to identify important weights per task. The masks are
formulated to layerwise cumulative attention vectors during
continual learning. A recent method, LL-Tickets [ 11], shows
a sparse subnetwork called lifelong tickets that performs
well on all tasks during continual learning. The method
searches for more prominent tickets from current ones if the
obtained tickets cannot sufÔ¨Åciently learn the new task while
maintaining performance on past tasks. However, LL-Tickets
require external data to maximize knowledge distillation
with learned models for prior tasks, and the ticket expansion
process involves retraining and pruning steps. WSN [ 36]
is another method that jointly learns the model and task-
adaptive binary masks on subnetworks associated with each
task. It attempts to select a small set of weights activated
(winning ticket) by reusing weights of the prior subnetworks.
Soft-subnetwork. Recent studies have shown that context-
dependent gating of sub-spaces [ 25], parameters [ 50], [26],
[52], or layers [ 63] of a single deep neural network is effec-
tive in addressing catastrophic forgetting during continual
learning. Moreover, combining context-dependent gating
with constraints that prevent signiÔ¨Åcant changes in model
weights, such as SI [ 82] and EWC [ 38], can lead to further
performance improvements, as shown by Masse et al. [51].
Flat minima, which can be seen as acquiring sub-spaces,
have also been proposed to address catastrophic forgetting.
Previous studies have demonstrated that a Ô¨Çat minimizer
is more robust to random perturbations [ 29], [30], [33]. In a
recent study by Shi et al. [65], obtaining Ô¨Çat loss minima in
the base session, which refers to the Ô¨Årst task session with
a sufÔ¨Åcient number of training instances, was found to be
necessary to alleviate catastrophic forgetting in Few-Shot
Class Incremental Learning (FSCIL). They achieved this by
shifting the model weights on the obtained Ô¨Çat loss contour.
Our work investigates the performance of two proposed
architecture-based continual learning methods: WSN / Soft-
Subnetworks (SoftNet). We select sub-networks [ 18], [46],
[81], [85], [71], [58], [36], [14] and optimize the regularized
sub-network parameters in a sub-space [ 37] in Task Incremen-
tal Learning (TIL) and Few-Shot Class Incremental Learning
(FSCIL) settings.3 S OFT-WINNING SUBNETWORKS
In this section, we present our pruning-based continual
learning methods, Winning SubNetworks (WSN) [ 36] and
Soft-Winning SubNetworks (SoftNet) [ 37]. In WSN, the neural
network searches for the task-adaptive winning tickets and
updates only the weights that have not been trained on the
previous tasks. After training on each task, the subnetwork
parameters of the model are frozen to ensure that the
proposed method is inherently immune to catastrophic
forgetting. The WSN method is designed to selectively
transfer previously learned knowledge to future tasks (i.e.,
forward transfer), which can signiÔ¨Åcantly reduce the training
time needed for convergence during sequential learning.
This feature is especially critical for large-scale learning
problems where a continual learner trains on multiple tasks
sequentially, leading to signiÔ¨Åcant time and computational
savings. Originally, SoftNet was proposed to address the
issues of forgetting previous sessions and overÔ¨Åtting a few
samples of new sessions. To achieve this, the method trains
two types of subnetworks concurrently.
Problem Statement. Consider a supervised learning setup
whereTtasks arrive to a learner sequentially. We denote that
Dt=fxi;t;yi;tgnt
i=1is the dataset of task t, composed of nt
pairs of raw instances and corresponding labels. We assume a
neural network f(;), parameterized by the model weights
and standard continual learning scenario aims to learn
a sequence of tasks by solving the following optimization
procedure at each step t:
= minimize
1
ntntX
i=1L(f(xi;t;);yi;t); (1)
whereL(;)is a classiÔ¨Åcation objective loss such as cross-
entropy loss.Dtfor tasktis only accessible when learning
taskt. Note rehearsal-based continual learning methods
allow memorizing a small portion of the dataset to replay.
We further assume that task identity is given in the training
and testing stages.
Continual learners frequently use over-parameterized
deep neural networks to ensure enough capacity for learning
future tasks. This approach often leads to the discovery
of subnetworks that perform as well as or better than the
original network. Given the neural network parameters ,
the binary attention mask m
tthat describes the optimal
subnetwork for task tsuch thatjm
tjis less than the model
capacitycfollows as:
m
t= minimize
mt2f0;1gjj1
ntntX
i=1L f(xi;t;mt);yi;t J
subject tojm
tjc;(2)
where task lossJ=L f(xi;t;);yi;t
andcjj(used
as the selected proportion %of model parameters in the
following section). In the optimization section, we describe
how to obtain m
tusing a single learnable weight score s
that is subject to updates while minimizing task loss jointly
for each task.
3.1 Winning SubNetworks (WSN)
Let each weight be associated with a learnable parameter
we call weight score s, which numerically determines the

--- PAGE 4 ---
PREPRINT, MARCH 2023 4
t-1
t
t
t
(a) selected weights ^t 1
at prior task(b) forward pass
using reused weights(c) backward pass
only on non-used weights(d) selected weights ^t
with subsets of reused weights
Fig. 2: An illustration of Winning SubNetworks (WSN): (a) The top-c% weights ^t 1at prior task are obtained, (b) In the
forward pass of a new task, WSN reuses weights selected from prior tasks, (c) In the backward pass, WSN updates only
non-used weights, and (d) after several iterations of (b) and (c), we acquire again the top-c% weights ^tincluding subsets of
reused weights for the new task.
importance of the weight associated with it; that is, a weight
with a higher weight score is seen as more important. We Ô¨Ånd
a sparse subnetwork ^tof the neural network and assign
it as a solver of the current task t. We use subnetworks
instead of the dense network as solvers for two reasons:
(1) Lottery Ticket Hypothesis [ 18] shows the existence of a
subnetwork that performs as well as the whole network, and
(2) subnetwork requires less capacity than dense networks,
and therefore it inherently reduces the size of the expansion
of the solver.
Motivated by such beneÔ¨Åts, we propose a novel Winning
SubNetworks (WSN1), which is the joint-training method for
continual learning that trains on task - while selecting an
important subnetwork given the task tas shown in Fig. 2. The
illustration of WSN explains how to acquire binary weights
within a dense network step by step. We Ô¨Ånd ^tby selecting
thec% weights with the highest weight scores s, wherecis
the target layerwise capacity ratio in %. A task-dependent
binary weight represents the selection of weights mtwhere
a value of 1denotes that the weight is selected during the
forward pass and 0otherwise. Formally, mtis obtained by
applying a indicator function 1conswhere 1c(s) = 1 if
sbelongs to top- c%scores and 0otherwise. Therefore, the
subnetwork ^tfor tasktis obtained by ^t=mt.
3.2 Soft-Subnetworks (SoftNet)
Several works have addressed overÔ¨Åtting issues in continual
learning from different perspectives, including NCM [ 31],
BiC [ 73], OCS [ 79], and FSLL [ 52]. To mitigate the over-
Ô¨Åtting issue in subnetworks, we use a simple yet efÔ¨Åcient
method named SoftNet proposed by [ 37]. The following new
paradigm, referred to as Regularized Lottery Ticket Hypothe-
sis[37] which is inspired by the Lottery Ticket Hypothesis [18]
has become the cornerstone of SoftNet:
Regularized Lottery Ticket Hypothesis (RLTH). A
randomly-initialized dense neural network contains a regularized
subnetwork that can retain the prior class knowledge while
1. WSN code is available at https://github.com/ihaeyong/WSN.gitproviding room to learn the new class knowledge through isolated
training of the subnetwork.
SoftNet . Based on RLTH, we propose a method, referred
to as Soft-Sub Networks ( SoftNet2). SoftNet jointly learns
the randomly initialized dense model, and soft mask m2
[0;1]jjon Soft-subnetwork on each task training; the soft
mask consists of the major part of the model parameters
m= 1 and the minor ones m< 1wherem= 1 is obtained
by the top-c%of model parameters and m< 1is obtained
by the remaining ones ( 100 top-c%) sampled from the
uniform distribution U(0;1). Here, it is critical to select minor
parameters m< 1in a given dense network.
1
0.4
0.2
0.0
0.2
0.42
0.4
0.2
0.0
0.2
0.4Loss
02468101214
1
0.4
0.2
0.0
0.2
0.42
0.4
0.2
0.0
0.2
0.4Loss
02468101214
(a) epoch=70, c= 10:0% (b) epoch=100, c= 10:0%
Fig. 3: Loss landscapes of Dense Network, HardNet (WSN),
and SoftNet: Subnetworks provide a more Ô¨Çat global minimum
than dense neural networks. To demonstrate the loss landscapes,
we trained a simple three-layered, fully connected model (fc-4-
25-30-3) on the Iris Flower dataset (which is three classiÔ¨Åcation
problem) for 100 epochs [22], [15].
3.3 Convergence of Subnetworks
Convergences of HardNet (WSN) / SoftNet. To interpret the
convergence of SoftNet, we follow the Lipschitz-continuous
2.SoftNet code is available at https://github.com/ihaeyong/
SoftNet-FSCIL.git

--- PAGE 5 ---
PREPRINT, MARCH 2023 5
objective gradients [ 5], [4]: the objective function of dense
networksR:Rd!Ris continuously differentiable and the
gradient function of R, namely,rR:Rd!Rd,Lipschitz
continuous with Lipschitz constant L>0, i.e.,
jjrR() rR(0)jj2Ljj 0jj
for allf;0gRd:(3)
Proposition. Subnetwork achieves a faster rate than dense
networks. To prove this, following the same formula, we
deÔ¨Åne the Lipschitz-continuous objective gradients of sub-
networks as follows:
jjrR(m) rR(0m)jj2Ljj( 0)mjj
for allf;0gRd:(4)
wheremis a binary mask. In comparision of Eq. 3 and
4, we use the theoretical analysis [ 77] where subnetwork
achieve a faster rate of R(m) =O(1=jjmjj2
1)at most.
The comparison is as follows:
jjrR(m) rR(0m)jj2
jj( 0)mjj<
jjrR() rR(0)jj2
jj 0jjL(5)
The smaller the value is, the Ô¨Çatter the solution (loss
landscape) has. The equation is established from the
relationship R(m)R(), whereR(denotes the
best possible loss achievable by convex combinations of all
parameters despite jj( 0)mjj<jj 0jj.
Corollary. Furthermore, we have the following inequality
ifjjR(mhard) R(msoft)jj' 0andjjmhardjj<
jjmsoftjj:
jjrR(mhard) rR(0mhard)jj2
jj( 0)mhardjj
jjrR(msoft) rR(0msoft)jj2
jj( 0)msoftjj(6)
where the equality holds iffjjmhardjj=jjmsoftjj. We
prepare the loss landscapes of Dense Network, HardNet
(WSN), and SoftNet as shown in Figure 3 as an example to
support the above subnetwork‚Äôs inequality.
3.4 Optimization of Winning SubNetworks (WSN)
To jointly learn the model weights and task-adaptive binary
masks of subnetworks associated with each task, given an
objectiveL(), we optimize andswith:
minimize
;sL(mt;Dt): (7)
However, this vanilla optimization procedure presents two
problems: (1) updating all when training for new tasks
will cause interference to the weights allocated for previous
tasks, and (2) the indicator function always has a gradient
value of 0; therefore, updating the weight scores swith its
loss gradient is not possible. To solve the Ô¨Årst problem, we
selectively update the weights by allowing updates only on
the weights not selected in the previous tasks. To do that,
we use an accumulate binary mask Mt 1=_t 1
i=1miwhenlearning task t, then for an optimizer with learning rate ;
theis updated as follows:
  @L
@(1 Mt 1)
; (8)
effectively freezing the weights of the subnetworks selected
for the previous tasks. To solve the second problem, we use
Straight-through Estimator [ 28], [3], [58] in the backward
pass since mtis obtained by top- c%scores. SpeciÔ¨Åcally, we
ignore the derivatives of the indicator function and update
the weight score as follows:
s s @L
@s
: (9)
The use of separate weight scores sas the basis for
selecting subnetwork weights makes it possible to reuse
some of the weights from previously chosen weights mt
in solving the current task t, which can be viewed as
transfer learning . Likewise, previously chosen weights that
are irrelevant to the new tasks are not selected; instead,
weights from the set of not-yet-chosen weights are selected
to meet the target network capacity for each task, which can
be viewed as Ô¨Ånetuning from tasksf1;:::;t 1gto taskt.
Our WSN optimizing procedure is summarized in pseudo
algorithm 1.
Algorithm 1 Winning SubNetworks (WSN) for TIL.
inputfDtgT
t=1, model weights , score weights s, binary mask
M0=0jj, layer-wise capacity c
1:Randomly initialize ands.
2:fortaskt= 1;:::;Tdo
3: forbatch btDtdo
4: Obtain soft-mask mtof the top-c%scores sat each layer
5: ComputeL(mt;bt)
6:   @L
@(1 Mt 1)
.Weight update
7: s s (@L
@s) .Weight score update
8: end for
9:Mt Mt 1_mt .Accumulate binary mask
10:end for
3.5 Winning Ticket Encoding
A binary mask is needed to store task-speciÔ¨Åc weights
for each task in WSN. However, the main issue is that
as the number of tasks increases in deep-learning models,
the number of binary masks required also increases. We
use a compression algorithm to compress all binary task
masks to address this capacity issue and achieve forget-free
continual learning. To accomplish this, we Ô¨Årst convert a
sequence of binary masks into a single accumulated decimal
mask and then convert each integer into an ASCII code
to represent a unique symbol. Next, we apply Huffman
encoding [ 32], a lossless compression algorithm, to the
symbols. We empirically observed that Huffman encoding
compresses 7-bit binary maps with a compression rate of
approximately 78% and decompresses them without any bit
loss. Furthermore, experimental results demonstrate that the
compression rate is sub-linearly increasing with the size of
binary bits.

--- PAGE 6 ---
PREPRINT, MARCH 2023 6
3.6 SoftNet via Complementary Winning Tickets
Similar to WSN‚Äôs optimization discussed in Section 3.4, let
each weight be associated with a learnable parameter we call
weight scores, which numerically determines the importance
of the associated weight. In other words, we declare a weight
with a higher score more important. At Ô¨Årst, we Ô¨Ånd a
subnetwork=m
tof the dense neural network
and then assign it as a solver of the current session t. The
subnetworks associated with each session jointly learn the
model weight and binary mask mt. Given an objective Lt,
we optimizeas follows:
;m
t= minimize
;sLt(mt;Dt): (10)
wheremtis obtained by applying an indicator function 1c
on weight scores s. Note 1c(s) = 1 ifsbelongs to top-c %
scores and 0otherwise.
In the optimization process for FSCIL, however, we
consider two main problems: (1) Catastrophic forgetting: up-
dating allmt 1when training for new sessions will cause
interference with the weights allocated for previous tasks;
thus, we need to freeze all previously learned parameters
mt 1; (2) OverÔ¨Åtting: the subnetwork also encounters
overÔ¨Åtting issues when training an incremental task on a
few samples, as such, we need to update a few parameters
irrelevant to previous task knowledge., i.e., (1 mt 1).
To acquire the optimal subnetworks that alleviate the two
issues, we deÔ¨Åne a soft-subnetwork by dividing the dense
neural network into two parts-one is the major subnetwork
mmajor, and another is the minor subnetwork mminor . The
deÔ¨Åned Soft-SubNetwork (SoftNet) follows as:
msoft=mmajormminor; (11)
wheremmajor is a binary mask and mminorU(0;1)and
represents an element-wise summation. As such, a soft-mask
is given asm
t2[0;1]jjin Eq.10. In the all-experimental
FSCIL setting, mmajor maintains the base task knowledge
t= 1 whilemminor acquires the novel task knowledge t2.
Then, with base session learning rate ;theis updated
as follows:   @L
@msoft
effectively regularize
the weights of the subnetworks for incremental learning.
The subnetworks are obtained by the indicator function
that always has a gradient value of 0; therefore, updating
the weight scores swith its loss gradient is impossible. We
update the weight scores by using Straight-through Estimator
[28], [3], [58] in the backward pass. SpeciÔ¨Åcally, we ignore
the derivatives of the indicator function and update the
weight score s s  @L
@smsoft
, wheremsoft=1for
exploring the optimal subnetwork for base session training.
Our Soft-subnetwork optimizing procedure is summarized in
Algorithm 2. Once a single soft-subnetwork msoftis obtained
in the base session, then we use the soft-subnetwork for the
entire new sessions without updating.
3.7 Soft-SubNetwork for Incremental Learning
We now describe the overall procedure of our soft-pruning-
based incremental learning/inference method under the
following FSCIL settings. This includes the training phase
with a normalized informative measurement, as outlined in
prior work [65], and the inference phase.Algorithm 2 Soft-Subnetworks (SoftNet) for FSCIL.
inputfDtgT
t=1, model weights , and score weights s, layer-
wise capacity c
1:// Training over base classes t= 1
2:Randomly initialize ands.
3:forepoche= 1;2;do
4: Obtain softmask msoftofmmajor andmminorU(0;1)
at each layer
5: forbatchbtDtdo
6: ComputeLbase(msoft;bt)by Eq. 10
7:   @L
@msoft
8:s s  @L
@smsoft
9: end for
10:end for
11:// Incremental learning t2
12:Combine the training data Dt
13: and the exemplars saved in previous few-shot sessions
14:forepoche= 1;2;do
15: forbatchbtDtdo
16: ComputeLm(msoft;bt)by Eq. 12
17:   @L
@mminor
18: end for
19:end for
output model parameters ,s, andmsoft.
Few-shot Class Incremental Learning (FSCIL) aims to learn
new sessions with only a few examples continually. A
FSCIL model learns a sequence of Ttraining sessions
fD1;;DTg, whereDt=fzt
i= (xt
i;yt
i)gnt
iis the training
data of session tandxt
iis an example of class yt
i2Ot. In
FSCIL, the base session D1usually contains a large number
of classes with sufÔ¨Åcient training data for each class. In
contrast, the subsequent sessions ( t2) will only contain a
small number of classes with a few training samples per
class, e.g., the tthsessionDtis often presented as a N-
way K-shot task. In each training session t, the model can
access only the training data Dtand a few examples stored
in the previous session. When the training of session tis
completed, we evaluate the model on test samples from all
classesO=St
i=1Oi, whereOiTOj6=i=;for8i;jT.
Base Training (t= 1) . In the base learning session, we
optimize the soft-subnetwork parameter (including a fully-
connected layer as a classiÔ¨Åer) and weight score swith
cross-entropy loss jointly using the training examples of D1.
Incremental Training (t2). In the incremental few-shot
learning sessions (t2), leveraged by msoft, we
Ô¨Åne-tune few minor parameters mminor of the soft-
subnetwork to learn new classes. Since mminor<1, the
soft-subnetwork alleviates the overÔ¨Åtting of a few samples.
Furthermore, instead of Euclidean distance [65], we employ
a metric-based classiÔ¨Åcation algorithm with cosine distance
to Ô¨Ånetune the few selected parameters. In some cases,
Euclidean distance fails to give the real distances between
representations, especially when two points with the same
distance from prototypes do not fall in the same class. In
contrast, representations with a low cosine distance are
located in the same direction from the origin, providing
a normalized informative measurement. We deÔ¨Åne the loss

--- PAGE 7 ---
PREPRINT, MARCH 2023 7
function as:
Lm(z;msoft) =
 X
z2DX
o2O1(y=o) log 
e d(po;f(x;msoft))
P
ok2Oe d(pok;f(x;msoft))!
(12)
whered(;)denotes cosine distance, pois the prototype
of classo,O=St
i=1Oirefers to all encountered classes,
andD=DtSPdenotes the union of the current training
dataDtand the exemplar set P=fp2;pt 1g, where
Pte(2te<t)is the set of saved exemplars in session te.
Note that the prototypes of new classes are computed by
po=1
NoP
i 1(yi=o)f(xi;msoft)and those of base
classes are saved in the base session, and Nodenotes the
number of the training images of class o. We also save the
prototypes of all classes in Otfor later evaluation.
Inference for Incremental Soft-Subnetwork. In each ses-
sion, the inference is also conducted by a simple nearest
class mean (NCM) classiÔ¨Åcation algorithm [ 54], [65] for fair
comparisons. SpeciÔ¨Åcally, all the training and test samples
are mapped to the embedding space of the feature extractor
f, and Euclidean distance du(;)is used to measure the sim-
ilarity between them. The classiÔ¨Åer gives the kth prototype
indexo
k= arg min o2Odu(f(x;msoft);po)as output.
4 E XPERIMENTS
We now validate our method on several benchmark datasets
against relevant continual learning baselines on Task-
Incremental Learning (TIL) and Few-shot Class Incremental
Learning (FSCIL).
4.1 Task-incremental Learning (TIL)
First, we consider task-incremental continual learning with
a multi-head conÔ¨Åguration for all experiments in the paper.
We follow the experimental setups in recent works [ 62], [78],
[16].
Datasets and architectures. We use six different popular
sequential datasets for CL problems with Ô¨Åve different neural
network architectures as follows: 1) Permuted MNIST
(PMNIST): A variant of MNIST [ 42] where each task has
a deterministic permutation to the input image pixels. 2)
5-Datasets: A mixture of 5 different vision datasets [ 62]:
CIFAR-10 [ 39], MNIST [ 42], SVHN [ 56], FashionMNIST [ 74],
and notMNIST [ 6].3) Omniglot Rotation: An OCR images
dataset composed of 100 tasks, each including 12 classes.
We further preprocess the raw images by generating their
rotated version in 90;180;and270, followed by [ 78].4)
CIFAR-100 Split [39]:A visual object dataset constructed
by randomly dividing 100 classes of CIFAR-100 into ten
tasks with ten classes per task. 5) CIFAR-100 Superclass: We
follow the setting from [ 78] that divides CIFAR-100 dataset
into 20 tasks according to the 20 superclasses, and each
superclass contains Ô¨Åve different but semantically related
classes. 6) TinyImageNet [67]:A variant of ImageNet [ 40]
containing 40 of 5-way classiÔ¨Åcation tasks with the image
size by 64643.
We use two-layered MLP with 100 neurons per layer
for PMNIST, variants of LeNet [ 42] for the experiments onOmniglot Rotation and CIFAR-100 Superclass experiments.
Also, we use a modiÔ¨Åed version of AlexNet similar to [ 64],
[62] for the CIFAR-100 Split dataset and a reduced ResNet-18
similar to [ 9], [62] for 5-Datasets. For TinyImageNet, we
also use the same network architecture as [ 20], [16], which
consists of 4 Conv layers and three fully connected layers.
Baselines. We compare our WSN with strong CL baselines;
regularization-based methods: HAT [64] and EWC [38],
rehearsal-based methods: GPM [62], and a pruning-based
method: PackNet [50] and SupSup [72].PackNet and
SupSup is set to the baseline to show the effectiveness of
re-used weights. We also compare with naive sequential
training strategy, referred to as FINETUNE .Multitask
Learning (MTL) and Single-task Learning (STL) are not a
CL method. MTL trains on multiple tasks simultaneously,
and STL trains on single tasks independently.
We summarize the architecture-based baselines as follows:
1)PackNet [50]: iterative pruning and network re-training
architecture for packing multiple tasks into a single
network.
2)SupSup [72]: Ô¨Ånding supermasks (subnetworks) within
a randomly initialized network for each task in continual
learning.
3)WSN / SoftNet (ours): jointly training model and Ô¨Ånding
task-adaptive subnetworks of novel/prior parameters
for continual learning. Note that, in the inference step,
SoftNet is acquired empirically by injecting small noises
U(0;1e-3)to the backgrounds 0of acquired task WSN
while holding the foregrounds 1of WSN.
Experimental settings. As we directly implement our
method from the ofÔ¨Åcial code of [ 62], we provide the values
for HAT and GPM reported in [ 62]. For Omniglot Rotation
and Split CIFAR-100 Superclass, we deploy the proposed
architecture in multi-head settings with hyperparameters
as reported in [ 78]. All our experiments run on a single-
GPU setup of NVIDIA V100. We provide more details
of the datasets, architectures, and experimental settings,
including the hyperparameter conÔ¨Ågurations for all methods
in Section A.
20 40 60 80 100
Total Capacity Used8990919293Accuracy
WSN (c = 50% )  EWC
GPM
PackNet
SupSup
HATWSN (c = 3% )  
WSN (c = 5% )  
WSN (c = 10% )  
WSN (c = 30% )  
100 420
Total Capacity Used30405060708090Accuracy
EWC
GPM
PackNet
SupSupWSN (c = 3%) 
WSN (c = 
5%)  
WSN (c =     
10%)  
WSN (c = 30%) 
WSN (c = 
50%)
(a) 5-Dataset - ResNet-18 (b) Omniglot Rotation - LeNet
Fig. 4: Accuracy over Total Capacity Usage : The performances of
Hard-WSN gets better than others as total model capacities increase,
and then, saturated approximately at 80 %.
Performance metrics. We evaluate all methods based on the
following four metrics:

--- PAGE 8 ---
PREPRINT, MARCH 2023 8TABLE 1: Performance comparison of the proposed method and baselines on various benchmark datasets. We report the
mean and standard deviation of the average accuracy (ACC), average capacity (CAP), and average forward / backward
transfer (FWT / BWT) across Ô¨Åve independent runs. The best results are highlighted in bold. Values with yanddenote
reported performances from [62] and [78]. We consider PackNet [50] and SupSup [72] as the baselines.
Method Permuted MNIST 5 Datasets Omniglot Rotation
ACC (%) CAP (%) FWT / BWT ACC (%) CAP (%) FWT / BWT ACC (%) CAP (%) FWT / BWT
STL 97.37 (0.01) 1,000.0 - / - 93.44 (0.12) 500.0 - / - 82.13 (0.08)10,000.0 - / -
FINETUNE 78.22 (0.84) 100.0 - / -0.21 (0.01) 80.06 (0.74) 100.0 - / -0.17 (0.01) 44.48 (1.68) 100.0 - / -0.45 (0.02)
EWC [38] 92.01 (0.56) 100.0 - / -0.03 (0.00) 88.64 (0.26)y100.0y- / -0.04 (0.01)y68.66 (1.92)100.0- / -
HAT [64] - - - / - 91.32 (0.18)y100.0y- / -0.03 (0.00)y- - - / -
GPM [62] 94.96 (0.07) 100.0 - / -0.02 (0.01) 91.22 (0.20)y100.0 - / -0.01 (0.00)y85.24 (0.37) 100.0 - / -0.01 (0.00)
PackNet [50] 96.37 (0.1) 96.38 -9.52 (0.6) / 0.0 92.81 (0.1) 82.86 -9.73 (0.5) / 0.0 30.70 (1.5) 399.2 -9.02 (0.8) / 0.0
SupSup [72] 96.31 (0.1) 122.89 (0.1) -9.63 (0.7) / 0.0 93.28 (0.2) 104.27 (0.2) -9.52 (0.6) / 0.0 58.14 (2.4) 407.12 (0.2) -8.87 (0.6) / 0.0
WSN,c= 3% 94.84 (0.1) 19.87 (0.2) -10.25 (0.0) / 0.0 90.57 (0.7) 12.11 (0.1) -9.62 (0.6) / 0.0 80.68 (2.6) 75.87 (1.2) -8.33 (0.1) / 0.0
WSN,c= 5% 95.65 (0.0) 26.49 (0.2) -9.96 (0.7) / 0.0 91.61 (0.2) 17.26 (0.3) -9.86 (0.4) / 0.0 87.28 (0.7) 79.85 (1.2) -8.42 (0.1) / 0.0
WSN,c= 10% 96.14 (0.0) 40.41 (0.6) -10.14 (1.4) / 0.0 92.67 (0.1) 28.01 (0.3) -9.33 (0.4) /0.0 83.10 (1.6) 83.08 (1.6) -8.29 (0.1) /0.0
WSN,c= 30% 96.41 (0.1) 77.73 (0.4) -9.52 (0.4) / 0.0 93.22 (0.3) 62.30 (0.7) -9.68 (0.6) / 0.0 81.89 (1.2) 102.2 (0.9) -8.34 (0.1) / 0.0
WSN,c= 50% 96.24 (0.1) 98.10 (0.3) -9.39 (0.8) /0.0 93.41 (0.1) 86.10 (0.6) -9.51 (0.4) / 0.0 79.80 (2.2) 121.2 (0.5) -8.33 (0.1) / 0.0
WSN,c= 70% 96.29 (0.0) 102.56 (0.1) -9.67 (0.7) / 0.0 92.38 (0.5) 98.73 (0.4) -9.53 (0.4) / 0.0 79.02 (2.5) 140.6 (0.3) -8.32 (0.1) / 0.0
SoftNet,c= 3% 94.84 (0.1) 19.87 (0.2) 23.76 (1.8) /0.0 90.57 (0.7) 12.11 (0.1) 36.08 (7.1) / 0.0 80.68 (2.6) 75.87 (1.2) 51.52 (4.3) / 0.0
SoftNet,c= 5% 95.65 (0.0) 26.49 (0.2) 23.44 (2.0) / 0.0 91.61 (0.2) 17.26 (0.3) 40.19 (4.6) / 0.0 87.28 (0.7) 79.85 (1.2) 56.54 (4.1) /0.0
SoftNet,c= 10% 96.14 (0.0) 40.41 (0.6) 22.12 (1.6) / 0.0 92.67 (0.1) 28.01 (0.3) 38.06 (9.6) / 0.0 83.10 (1.6) 83.08 (1.6) 54.14 (3.5) / 0.0
SoftNet,c= 30% 96.41 (0.1) 77.73 (0.4) 22.12 (1.6) / 0.0 93.22 (0.3) 62.30 (0.7) 40.83 (5.7) / 0.0 81.89 (1.2) 102.2 (0.9) 53.03 (1.5) / 0.0
SoftNet,c= 50% 96.24 (0.1) 98.10 (0.3) 21.91 (1.9) / 0.0 93.41 (0.1) 86.10 (0.6) 40.81 (7.2) / 0.0 79.80 (2.2) 121.2 (0.5) 50.15 (0.7) / 0.0
SoftNet,c= 70% 96.29 (0.0) 99.21 (0.1) 20.57 (1.2) / 0.0 92.38 (0.5) 98.73 (0.4) 41.04 (5.2) /0.0 79.02 (2.5) 140.6 (0.3) 49.62 (1.7) / 0.0
MTL 96.70 (0.02)y100.0 - / - 91.54 (0.28)y100.0 - / - 81.23 (0.52) 100.0 - / -
1)Accuracy (ACC) measures the average of the Ô¨Ånal classiÔ¨Å-
cation accuracy on all tasks: ACC =1
TPT
i=1AT;i, where
AT;iis the test accuracy for task iafter training on task T.
2)Capacity (CAP) measures the total percentage of non-
zero weights plus the prime masks for all tasks as follows:
Capacity = (1 S) +(1 )T
32, where we assume all task
weights of 32-bit precision. Sis the sparsity of MTand
the average compression rate 0:78that we acquired
through 7bit Huffman encoding, which depends on the
size of bit-binary maps; the compression rate also
depends on compression methods typically.
3)Forward Transfer (FWT) measures how much the repre-
sentations that we learned so far help learn new tasks,
namely:FWT =1
TPT
i=2Ai 1;i RiwhereRiis the
accuracy of a randomly initialized network on task i.
4)Backward Transfer (BWT) measures the forgetting during
continual learning. Negative BWT means that learning
new tasks causes the forgetting on past tasks: BWT =
1
T 1PT 1
i=1AT;i Ai;i.
4.2 Few-shot Class Incremental Learning (FSCIL)
We introduce experimental setups - Few-Shot Class
Incremental Learning (FSCIL) settings to provide soft-
subnetworks‚Äô effectiveness. We empirically evaluate and
compare our soft-subnetworks with state-of-the-art methods
and vanilla subnetworks in the following subsections.
Datasets. To validate the effectiveness of the soft-subnetwork,
we follow the standard FSCIL experimental setting. We
randomly select 60 classes as the base and 40 as new classes
for CIFAR-100 and miniImageNet. In each incremental
learning session, we construct 5-way 5-shot tasks by
randomly picking Ô¨Åve classes and sampling Ô¨Åve training
examples for each class.
Baselines. We mainly compare our SoftNet with architecture-
based methods for FSCIL: FSLL [ 52] that selects important
parameters for each session, and HardNet, representing a
binary subnetwork. Furthermore, we compare other FSCILmethods such as iCaRL [ 59], Rebalance [ 31], TOPIC [ 68],
IDLVQ-C [ 10], and F2M [ 65]. We also include a joint training
method [ 65] that uses all previously seen data, including
the base and the following few-shot tasks for training as
a reference. Furthermore, we Ô¨Åx the classiÔ¨Åer re-training
method (cRT) [ 35] for long-tailed classiÔ¨Åcation trained with
all encountered data as the approximated upper bound.
Experimental settings. The experiments are conducted with
NVIDIA GPU RTX8000 on CUDA 11.0. We also randomly
split each dataset into multiple sessions. We run each
algorithm ten times for each dataset and report their mean
accuracy. We adopt ResNet18 [ 24] as the backbone network.
For data augmentation, we use standard random crop and
horizontal Ô¨Çips. In the base session training stage, we select
top-c%weights at each layer and acquire the optimal soft-
subnetworks with the best validation accuracy. In each
incremental few-shot learning session, the total number of
training epochs is 6, and the learning rate is 0:02. We train
new class session samples using a few minor weights of the
soft-subnetwork (Conv4x layer of ResNet18) obtained by the
base session learning. We specify further experiment details
in Appendix.
5 R ESULTS ONTASK-INCREMENTAL LEARNING
5.1 Comparisons with Baselines
We evaluate our algorithm on three-standard benchmark
datasets: Permuted MNIST, 5-Datasets, and Omniglot Rota-
tion. We set PackNet and SupSup to baselines as non-reused
weight methods and compared WSN with the baselines,
including other algorithms as shown in Table 1. Our WSN
outperformed all the baselines in three measurements, achiev-
ing the best average accuracy of 96.41%, 93.41%, and 87.28%
while using the least capacity compared to the other existing
methods, respectively. Moreover, our WSN was proved to be
a forget-free model like PackNet. Compared with PackNet,
however, WSN showed the effectiveness of reused weights
for continual learning and its scalability in the Omniglot
Rotation experiments with the least capacities by a large

--- PAGE 9 ---
PREPRINT, MARCH 2023 9
1 5 10 15 20 25 30 35 40
Increasing No. of Tasks55606570758085Average Accuracy (%)
c =  3% 
c = 10%
c = 50%
1 5 10 15 20 25 30 35 40
Increasing No. of Tasks20406080100120140Average Progressive Capacity (%)
c=10% + 3bit-Huffman 
 c=  10% + 7bit-Huffman 
 c = 10% c=  10% + Binary Map c=10% + 5bit-Huffman 
(a) Per Task Accuracy over c (b) Model Capacity
Fig. 5: WSN‚Äôs Performances and Compressed Capacities :
Sequence of TinyImageNet Dataset Experiments: (a) Setting
c= 10% shows generalized performances over others and
(b) Within 40 tasks, the 7-bits compressed model increase its
capacity the least.
margin. Figure 4 provides the accuracy over total capac-
ity usage. Our WSN‚Äôs accuracy is higher than PackNet‚Äôs,
approximately at 80% total capacity usage on 5-Dataset,
and WSN outperformed others at 80% total capacity usage
on Omniglot Rotation. The lower performances of PackNet
might attribute to Omniglot Rotation dataset statistics since,
regardless of random rotations, tasks could share visual
features in common such as circles, curves, and straight
lines. Therefore, non-reused methods might be brutal to train
a new task model unless prior weights were transferred
to the current model, a.k.a. forward transfer learning. To
show the WSN‚Äôs power of forward transferring knowledge,
SoftNet was prepared by initializing zero-part of masks to
small random perturbations. Then, we observed that, in
the inference step, SoftNet showed great power to transfer
knowledge while maintaining WSN‚Äôs performances of ACC
and CAP .
5.2 Comparisons with the SOTA in TIL
We use a multi-head setting to evaluate our WSN algorithm
under the more challenging visual classiÔ¨Åcation benchmarks.
The WSN‚Äôs performances are compared with others w.r.t
three measurements on three major benchmark datasets
as shown in Table 2. Our WSN outperformed all state-
of-the-art, achieving the best average accuracy of 76.38%,
61.79%, and 71.96%. WSN is also a forget-free model (BWT =
ZERO) with the least model capacity in these experiments.
Note that we assume the model capacities are compared
based on the model size without extra memory, such as
samples. We highlight that our method achieves the highest
accuracy, the lowest capacity, and backward transfer on all
datasets. Figure 5 shows the process of performance and
compressed capacity changing with the number of tasks on
the TinyImageNet datasets, where the ‚ÄúAverage Progressive
Capacity‚Äù metric is deÔ¨Åned as the average capacity (the
proportion of the number of network weights used for any
one of the tasks) after Ô¨Åve runs of the experiment with
different seed values. Furthermore, we consistently showed
progressively improved performances of WSN than others on
CIFAR-100 Split datasets as shown in Figure 7. The increasing
number of reused weights (see Figure 9) could explain the
progressively improved performances as shown in Figure 8.
Other forward-transferring results are depicted in Appendix.5.3 Forget-Free Performance and Model Capacity
We prepare results on performance and bit-map compressed
capacity on the TinyImageNet dataset as shown in Figure 5
- ‚Äúc=0.1‚Äù and ‚Äúc=0.1+7bit-Huffman‚Äù refers respectively to
using 10% of network weights and no compression on the
binary mask and the latter refers to the same with 7bit-
Huffman encoding. In Figure 5 (a), using initial capacity,
c= 0:1shows better performances over others. With Ô¨Åxed
c= 0:1, the bit-wise Huffman compression rate delivers
positive as the number of tasks increases, as shown in
Figure 5 (b). The most interesting part is that the average
compression rate increases as the number of bits to compress
increases, and the increasing ratio of reused weights (symbols
with a high probability in the Huffman encoding) might
affect the compression rate (symbols with a small probability
might be rare in the Huffman tree, where the infrequent
symbols tend to have long bit codes). We investigated how
the compression rate is related to the total model capacity.
The more bits the binary mask compression does, the less
the model capacity to save is required. This shows that
within 40 tasks, N-bit Huffman compressed capacities are
sub-linearly increasing as binary map capacities increase
linearly. The 7-bit Huffman encoding is enough to compress
binary maps without exceeding the model capacity, even
though the compression rate depends on compression
methods typically.
5.4 Catastrophic Forgetting From WSN‚Äôs Viewpoint
We interpreted how reused weights affect the inference
performances on the TinyImageNet dataset as shown in
Figure 6. We divide all used weights for solving sequential
tasks into speciÔ¨Åc sets for more precise interpretability.
All used weights (a) within a trained dense network are
separated as follows: all used represents all activated sets of
weights up to task t 1.per task represents an activated set of
weights at task t.new per task represents a new activated set
of weights at task t.reused per task represents an intersection
set of weights per task and all used weights. reused for all
tasks represents an intersection set of weights reused from
task1up to taskt 1.
First, our WSN adaptively reuses weights to solve the
sequential tasks. In Figure 6 (b), initial and progressive task
capacity start from cvalue; the proportion of reused weights
per task decreases in the early task learning stage. However,
it tends to be progressively saturated to c= 0:1since the
number of the new activated set of weights decreases, and
the proportion of reused weights for all prior tasks tends
to decrease. In other words, WSN uses diverse weights to
solve the sequential tasks within all used weights rather than
depending on the reused weights for all prior tasks as the
number of tasks increases.
Second, our WSN provides a stepping stone for forget-
free continual learning. Regarding the beneÔ¨Åts of WSN, in
Figure 6 (c), we interpret the importance of three types of
weights through an ablation study. The additional evalua-
tions were performed by the acquired task binary masks
and trained models to investigate the importance of reused
weights in each layer, where the ‚Äùw/‚Äù refers to ‚Äùwith
reused network weights‚Äù and the ‚Äùw/o‚Äù refers to ‚Äùwithout
reused network weights.‚Äù Model forgetting occurred from

--- PAGE 10 ---
PREPRINT, MARCH 2023 10
TABLE 2: Performance comparisons of the proposed method and other state-of-the-art including baselines - PackNet [ 50]
and SupSup [ 72] - on various benchmark datasets. We report the mean and standard deviation of the average accuracy
(ACC), average capacity (CAP), and average forward / backward transfer (FWT / BWT) across 5independent runs with
Ô¨Åve seeds under the same experimental setup [ 16]. The best results are highlighted in bold. Also, ydenotes results reported
from [16].
Method CIFAR-100 Split CIFAR-100 Superclass TinyImageNet
ACC (%) CAP (%) FWT / BWT (%) ACC (%) CAP (%) FWT / BWT (%) ACC (%) CAP (%) FWT / BWT (%)
La-MaML [20] 71.37 (0.7)y100.0 - / -5.39 (0.5)y54.44 (1.4)y100.0 - / -6.65 (0.9)y66.90 (1.7)y100.0 - / -9.13 (0.9)y
GPM [62] 73.18 (0.5)y100.0 - / -1.17 (0.3)y57.33 (0.4)y100.0 - / -0.37 (0.1)y67.39 (0.5)y100.0 - / 1.45 (0.2)y
FS-DGPM [16] 74.33 (0.3)y100.0 - / -2.71 (0.2)y58.81 (0.3)y100.0 - / -2.97 (0.4)y70.41 (1.3)y100.0 - / -2.11 (0.9)y
PackNet [50] 72.39 (0.4) 96.38 (0.0) 0.12 (0.6) / 0.0 58.78 (0.5) 126.65 (0.0) 0.56 (0.8) / 0.0 55.46 (1.2) 188.67 (0.0) -0.44 (0.5) / 0.0
SupSup [72] 75.47 (0.3) 129.00 (0.1) 0.06 (0.5) / 0.0 61.70 (0.3) 162.49 (0.0) -0.50 (0.6) / 0.0 59.60 (1.1) 214.52 (0.9) -0.82 (0.6) / 0.0
WSN, c= 3% 72.23 (0.3) 18.56 (0.3) 0.13 (0.2) /0.0 54.99 (0.7) 22.30 (0.2) 0.15 (0.9) /0.0 68.72 (1.6) 37.19 (0.2) -0.49 (0.3) / 0.0
WSN, c= 5% 73.56 (0.2) 25.09 (0.4) 0.05 (0.5) / 0.0 57.99 (1.3) 27.37 (0.3) 0.59 (0.6) / 0.0 71.22 (0.9) 41.98 (0.5) -0.73 (0.3) / 0.0
WSN, c= 10% 75.46 (0.1) 39.87 (0.6) 0.07 (0.3) / 0.0 60.45 (0.4) 38.55 (0.2) -0.35 (0.5) / 0.0 71.96 (1.4) 48.65 (3.0) -0.63 (0.4) / 0.0
WSN, c= 30% 77.12 (0.3) 80.26 (1.5) -0.02 (0.5) / 0.0 61.47 (0.3) 63.47 (1.3) -0.49 (0.5) / 0.0 70.92 (1.3) 73.44 (2.4) -0.32 (0.1) /0.0
WSN, c= 50% 77.46 (0.4) 99.13 (0.5) -0.04 (0.3) / 0.0 61.79 (0.2) 80.93 (1.6) -0.26 (0.7) / 0.0 69.06 (0.8) 92.03 (1.8) -0.33 (0.1) / 0.0
WSN, c= 70% 78.08 (0.3) 105.77 (0.2) -0.06 (0.1) / 0.0 61.24 (0.2) 97.40 (1.0) -0.16 (1.2) / 0.0 67.18 (1.6) 108.82 (1.3) -0.37 (0.4) / 0.0
SoftNet, c= 3% 72.23 (0.3) 18.56 (0.3) 52.07 (0.5) /0.0 55.00 (0.7) 22.30 (0.2)34.88 (1.1) /0.0 68.72 (1.6) 37.19 (0.2)49.98 (1.4) / 0.0
SoftNet, c= 5% 73.57 (0.2) 25.09 (0.4) 51.70 (0.5) / 0.0 57.99 (1.3) 27.37 (0.3) 34.76 (1.1) / 0.0 71.22 (0.9) 41.98 (0.5) 50.83 (0.6) / 0.0
SoftNet, c= 10% 75.47 (0.1) 39.87 (0.6) 51.58 (0.6) / 0.0 60.46 (0.4) 38.55 (0.2) 32.21 (0.9) / 0.0 71.97 (1.4) 48.65 (3.0) 48.86 (1.6) / 0.0
SoftNet, c= 30% 77.12 (0.3) 80.26 (1.5) 51.64 (0.9) / 0.0 61.47 (0.3) 63.47 (1.3) 32.04 (1.3) / 0.0 70.92 (1.4) 73.44 (2.4) 47.54 (1.0) / 0.0
SoftNet, c= 50% 77.46 (0.4) 99.13 (0.5) 52.06 (1.3) / 0.0 61.80 (0.2) 80.93 (1.6) 30.40 (0.7) / 0.0 69.06 (0.8) 92.03 (1.8) 47.80 (1.1) / 0.0
SoftNet, c= 70% 78.08 (0.3) 105.77 (0.2) 54.07 (0.5) / 0.0 61.24 (0.2) 97.40 (1.0) 30.94 (1.4) /0.0 67.19 (1.6) 108.82 (1.3) 48.27 (0.9) / 0.0
Multitask 79.75 (0.4)y100.0 - / - 61.00 (0.2)y100.0 - / - 77.10 (1.1)y100.0 - / -
A Dense  Networkall used per task
reused for all tasksreused per taskt-1 t
1 5 10 15 20 25 30 35 40
Increasing No. of Tasks01020304050Average Progressive Capacity (%)
c=10%
c=10% new per task
c=10% reused per task  
c=10% reused for all tasks
1 5 10 15 20 25 30 35 40
Increasing No. of Tasks020406080100120140Average Accuracy (%)
c=10% , w/o reused for all tasksc=10%, w/  reused for all tasksc=10%, w/  reused per taskc=10% 
c=10%, w/o reused per task
1 5 10 15 20 25 30 35 40
Increasing No. of Tasks020406080100120140Average Accuracy (%)
c=10% , 
 c=10%, w/o reused per task of linear2 c=10%, w/o reused per task of conv4c=10%, w/o reused per task of conv1 
(a) A Diagram of Capacities (b) Capacities over c (c) Acc. of Reused Weights (d) Acc. w/o Reused Weights
Fig. 6: Layer-wise Analysis of WSN on TinyImageNet Dataset Experiments: (a) Weights reusability within a dense network,
(b) Capacities except to binary maps are determined by c= 10% , (c) The most signiÔ¨Åcant forgetting occurs from weights
without reused per task; the signiÔ¨Åcance of weights reused for all tasks gets lower, and from task 7, all used weights seem to
be enough to infer all tasks, and (d) In the inference step, we inspected a layer-wise forgetting caused by removing (setting
masking value 1 to 0) reused weights per task on the trained model and observed performance drops signiÔ¨Åcantly at Conv1
layer.
the performances without using weights reused per task
severely. The most signiÔ¨Åcant weights were weights reused
per task, the subset of all used weights; the importance of the
weights reused for all prior tasks decreases as the number
of tasks increases since its capacity gets small relatively, as
shown in Figure 6 (b). Moreover, in Figure 6 (d), we inspected
layer-wise forgetting caused by removing weights reused
per task of network layers; the performance sensitivities
were quite diverse. In particular, we observed that the most
performance drops at the Conv1 layer.
Finally, our WSN reuses weights if the knowledge from
previous tasks is already enough to solve the task at hand
and employs a few new weights otherwise. SpeciÔ¨Åcally, from
task 7, all used weights seem enough to infer all tasks
since the weights reused per task catch up with the task
performances. For more generalized forget-free continual
learning, the model should consider the layer-wise sensitivity
of weights reused per task when selecting weights reused for
all prior tasks. These analyses might broadly impact other
machine learning Ô¨Åelds, such as transfer, semi-supervised,
and domain adaptation.
1 2 3 4 5 6 7 8 9 10
Increasing No. of Tasks666870727476788082Average Accuracy (%)EWC
La-MAML
GPM
FS-DGPM
WSN (ours)Fig. 7: Comparisons on CIFAR100-Split with [ 16]: It was
difÔ¨Åcult to determine the superiority of WSN only by
comparing the performances in the early stage, however,
it shows the progressively better performances of WSN than
others.
5.5 Sparse Binary Maps
We prepared task-wise binary mask correlations to inves-
tigate how WSN reuses weights over sequential tasks. As

--- PAGE 11 ---
PREPRINT, MARCH 2023 11
TABLE 3: Computational efÔ¨Åciency of WSN compared with PackNet and SupSup. We report model training time in hours.
Method Permuted MNIST 5 Dataset Omniglot Rotation
ACC (%) CAP (%) Tr. TIME (h) ACC (%) CAP (%) Tr. TIME (h) ACC (%) CAP (%) Tr. TIME (h)
PackNet 96.37 (0.04) 96.38 (0.00) 0.49 (0.03) 92.81 (0.12) 82.86 (0.00) 3.38 (0.11) 30.70 (1.50) 399.2 (0.00) 7.30 (0.01)
SupSup 96.31 (0.09) 122.89 (0.07) 0.48 (0.06) 93.28 (0.21) 104.27 (0.21) 3.20 (0.01) 58.14 (2.42) 407.12 (0.17) 6.92 (0.03)
WSN (best) 96.41 (0.07) 77.73 (0.36) 0.35 (0.02) 93.41 (0.13) 86.10 (0.57) 3.02 (0.03) 87.28 (0.72) 79.85 (1.19) 6.33 (0.04)
Method CIFAR-100 Split CIFAR-100 Superclass TinyImageNet
ACC (%) CAP (%) Tr. TIME (h) ACC (%) CAP (%) Tr. TIME (h) ACC (%) CAP (%) Tr. TIME (h)
PackNet 72.39 (0.37) 96.38 (0.00) 1.04 (0.19) 58.78 (0.52) 126.65 (0.00) 0.46 (0.01) 55.46 (1.22) 188.67 (0.00) 1.39 (0.03)
SupSup 75.47 (0.30) 129.00 (0.03) 0.79 (0.14) 61.70 (0.31) 162.49 (0.00) 0.37 (0.00) 59.60 (1.05) 214.52 (0.89) 0.92 (0.00)
WSN (best) 76.38 (0.34) 99.13 (0.48) 0.71 (0.09) 61.79 (0.23) 80.93 (1.58) 0.36 (0.00) 71.96 (1.41) 48.65 (3.03) 0.89 (0.00)
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1076.3 1.1 0.8 0.9 0.5 1.5 0.7 1.1 1.0 0.6
76.3 68.4 1.0 0.8 0.8 1.2 0.8 0.8 1.0 1.1
76.3 68.4 73.1 0.9 0.6 0.9 0.7 1.1 0.5 0.5
76.3 68.4 73.1 69.8 0.9 0.7 0.6 1.0 0.5 0.9
76.3 68.4 73.1 69.8 73.9 0.8 0.9 1.3 0.9 0.9
76.3 68.4 73.1 69.8 73.9 74.4 0.6 1.2 0.8 1.3
76.3 68.4 73.1 69.8 73.9 74.4 75.9 1.9 0.6 1.0
76.3 68.4 73.1 69.8 73.9 74.4 75.9 72.9 0.7 0.8
76.3 68.4 73.1 69.8 73.9 74.4 75.9 72.9 73.1 0.9
76.3 68.4 73.1 69.8 73.9 74.4 75.9 72.9 73.1 77.8
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1076.2 43.3 35.4 27.0 24.8 19.7 16.1 15.2 15.1 15.9
76.2 68.5 48.0 40.7 35.6 27.9 26.3 21.5 19.1 21.9
76.2 68.5 73.2 45.8 40.2 27.7 30.9 27.7 23.1 22.3
76.2 68.5 73.2 69.9 52.6 42.8 38.5 36.8 29.8 32.3
76.2 68.5 73.2 69.9 73.9 52.2 45.0 45.1 39.4 39.6
76.2 68.5 73.2 69.9 73.9 74.4 55.5 51.5 43.1 44.9
76.2 68.5 73.2 69.9 73.9 74.4 75.9 60.3 49.3 48.8
76.2 68.5 73.2 69.9 73.9 74.4 75.9 72.9 52.7 52.9
76.2 68.5 73.2 69.9 73.9 74.4 75.9 72.9 73.1 63.4
76.2 68.5 73.2 69.9 73.9 74.4 75.9 72.9 73.1 77.8
(a) WSN,c= 5% (b) SoftNet, c= 5%
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1075.8 1.1 0.7 0.6 1.1 1.9 0.2 1.3 1.5 1.0
75.8 73.3 1.1 0.7 0.7 1.5 0.2 1.1 1.0 1.0
75.8 73.3 77.9 0.7 0.9 2.1 0.4 1.5 1.0 1.3
75.8 73.3 77.9 76.8 1.2 1.4 0.4 1.1 0.9 1.3
75.8 73.3 77.9 76.8 79.2 1.6 0.6 0.9 1.2 1.5
75.8 73.3 77.9 76.8 79.2 79.0 0.5 1.0 0.8 1.6
75.8 73.3 77.9 76.8 79.2 79.0 80.6 0.7 0.8 1.6
75.8 73.3 77.9 76.8 79.2 79.0 80.6 76.6 0.7 1.1
75.8 73.3 77.9 76.8 79.2 79.0 80.6 76.6 77.8 0.7
75.8 73.3 77.9 76.8 79.2 79.0 80.6 76.6 77.8 83.8
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1075.8 41.4 45.2 34.1 36.1 29.7 36.2 41.3 35.4 38.5
75.8 73.3 53.8 49.9 47.3 39.7 45.7 47.0 43.7 48.4
75.8 73.3 77.9 51.4 46.4 38.9 44.7 47.1 39.0 46.6
75.8 73.3 77.9 76.8 57.1 48.0 51.0 50.8 44.4 56.7
75.8 73.3 77.9 76.8 79.2 53.1 52.0 50.6 47.1 55.6
75.8 73.3 77.9 76.8 79.2 79.0 57.2 53.4 47.5 55.8
75.8 73.3 77.9 76.8 79.2 79.0 80.6 62.0 52.7 62.0
75.8 73.3 77.9 76.8 79.2 79.0 80.6 76.6 55.2 61.7
75.8 73.3 77.9 76.8 79.2 79.0 80.6 76.6 77.8 64.2
75.8 73.3 77.9 76.8 79.2 79.0 80.6 76.6 77.8 83.8
(c) WSN,c= 70% (d) SoftNet, c= 70%
Fig. 8: Average Forward Transfer Matrix on CIFAR-100 Split.
WSN / SoftNet with c= 5% andc= 70% , respectively.
shown in Figure 9 (a) and (b), WSN tends to progressively
transfer weights used for prior tasks to weights for new ones
compared with PackNet. Figure 9 (c) and (d) showed that the
tendency of reused weights differs according to the c. This
result might suggest that more sparse reused binary maps
lead to generalization than others.
6 R ESULTS ONFEW-SHOT CIL
6.1 Results and Comparisons
We compared SoftNet with the architecture-based FSLL and
HardNet (WSN) methods. We pick FSLL as an architecture-
based baseline since it selects important parameters for
acquiring old/new class knowledge. The architecture-based
results on CIFAR-100 and miniImageNet are presented in Ta-
ble 4 and Table 5 respectively. The performances of HardNet
show the effectiveness of the subnetworks that go with less
model capacity compared to dense networks. To emphasize
our point, we found that ResNet18, with approximately 50%
parameters, achieves comparable performances with FSLL on
CIFAR-100 and miniImageNet. In addition, the performances
of ResNet20 with 30% parameters (HardNet) are comparable
with those of FSLL on CIFAR-100.
Experimental results are prepared to analyze the overall
performances of SoftNet according to the sparsity and dataset
as shown in Figure 10. As we increase the number of
T1T10 T20 T30 T40 T50 T60 T70 T80 T90T100T1
T10
T20
T30
T40
T50
T60
T70
T80
T90
T100
T1T10 T20 T30 T40 T50 T60 T70 T80 T90T100T1
T10
T20
T30
T40
T50
T60
T70
T80
T90
T100(a) PackNet (b) WSN, c= 5%
T1 T5 T10 T15 T20 T25 T30 T35 T40T1
T5
T10
T15
T20
T25
T30
T35
T40
T1 T5 T10 T15 T20 T25 T30 T35 T40T1
T5
T10
T15
T20
T25
T30
T35
T40
(c) WSN,c= 10% (d) WSN,c= 50%
Fig. 9: Task-wise Binary Map Correlations on Omniglot
Rotation (Top row) and TinyImageNet (Bottom row). WSN
reuses the weights, while PackNet does not. WSN with c=
50% reuses task weights more than the case of c= 10% as
shown in higher correlation results than others.
parameters employed by SoftNet, we achieve performance
gain on both benchmark datasets. The performance variance
of SoftNet‚Äôs sparsity seems to depend on datasets because
the performance variance on CIFAR-100 is less than that
on miniImageNet. In addition, SoftNet retains prior session
knowledge successfully in both experiments as described in
the dashed line, and the performances of SoftNet ( c= 60:0%)
on the new class session (8, 9) of CIFAR-100 than those of
SoftNet (c= 80:0%) as depicted in the dashed-dot line. From
these results, we could expect that the best performances
depend on the number of parameters and properties of
datasets.
Our SoftNet outperforms the state-of-the-art methods
and cRT, which is used as the approximate upper bound
of FSCIL [ 65] as shown in Table 4 and Table 5. Moreover,
Figure 11 represents the outstanding performances of SoftNet
on CIFAR-100 and miniImageNet. SoftNet provides a new
upper bound on each dataset, outperforming cRT, while
HardNet (WSN) provides new baselines among pruning-
based methods.

--- PAGE 12 ---
PREPRINT, MARCH 2023 12
TABLE 4: ClassiÔ¨Åcation accuracy of ResNet18 on CIFAR-100 for 5-way 5-shot incremental learning. Underbar denotes the
comparable results with FSLL [52]. denotes the results reported from [65].
MethodsessionsThe gap
with cRT1 2 3 4 5 6 7 8 9
cRT [65] 65.18 63.89 60.20 57.23 53.71 50.39 48.77 47.29 45.28 -
iCaRL [59]66.52 57.26 54.27 50.62 47.33 44.99 43.14 41.16 39.49 -5.79
Rebalance [31]66.66 61.42 57.29 53.02 48.85 45.68 43.06 40.56 38.35 -6.93
FSLL [52]65.18 56.24 54.55 51.61 49.11 47.27 45.35 43.95 42.22 -3.08
iCaRL [59] 64.10 53.28 41.69 34.13 27.93 25.06 20.41 15.48 13.73 -31.55
Rebalance [31] 64.10 53.05 43.96 36.97 31.61 26.73 21.23 16.78 13.54 -31.74
TOPIC [12] 64.10 55.88 47.07 45.16 40.11 36.38 33.96 31.55 29.37 -15.91
F2M [65] 64.71 62.05 59.01 55.58 52.55 49.96 48.08 46.28 44.67 -0.61
FSLL [52] 64.10 55.85 51.71 48.59 45.34 43.25 41.52 39.81 38.16 -7.12
HardNet (WSN), c= 50% 64.80 60.77 56.95 53.53 50.40 47.82 45.93 43.95 41.91 -3.37
HardNet (WSN), c= 80% 69.65 64.60 60.59 56.93 53.60 50.80 48.69 46.69 44.63 -0.65
HardNet (WSN), c= 99% 71.95 66.83 62.75 59.09 55.92 53.03 50.78 48.52 46.31 +1.03
SoftNet, c= 50% 69.20 64.18 60.01 56.43 53.11 50.62 48.60 46.51 44.61 -0.67
SoftNet, c= 80% 70.38 65.04 60.94 57.26 54.13 51.58 49.52 47.36 45.16 -0.12
SoftNet, c= 99% 72.62 67.31 63.05 59.39 56.00 53.23 51.06 48.83 46.63 +1.35
TABLE 5: ClassiÔ¨Åcation accuracy of ResNet18 on miniImageNet for 5-way 5-shot incremental learning. Underbar denotes
the comparable results with FSLL [52]. denotes the results reported from [65].
MethodsessionsThe gap
with cRT1 2 3 4 5 6 7 8 9
cRT [65] 67.30 64.15 60.59 57.32 54.22 51.43 48.92 46.78 44.85 -
iCaRL [59]67.35 59.91 55.64 52.60 49.43 46.73 44.13 42.17 40.29 -4.56
Rebalance [31]67.91 63.11 58.75 54.83 50.68 47.11 43.88 41.19 38.72 -6.13
FSLL [52]67.30 59.81 57.26 54.57 52.05 49.42 46.95 44.94 42.87 -1.11
iCaRL [59] 61.31 46.32 42.94 37.63 30.49 24.00 20.89 18.80 17.21 -27.64
Rebalance [31] 61.31 47.80 39.31 31.91 25.68 21.35 18.67 17.24 14.17 -30.68
TOPIC [12] 61.31 50.09 45.17 41.16 37.48 35.52 32.19 29.46 24.42 -20.43
IDLVQ-C [10] 64.77 59.87 55.93 52.62 49.88 47.55 44.83 43.14 41.84 -3.01
F2M [65] 67.28 63.80 60.38 57.06 54.08 51.39 48.82 46.58 44.65 -0.20
FSLL [52] 66.48 61.75 58.16 54.16 51.10 48.53 46.54 44.20 42.28 -2.57
HardNet (WSN), c= 50% 65.13 60.37 56.12 53.17 50.17 47.74 45.34 43.35 42.13 -2.72
HardNet (WSN), c= 80% 69.73 64.46 60.42 57.09 54.09 51.18 48.76 46.81 45.66 +0.81
HardNet (WSN), c= 90% 64.68 59.80 55.70 52.82 50.01 47.30 45.17 43.34 42.09 -2.76
SoftNet, c= 50% 72.83 67.23 62.82 59.41 56.44 53.55 50.92 48.99 47.60 +2.75
SoftNet, c= 80% 76.63 70.13 65.92 62.52 59.49 56.56 53.71 51.72 50.48 +5.63
SoftNet, c= 90% 77.00 70.38 65.94 62.45 59.32 56.25 53.76 51.75 50.39 +5.54
SoftNet, c= 97% 77.17 70.32 66.15 62.55 59.48 56.46 53.71 51.68 50.24 +5.39
6.2 Considerations From SoftNet
Through extensive experiments, we deduce the following
conclusions for incorporating our method in the few-shot
class incremental learning regarding architectures.
Comparisions of HardNet (WSN) and SoftNet . Further-
more, increasing the number of network parameters leads
to better overall performance in both subnetworks types,
as shown in Figure 12 and Figure 13. Subnetworks, in
the form of HardNet and SoftNet, tend to retain prior
(base) session knowledge denoted in dashed ( ) line,
and HardNet seems to be able to classify new session class
samples without continuous updates stated in dashed-dot
( ) line. From this, we could expect how much previous
knowledge HardNet learned at the base session to help
learn new incoming tasks (Forward Transfer). The overall
performances of SoftNet are better than HardNet since
SoftNet improves both base/new session knowledge by
updating minor subnetworks. Subnetworks have a broaderspectrum of performances on miniImageNet (Figure 13) than
on CIFAR-100 (Figure 12). This could be an observation
caused by the dataset complexity - i.e., if the miniImagenet
dataset is more complex or harder to learn for a subnetwork
or a deep model as such subnetworks need more parameters
to learn miniImageNet than the CIFAR-100 dataset.
Smoothness of SoftNet. SoftNet has a broader spectrum
of performances than HardNet on miniImageNet. 20% of
minor subnet might provide a smoother representation
than HardNet because the performance of SoftNet was
the best approximately at c= 80% . We could expect that
model parameter smoothness guarantees quite competitive
performances from these results. To support the claim, we
prepared the loss landscapes of a dense neural network,
HardNet, and SoftNet on two Hessian eigenvectors [ 76] as
shown in Fig. 3. We observed the following points through
simple experiments. From these results, we can expect how
much knowledge the speciÔ¨Åed subnetworks can retain and

--- PAGE 13 ---
PREPRINT, MARCH 2023 13
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=80.0%
SoftNet with c=60.0%
SoftNet with c=30.0%
SoftNet with c=20.0%
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=80.0%
SoftNet with c=60.0%
SoftNet with c=30.0%
SoftNet with c=20.0%
(a) CIFAR-100 (b) miniImageNet
Fig. 10: ClassiÔ¨Åcation accuracy of SoftNet on CIFAR-100 and miniImageNet for 5-way 5-shot FSCIL: the overall performance
depends on capacity cand the softness of subnetwork. Note that solid( ), dashed( ), and dashed-dot( ) lines denote
overall, base, and novel class performances respectively.
1 2 3 4 5 6 7 8 9
Sessions3040506070Accuracy (%)
CIFAR-100
1 2 3 4 5 6 7 8 9
Sessions3040506070Accuracy (%)
miniImageNet
cRT iCaRL Rebalance FSLL TOPIC F2M HardNet (best) SoftNet (best)
Fig. 11: Comparision of subnetworks (HardNet (WSN) and SoftNet) with state-of-the-art methods.
acquire on each dataset. The loss landscapes of Subnetworks
(HardNet and SoftNet) were Ô¨Çatter than those of dense
neural networks. The minor subnet of SoftNet helped Ô¨Ånd a
Ô¨Çat global minimum despite random scaling weights in the
training process.
Moreover, we compared the embeddings using t-SNE
plots as shown in Figure 14. In t-SNE‚Äôs 2D embedding spaces,
the overall discriminative of SoftNet is better than that of
HardNet in terms of base class set and novel class set. This
70% of minor subnet affects SoftNet positively in base session
training and offers good initialized weights in novel session
training.
Preciseness. Regarding Ô¨Åne-grained and small-sized
CUB200-2011 FSCIL settings as shown in Appendix Table,
HardNet (WSN) also shows comparable results with the base-
lines, and SoftNet outperforms others as denoted in Table 9.
In this FSCIL setting, we acquired the best performances
of SoftNet through the speciÔ¨Åc parameter selections. As of
now, our SoftNet achieves state-of-the-art results on the three
datasets.
6.3 Additional Comparisons with SOTA
Comparisons with SOTA . We compare SoftNet with the
following state-of-art-methods on TOPIC class split [ 68]
of three benchmark datasets - CIFAR100 (Appendix, Table.7), miniImageNet (Appendix, Table. 8), and CUB-200-2011
(Appendix, Table. 9). We summarize the current FSCIL
methods such as CEC [83],LIMIT [84],MetaFSCIL [13],
C-FSCIL [27],Subspace Reg. [1],Entropy-Reg [47], and
ALICE [57]. Leveraged by regularized backbone ResNet,
SoftNet outperformed all existing current works on CI-
FAR100, miniImageNet. On CUB-200-201, the performances
of SoftNet were comparable with those of ALICE and LIMIT,
considering that ALICE used class/data augmentations and
LIMIT added an extra multi-head attention layer.
7 C ONCLUSION
Inspired by Regularized Lottery Ticket Hypothesis (RLTH) ,
which states that competitive smooth (non-binary) subnet-
works exist within a dense network in continual learning
tasks, we investigated the performances of the proposed
two architecture-based continual learning methods referred
to as Winning SubNetworks (WSN) which sequentially
learns and selects an optimal binary-subnetwork (WSN)
and an optimal non-binary Soft-Subnetwork (SoftNet) for
each task, respectively. SpeciÔ¨Åcally, WSN and SoftNet jointly
learned the regularized model weights and task-adaptive
non-binary masks of subnetworks associated with each
task whilst attempting to select a small set of weights to
be activated (winning ticket) by reusing weights of the

--- PAGE 14 ---
PREPRINT, MARCH 2023 14
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=10.0%
HardNet with c=10.0%
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=30.0%
HardNet with c=30.0%
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=80.0%
HardNet with c=80.0%
(a)c= 10% (b)c= 30% (c)c= 80%
Fig. 12: Performances of HardNet (WSN) v.s. SoftNet on CIFAR-100 for 5-way 5-shot FSCIL: the overall performance depends
on capacity cand the softness of subnetwork. Note that solid( ), dashed( ), and dashed-dot( ) lines denote overall, base,
and novel class performances respectively.
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=10.0%
HardNet with c=10.0%
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=30.0%
HardNet with c=30.0%
1 2 3 4 5 6 7 8 9
Sessions0102030405060708090Overall Acc.(%)
SoftNet with c=80.0%
HardNet with c=80.0%
(a)c= 10% (b)c= 30% (c)c= 80%
Fig. 13: Performances of HardNet (WSN) v.s. SoftNet on miniImageNet for 5-way 5-shot FSCIL: the overall performance
depends on capacity cand the softness of the subnetwork. Note that solid( ), dashed( ), and dashed-dot( ) lines denote
overall, base, and novel class performances respectively.
15
 10
 5
 0 5 10 15
tsne-2d-one15
10
5
051015tsne-2d-two0246810
1214
16
1820
22
24 26
283032
343638404244464850
5254 5658606264
15
 10
 5
 0 5 10 15
tsne-2d-one15
10
5
051015tsne-2d-two02
46810
12
14
16
18
20
22
2426
2830323436 38
40
42
44
4648
5052
5456
586062 64
(a) HardNet (WSN) with c= 30% @Session2 (b) SoftNet with c= 30% @Session2
Fig. 14: t-SNE Plots of HardNet (WSN) v.s. SoftNet on miniImageNet for 5-way 5-shot FSCIL: t-SNE plots represent the
embeddings of the even-numbered test class samples and compare one another. Note Session1 Class Set: f0;;59gand Session2
Novel Class Set:f60;;64g.
prior subnetworks. The proposed WSN and SoftNet were
inherently immune to catastrophic forgetting as each selected
subnetwork model does not infringe upon other subnetworks
in Task Incremental Learning (TIL). In TIL, binary masks
spawned per winning ticket were encoded into one N-bit
binary digit mask, then compressed using Huffman coding
for a sub-linear increase in network capacity to the number
of tasks. Surprisingly, we observed that in the inference
step, SoftNet generated by injecting small noises to the
backgrounds of acquired WSN (holding the foregrounds ofWSN) provides excellent forward transfer power for future
tasks in TIL. Softnet showed its effectiveness over WSN in
regularizing parameters to tackle the overÔ¨Åtting, to a few
examples in Few-shot Class Incremental Learning (FSCIL).
REFERENCES
[1] Afra Feyza Aky ¬®urek, Ekin Aky ¬®urek, Derry Wijaya, and Jacob
Andreas. Subspace regularizers for few-shot class incremental
learning. arXiv preprint arXiv:2110.07059 , 2021.

--- PAGE 15 ---
PREPRINT, MARCH 2023 15
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Char-
lin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online
continual learning with maximal interfered retrieval. In Advances
in Neural Information Processing Systems (NeurIPS) , 2019.
[3] Yoshua Bengio, Nicholas L ¬¥eonard, and Aaron C. Courville. Esti-
mating or propagating gradients through stochastic neurons for
conditional computation. CoRR , 2013.
[4] L¬¥eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization
methods for large-scale machine learning. Siam Review , 60(2):223‚Äì
311, 2018.
[5] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex
optimization . Cambridge university press, 2004.
[6] Yaroslav Bulatov. notmnist dataset. 2011.
[7] Arslan Chaudhry, Naeemullah Khan, Puneet K Dokania, and
Philip HS Torr. Continual learning in low-rank orthogonal
subspaces. In Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
[8] Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus Rohrbach, and
Mohamed Elhoseiny. EfÔ¨Åcient lifelong learning with a-gem. In
Proceedings of the International Conference on Learning Representations
(ICLR) , 2019.
[9] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Tha-
laiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and
M Ranzato. Continual learning with tiny episodic memories. arXiv
preprint arXiv:1902.10486 , 2019.
[10] Kuilin Chen and Chi-Guhn Lee. Incremental few-shot learning
via vector quantization in deep embedded space. In International
Conference on Learning Representations , 2020.
[11] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and
Zhangyang Wang. Long live the lottery: The existence of winning
tickets in lifelong learning. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2021.
[12] Ali Cheraghian, ShaÔ¨Ån Rahman, Pengfei Fang, Soumava Kumar
Roy, Lars Petersson, and Mehrtash Harandi. Semantic-aware
knowledge distillation for few-shot class-incremental learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2534‚Äì2543, 2021.
[13] Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao Yu, and
Jin Tang. Metafscil: A meta-learning approach for few-shot class
incremental learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 14166‚Äì14175, 2022.
[14] Daiki Chijiwa, Shin‚Äôya Yamaguchi, Atsutoshi Kumagai, and Yasu-
toshi Ida. Meta-ticket: Finding optimal subnetworks for few-shot
learning within randomly initialized neural networks. In Advances
in Neural Information Processing Systems , 2022.
[15] Belur V Dasarathy. Nosing around the neighborhood: A new
system structure and classiÔ¨Åcation rule for recognition in partially
exposed environments. IEEE Transactions on Pattern Analysis and
Machine Intelligence , (1):67‚Äì71, 1980.
[16] Danruo Deng, Guangyong Chen, Jianye Hao, Qiong Wang, and
Pheng-Ann Heng. Flattening sharpness for dynamic gradient
projection memory beneÔ¨Åts continual learning. In Advances in
Neural Information Processing Systems (NeurIPS) , 2021.
[17] Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato,
and Nando de Freitas. Predicting parameters in deep learning. In
Advances in Neural Information Processing Systems (NeurIPS) , 2013.
[18] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis:
Finding sparse, trainable neural networks. In Proceedings of the
International Conference on Learning Representations (ICLR) , 2019.
[19] Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual
learning via neural pruning. arXiv preprint arXiv:1903.04476 , 2019.
[20] Gunshi Gupta, Karmesh Yadav, and Liam Paull. La-maml: Look-
ahead meta learning for continual learning. In Advances in Neural
Information Processing Systems (NeurIPS) , 2020.
[21] Song Han, Jeff Pool, John Tran, and William Dally. Learning both
weights and connections for efÔ¨Åcient neural network. In Proceedings
of the International Conference on Learning Representations (ICLR) ,
2016.
[22] Peter Hart. The condensed nearest neighbor rule (corresp.). IEEE
transactions on information theory , 14(3):515‚Äì516, 1968.
[23] Demis Hassabis, Dharshan Kumaran, Christopher SummerÔ¨Åeld,
and Matthew Botvinick. Neuroscience-inspired artiÔ¨Åcial intelli-
gence. Neuron , 95(2):245‚Äì258, 2017.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 770‚Äì778,
2016.[25] Xu He and Herbert Jaeger. Overcoming catastrophic interference
using conceptor-aided backpropagation. In International Conference
on Learning Representations , 2018.
[26] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu,
Yee Whye Teh, and Razvan Pascanu. Task agnostic continual
learning via meta learning. arXiv preprint arXiv:1906.05201 , 2019.
[27] Michael Hersche, Geethan Karunaratne, Giovanni Cherubini, Luca
Benini, Abu Sebastian, and Abbas Rahimi. Constrained few-shot
class-incremental learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9057‚Äì9067, 2022.
[28] Geoffrey Hinton. Neural networks for machine learning, 2012.
[29] Geoffrey E Hinton and Drew Van Camp. Keeping the neural
networks simple by minimizing the description length of the
weights. In Proceedings of the sixth annual conference on Computational
learning theory , pages 5‚Äì13, 1993.
[30] Sepp Hochreiter and J ¬®urgen Schmidhuber. Simplifying neural nets
by discovering Ô¨Çat minima. Advances in neural information processing
systems , 7, 1994.
[31] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua
Lin. Learning a uniÔ¨Åed classiÔ¨Åer incrementally via rebalancing.
InProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 831‚Äì839, 2019.
[32] David A Huffman. A method for the construction of minimum-
redundancy codes. Proceedings of the IRE , 40(9):1098‚Äì1101, 1952.
[33] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan,
and Samy Bengio. Fantastic generalization measures and where to
Ô¨Ånd them. arXiv preprint arXiv:1912.02178 , 2019.
[34] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon.
Continual learning with node-importance based adaptive group
sparse regularization. In Advances in Neural Information Processing
Systems (NeurIPS) , 2020.
[35] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert
Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling represen-
tation and classiÔ¨Åer for long-tailed recognition. arXiv preprint
arXiv:1910.09217 , 2019.
[36] Haeyong Kang, Rusty John Lloyd Mina, Sultan Rizky Hikmawan
Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung Ju Hwang,
and Chang D Yoo. Forget-free continual learning with winning
subnetworks. In International Conference on Machine Learning , pages
10734‚Äì10750. PMLR, 2022.
[37] Haeyong Kang, Jaehong Yoon, Sultan Rizky Hikmawan Madjid,
Sung Ju Hwang, and Chang D Yoo. On the soft-subnetwork for
few-shot class incremental learning. arXiv preprint arXiv:2209.07529 ,
2022.
[38] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness,
Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan,
Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcom-
ing catastrophic forgetting in neural networks. 2017.
[39] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers
of features from tiny images. 2009.
[40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet
classiÔ¨Åcation with deep convolutional neural networks. Advances
in neural information processing systems , 25:1097‚Äì1105, 2012.
[41] Abhishek Kumar and Hal Daume III. Learning task grouping and
overlap in multi-task learning. In Proceedings of the International
Conference on Machine Learning (ICML) , 2012.
[42] Yann LeCun. The mnist database of handwritten digits. 1998.
[43] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Pe-
ter Graf. Pruning Ô¨Ålters for efÔ¨Åcient convnets. arXiv preprint
arXiv:1608.08710 , 2016.
[44] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming
Xiong. Learn to grow: A continual structure learning framework for
overcoming catastrophic forgetting. In Proceedings of the International
Conference on Machine Learning (ICML) , 2019.
[45] Zhizhong Li and Derek Hoiem. Learning without forgetting. In
Proceedings of the European Conference on Computer Vision (ECCV) ,
2016.
[46] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando,
and Koray Kavukcuoglu. Hierarchical representations for efÔ¨Åcient
architecture search, 2017.
[47] Huan Liu, Li Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu, Jun Chen,
and Jin Tang. Few-shot class-incremental learning via entropy-
regularized data-free replay. arXiv preprint arXiv:2207.11213 , 2022.
[48] David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient episodic
memory for continual learning. In Advances in Neural Information
Processing Systems (NeurIPS) , 2017.

--- PAGE 16 ---
PREPRINT, MARCH 2023 16
[49] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback:
Adapting a single network to multiple tasks by learning to mask
weights. In Proceedings of the European Conference on Computer Vision
(ECCV) , 2018.
[50] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple
tasks to a single network by iterative pruning. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recognition , pages
7765‚Äì7773, 2018.
[51] Nicolas Y Masse, Gregory D Grant, and David J Freedman.
Alleviating catastrophic forgetting using context-dependent gating
and synaptic stabilization. Proceedings of the National Academy of
Sciences , 115(44):E10467‚ÄìE10475, 2018.
[52] Pratik Mazumder, Pravendra Singh, and Piyush Rai. Few-shot
lifelong learning. arXiv preprint arXiv:2103.00991 , 2021.
[53] Michael McCloskey and Neal J Cohen. Catastrophic interference
in connectionist networks: The sequential learning problem. In
Psychology of learning and motivation , volume 24, pages 109‚Äì165.
Elsevier, 1989.
[54] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela
Csurka. Distance-based image classiÔ¨Åcation: Generalizing to new
classes at near-zero cost. IEEE transactions on pattern analysis and
machine intelligence , 35(11):2624‚Äì2637, 2013.
[55] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan
Pascanu, and Hassan Ghasemzadeh. Linear mode connectivity in
multitask and continual learning. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2021.
[56] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
Bo Wu, and Andrew Y. Ng. Reading digits in natural images
with unsupervised feature learning. In NIPS Workshop on Deep
Learning and Unsupervised Feature Learning 2011 , 2011.
[57] Can Peng, Kun Zhao, Tianren Wang, Meng Li, and Brian C Lovell.
Few-shot class-incremental learning from an open-set perspective.
InEuropean Conference on Computer Vision , pages 382‚Äì397. Springer,
2022.
[58] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali
Farhadi, and Mohammad Rastegari. What‚Äôs hidden in a randomly
weighted neural network? In Proceedings of the IEEE International
Conference on Computer Vision and Pattern Recognition (CVPR) , 2020.
[59] Sylvestre-Alvise RebufÔ¨Å, Alexander Kolesnikov, Georg Sperl, and
Christoph H Lampert. icarl: Incremental classiÔ¨Åer and represen-
tation learning. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition , pages 2001‚Äì2010, 2017.
[60] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina
Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without
forgetting by maximizing transfer and minimizing interference.
arXiv preprint arXiv:1810.11910 , 2018.
[61] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert
Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu,
and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671 , 2016.
[62] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection
memory for continual learning. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2021.
[63] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
Overcoming catastrophic forgetting with hard attention to the task.
InInternational Conference on Machine Learning , pages 4548‚Äì4557.
PMLR, 2018.
[64] Joan Serr `a, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
Overcoming catastrophic forgetting with hard attention to the task.
InProceedings of the International Conference on Machine Learning
(ICML) , 2018.
[65] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and
Xiao-Ming Wu. Overcoming catastrophic forgetting in incremental
few-shot learning by Ô¨Ånding Ô¨Çat minima. Advances in Neural
Information Processing Systems , 34, 2021.
[66] Hanul Shin, Jung Kwon Lee, Jaehon Kim, and Jiwon Kim. Continual
learning with deep generative replay. In Advances in Neural
Information Processing Systems (NeurIPS) , 2017.
[67] Stanford. Available online at http://cs231n.stanford.edu/tiny-
imagenet-200.zip. CS 231N , 2021.
[68] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing
Wei, and Yihong Gong. Few-shot class-incremental learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12183‚Äì12192, 2020.
[69] Sebastian Thrun. A Lifelong Learning Perspective for Mobile Robot
Control . Elsevier, 1995.[70] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews,
Razvan Pascanu, and Yee Whye Teh. Functional regularisation for
continual learning with gaussian processes. In Proceedings of the
International Conference on Learning Representations (ICLR) , 2020.
[71] Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari. Dis-
covering neural wirings. Advances in Neural Information Processing
Systems , 32, 2019.
[72] Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha
Kembhavi, Mohammad Rastegari, Jason Yosinski, and Ali Farhadi.
Supermasks in superposition. In Advances in Neural Information
Processing Systems (NeurIPS) , 2020.
[73] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu,
Yandong Guo, and Yun Fu. Large scale incremental learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 374‚Äì382, 2019.
[74] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a
novel image dataset for benchmarking machine learning algorithms.
arXiv , 2017.
[75] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In
Advances in Neural Information Processing Systems (NeurIPS) , 2018.
[76] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney.
Pyhessian: Neural networks through the lens of the hessian. In 2020
IEEE international conference on big data (Big data) , pages 581‚Äì590.
IEEE, 2020.
[77] Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans,
and Qiang Liu. Good subnetworks provably exist: Pruning via
greedy forward selection. In International Conference on Machine
Learning , pages 10820‚Äì10830. PMLR, 2020.
[78] Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang.
Scalable and order-robust continual learning with additive parame-
ter decomposition. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2020.
[79] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang.
Online coreset selection for rehearsal-based continual learning. In
Proceedings of the International Conference on Learning Representations
(ICLR) , 2022.
[80] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang.
Lifelong learning with dynamically expandable networks. In
Proceedings of the International Conference on Learning Representations
(ICLR) , 2018.
[81] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang,
Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, and Yingyan
Lin. Drawing early-bird tickets: Towards more efÔ¨Åcient training of
deep networks. arXiv preprint arXiv:1909.11957 , 2019.
[82] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual
learning through synaptic intelligence. In International Conference
on Machine Learning , pages 3987‚Äì3995. PMLR, 2017.
[83] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and
Yinghui Xu. Few-shot incremental learning with continually
evolved classiÔ¨Åers. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 12455‚Äì12464, 2021.
[84] Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di Xie, Shiliang Pu, and De-
Chuan Zhan. Few-shot class-incremental learning by sampling
multi-phase tasks. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022.
[85] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. De-
constructing lottery tickets: Zeros, signs, and the supermask. In
Advances in Neural Information Processing Systems (NeurIPS) , 2019.
[86] Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, and Zheng-Jun Zha.
Self-promoted prototype reÔ¨Ånement for few-shot class-incremental
learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 6801‚Äì6810, 2021.
Haeyong Kang (S‚Äô05) received the M.S. degree
in Systems and Information Engineering from
University of Tsukuba in 2007. From April 2007
to October 2010, he worked as an associate re-
search engineer at LG Electronics. With working
experiences at Korea Institute of Science and
Technology (KIST) and the University of Tokyo,
He is currently pursuing the Ph.D at School
of Electrical Engineering, KAIST. His current
research interests include unbiased machine
learning and continual learning.

--- PAGE 17 ---
PREPRINT, MARCH 2023 17
Jaehong Yoon He received the B.S. and M.S. de-
grees in Computer Science from Ulsan National
Institute of Science and Technology (UNIST), and
received the Ph.D. degree in the School of Com-
puting from Korea Advanced Institute of Science
and Technology (KAIST). He is currently working
as a postdoctoral research fellow at KAIST. His
current research interests include efÔ¨Åcient deep
learning, on-device learning, and learning with
real-world data.
Sultan Rizky Madjid received a B.S. degree in
Electrical Engineering with a double major in
Mechanical Engineering from KAIST in 2021 and
an M.S. degree in Electrical Engineering from
KAIST in 2023. His research interests include
model compression, sparse representations in
deep learning, and continual learning.
Sung Ju Hwang He received the B.S. degree in
Computer Science and Engineering from Seoul
National University. He received the M.S. and
Ph.D. degrees in Computer Science from The
University of Texas at Austin. From September
2013 to August 2014, he was a postdoctoral
research associate at Disney Research. From
September 2013 to December 2017, he was an
assistant professor in the School of Electric and
Computer Engineering at UNIST. Since 2017, he
has been on the faculty at the Korea Advanced
Institute of Science and Technology (KAIST), where he is currently a
KAIST Endowed Chair Professor in the Kim Jaechul School of ArtiÔ¨Åcial
Intelligence and School of Computing at KAIST.
Chang D. Yoo (Senior Member, IEEE) He re-
ceived the B.S. degree in Engineering and Ap-
plied Science from the California Institute of Tech-
nology, the M.S. degree in Electrical Engineering
from Cornell University, and the Ph.D. degree in
Electrical Engineering from the Massachusetts
Institute of Technology. From January 1997 to
March 1999, he was Senior Researcher at Korea
Telecom (KT). Since 1999, he has been on the
faculty at the Korea Advanced Institute of Science
and Technology (KAIST), where he is currently
a Full Professor with tenure in the School of Electrical Engineering and
an Adjunct Professor in the Department of Computer Science. He also
served as Dean of the OfÔ¨Åce of Special Projects and Dean of the OfÔ¨Åce
of International Relations.
APPENDIX A
EXPERIMENTAL DETAILS OF WSN FOR TIL
We followed similar experimental setups (architectures and
hyper-parameters) described in [ 62] for baseline comparisons
and explained in [16] for SOTA comparisons.A.1 Architecture Details
All the networks for our experiments are implemented in
a multi-head setting. Two-layered MLP: In conducting the
PMNIST experiments, we are following the exact setup as
denoted by [ 62] fully-connected network with two hidden
layers of 100 [48].
Reduced ResNet18: In conducting the 5-Dataset experi-
ments, we use a smaller version of ResNet18 with three
times fewer feature maps across all layers as denoted by [ 48].
ModiÔ¨Åed LeNet: In conducting the Omniglot Rotation and
CIFAR-100 Superclass experiments, we use a large variant of
LeNet as the base network with 64-128-2500-1500 neurons
based on [78].
ModiÔ¨Åed AlexNet: In conducting the split CIFAR-100
dataset, we use a modiÔ¨Åed version of AlexNet similar to
[64], [62].
4 Conv layers and 3 Fully connected layers: For TinyIma-
geNet, we use the same network architecture as [20], [16].
A.2 List of Hyperparameters
TABLE 6: List of hyperparameters for the baselines and our
WSN. Here, ‚Äôlr‚Äô and ‚Äôoptim‚Äô represents (initial) learning rate and
optimizer used for training. We represent PMNIST as ‚Äôperm‚Äô, 5-
Datasets as ‚Äô5data‚Äô, Omniglot Rotation as ‚Äôomniglot‚Äô, CIFAR-100
Split as ‚Äôcifar100-split‚Äô, and CIFAR-100 Superclass as ‚Äôcifar100-
sc‚Äô.
Methods Hyperparameters
EWC lr : 0.03 (perm)
optim : sgd
regularization coefÔ¨Åcient : 1000 (perm)
GPM lr : 0.01(perm, omniglot), 0.1 (5data)
optim : sgd
ns: 300 (perm), 100 (5data), 125 (omniglot)
PackNet lr = 0.001 (perm, 5data, omniglot)
optim : adam
c: 0.1 (perm), 0.2 (5data), 0.02 (omniglot)
WSN (ours) lr = 0.001
(perm, 5data, omniglot,
cifar100-split, cifar100-sc, tinyimagenet)
optim : adam
Table 6 details the hyperparameter setup for the baselines
and our approach. nsin GPM denotes the number of
random training examples sampled from the replay buffer
to construct the representation matrix for each architecture
layer.
APPENDIX B
EXPERIMENTAL DETAILS OF SOFTNET FOR FSCIL
B.1 Datasets
The following datasets are utilized for comparisons with
current state-of-the-art:
CIFAR-100 In CIFAR-100, each class contains 500images for
training and 100images for testing. Each image has a size
of3232. Here, we follow an identical FSCIL procedure as
in [65], dividing the dataset into a base session with 60 base
classes and eight novel sessions with a 5-way 5-shot problem
on each session.

--- PAGE 18 ---
PREPRINT, MARCH 2023 18
T1 T5 T10T1
T5
T10
T1 T5 T10T1
T5
T10
(a)c= 5% (b)c= 10%
T1 T5 T10T1
T5
T10
T1 T5 T10T1
T5
T10
(c)c= 30% (d)c= 50%
Fig. 15: Average Binary Map Correlation on Sequence of PMNIST
Experiments: the binary maps get overlapped with prior ones as the
number of tasks increases.
T1 T5T1
T5
T1 T5T1
T5
(a)c= 5% (b)c= 10%
T1 T5T1
T5
T1 T5T1
T5
(c)c= 30% (d)c= 50%
Fig. 16: Average Binary Map Correlation on Sequence of 5-Dataset
Experiments: the binary maps get overlapped with prior ones as the
number of tasks increases.
miniImageNet miniImageNet consists of RGB images from
100 different classes, where each class contains 500training
images and 100test images of size 8484. Initially proposed
for few-shot learning problems, miniImageNet is part of a
much larger ImageNet dataset. Compared with CIFAR-100,
the miniImageNet dataset is more complex and suitable for
prototyping. The setup of miniImageNet is similar to that of
CIFAR-100. To proceed with our evaluation, we follow the
procedure described in [ 65], incorporating 60 base classes
and eight novel sessions through 5-way 5-shot problems.
CUB-200-2011 CUB-200-2011 contains 200Ô¨Åne-grained bird
species with 11;788 images with varying images for each
class. To proceed with experiments, we split the dataset
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1096.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
96.9 96.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
96.9 96.5 96.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
96.9 96.5 96.0 96.2 0.0 0.0 0.0 0.0 0.0 0.0
96.9 96.5 96.0 96.2 95.6 0.0 0.0 0.0 0.0 0.0
96.9 96.5 96.0 96.2 95.6 95.4 0.0 0.0 0.0 0.0
96.9 96.5 96.0 96.2 95.6 95.4 95.4 0.0 0.0 0.0
96.9 96.5 96.0 96.2 95.6 95.4 95.4 95.1 0.0 0.0
96.9 96.5 96.0 96.2 95.6 95.4 95.4 95.1 94.9 0.0
96.9 96.5 96.0 96.2 95.6 95.4 95.4 95.1 94.9 94.7
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1096.9 32.1 16.6 10.5 10.2 10.9 9.811.4 8.711.4
96.9 96.5 32.0 16.4 12.6 12.3 8.712.3 11.3 11.6
96.9 96.5 96.0 28.7 16.5 13.2 11.7 12.4 10.2 11.6
96.9 96.5 96.0 96.2 31.0 19.8 13.0 13.6 10.2 13.4
96.9 96.5 96.0 96.2 95.6 31.6 19.1 14.7 11.9 13.1
96.9 96.5 96.0 96.2 95.6 95.4 34.5 19.9 15.3 15.3
96.9 96.5 96.0 96.2 95.6 95.4 95.4 38.0 20.0 16.4
96.9 96.5 96.0 96.2 95.6 95.4 95.4 95.1 35.0 23.4
96.9 96.5 96.0 96.2 95.6 95.4 95.4 95.1 94.9 37.7
96.9 96.5 96.0 96.2 95.6 95.4 95.4 95.1 94.9 94.7(a) WSN,c= 5% (b) SoftNet, c= 5%
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1097.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
97.5 96.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
97.5 96.4 95.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0
97.5 96.4 95.7 95.5 0.0 0.0 0.0 0.0 0.0 0.0
97.5 96.4 95.7 95.5 95.6 0.0 0.0 0.0 0.0 0.0
97.5 96.4 95.7 95.5 95.6 95.3 0.0 0.0 0.0 0.0
97.5 96.4 95.7 95.5 95.6 95.3 95.5 0.0 0.0 0.0
97.5 96.4 95.7 95.5 95.6 95.3 95.5 95.5 0.0 0.0
97.5 96.4 95.7 95.5 95.6 95.3 95.5 95.5 95.6 0.0
97.5 96.4 95.7 95.5 95.6 95.3 95.5 95.5 95.6 95.4
T1T2T3T4T5T6T7T8T9T10T1
T2
T3
T4
T5
T6
T7
T8
T9
T1097.5 24.1 18.1 15.8 11.5 13.2 13.9 12.9 12.1 12.3
97.5 96.4 29.8 23.6 16.7 16.3 19.7 15.9 18.0 19.4
97.5 96.4 95.7 28.9 20.3 21.7 22.9 16.1 21.0 18.1
97.5 96.4 95.7 95.5 29.1 24.6 21.7 18.1 20.7 16.0
97.5 96.4 95.7 95.5 95.6 31.2 25.4 18.4 20.5 19.0
97.5 96.4 95.7 95.5 95.6 95.3 33.3 20.5 25.6 23.1
97.5 96.4 95.7 95.5 95.6 95.3 95.5 27.7 24.4 24.2
97.5 96.4 95.7 95.5 95.6 95.3 95.5 95.5 34.4 25.0
97.5 96.4 95.7 95.5 95.6 95.3 95.5 95.5 95.6 33.7
97.5 96.4 95.7 95.5 95.6 95.3 95.5 95.5 95.6 95.4
(c) WSN,c= 70% (d) SoftNet, c= 70%
Fig. 17: Average Forward Transfer Matrix on Permuted
MNIST. WSN / SoftNet with c = 5% and c = 70%, respectively
T1 T2 T3 T4 T5T1
T2
T3
T4
T568.9 0.0 0.0 0.0 0.0
68.9 99.2 0.0 0.0 0.0
68.9 99.2 90.6 0.0 0.0
68.9 99.2 90.6 99.2 0.0
68.9 99.2 90.6 99.2 94.3
T1 T2 T3 T4 T5T1
T2
T3
T4
T568.7 37.8 23.2 15.5 16.7
68.7 99.2 31.1 90.3 27.0
68.7 99.2 90.3 62.2 35.2
68.7 99.2 90.3 99.2 69.2
68.7 99.2 90.3 99.2 94.3
(a) WSN,c= 5% (b) SoftNet, c= 5%
T1 T2 T3 T4 T5T1
T2
T3
T4
T576.9 0.0 0.0 0.0 0.0
76.9 99.3 0.0 0.0 0.0
76.9 99.3 92.2 0.0 0.0
76.9 99.3 92.2 99.2 0.0
76.9 99.3 92.2 99.2 94.4
T1 T2 T3 T4 T5T1
T2
T3
T4
T576.9 35.1 25.6 20.8 26.8
76.9 99.3 41.6 92.0 56.6
76.9 99.3 92.2 57.0 52.5
76.9 99.3 92.2 99.2 68.6
76.9 99.3 92.2 99.2 94.4
(c) WSN,c= 70% (d) SoftNet, c= 70%
Fig. 18: Average Forward Transfer Matrix on 5-Dataset.
WSN / SoftNet with c = 5% and c = 70%, respectively
into 6;000 training images, and 6;000 test images as in
[68]. During training, We randomly crop each image to size
224224. We Ô¨Åx the Ô¨Årst 100 classes as base classes, utilizing
all samples in these respective classes to train the model. On
the other hand, we treat the remaining 100 classes as novel
categories split into ten novel sessions with a 10-way 5-shot
problem in each session.
B.2 Comparisons with SOTA
We compare SoftNet with the following state-of-art-methods
on TOPIC class split [ 68] of three benchmark datasets -
CIFAR100 (Table 7), miniImageNet (Table 8), and CUB-200-
2011 (Table 9).

--- PAGE 19 ---
PREPRINT, MARCH 2023 19
TABLE 7: ClassiÔ¨Åcation accuracy of ResNet18 on CIFAR-100 for 5-way 5-shot incremental learning with the same class split
as in TOPIC [12].denotes the results reported from [65].yrepresents our reproduced results.
Methodsessions The gap
with cRT 1 2 3 4 5 6 7 8 9
cRT [65]72.28 69.58 65.16 61.41 58.83 55.87 53.28 51.38 49.51
TOPIC [12] 64.10 55.88 47.07 45.16 40.11 36.38 33.96 31.55 29.37 -20.14
CEC [83] 73.07 68.88 65.26 61.19 58.09 55.57 53.22 51.34 49.14 -0.37
F2M [65] 71.45 68.10 64.43 60.80 57.76 55.26 53.53 51.57 49.35 -0.16
LIMIT [84] 73.81 72.09 67.87 63.89 60.70 57.77 55.67 53.52 51.23 +1.72
MetaFSCIL [13] 74.50 70.10 66.84 62.77 59.48 56.52 54.36 52.56 49.97 +0.46
ALICE [57] 79.00 70.50 67.10 63.40 61.20 59.20 58.10 56.30 54.10 +4.59
Entropy-Reg [47] 74.40 70.20 66.54 62.51 59.71 56.58 54.52 52.39 50.14 +0.63
C-FSCIL [27] 77.50 72.45 67.94 63.80 60.24 57.34 54.61 52.41 50.23 +0.72
FSLL [52] 64.10 55.85 51.71 48.59 45.34 43.25 41.52 39.81 38.16 -11.35
HardNet (WSN), c= 50% 78.35 74.12 70.13 65.88 62.74 59.56 57.98 56.31 54.32 +4.81
HardNet (WSN), c= 80% 79.27 75.38 71.11 66.68 63.32 60.06 58.16 56.40 54.31 +4.80
SoftNet, c= 50% 79.88 75.54 71.64 67.47 64.45 61.09 59.07 57.29 55.33 +5.82
SoftNet, c= 80% 80.33 76.23 72.19 67.83 64.64 61.39 59.32 57.37 54.94 +5.43
TABLE 8: ClassiÔ¨Åcation accuracy of ResNet18 on miniImageNet for 5-way 5-shot incremental learning with the same class
split as in TOPIC [12].denotes results reported from [65].
Methodsessions The gap
with cRT 1 2 3 4 5 6 7 8 9
cRT [65]72.08 68.15 63.06 61.12 56.57 54.47 51.81 49.86 48.31 -
TOPIC [12] 61.31 50.09 45.17 41.16 37.48 35.52 32.19 29.46 24.42 -23.89
IDLVQ-C [10] 64.77 59.87 55.93 52.62 49.88 47.55 44.83 43.14 41.84 -6.47
CEC [83] 72.00 66.83 62.97 59.43 56.70 53.73 51.19 49.24 47.63 -0.68
F2M [65] 72.05 67.47 63.16 59.70 56.71 53.77 51.11 49.21 47.84 -0.43
LIMIT [84] 73.81 72.09 67.87 63.89 60.70 57.77 55.67 53.52 51.23 +2.92
MetaFSCIL [13] 72.04 67.94 63.77 60.29 57.58 55.16 52.90 50.79 49.19 +0.88
ALICE [57] 80.60 70.60 67.40 64.50 62.50 60.00 57.80 56.80 55.70 +7.39
C-FSCIL [27] 76.40 71.14 66.46 63.29 60.42 57.46 54.78 53.11 51.41 +3.10
Entropy-Reg [47] 71.84 67.12 63.21 59.77 57.01 53.95 51.55 49.52 48.21 -0.10
Subspace Reg. [1] 80.37 71.69 66.94 62.53 58.90 55.00 51.94 49.76 46.79 -1.52
FSLL [52] 66.48 61.75 58.16 54.16 51.10 48.53 46.54 44.20 42.28 -6.03
HardNet (WSN), c= 80% 78.70 72.55 68.26 64.45 61.74 58.93 55.99 54.09 52.74 +4.43
HardNet (WSN), c= 87% 79.17 73.05 69.16 65.43 62.61 59.31 56.73 54.69 53.47 +5.16
HardNet (WSN), c= 90% 79.15 72.03 68.76 65.32 62.00 58.21 56.52 53.66 53.07 +4.76
SoftNet, c= 80% 79.37 74.31 69.89 66.16 63.40 60.75 57.62 55.67 54.34 +6.03
SoftNet, c= 87% 79.77 75.08 70.59 66.93 64.00 61.00 57.81 55.81 54.68 +6.37
SoftNet, c= 90% 79.72 74.25 70.00 66.35 63.19 60.04 57.36 55.38 54.14 +5.83
TABLE 9: ClassiÔ¨Åcation accuracy of ResNet18 on CUB-200-2011 for 10-way 5-shot incremental learning (TOPIC class
split [68]).denotes results reported from [65].yrepresents our reproduced results.
MethodsessionsThe gap
with cRT1 2 3 4 5 6 7 8 9 10 11
cRT [65]77.16 74.41 71.31 68.08 65.57 63.08 62.44 61.29 60.12 59.85 59.30 -
TOPIC [12] 68.68 62.49 54.81 49.99 45.25 41.40 38.35 35.36 32.22 28.31 26.28 -34.80
SPPR [86] 68.68 61.85 57.43 52.68 50.19 46.88 44.65 43.07 40.17 39.63 37.33 -21.97
CEC [83] 75.85 71.94 68.50 63.50 62.43 58.27 57.73 55.81 54.83 53.52 52.28 -7.02
F2M [65] 77.13 73.92 70.27 66.37 64.34 61.69 60.52 59.38 57.15 56.94 55.89 -3.41
LIMIT [84] 75.89 73.55 71.99 68.14 67.42 63.61 62.40 61.35 59.91 58.66 57.41 -1.89
MetaFSCIL [13] 75.90 72.41 68.78 64.78 62.96 59.99 58.30 56.85 54.78 53.82 52.64 -6.66
ALICE [57] 77.40 72.70 70.60 67.20 65.90 63.40 62.90 61.90 60.50 60.60 60.10 -0.02
Entropy-Reg [47] 75.90 72.14 68.64 63.76 62.58 59.11 57.82 55.89 54.92 53.58 52.39 -6.91
FSLL [52] 72.77 69.33 65.51 62.66 61.10 58.65 57.78 57.26 55.59 55.39 54.21 -6.87
HardNet (WSN), c= 88% 76.89 73.40 69.77 66.15 64.00 60.98 59.56 58.05 56.05 55.84 55.20 -4.10
HardNet (WSN), c= 90% 77.23 73.62 70.20 66.36 64.32 61.40 59.86 58.28 56.36 55.88 55.30 -4.00
HardNet (WSN), c= 93% 77.76 73.97 70.41 66.60 64.47 61.35 59.80 58.18 56.17 55.73 55.18 -4.12
SoftNet, c= 88% 78.14 74.61 71.28 67.46 65.14 62.39 60.84 59.17 57.41 57.12 56.64 -2.66
SoftNet, c= 90% 78.07 74.58 71.37 67.54 65.37 62.60 61.07 59.37 57.53 57.21 56.75 -2.55
SoftNet, c= 93% 78.11 74.51 71.14 62.27 65.14 62.27 60.77 59.03 57.13 56.77 56.28 -3.02
