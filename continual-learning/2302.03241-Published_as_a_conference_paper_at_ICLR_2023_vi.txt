Được xuất bản như một bài báo hội nghị tại ICLR 2023

TIỀN HUẤN LUYỆN LIÊN TỤC CỦA CÁC MÔ HÌNH NGÔN NGỮ

Zixuan Ke1, Yijia Shao2, Haowei Lin2, Tatsuya Konishi3y, Gyuhak Kim1, và Bing Liu1z
1Khoa Khoa học Máy tính, Đại học Illinois tại Chicago
2Viện Công nghệ Máy tính Wangxuan, Đại học Bắc Kinh
3KDDI Research
1{zke4,gkim87,liub}@uic.edu
2{shaoyj,linhaowei}@pku.edu.cn
3tt-konishi@kddi-research.jp

TÓM TẮT
Các mô hình ngôn ngữ (LM) đã đóng vai trò quan trọng trong sự tiến bộ nhanh chóng của xử lý ngôn ngữ tự nhiên. Bài báo này nghiên cứu tiền huấn luyện liên tục của các LM, đặc biệt là tiền huấn luyện thích ứng miền liên tục (hay huấn luyện DAP liên tục). Các nghiên cứu hiện có đã chỉ ra rằng việc tiếp tục tiền huấn luyện một LM sử dụng một kho ngữ liệu miền để thích ứng LM với miền đó có thể cải thiện hiệu suất tác vụ cuối trong miền. Bài báo này đề xuất một phương pháp mới để liên tục huấn luyện DAP một LM với một chuỗi các kho ngữ liệu miền không nhãn để thích ứng LM với các miền này nhằm cải thiện hiệu suất tác vụ cuối của chúng. Điểm mới chính của phương pháp chúng tôi là một cơ chế che mềm trực tiếp điều khiển việc cập nhật LM. Một proxy mới cũng được đề xuất để bảo tồn kiến thức tổng quát trong LM ban đầu. Ngoài ra, nó tương phản các biểu diễn của kiến thức miền đã học trước đó (bao gồm kiến thức tổng quát trong LM đã được tiền huấn luyện) và kiến thức từ mạng đầy đủ hiện tại để đạt được tích hợp kiến thức. Phương pháp này không chỉ khắc phục quên thảm khốc mà còn đạt được chuyển giao kiến thức để cải thiện hiệu suất tác vụ cuối. Đánh giá thực nghiệm chứng minh tính hiệu quả của phương pháp được đề xuất.1

1 GIỚI THIỆU
Các mô hình ngôn ngữ đã được tiền huấn luyện (LM) như BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019) đã thúc đẩy đáng kể NLP. Gần đây, các LM cũng đã được sử dụng bởi nhiều hệ thống học liên tục (CL) để học một chuỗi các tác vụ cuối một cách tăng dần (Ke et al., 2021a; Sun et al., 2020; Huang et al., 2021), mà chúng tôi gọi là học tác vụ cuối liên tục. Việc tiền huấn luyện liên tục các LM bản thân chúng cũng là điều mong muốn. Điều này bao gồm (1) tiền huấn luyện tổng quát liên tục, cập nhật tăng dần LM sử dụng dữ liệu gần đây nhất có phân phối tương tự như dữ liệu tiền huấn luyện, và (2) tiền huấn luyện thích ứng miền liên tục, tiếp tục tiền huấn luyện một LM một cách tăng dần để thích ứng nó với một chuỗi các miền. Lưu ý rằng chỉnh sửa LM (có hoặc không có học liên tục) (Mitchell et al., 2022) để sửa các lỗi đã học trong LM là một trường hợp đặc biệt của học tác vụ cuối liên tục (Kim et al., 2022) vì mỗi tác vụ chỉnh sửa hoặc một nhóm các tác vụ chỉnh sửa được học cùng nhau về cơ bản là một tác vụ trong học liên tục, nhằm thực hiện các chỉnh sửa một cách chính xác mà không can thiệp hoặc quên kiến thức khác đã được học trong LM hiện tại.

Bài báo này tập trung vào tiền huấn luyện thích ứng miền liên tục (hay huấn luyện DAP liên tục) của các LM. Đã biết rằng huấn luyện DAP2 một LM (không có học liên tục) sử dụng một kho ngữ liệu miền lớn không nhãn trước khi tinh chỉnh tác vụ cuối đạt được kết quả tốt hơn (Gururangan et al., 2020; Xu et al., 2019; Ke et al., 2022b). Bài báo này tiến thêm một bước để liên tục học cải thiện khả năng của LM trong việc xử lý các miền hoặc chủ đề mới hoặc đang nổi lên mà không quên các kỹ năng hoặc kiến thức đã học trong quá khứ. Điều này quan trọng trong thế giới thực, nơi dữ liệu liên tục thay đổi và các miền, sự kiện hoặc chủ đề mới liên tục xuất hiện (Ke et al., 2022b) và LM cần được cập nhật để phục vụ người dùng tốt hơn.

Chúng tôi gọi vấn đề này là huấn luyện DAP liên tục. Bắt đầu từ một LM tổng quát đã được tiền huấn luyện (tức là LM đã được tiền huấn luyện trên D0), chúng tôi huấn luyện DAP tăng dần một chuỗi các kho ngữ liệu miền D1;D2;:::. Một khi một miền được huấn luyện, dữ liệu của nó không còn có thể truy cập được. Điều này khác với học liên tục thông thường (CL) nơi mỗi tác vụ là một tác vụ cuối. Trong huấn luyện DAP liên tục được đề xuất, mỗi tác vụ là một kho ngữ liệu miền không nhãn cần được học. Một tác vụ cuối tinh chỉnh LM được huấn luyện DAP liên tục để đánh giá hiệu suất của nó. Đáng chú ý là D0 thường là một miền rộng hoặc tổng quát (ví dụ: Tin tức). Trong thực tế, một LM được huấn luyện DAP liên tục có thể được huấn luyện bởi người dùng cá nhân, tổ chức hoặc kết hợp cả hai có một hoặc nhiều kho ngữ liệu lớn của một số miền cụ thể. Trong những trường hợp như vậy, dữ liệu thô có thể không được chia sẻ, nhưng LM cuối cùng có thể được chia sẻ bởi tất cả.

Có nhiều yêu cầu mong muốn cho một hệ thống huấn luyện DAP liên tục: (1) Nó không nên bị quên thảm khốc (CF), tức là nó nên hoạt động hợp lý trên các miền đã học. Điều này yêu cầu hệ thống (a) khắc phục CF cho mỗi miền mới và (b) khắc phục CF cho kiến thức ngôn ngữ tổng quát trong LM. Điều này quan trọng vì kiến thức học được từ mỗi miền riêng lẻ sẽ không đủ cho hiệu suất tác vụ cuối tốt. (2) Nó nên khuyến khích chuyển giao kiến thức (KT) giữa các miền để đạt được hiệu suất tác vụ cuối được cải thiện. Điều này yêu cầu hệ thống cho phép (a) chuyển giao tiến, học một miền mới bằng cách tận dụng kiến thức từ các miền trước đó, và (b) chuyển giao ngược, đạt được hiệu suất cải thiện trên các miền trước đó sau khi học một miền mới có liên quan. (3) Nó nên hoạt động mà không yêu cầu domain-ID cho mỗi tinh chỉnh tác vụ cuối.

Không có phương pháp CL hiện tại nào có thể đạt được tất cả những điều trên. Bài báo này đại diện cho một bước hướng tới việc đạt được chúng. Phương pháp được đề xuất được gọi là DAS (Tiền huấn luyện DA liên tục của các LM với Che mềm). DAS đề xuất một cơ chế che mềm mới tính toán tầm quan trọng (một số thực giữa 0 và 1) của các đơn vị3 cho kiến thức tổng quát hoặc miền và che mềm chúng dựa trên các giá trị tầm quan trọng của chúng để điều khiển luồng gradient ngược. Trong lượt truyền tiến, che mềm không được áp dụng, điều này khuyến khích KT giữa các miền. Nó không cô lập bất kỳ mạng con nào cho bất kỳ miền nào để kiến thức trong LM đầy đủ có thể được tận dụng cho tinh chỉnh tác vụ cuối.

Để áp dụng cơ chế này, DAS thực hiện hai chức năng: (1) Khởi tạo, tính toán tầm quan trọng của các đơn vị đối với kiến thức tổng quát trong LM mà không truy cập dữ liệu tiền huấn luyện LM (D0). Nó được áp dụng trên LM đã được tiền huấn luyện trước khi học liên tục bắt đầu, và (2) học liên tục, huấn luyện DAP mỗi miền trong khi ngăn chặn CF trên kiến thức tổng quát và miền và khuyến khích KT liên miền. Trong (1), không rõ ràng làm thế nào để tính toán tầm quan trọng mà không có dữ liệu tiền huấn luyện. DAS đề xuất một proxy mới dựa trên độ mạnh mẽ để tính toán tầm quan trọng của các đơn vị cho kiến thức tổng quát. Trong (2), che mềm có thể áp dụng trực tiếp vì chúng ta có dữ liệu miền và tầm quan trọng có thể được tính toán dựa trên gradient của nó được lấy cảm hứng từ cộng đồng cắt tỉa (Li et al., 2021; Michel et al., 2019). Hơn nữa, DAS tương phản kiến thức đã học trước đó và kiến thức đầy đủ (bao gồm cả các miền đã học và miền hiện tại) để khuyến khích biểu diễn miền hiện tại học kiến thức không có sẵn trong kiến thức đã học từ các miền trước đó và tích hợp nó với kiến thức đã học4. Trong tinh chỉnh tác vụ cuối, DAS không yêu cầu domain-ID vì tất cả kiến thức được tích lũy vào LM được huấn luyện DAP.

Tóm lại, công trình này đóng góp những điều sau. (i) Nó nghiên cứu vấn đề mới của huấn luyện DAP liên tục và phát hiện rằng LM đầy đủ là cần thiết cho một phương pháp huấn luyện DAP liên tục tốt. Cách tiếp cận cô lập tham số phổ biến để khắc phục CF trong CL thông thường không phù hợp. (ii) Nó đề xuất một phương pháp che mềm mới để khắc phục CF và khuyến khích KT, và một phương pháp dựa trên học tương phản để tích hợp kiến thức. (iii) Để bảo tồn kiến thức tổng quát trong LM, một proxy mới cũng được đề xuất. (iv) Kết quả thực nghiệm chứng minh tính hiệu quả của DAS.

2 CÔNG TRÌNH LIÊN QUAN

Huấn luyện DAP. Huấn luyện DAP có thể đạt được bằng cách cập nhật trực tiếp LM (Xu et al., 2019; Sun et al., 2019; Lee et al., 2020; Alsentzer et al., 2019; Gururangan et al., 2020; Chakrabarty et al., 2019; Ke et al., 2022b) hoặc bằng cách chỉ huấn luyện một tập nhỏ các tham số bổ sung. Ví dụ, Pfeiffer et al. (2020); Wang et al. (2020a); Ke et al. (2021a;b;c) đã huấn luyện các adapter và Gu et al. (2021) đã huấn luyện một prompt để thích ứng với một miền. Mặc dù adapter và prompt có thể hiệu quả, việc chuyển giao kiến thức giữa các mô-đun bổ sung này thường khó khăn và có thể không chính xác. DAS thuộc họ trước đây cập nhật trực tiếp LM. Điều này rất thách thức đối với CL do CF. Theo hiểu biết của chúng tôi, không có hệ thống hiện tại nào trong họ này về CL.

Học liên tục. Hầu hết các phương pháp CL được đề xuất để khắc phục CF: (1) Các phương pháp chính quy hóa (Kirkpatrick et al., 2016; Seff et al., 2017) tính toán tầm quan trọng của mỗi tham số đối với các tác vụ trước đó và sử dụng một regularizer để phạt tổng các thay đổi. DAS có liên quan nhưng cũng rất khác với EWC (Kirkpatrick et al., 2016). (1) DAS không điều khiển từng tham số/trọng số, mà chỉ điều khiển các đầu attention hoặc neuron dựa trên điểm tầm quan trọng của chúng. Điều này cho ít quên hơn (xem tỷ lệ quên trong Bảng 2) vì ngay cả một thay đổi nhỏ đối với mỗi tham số cho một neuron có thể tạo ra một thay đổi tổng lớn đối với kích hoạt của neuron. (2) DAS điều khiển trực tiếp luồng gradient ngược trên mỗi neuron, điều này tinh tế và hiệu quả hơn so với tổng các thay đổi của tất cả các tham số. Kết quả thực nghiệm của chúng tôi xác nhận rằng EWC kém hơn đáng kể so với DAS (xem Bảng 2). (2) Các phương pháp phát lại giữ lại (Rebuffi et al., 2017; Wang et al., 2020b) hoặc tạo ra một số dữ liệu của các tác vụ cũ (Shin et al., 2017; He & Jaeger, 2018) và sử dụng chúng trong việc học một tác vụ mới; (3) các phương pháp cô lập tham số (Serrà et al., 2018; Wortsman et al., 2020) phân bổ neuron và tham số hoặc mạng con cho các tác vụ/miền khác nhau và che chúng trong việc học tác vụ. Đối với huấn luyện DAP liên tục, điều này có nghĩa là các tác vụ cuối không thể sử dụng kiến thức tổng quát trong LM, dẫn đến hiệu suất tác vụ cuối kém.

Trong NLP, CL đã được sử dụng cho slot filling (Shen et al., 2019), học ngôn ngữ (Li et al., 2019), phân tích tình cảm (Ke et al., 2021a), mô hình hóa chủ đề (Gupta et al., 2020), trả lời câu hỏi (Greco et al., 2019) và phân loại văn bản (Sun et al., 2020; Huang et al., 2021; Chuang et al., 2020). Nhưng không có cái nào dành cho huấn luyện DAP. Một số bài báo CL gần đây liên quan đến LM. Hệ thống trong (Madotto et al., 2020) học các adapter riêng biệt cho các miền khác nhau và do đó không có CF hoặc KT. DEMIX (Gururangan et al., 2021) khởi tạo adapter mới với adapter cũ gần nhất. CPT (Ke et al., 2022a) và ELLE (Qin et al., 2022) liên quan chặt chẽ nhất với DAS. Tuy nhiên, CPT sử dụng cách tiếp cận cô lập tham số để học và bảo vệ mỗi tác vụ, điều này yếu (xem Mục 4.2). Nó cũng cần domain-ID trong tinh chỉnh tác vụ cuối. ELLE phải bắt đầu từ tiền huấn luyện LM chính nó thay vì từ một LM đã được tiền huấn luyện như DAS. Nó cũng sử dụng một bộ nhớ lớn (1G mỗi miền) để lưu trữ dữ liệu phát lại (bao gồm dữ liệu tiền huấn luyện) và mở rộng mạng cho mỗi miền. Cả hai đều không được yêu cầu trong DAS. Jin et al. (2021) đã đánh giá một số kỹ thuật CL hiện có trong một thiết lập tương tự như DAS và thực hiện phân tích về việc xử lý CF. Tuy nhiên, không có kỹ thuật mới nào được đề xuất trong bài báo.

Cắt tỉa mạng nơ-ron. Nhiều tham số trong một mạng là thừa và có thể được cắt tỉa (Li et al., 2021; Lai et al., 2021; Michel et al., 2019; Voita et al., 2019). Các phương pháp hiện có bao gồm loại bỏ các tham số có giá trị tuyệt đối nhỏ (Han et al., 2015; Guo et al., 2016), gradient tích lũy (Michel et al., 2019), và giả thuyết vé số độc đắc (Brix et al., 2020). Tuy nhiên, các phương pháp này không áp dụng trực tiếp vì chúng ta cần bảo tồn không chỉ kiến thức miền cá nhân mà còn kiến thức tổng quát trong LM. Đối với kiến thức tổng quát, vì chúng ta không có bất kỳ dữ liệu tiền huấn luyện nào, một proxy được đề xuất dựa trên độ mạnh mẽ. Đối với kiến thức miền, chúng tôi áp dụng một phương pháp cắt tỉa nhưng sử dụng tầm quan trọng làm mặt nạ mềm vì chúng tôi muốn tích lũy kiến thức thay vì nén LM.

Học tương phản. Học tương phản (Chen et al., 2020; He et al., 2020) học các biểu diễn tốt bằng cách tối đa hóa sự tương tự của các cặp dương và tối thiểu hóa của các cặp âm,
Lcontrast = 1/N ∑(n=1 to N) log[e^(sim(qn,q+n)/τ) / ∑(j=1 to N) e^(sim(qn,q+j)/τ)], (1)
trong đó N là kích thước batch, τ là một tham số nhiệt độ, sim() là một metric tương tự, và qn và q+n là các biểu diễn cho các cặp dương xn và x+n. DAS tương phản kiến thức đã học từ các miền trước đó và LM đã được tiền huấn luyện (kiến thức tổng quát) với kiến thức đầy đủ (bao gồm cả kiến thức miền trước đó và miền hiện tại) để đạt được hiệu ứng bổ sung.

3 KỸ THUẬT DAS ĐỀ XUẤT

Huấn luyện DAP liên tục trong DAS dựa trên hai ý tưởng chính: (1) bảo tồn kiến thức ngôn ngữ tổng quát quan trọng trong LM và kiến thức đã học từ các miền trước đó để khắc phục CF bằng cách che mềm các đơn vị dựa trên tầm quan trọng của chúng, điều này cũng tạo điều kiện cho chuyển giao kiến thức liên tác vụ (KT), và (2) khuyến khích mô hình học các biểu diễn bổ sung của miền hiện tại và các miền trước đó để đạt được tích hợp kiến thức. Hình 1 đưa ra tổng quan về DAS.

Toàn bộ quá trình học bao gồm hai chức năng chính: (i) khởi tạo và (ii) học liên tục. (i) tính toán tầm quan trọng của các đơn vị đối với kiến thức ngôn ngữ tổng quát trong LM. Nó được thực hiện trước khi học liên tục bắt đầu. (ii) dành cho học liên tục, bao gồm hai bước: (a) huấn luyện miền và (b) tính toán tầm quan trọng. (a) lấy các điểm tầm quan trọng được tích lũy cho đến nay (bao gồm những điểm đối với kiến thức tổng quát trong LM ban đầu và đối với kiến thức đã học từ các miền trước đó) và dữ liệu đầu vào của miền hiện tại để học miền và đạt được (1) và (2) ở trên, trong khi (b) tính toán các điểm tầm quan trọng cho miền hiện tại để sử dụng trong tương lai. Các tiểu mục sau trình bày chi tiết từng chức năng và bước.

3.1 KHỞI TẠO: TÍNH TOÁN TẦM QUAN TRỌNG CỦA CÁC ĐƠN VỊ ĐỐI VỚI KIẾN THỨC TỔNG QUÁT

Chức năng khởi tạo này tính toán tầm quan trọng của các đơn vị (các đầu attention và neuron) trong Transformer đối với kiến thức tổng quát trong LM ban đầu. Các thành phần chính của một Transformer là lớp attention đa đầu, lớp trung gian và lớp đầu ra. Dưới đây, chúng tôi sử dụng "lớp" hoặc l để chỉ bất kỳ cái nào trong ba lớp này vì phương pháp của chúng tôi xử lý ba lớp tương tự nhau.

Tầm quan trọng của các đơn vị trong một lớp. Đã được phát hiện rằng không phải tất cả các đơn vị trong một lớp đều quan trọng (Michel et al., 2019). Chúng tôi giới thiệu một tham số ảo gl để tính toán tầm quan trọng của các đơn vị trong lớp l. Chúng tôi gọi những tham số này là ảo vì mỗi g(k) được khởi tạo thành 1. Chúng tôi chỉ cần gradient trên mỗi tham số để tính toán tầm quan trọng của đơn vị tương ứng, không cập nhật bất kỳ tham số nào.

ôl = gl ⊙ ol, (2)

trong đó ol đề cập đến đầu ra của lớp l (có thể là bất kỳ cái nào trong ba lớp được đề cập ở trên). ⊙ đề cập đến phép nhân từng phần tử, tức là mỗi biến gl,i trong gl tương ứng với một đơn vị (một neuron hoặc đầu attention) trong lớp. Chúng tôi thích ứng phương pháp phát hiện tầm quan trọng dựa trên gradient trong (Michel et al., 2019) cho mục đích của chúng tôi. Cho một tập dữ liệu D = {(xn,yn)}N_n=1 của N mẫu (yn là nhãn lớp của xn như (Michel et al., 2019) đã làm việc trên học có giám sát), tầm quan trọng của neuron hoặc đầu trong lớp được ước tính với một điểm proxy dựa trên gradient

Il = 1/N ∑(n=1 to N) |∂Limpt(xn,yn)/∂gl|, (3)

trong đó Limpt là một hàm mất mát cụ thể cho tác vụ. Lưu ý tham số ảo gl được khởi tạo là tất cả 1, và không được thay đổi. Điều này là vì chúng ta chỉ cần gradient trung bình ∇gl (thuật ngữ trong | | trong Eq. 3) của nó trên tất cả dữ liệu để tính toán tầm quan trọng và sẽ không sử dụng gradient để cập nhật tham số ảo. Trong huấn luyện (Mục 3.2 và Hình 1 (B)), tham số ảo có thể được loại bỏ. Il kết quả có cùng kích thước với gl, mỗi mục tương ứng với tầm quan trọng của một đơn vị (một neuron hoặc đầu attention).

Nhớ lại rằng chức năng khởi tạo là để học tầm quan trọng của các đơn vị đối với kiến thức tổng quát trong LM (ký hiệu là I(0)_l). Mặc dù Eq. 3 cung cấp một cách có thể, nó không áp dụng trực tiếp. Nếu chúng ta sử dụng dữ liệu miền hiện có và sử dụng mất mát MLM làm Limpt, ∇gl chỉ cho tầm quan trọng đối với kiến thức cụ thể miền. Tuy nhiên, để tính toán tầm quan trọng của các đơn vị đối với kiến thức tổng quát trong LM (đó là mục tiêu của chúng ta), chúng ta cần dữ liệu gốc được sử dụng trong tiền huấn luyện LM để tính toán Limpt. Trong thực tế, dữ liệu như vậy không thể truy cập được đối với người dùng LM. Hơn nữa, nhãn cần thiết trong Eq. 3 nhưng kho ngữ liệu miền của chúng ta không có nhãn trong huấn luyện DAP. Để giải quyết những vấn đề này, chúng tôi đề xuất một mất mát KL-divergence proxy (Lproxy) để thay thế Limpt để học tầm quan trọng đơn vị cho kiến thức tổng quát.

Mất mát KL-divergence proxy. Chúng tôi đề xuất sử dụng độ mạnh mẽ của mô hình làm proxy, tức là chúng tôi cố gắng phát hiện các đơn vị quan trọng cho độ mạnh mẽ của LM. Gradient của chúng, ∇gl, sau đó chỉ ra độ mạnh mẽ và tầm quan trọng đối với mô hình LM. Lý luận của chúng tôi như sau: Nếu I(0)_l,i (tầm quan trọng của đơn vị i trong lớp l) có giá trị cao, thì nó quan trọng đối với độ mạnh mẽ của LM vì sự thay đổi của nó có thể khiến LM thay đổi nhiều. Do đó nó là một đơn vị quan trọng. Ngược lại, nếu I(0)_l,i nhỏ, nó là một đơn vị ít quan trọng hơn.

Để tính toán độ mạnh mẽ của LM, chúng tôi lấy một tập con của dữ liệu miền hiện tại {x^sub_n}5 (không có nhãn trong huấn luyện DAP) và nhập x^sub_n hai lần vào LM để có được hai biểu diễn của nó và sau đó tính toán KL-divergence giữa chúng,

Limpt = KL(f1_LM(x^sub_n), f2_LM(x^sub_n)); (4)

trong đó f1_LM và f2_LM là LM với các mặt nạ dropout khác nhau. Chúng tôi không cần thêm bất kỳ dropout bổ sung nào để thực hiện hai cái này vì Transformer đã có các mặt nạ dropout được đặt trên các lớp fully-connected và xác suất attention. Do đó, đơn giản là đưa cùng một đầu vào vào Transformer hai lần sẽ có được hai biểu diễn với các mặt nạ dropout khác nhau. Vì dropout tương tự như việc thêm nhiễu, sự khác biệt giữa hai biểu diễn có thể được coi là độ mạnh mẽ của LM.

3.2 HUẤN LUYỆN: HỌC MỘT MIỀN MỚI THÔNG QUA CHE MỀM VÀ MẤT MÁT TƯƠNG PHẢN

Nhớ lại chúng ta muốn bảo tồn kiến thức đã học trong LM trong quá trình huấn luyện DAP bằng cách sử dụng tầm quan trọng tích lũy I(≤t-1)_l khi chúng ta học miền t, bao gồm cả tầm quan trọng đối với kiến thức tổng quát I(0)_l (Mục 3.1) và kiến thức cụ thể miền đã học I(k)_l của mỗi miền k (k có thể là bất kỳ miền nào trong {1···t-1}) đã được học (Mục 3.3). Điều này đạt được bằng cách che mềm việc học dựa trên tầm quan trọng tích lũy như sau.6

Tích lũy tầm quan trọng. Chúng tôi tích lũy tầm quan trọng sau khi tác vụ t-1 được học được thực hiện thông qua max từng phần tử (EMax) như sau:

I(≤t-1)_l = EMax({I(≤t-1)_l, I(≤t-2)_l}); (5)

trong đó t đề cập đến task-ID hiện tại và I(≤t-2)_l đề cập đến tầm quan trọng được tích lũy trước đó tại tác vụ t-2. Chúng tôi không cần lưu I0_l và tất cả {I(k)_l}^(t-1)_(k=1) cho Eq. 5. Chúng tôi chỉ lưu tầm quan trọng được tích lũy tăng dần sau khi huấn luyện mỗi tác vụ.

Che mềm các đơn vị. Cho tầm quan trọng tích lũy I(≤t-1)_l của lớp l và mất mát huấn luyện DAP LDAP-train (thường là mất mát MLM; chúng tôi cũng đề xuất một mất mát bổ sung trong Eq. 7), chúng tôi ràng buộc (hoặc che mềm) luồng gradient tương ứng (∇l) như sau,

∇̂l = (1 - I(≤t-1)_l) ⊙ ∇l; (6)

Như đã đề cập trong Mục 3.1, chúng tôi mở rộng (bằng cách sao chép) tầm quan trọng I(≤t-1)_l để khớp với các chiều của ∇l để áp dụng nó cho tất cả các tham số liên quan. Đây là che mềm vì mỗi phần tử trong I(≤t-1)_l là một số thực trong [0,1] (không phải nhị phân {0, 1}), điều này cho mô hình tính linh hoạt để điều chỉnh bất kỳ đơn vị nào.

Chúng tôi lưu ý rằng các mặt nạ mềm trên chỉ được áp dụng trong lượt truyền ngược, nhưng không trong lượt truyền tiến, điều này khuyến khích chuyển giao kiến thức vì mỗi huấn luyện miền có thể tận dụng kiến thức đã học từ tất cả các miền trước đó. Để tiếp tục khuyến khích mô hình học một biểu diễn tốt từ cả kiến thức tích lũy (I(≤t-1)_l) và kiến thức đầy đủ (cả kiến thức tích lũy và miền hiện tại), chúng tôi giới thiệu một phương pháp học tương phản để khuyến khích biểu diễn bổ sung.

Tích hợp kiến thức đã học trước đó và kiến thức miền hiện tại. Che mềm giúp ngăn chặn việc quên kiến thức đã học trước đó. Chúng tôi muốn tiếp tục khuyến khích chuyển giao kiến thức bằng cách tích hợp kiến thức mới và đã học. Chúng tôi đề xuất tương phản kiến thức đã học trước đó và kiến thức đầy đủ (cả kiến thức đã học trước đó và kiến thức miền hiện tại). Lưu ý rằng việc tương phản không thể làm gì với kiến thức quá khứ được chia sẻ vì nó được bảo vệ bởi mặt nạ mềm. Do đó, nó hiệu quả đẩy kiến thức miền hiện tại xa để bổ sung cho kiến thức quá khứ. Điều này được thực hiện dựa trên dữ liệu miền hiện tại như sau.

Tương phản kiến thức đã học và đầy đủ. Chúng tôi ký hiệu đầu ra của LM mà không xem xét bất kỳ tầm quan trọng nào là o^full, đề cập đến kiến thức đầy đủ. Chúng tôi tiếp tục ký hiệu đầu ra của LM được nhân với tầm quan trọng (tức là I(≤t-1)_l ⊙ ol) là o^prev, đề cập đến kiến thức đã học trước đó. Chúng tôi tương phản hai cái bằng cách sử dụng o^full làm mỏ neo và o^full với dropout khác nhau làm mẫu dương (ký hiệu là o^full+). o^prev được sử dụng làm các thể hiện âm.

Chính thức, với o^full_n, o^full+_n, và o^prev_n, mất mát tương phản của chúng tôi là (sim() là độ tương tự cosine),

Lcontrast = 1/N ∑(n=1 to N) log[e^(sim(o^full_n,o^full+_n)/τ) / ∑(j=1 to N)(e^(sim(o^full_n,o^full+_j)/τ) + e^(sim(o^full_n,o^prev_j)/τ))]: (7)

So với Eq. 1, thuật ngữ thứ hai được thêm vào mẫu số, tức là các biểu diễn trong kiến thức đã học trước đó làm các thể hiện âm bổ sung. Hình 1 (B) hiển thị một mũi tên đỏ trỏ từ o^full đến chính nó, chỉ ra các thể hiện dương là từ việc nhập hai lần. Mũi tên đỏ đứt nét trỏ đến o^prev chỉ ra các thể hiện âm tương phản kiến thức đầy đủ và đã học trước đó.

Hàm mất mát cuối cùng. Mất mát huấn luyện DAP cuối cùng kết hợp mất mát Masked Language Model (MLM) sau khi áp dụng che mềm được đề xuất cho kiến thức tổng quát (Mục 3.1) và mất mát tương phản được đề xuất (λ là một siêu tham số),

LDAP-train = LMLM + λLcontrast (8)

3.3 TÍNH TOÁN TẦM QUAN TRỌNG CỦA CÁC ĐƠN VỊ ĐỐI VỚI MIỀN HIỆN TẠI

Sau khi huấn luyện miền mới/hiện tại t, chúng tôi học tầm quan trọng đơn vị bằng cách áp dụng Eq. 3 cho miền. Chúng tôi không cần bất kỳ proxy nào để tính toán Limpt như trong Eq. 4 vì chúng tôi có thể sử dụng trực tiếp dữ liệu miền hiện tại. Cụ thể, chúng tôi lấy mẫu ngẫu nhiên một tập con (một siêu tham số) của dữ liệu miền hiện tại {(x^sub_n, y^sub_n)}, trong đó x^sub_n là đầu vào và y^sub_n là token bị che như trong mất mát tự giám sát MLM. Sau đó chúng tôi có thể dễ dàng tính toán tầm quan trọng I(t)_l bằng cách thế LMLM vào Limpt trong Eq. 3. I(t)_l kết quả sẽ được sử dụng trong tác vụ tiếp theo bằng cách tích lũy với tầm quan trọng được tích lũy trước đó (Eq. 5) và che mềm việc học (Eq. 6).

4 CÁC THỰC NGHIỆM

Chúng tôi sử dụng RoBERTa (Liu et al., 2019)7 làm LM. Theo thiết lập đánh giá tiêu chuẩn (Lange et al., 2019) và, sau khi một miền được huấn luyện, dữ liệu huấn luyện của nó bị loại bỏ. Sau khi tất cả các miền được học tăng dần, mô hình cuối cùng được đánh giá bằng cách tinh chỉnh các tác vụ cuối trong tất cả các miền.

4.1 TẬP DỮ LIỆU VÀ BASELINE

Tập dữ liệu: Bảng 1 hiển thị thống kê của 6 kho ngữ liệu miền không nhãn cho huấn luyện DAP và 6 tập dữ liệu phân loại tác vụ cuối tương ứng của chúng.8 3 trong số chúng về đánh giá: Yelp Restaurant (Xu et al., 2019), Amazon Phone (Ni et al., 2019), Amazon Camera (Ni et al., 2019); 3 trong số chúng là các bài báo học thuật: ACL Papers (Lo et al., 2020), AI Papers (Lo et al., 2020), và PubMed Papers9. Các tập dữ liệu phân loại tác vụ cuối tương ứng của chúng là:10 Restaurant11, Phone (Ding et al., 2008; Hu & Liu, 2004), Camera (Ding et al., 2008; Hu & Liu, 2004), ACL (ACL-ARC trong (Jurgens et al., 2018)), AI (SCIERC trong (Luan et al., 2018)), và PubMed (CHEMPORT trong (Kringelum et al., 2016)).

Baseline. Chúng tôi sử dụng 16 baseline, bao gồm không học liên tục (Non-CL) và học liên tục (CL) baseline. Tất cả CL baseline ban đầu dành cho học dữ liệu có giám sát ngoại trừ DEMIX. Chúng tôi thích ứng chúng và thay thế backbone của chúng bằng RoBERTa. Chi tiết của mỗi baseline được đưa ra trong Phụ lục B.

Baseline Non-CL: Mỗi baseline ở đây xây dựng một mô hình riêng biệt cho mỗi tác vụ. (1) Pool. Chúng tôi gộp dữ liệu của tất cả các miền lại với nhau và chỉ huấn luyện một mô hình cho tất cả các miền. (2) RoBERTa (Liu et al., 2019) sử dụng RoBERTa cho tinh chỉnh tác vụ cuối mà không có huấn luyện DAP. (3) DAP-RoBERTa sử dụng phương pháp huấn luyện DAP hiện có (MLM) trong (Gururangan et al., 2020) để post-train mỗi miền riêng biệt. (4) DAP-Adapter thêm các lớp adapter trong Transformer cho mỗi miền cho huấn luyện DAP (Jang et al., 2021; Madotto et al., 2020; Houlsby et al., 2019). Chỉ các adapter được thêm vào mới có thể huấn luyện được. Trong tinh chỉnh tác vụ cuối, cả RoBERTa và các adapter đều có thể huấn luyện được. (5) DAP-Prompt từ (Lester et al., 2021). Trong huấn luyện DAP, RoBERTa (LM) được cố định và chỉ các prompt được huấn luyện. Trong tinh chỉnh tác vụ cuối, cả LM và prompt được huấn luyện đều có thể huấn luyện được.

Baseline CL: Chúng tôi sử dụng 2 baseline naïve, tiếp tục học thêm các miền mà không có cơ chế để xử lý CF hoặc chuyển giao. (6) NCL (Naive CL) liên tục huấn luyện DAP RoBERTa; và (7) NCL-Adapter liên tục huấn luyện DAP một tập các adapter (Houlsby et al., 2019).

8 baseline là các hệ thống CL: (8) DEMIX (Gururangan et al., 2021) thêm một adapter mới cho mỗi miền mới và khởi tạo nó với một adapter trước đó gần nhất với miền mới; (9) BCL (Ke et al., 2021c) sử dụng mạng capsule. (10) CLASSIC (Ke et al., 2021b) sử dụng học tương phản. (11) KD là chưng cất kiến thức (Hinton et al., 2015). (12) EWC (Buzzega et al., 2020) là một phương pháp dựa trên chính quy hóa phổ biến. (13) DER++ (Buzzega et al., 2020) là một phương pháp phát lại dựa trên chưng cất kiến thức. 16.4K token được lưu cho mỗi miền trong bộ nhớ phát lại, đây là bộ nhớ lớn nhất chúng tôi có thể sử dụng để hệ thống chạy được. (14) HAT (Serrà et al., 2018) là một phương pháp cô lập tham số hiệu quả. HAT được áp dụng cho các lớp Transformer (tức là self-attention, trung gian và đầu ra). (15) HAT-All là một biến thể HAT sử dụng tất cả các tính năng từ LM để thực hiện các tác vụ cuối (thay vì chỉ các tính năng từ mạng con miền của nó như trong HAT). (16) HAT-Adapter (Ke et al., 2021c) sử dụng HAT trong các adapter. ELLE (Qin et al., 2022) không được bao gồm vì chúng tôi đã thích ứng nó cho mục đích của chúng tôi bằng cách học từ RoBERTa, nhưng nó không thể hội tụ.

4.2 PHÂN TÍCH KẾT QUẢ VÀ NGHIÊN CỨU ABLATION

Do giới hạn không gian, Chi tiết thực hiện được đưa ra trong Phụ lục C. Bảng 2 báo cáo kết quả tinh chỉnh tác vụ cuối của tất cả 15 hệ thống trên 6 tập dữ liệu. Chúng ta có thể thấy rằng DAS được đề xuất vượt trội hơn tất cả các baseline về trung bình và cũng đạt được chuyển giao kiến thức tốt nhất (tỷ lệ quên âm).

(1) DAS hơi tốt hơn Pool về trung bình. Điều này có thể là do (a) một số miền khá khác nhau (ví dụ: đánh giá camera và bài báo ACL), dẫn đến một số chuyển giao âm trong Pool. (b) DAS có thể học với kiến thức tổng quát và miền trước đó được bảo vệ bởi mặt nạ mềm.

(2). DAS đạt được cả ngăn chặn quên và chuyển giao kiến thức. Những baseline (KD, EWC, DER++) chỉ tập trung vào ngăn chặn quên cho hiệu suất kém hơn vì chúng hy sinh độ chính xác để tránh CF. Những baseline (BCL, CLASSIC và DEMIX) thực hiện chuyển giao kiến thức đạt được kết quả tốt hơn nhưng vẫn kém hơn DAS. DEMIX có chuyển giao rất yếu. BCL, có thể tránh CF trong khi đạt được một số chuyển giao, yếu hơn NCL. Nói chung, các baseline CL đều kém hơn DAS vì chúng không có phương pháp để khuyến khích chuyển giao kiến thức hoặc chúng phải dựa vào adapter.

(3). Học trực tiếp các miền trong LM giúp DAS đạt được kết quả tốt hơn so với các phương pháp dựa trên adapter và prompt. DAS tốt hơn các hệ thống dựa trên adapter (DAP-Adapter, NCL-Adapter và HAT-Adapter) và hệ thống dựa trên prompt (DAP-Prompt). Điều này là do adapter và prompt không có đủ tham số có thể huấn luyện, cũng được khởi tạo ngẫu nhiên và có thể khó huấn luyện.

(4). Sử dụng LM đầy đủ để học tất cả các tác vụ thay vì sử dụng mạng con (của các phương pháp dựa trên HAT) làm cho DAS hiệu quả hơn. HAT hoạt động kém, cho thấy nó không phù hợp cho huấn luyện DAP như đã thảo luận trong Mục 1. Ngay cả khi chúng ta sử dụng tất cả các tính năng (không chỉ tính năng từ mạng con tương ứng), chúng ta vẫn có kết quả kém (HAT-All) vì các tính năng được sử dụng trong huấn luyện DAP (trong một mạng con LM) khác với các tính năng được sử dụng trong tinh chỉnh tác vụ cuối (tính năng từ toàn bộ LM).

Chuyển giao kiến thức và tránh quên. Để xem các mô hình hoạt động như thế nào về CF và chuyển giao kiến thức, chúng tôi so sánh tỷ lệ quên (forget R.) (Liu et al., 2020), 1/(t-1) ∑^(t-1)_(k=1) Ak,k - At,k, trong đó Ak,k là độ chính xác tác vụ cuối ngay sau khi miền k của nó được huấn luyện DAP, và At,k là độ chính xác của tác vụ cuối của miền k sau khi huấn luyện DAP miền cuối cùng t. Chúng tôi tính trung bình trên tất cả các tác vụ cuối ngoại trừ tác vụ cuối cùng vì miền cuối cùng không có quên. Tỷ lệ quên càng cao, càng có nhiều quên. Tỷ lệ âm cho thấy chuyển giao kiến thức dương. Rõ ràng, DAS có tỷ lệ quên âm mạnh nhất, cho thấy nó hoạt động tốt cả về ngăn chặn quên và chuyển giao kiến thức.

NCL, NCL-Adapter, DEMIX, EWC, KD và DER++ đều bị một số quên. HAT không có quên nhưng nó không thể học tốt. HAT và BCL không có quên nhưng yếu trong chuyển giao.

Hiệu quả của mất mát KL-divergence proxy. Chúng tôi sử dụng mất mát KL-divergence proxy trong chức năng khởi tạo (Mục 3.1) để tính toán tầm quan trọng của các đơn vị cho kiến thức tổng quát. Chúng tôi quan tâm đến proxy này tốt như thế nào. Chúng tôi sử dụng hai loại thực nghiệm để cung cấp bằng chứng.

(1) So sánh với một tập mẫu của D0. Trong một số trường hợp, người dùng huấn luyện DAP liên tục có thể có dữ liệu D0 đã được sử dụng để tiền huấn luyện LM. Sau đó chúng ta có thể chỉ lấy mẫu một tập con từ D0 để tính toán tầm quan trọng tham số đối với kiến thức tổng quát trong LM. Tuy nhiên, vì chúng ta không có D0 đã được sử dụng để tiền huấn luyện RoBERTa, chúng tôi sử dụng dữ liệu Wiki (Merity et al., 2017) làm tập mẫu của D0. Chúng tôi chọn nó vì đây là một tập dữ liệu tổng quát với phạm vi chủ đề rộng và đã được sử dụng để tiền huấn luyện một LM, và nó có kích thước tương tự như dữ liệu miền của chúng tôi (khoảng 700M). Chúng tôi đã tiến hành hai thực nghiệm sử dụng dữ liệu: (a) DAS (Wiki+MLM), sử dụng MLM làm mất mát trong giai đoạn khởi tạo để tính toán tầm quan trọng của các đơn vị (để xác định kiến thức tổng quát) giống như bất kỳ miền nào khác trong phần học liên tục, và (b) DAS (Wiki+KL), sử dụng KL-divergence trong giai đoạn khởi tạo giống như phương pháp proxy được đề xuất. Kết quả được đưa ra trong Bảng 3.

Chúng ta có thể thấy rằng DAS (Wiki + KL) hoạt động tương tự như DAS nhưng vượt trội hơn DAS (Wiki + MLM). Điều này cho thấy proxy KL-divergence được đề xuất hiệu quả hơn. MLM thực sự thích ứng LM với dữ liệu Wikipedia, có thể không đủ đại diện cho dữ liệu gốc được sử dụng trong tiền huấn luyện LM. Kết quả là, nó kết thúc bằng việc xác định kiến thức chỉ phù hợp cho dữ liệu Wikipedia. Ngược lại, proxy KL-divergence được đề xuất tận dụng mặt nạ dropout ngẫu nhiên và đo lường độ mạnh mẽ, ít liên quan đến một miền cụ thể và do đó phản ánh kiến thức (tổng quát) trong LM ban đầu tốt hơn.

(2) So sánh kiến thức tổng quát được tính toán từ các kho ngữ liệu miền khác nhau. Ở đây, chúng tôi cũng cung cấp một số bằng chứng gián tiếp để chỉ ra hiệu quả của phương pháp proxy để tính toán tầm quan trọng của các đơn vị đối với kiến thức tổng quát trong LM. Chúng tôi tiến hành một thực nghiệm non-CL riêng biệt để so sánh các vector điểm tầm quan trọng của các đầu attention sau khi áp dụng proxy sử dụng dữ liệu từ các miền khác nhau.12 Cho mỗi miền i, chúng tôi so sánh vector tầm quan trọng của nó với vector tầm quan trọng của mọi miền khác, và sau đó tính trung bình độ tương tự cosine để có được giá trị cho miền i. Chúng tôi có 0.92 cho Restaurant, cùng 0.91 cho ACL, AI, và Phone, 0.89 cho PubMed và 0.92 cho Camera. Chúng ta thấy rằng các miền khác nhau cho các giá trị tầm quan trọng tương tự, điều này gián tiếp cho thấy proxy của chúng tôi có thể xác định gần đúng kiến thức tổng quát chung.

Ablation. Chúng tôi muốn biết liệu (1) khởi tạo được đề xuất (Mục 3.1), (2) che mềm, và (3) học tương phản có hữu ích không. Để trả lời (1), chúng tôi tiến hành ablation DAS (w/o initialization), nơi chúng tôi loại bỏ khởi tạo và trực tiếp thực hiện học liên tục mà không xem xét kiến thức tổng quát trong LM. Để trả lời (2), chúng tôi tiến hành các ablation (1) DAS (w/o softmask), nơi chúng tôi loại bỏ mặt nạ mềm, và chỉ sử dụng học tương phản dựa trên Eq. 7 (với thuật ngữ thứ hai trong mẫu số được loại bỏ); và (2) DAS (random) với các điểm tầm quan trọng được tạo ngẫu nhiên để thực hiện che mềm và học tương phản. Để trả lời (3), chúng tôi tiến hành hai ablation: (i) DAS (w/o contrast) nơi chúng tôi loại bỏ mất mát tương phản và chỉ che mềm theo tầm quan trọng; (ii) DAS (domain-specific) nơi chúng tôi tương phản kiến thức cụ thể miền và đã học (Mục 3.2).

Bảng 4 cho thấy DAS đầy đủ là tốt nhất về trung bình và cho hầu hết các miền, cho thấy mọi thành phần đều đóng góp. Các quan sát bổ sung là: (1) Lợi ích của DAS một phần từ kiến thức tổng quát được bảo tồn. Chúng ta có thể thấy DAS (w/o initialization) kém hơn về trung bình; (2) Che mềm giúp ích vì DAS (w/o softmask) kém hơn DAS. Điều này hợp lý vì che mềm có thể bảo tồn các miền đã học. Bên cạnh đó, mặt nạ dựa trên gradient của chúng tôi có thông tin vì DAS (random) tệ hơn DAS; (3) Học tương phản hiệu quả vì DAS (w/o contrast) và DAS (domain-specific) đều kém hơn, cho thấy học tương phản trong DAS có thể giúp học các biểu diễn tốt

5 KẾT LUẬN

Bài báo này đề xuất một phương pháp mới DAS cho huấn luyện DAP liên tục của một LM. Nó có ba ý tưởng chính: (1) Bảo tồn kiến thức quan trọng trước đó bằng cách che mềm các đơn vị theo tầm quan trọng của chúng để khắc phục CF và tạo điều kiện cho chuyển giao kiến thức. (2) Sử dụng một proxy mới để tính toán tầm quan trọng của các đơn vị đối với kiến thức tổng quát trong LM. (3) Học các biểu diễn bổ sung để tích hợp kiến thức. Một bộ kỹ thuật được đề xuất để đạt được chúng. Các thực nghiệm mở rộng cho thấy hiệu quả của DAS. Cách tiếp cận hiện tại bao gồm hai chức năng trong việc học. Chúng tôi sẽ nghiên cứu cách kết hợp chúng để cải thiện thêm kết quả trong tương lai.

LỜI CẢM ƠN

Công trình của Zixuan Ke, Gyuhak Kim, và Bing Liu được hỗ trợ một phần bởi hợp đồng nghiên cứu từ KDDI, hợp đồng nghiên cứu từ DARPA (HR001120C0023), và ba tài trợ NSF (IIS-1910424, IIS-1838770, và CNS-2225427).

TÀI LIỆU THAM KHẢO

Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, và Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.

Iz Beltagy, Kyle Lo, và Arman Cohan. SciBERT: A pretrained language model for scientific text. Trong EMNLP-IJCNLP, 2019.

Christopher Brix, Parnia Bahar, và Hermann Ney. Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. Trong ACL, Tháng 7 2020.

Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, và Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. arXiv preprint arXiv:2004.07211, 2020.

Tuhin Chakrabarty, Christopher Hidey, và Kathleen McKeown. Imho fine-tuning improves claim detection. arXiv preprint arXiv:1905.07000, 2019.

Ting Chen, Simon Kornblith, Mohammad Norouzi, và Geoffrey Hinton. A simple framework for contrastive learning of visual representations. Trong International conference on machine learning, trang 1597–1607. PMLR, 2020.

Yung-Sung Chuang, Shang-Yu Su, và Yun-Nung Chen. Lifelong language knowledge distillation. arXiv preprint arXiv:2010.02123, 2020.

Lucio M Dery, Paul Michel, Ameet Talwalkar, và Graham Neubig. Should we be pre-training? an argument for end-task aware training as an alternative. arXiv preprint arXiv:2109.07437, 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. Trong Jill Burstein, Christy Doran, và Thamar Solorio (eds.), NAACL-HLT, 2019.

Xiaowen Ding, Bing Liu, và Philip S Yu. A holistic lexicon-based approach to opinion mining. Trong Proceedings of the 2008 international conference on web search and data mining, 2008.

Claudio Greco, Barbara Plank, Raquel Fernández, và Raffaella Bernardi. Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering. arXiv preprint arXiv:1906.04229, 2019.

Yuxian Gu, Xu Han, Zhiyuan Liu, và Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021.

Yiwen Guo, Anbang Yao, và Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 29, 2016.

Pankaj Gupta, Yatin Chaudhary, Thomas Runkler, và Hinrich Schuetze. Neural topic modeling with continual lifelong learning. Trong International Conference on Machine Learning, trang 3907–3917. PMLR, 2020.

Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, và Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. Trong ACL, 2020.

Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, và Luke Zettlemoyer. Demix layers: Disentangling domains for modular language modeling. arXiv preprint arXiv:2108.05036, 2021.

Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, và Ross Girshick. Momentum contrast for unsupervised visual representation learning. Trong CVPR, trang 9729–9738, 2020.

Xu He và Herbert Jaeger. Overcoming catastrophic interference using conceptor-aided backpropagation. Trong ICLR, 2018.

Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Parameter-efficient transfer learning for NLP. Trong Kamalika Chaudhuri và Ruslan Salakhutdinov (eds.), ICML, 2019.

Minqing Hu và Bing Liu. Mining and summarizing customer reviews. Trong Proceedings of ACM SIGKDD, 2004.

Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, và Diyi Yang. Continual learning for text classification with information disentanglement based regularization. arXiv preprint arXiv:2104.05489, 2021.

Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, và Minjoon Seo. Towards continual knowledge learning of language models, 2021.

Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, và Xiang Ren. Lifelong pretraining: Continually adapting language models to emerging corpora. arXiv preprint arXiv:2110.08534, 2021.

David Jurgens, Srijan Kumar, Raine Hoover, Daniel A. McFarland, và Dan Jurafsky. Measuring the evolution of a scientific field through citation frames. TACL, 2018.

Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, và Lei Shu. Achieving forgetting prevention and knowledge transfer in continual learning. Advances in Neural Information Processing Systems, 34, 2021a.

Zixuan Ke, Bing Liu, Hu Xu, và Lei Shu. Classic: Continual and contrastive learning of aspect sentiment classification tasks. Trong EMNLP, 2021b.

Zixuan Ke, Hu Xu, và Bing Liu. Adapting bert for continual learning of a sequence of aspect sentiment classification tasks. Trong NAACL, trang 4746–4755, 2021c.

Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, và Bing Liu. Continual training of language models for few-shot learning. Trong Empirical Methods in Natural Language Processing (EMNLP), 2022a.

Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, và Bing Liu. Adapting a language model while preserving its general knowledge. Trong Empirical Methods in Natural Language Processing (EMNLP), 2022b.

Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, và Bing Liu. A theoretical study on solving continual learning. Trong Advances in Neural Information Processing Systems, 2022.

James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, và Raia Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, 2016.

Jens Kringelum, Sonny Kim Kjaerulff, Søren Brunak, Ole Lund, Tudor I Oprea, và Olivier Taboureau. Chemprot-3.0: a global chemical biology diseases mapping. Database, 2016, 2016.

Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khurana, David Cox, và Jim Glass. Parp: Prune, adjust and re-prune for self-supervised speech recognition. NeurIPS, 34, 2021.

Matthias De Lange, Rahaf Aljundi, Marc Masana, và Tinne Tuytelaars. Continual learning: A comparative study on how to defy forgetting in classification tasks. CoRR, 2019.

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, và Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240, 2020.

Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt tuning. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih (eds.), EMNLP, 2021.

Jiaoda Li, Ryan Cotterell, và Mrinmaya Sachan. Differentiable subset pruning of transformer heads. Transactions of the Association for Computational Linguistics, 9:1442–1459, 2021.

Yuanpeng Li, Liang Zhao, Kenneth Church, và Mohamed Elhoseiny. Compositional language continual learning. Trong International Conference on Learning Representations, 2019.

Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, và Qianru Sun. Mnemonics training: Multi-class incremental learning without forgetting. Trong CVPR, 2020.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, 2019.

Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, và Daniel S. Weld. S2ORC: the semantic scholar open research corpus. Trong Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel R. Tetreault (eds.), ACL, 2020.

Yi Luan, Luheng He, Mari Ostendorf, và Hannaneh Hajishirzi. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Trong Ellen Riloff, David Chiang, Julia Hockenmaier, và Jun'ichi Tsujii (eds.), ACL, 2018.

Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, và Zhiguang Wang. Continual learning in task-oriented dialogue systems. arXiv preprint arXiv:2012.15504, 2020.

JS McCarley, Rishav Chakravarti, và Avirup Sil. Structured pruning of a bert-based question answering model. arXiv preprint arXiv:1910.06360, 2019.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture models. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.

Paul Michel, Omer Levy, và Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019.

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, và Christopher D. Manning. Fast model editing at scale. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.

Jianmo Ni, Jiacheng Li, và Julian J. McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. Trong Kentaro Inui, Jing Jiang, Vincent Ng, và Xiaojun Wan (eds.), EMNLP, trang 188–197. Association for Computational Linguistics, 2019.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, và Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. Trong EACL, 2020.

Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, và Jie Zhou. ELLE: efficient lifelong pre-training for emerging data. Trong Smaranda Muresan, Preslav Nakov, và Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, trang 2789–2810. Association for Computational Linguistics, 2022. URL https://aclanthology.org/2022.findings-acl.220.

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H. Lampert. icarl: Incremental classifier and representation learning. Trong CVPR, 2017.

Ari Seff, Alex Beatson, Daniel Suo, và Han Liu. Continual learning in generative adversarial nets. CoRR, abs/1705.08395, 2017.

Joan Serrà, Didac Suris, Marius Miron, và Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. Trong ICML, 2018.

Yilin Shen, Xiangyu Zeng, và Hongxia Jin. A progressive model to enable continual learning for semantic slot filling. Trong EMNLP-IJCNLP, Tháng 11 2019.

Hanul Shin, Jung Kwon Lee, Jaehong Kim, và Jiwon Kim. Continual learning with deep generative replay. Trong NIPS, 2017.

Chi Sun, Xipeng Qiu, Yige Xu, và Xuanjing Huang. How to fine-tune bert for text classification? Trong China national conference on Chinese computational linguistics, trang 194–206. Springer, 2019.

Fan-Keng Sun, Cheng-Hao Ho, và Hung-Yi Lee. Lamol: Language modeling is all you need for lifelong language learning. Trong ICLR, 2020. URL https://openreview.net/forum?id=Skgxcn4YDS.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, và Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.

Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808, 2020a.

Zirui Wang, Sanket Vaibhav Mehta, Barnabás Póczos, và Jaime Carbonell. Efficient meta lifelong-learning with limited memory. Trong EMNLP, 2020b.

Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, và Ali Farhadi. Supermasks in superposition. Trong H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, và H. Lin (eds.), NeurIPS, 2020.

Hu Xu, Bing Liu, Lei Shu, và Philip S. Yu. BERT post-training for review reading comprehension and aspect-based sentiment analysis. Trong Jill Burstein, Christy Doran, và Thamar Solorio (eds.), NAACL-HLT, 2019.

A CHI TIẾT TẬP DỮ LIỆU
Bảng 1 trong bài báo chính đã hiển thị số lượng ví dụ trong mỗi tập dữ liệu. Ở đây chúng tôi cung cấp chi tiết bổ sung về 4 loại tác vụ cuối.

(1) (Phone, Camera và Restaurant) Phân loại tình cảm khía cạnh (ASC) được định nghĩa như sau: cho một khía cạnh hoặc tính năng sản phẩm (ví dụ: chất lượng hình ảnh trong đánh giá camera) và một câu đánh giá chứa khía cạnh trong một miền hoặc danh mục sản phẩm (ví dụ: camera), phân loại xem câu có thể hiện tình cảm hoặc cực tính tích cực, tiêu cực, hoặc trung tính (không có ý kiến) về khía cạnh (đối với tập dữ liệu Phone và Camera, chỉ có cực tính tiêu cực và tích cực trong dữ liệu).

(2) (ACL) Phân loại ý định trích dẫn được định nghĩa như sau: cho một câu trích dẫn (một câu chứa một trích dẫn), phân loại xem câu có thể hiện một chức năng trích dẫn trong số "background", "motivation", "uses", "extension" và "comparison or contrast future".

(3) (AI) Phân loại quan hệ được định nghĩa như sau: cho một đoạn từ trong câu chứa một cặp thực thể, phân loại xem đoạn có thể hiện một quan hệ trong số "feature of", "conjunction", "evaluate for", "hyponym of", "used for", "part of" và "compare".

(4) (PubMed) Phân loại tương tác hóa chất-protein được định nghĩa như sau: cho một đoạn chứa một cặp hóa chất và protein, phân loại xem đoạn có thể hiện một tương tác hóa chất-protein trong số "downregulator", "substrate", "indirect-upregulator", "indirect-downregulator", "agnonist", "activator", "product of", "agonist-activator", "inhibitor", "upregulator", "substrate product of", "agonist-inhibitor" và "antagonist".

B CHI TIẾT BASELINE

Baseline Không học liên tục: Mỗi baseline này xây dựng một mô hình riêng biệt cho mỗi tác vụ độc lập. Do đó nó không có chuyển giao kiến thức hoặc CF.

(1) Không huấn luyện DAP (RoBERTa) Liu et al. (2019) sử dụng RoBERTa gốc cho tinh chỉnh tác vụ cuối mà không có bất kỳ huấn luyện DAP nào. Đây là cái duy nhất không có bất kỳ huấn luyện DAP nào. Tất cả các baseline sau đây sử dụng mất mát mô hình ngôn ngữ có che (MLM) cho huấn luyện DAP.

(2) Huấn luyện DAP sử dụng mất mát mô hình ngôn ngữ có che (DAP-RoBERTa) là phương pháp huấn luyện DAP hiện có trong Gururangan et al. (2020). Theo hiểu biết của chúng tôi, các hệ thống huấn luyện DAP hiện có đều dựa trên mất mát MLM.

(3) Huấn luyện DAP sử dụng adapter-tuning Madotto et al. (2020); Houlsby et al. (2019) thêm các lớp adapter nhỏ giữa các lớp của Transformer cho huấn luyện DAP. Chúng tôi tuân theo thiết kế adapter trong Madotto et al. (2020); Houlsby et al. (2019): Một adapter đơn giản là 2 lớp mạng fully connected. Trong huấn luyện DAP, Transformer được cố định, chỉ các adapter được thêm vào mới có thể huấn luyện được. Kích thước bottleneck (kích thước adapter) được đặt thành 128. Trong tinh chỉnh tác vụ cuối, cả RoBERTa và adapter đều có thể huấn luyện được để đảm bảo so sánh công bằng.

(4) Huấn luyện DAP sử dụng prompt-tuning Lester et al. (2021) thêm một chuỗi các token vector thực (được gọi là token ảo hoặc token prompt) vào cuối chuỗi gốc. Trong huấn luyện DAP, RoBERTa (LM) được cố định và chỉ các token prompt được huấn luyện. Trong tinh chỉnh tác vụ cuối, cả LM và prompt được huấn luyện đều có thể huấn luyện được. Chúng tôi khởi tạo 100 token và đặt tỷ lệ học của token prompt thành 0.3 trong huấn luyện DAP, tuân theo thiết lập trong Lester et al. (2021).

Baseline Học liên tục (CL).

(5) Học liên tục naïve (NCL) là một mở rộng naïve của Gururangan et al. (2020), liên tục/tăng dần huấn luyện DAP LM để học tất cả các miền sử dụng mất mát MLM mà không có cơ chế để xử lý CF.

(6) Học liên tục với adapter (NCL-Adapter) Houlsby et al. (2019) tương tự như hệ thống dựa trên adapter. Sự khác biệt duy nhất là cùng một tập adapter được chia sẻ qua tất cả các miền, thay vì sử dụng một adapter mới cho mỗi miền mới.

(7) DEMIX (DEMIX) Gururangan et al. (2021) là một mô hình gần đây để thích ứng LM đã được tiền huấn luyện với các miền mới. Nó thêm một adapter mới khi một miền mới đến (cần mở rộng mạng) và khởi tạo adapter mới với các tham số của adapter được huấn luyện trước đó gần nhất với dữ liệu miền mới. Họ sử dụng perplexity trên một mẫu held-out để chọn adapter có khả năng nhất. Để so sánh công bằng, chúng tôi sử dụng cùng kích thước như {x^sub_n} làm các mẫu held-out.

(8) Hard attention để khắc phục quên (HAT-Adapter) Ke et al. (2021c) được dẫn xuất từ HAT Serrà et al. (2018), phương pháp dựa trên cô lập tham số hiện đại nhất với hầu như không có quên. Tuy nhiên, HAT yêu cầu thông tin task id trong tinh chỉnh tác vụ cuối (DAS hoạt động theo cách bất khả tri miền và không cần thông tin task id; xem Mục 1). HAT cũng cần huấn luyện một task embedding bổ sung để che từng lớp của mạng làm cho huấn luyện DAP không hiệu quả.

(9) Plugin học liên tục với capsule (BCL) Ke et al. (2021c) là một mô hình học liên tục có thể tránh quên và khuyến khích chuyển giao kiến thức. Nó tương tự như NCL-Adapter. Sự khác biệt là các adapter của nó bao gồm hai mô-đun, một là mạng capsule (một capsule mới được thêm vào khi một miền mới đến) để khuyến khích chuyển giao và cái khác tương tự như HAT để tránh quên. Tương tự như HAT, thông tin tác vụ/miền cần thiết trong tinh chỉnh tác vụ cuối. Chúng tôi thay thế mạng backbone từ BERT bằng RoBERTa để so sánh công bằng.

(10) Plugin học liên tục với chuyển giao tương phản (CLASSIC) Ke et al. (2021b) là một mô hình học liên tục có thể tránh quên và khuyến khích chuyển giao kiến thức thông qua mất mát tương phản. Nó tương tự như HAT nhưng 3 mất mát tương phản bổ sung được sử dụng cho chưng cất, chuyển giao kiến thức và tương phản có giám sát. Vì DAS đang làm việc trên dữ liệu không giám sát, chúng tôi loại bỏ mất mát tương phản có giám sát. Tương tự như HAT, thông tin tác vụ cần thiết trong tinh chỉnh tác vụ cuối. Chúng tôi thay thế mạng backbone từ BERT bằng RoBERTa để so sánh công bằng.

(11) Chưng cất kiến thức (KD) Hinton et al. (2015) tối thiểu hóa độ lệch biểu diễn giữa biểu diễn đã học và biểu diễn mới trong huấn luyện DAP. Chúng tôi tính toán KL divergence giữa các biểu diễn (đầu ra trước đầu dự đoán mô hình ngôn ngữ có che) của mỗi token của LM được huấn luyện DAP trước đó và LM hiện tại làm mất mát chưng cất.

(12) EWC Buzzega et al. (2020) là một phương pháp dựa trên chính quy hóa phổ biến áp dụng elastic weights consolidation để thêm chính quy hóa L2 vào các thay đổi tham số.

(13) DER++ Buzzega et al. (2020) là một phương pháp phát lại gần đây sử dụng chưng cất để chính quy hóa việc huấn luyện tác vụ mới. Chúng tôi lưu trữ 16.4K token cho mỗi miền đã học làm bộ nhớ, đây là bộ nhớ lớn nhất chúng tôi có thể sử dụng để hệ thống chạy được.

(14) HAT Serrà et al. (2018) được sử dụng trong các lớp Transformer (bao gồm self-attention, trung gian và đầu ra) thay vì các lớp adapter được thêm vào. Task embedding bổ sung và thông tin tác vụ cho tinh chỉnh tác vụ cuối là cần thiết.

C CHI TIẾT THỰC HIỆN

Kiến trúc. Chúng tôi áp dụng RoBERTa BASE làm LM backbone. Một đầu mô hình ngôn ngữ có che được áp dụng cho huấn luyện DAP. Tinh chỉnh tác vụ cuối của RoBERTa tuân theo thực hành tiêu chuẩn. Đối với ba tác vụ ASC (xem Bảng 1), chúng tôi áp dụng công thức ASC trong Xu et al. (2019), nơi khía cạnh (ví dụ: "sound") và câu đánh giá (ví dụ: "The sound is great") được nối qua </s>.

Siêu tham số. Trừ khi được nêu khác, cùng siêu tham số được sử dụng trong tất cả các thực nghiệm. Độ dài đầu vào tối đa được đặt thành 164, đủ cho tất cả các tập dữ liệu. Tối ưu hóa Adam được sử dụng cho cả huấn luyện DAP và tinh chỉnh tác vụ cuối. Độ dài chuỗi tối đa cũng được đặt thành 164.

Huấn luyện DAP. Tỷ lệ học được đặt thành 1e-4 và kích thước batch thành 256. Chúng tôi huấn luyện 2.5K bước cho mỗi miền, xấp xỉ một lượt đầy đủ qua dữ liệu miền, tuân theo Gururangan et al. (2020); Xu et al. (2019). Tập con dữ liệu {x^sub_n} để tính toán Limpt để xác định tầm quan trọng đầu trong Mục 3.1 và 3.3 được đặt thành 1.64 triệu token, đủ trong các thực nghiệm của chúng tôi. λ trong Eq. 8 được đặt thành 1 và τ trong Eq. 7 được đặt thành 0.05.

Tinh chỉnh tác vụ cuối. Tỷ lệ học được đặt thành 1e-5 và kích thước batch thành 16. Chúng tôi huấn luyện trên các tập dữ liệu tinh chỉnh tác vụ cuối trong 5 epoch cho Restaurant; 10 epoch cho ACL, AI và PubMed; và 15 epoch cho Phone và Camera. Chúng tôi đơn giản lấy kết quả cho epoch cuối cùng, giả định không có tập validation. Chúng tôi thấy rằng số epoch trên cho chúng tôi kết quả ổn định và hội tụ.

D HUẤN LUYỆN DAP THEO CÁC THỨ TỰ KHÁC NHAU

Bảng 2 trong bài báo chính báo cáo kết quả cho thứ tự Restaurant→ACL→AI→Phone→PubMed→Camera. Bây giờ chúng tôi xem thứ tự ảnh hưởng đến kết quả như thế nào. Do tính chất tính toán chuyên sâu của huấn luyện DAP, chúng tôi chỉ báo cáo kết quả baseline tốt nhất (NCL) và DAS với các thứ tự miền khác nhau. Bảng 5 hiển thị kết quả NCL và DAS của 5 thứ tự khác nhau. Chúng ta có thể thấy DAS luôn tốt hơn NCL, chứng minh hiệu quả của DAS.

E ĐỘ LỆCH CHUẨN

Bảng 6 báo cáo độ lệch chuẩn của các kết quả tương ứng trong Bảng 2 (trong bài báo chính) của DAS và các baseline được xem xét qua 5 lần chạy với các seed ngẫu nhiên. Chúng ta có thể thấy kết quả của DAS ổn định. Một số baseline (ví dụ: RoBERTa trong AI, DAP-RoBERTa trong Camera) có thể có độ lệch chuẩn khá lớn.

Bảng 7 báo cáo độ lệch chuẩn của các kết quả tương ứng trong Bảng 4 (trong bài báo chính) của DAS và các baseline được xem xét qua 5 lần chạy với các seed ngẫu nhiên. Chúng ta có thể thấy kết quả của DAS và các biến thể của nó ổn định.
