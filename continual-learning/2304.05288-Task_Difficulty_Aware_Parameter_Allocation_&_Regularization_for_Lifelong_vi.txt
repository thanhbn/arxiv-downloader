# Phân Bổ Tham Số & Regularization Nhận Biết Độ Khó Nhiệm Vụ cho Học Suốt Đời

Wenjin Wang, Yunqing Hu, Qianglong Chen, Yin Zhang*
Đại học Zhejiang, Hàng Châu, Trung Quốc
{wangwenjin,yunqinghu,chenqianglong,zhangyin98}@zju.edu.cn

## Tóm tắt

Các phương pháp regularization hoặc phân bổ tham số có hiệu quả trong việc khắc phục quên thảm khốc trong học suốt đời. Tuy nhiên, chúng giải quyết tất cả các nhiệm vụ trong một chuỗi một cách đồng nhất và bỏ qua sự khác biệt về độ khó học của các nhiệm vụ khác nhau. Do đó, các phương pháp regularization tham số phải đối mặt với việc quên đáng kể khi học một nhiệm vụ mới rất khác so với các nhiệm vụ đã học, và các phương pháp phân bổ tham số phải đối mặt với chi phí tham số không cần thiết khi học các nhiệm vụ đơn giản. Trong bài báo này, chúng tôi đề xuất Phân Bổ Tham Số & Regularization (PAR), lựa chọn thích ứng một chiến lược phù hợp cho mỗi nhiệm vụ từ phân bổ tham số và regularization dựa trên độ khó học của nó. Một nhiệm vụ dễ dàng cho một mô hình đã học các nhiệm vụ liên quan đến nó và ngược lại. Chúng tôi đề xuất một phương pháp ước lượng phân kỳ dựa trên khoảng cách Nearest-Prototype để đo lường mức độ liên quan của nhiệm vụ chỉ sử dụng các đặc trưng của nhiệm vụ mới. Hơn nữa, chúng tôi đề xuất một chiến lược tìm kiếm kiến trúc dựa trên sampling nhận biết mức độ liên quan tiết kiệm thời gian để giảm chi phí tham số cho phân bổ. Kết quả thực nghiệm trên nhiều benchmark cho thấy, so với các SOTA, phương pháp của chúng tôi có khả năng mở rộng và giảm đáng kể sự dư thừa của mô hình trong khi cải thiện hiệu suất của mô hình. Phân tích định tính tiếp theo cho thấy PAR có được mức độ liên quan nhiệm vụ hợp lý.

## 1. Giới thiệu

Gần đây, khả năng học suốt đời [9] của mạng neural, tức là học liên tục từ một chuỗi liên tục các nhiệm vụ, đã được nghiên cứu rộng rãi. Việc liên tục học hỏi và tích lũy kiến thức từ các nhiệm vụ rồi sau đó sử dụng nó để hỗ trợ học tập trong tương lai là điều tự nhiên đối với con người. Tuy nhiên, các mô hình cổ điển [13, 18, 41] gặp phải tình trạng quên thảm khốc [12], tức là hiệu suất của mô hình trên các nhiệm vụ đã học suy giảm nhanh chóng sau khi học một nhiệm vụ mới.

Để khắc phục tình trạng quên thảm khốc, nhiều phương pháp regularization hoặc phân bổ tham số đã được đề xuất. Các phương pháp regularization tham số [10, 16, 19, 21, 23, 25, 27] giảm bớt việc quên bằng cách thêm một hạng regularization vào hàm mất mát và hoạt động tốt khi nhiệm vụ mới không khác nhiều so với các nhiệm vụ đã học. Các phương pháp phân bổ tham số dựa trên mô hình tĩnh [7, 15, 31, 36] và mô hình động [2,20,24,26,29,30,34,38,39,42,45,47] phân bổ các tham số khác nhau cho các nhiệm vụ khác nhau và có thể thích ứng với các nhiệm vụ mới khá khác so với các nhiệm vụ đã học. Tuy nhiên, các phương pháp trên giải quyết tất cả các nhiệm vụ trong một chuỗi một cách đồng nhất, và bỏ qua sự khác biệt về độ khó học của các nhiệm vụ khác nhau. Điều này dẫn đến việc quên đáng kể trong các phương pháp regularization tham số khi học một nhiệm vụ mới khá khác so với các nhiệm vụ đã học, và cũng dẫn đến chi phí tham số không cần thiết trong các phương pháp phân bổ tham số khi học một số nhiệm vụ đơn giản.

Trong bài báo này, chúng tôi đề xuất một phương pháp nhận biết độ khó là Phân Bổ Tham Số & Regularization (PAR). Như được thể hiện trong Hình 1, chúng tôi giả định rằng độ khó học của một nhiệm vụ trong học liên tục không chỉ phụ thuộc vào bản thân nhiệm vụ mà còn phụ thuộc vào kiến thức tích lũy trong mô hình. Một nhiệm vụ mới dễ thích ứng cho một mô hình nếu nó đã học các nhiệm vụ liên quan trước đó và ngược lại. Dựa trên giả định này, PAR áp dụng thích ứng phân bổ tham số cho các nhiệm vụ khó và regularization tham số cho các nhiệm vụ dễ. Cụ thể, PAR chia các nhiệm vụ thành các nhóm nhiệm vụ và gán cho mỗi nhóm một mô hình chuyên gia chuyên dụng. Với một nhiệm vụ mới, PAR đo lường mức độ liên quan giữa nó và các nhóm nhiệm vụ hiện có trước tiên. Nếu nhiệm vụ mới liên quan đến một trong các nhóm hiện có, nó dễ dàng cho chuyên gia tương ứng. PAR thêm nhiệm vụ vào nhóm liên quan và học nó bằng chuyên gia thông qua regularization tham số. Nếu không, nhiệm vụ mới khó đối với tất cả các chuyên gia hiện có, và PAR gán nó cho một nhóm nhiệm vụ mới và phân bổ một chuyên gia mới để học nó.

Có hai thách thức trong công việc này: đo lường mức độ liên quan và sự bùng nổ tham số liên quan đến phân bổ tham số. Đối với thách thức đầu tiên, chúng tôi cố gắng đo lường mức độ liên quan bằng phân kỳ KL giữa các phân phối đặc trưng của các nhiệm vụ. Tuy nhiên, phân kỳ KL không thể tính toán được và cần được ước lượng vì các phân phối đặc trưng của các nhiệm vụ thường không được biết. Ngoài ra, ràng buộc của học suốt đời rằng chỉ dữ liệu của nhiệm vụ hiện tại có sẵn làm trầm trọng thêm khó khăn trong việc ước lượng. Để giải quyết các vấn đề trên, lấy cảm hứng từ ước lượng phân kỳ dựa trên khoảng cách k-NN [44], chúng tôi đề xuất phương pháp ước lượng phân kỳ dựa trên khoảng cách prototype, chỉ phụ thuộc vào dữ liệu của nhiệm vụ hiện tại. Đối với thách thức thứ hai, chúng tôi cố gắng giảm chi phí tham số trên mỗi chuyên gia bằng cách tìm kiếm kiến trúc compact cho nó. Tuy nhiên, hiệu quả thời gian và bộ nhớ thấp là một trở ngại đối với việc áp dụng tìm kiếm kiến trúc cho một chuỗi các nhiệm vụ trong học suốt đời. Để cải thiện hiệu quả của tìm kiếm kiến trúc, chúng tôi đề xuất tìm kiếm phân cấp dựa trên sampling nhận biết mức độ liên quan. Các đóng góp chính của công việc này như sau:

• Chúng tôi đề xuất một framework học suốt đời có tên là Phân Bổ Tham Số & Regularization (PAR), lựa chọn một chiến lược phù hợp từ phân bổ tham số và regularization cho mỗi nhiệm vụ dựa trên độ khó học. Độ khó phụ thuộc vào việc liệu mô hình đã học các nhiệm vụ liên quan trước đây hay chưa.

• Chúng tôi đề xuất một phương pháp ước lượng phân kỳ dựa trên khoảng cách prototype để đo lường khoảng cách giữa nhiệm vụ mới và các nhiệm vụ đã học trước đây chỉ với dữ liệu của nhiệm vụ mới. Đồng thời, chúng tôi đề xuất tìm kiếm kiến trúc dựa trên sampling nhận biết mức độ liên quan để giảm chi phí tham số của phân bổ tham số.

• Kết quả thực nghiệm trên CTrL, Mixed CIFAR100 và F-CelebA, CIFAR10-5, CIFAR100-10, CIFAR100-20 và MiniImageNet-20 cho thấy PAR có khả năng mở rộng và giảm đáng kể sự dư thừa mô hình trong khi cải thiện hiệu suất mô hình. Các nghiên cứu ablation toàn diện cho thấy hiệu quả của các thành phần trong PAR và các visualization cho thấy tính hợp lý của khoảng cách nhiệm vụ trong PAR.

## 2. Công trình Liên quan

### 2.1. Học Suốt đời

Nhiều phương pháp đã được đề xuất để khắc phục quên thảm khốc. Các phương pháp replay cố gắng replay các mẫu của các nhiệm vụ trước đây khi học một nhiệm vụ mới từ bộ nhớ episodic [32,40] hoặc bộ nhớ generative [3,28,37]. Các phương pháp regularization tham số, bao gồm regularization tập trung vào prior [16, 19, 23, 27] và regularization tập trung vào dữ liệu [10, 21, 25], cố gắng giảm bớt việc quên bằng cách đưa ra một hạng regularization trong hàm mất mát của nhiệm vụ mới. Các phương pháp phân bổ tham số dựa trên mô hình tĩnh [7,15,31,36] và mô hình động [2,20,24,26,29,30,34,38,39,42,45,47] khắc phục quên thảm khốc bằng cách phân bổ các tham số khác nhau cho các nhiệm vụ khác nhau.

**Các phương pháp với mức độ liên quan.** Một số phương pháp cũng xem xét tính hữu ích của mức độ liên quan nhiệm vụ [2, 15]. Expert Gate [2] gán các chuyên gia chuyên dụng và auto-encoders cho các nhiệm vụ và tính toán mức độ liên quan nhiệm vụ bằng lỗi reconstruction của auto-encoders. Mức độ liên quan nhiệm vụ được sử dụng để chuyển giao kiến thức từ nhiệm vụ trước đây liên quan nhất. CAT [15] định nghĩa sự tương tự nhiệm vụ bằng việc chuyển giao kiến thức tích cực. Nó tập trung vào việc chuyển giao có chọn lọc kiến thức từ các nhiệm vụ trước đây tương tự và xử lý việc quên giữa các nhiệm vụ không tương tự bằng hard attention. Trong bài báo này, chúng tôi đề xuất một phương pháp ước lượng phân kỳ dựa trên khoảng cách prototype để tính toán mức độ liên quan nhiệm vụ được sử dụng để đo lường độ khó học nhiệm vụ.

**Các phương pháp với NAS.** Các phương pháp dựa trên mô hình tĩnh [7,31] cố gắng tìm kiếm một sub-model cho mỗi nhiệm vụ với neural architecture search (NAS). Các phương pháp dựa trên mô hình động [20, 42, 46] áp dụng NAS để lựa chọn một chiến lược mở rộng mô hình phù hợp cho mỗi nhiệm vụ và phải đối mặt với chi phí GPU memory, tham số và thời gian cao. Thay vào đó, trong bài báo này, chúng tôi đề xuất tìm kiếm kiến trúc dựa trên cell phân cấp nhận biết mức độ liên quan để tìm kiếm kiến trúc compact cho mỗi chuyên gia với chi phí GPU memory và thời gian thấp hơn. [8,14] áp dụng NAS trên multi-task learning.

### 2.2. Cell-based NAS

Neural architecture search (NAS) [11,33] nhằm tìm kiếm các kiến trúc mạng neural hiệu quả từ một không gian tìm kiếm được định nghĩa trước theo cách hướng dữ liệu. Để giảm kích thước của không gian tìm kiếm, các phương pháp cell-based NAS [22, 48, 49] cố gắng tìm kiếm kiến trúc cell từ một không gian tìm kiếm cell được định nghĩa trước, trong đó một cell là một mạng convolutional nhỏ ánh xạ một tensor H×W×F thành tensor H'×W'×F' khác. Mô hình cuối cùng bao gồm một số lượng được định nghĩa trước các cell được xếp chồng. Cell trong NAS tương tự như residual block trong residual network (ResNet), nhưng kiến trúc của nó phức tạp hơn và là một đồ thị có hướng không chu trình (DAG). Các phép toán trong không gian tìm kiếm thường hiệu quả về tham số. Các phương pháp NAS thường tạo ra các kiến trúc compact hơn so với các thiết kế handcrafted.

## 3. Phương pháp

Chúng tôi tập trung vào kịch bản task-incremental của học suốt đời. Cụ thể, mô hình học một chuỗi các nhiệm vụ T={T₁, . . . , Tₜ, . . . , Tₙ} từng cái một và task id của mẫu có sẵn trong cả quá trình training và inference. Mỗi nhiệm vụ Tₜ có một dataset training, Dₜᵗʳᵃⁱⁿ = {(xₜᵢ, yₜᵢ); i = 1, . . . , nₜᵗʳᵃⁱⁿ}, trong đó yₜᵢ là nhãn thực và nₜᵗʳᵃⁱⁿ là số lượng ví dụ training. Tương tự, chúng tôi ký hiệu validation và test dataset của nhiệm vụ Tₜ là Dₜᵛᵃˡⁱᵈ và Dₜᵗᵉˢᵗ.

Như được hiển thị trong Hình 2, PAR chia các nhiệm vụ đã học trước đây thành các nhóm nhiệm vụ G và gán cho mỗi nhóm Gᵢ một mô hình chuyên gia chuyên dụng Eᵢ. Đối với một nhiệm vụ mới, PAR tính toán các khoảng cách, phản ánh mức độ liên quan, giữa nó và các nhóm nhiệm vụ hiện có trước tiên. Sau đó, nó áp dụng một chiến lược học thích hợp từ phân bổ tham số và regularization cho nhiệm vụ mới dựa trên các khoảng cách.

### 3.1. Khoảng cách Nhiệm vụ thông qua Khoảng cách Nearest-Prototype

PAR tính toán khoảng cách giữa nhiệm vụ mới và mỗi nhiệm vụ hiện có trước tiên, sau đó nó tính trung bình các khoảng cách của các nhiệm vụ trong cùng một nhóm như khoảng cách giữa nhiệm vụ mới và nhóm.

Chúng tôi tính toán khoảng cách giữa hai nhiệm vụ bằng phân kỳ KL giữa các phân phối của chúng. Tuy nhiên, phân kỳ KL không thể tính toán được vì các phân phối thực của các nhiệm vụ thường không được biết. Thay vào đó, chúng tôi ước lượng phân kỳ KL bằng các đặc trưng của dữ liệu. Để tăng cường độ ổn định ước lượng, chúng tôi giới thiệu một extractor được pre-trained bổ sung¹ để tạo ra một đặc trưng robust Xₜᵢ cho mỗi hình ảnh xₜᵢ trong nhiệm vụ Tₜ. Extractor được pre-trained chỉ được sử dụng cho ước lượng phân kỳ và không ảnh hưởng đến việc học các nhiệm vụ. Số lượng tham số bổ sung được giới thiệu bởi mô hình là một hằng số và không ảnh hưởng đến khả năng mở rộng của phương pháp của chúng tôi.

**k-Nearest Neighbor Distance** Cho các đặc trưng của phân phối, một phương pháp ước lượng phân kỳ cổ điển cho densities đa chiều là khoảng cách k-Nearest Neighbor (k-NN) [44]. Giả sử các phân phối đặc trưng của nhiệm vụ mới Tᵢ và một nhiệm vụ hiện có Tⱼ là qᵢ và qⱼ tương ứng, và các đặc trưng Xᵢ = {Xᵢ₁, . . . , Xᵢₙ} và Xⱼ = {Xⱼ₁, . . . , Xⱼₘ} được rút ra độc lập từ các phân phối qᵢ và qⱼ, trong đó n và m là số lượng mẫu. Khoảng cách k-NN của đặc trưng Xᵢₗ trong Xᵢ và Xⱼ được ký hiệu là νₖ(l) và ρₖ(l) tương ứng. Cụ thể, ρₖ(l) là khoảng cách Euclidean từ Xᵢₗ đến k-NN của nó trong {Xᵢᵤ}ᵤ≠ₗ. Tương tự, νₖ(l) là khoảng cách từ Xᵢₗ đến k-NN của nó trong Xⱼ. Sau đó, phân kỳ KL giữa các phân phối qᵢ và qⱼ được ước lượng như sau:

KL(qᵢ||qⱼ) ≈ d̂KL(qᵢ||qⱼ) = d/n ∑ₗ₌₁ⁿ log(νₖ(l)/ρₖ(l)) + log(m/(n-1)), (1)

trong đó d là kích thước đặc trưng.

Khoảng cách k-NN là bất đối xứng và điều này phù hợp với tính bất đối xứng của phân kỳ KL. Động lực đằng sau Phương trình (1) là khoảng cách k-NN là bán kính của một open Euclidean ball d-chiều được sử dụng để ước lượng mật độ k-NN. Độc giả có thể tham khảo [44] để biết thêm chi tiết như phân tích hội tụ.

**Nearest-Prototype Distance** Tuy nhiên, việc tính toán khoảng cách k-NN liên quan đến các đặc trưng của hai nhiệm vụ Tᵢ và Tⱼ và vi phạm ràng buộc của học suốt đời rằng chỉ dữ liệu của nhiệm vụ hiện tại Tᵢ có sẵn. Để khắc phục vấn đề này, chúng tôi đề xuất khoảng cách Nearest-Prototype để thay thế khoảng cách k-NN. Cụ thể, giả sử tập hợp các lớp trong nhiệm vụ Tᵢ là Cᵢ, cho mỗi lớp c ∈ Cᵢ, chúng tôi duy trì một đặc trưng prototype Uᵢc, là trung bình của các đặc trưng của các mẫu thuộc lớp c. Sau đó, khoảng cách Nearest-Prototype của Xᵢₗ đến Xᵢ được định nghĩa như sau:

ρ(l) = ||Xᵢₗ - Uᵢc(l)||, (2)

trong đó || · || là khoảng cách Euclidean và c(l) là lớp của Xᵢₗ. Tương tự, chúng tôi ký hiệu tập hợp các lớp trong nhiệm vụ Tⱼ là Cⱼ và các đặc trưng prototype của Xⱼ cho mỗi lớp c ∈ Cⱼ là Uⱼc. Sau đó, khoảng cách Nearest-Prototype của Xᵢₗ đến Xⱼ được ký hiệu là

ν(l) = min_c ||Xᵢₗ - Uⱼc||₂, c ∈ Cⱼ. (3)

Sau đó, bằng cách thay thế các khoảng cách k-NN νₖ(l) và ρₖ(l) trong Phương trình (1) bằng các khoảng cách Nearest-Prototype ν(l) và ρ(l), phân kỳ KL được ước lượng bởi khoảng cách Nearest-Prototype như sau:

KL(qᵢ||qⱼ) ≈ ĝKL(qᵢ||qⱼ) = ∑c∈Cᵢ (1/nc) ∑ₗ₌₁ⁿᶜ log(ν(l)/ρ(l)), (4)

trong đó các hạng constant được bỏ qua vì chúng tôi chỉ quan tâm đến độ lớn tương đối của phân kỳ KL. Các Phương trình (2) đến (4) chỉ liên quan đến các đặc trưng của nhiệm vụ mới Tᵢ và các prototype của nhiệm vụ hiện có Tⱼ, thỏa mãn ràng buộc của học suốt đời. Chi phí lưu trữ của các prototype là nhỏ và có thể bỏ qua.

Động lực đằng sau Phương trình (4) là trực quan. Khoảng cách Nearest-Prototype ρ(l) phản ánh mối quan hệ giữa mẫu Xᵢₗ trong nhiệm vụ Tᵢ và prototype lớp của nó trong không gian đặc trưng. Khoảng cách Nearest-Prototype ν(l) phản ánh mối quan hệ giữa mẫu Xᵢₗ và các prototype lớp của nhiệm vụ hiện có Tⱼ trong không gian đặc trưng. Nếu giá trị của ρ(l) và ν(l) bằng nhau cho mỗi Xᵢₗ, các prototype lớp của hai nhiệm vụ gần nhau. Trong trường hợp này, phân phối của hai nhiệm vụ tương tự, và phân kỳ KL được ước lượng bởi Phương trình (4) là 0.

**Chiến lược Học Thích ứng** Cho khoảng cách giữa nhiệm vụ mới Tᵢ và mỗi nhiệm vụ hiện có bằng phân kỳ KL, chúng tôi ký hiệu khoảng cách s'ᵢ,g giữa nhiệm vụ Tᵢ và một nhóm nhiệm vụ Gg bằng trung bình của các khoảng cách của các nhiệm vụ trong nhóm như sau:

s'ᵢ,g = (1/|Gg|) ∑j'∈Gg KL(qᵢ||qⱼ'), (5)

trong đó qᵢ và qⱼ' đại diện cho các phân phối đặc trưng của nhiệm vụ Tᵢ và nhiệm vụ thứ j' trong nhóm Gg. s'ᵢ,g ∈ [0,∞) vì phạm vi của phân kỳ KL là [0,∞). Tuy nhiên, chúng tôi cố gắng sử dụng khoảng cách để phản ánh mức độ liên quan, thường được đo bằng một giá trị giữa 0 và 1. Vì vậy chúng tôi ánh xạ s'ᵢ,g vào [0,1] bằng một hàm tăng đơn điệu như sau:

sᵢ,g = min(s'ᵢ,g, 1-e^(-2×s'ᵢ,g)), (6)

trong đó e là số Euler.

Giả sử khoảng cách nhỏ nhất giữa nhiệm vụ mới Tₜ và các nhóm hiện có là sₜ,g* và nhóm nhiệm vụ tương ứng là Gg*, PAR lựa chọn chiến lược học bằng cách so sánh sₜ,g* với một siêu tham số α. Nếu sₜ,g* ≤ α, nhiệm vụ mới được thêm vào nhóm liên quan Gg* này và học bằng regularization tham số. Nếu không, nhiệm vụ mới được gán cho một nhóm mới và học bằng phân bổ tham số vì không có nhóm nhiệm vụ hiện có nào liên quan đến nó.

### 3.2. Parameter Regularization

Một nhiệm vụ mới Tₜ dễ dàng cho mô hình chuyên gia Eg* nếu nó liên quan đến nhóm Gg*. PAR tái sử dụng chuyên gia Eg* để học nhiệm vụ này bằng regularization tham số. Lấy cảm hứng từ LwF [21], chúng tôi áp dụng một phương pháp regularization tham số dựa trên knowledge distillation. Cụ thể, hàm mất mát bao gồm một mất mát training Lnew và một mất mát distillation Lold. Mất mát training khuyến khích chuyên gia Eg* thích ứng với nhiệm vụ mới Tₜ và là cross-entropy cho classification như sau:

Lnew = -∑(x,y)∈Dₜᵗʳᵃⁱⁿ log(pEg*(y|x)). (7)

Mất mát distillation khuyến khích chuyên gia Eg* duy trì hiệu suất trên các nhiệm vụ trước đây trong nhóm Gg*. Để tính toán nó, chúng tôi ghi lại các logits yⱼ của output head của mỗi nhiệm vụ trước đây Tⱼ cho mỗi mẫu x của nhiệm vụ Tₜ. Mất mát distillation như sau:

Lold = -∑(x,y)∈Dₜᵗʳᵃⁱⁿ ∑Tⱼ∈Gg* yⱼ · log(pEg*(x)ⱼ), (8)

trong đó yⱼ và pEg*(x)ⱼ là các vector có độ dài bằng số lượng danh mục của nhiệm vụ trước đây Tⱼ. Tổng mất mát là

LPR = Lnew + λLold, (9)

trong đó λ là một siêu tham số để cân bằng mất mát training và distillation. Lưu ý rằng phương pháp của chúng tôi không có bộ nhớ và không yêu cầu lưu trữ mẫu cho các nhiệm vụ trước đây.

Tuy nhiên, chuyên gia Eg* có thể over-fit nhiệm vụ mới có kích thước mẫu ít hơn nhiều so với các nhiệm vụ trong nhóm Gg*, ngay cả khi nhiệm vụ mới liên quan đến nhóm. Điều này dẫn đến sự can thiệp với kiến thức đã tích lũy trước đây trong chuyên gia Eg*. Để tránh vấn đề trên, PAR ghi lại kích thước mẫu tối đa của các nhiệm vụ trong mỗi nhóm nhiệm vụ. Giả sử kích thước mẫu tối đa trong nhóm Gg* là Q, PAR đóng băng các tham số của chuyên gia Eg* ngoại trừ task-specific classification head trong quá trình học nếu kích thước mẫu của nhiệm vụ mới ít hơn 10 phần trăm của Q. Bằng cách chuyển giao kiến thức hiện có trong chuyên gia Eg*, chỉ tối ưu hóa classification header là đủ để thích ứng với nhiệm vụ mới vì nhiệm vụ mới liên quan đến nhóm Gg*.

### 3.3. Parameter Allocation

Nếu không có nhóm hiện có nào liên quan đến nhiệm vụ mới Tₜ, PAR gán nó cho một nhóm nhiệm vụ mới GM+1 với một chuyên gia mới EM+1, trong đó M là số lượng nhóm hiện có. Chúng tôi áp dụng mất mát cross-entropy cho nhiệm vụ Tₜ như sau:

LPA = -∑(x,y)∈Dₜᵗʳᵃⁱⁿ log(pEM+1(y|x)). (10)

Số lượng chuyên gia và tham số trong PAR tỷ lệ với số lượng nhóm nhiệm vụ, giảm thiểu sự tăng trưởng của tham số. Để tiếp tục giảm chi phí tham số của mỗi chuyên gia, chúng tôi áp dụng cell-based NAS (tham khảo Mục 2.2 để biết chi tiết) để tìm kiếm kiến trúc compact cho nó.

Mỗi chuyên gia trong PAR được xếp chồng với nhiều cell và việc tìm kiếm kiến trúc chuyên gia tương đương với việc tìm kiếm kiến trúc cell. Vì chi phí thời gian của NAS trở nên không thể chịu đựng khi số lượng nhiệm vụ tăng lên, chúng tôi đề xuất một chiến lược tìm kiếm kiến trúc dựa trên sampling nhận biết mức độ liên quan để cải thiện hiệu quả.

Như được hiển thị trong Hình 2, chúng tôi xây dựng một không gian tìm kiếm phân cấp. Không gian tìm kiếm coarse-grained chứa các cell được sử dụng bởi các chuyên gia hiện có và một cell chưa biết có kiến trúc sẽ đến từ không gian tìm kiếm fine-grained. Theo thực hành thông thường [22, 48], không gian tìm kiếm fine-grained là một đồ thị có hướng không chu trình (DAG) với bảy node (hai input node i₁, i₂, một chuỗi có thứ tự các intermediate node n₁, n₂, n₃, n₄, và một output node). Các input node đại diện cho các output của hai layer trước đó. Output được concatenate từ tất cả các intermediate node. Mỗi intermediate node có một candidate edge có hướng từ mỗi predecessor của nó. Mỗi candidate edge được liên kết với sáu candidate operation hiệu quả về tham số.

Để tìm kiếm một cell cho nhiệm vụ mới, chúng tôi giới thiệu một siêu tham số β. Nếu sₜ,g* ≤ β, PAR tái sử dụng cell của chuyên gia Eg* cho nhiệm vụ Tₜ. Khoảng cách nhiệm vụ lớn hơn α và nhỏ hơn β cho thấy nhiệm vụ mới không đủ liên quan để chia sẻ cùng một chuyên gia với các nhiệm vụ trong nhóm Gg* nhưng có thể sử dụng cùng kiến trúc. Nếu sₜ,g* > β, PAR gán cell chưa biết cho chuyên gia mới và áp dụng một phương pháp NAS dựa trên sampling có tên MDL [48] để xác định kiến trúc của nó. Do hạn chế về không gian, chúng tôi để lại chi tiết của MDL trong phần bổ sung.

## 4. Thí nghiệm

### 4.1. Cài đặt Thí nghiệm

**Benchmarks** Chúng tôi áp dụng hai benchmark là CTrL [42] và Mixed CIFAR100 và F-CelebA [15], chứa các nhiệm vụ tương tự và không tương tự được trộn lẫn. CTrL [42] bao gồm 6 stream của các nhiệm vụ image classification: S₋ được sử dụng để đánh giá khả năng chuyển giao trực tiếp, S₊ được sử dụng để đánh giá khả năng cập nhật kiến thức, Sᵢₙ và Sₒᵤₜ được sử dụng để đánh giá việc chuyển giao đến các phân phối input và output tương tự tương ứng, Sₚₗ được sử dụng để đánh giá tính dẻo dai, Sₗₒₙg bao gồm 100 nhiệm vụ và được sử dụng để đánh giá khả năng mở rộng. Tương tự, Mixed CIFAR100 và F-CelebA [15] bao gồm 10 nhiệm vụ tương tự được trộn lẫn từ F-CelebA và 10 nhiệm vụ không tương tự từ CIFAR100 [17].

Hơn nữa, chúng tôi áp dụng các benchmark task incremental learning cổ điển: CIFAR10-5, CIFAR100-10, CIFAR100-20 và MiniImageNet-20. CIFAR10-5 được xây dựng bằng cách chia CIFAR10 [17] thành 5 nhiệm vụ và mỗi nhiệm vụ có 2 lớp. Tương tự, CIFAR100-10 và CIFAR100-20 được xây dựng bằng cách chia CIFAR100 [17] thành 10 nhiệm vụ 10-classification và 20 nhiệm vụ 5-classification tương ứng. MiniImageNet-20 được xây dựng bằng cách chia MiniImageNet [43] thành 20 nhiệm vụ và mỗi nhiệm vụ có 5 lớp. Chúng tôi để lại kết quả trên CIFAR10-5 và CIFAR100-20 trong phần bổ sung.

**Baselines** Trước tiên, chúng tôi so sánh phương pháp của chúng tôi với hai baseline đơn giản là Finetune và Independent. Trong khi Finetune học các nhiệm vụ từng cái một mà không có bất kỳ ràng buộc nào, Independent xây dựng một mô hình cho mỗi nhiệm vụ độc lập. Sau đó, chúng tôi trình bày các phương pháp regularization tham số, bao gồm EWC [16], LwF [21], và MAS [1], và các phương pháp replay, bao gồm ER [6], GPM [35], và A-GEM [5]. Đồng thời, chúng tôi so sánh các phương pháp phân bổ tham số với mô hình tĩnh, bao gồm HAT [36] và CAT [15], và các phương pháp phân bổ tham số với mô hình động, bao gồm PN [34], Learn to Grow [20], SG-F [24], MNTDP [42], LMC [29] với PAR.

**Metrics** Ký hiệu hiệu suất của mô hình trên Tⱼ sau khi học nhiệm vụ Tᵢ là rᵢ,ⱼ trong đó j ≤ i. Giả sử nhiệm vụ hiện tại là Tₜ, hiệu suất trung bình (AP) và việc quên trung bình (AF) như sau:

AP = (1/t) ∑ⱼ₌₁ᵗ rₜ,ⱼ, AF = (1/t) ∑ⱼ₌₁ᵗ rₜ,ⱼ - rⱼ,ⱼ. (11)

Để đánh giá chi phí tham số, chúng tôi ký hiệu tổng số của mô hình là M.

**Chi tiết Triển khai** Chúng tôi triển khai PAR bằng PyTorch và mở source code². Chúng tôi đặt số lượng cell của mỗi chuyên gia trong PAR là 4, và đặt α là 0.5 và β là 0.75 cho tất cả các nhiệm vụ. Chúng tôi áp dụng optimizer SGD có learning rate ban đầu là 0.01 và anneals theo lịch trình cosine. Chúng tôi cũng đặt momentum của SGD là 0.9 và tìm kiếm weight decay từ [0.0003,0.001,0.003,0.01] theo hiệu suất validation. Kết quả được tính trung bình trên ba lần chạy với các random seed khác nhau.

Không có hướng dẫn đặc biệt, theo MNTDP [42], chúng tôi áp dụng phiên bản nhẹ của ResNet18 [13] với nhiều output head làm backbone cho các baseline. Đối với hiệu suất trên CTrL, chúng tôi báo cáo kết quả từ MNTDP [42] và LMC [29]. Đối với hiệu suất trên mixed CIFAR100 và F-CelebA, chúng tôi báo cáo kết quả từ CAT [15]. Đối với hiệu suất trên các benchmark cổ điển, chúng tôi: báo cáo kết quả của RPSnet [31], InstAParam [7], và BNS [30] từ các bài báo gốc; áp dụng triển khai của [4] cho các phương pháp dựa trên bộ nhớ; và áp dụng triển khai của chúng tôi cho những phương pháp khác.

### 4.2. So sánh với baselines

Trước tiên, chúng tôi đánh giá PAR trên các benchmark với các nhiệm vụ tương tự và không tương tự được trộn lẫn. Như được hiển thị trong Bảng 1, PAR vượt trội hơn các baseline trên tất cả sáu stream trong CTrL. Một lý do là PAR cho phép chuyển giao kiến thức giữa các nhiệm vụ liên quan trong khi ngăn chặn sự can thiệp giữa các nhiệm vụ không liên quan. Ví dụ, hiệu suất trên stream S₊ cho thấy PAR có thể cập nhật kiến thức của chuyên gia cho nhiệm vụ tₜ₋₁ bằng nhiệm vụ tₜ₊₁ với cùng phân phối và nhiều mẫu hơn. Hiệu suất trên các stream S₋ và Sₒᵤₜ cho thấy phương pháp của chúng tôi có thể chuyển giao kiến thức từ các chuyên gia của các nhiệm vụ liên quan đến nhiệm vụ mới. Một lý do khác là kiến trúc tailored cho nhiệm vụ giúp mỗi chuyên gia hoạt động tốt hơn. Hơn nữa, hiệu suất trên stream Sₗₒₙg cho thấy PAR có khả năng mở rộng cho chuỗi nhiệm vụ dài. Tương tự, kết quả trong Bảng 2 cho thấy PAR vượt trội hơn các phương pháp phân bổ tham số với mô hình tĩnh.

Hơn nữa, chúng tôi đánh giá PAR trên các benchmark cổ điển. Kết quả trong Bảng 3 cho thấy PAR vượt trội hơn các phương pháp regularization tham số, các phương pháp dựa trên bộ nhớ, và các phương pháp phân bổ tham số với mô hình động trên CIFAR100-10 và MiniImageNet-20.

Cuối cùng, chúng tôi phân tích chi phí tham số³ của PAR trên CIFAR100-10 và Sₗₒₙg. So với các baseline, PAR có được hiệu suất trung bình tốt hơn với ít tham số mô hình hơn. Lý do là các chuyên gia được phân bổ trong PAR có kiến trúc compact và hiệu quả về tham số. Hiệu suất trên stream Sₗₒₙg cho thấy PAR có khả năng mở rộng.

### 4.3. Nghiên cứu Ablation

Chúng tôi phân tích ảnh hưởng của các thành phần trong PAR và kết quả được liệt kê trong Bảng 5. Trước tiên, chúng tôi đánh giá tác động của tìm kiếm kiến trúc phân cấp đến phân bổ tham số. So với việc sử dụng một cell cố định từ DARTs [22] (#1), việc tìm kiếm cell từ không gian fine-grained có thể cải thiện hiệu suất mô hình (#2). Kết hợp với không gian coarse-grained, việc tìm kiếm từ không gian phân cấp có thể cải thiện hiệu suất hơn nữa trong khi giảm chi phí thời gian và tham số (#3). Sau đó, chúng tôi thấy rằng chỉ regularization tham số có hiệu quả về thời gian, nhưng hiệu suất của nó thấp (#4). Cuối cùng, bằng cách lựa chọn một chiến lược phù hợp từ phân bổ tham số và regularization dựa trên độ khó học của mỗi nhiệm vụ mới, PAR tạo ra sự cân bằng giữa hiệu suất trung bình, chi phí tham số và chi phí thời gian.

Chúng tôi phân tích tác động của các siêu tham số quan trọng α và β trong PAR trên CIFAR100-10. α xác định việc tái sử dụng chuyên gia trong quá trình học và α lớn hơn khuyến khích mô hình tái sử dụng chuyên gia. Kết quả trong Hình 3 cho thấy kích thước mô hình và số lượng chuyên gia giảm theo sự tăng của α. β xác định tần suất của tìm kiếm kiến trúc và β lớn hơn khuyến khích mô hình tái sử dụng kiến trúc của các chuyên gia hiện có. Kết quả trong Hình 4 cho thấy thời gian học giảm theo sự tăng của β.

### 4.4. Visualization

Để phân tích khoảng cách nhiệm vụ trong PAR, chúng tôi xây dựng một benchmark mới CIFAR100-coarse bằng cách chia CIFAR100 thành 20 nhiệm vụ dựa trên các nhãn coarse-grained. Minh họa trong Hình 5 cho thấy PAR có thể có được các nhóm nhiệm vụ hợp lý. Ví dụ, PAR thích ứng đặt các nhiệm vụ về động vật vào cùng nhóm, như Động vật có vú thủy sinh, Động vật ăn tạp và ăn cỏ lớn, Cá, v.v. Các hộp đựng thực phẩm và Thiết bị điện gia dụng được chia vào cùng nhóm vì cả hai đều chứa các vật thể hình trụ và hình tròn. Nó cũng tìm thấy mối quan hệ giữa Động vật không xương sống không phải côn trùng và Côn trùng.

Hơn nữa, chúng tôi visualize các heat map của khoảng cách nhiệm vụ thu được bởi PAR trên bốn stream của CTrL trong Hình 6. Khoảng cách giữa nhiệm vụ đầu tiên và cuối cùng trên các stream S₋, S₊ và Sₒᵤₜ nhỏ, phù hợp với thực tế là chúng đều đến từ cùng một phân phối dữ liệu. Thay vào đó, phân phối của nhiệm vụ cuối cùng trên stream Sᵢₙ bị nhiễu từ phân phối của nhiệm vụ đầu tiên, vì vậy khoảng cách của chúng lớn. Các kết quả trên cho thấy PAR có thể có được khoảng cách và mức độ liên quan nhiệm vụ hợp lý.

## 5. Kết luận

Trong bài báo này, chúng tôi đề xuất một phương pháp mới có tên là Phân Bổ Tham Số & Regularization (PAR). Nó cho phép mô hình lựa chọn thích ứng một chiến lược phù hợp từ phân bổ tham số và regularization cho mỗi nhiệm vụ dựa trên độ khó học nhiệm vụ. Kết quả thực nghiệm trên nhiều benchmark cho thấy PAR có khả năng mở rộng và giảm đáng kể sự dư thừa mô hình trong khi cải thiện hiệu suất. Hơn nữa, phân tích định tính cho thấy PAR có thể có được mức độ liên quan nhiệm vụ hợp lý. Phương pháp của chúng tôi linh hoạt, và trong tương lai, chúng tôi sẽ giới thiệu thêm các chiến lược ước lượng mức độ liên quan, regularization và phân bổ vào PAR.

## Lời cảm ơn

Công việc này được hỗ trợ bởi các dự án NSFC (Số 62072399, Số U19B2042), Trung tâm Tri thức Trung Quốc về Khoa học và Công nghệ Kỹ thuật, Trung tâm Nghiên cứu Kỹ thuật Thư viện Số của Bộ Giáo dục, Trung tâm Nghiên cứu Trung Quốc về Dữ liệu và Tri thức cho Khoa học và Công nghệ Kỹ thuật, và Quỹ Nghiên cứu Cơ bản cho các Trường đại học Trung ương.
