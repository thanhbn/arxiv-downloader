# Tăng tốc học tích cực theo batch bằng kỹ thuật học liên tục

Arnav Das∗†Gantavya Bhatt∗†Megh Bhalerao†Vianne Gao⋄Rui Yang⋄Jeff Bilmes†
†Đại học Washington, Seattle⋄Trung tâm Ung thư Memorial Sloan Kettering
{arnavmd2, gbhatt2, bilmes}@uw.edu

Tóm tắt
Một vấn đề lớn với Học tích cực (AL) là chi phí huấn luyện cao vì các mô hình thường được huấn luyện lại từ đầu sau mỗi vòng truy vấn. Chúng tôi bắt đầu bằng việc chứng minh rằng AL tiêu chuẩn trên mạng nơ-ron với khởi động ấm thất bại, cả trong việc tăng tốc huấn luyện và tránh quên thảm khốc khi sử dụng tinh chỉnh qua các vòng truy vấn AL. Sau đó chúng tôi phát triển một lớp kỹ thuật mới, khắc phục vấn đề này, bằng cách thiên lệch việc huấn luyện tiếp theo về phía các tập được gán nhãn trước đó. Chúng tôi thực hiện điều này bằng cách sử dụng các thuật toán Học liên tục (CL) dựa trên replay hiện có và phát triển các thuật toán mới, có hiệu quả trong việc học nhanh những điều mới mà không quên những điều cũ, đặc biệt khi dữ liệu đến từ một phân phối đang phát triển. Chúng tôi gọi mô hình này là "Học tích cực liên tục" (CAL). Chúng tôi cho thấy CAL đạt được sự tăng tốc đáng kể bằng cách sử dụng một loạt các sơ đồ replay sử dụng chưng cất mô hình và chọn các điểm đa dạng/không chắc chắn từ lịch sử. Chúng tôi tiến hành thí nghiệm trên nhiều miền dữ liệu, bao gồm ngôn ngữ tự nhiên, thị giác, hình ảnh y tế và sinh học tính toán, mỗi miền có các kiến trúc mạng nơ-ron và kích thước tập dữ liệu khác nhau. CAL liên tục cung cấp mức giảm ∼3x về thời gian huấn luyện, trong khi vẫn duy trì hiệu suất và độ bền vững ngoài phân phối, cho thấy khả năng ứng dụng rộng rãi của nó.

1 Giới thiệu
Trong khi mạng nơ-ron đã thành công trong nhiều bối cảnh học có giám sát khác nhau, hầu hết các phương pháp này đều đói dữ liệu có nhãn và đòi hỏi tính toán đáng kể. Từ một nhóm lớn dữ liệu không nhãn, học tích cực (AL) chọn các tập con điểm để gán nhãn bằng cách trao cho người học khả năng truy vấn một người chú thích. Các phương pháp như vậy tăng dần điểm vào nhóm có nhãn bằng cách lặp lại: (1) huấn luyện mô hình từ đầu trên nhóm có nhãn hiện tại và (2) sử dụng một số thước đo độ không chắc chắn và/hoặc tính đa dạng của mô hình để chọn một tập điểm để truy vấn người chú thích (Settles, 2009; 2011; Wei et al., 2015; Ash et al., 2020; Killamsetty et al., 2021a). AL đã được chứng minh là giảm lượng dữ liệu huấn luyện cần thiết nhưng có thể tốn kém về mặt tính toán vì nó đòi hỏi huấn luyện lại mô hình, thường từ đầu, sau mỗi vòng truy vấn.

Một giải pháp đơn giản là khởi động ấm các tham số mô hình giữa các vòng truy vấn. Tuy nhiên, sự tăng tốc quan sát được thường vẫn bị hạn chế vì mô hình phải thực hiện nhiều lần qua một nhóm dữ liệu ngày càng tăng. Hơn nữa, chỉ khởi động ấm thôi trong một số trường hợp có thể làm tổn hại khả năng tổng quát hóa, như được thảo luận trong Ash & Adams (2020) và Beck et al. (2021). Một mở rộng khác cho điều này là chỉ huấn luyện trên batch mẫu mới được gán nhãn để tránh khởi tạo lại. Tuy nhiên, như chúng tôi cho thấy trong Phần 4, việc tinh chỉnh ngây thơ không thể duy trì độ chính xác trên các mẫu đã thấy trước đó vì phân phối của nhóm truy vấn có thể thay đổi mạnh mẽ với mỗi vòng.

Vấn đề quên thảm khốc này trong khi học tăng dần từ một loạt các nhiệm vụ mới với các phân phối thay đổi là một câu hỏi trung tâm trong một mô hình khác gọi là Học liên tục (CL) (French, 1999; McCloskey & Cohen, 1989; McClelland et al., 1995; Kirkpatrick et al., 2017c). CL gần đây đã trở nên phổ biến, và nhiều thuật toán đã được giới thiệu để cho phép các mô hình thích ứng nhanh chóng với các nhiệm vụ mới mà không quên (Riemer et al., 2018; Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019; Aljundi et al., 2019b; Chaudhry et al., 2020; Kirkpatrick et al., 2017b).

Trong công trình này, chúng tôi đề xuất Học tích cực liên tục (CAL), áp dụng các chiến lược Học liên tục để tăng tốc Học tích cực theo batch. Trong CAL, chúng tôi áp dụng CL để cho phép mô hình học các điểm mới được gán nhãn mà không quên các điểm được gán nhãn trước đó trong khi sử dụng hiệu quả các mẫu trong quá khứ bằng các phương pháp dựa trên replay. Do đó, chúng tôi quan sát thấy CAL đạt được sự tăng tốc thời gian huấn luyện đáng kể so với AL tiêu chuẩn. Điều này có lợi vì những lý do sau: (1): Khi mạng nơ-ron phình to (Shoeybi et al., 2019), chi phí huấn luyện mô hình về môi trường và tài chính cũng tăng theo (Bender et al., 2021; Dhar, 2020; Schwartz et al., 2020). Giảm số lượng cập nhật gradient cần thiết cho AL sẽ giúp giảm thiểu những chi phí như vậy, đặc biệt với các mô hình quy mô lớn. (2): Giảm yêu cầu tính toán AL làm cho AL dễ tiếp cận hơn cho điện toán biên, IoT và triển khai thiết bị tài nguyên thấp (Senzaki & Hamelain, 2021) như với học liên kết (Li et al., 2020). (3): Phát triển các thuật toán AL/hàm thu thập mới, hoặc tìm kiếm kiến trúc như được thực hiện với NAS/AutoML phù hợp đặc biệt cho AL, có thể đòi hỏi hàng trăm hoặc thậm chí hàng nghìn lần chạy. Vì sự tăng tốc của CAL không phụ thuộc vào thuật toán AL và kiến trúc mạng nơ-ron, những thí nghiệm như vậy có thể được tăng tốc đáng kể. Nhìn chung, tầm quan trọng của việc tăng tốc các quy trình huấn luyện học máy được công nhận rộng rãi, như được chứng minh bởi rất nhiều nỗ lực trong cộng đồng hệ thống tính toán (Jia et al., 2022; Zhang et al., 2017; Zheng et al., 2022).

Ngoài ra, CAL chứng minh một ứng dụng thực tế khác cho các phương pháp CL. Nhiều bối cảnh được sử dụng để đánh giá các phương pháp CL trong công trình gần đây là hơi giả tạo (Farquhar & Gal, 2018; Wallingford et al., 2023). Hầu hết công trình CL xem xét bối cảnh tăng dần lớp/miền, nơi chỉ các mẫu thuộc về một tập con của tập các lớp/miền của tập dữ liệu gốc có sẵn cho mô hình tại bất kỳ thời điểm nào. Bối cảnh này không cần phải là điểm chuẩn duy nhất mà các phương pháp CL được đánh giá. Chúng tôi đề xuất rằng việc đánh giá các thuật toán CL mới trong tương lai nên được xác định không chỉ dựa trên các sơ đồ và điểm chuẩn đánh giá CL truyền thống mà còn dựa trên hiệu suất của chúng trong bối cảnh CAL.

Trong công trình hiện tại, chúng tôi không sử dụng CL để cải thiện hoặc thay đổi các chiến lược truy vấn AL. Chúng tôi xem điều này vừa là một điểm mạnh của công trình hiện tại và một cơ hội cho công trình tương lai. Thứ nhất, đó là một điểm mạnh của công trình hiện tại vì bất kỳ chiến lược truy vấn AL nào, cả cũ và mới, về nguyên tắc đều có thể được áp dụng trong bối cảnh CAL, cả để tăng tốc chiến lược AL và cung cấp, như đã lập luận ở trên, một bãi thử nghiệm cho CL. Thật vậy, một thách thức nghiên cứu AL lớn bao gồm việc kết hợp AL với các kỹ thuật khác, như chúng tôi tin rằng chúng tôi đã làm ở đây. Thứ hai, đó là một cơ hội vì không có gì vốn có trong định nghĩa CL ngăn cản các nhiệm vụ CL phụ thuộc vào mô hình như chúng tôi cho thấy bên dưới. Có thể có một cách để CL mở ra cánh cửa cho các chính sách truy vấn AL mới về bản chất của việc đạt được các batch dữ liệu không nhãn mới. Điều này chúng tôi để lại cho tương lai.

Mục tiêu cốt lõi hiện tại của chúng tôi là tăng tốc huấn luyện AL batch thông qua các kỹ thuật CL trong khi duy trì độ chính xác. Theo hiểu biết của chúng tôi, ứng dụng các thuật toán CL để tăng tốc AL batch này chưa bao giờ được khám phá. Đóng góp của chúng tôi có thể được tóm tắt như sau: (1) Chúng tôi trước tiên chứng minh rằng các kỹ thuật học tích cực batch có thể hưởng lợi từ các kỹ thuật học liên tục, và sự hợp nhất của chúng tạo ra một lớp kỹ thuật mới mà chúng tôi đề xuất gọi là "khung CAL." (2) Chúng tôi đánh giá một số phương pháp CL hiện có (CAL-ER, CAL-DER, CAL-MIR) cũng như các phương pháp mới (CAL-SD, CAL-SDS2) và đánh giá chúng trên các tập dữ liệu đa dạng dựa trên độ chính xác/tăng tốc mà chúng có thể đạt được so với AL tiêu chuẩn. (3) Chúng tôi nghiên cứu các đánh đổi tăng tốc/hiệu suất trên các tập dữ liệu thay đổi về phương thức (ngôn ngữ tự nhiên, thị giác, hình ảnh y tế và sinh học tính toán), kiến trúc mạng nơ-ron với các mức độ tính toán khác nhau (Transformers/CNNs/MLPs), quy mô dữ liệu (bao gồm một số tập dữ liệu lớn hơn, một tập có 2M mẫu), và cân bằng lớp. Và (4), cuối cùng, chúng tôi chứng minh rằng các mô hình được huấn luyện với CAL và các mô hình AL tiêu chuẩn hoạt động tương tự, ở chỗ cả hai lớp mô hình đều đạt được điểm không chắc chắn tương tự trên các tập dữ liệu được giữ lại và đạt được hiệu suất bền vững tương tự trên dữ liệu ngoài phân phối. Hình 1 tóm tắt kết quả của chúng tôi, được chi tiết sau này trong bài báo và được chi tiết rất nhiều trong các phụ lục.

2 Công trình liên quan

Học tích cực (Atlas et al., 1989; Cohn et al., 1994; Wei et al., 2015; Killamsetty et al., 2021a; Ash et al., 2020) đã chứng minh hiệu quả nhãn so với học thụ động. Ngoài ra, đã có công trình mở rộng về các khía cạnh lý thuyết của AL (Guillory et al., 2009; Hanneke, 2009; 2007; Balcan et al., 2010) nơi Hanneke (2012) cho thấy lợi thế độ phức tạp mẫu so với học thụ động trong học phân loại không nhiễu cho các lớp VC. Gần đây hơn, Học tích cực cũng đã được nghiên cứu như một quy trình để học tăng dần phân phối dữ liệu cơ bản với sự giúp đỡ của khung sự khác biệt (Mathelin et al., 2022; Cui & Sato, 2020; Shui et al., 2019).

Gần đây đã có sự quan tâm đến việc tăng tốc học tích cực vì hầu hết học sâu đều đòi hỏi tính toán cao. Kirsch et al. (2019); Pinsler et al. (2019); Sener & Savarese (2018) nhằm giảm số lần lặp truy vấn bằng cách có kích thước batch truy vấn lớn. Tuy nhiên, họ không khai thác các mô hình đã học từ các vòng trước cho các vòng tiếp theo và do đó bổ sung cho CAL. Công trình như Coleman et al. (2020a); Ertekin et al. (2007); Mayer & Timofte (2020); Zhu & Bento (2017); Zhang et al. (2023) tăng tốc việc chọn tập truy vấn mới bằng cách hạn chế không gian tìm kiếm một cách thích hợp hoặc bằng cách sử dụng các phương pháp sinh. Công trình này có thể dễ dàng tích hợp vào khung của chúng tôi vì CAL hoạt động trên phía huấn luyện của học tích cực, không phải trên việc chọn truy vấn. Mặt khác, Lewis & Catlett (1994); Coleman et al. (2020b); Yoo & Kweon (2019) sử dụng mô hình proxy nhỏ hơn để giảm chi phí tính toán, tuy nhiên, họ vẫn tuân theo giao thức học tích cực tiêu chuẩn, và do đó có thể được tăng tốc khi tích hợp với CAL.

Tồn tại công trình khám phá học liên tục/chuyển giao và học tích cực trong cùng một bối cảnh. Perkonigg et al. (2021) đề xuất một phương pháp cho phép học tích cực được áp dụng cho các luồng dữ liệu hình ảnh y tế bằng cách giới thiệu một mô-đun phát hiện sự dịch chuyển miền. Điều này khá khác biệt so với công trình của chúng tôi: công trình của chúng tôi sử dụng các thuật toán CL để ngăn chặn quên thảm khốc và tăng tốc học. Zhou et al. (2021) nghiên cứu khi học tích cực tiêu chuẩn được sử dụng để tinh chỉnh mô hình đã được huấn luyện trước, và sử dụng học chuyển giao — tuy nhiên điều này không xem xét học liên tục và học tích cực cùng nhau, và do đó không liên quan đến công trình của chúng tôi. Cuối cùng, Ayub & Fendley (2022) nghiên cứu nơi một robot quan sát dữ liệu không nhãn được lấy mẫu từ một phân phối thay đổi, nhưng không khám phá tăng tốc học tích cực.

Để ngăn chặn quên thảm khốc, chúng tôi chủ yếu tập trung vào các thuật toán dựa trên replay là các phương pháp tiên tiến trong CL. Tuy nhiên, như được chứng minh trong Phần 4 về cách các vòng học tích cực có thể được xem trong bối cảnh học liên tục, người ta có thể áp dụng các phương pháp khác như EWC (Kirkpatrick et al., 2017a), tiên nghiệm phân kỳ Bayesian Li & Bilmes (2007), điều hòa cấu trúc (Li et al., 2021) hoặc điều hòa chức năng (Titsias et al., 2020) cũng vậy.

Ảnh hưởng của huấn luyện mô hình khởi động ấm đến khả năng tổng quát hóa và tốc độ hội tụ đã được khám phá bởi Ash & Adams (2020) chứng minh thực nghiệm rằng một mô hình đã được huấn luyện trước trên tập dữ liệu nguồn hội tụ nhanh hơn nhưng thể hiện khả năng tổng quát hóa tệ hơn trên tập dữ liệu đích khi so sánh với mô hình được khởi tạo ngẫu nhiên. Tuy nhiên, công trình đó chỉ xem xét bối cảnh nơi các tập dữ liệu nguồn và đích là các ước lượng không thiên lệch của cùng một phân phối. Điều này khác biệt so với công trình của chúng tôi vì các phân phối mà chúng tôi xem xét đều phụ thuộc vào mô hình tại mỗi vòng AL. Hơn nữa, công trình của chúng tôi sử dụng các phương pháp CL ngoài khởi động ấm, cũng không được xem xét trong Ash & Adams (2020).

3 Nền tảng

3.1 Học tích cực theo batch

Định nghĩa [n] = {1,...,n}, và cho X và Y biểu thị các miền đầu vào và đầu ra tương ứng. AL thường bắt đầu với một tập dữ liệu không nhãn U = {xi}i∈[n], nơi mỗi xi ∈ X. Bối cảnh AL cho phép mô hình f, với tham số θ, truy vấn người dùng cho nhãn cho bất kỳ x ∈ U nào, nhưng tổng số nhãn bị giới hạn ở ngân sách b, nơi b ≤ n. Trong suốt công trình, chúng tôi xem xét các nhiệm vụ phân loại nên đầu ra của f(x;θ) là một phân phối xác suất trên các lớp. Mục tiêu của AL là đảm bảo rằng f có thể đạt được lỗi thấp khi được huấn luyện chỉ trên tập b điểm có nhãn.

Thuật toán 1 chi tiết quy trình AL chung. Dòng 3-6 xây dựng tập hạt giống D1 bằng cách lấy mẫu ngẫu nhiên một tập con điểm từ U và gán nhãn cho chúng. Dòng 7-14 mở rộng lặp đi lặp lại tập có nhãn trong T vòng bằng cách huấn luyện mô hình từ khởi tạo ngẫu nhiên trên Dt cho đến khi hội tụ và chọn bt điểm (nơi ∑t∈[T] bt = b) từ U dựa trên một số tiêu chí chọn phụ thuộc vào θt. Tiêu chí chọn thường chọn mẫu dựa trên độ không chắc chắn và/hoặc tính đa dạng của mô hình (Lewis & Gale, 1994; Dagan & Engelson, 1995; Settles; Killamsetty et al., 2021a; Wei et al., 2015; Ash et al., 2020; Sener & Savarese, 2017). Trong công trình này, chúng tôi chủ yếu xem xét lấy mẫu độ không chắc chắn Lewis & Gale (1994); Dagan & Engelson (1995); Settles, mặc dù chúng tôi cũng thử nghiệm các tiêu chí chọn khác trong Phần C trong Phụ lục.

Lấy mẫu độ không chắc chắn là một phương pháp AL thực tế được sử dụng rộng rãi chọn Ut = {x1,...,xbt} ⊆ U bằng cách chọn các mẫu tối đa hóa một khái niệm về độ không chắc chắn của mô hình. Chúng tôi xem xét entropy (Dagan & Engelson, 1995) như thước đo độ không chắc chắn, vậy nếu hθ(x) ≜ -∑i∈[k] f(x;θ)i log f(x;θ)i, thì Ut+1 ∈ argmax A⊂U:|A|=bt ∑x∈A hθt(x).

Dịch chuyển phân phối Như được hiển thị trong Thuật toán 1, tập các mẫu được gán nhãn tại một vòng truy vấn nhất định phụ thuộc vào các tham số mô hình. Vì mô hình được cập nhật liên tục, phân phối của các mẫu mới được gán nhãn có thể thay đổi giữa các vòng AL khác nhau. Điều này được minh họa trong Hình 2, nơi chúng tôi thực hiện AL tiêu chuẩn trong một bối cảnh đơn giản hóa. Trong ví dụ, chúng tôi sử dụng mô hình tuyến tính để thực hiện phân loại trên tập dữ liệu tổng hợp 2D và thực hiện AL với lấy mẫu độ không chắc chắn dựa trên entropy như hàm thu thập. Khi trực quan hóa tập các mẫu được truy vấn bởi mô hình tại các vòng AL khác nhau, rõ ràng là sự dịch chuyển phân phối xảy ra. Chúng tôi chứng minh thực nghiệm trong Hình 3 rằng huấn luyện chỉ trên các mẫu mới được gán nhãn sẽ gây ra quên thảm khốc do sự dịch chuyển phân phối.

3.2 Học liên tục

Chúng tôi định nghĩa D1:n = ⋃i∈[n] Di. Trong CL, tập dữ liệu bao gồm T nhiệm vụ {D1,...,DT} được trình bày cho mô hình tuần tự, nơi Dt = {(xi,yi)}i∈Nt, Nt là các chỉ số mẫu của nhiệm vụ-t, và nt = |Nt|. Tại thời điểm t ∈ [T], các cặp dữ liệu/nhãn được lấy mẫu từ nhiệm vụ hiện tại (x,y) ∼ Dt, và mô hình chỉ có quyền truy cập hạn chế vào lịch sử D1:t-1. Mục tiêu CL là thích ứng hiệu quả mô hình với Dt trong khi đảm bảo hiệu suất trên lịch sử không suy giảm đáng kể. Chúng tôi tập trung vào các kỹ thuật CL dựa trên replay cố gắng giải quyết gần đúng tối ưu hóa CL bằng cách sử dụng các mẫu từ D1:t-1 để điều hòa mô hình trong khi thích ứng với Dt. Vui lòng tham khảo phụ lục B để biết thêm chi tiết về CL.

Các bài báo Học liên tục khác nhau có giả định truy cập lịch sử khác nhau. Trong nhiều bối cảnh thực tế, D1:T quá lớn để lưu trữ trong bộ nhớ hoặc T không được biết nên các thuật toán CL giả định quyền truy cập hạn chế hoặc không có quyền truy cập vào các mẫu từ lịch sử. Một số phương pháp này loại bỏ các mẫu không quan trọng khỏi lịch sử Aljundi et al. (2019b); Mai et al. (2020), trong khi những phương pháp khác chỉ dựa vào điều hòa các tham số mô hình mà không replay bất kỳ mẫu nào từ lịch sử Kirkpatrick et al. (2017b); Li & Hoiem (2017). Trong bối cảnh CAL, D1:T đã được lưu trữ trong bộ nhớ như được thực hiện trong bối cảnh AL tiêu chuẩn nên việc sử dụng thuật toán CL áp đặt ràng buộc bộ nhớ sẽ không cần thiết khiến CAL hoạt động kém. Trong Bảng 1, một số công trình CL trước được sắp xếp theo những giả định về truy cập bộ nhớ/lịch sử mà chúng đưa ra; các phương pháp không đưa ra bất kỳ giả định ràng buộc bộ nhớ nào là phù hợp nhất cho CAL. Một tổng quan toàn diện và đầy đủ hơn về các phương pháp CL có thể được tìm thấy trong De Lange et al. (2022).

4 Kết hợp Học liên tục và Học tích cực

Một điểm không hiệu quả rõ ràng của AL là mô hình f được huấn luyện lại từ đầu trên toàn bộ nhóm có nhãn sau mỗi vòng truy vấn. Một ý tưởng giải pháp tiềm năng là đơn giản chỉ tiếp tục huấn luyện mô hình chỉ trên các mẫu AL-truy vấn mới và, thông qua quá trình khởi động ấm, hy vọng rằng lịch sử sẽ không phai mờ. Thật không may cho phương pháp này, Hình 3 (trên) cho thấy rằng khi mô hình được huấn luyện khởi động ấm chỉ trên nhiệm vụ t (các mẫu được gán nhãn tại vòng AL t sử dụng lấy mẫu entropy), hiệu suất mẫu lịch sử giảm mạnh trong khi hiệu suất trên tập validation bằng phẳng. Có nghĩa là, tại vòng AL t (trục x), chúng tôi tiếp tục huấn luyện mô hình cho đến khi hội tụ trên nhiệm vụ t và theo dõi độ chính xác (trục y) trên mỗi nhiệm vụ trước đó và cũng trên tập validation. Hiệu suất của nhiệm vụ 1, sau khi giảm ban đầu, theo dõi hiệu suất của tập validation, vì nhiệm vụ 1 là một tập con truy vấn ngẫu nhiên không thiên lệch độc lập với mô hình của dữ liệu huấn luyện. Tuy nhiên, hiệu suất của nhiệm vụ i, cho i > 1, mỗi nhiệm vụ là kết quả của truy vấn AL có điều kiện mô hình, cho thấy việc quên lịch sử nguy hiểm. Cuối cùng, mô hình hoạt động tệ hơn đáng kể trên tất cả các nhiệm vụ lịch sử (ngoài nhiệm vụ 1) so với trên tập validation, mặc dù nó đã được huấn luyện trên những nhiệm vụ đó và không được huấn luyện trên tập validation. Thí nghiệm này cho thấy rằng: (1) phân phối của mỗi nhiệm vụ AL-truy vấn t > 1 khác với phân phối dữ liệu; (2) tinh chỉnh cho nhiệm vụ t có thể dẫn đến quên thảm khốc; và (3) các kỹ thuật để chống lại quên thảm khốc là cần thiết để hiệu quả kết hợp thông tin mới giữa các vòng AL liên tiếp.

Phương pháp CAL, được hiển thị trong Thuật toán 3, sử dụng các kỹ thuật CL để giảm thiểu quên thảm khốc. Sự khác biệt chính giữa CAL và Thuật toán 1 là dòng 9. Thay vì huấn luyện tiêu chuẩn, CL dựa trên replay được sử dụng để thích ứng f với Dt trong khi duy trì hiệu suất trên D1:t-1. Sự tăng tốc đến từ hai nguồn: (1) chúng tôi đang tính toán các cập nhật gradient chỉ trên một tập con hữu ích của lịch sử D1:t-1 thay vì tất cả cho các lựa chọn hợp lý của m(h); và (2) mô hình hội tụ nhanh hơn vì nó bắt đầu ấm. Khả năng của CAL để chống lại quên thảm khốc được hiển thị trong Hình 3 (dưới). Trong phần còn lại của phần này, định nghĩa Lc(θ) ≜ E(x,y)∼Bcurrent [ℓ(y,f(x;θ))]. Chúng tôi tiếp theo định nghĩa và so sánh một số phương pháp CAL và đánh giá hiệu suất của chúng dựa trên hiệu suất của chúng trên tập test và sự tăng tốc mà chúng đạt được so với AL tiêu chuẩn.

Experience Replay (CAL-ER) là phương pháp dựa trên replay đơn giản và lâu đời nhất (Ratcliff, 1990; Robins, 1995). Trong phương pháp này, Bcurrent và Breplay được xen kẽ để tạo một minibatch B có kích thước m + m(h) và Breplay được chọn ngẫu nhiên đều từ D1:t-1. Các tham số θ của mô hình f được cập nhật dựa trên gradient được tính trên B.

Maximally Interfered Retrieval (CAL-MIR) chọn một tập con kích thước-m(h) của các điểm từ D1:t-1 có khả năng bị quên nhất (Aljundi et al., 2019a). Cho một batch m mẫu có nhãn Bcurrent được lấy mẫu từ Dt và tham số mô hình θ, θv được tính bằng cách thực hiện một bước gradient "ảo" tức là, θv = θ - η∇Lc(θ) nơi η là tốc độ học. Sau đó đối với mỗi ví dụ x trong lịch sử, sMIR(x) = ℓ(f(x;θ),y) - ℓ(f(x;θv),y) (tức là, sự thay đổi trong loss sau khi thực hiện một bước gradient duy nhất) được tính toán. Các mẫu m(h) có điểm sMIR cao nhất được chọn cho Breplay, và phần còn lại tương tự như experience replay. Bcurrent và Breplay được nối để tạo thành minibatch (như trong CAL-ER), mà cập nhật gradient được tính toán. Trong thực tế, việc chọn được thực hiện trên một tập con ngẫu nhiên của D1:t-1 để tăng tốc, vì tính toán sMIR cho mọi mẫu lịch sử quá tốn kém.

Dark Experience Replay (CAL-DER) sử dụng phương pháp chưng cất để điều hòa các cập nhật (Buzzega et al., 2020). Cho g(x;θ) biểu thị các logit trước softmax của bộ phân loại f(x;θ), tức là, f(x;θ) = softmax(g(x;θ)). Trong DER, mọi x' ∈ D1:t-1 có một z' liên kết tương ứng với logit của mô hình ở cuối nhiệm vụ khi x được quan sát lần đầu — nếu x' ∈ Dt', thì z' ≜ g(x';θ*t') nơi t' ∈ [t-1] và θ*t' là các tham số thu được sau vòng t'. DER tối thiểu hóa LDER(θ) được định nghĩa là:

LDER(θ) ≜ Lc(θ) + E(x',y',z')∼Breplay [α||g(x';θ) - z'||²₂ + βℓ(y',f(x';θ))], (1)

nơi Breplay là một batch được lấy mẫu ngẫu nhiên đều (không hoàn lại) từ D1:t-1, và α và β là các siêu tham số có thể điều chỉnh. Số hạng đầu tiên đảm bảo rằng các mẫu từ nhiệm vụ hiện tại được phân loại chính xác. Số hạng thứ hai bao gồm một loss phân loại và một loss chưng cất dựa trên mean squared error (MSE) áp dụng cho các mẫu lịch sử.

Scaled Distillation (CAL-SD) LSD(θ) là một mục tiêu mới được đề xuất trong công trình này được định nghĩa thông qua:

Lreplay(θ) ≜ E(x',y',z')∼Breplay [αDKL(softmax(z')||f(x';θ)) + (1-α)ℓ(y',f(x';θ))], (2)

và sau đó LSD(θ) ≜ λtLc(θ) + (1-λt)Lreplay(θ) nơi λt ≜ |Dt|/(|Dt| + |D1:t-1|). Tương tự như CAL-DER, Lreplay là tổng của hai số hạng: một loss chưng cất và một loss phân loại. Loss chưng cất biểu thị divergence KL giữa các xác suất posterior được tạo ra bởi f và softmax(z'), nơi z' được định nghĩa trong phần DER. Chúng tôi sử dụng divergence KL thay vì loss MSE trên logit để loss chưng cất và loss phân loại có cùng thang đo và phạm vi động, và cũng vì nó cho phép logit dịch chuyển bởi một số hạng hằng số không ảnh hưởng đến đầu ra softmax nhưng ảnh hưởng đến loss MSE. α ∈ [0,1] là một siêu tham số có thể điều chỉnh. Trọng số của mỗi số hạng được xác định thích ứng bởi một số hạng đánh đổi "ổn định/tính dẻo" λt. Một tình thế khó xử ổn định-tính dẻo thường được tìm thấy trong cả mạng nơ-ron sinh học và nhân tạo (Abraham & Robins, 2005; Mermillod et al., 2013). Một mạng ổn định nếu nó có thể duy trì hiệu quả thông tin quá khứ nhưng không thể thích ứng với các nhiệm vụ mới một cách hiệu quả, trong khi một mạng dẻo có thể học nhanh các nhiệm vụ mới nhưng dễ quên. Sự đánh đổi giữa ổn định và tính dẻo là một ràng buộc được biết đến trong CL (Mermillod et al., 2013). Đối với CAL, chúng tôi muốn mô hình dẻo sớm, và ổn định sau này. Chúng tôi áp dụng trực giác này với λt: các giá trị cao hơn chỉ ra tính dẻo cao hơn, vì việc tối thiểu hóa lỗi phân loại của các mẫu từ nhiệm vụ hiện tại được ưu tiên. Vì D1:t-1 tăng với t, λt giảm và mô hình trở nên ổn định hơn trong các vòng huấn luyện sau.

Scaled Distillation w/ Submodular Sampling (CAL-SDS2) CAL-SDS2 là một phương pháp CL mới khác mà chúng tôi giới thiệu trong công trình này. CAL-SDS2 sử dụng CAL-SD để điều hòa mô hình và sử dụng một quy trình lấy mẫu submodular để chọn một tập đại diện đa dạng của các điểm lịch sử để replay. Các hàm submodular được biết đến là phù hợp để nắm bắt các khái niệm về tính đa dạng và tính đại diện (Lin & Bilmes, 2011; Wei et al., 2015; Bilmes, 2022) và thuật toán greedy đơn giản có thể tối đa hóa gần đúng, dưới ràng buộc cardinality, một hàm submodular monotone lên đến một bảo đảm nhân thừa số hằng số 1-e⁻¹ (Fisher et al., 1978; Minoux, 1978; Mirzasoleiman et al., 2015). Chúng tôi định nghĩa hàm submodular G của chúng tôi là:

G(S) ≜ Σxi∈A max xj∈S wij + λlog(1 + Σxi∈S h(xi)). (3)

Số hạng đầu tiên là một hàm facility location, nơi wij là điểm tương đồng giữa các mẫu xi và xj. Chúng tôi sử dụng wij = exp(-||zi - zj||²/2σ²) nơi zi là lớp áp cuối của mô hình f cho xi và σ là một siêu tham số. Số hạng thứ hai là một hàm concave over modular (Liu et al., 2013) và h(xi) là một thước đo AL tiêu chuẩn về độ không chắc chắn của mô hình, như entropy của phân phối đầu ra của mô hình. Cả hai số hạng đều được biết đến là monotone non-decreasing submodular, cũng như tổng có trọng số không âm của chúng (Bilmes, 2022). Lý do cốt lõi để áp dụng một hàm concave trên một hàm modular dựa trên điểm-độ-không-chắc-chắn-mô hình, thay vì giữ nó như một hàm modular thuần túy, là để căn chỉnh tốt hơn các giá trị độ không chắc chắn modular với hàm facility location. Nếu không, nếu chúng ta không áp dụng hàm concave, hàm facility location chiếm ưu thế trong các bước đầu của thuật toán greedy và hàm modular chiếm ưu thế trong các bước sau của greedy. Để tăng tốc SDS2 (để tránh cần phải thực hiện các lần forward pass trên toàn bộ lịch sử trước một bước duy nhất), chúng tôi lấy mẫu phụ ngẫu nhiên các chỉ số lịch sử để tạo ra A và tính toán lại các lần forward pass để tạo ra các giá trị zi tươi cho i ∈ A trước khi tái xây dựng G(S), và chúng tôi sau đó thực hiện tối đa hóa submodular; do đó S ⊂ A ⊂ D1:t-1. Mục tiêu của CAL-SDS2 là đảm bảo rằng tập các mẫu được replay vừa khó vừa đa dạng, tương tự như động lực của heuristic được sử dụng trong Wei et al. (2015).

Baseline Tất cả các phương pháp CAL được đề xuất được so sánh với hai baseline AL. Baseline đầu tiên là học tích cực tiêu chuẩn, được ký hiệu là AL, không khác gì quy trình được hiển thị trong Thuật toán 1. Chúng tôi cũng xem xét học tích cực với khởi động ấm (AL w/ WS), sử dụng mô hình hội tụ từ vòng trước để khởi tạo mô hình cho vòng hiện tại. Cả hai mô hình đều huấn luyện lại trên toàn bộ tập dữ liệu tại mỗi vòng truy vấn.

5 Thí nghiệm và Kết quả

Chúng tôi đánh giá hiệu suất validation của mô hình khi chúng tôi huấn luyện trên các phần khác nhau (b/n) của tập dữ liệu đầy đủ. Chúng tôi tính toán sự tăng tốc đạt được bởi một phương pháp CAL bằng cách chia thời gian huấn luyện wall-clock của phương pháp AL baseline cho thời gian huấn luyện wall-clock của phương pháp CAL. Chúng tôi thử nghiệm các phương pháp CAL trên nhiều tập dữ liệu khác nhau trải dài trên nhiều phương thức. Hai baseline không sử dụng CAL là AL tiêu chuẩn (học tích cực) cũng như AL w/ WS (học tích cực với khởi động ấm nhưng vẫn huấn luyện sử dụng tất cả dữ liệu có nhãn hiện có).

Mục tiêu của chúng tôi là chứng minh: (1) ít nhất một phương pháp dựa trên CAL tồn tại có thể bằng hoặc vượt trội hơn kỹ thuật học tích cực tiêu chuẩn trong khi đạt được sự tăng tốc đáng kể cho mọi ngân sách và tập dữ liệu và (2) các mô hình đã được huấn luyện sử dụng phương pháp dựa trên CAL hoạt động không khác gì các mô hình tiêu chuẩn. Chúng tôi nhấn mạnh rằng mục đích của công trình này không phải để ủng hộ một phương pháp duy nhất, mà là để trưng bày một loạt các phương pháp trong mô hình CAL đạt được các đánh đổi hiệu suất/tăng tốc khác nhau. Cuối cùng, chúng tôi muốn chỉ ra rằng một số phương pháp CAL là ablation của nhau. Ví dụ, CAL-ER là ablation cho CAL-DER (hoặc CAL-SD) khi chúng ta thay thế thành phần chưng cất. Tương tự, CAL-SD là ablation của CAL-SDS2, nơi chúng ta loại bỏ phần lấy mẫu submodular.

5.1 Tập dữ liệu và Thiết lập thí nghiệm

Chúng tôi sử dụng các tập dữ liệu sau, trải dài trên một phổ các phương thức dữ liệu, quy mô (cả về kích thước tập dữ liệu, và dấu chân tính toán/bộ nhớ của mô hình), và cân bằng lớp.

FMNIST: Tập dữ liệu FMNIST bao gồm 70.000 hình ảnh grayscale 28×28 của các vật phẩm thời trang thuộc 10 lớp (Xiao et al., 2017). Một kiến trúc ResNet-18 (He et al., 2016) với SGD được sử dụng. Chúng tôi áp dụng data augmentation, như trong Beck et al. (2021), bao gồm lật ngang ngẫu nhiên và cắt ngẫu nhiên.

CIFAR-10: CIFAR-10 bao gồm 60.000 hình ảnh màu 32×32 với 10 danh mục khác nhau (Krizhevsky, 2009). Chúng tôi sử dụng ResNet-18 và sử dụng optimizer SGD cho tất cả thí nghiệm CIFAR-10. Chúng tôi áp dụng data augmentation bao gồm lật ngang ngẫu nhiên và cắt ngẫu nhiên.

MedMNIST: Chúng tôi sử dụng tập dữ liệu DermaMNIST trong bộ sưu tập MedMNIST (Yang et al., 2021a;b) để đánh giá hiệu suất của CAL trên các phương thức hình ảnh y tế. Nó bao gồm các hình ảnh dermatoscopy 3 kênh màu của 7 bệnh da khác nhau, ban đầu được lấy từ Codella et al. (2019); Tschandl et al. (2018). Một kiến trúc ResNet-18 được sử dụng cho tất cả thí nghiệm DermaMNIST.

Amazon Polarity Review: (Zhang et al., 2015) là một tập dữ liệu NLP bao gồm các đánh giá từ Amazon và xếp hạng sao tương ứng của chúng (5 lớp) được sử dụng cho học tích cực trong Coleman et al. (2020b). Tương tự như công trình trước đó, chúng tôi xem xét tổng nhóm không nhãn có kích thước 2 triệu câu và sử dụng kiến trúc VDCNN-9 (Schwenk et al., 2017) được huấn luyện bằng Adam.

COLA: COLA (Warstadt et al., 2018) nhằm kiểm tra tính chấp nhận ngôn ngữ của một câu thông qua phân loại nhị phân. Chúng tôi xem xét nhóm không nhãn có kích thước 7000 tương tự như Ein-Dor et al. (2020) và sử dụng backbone BERT (Devlin et al., 2019) được huấn luyện bằng Adam.

Single-Cell Cell Type Identity Classification: Các công nghệ giải trình tự RNA tế bào đơn (scRNA-seq) gần đây đã cho phép đặc tính hóa quy mô lớn của hàng trăm nghìn đến hàng triệu tế bào trong các mô phức tạp, và chú thích loại tế bào chính xác là một bước quan trọng trong nghiên cứu các tập dữ liệu như vậy. Vì mục đích này, một số mô hình học sâu đã được đề xuất để tự động gán nhãn các tập dữ liệu scRNA-seq mới (Xie et al., 2021). Tập dữ liệu HCL rất mất cân bằng lớp và bao gồm dữ liệu scRNA-seq cho 562.977 tế bào trên 63 loại tế bào được đại diện trong 56 mô người (Han et al., 2020). Chúng tôi sử dụng mô hình ACTINN (Ma & Pellegrini, 2019), một perceptron đa lớp bốn lớp dự đoán loại tế bào cho mỗi tế bào cho biểu hiện của 28832 gene, và sử dụng optimizer SGD.

Siêu tham số: Chi tiết về phần cứng cụ thể và các lựa chọn siêu tham số được sử dụng để huấn luyện mô hình cho mỗi kỹ thuật có thể được tìm thấy trong Phụ lục A.5.

Thiết lập học tích cực: Như được thực hiện trong công trình trước đây (Coleman et al., 2020b; Killamsetty et al., 2021a) cho CIFAR10 và đánh giá Amazon polarity, ngân sách đi từ 10% đến 50% với bước nhảy 10% (kết quả cho các kích thước truy vấn khác được trình bày trong Phụ lục C.3). Đối với FMNIST, MedMNIST, và tập dữ liệu Cell-type, nó đi từ 10% đến 30% với bước nhảy 5%. Cuối cùng, đối với COLA, chúng tôi tuân theo ngân sách sử dụng kích thước tuyệt đối từ 200 đến 1000 với bước nhảy 200 (tương tự như Ein-Dor et al. (2020)). Chúng tôi áp dụng khung AL được đề xuất trong Beck et al. (2021) cho tất cả thí nghiệm. Trong bài báo chính, chúng tôi ở đây trình bày kết quả cho hàm thu thập dựa trên lấy mẫu độ không chắc chắn. Tuy nhiên, chúng tôi cung cấp kết quả sử dụng các hàm thu thập khác trong Phụ lục C.

5.2 Hiệu suất vs Tăng tốc

Mục tiêu của chúng tôi là cho thấy rằng đối với mọi tập dữ liệu và ngân sách, tồn tại ít nhất một phương pháp CAL hoạt động tương đương (nếu không tốt hơn) so với AL baseline. Tuy nhiên, với độ chính xác thô, khó có thể so sánh các phương pháp CAL khác nhau và AL baseline trên các tập dữ liệu khác nhau ở các ngân sách khác nhau. Do đó, chúng tôi bắt đầu bằng cách quan sát mức tăng tương đối về độ chính xác so với AL baseline. Các mức tăng tương đối làm cho việc lấy trung bình trên các ngân sách, cho một tập dữ liệu nhất định, và lấy trung bình trên các tập dữ liệu, cho một ngân sách nhất định trở nên khả thi. Đối với mọi ngân sách, chúng tôi chuẩn hóa độ chính xác của mỗi phương pháp bởi phương pháp của AL baseline. Điều này làm cho độ chính xác baseline luôn là 1, bất kể ngân sách và tập dữ liệu. Hiệu suất tương đối lớn hơn 1 chỉ ra độ chính xác tốt hơn baseline (và ngược lại cho trường hợp nhỏ hơn 1). Nói như vậy: (1) giữ ngân sách cố định ở 10%, 20%, và 30% và lấy trung bình trên các tập dữ liệu (ngoại trừ COLA, vì nó có ngân sách khác) cho chúng ta Hình 5; (2) giữ tập dữ liệu cố định, lấy trung bình độ chính xác tương đối vs tăng tốc trên các ngân sách khác nhau cho chúng ta Hình 4; và (3) lấy trung bình thêm các điều trên trên các tập dữ liệu khác nhau cho chúng ta Hình 1. Các phương pháp ở góc trên bên phải là tốt hơn. Vì lý do không gian, chúng tôi chỉ hiển thị độ chính xác tương đối trong bài báo chính, nhưng tất cả độ chính xác tuyệt đối và độ lệch chuẩn cho mỗi phương pháp cho mọi tập dữ liệu và mọi ngân sách có sẵn trong Phụ lục A.2.

Từ mô tả tổng thể và từng tập dữ liệu về hiệu suất của CAL (Hình 1 và 4, tương ứng), rõ ràng là tồn tại một phương pháp CAL đạt được sự tăng tốc đáng kể so với kỹ thuật AL tiêu chuẩn cho mọi tập dữ liệu và ngân sách trong khi duy trì độ chính xác tập test. Từ Hình 4, chúng ta có thể thấy thêm rằng đối với một số tập dữ liệu (như FMNIST và CIFAR-10), CAL-ER, một phương pháp không chưng cất và dựa trên lấy mẫu đều, chỉ gây ra một sự giảm nhỏ về hiệu suất nhưng đạt được sự tăng tốc cao nhất. Điều này cho thấy rằng việc thiên lệch ngây thơ học về phía các nhiệm vụ gần đây có thể đủ để thích ứng mô hình với một tập điểm mới giữa các vòng AL. Tuy nhiên, như chúng tôi cho thấy trong Hình 5, điều này không đúng phổ quát cho tất cả các tập dữ liệu (ở các ngân sách khác nhau). Do đó, các phương pháp bao gồm một số loại số hạng chưng cất (CAL-DER, CAL-SD, CAL-SDS2) thường hoạt động tốt nhất trong tất cả các phương pháp CAL. Chúng tôi tin rằng phương pháp dựa trên lấy mẫu submodular (CAL-SDS2) có thể được tăng tốc sử dụng các phương pháp ngẫu nhiên và kết quả được cải thiện bằng cách xem xét các hàm submodular khác, điều này chúng tôi để lại cho công trình tương lai. Tuy nhiên, cần phải đề cập rằng hàm concave trên h(xi) là cần thiết cho hiệu suất của CAL-SDS2.

5.3 So sánh giữa Mô hình tiêu chuẩn và CAL

Trong phần này, chúng tôi tiếp theo đánh giá xem huấn luyện CAL có ảnh hưởng xấu nào đến hành vi của mô hình cuối cùng không. Chúng tôi trước tiên chứng minh rằng CAL không dẫn đến bất kỳ sự suy giảm nào của độ bền vững mô hình (Phần 5.3.1). Sau đó chúng tôi chứng minh rằng các mô hình CAL và các mô hình được huấn luyện baseline không chắc chắn về một tập mẫu chưa thấy tương tự (Phần 5.3.2). Cuối cùng, trong phụ lục A.5 chúng tôi cung cấp một phân tích độ nhạy của các phương pháp được đề xuất, nơi chúng tôi chứng minh rằng các phương pháp CAL bền vững với các thay đổi đối với các siêu tham số.

5.3.1 Độ bền vững

Các phân phối thời gian test có thể thay đổi so với các phân phối huấn luyện, vì vậy điều quan trọng là đảm bảo rằng các mô hình có thể tổng quát hóa trên các miền khác nhau. Vì các mô hình được huấn luyện sử dụng các phương pháp CAL đòi hỏi ít bước gradient hơn đáng kể, quy trình huấn luyện đã sửa đổi có thể tạo ra các mô hình hay thay đổi ít bền vững hơn với sự dịch chuyển miền. Để đảm bảo chống lại điều này, chúng tôi đánh giá độ bền vững của mô hình được huấn luyện bằng phương pháp CAL trong phần này. Chúng tôi xem xét CIFAR-10C Hendrycks & Dietterich (2019), một tập dữ liệu bao gồm 19 biến dạng khác nhau mỗi biến dạng được thực hiện ở 5 mức độ nghiêm trọng. Đối với mỗi mô hình được huấn luyện lên đến ngân sách 50%, chúng tôi báo cáo độ chính xác phân loại trung bình trên mỗi biến dạng và so sánh nó với baseline trong Hình 6; mỗi kết quả là trung bình của ba random seed. Chúng tôi lưu ý rằng hầu hết các phương pháp CAL hoạt động tương tự thống kê với học tích cực tiêu chuẩn, tất cả trong khi cung cấp tăng tốc đáng kể. Hơn nữa, trung bình trên tất cả các bài kiểm tra, các mô hình được huấn luyện với CAL-SDS2 tốt hơn các mô hình được huấn luyện với CAL-DER, nơi chúng ta thấy sự khác biệt lên đến 5% trong các biến dạng như glass blur; vui lòng tham khảo phụ lục A.3 bảng 8, bảng 9 và 10 cho độ chính xác tuyệt đối và kiểm định thống kê. Lấy mẫu submodular replay một tập con đại diện đa dạng của lịch sử có khả năng là lý do đằng sau độ bền vững tốt hơn của CAL-SDS2. Mối quan hệ của tính đa dạng với độ bền vững cũng đã được khám phá trong các công trình trước đây bao gồm Killamsetty et al. (2021b); Fang et al. (2022); Rozen et al. (2019); Gong et al. (2018).

5.3.2 Tương quan của điểm số độ không chắc chắn

Để các mô hình được huấn luyện sử dụng các kỹ thuật CAL được sử dụng như những thay thế hợp lệ cho các mô hình AL tiêu chuẩn, hai lớp mô hình này cần truy vấn các mẫu tương tự tại mỗi vòng AL. Điều này đặc biệt quan trọng nếu AL được sử dụng chỉ như một quy trình lựa chọn tập con dữ liệu (nơi người dùng quan tâm đến chất lượng của tập dữ liệu có nhãn kết quả thay vì mô hình cuối cùng). Khi sử dụng lấy mẫu độ không chắc chắn như hàm thu thập AL, tính tương quan Pearson giữa các điểm entropy của các mô hình baseline và CAL trên tập validation sau mỗi vòng truy vấn là một cách để xác định điều này. Vì hầu hết các chính sách AL kết hợp một số khái niệm về độ không chắc chắn, chúng tôi giả thuyết rằng kết quả của thí nghiệm này nên mở rộng cho các hàm thu thập khác. Một phân tích tương tự được thực hiện trong Coleman et al. (2020b).

Như được thấy trong Hình 7, tương quan Pearson giữa tất cả các cặp mô hình là dương tại mỗi vòng truy vấn AL. Do đó, bản chất của các mẫu mà các mô hình không chắc chắn, và do đó có khả năng được chọn tại mỗi vòng, tương tự giữa các mô hình được huấn luyện CAL và các mô hình được huấn luyện AL baseline. Một phân tích chi tiết của các tương quan này tại mỗi vòng được cung cấp trong Hình 14 trong Phụ lục.

6 Kết luận và Công trình tương lai

Chúng tôi đã đề xuất khung CAL, phương pháp đầu tiên để khắc phục vấn đề phải huấn luyện lại các mô hình giữa các vòng AL batch. Trên các tập dữ liệu thị giác, ngôn ngữ tự nhiên, hình ảnh y tế và sinh học, chúng tôi cho thấy luôn có một phương pháp dựa trên CAL bằng hoặc vượt trội hơn AL tiêu chuẩn trong khi đạt được sự tăng tốc đáng kể. Vì CAL độc lập với kiến trúc mô hình và chiến lược AL, khung này áp dụng cho một phạm vi rộng các bối cảnh.

Mở rộng thực nghiệm Các hướng thực nghiệm tương lai có thể bao gồm những điều sau: (1): CAL giảm thời gian huấn luyện của mô hình, nhưng không giảm thời gian truy vấn AL mặc dù việc giảm thời gian truy vấn có thể là một nhánh phụ của công trình này; (2): CAL hoạt động sử dụng các hàm thu thập truy vấn AL hiện có, nhưng có thể tùy chỉnh các hàm thu thập cho các phương pháp CAL mang lại những cải thiện tổng quát hóa và tính toán thêm; (3): trong khi việc sử dụng tính submodular của SDS2 giúp ích cho độ bền vững, các chiến lược submodular bổ sung có thể được sử dụng để cải thiện thêm kết quả và cũng cho việc lựa chọn truy vấn AL đa dạng; và (4): CAL cung cấp một ứng dụng mới cho CL; công trình CL tương lai có thể được đánh giá một phần dựa trên hiệu suất CAL của nó.

Mở rộng lý thuyết Khung Học tích cực dựa trên sự khác biệt đầy hứa hẹn để thiết lập các giới hạn lỗi tổng quát hóa cho khung CAL dựa trên replay. Cụ thể, tận dụng các kết quả tổng quát hóa được trình bày trong Định lý 2 của Cui & Sato (2020), giới hạn trên của hàm rủi ro dựa trên phân phối dữ liệu thực (được gọi là P trong định lý) phụ thuộc vào phân phối thực nghiệm của dữ liệu hiện được gán nhãn (được gọi là Q trong định lý), bao gồm batch truy vấn mới được thu thập.

Trái ngược với việc sử dụng toàn bộ dữ liệu có nhãn có sẵn, CAL chiến lược lựa chọn một tập con của dữ liệu này thông qua việc sử dụng các phương pháp replay, để tăng tốc. Do điều này, phân phối thực nghiệm của các ví dụ được chọn bởi CAL (gọi là Q') có thể lệch khỏi Q. Do đó, người ta có thể sửa đổi giới hạn trên của hàm rủi ro dựa trên phân phối dữ liệu thực trong Định lý 2 bằng cách thêm số hạng lỗi (sẽ phụ thuộc vào một số khái niệm khoảng cách giữa Q và Q') biến mất khi chúng ta xem xét tất cả các ví dụ hiện được gán nhãn, do đó, khôi phục AL tiêu chuẩn.

Lời cảm ơn

Công trình này được hỗ trợ một phần bởi CONIX Research Center, một trong sáu trung tâm trong JUMP, một chương trình Semiconductor Research Corporation (SRC) được tài trợ bởi DARPA, bởi National Science Foundation dưới Grant Nos. IIS-2106937 và IIS-2148367, và bởi giải thưởng NIH/NHGRI U01 HG009395. Chúng tôi cảm ơn Lilly Kumari, Tianyi Zhou, Shengjie Wang và tất cả các thành viên khác của MELODI lab vì những thảo luận và phản hồi hữu ích của họ. Chúng tôi cũng cảm ơn TMLR Action Editor và các reviewer vì những bình luận xây dựng của họ.
