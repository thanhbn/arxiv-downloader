Tài liệu tham khảo
David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, và Satinder Singh. A definition of continual reinforcement learning. arXiv preprint arXiv:2307.11046, 2023.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.

Sridhar Alla, Suman Kalyan Adari, Sridhar Alla, và Suman Kalyan Adari. What is MLOPs? Beginning MLOps with MLFlow: Deploy Models in AWS SageMaker, Google Cloud, and Microsoft Azure, pp. 79–124, 2021.

Dario Amodei và Danny Hernandez. AI and compute. https://openai.com/research/ai-and-compute, 2018. Trực tuyến; truy cập 20-Tháng 6-2023.

Jordan Ash và Ryan P Adams. On warm-starting neural network training. Advances in Neural Information Processing Systems, 33:3884–3894, 2020.

Iz Beltagy, Kyle Lo, và Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019.

Abhijit Bendale và Terrance Boult. Towards open world recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1893–1902, 2015.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert, và Jason Weston. Curriculum learning. In Proceedings of the 26th annual International Conference on Machine Learning, pp. 41–48, 2009.

Yoshua Bengio, Aaron Courville, và Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.

Tudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan Pascanu, và Claudia Clopath. A study on the plasticity of neural networks. arXiv preprint arXiv:2106.00042, 2021.

Vivek Chavan, Paul Koch, Marian Schlüter, và Clemens Briese. Towards realistic evaluation of industrial continual learning scenarios with an emphasis on energy consumption and computational footprint. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11506–11518, 2023.

Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, và Yuval Atzmon. "this is my unicorn, fluffy": Personalizing frozen vision-language representations. In European Conference on Computer Vision, pp. 558–577. Springer, 2022.

Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, và Davide Bacciu. Continual pre-training mitigates forgetting in language and vision. arXiv preprint arXiv:2205.09357, 2022.

William Dally. On the model of computation: point. Communications of the ACM, 65(9):30–32, 2022.

Shai Ben David, Tyler Lu, Teresa Luu, và Dávid Pál. Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129–136. JMLR Workshop and Conference Proceedings, 2010.

Saket Dingliwal, Monica Sunkara, Srikanth Ronanki, Jeff Farris, Katrin Kirchhoff, và Sravan Bodapati. Personalization of CTC speech recognition models. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 302–309. IEEE, 2023.

Shibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, và Rupam Mahmood. Loss of plasticity in deep continual learning. Research Square preprint PPR: PPR727015, 2023. doi: 10.21203/rs.3.rs-3256479/v1.

Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, và Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. International Conference on Learning Representations, 2020.

European Union. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation). Official Journal L110, 59:1–88, 2016.

Enrico Fini, Victor G Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, và Julien Mairal. Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9621–9630, 2022.

Robert M French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4):128–135, 1999.

Haotian Fu, Shangqun Yu, Michael Littman, và George Konidaris. Model-based lifelong reinforcement learning with bayesian exploration. Advances in Neural Information Processing Systems, 35:32369–32382, 2022.

Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, Ameya Prabhu, Philip HS Torr, và Bernard Ghanem. Real-time evaluation in online continual learning: A new hope. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11888–11897, 2023.

Eileen Guo. A roomba recorded a woman on the toilet. how did screenshots end up on facebook? https://www.technologyreview.com/2022/12/19/1065306/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/, 2022. Trực tuyến; truy cập 11-Tháng 10-2023.

Guy Hacohen, Leshem Choshen, và Daphna Weinshall. Let's agree to agree: Neural networks share classification order on real datasets. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 3950–3960. PMLR, 2020.

Raia Hadsell, Dushyant Rao, Andrei A Rusu, và Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in Cognitive Sciences, 24(12):1028–1040, 2020.

Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, và Michal Irani. Reconstructing training data from trained neural networks. Advances in Neural Information Processing Systems, 35:22911–22924, 2022.

Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, và Christopher Kanan. How efficient are today's continual learning algorithms? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 2431–2436, June 2023a.

Md Yousuf Harun, Jhair Gallardo, Tyler L Hayes, Ronald Kemker, và Christopher Kanan. SIESTA: Efficient online continual learning with sleep. Transactions on Machine Learning Research, 2023b.

Md Yousuf Harun, Jhair Gallardo, và Christopher Kanan. GRASP: A rehearsal policy for efficient online continual learning. arXiv preprint arXiv:2308.13646, 2023c.

Tyler L Hayes và Christopher Kanan. Online continual learning for embedded devices. In Conference on Lifelong Learning Agents, 2022.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=nZeVKeeFYf9.

Hexiang Hu, Ozan Sener, Fei Sha, và Vladlen Koltun. Drinking from a firehose: Continual learning with web-scale natural language. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5684–5696, 2022b.

Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, và Xinping Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. Computer Science Review, 37:100270, 2020.

E. Hüllermeier và W. Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110(3):457–506, 2021. doi: 10.1007/s10994-021-05946-3.

Chip Huyen. Real-time machine learning: challenges and solutions, Jan 2022. URL https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html#towards-continual-learning. Trực tuyến; truy cập 14-Tháng 11-2023.

Khurram Javed, Haseeb Shah, Richard S Sutton, và Martha White. Scalable real-time recurrent learning using columnar-constructive networks. Journal of Machine Learning Research, 24:1–34, 2023.

Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, và Ser-Nam Lim. Visual prompt tuning. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII, pp. 709–727. Springer, 2022.

Dahuin Jung, Dongyoon Han, Jihwan Bang, và Hwanjun Song. Generating instance-level prompts for rehearsal-free continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11847–11857, 2023.

Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, và Lei Shu. Achieving forgetting prevention and knowledge transfer in continual learning. Advances in Neural Information Processing Systems, 34, 2021.

Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, và Bing Liu. Adapting a language model while preserving its general knowledge. In Proceedings of The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022), 2022.

Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, và Bing Liu. A theoretical study on solving continual learning. In Advances in Neural Information Processing Systems, 2022.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.

Varsha Kishore, Chao Wan, Justin Lovelace, Yoav Artzi, và Kilian Q Weinberger. Incdsi: incrementally updatable document retrieval. In International Conference on Machine Learning, pp. 17122–17134. PMLR, 2023.

Jeremias Knoblauch, Hisham Husain, và Tom Diethe. Optimal continual learning has perfect memory and is np-hard. In International Conference on Machine Learning, pp. 5327–5337. PMLR, 2020.

Akinwande Komolafe. Retraining model during deployment: Continuous training and continuous testing, 2023. URL https://neptune.ai/blog/retraining-model-during-deployment-continuous-training-continuous-testing. Trực tuyến; truy cập 30-Tháng 6-2023.

Dominik Kreuzberger, Niklas Kühl, và Sebastian Hirschl. Machine learning operations (MLOPS): Overview, definition, and architecture. IEEE Access, 2023.

Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Blackiston, Josh Bongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, et al. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4(3):196–210, 2022.

Dhireesha Kudithipudi, Anurag Daram, Abdullah Zyarah, Fatima tuz Zohora, James B. Aimone, Angel Yanguas-Gil, Nicholas Soures, Emre Neftci, Matthew Mattina, Vincenzo Lomonaco, Clare D. Thiem, và Benjamin Epstein. Uncovering design principles for lifelong learning ai accelerators. Nature Electronics (Final Revisions), 2023.

Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, và Benjamin Van Roy. Continual learning as computationally constrained reinforcement learning. arXiv preprint arXiv:2307.04345, 2023.

Lilly Kumari, Shengjie Wang, Tianyi Zhou, và Jeff A Bilmes. Retrospective adversarial replay for continual learning. Advances in Neural Information Processing Systems, 35:28530–28544, 2022.

Sebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, và Klaus-Robert Müller. Unmasking clever hans predictors and assessing what machines really learn. Nature communications, 10(1):1096, 2019.

Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing temporal generalization in neural language models. Advances in Neural Information Processing Systems, 34:29348–29363, 2021.

Sergey Levine, Aviral Kumar, George Tucker, và Justin fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.

Zhizhong Li và Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017.

Zhuowei Li, Long Zhao, Zizhao Zhang, Han Zhang, Di Liu, Ting Liu, và Dimitris N Metaxas. Steering prototype with prompt-tuning for rehearsal-free continual learning. arXiv preprint arXiv:2303.09447, 2023.

Junfan Lin, Zhongzhan Huang, Keze Wang, Xiaodan Liang, Weiwei Chen, và Liang Lin. Continuous transition: Improving sample efficiency for continuous control problems via mixup. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 9490–9497. IEEE, 2021.

Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8:293–321, 1992.

Clare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, và Yarin Gal. Learning dynamics and generalization in deep reinforcement learning. In International Conference on Machine Learning, pp. 14560–14581. PMLR, 2022.

Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew D Bagdanov, và Joost van de Weijer. Class-incremental learning: survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, và Hassan Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Fmg_fQYUejf.

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, và Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0DcZxeWfOPt.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.

Martin Mundt, Steven Lang, Quentin Delfosse, và Kristian Kersting. CLEVA-compass: A continual learning evaluation assessment compass to promote research transparency and comparability. International Conference on Learning Representations, 2022.

Martin Mundt, Yongwon Hong, Iuliia Pliushch, và Visvanathan Ramesh. A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning. Neural Networks, 160:306–336, 2023.

V.L. Nguyen, M.H. Shaker, và E. Hüllermeier. How to measure uncertainty in uncertainty sampling for active learning. Machine Learning, 111(1):89–122, 2022. doi: 10.1007/s10994-021-06003-9.

Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.

Oleksiy Ostapenko, Timothee Lesort, Pau Rodríguez, Md Rifat Arefin, Arthur Douillard, Irina Rish, và Laurent Charlin. Foundational models for continual learning: An empirical study of latent replay, 2022. URL https://arxiv.org/abs/2205.00329.

Aristeidis Panos, Yuriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, và Richard E Turner. First session adaptation: A strong replay-free baseline for class-incremental learning. arXiv preprint arXiv:2303.13199, 2023.

Anastasia Pentina và Christoph H. Lampert. A PAC-bayesian bound for lifelong learning. In ICML, 2014.

Iuliia Pliushch, Martin Mundt, Nicolas Lupp, và Visvanathan Ramesh. When Deep Classifiers Agree: Analyzing Correlations Between Learning Order and Image Statistics. European Conference on Computer Vision (ECCV), pp. 397–413, 2022.

Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet K Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, và Adel Bibi. Computationally budgeted continual learning: What does matter? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3698–3707, 2023a.

Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, và Ozan Sener. Online continual learning without the storage constraint. arXiv preprint arXiv:2305.09253, 2023b.

Diana Benavides Prado và Patricia Riddle. A theory for knowledge transfer in continual learning. In Conference on Lifelong Learning Agents, 2022.

Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, và Charles Blundell. Neural episodic control. In International Conference on Machine Learning, pp. 2827–2836. PMLR, 2017.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.

Rahul Ramesh và Pratik Chaudhari. Model zoo: A growing" brain" that learns continually. arXiv preprint arXiv:2106.03027, 2021.

David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, và Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.

Khalid Salama, Jarek Kazmierczak, và Donna Schut. Practitioners guide to MLOPS: A framework for continuous delivery and automation of machine learning. Google Could White paper, 2021.

Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, và Aleksander Madry. Editing a classifier by rewriting its prediction rules. Advances in Neural Information Processing Systems, 34:23359–23373, 2021.

Tom Schaul, John Quan và Ioannis Antonoglou, và David Silver. Prioritized experience replay. In 4th International Conference on Learning Representations, ICLR 2016, 2016.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.

Roy Schwartz, Jesse Dodge, Noah A Smith, và Oren Etzioni. Green AI. Communications of the ACM, 63(12):54–63, 2020.

Burr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison, 2009.

David Silver, Richard S Sutton, và Martin Müller. Sample-based learning and search with permanent and transient memories. In Proceedings of the 25th International Conference on Machine learning, pp. 968–975, 2008.

Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In AAAI, volume 92, pp. 171–176. San Jose, CA, 1992.

Richard S. Sutton và Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html.

Richard S Sutton, Anna Koop, và David Silver. On the role of tracking in stationary environments. In Proceedings of the 24th International Conference on Machine learning, pp. 871–878, 2007.

Tinne Tuytelaars, Bing Liu, Vincenzo Lomonaco, Gido van de Ven, và Andrea Cossu. Deep Continual Learning (Dagstuhl Seminar 23122). Dagstuhl Reports, 13(3):74–91, 2023. ISSN 2192-5283. doi: 10.4230/DagRep.13.3.74. URL https://drops.dagstuhl.de/entities/document/10.4230/DagRep.13.3.74.

Gido M van de Ven, Tinne Tuytelaars, và Andreas S Tolias. Three types of incremental learning. Nature Machine Intelligence, 4(12):1185–1197, 2022.

Liyuan Wang, Xingxing Zhang, Qian Li, Jun Zhu, và Yi Zhong. Coscl: Cooperation of small continual learners is stronger than a big one. In European Conference on Computer Vision, pp. 254–271. Springer, 2022a.

Liyuan Wang, Xingxing Zhang, Hang Su, và Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. arXiv preprint arXiv:2302.00487, 2023.

Zifeng Wang, Zheng Zhan, Yifan Gong, Geng Yuan, Wei Niu, Tong Jian, Bin Ren, Stratis Ioannidis, Yanzhi Wang, và Jennifer Dy. SparCL: Sparse continual learning on the edge. Advances in Neural Information Processing Systems, 35:20366–20380, 2022b.

Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, và Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139–149, 2022c.

Maciej Wołczyk, Michał Zając, Razvan Pascanu, Łukasz Kuciński, và Piotr Miłoś. Continual world: A robotic benchmark for continual reinforcement learning. Advances in Neural Information Processing Systems, 34:28496–28510, 2021.

Maciej Wolczyk, Michał Zając, Razvan Pascanu, Łukasz Kuciński, và Piotr Miłoś. Disentangling transfer in continual reinforcement learning. Advances in Neural Information Processing Systems, 35:6304–6317, 2022.

Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959–7971, 2022.

Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, và Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. arXiv preprint arXiv:2305.10626, 2023.

Shipeng Yan, Jiangwei Xie, và Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3014–3023, 2021.

Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, và Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 3712–3722, 2018.

Zeyu Zhao. The application of the right to be forgotten in the machine learning context: From the perspective of european laws. Cath. UJL & Tech, 31:73, 2022.

Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, và Ziwei Liu. Deep class-incremental learning: A survey. arXiv preprint arXiv:2302.03648, 2023.

Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, và Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43–76, 2020.

Alexander Zimin và Christoph H. Lampert. Tasks without borders: A new approach to online multi-task learning. In ICML Workshop on Adaptive & Multitask Learning, 2019. URL https://openreview.net/forum?id=HkllV5Bs24.

A Chi tiết phân tích trong Mục 2

Để xác minh các từ khóa 'incremental', 'continual', 'forgetting', 'lifelong' và 'catastrophic', được sử dụng để lọc các bài báo dựa trên tiêu đề của họ, chúng tôi đã kiểm tra chúng bằng cách sử dụng một tập hợp xác thực được thu thập thủ công mà chúng tôi chắc chắn rằng chúng liên quan đến học liên tục. Tập hợp này được thu thập thủ công trong khi nghiên cứu về học liên tục trong vài năm qua. Các từ khóa có mặt trong 96% tiêu đề bài báo. Từ mỗi hội nghị, chúng tôi ngẫu nhiên chọn tối đa 20 trong số tất cả các bài báo khớp, bỏ qua các kết quả dương tính giả.

Thông thường để đánh giá các phương pháp và phân tích mới trên nhiều hơn một điểm chuẩn. Thường điều này có nghĩa là phần trăm mẫu được lưu trữ không đồng nhất trên các thí nghiệm trong một bài báo. Trong Hình 1, chúng tôi đã hiển thị phần trăm tối thiểu được sử dụng, trong Hình 3 chúng tôi hiển thị tối đa. Kết luận vẫn giữ nguyên, và lượng mẫu được lưu trữ bị ràng buộc trong tất cả trừ hai điểm chuẩn.

Trong Bảng 1, chúng tôi cung cấp một bảng của tất cả các bài báo chúng tôi đã sử dụng trong phân tích Mục 2, hiển thị tỷ lệ lưu trữ mẫu tối thiểu và tối đa (SSR) của họ, tức là phần trăm mẫu được lưu trữ, cũng như có thể tiêu thụ bộ nhớ khác. Cột cuối cùng đề cập đến cách họ tiếp cận chi phí tính toán.

Bảng 1: Tất cả các bài báo được sử dụng trong kiểm tra của Mục 2. SSR đề cập đến tỷ lệ lưu trữ mẫu, tức là bao nhiêu mẫu được lưu trữ liên quan đến toàn bộ tập dữ liệu.

[Bảng chi tiết với 77 bài báo và các thông tin về tỷ lệ lưu trữ mẫu và chi phí tính toán như trong văn bản gốc]
