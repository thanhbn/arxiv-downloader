# SLCA: Học Chậm với Căn Chỉnh Bộ Phân Loại cho Học Liên Tục trên Mô Hình Được Huấn Luyện Trước

Gengwei Zhang1*, Liyuan Wang2*, Guoliang Kang3,4, Ling Chen1, Yunchao Wei5,6
1AAII, University of Technology Sydney2Tsinghua University
3Beihang University4Zhongguancun Laboratory
5Institute of Information Science, Beijing Jiaotong University
6Beijing Key Laboratory of Advanced Information Science and Network
{zgwdavid, kgl.prml, wychao1987 }@gmail.com; wly19@mail.tsinghua.org.cn; ling.chen@uts.edu.au

## Tóm tắt

Mục tiêu của học liên tục là cải thiện hiệu suất của các mô hình nhận dạng trong việc học dữ liệu đến tuần tự. Mặc dù hầu hết các nghiên cứu hiện tại được thiết lập dựa trên tiền đề học từ đầu, những nỗ lực ngày càng tăng đã được dành cho việc kết hợp các lợi ích của huấn luyện trước. Tuy nhiên, việc làm thế nào để khai thác thích ứng kiến thức được huấn luyện trước cho mỗi tác vụ tăng dần trong khi duy trì khả năng tổng quát hóa vẫn là một câu hỏi mở. Trong nghiên cứu này, chúng tôi trình bày một phân tích toàn diện cho học liên tục trên mô hình được huấn luyện trước (CLPM), và quy kết thách thức chính cho vấn đề overfitting tiến triển. Quan sát thấy rằng việc giảm có chọn lọc tỷ lệ học có thể gần như giải quyết vấn đề này trong lớp biểu diễn, chúng tôi đề xuất một phương pháp đơn giản nhưng cực kỳ hiệu quả có tên là Học Chậm với Căn Chỉnh Bộ Phân Loại (SLCA), điều này cải thiện thêm lớp phân loại bằng cách mô hình hóa các phân phối theo lớp và căn chỉnh các lớp phân loại theo cách hậu hoc. Qua nhiều kịch bản khác nhau, đề xuất của chúng tôi mang lại những cải thiện đáng kể cho CLPM (ví dụ, lên đến 49.76%, 50.05%, 44.69% và 40.16% trên Split CIFAR-100, Split ImageNet-R, Split CUB-200 và Split Cars-196, tương ứng), và do đó vượt trội hơn các phương pháp hiện đại với biên độ lớn. Dựa trên baseline mạnh như vậy, các yếu tố quan trọng và hướng hứa hẹn được phân tích sâu để tạo điều kiện cho nghiên cứu tiếp theo. Mã nguồn đã được công khai tại: https://github.com/GengDavid/SLCA.

## 1. Giới thiệu

Học liên tục nhằm học hiệu quả từ dữ liệu đến tuần tự, hoạt động như thể chúng được quan sát đồng thời. Các nỗ lực hiện tại chủ yếu dựa trên tiền đề học từ đầu, cố gắng giảm thiểu quên thảm khốc [27] của kiến thức đã học trước đó khi thích ứng với mỗi tác vụ tăng dần. Tuy nhiên, thành công của huấn luyện trước quy mô lớn đã cách mạng hóa mô hình huấn luyện của mạng neuron sâu. Giai đoạn huấn luyện trước mang lại cả chuyển giao kiến thức mạnh mẽ và khả năng chống chịu quên thảm khốc cho học liên tục downstream [43], điều này có xu hướng trở nên quan trọng hơn khi quy mô của huấn luyện trước tăng lên [31, 28]. Do đó, học liên tục trên mô hình được huấn luyện trước (CLPM) trở thành một hướng mới nổi và nhận được sự chú ý ngày càng tăng.

Đối với CLPM, kiến thức được huấn luyện trước thường được thể hiện bởi lớp biểu diễn, thích ứng với một chuỗi các tác vụ tăng dần. Có hai chiến lược chính để tận dụng kiến thức được huấn luyện trước [43]: (1) tinh chỉnh các biểu diễn, hoặc (2) giữ nguyên biểu diễn cố định trong khi học một số tham số bổ sung (ví dụ, adaptor [16], prompt [24], instruction [8], v.v.). Mặc dù (2) đang trở nên chiếm ưu thế trong xử lý ngôn ngữ tự nhiên (NLP) [18], việc lựa chọn (1) và (2) vẫn là một câu hỏi mở cho học liên tục trong thị giác máy tính (CV). Các phương pháp dựa trên prompt được đề xuất gần đây, như L2P [46] và DualPrompt [45], theo chiến lược thứ hai và được báo cáo là vượt trội hơn nhiều so với các baseline học liên tục truyền thống của việc tinh chỉnh lớp biểu diễn. Mặt khác, vì lượng lớn dữ liệu huấn luyện trước thường không được gán nhãn và cũng có thể đến tăng dần, có vẻ hợp lý hơn khi sử dụng huấn luyện trước tự giám sát thay vì huấn luyện trước có giám sát [5, 43], cũng xem xét rằng học liên tục (upstream) theo cách tự giám sát nói chung mạnh mẽ hơn với quên thảm khốc [17, 26, 9].

Tuy nhiên, trong nghiên cứu này, chúng tôi trình bày một phân tích toàn diện để xem xét lại tiến bộ hiện tại và lộ trình kỹ thuật cho CLPM trong CV. Cụ thể, CLPM đặt ra một thách thức quan trọng cho học liên tục rằng kiến thức được huấn luyện trước nên được khai thác thích ứng cho tác vụ hiện tại trong khi duy trì khả năng tổng quát hóa cho các tác vụ tương lai. Đối với các baseline học liên tục truyền thống tinh chỉnh tuần tự (Seq FT) toàn bộ mô hình, tỷ lệ học đồng nhất thường quá lớn cho lớp biểu diễn, dẫn đến vấn đề overfitting tiến triển nơi các biểu diễn được huấn luyện trước overfit với mỗi tác vụ tăng dần và dần mất khả năng tổng quát hóa. Đây là nguyên nhân chính của sự kém cỏi của chúng so với các phương pháp dựa trên prompt gần đây. Hiện tượng này càng trở nên tồi tệ hơn khi sử dụng huấn luyện trước tự giám sát thực tế hơn, thường yêu cầu cập nhật lớn hơn của các biểu diễn được huấn luyện trước để phù hợp đầy đủ với mỗi tác vụ tăng dần.

Để giải quyết những thách thức trên, chúng tôi đề xuất một phương pháp đơn giản nhưng cực kỳ hiệu quả có tên là Học Chậm với Căn Chỉnh Bộ Phân Loại (SLCA). Đầu tiên, chúng tôi quan sát rằng vấn đề overfitting tiến triển của lớp biểu diễn có thể được tránh gần như hoàn toàn bằng cách giảm có chọn lọc tỷ lệ học của nó (tức là, sử dụng một học viên chậm), đủ để đạt được sự cân bằng tốt giữa tính đặc hiệu của tác vụ và khả năng tổng quát hóa trong việc cập nhật liên tục các biểu diễn được huấn luyện trước. Dựa trên học viên chậm được đề xuất, chúng tôi cải thiện thêm lớp phân loại bằng cách mô hình hóa các phân phối theo lớp và căn chỉnh các dự đoán phân loại, để cân bằng đúng cách mối quan hệ giữa các tác vụ khác nhau.

Qua nhiều benchmark học liên tục khác nhau dưới huấn luyện trước có giám sát hoặc tự giám sát, đề xuất của chúng tôi mang lại những cải thiện đáng kể cho CLPM trong CV và lấp đầy khoảng cách một cách đáng kể của tiến bộ hiện tại từ hiệu suất giới hạn trên. Ví dụ, Seq FT đơn giản được cải thiện 49.76%, 50.05%, 44.69% và 40.16% trên Split CIFAR-100, Split ImageNet-R, Split CUB-200 và Split Cars-196, tương ứng, do đó vượt trội hơn các phương pháp SOTA với biên độ lớn (tóm tắt trong Hình 1). Trên Split CIFAR-100 và Split ImageNet-R, khoảng cách hiệu suất chỉ ít hơn 2% cho huấn luyện trước có giám sát và ít hơn 4% cho huấn luyện trước tự giám sát.

Đóng góp của chúng tôi bao gồm ba khía cạnh: (1) Chúng tôi trình bày một phân tích toàn diện về học liên tục trên mô hình được huấn luyện trước (CLPM), và chứng minh rằng vấn đề overfitting tiến triển là thách thức chính cho các baseline học liên tục truyền thống. (2) Chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả để giải quyết vấn đề này, mang lại những cải thiện đáng kể cho CLPM và rõ ràng vượt trội hơn các phương pháp hiện đại, phục vụ như một baseline mạnh để đánh giá lại tiến bộ hiện tại và lộ trình kỹ thuật. (3) Kết quả của chúng tôi tiếp tục xác định các yếu tố quan trọng và hướng hứa hẹn cho CLPM, như mô hình huấn luyện trước và mức độ chi tiết downstream, để tạo điều kiện cho nghiên cứu tiếp theo.

## 2. Nghiên cứu liên quan

Nghiên cứu hiện tại về học liên tục chủ yếu tập trung vào huấn luyện tuần tự mạng neuron sâu từ đầu, tìm cách học hiệu quả mỗi tác vụ mới mà không quên nghiêm trọng các tác vụ cũ. Các chiến lược tiêu biểu bao gồm các phương pháp dựa trên regularization [19, 1, 51, 25, 41, 7], bảo tồn mô hình cũ và ổn định có chọn lọc những thay đổi của tham số hoặc dự đoán; các phương pháp dựa trên replay [49, 30, 3, 44, 40, 39], xấp xỉ và khôi phục các phân phối dữ liệu đã học trước đó; các phương pháp dựa trên kiến trúc [34, 33, 42, 50], phân bổ không gian con tham số chuyên dụng cho mỗi tác vụ tăng dần; v.v.

Các nghiên cứu gần đây đã khám phá ngày càng nhiều lợi ích của huấn luyện trước cho học liên tục. Ví dụ, các biểu diễn thu được từ huấn luyện trước có giám sát đã được chứng minh là tạo điều kiện không chỉ cho chuyển giao kiến thức mà còn cho khả năng chống chịu quên thảm khốc cho học liên tục downstream [31, 28, 57]. Ngoài ra, việc học một số lượng lớn các lớp cơ sở trong giai đoạn huấn luyện ban đầu cho phép học tăng dần lớp với các thích ứng nhỏ [48]. Lấy cảm hứng từ các kỹ thuật sử dụng kiến thức được huấn luyện trước trong NLP, L2P [46] sử dụng một tập hợp tham số có thể học bổ sung gọi là "prompts" hướng dẫn động lớp biểu diễn được huấn luyện trước để học các tác vụ tăng dần. DualPrompt [45] mở rộng ý tưởng này bằng cách gắn các prompt bổ sung vào lớp biểu diễn được huấn luyện trước để học các hướng dẫn bất biến tác vụ và đặc hiệu tác vụ. Các phương pháp dựa trên prompt như vậy được báo cáo là vượt trội hơn nhiều so với các baseline học liên tục truyền thống, điều này có thể thách thức mô hình hiện tại của việc sử dụng kiến thức được huấn luyện trước trong CV.

Vì lượng lớn mẫu huấn luyện cần thiết để xây dựng mô hình được huấn luyện trước thường không được gán nhãn và cũng có thể đến tăng dần, huấn luyện trước tự giám sát nổi lên như một lựa chọn ưa thích hơn so với huấn luyện trước có giám sát. Một số nghiên cứu gần đây phát hiện rằng học liên tục theo cách tự giám sát ít bị quên thảm khốc hơn [17, 26, 9]. Thật vậy, các mô hình tự giám sát đã được chứng minh là thích ứng tốt hơn với học liên tục upstream [5]. Tuy nhiên, hiệu quả của huấn luyện trước tự giám sát cho học liên tục downstream vẫn cần được điều tra.

## 3. Học Liên tục trên Mô hình Được Huấn luyện Trước

Trong phần này, chúng tôi giới thiệu công thức vấn đề của học liên tục trên mô hình được huấn luyện trước (CLPM), thực hiện phân tích toàn diện, và sau đó trình bày phương pháp của chúng tôi.

### 3.1. Công thức Vấn đề

Hãy xem xét một mạng neuron Mθ(·) = hθcls(fθrps(·)) với tham số θ = {θrps, θcls} cho các tác vụ phân loại, bao gồm một lớp biểu diễn fθrps(·) chiếu ảnh đầu vào thành biểu diễn đặc trưng, và một lớp phân loại hθcls(·) chiếu biểu diễn đặc trưng thành dự đoán đầu ra. θrps được khởi tạo trên tập dữ liệu huấn luyện trước Dpt theo cách có giám sát hoặc tự giám sát (nhãn không cần thiết cho cách sau). Sau đó, Mθ cần học một chuỗi các tác vụ tăng dần từ tập huấn luyện của chúng Dt, t = 1, ..., T và cố gắng hoạt động tốt trên tập kiểm tra của chúng.

Theo các nghiên cứu trước về CLPM trong CV [46, 45], chúng tôi chủ yếu tập trung vào kịch bản tăng dần lớp của học liên tục [37]. Chi tiết, Dt = ∪c∈Ct{(xc,n, yc,n)}^Nc_n=1 giới thiệu một tập hợp các lớp mới Ct, trong đó Nc biểu thị số lượng mẫu huấn luyện (xc,n, yc,n) cho lớp c, và tất cả các lớp từng được nhìn thấy được đánh giá mà không có nhãn tác vụ.

Để đạt được mục tiêu này, mạng cần (1) chuyển giao hiệu quả kiến thức được huấn luyện trước cho mỗi tác vụ tăng dần trong khi duy trì khả năng tổng quát hóa cho các tác vụ tương lai, và (2) cân bằng đúng cách tính dẻo học của tác vụ mới với tính ổn định bộ nhớ của tác vụ cũ. Một baseline đơn giản là tinh chỉnh tuần tự toàn bộ mô hình Mθ trên mỗi Dt, trong đó fθrps và hθcls được cập nhật với tốc độ tương tự (tức là, sử dụng cùng tỷ lệ học). Tuy nhiên, do thiếu Dpt và D1:t-1 = ∪^{t-1}_{i=1} Di, hiệu suất của Mθ bị hạn chế nghiêm trọng bởi vấn đề overfitting tiến triển trong cả hai khía cạnh.

Cụ thể, (1) kiến thức của Dpt bị can thiệp bởi Dt, vì θrps được cập nhật liên tục để phù hợp với các tác vụ tăng dần trong khi khả năng tổng quát hóa thu được từ giai đoạn huấn luyện trước bị mất dần; và (2) kiến thức của D1:t-1 bị can thiệp bởi Dt, vì θcls (và θrps) quên thảm khốc các tác vụ cũ khi học mỗi tác vụ mới.

Hầu hết các phương pháp học liên tục [19, 25, 30, 49, 3] được thiết lập dựa trên tiền đề học từ đầu và tập trung vào cải thiện khía cạnh thứ hai. Chúng tôi gọi chúng là "baseline truyền thống". Để giải quyết khía cạnh đầu tiên, các phương pháp dựa trên prompt mới được đề xuất [46, 45] cố định lớp biểu diễn và sử dụng một tập hợp tham số có thể học bổ sung để hướng dẫn mô hình được huấn luyện trước, báo cáo hiệu suất tốt hơn đáng kể cho CLPM so với các baseline truyền thống khác cập nhật lớp biểu diễn.

### 3.2. Học Chậm là (Gần như) Tất cả Những gì Bạn Cần?

**Triển khai Học Chậm**: Tuy nhiên, chúng tôi lập luận rằng hiệu suất của các baseline truyền thống được báo cáo trong [46, 45] bị hạn chế nghiêm trọng bởi việc sử dụng tỷ lệ học đồng nhất (ví dụ, 0.005) cho cả θrps và θcls. Thông qua các thí nghiệm toàn diện, chúng tôi quan sát rằng việc sử dụng tỷ lệ học nhỏ hơn nhiều (0.0001) cho θrps và tỷ lệ học lớn hơn một chút (0.01) cho θcls có thể cải thiện đáng kể hiệu suất của các baseline truyền thống (Hình 2). Hiệu suất được báo cáo của tinh chỉnh tuần tự (Seq FT) được cải thiện hơn 40% cho các benchmark học liên tục thách thức như Split CIFAR-100 và Split ImageNet-R, tương ứng, do đó rõ ràng vượt trội hơn các phương pháp dựa trên prompt như L2P [46] và DualPrompt [45]. Chúng tôi gọi chiến lược đơn giản nhưng hiệu quả đáng ngạc nhiên này là "Học Chậm (SL)", tương ứng với việc làm chậm tốc độ cập nhật của lớp biểu diễn. Trái ngược với các nghiên cứu trước về sử dụng tỷ lệ học khác nhau cho transfer learning [10, 53, 14], lợi ích của đề xuất của chúng tôi đặc trưng cho học liên tục, vì hiệu suất giới hạn trên (tức là, huấn luyện kết hợp) chỉ được cải thiện một cách cận biên bởi SL trong khi khoảng cách hiệu suất giữa học liên tục và huấn luyện kết hợp được lấp đầy đáng kể (ví dụ, chỉ 4.36% trên Split CIFAR-100 và 7.80% trên Split ImageNet-R cho Seq FT w/ SL).

**Ảnh hưởng của Mô hình Huấn luyện Trước**: Sau đó chúng tôi đánh giá ảnh hưởng của các mô hình huấn luyện trước (tức là, có giám sát hoặc tự giám sát) trên học liên tục downstream. Ở đây chúng tôi tập trung vào ImageNet-1K như tập dữ liệu huấn luyện trước, vì hầu hết các phương pháp tự giám sát chỉ phát hành checkpoint trên nó. Xem xét tính nhất quán kiến trúc với các nghiên cứu trước về CLPM [46, 45], chúng tôi chọn các phương pháp tự giám sát tiêu biểu (tức là, MoCo v3 [4], MAE [13] và BEiT [2]) phát hành checkpoint trên ViT-B/16 trong so sánh của chúng tôi. Chúng tôi tiếp tục so sánh DeiT [36], một phương pháp có giám sát mạnh cho (huấn luyện trước) vision transformer.

Như được hiển thị trong Hình 3, huấn luyện trước tự giám sát, trong khi thực tế hơn về yêu cầu gán nhãn và học liên tục upstream, thường dẫn đến khoảng cách hiệu suất lớn hơn giữa Seq FT và huấn luyện kết hợp so với huấn luyện trước có giám sát. Tương tự, việc sử dụng SL có thể giảm hiệu quả khoảng cách này. Thú vị thay, hiệu suất của Seq FT w/ SL cho MoCo v3 [4] vượt xa hiệu suất của MAE [13] gần đây hơn, mặc dù hiệu suất huấn luyện kết hợp của chúng tương đương. Điều này có thể bởi vì các biểu diễn được huấn luyện trước của MoCo v3 [4] yêu cầu các cập nhật nhỏ hơn đáng kể để học tất cả các tác vụ tốt (Hình 4, trục Y bên trái), do đó giảm thiểu vấn đề overfitting tiến triển. Trong khi đó, việc sử dụng SL cho phép MoCo v3 [4] học các biểu diễn gần hơn nhiều với huấn luyện kết hợp (Hình 4, trục Y bên phải). Các kết quả trên gợi ý một hướng mới cho thiết kế các mô hình tự giám sát, tức là làm thế nào để thực hiện hiệu quả học liên tục downstream và kết hợp các ưu điểm của SL.

**Đánh giá Biểu diễn**: Vậy, tại sao Học Chậm lại hiệu quả như vậy, và điều gì giải thích cho khoảng cách hiệu suất còn lại? Chúng tôi thực hiện một thí nghiệm linear probing [13] để đánh giá hiệu suất của lớp biểu diễn. Cụ thể, sau khi học mỗi tác vụ tăng dần (ví dụ, 10 lớp mỗi tác vụ cho 10 tác vụ trong Split CIFAR-100) qua Seq FT w/ SL, chúng tôi cố định lớp biểu diễn và sử dụng một lớp phân loại bổ sung, gọi là linear probe, để học tất cả các lớp của tập dữ liệu benchmark tương ứng (ví dụ, tổng cộng 100 lớp trong tập dữ liệu CIFAR-100). Hiệu suất của các linear probe này được trình bày trong Hình 5, có xu hướng tăng theo việc học nhiều tác vụ hơn, cho thấy rằng lớp biểu diễn đang tích lũy kiến thức để thích ứng tốt hơn.

Sau khi học tất cả các tác vụ tăng dần, có thể thấy rõ rằng việc sử dụng lớp biểu diễn học liên tục để huấn luyện kết hợp một bộ phân loại bổ sung cho tất cả các lớp có thể gần như đạt được hiệu suất huấn luyện kết hợp của toàn bộ mô hình, và vượt trội hơn nhiều so với đối tác của nó với bộ phân loại học liên tục (tức là, Seq FT w/ SL trong Hình 5). Do đó, SL được đề xuất có thể gần như giải quyết vấn đề của lớp biểu diễn, nhưng lớp phân loại vẫn chưa tối ưu. Đặc biệt, vấn đề của lớp phân loại trở nên nghiêm trọng hơn cho các benchmark học liên tục chi tiết như Split CUB-200 và Split Cars-196.

### 3.3. Học Chậm với Căn Chỉnh Bộ Phân Loại

Để cải thiện thêm lớp phân loại, chúng tôi đề xuất căn chỉnh thống kê của các lớp đã học trước đó theo cách hậu hoc (xem Hình 6 và Thuật toán 1). Cụ thể, khi học mỗi tác vụ tăng dần, chỉ các tham số tương ứng với các lớp của tác vụ hiện tại được huấn luyện cùng với lớp biểu diễn. Sau khi học mỗi tác vụ, chúng tôi thu thập biểu diễn đặc trưng Fc = [rc,1, ..., rc,Nc] cho mỗi lớp c ∈ Ct của tác vụ hiện tại, trong đó rc,n = fθrps(xc,n) và Nc biểu thị lượng của nó. Thay vì lưu các đặc trưng đã trích xuất Fc của mẫu huấn luyện, CA bảo tồn trung bình µc ∈ Rd và hiệp phương sai Σc ∈ Rd×d của chúng cho mỗi lớp c (d biểu thị chiều đặc trưng).

Vì việc sử dụng huấn luyện trước cung cấp các biểu diễn được phân phối tốt, mỗi lớp có xu hướng có một đỉnh và có thể được mô hình hóa tự nhiên như một Gaussian N(µc, Σc).

Bất cứ khi nào mô hình cần được đánh giá, các lớp phân loại được căn chỉnh thêm như sau. Đầu tiên chúng tôi lấy mẫu các đặc trưng được tạo F̂c = [r̂c,1, ..., r̂c,Sc] từ phân phối N(µc, Σc) của mỗi lớp c ∈ C1:T từng được nhìn thấy trong C1:T = ∪Ti=1Ci, trong đó Sc là lượng đặc trưng được tạo cho mỗi lớp (Sc = 256 trong thí nghiệm của chúng tôi), và lượng tác vụ T có thể là bất kỳ số nguyên dương nào mà không cần biết trước. Tiếp theo, chúng tôi áp dụng cross-entropy loss được sử dụng rộng rãi để điều chỉnh lớp phân loại hθcls bằng cách đưa F̂1:T = [F̂1, ..., F̂|C1:T|] làm đầu vào của lớp phân loại, trong đó |C1:T| biểu thị số lượng lớp trong C1:T.

**Thuật toán 1** Học Chậm với Căn Chỉnh Bộ Phân Loại (SLCA)
**Đầu vào**: Tập dữ liệu huấn luyện trước Dpt; tập dữ liệu huấn luyện Dt cho tác vụ t = 1, ..., T; mạng Mθ(·) = hθcls(fθrps(·)) với tham số θ = {θrps, θcls}; tỷ lệ học α cho θrps và β cho θcls (α < β); tham số nhiệt độ τ.
**Khởi tạo**: Khởi tạo θrps bằng huấn luyện trước trên Dpt; khởi tạo θcls ngẫu nhiên.

1: # các tác vụ tuần tự.
2: cho tác vụ t = 1, ..., T thực hiện
3:    # tỷ lệ học khác nhau cho θrps và θcls.
4:    trong khi chưa hội tụ thực hiện
5:       Huấn luyện Mθ với cross-entropy loss trên Dt.
6:    kết thúc trong khi
7:    Thu thập Fc = [rc,1, ..., rc,Nc] cho c ∈ Ct.
8:    Lưu trung bình µc và hiệp phương sai Σc của Fc cho c ∈ Ct.
9: kết thúc cho
10: # căn chỉnh bộ phân loại.
11: Lấy mẫu F̂c từ N(µc, Σc) cho c ∈ C1:T.
12: trong khi chưa hội tụ thực hiện
13:    Tính logit H1:T và độ lớn ∥H1:T∥ của nó.
14:    Huấn luyện hθcls với logit được chuẩn hóa trong Phương trình 1.
15: kết thúc trong khi

Tuy nhiên, việc huấn luyện kéo dài của lớp phân loại có thể dẫn đến vấn đề quá tự tin, có thể làm tổn hại khả năng tổng quát hóa cho (các) tập kiểm tra. Để khắc phục điều này, chúng tôi lấy cảm hứng từ phát hiện ngoài phân phối (OOD) [47] và chuẩn hóa độ lớn của đầu ra mạng khi tính cross-entropy. Gọi H1:T = hθcls(F̂1:T) = [hθcls(F̂1), ..., hθcls(F̂|C1:T|)] := [l1, ..., l|C1:T|] biểu thị logit (tức là, đầu ra trước softmax) của F̂1:T, có thể được viết lại như tích của hai thành phần: H1:T = ∥H1:T∥ · H̃1:T, trong đó ∥ · ∥ biểu thị L2-norm. ∥H1:T∥ = √∑c∈C1:T ∥lc∥² đại diện cho độ lớn của H1:T, và H̃1:T đại diện cho hướng của nó. Sau đó chúng tôi áp dụng cross-entropy loss đã sửa đổi với chuẩn hóa logit để thực hiện căn chỉnh bộ phân loại:

L(θcls; F̂1:T) = -log(e^(ly/(τ∥H1:T∥)) / ∑c∈C1:T e^(lc/(τ∥H1:T∥))), (1)

trong đó ly biểu thị phần tử thứ y của H1:T tương ứng với nhãn thực tế y. τ là tham số nhiệt độ. Trực giác là việc chuẩn hóa H1:T với hằng số phụ thuộc đầu vào τ∥H1:T∥ sẽ không thay đổi kết quả của dự đoán softmax arg maxc∈C1:T(lc), trong khi buộc độ lớn ∥H1:T∥ trước softmax trở thành 1/τ có thể làm cho tiêu chí chỉ điều chỉnh hướng H̃1:T [47]. Do đó, chuẩn hóa trong Phương trình 1 có thể giảm thiểu vấn đề quá tự tin trong căn chỉnh bộ phân loại. Trong thực tế, chúng tôi quan sát rằng tham số nhiệt độ không nhạy cảm và thực nghiệm tìm thấy τ = 0.1 là lựa chọn hợp lý.

## 4. Thí nghiệm

Trong phần này, chúng tôi mô tả ngắn gọn các thiết lập thí nghiệm, và sau đó trình bày kết quả thí nghiệm.

### 4.1. Thiết lập Thí nghiệm

**Benchmark**: Theo L2P [46] và DualPrompt [45], chúng tôi áp dụng huấn luyện trước từ tập dữ liệu ImageNet-21K [32], còn được biết đến như ImageNet đầy đủ [6] bao gồm 14,197,122 ảnh với 21,841 lớp. Chúng tôi cũng xem xét huấn luyện trước từ tập dữ liệu ImageNet-1K [23], một tập con của ImageNet-21K được giới thiệu cho thử thách nhận dạng thị giác ILSVRC2012, bao gồm ảnh 1000-lớp.

Để đánh giá hiệu suất của học liên tục downstream, chúng tôi xem xét bốn tập dữ liệu benchmark tiêu biểu và ngẫu nhiên chia mỗi tập thành 10 tác vụ rời rạc: Hai tập đầu tiên theo các nghiên cứu trước [46, 45] và tương đối thô về phân loại, trong khi hai tập cuối tương đối chi tiết. Cụ thể, tập dữ liệu CIFAR-100 [22] bao gồm ảnh tự nhiên 100-lớp với 500 mẫu huấn luyện mỗi lớp. Tập dữ liệu ImageNet-R [15] chứa ảnh 200-lớp, chia thành 24,000 và 6,000 ảnh cho huấn luyện và kiểm tra (tỷ lệ tương tự cho mỗi lớp), tương ứng. Lưu ý rằng mặc dù các danh mục ảnh của ImageNet-R chồng lấp với ImageNet-21K, tất cả ảnh đều là mẫu ngoài phân phối cho tập dữ liệu huấn luyện trước, tức là các ví dụ khó từ ImageNet hoặc dữ liệu mới được thu thập với các phong cách khác nhau. Nó yêu cầu các thích ứng đáng kể của mô hình được huấn luyện trước, do đó phục vụ như một benchmark thách thức cho học liên tục. Tập dữ liệu CUB-200 [38] bao gồm ảnh chim 200-lớp với khoảng 60 ảnh mỗi lớp, 30 trong số đó được sử dụng cho huấn luyện và phần còn lại cho kiểm tra. Tập dữ liệu Cars-196 [21] bao gồm ảnh xe hơi 196 loại, chia thành 8,144 và 8,040 ảnh cho huấn luyện và kiểm tra (tỷ lệ tương tự cho mỗi lớp), tương ứng. Chúng tôi trình bày độ chính xác trung bình của tất cả các lớp sau khi học tác vụ cuối cùng, ký hiệu là Last-Acc (tương đương với "Avg. Acc" trong [46, 45]). Chúng tôi cũng tính độ chính xác trung bình của các lớp từng được nhìn thấy sau khi học mỗi tác vụ tăng dần và sau đó trình bày trung bình của chúng, ký hiệu là Inc-Acc.

**Triển khai**: Theo các nghiên cứu trước [46, 45], chúng tôi áp dụng backbone ViT-B/16 được huấn luyện trước cho tất cả baseline. Ngoài huấn luyện trước có giám sát, chúng tôi xem xét các mô hình tự giám sát tiêu biểu cung cấp checkpoint được huấn luyện trước trên ViT-B/16, tức là MoCo v3 [4], BEiT [2] và MAE [13]. Cho học liên tục của các tác vụ downstream, chúng tôi theo triển khai trước đó sử dụng optimizer Adam cho L2P [46] và DualPrompt [45] trong khi optimizer SGD cho các baseline khác, với cùng batch size 128. Học Chậm của chúng tôi áp dụng tỷ lệ học 0.0001 cho lớp biểu diễn và 0.01 cho lớp phân loại, khác với [46, 45] sử dụng 0.005 cho toàn bộ mô hình.

**Baseline**: Chúng tôi áp dụng huấn luyện kết hợp như hiệu suất giới hạn trên và xem xét các baseline học liên tục có hoặc không có replay các mẫu huấn luyện cũ. Đối với trường hợp trước, một bộ đệm bộ nhớ 1000 ảnh được duy trì, và chúng tôi đánh giá ba phương pháp dựa trên replay tiêu biểu như BiC [49], GDumb [30] và DER++ [3]. Đối với trường hợp sau, chúng tôi đánh giá các phương pháp dựa trên regularization tiêu biểu như EWC [19] và LwF [25], và các phương pháp dựa trên prompt như L2P [46] và DualPrompt [45]. Lưu ý rằng tinh chỉnh tuần tự thường phục vụ như hiệu suất giới hạn dưới của học liên tục, nhưng chúng tôi quan sát rằng việc đơn giản điều chỉnh tỷ lệ học (tức là, sử dụng Học Chậm) làm cho nó trở thành một baseline mạnh đáng ngạc nhiên.

### 4.2. Kết quả Thí nghiệm

**Hiệu suất Tổng thể**: Tất cả các phương pháp baseline trong Bảng 1, 2 được huấn luyện với Học Chậm của chúng tôi để so sánh công bằng, ngoại trừ các phương pháp dựa trên prompt giữ lớp biểu diễn cố định. Đối với học liên tục của các tác vụ phân loại tương đối thô, như Split CIFAR-100 và Split ImageNet-R trong Bảng 1 (cũng được hiển thị trong Hình 2), SL có thể cải thiện đáng kể hiệu suất của học liên tục. Với sự trợ giúp của Căn Chỉnh Bộ Phân Loại (CA) và Chuẩn Hóa Logit (LN) của nó, phương pháp của chúng tôi rõ ràng vượt trội hơn L2P [46] và DualPrompt [45], và gần như đạt được giới hạn trên huấn luyện kết hợp (khoảng cách hiệu suất ít hơn 2% cho huấn luyện trước có giám sát và 4% cho huấn luyện trước tự giám sát).

Đối với các benchmark học liên tục chi tiết trong Bảng 2, SLCA đạt được hiệu suất mạnh nhất cho huấn luyện trước có giám sát, trong khi cho huấn luyện trước tự giám sát hiệu suất tương đương hoặc hơi kém hơn phiên bản SL của BiC [49], sử dụng thêm một số mẫu huấn luyện cũ. Xin lưu ý rằng triển khai SLCA được trình bày có thể được xem như việc thêm CA+LN cùng với SL vào baseline tinh chỉnh tuần tự đơn giản nhất (Seq FT). Trên thực tế, SLCA được đề xuất có thể tự nhiên cắm và chạy với các phương pháp học liên tục khác. Chúng tôi để điều này như một nghiên cứu tiếp theo.

Đáng chú ý rằng các phương pháp dựa trên replay khác nhau (w/ SL) hoạt động khác nhau trong CLPM. Nói chung, BiC [49], cập nhật toàn bộ mô hình với các mẫu huấn luyện cũ theo cách tương tự như học các mẫu mới và thêm một lớp sửa lỗi bias để giảm thiểu mất cân bằng giữa các lớp cũ và mới, nhận được những cải thiện đáng kể nhất. Trong khi GDumb [30], đơn giản sử dụng các mẫu huấn luyện cũ để huấn luyện mô hình mới từ đầu tại thời điểm kiểm tra, gặp khó khăn trong việc thích ứng mô hình được huấn luyện trước với các mẫu huấn luyện hạn chế và do đó hoạt động tệ nhất.

**Nghiên cứu Ablation**: Chúng tôi trình bày một nghiên cứu ablation toàn diện về phương pháp của chúng tôi trong Bảng 3. Để chứng minh tính cần thiết của Học Chậm được đề xuất (SL), chúng tôi xem xét hai baseline: (1) tinh chỉnh tuần tự (Seq FT), áp dụng tỷ lệ học đồng nhất 0.005; và (2) Seq FT với θrps cố định, giữ lớp biểu diễn cố định và điều chỉnh liên tục lớp phân loại. Seq FT với θrps cố định nói chung vượt trội hơn Seq FT trong khi kém hơn đáng kể so với Seq FT w/ SL, cho thấy tính cần thiết của việc cập nhật lớp biểu diễn (nhưng với tỷ lệ học được giảm đúng cách để giảm thiểu vấn đề overfitting tiến triển).

Chúng tôi tiếp tục xác thận hiệu quả của Căn Chỉnh Bộ Phân Loại được đề xuất (CA) và Chuẩn Hóa Logit (LN) của nó. Đặc biệt, vì các benchmark học liên tục chi tiết làm trầm trọng nghiêm trọng vấn đề của lớp phân loại (Hình 5), lợi ích của SL vẫn đáng kể nhưng hơi giảm (ví dụ, cho huấn luyện trước có giám sát ImageNet-21K, các cải thiện của SL là 47.09%, 44.85%, 28.05% và 22.17% trên Split CIFAR-100, Split ImageNet-R, Split CUB-200 và Split Cars-196, tương ứng), trong khi lợi ích của CA+LN được tăng cường đáng kể (ví dụ, các cải thiện của CA+LN là 2.67%, 5.20%, 19.64% và 17.99% trên Split CIFAR-100, Split ImageNet-R, Split CUB-200 và Split Cars-196, tương ứng cho huấn luyện trước IN21K-Sup). Chúng tôi cũng đánh giá CA với biểu diễn θrps cố định, cũng đạt được những cải thiện nhất quán. Xin lưu ý rằng CA hoặc CA+LN của chúng tôi được vận hành theo cách hậu hoc thay vì căn chỉnh bộ phân loại trong quá trình học biểu diễn [56, 11, 12], vì cách sau thậm chí còn làm tồi tệ hơn hiệu suất (ví dụ, 27.89% trên Split CIFAR-100).

**Mô hình Huấn luyện Trước và Mức độ Chi tiết Downstream**: Ở đây chúng tôi phân tích hai yếu tố quan trọng cho CLPM được xác định trong kết quả của chúng tôi. So với huấn luyện trước có giám sát, huấn luyện trước tự giám sát thường dẫn đến khoảng cách hiệu suất lớn hơn giữa các baseline học liên tục và huấn luyện kết hợp (Hình 3, Bảng 1, 2). Mặc dù đề xuất của chúng tôi có thể cải thiện đáng kể hiệu suất của học liên tục, hiệu quả thay đổi tùy theo lựa chọn các phương pháp tự giám sát (Hình 3). Điều này là bởi vì chúng khác nhau về độ lớn của các cập nhật cần thiết để học tất cả các tác vụ tốt (Hình 4), dẫn đến các mức độ khó khăn khác nhau trong việc tinh chỉnh lớp biểu diễn.

Với việc lượng lớn dữ liệu cần thiết cho huấn luyện trước thường không được gán nhãn và được thu thập tăng dần, chúng tôi đề xuất các nghiên cứu tiếp theo phát triển các mô hình huấn luyện trước tự giám sát phù hợp hơn cho học liên tục downstream, và có thể sử dụng điều này như một tiêu chí để đánh giá tiến bộ của học tự giám sát. Mặt khác, khoảng cách hiệu suất tăng lên khi học liên tục downstream trở nên chi tiết hơn (Bảng 1, 2, 4.2). Điều này chủ yếu do lớp phân loại chưa tối ưu (Hình 5), có thể được cải thiện đáng kể bằng chiến lược căn chỉnh bộ phân loại của chúng tôi (Bảng 4.2).

**Khả năng Mở rộng**: Chúng tôi tiếp tục thảo luận về khả năng mở rộng của phương pháp chúng tôi. Đầu tiên, các đặc trưng được tạo chỉ được sử dụng để căn chỉnh lớp đầu ra tại thời điểm kiểm tra thay vì huấn luyện toàn bộ backbone, do đó tính toán hiệu quả và không tích lũy trong học liên tục (ví dụ, chỉ từ 0.67% đến 5% tổng thời gian chạy cho các benchmark khác nhau). Thứ hai, hiệp phương sai Σc có thể được đơn giản hóa thêm như phương sai σ²c ∈ Rd với suy giảm hiệu suất có thể chấp nhận được (ví dụ, chỉ lên đến 0.61% trên Split CIFAR-100 và 0.83% trên Split ImageNet-R). Theo cách này, việc lưu trữ µc và σ²c cho 100 lớp tương ứng với chỉ 0.18% tham số của backbone ViT-B/16, rõ ràng là nhẹ.

## 5. Kết luận

Học liên tục trên mô hình được huấn luyện trước (CLPM) yêu cầu chuyển giao hiệu quả kiến thức được huấn luyện trước cho mỗi tác vụ tăng dần trong khi duy trì khả năng tổng quát hóa cho các tác vụ tương lai. Tuy nhiên, việc sử dụng tỷ lệ học đồng nhất để cập nhật toàn bộ mô hình làm cho các baseline học liên tục truyền thống thất bại trong việc phù hợp với cả hai mục tiêu. Quan sát rằng việc sử dụng học viên chậm có thể gần như giải quyết vấn đề thách thức này trong lớp biểu diễn, chúng tôi cải thiện thêm lớp phân loại thông qua chiến lược căn chỉnh bộ phân loại. Qua nhiều kịch bản upstream và downstream khác nhau, đề xuất của chúng tôi cho phép baseline tinh chỉnh tuần tự đơn giản nhất gần như đạt được giới hạn trên huấn luyện kết hợp, vượt xa hiệu suất hiện đại hiện tại. Phương pháp đơn giản nhưng cực kỳ hiệu quả như vậy cung cấp một tiêu chí mạnh để đánh giá lại tiến bộ hiện tại và lộ trình kỹ thuật cho CLPM trong CV. Ngoài ra, nghiên cứu thực nghiệm của chúng tôi chứng minh hai yếu tố quan trọng cho CLPM, như mô hình huấn luyện trước và mức độ chi tiết downstream. Các nghiên cứu tiếp theo có thể khám phá thêm những hướng này và phát triển các chiến lược mạnh mẽ hơn dựa trên đề xuất của chúng tôi, để tận dụng tốt hơn kiến thức được huấn luyện trước cho học liên tục.

**Thảo luận về Hạn chế**. Mặc dù chúng tôi lập luận rằng huấn luyện trước tự giám sát có lợi thế ít quên hơn trong học liên tục upstream và đề xuất một phương pháp hiệu quả để khắc phục những thiếu sót của nó trong học liên tục downstream, việc làm thế nào để thực hiện học liên tục upstream và downstream cùng nhau vẫn cần được khám phá. Bên cạnh đó, kết quả của chúng tôi dựa trên backbone ViT được huấn luyện trước để so sánh công bằng với các nghiên cứu trước [46, 45], không xem xét các kiến trúc tiêu biểu khác như ResNet và các ứng dụng CL khác trên các tác vụ thị giác máy tính downstream như phát hiện đối tượng [35], phân đoạn ảnh [54, 55, 52]. Chúng tôi để việc khám phá nghiên cứu của chúng tôi trong các khía cạnh đã đề cập cho các nghiên cứu tương lai.
