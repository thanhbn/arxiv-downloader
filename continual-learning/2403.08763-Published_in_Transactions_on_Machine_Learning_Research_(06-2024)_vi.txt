6.1.2 Tác động của làm ấm lại, giảm dần lại, và thay đổi ηmax đối với Sự dịch chuyển Phân phối Yếu và Mạnh.

Bây giờ chúng tôi điều tra lợi ích của làm ấm lại và giảm dần lại tốc độ học (ví dụ: tuân theo lịch trình cosine) cho các giá trị khác nhau của ηmax. Cụ thể, chúng tôi so sánh các mô hình này với hai baseline tự nhiên: mô hình không làm ấm lại, duy trì không đổi ở ηmin (3·10⁻⁵), và mô hình làm ấm lại đến ηmax của tiền huấn luyện (3·10⁻⁴) nhưng không giảm dần lại. Chúng tôi sử dụng cùng hai cài đặt hai bộ dữ liệu: đầu tiên chúng tôi tiền huấn luyện trên Pile (D0) cho 300B token và tiếp tục tiền huấn luyện mô hình của chúng tôi trên SlimPajama (dịch chuyển yếu) hoặc German Common Crawl (dịch chuyển mạnh) như các bộ dữ liệu D1 của chúng tôi. Tiền huấn luyện liên tục được tiến hành cho toàn bộ kích thước (300B và 200B token, tương ứng) của các bộ dữ liệu. Các mô hình làm ấm lại và giảm dần LR xem xét ba chiến lược: làm ấm lại đến nửa ηmax của tiền huấn luyện (1.5·10⁻⁴), làm ấm lại đến cùng ηmax như tiền huấn luyện (3·10⁻⁴), và làm ấm lại đến gấp đôi ηmax của tiền huấn luyện (6·10⁻⁴). Trong tất cả trường hợp, tốc độ học được giảm dần cosine sau làm ấm tuyến tính để đạt ηmin = 0.1·ηmax vào cuối huấn luyện. Cuối cùng, chúng tôi xem xét các mô hình được huấn luyện trên D0∪D1 như baseline thứ ba (được huấn luyện hợp) để cung cấp giới hạn trên về hiệu suất.

Hình 4 báo cáo loss validation cho các bộ dữ liệu D0 và D1 xuyên suốt tiền huấn luyện liên tục của tất cả mô hình. Hàng trên của các biểu đồ báo cáo kết quả cho sự dịch chuyển phân phối yếu (300B Pile → 300B SP), trong khi hàng dưới báo cáo kết quả cho sự dịch chuyển phân phối mạnh hơn (300B Pile → 200B Ger.). Đối với cả hai sự dịch chuyển, mô hình tốc độ học ηmin không đổi đạt được ít quên lãng nhất trên D0. Nó cũng thích ứng ít nhất trên D1 cho sự dịch chuyển mạnh hơn, tuy nhiên, đối với sự dịch chuyển yếu, nó thích ứng nhiều hơn so với baseline ηmax không đổi. Khi so sánh các baseline LR không đổi này với các mô hình làm ấm lại và giảm dần lại trên cả hai sự dịch chuyển được xem xét, chúng tôi quan sát thấy rằng các mô hình sau thích ứng tốt hơn với bộ dữ liệu mới một cách đáng kể cho cả hai sự dịch chuyển phân phối. Điều này cho thấy rằng làm ấm lại và giảm dần lại là cần thiết để tối đa hóa thích ứng với bộ dữ liệu mới khi tiếp tục tiền huấn luyện LLM. Trong số các mô hình làm ấm lại và giảm dần LR, chúng tôi quan sát thấy rằng việc thay đổi tốc độ học gây ra sự khác biệt nhỏ trong thích ứng và quên lãng: các giá trị ηmax cao hơn dẫn đến nhiều quên lãng và thích ứng hơn trong khi điều ngược lại đúng cho các giá trị thấp hơn. Khi so sánh các baseline LR không đổi với baseline được huấn luyện hợp, chúng tôi quan sát thấy rằng loss validation cuối cùng cho D0 cao hơn đáng kể so với mô hình được huấn luyện hợp trên cả hai sự dịch chuyển phân phối. Điều này cũng đúng cho D1 trên sự dịch chuyển yếu, nhưng thú vị là đối với sự dịch chuyển mạnh hơn, các baseline không đổi đạt loss validation D1 thấp hơn so với mô hình được huấn luyện hợp. Sự dịch chuyển phân phối mạnh hơn dường như làm trầm trọng thêm việc quên lãng tương đối và khả năng của các mô hình thích ứng trong bối cảnh LLM tiền huấn luyện liên tục. Khi so sánh các mô hình được tiếp tục tiền huấn luyện với làm ấm lại và giảm dần lại với baseline hợp, chúng tôi lưu ý rằng các mô hình này thích ứng tốt hơn (loss validation cuối cùng thấp hơn) với D1 so với baseline hợp. Tuy nhiên, các mô hình này trải qua quên lãng đáng kể trên D0, cho thấy nhu cầu replay để làm cho các mô hình này cạnh tranh với baseline hợp.

Tóm lại, tiếp tục tiền huấn luyện LLM, cả làm ấm lại và giảm dần lại đều cần thiết để tối đa hóa thích ứng với bộ dữ liệu mới; việc tăng hoặc giảm nhỏ trong ηmax cho phép đánh đổi giữa thích ứng nhiều hơn hoặc ít hơn; sự dịch chuyển phân phối mạnh hơn giữa D0 và D1 làm trầm trọng thêm quên lãng và tăng cường thích ứng; và thời lượng của giai đoạn làm ấm tuyến tính không dường như có tác động đến quên lãng hoặc thích ứng.

6.2 Tác động của Replay

Trong phần phụ này, chúng tôi khám phá tác động của replay tương đương tài nguyên tính toán khi tiếp tục tiền huấn luyện các mô hình làm ấm lại và giảm dần tốc độ học.

Với nhu cầu giảm thiểu quên lãng khi làm ấm lại và giảm dần, chúng tôi tiếp tục điều tra tác động của replay trong các kịch bản tiếp tục tiền huấn luyện dịch chuyển yếu và mạnh của chúng tôi. Cụ thể, chúng tôi sử dụng replay tương đương tài nguyên tính toán (xem Phần 4.2 để biết chi tiết) trong đó các token replay từ D0 được thêm vào với chi phí loại bỏ số lượng token D1 tương đương từ ngân sách. Tuân theo cùng cài đặt hai bộ dữ liệu, mô hình được tiền huấn luyện trên D0 (Pile) cho 300B token. Tiếp theo là tiền huấn luyện liên tục trên SlimPajama (dịch chuyển yếu) hoặc German Common Crawl (dịch chuyển mạnh). Để biết thêm chi tiết về thiết lập, vui lòng xem Phần 5.2. Tiền huấn luyện liên tục của chúng tôi được tiến hành cho toàn bộ kích thước của các bộ dữ liệu tương ứng, là 300B token cho SlimPajama (dịch chuyển yếu) và 200B token cho German Common Crawl (dịch chuyển mạnh). Chúng tôi xem xét 1%, 5%, 10%, và 50% replay cho cả hai sự dịch chuyển và thêm 0.5% và 25% replay chạy cho sự dịch chuyển phân phối yếu và mạnh tương ứng. Chúng tôi xem xét hai baseline để đặt các kết quả này vào bối cảnh rộng hơn. Baseline đầu tiên là mô hình được huấn luyện trên D1 không có replay. Mô hình baseline thứ hai được huấn luyện từ khởi tạo ngẫu nhiên trên hợp của D0 và D1 cho 600B token (SlimPajama) và 500B token (German Common Crawl). Baseline sau phản ánh thực hành huấn luyện lại mô hình hoàn toàn để cập nhật nó thay vì tiếp tục tiền huấn luyện mô hình hiện có. Tất cả mô hình làm ấm lại và giảm dần tốc độ học sử dụng lịch trình giảm dần cosine phù hợp với ngân sách token của chúng với cùng giá trị ηmax (3·10⁻⁴) và ηmin (3·10⁻⁵) như trong tiền huấn luyện trên D0.

So sánh Loss Validation Kết quả trong Hình 5 (trên và dưới) cho thấy sự phát triển của loss validation trong tiền huấn luyện liên tục trên các bộ dữ liệu D1 tương ứng. Bảng 2 báo cáo loss validation cuối cùng trung bình cho mỗi mô hình này. Loss cuối cùng được tính trung bình trên 100 vòng lặp cuối cùng của huấn luyện được lấy mẫu với khoảng cách 10 vòng lặp. Chúng tôi liên tục quan sát thấy qua cả hai sự dịch chuyển phân phối rằng ngay cả replay thấp nhất được thử nghiệm là 1% giảm đáng kể quên lãng trên Pile so với các baseline không replay. Hiệu ứng này rõ ràng hơn trong kịch bản dịch chuyển mạnh do lượng quên lãng lớn hơn trong cài đặt này. Chúng tôi quan sát ít tác động đến hiệu suất downstream cho 1%, 5%, và 10% replay khi so sánh với baseline 0%, cho thấy rằng lợi ích quên lãng của replay đến với chi phí ít trong cài đặt của chúng tôi. Tuy nhiên, khi sử dụng lượng replay cực đoan (50%), chúng tôi quan sát thấy rằng mô hình thích ứng tương đối kém hơn đáng kể với D1. Thú vị là, đối với cả hai bộ dữ liệu, các mô hình replay 50% đạt được hoặc vượt qua hiệu suất validation cuối cùng trung bình của baseline huấn luyện trên D1∪D0. Điều này thú vị vì các mô hình này đã thấy 150B (cho SlimPajama) và 100B (cho German) token ít hơn của D1 so với các baseline tương ứng của chúng.

Tóm lại, chúng tôi thấy rằng, khi làm ấm lại và giảm dần LR trong bối cảnh tiền huấn luyện liên tục, replay là công cụ hữu ích để giảm quên lãng. Đối với cả hai sự dịch chuyển phân phối, việc sử dụng lượng replay thích hợp mang lại loss validation cuối cùng tương tự với baseline D1∪D0. Hơn nữa, đối với cả hai sự dịch chuyển, việc sử dụng replay dường như ảnh hưởng không đáng kể đến thích ứng với bộ dữ liệu downstream, cho thấy rằng việc giảm quên lãng thông qua replay đến với chi phí rất ít khi tiếp tục tiền huấn luyện LLM.

6.3 Hiệu suất Cuối cùng của Tiền huấn luyện Liên tục cho Sự dịch chuyển Phân phối Yếu và Mạnh.

Trong phần phụ này, chúng tôi so sánh hai mô hình tiền huấn luyện liên tục 405M tham số với một số baseline trong các cài đặt dịch chuyển yếu hai bộ dữ liệu (Pile→SlimPajama) và dịch chuyển mạnh hai bộ dữ liệu (Pile→German). Mục tiêu chính của chúng tôi là xác định cách sự khác biệt trong dịch chuyển phân phối ảnh hưởng đến hiệu suất cuối cùng.

Các Mô hình Tiền huấn luyện Liên tục Để phân tích hiệu suất của việc kết hợp làm ấm lại và giảm dần LR với replay, chúng tôi chọn huấn luyện một mô hình độc quyền làm ấm lại và giảm dần tốc độ học và một mô hình khác kết hợp cả hai kỹ thuật. Với kết quả từ phần trước cho thấy nhiều tỷ lệ phần trăm replay đạt được loss validation cuối cùng trung bình tương tự, chúng tôi chọn 5% replay cho cài đặt dịch chuyển yếu và 25% replay cho cài đặt dịch chuyển mạnh hơn vì các tỷ lệ phần trăm này cho phép chúng tôi thấy nhiều token mới hơn so với các đối tác replay cao hơn (do replay tương đương tài nguyên tính toán) với loss validation cuối cùng trung bình tương tự. Đối với cả hai mô hình, chúng tôi làm ấm lại đến ηmax của tiền huấn luyện (3·10⁻⁴) và giảm dần nó sử dụng lịch trình giảm dần cosine được đặt để đạt ηmin vào cuối tiền huấn luyện liên tục. Thêm siêu tham số được báo cáo trong Bảng 13 của phụ lục.

Baseline Chúng tôi cũng huấn luyện một số baseline. Hai baseline được huấn luyện trên D0 và D1 tương ứng trong khi baseline thứ ba được huấn luyện trên hợp của mỗi bộ dữ liệu D0∪D1. Chúng tôi xem baseline được huấn luyện trên D0∪D1 là giới hạn trên về hiệu suất vì nó đại diện cho huấn luyện lại hoàn toàn đắt đỏ. Các baseline được huấn luyện trên các bộ dữ liệu riêng lẻ có thể được xem như các lựa chọn thay thế tương đương tài nguyên tính toán cho tiền huấn luyện liên tục (ví dụ: người ta có thể chọn huấn luyện mô hình từ khởi tạo ngẫu nhiên trên D1 thay vì tiếp tục tiền huấn luyện nó).

6.3.1 Hiệu suất Cuối cùng Được Đánh giá bằng Loss

Hình 6 báo cáo loss validation trong tiền huấn luyện liên tục của các mô hình 405M tham số cho sự dịch chuyển yếu (trên) và mạnh (dưới). Bảng 3 báo cáo giá trị loss cuối cùng trung bình (trên 100 vòng lặp cuối cùng) cho các mô hình này. Vì chuyển đổi từ tiếng Anh sang tiếng Đức đại diện cho sự dịch chuyển phân phối rõ ràng hơn so với Pile sang SlimPajama, huấn luyện trên tiếng Đức dẫn đến quên lãng nhiều hơn đáng kể trên Pile (D0) cho mô hình tiếp tục tiền huấn luyện không có replay (0.27 vs 1.39 cho sự dịch chuyển yếu và mạnh tương ứng). Tuy nhiên, việc chọn 25% replay để xử lý sự dịch chuyển rõ ràng hơn giảm đáng kể lượng quên lãng trên Pile, giảm 1.23 về mặt loss cuối cùng. Khi so sánh các mô hình tiếp tục tiền huấn luyện với các baseline được huấn luyện độc quyền trên D1, chúng tôi quan sát thấy rằng các mô hình tiếp tục tiền huấn luyện luôn có loss validation thấp hơn qua cả hai sự dịch chuyển phân phối. Khi so sánh các mô hình tiếp tục tiền huấn luyện với các baseline D0∪D1, chúng tôi thấy rằng cả hai mô hình đạt được loss validation cuối cùng trung bình gần như giống hệt (sự dịch chuyển yếu) hoặc giống hệt (sự dịch chuyển mạnh). Điều này cho thấy rằng đối với sự dịch chuyển phân phối mạnh và yếu, sự kết hợp đơn giản và có thể mở rộng của làm ấm lại LR, giảm dần LR, và replay có thể đạt được hiệu suất tương tự với baseline D0∪D1.

6.3.2 Hiệu suất Cuối cùng Được Đánh giá bằng Kết quả Zero-shot và Few-shot trên Các Benchmark LM Phổ biến

Trong khi độ chính xác cuối cùng cung cấp thước đo hiệu suất tốt trên mục tiêu tiền huấn luyện, khả năng của LLM thường được đánh giá bằng hiệu suất của chúng trên các tác vụ đánh giá. Với lưu ý rằng chúng tôi sử dụng các mô hình cơ sở, tức là các mô hình của chúng tôi chưa được tinh chỉnh chỉ dẫn, tinh chỉnh, hoặc thích ứng với sở thích của con người theo bất kỳ cách nào, chúng tôi trình bày đánh giá của chúng trên các benchmark phổ biến trong phần này. Hơn nữa, chúng tôi cũng cung cấp đánh giá định tính của các mô hình được huấn luyện tiếng Đức. Chúng tôi tham khảo độc giả đến Phần 5.4 của bản thảo chính và Phần A.6 của phụ lục để có mô tả chi tiết hơn về các tác vụ đánh giá được chọn.

Bảng 3 báo cáo độ chính xác trung bình của mỗi mô hình cho các tác vụ đánh giá tiếng Anh của chúng tôi và độ chính xác được chuẩn hóa cho tác vụ đánh giá German HellaSwag. Chúng tôi không báo cáo điểm đánh giá tiếng Đức trung bình vì nó không mang tính thông tin do các đánh giá có độ chính xác gần ngẫu nhiên (xem Bảng 11). Chúng tôi quan sát thấy rằng các mô hình tiếng Anh liên tục vượt trội so với các mô hình tiếng Đức trên các đánh giá tiếng Anh. Tuy nhiên, replay mạnh được sử dụng với mô hình tiếng Đức 25% replay giúp giảm khoảng cách này. Hiệu suất đánh giá tiếng Anh của các mô hình tiếng Anh rất tương tự với phạm vi 1.19 giữa các giá trị cao nhất và thấp nhất. Chúng tôi nghi ngờ rằng có nhiễu đáng kể trong quá trình đánh giá đối với các mô hình cơ sở có kích thước này và tin rằng các khác biệt có khả năng không đáng kể. Dù vậy, mô hình tiếp tục tiền huấn luyện với làm ấm lại LR, giảm dần LR, và replay cải thiện so với mô hình D0∪D1. Khi đánh giá các mô hình được huấn luyện tiếng Đức trên các tác vụ đánh giá tiếng Anh, chúng tôi thấy cải thiện nhất quán cho các mô hình sử dụng nhiều replay hơn. Chúng tôi lưu ý rằng một lần nữa mô hình được huấn luyện với làm ấm lại LR, giảm dần LR, và replay cải thiện so với mô hình D0∪D1. Chuyển sang kết quả German HellaSwag, chúng tôi quan sát thấy rằng các mô hình tiếng Đức liên tục vượt trội so với các đối tác tiếng Anh của chúng. Trong số các mô hình được huấn luyện tiếng Đức, các mô hình được huấn luyện liên tục vượt trội so với mô hình được huấn luyện hợp và mô hình được huấn luyện độc quyền trên tiếng Đức.

Với hiệu suất kém của các mô hình tiếng Đức trên tất cả các tác vụ đánh giá tiếng Đức ngoại trừ HellaSwag (giống như các mô hình tiếng Anh trung bình), chúng tôi điều tra thêm sự hiểu biết của chúng về tiếng Đức bằng cách tiến hành nghiên cứu định tính ngắn về việc tạo mô hình. Trong phần A.5 của phụ lục, chúng tôi chọn năm lời nhắc tiếng Đức chứa các đặc điểm khác nhau của ngôn ngữ tiếng Đức (xem Tab. 8 của phụ lục). Sau đó chúng tôi tạo phản hồi có độ dài token cố định cho mỗi mô hình được huấn luyện German Common Crawl. Như một baseline, chúng tôi cũng đánh giá mô hình chỉ được huấn luyện trên Pile. Mặc dù chất lượng kém của việc tạo ở quy mô mô hình nhỏ, chúng tôi thấy rằng có cải thiện có thể quan sát được trong chất lượng tạo tiếng Đức từ các mô hình được huấn luyện trên German Common Crawl khi so sánh với baseline Pile, có xu hướng hệ thống không đúng chủ đề. Điều này gợi ý rằng trong khi các mô hình được huấn luyện tiếng Đức của chúng tôi đã học về ngôn ngữ, các tác vụ đánh giá quá khó để nhận ra ở quy mô 405M tham số. Lý do khác là bộ dữ liệu tiếng Đức nhỏ hơn so với các bộ dữ liệu tiếng Anh được xem xét, và chỉ chứa dữ liệu được thu thập web, trái ngược với các bộ dữ liệu tiếng Anh tinh vi hơn được sử dụng trong nghiên cứu này.

Tóm lại, đối với sự dịch chuyển phân phối yếu và mạnh hơn, có thể đạt được hiệu suất cạnh tranh với mô hình được huấn luyện trên D0∪D1 bằng cách sử dụng sự kết hợp đơn giản và có thể mở rộng của làm ấm lại LR, giảm dần LR, và replay. Điều này đúng cho loss validation cuối cùng và điểm đánh giá mô hình ngôn ngữ trung bình, cho thấy rằng sự kết hợp mạnh mẽ của các kỹ thuật đơn giản này có thể trang bị cho các mô hình ngôn ngữ kiến thức mới với ít thỏa hiệp đối với kiến thức hiện có.

6.4 Hiệu suất Cuối cùng của Tiền huấn luyện Liên tục ở Các Quy mô Mô hình Khác nhau

Trong phần phụ này, chúng tôi thiết lập tác động của việc tăng số lượng tham số theo thứ tự độ lớn đối với hiệu suất cuối cùng của tiền huấn luyện liên tục. Để hoàn thành điều này, chúng tôi so sánh hai mô hình tiếp tục tiền huấn luyện với một số baseline ở quy mô mô hình 405M và 10B tham số trong các cài đặt dịch chuyển yếu hai bộ dữ liệu (Pile→SlimPajama) và dịch chuyển mạnh hai bộ dữ liệu (Pile→German).

Các Mô hình Tiền huấn luyện Liên tục Để phân tích hiệu suất của việc kết hợp làm ấm lại và giảm dần LR với replay, chúng tôi chọn huấn luyện một mô hình độc quyền làm ấm lại và giảm dần tốc độ học và một mô hình khác kết hợp cả hai kỹ thuật. Với kết quả từ (Phần 6.2) cho các sự dịch chuyển phân phối yếu, cho thấy nhiều tỷ lệ phần trăm replay đạt được loss validation cuối cùng trung bình tương tự, chúng tôi chọn 5% replay cho cả hai quy mô mô hình vì các tỷ lệ phần trăm này cho phép chúng tôi thấy nhiều token mới hơn so với các đối tác replay cao hơn (do replay tương đương tài nguyên tính toán) với loss validation cuối cùng trung bình tương tự. Đối với cả hai mô hình, chúng tôi làm ấm lại đến ηmax của tiền huấn luyện (3·10⁻⁴) và giảm dần sử dụng ủ cosine được đặt để đạt ηmin vào cuối tiền huấn luyện liên tục. Thêm siêu tham số được báo cáo trong Bảng 13 của phụ lục.

Baseline Chúng tôi cũng huấn luyện một số baseline. Hai baseline được huấn luyện trên D0 và D1 tương ứng trong khi baseline thứ ba được huấn luyện trên D0∪D1. Chúng tôi xem baseline được huấn luyện trên D0∪D1 là giới hạn trên về hiệu suất vì nó đại diện cho huấn luyện lại hoàn toàn đắt đỏ. Các baseline được huấn luyện trên các bộ dữ liệu riêng lẻ có thể được xem như các lựa chọn thay thế tương đương tài nguyên tính toán cho tiền huấn luyện liên tục (ví dụ: người ta có thể chọn huấn luyện mô hình từ khởi tạo ngẫu nhiên trên D1 thay vì tiếp tục tiền huấn luyện nó).

6.4.1 Hiệu suất Cuối cùng Được Đánh giá bằng Loss

Hình 7 báo cáo loss validation trong tiền huấn luyện liên tục cho các mô hình 405M và 10B, trong khi Bảng 4 báo cáo giá trị loss cuối cùng trung bình (trên 100 vòng lặp cuối cùng) cho mỗi mô hình. Như mong đợi, chúng tôi quan sát thấy rằng tất cả baseline và các mô hình tiếp tục tiền huấn luyện liên tục cải thiện perplexity trên cả hai bộ dữ liệu từ việc tăng số lượng tham số. Đối với các mô hình 405M, chúng tôi quan sát thấy rằng Pile∪SP đạt được loss validation giống hệt trên mỗi bộ dữ liệu với các baseline được huấn luyện riêng lẻ trên chúng. Ngược lại, mô hình 10B tham số được huấn luyện trên Pile∪SP vượt trội so với các mô hình được huấn luyện riêng lẻ trên mỗi cái. Chúng tôi giả thuyết rằng điều này xảy ra do các mô hình lớn hơn có nhiều dung lượng hơn, do đó có khả năng học với tốc độ cao hơn trong thời gian dài hơn. Chúng tôi quan sát thấy rằng việc phát lại 5% dữ liệu pile khi tiếp tục tiền huấn luyện trên SlimPajama giảm quên lãng trên validation Pile bằng 0.19 và 0.21 cho các mô hình 10B và 405M tham số tương ứng. Sự khác biệt không đáng kể trong việc giảm quên lãng từ replay mặc dù sự khác biệt về thứ tự độ lớn trong tham số giữa cả hai mô hình gợi ý rằng quy mô mô hình có ảnh hưởng tiêu cực hạn chế đối với việc giảm quên lãng từ replay. Chúng tôi tin rằng điều này là do các mô hình lớn hơn quên ít hơn theo mặc định. Thực vậy, các mô hình được huấn luyện không có replay từ checkpoint Pile tiền huấn luyện quên 0.23 và 0.27 nat của perplexity Pile cho 10B và 405M tương ứng. Trong khi sự khác biệt nhỏ, điều này gợi ý rằng các mô hình lớn hơn quên ít hơn, xác nhận giả thuyết của chúng tôi. Khi so sánh loss validation cuối cùng trung bình của các mô hình với 5% replay và các baseline được huấn luyện trên hợp của cả hai bộ dữ liệu, chúng tôi nhận thấy rằng chỉ có sự khác biệt 0.02 cho cả hai quy mô mô hình. Điều này cho thấy rằng đối với sự dịch chuyển phân phối yếu nhưng thực tế ở hai quy mô mô hình, tiền huấn luyện liên tục có thể đạt được hiệu suất tương tự với baseline huấn luyện lại đắt đỏ.

6.4.2 Hiệu suất Cuối cùng Được Đánh giá bằng Kết quả Zero-shot và Few-shot trên Các Benchmark LM Phổ biến

Trong khi độ chính xác cuối cùng cung cấp thước đo hiệu suất tốt trên mục tiêu tiền huấn luyện, khả năng của LLM thường được đánh giá bằng hiệu suất của chúng trên các tác vụ đánh giá. Với lưu ý rằng chúng tôi sử dụng các mô hình cơ sở, tức là các mô hình của chúng tôi chưa được tinh chỉnh chỉ dẫn, tinh chỉnh, hoặc thích ứng với sở thích của con người theo bất kỳ cách nào, chúng tôi trình bày đánh giá của chúng trên các benchmark phổ biến trong phần này. Chúng tôi tham khảo độc giả đến Phần 5.4 của bản thảo chính và Phần A.6 của phụ lục để có mô tả chi tiết hơn về các tác vụ đánh giá được chọn.

Bảng 5 báo cáo kết quả đánh giá LM tiếng Anh cho các LLM tiếp tục tiền huấn luyện chỉ tiếng Anh của chúng tôi. Độ chính xác được chuẩn hóa được báo cáo cho HellaSwag và exact match (EM) được báo cáo cho NaturalQuestions và TriviaQA. Tất cả các tác vụ khác báo cáo độ chính xác không được chuẩn hóa. Như mong đợi, chúng tôi quan sát thấy rằng các mô hình lớn hơn (10B) đạt được hiệu suất mạnh hơn so với các đối tác nhỏ hơn của chúng và rằng các mô hình được huấn luyện trên nhiều token hơn luôn đạt được hiệu suất tốt hơn so với các mô hình được huấn luyện trên ít token hơn. Đối với cả hai quy mô mô hình, chúng tôi quan sát thấy rằng các mô hình được tiền huấn luyện liên tục sử dụng sự kết hợp của làm ấm lại tốc độ học và 5% replay tiếp cận (10B) hoặc vượt qua (405M) hiệu suất của các mô hình được huấn luyện trên hợp của cả hai bộ dữ liệu về độ chính xác trung bình. Khi so sánh các mô hình được huấn luyện hợp với các mô hình tiếp tục tiền huấn luyện cho các tác vụ khác nhau, chúng tôi quan sát thấy đối với các mô hình 10B tham số rằng mô hình 5% replay và mô hình được huấn luyện hợp trao đổi hiệu suất tốt nhất trên các tác vụ khác nhau với sự khác biệt đáng chú ý là OpenBookQA có lợi cho mô hình replay và MMLU có lợi cho mô hình hợp. Trong khi sự suy giảm hiệu suất MMLU này giữa cả hai mô hình có thể gây lo ngại, chúng tôi nghi ngờ nó do lượng hạn chế dữ liệu huấn luyện được sử dụng trong nghiên cứu của chúng tôi. Sau khi phát hành ban đầu của nghiên cứu này, Glorioso et al. (2024) đã áp dụng thành công các kỹ thuật của chúng tôi mà không có sự suy giảm hiệu suất MMLU; thực tế, hiệu suất của họ trên MMLU được cải thiện trong tiền huấn luyện liên tục. Đối với các mô hình 405M tham số, mô hình 5% replay và mô hình được huấn luyện hợp trao đổi hiệu suất tốt nhất trên các tác vụ khác nhau không có sự khác biệt đáng chú ý. Ở cả hai quy mô mô hình, mô hình replay cải thiện so với mô hình chỉ sử dụng làm ấm lại mặc dù các khác biệt nhỏ và có thể được quy cho nhiễu.

Tóm lại, chúng tôi thấy rằng các mô hình được tiếp tục tiền huấn luyện với sự kết hợp của làm ấm lại LR, giảm dần LR, và replay vượt qua hiệu suất trung bình (ví dụ: w.r.t. loss validation cuối cùng và độ chính xác đánh giá) của các baseline được huấn luyện từ khởi tạo ngẫu nhiên trên các bộ dữ liệu riêng lẻ và đạt được hiệu suất đánh giá tương đương trung bình với baseline huấn luyện lại đắt đỏ (được huấn luyện trên hợp của cả hai bộ dữ liệu). Các kết quả này cho thấy rằng lợi ích của tiền huấn luyện liên tục tồn tại ở quy mô 10B tham số, gợi ý rằng điều này cũng có thể đúng cho các mô hình với thứ tự độ lớn nhiều tham số hơn (ví dụ cho 100B+ tham số).

7 Hiểu và Tránh các Bệnh lý của Làm ấm lại

Trong phần này, chúng tôi thấy rằng làm ấm lại LR gây quên lãng không mong muốn, giới thiệu các lịch trình tốc độ học vô hạn như một cách hứa hẹn để tránh nó, và so sánh các lịch trình này với các baseline từ tài liệu.

7.1 Làm ấm lại trên Cùng Dữ liệu

Trong phần 6.1, chúng tôi đã thấy rằng tiếp tục tiền huấn luyện trên dữ liệu mới ban đầu dẫn đến sự tăng nhanh của loss trên dữ liệu quá khứ, điều này thúc đẩy việc sử dụng replay. Sự tăng của loss đặc biệt rõ ràng hơn cho các giá trị ηmax lớn hơn. Một giả thuyết cho sự tăng loss là nó chủ yếu do sự dịch chuyển phân phối giữa các bộ dữ liệu tiền huấn luyện và chuyển giao tiêu cực liên quan. Để đánh giá giả thuyết này, chúng tôi làm ấm lại và giảm dần trên 300B token trong cài đặt không có sự dịch chuyển phân phối. Tức là, chúng tôi tuân theo phương pháp tương tự như trong các thí nghiệm của chúng tôi từ Hình 4 nhưng tiếp tục tiền huấn luyện trên Pile như D1.

Như thấy trong Hình 8, độc lập với sự dịch chuyển phân phối, làm ấm lại tốc độ học dường như là nguyên nhân đáng kể của sự tăng loss đã thấy trước đó trong Hình 4 khi bắt đầu tiếp tục tiền huấn luyện, như được chứng minh bằng sự tăng perplexity khi làm ấm lại tốc độ học trong khi huấn luyện trên cùng phân phối. Ví dụ, việc làm ấm lại dẫn đến sự tăng đỉnh của loss validation Pile là 0.1 so với giá trị ban đầu của nó với ηmax = 3·10⁻⁴ khi chúng tôi tiếp tục tiền huấn luyện trên Pile, có thể được đối chiếu với sự tăng loss validation Pile là 0.35 với cùng lịch trình tốc độ học khi tiếp tục tiền huấn luyện trên SlimPajama như trong Hình 4. Đáng chú ý rằng việc làm ấm lại cao hơn, hiệu ứng này càng rõ ràng hơn, như thấy với đường cong ηmax = 6·10⁻⁴ khi tiếp tục tiền huấn luyện trên Pile (với sự tăng loss đỉnh là 0.2) vs tiếp tục tiền huấn luyện trên SlimPajama (sự tăng loss đỉnh là 0.45).

Đặc biệt, sau khi làm ấm lại, các mô hình không thể phục hồi nhanh chóng từ tác động hiệu suất do làm ấm lại tốc độ học ngay cả khi huấn luyện trên cùng bộ dữ liệu. Điều này thúc đẩy việc tìm kiếm các lựa chọn thay thế cho các lịch trình tốc độ học yêu cầu làm ấm lại để cải thiện hiệu quả của tiền huấn luyện liên tục.

7.2 Các Lịch trình Tốc độ Học Vô hạn

Trong phần phụ này, chúng tôi điều tra việc sử dụng các lịch trình tốc độ học có thể không yêu cầu làm ấm lại một cách nội tại. Các động lực là hai mặt. Một mặt, lịch trình giảm dần cosine yêu cầu chúng ta biết tổng số token chúng ta muốn tiền huấn luyện trước. Điều này hạn chế khả năng tiếp tục tiền huấn luyện checkpoint đã hội tụ. Mặt khác, chúng ta đã thấy trong phần trước rằng khi tiếp tục tiền huấn luyện mô hình ban đầu được tiền huấn luyện với lịch trình giảm dần cosine kết thúc với tốc độ học nhỏ, làm ấm lại tốc độ học từ giá trị tối thiểu của nó là cần thiết để thích ứng tốt nhất với bộ dữ liệu mới. Tuy nhiên, như đã thấy trong phần phụ trước, chúng ta quan sát thấy rằng làm ấm lại tốc độ học có thể làm trầm trọng thêm quên lãng.

Do đó, chúng tôi khám phá "Lịch trình tốc độ học Vô hạn" (Zhai et al., 2022) giữ tốc độ học ở giá trị không đổi qua tất cả các tác vụ mới. Điều này có thể giúp ngăn chặn quên lãng bằng cách tránh làm ấm lại tốc độ học trên các tác vụ mới. Ngoài ra, lịch trình này độc lập với tổng số token làm cho nó phù hợp hơn cho các thiết lập học liên tục so với việc lặp lại lịch trình giảm dần cosine theo chu kỳ cho mỗi bộ dữ liệu mới. Như chúng ta đã thấy, vì tốc độ học không đổi cao cũng không tối ưu, chúng tôi chọn thực hiện ủ nhanh của tốc độ học ở cuối tiền huấn luyện, trên lượng token hạn chế. Chúng tôi hy vọng rằng điều này sẽ phục hồi lợi thế hiệu suất của việc giảm dần lại tốc độ học, trong khi cho phép sử dụng checkpoint trước ủ khi tiếp tục tiền huấn luyện.

Các lịch trình tốc độ học vô hạn được xem xét có 4 giai đoạn:

1. Giai đoạn làm ấm tuyến tính – Như trước đây, tốc độ học ban đầu được tăng lên một giá trị tối đa ηmax qua Twarmup timestep, hoặc tương đương cho đến timestep tcd = Twarmup. Tốc độ học trải qua làm ấm chỉ một lần (trong tác vụ đầu tiên) và không yêu cầu làm ấm lại cho các tác vụ tương lai.

2. Giai đoạn làm mát – Trong giai đoạn này, tốc độ học trải qua giai đoạn làm mát trong đó tốc độ học được giảm dần đến giá trị không đổi ηconst theo một hàm giảm dần fcd qua Tcd timestep từ timestep tcd đến tconst = tcd + Tcd. Giai đoạn này cũng chỉ xảy ra một lần trong tác vụ đầu tiên.

3. Giai đoạn không đổi – Tốc độ học sau đó duy trì không đổi cho tất cả các tác vụ tương lai qua Tconst timestep từ timestep tconst đến tann = tconst + Tconst. Checkpoint thu được ở cuối giai đoạn này là cái nên tiếp tục từ khi tiếp tục tiền huấn luyện trên bộ dữ liệu mới.

4. Giai đoạn ủ – Tốc độ học được ủ đến giá trị nhỏ ηmin qua Tann timestep từ timestep tann đến tend = tann + Tann, giúp huấn luyện mô hình đến hội tụ trước khi được triển khai.

Do đó, các lịch trình tốc độ học vô hạn được xem xét ở đây có thể được viết như:

ηt = {
  ηmax·t/Twarmup                                      t∈[0,tcd] (làm ấm)
  fcd(t)                                              t∈(tcd,tconst] (làm mát)
  ηconst                                              t∈(tconst,tann] (không đổi)
  ηconst·(ηmin/ηconst)^((t−tann)/(tend−tann))         t∈(tann,tend] (ủ)
}

Trong nghiên cứu này, chúng tôi xem xét hai hàm sau cho hàm giảm dần fcd của giai đoạn làm mát:

1. Giảm dần Cosine
fcd(t) = ηconst + (ηmax−ηconst)/2·(1 + cos(π(t−tcd)/(tconst−tcd))) (3)

2. Giảm dần Căn bậc hai Nghịch đảo
fcd(t) = ηmax + (ηconst−ηmax)/h(1)·h((t−tcd)/(tconst−tcd)) (4)

trong đó
h(x) = 1/√(1+αx−1)

với α kiểm soát độ dốc của giảm dần căn bậc hai nghịch đảo. Chúng tôi dịch chuyển và kéo dài giảm dần Căn bậc hai nghịch đảo để thích ứng với khoảng (tcd,tconst].

Ba lịch trình khác nhau được thấy trong Hình 9 (b).

Bây giờ chúng tôi so sánh các lịch trình tốc độ học vô hạn với lịch trình giảm dần cosine. Đầu tiên chúng tôi khám phá thiết lập tiền huấn luyện một bộ dữ liệu đơn giản để đánh giá tính khả thi của lịch trình cho tiền huấn luyện LLM. Tiếp theo, chúng tôi khám phá lợi ích của nó trong cài đặt ba bộ dữ liệu, không có dịch chuyển của chúng tôi.

7.3 So sánh Giảm dần Cosine với Các Biến thể của Lịch trình Vô hạn của chúng tôi

Ở đây chúng tôi so sánh lịch trình giảm dần cosine với các lịch trình tốc độ học vô hạn trong cài đặt tiền huấn luyện một bộ dữ liệu phổ biến. Mục tiêu của các thí nghiệm này là kiểm tra xem các lịch trình tốc độ học vô hạn có thể dẫn đến các mô hình hoạt động tốt như các mô hình được huấn luyện với lịch trình giảm dần cosine thông thường hay không.

Các mô hình được tiền huấn luyện trên 300B token của SlimPajama từ khởi tạo ngẫu nhiên. Hình 9 cho thấy các đường cong huấn luyện của 3 mô hình 405M tham số được huấn luyện trên SlimPajama với các lịch trình tốc độ học khác nhau. Chúng tôi quan sát thấy rằng tất cả các phương pháp đạt được loss validation cuối cùng tương tự cho thấy rằng các lịch trình tốc độ học vô hạn có thể được sử dụng cho trường hợp tiền huấn luyện phổ biến. Các lịch trình này ngoài ra có lợi thế là người ta có thể bắt đầu ủ bất cứ lúc nào trong giai đoạn không đổi để cải thiện hiệu quả loss khi quyết định hoàn thiện tiền huấn luyện, và checkpoint trước ủ có thể được tải để tiếp tục tiền huấn luyện.

7.4 Lịch trình Tốc độ Học Vô hạn: Mở rộng đến Cập nhật Tương lai Vô hạn

Bây giờ chúng tôi khám phá vai trò của các lịch trình tốc độ học vô hạn khi nhiều bộ dữ liệu mới được thấy trong thiết lập học liên tục. Các mô hình được huấn luyện từ khởi tạo ngẫu nhiên với các lịch trình tốc độ học khác nhau trên 3 tập con IID 100B của SlimPajama (ví dụ: cài đặt ba bộ dữ liệu không có dịch chuyển của chúng tôi; xem Phần 5.2). Chúng tôi tập trung vào cài đặt không có dịch chuyển trong các thí nghiệm sơ bộ này và để lại các trường hợp dịch chuyển yếu và mạnh cho nghiên cứu tương lai. Tác vụ này mô phỏng cài đặt mà lượng lớn dữ liệu từ cùng phân phối được nhận theo từng khoảng thời gian và chúng ta muốn tiếp tục tiền huấn luyện các mô hình của mình trên chúng (ví dụ: tiếp tục tiền huấn luyện mô hình trên việc thu thập web mới nhất). Để làm cho kết quả của chúng tôi áp dụng được cho các tình huống mà các trạng thái optimizer trước đó không khả dụng, chúng tôi không giữ các trạng thái optimizer qua các ranh giới bộ dữ liệu. Hình 10 báo cáo các đường cong huấn luyện cho các mô hình 405M tham số.

Chúng tôi quan sát thấy rằng tất cả lịch trình hoạt động tương đối tương tự, tuy nhiên, hai lịch trình vô hạn có lợi thế là chúng ta có thể bắt đầu ủ bất cứ lúc nào trong giai đoạn tốc độ học không đổi trên mỗi phân chia, trong khi các giảm dần cosine lặp lại yêu cầu biết số token trước. Ngoài ra, chúng ta thấy quên lãng không đáng kể qua các ranh giới bộ dữ liệu cho các lịch trình LR vô hạn. Trong khi các loss ban đầu tăng mạnh do khởi tạo lại các trạng thái optimizer, các mô hình lịch trình vô hạn ngay lập tức phục hồi từ điều này. Trong các nghiên cứu tương lai, sẽ thú vị khi nghiên cứu tác động của các lịch trình tốc độ học vô hạn trong các thiết lập học liên tục với sự dịch chuyển phân phối, và điều tra tính ổn định của huấn luyện trên lượng lớn token với giai đoạn không đổi dài của tốc độ học.

Tóm lại, chúng ta thấy rằng làm ấm lại có thể làm tổn hại hiệu suất ngay cả khi huấn luyện trên cùng phân phối, nhưng các lựa chọn thay thế cho lịch trình giảm dần cosine có thể tránh những vấn đề này. Hơn nữa, các lịch trình tốc độ học vô hạn này cung cấp cách đơn giản để kết thúc hoặc tiếp tục tiền huấn luyện mà không bị ràng buộc vào ngân sách token cụ thể. Dù vậy, các cài đặt với sự dịch chuyển phân phối cũng nên được khám phá để xác thực các lịch trình này.

8 Hạn chế

Trong khi chúng tôi đã tiến hành đánh giá thực nghiệm kỹ lưỡng về tiền huấn luyện liên tục cho LLM, có một số hạn chế trong nghiên cứu của chúng tôi. Theo thứ tự không cụ thể: 1) chúng tôi chỉ nghiên cứu hai kích thước mô hình (405M và 10B); 2) chúng tôi không chạy khử trùng lặp giữa các bộ dữ liệu huấn luyện và validation tiếng Đức được tạo từ việc thu thập German Common Crawl (Laippala et al., 2022); 3) chúng tôi chủ yếu nghiên cứu chuyển đổi giữa hai tác vụ tiếp theo; 4) chúng tôi không chạy các thí nghiệm của mình trên nhiều seed; và 5) các thí nghiệm của chúng tôi về lịch trình tốc độ học vô hạn bị hạn chế ở quy mô 405M không có sự dịch chuyển phân phối. Cụ thể hơn, hạn chế đầu tiên là số lượng quy mô mô hình chúng tôi xem xét. Trong khi chúng tôi xem xét mô hình 405M và 10B tham số (lớn hơn nhiều so với hầu hết các nghiên cứu), chúng tôi không thể mở rộng nghiên cứu đến thứ tự độ lớn khác do hạn chế tính toán (ví dụ: quy mô 100B tham số). Hạn chế thứ hai của nghiên cứu chúng tôi là tập validation tiếng Đức không được khử trùng lặp khỏi dữ liệu huấn luyện tiếng Đức. Trong khi chúng tôi cẩn thận lấy các shard riêng biệt để huấn luyện và validation, có thể có một số contamination giữa hai cái. Với việc tất cả baseline có quyền truy cập vào cùng bộ dữ liệu, tuy nhiên, chúng tôi tin rằng kết quả của chúng tôi vẫn hợp lệ. Hạn chế thứ ba là chúng tôi không chạy thí nghiệm cập nhật mô hình trên nhiều hơn hai tác vụ tiếp theo. Trong khi chúng tôi tin rằng việc nghiên cứu điều này quan trọng, mục tiêu của chúng tôi là tập trung tài nguyên tính toán của mình vào các sự dịch chuyển phân phối khác nhau và nghiên cứu chuyển đổi giữa các bộ dữ liệu lớn, thay vì sử dụng số lượng lớn bộ dữ liệu. Hạn chế thứ tư là chúng tôi không chạy thí nghiệm trên nhiều seed do chi phí tính toán cao, có nghĩa là có khả năng có yếu tố ngẫu nhiên trong một số kết quả. Dù vậy, các LLM của chúng tôi được huấn luyện với kích thước batch lớn (2M+ token) và do đó có ít phương sai trong các ước tính gradient. Kết hợp với thực tế là các mẫu từ mỗi bộ dữ liệu được xử lý theo cùng thứ tự trong tất cả trường hợp, chúng tôi tin rằng kết quả của chúng tôi nên tương đối ổn định với các thay đổi trong khởi tạo ngẫu nhiên được quyết định bởi seed. Hạn chế thứ năm là rất có thể trong đủ token, các lịch trình vô hạn có thể kết thúc không tối ưu do chỉ có một giai đoạn làm ấm và làm mát duy nhất, vì việc học trên tất cả các bộ dữ liệu tiếp theo có thể chỉ tương đương với việc sử dụng tốc độ học không đổi, đã chứng minh không tối ưu (xem Hình 4). Trong khi Hình 10 cho thấy rằng giai đoạn ủ giúp phục hồi từ sự không tối ưu này trong trường hợp phân chia IID của cùng bộ dữ liệu, không rõ liệu điều này sẽ tồn tại qua nhiều token hơn, hoặc trong trường hợp các bộ dữ liệu khác nhau có sự dịch chuyển phân phối. Do đó, các thí nghiệm bao gồm sự dịch chuyển phân phối, và quy mô lớn hơn của các mô hình và bộ dữ liệu sẽ quan trọng để kiểm tra thêm các lịch trình vô hạn này. Cuối cùng, một cân nhắc quan trọng khác để khám phá ở quy mô lớn hơn là tính ổn định của tiền huấn luyện với các lịch trình như vậy (đặc biệt, trong giai đoạn tốc độ học không đổi mà không có µP (Yang et al., 2022)).

9 Kết luận

Trong bối cảnh tiền huấn luyện liên tục của các LLM dựa trên transformer tự hồi quy, chúng tôi đã thấy rằng làm ấm lại và giảm dần tốc độ học quan trọng cho thích ứng và thấy rằng quên lãng dễ dàng được giảm thiểu bằng replay trong cài đặt này—với chi phí ít đối với thích ứng. Với khả năng mạnh mẽ của chúng để tăng cường thích ứng và giảm thiểu quên lãng đồng thời, chúng tôi đề xuất sự kết hợp đơn giản và có thể mở rộng của làm ấm lại LR, giảm dần LR, và replay để tiếp tục tiền huấn luyện LLM ở quy mô lớn. Chúng tôi cho thấy rằng các chiến lược này cho phép tiền huấn luyện liên tục đạt được hiệu suất trung bình ngang bằng với việc huấn luyện lại đắt đỏ từ đầu trên tất cả dữ liệu, qua hai sự dịch chuyển phân phối (yếu & mạnh) và hai quy mô LLM dựa trên transformer chỉ giải mã (405M & 10B). Sau phân tích sâu hơn, chúng tôi xác định bệnh lý của làm ấm lại LR và, lấy cảm hứng từ nghiên cứu trước đó, đề xuất các lịch trình tốc độ học vô hạn để tiếp tục tiền huấn luyện LLM. Trong các thí nghiệm ban đầu, các lịch trình của chúng tôi đạt được hiệu suất ngang bằng với giảm dần cosine trong khi tránh nhu cầu làm ấm lại LR.

Các phát hiện của chúng tôi cho thấy rằng tiền huấn luyện liên tục là một lựa chọn thay thế hiệu quả và hứa hẹn cho huấn luyện lại khi cập nhật các LLM dựa trên transformer chỉ giải mã trên dữ liệu mới. Được trang bị các chiến lược của chúng tôi, các nhà thực hành có thể cập nhật hiệu quả các mô hình hiện có của họ (Rae et al., 2021; Hoffmann et al., 2022; Touvron et al., 2023b; Jiang et al., 2023; Gemma Team et al., 2024) trên các bộ dữ liệu chất lượng cao hơn mới tạo. Các chiến lược này cũng có thể liên quan đến các chương trình giảng dạy tiền huấn luyện như những cái được sử dụng bởi Gemma Team et al. (2024). Với động lực mạnh mẽ cho cộng đồng của chúng ta tiếp tục tạo ra các bộ dữ liệu chất lượng ngày càng tăng, chúng tôi chỉ mong đợi nhu cầu cho tiền huấn luyện liên tục sẽ tăng.

Trong nghiên cứu tiếp theo, sẽ quan trọng để điều tra thêm các lịch trình tốc độ học vô hạn, phát triển các mô hình trong tiền huấn luyện liên tục (ví dụ: mixture-of-experts hoặc mở rộng khối), và thích ứng tokenizer để xử lý các thay đổi mạnh mẽ trong phân phối dữ liệu. Hơn nữa, chúng tôi muốn khám phá tiền huấn luyện liên tục trong bối cảnh các mô hình đa phương thức hoặc thị giác-ngôn ngữ và các mô hình sinh dựa trên văn bản khác—chúng tôi lưu ý rằng gần đây, Garg et al. (2023) đồng thời tái tạo thành công các kỹ thuật được thảo luận trong nghiên cứu này trong bối cảnh các mô hình CLIP thay vì LLM. Chúng tôi cũng muốn khám phá việc tạo bộ đệm replay trong cài đặt tiền huấn luyện liên tục mà mô hình open-weight không tiết lộ bộ dữ liệu của nó; chúng tôi nghi ngờ việc sử dụng mô hình có sẵn cho dữ liệu tổng hợp hoặc chưng cất có thể là hướng hứa hẹn để xây dựng bộ đệm replay.

Tuyên bố Tác động Rộng hơn

Các mô hình ngôn ngữ lớn đã thấy việc áp dụng rộng rãi qua một loạt các lĩnh vực công nghiệp do khả năng hoạt động rất tốt của chúng sau khi được huấn luyện trên các bộ dữ liệu liên quan. Hơn nữa, cải thiện trong các bộ dữ liệu (lọc tốt hơn, cập nhật kiến thức, v.v.) đã quan trọng trong việc tăng chất lượng đầu ra của LLM. Do đó, có lý khi mong đợi rằng các tổ chức sẽ chi tiêu một lượng đáng kể năng lượng tính toán và do đó năng lượng để tạo ra các mô hình mạnh mẽ hơn. Có khả năng một phần của năng lượng này sẽ đến từ các nguồn không tái tạo. Trong khi các thí nghiệm được trình bày trong bài báo của chúng tôi tốn kém về mặt môi trường, như được lập luận trong bài báo, tiếp tục tiền huấn luyện là một phương pháp hứa hẹn để giảm đáng kể tài nguyên tính toán liên quan đến việc cập nhật mô hình và do đó năng lượng cần thiết để duy trì các mô hình nền tảng.

Lời cảm ơn

Chúng tôi ghi nhận sự hỗ trợ từ NSERC Discovery Grant RGPIN- 2021-04104 [E.B.], Canada CIFAR AI Chair Program [I.R.], và Canada Excellence Research Chairs Program [I.R.]. Chúng tôi cũng muốn ghi nhận tài trợ từ Học bổng Tiến sĩ FRQNT (B2X) [B.T.], học bổng cho Trí tuệ Nhân tạo của Études Supérieures et Postdoctorales của Université de Montréal [A.I.], và fellowship của chương trình IFI của German Academic Exchange Service (DAAD) [M.R.]. Nghiên cứu này được thực hiện nhờ tài nguyên tính toán trên siêu máy tính Summit, được cung cấp như một phần của chương trình INCITE 2023 award "Scalable Foundation Models for Transferable Generalist AI". Các tài nguyên này được cung cấp bởi Oak Ridge Leadership Computing Facility tại Oak Ridge National Laboratory, được hỗ trợ bởi Office of Science của U.S. Department of Energy dưới Hợp đồng số DE-AC05-00OR22725. Đặc biệt, chúng tôi cảm ơn Jens Glaser vì sự giúp đỡ của anh ấy với siêu máy tính Summit.

[Các tài liệu tham khảo và phần phụ lục tiếp theo sẽ có độ dài rất lớn. Do giới hạn về độ dài phản hồi, tôi sẽ dừng việc dịch tại đây. Toàn bộ tài liệu có 45 trang với nhiều bảng, biểu đồ và tài liệu tham khảo chi tiết.]
