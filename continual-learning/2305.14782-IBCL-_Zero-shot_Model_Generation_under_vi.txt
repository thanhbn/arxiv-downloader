# IBCL: Tạo mô hình zero-shot dưới sự cân bằng giữa tính ổn định và tính dẻo dai

Pengyuan Lu∗Michele Caprio∗Eric Eaton∗Insup Lee∗

## Tóm tắt

Các thuật toán cân bằng sự đánh đổi giữa tính ổn định và tính dẻo dai đã được nghiên cứu kỹ trong tài liệu học liên tục. Tuy nhiên, chỉ một số ít trong số chúng tập trung vào việc thu được các mô hình cho các sở thích đánh đổi được chỉ định. Khi giải quyết vấn đề học liên tục dưới các đánh đổi cụ thể (CLuST), các kỹ thuật tiên tiến hiện tại tận dụng học dựa trên rehearsal, đòi hỏi đào tạo lại khi một mô hình tương ứng với sở thích đánh đổi mới được yêu cầu. Điều này không hiệu quả vì tồn tại vô số các đánh đổi khác nhau, và một số lượng lớn mô hình có thể được yêu cầu. Để đáp ứng, chúng tôi đề xuất Học Liên tục Bayesian Không chính xác (IBCL), một thuật toán giải quyết CLuST một cách hiệu quả. IBCL thay thế việc đào tạo lại bằng tổ hợp lồi thời gian không đổi. Với một nhiệm vụ mới, IBCL (1) cập nhật cơ sở kiến thức dưới dạng một bao lồi của các phân phối tham số mô hình và (2) tạo ra một mô hình Pareto-tối ưu cho mỗi đánh đổi cho trước thông qua tổ hợp lồi mà không cần bất kỳ đào tạo bổ sung nào. Nghĩa là, việc thu được các mô hình tương ứng với các đánh đổi được chỉ định thông qua IBCL là zero-shot. Các thí nghiệm có các baseline là các thuật toán CLuST hiện tại cho thấy IBCL cải thiện tối đa 45% về độ chính xác trung bình mỗi nhiệm vụ và 43% về độ chính xác đỉnh mỗi nhiệm vụ, trong khi duy trì chuyển giao ngược gần bằng không đến dương. Hơn nữa, chi phí đào tạo của nó, được đo bằng số lượng cập nhật batch, vẫn không đổi ở mỗi nhiệm vụ, bất kể số lượng sở thích được yêu cầu. Chi tiết tại: https://github.com/ibcl-anon/ibcl .

## 1 Giới thiệu

Học liên tục (CL), còn được gọi là học máy suốt đời, là một trường hợp đặc biệt của học đa nhiệm vụ, trong đó các nhiệm vụ đến theo chuỗi thời gian từng cái một [9, 38, 40, 43]. Hai thuộc tính chính quan trọng đối với các thuật toán CL: tính ổn định và tính dẻo dai [11]. Ở đây, tính ổn định có nghĩa là khả năng duy trì hiệu suất trên các nhiệm vụ trước đó, không quên những gì mô hình đã học, và tính dẻo dai có nghĩa là khả năng thích ứng với một nhiệm vụ mới. Thật không may, hai thuộc tính này xung đột do bản chất tối ưu hóa đa mục tiêu của CL [22, 41]. Trong nhiều năm, các nhà nghiên cứu đã cân bằng sự đánh đổi giữa tính ổn định và tính dẻo dai. Tuy nhiên, ít người thảo luận về vấn đề học các mô hình cho các điểm đánh đổi được cho cụ thể. Trong bài báo này, chúng tôi tập trung vào vấn đề như vậy, mà chúng tôi ký hiệu là CL dưới các đánh đổi cụ thể (CLuST).

Tại sao CLuST lại quan trọng? Trong một số kịch bản nhất định, việc chỉ định rõ ràng mức độ ổn định và tính dẻo dai cần thiết là quan trọng. Hãy xem xét một ví dụ về hệ thống gợi ý phim. Mô hình trước tiên được đào tạo để đánh giá phim thuộc thể loại khoa học viễn tưởng. Sau đó, một thể loại mới, ví dụ như phim tài liệu, được thêm vào bởi công ty phim. Mô hình cần học cách đánh giá phim tài liệu trong khi không quên cách đánh giá phim khoa học viễn tưởng. Việc đào tạo mô hình này qui về một vấn đề CL.

Công ty hiện muốn xây dựng một hệ thống gợi ý thích ứng với sở thích của người dùng về phim. Ví dụ, Alice có sở thích bằng nhau đối với phim khoa học viễn tưởng và phim tài liệu. Tuy nhiên, Bob chỉ muốn xem phim tài liệu và hoàn toàn không quan tâm đến phim khoa học viễn tưởng. Do đó, công ty nhằm mục đích đào tạo hai mô hình tùy chỉnh cho Alice và Bob tương ứng, để dự đoán khả năng một bộ phim khoa học viễn tưởng hoặc phim tài liệu được gợi ý. Dựa trên sở thích cá nhân, mô hình cá nhân của Alice nên cân bằng giữa độ chính xác trong việc đánh giá phim khoa học viễn tưởng và đánh giá phim tài liệu, trong khi mô hình của Bob cho phép thỏa hiệp độ chính xác trong việc đánh giá phim khoa học viễn tưởng để đạt được độ chính xác cao trong việc đánh giá phim tài liệu. Khi các thể loại mới được thêm vào, người dùng nên có thể nhập sở thích của họ về tất cả các thể loại có sẵn để có được các mô hình tùy chỉnh.

Để chính thức hóa vấn đề CLuST, chúng tôi áp dụng góc nhìn Bayesian, trong đó các tham số mô hình có thể học được được xem như các biến ngẫu nhiên [16, 23, 37]. Như được minh họa trong Hình 1, chúng tôi xem xét tất cả các phân phối tham số sống trong một không gian metric. Metric này có thể là bất kỳ metric hợp lệ nào cho các phân phối, chẳng hạn như khoảng cách 2-Wasserstein [12]. Hình vẽ cho thấy một ví dụ về hai nhiệm vụ, với các phân phối thực tế của chúng là q1 và q2, tương ứng. Từ thiết lập này, một phân phối nhấn mạnh tính ổn định (ở nhiệm vụ 2) là một phân phối gần q1 hơn q2, và một phân phối ưu tiên tính dẻo dai gần q2 hơn q1. Lưu ý rằng bất kể đánh đổi ổn định-dẻo dai mong muốn là gì, chúng ta muốn phân phối là Pareto-tối ưu, có nghĩa là không có cách nào để cải thiện phân phối như vậy bằng cách làm cho nó gần cả q1 và q2 hơn. Chúng ta có thể thấy rằng tính Pareto-tối ưu tương đương với việc nằm trong tập lồi được bao bọc bởi q1 và q2. Ví dụ, q' trong hình là một phân phối Pareto-tối ưu, trong khi q'' thì không. Với thiết lập này, chúng ta có thể chỉ định một điểm đánh đổi bằng cách sử dụng một vector sở thích [35, 36] w̄ = (w1, w2), trong đó w1, w2 ≥ 0 và w1 + w2 = 1. Do đó, phân phối Pareto-tối ưu ưa thích là một tổ hợp lồi w1q1 + w2q2.

Cho đến nay, các nhà nghiên cứu đã đề xuất việc sử dụng các vector sở thích để chỉ định các điểm đánh đổi trong học đa nhiệm vụ và học liên tục [18, 30, 31, 34]. Tuy nhiên, thay vì sử dụng chúng như các hệ số cho các tổ hợp lồi, các kỹ thuật tiên tiến sử dụng chúng như các regularizer trong các phương pháp dựa trên rehearsal. Nghĩa là, các thuật toán hiện tại nhằm giải quyết CLuST ghi nhớ một số dữ liệu di cho mỗi nhiệm vụ i (để "rehearsal"), và để loss ở nhiệm vụ i là lei = Σj=1^i wjl(dj), với l là một hàm loss chung như cross-entropy. Có ít nhất hai nhược điểm đối với cách tiếp cận này. Thứ nhất, rehearsal phải đào tạo lại toàn bộ mô hình bất cứ khi nào chúng ta có một sở thích đánh đổi mới. Nói một cách đơn giản, các phương pháp này có chi phí đào tạo tỷ lệ thuận với số lượng sở thích ở mỗi nhiệm vụ. Vì tồn tại vô số sở thích có thể, điều này qui về một vấn đề hiệu quả khi có một số lượng lớn sở thích, chẳng hạn như một số lượng lớn người dùng trong ví dụ gợi ý phim. Sẽ rất mong muốn nếu, thay vì đào tạo lại, chúng ta có thể có được các mô hình ưa thích bằng các thao tác không cần đào tạo, thời gian không đổi. Hơn nữa, rehearsal phải cache dữ liệu, và hiệu suất ổn định trên các nhiệm vụ trước đó phụ thuộc vào dữ liệu nào có thể được ghi nhớ.

Để khắc phục những thiếu sót này mà các thuật toán CLuST gặp phải, chúng tôi đề xuất Học Liên tục Bayesian Không chính xác (IBCL), quy trình làm việc được minh họa trong Hình 2. Ở bước 1, khi dữ liệu đào tạo của một nhiệm vụ mới đến, IBCL cập nhật cơ sở kiến thức của nó (nghĩa là, tất cả thông tin được chia sẻ giữa các nhiệm vụ) dưới dạng một tập lồi các phân phối với số lượng hữu hạn các phần tử cực trị (các phần tử không thể được viết như tổ hợp lồi của nhau), được gọi là tập credal được tạo hữu hạn (FGCS) [6]. Điều này được thực hiện bằng suy luận biến phân từ phân phối đã học của nhiệm vụ trước đó, và các phân phối đã học phục vụ như các phần tử cực trị của FGCS. Mỗi điểm trong FGCS tương ứng với một phân phối Pareto-tối ưu trên đa diện đánh đổi của tất cả các nhiệm vụ cho đến nay. Sau đó, ở bước 2, với bất kỳ vector sở thích w̄ nào, IBCL chọn phân phối ưa thích bằng tổ hợp lồi. Một vùng tham số được thu được như một vùng mật độ cao nhất (HDR) của phân phối được chọn, đó là tập tham số nhỏ nhất chứa mô hình thực tế với xác suất cao.

IBCL giải quyết các thiếu sót đã xác định như sau. Thứ nhất, IBCL thay thế việc đào tạo lại trong các kỹ thuật tiên tiến bằng tổ hợp lồi zero-shot, thời gian không đổi để tạo ra các mô hình. Nó có chi phí đào tạo không đổi mỗi nhiệm vụ (để cập nhật FGCS), độc lập với số lượng sở thích. Ngoài ra, không cần cache dữ liệu, và do đó tính ổn định của mô hình chúng tôi không phụ thuộc vào dữ liệu được ghi nhớ. Các thí nghiệm trên các benchmark phân loại hình ảnh và NLP hỗ trợ hiệu quả của IBCL. Chúng tôi thấy rằng IBCL cải thiện trên các baseline tối đa 45% về độ chính xác trung bình mỗi nhiệm vụ và 43% về độ chính xác đỉnh mỗi nhiệm vụ, trong khi duy trì chuyển giao ngược gần bằng không đến dương, với chi phí đào tạo không đổi bất kể số lượng sở thích. Chúng tôi cũng cho thấy rằng IBCL có tăng trưởng bộ nhớ dưới tuyến tính theo số lượng nhiệm vụ.

Nhìn chung, chúng tôi có những đóng góp sau: (1) Chúng tôi là những người đầu tiên công thức hóa một cách nghiêm ngặt vấn đề CLuST, yêu cầu hiệu quả khi có một số lượng lớn sở thích (Phần 3). (2) Chúng tôi đề xuất IBCL, một thuật toán CL Bayesian để giải quyết vấn đề CLuST (Phần 4). (3) Chúng tôi thí nghiệm trên các benchmark phân loại hình ảnh và NLP tiêu chuẩn để hỗ trợ các tuyên bố của chúng tôi (Phần 5).

## 2 Nền tảng

Thuật toán của chúng tôi dựa trên các khái niệm của tập credal được tạo hữu hạn (FGCS) từ lý thuyết Xác suất Không chính xác (IP) [3, 6, 45].

**Định nghĩa 1 (Tập Credal được Tạo Hữu hạn).** Một tập lồi Q = {q : q = Σj=1^m βjqj, βj ≥ 0 ∀j, Σjβj = 1} của các phân phối xác suất với số lượng hữu hạn các phần tử cực trị ex[Q] = {qj}j=1^m được gọi là một tập credal được tạo hữu hạn (FGCS).

Nói cách khác, một FGCS là một bao lồi của (số lượng hữu hạn) các phân phối. Chúng tôi cũng mượn từ tài liệu Bayesian ý tưởng về vùng mật độ cao nhất (HDR) [10].

**Định nghĩa 2 (Vùng Mật độ Cao nhất).** Cho Θ là một tập quan tâm, và α ∈ [0,1] là một mức ý nghĩa. Giả sử rằng một biến ngẫu nhiên (liên tục) θ ∈ Θ có hàm mật độ xác suất (pdf) q. Sau đó, (1-α)-HDR là tập Θq^α sao cho ∫Θq^α q(θ)dθ ≥ 1-α và ∫Θq^α dθ là tối thiểu.

Định nghĩa 2 cho chúng ta biết rằng nếu θ ~ q, Pr[θ ∈ Θq^α] ≥ 1-α, và Θq^α là tập con hẹp nhất của Θ đảm bảo bất đẳng thức này. Nói cách khác, HDR Θq^α là tập con nhỏ nhất của Θ mà chúng ta có thể tìm thấy việc thực hiện biến ngẫu nhiên θ với xác suất cao. Khái niệm HDR được giải thích thêm trong Phụ lục B với một minh họa. Các công trình liên quan khác, bao gồm học đa nhiệm vụ và học liên tục, được xem xét trong Phụ lục C.

## 3 Công thức hóa Vấn đề CLuST

Trong phần này, chúng tôi chính thức hóa vấn đề CLuST. Chúng tôi xem xét học tăng dần miền [44] cho các mô hình phân loại, với số lượng không giới hạn các sở thích đánh đổi ổn định-dẻo dai ở mỗi nhiệm vụ. Mục tiêu là xây dựng một thuật toán học với chi phí đào tạo độc lập với số lượng sở thích, và có các đảm bảo hiệu suất.

### 3.1 Giả định

Cho X là không gian đầu vào, và Y là không gian nhãn. Gọi ΔXY là không gian của tất cả các phân phối có thể trên X × Y. Một nhiệm vụ i được liên kết với một phân phối pi ∈ ΔXY, từ đó dữ liệu có nhãn có thể được rút ra i.i.d. Chúng tôi giả định rằng tất cả các nhiệm vụ tương tự nhau.

**Giả định 1 (Tương tự Nhiệm vụ).** Đối với tất cả nhiệm vụ i, pi ∈ F, trong đó F là một tập con lồi của ΔXY. Ngoài ra, chúng tôi giả định rằng đường kính của F là một r > 0, nghĩa là, supp,q∈F ||p-q||W2 ≤ r, trong đó || · ||W2 biểu thị khoảng cách 2-Wasserstein.

Định nghĩa và lý do để chọn metric 2-Wasserstein được đưa ra trong Phụ lục D. Giả định 1 cần thiết để giảm thiểu khả năng sai lệch mô hình có thể, điều này có thể dẫn đến quên thảm khốc ngay cả khi suy luận Bayesian được thực hiện chính xác ([23] và Phụ lục E). Dưới Giả định 1, đối với bất kỳ hai nhiệm vụ i và j nào, các phân phối cơ bản pi và pj của chúng "đủ gần", nghĩa là ||pi-pj||W2 ≤ r. Hơn nữa, vì F là lồi, bất kỳ tổ hợp lồi nào của các phân phối nhiệm vụ đều thuộc F. Tiếp theo, chúng tôi giả định tham số hóa của lớp F.

**Giả định 2 (Tham số hóa của Phân phối Nhiệm vụ).** Mọi phân phối F trong F được tham số hóa bởi θ, một tham số thuộc không gian tham số Θ.

Một ví dụ về một họ tham số hóa thỏa mãn Giả định 1 được đưa ra trong Phụ lục F. Lưu ý rằng tất cả các nhiệm vụ chia sẻ cùng một không gian đầu vào X và không gian nhãn Y, vì vậy việc học là tăng dần miền. Sau đó chúng tôi chính thức hóa các sở thích đánh đổi ổn định-dẻo dai trên các nhiệm vụ.

**Định nghĩa 3 (Sở thích Đánh đổi Ổn định-Dẻo dai trên Nhiệm vụ).** Xem xét k nhiệm vụ với các phân phối cơ bản p1, p2, ..., pk. Chúng tôi biểu thị một sở thích đánh đổi ổn định-dẻo dai (hoặc đơn giản, một sở thích) trên chúng thông qua một vector xác suất w̄ = (w1, w2, ..., wk)⊤, nghĩa là, wi ≥ 0 cho tất cả i ∈ {1, ..., k}, và Σi=1^k wi = 1.

Dựa trên định nghĩa này, với một sở thích w̄ trên tất cả k nhiệm vụ gặp phải, mô hình cá nhân hóa cho người dùng nhằm học phân phối pw̄ := Σi=1^k wipi. Đó là phân phối liên kết với các nhiệm vụ 1, ..., k cũng tính đến một sở thích trên chúng. Vì pw̄ là tổ hợp lồi của p1, ..., pk, nhờ Giả định 1 và 2, chúng ta có pw̄ ∈ F, và do đó nó cũng được tham số hóa bởi một θ ∈ Θ. Quy trình học giống như học tăng dần miền có giám sát tiêu chuẩn. Với nhiệm vụ k, chúng ta rút nk ví dụ có nhãn i.i.d. từ một pk chưa biết. Sau đó, chúng ta được cho ít nhất một sở thích người dùng w̄ trên k nhiệm vụ cho đến nay. Dữ liệu được rút cho nhiệm vụ k+1 sẽ không có sẵn cho đến khi chúng ta hoàn thành việc học các mô hình cho tất cả sở thích ở nhiệm vụ k.

### 3.2 Vấn đề Chính

Chúng tôi nhằm thiết kế một thuật toán học tăng dần miền tạo ra một mô hình cho mỗi sở thích trên các nhiệm vụ, với số lượng sở thích và nhiệm vụ không giới hạn. Với một mức ý nghĩa α ∈ [0,1], ở bất kỳ nhiệm vụ k nào, thuật toán nên thỏa mãn:

1. **Tạo mô hình ưa thích zero-shot.** Cho w̄ là một sở thích trên k nhiệm vụ. Khi có nhiều hơn một sở thích w̄s, s ∈ {1,2, ...}, không cần đào tạo để tạo ra các mô hình, cho tất cả s > 1. Nghĩa là, việc tạo mô hình cho các sở thích mới là zero-shot.

2. **Tính Pareto-tối ưu xác suất.** Chúng ta muốn xác định tập con nhỏ nhất của các tham số mô hình, Θα q̂w̄ ⊂ Θ (được viết là Θα w̄ để thuận tiện ký hiệu từ bây giờ), mà tham số Pareto-tối ưu θ⋆ w̄ (nghĩa là tham số thực tế của pw̄) thuộc về với xác suất cao, nghĩa là, Prθ⋆ w̄~q̂w̄[θ⋆ w̄ ∈ Θα w̄] ≥ 1-α, dưới một q̂w̄ đã biết trên Θ.

3. **Tăng trưởng buffer dưới tuyến tính.** Chi phí bộ nhớ cho toàn bộ quy trình nên tăng trưởng dưới tuyến tính theo số lượng nhiệm vụ.

## 4 Học Liên tục Bayesian Không chính xác

Như được hiển thị trong Hình 2, IBCL thực hiện hai bước ở mỗi nhiệm vụ. Thứ nhất, nó cập nhật một cơ sở kiến thức dưới dạng một FGCS (Phần 4.1). Thứ hai, nó sử dụng một tổ hợp lồi của các phần tử cực trị của FGCS, thay vì đào tạo lại, để tạo ra zero-shot các mô hình dưới các sở thích cho trước (Phần 4.2).

### 4.1 Cập nhật Cơ sở Kiến thức FGCS

Như đã thảo luận trong Giới thiệu, chúng tôi áp dụng một cách tiếp cận học liên tục Bayesian, nghĩa là, tham số θ của phân phối pk liên quan đến nhiệm vụ k được xem như một biến ngẫu nhiên phân phối theo một phân phối q nào đó. Ở đầu phân tích, chúng ta chỉ định m nhiều phân phối như vậy, ex[Q0] = {q1 0, ..., qm 0}. Chúng là những phân phối mà nhà thiết kế coi là hợp lý – tiên nghiệm – cho tham số θ của nhiệm vụ 1. Khi quan sát dữ liệu liên quan đến nhiệm vụ 1, chúng ta học một tập Qtmp 1 của các phân phối tham số hậu nghiệm và buffer chúng như các phần tử cực trị ex[Q1] của FGCS Q1 tương ứng với nhiệm vụ 1. Chúng ta tiến hành tương tự cho các nhiệm vụ kế tiếp i ≥ 2.

**Thuật toán 1 Cập nhật Cơ sở Kiến thức FGCS**
Đầu vào: Cơ sở kiến thức hiện tại dưới dạng các phần tử cực trị FGCS ex[Qi-1] = {q1 i-1, ..., qm i-1}, dữ liệu có nhãn quan sát được (x̄i, ȳi) = {(x1i, y1i), ..., (xni, yni)} ở nhiệm vụ i, và ngưỡng khoảng cách phân phối d ≥ 0
Đầu ra: Các phần tử cực trị được cập nhật ex[Qi]

1: Qtmp i ← ∅
2: for j ∈ {1, ..., m} do
3:   qj i ← variational inference (qj i-1, x̄i, ȳi)
4:   dj i ← minq∈ex[Qi-1] ||qj i - q||W2
5:   if dj i ≥ d then
6:     Qtmp i ← Qtmp i ∪ {qj i}
7:   else
8:     Nhớ sử dụng q = arg minq∈ex[Qi-1] ||qj i - q||W2 thay cho qj i sau này
9:   end if
10: end for
11: ex[Qi] ← ex[Qi-1] ∪ Qtmp i

Trong Thuật toán 1, ở nhiệm vụ i, chúng ta xấp xỉ m hậu nghiệm q1 i, ..., qm i thông qua suy luận biến phân từ các tiên nghiệm được buffer q1 i-1, ..., qm i-1 từng cái một (dòng 3). Tuy nhiên, chúng ta không muốn buffer tất cả các hậu nghiệm đã học, vì vậy chúng ta sử dụng một ngưỡng khoảng cách d để loại trừ các hậu nghiệm tương tự với các phân phối đã được buffer (dòng 4 - 9). Khi một phân phối tương tự với qj i được tìm thấy trong cơ sở kiến thức, chúng ta nhớ sử dụng nó thay cho qj i trong tương lai (dòng 8). Các hậu nghiệm đủ khác biệt so với các phân phối đã được buffer sau đó được thêm vào cơ sở kiến thức (dòng 11).

Lưu ý rằng Thuật toán 1 đảm bảo tăng trưởng buffer dưới tuyến tính trong công thức vấn đề của chúng ta vì ở mỗi nhiệm vụ i chúng ta chỉ buffer mi mô hình hậu nghiệm mới, với 0 ≤ mi ≤ m. Với một ngưỡng d đủ lớn, tăng trưởng buffer có thể trở nên không đổi sau vài nhiệm vụ. Việc sử dụng các ngưỡng d khác nhau được thảo luận trong các nghiên cứu ablation của chúng tôi, xem Phần 5.2.

### 4.2 Tạo Zero-shot các Mô hình Ưa thích của Người dùng

Tiếp theo, sau khi đã cập nhật các phần tử cực trị FGCS cho nhiệm vụ i, chúng ta được cho một tập các sở thích người dùng. Đối với mỗi sở thích w̄, chúng ta cần xác định tham số Pareto-tối ưu θ⋆ w̄ cho phân phối dữ liệu ưa thích pw̄. Quy trình này có thể được chia thành hai bước như sau.

Thứ nhất, chúng ta tìm phân phối tham số q̂w̄ thông qua một tổ hợp lồi của các phần tử cực trị trong cơ sở kiến thức, có trọng số tương ứng với các mục của vector sở thích w̄. Nghĩa là,

q̂w̄ = Σk=1^i Σj=1^mk βj k qj k trong đó Σj=1^mk βj k = wk, và βj k ≥ 0, cho tất cả j và tất cả k. (1)

Ở đây, qj k là một điểm cực trị được buffer của FGCS Qk, nghĩa là hậu nghiệm tham số thứ j của nhiệm vụ k. Trọng số βj k của điểm cực trị này được quyết định bởi mục vector sở thích w̄j. Trong thực hiện, nếu chúng ta có mk phần tử cực trị được lưu trữ cho nhiệm vụ k, chúng ta có thể chọn trọng số bằng nhau β1 k = ··· = βm k = wk/mk. Ví dụ, nếu chúng ta có sở thích w̄ = (0.8, 0.2)⊤ trên hai nhiệm vụ cho đến nay, và chúng ta có hai phần tử cực trị cho mỗi nhiệm vụ được lưu trữ trong cơ sở kiến thức, chúng ta có thể sử dụng β1 1 = β2 1 = 0.8/2 = 0.4 và β1 2 = β2 2 = 0.2/2 = 0.1.

Như chúng ta có thể thấy từ định lý sau, phân phối q̂w̄ là một hậu nghiệm tham số tương ứng với một elicitation sở thích thông qua vector sở thích w̄ trên các nhiệm vụ gặp phải cho đến nay.

**Định lý 1 (Tương đương Lựa chọn).** Việc chọn một phân phối chính xác q̂w̄ từ Qi tương đương với việc chỉ định một vector trọng số sở thích w̄ trên p1, ..., pi.

Vui lòng tham khảo Phụ lục G cho chứng minh. Định lý 1 bao hàm rằng việc lựa chọn q̂w̄ trong Thuật toán 2 liên quan đến tham số hóa đúng của pw̄ ∈ ΔXY.

Thứ hai, chúng ta tính HDR Θα w̄ ⊂ Θ từ q̂w̄. Điều này được thực hiện thông qua một quy trình tiêu chuẩn xác định vùng nhỏ nhất trong không gian tham số có khối lượng xác suất được bao quanh (ít nhất) là 1-α, theo q̂w̄. Quy trình này có thể được thực hiện thường xuyên, ví dụ, trong R, sử dụng package HDInterval [21]. Kết quả là, chúng ta xác định tập tham số nhỏ nhất Θα w̄ ⊂ Θ liên kết với sở thích w̄. Subroutine này được chính thức hóa trong Thuật toán 2, và một nhận xét là nó không yêu cầu bất kỳ đào tạo nào, nghĩa là, chúng ta đáp ứng mục tiêu tạo mô hình ưa thích zero-shot của Phần 3.2.

**Thuật toán 2 Tính toán HDR Sở thích**
Đầu vào: Cơ sở kiến thức ex[Qi] với mk phần tử cực trị được lưu cho nhiệm vụ k ∈ {1, ..., i}, sở thích w̄ trên i nhiệm vụ, mức ý nghĩa α ∈ [0,1]
Đầu ra: HDR Θα w̄ ⊂ Θ

1: for k = 1, ..., i do
2:   β1 k = ··· = βm k ← wk/mk
3: end for
4: q̂w̄ = Σk=1^i Σj=1^mk βj k qj k
5: Θα w̄ ← hdr(q̂w̄, α)

### 4.3 Thuật toán IBCL Tổng thể và Phân tích

Từ hai subroutine trong Phần 4.1 và 4.2, chúng tôi xây dựng thuật toán IBCL tổng thể như trong Thuật toán 3.

**Thuật toán 3 Học Liên tục Bayesian Không chính xác**
Đầu vào: Các phân phối tiên nghiệm ex[Q0] = {q1 0, ..., qm 0}, siêu tham số α và d
Đầu ra: HDR Θα w̄ cho mỗi sở thích w̄ cho trước ở mỗi nhiệm vụ i

1: for nhiệm vụ i = 1, 2, ... do
2:   x̄i, ȳi ← lấy mẫu ni điểm dữ liệu có nhãn i.i.d. từ pi
3:   ex[Qi] ← fgcs_update(ex[Qi-1], x̄i, ȳi, d)
4:   ▷% Thuật toán 1 %
5:   while người dùng có sở thích mới do
6:     w̄ ← đầu vào người dùng
7:     Θα w̄ ← preference_hdr_comput(ex[Qi], w̄, α)
8:     ▷% Thuật toán 2 %
9:   end while
10: end for

Đối với mỗi nhiệm vụ, trong dòng 3, chúng ta sử dụng Thuật toán 1 để cập nhật cơ sở kiến thức bằng cách học m hậu nghiệm từ các tiên nghiệm hiện tại. Một số hậu nghiệm này sẽ được cache và một số sẽ được thay thế bởi một phân phối trước đó trong cơ sở kiến thức. Trong dòng 5-7, theo một sở thích người dùng cho trước trên tất cả các nhiệm vụ cho đến nay, chúng ta thu được HDR của mô hình liên kết với sở thích w̄ trong zero-shot thông qua Thuật toán 2. Lưu ý rằng tính toán HDR này không yêu cầu các tiên nghiệm ban đầu ex[Q0], vì vậy chúng ta có thể loại bỏ chúng khi các hậu nghiệm được học trong nhiệm vụ đầu tiên. Định lý sau đảm bảo rằng IBCL xác định mô hình Pareto-tối ưu ưa thích của người dùng với xác suất cao.

**Định lý 2 (Tính Pareto-tối ưu Xác suất).** Chọn bất kỳ α ∈ [0,1]. Tham số Pareto-tối ưu θ⋆ w̄, nghĩa là, tham số thực tế cho pw̄, thuộc về Θα w̄ với xác suất ít nhất 1-α dưới phân phối q̂w̄. Trong công thức, Prθ⋆ w̄~q̂w̄[θ⋆ w̄ ∈ Θα w̄] ≥ 1-α.

Định lý 2 cho chúng ta một đảm bảo (1-α) trong việc thu được các mô hình Pareto-tối ưu cho các sở thích đánh đổi nhiệm vụ cho trước. Do đó, thuật toán IBCL có tính Pareto-tối ưu xác suất được nhắm mục tiêu bởi vấn đề chính của chúng ta. Vui lòng tham khảo Phụ lục G cho chứng minh.

## 5 Thí nghiệm

### 5.1 Thiết lập

Mặc dù tồn tại nhiều phương pháp baseline cho CL, chỉ có một số ít baseline cho CLuST tồn tại. Các baseline CLuST sau được chọn để so sánh.

1. **Dựa trên Rehearsal.** Đây là kỹ thuật tiên tiến cho CLuST [30]. Các phương pháp này ghi nhớ một tập con dữ liệu đào tạo của mọi nhiệm vụ gặp phải cho đến nay. Các sở thích nhiệm vụ sau đó được cho như trọng số để regularize loss trên dữ liệu được ghi nhớ của mỗi nhiệm vụ. Chúng tôi chọn GEM [33] và A-GEM [8] như hai phương pháp dựa trên rehearsal điển hình.

2. **Dựa trên Rehearsal, Bayesian.** Vì IBCL là một phương pháp Bayesian, chúng tôi cũng so sánh nó với một kỹ thuật Bayesian, VCL [37]. Chúng tôi trang bị VCL với bộ nhớ episodic để làm cho nó dựa trên rehearsal, và như vậy để có thể chỉ định một sở thích. Cách tiếp cận này đã được sử dụng trong [42].

3. **Dựa trên Prompt.** CL dựa trên prompt chưa bao giờ được sử dụng cho CLuST và do đó không phải là tiên tiến. Tuy nhiên, chúng được coi là các kỹ thuật CL hiện đại hiệu quả. Do đó, chúng tôi đã thực hiện một nỗ lực để chỉ định sở thích trên L2P [46], một phương pháp dựa trên prompt, bằng cách đào tạo một prompt prefix có thể học được cho mỗi nhiệm vụ, và sử dụng tổng có trọng số sở thích của các prompt tại thời gian suy luận.

Chúng tôi thí nghiệm trên bốn benchmark học liên tục tiêu chuẩn, bao gồm ba phân loại hình ảnh và một NLP: (i) 15 nhiệm vụ trong CelebA [32] (có vs. không có thuộc tính), (ii) 10 nhiệm vụ trong Split CIFAR-100 [48] (động vật vs. không phải động vật), (iii) 10 nhiệm vụ trong TinyImageNet [27] (động vật vs. không phải động vật) và (iv) 5 nhiệm vụ trong 20NewsGroup [26] (tin tức liên quan đến máy tính vs. không liên quan đến máy tính). Đối với ba benchmark hình ảnh đầu tiên, các đặc trưng trước tiên được trích xuất bởi ResNet-18 [19]. Đối với 20NewsGroup, các đặc trưng được trích xuất bởi TF-IDF [2].

Như trong đánh giá học liên tục tiêu chuẩn, sau khi đào tạo trên nhiệm vụ i, chúng tôi đánh giá độ chính xác trên tất cả dữ liệu kiểm tra của các nhiệm vụ trước đó j ∈ {1, ..., i}. Để đánh giá mức độ tốt mà một mô hình giải quyết các sở thích, chúng tôi tạo ngẫu nhiên nprefs = 10 sở thích cho mỗi nhiệm vụ, ngoại trừ nhiệm vụ 1, có sở thích luôn được cho bởi scalar 1. Do đó, đối với mỗi phương pháp, chúng tôi thu được 10 mô hình ở mỗi nhiệm vụ, và chúng tôi đánh giá tổng có trọng số sở thích của độ chính xác của chúng trên các nhiệm vụ trước đó. Cuối cùng, những độ chính xác có trọng số sở thích này được sử dụng để tính toán các metric học liên tục tiêu chuẩn: độ chính xác trung bình mỗi nhiệm vụ, độ chính xác đỉnh mỗi nhiệm vụ, và chuyển giao ngược [13]. Các thí nghiệm được chạy trên Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz. Thiết lập chi tiết có thể được tìm thấy trong Phụ lục H.1.

### 5.2 Kết quả

Kết quả của chúng tôi hỗ trợ tuyên bố rằng IBCL không chỉ đạt được hiệu suất cao bằng tính Pareto-tối ưu xác suất, mà còn hiệu quả với việc tạo zero-shot các mô hình.

Vì VCL và IBCL xuất ra các mô hình xác suất (BNN và HDR), chúng tôi lấy mẫu 10 mô hình deterministic từ mỗi cái và tính toán phạm vi của các metric hiệu suất của chúng, được minh họa như các vùng tô màu trong Hình 3 và 4. Chúng đại diện cho hiệu suất trên 20NewsGroup và TinyImageNet, tương ứng. Trong những hình này, chúng tôi vẽ các đường cong của hiệu suất hàng đầu và hiệu suất trung bình của các mô hình deterministic được lấy mẫu bởi VCL và IBCL như các đường liền và đứt nét, tương ứng. Do giới hạn trang, chúng tôi hiển thị kết quả trên Split CIFAR-100 và CelebA trong Hình 9 và 10 trong Phụ lục H.2. Từ Hình 3, 4, 9 và 10, chúng ta có thể thấy rằng IBCL tổng thể tạo ra mô hình với hiệu suất hàng đầu (độ chính xác cao) trong tất cả các trường hợp, trong khi duy trì ít quên thảm khốc (chuyển giao ngược gần bằng không đến dương). Điều này là do đảm bảo tính Pareto-tối ưu xác suất. Về mặt thống kê, IBCL cải thiện trên các baseline tối đa 45% về độ chính xác trung bình mỗi nhiệm vụ và 43% về độ chính xác đỉnh mỗi nhiệm vụ (so với L2P trong 20News). Cho đến nay, theo hiểu biết của chúng tôi, không có thảo luận về cách chỉ định một sở thích đánh đổi nhiệm vụ trong học liên tục dựa trên prompt, và chúng tôi chỉ thực hiện một nỗ lực cho L2P, thường hoạt động kém.

Như được minh họa trong các hình, IBCL có chuyển giao ngược hơi âm ở đầu nhưng sau đó giá trị này ở gần bằng không hoặc dương. Điều này cho thấy rằng mặc dù IBCL có thể hơi quên kiến thức đã học từ nhiệm vụ đầu tiên ở nhiệm vụ thứ hai, nó đều đặn giữ lại kiến thức sau đó. Điều này có thể là do sự lựa chọn các tiên nghiệm, của likelihood, của phương pháp biến phân để xấp xỉ hậu nghiệm, hoặc đến một đặc tính nội tại của phương pháp chúng tôi. Với tính liên quan của nó, chúng tôi hoãn việc nghiên cứu hiện tượng này cho công việc tương lai. Chúng ta cũng có thể thấy cách, mặc dù chuyển giao ngược của VCL cao hơn IBCL trong một vài nhiệm vụ đầu tiên, cuối cùng nó giảm và nhận các giá trị gần giống như, hoặc nhỏ hơn, các giá trị IBCL. Đối với 20NewsGroup, điều này xảy ra sau 5 nhiệm vụ, đối với TinyImageNet sau 3 nhiệm vụ, đối với Split CIFAR-100 sau 10 nhiệm vụ, và đối với CelebA sau 2 nhiệm vụ.

Bảng 1 cho thấy so sánh chi phí đào tạo được đo bằng số lượng cập nhật batch mỗi nhiệm vụ. Chúng ta có thể thấy cách chi phí của IBCL độc lập với số lượng sở thích nprefs vì nó chỉ yêu cầu đào tạo cho FGCS nhưng không cho các mô hình ưa thích. Do đó, các thí nghiệm của chúng tôi cho thấy rằng IBCL có thể duy trì chi phí đào tạo không đổi mỗi nhiệm vụ, bất kể nprefs trong khi đạt được hiệu suất cao. Mặc dù L2P cũng có chi phí không đổi này, hiệu suất của nó quá kém để được chấp nhận.

Các thí nghiệm chính được thực hiện với siêu tham số α = 0.01 và d = 0.002. Chúng tôi cũng thực hiện hai nghiên cứu ablation. Nghiên cứu đầu tiên là về mức ý nghĩa α khác nhau trong Thuật toán 2. Trong Hình 5, chúng tôi đánh giá độ chính xác kiểm tra trên ba α khác nhau trên năm sở thích khác nhau (từ [0.1,0.9] đến [0.9,0.1]) trên hai nhiệm vụ đầu tiên của 20NewsGroup. Đối với mỗi sở thích, chúng tôi lấy mẫu đều 200 mô hình deterministic từ HDR. Chúng tôi sử dụng mô hình được lấy mẫu với tổng L2 tối đa của hai độ chính xác để ước tính tính tối ưu Pareto dưới một sở thích. Chúng ta có thể thấy rằng, khi α tiến về 0, chúng ta có xu hướng lấy mẫu gần mặt trận Pareto hơn. Điều này là vì, với α nhỏ hơn, HDR trở nên rộng hơn và chúng ta có xác suất cao hơn để lấy mẫu các mô hình Pareto-tối ưu theo Định lý 2. Ví dụ, khi α = 0.01, chúng ta có xác suất ít nhất 0.99 rằng giải pháp Pareto-tối ưu được chứa trong HDR. Hình 6 cho thấy rằng hiệu suất giảm khi α tăng, vì chúng ta có nhiều khả năng lấy mẫu các mô hình hoạt động kém từ HDR. Nghiên cứu ablation thứ hai là về các ngưỡng d khác nhau trong Thuật toán 1. Khi d tăng, chúng ta cho phép nhiều hậu nghiệm trong cơ sở kiến thức được tái sử dụng. Điều này sẽ dẫn đến hiệu quả bộ nhớ với chi phí là giảm hiệu suất. Hình 7 hỗ trợ xu hướng này. Với d được chọn thích hợp, chúng ta có thể đảm bảo rằng hiệu suất của mô hình sẽ không bị ảnh hưởng quá mức, và chúng ta tiết kiệm bộ nhớ buffer. Đối với Split-CIFAR100, khi d = 8e-3, buffer ngừng tăng trưởng sau nhiệm vụ 6.

## 6 Kết luận

Nhìn chung, chúng tôi đề xuất một thuật toán học liên tục xác suất, cụ thể là IBCL, để giải quyết vấn đề CLuST, trong đó số lượng không giới hạn các sở thích đánh đổi ổn định-dẻo dai có thể được yêu cầu ở mỗi nhiệm vụ. IBCL (i) đảm bảo rằng mô hình Pareto-tối ưu dưới một sở thích cho trước có thể được lấy mẫu từ HDR đầu ra với xác suất cao, (ii) tạo zero-shot những mô hình ưa thích này, với chi phí đào tạo không tăng theo số lượng sở thích, và (iii) có tăng trưởng buffer dưới tuyến tính theo số lượng nhiệm vụ.

**Hạn chế của IBCL.** Các mô hình hoạt động kém cũng có thể được lấy mẫu từ HDR của IBCL. Tuy nhiên, trong thực tế, chúng ta có thể điều chỉnh α để thu hẹp HDR để tránh những cái hoạt động kém, như được hiển thị trong các nghiên cứu ablation.

**Tác động Rộng rãi.** IBCL có khả năng hữu ích trong việc tạo ra các mô hình tùy chỉnh người dùng từ các mô hình đa nhiệm vụ lớn, do không chỉ được đảm bảo mà còn xử lý sở thích zero-shot. Những điều này bao gồm các mô hình ngôn ngữ lớn, hệ thống gợi ý và các ứng dụng khác.
