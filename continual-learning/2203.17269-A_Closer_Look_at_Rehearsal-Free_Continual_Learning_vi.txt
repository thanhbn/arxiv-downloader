Cái nhìn gần hơn về học liên tục không có luyện tập
James Seale Smith1*, Junjiao Tian1, Shaunak Halbe1, Yen-Chang Hsu2, Zsolt Kira1
1Viện Công nghệ Georgia, 2Nghiên cứu Samsung Mỹ

Tóm tắt
Học liên tục là một môi trường mà các mô hình học máy học các khái niệm mới từ dữ liệu huấn luyện liên tục thay đổi, đồng thời tránh suy giảm kiến thức về các lớp đã thấy trước đó có thể biến mất khỏi dữ liệu huấn luyện trong thời gian dài (một hiện tượng được gọi là vấn đề quên thảm khốc). Các phương pháp hiện tại cho học liên tục của một tác vụ mở rộng đơn lẻ (hay còn gọi là học liên tục tăng dần theo lớp) đòi hỏi luyện tập rộng rãi dữ liệu đã thấy trước đó để tránh suy giảm kiến thức này. Thật không may, luyện tập có chi phí về bộ nhớ và cũng có thể vi phạm quyền riêng tư dữ liệu. Thay vào đó, chúng tôi khám phá việc kết hợp chưng cất kiến thức và điều hòa tham số theo những cách mới để đạt được hiệu suất học liên tục mạnh mà không cần luyện tập. Cụ thể, chúng tôi nghiên cứu sâu vào các kỹ thuật học liên tục phổ biến: chưng cất dự đoán, chưng cất đặc trưng, điều hòa tham số L2, và điều hòa tham số EWC. Trước tiên, chúng tôi bác bỏ giả định phổ biến rằng các kỹ thuật điều hòa tham số thất bại đối với học liên tục không luyện tập của một tác vụ mở rộng đơn lẻ. Tiếp theo, chúng tôi khám phá cách tận dụng kiến thức từ một mô hình được huấn luyện trước trong học liên tục không luyện tập và thấy rằng điều hòa tham số L2 thông thường vượt trội hơn điều hòa tham số EWC và chưng cất đặc trưng. Cuối cùng, chúng tôi khám phá điểm chuẩn ImageNet-R gần đây phổ biến, và chỉ ra rằng điều hòa tham số L2 được triển khai trong các khối tự chú ý của transformer ViT vượt trội hơn các phương pháp gợi ý cho học liên tục phổ biến gần đây.

1. Giới thiệu
Các mô hình học sâu cho các ứng dụng học máy thường được huấn luyện ngoại tuyến trên một tập dữ liệu lớn, tĩnh. Sau đó mô hình được triển khai vào thế giới thực với giả định rằng phân phối dữ liệu mà nó sẽ gặp phải khớp với phân phối dữ liệu mà nó được huấn luyện trên đó. Thật không may, giả định này không đúng cho nhiều ứng dụng vì mô hình sẽ gặp phải sự thay đổi phân phối tự nhiên trong dữ liệu mục tiêu theo thời gian. Những thay đổi này dẫn đến suy giảm hiệu suất, đòi hỏi mô hình phải được thay thế.

Một cách để thay thế mô hình là thu thập thêm dữ liệu huấn luyện, kết hợp dữ liệu huấn luyện mới này với dữ liệu huấn luyện cũ, và sau đó huấn luyện lại mô hình từ đầu. Mặc dù điều này sẽ đảm bảo hiệu suất mô hình cao, nhưng nó không thực tế cho các ứng dụng quy mô lớn có thể đòi hỏi thời gian huấn luyện dài cho mô hình. Điều này có thể dẫn đến chi phí tài chính [24] và môi trường [32] cao sau nhiều lần thay thế. Thay vào đó, cách ưa thích là cập nhật mô hình theo cách hiệu quả nhất có thể. Cách đơn giản nhất để cập nhật mô hình là huấn luyện nó chỉ trên dữ liệu huấn luyện mới. Tuy nhiên, điều này dẫn đến một hiện tượng được gọi là quên thảm khốc [42], nơi mô hình ghi đè kiến thức đã có trước đó khi học dữ liệu mới. Điều này dẫn đến suy giảm hiệu suất nghiêm trọng, hay "quên", đối với phân phối dữ liệu huấn luyện đã học trước đó.

Nghiên cứu về quên thảm khốc được gọi là học liên tục. Trong môi trường này, một mô hình học tuần tự từ dữ liệu tác vụ mới trong khi tránh quên thảm khốc dữ liệu đã thấy trước đó. Dữ liệu tác vụ này thường chứa các thay đổi phân phối ngữ nghĩa (ví dụ, chúng ta gặp các lớp đối tượng mới) thay vì thay đổi phân phối hiệp biến (ví dụ, chúng ta gặp điều kiện ánh sáng hoặc nền mới). Mục tiêu của bài toán học liên tục là tìm chiến lược huấn luyện hiệu quả nhất để cập nhật các mô hình được huấn luyện tuần tự trên các chuỗi tác vụ này. Các chiến lược thường được đánh giá trên các chỉ số như hiệu suất tác vụ (ví dụ, độ chính xác phân loại cho bài toán phân loại), hiệu quả tính toán (ví dụ, thời gian huấn luyện), và hiệu quả bộ nhớ (ví dụ, số lượng tham số được lưu trữ).

Trong bài báo này, chúng tôi tập trung vào học liên tục trên một đầu phân loại mở rộng đơn lẻ. Điều này khác với môi trường học liên tục đa tác vụ, được gọi là học liên tục tăng dần theo tác vụ, nơi chúng ta học các đầu phân loại riêng biệt cho mỗi tác vụ (và nhãn tác vụ được cung cấp trong quá trình suy luận) [23]. Thật không may, các phương pháp SOTA cho học liên tục không có nhãn tác vụ đòi hỏi một tập con dữ liệu huấn luyện phải được lưu trữ hoặc tạo ra để trộn với dữ liệu tác vụ tương lai, một chiến lược được gọi là luyện tập. Nhiều ứng dụng không thể lưu trữ dữ liệu này vì chúng làm việc với dữ liệu người dùng riêng tư không thể được lưu trữ lâu dài. Ví dụ, một số công ty sẽ thu thập dữ liệu người dùng để cập nhật các mô hình trong thời gian ngắn (từ giờ đến ngày) nhưng dữ liệu này có thể có dấu thời gian và cần phải xóa.

Trong bài báo này, chúng tôi xem xét kỹ hơn các chiến lược không luyện tập cho học liên tục mà không lưu trữ dữ liệu huấn luyện. Thay vì đề xuất một phương pháp mới, chúng tôi đưa ra một góc nhìn mới thú vị và có tác động dựa trên các chiến lược hiện có. Cụ thể, chúng tôi bắt đầu bằng việc đặt câu hỏi: loại điều hòa nào (không gian tham số hay không gian dự đoán) là tốt nhất cho học liên tục không luyện tập? Chúng tôi cung cấp phân tích về cách các phương pháp này quên từ góc độ trôi đặc trưng, và chỉ ra rằng điều hòa tham số hiệu quả nhất trong việc giảm quên trong bộ mã hóa đặc trưng trong khi chưng cất dự đoán sử dụng sigmoid đa lớp thay vì softmax hiệu quả nhất trong việc giảm quên và thiên lệch trong đầu phân loại.

Thật không may, chúng tôi chỉ ra rằng khoảng cách giữa các phương pháp có luyện tập và không luyện tập vẫn còn lớn. Chúng tôi phỏng đoán rằng huấn luyện trước có thể giúp thu hẹp khoảng cách này, dẫn chúng tôi đến câu hỏi tiếp theo: loại điều hòa nào (không gian tham số hay không gian dự đoán) có thể tận dụng tốt nhất một mô hình được huấn luyện trước cho học liên tục không luyện tập? Chúng tôi ngạc nhiên thấy rằng, trong khi điều hòa L2 có độ chính xác thấp khi mô hình được khởi tạo ngẫu nhiên từ đầu, nó thực sự hoạt động tốt nhất trong môi trường huấn luyện trước này và đánh bại các phương pháp tinh vi hơn, bao gồm các phương pháp gợi ý cho học liên tục gần đây [54, 60, 61].

Cuối cùng, chúng tôi chỉ ra rằng một phương pháp đơn giản được rút ra từ những phát hiện của chúng tôi thậm chí có thể vượt trội hơn các phương pháp dựa trên luyện tập trên một điểm chuẩn học liên tục tiêu chuẩn. Tóm lại, chúng tôi đưa ra những phát hiện và đóng góp sau:

1. Chúng tôi cung cấp cái nhìn gần hơn về học liên tục không luyện tập với các thực hành tốt nhất, xác định rằng quên chủ yếu xảy ra trong các lớp sau. Biện pháp giảm thiểu hiệu quả nhất là thông qua điều hòa các dự đoán cuối cùng khi huấn luyện trước không có sẵn.

2. Chúng tôi mở rộng các nghiên cứu trên đến tình huống có huấn luyện trước và thấy rằng điều hòa tham số hiệu quả hơn điều hòa dự đoán, chỉ ra hiệu quả của các phương pháp có thể thay đổi đáng kể với các môi trường bài toán học liên tục.

3. Chúng tôi đạt được kết quả SOTA trong môi trường không luyện tập và thậm chí vượt trội hơn các phương pháp gợi ý cho học liên tục SOTA gần đây [54, 60, 61].

2. Bối cảnh và Nghiên cứu Liên quan

Học liên tục: Các phương pháp học liên tục có thể được tổ chức thành một vài danh mục rộng mà tất cả đều hữu ích tùy thuộc vào môi trường bài toán và các ràng buộc. Một nhóm phương pháp mở rộng kiến trúc của mô hình khi gặp các tác vụ mới; những phương pháp này rất hiệu quả cho các ứng dụng mà mô hình phát triển cùng với các tác vụ là thực tế [14,34,37,40,51]. Chúng tôi không xem xét các phương pháp này vì các tham số mô hình tăng theo số lượng tác vụ, nhưng thừa nhận rằng các đóng góp của chúng tôi có thể được kết hợp vào các phương pháp này.

Một phương pháp khác là điều hòa mô hình đối với kiến thức tác vụ trong quá khứ trong khi huấn luyện tác vụ mới. Điều này có thể được thực hiện bằng cách điều hòa mô hình trong không gian trọng số (tức là, phạt các thay đổi đối với tham số mô hình) [2, 13, 29, 55, 66] hoặc không gian dự đoán (tức là, phạt các thay đổi đối với dự đoán mô hình) [1,6,21,33,36]. Điều hòa kiến thức trong không gian dự đoán được thực hiện bằng cách sử dụng chưng cất kiến thức [20] và đã được tìm thấy hoạt động tốt hơn các phương pháp dựa trên điều hòa mô hình cho học liên tục khi nhãn tác vụ không được đưa ra [35, 57].

Luyện tập với dữ liệu được lưu trữ [3–5,7,8,15,16,22,28,39,47–49, 59] hoặc mẫu từ một mô hình tạo sinh [26, 27, 43, 52, 56] rất hiệu quả khi lưu trữ dữ liệu huấn luyện hoặc huấn luyện/lưu một mô hình tạo sinh là có thể. Thật không may đối với nhiều ứng dụng học máy, lưu trữ dữ liệu huấn luyện lâu dài sẽ vi phạm quyền riêng tư dữ liệu, cũng như phát sinh chi phí bộ nhớ lớn. Đối với mô hình tạo sinh, quá trình huấn luyện này tốn nhiều tính toán và bộ nhớ hơn so với mô hình phân loại và có thể vi phạm các mối quan tâm về tính hợp pháp của dữ liệu vì sử dụng mô hình tạo sinh tăng khả năng ghi nhớ dữ liệu có thể nhạy cảm [41]. Điều này thúc đẩy chúng tôi làm việc trên môi trường quan trọng của các phương pháp không luyện tập để giảm thiểu quên thảm khốc.

Học liên tục không luyện tập trực tuyến: Các nghiên cứu khác đã xem xét học liên tục không luyện tập từ góc độ học "streaming" trực tuyến sử dụng một mô hình được huấn luyện trước, đông lạnh [17,38]. Trong khi các nghiên cứu này tập trung vào học trực tuyến hiệu quả từ một không gian đặc trưng cố định, đông lạnh, chúng tôi thay vào đó phân tích các mô hình không đông lạnh được phép huấn luyện "đến hội tụ" trên dữ liệu tác vụ (như thường thấy đối với học liên tục ngoại tuyến [63]). Do đó, môi trường của chúng tôi rất khác với các nghiên cứu này.

Các phương pháp dựa trên nguyên mẫu cho học liên tục: Các nguyên mẫu có thể được tận dụng cho học liên tục như một phương tiện để tránh quên thảm khốc mà không lưu trữ dữ liệu. Các phương pháp gần đây học một không gian đặc trưng cho các nguyên mẫu với các phương pháp như học một mạng nhúng [65] hoặc tận dụng các tăng cường mạnh cho học tự giám sát [62, 68]. Trong khi học các nguyên mẫu trong một mạng nhúng [65] có thể giảm thiểu quên tốt hơn so với phân loại cross-entropy, chúng tôi tránh các phương pháp như vậy vì huấn luyện một mạng nhúng với học metric thường có thể là một thách thức khó [68]. Trong khi tận dụng tự giám sát mạnh để tăng cường dữ liệu và nguyên mẫu có thể đạt được hiệu suất SOTA cho học liên tục không luyện tập [62,68], không rõ liệu sự gia tăng hiệu suất là do giảm thiểu quên hay có các đặc trưng tốt hơn nói chung do tập dữ liệu mở rộng của các tăng cường dữ liệu mạnh [11]. Ngoài ra, các phương pháp này đòi hỏi một tác vụ đầu tiên lớn để học một không gian đặc trưng ban đầu mạnh (điều này không phải lúc nào cũng hợp lệ). Tóm lại, trong khi các chiến lược tiên tiến này hoạt động tốt khi không có dữ liệu được lưu trữ, chúng tôi thay vào đó đưa ra nghiên cứu của mình như một góc nhìn khác về các chiến lược đơn giản, hiện có, được áp dụng rộng rãi thay vì một phương pháp phức tạp, SOTA đòi hỏi các giả định bổ sung (ví dụ, có một tác vụ đầu tiên lớn).

Học liên tục không luyện tập: Các nghiên cứu gần đây học các gợi ý trong một mô hình transformer được huấn luyện trước, đông lạnh cho học liên tục [54, 60, 61]. Mặc dù hiệu quả, phương pháp này giả định rằng dữ liệu trong chuỗi học liên tục có thể được tách biệt với một bộ mã hóa được huấn luyện trước; vì giả định này thường không hợp lệ, việc hiểu cách các phương pháp dựa trên tinh chỉnh quên trong môi trường không luyện tập vẫn rất được mong muốn. Các nghiên cứu khác đề xuất tạo ra hình ảnh cho luyện tập bằng cách sử dụng nghịch đảo mô hình sâu [9, 53, 64]. Trong khi các phương pháp này hoạt động tốt so với các phương pháp mô hình tạo sinh và đơn giản là luyện tập từ một số lượng nhỏ hình ảnh được lưu trữ, chúng tôi lập luận rằng các phương pháp này có rủi ro tương tự như các phương pháp tạo sinh. Cụ thể, nghịch đảo mô hình là một quá trình chậm liên quan đến chi phí tính toán cao trong môi trường học liên tục [53] và nghịch đảo hình ảnh từ một mô hình được huấn luyện cũng có thể vi phạm các mối quan tâm về quyền riêng tư dữ liệu tương tự [25]. Điều này thúc đẩy chúng tôi đặt câu hỏi: "làm thế nào chúng ta có thể hoàn toàn loại bỏ luyện tập bao gồm dữ liệu huấn luyện được lưu trữ, được huấn luyện, hoặc được nghịch đảo?"

3. Kiến thức cơ bản

Học liên tục: Trong học liên tục, một mô hình được hiển thị dữ liệu có nhãn tương ứng với M lớp đối tượng ngữ nghĩa c1, c2, . . . , cM qua một chuỗi N tác vụ tương ứng với các tập con không chồng chéo của các lớp. Chúng tôi sử dụng ký hiệu Tn để biểu thị tập hợp các lớp được giới thiệu trong tác vụ n, với |Tn| biểu thị số lượng lớp đối tượng trong tác vụ n. Mỗi lớp chỉ xuất hiện trong một tác vụ duy nhất, và mục tiêu là học tăng dần để phân loại các lớp đối tượng mới khi chúng được giới thiệu trong khi duy trì hiệu suất trên các lớp đã học trước đó. Để mô tả mô hình suy luận của chúng tôi, chúng tôi ký hiệu θi,n là mô hình θ tại thời điểm i đã được huấn luyện với các lớp từ tác vụ n. Ví dụ, θn,1:n đề cập đến mô hình được huấn luyện trong tác vụ n và các đầu phân loại tuyến tính liên quan đến tất cả các tác vụ lên đến và bao gồm lớp n. Chúng tôi bỏ chỉ số thứ hai khi mô tả mô hình được huấn luyện trong tác vụ n với tất cả các đầu phân loại tuyến tính (ví dụ, θn).

Trong bài báo này, chúng tôi đối phó với môi trường học liên tục tăng dần theo lớp thay vì môi trường học liên tục tăng dần theo tác vụ. Học liên tục tăng dần theo lớp là thách thức vì người học phải hỗ trợ phân loại trên tất cả các lớp đã thấy đến tác vụ n [23] (tức là, không có nhãn tác vụ nào được cung cấp cho người học trong quá trình suy luận). Học liên tục tăng dần theo tác vụ là một môi trường đa tác vụ đơn giản hơn nơi các nhãn tác vụ được đưa ra trong cả quá trình huấn luyện và suy luận.

4. Điều hòa không luyện tập

Khi huấn luyện trên một tác vụ mới n, chìa khóa để giảm thiểu quên là chuyển giao kiến thức từ một mô hình "checkpoint", θn−1 (được sao chép và đông lạnh ở cuối tác vụ n−1), vào mô hình đang được cập nhật, θn. Trong phần này, chúng tôi trước tiên xem xét ba cách cổ điển để chuyển giao kiến thức trong học liên tục có thể được mô tả là "không luyện tập". Các phương pháp này được minh họa trong Hình 1, và chúng tôi khuyến khích người đọc tham khảo lại Hình 1 trong suốt quá trình đọc phần này. Sau đó chúng tôi lập luận rằng một trong những phương pháp này, chưng cất dự đoán, quan trọng hơn để chuyển giao kiến thức từ bộ phân loại của mô hình, trong khi hai phương pháp khác, điều hòa tham số và chưng cất đặc trưng, quan trọng hơn để chuyển giao kiến thức từ bộ mã hóa đặc trưng của mô hình. Chúng tôi sẽ sử dụng phần này như một nền tảng để thúc đẩy và hiểu các phát hiện được trình bày trong Phần 5.

4.1. Điều hòa không gian tham số

Một trong những phương pháp sớm nhất cho học liên tục, EWC, đã đề xuất điều hòa mô hình trong không gian tham số mô hình [29]. Ở mức độ cao, phương pháp này tìm kiếm một giải pháp trong mỗi tác vụ mới nằm trong không gian trọng số của các giải pháp cho các tác vụ trước đó. Điều này được thực hiện bằng cách tính toán khoảng cách L2 giữa mỗi tham số mô hình trong θn−1 và mỗi tham số mô hình trong θn, hoặc:

Lewc = ∑(j=1 to Nparams) Fjj^(n-1) (θj^n - θj^(n-1))^2     (1)

nơi Fjj^(n-1) là phần tử đường chéo thứ j của ma trận thông tin Fisher thứ n−1, Fn−1, được tính toán sử dụng dữ liệu và hàm mất mát trong tác vụ n−1. Chúng tôi gọi phương pháp này là EWC trong suốt bài báo này. Quan sát rằng nếu F được đưa ra như ma trận đồng nhất, Lewc đơn giản trở thành điều hòa L2 giữa các tham số mô hình. Chúng tôi cũng sẽ phân tích phương pháp này và gọi nó đơn giản là L2 cho phần còn lại của bài báo này. Một ưu điểm mạnh của điều hòa L2 so với điều hòa EWC là điều hòa L2 có thể được áp dụng khi không có ma trận trọng số quan trọng (ví dụ, L2 có thể được áp dụng trong tác vụ đầu tiên của một chuỗi học liên tục khi có huấn luyện trước để giữ lại kiến thức được huấn luyện trước). Trong khi nghiên cứu gốc cho thấy rằng sử dụng ma trận đồng nhất cho F làm tổn hại hiệu suất, chúng tôi sẽ chỉ ra sau đó rằng L2 thực sự có thể vượt trội hơn EWC trong các môi trường học liên tục nhất định.

4.2. Điều hòa không gian đặc trưng

Một phương pháp khác cho học liên tục là tận dụng chưng cất kiến thức từ θn−1 để điều hòa việc học của θn. Điều này lần đầu tiên được giới thiệu cho học liên tục trong phương pháp Learning without Forgetting (LwF) [36] như chưng cất kiến thức trong không gian dự đoán. Chúng tôi sẽ gọi điều này là PredKD trong suốt bài báo.

Hãy ký hiệu pθ(y|x) là phân phối lớp dự đoán được tạo ra bởi mô hình θ cho đầu vào x. Sử dụng ký hiệu này, hàm mất mát cho PredKD được định nghĩa là:

LPredKD = CE(pθn,1:n−1(x), pθn−1,1:n−1(x))     (2)

nơi CE là hàm mất mát cross-entropy tiêu chuẩn. Kiến thức cũng có thể được chưng cất trong không gian đặc trưng thay vì không gian dự đoán. Trực giác ở đây là căn chỉnh trực tiếp không gian đặc trưng của các mô hình để không gian đặc trưng không trôi xa khỏi giải pháp checkpoint trước đó. Chúng tôi sẽ gọi điều này là FeatureKD trong suốt bài báo với hàm mất mát được đưa ra là:

LFeatKD = ||θl^n(x) - θl^(n-1)(x)||²₂     (3)

Lưu ý rằng, vì chúng ta không tạo ra các dự đoán lớp pθ(y|x) ở không gian đặc trưng trung gian, chúng ta thay vào đó tối thiểu hóa sai số bình phương.

4.3. Thiên lệch tác vụ

Một hiện tượng học liên tục khác tồn tại khi không có nhãn tác vụ trong quá trình suy luận là thiên lệch tác vụ đối với dữ liệu tác vụ gần đây. Điều này thường được giảm thiểu với các giải pháp dựa vào dữ liệu luyện tập [1, 63]. Vì chúng ta không thể giảm thiên lệch tác vụ với dữ liệu luyện tập trong môi trường của chúng ta, chúng tôi mượn từ phương pháp học liên tục không luyện tập LWF.MC [47] và sử dụng hàm mất mát phân loại sigmoid binary cross-entropy (được gọi là BCE) thay vì hàm mất mát phân loại softmax cross-entropy thông thường (chúng tôi lưu ý ở đây rằng điều này gần như tương đương với việc sử dụng "thủ thuật nhãn" từ [67]). Trực giác ở đây là phân loại softmax không có dữ liệu luyện tập dẫn đến thiên lệch mạnh chống lại các lớp đã thấy trước đó vì tối thiểu hóa mất mát này làm giảm độ lớn của các đầu ra logit tương ứng của các lớp cũ. Chúng tôi sẽ chỉ ra rằng bộ phân loại BCE đẩy các phương pháp EWC, L2, và FeatKD thành các phương pháp SOTA cạnh tranh, mặc dù trước đó đã được báo cáo là "thất bại" trong môi trường học liên tục khi nhãn tác vụ không có mặt [23].

5. Thí nghiệm

Trong phần này, chúng tôi xem xét kỹ hơn EWC, L2, PredKD, và FeatKD trong môi trường học liên tục không luyện tập. Chúng tôi phân tích hiệu suất của bốn hàm mất mát này bên cạnh cả i) một mô hình ngây thơ được huấn luyện chỉ với mất mát phân loại (được gọi là ngây thơ) và ii) một mô hình giới hạn trên được huấn luyện với dữ liệu huấn luyện chung từ tất cả các tác vụ (được gọi là giới hạn trên). Chúng tôi đầu tiên cung cấp kết quả điểm chuẩn trên tập dữ liệu CIFAR-100 [30] chứa 100 lớp hình ảnh 32x32x3. Chúng tôi huấn luyện với ResNet 18 lớp [18] trong 250 epoch sử dụng tối ưu hóa Adam; tốc độ học được đặt thành 1e-3 và được giảm 10 lần sau 100, 150, và 200 epoch. Chúng tôi sử dụng weight decay là 0.0002 và kích thước batch là 128. Quan trọng, chúng tôi không điều chỉnh các siêu tham số của mình (tức là, trọng số mất mát) trên tập tác vụ đầy đủ vì điều chỉnh siêu tham số với dữ liệu giữ lại từ tất cả các tác vụ có thể vi phạm nguyên tắc học liên tục rằng mỗi tác vụ chỉ được thăm một lần [58]. Thay vào đó, chúng tôi đã điều chỉnh các siêu tham số của mình (bao gồm trọng số mất mát cho mỗi phương pháp) (sử dụng quét tuyến tính nửa thập kỷ từ 1e−3 đến 1e2) trên một chuỗi tác vụ nhỏ của mỗi tập dữ liệu.

Chỉ số đánh giá: Chúng tôi đánh giá các phương pháp bằng độ chính xác cuối cùng, hoặc độ chính xác đối với tất cả các lớp trong quá khứ sau khi đã thấy tất cả N tác vụ (được gọi là AN,1:N). Cụ thể, chúng tôi có:

Ai,n = (1/|Dtest_n|) ∑(x,y)∈Dtest_n 1(ŷ(x, θi,n) = y|ŷ ∈ Tn)     (4)

Lưu ý rằng Ai,n cho độ chính xác tác vụ cục bộ (tức là, suy luận trong học tăng dần theo tác vụ nơi nhãn tác vụ được đưa ra, được sử dụng để tính toán quên cục bộ FL_N bên dưới) và Ai,1:n cho độ chính xác tác vụ toàn cục (tức là, độ chính xác khi nhãn tác vụ không được biết, được sử dụng để tính toán quên toàn cục FG_N bên dưới). Đối với độ chính xác tác vụ cuối cùng trong kết quả của chúng tôi, chúng tôi sẽ ký hiệu AN,1:N đơn giản là A1:N. Chúng tôi cũng đo: (I) quên toàn cục, hoặc phép đo giảm hiệu suất trung bình trên tác vụ n đối với tác vụ toàn cục nơi không có nhãn tác vụ nào được đưa ra (được gọi là FG_N); và (II) quên cục bộ, hoặc phép đo giảm hiệu suất trung bình trên tác vụ n đối với tác vụ cục bộ nơi chỉ số tác vụ được đưa ra (được gọi là FL_N). Quên toàn cục được lấy từ [33] và được đưa ra là:

FG_N = (1/(N-1)) ∑(i=2 to N) ∑(n=1 to i-1) (|Tn|/|T1:i|)(Rn,n - Ri,n)     (5)

nơi:

Ri,n = (1/|Dtest_n|) ∑(x,y)∈Dtest_n 1(ŷ(x, θi,1:n) = y)     (6)

và quên cục bộ được lấy từ [39] và được đưa ra là:

FL_N = (1/(N-1)) ∑(n=1 to N-1) (AN,n - An,n)     (7)

5.1. Học liên tục không luyện tập

Chúng tôi bắt đầu bằng việc phân tích hiệu suất trên một chuỗi 10 tác vụ từ CIFAR-100. Ở đây, mô hình của chúng tôi được hiển thị 10 tác vụ khác nhau được rút ra từ 10 lớp mỗi tác vụ từ tập dữ liệu CIFAR-100. Chúng tôi sử dụng trọng số mất mát {1e1,5e−1,1,5} cho EWC, L2, PredKD, và FeatKD. Phát hiện đầu tiên của chúng tôi là PredKD và BCE là nền tảng cho học liên tục không luyện tập. Trong Bảng 1b, chúng tôi tách biệt hai phương pháp giảm thiểu "trôi đặc trưng": EWC và FeatKD. Đối với hai bên của bảng này, các hàng trên đề cập đến phương pháp trôi đặc trưng (EWC hoặc FeatKD) sử dụng BCE khi kết hợp với PredKD. Bên dưới, chúng tôi phá bỏ hai phương pháp riêng biệt, cho thấy hiệu suất khi i) PredKD bị loại bỏ, ii) BCE được thay thế bằng phân loại softmax, và iii) khi PredKD bị loại bỏ và BCE được thay thế bằng phân loại softmax.

Hàng dưới cùng chứng minh rằng EWC và FeatKD vanilla thất bại đối với học liên tục (A1:N kém) nhưng làm khá tốt trong việc giảm thiểu quên cục bộ FL_N khi một softmax PredKD được thêm vào (tức là, chúng hoạt động tốt cho học tăng dần theo tác vụ nơi nhãn tác vụ được đưa ra) [23]. Phát hiện sâu sắc hơn ở đây là cả EWC và FeatKD đều hoạt động tốt trong việc điều hòa trôi đặc trưng nhưng thất bại trong việc điều hòa/khử thiên lệch đầu phân loại. Như được thúc đẩy trong phần trước, chúng ta thấy rằng một bước nhảy đáng kể về hiệu suất được đạt được khi kết hợp cả BCE và PredKD.

Để xem xét kỹ hơn các tác động của điều hòa tham số và chưng cất đặc trưng đối với quên thảm khốc, chúng tôi xem xét các phương pháp sau i) PredKD, ii) PredKD + FeatKD, iii) PredKD + EWC, và iv) PredKD + L2. Cụ thể, chúng tôi muốn hiểu nơi quên đang xảy ra trong các phương pháp này. Chúng tôi mượn thực hành từ [45] và xem xét độ tương tự căn chỉnh kernel tâm (CKA) giữa các biểu diễn đặc trưng theo thời gian cho các lớp khác nhau trong mô hình (cao hơn là tốt hơn). Trong Hình 2, chúng ta thấy điểm số tương tự CKA được vẽ cho mỗi lớp trong mỗi mô hình qua các tác vụ cho dữ liệu tác vụ-1. Trục x tại tác vụ n cho điểm số tương tự CKA giữa các đặc trưng được đánh giá trên dữ liệu giữ lại tác vụ-1 từ θ1 so với các đặc trưng được tạo ra trên dữ liệu giữ lại tác vụ-1 từ θn. Chúng tôi tính toán CKA tại các lớp sau: Linear, hoặc đầu ra của lớp tuyến tính; pen, hoặc đầu ra của lớp gần cuối; và L-2,L-3,L-4, hoặc các đầu ra tại lớp thứ hai từ cuối, thứ ba từ cuối, và thứ tư từ cuối. Acc đề cập đến độ chính xác An,1:n, nơi n là số tác vụ (tức là, trục x). Chúng ta có thể giải thích các điểm số này với kết quả đầy đủ trong Bảng 2. Đối với thí nghiệm này, chúng ta thấy rằng PredKD hội tụ đến độ chính xác cuối cùng cao nhất, trong khi PredKD + EWC có mức quên thấp nhất. Tại sao vậy? Khi chúng ta nhìn vào Hình 2, chúng ta thấy một sự đánh đổi giữa việc giữ lại sự tương tự tác vụ 1 qua tất cả các lớp so với độ chính xác cuối cùng. Cụ thể, việc thêm các mất mát điều hòa tham số (EWC và L2) gây ra mức quên thấp nhưng với chi phí là tính dẻo dai thấp (tức là, khả năng học một tác vụ mới). Một phát hiện đáng ngạc nhiên (mà mở rộng đến phần còn lại của bài báo này) là FeatKD làm giảm độ chính xác cuối cùng mà không có bất kỳ cải thiện nào trong việc quên. Tóm lại, những điểm chính từ phần này là: 1) PredKD và BCE tạo ra một đường cơ sở mạnh cho học liên tục không luyện tập và 2) Điều hòa tham số bên cạnh đường cơ sở này làm giảm quên, nhưng với chi phí là tính dẻo dai thấp và do đó độ chính xác cuối cùng thấp.

5.2. Cách tận dụng các mô hình được huấn luyện trước

Bởi vì khoảng cách giữa SOTA và giới hạn trên cho học liên tục không luyện tập vẫn còn lớn, chúng tôi khám phá việc tận dụng các mô hình được huấn luyện trước. Cụ thể, chúng tôi đặt câu hỏi: loại điều hòa nào (không gian tham số hay không gian dự đoán) có thể tận dụng tốt nhất việc huấn luyện trước mô hình (từ một tập dữ liệu phụ trợ) cho học liên tục không luyện tập? Chúng tôi lưu ý ở đây rằng nghiên cứu của chúng tôi khác với [46] ở chỗ chúng tôi phân tích tác động của điều hòa đối với quên trong các mô hình được huấn luyện trước thay vì chỉ ra rằng các mô hình được huấn luyện trước có khả năng chống quên tốt hơn. Chúng tôi lặp lại các thí nghiệm từ phần trước, nhưng lần này mô hình của chúng tôi được khởi tạo với huấn luyện trước ImageNet [50]. Chúng tôi sử dụng trọng số mất mát {1e2,1e−1,5,5} cho EWC, L2, PredKD, và FeatKD. Kết quả chính được tìm thấy trong Bảng 3. Đáng ngạc nhiên, chúng tôi thấy thứ tự hiệu suất giữa PredKD, PredKD + EWC, và PredKD + L2 đã được đảo ngược! Trong khi các chỉ số quên không bị ảnh hưởng đáng kể, chúng ta thấy rằng A1:N vẫn giữ nguyên cho PredKD nhưng tăng lên đáng kể cho EWC và L2. Những kết quả này là hợp lý sau khi xem xét điều sau: với huấn luyện trước, cần ít tính dẻo dai hơn vì mô hình (đặc trưng) đã hữu ích cho các tác vụ mới; do đó, các phương pháp đạt được mức quên thấp với chi phí hiệu suất thấp trên các tác vụ mới trong tình huống không có huấn luyện trước bây giờ có "chi phí" được loại bỏ. Để xác nhận thêm về góc nhìn này, chúng tôi nhận thấy rằng các điểm số tương tự CKA được trình bày trong Hình 3 không thay đổi nhiều so với Hình 2, nhưng sự khác biệt về hiệu suất trên các tác vụ mới, được trình bày trong Hình 4, là lớn một cách nổi bật. Đó là, huấn luyện trước dường như không ảnh hưởng đến "quên" cho các phương pháp này mà thay vào đó tăng cường khả năng học các tác vụ mới mà không quên. Tóm lại, những điểm chính từ phần này là: 1) L2 với PredKD vượt trội hơn EWC với PredKD khi có huấn luyện trước, và cả hai phương pháp này đều vượt xa PredKD mà không có điều hòa tham số và 2) Đối với các phương pháp điều hòa tham số, huấn luyện trước cải thiện đáng kể khả năng học các tác vụ mới nhưng có ít tác động đến quên.

5.3. Bối cảnh với Nghiên cứu hiện tại - ResNet

Chúng tôi so sánh kết quả của mình với các phương pháp SOTA trong môi trường học liên tục không có mẫu trên CIFAR-100 sử dụng cùng backbone ResNet 18 lớp. Sự khác biệt giữa môi trường học liên tục không luyện tập của chúng tôi và môi trường học liên tục không có mẫu này là, trong khi cả hai đều không lưu trữ hình ảnh cho luyện tập, môi trường sau thường tổng hợp những hình ảnh này. Như đã thảo luận trong Phần 2, việc tạo ra hình ảnh tổng hợp với nghịch đảo mô hình là một quy trình tốn kém về mặt tính toán và vẫn có thể vi phạm các mối quan tâm về quyền riêng tư dữ liệu. Chúng tôi so sánh trong Bảng 4, cho thấy độ chính xác cuối cùng cho các phương pháp được thảo luận trong bài báo này. Chúng tôi thấy rằng 1) Huấn luyện trước có thể vượt trội hơn các phương pháp luyện tập SOTA từ dữ liệu tổng hợp, và 2) Huấn luyện trước thậm chí có thể vượt trội hơn các phương pháp luyện tập đơn giản lưu trữ một coreset 2000 hình ảnh dữ liệu.

5.4. Bối cảnh với Nghiên cứu hiện tại - ViT

Tiếp theo, được thúc đẩy bởi những phát hiện của chúng tôi trong các phần trước, chúng tôi hỏi: điều hòa tham số có thể vượt trội hơn các phương pháp gợi ý cho học liên tục không? [54, 60, 61]. Cụ thể, chúng tôi phỏng đoán rằng, với một triển khai và so sánh công bằng, nhắm mục tiêu chỉ sửa đổi cùng một điểm của mô hình ViT như các phương pháp gợi ý, điều hòa tham số có thể vượt trội hơn gợi ý cho những điểm chuẩn này. Chúng tôi đánh giá bằng cách sử dụng ImageNet-R [19,60] được cấu thành từ 200 lớp đối tượng với một bộ sưu tập rộng lớn các phong cách hình ảnh, bao gồm hoạt hình, graffiti, và các ví dụ khó từ tập dữ liệu ImageNet gốc [50]. Điểm chuẩn này hấp dẫn vì phân phối dữ liệu huấn luyện có khoảng cách đáng kể đến dữ liệu huấn luyện trước (ImageNet), do đó cung cấp một môi trường bài toán công bằng và thách thức. Chúng tôi sử dụng chính xác cùng thiết lập thí nghiệm như bài báo CODA-Prompt [54] gần đây. Chúng tôi triển khai phương pháp của mình và tất cả các đường cơ sở trong PyTorch [44] sử dụng backbone ViT-B/16 [12] được huấn luyện trước trên ImageNet-1K [50]. Chúng tôi so sánh với các phương pháp sau (cùng những so sánh không luyện tập của CODA-Prompt): Learning without Forgetting (LwF) [36], Learning to Prompt (L2P) [61], một phiên bản sửa đổi của L2P (L2P++) [54], và DualPrompt [60]. Ngoài ra, chúng tôi báo cáo hiệu suất giới hạn trên (UB) và hiệu suất cho một mạng neural được huấn luyện chỉ trên mất mát phân loại sử dụng dữ liệu huấn luyện tác vụ mới (chúng tôi gọi điều này là FT).

Chúng tôi đông lạnh hầu hết backbone và chỉ tinh chỉnh các ma trận chiếu QKV của các khối tự chú ý trong suốt mô hình ViT. Trực giác là chúng tôi đang sửa đổi cùng các module như các phương pháp gợi ý, nhưng sử dụng các phương pháp học liên tục cổ điển tinh chỉnh với điều hòa thay vì thêm gợi ý. Chúng tôi sử dụng trọng số mất mát {1e3,1} cho EWC và L2, tương ứng. Quan trọng, chúng tôi sử dụng cùng đầu phân loại như L2P, DualPrompt, và CODA-Prompt, và bổ sung so sánh với một biến thể FT, FT++, sử dụng cùng bộ phân loại như các phương pháp gợi ý và chịu ít quên hơn. Để biết thêm chi tiết, chúng tôi đề cập người đọc đến bài báo CODA-Prompt [54].

Trong Bảng 5, chúng tôi đánh giá so với các phương pháp học liên tục không luyện tập phổ biến và gần đây. Chúng tôi thấy rằng L2 đạt được một trạng thái nghệ thuật cao trong môi trường này. So với các phương pháp gợi ý L2P, DualPrompt, và CODA-Prompt gần đây, L2 có những cải thiện rõ ràng và đáng kể, trong khi EWC có hiệu suất kém. Trực giác của chúng tôi là L2 mạnh hơn nhiều vì nó bắt đầu điều hòa trong tác vụ 1 (thay vì tác vụ 2, như EWC), và điều hòa không chỉ cho các tác vụ trong quá khứ mà còn cho các tác vụ tương lai bằng cách khuyến khích các tham số mô hình ở gần trạng thái huấn luyện trước ban đầu phong phú. Tóm lại, điểm chính từ thí nghiệm này là tinh chỉnh với điều hòa tham số L2 trong các ma trận chiếu QKV của các khối tự chú ý ViT vượt trội hơn các phương pháp gợi ý cho học liên tục.

6. Kết luận

Trong nghiên cứu này, chúng tôi xem xét kỹ hơn một số chiến lược học liên tục phổ biến trong môi trường học liên tục không luyện tập. Môi trường này phản ánh các ứng dụng học máy không thể lưu trữ hoặc tạo ra dữ liệu huấn luyện đã thấy trong quá khứ do các mối quan tâm về quyền riêng tư hoặc ràng buộc bộ nhớ. Chúng tôi đầu tiên chỉ ra rằng các kỹ thuật điều hòa tham số như L2 và EWC có thể thành công trong môi trường học liên tục không luyện tập nếu softmax được loại bỏ khỏi đầu phân loại. Sau đó, chúng tôi so sánh điều hòa tham số, chưng cất đặc trưng, và chưng cất dự đoán trên một điểm chuẩn học liên tục 10 tác vụ. Chúng tôi thấy rằng với một mô hình được khởi tạo ngẫu nhiên, các phương pháp điều hòa tham số đạt được mức quên thấp nhưng với chi phí là độ chính xác thấp. Khi chúng tôi khởi tạo mô hình với trọng số được huấn luyện trước, chúng tôi thấy rằng điều hòa tham số tiêm cả mức quên thấp và độ chính xác cao. Đáng ngạc nhiên, chúng tôi thấy rằng điều hòa L2 vượt trội hơn EWC trong tình huống mô hình được huấn luyện trước. Để xác nhận những phát hiện này, chúng tôi chứng minh rằng điều hòa tham số L2 được triển khai trong một transformer ViT vượt trội hơn các phương pháp gợi ý cho học liên tục phổ biến gần đây. Kết luận, nghiên cứu của chúng tôi đã cung cấp những hiểu biết có giá trị về hiệu quả của các loại điều hòa khác nhau cho học liên tục và làm nổi bật tiềm năng của điều hòa trong các môi trường không luyện tập.

Lời cảm ơn
Tài liệu này dựa trên công việc được hỗ trợ bởi Quỹ Khoa học Quốc gia dưới Tài trợ số 2239292.

Tài liệu tham khảo
[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 844–853, October 2021. 2, 4

[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. 2

[3] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems, pages 11849–11860, 2019. 2

[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems, pages 11816–11825, 2019. 2

[5] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8218–8227, 2021. 2

[6] Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 233–248, 2018. 2

[7] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-GEM. In International Conference on Learning Representations, 2019. 2

[8] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc'Aurelio Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. 2

[9] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Dual-teacher class-incremental learning with data-free generative replay. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3543–3552, 2021. 3

[10] Nikhil Churamani, Ozgur Kara, and Hatice Gunes. Domain-incremental continual learning for mitigating bias in facial expression and action unit recognition. arXiv preprint arXiv:2103.08637, 2021. 1

[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 3

[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8

[13] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. arXiv preprint arXiv:1906.02425, 2019. 2

[14] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. Adversarial continual learning. arXiv preprint arXiv:2003.09553, 2020. 2

[15] Alexander Gepperth and Cem Karaoguz. Incremental learning with self-organizing maps. 2017 12th International Workshop on Self-Organizing Maps and Learning Vector Quantization, Clustering and Data Visualization (WSOM), pages 1–8, 2017. 2

[16] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efficient experience replay for streaming learning. In 2019 International Conference on Robotics and Automation (ICRA), pages 9769–9776. IEEE, 2019. 2

[17] Tyler L Hayes and Christopher Kanan. Lifelong machine learning with deep streaming linear discriminant analysis. arXiv preprint arXiv:1909.01520, 2019. 2

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 4, 8

[19] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. 7

[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 2

[21] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Lifelong learning via progressive distillation and retrospection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 437–452, 2018. 2

[22] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 831–839, 2019. 2

[23] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. 1, 3, 4, 5

[24] Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. Predicting the computational cost of deep learning models. In 2018 IEEE international conference on big data (Big Data), pages 3873–3882. IEEE, 2018. 1

[25] Georgios Kaissis, Alexander Ziller, Jonathan Passerat-Palmbach, Théo Ryffel, Dmitrii Usynin, Andrew Trask, Ionésio Lima, Jason Mancuso, Friederike Jungmann, Marc-Matthias Steinborn, et al. End-to-end privacy preserving deep learning on multi-institutional medical imaging. Nature Machine Intelligence, 3(6):473–484, 2021. 3

[26] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual learning. arXiv preprint arXiv:1710.10368, 2017. 2

[27] Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. International Conference on Learning Representations (ICLR), 2018. 2

[28] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. AAAI Conference on Artificial Intelligence, 2018. 2

[29] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. 2, 3, 4

[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Tech Report, 2009. 4

[31] Jogendra Nath Kundu, Rahul Mysore Venkatesh, Naveen Venkat, Ambareesh Revanur, and R Venkatesh Babu. Class-incremental domain adaptation. In European Conference on Computer Vision, pages 53–69. Springer, 2020. 1

[32] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. 1

[33] Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with unlabeled data in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 312–321, 2019. 2, 5

[34] Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model for task-free continual learning. arXiv preprint arXiv:2001.00689, 2020. 2

[35] Timothée Lesort, Hugo Caselles-Dupré, Michael Garcia-Ortiz, Andrei Stoian, and David Filliat. Generative models from the perspective of continual learning. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2019. 2

[36] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. 2, 4, 8

[37] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. arXiv preprint arXiv:1705.03550, 2017. 2

[38] Vincenzo Lomonaco, Davide Maltoni, and Lorenzo Pellegrini. Rehearsal-free continual learning over small non-iid batches. In CVPR Workshops, pages 989–998, 2020. 2

[39] David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pages 6470–6479, USA, 2017. Curran Associates Inc. 2, 5

[40] Davide Maltoni and Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios. Neural Networks, 116:56–73, 2019. 2

[41] Vaishnavh Nagarajan, Colin Raffel, and Ian J Goodfellow. Theoretical insights into memorization in gans. In Neural Information Processing Systems Workshop, 2018. 2

[42] Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto. Toward understanding catastrophic forgetting in continual learning. arXiv preprint arXiv:1908.01091, 2019. 1

[43] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11321–11329, 2019. 2

[44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 8

[45] Vinay V Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. arXiv preprint arXiv:2007.07400, 2020. 5

[46] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations, 2021. 6

[47] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR'17, pages 5533–5542, 2017. 2, 4, 5

[48] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123–146, 1995. 2

[49] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In Advances in Neural Information Processing Systems, pages 348–358, 2019. 2

[50] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015. 6, 7, 8

[51] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. 2

[52] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2990–2999. Curran Associates, Inc., 2017. 2

[53] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. Always be dreaming: A new approach for data-free class-incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9374–9384, October 2021. 3, 8

[54] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. arXiv preprint arXiv:2211.13218, 2022. 2, 3, 7, 8

[55] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In International Conference on Learning Representations, 2019. 2

[56] Gido M van de Ven, Hava T Siegelmann, and Andreas S Tolias. Brain-inspired replay for continual learning with artificial neural networks. Nature communications, 11(1):1–14, 2020. 2

[57] Gido M van de Ven and Andreas S Tolias. Generative replay with feedback connections as a general strategy for continual learning. arXiv preprint arXiv:1809.10635, 2018. 2

[58] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019. 1, 4

[59] Johannes von Oswald, Christian Henning, João Sacramento, and Benjamin F Grewe. Continual learning with hypernetworks. arXiv preprint arXiv:1906.00695, 2019. 2

[60] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. arXiv preprint arXiv:2204.04799, 2022. 2, 3, 7, 8

[61] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139–149, 2022. 2, 3, 7, 8

[62] Guile Wu, Shaogang Gong, and Pan Li. Striking a balance between stability and plasticity for class-incremental learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1124–1133, 2021. 2

[63] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 374–382, 2019. 2, 4

[64] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8715–8724, 2020. 3, 8

[65] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de Weijer. Semantic drift compensation for class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6982–6991, 2020. 2

[66] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning, 2017. 2

[67] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning using online variational bayes. arXiv preprint arXiv:1803.10123, 2018. 4

[68] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5871–5880, 2021. 2
