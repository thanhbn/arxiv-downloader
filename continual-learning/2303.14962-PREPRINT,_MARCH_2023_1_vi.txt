# 2303.14962.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2303.14962.pdf
# Kích thước tệp: 2962790 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
BẢN THẢO, THÁNG 3 2023 1
Học Liên Tục Không Quên với
Mạng Con Chiến Thắng Mềm
Haeyong Kang, Jaehong Yoon, Sultan Rizky Madjid, Sung Ju Hwang, và Chang D. Yoo

Tóm tắt —Được truyền cảm hứng từ Giả Thuyết Vé Số May Mắn Có Điều Chỉnh (RLTH), trong đó nêu rằng các mạng con mượt mà (không nhị phân) cạnh tranh tồn tại trong một mạng dày đặc trong các tác vụ học liên tục, chúng tôi điều tra hai phương pháp học liên tục dựa trên kiến trúc được đề xuất, học tuần tự và chọn các Mạng Con Nhị Phân thích ứng (WSN) và Mạng Con Mềm không nhị phân (SoftNet) cho từng tác vụ. WSN và SoftNet cùng học các trọng số mô hình được điều chỉnh và các mặt nạ không nhị phân thích ứng tác vụ của các mạng con liên quan đến từng tác vụ trong khi cố gắng chọn một tập nhỏ các trọng số được kích hoạt (vé chiến thắng) bằng cách tái sử dụng các trọng số của các mạng con trước đó. WSN và SoftNet được đề xuất vốn dĩ miễn nhiễm với việc quên thảm khốc vì mỗi mô hình mạng con được chọn không xâm phạm các mạng con khác trong Học Tăng Dần Tác Vụ (TIL). Trong TIL, các mặt nạ nhị phân được tạo ra cho mỗi vé chiến thắng được mã hóa thành một mặt nạ chữ số nhị phân N-bit, sau đó được nén bằng mã hóa Huffman để tăng dung lượng mạng dưới tuyến tính theo số lượng tác vụ. Đáng ngạc nhiên, trong bước suy luận, SoftNet được tạo ra bằng cách tiêm nhiễu nhỏ vào nền của WSN đã có (giữ nguyên tiền cảnh của WSN) cung cấp khả năng chuyển tiếp xuất sắc cho các tác vụ tương lai trong TIL. SoftNet thể hiện hiệu quả vượt trội so với WSN trong việc điều chỉnh các tham số để giải quyết việc quá khớp với một vài ví dụ trong Học Tăng Dần Lớp Ít Mẫu (FSCIL).

Từ khóa chỉ mục —Học Liên Tục (CL), Học Tăng Dần Tác Vụ (TIL), Học Tăng Dần Lớp Ít Mẫu (FSCIL), Giả Thuyết Vé Số May Mắn Có Điều Chỉnh (RLTH), Mạng Con Chiến Thắng (WSN), Mạng Con Mềm (SoftNet)

F
1 GIỚI THIỆU

HỌC Liên Tục (CL), còn được gọi là Học Suốt Đời [69], [61], [82], [23], là một mô hình học tập trong đó một chuỗi các tác vụ được học tuần tự. Mục tiêu chính của học liên tục là tái tạo nhận thức con người, được đặc trưng bởi khả năng học các khái niệm hoặc kỹ năng mới một cách tăng dần trong suốt cuộc đời. Một hệ thống học liên tục tối ưu có thể tạo điều kiện cho việc chuyển tiếp tích cực về phía trước và phía sau, tận dụng kiến thức thu được từ các tác vụ trước đó để giải quyết các tác vụ mới trong khi cũng cập nhật sự hiểu biết về các tác vụ trước đó với kiến thức mới.

Tuy nhiên, xây dựng các hệ thống học liên tục thành công là một thách thức do sự xuất hiện của việc quên thảm khốc hoặc can thiệp thảm khốc [53], một hiện tượng trong đó hiệu suất của mô hình trên các tác vụ trước đó suy giảm đáng kể khi nó học các tác vụ mới. Điều này có thể khiến việc giữ lại kiến thức thu được từ các tác vụ trước đó trở nên khó khăn, cuối cùng dẫn đến sự suy giảm hiệu suất tổng thể.

Để giải quyết vấn đề quên thảm khốc trong học liên tục, nhiều phương pháp đã được đề xuất, có thể được phân loại rộng rãi như sau: (1) Các phương pháp dựa trên điều chỉnh [38], [7], [34], [70], [55] nhằm giữ lại thông tin đã học của các tác vụ quá khứ trong quá trình đào tạo liên tục được hỗ trợ bởi các thuật ngữ điều chỉnh được thiết kế tinh vi, (2) Các phương pháp dựa trên luyện tập [59], [60], [8], [9], [62] sử dụng một tập dữ liệu thực hoặc được tổng hợp từ các tác vụ trước đó và xem xét lại chúng, và (3) Các phương pháp dựa trên kiến trúc [49], [64], [44], [72], [36], [37] đề xuất giảm thiểu sự can thiệp giữa các tác vụ thông qua các thành phần kiến trúc mới được thiết kế.

Bất chấp sự thành công đáng kể của các công trình gần đây về học liên tục dựa trên luyện tập và kiến trúc, phần lớn các phương pháp gần đây yêu cầu bộ nhớ bên ngoài khi có các tác vụ mới, khiến mô hình khó mở rộng cho các tác vụ lớn hơn và phức tạp hơn. CL dựa trên luyện tập yêu cầu lưu trữ bổ sung để lưu trữ bộ đệm phát lại hoặc các mô hình tạo sinh, và các phương pháp dựa trên kiến trúc tận dụng dung lượng mô hình bổ sung để tính toán cho các tác vụ mới. Những xu hướng này dẫn đến một câu hỏi thiết yếu: làm thế nào chúng ta có thể xây dựng một mô hình CL hiệu quả về bộ nhớ không vượt quá dung lượng của mạng xương sống hoặc thậm chí yêu cầu dung lượng nhỏ hơn nhiều? Một số nghiên cứu đã chỉ ra rằng các mạng nơ-ron sâu bị tham số hóa quá mức [17], [21], [43] và do đó việc loại bỏ các trọng số dư thừa/không cần thiết có thể đạt được hiệu suất ngang bằng hoặc thậm chí tốt hơn so với mạng dày đặc ban đầu. Gần đây hơn, Giả Thuyết Vé Số May Mắn (LTH) [18] chứng minh sự tồn tại của các mạng con thưa, được gọi là vé chiến thắng, giữ nguyên hiệu suất của một mạng dày đặc. Tuy nhiên, tìm kiếm các vé chiến thắng tối ưu trong quá trình học liên tục với các phương pháp cắt tỉa lặp đi lặp lại yêu cầu cắt tỉa và đào tạo lại lặp đi lặp lại cho mỗi tác vụ đến, điều này có thể thực tế hơn. Hơn nữa, được tận dụng bởi Giả Thuyết Vé Số May Mắn Có Điều Chỉnh (RLTH) [37], các mạng con ví dụ rằng nó có thể quá khớp một vài dữ liệu tác vụ, có khả năng hạn chế hiệu quả của chúng trên các tác vụ hoặc bộ dữ liệu mới.

Để giải quyết các vấn đề về bộ đệm phát lại bên ngoài, dung lượng và quá khớp, chúng tôi đề xuất một phương pháp CL được điều chỉnh mới tìm ra Mạng Con Chiến Thắng Có Điều Chỉnh hiệu suất cao được gọi là Mạng Con Mềm (SoftNet) [37] cho các tác vụ mà không cần đào tạo lại và tua lại. Như một đường cơ sở của SoftNet, các Mạng Con Chiến Thắng không được điều chỉnh được gọi là WSN [36] được trình bày lại, như được hiển thị trong Hình 1 (d). Ngoài ra, chúng tôi đặt các phương pháp CL dựa trên cắt tỉa trước đó [49], [72] (xem Hình 1 (a)) làm đường cơ sở của các kiến trúc, thu được các mạng con cụ thể cho tác vụ với một mạng xương sống được đào tạo trước. WSN của chúng tôi học tăng dần các trọng số mô hình và các mặt nạ nhị phân thích ứng tác vụ (các mạng con) trong mạng nơ-ron. Để cho phép chuyển tiếp về phía trước khi một mô hình học trên một tác vụ mới, chúng tôi tái sử dụng các trọng số mạng con đã học cho các tác vụ trước đó, tuy nhiên một cách có chọn lọc, trái ngược với việc sử dụng tất cả các trọng số [50] (xem Hình 1 (b)), điều này có thể dẫn đến chuyển tiếp thiên lệch. Hơn nữa, WSN loại bỏ mối đe dọa quên thảm khốc trong quá trình học liên tục bằng cách đóng băng các trọng số mạng con cho các tác vụ trước đó và không bị ảnh hưởng bởi chuyển tiếp tiêu cực, không giống như [80] (xem Hình 1 (c)), mà các trọng số mạng con cho các tác vụ trước đó có thể được cập nhật khi đào tạo trên các tác vụ mới.

LTH thường tận dụng độ lớn của các trọng số như một tiêu chí cắt tỉa để tìm ra các mạng con tối ưu. Tuy nhiên, trong CL, chỉ dựa vào độ lớn trọng số có thể không hiệu quả vì các trọng số được chia sẻ giữa các lớp, và do đó việc đào tạo trên các tác vụ mới sẽ thay đổi các trọng số được đào tạo cho các tác vụ trước đó (các trọng số được tái sử dụng). Việc cập nhật các trọng số được tái sử dụng sẽ kích hoạt một hiệu ứng tuyết lở trong đó các trọng số được chọn để trở thành một phần của các mạng con cho các tác vụ sau sẽ luôn tốt hơn trong mắt người học, điều này sẽ dẫn đến việc quên thảm khốc kiến thức cho các tác vụ trước đó. Do đó, trong CL, người học phải đào tạo trên các tác vụ mới mà không thay đổi các trọng số được tái sử dụng. Để tìm ra các mạng con tối ưu, chúng tôi tách biệt thông tin của tham số học tập và cấu trúc mạng thành hai tham số có thể học riêng biệt, cụ thể là trọng số và điểm số trọng số. Điểm số trọng số là các mặt nạ nhị phân có cùng hình dạng với các trọng số. Bây giờ, các mạng con được tìm thấy bằng cách chọn các trọng số với k phần trăm hàng đầu điểm số xếp hạng trọng số. Quan trọng hơn, việc tách biệt các trọng số và cấu trúc cho phép chúng tôi tìm ra mạng con tối ưu trực tuyến mà không cần đào tạo lại, cắt tỉa và tua lại lặp đi lặp lại. Có một điều nữa cần xem xét để tìm ra các mạng con tối ưu được tạo ra bởi các mặt nạ nhị phân trong CL. Theo các tác vụ CL, các mạng con có xu hướng quá khớp với một vài mẫu, tức là Học Tăng Dần Lớp Ít Mẫu (FSCIL). Do đó, chúng ta có thể tìm thấy các mạng con được điều chỉnh được tạo ra bởi các mặt nạ mượt (mềm). Để đạt được điều này, các phương pháp được đề xuất được thiết kế để cùng học các trọng số và cấu trúc của các mạng con được điều chỉnh tối ưu, có kích thước tổng thể nhỏ hơn một mạng dày đặc.

Các đóng góp của chúng tôi có thể được tóm tắt như sau:

• Được truyền cảm hứng từ Giả Thuyết Vé Số May Mắn Có Điều Chỉnh (RLTH), chúng tôi đề xuất các phương pháp học liên tục không quên mới được gọi là WSN và SoftNet, học một mạng con nhỏ gọn cho mỗi tác vụ trong khi giữ nguyên các trọng số được chọn bởi các tác vụ trước đó.

• WSN và SoftNet được đề xuất không thực hiện cắt tỉa rõ ràng để học mạng con. Các phương pháp của chúng tôi loại bỏ việc quên thảm khốc và cho phép chuyển tiếp về phía trước từ các tác vụ trước đó sang các tác vụ mới trong Học Tăng Dần Tác Vụ (TIL).

• WSN của chúng tôi thu được các mạng con nhỏ gọn sử dụng mã hóa Huffman với sự tăng dưới tuyến tính trong dung lượng mạng, vượt trội hơn các phương pháp học liên tục hiện có về sự đánh đổi độ chính xác-dung lượng và chuyển tiếp về phía trước/phía sau trong TIL.

• SoftNet của chúng tôi đào tạo hai loại mạng con khác nhau để giải quyết vấn đề FSCIL, giảm thiểu người học liên tục khỏi việc quên các phiên trước đó và quá khớp đồng thời, vượt trội hơn các đường cơ sở mạnh trên các tác vụ điểm chuẩn công khai.

2 CÁC CÔNG TRÌNH LIÊN QUAN

Học Liên Tục. Học liên tục [53], [69], [41], [45], còn được gọi là học suốt đời, là thách thức học một chuỗi các tác vụ liên tục trong khi sử dụng và bảo tồn kiến thức đã học trước đó để cải thiện hiệu suất trên các tác vụ mới. Một số phương pháp chính đã được đề xuất để giải quyết các thách thức của học liên tục, chẳng hạn như việc quên thảm khốc. Một phương pháp như vậy là các phương pháp dựa trên điều chỉnh [38], [7], [34], [70], [55], nhằm giảm thiểu việc quên thảm khốc bằng cách áp đặt các ràng buộc điều chỉnh ngăn cản các thay đổi đối với các trọng số hoặc nút liên quan đến các tác vụ quá khứ. Các phương pháp dựa trên luyện tập [59], [8], [9], [62], [16] lưu trữ các tóm tắt dữ liệu nhỏ cho các tác vụ quá khứ và phát lại chúng trong quá trình đào tạo để giữ lại kiến thức đã có. Một số phương pháp trong dòng công việc này [66], [2] chứa mô hình tạo sinh để xây dựng các luyện tập giả cho các tác vụ trước đó. Các phương pháp dựa trên kiến trúc [49], [64], [44], [72], [36], [37] sử dụng dung lượng bổ sung để mở rộng [75], [80] hoặc cô lập [61] các tham số mô hình, bảo tồn kiến thức đã học và ngăn chặn việc quên. Cả hai phương pháp dựa trên luyện tập và kiến trúc đều cho thấy hiệu quả đáng kể trong việc ngăn chặn việc quên thảm khốc nhưng yêu cầu dung lượng bổ sung cho các tham số thích ứng tác vụ [72] hoặc các bộ đệm phát lại.

Học Liên Tục Dựa Trên Cắt Tỉa. Trong khi hầu hết các công trình nhằm tăng hiệu suất của những người học liên tục bằng cách thêm bộ nhớ, một số nhà nghiên cứu đã tập trung vào việc xây dựng những người học liên tục hiệu quả về bộ nhớ và tính toán bằng cách sử dụng các ràng buộc dựa trên cắt tỉa. CLNP [19] là một ví dụ về phương pháp chọn các nơ-ron quan trọng cho một tác vụ nhất định sử dụng điều chỉnh ℓ₁ để tạo ra độ thưa và đóng băng chúng để duy trì hiệu suất. Các nơ-ron không được chọn được khởi tạo lại cho việc đào tạo tác vụ tương lai. Một phương pháp khác, Piggyback [49], đào tạo các mặt nạ nhị phân cụ thể cho tác vụ trên các trọng số cho một mô hình được đào tạo trước. Tuy nhiên, phương pháp này không cho phép chuyển giao kiến thức giữa các tác vụ, và hiệu suất của nó phụ thuộc nhiều vào chất lượng của mô hình xương sống. HAT [64] đề xuất các vectơ chú ý có thể học cụ thể cho tác vụ để xác định các trọng số quan trọng cho mỗi tác vụ. Các mặt nạ được công thức hóa thành các vectơ chú ý tích lũy theo lớp trong quá trình học liên tục. Một phương pháp gần đây, LL-Tickets [11], cho thấy một mạng con thưa được gọi là vé suốt đời hoạt động tốt trên tất cả các tác vụ trong quá trình học liên tục. Phương pháp tìm kiếm các vé nổi bật hơn từ những vé hiện tại nếu các vé thu được không thể học đủ tác vụ mới trong khi duy trì hiệu suất trên các tác vụ quá khứ. Tuy nhiên, LL-Tickets yêu cầu dữ liệu bên ngoài để tối đa hóa việc chưng cất kiến thức với các mô hình đã học cho các tác vụ trước đó, và quá trình mở rộng vé bao gồm các bước đào tạo lại và cắt tỉa. WSN [36] là một phương pháp khác cùng học mô hình và các mặt nạ nhị phân thích ứng tác vụ trên các mạng con liên quan đến từng tác vụ. Nó cố gắng chọn một tập nhỏ các trọng số được kích hoạt (vé chiến thắng) bằng cách tái sử dụng các trọng số của các mạng con trước đó.

Mạng con mềm. Các nghiên cứu gần đây đã chỉ ra rằng việc cổng phụ thuộc ngữ cảnh của các không gian con [25], tham số [50], [26], [52], hoặc các lớp [63] của một mạng nơ-ron sâu đơn lẻ có hiệu quả trong việc giải quyết việc quên thảm khốc trong quá trình học liên tục. Hơn nữa, việc kết hợp cổng phụ thuộc ngữ cảnh với các ràng buộc ngăn chặn những thay đổi đáng kể trong trọng số mô hình, chẳng hạn như SI [82] và EWC [38], có thể dẫn đến những cải thiện hiệu suất hơn nữa, như được chỉ ra bởi Masse et al. [51]. Các cực tiểu phẳng, có thể được xem như việc thu được các không gian con, cũng đã được đề xuất để giải quyết việc quên thảm khốc. Các nghiên cứu trước đó đã chứng minh rằng một bộ tối thiểu hóa phẳng mạnh mẽ hơn đối với các nhiễu ngẫu nhiên [29], [30], [33]. Trong một nghiên cứu gần đây của Shi et al. [65], việc thu được các cực tiểu tổn thất phẳng trong phiên cơ sở, đề cập đến phiên tác vụ đầu tiên với số lượng đủ các thể hiện đào tạo, được tìm thấy là cần thiết để giảm thiểu việc quên thảm khốc trong Học Tăng Dần Lớp Ít Mẫu (FSCIL). Họ đạt được điều này bằng cách dịch chuyển các trọng số mô hình trên đường viền tổn thất phẳng thu được. Công trình của chúng tôi điều tra hiệu suất của hai phương pháp học liên tục dựa trên kiến trúc được đề xuất: WSN / Mạng Con Mềm (SoftNet). Chúng tôi chọn các mạng con [18], [46], [81], [85], [71], [58], [36], [14] và tối ưu hóa các tham số mạng con được điều chỉnh trong một không gian con [37] trong các cài đặt Học Tăng Dần Tác Vụ (TIL) và Học Tăng Dần Lớp Ít Mẫu (FSCIL).

3 MẠNG CON CHIẾN THẮNG MỀM

Trong phần này, chúng tôi trình bày các phương pháp học liên tục dựa trên cắt tỉa của chúng tôi, Mạng Con Chiến Thắng (WSN) [36] và Mạng Con Chiến Thắng Mềm (SoftNet) [37]. Trong WSN, mạng nơ-ron tìm kiếm các vé chiến thắng thích ứng tác vụ và chỉ cập nhật các trọng số chưa được đào tạo trên các tác vụ trước đó. Sau khi đào tạo trên mỗi tác vụ, các tham số mạng con của mô hình được đóng băng để đảm bảo rằng phương pháp được đề xuất vốn dĩ miễn nhiễm với việc quên thảm khốc. Phương pháp WSN được thiết kế để chuyển tiếp có chọn lọc kiến thức đã học trước đó cho các tác vụ tương lai (tức là chuyển tiếp về phía trước), có thể giảm đáng kể thời gian đào tạo cần thiết để hội tụ trong quá trình học tuần tự. Tính năng này đặc biệt quan trọng đối với các vấn đề học tập quy mô lớn trong đó một người học liên tục đào tạo trên nhiều tác vụ tuần tự, dẫn đến tiết kiệm đáng kể về thời gian và tính toán. Ban đầu, SoftNet được đề xuất để giải quyết các vấn đề quên các phiên trước đó và quá khớp một vài mẫu của các phiên mới. Để đạt được điều này, phương pháp đào tạo hai loại mạng con đồng thời.

Phát Biểu Vấn Đề. Xem xét một thiết lập học có giám sát trong đó T tác vụ đến với một người học tuần tự. Chúng tôi ký hiệu rằng Dt = {xi,t, yi,t}ⁿᵗᵢ₌₁ là bộ dữ liệu của tác vụ t, bao gồm nt cặp thể hiện thô và nhãn tương ứng. Chúng tôi giả định một mạng nơ-ron f(·; θ), được tham số hóa bởi các trọng số mô hình θ và kịch bản học liên tục tiêu chuẩn nhằm học một chuỗi các tác vụ bằng cách giải quyết thủ tục tối ưu hóa sau tại mỗi bước t:

θ* = minimize_θ (1/nt) Σᵢ₌₁ⁿᵗ L(f(xi,t; θ), yi,t);                    (1)

trong đó L(·, ·) là một tổn thất mục tiêu phân loại như tổn thất entropy chéo. Dt cho tác vụ t chỉ có thể truy cập được khi học tác vụ t. Lưu ý rằng các phương pháp học liên tục dựa trên luyện tập cho phép ghi nhớ một phần nhỏ của bộ dữ liệu để phát lại. Chúng tôi tiếp tục giả định rằng danh tính tác vụ được đưa ra trong các giai đoạn đào tạo và kiểm tra.

Những người học liên tục thường sử dụng các mạng nơ-ron sâu được tham số hóa quá mức để đảm bảo đủ dung lượng cho việc học các tác vụ tương lai. Phương pháp này thường dẫn đến việc khám phá các mạng con hoạt động tốt như hoặc tốt hơn so với mạng ban đầu. Với các tham số mạng nơ-ron θ, mặt nạ chú ý nhị phân m*t mô tả mạng con tối ưu cho tác vụ t sao cho |m*t| nhỏ hơn dung lượng mô hình c như sau:

m*t = minimize_(mt∈{0,1}|θ|) (1/nt) Σᵢ₌₁ⁿᵗ L[f(xi,t; θ ⊙ mt), yi,t]
chịu ràng buộc |mt| ≤ c;                                         (2)

trong đó tổn thất tác vụ J = L[f(xi,t; θ), yi,t] và c ≤ |θ| (được sử dụng như tỷ lệ phần trăm được chọn % của các tham số mô hình trong phần sau). Trong phần tối ưu hóa, chúng tôi mô tả cách thu được m*t sử dụng một điểm số trọng số có thể học đơn lẻ s chịu cập nhật trong khi tối thiểu hóa tổn thất tác vụ cùng cho mỗi tác vụ.

3.1 Mạng Con Chiến Thắng (WSN)

Để mỗi trọng số được liên kết với một tham số có thể học mà chúng tôi gọi là điểm số trọng số s, xác định bằng số tầm quan trọng của trọng số liên kết với nó; nghĩa là, một trọng số có điểm số trọng số cao hơn được xem là quan trọng hơn. Chúng tôi tìm một mạng con thưa θ̂t của mạng nơ-ron và gán nó như một bộ giải của tác vụ hiện tại t. Chúng tôi sử dụng các mạng con thay vì mạng dày đặc như các bộ giải vì hai lý do: (1) Giả Thuyết Vé Số May Mắn [18] cho thấy sự tồn tại của một mạng con hoạt động tốt như toàn bộ mạng, và (2) mạng con yêu cầu ít dung lượng hơn so với các mạng dày đặc, và do đó nó vốn dĩ giảm kích thước của việc mở rộng bộ giải.

Được thúc đẩy bởi những lợi ích như vậy, chúng tôi đề xuất một Mạng Con Chiến Thắng (WSN¹) mới, đó là phương pháp đào tạo kết hợp cho học liên tục đào tạo trên tác vụ t trong khi chọn một mạng con quan trọng cho tác vụ t như được hiển thị trong Hình 2. Hình minh họa của WSN giải thích cách thu được các trọng số nhị phân trong một mạng dày đặc từng bước một. Chúng tôi tìm θ̂t bằng cách chọn c% trọng số có điểm số trọng số s cao nhất, trong đó c là tỷ lệ dung lượng theo lớp mục tiêu trong %. Một trọng số nhị phân phụ thuộc tác vụ đại diện cho việc chọn trọng số mt trong đó giá trị 1 biểu thị rằng trọng số được chọn trong lượt truyền thuận và 0 ngược lại. Chính thức, mt được thu được bằng cách áp dụng một hàm chỉ thị 1c trong đó 1c(s) = 1 nếu s thuộc về c% điểm số hàng đầu và 0 ngược lại. Do đó, mạng con θ̂t cho tác vụ t được thu được bởi θ̂t = mt ⊙ θ.

3.2 Mạng Con Mềm (SoftNet)

Một số công trình đã giải quyết các vấn đề quá khớp trong học liên tục từ các góc độ khác nhau, bao gồm NCM [31], BiC [73], OCS [79], và FSLL [52]. Để giảm thiểu vấn đề quá khớp trong các mạng con, chúng tôi sử dụng một phương pháp đơn giản nhưng hiệu quả tên là SoftNet được đề xuất bởi [37]. Mô hình mới sau, được gọi là Giả Thuyết Vé Số May Mắn Có Điều Chỉnh [37] được truyền cảm hứng từ Giả Thuyết Vé Số May Mắn [18] đã trở thành nền tảng của SoftNet:

Giả Thuyết Vé Số May Mắn Có Điều Chỉnh (RLTH). Một mạng nơ-ron dày đặc được khởi tạo ngẫu nhiên chứa một mạng con được điều chỉnh có thể giữ lại kiến thức lớp trước đó trong khi cung cấp không gian để học kiến thức lớp mới thông qua việc đào tạo cô lập của mạng con.

SoftNet. Dựa trên RLTH, chúng tôi đề xuất một phương pháp, được gọi là Mạng Con Mềm (SoftNet²). SoftNet cùng học mô hình dày đặc được khởi tạo ngẫu nhiên, và mặt nạ mềm m ∈ [0,1]|θ| trên Mạng con mềm trong mỗi lần đào tạo tác vụ; mặt nạ mềm bao gồm phần chính của các tham số mô hình m = 1 và những phần nhỏ m < 1 trong đó m = 1 được thu được bởi c% hàng đầu của các tham số mô hình và m < 1 được thu được bởi những phần còn lại (100 - c% hàng đầu) được lấy mẫu từ phân phối đều U(0,1). Ở đây, việc chọn các tham số nhỏ m < 1 trong một mạng dày đặc cho trước là rất quan trọng.

[Tiếp tục với các phần còn lại của tài liệu...]

3.3 Hội tụ của Mạng Con

Hội tụ của HardNet (WSN) / SoftNet. Để giải thích sự hội tụ của SoftNet, chúng tôi tuân theo các gradient mục tiêu liên tục Lipschitz [5], [4]: hàm mục tiêu của các mạng dày đặc R: Rᵈ → R có thể vi phân liên tục và hàm gradient của R, cụ thể là ∇R: Rᵈ → Rᵈ, liên tục Lipschitz với hằng số Lipschitz L > 0, tức là

||∇R(θ) - ∇R(θ')|| ≤ L||θ - θ'||

cho tất cả {θ, θ'} ∈ Rᵈ:                                        (3)

Mệnh đề. Mạng con đạt được tốc độ nhanh hơn so với các mạng dày đặc. Để chứng minh điều này, tuân theo cùng công thức, chúng tôi định nghĩa các gradient mục tiêu liên tục Lipschitz của các mạng con như sau:

||∇R(θ ⊙ m) - ∇R(θ' ⊙ m)|| ≤ L||(θ - θ') ⊙ m||

cho tất cả {θ, θ'} ∈ Rᵈ:                                        (4)

trong đó m là một mặt nạ nhị phân. Trong so sánh Phương trình 3 và 4, chúng tôi sử dụng phân tích lý thuyết [77] trong đó mạng con đạt được tốc độ nhanh hơn của R(θ ⊙ m) = O(1/||m||₁²) nhiều nhất. So sánh như sau:

||∇R(θ ⊙ m) - ∇R(θ' ⊙ m)||
||(θ - θ') ⊙ m|| < 
||∇R(θ) - ∇R(θ')||
||θ - θ'|| ≤ L                                                  (5)

Giá trị càng nhỏ, giải pháp (cảnh quan tổn thất) càng phẳng. Phương trình được thiết lập từ mối quan hệ R(θ ⊙ m) ≤ R(θ), trong đó R(θ) biểu thị tổn thất tốt nhất có thể đạt được bằng các kết hợp lồi của tất cả các tham số mặc dù ||(θ - θ') ⊙ m|| < ||θ - θ'||.

Hệ quả. Hơn nữa, chúng tôi có bất đẳng thức sau nếu ||R(mhard) - R(msoft)|| ≈ 0 và ||mhard|| < ||msoft||:

||∇R(mhard) - ∇R(θ' ⊙ mhard)||
||(θ - θ') ⊙ mhard|| ≤ 
||∇R(msoft) - ∇R(θ' ⊙ msoft)||
||(θ - θ') ⊙ msoft||                                           (6)

trong đó đẳng thức giữ nếu ||mhard|| = ||msoft||. Chúng tôi chuẩn bị các cảnh quan tổn thất của Mạng Dày Đặc, HardNet (WSN), và SoftNet như được hiển thị trong Hình 3 như một ví dụ để hỗ trợ bất đẳng thức mạng con ở trên.

3.4 Tối Ưu Hóa Mạng Con Chiến Thắng (WSN)

Để cùng học các trọng số mô hình và các mặt nạ nhị phân thích ứng tác vụ của các mạng con liên quan đến từng tác vụ, cho một mục tiêu L(·), chúng tôi tối ưu hóa θ và s với:

minimize θ,s L(θ ⊙ mt, Dt):                                    (7)

Tuy nhiên, thủ tục tối ưu hóa đơn giản này trình bày hai vấn đề: (1) cập nhật tất cả θ khi đào tạo cho các tác vụ mới sẽ gây ra can thiệp với các trọng số được phân bổ cho các tác vụ trước đó, và (2) hàm chỉ thị luôn có giá trị gradient bằng 0; do đó, cập nhật điểm số trọng số s với gradient tổn thất của nó là không thể. Để giải quyết vấn đề đầu tiên, chúng tôi cập nhật có chọn lọc các trọng số bằng cách chỉ cho phép cập nhật trên các trọng số không được chọn trong các tác vụ trước đó. Để làm điều đó, chúng tôi sử dụng một mặt nạ nhị phân tích lũy Mt-1 = ⋃ᵢ₌₁ᵗ⁻¹ mi khi học tác vụ t, sau đó cho một bộ tối ưu hóa với tốc độ học α, θ được cập nhật như sau:

θ ← θ - α ∂L/∂θ ⊙ (1 - Mt-1);                                 (8)

hiệu quả đóng băng các trọng số của các mạng con được chọn cho các tác vụ trước đó. Để giải quyết vấn đề thứ hai, chúng tôi sử dụng Straight-through Estimator [28], [3], [58] trong lượt truyền ngược vì mt được thu được bởi c% điểm số hàng đầu. Cụ thể, chúng tôi bỏ qua các đạo hàm của hàm chỉ thị và cập nhật điểm số trọng số như sau:

s ← s - α ∂L/∂s:                                              (9)

Việc sử dụng điểm số trọng số riêng s làm cơ sở cho việc chọn trọng số mạng con làm cho việc tái sử dụng một số trọng số từ các trọng số đã chọn trước đó mt trong việc giải quyết tác vụ hiện tại t trở nên khả thi, có thể được xem như học chuyển tiếp. Tương tự, các trọng số đã chọn trước đó không liên quan đến các tác vụ mới không được chọn; thay vào đó, các trọng số từ tập hợp các trọng số chưa được chọn được chọn để đáp ứng dung lượng mạng mục tiêu cho mỗi tác vụ, có thể được xem như tinh chỉnh từ các tác vụ {1, ..., t-1} đến tác vụ t.

Thủ tục tối ưu hóa WSN của chúng tôi được tóm tắt trong thuật toán giả 1.

Thuật toán 1 Mạng Con Chiến Thắng (WSN) cho TIL.
đầu vào {Dt}ᵀt=1, trọng số mô hình θ, trọng số điểm s, mặt nạ nhị phân M0 = 0|θ|, dung lượng theo lớp c
1: Khởi tạo ngẫu nhiên θ và s.
2: cho tác vụ t = 1, ..., T thì
3:   cho batch bt ∈ Dt thì
4:     Thu được soft-mask mt của c% điểm số hàng đầu s tại mỗi lớp
5:     Tính toán L(θ ⊙ mt, bt)
6:     θ ← θ - α ∂L/∂θ ⊙ (1 - Mt-1)    .Cập nhật trọng số
7:     s ← s - α(∂L/∂s)                  .Cập nhật điểm số trọng số
8:   kết thúc cho
9:   Mt ← Mt-1 ⋃ mt                      .Tích lũy mặt nạ nhị phân
10: kết thúc cho

3.5 Mã Hóa Vé Chiến Thắng

Một mặt nạ nhị phân cần thiết để lưu trữ các trọng số cụ thể cho tác vụ cho mỗi tác vụ trong WSN. Tuy nhiên, vấn đề chính là khi số lượng tác vụ tăng trong các mô hình học sâu, số lượng mặt nạ nhị phân cần thiết cũng tăng. Chúng tôi sử dụng một thuật toán nén để nén tất cả các mặt nạ tác vụ nhị phân để giải quyết vấn đề dung lượng này và đạt được học liên tục không quên. Để thực hiện điều này, trước tiên chúng tôi chuyển đổi một chuỗi các mặt nạ nhị phân thành một mặt nạ thập phân tích lũy duy nhất và sau đó chuyển đổi mỗi số nguyên thành một mã ASCII để đại diện cho một ký hiệu duy nhất. Tiếp theo, chúng tôi áp dụng mã hóa Huffman [32], một thuật toán nén không mất dữ liệu, cho các ký hiệu. Chúng tôi quan sát thực nghiệm rằng mã hóa Huffman nén các bản đồ nhị phân 7-bit với tỷ lệ nén khoảng 78% và giải nén chúng mà không mất bất kỳ bit nào. Hơn nữa, kết quả thực nghiệm chứng minh rằng tỷ lệ nén tăng dưới tuyến tính với kích thước của các bit nhị phân.

3.6 SoftNet thông qua Vé Chiến Thắng Bổ Sung

Tương tự như tối ưu hóa WSN được thảo luận trong Phần 3.4, để mỗi trọng số được liên kết với một tham số có thể học mà chúng tôi gọi là điểm số trọng số, xác định bằng số tầm quan trọng của trọng số liên kết. Nói cách khác, chúng tôi tuyên bố một trọng số có điểm số cao hơn quan trọng hơn. Đầu tiên, chúng tôi tìm một mạng con θ̂ = θ ⊙ m*t của mạng nơ-ron dày đặc và sau đó gán nó như một bộ giải của phiên hiện tại t. Các mạng con liên quan đến mỗi phiên cùng học trọng số mô hình θ và mặt nạ nhị phân mt. Cho một mục tiêu Lt, chúng tôi tối ưu hóa θ như sau:

θ*, m*t = minimize θ,s Lt(θ ⊙ mt, Dt):                        (10)

trong đó mt được thu được bằng cách áp dụng một hàm chỉ thị 1c trên điểm số trọng số s. Lưu ý 1c(s) = 1 nếu s thuộc về c% điểm số hàng đầu và 0 ngược lại.

Trong quá trình tối ưu hóa cho FSCIL, tuy nhiên, chúng tôi xem xét hai vấn đề chính: (1) Quên thảm khốc: cập nhật tất cả θ ⊙ mt-1 khi đào tạo cho các phiên mới sẽ gây ra can thiệp với các trọng số được phân bổ cho các tác vụ trước đó; do đó, chúng tôi cần đóng băng tất cả các tham số đã học trước đó θ ⊙ mt-1; (2) Quá khớp: mạng con cũng gặp phải các vấn đề quá khớp khi đào tạo một tác vụ tăng dần trên một vài mẫu, vì vậy, chúng tôi cần cập nhật một vài tham số không liên quan đến kiến thức tác vụ trước đó, tức là θ ⊙ (1 - mt-1).

Để có được các mạng con tối ưu giảm thiểu hai vấn đề, chúng tôi định nghĩa một mạng con mềm bằng cách chia mạng nơ-ron dày đặc thành hai phần - một là mạng con chính mmajor, và một khác là mạng con phụ mminor. Mạng Con Mềm (SoftNet) được định nghĩa như sau:

msoft = mmajor ⊕ mminor;                                       (11)

trong đó mmajor là một mặt nạ nhị phân và mminor ~ U(0,1) và ⊕ đại diện cho phép cộng theo từng phần tử. Như vậy, một mặt nạ mềm được cho là m*t ∈ [0,1]|θ| trong Phương trình 10. Trong tất cả các cài đặt thực nghiệm FSCIL, mmajor duy trì kiến thức tác vụ cơ sở t = 1 trong khi mminor thu được kiến thức tác vụ mới t ≥ 2. Sau đó, với tốc độ học phiên cơ sở α, θ được cập nhật như sau: θ ← θ - α ∂L/∂θ ⊙ msoft hiệu quả điều chỉnh các trọng số của các mạng con cho học tăng dần.

Các mạng con được thu được bởi hàm chỉ thị luôn có giá trị gradient bằng 0; do đó, cập nhật điểm số trọng số s với gradient tổn thất của nó là không thể. Chúng tôi cập nhật điểm số trọng số bằng cách sử dụng Straight-through Estimator [28], [3], [58] trong lượt truyền ngược. Cụ thể, chúng tôi bỏ qua các đạo hàm của hàm chỉ thị và cập nhật điểm số trọng số s ← s - α ∂L/∂s ⊙ msoft, trong đó msoft = 1 để khám phá mạng con tối ưu cho việc đào tạo phiên cơ sở.

Thủ tục tối ưu hóa Mạng con mềm của chúng tôi được tóm tắt trong Thuật toán 2. Một khi một mạng con mềm duy nhất msoft được thu được trong phiên cơ sở, sau đó chúng tôi sử dụng mạng con mềm cho toàn bộ các phiên mới mà không cập nhật.

3.7 Mạng Con Mềm cho Học Tăng Dần

Bây giờ chúng tôi mô tả thủ tục tổng thể của phương pháp học tăng dần/suy luận dựa trên cắt tỉa mềm của chúng tôi dưới các cài đặt FSCIL sau. Điều này bao gồm giai đoạn đào tạo với một phép đo thông tin được chuẩn hóa, như được nêu trong công trình trước đó [65], và giai đoạn suy luận.

Thuật toán 2 Mạng Con Mềm (SoftNet) cho FSCIL.
đầu vào {Dt}ᵀt=1, trọng số mô hình θ, và trọng số điểm s, dung lượng theo lớp c
1: // Đào tạo trên các lớp cơ sở t = 1
2: Khởi tạo ngẫu nhiên θ và s.
3: cho epoch e = 1, 2, ... thì
4:   Thu được mặt nạ mềm msoft của mmajor và mminor ~ U(0,1) tại mỗi lớp
5:   cho batch bt ∈ Dt thì
6:     Tính toán Lbase(θ ⊙ msoft, bt) bởi Phương trình 10
7:     θ ← θ - α ∂L/∂θ ⊙ msoft
8:     s ← s - α ∂L/∂s ⊙ msoft
9:   kết thúc cho
10: kết thúc cho
11: // Học tăng dần t ≥ 2
12: Kết hợp dữ liệu đào tạo Dt
13: và các exemplar được lưu trong các phiên ít mẫu trước đó
14: cho epoch e = 1, 2, ... thì
15:   cho batch bt ∈ Dt thì
16:     Tính toán Lm(θ ⊙ msoft, bt) bởi Phương trình 12
17:     θ ← θ - α ∂L/∂θ ⊙ mminor
18:   kết thúc cho
19: kết thúc cho
đầu ra các tham số mô hình θ, s, và msoft.

Học Tăng Dần Lớp Ít Mẫu (FSCIL) nhằm học các phiên mới chỉ với một vài ví dụ một cách liên tục. Một mô hình FSCIL học một chuỗi T phiên đào tạo {D1, ..., DT}, trong đó Dt = {z^t_i = (x^t_i, y^t_i)}^{n_t}_{i=1} là dữ liệu đào tạo của phiên t và x^t_i là một ví dụ của lớp y^t_i ∈ Ot. Trong FSCIL, phiên cơ sở D1 thường chứa một số lượng lớn các lớp với dữ liệu đào tạo đủ cho mỗi lớp. Ngược lại, các phiên tiếp theo (t ≥ 2) sẽ chỉ chứa một số lượng nhỏ các lớp với một vài mẫu đào tạo cho mỗi lớp, ví dụ, phiên thứ t Dt thường được trình bày như một tác vụ N-way K-shot. Trong mỗi phiên đào tạo t, mô hình chỉ có thể truy cập dữ liệu đào tạo Dt và một vài ví dụ được lưu trữ trong phiên trước đó. Khi việc đào tạo của phiên t được hoàn thành, chúng tôi đánh giá mô hình trên các mẫu kiểm tra từ tất cả các lớp O = ⋃^t_{i=1} Oi, trong đó Oi ∩ Oj≠i = ∅ cho ∀i, j ≤ T.

Đào Tạo Cơ Sở (t = 1). Trong phiên học cơ sở, chúng tôi tối ưu hóa tham số mạng con mềm θ (bao gồm một lớp kết nối đầy đủ như một bộ phân loại) và điểm số trọng số s với tổn thất entropy chéo cùng sử dụng các ví dụ đào tạo của D1.

Đào Tạo Tăng Dần (t ≥ 2). Trong các phiên học ít mẫu tăng dần (t ≥ 2), được tận dụng bởi θ ⊙ msoft, chúng tôi tinh chỉnh một vài tham số nhỏ mminor của mạng con mềm để học các lớp mới. Vì mminor < 1, mạng con mềm giảm thiểu việc quá khớp của một vài mẫu. Hơn nữa, thay vì khoảng cách Euclid [65], chúng tôi sử dụng một thuật toán phân loại dựa trên metric với khoảng cách cosine để tinh chỉnh một vài tham số được chọn. Trong một số trường hợp, khoảng cách Euclid không cung cấp được khoảng cách thực giữa các biểu diễn, đặc biệt là khi hai điểm có cùng khoảng cách từ các nguyên mẫu không rơi vào cùng một lớp. Ngược lại, các biểu diễn có khoảng cách cosine thấp được đặt trong cùng hướng từ gốc tọa độ, cung cấp một phép đo thông tin được chuẩn hóa. Chúng tôi định nghĩa hàm tổn thất như:

Lm(z; θ ⊙ msoft) = ∑_{z∈D} ∑_{o∈O} 1(y=o) log (e^{d(p_o,f(x;θ⊙m_{soft}))} / ∑_{o_k∈O} e^{d(p_{o_k},f(x;θ⊙m_{soft}))})   (12)

trong đó d(·,·) biểu thị khoảng cách cosine, po là nguyên mẫu của lớp o, O = ⋃^t_{i=1} Oi đề cập đến tất cả các lớp gặp phải, và D = Dt ∪ P biểu thị sự hợp nhất của dữ liệu đào tạo hiện tại Dt và tập exemplar P = {p2, ..., pt-1}, trong đó P_{te} (2 ≤ te < t) là tập các exemplar được lưu trong phiên te. Lưu ý rằng các nguyên mẫu của các lớp mới được tính toán bởi po = (1/No) ∑_i 1(yi = o)f(xi; θ ⊙ msoft) và những của các lớp cơ sở được lưu trong phiên cơ sở, và No biểu thị số lượng hình ảnh đào tạo của lớp o. Chúng tôi cũng lưu các nguyên mẫu của tất cả các lớp trong Ot để đánh giá sau này.

Suy Luận cho Mạng Con Mềm Tăng Dần. Trong mỗi phiên, suy luận cũng được thực hiện bởi một thuật toán phân loại trung bình lớp gần nhất (NCM) đơn giản [54], [65] để so sánh công bằng. Cụ thể, tất cả các mẫu đào tạo và kiểm tra được ánh xạ đến không gian nhúng của bộ trích xuất đặc trưng f, và khoảng cách Euclid du(·,·) được sử dụng để đo độ tương tự giữa chúng. Bộ phân loại đưa ra chỉ số nguyên mẫu thứ k o*k = arg min_{o∈O} du(f(x; θ ⊙ msoft), po) như đầu ra.

4 THỰC NGHIỆM

Bây giờ chúng tôi xác thực phương pháp của chúng tôi trên một số bộ dữ liệu điểm chuẩn chống lại các đường cơ sở học liên tục liên quan trên Học Tăng Dần Tác Vụ (TIL) và Học Tăng Dần Lớp Ít Mẫu (FSCIL).

4.1 Học Tăng Dần Tác Vụ (TIL)

Đầu tiên, chúng tôi xem xét học liên tục tăng dần tác vụ với cấu hình đa đầu cho tất cả các thực nghiệm trong bài báo. Chúng tôi tuân theo các thiết lập thực nghiệm trong các công trình gần đây [62], [78], [16].

Bộ dữ liệu và kiến trúc. Chúng tôi sử dụng sáu bộ dữ liệu tuần tự khác nhau phổ biến cho các vấn đề CL với năm kiến trúc mạng nơ-ron khác nhau như sau: 1) Permuted MNIST (PMNIST): Một biến thể của MNIST [42] trong đó mỗi tác vụ có một hoán vị xác định cho các pixel hình ảnh đầu vào. 2) 5-Datasets: Một hỗn hợp của 5 bộ dữ liệu thị giác khác nhau [62]: CIFAR-10 [39], MNIST [42], SVHN [56], FashionMNIST [74], và notMNIST [6]. 3) Omniglot Rotation: Một bộ dữ liệu hình ảnh OCR bao gồm 100 tác vụ, mỗi tác vụ bao gồm 12 lớp. Chúng tôi tiếp tục tiền xử lý các hình ảnh thô bằng cách tạo ra phiên bản quay của chúng ở 90°, 180°, và 270°, theo [78]. 4) CIFAR-100 Split [39]: Một bộ dữ liệu đối tượng thị giác được xây dựng bằng cách chia ngẫu nhiên 100 lớp của CIFAR-100 thành mười tác vụ với mười lớp cho mỗi tác vụ. 5) CIFAR-100 Superclass: Chúng tôi tuân theo cài đặt từ [78] chia bộ dữ liệu CIFAR-100 thành 20 tác vụ theo 20 siêu lớp, và mỗi siêu lớp chứa năm lớp khác nhau nhưng liên quan về mặt ngữ nghĩa. 6) TinyImageNet [67]: Một biến thể của ImageNet [40] chứa 40 tác vụ phân loại 5-way với kích thước hình ảnh 64×64×3.

Chúng tôi sử dụng MLP hai lớp với 100 nơ-ron cho mỗi lớp cho PMNIST, các biến thể của LeNet [42] cho các thực nghiệm trên Omniglot Rotation và CIFAR-100 Superclass. Ngoài ra, chúng tôi sử dụng một phiên bản sửa đổi của AlexNet tương tự như [64], [62] cho bộ dữ liệu CIFAR-100 Split và một ResNet-18 thu gọn tương tự như [9], [62] cho 5-Datasets. Đối với TinyImageNet, chúng tôi cũng sử dụng cùng kiến trúc mạng như [20], [16], bao gồm 4 lớp Conv và ba lớp kết nối đầy đủ.

Đường cơ sở. Chúng tôi so sánh WSN của chúng tôi với các đường cơ sở CL mạnh; các phương pháp dựa trên điều chỉnh: HAT [64] và EWC [38], các phương pháp dựa trên luyện tập: GPM [62], và một phương pháp dựa trên cắt tỉa: PackNet [50] và SupSup [72]. PackNet và SupSup được đặt làm đường cơ sở để thể hiện hiệu quả của các trọng số được tái sử dụng. Chúng tôi cũng so sánh với chiến lược đào tạo tuần tự đơn giản, được gọi là FINETUNE. Học Đa Tác Vụ (MTL) và Học Đơn Tác Vụ (STL) không phải là phương pháp CL. MTL đào tạo trên nhiều tác vụ đồng thời, và STL đào tạo trên các tác vụ đơn lẻ một cách độc lập.

Chúng tôi tóm tắt các đường cơ sở dựa trên kiến trúc như sau:
1) PackNet [50]: kiến trúc cắt tỉa lặp đi lặp lại và đào tạo lại mạng để đóng gói nhiều tác vụ vào một mạng đơn.
2) SupSup [72]: tìm siêu mặt nạ (mạng con) trong một mạng được khởi tạo ngẫu nhiên cho mỗi tác vụ trong học liên tục.
3) WSN / SoftNet (của chúng tôi): đào tạo kết hợp mô hình và tìm các mạng con thích ứng tác vụ của các tham số mới/trước đó cho học liên tục. Lưu ý rằng, trong bước suy luận, SoftNet được thu được thực nghiệm bằng cách tiêm nhiễu nhỏ U(0, 1e-3) vào nền 0 của WSN tác vụ đã có trong khi giữ nguyên tiền cảnh 1 của WSN.

Cài đặt thực nghiệm. Vì chúng tôi triển khai trực tiếp phương pháp của chúng tôi từ mã chính thức của [62], chúng tôi cung cấp các giá trị cho HAT và GPM được báo cáo trong [62]. Đối với Omniglot Rotation và Split CIFAR-100 Superclass, chúng tôi triển khai kiến trúc được đề xuất trong cài đặt đa đầu với các siêu tham số như được báo cáo trong [78]. Tất cả các thực nghiệm của chúng tôi chạy trên thiết lập GPU đơn của NVIDIA V100. Chúng tôi cung cấp thêm chi tiết về các bộ dữ liệu, kiến trúc, và cài đặt thực nghiệm, bao gồm các cấu hình siêu tham số cho tất cả các phương pháp trong Phần A.

Các metric hiệu suất. Chúng tôi đánh giá tất cả các phương pháp dựa trên bốn metric sau:

1) Độ chính xác (ACC) đo trung bình của độ chính xác phân loại cuối cùng trên tất cả các tác vụ: ACC = (1/T) ∑^T_{i=1} AT,i, trong đó AT,i là độ chính xác kiểm tra cho tác vụ i sau khi đào tạo trên tác vụ T.

2) Dung lượng (CAP) đo tổng phần trăm của các trọng số khác không cộng với các mặt nạ chính cho tất cả các tác vụ như sau: Capacity = (1-S) + (1-ρ)×T×(32), trong đó chúng tôi giả định tất cả các trọng số tác vụ có độ chính xác 32-bit. S là độ thưa của MT và tỷ lệ nén trung bình ρ ≈ 0.78 mà chúng tôi thu được thông qua mã hóa Huffman 7bit, phụ thuộc vào kích thước của các bản đồ bit-nhị phân; tỷ lệ nén cũng phụ thuộc vào các phương pháp nén điển hình.

3) Chuyển Tiếp Về Phía Trước (FWT) đo mức độ các biểu diễn mà chúng ta đã học cho đến nay giúp học các tác vụ mới, cụ thể: FWT = (1/T) ∑^T_{i=2} Ai-1,i - Ri trong đó Ri là độ chính xác của một mạng được khởi tạo ngẫu nhiên trên tác vụ i.

4) Chuyển Tiếp Về Phía Sau (BWT) đo việc quên trong quá trình học liên tục. BWT âm có nghĩa là học các tác vụ mới gây ra việc quên trên các tác vụ quá khứ: BWT = (1/(T-1)) ∑^{T-1}_{i=1} AT,i - Ai,i.

4.2 Học Tăng Dần Lớp Ít Mẫu (FSCIL)

Chúng tôi giới thiệu các thiết lập thực nghiệm - cài đặt Học Tăng Dần Lớp Ít Mẫu (FSCIL) để cung cấp hiệu quả của các mạng con mềm. Chúng tôi đánh giá thực nghiệm và so sánh các mạng con mềm của chúng tôi với các phương pháp tiên tiến và mạng con đơn giản trong các phần phụ sau.

Bộ dữ liệu. Để xác thực hiệu quả của mạng con mềm, chúng tôi tuân theo cài đặt thực nghiệm FSCIL tiêu chuẩn. Chúng tôi chọn ngẫu nhiên 60 lớp làm cơ sở và 40 lớp làm lớp mới cho CIFAR-100 và miniImageNet. Trong mỗi phiên học tăng dần, chúng tôi xây dựng các tác vụ 5-way 5-shot bằng cách chọn ngẫu nhiên năm lớp và lấy mẫu năm ví dụ đào tạo cho mỗi lớp.

Đường cơ sở. Chúng tôi chủ yếu so sánh SoftNet của chúng tôi với các phương pháp dựa trên kiến trúc cho FSCIL: FSLL [52] chọn các tham số quan trọng cho mỗi phiên, và HardNet, đại diện cho một mạng con nhị phân. Hơn nữa, chúng tôi so sánh các phương pháp FSCIL khác như iCaRL [59], Rebalance [31], TOPIC [68], IDLVQ-C [10], và F2M [65]. Chúng tôi cũng bao gồm một phương pháp đào tạo kết hợp [65] sử dụng tất cả dữ liệu đã thấy trước đó, bao gồm cơ sở và các tác vụ ít mẫu tiếp theo để đào tạo làm tham chiếu. Hơn nữa, chúng tôi cố định phương pháp đào tạo lại bộ phân loại (cRT) [35] cho phân loại đuôi dài được đào tạo với tất cả dữ liệu gặp phải như cận trên xấp xỉ.

Cài đặt thực nghiệm. Các thực nghiệm được thực hiện với NVIDIA GPU RTX8000 trên CUDA 11.0. Chúng tôi cũng chia ngẫu nhiên mỗi bộ dữ liệu thành nhiều phiên. Chúng tôi chạy mỗi thuật toán mười lần cho mỗi bộ dữ liệu và báo cáo độ chính xác trung bình của chúng. Chúng tôi sử dụng ResNet18 [24] làm mạng xương sống. Để tăng cường dữ liệu, chúng tôi sử dụng cắt ngẫu nhiên tiêu chuẩn và lật ngang. Trong giai đoạn đào tạo phiên cơ sở, chúng tôi chọn c% trọng số hàng đầu tại mỗi lớp và thu được các mạng con mềm tối ưu với độ chính xác xác thực tốt nhất. Trong mỗi phiên học ít mẫu tăng dần, tổng số epoch đào tạo là 6, và tốc độ học là 0.02. Chúng tôi đào tạo các mẫu phiên lớp mới sử dụng một vài trọng số nhỏ của mạng con mềm (lớp Conv4x của ResNet18) thu được bởi việc học phiên cơ sở. Chúng tôi chỉ định thêm chi tiết thực nghiệm trong Phụ lục.

5 KẾT QUẢ VỀ HỌC TĂNG DẦN TÁC VỤ

5.1 So Sánh với Đường Cơ Sở

Chúng tôi đánh giá thuật toán của chúng tôi trên ba bộ dữ liệu điểm chuẩn tiêu chuẩn: Permuted MNIST, 5-Datasets, và Omniglot Rotation. Chúng tôi đặt PackNet và SupSup làm đường cơ sở như các phương pháp trọng số không tái sử dụng và so sánh WSN với các đường cơ sở, bao gồm các thuật toán khác như được hiển thị trong Bảng 1. WSN của chúng tôi vượt trội hơn tất cả các đường cơ sở trong ba phép đo, đạt được độ chính xác trung bình tốt nhất lần lượt là 96.41%, 93.41%, và 87.28% trong khi sử dụng ít dung lượng nhất so với các phương pháp hiện có khác. Hơn nữa, WSN của chúng tôi được chứng minh là một mô hình không quên giống như PackNet. So sánh với PackNet, tuy nhiên, WSN cho thấy hiệu quả của các trọng số tái sử dụng cho học liên tục và khả năng mở rộng của nó trong các thực nghiệm Omniglot Rotation với ít dung lượng nhất với một biên độ lớn. Hình 4 cung cấp độ chính xác trên tổng sử dụng dung lượng. Độ chính xác WSN của chúng tôi cao hơn so với PackNet, khoảng 80% tổng sử dụng dung lượng trên 5-Dataset, và WSN vượt trội hơn các phương pháp khác ở 80% tổng sử dụng dung lượng trên Omniglot Rotation. Hiệu suất thấp hơn của PackNet có thể do thống kê bộ dữ liệu Omniglot Rotation vì, bất kể các phép quay ngẫu nhiên, các tác vụ có thể chia sẻ các đặc trưng thị giác chung như vòng tròn, đường cong, và đường thẳng. Do đó, các phương pháp không tái sử dụng có thể khắc nghiệt để đào tạo một mô hình tác vụ mới trừ khi các trọng số trước đó được chuyển đến mô hình hiện tại, tức là học chuyển tiếp về phía trước. Để thể hiện sức mạnh chuyển tiếp kiến thức của WSN, SoftNet được chuẩn bị bằng cách khởi tạo phần không của mặt nạ thành các nhiễu ngẫu nhiên nhỏ. Sau đó, chúng tôi quan sát rằng, trong bước suy luận, SoftNet cho thấy sức mạnh tuyệt vời để chuyển tiếp kiến thức trong khi duy trì hiệu suất ACC và CAP của WSN.

5.2 So Sánh với SOTA trong TIL

Chúng tôi sử dụng cài đặt đa đầu để đánh giá thuật toán WSN của chúng tôi dưới các điểm chuẩn phân loại thị giác thách thức hơn. Hiệu suất của WSN được so sánh với các phương pháp khác w.r.t ba phép đo trên ba bộ dữ liệu điểm chuẩn chính như được hiển thị trong Bảng 2. WSN của chúng tôi vượt trội hơn tất cả tiên tiến, đạt được độ chính xác trung bình tốt nhất 76.38%, 61.79%, và 71.96%. WSN cũng là một mô hình không quên (BWT = ZERO) với ít dung lượng mô hình nhất trong các thực nghiệm này. Lưu ý rằng chúng tôi giả định các dung lượng mô hình được so sánh dựa trên kích thước mô hình mà không có bộ nhớ bổ sung, chẳng hạn như mẫu. Chúng tôi nhấn mạnh rằng phương pháp của chúng tôi đạt được độ chính xác cao nhất, dung lượng thấp nhất, và chuyển tiếp ngược trên tất cả các bộ dữ liệu. Hình 5 cho thấy quá trình hiệu suất và dung lượng nén thay đổi với số lượng tác vụ trên các bộ dữ liệu TinyImageNet, trong đó metric "Dung Lượng Tiến Bộ Trung Bình" được định nghĩa là dung lượng trung bình (tỷ lệ số lượng trọng số mạng được sử dụng cho bất kỳ tác vụ nào) sau năm lần chạy thực nghiệm với các giá trị seed khác nhau. Hơn nữa, chúng tôi liên tục cho thấy hiệu suất được cải thiện tiến bộ của WSN hơn các phương pháp khác trên các bộ dữ liệu CIFAR-100 Split như được hiển thị trong Hình 7. Số lượng trọng số tái sử dụng tăng (xem Hình 9) có thể giải thích các hiệu suất được cải thiện tiến bộ như được hiển thị trong Hình 8. Các kết quả chuyển tiếp về phía trước khác được mô tả trong Phụ lục.

5.3 Hiệu Suất Không Quên và Dung Lượng Mô Hình

Chúng tôi chuẩn bị kết quả về hiệu suất và dung lượng nén bit-map trên bộ dữ liệu TinyImageNet như được hiển thị trong Hình 5 - "c=0.1" và "c=0.1+7bit-Huffman" đề cập tương ứng đến việc sử dụng 10% trọng số mạng và không nén trên mặt nạ nhị phân và cái sau đề cập đến cùng với mã hóa Huffman 7bit. Trong Hình 5 (a), sử dụng dung lượng ban đầu, c = 0.1 cho thấy hiệu suất tốt hơn so với các phương pháp khác. Với c = 0.1 cố định, tỷ lệ nén Huffman theo bit mang lại kết quả tích cực khi số lượng tác vụ tăng, như được hiển thị trong Hình 5 (b). Phần thú vị nhất là tỷ lệ nén trung bình tăng khi số lượng bit để nén tăng, và tỷ lệ tăng của các trọng số tái sử dụng (ký hiệu có xác suất cao trong mã hóa Huffman) có thể ảnh hưởng đến tỷ lệ nén (các ký hiệu có xác suất nhỏ có thể hiếm trong cây Huffman, trong đó các ký hiệu không thường xuyên có xu hướng có mã bit dài). Chúng tôi điều tra cách tỷ lệ nén liên quan đến tổng dung lượng mô hình. Việc nén mặt nạ nhị phân càng nhiều bit, dung lượng mô hình cần lưu càng ít. Điều này cho thấy rằng trong 40 tác vụ, dung lượng nén Huffman N-bit tăng dưới tuyến tính khi dung lượng bản đồ nhị phân tăng tuyến tính. Mã hóa Huffman 7-bit đủ để nén các bản đồ nhị phân mà không vượt quá dung lượng mô hình, mặc dù tỷ lệ nén phụ thuộc vào các phương pháp nén điển hình.

5.4 Quên Thảm Khốc Từ Góc Nhìn của WSN

Chúng tôi giải thích cách các trọng số tái sử dụng ảnh hưởng đến hiệu suất suy luận trên bộ dữ liệu TinyImageNet như được hiển thị trong Hình 6. Chúng tôi chia tất cả các trọng số được sử dụng để giải quyết các tác vụ tuần tự thành các tập hợp cụ thể để giải thích chính xác hơn. Tất cả các trọng số được sử dụng (a) trong một mạng dày đặc được đào tạo được tách biệt như sau: tất cả được sử dụng đại diện cho tất cả các tập hợp trọng số được kích hoạt cho đến tác vụ t-1. mỗi tác vụ đại diện cho một tập hợp trọng số được kích hoạt tại tác vụ t. mới mỗi tác vụ đại diện cho một tập hợp trọng số được kích hoạt mới tại tác vụ t. tái sử dụng mỗi tác vụ đại diện cho một tập hợp giao của trọng số mỗi tác vụ và tất cả các trọng số được sử dụng. tái sử dụng cho tất cả tác vụ đại diện cho một tập hợp giao của các trọng số tái sử dụng từ tác vụ 1 đến tác vụ t-1.

Đầu tiên, WSN của chúng tôi tái sử dụng các trọng số một cách thích ứng để giải quyết các tác vụ tuần tự. Trong Hình 6 (b), dung lượng tác vụ ban đầu và tiến bộ bắt đầu từ giá trị c; tỷ lệ trọng số tái sử dụng mỗi tác vụ giảm trong giai đoạn học tác vụ sớm. Tuy nhiên, nó có xu hướng bão hòa tiến bộ đến c = 0.1 vì số lượng tập hợp trọng số được kích hoạt mới giảm, và tỷ lệ trọng số tái sử dụng cho tất cả các tác vụ trước đó có xu hướng giảm. Nói cách khác, WSN sử dụng các trọng số đa dạng để giải quyết các tác vụ tuần tự trong tất cả các trọng số được sử dụng thay vì phụ thuộc vào các trọng số tái sử dụng cho tất cả các tác vụ trước đó khi số lượng tác vụ tăng.

Thứ hai, WSN của chúng tôi cung cấp một bước đệm cho học liên tục không quên. Về lợi ích của WSN, trong Hình 6 (c), chúng tôi giải thích tầm quan trọng của ba loại trọng số thông qua một nghiên cứu loại bỏ. Các đánh giá bổ sung được thực hiện bởi các mặt nạ nhị phân tác vụ đã có và các mô hình được đào tạo để điều tra tầm quan trọng của các trọng số tái sử dụng trong mỗi lớp, trong đó "w/" đề cập đến "với trọng số mạng tái sử dụng" và "w/o" đề cập đến "không có trọng số mạng tái sử dụng." Việc quên mô hình xảy ra từ hiệu suất mà không sử dụng trọng số tái sử dụng mỗi tác vụ một cách nghiêm trọng. Các trọng số quan trọng nhất là trọng số tái sử dụng mỗi tác vụ, tập con của tất cả các trọng số được sử dụng; tầm quan trọng của các trọng số tái sử dụng cho tất cả các tác vụ trước đó giảm khi số lượng tác vụ tăng vì dung lượng của nó trở nên nhỏ tương đối, như được hiển thị trong Hình 6 (b). Hơn nữa, trong Hình 6 (d), chúng tôi kiểm tra việc quên theo lớp gây ra bởi việc loại bỏ trọng số tái sử dụng mỗi tác vụ của các lớp mạng; độ nhạy hiệu suất khá đa dạng. Đặc biệt, chúng tôi quan sát rằng hiệu suất giảm nhiều nhất ở lớp Conv1.

Cuối cùng, WSN của chúng tôi tái sử dụng các trọng số nếu kiến thức từ các tác vụ trước đó đã đủ để giải quyết tác vụ hiện tại và sử dụng một vài trọng số mới ngược lại. Cụ thể, từ tác vụ 7, tất cả các trọng số được sử dụng dường như đủ để suy luận tất cả các tác vụ vì các trọng số tái sử dụng mỗi tác vụ đuổi kịp hiệu suất tác vụ. Để học liên tục không quên tổng quát hơn, mô hình nên xem xét độ nhạy theo lớp của các trọng số tái sử dụng mỗi tác vụ khi chọn các trọng số tái sử dụng cho tất cả các tác vụ trước đó. Những phân tích này có thể tác động rộng rãi đến các lĩnh vực học máy khác, chẳng hạn như chuyển tiếp, bán giám sát, và thích ứng miền.

5.5 Bản Đồ Nhị Phân Thưa

Chúng tôi chuẩn bị các tương quan mặt nạ nhị phân theo tác vụ để điều tra cách WSN tái sử dụng các trọng số qua các tác vụ tuần tự. Như được hiển thị trong Hình 9 (a) và (b), WSN có xu hướng chuyển tiếp tiến bộ các trọng số được sử dụng cho các tác vụ trước đó thành trọng số cho các tác vụ mới so với PackNet. Hình 9 (c) và (d) cho thấy rằng xu hướng của các trọng số tái sử dụng khác nhau theo c. Kết quả này có thể gợi ý rằng các bản đồ nhị phân tái sử dụng thưa hơn dẫn đến tổng quát hóa hơn so với các phương pháp khác.

6 KẾT QUẢ VỀ CIL ÍT MẪU

6.1 Kết Quả và So Sánh

Chúng tôi so sánh SoftNet với các phương pháp FSLL và HardNet (WSN) dựa trên kiến trúc. Chúng tôi chọn FSLL làm đường cơ sở dựa trên kiến trúc vì nó chọn các tham số quan trọng để thu được kiến thức lớp cũ/mới. Kết quả dựa trên kiến trúc trên CIFAR-100 và miniImageNet được trình bày trong Bảng 4 và Bảng 5 tương ứng. Hiệu suất của HardNet cho thấy hiệu quả của các mạng con đi với ít dung lượng mô hình hơn so với các mạng dày đặc. Để nhấn mạnh quan điểm của chúng tôi, chúng tôi phát hiện rằng ResNet18, với khoảng 50% tham số, đạt được hiệu suất tương đương với FSLL trên CIFAR-100 và miniImageNet. Ngoài ra, hiệu suất của ResNet20 với 30% tham số (HardNet) tương đương với những hiệu suất của FSLL trên CIFAR-100.

Kết quả thực nghiệm được chuẩn bị để phân tích hiệu suất tổng thể của SoftNet theo độ thưa và bộ dữ liệu như được hiển thị trong Hình 10. Khi chúng tôi tăng số lượng tham số được sử dụng bởi SoftNet, chúng tôi đạt được mức tăng hiệu suất trên cả hai bộ dữ liệu điểm chuẩn. Phương sai hiệu suất của độ thưa SoftNet dường như phụ thuộc vào bộ dữ liệu vì phương sai hiệu suất trên CIFAR-100 ít hơn so với trên miniImageNet. Ngoài ra, SoftNet giữ lại kiến thức phiên trước đó thành công trong cả hai thực nghiệm như được mô tả trong đường nét đứt, và hiệu suất của SoftNet (c = 60.0%) trên phiên lớp mới (8, 9) của CIFAR-100 hơn so với những của SoftNet (c = 80.0%) như được mô tả trong đường nét đứt-chấm. Từ những kết quả này, chúng ta có thể mong đợi rằng hiệu suất tốt nhất phụ thuộc vào số lượng tham số và đặc tính của bộ dữ liệu.

SoftNet của chúng tôi vượt trội hơn các phương pháp tiên tiến và cRT, được sử dụng như cận trên xấp xỉ của FSCIL [65] như được hiển thị trong Bảng 4 và Bảng 5. Hơn nữa, Hình 11 đại diện cho hiệu suất xuất sắc của SoftNet trên CIFAR-100 và miniImageNet. SoftNet cung cấp một cận trên mới trên mỗi bộ dữ liệu, vượt trội hơn cRT, trong khi HardNet (WSN) cung cấp các đường cơ sở mới trong số các phương pháp dựa trên cắt tỉa.

6.2 Xem Xét Từ SoftNet

Thông qua các thực nghiệm mở rộng, chúng tôi suy ra các kết luận sau cho việc kết hợp phương pháp của chúng tôi trong học tăng dần lớp ít mẫu liên quan đến kiến trúc.

So sánh HardNet (WSN) và SoftNet. Hơn nữa, việc tăng số lượng tham số mạng dẫn đến hiệu suất tổng thể tốt hơn trong cả hai loại mạng con, như được hiển thị trong Hình 12 và Hình 13. Các mạng con, dưới dạng HardNet và SoftNet, có xu hướng giữ lại kiến thức phiên trước đó (cơ sở) được biểu thị trong đường nét đứt (---), và HardNet dường như có thể phân loại các mẫu lớp phiên mới mà không cần cập nhật liên tục được nêu trong đường nét đứt-chấm (-..-). Từ điều này, chúng ta có thể mong đợi bao nhiêu kiến thức trước đó HardNet đã học tại phiên cơ sở để giúp học các tác vụ đến mới (Chuyển Tiếp Về Phía Trước). Hiệu suất tổng thể của SoftNet tốt hơn HardNet vì SoftNet cải thiện cả kiến thức phiên cơ sở/mới bằng cách cập nhật các mạng con nhỏ. Các mạng con có phổ hiệu suất rộng hơn trên miniImageNet (Hình 13) so với trên CIFAR-100 (Hình 12). Điều này có thể là một quan sát gây ra bởi độ phức tạp bộ dữ liệu - tức là, nếu bộ dữ liệu miniImagenet phức tạp hơn hoặc khó học hơn cho một mạng con hoặc một mô hình sâu vì các mạng con cần nhiều tham số hơn để học miniImageNet so với bộ dữ liệu CIFAR-100.

Độ mượt của SoftNet. SoftNet có phổ hiệu suất rộng hơn so với HardNet trên miniImageNet. 20% của mạng con nhỏ có thể cung cấp một biểu diễn mượt hơn so với HardNet vì hiệu suất của SoftNet là tốt nhất khoảng c = 80%. Chúng ta có thể mong đợi rằng độ mượt tham số mô hình đảm bảo hiệu suất khá cạnh tranh từ những kết quả này. Để hỗ trợ tuyên bố, chúng tôi chuẩn bị các cảnh quan tổn thất của một mạng nơ-ron dày đặc, HardNet, và SoftNet trên hai eigenvector Hessian [76] như được hiển thị trong Hình 3. Chúng tôi quan sát các điểm sau thông qua các thực nghiệm đơn giản. Từ những kết quả này, chúng ta có thể mong đợi bao nhiêu kiến thức các mạng con được chỉ định có thể giữ lại và thu được trên mỗi bộ dữ liệu. Các cảnh quan tổn thất của Mạng con (HardNet và SoftNet) phẳng hơn so với những của các mạng nơ-ron dày đặc. Mạng con nhỏ của SoftNet giúp tìm một cực tiểu toàn cục phẳng mặc dù có trọng số tỷ lệ ngẫu nhiên trong quá trình đào tạo.

Hơn nữa, chúng tôi so sánh các nhúng sử dụng các biểu đồ t-SNE như được hiển thị trong Hình 14. Trong không gian nhúng 2D của t-SNE, khả năng phân biệt tổng thể của SoftNet tốt hơn so với HardNet về tập lớp cơ sở và tập lớp mới. 70% của mạng con nhỏ ảnh hưởng tích cực đến SoftNet trong đào tạo phiên cơ sở và cung cấp các trọng số được khởi tạo tốt trong đào tạo phiên mới.

Độ chính xác. Về cài đặt FSCIL CUB200-2011 có kích thước nhỏ và chi tiết như được hiển thị trong Bảng Phụ lục, HardNet (WSN) cũng cho thấy kết quả tương đương với các đường cơ sở, và SoftNet vượt trội hơn các phương pháp khác như được biểu thị trong Bảng 9. Trong cài đặt FSCIL này, chúng tôi đạt được hiệu suất tốt nhất của SoftNet thông qua các lựa chọn tham số cụ thể. Tính đến nay, SoftNet của chúng tôi đạt được kết quả tiên tiến trên ba bộ dữ liệu.

6.3 So Sánh Bổ Sung với SOTA

So sánh với SOTA. Chúng tôi so sánh SoftNet với các phương pháp tiên tiến sau trên chia lớp TOPIC [68] của ba bộ dữ liệu điểm chuẩn - CIFAR100 (Phụ lục, Bảng.7), miniImageNet (Phụ lục, Bảng. 8), và CUB-200-2011 (Phụ lục, Bảng. 9). Chúng tôi tóm tắt các phương pháp FSCIL hiện tại như CEC [83], LIMIT [84], MetaFSCIL [13], C-FSCIL [27], Subspace Reg. [1], Entropy-Reg [47], và ALICE [57]. Được tận dụng bởi ResNet được điều chỉnh, SoftNet vượt trội hơn tất cả các công trình hiện tại trên CIFAR100, miniImageNet. Trên CUB-200-201, hiệu suất của SoftNet tương đương với những của ALICE và LIMIT, xem xét rằng ALICE sử dụng tăng cường lớp/dữ liệu và LIMIT thêm một lớp chú ý đa đầu bổ sung.

7 KẾT LUẬN

Được truyền cảm hứng từ Giả Thuyết Vé Số May Mắn Có Điều Chỉnh (RLTH), trong đó nêu rằng các mạng con mượt mà (không nhị phân) cạnh tranh tồn tại trong một mạng dày đặc trong các tác vụ học liên tục, chúng tôi đã điều tra hiệu suất của hai phương pháp học liên tục dựa trên kiến trúc được đề xuất được gọi là Mạng Con Chiến Thắng (WSN) học tuần tự và chọn một mạng con nhị phân tối ưu (WSN) và một Mạng Con Mềm không nhị phân tối ưu (SoftNet) cho từng tác vụ, tương ứng. Cụ thể, WSN và SoftNet cùng học các trọng số mô hình được điều chỉnh và các mặt nạ không nhị phân thích ứng tác vụ của các mạng con liên quan đến từng tác vụ trong khi cố gắng chọn một tập nhỏ các trọng số được kích hoạt (vé chiến thắng) bằng cách tái sử dụng các trọng số của các mạng con trước đó. WSN và SoftNet được đề xuất vốn dĩ miễn nhiễm với việc quên thảm khốc vì mỗi mô hình mạng con được chọn không xâm phạm các mạng con khác trong Học Tăng Dần Tác Vụ (TIL). Trong TIL, các mặt nạ nhị phân được tạo ra cho mỗi vé chiến thắng được mã hóa thành một mặt nạ chữ số nhị phân N-bit, sau đó được nén bằng mã hóa Huffman để tăng dung lượng mạng dưới tuyến tính theo số lượng tác vụ. Đáng ngạc nhiên, chúng tôi quan sát rằng trong bước suy luận, SoftNet được tạo ra bằng cách tiêm nhiễu nhỏ vào nền của WSN đã có (giữ nguyên tiền cảnh của WSN) cung cấp khả năng chuyển tiếp xuất sắc cho các tác vụ tương lai trong TIL. Softnet thể hiện hiệu quả vượt trội so với WSN trong việc điều chỉnh các tham số để giải quyết việc quá khớp với một vài ví dụ trong Học Tăng Dần Lớp Ít Mẫu (FSCIL).

TÀI LIỆU THAM KHẢO

[1] Afra Feyza Akyürek, Ekin Akyürek, Derry Wijaya, và Jacob Andreas. Subspace regularizers for few-shot class incremental learning. arXiv preprint arXiv:2110.07059, 2021.

[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, và Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

[3] Yoshua Bengio, Nicholas Léonard, và Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, 2013.

[4] Léon Bottou, Frank E Curtis, và Jorge Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223–311, 2018.

[5] Stephen Boyd, Stephen P Boyd, và Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

[6] Yaroslav Bulatov. notmnist dataset. 2011.

[7] Arslan Chaudhry, Naeemullah Khan, Puneet K Dokania, và Philip HS Torr. Continual learning in low-rank orthogonal subspaces. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[8] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, và Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.

[9] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, và M Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019.

[10] Kuilin Chen và Chi-Guhn Lee. Incremental few-shot learning via vector quantization in deep embedded space. In International Conference on Learning Representations, 2020.

[11] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, và Zhangyang Wang. Long live the lottery: The existence of winning tickets in lifelong learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[12] Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, và Mehrtash Harandi. Semantic-aware knowledge distillation for few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2534–2543, 2021.

[13] Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao Yu, và Jin Tang. Metafscil: A meta-learning approach for few-shot class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14166–14175, 2022.

[14] Daiki Chijiwa, Shin'ya Yamaguchi, Atsutoshi Kumagai, và Yasutoshi Ida. Meta-ticket: Finding optimal subnetworks for few-shot learning within randomly initialized neural networks. In Advances in Neural Information Processing Systems, 2022.

[15] Belur V Dasarathy. Nosing around the neighborhood: A new system structure and classification rule for recognition in partially exposed environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, (1):67–71, 1980.

[16] Danruo Deng, Guangyong Chen, Jianye Hao, Qiong Wang, và Pheng-Ann Heng. Flattening sharpness for dynamic gradient projection memory benefits continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

[17] Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, và Nando de Freitas. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2013.

[18] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.

[19] Siavash Golkar, Michael Kagan, và Kyunghyun Cho. Continual learning via neural pruning. arXiv preprint arXiv:1903.04476, 2019.

[20] Gunshi Gupta, Karmesh Yadav, và Liam Paull. La-maml: Look-ahead meta learning for continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[21] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.

[22] Peter Hart. The condensed nearest neighbor rule (corresp.). IEEE transactions on information theory, 14(3):515–516, 1968.

[23] Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, và Matthew Botvinick. Neuroscience-inspired artificial intelligence. Neuron, 95(2):245–258, 2017.

[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[25] Xu He và Herbert Jaeger. Overcoming catastrophic interference using conceptor-aided backpropagation. In International Conference on Learning Representations, 2018.

[26] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, và Razvan Pascanu. Task agnostic continual learning via meta learning. arXiv preprint arXiv:1906.05201, 2019.

[27] Michael Hersche, Geethan Karunaratne, Giovanni Cherubini, Luca Benini, Abu Sebastian, và Abbas Rahimi. Constrained few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9057–9067, 2022.

[28] Geoffrey Hinton. Neural networks for machine learning, 2012.

[29] Geoffrey E Hinton và Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5–13, 1993.

[30] Sepp Hochreiter và Jürgen Schmidhuber. Simplifying neural nets by discovering flat minima. Advances in neural information processing systems, 7, 1994.

[31] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, và Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 831–839, 2019.

[32] David A Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):1098–1101, 1952.

[33] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, và Samy Bengio. Fantastic generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.

[34] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, và Taesup Moon. Continual learning with node-importance based adaptive group sparse regularization. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[35] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, và Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.

[36] Haeyong Kang, Rusty John Lloyd Mina, Sultan Rizky Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung Ju Hwang, và Chang D Yoo. Forget-free continual learning with winning subnetworks. In International Conference on Machine Learning, pages 10734–10750. PMLR, 2022.

[37] Haeyong Kang, Jaehong Yoon, Sultan Rizky Hikmawan Madjid, Sung Ju Hwang, và Chang D Yoo. On the soft-subnetwork for few-shot class incremental learning. arXiv preprint arXiv:2209.07529, 2022.

[38] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, và Raia Hadsell. Overcoming catastrophic forgetting in neural networks. 2017.

[39] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[40] Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012.

[41] Abhishek Kumar và Hal Daume III. Learning task grouping and overlap in multi-task learning. In Proceedings of the International Conference on Machine Learning (ICML), 2012.

[42] Yann LeCun. The mnist database of handwritten digits. 1998.

[43] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.

[44] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, và Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In Proceedings of the International Conference on Machine Learning (ICML), 2019.

[45] Zhizhong Li và Derek Hoiem. Learning without forgetting. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.

[46] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, và Koray Kavukcuoglu. Hierarchical representations for efficient architecture search, 2017.

[47] Huan Liu, Li Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu, Jun Chen, và Jin Tang. Few-shot class-incremental learning via entropy-regularized data-free replay. arXiv preprint arXiv:2207.11213, 2022.

[48] David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

[49] Arun Mallya, Dillon Davis, và Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.

[50] Arun Mallya và Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7765–7773, 2018.

[51] Nicolas Y Masse, Gregory D Grant, và David J Freedman. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization. Proceedings of the National Academy of Sciences, 115(44):E10467–E10475, 2018.

[52] Pratik Mazumder, Pravendra Singh, và Piyush Rai. Few-shot lifelong learning. arXiv preprint arXiv:2103.00991, 2021.

[53] Michael McCloskey và Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989.

[54] Thomas Mensink, Jakob Verbeek, Florent Perronnin, và Gabriela Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. IEEE transactions on pattern analysis and machine intelligence, 35(11):2624–2637, 2013.

[55] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, và Hassan Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[56] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, và Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011.

[57] Can Peng, Kun Zhao, Tianren Wang, Meng Li, và Brian C Lovell. Few-shot class-incremental learning from an open-set perspective. In European Conference on Computer Vision, pages 382–397. Springer, 2022.

[58] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, và Mohammad Rastegari. What's hidden in a randomly weighted neural network? In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

[59] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017.

[60] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, và Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.

[61] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

[62] Gobinda Saha, Isha Garg, và Kaushik Roy. Gradient projection memory for continual learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[63] Joan Serra, Didac Suris, Marius Miron, và Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International Conference on Machine Learning, pages 4548–4557. PMLR, 2018.

[64] Joan Serrà, Didac Suris, Marius Miron, và Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Proceedings of the International Conference on Machine Learning (ICML), 2018.

[65] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, và Xiao-Ming Wu. Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima. Advances in Neural Information Processing Systems, 34, 2021.

[66] Hanul Shin, Jung Kwon Lee, Jaehon Kim, và Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

[67] Stanford. Available online at http://cs231n.stanford.edu/tiny-imagenet-200.zip. CS 231N, 2021.

[68] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, và Yihong Gong. Few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12183–12192, 2020.

[69] Sebastian Thrun. A Lifelong Learning Perspective for Mobile Robot Control. Elsevier, 1995.

[70] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, và Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.

[71] Mitchell Wortsman, Ali Farhadi, và Mohammad Rastegari. Discovering neural wirings. Advances in Neural Information Processing Systems, 32, 2019.

[72] Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, và Ali Farhadi. Supermasks in superposition. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[73] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, và Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 374–382, 2019.

[74] Han Xiao, Kashif Rasul, và Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv, 2017.

[75] Ju Xu và Zhanxing Zhu. Reinforced continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

[76] Zhewei Yao, Amir Gholami, Kurt Keutzer, và Michael W Mahoney. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581–590. IEEE, 2020.

[77] Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, và Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In International Conference on Machine Learning, pages 10820–10830. PMLR, 2020.

[78] Jaehong Yoon, Saehoon Kim, Eunho Yang, và Sung Ju Hwang. Scalable and order-robust continual learning with additive parameter decomposition. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.

[79] Jaehong Yoon, Divyam Madaan, Eunho Yang, và Sung Ju Hwang. Online coreset selection for rehearsal-based continual learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.

[80] Jaehong Yoon, Eunho Yang, Jeongtae Lee, và Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.

[81] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, và Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv preprint arXiv:1909.11957, 2019.

[82] Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning, pages 3987–3995. PMLR, 2017.

[83] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, và Yinghui Xu. Few-shot incremental learning with continually evolved classifiers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12455–12464, 2021.

[84] Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di Xie, Shiliang Pu, và De-Chuan Zhan. Few-shot class-incremental learning by sampling multi-phase tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[85] Hattie Zhou, Janice Lan, Rosanne Liu, và Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

[86] Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, và Zheng-Jun Zha. Self-promoted prototype refinement for few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6801–6810, 2021.

Haeyong Kang (S'05) nhận bằng Thạc sĩ ngành Hệ thống và Kỹ thuật Thông tin từ Đại học Tsukuba năm 2007. Từ tháng 4 năm 2007 đến tháng 10 năm 2010, ông làm việc như một kỹ sư nghiên cứu phụ tại LG Electronics. Với kinh nghiệm làm việc tại Viện Khoa học và Công nghệ Hàn Quốc (KIST) và Đại học Tokyo, hiện tại ông đang theo đuổi bằng Tiến sĩ tại Trường Kỹ thuật Điện, KAIST. Các lĩnh vực nghiên cứu hiện tại của ông bao gồm học máy không thiên lệch và học liên tục.

Jaehong Yoon Ông nhận bằng Cử nhân và Thạc sĩ ngành Khoa học Máy tính từ Viện Khoa học và Công nghệ Quốc gia Ulsan (UNIST), và nhận bằng Tiến sĩ tại Trường Tin học từ Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc (KAIST). Hiện tại ông đang làm việc như một nghiên cứu viên sau tiến sĩ tại KAIST. Các lĩnh vực nghiên cứu hiện tại của ông bao gồm học sâu hiệu quả, học trên thiết bị, và học với dữ liệu thế giới thực.

Sultan Rizky Madjid nhận bằng Cử nhân ngành Kỹ thuật Điện với chuyên ngành kép Kỹ thuật Cơ khí từ KAIST năm 2021 và bằng Thạc sĩ ngành Kỹ thuật Điện từ KAIST năm 2023. Các lĩnh vực nghiên cứu của ông bao gồm nén mô hình, biểu diễn thưa trong học sâu, và học liên tục.

Sung Ju Hwang Ông nhận bằng Cử nhân ngành Khoa học Máy tính và Kỹ thuật từ Đại học Quốc gia Seoul. Ông nhận bằng Thạc sĩ và Tiến sĩ ngành Khoa học Máy tính từ Đại học Texas tại Austin. Từ tháng 9 năm 2013 đến tháng 8 năm 2014, ông là một nghiên cứu viên sau tiến sĩ tại Disney Research. Từ tháng 9 năm 2013 đến tháng 12 năm 2017, ông là giáo sư trợ lý tại Trường Kỹ thuật Điện và Máy tính tại UNIST. Từ năm 2017, ông đã gia nhập đội ngũ giảng viên tại Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc (KAIST), nơi hiện tại ông là Giáo sư Tài trợ KAIST tại Trường Trí tuệ Nhân tạo Kim Jaechul và Trường Tin học tại KAIST.

Chang D. Yoo (Thành viên Cấp cao, IEEE) Ông nhận bằng Cử nhân ngành Kỹ thuật và Khoa học Ứng dụng từ Viện Công nghệ California, bằng Thạc sĩ ngành Kỹ thuật Điện từ Đại học Cornell, và bằng Tiến sĩ ngành Kỹ thuật Điện từ Viện Công nghệ Massachusetts. Từ tháng 1 năm 1997 đến tháng 3 năm 1999, ông là Nghiên cứu viên Cấp cao tại Korea Telecom (KT). Từ năm 1999, ông đã gia nhập đội ngũ giảng viên tại Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc (KAIST), nơi hiện tại ông là Giáo sư chính thức có chức vụ tại Trường Kỹ thuật Điện và Giáo sư kiêm nhiệm tại Khoa Khoa học Máy tính. Ông cũng từng là Trưởng phòng Dự án Đặc biệt và Trưởng phòng Quan hệ Quốc tế.

PHỤ LỤC A
CHI TIẾT THỰC NGHIỆM CỦA WSN CHO TIL

Chúng tôi tuân theo các thiết lập thực nghiệm tương tự (kiến trúc và siêu tham số) được mô tả trong [62] để so sánh đường cơ sở và được giải thích trong [16] để so sánh SOTA.

A.1 Chi tiết Kiến trúc

Tất cả các mạng cho các thực nghiệm của chúng tôi được triển khai trong cài đặt đa đầu. MLP hai lớp: Trong việc thực hiện các thực nghiệm PMNIST, chúng tôi tuân theo thiết lập chính xác như được biểu thị bởi [62] mạng kết nối đầy đủ với hai lớp ẩn của 100 [48].

ResNet18 thu gọn: Trong việc thực hiện các thực nghiệm 5-Dataset, chúng tôi sử dụng một phiên bản nhỏ hơn của ResNet18 với ít hơn ba lần bản đồ đặc trưng trên tất cả các lớp như được biểu thị bởi [48].

LeNet sửa đổi: Trong việc thực hiện các thực nghiệm Omniglot Rotation và CIFAR-100 Superclass, chúng tôi sử dụng một biến thể lớn của LeNet như mạng cơ sở với 64-128-2500-1500 nơ-ron dựa trên [78].

AlexNet sửa đổi: Trong việc thực hiện bộ dữ liệu CIFAR-100 chia, chúng tôi sử dụng một phiên bản sửa đổi của AlexNet tương tự như [64], [62].

4 lớp Conv và 3 lớp kết nối đầy đủ: Đối với TinyImageNet, chúng tôi sử dụng cùng kiến trúc mạng như [20], [16].

A.2 Danh sách Siêu tham số

BẢNG 6: Danh sách siêu tham số cho các đường cơ sở và WSN của chúng tôi. Ở đây, 'lr' và 'optim' đại diện cho tốc độ học (ban đầu) và bộ tối ưu hóa được sử dụng để đào tạo. Chúng tôi đại diện PMNIST là 'perm', 5-Datasets là '5data', Omniglot Rotation là 'omniglot', CIFAR-100 Split là 'cifar100-split', và CIFAR-100 Superclass là 'cifar100-sc'.

Phương pháp | Siêu tham số
--- | ---
EWC | lr: 0.03 (perm), optim: sgd, hệ số điều chỉnh: 1000 (perm)
GPM | lr: 0.01(perm, omniglot), 0.1 (5data), optim: sgd, ns: 300 (perm), 100 (5data), 125 (omniglot)
PackNet | lr = 0.001 (perm, 5data, omniglot), optim: adam, c: 0.1 (perm), 0.2 (5data), 0.02 (omniglot)
WSN (của chúng tôi) | lr = 0.001 (perm, 5data, omniglot, cifar100-split, cifar100-sc, tinyimagenet), optim: adam

Bảng 6 chi tiết thiết lập siêu tham số cho các đường cơ sở và phương pháp của chúng tôi. ns trong GPM biểu thị số lượng ví dụ đào tạo ngẫu nhiên được lấy mẫu từ bộ đệm phát lại để xây dựng ma trận biểu diễn cho mỗi lớp kiến trúc.

PHỤ LỤC B
CHI TIẾT THỰC NGHIỆM CỦA SOFTNET CHO FSCIL

B.1 Bộ dữ liệu

Các bộ dữ liệu sau được sử dụng để so sánh với tiên tiến hiện tại:

CIFAR-100 Trong CIFAR-100, mỗi lớp chứa 500 hình ảnh để đào tạo và 100 hình ảnh để kiểm tra. Mỗi hình ảnh có kích thước 32×32. Ở đây, chúng tôi tuân theo quy trình FSCIL giống hệt như trong [65], chia bộ dữ liệu thành một phiên cơ sở với 60 lớp cơ sở và tám phiên mới với một vấn đề 5-way 5-shot trên mỗi phiên.

miniImageNet miniImageNet bao gồm các hình ảnh RGB từ 100 lớp khác nhau, trong đó mỗi lớp chứa 500 hình ảnh đào tạo và 100 hình ảnh kiểm tra có kích thước 84×84. Ban đầu được đề xuất cho các vấn đề học ít mẫu, miniImageNet là một phần của bộ dữ liệu ImageNet lớn hơn nhiều. So với CIFAR-100, bộ dữ liệu miniImageNet phức tạp hơn và phù hợp để tạo nguyên mẫu. Thiết lập của miniImageNet tương tự như của CIFAR-100. Để tiến hành đánh giá của chúng tôi, chúng tôi tuân theo quy trình được mô tả trong [65], kết hợp 60 lớp cơ sở và tám phiên mới thông qua các vấn đề 5-way 5-shot.

CUB-200-2011 CUB-200-2011 chứa 200 loài chim chi tiết với 11.788 hình ảnh có số lượng hình ảnh khác nhau cho mỗi lớp. Để tiến hành thực nghiệm, chúng tôi chia bộ dữ liệu thành 6.000 hình ảnh đào tạo, và 6.000 hình ảnh kiểm tra như trong [68]. Trong quá trình đào tạo, chúng tôi cắt ngẫu nhiên mỗi hình ảnh thành kích thước 224×224. Chúng tôi cố định 100 lớp đầu tiên làm lớp cơ sở, sử dụng tất cả mẫu trong các lớp tương ứng này để đào tạo mô hình. Mặt khác, chúng tôi coi 100 lớp còn lại là các danh mục mới được chia thành mười phiên mới với một vấn đề 10-way 5-shot trong mỗi phiên.

B.2 So sánh với SOTA

Chúng tôi so sánh SoftNet với các phương pháp tiên tiến sau trên chia lớp TOPIC [68] của ba bộ dữ liệu điểm chuẩn - CIFAR100 (Bảng 7), miniImageNet (Bảng 8), và CUB-200-2011 (Bảng 9).
