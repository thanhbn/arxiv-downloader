# 2305.03648.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2305.03648.pdf
# File size: 2779421 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
On the Effectiveness of Equivariant Regularization
for Robust Online Continual Learning
Lorenzo Bonicelli1Matteo Boschini1Emanuele Frascaroli1Angelo Porrello1Matteo Pennisi2
Giovanni Bellitto2Simone Palazzo2Concetto Spampinato2Simone Calderara1
1AImageLab - University of Modena and Reggio Emilia
2PeRCeiVe Lab - University of Catania
Abstract
Humans can learn incrementally, whereas neural net-
works forget previously acquired information catastrophi-
cally. Continual Learning (CL) approaches seek to bridge
this gap by facilitating the transfer of knowledge to both
previous tasks (backward transfer) and future ones (forward
transfer) during training. Recent research has shown that
self-supervision can produce versatile models that can gen-
eralize well to diverse downstream tasks. However, con-
trastive self-supervised learning (CSSL), a popular self-
supervision technique, has limited effectiveness in online
CL (OCL). OCL only permits one iteration of the input
dataset, and CSSL’s low sample efﬁciency hinders its use
on the input data-stream.
In this work, we propose Continual Learning via Equiv-
ariant Regularization (CLER) , an OCL approach that
leverages equivariant tasks for self-supervision, avoiding
CSSL’s limitations. Our method represents the ﬁrst attempt
at combining equivariant knowledge with CL and can be
easily integrated with existing OCL methods. Extensive ab-
lations shed light on how equivariant pretext tasks affect the
network’s information ﬂow and its impact on CL dynamics.
1. Introduction
When dealing with non-stationary input distributions,
Artiﬁcial Neural Networks (ANNs) show a bias towards the
incoming training data and thus forget previously acquired
knowledge catastrophically [39]. Continual Learning (CL)
is a rapidly growing area of machine learning that aims
at designing approaches to counteract this effect [42, 17].
Based on either parameter segregation [38, 48], regulariza-
tion [31, 33] or replay [47, 8, 9] – CL methods allow ma-
chine learning systems to adapt constantly while remaining
effective on old data. To assess the merits of these works,
a plethora of experimental settings have been proposed in
recent years; among those, we focus on the challenging On-
line CL (OCL) scenario [2, 12, 9] in light of its applicability
Finetune Finetune + CSSL Finetune + Rot. Prediction
23456789
Incremental Task Ti-.4-.3-.2-.10.1cos(∇Li,∇Li+1)
2345678910
Incremental Task Ti2530354045505560Accuracy onTi
(a) (b)
Figure 1. Effects of SSL in OCL (Seq. CIFAR-100) comparing
aFinetuning baseline with no additional regularization ( green ),
with a Contrastive SSL auxiliary objective ( orange ) and with an
Equivariant rotation prediction pretext task ( blue). (a) Similar-
ity between the gradients induced on the model by task Tiand
Ti+1after training onTi. (b) Accuracy on task Tiafter training on
Ti. Results are reported after a warm-up task ( best in colors ).
to real-world problems: as it only allows a single pass on
training data, it embodies the realistic assumption that an
in-the-wild CL learner would hardly ever be exposed to the
same input twice.
Motivated by the success of Contrastive Self-Supervised
Learning (CSSL) [15, 51, 5], several recent CL approaches
pivot on self-supervised representation learning [43, 10, 22,
36]. Indeed, as self-supervised representations are gener-
ally acknowledged to be agnostic and easily transferable to
diverse downstream tasks [14], their exploitation appears
especially promising in the online scenario, where learn-
ing a shared representation across tasks is as important as
the prevention of forgetting. Moreover, we argue that bind-
ing the incoming classes to general-purpose representations
encourages the emergence of a horizontal and shareable
knowledge base, that will be less subject to forgetting.
However, we reckon that the CSSL paradigm is not a
silver bullet: indeed, contrastive methods are character-
ized by low sample efﬁciency as their convergence requiresarXiv:2305.03648v1  [cs.LG]  5 May 2023

--- PAGE 2 ---
large amounts of resources. As a result, CL methods need
a higher number of training epochs when equipped with
contrastive regularization [10], which clashes with the con-
straints of OCL. Moreover, they usually focus their repre-
sentation learning on a small memory buffer [43], which
entails a high risk of overﬁtting [6].
This work addresses these limitations, revealing the ben-
eﬁts of equivariant self-supervised tasks ( e.g., rotation pre-
diction, jigsaw puzzle, ...) for the OCL scenario. To pro-
vide an insight, Fig. 1 considers a simple learner based on
Finetuning ( i.e., no counter-measure against forgetting) and
reports its performance in the online scenario allowing only
one epoch per task: in doing so, we compare the effects
of the auxiliary objective based either on equivariant self-
supervised learning (in this case, four-fold rotation predic-
tion) or on Barlow Twins [51], a recent CSSL-based ap-
proach that has also shown its merit in CL [43]. We observe
that both representation learning tasks allow for a lower in-
terference between features learned by SSL, as supported by
the more favorable alignment of gradients between current
and subsequent tasks (Fig. 1a). Surprisingly, Fig. 1b shows
that only the rotation-aided model has a signiﬁcant proﬁt
in terms of individual task accuracy for the CSSL-based
objective. We conjecture that the limited amount of train-
ing steps in online CL is not sufﬁcient for contrastive ap-
proaches (such as Barlow Twins) to produce effective rep-
resentations for the downstream task.
To address the aforementioned CSSL limitations in the
OCL setting, we propose Continual Learning via Equiv-
ariant Regularization (CLER) , a novel OCL regularizer
built on top of equivariant pretext tasks – to the best of our
knowledge, this is the ﬁrst attempt to exploit equivariant
information in CL. We demonstrate that our proposal can
be easily combined with existing state-of-the-art CL ap-
proaches, leading to a generalized improvement in perfor-
mance. Through additional experiments, we highlight the
structural and predictive properties conferred by CLER and
draw a detailed comparison with CSSL-based alternatives.
2. Related Work
(Online) Continual Learning is a ﬁeld of machine learn-
ing that studies training over sequences of non-i.i.d. tasks,
with the objective of retaining as much knowledge as pos-
sible from older tasks and mitigating catastrophic forget-
ting [39]. The existing literature offers different techniques
to tackle this problem: regularization-based [31, 33] meth-
ods are designed to control parameter updates in order to
prevent disruptive modiﬁcations to features important for
previous tasks; segregation-based [38, 48] approaches iden-
tify subsets of task-relevant parameters and prevent their
alteration by combining parameter freezing, model expan-
sion, and feature gating; replay-based [47, 46, 8, 9] methods
store examples from the past in a memory buffer, with theobjective of periodically refreshing older knowledge. De-
spite its simplicity, the latter approach is usually regarded
as the most effective solution to date [21, 50, 13].
These methods are typically evaluated in a relaxed train-
ing setting, where the current task can be experienced over
multiple epochs. In practical applications, this requirement
is rarely satisﬁed; Online CL (OCL) [37, 35, 3] is a chal-
lenging and realistic scenario that adds the condition that
each sample of the stream can be seen only once. Works
targeting OCL typically all belong to the replay-based fam-
ily [35, 13]1. Among recent proposals, MIR [2] and GSS [3]
propose enhanced replay sample selection procedures, ER-
AML/ER-ACE [9] encourage balance in learning by means
of carefully designed loss functions, CoPE [18] learns by
exploiting slowly evolving class summaries.
Self-Supervised Representation Learning in CL. Self-
Supervised Learning aims at learning useful representations
directly from the data, i.e., with no need for manual annota-
tions. Recent SSL works show that these methods are able
to learn strong representations that can reach or even out-
perform those of supervised learning [14, 15, 51]. In the
context of CL, SSL methods are typically trained to encour-
age the backbone network to be invariant to the given trans-
formations [10, 22, 43, 36, 30]. Co2L [10] learns the rep-
resentations for new tasks with a modiﬁed supervised con-
trastive learning procedure [29], where current task samples
are used as anchors and elements in the buffer are used as
negative samples – all this while preserving past knowledge
through distillation. However, applying SSL methods in CL
is not straightforward: SSL beneﬁts from large batch sizes
and require several training steps to converge [14]; this rep-
resents a limit for Co2L, as the number of negative samples
is limited by the small buffer size. DualNet [43] decouples
representation learning from the CL objective through two
complementary networks: a slow net exploits buffer sam-
ples to learn an overall representation, while a fast net se-
quentially learns from the input stream, using the features
from the slow net to guide the process.
Pretext Self-Supervised Learning and Rotations. Differ-
ently from CSSL, [25] employs a four-fold rotation predic-
tion pretext task to provide a powerful learning signal for
representation learning. In [24], the rotation pretext task is
applied in the context of few-shot learning; similarly, [16]
pairs rotation prediction to existing SSL methods, leading
to a consistent performance improvement. Recently, the au-
thors of [1] investigated the role of invariance and equiv-
ariance in SSL, suggesting that some transformations ( e.g.,
four-fold rotations, jigsaw puzzle) can be effective when
employed to encourage equivariance, but can lead to dis-
ruptive effects when enforcing invariance.
1All contemporary OCL works consider only replay approaches, due
to their clear performance superiority over all alternatives [37, 9].

--- PAGE 3 ---
ℒ!"ℒ#
DataAugmentation
Pretext Transform
180°270°90°0°ℎ$ℎ%𝑔&
Equivariant Pretext Task (Rotation or Jigsaw)
90°Figure 2. Overview of CLER . Two versions of the input image are fed into the in-training model: i)standard data augmentation is used to
train the classiﬁcation head ( green );ii)an equivariant transformation-based task (rotation, alternatively jigsaw) is used to train the pretext
head ( blue) (best in colors ).
3. Method
3.1. Online Continual Learning
In Online Continual Learning (OCL) [3, 12], a single
DNNfis trained on a sequence of classiﬁcation tasks
T1;:::;TT. Each task consists of disjoint input and output
distributions (Ti= (Xi;Yi), withYi\Yj=;fori6=j) and
each example-label pair may only be shown to the model
once. At taskTc, CL aims at optimizing fon allTtasks,
while only having access to data from Tcitself:
L=TX
i=1Ri=c 1X
i=1Ri
|{z}
1
data no longer
available+Rc|{z}
2
data available+TX
j=c+1Rj
|{z}
3
data not yet
available;(1)
whereRi=E(x;y)2Ti
`(f(x);y)
denotes the empirical
risk associated with the data of task Ti.
In Eq. 1, term 1 (stability) requires fto maintain pre-
dictive efﬁcacy on previously encountered data, whereas
term 3 (plasticity) suggests that the model should prepare
for ﬁtting novel data distributions in later tasks. Only 2
can be directly pursued by training on data; instead, 1 and
3 are achieved by means of auxiliary loss terms. CL meth-
ods endeavor to balance the three terms, which are typically
understood to interfere with one another [46, 4, 34].
3.2. OCL via Equivariant Regularization
The objectives 1 and 3 from Eq. 1 characterize the
main challenges that come when designing a CL model.However, both can be addressed by learning a representa-
tion that can be shared across multiple tasks. To achieve
this, we equip the online learner with an auxiliary SSL ob-
jective. Works in current literature pursue this objective
through CSSL loss terms [10, 43]; instead, we follow the
insights presented in Sec. 1 and opt for an equivariant pre-
text task [16], deﬁned as follows.
LetA=fAigK
i=1be a family of input transforms
Ai:X!X (e.g., rotations, jigsaw puzzle), we transform
each input exemplar with a randomly chosen Akand re-
quest the in-training model to recognize the transformation
by predicting the correct label k2YA=f1;:::;Kg. For
this purpose, we rewrite fashg , whereg is the early
part of the network, devoted to the extraction of features,
andhencompasses the latter part of the model, includ-
ing the ﬁnal multi-layer classiﬁcation head for the CL task.
Subsequently, we introduce h: a separate sub-network fol-
lowing the same structure as h, ﬁnally projecting the rep-
resentationg ()on the setYA.
We treat the choice of Aas a hyperparameter. In our
experiments, we explore two different kinds of transforma-
tions: the set of 4non-distorting image rotations fRot 0;
Rot 90;Rot 180;Rot 270g[24, 25], and the 24permuta-
tions of patches produced by a 22jigsaw puzzle [41].
The resulting approach, called CLER, consists of a regular-
ization termLrthat can be readily applied on a backbone
network as shown in Fig. 2. Let x2Binbe a sample com-
ing from the input batch, we deﬁne Lras:
Lr=rE
xBin
kYA"
CE
h(g (Ak(x)));k#
;(2)

--- PAGE 4 ---
where CE is the cross-entropy loss and ris a scalar
hyper-parameter to control the strength of the regulariza-
tion. We highlight that the label space YAof the pretext
task remains constant over time. The objective of CLER
can hence be compared to classiﬁcation problems where
only the data-generating distribution is subject to changes
(Domain-Incremental learning [50]).
Equivariance & invariance . A function fis said to be
equivariant w.r.t.Aif there exists a mapping MAsuch that:
f(T(x)) =MA(f(x));8x2X: (3)
While the learning objective in Eq. 2 promotes sensitivity
to the chosen set of transformations, solving the CL task
forces the model to become invariant w.r.t. employed data
augmentations. To avoid overlapping between the two ob-
jectives, we compute Eq. 2 only on non-augmented inputs.
4. Experiments
4.1. Experimental setting
Benchmarks. We build our OCL benchmarks by taking im-
age classiﬁcation datasets and splitting their classes equally
into a series of disjoint tasks. In the online learning sce-
nario, the learner will then experience each task only once
(single epoch). For additional details regarding the experi-
ments, we refer the reader to the supplementary material.
•Seq. CIFAR-100 [52, 45, 13] is obtained by splitting the
original 100classes of CIFAR-100 [32] into 10consecu-
tive tasks. For each class, train and test sets include 500
and100 3232RGB images respectively.
•Seq. mini ImageNet [13, 20, 19] is a challenging dataset
that includes a total of 100classes from the popular Ima-
geNet dataset and a longer sequence of tasks. While the
number of samples is the same as in Seq. CIFAR-100, im-
ages are resized to 8484and split into 20 5-way tasks.
Evaluation protocol. We primarily focus our evaluation
on the online Class-Incremental (oCIL) setting, where the
model is asked to gradually learn to solve all tasks, with
no information regarding the task identiﬁer (Task-ID). Dif-
ferently from the online Task-Incremental (oTIL) setting,
where the task Task-ID is available during inference, oCIL
forces the learner to build a single-headed classiﬁer. We
present extensive results in both the oCIL and oTIL settings.
Baseline methods. We report the results of CLER on a se-
lection of current state-of-the-art (SOTA) methods viable
for the oCIL setting.
•Experience Replay with Asymmetric Cross-Entropy
(ER-ACE) [9]. Starting from the popular store-and-
replay baseline (Experience Replay [44, 47]), the authors
propose an alteration aimed at preventing imbalances due
to the simultaneous optimization of current and past data.•eXtended Dark Experience Replay (X-DER) [7] is a
model that combines replay with self-distillation, while
adopting careful design choices to harmonically blend
predictive functions learned at different times.
•Continual Prototype Evolution: Learning Online
from Non-Stationary Data Streams (CoPE) [18] pro-
poses a classiﬁer based on class prototypes, whose care-
ful update scheme allows for learning incrementally while
avoiding sudden disruptions in the latent space.
•DualNet [43] is a dual-backbone architecture decoupling
the issue of incremental classiﬁcation from the one of
learning an overall transferable representation. The latter
task is demanded to one of the backbones ( slow learner ),
trained with a CSSL loss term on i.i.d. data coming from
the replay buffer; the other backbone ( fast learner ) is in-
stead tasked with ﬁtting the CL tasks while taking advan-
tage of the representations produced by the slow learner.
All models are trained for a single epoch with SGD, with
a ﬁxed batch size of 10both on the input stream and the
replay buffer. We benchmark all models with two different
sizes for the memory buffer: 500and2000 for Seq. CIFAR-
100 and 2000 and8000 for Seq. miniImageNet. For these
methods the input Binin Eq. 2 is the concatenation of the
images coming both from the stream and the buffer.
To better compare the effect of CLER, we also include
the results of a model jointly trained on all classes for one
epoch ( Joint-online ) and for 30and50epochs respectively
on Seq. CIFAR-100 and Seq. miniImageNet ( Joint-ofﬂine ).
Also, we include the results of a model trained on the task
sequence with no forgetting countermeasures ( Finetune ).
Architecture. We rely on ResNet18 [27] as backbone in all
experiments. For DualNet, we use this model as the slow
learner and – in line with [43] – construct the fast learner as
a feed-forward network with the same number of convolu-
tional layers as residual blocks in the slow learner.
Regardless of the underlying CL method, we deﬁne the
feature extractor gand the classiﬁcation heads handh
by splitting the ResNet backbone at the second-last residual
block; namely, handhare comprised of the last residual
block, followed by a linear projection onto the respective
sets of classesY=[T
i=1YiandYA.
Metrics. As a primary indicator of a model’s performance
at the end of OCL, we report its Final Average Accuracy
(AF). Letaj
ibe the accuracy of the model at the end of task
jcomputed on the test set of task Ti,AFis computed as:
AF=1
TTX
i=1aT
i: (4)
To further assess learning as tasks progress, we report the

--- PAGE 5 ---
oCIL Seq. CIFAR-100 Seq. mini ImageNet
Joint-ofﬂine 69:47( ) 63:31( )
Joint-online 23:14( ) 10:68( )
Finetune 7:00(100) 3:21(100)
Buffer Size 500 2000 2000 8000
ER-ACE [9] 20:17(38:75) 26:95(23:69) 15:03(35:01) 16:07(37:94)
+ CLER 24.53JS(33:76)30.89JS(20:24)18.08R(32:53)18.43JS(33:22)
X-DER [7] 25:80(39:54) 30:44(31:52) 17:51(34:25) 18:01(50:84)
+ CLER 29.35JS(35:56)34.57JS(29:71)21.26JS(34:07)21.71JS(34:76)
CoPE [18] 19:98(75:32) 34:09(46:39) 22:67(57:96) 24:54(55:09)
+ CLER 26.15JS(69:28)38.48JS(45:50)25.91R(57:73)26.76R(52:69)
DualNet [43] 11:09(92:42) 19:93(73:44) 16:21(80:35) 25:33(59:60)
+ CLER 11.89R(89:97)20.88JS(73:02)18.66R(72:74)30.90R(52:14)
Table 1. Final Average Accuracy AF(") and Final Average Adjusted Forgetting (F
F) (#) on the oCIL setting.Rindicates a result
obtained with rotation,JSa result obatined with 22jigsaw puzzle.
Final Average Adjusted Forgetting (F
F), deﬁned as follows:
F
F=1
T 1T 1X
i=1a
i aT
i
a
i+
;
wherea
i= max
t2fi;:::;T 1gat
i;8i2f1;:::;T 1g:(5)
F
Fis a novel measure derived from the widely employed
Forgetting metric [11] to facilitate the comparison between
unevenly performing approaches. In particular, while the
original Forgetting is upper-bounded by a model’s accuracy,
F
Fvaries in [0;100].F
F= 100 denotes a method that
retains no accuracy on previous tasks ( e.g., Finetune) and
F
F= 0one that has no performance decrease on past tasks.
We repeat each experiment 10times and report the mean
AFandF
F, and the standard deviation of the former. Please
refer to the supplementary material for the standard devia-
tions and statistical signiﬁcance.
4.2. Comparison with the State-Of-The-Art
We include the results of our evaluation on Seq. CIFAR-
100 and Seq. miniImageNet for oCIL and oTIL in Tab. 1
and 2 respectively. For each experiment, we report the
best performer among the 2 2 jigsaw and rotation pre-
text tasks2. The evidence we present strongly supports our
initial claims, with CLER improving the SOTA methods
in all benchmarks. Speciﬁcally, we witness an improve-
ment across the board regarding the AF, while F
Findicates
stronger resistance against forgetting.
Interestingly, the effect of our regularization is main-
tained regardless of the choice of buffer size, with an aver-
age oCIL improvement of 3:59and3:40on Seq. CIFAR-
100 and 3:12and3:46on Seq. miniImageNet. We ﬁnd
2Please refer to Sec. 5.2 for a detailed comparison between the two
choices of pretext task.the only notable exception is in the case of DualNet on
Seq. CIFAR-100. Indeed, even without our regularization,
the lower FAA and higher forgetting compared with the
other baselines suggests that the model cannot proﬁt from
the memory buffer. This might be due to the fact that the
slow learner is only trained with a CSSL objective on sam-
ples from the buffer, which limits the quality of its repre-
sentation when the latter is of moderate size. However, its
results on the challenging Seq. miniImageNet, when com-
bined with CLER, suggest that such an effect can be miti-
gated by leveraging equivariant SSL, which allows the fast
learner to develop better representations during OCL.
5. Model Analysis
In the remainder, we analyze the various contributions
of CLER and gather further insights on its overall effect on
the CL tasks. To the best of our knowledge, our work is the
ﬁrst to consider the effect of equivariant-based pretext tasks
in an incremental setting.
5.1. Effects of CLER on the Backbone
For an in-depth analysis of the effects induced on the
backbone, we consider ER-ACE with and without CLER
and conduct three additional experiments, drawing inspira-
tion from the Network Pruning literature [40]. Our aim here
is to unveil how the information carried by the learned fea-
tures distributes across the parameters of the backbone.
Importance and redundancy. First, we quantify each pa-
rameter’s contribution to the overall loss after training on
Seq. CIFAR-100 by computing the importance measure
^I(1)
mproposed in [40]. In Fig. 3a, we focus on the convolu-
tional layers and report the proportion of parameters whose
importance score is higher than the layer’s average to pro-
vide a compact per-layer evaluation.

--- PAGE 6 ---
oTIL Seq. CIFAR-100 Seq. mini ImageNet
Joint-ofﬂine 82:69( ) 87:55( )
Joint-online 54:12( ) 52:62( )
Finetune 35:42(44:32) 31:55(28:75)
Buffer Size 500 2000 2000 8000
ER-ACE [9] 56:06(9:48) 64:94(3:19) 64:68(3:77) 66:17(4:10)
+ CLER 61.60JS(9:21) 69.33JS(3:04)68.02R(5:27)69.13JS(4:11)
X-DER [7] 63:10(4:31) 69:00(1:38) 67:67(4:71) 68:97(4:39)
+ CLER 68.19JS(2:98) 73.45JS(0:97)71.32JS(3:01)72.39JS(2:66)
CoPE [18] 51:89(23:46) 66:56(7:48) 70:10(4:89) 73:61(3:58)
+ CLER 60.19JS(20:34)71.91JS(6:42)71.17R(5:30)75.33R(2:54)
DualNet [43] 49:38(25:20) 57:05(13:85) 68:43(9:99) 73:89(5:54)
+ CLER 50.11R(23:94)59.66JS(12:99)70.26R(7:39)76.97R(3:87)
Table 2. Final Average Accuracy AF(") and Final Average Adjusted Forgetting (F
F) (#) on the oTIL setting.Rindicates a result
obtained with rotation,JSa result obtained with 22jigsaw puzzle.
ER-ACE ER-ACE + CLER
c1010203040%ˆI(1)
m>avg
L1 L2 L3 L4.1.2.3.4.5.6.7.8.9
% drop2.12.22.32.4avgg(·)[×102]
051015Accuracy onT6
30% drop
051015
50% drop
0 510 15 20 25
Batches051015Accuracy onT6
70% drop
0 510 15 20 25
Batches051015
90% drop
(a) (b)
c1010203040%ˆI(1)
m>avg
L1 L2 L3 L4.1.2.3.4.5.6.7.8.9
% drop2.12.22.32.4avgg(·)[×102]
051015Accuracy onT6
30% drop
051015
50% drop
0 510 15 20 25
Batches051015Accuracy onT6
70% drop
0 510 15 20 25
Batches051015
90% drop
c1010203040%ˆI(1)
m>avg
L1 L2 L3 L4.1.2.3.4.5.6.7.8.9
% drop2.12.22.32.4avgg(·)[×102]
051015Accuracy onT6
30% drop
051015
50% drop
0 510 15 20 25
Batches051015Accuracy onT6
70% drop
0 510 15 20 25
Batches051015
90% drop
(c)
Figure 3. Structural analysis of ER-ACE with and without
CLER on Seq. CIFAR-100. (a) Percentage of important neurons
in each layer with higher-than-average importance score ^I(1)
m;
(b) within-layer similarity score gafter pruning with Geometric
Median; (c) accuracy after dropping conv. ﬁlters and training on
a few batches from T6, with the pre-drop accuracy serving as a
target value ( redline) ( best seen in colors ).
Additionally, we perform a Geometric Median prun-
ing [28] on the model, thus discarding those ﬁlters Fdthat
are the most redundant - i.e., averagely most similar to allothers in the same layer. In Fig. 3b we report the average
within-layer similarity gfor the discarded kernels:
g(Fd) =1
FFX
j=1jFd Fjj; (6)
withFthe total number of ﬁlters in the considered layer.
Our results reveal that CLER pushes the model to ﬁt
the learned task with dense conﬁgurations of parameters
(higher ^I(1)
min Fig. 3a) that are also more similar to each
other (lower gin Fig. 3b). We conjecture that this can be
linked to the performance increase reported in Sec. 4.2: as
the knowledge of a speciﬁc task does not rely on only a few
parameters but instead appears more distributed, it is less
likely that subsequent weights’ updates will entirely erase
the previously acquired knowledge. Moreover, the higher
rate of important parameters, coupled with the higher re-
dundancy, suggests that those important ﬁlters erased by
forgetting could be restored as needed, by simply leveraging
redundant groups of parameters.
Recovery. To support our intuitions, we conducted an ad-
ditional evaluation probing the dynamics of learning with
CLER. After training on the 6thtask of Seq. CIFAR-100,
we randomly drop a portion of the convolutional ﬁlters in
our models and retrain using only the cross-entropy loss on
a few batches from the same task, reporting the accuracy
after each batch in Fig. 3c. Interestingly, the distributed im-
portance induced by our training objective leads to a higher
initial drop in accuracy for CLER. However, our proposed
approach swiftly recovers its performance, reaching the tar-
get pre-drop accuracy in fewer steps w.r.t. the baseline.
5.2. Invariance & Equivariance
While in previous sections we explored the role of equiv-
ariance as a regularizer for OCL, we now wish to better

--- PAGE 7 ---
Model Seq. CIFAR-100 (oCIL) Seq. CIFAR-100 (oTIL)
Buffer Size 500 2000 500 2000
ER-ACE [9] 20:17(38:75) 26:95(23:69) 56:06(9:48) 64:94(3:19)
+ CSSL 20:89(36:03) 27:80(21:12) 56:22(9:88) 65:91(2:42)
+ CLER 24.53JS(33:76)30.89JS(20:24)61.60JS(9:21)69.33JS(3:04)
X-DER [7] 25:80(39:54) 30:44(31:52) 63:10(4:31) 69:00(1:38)
+ CSSL 21:91(36:07) 23:59(40:53) 57:26(2:76) 62:56(0:85)
+ CLER 29.35JS(35:56)34.57JS(29:71)68.19JS(2:98)73.45JS(0:97)
CoPE [18] 19:98(75:32) 34:09(46:39) 51:89(23:46) 66:56(7:48)
+ CSSL 17:23(74:28) 25:76(54:72) 49:56(18:98) 62:48(3:64)
+ CLER 26.15JS(69:28)38.48JS(45:50)60.19JS(20:34)71.91JS(6:42)
Table 3. Performance comparison between our proposal CLER and a similar Contrastive-based SSL (CSSL) method, as measured by
Final Average Accuracy AFstd(") and Final Average Adjusted Forgetting (F
F) (#) on the Seq. CIFAR-100 benchmark.
ER-ACE X-DER CoPE010203040¯AF
BRJBRJ BRJBRJ BRJBRJSeq. CIFAR-100
Base
Rotation
JigsawBuf. 500
Buf. 2000
ER-ACE X-DER CoPE0102030¯AF
BRJBRJ BRJBRJ BRJBRJSeq. miniImageNet
Base
Rotation
JigsawBuf. 2000
Buf. 8000
Figure 4. Final Average Accuracy AFof various baseline meth-
ods when equipped with different equivariant pretext tasks :
four-fold rotation prediction and22jigsaw solving . Both meth-
ods achieve higher results w.r.t. the baseline, with jigsaw solving
usually leading to the best performance ( best seen in colors ).
characterize the different pretext tasks, as well as compare
with an invariance-based CSSL objective.
Rotations vsJigsaw. The results presented so far depict
a clear advantage of the jigsaw puzzle pretext task, which
might suggest that the performance gain is not speciﬁcally
tied to equivariance but to the former. To address such con-
cern, in Fig. 4 we present detailed results for the evaluation
of Sec. 4.2 on the oCIL setting both with four-fold rota-
tion and jigsaw puzzle. Our results depict a clear advantage
of both equivariant pretext tasks w.r.t. the baseline method.
Moreover, the similar performance achieved by the two (es-
pecially on the challenging Seq. miniImageNet benchmark)
further proves our initial assumption about the effectiveness
of equivariant-based SSL methods in CL.
Comparison with CSSL methods. Our initial analysis
shows that enforcing equivariance to a set of input trans-ER-ACE [9] + CSSL + CLER
Epochs Buffer size 500
1 (OCL) 20:17(38:75)20:89(36:03)25.08JS(32:84)
5 32:47(47:70)33:53(46:29)34.88JS(45:52)
20 37:38(46:79)37:78(50:55)39.35JS(46:84)
50 37:94(51:49)39:61(43:75)41.27JS(46:78)
Epochs Buffer size 2000
1 (OCL) 26:95(23:69)27:80(21:12)30.89JS(20:24)
5 42:35(27:49)43:62(27:11)45.67JS(24:92)
20 48:03(33:33)49:16(31:86)50.27JS(31:20)
50 49:05(33:91)50:66(34:48)52.17JS(32:56)
Table 4. Performance comparison for Equivariant- and
Contrastive-based SSL objectives in a multi-epoch setting , eval-
uated on Seq. CIFAR-100. We measure the Final Average Accu-
racy AF(") and ﬁnd generally stronger performance for CLER
even when the online constraint is relaxed.
formations efﬁciently allows CLER to learn a representa-
tion robust against forgetting, by spreading the contribution
of each feature on all the learnable parameters. This is in
contrast with current CL literature, which instead relies on
CSSL tasks [10, 43] to learn a representation that is invari-
antto strong data augmentation and input transformations.
To further prove our contribution, in Tab. 3 we compare
our proposal of an equivariant loss term against one that
promotes invariance by means of a CSSL objective. For
the latter, we take inspiration from [43] and opt for Bar-
low Twins. Our results indicate a superior regularization
effect for CLER, with CSSL even hurting the performance
in some scenarios. This suggests that the few training itera-
tions allowed in OCL do not allow CSSL to transfer useful
knowledge, thus eventually hindering incremental learning.
Applicability to the multi-epoch setting. While we focus
our evaluation on OCL, we reckon that our proposed ap-

--- PAGE 8 ---
Method Seq. CIFAR-100 Seq. mini ImageNet
Joint-ofﬂine 69:851:43 62:421:13
+ CSSL 70:240:47 63:100:61
+ CLER 70:92JS0:74 63:11JS0:16
Joint-online 23:140:74 10:680:67
+ CSSL 23:160:82 13:790:79
+ CLER 28:38JS1:82 14:77JS0:78
Table 5. Accuracy ofJoint methods with CSSL and CLER . The
epochs are set to 30, 50 for CIFAR-100 and miniImg respectively.
proach might also prove beneﬁcial in a less strict environ-
ment that allows for multiple iterations. Such a setting sim-
ulates a realistic low-latency scenario, where the desiderata
is an algorithm capable of rapidly adapting to the changing
data stream while retaining knowledge from the past. Re-
sults of this evaluation on the Seq. CIFAR-100 benchmark
are summarized in Tab. 4. Due to space constraints, we only
include results on the Class-Incremental scenario.
Unsurprisingly, as the number of epochs increases, the
model can start to fully leverage the knowledge that comes
from the stream. However, as CSSL tasks usually require a
large number of iterations to converge, our CLER remains
a better choice for the task of preventing forgetting while
boosting the representation of the base model.
5.3. Is CLER’s advantage actually tied to OCL?
The consistently enhanced performance of baseline
methods when combined with CLER could raise the sus-
picion that SSL regularization is generally effective and not
particularly relevant to Continual Learning per se . To shed
light on this point, we apply both CSSL and CLER regular-
ization on a multi-epoch Joint upper bound (Joint-ofﬂine)
and report the results in Tab. 5; this simple test clearly
shows that – if enough epochs are allowed and the method
achieves full convergence – the presence of additional SSL
terms does not impact the attained accuracy signiﬁcantly.
To complement this result, we also apply the proposed
technique on top of single-epoch Joint training. In this con-
text, CLER proves effective and more so than CSSL. In line
with what shown in Fig. 1, this result conﬁrms that SSL fa-
cilitates the convergence of the learner when having only
few data-points and that the equivariant approach of CLER
is more sample-efﬁcient than typical CSSL methods.
In conclusion, we summarize that self-supervised regu-
larization is not effective in a multi-epoch non-continual
setting (Tab. 5 top); it becomes relevant in either single-
epoch (Tab. 5 bottom ) or continual (Tab. 4) setting. Due to
its enhanced sample efﬁciency, the equivariant approach
pursued by CLER is particularly effective when fewer
epochs are performed . For this reason, its application is
ideal for the OCL setting.Method Seq. CIFAR-100 Seq. mini ImageNet
LWF.MC [45] 36:15(49:78) 20:75(63:67)
+ CLER 37.07R(49:37) 21.64R(62:79)
R-DFCIL [23] 34:98(54:59) 13:15(83:47)
+ CLER 36.74R(52:31) 18.80JS(75:43)
Table 6. Class-IL Final Average Accuracy AFofDFCIL meth-
ods ( no buffer )with and without CLER . We conduct 30, 50
epochs on CIFAR-100, miniImg respectively.
5.4. Applicability to Data-Free Continual Learning
The SOTA competitors on top of which we validate
CLER in Sec. 4 belong to the rehearsal-based family of
CL methods. These represent by far the preferred ap-
proach in the challenging oCIL scenario, on which the per-
formance of other classes of methods is severely compro-
mised [37, 9, 26, 53]. However, a very recent line of works
raises criticism on the adoption of replay, citing potential
privacy issues [49, 23]. They instead focus on the so-
called Data-Free Class-Incremental Learning (DFCIL)
setting, i.e.,multi-epoch Class-Incremental Learning with-
out a memory buffer.
To provide a clear picture of the ﬂexibility of our
proposal, we further showcase its application on top of
two DFCIL methods: the model inversion-based Relation-
Guided Representation Learning (R-DFCIL) [23] and the
distillation-based Multi-Class Learning without Forgetting
(LWF.MC) [45]. The results in Tab. 6 illustrate that CLER
delivers a steady performance improvement even in DFCIL,
which reveals that its effectiveness is not dependent on the
availability of replay data.
6. Conclusions
We present Continual Learning via Equivariant Reg-
ularization (CLER ), a novel approach for Online Contin-
ual Learning (OCL) that encourages representations to be
sensitive to a set of input transformations. Our method
introduces a regularization technique based on equivariant
SSL pretext tasks (jigsaw puzzle solving and four-fold ro-
tation prediction). By experimental means, we show that
the application of CLER to state-of-the-art methods consis-
tently leads to better performance. Furthermore, we provide
an in-depth analysis of the effect of CLER on the parame-
ters of the backbone network and compare it against other
Contrastive Self-Supervised Learning methods.
Our strong results with different choices of equivariant
pretext tasks further support our initial hypothesis, laying
the foundation for better OCL models based on equivariant
constraints. We leave this analysis for future work.

--- PAGE 9 ---
References
[1] Sravanti Addepalli, Kaushal Bhogale, Priyam Dey, and
R Venkatesh Babu. Towards efﬁcient and effective self-
supervised learning of visual representations. In ECCV ,
2022. 2
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
Caccia. Online continual learning with maximal interfered
retrieval. In ANeurIPS , 2019. 1, 2
[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient Based Sample Selection for Online Continual
Learning. In ANeurIPS , 2019. 2, 3
[4] Vladimir Araujo, Julio Hurtado, Alvaro Soto, and Marie-
Francine Moens. Entropy-based stability-plasticity for life-
long learning. In CVPR , 2022. 3
[5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vi-
creg: Variance-invariance-covariance regularization for self-
supervised learning. In ICLR , 2022. 1
[6] Lorenzo Bonicelli, Matteo Boschini, Angelo Porrello, Con-
cetto Spampinato, and Simone Calderara. On the Effective-
ness of Lipschitz-Driven Rehearsal in Continual Learning.
InANeurIPS , 2022. 2
[7] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo
Porrello, and Simone Calderara. Class-incremental continual
learning into the extended der-verse. IEEE TPAMI , 2022. 4,
5, 6
[8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark Experience for Gen-
eral Continual Learning: a Strong, Simple Baseline. In
ANeurIPS , 2020. 1, 2
[9] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuyte-
laars, Joelle Pineau, and Eugene Belilovsky. New Insights
on Reducing Abrupt Representation Change in Online Con-
tinual Learning. In ICLR , 2022. 1, 2, 4, 5, 6, 7, 8
[10] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Con-
trastive continual learning. In ICCV , 2021. 1, 2, 3, 7
[11] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremen-
tal learning: Understanding forgetting and intransigence. In
ECCV , 2018. 5
[12] Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip
Torr, and David Lopez-Paz. Using hindsight to anchor past
knowledge in continual learning. In AAAI Conference on Ar-
tiﬁcial Intelligence , 2021. 1, 3
[13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marc’Aurelio Ranzato. On tiny episodic memo-
ries in continual learning. In ICML Workshops , 2019. 2, 4
[14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , 2020. 1, 2
[15] Xinlei Chen and Kaiming He. Exploring simple siamese
representation learning. Technical report, Facebook AI Re-
search, 2020. 1, 2
[16] Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han,
Akash Srivastava, Brian Cheung, Pulkit Agrawal, and MarinSolja ˇci´c. Equivariant contrastive learning. In ICLR , 2022. 2,
3
[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne
Tuytelaars. A continual learning survey: Defying forgetting
in classiﬁcation tasks. IEEE TPAMI , 2021. 1
[18] Matthias De Lange and Tinne Tuytelaars. Continual pro-
totype evolution: Learning online from non-stationary data
streams. In ICCV , 2021. 2, 4, 5, 6
[19] Mohammad Mahdi Derakhshani, Xiantong Zhen, Ling Shao,
and Cees Snoek. Kernel continual learning. In ICML , 2021.
4
[20] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor
Darrell, and Marcus Rohrbach. Adversarial continual learn-
ing. In ECCV , 2020. 4
[21] Sebastian Farquhar and Yarin Gal. Towards Robust Evalu-
ations of Continual Learning. In ICML Workshops , 2018.
2
[22] Enrico Fini, Victor G Turrisi da Costa, Xavier Alameda-
Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-
supervised models are continual learners. In CVPR , 2022. 1,
2
[23] Qiankun Gao, Chen Zhao, Bernard Ghanem, and Jian
Zhang. R-DFCIL: Relation-Guided Representation Learning
for Data-Free Class Incremental Learning. In ECCV , 2022.
8
[24] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick
P´erez, and Matthieu Cord. Boosting few-shot visual learning
with self-supervision. In ICCV , 2019. 2, 3
[25] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. In ICLR , 2018. 2, 3
[26] Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng. Not just se-
lection, but exploration: Online class-incremental continual
learning via dual view consistency. In CVPR , 2022. 8
[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 4
[28] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang.
Filter pruning via geometric median for deep convolutional
neural networks acceleration. In CVPR , 2019. 6
[29] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu,
and Dilip Krishnan. Supervised Contrastive Learning. In
ANeurIPS , 2020. 2
[30] Chris Dongjoo Kim, Jinseo Jeong, Sangwoo Moon, and
Gunhee Kim. Continual learning on noisy data streams via
self-puriﬁed replay. In ICCV , 2021. 2
[31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. PNAS , 2017. 1, 2
[32] Alex Krizhevsky et al. Learning multiple layers of features
from tiny images. Technical report, Citeseer, 2009. 4
[33] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE TPAMI , 2017. 1, 2

--- PAGE 10 ---
[34] Guoliang Lin, Hanlu Chu, and Hanjiang Lai. Towards better
plasticity-stability trade-off in incremental learning: A sim-
ple linear connector. In CVPR , 2022. 3
[35] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. In ANeurIPS , 2017.
2
[36] Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu,
and Sung Ju Hwang. Rethinking the representational con-
tinuity: Towards unsupervised continual learning. In ICLR ,
2022. 1, 2
[37] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyun-
woo Kim, and Scott Sanner. Online continual learning in
image classiﬁcation: An empirical survey. Neurocomputing ,
2022. 2, 8
[38] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In CVPR ,
2018. 1, 2
[39] Michael McCloskey and Neal J Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. Psychol. Learn. Motiv. , 1989. 1, 2
[40] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
and Jan Kautz. Importance estimation for neural network
pruning. In CVPR , 2019. 5
[41] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In ECCV ,
2016. 3
[42] German I Parisi, Ronald Kemker, Jose L Part, Christopher
Kanan, and Stefan Wermter. Continual lifelong learning with
neural networks: A review. Neural Netw. , 2019. 1
[43] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Con-
tinual learning, fast and slow. In ANeurIPS , 2021. 1, 2, 3, 4,
5, 6, 7
[44] Roger Ratcliff. Connectionist models of recognition mem-
ory: constraints imposed by learning and forgetting func-
tions. Psychol. Rev. , 1990. 4
[45] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. iCaRL: Incremental classi-
ﬁer and representation learning. In CVPR , 2017. 4, 8
[46] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,
Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to Learn
without Forgetting by Maximizing Transfer and Minimizing
Interference. In ICLR , 2019. 2, 3
[47] Anthony Robins. Catastrophic forgetting, rehearsal and
pseudorehearsal. Conn. Sci. , 1995. 1, 2, 4
[48] Joan Serra, Didac Suris, Marius Miron, and Alexandros
Karatzoglou. Overcoming Catastrophic Forgetting with
Hard Attention to the Task. In ICML , 2018. 1, 2
[49] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen,
Hongxia Jin, and Zsolt Kira. Always be dreaming: A new
approach for data-free class-incremental learning. In ICCV ,
2021. 8
[50] Gido M van de Ven, Tinne Tuytelaars, and Andreas S To-
lias. Three types of incremental learning. Nat. Mach. Intell. ,
2022. 2, 4
[51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and
St´ephane Deny. Barlow twins: Self-supervised learning via
redundancy reduction. In ICML , 2021. 1, 2[52] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In ICML , 2017.
4
[53] Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert
Bifet, Nick Jin Sean Lim, and Yunzhe Jia. A simple but
strong baseline for online continual learning: Repeated Aug-
mented Rehearsal. In ANeurIPS , 2022. 8
