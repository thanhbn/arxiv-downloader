# 2305.14782.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2305.14782.pdf
# File size: 2247457 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IBCL: Zero-shot Model Generation under
Stability-Plasticity Trade-offs
Pengyuan Lu∗Michele Caprio∗Eric Eaton∗Insup Lee∗
Abstract
Algorithms that balance the stability-plasticity trade-off are well-studied in the
continual learning literature. However, only a few of them focus on obtaining
models for specified trade-off preferences. When solving the problem of continual
learning under specific trade-offs (CLuST), state-of-the-art techniques leverage
rehearsal-based learning, which requires retraining when a model corresponding
to a new trade-off preference is requested. This is inefficient since there exist in-
finitely many different trade-offs, and a large number of models may be requested.
As a response, we propose Imprecise Bayesian Continual Learning (IBCL), an
algorithm that tackles CLuST efficiently. IBCL replaces retraining with constant-
time convex combination. Given a new task, IBCL (1) updates the knowledge
base in the form of a convex hull of model parameter distributions and (2) gener-
ates one Pareto-optimal model per given trade-off via convex combination with-
out any additional training. That is, obtaining models corresponding to specified
trade-offs via IBCL is zero-shot. Experiments whose baselines are current CLuST
algorithms show that IBCL improves by at most 45% on average per task accuracy
and by 43% on peak per task accuracy, while maintaining a near-zero to positive
backward transfer. Moreover, its training overhead, measured by number of batch
updates, remains constant at every task, regardless of the number of preferences
requested. Details at: https://github.com/ibcl-anon/ibcl .
1 Introduction
Continual learning (CL), also known as lifelong machine learning, is a special case of multi-task
learning, where tasks arrive in temporal sequence one-by-one [9, 38, 40, 43]. Two key properties
matter for CL algorithms: stability and plasticity [11]. Here, stability means the ability to maintain
performance on previous tasks, not forgetting what the model has learned, and plasticity means
the ability to adapt to a new task. Unfortunately, these two properties are conflicting due to the
multi-objective optimization nature of CL [22, 41]. For years, researchers have been balancing
the stability-plasticity trade-off. However, few have discussed the problem of learning models for
specifically given trade-off points. In this paper, we focus on such a problem, which we denote as
CL under specific trade-offs (CLuST).
Why is CLuST important? In certain scenarios, it is important to explicitly specify how much
stability and plasticity are needed. Let us consider an example of a movie recommendation system.
The model is first trained to rate movies in the sci-fi genre. Then, a new genre, e.g. documentaries,
is added by the movie company. The model needs to learn how to rate documentaries while not
forgetting how to rate sci-fis. Training this model boils down to a CL problem.
The company now wants to build a recommendation system that adapts to users’ tastes in movies.
For example, Alice has equal preferences over sci-fis and documentaries. Bob, however, wants to
watch only documentaries and has no interest in sci-fis at all. Consequently, the company aims
∗University of Pennsylvania, {pelu,caprio,eeaton,lee }@seas.upenn.edu
Preprint. Under review.arXiv:2305.14782v3  [cs.LG]  20 Sep 2024

--- PAGE 2 ---
Figure 1: A Bayesian view of a Pareto-optimal distribution q′and a non-Pareto-optimal distribution q′′.
to train two customized models for Alice and Bob respectively, to predict how likely a sci-fi or a
documentary is to be recommended. Based on individual preferences, Alice’s personal model should
balance between the accuracy in rating sci-fis and rating documentaries, while Bob’s model allows
to compromise the accuracy in rating sci-fis to achieve a high accuracy in rating documentaries. As
new genres are added, users should be able to input their preferences over all available genres to
obtain customized models.
To formalize the CLuST problem, we take a Bayesian perspective, where learnable model parame-
ters are viewed as random variables [16, 23, 37]. As illustrated in Figure 1, we consider all parameter
distributions living in a metric space. This metric can be any valid metric for distributions, such as
the2-Wasserstein distance [12]. The figure shows an example of two tasks, with their ground-truth
distributions being q1andq2, respectively. From this setup, a distribution emphasizing stability (at
task 2) is a distribution closer to q1thanq2, and one prioritizing plasticity is closer to q2thanq1.
Notice that irrespective of the desired stability-plasticity trade-off, we want the distribution to be
Pareto-optimal , which loosely means that there is no way to improve such a distribution by mak-
ing it closer to bothq1andq2. We can see that Pareto-optimality is equivalent to being inside the
convex set enclosed by q1andq2. For example, q′in the figure is a Pareto-optimal distribution,
while q′′is not. With this setting, we can specify a trade-off point using a preference vector [35, 36]
¯w= (w1, w2), where w1, w2≥0andw1+w2= 1. The preferred Pareto-optimal distribution is
therefore a convex combination w1q1+w2q2.
So far, researchers have already proposed the use of preference vectors to specify trade-off points
in multi-task and continual learning [18, 30, 31, 34]. However, instead of using them as coefficients
for convex combinations, state-of-the-art techniques use them as regularizers in rehearsal-based
methods . That is, existing algorithms that aim to solve CLuST memorize some data difor each
taski(for “rehearsal”), and let the loss at task ibeli=Pi
j=1wjl(dj), with lbeing a generic loss
function like cross-entropy. There are at least two drawbacks to this approach. First, rehearsals have
to retrain the entire model whenever we have a new trade-off preference. In plain words, these
methods have a training overhead proportional to the number of preferences at each task. As
there exist infinitely many possible preferences, this boils down to an efficiency issue when there is a
large number of preferences, such as a large number of users in the movie recommendation example.
It would be desirable if, instead of retraining, we could obtain the preferred models by constant-
time, training-free operations. Moreover, rehearsals have to cache data, and stable performance on
previous tasks depends on which data can be memorized.
To overcome these shortcomings faced by CLuST algorithms, we propose Imprecise Bayesian Con-
tinual Learning (IBCL), whose workflow is illustrated in Figure 2. At step 1, upon the arrival of
a new task’s training data, IBCL update its knowledge base (that is, all information shared across
tasks) in the form of a convex set of distributions with finitely many extreme elements (the elements
that cannot be written as convex combinations of one another), called finitely generated credal set
(FGCS) [6]. This is done by variational inference from the previous task’s learned distribution, and
the learned distributions serve as extreme elements of the FGCS. Each point in the FGCS corre-
sponds to one Pareto-optimal distribution on the trade-off polytope of all tasks so far. Then, at step
2, given any preference vector ¯w, IBCL selects the preferred distribution by convex combination. A
parameter region is obtained as a highest density region (HDR) of the distribution selected, which is
the smallest parameter set that contains the ground-truth model with high probability.
IBCL tackles the identified shortcomings as follows. First, IBCL replaces retraining in state-of-
the-art with constant-time, zero-shot convex combination to generate models. It has constant
training overhead per task (to update the FGCS), independent of the number of preferences.
2

--- PAGE 3 ---
Figure 2: The workflow of Imprecise Bayesian Continual Learning (IBCL). Here, we start from 1 prior, but in
practice there may be more than 1 to reduce epistemic uncertainty.
In addition, no cache of data is required, and therefore the stability of our model does not depend
on the data memorized. Experiments on image classification and NLP benchmarks support the
effectiveness of IBCL. We find that IBCL improves on baselines by at most 45% in average per task
accuracy and by 43% in peak per task accuracy, while maintaining a near-zero to positive backward
transfer, with a constant training overhead regardless of number of preferences. We also show that
IBCL has a sublinear memory growth along the number of tasks.
Overall, we have the following contributions: (1) We are the first to rigorously formulate the CLuST
problem, which asks for efficiency upon a large number of preferences (Section 3). (2) We propose
IBCL, a Bayesian CL algorithm to solve the CLuST problem (Section 4). (3) We experiment on
standard image classification and NLP benchmarks to support our claims (Section 5).
2 Background
Our algorithm hinges upon the concepts of finitely generated credal set (FGCS) from Imprecise
Probability (IP) theory [3, 6, 45].
Definition 1 (Finitely Generated Credal Set) .A convex set Q={q:q=Pm
j=1βjqj,βj≥
0∀j,P
jβj= 1}of probability distributions with finitely many extreme elements ex [Q] ={qj}m
j=1
is called a finitely generated credal set (FGCS).
In other words, an FGCS is a convex hull of (finitely many) distributions. We also borrow from the
Bayesian literature the idea of highest density region (HDR) [10].
Definition 2 (Highest Density Region) .LetΘbe a set of interest, and α∈[0,1]be a significance
level. Suppose that a (continuous) random variable θ∈Θhas probability density function (pdf) q.
Then, the (1−α)-HDR is the set Θα
qsuch thatR
Θαqq(θ)dθ≥1−αandR
Θαqdθis a minimum.
Definition 2 tells us that if θ∼q,Prθ∼q[θ∈Θα
q]≥1−α, and Θα
qis the narrowest subset of Θthat
guarantees this inequality. In other words, the HDR Θα
qis the smallest subset of Θwhere we can find
the realization of random variable θwith high probability. The concept of HDR is further explained
in Appendix B with an illustration. Other related works, including multi-task and continual learning,
are reviewed in Appendix C.
3 Formulating the CLuST Problem
In this section we formalize the CLuST problem. We consider domain-incremental learning [44]
for classification models, with an unbounded number of stability-plasticity trade-off preferences at
each task. The goal is to construct a learning algorithm with training overhead independent of the
number of preferences, and that enjoys performance guarantees.
3

--- PAGE 4 ---
3.1 Assumptions
LetXbe the space of inputs, and Ybe the space of labels. Call ∆XYthe space of all possible
distributions on X × Y . A task iis associated with a distribution pi∈∆XY, from which labeled
data can be i.i.d. drawn. We assume that all tasks are similar to each other.
Assumption 1 (Task Similarity) .For all task i,pi∈ F, where Fis a convex subset of ∆XY. Also,
we assume that the diameter of Fis some r >0, that is, supp,q∈F∥p−q∥W2≤r, where ∥ · ∥ W2
denotes the 2-Wasserstein distance.
The definition of and the reason for choosing the 2-Wasserstein metric are given in Appendix D.
Assumption 1 is needed to mitigate the possible model misspecification, which in turn could lead to
catastrophic forgetting even when Bayesian inference is carried out exactly ([23] and Appendix E).
Under Assumption 1, for any two tasks iandj, their underlying distributions piandpjare “close
enough”, i.e. ∥pi−pj∥W2≤r. Moreover, since Fis convex, any convex combination of task
distributions belongs to F. Next, we assume the parameterization of class F.
Assumption 2 (Parameterization of Task Distributions) .Every distribution FinFis parameterized
byθ, a parameter belonging to a parameter space Θ.
An example of a parameterized family that satisfies Assumption 1 is given in Appendix F. Notice
that all tasks share the same input space Xand label space Y, so the learning is domain-incremental.
We then formalize stability-plasticity trade-off preferences over tasks.
Definition 3 (Stability-plasticity Trade-off Preferences over Tasks) .Consider ktasks with under-
lying distributions p1, p2, . . . , p k. We express a stability-plasticity trade-off preference (or simply,
a preference) over them via a probability vector ¯w= (w1, w2, . . . , w k)⊤, that is, wi≥0for all
i∈ {1, . . . , k }, andPk
i=1wi= 1.
Based on this definition, given a preference ¯wover all ktasks encountered, the personalized model
for the user aims to learn the distribution p¯w:=Pk
i=1wipi. It is the distribution associated with
tasks 1, . . . , k that also takes into account a preference over them. Since p¯wis the convex combi-
nation of p1, . . . , p k, thanks to Assumptions 1 and 2, we have that p¯w∈ F, and therefore it is also
parameterized by some θ∈Θ. The learning procedure is the same as standard supervised domain-
incremental learning. Given task k, we draw nklabeled examples i.i.d. from an unknown pk. Then,
we are given at least one user preference ¯wover the ktasks so far. The data drawn for task k+ 1
will not be available until we have finished learning models for all preferences at task k.
3.2 Main Problem
We aim to design a domain-incremental learning algorithm that generates one model per preference
over tasks , with unbounded number of preferences and tasks. Given a significance level α∈[0,1],
at any task k, the algorithm should satisfy:
1.Zero-shot preferred model generation . Let ¯wbe a preference over the ktasks. When there are
more than one preference ¯ws,s∈ {1,2, . . .}, no training is needed for generating models, for all
s >1. That is, the model generation for new preferences is zero-shot.
2.Probabilistic Pareto-optimality . We want to identify the smallest subset of model parameters,
Θα
ˆq¯w⊂Θ(written as Θα
¯wfor notational convenience from now on), that the Pareto-optimal
parameter θ⋆
¯w(i.e. the ground-truth parameter of p¯w) belongs to with high probability, i.e.,
Prθ⋆
¯w∼ˆq¯w[θ⋆
¯w∈Θα
¯w]≥1−α, under a known ˆq¯woverΘ.
3.Sublinear buffer growth . The memory overhead for the entire procedure should be growing
sublinearly in the number of tasks.
4 Imprecise Bayesian Continual Learning
As shown in Figure 2, IBCL performs two steps at each task. First, it updates a knowledge base in
form of an FGCS (Section 4.1). Second, it uses a convex combination of the extreme elements of
the FGCS, instead of retraining, to zero-shot generate models under given preferences (Section 4.2).
4

--- PAGE 5 ---
4.1 FGCS Knowledge Base Update
As discussed in the Introduction, we take a Bayesian continual learning approach, that is, the pa-
rameter θof distribution pkpertaining to task kis viewed as a random variable distributed according
to some distribution q. At the beginning of the analysis, we specify mmany such distributions,
ex[Q0] ={q1
0, . . . , qm
0}. They are the ones that the designer deems plausible – a priori – for param-
eterθof task 1. Upon observing data pertaining to task 1, we learn a set Qtmp
1of posterior parameter
distributions and buffer them as extreme elements ex [Q1]of the FGCS Q1corresponding to task 1.
We proceed similarly for the successive tasks i≥2.
Algorithm 1 FGCS Knowledge Base Update
Input : Current knowledge base in the form of FGCS extreme elements ex [Qi−1] =
{q1
i−1, . . . , qm
i−1}, observed labeled data (¯xi,¯yi) ={(x1i, y1i), . . . , (xni, yni)}at task i, and dis-
tribution distance threshold d≥0
Output : Updated extreme elements ex [Qi]
1:Qtmp
i← ∅
2:forj∈ {1, . . . , m }do
3: qj
i←variational inference (qj
i−1,¯xi,¯yi)
4: dj
i←minq∈ex[Qi−1]∥qj
i−q∥W2
5: ifdj
i≥dthen
6: Qtmp
i← Qtmp
i∪ {qj
i}
7: else
8: Remember to use q= arg minq∈ex[Qi−1]∥qj
i−q∥W2in place of qj
ilater on
9: end if
10:end for
11:ex[Qi]←ex[Qi−1]∪ Qtmp
i
In Algorithm 1, at task i, we approximate mposteriors q1
i, . . . qm
ivia variational inference from
buffered priors q1
i−1, . . . qm
i−1one-by-one (line 3). However, we do not want to buffer all learned
posteriors, so we use a distance threshold dto exclude the posteriors that are similar to the distri-
butions that are already buffered (lines 4 - 9). When a distribution similar to qj
iis found in the
knowledge base, we remember to use it in place of qj
iin the future (line 8). The posteriors that
are sufficiently different from the already buffered distributions are then appended to the knowledge
base (line 11).
Notice that Algorithm 1 ensures sublinear buffer growth in our problem formulation because at
each task iwe only buffer minew posterior models, with 0≤mi≤m. With a sufficiently large
threshold d, the buffer growth can become constant after several tasks. The use of different threshold
d’s is discussed in our ablation studies, see Section 5.2.
4.2 Zero-shot Generation of User Preferred Models
Next, after having updated the FGCS extreme elements for task i, we are given a set of user prefer-
ences. For each preference ¯w, we need to identify the Pareto-optimal parameter θ⋆
¯wfor the preferred
data distribution p¯w. This procedure can be divided into two steps as follows.
First, we find the parameter distribution ˆq¯wvia a convex combination of the extreme elements in the
knowledge base, whose weights correspond to the entries of preference vector ¯w. That is,
ˆq¯w=iX
k=1mkX
j=1βj
kqj
kwheremkX
j=1βj
k=wk, and βj
k≥0,for all jand all k. (1)
Here, qj
kis a buffered extreme point of FGCS Qk, i.e. the j-th parameter posterior of task k. The
weight βj
kof this extreme point is decided by preference vector entry ¯wj. In implementation, if
we have mkextreme elements stored for task k, we can choose equal weights β1
k=···=βm
k=
wk/mk. For example, if we have preference ¯w= (0.8,0.2)⊤on two tasks so far, and we have two
5

--- PAGE 6 ---
extreme elements per task stored in the knowledge base, we can use β1
1=β2
1= 0.8/2 = 0 .4and
β1
2=β2
2= 0.2/2 = 0 .1.
As we can see from the following theorem, distribution ˆq¯wis a parameter posterior corresponding
to a preference elicitation via preference vector ¯wover the tasks encountered so far.
Theorem 1 (Selection Equivalence) .Selecting a precise distribution ˆq¯wfromQiis equivalent to
specifying a preference weight vector ¯wonp1, . . . , p i.
Please refer to Appendix G for the proof. Theorem 1 entails that the selection of ˆq¯win Algorithm 2
is related to the correct parameterization of p¯w∈∆XY.
Second, we compute the HDR Θα
¯w⊂Θfrom ˆq¯w. This is implemented via a standard procedure
that locates the smallest region in the parameter space whose enclosed probability mass is (at least)
1−α, according to ˆq¯w. This procedure can be routinely implemented, e.g., in R, using package
HDInterval [21]. As a result, we locate the smallest set of parameters Θα
¯w⊂Θassociated with
the preference ¯w. This subroutine is formalized in Algorithm 2, and one remark is that it does not
require any training, i.e., we meet our goal of zero-shot preferred model generation of Section
3.2.
Algorithm 2 Preference HDR Computation
Input : Knowledge base ex [Qi]withmkextreme elements saved for task k∈ {1, . . . , i},
preference ¯won the itasks, significance level α∈[0,1]
Output : HDR Θα
¯w⊂Θ
1:fork= 1, . . . , i do
2: β1
k=···=βm
k←wk/mk
3:end for
4:ˆq¯w=Pi
k=1Pmk
j=1βj
kqj
k
5:Θα
¯w←hdr(ˆq¯w, α)
4.3 Overall IBCL Algorithm and Analysis
From the two subroutines in Sections 4.1 and 4.2, we construct the overall IBCL algorithm as in
Algorithm 3.
Algorithm 3 Imprecise Bayesian Continual Learning
Input : Prior distributions ex [Q0] ={q1
0, . . . , qm
0}, hyperparameters αandd
Output : HDR Θα
¯wfor each given preference ¯wat each task i
1:fortaski= 1,2, ...do
2: ¯xi,¯yi←sample nilabeled data points i.i.d. from pi
3: ex[Qi]←fgcs update (ex[Qi−1],¯xi,¯yi, d)
4: ▷% Algorithm 1 %
5: while user has a new preference do
6: ¯w←user input
7: Θα
¯w←preference hdrcomput (ex[Qi],¯w, α)
8: ▷% Algorithm 2 %
9: end while
10:end for
For each task, in line 3, we use Algorithm 1 to update the knowledge base by learning mposteriors
from the current priors. Some of these posteriors will be cached and some will be substituted by
a previous distribution in the knowledge base. In lines 5-7, according to a user-given preference
over all tasks so far, we obtain the HDR of the model associated with preference ¯win zero-shot via
Algorithm 2. Notice that this HDR computation does not require the initial priors ex [Q0], so we can
discard them once the posteriors are learned in the first task. The following theorem ensures that
IBCL locates the user-preferred Pareto-optimal model with high probability.
Theorem 2 (Probabilistic Pareto-optimality) .Pick any α∈[0,1]. The Pareto-optimal parameter
θ⋆
¯w, i.e., the ground-truth parameter for p¯w, belongs to Θα
¯wwith probability at least 1−αunder
distribution ˆq¯w. In formulas, Prθ⋆
¯w∼ˆq¯w[θ⋆
¯w∈Θα
¯w]≥1−α.
6

--- PAGE 7 ---
Theorem 2 gives us a (1−α)-guarantee in obtaining Pareto-optimal models for given task trade-off
preferences. Consequently, the IBCL algorithm enjoys the probabilistic Pareto-optimality targeted
by our main problem. Please refer to Appendix G for the proof.
5 Experiments
5.1 Setup
Although there exist many baseline methods for CL, only a few baselines for CLuST exist. The
following CLuST baselines are selected for comparison.
1.Rehearsal-based. This is the state-of-the-art technique for CLuST [30]. These methods memo-
rize a subset of training data of every task encountered so far. Task preferences are then given as
weights to regularize the loss on each task’s memorized data. We choose GEM [33] and A-GEM
[8] as two typical rehearsal-based methods.
2.Rehearsal-based, Bayesian. Since IBCL is a Bayesian method, we also compare it to a
Bayesian technique, VCL [37]. We equip VCL with episodic memory to make it rehearsal-
based, and so to be able to specify a preference. This approach has been used in [42].
3.Prompt-based. Prompt-based CL has never been used for CLuST and therefore not a state-
of-the-art. Still, they are considered efficient modern CL techniques. Therefore, we made an
attempt to specify preferences on L2P [46], a prompt-based method, by training one learnable
prompt prefix per task, and use a preference-weighted sum of the prompts at inference time.
We experiment on four standard continual learning benchmarks, including three image classification
and one NLP: (i) 15 tasks in CelebA [32] (with vs. without attributes), (ii) 10 tasks in Split CIFAR-
100 [48] (animals vs. non-animals), (iii) 10 tasks in TinyImageNet [27] (animals vs. non-animals)
and (iv) 5 tasks in 20NewsGroup [26] (news related to computers vs. not related to computers). For
the first three image benchmarks, features are first extracted by ResNet-18 [19]. For 20NewsGroup,
features are extracted by TF-IDF [2].
As in standard continual learning evaluation, after training on task i, we evaluate the accuracy on all
testing data of previous tasks j∈ {1, . . . , i}. To evaluate how well does a model address preferences,
we randomly generate nprefs= 10 preferences per task, except for task 1, whose preference is always
given by scalar 1. Therefore, for each method, we obtain 10 models at each task, and we evaluate a
preference-weighted sum of their accuracies on previous tasks. Finally, these preference-weighted
accuracies are used to compute standard continual learning metrics: average per task accuracy, peak
per task accuracy, and backward transfer [13]. Experiments are run on Intel(R) Core(TM) i7-8550U
CPU @ 1.80GHz. Detailed setup can be found in Appendix H.1.
5.2 Results
Figure 3: Results of 20NewsGroup. Since VCL and IBCL produce probabilistic models, we sample 10 de-
terministic models for each. The solid blue curve illustrates the top performance of deterministic models by
IBCL, and the dashed blue curve is the mean performance. The shaded blue region is the performance range
by IBCL. The same illustration method is used for VCL in green color.
Our results support the claim that IBCL not only achieves high performance by probabilistic Pareto-
optimality, but is also efficient with zero-shot generation of models.
Since VCL and IBCL output probabilistic models (BNNs and HDRs), we sample 10 deterministic
models from each and compute the range of their performance metrics, illustrated as shaded areas in
7

--- PAGE 8 ---
Figure 4: Results of TinyImageNet. The illustration method is the same as in Figure 3.
Table 1: Training overhead comparison, measured as # of batch updates required at a task. Here, ni: # of
training data points at task i,nprefs: # of preferences per task, nmem: # of data points memorized per task in
rehearsal, npriors: # of priors in IBCL, e: # of epochs and b: batch size. Notice that the overhead of rehearsal
based methods are proportional to nprefs.
# batch updates at task i# batch updates at last task
CelebA CIFAR100 TImgNet 20News
RehearsalGEM nprefs
×(ni+ (i−1)×nmem)
×e/b99747 19532 13594 35313 A-GEM
VCL
Prompt L2P ni×e/b 9538 1250 938 2907
IBCL (ours) npriors×ni×e/b 28614 3750 2814 8721
Figures 3 and 4. They represent performances on the 20NewsGroup and TinyImageNet, respectively.
In these figures, we draw the curves of top performance and mean performance of the sampled
deterministic models by VCL and IBCL as solid and dashed lines, respectively. Due to page limit,
we show the results on Split CIFAR-100 and CelebA in Figure 9 and 10 in Appendix H.2. From
Figures 3, 4, 9 and 10, we can see that IBCL overall generates the model with top performance
(high accuracy) in all cases, while maintaining little catastrophic forgetting (near-zero to positive
backward transfer). This is due to the probabilistic Pareto-optimality guarantee. Statistically, IBCL
improves on the baselines by at most 45% on average per task accuracy and by 43% on peak per
task accuracy (compared to L2P in 20News). So far, to our knowledge, there is no discussion on
how to specify a task trade-off preference in prompt-based continual learning, and we only make an
attempt for L2P, which generally works poorly.
As illustrated in the figures, IBCL has a slightly negative backward transfer in the very beginning
but then this value stays near-zero or positive. This shows that although IBCL may slightly forget
the knowledge learned from the first task at the second task, it steadily retains knowledge after-
wards. This may be due to the choice of the priors, of the likelihood, of the variational method to
approximate the posterior, or to an intrinsic characteristic of our method. Given its relevance, we
defer studying this phenomenon to future work. We can also see how, although VCL’s backward
transfer is higher than IBCL’s in the first few tasks, it eventually decreases and takes values that are
nearly identical to, or smaller than, the IBCL ones. For 20NewsGroup, this happens after 5 tasks,
for TinyImageNet after 3 taks, for Split CIFAR-100 after 10 tasks, and for CelebA after 2 tasks.
Table 1 shows the training overhead comparison measured in number of batch updates per task. We
can see how IBCL’s overhead is independent of the number of preferences nprefsbecause it only
requires training for the FGCS but not for the preferred models. Consequently, our experiments
show that IBCL is able to maintain a constant training overhead per task, regardless of nprefs
while achieving high performance. Although L2P also has this constant overhead, its perfor-
mance is too poor to be acceptable.
The main experiments are conducted with hyperparameters α= 0.01andd= 0.002. We also
conduct two ablation studies. The first one is on different significance level αin Algorithm 2.
In Figure 5, we evaluate testing accuracy on three different α’s over five different preferences (from
[0.1,0.9]to[0.9,0.1]) on the first two tasks of 20NewsGroup. For each preference, we uniformly
sample 200 deterministic models from the HDR. We use the sampled model with the maximum
L2 sum of the two accuracies to estimate the Pareto optimality under a preference. We can see
that, as αapproaches 0, we tend to sample closer to the Pareto front. This is because, with a
smaller α, HDRs become wider and we have a higher probability to sample Pareto-optimal models
8

--- PAGE 9 ---
Figure 5: Different α’s on different preferences over the first two tasks in 20NewsGroup.
Figure 6: Different α’s on randomly generated preferences over all tasks in 20NewsGroup.
Figure 7: Different d’s on 20NewsGroup and Split-CIFAR100. The buffer growth curves of d= 5e−3and
d= 2e−3of 20NewsGroup are overlapping.
according to Theorem 2. For instance, when α= 0.01, we have a probability of at least 0.99that
the Pareto-optimal solution is contained in the HDR. Figure 6 shows that the performance drops as
αincreases, because we are more likely to sample poorly performing models from the HDR. The
second ablation study is on different thresholds din Algorithm 1. As dincreases, we are allowing
more posteriors in the knowledge base to be reused. This will lead to memory efficiency at the
cost of a performance drop. Figure 7 supports this trend. With an appropriately selected d, we can
guarantee that the model’s performance will not be overly affected, and that we save buffer memory.
For Split-CIFAR100, when d= 8e−3, the buffer stops growing after task 6.
6 Conclusion
Overall, we propose a probabilistic continual learning algorithm, namely IBCL, to tackle the CLuST
problem, where an unbounded number of stability-plasticity trade-off preferences may be requested
at each task. IBCL (i) guarantees that the Pareto-optimal model under a given preference can be
sampled from the output HDR with high probability, (ii) zero-shot generates these preferred model,
with training overhead not scaling up with number of preferences, and (iii) has a sublinear buffer
growth in number of tasks.
Limitations of IBCL. Poorly performing models can also be sampled from IBCL’s HDRs. However,
in practice, we can fine-tune αto shrink down the HDR to avoid poorly performing ones, as shown
in the ablation studies.
Broader Impacts. IBCL is potentially useful in deriving user-customized models from large multi-
task models, due to its not only guaranteed but also zero-shot preference addressing. These include
large language models, recommendation systems and other applications.
9

--- PAGE 10 ---
References
[1] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and
Babak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition , 2020.
[2] Akiko Aizawa. An information-theoretic perspective of tf–idf measures. Information Process-
ing & Management , 39(1):45–65, 2003.
[3] Thomas Augustin, Frank P. A. Coolen, Gert de Cooman, and Matthias C. M. Troffaes. Intro-
duction to imprecise probabilities . John Wiley & Sons,, West Sussex, England, 2014.
[4] Patrick Billingsley. Probability and Measure . John Wiley and Sons, second edition, 1986.
[5] Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, and Maja Pantic. Incremental multi-
domain learning with network latent tensor factorization. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence . AAAI Press, 2020.
[6] Michele Caprio, Souradeep Dutta, Kuk Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, and
Insup Lee. Credal Bayesian Deep Learning. arXiv preprint arXiv:2302.09656 , 2024.
[7] Rich Caruana. Multitask learning. Machine learning , 28:41–75, 1997.
[8] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Effi-
cient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420 , 2018.
[9] Z. Chen and B. Liu. Lifelong Machine Learning . Synthesis Lectures on Artificial Intelligence
and Machine Learning. Morgan & Claypool Publishers, 2016.
[10] Frank P. A. Coolen. Imprecise highest density regions related to intervals of measures. Mem-
orandum COSOR , 9254, 1992.
[11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale ˇs Leonardis, Gre-
gory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in
classification tasks. IEEE transactions on pattern analysis and machine intelligence , 44(7):
3366–3385, 2021.
[12] Michel Marie Deza and Elena Deza. Encyclopedia of Distances . Springer Berlin, Heidelberg,
2nd edition, 2013.
[13] Natalia D ´ıaz-Rodr ´ıguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don’t
forget, there is more than forgetting: new metrics for continual learning. arXiv preprint
arXiv:1810.13166 , 2018.
[14] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-
guided continual learning with bayesian neural networks. arXiv preprint arXiv:1906.02425 ,
2019.
[15] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for
continual learning. In Proceedings of the International Conference on Artificial Intelligence
and Statistics , 2020.
[16] Sebastian Farquhar and Yarin Gal. A unifying Bayesian view of continual learning. arXiv
preprint arXiv:1902.06494 , 2019.
[17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for fast
adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the
34th International Conference on Machine Learning , volume 70 of Proceedings of Machine
Learning Research , pages 1126–1135. PMLR, 2017.
[18] Soumyajit Gupta, Gurpreet Singh, Raghu Bollapragada, and Matthew Lease. Scalable
unidirectional pareto optimality for multi-task learning with constraints. arXiv preprint
arXiv:2110.15442 , 2021.
10

--- PAGE 11 ---
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 770–778, 2016.
[20] Rob J. Hyndman. Computing and graphing highest density regions. The American Statistician ,
50(2):120–126, 1996.
[21] Ngumbang Juat, Mike Meredith, and John Kruschke. Package ‘hdinterval ˆaC™,
2022. URL https://cran.r-project.org/web/packages/HDInterval/
HDInterval.pdf . Accessed on May 9, 2023.
[22] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh
losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7482–7491, 2018.
[23] Samuel Kessler, Adam Cobb, Tim G. J. Rudner, Stefan Zohren, and Stephen J. Roberts. On
sequential bayesian inference for continual learning. arXiv preprint arXiv:2301.01828 , 2023.
[24] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy
of sciences , 114(13):3521–3526, 2017.
[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical Report, University of Toronto , 2009.
[26] Ken Lang. Newsweeder: Learning to filter netnews. In Machine learning proceedings 1995 ,
pages 331–339. Elsevier, 1995.
[27] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7(7):3, 2015.
[28] Seungwon Lee, James Stokes, and Eric Eaton. Learning shared knowledge for deep lifelong
learning using deconvolutional networks. In IJCAI , pages 2837–2844, 2019.
[29] Honglin Li, Payam Barnaghi, Shirin Enshaeifar, and Frieder Ganz. Continual learning using
bayesian neural networks. IEEE transactions on neural networks and learning systems , 32(9):
4243–4252, 2020.
[30] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task
learning. Advances in neural information processing systems , 32, 2019.
[31] Xi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong. Controllable Pareto multi-task learn-
ing. arXiv preprint arXiv:2010.06313 , 2020.
[32] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of International Conference on Computer Vision (ICCV) , December
2015.
[33] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learn-
ing. Advances in neural information processing systems , 30, 2017.
[34] Pingchuan Ma, Tao Du, and Wojciech Matusik. Efficient continuous pareto exploration in
multi-task learning. In International Conference on Machine Learning , pages 6522–6531.
PMLR, 2020.
[35] Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient
descent with controlled ascent in pareto optimization. In International Conference on Machine
Learning , pages 6597–6607. PMLR, 2020.
[36] Debabrata Mahapatra and Vaibhav Rajan. Exact pareto optimal search for multi-task learning:
touring the pareto front. arXiv preprint arXiv:2108.00597 , 2021.
[37] Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual
learning. In International Conference on Learning Representations , 2018. URL https:
//openreview.net/forum?id=BkQqq0gRb .
11

--- PAGE 12 ---
[38] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Con-
tinual lifelong learning with neural networks: A review. Neural networks , 113:54–71, 2019.
[39] P. Ruvolo and E. Eaton. Active task selection for lifelong machine learning. Proceedings of
the AAAI Conference on Artificial Intelligence , 27(1), 2013.
[40] Paul Ruvolo and Eric Eaton. ELLA: An efficient lifelong learning algorithm. In San-
joy Dasgupta and David McAllester, editors, Proceedings of the 30th International Confer-
ence on Machine Learning , volume 28 of Proceedings of Machine Learning Research , pages
507–515. PMLR, 17–19 Jun 2013. URL https://proceedings.mlr.press/v28/
ruvolo13.html .
[41] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances
in neural information processing systems , 31, 2018.
[42] Sandra Servia-Rodriguez, Cecilia Mascolo, and Young D Kwon. Knowing when we do
not know: Bayesian continual learning for sensing-based analysis tasks. arXiv preprint
arXiv:2106.05872 , 2021.
[43] Sebastian Thrun. Lifelong learning algorithms. Learning to learn , 8:181–209, 1998.
[44] Gido M. Van de Ven and Andreas S. Tolias. Three scenarios for continual learning. arXiv
preprint arXiv:1904.07734 , 2019.
[45] Peter Walley. Statistical Reasoning with Imprecise Probabilities , volume 42 of Monographs
on Statistics and Applied Probability . London : Chapman and Hall, 1991.
[46] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su,
Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
139–149, 2022.
[47] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian Model-Agnostic Meta-Learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems , volume 31. Curran Associates, Inc., 2018.
[48] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intel-
ligence. In International conference on machine learning , pages 3987–3995. PMLR, 2017.
Appendix A Reason to adopt a Bayesian continual learning approach
Letq0(θ)be our prior probability density/mass function (pdf/pmf) on parameter θ∈Θat time
t= 0. At time t= 1, we collect data (¯x1,¯y1)pertaining to task 1, we elicit likelihood pdf/pmf
ℓ1(¯x1,¯y1|θ), and we compute q1(θ|¯x1,¯y1)∝q0(θ)×ℓ1(¯x1,¯y1|θ). At time t= 2, we collect
data(¯x2,¯y2)pertaining to task 2and we elicit likelihood pdf/pmf ℓ2(¯x2,¯y2|θ). Now we have two
options.
(i) Bayesian Continual Learning (BCL): we let the prior pdf/pmf at time t= 2be the posterior
pdf/pmf at time t= 1. That is, our prior pdf/pmf is q1(θ|¯x1,¯y1), and we compute q2(θ|
¯x1,¯y1,¯x2,¯y2)∝q1(θ|¯x1,¯y1)×ℓ2(¯x2,¯y2|θ)∝q0(θ)×ℓ1(¯x1,¯y1|θ)×ℓ2(¯x2,¯y2|θ);2
(ii) Bayesian Isolated Learning (BIL): we let the prior pdf/pmf at time t= 2be a generic prior
pdf/pmf q′
0(θ). We compute q′
2(θ|¯x2,¯y2)∝q′
0(θ)×ℓ2(¯x2,¯y2|θ). We can even re-use
the original prior, so that q′
0=q0.
As we can see, in option (i) we assume that the data generating process at time t= 2 takes into
account both tasks, while in option (ii) we posit that it only takes into account task 2. Denote by
σ(X)the sigma-algebra generated by a generic random variable X. Let also Q2be the probability
measure whose pdf/pmf is q2, and Q′
2be the probability measure whose pdf/pmf is q′
2. Then, we
have the following.
2Here we tacitly assume that the likelihoods are independent.
12

--- PAGE 13 ---
Proposition 1. Posterior probability measure Q2can be written as a σ(¯X1,¯Y1,¯X2,¯Y2)-measurable
random variable taking values in [0,1], while posterior probability measure Q′
2can be written as a
σ(¯X2,¯Y2)-measurable random variable taking values in [0,1].
Proof. Pick any A⊂Θ. Then, Q2[A|σ(¯X1,¯Y1,¯X2,¯Y2)] =EQ2[ 1A|σ(¯X1,¯Y1,¯X2,¯Y2)], a
σ(¯X1,¯Y1,¯X2,¯Y2)-measurable random variable taking values in [0,1]. Notice that 1Adenotes the
indicator function for set A. Similarly, Q′
2[A|σ(¯X2,¯Y2)] =EQ′
2[ 1A|σ(¯X2,¯Y2)], aσ(¯X2,¯Y2)-
measurable random variable taking values in [0,1]. This is a well-known result in measure theory
[4].
Of course Proposition 1 holds for all t≥2. Recall that the sigma-algebra σ(X)generated by a
generic random variable Xcaptures the idea of information encoded in observing X. An immediate
corollary is the following.
Corollary 1. Lett≥2. Then, if we opt for BIL, we lose all the information encoded in
{(¯Xi,¯Yi)}t−1
i=1.
In turn, if we opt for BIL, we obtain a posterior that is not measurable with respect to
σ({(¯Xi,¯Yi)}t
i=1)\σ(¯Xt,¯Yt). If the true data generating process ptis a function of the previ-
ous data generating processes pt′,t′≤t, this leaves us with a worse approximation of the “true”
posterior Qtrue∝Q0×pt.
The phenomenon in Corollary 1 is commonly referred to as catastrophic forgetting . Continual
learning literature is unanimous in labeling catastrophic forgetting as undesirable – see e.g. [16, 29].
For this reason, in this work we adopt a BCL approach. In practice, we cannot compute the posterior
pdf/pmf exactly, and we will resort to variational inference to approximate them – an approach
often referred to as Variational Continual Learning (VCL) [37]. As we shall see in Appendix E,
Assumption 1 is needed in VCL to avoid catastrophic forgetting.
A.1 Relationship between IBCL and other BCL techniques
Like [16, 29], the weights in our Bayesian neural networks (BNNs) have Gaussian distribution
with diagonal covariance matrix. Because IBCL is rooted in Bayesian continual learning, we can
initialize IBCL with a much smaller number of parameters to solve a complex task as long as it can
solve a set of simpler tasks. In addition, IBCL does not need to evaluate the importance of parameters
by measures such as computing the Fisher information, which are computationally expensive and
intractable in large models.
A.1.1 Relationship between IBCL and MAML
In this section, we discuss the relationship between IBCL and the Model-Agnostic Meta-Learning
(MAML) and Bayesian MAML (BMAML) procedures introduced in [17, 47], respectively. These
are inherently different than IBCL, since the latter is a continual learning procedure, while MAML
and BMAML are meta-learning algorithms. Nevertheless, given the popularity of these procedures,
we feel that relating IBCL to them would be useful to draw some insights on IBCL itself.
In MAML and BMAML, a task iis specified by a ni-shot dataset Dithat consists of a small number
of training examples, e.g. observations (x1i, y1i), . . . , (xni, yni). Tasks are sampled from a task
distribution Tsuch that the sampled tasks share the statistical regularity of the task distribution. In
IBCL, Assumption 1 guarantees that the tasks pishare the statistical regularity of class F. MAML
and BMAML leverage this regularity to improve the learning efficiency of subsequent tasks.
At each meta-iteration i,
1.Task-Sampling : For both MAML and BMAML, a mini-batch Tiof tasks is sampled from the
task distribution T. Each task τi∈Tiprovides task-train and task-validation data, Dtrn
τiandDval
τi,
respectively.
2.Inner-Update : For MAML, the parameter of each task τi∈Tiis updated starting from the
current generic initial parameter θ0, and then performing nigradient descent steps on the task-
train loss. For BMAML, the posterior q(θτi|Dtrn
τi, θ0)is computed, for all τi∈Ti.
13

--- PAGE 14 ---
3.Outer-Update : For MAML, the generic initial parameter θ0is updated by gradient descent. For
BMAML, it is updated using the Chaser loss [47, Equation (7)].
Notice how in our work ¯wis a probability vector. This implies that if we fix a number of task kand
we let ¯wbe equal to (w1, . . . , w k)⊤, then ¯w·¯pcan be seen as a sample from Tsuch that T(pi) =wi,
for all i∈ {1, . . . , k }.
Here lies the main difference between IBCL and BMAML. In the latter the information provided
by the tasks is used to obtain a refinement of the (parameter of the) distribution Ton the tasks
themselves. In IBCL, instead, we are interested in the optimal parameterization of the posterior
distribution associated with ¯w·¯p. Notice also that at time k+ 1, in IBCL the support of Tchanges:
it is{p1, . . . , p k+1}, while for MAML and BMAML it stays the same.
Also, MAML and BMAML can be seen as ensemble methods, since they use different values
(MAML) or different distributions (BMAML) to perform the Outer-Update and come up with a
single value (MAML) or a single distributions (BMAML). Instead, IBCL keeps distributions sepa-
rate via FGCS, thus capturing the ambiguity faced by the designer during the analysis.
Furthermore, we want to point out how while for BMAML the tasks τiare all “candidates” for the
true data generating process (dgp) pi, in IBCL we approximate the pdf/pmf of piwith the productQi
h=1ℓhof the likelihoods up to task i. The idea of different candidates for the true dgp is beneficial
for IBCL as well: in the future, we plan to let go of Assumption 1 and let each pibelong to a credal
setPi. This would capture the epistemic uncertainty faced by the agent on the true dgp.
To summarize, IBCL is a continual learning technique whose aim is to find the correct parameteriza-
tion of the posterior associated with ¯w·¯p. Here, ¯wexpresses the developer’s preferences on the tasks.
MAML and BMAML, instead, are meta-learning algorithms whose main concern is to refine the dis-
tribution Tfrom which the tasks are sampled. While IBCL is able to capture the preferences of, and
the ambiguity faced by, the designer, MAML and BMAML are unable to do so. On the contrary,
these latter seem better suited to solve meta-learning problems. An interesting future research direc-
tion is to come up with imprecise BMAML, or IBMAML, where a credal set Conv ({T1, . . . ,Tk})
is used to capture the ambiguity faced by the developer in specifying the correct distribution on the
possible tasks. The process of selecting one element from such credal set may lead to computational
gains.
Appendix B Highest Density Region
Equivalently to Definition 2, an HDR is defined as follows [20].
Definition 4. LetΘbe a set of interest, and consider a significance level α∈[0,1]. Suppose that a
(continuous) random variable θ∈Θhas probability density function (pdf) q.3Theα-level Highest
Density Region (HDR) Θα
qis the subset of Θsuch that
Θα
q={θ∈Θ :q(θ)≥qα}, (2)
where qαis a constant value. In particular, qαis the largest constant such that Prθ∼q[θ∈Θα
q]≥
1−α.
Some scholars indicate HDRs as the Bayesian counterpart to the frequentist concept of confidence
intervals. In dimension 1,Θα
qcan be interpreted as the narrowest interval – or union of intervals
– in which the value of the (true) parameter falls with probability of at least 1−αaccording to
distribution q. We give a simple visual example in Figure 8.
Appendix C Additional Related Work
Multi-task Learning under Preferences. Learning for Pareto-optimal models under task perfor-
mance trade-offs has been studied by researchers in multi-task learning [7, 41]. Various techniques
have been applied to obtain models that address particular trade-off points [30, 31, 34, 18]. The idea
of preferences on the trade-off points is introduced in multi-objective optimization [31, 41], and a
3Here too, for ease of notation, we do not distinguish between a random variable and its realization.
14

--- PAGE 15 ---
/uni0302p0.25y/uni0302p(y)q(θ)θq0.25Figure 8: The 0.25-HDR for a Normal Mixture density. This picture is a replica of Figure 1 in [20]. The
geometric representation of “ 75% probability according to q” is the area between the pdf curve q(θ)and the
horizontal bar corresponding to q0.25. A higher probability coverage (according to q) would correspond to a
lower constant, so qα< q0.25, for all α < 0.25. In the limit, we recover 100% coverage at q0= 0.
preference can guide learning algorithms to search for a particular model. We borrow the formaliza-
tion of preferences from [35], where a preference is given by a vector of non-negative real weights
¯w, with each entry wicorresponding to task i. That is, wi≥wj⇐⇒ i⪰j. This means that if
wi≥wj, then task iis preferred to task j. However, state-of-the-art algorithms require training one
model per preference, imposing large overhead when there is a large number of preferences.
Continual Learning. Continual learning, also known as lifelong learning, is a special case of multi-
task learning, where tasks arrive sequentially instead of simultaneously [43, 39]. In this paper, we
leverage Bayesian inference in the knowledge base update [14]. Like generic multi-task learning,
continual learning also faces the stability-plasticity trade-off [11], which balances between perfor-
mance on new tasks and resistance to catastrophic forgetting [24]. Current methods identify models
to address trade-off preferences by techniques such as loss regularization [42], meaning at least one
model needs to be trained per preference.
Researchers in CL have proposed various approaches to retain knowledge while updating a model
on new tasks. These include modified loss landscapes for optimization [15], preservation of critical
pathways via attention [1], memory-based methods [33], shared representations [28], and dynamic
representations [5]. Bayesian, or probabilistic methods such as variational inference are also adopted
[14, 37].
Appendix D 2-Wasserstein metric
In the main portion of the paper, we endowed ∆XYwith the 2-Wasserstein metric. It is defined as
∥p−p′∥W2≡W2(p, p′) :=r
inf
γ∈Γ(p,p′)E((x1,y1),(x2,y2))∼γ[d((x1, y1),(x2, y2))2], (3)
where
1.p, p′∈∆XY;
2.Γ(p, p′)is the set of all couplings of pandp′. A coupling γis a joint probability measure on
(X × Y )×(X × Y )whose marginals are pandp′on the first and second factors, respectively;
3.dis the product metric endowed to X × Y [12, Section 4.2].4
We choose the 2-Wasserstein distance for the ease of computation. In practice, when all distributions
are modeled by Bayesian neural networks with independent Gaussian weights and biases, we have
∥q1−q2∥2
W2=∥µ2
q1−µ2
q2∥2
2+∥σ2
q11−σ2
q21∥2
2, (4)
where ∥ · ∥2denotes the Euclidean norm, 1is a vector of all 1’s, and µqandσqare respectively the
mean and standard deviation of a multivariate normal distribution qwith independent dimensions,
q=N(µq, σ2
qI),Ibeing the identity matrix. Therefore, computing the W2-distance between two
distributions is equivalent to computing the difference between their means and variances.
4We denote by dXanddYthe metrics endowed to XandY, respectively.
15

--- PAGE 16 ---
Appendix E Importance of Assumption 1
We need Assumption 1 in light of the results in [23]. There, the authors show that misspecified
models can forget even when Bayesian inference is carried out exactly. By requiring that diam (F) =
r, we control the amount of misspecification via r. In [23], the authors design a new approach –
called Prototypical Bayesian Continual Learning, or ProtoCL – that allows dropping Assumption 1
while retaining the Bayesian benefit of remembering previous tasks. Because the main goal of this
paper is to come up with a procedure that allows the designer to express preferences over the tasks,
we retain Assumption 1, and we work in the classical framework of Bayesian Continual Learning.
In the future, we plan to generalize our results by operating with ProtoCL.5
Appendix F An example of a parameterized family F
Let us give an example of a parameterized family F. Suppose that we have one-dimensional data
points and labels. At each task i, the marginal on Xofpiis a Normal N(µ,1), while the conditional
distribution of label y∈ Y given data point x∈ X is a categorical Cat (ϑ). Hence, the parameter
forpiisθ= (µ, ϑ), and it belongs to Θ =R×R|Y|. In this situation, an example of a family F
satisfying Assumptions 1 and 2 is the convex hull of distributions that can be decomposed as we just
described, and whose distance according to the 2-Wasserstein metric does not exceed some r >0.
Appendix G Proofs of the Theorems
Proof of Theorem 1. Without loss of generality, suppose we have encountered i= 2tasks so far, so
the FGCS is Q2. Let ex [Q1] ={qj
1}m1
j=1and ex [Q2]\ex[Q1] ={qj
2}m2
j=1. Let ˆqbe any element of
Q2. Then, there exists a probability vector ¯β= (β1
1, . . . , βm1
1, β1
2, . . . , βm2
2)⊤such that
ˆq=m1X
j=1βj
1qj
1+m2X
j=1βj
2qj
2∝ˆp1m1X
j=1βj
1qj
0+ ˆp2m2X
j=1βj
2qj
0. (5)
Here, ˆpi=Qi
k=1ℓk, andℓkis the likelihood at task k. It estimates the pdf of the true data generating
process piof task i. The proportional relationship in (5) is based on the Bayesian inference step (line
3, approximated via variational inference) of Algorithm 1. We can then find a vector ¯w= (w1=Pm1
j=1βj
1, w2=Pm2
j=1βj
2)⊤that expresses the designer’s preferences over tasks 1and2. As we can
see, then, the act of selecting a generic distribution ˆq∈ Q 2is equivalent to specifying a preference
vector ¯wover tasks 1and2. This concludes the proof.
Proof of Theorem 2. For maximum generality, assume Θis uncountable. Recall from Definition 2
thatα-level Highest Density Region Θα
¯wis defined as the subset of the parameter space Θsuch that
Z
Θα
¯wˆq¯w(θ)dθ≥1−α andZ
Θα
¯wdθis a minimum.
We needR
Θα
¯wdθto be a minimum because we want Θα
¯wto be the smallest possible region that gives
us the desired probabilistic coverage. Equivalently, from Definition 4 we can write that Θα
¯w={θ∈
Θ : ˆq¯w(θ)≥ˆqα
¯w}, where ˆqα
¯wis the largest constant such that Prθ∼ˆq¯w[θ∈Θα
¯w]≥1−α. Our result
Prθ⋆
¯w∼ˆq¯w[θ⋆
¯w∈Θα
¯w)]≥1−α, then, comes from the fact that Prθ⋆
¯w∼ˆq¯w[θ⋆
¯w∈Θα
¯w)] =R
Θα
¯wˆq¯w(θ)dθ,
a consequence of a well-known equality in probability theory [4].
Appendix H Details of Experiments
H.1 Setup Details
We select 15 tasks from CelebA. All tasks are binary image classification on celebrity face images.
Each task iis to classify whether the face has an attribute such as wearing eyeglasses or having
5In [23], the authors also show that if there is a task dataset imbalance, then the model can forget under
certain assumptions. To avoid complications, in this work we tacitly assume that task datasets are balanced.
16

--- PAGE 17 ---
a mustache. The first 15 attributes (out of 40) in the attribute list [32] are selected for our tasks.
The training, validation and testing sets are already split upon download, with 162,770, 19,867 and
19,962 images, respectively. All images are annotated with binary labels of the 15 attributes in our
tasks. We use the same training, validation and testing set for all tasks, with labels being the only
difference.
We select 20 classes from CIFAR100 [25] to construct 10 Split-CIFAR100 tasks [48]. Each task is a
binary image classification between an animal class (label 0) and a non-animal class (label 1). The
classes are (in order of tasks):
1. Label 0: aquarium fish, beaver, dolphin, flatfish, otter, ray, seal, shark, trout, whale.
2. Label 1: bicycle, bus, lawn mower, motorcycle, pickup truck, rocket, streetcar, tank, tractor,
train.
That is, the first task is to classify between aquarium fish images and bicycle images, and so on. We
want to show that the continual learning model incrementally gains knowledge of how to identify
animals from non-animals throughout the task sequence. For each class, CIFAR100 has 500 training
data points and 100 testing data points. We hold out 100 training data points for validation. There-
fore, at each task we have 400 ×2 = 800 training data, 100 ×2 = 200 validation data and 100 ×2
= 200 testing data.
We also select 20 classes from TinyImageNet [27]. The setup is similar to Split-CIFAR100, with
label 0 being animals and 1 being non-animals.
1. Label 0: goldfish, European fire salamander, bullfrog, tailed frog, American alligator, boa con-
strictor, goose, koala, king penguin, albatross.
2. Label 1: cliff, espresso, potpie, pizza, meatloaf, banana, orange, water tower, via duct, tractor.
The dataset already splits 500, 50 and 50 images for training, validation and testing per class. There-
fore, each task has 1000, 100 and 100 images for training, validation and testing, respectively.
20NewsGroups [26] contains news report texts on 20 topics. We select 10 topics for 5 binary text
classification tasks. Each task is to distinguish whether the topic is computer-related (label 0) or not
computer-related (label 1), as follows.
1. Label 0: comp.graphics, comp.os.ms-windows.misc, comp.sys.ibm.pc.hardware,
comp.sys.mac.hardware, comp.windows.x.
2. Label 1: misc.forsale, rec.autos, rec.motorcycles, rec.sport.baseball, rec.sport.hockey.
Each class has different number of news reports. On average, a class has 565 reports for training and
376 for testing. We then hold out 100 reports from the 565 for validation. Therefore, each binary
classification task has 930, 200 and 752 data points for training, validation and testing, on average
respectively.
All data points are first preprocessed by a feature extractor. For images, the feature extractor is a
pre-trained ResNet18 [19]. We input the images into the ResNet18 model and obtain its last hidden
layer’s activations, which has a dimension of 512. For texts, the extractor is TF-IDF [2] succeeded
with PCA to reduce the dimension to 512 as well.
Each Bayesian network model is trained with evidence lower bound (ELBO) loss, with a fixed feed-
forward architecture (input=512, hidden=64, output=1). The hidden layer is ReLU-activated and the
output layer is sigmoid-activated. Therefore, our parameter space Θis the set of all values that can
be taken by this network’s weights and biases.
The three variational inference priors, learning rate, batch size and number of epcohs are tuned on
validation sets. The tuning results are as follows. Here, “lr” stands for learning rate.
1. CelebA: priors = {N(0,0.22I),N(0,0.252I),N(0,0.32I)}, lr =1e−3, batch size = 64, epochs
=10.
2. Split-CIFAR100: priors = {N(0,22I),N(0,2.52I),N(0,32I)}, lr = 5e−4, batch size = 32,
epochs = 50.
3. TinyImageNet: priors = {N(0,22I),N(0,2.52I),N(0,32I)}, lr = 5e−4, batch size = 32,
epochs = 30.
17

--- PAGE 18 ---
4. 20NewsGroup: priors = {N(0,22I),N(0,2.52I),N(0,32I)}, lr = 5e−4, batch size = 32,
epochs = 100.
For the baseline methods, we use exactly the same learning rate, batch sizes and epochs. For proba-
bilistic baseline methods (VCL), we use the prior with the median standard deviation. For example,
on CelebA tasks, VCL uses the normal prior N(0,0.252I).
For rehearsal-based baselines, the memory size per task for CelebA is 200, and for the rest is 50.
Together with the numbers above, we can compute the numerical values in Table 1.
H.2 Additional Results
In Figure 9 and 10, we provide visual representations of the performances of IBCL vs. the baseline
methods on Split CIFAR-100 and CelebA, respectively. So far, to our knowledge, there is no dis-
cussion on how to specify a task trade-off preference in prompt-based continual learning, and we
make an attempt by using a preference-weighted sum of all learned prompts in L2P. We can see how
this approach generally works poorly, except for CelebA, where L2P performs nearly well as IBCL.
We believe the performance by prompts trained in L2P depends on its frozen model, and how to use
prompt-based methods to generate preference-specified models is still an open problem.
Figure 9: Results of Split CIFAR-100.
Figure 10: Results of CelebA.
18
