# 2212.02846.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2212.02846.pdf
# File size: 6155978 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Statistical mechanics of continual learning: variational principle
and mean-field potential
Chan Li1,∗Zhenye Huang2,∗Wenxuan Zou1,∗and Haiping Huang1†
1PMI Lab, School of Physics, Sun Yat-sen University,
Guangzhou 510275, People’s Republic of China and
2CAS Key Laboratory for Theoretical Physics,
Institute of Theoretical Physics,Chinese Academy of Sciences,
Beijing 100190, People’s Republic of China
(Dated: June 21, 2023)
1arXiv:2212.02846v4  [cond-mat.stat-mech]  20 Jun 2023

--- PAGE 2 ---
Abstract
An obstacle to artificial general intelligence is set by continual learning of multiple tasks of dif-
ferent nature. Recently, various heuristic tricks, both from machine learning and from neuroscience
angles, were proposed, but they lack a unified theory ground. Here, we focus on continual learning
in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learn-
ing setting is thus proposed, where the neural networks are trained in a field-space, rather than
gradient-ill-defined discrete-weight space, and furthermore, weight uncertainty is naturally incor-
porated, and modulates synaptic resources among tasks. From a physics perspective, we translate
the variational continual learning into Franz-Parisi thermodynamic potential framework, where
previous task knowledge acts as a prior and a reference as well. We thus interpret the continual
learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential computa-
tion. The learning performance can then be analytically studied with mean-field order parameters,
whose predictions coincide with numerical experiments using stochastic gradient descent methods.
Based on the variational principle and Gaussian field approximation of internal preactivations in
hidden layers, we also derive the learning algorithm considering weight uncertainty, which solves the
continual learning with binary weights using multi-layered neural networks, and performs better
than the currently available metaplasticity algorithm where binary synapses bear hidden continuous
states and the synaptic plasticity is modulated by a heuristic regularization function. Our proposed
principled frameworks also connect to elastic weight consolidation, weight-uncertainty modulated
learning, and neuroscience inspired metaplasticity, providing a theory-grounded method for the
real-world multi-task learning with deep networks.
∗Equal contribution.
†Electronic address: huanghp7@mail.sysu.edu.cn
2

--- PAGE 3 ---
I. INTRODUCTION
The environment an intelligent agent faces is commonly highly structured, and moreover,
multiple tasks encoding this structure occur in sequence. Therefore, it is important for the
agent to learn the continually evolving structures embedded in sequential tasks, i.e., transfer
the knowledge gained from previous experiences to the learning of a current novel or unfa-
miliar task. However, during this continual learning, it is well-known that the previous task
knowledge may be erased after learning a new task (so-called catastrophic forgetting [1, 2]).
Uncovering neural mechanisms underlying a successful continual learning especially in the
natural world presents a challenge in the current AI and even neuroscience research. There
also emerge recently interesting works on biological neuronal networks in this regard [3–5],
and the neuroscience research provides in turn insights for improving the performance of
continual learning in artificial neural networks [6–8].
To avoid catastrophic forgetting, the machine learning community also proposed many
heuristic strategies. For example, the elastic weight consolidation method introduces the
Fisher information matrix to measure weight importance in the consecutive task learning [9],
which is further improved by tracking individual weight contribution over the entire dynam-
ics of training loss [10]. An attention mask can also be learned to alleviate the catastrophic
forgetting [11]. Another important line is using the Bayesian approach [12]. This line shows
that the synaptic uncertainty plays a significant role in taking the learning trade-off between
two consecutive tasks [13–16]. We remark that these heuristic strategies have diverse de-
sign principles, but from a statistical physics perspective, they can be put under a unified
framework of variational mean-field theory. Although recent theoretical works focused on
phase transitions in transfer learning from source task to target task [17] and on-line learn-
ing dynamics of teacher-student setup [18–20], these works did not take into account weight
uncertainty, which is an essential factor in learning neural networks [21], including more
efficient and robust binary-weight networks. In addition, a recent study pointed out that
the concept of meta-plasticity from brain science plays a key role in the continual learning
of binary-weight neural networks [8]. This concept highlights that the binary synapse bears
a hidden continuous state, and the synaptic plasticity is modulated by a heuristic regular-
ization function. Our theoretical framework demonstrates that a variational principle can
be constructed to explain the role of synaptic uncertainty, and moreover, the knowledge-
3

--- PAGE 4 ---
transfer between tasks can be actually captured by a thermodynamic potential [22], from
which the learning performance can be predicted.
In this work, we not only carry out a thorough theoretical analysis of a toy teacher-
student learning setting, where both tasks of a certain level of similarity are learned in
sequence, but also apply the same principle to deep continual learning of structured datasets,
which demonstrates the effectiveness of the variational mean-field principle, especially in the
binary-weight neural networks where only meta-plasticity was previously proposed. Overall,
our theory bridges statistical physics, especially the concept of the Franz-Parisi potential,
originally studied in mean-field spin glass models [23], to theoretical underpinnings of the
challenging continual learning. This connection may prove fruitful in future researches.
II. CONTINUAL LEARNING WITH BINARY PERCEPTRON
The binary perceptron offers an ideal candidate for understanding non-convex learning,
as a theoretical analysis is possible by using statistical physics methods [24]. Here, we will
use a teacher-student setting to perform the theoretical analysis of variational continual
learning, in which the ground truth network is quenched before learning. In this section
of toy model analysis, we use ξandWto indicate the student’s and teacher’s weights,
respectively. In the next section of training deep networks (no ground truth in this case),
we use wto indicate the weights to learn.
A. Learning setting
The standard perceptron is a single-layered network with Nbinary input nodes, xi=
±1 (i= 1,2, ..., N ), and a single binary output node, y=±1, which is connected by
Nbinary weights ξi=±1 (i= 1,2, ..., N ). Given an input x, the output is specified
byy= sign(1√
NP
ixiξi), where sign( x) is the sign function. A perceptron can be used
to classify inputs according to their respective labels ( ±1 here). A statistical mechanics
analysis revealed that the network can store up to a critical threshold of pattern density
(or sample complexity) α≃0.83 [25], where α=M
Nis the random-pattern (as inputs)
density, and Mis the number of random patterns. Instead of this classic random pattern
storage setting, we consider learning task of random patterns with respective labels generated
4

--- PAGE 5 ---
by teacher networks (corresponding to different tasks). This is called the teacher-student
setting [26, 27], where the student network learns to infer the teachers’ rule embedded in
the supplied data.
With increasing number of supplied learning examples, the size of the candidate-solution
space of weights shrinks, and thus the generalization error on fresh data examples decreases.
The statistical mechanics analysis also predicted that at α≃1.245, a first order phase
transition to perfect generalization occurs [26, 27], which is the single-task learning. In our
continual learning setting, we design two teacher networks with binary weights W1∈ {± 1}N
andW2∈ {± 1}N, respectively. Both teacher networks are ground truth for corresponding
tasks. By definition, both teachers share an adjustable level of correlations in their weights,
representing the similarity across tasks. In practice, their weights follow a joint distribution
as
P(W1,W2) =NY
i=1P0(W1
i, W2
i) =NY
i=11 +r0
4δ(W1
i−W2
i) +1−r0
4δ(W1
i+W2
i)
,(1)
where r0∈[−1,1] denotes the task similarity. r0also denotes the overlap of the two teacher
networks, since r0=1
NPN
i=1W1
iW2
i. The marginal joint probability P0(W1
i, W2
i) can be
rewritten as P0(W1
i, W2
i) =p(W1
i)p(W2
i|W1
i), where p(W1
i) =1
2δ(W1
i−1) +1
2δ(W1
i+ 1),
andp(W2
i|W1
i) =1+r0
2δ(W1
i−W2
i) +1−r0
2δ(W1
i+W2
i). To generate weights of the two
teacher networks, we can first generate a set of random binary weights from the Rademacher
distribution, and then flip the weight by a probability1−r0
2. The random patterns for the two
tasks are independently sampled from the Rademacher distribution as well, and then we have
the training dataset {xt,µ}Mt
µ=1, where the task index t= 1,2. Given the sampled patterns,
the teacher networks generate corresponding labels for each task, {yt,µ}Mt
µ=1. Hereafter, we
useDt={xt,µ, yt,µ}Mt
µ=1, to denote the two datasets corresponding to the consecutive two
tasks. The student network is another binary perceptron, whose goal is to learn the task
rule provided by the teacher networks.
In the above setting, the student network shares the same structure (connection topology)
with the two teacher networks, which implies that the student can not simultaneously learn
both tasks perfectly, depending on the task similarity. However, this setting allows us to
explore how the student adapts its weights to avoid catastrophic forgetting during learning
of a new task, and how the network takes a trade-off between new and old knowledges in
5

--- PAGE 6 ---
0 250 500 750 1000 1250 1500
Epoch50
40
30
20
10
010
UB
ELLFIG. 1: Comparison between expected log-likelihood (ELL) and its upper bound (UB) in a simple
network with 10 synapses and M= 10 examples to learn. In this case, ELL can be exactly
computed by an exhaustive enumeration. For the numerical purpose, we use the surrogate Θ( x) =
limκ→∞eκx
2 cosh( κx). We take κ= 10.0. Equation (2) is used for ELL, while Eq. (3) is used for UB.
continual learning. Studying this simple system could also provide us insights about the
continual learning in more complex applications, such as deep learning in real-world data.
B. Variational learning principle
Instead of training point weights, we consider learning the distribution of the weights in
the sense that we train the student network to find an optimal distribution of weights [28].
Along this line, the variational method is an ideal framework for neural network learning [29,
30], since we can use simple trial distribution to approximate the original intractable weight
distribution. The learning becomes then finding an optimal trial distribution parameterized
by variational parameters [24].
In the first task, we introduce a variational distribution for synaptic weights, qθ(ξ) =
Q
ieβθiξi
cosh( βθi), where θare variational parameters, and βis a hyperparameter. The optimal
6

--- PAGE 7 ---
distribution can be approximated by maximizing the expected log-likelihood
θ∗= arg max
θEqθlnP(D1|ξ). (2)
Given an input x, we choose the probability P(D1|ξ) = P(y|x,ξ) as P(y|x,ξ) =
Θ
yPN
i=1ξixi
, where Θ( x) is the Heaviside function such that Θ( x) = 1, if x > 0 and
Θ(x) = 0 otherwise. In practice, based on the Jensen’s inequality, we actually update the
variational parameters by maximizing the upper bound ln EqθP(D1|ξ), which is less com-
putationally challenging than the original one, and the optimization problem can then be
formulated as
θ∗= arg min
θ{−lnEqθP(D1|ξ)}
= arg min
θ(
−lnEqθY
µΘ
yµNX
i=1ξixµ
i)
.(3)
Maximizing the upper bound has been proved to be effective in unsupervised learning with
many hidden neurons [30]. When the number of weights are about 10, the expected log-
likelihood can be exactly computed, and we have checked that the bound could be tight (see
Fig. 1). Based on the assumption of large Nand the central limit theorem, Eq. (3) can be
recast into the following form [29]
θ∗= arg min
θ

−X
µlnH
−yµP
ixµ
itanhβθiqP
i(1−tanh2βθi)


, (4)
where H(x) =R∞
xdze−z2
2/√
2π=R∞
xDz, where Dzdenotes a standard Gaussian measure.
Setting the loss function as L=−P
µlnH
−yµP
ixµ
itanhβθi√P
i(1−tanh2βθi)
, we can use the stochastic
gradient descent (SGD)-based method to find a good trial distribution of weights, which
may be a local or global minimum since the loss is a non-convex function. The gradients
can be derived below,
∂L
∂θt
j=β(σt
j)2 
yxjP
i(σt
i)2+ tanh( βθt
j)P
ixitanh( βθt
i)
(P
i(σt
i)2)3
2
×H′ 
−yP
ixitanh( βθt
i)qP
i(1−tanh2(βθt
i))!
×H−1 
−yP
ixitanh( βθt
i)qP
i(1−tanh2(βθt
i))!
(5)
7

--- PAGE 8 ---
where σ2
j= 1−tanh2(βθj) captures the weight uncertainty, the data index µis neglected,
andtdenotes the iterative time step. The synaptic plasticity is thus modulated by the
weight uncertainty, which is biologically plausible [31] and bears the similarity with other
heuristic strategies [8, 15]. Another salient feature is that, provided that the uncertainty of
a weight is small, this weight can be less plastic because of encoding important information
of previous tasks. In addition, the weight’s synaptic plasticity is also tuned by the total
uncertainty of the network,P
i(σt
i)2, which plays a role of global regularization.
During the second-task learning, the posterior distribution of weights becomes
P(ξ|D1,D2) =P(D2|ξ,D1)P(ξ|D1)
P(D2|D1). (6)
We assume that when the student learns the second task, the knowledge from the first task
becomes a prior constraining the subsequent learning, i.e., P(ξ|D1)≃qθ1(ξ), where qθ1(ξ) is
the variational distribution after learning the first task. We model the posterior of weights
during learning of the second task as qθ2(ξ) =Q
ieβθ2
iξi
cosh( βθ2
i). Optimal variational parameters
can be obtained by minimizing the Kullback-Leibler (KL) divergence between variational
distribution and posterior distribution [30],
θ2∗= arg min
θ2Eqθ2lnqθ2(ξ)
P(ξ|D1,D2)
= arg min
θ2Eqθ2lnqθ2(ξ)
qθ1(ξ)−Eqθ2lnP(D2|ξ)
≃arg min
θ2KL
qθ2(ξ)||qθ1(ξ)
−lnEqθ2P(D2|ξ),(7)
where we discard P(D2|D1) because this term does not depend on the model parameters,
and we use P(D2|ξ,D1) =P(D2|ξ), and we also approximate the objective function Lby
minimizing the lower bound of the KL divergence (in other words, we train the network
to make the bound as tight as possible). We remark here that one can also minimize the
KL divergence between a trial probability qθand the posterior P(ξ|D1) for the first-task
learning [see Eq. (2)], which would require a prior probability of ξ. Even if we set this prior
to a uniform one, the system exhibits a similar learning behavior but the learning becomes
harder as more data samples are required for reaching the same low generalization error with
the learning using Eq. (2). Therefore, we use Eq. (2) as our first-task learning framework.
8

--- PAGE 9 ---
The first term in Eq. (7) is a regularized term that makes the network to maintain the
learned information of the first task. The second term is the expected log-likelihood term
that leads the network to explain new data. The SGD-based method can then be applied to
obtain the optimal solution (local or global minimum). The gradient can be computed as,
∂L
∂θ2,t
j=β(σ2,t
j)2 
β(θ2,t
j−θ1
j) +yxjP
i(σ2,t
i)2+ tanh( βθ2,t
j)P
ixitanh( βθ2,t
i)
(P
i(σ2,t
j)2)3
2
×H′ 
−yP
ixitanh( βθ2,t
i)qP
i(1−tanh2(βθ2,t
i))!
×H−1 
−yP
ixitanh( βθ2,t
i)qP
i(1−tanh2(βθ2,t
i))!
,(8)
where Lis the objective function to minimize in Eq. (7), and the data index is neglected
and must refer to the task 2, and the gradient is still modulated by the weight uncertainty.
Compared to the gradient of the first-task learning, the additional term comes from the KL
divergence term. This term encourages the network to remember the first-task information.
Therefore, this synaptic plasticity rule expresses the competition between old and new tasks
(the second term). This trade-off allows the network to maintain the old knowledge but still
adapt to the new task, thereby avoiding catastrophic forgetting to some extent.
We first show the simulation performance of our toy variational continual learning set-
ting. In Fig. 2 (a), with increasing amount of provided examples, the single-task learning
performance improves. In Fig. 2 (b), when the task transition occurs, the test error of the
first task increases, yet finally achieving a stable value across training. The test error of
the second task decreases, but can not achieve the error level that can be reached when
the task is trained in isolation [Fig. 2(c)]. This is because the network does not forget the
distinct characteristics of the first task completely, due to the regularization term. The lower
test error would be achieved given more training examples for the second task. Figure 2
(d) illustrates the effect of the KL term, where we plot the overlap between the student
inference and the common part of both teachers. Without the KL term, the overlap falls
more sharply and then increases more rapidly, while the presence of the KL term makes the
overlap change relatively slowly. This suggests that, the KL term makes the network tend
to protect the first task from a fast forgetting (see the poor performance of the traditional
SGD in continual learning in Fig. 2(b,c)]. The lower overlap (but still closer to one) allows
more flexibility to balance the continual learning.
9

--- PAGE 10 ---
b
c
a
d
FIG. 2: Learning performance of the toy model. (a) Test error of the first task with different α. (b)
Test error of the first task. (c) Test error of the second task. In (b,c), the task transition occurs at
the 3000-th epoch, and r0= 0. α1= 2, but α2varies. Results are averaged over 20 trials. (d) The
overlap qcombetween the student weights and the common part of both teachers (task similarity).
qcom=1
NcomP
iˆmiˆWi, where ˆ midenotes the student’s magnetization of the i-th synaptic weight
and we choose isuch that W1
i=W2
i(=ˆWi).Ncomdenotes the total number of common weights
in both teachers. In simulations, r0= 0.5,α1= 3 and α2= 4. For the traditional (full batch) GD
algorithm, the optimization for two tasks is in the magnetization space (see Eq. (10) for the first
task, and the second task has a similar form), and no KL divergence terms are used. The other
two algorithms are implemented in the field ( θ) space. The task switch (the dashed line) occurs
at the 3000-th epoch. The network size N= 1 000. For (a,b,c), SGD is applied. For (d), the full
batch GD is used. The performance of the traditional SGD (dashed lines) is also shown in (b,c)
for comparison.
We next derive the mean-field theory to evaluate analytically the continual learning per-
formance.
10

--- PAGE 11 ---
C. Mean-field theory: Franz-Parisi Potential
Mean-field theory is a powerful tool for analyzing complex systems in statistical physics.
In the previous section, we describe the variational method in training the binary perceptron
to realize continual learning. In this section, we derive mean-field theory to analyze the
variational continual learning. Instead of the local fields θ1andθ2(useful for practical
training due to their unbounded values), we parameterize the variational distribution with
weight-magnetization m1,i= tanh βθ1
iandm2,i= tanh βθ2
i, for the sake of analytical studies.
The variational distributions are specified respectively by
Qm1(ξ) =NY
i=11 +ξim1,i
2,
Qm2(ξ) =NY
i=11 +ξim2,i
2,(9)
where m1,i,m2,i∈[−1,1] are the magnetization of the ithsynaptic weight in the first and
second task learning respectively. We perform the statistical mechanics analysis on the
two-task learning, with the goal of extracting the role of model parameters (e.g., sample
complexity, task similarity and so on) in the continual learning.
1. The first-task analysis
To perform the mean field theory analysis of the first-task learning, we define the loss
function in the variational method as the Hamiltonian,
L1(m) =−M1X
µ=1lnH 
−yµP
imix1,µ
ipP
i(1−m2
i)!
, (10)
where yµ= sign P
iW1
ix1,µ
i
is the label generated by the teacher network. The Boltzmann
distribution reads
P(m) =1
Ze−βL1(m), (11)
where βis an inverse temperature, Z=R
ΩQ
idmie−βL1(m)is the partition function and the
integral domain Ω = [ −1,1]N. To obtain the equilibrium properties, we should first compute
11

--- PAGE 12 ---
the disorder-averaged free energy (or the log-partition-function), which can be achieved by
using the replica trick. The replica trick proceeds as ⟨lnZ⟩= lim n→0ln⟨Zn⟩
n, where ⟨·⟩denotes
the average over the quenched disorder. Then, we have
⟨Zn⟩=Z
ΩnnY
a=1NY
i=1dma
i*nY
a=1M1Y
µ=1Hβ 
−sign(P
iW1
ix1,µ
i)P
ima
ix1,µ
ipP
i1−(ma
i)2!+
. (12)
Under the replica symmetric (RS) Ans¨ atz (detailed in the appendix C), the free energy
density at a given data density α=M
Nis given by
−βfRS= lim
n→0,N→∞ln⟨Zn⟩
nN= lim
n→0−1
2(ˆqdqd+ (n−1)ˆq0q0)−ˆr1r1+lnGS
n+α1lnGE
n,(13)
where
GE=Z
Dz2H 
−r1p
q0−r2
1z!Z
Dσ Hβ
−√qd−q0σ+√q0z√1−qdn
,
GS=Z
DzZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆrmn
.(14)
Order parameters are introduced as q0=1
NP
ima
imb
ifora̸=bindicating the overlap
of different equilibrium states, qd=1
NP
ima
ima
iindicating the self-overlap of states (the
self-overlap relates to the size of valid weight space ˆ σ= 1−qd), and finally r1=1
NP
imiW1
i
indicating the overlap between the student’s inference and the teacher’s ground truth. In
practice, qdcan be also estimated from the gradient descent dynamics during training, and
we use q∗to denote this measure, i.e., q∗(t) =1
NP
i(mi(t))2, and ∆ mi(t)∝∂miL1(m).
{ˆq0,ˆqd,ˆr1}are the conjugated order parameters introduced by the Fourier transform. These
order parameters can be obtained by solving saddle point equations (detailed in appendix C).
To evaluate the learning performance, we define the generalization error ϵ1
g=⟨Ex∗Θ(−y∗ˆy∗)⟩,
where ( x∗, y∗) is the fresh data sample, and ˆ y∗is the student’s prediction, and ⟨·⟩denotes
the disorder average. The test error can be calculated as (see details in appendix C)
ϵ1
g=Z
Dz2H 
−p1p
1−p2
1z!
Θ(−z) =1
πarccos( p1), (15)
where p1=1
NP
isign(mi)W1
idenotes the overlap between the decoded weights and the
12

--- PAGE 13 ---
teacher’s weights, and can be obtained by solving the saddle point equations as well.
We first show how order parameters change with respect to the data density α. As
defined, qdsignals the size of the valid weight space. As the sample complexity increases,
the weight space shrinks down to a singe point representing the ground truth [Fig. 3 (a)].
As shown in Fig. 3 (b), the stochastic gradient descent dynamics results are approximately
consistent with the theoretical predictions (at least qualitatively). The deviations may be
caused by the fact that the SGD could be trapped by local minima (suboptimal solutions)
of the variational energy landscape. But for a fixed α, we can dynamically rescaled the
norm of mafter every update (slow type SGD [29]), and compare the dynamical ϵ1
gwith
its equilibrium counterpart (with the same value of q∗). We find that the SGD results
are comparable with the equilibrium predictions, at least qualitatively [Fig. 3 (d)]. The
deviation may be caused by the finite size effects (see also the previous work [29]).
In particular, Fig. 3 (b) reveals a continuous phase transition (from poor to perfect gener-
alization) in the variational parameter space (despite a binary perceptron learning considered
in our setting). Due to the numerical accuracy of the replica results at a large α, we find that
after α= 1.7, a power law scaling of the generalization error with a large exponent [ ∼13.1,
see the inset of Fig. 3 (b)] is observed. We remark that the SGD could reach zero error
(perfect generalization) after αc≃1.7, while the replica result obtained at a large β= 20
has a fast decay (lower than 10−2) after αc[see the inset of Fig. 3 (b)]. It is expected that the
replica prediction of the generalization error will reach lower values with increasing β, which
requires a huge number of Monte-Carlo samples to get an accurate estimate of the integral
in Eq. (15) and also in solving the saddle-point equations (see details in appendix C). The
power-law fitting for the error below 10−3may thus be unreliable. We conclude that in the
zero temperature limit, the perfect generalization is conjectured to be achievable, although
a high βmay lead to replica symmetry breaking [29]. We could alternatively estimate the
transition threshold by analyzing the convergence time of the learning algorithm [Fig. 3
(c)]. The convergence time is peaked at αc≃1.7, which is in stark contrast to the case of
training in the direct binary-weight space [26, 27], which leads to a discontinuous transition
atαc= 1.245 (a spinodal point locates at αsp= 1.492). This suggests that the variational
learning erases the metastable regime where the poor generalization persists until the spin-
odal point. Thus the variational framework bears optimization benefits for learning in neural
networks with discrete weights.
13

--- PAGE 14 ---
B
DC
EFIG. 3: Mean-field results of the first-task learning compared with SGD simulations. (a) The order
parameters versus data density α(β= 20). (b) Generalization error versus data density ( β= 20).
The connected symbols represent the result of SGD. The inset shows a power-law scaling for replica
results when α≥1.7. The symbol (Data) in the inset is the replica theory. (c) Convergence time
stepTcof the full batch gradient descent simulation. Tcis recorded when the drop of ϵgduring
[Tc, Tc+ 1000] starts to be less than 0 .0005. The convergence time peaks at α= 1.7. The results
are averaged over 20 trials (network size N= 3 000). (d) Generalization error versus quenched q∗
(α= 2). The slow SGD means a rescale of the norm of mtoq⋆after each update. The results of
SGD are averaged over 20 trials (network size N= 1 000).
2. The second-task analysis
Similarly, we specify the second-task Hamiltonian as follows,
L2(m) =−M2X
µ=1lnH 
−sign(P
iW2
ix2,µ
i)P
imix2,µ
ipP
i(1−m2
i)!
+NX
i=1KL(Qmi∥Qm1,i). (16)
14

--- PAGE 15 ---
As explained before, the first term is a reconstruction term that maximizes the log-likelihood
of the second-task data, and the second term prevents the network from forgetting the previ-
ously acquired knowledge. Due to the regularization term, we have to treat the equilibrium
analysis differently from the single task learning. It is natural to put the analysis within the
Franz-Parisi potential framework [22, 23]. More precisely, we define the potential for the
second-task learning as
Φ =1
˜ZZ
˜ΩNY
i=1d ˜mie−˜βL1(˜m)lnZ
ΩNY
i=1dmie−βL2(m,˜m). (17)
Taking the replica symmetric (RS) Ans¨ atz, the disorder averaged potential is related to the
following action S(see details in appendix C),
S= lim
n→0lim
s→0−1
2
nˆ˜qd˜qd+s(s−1)ˆ˜q0q0
−1
2(sˆqdqd+n(n−1)ˆq0q0)−nˆ˜r1˜r1−sˆr2r2
+ lnGS+α1lnG1
E+α2lnG2
E,(18)
where
G1
E=*nY
a=1H˜β
−sign(˜v1)˜ua
√1−˜qaa+
=Z
Dz2H 
−˜r1p
˜q0−˜r2
1z!Z
Dσ H˜β
−√˜qd−˜q0σ+√˜q0z√1−˜qdn
,(19)
and
G2
E=*sY
c=1Hβ
−sign(v2)uc
√1−qcc+
=Z
Dz2H 
−r2p
q0−r22z!Z
Dσ Hβ
−√qd−q0σ+√q0z√1−qds
,(20)
and
GS=1 +r0
2Z
Dz1Z+1
−1d ˜m e˜I( ˜m,z1)n−1Z+1
−1d ˜m e˜I( ˜m,z1)Z
Dz2Z+1
−1dm eJ+(m,˜m,z2)s
+1−r0
2Z
Dz1Z+1
−1d ˜m e˜I( ˜m,z1)n−1Z+1
−1d ˜m e˜I( ˜m,z1)Z
Dz2Z+1
−1dm eJ−(m,˜m,z2)s
.
(21)
15

--- PAGE 16 ---
Note that, ˜ v1andv2are related to quenched disorder (see appendix C), {q0, r2, qd}are order
parameters in parallel to the first-task learning, while {˜q0,˜r1,˜qd}can be obtained by solving
the single-task saddle point equations (inherited from the first-task analysis). The functions
˜IandJ±are defined in appendix C. The test error of the second task can be derived in
the form,
ϵ2
g=Z
Dz2H 
−p2p
1−p2
2z!
Θ(−z) =1
πarccos( p2), (22)
where p2=1
NP
isign(m)W2
idenote the overlap between the decoded weights and the
second-teacher weights. Similarly, the test error of the first-task after learning both tasks is
given by
ϵ1
g=Z
Dz2H 
−p1p
1−p2
1z!
Θ(−z) =1
πarccos( p1), (23)
where p1=1
NP
isign(mi)W1
i.
We finally study the theoretically predicted performances compared with numerical sim-
ulations. In Fig. 4 (a), we find that the task similarity strongly impacts the learning per-
formance of the second task. When r0takes a negative value, the learning becomes much
harder, as more data examples are required to decrease the generalization error, while a pos-
itive task similarity makes the learning of the second task easier. The SGD results match
well with the theoretical prediction, except for the region around the transition, which may
call for longer simulation time in searching for good solutions. As expected, the general-
ization of the first task will increase during learning the second task [Fig. 4 (b)], which
is due to the fact that both tasks share a partial similarity (i.e., not completely the same).
We also multiply the KL term by a factor γ, and study the effect of this term by tuning
down this factor [e.g., γ= 0.1 in Fig. 4 (c,d)]. We find that the learning of the second task
becomes fast as less data examples are required, and the critical value of αis also impacted.
Furthermore, the memorization of the first task is strongly degraded. This result is consisted
with that found in Fig. 2 (d).
For a numerical verification of the mean-field replica theory, we train a perceptron with
the number of synapses N= 5000. The learning rate equals to 0 .001 for the whole training
process. If we use SGD, the size of a mini-batch is set to 32. In the replica analysis, the
hyperparameter β1=β2= 20 for the first and second tasks. Once the algorithm for the
present task converges (e.g., the accuracy is stable), we shift the learning to a new task.
16

--- PAGE 17 ---
a b
c dFIG. 4: Generalization versus αfor the variational continual learning. The symbols connected
by dashed lines are simulation results of GD (twenty trials are averaged), while those connected
by full lines are replica predictions. α1= 2, N= 1 000, and different learning rates are used for
different values of α2. Different task similarities are considered. The KL term is multiplied by a
tuning factor γ. (a,b) γ= 1.0. (c,d) γ= 0.1.
Figure 5 shows an excellent agreement between equilibrium predictions obtained by replica
analysis and real training of perceptrons.
III. CONTINUAL LEARNING IN DEEP NEURAL NETWORKS
Catastrophic forgetting is an unfavored property for deep neural networks applied to
continual learning or multi-task learning. In this section, we extend the variational methods
for the toy binary perceptron to deep neural networks in classifying structured dataset.
17

--- PAGE 18 ---
a bFIG. 5: The comparison between replica results and simulation in perceptron. The simulation
results are averaged over five independent trials. The solid line shows the accuracy obtained from
training the perceptron, while the symbols indicate the replica results. The dashed line indicates
the task switch. (a) r0= 0.6, and α1=α2= 3.0. (b) r0= 0.5,α1= 4.0, and α2= 3.0.
A. Variational learning principle
Variational learning principle is a popular variational Bayesian framework applied in
a wide range of scenarios [12–15], which focus mainly on deep networks with real-valued
weights. To learn a computationally efficient (binary weights) deep network, we adapt the
variational principle to the continual learning, in theory and practical training, comparing
the performance with that of the heuristic metaplasticity algorithm [8], a unique available
method for comparison in our current context. Within this framework, the posterior of
parameters wis learned from Tcontinually presented datasetsn
x(n)
t,y(n)
toNt
n=1, where t
denotes the task index ranging from 1 to T, and Ntdenotes the size of dataset t. When the
multi-task data examples are sequentially shown to the machine, the posterior distribution
ofwis denoted as p(w|Dk), after k-th training steps based on the dataset Dk(minibatch at
thek-th step), can be calculated using the Bayes’ rule as p(w|Dk) =p(Dk|w)p(w)
p(Dk). The prior
p(w) depends on the ( k−1)-th step, which can be taken to be the posterior in the previous
training step p(w(k−1)|Dk−1) . Taken together, the posterior p(w| D) can be written as
p 
w|Dk
=p 
Dk|w
p 
w(k−1)|Dk−1
p(Dk). (24)
Unfortunately, the difficulty here is that the posterior is typically intractable for most of
probabilistic models, which thereby requires an application of the variational method. We
approximate the true posterior with a tractable distribution parameterized by the variational
18

--- PAGE 19 ---
parameter θ. By updating θ, we approach the target distribution as close as possible.
Given a simple trial probability distribution over the latent variable wparameterized by
θ, i.e., qθ(w), the minimization of the KL divergence between qθ(w) and p(w|D) results in
the following solution
θ∗= arg min
θKL [qθ(w)∥p(w|D)]. (25)
Therefore, we can define the loss function Las
L= KL [ qθ(w)∥p(w|D)] =Lreg+Lrec, (26)
where we replace the prior p(w) with the variational posterior in the previous time step
qθk−1(w), and we define the regularization term Lreg= KL [ qθk(w)∥qθk−1(w)], and the re-
construction term Lrec=−Eqθk(w)[lnp(D|w)]. The term Lreccomputes the averaged log-
likelihood of the network output, which can be crudely approximated by considering a single
sample wsofqθk(w) from a rough Monte-Carlo sampling estimation. By computing gradi-
ents of θon this loss function L, we arrive at
∂L
∂θk
i=X
wi∂qθk
i(wi)
∂θk
i
1−lnqθk
i(wi)−lnqθk−1
i(wi)
−∂lnp(D |ws)
∂θk
i.(27)
Learning of the variational parameter θican be achieved by a gradient descent of the objec-
tive function, i.e.,
θk+1
i=θk
i−η∂L
∂θk
i, (28)
where ηdenotes the learning rate, and θk
irefers to the i-th connection in one layer a deep
network (e.g., θl,k
ijfor the connection ( ij) at layer lbelow).
We consider a deep neural network with Llayers, and Nldenotes the width of lth layer.
wl
ijindicates the weight connecting neuron iat the upstream layer lto neuron jat the
downstream layer l+ 1. The state of neuron jat the l+ 1th layer hl+1
jis a non-linear
transformation of the preactivation zl+1
j=1√NlP
iwl
ijhl
i. The transfer function f(·) for layers
l= 1,2, . . . , L −1 is chosen to be the rectified linear unit (ReLU), which is defined as f(z) =
max(0 , z). For the output layer, the softmax function hk=ezk/P
ieziis used, specifying
19

--- PAGE 20 ---
the probability over all classes of the input images, where ziis the preactivation of neuron
iat the output layer. The supervised learning is considered, where ˆhkindicates the target
ofhL
k, and the cross entropy Lce=−P
iˆhilnhiis used as a cost function corresponding to
Lrec. In our setting, a double-peak distribution is applied to model the binary weight as
qθl
ij 
wl
ij
=eβwl
ijθl
ij
eβθl
ij+e−βθl
ij, where βis a hyperparameter, and the field-like parameter θijcontrols
the probability distribution of wijasqθl
ij(+1) =eβθl
ij
eβθl
ij+e−βθl
ijandqθl
ij(−1) =e−βθl
ij
eβθl
ij+e−βθl
ij.
Therefore, the gradients of θonLregcan be computed as
∂KLh
qθl,k
ij(wl,k
ij)∥qθl,k−1
ij(wl,k−1
ij)i
∂θl,k
ij
=β2
θl,k
ij−θl,k−1
ij
σl,k
ij2
=β2
σl,k
ij2
∆l,k
ij,(29)
where the superscript landkdenote the layer index and iteration step respectively, and we
define ∆l,k
ijas the increments of the variational parameter ∆l,k
ij=θl,k
ij−θl,k−1
ij between two
successive steps.
σl,k
ij2
indicates the variance of wl
ijas
σl,k
ij2
= 1−tanh2
βθl,k
ij
, and
thus captures the synaptic uncertainty.
To derive the gradients of Lrec, we apply the mean-field method [28]. The first and
second moments of wl
ijare given by µl
ij=⟨wl
ij⟩= tanh 
βθl
ij
and 
σl
ij2= 1− 
µl
ij2,
respectively. Given that the width of layer is large, the central-limit theorem indicates that
the preactivation zl+1
jfollows a Gaussian distribution N(z|ml+1
j;vl+1
j), where the mean and
variance are given below,
ml+1
j=⟨zl
i⟩=1√NlX
jµl
ijhl
i
 
vl+1
j2=⟨ 
zl+1
j2⟩ − ⟨zl+1
j⟩2=1
NlX
j 
σl
ij2 
hl
i2.(30)
Therefore we write the preactivation as zl
i=ml
i+ϵl
ivl
i, where ϵl
idenotes a standard Gaussian
variable relying on the layer and weight-component index. Then, we can compute the
gradients as follows,
∂Lrec
∂θl
ij=∂Lrec
∂zl+1
j∂zl+1
j
∂θl
ij=Kl+1
j 
∂ml+1
j
∂θl
ij+ϵl+1
j∂vl+1
j
∂θl
ij!
, (31)
20

--- PAGE 21 ---
where we have defined Kl+1
j=∂Lrec
∂zl+1
j, which could be solved using the chain rule. The term
∂ml+1
j
∂θl
ijand∂vl+1
j
∂θl
ijcan be directly derived from Eq.(30), as shown below,
∂ml+1
j
∂θl
ij=1√Nlβhl
i 
σl
ij2,
∂vl+1
j
∂θl
ij=−β 
hl
i2
Nlvl+1
jµl
ij 
σl
ij2,(32)
andKl
ican be estimated by the chain rule from the value at the top layer, i.e.,
Kl
i=X
jKl+1
j
1√Nlµl
ij+ϵl+1
j
Nlq 
vl+1
j2 
σl
ij2hl
i
f′ 
zl
i
, (33)
where f′(·) is the derivative of transfer function, and on the top layer, KL
ican be directly
estimated as KL
i=−ˆhi(1−hL
i). Taken together, the total gradients on the loss function L
take the form as
∂L
∂θl,k
ij=β
σl,k
ij2
β∆l,k
ij+δl,k
ij
, (34)
where we add the step index kasδl,k
ij=Kl+1
j
1√Nlhl
i−ϵl+1
j
Nlvl+1
j 
hl
i2µl
ij
. It can be clearly
seen that the variance of wl
ijat the iteration step ktogether with the inverse temperature
β
σl,k
ij2
tunes the learning rate η, where a larger variance leads to larger gradients in the
iteration step k. In addition, the regularization term β∆l,k−1
ij measures the similarity be-
tween the variational posterior probabilities across successive learning steps, which regulates
the distance from current guess to previous one, providing a principled way to use the in-
formation from previous task knowledges. Therefore, this variational continual learning can
be used in scenarios where task boundaries are not available [13], which is also more cogni-
tively plausible from our humans’ learning experiences. Hereafter, we call this variational
continual learning scheme as VCL.
We emphasize the relationship between the VCL used in toy model analysis in previous
section and that used for practical continual deep learning in this section. In essence, the
VCL in these two sections bears the same principle [see Eq. (26)]. In the toy model analysis,
we specify the task boundary, which allows us to derive the Franz-Parisi potential of the
continual learning. However, in a practical training, a task agnostic training is favored (like
21

--- PAGE 22 ---
humans), which is exactly captured in Eq. (34). Therefore, the last training step acts as a
reference in the language of the Franz-Parisi framework, i.e., the learning of the next step can
be described by an equilibrium system with an anchored external preference. Furthermore,
in the toy model analysis, we set the hyperparameter βfor both tasks the same value. In
the practical deep learning, βis allowed to increase with epoch [one example is shown in
Fig. 7 (a)].
In particular, our learning protocol emphasizes how synaptic uncertainty tunes the contin-
ual learning, and thus provides a principled way to understand engineering heuristics [15, 16]
and neuroscience inspired heuristics [6, 8, 9]. For the deep networks with binary weights,
the previous work uses discretization operation of a continuous weight, surrogate gradient
and a metaplasticity function (see details in Appendix A), while our VCL does not require
these tricks. In addition, other heuristic strategies such as elastic weight consolidation and
its variants [9] can be also unified in our current framework. For example, the part Lregcan
be approximated by a term involving the Fisher information matrix as
F(θ) =Eqθ(w) ∂lnqθ(w)
∂θ∂lnqθ(w)
∂θ⊤!
. (35)
and then
Lreg≈1
2 
θk−θk−1⊤F 
θk−1 
θk−θk−1
, (36)
where we take only the diagonal elements of the Fisher information matrix F 
θk−1
≈
β2(σk−1)2, recovering the elastic weight consolidation algorithm. The technical proof is
given in appendix B.
B. Learning performance and roles of synaptic uncertainty
In this section, we compare the performance of VCL and the metaplasticity algorithm
for neural networks with binary weights. Algorithmic details are given in appendix A. We
consider two tasks first— sequential learning of MNIST and Fashion-MNIST (f-MNIST)
datasets [8], and this setting requires the network to sequentially learn from two datasets:
MNIST and f-MNIST. We next consider a popular continual learning benchmark, namely
the permuted MNIST learning task [9]. The permuted MNIST learning task is composed
22

--- PAGE 23 ---
010 20 30 40 50 60 70 80
Epoch020406080Accuracy %f-MNIST
MNIST
f-MNIST (meta)
MINST (meta)
010 20 30 40 50 60 70 80
Epoch20406080100Accuracy %f-MNIST
MNIST
f-MNIST (meta)
MINST (meta)FIG. 6: Continual learning on MNIST and Fashion-MNIST (f-MNIST) datasets. The network has
the architecture [784 ,400,200,10], each number indicates the layer width. The results are averaged
over five independent trials. (Left panel) The training order is f-MNIST first and then MNIST.
β1= 1 for the first task and β2= 12.5 for the second one in the VCL setting. m1= 0.5 for the
first task and m2= 0.9 for the second in the metaplasticity algorithm. (Right panel) MNIST is
applied first followed by the f-MNIST dataset. The same network architecture is used as (a), while
β1= 1,β2= 17.5,m1= 0.5 and m2= 0.7.
a b
FIG. 7: Continual learning of permuted MNIST learning tasks. Five sequential tasks are considered,
and each is trained for 40 epochs. The network has the architecture [784 ,512,512,10], each number
indicates the layer width. The results are averaged over five independent trials. (a) Test accuracy
based on VCL, βℓ=atanh( δ+bℓ/M ), where ℓdenotes the epoch index, and Mis the total
number of epochs. We use a= 10.0,δ= 0.1, and b= 2.0. (b) Test accuracy of the metaplasticity
algorithm, for which m= 0.43 for all the tasks.
of continual learning of several datasets, and each task contains labeled images of a fixed
random spatial permutation of pixels.
Figure 6 shows that for both training orders (f-MNIST first or MNIST first), VCL achieves
a much better performance than that of the metaplasticity algorithm (a shorthand as meta),
showing the benefit of less forgetting of learned tasks and thus better performance for new
coming tasks. The same phenomenon can be also observed in Fig. 7, where five permuted
23

--- PAGE 24 ---
FIG. 8: The averaged level of synaptic uncertainty ( ⟨σ⟩) evolves through training for all the layers,
and the inset shows the details of the training stage after 40 epochs. The network architecture is
the same as that of Fig. 7.
MNIST datasets are sequentially presented to the network. Networks trained with VCL
[Fig. 7 (a)] are learning better and forgetting less previous task knowledge compared with
those trained with meta [Fig. 7 (b)]. The plot shows the classification accuracy for task t
after learning tasks t′≥t. A perfect continual learning must provide high accuracy for task
t, and moreover preserve the performance even when subsequent tasks are learned, which
is independent of the training time of each task (e.g., increasing the training time to 100
epochs).
We also plot the evolution of synaptic uncertainty by calculating the average ⟨σ⟩=
1
#weightsP
(ij)σ2
ijfor every epoch and every layer. As expected, the mean uncertainty de-
creases during the continual learning. However, the level rises with a minor magnitude after
a task switch, but then drops again. In addition, the reduction of the uncertainty is also evi-
dent for upstream layers, indicating that these layers tend to freeze most of weights, or make
them less plastic. In contrast, the last layer maintain a low level of synaptic uncertainty for
reading out the key category information. We also observe that the synaptic plasticity with a
larger uncertainty has a larger contribution to how strong the KL divergence should change,
which thereby plays an important role in minimizing the overall objective. To conclude,
the synaptic variance is a key quantity determining the behavior of continual learning. The
VCL can adjust the synaptic resources during sequentially learning of multiple tasks.
24

--- PAGE 25 ---
IV. CONCLUSION
In this study, we focus on the continual learning in deep (or shallow) neural networks
with binary weights. Recent works already argued that the variational training is effec-
tive in neural networks of real-valued weights [13–15], and a brain-inspired metaplasticity
method is also effective in training binary neural networks [8]. However, how to unify these
diverse strategies within a statistical physics model is challenging. Here, we propose a vari-
ational mean-field framework to incorporate synaptic uncertainty, task-knowledge transfer
and mean-field potential for multi-task learning. First, we argue that the synaptic uncer-
tainty plays a key role in modulating continual learning performance, through the lens of
variational weight distribution. Specifically, the synaptic variance becomes a modulating
factor in the synaptic plasticity rules, based on our theory. Second, the task-knowledge
transfer can be interpreted in physics. The knowledge from the previous task behaves as a
reference configuration in the Franz-Parisi potential formula [22, 23], an anchor for learning
new knowledge. The learning of new task can thus be described by an equilibrium sys-
tem with an anchored external preference. The derived theory matches well the numerical
simulations using stochastic gradient descent algorithms.
Our theory of variational continual learning also predicts that a single-task learning ex-
hibits a continuous transition with increasing amount of data (sample complexity), which is
in stark contrast to the previous findings in mean-field theory of generalization (in the direct
discrete or continuous weight space) [26, 27]. This new theoretical prediction suggests that
the current variational continual learning proves efficient in practical learning, since a trap-
ping by metastable states is absent. We remark that this absence of a first-order transition
holds only for shallow networks. It is thus interesting to extend our theoretical analysis to
multi-layered networks to see if this conclusion is present or not.
We finally demonstrate that our framework can be applied to continual learning of real
datasets, achieving similar or even better performances with those obtained by heuristic
strategies, such as metaplasticity. Therefore, this work can be a promising starting point to
explore further the important yet challenging question of how to build theoretically-grounded
neural representation that helps an intelligent agent avoid catastrophic forgetting and adapt
continuously to new tasks, based on accumulated knowledges from previous tasks.
25

--- PAGE 26 ---
Appendix A: Algorithmic details
In this section, we provide the details of metaplasticity algorithm and VCL, which are
compared in the main text. The pseudocode of the metaplasticity algorithm [8] is summa-
rized in Algorithm 1.
Algorithm 1 Metaplasticity continual learning
1:Input: hl−1
j,wl
ij= sign( al
ij); meta parameter m; learning rate η
2:Feedforward propagation: zl
i=1√
Nl−1P
jwl
jihl−1
j, hl
i=f 
zl
i
, f(x) = ReLu ( x);
3:Backpropagation:∂L
∂wl
ij=∂L
∂zl
j∂zl
j
∂wl
ij=
1√
Nl−1P
k∂L
∂zl+1
kwl+1
jkf′ 
zl
j
hl−1
i;
4:Parameter update: 
al
ijk+1− 
al
ijk=−η
1−ζtanh2(m 
al
ijk)
∂L
∂wl
ij, where ζ=
1
2"
sign 
wl
ij∂L
∂wl
ij!
+ 1#
.
In the meta algorithm, Ldenotes the loss function, and al
ijdenotes the latent real-
valued weights underlying binary counterpart, and the core idea is the introduction of a
modulation function fmeta(m, x) = 1−tanh2(mx) which is a decreasing function of |x|(or
the absolute value of the hidden weights). This modulation called metaplasticity makes the
hidden weight change less likely if the corresponding magnitude are growing (consolidation
of some useful information, expressed as the ζfactor). Therefore, this metaplasticity can be
heuristically thought of as a sort of weight consolidation. In contrast, our VCL gives rise
to an alternative modulation related to the synaptic uncertainty, thereby bearing a more
solid theoretical ground. In addition, the gradient with respect to a discrete weight value
in the meta algorithm is ill-defined, which does not appear in our VCL. We remark that
this algorithm is sensitive to the network size; if the size of network is not big enough, this
algorithm may fail to give satisfied learning performance.
Our VCL algorithm is summarized in the pseudocode 2. In the main text, we use learning
rate—0 .01 and mini-batch size—64 for both tasks. Note that the mean and variance of the
preactivation zis computed for each single data sample, given the statistics of the weight.
All codes to reproduce our results in the main text are available at the Github link [32].
26

--- PAGE 27 ---
Algorithm 2 VCL algorithm
1:Input: single sample x∈RN0,wl∈RNl×Nl+1from the distribution qθl
ij 
wl
ij
=
eβwl
ijθl
ij
eβθl
ij+e−βθl
ij;
2:Compute the mean and variance of elements wl:µl, 
σl2;
3:Compute the mean mland variance 
vl2of preactivation zl=1√Nl(wl−1)⊤hl−1,h0=x;
4:Sample ϵl∈RNlindependently from N ∼ (0,1);
5:Output: zl= (ml+ϵl⊙vl),hl=f 
zl
;
6:Parameter update: θl,k+1
ij = θl,k
ij−η∂L
∂θl,k
ij, where∂L
∂θl,k
ij=
β
σl,k
ij2
β∆l,k−1
ij+Kl+1
j
1√Nlhl
i−ϵl+1
j
Nlvl+1
j 
hl
i2µl
ij
, and ∆l,k−1
ij=θl,k−1
ij−θl,k−2
ij.
Appendix B: Connection to elastic weight consolidation
In this section, we provide a proof of the elastic weight consolidation as a special example
of VCL. We first write D 
θk,θk−1
≡ L reg= KL [ qθk(w)∥qθk−1(w)], and then we have
D 
θk,θk−1
=Z
q(w;θk) lnq(w;θk)
q(w;θk−1)dw, (B1)
where the integral can be interpreted as the summation for the considered discrete weight
variable. Hereafter, we write qθk(w) =q(w;θk). We next assume the two consecutive
solutions are sufficiently close, i.e., θk≈θk−1, orθk=θk−1+ ∆θ(∆θ→0), and then
we consider alternatively D 
θk−1,θk
because of small ∆ θ, and further expand ln q(w;θk)
around θk−1up to the second order,
D 
θk−1,θk
=Eq(w;θk−1)[lnq(w;θk−1)−lnq(w;θk)],
≈ − 
θk−θk−1⊤Eq(w;θk−1)∂lnq(w;θk−1)
∂θk−1
−1
2 
θk−θk−1⊤Eq(w;θk−1)∂2
∂(θk−1)2lnq(w;θk−1) 
θk−θk−1
.(B2)
27

--- PAGE 28 ---
We also find that
Eq(w;θk−1)∂lnq(w;θk−1)
∂θk−1=Z
q(w;θk−1)1
q(w;θk−1)∂q(w;θk−1)
∂θk−1dw,
=∂
∂θk−1Z
q(w;θk−1)dw,
=∂1
∂θk−1= 0.(B3)
Notice that Eq
∂2lnq
∂(θk−1)2
=Eq
1
q∂2q
∂(θk−1)2− 
∂lnq
∂θk−1!2
, where qrepresents q(w;θk−1), and
we have used∂2R
dwq
∂(θk−1)2= 0, we finally arrive at
D 
θk,θk−1
=1
2 
θk−θk−1⊤F(θ) 
θk−θk−1
, (B4)
where D(θk,θk−1)≃ D(θk−1,θk) when ∆ θ→ 0, and F(θ) is ex-
actly the Fisher information matrix whose definition is given by F(θ) =
Eq(w;θk−1)[ ∂
∂θk−1lnq(w;θk−1) ∂
∂θk−1lnq(w;θk−1)⊤]. To conclude, when we take only
the diagonal elements of the Fisher information matrix, we recover the elastic weight
consolidation algorithm [9].
Appendix C: Details for replica computation
In this section, we demonstrate how to predict the generalization errors of variational
continual learning by replica computation. First, we summarize our problem settings: we
consider a teacher-student continual learning problem on binary perceptron, where the stu-
dent learns task 1 first and then task 2. For the data in these two tasks x1andx2, the labels
are given by two teachers, y1= sign(P
iW1
ix1
i) and y2= sign(P
iW2
ix2
i). Note that, each
dimension of the input data follows a uniform Bernoulli distribution, xt,µ
i∈[−1,+1] and
the training datasets of the two tasks consider different realizations, Dt={xt,µ, yt,µ}Mt
µ=1,
where t= 1,2. There is also a correlation between these two teachers, described by an
overlap r0=1
NP
iW1
iW2
i. Therefore, the joint distribution of teachers’ weights can be
28

--- PAGE 29 ---
parameterized as,
P0(W1
i, W2
i) =1 +r0
4δ(W1
i−W2
i) +1−r0
4δ(W1
i+W2
i), (C1)
where we use the notation—teacher-average to denote the average over such distribution.
For the sake of convenience, we assume that during the learning of a certain task, the loss
function is fixed. According to the variational theory in the main text, the loss functions
for the continual learning are listed as follows,
L1(m) =−M1X
µ=1lnH 
−sign(P
iW1
ix1,µ
i)P
imix1,µ
ipP
i(1−m2
i)!
,
L2(m) =−M2X
µ=1lnH 
−sign(P
iW1
ix2,µ
i)P
imix2,µ
ipP
i(1−m2
i)!
+NX
i=1KL(Qmi∥Qm1,i).(C2)
where H(x) =1
2erfc
x√
2
andm1is the trained weight after learning task 1. In the
following, the learning procedures of task 1 and task 2 will be called as single-task learning
and multi-task learning respectively, which actually reflects the learning’s essence.
To predict the generalization errors in both learning scenarios, we apply replica method
under the replica symmetry Ans¨ atz. For a specific learning scenario, the derivations can
be unfolded in two steps: First, we treat the loss function for gradient-descent training as
the Hamiltonian in canonical ensemble and compute its averaged free energy, which entails
the replica trick; Second, with the knowledge of the free energy, we show how to obtain
the generalization errors of both tasks. In the following, the value of Mcan be M1orM2
depending on the learning stage.
1. Thermodynamic system for single-task learning
In this scenario, the thermodynamic system can be defined by a partition function,
Z=Z
ΩNY
i=1dmie−βL1(m)=Z
ΩNNY
i=1dmiM1Y
µ=1Hβ 
−sign(P
iW1
ix1,µ
i)P
imix1,µ
ipP
i(1−m2
i)!
,(C3)
where Ω = [ −1,+1]. Our goal is to compute the quenched average of the free energy ⟨lnZ⟩
over the dataset D1and teacher average. We can first remove the average over P(W1) by
29

--- PAGE 30 ---
performing a gauge transformation: x1,µ
i→W1
ix1,µ
i,mi→W1
imi. The result can be seen as
setting W1
i= 1,∀i. Then, we apply replica trick, ⟨lnZ⟩= lim n→∞ln⟨Zn⟩
n, which requires to
compute the replicated partition function,
⟨Zn⟩=Z
ΩnNnY
a=1NY
i=1dma
i*nY
a=1M1Y
µ=1Hβ 
−sign(P
iW1
ix1,µ
i)P
ima
ix1,µ
ipP
i1−(ma
i)2!+
. (C4)
Now, we introduce the local field,
ua=P
ima
ix1
i√
N, v 1=P
iW1
ix1
i√
N, (C5)
where the data index µis omitted in advance. Note that, the statistics of local fields stem
from the data distribution. According to central limit theorem, local fields should obey joint
Gaussian distribution in the thermodynamics limit N→ ∞ . Thus, we have the following
statistics
⟨ua⟩= 0,⟨v1⟩= 0. (C6)
In addition,
⟨uaua⟩ − ⟨ua⟩⟨ua⟩=P
ima
ima
i
N,
⟨uaub⟩ − ⟨ua⟩⟨ub⟩=P
ima
imb
i
N,
⟨v1ua⟩ − ⟨v1⟩⟨ua⟩=P
iW1
ima
i
N,(C7)
where order parameters qab=P
ima
imb
i
N,qaa=P
ima
ima
i
N,r1
a=P
iW1
ima
i
Nnaturally appear. We
enforce the definitions of these order parameters to the replicated partition function by the
Fourier Integral of Dirac delta functions,
δ X
ima
imb
i−qabN!
=Z1
2πieˆqab(PN
ima
imb
i−qabN)dˆqab,
δ X
ima
ima
i−qaaN!
=Z1
4πie1
2ˆqaa(PN
ima
ima
i−qaaN)dˆqaa,
δ X
iW1
ima
i−r1
aN!
=Z1
2πieˆr1
a(PN
iW1
ima
i−r1
aN)dˆr1
a,(C8)
30

--- PAGE 31 ---
and then we obtain,
⟨Zn⟩=ZY
adˆr1
adr1
a
2πi/NY
adˆqaadqaa
4πi/NY
a<bdˆqabdqab
2πi/Ne−NP
a<bˆqabqab−1
2NP
aˆqaaqaa−NP
aˆr1
ar1
a
Z
ΩnnY
a=1NY
i=1dma
ieP
a<bˆqabP
ima
imb
i+1
2P
aˆqaaP
ima
ima
ieP
aˆr1
aP
ima
i*M1Y
µ=1nY
a=1Hβ
−sign(v1)ua
√1−qaa+
(C9)
Note that we introduce a prefactor 1 /√
Nin the summations in Eq. (C3), which does not
affect the result. Here, we consider the replica symmetry Ans¨ atz: qab=q0,ˆqab= ˆq0, qaa=
qd,ˆqaa= ˆqd, r1
a=r1,ˆr1
a= ˆr1. Next, we will define and compute three terms separately and
put them together in the final expression of the free energy:
The first term is the interaction term GI,
GI=−1
2X
a,bˆqabqab−X
aˆr1
ar1
a
=−1
2 X
aˆqaaqaa+X
a̸=bˆqabqab!
−nˆr1r1
=−1
2(nˆqdqd+n(n−1)ˆq0q0)−nˆr1r1.(C10)
The second contribution is the entropy term GS,
GS=Z
[−1,1]nY
admae1
2P
abˆqabmamb+ˆr1P
ama,
=Z
[−1,1]nY
admae1
2ˆqdP
amama−1
2ˆq0P
amama+1
2ˆq0(P
ama)2+ˆr1P
ama,
=Z
[−1,1]nY
admae1
2ˆqdP
amama−1
2ˆq0P
amama+ˆr1P
amaZ
Dz e√ˆq0P
amaz,
=Z
DzZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1mn
.(C11)
Finally, we compute the energy term GE,
GE=*Y
aHβ
−sign(v1)ua
√1−qd+
, (C12)
where ⟨·⟩denotes the average over the joint distribution of the local fields ( ua,v1). Based
on their statistics, ⟨uaua⟩=qd,
uaub
=q0,⟨v1ua⟩=r1,⟨v1v1⟩= 1, they can thus be
31

--- PAGE 32 ---
parametrized as
ua=√qd−q0σa+√q0z,
v1=r1√q0z+s
1−r2
1
q0y,(C13)
where σa,z,yare all standard Gaussian variables. Substituting Eq. (C13) into the energy
term arrives at
GE=*Y
aHβ
−sign(v1)ua
√1−qd+
=Z
DzZ
DyY
aZ
DσaHβ
−sign
r1√q0z+q
1−r2
1
q0y √qd−q0σa+√q0z
√1−qd

=Z
DzZ
Dy
Z
Dσ Hβ
−sign
r1√q0z+q
1−r2
1
q0y √qd−q0σ+√q0z
√1−qd

n
=Z
Dz"
H 
−r1p
q0−r2
1z!Z
Dσ Hβ
−√qd−q0σ+√q0z√1−qdn
+
 
1−H 
−r1p
q0−r2
1z!!Z
Dσ Hβ√qd−q0σ+√q0z√1−qdn#
=Z
Dz2H 
−r1p
q0−r2
1z!Z
Dσ Hβ
−√qd−q0σ+√q0z√1−qdn
.
(C14)
Note that Dzindicates the standard Gaussian measure.
Finally, under the replica symmetry Ans¨ atz, the replicated partition function can be
written as
⟨Zn⟩=ZY
adˆr1
adr1
a
2πi/NY
adˆqaadqaa
4πi/NY
a<bdˆqabdqab
2πi/Ne−Nnf RS. (C15)
Then, under the saddle-point approximation in the large Nlimit, the free energy density is
given by
−βfRS= lim
n→0,N→∞ln⟨Zn⟩
nN= lim
n→0−1
2(ˆqdqd+ (n−1)ˆq0q0)−ˆr1r1+lnGS
n+α1lnGE
n.(C16)
The free energy should be optimized with respect to the order parameters, and thus we have
32

--- PAGE 33 ---
to derive the corresponding saddle-point equations through setting the gradients zero. We
first compute gS= lim n→0lnGS
nandgE= lim n→0lnGE
n,
gS= lim
n→0lnGS
n= lim
n→01
nZ
DzZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1mn
=Z
DzlnZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1m
.
gE= lim
n→0lnGE
n= lnZ
Dz2H 
−r1p
q0−r2
1z!Z
Dσ Hβ
−√qd−q0σ+√q0z√1−qdn
=Z
Dz2H 
−r1p
q0−r2
1z!
lnZ
Dσ Hβ
−√qd−q0σ+√q0z√1−qd
.
(C17)
Thus, the saddle-point equations can be expressed as,
qd= 2∂gS
∂ˆqd, q 0=−2∂gS
∂ˆq0, r 1=∂gS
∂ˆr1;
ˆqd= 2α1∂gE
∂qd,ˆq0=−2α1∂gE
∂q0,ˆr1=α1∂gE
∂r1.(C18)
To make the expressions more compact, we define a probability measure,
⟨⟨O⟩⟩H(m)=R+1
−1OeH(m)dm
R+1
−1eH(m)dm. (C19)
Then, we have,
qd= 2Z
DzR+1
−11
2m2eI(m,z)dm
R+1
−1eI(m,z)dm=Z
Dz

m2
I(m,z),
q0=−2Z
DzR+1
−1
−1
2m2+z
2√ˆq0m
eI(m,z)dm
R+1
−1eI(m,z)dm=Z
Dz
m2−z√ˆq0m
I(m,z),
r1=Z
DzR+1
−1m eI(m,z)dm
R+1
−1eI(m,z)dm=Z
Dz⟨⟨m⟩⟩I(m,z),(C20)
where
I(m, z) =1
2ˆqdm2−1
2ˆq0m2+p
ˆq0mz+ ˆr1m. (C21)
33

--- PAGE 34 ---
Th other derivatives related to gEcan be computed as,
ˆqd= 4α1Z
Dz H(λ(z))R
Dσ βHβ−1(γ(z, σ))H′(γ(z, σ))u(z, σ)R
Dσ Hβ(γ(z, σ)),
ˆq0=−4α1Z
Dz H′(λ(z))v1(z) lnZ
Dσ Hβ(γ(z, σ))
−4α1Z
Dz H(λ(z))R
Dσ βHβ−1(γ(z, σ))H′(γ(z, σ))w(z, σ)R
Dσ Hβ(γ(z, σ)),
ˆr1= 2α1Z
Dz H′(λ(z))h(z) lnZ
Dσ Hβ(γ(z, σ)),(C22)
where
λ(z) =−r1p
q0−r2
1z,
γ(z, σ) =−√qd−q0σ+√q0z√1−qd,
u(z, σ) =(q0−1)σ−√q0√qd−q0z
2(1−qd)3
2√qd−q0,
v1(z) =r1z
2(q0−r2
1)3
2,
w(z, σ) =√q0σ−√qd−q0z
2√1−qd√qd−q0√q0,
h(z) =−q0z
(q0−r2
1)3
2.(C23)
a. Generalization error for task 1
During the learning of task 1, the generalization error can be defined as
ϵ1
g=*
Θ 
−sign X
iW1
ix∗
i!X
isign(mi)x∗
i!+
, (C24)
where x∗is one fresh example (or a test data). Note that, the average ⟨·⟩refers to the
ensemble average based on the thermodynamic system for task 1. To handle this average,
we define similar local fields for test data,
u∗=P
isign(mi)x∗
i√
N, v∗
1=P
iW1
ix∗
i√
N, (C25)
34

--- PAGE 35 ---
whose statistical properties are as follows, ⟨v∗
1v∗
1⟩= 1, ⟨u∗u∗⟩= 1, p1=⟨v∗
1u∗⟩=
1
NP
isign(mi)W1
i. Then, local fields are parametrized as
u∗=z,
v∗
1=p1z+q
1−p2
1y.(C26)
The generalization error becomes
ϵ1
g=Z
DzZ
DyΘ
−sign(p1z+q
1−p2
1y)z
=Z
Dz2H 
−p1p
1−p2
1z!
Θ(−z)
=1
πarccos( p1),(C27)
which introduces a new order parameter p1. To obtain the value of p1, we should first
introduce the order parameter to the original replicated partition function by a Fourier
integral representation of the Dirac delta function. After some manipulations, the free
energy density under the replica symmetry Ans¨ atz and the limit of n→0 is rewritten as
−βfRS=−1
2(ˆqdqd+ (n−1)ˆq0q0)−ˆr1r1−ˆp1p1+g′
S+α1gE (C28)
where the energy term gEremains the same, but the entropy term g′
Schanges as follows,
g′
S=Z
DzlnZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1m+ˆp1sign(m)
. (C29)
In the new saddle-point equations, it is easy to find that ˆ p1=α1∂gE
∂p1= 0, which means
the original saddle-point equations [Eqs. (C20), (C22)] are independent of the new order
parameters p1and ˆp1. Thus, after the convergence of iterating the original saddle-point
equations, we compute p1byp1=∂g′
S
∂ˆp1and obtain the following result as
p1=Z
Dz⟨⟨sign(m)⟩⟩I(m,z). (C30)
35

--- PAGE 36 ---
b. Generalization error for task 2
The calculation of generalization error for task 2 follows a similar line, except for one
important difference, which is the involvement of the teacher-average over the joint distri-
bution P(W1,W2). In the following, we will omit the similar process in computing ϵ1
g, and
instead focus on the treatment of teacher-average.
In analogous to ϵ1
g, the generalization error for task 2 can be expressed as
ϵ2
g=1
πarccos( p2), (C31)
where p2=1
NP
isign(mi)W2
i. Introducing p2to the replicated partition function results in
a modified entropy term before taking the replica symmetry Ans¨ atz,
(G′
S)N=Z
ΩnNnY
a=1NY
i=1dma
ieP
a<bˆqabP
ima
imb
i+1
2P
aˆqaaP
ima
ima
iETh
eP
aˆr1
aP
iW1
ima
i+P
aˆp2
aP
iW2
isign(ma
i)i
=Z
ΩnNnY
a=1NY
i=1dma
ieP
a<bˆqabP
ima
imb
i+1
2P
aˆqaaP
ima
ima
iETh
eP
aˆr1
aP
ima
i+P
aˆp2
aP
iW1
iW2
isign(ma
i)i
,
(C32)
where the ET[·] denotes the teacher average. A gauge transformation ma
i→ma
iW1
iis used
in the second equality. Thus, the expectation can be computed as,
ETh
eP
aˆr1
aP
ima
i+P
aˆp2
aP
iW1
iW2
isign(ma
i)i
=X
W1,W2NY
i=1P0(W1
i, W2
i)eP
aˆr1
aP
ima
i+P
aˆp2
aP
iW1
iW2
isign(ma
i)
=NY
i=1X
W1
i,W2
iP0(W1
i, W2
i)eP
aˆr1
ama
i+P
aˆp2
aW1
iW2
isign(ma
i)
=NY
i=1"
1 +r0
2cosh X
aˆr1
ama
i+X
aˆp2
asign(ma
i)!
+1−r0
2cosh X
aˆr1
ama
i−X
aˆp2
asign(ma
i)!#
.
(C33)
36

--- PAGE 37 ---
Under the replica symmetry Ans¨ atz, we entropy term becomes,
G′
S=Z
Dz"Z+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1m+ˆp2sign(m)n1 +r0
2
+Z+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1m−ˆp2sign(m)n1−r0
2#
.(C34)
After taking the limitation of n→0, we have,
g′
S=Z
DzlnZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1m+ˆp2sign(m)1 +r0
2
+Z
DzlnZ+1
−1dm e1
2ˆqdm2−1
2ˆq0m2+√ˆq0mz+ˆr1m−ˆp2sign(m)1−r0
2.(C35)
Therefore, we can finally get p2byp2=∂g′
S
∂ˆp2, which results in
p2=r0Z
Dz⟨⟨sign(m)⟩⟩I(m,z)=r0p1, (C36)
which is actually linear to p1, and r0characterizes the task similarity.
c. Constrained partition function for learning process
The partition function Eq. (C3) defined previously focuses on the stationary state of the
system, which helps to predict the optimal performance of learning given the corresponding
Hamiltonian (loss function). However, by introducing a constraint in this system, we can
study the stationary state during the learning process. The constrained partition function
can be written as,
Z=Z
ΩNY
i=1dmiδ X
im2
i−q⋆N!
e−βL1(m), (C37)
where N−q⋆Nis the total extent of the weight fluctuation. As the learning goes on, q⋆
will gradually increase until a saturation to 1. Therefore, to explore the stationary state
during learning, we can set a value of q⋆manually, and then solve the corresponding partition
function Eq. (C37).
Note that, the calculation of the constrained partition function follows the similar pro-
cedure for the non-constrained one, except for one subtle difference, which is that the order
37

--- PAGE 38 ---
parameter qaaas well as qdunder the replica symmetry Ans¨ atz are replaced by a pre-specified
constant q⋆. Hence, we can directly derive the free energy density −βfRS,
−βfRS= lim
n→0−1
2(ˆqdq⋆+ (n−1)ˆq0q0)−ˆr1r1+lnGS
n+α1lnG⋆
E
n, (C38)
where the entropy term remains unchanged, and the energy term becomes
g⋆
E= lim
n→0lnG⋆
E
n=Z
Dz2H 
−r1p
q0−r2
1z!
lnZ
Dσ Hβ
−√q⋆−q0σ+√q0z√1−q⋆
.(C39)
Notice that q⋆is a constant, and thus the saddle-point equations get simplified to
q⋆= 2∂gS
∂ˆqd, q 0=−2∂gS
∂ˆq0, r 1=∂gS
∂ˆr1,ˆq0=−2α1∂g⋆
E
∂q0,ˆr1=α1∂g⋆
E
∂r1, (C40)
where the first one is an explicit equation for ˆ qdwhose value can be numerically found (e.g.,
by using the secant method).
2. Thermodynamic system for multi-task learning
In the scenario of multi-task learning, a distinct characteristic is that previous task infor-
mation is incorporated into the learning procedure of the current task by a regularization
term. The task information from previous task refers to the trained weight for task 1, which
can be captured by the associated partition function in the single-task learning section. In
terms of the current task, a similar partition function should be established once the trained
weights of the first task are given. This observation indicates that the variational continual
learning can be mapped to the form of Franz-Parisi potential originally proposed in spin
glass theory [23] and later in neural networks [22],
Φ =1
˜ZZ
˜ΩNY
i=1d ˜mie˜βL1(˜m)lnZ
ΩNY
i=1dmieβL2(m,˜m), (C41)
38

--- PAGE 39 ---
where
L1(˜m) =M1X
µ=1lnH 
−sign(P
iW1
ix1,µ
i)P
imix1,µ
ipP
i(1−m2
i)!
,
L2(m,˜m) =M2X
µ=1lnH 
−sign(P
iW1
ix2,µ
i)P
imix2,µ
ipP
i(1−m2
i)!
−NX
i=1KL(Qmi∥Qm1
i).(C42)
Next, our goal is to compute the quenched disorder average of the thermodynamic potential
Φ, where the averages are threefold, consisting of two data averages over D1andD2, as well
as the teacher average, and can be explicitly worked out as
⟨Φ⟩=ET*
1
˜ZZ
˜ΩNY
i=1d ˜mie˜βL1(˜m)lnZ
ΩNY
idmieβL2(m,˜m)+
D1,D2
=ET1
˜ZZ
˜ΩNY
i=1d ˜miM1Y
µ=1*
H˜β 
−sign(P
iW1
ix1,µ
i)P
i˜mix1,µ
ipP
i(1−˜m2
i)!+
D1
lnZ
ΩNY
i=1dmiM2Y
µ=1*
Hβ 
−sign(P
iW2
ix2,µ
i)P
imix2,µ
ipP
i(1−m2
i)!+
D2e−βP
iKL(mi,˜mi).(C43)
Note that two data averages decouple directly due to the independence between two datasets
(but the labels can be correlated). We now omit the subscripts for the data averages. To
start the calculation, we introduce two useful replica formulas,1
˜Z= lim n→0˜Zn−1, lnZ=
lims→0∂sZs.Then the potential turns out to be
⟨Φ⟩= lim
n→0lim
s→0∂sETZ
˜ΩnNnY
a=1NY
i=1d ˜ma
inY
a=1M1Y
µ=1*
H˜β 
−sign(P
iW1
ix1,µ
i)P
i˜ma
ix1,µ
ipP
i(1−( ˜ma
i)2)!+
Z
ΩsNsY
c=1NY
i=1dmc
isY
c=1M2Y
µ=1*
Hβ 
−sign(P
iW2
ix2,µ
i)P
imc
ix2,µ
ipP
i(1−(mc
i)2)!+
e−βP
cP
iKL(mc
i,˜ma=1
i).
(C44)
Local fields are introduced for both loss functions,
˜ua=P
i˜ma
ix1
i√
N,˜v1=P
iW1
ix1
i√
N, uc=P
imc
ix2
i√
N, v 2=P
iW2
ix2
i√
N, (C45)
where we omit superscript µ. Based on the central limit theorem, the local fields follow the
39

--- PAGE 40 ---
joint Gaussian distribution with zero mean and the non-zero second moments as,
⟨˜ua˜ua⟩=P
i˜ma
i˜ma
i
N,⟨˜ua˜ub⟩=P
i˜ma
i˜mb
i
N,⟨˜v1˜ua⟩=P
iW1
i˜ma
i
N,⟨˜v1˜v1⟩= 1,
⟨ucuc⟩=P
imc
imc
i
N,⟨ucud⟩=P
imc
imd
i
N,⟨v2uc⟩=P
iW2
imc
i
N,⟨v2v2⟩= 1.(C46)
We can therefore define the order parameters, ˜ qaa=P
i˜ma
i˜ma
i
N, ˜qab=P
i˜ma
i˜mb
i
N, ˜r1
a=P
iW1
i˜ma
i
N,
qcc=P
imc
imc
i
N,qcd=P
imc
imd
i
N,r2
c=P
iW2
imc
i
Nand enforce these definitions in potential Φ by
Dirac delta function δ(·). After a few algebra manipulations, we arrive at
⟨Φ⟩= lim
n→0lim
s→0∂sZY
adˆ˜qaad˜qaa
4πi/NY
a<bdˆ˜qabd˜qab
2πi/NY
adˆ˜r1
ad˜r1
a
2πi/NY
cdˆqccdqcc
4πi/NY
c<ddˆqcddqcd
2πi/NY
cdˆr2
cdr2
c
2πi/NeNS,
(C47)
where a similar manipulation of the teacher average to Eq. (C33) is carried out, and the
action Sfinally reads
S=−1
2X
a,bˆ˜qab˜qab−X
aˆ˜r1
a˜r1
a−1
2X
c,dˆqcdqcd−X
cˆr2
cr2
c
+ lnZ
˜ΩnNZ
ΩsNY
ad ˜maY
cdmce1
2P
a,bˆ˜qab˜ma˜mb++1
2P
c,dˆqcdmcmd−βP
cKL(mc
i,˜ma=1
i)
×"
1 +r0
2cosh X
aˆ˜r1
a˜ma
i+X
cˆr2
cmc
i!
+1−r0
2cosh X
aˆ˜r1
a˜ma
i−X
cˆr2
cmc
i!#
+α1ln*nY
a=1H˜β
−sign(˜v1)˜ua
√1−˜qaa+
+α2ln*sY
c=1Hβ
−sign(v2)uc
√1−qcc+
.
(C48)
The maximum of Sdominates the integrand under the large Nlimit. Thus, we derive
the saddle-point equations by taking derivatives of the action Swith respect to the order
parameters. We then apply the replica symmetry Ans¨ atz,
˜qab= ˜q0,ˆ˜qab=ˆ˜q0,˜qaa= ˜qd,ˆ˜qaa=ˆ˜qd,˜r1
a= ˜r1,ˆ˜r1
a=ˆ˜r1,
qcd=q0,ˆqcd= ˆq0, q cc=qd,ˆqcc= ˆqd, r2
c=r2,ˆr2
c= ˆr2.(C49)
To make the calculation neat, we divide the action into three parts and compute their
40

--- PAGE 41 ---
contributions respectively. First, the interaction term reads
GI=−1
2X
a,bˆ˜qab˜qab−X
aˆ˜r1
a˜r1
a−1
2X
c,dˆqcdqcd−X
cˆr2
cr2
c
=−1
2 X
aˆ˜qaa˜qaa+X
a̸=bˆ˜qab˜qab!
−1
2 X
cˆqccqcc+X
c̸=dˆqcdqcd!
−nˆ˜r1˜r1−sˆr2r2
=−1
2
nˆ˜qd˜qd+s(s−1)ˆ˜q0q0
−1
2(sˆqdqd+n(n−1)ˆq0q0)−nˆ˜r1˜r1−sˆr2r2.(C50)
Then, the entropy term can be given by
GS=Z
˜ΩnNZ
ΩsNY
ad ˜maY
cdmce1
2P
a,bˆ˜qab˜ma˜mb++1
2P
c,dˆqcdmcmd−βP
cKL(mc
i,˜ma=1
i)
×"
1 +r0
2cosh X
aˆ˜r1
a˜ma
i+X
cˆr2
cmc
i!
+1−r0
2cosh X
aˆ˜r1
a˜ma
i−X
cˆr2
cmc
i!#
=Z
˜ΩnNZ
ΩsNY
ad ˜maY
cdmcZ
Dz1Z
Dz2e1
2(ˆ˜qd−ˆ˜q0)P
a( ˜ma)2+√
ˆ˜q0z1P
a˜ma+1
2(ˆqd−ˆq0)P
c(mc)2+√ˆq0z2P
cmc
e−βP
cKL(mc
i,˜ma=1
i)"
1 +r0
2cosh X
aˆ˜r1
a˜ma
i+X
cˆr2
cmc
i!
+1−r0
2cosh X
aˆ˜r1
a˜ma
i−X
cˆr2
cmc
i!#
=1 +r0
2Z
Dz1Z+1
−1d ˜m e˜I( ˜m,z1)n−1Z+1
−1d ˜m e˜I( ˜m,z1)Z
Dz2Z+1
−1dm eJ+(m,˜m,z2)s
+1−r0
2Z
Dz1Z+1
−1d ˜m e˜I( ˜m,z1)n−1Z+1
−1d ˜m e˜I( ˜m,z1)Z
Dz2Z+1
−1dm eJ−(m,˜m,z2)s
,
(C51)
where
˜I( ˜m, z 1) =1
2(ˆ˜qd−ˆ˜q0) ˜m2+
ˆ˜r1+q
ˆ˜q0z1
˜m,
J+(m,˜m, z 2) =1
2(ˆqd−ˆq0)m2+
ˆr2+p
ˆq0z2
m−βKL(m,˜m),
J−(m,˜m, z 2) =1
2(ˆqd−ˆq0)m2+
ˆr2+p
ˆq0z2
m−βKL(m,−˜m),
KL(x, y) =−X
z=±1
K(1 +xz
2,1 +yz
2)− K(1 +xz
2,1 +xz
2)
,(C52)
41

--- PAGE 42 ---
where K(x, y) =xlny. Finally, we derive the energy term, expressed as
G1
E=*nY
a=1H˜β
−sign(˜v1)˜ua
√1−˜qaa+
=Z
Dz2H 
−˜r1p
˜q0−˜r2
1z!Z
Dσ H˜β
−√˜qd−˜q0σ+√˜q0z√1−˜qdn
,(C53)
and
G2
E=*sY
c=1Hβ
−sign(v2)uc
√1−qcc+
=Z
Dz2H 
−r2p
q0−r22z!Z
Dσ Hβ
−√qd−q0σ+√q0z√1−qds
,(C54)
where we follow the same computation as deriving Eq. (C14). Thus, we summarize the
result as
S=−1
2
nˆ˜qd˜qd+s(s−1)ˆ˜q0q0
−1
2(sˆqdqd+n(n−1)ˆq0q0)−nˆ˜r1˜r1−sˆr2r2+lnGS+α1lnG1
E+α2lnG2
E.
(C55)
Calculation of saddle-point equations requires to consider the limits of lim n→0and lim s→0,
which leads to the computation of lim n→0lims→0lnGS
n, lim n→0lims→0lnGS
s, lim n→0lims→0lnG1
E
n,
42

--- PAGE 43 ---
and lim n→0lims→0lnG2
E
s. Thus, we define and compute these quantities first.
˜gS= lim
n→0lim
s→0lnGS
n
=Z
DzlnZ+1
−1d ˜m e˜I( ˜m,z),
gS= lim
n→0lim
s→0lnGS
s
=1 +r0
2Z
Dz1Z
Dz2lnZ+1
−1eJ+(m,˜m,z2)
˜I( ˜m,z1)
+1−r0
2Z
Dz1Z
Dz2lnZ+1
−1eJ−(m,˜m,z2)
˜I( ˜m,z1),
˜gE= lim
n→0lim
s→0lnG1
E
n
=Z
Dz2H 
−˜r1p
˜q0−˜r2
1z!
lnZ
Dσ H˜β
−√˜qd−˜q0σ+√˜q0z√1−˜qd
,
gE= lim
n→0lim
s→0lnG2
E
s
=Z
Dz2H 
−r2p
q0−r2
2z!
lnZ
Dσ Hβ
−√qd−q0σ+√q0z√1−qd
.(C56)
Then, we can arrive at the saddle-point equations given below.
˜qd= 2∂˜gS
∂ˆ˜qd,˜q0=−2∂˜gS
∂ˆ˜q0,˜r1=∂˜gS
∂ˆ˜r1,ˆ˜qd= 2α1∂˜gE
∂˜qd,ˆ˜q0=−2α1∂˜gE
∂˜q0,ˆ˜r1=α1∂˜gE
∂˜r1;
qd= 2∂gS
∂ˆqd, q 0=−2∂gS
∂ˆq0, r 2=∂gS
∂ˆr2,ˆqd= 2α2∂gE
∂qd,ˆq0=−2α2∂gE
∂q0,ˆr2=α2∂gE
∂r2.
(C57)
It is easy to verify that the tilde order parameters are exactly the same as those in
Eq. (C18), which are independent of the non-tilded order parameters. This is reasonable
because in the multi-task loss function L2(m,˜m), the magnetization ˜min the KL-divergence
is the solution after learning the first task, which is described by the single-task partition
function Eq. (C3). As for the non-tilded order parameters, the hatted ones are in the same
form with Eqs. (C22,C23), except for the replacement of r1byr2in Eq. (C23). After a few
43

--- PAGE 44 ---
manipulations, the second-task related order parameters are expressed as follows,
qd=1 +r0
2Z
Dz1Z
Dz2

m2
J+(m,˜m,z2)
˜I( ˜m,z1)
+1−r0
2Z
Dz1Z
Dz2

m2
J−(m,˜m,z2)
˜I( ˜m,z1),
q0=1 +r0
2Z
Dz1**Z
Dz2
m2−z√q0m
J+(m,˜m,z2)++
˜I( ˜m,z1)
+1−r0
2Z
Dz1**Z
Dz2
m2−z√q0m
J−(m,˜m,z2)++
˜I( ˜m,z1),
r2=1 +r0
2Z
Dz1Z
Dz2⟨⟨m⟩⟩J+(m,˜m,z2)
˜I( ˜m,z1)
+1−r0
2Z
Dz1Z
Dz2⟨⟨m⟩⟩J−(m,˜m,z2)
˜I( ˜m,z1).(C58)
a. Generalization error for two tasks
The derivation of generalization error in the multi-task scenario follows the same pro-
cedure with the single-task scenario. Thus, we present the final results directly. After the
convergence of all order parameters, the generalization error for task 2 reads
ϵ2
g=1
πarccos( p2), (C59)
where
p2=1 +r0
2Z
Dz1Z
Dz2⟨⟨sign(m)⟩⟩J+(m,˜m,z2)
˜I( ˜m,z1)
+1−r0
2Z
Dz1Z
Dz2⟨⟨sign(m)⟩⟩J−(m,˜m,z2)
˜I( ˜m,z1).(C60)
The generalization error for task 1 reads
ϵ1
g=1
πarccos( p1), (C61)
44

--- PAGE 45 ---
where
p1=1 +r0
2Z
Dz1Z
Dz2⟨⟨sign(m)⟩⟩J+(m,˜m,z2)
˜I( ˜m,z1)
−1−r0
2Z
Dz1Z
Dz2⟨⟨sign(m)⟩⟩J−(m,˜m,z2)
˜I( ˜m,z1).(C62)
b. The case of tunned KLterms
To investigate the regularization term, we can multiply this term with a factor γ, and
then derive the saddle point equations for the multi-task learning as above. Finally we can
change the value of the modulation factor to probe effects of the regularization term. The
objective function then reads
L2(m,˜m, γ) =M2X
µ=1lnH 
−sign(P
iW1
ix2,µ
i)P
imix2,µ
ipP
i(1−m2
i)!
−γNX
i=1KL(Qmi|Qm1
i).(C63)
This minor change will not affect the whole calculation process, but only induce a corre-
sponding factor in the auxiliary terms,
J+(m,˜m, z 2, γ) =1
2(ˆqd−ˆq0)m2+
ˆr2+p
ˆq0z2
m−γβKL(m,˜m),
J−(m,˜m, z 2, γ) =1
2(ˆqd−ˆq0)m2+
ˆr2+p
ˆq0z2
m−γβKL(m,−˜m).(C64)
Thus, the saddle-points equations remain the same except for the following differences,
qd=1 +r0
2Z
Dz1Z
Dz2

m2
J+(m,˜m,z2,γ)
˜I( ˜m,z1)
+1−r0
2Z
Dz1Z
Dz2

m2
J−(m,˜m,z2,γ)
˜I( ˜m,z1),
q0=1 +r0
2Z
Dz1**Z
Dz2
m2−z√q0m
J+(m,˜m,z2,γ)++
˜I( ˜m,z1)
+1−r0
2Z
Dz1**Z
Dz2
m2−z√q0m
J−(m,˜m,z2,γ)++
˜I( ˜m,z1),
r2=1 +r0
2Z
Dz1Z
Dz2⟨⟨m⟩⟩J+(m,˜m,z2,γ)
˜I( ˜m,z1)
+1−r0
2Z
Dz1Z
Dz2⟨⟨m⟩⟩J−(m,˜m,z2,γ)
˜I( ˜m,z1).(C65)
45

--- PAGE 46 ---
Acknowledgments
We thank the referee for many constructive comments to improve the quality of the
paper. This research was supported by the National Key R&D Program of China
(2019YFA0706302) and the National Natural Science Foundation of China for Grant Number
12122515 (H.H.), and the National Natural Science Foundation of China for Grant Number
11975295 (Z.H.), and Guangdong Provincial Key Laboratory of Magnetoelectric Physics
and Devices (No. 2022B1212010008), and Guangdong Basic and Applied Basic Research
Foundation (Grant No. 2023B1515040023).
[1] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. Psychology of Learning and Motivation , 24:109–165, 1989.
[2] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter.
Continual lifelong learning with neural networks: A review. Neural Networks , 113:54–71,
2019.
[3] Salomon Z. Muller, Abigail N. Zadina, L.F. Abbott, and Nathaniel B. Sawtell. Continual
learning in a multi-layer network of an electric fish. Cell, 179(6):1382–1392.e10, 2019.
[4] Yang Shen, Sanjoy Dasgupta, and Saket Navlakha. Algorithmic insights on continual learning
from fruit flies. arXiv:2107.07617 , 2021.
[5] Timo Flesch, Andrew M. Saxe, and Christopher Summerfield. Continual task learning in
natural and artificial agents. arXiv:2210.04520 , 2022.
[6] Nicolas Y. Masse, Gregory D. Grant, and David J. Freedman. Alleviating catastrophic forget-
ting using context-dependent gating and synaptic stabilization. Proceedings of the National
Academy of Sciences , 115(44):E10467–E10475, 2018.
[7] Gido M. van de Ven, Hava T. Siegelmann, and Andreas S. Tolias. Brain-inspired replay for
continual learning with artificial neural networks. Nature Communications , 11(1):4069, 2020.
[8] Axel Laborieux, Maxence Ernoult, Tifenn Hirtzlin, and Damien Querlioz. Synaptic metaplas-
ticity in binarized neural networks. Nature Communications , 12(1):2549, 2021.
[9] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,
46

--- PAGE 47 ---
Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catas-
trophic forgetting in neural networks. Proceedings of the National Academy of Sciences ,
114(13):3521–3526, 2017.
[10] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic in-
telligence. Proceedings of machine learning research , 70:3987–3995, 2017.
[11] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause, editors, Pro-
ceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings
of Machine Learning Research , pages 4548–4557. PMLR, 2018.
[12] Sebastian Farquhar and Yarin Gal. A unifying bayesian view of continual learning.
arXiv:1902.06494 , 2019.
[13] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning
using online variational bayes. arXiv:1803.10123 , 2018.
[14] Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual
learning. In International Conference on Learning Representations , 2018.
[15] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-
guided continual learning with bayesian neural networks. In International Conference on
Learning Representations , 2020.
[16] Tameem Adel, Han Zhao, and Richard E. Turner. Continual learning with adaptive weights
(claw). In International Conference on Learning Representations , 2020.
[17] Oussama Dhifallah and Yue M. Lu. Phase transitions in transfer learning for high-dimensional
perceptrons. Entropy , 23:400, 2021.
[18] Anthony Ndirango and Tyler Lee. Generalization in multitask deep neural classifiers: a statis-
tical physics approach. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32.
Curran Associates, Inc., 2019.
[19] Haruka Asanuma, Shiro Takagi, Yoshihiro Nagano, Yuki Yoshida, Yasuhiko Igarashi, and
Masato Okada. Statistical mechanical analysis of catastrophic forgetting in continual learning
with teacher and student networks. Journal of the Physical Society of Japan , 90(10):104001,
2021.
[20] Sebastian Lee, Sebastian Goldt, and Andrew Saxe. Continual learning in the teacher-student
47

--- PAGE 48 ---
setup: Impact of task similarity. arXiv:2107.04384 , 2021.
[21] Alexandre Pouget, Jeffrey M Beck, Wei Ji Ma, and Peter E Latham. Probabilistic brains:
knowns and unknowns. Nature Neuroscience , 16(9):1170–1178, 2013.
[22] Haiping Huang and Yoshiyuki Kabashima. Origin of the computational hardness for learning
with binary synapses. Physical review. E , 90:052813, 2014.
[23] Silvio Franz and Giorgio Parisi. Recipes for metastable states in spin glasses. Journal De
Physique I , 5(11):1401–1415, 1995.
[24] Haiping Huang. Statistical Mechanics of Neural Networks . Springer, Singapore, 2022.
[25] W. Krauth and M. M´ ezard. Storage capacity of memory networks with binary couplings. J.
Phys. (France) , 50:3057, 1989.
[26] G Gyorgyi. First-order transition to perfect generalization in a neural network with binary
synapses. Physical Review A , 41(12):7097–7100, 1990.
[27] H. Sompolinsky, N. Tishby, and Hyunjune Sebastian Seung. Learning from examples in large
neural networks. Physical review letters , 65:1683–1686, 1990.
[28] Chan Li and Haiping Huang. Learning credit assignment. Phys. Rev. Lett. , 125:178301, 2020.
[29] Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello, Luca Saglietti, Enzo
Tartaglione, and Riccardo Zecchina. Role of synaptic stochasticity in training low-precision
neural networks. Phys. Rev. Lett. , 120:268103, 2018.
[30] Haiping Huang. Variational mean-field theory for training restricted boltzmann machines with
binary synapses. Phys. Rev. E , 102:030301(R), 2020.
[31] Laurence Aitchison, Jannes Jegminat, Jorge Aurelio Menendez, Jean-Pascal Pfister, Alexandre
Pouget, and Peter E. Latham. Synaptic plasticity as bayesian inference. Nature neuroscience ,
24:565–571, 2021.
[32] https://github.com/Chan-Li/VCL.
48
