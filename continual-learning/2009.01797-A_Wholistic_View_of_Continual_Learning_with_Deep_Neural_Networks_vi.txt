# 2009.01797.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2009.01797.pdf
# Kích thước tệp: 3053114 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Góc Nhìn Toàn Diện về Học Liên Tục với Mạng Nơ-ron Sâu:
Những Bài Học Bị Lãng Quên và Cầu Nối tới Học Tích Cực và Học Thế Giới Mở
Martin Mundta,c,<, Yongwon Hongb, Iuliia Pliushchaand Visvanathan Ramesha
aBộ môn Khoa học Máy tính, Đại học Goethe, Theodor-W.-Adorno-Platz 1, 60323 Frankfurt, Đức
bBộ môn Khoa học Máy tính, Đại học Yonsei, 50 Yonsei-ro, Seodaemun-gu, 03722 Seoul, Hàn Quốc
cBộ môn Khoa học Máy tính, TU Darmstadt, Karolinenplatz 5, 64289 Darmstadt, Đức

THÔNG TIN BÀI BÁO
Từ khóa :
Học Sâu Liên Tục
Học Máy Suốt Đời
Học Tích Cực
Nhận Dạng Tập Mở
Học Thế Giới Mở

TÓM TẮT
Các phương pháp học sâu hiện tại được coi là thuận lợi nếu chúng thực hiện tốt trên các bộ dữ liệu kiểm tra chuyên dụng. Tư duy này được phản ánh một cách liền mạch trong lĩnh vực học liên tục đang nổi lên, nơi dữ liệu đến liên tiếp được điều tra. Thách thức cốt lõi được đóng khung là bảo vệ các biểu diễn đã học trước đó khỏi bị quên một cách thảm khốc. Tuy nhiên, việc so sánh các phương pháp riêng lẻ vẫn được thực hiện một cách biệt lập với thế giới thực bằng cách giám sát hiệu suất tích lũy trên bộ dữ liệu kiểm tra chuẩn. Giả định thế giới đóng vẫn chiếm ưu thế, tức là các mô hình được đánh giá trên dữ liệu được đảm bảo bắt nguồn từ cùng một phân bố như được sử dụng để huấn luyện. Điều này đặt ra một thách thức lớn vì các mạng nơ-ron được biết đến là đưa ra những dự đoán sai lầm quá tự tin trên các trường hợp chưa biết và bị hỏng. Trong công trình này, chúng tôi khảo sát một cách phê phán tài liệu và lập luận rằng những bài học đáng chú ý từ nhận dạng tập mở, xác định các ví dụ chưa biết bên ngoài tập quan sát, và lĩnh vực liền kề của học tích cực, truy vấn dữ liệu để tối đa hóa lợi ích hiệu suất mong đợi, thường bị bỏ qua trong kỷ nguyên học sâu. Do đó, chúng tôi đề xuất một quan điểm hợp nhất để kết nối học liên tục, học tích cực và nhận dạng tập mở trong mạng nơ-ron sâu. Cuối cùng, các hiệp tác đã được thiết lập được hỗ trợ bằng thực nghiệm, cho thấy sự cải thiện chung trong việc giảm thiểu quên thảm khốc, truy vấn dữ liệu, lựa chọn thứ tự nhiệm vụ, đồng thời thể hiện ứng dụng thế giới mở mạnh mẽ.

1. Giới thiệu
Với sự trưởng thành đang diễn ra của các hệ thống học máy thực tế, cộng đồng đã tìm thấy sự quan tâm nổi lên trở lại trong học liên tục (Thrun, 1996b,a). Trái ngược với việc thực hành rộng rãi học riêng biệt, nơi giai đoạn huấn luyện thuật toán của một hệ thống bị giới hạn trong một giai đoạn duy nhất dựa trên một bộ dữ liệu i.i.d. đã thu thập trước đó, học liên tục đòi hỏi một quá trình học tận dụng dữ liệu khi nó đến theo thời gian. Mặc dù mô hình này đã tìm thấy nhiều ứng dụng trong nhiều hệ thống học máy, để có một đánh giá hãy xem cuốn sách gần đây về học máy suốt đời của Chen và Liu (2017), sự ra đời của học sâu dường như đã hướng trọng tâm của các nỗ lực nghiên cứu hiện tại về một hiện tượng được gọi là nhiễu thảm khốc hoặc quên thảm khốc (McCloskey và Cohen, 1989; Ratcliff, 1990), như được đề xuất bởi các đánh giá gần đây (Farquhar và Gal, 2018b; Parisi et al., 2019; De Lange et al., 2021; Lesort et al., 2020) và các khảo sát thực nghiệm về học liên tục sâu (De Lange et al., 2021; Lesort et al., 2019; Pfülb và Gepperth, 2019). Hiện tượng sau là một hiệu ứng đặc biệt đối với các mô hình học máy cập nhật tham số của chúng một cách tham lam theo dân số dữ liệu được trình bày, chẳng hạn như một mạng nơ-ron lặp đi lặp lại cập nhật trọng số của nó với các ước lượng gradient ngẫu nhiên. Khi dữ liệu đến liên tục được bao gồm dẫn đến bất kỳ sự dịch chuyển nào trong phân bố dữ liệu, tập hợp các biểu diễn đã học được hướng dẫn một chiều về phía xấp xỉ giải pháp của bất kỳ nhiệm vụ nào trên các trường hợp dữ liệu mà hệ thống hiện đang được tiếp xúc. Hậu quả tự nhiên là ghi đè các biểu diễn đã học trước đó, dẫn đến việc quên đột ngột thông tin đã được thu thập trước đó.

Trong khi các công trình hiện tại chủ yếu tập trung vào việc giảm thiểu việc quên như vậy trong học liên tục sâu thông qua thiết kế các cơ chế chuyên biệt, chúng tôi lập luận rằng có một rủi ro ngày càng tăng trong lĩnh vực học liên tục trở nên quá hẹp. Rõ ràng đã có những nỗ lực đáng khen ngợi hướng tới việc bảo tồn các biểu diễn mạng nơ-ron trong quá trình huấn luyện liên tục. Tuy nhiên, một trọng tâm cao như vậy được đặt vào các yêu cầu thực tế và sự đánh đổi của các chỉ số xung quanh việc quên thảm khốc (Kemker et al., 2018), ví dụ bao gồm dấu chân bộ nhớ, chi phí tính toán, chi phí lưu trữ dữ liệu, độ dài chuỗi nhiệm vụ và số lượng vòng lặp huấn luyện, § (Díaz-Rodríguez et al., 2018; Farquhar và Gal, 2018b), rằng nó gần như có thể được coi là gây hiểu lầm khi hầu hết các hệ thống hiện tại bị hỏng ngay lập tức nếu dữ liệu chưa biết không được nhìn thấy hoặc các hỏng hóc nhỏ được gặp phải trong quá trình triển khai (Matan et al., 1990; Boult et al., 2019; Hendrycks và Dietterich, 2019). Giả định về một thế giới đóng dường như có mặt khắp nơi. Nói cách khác, có một niềm tin chung rằng mô hình sẽ luôn chỉ gặp phải dữ liệu xuất phát từ cùng một phân bố dữ liệu như được gặp phải trong quá trình huấn luyện. Điều này rất không thực tế trong thế giới mở thực tế, nơi dữ liệu có thể thay đổi đến mức không thực tế để nắm bắt vào các bộ huấn luyện hoặc người dùng có khả năng đưa ra đầu vào gần như tùy ý cho các hệ thống để dự đoán. Đây là một thực tế đã được biết đến trong nhiều thập kỷ rằng các mạng nơ-ron có sự tự tin sai lầm trong các môi trường thế giới thực như vậy (Matan et al., 1990). Mặc dù có nguy cơ không thể tránh khỏi của các mạng nơ-ron tạo ra những dự đoán hoàn toàn vô nghĩa khi gặp phải các trường hợp dữ liệu chưa biết không được nhìn thấy, các nỗ lực hiện tại hướng tới việc đánh giá học liên tục một cách thuận tiện lảng tránh thách thức này. Chọn lọc ngoại lệ cố gắng giải quyết các nhiệm vụ nhận dạng các ví dụ chưa được nhìn thấy và chưa biết, từ chối các dự đoán vô nghĩa hoặc đặt chúng sang một bên để sử dụng sau này, thường được tóm tắt dưới ô nhận dạng tập mở. Tuy nhiên, đa số các hệ thống học liên tục sâu hiện có vẫn là những hộp đen không may không thể hiện sự mạnh mẽ mong muốn đối với các dự đoán sai tương ứng trên dữ liệu chưa biết, các ngoại lệ tập dữ liệu hoặc các hỏng hóc thông thường (Hendrycks và Dietterich, 2019).

Ngoài các thực hành đánh giá hiện tại vẫn bị giới hạn trong thế giới đóng, một xu hướng không may khác là thiếu hiểu biết về bản chất của các bộ dữ liệu học liên tục được tạo ra. Cả mô hình hóa sinh học liên tục, Shin et al. (2017); Achille et al. (2018); Farquhar và Gal (2018a); Nguyen et al. (2018); Wu et al. (2018); Zhai et al. (2019), cũng như phần lớn các công trình học liên tục tăng dần lớp Li và Hoiem (2016); Kirkpatrick et al. (2017); Rebuffi et al. (2017); Lopez-Paz và Ranzato (2017); Kemker et al. (2018); Kemker và Kanan (2018); Xiang et al. (2019) thường điều tra các phiên bản tuần tự hóa của các chuẩn mực phân loại hình ảnh đã được kiểm tra theo thời gian. Ví dụ, trong MNIST tăng dần lớp phổ biến (LeCun et al., 1998), CIFAR (Krizhevsky, 2009) hoặc ImageNet (Russakovsky et al., 2015), các lớp riêng lẻ chỉ đơn giản được chia thành các tập hợp rời rạc và được hiển thị theo trình tự. Để ủng hộ việc duy trì khả năng so sánh trên một chuẩn mực, các câu hỏi về hiệu ứng của thứ tự nhiệm vụ hoặc tác động của sự chồng chéo giữa các nhiệm vụ thường xuyên bị bỏ qua. Đáng chú ý, các bài học rút ra từ lĩnh vực liền kề của học máy tích cực, một dạng đặc biệt của học bán giám sát, dường như không được tích hợp vào thực hành học liên tục hiện đại. Trong học tích cực, mục tiêu là học để tìm dần dần xấp xỉ tốt nhất cho giải pháp của một nhiệm vụ dưới thách thức cho phép hệ thống tự truy vấn dữ liệu nào sẽ bao gồm tiếp theo. Như vậy, nó có thể được coi như một kẻ thù của việc giảm thiểu quên thảm khốc. Trong khi học liên tục hiện tại bận tâm với việc duy trì thông tin được thu thập trong mỗi bước mà không tích lũy dữ liệu vô tận, học tích cực đã tập trung vào câu hỏi bổ sung về việc xác định dữ liệu phù hợp để đưa vào một hệ thống huấn luyện tăng dần. Mặc dù các công trình tiên phong ban đầu trong học tích cực đã nhanh chóng xác định những thách thức của ứng dụng mạnh mẽ và những cạm bẫy đối mặt thông qua việc sử dụng heuristics (Roy và McCallum, 2001; Settles và Craven, 2008; Li và Guo, 2013), những cái sau vẫn chiếm ưu thế trong kỷ nguyên học sâu (Beluch et al., 2018; Geifman và El-Yaniv, 2019; Gal và Ghahramani, 2015; Srivastava et al., 2014) và những thách thức dường như được đối mặt một lần nữa.

Với những thách thức trên trong tâm trí, chúng ta có thể nhanh chóng xây dựng trực giác của mình về lý do tại sao chúng được kết nối nếu chúng ta ngắn gọn nhìn vào lái xe tự động, như một ví dụ thực tế. Nếu chúng ta muốn học cách lái xe trong các môi trường mới, không chỉ đủ để đảm bảo rằng chúng ta có khả năng học từ dữ liệu mới trong khi bảo tồn kiến thức hiện có. Điều tương tự quan trọng là thừa nhận rằng dữ liệu mới đến có thể bị lệch theo những cách không thú vị hoặc có khả năng có hại. Các đối tượng mới được dự đoán sai có thể xuất hiện, các sự kiện hiếm đặc biệt có thể gây ra mối đe dọa, và các cảm biến có thể xấu đi hoặc hỏng. Việc xác định những gì đã được biết và phân biệt nó với các trường hợp mới chưa được nhìn thấy là thiết yếu. Quyết định dữ liệu mới nào là có ý nghĩa và dữ liệu nào nên được loại bỏ cho việc học tương lai cung cấp sự đóng cửa cho chu kỳ học.

Trong công trình này, chúng tôi do đó thực hiện nỗ lực đầu tiên hướng tới một quan điểm có nguyên tắc và hợp nhất về học liên tục sâu, học tích cực và học trong thế giới mở. Chúng tôi bắt đầu với một góc nhìn lịch sử và bằng cách cung cấp một đánh giá về từng chủ đề một cách riêng biệt. Sau đó chúng tôi tiến hành xác định những bài học đáng chú ý trước đây dường như nhận được ít sự chú ý hơn trong học sâu hiện đại. Nói một cách cường điệu, chúng dường như đã bị "quên" trong nhiều công trình học liên tục gần đây. Như chúng ta sẽ thấy trong suốt cuộc khảo sát, các lĩnh vực riêng lẻ có thể đã được nghiên cứu rộng rãi bởi chính chúng một cách riêng biệt, nhưng tác động của chúng dường như bị bỏ qua phần lớn khi được xem xét cùng nhau. Chúng tôi sẽ tiếp tục lập luận rằng những chủ đề dường như riêng biệt này không chỉ được hưởng lợi từ quan điểm của cái khác, mà nên được coi là kết hợp. Theo nghĩa này, chúng tôi đề xuất mở rộng các thực hành học liên tục hiện tại hướng tới một quan điểm rộng hơn về học liên tục như một thuật ngữ bao trùm. Cuộc khảo sát của chúng tôi do đó bổ sung cho các đánh giá học liên tục hiện có (Farquhar và Gal, 2018b; Parisi et al., 2019; De Lange et al., 2021; Lesort et al., 2020), nhưng thay vì khảo sát nền tảng toán học của mỗi thuật toán riêng lẻ để giảm thiểu quên thảm khốc một cách chi tiết, chúng tôi cung cấp một tổng quan phê phán hơn. Như một sự khác biệt quan trọng, chúng tôi kết nối các mẫu suy nghĩ hướng tới học liên tục mà một cách tự nhiên bao gồm và xây dựng dựa trên những hiểu biết trước đó từ học tích cực và nhận dạng tập mở. Để làm nổi bật các hiệp tác được phát triển tương ứng và giới thiệu tiềm năng thực tế của chúng, chúng tôi bổ sung cuộc khảo sát hợp nhất của mình với bằng chứng thực nghiệm hỗ trợ các khía cạnh quan trọng khác nhau: trích xuất các mẫu hoặc tập lõi, truy vấn dữ liệu tích cực, sự mạnh mẽ đối với các hỏng hóc thế giới mở, và chọn một chương trình giảng dạy thứ tự nhiệm vụ. Cho mục đích này, chúng tôi điều chỉnh và mở rộng một phương pháp được đề xuất gần đây dựa trên suy luận Bayesian biến phân trong mạng nơ-ron (Mundt et al., 2022, 2019) để minh họa một lựa chọn tiềm năng hướng tới một khung toàn diện. Quan trọng, chúng tôi nhấn mạnh rằng chúng tôi không đề xuất phương pháp này như một giải pháp phổ quát hoặc duy nhất, mà sử dụng nó để làm nổi bật tầm quan trọng của các quan điểm được phát triển trong bài báo này.

2. Lời mở đầu: học máy liên tục
Có thể ý tưởng về học máy liên tục có từ một khoảng thời gian tương tự với sự nổi lên của bản thân học máy. Đã có nhiều nỗ lực định nghĩa các khái niệm như học máy liên tục, suốt đời hoặc liên tục. Thường thì những thuật ngữ này có những sắc thái không đáng kể và thường có thể được coi như các từ đồng nghĩa. Tuy nhiên có vẻ khó khăn, và có lẽ không mang tính xây dựng, để cố gắng xác định chính xác khởi đầu của khi nào một cái gì đó nên được gọi là học liên tục hoặc suốt đời. Thay vào đó, trong lời mở đầu này, chúng tôi sẽ trình bày các định nghĩa và mô hình liên quan đã được hưởng sự phổ biến lớn trong cộng đồng học máy. Lưu ý rằng nhiều định nghĩa này không nhất thiết phải chính thức hoặc toán học, nhưng vẫn được minh họa ở đây cho một góc nhìn lịch sử. Một số mô hình đã là, hoặc nếu chưa, nên được coi là tập con của học liên tục (CL). Như một mô hình độc lập, chúng khác nhau chủ yếu trong các giao thức đánh giá hiện tại của chúng. Chúng tôi sẽ giới thiệu ngắn gọn từng mô hình này và sau đó tiến hành tóm tắt và xác định những khác biệt đặc trưng đối với thuật ngữ rộng hơn của học liên tục hiện đại.

Định nghĩa đầu tiên được lưu hành rộng rãi về học máy suốt đời (LML) có nguồn gốc từ công trình được đề xuất bởi Thrun (1996b,a). Định nghĩa này như sau:

Định nghĩa 1. Thrun (1996b,a) - Học Máy Suốt Đời: Hệ thống đã thực hiện N nhiệm vụ. Khi đối mặt với nhiệm vụ thứ (N+1), nó sử dụng kiến thức thu được từ N nhiệm vụ để giúp nhiệm vụ thứ (N+1).

Ở đây, bản chất không được đề cập là dữ liệu của N nhiệm vụ đầu tiên thường được giả định là không còn khả dụng tại thời điểm học về nhiệm vụ thứ (N+1). Nghĩa là, dữ liệu quan sát không chỉ được tích lũy và lưu trữ một cách rõ ràng vô tận. Trong khi định nghĩa này nắm bắt ý tưởng cơ bản đằng sau việc học liên tục, nó cũng mơ hồ đối với định nghĩa của nhiệm vụ và kiến thức. Đã có nhiều nỗ lực tìm ra một định nghĩa ngắn gọn hơn trong tài liệu qua các năm. Một trong những định nghĩa ngắn gọn nhưng vẫn khá chung chung đã theo sau trong công trình của Chen và Liu (2017):

Định nghĩa 2. Chen và Liu (2017) - Học Máy Suốt Đời: Học Máy Suốt Đời là một quá trình học liên tục. Tại bất kỳ thời điểm nào, người học đã thực hiện một chuỗi N nhiệm vụ học, T1; T2; §; TN. Những nhiệm vụ này có thể cùng loại hoặc khác loại và từ cùng một miền hoặc khác miền. Khi đối mặt với nhiệm vụ thứ (N+1) TN+1 (được gọi là nhiệm vụ mới hoặc hiện tại) với dữ liệu DN+1 của nó, người học có thể tận dụng kiến thức quá khứ trong cơ sở kiến thức (KB) để giúp học TN+1. Mục tiêu của LML thường là tối ưu hóa hiệu suất trên nhiệm vụ mới TN+1, nhưng nó có thể tối ưu hóa bất kỳ nhiệm vụ nào bằng cách coi phần còn lại của các nhiệm vụ như các nhiệm vụ trước đó. KB duy trì kiến thức đã học và tích lũy từ việc học nhiệm vụ trước đó. Sau khi hoàn thành việc học TN+1, KB được cập nhật với kiến thức (ví dụ: kết quả trung gian cũng như cuối cùng) thu được từ việc học TN+1. Việc cập nhật có thể liên quan đến kiểm tra tính nhất quán, lý luận và khai thác meta của kiến thức cấp cao hơn bổ sung.

Các tác giả của định nghĩa sau này lập luận rằng nó có thể được tóm tắt thành ba đặc điểm chính: học liên tục; tích lũy và duy trì kiến thức trong cơ sở kiến thức (KB); khả năng sử dụng kiến thức quá khứ để giúp học tương lai. Trái ngược với định nghĩa trước đó của Thrun (1996b,a), chủ yếu là khái niệm về một cơ sở kiến thức được duy trì được giới thiệu. Ở đây LML hiện được định nghĩa sao cho tại bất kỳ thời điểm nào hiệu suất có thể được tối ưu hóa cho bất kỳ nhiệm vụ nào bằng cách coi tất cả các nhiệm vụ khác như đã được trình bày trước đó, bất kể thứ tự ban đầu của chúng. Trong khi định nghĩa ban đầu được tối ưu hóa hướng tới việc mang lại lợi ích cho TN+1 chỉ theo một hướng, do đó cho phép hiệu suất của các nhiệm vụ trước đó giảm theo thời gian, Chen và Liu (2017) một cách rõ ràng hình thành việc bảo tồn tất cả thông tin tích lũy như một mục tiêu cơ bản của LML. Trong một lần lặp thứ hai gần đây của định nghĩa này, các tác giả đã thêm hai desiderata bổ sung: khả năng khám phá các nhiệm vụ mới và khả năng học trong khi làm việc. Chúng tôi đã hình dung hóa năm trụ cột thiết yếu của LML này trong Hình 1.

[Text continues with extensive details about machine learning definitions, paradigms, and evaluation methods...]

Martin Mundt et al.: Được chấp nhận tại Neural Networks, phiên bản cuối cùng: https://doi.org/10.1016/j.neunet.2023.01.014 Trang 1 trên 37
