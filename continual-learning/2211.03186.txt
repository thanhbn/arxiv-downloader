# 2211.03186.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2211.03186.pdf
# File size: 155752 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2211.03186v1  [cs.LG]  6 Nov 2022Momentum-based Weight Interpolation of Strong
Zero-Shot Models for Continual Learning
Zaﬁr Stojanovski1,∗, Karsten Roth1,∗, Zeynep Akata1,2
1University of Tübingen,2MPI for Intelligent Systems
Abstract
Large pre-trained, zero-shot capable models have shown con siderable success
both for standard transfer and adaptation tasks, with parti cular robustness towards
distribution shifts. In addition, subsequent ﬁne-tuning c an considerably improve
performance on a selected downstream task. However, throug h naive ﬁne-tuning,
these zero-shot models lose their generalizability and rob ustness towards distribu-
tion shifts. This is a particular problem for tasks such as Co ntinual Learning (CL),
where continuous adaptation has to be performed as new task d istributions are in-
troduced sequentially. In this work, we showcase that where ﬁne-tuning falls short
to adapt such zero-shot capable models, simple momentum-ba sed weight interpo-
lation can provide consistent improvements for CL tasks in b oth memory-free and
memory-based settings. In particular, we ﬁnd improvements of over+4% on stan-
dard CL benchmarks, while reducing the error to the upper lim it of jointly training
on all tasks at once in parts by more than half, allowing the co ntinual learner to
inch closer to the joint training limits.
1 Introduction
Continual Learning (CL) tackles the problem of learning fro m a non-stationary data stream, where
training data is presented to the model not at once, but only i n a sequence, and with limited capacity
for retention and retraining. Not only does this require eff ective use of previously seen data, but also
adaptation to novel context under continuously changing di stribution shifts without catastrophic
forgetting [11, 34, 35, 36]. Use cases are widespread, rangi ng from particularly compute-, time- or
memory-limited to privacy-concerned applications [14, 3, 16, 40, 25].
Consequentially, previous research has introduced a wide r ange of methods to address training under
continual shifts, such as through the use of efﬁcient data re play [7, 3, 1, 28], regularization on the
training dynamics [11, 36] or optimization procedures seek ing for ﬂat minima [23, 38]. Generally,
these methods start from an untrained model which is then ada pted to the data stream at hand.
While this has found practical success, more recently the us e of large-scale pre-trained models
("foundation models" [2, 25]) has become ubiquitous, as the y have shown strong zero-shot gen-
eralizability to a variety of downstream tasks, with strong robustness to distribution shifts [2].
Their application to the CL problem set, which tackles a cont inuous distribution shift, stands to
reason, with recent works showing notable beneﬁts in the use of foundation models [40, 21, 31,
42], particularly highlighting a reduction in catastrophi c forgetting. Still, as learners are adapted
to continuously shifting training distribution, even foun dation models will suffer from forgetting
through ﬁne-tuning [41].
∗Denotes equal contribution.
First Workshop on Interpolation Regularizers and Beyond. 3 6th Conference on Neural Information Processing
Systems (NeurIPS 2022).

--- PAGE 2 ---
To maximize the beneﬁts we can extract from the main continua l learning process as well as the
ability to classify novel samples at test time, it is thus imp ortant to minimize the impact on the
generalizability of the adapted foundation model in order t o account for potential further adaptations.
To allow for improved deployment in the CL setting, in this wo rk we show how momentum-based
weight-interpolation can help remedy issues of such models adapted in a continual fashion. In
particular, as we want to maximally retain the generalizabi lity of our adapted foundation model, we
introduce a bifurcated adaptation mechanism by retaining a n additional copy of the initial foundation
model (denoted as slow model). This slow model is excluded from the direct CL optimi zation
process, and is only updated through linear momentum-inter polation with a task-adapted model
copy (denoted as fastmodel).
This is motivated by insights made in [41], who show that simp le linear interpolation in weight space
between the original zero-shot model and a variant ﬁne-tune d to a task at hand allows for adaptation,
while retaining better generalizability as compared to sol e ﬁne-tuning. However, retaining a large
collection of ﬁne-tuned task expert models in the CL setting is memory intensive, impractical, and
undesired. Instead, we show that we can simulate the empiric al beneﬁts highlighted in [41] through
repeated momentum interpolation between our foundation mo del and a continuously ﬁne-tuned vari-
ant. This allows us to avoid the drawbacks of pure ﬁne-tuning , while both specializing on the new
stream of tasks, and retaining the generalizability of our f oundation model.
Indeed, experiments on three standard CL benchmarks (Seq-C IFAR-10, Seq-CIFAR-100 and Seq-
Tiny-ImageNet) show improvements in class- and task-incre mental settings on both memory-based
and memory-free methods by up to +4% , and partly more than halving the error to the joint train-
ing performance bound. These results indicate that for prac tical usage of foundation models in a
continuously distribution-shifting training scenario, m omentum-based weight interpolation can be a
reliable tool for consistent improvements that works well a longside any CL method.
2 Related Work
Regularization-based methods augment the training objective to mitigate forgetting by ke eping
the current parameters close to previous task parameters, s uch as through moment matching [15]
or Elastic Weight Consolidation (EWC) [12], which performs Laplace approximations on the
parameter posterior for each preceding task, using the mean s and covariances to regularize the
current parameters via Mahalanobis distance minimization . Online Elastic Weight Consolidation
(oEWC)[36] computes a momentum average of a single covarian ce matrix, and keeps the param-
eters from the last task only. Learning without Forgetting ( LwF)[17] also keeps the parameters
from the last task and adds a cross-entropy term between logi ts computed with the old and current
parameters, using data from the current task. [22] show that dropout forces the model to learn a
gating such that for different tasks, different paths of the network are active.
Rehearsal-based methods utilize Experience Replay [32] [33] by storing a small subse t of the
training data into a buffer, and continually replaying it as the model moves on to learn new tasks.
Dark Experience Replay (DER) [4] introduces regularizatio n in the rehearsal scheme by matching
the logits of the past with the logits computed by the current network parameters. Gradient Episodic
Memory (GEM) [18] and Average Gradient Episodic Memory (A-G EM) [8] enforce optimization
constraints in the current task using data from past tasks. G Dumb [29] greedily stores samples
in memory, and only trains the model at test time using buffer data. DualNet [27] uses a slow
network for learning task-agnostic features through Self- Supervised Learning, and a fast network
for learning task-speciﬁc features. Contrastive Continua l Learning (Co2L) [5] learns contrastive
task-agnostic features, and trains a linear classiﬁer usin g only buffer data. Our approach also
bears conceptual similarities to the lookahead-style of op timization (see e.g. [43]), adapted to the
continual learning problem.
Flatness-seeking methods aim to operate in ﬂat minima regions for each task in sequence , thereby re-
taining antecedent performance. Finding Flat Minima (F2M) [39] independently adds small random
noise to the parameters, thereby obtaining similar but diff erent loss functions which are optimized
jointly in order to locate ﬂat minima. [24] studies how batch size, dropout and learning rate decay
2

--- PAGE 3 ---
affect the model’s ability to ﬁnd ﬂat basins. [20] uses the Sh arpness-Aware Minimization (SAM) [9]
procedure, which explicitly optimizes for parameters lyin g in ﬂat basins.
3 Method
In CL, a model fθis trained on a sequence of Ttasks, where for each task t∈{1,...,T}the learner
only gets access to a subset of samples Dt={(xi,yi)}Nt
i=1, but is eventually evaluated on joint
performance, i.e. we optimize
θ∗= argminθ/summationtextT
t=1E(x,y)∼Dt[L(fθ(x),y)].
The main challenge is that at task t, the model has no access to data from previous tasks ˜t∈
{1,...,t−1}, therefore violating the typical IID data assumption. In th is work, we investigate both
the class-incremental setting, where subsets of classes ar e introduced in sequence, and the much
easier task-incremental setting which jointly also provid es respective task ids.
3.1 Momentum-based Weight Interpolation for Continual Lea rning (MCL)
To allow for effective and continuous adaptation of foundat ion models, we introduce momentum-
based weight interpolation for CL. As our primary target is t he retention of the generalizability and
shift robustness of the underlying foundation model, it is i mportant that minimal adaptation and
ﬁne-tuning is performed, while still allowing for a certain degree of adaptation to the target tasks at
hand. For that, we suggest a retention of a slow model copy θslowwhich is kept disconnected from
the entire adaptation process, while a second instantiatio nθfastis updated throughout the continual
learning process. As θfastadapts to the target distribution at hand, at every iteratio n we simultane-
ously perform an iterative updating on our slow weights thro ugh weight-space interpolation:
θslow=τ·θslow+(1−τ)·θfast
whereτis our momentum hyperparameter. A simpliﬁed version of the procedure is sum marized
in Algorithm 1. As this mechanism is task- and memory-agnost ic with no dependence on task
boundaries, it can be applied to any continual learning fram ework, both memory-based and memory-
free. And while straightforward and simple, the intuitivel y better retention of foundation model
weights in the continual learning setting is well motivated .
Beyond a conceptual connection to the Complimentary Learni ng Systems (CLS) [19, 6] theory from
neuroscience which depicts human continual learning as an i nterplay of a fast adaptive and a slow
retentive system, on a methodological level [10] show that m aintaining a running average of weights
leads to wider optima and retained generalization during th e standard ﬁne-tuning process of a pre-
trained model.
In addition, [41] showcase that zero-shot and ﬁne-tuned mod el weights are often connected by a
linear path which retains performance. It therefore stands to reason that our linear momentum-based
interpolation across task iterations allows us to connect t o the performance of our task-adapted
fast variant, while maintaining the generalizability our f oundation model weights θslow. The conse-
quently sustained implicit optimization for a ﬂatter minim um around θslow, which is only updated
through momentum-based interpolation, has strong ties to i mproved generalization across task se-
quences in continual learning [39, 24, 9], which we see reﬂec ted in our benchmark experiments in
the next section.
4 Experiments
Datasets. We evaluate our method on three datasets commonly used in the literature: CIFAR-
10 [13], CIFAR-100 [13], and Tiny ImageNet. We split each dat aset into several tasks of
non-overlapping classes: Seq-CIFAR-10 consisting of 5 tas ks (2 classes each) and Seq-CIFAR-
100/Seq-Tiny-ImageNet consisting of 10 tasks (10 and 20 cla sses each, respectively).
3

--- PAGE 4 ---
Algorithm 1 Momentum-based Weight Interpolation for Continual Learni ng (MCL)
Require: Pre-trained weights θpre, Momentum τ∈[0,1]
1:θfast←θpre
2:θslow←θpre
3:fort←1. . . num_tasks do
4: fore←1. . . num_epochs do
5: for(x,y)∼Dtdo
6: θfast←θfast−α∇L(fθfast(x),y)
7: θslow←τ·θslow+(1−τ)·θfast
8: end for
9: end for
10:end for
11:θfast←θslow
Training. For our zero-shot model we use a pre-trained CLIP ViT-B/16 [3 0]. We built our CL
experiments on [4] which implements several CL benchmarks i n PyTorch [26]. All methods follow
a standardized training protocol - trained on Nvidia 2080Ti ’s using SGD [37], a ﬁxed learning rate
and no scheduler, with the same ﬁne-tuning budget of 10epochs. We perform grid searches on a
random train subset to select the best learning rate α∈{10−2,10−3,10−4,10−5,10−6,10−7}as
well as the best momentum strength τ∈{0.995,0.997,0.999,0.9995,0.9997,0.9999}. We refer
the reader to the appendix (§A.1) for an ablation study of the hyperparameters.
Evaluation. For both Task Incremental Learning (Task-IL) and Class Incr emental Learning (Class-
IL) scenarios, we report the ﬁnal classiﬁcation accuracy ov er all encountered classes, with task
identities also provided in the Task-IL setting (making it a noticeably easier problem to solve).
Table 1: Baselines
Baseline CIFAR-10 CIFAR-100 Tiny-ImageNet
ZERO-SHOT 88.77 63 .11 58 .53
JOINT 97.53±0.08 87.22±0.54 78 .86±1.38
4.1 Experimental Results
In this section, we experiment with the use of momentum-base d weight interpolation in three stan-
dard CL method categories: ﬁne-tuning (pure SGD [37]), regu larization-based (oEWC [36]), and
rehearsal-based (DER++ [4] with buffer size 500and5000 ).
The results presented below are obtained over three seeds, a longside which we provide the zero-shot
lower bound (Tab. 1). Interestingly, the non-adapted zero- shot performance already in parts vastly
outperforms comparable adaptation with state-of-the-art methods not relying on foundation models,
with e.g. DER++ [4] reporting 72.70±1.36% with a buffer of 500, and 85.40±0.49% with a buffer of
5000 on CIFAR-10, while zero-shot performance of our founda tion model already achieves 88.77%.
This difference is even further exacerbated on Tiny-ImageN et, with19.38±1.41% and39.02±0.97
for buffer sizes of 500 and 5000 respectively, versus 58.53% for zero-shot performance, verifying
the potential [21, 31] of foundation models in CL.
To provide an upper bound, we train on all tasks jointly (Tab. 1). Since joint training is evaluated
without task boundaries, this upper bound does not hold for T ask-IL scenarios. Next, in Tab. 2 we
present the results on the CL benchmarks. We empirically sho w that, as motivated in Sec. 3, keeping
a momentum-interpolated version of the foundation model re sults in consistent improvements.
In particular, our results show that adaptation to the task d istribution at hand is beneﬁcial even
with simple ﬁne-tuning. Even when accounting for a change in learning rate (as noted in §4 and
done for every baseline), we ﬁnd that additional momentum-b ased weight interpolation offers con-
sistent beneﬁts in both class- and task-incremental settin gs, with nearly +4% improvement on both
Seq-CIFAR-100 and Seq-Tiny-ImageNet. Furthermore, throu gh momentum-updating, we can push
simple ﬁne-tuning close or even over the performance of a sta te-of-the-art CL framework (DER++).
4

--- PAGE 5 ---
Table 2: Continual Learning setting – training and evaluati ng on sequences of tasks.
Method MomentumSeq-CIFAR-10 Seq-CIFAR-100 Seq-Tiny-ImageNet
Class-IL Task-IL Class-IL Task-IL Class-IL Task-IL
SGDno 91.38±0.04 98.17±0.01 74.36±0.03 93.59±0.04 67.30±0.08 82.12±0.07
yes 92.46±0.11 98.43±0.01 77.52±0.37 94.98±0.17 71.09±0.28 85.22±0.32
oEWCno 90.67±0.01 98.17±0.01 74.07±0.20 93.80±0.02 66.60±0.02 81.79±0.02
yes 91.87±0.57 98.88±0.12 77.25±0.31 95.09±0.01 71.57±0.05 85.94±0.07
DER++ (500)no 94.65±0.16 99.38±0.10 76.68±0.23 95.05±0.09 71.05±0.12 84.42±0.22
yes 95.73±0.21 99.50±0.04 82.01±0.31 96.69±0.03 75.11±0.02 87.80±0.27
DER++ (5000)no 97.08±0.04 99.60±0.01 83.16±0.20 97.03±0.11 76.54±0.10 88.44±0.04
yes 97.21±0.11 99.62±0.01 84.94±0.07 97.13±0.05 78.26±0.14 89.00±0.11
Additionally, we observe similar performance improvement s even when applied on top of separate
CL frameworks, both memory-free (oEWC, e.g. 74.07±0.20→77.25±0.31on Seq-CIFAR-100)
and memory-based (DER++ with 500 memory samples, 76.78±0.23→82.01±0.31).
Interestingly, a momentum-extended DER++ with a buffer siz e of 500 also almost closes the gap in
performance to the non-momentum based DER++ with a much larg er buffer size 5000, which, even
with such a large memory, also sees signiﬁcant improvements on the particularly more complex CL
tasks (Seq-Tiny-ImageNet, 76.54±0.10→78.26±0.14).
This demonstrates that the need for buffer sizes in CL framew orks built around foundation models
can decrease signiﬁcantly (in this case, 10-fold) through m omentum-based weight-space interpola-
tion. We do note that while not necessary for the benchmarks a t hand, longer task sequence may
beneﬁt from a re-synchronization of θslowandθfast.
Finally, we ﬁnd that momentum-based DER++ with a buffer of 50 00 even further closes the gap to
the joint optimization upper bound - looking at the error, we ﬁnd a drop of 0.45%→0.32% on
Seq-CIFAR-10, 4.06%→2.28% on Seq-CIFAR-100, and 2.32%→0.6%on Seq-Tiny-ImageNet,
which marks a nearly 75% reduction. Conclusively, these results indicate the signi ﬁcant beneﬁts of
retaining a momentum-updated model copy when introducing f oundation models into the CL setting,
both for consistent relative improvements, but also to mini mize the performance drop when moving
from the standard joint optimization to a continual learnin g scenario.
5 Conclusion
This work tackles the adaptation of large-scale pre-traine d zero-shot models to continual learning
(CL). To retain the strong generalizability and robustness of these models even under continuous
ﬁne-tuning, we propose the use of a momentum-based interpol ation between a slow-moving zero-
shot model excluded from the direct CL process and a task-ada pted fast variant. Through this simple
extension, we ﬁnd consistent improvements in performance a cross three standard CL benchmarks
(Seq-CIFAR-10, Seq-CIFAR-100, Seq-Tiny-ImageNet) on bot h memory-based and memory-free
approaches, of in parts more than +4% . In addition, we ﬁnd the distance between continual learnin g
and joint task optimization performance in some cases to eve n be more than halved. Based on these
insights, the generalizability of large-scale pre-traine d zero-shot models, and the simplicity of the
proposed setup, we believe the adoption of our approach to be of high practical interest.
Acknowledgements
Karsten Roth thanks the International Max Planck Research S chool as well as the European Lab-
oratory for Learning and Intelligent Systems (ELLIS) PhD pr ogram for support. Zeynep Akata
acknowledges partial funding by the ERC (853489 - DEXIM) and DFG (2065/1 - Project number
390727645) under Germany’s Excellence Strategy.
References
[1] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and J onghyun Choi. Rainbow memory: Contin-
ual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 8218–8227, June 2021.
5

--- PAGE 6 ---
[2] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altma n, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Em ma Brunskill, Erik Brynjolfsson, Shyamal
Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, A nnie Chen, Kathleen Creel, Jared Quincy Davis,
Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus , Stefano Ermon, John Etchemendy,
Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, La uren Gillespie, Karan Goel, Noah Goodman,
Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Hen derson, John Hewitt, Daniel E. Ho, Jenny
Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Ju rafsky, Pratyusha Kalluri, Siddharth
Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, P ang Wei Koh, Mark Krass, Ranjay Krishna,
Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, T ony Lee, Jure Leskovec, Isabelle Levent,
Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric
Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, De epak Narayanan, Ben Newman, Allen Nie,
Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Gir ay Ogut, Laurel Orr, Isabel Papadimitriou,
Joon Sung Park, Chris Piech, Eva Portelance, Christopher Po tts, Aditi Raghunathan, Rob Reich, Hongyu
Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Chr istopher Ré, Dorsa Sadigh, Shiori Sagawa,
Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tam kin, Rohan Taori, Armin W. Thomas, Flo-
rian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michi-
hiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, T ianyi Zhang, Xikun Zhang, Yuhui Zhang,
Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportuni ties and risks of foundation models, 2021.
[3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davi de Abati, and Simone Calderara. Dark experience
for general continual learning: a strong, simple baseline. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages
15920–15930. Curran Associates, Inc., 2020.
[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davi de Abati, and SIMONE CALDERARA. Dark
experience for general continual learning: a strong, simpl e baseline. In H. Larochelle, M. Ranzato, R. Had-
sell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33,
pages 15920–15930. Curran Associates, Inc., 2020.
[5] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrasti ve continual learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICC V), pages 9516–9525, October 2021.
[6] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrasti ve continual learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICC V), pages 9516–9525, October 2021.
[7] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbac h, and Mohamed Elhoseiny. Efﬁcient lifelong
learning with a-gem. In ICLR , 2019.
[8] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbac h, and Mohamed Elhoseiny. Efﬁcient lifelong
learning with a-GEM. In International Conference on Learning Representations , 2019.
[9] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
efﬁciently improving generalization. In International Conference on Learning Representations , 2021.
[10] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, D mitry Vetrov, and Andrew Gordon Wilson. Aver-
aging weights leads to wider optima and better generalizati on.arXiv preprint arXiv:1803.05407 , 2018.
[11] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, J oel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Gra bska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcomin g catastrophic forgetting in neural net-
works. Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017.
[12] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, J oel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Gra bska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcomin g catastrophic forgetting in neural net-
works. Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017.
[13] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
[14] Cecilia Lee and Aaron Lee. Clinical applications of con tinual learning machine learning. The Lancet
Digital Health , 2:e279–e281, 06 2020.
[15] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, an d Byoung-Tak Zhang. Overcoming catas-
trophic forgetting by incremental moment matching. In I. Gu yon, U. V on Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017.
[16] Timothée Lesort, Oleksiy Ostapenko, Diganta Misra, Md Rifat Areﬁn, Pau Rodríguez, Laurent Charlin,
and Irina Rish. Scaling the number of tasks in continual lear ning, 2022.
[17] Zhizhong Li and Derek Hoiem. Learning without forgetti ng.IEEE Transactions on Pattern Analysis and
Machine Intelligence , 40(12):2935–2947, 2018.
[18] David Lopez-Paz and Marc' Aurelio Ranzato. Gradient ep isodic memory for continual learning. In I.
Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vi shwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
[19] J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly. W hy there are complementary learning systems
in the hippocampus and neocortex: insights from the success es and failures of connectionist models of
learning and memory. Psychol Rev , 102(3):419–457, Jul 1995.
6

--- PAGE 7 ---
[20] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, a nd Emma Strubell. An empirical investigation of
the role of pre-training in lifelong learning. arXiv preprint arXiv:2112.09153 , 2021.
[21] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, a nd Emma Strubell. An empirical investigation of
the role of pre-training in lifelong learning, 2022.
[22] Seyed Iman Mirzadeh, Mehrdad Farajtabar, and Hassan Gh asemzadeh. Dropout as an implicit gating
mechanism for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision a nd
Pattern Recognition (CVPR) Workshops , June 2020.
[23] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pasca nu, and Hassan Ghasemzadeh. Understanding
the role of training regimes in continual learning. In H. Lar ochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 7308–7320.
Curran Associates, Inc., 2020.
[24] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pasca nu, and Hassan Ghasemzadeh. Understanding
the role of training regimes in continual learning. In H. Lar ochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 7308–7320.
Curran Associates, Inc., 2020.
[25] Oleksiy Ostapenko, Timothee Lesort, Pau Rodríguez, Md Rifat Areﬁn, Arthur Douillard, Irina Rish, and
Laurent Charlin. Continual learning with foundation model s: An empirical study of latent replay, 2022.
[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, Ja mes Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alba n Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chil amkurthy, Benoit Steiner, Lu Fang, Junjie
Bai, and Soumith Chintala. Pytorch: An imperative style, hi gh-performance deep learning library. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems 32 , pages 8024–8035. Curran Associates, Inc., 2019.
[27] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Cont inual learning, fast and slow. In M. Ran-
zato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural
Information Processing Systems , volume 34, pages 16131–16144. Curran Associates, Inc., 20 21.
[28] Ameya Prabhu, Philip Torr, and Puneet Dokania. Gdumb: A simple approach that questions our progress
in continual learning. In The European Conference on Computer Vision (ECCV) , August 2020.
[29] Ameya Prabhu, Philip Torr, and Puneet Dokania. Gdumb: A simple approach that questions our progress
in continual learning. In The European Conference on Computer Vision (ECCV) , August 2020.
[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Rame sh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretche n Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervis ion. In Marina Meila and Tong Zhang, editors,
Proceedings of the 38th International Conference on Machin e Learning , volume 139 of Proceedings of
Machine Learning Research , pages 8748–8763. PMLR, 18–24 Jul 2021.
[31] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan D yer. Effect of scale on catastrophic forgetting
in neural networks. In International Conference on Learning Representations , 2022.
[32] R. Ratcliff. Connectionist models of recognition memo ry: constraints imposed by learning and forgetting
functions. Psychol Rev , 97(2):285–308, Apr 1990.
[33] ANTHONY ROBINS. Catastrophic forgetting, rehearsal a nd pseudorehearsal. Connection Science ,
7(2):123–146, 1995.
[34] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardi ns, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressiv e neural networks. CoRR , abs/1606.04671,
2016.
[35] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketin a, Agnieszka Grabska-Barwinska, Yee Whye Teh,
Razvan Pascanu, and Raia Hadsell. Progress & compress: A sca lable framework for continual learning. In
Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machin e
Learning , volume 80 of Proceedings of Machine Learning Research , pages 4528–4537. PMLR, 10–15
Jul 2018.
[36] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketin a, Agnieszka Grabska-Barwinska, Yee Whye Teh,
Razvan Pascanu, and Raia Hadsell. Progress and compress: A s calable framework for continual learning.
In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machin e
Learning , volume 80 of Proceedings of Machine Learning Research , pages 4528–4537. PMLR, 10–15
Jul 2018.
[37] Ohad Shamir and Tong Zhang. Stochastic gradient descen t for non-smooth optimization: Convergence
results and optimal averaging schemes. In Sanjoy Dasgupta a nd David McAllester, editors, Proceedings of
the 30th International Conference on Machine Learning , volume 28 of Proceedings of Machine Learning
Research , pages 71–79, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.
[38] Guangyuan SHI, Jiaxin Chen, Wenlong Zhang, Li-Ming Zha n, and Xiao-Ming Wu. Overcoming catas-
trophic forgetting in incremental few-shot learning by ﬁnd ing ﬂat minima. In A. Beygelzimer, Y . Dauphin,
P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems , 2021.
[39] Guangyuan SHI, JIAXIN CHEN, Wenlong Zhang, Li-Ming Zha n, and Xiao-Ming Wu. Overcoming catas-
trophic forgetting in incremental few-shot learning by ﬁnd ing ﬂat minima. In M. Ranzato, A. Beygelzimer,
7

--- PAGE 8 ---
Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing
Systems , volume 34, pages 6747–6761. Curran Associates, Inc., 2021 .
[40] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruox i Sun, Xiaoqi Ren, Guolong Su, Vincent
Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning, 2022.
[41] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mik e Li, Simon Kornblith, Rebecca Roelofs,
Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, H ongseok Namkoong, and Ludwig Schmidt.
Robust ﬁne-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 7959–7971, June 2022.
[42] Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, G uilin Qi, and Gholamreza Haffari. Pretrained
language model in continual learning: A comparative study. InInternational Conference on Learning
Representations , 2022.
[43] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hin ton. Lookahead optimizer: k steps forward,
1 step back. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
8

--- PAGE 9 ---
A Appendix
0.995 0.997 0.9990.9995 0.9997 0.9999929496
τClass-IL AccuracySeq-CIFAR-10
0.995 0.997 0.9990.9995 0.9997 0.9999758085
τSeq-CIFAR-100
0.995 0.997 0.9990.9995 0.9997 0.99997075
τSeq-Tiny-ImageNet
DER++ (500)
DER++ (5000)
oEWC
SGD
Figure 1: The effect of the hyper-parameter τon the Class-IL Accuracy
A.1 Ablation study
Momentum strength. In Figure 1 we show how the momentum strength τaffects the model’s Class-IL
Accuracy. While we ﬁnd that the optimal value of τis dataset-dependent, it is encouraging that the vastly
different methods show surprisingly similar behavior for a given dataset.
Restart frequency. Next, we examine whether it is beneﬁcial to restart the fast w eightsθfastwith the slow
weightsθslowduring training (instead of only at the end as per default, i. e. Line 11 in Algorithm 1). To this
end, we introduce a new hyperparameter restart frequency which speciﬁes after how many gradient steps we
perform a restart. From the results detailed in Figure 2, we ﬁ nd that restarting the fast weights is not beneﬁcial
to the generalization performance.
no restart 1 10 10070758085
restart frequencyClass-IL AccuracyDER++ (500)
DER++ (5000)
oEWC
SGD
no restart 1 10 1009092949698
restart frequencyTask-IL Accuracy
Figure 2: The effect of restarting the fast weights with the s low weights at various restart frequencies.
Update frequency. Finally, we examine whether it is beneﬁcial to perform the up date of the slow weights (Line
7 in Algorithm 1) at various frequencies. For this purpose, w e introduce a new hyperparameter update frequency
which speciﬁes after how many gradient steps we update the sl ow weights. From the results summarized in
Figure 3, we ﬁnd that updating at frequencies higher than 1 (w here 1 is the default behavior of our algorithm)
does not provide a boost in performance.
Figure 3: The effect of computing the momentum update at vari ous update frequencies.
9
