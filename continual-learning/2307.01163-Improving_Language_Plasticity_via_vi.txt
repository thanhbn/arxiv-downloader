# 2307.01163.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2307.01163.pdf
# Kích thước tệp: 5551079 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cải thiện Tính linh hoạt Ngôn ngữ thông qua
Tiền huấn luyện với Quên tích cực
Yihong ChenàáKelly MarchisioäRoberta RaileanuáDavid Ifeoluwa Adelanià
Pontus StenetorpàSebastian RiedelàMikel Artetxeê
àTrung tâm Trí tuệ nhân tạo UCL
áMeta AIêReka AIäCohere AI
{yihong.chen, d.adelani, p.stenetorp, s.riedel}@cs.ucl.ac.uk
mikel@reka.ai kelly@cohere.com raileanu@meta.com
Tóm tắt
Các mô hình ngôn ngữ được tiền huấn luyện (PLM) ngày nay là mô hình chính cho xử lý ngôn ngữ tự nhiên. Mặc dù có hiệu suất downstream ấn tượng, việc áp dụng PLM cho ngôn ngữ mới có thể khó khăn, tạo rào cản cho việc làm cho khả năng của chúng có thể truy cập được một cách phổ quát. Trong khi nghiên cứu trước đây đã chỉ ra rằng có thể giải quyết vấn đề này bằng cách học một lớp embedding mới cho ngôn ngữ mới, việc làm như vậy không hiệu quả về cả dữ liệu và tính toán. Chúng tôi đề xuất sử dụng cơ chế quên tích cực trong quá trình tiền huấn luyện, như một cách đơn giản để tạo ra các PLM có thể nhanh chóng thích ứng với ngôn ngữ mới. Cụ thể, bằng cách đặt lại lớp embedding mỗi K cập nhật trong quá trình tiền huấn luyện, chúng tôi khuyến khích PLM cải thiện khả năng học embedding mới trong số lượng cập nhật hạn chế, tương tự như hiệu ứng meta-learning. Các thí nghiệm với RoBERTa cho thấy các mô hình được tiền huấn luyện với cơ chế quên của chúng tôi không chỉ thể hiện sự hội tụ nhanh hơn trong quá trình thích ứng ngôn ngữ, mà còn vượt trội hơn các mô hình tiêu chuẩn trong chế độ dữ liệu thấp, đặc biệt đối với các ngôn ngữ xa với tiếng Anh. Mã nguồn sẽ có sẵn tại https://github.com/facebookresearch/language-model-plasticity .

1 Giới thiệu
Các mô hình ngôn ngữ được tiền huấn luyện (PLM) đã nhanh chóng định hình lại bối cảnh xử lý ngôn ngữ tự nhiên (NLP) bằng cách cải thiện các điểm chuẩn được chuẩn hóa trên khắp bảng [Radford và Narasimhan, 2018, Devlin et al., 2019, Liu et al., 2019, Brown et al., 2020]. Về cốt lõi, chúng thu thập kiến thức bằng cách tiêu thụ các tập dữ liệu lớn và lưu trữ kiến thức này trong các tham số của chúng trong quá trình tiền huấn luyện. Sử dụng tinh chỉnh hoặc prompting [Brown et al., 2020], kiến thức như vậy sau đó có thể được áp dụng cho các ứng dụng downstream, chẳng hạn như phân tích ngữ nghĩa, trả lời câu hỏi và những ứng dụng khác.

Mặc dù thành công, PLM vẫn có một số thiếu sót [Weidinger et al., 2021, 2022]. Cụ thể, nó đòi hỏi dữ liệu và tính toán khổng lồ để tiền huấn luyện chúng [Gururangan et al., 2020, Kaplan et al., 2020, Hernandez et al., 2021, Hu et al., 2021, Touvron et al., 2023]. Việc huấn luyện lại một PLM mới một cách ngây thơ để phù hợp với mọi sự thay đổi không gian ngôn ngữ¹ sẽ cực kỳ tốn kém. Điều này làm cho việc tạo ra các PLM có thể được thích ứng hiệu quả với các không gian ngôn ngữ mới trở thành mục tiêu nghiên cứu có tính liên quan cao.

Trong khi quên trong bối cảnh của cả học tập con người và máy thường được coi là điều gì đó tiêu cực (ví dụ như quên thảm khốc [McCloskey và Cohen, 1989, Ratcliff, 1990, Kirkpatrick et al., 2017]), các nghiên cứu gần đây đã cho thấy rằng đối với mạng nơ-ron nhân tạo, quên

¹Chúng tôi sử dụng thuật ngữ thay đổi không gian ngôn ngữ để mô tả những thay đổi trong việc sử dụng ngôn ngữ giữa tiền huấn luyện và ứng dụng downstream mục tiêu, gây ra bởi các yếu tố như thay đổi ngôn ngữ, tiến hóa thời gian hoặc chuyển đổi miền. Một mô hình có tính linh hoạt ngôn ngữ cao sẽ nhanh chóng thích ứng với những thay đổi này.

Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2023).arXiv:2307.01163v3  [cs.CL]  12 Jan 2024

--- TRANG 2 ---
cũng có thể đóng vai trò tích cực trong việc tăng "tính linh hoạt" của chúng, chẳng hạn như cải thiện khả năng tổng quát hóa đối với dữ liệu chưa thấy [Zhou et al., 2022, Chen et al., 2022, Igl et al., 2021], cho phép học tập trong chế độ dữ liệu thấp [Alabdulmohsin et al., 2021, Taha et al., 2021], hoặc chống lại thiên kiến ưu tiên [Nikishin et al., 2022, D'Oro et al., 2023]. Với những phát triển này, trong công trình này chúng tôi đặt câu hỏi liệu chúng ta có thể sử dụng quên như một cơ chế để cải thiện tiền huấn luyện và truyền cho PLM những lợi ích tương tự.

Hình 1: Nối lại thông qua học lại token embedding: nơi transformer body (phần màu tím) được "đóng băng" và tái sử dụng cho ngôn ngữ mới, nhưng token embedding được học lại để phù hợp với ngôn ngữ mới.

Việc thiết lập rõ ràng trong cộng đồng NLP rằng các mô hình gặp khó khăn trong việc tổng quát hóa giữa các ngôn ngữ mà không có sự can thiệp đáng kể [Conneau et al., 2020, Pfeiffer et al., 2020, 2022, Ansell et al., 2022], điều này đặc biệt đúng với các ngôn ngữ ít tài nguyên. Do đó chúng tôi coi đây là một sân thử nghiệm đầy hứa hẹn cho các kỹ thuật quên. Trọng tâm của chúng tôi là lớp đầu vào của PLM, lớp token embedding, vì việc học nó đã được chứng minh là rất hiệu quả khi thích ứng giữa các ngôn ngữ [Artetxe et al., 2020].

Cụ thể, chúng tôi giới thiệu một cơ chế quên tích cực, đặt lại token embedding theo khoảng thời gian đều đặn, trong khi để nguyên tất cả các tham số khác không thay đổi trong suốt quá trình tiền huấn luyện. Chúng tôi nghiên cứu xem phương pháp quên này có tạo ra một PLM có thể dễ dàng nối lại (Hình 1) với một ngôn ngữ chưa thấy (có thể xa) hay không. Theo trực giác, việc đặt lại embedding buộc transformer body phải tái suy luận mỗi lần thay vì dựa vào các đường tắt được ghi nhớ. Thông qua lặp lại, body học được lý luận trừu tượng hơn, cấp cao hơn. Một mô hình có tính trừu tượng lớn hơn có thể dễ dàng chuyển giao giữa các ngôn ngữ, vì lý luận cấp cao có tính không phụ thuộc ngôn ngữ hơn.

Các đánh giá zero-shot của chúng tôi trên một số điểm chuẩn chuyển giao đa ngôn ngữ cho thấy rằng đối với các trường hợp mà corpus thích ứng không được gán nhãn cho ngôn ngữ chưa thấy có ít nhất 5 triệu token (chế độ dữ liệu thấp), PLM quên vượt trội hơn baseline với biên độ lớn: mức tăng trung bình +21,2% trên XNLI, +33,8% trên MLQA, và +60,9% trên XQuAD. Ngoài ra, các mô hình được tiền huấn luyện bằng quên tích cực hội tụ nhanh hơn trong quá trình thích ứng ngôn ngữ. Cuối cùng, chúng tôi thấy rằng quên đặc biệt có lợi cho các ngôn ngữ xa với tiếng Anh, chẳng hạn như tiếng Ả Rập, Hindi, Thái và Thổ Nhĩ Kỳ.

2 Nối lại PLM cho Ngôn ngữ mới

Sử dụng dữ liệu không được gán nhãn, Artetxe et al. [2020] chứng minh khả năng nối lại PLM đơn ngôn ngữ cho ngôn ngữ mới; họ đề xuất học lại lớp embedding cho ngôn ngữ mới trong khi giữ nguyên tất cả các tham số khác. Giả định cơ bản là lớp token embedding và transformer body (các tham số không phải token-embedding) chia sẻ trách nhiệm theo cách mà cái trước xử lý ý nghĩa từ vựng cụ thể theo ngôn ngữ, trong khi cái sau xử lý lý luận chung cấp cao. Do đó, việc nối lại PLM tiếng Anh cho ngôn ngữ mới quy về việc thích ứng riêng biệt cái trước với dữ liệu không được gán nhãn trong ngôn ngữ mới và cái sau với dữ liệu tác vụ tiếng Anh.

Quy trình có thể được tóm tắt như sau:
1. Tiền huấn luyện: Một mô hình dựa trên transformer được tiền huấn luyện trên corpus tiếng Anh. Trong các thí nghiệm của chúng tôi, chúng tôi chọn tiền huấn luyện RoBERTa-base Liu et al. [2019], một mô hình dựa trên transformer 12 lớp, trên CC100 tiếng Anh [Conneau et al., 2020].

2. Thích ứng Ngôn ngữ: Lớp token embedding được tinh chỉnh bằng dữ liệu không được gán nhãn trong ngôn ngữ mới, trong khi transformer body được đóng băng.

3. Thích ứng Tác vụ: Transformer body được tinh chỉnh bằng dữ liệu tác vụ downstream bằng tiếng Anh, trong khi lớp token embedding được đóng băng.

4. Lắp ráp: Mô hình cuối cùng được lắp ráp bằng cách lấy lớp token embedding đã thích ứng từ giai đoạn 2 và transformer body đã thích ứng từ giai đoạn 3.

--- TRANG 3 ---
Hình 2: Chuyển giao đa ngôn ngữ zero-shot không giám sát. Trái: trong giai đoạn tiền huấn luyện, chúng tôi so sánh tiền huấn luyện tiêu chuẩn với tiền huấn luyện quên, nơi token embedding được quên tích cực theo khoảng thời gian đều đặn trong khi transformer body được học như tiền huấn luyện tiêu chuẩn. Giữa: giai đoạn thích ứng tác vụ và giai đoạn thích ứng ngôn ngữ riêng biệt thích ứng transformer body bằng dữ liệu tác vụ tiếng Anh và token embedding bằng dữ liệu không được gán nhãn trong ngôn ngữ mới. Phải: giai đoạn lắp ráp tái lắp ráp body đã thích ứng và lớp token embedding thành một PLM có thể sử dụng được.

2.1 Về Khó khăn của việc Nối lại PLM thông qua Học lại Token Embedding

Trong khi quy trình trên [Artetxe et al., 2020] cung cấp một khung chung để nối lại PLM đơn ngôn ngữ với dữ liệu không được gán nhãn trong ngôn ngữ mới, không rõ việc nối lại như vậy có thể hiệu quả như thế nào, bao gồm cả hiệu quả mẫu và hiệu quả tính toán. Để hiểu rõ hơn về khó khăn của việc nối lại PLM thông qua học lại token embedding, chúng tôi thiết kế một thí nghiệm nơi chúng tôi học lại lớp token embedding bằng cách sử dụng các lượng dữ liệu thích ứng khác nhau. Để minh họa, chúng tôi chọn tiếng Anh làm "ngôn ngữ thích ứng" giả vì tập dữ liệu tiếng Anh đủ lớn để bootstrap một loạt các tập dữ liệu con với số lượng khác nhau. Chúng tôi tạo các tập dữ liệu con với [1K,10K,100K,1M,5M,10M,100M,1B,10B] token và học lại embedding tiếng Anh trong khi giữ nguyên transformer body đóng băng.

Đường gạch nét màu xanh trong Hình 3 tóm tắt ảnh hưởng của số lượng dữ liệu thích ứng lên chất lượng của PLM được nối lại (embedding được học lại được lắp ráp với English NLI task body). Chúng ta có thể thấy rằng PLM tiêu chuẩn dễ nối lại nếu có đủ dữ liệu thích ứng. Tuy nhiên nếu corpus thích ứng chứa ít hơn 10 triệu token, hiệu suất của PLM tiêu chuẩn được nối lại (đường gạch nét màu xanh trong hình) giảm mạnh khi số lượng dữ liệu thích ứng giảm, từ gần 80 xuống khoảng 35, mức độ đoán ngẫu nhiên cho các tác vụ NLI. Điều này thúc đẩy chúng tôi tạo ra các PLM có thể nối lại hơn, tức là PLM có tính linh hoạt hơn để quá trình nối lại có thể nhanh hơn và tiêu thụ ít dữ liệu hơn.

Hình 3: Hiệu suất nối lại cho PLM tiêu chuẩn (đường gạch nét màu xanh) giảm mạnh nếu token thích ứng ≤10M.

--- TRANG 4 ---
3 Tiền huấn luyện PLM Dễ nối lại thông qua Quên tích cực

Các nghiên cứu gần đây đã chỉ ra rằng việc kết hợp quên thông qua đặt lại trọng số lặp đi lặp lại có thể tăng "tính linh hoạt" của mạng nơ-ron, cho phép chúng học từ dữ liệu nhỏ và tổng quát hóa tốt hơn đối với dữ liệu chưa thấy trong học có giám sát [Alabdulmohsin et al., 2021, Taha et al., 2021, Zhou et al., 2022]. Dựa trên những nỗ lực này, chúng tôi nghiên cứu xem liệu chúng ta có thể đưa việc quên như vậy vào giai đoạn tiền huấn luyện để PLM kết quả có tính linh hoạt hơn, cho phép nối lại dễ dàng hơn với ngôn ngữ mới.

Giả thuyết của chúng tôi. Thực tế, khi Artetxe et al. [2020] học lại lớp token embedding, việc khởi tạo lại embedding có thể được coi là quên được áp dụng một lần tại đầu giai đoạn thích ứng ngôn ngữ. Tuy nhiên, PLM (cụ thể là transformer body) chưa bao giờ gặp phải quên trước giai đoạn này và có thể gặp khó khăn trong việc xử lý tình huống mới này. Không có tiếp xúc sớm với quên, PLM có thể gặp phải phục hồi chậm do quên trước khi cuối cùng được hưởng lợi từ nó. Việc học lớp lexical embedding mới trong PLM do đó tiêu thụ nhiều dữ liệu trong ngôn ngữ mới cùng với thời gian huấn luyện dài như được chỉ ra trong Phần 2.1. Trong bài báo này, để đảm bảo việc học nhanh các ngôn ngữ mới với cả hiệu quả mẫu cao và tốc độ hội tụ, chúng tôi lập luận rằng PLM phải được tiếp xúc với quên trong quá trình tiền huấn luyện, cho phép bản thân tối đa hóa tác động tích cực của quên và giảm thiểu chi phí phục hồi.

Phương pháp của chúng tôi. Với giả thuyết này trong tâm trí, chúng tôi đề xuất thêm một cơ chế quên tích cực vào quy trình tiền huấn luyện, đặt lại lớp token embedding định kỳ như được mô tả trong Thuật toán 1. Cụ thể, cơ chế quên hoạt động bằng cách cố ý xóa trọng số của lớp embedding, lưu trữ các biểu diễn tĩnh cho tất cả token, và khởi tạo lại chúng thành một tập hợp các giá trị ngẫu nhiên mới mỗi K cập nhật gradient. Vì tiền huấn luyện liên quan đến các chiến lược huấn luyện tiên tiến, như optimizer với trạng thái và bộ lên lịch tốc độ học, chúng tôi cũng đặt lại chúng cùng với lớp token embedding. Chúng tôi gọi các mô hình ngôn ngữ được tiền huấn luyện với cơ chế quên tích cực như vậy là PLM quên, trái ngược với PLM tiêu chuẩn được tiền huấn luyện theo cách tiêu chuẩn. Đường cong loss tiền huấn luyện của PLM quên có tính episodic (Hình 4), giống như trong học tăng cường hoặc meta-learning. Việc học episodic này chứng minh rằng cơ chế quên tích cực có thể đưa ra sự đa dạng mà không cần dữ liệu mới thực sự. Mỗi sự kiện quên như thể "phân nhánh ra" một môi trường mới để mô hình khám phá, như thể khởi tạo một episode học tập mới.

Câu hỏi nghiên cứu. Để kiểm tra thêm cơ chế quên được đề xuất, chúng tôi so sánh PLM quên và PLM tiêu chuẩn về hiệu quả mẫu và tốc độ hội tụ trong quá trình thích ứng ngôn ngữ, hai khía cạnh chính của tính linh hoạt mô hình. Nghiên cứu của chúng tôi điều tra:

• RQ1: Các ngôn ngữ ít tài nguyên trong thế giới thực thường có dữ liệu khan hiếm để thích ứng mô hình. Liệu tiền huấn luyện với quên tích cực có truyền đủ tính linh hoạt cho PLM quên, cho phép chúng học ngôn ngữ mới ngay cả với dữ liệu hạn chế như vậy?

• RQ2: Triển khai PLM thường gặp phải hạn chế tính toán. Được trao thêm tính linh hoạt, liệu PLM quên có thể giảm thời gian thích ứng cho các kịch bản tính toán thấp như vậy?

• RQ3: Ngôn ngữ mới có thể rất giống hoặc khác so với ngôn ngữ tiền huấn luyện. Liệu sự tương đồng/khác biệt này có tác động đến lợi ích tương đối của PLM quên so với PLM tiêu chuẩn?

4 Đánh giá PLM quên cho Chuyển giao Đa ngôn ngữ Không giám sát

Để đánh giá hiệu quả của PLM quên và giải quyết RQ1-RQ3, chúng tôi tiến hành thí nghiệm trên một số điểm chuẩn chuyển giao đa ngôn ngữ.

4.1 Thiết lập Thí nghiệm

Trong công trình của chúng tôi, chúng tôi tuân thủ chặt chẽ thiết lập trong Artetxe et al. [2020] và Marchisio et al. [2022]. Mô hình tiền huấn luyện của chúng tôi là RoBERTa-base, một mô hình ngôn ngữ dựa trên transformer 12 lớp tiêu chuẩn. Chúng tôi huấn luyện các tokenizer sentencepiece theo ngôn ngữ [Kudo và Richardson, 2018] với kích thước từ vựng 50K trên các tập dữ liệu con tương ứng trong CC100. Mô hình được tiền huấn luyện với tập con tiếng Anh của tập dữ liệu CC-100. Quá trình tiền huấn luyện bao gồm 125K cập nhật, với kích thước batch 2048. Chúng tôi sử dụng bộ lên lịch tốc độ học với độ suy giảm tuyến tính và tốc độ học ban đầu 7e−4, với

--- TRANG 5 ---
Hình 4: Đường cong loss tiền huấn luyện của mô hình ngôn ngữ quên và tiêu chuẩn. Cơ chế quên mang lại một mẫu episodic vào đường cong loss: mỗi lần quên embedding tạo ra một đỉnh loss, từ đó mô hình học cách phục hồi. Thông qua việc lặp lại quên-học lại như vậy, mô hình quen với việc học embedding mới từ đầu.

Thuật toán 1 Cơ chế quên tích cực. Lớp token embedding được đặt lại mỗi K cập nhật.
Đầu vào: K, khoảng cách giữa hai lần quên liên tiếp; nbody/emb, số cập nhật hiệu quả hiện tại cho body hoặc lớp token embedding; αbody/emb, tốc độ học hiện tại cho body hoặc lớp token embedding; Pn body/emb, tham số sau cập nhật thứ n cho body hoặc lớp token embedding; On body/emb, trạng thái optimizer sau cập nhật thứ n cho body hoặc lớp token embedding; Θ, tham số embedding được khởi tạo ngẫu nhiên, mỗi phần tử được rút từ N(0,0.02); f, hàm tính gradient w.r.t các tham số sử dụng dữ liệu được lấy mẫu; g, hàm cập nhật tham số dựa trên gradient (ví dụ một bước trong optimizer Adam) s, hàm cập nhật tốc độ học (ví dụ một bước trong bộ lên lịch tốc độ học polynomial).
Đầu ra: các tham số và trạng thái optimizer được cập nhật P(n)={P(n) emb, P(n) body},O(n)={O(n) emb, O(n) body}.
1: nemb = n mod K
2: αbody = s(nbody) {điều chỉnh tốc độ học dựa trên n}
3: αemb = s(nemb)
4: G(n) = f(P(n−1),·) {tính tất cả gradient}
5: P(n) body, o(n) body = g(G(n) body, P(n−1) body, o(n−1) body, αbody, n) {cập nhật transformer body}
6: if nemb == 0 then
7: P(n) emb = Θ {đặt lại định kỳ token embedding và các trạng thái optimizer liên quan}
8: o(n−1) emb = 0
9: end if
10: P(n) emb, o(n) emb = g(G(n) emb, P(n−1) emb, o(n−1) emb, αemb, nemb) {cập nhật token embedding}

--- TRANG 6 ---
[Bảng dữ liệu hiệu suất so sánh PLM quên và tiêu chuẩn trên XNLI và các điểm chuẩn khác được giữ nguyên trong tiếng Việt]

10K cập nhật khởi động. Checkpoint được lưu mỗi 500 cập nhật và chúng tôi luôn chọn checkpoint tiền huấn luyện cuối cùng khi có thể để có hiệu suất tối ưu. Đối với tiền huấn luyện quên, chúng tôi chọn checkpoint tương ứng với perplexity validation tốt nhất kể từ checkpoint cuối cùng có thể có token embedding được đặt lại. Chúng tôi đặt tần suất quên K = 1000 và sử dụng clip-norm là 0.5.

Trong giai đoạn thích ứng ngôn ngữ, chúng tôi giữ hầu hết các siêu tham số giống như đối với tiền huấn luyện. Chúng tôi tinh chỉnh lớp token embedding trong khi giữ nguyên các tham số khác đóng băng, như được mô tả trong Phần 2. Lưu ý rằng không có quên nào xảy ra trong giai đoạn này vì chúng tôi muốn các mô hình học ngôn ngữ mới tốt nhất có thể. Trong giai đoạn thích ứng tác vụ, cả hai mô hình đều được tinh chỉnh trong 10 epoch trên dữ liệu tác vụ tiếng Anh, cụ thể là MultiNLI [Williams et al., 2018] cho tác vụ NLI và SQUAD Rajpurkar et al. [2016] cho tác vụ QA. Sau giai đoạn lắp ráp, chúng tôi đánh giá hiệu suất zero-shot của mô hình được lắp ráp trên XNLI [Conneau et al., 2018], một tác vụ NLI đa ngôn ngữ, cùng với XQuAD [Artetxe et al., 2020] và MLQA [Lewis et al., 2020], hai tác vụ QA đa ngôn ngữ. Chúng tôi báo cáo độ chính xác NLI và F1 QA trên các tập kiểm tra.

Các thí nghiệm của chúng tôi được triển khai bằng fairseq [Ott et al., 2019]. Các thí nghiệm tiền huấn luyện và thích ứng ngôn ngữ được tiến hành trên 32 GPU Tesla V100 (mỗi GPU có bộ nhớ 32GB) và mất khoảng 24-36 giờ để hoàn thành. Thời gian thực hiện cho cả hai giai đoạn khá gần nhau mặc dù giai đoạn sau chỉ liên quan đến việc tinh chỉnh embedding. Điều này chứng minh tầm quan trọng của việc giảm chi phí tính toán của giai đoạn thích ứng ngôn ngữ.

Khác với nghiên cứu trước [Artetxe et al., 2020, Marchisio et al., 2022], chúng tôi tập trung vào thích ứng ngôn ngữ trong chế độ dữ liệu thấp. Chúng tôi mô phỏng các kịch bản ít tài nguyên bằng cách giới hạn dữ liệu thích ứng cho mỗi ngôn ngữ downstream chỉ còn 5M subword token từ CC100. Điều này trái ngược với các thiết lập thông thường, nơi tất cả token trong các ngôn ngữ tương ứng trong CC100 được sử dụng để thích ứng ngôn ngữ. Như Bảng 6 cho thấy, các thiết lập như vậy tiêu thụ nhiều bậc độ lớn hơn dữ liệu so với thiết lập 5M-token của chúng tôi; ví dụ, tập con Swahili CC100 chứa 345M token, lớn hơn khoảng 69 lần so với corpus của chúng tôi, và tập con tiếng Nga chứa 34.9B token, lớn hơn khoảng 6,980 lần. Do đó, PLM có thể thành công học ngôn ngữ mới với dữ liệu phong phú dưới các thiết lập truyền thống có thể gặp khó khăn khi làm như vậy với corpus 5M-token hạn chế của chúng tôi.

4.2 PLM quên Hoạt động tốt hơn trong Chế độ Dữ liệu thấp (RQ1)

PLM tiêu chuẩn gặp khó khăn trong thích ứng ngôn ngữ dữ liệu thấp, giảm từ 86.1 độ chính xác NLI tiếng Anh xuống chỉ 53.3 độ chính xác trung bình trên XNLI với dữ liệu thích ứng 5M token hạn chế. So với nghiên cứu trước sử dụng dữ liệu đầy đủ từ Wikipedia [Artetxe et al., 2020] hoặc từ CC100 [Marchisio et al., 2022], độ chính xác trung bình trên XNLI giảm khoảng 18% (từ 66.8/66.3 xuống 53.3). Điều này chỉ ra PLM tiêu chuẩn không đối phó tốt với chế độ dữ liệu thấp. Ngược lại, PLM quên đạt được 62.7 độ chính xác XNLI trung bình khá tốt, mức tăng tương đối +21.2% so với PLM tiêu chuẩn, như được chỉ ra trong Bảng 2.

PLM quên cũng vượt trội hơn PLM tiêu chuẩn trên MLQA và XQuAD, với mức tăng F1 tương đối trung bình +33.8% và +60.9% trên các ngôn ngữ, như được chứng minh tương ứng trong Bảng 3 và Bảng 4. Trên các tác vụ NLI và QA, PLM quên liên tục vượt trội hơn PLM tiêu chuẩn trong chế độ dữ liệu thấp.

--- TRANG 7 ---
[Các bảng so sánh hiệu suất được giữ nguyên]

Tại sao PLM quên xử lý chế độ dữ liệu thấp tốt hơn? Chúng tôi giả thuyết điều này là do PLM quên mạnh mẽ hơn đối với các khởi tạo embedding khác nhau. Chúng mã hóa kiến thức phổ quát hơn trong transformer body. PLM tiêu chuẩn có thể mã hóa kiến thức "đường tắt" hơn dựa vào một số khởi tạo embedding nhất định. Trong dữ liệu thấp, PLM tiêu chuẩn không thể điều chỉnh embedding hướng tới các tuyến đường tắt mà không truy cập đủ dữ liệu. PLM quên không dựa vào đường tắt nên hoạt động tốt hơn.

4.3 PLM quên Học Ngôn ngữ mới với Ít Cập nhật Tham số hơn (RQ2)

Chúng tôi cũng quan tâm đến việc PLM quên và PLM tiêu chuẩn có thể học ngôn ngữ mới nhanh như thế nào. Hình 5 tóm tắt các đường cong thích ứng trên XNLI, MLQA và XQuAD, với mỗi điểm đại diện cho hiệu suất trung bình trên tất cả ngôn ngữ. Chỉ trong 5K bước (≈4% thích ứng đầy đủ), PLM quên đạt 57.8 độ chính xác trên XNLI trong khi PLM tiêu chuẩn gặp khó khăn ở mức đoán ngẫu nhiên 37.2. Xu hướng tương tự xảy ra với MLQA và XQuAD. Sau 5K bước, PLM quên đạt 92% hiệu suất đầy đủ của chúng trên XQuAD so với chỉ 53% đối với PLM tiêu chuẩn (xem biểu đồ cuối trong Hình 5).

Tại sao PLM quên hội tụ nhanh hơn? Chúng tôi giả thuyết đó là do việc đặt lại embedding định kỳ buộc body dần định vị bản thân trên một đa tạp cụ thể, nơi nó có thể dễ dàng hợp tác với embedding mới. Điều này làm cho body khuyến khích cập nhật embedding lớn hơn khi thích ứng với ngôn ngữ mới. Quên tích cực mô phỏng chuyển đổi ngôn ngữ trong quá trình tiền huấn luyện² đưa ra sự đa dạng mà không cần dữ liệu mới. Điều này cho phép thích ứng nhanh hơn với ngôn ngữ mới thực sự.

4.4 Ngôn ngữ xa với tiếng Anh Hưởng lợi nhiều nhất từ PLM quên (RQ3)

Cho đến thời điểm này, chúng tôi chủ yếu trình bày hiệu suất trung bình. Trong phần này, chúng tôi đi sâu vào so sánh chi tiết hiệu suất theo ngôn ngữ cụ thể giữa PLM quên và PLM tiêu chuẩn trên XNLI, MLQA và XQuAD. Để có cái nhìn sâu sắc hơn về ngôn ngữ nào hưởng lợi nhiều nhất từ việc sử dụng quên, chúng tôi trình bày các thay đổi hiệu suất tương đối trong Hình 6 cho XNLI và MLQA. Kết quả cho XQuAD có thể được tìm thấy trong Hình 8 trong phụ lục. Trên phổ ngôn ngữ (Bảng 5), chúng tôi quan sát thấy rằng quên mang lại lợi ích lớn hơn cho các ngôn ngữ xa với ngôn ngữ tiền huấn luyện (tiếng Anh) về mặt họ ngôn ngữ, chữ viết và hình thái học. Cụ thể, quên mang lại mức tăng tương đối lớn cho các ngôn ngữ như tiếng Ả Rập, Hindi, Thái, Thổ Nhĩ Kỳ và Urdu so với các ngôn ngữ gần hơn như tiếng Đức. Chữ viết có vẻ quan trọng - quên giúp tiếng Việt và Swahili ít hơn mặc dù khoảng cách từ tiếng Anh, có thể do chung chữ Latin. Kiểm tra các đường cong thích ứng trong 5K bước đầu tiên, PLM quên đạt hiệu suất vượt trội đáng kể so với PLM tiêu chuẩn cho hầu hết tất cả ngôn ngữ ngoại trừ Urdu, trong khi PLM tiêu chuẩn gặp khó khăn ở mức đoán ngẫu nhiên (xem Hình 7 và Phụ lục D). Điều này chứng minh khả năng của PLM quên trong việc thích ứng hiệu quả với ngôn ngữ mới, đặc biệt là những ngôn ngữ khác biệt, trong thiết lập dữ liệu thấp.

²Chính xác, nó mô phỏng hoán đổi từ vựng, gây ra thay đổi mạnh mẽ đến đầu vào của body.

--- TRANG 8 ---
[Các hình vẽ và biểu đồ được giữ nguyên với mô tả bằng tiếng Việt]

--- TRANG 9 ---
[Nội dung tiếp tục được dịch sang tiếng Việt...]

5 Nghiên cứu Liên quan

5.1 Quên và Vai trò Tích cực của nó

Nhận thức chung về quên là nó ngụ ý trí nhớ yếu và mất kiến thức đã thu được, do đó nó thường được coi là dấu hiệu của sự không thông minh hoặc một thuộc tính không mong muốn. Trong mạng nơ-ron, quên thảm khốc [McCloskey và Cohen, 1989, Ratcliff, 1990, Kirkpatrick et al., 2017] được mô tả như một hiện tượng quên nơi mạng nơ-ron mất khả năng dự đoán các mẫu cũ sau khi đầu vào mới thay đổi trọng số của chúng. Quên trong bối cảnh này có hậu quả tiêu cực, vì kiến thức mới ghi đè lên kiến thức cũ. Nhiều nghiên cứu trước đây nỗ lực vượt qua quên thảm khốc và cho phép học liên tục [Schmidhuber, 2013, Kirkpatrick et al., 2017, Lopez-Paz và Ranzato, 2017, Shin et al., 2017, Schwarz et al., 2018, Mallya và Lazebnik, 2018, Parisi et al., 2019, Rolnick et al., 2019, Beaulieu et al., 2020, Veniat et al., 2020, Gaya et al., 2023, Khetarpal et al., 2022].

Công trình của chúng tôi khác với những công trình trên ở chỗ chủ đề của chúng tôi là quên có chủ ý chứ không phải quên thụ động và tác động tiêu cực liên quan. Nói cách khác, chúng tôi tìm cách hiểu cách quên - nếu được cố ý kết hợp như một quá trình tích cực trong quá trình huấn luyện - có thể giúp việc học mới. Các vai trò tích cực tương tự của quên đã được thảo luận trong tài liệu. Cụ thể, Pastötter et al. [2008] chứng minh quên nâng cao việc học thông tin mới bằng cách đặt lại quá trình mã hóa và giữ sự chú ý ở mức độ cao; Levy et al. [2007] cho thấy rằng nó giúp thu nhận ngôn ngữ thứ hai bằng cách ức chế ngôn ngữ bản địa; Barrett và Zollman [2009] thấy rằng nó thúc đẩy sự xuất hiện của một ngôn ngữ tối ưu bằng cách ngăn chặn thành công một phần củng cố thực hành dưới tối ưu. Nørby [2015] tiếp tục đề xuất quên phục vụ các chức năng thích ứng, giúp mọi người điều hòa cảm xúc, thu nhận kiến thức và luôn chú ý đến bối cảnh. Gần đây hơn Anderson và Hulbert [2021] xem xét bằng chứng về quên tích cực bằng kiểm soát tiền trán và cho thấy cách nó có thể thích ứng trí nhớ để phù hợp với mục tiêu cảm xúc hoặc nhận thức.

5.2 Quên thông qua Đặt lại Trọng số Nơ-ron Một phần

Trong mạng nơ-ron, quên có thể được thể hiện dưới nhiều hình thức. Một cách đơn giản là đặt lại các tập hợp con của tham số trước vòng học tiếp theo. Các lần lặp của việc đặt lại như vậy đã được chỉ ra là có lợi cho tổng quát hóa với tính toán thấp và dữ liệu thấp cho các tác vụ thị giác máy tính [Frankle và Carbin, 2019, Alabdulmohsin et al., 2021, Taha et al., 2021, Ramkumar et al., 2023]. Gần đây hơn, Zhou et al. [2022] chứng minh một chiến lược quên tương tự giúp phân loại hình ảnh và xuất hiện ngôn ngữ. Liên quan chặt chẽ với phương pháp của chúng tôi, Chen et al. [2022] quên node embedding để cắt bớt truyền thông điệp vô hạn giữa các node và do đó hỗ trợ lý luận đồ thị mới với các node mới. Công trình của chúng tôi sử dụng cơ chế quên tương tự trên token embedding, cải thiện lý luận ngôn ngữ mới với token mới. Theo như chúng tôi biết, chúng tôi là những người đầu tiên đưa quên vào tiền huấn luyện và chứng minh rằng tiền huấn luyện quên tăng cường tính linh hoạt ngôn ngữ. Một luồng nghiên cứu liên quan trong nghiên cứu học tăng cường (RL) nghiên cứu hiện tượng mất tính linh hoạt [Lyle et al., 2023, Nikishin et al., 2023]. Công trình gần đây khám phá các phương pháp quên tương tự để cải thiện tính linh hoạt. Igl et al. [2021] định kỳ đặt lại chính sách hiện tại bằng cách chưng cất nó vào một mạng được khởi tạo lại trong suốt quá trình huấn luyện. Theo trực giác, điều này giải phóng dung lượng mạng lưu trữ các chính sách dưới tối ưu và mở ra không gian cho chính sách tối ưu (cuối cùng) chưa được khám phá. Các phương pháp đơn giản hơn chỉ đặt lại các lớp cuối của agent [Nikishin et al., 2022], ngăn chặn overfitting với các trải nghiệm sớm và thiên kiến ưu tiên. Đặt lại tham số cũng cải thiện hiệu quả mẫu bằng cách cho phép nhiều cập nhật hơn cho mỗi tương tác môi trường [D'Oro et al., 2023].

5.3 Làm cho Mô hình Ngôn ngữ Tiền huấn luyện Đa ngôn ngữ

Tiền huấn luyện trên dữ liệu đa ngôn ngữ làm cho PLM đa ngôn ngữ [Conneau et al., 2020] nhưng có nhược điểm như cần corpus đa ngôn ngữ lớn với việc trộn phù hợp, khả năng can thiệp giữa các ngôn ngữ và khó khăn trong việc bao phủ tất cả ngôn ngữ. Thay vào đó, dòng nghiên cứu về chuyển giao đa ngôn ngữ làm cho PLM đa ngôn ngữ bằng cách mở rộng PLM chỉ tiếng Anh sang ngôn ngữ khác. Artetxe et al. [2020] chứng minh khả năng mở rộng như vậy bằng cách học lại lớp embedding với dữ liệu không giám sát từ ngôn ngữ mới. Marchisio et al. [2022] tiếp tục tăng hiệu quả tính toán bằng cách sử dụng proxy mô hình mini. Liu et al. [2023a] sử dụng phương pháp đặt lại-khởi tạo một phần tương tự trong thiết lập thị giác-ngôn ngữ. Các phương pháp dựa trên adapter và tinh chỉnh thưa thớt cũng đã được đề xuất [Pfeiffer et al., 2020, 2022, 2021, Ansell et al., 2022]. Adapter là các lớp nút thắt cổ chai (thường được đặt sau các lớp feedforward) thêm dung lượng bổ sung khi thích ứng với tác vụ hoặc ngôn ngữ khác. Cơ chế quên được đề xuất của chúng tôi có thể được áp dụng cho các phương pháp dựa trên adapter vì chúng tôi có thể cho phép quên xảy ra trong các lớp adapter. Việc lựa chọn hiện tại về quên embedding giữ nguyên kiến trúc và không phát sinh thêm việc tinh chỉnh siêu tham số, cho phép chúng tôi hiểu khả năng cơ bản của tiền huấn luyện quên.

6 Kết luận & Công trình tương lai

6.1 Kết luận

Trong khi quên thường được coi là tiêu cực, nghiên cứu gần đây chỉ ra rằng nó cũng có thể có lợi trong một số trường hợp nhất định, đặc biệt để nhanh chóng học các tác vụ mới, huấn luyện trong môi trường không ổn định [Igl et al., 2021, Nikishin et al., 2022, D'Oro et al., 2023] và cải thiện hiệu quả mẫu [Taha et al., 2021, Zhou et al., 2022]. Tham gia vào dòng công trình này, bài báo của chúng tôi chứng minh rằng các kỹ thuật quên có thể cải thiện các mô hình ngôn ngữ tiền huấn luyện bằng cách truyền cho chúng tính linh hoạt ngôn ngữ hơn. Cụ thể, cơ chế quên tích cực được đề xuất của chúng tôi có thể tạo ra các PLM dễ nối lại hơn cho các không gian ngôn ngữ mới. Các thí nghiệm với RoBERTa cho thấy các mô hình được tiền huấn luyện thông qua quên tích cực có thể học tốt hơn từ lượng dữ liệu nhỏ đồng thời cũng được hưởng lợi từ sự hội tụ nhanh hơn trong quá trình thích ứng ngôn ngữ, đặc biệt đối với các ngôn ngữ xa với tiếng Anh.

Vượt ra ngoài thích ứng ngôn ngữ, chúng tôi lập luận rằng PLM có tính linh hoạt hơn là một hướng nghiên cứu đầy hứa hẹn cho tương lai, vì chúng cho phép thích ứng dễ dàng hơn với các tác vụ, miền, ngôn ngữ khác nhau và có thể tiến hóa nhanh hơn khi thế giới thực thay đổi. Không giống như các phương pháp ký hiệu, chẳng hạn như đồ thị kiến thức, có thể dễ dàng nối lại một sự thật bằng cách sửa đổi triplet kiến thức tương ứng, PLM tĩnh hiện tại khó nối lại hơn vì thay đổi một sự thật thông qua cập nhật trọng số mô hình có thể làm gián đoạn nhiều sự thật khác mà không có sự can thiệp hậu hoc đáng kể. Cải thiện khả năng nối lại thông qua tiền huấn luyện quên do đó có thể được coi là một cách truyền cho PLM những lợi ích tương tự như các phương pháp ký hiệu (làm cho mô hình kết quả có thể kiểm soát được tức là có thể được sửa đổi với chi phí nhỏ), bổ sung cho dòng nghiên cứu chỉnh sửa mô hình hậu hoc [Mitchell et al., 2021, 2022].

6.2 Hạn chế

Công trình của chúng tôi sử dụng phương pháp quên đơn giản nhất - trực tiếp đặt lại embedding về khởi tạo ngẫu nhiên. Các kỹ thuật tiên tiến như dần dần tiêm nhiễu có thể được khám phá. Chúng tôi tập trung vào tiền huấn luyện mô hình ngôn ngữ có mặt nạ với tokenizer theo ngôn ngữ cụ thể. Áp dụng quên tích cực cho LM tự hồi quy, các phương pháp tiền huấn luyện khác (ví dụ tiền huấn luyện deberta [He et al., 2021b,a]), và tokenization khác nhau là công trình tương lai đầy hứa hẹn. Rộng hơn, các mô hình ngôn ngữ lớn hiện tại cần tính linh hoạt hơn để mở rộng qua các công cụ, tác vụ và miền. Công trình của chúng tôi thực hiện một bước ban đầu, cho thấy việc trực tiếp đặt lại embedding có thể cải thiện đáng kể tính linh hoạt mô hình. Nghiên cứu thêm về các kỹ thuật quên tinh vi hơn trong quá trình tiền huấn luyện có thể mở khóa những lợi ích bổ sung.

Về mặt lý thuyết, có thể tạo ra kết nối tiềm năng giữa quên và meta-learning [Schaul và Schmidhuber, 2010, Thrun và Pratt, 2012, Andrychowicz et al., 2016, Finn et al., 2017] vì cả hai đều cố gắng học các giải pháp có thể nhanh chóng thích ứng với đầu vào mới. Một giải thích lý thuyết khả thi khác cho việc tại sao quên tích cực hoạt động tốt như vậy có thể liên quan đến độ phẳng của giải pháp trong bối cảnh loss [Alabdulmohsin et al., 2021]. Minima phẳng hơn có xu hướng có khả năng tổng quát hóa tốt hơn [Liu et al., 2023b]. Do đó, có thể đáng để nghiên cứu độ phẳng của transformer body trong quá trình tiền huấn luyện quên.

--- TRANG 11 ---
Lời cảm ơn và Tiết lộ Tài trợ

Chúng tôi muốn cảm ơn các nhà đánh giá vì những đề xuất có giá trị của họ. Chúng tôi cũng biết ơn những người tham gia vào các cuộc thảo luận thú vị trong dự án, bao gồm Pasquale Minervini, Xuanli He, Jiayi Wang, Yuxiang Wu, Hila Gonen, Dieuwke Hupkes, Fabio Petroni, Naila Murray, Alexis Thual, Nicola Cancedda, Yingchen Xu, và Hubert Jacob Banville. Yihong muốn bày tỏ lòng biết ơn đối với chương trình PhD FAIR-UCL vì đã hào phóng tài trợ cho PhD của cô. David Adelani ghi nhận sự hỗ trợ của chương trình DeepMind Academic Fellowship.

[Phần tài liệu tham khảo từ trang 11-20 được giữ nguyên định dạng gốc với các trích dẫn và thông tin xuất bản]
