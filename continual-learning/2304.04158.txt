# 2304.04158.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2304.04158.pdf
# File size: 1057090 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DOES CONTINUAL LEARNING EQUALLY FORGET ALL
PARAMETERS ?
Haiyan Zhao1Tianyi Zhou2Guodong Long1Jing Jiang1Chengqi Zhang1
1University of Technology Sydney
2University of Maryland
Haiyan.Zhao-2@student.uts.edu.au, zhou@umiacs.umd.edu,
fguodong.long, jing.jiang, Chengqi.Zhang g@uts.edu.au
ABSTRACT
Distribution shift (e.g., task or domain shift) in continual learning (CL) usually results in catastrophic
forgetting of neural networks. Although it can be alleviated by repeatedly replaying buffered data,
the every-step replay is time-consuming. In this paper, we study which modules in neural networks
are more prone to forgetting by investigating their training dynamics during CL. Our proposed met-
rics show that only a few modules are more task-speciﬁc and sensitively alter between tasks, while
others can be shared across tasks as common knowledge. Hence, we attribute forgetting mainly to
the former and ﬁnd that ﬁnetuning them only on a small buffer at the end of any CL method can
bring non-trivial improvement. Due to the small number of ﬁnetuned parameters, such “Forget-
ting Prioritized Finetuning (FPF)” is efﬁcient in computation. We further propose a more efﬁcient
and simpler method that entirely removes the every-step replay and replaces them by only k-times
of FPF periodically triggered during CL. Surprisingly, this “ k-FPF” performs comparably to FPF
and outperforms the SOTA CL methods but signiﬁcantly reduces their computational overhead and
cost. In experiments on several benchmarks of class- and domain-incremental CL, FPF consistently
improves existing CL methods by a large margin, and k-FPF further excels in efﬁciency without
degrading the accuracy. We also empirically studied the impact of buffer size, epochs per task, and
ﬁnetuning modules on the cost and accuracy of our methods.
1 Introduction
Empowered by advancing deep learning techniques and neural networks, machine learning has achieved unprece-
dented promising performance on challenging tasks in different ﬁelds, mostly under the i.i.d. ofﬂine setting. However,
its reliability and performance degenerate drastically in continual learning (CL) where the data distribution or task in
training changes over time, as the model quickly adapts to a new task and overwrites the previously learned weights.
This leads to a severe bias toward more recent tasks and “catastrophic forgetting” of previously learned knowledge,
which is detrimental to a variety of practical applications.
A widely studied strategy to mitigate forgetting is experience replay (ER) [1, 2] and its variants [3, 4, 5], which
store a few data from previous tasks in the limited memory and train the model using both the current and buffered
data. However, they only bring marginal improvements when the memory is too small to store sufﬁcient data for
recovering previously learned knowledge, which is common due to the complicated distributions of previous tasks. In
contrast, multi-task learning [6] usually adopts a model architecture composed of a task-agnostic backbone network
and multiple task-speciﬁc adaptors on top of it. While the backbone needs to be pre-trained on large-scale data, the
adaptors are usually lightweight and can be achieved using a few data. In CL, however, we cannot explicitly pre-deﬁne
and separate the task-agnostic parts and task-speciﬁc parts. Although previous methods [7, 8] have studied to restrict
the change of parameters critical to previous tasks, a such extra constraint might degrade the training performance and
discourage task-agnostic modules capturing shared knowledge.
In this paper, we study a fundamental but open problem in CL, i.e., are most parameters task-speciﬁc and sensitively
changing with the distribution shift? Or is the catastrophic forgetting mainly caused by the change in a few task-arXiv:2304.04158v1  [cs.LG]  9 Apr 2023

--- PAGE 2 ---
speciﬁc parameters? It naturally relates to the plasticity-stability trade-off in biological neural systems [9]: more
task-speciﬁc parameters improve the plasticity but may cause severe forgetting, while the stability can be improved by
increasing parameters shared across tasks. In addition, how many task-speciﬁc parameters sufﬁce to achieve promising
performance on new task(s)? Is every-step replay necessary?
To answer these questions, we investigate the training dynamics of model parameters during the course of CL by
measuring their changes over time. For different CL methods training on different neural networks, we consistently
observe that only a few parameters change more drastically than others between tasks . The results indicate that
most parameters can be shared across tasks, and we only need to ﬁnetune a few task-speciﬁc parameters to retain
the previous tasks’ performance . Since these parameters only contain a few layers of various network architectures,
they can be efﬁciently and accurately ﬁnetuned using a small buffer.
The empirical studies immediately motivate a simple yet effective method, “forgetting prioritized ﬁnetuning (FPF)”
which ﬁnetunes the task-speciﬁc parameters using buffered data at the end of CL methods. Surprisingly, on multiple
datasets, FPF consistently improves several widely-studied CL methods and substantially outperforms a variety of
baselines. Moreover, we extend FPF to a more efﬁcient replay-free CL method “k-FPF” that entirely eliminates
the cost of every-step replay by replacing such frequent replay with occasional FPF. k-FPF applies FPF only ktimes
during CL. We show that a relatively small ksufﬁces to enable k-FPF to achieve comparable performance with that
of FPF+SOTA CL methods and meanwhile signiﬁcantly reduces the computational cost. In addition, we explore
different groups of parameters to ﬁnetune in FPF and k-FPF by ranking their sensitivity to task shift evaluated in
the empirical studies. For FPF, we compare them under different choices for the buffer size, the number of epochs
per task, the CL method, and the network architecture. FPF can signiﬁcantly improve existing CL methods by only
ﬁnetuning1.13% parameters. For k-FPF, we explore different groups of parameters, k, and the ﬁnetuning steps per
FPF.k-FPF can achieve a promising trade-off between efﬁciency and performance. Our experiments are conducted
on a broad range of benchmarks for class- and domain-incremental CL in practice, e.g., medical image classiﬁcation
and realistic domain shift between image styles.
2 Related Work
Continual Learning and Catastrophic Forgetting A line of methods stores samples of past tasks to combat the
forgetting of previous knowledge. ER [3] applies reservoir sampling [10] to maintain a memory buffer of uniform
samples over all tasks. MIR [11] proposes a new strategy to select memory samples suffering the largest loss increase
induced by the incoming mini-batch, so those at the forgetting boundary are selected. DER and DER++ [4] apply
knowledge distillation to mitigate forgetting by storing the output logits for buffered data during CL. iCaRL [12] selects
samples closest to the representation mean of each class and trains a nearest-mean-of-exemplars classiﬁer to preserve
the class information of samples. Our methods are complementary techniques to these memory-based methods. It
can further improve their performance by ﬁnetuning a small portion of task-speciﬁc parameters on buffered data once
(FPF) or occasionally ( k-FPF).
Another line of work imposes a regularization on model parameters or isolates task-speciﬁc parameters to retain
previous knowledge. oEWC [7] constrains the update of model parameters important to past tasks by a quadratic
penalty. To select task-speciﬁc parameters, SI [8] calculates the effect of the parameter change on the loss while
MAS [13] calculates the effect of the parameter change on the model outputs when each new task comes. PackNet [14]
and HAT [15] iteratively assign a subset of parameters to consecutive tasks via binary masks. All these works try to
identify critical parameters for different tasks during CL and restrict the update of these parameters. But they can also
prevent task-agnostic parameters from learning shared knowledge across tasks. From the training dynamics of CL,
we identify the parameters sensitive to distribution shift. FPF and k-FPF ﬁnetune these parameters to mitigate bias
without restricting the update of task-agnostic parameters.
Different modules in neural networks
Pham et al. [16] and Lesort et al. [17] only study the effect of different normalization layers and classiﬁers on CL in
a given setting, while our method investigates the sensitivity of all parameters in different network architectures and
scenarios. Wu et al. [18] study the forgetting of different blocks in the pre-trained language models by investigating
their representation ability. We provide a more ﬁne-grained analysis of the forgetting of each module by their training
dynamics. And we ﬁnd that parameters of different kinds of modules have different sensitivities to forgetting. Ra-
masesh et al. [19] show that freezing earlier layers after training the ﬁrst task has little impact on the performance
of the second task. This is because their unfrozen part covers the last FC layer and many BN parameters, which are
the most sensitive/critical according to our empirical study. Zhang et al. [20] ﬁnd that in different architectures, the
parameters in the top layers(close to input) are more critical and perturbing them leads to poor performance. Our
2

--- PAGE 3 ---
empirical study is consistent with their ﬁndings in that the earlier convolutional layer is sensitive to task drift and the
induced biases on them lead to catastrophic forgetting.
3 Problem Setup
Notations We consider the CL setting, where the model is trained on a sequence of tasks indexed by t2f1;2;:::;Tg.
During each task t, the training samples (x;y)(with labely) are drawn from an i.i.d. distribution Dt. Given a neural
networkf()ofLlayers with parameter  =f`g`=1:L,`=f`;igi=1:n`denote all parameters in layer- `where
`;idenotes parameter- i. On each task, f()is trained for Nepochs. We denote all parameters and the layer- `’s
parameters at the end of the n-th epoch of task tbyt
nandt
`;n,n2f1;:::;Ng, respectively.
Settings In this paper, we mainly focus on class-incremental learning (class-IL) and domain-incremental learning
(domain-IL). In class-IL, Dtare drawn from a subset of classes Ct, andfCtgT
t=1for different tasks are assumed to
be disjoint. Class-IL is a more challenging setting of CL[21] than task-incremental learning (task-IL) [22]. Unlike
task-IL, class-IL cannot access the task label during inference and has to distinguish among all classes from all tasks.
In domain-IL, tasks to be learned remain the same, but the domain varies, i.e. the input data distribution Dtchanges.
The model is expected to adapt to the new domain without forgetting the old ones. The goal of the class-IL and
domain-IL is: minL(),PT
t=1E(x;y)Dt[l(y;f(x))], wherelis the objective function.
Datasets We conduct class-IL experiments on Seq-MNIST, Seq-OrganAMNIST, Seq-PathMNIST, Seq-CIFAR-10,
and Seq-TinyImageNet. Seq-OrganAMNIST and Seq-PathMnist are generated by splitting OrganAMNIST or PathM-
NIST from MedMNIST[23], a medical image classiﬁcation benchmark. CL on medical images is essential in practice
but also challenging since medical images always come as a stream with new patients and new diseases. Moreover,
medical images of different classes might only have subtle differences that are hard to distinguish. For domain-IL
experiments, we use PACS dataset [24], which is widely used for domain generalization. It can present a more re-
alistic domain-shift challenge than the toy-setting of PermuteMNIST [25]. Details of these datasets can be found in
Appendix. H.
Models We follow the standard network architectures adopted in most previous CL works. For Seq-MNIST,
following [3], we employ an MLP, i.e., a fully-connected (FC) network with two hidden layers, each composed of
100 ReLU units. Following [26, 27], we train ResNet-18 [28] on other ﬁve datasets. In addition, we also extend our
empirical study to another architecture, i.e., VGG-11 [29] on Seq-CIFAR-10.
4 Forgetting of Different Parameters: An Empirical study
A fundamental and long-lasting question in CL is how the distribution shift impacts different model parameters
and why it leads to harmful forgetting. Its answer could unveil the plasticity-stability trade-off in CL, where some
parameters are plastic and task-speciﬁc and thus have to be ﬁnetuned before deploying the model, while the stable
ones can be shared with and generalized to new tasks. In order to answer this question, we conduct a comprehensive
empirical study that compares the training dynamics of different parameters in three widely studied neural networks.
4.1 Measuring Forgetting via Training Dynamics
To measure and compare the forgetting effects of different parameters, we adopt two intuitive metrics to compute the
change of parameters and investigate their dynamics over CL. The ﬁrst metric calculates the difference between two
consecutive epochs, e.g., for parameter `, it computes (1) (1=j`j)kt
`;n t
`;n 1k1between epoch- (n 1)and epoch-
nwithin a task- tand (2) (1=j`j)kt+1
`;1 t
`;Nk1between the last epoch of task- tand the ﬁrst epoch of task- (t+ 1) .
The training dynamics of this metric on different groups of parameters for different networks are shown in plots (a,c)
of Fig. 1. In CL, the unstable changes of parameters are mainly caused by the task shift, while the learning within each
task usually leads to smooth changes. Hence, the second metric focuses on the difference between two consecutive
tasks, e.g., the change of parameters between epoch- nof the two tasks, i.e., C`= (1=j`j)kt+1
`;n t
`;nk1. Its results
on different neural networks are displayed in plots (b,d,e) of Fig. 1.
4.2 Forgetting of Different Parameters During CL
We ﬁrst investigate and compare the training dynamics of different parameters in three types of networks. To gain
insights applicable to all CL methods, we exclude any speciﬁc CL techniques but simply apply SGD to train a model
3

--- PAGE 4 ---
0 3 6 9 12 15 18 21 24
Training epochs105
104
103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
FC layer 1
FC layer 2
FC layer 3(a) VGG(consecutive epochs)
6 9 12 15 18 21 24
Training epochs104
103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
FC layer 1
FC layer 2
FC layer 3 (b) VGG(consecutive tasks)
0 3 6 9 12 15 18 21 24
Training epochs107
106
105
104
103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
BN Mean&Var Layers
BN Weight&Bias Layers
FC layer (c) ResNet(consecutive epochs)
6 9 12 15 18 21 24
Training epochs106
105
104
103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
BN Mean&Var Layers
BN Weight&Bias Layers
FC layer
(d) ResNet(consecutive tasks)
2 3 4 5
Training epochs102
Dynamics of parametersFC layer 1
FC layer 2
FC layer 3 (e) MLP
6 8 10 12 14 16 18
Training epochs104
103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
BN Mean&Var Layers
BN Weight&Bias Layers
FC layer (f) Seq-OrganAMNIST
6 9 12 15 18 21 24
Training epochs104
103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
BN Mean&Var Layers
BN Weight&Bias Layers
FC layer
(g) ER(buffer size = 2000 )
6 8 10 12 14 16 18
Training epochs103
102
101
Dynamics of parametersConv Layer 1
Conv Block 1
Conv Block 2
Conv Block 3
Conv Block 4
BN Mean&Var Layers
BN Weight&Bias Layers
FC layer (h) Seq-PACS
Figure 1: (a-e) The training dynamics of different metrics for different groups of parameters when applying SGD in CL to train
three types of deep neural networks; The training dynamics of other scenarios in ResNet-18: (e) on a non-standard dataset; (g)
using a different CL method with a different buffer size; (h) in the domain-IL setting. Note the y-axis is of logarithmic scale.
on a sequence of tasks without any countermeasure to forgetting. Then, we extend the experiment to different CL
methods and datasets to verify whether the observations still hold.
Dynamics between Consecutive Epochs Plots (a,c) of Fig. 1 show the training dynamics of consecutive epochs for
parameters in VGG-11 and ResNet-18 when trained on Seq-CIFAR10. We partition all parameters in VGG-11 into
several groups, i.e., the bottom convolutional layer (closest to the input), convolutional layers in different blocks, and
three FC layers. Besides the groups of VGG-11, ResNet-18 applies batch-normalization (BN), which has two groups
of parameters, i.e., (1) weights and biases and (2) mean and variance. In the plots, all parameters experience more
changes at the epoch of task switching and quickly converge after a few epochs in the same task. Hence, the dynamic
patterns of this metric can be used to detect task boundaries.
Dynamics between Consecutive Tasks Plots (b,d,e) of Fig. 1 show the training dynamics of consecutive tasks
for parameters in VGG-11, ResNet-18 and MLP. We train a three-layer MLP for Seq-MNIST. Since each task in
Seq-MNIST is trained only 1epoch, the dynamics of MLP for consecutive epochs and consecutive tasks are the
same. From the plots of different neural networks, the last FC layer is more sensitive to task shift than other layers.
When BN is included in the network, BN layers’ mean and variance become the most changed parameters. These
observations are similar to studies in research domains like multi-task learning and domain adaptation [30, 31] that the
last FC layer and BN layers are task-speciﬁc and cannot be shared among tasks. In CL, the last FC layer is sensitive
because tasks in class-IL differ on their predicted classes, which are the outputs of the last FC layer. It is also intuitive
that BN layers are task-speciﬁc since the mean and variance of BN layers capture the ﬁrst and second-order moments
of the distribution for the latent representations. The variance of BN weight and bias is relatively large compared to
other layers. Please refer to Appendix. D for details.
4

--- PAGE 5 ---
One interesting observation of VGG-11 and ResNet-18 is that the sensitivity of convolutional layers increases as the
layer gets closer to the input. The reason may be that they are producing representations for the input images, whose
distribution shift directly impacts the bottom convolutional layer. The functionality of ﬁlters in the top layers is to
integrate patterns learned in the bottom layers to produce high-level patterns, so ﬁlters in the top layers are relatively
stable. In Fig. 8 of Appendix, we further study the training dynamics of each ﬁlter within a task or cross tasks in
different layers of a network. Firstly, the training dynamics of each ﬁlter in the bottom layer are much larger than that
of the top layer, which is in line with our above observation. We also ﬁnd that in the same layer, when tasks shift, the
dynamics of a small number of ﬁlters increase signiﬁcantly. These ﬁlters should be task-speciﬁc and critical to the
learning of the new task.
Dynamics on different scenarios The above empirical study is limited to SGD without applying any other CL tech-
niques and only focuses on class-IL. In the following, we extend the studies to different CL methods, non-standard
datasets, and domain-IL while ﬁxing the model to be ResNet-18. Fig. 1 (f) extends the empirical study to a medical
dataset Seq-OrganAMNIST. Compared to Seq-CIFAR-10, it differs in the number of tasks, dataset size, image size,
and data type. We further replace SGD with ER using replay buffer, whose results are reported in Fig. 1 (g). The rank-
ing order of parameter groups in terms of sensitivity stays consistent under the change of dataset and replay strategy.
In domain-IL, as shown in Fig. 1 (h), the training dynamics of different parameters are in line with our observations
in class-IL: only a small portion of parameters are task-speciﬁc. However, one difference is worth noting. Since the
output classes stay the same across tasks and only the input domain changes, the last FC layer which is the most
sensitive in class-IL, becomes equally or less sensitive than the bottom convolutional layer. Hence, the plasticity and
stability of parameters are impacted by how close they are to the changed data distributions.
Inspiration from Empirical Studies. Above studies shed light on the improvements of CL methods. (1)We compare
the sensitivity of different parameters in three types of deep neural networks and observe that only a small portion of
them are much more sensitive than others. This implies that only ﬁnetuning these task-speciﬁc (or plastic) parameters
may sufﬁce to retain the previous tasks. (2)The dynamics between consecutive epochs show that all layers experience
more changes when tasks shift, which can be used to detect task boundaries during CL. Knowing task boundaries is
a critical prerequisite for lots of CL methods. The proposed metric makes the CL problem much easier, makes these
methods more general and can contribute to better CL methods. (3)According to Fig. 8, in convolutional layers,
only a mall part of task-speciﬁc ﬁlters leads to the great change of dynamics when tasks shift. The regularization or
isolation of these ﬁlters can improve the performance of CL.
Task 1Task 2Task 3Task N…SGD + replay (replay-based method)Task 1Task 2Task 3Task N…SGD + replay + FPFTask 1Task 2Task 3Task N…SGDreplayFPFTask 1Task 2Task 3Task N…SGD + k-FPF
Figure 2: Comparison of SGD, replay-based method, FPF and k-FPF. SGD trains tasks sequentially without replay. Replay-based
methods train models on buffered and current data simultaneously. FPF ﬁnetunes the most sensitive parameters for a few iterations
using buffered data at the end of arbitrary CL methods. k-FPF periodically (regardless of task boundaries) applies FPF for ktimes
during training.
5 Forgetting Prioritized Finetuning Methods
The above empirical study of the training dynamics on parameters immediately motivates a simple but novel method
for CL, i.e., “forgetting prioritized ﬁnetuning (FPF)”, which can be applied to any existing CL method. In the more
efﬁcientk-FPF, we further remove the every-step replay and any other CL techniques but simply applies k-times
of FPF in SGD training. In Fig. 2, we provide an illustration that compares SGD, replay-based methods and our
methods. At last, we propose a metric to automatically identify sensitive parameters in each neural network. See
Appendix. A for the detailed procedure of FPF and k-FPF.
FPF to improve CL performance. FPF applies lightweight ﬁnetuning to the most task-speciﬁc parameters using the
buffered data after the training of arbitrary CL methods. Hence, it is complementary to any existing CL methods as
a correction step to remove their biases in the task-speciﬁc parameters by ﬁnetuning them on unbiased buffered data.
Thereby, it can improve the performance of any existing CL methods without causing notably extra computation.
5

--- PAGE 6 ---
k-FPF to improve CL efﬁciency and performance. FPF is a simple technique that brings non-trivial improvement,
but it is applied after the training of an existing CL method. Unfortunately, many SOTA CL methods require time-
consuming replay in every step, which at least doubles the total computation. Since only a few parameters are sensitive
during the task shift, can we develop a replay-free and lazy CL that replaces every-step replay with occasional FPF? We
proposek-FPF that applies FPF ktimes during CL as shown in Fig. 2. Without the costly experience replay, k-FPF can
still achieve comparable performance as FPF+SOTA CL methods but only requires nearly half of their computation.
We can apply k-FPF with any replay-free method, e.g., SGD, which is usually used as a lower-bound for CL methods.
We still maintain a small buffer by reservoir sampling, but it is only for FPF, so SGD never accesses it. We lazily apply
FPF on the buffer after every SGD step (in total ktimes overkSGD steps) without knowing the task boundaries.
k-FPF-CE+SGD We propose two variants of k-FPF, i.e.,k-FPF-CE+SGD and k-FPF-KD+SGD. k-FPF-CE+SGD
uses the cross-entropy loss to update the sensitive parameters during each FPF. In this paper, k-FPF-CE refers to
k-FPF-CE+SGD if not speciﬁed. The objective of FPF in k-FPF-CE is: min?L(?),E(x;y)B[lCE(y;f(x))]
where ?denotes selected groups of task-speciﬁc parameters, Brefers to the buffered data and lCEis the cross-
entropy loss.
k-FPF-KD+SGD to further improve performance Inspired by DER [4], we further propose k-FPF-KD that in-
troduces knowledge distillation (KD) [32] to the objective in k-FPF-CE. In this paper, k-FPF-KD refers to k-FPF-
KD+SGD if not speciﬁed. Same as DER, the pre-softmax responses (i.e. logits) for buffered data at training time
are stored in the buffer as well. During FPF, the current model is trained to match the buffered logits to retain the
knowledge of previous models. The objective of FPF in k-FPF-KD is: min?L(?),E(x;y)B[lCE(y;f(x))] +
E(x;z)B[lMSE(z;h(x))]wherezis the logits of buffered sample x,lMSE refers to the mean-squared loss, h(x)
computes the pre-softmax logits and is a hyper-parameter balancing the two terms. Compared to the computation of
k-FPF-CE, the additional computation of k-FPF-KD is negligible.
Selection of sensitive parameters for FPF and k-FPF
A key challenge in FPF and k-FPF is to select the task-speciﬁc parameters for ﬁnetuning. Examples of the training
dynamics for different layers of various networks are shown in plots (b,d,e) of Fig. 1, and their ranking does not
change over epochs. So we propose to select sensitive parameters for different neural networks according to their
training dynamics in the early epochs. Speciﬁcally, for each neural network, its layers are partitioned into Ggroups as
shown in Fig. 1, we calculate the sensitive score Sgfor each group of layers in the neural network by
Sg=(1=jgj)P
`2gC`
PG
g=1(1=jgj)P
`2gC`G (1)
whereC`is the training dynamics mentioned in Sec. 4.1. We calculate the ratio of sensitivity for group gover all
Ggroups in the network. Since each network consists of a different number of parameter groups, we multiply Gto
rescale the sensitivity score.
In the experiments later, under different scenarios and on various benchmarks, we evaluate the performance of FPF and
k-FPF when selecting different subsets of task-speciﬁc parameters. In a nutshell, ﬁnetuning parameters of higher sensi-
tivity achieve more improvement, which is in line with our ﬁndings in empirical studies. FPF outperforms all baselines
when parameter groups whose Sg>1are regarded as sensitive parameters and account for only 1:13%;0:32% and
0:15% of the number of all parameters in MLP, VGG-11 and ResNet-18. For k-FPF, ﬁnetuning more parameters, i.e.,
the earlier convolutional layers, achieves the best performance. This is the price of removing replay, which halves the
computational cost. We set the threshold of sensitivity score to 0:3so thatk-FPF achieves SOTA performance and
only12:40%;1:69% and24:91% of parameters in MLP, VGG-11 and ResNet-18 are regarded as sensitive parameters.
6 Experiments
In this section, to compare FPF and k-FPF with SOTA CL methods, we conduct our experiments mainly on ResNet-
18. We apply FPF and k-FPF to multiple benchmark datasets and compare them with SOTA CL baselines in terms
of test accuracy and efﬁciency. Besides, we also compare the performance of ﬁnetuning different parameters in FPF
andk-FPF and show that ﬁnetuning a small portion of task-speciﬁc parameters sufﬁces to improve CL. FPF improves
SOTA CL methods by a large margin under all these scenarios, while k-FPF achieves comparable performance with
FPF but is more efﬁcient. Please refer to the Appendix for more results and analysis.
Implementation Details. We follow the settings in [4] to train various SOTA CL methods on different datasets,
except training each task for only 5epochs, which is more practical than 50or100epochs in [4] for the streaming
setting of CL. Since the epochs are reduced, we re-tune the learning rate and hyper-parameters for different scenarios
by performing a grid-search on a validation set of 10% samples drawn from the original training set. For both FPF
6

--- PAGE 7 ---
Table 1: Test accuracy ( %) of baselines, FPF and k-FPF. “-” indicates that the algorithm is not applicable to the setting. k-FPF-KD
applies an additional knowledge distillation loss to the objective of k-FPF-CE. Bold andBold gray mark the best and second best
accuracy.
BUFFER METHODSCLASS -IL D OMAIN -IL
SEQ-ORGAN AMNIST S EQ-PATHMNIST S EQ-CIFAR-10 S EQ-TINY-IMAGE NET SEQ-PACS
JOINT 91.92 0.46 82.47 2.99 81.05 1.67 41.57 0.55 70.85 8.90
SGD 24.19 0.15 23.65 0.07 19.34 0.06 7.10 0.14 31.43 6.39
OEWC [7] 22.71 0.67 22.36 1.18 18.48 0.71 6.58 0.12 35.96 4.59
200GDUMB [33] 61.78 2.21 46.31 5.64 30.36 2.65 2.43 0.31 34.16 3.45
k-FPF-CE 75.212.03 72.883.22 57.971.53 13.76 0.72 60.70 2.81
k-FPF-KD 80.32 1.16 74.684.72 58.501.03 14.740.94 63.151.19
ER [3] 71.69 1.71 51.66 5.86 45.71 1.44 8.15 0.25 51.53 5.10
FPF +ER 76.92 2.26 67.34 2.68 57.68 0.76 13.08 0.65 65.161.97
AGEM [34] 24.16 0.17 27.93 4.24 19.29 0.04 7.22 0.15 40.54 3.43
FPF +AGEM 72.22 2.45 66.88 3.05 55.33 2.19 12.27 0.49 57.33 0.76
ICARL [12] 79.61 0.56 54.35 0.94 59.60 1.06 12.13 0.20 -
FPF +ICARL 80.28 0.58 71.20 2.19 63.360.91 16.99 0.37 -
FDR [35] 68.29 3.27 44.27 3.20 41.77 4.24 8.81 0.19 45.91 3.54
FPF +FDR 76.10 0.87 70.06 2.78 51.91 2.77 11.52 0.72 57.17 1.31
DER [4] 73.28 1.33 54.45 5.92 47.04 3.03 9.89 0.58 46.93 4.94
FPF +DER 79.63 1.21 67.29 3.75 56.67 2.19 12.65 0.60 61.49 1.37
DER++ [4] 78.22 2.05 62.00 3.79 59.13 0.81 12.12 0.69 55.75 2.02
FPF +DER++ 80.990.91 68.782.99 61.690.97 13.720.40 65.281.02
500GDUMB [33] 73.29 1.82 63.55 5.62 42.18 2.05 3.67 0.25 43.29 2.53
k-FPF-CE 81.280.71 76.72 1.94 64.35 0.87 19.57 0.37 65.90 0.72
k-FPF-KD 85.160.67 79.203.89 66.430.50 20.560.32 66.42 2.21
ER [3] 80.45 0.99 57.54 3.05 57.64 4.27 10.09 0.34 52.72 4.01
FPF +ER 84.07 1.26 69.83 2.87 65.47 2.64 18.61 0.70 64.27 1.91
AGEM [34] 24.00 0.18 27.33 3.93 19.47 0.03 7.14 0.10 35.29 4.94
FPF +AGEM 78.98 1.80 73.32 3.73 57.84 1.98 16.16 0.30 62.40 1.89
ICARL [12] 82.95 0.47 57.67 1.13 62.26 1.09 14.81 0.37 -
FPF +ICARL 83.88 0.69 73.56 3.00 67.750.67 16.690.29 -
FDR [35] 76.62 1.81 40.08 4.13 43.52 1.74 11.33 0.33 48.50 4.67
FPF +FDR 82.32 0.91 73.64 3.85 63.09 0.81 17.10 0.35 65.39 1.83
DER [4] 82.52 0.52 66.71 3.40 55.98 3.35 11.54 0.70 47.63 3.85
FPF +DER 85.180.39 74.133.12 67.52 0.83 17.34 0.53 65.69 1.66
DER++ [4] 84.25 0.47 71.09 2.60 67.06 0.31 17.14 0.66 57.77 2.54
FPF +DER++ 85.400.26 77.37 1.32 69.080.92 20.17 0.35 66.891.32
andk-FPF, we use the same optimizer, i.e., SGD with the cosine-annealing learning rate schedule, and ﬁnetune the
selected parameters with a batchsize of 32 for all scenarios. The ﬁnetuning steps for FPF and k-FPF are 300and
100, respectively. We perform a grid-search on the validation set to tune the learning rate and other hyper-parameters.
Please refer to Appendix. L for the hyper-parameters we explored.
Baseline methods. We apply FPF to several SOTA memory-based CL methods: ER, iCaRL, A-GEM [34], FDR [35],
DER and DER++. Besides, we also compare our methods with GDUMB [33] and oEWC. We take JOINT as the upper
bound for CL, which trains all tasks jointly, and SGD as the lower bound, which trains tasks sequentially without any
countermeasure to forgetting. For FPF, k-FPF, and all memory-based methods, the performance with buffer sizes 200
and 500 are reported. All results reported in Tab. 1 are averaged over ﬁve trials with different random seeds.
6.1 Main Results
FPF considerably improves the performance of all memory-based CL methods and achieves SOTA performance
over all scenarios in class-IL and domain-IL in Tab. 1. For methods with catastrophic forgetting, like AGEM, the
accuracy of FPF increases exponentially. The surge in performance illustrates that FPF can eliminate bias by ﬁnetuning
task-speciﬁc parameters to adapt to all seen tasks.
k-FPF-CE replaces the costly every-step replay with efﬁcient occasional FPF. In Tab. 1, the performance of k-FPF-
CE on Seq-PathMNIST, Seq-Tiny-ImageNet and Seq-PACS are better than the best CL methods, and its performance
on Seq-OrganAMNIST and Seq-Cifar10 are also better than most CL methods, which implies that ﬁnetuning the task-
speciﬁc parameters on a small number of buffer during SGD can help retain the previous knowledge and mitigate
forgetting, every-step replay is not necessary . In Fig. 3, the number of training FLOPs and the accuracy of different
7

--- PAGE 8 ---
ERAGEM ICARL FDR DERDER++ SGD
CL Methods0.00.51.01.52.0FLOPs1e15
0.00.20.40.60.8
Accuracy(a) Seq-PathMNIST
ERAGEM ICARL FDR DERDER++ SGD
CL Methods0.00.20.40.60.81.0FLOPs1e16
0.000.050.100.150.20
Accuracy (b) Seq-Tiny-ImageNet
ER AGEM FDR DER DER++ SGD
CL Methods02468FLOPs1e15
0.00.10.20.30.40.50.60.7
AccuracyFLOPs of CL Methods
FLOPs of FPF(Ours)+CL Methods
FLOPs of k-FPF-CE(Ours)
FLOPs of k-FPF-KD(Ours)
Accuracy of CL Methods
Accuracy of FPF(Ours)+CL Methods
Accuracy of k-FPF-CE(Ours)
Accuracy of k-FPF-KD(Ours) (c) Seq-PACS
Figure 3: Comparison of FLOPs and accuracy between FPF, k-FPF and SOTA methods. FPF improves all CL methods by
a large margin without notably extra computation. k-FPF consumes much less computation but achieves comparable
performance as FPF. A large and clear version can be found in Appendix. K.
0.00.20.40.60.81.0Accuracy
FLOPs of ER
FLOPs of FPF+ERAccuracy of ER
Accuracy of FPF+ERNumber of trainable parameters during finetune
BN FCCONV1 BN+FCBN+CONV1 FC+CONV1Basis
Basis+Block1 Basis+Block2 Basis+Block3 Basis+Block4Basis+Block1~2 Basis+Block1~3
FPF Finetuned Parameters0.00.20.40.60.81.0Accuracy
FLOPs of SGD
FLOPs of k-FPF-CEAccuracy of k-FPF-CE
Number of trainable parameters during finetune
Figure 4: Comparison of FLOPs, number of ﬁnetuned parameters, and accuracy for FPF(Top) and k-FPF(Bottom) ﬁnetuning
different combinations of parameters. All FLOPs are normalized together to (0,1], as well as the number of ﬁnetuning parameters.
“Basis” in x-label refers to “BN+FC+CONV1”. Red stars highlight the best accuracy and show both FPF and k-FPF only
require to ﬁnetune a small portion of task-speciﬁc parameters. k-FPF halves FPF’s FLOPs. A clear version can be found in
Appendix. K.
methods are reported. Compared to the training FLOPs of several CL methods, the computation cost of FPF and
k-FPF-CE is almost negligible. The overall training FLOPs of k-FPF-CE are still much less than SOTA CL methods
while its performance is better, which shows the efﬁciency of k-FPF-CE.
k-FPF-KD further improves the performance of k-FPF-CE to be comparable to FPF. k-FPF-CE proposes the
efﬁciency of CL methods, but its performance is a bit worse than that of FPF. One of the most differences between
k-FPF and FPF is the experience replay during the training of CL. Inspired by DER, we propose k-FPF-KD, which
uses knowledge distillation to match the outputs of previous models on buffered data, hence retaining the knowledge
of previous tasks. The results of k-FPF-KD in Tab. 1 show that it is comparable to FPF in most scenarios. Fig. 3 shows
that the FLOPs of k-FPF-KD are similar to k-FPF-CE but much less than other CL methods and FPF, and in some
cases, it outperforms FPF. k-FPF-KD shows SOTA performance in both efﬁciency and accuracy .
8

--- PAGE 9 ---
6.2 Comparison of ﬁnetuning different parameters in FPF and k-FPF
FPF andk-FPF get the best performance when only a small portion of task-speciﬁc parameters are ﬁnetuned.
In Fig. 4, the accuracy, training FLOPs and the number of trainable parameters during ﬁnetune of applying FPF or k-
FPF to different task-speciﬁc parameters in ResNet-18 on Seq-PathMNIST are compared. Overall different scenarios,
k-FPF only needs about half FLOPs of FPF with better performance (indicated by Red Stars). When ﬁnetuning on
different task-speciﬁc parameters, FPF performs the best when BN+FC layers are ﬁnetuned, which is only 0:127% of
all parameters (indicated by Orange Stars). This is consistent with our observations in empirical studies where BN
and FC layers are the most sensitive parameters to distribution shift. And the results show that only ﬁnetuning a small
portion of task-speciﬁc parameters can mitigate catastrophic forgetting and generalize the model.
The phenomenon for k-FPF is a little different. (1) In the bottom plot of Fig. 4, when FC layer is not selected for
ﬁnetuning in k-FPF, the performance is much worse. This is because, in class-IL, the output classes change across
tasks, so the FC layer is trained to only output the classes for the current task [36]. In contrast, when applying k-FPF
to domain-IL on Seq-PACS, where the output classes keep the same for different tasks, Fig. 7 in Appendix. G shows
that ﬁnetuning FC layer performs similarly as ﬁnetuning other parameters. Hence, the last FC layer is more sensitive
in class-IL than in Domain-IL. This is also shown in Fig. 1 (d,h). (2) As the red star indicates, k-FPF needs to ﬁnetune
a little more parameters (Block3 of convolutional layers, 18:91% of all parameters) to achieve a comparable accuracy
with FPF. Without experience replay during SGD, the model has a larger bias on the current task, and thus more
task-speciﬁc parameters are needed to be ﬁnetuned. This also indicates that such bias of task-speciﬁc parameters is
the main reason for catastrophic forgetting. When Block4 ( 75:22% of all parameters) is ﬁnetuned, since it is the most
stable group of parameters in our empirical study, the performance of k-FPF degrades.
(a) FLOPs-Accuracy in k-FPF(b) Effect of different buffer sizes and training epochs during CL for FPF
Figure 5: (a)Trade-off between FLOPs and accuracy for k-FPF with different kand(the SGD steps between two consecutive
FPF). By increasing the ﬁnetuning steps per FPF, the accuracy quickly saturates. The best trade-off is highlighted at the top-left
corner when k= 9(= 1500) .(b)Comparison between ER and FPF+ER ﬁnetuning different parameters with different buffer
sizes and number of epochs per task. In all scenarios, FPF can signiﬁcantly improve ER by only ﬁnetuning BN+FC.
6.3 Analysis of FPF and k-FPF in Different Scenarios
Different training FLOPs for k-FPF In Fig. 5(a), we study the trade-off between the training FLOPs and the accuracy
ofk-FPF on Seq-PathMNIST by changing kand the number of ﬁnetuning steps. in the legend refers to the interval
of two consecutive FPF. Fixing k,k-FPF saturates quickly as the ﬁnetuning steps increase. This implies that k-FPF is
efﬁcient on FLOPs to achieve the best performance. For experiments with small k, e.g.k=2, though the computation
required is very low, performance cannot be further improved. This implies that FPF needs to be applied on buffered
samples more frequently to mitigate forgetting. When kis large, e.g., k=41 or 121, the accuracy slightly improves
with the price of much more required computation. As the red star in the plot indicates, applying FPF every 1500
training steps can achieve the best computation-accuracy trade-off.
Different buffer sizes and training epochs for FPF The buffer size and the training epochs per task are usually crucial
in replay-based CL methods. In Fig. 5(b), when the buffer size or the number of epochs increases, the performance
of ER improves as well. However, increasing the buffer size brings more beneﬁts. When the buffer size or epochs
grow too large, the performance of ER seems to saturate and increases slowly. For all scenarios, ﬁnetuning BN+FC
layers is highly effective in alleviating the current task’s bias and promoting performance, which is consistent with our
observations from the empirical studies.
7 Conclusion
We study a fundamental problem in CL, i.e., which parts of a neural network are task-speciﬁc and more prone to
catastrophic forgetting. Extensive empirical studies in diverse settings consistently show that only a small portion
9

--- PAGE 10 ---
of parameters is task-speciﬁc and sensitive. This discovery leads to a simple yet effective “forgetting prioritized
ﬁnetuning (FPF)” that only ﬁnetunes a subset of these parameters on the buffered data before model deployment.
FPF is complementary to existing CL methods and can consistently improve their performance. We further replace
the costly every-step replay with k-times of occasional FPF during CL to improve efﬁciency. Such k-FPF achieves
comparable performance as FPF+SOTA CL while consuming nearly half of its computation. In future work, we will
study how to further reduce the memory size required by FPF.
References
[1] Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting
functions. Psychological review , 97(2):285, 1990.
[2] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science , 7(2):123–146,
1995.
[3] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro.
Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint
arXiv:1810.11910 , 2018.
[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for
general continual learning: a strong, simple baseline. arXiv preprint arXiv:2004.07211 , 2020.
[5] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Class-incremental
continual learning into the extended der-verse. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
2022.
[6] Rich Caruana. Multitask learning. Machine learning , 28(1):41–75, 1997.
[7] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan
Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International
Conference on Machine Learning , pages 4528–4537. PMLR, 2018.
[8] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Interna-
tional Conference on Machine Learning , pages 3987–3995. PMLR, 2017.
[9] Martial Mermillod, Aur ´elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the
continuum from catastrophic forgetting to age-limited learning effects. Frontiers in psychology , 4:504, 2013.
[10] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS) ,
11(1):37–57, 1985.
[11] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuyte-
laars. Online continual learning with maximally interfered retrieval. arXiv preprint arXiv:1908.04742 , 2019.
[12] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental
classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition , pages 2001–2010, 2017.
[13] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory
aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision
(ECCV) , pages 139–154, 2018.
[14] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In
Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 7765–7773, 2018.
[15] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with
hard attention to the task. In International Conference on Machine Learning , pages 4548–4557. PMLR, 2018.
[16] Quang Pham, Chenghao Liu, and Steven HOI. Continual normalization: Rethinking batch normalization for
online continual learning. In International Conference on Learning Representations , 2022.
[17] Timoth ´ee Lesort, Thomas George, and Irina Rish. Continual learning in deep networks: an analysis of the last
layer. arXiv preprint arXiv:2106.01834 , 2021.
[18] Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and Gholamreza Haffari. Pretrained
language model in continual learning: A comparative study. In International Conference on Learning Represen-
tations , 2022.
[19] Vinay V Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden representations
and task semantics. arXiv preprint arXiv:2007.07400 , 2020.
10

--- PAGE 11 ---
[20] Chiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint arXiv:1902.01996 ,
2019.
[21] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint
arXiv:1904.07734 , 2019.
[22] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in
neural information processing systems , 30:6467–6476, 2017.
[23] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pﬁster, and Bingbing Ni.
Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classiﬁcation. arXiv preprint
arXiv:2110.14795 , 2021.
[24] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generaliza-
tion. In Proceedings of the IEEE international conference on computer vision , pages 5542–5550, 2017.
[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017.
[26] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Sequential learning for domain generalization. In
European Conference on Computer Vision , pages 603–619. Springer, 2020.
[27] Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Tom van Sonsbeek, Xiantong Zhen, Dwarikanath Mahap-
atra, Marcel Worring, and Cees GM Snoek. Lifelonger: A benchmark for continual disease classiﬁcation. arXiv
preprint arXiv:2204.05737 , 2022.
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.
[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
[30] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adap-
tation networks. In International conference on machine learning , pages 97–105. PMLR, 2015.
[31] Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-speciﬁc batch normal-
ization for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on Computer Vision
and Pattern Recognition , pages 7354–7362, 2019.
[32] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2(7), 2015.
[33] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress
in continual learning. In European conference on computer vision , pages 524–540. Springer, 2020.
[34] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning
with a-gem. arXiv preprint arXiv:1812.00420 , 2018.
[35] Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space.
arXiv preprint arXiv:1805.08289 , 2018.
[36] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a uniﬁed classiﬁer incremen-
tally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 831–839, 2019.
[37] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[38] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[39] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7(7):3, 2015.
[40] Riccardo V olpi, Diane Larlus, and Gr ´egory Rogez. Continual adaptation of visual representations via domain
randomization and meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4443–4453, 2021.
11

--- PAGE 12 ---
A Detailed procedure of FPF and k-FPF
The detailed algorithm of FPF, k-FPF and reservoir sampling are shown in Alg. 1, Alg. 2 and Alg. 3. For FPF, if the
existing CL method belongs to replay-based methods, the buffer will be reserved according to this method. Otherwise,
FPF will reserve the buffer by reservoir sampling. For both FPF and k-FPF, the data of each task may be trained for
more than one epoch.
Algorithm 1 FPF
Input : DatasetD, an existing CL method M, early epochs iteration I, number of ﬁnetuning iterations K
Output : A well-trained CL model
Initialize: BufferB ;, training iterations i= 0
1for(x,y)2Ddo
2 Run a batch of CL method M;
3i i+ 1;
4 ifi=Ithen
5 Identify sensitive groups of parameters fggby Eq. 1.
6 end
7B←reservoir(B, (x,y)).
8end
9Finetune the sensitive groups of parameters fggforKiterations on B.
Algorithm 2 k-FPF
Input : DatasetD, CL method SGD, early epochs iteration I, interval of FPF , number of ﬁnetuning iterations K
Output : A well-trained CL model
Initialize: BufferB ;, training iterations i= 0
1for(x,y)2Ddo
2 Run a batch of SGD on ( x,y);
3i i+ 1;
4 ifi=Ithen
5 Identify sensitive groups of parameters fggby Eq. 1.
6 end
7 ifi%= 0then
8 Finetune the sensitive groups of parameters fggforKiterations on B.
9 end
10B←reservoir(B, (x,y)).
11end
12Finetune the sensitive groups of parameters fggforKiterations on B.
Algorithm 3 RESERVOIR SAMPLING
Input : BufferB, number of seen examples S, examplex, labely
Output : An updated buffer B
1ifjBj>S then
2B[S] (x;y)
3else
4k=randomInteger(min = 0, max =S));
5 ifk<jBjthen
6B[k] (x;y)
7 end
8end
12

--- PAGE 13 ---
B Comparison between FPF and the method ﬁnetuning all parameters
In Tab.2, we compare FPF with FPF-ALL (which ﬁnetunes all parameters) when applied to different CL methods for
two types of CL, i.e., class-IL and domain-IL. The results show that FPF consistently achieves comparable or slightly
higher accuracy than FPF-ALL by spending signiﬁcantly fewer FLOPs. This demonstrates the advantage of FPF on
efﬁciency.
Table 2: Comparison of accuracy and FLOPs between FPF and FPF-ALL(ﬁnetuning all parameters).
MethodsSeq-PathMNIST Seq-PACS
Accuracy FLOPs(B) Accuracy FLOPs(B)
k-FPF-CE 76.721.94 21.35 65.90 0.72 148.25
k-FPF-ALL-CE 75.742.91 43.95 64.48 2.23 174.60
FPF+ER 69.83 2.87 4.68 64.27 1.91 24.39
FPF-ALL +ER 70.64 4.00 8.79 63.81 2.33 34.92
FPF+AGEM 73.32 3.73 7.07 62.40 1.89 18.47
FPF-ALL +AGEM 74.80 3.12 8.79 62.65 1.65 34.92
FPF+iCaRL 73.56 3.00 4.27 - -
FPF-ALL +iCaRL 72.77 4.12 8.79 - -
FPF+FDR 73.64 3.85 2.94 65.39 1.83 11.70
FPF-ALL +FDR 74.24 1.48 8.79 64.88 2.28 34.92
FPF+DER 74.13 3.12 2.96 65.69 1.66 18.47
FPF-ALL +DER 74.54 3.19 8.79 66.22 0.87 34.92
FPF+DER++ 77.37 1.32 4.68 66.89 1.32 24.39
FPF-ALL +DER++ 77.16 1.45 8.79 65.19 1.33 34.92
C Performance of various methods during the training of CL
In Tab. 3 and Tab. 4, the average test accuracy of previous tasks at the end of each task during the training of CL on
Seq-PathMNIST and Seq-PACS is reported. The results show that during training, k-FPF can always achieve the best
performance among various CL methods. Whenever the training stops, k-FPF can always achieve a model performing
well on previous tasks.
Table 3: The average accuracy of previous tasks at the end of each task during the training of CL on Seq-PathMNIST.
Methods Task 1 Task 2 Task 3 Task 4
k-FPF-CE 99.95 0.04 95.41 1.98 81.92 2.26 76.72 1.94
ER 98.62 1.59 83.06 3.12 74.60 3.18 57.54 3.05
AGEM 99.71 0.19 46.58 3.13 36.12 3.17 27.33 3.93
iCaRL 99.98 0.02 86.86 5.47 66.62 5.64 57.67 1.13
FDR 99.97 0.06 48.06 0.82 55.75 6.55 40.08 4.13
DER 99.98 0.02 91.92 3.42 76.50 5.77 66.71 3.40
DER++ 99.95 0.06 94.06 6.14 80.35 3.32 71.09 2.60
D Detailed dynamics of BN weights and bias in different groups
In Fig. 6, the training dynamics of BN weights and biases in different groups are reported. This provides a ﬁne-grained
explanation of the phenomenon in Fig. 1 (d): the bottom BN layer is much more sensitive and task-speciﬁc than other
BN layers. Consistent with convolutional layers, the deep BN layers are less sensitive to task drift than the shallower
ones.
In a neural network, lower layers are closer to the input. Since the distribution of the inputs changes, the parameters of
lower convolutional layers change sensitively to adapt to the distribution shift. The weights and biases of BN, which
13

--- PAGE 14 ---
Table 4: The average accuracy of previous tasks at the end of each task during the training of CL on Seq-PACS.
Methods Task 1 Task 2 Task 3 Task 4
k-FPF-CE 70.94 2.02 73.75 2.68 62.37 0.49 65.90 0.72
ER 56.64 9.04 54.34 9.44 46.79 8.48 52.72 4.01
AGEM 47.34 7.35 38.02 5.82 32.70 7.13 35.29 4.94
FDR 58.59 4.36 54.00 4.01 46.38 4.80 48.50 4.67
DER 48.49 9.40 45.28 8.88 34.48 7.81 47.63 3.85
DER++ 55.33 7.45 64.43 6.50 50.19 7.30 57.77 2.54
are the scale and shift of the featuremap, will change along with the convolutional parameters to adjust the distribution
of the output featuremap. In the deeper layers, the functionality of each ﬁlter is relatively stable, so the distribution of
the featuremap need not change drastically.
6 9 12 15 18 21 24
Training epochs104
103
Dynamics of parametersBN Layer 1
BN Weight&Bias Block 1
BN Weight&Bias Block 2
Bn Weight&Bias Block 3
Bn Weight&Bias Block 4
Figure 6: The training dynamics of different groups of BN weights and biases in ResNet-18.
E Results of other neural networks
In Tab. 5, the results of various CL benchmarks and FPF on MLP and VGG-11 are reported. Similar to the results in
Tab.1, by ﬁnetuning the most sensitive parameters in MLP and VGG-11, FPF can further improve the performance of
all SOTA CL methods and achieve the best performance. k-FPF-CE also achieves comparable performance as FPF +
SOTA methods. Our methods can be generalized to various neural networks.
F Comparison between k-FPF, DER and DER++ with a large number of epochs for each
task.
We compare the accuracy and FLOPs of our methods with the original results in [4] when allowing a large number of
epochs on the same data for each task. The results are shown in Tab. 6. R-MNIST is a domain-IL dataset applied in
[4]. In both class-IL and domain-IL, k-FPF-CE is comparable to DER++ and k-FPF-KD is better than DER++ on the
accuracy but spends much less FLOPs. These results demonstrate that our methods can outperform SOTA methods in
various scenarios.
G Performance of ﬁnetuning different parameters for FPF and k-FPF on domain-IL
dataset
In Figure 7, the performance of ﬁnetuning different parameters for FPF and k-FPF on domain-IL dataset Seq-PACS
are reported.
14

--- PAGE 15 ---
Table 5: Classiﬁcation results for CL benchmarks and FPF on MLP and VGG-11. Bold and underline indicate the
best and second-best algorithms in each setting.
BUFFER METHODSCLASS -IL
SEQ-MNIST (MLP) S EQ-CIFAR10(VGG-11)
JOINT 95.58 0.33 69.50 0.73
SGD 19.64 0.07 18.71 0.33
OEWC 20.69 1.34 18.46 0.23
500GDUMB 90.60 0.37 41.65 0.78
k-FPF-CE 90.63 0.57 55.45 1.16
ER 86.73 1.03 46.27 1.18
FPF+ER 91.15 0.16 53.48 1.08
AGEM 51.03 4.94 19.40 1.09
FPF+AGEM 89.26 0.52 29.84 1.37
ICARL 58.12 1.94 45.63 1.94
FTF+ ICARL 80.83 0.49 48.03 0.65
FDR 83.79 4.15 45.56 2.23
FPF+FDR 89.67 0.37 55.59 1.56
DER 91.17 0.94 51.12 2.47
FPF+DER 91.250.89 57,46 1.15
DER++ 91.18 0.74 47.60 3.23
FTF+DER++ 91.22 0.67 54.690.73
0.00.20.40.60.81.0Accuracy
FLOPs of ER
FLOPs of FPF+ERAccuracy of ER
Accuracy of FPF+ERNumber of trainable parameters  during finetune
BN FC
CONV1 BN+FC
BN+CONV1 FC+CONV1Basis
Basis+Block1 Basis+Block2 Basis+Block3 Basis+Block4Basis+Block1~2 Basis+Block1~3
FPF Finetuned Parameters0.00.20.40.60.81.0Accuracy
FLOPs of SGD
FLOPs of k-FPF-CEAccuracy of k-FPF-CE
Number of trainable parameters  during finetune
Figure 7: Comparison of FLOPs, number of ﬁnetuned parameters, and accuracy for FPF(Top) and k-FPF(Bottom) ﬁnetuning
different combinations of parameters. All FLOPs are normalized together to (0,1], as well as the number of ﬁnetuning parameters.
“Basis” in the x-label refers to “BN+FC+CONV1”. Red stars highlight the best accuracy and show both FPF and k-FPF only
require to ﬁnetune a small portion of task-speciﬁc parameters. k-FPF halves FPF’s FLOPs. Different from the results of k-FPF
in class-IL, in Seq-PACS, since the output classes for different tasks are always the same, the last FC layer will not have
a large bias on particular classes. Only ﬁnetuning BN or CONV1 layers for k-FPF can get comparable performance with
ER. Similar to class-IL, since experience replay is not allowed during the training of CL method SGD, a little more parameters are
required to be ﬁnetuned by k-FPF to get comparable performance with FPF (about 24:92% of all parameters).
15

--- PAGE 16 ---
Table 6: Comparison of accuracy and FLOPs of k-FPF with the original results in [4] which has a large number of training epochs
for each task.
Methods Seq-CIFAR-10 Accuracy Seq-CIFAR-10 FLOPs (B) R-MNIST Accuracy R-MNIST FLOPs (B)
k-FPF-CE 71.93 0.58 9208.85 91.15 0.29 0.64
k-FPF-KD 74.320.32 9208.85 93.610.45 0.64
DER 70.51 1.67 16726.26 92.24 1.12 1.29
DER++ 72.70 1.36 25089.39 92.77 1.05 1.93
H Details of datasets
We conduct class-IL experiments on Seq-MNIST, Seq-OrganAMNIST, Seq-PathMNIST, Seq-CIFAR-10, and Seq-
TinyImageNet. Both Seq-OrganAMNIST and Seq-PathMnist consist of 4 disjoint classiﬁcation tasks. The number of
classes per task in Seq-OrganAMNIST and Seq-PathMnist are [3, 3, 3, 2] and [3, 2, 2, 2], respectively. Seq-MNIST
(Seq-CIFAR-10) are generated by splitting the 10 classes in MNIST[37] (CIFAR-10[38]) into ﬁve binary classiﬁcation
tasks. Seq-TinyImageNet partitions the 200 classes of TinyImageNet[39] into 10 disjoint classiﬁcation tasks with 20
classes per task.
For domain-IL experiments, we use PACS dataset [24], which is widely used for domain generalization. Images in
PACS come from seven classes and belong to four domains: Paintings, Photos, Cartoons, and Sketches. In Seq-PACS
for CL, each task only focuses on one domain and the sequence of tasks is Sketches !Cartoons!Paintings!
Photos (increasing the level of realism over time) [40].
I Comparison with related works [19]
Paper “Anatomy of catastrophic forgetting: Hidden representations and task sementics” shows that freezing bottom
layers had little impact on the performance of the second task. (i) Their setting is different: our study and most CL
methods focus on the performance of ALL tasks. And it is unfair in terms of parameter amount to compare the freezing
effects of multiple layers/blocks (e.g., block 1-3) vs. one layer/block. (ii) Their result is partially consistent with ours
since their unfrozen part covers the last layer and many BN parameters, which are the most sensitive/critical part to
ﬁnetune in our paper. (iii) The rest difference is due to our ﬁner-grained study on parameters and on >2tasks, but this
paper only studies two tasks and focuses on the second. Table 7 shows the class-IL accuracy at the end of each task
if freezing different single ResNet blocks (bottom to top: block-1 to block-4). At the end of task 2, our observation is
the same as this paper and freezing bottom blocks showed little reduction of accuracy. However, at the end of tasks
3-5, their performance drops, and freezing block-1 drops the most.
Table 7: class-IL accuracy of ER at the end of each task on Seq-CIFAR-10
Task-1 Task-2 Task-3 Task-4 Task-5
No Freeze 97:520:23 80:530:80 63:960:51 58:051:91 57:032:29
Freeze conv-1 97:520:23 79:622:75 63:282:13 56:110:61 55:581:31
Freeze block-1 97:520:23 78:883:01 60:070:61 55:490:22 52:751:90
Freeze block-2 97:520:23 78:933:34 63:782:32 56:230:82 56:553:17
Freeze block-3 97:520:23 80:372:35 64:312:23 57:210:40 56:520:76
Freeze block-4 97:520:23 80:681:53 64:891:00 53:783:37 54:012:07
16

--- PAGE 17 ---
J Distribution of ﬁlters’ dynamics in different layers of neural networks
Bottom convolutional layer
Middle convolutional layer
Top convolutional layer
Figure 8: Comparison between the distribution of ﬁlters’ training dynamics within a task and that cross tasks in different convolu-
tional layers of VGG-11. When tasks shift, for all layers, only a small part of the ﬁlters in each layer experience more changes.
17

--- PAGE 18 ---
K A more clear version of Fig. 3 and Fig.4
In Fig.9 and Fig.10, to make Fig.3 and Fig.4 more concise and easy to understand, we draw the barplots of different
parts separately.
ER AGEM ICARL FDR DER DER++ SGD
CL Methods0.00.51.01.52.0FLOPs1e15
FLOPs of CL Methods
FLOPs of FPF(Ours)+CL Methods
FLOPs of k-FPF-CE(Ours)
FLOPs of k-FPF-KD(Ours)
ER AGEM ICARL FDR DER DER++ SGD0.00.20.40.60.8Accuracy
Accuracy of CL Methods
Accuracy of FPF(Ours)+CL Methods
Accuracy of k-FPF-CE(Ours)
Accuracy of k-FPF-KD(Ours)
(a) Seq-PathMNIST
ER AGEM ICARL FDR DER DER++ SGD
CL Methods0.00.20.40.60.81.0FLOPs1e16
FLOPs of CL Methods
FLOPs of FPF(Ours)+CL Methods
FLOPs of k-FPF-CE(Ours)
FLOPs of k-FPF-KD(Ours)
ER AGEM ICARL FDR DER DER++ SGD0.000.050.100.150.20Accuracy
Accuracy of CL Methods
Accuracy of FPF(Ours)+CL Methods
Accuracy of k-FPF-CE(Ours)
Accuracy of k-FPF-KD(Ours)
(b) Seq-Tiny-ImageNet
ER AGEM FDR DER DER++ SGD
CL Methods02468FLOPs1e15
FLOPs of CL Methods
FLOPs of FPF(Ours)+CL Methods
FLOPs of k-FPF-CE(Ours)
FLOPs of k-FPF-KD(Ours)
ER AGEM FDR DER DER++ SGD0.00.10.20.30.40.50.60.7Accuracy
Accuracy of CL Methods
Accuracy of FPF(Ours)+CL Methods
Accuracy of k-FPF-CE(Ours)
Accuracy of k-FPF-KD(Ours)
(c) Seq-PACS
Figure 9: Comparison of FLOPs and accuracy between FPF, k-FPF and SOTA CL methods. FPF improves all CL methods
by a large margin without notably extra computation. k-FPF consumes much less computation but achieves comparable
performance as FPF.
18

--- PAGE 19 ---
0.00.10.20.30.40.50.60.7Accuracy
Accuracy of ER Accuracy of FPF+ER
5.05.15.25.35.45.55.6FLOPs1e15
FLOPs of ER FLOPs of FPF+ER
BN FCCONV1 BN+FCBN+CONV1 FC+CONV1Basis
Basis+Block1 Basis+Block2 Basis+Block3 Basis+Block4Basis+Block1~2 Basis+Block1~3
FPF Finetuned Parameters012345678Number of Parameters1e6
Number of trainable parameters during finetune of FPF0.00.10.20.30.40.50.60.70.8
Accuracy of k-FPF-CE
2.02.22.42.62.83.03.21e15
FLOPs of SGD FLOPs of k-FPF-CE
BN FCCONV1 BN+FCBN+CONV1 FC+CONV1Basis
Basis+Block1 Basis+Block2 Basis+Block3 Basis+Block4Basis+Block1~2 Basis+Block1~3
FPF Finetuned Parameters0123456781e6
Number of trainable parameters during finetune of k-FPFFigure 10: Comparison of FLOPs, number of ﬁnetuned parameters, and accuracy for FPF(Top) and k-FPF(Bottom) ﬁnetuning
different combinations of parameters. All FLOPs are normalized together to (0,1], as well as the number of ﬁnetuning parameters.
“Basis” in the x-label refers to “BN+FC+CONV1”. Red stars highlight the best accuracy and show both FPF and k-FPF only
require to ﬁnetune a small portion of task-speciﬁc parameters. k-FPF halves FPF’s FLOPs.
19

--- PAGE 20 ---
L Hyper-parameter Search Space
In the following, we provide a list of all the hyper-parameter combinations that were considered for FPF and k-FPF.
Table 8: The hyper-parameter search space for FPF on different datasets. For all experiments of FPF, we use the same number of
batch size 32 and ﬁnetuning steps 300. The hyper-parameter spaces of ﬁnetuning different parameters in the models generated by
different CL methods are always the same for a given dataset. ft-lr refers to the learning rate during ﬁnetuning of FPF.
Dataset Hyper-parameter Values
Seq-OrganAMNIST lr [1, 0.3, 0.1, 0.03, 0.01]
Seq-PathMNIST lr [1, 0.75, 0.3, 0.05, 0.03]
Seq-CIFAR-10 lr [1, 0.3, 0.1, 0.03, 0.01]
Seq-Tiny-ImageNet lr [1, 0.5, 0.3, 0.075, 0.05]
Seq-PACS lr [1, 0.5, 0.3, 0.05, 0.03, 0.005, 0.003]
Table 9: The hyper-parameter search space for k-FPF-SGD on different datasets. For all experiments of k-FPF-SGD, we use the
same number of batch size 32 and ﬁnetuning steps 100. The hyper-parameter spaces of ﬁnetuning different parameters are always
the same for a given dataset. lr refers to the learning rate during training of CL method SGD. ft-lr refers to the learning rate during
ﬁnetuning.
Dataset Hyper-parameter Values
Seq-OrganAMNIST lr [0.2, 0.15, 0.1, 0.075]
ft-lr [0.5, 0.2, 0.15, 0.1]
Seq-PathMNIST lr [0.05, 0.03, 0.01]
lr [0.1, 0.075, 0.05, 0.03, 0.01]
Seq-CIFAR-10 lr [0.05, 0.03, 0.01]
ft-lr [0.075, 0.05, 0.03, 0.01]
Seq-Tiny-ImageNet lr [0.075, 0.05, 0.03]
ft-lr [0.1, 0.075, 0.05]
Seq-PACS lr [0.05, 0.03, 0.01]
ft-lr [0.075, 0.05, 0.03, 0.0075]
20

--- PAGE 21 ---
Table 10: The hyper-parameter search space for k-FPF-KD on different datasets. For all experiments of k-FPF-KD, we use the
same number of batch size 32 and ﬁnetuning steps 100. The hyper-parameter spaces of ﬁnetuning different parameters are always
the same for a given dataset. lr refers to the learning rate during training of CL method SGD. ft-lr refers to the learning rate during
ﬁnetuning.is the hyper-parameter to balance the two losses.
Dataset Hyper-parameter Values
Seq-OrganAMNIST lr [0.2, 0.15, 0.1, 0.075]
ft-lr [0.5, 0.2, 0.15, 0.1]
 [1, 0.5, 0.2, 0.1]
Seq-PathMNIST lr [0.05, 0.03, 0.01]
lr [0.1, 0.075, 0.05, 0.03, 0.01]
 [1, 0.5, 0.2, 0.1]
Seq-CIFAR-10 lr [0.05, 0.03, 0.01]
ft-lr [0.075, 0.05, 0.03, 0.01]
 [0.5, 0.2, 0.1]
Seq-Tiny-ImageNet lr [0.075, 0.05, 0.03]]
ft-lr [0.1, 0.075, 0.05]
 [1, 0.5, 0.2]
Seq-PACS lr [0.05, 0.03, 0.01]
ft-lr [0.075, 0.05, 0.03, 0.0075]
 [1, 0.5 0.2 0.1]
21
