# 2105.00157.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2105.00157.pdf
# Kích thước tệp: 4108919 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Một Khung Deep Learning cho Học Máy Suốt Đời
Charles X. Ling CHARLES .LING @UWO .CA
Tanner Bohn TBOHN @UWO .CA
Đại học Western
1151 Richmond St.
London ON, N6A 3K7, Canada

Tóm tắt
Con người có thể học nhiều khái niệm và kỹ năng khác nhau một cách tăng dần trong suốt cuộc đời của họ trong khi thể hiện nhiều tính chất mong muốn, như học liên tục mà không quên, chuyển giao kiến thức về phía trước và ngược lại, và học một khái niệm hoặc nhiệm vụ mới chỉ với vài ví dụ. Một số hướng nghiên cứu học máy, như học máy suốt đời, học few-shot, và học chuyển giao cố gắng nắm bắt những tính chất này. Tuy nhiên, hầu hết các phương pháp trước đây chỉ có thể thể hiện các tập con của những tính chất này, thường thông qua các cơ chế phức tạp khác nhau. Trong công trình này, chúng tôi đề xuất một khung deep learning thống nhất đơn giản nhưng mạnh mẽ hỗ trợ hầu hết tất cả những tính chất và phương pháp này thông qua một cơ chế trung tâm. Các thí nghiệm trên các ví dụ đơn giản hỗ trợ các tuyên bố của chúng tôi. Chúng tôi cũng rút ra các kết nối giữa nhiều đặc điểm của việc học của con người (như mất trí nhớ và "rain man") và khung của chúng tôi.

Là các nhà học thuật, chúng tôi thường thiếu tài nguyên cần thiết để xây dựng và huấn luyện các mạng neural sâu với hàng tỷ tham số trên hàng trăm TPU. Vì vậy, trong khi khung của chúng tôi vẫn còn là khái niệm và kết quả thí nghiệm của chúng tôi chắc chắn không phải SOTA, chúng tôi hy vọng rằng khung học suốt đời thống nhất này sẽ truyền cảm hứng cho công việc mới hướng tới các thí nghiệm quy mô lớn và hiểu biết về việc học của con người nói chung.

Bài báo này được tóm tắt trong hai video YouTube ngắn: https://youtu.be/gCuUyGETbTU (phần 1) và https://youtu.be/XsaGI01b-1o (phần 2).

Từ khóa: Học máy suốt đời, Học đa nhiệm vụ, Deep learning, Học lấy cảm hứng từ con người, Hợp nhất trọng số, Mạng neural

1. Giới thiệu
Thập kỷ qua đã chứng kiến sự tăng trưởng đáng kể trong khả năng của trí tuệ nhân tạo và học máy (ML). Đặc biệt deep learning đã đạt được những thành công lớn trong nhận dạng hình ảnh y tế và chẩn đoán (Litjens et al., 2017; Shen, Wu, and Suk, 2017), các tác vụ về xử lý ngôn ngữ tự nhiên (Radford et al., 2019; Devlin et al., 2019), các trò chơi khó (Silver et al., 2017), và thậm chí cả nông nghiệp (Kamilaris and Prenafeta-Bold ´u, 2018). Tuy nhiên, các mô hình deep learning hầu như luôn cần hàng nghìn hoặc hàng triệu mẫu huấn luyện, được cung cấp cùng một lúc để huấn luyện các mô hình, để hoạt động tốt. Điều này tương phản rõ rệt với việc học của con người, thường học một khái niệm mới với một số lượng nhỏ mẫu, và tiếp tục học trong suốt cuộc đời. Các điểm yếu chính khác trong deep learning hiện tại, khi so sánh với việc học của con người, bao gồm khó khăn trong việc tận dụng kiến thức đã học trước đó để học tốt hơn các khái niệm mới (và ngược lại) và học nhiều tác vụ một cách tuần tự mà không quên những tác vụ trước đó.

Một số hướng nghiên cứu trong học có giám sát tồn tại để khắc phục những điểm yếu này. Học đa nhiệm vụ (Caruana, 1997) xem xét cách học nhiều khái niệm cùng một lúc sao cho chúng giúp đỡ lẫn nhau để được học tốt hơn. Lĩnh vực liên quan của học chuyển giao (Pan and Yang, 2009) giả định rằng một số khái niệm đã được học trước đó và chúng ta muốn chuyển giao kiến thức của chúng để hỗ trợ học các khái niệm mới. Học few-shot (Fei-Fei, Fergus, and Perona, 2006) nhằm học các tác vụ với một số lượng nhỏ dữ liệu có nhãn. Học máy suốt đời (LML) (Thrun, 1998; Thrun., 1995), còn được gọi là học liên tục (Parisi et al., 2019) hoặc học tuần tự (McCloskey and Cohen, 1989), xem xét cách học và chuyển giao kỹ năng qua các chuỗi tác vụ dài.

Tuy nhiên, hầu hết các phương pháp LML trước đây chỉ thể hiện các tập con của những tính chất giống con người này bằng các cơ chế phức tạp khác nhau. Chúng tôi tin rằng việc học khái niệm suốt đời của con người với các ví dụ có nhãn có thể sử dụng một (hoặc một tập nhỏ) cơ chế. Điều này là bởi vì con người học nhiều khái niệm trong đời họ, và quá trình này liên tục mà không có ranh giới rõ ràng. Ví dụ, học một khái niệm mới (như một hình dạng mới) hoặc cập nhật một khái niệm đã học (như nhận thêm dữ liệu của một hình dạng đã học trước đó) có thể ảnh hưởng tinh tế đến các khái niệm đã học trước và sau. Sử dụng thuật ngữ LML, ảnh hưởng tinh tế này là do chuyển giao về phía trước và ngược lại, không quên một số khái niệm trước đó, và quên một cách nhẹ nhàng những khái niệm khác. Những ảnh hưởng này dường như kết hợp một cách liền mạch ở con người, cung cấp khả năng học suốt đời hiệu quả. Vì vậy, chúng tôi muốn tìm một khung học tập với một cơ chế trung tâm có thể triển khai liền mạch những ảnh hưởng như vậy, và thể hiện nhiều tính chất học suốt đời giống con người.

[Hình 1: Tổng quan về khung thống nhất của chúng tôi: bằng cách kết hợp một cơ chế hợp nhất trung tâm với các công cụ bổ sung, chúng tôi có thể thể hiện nhiều tính chất học suốt đời và thể hiện các kết nối chặt chẽ với các thiết lập ML khác...]

Trong bài báo này, chúng tôi đề xuất một khung thống nhất như vậy với một cơ chế trung tâm được hỗ trợ bởi các cơ chế khác như mở rộng mạng và luyện tập lại. Một cái nhìn tổng quan cấp cao về khung được mô tả trong Hình 1. Chúng tôi thể hiện đặc tính thống nhất của nó bằng cách thảo luận về cách nó có thể minh họa nhiều tính chất LML mong muốn như không quên, chuyển giao về phía trước và ngược lại, và quên nhẹ nhàng (Mục 3). Để hỗ trợ khung của chúng tôi, chúng tôi bao gồm các kết quả thí nghiệm bằng chứng khái niệm cho các tính chất LML của chúng tôi trong Mục 4. Chúng tôi cũng thảo luận về các kết nối với các thiết lập ML khác, và rút ra những điểm tương đồng với nhiều đặc điểm được thấy trong việc học của con người, như mất trí nhớ và "rain man" trong Mục 5.5. Chúng tôi hy vọng rằng quan điểm này có thể làm sáng tỏ việc học của con người.

Bài báo của chúng tôi khác với hầu hết các bài báo deep learning nhằm đạt được kết quả hiện đại nhất. Thay vào đó, bài báo của chúng tôi thể hiện tính tổng quát của một khung mới có thể thể hiện nhiều tính chất, áp dụng cho nhiều thiết lập ML, và bao gồm các công trình trước đó. Chúng tôi hy vọng nó truyền cảm hứng cho nhiều nhà nghiên cứu tham gia vào các chủ đề liên quan khác nhau hướng tới hiểu biết tốt hơn về ML, trí tuệ tổng quát của con người, và việc học của con người.

--- TRANG 2 ---
2. Học Suốt Đời và các Tính chất của nó

Trong mục này, chúng tôi sẽ mô tả thiết lập LML của chúng tôi và mối quan hệ của nó với các thiết lập học khác. Sau đó chúng tôi sẽ thảo luận về một tập hợp rộng các tính chất LML quan trọng. Cuối cùng, chúng tôi sẽ cung cấp một so sánh về các phương pháp phổ biến cho LML.

2.1 Thiết lập Học Suốt Đời

Trong thiết lập suốt đời của chúng tôi, chúng tôi chủ yếu xem xét các tác vụ phân loại tăng dần theo nhiệm vụ, trong đó các lô dữ liệu cho các tác vụ mới đến một cách tuần tự. Nghĩa là, một chuỗi (T0;D0);(T1;D1);::: được cung cấp, trong đó Di là dữ liệu huấn luyện có nhãn của tác vụ Ti. Các mô hình phân loại cho (T0;T1;:::;Tj) phải được học và hoạt động trước khi (Tj+1;Dj+1) đến. Điều này mô hình hóa quá trình tăng dần của việc học suốt đời của con người.

Như một ví dụ chạy đơn giản cho bài báo này, chúng tôi giả định rằng chuỗi các tác vụ phân loại là học phân loại các số và chữ cái viết tay, bắt đầu với T0="0", sau đó T1="1", sau đó T2="2", và tiếp tục, như thấy trong Hình 2. Sẽ rõ ràng sớm tại sao chúng tôi sử dụng các chữ cái viết tay "O" và "Z" trong ví dụ chạy của chúng tôi.

[Hình 2: Chuỗi các tác vụ phân loại nhị phân được sử dụng trong bài báo của chúng tôi như một ví dụ chạy...]

Thiết lập LML của chúng tôi khác biệt với hầu hết các thiết lập học có giám sát và học đa nhiệm vụ theo hai cách quan trọng:

1. Dữ liệu cho tác vụ Tj chỉ có sẵn khi học Tj (và không trước đó). Tương tự, dữ liệu cho các tác vụ Tj+1 và tiếp theo chưa có sẵn, nhưng các bộ phân loại cho Tác vụ T0 đến Tj phải được xây dựng. Nghĩa là, không thể chờ đến khi tất cả dữ liệu có sẵn để xây dựng bộ phân loại, như trong "chế độ lô" của việc học.

2. Cũng đáng xem xét cho ví dụ đơn giản này là nơi các ví dụ âm của mỗi tác vụ bao gồm một số mẫu dương của các tác vụ đã đến trước đó (vì dữ liệu này sẽ được cung cấp).

3

--- TRANG 3 ---
2. Khi xây dựng một bộ phân loại mới cho Tj, thuật toán LML nên tránh, càng nhiều càng tốt, việc sử dụng dữ liệu từ các tác vụ trước đó; nếu không, khi học tác vụ cuối cùng sử dụng dữ liệu của tất cả các tác vụ trước đó, nó lại trở thành "chế độ lô" truyền thống.

Chúng tôi tin rằng thiết lập học suốt đời của chúng tôi phù hợp chặt chẽ với cách con người học các khái niệm theo trình tự trong suốt cuộc đời của họ.

Thiết lập LML của chúng tôi để học một khái niệm tại một thời điểm có thể dễ dàng được mở rộng để học các bộ phân loại đa lớp theo trình tự. Ví dụ, tác vụ đầu tiên có thể học phân loại các động vật có vú khác nhau (được cung cấp dữ liệu về động vật có vú), và sau đó học phân loại các loài chim khác nhau (được cung cấp dữ liệu về chim), và tiếp tục. Điều này được minh họa trong Hình 3. Một hướng mở rộng khác là LML "bất cứ lúc nào", trong đó dữ liệu của bất kỳ tác vụ nào có thể được nhận vào bất kỳ lúc nào, và bộ phân loại liên quan phải được cập nhật với việc sử dụng tối thiểu dữ liệu và can thiệp vào các bộ phân loại đã học khác.

[Hình 3: Một ví dụ về chuỗi các tác vụ đa lớp, đầu tiên học phân loại các động vật có vú khác nhau và sau đó là chim.]

2.2 Tính chất Học Suốt Đời

Ở đây chúng tôi thảo luận ở cấp độ cao một số tính chất mà một phương pháp LML lý tưởng sẽ thể hiện. Chúng tôi sẽ sử dụng ví dụ chạy từ Hình 2 về việc học một chuỗi các bộ phân loại nhị phân cho "0", "1", "2", "3", "O", và "Z".

Học liên tục và triển khai Như đã đề cập trước đó, trước khi bắt đầu học một tác vụ mới, Tj, phương pháp LML của chúng tôi nên có thể hoạt động tốt trên tất cả các tác vụ trước đó, T<j. Dữ liệu, Dj, cho Tj, chỉ có sẵn khi học Tj. Trong khi học Tj, LML nên tránh sử dụng dữ liệu tác vụ trước đó, D<j. Điều này trái ngược với học đa nhiệm vụ (lô) tiêu chuẩn, nơi tất cả dữ liệu của tất cả các tác vụ được sử dụng để huấn luyện cùng một lúc. Điều kiện học liên tục này đảm bảo rằng mô hình là 1) hữu ích, vì mỗi tác vụ phải được học đến mức hiệu suất chấp nhận được bất cứ khi nào dữ liệu có sẵn, 2) linh hoạt, ở chỗ các tác vụ mới có thể được điều chỉnh liên tục, 3) hiệu quả, ở chỗ các tác vụ được học với hiệu quả tính toán và dữ liệu cao, và 4) giống con người, ở chỗ con người dường như học theo cách liên tục tương tự trong suốt cuộc đời của họ. Xem Mục 3.2 để biết chi tiết.

Không quên Đây là khả năng tránh quên thảm khốc (McCloskey and Cohen, 1989), nơi việc học Tj gây ra mất mát nghiêm trọng về hiệu suất trên T<j. Lý tưởng nhất, việc học Tj khi chỉ sử dụng dữ liệu của Tj sẽ không ảnh hưởng tiêu cực đến T<j. Ví dụ, việc học bộ phân loại cho T1 của "1" không nên làm hiệu suất trên tác vụ trước đó của "0" giảm, khi bộ phân loại cho tác vụ "0" không được huấn luyện vào thời điểm đó. Xem Mục 3.2 để biết chi tiết.

4

--- TRANG 4 ---
Chuyển giao về phía trước Đây là khả năng học các tác vụ mới, Tj, dễ dàng và tốt hơn sau các tác vụ tương tự đã học trước đó, T<j. Điều này cũng được gọi là chuyển giao kiến thức (Pan and Yang, 2009). Đạt được chuyển giao về phía trước dương đủ cũng cho phép học few-shot của các khái niệm sau này. Ví dụ, việc học phân loại "0" đầu tiên nên cho phép tác vụ sau này của "O" được học nhanh hơn, vì chúng rất giống nhau về mặt hình ảnh. Mặt khác, chuyển giao giữa các tác vụ không tương tự có thể dẫn đến chuyển giao âm, làm suy giảm hiệu suất của tác vụ mới. Xem Mục 3.3 để biết chi tiết.

Giảm nhầm lẫn Các thuật toán phân loại thường tìm tập tối thiểu các đặc trưng phân biệt cần thiết để học các tác vụ hiện tại. Khi việc huấn luyện của mỗi tác vụ mới được thực hiện riêng lẻ với dữ liệu huấn luyện của nó, độ chính xác kiểm tra có thể cao khi kiểm tra trên dữ liệu kiểm tra riêng của nó. Tuy nhiên, khi được kiểm tra trên dữ liệu của tất cả các tác vụ, có thể xảy ra nhầm lẫn. Ví dụ, khi học chuỗi các tác vụ của "0", "1", "2", "3", "O", và "Z", hai tác vụ của "0" và "O" có thể bị nhầm lẫn (hình ảnh của "0" có thể được dự đoán là "O", và ngược lại). Tương tự cho "2" và "Z". Trong những trường hợp như vậy, chúng ta sẽ cần giải quyết sự nhầm lẫn giữa các cặp tác vụ tương tự.

Trong việc học suốt đời của con người, loại nhầm lẫn này cũng có thể xảy ra. Ví dụ, khi gặp những người mới, nếu hai người đầu tiên khác biệt rõ ràng về mặt hình ảnh (như rất cao so với rất thấp) chúng ta có thể chỉ dựa vào đặc trưng này để phân biệt họ. Tuy nhiên, nếu có thêm người đến và họ tương tự với hai người đầu tiên, ban đầu chúng ta có thể nhầm lẫn họ và phải tìm các chi tiết tinh tế hơn để giảm nhầm lẫn, hoặc để phân biệt duy nhất họ. Trong trường hợp cực đoan khi chúng ta gặp cặp song sinh giống hệt nhau, có thể cần nỗ lực đáng kể để học các chi tiết cần thiết (bằng cách sử dụng lại dữ liệu hình ảnh khuôn mặt của họ) để giải quyết sự nhầm lẫn. Xem Mục 3.4 để biết chi tiết.

Quên nhẹ nhàng Một tính chất có giá trị trái ngược với không quên là quên nhẹ nhàng (Aljundi et al., 2018), thường thấy ở con người. Trong khung của chúng tôi, việc học các tác vụ mới đòi hỏi dung lượng mô hình bổ sung, và khi việc mở rộng đủ không thể thực hiện được, mô hình có thể chuyển sang quên nhẹ nhàng các tác vụ không quan trọng để giải phóng dung lượng cho các tác vụ mới. Xem Mục 3.2 và 3.5 để biết chi tiết.

Chuyển giao ngược Đây là chuyển giao kiến thức từ Tj đến T<j, hướng ngược lại so với chuyển giao về phía trước. Khi học một tác vụ, Tj, nó có thể giúp cải thiện hiệu suất của T<j. Điều này giống như một "ôn tập" trước kỳ thi cuối cùng sau khi tài liệu của tất cả các chương đã được dạy và học. Tài liệu sau này thường có thể giúp hiểu rõ hơn tài liệu trước đó. Xem Mục 3.6 để biết chi tiết.

Học giống con người Là một loại tiêu chí đánh giá mới cho LML, chúng ta có thể xem xét mức độ một phương pháp có thể dự đoán hành vi học của con người. Nếu một phương pháp hoặc khung LML cũng có thể cung cấp sức mạnh giải thích và phù hợp với các đặc điểm của việc học con người (như nhầm lẫn hoặc chuyển giao kiến thức trong các tình huống nhất định), nó sẽ có giá trị trong các lĩnh vực ngoài ML. Chúng tôi thảo luận về những kết nối như vậy giữa khung của chúng tôi và việc học của con người trong Mục 5.5.

2.3 So sánh các Phương pháp Học Suốt Đời khác nhau

Các cơ chế được sử dụng trong công việc trước đó để thực hiện LML có xu hướng rơi vào ba loại, và chúng thường chỉ có thể thể hiện các tập con của các tính chất LML như đã thảo luận trước đó. Cơ chế đầu tiên, phát lại, thường hoạt động bằng cách lưu trữ dữ liệu tác vụ trước đó và huấn luyện trên nó cùng với dữ liệu tác vụ mới (Rebuffi et al., 2017; Isele and Cosgun, 2018; Chaudhry et al., 2019; Wu et al., 2019). Do sự không hiệu quả về dữ liệu và tính toán của nó, chúng tôi coi nó thường không phải là một cơ chế học giống con người. Một ngoại lệ của điều này là phát lại tạo sinh, nơi việc lưu trữ các mẫu tác vụ trước đó được thay thế bằng một mô hình tạo sinh sâu được huấn luyện để tạo ra các mẫu để xen kẽ với các mẫu tác vụ mới. Phương pháp này, đòi hỏi chi phí bộ nhớ không đổi được lấy cảm hứng từ khả năng tạo sinh của vùng hải mã linh trưởng (Shin et al., 2017; Kamra, Gupta, and Liu, 2017).

5

--- TRANG 5 ---
Cơ chế thứ hai là regularization. Cơ chế này hoạt động bằng cách hạn chế các thay đổi trọng số (làm cho chúng ít "linh hoạt" hơn) thông qua một hàm mất mát để việc học các tác vụ mới không ảnh hưởng đáng kể đến hiệu suất tác vụ trước đó (Kirkpatrick et al., 2016; Zenke, Poole, and Ganguli, 2017; Chaudhry et al., 2018; Ritter, Botev, and Barber, 2018; Li and Hoiem, 2017; Zhang et al., 2020). Chúng tôi sử dụng cơ chế này như một thành phần thiết yếu trong khung thống nhất của chúng tôi. So với các phương pháp trước đó, chúng tôi đề xuất sử dụng regularization một cách chiến lược hơn. Thay vì chỉ đơn giản là kiểm soát tính linh hoạt của trọng số để không quên, chúng tôi tận dụng nó để cũng khuyến khích chuyển giao về phía trước và ngược lại (Mục 3.3 và 3.6), giảm nhầm lẫn (Mục 3.4), và thực hiện quên nhẹ nhàng (Mục 3.5).

Cơ chế thứ ba, kiến trúc động, thường hoạt động bằng cách thêm các trọng số mới cho mỗi tác vụ và chỉ cho phép những trọng số đó được điều chỉnh (Rusu et al., 2016; Yoon et al., 2018; Xu and Zhu, 2018). Điều này thường được thực hiện mà không cần dữ liệu tác vụ trước đó và hoàn toàn giảm việc quên trong khi cũng cho phép kiến thức tác vụ trước đó tăng tốc việc học tác vụ mới. Trong khi cơ chế này là cần thiết cho LML của một chuỗi tác vụ dài tùy ý (bất kỳ mạng có kích thước cố định nào cuối cùng cũng sẽ đạt đến dung lượng tối đa), nó nên được sử dụng một cách tiết kiệm để tránh chi phí tính toán không cần thiết. Trong Mục 3.2, 3.3, và 3.4, chúng tôi mô tả cách các kiến trúc động có thể được sử dụng hiệu quả để giúp đạt được nhiều tính chất LML bằng cách kết hợp nó với cơ chế trung tâm của chúng tôi.

3. Khung Thống nhất cho Học Suốt Đời

Trong mục này chúng tôi mô tả cách khung thống nhất của chúng tôi hoạt động. Chúng tôi bắt đầu bằng việc giới thiệu cơ chế trung tâm và trong phần còn lại của mục, thảo luận về cách sử dụng cơ chế này và kết hợp nó với các cơ chế bổ sung để đạt được một số tính chất LML mong muốn được mô tả trong Mục 2.2.

Mặc dù không bị hạn chế đến một loại mạng neural cụ thể, chúng tôi chủ yếu xem xét khung của chúng tôi như được áp dụng cho các mạng neural sâu, đã trở nên phổ biến trong những năm gần đây, và là một loại mô hình ML hấp dẫn do khả năng tự động học các đặc trưng trừu tượng từ dữ liệu.

3.1 Cơ chế Hợp nhất Trung tâm

Chúng tôi đề xuất một khung LML đặt một chính sách hợp nhất như cơ chế trung tâm. Chính sách hợp nhất hoạt động thông qua một siêu tham số, bbb, kiểm soát tính linh hoạt của các tham số mô hình được học. Khi sử dụng mạng neural (như được xem xét trong bài báo này), bbb kiểm soát mức độ linh hoạt mà các trọng số có thể được sửa đổi trong quá trình gradient descent (hoặc bởi bất kỳ thuật toán cập nhật trọng số nào). Trong hồi quy, bbb kiểm soát mức độ các hệ số có thể được sửa đổi. Trong học dựa trên quy tắc, bbb có thể kiểm soát hoặc điều chỉnh mức độ các điều kiện quy tắc có thể được thêm hoặc xóa, hoặc mức độ độ tin cậy quy tắc có thể được cập nhật, và tiếp tục. Trong bài báo này, chúng tôi sẽ chủ yếu tập trung vào các mạng neural sâu.

Vì deep learning về cơ bản hoạt động thông qua việc tối thiểu hóa một hàm mất mát được định nghĩa đúng, cơ chế hợp nhất của chúng tôi về cơ bản hoạt động như một thuật ngữ regularization trong hàm mất mát. Cụ thể hơn, nếu mỗi trọng số mạng, i, được liên kết với một giá trị hợp nhất bbби₀, hàm mất mát mới, Lnew trong khi học một tác vụ với chỉ số t được định nghĩa như sau:

Lnew(θ) = Lt(θ) + Σᵢ bbbᵢ(θᵢ - θᵢᵗᵃʳᵍᵉᵗ)² (1)

6

--- TRANG 6 ---
Ở đây, θᵢᵗᵃʳᵍᵉᵗ là giá trị mục tiêu cho một trọng số được thay đổi thành. Lt là một mất mát tiêu chuẩn (như cross-entropy) trên tác vụ t. Mất mát này có hành vi sau: một bbbᵢ lớn khiến việc thay đổi θᵢ khỏi θᵢᵗᵃʳᵍᵉᵗ bị phạt mạnh trong quá trình huấn luyện. Khi bbbᵢ = ∞, chúng tôi gọi những trọng số này là "đông lạnh", và đơn giản là cố định chúng trong quá trình huấn luyện. Trong trường hợp này chúng ta có thể coi θᵢ được che trong quá trình lan truyền ngược và hoàn toàn ngăn chặn thay đổi để cải thiện hiệu quả. Ngược lại, bbbᵢ = 0 chỉ ra rằng trọng số tự do thay đổi, tức là nó "không đông lạnh".

3.2 Học Liên tục các Tác vụ Mới mà không Quên

Trong cả việc học suốt đời và học của con người, chúng ta mong muốn học các tác vụ mới sau khi học các tác vụ trước đó. Ở con người, điều này được hỗ trợ bởi khả năng liên tục phát triển các kết nối mới giữa các nơ-ron và loại bỏ các kết nối cũ (Cunha, Brambilla, and Thomas, 2010). Khi khả năng này bị tổn hại, khả năng học những điều mới của chúng ta cũng vậy. Tương tự, trong khung khái niệm của chúng tôi, chúng tôi xem xét việc học các tác vụ mới thông qua việc sử dụng chiến lược và linh hoạt việc mở rộng mạng.

Mã giả trong Thuật toán 1 mô tả cách học một tác vụ mới, Tj, trong một mạng neural sâu trong khung khái niệm của chúng tôi sau khi các tác vụ trước đó, T₀; :::; Tj₋₁, đã được học.

Thuật toán 1: Học Liên tục mà không Quên
// Cho rằng các tác vụ T₀; :::; Tj₋₁ đã được học
1 Tuyển dụng các đơn vị tự do cho Tj và không đông lạnh các trọng số mới // Liên kết xanh trong Hình 4
2 Đông lạnh trọng số của các tác vụ trước đó // Liên kết đỏ trong Hình 4 để không quên
3 Khởi tạo trọng số từ các đơn vị trước đó đến các đơn vị mới được tuyển dụng như mô tả trong Mục 3.3
// Liên kết xanh lá trong Hình 4 để chuyển giao về phía trước
4 Huấn luyện tác vụ mới Tj để tối thiểu hóa Phương trình 1 // chỉ trên dữ liệu của tác vụ mới Tj

Như thấy trong Hình 4, vai trò của trọng số đỏ là bị đông lạnh, trọng số xanh được khởi tạo ngẫu nhiên và tự do điều chỉnh, và trọng số xanh lá được khởi tạo có chọn lọc và hợp nhất để khuyến khích chuyển giao về phía trước (Mục 3.3). Phương pháp mở rộng này khác với Progressive Neural Networks (Rusu et al., 2016) ở chỗ thay vì kết nối các liên kết xanh lá với một bộ chuyển đổi và thêm đầu ra của bộ chuyển đổi vào đầu vào của lớp mới, tất cả đầu vào cho lớp mới chỉ đơn giản được nối với nhau. Khi học mỗi tác vụ phân loại mới, việc tối thiểu hóa trên hàm mất mát (Phương trình 1) được áp dụng chỉ trên bộ phân loại đó với dữ liệu cho tác vụ đó.

Một câu hỏi quan trọng cần đặt ra khi học một tác vụ mới, Tj, là cần thêm bao nhiêu nơ-ron mới. Đây là một câu hỏi khó, bị ảnh hưởng bởi nhiều yếu tố, bao gồm tác vụ mới Tj phức tạp như thế nào, và liệu các tác vụ đã học trước đó có thể giúp học Tj (chuyển giao dương; xem Mục 3.3). Trong Mục 4.1 chúng tôi sẽ đề xuất một chiến lược đơn giản và hiệu quả cho việc mở rộng mạng, và chỉ ra rằng thực sự, khi học một chuỗi tác vụ với các giá trị hợp nhất được kiểm soát như mô tả ở đây, các tác vụ đã học trước đó sẽ không bị ảnh hưởng.

Mặt khác, trong Mục 4.4, chúng tôi sẽ chỉ ra rằng nếu bbb được giảm cho một số tác vụ trước đó, hiệu suất trên tác vụ sẽ giảm. Trong trường hợp này, quên nhẹ nhàng đã xảy ra.

7

--- TRANG 7 ---
[Hình 4: Học liên tục với mở rộng mạng và không quên. Trong (a), T₀ đang được huấn luyện. Tất cả trọng số xanh được khởi tạo ngẫu nhiên và tự do điều chỉnh (bbb = 0). Trong (b), T₁ đang được học mà không quên T₀, bằng cách đông lạnh trọng số cho T₀ (liên kết đỏ; bbb = ∞). Lưu ý rằng hàm mất mát chỉ được tối thiểu hóa cho bộ phân loại "1" và dữ liệu cho "0" không cần thiết. Ở đây, liên kết xanh lá là để chuyển giao về phía trước (xem Mục 3.3), và liên kết xanh tự do điều chỉnh. Trong (c), T₃ được học mà không quên các tác vụ trước đó. Hàm mất mát chỉ được tối thiểu hóa cho bộ phân loại "3".]

3.3 Chuyển giao về Phía trước

Trong khi học liên tục mà không quên đảm bảo hiệu suất tác vụ trong quá khứ được duy trì, các tác vụ trước đó không có lợi cho việc học các tác vụ mới, một khái niệm nổi bật trong học đa nhiệm vụ và chuyển giao (Pan and Yang, 2009; Zhang and Yang, 2017), và xuất hiện trong LML như "chuyển giao về phía trước".

Chuyển giao về phía trước trong các mạng neural sâu được hỗ trợ dễ dàng trong khung LML của chúng tôi. Hình 5 minh họa một số trường hợp cần xem xét với các mức độ chuyển giao về phía trước khác nhau. Khi một tác vụ mới (như "O") đến, chúng ta đầu tiên kiểm tra xem nó có đủ tương tự với một tác vụ đã học trước đó (như "0", "1", ...). Độ tương tự có thể được ước tính theo nhiều cách khác nhau, như một thước đo độ tương tự của dữ liệu, mức độ hoạt động tốt của bộ phân loại đã học trên tác vụ mới, và tiếp tục. Trong Mục 4.2 chúng tôi giải thích cách chúng tôi đánh giá xem hai tác vụ phân loại có tương tự không chi tiết hơn. Sau đó một hoặc một số tác vụ tương tự trước đó sẽ được chọn để chuyển giao kiến thức đã học của chúng sang tác vụ mới. Một lần nữa, nhiều khả năng tồn tại. Ở đây chúng tôi mô tả một chiến lược đơn giản, được phản ánh trong Hình 5. Đối với trọng số lớp đầu ra của tác vụ mới, sao chép trọng số của lớp đầu ra từ tác vụ tương tự nhất trước đó. Và đối với các lớp trung gian, chỉ khởi tạo ngẫu nhiên trọng số xanh lá khi độ tương tự tác vụ trên một ngưỡng nào đó.

Cơ chế chuyển giao về phía trước của chúng tôi được dự định có tác dụng là nếu một tác vụ mới rất tương tự với một tác vụ trước đó, chuyển giao dương sẽ xảy ra, cho phép tác vụ mới được học với ít dữ liệu huấn luyện hơn. Điều này đạt được học few-shot. Như chúng tôi sẽ chỉ ra trong Mục 4.2, sau khi học "0", "1", "2", "3", và nếu "O" được học tiếp theo, vì "O" rất tương tự với "0", trọng số đầu ra của "0" sẽ được sao chép để học "O". Chúng tôi sẽ chỉ ra rằng "O" được học đến một lỗi kiểm tra thấp với nhiều ví dụ ít hơn. Chúng tôi cũng chỉ ra rằng nếu chuyển giao về phía trước bị "ép buộc" không chính xác từ các tác vụ không tương tự khi học "O" và "Z", lỗi dự đoán cao hơn đáng kể. Bằng cách quan sát sự khác biệt trong lỗi dự đoán qua các chiến lược chuyển giao, chúng ta có thể phân biệt giữa và định lượng chuyển giao dương và âm.

8

--- TRANG 8 ---
[Hình 5: Thêm chuyển giao về phía trước phù hợp trong học liên tục mà không quên trong khung LML của chúng tôi. Ba trường hợp đặc biệt của chuyển giao về phía trước được xem xét ở đây. Trong (a), khi tác vụ mới (như "O") rất tương tự với một tác vụ trước đó (như "0"), có thể không cần nút mới (tức là, không mở rộng mạng như trong Mục 3.2). Trọng số lớp đầu ra của tác vụ mới có thể được sao chép từ tác vụ tương tự trước đó trong quá trình khởi tạo trọng số. Lưu ý rằng những trọng số ban đầu đó vẫn có thể được tinh chỉnh trong việc tối thiểu hóa mất mát của tác vụ mới ("O" ở đây). Trong (b), nếu tác vụ mới (như "Z") đủ tương tự (theo một ngưỡng nào đó, xem sau) với một tác vụ trước đó (như "2"), chuyển giao dương được mong đợi từ tác vụ trước đó sang tác vụ mới. Ở đây, lượng mở rộng cho tác vụ mới có thể nhỏ hơn, nếu độ tương tự cao. Tất cả trọng số xanh lá và xanh sẽ được điều chỉnh khi tối thiểu hóa mất mát của tác vụ mới ("Z" ở đây). Trong (c), nếu tác vụ mới rất khác với bất kỳ tác vụ trước đó nào, các liên kết chuyển giao bị ngắt kết nối để ngăn chuyển giao âm có thể. Khi chuyển giao dương xảy ra, chúng ta mong đợi rằng các yêu cầu huấn luyện sẽ được giảm đáng kể, tức là, học few-shot sẽ xảy ra (xem thí nghiệm trong Mục 4.2).]

3.4 Giảm Nhầm lẫn

Như đã thảo luận trong Mục 2.2, trong khung LML của chúng tôi, các tác vụ được học theo trình tự, và dữ liệu cho các tác vụ mới chỉ có sẵn khi học các tác vụ mới. Trong trường hợp này, "nhầm lẫn" có thể xảy ra giữa hai tác vụ tương tự (như "0" và "O", "2" và "Z"). Điều thú vị cần lưu ý là hai tác vụ càng tương tự, chuyển giao về phía trước càng giúp ích trong việc học tác vụ mới (như thấy trong Mục 4.2), nhưng đồng thời, càng nhiều nhầm lẫn xảy ra giữa chúng. Nói chung, nhầm lẫn nhỏ có thể xảy ra giữa bất kỳ cặp hoặc tập con nào của các tác vụ phân loại, và khi nhầm lẫn trên một ngưỡng nào đó, chúng ta cần cách để giảm nó.

Chúng tôi đề xuất giải quyết nhầm lẫn như vậy theo cách từng cặp. Để giải quyết nhầm lẫn giữa Ti (như "0") và Tj (như "O"), một số dữ liệu huấn luyện của cả Ti và Tj sẽ cần thiết, dù được lưu trữ hay tạo ra. Ở đây, nhầm lẫn được đo bằng tổng lỗi được đánh giá trên Ti và Tj khi dữ liệu của cả hai tác vụ được trình bày như đầu vào. Việc huấn luyện là để tối thiểu hóa lỗi trên các tác vụ này cùng nhau, trong khi trọng số cho các tác vụ khác bị đông lạnh.

Cụ thể hơn, bất cứ khi nào nhầm lẫn giữa Ti và Tj lớn hơn một ngưỡng nào đó, ε ∈ [0,1], chúng tôi đề xuất một chiến lược hai bước để giảm nhầm lẫn. Đầu tiên, sử dụng mạng hiện tại, đồng thời tinh chỉnh trọng số lớp cuối của Ti và trọng số của Tj trên các mẫu của các tác vụ bị nhầm lẫn. Bước này được thể hiện trong Hình 6a. Khi nhầm lẫn nhỏ, chỉ riêng bước này có thể giảm nhầm lẫn xuống dưới ε. Nếu dung lượng mạng hiện tại không đủ để giải quyết nhầm lẫn, thì chúng ta có thể chuyển sang bước thứ hai, nơi chúng ta mở rộng mô hình theo một lượng nào đó, và tất cả trọng số mới bây giờ có thể được học. Bước này được phản ánh trong Hình 6b. Trong cả hai trường hợp, chỉ những trọng số liên quan đến các tác vụ bị nhầm lẫn được điều chỉnh, để lại các tác vụ khác không bị ảnh hưởng.

[Hình 6: Hai bước của quá trình giảm nhầm lẫn. Trong (a) chúng ta đầu tiên cố gắng giảm nhầm lẫn sử dụng dung lượng mạng hiện tại, đông lạnh các trọng số cần thiết để duy trì hiệu suất trên các tác vụ khác. Trong (b), nơi nhầm lẫn không thể giảm chỉ bằng tinh chỉnh, chúng ta mở rộng mạng và tất cả trọng số mới thêm có thể được học.]

Trong các thí nghiệm được mô tả trong Mục 4.3, chúng tôi thể hiện rằng cả hai bước của quá trình này đều góp phần giảm nhầm lẫn giữa các tác vụ rất tương tự.

9

Lưu ý: Tôi sẽ tiếp tục dịch các trang còn lại nếu cần thiết.

--- TRANG 9 ---
3.5 Quên Nhẹ nhàng

Khi chúng ta học ngày càng nhiều tác vụ trong khung LML của chúng tôi, mạng mở rộng có thể đạt đến một số giới hạn kích thước. Cuối cùng, chúng ta có thể đạt đến một giới hạn nơi mạng không thể cung cấp các đơn vị tự do để học một tác vụ mới tốt mà không quên đáng kể các tác vụ đã học. Trong những trường hợp như vậy, chúng ta có thể xem xét "quên nhẹ nhàng" (Aljundi et al., 2018) các tác vụ trước đó.

Ví dụ, xem xét trường hợp sau khi học T₀;T₁;T₂, có rất ít đơn vị tự do để học T₃. Nếu chúng ta đông lạnh tất cả trọng số tác vụ trước đó với bbb = ∞ và huấn luyện trên dữ liệu của T₃, chúng ta sẽ thấy rằng T₃ không thể được học tốt ngay cả sau nhiều epoch huấn luyện (xem Hình 7a và Mục 4.4). Trong trường hợp này, chúng ta có thể đặt các giá trị bbb cho một số tác vụ ít quan trọng, chẳng hạn T₀, thành nhỏ hoặc 0, và sau đó huấn luyện với các mẫu của T₃ lại, như trong Hình 7b. Trong trường hợp này chúng ta mong đợi thấy rằng lỗi trên T₃ giảm, trong khi hy sinh hiệu suất của T₀ một cách dần dần và nhẹ nhàng. Vì T₀ cũng ảnh hưởng gián tiếp đến T₁ và T₂ (mặc dù bbb của chúng = ∞), độ chính xác dự đoán của chúng cũng có thể giảm nhẹ.

Người ta có thể tự hỏi tại sao T₀ lại được chọn để bị quên nhẹ nhàng. Lựa chọn này có thể cụ thể theo miền và tình huống. Ví dụ, nếu bộ phân loại T₀ đã không được cần để đưa ra dự đoán trong một thời gian dài, "tầm quan trọng" của nó có thể thấp hơn các bộ phân loại khác, và các giá trị bbb của nó có thể được giảm dần để cho phép quên nhẹ nhàng. Điều này sẽ đặc biệt hữu ích khi không còn nơ-ron tự do để học các tác vụ mới. Trong Mục 5.5, chúng tôi sẽ thảo luận về những điểm tương đồng với việc học của con người, có vẻ như thể hiện một hiện tượng tương tự. Ví dụ, một người bạn mà bạn đã không gặp và nghĩ về trong một thời gian dài có thể có xu hướng bị quên dần dần qua nhiều năm.

[Hình 7: Quên nhẹ nhàng một tác vụ cũ khi học một tác vụ mới mà không có dung lượng mạng tự do. Trong (a) khi tối thiểu hóa mất mát trên "3", nó không thể được giảm thêm. Trong (b), tác vụ "0" được chọn để bị quên, và trọng số của nó (liên kết xanh) bị không đông lạnh (bbb được đặt nhỏ hoặc 0). Việc huấn luyện trên tác vụ "3" tiếp tục để đạt được mất mát nhỏ hơn nhiều khi các tác vụ trước đó bị quên nhẹ nhàng.]

3.6 Chuyển giao Ngược

Trong Mục 3.3 chúng tôi đã thảo luận về chuyển giao về phía trước, nơi các tác vụ trước đó giúp học tác vụ mới. Liệu các tác vụ mới có thể tương tự giúp cải thiện hiệu suất trên các tác vụ trước đó để đạt được chuyển giao ngược dương? Khung của chúng tôi có thể đạt được chuyển giao ngược như vậy bằng cách khởi tạo các liên kết chuyển giao ngược (giữa các tác vụ đủ tương tự) và tinh chỉnh trên các tác vụ mong muốn. Trong Hình 8, các liên kết hỗ trợ chuyển giao ngược từ "O" đến "0" được ghi nhãn. Trong Mục 4.5 chúng tôi đánh giá phương pháp chuyển giao ngược này. Tương tự như chuyển giao về phía trước, chúng tôi thấy rằng khi các tác vụ đủ tương tự, chuyển giao ngược hoạt động tốt. Điều này có vẻ tương tự như việc học của con người, nơi việc học các khái niệm liên quan củng cố lẫn nhau.

Mặc dù chuyển giao về phía trước và ngược lại, cũng như giảm nhầm lẫn được thực hiện với các cặp tác vụ, có thể mở rộng nó để hoạt động trên các tập con lớn hơn của các tác vụ, hoặc thậm chí tất cả các tác vụ cùng một lúc, để thực hiện "tinh chỉnh tổng thể". Việc tinh chỉnh tổng thể như vậy rõ ràng tương tự như thiết lập học đa nhiệm vụ lô tốn nhiều tài nguyên hơn, nơi chúng ta giả định rằng chúng ta có thể huấn luyện trên tất cả các tác vụ cùng một lúc. Tuy nhiên, trong thiết lập LML của chúng tôi, sau khi đã chăm sóc để đạt được chuyển giao về phía trước và giảm nhầm lẫn một cách riêng biệt và hiệu quả hơn, quá trình tinh chỉnh này có thể được thực hiện không thường xuyên và nhanh chóng.

[Hình 8: Hợp nhất trọng số trong quá trình chuyển giao ngược từ "O" đến các tác vụ trước đó ("0", "1", ...). Vì tác vụ trước đó đã được huấn luyện, quá trình này sẽ cần ít thời gian huấn luyện để hội tụ (vì chỉ các liên kết ngược được học từ đầu).]

--- TRANG 10 ---
4. Xác minh Thí nghiệm

Trong mục này chúng tôi sẽ trình bày kết quả thí nghiệm thể hiện khả năng của khung LML được đề xuất. Những thí nghiệm này chủ yếu sử dụng ví dụ chạy trong Hình 2, và do đó chúng tôi coi chúng là các thí nghiệm bằng chứng khái niệm.

Chuỗi tác vụ Chúng tôi sẽ sử dụng chuỗi tác vụ phân loại nhị phân được minh họa trong Hình 2, với các mẫu được lấy từ tập dữ liệu EMNIST cân bằng (Cohen et al., 2017). Chuỗi tác vụ này là một trường hợp tối thiểu cho phép các thí nghiệm bằng chứng khái niệm nơi chúng ta có thể chắc chắn rằng có a) không gian rõ ràng cho chuyển giao về phía trước (ví dụ từ "0" đến "O" hoặc "2" đến "Z") và b) các trường hợp rõ ràng của nhầm lẫn (ví dụ giữa "0" và "O"). Trong một chuỗi tác vụ phức tạp hơn, sẽ khó xác minh xem các cơ chế được đề xuất có hoạt động như dự định không. Tất cả kết quả sẽ được lấy trung bình qua 15 hạt giống ngẫu nhiên. Sau khi thiết lập tính khả thi đầu tiên với những thí nghiệm đơn giản này, đó là công việc tương lai của chúng tôi để xác minh các thuật toán LML được đề xuất trên các chuỗi tác vụ phức tạp hơn. Ngoài việc sử dụng các thí nghiệm đơn giản hóa, chúng tôi chỉ đặt các giá trị bbb thành ∞ hoặc 0 trong các thí nghiệm của chúng tôi, tương đương với việc che các trọng số được chọn trong quá trình gradient descent. Đó sẽ là công việc tương lai của chúng tôi để đặt bbb thành các giá trị trung gian để quan sát các hành vi dần dần hơn của các thuật toán LML của chúng tôi.

Kiến trúc và huấn luyện Chúng tôi sẽ sử dụng kiến trúc mạng với hai lớp ẩn với kích hoạt ReLU. Bộ tối ưu hóa Adam (Kingma and Ba, 2014) sẽ được sử dụng, với các siêu tham số mặc định được cung cấp bởi Keras (Chollet and others, 2015). Chúng tôi sẽ sử dụng kích thước lô 64 và huấn luyện trong 10 epoch cho mỗi tác vụ, trừ khi có quy định khác. Chi tiết thêm sẽ được đưa ra cho mỗi thí nghiệm.

Phần tóm tắt của mục này như sau. Trong Mục 4.1, chúng tôi sẽ đánh giá khung của chúng tôi về học liên tục mà không quên. Trong Mục 4.2, chúng tôi sẽ đánh giá nó trong việc tăng tốc học tác vụ với chuyển giao về phía trước. Trong Mục 4.3, chúng tôi sẽ đánh giá khả năng giảm nhầm lẫn giữa các tác vụ. Trong Mục 4.4, chúng tôi sẽ đánh giá khả năng quên nhẹ nhàng các tác vụ để cho phép học tác vụ mới. Cuối cùng, trong Mục 4.5, chúng tôi sẽ đánh giá khả năng hỗ trợ chuyển giao ngược.

4.1 Học Liên tục các Tác vụ Mới mà không Quên

Để đánh giá khả năng của khung được đề xuất để liên tục học các tác vụ mới mà không quên, chúng tôi sẽ quan sát AUC kiểm tra (diện tích dưới đặc tính hoạt động của người nhận) của mỗi tác vụ khi có thêm tác vụ được học. AUC là một thước đo phù hợp để đánh giá hiệu suất trên những tác vụ nhị phân này do dữ liệu mất cân bằng (đối với mỗi tác vụ, tỷ lệ mẫu dương so với âm là 1:4). Khi AUC trên một tác vụ vẫn không đổi sau khi nó được học ban đầu, nó chỉ ra rằng việc quên đã được tránh. Đối với thí nghiệm này, chúng tôi sẽ sử dụng tỷ lệ mở rộng mạng không đổi là 25 đơn vị mỗi lớp ẩn mỗi tác vụ. Tất cả các liên kết chuyển giao về phía trước (liên kết xanh lá trong Hình 4) sẽ được kích hoạt và khởi tạo ngẫu nhiên. Ngoài ra, chúng tôi sẽ sử dụng 100 mẫu dương cho mỗi tác vụ (và 400 âm - 100 mỗi loại ký tự trong lớp âm).

Quan sát Từ Hình 9, chúng ta có thể thấy rõ ràng rằng việc kích hoạt cơ chế không quên bằng cách đông lạnh trọng số tác vụ đã học thực sự ngăn chặn việc quên, như được chỉ ra bởi các đường nét đứt vẫn ở mức AUC không đổi. Trong khi hiệu suất ban đầu của các tác vụ sau có thể thấp hơn so với không đông lạnh, cuối cùng việc đông lạnh sẽ có hiệu quả. Ví dụ, trong khi hiệu suất tác vụ đã học "2" thấp hơn khi không quên được kích hoạt (vì có ít trọng số có thể điều chỉnh trong mạng), cuối cùng hiệu suất của mô hình không đông lạnh giảm thậm chí còn thấp hơn (một khi tác vụ "O" được học).

[Hình 9: Đánh giá khả năng không quên của khung được đề xuất. Các đường liền chỉ ra lịch sử hiệu suất của mỗi tác vụ khi không sử dụng đông lạnh. Các đường nét đứt tương ứng với hiệu suất với đông lạnh. Để tránh lộn xộn, chỉ lịch sử của ba tác vụ đầu tiên được hiển thị.]

--- TRANG 11 ---
4.2 Chuyển giao về Phía trước

Để đánh giá khả năng của khung chúng tôi hỗ trợ chuyển giao về phía trước, chúng tôi sẽ quan sát AUC kiểm tra của các tác vụ sử dụng nhiều chiến lược khởi tạo và sao chép trọng số khác nhau. Đối với thí nghiệm này, chúng tôi sẽ kích hoạt không quên và cũng sử dụng mở rộng dựa trên độ khó (lên đến tỷ lệ mở rộng tối đa 25 mỗi lớp mỗi tác vụ).

Phương pháp mở rộng dựa trên độ tương tự và độ khó Để điều chỉnh một tác vụ mới, Tj, chúng tôi mở rộng chiều rộng mạng neural theo một lượng, Nj, tỷ lệ với độ khó ước tính của tác vụ. Để tính Nj, chúng tôi đầu tiên tính độ tương tự tối đa với các tác vụ trước đó. Để tính độ tương tự giữa hai tác vụ, sim(Ti;Tj), chúng tôi đưa các mẫu dương của tác vụ mới, Tj, vào mạng hiện tại, và lấy trung bình các xác suất đầu ra bởi mô hình cho Ti. Khi độ tương tự giữa Tj và bất kỳ tác vụ trước đó nào cao (tức là các mẫu mới tương tự với những mẫu của một tác vụ trước đó), tỷ lệ ít nút được thêm vào. Nghĩa là, Nj = Nmax(1 - max i=1;:::j-1 sim(Ti;Tj)). Trong những thí nghiệm này, Nmax = 25. Trong trường hợp cực đoan nơi một tác vụ mới giống hệt (hoặc rất tương tự) với một tác vụ trước đó, có thể không cần thêm nút mới (ngoài đầu ra).

Đối với bốn tác vụ đầu tiên, chúng tôi sẽ sử dụng 100 mẫu dương cho mỗi tác vụ, nhưng chỉ 10 cho hai tác vụ cuối cùng, để sự khác biệt giữa các chiến lược chuyển giao về phía trước khác nhau có thể được làm nổi bật.

Bốn chiến lược chúng tôi so sánh như sau:

• ALLRANDOM INIT: Chiến lược này đơn giản là khởi tạo ngẫu nhiên tất cả các liên kết chuyển giao về phía trước (như trong thí nghiệm trước đó).

• ONESIMILAR: Chiến lược này đầu tiên tính độ tương tự giữa tất cả các tác vụ trước đó và tác vụ mới. Khi độ tương tự trên τ = 0:5, thì các liên kết chuyển giao về phía trước tương ứng trong các lớp trung gian được khởi tạo ngẫu nhiên. Ngoài ra, khi tác vụ trước đó tương tự nhất có độ tương tự > τ, thì trọng số lớp đầu ra được sao chép từ tác vụ đó, như được phản ánh trong Hình 5.

• ONERANDOM: Chiến lược này tương tự như chiến lược trước đó, ngoại trừ việc một tác vụ trước đó ngẫu nhiên được chọn để sao chép trọng số từ đó, độc lập với độ tương tự.

• ONEWORST: Chiến lược này chỉ khởi tạo các liên kết chuyển giao về phía trước lớp trung gian từ tác vụ ít tương tự nhất, và chỉ sao chép trọng số đầu ra từ tác vụ ít tương tự nhất.

Quan sát Từ Hình 10, chúng ta có thể thấy rằng trên hai tác vụ few-shot cuối cùng, chiến lược ONESIMILAR hoạt động tốt nhất, đạt được kết quả tương đương với các tác vụ có dữ liệu gấp 10 lần. Điều này tất nhiên có thể thông qua độ tương tự rõ ràng giữa "O" và "0" và giữa "Z" và "2". Chiến lược ONERANDOM, hoạt động tương tự như chiến lược ONESIMILAR cho các lớp trung gian, hoạt động tốt thứ hai, cho thấy rằng trong khi việc sao chép trọng số từ tác vụ tương tự nhất có lợi ích lớn nhất, việc khởi tạo các liên kết chuyển giao từ các tác vụ tương tự cũng hữu ích.

Trong Hình 11, chúng ta có thể thấy điều gì xảy ra khi, thay vì chỉ sao chép trọng số khi một tác vụ trước đó đủ tương tự (tức là ONESIMILAR), chúng ta "ép buộc" trọng số được sao chép ngay cả khi tác vụ tương tự nhất không rất tương tự. Chúng tôi gọi chiến lược này là ONEALWAYS. Sau khi học đầu tiên "0", "1", "2", và "3" với 100 mẫu dương, chúng ta thấy rằng đối với nhiều lựa chọn cho tác vụ thứ năm (một lần nữa chỉ với 10 mẫu dương), hiệu suất đạt được tốt hơn so với chiến lược cơ sở của ALLRANDOM INIT. Tuy nhiên có một số ký tự nơi chiến lược này làm tổn hại hiệu suất (ví dụ "I", "J", "X"), cung cấp các trường hợp rõ ràng của chuyển giao âm.

[Hình 10: Đánh giá cơ chế chuyển giao về phía trước được đề xuất của khung chúng tôi (được hiển thị ở đây là ONESIMILAR). Bốn chiến lược được hiển thị trong hình được mô tả trong văn bản. Bốn tác vụ đầu tiên có 100 mẫu dương mỗi tác vụ, trong khi hai tác vụ cuối chỉ có 10, yêu cầu học few-shot. Cơ chế khởi tạo và sao chép trọng số được đề xuất của chúng tôi (đỏ), hoạt động tốt nhất. Nó cho phép đạt được AUC kiểm tra cao hơn nhiều so với các chiến lược thay thế.]

[Hình 11: Sự khác biệt AUC kiểm tra giữa việc sử dụng chiến lược ONEALWAYS và ALLRANDOM INIT cho một loạt ký tự. Giá trị dương chỉ ra rằng ONEALWAYS hoạt động tốt hơn. Các ký tự ở đây được học sau "0", "1", "2", "3". Lưu ý rằng "P", "Q", "R", và "S" bị bỏ qua, vì chúng được bao gồm trong lớp âm.]

--- TRANG 12 ---
4.3 Giảm Nhầm lẫn

Thí nghiệm này sẽ đánh giá khả năng của khung chúng tôi giảm nhầm lẫn giữa các tác vụ, sử dụng chiến lược thiết lập bbb hai giai đoạn và mở rộng được minh họa trong Hình 6. Chúng tôi sẽ quan sát nhầm lẫn tối đa của hai tác vụ cuối cùng ("O" và "Z") qua các giai đoạn giảm nhầm lẫn - giá trị thấp hơn chỉ ra việc giảm nhầm lẫn tốt hơn. Nhầm lẫn giữa một cặp tác vụ là phần trăm thời gian mà các mẫu dương từ một trong hai tác vụ bị phân loại sai thành tác vụ kia. Đối với "O", nhầm lẫn tối đa hầu như chỉ đề cập đến nhầm lẫn với "0", và đối với "Z", là với "2".

Đối với thí nghiệm này, chúng tôi bây giờ sẽ sử dụng chiến lược chuyển giao về phía trước ONESIMILAR với 100 mẫu dương mỗi tác vụ. Ngoài ra, chúng tôi sẽ sử dụng ngưỡng nhầm lẫn ε = 0:1, để mỗi giai đoạn giảm nhầm lẫn sẽ chạy nếu nhầm lẫn chưa dưới 10%. Chúng tôi cũng sẽ đánh giá lượng mở rộng nhầm lẫn (giai đoạn thứ hai) của cả 5 và 10 mỗi lớp.

[Hình 12: Đánh giá hiệu quả giảm nhầm lẫn của khung được đề xuất. Chúng tôi báo cáo ở đây nhầm lẫn tối đa giữa "O" và các tác vụ trước đó ("0", "1", "2", "3") và giữa "Z" và các tác vụ trước đó. Thường xuyên nhất, nhầm lẫn xảy ra giữa "O" và "0" và giữa "Z" và "2".]

Quan sát Từ Hình 12, chúng ta có thể thấy rằng cả hai giai đoạn của cơ chế giảm nhầm lẫn (điều chỉnh đầu tiên, tức là pre-expansion và sau đó mở rộng cuối cùng với điều chỉnh, tức là post-expansion) đều góp phần giảm nhầm lẫn. Chúng ta cũng thấy rằng việc sử dụng lượng mở rộng nhầm lẫn lớn hơn góp phần giảm nhầm lẫn thêm.

4.4 Quên Nhẹ nhàng

Để đánh giá khả năng của khung chúng tôi thực hiện quên nhẹ nhàng, chúng tôi sẽ cố gắng học một tác vụ mới, "3", với một số lượng rất nhỏ các nút không đông lạnh mới (sau khi học đầu tiên "0", "1", "2"), và quan sát AUC kiểm tra của tất cả các tác vụ trong quá trình huấn luyện tác vụ này. Quên nhẹ nhàng thành công nên dẫn đến hiệu suất của tác vụ mới tăng đáng kể trong khi các tác vụ trước đó trải qua tỷ lệ quên thấp.

Đối với thí nghiệm này, chúng tôi sẽ quay lại sử dụng mở rộng không đổi 25 nút mỗi tác vụ mỗi lớp, ngoại trừ tác vụ 3, nơi chỉ một nút mới mỗi lớp sẽ được thêm vào. Chúng tôi cũng sẽ kích hoạt không quên (ngoại trừ nơi quên nhẹ nhàng được thực hiện), và khởi tạo ngẫu nhiên tất cả các liên kết chuyển giao về phía trước. Chúng tôi sẽ huấn luyện tác vụ "3" trong 10 epoch trước khi thực hiện quên nhẹ nhàng, tại điểm đó chúng tôi huấn luyện nó thêm 10 epoch.

Quan sát Từ Hình 13 và 14, chúng ta có thể thấy rằng quên nhẹ nhàng có tác động dương lớn đến việc học tác vụ thứ tư, nơi chỉ có tác vụ "0" bị quên (như trong Hình 13) hoặc "0", "1" và "2" bị quên (như trong Hình 14). Khi cả ba tác vụ trước đó bị không đông lạnh, tác vụ "3" dường như cải thiện nhanh hơn như chúng ta có thể mong đợi. Trong cả hai trường hợp, việc quên được trải qua bởi các tác vụ trước đó là chậm.

[Hình 13: Quên nhẹ nhàng với việc quên tác vụ đầu tiên ở mốc 10 epoch. Chúng tôi thấy rằng hiệu suất trên tác vụ "3" nhanh chóng tăng sau khi tác vụ 0 bị không đông lạnh, với hầu hết việc quên xảy ra trong tác vụ 0.]

[Hình 14: Quên nhẹ nhàng với việc quên ba tác vụ đầu tiên ở mốc 10 epoch. Chúng tôi thấy rằng việc học tác vụ "3" nhanh hơn sau khi không đông lạnh các tác vụ trước đó, với việc quên được phân bố nhiều hơn.]

--- TRANG 13 ---
4.5 Chuyển giao Ngược

Thí nghiệm này sẽ đánh giá khả năng của khung chúng tôi đạt được chuyển giao ngược, nơi kiến thức từ các tác vụ mới hơn được sử dụng để cho phép các tác vụ cũ hơn được học tốt hơn. Điều này được thực hiện bằng cách khởi tạo các liên kết chuyển giao ngược và thực hiện tinh chỉnh các tác vụ.

Để đánh giá khả năng của khung chúng tôi đạt được chuyển giao ngược kiến thức, chúng tôi sẽ đầu tiên học một tác vụ "0" với ít mẫu, và học một tác vụ thứ hai tương tự ("O") hoặc một tác vụ thứ hai không tương tự ("Z"). Sau khi học tác vụ thứ hai, chúng tôi sẽ khởi tạo các liên kết chuyển giao ngược (được ghi nhãn trong Hình 8) và tinh chỉnh "0". Đối với thí nghiệm này chúng tôi thấy đủ để đặt bbb = 0 chỉ cho các trọng số (cả mới và cũ) trong lớp đầu ra cho "0". Chúng tôi sẽ quan sát AUC kiểm tra của tác vụ "0" trong quá trình tinh chỉnh. Chuyển giao ngược hiệu quả sẽ dẫn đến hiệu suất của tác vụ "0" tăng nhanh hơn trong quá trình điều chỉnh khi tác vụ thứ hai chứa kiến thức liên quan ("O").

Đối với thí nghiệm này, chúng tôi sẽ sử dụng mở rộng không đổi 25 nút mỗi tác vụ mỗi lớp, với không quên và khởi tạo ngẫu nhiên tất cả các liên kết chuyển giao về phía trước (và liên kết chuyển giao ngược khi áp dụng). Chúng tôi sẽ huấn luyện tác vụ đầu tiên chỉ với 10 mẫu dương và tác vụ thứ hai với 50. Ít mẫu hơn cho tác vụ đầu tiên sẽ cho phép sự khác biệt giữa hiệu suất chuyển giao ngược cho các chuỗi tác vụ khác nhau được nhấn mạnh.

Quan sát Từ Hình 15, chúng ta có thể thấy rằng khi không có liên kết chuyển giao ngược được kích hoạt (đường xanh), hầu như không có sự gia tăng hiệu suất trong quá trình điều chỉnh. Điều này được mong đợi vì tất cả trọng số được điều chỉnh đã được huấn luyện trên dữ liệu cho "0". Trong các trường hợp nơi các liên kết chuyển giao ngược được kích hoạt (chỉ cho lớp đầu ra trong các thí nghiệm của chúng tôi), chúng ta thấy rằng hiệu suất thực sự giảm lúc đầu. Điều này là kết quả của các trọng số khởi tạo ngẫu nhiên ảnh hưởng tiêu cực đến bộ phân loại. Tuy nhiên một khi tinh chỉnh bắt đầu, khi các liên kết chuyển giao từ một tác vụ tương tự ("O" - đường đỏ), hiệu suất của "0" tăng đến mức tổng thể cao hơn so với khi chuyển giao từ một tác vụ không tương tự ("Z" - đường vàng). Hiệu suất của "0" thực sự tăng trong cả hai trường hợp, chỉ ra rằng dung lượng đại diện lớn hơn, không chỉ các đặc trưng hữu ích, góp phần vào sự gia tăng.

[Hình 15: Khám phá khả năng chuyển giao ngược của khung. Chúng ta thấy rằng khi tác vụ thứ hai tương tự với tác vụ đầu tiên ("O" tương tự với "0" hơn "Z"), lợi ích từ việc điều chỉnh với các liên kết chuyển giao ngược được kích hoạt lớn hơn.]

--- TRANG 14 ---
5. Thảo luận

Trong mục này chúng tôi sẽ thảo luận về cách khung thống nhất của chúng tôi có thể được áp dụng cho các thiết lập học tập ngoài mạng neural kết nối đầy đủ hoặc chuỗi tác vụ đơn giản. Một số thiết lập học tập này có liên quan chặt chẽ đến LML trong thiết lập và phạm vi của chúng: học đa nhiệm vụ, học chương trình giảng dạy, học few-shot, và mạng tích chập. Chúng tôi sẽ cung cấp một thảo luận ngắn gọn về các kết nối của chúng với khung của chúng tôi. Mục này kết thúc với một thảo luận về những điểm tương đồng với việc học của con người.

5.1 Học Đa nhiệm vụ và Chương trình Giảng dạy

Các trường hợp đặc biệt của LML là học đa nhiệm vụ (Caruana, 1997) và học chương trình giảng dạy (Bengio et al., 2009). Trong học đa nhiệm vụ, tất cả (Ti;Di) được cung cấp cùng nhau, cho phép mô hình được huấn luyện trên tất cả các tác vụ cùng một lúc. Trong học chương trình giảng dạy, tất cả dữ liệu tương tự được cung cấp, nhưng vấn đề là xác định thứ tự tối ưu để huấn luyện trên dữ liệu để học hiệu quả và hiệu suất nhất. Một ví dụ về loại chương trình giảng dạy trực quan là học các tác vụ từ "dễ" đến "khó" (Elman, 1993), tương tự như cách con người thường học các khái niệm mới.

5.2 Học Few-shot và Bài toán Bongard

Học suốt đời và chương trình giảng dạy có tiềm năng lớn để đạt được học few-shot của các khái niệm khó. Bằng cách học các khái niệm dễ hơn liên quan trước đó với LML, những khái niệm khó hơn sau này có thể được học với ít ví dụ hơn nhiều thông qua chuyển giao về phía trước kiến thức.

Con người thường có thể khám phá các quy tắc và mẫu cơ bản của các khái niệm phức tạp chỉ với vài ví dụ. Chúng ta có thể sử dụng Bài toán Bongard (BPs) (Bongard, 1967) để minh họa điều này. Một BP cụ thể yêu cầu con người nhận ra quy tắc cơ bản của phân loại chỉ với 6 ví dụ bên trái (giả sử chúng là dương) và 6 ví dụ bên phải (mẫu âm). Hai BP như vậy được thể hiện trong Hình 16, nơi (a) là một BP đơn giản và (b) là một BP khó cho con người. Để giải quyết BPs, con người thường dựa vào các quy tắc trừu tượng cao được học trước đó trong cuộc đời của họ (hoặc trong các BP dễ hơn). Một số hình thức LML của nhiều khái niệm và hình dạng trừu tượng trực quan có thể đã xảy ra ở con người để họ có thể giải quyết BPs tốt chỉ với 12 ví dụ tổng cộng.

[Hình 16: Hai ví dụ về BPs. (a) BP #5 đơn giản, nơi quy tắc là hình dạng đa giác bên trái và hình dạng cong bên phải. (b) BP #99 khá khó, nơi quy tắc là các hình dạng lớn hơn được hình thành bằng cách kết nối các hình dạng nhỏ tương tự chồng lắp so với các hình dạng lớn hơn không chồng lắp. Những quy tắc trừu tượng cao như vậy phổ biến trong các BPs. Đối với các BPs khó, chúng tôi thấy rằng đối với cả con người và mô hình ML, việc thêm nhiều mẫu huấn luyện thường không giúp ích. Thay vào đó, nó cần một "tia sáng của sự hiểu biết" hoặc kiến thức đã có đủ tương tự để giải quyết nó.]

Chúng tôi có thể phác thảo một quá trình học few-shot có thể để học các BPs như vậy trong khung LML của chúng tôi, theo cách giống như chương trình giảng dạy. Đầu tiên, chúng ta có thể huấn luyện (hoặc tiền huấn luyện) trên các tác vụ đơn giản hơn để nhận ra các hình dạng đơn giản hơn. Tiếp theo, các hình dạng trực quan khó hơn và phức tạp hơn và BPs sẽ được huấn luyện. Ý tưởng sử dụng học chương trình giảng dạy để "làm việc lên" các BPs khó hơn được phản ánh trong Hình 17. Trong ví dụ minh họa này, chúng tôi đưa các nhãn lớp trước đó (các nút đầu ra) vào các nút của các tác vụ mới để cung cấp cơ hội cho chuyển giao về phía trước kiến thức. Với phương pháp này, BPs có thể được giải quyết với ít ví dụ hơn, như được chỉ ra bởi Yun, Bohn, and Ling (2020). Lưu ý rằng trong ví dụ minh họa này, các mạng của các tác vụ tiếp theo có thể có độ sâu khác nhau và tăng dần để giải quyết các vấn đề khó hơn sau khi các vấn đề dễ hơn được học.

Để giải quyết các BPs khó với vài ví dụ (như BP #99 trong Hình 16b), chuyển giao về phía trước kiến thức là rất quan trọng. Nếu một người không thể giải quyết BP #99 (hoặc các BPs khó khác) trong vài phút, thường nhiều dữ liệu huấn luyện sẽ không giúp được nhiều. Thường "một tia sáng của cảm hứng" hoặc khoảnh khắc "Aha!" sẽ đột nhiên xảy ra và vấn đề được giải quyết. Điều này là bởi vì một người đã học có lẽ hàng nghìn hoặc hàng chục nghìn khái niệm và mối quan hệ trừu tượng trong cuộc đời của họ, và kiến thức chính xác được kết hợp đột nhiên để giải quyết BP khó.

[Hình 17: Một ví dụ minh họa về học chương trình giảng dạy để làm việc lên việc giải quyết các bài toán Bongard với một số lượng nhỏ ví dụ dương và âm. Sau khi học phân biệt giữa các loại nét cơ bản, hình dạng, và có thể các tác vụ đầu khác, những đại diện này bao gồm các lớp đầu ra và ẩn có thể được chuyển giao cho Tj để có thể giải quyết các BPs như BP#5 (góc nhọn so với đường cong mượt) với vài ví dụ. Quá trình này có thể tiếp tục để cho phép chúng ta giải quyết một vấn đề lý luận trực quan phức tạp hơn với vài mẫu. Lưu ý rằng các mạng của các tác vụ tiếp theo có thể có độ sâu tăng dần để giải quyết các BPs khó hơn sau khi những cái dễ hơn đã được học.]

--- TRANG 15 ---
5.3 Phương pháp Tiền Huấn luyện

Tiền huấn luyện là một quá trình phổ biến trong deep learning (Devlin et al., 2019; Girshick et al., 2014; Wang and Gupta, 2015). Trong thị giác máy tính (CV), các phương pháp hiện đại thường tiền huấn luyện trên một tác vụ không phải mục tiêu mà có dữ liệu dồi dào (Mahajan et al., 2018), như tập dữ liệu ImageNet (Russakovsky et al., 2015). Các mô hình CV đã tiền huấn luyện như VGG-16 (Simonyan and Zisserman, 2014) đã được phát hành công khai, cho phép bất kỳ ai tinh chỉnh mô hình của họ để đạt được hiệu suất không thể thực hiện được trên tác vụ mục tiêu. Bằng cách giảm nhu cầu dữ liệu tác vụ mục tiêu, các mô hình CV đã tiền huấn luyện đã trở thành một phương pháp phổ biến cho học few-shot (Ramalho, Sousbie, and Peluchetti, 2019). Trong xử lý ngôn ngữ tự nhiên, các mô hình ngôn ngữ đã tiền huấn luyện đã trở nên phổ biến (Conneau et al., 2017; Peters et al., 2018; Devlin et al., 2019). Ví dụ, BERT (Bidirectional Encoder Representations from Transformers) đã chỉ ra rằng hiệu suất hiện đại có thể đạt được trên nhiều tác vụ NLP khác nhau với một mô hình đã tiền huấn luyện (Devlin et al., 2019).

Với tiền huấn luyện, thường một mạng neural lớn đầu tiên được huấn luyện trên một hoặc nhiều tác vụ với các tập dữ liệu huấn luyện có nhãn rất lớn. Mạng neural đã huấn luyện sau đó sẽ bị đông lạnh, và một mạng nhỏ hơn được xếp chồng lên trên mạng lớn và được huấn luyện để cải thiện hiệu suất trên các tác vụ mục tiêu. Từ quan điểm của khung chúng tôi, các giá trị hợp nhất của các mạng nhỏ hơn sẽ được coi là không đông lạnh để huấn luyện.

Tiền huấn luyện do đó có thể được xem như các chính sách hợp nhất trọng số nhất định trong khung LML của chúng tôi. Sự khác biệt chính là hầu hết các thuật toán hợp nhất của chúng tôi trong hai mục cuối áp dụng cho trọng số mỗi tác vụ, hoặc theo cột trong các hình của chúng tôi, trong khi đối với tiền huấn luyện, hợp nhất là theo lớp. Một chủ đề nghiên cứu thú vị do đó sẽ là xem xét thiết kế các chính sách hợp nhất thực hiện cả hợp nhất theo lớp và theo cột dựa trên tác vụ hiện tại.

Một cách tự nhiên để tinh chỉnh chính sách hợp nhất khi điều chỉnh trên tác vụ mục tiêu là xem xét mức độ không đông lạnh mỗi lớp. Thay vì chỉ đơn giản điều chỉnh lớp đầu ra hoặc điều chỉnh toàn bộ mạng, chúng ta có thể nội suy giữa hai chính sách này, để cho phép không đông lạnh dần dần các lớp trong mạng để thích ứng tốt nhất với tác vụ mục tiêu mà không mất lợi ích tổng quát của tiền huấn luyện. Xem (Erhan et al., 2010) cho một ví dụ về những hướng như vậy.

5.4 Mạng Tích chập

Chúng ta có thể xem xét việc áp dụng khung của chúng tôi cho các mạng neural tích chập (CNNs), không chỉ các mạng feed-forward kết nối đầy đủ. Việc huấn luyện trên các tác vụ hình ảnh thường được thực hiện với học lô tốn nhiều tài nguyên, trong khi khung của chúng tôi sẽ cho phép tăng hiệu quả trong khi cẩn thận duy trì hiệu suất cao liên quan đến học lô.

Để điều chỉnh cơ chế hợp nhất cho CNNs, mỗi bbbᵢ có thể tương ứng với một bộ lọc thay vì một trọng số đơn (như trong một lớp kết nối dày đặc). Một bộ lọc về cơ bản là một tập hợp các trọng số lấy đại diện từ lớp trước đó (bản đồ đặc trưng), và biến đổi nó thành một bản đồ đặc trưng mới. bbb lớn trên một bộ lọc có nghĩa là nó không thể dễ dàng được sửa đổi trong quá trình học. Tiếp theo, chúng ta xem xét cách đạt được các tính chất LML khác nhau trong CNNs:

[Hình 18: Một hình ảnh hóa đơn giản về cách khung của chúng tôi có thể được áp dụng cho các mạng tích chập. Trong (a) là một sơ đồ của cơ chế hợp nhất và mở rộng của chúng tôi được áp dụng để học hai tác vụ với một mạng kết nối đầy đủ. Trong (b) là CNN tương ứng với cùng một cấu trúc liên kết cao như (a) được áp dụng cho ví dụ động vật có vú cho T₀ và chim cho T₁. Các liên kết với đầu mũi tên không tô chỉ ra trọng số bộ lọc, và các liên kết với đầu mũi tên tô chỉ ra trọng số giữa các lớp kết nối đầy đủ. Nét đứt (đỏ) chỉ ra giá trị bbb lớn, liền (xanh) chỉ ra giá trị bbb bằng 0, và nét gạch (xanh lá) chỉ ra các liên kết chuyển giao mà việc hợp nhất có thể phụ thuộc vào độ tương tự tác vụ.]

Học liên tục các tác vụ mới mà không quên Để mở rộng mạng cho các tác vụ mới, bây giờ chúng ta thêm các cột bộ lọc tích chập, như được phản ánh trong Hình 18b (liên kết liền). Để các tác vụ trước đó không bị quên, bbb của chúng = ∞ (xem các liên kết nét đứt đỏ của Hình 18). Đối với các tác vụ khó hơn, chúng ta có thể thêm một số lượng lớn hơn các bộ lọc (tương tự như một số lượng lớn hơn các nút).

21

--- TRANG 16 ---
Chuyển giao về phía trước Hai kỹ thuật để khuyến khích chuyển giao về phía trước cũng mở rộng cho CNNs. Đầu tiên, thay vì sao chép trọng số từ các tác vụ trước đó khi tương tự với tác vụ mới, bây giờ chúng ta có thể sao chép giá trị bộ lọc. Điều này có thể được thực hiện bằng cách sử dụng các ý tưởng tương tự như những ý tưởng được đưa ra trong Mục 3.3. Thứ hai, chúng ta có thể khuyến khích chuyển giao về phía trước và tránh chuyển giao âm thông qua việc ngắt kết nối các liên kết chuyển giao về phía trước; về cơ bản các bộ lọc này được khởi tạo bằng không và bbb = ∞, ngăn chặn chuyển giao âm kiến thức từ tác vụ trước đó.

5.5 Suy đoán về Những điểm Tương đồng với Học Con người

[Hình 19: Minh họa về những điểm tương đồng giữa các hành vi học của con người và những hành vi có thể được thể hiện bởi khung LML thống nhất của chúng tôi. Các liên kết nét đứt màu cam đại diện cho các giá trị bbb trung gian.]

Vì khung của chúng tôi tập trung vào việc kiểm soát tính linh hoạt của các trọng số mạng riêng lẻ, tự nhiên là phải hỏi cách phương pháp của chúng tôi phù hợp với bộ não động vật có vú mà các hành vi học khái niệm suốt đời của chúng ta đang cố gắng nắm bắt. Chúng tôi sẽ thảo luận ngắn gọn về cách các hành vi nhất định có thể được thể hiện bởi khung của chúng tôi với kích thước mạng cụ thể, huấn luyện, và các chính sách hợp nhất, và cách nó có thể dịch sang những gì xảy ra trong não con người ở mức độ trừu tượng tương tự.

Tài nguyên và đa năng Lý tưởng nhất, việc học của con người cho phép chúng ta có được nhiều kiến thức, nhưng vẫn đủ linh hoạt để thích ứng với những trải nghiệm mới và tạo ra những kết nối có ý nghĩa giữa các trải nghiệm. Điều này tương tự với khung của chúng tôi khi mọi thứ hoạt động hoàn hảo, bao gồm nhưng không giới hạn ở: mạng có nguồn cung ứng dồi dào các đơn vị tự do cho các tác vụ mới (Mục 3.2), sử dụng kiến thức trước đó để học các tác vụ mới (Mục 3.3), không quên các tác vụ trước đó trong khi học những tác vụ mới (Mục 3.2), và sử dụng kiến thức mới để tinh chỉnh các kỹ năng cũ (Mục 3.6).

"Rain man" Nếu các giá trị bbb cho các tác vụ trước đó rất lớn để đạt được không quên (Mục 3.2), và không có kết nối nào được tạo ra giữa các tác vụ trước đó và mới, chuyển giao kiến thức (Mục 3.3 và 3.6) có thể sẽ không xảy ra. Điều này gợi nhớ đến Kim Peek, người có thể nhớ lượng thông tin khổng lồ, nhưng hoạt động kém trong các tác vụ liên quan đến trừu tượng hóa đòi hỏi kết nối các kỹ năng và thông tin không liên quan và là nguồn cảm hứng cho nhân vật chính của bộ phim Rain Man (Treffert and Christensen, 2005). Xem Hình 19a để minh họa khung của chúng tôi được sử dụng để mô hình hóa hành vi tương tự.

Mất trí nhớ Như đã thể hiện trong Mục 3.5, khi tính khả dụng của các đơn vị tự do bị hạn chế, hoặc tệ hơn, nếu các đơn vị của mạng neural bị cắt tỉa đi, quên nhẹ nhàng có thể được sử dụng để học các tác vụ mới. Trong một số bệnh não, một số "tác vụ" có thể bị quên trước. Ví dụ, giai đoạn đầu của bệnh Alzheimer (một bệnh thần kinh rất phức tạp chưa được hiểu đầy đủ) thường được đặc trưng bởi trí nhớ tốt về các sự kiện nhiều năm trước nhưng kém về các sự kiện gần đây (Tierney et al., 1996). Điều này có thể được mô hình hóa bằng các giá trị bbb lớn cho các tác vụ cũ và giá trị nhỏ cho những tác vụ gần đây (được hiển thị như các liên kết màu cam trong Hình 19b). Việc cắt tỉa nơ-ron mạnh mẽ cũng có thể đang xảy ra (tương tự như cắt tỉa nút trong các mạng neural sâu) để làm giảm hiệu suất của các tác vụ.

Thiếu ngủ Bộ não con người được nghi ngờ thực hiện các quá trình quan trọng liên quan đến trí nhớ trong khi ngủ, và thiếu ngủ có hại cho hiệu suất trí nhớ (Walker, 2010; Killgore, 2010). Giảm nhầm lẫn và chuyển giao ngược là các giai đoạn quan trọng của phương pháp được đề xuất của chúng tôi sử dụng luyện tập lại (chức năng tương tự như phát lại trí nhớ), nơi mô hình được tiếp xúc với các mẫu từ các tác vụ trong quá khứ để thực hiện tinh chỉnh để đạt được các tính chất khác nhau (Mục 3.4 và 3.6). Không có những bước sử dụng luyện tập lại này, mô hình có thể ít có khả năng phân biệt giữa các mẫu của các lớp tương tự. Ngoài ra, khả năng xác định các kết nối giữa các tác vụ mới hơn và cũ hơn sẽ bị mất, để các kỹ năng mới có ích có thể không mang lại lợi ích cho các tác vụ cũ hơn. Ngoài ra, khi bộ não "mệt mỏi" và không được nghỉ ngơi tốt, điều này có thể tương tự như việc tối ưu hóa kém các chính sách trong khung của chúng tôi (và trong tất cả các thuật toán ML). Điều này được phản ánh trong Hình 19c thông qua các trọng số chuyển giao và tác vụ mới màu cam, điều này sẽ làm giảm khả năng cho tác vụ mới được học tốt hoặc hiệu quả sử dụng kiến thức tác vụ trước đó. Nếu việc tối ưu hóa không kỹ lưỡng trong Phương trình 1, hiệu suất sẽ kém trong tất cả các khía cạnh của LML. Xem Hình 19c để minh họa.

--- TRANG 17 ---
6. Kết luận

Trong công trình này, chúng tôi đã trình bày một khung khái niệm thống nhất cho LML sử dụng một cơ chế trung tâm dựa trên hợp nhất. Chúng tôi đã thảo luận về cách phương pháp của chúng tôi có thể nắm bắt nhiều tính chất quan trọng của việc học khái niệm suốt đời, bao gồm không quên, chuyển giao về phía trước và ngược lại, giảm nhầm lẫn, và tiếp tục, dưới một mái nhà. Kết quả bằng chứng khái niệm đã được báo cáo để hỗ trợ tính khả thi của những tính chất này. Thay vì nhằm mục đích đạt được kết quả hiện đại, bài báo này đề xuất các hướng nghiên cứu để giúp thông báo cho nghiên cứu LML tương lai, bao gồm các thuật toán và kết quả lý thuyết mới. Cuối cùng, chúng tôi lưu ý một số điểm tương đồng giữa các mô hình của chúng tôi với các chính sách huấn luyện và hợp nhất khác nhau và các hành vi nhất định trong việc học của con người.

Lời cảm ơn

Chúng tôi biết ơn về những thảo luận mang tính xây dựng và ý kiến từ các thành viên phòng thí nghiệm và đồng nghiệp về nhiều bản thảo của công trình này. Chúng tôi cũng ghi nhận sự hỗ trợ của Hội đồng Nghiên cứu Khoa học và Kỹ thuật Tự nhiên Canada (NSERC) thông qua Chương trình Tài trợ Khám phá. NSERC đầu tư hàng năm hơn 1 tỷ đô la vào con người, khám phá và đổi mới.

--- TRANG 18 ---
Tài liệu tham khảo

Aljundi, R.; Babiloni, F.; Elhoseiny, M.; Rohrbach, M.; and Tuytelaars, T. 2018. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), 139–154.

Bengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, 41–48. ACM.

Bongard, M. M. 1967. The problem of recognition. Fizmatgiz, Moscow.

Caruana, R. 1997. Multitask learning. Machine learning 28(1):41–75.

Chaudhry, A.; Dokania, P. K.; Ajanthan, T.; and Torr, P. H. S. 2018. Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence. CoRR abs/1801.10112.

Chaudhry, A.; Rohrbach, M.; Elhoseiny, M.; Ajanthan, T.; Dokania, P. K.; Torr, P. H.; and Ranzato, M. 2019. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486.

Chollet, F., et al. 2015. Keras.

Cohen, G.; Afshar, S.; Tapson, J.; and van Schaik, A. 2017. EMNIST: an extension of MNIST to handwritten letters. CoRR abs/1702.05373.

Conneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Bordes, A. 2017. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 670–680. Copenhagen, Denmark: Association for Computational Linguistics.

Cunha, C.; Brambilla, R.; and Thomas, K. L. 2010. A simple role for BDNF in learning and memory? Frontiers in molecular neuroscience 3:1.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. Minneapolis, Minnesota: Association for Computational Linguistics.

Elman, J. L. 1993. Learning and development in neural networks: The importance of starting small. Cognition 48(1):71–99.

Erhan, D.; Bengio, Y.; Courville, A.; Manzagol, P.-A.; Vincent, P.; and Bengio, S. 2010. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research 11(Feb):625–660.

Fei-Fei, L.; Fergus, R.; and Perona, P. 2006. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence 28(4):594–611.

Girshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 580–587.

24

--- TRANG 19 ---
Isele, D., and Cosgun, A. 2018. Selective experience replay for lifelong learning. In Thirty-second AAAI conference on artificial intelligence.

Kamilaris, A., and Prenafeta-Boldú, F. X. 2018. Deep learning in agriculture: A survey. Computers and electronics in agriculture 147:70–90.

Kamra, N.; Gupta, U.; and Liu, Y. 2017. Deep generative dual memory network for continual learning. arXiv preprint arXiv:1710.10368.

Killgore, W. D. 2010. Effects of sleep deprivation on cognition. In Progress in brain research, volume 185. Elsevier. 105–129.

Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.

Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Desjardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.; Grabska-Barwinska, A.; Hassabis, D.; Clopath, C.; Kumaran, D.; and Hadsell, R. 2016. Overcoming catastrophic forgetting in neural networks. cite arxiv:1612.00796.

Li, Z., and Hoiem, D. 2017. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence 40(12):2935–2947.

Litjens, G.; Kooi, T.; Bejnordi, B. E.; Setio, A. A. A.; Ciompi, F.; Ghafoorian, M.; Van Der Laak, J. A.; Van Ginneken, B.; and Sánchez, C. I. 2017. A survey on deep learning in medical image analysis. Medical image analysis 42:60–88.

Mahajan, D.; Girshick, R.; Ramanathan, V.; He, K.; Paluri, M.; Li, Y.; Bharambe, A.; and van der Maaten, L. 2018. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), 181–196.

McCloskey, M., and Cohen, N. J. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. The psychology of learning and motivation 109–165.

Pan, S. J., and Yang, Q. 2009. A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22(10):1345–1359.

Parisi, G. I.; Kemker, R.; Part, J. L.; Kanan, C.; and Wermter, S. 2019. Continual lifelong learning with neural networks: A review. Neural Networks.

Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.

Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1(8):9.

Ramalho, T.; Sousbie, T.; and Peluchetti, S. 2019. An empirical study of pretrained representations for few-shot classification. arXiv preprint arXiv:1910.01319.

Rebuffi, S.-A.; Kolesnikov, A.; Sperl, G.; and Lampert, C. H. 2017. iCaRL: Incremental Classifier and Representation Learning. In CVPR, 5533–5542. IEEE Computer Society.

25

--- TRANG 20 ---
Ritter, H.; Botev, A.; and Barber, D. 2018. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, 3738–3748.

Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115(3):211–252.

Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.; Kirkpatrick, J.; Kavukcuoglu, K.; Pascanu, R.; and Hadsell, R. 2016. Progressive Neural Networks. CoRR abs/1606.04671.

Shen, D.; Wu, G.; and Suk, H.-I. 2017. Deep learning in medical image analysis. Annual review of biomedical engineering 19:221–248.

Shin, H.; Lee, J. K.; Kim, J.; and Kim, J. 2017. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, 2990–2999.

Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton, A.; et al. 2017. Mastering the game of go without human knowledge. Nature 550(7676):354–359.

Simonyan, K., and Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

Thrun., S. 1995. A lifelong learning perspective for mobile robot control. Intelligent Robots and Systems.

Thrun, S. 1998. Lifelong learning algorithms. In Learning to learn. Springer. 181–209.

Tierney, M.; Szalai, J.; Snow, W.; Fisher, R.; Nores, A.; Nadon, G.; Dunn, E.; and George-Hyslop, P. S. 1996. Prediction of probable Alzheimer's disease in memory-impaired patients: A prospective longitudinal study. Neurology 46(3):661–665.

Treffert, D. A., and Christensen, D. D. 2005. Inside the mind of a savant. Scientific American 293(6):108–113.

Walker, M. P. 2010. Sleep, memory and emotion. In Progress in brain research, volume 185. Elsevier. 49–68.

Wang, X., and Gupta, A. 2015. Unsupervised learning of visual representations using videos. In Proceedings of the IEEE International Conference on Computer Vision, 2794–2802.

Wu, Y.; Chen, Y.; Wang, L.; Ye, Y.; Liu, Z.; Guo, Y.; and Fu, Y. 2019. Large Scale Incremental Learning.

Xu, J., and Zhu, Z. 2018. Reinforced continual learning. In Advances in Neural Information Processing Systems, 899–908.

Yoon, J.; Yang, E.; Lee, J.; and Hwang, S. J. 2018. Lifelong Learning with Dynamically Expandable Networks. In ICLR (Poster). OpenReview.net.

26

--- TRANG 21 ---
Yun, X.; Bohn, T.; and Ling, C. 2020. A Deeper Look at Bongard Problems. In Canadian Conference on Artificial Intelligence, 528–539. Springer.

Zenke, F.; Poole, B.; and Ganguli, S. 2017. Continual Learning Through Synaptic Intelligence. In Precup, D., and Teh, Y. W., eds., ICML, volume 70 of Proceedings of Machine Learning Research, 3987–3995. PMLR.

Zhang, Y., and Yang, Q. 2017. A survey on multi-task learning. arXiv preprint arXiv:1707.08114.

Zhang, J.; Zhang, J.; Ghosh, S.; Li, D.; Tasci, S.; Heck, L.; Zhang, H.; and Kuo, C.-C. J. 2020. Class-incremental learning via deep model consolidation. In The IEEE Winter Conference on Applications of Computer Vision, 1131–1140.

27
