# 2210.10209.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2210.10209.pdf
# File size: 765371 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Exclusive Supermask Subnetwork Training for Continual Learning
Prateek Yadav & Mohit Bansal
Department of Computer Science
UNC Chapel Hill
{praty,mbansal}@cs.unc.edu
Abstract
Continual Learning (CL) methods focus on ac-
cumulating knowledge over time while avoid-
ing catastrophic forgetting. Recently, Worts-
man et al. (2020) proposed a CL method,
SupSup, which uses a randomly initialized,
fixed base network (model) and finds a su-
permask for each new task that selectively
keeps or removes each weight to produce a
subnetwork . They prevent forgetting as the
network weights are not being updated. Al-
though there is no forgetting, the performance
of SupSup is sub-optimal because fixed weights
restrict its representational power. Further-
more, there is no accumulation or transfer of
knowledge inside the model when new tasks
are learned. Hence, we propose EXSSN ET
(Exclusive Supermask SubNEtwork Training),
that performs exclusive andnon-overlapping
subnetwork weight training. This avoids con-
flicting updates to the shared weights by subse-
quent tasks to improve performance while still
preventing forgetting. Furthermore, we pro-
pose a novel KNN-based Knowledge Transfer
(KKT) module that utilizes previously acquired
knowledge to learn new tasks better and faster.
We demonstrate that EXSSN EToutperforms
strong previous methods on both NLP and
Vision domains while preventing forgetting.
Moreover, EXSSN ETis particularly advan-
tageous for sparse masks that activate 2-10%
of the model parameters, resulting in an aver-
age improvement of 8.3% over SupSup. Fur-
thermore, EXSSN ETscales to a large num-
ber of tasks (100). Our code is available at
https://github.com/prateeky2806/exessnet.
1 Introduction
Artificial intelligence aims to develop agents that
can learn to accomplish a set of tasks. Continual
Learning (CL) (Ring, 1998; Thrun, 1998) is crucial
for this, but when a model is sequentially trained
on different tasks with different data distributions,
it can lose its ability to perform well on previoustasks, a phenomenon is known as catastrophic for-
getting (CF) (McCloskey and Cohen, 1989; Zhao
and Schmidhuber, 1996; Thrun, 1998). This is
caused by the lack of access to data from previ-
ous tasks, as well as conflicting updates to shared
model parameters when sequentially learning mul-
tiple tasks, which is called parameter interference
(McCloskey and Cohen, 1989).
Recently, some CL methods avoid parameter
interference by taking inspiration from the Lottery
Ticket Hypothesis (Frankle and Carbin, 2018) and
Supermasks (Zhou et al., 2019) to exploit the
expressive power of sparse subnetworks. Given
that we have a combinatorial number of sparse
subnetworks inside a network, Zhou et al. (2019)
noted that even within randomly weighted neural
networks, there exist certain subnetworks known
assupermasks that achieve good performance. A
supermask is a sparse binary mask that selectively
keeps or removes each connection in a fixed
and randomly initialized network to produce a
subnetwork with good performance on a given
task. We call this the subnetwork as supermask
subnetwork that is shown in Figure 1, highlighted
in red weights. Building upon this idea, Wortsman
et al. (2020) proposed a CL method, SupSup ,
which initializes a network with fixed and random
weights and then learns a different supermask for
each new task. This allows them to prevent catas-
trophic forgetting (CF) as there is no parameter
interference (because the model weights are fixed).
Although SupSup (Wortsman et al., 2020) pre-
vents CF, there are some problems with using su-
permasks for CL: (1) Fixed random model weights
in SupSup limits the supermask subnetwork’s rep-
resentational power resulting in sub-optimal perfor-
mance. (2) When learning a task, there is no mecha-
nism for transferring learned knowledge from previ-
ous tasks to better learn the current task. Moreover,
the model is not accumulating knowledge over time
as the weights are not being updated.arXiv:2210.10209v2  [cs.CV]  5 Jul 2023

--- PAGE 2 ---
Overlapping weights are not updated Randomly initialized
weights Mask  over
weights  Mask  over
weights  Mask  over
weights  
Find Mask 
Find Mask 
Find Mask Task1 T raining
Task2 T raining
Task3 T rainingTrain  
non-overlapping
weights  Train 
non-overlapping  
 weightsTrain  
non-overlapping  
weights  
: Trained weights: Untrained weights
: Overlapping weights: Task1 : Task2
: Task3Figure 1: EXSSN ETdiagram. We start with random weights W(0). For task 1, we first learn a supermask M1(the corresponding
subnetwork is marked by red color, column 2 row 1) and then train the weight corresponding to M1resulting in weights W(1)
(bold red lines, column 1 row 2). For task 2, we learn the mask M2over fixed weights W(1). If mask M2weights overlap with
M1(marked by bold dashed green lines in column 3 row 1), then only the non-overlapping weights (solid green lines) of the task
2 subnetwork are updated (as shown by bold and solid green lines column 3 row 2). These already trained weights (bold lines)
are not updated by any subsequent task. Finally, for task 3, we learn the mask M3(blue lines) and update the solid blue weights.
To overcome the aforementioned issues, we
propose our method, EXSSN ET(Exclusive
Supermask SubNEtwork Training), pronounced
as‘excess-net’ , which first learns a mask for a task
and then selectively trains a subset of weights from
the supermask subnetwork. We train the weights of
this subnetwork via exclusion that avoids updating
parameters from the current subnetwork that
have already been updated by any of the previous
tasks. In Figure 1, we demonstrate EXSSN ET
that also helps us to prevent forgetting. Training
the supermask subnetwork’s weights increases its
representational power and allows EXSSN ETto
encode task-specific knowledge inside the subnet-
work (see Figure 2). This solves the first problem
and allows EXSSN ETto perform comparably to a
fully trained network on individual tasks; and when
learning multiple tasks, the exclusive subnetwork
training improves the performance of each task
while still preventing forgetting (see Figure 3).
To address the second problem of knowledge
transfer, we propose a k-nearest neighbors-based
knowledge transfer (KKT) module that is able to
utilize relevant information from the previously
learned tasks to improve performance on new tasks
while learning them faster. Our KKT module uses
KNN classification to select a subnetwork from
the previously learned tasks that has a better than
random predictive power for the current task and
use it as a starting point to learn the new tasks.
Next, we show our method’s advantage by ex-
perimenting with both natural language and vi-
sion tasks. For natural language, we evaluate onWebNLP classification tasks (de Masson d 'Autume
et al., 2019) and GLUE benchmark tasks (Wang
et al., 2018), whereas, for vision, we evaluate on
SplitMNIST (Zenke et al., 2017), SplitCIFAR100
(De Lange and Tuytelaars, 2021), and SplitTiny-
ImageNet (Buzzega et al., 2020) datasets. We
show that for both language and vision domains,
EXSSN EToutperforms multiple strong and recent
continual learning methods based on replay, reg-
ularization, distillation, and parameter isolation.
For the vision domain, EXSSN EToutperforms the
strongest baseline by 4.8% and 1.4% on SplitCI-
FAR and SplitTinyImageNet datasets respectively,
while surpassing multitask model and bridging the
gap to training individual models for each task.
In addition, for GLUE datasets, EXSSN ETis 2%
better than the strongest baseline methods and sur-
passes the performance of multitask learning that
uses all the data at once. Moreover, EXSSN ETob-
tains an average improvement of 8.3% over SupSup
for sparse masks with 2−10% of the model param-
eters and scales to a large number of tasks (100).
Furthermore, EXSSN ETwith the KKT module
learns new tasks in as few as 30 epochs compared
to 100 epochs without it, while achieving 3.2%
higher accuracy on the SplitCIFAR100 dataset. In
summary, our contributions are listed below:
•We propose a simple and novel method to im-
prove mask learning by combining it with ex-
clusive subnetwork weight training to improve
CL performance while preventing CF.
•We propose a KNN-based Knowledge Transfer
(KKT) module for supermask initialization that

--- PAGE 3 ---
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.930405060
Methods
ExSSNeT
SSNeT
SupSup
Fully T rained
Mask DensityAverage T est AccuracyFigure 2: Test accuracy versus the mask density for 100-
way CIFAR100 classification. Averaged over 3 seeds.
dynamically identifies previous tasks to transfer
knowledge to learn new tasks better and faster.
•Extensive experiments on NLP and vision tasks
show that EXSSN EToutperforms strong base-
lines and is comparable to multitask model for
NLP tasks while surpassing it for vision tasks.
Moreover, EXSSN ETworks well for sparse
masks and scales to a large number of tasks.
2 Motivation
Using sparsity for CL is an effective technique to
learn multiple tasks, i.e., by encoding them in dif-
ferent subnetworks inside a single model. SupSup
(Wortsman et al., 2020) is an instantiation of this
that initializes the network weights randomly and
then learns a separate supermask for each task (see
Figure 7). They prevent CF because the weights of
the network are fixed and never updated. However,
this is a crucial problem as discussed below.
Problem 1 - Sub-Optimal Performance of Su-
permask: Although fixed network weights in
SupSup prevent CF, this also restricts the repre-
sentational capacity, leading to worse performance
compared to a fully trained network. In Figure 2,
we report the test accuracy with respect to the frac-
tion of network parameters selected by the mask,
i.e., the mask density for an underlying ResNet18
model on a single 100-way classification on CI-
FAR100 dataset. The fully trained ResNet18 model
(dashed green line) achieves an accuracy of 63.9%.
Similar to Zhou et al. (2019), we observe that the
performance of SupSup (yellow dashed line) is
at least 8.3% worse compared to a fully trained
model. As a possible partial remedy, we propose a
simple solution, SSN ET(Supermask SubNEtwork
Training), that first finds a subnetwork for a task
and then trains the subnetwork’s weights. This in-
creases the representational capacity of the subnet-
work because there are more trainable parameters.
For a single task, the test accuracy of SSN ETis bet-
ter than SupSup for all mask densities and matches
the performance of the fully trained model beyond
a density threshold. But as shown below, when
0 10 20 30 40 50 60 70 80304050607080
Methods
ExSSNeT
SSNeT
SupSup
Average Sparse OverlapAverage T est AccuracyFigure 3: Average Test accuracy on five 20-way tasks from
SplitCIFAR100 versus sparse overlap. Averaged over 3 seeds.
learning multiple tasks sequentially, SSN ETgives
rise to parameter interference that results in CF.
Problem 2 - Parameter Interference Due to Sub-
network Weight Training for Multiple Tasks:
Next, we demonstrate that when learning multi-
ple tasks sequentially, SSN ETcan still lead to CF.
In Figure 3, we report the average test accuracy
versus the fraction of overlapping parameters be-
tween the masks of different tasks, i.e., the sparse
overlap (see Equation 2) for five different 20-way
classification tasks from SplitCIFAR100 dataset
with ResNet18 model. We observe that SSN ET
outperforms SupSup for lower sparse overlap but
as the sparse overlap increases, the performance
declines because the supermask subnetworks for
different tasks have more overlapping (common)
weights (bold dashed lines in Figure 1). This leads
to higher parameter interference resulting in in-
creased forgetting which suppresses the gain from
subnetwork weight training.
Ourfinal proposal ,EXSSN ET, resolves both of
these problems by selectively training a subset of
the weights in the supermask subnetwork to prevent
parameter interference. When learning multiple
tasks, this prevents CF, resulting in strictly better
performance than SupSup (Figure 3) while having
the representational power to match bridge the gap
with fully trained models (Figure 2).
3 Method
As shown in Figure 1, when learning a new task
ti,EXSSN ETfollows three steps: (1) We learn
a supermask Mifor the task; (2) We use all the
previous tasks’ masks M1, . . . , M i−1to create a
free parameter mask Mfree
i, that finds the parame-
ters selected by the mask Mithat were not selected
by any of the previous masks; (3) We update the
weights corresponding to the mask Mfree
i as this
avoids parameter interference. Now, we formally

--- PAGE 4 ---
describe all the step of our method EXSSN ET
(Exclusive Supermask SubNEtwork Training) for
a Multi-layer perceptron (MLP).
Notation: During training, we can treat each
layer lof an MLP network separately. An inter-
mediate layer lhasnlnodes denoted by V(l)=
{v1, . . . , v nl}. For a node vin layer l, letIvdenote
its input and Zv=σ(Iv)denote its output, where
σ(.)is the activation function. Given this nota-
tion,Ivcan be written as Iv=P
u∈V(l−1)wuvZu,
where wuvis the network weight connecting node
uto node v. The complete network weights for the
MLP are denoted by W. When training the task ti,
we have access to the supermasks from all previ-
ous tasks {Mj}i−1
j=1and the model weights W(i−1)
obtained after learning task ti−1.
3.1 E XSSN ET: Exclusive Supermask
SubN Etwork Training
Finding Supermasks: Following Wortsman et al.
(2020), we use the algorithm of Ramanujan et al.
(2019) to learn a supermask Mifor the current
taskti. The supermask Miis learned with re-
spect to the underlying model weights W(i−1)and
the mask selects a fraction of weights that lead to
good performance on the task without training the
weights. To achieve this, we learn a score suvfor
each weight wuv, and once trained, these scores
are thresholded to obtain the mask. Here, the input
to a node visIv=P
u∈V(l−1)wuvZumuv, where
muv=h(suv)is the binary mask value and h(.)
is a function which outputs 1 for top- k%of the
scores in the layer with kbeing the mask density.
Next, we use a straight-through gradient estimator
(Bengio et al., 2013) and iterate over the current
task’s data samples to update the scores for the
corresponding supermask Mias follows,
suv=suv−αˆgsuv; ˆgsuv=∂L
∂Iv∂Iv
∂suv=∂L
∂IvwuvZu
(1)
Finding Exclusive Mask Parameters: Given a
learned mask Mi, we use all the previous tasks’
masks M1, . . . , M i−1to create a free parameter
mask Mfree
i, that finds the parameters selected by
the mask Mithat were not selected by any of the
previous masks. We do this by – (1) creating a new
mask M1:i−1containing all the parameters already
updated by any of the previous tasks by taking a
union of all the previous masks {Mj}i−1
j=1by using
the logical oroperation, and (2) Then we obtain
a mask Mfree
i by taking the intersection of all thenetwork parameters not used by any previous task
which is given by the negation of the mask M1:i−1
with the current task mask Mivia a logical and
operation. Next, we use this mask Mfree
i for the
exclusive supermask subnetwork weight training.
Exclusive Supermask Subnetwork Weight
Training: For training the subnetwork param-
eters for task tigiven the free parameter mask
Mfree
i, we perform the forward pass on the model
asmodel (x, W⊙ˆMi)where ˆMi=Mfree
i+((1−
Mfree
i)⊙Mi).detach (), where ⊙is the element-
wise multiplication. Hence, ˆMiallows us to use
all the connections in Miduring the forward pass
of the training but during the backward pass, only
the parameters in Mfree
i are updated because the
gradient value is 0 for all the weights wuvwhere
mfree
uv= 0. While during the inference on task ti
we use the mask Mi. In contrast, SSN ET uses the
task mask Miboth during the training and infer-
ence as model (x, W(i−1)⊙Mi). This updates all
the parameters in the mask including the parame-
ters that are already updated by previous tasks that
result in CF. Therefore, in cases where the sparse
overlap is high, EXSSN ETis preferred over SS-
NET. To summarize, EXSSN ETcircumvents the
CF issue of SSN ETwhile benefiting from the sub-
network training to improve overall performance
as shown in Figure 3.
3.2 KKT: Knn-Based Knowledge Transfer
When learning multiple tasks, it is a desired
property to transfer information learned by the
previous tasks to achieve better performance on
new tasks and to learn them faster (Biesialska et al.,
2020). Hence, we propose a K-Nearest Neighbours
(KNN) based knowledge transfer (KKT) module
that uses KNN classification to dynamically find
the most relevant previous task (Veniat et al., 2021)
to initialize the supermask for the current task.
To be more specific, before learning the mask
Mifor the current task ti, we randomly sample
a small fraction of data from task tiand split it
into a train and test set. Next, we use the trained
subnetworks of each previous task t1, . . . , t i−1to
obtain features on this sampled data. Then we learn
i−1independent KNN-classification models using
these features. Then we evaluate these i−1models
on the sampled test set to obtain accuracy scores
which denote the predictive power of features from
each previous task for the current task. Finally, we
select the previous task with the highest accuracy

--- PAGE 5 ---
on the current task. If this accuracy is better than
random then we use its mask to initialize the
current task’s supermask. This enables EXSSN ET
to transfer information from the previous task to
learn new tasks better and faster. We note that the
KKT module is not limited to SupSup and can be
applied to a broader category of CL methods that
introduce additional parameters for new tasks.
4 Experiments
4.1 Experimental Setup and Training Details
Datasets: For natural language domain, we fol-
low the shared text classification setup of IDBR
(Huang et al., 2021), LAMOL (Sun et al., 2019),
and MBPA++ (De Lange et al., 2019) to sequen-
tially learn five text classification tasks; (1) Yelp
Sentiment analysis (Zhang et al., 2015); (2) DB-
Pedia for Wikipedia article classification (Mendes
et al., 2012) (3) Yahoo! Answer for Q&A clas-
sification (Chang et al., 2008); (4) Amazon sen-
timent analysis (McAuley and Leskovec, 2013)
(5) AG News for news classification (Zhang et al.,
2015). We call them WebNLP classification tasks
for easier reference. While comparing with the
previous state-of-the-art text methods, we use the
same training and test set as IDBR and LAMOL
containing 115,000/500/7,600 Train/Val/Test ex-
amples. For our ablation studies, we follow IDBR
and use a sampled dataset, please see Appendix
Table 7 for statistics. Additionally, we create a
CL benchmark using the popular GLUE classifica-
tiontasks (Wang et al., 2018) consisting of more
than 5k train samples. We use the official vali-
dation split as test data and use 0.1%of the train
data to create a validation set. Our final bench-
mark includes five tasks; MNLI (353k/39k/9.8k),
QQP (327k/36k/40k), QNLI (94k/10k/5.4k), SST-
2 (60k/6.7k/872), CoLA (7.6k/856/1k). For vi-
sion experiments, we follow SupSup and use three
CL benchmarks, SplitMNIST (Zenke et al., 2017),
SplitCIFAR100 (Chaudhry et al., 2018), and Split-
TinyImageNet (Buzzega et al., 2020) datasets with
10, 100 and 200 total classes respectively.
Metrics: We follow Chaudhry et al. (2018) and
evaluate our model after learning task ton all the
tasks, denoted by T. This gives us an accuracy
matrix A∈Rn×n, where ai,jrepresents the classi-
fication accuracy on task jafter learning task i. We
want the model to perform well on all the tasks it
has been learned. This is measured by the average
accuracy ,A(T) =1
NPN
k=1aN,k, where Nis thenumber of tasks. Next, we want the model to retain
performance on the previous tasks when learning
multiple tasks. This is measured by the forgetting
metric (Lopez-Paz and Ranzato, 2017), F(T) =
1
N−1PN−1
t=1(max k∈{1,...,N−1}ak,t−aN,t). This is
the average difference between the maximum ac-
curacy obtained for task tand its final accuracy.
Higher accuracy and lower forgetting are desired.
Sparse Overlap to Quantify Parameter Interfer-
ence: Next, we propose sparse overlap , a mea-
sure to quantify parameter interference for a task i,
i.e., the fraction of the parameters in mask Mithat
are already updated by some previous task. For a
formal definition refer to Appendix A.1
Previous Methods and Baselines: For both
vision and language ( VL) tasks, we compare
with: (VL.1) Naive Training (Yogatama et al.,
2019): where all model parameters are sequentially
trained/finetuned for each task. (VL.2) Experience
Replay (ER) (de Masson d 'Autume et al., 2019):
we replay previous tasks examples when we train
new tasks; (VL.3) Multitask Learning (Craw-
shaw, 2020): where all the tasks are used jointly to
train the model; (VL.4) Individual Models : where
we train a separate model for each task. This is
considered an upper bound for CL; (VL.5) Sup-
sup(Wortsman et al., 2020). For natural language
(L), we further compare with the following meth-
ods: (L.6) Regularization (Huang et al., 2021):
Along with the Replay method, we regularize the
hidden states of the BERT classifier with an L2 loss
term; We show three Adapter BERT (Houlsby et al.,
2019) variants, (L.7) AdaptBERT + FT where we
have single adapter which is finetuned for all task;
(L.8) AdaptBERT + ER where a single adapter
is finetuned with replay; (L.9) MultiAdaptBERT
where a separate adapter is finetuned for each task;
(L.10) Prompt Tuning (Li and Liang, 2021) that
learns 50 different continuous prompt tokens for
each task. (L.11) MBPA++ (de Masson d 'Autume
et al., 2019) perform replay with random examples
during training and does local adaptation during in-
ference to select replay example; (L.12) LAMOL
(Sun et al., 2019) uses a language model to gener-
ate pseudo-samples for previous tasks for replay;
(L.13) IDBR (Huang et al., 2021) disentangles hid-
den representations into generic and task-specific
representations and regularizes them while also
performing replay. For vision task ( V), we addi-
tionally compare with two popular regularization

--- PAGE 6 ---
Method ( ↓) GLUE WebNLP
Order ( →) S1 S2 S3 S4 S5 Average
Random 33.3 (-) 7.14 (-) 7.14 (-) 7.14 (-) 7.14 (-) 7.14 (-)
Multitask 79.9 (0.0) 77.2 (0.0) 77.2 (0.0) 77.2 (0.0) 77.2 (0.0) 77.2 (0.0)
Individual 87.7 (0.0) 79.5 (0.0) 79.5 (0.0) 79.5 (0.0) 79.5 (0.0) 79.5 (0.0)
FT 14.1 (86.0) 26.9 (62.1) 22.8 (67.6) 30.6 (55.9) 15.6 (76.8) 24.0 (65.6)
AdaptBERT + FT 24.7 (53.4) 20.8 (68.4) 19.1 (70.9) 23.6 (64.5) 14.6 (76.0) 19.6 (70.0)
AdaptBERT + Replay 76.8 (3.8) 73.2 (3.0) 74.5 (2.0) 74.5 (2.0) 74.6 (2.0) 74.2 (2.3)
MultiAdaptBERT 78.5 (0.0) 76.7 (0.0) 76.7 (0.0) 76.7 (0.0) 76.7 (0.0) 76.7 (0.0)
Prompt Tuning 76.3 (0.0) 66.3 (0.0) 66.3 (0.0) 66.3 (0.0) 66.3 (0.0) 66.3 (0.0)
Regularization 72.5 (8.8) 76.0 (2.8) 74.9 (3.8) 76.4 (1.8) 76.5 (2.0) 76.0 (2.6)
Replay 77.7 (4.8) 75.1 (3.1) 74.6 (3.5) 75.2 (2.2) 75.7 (3.1) 75.1 (3.0)
MBPA++ † - 74.9 (-) 73.1 (-) 74.9 (-) 74.1 (-) 74.3 (-)
LAMOL † - 76.1 (-) 76.1 (-) 77.2 (-) 76.7 (-) 76.5 (-)
IDBR 73.0 (6.8) 75.9 (2.7) 75.4 (3.5) 76.5 (1.6) 76.4 (1.9) 76.0 (2.4)
SupSup 78.3 (0.0) 75.9 (0.0) 76.1 (0.0) 76.0 (0.0) 75.9 (0.0) 76.0 (0.0)
SSN ET 78.4 (3.6) 76.3 (0.8) 76.3 (0.8) 76.4 (0.3) 76.3 (0.3) 76.3 (0.6)
EXSSN ET 80.5 (0.0) 77.0 (0.0) 77.1 (0.0) 76.7 (0.0) 76.9 (0.0) 76.9 (0.0)
Table 1: Comparing average test accuracy ↑(and forgetting metric ↓)for multiple tasks and sequence orders with state-of-the-
art (SotA) methods. Results with †are taken from (Huang et al., 2021).
based methods, (V .6) Online EWC (Schwarz et al.,
2018), (V .7) Synaptic Intelligence (SI) (Zenke
et al., 2017); one knowledge distillation method,
(V .8) Learning without Forgetting (LwF) (Li
and Hoiem, 2017), three additional experience re-
play method, (V .9) AGEM (Chaudhry et al., 2018),
(V .10) Dark Experience Replay (DER) (Buzzega
et al., 2020), (V .11) DER++ (Buzzega et al., 2020),
and a parameter isolation method (V .12) CGATE
(Abati et al., 2020).
Implementation Details: Following Huang et al.
(2021), for WebNLP datasets we learn different
task orders S1-S51that are provided in Appendix
Table 6. Following Huang et al. (2021), for NLP
experiments, we use a pre-trained BERT as our
base model for all methods. For SupSup, SSN ET,
andEXSSN ET, we use a CNN-based classification
head. Unless specified, we randomly split all the
vision datasets to obtain five tasks with disjoint
classes. For the vision experiments, we do not use
pre-trained models. All methods employ the same
number of epochs over datasets. For additional
implementation details refer to Appendix A.3.
4.2 Main Results
Q1. Does Supermask Subnetwork Training
Help? In these experiments, we show that EXSS-
NEToutperforms multiple strong baseline methods
including SupSup. For our main language exper-
iments in Table 1, we sequentially learn multiple
task orders, S1 - S51corresponding to the GLUE
and WebNLP benchmarks. These task orders are
1For example, in S2 order the model learns the task in this
order, ag yelp amazon yahoo dbpediaMethod S-MNIST S-CIFAR100 S-TinyImageNet
Multitask 96.5 (0.0) 53.0 (0.0) 45.9 (0.0)
Individual 99.7 (0.0) 75.5 (0.0) 53.7 (0.0)
Naive Sequential 49.6 (25.0) 19.3 (73.7) 11.5 (43.9)
EWC 96.1 (4.5) 32.4 (60.5) 20.5 (52.1)
SI 99.2 (0.6) 46.1 (47.8) 19.5 (46.2)
LwF 99.2 (0.8) 29.5 (70.2) 18.1 (56.5)
AGEM 98.3 (1.9) 52.1 (42.0) 21.6 (54.9)
ER 99.2 (0.6) 60.1 (27.5) 35.6 (36.0)
DER 98.9 (1.2) 62.5 (28.4) 35.9 (37.7)
DER++ 98.3 (1.8) 62.5 (27.5) 36.2 (35.7)
CGATE 99.6 (0.0) 60.1 (0.0) 49.2 (0.0)
SupSup 99.6 (0.0) 62.1 (0.0) 50.6 (0.0)
SSN ET 99.7 (0.0) 23.9 (54.4) 49.6 (1.9)
EXSSN ET 99.7 (0.0) 67.3 (0.0) 52.0 (0.0)
Table 2: Average accuracy ↑(Forgetting metric ↓)on all
tasks for vision. For our method, we report the results are
averaged over three random seeds.
listed in Appendix Table 6. We report the average
test accuracy (and forgetting in parentheses). For
natural language, we perform better than previous
SOTA CL methods in four out of five cases, across
multiple task orders, and in aggregate. Specifically,
on the GLUE benchmark, EXSSN ETis at least
2.0% better than other methods while avoiding CF.
Furthermore, EXSSN ETeither outperforms or is
close to the performance of the multitasking base-
line which is a strong baseline for CL methods.
For vision tasks, we split the MNIST, CIFAR100,
and TinyImageNet datasets into five different tasks
with an equal number of disjoint classes and report
results. From Table 2, we observe that EXSSN ET
leads to a 4.8% and 1.4% improvement over
the strongest baseline for Split-CIFAR100 and
Split-TinyImageNet datasets. Furthermore, both
EXSSN ETand SupSup outperform the multitask
baseline. Moreover, EXSSN ETbridges the gap
to individually trained models significantly, for
TinyImageNet we reach within 1.7% of individual

--- PAGE 7 ---
Method S-MNIST S-CIFAR100 S-TinyImageNet
SupSup 99.6 62.1 50.6
+ KKT 99.6 [+0.0] 67.1 [+5.0] 53.3 [+2.7]
SSN ET 99.7 23.9 49.6
+ KKT 99.3 [-0.4] 23.5 [-0.4] 51.8 [+2.2]
EXSSN ET99.7 67.3 52.0
+ KKT 99.7 [+0.0] 70.5 [+3.2] 54.0 [+2.0]
Table 3: Average test accuracies ↑[and gains from
KKT] when using the KKT knowledge sharing module.
0 20 40 60 8040506070
0 20 40 60 8040506070
0 20 40 60 8040506070
0 20 40 60 804050607080
ExSSNeT + KKT
ExSSNeT
Epochs EpochsVal. Accuracy Val. AccuracySpeedup
Figure 4: We plot validation accuracy vs Epoch for EXSS-
NETandEXSSN ET+ KKT. We observe that KKT helps to
learn the subsequent tasks faster and improves performance.
models’ performance. The average sparse overlap
ofEXSSN ETis19.4% across all three datasets im-
plying that there is a lot more capacity in the model.
See appendix Table 11 for sparse overlap of other
methods and Appendix A.4.1 for best-performing
methods results on Imagenet Dataset.
Note that, past methods require tricks like lo-
cal adaptation in MBPA++, and experience replay
in AGEM, DER, LAMOL, and ER. In contrast,
EXSSN ET is simple and does not require replay.
Q2. Can KKT Knowledge Transfer Module
Share Knowledge Effectively? In Table 3, we
show that adding the KKT module to EXSSN ET,
SSN ET, and SupSup improves performance on vi-
sion benchmarks. The experimental setting here
is similar to Table 2. We observe across all
methods and datasets that the KKT module im-
proves average test accuracy. Specifically, for
the Split-CIFAR100 dataset, the KKT module re-
sults in 5.0%, and 3.2% improvement for Sup-
Sup and EXSSN ETrespectively; while for Split-
TinyImageNet, EXSSN ET+ KKT outperforms the
individual models. We observe a performance de-
cline for SSN ETwhen using KKT because KKT
promotes sharing of parameters across tasks which
can lead to worse performance for SSN ET. Fur-
thermore, EXSSN ET+ KKT outperforms all other
methods on both the Split-CIFAR100 and Split-
TinyImageNet datasets. For EXSSN ET+ KKT,
0.020.040.060.080.10.20.30.50.70.9304050607080
Methods
ExSSNeT
SSNeT
SupSup
Mask Density (Log Scale)Average T est AccuracyFigure 5: Average test accuracy versus mask density on
SplitCIFAR100 dataset.
Method S-TinyImageNet Avg. Sparse Overlap
SupSup 90.34 (0.0) 90.1
SSN ET 89.02 (2.2) 90.0
EXSSN ET 91.21 (0.0) 90.0
Table 4: Average accuracy ↑(forgetting metric ↓) and
average sparse overlap when learning 100 tasks.
the average sparse overlap is 49.6% across all three
datasets (see appendix Table 11). These results sug-
gest that combining weight training with the KKT
module leads to further improvements.
Q3. Can KKT Knowledge Transfer Module
Improve Learning Speed of Subsequent Tasks?
Next, we show that the KKT module enables us
to learn new tasks faster. To demonstrate this,
in Figure 4 we plot the running mean of the
validation accuracy vs epochs for different tasks
from the Split-CIFAR100 experiment in Table 3.
We show curves for EXSSN ETwith and without
the KKT module and omit the first task as both
these methods are identical for Task 1 because
there is no previous task to transfer knowledge.
For all the subsequent tasks (Task 2,3,4,5), we
observe that – (1) EXSSN ET+ KKT starts off
with a much better initial performance compared
toEXSSN ET(2) given a fixed number of epochs
for training, EXSSN ET+ KKT always learns the
task better because it has a better accuracy at all
epochs; and (3) EXSSN ET+ KKT can achieve
similar performance as EXSSN ETin much
fewer epochs as shown by the green horizontal
arrows. This clearly illustrates that using the KKT
knowledge-transfer module not only helps to learn
the tasks better (see Table 3) but also learn them
faster. For an efficiency and robustness analysis of
the KKT module, please refer to Appendix A.4.2.
4.3 Additional Results and Analysis
Q4. Effect of Mask Density on Performance:
Next, we show the advantage of using EXSSN ET
when the mask density is low. In Figure 5, we
show the average accuracy for the Split-CIFAR100
dataset as a function of mask density. We observe

--- PAGE 8 ---
Method FastText Glove BERT
SupSup 54.01 55.52 74.0
SSN ET 60.41 [+6.4] 59.78 [+4.3] 74.5 [+0.5]
EXSSN ET 62.52 [+8.5] 62.81 [+7.3] 74.8 [+0.8]
Table 5: Ablation result for token embeddings. We report
average accuracy ↑[and gains over SupSup]
thatEXSSN ETobtains 7.9%,18.4%,8.4%, and
4.7% improvement over SupSup for mask density
values 0.02,0.04,0.06,0.08respectively. This is
an appealing property as tasks select fewer parame-
ters which inherently reduces sparse overlap allow-
ing E XSSN ET to learn a large number of tasks.
Q5. Can EXSSN ETLearn a Large Number of
Tasks? SupSup showed that it can scale to a large
number of tasks. Next, we perform experiments
to learn 100 tasks created by splitting the Tiny-
ImageNet dataset. In Table 4, we show that this
property is preserved by EXSSN ETwhile resulting
in a performance improvement over SupSup. We
note that as the number of task increase, the sparse
overlap between the masks also increases resulting
in fewer trainable model weights. In the extreme
case where there are no free weights, EXSSN ET
by design reduces to SupSup because there will
be no weight training. Moreover, if we use larger
models there are more free parameters, leading to
even more improvement over SupSup.
Q6. Effect of Token Embedding Initialization
for NLP: For our language experiments, we use
a pretrained BERT model (Devlin et al., 2019) to
obtain the initial token representations. We perform
ablations on the token embedding initialization to
understand its impact on CL methods. In Table 5,
we present results on the S21task-order sequence
of the sampled version of WebNLP dataset (see
Section 4.1, Datasets). We initialize the token repre-
sentations using FastText (Bojanowski et al., 2016),
Glove (Pennington et al., 2014), and BERT embed-
dings. From Table 5, we observe that – (1) the
performance gap between EXSSN ETand SupSup
increases from 0.8%→7.3%and0.8%→8.5%
when moving from BERT to Glove and FastText
initializations respectively. These gains imply that
it is even more beneficial to use EXSSN ETin ab-
sence of good initial representations, and (2) the
performance trend, EXSSN ET>SSN ET> Sup-
Sup is consistent across initialization.5 Related Work
Regularization-based methods estimate the im-
portance of model components and add importance
regularization terms to the loss function. Zenke
et al. (2017) regularize based on the distance of
weights from their initialization, whereas Kirk-
patrick et al. (2017b); Schwarz et al. (2018) use
an approximation of the Fisher information matrix
(Pascanu and Bengio, 2013) to regularize the pa-
rameters. In NLP, Han et al. (2020); Wang et al.
(2019) use regularization to constrain the relevant
information from the huge amount of knowledge
inside large language models (LLM). Huang et al.
(2021) first identifies hidden spaces that need to be
updated versus retained via information disentan-
glement (Fu et al., 2017; Li et al., 2020) and then
regularize these hidden spaces separately.
Replay based methods maintain a small mem-
ory buffer of data samples (De Lange et al., 2019;
Yan et al., 2022) or their relevant proxies (Rebuffi
et al., 2017) from the previous tasks and retrain on
them later to prevent CF. Chaudhry et al. (2018) use
the buffer during optimization to constrain parame-
ter gradients. Shin et al. (2017); Kemker and Kanan
(2018) uses a generative model to sample and re-
play pseudo-data during training, whereas Rebuffi
et al. (2017) replay distilled knowledge from the
past tasks. de Masson d 'Autume et al. (2019) em-
ploy episodic memory along with local adaptation,
whereas Sun et al. (2019) trains a language model
to generate a pseudo-sample for replay.
Architecture based methods can be divided into
two categories: (1) methods that add new mod-
ules over time (Li et al., 2019; Veniat et al., 2021;
Douillard et al., 2022); and (2) methods that iso-
late the network’s parameters for different tasks
(Kirkpatrick et al., 2017a; Fernando et al., 2017;
Mallya and Lazebnik, 2018; Fernando et al., 2017).
Rusu et al. (2016) introduces a new network for
each task while Schwarz et al. (2018) distilled the
new network after each task into the original one.
Recent prompt learning-based CL models for vi-
sion (Wang et al., 2022a,b) assume access to a
pre-trained model to learn a set of prompts that
can potentially be shared across tasks to perform
CL this is orthogonal to our method that trains
from scratch. Mallya and Lazebnik (2018) allo-
cates parameters to specific tasks and then trains
them in isolation which limits the number of tasks
that can be learned. In contrast, Mallya et al. (2018)
use a frozen pretrained model and learns a new

--- PAGE 9 ---
mask for each task but a pretrained model is crucial
for their method’s good performance. Wortsman
et al. (2020) removes the pretrained model depen-
dence and learns a mask for each task over a fixed
randomly initialized network. EXSSN ETavoids
the shortcomings of Mallya and Lazebnik (2018);
Mallya et al. (2018) and performs supermask sub-
network training to increase the representational
capacity compared to (Wortsman et al., 2020) while
performing knowledge transfer and avoiding CF.
6 Conclusion
We introduced a novel Continual Learning method,
EXSSN ET(Exclusive Supermask SubNetwork
Training), that delivers enhanced performance by
utilizing exclusive, non-overlapping subnetwork
weight training, overcoming the representational
limitations of the prior SupSup method. Through
the avoidance of conflicting weight updates,
EXSSN ETnot only improves performance but
also eliminates forgetting, striking a delicate
balance. Moreover, the inclusion of the Knowledge
Transfer (KKT) module propels the learning
process, utilizing previously acquired knowledge
to expedite and enhance the learning of new tasks.
The efficacy of EXSSN ETis substantiated by
its superior performance in both NLP and Vision
domains, its particular proficiency for sparse
masks, and its scalability up to a hundred tasks.
Limitations
Firstly, we note that as the density of the mask
increases, the performance improvement over the
SupSup method begins to decrease. This is due to
the fact that denser subnetworks result in higher
levels of sparse overlap, leaving fewer free parame-
ters for new tasks to update. However, it is worth
noting that even in situations where mask densi-
ties are higher, all model weights are still trained
by some task, improving performance on those
tasks and making our proposed method an upper
bound to the performance of SupSup. Additionally,
the model size and capacity can be increased to
counterbalance the effect of higher mask density.
Moreover, in general, a sparse mask is preferred
for most applications due to its efficiency.
Secondly, we have focused on the task incremen-
tal setting of continual learning for two main rea-
sons: (1) in the domain of natural language process-
ing, task identities are typically easy to obtain, and
popular methods such as prompting and adaptorsassume access to task identities. (2) the primary
focus of our work is to improve the performance of
supermasks for continual learning and to develop
a more effective mechanism for reusing learned
knowledge, which is orthogonal to the question
of whether task identities are provided during test
time.
Moreover, it is worth noting that, similar to the
SupSup method, our proposed method can also
be extended to situations where task identities are
not provided during inference. The SupSup paper
presents a method for doing this by minimizing
entropy to select the best mask during inference,
and this can also be directly applied to our pro-
posed method, ExSSNeT, in situations where task
identities are not provided during inference. This
is orthogonal to the main questions of our study,
however, we perform some experiments on Class
Incremental Learning in the appendix A.4.3.
Acknowledgements
We thank Marc’Aurelio Ranzato for the helpful dis-
cussions to formulate the initial idea. We thank the
reviewers and Xiang Zhou, Swarnadeep Saha, and
Archiki Prasad for their valuable feedback on this
paper. This work was supported by NSF-CAREER
Award 1846185, DARPA Machine-Commonsense
(MCS) Grant N66001-19-2-4031, Microsoft Inves-
tigator Fellowship, and Google and AWS cloud
compute awards. The views contained in this arti-
cle are those of the authors and not of the funding
agency.
References
Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Si-
mone Calderara, Rita Cucchiara, and Babak Ehte-
shami Bejnordi. 2020. Conditional channel gated
networks for task-aware continual learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3931–3940.
Yoshua Bengio, Nicholas Léonard, and Aaron C.
Courville. 2013. Estimating or propagating gradients
through stochastic neurons for conditional computa-
tion. CoRR , abs/1308.3432.
Magdalena Biesialska, Katarzyna Biesialska, and
Marta R. Costa-jussà. 2020. Continual lifelong learn-
ing in natural language processing: A survey. In
Proceedings of the 28th International Conference
on Computational Linguistics , pages 6523–6541,
Barcelona, Spain (Online). International Committee
on Computational Linguistics.

--- PAGE 10 ---
Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Da-
vide Abati, and Simone Calderara. 2020. Dark expe-
rience for general continual learning: a strong, simple
baseline. In Advances in Neural Information Process-
ing Systems , volume 33, pages 15920–15930. Curran
Associates, Inc.
Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek
Srikumar. 2008. Importance of semantic representa-
tion: Dataless classification. In Proceedings of the
23rd National Conference on Artificial Intelligence -
Volume 2 , AAAI’08, page 830–835. AAAI Press.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus
Rohrbach, and Mohamed Elhoseiny. 2018. Effi-
cient lifelong learning with a-gem. arXiv preprint
arXiv:1812.00420 .
Michael Crawshaw. 2020. Multi-task learning
with deep neural networks: A survey. ArXiv ,
abs/2009.09796.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh,
and Tinne Tuytelaars. 2019. Continual learning: A
comparative study on how to defy forgetting in clas-
sification tasks. arXiv preprint arXiv:1909.08383 ,
2(6).
Matthias De Lange and Tinne Tuytelaars. 2021. Con-
tinual prototype evolution: Learning online from
non-stationary data streams. In Proceedings of the
IEEE/CVF International Conference on Computer
Vision (ICCV) , pages 8250–8259.
Cyprien de Masson d 'Autume, Sebastian Ruder, Ling-
peng Kong, and Dani Yogatama. 2019. Episodic
memory in lifelong language learning. In Ad-
vances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. Imagenet: A large-scale
hierarchical image database. In CVPR 2009 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.Arthur Douillard, Alexandre Ramé, Guillaume Coua-
iron, and Matthieu Cord. 2022. Dytox: Transformers
for continual learning with dynamic token expansion.
InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) .
Chrisantha Fernando, Dylan Banarse, Charles Blundell,
Yori Zwols, David Ha, Andrei A. Rusu, Alexander
Pritzel, and Daan Wierstra. 2017. Pathnet: Evolution
channels gradient descent in super neural networks.
CoRR , abs/1701.08734.
Jonathan Frankle and Michael Carbin. 2018. The lottery
ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 .
Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,
and Rui Yan. 2017. Style transfer in text: Exploration
and evaluation. arXiv preprint arXiv:1711.06861 .
Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu,
Peng Li, Maosong Sun, and Jie Zhou. 2020. Contin-
ual relation learning via episodic memory activation
and reconsolidation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 6429–6440, Online. Association
for Computational Linguistics.
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.
2016. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 770–778.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799.
PMLR.
Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang,
and Diyi Yang. 2021. Continual learning for text clas-
sification with information disentanglement based
regularization. In Proceedings of the 2021 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies , pages 2736–2746, Online. As-
sociation for Computational Linguistics.
Ronald Kemker and Christopher Kanan. 2018. Fearnet:
Brain-inspired model for incremental learning. In In-
ternational Conference on Learning Representations .
Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 1746–1751,
Doha, Qatar. Association for Computational Linguis-
tics.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

--- PAGE 11 ---
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A. Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell.
2017a. Overcoming catastrophic forgetting in neural
networks. Proceedings of the National Academy of
Sciences , 114(13):3521–3526.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, et al. 2017b. Over-
coming catastrophic forgetting in neural networks.
Proceedings of the national academy of sciences ,
114(13):3521–3526.
Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278–2324.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and
Caiming Xiong. 2019. Learn to grow: A continual
structure learning framework for overcoming catas-
trophic forgetting. In International Conference on
Machine Learning , pages 3925–3934. PMLR.
Yuan Li, Chunyuan Li, Yizhe Zhang, Xiujun Li, Guo-
qing Zheng, Lawrence Carin, and Jianfeng Gao.
2020. Complementary auxiliary classifiers for label-
conditional text generation. Proceedings of the AAAI
Conference on Artificial Intelligence , 34(05):8303–
8310.
Zhizhong Li and Derek Hoiem. 2017. Learning without
forgetting. IEEE transactions on pattern analysis
and machine intelligence , 40(12):2935–2947.
David Lopez-Paz and Marc’Aurelio Ranzato. 2017.
Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems ,
pages 6467–6476.
Ilya Loshchilov and Frank Hutter. 2016. Sgdr: Stochas-
tic gradient descent with warm restarts.
Arun Mallya, Dillon Davis, and Svetlana Lazebnik.
2018. Piggyback: Adapting a single network to mul-
tiple tasks by learning to mask weights. In Proceed-
ings of the European Conference on Computer Vision
(ECCV) , pages 67–82.
Arun Mallya and Svetlana Lazebnik. 2018. Packnet:
Adding multiple tasks to a single network by iterative
pruning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages
7765–7773.Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: Understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM Conference on Recommender Systems , RecSys
’13, page 165–172, New York, NY , USA. Association
for Computing Machinery.
Michael McCloskey and Neal J Cohen. 1989. Catas-
trophic interference in connectionist networks: The
sequential learning problem. In Psychology of learn-
ing and motivation , volume 24, pages 109–165. Else-
vier.
Pablo Mendes, Max Jakob, and Christian Bizer. 2012.
DBpedia: A multilingual cross-domain knowledge
base. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC’12) , pages 1813–1817, Istanbul, Turkey. Eu-
ropean Language Resources Association (ELRA).
Razvan Pascanu and Yoshua Bengio. 2013. Revisiting
natural gradient for deep networks. arXiv preprint
arXiv:1301.3584 .
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP) , pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kem-
bhavi, Ali Farhadi, and Mohammad Rastegari. 2019.
What’s hidden in a randomly weighted neural net-
work? arXiv preprint arXiv:1911.13299 .
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. 2017. icarl: In-
cremental classifier and representation learning. In
Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition , pages 2001–2010.
Mark B Ring. 1998. Child: A first step towards contin-
ual learning. In Learning to learn , pages 261–292.
Springer.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Des-
jardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
2016. Progressive neural networks. arXiv preprint
arXiv:1606.04671 .
Jonathan Schwarz, Jelena Luketina, Wojciech M Czar-
necki, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. 2018.
Progress & compress: A scalable framework for con-
tinual learning. arXiv preprint arXiv:1805.06370 .
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon
Kim. 2017. Continual learning with deep generative
replay. In Advances in Neural Information Process-
ing Systems , pages 2990–2999.
Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2019.
Lamol: Language modeling for lifelong language
learning. In International Conference on Learning
Representations .

--- PAGE 12 ---
Sebastian Thrun. 1998. Lifelong learning algorithms.
InLearning to learn , pages 181–209. Springer.
Tom Veniat, Ludovic Denoyer, and MarcAurelio Ran-
zato. 2021. Efficient continual learning with modular
networks and task-driven priors. In International
Conference on Learning Representations .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Hong Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo,
Shiyu Chang, and William Yang Wang. 2019. Sen-
tence embedding alignment for lifelong relation ex-
traction. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
796–806, Minneapolis, Minnesota. Association for
Computational Linguistics.
Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi
Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong
Su, Vincent Perot, Jennifer Dy, et al. 2022a. Dual-
prompt: Complementary prompting for rehearsal-
free continual learning. European Conference on
Computer Vision .
Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,
Jennifer Dy, and Tomas Pfister. 2022b. Learning to
prompt for continual learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 139–149.
Yeming Wen, Dustin Tran, and Jimmy Ba. 2020.
Batchensemble: an alternative approach to efficient
ensemble and lifelong learning. arXiv preprint
arXiv:2002.06715 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu,
Aniruddha Kembhavi, Mohammad Rastegari, Jason
Yosinski, and Ali Farhadi. 2020. Supermasks in su-
perposition. In Advances in Neural Information Pro-
cessing Systems , volume 33, pages 15173–15184.
Curran Associates, Inc.Qingsen Yan, Dong Gong, Yuhang Liu, Anton van den
Hengel, and Javen Qinfeng Shi. 2022. Learning
bayesian sparse networks with full experience re-
play for continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 109–118.
Dani Yogatama, Cyprien de Masson d’Autume, Jerome
Connor, Tomas Kocisky, Mike Chrzanowski, Ling-
peng Kong, Angeliki Lazaridou, Wang Ling, Lei
Yu, Chris Dyer, et al. 2019. Learning and evaluat-
ing general linguistic intelligence. arXiv preprint
arXiv:1901.11373 .
Friedemann Zenke, Ben Poole, and Surya Ganguli.
2017. Continual learning through synaptic intel-
ligence. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70 , pages
3987–3995. JMLR. org.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in neural information pro-
cessing systems , pages 649–657.
Jieyu Zhao and Jurgen Schmidhuber. 1996. Incremen-
tal self-improvement for life-time multi-agent rein-
forcement learning. In From Animals to Animats 4:
Proceedings of the Fourth International Conference
on Simulation of Adaptive Behavior, Cambridge, MA ,
pages 516–525.
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosin-
ski. 2019. Deconstructing lottery tickets: Zeros,
signs, and the supermask. In Advances in Neural
Information Processing Systems , pages 3592–3602.
A Appendix for E XSSN ET
A.1 Sparse Overlap to Quantify Parameter
Interference
Next, we propose a measure to quantify parameter
interference for a task i, i.e., the fraction of the
parameters in mask Mithat are already updated by
some previous task. We define sparse overlap as
the difference between the number of parameters
selected by mask MiandMfree
i divided by the to-
tal parameters selected by Mi. Formally, we define
sparse overlap (SO) between current supermask
Miand supermasks for previous tasks {Mj}i−1
j=1
as,
SO(Mi,{Mj}i−1
j=1) =sum(Mi)−sum(Mfree
i)
sum(Mi)
(2)
andMfree
i=Mi∧ ¬(∨i−1
j=1(Mj))
where ∧,∨,¬are logical and,or, and notopera-
tions.

--- PAGE 13 ---
ID Task Sequence
S1 mnli qqp qnli sst2 cola (Dec. data Size)
S2 ag yelp amazon yahoo dbpedia
S3 yelp yahoo amazon dbpedia ag
S4 dbpedia yahoo ag amazon yelp
S5 yelp ag dbpedia amazon yahoo
S6 ag yelp yahoo
S7 yelp yahoo ag
S8 yahoo ag yelp
Table 6: Task sequences used in text experiments. For the
GLUE dataset, we use order corresponding to decreasing
train data size. Sequence S2-S8 are from (Huang et al.,
2021; de Masson d 'Autume et al., 2019; Sun et al., 2019).
Dataset Class Type Train Validation Test
AGNews 4 News 8k 8k 7.6k
Yelp 5 Sentiment 10k 10k 7.6k
Amazon 5 Sentiment 10k 10k 7.6k
DBPedia 14 Wikipedia 28k 28k 7.6k
Yahoo 10 Q&A 20k 20k 7.6k
Table 7: Statistics for sampled data used from Huang et al.
(2021) for hyperparameter tuning. The validation set is the
same size as the train set. Class means the number of output
classes for the text classification task. Type is the domain of
text classification.
A.2 Space, Time, and Memory Complexity of
EXSSN ET
For training, we store an additional set of scores on
GPU with size as the model weight. The additional
GPU memory required is a small fraction because
the model activations account for a huge fraction
of the total GPU memory. Our runtime is similar to
training the weight of a model with <5%overhead
due to the logical operations on masks and mask-
ing weight during the forward passes. For training
time comparisons refer to Appendix Table 13. On
the disk, we need to store k∗ |W|updated weights
of 32-bits and boolean mask which takes 1-bit for
each parameter. Hence, we take max(|W| ∗k∗
t,|W|)∗32 +|W| ∗1bits in total as in the worst
case we need to store all |W|model weights.
A.3 Experimental setup and hyperparameters
Unless otherwise specified, we obtain supermasks
with a mask density of 0.1. In our CNN models, we
use non-affine batch normalization to avoid storing
their means and variance parameters for all tasks
(Wortsman et al., 2020). Similar to (Wortsman
et al., 2020), bias terms in our model are 0 and
we randomly initialize the model parameters using
signed kaiming constant (Ramanujan et al., 2019).
We use Adam optimizer (Kingma and Ba, 2014)along with cosine decay (Loshchilov and Hutter,
2016) and conduct our experiments on GPUs with
12GB of memory. We used approximately 6 days
of GPU runtime. For our main experiment, we run
three independent runs for each experiment and
report the averages for all the metrics and experi-
ments. For natural language tasks, unless specified
otherwise we initialize the token embedding for
our methods using a frozen BERT-base-uncased
(Devlin et al., 2018) model’s representations using
Huggingface (Wolf et al., 2020). We use a static
CNN model from Kim (2014) as our text classifier
over BERT representations. The model employs
1D convolutions along with Tanh activation. The
total model parameters are ∼110M Following Sun
et al. (2019); Huang et al. (2021), we evaluate our
model on various task sequences as provided in
Appendix Table 6, while limiting the maximum
number of tokens to 256. Following (Wortsman
et al., 2020), we use LeNet (Lecun et al., 1998)
for SplitMNIST dataset, a Resnet-18 model with
fewer channels (Wortsman et al., 2020) for Split-
CIFAR100 dataset, a ResNet50 model (He et al.,
2016) for TinyImageNet dataset. Unless specified,
we randomly split all the vision datasets to obtain
five tasks with disjoint classes. We use the code-
base of DER (Buzzega et al., 2020) to obtain the
vision baselines. In all our experiments, all meth-
ods perform an equal number of epochs over the
datasets. We use the hyperparameters from Worts-
man et al. (2020) for our vision experiments.
For the ablation experiment on natural language
data, following Huang et al. (2021), we use a sam-
pled version of the WebNLP datasets due to lim-
ited resources. The reduced dataset contains 2000
training and validation examples from each out-
put class. The test set is the same as the main
experiments. The dataset statistics are summarized
in Table 7. For WebNLP datasets, we tune the
learning rate on the validation set across the values
{0.01,0.001,0.0001}, for GLUE datasets we use
the default learning rate of the BERT model. For
our vision experiments, we use the default learning
rate for the dataset provided in their original im-
plementation. For TinyImageNet, SplitCIFAR100,
SplitMNIST dataset, we run for 30, 100, and 30
epochs respectively. We store 0.1% of our vision
datasets for replay while for our language exper-
iments we use 0.01% of the data because of the
large number of datasets available for them.

--- PAGE 14 ---
Method Average Accuracy Forgetting
SupSup 68.07 0.00
ExSSNeT 74.77 0.00
Table 8: Comparision between EXSSN ETand the best base-
line SupSup on Imagenet Dataset.
K 1 5 10 20 50
EXSSN ET71.38 71.66 71.01 70.46 69.74
Table 9: Effect of varying kwhile keeping the number of
batches used for the KKT module fixed.
Num. Batches 2 5 10 50 100
EXSSN ET 70.65 70.63 71.01 71.07 71.6
Table 10: Effect of varying the number of batches while
keeping the kfor top- kneighbours fixed for KKT module
fixed.
Method S-MNIST S-CIFAR100 S-TinyImageNet
SupSup 22.6 18.9 18.1
+ KKT 46.4 48.3 52.4
SSN ET 22.5 17.6 18.6
+ KKT 52.7 49.9 52.4
EXSSN ET 22.5 17.3 18.5
+ KKT 47.8 48.8 52.4
Table 11: We report the average sparse overlap for all
method and dataset combinations reported in Table 3.
A.4 Additional Results
A.4.1 Results on Imagenet Dataset
In this experiment, we take the ImageNet dataset
(Deng et al., 2009) with 1000 classes and divide it
into 10 tasks where each task is a 100-way classifi-
cation problem. In Table 8, we report the results for
ExSSNeT and the strongest vision baseline method,
SupSup. We omit other methods due to resource
constraints. We observe a strong improvement of
6.7% of EXSSN ETover SupSup, indicating that
the improvements of our methods exist for large
scale datasets as well.
A.4.2 Analysis of Efficiency, Runtime, and
hyperparameters of the KKT module
Firstly, we would like to note that the KKT module
is lightweight and efficient because it only runs
once for each task before we start training on it
and only uses a few batches to estimate the rele-
vant mask. Given that we perform multiple epochs
over the task’s data, the cost of the KKT module
becomes negligible in comparison to it and runs in
almost similar clock time as without it. The run-
time on splitcifar100 datasets with 100 epochs for
ExSSNeT is 168 minutes and for ExSSNeT + KKT
20 40 60 80 10030405060708090
Naive
LwF
SI
EWC
AGEM
RPC
ER
DER
DER++
SupSup
SSNeT
ExSSNeT
Num. Seen ClassesAverage AccuracyFigure 6: Average Accuracy of all seen tasks as a function of
the number of learned classes for the Split-CIFAR100 dataset.
is 173 minutes which is a very small difference.
Second, there are two main hyperparameters in
the KKT module – (1) kfor taking the majority
vote of top- kneighbors, and (2) the total number
of batches used from the current task in this learn-
ing and prediction process. We present additional
results on the splitcifar100 dataset when changing
these hyperparameters one at a time.
In Table 9, we use 10 batches for KKT with a
batch size of 64, resulting in 640 samples from
the current task used for estimation. We report
the performance of EXSSN ETwhen varying k.
From this table, we observe that the performance
increases with kand then starts to decrease but in
general most values of kwork well.
Next, in Table 10, we use a fixed k=10 and vary
the number of batches used for KKT with a batch
size of 64 and report the performance of EXSS-
NET. We observe that as the number of batches
used for finding the best mask increases the pre-
diction accuracy increases because of better mask
selection. Moreover, as few as 5-10 batches work
reasonably well in terms of average accuracy.
From both of these experiments, we can observe
that the KKT module is fairly robust to different
values of these hyperparameters but carefully se-
lecting them hyperparameters can lead to slight
improvement.
A.4.3 Class Incremental Learning
We performed Class Incremental Learning experi-
ments on the TinyImageNet dataset (10-tasks, 20-
classes in each) and used the One-Shot algorithm
from SupSup (Wortsman et al., 2020) to select the
mask for inference. Please refer to Section-3.3 and
Equation-4 of the SupSup paper (Wortsman et al.,
2020) for details. From Table 12, we observe that
EXSSN EToutperforms all baseline methods that

--- PAGE 15 ---
Method BufferSize TinyImageNet
SGD 0 7.92
oEWC 0 7.58
LwF 0 8.46
ER 200 8.49
A-GEM 200 8.07
iCARL 200 7.53
DER 200 11.87
DER++ 200 10.96
SupSup 0 10.27
ExSSNeT 0 11.21
Table 12: Results for CIL setting.
do not use Experience Replay by at least 2.75%.
Moreover, even with the need for a replay buffer,
EXSSN EToutperforms most ER-based methods
and is comparable to that of DER.
A.4.4 Sparse Overlap Numbers
In Table 11, we report the sparse overlap num-
bers for SupSup, SSN ET, and EXSSN ETwith and
without the KKT knowledge transfer module. This
table corresponds to the results in main paper Table
3.
A.4.5 Average Accuracy Evolution
In Figure 6, we plotP
i≤tAtivst, that is the av-
erage accuracy as a function of observed classes.
This plot corresponds to the SplitCIFAR100 re-
sults provided in the main paper Table 2. We can
observe from these results that Supsup and ExSS-
NeT performance does not degrade when we learn
new tasks leading to a very stable curve whereas
for other methods the performance degrades as we
learn new tasks indicating some degree of forget-
ting.Algorithm 1 EXSSN ET training procedure.
Input: TasksT, a model M, mask sparsity k, exclusive=True
Output: Trained model
▷Initialize model weights W(0)
initialize_model_weights( M)
forall i∈range (|T |)do
▷Set the mask Micorresponding to task tifor
optimization.
mask_opt_params = Mi
▷Learn the supermask Miusing edge-popup
forall em∈mask_epochs do
Mi= learn_supermask(model, mask_opt_params, ti)
end
▷Model weight at this point are same as the
last iteration W(i−1)
ifi >1and exclusive then
▷Find mask for all the weights used by
previous tasks.
M1:i−1=∨i−1
j=1(Mj)
▷Get mask for weights in Miwhich are not
in{Mi}i−1
j=1
Mfree
i=Mi∧ ¬M1:i−1
▷Find non-overlapping weight for updating.
W(i)
free=Mfree
i⊙W(i−1)
else if not exclusive then
W(i)
free=W(i−1)
end
weight_opt_params = W(i)
free
▷Learn the free weight in the supermask Mi
forall em∈weight_epochs do
W(i)= update_weights(model, weight_opt_params, ti)
end
end
A.4.6 Runtime Comparison across methods
In this Section, we provide the result to compare
the runtime of various methods used in the paper.
We ran each method on the sampled version of the
WebNLP dataset for the S2task order as defined
in Table 6. We report the runtime of methods for
four epochs over each dataset in Table 13. Note
that the masking-based method, SupSup, SSN ET,
EXSSN ETtakes much lower time because they
are not updating the BERT parameters and are just
finding a mask over a much smaller CNN-based
classification model using pretrained representation
from BERT. This gives our method an inherent
advantage that we are able to improve performance
but with significantly lower runtime while learning
a mask over much fewer parameters for the natural
language setting.
A.4.7 Validation results
In Table 14, we provide the average validation ac-
curacies for the main natural language results pre-
sented in Table 1. We do not provide the validation
results of LAMOL (Sun et al., 2019) and MBPA++
(de Masson d 'Autume et al., 2019) as we used the
results provided in their original papers. For the
vision domain, we did not use a validation set be-
cause no hyperparameter tuning was performed as
we used the experimental setting and default param-

--- PAGE 16 ---
Method Runtime (in minutes)
Multitask 200
Finetune 175
Replay 204
AdapterBERT + FT 170
AdapterBERT + Replay 173
MultiAdaptBERT 170
Regularization 257
IDBR 258
SupSup 117
SSN ET 117
EXSSN ET 117
Table 13: Runtime comparison of different methods used in
the text experiments.
Method ( ↓) GLUE WebNLP
Order ( →) S1 S2 S3 S4 S5 Average
Random 33.3 7.14 7.14 7.14 7.14 7.14
Multitask 80.6 77.4 77.5 76.9 76.8 77.1
FT 14.0 27.0 22.9 30.4 15.6 24.0
Replay 79.7 75.2 74.5 75.2 75.5 75.1
AdaptBERT + FT 25.1 20.8 19.1 23.6 14.6 19.5
AdaptBERT + Replay 78.6 73.3 74.3 74.7 74.6 74.2
MultiAdaptBERT 83.6 76.7 76.7 76.7 76.7 76.7
Regularization 75.5 75.9 75.0 76.5 76.3 75.9
IDBR 77.5 75.8 75.4 76.4 76.4 76.0
SupSup 78.1 75.7 76.0 76.0 75.9 75.9
SSN ET 77.2 76.3 76.3 77.0 76.1 76.4
EXSSN ET 80.1 77.1 77.3 77.2 77.1 77.2
Table 14: Average validation accuracy ( ↑) for multiple tasks
and sequence orders with previous state-of-the-art (SotA)
methods.
eters from the original source code from (Wortsman
et al., 2020; Wen et al., 2020).
A.4.8 Effect of Task Order and Number of
Tasks
Following Huang et al. (2021), we conduct experi-
ments to study the effect of task length and order
in the language domain. We use task sequences
of lengths three and five, with multiple different
task orders on the sampled data (Section 4.1, Ta-
ble 6, and Appendix) to characterize the impact
of these variables on the performance. In Table
15, we present the average test accuracy averaged
over three different random seeds. We observe
that across all six different settings our method
performs better compared to all the baseline meth-
ods. Our methods bridge the gap toward multitask
methods’ performance, leaving a gap of 0.36% and
1.19% for lengths three and five sequences, respec-
tively.
A.5 Additional Model Details
A.5.1 Algorithm for E XSSN ET
In Algorithm 1, we provide a pseudo-code for our
method EXSSN ETfor easier reference and un-
derstanding. We also attach our working code assupplementary material to encourage reproducibil-
ity.
A.5.2 Model Diagram for Supsup
In Figure 7, we provide the canonical model dia-
gram for SupSup. Please read the figure description
for more details regarding the distinctions between
SupSup and ExSSNeT.

--- PAGE 17 ---
Model ( ↓) Length-5 WebNLP Length-3 WebNLP
Order ( →) S2 S3 S4 Average S6 S7 S8 Average
Random 7.14 7.14 7.14 7.14 10.0 10.0 10.0 10.0
MTL 75.09 75.09 75.09 75.09 74.16 74.16 74.16 74.16
Finetune † 32.37 32.22 26.44 30.34 25.79 36.56 41.01 34.45
Replay † 68.25 70.52 70.24 69.67 69.32 70.25 71.31 70.29
Regularization † 72.28 73.03 72.92 72.74 71.50 70.88 72.93 71.77
AdaptBERT 30.49 20.16 23.01 24.55 24.48 31.08 26.67 27.41
AdaptBERT + Replay 69.30 67.91 71.98 69.73 66.12 69.15 71.62 68.96
IDBR† 72.63 73.72 73.23 73.19 71.80 72.72 73.08 72.53
SupSup 74.01 74.04 74.18 74.08 72.01 72.35 72.53 72.29
SSN ET 74.5 74.5 74.65 74.55 73.1 72.92 73.07 73.03
EXSSN ET 74.78 74.72 74.71 74.73 72.67 72.99 73.24 72.97
Table 15: Average test accuracy reported over task sequences for three independent runs on sub-sampled data. Results with
†are taken from Huang et al. (2021).
Randomly initialized
weights 
Mask  over
weights  Mask  over
weights  Mask  over
weights  : Untrained weights
: Task 1 : Task 2
: Task 3
Find Mask
for Task 1 
 Find Mask
for T
ask 3
 Find Mask
for Task 2  
Figure 7: This is a canonical model diagram for SupSup. In SupSup, the model weights are always fixed at the random
initialization W(0). For each task SupSup learns a new mask (in this case M1, M 2, M 3) over the weights W(0). A mask
selectively activates a subset of weights for a particular task. This subset of selected weights forms a subnetwork inside the
full model which we refer to as the supermask subnetwork. For example, when learning Task 2, SupSup learns the mask M2
(the weights activated by the mask are highlighted in green) over the fixed weight W(0). These highlighted weights along with
the participating nodes are the subnetwork formed by mask M2. Whenever a prediction is made for Task 2 samples, this mask
is selected and used to obtain the predictions. Please note that the model weights W(0)are never updated after their random
initialization. Hence, for SupSup there is no learned knowledge sharing across tasks. This is in contrast to our setup in Figure 1,
where for the first task the mask is learned over the weights W(0)but once the mask is selected the weights of the corresponding
subnetwork are also updates to obtain new weight W(1). Then the next task’s mask is learned over these new set of weights
W(1)and so on. Also note that in Figure 1, we do not show the KKT knowledge transfer module here to avoid confusion.
