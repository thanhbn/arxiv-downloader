# 2305.06408.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2305.06408.pdf
# File size: 2296989 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Accelerating Batch Active Learning Using Continual Learning
Techniques
Arnav Das∗†Gantavya Bhatt∗†Megh Bhalerao†Vianne Gao⋄Rui Yang⋄Jeff Bilmes†
†University of Washington, Seattle⋄Memorial Sloan Kettering Cancer Center
{arnavmd2, gbhatt2, bilmes}@uw.edu
Abstract
A major problem with Active Learning (AL) is high training costs since models are typically
retrained from scratch after every query round. We start by demonstrating that standard
AL on neural networks with warm starting fails, both to accelerate training and to avoid
catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new
class of techniques, circumventing this problem, by biasing further training towards previously
labeled sets. We accomplish this by employing existing, and developing novel, replay-based
Continual Learning (CL) algorithms that are effective at quickly learning the new without
forgetting the old, especially when data comes from an evolving distribution. We call this
paradigm "Continual Active Learning" (CAL) . We show CAL achieves significant speedups
usingaplethoraofreplayschemesthatusemodeldistillationandthatselectdiverse/uncertain
pointsfromthehistory. Weconductexperimentsacrossmanydatadomains, includingnatural
language, vision, medical imaging, and computational biology, each with different neural
architectures and dataset sizes. CAL consistently provides a ∼3x reduction in training time,
while retaining performance and out-of-distribution robustness, showing its wide applicability.
1 Introduction
While neural networks have been successful in a variety of different supervised settings, most such approaches
are labeled-data hungry and require significant computation. From a large pool of unlabeled data, active
learning (AL) selects subsets of points to label by imparting the learner with the ability to query a human
annotator. Such methods incrementally add points to the labeled pool by repeatedly: (1) training a model
from scratch on the current labeled pool and (2) using some measure of model uncertainty and/or diversity
to select a set of points to query the annotator (Settles, 2009; 2011; Wei et al., 2015; Ash et al., 2020;
Killamsetty et al., 2021a). AL has been shown to reduce the amount of training data required but can be
computationally expensive since it requires retraining a model, typically from scratch, after each query round.
Asimplesolution is to warm start the model parameters between query rounds. However, the observed
speedups tend to still be limited since the model must make several passes through an ever-increasing pool
of data. Moreover, warm starting alone in some cases can hurt generalization, as discussed in Ash & Adams
(2020) and Beck et al. (2021). Another extension to this is to solely train on the newly labeled batch of
examples to avoid re-initialization. However, as we show in Section 4, naive fine-tuning fails to retain accuracy
on previously seen examples since the distribution of the query pool may drastically change with each round.
This problem of catastrophic forgetting while incrementally learning from a series of new tasks with shifting
distributions is a central question in another paradigm called Continual Learning (CL) (French, 1999;
McCloskey & Cohen, 1989; McClelland et al., 1995; Kirkpatrick et al., 2017c). CL has recently gained
popularity, and many algorithms have been introduced to allow models to quickly adapt to new tasks without
forgetting (Riemer et al., 2018; Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019; Aljundi et al., 2019b;
Chaudhry et al., 2020; Kirkpatrick et al., 2017b).
∗Equal contribution
1arXiv:2305.06408v2  [cs.LG]  12 Dec 2023

--- PAGE 2 ---
In this work, we propose Continual Active Learning (CAL)1, which applies Continual Learning strategies to
accelerate batch Active Learning. In CAL, we apply CL to enable the model to learn the newly labeled points
without forgetting previously labeled points while using past samples efficiently using replay-based methods.
As such, we observe that CAL attains significant training time speedups over standard AL. This is beneficial
for the following reasons: (1):As neural networks swell (Shoeybi et al., 2019), so do the environmental
and financial model training costs (Bender et al., 2021; Dhar, 2020; Schwartz et al., 2020). Reducing the
number of gradient updates required for AL will help mitigate such costs, especially with large-scale models.
(2):Reducing AL computational requirements makes AL more accessible for edge computing, IoT, and low-
resource device deployment (Senzaki & Hamelain, 2021) such as with federated learning (Li et al., 2020). (3):
Developing new AL algorithms/acquisition functions, or searching for architectures as done with NAS/AutoML
that are well-suited specifically for AL, can require hundreds or even thousands of runs. Since CAL’s speedups
are agnostic to the AL algorithm and the neural architecture, such experiments can be significantly sped up.
Overall, the importance of speeding machine learning training processes is well recognized, as evidenced by the
plethora of efforts in the computing systems community (Jia et al., 2022; Zhang et al., 2017; Zheng et al., 2022).
1.0 1.5 2.0 2.5 3.0
Average Speedup0.9850.9900.9951.0001.005Avg. relative accuracyCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Figure 1: Summary of our results: Benchmarking average
relative accuracy vs. speedup for the proposed methods
(solid markers ) against the baseline. Each point is averaged
across datasets and labeling budgets; top-right region is
desiderata . This shows that proposed CAL methods (partic-
ularly CAL-DER/SD/SDS2) have equivalent performance
relative to the baseline (if not better) on multiple datasets
and across different budgets, all while providing significant
speedups.In addition, CAL demonstrates another practical
application for CL methods. Many settings used to
benchmark CL methods in recent work are somewhat
contrived (Farquhar & Gal, 2018; Wallingford et al.,
2023). Most CL work considers the class/domain
incremental setting, where only the samples that
belong to a subset of the set of classes/domains of
the original dataset are available to the model at
any given time. This setting need not be the only
benchmark upon which CL methods are evaluated.
We suggest that the evaluation of future new CL
algorithms should be determined not only on tradi-
tional CL evaluation schemes and benchmarks but
also on their performance in the CAL setting.
In the present work, we are not using CL to improve
or change AL querying strategies. We view this as
both a strength of the present work and an oppor-
tunity for future work. Firstly, it is a strength of
our present work since any AL query strategy, both
old and new, can in principle be applied in the CAL
setting, both for speeding up the AL strategy and offering, as argued above, a test bed for CL. Indeed, a
major AL research challenge includes combining AL with other techniques, as we believe we have done herein.
Secondly, it is an opportunity since there is nothing inherent in the definition of CL that precludes the CL
tasks from being dependent on the model as we show below. There may be a way for CL to open doors to
intrinsically new AL querying policies of attaining new batches of unlabeled data. This we leave to the future.
Our present core goal is to accelerate batch AL training via CL techniques while preserving accuracy .
To the best of our knowledge, this application of CL algorithms to accelerate batch AL has never been
explored. Our contributions can be summarized as follows: (1)We first demonstrate that batch active learning
techniques can benefit from continual learning techniques, and their merger creates a new class of techniques
that we propose to call the “CAL framework.” (2)We benchmark several existing CL methods (CAL-ER,
CAL-DER, CAL-MIR) as well as novel methods (CAL-SD, CAL-SDS2) and evaluate them on diverse datasets
based on the accuracy/speedup they can attain over standard AL. (3)We study speedup/performance trade-
offs on datasets that vary in modality (natural language, vision, medical imaging, and computational biology),
neural architecture with varying degrees of computation (Transformers/CNNs/MLPs), data scale (including
some larger datasets, one having 2M samples), and class-balance. And (4), lastly, we demonstrate that models
trained with CAL and standard AL models behave similarly, in that both classes of models attain similar
1There have been papers published with titles containing the “Continual Active Learning” phrase, but these do not merge
Continual Learning with Active Learning as we do, hence our name.
2

--- PAGE 3 ---
uncertainty scores on held-out datasets and achieve similar robustness performance on out-of-distribution
data. Figure 1 summarizes our results, detailed later in the paper and greatly detailed in the appendices.
2 Related Work
Active learning (Atlas et al., 1989; Cohn et al., 1994; Wei et al., 2015; Killamsetty et al., 2021a; Ash et al.,
2020) has demonstrated label efficiency over passive learning. In addition, there has been extensive work
on theoretical aspects of AL (Guillory et al., 2009; Hanneke, 2009; 2007; Balcan et al., 2010) where Hanneke
(2012) shows sample complexity advantages over passive learning in noise-free classifier learning for VC classes.
More recently, Active Learning has also been studied as a procedure to incrementally learn the underlying data
distribution with the help of discrepancy framework (Mathelin et al., 2022; Cui & Sato, 2020; Shui et al., 2019).
Recently there has been an interest in speeding up active learning since most deep learning is computationally
demanding. Kirsch et al. (2019); Pinsler et al. (2019); Sener & Savarese (2018) aim to reduce the number of
query iterations by having large query batch sizes. However, they do not exploit the learned models from
previous rounds for the subsequent ones and are therefore complementary to CAL. Work such as Coleman
et al. (2020a); Ertekin et al. (2007); Mayer & Timofte (2020); Zhu & Bento (2017); Zhang et al. (2023) speeds
up the selection of the new query set by appropriately restricting the search space or by using generative
methods. This work can be easily integrated into our framework because CAL works on the training side
of active learning, not on the query selection. On the other hand, Lewis & Catlett (1994); Coleman et al.
(2020b); Yoo & Kweon (2019) use a smaller proxy model to reduce computation overhead, however, they still
follow the standard active learning protocol, and therefore can be accelerated when integrated with CAL.
There exists work that explores continual/transfer learning and active learning in the same context. Perkonigg
et al. (2021) propose an approach that allows active learning to be applied to data streams of medical images
by introducing a module that detects domain shifts. This is quite distinct from our work: our work uses
CL algorithms to prevent catastrophic forgetting and to accelerate learning. Zhou et al. (2021) study when
standard active learning is used to fine-tune a pre-trained model, and employs transfer learning — this does
not consider continual learning and active learning together, however, and is therefore not related to our
work. Finally, Ayub & Fendley (2022) studies where a robot observes unlabeled data sampled from a shifting
distribution, but does not explore active learning acceleration.
For preventing catastrophic forgetting, we mostly focus on replay-based algorithms that are state-of-the-
art methods in CL. However, as demonstrated in Section 4 on how active learning rounds can be viewed
in a continual learning context, one can apply other methods such as EWC (Kirkpatrick et al., 2017a),
Bayesian divergence priors Li & Bilmes (2007), structural regularization (Li et al., 2021) or functional
regularization (Titsias et al., 2020) as well.
The effect of warm-started model training on generalization and convergence speed has been explored by
Ash & Adams (2020) which empirically demonstrates that a model that has been pretrained on a source
dataset converges faster but exhibits worse generalization on a target dataset when compared to a randomly
initialized model. However, that work only considers the setting where the source and target datasets are
unbiased estimates of the same distribution. This is distinct from our work since the distributions we consider
are all dependent on the model at each AL round. Furthermore, our work employs CL methods in addition
to warm-starting, also not considered in Ash & Adams (2020).
3 Background
3.1 Batch Active Learning
Define [n] ={1,...,n}, and letXandYdenote the input and output domains respectively. AL typically
starts with an unlabeled dataset U={xi}i∈[n], where each xi∈X. The AL setting allows the model f, with
parameters θ, to query a user for labels for any x∈U, but the total number of labels is limited to a budget b,
whereb≤n. Throughout the work, we consider classification tasks so the output of f(x;θ)is a probability
distribution over classes. The goal of AL is to ensure that fcan attain low error when trained only on the
set ofblabeled points.
3

--- PAGE 4 ---
Figure 2: Visualization of standard AL when training a linear model on a simple 2D synthetic dataset. Each plot
shows which samples were chosen by uncertainty sampling to be added to the labeled pool at given a query round.
Notably, the distributions of the new labeled points vary significantly between rounds motivating CAL.
Algorithm 1 details the general AL procedure. Lines 3-6 construct the seed set D1by randomly sampling
a subset of points from Uand labeling them. Lines 7-14 iteratively expand the labeled set for Trounds
by training the model from a random initialization on Dtuntil convergence and selecting btpoints (where/summationtext
t∈[T]bt=b) fromUbased on some selection criteria that is dependent on θt. The selection criteria generally
select samples based on model uncertainty and/or diversity (Lewis & Gale, 1994; Dagan & Engelson, 1995;
Settles; Killamsetty et al., 2021a; Wei et al., 2015; Ash et al., 2020; Sener & Savarese, 2017). In this work, we
primarily consider uncertainty sampling Lewis & Gale (1994); Dagan & Engelson (1995); Settles, though we
also test other selection criteria in Section C in the Appendix.
Algorithm 1 Batch Active Learning
1:procedure ActiveLearning (f,U,b1:T,T)
2:t←1,L←∅ ▷Initialize
3:Ut∼U ▷Drawb1samples fromU
4:Dt←{ (xi,yi)|xi∈Ut} ▷Provide labels
5:U←U\U t ▷Remove from unlabeled set
6: L←L∪Dt ▷Add to labeled set
7: whilet≤Tdo
8: Randomly initialize θinit
9:θt←Train (f,θinit,L)
10:Ut+1←Select (f,θt,U,bt) ▷Selectbtpoints based on θt
11:Dt+1←{ (xi,yi)|xi∈Ut}
12:U←U\U t+1;L←L∪Dt+1;t←t+ 1
13:θT←Train (f,θinit,L)
14:return L,θTUncertainty Sampling is a widely-
used practical AL method that selects
Ut={x1,...,xbt} ⊆ U by choos-
ing the samples that maximize a no-
tion of model uncertainty. We consider
entropy (Dagan & Engelson, 1995) as
the uncertainty metric, so if hθ(x)≜
−/summationtext
i∈[k]f(x;θ)ilogf(x;θ)i, thenUt+1∈
argmax
A⊂U :|A|=bt/summationtext
x∈Ahθt(x).
Distribution Shift As shown in Algo-
rithm1, thesetofsamplesthatarelabeled
at a given query round are dependent on
the model parameters. Since the model is
repeatedly updated, the distribution of newly labeled samples may vary across different AL rounds. This is
illustrated in Figure 2, where we perform standard AL in a simplified setting. In the example, we use a linear
model to perform classification on a 2D synthetic dataset and perform AL with entropy-based uncertainty
sampling as the acquisition function. Upon visualizing the set of samples that are queried by the model at
different AL rounds, it is evident that distribution shift does occur. We demonstrate empirically in Figure 3
that training on newly labeled samples alone will cause catastrophic forgetting due to distribution shifts.
3.2 Continual Learning
We defineD1:n=/uniontext
i∈[n]Di. In CL, the dataset consists of Ttasks{D1,...,DT}that are presented to the
model sequentially, where Dt={(xi,yi)}i∈Nt,Ntare the task- tsample indices, and nt=|Nt|. At timet∈[T],
the data/label pairs are sampled from the current task (x,y)∼Dt, and the model has only limited access
to the historyD1:t−1. The CL objective is to efficiently adapt the model to Dtwhile ensuring performance
on the history does not appreciably degrade. We focus on replay-based CL techniques that attempt to
approximately solve CL optimization by using samples from D1:t−1to regularize the model while adapting to
Dt. Please refer to appendix B for more details on CL.
4

--- PAGE 5 ---
Continual Learning PapersHistory Access
None Partial Full
Kirkpatrick et al. (2017b), Li & Hoiem (2017) ✓× ×
Aljundi et al. (2019b), Mai et al. (2020) × ✓×
Buzzega et al. (2020), Aljundi et al. (2019a)
Lopez-Paz & Ranzato (2017), Chaudhry et al. (2019) × ✓ ✓
Ratcliff (1990)
Table 1: Different previous CL work organized by the memory access assumptions made in their settings. In
the CAL setting, we do not need to assume that we have limited access to the history so we only consider
CL methods that are applicable to the setting where the history is fully accessible. This category primarily
contains replay-based methods.
Algorithm 2 Continual Learning
1:procedure ContinualTrain (f,θ0,D,H,m,m(h))
2:τ←0 ▷τis a local iteration index.
3: whilenot converged do
4:τ←τ+ 1
5:Bcurrent←{ (xi,yi)}m
i=1∼D ▷Sample from new task.
6:Breplay←Select (f,θτ−1,H,m(h))▷Sample from history.
7:θτ←Update (f,θτ−1,Bcurrent,Breplay )
8:returnθτAlgorithm 2 outlines general replay-based
CL, where the objective is to adapt fpa-
rameterized by θ0toDwhile using sam-
ples from the history H.Bcurrentconsists
ofmpoints randomly sampled from the
current time’sD, andBreplayconsists of
m(h)points chosen based on a criterion
that selects from H. In line 6, θτis com-
puted based on an update rule that uti-
lizes bothBreplayandBcurrent.
Different CL methods assume varying degrees of access to the history. In many practical settings, D1:Tis too
large to store in memory or Tis unknown so CL algorithms assume limited or no access to samples from the
history. Some of these approaches remove unimportant samples from the history Aljundi et al. (2019b); Mai
et al. (2020), while others solely rely on regularizing the model parameters without replaying any samples
from the history Kirkpatrick et al. (2017b); Li & Hoiem (2017). In the CAL setting, D1:Tis already stored in
memory as is done in the standard AL setting so employing a CL algorithm that imposes a memory constraint
would needlessly cause CAL to underperform. In Table 1, a handful of prior CL works are sorted by what
assumptions about memory/history access they make; methods that do not make any memory constraint
assumption are the most well-suited for CAL. A more comprehensive and complete overview of CL methods
can be found in De Lange et al. (2022).
4 Blending Continual and Active Learning
A clear AL inefficiency is that the model fis retrained from scratch on the entire labeled pool after every query
round. One potential solution idea is to simply continue training the model only on the newly AL-queried
samples and, via the process of warm starting, hope that history will not fade. Unfortunately for this
approach, Figure 3 (top) shows that when the model is warm-start trained onlyon the task t(samples
labeled at AL round tusing entropy sampling), historical sample performance deteriorates precipitously while
performance on the validation set flatlines. That is, at AL round t(x-axis), we continue to train the model
until convergence on task tand track accuracy (y-axis) on each previous task and also on the validation set.
The performance of task 1, after the initial drop, tracks that of the validation set, since task 1 is a model
agnostic initial unbiased random subset query of the training data. The performance of task i, fori>1,
however, each of which is the result of model-conditioned AL query, shows perilous historical forgetting. In
the end, the model performs considerably worse on all of the historical tasks (aside from task 1) than on the
validation set, even though it has been trained on those tasks and not on the validation set. This experiment
suggests that: (1) the distribution of each AL-queried task t >1is different than the data distribution;
(2) fine-tuning to task tcan result in catastrophic forgetting; and (3) techniques to combat catastrophic
forgetting are necessary to effectively incorporate new information between successive AL rounds.
5

--- PAGE 6 ---
5060708090100
Forgetting w/ Finetuning
1 2 3 4 5 6 7 8 9
AL Query Round5060708090100
Forgetting w/ CAL-SDS2
Val Set
Task 1
Task 2
Task 3
Task 4
Task 5
Task 6
Task 7
Task 8
Task 9Accuracy (%)Figure 3: A ResNet-18’s performance on CIFAR-10 on the validation set (blue X) and task ifori∈{1,2,..., 9}
(other colors) after training the warm-started model and training just on the new task data (top) compared to
efficiently adapting to newly labeled data with a CAL method (bottom). In the top plot, at each round a new 5%
of the full dataset is added to the labeled pool and the previously trained model is further trained juston the new
points (the isolated upper-right cyan point shows task 9’s performance after round 9). After each round, the accuracy
on the corresponding task does well, but all previous tasks’ accuracies drop precipitously, demonstrating a form
of catastrophic forgetting. In the bottom plot, the same setting is considered but the model is adapted to the new
data with CAL-SDS2, one of the CAL methods. Also, the validation set performance has flatlined despite continued
training on new data when finetuning, but continues to increase when CAL-SDS2 is employed. This demonstrates
that naively fine-tuning a model to newly labeled points can work poorly.
The CAL approach, shown in Algorithm 3, uses CL techniques to ameliorate catastrophic forgetting. The
key difference between CAL and Algorithm 1 is line 9. Instead of standard training, replay-based CL is
used to adapt ftoDtwhile retaining performance on D1:t−1. The speedup comes from two sources: (1)
we are computing gradient updates only on a useful subset of the history D1:t−1rather than all of it for
reasonable choices of m(h); and (2) the model converges faster since it starts warm. The ability of CAL
to combat catastrophic forgetting is shown in Figure Figure 3 (bottom). In the rest of the section, define
Lc(θ)≜E(x,y)∼Bcurrent [ℓ(y,f(x;θ))]. We next define and compare several CAL methods and assess their
performance based on their performance on the test set and the speedup they attain compared to standard
AL.
Algorithm 3 The general CAL approach.
1:procedure CAL(f,U,b1:T,T,m,m(h))
2:t←1,L←∅ ▷Initialize
3:Ut∼U ▷Drawb1samples fromU
4:Dt←{ (xi,yi)|xi∈Ut} ▷Provide labels
5:U←U\U t
6: L←L∪Dt
7:Randomly initialize θ0
8: whilet≤Tdo
9:θt←ContinualTrain( f,θt−1,Dt,D1:t−1,m,m(h))
10:Ut+1←Select (f,θt,U,bt) ▷Selectbtpoints fromU
11:Dt+1←{ (xi,yi)|xi∈Ut+1}
12:U←U\U t+1;L←L∪Dt+1;t←t+ 1
13:θT←ContinualTrain( f,θT−1,DT,D1:T−1,m,m(h))
14:return L,θTExperience Replay (CAL-ER) is
the simplest and oldest replay-based
method (Ratcliff, 1990; Robins, 1995). In
this approach,BcurrentandBreplayare in-
terleaved to create a minibatch Bof size
m+m(h)andBreplayis chosen uniformly
at random fromD1:t−1. The parameters
θof modelfare updated based on the
gradient computed on B.
Maximally Interfered Retrieval
(CAL-MIR) chooses a size- m(h)
subset of points from D1:t−1most likely
to be forgotten (Aljundi et al., 2019a).
Given a batch of mlabeled samples
Bcurrentsampled fromDtand model
parameters θ,θvis computed by taking
a “virtual” gradient step i.e., θv=θ−η∇Lc(θ)whereηis the learning rate. Then for every example xin
the history, sMIR(x) =ℓ(f(x;θ),y)−ℓ(f(x;θv),y)(i.e., the change in loss after taking a single gradient
6

--- PAGE 7 ---
step) is computed. The m(h)samples with the highest sMIRscore are selected for Breplay, and the remainder
is similar to experience replay. BcurrentandBreplayare concatenated to form the minibatch (as in CAL-ER),
upon which the gradient update is computed. In practice, selection is done on a random subset of D1:t−1
for speed, since computing sMIRfor every historical sample is prohibitively expensive.
Dark Experience Replay (CAL-DER) uses a distillation approach to regularize updates (Buzzega
et al., 2020). Let g(x;θ)denote the pre-softmax logits of classifier f(x;θ), i.e.,f(x;θ) =softmax (g(x;θ)). In
DER, every x′∈D1:t−1has an associated z′which corresponds to the model’s logits at the end of the task
whenxwas first observed — if x′∈Dt′, thenz′≜g(x′;θ∗
t′))wheret′∈[t−1]andθ∗
t′are the parameters
obtained after round t′. DER minimizes LDER(θ)defined as:
LDER(θ)≜Lc(θ) + E
(x′,y′,z′)∼Breplay/bracketleftbig
α∥g(x′;θ)−z′∥2
2+βℓ(y′,f(x′;θ))/bracketrightbig
,(1)
whereBreplayis a batch uniformly at randomly (w/o replacement) sampled from D1:t−1, andαandβare
tuneable hyperparameters. The first term ensures that samples from the current task are classified correctly.
The second term consists of a classification loss and a mean squared error (MSE) based distillation loss
applied to historical samples.
Scaled Distillation (CAL-SD) LSD(θ)is a new objective proposed in this work defined via:
Lreplay (θ)≜ E
(x′,y′,z′)∼Breplay/bracketleftbigg
αDKL/parenleftig
softmax (z′)||f(x′;θ)/parenrightig
+ (1−α)ℓ(y′,f(x′;θ))/bracketrightbigg
, (2)
and thenLSD(θ)≜λtLc(θ) + (1−λt)Lreplay (θ)whereλt≜|Dt|/(|Dt|+|D1:t−1|). Similar to CAL-DER,
Lreplayis a sum of two terms: a distillation loss and a classification loss. The distillation loss expresses the KL-
divergence between the posterior probabilities produced by fandsoftmax (z′), wherez′is defined in the DER
section. We use KL-divergence instead of MSE loss on the logits so that the distillation and the classification
losses have the same scale and dynamic range, and also since it allows the logits to drift by a constant term
that does not affect the softmax output but does effect the MSE loss. α∈[0,1]is a tuneable hyperparameter.
The weight of each term is determined adaptively by a “stability/plasticity” trade-off term λt. A stability-
plasticity dilemma is commonly found in both biological and artificial neural networks (Abraham & Robins,
2005; Mermillod et al., 2013). A network is stableif it can effectively retain past information but cannot
adapt to new tasks efficiently, whereas a network that is plasticcan quickly learn new tasks but is prone
to forgetting. The trade-off between stability and plasticity is a well-known constraint in CL (Mermillod
et al., 2013). For CAL, we want the model to be plastic early on, and stable later on. We apply this intuition
withλt: higher values indicate higher plasticity, since minimizing the classification error of samples from the
current task is prioritized. Since D1:t−1increases with t,λtdecreases and the model becomes more stable in
later training rounds.
Scaled Distillation w/ Submodular Sampling (CAL-SDS2) CAL-SDS2 is another new CL approach
we introduce in this work. CAL-SDS2 uses CAL-SD to regularize the model and uses a submodular sampling
procedure to select a diverse representative set of history points to replay. Submodular functions are well-
known to be suited to capture notions of diversity and representativeness (Lin & Bilmes, 2011; Wei et al., 2015;
Bilmes, 2022) and the simple greedy algorithm can approximately maximize, under a cardinality constraint,
a monotone submodular function up to a 1−e−1constant factor multiplicative guarantee (Fisher et al., 1978;
Minoux, 1978; Mirzasoleiman et al., 2015). We define our submodular function Gas:
G(S)≜/summationdisplay
xi∈Amax
xj∈Swij+λlog/parenleftigg
1 +/summationdisplay
xi∈Sh(xi)/parenrightigg
. (3)
The first term is a facility location function, where wijis a similarity score between samples xiandxj. We use
wij=exp (−∥zi−zj∥2/2σ2)whereziis the penultimate layer of model fforxiandσis a hyperparameter.
The second term is a concave over modular function (Liu et al., 2013) and h(xi)is a standard AL measure
of model uncertainty, such as entropy of the model’s output distribution. Both terms are well known to be
7

--- PAGE 8 ---
monotone non-decreasing submodular, as is their non-negatively weighted sum (Bilmes, 2022). The core
reason for applying a concave function over a model-uncertainty-score-based modular function, instead of
keeping it as a pure modular function, is to better align the modular uncertainty values with the facility
location function. Otherwise, if we do not apply the concave function, the facility location function dominates
during the early steps of the greedy algorithm and the modular function dominates in the later steps of greedy.
In order to speed up SDS2 (to avoid the need to perform forward passes over the entire history before a single
step), we randomly subsample the history indices to produce Aand re-compute forward passes to produce
freshzivalues fori∈Abefore re-constructing G(S), and we then perform submodular maximization; thus
S⊂A⊂D 1:t−1. The objective of CAL-SDS2 is to ensure that the set of samples that are replayed are both
difficult and diverse, similar to the motivation of the heuristic employed in Wei et al. (2015).
Baselines All proposed CAL methods are compared against two AL baselines. The first baseline is standard
active learning, denoted as AL, which is no different from the procedure shown in Algorithm 1. We also
consider active learning with warm starting (AL w/ WS), which uses the converged model from the previous
round to initialize the model for the current round. Both models retrain on the entire dataset at each query
round.
5 Experiments and Results
1 2 3 40.9951.0001.0051.010Avg. relative accuracyCOLA
1 2 3 41.0001.0021.0041.006CIFAR10
1 2 3 40.9991.0001.001FMNIST
1 2 3 40.9000.9250.9500.9751.000MedMNIST
1 2 3 41.0001.0051.0101.0151.020Celltype
1 2 3 40.9950.9981.0001.002Amazon Polarity
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Average Speedup
Figure 4: Relative accuracy vs. speedup averaged over different labeling budgets. For every dataset, we have at least
oneCAL method, that is faster with same accuracy than baseline ( top right region is desiderata ).
We evaluate the validation performance of the model when we train on different fractions ( b/n) of the full
dataset. We compute the speedupattained by a CAL method by dividing the wall-clock training time of
the baseline AL method over the wall-clock training time of the CAL method. We test the CAL methods
on a variety of different datasets spanning multiple modalities. Two baselines do not utilize CAL which are
standard AL(active learning) as well as AL w/ WS (active learning with warm starting but still training
using all the presently available labeled data).
Our objective is to demonstrate: (1) at least one CAL-based method exists that can match or outperform
a standard active learning technique while achieving a significant speedup for every budget and dataset and
(2) models that have been trained using a CAL-based method behave no differently than standard models.
We emphasize that the purpose of this work is notto champion a single method, but rather to showcase
an assortment of approaches in the CAL paradigm that achieve different performance/speedup trade-offs.
Lastly, we would like to point out that some of the CAL methods are ablations of each other. For example,
CAL-ER is ablation for CAL-DER (or CAL-SD) when we replace the distillation component. Similarly,
CAL-SD is ablation of CAL-SDS2, where we remove the submodular selection part.
5.1 Datasets and Experimental Setup
We use the following datasets, which span a spectrum of data modalities, scale (both in terms of dataset size,
and model’s computational/memory footprint), and class balance.
FMNIST: The FMNIST dataset consists of 70,000 28 ×28 grayscale images of fashion items belonging to 10
classes (Xiao et al., 2017). A ResNet-18 architecture (He et al., 2016) with SGD is used. We apply data
augmentation, as in Beck et al. (2021), consisting of random horizontal flips and random croppings.
CIFAR-10: CIFAR-10 consists of 60,000 32 ×32 color images with 10 different categories (Krizhevsky, 2009).
We use a ResNet-18 and use the SGD optimizer for all CIFAR-10 experiments. We apply data augmentations
consisting of random horizontal flips and random croppings.
8

--- PAGE 9 ---
MedMNIST: We use the DermaMNIST dataset within the MedMNIST collection (Yang et al., 2021a;b) for
performance evaluation of CAL on medical imaging modalities. It consists of 3-color channel dermatoscopy
images of 7 different skin diseases, originally obtained from Codella et al. (2019); Tschandl et al. (2018). A
ResNet-18 architecture is used for all DermaMNIST experiments.
Amazon Polarity Review: (Zhang et al., 2015) is an NLP dataset consisting of reviews from Amazon
and their corresponding star-ratings (5 classes) which was used for active learning in Coleman et al. (2020b).
Similar to the previous work we consider a total unlabeled pool of size 2 million sentences and use a
VDCNN-9 (Schwenk et al., 2017) architecture trained using Adam.
COLA: COLA (Warstadt et al., 2018) aims to check the linguistic acceptability of a sentence via binary
classification. We consider an unlabeled size-7000 pool similar to Ein-Dor et al. (2020) and use a BERT (Devlin
et al., 2019) backbone trained using Adam.
Single-Cell Cell Type Identity Classification: Recent single-cell RNA sequencing (scRNA-seq) technolo-
gies have enabled large-scale characterization of hundreds of thousands to millions of cells in complex tissues,
and accurate cell type annotation is a crucial step in the study of such datasets. To this end, several deep
learning models have been proposed to automatically label new scRNA-seq datasets (Xie et al., 2021). The
HCL dataset is highly class-imbalanced and consists of scRNA-seq data for 562,977 cells across 63 cell types
represented in 56 human tissues (Han et al., 2020). We use the ACTINN model (Ma & Pellegrini, 2019), a
four-layer multi-layer perceptron that predicts the cell-type for each cell given its expression of 28832 genes,
and uses an SGD optimizer.
Hyperparameters: Details about the specific hardware and the choices of hyperparameters used to train
models for each technique can be found in Appendix A.5.
Active Learning setup: As done in previous work (Coleman et al., 2020b; Killamsetty et al., 2021a) for
CIFAR10 and Amazon polarity review, budgets go from 10% to 50% in increments of 10% (results for other
query sizes are presented in Appendix C.3). For FMNIST, MedMNIST, and Cell-type datasets, it goes from
10% to 30% in increments of 5%. Lastly, for COLA, we follow a budget using absolute sizes from 200 to 1000
in increments of 200 (similar to Ein-Dor et al. (2020)).2We adopt the AL framework proposed in Beck et al.
(2021) for all experiments. In the main paper, we here present results for an uncertainty sampling-based
acquisition function. However, we provide results using other acquisition functions in Appendix C .
5.2 Performance vs Speedup
1 2 30.980.991.001.01Avg. relative accuracyBudget = 10%
1 2 30.990.991.001.001.01Budget = 20%
1 2 30.980.991.00Budget = 30%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Average Speedup
Figure 5: Relative accuracy vs. speedup
averagedoverdifferentdatasets. Forevery
given budget, we have at least one CAL
method that is faster with the same (if
not better) accuracy than the baseline.
Our goal is to show that for every dataset and budget, there exists at least one CAL method that performs
equivalent (if not better) than baseline AL. However, with raw accuracies, it is difficult to compare different
CAL methods and baseline AL over different datasets at different budgets. Therefore, we begin by observing
the relative gain in accuracy over the AL baseline. Relative gains further make it feasible to take an average
across the budgets, for a given dataset, and to take averages across the datasets, for a given budget. For
every budget , we normalize the accuracies of each method by that of baseline AL. This makes the baseline
accuracy always 1, irrespective of budget and dataset. Relative performances greater than 1 indicate better
than baseline accuracy (and the opposite for the less than 1 case). Having said that: (1) keeping the budgets
fixed to 10%, 20%, and 30% and averaging over the datasets (except COLA, since it has a different budget)
give us Figure 5; (2) keeping the dataset fixed, averaging the relative accuracy vs. speedups across different
budgets give us Figure 4; and (3) further averaging the above across different datasets give us Figure 1.
Methods in the top right corner are preferable. For space reasons, we display only relative accuracies in the
2We follow the query set sizes from (Ein-Dor et al., 2020)
9

--- PAGE 10 ---
CAL-ER CAL-MIR CAL-DER CAL-SD CAL-SDS2 AL AL w/ WS
AL procedures0.960.970.980.991.001.011.021.031.04Avg. relative accuracyFigure 6: Comparison of CAL methods with the baseline
on CIFAR-10C; absolute accuracies on individual bench-
mark and its average difference with the baseline AL are
reported in Appendix A.2 table 8 and 10, respectively. All
CAL methods perform within a standard deviation of the
standard AL baseline, and CAL-SDS2 achieves the highest
robust accuracy on average.
main paper, but all absolute accuracies and standard deviations for each method for every dataset and every
budget are available in Appendix A.2.
From the overall and per-dataset depiction of CAL’s performance (Figures 1 and 4, respectively), it is evident
thatthere exists a CAL method that attains a significant speedup over a standard AL technique for every
dataset and budget while preserving test set accuracy. From Figure 4, we can further see that for some
datasets (such as FMNIST and CIFAR-10), CAL-ER, a non-distillation, and uniform sampling-based method,
only incur a minor drop in performance but attain the highest speedup. This suggests that naively biasing
learning towards recent tasks can be sufficient to adapt the model to a new set of points between AL rounds.
However, as we show in Figure 5 it is not universally true for all the datasets (at different budgets). Hence, the
methods which include some type of distillation term (CAL-DER, CAL-SD, CAL-SDS2) generally perform
the best out of all CAL methods. We believe that the submodular sampling-based method (CAL-SDS2) can
be accelerated using stochastic methods and results improved by considering other submodular functions,
which we leave as future work. It should be mentioned, however, that the concave function on h(xi)was
essential for CAL-SDS2’s performance.
5.3 Comparison Between Standard and CAL Models
In this section, we next assess whether CAL training has any adverse effect on the final model’s behavior.
We first demonstrate that CAL does notresult in any deterioration of model robustness (Section 5.3.1). We
then demonstrate that CAL models and baseline trained models are uncertain about a similar set of unseen
examples (Section 5.3.2). Lastly, in appendix A.5 we provide a sensitivity analysis of our proposed methods,
where we demonstrate that CAL methods are robust to the changes to the hyperparameters.
5.3.1 Robustness
Test time distributions can vary from training distributions, so it is important to ensure that models can
generalize across different domains. Since models trained using CAL methods require significantly fewer
gradient steps, the modified training procedure may produce fickler models that are less robust to domain
shifts. To ensure against this, we evaluate CAL-method-trained model robustness in this section. We consider
CIFAR-10C Hendrycks & Dietterich (2019), a dataset comprising 19 different corruptions each done at 5 levels
of severity. For each model trained up to a 50%budget, we report the average classification accuracy over
each corruption and compare it against the baseline in Figure 6; each result is an average of over three random
seeds. We note that most of the CAL methods perform statistically similarly to standard active learning, all
while providing significant acceleration. Moreover, on average across all the tests, models trained with
CAL-SDS2 are better than the models trained with CAL-DER , where we see a difference of as
much as 5% in corruptions such as glass blur ; please refer to appendix A.3 table 8, table 9 and 10 for the
absolute accuracies and statistical tests. Submodular sampling replays a diverse representative subset of
history which is likely the reason behind CAL-SDS2’s better robustness. The relationship of diversity with
robustness has also been explored in previous works including Killamsetty et al. (2021b); Fang et al. (2022);
Rozen et al. (2019); Gong et al. (2018).
10

--- PAGE 11 ---
5.3.2 Correlation of Uncertainty Scores
For models trained using CAL techniques to be used as valid substitutes for standard AL models, these two
classes of models need to query similar samples at each AL round. This is particularly important if AL is
being used solely as a data subset selection procedure (where the user is concerned about the quality of
the resulting labeled dataset as opposed to the final model). When using uncertainty sampling as the AL
acquisition function, computing the Pearson correlation between the entropy scores of baseline and CAL
models on the validation set after every query round is one way of determining this. Since most AL policies
incorporate some notion of uncertainty, we hypothesize that the results of this experiment should extend to
other acquisition functions as well. A similar analysis is done in Coleman et al. (2020b).
In the grid of cells below-left, we show the mean, max, std, 
and min (clockwise starting at left of each cell) of the cor-
relations of the entropies of the models trained using each 
of the seven techniques, where the statistics in 
each case are computed over the diﬀerent active 
query rounds. In general, high mean, and low std, 
correlation implies that the unlabeled samples 
each model is uncertain about are simi-
lar to each other. We see positive 
mostly strong correlations and low stds, 
implying that CAL-based 
methods are not changing 
the inherent behavior of the 
active learning 
processes, provid-
ing further evi-
dence for the utility 
of the 
CAL 
methods. 
Figure 7: Cross-Method Entropy Correlation Statistics.As seen in Figure 7, the Pearson correlation between
all pairs of models is positive at every AL query
round. Thus, the nature of samples that the models
are uncertain about, and thus are likely to be chosen
at each round, is similar between the CAL-trained
models and baseline AL-trained models. A break-
down of these correlations at every round is provided
in Figure 14 in the Appendix.
6 Conclusion and Future Work
We proposed the CAL framework, the first method
to circumvent the problem of having to retrain mod-
els between batch AL rounds. Across vision, natural
language, medical imaging, and biological datasets,
we show there is always a CAL-based method that
either matches or outperforms standard AL while
achieving considerable speedups. Since CAL is inde-
pendent of the model architecture and AL strategy,
this framework applies to a broad range of settings.
Empirical Extensions Future empirical directions may include the following: (1):CAL reduces the
training time of the model, but not the AL query time although query time reductions could be an offshoot
of this work; (2):CAL operates using existing AL query acquisition functions, but it is possible to tailor
acquisition functions for CAL methods yielding further generalization and computational improvements; (3):
while SDS2’s use of submodularity helped robustness, additional submodular strategies can be used to further
improve results and also for diverse AL query selection; and (4):CAL provides a novel application for CL;
future CL work can be partially assessed based on its CAL performance.
Theoretical Extensions The discrepancy-based Active Learning framework is promising to establish
generalization error bounds for the replay-based CAL framework. Specifically, leveraging the generalization
results presented in Theorem 2 of Cui & Sato (2020), the upper bound on the risk function based on true data
distribution (referred to as Pin the theorem) is contingent upon the empirical distribution of the presently
labeled data (referred to as Qin the theorem), that includes the newly acquired query batch.
In contrast to utilizing the entirety of the available labeled data, CAL strategically opts for a subset of this
data through the employment of replay methods, for acceleration. Due to this, the empirical distribution of
examples chosen by CAL (call it Q′) could deviate from Q. Consequently, one can modify the upper bound
of the risk function based on true data distribution in Theorem 2 by adding the error term (which would
depend on some notion of distance between QandQ′) which vanishes when we consider all the currently
labeled examples, thus, recovering standard AL.
Acknowledgments
This work was supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor
Research Corporation (SRC) program sponsored by DARPA, by the National Science Foundation under
11

--- PAGE 12 ---
Grant Nos. IIS-2106937 and IIS-2148367, and by NIH/NHGRI U01 award HG009395. We thank Lilly Kumari,
Tianyi Zhou, Shengjie Wang and all other MELODI lab members for their helpful discussions and feedback.
We also thank the TMLR Action Editor and reviewers for their constructive comments.
References
Wickliffe C Abraham and Anthony Robins. Memory retention–the synaptic stability versus plasticity dilemma.
Trends Neurosci , 28(2):73–78, February 2005.
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne
Tuytelaars. Online continual learning with maximally interfered retrieval. CoRR, abs/1908.04742, 2019a.
URL http://arxiv.org/abs/1908.04742 .
Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online
continual learning. arXiv preprint arXiv:1903.08671 , 2019b.
Jordan Ash and Ryan P Adams. On warm-starting neural network training. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33,
pp. 3884–3894. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/288cd2567953f06e460a33951f55daaf-Paper.pdf .
Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch
active learning by diverse, uncertain gradient lower bounds. ArXiv, abs/1906.03671, 2020.
Les Atlas, David Cohn, and Richard Ladner. Training connectionist networks with queries and selective
sampling. Advances in neural information processing systems , 2, 1989.
Ali Ayub and Carter Fendley. Few-shot continual active learning by a robot, 2022. URL https://arxiv.
org/abs/2210.04137 .
Maria-Florina Balcan, Steve Hanneke, and Jennifer Wortman Vaughan. The true sample complexity of
active learning. Machine Learning , 80(2-3):111–139, April 2010. doi: 10.1007/s10994-010-5174-y. URL
https://doi.org/10.1007/s10994-010-5174-y .
Nathan Beck, Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan, and Rishabh K. Iyer. Effective
evaluation of deep active learning on image classification tasks. CoRR, abs/2106.15324, 2021. URL
https://arxiv.org/abs/2106.15324 .
Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers
of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference
on Fairness, Accountability, and Transparency , FAccT ’21, pp. 610–623, New York, NY, USA, 2021.
Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL
https://doi.org/10.1145/3442188.3445922 .
Jeff A. Bilmes. Submodularity in machine learning and artificial intelligence. CoRR, abs/2202.00132, 2022.
URL https://arxiv.org/abs/2202.00132 .
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and SIMONE CALDERARA. Dark
experience for general continual learning: a strong, simple baseline. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp.
15920–15930. Curran Associates, Inc., 2020.
Arslan Chaudhry, Marc 'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong
learning with a-gem. In ICLR, 2019.
Arslan Chaudhry, Albert Gordo, David Lopez-Paz, Puneet K. Dokania, and Philip Torr. Using hindsight to an-
chor past knowledge in continual learning, 2020. URL https://openreview.net/forum?id=Hke12T4KPS .
12

--- PAGE 13 ---
Noel Codella, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian
Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, Harald Kittler, and Allan Halpern. Skin
lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging
collaboration (isic), 2019. URL https://arxiv.org/abs/1902.03368 .
David A. Cohn, Les E. Atlas, and Richard E. Ladner. Improving generalization with active learning. Mach.
Learn., 15(2):201–221, 1994. URL http://dblp.uni-trier.de/db/journals/ml/ml15.html#CohnAL94 .
Cody Coleman, Edward Chou, Sean Culatana, Peter Bailis, Alexander C. Berg, Roshan Sumbaly, Matei
Zaharia, and I. Zeki Yalniz. Similarity search for efficient active learning and search of rare concepts. CoRR,
abs/2007.00077, 2020a. URL https://arxiv.org/abs/2007.00077 .
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure
Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. In International
Conference on Learning Representations , 2020b. URL https://openreview.net/forum?id=HJg2b0VYDr .
Zhenghang Cui and Issei Sato. Active learning using discrepancy. https://realworldml.github.io/files/
cr/7_cui_paper.pdf , 2020. [Accessed 21-11-2023].
Ido Dagan and Sean P. Engelson. Committee-based sampling for training probabilistic classifiers. In
Proceedings of the Twelfth International Conference on International Conference on Machine Learning ,
ICML’95, pp. 150–157, San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1558603778.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh,
and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 44(7):3366–3385, 2022. doi: 10.1109/TPAMI.
2021.3057446.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. ArXiv, abs/1810.04805, 2019.
Payal Dhar. The carbon impact of artificial intelligence. Nature Machine Intelligence , 2(8):423–425, Aug 2020.
ISSN 2522-5839. doi: 10.1038/s42256-020-0219-9. URL https://doi.org/10.1038/s42256-020-0219-9 .
LiatEin-Dor, AlonHalfon, ArielGera, EyalShnarch, LenaDankin, LeshemChoshen, MarinaDanilevsky, Ranit
Aharonov, Yoav Katz, and Noam Slonim. Active Learning for BERT: An Empirical Study. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 7949–7962,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.638.
URL https://aclanthology.org/2020.emnlp-main.638 .
Seyda Ertekin, Jian Huang, Leon Bottou, and Lee Giles. Learning on the border: Active learning in imbalanced
data classification. In Proceedings of the Sixteenth ACM Conference on Conference on Information and
Knowledge Management , CIKM ’07, pp. 127–136, New York, NY, USA, 2007. Association for Computing
Machinery. ISBN 9781595938039. doi: 10.1145/1321440.1321461. URL https://doi.org/10.1145/
1321440.1321461 .
Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig
Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip), 2022.
Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint
arXiv:1805.09733 , 2018.
M.L. Fisher, G.L. Nemhauser, and L.A. Wolsey. An analysis of approximations for maximizing submodular
set functions—II. In Polyhedral combinatorics , 1978.
Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences , 3(4):
128–135, 1999.
Zhiqiang Gong, Ping Zhong, and Weidong Hu. Diversity in machine learning. CoRR, abs/1807.01477, 2018.
URL http://arxiv.org/abs/1807.01477 .
13

--- PAGE 14 ---
Andrew Guillory, Erick Chastain, and Jeff Bilmes. Active learning as non-convex optimization. In Twelfth
International Conference on Artificial Intelligence and Statistics (AISTAT) , Clearwater Beach, Florida,
April 2009.
Xiaoping Han, Ziming Zhou, Lijiang Fei, Huiyu Sun, Renying Wang, Yao Chen, Haide Chen, Jingjing Wang,
Huanna Tang, Wenhao Ge, Yincong Zhou, Fang Ye, Mengmeng Jiang, Junqing Wu, Yanyu Xiao, Xiaoning
Jia, Tingyue Zhang, Xiaojie Ma, Qi Zhang, Xueli Bai, Shujing Lai, Chengxuan Yu, Lijun Zhu, Rui Lin,
Yuchi Gao, Min Wang, Yiqing Wu, Jianming Zhang, Renya Zhan, Saiyong Zhu, Hailan Hu, Changchun
Wang, Ming Chen, He Huang, Tingbo Liang, Jianghua Chen, Weilin Wang, Dan Zhang, and Guoji Guo.
Construction of a human cell landscape at single-cell level. Nature, 581(7808):303–309, March 2020.
Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th
International Conference on Machine Learning , ICML ’07, pp. 353–360, New York, NY, USA, 2007.
Association for Computing Machinery. ISBN 9781595937933. doi: 10.1145/1273496.1273541. URL
https://doi.org/10.1145/1273496.1273541 .
Steve Hanneke. Theoretical foundations of active learning . Carnegie Mellon University, 2009.
Steve Hanneke. Activized learning: Transforming passive to active with improved label complexity. Journal
of Machine Learning Research , 13(49):1469–1587, 2012. URL http://jmlr.org/papers/v13/hanneke12a.
html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi:
10.1109/CVPR.2016.90.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. Proceedings of the International Conference on Learning Representations , 2019.
Zhihao Jia, Colin Unger, Wei Wu, Sina Lin, Mandeep Baines, Vinay Ramakrishnaiah Carlos Efrain, Nirmal
Prajapati, Pat McCormick, Jamaludin Mohd-Yusof, Xi Luo, Dheevatsa Mudigere, Jongsoo Park, Misha
Smelyanskiy, and Alex Aiken. Unity: Accelerating DNN training through joint optimization of algebraic
transformations and parallelization. In 16th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 22) , Carlsbad, CA, July 2022. USENIX Association. URL https://www.usenix.
org/conference/osdi22/presentation/jia .
KrishnaTeja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh K. Iyer. GLISTER:
generalization based data subset selection for efficient and robust learning. In Thirty-Fifth AAAI Conference
on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI
2021, Virtual Event, February 2-9, 2021 , pp. 8110–8118. AAAI Press, 2021a. URL https://ojs.aaai.
org/index.php/AAAI/article/view/16988 .
KrishnaTeja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh K. Iyer. RETRIEVE: coreset selection for
efficient and robust semi-supervised learning. CoRR, abs/2106.07760, 2021b. URL https://arxiv.org/
abs/2106.07760 .
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia
Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.
Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017a. doi: 10.1073/pnas.1611835114.
URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114 .
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia
Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.
Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017b. doi: 10.1073/pnas.1611835114.
14

--- PAGE 15 ---
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017c.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acqui-
sition for deep bayesian active learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , vol-
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
95323660ed2124450caaac2c46b5ed90-Paper.pdf .
Alex Krizhevsky. Learning multiple layers of features from tiny images. pp. 32–33, 2009. URL https:
//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf .
David D. Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In William W.
Cohen and Haym Hirsh (eds.), Machine Learning Proceedings 1994 , pp. 148–156. Morgan Kaufmann, San
Francisco (CA), 1994. ISBN 978-1-55860-335-6. doi: https://doi.org/10.1016/B978-1-55860-335-6.50026-X.
URL https://www.sciencedirect.com/science/article/pii/B978155860335650026X .
David D. Lewis and William A. Gale. A sequential algorithm for training text classifiers, 1994. URL
http://arxiv.org/abs/cmp-lg/9407020 . active learning roots.
Haoran Li, Aditya Krishnan, Jingfeng Wu, Soheil Kolouri, Praveen K. Pilly, and Vladimir Braverman.
Lifelong learning with sketched structural regularization. CoRR, abs/2104.08604, 2021. URL https:
//arxiv.org/abs/2104.08604 .
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods,
and future directions. IEEE signal processing magazine , 37(3):50–60, 2020.
Xiao Li and Jeff Bilmes. A Bayesian divergence prior for classifier adaptation. In Artificial Intelligence and
Statistics , pp. 275–282. PMLR, 2007.
Zhizhong Li and Derek Hoiem. Learning without forgetting, 2017.
Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies ,
pp. 510–520, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL
https://aclanthology.org/P11-1052 .
Yuzong Liu, Kai Wei, Katrin Kirchhoff, Yisong Song, and Jeff Bilmes. Submodular feature selection for
high-dimensional acoustic score spaces. In 2013 IEEE International Conference on Acoustics, Speech and
Signal Processing , pp. 7184–7188, 2013. doi: 10.1109/ICASSP.2013.6639057.
David Lopez-Paz and Marc 'Aurelio Ranzato. Gradient episodic memory for continual learning. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
Feiyang Ma and Matteo Pellegrini. ACTINN: automated identification of cell types in single cell RNA
sequencing. Bioinformatics , 36(2):533–538, 07 2019. ISSN 1367-4803. doi: 10.1093/bioinformatics/btz592.
URL https://doi.org/10.1093/bioinformatics/btz592 .
Zheda Mai, Dongsub Shim, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Adversarial
shapley value experience replay for task-free continual learning. CoRR, abs/2009.00093, 2020. URL
https://arxiv.org/abs/2009.00093 .
Antoine De Mathelin, François Deheeger, Mathilde MOUGEOT, and Nicolas Vayatis. Discrepancy-based
active learning for domain adaptation. In International Conference on Learning Representations , 2022.
URL https://openreview.net/forum?id=p98WJxUC3Ca .
15

--- PAGE 16 ---
C. Mayer and R. Timofte. Adversarial sampling for active learning. In 2020 IEEE Winter Conference on
Applications of Computer Vision (WACV) , pp. 3060–3068, Los Alamitos, CA, USA, mar 2020. IEEE
Computer Society. doi: 10.1109/WACV45572.2020.9093556. URL https://doi.ieeecomputersociety.
org/10.1109/WACV45572.2020.9093556 .
James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are complementary learning
systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models
of learning and memory. Psychological review , 102(3):419, 1995.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential
learning problem. In Psychology of learning and motivation , volume 24, pp. 109–165. Elsevier, 1989.
Martial Mermillod, Aurélia Bugaiska, and Patrick BONIN. The stability-plasticity dilemma: investigating
the continuum from catastrophic forgetting to age-limited learning effects. Frontiers in Psychology , 4,
2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00504. URL https://www.frontiersin.org/article/
10.3389/fpsyg.2013.00504 .
M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization
Techniques , 1978.
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.
Lazier than lazy greedy. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 29, 2015.
Matthias Perkonigg, Johannes Hofmanninger, and Georg Langs. Continual active learning for efficient
adaptation of machine learning models to changing image acquisition. In Lecture Notes in Computer
Science, Lecture notes in computer science, pp. 649–660. Springer International Publishing, Cham, 2021.
Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and José Miguel Hernández-Lobato. Bayesian batch
active learning as sparse subset approximation. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
84c2d4860a0fc27bcf854c444fb8b400-Paper.pdf .
R Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting
functions. Psychol Rev , 97(2):285–308, April 1990.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learn-
ing to learn without forgetting by maximizing transfer and minimizing interference. CoRR, abs/1810.11910,
2018.
Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science , 7(2):123–146,
1995. doi: 10.1080/09540099550039318. URL https://doi.org/10.1080/09540099550039318 .
Ohad Rozen, Vered Shwartz, Roee Aharoni, and Ido Dagan. Diversify your datasets: Analyzing generalization
via controlled variance in adversarial datasets. In Proceedings of the 23rd Conference on Computational
Natural Language Learning (CoNLL) , pp. 196–205, Hong Kong, China, November 2019. Association for
Computational Linguistics. doi: 10.18653/v1/K19-1019. URL https://aclanthology.org/K19-1019 .
Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM , 63(12):54–63, nov
2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https://doi.org/10.1145/3381831 .
Holger Schwenk, Loïc Barrault, Alexis Conneau, and Yann LeCun. Very deep convolutional networks for text
classification. In EACL, 2017.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach, 2017.
URL https://arxiv.org/abs/1708.00489 .
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In
International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=
H1aIuk-RW .
16

--- PAGE 17 ---
Yuya Senzaki and Christian Hamelain. Active learning for deep neural networks on edge devices, 2021. URL
https://arxiv.org/abs/2106.10836 .
Burr Settles. Active learning. Morgan & Claypool Publishers, 2012.
Burr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of
Wisconsin–Madison, 2009. URL http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.
activelearning.pdf .
Burr Settles. From theories to queries: Active learning in practice. In Isabelle Guyon, Gavin Cawley, Gideon
Dror, Vincent Lemaire, and Alexander Statnikov (eds.), Active Learning and Experimental Design workshop
In conjunction with AISTATS 2010 , volume 16 of Proceedings of Machine Learning Research , pp. 1–18,
Sardinia, Italy, 16 May 2011. PMLR. URL https://proceedings.mlr.press/v16/settles11a.html .
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR,
abs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053 .
Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang. Deep active learning: Unified and principled
method for query and training. CoRR, abs/1911.09162, 2019. URL http://arxiv.org/abs/1911.09162 .
Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, and Yee Whye Teh.
Functional regularisation for continual learning with gaussian processes. In International Conference on
Learning Representations , 2020. URL https://openreview.net/forum?id=HkxCzeHFDB .
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source
dermatoscopic images of common pigmented skin lesions. Scientific Data , 5, 2018.
Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron Walsman, Aniruddha Kembhavi, and
Ali Farhadi. FLUID: A unified evaluation framework for flexible sequential data. Transactions on Machine
Learning Research , 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=UvJBKWaSSH .
Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv
preprint arXiv:1805.12471 , 2018.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In
Proceedings of the 32nd International Conference on International Conference on Machine Learning -
Volume 37 , ICML’15, pp. 1954–1963. JMLR.org, 2015.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations , pp. 38–45, Online, October 2020. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/2020.emnlp-demos.6 .
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/abs/1708.07747 .
Bingbing Xie, Qin Jiang, Antonio Mora, and Xuri Li. Automatic cell type identification methods for single-cell
RNA sequencing. Computational and Structural Biotechnology Journal , 19:5874–5887, 2021. ISSN 2001-
0370. doi: https://doi.org/10.1016/j.csbj.2021.10.027. URL https://www.sciencedirect.com/science/
article/pii/S2001037021004499 .
Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl
benchmark for medical image analysis. In IEEE 18th International Symposium on Biomedical Imaging
(ISBI), pp. 191–195, 2021a.
17

--- PAGE 18 ---
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni.
Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification. arXiv
preprint arXiv:2110.14795 , 2021b.
Donggeun Yoo and In So Kweon. Learning loss for active learning. CoRR, abs/1905.03677, 2019. URL
http://arxiv.org/abs/1905.03677 .
Haoyu Zhang, Logan Stafman, Andrew Or, and Michael J. Freedman. Slaq: Quality-driven scheduling
for distributed machine learning. SoCC ’17, pp. 390–404, New York, NY, USA, 2017. Association for
Computing Machinery. ISBN 9781450350280. doi: 10.1145/3127479.3127490. URL https://doi.org/10.
1145/3127479.3127490 .
Jifan Zhang, Yifang Chen, Gregory Canal, Stephen Mussmann, Yinglun Zhu, Simon Shaolei Du, Kevin
Jamieson, and Robert D Nowak. Labelbench: A comprehensive framework for benchmarking label-efficient
learning, 2023.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification.
CoRR, abs/1509.01626, 2015. URL http://arxiv.org/abs/1509.01626 .
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang,
Yuanzhong Xu, Danyang Zhuo, Joseph E. Gonzalez, and Ion Stoica. Alpa: Automating inter- and
intra-operator parallelism for distributed deep learning. CoRR, abs/2201.12023, 2022. URL https:
//arxiv.org/abs/2201.12023 .
Zongwei Zhou, Jae Y. Shin, Suryakanth R. Gurudu, Michael B. Gotway, and Jianming Liang. Active,
continual fine tuning of convolutional neural networks for reducing annotation efforts. Medical Image
Analysis, 71:101997, 2021. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2021.101997. URL
https://www.sciencedirect.com/science/article/pii/S1361841521000438 .
Jia-Jie Zhu and José Bento. Generative adversarial active learning. CoRR, abs/1702.07956, 2017. URL
http://arxiv.org/abs/1702.07956 .
18

--- PAGE 19 ---
A Additional Experimental Details on Main Results
A.1 Procedure for normalized accuracy plots
Here we again explain the procedure to generate the normalized accuracy versus speedup plots, as reported
in figures 1, 5, and 4. These plots help us to understand and compare the performance of different CAL
methods against the baseline AL. For every dataset, we first get the respective accuracies and speedups of
CAL methods and baseline AL, for every budget. This is also reported as a tabular form besides the scatter
plot visualizations in section A.2.
Forevery budget , we normalize the accuracies of each method by that of baseline AL. This makes the
baseline always at 1, irrespective of budget and dataset. Relative performances greater than 1 indicate better
than baseline accuracy (and similarly for the less than 1 case). Note that, for CIFAR10 and Amazon polarity
review, budgets go from 10% to 50% in increments of 10%. However, for FMNIST, MedMNIST, and Celltype
datasets, it goes from 10% to 30% in increments of 5%. Lastly for COLA, the budgets are different from the
above, therefore, we don’t include that when we average across the dataset at a fixed budget. Having said
that,
•Keeping the budgets fixed to 10%, 20%, and 30% and averaging over the datasets (except COLA,
since it has a different budget) will give us figure 5.
•Keeping the dataset fixed, averaging the relative accuracy v.s. speedups across different budgets will
give us figure 4.
•Further averaging the above across different datasets will give us the main result figure 1.
From the results above, we can infer that CAL-DER and specialized methods such as CAL-SD/SDS2 always
provide a speedup, all while preserving the accuracy, if not better.
A.2 Results in Tabular Form
In this section, we expand our results mentioned in section 5.2. In particular, we report the absolute accuracies
for each dataset, at every budget, and plot it against the observed speedup. All methods highlighted in
blue are methods that use CAL. Note that all the results in this section are for uncertainty-based query
pool acquisition functions. The choice of budget scale is taken from the previous works (Beck et al., 2021;
Ein-Dor et al., 2020). Each dataset has a different complexity (and we train on them using different neural
architectures), therefore they differ in the labeling budget.
FMNIST Please refer to table 2 and figure 8.
1 2 39393Accuracy (%)Budget = 10%
1 2 3949494Budget = 15%
1 2 3949595Budget = 20%
1 2 3959595Budget = 25%
1 2 3949595Budget = 30%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Speedup
Figure 8: FMNIST Results
19

--- PAGE 20 ---
Test Accuracy (%) Factor Speedup
Method 10% 15% 20% 25% 30% 10% 15% 20% 25% 30%
CAL-ER 92.6±0.193.9±0.294.5±0.194.9±0.294.9±0.21.5×1.4×2.0×2.4×2.8×
CAL-MIR 92.6±0.393.9±0.294.5±0.094.9±0.194.9±0.00.9×1.2×1.3×1.5×1.7×
CAL-DER 92.7±0.193.9±0.194.5±0.194.8±0.294.9±0.11.4×2.0×2.4×2.7×3.1×
CAL-SD 92.6±0.194.0±0.294.5±0.194.8±0.294.9±0.11.4×2.0×2.4×2.7×3.1×
CAL-SDS2 92.6±0.194.0±0.294.6±0.294.9±0.194.9±0.11.1×1.5×1.7×1.9×2.1×
AL w/ WS 92.7±0.393.8±0.294.4±0.194.6±0.194.4±0.21.1×1.4×1.5×1.5×1.5×
AL 92.6±0.393.8±0.094.4±0.194.9±0.294.9±0.11.0×1.0×1.0×1.0×1.0×
Table 2: FMNIST Results
CIFAR10 Please refer to table 3 and figure 9.
12345682.082.583.083.5Accuracy (%)Budget = 10%
12345689.289.489.689.890.0Budget = 20%
12345692.292.492.692.8Budget = 30%
12345693.293.493.693.8Budget = 40%
12345693.593.894.094.2Budget = 50%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Speedup
Figure 9: CIFAR-10 Results
Test Accuracy (%) Factor Speedup
Method 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%
CAL-ER 82.1±0.589.9±0.392.4±0.193.5±0.193.7±0.31.6×2.8×4.0×5.3×6.5×
CAL-MIR 82.4±0.489.5±0.392.6±0.393.6±0.193.8±0.20.7×1.0×1.4×1.8×2.2×
CAL-DER 83.5±0.190.0±0.492.3±0.193.1±0.293.4±0.11.4×2.3×3.2×4.2×5.2×
CAL-SD 83.0±0.090.0±0.492.7±0.293.3±0.393.9±0.31.4×2.2×3.2×4.1×5.1×
CAL-SDS2 82.5±0.190.1±0.292.9±0.494.0±0.294.4±0.11.1×1.6×2.1×2.7×3.4×
AL w/ WS 81.9±0.489.6±0.592.4±0.293.5±0.194.1±0.11.2×1.1×1.1×1.1×1.1×
AL 82.0±0.389.1±0.292.1±0.493.5±0.393.8±0.21.0×1.0×1.0×1.0×1.0×
Table 3: CIFAR-10 Results
MedMNIST Please refer to table 4 and figure 10.
1 2 3 457.560.062.565.0Accuracy (%)Budget = 10%
1 2 3 464.066.068.070.0Budget = 15%
1 2 3 464.066.068.070.0Budget = 20%
1 2 3 464.066.068.070.0Budget = 25%
1 2 3 462.565.067.570.0Budget = 30%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Speedup
Figure 10: MedMNIST Results
20

--- PAGE 21 ---
Test Accuracy (%) Factor Speedup
Method 10% 15% 20% 25% 30% 10% 15% 20% 25% 30%
CAL-ER 64.7±4.463.1±5.268.4±1.266.1±3.767.5±2.21.4×2.0×2.4×2.9×3.4×
CAL-MIR 64.1±2.866.2±1.069.2±1.770.7±1.171.8±0.81.0×1.2×1.4×1.6×1.9×
CAL-DER 67.1±2.967.9±2.069.8±1.271.5±0.672.2±0.61.2×1.6×2.1×2.6×3.2×
CAL-SD 65.1±3.167.8±2.370.6±0.971.1±1.172.1±1.11.2×1.7×2.0×2.5×2.9×
CAL-SDS2 66.1±3.668.1±3.069.8±1.171.6±1.672.5±1.61.1×1.5×1.7×2.0×2.2×
AL w/ WS 67.0±1.467.5±0.769.5±0.670.3±1.071.3±1.01.1×1.3×1.5×1.7×2.0×
AL 67.2±0.768.3±1.368.8±0.869.0±2.070.9±1.51.0×1.0×1.0×1.0×1.0×
Table 4: MedMNIST Results
Amazon Polarity Review Please refer to table 5 and figure 11.
1234591.091.592.092.593.0Accuracy (%)Budget = 10%
1234592.593.093.594.0Budget = 20%
1234593.093.293.593.894.0Budget = 30%
1234593.293.493.693.8Budget = 40%
1234593.293.593.894.094.2Budget = 50%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Speedup
Figure 11: Amazon Polarity Results
Test Accuracy (%) Factor Speedup
Method 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%
CAL-ER 90.7±3.192.4±1.293.5±0.193.7±0.293.8±0.21.5x 3.5x 3.3x 4.1x 4.9x
CAL-MIR 92.0±0.992.9±0.193.3±0.393.7±0.193.7±0.20.9x 1.3x 1.8x 2.3x 2.7x
CAL-DER 92.9±0.394.1±0.394.0±0.793.6±0.894.2±0.31.5x 2.4x 3.2x 4.0x 4.7x
CAL-SD 92.1±0.392.6±0.493.7±0.193.8±0.194.1±0.11.5x 2.4x 3.2x 4.0x 4.7x
CAL-SDS2 92.6±0.393.2±0.193.6±0.193.8±0.494.1±0.01.2x 1.9x 2.5x 3.1x 3.7x
AL w/ WS 92.6±0.593.0±0.293.0±0.193.2±0.393.1±0.11.0x 1.0x 1.0x 1.0x 1.0x
AL 92.8±0.293.1±0.793.3±1.193.8±0.594.1±0.21.0x 1.0x 1.0x 1.0x 1.0x
Table 5: Amazon Polarity Results
COLA Please refer to table 6 and figure 12.
12345672.073.074.075.0Accuracy (%)Budget = 2.9%
12345675.075.576.0Budget = 5.7%
12345676.076.577.077.5Budget = 8.6%
12345676.577.077.578.0Budget = 11.4%
12345677.077.578.078.579.0Budget = 14.3%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Speedup
Figure 12: COLA Results
21

--- PAGE 22 ---
Test Accuracy (%) Factor Speedup
Method 2.9% 5.7% 8.6% 11.4% 14.3% 2.9% 5.7% 8.6% 11.4% 14.3%
CAL-ER 73.2±1.775.4±0.876.4±1.077.0±2.077.3±1.41.7x 2.8x 3.9x 5.0x 6.1x
CAL-MIR 75.1±0.275.5±1.276.6±1.076.5±0.477.0±0.30.8x 1.2x 1.6x 2.0x 2.4x
CAL-DER 71.5±2.774.9±3.275.7±1.576.9±1.678.3±0.81.7x 2.7x 3.7x 4.8x 5.8x
CAL-SD 73.6±1.974.9±1.177.7±1.376.4±0.378.0±0.91.7x 2.7x 3.7x 4.8x 5.8x
CAL-SDS2 74.7±2.875.5±1.077.2±0.978.1±0.879.2±0.51.4x 2.1x 2.9x 3.7x 4.5x
AL w/ WS 74.6±0.776.1±0.476.3±1.076.3±1.577.2±0.91.2x 1.7x 1.6x 1.8x 1.8x
AL 73.9±2.975.5±0.576.6±2.076.3±0.977.3±1.61.0x 1.0x 1.0x 1.0x 1.0x
Table 6: COLA Results.
Single-Cell Cell-Type Identity Please refer to table 7 and figure 13.
1 2 384858687Accuracy (%)Budget = 10%
1 2 387888888Budget = 15%
1 2 3888990Budget = 20%
1 2 38990Budget = 25%
1 2 388899091Budget = 30%
CAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
ALCAL-ER
CAL-MIR
CAL-DER
CAL-SD
CAL-SDS2
AL w/ WS
AL
Speedup
Figure 13: Single-Cell Cell-Type Identity Classification Results
Test Accuracy (%) Factor Speedup
Method 10% 15% 20% 25% 30% 10% 15% 20% 25% 30%
CAL-ER 86.3±0.188.3±0.189.7±0.390.6±0.291.0±0.11.5×2.0×2.4×2.9×3.4×
CAL-MIR 86.3±0.188.3±0.189.7±0.290.5±0.290.9±0.21.2×1.6×1.9×2.2×2.6×
CAL-DER 86.9±0.388.8±0.389.9±0.390.7±0.291.2±0.11.4×1.9×2.3×2.8×3.3×
CAL-SD 86.3±0.188.3±0.189.5±0.290.3±0.290.8±0.21.4×1.9×2.3×2.8×3.3×
CAL-SDS2 86.3±0.188.2±0.189.2±0.390.1±0.290.6±0.11.4×1.9×2.3×2.8×3.3×
AL w/ WS 86.3±0.188.3±0.188.4±0.888.2±0.888.1±0.81.0×1.0×1.2×1.6×1.9×
AL 83.6±1.087.0±0.388.6±0.189.5±0.289.9±0.31.0×1.0×1.0×1.0×1.0×
Table 7: Single-Cell Cell-Type Identity Classification Results
A.3 Out-of-distribution (OOD) generalization
Here we show results from the figure 6 in a tabular form in table 8. We compare the performance of CAL
methods with baseline AL for OOD generalization. Reported mean values and standard deviations are
computed over three different random seeds. As shown in the figure 6 in the main paper, and the table below,
CAL methods are as robust to perturbations as baseline AL, despite the speedup. Finally, in table 10 we
report that CAL-SDS2 is better compared to other CAL methods, on average.
22

--- PAGE 23 ---
Corruption CAL-ER CAL-MIR CAL-DER CAL-SD CAL-SDS2 AL AL w/ WS
saturate 90.4 ±0.490.7±0.290.4±0.190.5±0.490.8±0.290.7±0.490.8±0.2
impulse noise 52.4 ±1.453.4±0.853.3±3.953.6±2.655.8±1.354.5±2.948.7±2.7
defocus blur 79.6 ±1.381.6±0.479.3±1.280.6±1.179.6±1.780.9±1.080.4±0.7
contrast 74.6 ±0.376.1±0.873.8±0.975.1±1.074.2±0.677.1±1.476.4±2.5
frost 76.0 ±0.476.1±2.073.8±1.075.4±1.576.9±1.275.4±1.677.0±0.3
speckle noise 61.7 ±1.761.7±2.660.9±3.761.6±3.864.3±0.561.7±0.359.4±1.0
pixelate 74.4 ±1.373.9±2.275.2±0.974.8±1.676.5±0.775.9±1.377.0±1.3
zoom blur 74.4 ±1.076.7±1.074.4±2.375.4±1.574.1±2.875.9±1.975.3±1.0
elastic transform 82.5 ±0.183.5±0.282.0±0.582.1±0.882.6±0.682.0±1.382.9±0.2
spatter 81.8 ±0.782.9±1.282.4±0.682.6±0.582.8±0.883.0±1.082.5±1.2
snow 80.3 ±0.480.2±0.779.5±0.879.7±0.780.8±0.180.1±0.880.6±0.3
fog 86.8 ±0.386.9±0.385.9±0.986.3±0.586.5±0.486.6±0.387.9±0.7
Gaussian noise 46.6 ±1.046.6±3.344.9±6.346.7±4.750.7±0.945.9±0.443.2±1.1
brightness 92.1 ±0.392.4±0.392.1±0.392.2±0.192.5±0.192.7±0.192.6±0.2
Gaussian blur 69.7 ±2.272.3±1.369.3±2.571.3±1.569.3±2.871.6±1.670.4±1.3
motion blur 75.1 ±1.277.1±1.274.4±0.975.1±1.674.5±0.774.7±0.676.8±0.4
shot noise 58.6 ±1.558.5±2.957.3±4.858.6±4.161.6±0.558.3±0.256.1±1.0
jpeg compression 79.0 ±1.578.4±0.578.5±0.278.6±0.579.1±0.177.7±0.177.7±0.1
glass blur 51.8 ±1.454.6±3.048.6±2.850.7±2.153.9±2.449.4±3.152.6±2.1
Table 8: Accuracy (in %) comparison of CAL methods with the baseline on the CIFAR-10C dataset. Results
were reported as an average over three random seeds. Models trained with CAL procedure perform statistically
similar to the one trained with baseline AL.
Corruption CAL-ER/AL CAL-MIR/AL CAL-DER/AL CAL-SD/AL CAL-SDS2/AL
saturate 0.3957 0.9104 0.2118 0.4516 0.7961
impulse noise 0.3209 0.5692 0.7029 0.7134 0.5374
defocus blur 0.2477 0.2794 0.1561 0.7854 0.3178
contrast 0.0335 (+) 0.3023 0.0256 (-) 0.1020 0.0250 (-)
frost 0.5793 0.6639 0.2147 0.9548 0.2778
speckle noise 0.9873 0.9922 0.7409 0.9739 0.0017 (+)
pixelate 0.2243 0.2521 0.4661 0.3913 0.5636
zoom blur 0.2824 0.5567 0.4279 0.7238 0.3910
elastic transform 0.5482 0.1295 0.9975 0.9154 0.5176
spatter 0.1504 0.9401 0.4310 0.5843 0.7800
snow 0.7823 0.8905 0.3747 0.5455 0.1976
fog 0.4715 0.2534 0.2781 0.4512 0.7142
gaussian noise 0.3514 0.7485 0.7836 0.8056 0.0013 (+)
brightness 0.0309 (-) 0.2042 0.0341 (-) 0.0091 (-) 0.0474 (-)
gaussian blur 0.2969 0.5867 0.2605 0.8658 0.2997
motion blur 0.6474 0.0368 (+) 0.6263 0.6801 0.7135
shot noise 0.7252 0.8920 0.7311 0.9091 0.0004 (+)
jpeg compression 0.0110 (+) 0.1080 0.0016 (+) 0.0286 (+) 0.0000 (+)
glass blur 0.2820 0.1047 0.7633 0.5807 0.1182
Table 9: Pairwise p-values were computed to compare each Confidence-Aware Learning (CAL) method with
the standard Active Learning (AL) approach across various corruption tests. p-values less than 0.05 are
highlighted in bold, and instances, where CAL methods outperform (+) or underperform (-) compared to
standard AL, are indicated. Overall, our findings suggest that the disparities between CAL and AL methods
are typically not statistically significant, implying that CAL generally does not compromise robustness. When
a statistically significant difference does arise, CAL (particularly CAL-SDS2) tends to outperform AL more
frequently than it falls short.
A.4 Correlation between CAL and baseline AL
In figure 14, we provide the Pearson correlation between the uncertainty scores of models on the held-out
test set, before every query round. A positive correlation between CAL models and baseline AL models
23

--- PAGE 24 ---
Method Average Accuracy difference (in %)
CAL-ER -0.34
CAL-MIR 0.49
CAL-DER -0.96
CAL-SD -0.17
CAL-SDS2 0.63
AL w/ WS -0.32
Table 10: The average difference in the accuracy (in %) of models trained with baseline AL from models
trained with CAL, on different benchmarks across the CIFAR10-C dataset. Higher numbers are better. We
can see that CAL-SDS2 is better compared to other CAL methods, on average. We hypothesize that this can
be attributed to submodular sampling from history.
before every query round suggests that the nature of examples chosen by the CAL models is similar to that
of baseline AL models. Note that each entry in the correlation matrix is averaged over three random seeds
corresponding to the random initialization of each model.
24

--- PAGE 25 ---
CAL-DER
CAL-SD
CAL-SDS2
CAL-ER
CAL-MIR
AL
AL w/ WS10.58 0.53 0.53 0.51 0.49 0.49
0.58 1 0.6 0.62 0.59 0.51 0.54
0.53 0.6 1 0.6 0.62 0.49 0.53
0.53 0.62 0.6 10.64 0.49 0.54
0.51 0.59 0.62 0.64 10.48 0.54
0.49 0.51 0.49 0.49 0.48 10.45
0.49 0.54 0.53 0.54 0.54 0.45 110.43 0.41 0.42 0.42 0.39 0.39
0.43 10.49 0.51 0.5 0.46 0.45
0.41 0.49 10.49 0.5 0.44 0.44
0.42 0.51 0.49 10.51 0.44 0.44
0.42 0.5 0.5 0.51 10.45 0.46
0.39 0.46 0.44 0.44 0.45 10.42
0.39 0.45 0.44 0.44 0.46 0.42 11 0.4 0.38 0.4 0.38 0.35 0.34
0.4 10.49 0.51 0.5 0.44 0.43
0.38 0.49 10.49 0.49 0.44 0.41
0.4 0.51 0.49 1 0.5 0.44 0.43
0.38 0.5 0.49 0.5 10.45 0.42
0.35 0.44 0.44 0.44 0.45 1 0.4
0.34 0.43 0.41 0.43 0.42 0.4 1
CAL-DER
CAL-SD
CAL-SDS2
CAL-ER
CAL-MIR
AL
AL w/ WS10.43 0.4 0.43 0.41 0.38 0.37
0.43 10.49 0.54 0.52 0.47 0.43
0.4 0.49 1 0.5 0.5 0.43 0.43
0.43 0.54 0.5 10.53 0.46 0.44
0.41 0.52 0.5 0.53 10.46 0.43
0.38 0.47 0.43 0.46 0.46 10.41
0.37 0.43 0.43 0.44 0.43 0.41 110.45 0.41 0.44 0.43 0.4 0.39
0.45 10.51 0.56 0.54 0.49 0.47
0.41 0.51 10.51 0.5 0.45 0.44
0.44 0.56 0.51 10.55 0.48 0.46
0.43 0.54 0.5 0.55 10.47 0.46
0.4 0.49 0.45 0.48 0.47 10.41
0.39 0.47 0.44 0.46 0.46 0.41 110.54 0.49 0.54 0.53 0.48 0.45
0.54 10.54 0.58 0.59 0.52 0.48
0.49 0.54 10.53 0.54 0.47 0.44
0.54 0.58 0.53 10.58 0.51 0.47
0.53 0.59 0.54 0.58 1 0.5 0.48
0.48 0.52 0.47 0.51 0.5 10.44
0.45 0.48 0.44 0.47 0.48 0.44 1CAL-DER
CAL-SD
CAL-SDS2
CAL-ER
CAL-MIR
AL
AL w/ WSCAL-DER
CAL-SD
CAL-SDS2
CAL-ER
CAL-MIR
AL
AL w/ WS10.57 0.5 0.55 0.54 0.48 0.47
0.57 10.57 0.61 0.61 0.54 0.5
0.5 0.57 10.56 0.56 0.49 0.46
0.55 0.61 0.56 10.61 0.53 0.49
0.54 0.61 0.56 0.61 10.52 0.49
0.48 0.54 0.49 0.53 0.52 10.44
0.47 0.5 0.46 0.49 0.49 0.44 1
CAL-DER
CAL-SD
CAL-SDS2
CAL-ER
CAL-MIR
AL
AL w/ WS10.61 0.55 0.59 0.59 0.51 0.48
0.61 10.59 0.64 0.63 0.54 0.51
0.55 0.59 10.57 0.59 0.5 0.47
0.59 0.64 0.57 10.64 0.54 0.5
0.59 0.63 0.59 0.64 10.53 0.5
0.51 0.54 0.5 0.54 0.53 10.44
0.48 0.51 0.47 0.5 0.5 0.44 1
CAL-DER
CAL-SD
CAL-SDS2
CAL-ER
CAL-MIR
AL
AL w/ WS10.61 0.55 0.61 0.6 0.54 0.49
0.61 10.59 0.65 0.64 0.57 0.52
0.55 0.59 10.58 0.59 0.51 0.48
0.61 0.65 0.58 10.64 0.56 0.51
0.6 0.64 0.59 0.64 10.56 0.51
0.54 0.57 0.51 0.56 0.56 10.46
0.49 0.52 0.48 0.51 0.51 0.46 10.50.60.70.80.91.0
0.40.50.60.70.80.91.0
0.40.50.60.70.80.91.0
0.40.50.60.70.80.91.0
0.40.50.60.70.80.91.0
0.50.60.70.80.91.0
0.50.60.70.80.91.0
0.50.60.70.80.91.0
0.50.60.70.80.91.0
Figure 14: Pearson correlation between uncertainty scores of models on held out set after every query round. We are
showing nine query rounds in row-major order (so the top left is after the first query round, the top middle is after
the second, and so on). Positive correlation of uncertainty scores suggests that the nature of examples the models are
uncertain about, and thus likely to be chosen at every query round, is similar between the CAL-trained models and
baseline AL-trained models.
A.5 Hyperparameters
For every dataset and every CAL/AL strategy, learning rate ( lr) and batch size ( m) are chosen based on
whichever setting achieves the highest performance on standard AL. This means CAL methods can still
improve if we tune either of learning rate or batch size. Replay size is critical for the performance of continual
learning algorithms. On the one hand, we do not want the replay batch size, m(h), to be too small since then
we will forget some history. But on the other hand, we also do not want m(h)to be too large since there will
be a computational cost associated with that. Considering the mentioned constraints, for all CAL methods,
we, therefore, set replay size as m(h)∈{m,2m}(used in all CAL methods). Via experimentation on a subset
of the datasets, we found m(h)less than or greater than this range suffered either from forgetting (when too
small) or extra computation without an accuracy benefit (when too large). We set α∈{0.1,0.25,0.5,0.75}
(used in CAL-DER, CAL-SD, and CAL-SDS2), β∈{0.75,1}(used in CAL-DER). The scale and size of the
search space for αandβis inspired from Buzzega et al. (2020). Lastly, σ∈{0.1,1}(used in CAL-SDS2),
andλ∈{0.1,1,10}(used in CAL-SDS2).
25

--- PAGE 26 ---
We select the configuration for each CAL model that achieves the highest accuracy. “c”is the hyperparameter
used in CAL-MIR and CAL-SDS2 to subsample the history before finding the m(h)samples to replay, but
this parameter is not tuned for any of the presented results. We list the specific set of hyperparameters we
use for all the main experimental results in this section.
A.5.1 FMNIST
All experiments for FMNIST used a ResNet-18 with an SGD optimizer, with learning rate of 0.01 and batch
size of 64. For all the CAL methods, we fix m(h)= 128. A NVIDIA GeForce RTX 1080 GPU was used to
run all the reported experiments.
CAL-MIR c= 256
CAL-DER α= 0.1,β= 1
CAL-SD α= 0.25
CAL-SDS2 c= 256,α= 0.25,σ= 0.1,λ= 1
A.5.2 CIFAR-10
All experiments for CIFAR-10 used a ResNet-18 with an SGD optimizer, with learning rate of 0.02 and a
batch size of 20. For all the CAL methods, we fix m(h)= 40. Training is done on an NVIDIA GeForce RTX
2080.
CAL-MIR c= 100
CAL-DER α= 0.1,β= 1
CAL-SD α= 0.25
CAL-SDS2 c= 100,α= 0.25,σ= 0.1,λ= 0.1
A.5.3 MedMNIST
All experiments for MedMNIST used a ResNet-18 with an Adam optimizer, with learning rate of 0.001 and a
batch size of 128. For all CAL methods, we fix m(h)= 128. All reported models were trained on an NVIDIA
GeForce RTX 2080.
CAL-MIR c= 270
CAL-DER α= 0.1,β= 1
CAL-SD α= 0.5
CAL-SDS2 c= 270,α= 0.5,σ= 0.1,λ= 10
A.5.4 Amazon Polarity Review
Throughout our experiments, we sample 2M sentences and use them as the total training set instead. We use
Adam optimizer with default parameters with a learning rate of 0.001 and a batch size of 128 for 6 epochs.
For all the CAL methods, we fix m(h)= 128. All reported models were trained on an NVIDIA GeForce 1080
Ti.
CAL-MIR c= 256,
26

--- PAGE 27 ---
CAL-DER α= 0.25,β= 0.75
CAL-SD α= 0.5
CAL-SDS2 c= 256,α= 0.75,σ= 1,λ= 1
A.5.5 COLA
For all of our experiments we use Huggingface’s transformer library Wolf et al. (2020) and use a maximum
sentence length of 100. We use Adam optimizer and a learning rate of 5·10−5, use a batch size of 25 and
m(h)= 25. Models were trained on a single NVIDIA GeForce 1080 Ti.
CAL-MIR c= 50
CAL-DER α= 0.25,β= 0.75
CAL-SD α= 0.75
CAL-SDS2 c= 50,α= 0.5,σ= 1,λ= 1.
A.5.6 Single-Cell Cell-Type Identity Classification
All experiments use SGD optimizer with standard parameters with learning rate of 0.001 and a batch size
128. For all the CAL methods, we fix m(h)= 128. Training is done on an NVIDIA A100-PCIE-40GB.
CAL-MIR c= 200,
CAL-DER α= 0.1,β= 1
CAL-SD α= 1
CAL-SDS2 c= 100,α= 0.25,σ= 0.1,λ= 1
A.6 Hyperparameter Sensitivity Analysis for Proposed Methods
In this section, we provide a hyperparameter sensitivity analysis methods for CAL-SD and CAL-SDS2 on
CIFAR-10. For CAL-SD, only αis tuned and the best value is used for CAL-SDS2. Therefore, we tuned 1
hyperparameter for CAL-SD while σandλare tuned for CAL-SDS2. Note that the differences in final test
accuracy at 50% budget on CIFAR-10 across different configurations are negligible, as shown in the tables
below.
α0.10.250.75 0.9
93.7993.9093.5893.38
Table 11: CAL-SD Sensitivity
σλ0.10.5 1510
0.194.2094.2794.3294.3494.16
194.3394.2294.2894.1594.44
1094.3294.2994.2794.3094.18
Table 12: CAL-SDS2 Sensitivity
27

--- PAGE 28 ---
B A primer on Continual Learning
We defineD1:n=/uniontext
i∈[n]Di. In CL, the dataset consists of Ttasks{D1,...,DT}that are presented to the
model sequentially, where Dt={(xi,yi)}i∈Nt,Ntare the task- tsample indices, and nt=|Nt|. At time
t∈[T], the data/label pairs are sampled from the current task (x,y)∼Dt, and the model has only limited
access to the history D1:t−1. If the model is trained only on Dtusing standard optimization algorithms, the
model will exhibit catastrophic forgetting. The CL objective is to efficiently adapt the model to Dtwhile
retaining the performance on the history. Given a loss function ℓ:X×Y∝⇕⊣√∫⊔≀→ R, initial parameters θt−1, and a
modelf,θtcan be obtained as the solution to the CL optimization problem (Aljundi et al., 2019b; Chaudhry
et al., 2019; Lopez-Paz & Ranzato, 2017):
min
θE
(x,y)∼Dtℓ(y,f(x;θ))
s.t. E
(x′,y′)∼D 1:t−1ℓ/parenleftigg
y′,f(x′;θ))/parenrightigg
≤ E
(x′,y′)∼D 1:t−1ℓ/parenleftigg
y′,f(x′;θt−1))/parenrightigg
C Results for Additional Active Learning Strategies
In this section, we demonstrate that CAL methods can accelerate AL strategies other than entropy sampling
without incurring any significant performance drops. We test multiple AL strategies on FMNIST Xiao et al.
(2017) and CIFAR-10 Krizhevsky (2009). Note that the speedups are approximately the same as the ones
reported in Section A since the training time is generally independent of the selected AL strategy.
C.1 Overview of Strategies
Margin Score Sampling This strategy is another form of uncertainty sampling Settles (2009) as described
in the main paper. Instead of the entropy of f(x;θ), the margin score is used as the entropy score i.e.,
h(x)≜1−(f(x;θ)i−f(x;θ)j)whereiandjare the indices corresponding to the highest and second highest
values off(x;θ)respectively.
FASS FASS Wei et al. (2015) is a two-staged selection method that uses both uncertainty sampling and
submodular maximization. Initially, a set of samples Aof cardinality c∗btis chosen fromUusing uncertainty
sampling, where c>1is a tuneable hyperparameter. Next, Utis constructed by greedily selecting samples
that maximize a submodular set function G: 2A→R+defined on a ground set A. Entropy is once again
used as the uncertainty metric for the initial stage. For the second stage, Gis defined to be the facility
location function Wei et al. (2015) expressed below:
G(S) =/summationdisplay
xi∈Amax
xj∈Swij, (4)
whereS ⊆ A andwijis a similarity score between samples xiandxj. In our experiments, wij=
exp (−∥zi−zj∥2/2σ2)whereziis the penultimate layer representation of model fforxiandσis a hy-
perparameter.
GLISTER GLISTER Killamsetty et al. (2021a) solves a bi-level optimization problem in order to select
samples to label. Specifically, GLISTER solves
argmax
S⊆Ut,|S|≤btLLV(argmax
θLLT(θ,S),V) (5)
whereLLVis the log-likelihood on the validation set V, andLLTis the log-likelihood on the subset S.
28

--- PAGE 29 ---
C.2 Results
Test Accuracy (%)
Method 10% 15% 20% 25% 30%
CAL-ER 92.8±0.194.1±0.194.8±0.195.1±0.395.2±0.2
CAL-MIR 92.6±0.294.1±0.494.9±0.295.0±0.295.2±0.2
CAL-DER 91.8±0.593.1±0.194.3±0.394.6±0.194.8±0.2
CAL-SD 92.5±0.193.8±0.194.8±0.095.1±0.295.2±0.0
CAL-SDS2 87.8±1.193.4±0.194.6±0.195.0±0.295.2±0.1
AL w/ WS 92.8±0.094.0±0.394.6±0.194.8±0.195.0±0.2
AL 92.7±0.194.1±0.394.9±0.195.0±0.295.2±0.1
Table 13: FMNIST with Margin Score Sampling
Test Accuracy (%)
Method 10% 20% 30% 40% 50%
CAL-ER 81.5±0.189.3±0.192.2±0.293.4±0.193.8±0.0
CAL-MIR 81.9±0.189.6±0.292.2±0.493.6±0.094.0±0.2
CAL-DER 83.0±0.289.5±0.292.2±0.293.2±0.293.6±0.0
CAL-SD 82.6±0.489.9±0.492.4±0.293.5±0.193.8±0.2
CAL-SDS2 82.5±0.290.2±0.292.5±0.293.8±0.294.1±0.1
AL w/ WS 83.1±0.190.3±0.393.0±0.293.5±0.393.6±0.2
AL 75.1±1.287.1±1.090.2±0.592.0±0.092.8±0.5
Table 14: CIFAR-10 with Margin Score Sampling
Test Accuracy (%)
Method 10% 15% 20% 25% 30%
CAL-ER 92.6±0.193.9±0.294.6±0.295.0±0.194.9±0.0
CAL-MIR 92.5±0.193.8±0.394.6±0.194.8±0.194.9±0.2
CAL-DER 92.7±0.193.8±0.194.5±0.194.7±0.195.0±0.2
CAL-SD 92.8±0.193.9±0.194.7±0.194.8±0.394.9±0.1
CAL-SDS2 92.8±0.093.8±0.294.5±0.194.8±0.294.9±0.1
AL w/ WS 92.5±0.193.8±0.394.0±0.294.3±0.294.3±0.0
AL 92.7±0.493.9±0.194.5±0.194.7±0.394.8±0.1
Table 15: FMNIST with FASS
29

--- PAGE 30 ---
Test Accuracy (%)
Method 10% 20% 30% 40% 50%
CAL-ER 82.2±0.289.8±0.292.5±0.293.4±0.493.7±0.2
CAL-MIR 82.2±0.389.4±0.292.3±0.193.4±0.093.5±0.1
CAL-DER 83.1±0.389.7±0.291.9±0.193.1±0.293.5±0.1
CAL-SD 83.0±0.390.0±0.392.5±0.193.5±0.194.0±0.1
CAL-SDS2 83.0±0.190.1±0.192.7±0.293.5±0.294.0±0.0
AL w/ WS 82.8±0.490.3±0.192.8±0.293.6±0.193.7±0.3
AL 72.5±2.086.6±0.490.1±0.491.7±0.292.9±0.2
Table 16: CIFAR-10 with FASS
Test Accuracy (%)
Method 10% 15% 20% 25% 30%
CAL-ER 92.6±0.093.9±0.294.3±0.194.7±0.194.7±0.2
CAL-MIR 92.5±0.093.9±0.494.3±0.294.4±0.294.6±0.1
CAL-DER 92.7±0.193.9±0.294.3±0.394.7±0.294.9±0.3
CAL-SD 92.6±0.193.8±0.194.4±0.394.6±0.194.7±0.1
CAL-SDS2 92.6±0.193.9±0.294.4±0.294.6±0.394.7±0.2
AL w/ WS 92.5±0.193.6±0.193.9±0.194.1±0.194.3±0.1
AL 92.5±0.293.8±0.194.2±0.194.6±0.294.7±0.2
Table 17: FMNIST with GLISTER
Test Accuracy (%)
Method 10% 20% 30% 40% 50%
CAL-ER 81.7±0.389.2±0.291.9±0.293.0±0.193.3±0.1
CAL-MIR 81.6±0.389.3±0.491.7±0.292.9±0.193.5±0.2
CAL-DER 82.8±0.489.5±0.491.7±0.492.8±0.693.1±0.2
CAL-SD 82.5±0.389.6±0.292.1±0.293.1±0.293.8±0.1
CAL-SDS2 81.4±0.489.1±0.292.1±0.293.2±0.393.9±0.1
AL w/ WS 81.7±0.489.3±0.492.1±0.393.0±0.193.3±0.4
AL 81.0±0.688.5±0.591.5±0.393.0±0.293.4±0.3
Table 18: CIFAR-10 with GLISTER
C.3 Effect of Query Size
In this section, we demonstrate that speedups with CAL methods can be realized with different query sizes.
We demonstrate this by comparing AL with CAL-SDS2 on CIFAR-10, with three different choices of query
sizes. We use entropy based uncertainty sampling as the acquisition function. We observe that CAL-SDS2
achieves comparable performance to standard AL with different query sizes, and the speedup can increase
when the query size is reduced.
30

--- PAGE 31 ---
Test Accuracy (%) Factor Speedup
Query Size Method 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%
5000AL 79.1 88.6 91.7 93.5 93.8 1.0x 1.0x 1.0x 1.0x 1.0x
CAL-SDS2 79.1 89.4 92.4 94.094.31.0x 1.1x 1.4x 1.7x 2.0x
2500AL 81.9 89.6 92.4 93.5 94.1 1.0x 1.0x 1.0x 1.0x 1.0x
CAL-SDS2 82.5 90.1 92.9 94.0 94.41.1x 1.6x 2.1x 2.7x 3.4x
1000AL 84.490.4 93.2 93.8 94.2 1.0x 1.0x 1.0x 1.0x 1.0x
CAL-SDS2 84.389.8 92.5 93.2 93.5 1.6x 3.1x 4.2x 5.3x 6.4x
Table 19: Effect of Query Size on CIFAR-10
D Additional Details on Single-Cell Cell-Type Identity Classification Dataset
The human cell landscape (HCL) dataset consists of scRNA-seq data for 562,977 cells across 63 cell types
represented in 56 human tissues. Each cell type may be present in multiple tissues. The cell type classes are
highly imbalanced, with the rarest cell type, human embryonic stem cell, accounting for 0.06 % of the total
dataset and the most common, fibroblast, accounting for 6%. The raw data is first normalized for library size
and scaled to 10000 reads in total, followed by log transformation. We visualize the dataset using UMAP 15.
Figure 15: UMAP embedding of single cells in HCL annotated by their cell type.
31
