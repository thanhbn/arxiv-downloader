# MASIL: Hướng tới Biểu diễn Lớp Tách biệt Tối đa cho Học Tăng trưởng Lớp Ít mẫu
Anant Khandelwal
Nhà khoa học Ứng dụng, Amazon

## Tóm tắt
Học Tăng trưởng Lớp Ít mẫu (FSCIL) với ít ví dụ cho mỗi lớp trong mỗi phiên tăng trưởng là thiết lập thực tế của học liên tục vì việc thu thập số lượng lớn mẫu có chú thích là không khả thi và hiệu quả về chi phí. Chúng tôi trình bày framework MASIL như một bước tiến hướng tới học bộ phân loại tách biệt tối đa. Nó giải quyết vấn đề chung là quên các lớp cũ và overfitting với các lớp mới bằng cách học trọng số bộ phân loại để tách biệt tối đa giữa các lớp tạo thành một Khung Chặt Equiangular simplex. Chúng tôi đề xuất ý tưởng phân tích concept giải thích các đặc trưng collapsed cho các lớp phiên cơ bản dưới dạng cơ sở concept và sử dụng chúng để tạo ra simplex bộ phân loại cho các lớp ít mẫu. Chúng tôi tiếp tục thêm fine tuning để giảm bất kỳ lỗi nào xảy ra trong quá trình phân tích và huấn luyện bộ phân loại cùng nhau trên các lớp cơ bản và mới mà không giữ lại bất kỳ mẫu lớp cơ bản nào trong bộ nhớ. Kết quả thí nghiệm trên miniImageNet, CIFAR-100 và CUB-200 chứng minh rằng MASIL vượt trội so với tất cả các benchmark.

## 1. Giới thiệu
Thành công của Mạng Neural Tích chập (CNN) trong nhiều tác vụ thị giác máy tính [27,40,43,48,51,60] dựa trên thực tế rằng việc huấn luyện yêu cầu bộ dữ liệu hình ảnh quy mô lớn [14] và phân phối train và test gần như giống hệt nhau [41]. Tuy nhiên, khi triển khai chúng trong môi trường thế giới thực, nó yêu cầu các mô hình này phải nhanh chóng thích ứng với luồng dữ liệu thay đổi và do đó có thể nhận ra các lớp mới xuất hiện theo thời gian. Nhưng nút thắt cổ chai cơ bản cho việc thích ứng này là CNN yêu cầu thu thập một lượng lớn dữ liệu cho mỗi lớp mới, điều này tốn rất nhiều công sức con người để chú thích chúng, điều này là không khả thi. Tuy nhiên, việc chú thích chỉ một vài mẫu có vẻ hợp lý, chúng tôi gọi khả năng này để thích ứng với các lớp mới (chỉ với vài ví dụ) mà không quên các lớp cũ là học tăng trưởng lớp ít mẫu (FSCIL).

Fine-tuning mạng được pre-trained với số lượng hạn chế ví dụ huấn luyện chỉ của các lớp mới khiến mô hình quên các lớp cũ (quên thảm khốc) và overfitting trên các lớp mới gần đây [17, 22, 31, 68]. Một lượng lớn nghiên cứu đã được tiến hành để giải quyết vấn đề quên thảm khốc [25]. Điều này bao gồm các phương pháp dựa trên: ràng buộc thay đổi trọng số [2,16,37,44,78], giữ lại các mẫu từ dữ liệu trước đó trong bộ nhớ [2,6,13,54,63], data augmentation [72,77,84,86], kiến trúc mở rộng động (DEA) mở rộng mạng cho mỗi task id mới đến trong khi trọng số của mạng cơ bản được đóng băng để học giữ cả thông tin cũ và mới [18, 20, 24, 33, 45, 73]. Tất cả các phương pháp này được phân loại rộng rãi thành hai chủ đề chính tức là multi-task và multi-class. Các phương pháp multi-task như DEA yêu cầu giải quyết task id trong quá trình suy luận, điều này thường không có sẵn. Kịch bản multi-class đề cập đến việc học một bộ phân loại duy nhất với mục đích nhận ra các lớp cơ bản và mới trong một task duy nhất. Trong bài báo này, chúng tôi nghiên cứu vấn đề FSCIL trong kịch bản multi-class, vì nó thực tế và thiết thực hơn. Các phương pháp gần đây [1, 28, 80] đã đề xuất học mạng backbone như bộ trích xuất đặc trưng bằng dữ liệu của các lớp cơ bản, và sau đó sử dụng bộ trích xuất đặc trưng đóng băng này để học các prototype bộ phân loại cho các lớp mới một cách tăng trưởng. Nhưng điều này không đảm bảo khả năng tách biệt tối đa giữa các prototype bộ phân loại cho các lớp cơ bản và mới và do đó có thể dẫn đến sự nhầm lẫn giữa các lớp cũ và mới dẫn đến hiệu suất hạn chế. Các phương pháp khác [1, 9, 28], sử dụng các hàm loss tùy chỉnh và regularizer để học các prototype bộ phân loại cho các lớp mới cùng với việc ngăn chặn quên về các lớp cơ bản, cũng bị giới hạn bởi hiệu suất vì sự không đồng bộ giữa các đặc trưng cố định của các lớp cơ bản và bộ phân loại. Công trình gần đây NC-FSCIL [75] đề xuất sử dụng neural collapse để học bộ phân loại tách biệt tối đa. Họ đề xuất học bộ phân loại hai lớp với trọng số simplex cố định trước cho cơ bản và mỗi phiên tăng trưởng. Việc huấn luyện bộ phân loại hai lớp với trọng số cố định trước cho các lớp mới ít mẫu sẽ không thể tổng quát hóa tốt và do đó dẫn đến biểu diễn lớp overfitted. Ngược lại với họ, chúng tôi đề xuất học bản thân biểu diễn lớp tách biệt tối đa bằng thuộc tính Neural Collapse, nhưng chúng tôi sử dụng phân tích concept trên mạng backbone để có thể biểu diễn bất kỳ lớp nào nói chung và do đó thu được bộ phân loại tổng quát cho các lớp mới ít mẫu.

Trong công trình này, chúng tôi giải quyết vấn đề không đồng bộ này giữa các đặc trưng cố định của mạng backbone (bộ trích xuất đặc trưng) và các prototype bộ phân loại để ngăn chặn quên lớp cơ bản. Hướng tới điều đó, chúng tôi cố gắng học bộ phân loại tách biệt tối đa để tránh nhầm lẫn giữa các lớp cơ bản và mới trong mỗi phiên tăng trưởng. Công trình của chúng tôi được truyền cảm hứng từ hai nghiên cứu chính: 1) Neural Collapse (NC) cho dữ liệu không cân bằng [12, 53] và, 2) Phân tích Concept [19, 35].

Neural collapse là hiện tượng mà mạng khi được huấn luyện vượt quá lỗi zero hướng tới loss zero, dẫn đến việc collapse các đặc trưng lớp cuối cùng của mạng backbone để tạo thành một Khung Chặt Equiangular (ETF). Các đỉnh của khung này biểu thị vector đặc trưng đại diện cho lớp và được căn chỉnh với prototype bộ phân loại của lớp tương ứng [53]. Điều này đảm bảo một bộ phân loại tách biệt tối đa vì ETF là một cấu trúc hình học tạo thành một simplex nơi phương sai trong lớp được tối thiểu hóa (vì collapse thành một vector duy nhất) và phương sai giữa các lớp được tối đa hóa nằm ở các góc bằng nhau với nhau. Tuy nhiên, với simplex cố định trước cho các lớp cơ bản, bộ trích xuất đặc trưng dễ huấn luyện vì dữ liệu đủ có sẵn cho mỗi lớp trong phiên cơ bản (t=0) dẫn đến collapse nhưng đối với bất kỳ phiên tăng trưởng t≥1 với ít mẫu có nhãn, việc học các đặc trưng collapsed cho các lớp mới là thách thức vì với ít mẫu (nhiều nhất là 5 mẫu cho một lớp), bộ trích xuất đặc trưng cố định không thể căn chỉnh tốt với prototype lớp mới. Để giải quyết điều đó, chúng tôi giới thiệu cơ chế phân tích concept, nơi chúng tôi phân tích bộ trích xuất đặc trưng collapsed trên phiên cơ bản để xác định cơ sở concept trong các hình ảnh đầu vào. Khi cơ sở concept ("concept bank") được xác định từ phiên cơ bản, chúng tôi nhận ra chúng như khối xây dựng từ đó simplex bộ phân loại phiên tăng trưởng được tạo ra và do đó tập hệ số mới có thể được học để tạo ra simplex với các lớp mới. Điều này được fine-tuned thêm cùng với simplex lớp cơ bản để căn chỉnh thêm điều này với các instance ít mẫu để giảm bất kỳ lỗi không thể giảm nào xảy ra trong quá trình tính toán hệ số tối ưu cho "concept bank". Điều này đã được minh họa trong Hình 1. Để tóm tắt, các đóng góp chính của chúng tôi như sau:

• Chúng tôi giới thiệu framework mới MASIL như một nỗ lực để học bộ phân loại tách biệt tối đa cho FSCIL.

• Chúng tôi xác định cơ chế mà các đặc trưng collapsed phiên cơ bản (thu được theo thuộc tính Neural Collapse) có thể được phân tích thêm dưới dạng "concept bank", tạo thành cơ sở để xây dựng prototype bộ phân loại của các lớp mới gặp phải trong phiên tăng trưởng.

• Đánh giá trên ba bộ dữ liệu benchmark FSCIL phổ biến chứng minh hiệu suất state-of-the-art. Nghiên cứu ablation mở rộng đã được thực hiện để phân tích tầm quan trọng của hàm loss được giới thiệu bằng thuộc tính Neural collapse và lợi thế của simplex fine-tuning để giảm lỗi không thể giảm.

## 2. Công trình Liên quan

### 2.1. Học Ít mẫu
Ý tưởng của học ít mẫu (FSL) là thích ứng mô hình trên các lớp mới (chỉ với ít instance có nhãn) mà không quan tâm đến hiệu suất trên các lớp cơ bản. Hầu hết các công trình sử dụng meta-learning [21, 64–66] hoặc metric learning [64, 66, 69]. Gần đây, các phương pháp [23, 59] đã chứng minh việc sử dụng meta learning để nhận ra cả các lớp cơ bản và mới, bằng cách lấy mẫu "fake" few shot classification task từ các lớp cơ bản để học một bộ phân loại cho các lớp mới. Cuối cùng, các trọng số bộ phân loại đã học được kết hợp để cùng nhận ra các lớp cơ bản và mới. Một số công trình [59] coi điều này như một loại học tăng trưởng. Ngược lại, thiết lập FSCIL thực tế hơn nhiều nơi bộ dữ liệu cơ bản không thể truy cập được trong giai đoạn tăng trưởng và chúng ta phải thích ứng mô hình cho các lớp mới mà không có quên thảm khốc [17,68]. Các phương pháp metric learning tập trung vào việc học một mạng backbone mạnh để học các đặc trưng có thể chuyển giao qua các task, trên đó hàm tương tự (như k-nearest neighbours trong [69], metric khoảng cách phi tuyến trong [66]) được học để chứng minh khả năng phân loại các lớp mới với các đặc trưng có thể chuyển giao. Tuy nhiên, điều này yêu cầu huấn luyện nhiều hàm tương tự bằng số lượng phiên tăng trưởng trong FSCIL nhưng mục đích của FSCIL là huấn luyện một bộ phân loại thống nhất cho các lớp cơ bản và mới. Chúng tôi sẽ thảo luận trong phần tiếp theo về cách các công trình hiện tại đã xử lý vấn đề FSCIL khác với FSL.

### 2.2. Học Tăng trưởng Lớp Ít mẫu

**Học Tăng trưởng Lớp (CIL):** Chúng tôi bắt đầu bằng việc thảo luận về ý tưởng Học Tăng trưởng Lớp (CIL), nó nhằm học một bộ phân loại quản lý để liên tục cập nhật chính nó để nhận ra tất cả các lớp mới mà không quên các lớp cơ bản [7, 44, 58]. Để vượt qua cơ chế quên này, các nghiên cứu CIL đã được phân loại thành ba loại rộng: dựa trên regularization [16, 37, 44], dựa trên rehearsal [2, 6, 13, 54], và knowledge distillation [30, 58, 71]. Các phương pháp dựa trên regularization ràng buộc thay đổi trọng số trên các lớp mới do đó giữ thông tin đã học trước đó cho các lớp cơ bản, điều này khiến các phương pháp này gặp khó khăn về tổng quát hóa trên các lớp mới vì thay đổi nhỏ được cho phép trong trọng số. Các phương pháp dựa trên rehearsal trong đó mô hình liên tục được làm mới bằng kho dữ liệu cũ để nó duy trì học các lớp mới cùng với các lớp cũ. Điều này bị giới hạn bởi lượng dữ liệu cũ nó có thể giữ trong bộ nhớ, và cách các instance từ dữ liệu cũ được chọn để có thông tin tối đa với yêu cầu bộ nhớ tối thiểu. Các phương pháp này không thể mở rộng cho số lượng lớn các lớp vì bộ nhớ hạn chế. Ví dụ, iCaRL [58] học bộ phân loại nearest neighbour cho các lớp mới trong khi duy trì bộ nhớ của các exemplar từ phiên cơ bản. Các phương pháp dựa trên Knowledge Distillation yêu cầu sử dụng mô hình teacher lớn để hướng dẫn việc học của mô hình student nhỏ [29,74]. Nó hoạt động bằng cách chưng cất thông tin đã học trước đó sang mô hình mới với các lớp mới, để tránh quên các lớp cơ bản. Các nghiên cứu gần đây [16, 18, 30] tiến hành chưng cất ở mức đặc trưng thay vì ở mức logit đầu ra tại bộ phân loại. Tuy nhiên, các giải pháp này gặp vấn đề phân biệt giữa các lớp cơ bản và mới dẫn đến hạn chế trong hiệu suất.

**Học Tăng trưởng Lớp Ít mẫu (FSCIL):** So với thiết lập CIL, FSCIL nhằm học các lớp mới (cùng với các lớp cơ bản) với ít instance có nhãn [17, 68], điều này thực tế và khó hơn nhiều, vì việc học từ ít instance của các lớp mới gây ra overfitting trên các lớp mới [64,66]. Để làm điều này, một số nghiên cứu đã tập trung vào việc căn chỉnh phiên cơ bản và tăng trưởng bằng augmentation [56], tìm kiếm flat minima [62]. Tuy nhiên, để tránh overfitting với các lớp mới, cần thiết là các prototype bộ phân loại cho các lớp mới nên tách biệt tối đa với các lớp cơ bản. Việc điều chỉnh prototype cho các lớp cơ bản là không khả thi vì điều đó yêu cầu sử dụng dữ liệu phiên cơ bản. Tuy nhiên, các nghiên cứu này [80, 85] đã tập trung vào việc phát triển prototype cho các lớp mới. Số lượng lớn các công trình hiện tại đã tập trung vào việc xây dựng loss tùy chỉnh và regularizer [1, 28, 30, 34, 50, 59, 67, 74]. Tuy nhiên, các nhược điểm tương tự mà chúng tôi đã thảo luận trong CIL cho regularization và các hàm loss tùy chỉnh cũng áp dụng trong FSCIL. Trong công trình này, chúng tôi tập trung vào sự phát triển tối ưu của prototype cho các lớp mới được dẫn xuất từ cùng hàm sử dụng để phát triển prototype lớp cơ bản và đảm bảo khả năng tách biệt tối đa giữa các lớp cũ và mới.

## 3. Phát biểu Vấn đề và Bối cảnh

Trong phần này, chúng tôi sẽ giới thiệu định nghĩa vấn đề của Học Tăng trưởng Lớp Ít mẫu trong Phần 3.1 và bối cảnh trong các phần tiếp theo.

### 3.1. Học Tăng trưởng Lớp Ít mẫu

Chính thức, chúng tôi định nghĩa Học Tăng trưởng Lớp Ít mẫu (FSCIL) là luồng dữ liệu có nhãn theo trình tự thời gian là D₀,D₁, ....., trong đó Dₜ={(xᵢₜ;yᵢₜ)}ᵢ₌₁|Dₜ|. Cₜ là số lượng lớp trong tập huấn luyện Dₜ, trong đó ∀(i,j) Cᵢ ∩ Cⱼ = ∅. Cụ thể, chúng tôi coi D₀ là phiên cơ bản với không gian nhãn lớn C₀ với mỗi lớp c ∈ C₀ có đủ hình ảnh huấn luyện. Đối với t > 0 mỗi phiên tăng trưởng Dₜ chỉ có ít hình ảnh có nhãn cho mỗi lớp mới. FSCIL được định nghĩa là huấn luyện tăng trưởng theo bước thời gian của mô hình trên Dₜ ∀t > 0 mà không có quyền truy cập vào bất kỳ tập có nhãn trước đó nào từ D₀ đến Dₜ₋₁. Đối với t > 0 Dₜ chúng tôi ký hiệu thiết lập là C lớp với K ví dụ huấn luyện mỗi lớp là C-way K-Shot FSCIL trong đó Cₜ ∩ C₀ₜ = ∅ ∀t ≠ t'. Sau mỗi phiên huấn luyện tăng trưởng với Dₜ, mô hình được đánh giá để nhận ra tất cả các lớp huấn luyện gặp phải cho đến nay tức là ⋃ᵢ₌₀ᵗ Cᵢ. Do đó, FSCIL không chỉ nhằm nhận ra các lớp mới mà còn tránh quên các lớp cũ và thiết lập học các lớp mới rất mất cân bằng và cũng gặp vấn đề khan hiếm dữ liệu. Điều này làm cho thiết lập FSCIL phù hợp hơn cho các ứng dụng thế giới thực.

**Khởi tạo:** Giả sử C₀ là số lượng lớp cơ bản và chúng ta có tổng cộng T phiên tăng trưởng và mỗi phiên có k lớp, vậy sẽ có tổng cộng K = C₀ + T×k lớp. Để có thể thực hiện FSCIL, chúng tôi ký hiệu mô hình được huấn luyện trên phiên cơ bản bao gồm bộ trích xuất đặc trưng backbone f(·;θf) và tham số bộ phân loại W ∈ ℝᴷˣᵈ, trong đó W là bộ phân loại MLP bao gồm L lớp được ký hiệu là W = W₁W₂...WL. Đối với đầu vào X ∈ ℝⁿ chúng tôi ký hiệu các đặc trưng thu được từ bộ trích xuất đặc trưng là H = f(X;θf) ∈ ℝᵈˣᴺ, trong đó N là tổng số instance huấn luyện. Tương tự như [12] chúng tôi cũng coi các đặc trưng lớp cuối H là các biến tối ưu hóa tự do. Mục tiêu tối ưu hóa sau đó được định nghĩa như sau:

min W,H L(W,H) = 1/(2N)||WH - Y||²F + λW/2||W||²F + λH/2||H||²F (1)

trong đó Y ∈ ℝᴷˣᴺ, là nhãn lớp cho mỗi instance huấn luyện trong N instance và λW, λH là các siêu tham số regularization.

### 3.2. Neural Collapse

Trong các công trình gần đây [12, 53] đã nghiên cứu thực hành huấn luyện DNN vượt quá lỗi zero hướng tới loss zero. Điều này tiết lộ cấu trúc hình học dưới dạng simplex equiangular tight frame, được hình thành bởi các đặc trưng lớp cuối cùng cùng với trọng số bộ phân loại. Điều này đã được chứng minh trên dữ liệu cân bằng và các mô hình với nhiều kiến trúc phổ biến khác nhau. Neural Collapse như được định nghĩa trong [53] bao gồm bốn thuộc tính sau:

• (NC1) Variability Collapse: Các đặc trưng lớp cuối của mạng backbone cho một lớp cụ thể collapse thành mean trong lớp.

• (NC2) Convergence: dẫn đến các class-mean tối ưu được tách biệt bằng nhau và tối đa tạo thành một simplex Equiangular Tight Frame (ETF).

• (NC3) Classifier Convergence: Các class mean tối ưu tạo thành ETF được căn chỉnh với các trọng số bộ phân loại tương ứng theo rescaling.

• (NC4) Simplification to nearest class center: Khi (NC1)-(NC3) giữ, dự đoán mô hình sử dụng logit tôn trọng các trung tâm lớp gần nhất.

Ngoài dữ liệu cân bằng, [12] đã dẫn xuất phân tích hình học cho dữ liệu không cân bằng như sau:

**Định nghĩa 1.** Cho (W*,H*) là bộ tối ưu toàn cục của phương trình 1, r = min(K;d) và W = UW SW VᵀW là phân tích SVD của W. Sau đó, điều sau giữ cho dữ liệu không cân bằng:

• (NC1) dẫn đến collapse của đặc trưng trong cùng lớp H̄ = HY, trong đó H̄ = [h̄₁; h̄₂; ...; h̄K] ∈ ℝᵈˣᴷ

• (NC3) dẫn đến căn chỉnh giữa trọng số bộ phân loại và class mean tương ứng là w̄k = √nk H̄W h̄k ∀k ∈ [K], trong đó nk là số instance của lớp k.

• (NC2) dẫn đến các class mean tối ưu được tách biệt bằng nhau và tối đa tạo thành simplex Equiangular Tight Frame (ETF) WW⊤ = diag{s²k}ᵏₖ₌₁, trong đó sk là các giá trị singular của W̄

Một phương pháp khác tức là Deep Simplex Classifier [8], đề xuất bài toán tối ưu hóa là tối thiểu hóa các đặc trưng thu được từ bộ trích xuất đặc trưng tới các đỉnh của simplex như:

min hᵢ∈HT 1/n Σⁿᵢ₌₁||hᵢ - syᵢ||² (2)

trong đó syᵢ là đỉnh của simplex và được coi là trung tâm lớp cho lớp yᵢ.

## 4. MASIL

Framework tổng thể của phương pháp đề xuất của chúng tôi được minh họa trong Hình 1. FSCIL nhằm học trọng số bộ phân loại W ∈ ℝᴷˣᵈ hoạt động cho tất cả các lớp bất kể chúng thuộc về các lớp cơ bản trong t = 0 hay các lớp ít mẫu trong t > 0. Theo truyền thống, điều này đã được đạt được bằng cách đầu tiên học trọng số bộ phân loại cho C₀ lớp cơ bản và sau đó học trọng số cho các lớp mới W⁽ᵗ⁾ ∈ ℝᵏˣᵈ với ràng buộc regularized trong hàm loss rằng các trọng số cũ W ∈ ℝᶜ⁰⁺⁽ᵗ⁻¹⁾ᵏ được bảo tồn với ít hoặc không có cập nhật. Tuy nhiên, điều này dẫn đến sự không đồng bộ giữa các prototype bộ phân loại của các lớp cũ và mới gây ra nhầm lẫn cũ mới (ONC) [31] và quên thảm khốc [25]. Điều này gây ra sự sụt giảm hiệu suất của bộ phân loại FSCIL khi số lượng phiên tăng trưởng tăng lên dẫn đến khả năng tổng quát hóa kém ngay cả trong việc nhận ra các lớp cơ bản. Để giảm thiểu điều này, trong công trình này chúng tôi áp dụng các thuộc tính thu được từ Neural Collapse để học bộ phân loại tách biệt tối đa cùng với phân tích concept để học trọng số bộ phân loại (được tổ chức như simplex) cho các lớp mới với ít mẫu. Chúng tôi hạn chế bộ trích xuất đặc trưng khỏi các cập nhật trong quá trình huấn luyện phiên tăng trưởng và dựa vào phân tích concept của các activation thu được cho các lớp cơ bản để có được cơ sở của các concept gọi là "concept bank", sử dụng để có thể biểu diễn trọng số bộ phân loại tách biệt tối đa tức là simplex cho các lớp ít mẫu. Để biểu diễn simplex bộ phân loại bằng "concept bank" nó yêu cầu chỉ giải quyết cho ma trận hệ số có thể được thực hiện bằng cách chỉ giải quyết Non Negative Least Squares (NNLS).

### 4.1. Phân tích Concept

Ý tưởng của phân tích concept liên quan đến hiện tượng neural collapse, nơi nó học tách biệt tối đa các lớp bằng cách tạo thành simplex ở mức lớp trên cả hai mức của đặc trưng lớp và trọng số bộ phân loại. Để đạt được điều này, nó kết hợp các activation (trong quá trình forward pass) của cùng lớp cho đến khi tất cả hội tụ thành vector lớp one hot tại lớp logit như được mô tả trong phương trình 2. Điều này cho phép các vector đặc trưng theo lớp được tập trung ở các lớp cao hơn có thể được phân tách đệ quy thành nhiều concept di chuyển từ lớp cao nhất xuống các lớp thấp hơn truy vết trở lại các hình ảnh đầu vào nơi nó có thể được giải thích với các vùng làm concept, sự kết hợp của chúng làm cho có thể phân loại nó thành lớp cụ thể. Chúng tôi áp dụng NMF (Non Negative Matrix Factorization) như trong [19] của các activation thu được tại đầu ra của bộ trích xuất đặc trưng được cho là:

min P≥0,Q≥0 1/2||A - PQᵀ||²F (3)

trong đó, ||·||F là chuẩn Frobenius, các activation A ∈ ℝⁿˣᵈ thu được từ crop của hình ảnh Xᵢ = ψ(xᵢ) Xᵢ ∈ Xⁿ với ψ là hàm crop. Chúng tôi lấy các crop ngẫu nhiên (được điều chỉnh bởi ψ) của hình ảnh, điều này dẫn đến các concept độc đáo qua các danh mục để có thể xây dựng ngân hàng của các vector concept độc đáo gọi là "concept bank". Các activation tại lớp cuối của bộ trích xuất đặc trưng sau global pooling cho các crop ngẫu nhiên này được cho là A = f(X̃;θf) ∈ ℝⁿˣᵈ. NMF đơn giản là phân tích các activation concept A thành "concept bank" Q ∈ ℝᵛˣᵈ (trong đó nó tuân theo phân tích rank thấp v ≤ min(n;p)) và hệ số P ∈ ℝⁿˣᵛ biểu thị tầm quan trọng của mỗi concept trong việc giải thích các activation A. Khi "concept bank" được tính toán trước, chúng ta có thể thu được hệ số P(x) cho bất kỳ đầu vào x bằng NNLS (Non-Negative Least Squares) tức là min P≥0 1/2||f(x;θf) - P(x)Qᵀ||²F. Liên hệ phân tích activation trong phương trình 3 và neural collapse trong phương trình 2, ngụ ý rằng các activation khi collapse thành vector đặc trưng mean cho mỗi lớp tạo thành vector simplex lớp được tạo thành từ các vector cơ sở concept và các hệ số tương ứng, kết hợp cho tất cả các lớp cho cơ sở tổng thể gọi là "concept bank".

### 4.2. Lớp NMF

Trong quá trình phân tích NMF của phương trình 3, chúng tôi giữ bộ trích xuất đặc trưng f(·;θf) đóng băng. Chúng tôi tiếp cận giải pháp bài toán NMF bằng ADMM (Alternating Direction Method of Multipliers) [5] vì NMF là không lồi, nhưng tuy nhiên nó có thể được làm lồi bằng cách cố định giá trị của một trong hai yếu tố (P;Q) yêu cầu cập nhật xen kẽ của một trong hai yếu tố cố định một tại một thời điểm, tương đương với việc giải quyết bài toán Non-Negative Least Squares (NNLS) làm cho nó lồi. Cơ chế cập nhật xen kẽ này gọi là ADMM, được công thức hóa như:

Pᵗ⁺¹ = arg min P≥0 1/2||A - PQᵗᵀ||²F (4)

Qᵗ⁺¹ = arg min Q≥0 1/2||A - PᵗQᵀ||²F (5)

Nó đảm bảo minimum toàn cục hoặc cục bộ vì mỗi bài toán NNLS tuân theo điều kiện tối ưu Karush–Kuhn–Tucker (KKT) [36, 42]. Sử dụng các điều kiện này tạo thành hàm ngầm [26] làm cho việc vi phân ngầm [3, 26, 38] cho phép tính toán gradient (∂P/∂A; ∂Q/∂A), nhưng tuy nhiên chúng ta phải liên hệ các concept với các vùng hình ảnh đầu vào chúng ta yêu cầu tính toán (∂P/∂X; ∂Q/∂X). Điều này có thể được tính toán như:

∂P/∂X = ∂A/∂X ∂P/∂A; ∂Q/∂X = ∂A/∂X ∂Q/∂A (6)

Tính toán ∂A/∂X khá đơn giản bằng Pytorch. Chi tiết thêm về việc triển khai kết hợp gradient từ vi phân ngầm trong Jax[4,32] và gradient từ tính toán Pytorch được chi tiết trong Phần A. Khi chúng tôi tính toán trước "concept bank" cho các lớp cơ bản bằng phương trình 5 và 6, chúng tôi cố định Q và chỉ cho phép tính toán hệ số tối ưu P(x) cho bất kỳ đầu vào x bằng NNLS

min P≥0 1/2||f(x;θf) - P(x)Qᵀ||²F (7)

cho biểu diễn tối ưu của activation cho bất kỳ đầu vào x dưới dạng các vector cơ sở concept.

### 4.3. Biểu diễn Simplex Bộ phân loại

Phương trình 2 đang tối ưu hóa biểu diễn đặc trưng cho mỗi lớp dẫn đến biểu diễn collapsed cho lớp yᵢ là syᵢ. Tương tự, phương trình 1 dẫn đến biểu diễn simplex cho mỗi lớp tức là wyᵢ ∈ W. Vậy nếu chúng ta coi biểu diễn simplex chuẩn hóa trên một siêu cầu đơn vị [8] của mỗi lớp thì:

wᵀyᵢ syᵢ = 1 ∀yᵢ ∈ [⋃ʲ⁼⁰ᵗ Cⱼ (8)

dẫn đến hàm loss sửa đổi của phương trình 2 thành:

min hᵢ 1/|Dⱼ| ∑(xᵢ,yᵢ)∈Dⱼ ||wᵀyᵢ hᵢ - 1||²F (9)

s.t. wᵀyᵢ syᵢ = 1 giống như trong phương trình 1 và do đó tuân theo các thuộc tính neural collapse. Hơn nữa, tối ưu hóa phương trình 2, dẫn đến biểu diễn đặc trưng collapsed cho tất cả các instance thuộc về lớp đó. Thêm vào đó, phương trình 3 tính toán xấp xỉ tốt nhất của biểu diễn đặc trưng collapsed H ≈ PQᵀ. Đối với bất kỳ đầu vào (xᵢ;yᵢ) thuộc về Dⱼ; j > 0, thì hᵢ tối ưu thu được từ phương trình 7 được cho là:

hᵢ = P(xᵢ)Qᵀ (10)

Từ (NC1), biểu diễn đặc trưng collapsed của mỗi lớp hội tụ thành một vector độc đáo ví dụ cho lớp yᵢ biểu diễn đặc trưng của tất cả các instance được ký hiệu là Hyᵢ ∈ H, (NC1) ngụ ý covariance ∑Hyᵢ → 0. tức là các đặc trưng collapse thành class mean tương ứng của chúng tức là h̄yᵢ = ∑ⁿʸⁱᵢ₌₁ hᵢ, trong đó nyᵢ là số instance của lớp yᵢ, và theo loss trong phương trình 2, điều này tối thiểu khi syᵢ = h̄yᵢ, sau đó từ phương trình 8 và 10:

ŵyᵢ = 1/|Dⱼ| (∑(xᵢ,yᵢ)∈Dⱼ P(xᵢ))Qᵀ ∀j > 0 (11)

trong đó hệ số P(xᵢ) cho mỗi instance của lớp yᵢ được tính toán bằng NNLS theo phương trình 7, thêm vào đó, yᵢ ∈ Cⱼ; j > 0 là các lớp ít mẫu và trọng số bộ phân loại là biểu diễn simplex tối ưu cho các lớp ít mẫu. Đối với các lớp phiên cơ bản (j = 0) biểu diễn simplex bộ phân loại đơn giản là ŵyᵢ = sᵀyᵢ. Vì chúng tôi triển khai bộ phân loại bằng MLP với L = 2 lớp, cho mỗi lớp biểu diễn simplex là ŵₗ,yᵢ = (ŵyᵢ)¹/ᴸ.

### 4.4. Simplex Fine-tuning

Trong Phần 4.3 chúng tôi mô tả biểu diễn simplex tối ưu cho mỗi lớp thuộc về lớp ít mẫu yᵢ ∈ Cⱼ; j > 0. Nhưng, tuy nhiên do lỗi không thể giảm vốn có đối với NNLS, chúng tôi tiếp cận biểu diễn simplex tối ưu cho lớp ít mẫu bằng cách fine-tuning thêm trọng số bộ phân loại (giữ bộ trích xuất đặc trưng đóng băng) được khởi tạo bằng biểu diễn simplex như thu được trong phương trình 11. Để tránh làm lệch trọng số quá nhiều khỏi biểu diễn simplex tối ưu, chúng tôi thêm một ràng buộc vào loss trong phương trình 9 như:

min wyᵢ L(wyᵢ) = 1/|Dⱼ| ∑(xᵢ,yᵢ)∈Dⱼ ||wᵀyᵢ hᵢ - 1||²F + λ||wyᵢ - ŵyᵢ||²F; λ ∈ [0,1] (12)

trong đó yᵢ ∈ Cⱼ; j > 0 và bộ trích xuất đặc trưng được đóng băng và do đó tối ưu hóa cho wyᵢ tốt nhất. Vì huấn luyện phiên cơ bản (tức là j = 0 và dataset D₀) được điều chỉnh bằng hàm loss của phương trình 2 và do đó dẫn đến biểu diễn collapsed của đặc trưng tại lớp terminal cho mỗi lớp yᵢ ∈ C₀. Để biểu diễn simplex cho mỗi lớp trong C₀ vẫn tách biệt tối đa với những cái thu được cho lớp ít mẫu, chúng tôi sử dụng biểu diễn collapsed của đặc trưng cho mỗi lớp để fine-tune thêm biểu diễn simplex, nhưng mà không giữ các instance hình ảnh trong bộ nhớ chúng tôi ghi nhớ biểu diễn collapsed (là biểu diễn mean của đặc trưng instance cho mỗi lớp) trong M được cho là:

Myᵢ = 1/nyᵢ ∑ⁿʸⁱᵢ₌₁ hᵢ; ∀Myᵢ ∈ M (13)

trong đó, nyᵢ là số instance của lớp yᵢ. Hàm loss cập nhật trong giai đoạn fine-tuning bao gồm các lớp phiên cơ bản và lớp ít mẫu được cho là:

min wyᵢ L(wyᵢ) = 1/|Dⱼ| ∑(xᵢ,yᵢ)∈Dⱼ ||wᵀyᵢ hᵢ - 1||²F + 1/|M| ∑(Myᵢ,yᵢ)∈M ||wᵀyᵢ Myᵢ - 1||²F + λ||wyᵢ - ŵyᵢ||²F; λ ∈ [0,1] (14)

trong đó ràng buộc bây giờ có hiệu lực cho các lớp phiên cơ bản cũng như các lớp ít mẫu với thực tế rằng biểu diễn simplex cho mỗi lớp không nên lệch nhiều (phụ thuộc vào hệ số đóng góp λ) khỏi biểu diễn simplex tối ưu. Trong mỗi phiên tăng trưởng, chúng tôi huấn luyện mạng bộ phân loại của chúng tôi bằng hàm loss này sau khi dẫn xuất biểu diễn simplex cho mỗi lớp ít mẫu từ phương trình 11.

## 5. Thí nghiệm

Chúng tôi chứng minh tính hiệu quả của MASIL trên ba bộ dữ liệu benchmark FSCIL nổi tiếng (như trong ALICE [56]) được mô tả trong Phần 5.1 cùng với thiết lập FSCIL và so sánh hiệu suất của nó với các phương pháp state-of-the-art (Phần 5.2). Chi tiết huấn luyện và siêu tham số được thảo luận trong Phụ lục A.

### 5.1. Chi tiết Dataset

• **CIFAR-100** [39] bao gồm 100 lớp tổng cộng với hình ảnh màu kích thước 32×32. Mỗi lớp bao gồm 500 hình ảnh để huấn luyện và 100 hình ảnh để kiểm tra. Phiên cơ bản (t = 0) bao gồm 60 lớp và 40 lớp còn lại đóng góp cho 8 phiên tăng trưởng với thiết lập 5-way 5-shot (tức là 5 hình ảnh cho mỗi trong 5 lớp) cho 1 ≤ t ≤ 8.

• **miniImageNet** [61] là biến thể của ImageNet [15] với hình ảnh màu kích thước 84×84. Nó cũng bao gồm cùng số lượng lớp như CIFAR-100 và cùng số lượng hình ảnh trong train và test, dẫn đến cùng cấu hình cho phiên cơ bản và tăng trưởng.

• **CUB-200** [70] bao gồm 11,788 hình ảnh (kích thước 224×224) tổng cộng trải rộng trên 200 lớp. Có 5,994 hình ảnh trong train và 5,794 hình ảnh trong test. Phiên cơ bản (t = 0) bao gồm 100 lớp và 100 lớp còn lại đóng góp cho 10 phiên tăng trưởng (1 ≤ t ≤ 10) với thiết lập 10-way 5-shot (5 hình ảnh cho 10 lớp mỗi).

### 5.2. Đánh giá Benchmark

So sánh hiệu suất trên miniImageNet, CIFAR-100 và CUB-200 được chứng minh trong Bảng 1, 2 và 5 (cho trong Phụ lục B do hạn chế không gian) tương ứng. Phương pháp MASIL của chúng tôi vượt trội trong tất cả các phương pháp trong phiên cuối với cải thiện tương đối +3.16%, +2.05% và +0.14% trên miniImageNet, CIFAR-100, CUB-200 tương ứng so với baseline mạnh nhất ALICE [56]. Thêm vào đó, phương pháp của chúng tôi vượt trội tất cả các phương pháp trong tất cả các phiên (trừ trên CUB-200 phiên 2). Hơn nữa, về độ chính xác trung bình phương pháp của chúng tôi vượt trội ít nhất +1.79% so với baseline mạnh nhất, tập thể là một chỉ báo rằng mô hình của chúng tôi giúp giảm thiểu vấn đề quên trong thiết lập thực tế của học liên tục cụ thể là FSCIL.

### 5.3. Nghiên cứu Ablation

Chúng tôi xem xét các biến thể cho mô hình cơ bản (mạng backbone tức là ResNet-18 với bộ phân loại và bộ nhớ như giới thiệu trong phương trình 13) để xác nhận 1) hiệu ứng của loss được giới thiệu trong phương trình 9 (ETF) so với cross-entropy (CE) loss với và không có prototype bộ phân loại simplex được tạo ra bởi neural collapse, 2) hiệu ứng của simplex ít mẫu được tạo ra với phân tích concept và 3) hiệu ứng của simplex fine tuning. Để xác nhận hiệu ứng đầu tiên có hai mô hình. Mô hình đầu tiên (Learnable + CE) sử dụng bộ phân loại với trọng số học từ CE loss, đây là thực hành phổ biến nhất. Mô hình thứ hai (NC+ CE) sử dụng CE loss với trọng số bộ phân loại theo thuộc tính neural collapse nhưng sử dụng CE loss thay vì loss trong phương trình 9. Để xác nhận hiệu ứng thứ hai (mô hình thứ ba tức là NC+ ETF Loss) chúng tôi không khởi tạo trọng số bộ phân loại cho các lớp ít mẫu và huấn luyện chúng theo loss trong phương trình 9 với bộ nhớ của các lớp cơ bản như trong phương trình 13. Đối với hiệu ứng thứ ba tức là mô hình thứ tư (NC+ ETF Loss + CF) chúng tôi báo cáo hiệu suất mà không fine tuning và chỉ sử dụng trọng số bộ phân loại được tính toán từ phân tích concept (CF) như trong phương trình 11. Cuối cùng, chúng tôi báo cáo hiệu suất của MASIL để so sánh giữa tất cả chúng. Như được thể hiện trong Bảng 3, việc áp dụng hàm loss trong phương trình 9 chắc chắn giúp giảm thiểu sụt giảm hiệu suất so với CE loss ngay cả với trọng số bộ phân loại được giả định là tạo thành simplex, và nó tiếp tục giảm thiểu bằng cách sử dụng trọng số được khởi tạo với CF và hơn nữa với fine tuning. Nó chỉ ra sự thành công của CF cùng với neural collapse hướng tới giải pháp tối ưu cho FSCIL.

### 5.4. Phân tích Trọng số Bộ phân loại

Chúng tôi tiếp tục phân tích sự căn chỉnh trọng số bộ phân loại đối với mean feature (collapsed feature) của mỗi lớp. Chúng tôi sử dụng trọng số bộ phân loại và mean feature từ mỗi mô hình được mô tả trong nghiên cứu ablation để xác nhận hiệu ứng của MASIL trong việc học bộ phân loại tách biệt tối đa, nơi thuộc tính tách biệt giữa các lớp được đo bằng độ tương tự cosine. Cụ thể, chúng tôi vẽ độ tương tự cosine trung bình giữa mean feature và trọng số bộ phân loại của các lớp khác nhau tức là Avgₖ≠ₖ'{h̄ₖᵀw̄ₖ'} cho cả bộ dữ liệu train và test. Chúng tôi đã minh họa điều này cho miniImageNet trong Hình 2. Rõ ràng, trên cả train và test độ tương tự giữa các lớp khác nhau tiếp tục tăng cho mô hình "Learnable + CE". Trong khi sử dụng loss trong phương trình 9 (theo Neural Collapse) không có xu hướng tăng. Việc kết hợp phân tích concept và simplex fine tuning (trong MASIL) tiếp tục giảm độ tương tự khi phiên tiến triển và do đó giảm thiểu hiệu ứng quên và xác nhận khả năng tách biệt tối đa với MASIL.

## 6. Kết luận

Trong bài báo này, chúng tôi đề xuất framework mới MASIL như một bước tiến hướng tới học bộ phân loại tách biệt tối đa trong thiết lập cạnh tranh của học liên tục tức là FSCIL. Chúng tôi đề xuất việc tạo ra simplex từ phân tích concept giúp trong các trường hợp ít mẫu. Chúng tôi giới thiệu hàm loss mới nơi các lớp cơ bản và mới có thể được học cùng nhau trong quá trình fine tuning để giảm thiểu thêm việc quên và overfitting. Trong thí nghiệm MASIL vượt trội tất cả các benchmark với margin đủ trên ba bộ dữ liệu chứng minh hiệu quả của nó.
