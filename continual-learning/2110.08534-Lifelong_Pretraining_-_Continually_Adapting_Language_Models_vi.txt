# Tiền Huấn Luyện Suốt Đời: Liên Tục Thích Ứng Mô Hình Ngôn Ngữ với Các Kho Văn Bản Mới Nổi

Xisen Jin¹, Dejiao Zhang², Henghui Zhu², Wei Xiao²,
Shang-Wen Li², Xiaokai Wei², Andrew Arnold², Xiang Ren¹
¹Đại học Nam California, ²AWS AI Labs
{xisenjin,xiangren}@usc.edu
{dejiaoz, henghui, weixiaow, shangwenl, xiaokaiw, anarnld}@amazon.com

## Tóm tắt

Các mô hình ngôn ngữ tiền huấn luyện (PTLM) thường được học trên một kho văn bản lớn, tĩnh và sau đó được tinh chỉnh thêm cho các tác vụ hạ nguồn khác nhau. Tuy nhiên, khi triển khai trong thế giới thực, một mô hình dựa trên PTLM phải đối phó với các phân phối dữ liệu khác biệt so với những gì PTLM ban đầu được huấn luyện. Trong bài báo này, chúng tôi nghiên cứu thách thức tiền huấn luyện mô hình ngôn ngữ suốt đời, nơi một PTLM được cập nhật liên tục để thích ứng với dữ liệu mới nổi. Trên một luồng bài báo nghiên cứu theo từng lĩnh vực và một luồng tweet được sắp xếp theo thời gian, chúng tôi tiền huấn luyện tăng dần một PTLM với các thuật toán học liên tục khác nhau, và theo dõi hiệu suất tác vụ hạ nguồn (sau tinh chỉnh). Chúng tôi đánh giá khả năng thích ứng của PTLM với các kho văn bản mới trong khi vẫn giữ lại kiến thức đã học trong các kho văn bản trước đó. Các thí nghiệm của chúng tôi cho thấy các phương pháp dựa trên chưng cất là hiệu quả nhất trong việc duy trì hiệu suất hạ nguồn trong các lĩnh vực trước đó. Các thuật toán này cũng cải thiện việc chuyển giao kiến thức, cho phép các mô hình đạt được hiệu suất hạ nguồn tốt hơn trên dữ liệu mới nhất, và cải thiện khả năng tổng quát hóa thời gian khi tồn tại khoảng cách phân phối giữa huấn luyện và đánh giá do thời gian. Chúng tôi tin rằng cách thức phát biểu vấn đề, phương pháp và phân tích của chúng tôi sẽ truyền cảm hứng cho các nghiên cứu tương lai hướng tới tiền huấn luyện liên tục của mô hình ngôn ngữ.

## 1 Giới thiệu

Các mô hình ngôn ngữ tiền huấn luyện (PTLM) đã đạt được hiệu suất đáng chú ý trên các tập dữ liệu chuẩn cho nhiều tác vụ NLP (Liu et al., 2019b; Brown et al., 2020). Tuy nhiên, khi triển khai trong thực tế, các hệ thống NLP phải đối phó với dữ liệu mới nổi có phân phối dữ liệu liên tục thay đổi, khác biệt so với các kho văn bản mà chúng được tiền huấn luyện ban đầu — ví dụ, khi các lĩnh vực dữ liệu mới được giới thiệu (phần trên của Hình 1) (Gururangan et al., 2020), hoặc khi ngôn ngữ sử dụng và từ vựng thay đổi theo thời gian (phần dưới của Hình 1) (Lazaridou et al., 2021). Tinh chỉnh từ một PTLM tĩnh và có thể "lỗi thời" có thể hạn chế hiệu suất mô hình trên các tác vụ hạ nguồn, vì PTLM có thể không còn cung cấp một khởi tạo mô hình hiệu quả (Beltagy et al., 2019; Müller et al., 2020). Ở đây chúng tôi tìm hiểu liệu việc liên tục thích ứng một PTLM với dữ liệu mới nổi có thể mang lại lợi ích trên các tác vụ hạ nguồn khác nhau, và cách đạt được hiệu suất hạ nguồn tốt hơn cho việc thích ứng PTLM suốt đời như vậy.

Một số nghiên cứu gần đây đã cố gắng thích ứng PTLM cho một lĩnh vực dữ liệu mới. Gururangan et al. (2020); Yao et al. (2021) thích ứng mô hình ngôn ngữ với các kho văn bản thuộc các thể loại và chủ đề khác nhau và quan sát thấy cải thiện hiệu suất trong các tác vụ hạ nguồn theo lĩnh vực. Arumae et al. (2020) tiếp tục cho thấy rằng bằng cách điều chỉnh các tham số của PTLM, hiệu suất tác vụ hạ nguồn trên lĩnh vực tổng quát có thể được bảo toàn. Một hướng nghiên cứu khác tập trung vào sự dịch chuyển lĩnh vực thời gian (Hombaiah et al., 2021), phân tích ảnh hưởng của tiền huấn luyện trên dữ liệu cập nhật đến các tác vụ hạ nguồn. Röttger và Pierrehumbert (2021) tiếp tục nghiên cứu các phương pháp kết hợp từ vựng để cải thiện thích ứng với các kho văn bản cập nhật. Tuy nhiên, các nghiên cứu này tập trung vào việc thích ứng PTLM cho một lĩnh vực mới duy nhất; trong khi trên thực tế, các kho văn bản từ các lĩnh vực riêng biệt và dấu thời gian có thể xuất hiện tuần tự. Liệu có thể duy trì một PTLM duy nhất, cập nhật vẫn là một vấn đề mở. Liên quan đến điều này, Lazaridou et al. (2021) nghiên cứu việc thích ứng PTLM trên các luồng dữ liệu thời gian, nhưng chỉ tập trung vào mô hình hóa ngôn ngữ thay vì hiệu suất tinh chỉnh. Cũng quan trọng là hiểu nhiều khía cạnh của tính hữu ích của tiền huấn luyện PTLM suốt đời, chẳng hạn như việc giữ lại kiến thức trên tất cả dữ liệu đã thấy, và nghiên cứu những phương pháp nào có thể cải thiện tính hữu ích của PTLM trong quá trình tiền huấn luyện liên tục như vậy.

Trong bài báo này, chúng tôi hình thức hóa tác vụ Tiền Huấn Luyện Mô Hình Ngôn Ngữ Suốt Đời để mô phỏng các kịch bản thực tế của việc duy trì và thích ứng một PTLM trên các kho văn bản mới nổi, tạo ra một bàn thử nghiệm (cùng với các luồng dữ liệu tiền huấn luyện và tác vụ hạ nguồn) để nghiên cứu các thuật toán tiền huấn luyện liên tục, và trình bày một giao thức đánh giá có hệ thống để đo lường tiến bộ đạt được trên vấn đề thách thức này (xem Hình 2 để minh họa). Chúng tôi xem xét hai loại chuỗi kho văn bản khi xây dựng các luồng dữ liệu tiền huấn luyện, mỗi loại mô phỏng một trường hợp sử dụng đại diện và có trọng tâm hơi khác nhau trong đánh giá: học liên tục một mô hình duy nhất áp dụng được cho cả lĩnh vực cũ và mới; và cải thiện khả năng xử lý dữ liệu mới nhất của mô hình. Cụ thể, chúng tôi xây dựng 1) một luồng văn bản tăng dần theo lĩnh vực bao gồm các bài báo học thuật được xuất bản trong bốn lĩnh vực nghiên cứu, và 2) một luồng tweet thời gian bao gồm các tweet được thu thập từ bốn năm khác nhau. Bằng cách tiến hành các thí nghiệm có hệ thống trên hai luồng dữ liệu này, chúng tôi tìm cách trả lời một loạt câu hỏi phân tích: 1) liệu tiền huấn luyện liên tục có giữ lại hiệu suất tinh chỉnh trên các kho văn bản trước đó so với tiền huấn luyện ngoại tuyến truyền thống không, 2) liệu tiền huấn luyện có cải thiện hiệu suất hạ nguồn trên dữ liệu mới nhất không, và 3) liệu tiền huấn luyện có cải thiện khả năng tổng quát hóa thời gian khi huấn luyện và đánh giá có khoảng cách phân phối do thời gian không.

Để giải quyết các câu hỏi nghiên cứu ở trên, chúng tôi tiến hành đánh giá có hệ thống các thuật toán học liên tục (CL) hiện có, bao gồm các phương pháp dựa trên mở rộng mô hình, dựa trên bộ nhớ và dựa trên chưng cất. Kết quả của chúng tôi cho thấy các phương pháp dựa trên chưng cất hiệu quả nhất trong việc giữ lại kiến thức trong luồng bài báo nghiên cứu, đồng thời cải thiện thích ứng với dữ liệu mới nhất và khả năng tổng quát hóa thời gian trong luồng tweet. Chúng tôi tin rằng cách thức phát biểu vấn đề, thiết lập đánh giá, phương pháp và phân tích của chúng tôi có thể truyền cảm hứng cho nhiều nghiên cứu tương lai hơn về tiền huấn luyện liên tục của mô hình ngôn ngữ.

## 2 Phát biểu Vấn đề

Ở đây chúng tôi trình bày cách thức phát biểu vấn đề cho tiền huấn luyện suốt đời của PTLM, cung cấp chi tiết về quá trình xây dựng luồng dữ liệu và các tác vụ hạ nguồn, và giới thiệu giao thức đánh giá.

### 2.1 Tiền Huấn Luyện Suốt Đời của PTLM

Chúng tôi xem xét kịch bản nơi cần triển khai và/hoặc duy trì các mô hình NLP trên một chuỗi T lĩnh vực dữ liệu. Tại mỗi bước thời gian t, mô hình thăm một kho văn bản không nhãn Dt từ một lĩnh vực với phân phối dữ liệu P(Dt). Phân phối dữ liệu P(Dt) tiến hóa theo bước thời gian t, tạo thành một luồng dữ liệu D1::T = {D1; D2; :::DT}. Trong thực tế, sự dịch chuyển lĩnh vực dữ liệu có thể đề cập đến thay đổi chủ đề của nội dung văn bản (từ bài báo nghiên cứu khoa học máy tính sang bài báo y sinh), hoặc tiến hóa thời gian của văn bản (từ tweet quá khứ đến gần đây). Tác vụ tiền huấn luyện suốt đời của PTLM tìm cách liên tục thích ứng một mô hình ngôn ngữ f khi mô hình thăm kho văn bản (không nhãn) Dt từ luồng dữ liệu D1::T, để cung cấp một khởi tạo mô hình tốt cho việc tinh chỉnh trên các tác vụ hạ nguồn từ cùng lĩnh vực. Với việc lạm dụng ký hiệu một chút, chúng tôi cũng sử dụng Dt để trực tiếp đề cập đến một lĩnh vực dữ liệu.

Ở đây, chúng tôi giả định một mô hình ngôn ngữ f được cập nhật tuần tự trên mỗi kho văn bản tiền huấn luyện Dt, mà không truy cập các kho văn bản trước đó đầy đủ {Di}i<t trong luồng dữ liệu D1::T. Điều này nhằm nắm bắt các ràng buộc thực tế như hạn chế quyền riêng tư để lưu trữ dữ liệu trước đó, hoặc ngân sách tính toán để huấn luyện trên tất cả các kho văn bản trong D1::T. Chúng tôi sử dụng ft để biểu thị mô hình ngôn ngữ ngay sau khi cập nhật trên lĩnh vực Dt. Trong nghiên cứu của chúng tôi, f là một transformer RoBERTa-base (Liu et al., 2019b) và mô hình (f0) được khởi tạo với các trọng số RoBERTa tiền huấn luyện.

Tính hữu ích của các PTLM {ft} được đánh giá dựa trên hiệu suất mô hình tinh chỉnh của chúng trên các tác vụ hạ nguồn khác nhau. Sau khi cập nhật trên một lĩnh vực Di, mô hình fi có thể được tinh chỉnh trên các tác vụ hạ nguồn từ các lĩnh vực đã thăm Dt trong đó t ≤ i. Chúng tôi ghi chú tập hợp các tác vụ hạ nguồn liên quan đến lĩnh vực Dt là St = {Sj_t}^Nt_j=1, giả định số lượng tác vụ hạ nguồn là Nt. Lưu ý rằng trong giai đoạn tinh chỉnh, mô hình ft không có quyền truy cập vào bất kỳ kho văn bản tiền huấn luyện nào D1::T.

### 2.2 Luồng Dữ liệu & Tập Dữ liệu Hạ nguồn

Chúng tôi xây dựng các luồng dữ liệu để mô phỏng hai kịch bản đại diện của sự dịch chuyển lĩnh vực dữ liệu trong thực tế (cũng xem Hình 1): một luồng tăng dần theo lĩnh vực để mô phỏng các thay đổi tuần tự của các khu vực bài báo nghiên cứu; và một luồng được sắp xếp theo thời gian để mô phỏng tweet xuất hiện theo thời gian.

**Luồng Bài báo Tăng dần theo Lĩnh vực.** Luồng bài báo này bao gồm toàn văn của các bài báo nghiên cứu được xuất bản trong bốn lĩnh vực nghiên cứu: y sinh, khoa học máy tính, khoa học vật liệu và vật lý, được lọc từ tập dữ liệu S2ORC¹, được trình bày tuần tự cho mô hình. Đối với mỗi lĩnh vực, chúng tôi đánh giá hiệu suất hạ nguồn trên hai tập dữ liệu. Các tác vụ hạ nguồn trải rộng trên nhiều tác vụ như trích xuất quan hệ và nhận dạng thực thể có tên, và được tóm tắt trong Bảng 1. Chúng tôi mô tả chi tiết các tập dữ liệu này trong Phụ lục D.

**Luồng Tweet Được Sắp xếp Theo Thời gian.** Luồng dữ liệu tweet này bao gồm các tweet từ năm 2014, 2016, 2018 và 2020, được thu thập bởi Archive Team² và tiền xử lý theo Nguyen et al. (2020). Bốn kho văn bản tweet này được trình bày tuần tự cho mô hình ngôn ngữ theo thứ tự thời gian của năm tweet. Đối với các tác vụ hạ nguồn, chúng tôi giữ lại 1M tweet từ kho văn bản mỗi năm để xây dựng các tập dữ liệu dự đoán hashtag đa nhãn (Gong và Zhang, 2016) và các tập dữ liệu dự đoán emoji đơn nhãn (Barbieri et al., 2018). Trên hai tập dữ liệu, chúng tôi báo cáo điểm độ chính xác trung bình xếp hạng nhãn (một phiên bản đa nhãn của MRR) của các mô hình (Azeemi và Waheed, 2021) và Macro-F1 tương ứng. Quá trình xây dựng tập dữ liệu chi tiết được bao gồm trong Phụ lục D.

### 2.3 Giao thức Đánh giá

Chúng tôi xem xét ba khía cạnh chính để đánh giá tính hữu ích của các mô hình ngôn ngữ {ft} được cập nhật liên tục trên luồng dữ liệu D1::T, cũng được minh họa trong Hình 2: 1) giữ lại và chuyển giao kiến thức trên các kho văn bản tiền huấn luyện đã thấy trước đó; 2) thích ứng với lĩnh vực dữ liệu mới nhất, và 3) khả năng tổng quát hóa thời gian khi dữ liệu huấn luyện và đánh giá từ các bước thời gian khác nhau.

**Giữ lại Kiến thức.** Một tính hữu ích chính của tiền huấn luyện mô hình ngôn ngữ liên tục là có được một mô hình duy nhất áp dụng được cho tất cả các lĩnh vực. Chúng tôi tập trung vào đánh giá khả năng này với luồng bài báo tăng dần theo lĩnh vực, bởi vì đối với luồng tweet, nhu cầu thực tế về hiệu suất trên dữ liệu lỗi thời là hạn chế. Việc giữ lại kiến thức được đo bằng hiệu suất tác vụ hạ nguồn từ các lĩnh vực trước đó hoặc hiện tại mà mô hình tiền huấn luyện đã thăm. Một cách chính thức hơn, đối với mỗi điểm kiểm tra mô hình tiền huấn luyện trong {fi}, chúng tôi tinh chỉnh fi trên các tác vụ hạ nguồn {St} trong đó t ≤ i và đánh giá hiệu suất tập kiểm tra tương ứng. Quan trọng là các mô hình không bị quên lãng thảm khốc (Robins, 1995), tức là giảm đáng kể tính hữu ích khi fi được tinh chỉnh cho các tác vụ hạ nguồn St từ các lĩnh vực trước đó với t < i.

**Thích ứng với Lĩnh vực Dữ liệu Mới nhất.** Trong một số kịch bản nhất định, hiệu suất của các mô hình hạ nguồn trên lĩnh vực dữ liệu mới nhất nên được nhấn mạnh. Ví dụ, các bộ phân loại trong lĩnh vực tweet thường được huấn luyện và đánh giá với dữ liệu cập nhật cho việc triển khai thực tế. Một cách chính thức, chúng tôi tập trung vào hiệu suất tác vụ hạ nguồn của các mô hình được tinh chỉnh từ điểm kiểm tra mô hình tiền huấn luyện cuối cùng fT, trong đó các tác vụ hạ nguồn ST cũng từ lĩnh vực mới nhất. Để thành công trong các chỉ số này, quan trọng là mô hình phải chuyển giao kiến thức từ các lĩnh vực trước đó sang lĩnh vực mới nhất.

**Khả năng Tổng quát hóa Thời gian.** Chúng tôi xem xét một kịch bản tinh chỉnh thực tế khác trong luồng tweet nơi mô hình được huấn luyện trên dữ liệu lỗi thời và đánh giá trên dữ liệu mới nhất (Rijhwani và Preotiuc-Pietro, 2020; Huang và Paul, 2018), được gọi là khả năng tổng quát hóa thời gian. Một cách chính thức, chúng tôi tinh chỉnh điểm kiểm tra mô hình tiền huấn luyện cuối cùng fT trên tập huấn luyện của các tác vụ hạ nguồn St từ một bước thời gian trước đó (t < T), và đánh giá trên tập kiểm tra của các tác vụ hạ nguồn ST từ bước thời gian mới nhất T.

## 3 Phương pháp

Tiền huấn luyện mô hình ngôn ngữ suốt đời đưa ra những thách thức mới bởi vì các tập huấn luyện lớn và các giao thức đánh giá toàn diện hơn so với các tác vụ phân loại. Chúng tôi thiết lập một số đường cơ sở mạnh, và đánh giá hiệu suất của các thuật toán học liên tục từ các danh mục khác nhau trải rộng trên các phương pháp mở rộng mô hình, dựa trên bộ nhớ và dựa trên chưng cất. Chúng tôi minh họa các phương pháp trong Hình 3.

### 3.1 Đường Cơ sở Đơn giản

Chúng tôi xem xét một số đường cơ sở đơn giản mà các thuật toán học liên tục sẽ được so sánh.

**RoBERTa-base (f0)** tương ứng với việc không tiền huấn luyện trên bất kỳ kho văn bản theo lĩnh vực nào. Bằng cách tiền huấn luyện riêng biệt f0 trên mỗi kho văn bản D1; D2; :::DT, chúng tôi có được T mô hình tiền huấn luyện **Theo Tác vụ Cụ thể**. Chúng tôi cũng tiền huấn luyện f0 tuần tự trên D1::T, mà chúng tôi gọi là **tiền huấn luyện tuần tự**. Mặc dù nó cho phép chuyển giao kiến thức giữa các lĩnh vực so với các mô hình theo lĩnh vực, mà không có bất kỳ thuật toán học liên tục nào, tiền huấn luyện tuần tự dễ bị quên lãng thảm khốc (Robins, 1995). Cuối cùng, chúng tôi xáo trộn ngẫu nhiên các kho văn bản từ tất cả các lĩnh vực D1::T trước khi tiền huấn luyện, được ghi chú là **Học Đa Tác vụ (MTL)**. MTL tương ứng với một mô hình huấn luyện ngoại tuyến mô hình hóa các kho văn bản mới bằng cách huấn luyện lại trên tất cả các kho văn bản đã thấy trước đó. Nhược điểm là nó yêu cầu lưu trữ dữ liệu đầy đủ từ các lĩnh vực trước đó, và có thể cực kỳ tốn kém để huấn luyện lại lặp đi lặp lại trên dữ liệu trước đó nếu dữ liệu mới tiếp tục xuất hiện.

### 3.2 Phương pháp Dựa trên Mở rộng Mô hình và Điều chỉnh

Chúng tôi đầu tiên giới thiệu các phương pháp dựa trên mở rộng mô hình, thêm các mô-đun có thể huấn luyện nhỏ (ví dụ, perceptron đa lớp) vào mô hình cho mỗi lĩnh vực mới trong khi giữ các phần khác của mô hình cố định.

Phương pháp **Adapter** là một phương pháp đại diện học một tập hợp các lớp "adapter" gt = {gk_t}^K_k=1 cho mỗi lĩnh vực Dt và mỗi trong số K lớp transformer (Houlsby et al., 2019). Chúng tôi cũng thử nghiệm với một phương pháp **Mở rộng Lớp** đơn giản, học riêng biệt hai lớp trên cùng của transformer và đầu dự đoán cho mỗi lĩnh vực. Chúng tôi cũng bao gồm một đường cơ sở học liên tục dựa trên điều chỉnh, **EWC trực tuyến** (Schwarz et al., 2018), trực tiếp phạt thay đổi các tham số mô hình.

### 3.3 Phương pháp Tái hiện Bộ nhớ

Chúng tôi cũng thử nghiệm với **Tái hiện Kinh nghiệm (ER)** (Chaudhry et al., 2019), giảm thiểu việc quên bằng cách lưu trữ một tập con của các ví dụ trước đó và định kỳ huấn luyện lại (tái hiện) trên chúng. Chúng tôi duy trì một bộ nhớ có kích thước cố định M (100k ví dụ theo mặc định) và điền bộ nhớ M mỗi khi tiền huấn luyện trên một lĩnh vực Dt kết thúc với các ví dụ trong lĩnh vực hiện tại. Chúng tôi đảm bảo M luôn chứa một mẫu cân bằng của các ví dụ từ tất cả các lĩnh vực đã thấy D1::t. Chúng tôi tái hiện một mini-batch của các ví dụ từ bộ nhớ mỗi 10 bước huấn luyện.

### 3.4 Phương pháp CL Dựa trên Chưng cất

Mặc dù các kỹ thuật chưng cất kiến thức (KD) (Hinton et al., 2015) đã được nghiên cứu sâu rộng cho các mô hình ngôn ngữ tiền huấn luyện (Sun et al., 2019), việc áp dụng chúng cho học liên tục đã được khám phá ít ngoài các tác vụ phân loại hình ảnh (Li và Hoiem, 2018; Rebuffi et al., 2017; Hou et al., 2018). Các phương pháp CL dựa trên chưng cất lưu trữ một điểm kiểm tra mô hình trước đó của mô hình (được ghi chú là ft-1) và điều chỉnh sự khác biệt giữa ft-1 và mô hình hiện tại ft. Chúng tôi thích ứng một số kỹ thuật chưng cất kiến thức hiện có cho PTLM và sử dụng chúng cho học liên tục. Chúng tôi lưu ý, mặc dù các kỹ thuật chưng cất riêng lẻ không phải nguyên bản, việc thích ứng chúng cho các thuật toán CL có thể mới.

Chúng tôi thực hiện chưng cất với các ví dụ từ lĩnh vực hiện tại Dt và một bộ nhớ tái hiện M (tương tự như ER). Mặc dù có khoảng cách tiềm năng giữa Dt và dữ liệu huấn luyện của ft-1, phương pháp này cho phép sử dụng nhiều dữ liệu hơn cho chưng cất. Một cách chính thức, mỗi khi mô hình nhận một mini-batch của các ví dụ luồng xs hoặc rút một mini-batch của các ví dụ bộ nhớ xm từ M (cả hai được ghi chú là x), chúng tôi thu thập một số đầu ra nhất định của mô hình (ví dụ, logit đầu ra hoặc biểu diễn trung gian) với ft-1 và ft. Chúng tôi tính toán một mất mát chưng cất ℓKD(x; ft-1; ft) phạt sự khác biệt giữa các đầu ra mô hình, và tối ưu hóa chung nó với mất mát mô hình hóa ngôn ngữ có mặt nạ ℓMLM. Mục tiêu cuối cùng được viết là ℓ = ℓMLM + λℓKD, trong đó λ là một siêu tham số để cân bằng mất mát chưng cất.

**Chưng cất Logit.** Trong chưng cất logit (Hinton et al., 2015), chúng tôi thu thập các logit đầu ra của ft và ft-1, được ghi chú là yt và yt-1 tương ứng. Mất mát chưng cất được tính toán là DKL(yt; yt-1), trong đó DKL là hàm phân kỳ Kullback–Leibler.

**Chưng cất Biểu diễn.** Chúng tôi cũng xem xét việc tối thiểu hóa độ lệch biểu diễn của các câu giữa các mô hình trước đó và hiện tại (Sun et al., 2019; Jiao et al., 2020). Chúng tôi trích xuất biểu diễn của mỗi từ của hai mô hình, được ghi chú là h^(1:N)_(t-1) và h^(1:N)_t, trước đầu dự đoán mô hình hóa ngôn ngữ có mặt nạ, trong đó N là độ dài của câu. Sau đó, chúng tôi tính toán mất mát MSE ||h^(1:N)_(t-1) - h^(1:N)_t||²_2 làm mất mát chưng cất.

**Chưng cất Tương phản.** Ngoài logit đầu ra và biểu diễn ẩn, chúng tôi tiếp tục xem xét độ tương tự biểu diễn trong một batch của các ví dụ làm kiến thức bổ sung để chưng cất. Phương pháp này được thích ứng từ (Cha et al., 2021), ban đầu được nghiên cứu cho các tác vụ phân loại hình ảnh có giám sát. Chúng tôi giới thiệu ngắn gọn thuật toán đã thích ứng và để lại chi tiết trong Phụ lục E. Trong quá trình tiền huấn luyện liên tục, ngoài mục tiêu tiền huấn luyện mô hình ngôn ngữ, chúng tôi thêm một mục tiêu học tương phản không giám sát, cụ thể là mục tiêu SimCSE (Gao et al., 2021) để khuyến khích biểu diễn câu phản ánh độ tương tự ngữ nghĩa giữa các câu. Sau đó, chúng tôi tính toán các ma trận độ tương tự biểu diễn nội batch của biểu diễn câu (tức là giữa mỗi cặp ví dụ trong mini-batch) với ft-1 và ft, được ghi chú là Bt-1 và Bt, và tối thiểu hóa mất mát entropy chéo ℓdistill = (1/N) ∑^N_(i=1) ∑^N_(j=1) B^(ij)_(t-1) log B^(ij)_t.

**Chưng cất Tự Giám sát (SEED).** Chưng cất SEED được đề xuất bởi (Fang et al., 2021) có tinh thần tương tự như chưng cất tương phản. Sự khác biệt duy nhất là nó chưng cất độ tương tự biểu diễn giữa batch và một tập lớn các ví dụ khác. Chúng tôi để lại chi tiết của thuật toán trong Phụ lục E. Chúng tôi tiếp tục kết hợp **Chưng cất SEED** với chưng cất logit và gọi phương pháp này là **Chưng cất SEED-Logit**.

## 4 Kết quả

Chúng tôi tóm tắt các phát hiện của mình trên các luồng dữ liệu đã tạo. Chúng tôi hỏi liệu tiền huấn luyện suốt đời và các thuật toán học liên tục có hiệu quả dựa trên giao thức đánh giá mà chúng tôi đề xuất trong Mục 2.3.

### 4.1 Thiết lập Thí nghiệm

Chúng tôi sử dụng mô hình RoBERTa-base (Liu et al., 2019b), được khởi tạo với các trọng số RoBERTa-base trong suốt các thí nghiệm. Chúng tôi đặt độ dài chuỗi tối đa là 128 và kích thước batch huấn luyện hiệu quả là 2,048. Trên luồng bài báo nghiên cứu, các mô hình được huấn luyện trong 8k bước trong lĩnh vực đầu tiên và 4k bước trong các lĩnh vực tiếp theo. Trên luồng Tweet, chúng tôi huấn luyện các mô hình trong 4k bước trong mỗi lĩnh vực. Điều này tương ứng với ít hơn một lần qua dữ liệu trong mỗi lĩnh vực. Xem Phụ lục A để biết thiết lập chi tiết.

### 4.2 Luồng Dữ liệu Tăng dần theo Lĩnh vực

Như chúng tôi đã giới thiệu trong Mục 2.2, trong luồng bài báo nghiên cứu tăng dần theo lĩnh vực, chúng tôi mong đợi một mô hình ft hoạt động tốt trên tất cả các tác vụ hạ nguồn S1::t từ các lĩnh vực D1::t. Trong Bảng 2, chúng tôi báo cáo hiệu suất của các mô hình trên tất cả các tác vụ hạ nguồn S1::T được tinh chỉnh từ điểm kiểm tra tiền huấn luyện cuối cùng, fT. Chúng tôi hình dung sự thay đổi hoàn chỉnh hơn của hiệu suất tác vụ hạ nguồn trên các bước thời gian khác nhau của tiền huấn luyện (tức là, f1; f2; f3; f4) trong Hình 4. Chúng tôi cũng báo cáo log perplexity của mô hình hóa ngôn ngữ có mặt nạ (MLM) trong Bảng 2 như thông tin bổ sung.

**Liệu tiền huấn luyện suốt đời có giúp giữ lại kiến thức trên các kho văn bản lĩnh vực khác nhau?** Chúng tôi đầu tiên kiểm tra liệu tiền huấn luyện theo tác vụ cụ thể hoặc suốt đời có cải thiện hiệu suất trên các tác vụ hạ nguồn theo lĩnh vực cụ thể. So sánh Các LM Theo Tác vụ Cụ thể với RoBERTa-base trong Bảng 2, chúng tôi nhận thấy cải thiện hiệu suất nhất quán, đặc biệt trên các lĩnh vực Y sinh và Khoa học Máy tính (D1, D2). Chúng tôi cũng thấy Tiền huấn luyện Tuần tự có thể vượt trội RoBERTa-base một cách nhất quán. Tuy nhiên, so sánh giữa Tiền huấn luyện Tuần tự và Các LM Theo Tác vụ Cụ thể có kết quả hỗn hợp: trên D1, D2, D3, Tiền huấn luyện Tuần tự có thể vượt trội Các LM Theo Tác vụ Cụ thể chỉ trừ MNER; trong khi trên lĩnh vực y sinh sớm nhất (D1), Tiền huấn luyện Tuần tự đạt hiệu suất thấp hơn đáng kể. Từ Hình 4, chúng tôi thấy hiệu suất của Tiền huấn luyện Tuần tự trên Chemprot và RCT (từ D1) giảm đáng kể từ t = 1 đến 4. Kết quả ngụ ý tiền huấn luyện suốt đời cho phép các lĩnh vực sau này hưởng lợi từ việc chuyển giao kiến thức từ các lĩnh vực trước đó, nhưng hiệu suất trên các lĩnh vực trước đó bị hạn chế vì việc quên.

**Liệu các thuật toán học liên tục có giúp giữ lại kiến thức trong tiền huấn luyện tuần tự?** Tiếp theo, chúng tôi so sánh các loại thuật toán CL khác nhau và điều tra tác động của các thuật toán CL trong việc giảm thiểu quên lãng và cải thiện chuyển giao kiến thức. Bảng 2 cho thấy Online-EWC cải thiện một chút perplexity MLM so với Sequential PT, nhưng không mang lại cải thiện cho hiệu suất tinh chỉnh. Chúng tôi giả thuyết rằng điều chีnh trực tiếp trong không gian tham số như trong Online-EWC không hiệu quả khi không gian tham số có chiều rất cao. Adapter cải thiện điểm F1 tác vụ hạ nguồn trên lĩnh vực y-sinh (D1) 1.2% và 0.8%, nhưng không vượt trội Sequential Pretraining trong các lĩnh vực khác (tương tự cho phương pháp Simple Layer Expansion), có thể do phần lớn mô hình được giữ cố định.

Ngược lại, phương pháp dựa trên tái hiện bộ nhớ (ER) cho phép huấn luyện các tham số đầy đủ của mô hình và đã được chứng minh là rất hiệu quả trong học liên tục của các tác vụ phân loại (Wang et al., 2019; Chaudhry et al., 2019). Tuy nhiên, chúng tôi ngạc nhiên khi phát hiện rằng ER khó có thể cải thiện hơn Sequential Pretraining trừ D1. Một mô hình tương tự có thể được tìm thấy trong perplexity MLM. Chúng tôi giả thuyết rằng tác động tích cực của việc tái hiện ví dụ đã giảm vì việc quá khớp với các ví dụ bộ nhớ. Bảng 3 tóm tắt tác động của việc điều chỉnh siêu tham số trong ER. Khi chúng tôi giảm tần suất tái hiện (từ mỗi 10 bước xuống 100 bước), hiệu suất MLM cải thiện, điều này ngụ ý giảm quá khớp; tuy nhiên, hiệu suất tác vụ hạ nguồn không cải thiện. Khi chúng tôi tăng kích thước bộ nhớ |M| từ 100k lên 10M, perplexity MLM cũng cải thiện; vẫn vậy, vẫn không có cải thiện trong các tác vụ hạ nguồn. Điều này có thể ngụ ý ER bản thân không phải là một phương pháp hiệu quả cho tiền huấn luyện liên tục.

Không giống như ER, các phương pháp chưng cất sử dụng thông tin phong phú hơn như logit đầu ra hoặc độ tương tự biểu diễn để bảo tồn kiến thức quá khứ. Chúng tôi thấy Logit KD hoặc SEED-Logit KD là hiệu quả nhất tùy thuộc vào tác vụ, trong khi Rep-KD và Contrastive-KD ít hiệu quả hơn. Phương pháp chưng cất hoạt động tốt nhất cải thiện F1 so với Sequential Pretraining trên các tác vụ hạ nguồn từ D1, D2 ít nhất 1.0%. Tuy nhiên, hiệu suất trên D3, D4, xuất hiện sau trong luồng dữ liệu, không cải thiện so với Sequential Pretraining, có thể do số hạng mất mát chưng cất làm cho mô hình cứng nhắc trong việc thu nhận kiến thức mới.

**Khoảng cách giữa tiền huấn luyện suốt đời và học đa tác vụ trên tất cả các lĩnh vực là gì?** Học Đa Tác vụ đề cập đến mô hình huấn luyện ngoại tuyến, huấn luyện lại PTLM trên tất cả các kho văn bản (D1::t) mỗi khi một kho văn bản mới Dt trở nên có sẵn. Chúng tôi kiểm tra liệu tiền huấn luyện suốt đời có thể so sánh được với tiền huấn luyện đa tác vụ về hiệu suất. Từ Bảng 2 và Hình 4, chúng tôi thấy Sequential Pretraining nói chung kém hơn MTL trừ lĩnh vực cuối cùng. Tuy nhiên, một số phương pháp CL nhất định, như Logit-Distillation, có thể cải thiện hơn MTL trên tất cả các tác vụ hạ nguồn từ lĩnh vực thứ nhất và thứ hai. Chúng tôi suy đoán lý do là học liên tục tự nhiên cung cấp một chương trình giảng dạy (Xu et al., 2020; Shi et al., 2015) cho các mô hình nơi mỗi tác vụ riêng lẻ dễ học hơn. Kết quả có ý nghĩa tích cực rằng tiền huấn luyện suốt đời không chỉ hiệu quả hơn về mặt tính toán và yêu cầu ít lưu trữ dữ liệu quá khứ hơn, mà còn có thể cải thiện hiệu suất tiền huấn luyện.

**Liệu tiền huấn luyện suốt đời có làm cho các mô hình hiệu quả hơn về dữ liệu?** Trong Bảng 5, chúng tôi tiếp tục kiểm tra hiệu suất của các mô hình tiền huấn luyện cuối cùng dưới các lượng ví dụ huấn luyện khác nhau. Chúng tôi bao gồm kết quả đầy đủ trong Phụ lục B. Chúng tôi thấy nói chung, cải thiện hiệu suất đáng kể hơn trong thiết lập tài nguyên thấp.

**Chi phí Tính toán.** Chúng tôi định lượng chi phí tính toán của các thuật toán CL khác nhau bằng số lượng truyền tiến và lùi trong Bảng 4 và trình bày các thí nghiệm bổ sung với chi phí tính toán được kiểm soát trong Phụ lục F. Chúng tôi thấy chi phí tính toán bổ sung là cần thiết cho việc cải thiện hiệu suất của CL dựa trên chưng cất. Tuy nhiên, không thể đánh đổi hiệu suất đơn giản bằng cách đầu tư nhiều ngân sách tính toán hơn với các thuật toán CL tùy ý. Chúng tôi để lại thảo luận chi tiết trong Phụ lục F.

### 4.3 Luồng Dữ liệu Thời gian

Chúng tôi tiến hành phân tích về tiền huấn luyện PTLM trên các kho văn bản tweet được sắp xếp theo thời gian, để hiểu liệu tiền huấn luyện suốt đời có giúp thích ứng với dữ liệu mới nhất và cải thiện khả năng tổng quát hóa thời gian. Kết quả được tóm tắt trong Bảng 5.

**Liệu các LM sẽ lỗi thời?** Chúng tôi so sánh hiệu suất của Task-Specific (2014) với các mô hình Task-Specific được tiền huấn luyện vào năm của các tập dữ liệu hạ nguồn (được ghi chú là Task-Specific (Latest)) và nhận thấy cải thiện nhất quán trong các tác vụ hạ nguồn vào năm 2018 và 2020 (hai cột đầu tiên trong Bảng 5). Sequential Pretraining cũng có thể vượt trội mô hình Task-Specific (2014). Điều này xác minh rằng các mô hình ngôn ngữ có thể lỗi thời theo thời gian, nhưng vấn đề này có thể được giải quyết bằng tiền huấn luyện theo tác vụ cụ thể hoặc suốt đời trên các kho văn bản mới nhất.

**Liệu tiền huấn luyện suốt đời có giúp cải thiện hiệu suất của mô hình hạ nguồn trên dữ liệu mới nhất?** Chúng tôi cho thấy rằng hiệu suất của mô hình hạ nguồn trên dữ liệu sau này (D3, D4) có thể được cải thiện so với các mô hình Task-Specific khi áp dụng các thuật toán học liên tục. Từ hai cột đầu tiên của Bảng 5, chúng tôi thấy Logit-KD và SEED-KD cải thiện điểm dự đoán Hashtag trên dữ liệu của các năm 2018 và 2020. SEED-Logit KD tiếp tục cải thiện F1 dự đoán trên dự đoán Emoji. Lưu ý rằng những phát hiện này trái ngược với luồng bài báo nghiên cứu, nơi các thuật toán CL không cải thiện hiệu suất trong lĩnh vực mới nhất D4. Lý do có thể là độ tương tự cao hơn giữa các lĩnh vực trong các kho văn bản tweet làm cho việc chuyển giao kiến thức dễ dàng hơn, được thảo luận thêm trong Phụ lục I.

**Liệu tiền huấn luyện suốt đời có cải thiện khả năng tổng quát hóa thời gian?** Tổng quát hóa thời gian đánh giá hiệu suất hạ nguồn trên dữ liệu kiểm tra mới nhất khi được tinh chỉnh trên dữ liệu huấn luyện lỗi thời. Chúng tôi cho thấy tiền huấn luyện suốt đời mang lại cải thiện rõ ràng cho khả năng tổng quát hóa thời gian. Từ Bảng 5, chúng tôi thấy ngay cả Sequential Pretraining cũng có thể cải thiện so với mô hình được tiền huấn luyện chỉ trên dữ liệu năm 2020 (Task-Specific (2020)) một cách nhất quán. Chúng tôi thấy hiệu suất tiếp tục cải thiện với các thuật toán CL được áp dụng. SEED-Logit-KD hoạt động tốt nhất nói chung trên các tác vụ dự đoán hashtag qua năm. Trong dự đoán emoji qua năm, chúng tôi thấy Contrast-KD và SEED-KD hoạt động tốt nhất. Chúng tôi cũng thấy rằng SEED-Logit-KD có thể vượt trội Logit-KD một chút.

## 5 Các Nghiên cứu Liên quan

**Thích ứng Lĩnh vực và Thời gian của Mô hình Ngôn ngữ.** Gururangan et al. (2020) nghiên cứu thích ứng PTLM với các kho văn bản theo lĩnh vực cụ thể. Arumae et al. (2020) nghiên cứu các thuật toán để giảm thiểu quên lãng trong PTLM gốc, nhưng không điều tra việc quên lãng xảy ra trong một chuỗi các lĩnh vực. Maronikolakis và Schütze (2021); Röttger và Pierrehumbert (2021); Luu et al. (2021) đề xuất tiền huấn luyện tuần tự trên các lĩnh vực hoặc dữ liệu mới nổi, nhưng không điều tra các thuật toán CL. Một số nghiên cứu gần đây đã chứng minh sự cần thiết của việc thích ứng LM theo thời gian (Lazaridou et al., 2021) trong khi tập trung cụ thể vào kiến thức thực tế (Dhingra et al., 2021; Jang et al., 2021).

**Các Thuật toán Học Liên tục trong NLP.** Học liên tục trong NLP chủ yếu được nghiên cứu cho các tác vụ phân loại. Một phương pháp hiệu quả là sử dụng một số ví dụ quá khứ được lưu trữ (de Masson d'Autume et al., 2019; Wang et al., 2020), hoặc các ví dụ giả (ví dụ, những ví dụ được tạo ra với một PTLM (Sun et al., 2020; Kanwatchara et al., 2021)). Các mở rộng gần đây của thuật toán (Chuang et al., 2020) thực hiện chưng cất kiến thức với các ví dụ giả được tạo ra. Các hướng nghiên cứu khác tập trung vào điều chỉnh trên các biểu diễn câu (Wang et al., 2019; Huang et al., 2021; Liu et al., 2019a) hoặc trực tiếp hợp nhất các mô hình trong không gian tham số (Matena và Raffel, 2021). Các phương pháp dựa trên mở rộng mô hình (Liu et al., 2019a; Pfeiffer et al., 2021), bao gồm học các mô hình chuyên gia theo lĩnh vực cụ thể (Gururangan et al., 2021), cũng được nghiên cứu tích cực. Wu et al. (2022) trình bày một nghiên cứu so sánh các thuật toán trong bối cảnh tinh chỉnh liên tục trên các tác vụ NLP.

## 6 Kết luận

Trong bài báo này, chúng tôi đã hình thức hóa vấn đề tiền huấn luyện mô hình ngôn ngữ suốt đời và xây dựng hai luồng dữ liệu liên kết với các tập dữ liệu hạ nguồn. Chúng tôi đã đánh giá việc giữ lại kiến thức, thích ứng với dữ liệu mới nhất, và khả năng tổng quát hóa thời gian của các mô hình ngôn ngữ được tiền huấn luyện liên tục. Các thí nghiệm của chúng tôi cho thấy các phương pháp dựa trên chưng cất hiệu quả nhất trong các thiết lập đánh giá này.

Một hạn chế của công trình là nó chưa được giải quyết đầy đủ liệu có tồn tại một biến thể của phương pháp CL dựa trên chưng cất luôn vượt trội Logit-KD. Dựa trên quan sát hiện tại, chúng tôi kết luận rằng hiệu suất của các phương pháp KD khác nhau cho CL phụ thuộc rất nhiều vào tác vụ. Điều này đòi hỏi nhiều nghiên cứu tương lai hơn về các thuật toán học liên tục trong thiết lập vấn đề được đề xuất.
