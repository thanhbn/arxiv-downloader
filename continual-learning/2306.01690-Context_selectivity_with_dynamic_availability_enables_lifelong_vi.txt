Tính chọn lọc ngữ cảnh với khả năng động kích hoạt học tập liên tục suốt đời

Martin L.L.R. Barry1, Wulfram Gerstner1, Guillaume Bellec1,B
1Khoa Khoa học Sự sống, Khoa Khoa học Máy tính
École Polytechnique Fédérale de Lausanne (EPFL), Thụy Sĩ
Bguillaume.bellec@epfl.ch

Tóm tắt
"Bạn không bao giờ quên cách đi xe đạp", – nhưng điều đó có thể như thế nào? Não bộ có khả năng học các kỹ năng phức tạp, ngừng thực hành trong nhiều năm, học các kỹ năng khác ở giữa, và vẫn lấy lại được kiến thức ban đầu khi cần thiết. Các cơ chế của khả năng này, được gọi là học tập suốt đời (hoặc học tập liên tục, CL), là chưa được biết. Chúng tôi đề xuất một quy tắc meta-plasticity có tính khả thi sinh học dựa trên các công trình cổ điển trong CL mà chúng tôi tóm tắt thành hai nguyên tắc: (i) các neuron có tính chọn lọc ngữ cảnh, và (ii) một biến khả năng cục bộ đóng băng một phần tính dẻo nếu neuron có liên quan đến các nhiệm vụ trước đó. Trong một công thức hóa mới tập trung vào neuron của các nguyên tắc này, chúng tôi đề xuất rằng tính chọn lọc neuron và hợp nhất toàn neuron là một giả thuyết meta-plasticity đơn giản và khả thi để kích hoạt CL trong não. Trong mô phỏng, mô hình đơn giản này cân bằng quên lãng và hợp nhất dẫn đến học chuyển giao tốt hơn các thuật toán CL đương đại trên các tiêu chuẩn nhận dạng hình ảnh và xử lý ngôn ngữ tự nhiên CL.

Giới thiệu
Trong khi Mạng thần kinh nhân tạo (ANN) đã đạt được những tiến bộ ấn tượng trong các nhiệm vụ tính toán khác nhau, cách tiếp cận của chúng đối với học tập liên tục (CL) khác với não người. Các ANN truyền thống, bao gồm các kiến trúc như perceptron đa lớp [1], Bộ nhớ Ngắn hạn-Dài hạn [2], Mạng thần kinh tích chập [3], và Transformer [4], đối mặt với thách thức tích hợp kiến thức mới mà không vô tình làm gián đoạn thông tin trước đó—một hiện tượng được gọi là "quên lãng thảm khốc" [5–7] là một biểu hiện của bài toán ổn định-dẻo cổ điển trong khoa học thần kinh tính toán [8]. Tuy nhiên, hệ thống thần kinh con người có khả năng đáng chú ý để tiếp thu những trải nghiệm mới mà không hoàn toàn ghi đè lên những cái cũ, trong khi tổng quát hóa kiến thức trước đó cho các vấn đề mới (chuyển giao thuận) [9] và cập nhật kiến thức trước đó khi học một nhiệm vụ mới nhưng có liên quan (chuyển giao ngược). Sự khác biệt về hiệu suất giữa ANN và não người đã thu hút sự chú ý của cả các nhà khoa học thần kinh tính toán làm việc về lý thuyết học tập synaptic [10,11] cũng như các chuyên gia học máy quan tâm đến cảm hứng sinh học [5,6,12,13].

Các neuron vỏ não có tính chọn lọc đối với kích thích cảm giác [14,15] cũng như các khái niệm trừu tượng hơn [16,17]. Các quần thể neuron mã hóa các quy tắc hoặc ngữ cảnh trừu tượng [18,19], rất có thể được thực hiện bằng cách điều chế chức năng tăng ích của chúng [20–25]. Chúng tôi phỏng đoán rằng cơ chế mà con người đạt được học tập liên tục suốt đời có gốc rễ từ điều chế hoạt động neuron cụ thể theo ngữ cảnh [26] và metaplasticity [27] của các kết nối synaptic. Ở cấp độ nhận thức, giả thuyết của chúng tôi có thể được tóm tắt như sau: Khi chúng ta gặp một thách thức học tập mới (giả sử chúng ta học một ngôn ngữ nước ngoài mới) một điều chế chọn lọc ngữ cảnh của các neuron vỏ não dẫn đến chuyên môn hóa một phần của các neuron cho ngữ cảnh cụ thể; một khi các neuron chọn lọc học thông qua tính dẻo synaptic (chúng ta học ngôn ngữ này đủ tốt), tính dẻo của những neuron này sau đó được đóng băng một phần (ngôn ngữ mới sẽ không bao giờ bị quên) ngay cả khi nhiệm vụ học tập tiếp theo được gặp phải (một ngôn ngữ nước ngoài khác).

Để dẫn xuất một cơ chế chức năng và hợp lý cho CL, chúng tôi đã xác định hai nguyên tắc tóm tắt bản chất của các mô hình CL hiện có [6,10,13,28–35] (xem Thảo luận cho một đánh giá đầy đủ): (Nguyên tắc 1) Các neuron được gated để tạo ra tính chọn lọc ngữ cảnh, và (Nguyên tắc 2) các tham số có thể được đóng băng bằng Cản trở cập nhật tham số Neural để ngăn chặn việc ghi đè kiến thức quan trọng trước đó. Trong bài báo này, chúng tôi đề xuất một công thức hóa lại của Nguyên tắc 1 và 2 được gọi là "GateON" dựa trên hiểu biết toán học về CL. Mặc dù có sự đơn giản về mặt khái niệm rõ ràng của GateON, nó ít bị quên lãng hơn và đạt được học chuyển giao tốt hơn so với các thuật toán CL trước đây trên các tiêu chuẩn CL đã thiết lập như MNIST hoán vị [6,10], CIFAR-100 chia tách và các tiêu chuẩn xử lý ngôn ngữ tự nhiên (NLP) với mô hình BERT được đào tạo trước [36]. Trên hầu hết các chỉ số, GateON xếp hạng tốt hơn các thuật toán liên quan đến các cơ chế phức tạp hơn đáng kể như phát lại hoàn hảo các mẫu dữ liệu trước đó [13,28–33]. Ngoài ra, chúng tôi lập luận rằng GateON kích hoạt hai đóng góp độc đáo và mới:

Đầu tiên, biến khả năng không chỉ đóng băng các tham số có liên quan đến các nhiệm vụ trước đó, mà nó cũng có thể mở băng các tham số khi tài nguyên là cần thiết. Do đó, nó cân bằng tự động giữa quên lãng và hợp nhất. Mở băng các tham số được chứng minh là quyết định đối với các họ nhiệm vụ lớn có 100 nhiệm vụ.

Điều mới thứ hai là một hiểu biết tính toán mới về sinh học: chúng tôi mô tả một công thức hóa tập trung vào neuron của Nguyên tắc 1 và 2, được gọi là n-GateON. Thay vì đóng băng các trọng số synaptic riêng lẻ, tất cả các tham số neuron của một neuron đóng băng đồng thời. Chúng tôi đặc biệt cho thấy một xấp xỉ có nguyên tắc và hiệu quả của n-GateON nơi hoạt động neuron gần đây trực tiếp đóng băng các cập nhật tham số của neuron. Do đó giả thuyết metaplasticity đơn giản này không yêu cầu thông tin cụ thể theo nhiệm vụ không cục bộ như gradient mất mát. Minh chứng này là một nỗ lực để trưng bày các mô hình chức năng của CL đủ đơn giản để có thể bác bỏ trong khoa học thần kinh thực nghiệm. Cụ thể, cơ chế kết quả có thể giải thích một số đặc điểm của các neuron thực: tính chọn lọc thần kinh được điều chế bởi thông tin ngữ cảnh như quan sát rộng rãi trong não [20–26] được giải thích nhưng công thức hóa của Nguyên tắc 1 trong GateON; và Nguyên tắc 2 trong n-GateON công thức hóa giả thuyết rằng các biến động cụ thể theo neuron tích hợp lịch sử kích hoạt và kiểm soát khả năng cho các thay đổi dẻo trong tương lai liên kết với metaplasticity [27,37] và hợp nhất synaptic [38–41]. Ở cấp độ cao hơn, cho thấy tầm quan trọng cao của sự phối hợp của nguyên tắc 1 và 2 trong các mô phỏng của chúng tôi, chúng tôi suy đoán rằng tính chọn lọc neuron và hợp nhất toàn neuron có thể đã tiến hóa cùng nhau để kích hoạt CL trong não.

1 Kết quả
Kết quả của chúng tôi được trình bày theo thứ tự sau: Chúng tôi đầu tiên mô tả các nguyên tắc toán học mà chúng tôi đã xác định là nền tảng của CL hiệu quả (Phần 1.1). Sau đó chúng tôi dẫn xuất một triển khai khả thi sinh học của mô hình này minh họa cách tính chọn lọc thần kinh có thể là trụ cột của quy tắc metaplasticity cho CL (Phần 1.2). Các kết quả định lượng về hiệu suất của GateON trên các tiêu chuẩn học máy được báo cáo trong Phần 1.3.

1.1 Một lý thuyết chuẩn tắc cho học tập liên tục
Để nghiên cứu CL, chúng tôi tuân theo một mô hình đã thiết lập [6,10] và sử dụng một chuỗi K nhiệm vụ có giám sát T1, T2, ... TK, trong đó mỗi nhiệm vụ cũng được gọi là 'ngữ cảnh'. Hiệu suất của mạng trên nhiệm vụ Tk được định lượng bằng mất mát Lk được tính trung bình trên tất cả các điểm dữ liệu trong nhiệm vụ đó. Cùng một nhiệm vụ Tk được sử dụng trong thời gian không xác định trước khi chuyển đổi sang nhiệm vụ khác xảy ra. Một điểm thời gian đơn t tương ứng với việc trình bày một điểm dữ liệu đơn (hoặc một minibatch các điểm dữ liệu) tại lớp đầu vào của mạng, tiếp theo bởi xử lý mạng, và tính toán mất mát dựa trên đầu ra mạng. Mất mát tức thời cho một điểm dữ liệu đơn (hoặc một minibatch các điểm dữ liệu) là Lt.

Nguyên tắc 1: Tính chọn lọc ngữ cảnh có cổng. Để xác định mô hình tính chọn lọc ngữ cảnh trong các mô hình mạng thần kinh, chúng tôi giả định rằng mỗi đơn vị mạng được gated bởi một cổng sigmoid phụ thuộc vào ngữ cảnh. Chúng tôi mô tả ở đây trường hợp của mạng feedforward bao gồm L lớp với N neuron mỗi lớp (việc mở rộng cho Transformer được làm nổi bật trong phần 1.3 và cho Mạng thần kinh tích chập (CNN) trong Phương pháp 3.3). Hoạt động xl_i của neuron i trong lớp l≥1 được xác định bởi hàm tăng ích nhân [24,26] với yếu tố gating gl_i và hàm kích hoạt f:

xl_i = gl_i · f(∑_j wl_ij x(l-1)_j), trong đó gl_i = σ(vl_ik). (1)

Ở đây, wl_ij là trọng số kết nối từ neuron j trong lớp l-1 đến neuron i trong lớp l, trong khi vl_ik đại diện cho 'trọng số gating' cụ thể theo ngữ cảnh kiểm soát neuron i trong lớp l. Chỉ số k xác định ngữ cảnh nào đang hoạt động được xem là được cung cấp làm đầu vào trong tài liệu CL [12,42] và chúng tôi tuân theo giả định này trong phần lý thuyết này. Bias (ngưỡng) được xử lý như các trọng số bổ sung để đơn giản. Hàm σ được chọn là tang hyperbolic chỉnh hóa để đảm bảo rằng phạm vi của gl_i(t) được giới hạn giữa 0 và 1. Đầu vào mạng được biểu diễn bởi x(0)_i và đầu ra bởi yi = xL_i. Các trọng số gating được khởi tạo ngẫu nhiên và học với gradient descent bình thường (Nguyên tắc 2 không áp dụng cho các tham số này). Trên thực tế, gating này chọn một tập hợp phân tán các neuron tích cực tham gia vào xử lý thông tin cho ngữ cảnh k trong khi các neuron khác tham gia vào các ngữ cảnh khác được, ít nhất là một phần, tắt đi. Các cơ chế như ức chế bên [34,43] hoặc chuẩn hóa chia [44,45] có thể triển khai gating tương tự trong mô hình mạng vật lý sinh học.

Mở rộng Nguyên tắc 1 cho chuyển đổi ngữ cảnh không xác định. Nói chung, chuyển đổi ngữ cảnh không được đánh dấu: ví dụ, trẻ mới biết đi trong môi trường đa ngôn ngữ được mong đợi tự suy luận các điểm thay đổi giữa hai ngôn ngữ nói. Để minh họa rằng các Nguyên tắc GateON cũng áp dụng khi danh tính ngữ cảnh (tức là chỉ số nhiệm vụ) không được cung cấp, chúng tôi cũng nghiên cứu một biến thể của Nguyên tắc 1 trong đó chỉ số nhiệm vụ k tại mỗi bước thời gian được suy luận bằng cách phát hiện sự tăng đột ngột trong mất mát Lt. Đơn giản, mô hình thay thế này chỉ ra một điểm thay đổi mỗi khi trung bình động của Lt vượt qua một ngưỡng nào đó (xem Phương pháp 3.1 cho công thức hóa chính xác, mô hình đơn giản này cũng được truyền cảm hứng từ các mô hình khoa học thần kinh tính toán [43,46]). Quan trọng, trong khi với biến thể mô hình đơn giản này các biến thể ngữ cảnh được suy luận mà không có giám sát, không cần thay đổi bổ sung nào trong lý thuyết GateON (xem phần 1.3). Các bộ phát hiện ngữ cảnh khác đã được nghiên cứu chi tiết hơn [47,48] và có thể đã được sử dụng kết hợp với GateON. Để đưa ra so sánh công bằng với các thuật toán học máy, chúng tôi sử dụng mô hình CL tiêu chuẩn nơi chỉ số nhiệm vụ k được cung cấp trực tiếp.

Nguyên tắc 2: Cản trở Gradient - đóng băng và mở băng tính dẻo. Bổ sung cho tính chọn lọc ngữ cảnh, một mô hình CL chức năng yêu cầu thành phần thứ hai: trong phỏng đoán của chúng tôi khi các neuron được chuyên môn hóa cho một nhiệm vụ, các tham số của chúng nên đóng băng để tránh ghi đè sau này kiến thức đã thu được. Giả định rằng trong ngữ cảnh k tính dẻo triển khai gradient descent của mất mát Lk, chúng tôi công thức hóa dưới đây cách thức, và khi nào, các cập nhật gradient descent cần được cản trở (Nguyên tắc 2). Để làm cho mối quan hệ với tài liệu học máy rõ ràng, chúng tôi đầu tiên công thức hóa Nguyên tắc 2 trong quan điểm tập trung vào tham số nơi cập nhật của tham số θ bị cản trở.

Quan điểm tham số. Quan điểm tham số của GateON (p-GateON) được truyền cảm hứng từ các mô hình CL trước đây [13,49,50] duy trì các vector trọng số ổn định trong đường dẫn xử lý feedforward để giảm thiểu quên lãng thảm khốc. Tuy nhiên, có thể xây dựng các ví dụ nơi việc bảo tồn ngây thơ các trọng số được tối ưu hóa trước đó có thể cản trở việc đào tạo cho các nhiệm vụ tiếp theo - do đó việc mở băng một phần các trọng số đã học trước đó là ưu tiên hơn. Để điều hướng các thách thức, lý thuyết mới của p-GateON được công thức hóa sử dụng 'tốc độ học thích ứng' điều chỉnh linh hoạt các cập nhật gradient của các tham số riêng lẻ. Chính xác hơn, các cập nhật của tham số θ tỷ lệ với các gradient với tốc độ học được kiểm soát bởi 'khả năng' Aθ:

∆θ = η Aθ ∂Lt/∂θ (2)

trong đó η là tốc độ học danh nghĩa (SGD, Adam, ...). Khả năng Aθ được giới hạn giữa không và một và sẽ được xác định trong đoạn tiếp theo. Như thể hiện trong Phương trình (2), khả năng Al_θ càng nhỏ, sự cản trở gradient càng lớn. Nói cách khác, phương pháp của chúng tôi có thể đóng băng từng tham số riêng lẻ. Lưu ý rằng chỉ các trọng số feedforward wl_ij được điều chế bởi các khả năng, các trọng số ngữ cảnh vl_ik được tối ưu hóa với gradient descent tiêu chuẩn.

Định nghĩa khả năng. Chúng tôi định nghĩa 'khả năng' Aθ để theo dõi liệu tham số θ có liên quan đến các nhiệm vụ gần đây hay không. Mức độ liên quan cao dẫn đến khả năng thấp cho các nhiệm vụ tương lai do đó các cập nhật tham số tiếp theo bị cản trở. Toán học Aθ là một bộ tích hợp của 'mức độ liên quan chuẩn hóa' µnorm_θ đo lường tác động nhân quả của θ về hiệu suất nhiệm vụ Lk cho các nhiệm vụ trước đó. Một đổi mới lớn ở đây là định nghĩa của 'mức độ liên quan chuẩn hóa' µnorm_θ được đưa ra trong đoạn tiếp theo. Khả năng tích hợp đại lượng µnorm_θ theo thời gian sử dụng công thức:

Aθ(t + 1) = [Aθ(t)(1 - ηA(µnorm_θ - ε))]₁⁰, (3)

trong đó ηA và ε là các hằng số không âm. Hàm clip được ký hiệu [x]₁⁰ là hàm đồng nhất bên trong [0, 1], lấy giá trị 1 cho x > 1 và giá trị 0 cho x < 0. Bằng cấu trúc, Phương trình (3) có hai điểm cố định ổn định cho Aθ = 0 và Aθ = 1 và ε là ngưỡng kiểm soát sự hội tụ của phương trình này: khả năng phân rã theo cấp số nhân về không ngay khi µnorm_θ > ε, hoặc nó tăng theo cấp số nhân nhanh đến 1 nếu µnorm_θ < ε. Để tóm tắt, nếu một tham số có liên quan (µnorm_θ > ε) khả năng Aθ hội tụ về 0 điều này đóng băng tham số θ bằng cách cản trở các cập nhật tham số tiếp theo. Khi một tham số sau đó trở nên không liên quan cho nhiệm vụ khác, khả năng tăng lại chậm dẫn đến mở băng tham số. Mối quan hệ giữa các thang thời gian đóng băng và mở băng được thảo luận trong Phụ lục A.1. Siêu tham số ε quan trọng để cân bằng sự đánh đổi giữa bảo tồn các biểu diễn của các nhiệm vụ trước đó và duy trì tính linh hoạt của mạng cho các nhiệm vụ tương lai. Hiệu ứng của ε sẽ được nghiên cứu trong các mô phỏng.

Ước lượng mức độ liên quan thuật toán. Một thành phần quan trọng của lý thuyết là đo µnorm_θ để định lượng tác động nhân quả của θ về hiệu suất nhiệm vụ. Lý tưởng nhất, tác động nhân quả của neuron i về mất mát Lt sẽ được định nghĩa là sự khác biệt hiệu suất giữa mất mát hiện tại Lt và mất mát giả thuyết nếu tham số θ sẽ bị loại bỏ trong cùng một thiết lập:

µθ = (Lt|θ=0 - Lt)², (4)

trong đó Lt|θ=0 biểu thị mất mát dưới giả định rằng θ = 0 cho cùng điểm dữ liệu. Trực quan, một tham số có liên quan nếu mất mát thay đổi một lượng lớn khi nó bị loại bỏ. Việc đánh giá mức độ liên quan trong (4) tốn kém tính toán, vì nó sẽ yêu cầu đánh giá hàm mất mát một lần cho mỗi tham số. Do đó chúng tôi xấp xỉ nó bằng khai triển Taylor bậc nhất xung quanh tập hợp tham số hiện tại, tức là, Lt|θ=0 = Lt - ∂Lt/∂θ θ. Phương trình (4) sau đó trở thành:

µθ ≈ (∂Lt/∂θ θ)². (5)

Lưu ý rằng ∂Lt/∂θ θ có thể được tính hiệu quả vì nó liên quan đến cùng gradient đối với các tham số cũng được sử dụng trong BackProp. Cuối cùng, chúng tôi chuẩn hóa µθ để GateON không nhạy cảm với việc chia tỷ lệ của hàm mất mát Lk và các tham số ηA và ε không phải được tinh chỉnh cho mọi vấn đề CL. Để làm như vậy, chúng tôi sử dụng chuẩn hóa softmax

µnorm_θ = N² µθ / Σ_θ' µθ', (6)

trong đó N² là số tham số wl_ij trong lớp l (N là số neuron). Kết quả là 0 ≤ µnorm_θ ≤ N² và động lực khả năng không nhạy cảm với việc chia lại tỷ lệ mất mát vì nó bị hủy bỏ bởi softmax trong Phương trình (6). Lưu ý rằng giá trị µnorm_θ = 1 tương ứng với phân phối softmax đồng nhất, do đó phép nhân với N² đảm bảo rằng giá trị ngưỡng ε = 1 tự động cân bằng giữa đóng băng hoặc mở băng các tham số: Khi các biến khả năng thay đổi, có ít nhất một tham số được coi là không liên quan và mở băng (µnorm_θ < 1) và một tham số liên quan đóng băng µnorm_θ > 1. Nói cách khác, GateON với tham số ε = 1 được thiết kế để cân bằng giữa quên lãng và hợp nhất, trong các mô phỏng chúng tôi cũng đã thử nghiệm ε = 0.5 ưu tiên hơi nghiêng về hợp nhất hơn quên lãng.

1.2 Triển khai có khả thi sinh học của GateON
Chúng tôi mô tả bây giờ một mở rộng của lý thuyết GateON và giải thích tại sao chúng tôi tin rằng triển khai thay thế này tương thích hơn với dữ liệu và cơ chế sinh học hiện có.

Tính chọn lọc của các neuron vỏ não đối với ngữ cảnh thực nghiệm hỗ trợ Nguyên tắc 1. Để bắt đầu, chúng tôi xem Nguyên tắc 1 (tính chọn lọc ngữ cảnh có cổng, Phương trình 1) như một mô hình đơn giản của tính chọn lọc thần kinh được quan sát rộng rãi trong não. Cổ điển trong vỏ thị giác, nó được biết rõ rằng các neuron có tính chọn lọc cao đối với kích thích thị giác [14] sao cho quần thể các neuron chọn lọc xây dựng cùng nhau một mã thưa và phân tán của kích thích thị giác [15] hoặc các khái niệm trừu tượng hơn [16]. Quan trọng, các quy tắc ngữ cảnh được biểu diễn trong các vùng vỏ não không cảm giác như vỏ não trước trán: khi đào tạo khỉ trên nhiều nhiệm vụ thị giác, các neuron phát triển các biểu diễn đặc trưng phân tán bao gồm tính chọn lọc đối với quy tắc nhiệm vụ [18,19]. Chúng tôi thấy mô hình tính chọn lọc ngữ cảnh có cổng của Phương trình (1) như một mô hình tối thiểu của những quan sát chung này phù hợp với các mô hình điều chế hàm tăng ích nhân trước đó [24,26].

Một triển khai tập trung vào neuron hợp lý của Nguyên tắc 2. Cho tính chọn lọc ngữ cảnh của các neuron vỏ não, chúng tôi phỏng đoán rằng một cơ chế sinh học ẩn có khả năng theo dõi mức độ liên quan ngữ cảnh trên thang thời gian dài có khả năng được tìm thấy ở cấp độ neuron hơn là cấp độ synaptic. Dưới giả định này, chúng tôi tìm kiếm một triển khai tập trung vào neuron của Nguyên tắc 2 nơi biến khả năng liên quan đến các biến neuron (như tốc độ bắn hoặc điện thế màng) thay vì các biến synaptic phụ trợ. Sử dụng cùng lý thuyết chuẩn tắc mà chúng tôi đã sử dụng để dẫn xuất p-GateON, chúng tôi bây giờ dẫn xuất n-GateON như một mô hình tương tự, nhưng tập trung vào neuron.

Chúng tôi xem xét rằng mỗi neuron có một biến khả năng Al_i cản trở tính dẻo của tất cả các tham số của neuron i trong lớp l. Tương tự, như trước đây, khả năng Al_i này nên tích hợp tác động nhân quả của neuron i về mất mát Lk. Do đó trong quan điểm tập trung vào neuron này Nguyên tắc 2 được công thức hóa bởi hai phương trình:

∆wl_ij = η Al_i ∂Lt/∂wl_ij và µl_i ≈ (∂Lt/∂xl_i xl_i)², (7)

trong đó xl_i là hoạt động của neuron i trong lớp l. Khả năng được cập nhật khác như trong Phương trình (3) nơi chúng tôi chuẩn hóa µl_i trong phạm vi [0, N] thay vì [0, N²] vì chúng tôi có N neuron thay vì N² tham số. Có ít biến khả năng hơn đáng kể trong mô hình này sẽ có những ưu điểm và nhược điểm mà chúng tôi phân tích trong các mô phỏng trong Phần 1.3. Hình 1 dưới minh họa một ví dụ về khả năng của các neuron chọn lọc nhiệm vụ qua ba nhiệm vụ.

Đơn giản hóa thêm tính toán mức độ liên quan? Như nó đứng, chúng tôi xem công thức hóa của n-GateON như một hiện thực hóa khả thi sinh học thuyết phục của Nguyên tắc 1 và 2. Mô hình n-GateON sẽ được triển khai như mô tả ở trên trong tất cả các mô phỏng trừ khi được nêu rõ khác. Tuy nhiên, có thể lập luận rằng tính khả thi của mô hình không hoàn chỉnh vì đạo hàm ∂Lt/∂xl_i trong Phương trình (7) yêu cầu một triển khai khả thi sinh học của Backprop, bản thân nó vẫn là một vấn đề mở trong lĩnh vực này. Vấn đề này là một cơ chế khác biệt và phức tạp không kém mạng chính nó là cần thiết để tính toán lượt chuyền ngược nhưng không được tìm thấy trong não [51]. Nó làm phát sinh các xấp xỉ khả thi sinh học của back-prop [52–55], và quan trọng ở đây, hầu hết chúng đều tương thích với mô hình của chúng tôi. Để tính toán gradient ∂Lt/∂θ với các biến cục bộ hoặc tín hiệu từ trên xuống khả thi sinh học, phần khó nhất của những lý thuyết này là cung cấp một tính toán hợp lý của gradient ∂Lt/∂xl_i. Vì µl_i dựa vào chính xác thuật ngữ tương tự, bất kỳ sự kết hợp nào GateON với một mô hình như vậy là khả thi vì nó dựa vào thuật ngữ tương tự.

Tuy nhiên, chúng tôi thấy rằng một dẫn xuất thay thế của ảnh hưởng nhân quả của neuron i về phần còn lại của tính toán cũng có thể. Chúng tôi dẫn xuất trong Phương pháp 3.2 một công thức cho µl_i để đánh giá hiệu ứng của neuron i về các lớp tiếp theo hơn là mất mát chính nó. Hóa ra rằng dẫn xuất này tỷ lệ với

µl_i = (xl_i)² (8)

điều này mang lại một định nghĩa mức độ liên quan đơn giản hóa dễ tính toán cục bộ hơn nhiều mà không cần phản hồi từ mạng hạ nguồn. Hình 2 cho thấy rằng xấp xỉ của Phương trình (8) có chức năng mặc dù hiệu quả hơi kém hơn định nghĩa dựa trên gradient của n-GateON trong Phương trình (7).

1.3 Khung cho Kết quả Mô phỏng
Trước khi báo cáo kết quả mô phỏng cho p-GateON và n-GateON trên các tiêu chuẩn CL, chúng tôi giới thiệu nhiều chỉ số để định lượng thành công của mô hình CL.

Các nhiệm vụ T1, T2, ..., TK được trình bày lần lượt và đào tạo dừng khi nhiệm vụ cuối cùng TK đã hoàn thành. Gọi t0 biểu thị thời gian khi nhiệm vụ đầu tiên bắt đầu và tk điểm thời gian khi dữ liệu cuối cùng cho nhiệm vụ Tk được đưa ra. Trong CL, nhiều khía cạnh của độ bền và hiệu suất mạng có thể được kiểm tra. Cổ điển, các mạng thần kinh tiêu chuẩn được đào tạo với gradient descent chịu quên lãng thảm khốc, điều này có thể thấy khi hiệu suất giảm mạnh cho các nhiệm vụ trước đó Tk khi các nhiệm vụ mới Tk' với k' > k được học. Một vấn đề khác nổi lên cụ thể với các thuật toán CL nhằm 'đóng băng' các tham số để giữ kiến thức của các nhiệm vụ trước đó: nếu tất cả các tham số mô hình bị đóng băng sau k nhiệm vụ, chúng ta nói rằng mô hình bị bão hòa làm suy yếu hiệu suất của nó trên bất kỳ nhiệm vụ tương lai nào k' > k.

Câu hỏi cuối cùng là liệu mô hình có khả năng tái sử dụng kiến thức trước đó cho các nhiệm vụ tương lai hay không: độ chính xác trên nhiệm vụ T'_k tốt hơn sau khi học nhiệm vụ k so với nếu nhiệm vụ k' được học riêng biệt. Nếu vậy, đây là dấu hiệu của chuyển giao thuận. Ngược lại, học một nhiệm vụ sau Tk' có thể cải thiện độ chính xác trên nhiệm vụ Tk là dấu hiệu của chuyển giao ngược.

Trong thực tế, chúng tôi sử dụng bốn biện pháp định lượng trong Phương pháp 3.4 để định lượng những khía cạnh cấp cao này:

(i) Độ chính xác kiểm tra tức thời Ak_cc, đo độ chính xác kiểm tra trên một nhiệm vụ đơn Tk ngay sau khi đào tạo nhiệm vụ này, tức là, sau bước cập nhật tại thời gian tk. Nó đo bão hòa.

(ii) Độ chính xác liên tục Acont,k_cc, được tính bằng cách kiểm tra độ chính xác trên nhiệm vụ Tk sau khi đào tạo tất cả các nhiệm vụ sau Tk' với k' > k. Đại lượng được tính trung bình cho tất cả k' > k. Độ chính xác liên tục là một phép đo kết hợp của bão hòa và quên lãng.

(iii) Độ lệch độ chính xác ∆Ak_cc, tính toán sự khác biệt tương đối giữa độ chính xác kiểm tra tức thời trên nhiệm vụ Tk và độ chính xác của cùng một mạng được đào tạo chỉ trên Tk (∆Ak_cc > 0 cho k ≥ 2 ngụ ý chuyển giao thuận).

(iv) Tỷ lệ quên lãng FRk, là sự khác biệt giữa độ chính xác kiểm tra tức thời và độ chính xác liên tục. FRk dương có nghĩa là mô hình đã quên nhiệm vụ k, trong khi FRk âm ngụ ý rằng nhiệm vụ Tk được đào tạo trước đó tăng độ chính xác kiểm tra của nó trong quá trình đào tạo sau này là bằng chứng cho chuyển giao ngược.

Đối với tất cả bốn biện pháp, chúng tôi bỏ qua chỉ số k khi biện pháp được tính trung bình trên tất cả các nhiệm vụ k. Ví dụ, tỷ lệ quên lãng FR là trung bình của tất cả FRk.

So sánh mô hình trên các vấn đề CL hình ảnh đã thiết lập Để điều tra các tính chất của phương pháp GateON, chúng tôi áp dụng nó cho bốn vấn đề CL tiêu chuẩn [5,56,57] dẫn xuất từ tập dữ liệu MNIST. Đối với mỗi bốn vấn đề, mạng nhận các pixel hình ảnh làm đầu vào và đưa ra nhãn chữ số làm đầu ra. Bằng cách thay đổi quy ước đầu vào-đầu ra theo những cách khác nhau trong bốn mô hình khác nhau, chúng ta có thể thay thế đo độ bền đối với: hoán vị pixel đầu vào ngẫu nhiên (MNIST hoán vị), thay đổi đầu vào có cấu trúc qua xoay hình ảnh (MNIST xoay), thay đổi đầu ra ngẫu nhiên (MNIST xáo trộn) và bổ sung gia tăng các lớp nhãn mới (MNIST chia tách). Khi rõ ràng từ ngữ cảnh rằng chúng tôi đề cập đến MNIST hoán vị, chúng tôi sẽ đề cập đến tiêu chuẩn này là hoán vị. Các đặc tả nhiệm vụ được đưa ra trong Phương pháp 3.6

Để cho phép so sánh với các phương pháp CL Hiện đại, chúng tôi đã kiểm tra n-GateON và p-GateON trên một số vấn đề CL qua K = 10 nhiệm vụ liên quan đến MNIST (Bảng 1). Chúng tôi xem xét một mạng với 2000 neuron ẩn mỗi lớp. Chúng tôi quan sát rằng GateON hoạt động tốt trên tất cả ba vấn đề CL MNIST đã thiết lập. Độ chính xác liên tục được đạt gần với hiệu suất của các mô hình 'Riêng biệt' được tối ưu hóa cho từng nhiệm vụ riêng biệt. Chúng tôi nhấn mạnh đặc biệt rằng GateON đạt được độ chính xác liên tục cao mặc dù tính đơn giản khái niệm của nó. Ví dụ, GateON không phát lại các mẫu từ các nhiệm vụ trước đó như trong [28,42]. Tương tự, nó không sử dụng đầu ra mạng phụ thuộc nhiệm vụ như trong [12,58]. Trong khi hầu hết các thuật toán CL hiện có không thể được ánh xạ vào các cơ chế đơn giản như GateON (chỉ EWC và SI tương đối đơn giản), GateON đạt được độ chính xác liên tục cao hơn tất cả các mô hình CL khác trên các đặc tả nhiệm vụ tương đương. Thú vị là p-GateON và n-GateON đều đáng tin cậy hiệu quả qua hoán vị, xoay, chia tách và xáo trộn MNIST (xem cũng Bảng 3) cho thấy rằng lý thuyết GateON chung hoạt động cho các loại biến thể nhiệm vụ CL đầu vào và đầu ra khác nhau.

Để so sánh thêm, chúng tôi đã áp dụng GateON cho một vấn đề CL dẫn xuất từ tập dữ liệu hình ảnh CIFAR 100 sử dụng mô hình mạng tích chập ResNet thay vì mô hình kết nối đầy đủ. Vấn đề, Chia tách CIFAR 100, chứa 20 nhiệm vụ được xây dựng với các cặp lớp đối tượng xuất hiện tăng dần. Độ chính xác liên tục của GateON cao hơn tất cả các mô hình không phát lại mà chúng tôi tìm thấy trong tài liệu (Bảng 2). Thú vị, trên nhiệm vụ này, n-GateON hoạt động tốt hơn p-GateON.

Nghiên cứu loại bỏ với 100 nhiệm vụ MNIST. Khi số lượng nhiệm vụ tăng, hiệu ứng của bão hòa mạng và quên lãng trở nên quan trọng hơn vì các nhiệm vụ cạnh tranh cho tài nguyên mạng hạn chế. Do đó chúng tôi xem xét các vấn đề lớn hơn bao gồm 100 nhiệm vụ hoán vị, xoay và xáo trộn MNIST. Trong trường hợp Xáo trộn MNIST, chúng tôi tăng kích thước mạng từ 2000 lên 5000 neuron mỗi lớp (xem phương pháp).

Để nghiên cứu tầm quan trọng của Nguyên tắc 1 và 2 riêng biệt trong lý thuyết GateON, chúng tôi so sánh kết quả của GateON với bốn mô hình khác: một ANN cùng kích thước được đào tạo với gradient descent vanilla (tức là, không Nguyên tắc 1 hoặc 2 nào được triển khai); và hai mô hình bị loại bỏ 'Chỉ Gating' và 'Chỉ Cản trở' triển khai chỉ Nguyên tắc 1 hoặc 2, nhưng không phải cái khác. Mô hình gradient descent vanilla kém hiệu suất cho tất cả các nhiệm vụ, một hiện tượng quan sát trước đây khi nghiên cứu quên lãng thảm khốc [60,61]. Kết quả của Bảng 3 cho thấy rằng cả gating và cản trở đều cải thiện đáng kể hiệu suất so với đường cơ sở vanilla, nhưng không có cái nào trong số chúng hoạt động cạnh tranh riêng lẻ. Như một điểm tham khảo, phương pháp Active Dendrites [42] đạt được 91,2% độ chính xác liên tục trên nhiệm vụ hoán vị với 100 nhiệm vụ cao hơn các mô hình bị loại bỏ nhưng thấp hơn p-GateOn và n-GateON đạt được 96,5% và 95,7% độ chính xác liên tục tương ứng. Lưu ý rằng hầu hết các mô hình khác mà chúng tôi xem xét trước đây chỉ được kiểm tra trên 10 chứ không phải 100 nhiệm vụ, và chúng tôi lập luận dưới đây rằng với số lượng nhiệm vụ tăng, cơ chế mở băng như chuẩn hóa mức độ liên quan qua ngưỡng ε trong GateON trở nên quan trọng.

Đóng băng và mở băng các tham số với siêu tham số ε. Chúng tôi bây giờ cho thấy rằng tham số ε trở nên quan trọng để kiểm soát sự đánh đổi giữa bão hòa mạng và quên lãng khi số lượng nhiệm vụ lớn. Mặc dù phương pháp của chúng tôi với tham số ε = 0 đã đạt được kết quả hiện đại với 10 nhiệm vụ MNIST, vấn đề bão hòa mạng nổi lên với 100 nhiệm vụ MNIST: một khi tất cả các tham số bị đóng băng, mạng không thể học các nhiệm vụ mới. Vấn đề này chưa được xác định trong tài liệu trước đây vì số lượng nhiệm vụ nhỏ, tuy nhiên chúng ta thấy trong bảng 3 rằng vấn đề bão hòa quan trọng nhất đối với các nhiệm vụ không có cấu trúc như Hoán vị và Xáo trộn MNIST. Chúng tôi giải thích dưới đây cách tham số ε có thể giảm thiểu vấn đề này.

Đầu tiên hãy tập trung vào định nghĩa toán học của biến khả năng trong Phương trình (3). Với lựa chọn ε = 0, điểm cố định ổn định duy nhất của khả năng ở số không ngụ ý rằng cuối cùng tất cả các tham số sẽ bị đóng băng và mô hình bão hòa (nó không thể học nữa). Ngược lại, với ε > 0, nếu một tham số (hoặc neuron) không liên quan đến nhiệm vụ hiện tại, nó lấy giá trị µnorm_θ = 0, và kết quả là khả năng của nó tăng theo cấp số nhân nhanh đến 1 cho phép ghi đè tham số này. Nó tránh bão hòa hoàn toàn nhưng cho phép quên lãng kiến thức trước đó. Khi mức độ liên quan tham số cao hơn mức độ liên quan trung bình, biến mức độ liên quan chuẩn hóa µnorm_θ vượt quá giá trị một, do đó lựa chọn ε = 1 cung cấp ngưỡng chung để đóng băng và mở băng tùy thuộc vào mức độ liên quan tương đối của các tham số. Theo nghĩa này, thiết lập ε trong phạm vi này cung cấp giá trị trung gian được thiết kế để đánh đổi giữa quên lãng và bão hòa.

Trong thực tế, chúng tôi đã kiểm tra các giá trị ε = 0, 0,5 hoặc 1. Tác động đến bão hòa và quên lãng được minh họa trong Hình 3A. Sự tiến hóa của độ chính xác kiểm tra tức thời trong quá trình đào tạo tuần tự 100 nhiệm vụ tiết lộ sự giảm rõ ràng về độ chính xác sau 10 nhiệm vụ cho n-GateON với ε = 0 do bão hòa của mô hình. Chúng tôi quan sát rằng vấn đề này được giảm thiểu khi ε = 1 và ε = 0,5. Trong Hình 3B chúng tôi cho thấy một hệ quả khác của bão hòa, trong thiết lập nơi chỉ số nhiệm vụ k được suy luận tại mỗi mẫu và không được đưa ra làm đầu vào. Chúng tôi thấy rằng mô hình bão hòa trở nên không có khả năng suy luận chỉ số nhiệm vụ khi nó bị bão hòa. Tác động của ε đến độ chính xác liên tục tổng thể trên 100 nhiệm vụ MNIST được báo cáo trong Bảng 3.

Sự giảm độ chính xác liên tục có thể phản ánh bão hòa hoặc quên lãng. Tuy nhiên, đối với ε = 0 nơi chúng ta mong đợi bão hòa, điều này có thể thấy trên Hoán vị MNIST với n-GateON chỉ 73,6% độ chính xác liên tục do bão hòa nhưng hiệu suất này tăng lên 95,7% với ε = 1. Thú vị, chúng tôi cũng quan sát rằng vấn đề bão hòa có vẻ mạnh hơn với n-GateON so với p-GateON. Giải thích của chúng tôi là bão hòa ít nổi bật hơn nếu có nhiều biến khả năng hơn: với mạng kết nối đầy đủ, n-GateON có N biến khả năng mỗi lớp, trong khi p-GateON có N².

Chúng tôi báo cáo tỷ lệ quên lãng FR trong Bảng 4 (cột bên phải). Vì tất cả các giá trị đều dương, nó có nghĩa là mô hình hoạt động trong chế độ quên lãng trên nhiệm vụ này chứ không phải trong chế độ chuyển giao ngược. Quên lãng ít rõ rệt hơn đối với ε = 0 và p-GateON phù hợp với ý tưởng rằng bão hòa và quên lãng thể hiện xu hướng đối nghịch trong mô hình này.

Học chuyển giao trên MNIST xoay và xáo trộn. Chúng tôi bây giờ hỏi liệu mô hình của chúng tôi có khả năng tạo ra học chuyển giao hay không. Chúng tôi đầu tiên nghiên cứu chuyển giao thuận sử dụng độ lệch độ chính xác ∆Acc so sánh độ chính xác liên tục và riêng biệt: giá trị dương chỉ ra rằng mạng được hưởng lợi từ các nhiệm vụ trước đó. Kết quả của chúng tôi được tóm tắt trong Bảng 4 (bên trái). Tất cả các mục bảng hiển thị độ lệch độ chính xác dương (bằng chứng cho học chuyển giao thuận) được hiển thị màu xanh lá cây.

Chúng tôi thấy rằng tất cả các biến thể GateON có thể đạt được học chuyển giao thuận. Thú vị, chuyển giao thuận xảy ra trong MNIST Xoay và Xáo trộn nhưng không trên MNIST Hoán vị. Chính trên MNIST Xoay với n-GateOn và ε = 1 mà chúng tôi đạt được học chuyển giao thuận cao nhất. Chúng tôi giải thích rằng chuyển giao thuận dễ dàng hơn khi một số cấu trúc được chia sẻ qua các nhiệm vụ (ví dụ, các đặc trưng thị giác có thể được tái sử dụng trong Xoay, nhưng không trong Hoán vị MNIST).

Để hiểu cách học chuyển giao xảy ra, chúng tôi cố gắng kỹ thuật ngược mạng được đào tạo trong Hình 4. Chúng tôi hiển thị tương quan Pearson giữa ma trận trọng số gating qua 100 nhiệm vụ trong mỗi bốn lớp của mạng. Trong Hoán vị MNIST, sau 20 nhiệm vụ, chúng ta thấy tương quan cao trong lớp đầu ra, gợi ý rằng lớp cuối được chia sẻ qua các nhiệm vụ (tấm cuối của hàng đầu tiên trong Hình 4), nhưng không có tương quan hệ thống trong các lớp trước đó. Trong hàng thứ hai của Hình 4, chúng tôi hiển thị phân tích tương tự cho MNIST Xoay. Các nhiệm vụ được sắp xếp lại để đảm bảo rằng số nhiệm vụ lân cận tương ứng với các góc xoay tương tự. Trong trường hợp này, chúng ta thấy tương quan mạnh dọc theo đường chéo qua tất cả các lớp gợi ý rằng các đặc trưng được chia sẻ qua các nhiệm vụ với góc lân cận.

1.4 Học tập liên tục trong xử lý ngôn ngữ tự nhiên
Học tập liên tục đặc biệt liên quan đến các mô hình ngôn ngữ vì hai lý do: Ở cấp độ ẩn dụ, một mô hình thuyết phục của học tập suốt đời nên giải thích cơ chế cho phép học các ngôn ngữ nước ngoài mới mà không quên cái đã học trước đó. Ở cấp độ thực dụng hơn, một trường hợp sử dụng điển hình cho CL trong học máy sẽ là cập nhật kiến thức của mô hình ngôn ngữ được đào tạo trước từ năm này qua năm khác khi nhiều dữ liệu tích lũy. Trong công trình hiện tại, chúng tôi mô tả cách áp dụng nguyên tắc GateON cho các mô hình ngôn ngữ kích thước trung bình và báo cáo hiệu suất trên các tiêu chuẩn CL hiện có cho NLP: Phân loại Cảm xúc Khía cạnh (ASC) [62], Phân loại Cảm xúc Tài liệu (DSC) [63], và Phân loại Văn bản sử dụng dữ liệu 20News [64] (20News). Chúng tôi tóm tắt ba tiêu chuẩn CL dưới đây:

Vấn đề CL ASC liên quan đến phân tích cảm xúc về đánh giá sản phẩm, với mạng được yêu cầu phân loại từng khía cạnh của sản phẩm, như hình ảnh hoặc âm thanh, là tích cực, tiêu cực, hoặc trung tính. Vấn đề CL bao gồm 19 sản phẩm, với mỗi sản phẩm được coi là một nhiệm vụ riêng biệt. Thuật ngữ khía cạnh và câu được nối bằng token [SEP], và token [CLS] được sử dụng để dự đoán tính phân cực ý kiến. Ví dụ, mạng nhận "[CLS] đèn pin [SEP] đây là một đèn pin tốt" và nên đầu ra 'tích cực' cho khía cạnh đèn pin. Một số đánh giá có thể có nhiều khía cạnh.

Vấn đề CL DSC yêu cầu phân tích cảm xúc về toàn bộ đánh giá khách hàng, nơi mạng phải phân loại tông tổng thể là tích cực hoặc tiêu cực. Vấn đề CL bao gồm 10 sản phẩm (nhiệm vụ), và đầu vào bao gồm token [CLS] theo sau bởi đánh giá văn bản. Token đầu ra đầu tiên được sử dụng làm ước lượng cảm xúc.

Vấn đề CL 20News chia tách là nhiệm vụ phân loại chủ đề trên tập dữ liệu 20News. Nhiệm vụ ban đầu có 20 chủ đề, được chia ngẫu nhiên thành mười nhiệm vụ, mỗi nhiệm vụ chứa hai chủ đề. Đầu vào bao gồm token [CLS] theo sau bởi văn bản. Token đầu ra đầu tiên được sử dụng làm ước lượng chủ đề.

Tất cả đây đều là các vấn đề CL, có nghĩa là các nhiệm vụ của chúng được hiển thị tuần tự mà không nhớ lại. ASC và DSC tương tự như Hoán vị và Xoay theo nghĩa rằng mỗi nhiệm vụ có cùng mục tiêu phân loại nhưng với các tập dữ liệu khác nhau, trong khi 20News Chia tách thay đổi nhãn tương tự như MNIST Chia tách. Chúng tôi sử dụng cả Acont_cc và điểm Macro-F1 (MF1) để so sánh. MF1 là điểm F1 được tính trung bình trên các nhiệm vụ khác nhau và liên quan hơn độ chính xác trên các tập dữ liệu thiên vị như ASC. Để giải quyết các vấn đề CL này, chúng tôi sử dụng một trong những mô hình ngôn ngữ lớn được sử dụng nhiều nhất, BERT được đào tạo trước [36] mà chúng tôi đã áp dụng phương pháp GateON trên mỗi mô-đun, attention, trung gian, đầu ra, cho tất cả 12 lớp cũng như pooling và embedding.

Hiệu suất GateON cho tinh chỉnh mô hình ngôn ngữ. Đáng chú ý n-GateON nổi lên như một phương pháp đa năng đạt được hiệu suất cạnh tranh trên tất cả các vấn đề CL dựa trên ngôn ngữ (Bảng 5). Khi so sánh với tất cả các mô hình mà hiệu suất trên vấn đề này đã được chuẩn hóa bởi Ke et al. [29], chúng ta thấy rằng n-GateON liên tục là thuật toán tốt nhất hoặc tốt thứ hai trên tất cả các chỉ số. Điều này đáng chú ý vì chúng tôi chỉ sử dụng hai Nguyên tắc GateON tiêu chuẩn đơn giản về mặt khái niệm trong khi các thuật toán học máy khác thường được thiết kế cụ thể cho CL trên các nhiệm vụ này. Ví dụ, cả LAMOL và CTR là hai lựa chọn thay thế cạnh tranh nhất cho GateON đều được thiết kế cho các mô hình BERT và không thể tổng quát hóa cho các kiến trúc khác. Tiêu chuẩn khó nhất cho GateON có vẻ là DSC. Ở đó n-GateON là thuật toán tốt thứ hai sau LAMOL, nhưng LAMOL chỉ đạt được hiệu suất thấp trên 20News gợi ý rằng nó ít đa năng hơn GateON. Thú vị, p-GateON đạt được hiệu suất tương đối thấp trên ASC và DSC so với n-GateON. Chúng tôi tin rằng quan điểm tham số có thể yêu cầu nhiều dữ liệu hơn để tinh chỉnh mạnh mẽ tất cả các tham số liên quan.

Chuyển giao thuận và ngược trong NLP Như đã làm trước đây với MNIST, chúng tôi báo cáo độ lệch độ chính xác ∆At_cc và tỷ lệ quên lãng FR để nghiên cứu chuyển giao thuận và ngược qua các nhiệm vụ, tương ứng. Trái ngược với MNIST nơi tất cả các nhiệm vụ được thiết kế với cùng số lượng mẫu, các tiêu chuẩn NLP thể hiện phạm vi rộng kích thước nhiệm vụ, với một số nhiệm vụ được đặc trưng bởi các tập dữ liệu hạn chế và thiên vị. Theo nghĩa này, chúng tôi mong đợi rằng chuyển giao qua các nhiệm vụ là quan trọng cho các nhiệm vụ với ít điểm dữ liệu.

Chúng tôi báo cáo ∆At_cc để nghiên cứu chuyển giao thuận trong Bảng 6. Cả p- và n-GateON đều cho thấy độ lệch độ chính xác dương trên tất cả các nhiệm vụ nhấn mạnh khả năng chuyển giao thuận của phương pháp. Nó nổi bật nhất trong ASC và DSC nơi các nhiệm vụ liên quan đến "sản phẩm" cụ thể, và tập dữ liệu cụ thể theo sản phẩm đôi khi nhỏ hoặc rất thiên vị. Tập dữ liệu 20news Chia tách có kích thước tập dữ liệu nhất quán và các nhiệm vụ độc lập cao, góp phần vào tác động hạn chế hơn của học chuyển giao.

Để nghiên cứu chuyển giao ngược trong các tiêu chuẩn này, chúng tôi báo cáo các giá trị tỷ lệ quên lãng FR trong Bảng 7. Trong khi chúng tôi không thể thấy bằng chứng cho chuyển giao ngược trên các nhiệm vụ MNIST trước đây, bây giờ chúng ta thấy rằng n-GateON có tỷ lệ quên lãng âm trên ASC và DSC, gợi ý rằng mạng cải thiện hiệu suất của nó trên các nhiệm vụ trước đó thông qua chuyển giao ngược - mà không phát lại dữ liệu từ nhiệm vụ trước đó. Thú vị, n-GateON có vẻ liên tục tốt hơn về chuyển giao ngược so với p-GateON, làm nổi bật tầm quan trọng của quan điểm tập trung vào neuron.

2 Thảo luận
Tổng quan về lý thuyết GateON. Tóm lại, chúng tôi đã mô tả một mô hình tính dẻo dựa trên hai nguyên tắc: (1) Tính chọn lọc ngữ cảnh có cổng bằng điều chế nhân của hàm tăng ích neuron; và (2) đóng băng có thể điều chỉnh các cập nhật trọng số nếu các tham số (hoặc neuron) có liên quan đến các nhiệm vụ trước đó, tuy nhiên với khả năng mở băng sau này. Lý thuyết này được triển khai dưới hai hình thức: quan điểm tập trung vào tham số và tập trung vào neuron. Trong khi cái trước cung cấp lý thuyết chuẩn tắc liên kết với các mô hình thành công trước đây, chúng tôi thiết kế quan điểm tập trung vào neuron như một mô hình khả thi sinh học của học tập liên tục trong não.

Dự đoán thực nghiệm và ý nghĩa cho khoa học thần kinh Triển khai tập trung vào neuron của GateON cung cấp một giả thuyết cấp độ mạch cụ thể và có thể kiểm tra. Không giống như các lý thuyết CL trước đây tập trung vào neuron, lý thuyết của chúng tôi hợp nhất Nguyên tắc 1 và 2 của CL thành một cơ chế giả thuyết độc đáo và đơn giản hơn: CL trong vỏ não được hỗ trợ bởi tính chọn lọc nhiệm vụ của các neuron vỏ não (Nguyên tắc 1) và chúng tôi dự đoán sự tồn tại của các biến khả năng thần kinh theo dõi hoạt động cục bộ và kiểm soát (mở) đóng băng tính dẻo neuron khi neuron hoạt động chọn lọc trên nhiệm vụ k (Nguyên tắc 2). Khái niệm khả năng của chúng tôi liên kết chặt chẽ với khái niệm metaplasticity [27,37] nhưng cũng với hợp nhất synaptic mà có một số mô hình tồn tại [38–41]. Hàm tăng ích nhân có thể được triển khai bởi động lực dendrite [66].

Một dự đoán đầu tiên, dựa trên mô hình mức độ liên quan đơn giản hóa của Phương trình (8), như sau. Hãy theo dõi hoạt động spike của neuron trong thời gian dài. Để biến spike thành tốc độ bắn sẽ yêu cầu bộ lọc thông thấp mà chúng tôi ước lượng trong phạm vi một giây. Vì chúng tôi làm việc với mức độ liên quan chuẩn hóa, chúng tôi tập trung vào neuron với tốc độ bắn lớn hơn tốc độ bắn trung bình của các neuron khác trong cùng khu vực. Tốc độ bắn của neuron nên được bình phương để có mức độ liên quan, biến đổi thành mức độ liên quan chuẩn hóa, và sau đó chèn vào Phương trình (3) của động lực khả năng. Mô hình của chúng tôi dự đoán rằng đóng băng có thể thấy như metaplasticity, tức là, giảm lượng tính dẻo dưới giao thức cảm ứng tính dẻo tiêu chuẩn.

Hơn nữa, mở băng là dự đoán thứ hai của mô hình. Ở cấp độ định tính, mô hình của chúng tôi dự đoán rằng có tham số ngưỡng ε > 0 sao cho nếu tốc độ bắn, trong thời gian dài, thấp hơn tốc độ bắn trung bình của các neuron khác trong cùng khu vực, thì metaplasticity hiển thị như mở băng tính dẻo. Nói cách khác, các synapse neuron có thể trở nên dẻo lại. Tại thời điểm này, các tuyên bố trên là suy đoán, nhưng có thể đáng để kiểm tra thực nghiệm.

So sánh với các mô hình tính toán khác của CL. Bốn khía cạnh quan trọng để làm nổi bật điểm tương đồng và khác biệt với các mô hình hiện có.

(i) Ước lượng mức độ liên quan được sử dụng trong một số mô hình khác, hoặc kết hợp với cản trở gradient (ví dụ, HAT [12], và RMN [59]) hoặc kết hợp với regularization để ngăn chặn drift tham số lớn (EWC [6], SI [10]). HAT sử dụng mặt nạ attention cho mức độ liên quan, có thể ước lượng quá mức tầm quan trọng thực sự của tham số, trong khi RMN sử dụng ước lượng mức độ liên quan được học trong quá trình đào tạo. Ngược lại, chúng tôi tính toán trực tuyến một ước lượng phân tích về tác động của mỗi tham số về mất mát của nhiệm vụ hiện tại định nghĩa mức độ liên quan tức thời của tham số.

(ii) Khả năng động cho phép đóng băng và mở băng. Trái ngược với các phương pháp kiểm soát khả năng hiện có đóng băng các tham số của neuron có mức độ liên quan cao [12,59], GateON tích hợp mức độ liên quan dần dần thành một tập hợp các biến khả năng động xác định liệu, và bao nhiêu, các tham số được phép thay đổi. Một lợi thế chính của thiết kế của chúng tôi so với HAT hoặc RMN [12,59] là nó cho phép giới thiệu siêu tham số ε để kiểm soát mở băng các tham số không liên quan trong thời gian dài trong khi bảo vệ chống quên lãng thảm khốc. Các biến khả năng động có thể liên quan rất tốt với metaplasticity, như đã giải thích ở trên.

(iii) Gating đã trở thành thành phần chính cho CL hiệu quả [6,12,34,35,42]. Ví dụ, trái ngược với HAT [12] sử dụng cơ chế attention nhị phân cho gating và đầu ra đa đầu, cách tiếp cận của chúng tôi gate mọi đơn vị liên tục và không yêu cầu đầu ra đa đầu. Cách tiếp cận gating của chúng tôi chia sẻ điểm tương đồng với masking ngữ cảnh có thể học trong mô hình Active Dendrites [42], nhưng trái ngược với cách tiếp cận của họ, chúng tôi không thực thi số lượng neuron hoạt động nghiêm ngặt trên mỗi ngữ cảnh cũng không cần ước lượng embedding nhiệm vụ.

(iv) Phát hiện ngữ cảnh tự động. Trong khi các mô hình CL tiêu chuẩn đưa ra thông tin rõ ràng về số nhiệm vụ [59], chúng tôi giới thiệu một bộ phát hiện ngữ cảnh đơn giản cho phép suy luận không giám sát các nhiệm vụ. Trong khi các lĩnh vực học máy và xử lý tín hiệu đã kết hợp chuyển đổi ngữ cảnh qua phát hiện ngoài phân phối [46,67–70], chúng tôi lập luận rằng tồn tại các bộ phát hiện ngữ cảnh đơn giản như của chúng tôi, tính toán rẻ vì nó tập trung vào độ lệch trong mất mát được đánh giá dù sao cho mỗi mẫu. Các bộ phát hiện ngữ cảnh khác đã được nghiên cứu trong [47,48].

Kết luận. Mô hình của CL khác với học thống kê tiêu chuẩn vì dữ liệu đến như luồng đầu vào với phân phối dữ liệu không ổn định, có thể được đặc trưng bởi chuyển đổi sắc nét giữa các nhiệm vụ hoặc ngữ cảnh. Chúng tôi đã cho thấy rằng quan điểm tập trung vào neuron của gating xử lý feedforward kết hợp với metaplasticity của quy tắc học có nền tảng trong khoa học thần kinh và dẫn đến hiệu suất hiện đại trên các vấn đề học máy đã thiết lập và mới. Do tính tương đồng cấu trúc với học tập suốt đời trong sinh học, CL cũng sẽ trong tương lai tiếp tục kích hoạt một cuộc đối thoại có ích giữa khoa học thần kinh và học máy.
