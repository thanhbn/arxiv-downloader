# Tiềm năng Học máy Trọn đời
Marco Eckhoff∗và Markus Reiher†
ETH Zürich, Departement Chemie und Angewandte Biowissenschaften,
Vladimir-Prelog-Weg 2, 8093 Zürich, Thụy Sĩ.
(Ngày: 11 tháng 5, 2023)
Tiềm năng học máy (MLP) được huấn luyện trên dữ liệu hóa học lượng tử chính xác có thể giữ lại độ chính xác cao, trong khi chỉ tạo ra ít yêu cầu tính toán. Nhược điểm là chúng cần được huấn luyện cho từng hệ thống riêng lẻ. Trong những năm gần đây, một số lượng lớn MLP đã được huấn luyện từ đầu vì việc học dữ liệu bổ sung thường yêu cầu huấn luyện lại trên tất cả dữ liệu để không quên kiến thức đã có trước đó. Ngoài ra, hầu hết các bộ mô tả cấu trúc phổ biến của MLP không thể biểu diễn hiệu quả một số lượng lớn các nguyên tố hóa học khác nhau. Trong công trình này, chúng tôi giải quyết những vấn đề này bằng cách giới thiệu các hàm đối xứng trung tâm nguyên tử bao trọn nguyên tố (eeACSF) kết hợp các tính chất cấu trúc và thông tin nguyên tố từ bảng tuần hoàn. Những eeACSF này là chìa khóa cho sự phát triển tiềm năng học máy trọn đời (lMLP) của chúng tôi. Định lượng không chắc chắn có thể được khai thác để vượt qua MLP cố định, được huấn luyện trước để đến với lMLP thích ứng liên tục, vì có thể đảm bảo mức độ chính xác được xác định trước. Để mở rộng khả năng áp dụng của lMLP cho các hệ thống mới, chúng tôi áp dụng các chiến lược học liên tục để cho phép huấn luyện tự động và tại chỗ trên luồng dữ liệu mới liên tục. Đối với việc huấn luyện mạng nơ-ron sâu, chúng tôi đề xuất bộ tối ưu hóa liên tục có khả năng phục hồi (CoRe) và các chiến lược học tăng dần dựa trên luyện tập dữ liệu, điều hóa tham số và kiến trúc của mô hình.

Từ khóa: Học máy Trọn đời, Bộ tối ưu hóa Liên tục Có khả năng Phục hồi (CoRe), Hàm Đối xứng Trung tâm Nguyên tử Bao trọn Nguyên tố, Tiềm năng Mạng Nơ-ron Đa chiều Cao, Định lượng Không chắc chắn

1. GIỚI THIỆU
Để dự đoán và hiểu các tính chất và khả năng phản ứng của các hệ thống nguyên tử, việc biết bề mặt năng lượng tiềm năng là không thể tránh khỏi. Bề mặt năng lượng tiềm năng được cho bởi năng lượng electron như một hàm của vị trí hạt nhân và có thể thu được từ các phương pháp cấu trúc electron như lý thuyết phiếm hàm mật độ (DFT) hoặc các lý thuyết hàm sóng xấp xỉ [1, 2]. Những phương pháp này có thể áp dụng rộng rãi và đạt được kết quả chính xác cho nhiều hệ thống, nhưng ngay cả các phương pháp hiện đại nhất vẫn dẫn đến yêu cầu tính toán cao trong các mô phỏng mở rộng [3–5]. Thay vì tính toán rõ ràng năng lượng electron cho mỗi cấu trúc hạt nhân trong xấp xỉ Born-Oppenheimer, các trường lực thực nghiệm dựa vào các biểu thức giải tích xấp xỉ (đơn giản) của bề mặt năng lượng tiềm năng. Do đó, chúng tránh việc tích phân rõ ràng các phương trình cơ học lượng tử và từ đó cho phép mô phỏng nguyên tử hiệu quả. Tuy nhiên, các biểu thức trường lực có độ chính xác hạn chế và chúng thường không phổ quát cho tất cả các hệ thống hóa học [6–9].

Ngược lại, tiềm năng học máy (MLP) [10–15] có thể bảo tồn độ chính xác cao của các phương pháp cấu trúc electron, nhưng với chi phí tính toán thấp có thể so sánh với các trường lực. MLP không dựa trên các xấp xỉ vật lý, mà dựa vào các biểu thức toán học rất linh hoạt để thu được bề mặt năng lượng tiềm năng giải tích. Các tham số của những biểu thức này được huấn luyện trên dữ liệu tham chiếu cấu trúc electron bao gồm các cấu trúc hóa học và năng lượng cũng như lực nguyên tử tương ứng của chúng. Trong khi MLP thế hệ đầu chỉ có thể áp dụng cho các hệ thống chiều thấp [16], việc giới thiệu MLP thế hệ thứ hai dưới dạng tiềm năng mạng nơ-ron đa chiều cao (HDNNP) [17–19] đã tạo điều kiện cho mô phỏng nguyên tử của các hệ thống với hàng chục nghìn nguyên tử trên quy mô thời gian nano giây với độ chính xác tương tự như các phương pháp nguyên lý đầu tiên. Trong những năm gần đây, nhiều loại MLP khác đã được đề xuất như tiềm năng mạng nơ-ron [20–23], tiềm năng xấp xỉ Gaussian [24], tiềm năng tensor mô-men [25], và nhiều hơn nữa. MLP thế hệ thứ hai dựa vào tính cục bộ của phần chính của các tương tác nguyên tử [19]. Tuy nhiên, MLP thế hệ thứ ba thêm mô tả về các tương tác tầm xa [24, 26] và MLP thế hệ thứ tư thậm chí các tương tác không cục bộ [27, 28] vượt ra ngoài phạm vi cắt áp dụng cho các tương tác nguyên tử.

Các cấu trúc nguyên tử tham chiếu cần đại diện đủ tốt cho không gian cấu trúc được khám phá trong các mô phỏng tiếp theo. Lý do là MLP được thiết kế để nội suy các tương tác nguyên tử đã học nhưng khả năng ngoại suy xa ra ngoài không gian môi trường nguyên tử đã huấn luyện là hạn chế [12, 18]. Ví dụ, một MLP được huấn luyện trên nước sẽ thất bại đối với dihydrogen và dioxygen ở thể khí. Phương pháp xây dựng có hệ thống là không khả thi đối với hầu hết các hệ thống vì không gian cấu trúc khổng lồ thường có thể tiếp cận được đối với một hệ thống nguyên tử, đặc biệt khi xem xét các chuyển động biên độ lớn khác nhau hoặc phản ứng hóa học. Để có được các cấu trúc tham chiếu hợp lý, các giao thức học tích cực lặp đi lặp lại đã được xây dựng [29–33]. Trong các giao thức như vậy, các MLP sơ bộ được xác thực trong mô phỏng nguyên tử để xác định những cấu trúc quan trọng nhưng chưa biết.

Các cấu trúc này được tính toán lại bằng phương pháp tham chiếu và sau đó được thêm vào dữ liệu tham chiếu để huấn luyện MLP cải tiến. Tuy nhiên, khó đảm bảo rằng một tập dữ liệu tham chiếu hoàn chỉnh ngay cả đối với một ứng dụng cụ thể vì một số cấu trúc có thể chỉ xảy ra rất hiếm khi. Ngoài ra, các mục tiêu và mục đích khoa học khác nhau có thể, đối với cùng một hệ thống, làm nổi bật các vùng khác nhau của không gian cấu trúc của nó và do đó yêu cầu các cấu trúc tham chiếu khác nhau.

Các MLP hiện tại học một cách cô lập, tức là một mô hình được huấn luyện trên dữ liệu được xác định trước và sau đó được áp dụng trong mô phỏng mà không khai thác cho các MLP tương lai kiến thức đã học. Mặc dù có dạng hàm tổng quát, MLP do đó chỉ có thể được áp dụng đáng tin cậy cho một không gian cấu trúc cụ thể—có thể bao trùm các công thức hóa học và cấu hình khác nhau—và khả năng chuyển giao đáng kể vượt ra ngoài không gian môi trường nguyên tử đã học của chúng là hạn chế [8, 9]. Hậu quả là, một số lượng lớn MLP mục đích đơn lẻ đã được xây dựng [34–42]. Các công trình gần đây đã cố gắng vượt qua hạn chế này bằng cách tận dụng các tập dữ liệu huấn luyện rất lớn và rộng [43, 44], nhưng các ứng dụng tương lai vẫn có thể yêu cầu dữ liệu tham chiếu khác.

Để đạt được giải pháp mục đích chung, MLP cần có thể thích ứng để học các hệ thống hóa học mới dựa trên kiến thức đã có. Rõ ràng, các tập dữ liệu huấn luyện của MLP hiện đại có thể được mở rộng đơn giản để bao gồm các hệ thống bổ sung trong các quy trình học tích cực. Tuy nhiên, quá trình huấn luyện các tập dữ liệu mới hoặc mở rộng thường được bắt đầu từ đầu với các tham số MLP được khởi tạo ngẫu nhiên. Ngay cả khi các tham số đã có trước đó được sử dụng, việc huấn luyện thường không phân biệt giữa các điểm dữ liệu mới và cũ. Do đó, tất cả các điểm dữ liệu cần được huấn luyện đồng thời từ một lô dữ liệu tĩnh mỗi khi dữ liệu mới được thêm vào. Hậu quả là, việc học các tập dữ liệu lớn hơn trở nên ngày càng không hiệu quả khi lượng huấn luyện lặp lại dữ liệu cũ tăng lên. Xu hướng này ngược với cách bộ não con người hoạt động, vốn học hiệu quả kiến thức bổ sung dựa trên chuyên môn đã có [45–47].

Khả năng học tăng dần này được gọi là học trọn đời hoặc học liên tục trong lĩnh vực học máy [46, 47] và nó cũng được mong muốn cho MLP. Học máy trọn đời có nghĩa là mô hình liên tục thu thập và tinh chỉnh kiến thức. Đối với MLP, đây sẽ là kịch bản tác vụ tăng dần đơn lẻ vì các cấu trúc hóa học được thêm vào tuần tự nhưng năng lượng cần được phân biệt giữa tất cả các cấu trúc gặp phải [48]. Học trọn đời là một thách thức đối với học máy vì học tăng dần từ luồng thông tin liên tục thường dẫn đến quên lãng thảm khốc và can thiệp [49, 50]. Các tham số MLP được thích ứng với dữ liệu mới, trong khi bộ nhớ về dữ liệu cũ phai mờ đi.

Về nguyên tắc, xây dựng dựa trên kiến thức trước đó thậm chí có thể giảm lượng dữ liệu huấn luyện cần thiết và các tác vụ phức tạp hơn có thể được học hiệu quả hơn. Theo lý thuyết hệ thống học bổ sung [47], việc học dựa trên bộ nhớ tình huống và tổng quát hóa. Trong khi bộ nhớ tình huống học thông tin tùy ý nhanh chóng, tổng quát hóa là một quá trình học chậm để cấu trúc kiến thức. Do đó, bộ nhớ tình huống tương tự như tập dữ liệu huấn luyện, trong khi các tham số MLP được huấn luyện để tổng quát hóa dữ liệu. Hậu quả là, các tham số cần có tính dẻo để tích hợp kiến thức mới, nhưng đồng thời chúng cần ổn định để tránh quên lãng [45–47]. Trong những năm gần đây, một số thuật toán đã được phát triển để đạt được sự cân bằng ổn định-dẻo này và để giảm thiểu quên lãng thảm khốc; ví dụ là ExStream [51], bộ nhớ tình huống gradient [52], trí tuệ synap [53], AR1 [48], mạng nơ-ron tiến triển [54], và bộ nhớ kép phát triển [55]. Những phương pháp này dựa vào luyện tập dữ liệu huấn luyện được chọn, điều hòa các tham số học máy, và/hoặc kiến trúc của mô hình học máy.

Tình hình trở nên phức tạp hơn đối với MLP mục đích chung hoặc phổ quát vì nó sẽ bị cản trở thêm bởi việc mở rộng không thuận lợi về chi phí tính toán với số lượng nguyên tố hóa học. Đặc điểm này ảnh hưởng đến hầu hết các bộ mô tả MLP phổ biến [9, 56, 57] như hàm đối xứng trung tâm nguyên tử (ACSF) [58], chồng chéo mượt mà của vị trí nguyên tử [59], và nhiều hơn nữa [60, 61]—ngoại trừ các biểu diễn đồ thị [62–64]. Lý do là sự gia tăng nhanh chóng số lượng bộ mô tả cấu trúc vì chúng được xây dựng độc lập với nhau cho mỗi tổ hợp nguyên tố. Một số nghiên cứu đã cố gắng khắc phục vấn đề này bằng cách kết hợp thông tin nguyên tố và cấu trúc trong bộ mô tả cấu trúc thông qua việc bao gồm một trọng số phụ thuộc nguyên tố đơn lẻ [65–67]. Mặc dù có một số ứng dụng thành công, việc sử dụng, ví dụ, số nguyên tử như trong ACSF có trọng số [67] bị hạn chế đối với các nguyên tố có số nguyên tử tương tự. Nếu không, các đóng góp của nguyên tử nhẹ, như H, bị che khuất bởi các đóng góp của nguyên tử nặng, như iod (ví dụ, hệ số 53·53 trong các đóng góp góc).

Trong công trình này, chúng tôi khai thác các số hạng trọng số bổ sung dựa trên các tính chất khác nhau cho các ACSF riêng biệt có thể giảm thiên vị đối với các nguyên tố cụ thể và tăng khả năng phân biệt của biểu diễn bộ mô tả cho các cấu trúc khác nhau. Trực giác hóa học dựa trên các quy luật trong cấu trúc phân tử và xu hướng trong bảng tuần hoàn. Để khai thác những xu hướng này, chúng tôi sử dụng thông tin về chu kỳ và nhóm [68], hoặc chính xác hơn, vị trí trong khối s-, p-, d-, và f- thay vì số nguyên tử. Ví dụ, tất cả halogen dẫn đến loại liên kết tương tự, trong khi độ dài liên kết tăng trong các chu kỳ cao hơn.

Hơn nữa, chúng tôi đề xuất rằng định lượng không chắc chắn có thể cho phép áp dụng các tham số MLP có thể thích ứng. Các tham số có thể thích ứng yêu cầu độ tin cậy của MLP được thăm dò tại chỗ để tránh việc xác thực đầy đủ mỗi lần mở rộng huấn luyện MLP. Miễn là lỗi của MLP thấp hơn mức dung sai trong các tính toán sản xuất, những biến đổi nhỏ với quá trình học sẽ không ảnh hưởng đáng kể đến kết quả. Hơn nữa, MLP với định lượng không chắc chắn có thể báo cáo cảnh báo trong trường hợp các cấu trúc chưa biết, ngay cả khi chúng nằm trong không gian huấn luyện chung. Do đó, định lượng không chắc chắn có thể được sử dụng để ước tính khả năng chuyển giao của MLP, với độ không chắc chắn thấp có nghĩa là khả năng chuyển giao cao. Để có được định lượng không chắc chắn, ví dụ, cho HDNNP, một số nghiên cứu đã áp dụng phương pháp tập hợp [69–73]. Một tập hợp nhỏ HDNNP sau đó được huấn luyện khác nhau và độc lập trên cùng dữ liệu tham chiếu. Các độ lệch giữa các dự đoán của các HDNNP riêng lẻ có thể được sử dụng như một đại diện cho lỗi dự đoán do tính linh hoạt cao của mạng nơ-ron. Hơn nữa, việc lấy trung bình của tập hợp cải thiện độ chính xác của biểu diễn. Chúng tôi lưu ý rằng các biện pháp không chắc chắn như vậy nên đi kèm với bất kỳ loại phương pháp mô hình nào trong mô phỏng nguyên tử [74], mặc dù việc thực hiện chúng chỉ mới bắt đầu gần đây.

Hậu quả là, công trình này giới thiệu các vector hàm đối xứng trung tâm nguyên tử bao trọn nguyên tố (eeACSF) để khắc phục việc mở rộng không thuận lợi với số lượng nguyên tố lên đến một số tùy ý. Kết hợp với định lượng không chắc chắn và các chiến lược học liên tục, khái niệm tiềm năng học máy trọn đời (lMLP) được đề xuất, có thể được huấn luyện theo cách lăn bằng luồng dữ liệu mới liên tục. Để thực hiện quá trình huấn luyện đầy thách thức này, chúng tôi trình bày bộ tối ưu hóa liên tục có khả năng phục hồi (CoRe) mới, đây là một phương pháp thích ứng cho tối ưu hóa lặp bậc nhất ngẫu nhiên. CoRe được áp dụng kết hợp với các chiến lược huấn luyện trọn đời của chúng tôi, cung cấp lựa chọn thích ứng dữ liệu huấn luyện được sử dụng, bao gồm giảm kích thước tập dữ liệu và loại bỏ dữ liệu đáng ngờ. Chúng tôi chứng minh hiệu suất của khái niệm lMLP cho một tập dữ liệu bao gồm 42 phản ứng SN2 khác nhau và do đó mười nguyên tố hóa học khác nhau. Mặc dù công trình của chúng tôi về khái niệm lMLP dựa trên biểu diễn HDNNP thế hệ thứ hai, chúng tôi nhấn mạnh rằng khái niệm này cũng có thể được áp dụng cho một mô hình cơ sở khác. Hơn nữa, bộ tối ưu hóa CoRe và lựa chọn dữ liệu thích ứng trọn đời có thể được sử dụng trong huấn luyện các mô hình học máy ngoài lMLP và đóng góp vào sự phát triển của học máy trọn đời để giảm thiểu quên lãng thảm khốc.

Công trình này được tổ chức như sau: Trong Phần 2, chúng tôi tóm tắt phương pháp HDNNP và giới thiệu eeACSF, bộ tối ưu hóa CoRe, lựa chọn dữ liệu thích ứng trọn đời, định lượng không chắc chắn, và lMLP. Sau khi trình bày các chi tiết tính toán trong Phần 3, Phần 4 bắt đầu với mô tả dữ liệu tham chiếu. Phần 4 tiếp tục với đánh giá hiệu suất của biểu diễn eeACSF, so sánh giữa kết quả của CoRe và các bộ tối ưu hóa khác, kiểm tra hiệu suất của các chiến lược huấn luyện trọn đời, và xác thực định lượng không chắc chắn trong dự đoán bề mặt năng lượng tiềm năng. Công trình này kết thúc với kết luận trong Phần 5.

2. PHƯƠNG PHÁP

2.1. Tiềm năng Mạng Nơ-ron Đa chiều Cao với Chuẩn hóa

Đối với một hệ thống chứa Nelem nguyên tố và Nm atom nguyên tử của nguyên tố m, năng lượng HDNNP thế hệ thứ hai [17–19] được cho bởi tổng các đóng góp năng lượng nguyên tử Em atom,n của mọi nguyên tử n:

E=∑(m=1 đến Nelem)∑(n=1 đến Nm atom)Em atom,n. (1)

Mỗi đóng góp năng lượng nguyên tử được thu được từ một mạng nơ-ron feedforward,

Em atom,n=bm,3 1+∑(λ=1 đến n2)am,23 λ1·f2(bm,2 λ+∑(κ=1 đến n1)am,12 κλ·f1[bm,1 κ+∑(i=1 đến nG)am,01 iκ·αm i(Gm n,i−βm i)]). (2)

Các trọng số a và b được huấn luyện riêng cho mỗi nguyên tố m theo năng lượng và thành phần lực nguyên tử của một tập dữ liệu huấn luyện chứa nhiều cấu trúc hóa học. Chúng tôi lưu ý rằng số lớp ẩn có thể khác với trường hợp được hiển thị, dành cho hai lớp ẩn với n1 và n2 nơ-ron mỗi lớp. Một vector của các bộ mô tả cấu trúc cục bộ Gm n với chiều nG, được giải thích trong phần tiếp theo, được đưa vào làm đầu vào cho mạng nơ-ron nguyên tử này. Chúng tôi đề xuất hàm kích hoạt f(x) = 1.59223·tanh(x), được thảo luận trong Phần S1.1 của Thông tin Hỗ trợ cùng với một sơ đồ khởi tạo trọng số phù hợp [40, 75]. Lợi thế hội tụ và tăng độ chính xác của hàm kích hoạt và khởi tạo trọng số này so với hàm tangent hyperbolic và khởi tạo trọng số không phù hợp được hiển thị trong Bảng S3 và Hình S1 (a), S1 (b), và S2 trong Thông tin Hỗ trợ.

Trái ngược với phương pháp HDNNP gốc [17], các trọng số có thể huấn luyện αm i và βm i được giới thiệu để chuẩn hóa đầu vào bộ mô tả cấu trúc Gm n,i. Với mục đích này, bộ mô tả cấu trúc được dịch chuyển bởi βm i, được khởi tạo bằng trung bình của các giá trị bộ mô tả Gm n,i trong tập dữ liệu huấn luyện ban đầu cho tất cả nguyên tử n của nguyên tố tương ứng m. Kết quả được chia tỷ lệ bởi αm i, được khởi tạo bằng nghịch đảo độ lệch chuẩn của các giá trị bộ mô tả tương ứng trong dữ liệu huấn luyện ban đầu. Theo cách này, các trọng số am,01 iκ của lớp đầu vào được nhân với các giá trị, được căn giữa xung quanh số không và hiển thị độ lệch chuẩn bằng một cho dữ liệu huấn luyện ban đầu. Việc chuẩn hóa này có thể cải thiện hiệu suất huấn luyện và có thể điều chỉnh trong trường hợp dữ liệu huấn luyện bổ sung. Chúng tôi lưu ý rằng việc khởi tạo trọng số có thể bị hạn chế trong các giá trị trong một khoảng nhất định để tránh các vấn đề số. Cặp trọng số am,01 iκ và αm i có thể được kết hợp, nhưng chúng tôi xử lý chúng riêng biệt vì lý do kỹ thuật trong quá trình tối ưu hóa.

2.2. Hàm Đối xứng Trung tâm Nguyên tử Bao trọn Nguyên tố

HDNNP thường sử dụng các vector của nhiều hàm đối xứng trung tâm nguyên tử đa thể [58] để biểu diễn các môi trường nguyên tử cục bộ. Những bộ mô tả này thỏa mãn các bất biến tịnh tiến, quay và hoán vị của bề mặt năng lượng tiềm năng. Chúng phụ thuộc vào khoảng cách và góc của tất cả các nguyên tử lân cận nằm trong phạm vi cắt bán kính Rc. Bán kính này cần đủ lớn để tính đến tất cả các tương tác liên quan. Vì không có kết nối nào được sử dụng, HDNNP có thể mô tả phản ứng hóa học. Chiều của vector đầu vào không phụ thuộc vào môi trường nguyên tử riêng lẻ—một yêu cầu để có được mạng nơ-ron nguyên tử có thể áp dụng chung. Vector ACSF thông thường được xây dựng theo cách mà mỗi ACSF chỉ đại diện cho tất cả các tương tác giữa một cặp hoặc bộ ba nguyên tố hóa học cụ thể. Tuy nhiên, cấu trúc này dẫn đến việc mở rộng không thuận lợi số lượng bộ mô tả đối với số lượng nguyên tố.

Vì lý do này, chúng tôi giới thiệu hàm đối xứng trung tâm nguyên tử bao trọn nguyên tố (eeACSF) phụ thuộc rõ ràng vào thông tin nguyên tố H từ bảng tuần hoàn (xem đoạn tiếp theo). Tương tự như ACSF, có hai loại eeACSF. Các eeACSF xuyên tâm,

Grad n,i=[1/Hrad max,i ∑(j≠n đến Natom)Hrad i,j exp(−ηrad i·R2 nj)·fc(Rnj)]^(1/2), (3)

là một hàm của khoảng cách Rnj giữa nguyên tử trung tâm n và các nguyên tử lân cận j. Các eeACSF góc,

Gang n,i=(2^(−ζi)/Hang max,i ∑(j≠n đến Natom)∑(k≠n,j đến Natom)Hang i,jk[1 + λi cos(θnjk)]^ζi·exp(−ηang i(R2 nj+R2 nk))·fc(Rnj)·fc(Rnk))^(1/2), (4)

phụ thuộc thêm vào góc θnjk giữa nguyên tử n và hai lân cận j và k. Các giá trị khác nhau cho các tham số ηrad i≥0, ηang i≥0, λi=±1, và ξi≥1 của mỗi eeACSF i cuối cùng tạo ra dấu vân tay cấu trúc của môi trường nguyên tử của nguyên tử n. Hàm cắt,

fc(Rnj) = {exp(1−1/(1−R2 nj/R2c)−1) nếu Rnj< Rc; 0 nếu ngược lại}, (5)

làm mờ các đóng góp một cách mượt mà về số không vượt quá bán kính cắt Rc. Lợi thế của hàm cắt này là các đạo hàm của tất cả các bậc cũng bằng không tại Rc, điều này có lợi cho việc tính toán lực, chế độ chuẩn, v.v. Trái ngược với ACSF, căn bậc hai được áp dụng cho eeACSF để giảm thiểu tác động quá mạnh của số lượng lân cận lên giá trị eeACSF. Tác động mạnh như vậy có thể được quan sát đối với dữ liệu huấn luyện bao gồm kích thước phân tử rất khác nhau và mật độ hạt, và nó có thể làm giảm hiệu suất tham số hóa do phạm vi giá trị đầu vào quá rộng.

Như bộ mô tả nguyên tố, chúng tôi đề xuất n cho số chu kỳ của nguyên tố trong bảng tuần hoàn, m cho số nhóm trong khối s- và p- (nhóm chính 1 đến 8), và d cho số nhóm trong khối d-. Các nguyên tố nhóm chính tuân theo d = 0 và các nguyên tố khối d- m = 0. Trường hợp đặc biệt là helium với m = 8. Các giá trị của những bộ mô tả này cho nguyên tử lân cận j được sử dụng trong số hạng phụ thuộc nguyên tố Hrad i,j của eeACSF xuyên tâm,

Hrad i,j ∈ {1, nj, mj, dj, n̄j := X−nj, m̄j := 9−mj, d̄j := 11−dj}. (6)

Để xây dựng eeACSF xuyên tâm không phụ thuộc nguyên tố, Hrad i,j = 1 có thể được áp dụng. Trong công trình này, chu kỳ tối đa được đặt là X−1 = 5, tức là các nguyên tố lên đến xenon được xem xét. Các bộ mô tả n, m, và d được sử dụng để cân bằng đóng góp của các nguyên tố nhẹ và nặng cũng như của các nguyên tố có ít và nhiều electron hóa trị. Đối với các nguyên tố nhóm chính d = 0 được sử dụng và đối với các nguyên tố khối d- m = 0. Đối với các nguyên tố của chu kỳ cao hơn, nhóm trong khối f- có thể được thực hiện theo cách tương tự như đối với khối d- và X−1 có thể được đặt thành 7. Để giữ đóng góp của mỗi tương tác ở một giá trị từ 0 đến 1, eeACSF xuyên tâm được chia cho Hrad max,i là giá trị tối đa có thể của Hrad i,j.

Các số hạng phụ thuộc nguyên tố Hang i,jk của eeACSF góc được tính như tổ hợp tuyến tính,

Hang i,jk=Hrad i,j+γiHrad i,k+Ci, (7)

với γi=±1 và

Ci={0 cho γi= 1∨Hrad i,j=Hrad i,k= 0; 1 nếu ngược lại}. (8)

Hậu quả là, tiền tố phụ thuộc nguyên tố của eeACSF góc được định nghĩa là

Hang max,i={2Hrad max,i cho γi= 1; Hrad max,i nếu ngược lại}. (9)

Kết luận, đối với các hệ thống có/không có nguyên tố khối d- cần năm/bảy số hạng Hrad i,j khác nhau và chín/mười một số hạng Hang i,jk. Kết hợp với thường xung quanh năm giá trị tham số ηrad i, hai ηang i, hai λi, và ba ξi, vector bộ mô tả sẽ bao gồm 25/35 eeACSF xuyên tâm và 108/132 eeACSF góc độc lập với số lượng nguyên tố. Ngược lại, số lượng ACSF xuyên tâm tỷ lệ thuận với số lượng nguyên tố và số lượng ACSF góc chia tỷ lệ với ∑(m=1 đến Nelem)m. Ví dụ, trong trường hợp bốn nguyên tố, 20 ACSF xuyên tâm và 120 ACSF góc được thu được cho cùng số giá trị tham số. Do đó, khoảng bốn nguyên tố là điểm hòa vốn về chi phí tính toán, vì nỗ lực bổ sung để xác định Hrad i,j và Hang i,jk từ các giá trị phụ thuộc nguyên tố được liệt kê là nhỏ. Chúng tôi lưu ý rằng số lượng giá trị tham số khác nhau có thể ảnh hưởng đến độ phân giải của biểu diễn. Nút thắt cổ chai tính toán trong việc tính toán bộ mô tả trong các ứng dụng MLP là việc xác định các đạo hàm như một hàm của vị trí nguyên tử để thu được lực nguyên tử. Vì không có tính chất phụ thuộc vị trí bổ sung nào được bao gồm trong eeACSF so với ACSF, chi phí tính toán trên mỗi eeACSF tương tự như của ACSF.

2.3. Huấn luyện Tiềm năng Mạng Nơ-ron Đa chiều Cao

Để tối ưu hóa các trọng số của mạng nơ-ron nguyên tử đối với năng lượng tiềm năng Eref,r và thành phần lực nguyên tử Cartesian Fref,r α,n của các cấu trúc tham chiếu r, một hàm mất mát được định nghĩa:

Lt=q^2/Nconf ∑(r=1 đến Nconf)(Er,t−Eref,r)^2/(Nr atom)^2 + (3∑(r=1 đến Nconf)Nr atom)^(-1)·∑(r=1 đến Nconf)∑(n=1 đến Nr atom)∑(α=x,y,z)(Fr,t α,n−Fref,r α,n)^2. (10)

Thành phần lực nguyên tử là gradient âm của năng lượng đối với tọa độ Cartesian αr n = xr n, yr n, zr n của nguyên tử n của cấu trúc r,

Fr α,n=−∂Er/∂αrn. (11)

Một tập Nconf cấu trúc được huấn luyện đồng thời trong mỗi epoch huấn luyện t để tăng tốc tối ưu hóa và giảm quá khớp của các điểm dữ liệu đơn lẻ. Vì dự đoán HDNNP của một cấu trúc có thể phụ thuộc vào mạng nơ-ron nguyên tử của các nguyên tố hóa học khác nhau, những mạng này cũng được huấn luyện đồng thời. Để cân bằng đóng góp của năng lượng và lực vào hàm mất mát, siêu tham số q được sử dụng. Siêu tham số này cần được chọn cẩn thận vì nó có thể ảnh hưởng đáng kể đến hiệu suất huấn luyện. Để tối ưu hóa các trọng số, trong mỗi epoch huấn luyện, gradient của hàm mất mát đối với các trọng số,

wξ:=wm,μν χ,κλ∈ {am,μν κλ, bm,ν λ, αm κ, βm κ}, (12)

được tính toán, trong đó ξ là chỉ số duy nhất của mỗi trọng số.

2.4. Bộ tối ưu hóa Liên tục Có khả năng Phục hồi (CoRe)

Để cải thiện quá trình huấn luyện, chúng tôi đã phát triển bộ tối ưu hóa liên tục có khả năng phục hồi (CoRe) nhằm kết hợp tính mạnh mẽ của lan truyền ngược có khả năng phục hồi (RPROP) [76, 77] với hiệu suất của bộ tối ưu hóa Adam [78]. Hơn nữa, chúng tôi giới thiệu tỷ lệ phân rã thích ứng của trung bình động của gradient hàm mất mát, các yếu tố dẻo của trọng số thu được từ điểm số quan trọng, và phân rã trọng số ràng buộc các giá trị trọng số để tăng tốc độ hội tụ và độ chính xác cuối cùng vượt ra ngoài các bộ tối ưu hóa hiện đại.

CoRe là bộ tối ưu hóa dựa trên gradient bậc nhất. Nó sử dụng tỷ lệ học thích ứng riêng cho mỗi trọng số wξ, phụ thuộc vào lịch sử tối ưu hóa. Thuật toán được dành cho tối ưu hóa lặp ngẫu nhiên, tức là các mẫu con của lô dữ liệu huấn luyện được sử dụng trong mỗi epoch huấn luyện. Do đó, lợi ích hiệu quả tính toán của gradient descent ngẫu nhiên (SGD) [79] có thể được khai thác.

Theo tinh thần của bộ tối ưu hóa Adam, trung bình động hàm mũ của gradient,

gτ ξ=βτ 1·gτ−1 ξ+ (1−βτ 1)∂Lt/∂wt ξ, (13)

và gradient bình phương,

hτ ξ=β2·hτ−1 ξ+ (1−β2)(∂Lt/∂wt ξ)^2, (14)

với tỷ lệ phân rã βτ 1, β2∈[0,1) được kích hoạt trong việc tính toán tỷ lệ học thích ứng riêng của mỗi trọng số wt ξ. τ là bộ đếm bước tối ưu hóa cho mỗi trọng số, có thể khác với epoch huấn luyện t. Ví dụ, đối với HDNNP, một mẫu con dữ liệu huấn luyện của một epoch huấn luyện có thể không chứa mọi nguyên tố hóa học của toàn bộ tập dữ liệu huấn luyện dẫn đến cập nhật trọng số chỉ cho một số mạng nơ-ron nguyên tử trong epoch huấn luyện tương ứng. Theo cách này, mô hình HDNNP đã cung cấp một chiến lược kiến trúc để giảm thiểu quên lãng thảm khốc. Chúng tôi lưu ý rằng Phương trình (13) và (14) đại diện cho trường hợp tối thiểu hóa, trong khi đối với tối đa hóa, dấu của đạo hàm hàm mất mát đối với trọng số phải được đảo ngược.

Trái ngược với bộ tối ưu hóa Adam, β1 là một hàm của τ,

βτ 1=βb 1+(βa 1−βb 1)/exp[(τ−1)/βc 1^2]. (15)

Các siêu tham số βa 1, βb 1∈[0,1) định nghĩa các giá trị ban đầu và cuối cùng của β1. Những giá trị này được chuyển đổi lẫn nhau bởi một Gaussian với siêu tham số βc 1>0. Giá trị β1 lớn hơn tăng sự phụ thuộc vào thông tin gradient trước đó, tức là vào lịch sử tối ưu hóa của tất cả các mẫu con dữ liệu huấn luyện gần đây. Do đó, hiệu suất tối ưu hóa đối với toàn bộ tập dữ liệu huấn luyện có thể được cải thiện. Giá trị β1 nhỏ hơn mang lại sự phụ thuộc cao hơn vào gradient hiện tại của mẫu con, giảm một cách nào đó mô-men quán tính của quá trình tối ưu hóa. Cái sau có lợi cho hội tụ nhanh chóng và mạnh mẽ ở đầu tối ưu hóa, được bắt đầu từ trọng số được khởi tạo ngẫu nhiên. Kết luận, Phương trình (15) có thể được sử dụng để tăng β1 trong quá trình tối ưu hóa để cải thiện hiệu suất huấn luyện.

Một đóng góp của độ lớn cập nhật trọng số của CoRe được thu được, tương tự như bộ tối ưu hóa Adam, theo

uτ ξ=gτ ξ/(1−(βτ 1)τ) / [(hτ ξ/(1−(β2)τ))^(1/2) +ϵ]^(-1). (16)

Trung bình động gτ ξ và hτ ξ được hiệu chỉnh thiên vị bởi 1−(βτ 1)τ và 1−(β2)τ, tương ứng, để chống lại thiên vị khởi tạo của chúng về số không (g0 ξ, h0 ξ= 0). Việc chia hai trung bình động được hiệu chỉnh thiên vị làm cho độ lớn cập nhật trọng số bất biến với việc chia tỷ lệ lại gradient. Một dạng ủ bước được thu được, vì giá trị gradient giảm dẫn đến giảm cập nhật uτ ξ. Do đó, đối với các tối ưu hóa hoạt động tốt, giá trị tuyệt đối của uτ ξ thường giảm từ ±1 trong bước tối ưu hóa đầu tiên τ= 1 về phía số không. Giá trị cao hơn của siêu tham số β2 thúc đẩy việc ủ này. Siêu tham số ϵ⪆0 được thêm vào để ổn định số.

Như một đóng góp thứ hai vào cập nhật trọng số, yếu tố dẻo,

Pτ ξ={0 cho τ > t hist ∧ Sτ−1 ξ thuộc top-nm,μν frozen ,χ trong Sm,μν,τ −1 χ; 1 nếu ngược lại}, (17)

được giới thiệu để giảm thiểu quên lãng thông tin cũ. Trong giai đoạn huấn luyện ban đầu, yếu tố này bằng một. Khi τ > t hist, với siêu tham số thist>0, yếu tố dẻo có thể đóng băng giá trị của một số trọng số bằng cách đặt Pτ ξ thành số không. Việc lựa chọn những trọng số này phụ thuộc vào giá trị điểm số Sτ−1 ξ, xếp hạng tầm quan trọng của trọng số liên quan đến giảm hàm mất mát dự đoán trước đó, như sẽ được giải thích trong Phương trình (20). Giá trị điểm số Sτ−1 ξ được so sánh với tất cả các giá trị điểm số khác Sm,μν,τ −1 χ của cùng nhóm. Đối với HDNNP, một nhóm được tạo thành bởi các trọng số trong mạng nơ-ron nguyên tử của cùng nguyên tố m, với cùng loại trọng số χ, và cùng phân công lớp μν. Các trọng số thuộc nfrozen giá trị điểm số cao nhất trong nhóm tương ứng của chúng được đóng băng cho bước tối ưu hóa τ. Siêu tham số nm,μν frozen ,χ≥0 có thể được đặt riêng cho mỗi nhóm trọng số. Theo cách này, một điều hòa được thiết lập cho các trọng số có tầm quan trọng ước tính cao nhất.

Một đóng góp thêm của điều chỉnh kích thước bước được chuyển thể từ RPROP,

sτ ξ={min(η+·sτ−1 ξ, smax) cho gτ−1 ξ·gτ ξ·Pτ ξ>0; max(η−·sτ−1 ξ, smin) cho gτ−1 ξ·gτ ξ·Pτ ξ<0; sτ−1 ξ cho gτ−1 ξ·gτ ξ·Pτ ξ= 0}. (18)

Nói chung, kích thước bước sτ ξ không phụ thuộc vào độ lớn của gradient mà chỉ phụ thuộc vào dấu của nó, mang lại tối ưu hóa mạnh mẽ. Mỗi khi gτ ξ thay đổi dấu so với gτ−1 ξ, bước tối ưu hóa trước đó có thể đã nhảy qua một cực tiểu địa phương. Khi đó, kích thước bước trước đó sτ−1 ξ quá lớn và cần được giảm. Nếu dấu của gτ ξ và gτ−1 ξ giống nhau, sτ−1 ξ có thể được tăng để tăng tốc hội tụ. Yếu tố giảm η− và yếu tố tăng η+, với 0< η−≤1≤η+, được áp dụng để có được kích thước bước hiện tại sτ ξ, do đó sτ ξ bị ràng buộc bởi kích thước bước tối thiểu và tối đa smin, smax>0. Nếu Pτ ξ, gτ ξ, và/hoặc gτ−1 ξ bằng số không, việc cập nhật kích thước bước được bỏ qua. Kích thước bước s0 ξ cần được khởi tạo xác định kích thước bước tối ưu hóa đầu tiên s1 ξ. Giá trị có thể được chọn theo tỷ lệ hợp lý với các giá trị trọng số ban đầu và kinh nghiệm đã cho thấy rằng việc lựa chọn chính xác tham số này khá không quan trọng do khả năng thích ứng nhanh. Chúng tôi lưu ý rằng gradient không được đặt lại thành gτ ξ= 0 cho gτ−1 ξ·gτ ξ<0, trái ngược với một số biến thể RPROP bao gồm bước trọng số quay lui [76, 80]. Hậu quả là, lịch sử của gτ ξ có thể được giữ lại.

Để giảm nguy cơ quá khớp, cập nhật trọng số,

wt ξ=(1−dm,μν χ·uτ ξ·Pτ ξ·sτ ξ)wt−1 ξ−uτ ξ·Pτ ξ·sτ ξ, (19)

bao gồm phân rã trọng số với siêu tham số phân rã trọng số dm,μν χ∈[0,(smax)−1). Trọng số wt−1 ξ được giảm bởi phần thu được từ tích của dm,μν χ và cập nhật trọng số tuyệt đối hiện tại |uτ ξ|·Pτ ξ·sτ ξ. Chúng tôi lưu ý rằng dấu của cập nhật trọng số—bằng dấu của gτ ξ—chỉ được mã hóa trong uτ ξ. Siêu tham số phân rã trọng số nghịch đảo là giá trị trọng số tuyệt đối tối đa trong các tối ưu hóa hoạt động tốt, tức là uτ ξ≤ ±1, ngăn cản sự gia tăng hoặc giảm mạnh của trọng số. Để có được trọng số mới wt ξ, cập nhật trọng số hiện tại uτ ξ·Pτ ξ·sτ ξ được trừ khỏi trọng số trước đó wt−1 ξ. Do đó, trọng số trước đó được thay đổi theo hướng ngược lại với dấu của gτ ξ với tỷ lệ học được điều chỉnh riêng.

Giá trị điểm số,

Sτ ξ={Sτ−1 ξ+ (thist)−1gτ ξ·uτ ξ·Pτ ξ·sτ ξ cho τ≤thisth; (1−(thist)−1)Sτ−1 ξ+ (thist)−1gτ ξ·uτ ξ·Pτ ξ·sτ ξ nếu ngược lại}, (20)

tính đến các đóng góp cụ thể trọng số vào giảm hàm mất mát trước đó. Nó được lấy cảm hứng từ phương pháp trí tuệ synap [53]. Việc giảm hàm mất mát được ước tính bởi tích của trung bình động của gradient gτ ξ và cập nhật trọng số uτ ξ·Pτ ξ·sτ ξ cho mỗi bước τ. Đối với những thay đổi nhỏ vô cùng theo hướng ngược lại của gradient, tích của gradient và thay đổi bằng giảm hàm mất mát tương ứng. Đối với các cập nhật lớn hơn trong tối ưu hóa, như trong trường hợp hiện tại, việc giảm hàm mất mát thường được ước tính quá cao bởi tích này, nhưng vẫn hợp lý. Ngoài ra, gradient sẽ có nhiễu nếu nó được tính toán cho một mẫu con của toàn bộ tập dữ liệu huấn luyện. Việc đảo dấu của cập nhật được bỏ qua trong việc tính toán giá trị điểm số. Hậu quả là, giá trị điểm số dương cao hơn đại diện cho giảm hàm mất mát lớn hơn. Giá trị điểm số được khởi tạo là S0 ξ= 0. Đối với mỗi τ≤thist, gτ ξ·uτ ξ·Pτ ξ·sτ ξ được tổng với đóng góp bằng nhau (thist)−1 để xây dựng lịch sử ban đầu. Sau đó, Sτ ξ được tính toán như một trung bình động hàm mũ với tham số phân rã 1−(thist)−1. Giá trị điểm số xác định các trọng số quan trọng nhất cho dự đoán chính xác dữ liệu huấn luyện trước đó. Những trọng số này sau đó có thể bị hạn chế bởi yếu tố dẻo (Phương trình (17)), để cân bằng tỷ lệ ổn định-dẻo.

Như một biến thể, CoRe cũng có thể sử dụng dấu của trung bình động của gradient gτ ξ như yếu tố cập nhật uτ ξ, tức là uτ ξ= sgn(gτ ξ). Hơn nữa, CoRe có thể được sử dụng mà không có yếu tố dẻo, tức là nm,μν frozen ,χ= 0. Bộ tối ưu hóa Adam là trường hợp đặc biệt của CoRe cho các cài đặt siêu tham số βa 1=βb 1, nm,μν frozen ,χ= 0, η+, η−= 1, và dm,μν χ = 0. RPROP không có bước trọng số quay lui là trường hợp đặc biệt khác cho các cài đặt siêu tham số βa 1, βb 1, β2= 0, nm,μν frozen ,χ= 0, và dm,μν χ = 0.

Khuyến nghị chung về các giá trị siêu tham số có thể được cung cấp cho β2= 0.999, ϵ= 10−8, η−= 0.5, η+= 1.2, smin= 10−6, và smax= 1, chủ yếu phù hợp với bộ tối ưu hóa Adam và RPROP. Thông thường, s0 ξ= 10−3 là lựa chọn tốt cho tỷ lệ học ban đầu. Phân rã trọng số dm,μν χ phụ thuộc vào phạm vi mong muốn của các giá trị trọng số. Ví dụ, các trọng số liên quan đến nơ-ron đầu ra trong mạng nơ-ron nguyên tử không nên bị hạn chế để cho phép giá trị năng lượng nguyên tử tùy ý, trong khi các trọng số kết nối các lớp đầu vào và ẩn hoặc các lớp ẩn và ẩn áp dụng hàm tangent hyperbolic như hàm kích hoạt có thể bị hạn chế bằng cách sử dụng dm,μν χ = 0.1. βc 1 và thist phụ thuộc vào tốc độ hội tụ và tổng số epoch. Ví dụ, βc 1= 500 và thist= 500 được sử dụng trong công trình này, trong khi tổng số epoch là 1500, 2000, và 2500. Ngoài ra, nm,μν frozen ,χ được đặt thành 1% số lượng trọng số am,μν κλ và bm,ν λ trong nhóm tương ứng của chúng, ngoại trừ những trọng số liên quan đến nơ-ron đầu ra. βa 1 và βb 1 cần nhỏ hơn đối với gradient thay đổi nhanh chóng và mạnh mẽ. Ví dụ, βa 1= 0.45 và βb 1= 0.7, 0.725 được sử dụng trong công trình này. Chúng tôi lưu ý rằng đối với dữ liệu huấn luyện có tương quan thấp, việc tăng kích thước mẫu con có thể có lợi để giảm biến động gradient.

2.5. Lựa chọn Dữ liệu Thích ứng Trọn đời

Tối ưu hóa ngẫu nhiên, tức là tính toán gradient trên một mẫu con của dữ liệu huấn luyện trong mỗi epoch, có thể giữ yêu cầu tính toán ở mức có thể quản lý được đối với các tập dữ liệu lớn. Tuy nhiên, các mẫu con ngẫu nhiên có thể dẫn đến phương pháp không hiệu quả vì dữ liệu khác nhau thường được đại diện ở các mức độ chính xác khác nhau. Vấn đề này sẽ còn nghiêm trọng hơn nếu dữ liệu huấn luyện bổ sung được thêm vào trong quá trình huấn luyện trong học máy trọn đời. Hơn nữa, dữ liệu dư thừa và không chính xác cần được loại bỏ trong quá trình huấn luyện để có thể học tự động từ luồng dữ liệu mới liên tục. Tuy nhiên, luyện tập dữ liệu cũ đại diện rất quan trọng để tránh quên lãng thảm khốc.

Để tìm giải pháp cho những vấn đề này, chúng tôi đã phát triển thuật toán lựa chọn dữ liệu thích ứng trọn đời. Thành phần chính đầu tiên của thuật toán là yếu tố lựa chọn thích ứng Sr hist cho mỗi cấu trúc huấn luyện r phụ thuộc vào đóng góp hàm mất mát của cấu trúc và lịch sử huấn luyện. Thành phần chính thứ hai là sơ đồ cân bằng cho việc lựa chọn ngẫu nhiên dữ liệu huấn luyện được đại diện tốt và kém. Thuật toán có ý định loại trừ dữ liệu dư thừa và không chính xác và cải thiện hiệu suất huấn luyện bằng cách cân bằng điều chỉnh với dữ liệu mới hoặc được đại diện kém và luyện tập dữ liệu được đại diện tốt.

Thuật toán 1: Lựa chọn mẫu con dữ liệu huấn luyện được khớp trong một epoch huấn luyện trong thuật toán lựa chọn dữ liệu thích ứng trọn đời. Tất cả các phép toán vector đều theo phần tử. Các câu lệnh gán bao gồm điều kiện chỉ thay đổi các mục vector mà điều kiện đúng.

Nfit←min(Nfit,len(S>0 hist))
Ngood←int (pgood·Nfit)
Nbad←Nfit−Ngood
Lmax old←max(L\NaN old)
Pbad←ShistLold/Lmax old
Pbad←max (Shist) nếu Pr bad= NaN
Pbad←Pbad/sum (Pbad)
Dfit←random_choice ( D,Pbad, Nbad)
Pgood←S\fit hist[1−L\fit old/Lmax old]
Pmin good←min(P>0 good,1)·ϵ′
Pgood←Pmin good nếu L\fit old,r=Lmax old
Pgood←Pmin good nếu Pr good= NaN
Pgood←Pgood/sum (Pgood)
Dfit←Dfit∪random_choice(D\fit,Pgood, Ngood)

Thuật toán lựa chọn dữ liệu thích ứng trọn đời có thể được chia thành một phần được thực hiện trước khi tính toán hàm mất mát và một phần sau khi tính toán này. Trong phần đầu tiên (Thuật toán 1) mẫu con dữ liệu huấn luyện được chọn để tính toán hàm mất mát và gradient tương ứng. Mẫu con này được chỉ ra bởi chỉ số "fit" vì những dữ liệu này được sử dụng để khớp các trọng số. Số lượng cấu trúc được khớp mỗi epoch Nfit là siêu tham số huấn luyện và có thể được chọn dựa trên kích thước tập dữ liệu và tương quan giữa các điểm dữ liệu. Vì các cấu trúc có thể bị loại trừ bởi yếu tố lựa chọn thích ứng Sr hist (Thuật toán 2), Nfit không thể lớn hơn số lượng cấu trúc với Sr hist>0, tức là len(S>0 hist). Mẫu con dữ liệu huấn luyện được chia thành một tập Ngood cấu trúc đã được đại diện tốt và Nbad cấu trúc mới hoặc được đại diện không đầy đủ. Phần dữ liệu tốt được định nghĩa bởi pgood (Thuật toán 2), trong khi Ngood phải là số nguyên. pgood được khởi tạo là 0.

Để xác định xác suất Pbad và Pgood cho các cấu trúc được chọn làm dữ liệu xấu hoặc tốt, tương ứng, các đóng góp của cấu trúc vào hàm mất mát được sử dụng. Các đóng góp được tính cuối cùng Lold (hoặc Not a Number (NaN) nếu chưa có đóng góp nào được tính cho một cấu trúc cho đến nay) được chia cho đóng góp tối đa, không phải NaN Lmax old. Những thương số này được nhân với các yếu tố lựa chọn thích ứng cụ thể cấu trúc Shist để tạo ra xác suất Pbad. Shist được khởi tạo là vector đơn vị. Tất cả các thành phần NaN của Pbad được đặt thành tối đa của Shist. Để chuẩn hóa Pbad được chia cho tổng các thành phần của nó. Một lựa chọn ngẫu nhiên của Nbad cấu trúc với xác suất Pbad được chọn từ tập dữ liệu huấn luyện D để trở thành một phần của mẫu con dữ liệu huấn luyện.

Để có được Ngood dữ liệu tốt từ tập dữ liệu còn lại D\fit, xác suất Pgood được xác định. Pgood tỷ lệ thuận với hiệu số giữa vector đơn vị 1 và thương số của L\fit old và Lmax old. Xác suất không bằng không tối thiểu của Pgood được xác định và nhân với ϵ′⪆0 để có được xác suất rất thấp Pmin good cho các cấu trúc không bị loại trừ nhưng không được ưa thích. Các mục Pgood được đặt thành Pmin good nếu đóng góp hàm mất mát của cấu trúc tương ứng bằng đóng góp tối đa hoặc nếu mục Pgood là NaN. Sau đó, Pgood được chia cho tổng các đóng góp của nó. Mẫu con dữ liệu tốt là lựa chọn ngẫu nhiên của Ngood dữ liệu từ D\fit với xác suất Pgood. Hợp của dữ liệu xấu và tốt được chọn sau đó được sử dụng làm mẫu con dữ liệu huấn luyện để tính toán hàm mất mát.

Để cập nhật các yếu tố lựa chọn thích ứng Shist, các đóng góp hàm mất mát mới Lfit new của các cấu trúc trong mẫu con dữ liệu huấn luyện được sử dụng (Thuật toán 2). Do đó, tổng có trọng số của mất mát năng lượng Lfit E và mất mát lực Lfit F được sử dụng, được chia cho số lượng nguyên tử của các cấu trúc tương ứng Nfit atom. Lrel, là thương số của Lfit new và tổng mất mát Lfit new của epoch huấn luyện như được định nghĩa trong Phương trình (10), được so sánh với các ngưỡng khác nhau T1 F<1< T2 F< T3 F< TX để cập nhật Shist và bộ đếm đình công loại trừ X. Cái sau được giới thiệu để loại trừ các ngoại lệ dự đoán. Dưới giả định rằng mô hình hợp lý, các ngoại lệ có thể là các điểm dữ liệu không chính xác. Do đó, X— được khởi tạo là vector không—đếm có bao nhiều đánh giá liên tiếp Lr rel lớn hơn ngưỡng TX. Nếu Xr

Thuật toán 2: Cập nhật các yếu tố lựa chọn thích ứng cho mỗi cấu trúc trong mẫu con dữ liệu huấn luyện trong thuật toán lựa chọn dữ liệu thích ứng trọn đời. Tất cả các phép toán vector đều theo phần tử. Các câu lệnh gán bao gồm điều kiện chỉ thay đổi các mục vector mà điều kiện đúng.

Lfit new←q^2Lfit E+1/3Lfit F/Nfit atom
Lrel←Lfit new/Lfit new
X←Xr+ 1 nếu Lr rel> TX
X←0 nếu Lr rel≤TX
Shist←max (1 , Sr hist) nếu Lr rel≥T1 F
Shist←min ( Sr hist,1) nếu Lr rel≤T2 F
Shist←Sr hist·F−− nếu Lr rel< T1 F∧Lr new≤Lr old
Shist←Sr hist·F− nếu Lr rel< T1 F∧Lr new> Lr old
Shist←Sr hist·F+ nếu T2 F< Lr rel≤T3 F∧Lr new> Lr old
Shist←Sr hist·F++ nếu Lr rel> T3 F∧Lr new> Lr old
Shist←0 nếu Xr≥NX
Shist←0 nếu Sr hist< Smin hist∨Sr hist> Smax hist
pgood←clip[pgood+ sgn(Lfit new−Lfit old)p±,0, pmax good]
Lfit old←Lfit new
Lfit old←Lfit new

đạt siêu tham số NX, yếu tố lựa chọn thích ứng Sr hist được đặt thành số không và cấu trúc tương ứng do đó bị loại trừ khỏi huấn luyện tiếp theo.

Các yếu tố lựa chọn thích ứng Shist được sửa đổi thêm tùy thuộc vào các điều kiện sau: Nếu Lr rel≥T1 F và Sr hist<1, Sr hist được đặt thành một. Nếu Lr rel≤T2 F và Sr hist>1, Sr hist cũng được thay đổi thành một. Theo cách này, Sr hist của dữ liệu được đại diện tốt ( Lr rel< T1 F) có thể thấp hơn một dẫn đến giảm xác suất được chọn cho mẫu con dữ liệu huấn luyện. Mặt khác, Sr hist của dữ liệu được đại diện kém ( Lr rel> T2 F) có thể lớn hơn một tăng xác suất lựa chọn. Đối với các cấu trúc với T1 F≤Lr rel≤T2 F, Sr hist được đặt lại thành một, vì Lr rel xung quanh đóng góp mất mát trung bình. Ngoài ra, Shist có thể được sửa đổi bởi các yếu tố giảm F−− và F− và các yếu tố tăng F+ và F++. Theo cách này, Sr hist theo dõi lịch sử huấn luyện để đánh giá tầm quan trọng huấn luyện của cấu trúc liên quan r. Các yếu tố giảm và tăng được khởi tạo là F−(−) = (Smin)^(NF−(−))^(-1) và F+(+) = (Smax)^(NF+(+))^(-1), tương ứng. Hậu quả là, các siêu tham số NF−−, NF−, NF+, và NF++ định nghĩa có bao nhiều ứng dụng liên tiếp của yếu tố giảm và tăng liên quan dẫn đến yếu tố lựa chọn thích ứng dưới và trên các siêu tham số Smin và Smax, tương ứng. Đối với dữ liệu được đại diện tốt Sr hist được giảm bởi F−− nếu đóng góp hàm mất mát không đổi hoặc giảm so với cái được tính trước đó. Nếu không Sr hist của dữ liệu được đại diện tốt bị giảm ít mạnh hơn bởi F−. Đối với T2 F≤Lr rel≤T3 F và Lr new> Lr old, Sr hist được tăng bởi F+, trong khi nó được tăng bởi yếu tố lớn hơn F++ cho Lr rel≥T3 F và Lr new> Lr old.

Sau đó, tất cả Sr hist dưới Smin được đặt thành số không để giảm tập dữ liệu vì các cấu trúc liên quan được đại diện tốt một cách ổn định bởi mô hình. Do đó, những dữ liệu này có thể dư thừa và có thể bị loại trừ vì luyện tập dữ liệu khác và/hoặc các chiến lược học khác đủ để không quên những cấu trúc này. Tất cả Sr hist trên Smax cũng được đặt thành số không vì mô hình không thể trong nhiều bước tối ưu hóa đại diện chính xác những cấu trúc này. Do đó, dưới giả định rằng mô hình hợp lý, những cấu trúc này không nhất quán với phần chính của dữ liệu khác và có thể được loại bỏ khỏi dữ liệu huấn luyện. Theo cách này, hiệu suất huấn luyện cải thiện cho dữ liệu khác.

Để cân bằng thích ứng việc học dữ liệu được đại diện kém và không quên dữ liệu được đại diện tốt, phần dữ liệu tốt pgood được sửa đổi theo thay đổi của tổng mất mát Lfit new−Lfit old. pgood được tăng bởi p± nếu thay đổi mất mát dương, trong khi nó được giảm bởi p± cho thay đổi mất mát âm. Lold được khởi tạo là vô cùng và p± được định nghĩa là pmax good·(Np)^(-1), với giá trị pgood tối đa pmax good và tổng số giá trị pgood có thể Np. Cuối cùng, các đóng góp hàm mất mát mới Lfit new ghi đè các đóng góp hàm mất mát cũ Lfit old cho các cấu trúc của mẫu con dữ liệu huấn luyện và giá trị tổng mất mát mới Lfit new thay thế giá trị tổng mất mát cũ Lfit old.

2.6. Định lượng Không chắc chắn

Một lMLP yêu cầu dự đoán độ không chắc chắn của năng lượng ΔE và các thành phần lực nguyên tử ΔFα,n ngoài giá trị của chúng cho một cấu trúc hóa học. Lý do là điều kiện tiên quyết của lMLP là nó có thể tạo ra kết quả ở mọi giai đoạn huấn luyện. Vì các dự đoán ở các giai đoạn huấn luyện khác nhau có thể khác nhau, lMLP cần cung cấp biện pháp độ chính xác dự đoán. Khi đó, các dự đoán có thể được so sánh trong độ chính xác của chúng mà chúng nên đồng ý. Theo cách này, một phương pháp đáng tin cậy được thu được vẫn có thể thích ứng với thông tin mới theo thời gian. Chúng tôi lưu ý rằng định lượng không chắc chắn cũng tiến bộ đáng kể các phương pháp hóa học tính toán khác vì việc giải thích quá mức các đặc điểm vượt quá độ chính xác của phương pháp có thể được ngăn chặn.

Sự không chắc chắn của các mô hình học máy có thể được tách thành các đóng góp từ nhiễu trong dữ liệu tham chiếu, thiên vị của mô hình, và các đóng góp phương sai mô hình [81]. Việc xử lý dữ liệu huấn luyện có nhiễu được giải quyết bởi lựa chọn dữ liệu thích ứng trọn đời, trong khi nhiễu của dữ liệu kiểm tra không ảnh hưởng đến hiệu suất mô hình mà chỉ ảnh hưởng đến đánh giá hiệu suất. Đối với các tính toán cấu trúc electron hội tụ chính xác, nhiễu được xác định bởi các ngưỡng hội tụ và do đó nhỏ đối với dữ liệu tham chiếu MLP. Thiên vị mô hình được gây ra bởi các hạn chế trong kiến trúc mô hình, biểu diễn bởi các bộ mô tả, và kích thước tập dữ liệu tham chiếu. Do đó, kiến trúc mô hình cần được tối ưu hóa cho các ứng dụng khác nhau và các bộ mô tả tinh vi cần được kiểm tra để giữ đóng góp lỗi này nhỏ. Học trọn đời giải quyết phần còn lại của dữ liệu tham chiếu không đầy đủ. Lỗi phương sai mô hình có thể được giảm đáng tin cậy bởi các mô hình tập hợp (hoặc ủy ban) và kích thước của nó có thể được dự đoán bởi thống kê. Ước tính không chắc chắn tương ứng dựa trên tính linh hoạt khổng lồ của các biểu thức toán học cơ bản các mô hình học máy như mạng nơ-ron nhân tạo. Đối với không gian cấu trúc được huấn luyện tốt, các dự đoán thành viên tập hợp có thể đồng ý, trong khi các dự đoán là tùy ý đối với không gian cấu trúc chưa biết. Do đó, chúng có thể lan rộng lớn cho một tập hợp các MLP được huấn luyện độc lập và khác nhau, tức là sử dụng, ví dụ, các trọng số ban đầu khác nhau.

Hậu quả là, các tập hợp MLP đã được sử dụng để định lượng không chắc chắn đơn giản của các đóng góp phương sai mô hình [69–73, 82]. Dự đoán năng lượng cuối cùng E được thu được bằng trung bình của tập hợp NMLP năng lượng MLP Ep,

E=1/NMLP ∑(p=1 đến NMLP)Ep. (21)

Tương tự, các thành phần lực nguyên tử Fα,n được tính toán như trung bình của gradient năng lượng âm đối với tọa độ Cartesian αn của một tập hợp MLP,

Fα,n=−1/NMLP ∑(p=1 đến NMLP)∂Ep/∂αn. (22)

Độ không chắc chắn năng lượng ΔE sau đó có thể được thu được từ độ lệch chuẩn mẫu của các dự đoán năng lượng thành viên tập hợp,

ΔE= max(RMSE( E), [c^2/(NMLP−1) ∑(p=1 đến NMLP)(Ep−E)^2]^(1/2)), (23)

trong khi độ không chắc chắn tối thiểu được định nghĩa bởi sai số trung bình bình phương RMSE( E) của dữ liệu tham chiếu. Độ không chắc chắn thành phần lực nguyên tử ΔFα,n có thể được tính toán tương tự.

Yếu tố chia tỷ lệ c được giới thiệu để điều chỉnh độ lệch chuẩn mẫu cung cấp khoảng tin cậy nhất định rằng định lượng không chắc chắn bằng hoặc lớn hơn lỗi thực tế đối với dữ liệu tham chiếu. Tuy nhiên, định lượng không chắc chắn này bỏ lỡ các đóng góp thiên vị mô hình và do đó đánh giá thấp lỗi trong các trường hợp mà thiếu dữ liệu huấn luyện là nguồn lỗi chính. Mặc dù vậy, vì phương sai mô hình cũng tăng trong những trường hợp này, định lượng không chắc chắn này vẫn có thể báo hiệu liệu lỗi sẽ cao ngay cả khi giá trị không chắc chắn dự đoán không chính xác. Đối với các ứng dụng của MLP, thông tin này đủ vì độ không chắc chắn nhỏ có thể được dự đoán tốt và sự xuất hiện của lỗi lớn yêu cầu trong mọi cách cải thiện tính toán có thể đạt được bằng học máy trọn đời.

2.7. Quan điểm về Tiềm năng Học máy Trọn đời

Từ quan điểm dài hạn, một lMLP học ngày càng nhiều hệ thống theo thời gian trong khi việc huấn luyện được hưởng lợi từ kiến thức đã học trước đó về các tương tác hóa học là mục tiêu lớn. lMLP sau đó học theo cách tương tự như hóa học phát triển, tức là bằng cách mở rộng có hệ thống kiến thức trước đó. Vì công trình này là bằng chứng về khái niệm, các chiến lược huấn luyện trọn đời được trình bày tự nhiên chưa ở trạng thái tiên tiến như vậy để đạt được mục tiêu lớn này. Từ quan điểm ngắn hạn với các thuật toán được trình bày, lMLP có thể sẽ được huấn luyện trên dữ liệu của các hệ thống liên quan vì lợi ích là lớn nhất và khó khăn là thấp nhất trong trường hợp này. Khi các chiến lược huấn luyện trọn đời cũng như các mô hình MLP được phát triển đến trạng thái tinh vi hơn, ngày càng nhiều thông tin có thể được kết hợp trong một lMLP.

Công trình này trình bày cơ sở toàn diện các thành phần cho lMLP. Các công trình tương lai cần, ví dụ, phát triển thuật toán cho việc mở rộng liên tục các kiến trúc mô hình để tránh các hạn chế khả năng thông tin của các biểu diễn học sâu cơ bản. Ví dụ, tăng trưởng mạng nơ-ron có thể được thiết lập bằng cách sử dụng các kiến trúc có kích thước khác nhau của các thành viên tập hợp lMLP. Bằng cách theo dõi hiệu suất tương đối của chúng, yêu cầu tăng trưởng có thể được xác định và các thành viên tập hợp riêng lẻ có thể được thích ứng. Hơn nữa, các bộ nhận dạng cấu trúc thô như phương pháp bin và hash [83] có thể được áp dụng để tăng cường các tiêu chí lựa chọn và loại trừ dữ liệu huấn luyện được sử dụng để luyện tập. Ngoài ra, chúng có thể được sử dụng để kiểm tra nhanh liệu một cấu trúc nhất định có được đại diện bởi những cấu trúc tương tự trong dữ liệu huấn luyện hay không. Từ thông tin này, các cấu trúc chưa biết và được đại diện kém có thể được phân biệt để tránh các tính toán tham chiếu dư thừa.

Các mô hình MLP hiện tại thường bị hạn chế trong việc biểu diễn một trạng thái electron đơn lẻ. Do đó, ví dụ, chỉ có thể biểu diễn điện tích tổng cộng và bội số spin cụ thể bởi một MLP đơn lẻ. Tuy nhiên, các công trình gần đây nhắm mục tiêu những hạn chế này đối với các mô hình MLP tổng quát hơn [28, 33, 84] sau đó có thể được sử dụng làm mô hình cơ sở cho lMLP. Một hạn chế hiện tại của MLP là yêu cầu dữ liệu huấn luyện nhất quán, tức là tất cả dữ liệu huấn luyện phải được tính toán bằng cùng phương pháp, bộ cơ sở, v.v. Do đó, các mô hình MLP cần được phát triển, ví dụ, xử lý các phương pháp tham chiếu khác nhau theo cách tương tự như các trạng thái electron khác nhau để thiết lập lMLP học từ dữ liệu của các phương pháp tham chiếu khác nhau.

Hơn nữa, dữ liệu tham chiếu mở, miễn phí và có thể truy cập của công trình đã xuất bản rõ ràng rất quan trọng. Để tiếp tục huấn luyện công trình trước đó, không chỉ các giá trị trọng số cuối cùng mà còn các tính chất bộ tối ưu hóa CoRe liên quan của trọng số τ, gτ ξ, hτ ξ, sτ ξ, và Sτ ξ và các tính chất lựa chọn dữ liệu thích ứng trọn đời Lr old, Sr hist, và Xr của mọi cấu trúc huấn luyện vẫn cần thiết cũng như các giá trị Lold và pgood cần được lưu và cung cấp. Hơn nữa, các quy trình làm việc tự động cần được thiết lập trong công trình tương lai cho sự phát triển thân thiện với người dùng của lMLP với giao diện trực tiếp với phần mềm cấu trúc electron và mô phỏng nguyên tử.

3. CHI TIẾT TÍNH TOÁN

Trong các mô hình HDNNP của công trình này, mỗi mạng nơ-ron nguyên tử bao gồm một lớp đầu vào với nG= 153 (eeACSF) hoặc 156(ACSF) nơ-ron, ba lớp ẩn với n1= 102, n2= 61, và n3= 44 nơ-ron, và một nơ-ron đầu ra đơn lẻ. Việc khởi tạo trọng số được mô tả trong Phần S1.1 của Thông tin Hỗ trợ. Trong các dự đoán, kích thước tập hợp NMLP= 10 được áp dụng với yếu tố chia tỷ lệ lỗi c= 2. Chúng tôi lưu ý rằng kích thước tập hợp không được tối ưu hóa cho các trường hợp riêng lẻ, nhưng nó có thể trong các ứng dụng tương lai tùy thuộc vào cải thiện dự đoán giá trị và không chắc chắn so với yêu cầu tính toán bổ sung.

Để huấn luyện mỗi HDNNP, chúng tôi chia tập dữ liệu tham chiếu ngẫu nhiên thành 90% cấu trúc huấn luyện và 10% cấu trúc kiểm tra. Trong mỗi epoch huấn luyện, 10% của tất cả cấu trúc huấn luyện được sử dụng để khớp tập dữ liệu tham chiếu A và B (Bảng 2) và 4.07% được sử dụng để khớp tập dữ liệu tham chiếu C. Cái sau mang lại cùng số cấu trúc được huấn luyện mỗi epoch cho tập dữ liệu tham chiếu C so với B. Việc đếm này bao gồm các cấu trúc với yếu tố lựa chọn thích ứng Sr hist= 0 và các cấu trúc được thêm vào lần đầu tiên tại epoch huấn luyện muộn. Để biểu diễn các cấu trúc phân tử của các cấu trúc tham chiếu, các vector eeACSF được trang bị các tham số cho trong Bảng 1. Bán kính cắt lớn hơn bình thường Rc= 12Å được áp dụng để tránh các vấn đề với tương tác tĩnh điện, không phải là trọng tâm của công trình này. Tuy nhiên, các nghiên cứu tương lai có thể dễ dàng áp dụng các sơ đồ tinh vi hơn cho tĩnh điện như HDNNP thế hệ thứ tư [28].

Việc huấn luyện có giám sát được thực hiện trên tổng năng lượng DFT trừ tổng các năng lượng nguyên tử DFT của các nguyên tử trung tính tự do trong trạng thái spin thấp nhất (xem Bảng S1 trong Thông tin Hỗ trợ). Tất cả dữ liệu DFT của phản ứng SN2 được thu được cho điện tích tổng cộng −1e, với e là điện tích cơ bản, và bội số spin 1. Các tính toán DFT được thực hiện với chương trình hóa học lượng tử ORCA (phiên bản 5.0.3) [85, 86]. Phiếm hàm trao đổi-tương quan PBE [87] được chọn với bộ cơ sở def2-TZVP [88] và sử dụng xấp xỉ RI-J. Ngoài ra, hiệu chỉnh phân tán bán cổ điển D3 [89] với giảm chấn Becke-Johnson [90, 91] được áp dụng. Để cân bằng việc huấn luyện trên năng lượng và lực DFT tham chiếu, siêu tham số hàm mất mát được đặt thành q= 10.9.

Đối với các tối ưu hóa SGD, tỷ lệ học tốt nhất được tìm thấy là 0.00075 được áp dụng. Các tối ưu hóa RPROP được thực hiện

Bảng 1: Tất cả các tổ hợp của các tham số được liệt kê được áp dụng cho eeACSF xuyên tâm và góc, tương ứng. Bán kính cắt được đặt thành Rc= 12Å cho tất cả eeACSF. γ=±1 được sử dụng ngoại trừ Hang= 1, nơi chỉ γ= 1 được áp dụng.

eeACSF Xuyên tâm
Hrad 1,n,m,n̄,m̄
ηrad/Å−2 0,0.010702,0.023348,0.044203,
0.066118,0.104168,0.180285,
0.370959,1.115414

eeACSF Góc
Hang 1,n,m,n̄,m̄
ηang/Å−2 0.011238,0.090144
λ −1,1
ξ 1,2.409421,9.996864

với các siêu tham số được khuyến nghị trong Tham khảo 76. Một ngoại lệ là tỷ lệ học ban đầu, mà 0.001 được tìm thấy là lựa chọn tối ưu trong công trình này. Đối với bộ tối ưu hóa Adam, các siêu tham số tương tự được sử dụng như trong Tham khảo 78 vì không tìm thấy cải thiện chung nào bằng các biến thể siêu tham số.

Các siêu tham số của bộ tối ưu hóa CoRe là: βa 1= 0.45, βb 1= 0.7, 0.725, βc 1= 500, β2= 0.999, ϵ= 10−8, η−= 0.5, η+= 1.2, smin= 10−6, smax= 1, s0 ξ= 10−3, và thist= 500. nm,μν frozen ,χ được đặt thành 1% số lượng trọng số am,μν κλ và bm,ν λ trong nhóm tương ứng của chúng. Ngoại lệ là những trọng số liên quan đến nơ-ron đầu ra, mà nm,μν max frozen ,χ là 0. Đối với các trọng số α và β, nm,μν frozen ,χ= 0 được áp dụng. Siêu tham số phân rã trọng số dm,μν χ là 0.01 cho χ=α, β, 0 cho ν=νmax, và 0.1 nếu ngược lại. Huấn luyện được thực hiện cho 1500, 2000, và 2500 epoch.

Các siêu tham số của lựa chọn dữ liệu thích ứng trọn đời là: Smin hist= 0.1, Smax hist= 100, pT1 F= 0.9, pT2 F= 1.2, pT3 F= 2.0, NF−−= 30, NF−= 100, NF+= 500, NF++= 150, √TX= 7.5, NX= 5, pmax good=2/3, Np= 20, và ϵ′= 10−6.

Phần mềm lMLP được viết bằng Python và khai thác gói tính toán khoa học NumPy (phiên bản 1.21.6) [92] và khung học máy PyTorch (phiên bản 1.12.1) [93]. Nó có sẵn trên Zenodo (DOI: 10.5281/zenodo.7912832) cùng với các tập dữ liệu tham chiếu, đầu ra được tạo của công trình này, và các script để phân tích và vẽ đầu ra này.

Để so sánh công bằng và đáng tin cậy các huấn luyện MLP khác nhau, chúng tôi thực hiện cho mỗi cài đặt 20 huấn luyện với các số ngẫu nhiên khác nhau được sử dụng trong việc chia tách dữ liệu huấn luyện và kiểm tra, khởi tạo trọng số, và quá trình lựa chọn dữ liệu huấn luyện. Đối với mỗi cài đặt, mười MLP tốt nhất được sử dụng trong phân tích để giảm sự phụ thuộc vào các số ngẫu nhiên rõ ràng. Theo cách này, tác động của các ngoại lệ được tối thiểu hóa có thể bắt nguồn từ lựa chọn tham số ban đầu không thuận lợi, có thể ảnh hưởng đến tất cả các bộ tối ưu hóa. Do đó, các trung bình và độ lệch chuẩn được cung cấp đại diện cho hiệu suất trong các trường hợp hoạt động tốt. Vì chúng tôi sử dụng các phép chia khác nhau của dữ liệu huấn luyện và kiểm tra trong việc huấn luyện các thành viên tập hợp trong công trình này, dự đoán tập hợp của dữ liệu tham chiếu trộn lẫn dự đoán dữ liệu huấn luyện và kiểm tra. Trong khi phương pháp này sử dụng hiệu quả tất cả dữ liệu tham chiếu để huấn luyện, việc xác thực không thiên vị của tập hợp cần được thực hiện trên dữ liệu bổ sung.

4. KẾT QUẢ VÀ THẢO LUẬN

4.1. Dữ liệu Tham chiếu

Để so sánh hiệu suất của ACSF và eeACSF, một tập dữ liệu tham chiếu A được xây dựng chứa mười phản ứng SN2 pha khí khác nhau. Những phản ứng này được đại diện bởi 2026 cấu trúc tham chiếu và năng lượng cũng như thành phần lực nguyên tử tương ứng của chúng thu được từ phương pháp tham chiếu. Các cấu trúc tham chiếu là các cấu trúc hóa học khác nhau lấy mẫu không gian cấu trúc của các phản ứng SN2 bao gồm các nhóm rời X−=Cl−, I− và nucleophile Y−=Cl−, HCC−, I− cho nguyên tử carbon methyl trung tâm và nguyên tử carbon tert-butyl bậc ba (Hình 1). Chúng tôi lưu ý rằng cản trở không gian dẫn đến rào cản năng lượng cao trong phản ứng SN2 của nguyên tử carbon bậc ba. Tập phản ứng SN2 này chỉ bao gồm bốn nguyên tố khác nhau để việc áp dụng ACSF vẫn khả thi về yêu cầu tính toán. Sự tăng trưởng tổ hợp của vector ACSF với số lượng nguyên tố cản trở việc áp dụng cho dữ liệu tham chiếu bao gồm nhiều nguyên tố hơn.

Hình 1: Phản ứng SN2 tại nguyên tử carbon methyl với nhóm rời X và nucleophile Y.

Ngược lại, kích thước của vector eeACSF giữ không đổi với số lượng nguyên tố cho phép huấn luyện dữ liệu tham chiếu chứa số lượng nguyên tố tùy ý. Để kiểm tra eeACSF, một tập dữ liệu tham chiếu B được biên soạn bao gồm 8600 cấu trúc bao gồm 42 phản ứng SN2 khác nhau với nhóm rời X−=Cl−, I−, nucleophile Y−=Br−, Cl−, F−, H2N−, H3CO−, HCC−, HO−, HS−, HSe−, I−, NC−, và nguyên tử carbon methyl và tert-butyl trung tâm.

Cả tập dữ liệu tham chiếu A và B đều chứa các cấu trúc thu được trong tối ưu hóa DFT có ràng buộc. Khoảng cách carbon-nhóm rời và nucleophile-carbon được đặt trong phạm vi từ 1.05 đến 5.25Å bằng phương pháp lấy mẫu không đều. Một lưới sơ bộ của các giá trị khoảng cách, dày đặc hơn cho khoảng cách nhỏ hơn, được định nghĩa với khoảng cách tối thiểu riêng cho mỗi hệ thống phản ứng SN2. Để có được các giá trị khoảng cách cuối cùng, các thay đổi ngẫu nhiên trên các giá trị lưới được áp dụng với ràng buộc rằng các giá trị không thể nhỏ hơn hoặc lớn hơn các giá trị lưới liền kề.

Để thêm biểu diễn của các biến dạng cấu trúc, 12551 cấu trúc của các hệ thống phản ứng SN2 với nguyên tử carbon methyl trung tâm được tạo ra bằng dịch chuyển nguyên tử ngẫu nhiên. Đối với mỗi cấu trúc thu được trong tối ưu hóa DFT có ràng buộc, ba cấu trúc mới được tạo ra bằng cách dịch chuyển ngẫu nhiên tất cả vị trí nguyên tử bên trong các hình cầu trung tâm nguyên tử với cùng bán kính 0.05, 0.1, và 0.15Å. Nếu một số khoảng cách giữa nguyên tử trở nên quá nhỏ, quá trình được khởi động lại với một tập dịch chuyển ngẫu nhiên khác. Những cấu trúc này cùng với những cấu trúc của tập dữ liệu tham chiếu B tạo thành tập dữ liệu tham chiếu C. Chúng tôi nhấn mạnh rằng những dữ liệu tham chiếu này chỉ được sử dụng trong đánh giá hiệu suất của học trọn đời và có thể không đủ để thực hiện mô phỏng nguyên tử vì các cấu trúc tham chiếu liên quan có thể bị thiếu.

Việc tiền xử lý duy nhất của các tập dữ liệu tham chiếu là hạn chế đến các thành phần lực nguyên tử tuyệt đối tối đa 15 eVÅ−1 để loại trừ những cấu trúc chỉ xảy ra trong điều kiện cực đoan. Phạm vi năng lượng và thành phần lực nguyên tử và độ lệch chuẩn của các tập dữ liệu khác nhau được biên soạn trong Bảng 2 và được tham chiếu trong so sánh hiệu suất trong các phần sau. Phạm vi năng lượng trung bình và độ lệch chuẩn cho các hệ thống phản ứng SN2 riêng lẻ được cung cấp trong Bảng S4 trong Thông tin Hỗ trợ.

Bảng 2: Số phản ứng SN2 NSN2, cấu trúc Nconf, nguyên tử tổng cộng Ntotal atom, và nguyên tố Nelem cho các tập dữ liệu tham chiếu khác nhau A, B, và C. Ngoài ra, phạm vi năng lượng Eref range và độ lệch chuẩn Eref std và phạm vi thành phần lực nguyên tử Fref α,n,range và độ lệch chuẩn Fref α,n,std được cung cấp.

Tập dữ liệu tham chiếu A B C
NSN2 10 42 42
Nconf 2026 8600 21151
Ntotal atom 22983 100117 190065
Nelem 4 10 10
Eref range/meV atom−1 1715.3 1855.0 2018.0
Eref std/meV atom−1 382.6 373.8 391.5
Fref α,n,range/meVÅ−1 29883 29984 29998
Fref α,n,std/meVÅ−1 1098 1066 1706

4.2. Hàm Đối xứng Trung tâm Nguyên tử Bao trọn Nguyên tố

Đối với bốn nguyên tố, kích thước của vector ACSF được tối ưu hóa (156) và vector eeACSF (153) tương tự (xem Bảng S2 trong Thông tin Hỗ trợ cho các tham số của ACSF). Do đó, số lượng nguyên tố này là điểm chuyển giao mà tại đó biểu diễn eeACSF trở nên có lợi về mặt tính toán. Bảng 3 cho thấy rằng biểu diễn bằng ACSF và eeACSF của tập dữ liệu tham chiếu A mang lại HDNNP với RMSE tương tự cho dữ liệu kiểm tra biện minh cho biểu diễn cấu trúc bằng eeACSF (xem Hình S3 (a) và (b) trong Thông tin Hỗ trợ cho phân phối lỗi dự đoán của tập hợp). Độ chính xác của HDNNP sử dụng ACSF trung bình tốt hơn một chút. Xu hướng này được mong đợi do việc tách hoàn toàn các đóng góp từ các tổ hợp nguyên tố khác nhau, trong khi những đóng góp này được trộn lẫn trong eeACSF. Tuy nhiên, biểu diễn eeACSF ít dễ bị quá khớp hơn theo RMSE huấn luyện và kiểm tra đã cho. Lý do có thể là mọi giá trị eeACSF phụ thuộc vào tất cả nguyên tử lân cận bên trong bán kính cắt, trong khi giá trị ACSF chỉ phụ thuộc vào một số nguyên tử lân cận nhất định. Cái sau có thể điều chỉnh tốt hơn với các môi trường rất cụ thể nhưng làm xấu đi tổng quát hóa và khả năng chuyển giao. Hình S4 (a) và (b) và S5 trong Thông tin Hỗ trợ cho thấy rằng hội tụ và quá trình huấn luyện tương tự cho biểu diễn ACSF và eeACSF.

Bảng 3: Giá trị RMSE của HDNNP riêng lẻ, tức là trước khi tập hợp, và tập hợp được huấn luyện trên tập dữ liệu tham chiếu A sử dụng ACSF hoặc eeACSF. Bộ tối ưu hóa CoRe và lựa chọn dữ liệu thích ứng trọn đời được áp dụng cho 2000 epoch.

HDNNP Riêng lẻ ACSF eeACSF
RMSE( Etrain)/meV atom−1 2.0±0.2 2.5±0.3
RMSE( Etest)/meV atom−1 2.3±0.2 2.8±0.3
RMSE( Ftrain α,n)/meVÅ−1 59±3 73 ±3
RMSE( Ftest α,n)/meVÅ−1 87±9 92 ±8

Tập hợp
RMSE( E)/meV atom−1 0.9 1.3
RMSE( Fα,n)/meVÅ−1 34 44

Để huấn luyện tập dữ liệu tham chiếu B chứa mười nguyên tố, kích thước của vector ACSF sẽ là 750 để có được độ phân giải tương tự bằng các tham số ηrad, ηang, λ, và ξ. Yêu cầu tính toán cao kết quả có thể được ngăn chặn bằng cách áp dụng eeACSF với kích thước vector không đổi 153. Độ chính xác của HDNNP riêng lẻ là RMSE( Etrain) = (3.9±0.4) meV atom−1, RMSE( Etest) = (4.5±0.6) meV atom−1, RMSE( Ftrain α,n) = (99 ±7) meVÅ−1, và RMSE( Ftest α,n) = (116 ±4) meVÅ−1. Độ chính xác cao hơn của kết quả trong Bảng 3 là lý do của dữ liệu tham chiếu ít hơn và ít phức tạp hơn cần được huấn luyện, trong khi kiến trúc mô hình và siêu tham số huấn luyện không đổi. Tuy nhiên, đặc biệt độ chính xác tập hợp HDNNP của RMSE( E) = 2.6 meV atom−1 và RMSE( Fα,n) = 64 meV Å−1 (xem Hình S6 (a) và (b) trong Thông tin Hỗ trợ cho phân phối lỗi dự đoán) có thể so sánh với các MLP hiện đại khác được huấn luyện cho ít nguyên tố hơn [19] và chứng minh rằng eeACSF có thể biểu diễn các môi trường nguyên tử cục bộ khác nhau bao gồm các nguyên tố lân cận khác nhau. Hơn nữa, độ chính xác được cải thiện đáng kể của tập hợp HDNNP so với HDNNP riêng lẻ hỗ trợ việc sử dụng tập hợp vượt ra ngoài việc tiếp cận định lượng không chắc chắn. Để so sánh hiệu suất HDNNP với các MLP khác, chúng tôi tham khảo Tài liệu tham khảo 42 và 64.

RMSE năng lượng hơi lớn hơn bình thường do phạm vi năng lượng tương đối rộng cần được huấn luyện (Bảng 2). Hơn nữa, phạm vi năng lượng trung bình cho các hệ thống phản ứng SN2 riêng lẻ cũng rộng với 747 meV atom−1 cho những hệ thống có nguyên tử carbon methyl trung tâm và 438 meV atom−1 cho những hệ thống có nguyên tử carbon tert-butyl trung tâm. RMSE thành phần lực nguyên tử hơi thấp hơn bình thường mặc dù phạm vi rộng vì một phần đáng kể của lực gần bằng không.

4.3. Bộ tối ưu hóa Liên tục Có khả năng Phục hồi (CoRe)

MLP chính xác chỉ có thể thu được nếu bộ tối ưu hóa có thể tìm hiệu quả và đáng tin cậy các giá trị trọng số phù hợp trong không gian tham số đa chiều cao. Huấn luyện với tỷ lệ học cố định như trong SGD mang lại HDNNP có độ chính xác kém. Hầu hết các giá trị RMSE của kết quả SGD cho tập dữ liệu tham chiếu B được liệt kê trong Bảng 4 gần như lớn hơn một bậc độ lớn so với kết quả CoRe làm nổi bật tầm quan trọng của bộ tối ưu hóa. Chúng tôi tìm thấy RPROP và bộ tối ưu hóa Adam là các bộ tối ưu hóa hoạt động tốt nhất có sẵn trong PyTorch 1.12.1 cho mô hình học máy và dữ liệu tham chiếu đã cho. Chúng tôi lưu ý rằng RPROP được dành cho học lô trên tất cả dữ liệu cùng một lúc, trong khi chúng tôi thực hiện tối ưu hóa ngẫu nhiên với RPROP. RPROP hội tụ nhanh và mượt mà, nhưng đạt đỉnh ở độ chính xác không thỏa mãn (Hình 2 (a) và (b)). Ngược lại, bộ tối ưu hóa Adam yêu cầu nhiều bước hơn để đạt độ chính xác của RPROP, nhưng nó có thể đạt các giá trị RMSE thấp hơn cuối cùng. Tuy nhiên, hội tụ có nhiễu và do đó cản trở các ứng dụng liên tục.

Bộ tối ưu hóa CoRe của chúng tôi hội tụ thậm chí nhanh hơn RPROP và đạt độ chính xác cuối cùng tốt hơn bộ tối ưu hóa Adam (Hình 2 (a) và (b)). Hội tụ vẫn gần như mượt mà như RPROP. Do đó, CoRe kết hợp và cải thiện lợi ích của cả hai, RPROP và Adam. Chúng tôi lưu ý rằng những xu hướng này cũng đúng cho lựa chọn dữ liệu huấn luyện ngẫu nhiên (xem Hình S7 (a) và (b) trong Thông tin Hỗ trợ) thay vì lựa chọn dữ liệu thích ứng trọn đời cơ bản của kết quả trong Hình 2 (a) và

Bảng 4: Giá trị RMSE của HDNNP riêng lẻ và tập hợp được huấn luyện trên tập dữ liệu tham chiếu B sử dụng các bộ tối ưu hóa SGD, RPROP, Adam, và CoRe. Các bộ tối ưu hóa và lựa chọn dữ liệu thích ứng trọn đời được áp dụng cho 1500 (RPROP) và 2000 (SGD, Adam, CoRe) epoch.

HDNNP Riêng lẻ SGD RPROP Adam CoRe
RMSE(Etrain)/meV atom−1 37±7 9.5±0.7 6.1±1.5 3.9±0.4
RMSE(Etest)/meV atom−1 37±7 10.0±0.7 6.4±1.5 4.5±0.6
RMSE(Ftrain α,n)/meVÅ−1 564±17 191±5 119±8 99±7
RMSE(Ftest α,n)/meVÅ−1 557±11 205±8 127±8 116±4

Tập hợp
RMSE( E)/meV atom−1 33 6.8 3.9 2.6
RMSE( Fα,n)/meVÅ−1 529 131 95 64

(b).

4.4. Lựa chọn Dữ liệu Thích ứng Trọn đời

Giảm tập dữ liệu liên tục là thiết yếu cho học máy trọn đời để giữ lượng dữ liệu huấn luyện để luyện tập ở mức có thể quản lý được trong quá trình học tăng dần dữ liệu mới. Hình 3 tiết lộ cách tập dữ liệu huấn luyện được thu hẹp trong quá trình khớp. Giảm dữ liệu huấn luyện bắt đầu sau khoảng 600 epoch vì để loại trừ dữ liệu, yếu tố lựa chọn thích ứng Sr hist cần được giảm dưới Smin hist hoặc tăng trên Smax hist bởi một số ứng dụng liên tiếp được chỉ định của các yếu tố giảm hoặc tăng, tương ứng (Thuật toán 2). Tối ưu hóa với RPROP đạt đỉnh đã sau khoảng 600 epoch. Do đó, loại trừ dữ liệu quá nhanh trong các epoch tiếp theo, vì đóng góp mất mát của hầu hết cấu trúc không thay đổi nhiều, làm thiên vị đánh giá tầm quan trọng bởi các yếu tố lựa chọn thích ứng. Ngược lại, các biến động trong hành vi hội tụ của các tối ưu hóa với bộ tối ưu hóa Adam dẫn đến giảm chậm dữ liệu huấn luyện. Lý do cho điều này là các biện pháp tầm quan trọng dữ liệu cũng trải qua các biến động cản trở việc vượt qua các ngưỡng loại trừ. Giảm dữ liệu huấn luyện của các tối ưu hóa sử dụng CoRe nằm giữa RPROP và Adam mang lại quá trình cân bằng hơn. Số lượng cấu trúc bị loại trừ mỗi epoch cũng giảm cho các giai đoạn huấn luyện tiến bộ trong các epoch sau làm cho việc huấn luyện ổn định hơn (xem cũng Hình S5 trong Thông tin Hỗ trợ). Ngược lại, các tối ưu hóa với RPROP trở nên không ổn định sau khoảng 1500 epoch do giảm dữ liệu quá nhanh và mạnh.

Hình 3: Giảm tập dữ liệu huấn luyện của các bộ tối ưu hóa Adam, RPROP, và CoRe để huấn luyện tập dữ liệu tham chiếu B. Số lượng cấu trúc huấn luyện được xem xét Ntrain được hiển thị như một hàm của epoch huấn luyện nepoch. Các giá trị Ntrain của HDNNP riêng lẻ được đại diện bởi các chấm, trong khi trung bình của chúng được hiển thị bằng đường liền. Đường gạch ngang đen đại diện cho số lượng cấu trúc huấn luyện được sử dụng để khớp trong mỗi epoch.

Trong việc huấn luyện tập dữ liệu tham chiếu B sử dụng bộ tối ưu hóa CoRe, lựa chọn dữ liệu thích ứng trọn đời phân công trung bình (5.5±0.3)·10^3 cấu trúc huấn luyện là dư thừa sau 2000 epoch. Do đó, dữ liệu huấn luyện được giảm xuống 29% lượng ban đầu. Hình 2 (a) và (b) cho thấy rằng việc giảm dữ liệu này không dẫn đến suy giảm độ chính xác, điều có thể được mong đợi nếu huấn luyện được thực hiện trên một tập con không đại diện của các cấu trúc huấn luyện. Mặc dù giảm mạnh lượng dữ liệu, lựa chọn dữ liệu thích ứng trọn đời cải thiện đáng kể độ chính xác huấn luyện so với lựa chọn dữ liệu ngẫu nhiên dựa trên tất cả dữ liệu huấn luyện (xem Bảng S5 và Hình S7 (a) và (b) trong Thông tin Hỗ trợ). Xu hướng này được quan sát cho tất cả các bộ tối ưu hóa. Đối với bộ tối ưu hóa CoRe, lựa chọn dữ liệu thích ứng trọn đời mang lại cải thiện các giá trị RMSE tập hợp 31% cho năng lượng và 48% cho các thành phần lực nguyên tử so với lựa chọn dữ liệu ngẫu nhiên.

Trung bình 38±11 cấu trúc bị loại trừ khỏi huấn luyện vì mô hình không thể biểu diễn những cấu trúc này với độ chính xác cao. Theo cách này, việc cản trở quá trình huấn luyện bởi những cấu trúc này có thể được tránh. Vì cùng 24 cấu trúc bị loại trừ trong hơn một nửa các quá trình huấn luyện, những cấu trúc này có khả năng đáng ngờ. Các cấu trúc chỉ bị loại trừ bởi một số HDNNP riêng lẻ của tập hợp vẫn có thể được dự đoán bằng trung bình tập hợp (xem Hình S6 (a) và (b) trong Thông tin Hỗ trợ). Theo cách này, các quá trình huấn luyện riêng lẻ có thể được cải thiện, trong khi tổng quát hóa của dự đoán tập hợp vẫn được cung cấp. Độ không chắc chắn phát sinh đối với một số cấu trúc do việc loại trừ chúng trong một số huấn luyện HDNNP được bao phủ bởi định lượng không chắc chắn. Do đó, phương pháp này không ảnh hưởng đến độ tin cậy của phương pháp.

4.5. Tiềm năng Học máy Trọn đời

Ba trường hợp ví dụ thường xảy ra được khám phá, trong đó học trọn đời có thể có lợi so với các chu kỳ lặp của mở rộng tập dữ liệu và xây dựng MLP mới được huấn luyện trên tất cả dữ liệu. Những trường hợp này đại diện cho hoàn thành dữ liệu huấn luyện của không gian cấu trúc được lấy mẫu thưa thớt, mở rộng không gian cấu trúc được đại diện cho cùng hệ thống hóa học, và học các hệ thống hóa học bổ sung. Học trọn đời có thể thêm số lượng điểm dữ liệu mới tùy ý trong mỗi epoch huấn luyện và không phải được áp dụng theo sơ đồ khối như được sử dụng trong học tích cực thông thường cho MLP. Tuy nhiên, trong công trình này, chúng tôi chỉ thêm dữ liệu mới trong một epoch huấn luyện đơn lẻ để đặc tính hóa rõ ràng các hiệu ứng kết quả.

Không gian cấu trúc được lấy mẫu thưa thớt được thu được cho các epoch huấn luyện ban đầu khi một phần ngẫu nhiên cao của dữ liệu huấn luyện có sẵn lần đầu tiên tại một epoch muộn. Hình 4 (a) và (b) cho thấy rằng các chiến lược học trọn đời được đề xuất có thể xử lý trường hợp này rất tốt mang lại độ chính xác lMLP cuối cùng gần như không đổi đối với phần dữ liệu muộn plate. Số lượng epoch, sau đó phần dữ liệu muộn được thêm vào, thấp hơn cho plate cao vì nếu không phần nhỏ dữ liệu ban đầu sẽ bị quá khớp. Hình S8 (b) và S9 trong Thông tin Hỗ trợ cho thấy rằng dữ liệu bổ sung được thêm vào tại những epoch mà độ chính xác của các thành phần lực nguyên tử kiểm tra đạt đỉnh hoặc thậm chí tăng. Độ chính xác tập hợp trung bình cho plate∈[0.5,0.8] là RMSE( E) = 2.6 meV atom−1 và RMSE( Fα,n) = 68 meV Å−1 và do đó rất tương tự với plate= 0 (Bảng 4). Ngoài tính linh hoạt đạt được trong quá trình huấn luyện, phương pháp học trọn đời yêu cầu ít dữ liệu huấn luyện cần được xử lý trong các epoch ban đầu so với huấn luyện trên tất cả dữ liệu. Hình S9 trong Thông tin Hỗ trợ tiết lộ rằng số lượng cấu trúc bị loại trừ bởi lựa chọn dữ liệu thích ứng trọn đời tương tự sau 1500 epoch cho các giá trị khác nhau của plate.

Hình 4: Độ chính xác cuối cùng của lMLP mà một phần plate dữ liệu huấn luyện của tập dữ liệu tham chiếu B có sẵn lần đầu tiên tại epoch huấn luyện muộn. Những dữ liệu này được chọn ngẫu nhiên hoặc một khối nhất định được sử dụng. Thông tin chi tiết hơn về quy trình được cung cấp trong văn bản chính. Các giá trị RMSE kiểm tra của (a) năng lượng Etest và (b) các thành phần lực nguyên tử Ftest α,n được hiển thị như một hàm của phần dữ liệu muộn plate. Các giá trị RMSE của HDNNP riêng lẻ được đại diện bởi các chấm, trung bình của chúng bằng đường liền, và phạm vi của chúng bằng dải màu nhạt hơn. Bộ tối ưu hóa CoRe (βb 1= 0.7 cho "Khối" với plate>0 và βb 1= 0.725 nếu ngược lại) và lựa chọn dữ liệu thích ứng trọn đời được áp dụng cho 1500 epoch.

Để kiểm tra hiệu suất cho trường hợp mở rộng không gian cấu trúc được đại diện cho cùng hệ thống hóa học, một lMLP được huấn luyện đầu tiên trên tập dữ liệu tham chiếu B cho 1250 epoch. Sau đó, các cấu trúc biến dạng cấu trúc bổ sung của tập dữ liệu tham chiếu C được thêm vào và huấn luyện được tiếp tục thêm 1250 epoch. Bảng 5 tiết lộ rằng học trọn đời mang lại các giá trị RMSE cho HDNNP riêng lẻ cao hơn khoảng 13% so với học trên lô tĩnh của tất cả dữ liệu huấn luyện của tập dữ liệu tham chiếu C (xem Hình S10 (a) và (b) và S11 trong Thông tin Hỗ trợ cho quá trình huấn luyện). Tuy nhiên, hầu hết độ chính xác bị mất này được lấy lại bởi mô hình tập hợp giảm hiệu quả phương sai mô hình tăng (xem Bảng 5 và Hình S12 (a) và (b) trong Thông tin Hỗ trợ cho phân phối lỗi dự đoán). Do đó, khái niệm lMLP có thể mở rộng không gian cấu trúc được đại diện, trong khi nó giữ lại độ chính xác.

Bảng 5: Giá trị RMSE của HDNNP riêng lẻ và tập hợp được huấn luyện trên tập dữ liệu tham chiếu C sử dụng học trên lô tĩnh của tất cả dữ liệu huấn luyện và học trọn đời. Trong học trọn đời, chỉ các cấu trúc của tập dữ liệu tham chiếu B được huấn luyện cho 1250 epoch ban đầu và sau đó các cấu trúc bổ sung của tập dữ liệu tham chiếu C được thêm vào. Bộ tối ưu hóa CoRe với βb 1= 0.7 và lựa chọn dữ liệu thích ứng trọn đời được áp dụng cho 2500 epoch.

HDNNP Riêng lẻ Dữ liệu Học
Tĩnh trọn đời
RMSE( Etrain)/meV atom−1 6.1±0.2 6.9±0.6
RMSE( Etest)/meV atom−1 6.8±0.2 7.8±0.5
RMSE( Ftrain α,n)/meVÅ−1 168±3 185±12
RMSE( Ftest α,n)/meVÅ−1 182±4 205±10

Tập hợp
RMSE( E)/meV atom−1 4.3 4.5
RMSE( Fα,n)/meVÅ−1 121 122

Các giá trị RMSE cao hơn thu được trong việc huấn luyện tập dữ liệu tham chiếu C so với B là kết quả của phạm vi năng lượng thậm chí lớn hơn và phân phối thành phần lực nguyên tử rộng hơn (xem Bảng 2 và Bảng S4 trong Thông tin Hỗ trợ), trong khi kiến trúc mô hình và siêu tham số huấn luyện không đổi. Tuy nhiên, đặc biệt RMSE thành phần lực nguyên tử tập hợp tương tự với các HDNNP hiện đại khác được huấn luyện cho ít nguyên tố hơn [19].

Để điều tra hiệu quả cho việc học các hệ thống hóa học bổ sung, tập dữ liệu tham chiếu B được sắp xếp theo hệ thống được chia thành hai khối, do đó phần của khối thứ hai là plate. Các cấu trúc được sắp xếp theo thứ tự alphabet theo thứ tự nguyên tử carbon trung tâm, nhóm rời, và nucleophile. Học các phản ứng SN2 bổ sung cho nguyên tử carbon tert-butyl trung tâm, tức là thêm nucleophile và nhóm rời tại epoch muộn chỉ được biết cho nguyên tử carbon methyl trung tâm, mang lại độ chính xác tương tự như học trên tất cả dữ liệu từ đầu ( plate<0.5 trong Hình 4 (a) và (b)). Cho plate≥0.5 chỉ các phản ứng với nguyên tử carbon methyl trung tâm được chứa trong dữ liệu huấn luyện ban đầu dẫn đến tăng các giá trị RMSE kiểm tra cuối cùng. Chúng tôi nhấn mạnh rằng cho plate≥0.8 cũng có một số nguyên tố bị thiếu trong dữ liệu huấn luyện ban đầu. Tuy nhiên, độ chính xác tốt hơn so với thu được bằng RPROP và đối với năng lượng nó tương tự với kết quả Adam (Bảng 4).

Tương tự như trường hợp đã đề cập, tập hợp có thể giảm hiệu quả phương sai mô hình được giới thiệu bởi học tăng dần và do đó là một công cụ quan trọng cho học máy trọn đời. Độ chính xác tập hợp là RMSE( E) = 3.1 meV atom−1 và RMSE( Fα,n) = 78 meV Å−1 cho plate∈[0.5,0.8] và do đó khoảng 21% lớn hơn plate= 0. Tuy nhiên, trong những trường hợp học các hệ thống hóa học bổ sung này, các chiến lược học trọn đời yêu cầu phát triển thêm để ngang bằng với độ chính xác của huấn luyện trên tất cả dữ liệu.

Vì công trình này là bằng chứng về khái niệm cho lMLP, nhiều hệ thống hóa học khác nhau hơn cần được khám phá trong công trình tương lai để tinh chỉnh và cải thiện các chiến lược học trọn đời. Tuy nhiên, chúng tôi đã cho thấy rằng học trọn đời có thể đạt độ chính xác tương tự như huấn luyện trên tất cả dữ liệu, trong khi tính linh hoạt của quá trình huấn luyện được tăng đáng kể.

4.6. Dự đoán Tập hợp và Định lượng Không chắc chắn

Để xác thực dự đoán tập hợp và kiểm tra định lượng không chắc chắn, lMLP được huấn luyện bởi bộ tối ưu hóa CoRe trên tập dữ liệu tham chiếu B được áp dụng trên các cấu trúc thu được từ tối ưu hóa DFT có ràng buộc trên lưới dày đặc của khoảng cách rCl−C và rBr−C cho phản ứng SN2 Br–+ CH3ClBrCH3+ Cl–. Chúng tôi nhấn mạnh rằng tập dữ liệu tham chiếu B sử dụng lấy mẫu thưa thớt và không đều của phản ứng này và các cấu trúc xác thực không có khả năng trong tập dữ liệu tham chiếu B. Do đó, độ mượt của bề mặt năng lượng tiềm năng lMLP có thể được xác thực. Tập xác thực chứa 1062 cấu trúc với các thành phần lực nguyên tử tối đa 16 eVÅ−1.

Hình 5: Bề mặt năng lượng tiềm năng của phản ứng SN2 Br–+ CH3ClBrCH3+ Cl–. Năng lượng dự đoán tập hợp lMLP E được tham chiếu đến năng lượng DFT tham chiếu tối thiểu Eref min của không gian cấu trúc đã cho bao trùm bởi các cấu trúc được tối ưu hóa DFT với khoảng cách có ràng buộc rCl−C và rBr−C. Màu sắc đại diện cho lỗi của E đối với năng lượng DFT tham chiếu Eref. Các chấm đen hiển thị các đánh giá rõ ràng của lMLP. Các màu giữa các chấm đen được nội suy.

Hình 5 tiết lộ rằng hầu hết các phần của bề mặt năng lượng tiềm năng được đại diện được dự đoán trong độ chính xác hóa học, tức là 1 kcal mol−1= 4.184 kJ mol−1, đối với năng lượng DFT tham chiếu. Đối với hệ thống sáu nguyên tử này, lỗi ít hơn 7.2 meV atom−1 do đó được yêu cầu. Lỗi lớn hơn chỉ được quan sát đối với các cấu trúc năng lượng cao, mà chúng tôi mong đợi vì thành phần lực nguyên tử tối đa có thể cao hơn 1 eVÅ−1 so với dữ liệu huấn luyện. Các lỗi nhỏ với phân phối mượt mà trong không gian cấu trúc được huấn luyện chứng minh độ mượt của lMLP.

Độ tin cậy của định lượng không chắc chắn được giải quyết trong Hình 6 (a) và (b). Định lượng không chắc chắn bằng hoặc lớn hơn lỗi tuyệt đối đối với năng lượng DFT tham chiếu cho 98.6% dữ liệu xác thực với độ không chắc chắn ΔE≤10 meV atom−1. Đối với các thành phần lực nguyên tử, phần này là 99.7% cho ΔFα,n≤250 meV Å−1. Do đó, đối với hầu hết cấu trúc, độ lớn của lỗi được dự đoán đáng tin cậy. Đối với lỗi lớn, định lượng không chắc chắn được mong đợi đánh giá thấp các lỗi (xem Phần 2.6) để các phần đã đề cập giảm xuống 66% cho ΔE >

Hình 6: Giá trị tuyệt đối của lỗi đối với tham chiếu DFT và định lượng không chắc chắn cho dự đoán tập hợp của (a) năng lượng |ΔE| và (b) các thành phần lực nguyên tử |ΔFα,n| của dữ liệu xác thực. Thứ tự x sắp xếp các cấu trúc xác thực theo định lượng không chắc chắn của chúng.

10 meV atom−1 và 93% cho ΔFα,n>250 meV Å−1. Tuy nhiên, nhận dạng đáng tin cậy của lỗi lớn vẫn được cung cấp.

5. KẾT LUẬN

Công trình này giới thiệu khái niệm lMLP có thể tinh chỉnh và mở rộng biểu diễn của chúng theo cách lăn. Do đó, khái niệm lMLP kết hợp hiệu quả và độ chính xác mô hình MLP với tính linh hoạt. Đối với lMLP, biểu diễn cấu trúc nguyên tử phổ quát và hiệu quả tính toán, mô hình MLP, định lượng không chắc chắn, và các chiến lược học trọn đời cần được kết hợp.

Do đó, chúng tôi đã giới thiệu các vector eeACSF cho biểu diễn cấu trúc, độc lập kích thước đối với số lượng nguyên tố hóa học trái ngược với nhiều bộ mô tả MLP phổ biến khác. Hiệu suất biểu diễn của chúng đối với tập dữ liệu tham chiếu SN2 tương tự với các vector ACSF tại điểm hòa vốn của chi phí tính toán, đó là khoảng bốn nguyên tố khác nhau. Đối với nhiều nguyên tố hơn, các vector eeACSF trở thành lựa chọn hợp lý duy nhất về mặt tính toán do sự tăng trưởng tổ hợp của các vector ACSF. Mô hình HDNNP tập hợp sử dụng eeACSF có thể dự đoán tập dữ liệu tham chiếu SN2 với mười nguyên tố khác nhau với độ chính xác hiện đại của các HDNNP trước đó được huấn luyện trên ít nguyên tố hơn. Hơn nữa, tập hợp HDNNP là cách đáng tin cậy để định lượng không chắc chắn do phương sai mô hình và xác định các cấu trúc có độ không chắc chắn cao trong dự đoán. Ngoài ra, tập hợp tăng độ chính xác mang lại bề mặt năng lượng tiềm năng với độ chính xác hóa học cho các phản ứng SN2.

Như cơ sở cho các chiến lược học trọn đời của chúng tôi, chúng tôi đã giới thiệu bộ tối ưu hóa CoRe có thể kết hợp và cải thiện hội tụ nhanh của RPROP và độ chính xác cuối cùng cao của bộ tối ưu hóa Adam. Trong việc huấn luyện công trình này, bộ tối ưu hóa CoRe cải thiện đáng kể độ chính xác tập hợp HDNNP khoảng 33% cho năng lượng và lực so với bộ tối ưu hóa Adam. Áp dụng lựa chọn dữ liệu thích ứng trọn đời cải thiện thêm độ chính xác và cho phép thu hẹp tập dữ liệu huấn luyện và loại trừ dữ liệu đáng ngờ trong quá trình huấn luyện. Bộ tối ưu hóa CoRe và lựa chọn dữ liệu thích ứng trọn đời cũng có thể cải thiện huấn luyện các mô hình học máy ngoài lMLP.

Cuối cùng, lMLP có thể thích ứng với dữ liệu bổ sung có thể được thêm vào liên tục tại bất kỳ điểm nào trong quá trình huấn luyện. Theo cách này, cải thiện lMLP có thể mà không học lại trên tất cả dữ liệu trước đó và vẫn có được phương pháp đáng tin cậy do định lượng không chắc chắn. Trong các trường hợp học thu được trong học tích cực hoặc trong việc mở rộng không gian cấu trúc cho cùng hệ thống phản ứng, độ chính xác huấn luyện tương tự với học trên lô tĩnh của tất cả dữ liệu. Thậm chí thêm các hệ thống phản ứng mới có thể được thực hiện bởi các thuật toán được trình bày với chỉ mất độ chính xác vừa phải. Lợi ích của học trọn đời là tính linh hoạt tăng cường của quá trình huấn luyện cho phép khám phá lăn về phản ứng hóa học và tiếp tục huấn luyện của lMLP trước đó. Hơn nữa, khả năng thích ứng của lMLP đặc biệt có lợi cho các tập dữ liệu tham chiếu lớn nơi huấn luyện trên tất cả dữ liệu cùng một lúc rất đòi hỏi về mặt tính toán. Chúng tôi nhấn mạnh rằng khái niệm lMLP cũng có thể được áp dụng cho các phương pháp MLP khác ngoài HDNNP trong công trình tương lai.

LỜI CẢM ƠN

Công trình này được hỗ trợ bởi Học bổng Sau tiến sĩ ETH Zurich.

[1] C. J. Cramer, Essentials of Computational Chemistry: Theories and Models (Wiley, 2013).
[2] F. Jensen, Introduction to Computational Chemistry (Wiley, 2017).
[3] K. Burke, Perspective on density functional theory, J. Chem. Phys. 136, 150901 (2012).
[4] C. Riplinger, P. Pinski, U. Becker, E. F. Valeev, và F. Neese, Sparse maps—a systematic infrastructure for reduced-scaling electronic structure methods. II. Linear scaling domain based pair natural orbital coupled cluster theory, J. Chem. Phys. 144, 024109 (2016).
[5] S. Das, P. Motamarri, V. Gavini, B. Turcksin, Y. W. Li, và B. Leback, Fast, scalable and accurate finite-element based ab initio calculations using mixed precision computing: 46 PFLOPS simulation of a metallic dislocation system., trong International Conference for High Performance Computing, Networking, Storage and Analysis (SC19) (Denver, CO, USA, 2019).
[6] A. C. T. van Duin, S. Dasgupta, F. Lorant, và W. A. Goddard, ReaxFF: A reactive force field for hydrocarbons, J. Phys. Chem. A 105, 9396 (2001).
[7] S. Piana, K. Lindorff-Larsen, và D. E. Shaw, How robust are protein folding simulations with respect to force field parameterization?, Biophys. J. 100, L47 (2011).
[8] P. Friederich, F. Häse, J. Proppe, và A. Aspuru-Guzik, Machine-learned potentials for next-generation matter simulations, Nat. Mater. 20, 750 (2021).
[9] J. Behler và G. Csányi, Machine learning potentials for extended systems - a perspective, Eur. Phys. J. B 94, 142 (2021).
[10] J. Behler, Perspective: Machine learning potentials for atomistic simulations, J. Chem. Phys. 145, 170901 (2016).
[11] A. P. Bartók, S. De, C. Poelking, N. Bernstein, J. R. Kermode, G. Csányi, và M. Ceriotti, Machine learning unifiesthemodelingofmaterialsandmolecules,Sci.Adv. 3, e1701816 (2017).
[12] V. L. Deringer, M. A. Caro, và G. Csányi, Machine learning interatomic potentials as emerging tools for materials science, Adv. Mater. 31, 1902765 (2019).
[13] F. Noé, A. Tkatchenko, K.-R. Müller, và C. Clementi, Machine learning for molecular simulation, Ann. Rev. Phys. Chem. 71, 361 (2020).
[14] J. Westermayr, M. Gastegger, K. T. Schütt, và R. J. Maurer, Perspective on integrating machine learning into computational chemistry and materials science, J. Chem. Phys.154, 230903 (2021).
[15] S. Käser, L. I. Vazquez-Salazar, M. Meuwly, và K. Töpfer, Neural network potentials for chemistry: concepts, applications and prospects, Digital Discovery 2, 28 (2023).
[16] T. B. Blank, S. D. Brown, A. W. Calhoun, và D. J. Doren, Neural network models of potential energy surfaces, J. Chem. Phys. 103, 4129 (1995).
[17] J. Behler và M. Parrinello, Generalized neural-network representation of high-dimensional potential-energy surfaces, Phys. Rev. Lett. 98, 146401 (2007).
[18] J. Behler, First principles neural network potentials for reactive simulations of large molecular and condensed systems, Angew. Chem. Int. Ed. 56, 12828 (2017).
[19] J. Behler, Four generations of high-dimensional neural network potentials, Chem. Rev. 121, 10037 (2021).
[20] K. T. Schüett, F. Arbabzadah, S. Chmiela, K. R. Müller, và A. Tkatchenko, Quantum-chemical insights from deep tensor neural networks, Nat. Commun. 8, 13890 (2017).
[21] O. T. Unke và M. Meuwly, PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges, J. Chem. Theory Comput. 15, 3678 (2019).
[22] O. T. Unke, S. Chmiela, M. Gastegger, K. T. Schütt, H. E. Sauceda, và K.-R. Müller, SpookyNet: Learning force fields with electronic degrees of freedom and nonlocal effects, Nat. Commun. 12, 7273 (2021).
[23] I. Batatia, D. P. Kovács, G. N. C. Simm, C. Ortner, và G. Csányi, MACE: Higher order equivariant message passing neural networks for fast and accurate force fields, arXiv:2206.07697 [stat.ML] (2022).
[24] A. P. Bartók, M. C. Payne, R. Kondor, và G. Csányi, Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons, Phys. Rev. Lett.104, 136403 (2010).
[25] A. V. Shapeev, Moment tensor potentials: a class of systematicallyimprovableinteratomicpotentials,Multiscale Model. Simul. 14, 1153 (2016).
[26] N. Artrith, T. Morawietz, và J. Behler, High-dimensional neural-network potentials for multicomponent systems: Applications to zinc oxide, Phys. Rev. B 83, 153101 (2011).
[27] S. A. Ghasemi, A. Hofstetter, S. Saha, và S. Goedecker, Interatomic potentials for ionic systems with density functional accuracy based on charge densities obtained by a neural network, Phys. Rev. B 92, 045131 (2015).
[28] T. W. Ko, J. A. Finkler, S. Goedecker, và J. Behler, A fourth-generation high-dimensional neural network potential with accurate electrostatics including non-local charge transfer, Nat. Commun. 12, 398 (2021).
[29] N. Artrith và J. Behler, High-dimensional neural network potentials for metal surfaces: A prototype study for copper, Phys. Rev. B 85, 045439 (2012).
[30] J. Behler, Constructing high-dimensional neural network potentials: A tutorial review, Int. J. Quantum Chem. 115, 1032 (2015).
[31] E. V. Podryabinkin và A. V. Shapeev, Active learning of linearly parametrized interatomic potential, Comput. Mater. Sci. 140, 171 (2017).
[32] N. Bernstein, G. Csányi, và V. L. Deringer, De novo exploration and self-guided learning of potential-energy surfaces, npj Comput. Mater. 5, 99 (2019).
[33] M. Eckhoff và J. Behler, High-dimensional neural network potentials for magnetic systems using spin-dependent atom-centered symmetry functions, npj Comput. Mater. 7, 170 (2021).
[34] T. Morawietz, A. Singraber, C. Dellago, và J. Behler, How van der Waals interactions determine the unique properties of water, Proc. Natl. Acad. Sci. 113, 8368 (2016).
[35] M. Hellström và J. Behler, Concentration-dependent proton transfer mechanisms in aqueous NaOH solutions: From acceptor-driven to donor-driven and back, J. Phys. Chem. Lett. 7, 3302 (2016).
[36] B. Cheng, E. A. Engel, J. Behler, C. Dellago, và M. Ceriotti, Ab initio thermodynamics of liquid and solid water, Proc. Natl. Acad. Sci. 116, 1110 (2019).
[37] J.Westermayr, M.Gastegger, M.F.S.J.Menger, S.Mai, L. González, và P. Marquetand, Machine learning enables long time scale molecular photodynamics simulations, Chem. Sci. 10, 8100 (2019).
[38] S. Amabilino, L. A. Bratholm, S. J. Bennie, A. C. Vaucher, M. Reiher, và D. R. Glowacki, Training neural nets to learn reactive potential energy surfaces using interactive quantum chemistry in virtual reality, J. Phys. Chem. A123, 4486 (2019).
[39] M. Eckhoff và J. Behler, From molecular fragments to the bulk: Development of a neural network potential for MOF-5, J. Chem. Theory Comput. 15, 3793 (2019).
[40] M.Eckhoff, F.Schönewald, M.Risch, C.A.Volkert, P.E. Blöchl, và J. Behler, Closing the gap between theory andexperimentforlithiummanganeseoxidespinelsusing a high-dimensional neural network potential, Phys. Rev. B102, 174102 (2020).
[41] M. Eckhoff và J. Behler, Insights into lithium manganese oxide-water interfaces using machine learning potentials, J. Chem. Phys. 155, 244703 (2021).
[42] Y. Zuo, C. Chen, X. Li, Z. Deng, Y. Chen, J. Behler, G.Csányi, A.V.Shapeev, A.P.Thompson, M.A.Wood, và S. P. Ong, Performance and cost assessment of machine learning interatomic potentials, J. Phys. Chem. A 124, 731 (2020).
[43] J.S.Smith, O.Isayev,vàA.E.Roitberg,ANI-1: Anextensible neural network potential with DFT accuracy at force field computational cost, Chem. Sci. 8, 3192 (2017).
[44] S. Zhang, M. Z. Makoś, R. B. Jadrich, E. Kraka, K. M. Barros, B. T. Nebgen, S. Tretiak, O. Isayev, N. Lubbers, R. A. Messerly, và J. S. Smith, Exploring the frontiers of chemistry with a general reactive machine learning potential, 10.26434/chemrxiv-2022-15ct6-v2 (2022).
[45] D. Hassabis, D. Kumaran, C. Summerfield, và M. Botvinick, Neuroscience-inspired artificial intelligence, Neuron 95, 245 (2017).
[46] Z. Chen và B. Liu, Lifelong Machine Learning (Morgan & Claypool Publishers, 2018).
[47] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, và S. Wermter, Continual lifelong learning with neural networks: A review, Neural Netw. 113, 54 (2019).
[48] D. Maltoni và V. Lomonaco, Continuous learning in single-incremental-task scenarios, Neural Netw. 116, 56 (2019).
[49] S. Grossberg, Adaptive resonance theory: How a brain learns to consciously attend, learn, and recognize a changing world, Neural Netw. 37, 1 (2013).
[50] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, và Y. Bengio, An empirical investigation of catastrophic forgetting in gradient-based neural networks, arXiv:1312.6211v3 [stat.ML] (2015).
[51] T. L. Hayes, N. D. Cahill, và C. Kanan, Memory efficient experiencereplayforstreaminglearning,in International Conference on Robotics and Automation (ICRA) (Montreal, Canada, 2019) pp. 9769–9776.
[52] D. Lopez-Paz và M. A. Ranzato, Gradient episodic memory for continual learning, in 31stConference on Neural Information Processing Systems (NIPS) (Long Beach, CA, USA, 2017) pp. 6470–6479.
[53] F. Zenke, B. Poole, và S. Ganguli, Continual learning through synaptic intelligence, in 34thInternational Conference on Machine Learning (ICML) (Sydney, Australia, 2017) pp. 3987–3995.
[54] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J.Kirkpatrick, K.Kavukcuoglu, R.Pascanu,vàR.Hadsell, Progressive neural networks, arXiv:1606.04671v4 [cs.LG] (2022).
[55] G. I. Parisi, J. Tani, C. Weber, và S. Wermter, Lifelong learning of spatiotemporal representations with dual-memory recurrent self-organization, Front. Neurorobot. 12, 78 (2018).
[56] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Schütt, A. Tkatchenko, và K.-R. Müller, Machine learning force fields, Chem. Rev. 121, 10142 (2021).
[57] H. J. Kulik, T. Hammerschmidt, J. Schmidt, S. Botti, M. A. L. Marques, M. Boley, M. Scheffler, M. Todorović, P. Rinke, C. Oses, A. Smolyanyuk, S. Curtarolo, A. Tkatchenko, A. P. Bartók, S. Manzhos, M. Ihara, T. Carrington, J. Behler, O. Isayev, M. Veit, A. Grisafi, J. Nigam, M. Ceriotti, K. T. Schütt, J. Westermayr, M. Gastegger, R. J. Maurer, B. Kalita, K. Burke, R. Nagai, R. Akashi, O. Sugino, J. Hermann, F. Noé, S. Pilati, C. Draxl, M. Kuban, S. Rigamonti, M. Scheidgen, M. Esters, D. Hicks, C. Toher, P. V. Balachandran, I. Tamblyn, S. Whitelam, C. Bellinger, và L. M. Ghiringhelli, Roadmap on machine learning in electronic structure, Electron. Struct. 4, 023004 (2022).
[58] J. Behler, Atom-centered symmetry functions for constructing high-dimensional neural network potentials, J. Chem. Phys. 134, 074106 (2011).
[59] A. P. Bartók, R. Kondor, và G. Csányi, On representing chemical environments, Phys. Rev. B 87, 184115 (2013).
[60] F. Musil, A. Grisafi, A. P. Bartók, C. Ortner, G. Csányi, và M. Ceriotti, Physics-inspired structural representations for molecules and materials, Chem. Rev. 121, 9759 (2021).
[61] S. Gugler và M. Reiher, Quantum chemical roots of machine-learning molecular similarity descriptors, J. Chem. Theory Comput. 18, 6670 (2022).
[62] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, và G. Monfardini, The graph neural network model, IEEE Trans. Neural Netw. 20, 61 (2009).
[63] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, và G. E. Dahl, Neural message passing for quantum chemistry, in 34thInternational Conference on Machine Learning (ICML) (Sydney, Australia, 2017) pp. 1263–1272.
[64] C. Chen và S. P. Ong, A universal graph deep learning interatomic potential for the periodic table, Nat. Comput. Sci.2, 718 (2022).
[65] N. Artrith, A. Urban, và G. Ceder, Efficient and accurate machine-learning interpolation of atomic energies in compositionswithmanyspecies,Phys.Rev.B 96,014112 (2017).
[66] S. Rostami, M. Amsler, và S. A. Ghasemi, Optimized symmetry functions for machine-learning interatomic potentials of multicomponent systems, J. Chem. Phys. 149, 124106 (2018).
[67] M. Gastegger, L. Schwiedrzik, M. Bittermann, F. Berzsenyi, và P. Marquetand, wACSF–weighted atom-centered symmetry functions as descriptors in machine learning potentials, J. Chem. Phys. 148, 241709 (2018).
[68] F. A. Faber, A. S. Christensen, B. Huang, và O. A. von Lilienfeld, Alchemical and structural distribution based representation for universal quantum machine learning, J. Chem. Phys. 148, 241717 (2018).
[69] A. A. Peterson, R. Christensen, và A. Khorshidi, Addressing uncertainty in atomistic machine learning, Phys. Chem. Chem. Phys. 19, 10978 (2017).
[70] J. S. Smith, B. Nebgen, N. Lubbers, O. Isayev, và A. E. Roitberg, Less is more: Sampling chemical space with active learning, J. Chem. Phys. 148, 241733 (2018).
[71] J. S. Smith, B. T. Nebgen, R. Zubatyuk, N. Lubbers, C. Devereux, K. Barros, S. Tretiak, O. Isayev, và A. E. Roitberg, Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning, Nat. Commun. 10, 2903 (2019).
[72] C. Devereux, J. S. Smith, K. K. Huddleston, K. Barros, R. Zubatyuk, O. Isayev, và A. E. Roitberg, Extending the applicability of the ANI deep learning molecular potential to sulfur and halogens, J. Chem. Theory Comput. 16, 4192 (2020).
[73] G. Imbalzano, Y. Zhuang, V. Kapil, K. Rossi, E. A. Engel, F. Grasselli, và M. Ceriotti, Uncertainty estimation for molecular dynamics and sampling, J. Chem. Phys. 154, 074102 (2021).
[74] M.Reiher,Molecule-specificuncertaintyquantificationin quantum chemical studies, Isr. J. Chem. 62, e202100101 (2022).
[75] M. Eckhoff, K. N. Lausch, P. E. Blöchl, và J. Behler, Predicting oxidation and spin states by high-dimensional neural networks: Applications to lithium manganese oxide spinels, J. Chem. Phys. 153, 164107 (2020).
[76] M. Riedmiller và H. Braun, A direct adaptive method for faster backpropagation learning: The RPROP algorithm, in International Conference on Neural Networks (ICNN) (San Francisco, CA, USA, 1993) pp. 586–591.
[77] M. Riedmiller, Advanced supervised learning in multilayer perceptrons—from backpropagation to adaptive learning algorithms, Comput. Stand. Interfaces 16, 265 (1994).
[78] D. P. Kingma và J. Ba, Adam: A method for stochastic optimization, in 3rdInternational Conference on Learning Representations (ICLR) (San Diego, CA, USA, 2015).
[79] H. Robbins và S. Monro, A stochastic approximation method, Ann. Math. Stat. 22, 400 (1951).
[80] C. Igel và M. Hüsken, Improving the rprop learning algorithm, in Second International ICSC Symposium on Neural Computation (2000) pp. 115–121.
[81] E. Heid, C. J. McGill, F. H. Vermeire, và W. H. Green, Characterizing uncertainty in machine learning for chemistry, 10.26434/chemrxiv-2023-00vcg-v2 (2023).
[82] F. Musil, M. J. Willatt, M. A. Langovoy, và M. Ceriotti, Fast and accurate uncertainty estimation in chemical machine learning, J. Chem. Theory Comput. 15, 906 (2019).
[83] M. L. Paleico và J. Behler, A bin and hash method for analyzing reference data and descriptors in machinelearningpotentials,Mach.Learn.: Sci.Technol. 2,037001 (2021).
[84] J. Westermayr và P. Marquetand, Machine learning for electronically excited states of molecules, Chem. Rev. 121, 9873 (2021).
[85] F. Neese, The ORCA program system, WIREs Comput. Mol. Sci.2, 73 (2022).
[86] F. Neese, Software update: The ORCA program system—version5.0,WIREsComput.Mol.Sci. 12,e1606 (2022).
[87] J. P. Perdew, K. Burke, và M. Ernzerhof, Generalized gradient approximation made simple, Phys. Rev. Lett. 77, 3865 (1996).
[88] F. Weigend và R. Ahlrichs, Balanced basis sets of split valence, triple zeta valence and quadruple zeta valence quality for H to Rn: Design and assessment of accuracy, Phys. Chem. Chem. Phys. 7, 3297 (2005).
[89] S. Grimme, J. Antony, S. Ehrlich, và H. Krieg, A consistent and accurate ab initio parametrization of density functional dispersion correction (DFT-D) for the 94 elements H-Pu, J. Chem. Phys. 132, 154104 (2010).
[90] E. R. Johnson và A. D. Becke, A post-Hartree-Fock model of intermolecular interactions, J. Chem. Phys. 123, 024101 (2005).
[91] S. Grimme, S. Ehrlich, và L. Goerigk, Effect of the damping function in dispersion corrected density functional theory, J. Comput. Chem. 32, 1456 (2011).
[92] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. Fernández del Río, M. Wiebe, P. Peterson, P. Gérard-Marchant, K.Sheppard, T.Reddy, W.Weckesser, H.Abbasi, C. Gohlke, và T. E. Oliphant, Array programming with NumPy, Nature 585, 357 (2020).
[93] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, và S. Chintala, PyTorch: An imperative style, high-performance deep learning library, in 33rdInternational Conference on Neural Information Processing Systems (NIPS) (Vancouver, Canada, 2019) pp. 8026–8037.
