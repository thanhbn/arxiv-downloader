# 2305.13622.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2305.13622.pdf
# Kích thước tệp: 714453 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Liên Tục với Phương Pháp Phát Lại Trải Nghiệm Mạnh
Tao Zhuo1*, Zhiyong Cheng1, Zan Gao1, Hehe Fan2, Mohan Kankanhalli3
1Viện Trí tuệ Nhân tạo Shandong, Đại học Công nghệ Qilu (Viện Khoa học Shandong), Jinan, Trung Quốc.
2Khoa Khoa học Máy tính và Công nghệ, Đại học Zhejiang, Hàng Châu, Trung Quốc.
3Khoa Điện toán, Đại học Quốc gia Singapore, Singapore.
*Tác giả liên hệ. E-mail: zhuotao724@gmail.com;
Tác giả đóng góp: jason.zy.cheng@gmail.com; zangaosh4522@gmail.com;
hehe.fan.cs@gmail.com; mohan@comp.nus.edu.sg;

Tóm tắt
Học Liên Tục (CL) nhằm mục tiêu học gia tăng các nhiệm vụ mới mà không quên kiến thức đã thu được từ những nhiệm vụ cũ. Phương Pháp Phát Lại Trải Nghiệm (ER) là một chiến lược dựa trên luyện tập đơn giản và hiệu quả, tối ưu hóa mô hình với dữ liệu huấn luyện hiện tại và một tập con các mẫu cũ được lưu trữ trong bộ đệm bộ nhớ. Mặc dù nhiều phần mở rộng ER đã được phát triển trong những năm gần đây, mô hình được cập nhật gặp rủi ro overfitting bộ đệm bộ nhớ khi có ít mẫu trước đó, dẫn đến việc quên lãng. Trong nghiên cứu này, chúng tôi đề xuất phương pháp Phát Lại Trải Nghiệm Mạnh (SER) sử dụng hai mất mát nhất quán giữa mô hình mới và mô hình cũ để giảm thêm việc quên lãng. Ngoài việc chưng cất trải nghiệm quá khứ từ dữ liệu lưu trữ trong bộ đệm bộ nhớ để có tính nhất quán ngược, chúng tôi bổ sung khám phá trải nghiệm tương lai của mô hình cũ được mô phỏng trên dữ liệu huấn luyện hiện tại để có tính nhất quán thuận. So với các phương pháp trước đó, SER hiệu quả cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ trước đó và bảo tồn kiến thức đã học. Kết quả thực nghiệm trên nhiều bộ dữ liệu phân loại hình ảnh cho thấy phương pháp SER của chúng tôi vượt trội so với các phương pháp tiên tiến nhất với biên độ đáng chú ý. Mã nguồn của chúng tôi có sẵn tại: https://github.com/visiontao/cl.

Từ khóa: Học Liên Tục, Phát Lại Trải Nghiệm, Quên Lãng Thảm Khốc, Tính Nhất Quán Mô Hình

1 Giới thiệu
Mạng nơ-ron sâu đã được sử dụng rộng rãi trong các nhiệm vụ thị giác máy tính, chẳng hạn như phát hiện đối tượng trong hình ảnh và nhận dạng hành động trong video. Mạng nơ-ron sâu thông thường thường được huấn luyện ngoại tuyến với giả định rằng tất cả dữ liệu đều có sẵn. Tuy nhiên, đối với một dòng các nhiệm vụ không ổn định, một mô hình sâu phải liên tục học các nhiệm vụ mới mà không quên kiến thức đã thu được từ những nhiệm vụ cũ [19, 28]. Do các mẫu đã thấy trước đó không có sẵn để huấn luyện chung, việc chỉ tinh chỉnh mô hình thường dẫn đến suy giảm hiệu suất nghiêm trọng trên các nhiệm vụ cũ. Hiện tượng này được gọi là quên lãng thảm khốc và nó nghiêm trọng hạn chế các ứng dụng trong thực tế. Để giải quyết vấn đề này, Học Liên Tục (CL) [23, 7] nhằm mục đích bảo tồn kiến thức đã thu được khi học các nhiệm vụ mới trong môi trường động.

Thách thức cốt lõi trong CL là tạo ra sự cân bằng tối ưu giữa tính dẻo và tính ổn định của mô hình [18]. Nếu không xem lại bất kỳ mẫu nào đã thấy trước đó, rất khó đảm bảo rằng

arXiv:2305.13622v2  [cs.CV]  3 Dec 2023

--- TRANG 2 ---
Bộ nhớ
(b) tính nhất quán thuận (trải nghiệm tương lai)(a) tính nhất quán ngược (trải nghiệm quá khứ)lấy mẫu
dữ liệu trước đó
dữ liệu trước đóHình 1: Minh họa tính nhất quán ngược và thuận. Sự khác biệt chính là sử dụng dữ liệu khác nhau để tìm kiếm dự đoán nhất quán giữa các mô hình. (1) Tính nhất quán ngược ghi nhớ trải nghiệm quá khứ bằng cách sử dụng một tập con nhỏ dữ liệu trước đó được lưu trữ trong bộ đệm bộ nhớ. (2) Tính nhất quán thuận chưng cất trải nghiệm tương lai của mô hình cũ bằng cách tận dụng tất cả dữ liệu huấn luyện hiện tại. Ngoài ra, nó có thể truyền dự đoán nhất quán đến các nhiệm vụ mới theo thời gian.

một mô hình sâu được huấn luyện với các tham số được cập nhật vẫn phù hợp tốt với dữ liệu của các nhiệm vụ trước đó. Do đó, kỹ thuật dựa trên luyện tập [7, 14, 16, 35, 3, 41] giảm khó khăn của CL bằng cách huấn luyện chung mô hình trên dữ liệu mới và một tập con dữ liệu trước đó được lưu trữ trong bộ đệm bộ nhớ. Để tận dụng bộ đệm bộ nhớ một cách hiệu quả, Phương Pháp Phát Lại Trải Nghiệm (ER) [27] sử dụng phương pháp lấy mẫu bể chứa để cập nhật bộ đệm bộ nhớ theo thời gian. Sau đó mỗi dữ liệu có xác suất như nhau để được lưu trữ trong bộ đệm bộ nhớ mà không cần biết độ dài của dòng dữ liệu đầu vào trước. Dựa trên chiến lược đơn giản như vậy, ER hiệu quả giữ lại kiến thức đã thu được trong nhiều tình huống khác nhau.

Để giảm thêm số lượng mẫu được lưu trữ trong bộ đệm bộ nhớ và giảm thiểu thêm việc quên lãng, các phương pháp CL gần đây [7, 2, 31] mở rộng ER với các chiến lược khác nhau. Ví dụ, MIR [1] truy xuất các mẫu bị can thiệp được lưu trữ trong bộ đệm bộ nhớ thay vì lấy mẫu ngẫu nhiên. MER [27] kết hợp phát lại trải nghiệm với meta-learning để tối đa hóa chuyển giao và tối thiểu hóa can thiệp dựa trên gradient tương lai. DRI [39] tạo ra dữ liệu tưởng tượng và tận dụng chưng cất kiến thức [15] để truy xuất trải nghiệm quá khứ một cách cân bằng. Ngoài ra, để ngăn chặn những thay đổi mô hình nghiêm trọng, các phương pháp dựa trên luyện tập thường kết hợp mất mát nhất quán cho việc huấn luyện mô hình. DER++ [7] kết hợp luyện tập với chưng cất kiến thức trong bộ đệm bộ nhớ. CLS-ER [2] áp dụng phương pháp bộ nhớ kép để duy trì bộ nhớ ngữ nghĩa ngắn hạn và dài hạn, và nó cũng kết hợp mất mát nhất quán để ngăn chặn những thay đổi mô hình nghiêm trọng. TAMiL [5] bao gồm cả luyện tập trải nghiệm và quá trình sinh nơ-ron có thể mở rộng tự điều chỉnh để giảm thiểu thêm việc quên lãng thảm khốc.

Tuy nhiên, các phương pháp này gặp rủi ro overfitting bộ đệm bộ nhớ khi có ít mẫu trước đó [35], dẫn đến việc quên lãng.

Trong nghiên cứu này, chúng tôi đề xuất phương pháp Phát Lại Trải Nghiệm Mạnh (SER) mở rộng ER với hai mất mát nhất quán. Ngoài việc ghi nhớ nhãn của dữ liệu trước đó, chúng tôi tìm kiếm phân phối dự đoán nhất quán giữa mô hình mới và mô hình cũ. Lý tưởng, khi một mô hình mới có cùng logits đầu ra như các logits ban đầu của nó, kiến thức đã thu được có thể được coi là được bảo tồn tốt. Tuy nhiên, do các mẫu của các lớp mới chưa được mô hình cũ thấy, mục tiêu này không thể đạt được trong thực tế. Thay vào đó, chúng tôi mong đợi logits đầu ra của một mô hình được cập nhật có thể xấp xỉ các logits ban đầu của nó bằng cách sử dụng mất mát nhất quán. Khác với các phương pháp trước đó [7, 38, 2] chỉ tìm kiếm tính nhất quán trên dữ liệu được lưu trữ trong bộ đệm bộ nhớ hạn chế, chúng tôi bổ sung tìm kiếm dự đoán nhất quán trên dữ liệu huấn luyện hiện tại, cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ trước đó và giảm thêm việc quên lãng.

Như được minh họa trong Hình 1, tính nhất quán ngược ghi nhớ trải nghiệm quá khứ từ các mẫu trước đó được lưu trữ trong bộ đệm bộ nhớ. Tuy nhiên, khi có số lượng hạn chế các mẫu trước đó, việc sử dụng tính nhất quán ngược có thể dẫn đến giải pháp tối ưu cục bộ, gây ra overfitting. Để giảm thiểu vấn đề này, chúng tôi phát triển mất mát nhất quán thuận để cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ trước đó. Cụ thể, bằng cách mô phỏng trải nghiệm tương lai của mô hình cũ trên dữ liệu huấn luyện hiện tại, việc tìm kiếm dự đoán nhất quán sẽ tăng các dự đoán trùng lặp [20] giữa mô hình mới và mô hình cũ. Ngoài ra, vì các tham số của mô hình cũ được đóng băng, kiến thức trước đó có thể được bảo tồn tốt khi tối ưu hóa mô hình trên các nhiệm vụ mới. So với tính nhất quán ngược, tính nhất quán thuận tận dụng dữ liệu huấn luyện hoàn chỉnh và nó có thể truyền phân phối dự đoán đến các nhiệm vụ mới theo thời gian.

Phương pháp SER được đề xuất hiệu quả giảm vấn đề quên lãng bằng cách kết hợp cả tính nhất quán ngược và thuận. So với phương pháp gần nhất

--- TRANG 3 ---
phương pháp DER++ [7], phương pháp SER bổ sung kết hợp mất mát nhất quán thuận cho việc huấn luyện mô hình. Mặc dù đơn giản, các thí nghiệm mở rộng trên nhiều bộ dữ liệu phân loại hình ảnh chuẩn cho thấy SER vượt trội so với DER++ với biên độ lớn trong các tình huống tăng dần theo lớp, ví dụ: 9,2% trên bộ dữ liệu CIFAR100 (20 nhiệm vụ) và hơn 17,54% trên bộ dữ liệu TinyImageNet (10 nhiệm vụ), xem Bảng 1 và 2. Hơn nữa, các thí nghiệm toàn diện trên nhiều bộ dữ liệu phân loại hình ảnh cũng chứng minh tính ưu việt của phương pháp SER vượt trội so với các phương pháp tiên tiến nhất với biên độ đáng chú ý, đặc biệt khi có ít mẫu trước đó.

Tóm lại, đóng góp của chúng tôi như sau:
1. Chúng tôi đề xuất mất mát nhất quán thuận cải thiện khả năng tổng quát hóa mô hình bằng cách tìm kiếm phân phối dự đoán nhất quán trên dữ liệu huấn luyện hiện tại. So với mất mát nhất quán ngược được sử dụng trong các phương pháp trước đó, mất mát nhất quán thuận có thể tận dụng hiệu quả dữ liệu huấn luyện hoàn chỉnh và truyền phân phối dự đoán nhất quán đến các nhiệm vụ mới theo thời gian.

2. Chúng tôi đề xuất phương pháp Phát Lại Trải Nghiệm Mạnh (SER) chưng cất kiến thức đã thu được của một mô hình được huấn luyện từ cả trải nghiệm quá khứ được lưu trữ trong bộ đệm bộ nhớ và trải nghiệm tương lai được mô phỏng trên dữ liệu huấn luyện hiện tại. Mặc dù đơn giản, SER giảm đáng kể vấn đề quên lãng.

3. Phương pháp được đề xuất đơn giản và hiệu quả, các thí nghiệm mở rộng trên nhiều bộ dữ liệu phân loại hình ảnh cho thấy SER vượt trội so với các phương pháp tiên tiến nhất với biên độ lớn khi có ít mẫu trước đó.

2 Công trình liên quan

2.1 Phương pháp dựa trên Luyện tập
Các phương pháp dựa trên luyện tập [26, 29, 3, 39, 43] giảm quên lãng thảm khốc bằng cách phát lại một tập con các mẫu đã thấy trước đó được lưu trữ trong bộ đệm bộ nhớ. Phương Pháp Phát Lại Trải Nghiệm (ER) [27] là một phương pháp dựa trên luyện tập đơn giản nhưng hiệu quả huấn luyện chung mô hình với dữ liệu hiện tại và một mini-batch các mẫu cũ được chọn ngẫu nhiên. Ngoài ra, nó áp dụng chiến lược lấy mẫu bể chứa để cập nhật bộ đệm bộ nhớ theo thời gian. Dựa trên ý tưởng cốt lõi của ER, các phương pháp CL gần đây giảm thêm việc quên lãng với nhiều kỹ thuật khác nhau. Ví dụ, GSS [1] lưu trữ các ví dụ được chọn tối ưu trong bộ đệm bộ nhớ. GEM [24] và AGEM [12] tận dụng bộ nhớ episo để tránh quên lãng và ủng hộ chuyển giao ngược tích cực. ERT [8] áp dụng phương pháp lấy mẫu cân bằng và kiểm soát thiên lệch. MER [27] coi phát lại trải nghiệm như một vấn đề meta-learning để tối đa hóa chuyển giao và tối thiểu hóa can thiệp. iCaRL [26] sử dụng bộ phân loại nearest-mean-of-exemplars và một bộ đệm bộ nhớ bổ sung cho việc huấn luyện mô hình. Ngoài ra, nó áp dụng phương pháp chưng cất kiến thức để giảm thêm việc quên lãng. RM [3] phát triển chiến lược quản lý bộ nhớ mới dựa trên độ không chắc chắn phân loại từng mẫu và tăng cường dữ liệu để nâng cao tính đa dạng mẫu trong bộ nhớ. DER++ [7] kết hợp luyện tập với chưng cất kiến thức và regularization. CLS-ER [2] áp dụng phương pháp phát lại trải nghiệm bộ nhớ kép để duy trì bộ nhớ ngữ nghĩa ngắn hạn và dài hạn. LVT [38] thiết kế vision transformer cho học liên tục với phát lại. SCoMMER [31] thực thi độ thưa thớt kích hoạt cùng với cơ chế dropout ngữ nghĩa bổ sung để khuyến khích tính nhất quán. Khác với các phương pháp này chỉ chưng cất trải nghiệm quá khứ từ bộ đệm bộ nhớ hạn chế, chúng tôi cũng khám phá trải nghiệm tương lai được mô phỏng trên dữ liệu huấn luyện hiện tại, cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ trước đó.

2.2 Phương pháp dựa trên Regularization
Các phương pháp dựa trên regularization thường kết hợp một số hạng phạt bổ sung vào hàm mất mát để ngăn chặn thay đổi mô hình trong không gian tham số hoặc dự đoán [17]. Elastic Weight Consolidation (EWC) [19, 32], Synaptic Intelligence (SI) [44], và Riemmanian Walk (RW) [10] ngăn chặn thay đổi tham số giữa mô hình mới và mô hình cũ. LwF [23] và PASS [46] giảm thiểu quên lãng bằng chưng cất kiến thức đặc thù nhiệm vụ. Không có bất kỳ dữ liệu trước đó nào để phát lại, các nghiên cứu gần đây [7, 25] cho thấy các phương pháp này thường thất bại trong việc xử lý các tình huống tăng dần theo lớp. Do đó, chiến lược dựa trên regularization thường được sử dụng đồng thời với các phương pháp CL khác để có hiệu suất tốt hơn, chẳng hạn như iCaRL [26] và DER++ [7, 6]. Trong nghiên cứu này, ngoài việc ghi nhớ nhãn thực tế của các mẫu trước đó, chúng tôi cũng tìm kiếm phân phối logit nhất quán. Do đó,

--- TRANG 4 ---
chúng tôi kết hợp hai mất mát nhất quán như regularization mô hình để giảm quên lãng.

2.3 Các phương pháp CL khác
Các phương pháp CL đầu tiên thường áp dụng định danh nhiệm vụ để học kiến thức đặc thù nhiệm vụ. Ví dụ, PNN (Progressive Neural Networks) [30] khởi tạo mạng mới một cách gia tăng và áp dụng định danh nhiệm vụ trong cả thời gian huấn luyện và suy luận. Tuy nhiên, định danh có thể không được biết trong thời gian suy luận, hạn chế các ứng dụng của nó. Gần đây, DER [42] mở rộng một backbone mới cho mỗi nhiệm vụ gia tăng mà không sử dụng định danh nhiệm vụ trong thời gian kiểm tra. FOSTER [37] thêm giai đoạn nén mô hình bổ sung để duy trì lưu trữ mô hình hạn chế. L2P và Dualprompt [41, 40] áp dụng prompt pool để học và trích xuất kiến thức đặc thù nhiệm vụ. Ngoài ra, đối với các phương pháp dựa trên luyện tập, việc lưu trữ các mẫu thô đôi khi không thể do lo ngại về quyền riêng tư, một số phương pháp tạo sinh [33, 34] cố gắng sử dụng trình tạo dữ liệu (như GAN [13]) để tạo ra dữ liệu tổng hợp để phát lại. Nhưng nó cần thời gian dài để huấn luyện mô hình và dữ liệu được tạo có thể không đạt yêu cầu đối với các bộ dữ liệu phức tạp. Ngoài ra, để chưng cất thêm kiến thức từ các nhiệm vụ cũ, DMC [45] và GD [22] lấy mẫu dữ liệu bên ngoài để hỗ trợ huấn luyện mô hình. Mặc dù đã đạt được nhiều tiến bộ trong những năm gần đây, CL hiệu quả với ít quên lãng vẫn là một vấn đề thách thức.

3 Phương pháp

Trong phần này, trước tiên chúng tôi giới thiệu công thức vấn đề của học liên tục và phương pháp phát lại trải nghiệm thông thường. Sau đó chúng tôi mô tả chi tiết phương pháp Phát Lại Trải Nghiệm Mạnh (SER) được đề xuất.

3.1 Công thức Vấn đề
Chính thức, cho một chuỗi T bộ dữ liệu không ổn định {D1,···,DT}, (x, y)∈ Dt đại diện cho các mẫu x với nhãn thực tế tương ứng y của nhiệm vụ thứ t. Tại bước thời gian t∈ {1,···, T}, mục tiêu của một phương pháp CL là cập nhật tuần tự mô hình trên Dt mà không quên kiến thức đã học trước đó trên {D1,···,Dt−1}.

Nói chung, khi tất cả dữ liệu huấn luyện có sẵn trước, hàm mất mát của việc học thông thường có thể được công thức hóa như sau:

L=TX
t=1E(x,y)∼D t[ℓce(f(x;θ), y)], (1)

trong đó ℓce biểu thị mất mát cross-entropy và f(x;θ) biểu thị logits đầu ra của mạng, θ đại diện cho tham số mô hình cần được tối ưu hóa.

Ngược lại, đối với nhiệm vụ thứ t với dữ liệu huấn luyện Dt trong CL, cho một mô hình được huấn luyện với tham số khởi tạo θt−1, việc tinh chỉnh mô hình thông thường có thể đạt được bằng cách tối thiểu hóa hàm mất mát phân loại như sau:

Lt
cls=E(x,y)∼D t[ℓce(f(x;θt), y)]. (2)

Tuy nhiên, khi dữ liệu trước đó không thể truy cập, việc chỉ cập nhật mô hình với Phương trình 2 thường dẫn đến sụt giảm hiệu suất nghiêm trọng trên các bộ dữ liệu trước đó {D1,···,Dt−1}, tức là quên lãng thảm khốc.

3.2 Phương Pháp Phát Lại Trải Nghiệm (ER)
ER [27] là một chiến lược dựa trên luyện tập cổ điển làm giảm quên lãng bằng cách phát lại một tập con dữ liệu đã thấy trước đó được lưu trữ trong bộ đệm bộ nhớ vĩnh viễn M. Bằng cách huấn luyện chung mô hình trên dữ liệu huấn luyện hiện tại Dt và dữ liệu trước đó được lưu trữ trong M, mất mát huấn luyện được tính như sau:

L=Lt
cls+Lm
cls, (3)

trong đó Lm
cls là hàm mất mát phân loại trên các mẫu được lưu trữ trong bộ đệm bộ nhớ và nó được sử dụng để bảo tồn kiến thức đã thu được như sau:

Lm
cls=E(x,y)∼M[ℓce(f(x;θt), y]. (4)

Về mặt lý thuyết, bộ đệm bộ nhớ càng lớn thì càng nhiều kiến thức được bảo tồn. Đối với trường hợp cực đoan, khi tất cả dữ liệu trước đó được lưu trữ trong M, nó tương đương với huấn luyện chung trong một nhiệm vụ. Thật không may, do lo ngại về quyền riêng tư hoặc lưu trữ, dữ liệu trước đó có thể không có sẵn trong thực tế, việc lưu trữ một tập con dữ liệu trước đó trong M giảm khó khăn của CL.

Để tận dụng bộ đệm bộ nhớ hạn chế một cách hiệu quả, ER áp dụng phương pháp lấy mẫu bể chứa [36] để cập nhật bộ đệm theo thời gian. Theo cách này, khi chúng ta chọn ngẫu nhiên |M| mẫu từ dòng đầu vào, mỗi mẫu vẫn có cùng xác suất |M|/|S| được lưu trữ trong bộ đệm bộ nhớ mà không cần biết độ dài của dòng dữ liệu đầu vào |S| trước. Mặc dù ER làm giảm vấn đề quên lãng một cách hiệu quả và vượt trội so với nhiều phương pháp CL được thiết kế công phu [7], hiệu suất của nó có thể giảm nghiêm trọng khi có ít mẫu [35].

3.3 Phương Pháp Phát Lại Trải Nghiệm Mạnh (SER)
Để giảm thiểu thêm vấn đề quên lãng, nhiều phần mở rộng ER [7, 41, 2, 40] được phát triển để bảo tồn kiến thức đã học khi học các nhiệm vụ mới. Tuy nhiên, các phương pháp này có thể overfit bộ đệm bộ nhớ khi có ít mẫu. Trong nghiên cứu này, chúng tôi kết hợp chiến lược dựa trên luyện tập với regularization mô hình và chúng tôi mở rộng ER bằng cách kết hợp hai mất mát nhất quán, giúp mô hình CL phát triển nhất quán trong quá trình huấn luyện. Lý tưởng, nếu một mô hình được cập nhật vẫn có cùng đầu ra như các đầu ra ban đầu của nó khi học các nhiệm vụ mới, có thể coi rằng kiến thức đã học trước đó không bị quên. Tuy nhiên, mục tiêu này không thể đạt được trong thực tế vì các nhãn lớp không liền kề trong các tình huống tăng dần theo lớp. Thay vào đó, chúng tôi nhằm mục đích thu được logits đầu ra xấp xỉ giữa mô hình mới và mô hình cũ.

Phương pháp của chúng tôi bao gồm mất mát nhất quán ngược và mất mát nhất quán thuận. Cụ thể, tính nhất quán ngược Lm
bc được sử dụng để chưng cất trải nghiệm quá khứ của mô hình trên dữ liệu được lưu trữ trong bộ đệm bộ nhớ. Mặt khác, tính nhất quán thuận Lt
fc được sử dụng để chưng cất trải nghiệm tương lai của mô hình trên dữ liệu huấn luyện hiện tại, vì dữ liệu mới là từ tương lai đối với mô hình cũ. Tổng quan về phương pháp SER được đề xuất được minh họa trong Hình 2. Khác với tính nhất quán ngược được sử dụng trong DER++ [7], tính nhất quán thuận có thể tận dụng tất cả dữ liệu huấn luyện và nó có thể truyền dự đoán nhất quán đến các nhiệm vụ mới theo thời gian. Chi tiết về hai mất mát nhất quán này như sau.

Tính nhất quán ngược. Đối với nhiệm vụ thứ t, cho một mô hình được huấn luyện với tham số khởi tạo θt−1, chúng tôi đo tính nhất quán ngược giữa hai mô hình trên bộ đệm bộ nhớ M với mất mát Mean Square Error (MSE):

Lm
bc=Ex∼M[∥f(x;θt)−f(x;θt−1)∥2].(5)

Như đã thảo luận trong DER++ [7], việc tối ưu hóa Phương trình 5 tương đương với việc tối thiểu hóa mất mát phân kỳ KL trong chưng cất kiến thức [15]. Thay vì sử dụng phản hồi softmax làm mượt, việc sử dụng trực tiếp logits với mất mát MSE có thể tránh mất thông tin xảy ra trong không gian dự đoán do hàm squashing (ví dụ: softmax).

Ngoài ra, để giảm chi phí tính toán, chúng tôi lưu trữ logits của đầu ra mạng khi cập nhật bộ đệm bộ nhớ [7]. Để z=f(x;θ) biểu thị logits đầu ra của trải nghiệm quá khứ trên mẫu x, bằng cách thay thế f(x;θt−1) bằng z, thì Phương trình 5 có thể được viết lại như sau:

Lm
bc=Ex∼M[∥f(x;θt)−z∥2]. (6)

Để minh họa rõ ràng ý tưởng cốt lõi của phương pháp được đề xuất, chúng tôi sử dụng trực tiếp f(xm;θt−1) trong Hình 2, thay vì z được lưu trữ trong bộ đệm bộ nhớ. Dựa trên mất mát phân loại trên các mẫu được lưu trữ trong bộ đệm bộ nhớ và mất mát nhất quán ngược, mô hình được cập nhật sẽ mô phỏng quỹ đạo huấn luyện của nó và logits đầu ra của nó sẽ xấp xỉ các logits ban đầu, giảm vấn đề quên lãng.

Tính nhất quán thuận. Trực quan, dữ liệu trước đó được lưu trữ trong bộ đệm bộ nhớ M chỉ chứa một tập con nhỏ của {D1,···,Dt−1}. Khi có ít mẫu trước đó, việc chỉ sử dụng tính nhất quán ngược không thể làm cho đầu ra của mô hình mới xấp xỉ các đầu ra ban đầu của nó một cách thích hợp. Kết quả là, việc bảo tồn kiến thức đã học trên bộ đệm bộ nhớ hạn chế gặp rủi ro overfitting M, dẫn đến vấn đề quên lãng.

Để giải quyết vấn đề này, chúng tôi thiết kế mất mát nhất quán bổ sung để chưng cất trải nghiệm tương lai trên Dt. Cụ thể, trước tiên chúng tôi tính toán logits đầu ra của mô hình cũ trên Dt, sau đó sử dụng mất mát MSE như trong Phương trình 6 để đo tính nhất quán mới như sau:

Lt
fc=Ex∼D t[∥f(x;θt)−f(x;θt−1)∥2]. (7)

Vì dữ liệu trong Dt là từ một nhiệm vụ tương lai đối với mô hình cũ, chúng tôi đơn giản gọi Lt
fc là mất mát "nhất quán thuận".

Như được minh họa trong Hình 1, tính nhất quán ngược giữ lại một tập con trải nghiệm quá khứ được lưu trữ trong M. Ngược lại, tính nhất quán thuận có thể truyền trải nghiệm tương lai được mô phỏng trên

--- TRANG 5 ---
: bộ đệm bộ nhớ 
: dữ liệu huấn luyện hiện tại : mô hình huấn luyện hiện tại : mô hình khởi tạo : bộ phân loại
: dữ liệu từ : dữ liệu từ 
++ +
: mất mát phân loại trên : mất mát phân loại trên  
: mất mát nhất quán ngược trên  : mất mát nhất quán thuận trên  Hình 2: Minh họa phương pháp SER của chúng tôi. So với các phương pháp phát lại trải nghiệm thông thường, những khác biệt chính có thể được tóm tắt như sau. (1) Phương Pháp Phát Lại Trải Nghiệm (ER) [27]: Lt
cls+Lm
cls. (2) Dark Experience Replay++ (DER++) [7]: Lt
cls+Lm
cls+Lm
bc. (3) Strong Experience Replay (SER): Lt
cls+Lm
cls+Lm
bc+Lm
fc.

Dt đến các nhiệm vụ mới theo thời gian. Do đó, tính nhất quán thuận có thể tận dụng hiệu quả dữ liệu huấn luyện hoàn chỉnh. Hơn nữa, trong quá trình huấn luyện mô hình, các tham số của mô hình cũ được đóng băng, việc tìm kiếm dự đoán nhất quán trên dữ liệu huấn luyện hiện tại làm cho mô hình được cập nhật có sự trùng lặp lớn hơn với mô hình cũ, cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ cũ và bảo tồn thêm kiến thức đã thu được.

Tổng mất mát huấn luyện. Dựa trên phân tích trên, phương pháp SER được đề xuất kết hợp hai mất mát phân loại và hai mất mát nhất quán trong quá trình huấn luyện mô hình. Chính thức, tổng mất mát được sử dụng trong SER được biểu diễn như sau:

L=Lt
cls+Lm
cls+αLm
bc+βLt
fc (8)

trong đó α và β là các tham số cân bằng.

Thách thức cốt lõi trong CL là tình trạng tiến thoái lưỡng nan giữa tính ổn định và dẻo. Đối với nhiệm vụ thứ t, Lt
cls tập trung vào tính dẻo bằng cách tinh chỉnh mô hình trên bộ dữ liệu mới Dt; và các mất mát khác tập trung vào tính ổn định bằng cách ghi nhớ trải nghiệm của mô hình cũ. Cụ thể, Lm
cls và Lm
bc bảo tồn trải nghiệm quá khứ trên bộ đệm bộ nhớ M. Lt
fc giữ lại trải nghiệm tương lai trên dữ liệu huấn luyện hiện tại Dt. Do đó, so với phương pháp ER cổ điển, hai mất mát nhất quán được sử dụng trong SER hiệu quả cải thiện tính ổn định của mô hình, và chúng ngăn chặn thay đổi mô hình trong không gian dự đoán và giảm thêm việc quên lãng.

Triển khai đơn giản. Thuật toán SER được đề xuất được mô tả trong Thuật toán 1, trong đó "aug" biểu thị tăng cường dữ liệu. Trong quá trình huấn luyện mô hình, chúng tôi lấy mẫu ngẫu nhiên một batch mẫu từ bộ đệm bộ nhớ và dữ liệu được tăng cường được sử dụng cho phân loại và tính nhất quán ngược. Sau mỗi batch huấn luyện, phương pháp lấy mẫu bể chứa được sử dụng để cập nhật bộ đệm bộ nhớ. Để giảm chi phí tính toán, chúng tôi lưu trữ dữ liệu huấn luyện ban đầu với logits đầu ra của nó trong bộ đệm bộ nhớ. Có thể thấy rằng SER không cần bất kỳ tính toán phức tạp nào, làm cho nó dễ triển khai. Ngoài ra, lưu ý rằng chúng tôi không sử dụng các mất mát nhất quán cho nhiệm vụ đầu tiên, vì không có kiến thức trước đó nào cần được bảo tồn.

4 Thí nghiệm

Chúng tôi đánh giá phương pháp được đề xuất trong cả ba cài đặt CL: Học Tăng dần theo Lớp (Class-IL), Học Tăng dần theo Nhiệm vụ (Task-IL), và Học Tăng dần theo Miền (Domain-IL). Đối với Class-IL, mô hình cần học gia tăng các lớp mới trong mỗi nhiệm vụ tiếp theo và định danh nhiệm vụ không được biết khi suy luận. Đối với Task-IL, định danh nhiệm vụ được cung cấp trong quá trình kiểm tra. Bằng cách chọn

--- TRANG 6 ---
Thuật toán 1 Phương Pháp Phát Lại Trải Nghiệm Mạnh.
Đầu vào: tham số mô hình khởi tạo θt−1, bộ dữ liệu Dt, bộ đệm bộ nhớ M, các yếu tố cân bằng α và β.
Đầu ra: tham số tối ưu hóa θt, bộ đệm bộ nhớ cập nhật M.
1:for tất cả nhiệm vụ t∈ {1,2,···, T}do,
2: for(x, y)∈ Dtdo, ▷lấy mẫu hiện tại
3: (x′, y′, z)←sample (M), ▷lấy mẫu trước đó
4: xt←augment (x), ▷tăng cường dữ liệu
5: xm←augment (x′), ▷tăng cường dữ liệu
6: Lt
cls←ℓce(f(xt, θt), y), ▷mất mát phân loại trên Dt
7: ift== 1 then
8: L ← Lt
cls
9: else
10: Lm
cls←ℓce(f(xm, θt), y′), ▷mất mát phân loại trên M
11: Lm
bc← ∥f(xm;θt)−z∥2, ▷mất mát nhất quán ngược trên M
12: Lt
fc← ∥f(xt;θt)−f(xt;θt−1)∥2, ▷mất mát nhất quán thuận trên Dt
13: L ← Lt
cls+Lm
cls+αLm
bc+βLt
fc, ▷mất mát cuối cùng cho huấn luyện
14: end if
15: θt←minimize L với SGD, ▷tối ưu hóa mô hình
16: M ← (M,(x, y, f (x;θt))), ▷cập nhật bộ nhớ với lấy mẫu bể chứa
17: end for
18: θt−1←θt, ▷cập nhật tham số mô hình trước đó
19:end for

các bộ phân loại tương ứng cho suy luận, Task-IL dễ hơn Class-IL. Đối với Domain-IL, các lớp vẫn giống nhau trong mỗi nhiệm vụ, nhưng phân phối dữ liệu khác nhau.

Trong các thí nghiệm của chúng tôi, chúng tôi tuân thủ nghiêm ngặt các cài đặt thí nghiệm trong DER++ [7] và LVT [38]. Để đánh giá hiệu suất của phương pháp CL, chúng tôi không sử dụng bất kỳ định danh nhiệm vụ nào để chọn kiến thức đặc thù nhiệm vụ khi huấn luyện, ngay cả khi bao gồm Task-IL. Hơn nữa, kiến trúc mạng được sử dụng trong phương pháp của chúng tôi được cố định cho tất cả các nhiệm vụ, do đó tôn trọng ràng buộc bộ nhớ không đổi.

4.1 Cài đặt Thí nghiệm

Bộ dữ liệu. Phương pháp được đề xuất được đánh giá trên 5 bộ dữ liệu phân loại hình ảnh chuẩn. Bộ dữ liệu CIFAR100 chứa 100 lớp và mỗi lớp có 500 hình ảnh để huấn luyện và 100 hình ảnh để kiểm tra. Bộ dữ liệu CIFAR10 có 10 lớp, mỗi lớp bao gồm 5000 hình ảnh huấn luyện và 1000 hình ảnh kiểm tra. Kích thước hình ảnh của CIFAR10 và CIFAR100 là 32×32. TinyImageNet có 200 lớp và bao gồm 100,000 hình ảnh huấn luyện và 10,000 hình ảnh kiểm tra tổng cộng, và kích thước hình ảnh là 64×64. Cả Permuted MNIST [19] và Rotated MNIST [24] đều được xây dựng dựa trên bộ dữ liệu MNIST [21], có 60,000 hình ảnh huấn luyện và 10,000 hình ảnh kiểm tra, và kích thước hình ảnh là 28×28. Permuted MNIST áp dụng hoán vị ngẫu nhiên cho các pixel và Rotated MNIST xoay các chữ số theo góc ngẫu nhiên trong khoảng [0, π). Trong các thí nghiệm của chúng tôi, CIFAR10, CIFAR100, và TinyImageNet được sử dụng để đánh giá hiệu suất của Class-IL và Task-IL. Permuted MNIST và Rotated MNIST được sử dụng cho Domain-IL.

Baseline. Chúng tôi so sánh phương pháp SER được đề xuất với ba phương pháp không có luyện tập, bao gồm oEWC [32], SI [44], và LwF [23]. Chúng tôi cũng so sánh SER với nhiều phương pháp dựa trên luyện tập: iCaRL [26], ER [27], GEM [24], A-GEM [12], FDR [4], GSS [1], HAL [11], DER++ [7], Co2L [9], ERT [8], RM [3], LVT [38], CLS-ER [2], và TAMiL [5]. Lưu ý rằng CLS-ER [2] cập nhật ba mô hình (một mô hình dẻo, một mô hình ổn định, và mô hình huấn luyện hiện tại) trong quá trình huấn luyện. Ngược lại, SER chỉ cập nhật mô hình huấn luyện hiện tại (mô hình cũ được đóng băng). Để cho thấy hiệu quả của các phương pháp CL, chúng tôi bổ sung cung cấp hai baseline không có kỹ thuật CL: SGD (cận dưới) và Joint (cận trên). Kết quả của các baseline được lấy từ các bài báo đã xuất bản hoặc mã nguồn. Ngoài ra, các cấu hình tham số tốt nhất

--- TRANG 7 ---
Bảng 1: Kết quả phân loại trên bộ dữ liệu chuẩn CIFAR100 với số lượng nhiệm vụ khác nhau, trung bình trên 5 lần chạy.

[Bảng phức tạp với nhiều cột và hàng hiển thị kết quả so sánh các phương pháp khác nhau]

của các baseline được sử dụng để so sánh.

Độ đo. Chúng tôi đánh giá các phương pháp học liên tục với hai độ đo: độ chính xác trung bình cuối cùng và quên lãng trung bình. Để aT,t biểu thị độ chính xác kiểm tra trên nhiệm vụ thứ t khi mô hình được huấn luyện trên nhiệm vụ thứ T, và độ chính xác trung bình cuối cùng At trên tất cả T nhiệm vụ được tính như sau:

Accuracy = 1/T ∑(t=1 to T) aT,t. (9)

Ngoài ra, quên lãng trung bình trên T nhiệm vụ được định nghĩa như sau:

Forgetting = 1/(T-1) ∑(t=1 to T-1) max(i∈{1,···,T-1})(ai,t−aT,t). (10)

4.2 Chi tiết Triển khai

Kiến trúc. Chúng tôi sử dụng cùng kiến trúc mạng được sử dụng trong DER++ [7]. Đối với CIFAR10, CIFAR100, và Tiny ImageNet, chúng tôi áp dụng ResNet18 được sửa đổi không có pretraining. Đối với bộ dữ liệu MNIST, chúng tôi áp dụng mạng fully connected với hai lớp ẩn mỗi lớp bao gồm 100 đơn vị ReLU.

Tăng cường. Để công bằng, chúng tôi sử dụng cùng tăng cường dữ liệu như trong DER++ [7], áp dụng cắt ngẫu nhiên và lật ngang cho cả ví dụ dòng và bộ đệm. Đáng chú ý rằng các biến đổi dữ liệu nhỏ thực thi các ràng buộc nhất quán ngầm định [7].

Huấn luyện. Theo chiến lược huấn luyện trong DER++ [7], chúng tôi sử dụng trình tối ưu hóa Stochastic Gradient Descent (SGD) trong tất cả các thí nghiệm. Đối với bộ dữ liệu CIFAR10, chúng tôi huấn luyện mô hình với 20 epoch, và CIFAR100 với 50 epoch, tương ứng. Đối với bộ dữ liệu Tiny ImageNet, chúng tôi tăng

--- TRANG 8 ---
số epoch lên 100. Ngoài ra, chúng tôi chỉ sử dụng một epoch cho các biến thể MNIST. Tốc độ học và kích thước batch giống như trong DER++ [7]. Ngoài ra, chúng tôi sử dụng MultiStepLR để giảm tốc độ học theo tham số 0.1. Các mốc như sau. CIFAR10: [15]; CIFAR100: [35, 45]; TinyImageNet: [70, 90].

Do các tình huống học động trong học liên tục, rất khó để điều chỉnh siêu tham số tự động. Do đó, chúng tôi tuân theo các phương pháp trước đó [7, 2, 5] điều chỉnh các tham số cân bằng cho mỗi bộ dữ liệu. Trong các thí nghiệm của chúng tôi, các tham số cân bằng là CIFAR10 (α= 0.2, β= 0.2), CIFAR100 (α= 0.5, β= 0.5), và TinyImageNet (α= 0.2, β= 1), P-MNIST và R-MNIST (α= 0.2, β= 0.2). Các cấu hình tham số khác có thể đạt được hiệu suất tốt hơn.

4.3 So sánh với Phương pháp Trước đó

Đánh giá trên CIFAR100. Chúng tôi tuân theo cài đặt thí nghiệm trong LVT [38] để đánh giá phương pháp được đề xuất trong các tình huống Class-IL và Task-IL. Bảng 1 báo cáo hiệu suất của mỗi phương pháp CL trên CIFAR100 với 5, 10, và 20 nhiệm vụ và mỗi nhiệm vụ có nhãn lớp không liền kề. Có thể thấy rằng tất cả các phương pháp không có luyện tập đều thất bại trong việc giảm quên lãng trên bộ dữ liệu CIFAR100 khi so sánh với SGD (cận dưới). Mặc dù LwF tìm kiếm dự đoán nhất quán với chưng cất kiến thức, không có sự giúp đỡ của các mẫu trước đó để phát lại, mô hình đã học trôi dạt nghiêm trọng theo thời gian. Ngược lại, bằng cách lưu trữ một số lượng nhỏ các mẫu đã thấy trước đó trong bộ đệm bộ nhớ, các phương pháp dựa trên luyện tập có thể bảo tồn hiệu quả kiến thức đã học.

Trong số các phương pháp dựa trên luyện tập, SER vượt trội so với DER++ với biên độ lớn trên tất cả các nhiệm vụ. Ví dụ, với bộ đệm bộ nhớ 200 mẫu cho Class-IL, SER vượt trội so với DER++ 20.5% trên 5 nhiệm vụ và 9.2% trên 20 nhiệm vụ. Ngoài ra, đối với Task-IL với 10 nhiệm vụ, SER đạt được cải thiện độ chính xác 16.3%. Hình 3 và Hình 4 vẽ đồ thị độ chính xác trung bình của ER, DER++, và SER trên CIFAR100 khi học mỗi nhiệm vụ. Có thể thấy rằng SER cải thiện đáng kể độ chính xác trong cả Class-IL và Task-IL cho mỗi giai đoạn học. Bằng cách thêm mất mát nhất quán thuận, DER++ vượt trội so với ER với biên độ đáng chú ý

Số nhiệm vụ Độ chính xác (%)
0255075100
2 4 6 8 10ER DER++ SER(a) Class-IL

Số nhiệm vụ Độ chính xác (%)
60708090
2 4 6 8 10ER DER++ SER (b) Task-IL

Hình 3: Độ chính xác trung bình khi học gia tăng trên bộ dữ liệu CIFAR100 với 10 nhiệm vụ (kích thước bộ nhớ là 200).

Số nhiệm vụ Độ chính xác (%)
0255075100
5 10 15 20ER DER++ SER
(a) Class-IL

Số nhiệm vụ Độ chính xác (%)
60708090
5 10 15 20ER DER++ SER (b) Task-IL

Hình 4: Độ chính xác trung bình khi học gia tăng trên bộ dữ liệu CIFAR100 với 20 nhiệm vụ (kích thước bộ nhớ là 200).

trong cả Class-IL và Task-IL. Hơn nữa, với sự giúp đỡ của mất mát nhất quán thuận, SER vượt trội đáng kể so với DER++ trên CIFAR100 với các độ dài nhiệm vụ khác nhau.

Ngay cả khi so sánh với phương pháp LVT gần đây nhất trong Class-IL, SER vượt trội với biên độ lớn 8.3% trên 5 nhiệm vụ và 3.0% trên 20 nhiệm vụ khi lưu trữ 200 mẫu trước đó. Hơn nữa, SER đạt được ít nhất 6% cải thiện độ chính xác trong Task-IL. Hơn nữa, với bộ đệm bộ nhớ lớn hơn với 500 mẫu, có thể quan sát thấy rằng SER cũng đạt được cải thiện độ chính xác nhất quán. Những kết quả này chứng minh rằng SER có thể bảo tồn hiệu quả kiến thức đã học với phương pháp phát lại trải nghiệm mạnh.

Đánh giá trên CIFAR10. Bảng 2 cho thấy hiệu suất của mỗi phương pháp CL trên bộ dữ liệu CIFAR10 (5 nhiệm vụ). Tương tự như kết quả quan sát được trong Bảng 1, các phương pháp dựa trên luyện tập vượt trội so với các phương pháp không có luyện tập với biên độ lớn và SER của chúng tôi đạt hiệu suất tốt nhất trong số các thuật toán so sánh. CIFAR10 là một bộ dữ liệu nhỏ

--- TRANG 9 ---
Bảng 2: Kết quả phân loại trên các chuẩn CL tiêu chuẩn, trung bình trên 5 lần chạy. Do yêu cầu định danh nhiệm vụ, LwF, iCaRL, và TAMiL không thể được áp dụng trong cài đặt Domain-IL. Ngoài ra, thời gian huấn luyện của GEM, HAL, và GSS không khả thi trên TinyImageNet. Co2L sử dụng kiến trúc mạng phức tạp hơn trên các biến thể của bộ dữ liệu MNIST, chúng tôi không báo cáo độ chính xác của nó để so sánh công bằng.

[Bảng phức tạp với nhiều cột và hàng hiển thị kết quả so sánh]

chứa 10 lớp. Khi kích thước bộ nhớ là 200, số lượng mẫu được lưu trữ trung bình cho mỗi lớp là 20. Ngược lại, đối với bộ dữ liệu CIFAR100 với 100 lớp, chỉ có 2 mẫu cho mỗi lớp. Do đó, bằng cách lưu trữ nhiều mẫu hơn cho mỗi lớp, độ chính xác trung bình trên CIFAR10 tốt hơn nhiều cho tất cả các phương pháp CL.

So với DER++, SER giảm thêm vấn đề quên lãng và tăng hiệu suất với mất mát nhất quán thuận. Ví dụ, SER vượt trội so với DER++ 5% và 3.7% trong Class-IL khi kích thước bộ nhớ lần lượt là 200 và 500. Lưu ý rằng CLS-ER là kiến trúc phát lại trải nghiệm bộ nhớ kép, tính toán mất mát nhất quán trên dữ liệu được lưu trữ trong bộ đệm bộ nhớ và duy trì bộ nhớ ngữ nghĩa ngắn hạn và dài hạn với bộ nhớ episo. Ngược lại, SER của chúng tôi chỉ kết hợp mất mát nhất quán thuận trên dữ liệu huấn luyện hiện tại. So với CLS-ER trên CIFAR10, SER cải thiện độ chính xác 3.7% khi kích thước bộ nhớ là 200. Hơn nữa, mặc dù SER không sử dụng định danh nhiệm vụ trong quá trình huấn luyện mô hình, nó vượt trội nhẹ so với phương pháp TAMiL gần đây nhất 1%.

Đánh giá trên TinyImageNet. Trong Bảng 2, dữ liệu TinyImageNet được chia thành 10 nhiệm vụ và mỗi nhiệm vụ có 20 lớp. TinyImageNet thách thức hơn CIFAR10 và CIFAR100 đối với CL khi có ít mẫu trước đó. Như các kết quả được báo cáo trong Bảng 2, với bộ đệm bộ nhớ hạn chế có kích thước 200, có nghĩa là chỉ có 1 mẫu cho mỗi lớp trung bình. So với baseline cận dưới SGD (7.92% trong Class-IL), tất cả các phương pháp không có luyện tập và hầu hết các phương pháp dựa trên luyện tập

--- TRANG 10 ---
Bảng 3: Quên lãng trung bình của ER và các phần mở rộng của nó trên CIFAR10 (5 nhiệm vụ) và CIFAR100 (20 nhiệm vụ), thấp hơn là tốt hơn. Kích thước bộ nhớ là 200 cho cả hai bộ dữ liệu.

Phương pháp CIFAR10 CIFAR100
Class-IL Task-IL Class-IL Task-IL
ER [27] 54.12 5.72 83.54 23.72
DER++ [7] 39.84 8.25 71.02 20.64
SER (Ours) 20.25 2.52 60.61 13.61

không thể giảm hiệu quả việc quên lãng với biên độ đáng chú ý. Bởi vì chỉ có ít mẫu cho mỗi lớp, rất khó để giữ lại kiến thức đã thu được từ trải nghiệm quá khứ hạn chế. Do đó, các phương pháp dựa trên luyện tập này thất bại trong việc giảm quên lãng.

Bằng cách mô phỏng trải nghiệm tương lai của mô hình cũ trên dữ liệu huấn luyện hiện tại, SER có thể chưng cất kiến thức bổ sung và giảm thêm việc quên lãng. Từ Bảng 2, chúng ta có thể quan sát thấy rằng SER cải thiện đáng kể hiệu suất trên TinyImageNet khi so sánh với hầu hết các phương pháp CL trước đó. Cụ thể, độ chính xác trung bình của SER tốt hơn nhiều so với DER++ trong Class-IL, tức là 28.50% so với 10.96%. Đối với Task-IL, SER vượt trội đáng kể so với DER++ với cải thiện tuyệt đối 29.25%. So với phương pháp Co2L gần đây khác, SER cũng đạt được cải thiện độ chính xác 14.6% trong Class-IL và hơn 27.7% trong Task-IL. Hơn nữa, SER vượt trội so với CLS-ER và TAMiL với biên độ đáng chú ý lần lượt là 5.03% và 8.04% trong Class-IL. Những kết quả này cho thấy rằng SER có thể bảo tồn hiệu quả kiến thức đã học khi có ít mẫu cho phương pháp phát lại trải nghiệm.

Đánh giá trên P-MNIST và R-MNIST. Để đánh giá hiệu suất của phương pháp được đề xuất trong Domain-IL, chúng tôi tuân theo các cài đặt thí nghiệm trong DER++[7], áp dụng 20 nhiệm vụ cho Permuted MNIST (P-MNIST) và Rotated MNIST (R-MNIST). Như độ chính xác trung bình được báo cáo trong Bảng 2, dựa trên bộ đệm bộ nhớ với 500 mẫu, hầu hết các phương pháp dựa trên luyện tập vượt trội so với các phương pháp không có luyện tập (ngoại trừ A-GEM) trên cả hai bộ dữ liệu. Tuy nhiên, bằng cách giảm kích thước bộ nhớ xuống 200, hiệu suất của ER hơi tệ hơn oEWC trên P-MNIST. Ngược lại, DER++ và SER cũng hiệu quả trong Domain-IL bằng cách tìm kiếm dự đoán nhất quán. Ngoài ra, CLS-ER đạt hiệu suất tốt nhất trên P-MNIST và R-MNIST và nó có thể so sánh với SER và DER++ trên cả hai bộ dữ liệu.

Dựa trên những quan sát này, có thể kết luận rằng tính nhất quán thuận được đề xuất hiệu quả hơn trong Class-IL và Task-IL so với Domain-IL. Lý do là không gian danh mục trong Class-IL và Task-IL không liền kề trong khi nó giống nhau trong Domain-IL. Do đó, việc tìm kiếm tính nhất quán với dữ liệu huấn luyện hiện tại có thể ghi đè kiến thức trước đó và nó không giảm thiểu thêm việc quên lãng.

Quên lãng Trung bình. Ngoài độ chính xác trung bình, quên lãng là một độ đo quan trọng khác để đo hiệu suất của một phương pháp CL. Bảng 3 cho thấy quên lãng trung bình của ER, DER++, và SER của chúng tôi. Có thể quan sát thấy rằng SER đạt được quên lãng thấp nhất trong tất cả các thí nghiệm so sánh, bao gồm cả Class-IL và Task-IL trên hai bộ dữ liệu. Những kết quả này cho thấy rằng SER hiệu quả giảm vấn đề quên lãng, xác minh hiệu quả của mất mát nhất quán thuận được đề xuất.

4.4 Nghiên cứu Ablation

Để phân tích thêm hiệu quả của mỗi mất mát trong phương pháp của chúng tôi, chúng tôi báo cáo kết quả nghiên cứu ablation trong Bảng 4. Không lưu trữ bất kỳ mẫu trước đó nào, việc chỉ tinh chỉnh mô hình với Lt
cls trên dữ liệu huấn luyện hiện tại gần như quên tất cả kiến thức trước đó trong Class-IL, chỉ là 19.26% trên CIFAR10 và 4.73% trên CIFAR100. Bằng cách phát lại một số lượng nhỏ mẫu trước đó, ER hiệu quả giảm vấn đề quên lãng trên cả hai bộ dữ liệu với Lt
cls+Lm
cls. Ngoài ra, dựa trên mất mát nhất quán ngược Lm
bc, DER++ giảm thêm việc quên lãng bằng cách tìm kiếm dự đoán nhất quán trên dữ liệu được lưu trữ trong bộ đệm bộ nhớ. Hơn nữa, việc kết hợp hai mất mát phân loại và hai mất mát nhất quán, tức là Lt
cls+Lm
cls+Lm
bc+Lt
fc, SER đạt hiệu suất tốt nhất trong tất cả các thí nghiệm so sánh.

Đáng chú ý rằng việc chỉ mở rộng ER với mất mát nhất quán thuận Lt
fc một mình cũng vượt trội so với DER++. Ví dụ, từ kết quả trên CIFAR10 được báo cáo trong Bảng 4, độ chính xác trung bình được cải thiện từ 64.88% lên 69.04% trên Class-IL và 91.92% lên 95.79% trên Task-IL. Những quan sát này cho thấy rằng việc tìm kiếm dự đoán nhất quán trên dữ liệu huấn luyện hiện tại cũng có thể

--- TRANG 11 ---
Hình 5: Độ chính xác của các phương pháp CL khác nhau sau khi học tuần tự mỗi nhiệm vụ trên bộ dữ liệu CIFAR10 trong tình huống Class-IL. Các mô hình được đánh giá ở cuối mỗi nhiệm vụ (trục y) để đánh giá cách hiệu suất nhiệm vụ (trục x) bị ảnh hưởng khi quá trình huấn luyện tiến triển. Kích thước bộ đệm bộ nhớ là 200.

Bảng 4: Nghiên cứu ablation về mỗi mất mát của SER với độ chính xác trung bình. Kích thước bộ nhớ là 200. CIFAR10 bao gồm 5 nhiệm vụ và CIFAR100 bao gồm 20 nhiệm vụ. Theo các mất mát được sử dụng trong các phương pháp trước đó, một số kết hợp mất mát có thể được biểu diễn như (1) SGD: Lt
cls; (2) ER: Lt
cls+Lm
cls; (3) DER++: Lt
cls+ Lm
cls+Lm
bc.

[Bảng hiển thị các kết hợp mất mát và kết quả tương ứng]

giúp bảo tồn kiến thức đã thu được. Hơn nữa, tính nhất quán thuận có thể được tính toán trên dữ liệu huấn luyện hoàn chỉnh và nó có thể truyền phân phối nhất quán đến các nhiệm vụ mới theo thời gian. Do đó, mất mát nhất quán thuận thể hiện hiệu suất tốt hơn khi được cung cấp bộ đệm bộ nhớ nhỏ.

4.5 Tình trạng Tiến thoái Lưỡng nan Tính Ổn định-Dẻo

Để hiểu rõ hơn về cách các phương pháp CL khác nhau tạo ra sự cân bằng giữa tính ổn định và dẻo, chúng tôi báo cáo hiệu suất của một số phương pháp CL sau khi học tuần tự 5 nhiệm vụ trên bộ dữ liệu CIFAR10 trong Hình 5. Về độ chính xác trung bình, DER++, CLS-ER, và SER vượt trội so với ER với sự giúp đỡ của mất mát nhất quán, cho thấy hiệu quả của tính ổn định. Mặt khác, ER đạt được độ chính xác tốt hơn trên dữ liệu huấn luyện hiện tại, và nó vượt trội so với các phương pháp khác về tính dẻo. Dựa trên tính nhất quán thuận được đề xuất, SER cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ trước đó và nó duy trì sự cân bằng tốt hơn. So với DER++ và CLS-ER, SER bảo tồn nhiều kiến thức hơn của các nhiệm vụ trước đó và nó cũng đạt hiệu suất tốt trên nhiệm vụ gần đây. Do đó, SER vượt trội so với các phương pháp so sánh với độ đo độ chính xác trung bình, xác minh hiệu quả của nó.

4.6 Phân tích Chi phí Tính toán

Dựa trên cùng backbone, chi phí tính toán của các phương pháp CL khác nhau phụ thuộc vào hiệu quả huấn luyện mô hình. Vì chúng tôi áp dụng ít hoặc cùng số epoch huấn luyện như các phương pháp baseline [7, 2], chúng tôi chủ yếu phân tích chi phí tính toán bằng cách so sánh thiết kế mô hình. Như được minh họa trong Thuật toán 1, ngoài việc tối ưu hóa mô hình cơ bản trên dữ liệu huấn luyện hiện tại, chúng tôi lấy một batch mẫu từ bộ đệm bộ nhớ cho cả phương pháp phát lại trải nghiệm và tính nhất quán ngược cùng một lúc. Ngược lại, DER++ [7] lấy hai batch mẫu từ bộ đệm bộ nhớ, trong đó một batch mẫu dành cho phương pháp phát lại trải nghiệm trong khi batch khác dành cho tính nhất quán ngược. Do đó, mặc dù chúng tôi kết hợp mất mát nhất quán thuận bằng cách tính toán logits đầu ra của mô hình cũ trên dữ liệu huấn luyện hiện tại, SER không tăng chi phí tính toán khi so sánh với DER++. Ngược lại, CLS-ER [2] áp dụng mô hình hoạt động cho phương pháp phát lại trải nghiệm, và nó bổ sung tính toán tính nhất quán với bộ đệm bộ nhớ bằng mô hình ổn định và mô hình dẻo. So với chi phí tính toán của CLS-ER, phương pháp SER được đề xuất hiệu quả hơn.

4.7 Thảo luận

Phương pháp SER được đề xuất kết hợp phương pháp phát lại trải nghiệm và regularization mô hình trong CL. So với các phương pháp dựa trên luyện tập trước đó, chúng tôi bổ sung thiết kế mất mát nhất quán thuận để cải thiện khả năng tổng quát hóa mô hình trên các nhiệm vụ trước đó. Vì tính nhất quán thuận có thể tận dụng nhiều dữ liệu hơn để giảm thay đổi mô hình nghiêm trọng trong quá trình huấn luyện và truyền phân phối logit quá khứ đến các nhiệm vụ mới theo thời gian, vấn đề quên lãng được giảm hiệu quả. Hơn nữa, phương pháp được đề xuất dễ triển khai, và chi phí tính toán của nó không đắt khi so sánh với các phương pháp tiên tiến nhất, ví dụ: DER++ và CLS-ER. Do đó, phương pháp SER được đề xuất đơn giản và hiệu quả. Chúng tôi hy vọng nó có thể cải thiện khả năng học gia tăng của các mô hình sâu khác trong tương lai.

5 Kết luận

Chúng tôi đề xuất Phương Pháp Phát Lại Trải Nghiệm Mạnh (SER) chưng cất kiến thức đã học từ cả trải nghiệm quá khứ và tương lai. Ngoài việc ghi nhớ nhãn thực tế của các mẫu trước đó được lưu trữ trong bộ đệm bộ nhớ, SER cũng tìm kiếm dự đoán nhất quán giữa mô hình mới và mô hình cũ trên dữ liệu từ cả bộ đệm bộ nhớ và dòng đầu vào. Bằng cách áp dụng mất mát nhất quán ngược và thuận vào việc huấn luyện mô hình, SER hiệu quả giảm vấn đề quên lãng khi có ít mẫu. Các thí nghiệm mở rộng chứng minh rằng SER vượt trội so với các phương pháp tiên tiến nhất với biên độ lớn. Hơn nữa, kết quả nghiên cứu ablation trên hai bộ dữ liệu cho thấy hiệu quả của tính nhất quán thuận được đề xuất.

Lời cảm ơn. Nghiên cứu này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Grant 62002188, và Chương trình Quỹ Nhà khoa học Trẻ Ưu tú Shandong (Hải ngoại) 2023HWYQ-114.

Tính sẵn có của Dữ liệu. Tất cả các bộ dữ liệu được sử dụng trong bài viết này đều có sẵn công khai. Chia sẻ dữ liệu không áp dụng cho bài viết này vì không có bộ dữ liệu mới nào được tạo ra trong nghiên cứu hiện tại.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo từ [1] đến [46] với các chi tiết đầy đủ về tác giả, tiêu đề, và xuất bản]
