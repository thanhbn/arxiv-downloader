# 2307.05399.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2307.05399.pdf
# File size: 772365 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Domain-Agnostic Neural Architecture for Class Incremental Continual
Learning in Document Processing Platform
Mateusz Wójcik1,2, Witold Ko ´sciukiewicz1,2, Mateusz Baran1,2,
Tomasz Kajdanowicz1, Adam Gonczarek2
1Wroclaw University of Science and Technology2Alphamoon Ltd., Wrocław
{mateusz.wojcik,tomasz.kajdanowicz}@pwr.edu.pl
adam.gonczarek@alphamoon.ai
Abstract
Production deployments in complex systems
require ML architectures to be highly efficient
and usable against multiple tasks. Particularly
demanding are classification problems in which
data arrives in a streaming fashion and each
class is presented separately. Recent meth-
ods with stochastic gradient learning have been
shown to struggle in such setups or have limita-
tions like memory buffers, and being restricted
to specific domains that disable its usage in real-
world scenarios. For this reason, we present a
fully differentiable architecture based on the
Mixture of Experts model, that enables the
training of high-performance classifiers when
examples from each class are presented sepa-
rately. We conducted exhaustive experiments
that proved its applicability in various domains
and ability to learn online in production envi-
ronments. The proposed technique achieves
SOTA results without a memory buffer and
clearly outperforms the reference methods.
1 Introduction
Solutions based on deep neural networks have al-
ready found their applications in almost every do-
main that can be automated. An essential part
of them is NLP, the development of which has
gained particular momentum with the beginning
of the era of transformers (Vaswani et al., 2017).
Complex and powerful models made it possible
to solve problems such as text classification with
a previously unattainable accuracy. However, ex-
ploiting the capabilities of such architectures in
real-world systems requires online learning after
deployment. This is especially difficult in dynami-
cally changing environments that require the mod-
els to be frequently retrained due to domain or class
setup shifts. An example of such environment is
Alphamoon Workspace1where the presented archi-
tecture will be deployed as a model for document
1https://alphamoon.ai/
Figure 1: Continual learning in document processing
platform. Classification models need to learn incremen-
tally and handle domain shifts after deployment.
classification since we noticed the emerging need
for online learning. We observed that the users’
data in document classification process is changing
frequently and such shifts often decrease the model
accuracy. As a result, we have to retrain the models
manually ensuing a time-consuming process. Our
goal was to design an effective approach to incre-
mental learning that will be used in a continual
learning module of our system (Figure 1).
Recently, neural architectures have become ef-
fective and widely used in classification problems
(Devlin et al., 2018; Rawat and Wang, 2017). The
parameter optimization process based on gradi-
ent descent works well when the data set is suf-
ficiently large and fully available during the train-
ing process. Otherwise, the catastrophic forgetting
(French, 1999) may occur, which makes neural
networks unable to be trained incrementally. Con-
tinual learning aims to develop methods that enable
accumulating new knowledge without forgetting
previously learnt one.
In this paper, we present a domain-agnostic ar-
chitecture for online class incremental continual
learning called DE&E (Deep Encoders and Ensem-
bles). Inspired by the E&E method (Shanahan et al.,
2021), we proposed a method that increases its ac-
curacy, provides full differentiability, and, mostarXiv:2307.05399v1  [cs.LG]  11 Jul 2023

--- PAGE 2 ---
importantly, can effectively solve real-world classi-
fication problems in production environments. Our
contribution is as follows: 1) we introduced a differ-
entiable KNN layer (Xie et al., 2020) into the model
architecture, 2) we proposed a novel approach to
aggregate classifier predictions in the ensemble, 3)
we performed exhaustive experiments showing the
ability to learn incrementally and real-world us-
ability, 4) we demonstrate the effectiveness of the
proposed architecture by achieving SOTA results
on various data sets without a memory buffer.
2 Related work
2.1 Continual Learning
2.1.1 Methods
Currently, methods with a memory buffer such as
GEM (Lopez-Paz and Ranzato, 2017), A-GEM
(Chaudhry et al., 2019a) or DER (Buzzega et al.,
2020) usually achieve the highest performance in
all continual learning scenarios (Mai et al., 2022).
Such methods store part of the data in the mem-
ory and this data is successively replayed during
training on new, unseen examples. However, the
requirement to store data in memory disqualifies
these methods in many practical applications due
to privacy policies or data size (Salem et al., 2018).
This forces attention toward other approaches, such
as parameter regularization. The most popular
methods in this group include EWC (Kirkpatrick
et al., 2016) and LWF (Li and Hoiem, 2017). When
receiving a new dose of knowledge, these meth-
ods attempt to influence the model parameter up-
dating procedure to be minimally invasive. As
research shows (Van de Ven and Tolias, 2019),
regularization-based methods fail in class incre-
mental scenarios making them ineffective in many
real-world cases.
2.1.2 Approaches for NLP
Almost all prior works focus on the development of
continual learning methods in the computer vision
domain (Delange et al., 2021). Research on contin-
ual learning for NLP is limited and, as Biesial-
ska et al. (2020) observed, the majority of cur-
rent NLP methods are task-specific. Moreover,
these methods often use a memory buffer (de Mas-
son D’Autume et al., 2019) or relate to the language
model itself (Ke et al., 2021). To address this niche,
domain-agnostic approaches have to become much
more prevalent in the near future.2.2 Ensemble methods
Ensemble methods are widespread in the world of
machine learning (Zhang and Ma, 2012). By using
predictions of multiple weak learners, it is possi-
ble to get a model that performs surprisingly well
overall. Broad adoption of methods (Cao et al.,
2020; Li and Pan, 2022; Yang et al., 2021) demon-
strates the effectiveness of ensemble techniques in
a wide variety of tasks. Ensembles have also been
used successfully in the field of continual learning,
as evidenced by the BatchEnsemble (Wen et al.,
2020) or CN-DPM (Lee et al., 2020). Other con-
tributions present in literature (Doan et al., 2022)
tend to focus strongly on improving model perfor-
mance rather than increasing model efficiency. Fur-
thermore, ensemble approaches can also be used
indirectly through dropout (Srivastava et al., 2014)
or weights aggregation (Wortsman et al., 2022).
2.3 Mixture of Experts
Mixture of Experts (ME) (Jacobs et al., 1991)
is a technique based on the divide and conquer
paradigm. It assumes dividing the problem space
between several specialized models (experts). Ex-
perts are supervised by the gating network that
selects them based on the defined strategy. The
difference between the ensembles is that ME meth-
ods focus on selecting a few experts rather than
combining predictions of all available models. ME
techniques have found many applications in various
domains (Masoudnia and Ebrahimpour, 2014), in-
cluding continual learning (Shanahan et al., 2021),
and even nowadays such approaches are widely
used in NLP (Gao et al., 2022; Ravaut et al., 2022).
2.4 Real-world NLP systems
Over the last few years, the amount of real-world
NLP applications has grown rapidly (Sarker, 2022).
Despite major successes in the real-world appli-
cation of language technologies such as Google
Translate, Amazon Alexa, and ChatGPT, produc-
tion deployment and maintenance of such models
still remain a challenge. Researchers have shown
(Nowakowski et al., 2022; Karakanta et al., 2021),
that there are several issues related to maintaining
NLP models, including technical limitations, la-
tency, and performance evaluation. However, the
crucial problem is the shift of data domain that
forces models to be retrained and deployed again
over time (Hu et al., 2020). It is a major limitation
in dynamically changing environments where users

--- PAGE 3 ---
Figure 2: Architecture of the proposed model. An input is processed by the feature extractor. Obtained embeddings
are used to find the most relevant classifiers according to assigned keys. The soft KNN layer approximates the soft
KNN scores. Predictions are weighted in the voting layer by both cosine similarity and soft KNN scores. Final
output is the class with the highest voting score.
expect models to quickly adapt to them. Currently,
this problem has been tackled in several systems
(Afzal et al., 2019; Hancock et al., 2019), but many
of the solutions preclude maintaining model ac-
curacy when training incrementally making them
insufficient.
3 Our approach
3.1 Problem formulation
Class incremental continual learning involves
training a classification model f(·) :X7−→
Yon a sequence of Ttasks. The model is
trained on each task separately (one task at a
time). Each task Dtcontains data points Dt=
{(x1
t, y1
t), . . . , (xNt
t, yNt
t)}, where Ntis length of
Dt,x(i)
t∈RD, and y(i)
t∈Yt.Ytis a label set
for task tandYt∩Yt′=∅fort̸=t′. We want
the model to keep performing well on all previous
tasks after each update, and we assume to be work-
ing in the most challenging setup (Van de Ven and
Tolias, 2019), where one task consists of data from
one class.
3.2 Method
We present a flexible and effective domain-agnostic
architecture that can be used to solve various clas-
sification problems. The architecture is presented
in Figure 2.
Feature extractor. The first component of the
proposed architecture is a multi-layer feature ex-tractor that transforms input data into the embed-
ding space. It can be described by the following
mapping z=F(x), where x∈RDis an input
example and z∈RMis aM-dimensional embed-
ding. The approach we follow assumes the use of a
pre-trained model with frozen parameters. Such a
procedure makes it possible to completely prevent
the extractor from forgetting knowledge by isolat-
ing feature space learning from the classification
process.
Keys and classifiers. We use an ensemble of N
classifiers fn(·), where each of them maps the
embedding into a K-dimensional output vector
ˆ yn=fn(z). With each classifier, there is an asso-
ciated key vector kn∈RMwith the same dimen-
sionality as the embedding. The keys help to select
the most suitable models for specialization with
respect to the currently processed input example.
They are initialized randomly from normal distribu-
tion. We use simple single-layer neural networks
as classifiers, with fan-in variance scaling as the
weight initialization strategy. The network output
is activated by a hyperbolic tangent function ( tanh).
Softκ-nearest neighbors layer. The standard
KNN algorithm is often implemented using ordi-
nary sorting operations that make it impossible to
determine the partial derivatives with respect to the
input. It removes the ability to use KNN as part
of end-to-end neural models. However, it is pos-
sible to obtain a differentiable approximation of

--- PAGE 4 ---
the KNN model by solving the Optimal Transport
Problem (Peyré et al., 2019). Based on this concept,
we add a differentiable layer to the model architec-
ture. We call this layer soft κ-nearest neighbors
(soft KNN ). In order to determine the KNN approx-
imation, we first compute a cosine distance vector
c∈RNbetween the embedding and the keys:
cn= 1−cos(z,kn), (1)
where cos(·,·)denotes the cosine similarity. Next,
we follow the idea of a soft top- κoperator pre-
sented in (Xie et al., 2020), where κdenotes the
number of nearest neighbors. Let E∈RN×2be
the Euclidean distance matrix with the following
elements:
en,0= (cn)2, en,1= (cn−1)2. (2)
And let G∈RN×2denote the similarity matrix
obtained by applying the Gaussian kernel to E:
G= exp( −E/σ), (3)
where σdenotes the kernel width. The expopera-
tors are applied elementwise to the matrix E.
We then use the Bregman method, an algorithm
designed to solve convex constraint optimization
problems, to compute Literations of Bregman pro-
jections in order to approximate their stationary
points:
p(l+1)=µ
Gq(l),q(l+1)=ν
G⊤p(l+1), (4)
where l= 0, . . . , L −1,µ=1N/N,ν=
[κ/N, (N−κ)/N]⊤,q(0)=12/2, and1idenotes
thei-element all-ones vector. Finally, let Γdenotes
the optimal transport plan matrix and is given by:
Γ= diag( p(L))·G·diag(q(L)) (5)
As the final result γ∈RNof the soft κ-nearest
neighbor operator, we take the second column of
Γmultiplied by Ni.e.γ=NΓ:,2.γis a soft
approximation of a zero-one vector that indicates
which κout of Ninstances are the nearest neigh-
bors. Introducing the soft KNN enables to train
parts of the model that were frozen until now.
Voting layer. We use both cnandγto weight the
predictions by giving the higher impact for classi-
fiers with keys similar to extracted features. The
obtained approximation γhas two main functional-
ities. It eliminates the predictions from classifiersTable 1: Data sets setup for experiments.
Domain Data set Classes Train Test Avg. words
TextBBC News 5 1,668 557 380
Newsgroups 10 11314 7532 315
Complaints 10 16,000 4,000 228
Audio Speech Commands 10 18,538 2,567 —
ImageMNIST 10 60,000 10,000 —
CIFAR-10 10 50,000 10,000 —
outside κnearest neighbors and weights the result.
Since the Bregman method does not always com-
pletely converge, the vector κcontains continuous
values that are close to 1 for the most relevant clas-
sifiers. We make use of this property during the
ensemble voting procedure. The higher the κvalue
for a single classifier, the higher its contribution
toward the final ensemble decision. The final pre-
diction is obtained as follows:
ˆ y=PN
n=1γncnˆ ynPN
n=1cn(6)
Training To effectively optimize the model pa-
rameters, we follow the training procedure pre-
sented in (Shanahan et al., 2021). It assumes the
use of a specific loss function that is the inner prod-
uct between the ensemble prediction and the one-
hot coded label:
L(y,ˆy) =−y⊤ˆy (7)
Optimizing this criterion yields an advantage of
using a tanh activation function, significantly reduc-
ing catastrophic forgetting (Shanahan et al., 2021).
Following the reference method, we also use an op-
timizer that discards the value of the gradient and
uses only its sign to determine the update direction.
As a result, the parameters are being changed by a
fixed step during the training.
4 Experiments
4.1 Setup
In order to ensure experiment’s reproductivity, we
evaluated our method on the popular and publicly
available data sets.
Data sets We use three common text classifica-
tion data sets with different characteristics - News-
groups (Lang, 2008), BBC News (Greene and
Cunningham, 2006), and Consumer Finance Com-
plaints2. The goal of the experiments was to eval-
uate our method on tasks with with different dif-
2Source: https://huggingface.co/datasets/
consumer-finance-complaints

--- PAGE 5 ---
Table 2: Accuracy (%) and standard deviation for methods evaluated on various data sets. Speech Commands
data set was evaluated with 64 classifiers in ME, the remaining models have 128 classifiers. Regularization-based
methods completely failed on the difficult data sets due to the recency bias phenomenon (Mai et al., 2022).
Text Image Audio
Model Mem. NG BBC Compl. MNIST CIFAR-10 Sp. Comm.
Naive × 5.25±0.03 21.65 ±2.56 9.56±0.33 11.29 ±3.05 10.00 ±0.01 21.54 ±3.78
LwF × 5.20±0.05 18.60 ±2.03 10.04 ±0.20 11.47 ±2.75 10.00 ±0.01 20.61 ±3.88
EWC × 5.13±0.13 21.97 ±2.14 10.16 ±0.31 11.19 ±2.70 10.00 ±0.01 32.93 ±4.92
SI × 5.27±0.01 19.43 ±2.96 10.00 ±0.62 14.90 ±6.52 10.00 ±0.01 9.99±0.27
CWR* × 4.63±0.60 22.98 ±1.20 10.13 ±0.33 10.40 ±0.54 10.00 ±0.01 10.32 ±0.26
GEM ✓ 35.89 ±3.80 70.99 ±7.68 33.74 ±2.50 52.27 ±5.20 23.40 ±2.71 21.01 ±2.06
A-GEM ✓ 9.44±7.14 59.10 ±17.52 9.20±0.01 65.37 ±4.53 26.43 ±5.27 17.45 ±6.90
Replay ✓ 22.45 ±3.09 59.61 ±3.17 16.46 ±4.62 69.02 ±4.90 32.93 ±4.56 12.23 ±1.28
E&E × 46.07 ±2.91 75.87 ±3.88 44.80 ±1.62 87.10 ±0.21 53.97 ±1.31 79.15 ±0.60
Ours × 47.27 ±3.63 78.49 ±3.92 44.97 ±0.86 87.62 ±0.14 56.27 ±1.21 80.11 ±1.30
ficulty levels. We also conducted experiments for
audio classification using Speech Commands (War-
den, 2018) data set. For the evaluation purposes, we
selected the 10 most representative classes from the
Newsgroups, Complaints and Speech Commands.
Finally, we also conducted experiments on the pop-
ular MNIST and CIFAR-10 data sets as image do-
main representatives. The data set summary is
presented in Table 1. In all experiments we used
a train set to train model incrementally, and after-
ward we performed a standard evaluation using a
test set.
Feature extractors For all text data sets, we used
a Distilbert (Sanh et al., 2019), a light but still
very effective alternative for large language models.
Next, for Speech Commands, we utilized Pyannote
(Bredin et al., 2020), a pretrained model for produc-
ing meaningful audio features. For image data sets,
we used different extractors. MNIST features were
produced by the pretrained V AE and CIFAR-10
has a dedicated BYOL model (see A.4 for more
details).
4.2 Results
The results of the evaluation are presented in Ta-
ble 2. For all setups evaluated, our model per-
formed best improving results of the main reference
method (E&E) by up to 3 percent points (pp.). The
improvement scale varies across the data sets. We
also observed a significant difference in achieved
accuracy between the DE&E and the standard con-
tinual learning methods. Simple regularization-
based methods completely fail in the class incre-
mental scenario. It shows how demanding training
Figure 3: Number of parameters in DE&E architecture
(64, 128, 1024 classifiers) and achieved accuracy (%).
We calculated the number of parameters as the sum of
the parameters for all classifiers in the ME. Each mark
is the test accuracy averaged across 5 runs.
the model incrementally is when a set of classes
is not fixed, which often takes place in real-world
scenarios. Furthermore, our method achieved these
results without replaying training examples seen
in the past, making it more practical relative to the
SOTA memory-based methods (GEM, A-GEM, Re-
play) that store samples from every class. For the
ensemble of 128 classifiers and Speech Commands
data set, our architecture achieved an accuracy of
more than 59 pp. higher than the best method with
a memory buffer.
One of the most important hyperparameters of
the model is the number of classifiers (experts).
To investigate how it affects accuracy, we evalu-
ated our architecture in three variants: small - 64,
normal - 128, and large - 1024 classifiers. The
evaluation results are presented in Figure 3. We
observed that increasing the ensemble size trans-

--- PAGE 6 ---
Table 3: Accuracy (%) and standard deviation of DE&E
evaluated on Class Incremental and Domain Incremental
scenarios. We used the same setup as shown in Table 2.
Data set Class Incremental Domain incremental
BBC News 78.49 ±3.92 79.71 ±3.14
Newsgroups 47.27 ±3.63 44.55 ±1.40
Complaints 44.97 ±0.86 39.23 ±3.03
Speech Commands 81.46 ±0.85 79.31 ±0.49
MNIST 87.62 ±0.14 85.04 ±0.39
CIFAR-10 56.27 ±1.21 55.66 ±1.32
lates to higher accuracy, and gain depends on the
setup and data characteristics. The most significant
improvement was observed on BBC and CIFAR-
10 where the large model achieved an accuracy of
about 20pp. better than the small one. For the re-
maining data sets and the analogous setup, the gain
was up to 5pp. We explain this phenomenon as the
effect of insufficient specialization level achieved
by smaller ensembles. If experts are forced to solve
tasks that are too complicated they make mistakes
often. Increasing the number of experts allows
for dividing feature space into simpler sub-tasks.
However, such a procedure has natural limitations
related to the feature extractor. If features have low
quality, increasing the number of experts will be
ineffective. To select the optimal ensemble size we
suggest using the elbow rule which prevents the
model from being overparameterized and ensures
reasonable accuracy. However, in general, we rec-
ommend choosing larger ensembles that are better
suited for handling real-world cases.
Since real-world environments require deployed
models to quickly adapt to domain shifts, we tested
our method in a domain incremental scenario. In
such setup, each data batch can provide examples
from multiple classes that can be either known or
new (Van de Ven and Tolias, 2019). This way, the
model needs to learn incrementally, being prone to
frequent domain shifts. As shown in Table 3, the
proposed method handles both scenarios with com-
parable accuracy. We observed improved accuracy
for BBC News, but reduced for the remaining data
sets. Such property can be beneficial when there
is limited prior knowledge about the data or the
stream is imbalanced (Aguiar et al., 2022).
We have also investigated the importance of the
presented expert selection method. We trained the
DE&E method and for each training example, we
allowed it to choose random experts (rather than
the most relevant ones) with fixed probability p.
As shown in Figure 4, the selection method has astrong influence on the model performance. Accu-
racy decreases proportionally to the pover all data
sets studied. The proper expert selection technique
is crucial for the presented method. It is worth not-
ing that relatively easier data sets suffer less from
loss of accuracy than hard ones because even ran-
domly selected experts can still classify the data
by learning simple general patterns. In more dif-
ficult cases like Newsgroups and Complaints data
sets, model performance is comparable to random
guessing when p >0.5.
Figure 4: Influence of random classifier selection on
DE&E accuracy (%). All models consist of 128 classi-
fiers. Each mark is the accuracy for an independent run.
5 Conclusions
In this paper, we proposed a domain-agnostic archi-
tecture for continual learning with a training proce-
dure specialized in challenging class incremental
problems. The presented architecture is based on
the Mixture of Experts technique and handles many
practical issues related to the deployment of text
classification models in non-trivial real-world sys-
tems. As our main contribution, we introduced a
fully differentiable soft KNN layer and a novel pre-
diction weighting strategy. By conducting exhaus-
tive experiments, we showed improvement in accu-
racy for all the cases studied and achieved SOTA re-
sults without using a memory buffer. This enables
an effective and secure training, especially when
working with sensitive textual data. The presented
architecture is highly flexible, can effectively solve
classification problems in many domains, and can
be applied to real-world machine learning systems
requiring continuous improvement. Such work en-
ables researchers to make further steps toward over-
running many of the current challenges related to
language technology applications.

--- PAGE 7 ---
Limitations
The main limitations of the proposed architecture
are related to the presence of the frozen feature
extractor. The accuracy of the classification mod-
ule is proportional to the quality of features. Since
the ensemble weak learners are single-layer neu-
ral networks, the entire feature extraction process
relies on a pre-trained model that strongly limits
the upper bound of classification accuracy. Such
approach reduces the method complexity, but also
makes it prone to errors when embeddings have low
quality. Achieving accuracy at a satisfactory level,
which is crucial in real world systems, requires the
use of high quality feature extractors. Currently,
plenty of pretrained SOTA models are available for
free in domains such as text or image classification,
but if such extractor is not available, does not pro-
duce reasonable features or is too expensive to use,
our architecture may not be the best choice.
Another issue is relatively long training time
comparing to the reference methods (see A.3). The
introduction of a differentiable soft KNN layer
resulted in additional computational effort that
clearly impacted the model complexity. This lim-
its the use in low latency systems with machine
learning models trained online.
Ethics Statement
The authors foresee no ethical concerns with the
work presented in this paper, in particular concern-
ing any kind of harm and discrimination. Since
the presented architecture can have a wide range
of usages, the authors are not responsible for any
unethical applications of this work.
Acknowledgements
The research was conducted under the Implemen-
tation Doctorate programme of Polish Ministry of
Science and Higher Education and also partially
funded by Department of Artificial Intelligence,
Wroclaw Tech and by the European Union under
the Horizon Europe grant OMINO (grant number
101086321). It was also partially co-funded by
the European Regional Development Fund within
the Priority Axis 1 “Enterprises and innovation”,
Measure 1.2. “Innovative enterprises, sub-measure
1.2.1. “Innovative enterprises – horizontal com-
petition” as part of ROP WD 2014-2020, support
contract no. RPDS.01.02.01-02-0063/20-00.References
Shazia Afzal, Tejas Dhamecha, Nirmal Mukhi, Renuka
Sindhgatta, Smit Marvaniya, Matthew Ventura, and
Jessica Yarbro. 2019. Development and deployment
of a large-scale dialog-based intelligent tutoring sys-
tem. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 2 (Industry Papers) , pages 114–121,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Gabriel Aguiar, Bartosz Krawczyk, and Alberto Cano.
2022. A survey on learning from imbalanced data
streams: taxonomy, challenges, empirical study,
and reproducible experimental framework. arXiv
preprint arXiv:2204.03719 .
Magdalena Biesialska, Katarzyna Biesialska, and
Marta R Costa-Jussa. 2020. Continual lifelong learn-
ing in natural language processing: A survey. arXiv
preprint arXiv:2012.09823 .
Hervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gre-
gory Gelly, Pavel Korshunov, Marvin Lavechin,
Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and
Marie-Philippe Gill. 2020. pyannote.audio: neural
building blocks for speaker diarization. In ICASSP
2020, IEEE International Conference on Acoustics,
Speech, and Signal Processing , Barcelona, Spain.
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Da-
vide Abati, and Simone Calderara. 2020. Dark expe-
rience for general continual learning: a strong, simple
baseline. Advances in neural information processing
systems , 33:15920–15930.
Yue Cao, Thomas Andrew Geddes, Jean Yee Hwa
Yang, and Pengyi Yang. 2020. Ensemble deep learn-
ing in bioinformatics. Nature Machine Intelligence ,
2(9):500–508.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus
Rohrbach, and Mohamed Elhoseiny. 2019a. Efficient
lifelong learning with a-gem. In ICLR .
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-
seiny, Thalaiyasingam Ajanthan, Puneet K Doka-
nia, Philip HS Torr, and Marc’Aurelio Ranzato.
2019b. Continual learning with tiny episodic memo-
ries. arXiv preprint arXiv:1902.10486, 2019 .
Cyprien de Masson D’Autume, Sebastian Ruder, Ling-
peng Kong, and Dani Yogatama. 2019. Episodic
memory in lifelong language learning. Advances in
Neural Information Processing Systems , 32.
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and
Tinne Tuytelaars. 2021. A continual learning sur-
vey: Defying forgetting in classification tasks. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence .

--- PAGE 8 ---
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Thang Doan, Seyed Iman Mirzadeh, Joelle Pineau, and
Mehrdad Farajtabar. 2022. Efficient continual learn-
ing ensembles in neural network subspaces. arXiv
preprint arXiv:2202.09826 .
Robert M French. 1999. Catastrophic forgetting in con-
nectionist networks. Trends in cognitive sciences ,
3(4):128–135.
Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-
Yi Lu, and Ji-Rong Wen. 2022. Parameter-efficient
mixture-of-experts architecture for pre-trained lan-
guage models. In Proceedings of the 29th Inter-
national Conference on Computational Linguistics ,
pages 3263–3273, Gyeongju, Republic of Korea. In-
ternational Committee on Computational Linguistics.
Derek Greene and Pádraig Cunningham. 2006. Practi-
cal solutions to the problem of diagonal dominance
in kernel document clustering. In Proc. 23rd Interna-
tional Conference on Machine learning (ICML’06) ,
pages 377–384. ACM Press.
Braden Hancock, Antoine Bordes, Pierre-Emmanuel
Mazare, and Jason Weston. 2019. Learning from
dialogue after deployment: Feed yourself, chatbot!
InProceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics , pages 3667–
3684, Florence, Italy. Association for Computational
Linguistics.
Yipeng Hu, Joseph Jacob, Geoffrey JM Parker, David J
Hawkes, John R Hurst, and Danail Stoyanov. 2020.
The challenges of deploying artificial intelligence
models in a rapidly evolving pandemic. Nature Ma-
chine Intelligence , 2(6):298–300.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan,
and Geoffrey E Hinton. 1991. Adaptive mixtures of
local experts. Neural computation , 3(1):79–87.
Alina Karakanta, Sara Papi, Matteo Negri, and Marco
Turchi. 2021. Simultaneous speech translation for
live subtitling: from delay to display. In Proceedings
of the 1st Workshop on Automatic Spoken Language
Translation in Real-World Settings (ASLTRW) , pages
35–48, Virtual. Association for Machine Translation
in the Americas.
Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu.
2021. Achieving forgetting prevention and knowl-
edge transfer in continual learning. Advances in
Neural Information Processing Systems , 34:22443–
22456.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-
nowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell.
2016. Overcoming catastrophic forgetting in neural
networks. CoRR , abs/1612.00796.Lang. 2008. 20 newsgroups dataset.
Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gun-
hee Kim. 2020. A neural dirichlet process mix-
ture model for task-free continual learning. CoRR ,
abs/2001.00689.
Yang Li and Yi Pan. 2022. A novel ensemble deep learn-
ing model for stock prediction based on stock prices
and news. International Journal of Data Science and
Analytics , 13(2):139–149.
Zhizhong Li and Derek Hoiem. 2017. Learning without
forgetting. IEEE transactions on pattern analysis
and machine intelligence , 40(12):2935–2947.
Vincenzo Lomonaco and Davide Maltoni. 2017.
Core50: a new dataset and benchmark for contin-
uous object recognition. In Conference on Robot
Learning , pages 17–26. PMLR.
David Lopez-Paz and Marc’Aurelio Ranzato. 2017.
Gradient episodic memory for continual learning. In
Proceedings of the 31st International Conference on
Neural Information Processing Systems , NIPS’17,
page 6470–6479, Red Hook, NY , USA. Curran Asso-
ciates Inc.
Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe,
Hyunwoo Kim, and Scott Sanner. 2022. Online con-
tinual learning in image classification: An empirical
survey. Neurocomputing , 469:28–51.
Saeed Masoudnia and Reza Ebrahimpour. 2014. Mix-
ture of experts: a literature survey. The Artificial
Intelligence Review , 42(2):275.
Artur Nowakowski, Krzysztof Jassem, Maciej Lison,
Rafał Jaworski, Tomasz Dwojak, Karolina Wiater,
and Olga Posesor. 2022. nEYron: Implementation
and deployment of an MT system for a large au-
dit & consulting corporation. In Proceedings of the
23rd Annual Conference of the European Associa-
tion for Machine Translation , pages 183–189, Ghent,
Belgium. European Association for Machine Trans-
lation.
Gabriel Peyré, Marco Cuturi, et al. 2019. Computa-
tional optimal transport: With applications to data sci-
ence. Foundations and Trends ®in Machine Learn-
ing, 11(5-6):355–607.
Mathieu Ravaut, Shafiq Joty, and Nancy Chen. 2022.
SummaReranker: A multi-task mixture-of-experts
re-ranking framework for abstractive summarization.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 4504–4524, Dublin, Ireland.
Association for Computational Linguistics.
Waseem Rawat and Zenghui Wang. 2017. Deep
convolutional neural networks for image classifica-
tion: A comprehensive review. Neural computation ,
29(9):2352–2449.

--- PAGE 9 ---
Ahmed Salem, Yang Zhang, Mathias Humbert, Mario
Fritz, and Michael Backes. 2018. Ml-leaks: Model
and data independent membership inference attacks
and defenses on machine learning models. CoRR ,
abs/1806.01246.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .
Iqbal H Sarker. 2022. Ai-based modeling: Techniques,
applications and research issues towards automation,
intelligent and smart systems. SN Computer Science ,
3(2):158.
Murray Shanahan, Christos Kaplanis, and Jovana Mitro-
vic. 2021. Encoders and ensembles for task-free
continual learning. CoRR , abs/2105.13327.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search , 15(56):1929–1958.
Gido M Van de Ven and Andreas S Tolias. 2019. Three
scenarios for continual learning. arXiv preprint
arXiv:1904.07734 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Pete Warden. 2018. Speech commands: A dataset
for limited-vocabulary speech recognition. arXiv
preprint arXiv:1804.03209 .
Yeming Wen, Dustin Tran, and Jimmy Ba. 2020.
Batchensemble: an alternative approach to efficient
ensemble and lifelong learning. arXiv preprint
arXiv:2002.06715 .
Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak
Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
Ari S Morcos, Hongseok Namkoong, Ali Farhadi,
Yair Carmon, Simon Kornblith, et al. 2022. Model
soups: averaging weights of multiple fine-tuned mod-
els improves accuracy without increasing inference
time. arXiv preprint arXiv:2203.05482 .
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo
Zhao, Hongyuan Zha, Wei Wei, and Tomas Pfister.
2020. Differentiable top-k with optimal transport. In
Advances in Neural Information Processing Systems ,
volume 33, pages 20520–20531. Curran Associates,
Inc.
Yongquan Yang, Haijun Lv, and Ning Chen. 2021. A
survey on ensemble learning under the era of deep
learning. arXiv preprint arXiv:2101.08387 .Friedemann Zenke, Ben Poole, and Surya Ganguli.
2017. Continual learning through synaptic intel-
ligence. In International Conference on Machine
Learning , pages 3987–3995. PMLR.
Cha Zhang and Yunqian Ma. 2012. Ensemble machine
learning: methods and applications . Springer.
A Appendix
A.1 Code
Code is currently available as a Github repository
https://github.com/mateusz-wojcik-97/
domain-agnostic-architecture .
A.2 Computing resources
The machine we used had 128 GB RAM, an Intel
Core i9-11900 CPU, and an NVIDIA GeForce RTX
3060 GPU with 12GB VRAM. Every experiment
was performed using the GPU.
A.3 Time complexity
Table 4: Time (seconds) of training the ensemble models
with 128 classifiers on one task.
Dataset E&E Ours
Newsgroups 7.43 31.20
BBC News 14.96 151.79
Complaints 20.33 93.63
Sp. Commands 30.80 108.90
MNIST 28.01 270.30
CIFAR-10 104.25 355.82
The comparison in training time between E&E
and DE&E models is shown in Table 4. For all eval-
uated data sets, the training time of our model was
higher than the time to train the reference method.
The results vary between data sets. The introduc-
tion of a differentiable soft KNN layer resulted
in additional computational effort that clearly im-
pacted the time complexity of the model.
A.4 Implementation details
We use PyTorch to both reproduce the E&E results
and implement the DE&E method. For text classifi-
cation we used pretrained Distilbert3model and for
audio classification we used pretrained Pyannote4
model, both from the Huggingface repository. We
used a pre-trained ResNet-50 model as the feature
extractor for the CIFAR-10 data set. The model
is available in the following GitHub repository,
3https://huggingface.co/distilbert-base-uncased
4https://huggingface.co/pyannote/embedding

--- PAGE 10 ---
https://github.com/yaox12/BYOL-PyTorch ,
and is used under MIT Licence. For MNIST, we
trained a variational autoencoder on the Omniglot
data set and utilized encoder part as our feature
extractor. We based our implementation of the soft
KNN layer on the code provided with https://
proceedings.neurips.cc/paper/2020/hash/
ec24a54d62ce57ba93a531b460fa8d18-Abstract.
html . All data sets used are public.
Table 5: Architecture of neural networks used as back-
bones for baseline models depends on experimental
setup. Each network has a similar number of total pa-
rameters as in the ensemble.
Dataset Network layers
Newsgroups [1536, 1700, 768, 10]
Complaints [1536, 955, 512, 10]
BBC News [1536, 640, 5]
Sp. Commands [512, 1256, 10]
MNIST [512, 1256, 10]
CIFAR-10 [2048, 1274, 10]
Baselines We use Naive, LwF (Li and Hoiem,
2017), EWC (Kirkpatrick et al., 2016), SI (Zenke
et al., 2017), CWR* (Lomonaco and Maltoni,
2017), GEM (Lopez-Paz and Ranzato, 2017),
A-GEM (Chaudhry et al., 2019a) and Replay
(Chaudhry et al., 2019b) approaches as baselines
to compare with our method. We utilize the imple-
mentation from Avalanche ( https://avalanche.
continualai.org/ ), a library designed for con-
tinual learning tasks. The main purpose of this
comparison was to determine how the proposed
method performs against classical approaches and,
in particular, against the methods with memory
buffer, which gives a significant advantage in class
incremental problems. The recommended hyper-
parameters for each baseline method vary across
usages in literature, so we chose them based on
our own internal experiments. For a clarity, we
keep hyperparameter naming nomenclature from
the Avalnache library. For EWC we use lambda
=10000 . The LwF model was trained with alpha
=0.15andtemperature =1.5. For SI strategy,
we use lambda =5e7andeps=1e−7. The
hyperparameters of the memory based approach
GEM were set as follows: memory _strength =
0.5,patterns _per_exp =5, which implies that
with every task, 5 examples will be accumulated.
This has a particular importance when the number
of classes is large. With this setup and 10 classesin data set, memory contains 50 examples after
training on all tasks. Having a large memory buffer
makes achieving high accuracy much easier. For
the A-GEM method, use the same number of ex-
amples in memory and sample _size =20. All
models were trained using Adam optimizer with
alearning _rate of0.0005 andbatch _size of60.
We chose cross entropy as a loss function and per-
formed one training epoch for each experience. To
fairly compare baseline methods with ensembles,
as a backbone we use neural network with a similar
number of parameters (as in ensemble). Network
architectures for each experimental setup are shown
in Table 5. All baseline models were trained by pro-
viding embeddings produced by feature extractor
as an input.
Ensembles. We used E&E (Shanahan et al.,
2021) as the main reference method. It uses an
architecture similar to that of a classifier ensem-
ble, however the nearest neighbor selection mech-
anism itself is not a differentiable component and
the weighting strategy is different. In order to re-
liably compare the performance, the experimental
results of the reference method were fully repro-
duced. Both the reference method and the proposed
method used exactly the same feature extractors.
Thus, we ensured that the final performance is not
affected by the varying quality of the extractor, but
only depends on the solutions used in the model
architecture and learning method.
Both E&E and our DE&E were trained with the
same set of hyperparameters (excluding hyperpa-
rameters in the soft KNN layer for the DE&E). We
use ensembles of sizes 64, 128 and 1024. Based on
the data set, we used different hyperparameter sets
for the ensembles (Table 6).
The keys for classifiers in ensembles were ran-
domly chosen from the standard normal distribu-
tion and normalized using the L2norm. The same
normalization was applied to encoded inputs during
lookup for matching keys.
Soft KNN. We use the Sinkhorn algorithm to
perform the forward inference in soft KNN . The
Sinkhorn algorithm is useful in entropy-regularized
optimal transport problems thanks to its computa-
tional effort reduction. The soft KNN hasO(n)
complexity, making it scalable and allows us to
safely apply it to more computationally expensive
problems.
The values of soft KNN hyperparameters were

--- PAGE 11 ---
Table 6: Hyperparameters used for DE&E and E&E methods.
Dataset Classifiers Neighbors Batch size Learning rate Weight Decay
Newsgroups64 16
8 0.0001
0.0001128 32
1024 64
BBC News64 8
1 0.01 128 16
1024 32
Complaints64 16
8 0.0001 128 32
1024 64
Sp. Commands64 16
8 0.001 128 32
1024 64
MNIST 128 16 60 0.0001
CIFAR-10 128 16 60 0.0001
σ= 0.0005 andL= 400 . We utilize the contin-
uous character of an output vector to weight the
ensemble predictions. It is worth noting that we ad-
ditionally set the threshold of the minimum allowed
soft KNN score to 0.3. It means every element in
γlower than 0.3 is reduced to 0. We reject such
elements because they are mostly the result of non-
converged optimization and do not carry important
information. In this way, we additionally secure
the optimization result to be as representative as
possible.
