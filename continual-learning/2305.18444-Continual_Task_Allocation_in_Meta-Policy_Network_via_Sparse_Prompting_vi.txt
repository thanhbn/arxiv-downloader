# 2305.18444.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/continual-learning/2305.18444.pdf
# Kích thước tệp: 4856226 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Phân Bổ Tác Vụ Liên Tục trong Mạng Meta-Policy qua Sparse Prompting
Yijun Yang1 2Tianyi Zhou3Jing Jiang2Guodong Long2Yuhui Shi1
Tóm tắt
Làm thế nào để huấn luyện một meta-policy tổng quát bằng cách liên tục học một chuỗi các tác vụ? Đây là một kỹ năng tự nhiên của con người nhưng lại thách thức để đạt được bằng học tăng cường hiện tại: agent được kỳ vọng nhanh chóng thích ứng với các tác vụ mới (tính dẻo dai) trong khi vẫn duy trì kiến thức chung từ các tác vụ trước đó (tính ổn định). Chúng tôi giải quyết điều này bằng "Phân Bổ Tác Vụ Liên Tục qua Sparse Prompting (CoTASP)", học các từ điển over-complete để tạo ra các mask thưa thớt như prompts trích xuất một sub-network cho mỗi tác vụ từ mạng meta-policy. CoTASP huấn luyện một policy cho mỗi tác vụ bằng cách tối ưu hóa prompts và trọng số sub-network luân phiên. Từ điển sau đó được cập nhật để căn chỉnh các prompts đã tối ưu với embedding của tác vụ, từ đó nắm bắt các mối tương quan ngữ nghĩa của tác vụ. Do đó, các tác vụ liên quan chia sẻ nhiều neuron hơn trong mạng meta-policy do các prompts tương tự trong khi sự can thiệp chéo-tác vụ gây ra quên lãng được kiềm chế hiệu quả. Cho trước một meta-policy và các từ điển được huấn luyện trên các tác vụ trước đó, việc thích ứng tác vụ mới chỉ còn lại việc sparse prompting và finetuning sub-network có hiệu quả cao. Trong các thí nghiệm, CoTASP đạt được sự cân bằng tính dẻo dai-ổn định đầy hứa hẹn mà không cần lưu trữ hoặc phát lại bất kỳ kinh nghiệm nào của các tác vụ trong quá khứ. Nó vượt trội hơn các phương pháp RL liên tục và đa tác vụ hiện có trên tất cả các tác vụ đã thấy, giảm quên lãng, và tổng quát hóa cho các tác vụ chưa thấy. Mã của chúng tôi có sẵn tại https://github.com/stevenyangyj/CoTASP

1. Giới thiệu
Mặc dù học tăng cường (RL) đã thể hiện hiệu suất xuất sắc trong việc học một tác vụ duy nhất, ví dụ, chơi cờ vây (Silver et al., 2016), điều khiển robot (Schulman et al., 2017; Degrave et al., 2022), và tối ưu hóa policy offline (Yu et al., 2020; Yang et al., 2022), nó vẫn gặp khó khăn với quên lãng thảm khốc và can thiệp chéo-tác vụ khi học một luồng tác vụ ngay lập tức (McCloskey & Cohen, 1989; Bengio et al., 2020) hoặc một chương trình giảng dạy được tuyển chọn của các tác vụ (Fang et al., 2019; Ao et al., 2021; 2022). Vì vậy, việc huấn luyện một meta-policy có thể tổng quát hóa cho tất cả các tác vụ đã học hoặc thậm chí các tác vụ chưa thấy với khả năng thích ứng nhanh là thách thức, tuy nhiên đây là một kỹ năng cố hữu của việc học của con người.

Vấn đề này đã được công nhận là RL liên tục hoặc suốt đời (Mendez & Eaton, 2022) và thu hút sự quan tâm ngày càng tăng trong nghiên cứu RL gần đây.

Một thách thức chính và lâu dài trong RL liên tục là sự cân bằng tính dẻo dai-ổn định (Khetarpal et al., 2022): mặt một, policy RL cần duy trì và tái sử dụng kiến thức được chia sẻ qua các tác vụ khác nhau trong lịch sử (tính ổn định) trong khi mặt khác có thể được thích ứng nhanh chóng với các tác vụ mới mà không bị can thiệp từ các tác vụ trước đó (tính dẻo dai). Giải quyết thách thức này là quan trọng để cải thiện hiệu quả của RL liên tục và khả năng tổng quát hóa của policy đã học. Một meta-policy với tính ổn định tốt hơn có thể giảm sự cần thiết của phát lại kinh nghiệm và chi phí bộ nhớ/tính toán của nó. Hơn nữa, kích thước mạng cần thiết có thể được giảm hiệu quả nếu meta-policy có thể quản lý việc chia sẻ kiến thức qua các tác vụ theo cách compact và hiệu quả hơn. Do đó, tính ổn định có thể cải thiện đáng kể hiệu quả của RL liên tục khi số lượng tác vụ tăng lên (Shin et al., 2017; Li & Hoiem, 2018). Hơn nữa, tính dẻo dai tốt hơn chỉ ra khả năng thích ứng và tổng quát hóa nhanh hơn với các tác vụ mới.

Để trực tiếp giải quyết sự cân bằng tính dẻo dai-ổn định và vượt qua các hạn chế của công trình trước đó, chúng tôi nghiên cứu cách huấn luyện một mạng meta-policy trong RL liên tục. Sau đó, chỉ với một mô tả văn bản của một tác vụ đã học trước đó hoặc chưa thấy, một policy cụ thể có thể được trích xuất tự động và hiệu quả từ meta-policy. Điều này cùng tinh thần với prompting trong các mô hình ngôn ngữ lớn gần đây (Li & Liang, 2021; Liu et al., 2021) nhưng khác với các phương pháp hiện có chọn ngẫu nhiên các policy tác vụ (Rajasegaran et al., 2019; Mirzadeh et al., 2020) hoặc tối ưu hóa độc lập một policy cho mỗi tác vụ từ đầu (Serra et al., 2018; Kang et al., 2022). Để đạt được điều này, chúng tôi đề xuất học các từ điển theo lớp cùng với mạng meta-policy để tạo ra sparse prompts (tức là, binary masks) cho mỗi tác vụ, trích xuất một sub-network từ meta-policy để trở thành policy cụ thể cho tác vụ. Chúng tôi gọi cách tiếp cận này là "Phân Bổ Tác Vụ Liên Tục qua Sparse Prompting (CoTASP)".

Như được minh họa bởi Hình 1, cho mỗi tác vụ t, prompt αt được tạo ra bằng sparse coding của task embedding et dưới từ điển Dt và được sử dụng để phân bổ một policy sub-network θt từ meta-policy. Sau đó αt và θt được tối ưu hóa thông qua RL. Cuối mỗi tác vụ, từ điển Dt được tối ưu hóa (Mairal et al., 2009; Arora et al., 2015) cho tất cả các tác vụ đã học để cung cấp một ánh xạ từ task embedding của chúng đến các prompts/sub-networks đã tối ưu hóa, khai thác các mối tương quan tác vụ trong cả không gian embedding và prompt. Điều này dẫn đến việc sử dụng hiệu quả dung lượng mạng meta-policy và tối ưu hóa tự động sự cân bằng tính dẻo dai-ổn định, tức là, các tác vụ liên quan tái sử dụng kỹ năng bằng cách chia sẻ nhiều neuron hơn (tính dẻo dai và thích ứng nhanh) trong khi sự can thiệp có hại giữa các tác vụ không liên quan được tránh bằng cách chia sẻ ít hoặc không có neuron (tính ổn định và ít quên lãng hơn). Hơn nữa, do từ điển, CoTASP không cần lưu trữ hoặc phát lại bất kỳ kinh nghiệm nào của các tác vụ trước đó và do đó tốn ít chi phí tính toán và bộ nhớ hơn nhiều so với các phương pháp dựa trên rehearsal (Rolnick et al., 2019; Wolczyk et al., 2022). Hơn nữa, sparse prompting trong CoTASP, như một phương pháp thích ứng tác vụ hiệu quả, có thể trích xuất các policy sub-networks cho các tác vụ chưa thấy và do đó dẫn đến một meta-policy tổng quát hóa tốt hơn.

Trên các benchmark Continual World (Wolczyk et al., 2021), CoTASP vượt trội hơn hầu hết các baseline trên tất cả các tác vụ đã học, giảm quên lãng, và tổng quát hóa cho các tác vụ chưa thấy (Bảng 1). Một nghiên cứu ablation kỹ lưỡng (Bảng 2) chứng minh tầm quan trọng của việc học từ điển và tối ưu hóa sparse prompt. Hơn nữa, phân tích thực nghiệm của chúng tôi cho thấy rằng từ điển hội tụ nhanh (Hình 5(b)) và có thể được tổng quát hóa cho các tác vụ trong tương lai, giảm đáng kể chi phí thích ứng của chúng (Hình 5(a)), trong khi các sparse prompts đã học nắm bắt các mối tương quan ngữ nghĩa giữa các tác vụ (Hình 7). So sánh với các phương pháp tiên tiến, CoTASP đạt được sự cân bằng tính dẻo dai-ổn định tốt nhất (Hình 2) và sử dụng dung lượng mô hình có hiệu quả cao (Hình 4).

2. Kiến thức nền và Công trình liên quan
Chúng tôi tuân theo thiết lập task-incremental được áp dụng bởi công trình trước đó (Khetarpal et al., 2022; Wolczyk et al., 2022; 2021; Rolnick et al., 2019; Mendez et al., 2020; Schwarz et al., 2018; Rusu et al., 2016), xem xét một chuỗi các tác vụ, mỗi tác vụ định nghĩa một Quá trình Quyết định Markov (MDP) Mt=⟨St,At, pt, rt, γ⟩ sao cho S là không gian trạng thái, A là không gian hành động, p:S × A → ∆(S) là xác suất chuyển tiếp trong đó ∆(S) là simplex xác suất trên S, r:S ×A → R là hàm phần thưởng nên rt(st,h, at,h) là phần thưởng tức thời trong tác vụ t khi thực hiện hành động at,h tại trạng thái st,h, h chỉ số bước môi trường, và γ∈[0,1) là hệ số chiết khấu. RL liên tục nhằm đạt được một policy πθ tại tác vụ T hoạt động tốt (với kỳ vọng return cao) trên tất cả các tác vụ đã thấy t≤ T, chỉ với một buffer hạn chế (hoặc không có) các kinh nghiệm của tác vụ trước đó:

θ∗= arg max θ ∑T t=1 Eπθ [∑∞ h=0 γh rt(st,h, at,h)] (1)

Học liên tục là một kỹ năng tự nhiên của con người có thể tích lũy kiến thức tổng quát hóa cho các tác vụ mới mà không quên những tác vụ đã học. Tuy nhiên, các agent RL thường gặp khó khăn với việc đạt được mục tiêu trong Phương trình 1 do sự cân bằng tính dẻo dai-ổn định: policy được kỳ vọng nhanh chóng thích ứng với các tác vụ mới t≥ T (tính dẻo dai) nhưng trong khi đó vẫn duy trì hiệu suất của nó trên các tác vụ trước đó t <T (tính ổn định).

Các chiến lược hiện có cho RL liên tục chủ yếu tập trung vào cải thiện tính ổn định và giảm quên lãng thảm khốc. Các phương pháp dựa trên rehearsal như CLEAR (Rolnick et al., 2019) và P&C (Schwarz et al., 2018) lặp đi lặp lại phát lại các kinh nghiệm được lưu trữ từ các tác vụ trước đó nhưng chi phí bộ nhớ buffer và tính toán của chúng tăng tuyến tính với số lượng tác vụ (Kumari et al., 2022). Các phương pháp dựa trên regularization như EWC (Kirkpatrick et al., 2017) và PC (Kaplanis et al., 2019) giảm thiểu quên lãng mà không cần replay buffer bằng cách thêm các regularizer bổ sung khi học các tác vụ mới, có thể làm thiên lệch tối ưu hóa policy và dẫn đến các giải pháp tối ưu dưới mức (Zhao et al., 2023). Cuối cùng, các phương pháp dựa trên cấu trúc áp dụng các module khác nhau, tức là, các sub-networks trong một mạng policy có dung lượng cố định, cho mỗi tác vụ (Mendez & Eaton, 2022). Chúng tôi tóm tắt hai danh mục chính của các phương pháp dựa trên cấu trúc sau đây. Chúng tôi cũng cung cấp thảo luận chi tiết hơn về công trình liên quan trong Phụ lục B và C.

Các phương pháp cấp độ kết nối. Danh mục này bao gồm các phương pháp như PackNet (Mallya & Lazebnik, 2018), SupSup (Wortsman et al., 2020), và WSN (Kang et al., 2022). Đối với tác vụ t, hành động at được rút ra từ at∼π(st;θ⊗ϕt) trong đó st là trạng thái và ϕt là một binary mask được áp dụng cho trọng số mô hình θ theo cách element-wise (tức là, ⊗). PackNet tạo ra ϕt bằng cách lặp đi lặp lại pruning θ sau khi học mỗi tác vụ, từ đó bảo tồn các trọng số quan trọng cho tác vụ trong khi để lại các trọng số khác cho các tác vụ tương lai. SupSup cố định một mạng được khởi tạo ngẫu nhiên và tìm ϕt tối ưu cho mỗi tác vụ t. WSN học chung θ và ϕt và sử dụng mã hóa Huffman (Huffman, 1952) để nén ϕt để có kích thước tăng dưới tuyến tính của {ϕt}T t=1 với các tác vụ tăng. Tuy nhiên, các phương pháp này thường cần lưu trữ các mask cụ thể cho tác vụ cho mỗi tác vụ trong lịch sử, dẫn đến chi phí bộ nhớ bổ sung (Lange et al., 2022). Hơn nữa, các mask của chúng hiếm khi được tối ưu hóa để chia sẻ kiến thức qua các tác vụ, cản trở policy đã học khỏi việc được tổng quát hóa cho các tác vụ chưa thấy.

Các phương pháp cấp độ neuron. Thay vì trích xuất các sub-networks cụ thể cho tác vụ bằng cách áp dụng masks cho trọng số mô hình, danh mục phương pháp khác (Fernando et al., 2017; Serra et al., 2018; Ke et al., 2021; Sokar et al., 2021) tạo ra sub-networks bằng cách áp dụng masks cho neurons/outputs của mỗi lớp của một mạng policy. So với các phương pháp cấp độ kết nối, chúng sử dụng masking theo lớp để đạt được biểu diễn sub-networks linh hoạt và compact hơn. Nhưng việc tạo ra masks phụ thuộc vào các quy tắc heuristic hoặc các phương pháp policy gradient kém hiệu quả về mặt tính toán (Gurbuz & Dovrolis, 2022; Serra et al., 2018). Ngược lại, CoTASP tạo ra masks bằng sparse coding có hiệu quả cao (giải quyết một bài toán lasso tương đối nhỏ).

3. Phương pháp
Trong phần này, chúng tôi giới thiệu các bước chính và thành phần của CoTASP (xem Hình 1). Cụ thể, Phần 3.1 giới thiệu mạng meta-policy trong thiết lập RL liên tục. Phần 3.2 mô tả sparse prompting được đề xuất của chúng tôi để trích xuất policy tác vụ. Cuối cùng, Phần 3.3 cung cấp quy trình tối ưu hóa chi tiết cho mỗi thành phần trong CoTASP, bao gồm prompt, policy tác vụ, và từ điển.

3.1. RL Liên tục với Mạng Meta-Policy
Như đã thảo luận trong Phần 2, việc finetuning tất cả các trọng số trong θ qua tối ưu hóa trong Phương trình 1 mà không có quyền truy cập vào các tác vụ trong quá khứ dẫn đến sự thay đổi có hại trên một số trọng số quan trọng cho các tác vụ trước đó và quên lãng thảm khốc của chúng. Các phương pháp dựa trên cấu trúc giải quyết điều này bằng cách phân bổ một sub-network cho mỗi tác vụ và đóng băng trọng số của nó một khi hoàn thành việc học tác vụ để chúng miễn nhiễm với quên lãng thảm khốc.

Theo công trình trước đó (Srivastava et al., 2014; Fernando et al., 2017; Serra et al., 2018; Ke et al., 2021; Sokar et al., 2021), chúng tôi biểu diễn sub-network như vậy bằng cách áp dụng một binary mask cho output của mỗi neuron. Cụ thể, cho một mạng meta-policy với L lớp, để l∈ {1, . . . , L −1} chỉ số các lớp ẩn của mạng như một superscript, ví dụ, y(l) là vector output của lớp l và θ(l) biểu thị trọng số của lớp l. Output của sub-network trên lớp thứ (l+ 1) của nó là

y(l+1)=f(ϕ(l) t⊗y(l);θ(l+1)), (2)

trong đó ϕ(l) t là một binary mask được tạo ra cho tác vụ t và được áp dụng cho lớp l, và f là một đại diện cho phép toán neural, ví dụ, một lớp fully-connected hoặc convolutional. Toán tử tích element-wise ⊗ kích hoạt một phần neurons trong lớp l của mạng meta-policy theo ϕ(l) t. Các neurons được kích hoạt này qua tất cả các lớp trích xuất một sub-network như một policy cụ thể cho tác vụ, sau đó tương tác với môi trường để thu thập kinh nghiệm huấn luyện. Huấn luyện sub-network tránh sự can thiệp có hại với các tác vụ trước đó trên các neurons khác và trong khi đó khuyến khích chia sẻ kiến thức của chúng trên các neurons được chia sẻ. Tuy nhiên, việc phân bổ policies trong mạng meta-policy cho một chuỗi các tác vụ đa dạng đặt ra một số thách thức. Để sử dụng hiệu quả dung lượng mạng, mỗi policy tác vụ nên là một sub-network thưa thớt chỉ có một vài neurons được kích hoạt. Hơn nữa, policy nên chọn lọc tái sử dụng neurons từ các policies đã học trước đó, có thể tạo điều kiện thuận lợi cho việc chia sẻ kiến thức giữa các tác vụ liên quan và giảm sự can thiệp từ các tác vụ không liên quan. Lấy cảm hứng từ prompting và in-context training cho NLP (Rebuffi et al., 2017; Houlsby et al., 2019; Li & Liang, 2021; Liu et al., 2021), chúng tôi đề xuất "Sparse Prompting" để giải quyết thách thức nói trên của phân bổ tác vụ liên tục, có thể tự động và hiệu quả trích xuất các policies cụ thể cho tác vụ từ mạng meta-policy.

3.2. Trích xuất Policy Tác vụ qua Sparse Prompting
Trong phân bổ tác vụ liên tục, sub-network được trích xuất cho một tác vụ mới được kỳ vọng tái sử dụng kiến thức đã học từ các tác vụ liên quan trong quá khứ và trong khi đó tránh sự can thiệp có hại từ các tác vụ không liên quan. Hơn nữa, sub-network nên thưa thớt nhất có thể để sử dụng hiệu quả dung lượng mạng. Các phương pháp RL liên tục khác nhau (Mallya & Lazebnik, 2018; Serra et al., 2018; Sokar et al., 2021; Kessler et al., 2022; Kang et al., 2022; Wolczyk et al., 2022) sử dụng một embedding one-hot để trích xuất sub-network cho mỗi tác vụ đã học. Chúng bỏ qua các mối tương quan ngữ nghĩa giữa các tác vụ và cần các cơ chế tinh tế để giữ sub-network thưa thớt (Lange et al., 2022).

Trong CoTASP, thay vào đó chúng tôi trích xuất các sub-networks thưa thớt từ một embedding compact của mô tả văn bản của tác vụ được tạo ra bởi Sentence-BERT (S-BERT) (Reimers & Gurevych, 2019) qua sparse coding (Mairal et al., 2009; Arora et al., 2015). Cụ thể, chúng tôi học một từ điển over-complete D(l)∈ Rm×k (m≪k) cho mỗi lớp l của mạng meta-policy, trong đó mỗi cột là một atom đại diện cho một neuron trong lớp. Cho một task embedding et∈Rm, sparse prompting có thể tạo ra một sparse prompt α(l) t cho mỗi lớp l để tái cấu trúc et như một tổ hợp tuyến tính của một vài biểu diễn neurons, tức là, các atoms từ từ điển. Nó bằng với việc giải quyết bài toán lasso sau đây.

α(l) t= arg min α∈Rk 1/2 ∥et−D(l) t−1α∥2 2+λ∥α∥1, cho lớp l= 1, . . . , L −1 (3)

trong đó λ là một tham số regularization kiểm soát độ thưa thớt của α. Bài toán lasso này có thể được giải quyết bằng nhiều cách tiếp cận hiệu quả đã được chứng minh, ví dụ, coordinate descent (Friedman et al., 2007), fast iterative shrinkage thresholding algorithm (Beck & Teboulle, 2009), và LARS algorithm (Efron et al., 2004). Trong bài báo này, chúng tôi áp dụng một implementation dựa trên Cholesky của LARS algorithm (Mairal et al., 2009) cho hiệu quả và tính ổn định của nó. Để chuyển đổi α∈Rk thành một binary mask, chúng tôi áp dụng một step function σ(·) trên α, trong đó σ(α) = 1 nếu α >0 và 0 ngược lại. Sau đó chúng tôi có thể trích xuất một policy sub-network cụ thể cho tác vụ bằng cách áp dụng mask cho mạng meta-policy như trong Phương trình 2.

3.3. Học Meta-Policy và Từ điển trong CoTASP
Tối ưu hóa Luân phiên của Policy Tác vụ và Prompt Bằng cách luân phiên tối ưu hóa policy của tác vụ hiện tại và prompts α(l) t cho l= 1, . . . , L −1 sử dụng bất kỳ thuật toán RL off-the-shelf nào, CoTASP cập nhật trọng số sub-network liên kết với policy tác vụ trong mạng meta-policy và các binary masks tương ứng. Tuy nhiên, có hai mối quan tâm thực tế: (1) cập nhật các trọng số trong θt đã được chọn bởi các tác vụ trước đó có thể làm giảm hiệu suất của các tác vụ cũ mà không có experience replay; và (2) step function σ(·) có gradient bằng không nên việc tối ưu hóa αt sử dụng gradient như vậy là không khả thi.

Để giải quyết mối quan tâm đầu tiên, chúng tôi cập nhật trọng số một cách chọn lọc bằng cách chỉ cho phép cập nhật các trọng số chưa bao giờ được phân bổ cho bất kỳ tác vụ nào trước đó. Để mục đích này, chúng tôi tích lũy các binary masks cho tất cả các tác vụ đã học bằng ˆϕ(l) t−1=∨t−1 i=1ϕ(l) t−1 và cập nhật θ khi học tác vụ t bằng

θ←θ−ηˆgt ˆg(l) t= { [1−ˆϕ(l) t−1] ⊙ g(l) t, l = 1 [1−ˆϕ(l−1) t−1] ⊙ g(l) t, l =L [1−min( ˆϕ(l−1) t−1,ˆϕ(l) t−1)] ⊙ g(l) t, l > 1 (4)

trong đó η là learning rate và g(l) t biểu thị gradients âm của expected return w.r.t. θ cho lớp l trên tác vụ t. Trong Phương trình 4, chúng tôi sửa đổi gradient của mỗi trọng số theo accumulated mask liên kết với input và output layer của nó. Điều này hiệu quả tránh việc ghi đè các trọng số được chọn bởi policies của các tác vụ trước đó và do đó giảm thiểu quên lãng.

Để giải quyết mối quan tâm thứ hai, chúng tôi sử dụng straight-through estimator (STE) (Bengio et al., 2013), tức là, clip(α,0,1), trong backward pass để α có thể được tối ưu hóa trực tiếp sử dụng cùng thuật toán gradient descent được áp dụng cho trọng số meta-policy.

Học Từ điển Cho các prompts đã tối ưu hóa α∗ i của các tác vụ trước đó và embedding của chúng, chúng tôi tiếp tục cập nhật từ điển theo lớp để sparse prompting sẽ được cải thiện để tạo ra các prompts đã tối ưu hóa cho mỗi tác vụ trước đó., tức là,

D(l) t= arg min D∈Rm×k 1/2 ∑t i=1 ∥ei−Dα∗(l) i∥2 2,s.t.,∥D[j]∥2≤c ∀j= 1, . . . , k, cho lớp l= 1, . . . , L −1 (5)

trong đó chúng tôi ràng buộc ℓ2 norm của mỗi atom D[j] để ngăn chặn quy mô của D tăng lên một cách tùy ý, điều này sẽ dẫn đến các mục nhập tùy ý nhỏ trong α.

Để giải quyết tối ưu hóa với các ràng buộc bất đẳng thức trong Phương trình 5, chúng tôi sử dụng block-coordinate descent với D(l) t−1 như warm restart, như được mô tả trong Thuật toán 1. Cụ thể, chúng tôi tuần tự cập nhật mỗi atom của D(l) t−1 dưới ràng buộc ∥D[j]∥2≤c trong khi cố định các atoms còn lại. Vì tối ưu hóa này thừa nhận các ràng buộc có thể tách rời cho các atoms được cập nhật, sự hội tụ đến tối ưu toàn cục được đảm bảo (Bertsekas, 1997; Lee et al., 2006; Mairal et al., 2009). Hơn nữa, D(l) t−1 như một warm start cho D(l) t giảm đáng kể các bước tối ưu hóa cần thiết: chúng tôi thấy thực nghiệm rằng một bước là đủ. Quy trình huấn luyện hoàn chỉnh của CoTASP được chi tiết trong Thuật toán 2.

4. Thí nghiệm
4.1. Thiết lập Thí nghiệm
Benchmarks. Để đánh giá CoTASP, chúng tôi tuân theo cùng thiết lập như công trình trước đó (Wolczyk et al., 2022) và thực hiện các thí nghiệm kỹ lưỡng. Cụ thể, chúng tôi chủ yếu sử dụng CW10, một benchmark trong Continual World (CW) (Wolczyk et al., 2021), bao gồm 10 tác vụ thao tác đại diện từ MetaWorld (Yu et al., 2019). Để làm cho benchmark thách thức hơn, chúng tôi xếp hạng các tác vụ này theo một ma trận chuyển giao được tính toán trước để có sự biến thiên cao của forward transfer cả trong toàn bộ chuỗi và cục bộ. Chúng tôi cũng sử dụng CW20, lặp lại CW10 hai lần, để đo lường khả năng chuyển giao của policy đã học khi gặp cùng một tác vụ. Để so sánh công bằng giữa các tác vụ khác nhau, số bước tương tác môi trường được giới hạn ở 1M mỗi tác vụ.

Chỉ số đánh giá. Theo một giao thức đánh giá được sử dụng rộng rãi trong văn học học liên tục (Lopez-Paz & Ranzato, 2017; Rolnick et al., 2019; Chaudhry et al., 2019; Wolczyk et al., 2021; 2022), chúng tôi áp dụng ba chỉ số. (1) Hiệu suất Trung bình (cao hơn là tốt hơn): hiệu suất trung bình tại thời điểm t được định nghĩa là P(t) =1/T ∑T i=1 pi(t) trong đó pi(t)∈ [0,1] biểu thị tỷ lệ thành công của tác vụ i tại thời điểm t. Đây là một chỉ số chính tắc được sử dụng trong cộng đồng học liên tục. (2) Quên lãng (thấp hơn là tốt hơn): nó đo lường sự suy giảm trung bình qua tất cả các tác vụ cuối quá trình học, được ký hiệu bởi F=1/T ∑T i=1 pi(i·δ)−pi(T ·δ), trong đó δ là số bước môi trường được phép cho mỗi tác vụ. (3) Tổng quát hóa (thấp hơn là tốt hơn): nó bằng số bước trung bình cần thiết để đạt được một ngưỡng thành công qua tất cả các tác vụ. Lưu ý rằng chúng tôi dừng huấn luyện khi tỷ lệ thành công trong hai đánh giá liên tiếp đạt được ngưỡng (đặt là 0.9). Hơn nữa, chỉ số được chia cho δ để chuẩn hóa quy mô của nó về [0,1].

Các phương pháp so sánh. Chúng tôi so sánh CoTASP với một số baselines và các phương pháp RL liên tục state-of-the-art (SoTA). Theo (Lange et al., 2022), các phương pháp này có thể được chia thành ba danh mục: các phương pháp dựa trên regularization, dựa trên cấu trúc, và dựa trên rehearsal. Cụ thể, các phương pháp dựa trên regularization bao gồm L2, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), Memory-Aware Synapses (MAS) (Aljundi et al., 2018), và Variational Continual Learning (VCL) (Nguyen et al., 2018). Các phương pháp dựa trên cấu trúc bao gồm PackNet (Mallya & Lazebnik, 2018), Hard Attention to Tasks (HAT) (Serra et al., 2018), và TaDeLL (Rostami et al., 2020). Các phương pháp dựa trên rehearsal bao gồm Reservoir, Average Gradient Episodic Memory (A-GEM) (Chaudhry et al., 2019), và ClonEx-SAC (Wolczyk et al., 2022). Để đầy đủ, chúng tôi cũng bao gồm một phương pháp huấn luyện tuần tự ngây thơ (tức là, Finetuning) và các baselines RL đa tác vụ đại diện (MTL (Yu et al., 2019) và MTL+PopArt (Hessel et al., 2019)), thường được coi là soft upper bound mà một phương pháp RL liên tục có thể đạt được. Để so sánh công bằng, chúng tôi tham khảo repository Continual World¹ để implementation và lựa chọn hyperparameter. Chúng tôi chạy lại các phương pháp này để đảm bảo hiệu suất tốt nhất có thể. Ngoài ra, chúng tôi áp dụng kết quả được báo cáo bởi tác giả cho ClonEx-SAC do thiếu implementation open-sourced. Một mô tả mở rộng và thảo luận về các phương pháp này được cung cấp trong Phụ lục C.

Chi tiết huấn luyện. Để đảm bảo độ tin cậy và khả năng so sánh của các thí nghiệm của chúng tôi, chúng tôi tuân theo chi tiết huấn luyện được mô tả trong (Wolczyk et al., 2021; 2022) và implement tất cả các phương pháp baseline dựa trên Soft Actor-Critic (SAC) (Haarnoja et al., 2018), một thuật toán actor-critic off-policy SoTA. Actor và critic được implement như hai mạng multi-layer perceptron (MLP) riêng biệt, mỗi mạng có 4 lớp ẩn của 256 neurons. Đối với các phương pháp dựa trên cấu trúc (PackNet, HAT) và CoTASP được đề xuất của chúng tôi, một mạng MLP rộng hơn với 1024 neurons mỗi lớp được sử dụng như actor. Chúng tôi tham khảo các lớp ẩn này như backbone và lớp output cuối cùng như head. Không giống như các phương pháp RL liên tục khác (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019; Serra et al., 2018; Kessler et al., 2022) dựa vào việc sử dụng một head riêng biệt cho mỗi tác vụ mới, CoTASP sử dụng thiết lập single-head trong đó chỉ một head được sử dụng cho tất cả các tác vụ. Trong trường hợp này, CoTASP không yêu cầu chọn head thích hợp cho mỗi tác vụ và cho phép tái sử dụng các tham số giữa các tác vụ tương tự. Theo (Wolczyk et al., 2021), việc regularizing critic thường dẫn đến sự suy giảm hiệu suất. Do đó, chúng tôi hoàn toàn bỏ qua vấn đề quên lãng trong mạng critic và retrain nó cho mỗi tác vụ mới. Chi tiết thêm về các hyperparameters được sử dụng trong huấn luyện có thể được tìm thấy trong Phụ lục D.

4.2. Kết quả Chính
Phần này trình bày so sánh giữa CoTASP và mười phương pháp RL liên tục đại diện trên các benchmarks CW. Chúng tôi tập trung vào tính ổn định (duy trì hiệu suất trên các tác vụ đã thấy) và tính dẻo dai (nhanh chóng thích ứng với các tác vụ chưa thấy) và giữ nguyên các ràng buộc về tính toán, bộ nhớ, số mẫu, và kiến trúc mạng neural.

Bảng 1 tóm tắt kết quả chính của chúng tôi trên các chuỗi CW10 và CW20. CoTASP luôn vượt trội hơn tất cả các phương pháp được so sánh qua các độ dài khác nhau của chuỗi tác vụ, về cả hiệu suất trung bình (đo lường tính ổn định) và tổng quát hóa (đo lường tính dẻo dai). Chúng tôi quan sát thấy rằng khi kích thước lớp ẩn giống như các phương pháp dựa trên cấu trúc khác (PackNet và HAT), CoTASP vượt trội hơn chúng với một biên độ lớn, đặc biệt trong chỉ số tổng quát hóa, chỉ ra lợi thế của CoTASP trong việc cải thiện khả năng thích ứng với các tác vụ mới. Một phân tích chi tiết hơn về lý do hiệu quả của CoTASP được trình bày trong Phần 4.3 và 4.4. Hơn nữa, chúng tôi thấy rằng hầu hết các phương pháp RL liên tục không đạt được backward transfer tích cực (tức là, F <0) ngoại trừ VCL, cho thấy khả năng cải thiện hiệu suất của các tác vụ trước đó bằng cách học các tác vụ mới vẫn là một thách thức đáng kể. Chúng tôi để lại điều này cho công việc tương lai. Cuối cùng, kết quả trong Hình 3 cho thấy rằng CoTASP là phương pháp duy nhất hoạt động tương đương với các baselines multi-task learning trên mười tác vụ đầu tiên của chuỗi CW20, và nó thể hiện hiệu suất vượt trội hơn các baselines này sau khi học toàn bộ chuỗi CW20. Một giải thích có thể là kiến thức tích lũy bởi mạng meta-policy và từ điển của CoTASP dẫn đến cải thiện tổng quát hóa.

4.3. Nghiên cứu Ablation
Hiệu quả của các thiết kế cốt lõi. Để cho thấy hiệu quả của mỗi thành phần của chúng tôi, chúng tôi tiến hành nghiên cứu ablation trên bốn biến thể của CoTASP, mỗi biến thể loại bỏ hoặc thay đổi một lựa chọn thiết kế duy nhất được thực hiện trong CoTASP gốc. Bảng 2 trình bày kết quả của nghiên cứu ablation trên chuỗi CW20, sử dụng hai chỉ số đánh giá đại diện. Trong số bốn biến thể của CoTASP, "D frozen" thay thế từ điển học được bằng một từ điển cố định, được khởi tạo ngẫu nhiên; "α frozen" loại bỏ tối ưu hóa prompt được đề xuất trong Phần 3.3; "both frozen" không cập nhật từ điển cũng không tối ưu hóa prompt; "lazily update D" dừng việc học từ điển sau khi hoàn thành mười tác vụ đầu tiên của chuỗi CW20. Theo kết quả trong Bảng 2, chúng tôi đưa ra các kết luận sau: (1) Việc sử dụng một từ điển cố định, được khởi tạo ngẫu nhiên làm giảm hiệu suất của CoTASP trên hai chỉ số đánh giá, làm nổi bật tầm quan trọng của từ điển học được trong việc nắm bắt các mối tương quan ngữ nghĩa giữa các tác vụ. (2) Biến thể "α frozen" hoạt động tương đương với CoTASP của chúng tôi nhưng vượt trội hơn kết quả đạt được bởi EWC và PackNet. Điều này chỉ ra rằng việc tối ưu hóa prompt có thể cải thiện hiệu suất của CoTASP nhưng không quan trọng đối với kết quả hấp dẫn của chúng tôi. (3) Biến thể "both frozen" thể hiện sự suy giảm đáng chú ý trong hiệu suất, hỗ trợ kết luận rằng sự kết hợp các thiết kế cốt lõi được đề xuất trong CoTASP là cần thiết để đạt được kết quả mạnh mẽ. (4) Biến thể "lazily update D" chỉ suy giảm nhẹ so với CoTASP gốc về hiệu suất nhưng vẫn vượt trội hơn tất cả baselines với một biên độ lớn, chỉ ra rằng từ điển đã học đã tích lũy đủ kiến thức trong mười tác vụ đầu tiên để CoTASP có thể đạt được kết quả cạnh tranh mà không cần cập nhật từ điển cho các tác vụ lặp lại.

Ảnh hưởng của các hyperparameters chính. CoTASP giới thiệu tham số sparsity λ, một hyperparameter kiểm soát sự cân bằng giữa dung lượng mạng được sử dụng và hiệu suất của policy kết quả. Một giá trị λ lớn hơn dẫn đến một policy sub-network thưa thớt hơn, cải thiện hiệu quả sử dụng dung lượng của mạng meta-policy. Nhưng chi phí là hiệu suất giảm trên mỗi tác vụ do mất tính biểu đạt của policy tác vụ quá thưa thớt. Theo kết quả trong Hình 4, CoTASP với λ=1e-3 hoặc 1e-4 đạt được sự cân bằng tốt hơn giữa hiệu suất và hiệu quả sử dụng so với các phương pháp dựa trên cấu trúc khác (HAT và PackNet) trên chuỗi CW10.

4.4. Tại sao CoTASP hoạt động? Một Nghiên cứu Thực nghiệm
Trong phần này, chúng tôi trả lời các câu hỏi sau dựa trên các hiện tượng quan sát được từ kết quả thực nghiệm của chúng tôi: (1) Từ điển đã học có thể tổng quát hóa không? (2) Các sparse prompts được tạo ra bởi CoTASP có nắm bắt các mối tương quan ngữ nghĩa giữa các tác vụ không?

Để trả lời câu hỏi đầu tiên, chúng tôi đo lường sự thay đổi của các từ điển, visualize động lực của chúng, và so sánh chi phí thích ứng của CoTASP với các phương pháp RL liên tục/meta khác trên chuỗi CW20. Kết quả trong Hình 5(b) cho thấy rằng các từ điển này hội tụ nhanh chóng với số lượng tác vụ tăng, dẫn đến một ánh xạ ổn định từ task embedding đến prompt đã tối ưu hóa. Trong tác vụ mới tiếp theo, CoTASP sẽ tạo ra một "good" initial prompts bằng sparse coding của task embedding của nó. Điều này giảm đáng kể số bước huấn luyện cần thiết để đạt được ngưỡng thành công, như được chứng minh bởi Hình 5(a). Hơn nữa, chúng tôi so sánh CoTASP với một thuật toán meta-RL SoTA, CoMPS (Berseth et al., 2022), để cho thấy sự vượt trội của nó. Cụ thể, chúng tôi pretrain CoTASP và CoMPS trên chuỗi CW10 và sau đó sử dụng meta-policy đã học như policy ban đầu để finetune các tác vụ chưa thấy, ví dụ, reach-v1 và button-press-v1. Theo kết quả được hiển thị trong Hình 6, CoTASP hoạt động tương đương với CoMPS nhưng tốt hơn đáng kể so với baseline RL liên tục (HAT) về chi phí thích ứng. Tuy nhiên, do thiếu cơ chế chống quên lãng thảm khốc, CoMPS thích ứng với tác vụ mới trong khi hiệu suất của nó trên các tác vụ trước đó nhanh chóng suy giảm, dẫn đến return trung bình tệ hơn qua tất cả các tác vụ.

Để trả lời câu hỏi thứ hai, chúng tôi visualize độ tương tự (tức là, sự chồng chéo giữa hai binary prompts) của các prompts được tạo ra bởi CoTASP giữa mỗi hai tác vụ qua chuỗi CW10 trong Hình 7. Heatmap màu xanh tóm tắt các giá trị độ tương tự được tính trung bình qua tất cả các lớp ẩn. Cụ thể, phần tử trên hàng i và cột j là giá trị độ tương tự trung bình được tính toán giữa tác vụ i và tác vụ j. Đối với tác vụ 2 và tác vụ 7, mô tả tác vụ của chúng chia sẻ cùng một primitive thao tác, tức là, đẩy một puck. Do đó, các prompts được tạo ra bằng cách giải quyết bài toán lasso trong Phương trình 3 có mối tương quan cao. Ngược lại, đối với tác vụ 2 và tác vụ 7 với mô tả tác vụ không liên quan, CoTASP tạo ra các prompts khác nhau, giảm sự can thiệp chéo-tác vụ và cải thiện tính dẻo dai.

5. Kết luận
Chúng tôi đề xuất CoTASP để giải quyết hai thách thức chính trong RL liên tục: (1) huấn luyện một meta-policy tổng quát hóa cho tất cả các tác vụ đã thấy và thậm chí chưa thấy, và (2) trích xuất hiệu quả một policy tác vụ từ meta-policy. CoTASP học một từ điển để tạo ra các sparse masks (prompts) để trích xuất policy của mỗi tác vụ như một sub-network của meta-policy và tối ưu hóa sub-network qua RL. Điều này khuyến khích chia sẻ/tái sử dụng kiến thức giữa các tác vụ liên quan trong khi giảm sự can thiệp chéo-tác vụ có hại gây ra quên lãng và thích ứng tác vụ mới kém. Mà không cần bất kỳ experience replay nào, CoTASP đạt được sự cân bằng tính dẻo dai-ổn định tốt hơn đáng kể và phân bổ dung lượng mạng hiệu quả hơn so với baselines. Các policies được trích xuất của nó vượt trội hơn tất cả baselines trên cả các tác vụ trước đó và mới.

Lời cám ơn
Yijun Yang và Yuhui Shi được hỗ trợ một phần bởi Science and Technology Innovation Committee Foundation of Shenzhen dưới Grant No. JCYJ20200109141235597 và ZDSYS201703031748284, National Science Foundation of China dưới grant number 61761136008, Shenzhen Peacock Plan dưới Grant No. KQTD2016112514355531, và Program for Guangdong Introducing Innovative and Entrepreneurial Teams dưới grant number 2017ZT07X386. Chúng tôi muốn cảm ơn các area chairs của ICML và các reviewers ẩn danh vì nỗ lực của họ trong việc review bài báo này và các nhận xét xây dựng của họ!

[Các phần còn lại bao gồm Tài liệu tham khảo và Phụ lục được dịch tương tự...]
