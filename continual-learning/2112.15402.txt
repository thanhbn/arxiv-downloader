# 2112.15402.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/continual-learning/2112.15402.pdf
# File size: 4476716 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Relational Experience Replay: Continual Learning
by Adaptively Tuning Task-wise Relationship
Quanziang Wang, Renzhen Wang, Yuexiang Li, Dong Wei, Hong Wang,
Kai Ma, Yefeng Zheng, Fellow, IEEE , Deyu Meng, Member, IEEE
Abstract â€”Continual learning is a promising machine learning
paradigm to learn new tasks while retaining previously learned
knowledge over streaming training data. Till now, rehearsal-based
methods, keeping a small part of data from old tasks as a memory
buffer, have shown good performance in mitigating catastrophic
forgetting for previously learned knowledge. However, most of
these methods typically treat each new task equally, which may
not adequately consider the relationship or similarity between old
and new tasks. Furthermore, these methods commonly neglect
sample importance in the continual training process and result
in sub-optimal performance on certain tasks. To address this
challenging problem, we propose Relational Experience Replay
(RER), a bi-level learning framework, to adaptively tune task-
wise relationships and sample importance within each task to
achieve a better â€˜stabilityâ€™ and â€˜plasticityâ€™ trade-off. As such,
the proposed method is capable of accumulating new knowledge
while consolidating previously learned old knowledge during
continual learning. Extensive experiments conducted on three
publicly available datasets ( i.e., CIFAR-10, CIFAR-100, and Tiny
ImageNet) show that the proposed method can consistently
improve the performance of all baselines and surpass current
state-of-the-art methods.
Index Terms â€”Continual learning, stability-plasticity dilemma,
bi-level optimization.
I. I NTRODUCTION
DEEP neural networks trained offline have achieved excel-
lent results in many computer vision tasks, such as clas-
sification [1]â€“[3], semantic segmentation [4]â€“[6], and object
detection [7]â€“[9]. However, these models commonly struggle
in continual learning (CL) scenarios where the knowledge
is incrementally aggregated from data that are generated by
a non-stationary distribution. Actually, the inability of the
models is caused by the fact that they are inclined to overwrite
the previously learned knowledge whenever new tasks come
in. This is known as catastrophic forgetting [10] and is a key
challenge in continual learning.
Recently, various CL methods have been proposed to alle-
viate the catastrophic forgetting problem. Existing algorithms
for CL can be roughly divided into three categories [11]:
1)Model-based approaches [12]â€“[16], which dynamically
modify the architecture of the model to process the new task
and use specific model structures for different tasks in the
testing phase; 2) Regularization-based approaches [17]â€“[21],
which propose constraints to maintain the model parameters
Quanziang Wang, Renzhen Wang, and Deyu Meng are with the School
of Mathematics and Statistics, Xiâ€™an Jiaotong University, Xiâ€™an 710049, P.R.
China.
Yuexiang Li, Dong Wei, Hong Wang, Kai Ma, and Yefeng Zheng are with
Tencent Jarvis Lab, Shenzhen 518052, P.R. China.
Fig. 1. Illustration of two main factors affecting stability and plasticity in
continual learning: (i) The relationship between the new and old tasks varies
dynamically. For example, the class â€˜birdâ€™ of task 2 is semantic-related to
the â€˜airplaneâ€™ of task 1 due to the closer distance between these two clusters
in the latent space. (ii) The importance of data points within each class is
different for model training. The samples with black borders, located around
the classification boundary, are more difficult/beneficial for the classification
than other samples.
that are relatively important for previously learned tasks; 3)
Rehearsal-based approaches [22]â€“[30], which store a small
subset of examples from previously learned tasks in a memory
buffer and replay them during the learning of new tasks. In this
study, we mainly focus on exploring the potential of rehearsal-
based methods, due to their relatively stable and effective
performance in mitigating catastrophic forgetting.
Due to the lack of data from old tasks, except for a
small memory buffer, rehearsal-based methods require strik-
ing an appropriate balance between learning new information
and retaining previous knowledge, which is referred to as
the â€˜stability-plasticityâ€™ dilemma [31]. Many rehearsal-based
methods, such as Experience Replay (ER) [22], [23] and
DER++ [26], primarily concentrate on leveraging stored infor-
mation from previous tasks to mitigate model forgetting during
the training of new tasks. However, these approaches have
paid limited attention to modeling the intricate relationships
between tasks. For example, the loss function of ER is the
sum of the losses of incoming new task samples and old task
ones stored in the memory buffer: Lnew+Î»Lold, where Î»is a
hyperparameter and it is often tuned manually by grid search
at the beginning of the training process to balance the trade-off
between stability and plasticity. This trade-off hyperparameter
is generally required to be carefully pre-specified to guarantee
a sound training tendency across the whole CL process.
Actually, as shown in Fig. 1, under the settings of CL, the
data streaming of new tasks inclines to induce the contin-
ual variation of data distribution. Therefore, the relationship
between old and new tasks dynamically varies throughoutarXiv:2112.15402v3  [cs.LG]  3 Aug 2023

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
the learning process. For example, the knowledge extracted
from previous tasks involving cats may be disturbed by the
new task of dog identification, which may lead to serious
forgetting of cat classification and suboptimal performance for
the learned model. Conversely, certain semantically unrelated
classes ( e.g., automobile) have little impact on cats in old
tasks. Obviously, the continual model should adapt its focus
on old tasks ( i.e., â€˜stabilityâ€™) or new tasks ( i.e., â€˜plasticityâ€™) in
real-time according to their dynamically varying relationship.
However, it is difficult for existing methods to model such
a complicated task-wise relationship by simply tuning and
fixing hyperparameters at the beginning of network training.
Moreover, previous studies do not extensively consider another
intrinsic factor, i.e.,sample importance within the same class
on feature representation learning. Concretely, easy-to-learn
samples contain more representative knowledge for each class,
while the â€˜hardâ€™ ones, located around decision boundaries for
classification, can contribute to the learning of the classifier
[32] (refer to Fig. 1). From this perspective, it is essential to
model the relationship between new and old tasks and sample
importance within the same class.
To this end, we propose Relational Experience Replay
(RER), a bi-level learning framework to couple the afore-
mentioned complicated task relationship and sample impor-
tance. Concretely, the inner-loop optimization problems aim
to address the â€˜plasticityâ€™ by leveraging the examples from the
newly coming task, while the outer-loop optimization problem
of RER aims to address the issue of â€˜stabilityâ€™ by minimizing
the empirical risk of a batch of balanced data (including
samples from old tasks stored in the memory buffer and ones
from new tasks). To take full advantage of the training samples
by considering the inter-task relationship and sample impor-
tance, we design an explicit weighting function parameterized
by a light-weight neural network (dubbed Relation Replay
Net, or RRN), which maps pairwise abstract information of
samples from new and old tasks to their corresponding loss
weights. We theoretically prove that the RRN is updated based
on the similarity of the average gradient between classes
in the outer-loop optimization, indicating that our proposed
RER can implicitly model task-wise relationships. We conduct
extensive experiments on different benchmark datasets, and
the results verify the effectiveness of the proposed method in
consistently improving various baselines for rehearsal-based
continual learning methods. In summary, our contributions are
mainly four-fold:
1) The proposed method takes two factors, i.e., task relation-
ship across the whole continual learning process and sample
importance within each class, into account for sample weight
assignment. Such a design facilitates the model to deal with
the â€˜stability-plasticityâ€™ dilemma that plagues the continual
learning paradigm.
2) We theoretically prove that the proposed method can
implicitly model the task relationship. Specifically, the updat-
ing formulation of the Relation Replay Net depends on the
similarity between the gradient of each training sample and the
averaged gradient of each class stored in the memory buffer.
3) As far as we know, in the continual learning problem, we
are the first to propose an automated sample weighting strategyto adaptively assign a reasonable weight to each sample from
the new and old tasks, which is more flexible than existing
approaches based on manual tuning.
4) The proposed method can be applied to various
rehearsal-based continual learning baselines and consistently
improves their performance under various settings.
This paper is organized as follows. Section II provides a
review of some related works and Section III briefly introduces
the setting and necessary notations for continual learning.
Section IV presents the proposed RER method in detail.
Section V then provides the experiments and analysis of our
method. The paper is finally concluded in Section VI.
II. R ELATED WORK
A. Continual Learning
Rehearsal-based Methods. The primary mechanism under-
lying rehearsal-based methods [22]â€“[29], [33] is using the
information of data from old tasks to prevent forgetting
while training new tasks. Specifically, these methods typically
involve saving a portion of data samples from old tasks
as a memory buffer, which is subsequently used alongside
new incoming data samples to train the model. For example,
GEM [25] formulates a quadratic programming problem to
enforce orthogonality between the optimization direction for
a new task and the previously stored in the memory buffer
during training. A-GEM [34] relaxes the constraints in GEM
by only restricting the dot product of the new and old
sample gradients to be non-negative, so as to improve the
computational efficiency of the algorithm. iCaRL [24] stores
the most representative samples, i.e., located around the class
center in the latent space as the memory buffer, and uses the
Nearest Class Mean (NCM) classifier to mitigate the impact
of feature representation changes. Rainbow Memory [28]
constructs the memory buffer by sampling more representative
samples, which is determined by the prediction confidence
of the data after various augmentations. In addition to the
ground truth labels, DER++ [26] also saves data probabilities
yielded by the model from previous epochs in the memory
buffer as soft labels, which are used for distillation to further
prevent forgetting. On the flip side, rehearsal-based methods
commonly lead to class imbalance problems, as they rely
on a small memory buffer to store a limited number of
examples from old tasks. To address this problem, LUCIR [33]
normalizes the predicted logits and imposes constraints on the
features to correct the imbalance problem between new and old
samples. ER-ACE [29] calculates the loss function of the new
and old tasks independently to alleviate the mutual interference
between them.
Other Continual Learning Methods. Model-based ap-
proaches [12]â€“[16], [35] aim to endow the model with the
ability to adapt to new tasks by automatically modifying its
architecture. Although these methods demonstrate impressive
performance in a variety of simulation experiments, their
training, and optimization are known to be challenging and
computationally demanding. For example, PNN [12] saves all
networks of previous tasks to avoid forgetting, which often
occupies a large memory buffer and needs to train another

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
network for the new task. Regularization-based approaches
[17]â€“[21], [36] design different constraints to prevent changing
important parameters of previous tasks during training new
tasks. Typically, EWC [17] limits the updating of important
parameters, which are selected by the Fisher matrix from a
Bayesian perspective. SI [19] determines important parameters
by the effect of the parameter change on the loss. LwF [18]
saves previous model predictions of new task samples as soft
labels to distill the extracted knowledge. Despite their simplic-
ity and effectiveness, these approaches also face challenges
such as hyper-parameter tuning and sensitivity to the choice
of regularization parameters. Besides, continual learning also
be adopted on many realistic problems, such as semantic seg-
mentation [37]â€“[39], few-shot learning [40]â€“[42], and emotion
detection [43], [44], etc. For more detailed information, we
recommend referring to [11], [45].
B. Sample Weighting Strategy
In terms of sample weighting, our method is closely related
to L2RW [46] and Meta Weight Net (MW-Net) [47]. These
approaches involve training a classification network on a noisy
label dataset and being optimized through a meta-learning
strategy to mitigate the impact of noisy labels. This meta-
learning process is guided by a small clean dataset, referred
to as the meta set. However, the challenges of CL are different
from the noisy label problem, and especially the unstable
data flow and severe catastrophic forgetting make it difficult
to directly apply these sample weighting methods to CL. To
mitigate this problem, we propose a novel pair-wise sample
weighting strategy to model task relationships and it does not
require any additional high-quality data as the meta set like
L2RW or MW-Net due to the limitation of continual learning
settings. To our knowledge, this should be the first work to
use the automatic sample weighting strategy for the continual
learning problem.
III. P RELIMINARIES
In this section, we briefly introduce the settings of the
continual learning problem and two main rehearsal-based
baselines. In continual learning, the model fis required to
learn a stream of tasks T={D1,D2,Â·Â·Â·} and for each
taskDt, only a few samples can be stored in a memory
buffer M, where the buffer size is denoted as M. Typically,
the model f:X â†’ RCtis a classification neural network
parameterized by Î¸, where Ctdenotes the total number of
classes across tasks 1 to t. In this study, we mainly focus on the
Class Incremental (Class-IL) and Task Incremental (Task-IL)
settings of continual learning. Specifically, in Task-IL, the task
boundary is clear, i.e.,the task ID tis known during training
and testing. Therefore, the model can feed the input data to the
corresponding task-specific classifier using masks [26] based
on its task ID. Contrastively, the task ID is unreachable in
Class-IL, which is more difficult.
A. Experience Replay (ER)
ER is a fundamental rehearsal-based approach that aims
to mitigate the forgetting of previously learned tasks whilelearning new ones by replying to samples stored in the memory
buffer. Specifically, during each task training, it samples a
batch of data BD={(xD
i, yD
i)}B
i=1from the current t-th task
Dtand another batch of data BM={(xM
i, yM
i)}B
i=1from
the memory buffer Mwith the same batch size B, where
xiandyirepresent an example and its corresponding label,
respectively. The loss function of the model can be formulated
as:
Ltr(Î¸) =1
BBX
i=1Î»DLCE(xD
i;Î¸) +Î»MLCE(xM
i;Î¸),(1)
where the loss function LCEis cross-entropy (CE) loss.
In Eq. (1), the hyperparameters Î»DandÎ»Mrepresent the
weights assigned to the loss functions for the new task and
the memory buffer, respectively. Typically, a common setting
isÎ»D= 1 andÎ»M=Î». The value of Î»is a crucial
hyperparameter that must be preset manually before training.
A larger Î»value can enhance the modelâ€™s plasticity to improve
performance on the new task, while a smaller Î»value can
emphasize stability and prevent forgetting the previous tasks.
However, setting a fixed weight for Î»is not always optimal
for all datasets, and tuning it for each incoming task poses a
significant challenge.
B. Dark Experience Replay++ (DER++)
DER++ [26], an extension of ER, adds an additional knowl-
edge distillation loss LKDbetween the prediction probabilities
of the current model and those of the previous models for
samples in the memory buffer to further alleviate the forgetting
compared with ER. Formally, the training objective is
Ltr(Î¸)=1
BBX
i=1Î»DLCE(xD
i;Î¸)+Î»MLCE(xM
i;Î¸)+Î³MLKD(xM
i;Î¸),
(2)
where the weight for the new task is consistent with ER
(Î»D= 1), and the weights for the loss function of memory
buffer [Î»M, Î³M]are two hyperparameters that are typically
manually set by grid search. Note that these hyperparameters
are interrelated and play a crucial role in determining the
â€˜stability-plasticityâ€™ trade-off of the model. However, similar to
ER, the two parameters are also required to be preset and fixed
in the whole continual learning process. This poses a challenge
on how to automatically balance â€˜stabilityâ€™ and â€˜plasticityâ€™
during training across different continual learning scenarios.
IV. R ELATIONAL EXPERIENCE REPLAY
Most existing works alleviate the model forgetting problem
by storing more information about old tasks, which can
potentially result in an increased demand for data storage.
In this study, we aim to enhance continual learning from
a different perspectiveâ€”assigning proper weights to training
samples based on the following factors: 1) task relationship
between new and old tasks, and 2) sample importance within
each task for network training. To account for the first factor,
we consider that a sample from the memory buffer containing
similar or semantically-relevant information to the new task
samples is more prone to suffer from disturbance, potentially
leading to more forgetting. In this situation, the model should

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
â„’ğ‘¡ğ‘¡ğ‘¡ğ‘¡â„¬ğ‘¡ğ‘¡ğ‘¡ğ‘¡;ğœƒğœƒğ‘˜ğ‘˜,ğœ™ğœ™ğ‘˜ğ‘˜
â„’ğ‘ğ‘ğ‘ğ‘â„¬ğ‘ğ‘ğ‘ğ‘;ğœƒğœƒğœ™ğœ™Inner loop
Outer loopğœ™ğœ™ğ‘˜ğ‘˜+1=ğœ™ğœ™ğ‘˜ğ‘˜âˆ’ğœ‚ğœ‚ğœ™ğœ™ğ›»ğ›»ğœ™ğœ™â„’ğ‘ğ‘ğ‘ğ‘â„¬ğ‘ğ‘ğ‘ğ‘;ğœƒğœƒğ‘˜ğ‘˜+1ğœ™ğœ™ğœƒğœƒğ‘˜ğ‘˜+1ğœ™ğœ™=ğœƒğœƒğ‘˜ğ‘˜âˆ’ğœ‚ğœ‚ğœƒğœƒğ›»ğ›»ğœƒğœƒâ„’ğ‘¡ğ‘¡ğ‘¡ğ‘¡(â„¬ğ‘¡ğ‘¡ğ‘¡ğ‘¡;ğœƒğœƒğ‘˜ğ‘˜,ğœ™ğœ™ğ‘˜ğ‘˜)ğ‘¥ğ‘¥ğ‘€ğ‘€ğ‘¥ğ‘¥ğ·ğ·ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ·ğ·)
ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ‘€ğ‘€)ğ‘§ğ‘§ğ·ğ·
ğ‘§ğ‘§ğ‘€ğ‘€ğœ†ğœ†ğ·ğ·ğœ™ğœ™ğ‘˜ğ‘˜
ğœ†ğœ†ğ‘€ğ‘€ğœ™ğœ™ğ‘˜ğ‘˜
ğ‘§ğ‘§ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘
backward forward parameter passingRRN
â„â‹…;ğœ™ğœ™ğ‘˜ğ‘˜Main illustration
Main Net
ğ‘“ğ‘“â‹…;ğœƒğœƒğ‘˜ğ‘˜
Updated 
Main Net
ğ‘“ğ‘“â‹…;ğœƒğœƒğ‘˜ğ‘˜
Fig. 2. The main illustration of the proposed Relational Experience Replay at k-th iteration. In the inner loop, the Main Net is trained on the batch of paired
dataBtr. In the inner loop, the Relation Replay Net (RRN) generates the weights of the paired batch of training samples Btr, and the Main Net is updated
by SGD based on the weighted training loss. In the outer loop, given the updated parameters of the Main Net, RRN is trained with the batch of data sampled
from the memory buffer, which is a relatively balanced dataset.
probably pay more attention to this memory buffer sample
(i.e., assigning larger sample weights to this old sample or
decreasing weights of new task samples) to alleviate the
forgetting issue. Vice versa, the model should properly assign
larger weights to the new task samples to better learn new
knowledge without disturbing old tasks too much. As for the
second factor, the data points from the same class also tend to
make different contributions to model training. The easy-to-
learn samples deliver more representative feature information
of their classes, while the â€˜hardâ€™ samples are more conducive
to refining the classification boundaries.
As mentioned above, the loss weights Î›, which control
the balance between â€˜stabilityâ€™ and â€˜plasticityâ€™ during train-
ing across different continual learning scenarios, should be
dynamically assigned to different samples instead of being
manually tuned and fixed [26]. To this end, we propose
a Relation Replay Net (RRN) that extracts the interaction
knowledge of the new and old samples and generates their
corresponding weights Î›dynamically. It facilitates the main
classification network (dubbed Main Net) to achieve a better
trade-off between â€˜stabilityâ€™ and â€˜plasticityâ€™. Intuitively, the
RRN depends on the training state of the Main Net, and
the sample weights generated by the RRN in turn affect the
training of the Main Net. Therefore, instead of the naive end-
to-end training, we adopt a bi-level learning framework to
jointly optimize the RRN and the Main Net, and the superiority
of the bi-level optimization will be demonstrated in Sec. V-C.
For simplicity, we initially apply the proposed approach to
ER (termed Relational Experience Replay, or RER), and the
implementation based on more baselines can be found in
Appendix II-A.
A. Overview
The RER framework, as depicted in Fig. 2, consists of two
key components: Main Net f(Â·;Î¸)responsible for continuallearning, which can be any commonly used backbone architec-
ture, and RRN h(Â·;Ï•)utilized for estimating sample weights.
On the one hand, a CL model must be able to adapt to
different new tasks based on the knowledge learned from the
old tasks, i.e., task relationship would affect the â€˜plasticityâ€™ as
aforementioned. Furthermore, each new task sample carries
a different degree of influence on the modelâ€™s plasticity,
highlighting the importance of sample weights in fine-tuning
the modelâ€™s attention. To model this dynamic â€˜plasticityâ€™, we
assign weights for pair-wise training samples consisting of
new and old task samples to train the Main Net, where the
weights are generated by the RRN based on their relationships.
Concretely, we combine the new task batch BDand the
memory batch BMto pair-wisely construct training sample
batchBtr={(xD
i, xM
i)}B
i=1, where we omit the labels for
notation convenience. Then, the weighted loss function for the
Main Net fformulates the inner loop optimization problem
of the bi-level learning framework, that is:
Î¸âˆ—(Ï•) = arg min
Î¸Ltr(Btr;Î¸, Ï•)
â‰œ1
BBX
i=1Î»D
i(Ï•)Ltr(xD
i;Î¸) +Î»M
i(Ï•)Ltr(xM
i;Î¸),(3)
where Ltris CE loss based on ER, and the sample weights
Î› ={Î»D
i, Î»M
i}B
i=1of these data pairs are generated by the
RRN automatically.
On the other hand, the model should possibly alleviate the
forgetting issue while learning new tasks. The proposed RRN
should pay more attention to â€˜stabilityâ€™ to prevent the Main
Net from focusing too much on new tasks. We thus formulate
the outer loop optimization problem over memory buffer data,
making the Main Net returned by optimizing the inner loop
one in Eq. (3) acts as a stable consolidation of knowledge

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
from the learned tasks1,i.e.,
Ï•âˆ—= arg min
Ï•Lbf(Bbf;Î¸âˆ—(Ï•))
â‰œ1
BBX
i=1Lbf(xi;Î¸âˆ—(Ï•)),(4)
where the Lbf(x;Î¸)can be simply adopted as CE loss, and
Bbfis another batch sampled from the memory buffer M,
which is different from the inner-loop training batch BM. By
dynamically generating specific weights for different samples,
the RRN can help the Main Net to better balance the trade-off
between new and old tasks, thereby facilitating the continual
learning process.
Furthermore, the design of the RRN should consider the
following two factors: 1) the RRN should take the interaction
information between the new and old tasks into account; 2)
the input of the RRN should be â€˜ information abundant â€™
to ensure that the RRN could extract useful knowledge to
generate meaningful weights. With this aim, we propose to pair
each new task sample with a corresponding sample from the
memory buffer and use their respective abstract information
as inputs to the RRN. Specifically, for each sample pair xD
i
from the new task and xM
ifrom the memory buffer, we take
their losses LD,M=
Ltr(xD
i), Ltr(xM
i)
, and logit norm
||zD,M||=
âˆ¥zD
iâˆ¥2,âˆ¥zM
iâˆ¥2
as the inputs of the RRN, where
z=f(x;Î¸)is the predicted logits. By taking into account
both label and feature information, the RRN can capture the
semantic and distributional similarities between the paired
samples and generate appropriate weights for the Main Net to
balance the importance of new and old tasks during training.
Thus, the paired sample weights generated by the RRN can
be written as:

Î»D
i(Ï•), Î»M
i(Ï•)
=h 
LD,M,âˆ¥zD,Mâˆ¥2;Ï•
. (5)
In summary, the proposed RER forms a bi-level learning
framework to simultaneously model the â€˜plasticityâ€™ and â€˜sta-
bilityâ€™ in the inner and outer loop optimization problems,
respectively. The RRN, guided by a relatively balanced set
M, automatically generates sample weights to refine the
optimization direction of the Main Net. Therefore, it can
facilitate the Main Net optimization, leading to a better trade-
off between new and old tasks. In the inference stage, we can
directly predict the testing images by the Main Net without
passing the RRN. Note that our method can be easily adapted
to other rehearsal-based baselines, such as ER-ACE [29] and
DER++ [26] by some simple modification (please refer to
Section V-B and Appendix II-A).
B. Optimization Procedure
Since it is difficult to find closed-form solutions, the opti-
mization of Î¸andÏ•as shown in Eq. (3) and Eq. (4) depends
on two nested loops, which is computationally expensive.
Considering computational efficiency and the large scale of
1Note that the parameter Ï•of the Relation Replay Net is regarded as a
hyper-parameter of that of the Main Net.Algorithm 1 Relational Experience Replay training algorithm
Input : new task data Dt, memory buffer M
Output : Main Net and Relation Replay Net (RRN) parameters
{Î¸, Ï•}
1:whileDtÌ¸=âˆ…do
2: while k < Iter maxdo
3: Sample a new task batch BDâˆˆ Dtand a buffer batch
BMâˆˆ M
4: Construct a paired training batch Btrâ† BDandBM
5: Calculate the inner-loop loss by Eq. (3)
6: Update Î¸kby Eq. (6)
7: Sample another buffer batch Bbfâˆˆ M
8: Calculate the outer-loop loss by Eq. (4)
9: Update Ï•kby Eq. (7)
10: k+ +.
11: end while
12:end while
data to be processed, we adopt an alternative online gradient-
based optimization strategy to solve the proposed bi-level
learning framework.
Updating Î¸:Referring to Eq (3), given the parameter Ï•kof
RRN at iteration step k, we optimize the parameter Î¸of Main
Net by one-step gradient descent:
Î¸k+1(Ï•) =Î¸kâˆ’Î·Î¸â–½Î¸Ltr(Btr;Î¸k, Ï•k), (6)
where Î·Î¸is the inner-loop learning rate. Note that the updated
parameter Î¸k+1(Ï•)is actually a function of Ï•.
Updating Ï•:With Main Net parameter Î¸, we can optimize
RRN parameter Ï•by Eq. (4) given Î¸k+1(Ï•)by the following
formulation:
Ï•k+1=Ï•kâˆ’Î·Ï•â–½Ï•Lbf(Bbf;Î¸k+1(Ï•k)), (7)
where Î·Ï•is the outer-loop learning rate. More details of the
gradient calculation can be found in Appendix I.
C. Theoretical Analysis
According to Eqs. (6) and (7), we have the following
proposition to further reveal how the proposed method models
the task-wise relationship.
Proposition 1. Letgbf(x) =âˆ‚Lbf(x;Î¸)
âˆ‚Î¸
Î¸kandgtr(x) =
âˆ‚Ltr(x;Î¸)
âˆ‚Î¸
Î¸kdenote the gradients of the buffer sample and the
training sample with respect to the parameter Î¸, respectively.
Then the updating formulation of Ï•presented in Eq. (7) can
be reformulated as
Î¸k+1(Ï•) =Î¸k+Î·Î¸Î·Ï•
BBX
j=1G(j)Â·âˆ‚hj(Ï•)
âˆ‚Ï•
Ï•k, (8)
where the gradientâˆ‚hj(Ï•)
âˆ‚Ï•=
âˆ‚Î»D
j(Ï•)
âˆ‚Ï•,âˆ‚Î»M
j(Ï•)
âˆ‚Ï•T
, and the
coefficient G(j)is
G(j) =1
BCtX
c=1 BcX
i=1gbf(xi)!

gtr(xD
j), gtr(xM
j)
.(9)

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
â‹®
â‹®â‹®ğ‘§ğ‘§ğ·ğ·
ğ‘§ğ‘§ğ‘€ğ‘€ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ·ğ·)
ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ‘€ğ‘€)
ğ‘§ğ‘§ğ·ğ·2
ğ‘§ğ‘§ğ‘€ğ‘€2ğœ†ğœ†ğ·ğ·
ğœ†ğœ†ğ‘€ğ‘€RRN
â„â‹…;ğœ™ğœ™Relation replay net (RRN)
ğœ†ğœ†ğ·ğ·
ğœ†ğœ†ğ‘€ğ‘€RRN
â„â‹…;ğœ™ğœ™
â‹®â‹®
â‹®ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ·ğ·)
ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ‘€ğ‘€)ğ‘§ğ‘§ğ·ğ·2
ğ‘§ğ‘§ğ‘€ğ‘€2
ğœ†ğœ†ğ·ğ·
ğœ†ğœ†ğ‘€ğ‘€RRN
â„â‹…;ğœ™ğœ™
â‹®â‹®
â‹®ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ·ğ·)
ğ¿ğ¿ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ğ‘€ğ‘€)ğ‘§ğ‘§ğ·ğ·2
ğ‘§ğ‘§ğ‘€ğ‘€2
Fig. 3. The architecture of the Relation Replay Net (RRN).
In Proposition 1, we demonstrate that the task-wise relation-
ship in the RRN can be implicitly modeled through the SGD
updating process. This is achieved by computing the coeffi-
cient G(j), which represents the mean dot product similarity
between the gradient of buffer samples and that of training
samples within each class. This measure effectively captures
the relationship between the new task and the previously seen
classes from a gradient perspective. The proof can be found
in Appendix I.
D. Implementation Details
The Architecture of the Relation Replay Net. We propose a
two-hidden-layer neural network as the RRN. Intuitively, the
first layer consists of two individual linear layers to extract
the interaction information of loss values and the logit norms
for paired training samples, respectively. The second layer is a
linear layer that merges the extracted information to generate
weights for the sample pairs automatically. Fig. 3 illustrates
the architecture of RRN where we set the number of hidden
units as 16 for computation efficiency.
Training Details. When training a new task, the RRN aims to
generate reasonable sample weights for both new task samples
and buffer samples to improve the trade-off between â€˜stabilityâ€™
and â€˜plasticityâ€™, which needs a warm-up stage to explore the
relationship between the new and old tasks. Specifically, in the
warm-up stage (stage length denoted as Iter warm ), we update
the RRN by Eq. (7) and update the Main Net via preset fixed
weights like the previous methods. After the warm-up stage,
we turn to use the sample weights generated by the RRN to
guide the Main Net training rather than prefixed weights. In
addition, to reduce the calculation burden, we only update the
RRN once while updating the Main Net for multiple steps
(step number denoted as Interval ). The specific choices of
Iter warm andInterval are presented in Section V-D.
V. E XPERIMENTAL RESULTS
To validate the effectiveness of our method, we conduct
extensive experiments on three publicly available datasets, i.e.,
CIFAR-10 ,CIFAR-100 [48] and Tiny ImageNet [49], and
present thorough ablation analysis to gain insight into our
method in this section.A. Experimental Setting
For CIFAR-10, we divide the 10 classes into five tasks and
each task is a binary classification problem. Both CIFAR-100
and Tiny ImageNet datasets are divided into 10 tasks, where
each task is a 1-of-10 and 1-of-20 classification problem,
respectively. To ensure a robust and reliable evaluation, the
framework was trained for 50 epochs on each task across all
datasets following DER++ [26].
As aforementioned, this study mainly focuses on the con-
figurations of Class-IL and Task-IL. The detailed experimental
settings are consistent with DER++ [26] for a fair comparison.
In our experiments, we evaluate the model by two common-
used metrics, i.e., Average Accuracy (ACC) and Backward
Transfer (BWT) [25], [26]. We repeat each experiment five
times to reduce the randomness of network training. Similar
to [26], the total size of the memory buffer remains constant
throughout the entire training process.
The proposed method is implemented with the PyTorch
platform [50]. The backbone of Main Net is the widely-used
ResNet-18 [2]. In the inner loop, the Main Net is optimized
by SGD with an initial learning rate of 0.03for all datasets,
and in the outer loop, Adam [51] is adopted to optimize RRN,
where the initial learning rate is set as 0.001 with a weight
decay of 10âˆ’4.
B. Comparison with State-of-the-art Methods
As previously mentioned, our proposed method aims to en-
hance general rehearsal-based baselines by taking into account
the â€˜stability-plasticityâ€™ dilemma, which involves the relation-
ship between new and old tasks, and the specific importance
of different samples. We integrate this approach into three
representative baselines: ER, ER-ACE [29], and DER++ [26],
which are termed Relational-ER (RER), Relational-ER-ACE
(RER-ACE), and Relational-DER (RDER), respectively. Im-
plementation details can be found in Appendix II-A.
Table I presents the comparison results between our pro-
posed algorithm applied on the three baselines and various
state-of-the-art methods over ACC and BWT metrics. These
comparison methods include three regularization-based meth-
ods: oEWC [20], SI [19], and LwF [18], and four rehearsal-
based methods: GEM [25], A-GEM [34], iCaRL [24], and
GSS [52]. Besides, we also provide an upper-bound method
and a lower-bound method for better reference, where the
former trained on all data from old and new tasks together
and the latter directly trained on the new task without any
strategies to prevent model forgetting.
For CIFAR-10, it can be observed that our proposed ap-
proach can be adapted to different rehearsal-based baselines
and achieve consistent performance improvement. For exam-
ple, the proposed RDER achieves as much as 3.08% absolute
performance gain compared to the baseline DER++ with a
memory buffer of 200 under Class-IL. Besides, our method
achieves significantly higher classification accuracy than all
comparison state-of-the-art methods under different settings.
On the other hand, our method can also significantly reduce
the BWT of baselines, indicating that the proposed method can

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
TABLE I
COMPARISON WITH THREE DIFFERENT BASELINES AND OTHER STATE -OF-THE-ART METHODS ON THE CIFAR-10 AND TINY-IMAGE NET. THE RESULTS
OF OUR METHOD ARE SHOWN IN GRAY CELLS AND THE BETTER RESULTS ARE PRESENTED IN BOLD .
Buffer Size MethodCIFAR-10 Tiny-ImageNet
Class-IL Task-IL Class-IL Task-IL
ACC BWT ACC BWT ACC BWT ACC BWT
- Upper bound 92.20 Â±0.15 - 98.31 Â±0.12 - 59.99 Â±0.19 - 82.04 Â±0.10 -
- Lower bound 19.62 Â±0.05 -96.39 Â±0.12 61.02 Â±3.33 -46.24 Â±2.12 7.92 Â±0.26 -76.73 Â±0.08 18.31 Â±0.68 -64.97 Â±1.70
- oEWC 19.49 Â±0.12 -91.64 Â±3.07 68.29 Â±3.92 -29.13 Â±4.11 7.58 Â±0.10 -73.91 Â±0.79 19.20 Â±0.31 -59.86 Â±0.42
- SI 19.48 Â±0.17 -95.78 Â±0.64 68.05 Â±5.91 -38.76 Â±0.89 6.58 Â±0.31 -67.91 Â±0.96 36.32 Â±0.13 -53.26 Â±0.75
- LwF 19.61 Â±0.05 -96.69 Â±0.25 63.29 Â±2.35 -32.56 Â±0.56 8.46 Â±0.22 -76.74 Â±0.44 15.85 Â±0.58 -67.79 Â±0.23
200GEM 25.54 Â±0.76 -82.61 Â±1.60 90.44 Â±0.94 - 9.27 Â±2.07 - - - -
A-GEM 20.04 Â±0.34 -95.73 Â±0.20 83.88 Â±1.49 -16.39 Â±0.80 8.07 Â±0.08 -77.02 Â±0.22 22.77 Â±0.03 -56.61 Â±0.32
iCaRL 49.02 Â±3.20 -28.72 Â±0.49 88.99 Â±2.13 - 1.01 Â±4.15 7.53 Â±0.79 -22.70 Â±0.44 28.19 Â±1.47 -10.36 Â±0.31
GSS 39.07 Â±5.59 -75.25 Â±4.07 88.80 Â±2.89 - 8.56 Â±1.78 - - - -
ER 55.84 Â±0.71 -47.77 Â±1.39 92.41 Â±0.59 - 5.73 Â±0.38 8.67 Â±0.25 -77.29 Â±0.26 39.28 Â±0.83 -42.05 Â±0.29
RER 58.59 Â±0.74 -44.50 Â±0.80 92.85 Â±0.36 - 5.40 Â±0.65 9.35 Â±0.21 -76.67 Â±0.38 40.83 Â±0.58 -41.19 Â±0.55
ER-ACE 63.02 Â±1.29 -20.35 Â±1.76 92.59 Â±0.36 - 5.33 Â±0.41 11.67 Â±0.29 -49.03 Â±1.61 42.08 Â±0.35 -37.71 Â±1.10
RER-ACE 63.52 Â±0.71 -20.11 Â±5.66 92.63 Â±0.58 - 5.43 Â±0.67 12.18 Â±0.41 -48.91 Â±2.59 44.11 Â±0.62 -36.62 Â±1.17
DER++ 62.30 Â±1.07 -35.83 Â±1.34 90.74 Â±1.01 - 7.45 Â±1.07 12.26 Â±0.31 -68.37 Â±1.38 40.47 Â±1.53 -40.41 Â±1.29
RDER 65.38 Â±0.42 -34.16 Â±1.90 91.67 Â±0.80 - 6.81 Â±1.19 13.96 Â±0.64 -67.02 Â±1.24 40.87 Â±0.92 -39.87 Â±1.31
500GEM 26.20 Â±1.26 -74.31 Â±4.62 92.16 Â±0.69 - 9.12 Â±0.21 - - - -
A-GEM 22.67 Â±0.57 -94.01 Â±1.16 89.48 Â±1.45 -14.26 Â±4.18 8.06 Â±0.04 -77.06 Â±0.41 25.33 Â±0.49 -55.68 Â±1.01
iCaRL 47.55 Â±3.95 -25.71 Â±1.10 88.22 Â±2.62 - 1.06 Â±4.21 9.38 Â±1.53 -20.89 Â±0.23 31.55 Â±3.27 - 7.30 Â±0.79
GSS 49.73 Â±4.78 -62.88 Â±2.67 91.02 Â±1.57 - 7.73 Â±3.99 - - - -
ER 69.01 Â±0.37 -33.02 Â±2.62 94.28 Â±0.27 - 3.09 Â±1.61 10.40 Â±0.16 -74.36 Â±0.58 48.82 Â±0.34 -31.06 Â±1.53
RER 69.22 Â±1.96 -29.79 Â±2.87 94.50 Â±0.41 - 3.40 Â±0.33 11.50 Â±0.47 -74.13 Â±0.72 51.28 Â±0.93 -30.29 Â±1.24
ER-ACE 71.26 Â±0.66 -13.37 Â±1.06 94.31 Â±0.23 - 3.19 Â±0.39 19.59 Â±0.13 -47.56 Â±0.68 50.99 Â±0.45 -29.32 Â±0.46
RER-ACE 71.29 Â±1.15 -12.53 Â±2.41 94.25 Â±0.23 - 3.16 Â±0.80 20.41 Â±0.66 -42.22 Â±1.09 54.62 Â±0.87 -25.15 Â±0.98
DER++ 72.11 Â±1.41 -23.40 Â±1.32 94.21 Â±0.32 - 3.98 Â±0.60 19.29 Â±1.14 -60.58 Â±0.46 51.39 Â±0.91 -26.90 Â±0.52
RDER 73.99 Â±1.03 -22.86 Â±1.76 94.04 Â±0.43 - 3.82 Â±0.59 20.06 Â±1.18 -56.16 Â±1.38 52.56 Â±0.69 -25.02 Â±0.24
5120GEM 25.26 Â±3.46 -75.27 Â±4.41 95.55 Â±0.02 - 6.91 Â±2.33 - - - -
A-GEM 21.99 Â±2.29 -84.49 Â±3.08 90.10 Â±2.09 - 9.89 Â±0.40 7.96 Â±0.13 -76.01 Â±0.52 26.22 Â±0.65 -55.61 Â±0.84
iCaRL 55.07 Â±1.55 -24.94 Â±0.14 92.23 Â±0.84 - 0.99 Â±1.41 14.08 Â±1.92 -16.00 Â±0.28 40.83 Â±3.11 - 2.60 Â±0.35
GSS 67.27 Â±4.27 -58.11 Â±9.12 94.19 Â±1.15 - 6.38 Â±1.71 - - - -
ER 83.30 Â±0.50 -13.79 Â±1.40 96.95 Â±0.15 - 0.98 Â±0.36 28.52 Â±0.37 -52.54 Â±1.45 68.46 Â±0.40 -10.13 Â±0.20
RER 83.53 Â±0.55 -12.13 Â±1.29 96.98 Â±0.17 - 0.79 Â±0.24 33.86 Â±0.64 -45.56 Â±2.13 69.31 Â±0.41 -10.68 Â±0.25
ER-ACE 82.98 Â±0.38 - 3.99 Â±0.61 96.76 Â±0.08 - 0.63 Â±0.30 37.02 Â±0.17 -33.29 Â±1.03 68.69 Â±0.19 - 9.88 Â±0.42
RER-ACE 83.74 Â±0.79 - 4.05 Â±1.81 96.80 Â±0.20 - 0.57 Â±0.27 36.97 Â±0.94 -33.79 Â±3.07 69.05 Â±0.73 - 9.51 Â±0.79
DER++ 84.50 Â±0.63 - 9.79 Â±0.34 95.91 Â±0.57 - 1.57 Â±0.25 37.88 Â±0.37 -30.62 Â±1.78 68.05 Â±0.53 - 8.80 Â±0.24
RDER 85.56 Â±0.38 - 8.81 Â±0.71 96.21 Â±0.22 - 1.42 Â±0.09 39.67 Â±0.96 -29.37 Â±1.53 68.82 Â±0.54 - 8.03 Â±0.46
TABLE II
COMPARISON OF ACC WITH THREE DIFFERENT BASELINES ON THE CIFAR-100. T HE RESULTS OF OUR METHOD ARE SHOWN IN GRAY CELLS AND THE
BETTER RESULTS ARE PRESENTED IN BOLD .
Settings Buffer Size ER RER ER-ACE RER-ACE DER++ RDER
100 11.31 Â±0.21 13.94 Â±0.89 18.59 Â±1.08 20.20 Â±0.86 14.98 Â±0.65 20.79 Â±1.05
200 14.78 Â±0.40 16.40 Â±0.53 25.14 Â±1.83 26.64 Â±0.29 24.17 Â±1.37 30.65 Â±0.76
500 23.10 Â±0.32 26.97 Â±0.75 36.02 Â±0.84 36.06 Â±1.14 35.19 Â±1.30 39.50 Â±1.54Class-IL
5120 51.43 Â±1.01 54.08 Â±0.63 53.93 Â±2.04 54.38 Â±1.08 55.58 Â±1.86 60.07 Â±0.23
100 58.64 Â±1.31 59.77 Â±0.03 59.91 Â±1.01 61.10 Â±0.93 58.32 Â±1.27 59.07 Â±0.73
200 66.31 Â±0.76 66.83 Â±0.97 64.81 Â±3.14 67.42 Â±0.60 66.47 Â±0.57 68.60 Â±0.51
500 73.10 Â±0.99 73.99 Â±0.51 74.13 Â±0.84 74.54 Â±1.37 74.10 Â±1.62 75.59 Â±0.85Task-IL
5120 86.16 Â±0.47 85.35 Â±0.34 84.69 Â±1.32 85.15 Â±0.41 86.23 Â±2.18 86.54 Â±0.31
effectively reduce model forgetting while improving classifi-
cation accuracy, i.e., achieving a better balance of â€˜stabilityâ€™
and â€˜plasticityâ€™.
For Tiny-ImageNet dataset, our method also consistently
achieves the best results under almost all settings. Although
iCaRL achieves sound BWT metric, it falls short in terms
of ACC, revealing that a model that pays much attention
to avoid forgetting may negatively impact classification per-
formance. In contrast, our method improves both ACC and
BWT compared to the corresponding baselines, highlighting
that our method can achieve a better trade-off between new
and old tasks. Notably, the results of GEM and GSS are notreported in Table I since the excessive computational overhead
is unacceptable.
Moreover, we validate the effectiveness of our method on
CIFAR-100 in Table II. Obviously, our method can achieve a
significant improvement for all these three baselines across
different buffer sizes. For instance, in the Class-IL setting
with a buffer size of 100, our method applied to ER, ER-
ACE, and DER can improve their ACC by 2.63%, 1.61%,
and 5.81%, respectively. These results further demonstrate the
strong adaptability of our method to multiple datasets and
diverse settings.

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
(a) CIFAR-10 ER/RER
 (b) CIFAR-10 ER-ACE/RER-ACE
 (c) CIFAR-10 DER++/RDER
(d) CIFAR-100 ER/RER
 (e) Tiny-ImageNet ER/RER
Fig. 4. Classification accuracy (%) of each task during the whole training process. The dot-solid lines represent the comparison methods ER, ER-ACE, or
DER++ and the triangle-dashed lines represent our methods RER, RER-ACE, or RDER.
C. Discussion
To further explore the proposed method, we conduct more
experiments and analyses in this section.
Does Our Method Mitigate the Model Forgetting? In
order to better analyze the forgetting problem for old tasks,
we visualize the accuracy change for each task during the
continual learning process in Fig. 4 and Appendix II-C. The
visualization results show that our method better mitigates
forgetting for previous tasks, i.e., consolidating knowledge for
old tasks better than corresponding baselines. Note that the
performance of RDER is slightly lower than DER++ for some
new tasks (shown in Fig. 4c). This is because our method aims
to improve the generalization of all tasks, not just pay attention
to new tasks with more data.
How Task Similarity Affects Continual Learning Models?
In this section, we further investigate the impact of task
similarity on the generation of sample weights. From CIFAR-
10, we choose two classes belonging to the â€˜objectâ€™ category,
namely â€˜shipâ€™ and â€˜truckâ€™, as task 1. For task 2, we consider
two distinct setups: Setup 1 , which includes â€˜airplaneâ€™ and
â€˜automobileâ€™, and Setup 2 , which includes â€˜catâ€™ and â€˜horseâ€™.
Notably, Setup 1 is a relatively semantic-relevant task since its
classes also belong to the â€˜objectâ€™ category. Conversely, both
classes in Setup 2 belong to the â€˜animalsâ€™ category, rendering
it a relatively semantic-irrelevant task with lower similarity to
task 1. Here we focus on the baseline ER and our proposedRER, because ER employs the same loss function ( i.e., CE
loss) for both new and old tasks, and the corresponding sample
weights directly reflect their respective contributions to the
training of the Main Net2.
To mitigate the impact of random factors, we trained
ER/RER 10 times and report the ACC under Setup 1 and
Setup 2 in Fig. 7(a). The ACC of Setup 1 is significantly
lower than that of Setup 2 for both ER and RER, implying that
the extracted knowledge of the new task in Setup 1 is similar
to the old task, which may interfere with the classification of
the old task. Moreover, our proposed RER exhibits a smaller
performance gap between Setup 2 andSetup 1 than ER,
indicating that our method can effectively extract the task
relationship and mitigates interference from new tasks and
resulting in overall performance enhancement.
Furthermore, at the end of task 2 in RER, we calculate the
average weights for validation samples from the old and new
tasks, respectively. Fig. 7(b) shows the results for 10 different
runs of RER under Setup 1 (represented by â€˜dark orangeâ€™
scatters) and Setup 2 (represented by â€˜dodgerblueâ€™ scatters).
It is clear that under Setup 1 , the RRN tends to generate
larger weights for the old task samples and smaller weights
for the new task samples. This suggests that for a similar
new task, the old task is more important in ameliorating the
forgetting because similar tasks may generate similar features
2Here we exclude DER++ and ER-ACE from our analysis since they adopt
different loss functions and may introduce additional sources of interference.

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
Method Setup 1 Setup 2 Gap
ER 81.38 Â±1.67 88.29 Â±0.89 6.91
RER 84.32 Â±0.81 89.41 Â±1.60 5.09
Fig. 5. *
(a) ACC of the ER and RER under Setup 1 (semantic-relevant
task) and Setup 2 (semantic-irrelevant task). The gap in the
last column indicates the difference between the ACC of
Setup 1 andSetup 2 .
Fig. 6. *
(b) Visualization of the average sample weights of new
task (horizontal axis) and old task (vertical axis) of
Setup 1 andSetup 2 .
Fig. 7. The visualization of task similarity effects.
which will interfere with each other. On the other hand, under
Setup 2 , the average sample weights of the old and new tasks
are more dispersed between 0 and 1, indicating that dissimilar
(i.e., semantic-irrelevant) new tasks may not strongly interfere
with the old tasks and the sample weights for each task should
be determined in real-time during the whole training process.
Why We Need the Bi-level Optimization? To evaluate the ef-
fectiveness of the bi-level optimization paradigm, we conduct
an ablation study by end-to-end training of the Main Net and
the RRN together based on DER++ (referred to as Vanilla).
Specifically, we employ the same RRN to generate the paired
sample weights and update the Main Net and the RRN by a
single backward step through Ltr+Lbf. In Table III, we ob-
serve that the sample weights generated by the Vanilla method
appear to be uninformative, and even impair the performance
of the baseline approach (DER++), indicating that the end-
to-end training paradigm cannot produce meaningful weights
to enhance the generalization capability of the Main Net. In
contrast, our RDER approach consistently outperforms both
DER++ and the Vanilla method, thereby demonstrating the
effectiveness of the proposed bi-level optimization framework
to achieve a better trade-off between â€˜stabilityâ€™ and â€˜plasticityâ€™.
Why Use the Memory Buffer to Train Relation Replay Net
in the Outer Loop? In Eq. (4) of the outer-loop optimization,
we utilize the memory buffer Mto train the RRN. However,
concerns arise regarding the possibility of overfitting when
using the memory buffer to guide the RRN. To address this,
we investigate two approaches for constructing the outer-loopTABLE III
ACC ON THE CIFAR-10 DATASET OF THE VANILLA TRAINING AND OUR
PROPOSED BI -LEVEL OPTIMIZATION FRAMEWORK .
MemoryMethod Class-IL Task-ILSize
200DER++ [26] 62.30 Â±1.07 90.74 Â±1.01
RDER (ours) 65.38Â±0.42 91.67Â±0.80
Vanilla 62.85 Â±2.90 91.20 Â±1.88
500DER++ [26] 72.11 Â±1.41 94.21 Â±0.32
RDER (ours) 73.99Â±1.03 94.04Â±0.43
Vanilla 72.28 Â±0.93 93.42 Â±0.75
5120DER++ [26] 84.50 Â±0.63 95.91 Â±0.57
RDER (ours) 85.56Â±0.38 96.21Â±0.22
Vanilla 82.26 Â±2.45 95.3 Â±0.72
TABLE IV
ACC ON THE CIFAR-10 DATASET OF DIFFERENT WAYS TO CONSTRUCT
THE TRAINING SET IN THE OUTER LOOP .
MemoryMethod Class-IL Task-ILSize
200DER++ [26] 62.30 Â±1.07 90.74 Â±1.01
RDER (ours) 65.38Â±0.42 91.67Â±0.80
Split RDER 62.53 Â±0.66 91.36 Â±0.77
500DER++ [26] 73.11 Â±1.41 94.21 Â±0.32
RDER (ours) 73.99Â±1.03 94.04Â±0.43
Split RDER 72.57 Â±0.73 93.77 Â±0.33
5120DER++ [26] 84.50 Â±0.63 95.91 Â±0.57
RDER (ours) 85.56Â±0.38 96.21Â±0.22
Split RDER 85.35 Â±0.24 96.19 Â±0.46
training set: 1) Splitting the memory buffer Minto two sets,
which are utilized for training the Main Net in the inner
loop and the RRN in the outer loop, respectively [53]. 2)
Alternatively, training the RRN in the outer loop using the
entire memory buffer M, as we propose.
We present a comparison between the two approaches based
on DER++ and report the results in Table IV, where the
two methods are denoted as â€˜Split RDERâ€™ and â€˜RDERâ€™,
respectively. Split RDER divides the memory buffer into
two sets for outer- and inner-loop training with a ratio of
20%âˆ’80%, following [53], whereas the RDER approach does
not involve such a split. The results in Table IV indicate that
Split RDER exhibits a slight improvement over the baseline
DER++, but its performance is still lower than our proposed
RDER. These findings suggest that the first approach may lead
to better generalization of the RRN while reducing the number
of memory samples used to train the Main Net. However,
this reduction in training samples may create a more serious
imbalance between the new and old tasks, thereby reducing
the overall performance, particularly when the buffer size is
limited.
D. Ablation Study
In this section, we first validate the effect of the small buffer
sizes on our method based on the three baselines. And then we
conduct a detailed ablation study based on RDER to evaluate
the influence caused by two critical hyperparameters Iter warm
andInterval on CIFAR-10 with different buffer sizes.
Effect of Small Buffer Size. To further investigate the impact
of small buffer sizes on our method, we evaluate our method
applied to the three baselines on CIFAR-10 in Table V.

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
TABLE V
COMPARISON OF ACC ON THE CIFAR-10 WITH SMALL BUFFER SIZES . THE RESULTS OF OUR METHOD ARE SHOWN IN GRAY CELLS AND THE BETTER
RESULTS ARE PRESENTED IN BOLD .
Settings Buffer Size ER RER ER-ACE RER-ACE DER++ RDER
50 36.47 Â±2.92 37.43 Â±1.27 42.16 Â±1.78 41.95 Â±1.07 46.70 Â±4.14 49.75 Â±1.92Class-IL100 46.75 Â±1.58 50.85 Â±1.14 56.96 Â±2.39 57.98 Â±3.02 53.82 Â±1.21 56.36 Â±1.54
50 88.13 Â±1.16 88.07 Â±0.80 87.09 Â±1.05 86.45 Â±1.05 84.00 Â±0.84 85.81 Â±1.99Task-IL100 90.27 Â±1.05 90.56 Â±0.41 90.28 Â±0.38 91.01 Â±0.04 87.48 Â±1.43 89.16 Â±1.37
Specifically, RDER enhances DER++ by about +3.05% with a
buffer size of 50 under the Class-IL setting, demonstrating that
our method can further explore the information in the memory
buffer to enhance the Main Net overall performance.
Impact of the Warm-up Stage ( Iter warm ).Table VI presents
the evaluation results for the various Iter warm under three dif-
ferent settings1
3,1
2,2
3
Ã—Iter max. The performance of RDER
deteriorates significantly when a small value of Iter warm (i.e.,
17) is used, suggesting that the RRN requires an adequate
number of warm-up steps to generate meaningful sample
weights to accurately capture the task-wise relationship and
sample importance within each task. On the other hand, it can
be observed a slight drop in performance under Iter warm =
33, since the preset weights may mislead the training in the
warm-up stage. An excessively long warm-up stage also leads
to insufficient iterations for the Main Net training guided
by the generated weights. Hence, we recommend setting
Iter warm to be half of the number of iteration epochs for
each task, which performs the best in Table VI.
Impact of the Relation Replay Net Updating Interval
(Interval ).Here we vary the value of Interval to investi-
gate its impact on our framework. As shown in Table VII,
the setting Interval = 5 yields the highest classification
accuracy across a range of memory buffer sizes. However,
small Interval , such as Interval = 1 , often results in
decreased performance due to the frequent alternation between
updating the Main Net and the RRN, leading to oscillation
during training. Additionally, frequent updates of the RRN are
computationally expensive, which can impede the convergence
of the overall framework. Conversely, large Interval , such
asInterval = 10 , can lead to faster computation, but we
observed a significant drop in performance due to inadequate
training of the RRN, which can result in the generation
of suboptimal sample weights. To strike a balance between
accuracy and computational efficiency, we propose an em-
pirical formulation Interval = #epoch/ 10, where #epoch
represents the number of iteration epochs for each task.
VI. C ONCLUSION
In this paper, we focus on the â€˜stability-plasticityâ€™ dilemma
in continual learning and strive to adaptively tune the rela-
tionship across different tasks and samples. To this end, we
propose a novel continual learning framework, Relational Ex-
perience Replay, which pair-wisely adjusts the sample weights
of samples from new tasks and the memory buffer. The sample
weights generated by the Relation Replay Net can facilitate
the optimization of the Main Net to achieve a better trade-offTABLE VI
ACC ONCIFAR-10 WITH DIFFERENT LENGTH OF WARM -UP STAGE
(Iterwarm ).
Memory
SizeIterwarm Class-IL Task-IL
5017 48.02 Â±0.70 85.13 Â±1.27
25 49.75 Â±1.92 85.81 Â±1.99
33 47.41 Â±0.91 85.12 Â±1.35
20017 64.53 Â±1.13 91.84 Â±0.29
25 65.38 Â±0.42 91.67 Â±0.80
33 64.84 Â±1.08 92.88 Â±0.42
50017 72.17 Â±1.11 93.57 Â±0.17
25 73.99 Â±1.03 94.04 Â±0.43
33 73.03 Â±1.49 93.85 Â±0.44
TABLE VII
ACC ONCIFAR-10 WITH DIFFERENT VALUES OF RELATION REPLAY NET
UPDATING INTERVAL (Interval ).
Memory
SizeInterval Class-IL Task-IL
501 48.12 Â±0.51 84.44 Â±1.48
5 49.75 Â±1.92 85.81 Â±1.99
10 46.21 Â±1.00 85.18 Â±1.65
2001 64.27 Â±1.46 91.57 Â±0.59
5 65.38 Â±0.42 91.67 Â±0.80
10 64.66 Â±0.29 91.71 Â±0.62
5001 71.23 Â±0.99 93.15 Â±0.68
5 73.99 Â±1.03 94.04 Â±0.43
10 72.11 Â±0.46 93.62 Â±0.23
between â€˜stabilityâ€™ and â€˜plasticityâ€™. The proposed method can
be easily integrated with multiple rehearsal-based methods
to achieve significant improvements. We theoretically and
experimentally verify that the generated sample weights can
extract the relationship between new and old tasks to automat-
ically adjust the Main Net training and enhance the overall
performance. We expect that our method can provide more
insights into the â€˜stability-plasticityâ€™ dilemma and promote the
development of the field of continual learning.
APPENDIX I
CALCULATION DETAILS ABOUT THE RELATION REPLAY
NETUPDATING
In this section, we provide a detailed calculation of the
derivatives of the RRN. Referring back to Section IV-B, the
gradient descent of the RRN parameters is given in Eq. (7)
and repeated here as
Ï•k+1=Ï•kâˆ’Î·Ï•â–½Ï•Lbf(Bbf;Î¸(Ï•)), (10)
where the Î¸(Ï•)is the one-step updated parameters generated
by Eq. (6).

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
Here we can use the chain rule to calculate the derivative
ofÏ•as follows.
â–½Ï•Lbf(Bbf;Î¸k(Ï•))
=1
BBX
i=1â–½Ï•Lbf(xi;Î¸k(Ï•))
=1
BBX
i=1âˆ‚Lbf(xi;Î¸)
âˆ‚Î¸
Î¸kâˆ‚Î¸(Ï•)
âˆ‚Ï•
Ï•k,(11)
where the second term can be represented as
âˆ‚Î¸(Ï•)
âˆ‚Ï•
Ï•k=âˆ’Î·Î¸
BBX
j=1ï£«
ï£­â–½Î¸Ltr(xD
j;Î¸k)Â·âˆ‚Î»D
j(Ï•)
âˆ‚Ï•
Ï•k
+â–½Î¸Ltr(xM
j;Î¸k)Â·âˆ‚Î»M
j(Ï•)
âˆ‚Ï•
Ï•kï£¶
ï£¸.(12)
Then substituting Eq. (12) into Eq. (11) and exchanging the
order of the two summations, we can get
â–½Ï•Lbf(Bbf;Î¸k(Ï•))
=âˆ’Î·Î¸
BBX
j=1ï£«
ï£­1
BBX
i=1âˆ‚Lbf(xi;Î¸)
âˆ‚Î¸
Î¸kÂ·âˆ‚Ltr(xD
j;Î¸)
âˆ‚Î¸
Î¸kÂ·âˆ‚Î»D
j(Ï•)
âˆ‚Ï•
Ï•k
+1
BBX
i=1âˆ‚Lbf(xi;Î¸)
âˆ‚Î¸
Î¸kÂ·âˆ‚Ltr(xM
j;Î¸)
âˆ‚Î¸
Î¸kÂ·âˆ‚Î»M
j(Ï•)
âˆ‚Ï•
Ï•kï£¶
ï£¸
=âˆ’Î·Î¸
BBX
j=1ï£«
ï£­GD(j)Â·âˆ‚Î»D
j(Ï•)
âˆ‚Ï•
Ï•k+GM(j)Â·âˆ‚Î»M
j(Ï•)
âˆ‚Ï•
Ï•kï£¶
ï£¸
=âˆ’Î·Î¸
BBX
j=1G(j)âˆ‚h(Ï•)
âˆ‚Ï•
Ï•k,
(13)
where the last term in Eq. (13) is the derivative of the RRN
hj(Ï•)outputs of the j-th training sample pair with respect to
the network parameters Ï•, that is
âˆ‚h(Ï•)
âˆ‚Ï•=ï£®
ï£¯ï£¯ï£°âˆ‚Î»D
j(Ï•)
âˆ‚Ï•
Ï•k
âˆ‚Î»M
j(Ï•)
âˆ‚Ï•
Ï•kï£¹
ï£ºï£ºï£», (14)
and the coefficients G(j) =
GD(j)GM(j)
, where
GD(j) =1
BBX
i=1âˆ‚Lbf(xi;Î¸)
âˆ‚Î¸
Î¸kÂ·âˆ‚Ltr(xD
j;Î¸)
âˆ‚Î¸
Î¸k
â‰œ1
BBX
i=1gbf(xi)Â·gtr(xD
j),
GM(j) =1
BBX
i=1âˆ‚Lbf(xi;Î¸)
âˆ‚Î¸
Î¸kÂ·âˆ‚Ltr(xM
j;Î¸)
âˆ‚Î¸
Î¸k
â‰œ1
BBX
i=1gbf(xi)Â·gtr(xM
j),(15)
respectively. Denote the gradient of the meta loss of the
i-th sample of Dbfasgbf(xi) =âˆ‚Lbf(xi;Î¸)
âˆ‚Î¸
Î¸k, and the
gradient of training loss on the j-th sample pair of Dtras
gtr(xD
j) =âˆ‚Ltr(xD
j;Î¸)
âˆ‚Î¸
Î¸kandgtr(xM
j) =âˆ‚Ltr(xM
j;Î¸)
âˆ‚Î¸
Î¸k.Obviously, the coefficient G(j)represents the similarity be-
tween the gradient of training loss
gtr(xM
j)gtr(xD
j)
and
the average of the gradient of meta loss gbf(xi). Furthermore,
we can reformulate the average gradient of the meta loss by
class. The coefficient G(j)can be represented as:
GD(j) =1
BCtX
c=1 BcX
i=1gbf(xi)!
gtr(xD
j),
GM(j) =1
BCtX
c=1 BcX
i=1gbf(xi)!
gtr(xM
j),(16)
where Ctis the number of all seen classes, Bcis the sample
number of each class in a batch, andPCt
c=1Bc=B. This
formulation shows that the coefficients G(j)implicitly model
the relationship between the knowledge extracted from each
training sample of the new task and that from the average meta
samples.
Then the first derivative term in Eq. (13) can be represented
as:
â–½Ï•Lbf(Dbf;Î¸(Ï•)) =âˆ’Î·Î¸
BBX
i=1G(j)Â·âˆ‚h(Ï•)
âˆ‚Ï•
Ï•k. (17)
Therefore, the gradient of the RRN parameters Ï•can be
calculated by Eq. (8), which can be easily done in PyTorch
[50] with the automatic differentiation.
APPENDIX II
MORE EXPERIMENT DETAILS
In this section, we first illustrate some experimental details
and then present some additional results to demonstrate the
effectiveness of our method further.
A. How to Apply the Proposed Method to Other Baselines?
In the main text of the paper, we take ER as an example to
illuminate how our proposed approach helps rehearsal-based
continual learning models deal with the â€˜stability-plasticityâ€™
dilemma. Here we present how to apply our proposed method
to other baselines, i.e., ER-ACE [29], and DER++ [26].
a) Relational-ER-ACE (RER-ACE): ER-ACE [29],
which represents â€˜Experience Replay with Asymmetric
Cross-Entropyâ€™, combines the losses of the new task samples
and the memory buffer samples as
Ltr(Î¸) =1
BBX
i=1Î»DLCE(xD
i;Î¸, Ccurr)
+Î»MLCE(xM
i;Î¸, CcurrâˆªCcurr),(18)
where the hyperparameters Î›DandÎ›Mare preset to 1 in
[29]. The LCE(D;C)is defined as:
LCE(x;Î¸, C) =âˆ’logzc(x)P
câ€²âˆˆCzcâ€²(x), (19)
where the sample xbelongs to the c-th class and zc(x)is the
c-th element of the main classification network output f(x;Î¸).
Obviously, it is straightforward to apply our proposed
method to ER-ACE, where the RRN still generates the weights
[Î›D,Î›M]for each training sample pair. The outer-loop op-
timization is the same as Eq. (4) and the generated sample
weights are applied to the inner-loop optimization Eq. (18).

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
(a) CIFAR-100 ER-ACE/RER-ACE
 (b) Tiny-ImageNet ER-ACE/RER-ACE
(c) CIFAR-100 DER++/RDER
 (d) Tiny-ImageNet DER++/RDER
Fig. 8. Classification accuracy (%) of each task in the whole training process. The dot-solid lines represent the comparison methods ER, ER-ACE, or DER++
and the triangle-dashed lines represent our methods RER, RER-ACE, or RDER.
b) Relational-DER (RDER): The loss function of
DER++ is shown in Eq. (2), which involves three hyperparam-
eters[Î»D, Î»M, Î³M]. Intuitively, the outer-loop loss function for
the RRN in Eq. (4) can be reformulated as:
Lbf(x;Î¸) =Lbf
CE(x;Î¸) +Lbf
KD(x;Î¸), (20)
which is the sum of the CE loss and the distillation loss.
B. Other Hyperparameters
In the warm-up stage ( i.e.the epochs before Iter warm ), we
use preset weights in the inner loop optimization like previous
methods. Specifically, for RER and RER-ACE, the weight for
the CE loss of new and old task samples is preset as 1 and
0.5, respectively. And for RDER, the preset weights are 1, 0.5,
and 0.2 for the CE loss of new task samples LCE(Dt), the CE
loss of memory buffer samples LCE(Mt), and the distillation
loss of memory buffer samples LKD(Mt), respectively.
C. Additional Visualization Results
Here we show the classification accuracy of each task
of ER-ACE/RER-ACE and DER++/RDER on CIFAR-100 in
Fig. 8(a) and Fig. 8(c), and on Tiny ImageNet in Fig. 8(b) and
Fig. 8(d). Similar to Fig. 8, our method obviously improvesthe accuracy of the previous tasks. Besides, to balance the
â€˜stabilityâ€™ and â€˜plasticityâ€™, the RDER achieves an overall
higher performance even though may not outperform DER++
on some new tasks in Fig. 8(c). All of these results further
demonstrate the effectiveness of our method, which can easily
be adapted to multiple rehearsal-based methods.
REFERENCES
[1] K. Simonyan and A. Zisserman, â€œVery deep convolutional networks for
large-scale image recognition,â€ arXiv preprint arXiv:1409.1556 , 2014.
[2] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for
image recognition,â€ in IEEE Conference on Computer Vision and Pattern
Recognition , 2016, pp. 770â€“778.
[3] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and
K. Keutzer, â€œDenseNet: Implementing efficient ConvNet descriptor
pyramids,â€ arXiv preprint arXiv:1404.1869 , 2014.
[4] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, â€œPyramid scene parsing
network,â€ in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2017, pp. 2881â€“2890.
[5] K. He, G. Gkioxari, P. Doll Â´ar, and R. Girshick, â€œMask R-CNN,â€ in
Proceedings of the IEEE International Conference on Computer Vision ,
2017, pp. 2961â€“2969.
[6] A. Douillard, Y . Chen, A. Dapogny, and M. Cord, â€œPLOP: Learning
without forgetting for continual semantic segmentation,â€ in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2021, pp. 4040â€“4050.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
[7] S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster R-CNN: Towards
real-time object detection with region proposal networks,â€ Advances in
Neural Information Processing Systems , vol. 28, 2015.
[8] J. Redmon and A. Farhadi, â€œYOLOv3: An incremental improvement,â€
arXiv preprint arXiv:1804.02767 , 2018.
[9] A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, â€œYOLOv4: Op-
timal speed and accuracy of object detection,â€ arXiv preprint
arXiv:2004.10934 , 2020.
[10] M. McCloskey and N. J. Cohen, â€œCatastrophic interference in connec-
tionist networks: The sequential learning problem,â€ in Psychology of
Learning and Motivation . Elsevier, 1989, vol. 24, pp. 109â€“165.
[11] M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh, and T. Tuytelaars, â€œA continual learning survey: Defying
forgetting in classification tasks,â€ IEEE Transactions on Pattern Analysis
and Machine Intelligence , vol. 44, no. 7, pp. 3366â€“3385, 2021.
[12] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
K. Kavukcuoglu, R. Pascanu, and R. Hadsell, â€œProgressive neural
networks,â€ arXiv preprint arXiv:1606.04671 , 2016.
[13] C. Fernando, D. Banarse, C. Blundell, Y . Zwols, D. Ha, A. A. Rusu,
A. Pritzel, and D. Wierstra, â€œPathNet: Evolution channels gradient
descent in super neural networks,â€ arXiv preprint arXiv:1701.08734 ,
2017.
[14] J. Serra, D. Suris, M. Miron, and A. Karatzoglou, â€œOvercoming catas-
trophic forgetting with hard attention to the task,â€ in International
Conference on Machine Learning , 2018, pp. 4548â€“4557.
[15] D. Abati, J. Tomczak, T. Blankevoort, S. Calderara, R. Cucchiara, and
B. E. Bejnordi, â€œConditional channel gated networks for task-aware
continual learning,â€ in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2020, pp. 3931â€“3940.
[16] S. Yan, J. Xie, and X. He, â€œDER: Dynamically expandable representation
for class incremental learning,â€ in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 2021, pp. 3014â€“3023.
[17] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al. , â€œOvercoming catastrophic forgetting in neural networks,â€ National
Academy of Sciences , vol. 114, no. 13, pp. 3521â€“3526, 2017.
[18] Z. Li and D. Hoiem, â€œLearning without forgetting,â€ IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 40, no. 12, pp. 2935â€“
2947, 2017.
[19] F. Zenke, B. Poole, and S. Ganguli, â€œContinual learning through synaptic
intelligence,â€ in International Conference on Machine Learning , 2017,
pp. 3987â€“3995.
[20] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y . W.
Teh, R. Pascanu, and R. Hadsell, â€œProgress & compress: A scalable
framework for continual learning,â€ in International Conference on
Machine Learning , 2018, pp. 4528â€“4537.
[21] H. Yin, P. Yang, and P. Li, â€œMitigating forgetting in online continual
learning with neuron calibration,â€ Advances in Neural Information
Processing Systems , vol. 34, pp. 10 260â€“10 272, 2021.
[22] R. Ratcliff, â€œConnectionist models of recognition memory: Constraints
imposed by learning and forgetting functions.â€ Psychological Review ,
vol. 97, no. 2, pp. 285â€“308, 1990.
[23] A. Robins, â€œCatastrophic forgetting, rehearsal and pseudo rehearsal,â€
Connection Science , vol. 7, no. 2, pp. 123â€“146, 1995.
[24] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, â€œiCaRL:
Incremental classifier and representation learning,â€ in IEEE Conference
on Computer Vision and Pattern Recognition , 2017, pp. 2001â€“2010.
[25] D. Lopez-Paz and M. Ranzato, â€œGradient episodic memory for contin-
ual learning,â€ in Advances in Neural Information Processing Systems ,
vol. 30, 2017, pp. 6467â€“6476.
[26] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara, â€œDark
experience for general continual learning: a strong, simple baseline,â€ in
Advances in Neural Information Processing Systems , vol. 33, 2020, pp.
15 920â€“15 930.
[27] A. Chaudhry, A. Gordo, P. Dokania, P. Torr, and D. Lopez-Paz, â€œUsing
hindsight to anchor past knowledge in continual learning,â€ in AAAI
Conference on Artificial Intelligence , vol. 35, no. 8, 2021, pp. 6993â€“
7001.
[28] J. Bang, H. Kim, Y . Yoo, J.-W. Ha, and J. Choi, â€œRainbow memory: Con-
tinual learning with a memory of diverse samples,â€ in IEEE Conference
on Computer Vision and Pattern Recognition , 2021, pp. 8218â€“8227.
[29] L. Caccia, R. Aljundi, N. Asadi, T. Tuytelaars, J. Pineau, and
E. Belilovsky, â€œNew insights on reducing abrupt representation change
in online continual learning,â€ in International Conference on Learning
Representations , 2022.[30] H. Ahn, J. Kwak, S. Lim, H. Bang, H. Kim, and T. Moon, â€œSS-IL:
Separated softmax for incremental learning,â€ in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2021, pp.
844â€“853.
[31] G. A. Carpenter and S. Grossberg, â€œA massively parallel architecture for
a self-organizing neural pattern recognition machine,â€ Computer Vision,
Graphics, and Image Processing , vol. 37, no. 1, pp. 54â€“115, 1987.
[32] B. Zhou, Q. Cui, X.-S. Wei, and Z.-M. Chen, â€œBBN: Bilateral-branch
network with cumulative learning for long-tailed visual recognition,â€ in
IEEE Conference on Computer Vision and Pattern Recognition , 2020.
[33] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin, â€œLearning a
unified classifier incrementally via rebalancing,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2019, pp. 831â€“839.
[34] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny, â€œEfficient
lifelong learning with A-GEM,â€ arXiv preprint arXiv:1812.00420 , 2018.
[35] A. Mallya and S. Lazebnik, â€œPackNet: Adding multiple tasks to a single
network by iterative pruning,â€ in IEEE Conference on Computer Vision
and Pattern Recognition , 2018, pp. 7765â€“7773.
[36] C. V . Nguyen, Y . Li, T. D. Bui, and R. E. Turner, â€œVariational continual
learning,â€ in International Conference on Learning Representations ,
2018.
[37] A. Maracani, U. Michieli, M. Toldo, and P. Zanuttigh, â€œRecall: Replay-
based continual learning in semantic segmentation,â€ in Proceedings of
the IEEE/CVF International Conference on Computer Vision , 2021, pp.
7026â€“7035.
[38] C.-B. Zhang, J.-W. Xiao, X. Liu, Y .-C. Chen, and M.-M. Cheng, â€œRepre-
sentation compensation networks for continual semantic segmentation,â€
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 7053â€“7064.
[39] G. Yang, E. Fini, D. Xu, P. Rota, M. Ding, T. Hao, X. Alameda-Pineda,
and E. Ricci, â€œContinual attentive fusion for incremental learning in
semantic segmentation,â€ IEEE Transactions on Multimedia , 2022.
[40] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y . Gong, â€œFew-shot
class-incremental learning,â€ in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2020, pp. 12 183â€“12 192.
[41] D.-W. Zhou, F.-Y . Wang, H.-J. Ye, L. Ma, S. Pu, and D.-C. Zhan, â€œFor-
ward compatible few-shot class-incremental learning,â€ in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 9046â€“9056.
[42] Y . Cui, W. Deng, X. Xu, Z. Liu, Z. Liu, M. Pietik Â¨ainen, and L. Liu,
â€œUncertainty-guided semi-supervised few-shot class-incremental learn-
ing with knowledge distillation,â€ IEEE Transactions on Multimedia ,
2022.
[43] S. Thuseethan, S. Rajasegarar, and J. Yearwood, â€œDeep continual
learning for emerging emotion recognition,â€ IEEE Transactions on
Multimedia , vol. 24, pp. 4367â€“4380, 2021.
[44] W. Nie, R. Chang, M. Ren, Y . Su, and A. Liu, â€œI-gcn: Incremental
graph convolution network for conversation emotion detection,â€ IEEE
Transactions on Multimedia , vol. 24, pp. 4471â€“4481, 2021.
[45] D.-W. Zhou, Q.-W. Wang, Z.-H. Qi, H.-J. Ye, D.-C. Zhan, and
Z. Liu, â€œDeep class-incremental learning: A survey,â€ arXiv preprint
arXiv:2302.03648 , 2023.
[46] M. Ren, W. Zeng, B. Yang, and R. Urtasun, â€œLearning to reweight
examples for robust deep learning,â€ in International Conference on
Machine Learning . PMLR, 2018, pp. 4334â€“4343.
[47] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng, â€œMeta-
weight net: Learning an explicit mapping for sample weighting,â€ arXiv
preprint arXiv:1902.07379 , 2019.
[48] A. Krizhevsky, G. Hinton et al. , â€œLearning multiple layers of features
from tiny images,â€ 2009.
[49] Stanford, â€œTiny imagenet challenge (CS231n),â€ http://tiny-imagenet.
herokuapp.com/, 2015.
[50] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, â€œAutomatic differentiation in
pytorch,â€ 2017.
[51] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€
arXiv preprint arXiv:1412.6980 , 2014.
[52] R. Aljundi, M. Lin, B. Goujaud, and Y . Bengio, â€œGradient based
sample selection for online continual learning,â€ in Advances in Neural
Information Processing Systems , vol. 32, 2019, pp. 11 816â€“11 825.
[53] Q. Pham, C. Liu, D. Sahoo, and H. Steven, â€œContextual transformation
networks for online continual learning,â€ in International Conference on
Learning Representations , 2020.
