# 2310.02025.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/no-backprop/2310.02025.pdf
# File size: 1682011 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2024
DEEPZERO: SCALING UPZEROTH -ORDER OPTIMIZA -
TION FOR DEEPMODEL TRAINING
Aochuan Chenâ€ ,â‹†Yimeng Zhangâ€ ,â‹†Jinghan Jiaâ€ James Diffenderferâ€¡Jiancheng Liuâ€ 
Konstantinos Parasyrisâ€¡Yihua Zhangâ€ Zheng ZhangÂ§Bhavya Kailkhuraâ€¡Sijia Liuâ€ 
â€ Michigan State University,â€¡Lawrence Livermore National Laboratory,Â§UC Santa Barbara
âˆ—Equal contributions
ABSTRACT
Zeroth-order (ZO) optimization has become a popular technique for solving ma-
chine learning (ML) problems when first-order (FO) information is difficult or im-
possible to obtain. However, the scalability of ZO optimization remains an open
problem: Its use has primarily been limited to relatively small-scale ML problems,
such as sample-wise adversarial attack generation. To our best knowledge, no
prior work has demonstrated the effectiveness of ZO optimization in training deep
neural networks (DNNs) without a significant decrease in performance. To over-
come this roadblock, we develop DeepZero , a principled ZO deep learning (DL)
framework that can scale ZO optimization to DNN training from scratch through
three primary innovations. First , we demonstrate the advantages of coordinate-
wise gradient estimation (CGE) over randomized vector-wise gradient estimation
in training accuracy and computational efficiency. Second , we propose a sparsity-
induced ZO training protocol that extends the model pruning methodology using
only finite differences to explore and exploit the sparse DL prior in CGE. Third ,
we develop the methods of feature reuse and forward parallelization to advance
the practical implementations of ZO training. Our extensive experiments show
that DeepZero achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on
CIFAR-10, approaching FO training performance for the first time. Furthermore,
we show the practical utility of DeepZero in applications of certified adversarial
defense and DL-based partial differential equation error correction, achieving 10-
20% improvement over SOTA. We believe our results will inspire future research
on scalable ZO optimization and contribute to advancing DL with black box.
Codes are available at https://github.com/OPTML-Group/DeepZero .
1 I NTRODUCTION
In the realm of machine learning (ML), optimization algorithms have played a crucial role in en-
abling the training of complex models, yielding unprecedented insights and predictive capabilities
across diverse domains. Over the years, first-order (FO) gradient-based methods, such as stochastic
gradient descent (SGD) and its variants (Gardner, 1984; Amari, 1993; Bottou, 2010; 2012), have be-
come the default choice for model training. These methods rely on gradient information to iteratively
update model parameters, aiming to minimize a given loss function. Nonetheless, several practical
settings exist where FO gradient information is either unavailable or infeasible to compute, call-
ing for alternative strategies. Zeroth-order (ZO) optimization (Flaxman et al., 2005; Shamir, 2013;
Ghadimi & Lan, 2013; Nesterov & Spokoiny, 2015; Duchi et al., 2015; Liu et al., 2018b; Ilyas et al.,
2018b; Zhang et al., 2024) has emerged as a promising approach to address these challenges, as it
leverages finite differences of function values to estimate gradients, rather than requesting explicit
gradient information. Therefore, with minor modifications to FO algorithms, ZO optimization can
be applied to various real-world circumstances where FO gradients are difficult to obtain. For ex-
ample, in disciplines like physics and chemistry, ML models may interact with intricate simulators
or experiments where the underlying systems are non-differentiable (Thelen et al., 2022; Tsaknakis
et al., 2022; Louppe et al., 2019; Abreu de Souza et al., 2023; Baydin et al., 2020). Addition-
ally, black-box learning scenarios often arise when deep learning (DL) models are integrated with
third-party APIs, such as adversarial attack and defense against black-box DL models (Chen et al.,
2017; Ilyas et al., 2018a; Zhang et al., 2022c; Verma et al., 2023) and black-box prompt learning
1arXiv:2310.02025v4  [cs.LG]  15 Mar 2024

--- PAGE 2 ---
Published as a conference paper at ICLR 2024
Acceleration: Parallelization and Feature ReuseGradient
Collection
Frozen Features
Active Featuresâ„“(ðœ½ð‘–)
Forward Pass Backpropagation ZO Gradient Estimation
Query Based ZO Optimization AModel Pruning Guided SparsityActive Param.Dormant Param.
B
C
â€¦DataProcess 2
 Process 1
Process 4Parameter 
Allocation
Process 3
Process N
D Comparison with Other Methods
Computational 
graph FREEComputational 
graph BASED
Figure 1: Overview of our DeepZero framework. A:ZO gradient estimation via model queries (Sec. 3). B:
Model pruning guides gradient sparsity (Sec. 4). C:Acceleration by parallelization and feature reuse (Sec. 5).
D:DeepZero comparison with the computational graph free baseline Pattern Search (Chiang et al., 2023) and
computational graph dependent methods without BP, Align-Ada (Boopathy & Fiete, 2022), LG-FG-A and FG-
W (Ren et al., 2022), on CIFAR-10.
for language-model-as-a-service (Diao et al., 2022; Sun et al., 2022). Furthermore, the principled
backpropagation (BP) mechanism (Amari, 1993; Rumelhart et al., 1995) for calculating FO gradi-
ents may also not be supported when implementing DL models on hardware systems (Gu et al.,
2021b; Tavanaei et al., 2019; Greengard, 2020; Jabri & Flower, 1992; Gu et al., 2021c). In addition
to ZO optimization, another relevant research direction in the field of DL focuses on developing
biologically-plausible, BP-free methods. Examples include forward gradient-based methods (Ren
et al., 2022; Baydin et al., 2022; Silver et al., 2021; Belouze, 2022), greedy layer-wise learning
(NÃ¸kland & Eidnes, 2019), and Hebbian learning (Isomura & Toyoizumi, 2018; Moraitis et al.,
2022). However, these techniques require access to computational graphs and are highly dependent
on the used DL software frameworks and/or model architectures. In contrast, ZO optimization solely
relies on model queries and is free of computational graphs utilized. As a result, ZO optimization
has broad applicability to DL problems that involve black-box query-only components. Despite the
promise of ZO optimization, scalability bottlenecks hinder its application in medium or large-scale
DNN training (Wang et al., 2017; Liu et al., 2018b; Ohta et al., 2020; Cai et al., 2021; Zhang et al.,
2022c). As problem dimensionality increases, the accuracy and efficiency of traditional ZO meth-
ods deteriorate. This is because ZO finite difference-based gradient estimates are biased estimators
of FO gradients, and the bias becomes more pronounced in higher-dimensional spaces (Liu et al.,
2018b; Cai et al., 2021; Balasubramanian & Ghadimi, 2018).
These challenges motivate the central question addressed in this work: (Q)How to scale up ZO
optimization for training deep models? To address (Q), we propose a novel framework, â€˜DeepZeroâ€™,
which infuses novel model pruning and parallel computing techniques to scale up ZO DNN training
(seeFig. 1 for a schematic overview). Our main contributions are summarized below.
â¶We show that deterministic c oordinate-wise g radient e stimation (CGE) outperforms vector-wise
randomized g radient e stimation (RGE) in both accuracy and computation efficiency when scaling to
deep model training. Further, CGE becomes increasingly advantageous as model depth increases.
â·We show that sparsity is crucial for realizing model training via CGE with finite differences. In
contrast to prior work, we find that sparsity for black-box models can be obtained for â€˜freeâ€™ by ex-
tending the current pruning-at-initialization technique to the ZO learning paradigm. The established
synergy between pruning and CGE presents a promising avenue for efficient ZO training of DNNs.
â¸We identify the parallelization-fit property inherent to CGE-based ZO optimization and propose a
novel forward parallelization method based on this property. Our framework enables feature reuse
in deep learning, which further accelerates parallel training by eliminating redundant computations.
â¹We introduce our proposed ZO deep model training framework, â€˜DeepZeroâ€™. To demonstrate
the empirical superiority of DeepZero, we conduct extensive experiments on both standard image
classification benchmarks and real-world black-box DL applications. For example, when employing
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2024
DeepZero to train a ResNet20 on CIFAR-10, we obtain 86.94% testing accuracy, the best reported
in the literature of gradient-free model training. We also exemplify the vast potential and practi-
cal impact of DeepZero in two real-world DL tasks: black-box defense for adversarial robustness
(Zhang et al., 2022c) and physics-informed DL with solver-in-the-loop (Um et al., 2020).
To clarify, our work aims to extend the scalability of ZO optimization for DL applications, address-
ing cases where FO optimization becomes challenging or infeasible. Yet, it is essential to note that
the proposed advancements in ZO training are not intended to overcome the ultimate scalability
challenges to train deep networks at any scale.
2 R ELATED WORK
Classical gradient-free optimization. Early research efforts can be broadly categorized into two
groups: direct search-based methods (DSMs) and model-based methods (MBMs) (Wright et al.,
1999; Conn et al., 2009; Rios & Sahinidis, 2013; Larson et al., 2019). DSMs include techniques like
coordinate (Fermi, 1952) and pattern search (Torczon, 1991) methods and the Nelder-Mead simplex
method (Nelder & Mead, 1965). MBMs consist of model-based descent (Bortz & Kelley, 1998)
and trust region (Conn et al., 2000) methods. Evolutionary optimization offers a generic population-
based gradient-free computational framework including genetic algorithms (Grefenstette, 1993) and
particle swarm optimization (Vaz & Vicente, 2009). Bayesian optimization (Shahriari et al., 2015;
Eriksson et al., 2019) has garnered recent attention by using a Gaussian process (GP) to fit a black-
box objective function and estimate an optimization solution. However, acquiring an accurate GP is
computationally intensive.
Zeroth-order optimization. In contrast to classical gradient-free methods, ZO optimization ap-
proximates gradients using finite differences, simplifying implementation by minimizing modifica-
tions of FO gradient-based algorithms. Like FO methods, ZO enjoys provable convergence guar-
antees (Nesterov & Spokoiny, 2017; Duchi et al., 2015; Liu et al., 2020a). ZO optimization has
gained significant attention for its success in solving various emerging ML problems (Ghadimi &
Lan, 2013; Nesterov & Spokoiny, 2015; Flaxman et al., 2005; Duchi et al., 2015). Examples in-
clude adversarial attack and defense (Chen et al., 2017; Tu et al., 2019; Ye et al., 2018; Ilyas et al.,
2018a; Zhang et al., 2022c; Verma et al., 2023; Zhao et al., 2019; Hogan & Kailkhura, 2018; Shu
et al., 2022), model-agnostic contrastive explanation (Dhurandhar et al., 2019), visual prompting for
transfer learning (Tsai et al., 2020), computational graph unrolling (Vicol et al., 2023), automated
ML (Gu et al., 2021a; Wang et al., 2022), policy search in reinforcement learning (Vemula et al.,
2019), network resource management (Liu et al., 2018b), ML-based scientific workflow optimiza-
tion (Tsaknakis et al., 2022), and on-chip learning (Gu et al., 2021b). Despite ZOâ€™s successes in
solving ML problems, its application has been limited to relatively small scales. For instance, ZO
optimizers used for generating adversarial attacks, contrastive explanations, and visual prompts only
operate in the input parameter space, which has the dimension of a single input example. Some ac-
celeration techniques have been developed to improve ZO performance in larger problems, such as
using historical information to enhance a ZO gradient estimator (Meier et al., 2019; Cheng et al.,
2021), and exploiting gradient sparsity to reduce ZO dependence on problem size (Wang et al., 2017;
Cai et al., 2022; 2021; Balasubramanian & Ghadimi, 2018; Ohta et al., 2020; Gu et al., 2021b).
While gradient sparsity has been used to improve scalability (Bartoldson et al., 2023), we propose
an advanced strategy that leverages model pruning techniques to identify and exploit sparsity in neu-
ral network parameters effectively. Our approach is less restrictive than traditional gradient sparsity
assumptions and allows for greater flexibility in selecting what to prune. To the best of our knowl-
edge, no prior work has demonstrated the practicality of scalable ZO optimization for deep model
training without significant performance loss compared to the FO counterpart.
DL without backpropagation. Forward gradient learning (Baydin et al., 2022; Ren et al., 2022; Sil-
ver et al., 2021; Belouze, 2022), which builds upon the forward-mode automatic differentiation (AD)
capabilities of current DL software frameworks, does not rely on finite differences to approximate
FO gradients like ZO optimization. Instead, it relies on forward-mode AD to calculate a forward
(directional) gradient. This gradient is obtained by projecting the FO gradient onto a direction vec-
tor and is an unbiased estimator of the FO gradient (Baydin et al., 2022). In contrast, ZO gradient
estimation based on finite differences is biased (Duchi et al., 2015; Liu et al., 2020a). However, one
main limitation of forward gradient learning is that it requires full access to AD software and the
deep model, making it impractical for solving black-box DL problems. Recent advances in (Ren
et al., 2022) further improved the scalability of forward gradient learning by using finer-level model
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2024
information to design architecture-specific local objective functions. Other BP-free DL methods are
motivated by seeking a biological interpretation of DL but share similar limitations with forward
gradient learning. Some examples include greedy layer-wise learning (NÃ¸kland & Eidnes, 2019),
input-weight alignment for wide neural networks in the neural tangent kernel (NTK) regime (Boopa-
thy & Fiete, 2022), the Forward-Forward algorithm (Hinton, 2022), Hebbian Learning (Isomura &
Toyoizumi, 2018; Moraitis et al., 2022), and synthetic gradients (Jaderberg et al., 2017).
3 ZO O PTIMIZATION THROUGH FUNCTION VALUE -BASED GRADIENT
ESTIMATION : RANDOMIZED OR COORDINATE -WISE ?
We now introduce the ZO optimization setup and discuss two ZO gradient estimation schemes:
deterministic coordinate-wise gradient estimation ( CGE ) and randomized vector-wise gradient es-
timation ( RGE ). We will demonstrate the advantage of CGE over RGE for DNN training. This
inspires further improvements for scaling CGE-based ZO optimization.
ZO optimization and gradient estimation. Letâ„“(Î¸)denote a loss function that we want to mini-
mize over the optimization variables Î¸âˆˆRd(e.g., model parameters of a neural network). The ZO
optimizer interacts with the objective function â„“only by submitting inputs ( i.e., realizations of Î¸)
and receiving the corresponding function values. It slightly modifies the commonly-used first-order
(FO) gradient-based algorithm by approximating the FO gradient through function value-based gra-
dient estimates (Liu et al., 2020a). This is essential when explicit differentiation is difficult due to
the black-box nature of the loss function (Zhang et al., 2022c; Liu et al., 2020b; Chen et al., 2017), or
when explicit differentiation is undesired due to concerns about energy efficiency (Gu et al., 2021b;
Liu et al., 2018a). RGE (Nesterov & Spokoiny, 2017; Ghadimi & Lan, 2013; Duchi et al., 2015;
Spall, 1992) and CGE (Kiefer & Wolfowitz, 1952; Lian et al., 2016; Berahas et al., 2022) are two
commonly-used gradient estimators based on finite differences of â„“. RGE acquires finite differences
via random perturbations of Î¸while CGE uses deterministic coordinate-wise perturbations of Î¸(Liu
et al., 2020a). Their formal definitions are given by
(RGE )Ë†âˆ‡Î¸â„“(Î¸) =1
qqX
i=1â„“(Î¸+Âµui)âˆ’â„“(Î¸)
Âµui
;(CGE )Ë†âˆ‡Î¸â„“(Î¸) =dX
i=1â„“(Î¸+Âµei)âˆ’â„“(Î¸)
Âµei
,(1)
730 3700 6670 9640 12610
Number of Model Parameters4050607080Testing Accuracy (%)
42.09
41.65
37.5360.47
59.66
50.0670.69
70.04
65.3176.64
76.07
70.0577.01
76.95
68.38
FO Training
ZO CGE
ZO RGE
Figure 2: Performance comparison
of training a simple CNN with vary-
ing numbers of parameters on CIFAR-
10 using different training methods.where Ë†âˆ‡Î¸â„“denotes an estimation of the FO gradient âˆ‡Î¸â„“with
respect to Î¸. In (RGE), uidenotes a randomized perturbation
vector, e.g., drawn from the standard Gaussian distribution
N(0,I),Âµ > 0is a perturbation size ( a.k.a. smoothing pa-
rameter), and qis the number of random directions used to ac-
quire finite differences. In (CGE), eidenotes a standard basis
vector, andâ„“(Î¸+Âµei)âˆ’â„“(Î¸)
Âµprovides the finite-difference esti-
mation of the partial derivative of â„“(Î¸)at the ith coordinate Î¸i.
Finite difference approximations in (1) are motivated by di-
rectional derivatives . Take RGE (with q= 1) as an example.
AsÂµâ†’0, finite difference in RGE converges to directional
derivative â„“â€²(Î¸):=uTâˆ‡Î¸â„“(Î¸) = lim Âµâ†’0â„“(Î¸+Âµui)âˆ’â„“(Î¸)
Âµof
the function â„“at the point Î¸in the direction u(Urruty &
Lemar Â´echal, 1993). As a result, the expression â„“â€²(Î¸)uyields E[â„“â€²(Î¸)u] =E[(uuT)âˆ‡Î¸â„“(Î¸)] =
âˆ‡Î¸â„“(Î¸)(recall that E[uuT] =I). This implies that â„“â€²(Î¸)uis an unbiased gradient estimator of
âˆ‡Î¸â„“(Î¸)and its biased finite difference approximation is given by (1) (Duchi et al., 2015).
RGE or CGE? First, the function query costs for RGE and CGE differ, with RGE taking O(q)
queries and CGE taking O(d)queries based on (1). Compared to CGE, RGE has the flexibility to
specify q < d to reduce the number of function evaluations. Despite the query efficiency, it remains
uncertain whether RGE can deliver satisfactory accuracy when training a deep model from scratch.
To this end, we undertake a preliminary investigation wherein we train a basic convolutional neural
network (CNN) of different sizes on CIFAR-10, employing both RGE and CGE. To ensure a fair
comparison in query complexity, we set the query number qin RGE equal to the problem size dused
in CGE. Fig. 2 presents the test accuracy of the learned CNN against the number of model param-
eters (equivalent to the number of model queries). Here the training recipe is specified by the FO
SGD, the ZO RGE-based SGD, and the ZO CGE-based SGD. We observe that CGE can achieve test
accuracy comparable to FO training and significantly outperforms RGE. This experiment highlights
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2024
the superiority of CGE over RGE in terms of optimization accuracy even when the latter uses q=d.
This accuracy merit of CGE is particularly valuable when training more complex neural networks.
In Appx. C, we provide a detailed analysis of the computational costs using CGE vs. RGE. The
time cost relative to model depth is shown in Fig. A2. And Tab. A1 assesses gradient estimation
time costs. We find that CGE demonstrates greater time efficiency than RGE. The computation ef-
ficiency loss of RGE is in that it needs to generate and integrate a d-dimension perturbation vector
into the entire model at once every query. Based on the advantages of CGE over RGE in terms of
both accuracy and computation efficiency, we choose CGE as the preferred ZO gradient estimator.
However, query complexity of CGE is still a bottleneck, as it scales with model size d.
4 S PARSITY -ASSISTED ZO T RAINING : A P RUNING LENS AND BEYOND
One valuable property of CGE is the disentanglement of finite differences across coordinates, which
suggests that reducing CGEâ€™s query complexity is aligned with pruning the model weights that are
being optimized. With this in mind, we propose integrating ZO optimization with pruned gradients
to design a more effective inductive bias for ZO deep model training. It is worth noting that the spar-
sity has been explored in several existing ZO optimization methods to improve the query efficiency
of gradient estimation (Wang et al., 2017; Cai et al., 2022; 2021; Balasubramanian & Ghadimi,
2018; Ohta et al., 2020; Gu et al., 2021b). However, prior work suffered from two main limita-
tions. Firstly, exact sparsity was assumed in the original FO gradients, which required an additional
sparse learning method (such as LASSO (Wang et al., 2017)) to recover these sparse gradients from
function queries. Secondly, it remains unclear how to optimize the sparsity pattern via a ZO oracle,
as the existing method calls for overly heuristics-based pruning methods (e.g., random (Gu et al.,
2021b) or magnitude (Ohta et al., 2020; Zhang et al., 2022b) pruning). Overly increasing sparsity
ultimately limits optimization performance. In what follows, we propose a new pruning approach
that relies only on model queries, enjoys computation efficiency, and can improve ZO optimization
accuracy by inducing an appropriate gradient sparsity.
ZO-GraSP: Model pruning via ZO oracle. The compressibility of model weights for DL has
been extensively studied (Han et al., 2015; Frankle & Carbin, 2018; Ma et al., 2021; Zhang et al.,
2022a;b; Blalock et al., 2020; Tanaka et al., 2020; Lee et al., 2018; Wang et al., 2020; Su et al.,
2020; Diffenderfer et al., 2021). For instance, the lottery ticket hypothesis (Frankle & Carbin, 2018)
demonstrated that a randomly initialized, dense neural network contains a high-quality sparse sub-
network . However, current effective pruning methods incorporate model training as an intermediate
step (Frankle & Carbin, 2018; Ma et al., 2021; Zhang et al., 2022a; Diffenderfer & Kailkhura, 2021).
Thus, they are not well-suited for finding sparsity via a ZO oracle.
To address the above challenge, we draw inspiration from training-free pruning methods, known as
pruning-at-initialization (Tanaka et al., 2020; Lee et al., 2018; Wang et al., 2020). Within this family,
gradient signal preservation (GraSP) (Wang et al., 2020) is a method to identify the sparsity prior
of DL through the gradient flows of a randomly-initialized network. While GraSP still requires the
FO and second-order derivative information, we can estimate these derivatives using only function
queries to design the ZO version of GraSP (termed ZO-GraSP ). Specifically, GraSP (Wang et al.,
2020) assigns pruning scores (denoted by S) to model initialization Î¸. These scores reflect the
change in gradient flow after pruning the weights:
S=âˆ’Î¸âŠ™(Hg),H=âˆ‡2
Î¸,Î¸â„“(Î¸),g=âˆ‡Î¸â„“(Î¸), (2)
where recall that â„“is the loss function of model training, âŠ™denotes the entry-wise multiplication, and
Hgrepresents the Hessian-gradient product. Using the ZO learning paradigm, we can first approxi-
mate the Hessian-gradient product as the finite difference between two gradients ( i.e.,âˆ‡Î¸â„“(Î¸+Âµg)
andâˆ‡Î¸â„“(Î¸)), in the direction gwith the smoothing parameter Âµ. Second, we replace the FO gradient
âˆ‡Î¸â„“with the ZO gradient estimate Ë†âˆ‡Î¸â„“given in (1). Combining this yields ZO-GraSP:
Ë†S:=âˆ’Î¸âŠ™Ë†âˆ‡Î¸â„“(Î¸+ÂµË†g)âˆ’Ë†âˆ‡Î¸â„“(Î¸)
Âµ. (3)
In practice, we found that the pruning mask determined by ranking the entries in Ë†Sis resilient to
the ZO gradient estimation error. Therefore, we utilize RGE with a relatively small number of
queries ( q < d ) to implement ZO-GraSP. This reduces the function query cost without compromis-
ing pruning performance; see Tab. A2 andTab. A3 for empirical justifications. Our results show
that ZO-GraSP significantly outperforms random pruning and yields pruned models with accuracy
comparable to FO-GraSP.
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2024
Integrating sparsity with CGE. As finite differences in CGE (1) are decomposable over weights,
it is easy to incorporate sparsity into CGE. To retain the accuracy benefits of training dense models,
we incorporate gradient sparsity (in CGE) rather than weight sparsity. This ensures that we train a
dense model in the weight space, rather than training a sparse model where the sparsity determined
by ZO-GraSP is directly applied. Let SZO-GraSP be the coordinate set of unpruned model weights
found by ZO-GraSP. The sparsity-induced CGE is given by
Ë†âˆ‡Î¸â„“(Î¸) =X
iâˆˆS ZO-GraSPâ„“(Î¸+Âµei)âˆ’â„“(Î¸)
Âµei
. (Sparse-CGE)
It is clear that (Sparse-CGE) reduces the query complexity of the original CGE from O(d)to
O(|SZO-GraSP |), where |SZO-GraSP |denotes the cardinality of the coordinate set SZO-GraSP . There
may exist two direct methods for integrating (Sparse-CGE) into ZO optimization. M1: This
method involves alternating between ZO-GraSP and CGE-based ZO optimization. At each iter-
ation,SZO-GraSP is updated based on the model weights from the previous iteration and then used to
construct (Sparse-CGE) for updating Î¸at the current iteration. M2: This method involves perform-
ingpruning before ZO training . That is, ZO-GraSP is conducted at model initialization, and the
resulting SZO-GraSP is applied to (Sparse-CGE) and kept fixed during training. BothM1andM2
have limitations. M1requires repeated calls to ZO-GraSP to update SZO-GraSP , leading to a higher
query cost for ZO model training. M2addresses the query complexity by performing ZO-GraSP be-
fore training, but it can only produce a smaller model after training. It is known that heavily-pruned
models suffers from performance degradation ( e.g., 95% sparse model in Tab. A2 in Appx. D). Thus,
it is nontrivial to integrate ZO-GraSP with ZO training due to the requirement of balancing query
efficiency and training effectiveness. To address this, we propose ZO-GraSP-oriented dynamic
sparsity pattern , which leverages ZO-GraSP to determine layer-wise pruning ratios (LPRs) that
can capture DNN compressibility. This approach shares a similar essence with smart ratio intro-
duced in (Su et al., 2020). Specifically, we acquire LPRs from ZO-GraSP at randomly initialized
weights prior to ZO training, which is query-efficient like M2. However, unlike M2, LPRs allow
for random shuffling of sparse gradient positions in Î¸only if these LPRs are obeyed. This allows
us to mimic M1to alternate between model weight updates and SZO-GraSP updates, with the lat-
ter achieved by LPR-guided randomly updated sparsity patterns. Thus, ZO optimization can train
the dense model using iteratively-updated (Sparse-CGE) with LPRs-guided dynamic sparsity pat-
terns. Overall, our proposal has the query efficiency of M2with the training effectiveness of M1,
resulting in a balanced integration of ZO-GraSP into ZO training. We summarize the algorithmic
pipeline in Algorithm 1 in Appx. E, where CGE and ZO-GraSP-oriented dynamic sparsity pattern
are clearly described in a unified framework. We also refer readers to Appx. E for more explanation
and comparisons with M1andM2. We provide a convergence rate analysis in Appx. A.
5 I MPROVING SCALABILITY : FEATURE REUSE & F ORWARD PARALLEL
730 3700 6670 9640 12610
Number of Model Parameters020406080Training Time (hours)
0.991.66
5.1112.08
11.3928.03
18.9145.22
28.9470.32 CGE with feature reuse
CGE without feature reuse
Figure 3: Computation cost of
CGE-based ZO training w/ fea-
ture reuse vs. w/o feature reuse.
The setup follows Fig. A2.We investigate two characteristics of ZO training that can further
enhance implementation scalability: feature reuse andforward par-
allelization . The former disentangles intermediate features from
weight perturbations, while the latter uses the finite-difference na-
ture in CGE to enable scalable distributed implementation.
Reusing intermediate features. As shown in (1), CGE perturbs
each parameter element-wise. Thus, one can reuse the feature im-
mediately preceding the perturbed layer and carry out the remaining
forward pass operations instead of starting from the input layer, as
illustrated in Fig. 1 . The above characteristic of CGE-based model
training is referred to as â€˜ feature reuse â€™. More concretely, let fÎ¸(x)
be a deep model with parameters Î¸and input x. We can express
fÎ¸(x)as a multi-layer composite function
fÎ¸(x) =fÎ¸>l(zl) =fÎ¸Lâ—¦fÎ¸Lâˆ’1â—¦ Â·Â·Â· â—¦ fÎ¸l+1| {z }
fÎ¸>l(Â·)â—¦fÎ¸lâ—¦ Â·Â·Â· â—¦ fÎ¸1(x)|{z }
zl=fÎ¸1:l(x), (4)
where fÎ¸ldenotes the modelâ€™s lth layer, Lis the total number of model layers, and â—¦is the function
composition operation. Based on (4), if coordinate-wise weight perturbations are applied to the
(l+ 1) th layer and its subsequent layers ( i.e.,Î¸>l), the modelâ€™s outputs corresponding to these
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2024
perturbed weights can be efficiently obtained by keeping the intermediate features up to the lth layer
(i.e.,zl) intact. This efficiency becomes more pronounced when perturbing the parameters of deeper
layers ( i.e., for a larger l).Fig. 3 compares the runtime of CGE-based ZO training with and without
feature reuse. Empirically, CGE with feature reuse exhibits a 2Ã—reduction in training time.
Parallelization of coordinate-wise finite differences. CGE enables parallelization of model train-
ing due to its alignment of parameter perturbations with forward passes. If there exist Mprocesses
(across multiple GPUs), we can decompose CGE (1) based on its parameter coordinates yielding
Ë†âˆ‡Î¸â„“(Î¸) =MX
i=1Ë†gi,Ë†gi:=X
jâˆˆSiâ„“(Î¸+Âµej)âˆ’â„“(Î¸)
Âµej
, (5)
where Siis the set of active parameters assigned to process 1â‰¤iâ‰¤M. Hence, each process can
take|Si|forward passes. This decoupling property enables scaling forward passes via distributed
machines, which can significantly improve training speed. We refer to this parallelization for finite
differences as â€˜ forward parallelization â€™. It is worth noting that forward parallelization is different
from the conventional data parallelization used for FO distributed training (Goyal et al., 2017; You
et al., 2018). Our method avoids any performance loss that may result from data parallelization using
an overly large batch size, which can cause the optimizer to get stuck in suboptimal local minima
due to a lack of stochasticity.
6 E XPERIMENTS
In this section, we first train ResNet-20 for standard image classification on CIFAR-10 , demonstrat-
ing scalability and generalization capability over existing gradient-free learning methods. Second,
we apply DeepZero to enhance the robustness of a black-box DNN against adversarial attacks, where
limited access to the model is available on the defenderâ€™s end. Lastly, we leverage DeepZero to de-
sign a physics-informed ML system by incorporating a scientific PDE solver into the training loop
for reducing numerical errors, highlighting its capability to address complex scientific problems.
6.1 I MAGE CLASSIFICATION TASK
80 90 95 99
Sparsity (%)6065707580859095Testing Accuracy (%)
92.8588.59
86.9490.12
86.2586.4688.55
82.9584.2686.98
68.2273.3778.1
FO training
FO training with SparseWeight
FO training with SparseGradient
DeepZero (Ours)
Figure 4: Comparison between
DeepZero and FO training baselines on a
ResNet-20 for CIFAR-10. We report the
mean and standard deviation of 3 inde-
pendent runs for each experiment.Experiment setup. This study focuses on training ResNet-
20 (with 270K parameters) on CIFAR-10 for image classi-
fication. We adopt SGD (stochastic gradient descent) as the
FO training recipe, with a weight decay of 5Ã—10âˆ’4and
a momentum of 0.9. The learning rate is 0.1, governed by
a cosine decay scheduler. In the ZO training scenario, we
replace the FO gradient by (Sparse-CGE) with a smoothing
parameter Âµ= 5Ã—10âˆ’3. When implementing ZO-GraSP
(3), we set the query budget q= 192 and use the same Âµ
as CGE. Unless specified otherwise, the weight sparsity ra-
tio is chosen to be 90% and the specific sparsity patterns
are determined by SR (Smart Ratio). When implementing
DeepZero ( Algorithm 2 ), we choose the number of epochs
T= 50 . Experiments are run on 4 NVIDIA V100 GPUs
if not specified otherwise. We compare DeepZero with FO
training and two SOTA BP-free training: Pattern Search (Chiang et al., 2023) and Input-Weight
Alignment (Align-ada) (Boopathy & Fiete, 2022).
100 500 1000 2000 4000 8000 16000 32000
Dataset Size01020304050607080Testing Accuracy (%)
DeepZero
Pattern Search
050100150200250300Time (hours)Accuracy
Time Cost
Figure 5: Comparison of DeepZero and Pat-
tern Search on ResNet-20 for CIFAR-10 with
varying dataset sizes. All experiments are done
on a single NVIDIA A6000 GPU.Comparison with FO training. InFig. 4 , we compare
the accuracy of DeepZero-trained ResNet-20 with two
variants trained by FO recipes: (1) a dense ResNet-20
acquired through FO training and (2) a sparse ResNet-
20 acquired through FO training under FO-GraSP spar-
sity pattern. As we can see, the accuracy gap still exists
between (1) and the model trained with DeepZero in
the sparsity regime of 80% to 99%. This highlights the
challenge of ZO optimization for deep model training,
where achieving high sparsity is desired to reduce the
number of model queries in (Sparse-CGE) for scaling
to ResNet-20. Notably, in the sparsity regime of 90%
to 99%, DeepZero outperforms (2), showcasing the superiority of gradient sparsity in DeepZero
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2024
compared to weight sparsity ( i.e., directly training a sparse model). In Appx. F, we provide the
DeepZero training trajectory ( Fig. A4 ), performance vs. data batch setup ( Fig. A5 ) and training time
vs. GPU count ( Fig. A6 ).
Comparison with pattern search (Chiang et al., 2023). InFig. 5 , we compare the accuracy and
runtime cost of DeepZero with Pattern Search (Chiang et al., 2023) for deep model training. Pattern
Search has been shown to achieve comparable test accuracy to SGD in low sample regimes, how-
ever, effectiveness as the number of data samples increases remains unclear. To investigate this, we
conducted experiments using DeepZero (with 90% gradient sparsity) and pattern search on ResNet-
20 with CIFAR-10, with varying dataset sizes from 100 to 32K. We maintained a fixed total epoch
number of 40 for both methods to ensure a fair comparison. The results demonstrate DeepZero out-
performs Pattern Search in all data regimes, except in the case of 100. Further, the improvement of
DeepZero over pattern search (in both accuracy and efficiency) expands dramatically with increasing
dataset sizes, indicating the superior scalability of DeepZero.
Table 1: Performance of DeepZero vs. BP-free
methods on a 8-layer CNN w/ different widths
(Boopathy & Fiete, 2022).
Method DeepZero FA DFA Align-ada
Width 32 64 64 512 64 512 64 512
Accuracy 57.7 64.1 46.5 45.4 49.9 54.1 49.9 58.0
Time (h) 4.34 28.15 0.36 3.79 0.42 3.52 0.48 4.59Comparison with input-weight alignment
(Boopathy & Fiete, 2022). InTab. 1 , we present a
comparison between DeepZero and the Align-ada
approach (Boopathy & Fiete, 2022) for training
neural networks without BP on CIFAR-10. While
other BP-free training methods (Lillicrap et al.,
2014; NÃ¸kland, 2016; Baydin et al., 2022) exist,
Align-ada stands out as it applies to training wide
neural networks and achieves state-of-the-art performance on CIFAR-10, e.g., surpassing methods
such as feedback alignment (FA) (Lillicrap et al., 2014) and direct feedback alignment (DFA)
(NÃ¸kland, 2016). To ensure fairness, we apply DeepZero to the 8-layer CNN architecture from
(Boopathy & Fiete, 2022) and compare performance with Align-ada at varying model widths.
We note that the 512-width network was the widest model trained using Align-ada. In contrast,
the largest width network we train with DeepZero is 64. Our results clearly show that DeepZero
achieves significantly higher testing accuracy compared to Align-ada, even when training with
narrower networks. This demonstrates that the improved performance of DeepZero is attributed to
its inherent optimization advantages, rather than relying solely on the use of wider networks. Lastly,
it is worth noting that Align-ada and other BP-free methods rely on access to computational graphs,
making them efficient but unsuitable for black-box applications.
6.2 O THER BLACK -BOX APPLICATIONS
Table 2: CA (%) vs. â„“2-norm based per-
turbation radius on ImageNet-10 using FO
DS-based defense (FO-DS) (Salman et al.,
2020), ZO-AE-DS (Zhang et al., 2022c),
and our proposed DeepZero.
ImageNet (10 classes)
Radius rFO-DS ZO-AE-DS DeepZero
0.0 89.33 63.60 86.02
0.25 81.67 52.80 76.61
0.5 68.87 43.13 61.80
0.75 49.80 32.73 43.05Black-box defense against adversarial attacks. The
black-box defense problem arises when the owner of an
ML model is unwilling to share the model details with the
defender against adversarial attacks (Zhang et al., 2022c;
Verma et al., 2023). This poses a challenge for existing ro-
bustness enhancement algorithms (Madry et al., 2017; Co-
hen et al., 2019; Salman et al., 2020) that directly robus-
tify white-box ML models using FO training. To overcome
this challenge, ZO optimization was introduced in (Zhang
et al., 2022c) to design a white-box defense operation given
aquery-based black-box image classifier. To address di-
mensionality challenges with ZO, ZO-AE-DS (Zhang et al.,
2022c) introduces an autoencoder (AE) between the white-box denoised smoothing (DS) defense op-
eration (to be learned) and the black-box image classifier. By merging AEâ€™s encoder with the black-
box module, the dimension of ZO optimization is reduced; see Fig. A7 in Appx. G for a schematic
overview and (Zhang et al., 2022c) for details. The downside of ZO-AE-DS is poor scaling to high-
resolution datasets ( e.g., ImageNet) due to the use of AE, which compromises the fidelity of the
image input to the black-box image classifier and leads to inferior defense performance. In contrast,
DeepZero can directly learn the defense operation integrated with the black-box classifier, without
needing AE. Tab. 2 compares the defense performance of DeepZero with FO defense (DS (Salman
et al., 2020)) and ZO-AE-DS (Zhang et al., 2022c). To ensure a fair comparison, we used the same
number of queries (1152) per gradient estimation. Following (Zhang et al., 2022c), we selected a
10-class subset of ImageNet as the training set. The AE and black-box classifier are DnCNN (Zhang
et al., 2017) and ResNet-50, respectively. Defense performance is evaluated by certified accuracy
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2024
(CA), following the setup of (Salman et al., 2020; Zhang et al., 2022c). CA is defined using the â„“2
norm-based input perturbation radius r, where a larger rindicates a stronger adversarial threat. We
refer readers to Appx. G for more experiment details. Tab. 2 highlights that DeepZero consistently
outperforms ZO-AE-DS in terms of CA for all values of r >0. It is important to note that when
r= 0, CA is equivalent to the standard testing accuracy. This indicates that DeepZero excels over
ZO-AE-DS not only in adversarial robustness but also in overall generalization performance.
SRC NON ZO-SOL FO-SOL0.000.050.100.150.20Mean Absolute Error0.180
0.153
0.119
0.074
Figure 6: Average MAE of cor-
rected low-fidelity simulations com-
pared to high-fidelity simulations over
5 test simulations using different cor-
rection methods. Error bar is variance
of MAE across 5 test simulations.Simulation-coupled DL for discretized PDE error correc-
tion. Numerical methods, while instrumental in providing
physics-informed simulations, come with their own challenge:
the discretization unavoidably produces numerical errors. DL
has gained significant attention for addressing this error cor-
rection problem. The feasibility of training a corrective neural
network through looping interactions with the iterative partial
differential equation (PDE) solver, coined â€˜solver-in-the-loopâ€™
(SOL ), has been demonstrated in (Um et al., 2020). While
existing work focused on using or developing differentiable
simulators for model training, we extend SOL by leveraging
DeepZero, enabling its use with non-differentiable or black-
box simulators. We name our method ZO-SOL and refer read-
ers to Fig. A8 in Appx. H for a schematic overview. In this
experimental framework, the goal is to correct the output of a low fidelity ( i.e., coarse mesh) sim-
ulation using a learned DNN so that the corrected simulation more closely aligns with output of a
high fidelity ( i.e., fine mesh) simulation. In an effort to reduce the amount of correction required
by DNN, this correction is applied at each simulation timestep and the corrected simulation state
is provided to the simulator to compute the next timestep. For our experiments, we consider 2D
unsteady wake flow fluid dynamics benchmark from (Um et al., 2020), in which there is a static rod
around which fluid is flowing, and utilize PhiFlow (Holl et al., 2020) as the simulator. Additional de-
tails on the PDE, the solver-in-the-loop loss, and the DNN architecture are given in Appx. H. Fig. 6
compares the test error correction performance of ZO-SOL (via DeepZero) with three differentiable
approaches methods considered in (Um et al., 2020): SRC (low fidelity simulation without error
correction), NON (non-interactive training out of the simulation loop using pre-generated low and
high fidelity simulation data), and FO-SOL (FO training for SOL given a differentiable simulator).
The simulation error for each method in Fig. 6 is measured as the average across the five test simula-
tions (with varying Reynoldâ€™s numbers that were not utilized in the training set of simulation data).
The error for each test simulation is computed as the mean absolute error (MAE) of the corrected
simulation compared to the high fidelity simulation averaged across all simulation timesteps. We
implement DeepZero for ZO-SOL following the setup used in the image classification task, except
for choosing a 95% gradient sparsity. The ZO-SOL and FO-SOL use 16 unrolling steps in the loss
function to allow the correction function to interact with the simulator during training. The results
demonstrate that ZO-SOL achieved by DeepZero outperforms the SRC and NON baselines, and
narrows the performance gap with FO-SOL, despite only having query-based access to the simula-
tor. Comparing ZO-SOL with NON highlights the promise of ZO-SOL even when integrated with
black-box simulators.
7 C ONCLUSION
This paper introduces DeepZero, a framework designed to enhance the scalability of ZO optimiza-
tion for deep network training. Specifically, DeepZero integrates coordinate-wise gradient estima-
tion, ZO pruning-enabled gradient sparsity, feature reuse, and forward parallelization into a unified
training pipeline. Leveraging these innovations, DeepZero showcases both efficiency and effec-
tiveness in a wide range of applications, including image classification tasks and various practical
black-box DL scenarios. While DeepZero has made remarkable progress in training DL models
on datasets like ResNet-20 and CIFAR-10, it is important to acknowledge that scalability remains
a significant challenge when dealing with even larger models and more extensive datasets. Future
studies to accelerate ZO optimization for DL are necessary. Additionally, it is worthwhile to ex-
plore the applicability of DeepZero in other domains, such as digital twin applications that involve
non-differentiable physical entities, and on-device training where the computational overhead of
establishing computational graphs and backpropagation is not feasible. Lastly, we refer readers to
Appx. I for a discussion of the broader impact of this paper.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2024
ACKNOWLEDGMENT
We thank the U.S. Department of Energy via Lawrence Livermore National Laboratory under Con-
tract DE-AC52- 07NA27344 and the LLNL-LDRD Program under Project No. 23-ER-030 (LLNL-
CONF-849161) for their support.
REFERENCES
Fernando Abreu de Souza, Miguel Crispim Rom Ëœao, Nuno Filipe Castro, Mehraveh Nikjoo, and
Werner Porod. Exploring parameter spaces with artificial intelligence and machine learning black-
box optimization algorithms. Phys. Rev. D , 107:035004, Feb 2023. doi: 10.1103/PhysRevD.107.
035004. URL https://link.aps.org/doi/10.1103/PhysRevD.107.035004 .
Shun-ichi Amari. Backpropagation and stochastic gradient descent method. Neurocomputing , 5
(4-5):185â€“196, 1993.
Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochastic opti-
mization via conditional gradient and gradient updates. NeurIPS , 31, 2018.
Brian R Bartoldson, Bhavya Kailkhura, and Davis Blalock. Compute-efficient deep learning: Algo-
rithmic trends and opportunities. Journal of Machine Learning Research , 24:1â€“77, 2023.
AtÄ±lÄ±m G Â¨unes Baydin, Kyle Cranmer NYU, Matthew Feickert, Lindsey Gray, Lukas Heinrich,
Alexander Held NYU, Andrew Melo Vanderbilt Mark Neubauer, Jannicke Pearkes, Nathan Simp-
son, Nick Smith, et al. Differentiable programming in high-energy physics. Submitted as a Snow-
mass LOI , 2020.
AtÄ±lÄ±m G Â¨unes Â¸ Baydin, Barak A Pearlmutter, Don Syme, Frank Wood, and Philip Torr. Gradients
without backpropagation. arXiv preprint arXiv:2202.08587 , 2022.
Gabriel Belouze. Optimization without backpropagation. arXiv preprint arXiv:2209.06302 , 2022.
Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and
empirical comparison of gradient approximations in derivative-free optimization. Foundations of
Computational Mathematics , 22(2):507â€“560, 2022.
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? Proceedings of machine learning and systems , 2:129â€“146, 2020.
Akhilan Boopathy and Ila Fiete. How to train your wide neural network without backprop: An
input-weight alignment perspective. In ICML , pp. 2178â€“2205. PMLR, 2022.
David Matthew Bortz and Carl Tim Kelley. The simplex gradient and noisy optimization prob-
lems. In Computational Methods for Optimal Design and Control: Proceedings of the AFOSR
Workshop on Optimal Design and Control Arlington, Virginia 30 Septemberâ€“3 October, 1997 , pp.
77â€“90. Springer, 1998.
LÂ´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTATâ€™2010: 19th International Conference on Computational StatisticsParis France, Au-
gust 22-27, 2010 Keynote, Invited and Contributed Papers , pp. 177â€“186. Springer, 2010.
LÂ´eon Bottou. Stochastic gradient descent tricks. Neural Networks: Tricks of the Trade: Second
Edition , pp. 421â€“436, 2012.
HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order block coordinate
descent algorithm for huge-scale black-box optimization. arXiv preprint arXiv:2102.10707 , 2021.
HanQin Cai, Daniel Mckenzie, Wotao Yin, and Zhenliang Zhang. Zeroth-order regularized opti-
mization (zoro): Approximately sparse gradients and adaptive sampling. SIAM Journal on Opti-
mization , 32(2):687â€“714, 2022.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM workshop on artificial intelligence and security , pp. 15â€“26, 2017.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2024
Shuyu Cheng, Guoqiang Wu, and Jun Zhu. On the convergence of prior-guided zeroth-order op-
timization algorithms. Advances in Neural Information Processing Systems , 34:14620â€“14631,
2021.
Ping-yeh Chiang, Renkun Ni, David Yu Miller, Arpit Bansal, Jonas Geiping, Micah Goldblum,
and Tom Goldstein. Loss landscapes are all you need: Neural network generalization can be
explained without the implicit bias of gradient descent. In The Eleventh International Conference
on Learning Representations , 2023.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In international conference on machine learning , pp. 1310â€“1320. PMLR, 2019.
A. R. Conn, K. Scheinberg, and L. N. Vicente. Introduction to derivative-free optimization , vol-
ume 8. Siam, 2009.
Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust region methods . SIAM, 2000.
Amit Dhurandhar, Tejaswini Pedapati, Avinash Balakrishnan, Pin-Yu Chen, Karthikeyan Shan-
mugam, and Ruchir Puri. Model agnostic contrastive explanations for structured data. arXiv
preprint arXiv:1906.00117 , 2019.
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, and Tong Zhang.
Black-box prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531 ,
2022.
James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding ac-
curate binary neural networks by pruning a randomly weighted network. arXiv preprint
arXiv:2103.09377 , 2021.
James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A
winning hand: Compressing deep networks can improve out-of-distribution robustness. Advances
in Neural Information Processing Systems , 34:664â€“676, 2021.
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for
zero-order convex optimization: The power of two function evaluations. IEEE Transactions on
Information Theory , 61(5):2788â€“2806, 2015.
David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable
global optimization via local bayesian optimization. Advances in neural information processing
systems , 32, 2019.
Yu Fang, Jiancheng Liu, Mingrui Zhang, Jiasheng Zhang, Yidong Ma, Minchen Li, Yuanming Hu,
Chenfanfu Jiang, and Tiantian Liu. Complex locomotion skill learning via differentiable physics.
arXiv preprint arXiv:2206.02341 , 2022.
Enrico Fermi. Numerical solution of a minimum problem. Technical report, Los Alamos Scientific
Lab., Los Alamos, NM, 1952.
A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit set-
ting: Gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM
symposium on Discrete algorithms , pp. 385â€“394, 2005.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
William A Gardner. Learning characteristics of stochastic-gradient-descent algorithms: A general
study, analysis, and critique. Signal processing , 6(2):113â€“133, 1984.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM Journal on Optimization , 23(4):2341â€“2368, 2013.
Priya Goyal, Piotr Doll Â´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677 , 2017.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2024
Samuel Greengard. Neuromorphic chips take shape. Communications of the ACM , 63(8):9â€“11,
2020.
John J Grefenstette. Genetic algorithms and machine learning. In Proceedings of the sixth annual
conference on Computational learning theory , pp. 3â€“4, 1993.
Bin Gu, Guodong Liu, Yanfu Zhang, Xiang Geng, and Heng Huang. Optimizing large-scale hyper-
parameters via automated learning algorithm. arXiv preprint arXiv:2102.09026 , 2021a.
Jiaqi Gu, Chenghao Feng, Zheng Zhao, Zhoufeng Ying, Ray T Chen, and David Z Pan. Efficient on-
chip learning for optical neural networks through power-aware sparse zeroth-order optimization.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 7583â€“7591,
2021b.
Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Zixuan Jiang, Ray Chen, and David Pan. L2ight: Enabling
on-chip learning for optical neural networks via efficient in-situ subspace optimization. Advances
in Neural Information Processing Systems , 34:8649â€“8661, 2021c.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.
Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv preprint
arXiv:2212.13345 , 2022.
Thomas A Hogan and Bhavya Kailkhura. Universal decision-based black-box perturbations: Break-
ing security-through-obscurity defenses. arXiv preprint arXiv:1811.03733 , 2018.
Philipp Holl, Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differentiable physics,
2020.
Yuanming Hu, Jiancheng Liu, Andrew Spielberg, Joshua B Tenenbaum, William T Freeman, Jia-
jun Wu, Daniela Rus, and Wojciech Matusik. Chainqueen: A real-time differentiable physical
simulator for soft robotics. In ICRA , pp. 6265â€“6271. IEEE, 2019.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In International conference on machine learning , pp. 2137â€“
2146. PMLR, 2018a.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. arXiv preprint arXiv:1807.07978 , 2018b.
Takuya Isomura and Taro Toyoizumi. Error-gated hebbian rule: A local learning rule for principal
and independent component analysis. Scientific reports , 8(1):1835, 2018.
Marwan Jabri and Barry Flower. Weight perturbation: An optimal architecture and learning tech-
nique for analog vlsi feedforward and recurrent multilayer networks. IEEE Transactions on Neu-
ral Networks , 3(1):154â€“157, 1992.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In Inter-
national conference on machine learning , pp. 1627â€“1635. PMLR, 2017.
Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function.
The Annals of Mathematical Statistics , pp. 462â€“466, 1952.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Jeffrey Larson, Matt Menickelly, and Stefan M Wild. Derivative-free optimization methods. Acta
Numerica , 28:287â€“404, 2019.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2024
X. Lian, H. Zhang, C.-J. Hsieh, Y . Huang, and J. Liu. A comprehensive linear speedup analysis
for asynchronous stochastic parallel optimization from zeroth-order to first-order. In Advances in
Neural Information Processing Systems , pp. 3054â€“3062, 2016.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random feedback
weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247 , 2014.
Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred Hero. Zeroth-order online alternating direction method
of multipliers: Convergence analysis and applications. In International Conference on Artificial
Intelligence and Statistics , pp. 288â€“297. PMLR, 2018a.
Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-
order stochastic variance reduction for nonconvex optimization. Advances in Neural Information
Processing Systems , 31, 2018b.
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K
Varshney. A primer on zeroth-order optimization in signal processing and machine learning:
Principals, recent advances, and applications. IEEE Signal Processing Magazine , 37(5):43â€“54,
2020a.
Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Mingyi Hong, and
Una-May Oâ€™Reilly. Min-max optimization without gradients: Convergence and applications to
black-box evasion and poisoning attacks. In International conference on machine learning , pp.
6282â€“6293. PMLR, 2020b.
Gilles Louppe, Joeri Hermans, and Kyle Cranmer. Adversarial variational optimization of non-
differentiable simulators. In The 22nd International Conference on Artificial Intelligence and
Statistics , pp. 1438â€“1447. PMLR, 2019.
Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Ming-
hai Qin, Sijia Liu, Zhangyang Wang, et al. Sanity checks for lottery tickets: Does your winning
ticket really win the jackpot? Advances in Neural Information Processing Systems , 34:12749â€“
12760, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev
Arora. Fine-tuning language models with just forward passes. arXiv preprint arXiv:2305.17333 ,
2023.
Florian Meier, Asier Mujika, Marcelo Matheus Gauy, and Angelika Steger. Improving gradient esti-
mation in evolutionary strategies with past descent directions. arXiv preprint arXiv:1910.05268 ,
2019.
Timoleon Moraitis, Dmitry Toichkin, Adrien Journ Â´e, Yansong Chua, and Qinghai Guo. Softhebb:
Bayesian inference in unsupervised hebbian soft winner-take-all networks. Neuromorphic Com-
puting and Engineering , 2(4):044017, 2022.
John A Nelder and Roger Mead. A simplex method for function minimization. The computer
journal , 7(4):308â€“313, 1965.
Y . Nesterov and V . Spokoiny. Random gradient-free minimization of convex functions. Foundations
of Computational Mathematics , 2(17):527â€“566, 2015.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics , 17:527â€“566, 2017.
Arild NÃ¸kland. Direct feedback alignment provides learning in deep neural networks. Advances in
neural information processing systems , 29, 2016.
Arild NÃ¸kland and Lars Hiller Eidnes. Training neural networks with local error signals. In Inter-
national conference on machine learning , pp. 4839â€“4850. PMLR, 2019.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2024
Mayumi Ohta, Nathaniel Berger, Artem Sokolov, and Stefan Riezler. Sparse perturbations for im-
proved convergence in stochastic zeroth-order optimization. In Machine Learning, Optimization,
and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19â€“23, 2020, Re-
vised Selected Papers, Part II 6 , pp. 39â€“64. Springer, 2020.
Mengye Ren, Simon Kornblith, Renjie Liao, and Geoffrey Hinton. Scaling forward gradient with
local losses. arXiv preprint arXiv:2210.03310 , 2022.
L. M. Rios and N. V . Sahinidis. Derivative-free optimization: a review of algorithms and comparison
of software implementations. Journal of Global Optimization , 56(3):1247â€“1293, 2013.
David E Rumelhart, Richard Durbin, Richard Golden, and Yves Chauvin. Backpropagation: The
basic theory. Backpropagation: Theory, architectures and applications , pp. 1â€“34, 1995.
Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A
provable defense for pretrained classifiers. Advances in Neural Information Processing Systems ,
33:21945â€“21957, 2020.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proceedings of the IEEE , 104(1):
148â€“175, 2015.
O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In
Conference on Learning Theory , pp. 3â€“24, 2013.
Yao Shu, Zhongxiang Dai, Weicong Sng, Arun Verma, Patrick Jaillet, and Bryan Kian Hsiang Low.
Zeroth-order optimization with trajectory-informed derivative estimation. In ICLR , 2022.
David Silver, Anirudh Goyal, Ivo Danihelka, Matteo Hessel, and Hado van Hasselt. Learning by
directional gradient descent. In International Conference on Learning Representations , 2021.
James C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient
approximation. IEEE transactions on automatic control , 37(3):332â€“341, 1992.
Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee.
Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural
Information Processing Systems , 33:20390â€“20401, 2020.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning
for language-model-as-a-service. In International Conference on Machine Learning , pp. 20841â€“
20855. PMLR, 2022.
Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. Advances in neural information pro-
cessing systems , 33:6377â€“6389, 2020.
Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timoth Â´ee Masquelier, and
Anthony Maida. Deep learning in spiking neural networks. Neural networks , 111:47â€“63, 2019.
Adam Thelen, Xiaoge Zhang, Olga Fink, Yan Lu, Sayan Ghosh, Byeng D Youn, Michael D Todd,
Sankaran Mahadevan, Chao Hu, and Zhen Hu. A comprehensive review of digital twinâ€”part 1:
modeling and twinning enabling technologies. Structural and Multidisciplinary Optimization , 65
(12):354, 2022.
Virginia Torczon. On the convergence of the multidirectional search algorithm. SIAM journal on
Optimization , 1(1):123â€“145, 1991.
Yun-Yun Tsai, Pin-Yu Chen, and Tsung-Yi Ho. Transfer learning without knowing: Reprogramming
black-box machine learning models with scarce data and limited resources. In International
Conference on Machine Learning , pp. 9614â€“9624. PMLR, 2020.
Ioannis Tsaknakis, Bhavya Kailkhura, Sijia Liu, Donald Loveland, James Diffenderfer, Anna Maria
Hiszpanski, and Mingyi Hong. Zeroth-order sciml: Non-intrusive integration of scientific soft-
ware with deep learning. arXiv preprint arXiv:2206.02785 , 2022.
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2024
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attack-
ing black-box neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence ,
pp. 742â€“749, 2019.
Kiwon Um, Robert Brand, Yun Raymond Fei, Philipp Holl, and Nils Thuerey. Solver-in-the-loop:
Learning from differentiable physics to interact with iterative pde-solvers. NeurIPS , 33:6111â€“
6122, 2020.
Jean-Baptiste Hiriart Urruty and Claude Lemar Â´echal. Convex analysis and minimization algorithms .
Springer-Verlag, 1993.
A Ismael F Vaz and Lu Â´Ä±s Nunes Vicente. Pswarm: a hybrid solver for linearly constrained global
derivative-free optimization. Optimization Methods & Software , 24(4-5):669â€“685, 2009.
Anirudh Vemula, Wen Sun, and J Bagnell. Contrasting exploration in parameter and action space:
A zeroth-order optimization perspective. In The 22nd International Conference on Artificial In-
telligence and Statistics , pp. 2926â€“2935. PMLR, 2019.
Astha Verma, Siddhesh Bangar, A V Subramanyam, Naman Lal, Rajiv Ratn Shah, and Shinâ€™ichi
Satoh. Certified zeroth-order black-box defense with robust unet denoiser. arXiv preprint
arXiv:2304.06430 , 2023.
Paul Vicol, Zico Kolter, and Kevin Swersky. Low-variance gradient estimation in unrolled compu-
tation graphs with es-single. arXiv preprint arXiv:2304.11153 , 2023.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. arXiv preprint arXiv:2002.07376 , 2020.
Xiaoxing Wang, Wenxuan Guo, Jianlin Su, Xiaokang Yang, and Junchi Yan. Zarts: On zero-order
optimization for neural architecture search. Advances in Neural Information Processing Systems ,
35:12868â€“12880, 2022.
Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order opti-
mization in high dimensions. arXiv preprint arXiv:1710.10551 , 2017.
Stephen Wright, Jorge Nocedal, et al. Numerical optimization. Springer Science , 35(67-68):7, 1999.
Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li, and Tong Zhang. Hessian-aware zeroth-
order optimization for black-box adversarial attack. arXiv preprint arXiv:1812.11377 , 2018.
Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in
minutes. In Proceedings of the 47th International Conference on Parallel Processing , pp. 1â€“10,
2018.
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:
Residual learning of deep cnn for image denoising. IEEE transactions on image processing , 26
(7):3142â€“3155, 2017.
Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang,
and Sijia Liu. Advancing model pruning via bi-level optimization. In Advances in Neural Infor-
mation Processing Systems , 2022a.
Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu
Chen, Jason D Lee, Wotao Yin, Mingyi Hong, et al. Revisiting zeroth-order optimization for
memory-efficient llm fine-tuning: A benchmark. arXiv preprint arXiv:2402.11592 , 2024.
Yimeng Zhang, Akshay Karkal Kamath, Qiucheng Wu, Zhiwen Fan, Wuyang Chen, Zhangyang
Wang, Shiyu Chang, Sijia Liu, and Cong Hao. Data-model-hardware tri-design for energy-
efficient video intelligence. arXiv preprint arXiv:2210.08578 , 2022b.
Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jinfeng Yi, Mingyi Hong, Shiyu Chang, and Sijia Liu.
How to robustify black-box ml models? a zeroth-order optimization perspective. ICLR , 2022c.
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2024
Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, and Xue Lin. On the
design of black-box adversarial examples by leveraging gradient-free optimization and operator
splitting method. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pp. 121â€“130, 2019.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2024
APPENDIX
A R EMARK ON CONVERGENCE RATE .
For a rigorous convergence analysis, we can perceive the sparsity-enhanced ZO training method in-
troduced above as a specific instantiation within the broader framework of ZO stochastic coordinate
descent (Lian et al., 2016). Thus, under necessary theoretical assumptions regarding Lipschitz con-
tinuity, gradient smoothness, and bounded gradient variance, we can obtain an upper bound for the
convergence rate of our proposal in terms of the the gradient norm, serving as a stationarity-based
convergence measure for non-convex optimization:
PK
k=0E|âˆ‡f(Î¸k)|2
Kâ‰¤O 
1
K(1âˆ’p)+s
1
K(1âˆ’p)Ïƒ!
, (A1)
where krepresents the iteration index, Ksignifies the total number of iterations, Ois used in the
context of big O notation, pdenotes the gradient sparsity ratio, and Ïƒstands for the upper bound on
the variance of the stochastic gradients. The above highlights that as pincreases, the convergence
error of the proposed approach also grows. However, itâ€™s important to note that increased sparsity
leads to a reduced number of function queries. Consequently, we can see a provable tradeoff between
the convergence error and the query efficiency, aligning well with our understanding.
B T HESIMPLE CNN A RCHITECTURE CONSIDERED FOR TRAINING W / CGE
VS. RGE
ConvBNReLUMaxPoolingConv BlockConv Block1CNN of a varying depthConv Block2Conv BlocknAvgPoolingFCâ€¦â€¦.
Figure A1: Illustration of the simple CNN considered with different depths.
As illustrated in Fig. A1 , the depth-adjustable simple CNN architecture comprises multiple conv
blocks, an average pooling layer, and a fully-connected layer. Each conv block consists of four
distinct layers, including â‘ convolutional layer, â‘¡batch normalization layer, â‘¢ReLU layer, â‘£max
pooling layer. In our experiments, the simple CNN networks are trained over a course of 50 epochs
utilizing three distinct optimization strategies: FO (first-order) SGD (stochastic gradient descent)
training, ZO RGE-based SGD training, and ZO CGE-based SGD training, together with a cosine
learning rate scheduler is used.
C C OMPUTATION TIMECOMPARISON BETWEEN RGE AND CGE
730 3700 6670 9640 12610
Number of Model Parameters050100150200Training Time (hours)
1.663.02
12.0827.18
28.0366.12
45.22144.92
70.32206.28ZO CGE
ZO RGE
Figure A2: The computation cost of
using CGE and RGE to train the sim-
ple CNN in Fig. 2 over 50 epochs on
CIFAR-10 against parameter size.We find that CGE also has a computatoin efficiency merit over
RGE when training CNNs in Fig. 2 .Fig. A2 presents the com-
putation time of different training methods against the model
size and highlights that CGE is more computationally effi-
cient than RGE. To examine the efficiency merit of CGE in
detail, we dissect the computation of a single ZO gradient es-
timate into four stages :â‘ generation of directional vectors,
i.e.,uioreiin (1), â‘¡perturbation of model weights, i.e.,
Î¸+ÂµuiorÎ¸+Âµei,â‘¢model inference for calculating fi-
nite differences, and â‘£other arithmetic operations for gradi-
ent estimation. Our results show that RGE takes much longer
than CGE in stages â‘ andâ‘¡. This is because, at each model
query, RGE needs to generate a directional perturbation vec-
tor with the same dimension as Î¸and apply this perturbation
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2024
toÎ¸. By contrast, CGE can generate the basis vector for â€˜freeâ€™ and perturb the model weights using
a coordinate-wise indexing operation. Tab. A1 shows a comparison of RGE and CGE in â‘ -â‘£. It
is also worth noting that a concurrent study (Malladi et al., 2023) argues that language model fine-
tuning using RGE only necessitates the query budget q= 1. However, it is important to highlight
that their approach has certain restrictions, as it relies on having a high-quality pretraining model
and a meticulously crafted prompt.
Table A1: Average per-iteration computation
time comparison (seconds) between RGE and CGE.
Here the gradient estimation process is dissected
into 4 stages: â‘ generation of d irectional v ectors
(DV),â‘¡model w eights p erturbation (WP), â‘¢
model i nference (MI) for calculating finite differ-
ences, and â‘£other a rithmetic o perations (AO) for
gradient estimation.
Parameter #RGE ( q=d) / CGE
(d) â‘ DV â‘¡WP â‘¢MI â‘£AO
730 0.173 / 0 0.152 / 0.026 0.202 / 0.21 0.149 / 0.0312
3700 1.43 / 0 1.2 / 0.0907 2.11 / 1.98 1.1 / 0.124
6670 3.49 / 0 2.93 / 0.159 4.84 / 4.53 2.68 / 0.225
9640 6.74 / 0 5.47 / 0.246 8.3 / 8.09 4.97 / 0.336
12610 10.2 / 0 8.86 / 0.457 13.1 / 14.6 8.05 / 0.566Tab. A1 provides a comparison of the average
computation time per iteration for RGE and CGE-
based ZO training across four stages: â‘ genera-
tion of directional vectors, i.e.,uioreiin (1),â‘¡
perturbation of model weights, i.e.,Î¸+Âµuior
Î¸+Âµei,â‘¢model inference for calculating finite
differences, and â‘£other arithmetic operations for
gradient estimation. The comparison is conducted
for training models of different numbers of param-
eters ( d), ranging from 730to12610 . In stage â‘ ,
RGE incurs computation time due to the genera-
tion of random Gaussian vectors, whereas CGE is
free of this step since it estimates gradients in a
coordinate-wise manner. In stage â‘¡, RGE requires
more computation time compared to CGE across
all parameter sizes. This is because RGE perturbs
all model weights in a single query, while CGE only perturbs a single model weight coordinate per
query. In stage â‘¢, the computation times for both ZO algorithms increase with the number of param-
eters, with similar time consumption. In stage â‘£, RGE continues to require more computation time
than CGE for all parameter sizes. This is because RGE needs to perform extra arithmetic operations
(such as the sum operation) to aggregate multiple queries, whereas CGE handles multiple queries
coordinate-wise, resulting in a more efficient computation.
D P ERFORMANCE OF MODEL PRUNING VIA ZO-G RASP
We implement ZO-GraSP using RGE with the query number q= 192 (smaller than the model
sized) and compare its performance with random pruning and FO-GraSP. All GraSP variants
are performed over randomly initialized model parameters ( Î¸).Tab. A2 andTab. A3 present the
pruning performance vs. model pruning ratios in the data-model setup of (CIFAR-10, ResNet-20)
and (CIFAR-10, ResNet-18), respectively. Here the pruning performance is evaluated by measuring
the testing accuracy of a sparse model on CIFAR-10. To assess the impact of model pruning, we
utilize FO SGD as the optimizing method for sparse model training. As we can see, our proposed
ZO-GraSP method demonstrates comparable testing accuracy to FO-GraSP across various pruning
ratios and significantly outperforms random pruning.
Table A2: Performance comparison of different model pruning methods on (CIFAR-10, ResNet-20).
(CIFAR-10, ResNet-20)
Pruning ratio 10% 20% 30% 40% 50% 60% 70% 80% 90% 95%
Random 92.71Â±0.18 92.63 Â±0.04 92.40 Â±0.14 91.94 Â±0.03 91.77 Â±0.16 91.41 Â±0.20 90.57 Â±0.05 89.35 Â±0.19 86.55 Â±0.15 82.51 Â±0.16
FO-GraSP 92.64Â±0.10 92.44 Â±0.06 92.34 Â±0.11 92.35 Â±0.14 92.07 Â±0.11 92.00 Â±0.03 91.63 Â±0.16 90.75 Â±0.07 88.55 Â±0.09 85.48 Â±0.26
ZO-GraSP 92.74Â±0.07 92.56 Â±0.10 92.58 Â±0.20 92.46 Â±0.05 92.09 Â±0.10 91.61 Â±0.13 91.48 Â±0.23 90.21 Â±0.16 88.08 Â±0.09 84.80 Â±0.10
Table A3: Performance comparison of different model pruning methods on (CIFAR-10, ResNet-18).
(CIFAR-10, ResNet-18)
Pruning ratio 10% 20% 30% 40% 50% 60% 70% 80% 90% 95%
Random 95.45Â±0.06 95.51 Â±0.11 95.32 Â±0.15 95.34 Â±0.10 95.40 Â±0.20 95.09 Â±0.13 94.92 Â±0.14 94.58 Â±0.06 93.53 Â±0.10 92.13 Â±0.26
FO-GraSP 95.44Â±0.07 95.46 Â±0.30 95.53 Â±0.06 95.49 Â±0.05 95.44 Â±0.08 95.86 Â±0.09 95.99 Â±0.17 95.97 Â±0.05 95.91 Â±0.03 96.19 Â±0.10
ZO-GraSP 95.43Â±0.05 95.53 Â±0.03 95.53 Â±0.14 95.56 Â±0.18 95.50 Â±0.06 95.24 Â±0.09 95.36 Â±0.05 95.28 Â±0.05 94.94 Â±0.08 94.39 Â±0.13
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2024
E A LGORITHM DETAILS
Algorithm 1 ZO-GraSP-oriented-LPR-guided ZO training
1:GetSZO-GraSP through ZO-GraSP (3)
2:Obtain layer-wise pruning ratio Slayerbased on SZO-GraSP
3:forEpoch t= 0,1,2, . . . , T âˆ’1do
4: Randomly generate a sparse coordinate set Staccording to Slayer
5: forIterations per epoch do
6: Obtain (Sparse-CGE) Ë†âˆ‡Î¸â„“(Î¸)based on St
7: Update model weights: Î¸â†Î¸âˆ’Î±Ë†âˆ‡Î¸â„“(Î¸)
8: end for
9:end for
ZO-GraSP-oriented-LPR-guided ZO training Algorithm 1 shows the algorithmic steps to ful-
fill the ZO-GraSP-oriented-LPR-guided ZO training. At initialization, the algorithm acquires the
coordinate set of unpruned model weights SZO-GraSP by applying our proposed ZO-GraSP as shown
in (3). Subsequently, it computes the layer-wise pruning ratios, denoted as Slayer, derived from
SZO-GraSP . The (Sparse-CGE) Ë†âˆ‡Î¸â„“(Î¸)is then estimated using Slayer. The model weights are then up-
dated by subtracting the estimated sparse gradient to the current weight Î¸. This process iteratively
refines the weights for optimizing the modelâ€™s performance.
Figure A3: Performance (test-
ing accuracy) of SR-guided training
vs. pruning ratios on (CIFAR-10,
ResNet20).Fig. A3 demonstrates the superiority of SR-guided training to
another two alternative methods, M1built on alternative op-
timization between ZO-GraSP and sparse training, and M2
based on pruning before model training. To examine the ef-
fectiveness of SR in integrating ZO-GraSP with model train-
ing, we compare the pruning performance in the FO train-
ing regime ( i.e., the resulting sparse model is trained using
FO methods). As we can see, SR-guided training consis-
tently outperforms methods M1andM2, with the accuracy
improvement becoming more pronounced as the pruning ra-
tio increases. As the pruning ratio increases, the performance
gap with the original dense model also widens. However, our
method achieves the smallest performance gap compared to
the FO baseline.
DeepZero framework. We introduce the DeepZero frame-
work, a novel approach designed to train models using ZO optimization. This framework assimilates
three key features: the implementation of ZO-GraSP-oriented-LPR-guided ZO training, the reuse of
features, and the adoption of parallel computation techniques. These components of the DeepZero
framework are expounded in Algorithm 2 .
Algorithm 2 DeepZero Framework
1:Initialize: Total epochs T, sparsity update interval Ksparse , LPRs Rvia ZO-GraSP (3)
2:forEpoch t= 0,1,2, . . . , T âˆ’1do
3: iftmodKsparse == 0 then â–·Sparsity inducing
4: Update sparse coordinate index set Staccording to R
5: end if
6: forProcess i= 1,2, . . . , M do â–· M processes
7: Based on St, parallelized forward evaluations with feature reuse (4)-(5)
8: end for
9: (Sparse-CGE) estimation using above model evaluations
10: Model weights updating and synchronization
11:end for
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2024
F A DDITIONAL EXPERIMENTS OF IMAGE CLASSIFICATION
Fig. A4 presents the training trajectory comparison between DeepZero and the dense FO training
baseline. We observe that despite the marginal performance drop, DeepZero achieves a competitive
convergence rate as the FO counterpart. Besides, it is important to highlight that due to the small
variance of CGE on a reduced dimension (parameters after pruning), DeepZero achieves competi-
tively stable training as the FO baseline, as indicated by similar standard deviations.
Fig. A5 presents the performance and the time consumption of DeepZero in various batch size set-
tings. The impact of batch size on model performance has been well-documented in FO training,
with larger batch sizes often leading to decreased performance. We sought to examine the effect of
batch size selection on ZO training and determine whether increasing the size could reduce training
time. As depicted in Fig. A5 , we observed a marked decline in performance when the batch size
exceeded 2048. Additionally, when the batch size was set to 512, it reached a constant time con-
sumption of 60 minutes per epoch, rendering any further reduction in training time infeasible due to
the limitations of GPU computational capacity. We stress that reaching this constant time consump-
tion signs the full utilization of each GPU, which is easily achieved by forward parallelization.
Fig. A6 presents DeepZeroâ€™s training speed (iterations per hour) on CIFAR-10 and ResNet-20 when
using different counts of GPUs. We observe training speed scaling linearly with regard to the GPU
counts, which justifies the efficiency of acceleration by forward pass and feature reuse.
0 10 20 30 40 50
Epoch20406080100Accuracy
0.00.51.01.52.0LossDeepZero Accuracy
FO Accuracy
DeepZero Loss
FO Loss
Figure A4: Training trajec-
tory of DeepZero and FO train-
ing on (CIFAR-10, ResNet-20).
DeepZero adopts Sparse-CGE of
90% sparsity. The mean and stan-
dard deviation of 3 independent
runs are reported.
128 256 512 1024 2048 4096 8192
Batch Size5060708090Testing Accuracy (%)
50100150200250300Time per Epoch (min)
170
86
60 60 60 60 6085.0486.24 86.4685.8885.69
77.74
58.22Testing Accuracy Time Per EpochFigure A5: Accuracy performance
and computation time of DeepZero on
(CIFAR-10, ResNet-20) against dif-
ferent batch sizes. Other settings are
consistent with Fig. A4.
2 4 6 8
GPU counts50100150200250Iterations per hour
Figure A6: The training speed
of DeepZero on (CIFAR-10,
ResNet-20) against GPU counts.
Experiments are conducted under
NVIDIA TESLA V100 16G GPU
resources.
G A DDITIONAL ILLUSTRATION OF BLACK -BOX DEFENSE AGAINST
ADVERSARIAL ATTACKS
!Trainable                         
â„FixedWhite-boxDefense OperationEncoderDecoderBlack-boxClassifierBlack-boxClassifierOurs:Existing:
!
â„
!
!
â„
â„New Defense OperationNew Black-box ModuleFOZOChain RuleWhite-boxDefense OperationDeepZeroOptimizer
Figure A7: Schematic overview of base-
line ZO-AE-DS (Zhang et al., 2022c) and
DeepZero-enabled black-box defense.InFig. A7 , we compare our method, DeepZero, with the
baseline method ZO-AE-DS (Zhang et al., 2022c). ZO-AE-
DS utilizes an autoencoder (AE) to reduce the dimensional-
ity of the ZO gradient estimation by merging the AEâ€™s de-
coder with the black-box classifier and integrating the AEâ€™s
encoder with the white-box defense operation. However,
ZO-AE-DS suffers from poor scalability to high-resolution
datasets like ImageNet due to the use of AE, which compro-
mises the fidelity of the image input to the black-box classi-
fier. In contrast, DeepZero directly optimizes the white-box
defense operation without the need for an AE, as demon-
strated in Fig. A7.
Following (Salman et al., 2020; Zhang et al., 2022c), we
model the defense operation as a denoiser given by DnCNN
(Zhang et al., 2017). To optimize it for black-box defense,
we adopt the â€˜pretraining + fine-tuningâ€™ approach, as de-
tailed in (Zhang et al., 2022c). We first employ the Adam
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2024
optimizer (Kingma & Ba, 2014) to pretrain the denoiser over 90 epochs, based on the mean squared
error (MSE) loss function. The pre-training stage excludes the optimization with the black-box im-
age classifier. After the pretraining phase, we apply ZO-GraSP to the pretrained denoiser, with the
pruning ratio of 95%. Notably, the percentage of unpruned parameters of the denoiser tends to be
almost zero for all layers, except the first and the last layers. With this insight in mind, we specify the
finetuning strategy as partial finetuning, which targets only the last layer of the denoiser. However,
we still need to address the black-box optimization challenge, provided by the black-box classifier
f(x). LetÎ¸represent the denoiser to be finetuned, DeepZero then solves the following black-box
optimization problem:
minimize
Î¸EÎ´,x[â„“CE(f(DÎ¸(x+Î´)), f(x))], (A2)
where xsignifies the input, â„“CErefers to the Cross-Entropy (CE) loss, and Î´âˆˆ N(0, Ïƒ2I)represents
the standard Gaussian noise with variance Ïƒ2. We apply DeepZero to solve problem (A2) under 20
training epochs, and use a learning rate of 10âˆ’5, which is reduced by a factor of 10every four
epochs.
H S IMULATION -COUPLED DL FOR DISCRETIZED PDE E RROR CORRECTION
ZO-SOL:Solver-in-the-looptrainingviaDeepZeroCorrectiveNN
Simulator w/o Autodiff
Simulator w/o Autodiff
CorrectiveNNBPNON:Non-interactivetrainingoutofthesimulationloopDataâ„“(ðœ½)â„“(ðœ½)
DeepZero
Figure A8: Schematic overview of the base-
line approach NON given by training over pre-
generated simulation data and ZO-SOL by lever-
aging DeepZero to address the solver-in-the-loop
training challenge.The feasibility of training a corrective NN through
looping interactions with the iterative partial differ-
ential equation (PDE) solver, coined â€˜solver-in-the-
loopâ€™ (SOL), has been demonstrated in (Um et al.,
2020). By leveraging DeepZero, we can enable the
use of SOL with non-differentiable, or black-box,
simulators Hu et al. (2019); Fang et al. (2022). We
name our method ZO-SOL .Fig. A8 presents a com-
parison with the baseline method NON, given by
non-interactive training out of the simulation loop
using pre-generated low and high-fidelity simulation
data.
In our experiments, we consider an unsteady wake
flow simulationâ€”a standard benchmark case for
fluid dynamics (Um et al., 2020)â€”to assess the effi-
cacy of our method. The corrective neural network,
adopted from (Um et al., 2020), is a sequential con-
volutional network consisting of two convolutional layers with 5Ã—5kernels, totaling 56,898 param-
eters. The SOL problem can then be formulated as the optimization problem
arg min
Î¸PTâˆ’n
t=0Pnâˆ’1
i=0âˆ¥Ps(Ëœst+i) +CÎ¸(Ps(Ëœst+i))âˆ’yt+i+1âˆ¥2, (A3)
where Psdenotes the PDE solver used for the low-fidelity simulation, CÎ¸signifies the corrective
neural network parameterized by Î¸,yt+istands for the high-precision simulation state (regarded as
the ground truth) at time t+i,Tis the number of timesteps for the ground truth simulation, nis
the number of unrolling steps used in SOL, and Ëœst+isignifies the output from SOL at the ith step,
namely,
Ëœst+i= [(1+CÎ¸)â—¦ Ps]â—¦[(1+CÎ¸)â—¦ Ps]â—¦ Â·Â·Â· â—¦ [(1+CÎ¸)â—¦ Ps]| {z }
itimes(yt),
where â—¦is the function composition operation, 1is an identity function and (1+CÎ¸)stands for a
function expressed as (1(Â·) +CÎ¸(Â·)). Note that in our experiments, we take n= 16 during training,
that is we have 16 unrolling steps.
For the unsteady wake flow simulation, we follow the details outlined in (Um et al., 2020) which
we include here for completeness. Namely, the unsteady wake flow simulation solves a discretized
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2024
version the the incompressible Navier-Stokes equations given by:
âˆ‚ux
âˆ‚t+uâˆ‡ux=âˆ’1
Ïâˆ‡p+Î½âˆ‡ Â· âˆ‡ ux (A4)
âˆ‚uy
âˆ‚t+uâˆ‡uy=âˆ’1
Ïâˆ‡p+Î½âˆ‡ Â· âˆ‡ uy (A5)
âˆ‡ Â·u= 0, (A6)
where tis time, Ïis density, Î½is viscosity, pis pressure, u= (ux, uy)Tis velocity ( xandydirec-
tions), and âˆ‡Â·denotes the divergence operator. The domain is â„¦ = [0 ,1]Ã—[0,2]with open boundary
conditions and an initial state of u= (0,1)Talong xâˆˆ[0,1]andy= 0. Additionally, there is a
circular rod of diameter 0.1at position (0.5,0.5)Tâˆˆâ„¦. The domain â„¦for the low and high fidelity
simulations is discretized using a staggered grid of dimensions [32,64]and[64,128], respectively.
Additionally, we take T= 500 timesteps. We utilized the PhiFlow (Holl et al., 2020) simulation
code as the solver for our experiments and we based our implementation off of the solver in the loop
training process from the code https://github.com/tum-pbs/Solver-in-the-Loop .
For simplicity, the loss in (A3) is only expressed for a single simulation with Ttimesteps. In our
experiments, we utilize 6 different simulations in the training dataset each of which have different
Reynolds numbers (which affects fluid flow turbulence and can be altered by varying Î½in (A4)
and (A5)) in the set {97.7,195.3,390.6,781.3,1562.5,3125.0}, consistent with (Um et al., 2020).
Hence, during training the loss function has an additional outer summation to account for the 6
different training simulations. The test set consists of five different simulations computed using
Reynolds numbers in the set {146.5,293.0,585.9,1171.9,2343.8}, also consistent with (Um et al.,
2020).
To solve problem (A3), DeepZero under the Adam optimization framework is used with a learning
rate of 10âˆ’4over ten training epochs.
I B ROADER IMPACTS AND LIMITATIONS
Broader Impacts. Our proposed ZO learning for deep model training offers a valuable solution
for various ML problems that involve interactions with black-box APIs, such as language models as
a service, and on-chip learning problems where gradient calculation is infeasible on resource-limited
hardware. The applications of DeepZero and its black-box approaches explored in this work can also
contribute to advancements in optimization theory and model pruning in other fields. The insights
gained from studying DeepZeroâ€™s scalability, efficiency, and effectiveness can have far-reaching
implications beyond the realm of deep learning.
Limitations. One limitation of our approaches is the high number of model queries required,
which is inherent to ZO optimization in general. Improving query efficiency is an important area
for future research and compute-efficient techniques from (Bartoldson et al., 2023) likely will be
helpful. Additionally, the infusion of sparse deep priors in ZO training may not be suitable for non-
deep learning models. Therefore, it remains crucial to develop more generic and model-agnostic ZO
training methods that can handle large-scale and extreme-scale problems effectively.
22
