# 2205.11200.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/no-backprop/2205.11200.pdf
# File size: 1361291 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
BBTv2: Towards a Gradient-Free Future with Large Language Models
Tianxiang Sun}~Zhengfu He}Hong Qian
Yunhua Zhou}~Xuanjing Huang}~Xipeng Qiu}~
}School of Computer Science, Fudan University
~Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science and Technology, East China Normal University
{txsun19,zfhe19,zhouyh20,xjhuang,xpqiu}@fudan.edu.cn hqian@cs.ecnu.edu.cn
Abstract
Most downstream adaptation methods tune all
or part of the parameters of pre-trained models
(PTMs) through gradient descent, where the
tuning cost increases linearly with the growth
of the model size. By contrast, gradient-free
methods only require the forward computa-
tion of the PTM to tune the prompt, retain-
ing the beneﬁts of efﬁcient tuning and deploy-
ment. Though, past work on gradient-free tun-
ing often introduces gradient descent to seek a
good initialization of prompt and lacks versa-
tility across tasks and PTMs. In this paper, we
present BBTv2, an improved version of Black-
Box Tuning (Sun et al., 2022b), to drive PTMs
for few-shot learning. We prepend continuous
prompts to every layer of the PTM and pro-
pose a divide-and-conquer gradient-free algo-
rithm to optimize the prompts at different lay-
ers alternately. Extensive experiments across
various tasks and PTMs show that BBTv2 can
achieve comparable performance to full model
tuning and state-of-the-art parameter-efﬁcient
methods (e.g., Adapter, LoRA, BitFit, etc.) un-
der few-shot settings while maintaining much
fewer tunable parameters.
1 Introduction
The past few years have witnessed remarkable
progress of large language models (LLMs) (Devlin
et al., 2019; Raffel et al., 2020; Brown et al., 2020).
It has been repeatedly demonstrated that scaling
up the model size is promising to achieve better
performance. However, the growing model size
also leads to a linear increase in tuning cost. Fine-
tuning and deploying a separate copy of the LLM
for each downstream task become prohibitively ex-
pensive. To that end, much effort has been devoted
toparameter-efﬁcient tuning (PET) (He et al., 2021;
Ding et al., 2022), which only tunes a small portion
of parameters while keeping most of the parame-
ters of the LLM unchanged. By PET, LLMs can
Corresponding author.
1K 10K 100K 1M 10M 100M 1B
Tunable Parameters60.062.565.067.570.072.575.077.580.0Average PerformanceBBTBBTv2
Prompt TuningBitFitLoRA
P-Tuning v2AdapterModel TuningFigure 1: BBTv2 achieves comparable results to
gradient-based methods on average performance over 7
language understanding tasks (§5.1) with much fewer
tunable parameters. Size of the circle is proportional
to the standard deviation of the performance. All the
methods are evaluated on RoBERTa LARGE .
be specialized to a downstream task at inference
time by activating a small number of task-speciﬁc
parameters. Though it is deployment-efﬁcient, tun-
ing the small portion of parameters still requires
back-propagation through the entire LLM, which is
expensive or even infeasible for many practitioners.
To make LLMs beneﬁt a wider range of au-
diences, a common practice is to release LLMs
as a service and allow users to access the pow-
erful LLMs through their inference APIs (Brown
et al., 2020). In such a scenario, called Languaged-
Model-as-a-Service (LMaaS) (Sun et al., 2022b),
users cannot access or tune model parameters but
can only tune their prompts to accomplish language
tasks of interest. Brown et al. (2020) propose to re-
cast downstream tasks as a language modeling task
and perform different tasks by conditioning on task-
speciﬁc text prompts. Further, they demonstrate
that LLMs exhibit an emergent ability of in-context
learning, i.e., LLMs can learn to perform tasks with
a few demonstrations provided in the input contextarXiv:2205.11200v2  [cs.CL]  14 Oct 2022

--- PAGE 2 ---
without updating parameters. Nevertheless, its per-
formance has been shown to highly depend on the
choice of the prompt or the demonstrations (Zhao
et al., 2021; Liu et al., 2022) and still lags far be-
hind full model tuning.
Recently, Sun et al. (2022b) proposed Black-Box
Tuning (BBT), which optimizes continuous prompt
by only accessing model inference APIs. In con-
trast to gradient-based tuning that requires expen-
sive back-propagation1, BBT only requires model
forward computation, which can be highly opti-
mized by acceleration frameworks such as ONNX
Runetime and TensorRT. In addition, the optimiza-
tion cost of BBT is decoupled from the scale of the
model. Instead, larger models can be more favor-
able to BBT due to the lower intrinsic dimensional-
ity (Aghajanyan et al., 2021). Despite its efﬁciency
superiority and comparable performance to gradi-
ent descent, our pilot experiments (§3) show that
BBT lacks versatility across tasks and PTMs.
In this paper, we present BBTv2, an improved
version of BBT, to address these issues. Instead
of injecting continuous prompt tokens merely in
the input layer, BBTv2 prepends prompts to hid-
den states at every layer of the PTM (termed as
deep prompts ), incorporating an order of magni-
tude more parameters to handle more difﬁcult tasks.
However, the increased number of parameters also
poses a challenge for high-dimensional derivative-
free optimization (DFO, Shahriari et al. (2016);
Qian and Yu (2021)). Fortunately, we show that the
forward computation of modern PTMs can be de-
composed into an additive form w.r.t. hidden states
of each layer thanks to the residual connections (He
et al., 2016). Hence, the optimization of the deep
prompts can be decomposed into multiple low-
dimensional sub-problems, each corresponding to
the optimization of prompt at one layer. Based
on this insight, we propose a divide-and-conquer
algorithm to alternately optimize prompt at each
layer. For the optimization at each layer, we main-
tain a random linear transformation that projects
the prompt parameters into a low-dimensional sub-
space and perform DFO in the generated subspace.
To generalize BBTv2 to a variety of PTMs, we
generate the random projections using normal dis-
tributions with PTM-related standard deviations.
Experimental results show that BBTv2 signif-
1The computation and storage cost of back-propagation
is proportional to the forward compute. More widely used
variants of gradient descent, such as Adam (Kingma and Ba,
2015), even require higher compute resources.icantly improves BBT on average performance
across 7 language understanding tasks. As shown
in Figure 1, BBTv2 achieves comparable perfor-
mance to full model tuning and state-of-the-art
PET methods including Adapter (Houlsby et al.,
2019), BitFit (Zaken et al., 2022), LoRA (Hu
et al., 2021), and P-Tuning v2 (Liu et al., 2021b)
while with much fewer tunable parameters. Code
is publicly available at https://github.com/
txsun1997/Black-Box-Tuning .
2 Black-Box Tuning
Black-Box Tuning (BBT) (Sun et al., 2022b) is a
derivative-free framework to drive PTMs for few-
shot learning. In particular, for a batch of training
data(X;Y ), we ﬁrst convert the texts Xwith some
pre-deﬁned templates (e.g., " It was [MASK] ") into
~X, and the labels Ywith a pre-deﬁned map into
label words ~Y(e.g., " great " and " terrible "). By
this, we can formulate various downstream tasks
into a general-purpose (masked) language model-
ing task and utilize the pre-trained (masked) lan-
guage modeling head to solve them. Assume the
PTM inference API ftakes a continuous prompt
pand a batch of converted texts ~Xas input, and
outputs the logits of the tokens of interest (e.g.,
the[MASK] token). BBT seeks to ﬁnd the optimal
prompt p?= arg min p2PL(f(p;~X);~Y), where
Pis the prompt space and Lis some loss function
such as cross entropy. The closed form and the
gradients of fare not accessible to BBT.
The prompt p2RDusually has tens of thou-
sands of dimensions, making it infeasible to be
optimized with derivative-free optimization (DFO)
algorithms. Hence, BBT adopts a random projec-
tionA2RDdto generate a low-dimensional
subspaceZ2Rdand performs optimization in the
generated subspace, i.e.,
z?= arg min
z2ZL(f(Az+p0;~X);~Y);(1)
where p0is the initial prompt embedding. If not us-
ing pre-trained prompt embedding, p0is the word
embeddings randomly drawn from the vocabulary.
BBT adopts the Covariance Matrix Adaptation
Evolution Strategy (CMA-ES) (Hansen and Oster-
meier, 2001; Hansen et al., 2003) to optimize Eq.(1)
and obtain the desired prompt p?=Az?. The ran-
dom projection Ais frozen during optimization.

--- PAGE 3 ---
MRPC SNLI RTE4050607080PerformanceModel Tuning BBT BBTv2Figure 2: Performance on three entailment tasks. We
report F1 score for MRPC and accuracy for SNLI and
RTE. Without pre-trained prompt embedding, BBTv2
can still match the performance of full model tuning on
entailment tasks under the 16-shot setting.
3 Pilot Experiments
3.1 Limitations of BBT Across Tasks
Unsatisfactory Performance on Entailment
Tasks. As demonstrated by Sun et al. (2022b),
BBT can outperform model tuning on entailment
tasks when using pre-trained prompt embedding for
initialization. However, pre-trained prompt embed-
ding is not always available for many languages
and PTMs. Without pre-trained prompt embed-
ding, BBT still lags far behind model tuning on
entailment tasks. In other words, BBT does not
completely get rid of the dependence on gradients
to exceed model tuning . In contrast, as depicted
in Figure 2, BBTv2 can match the performance
of model tuning on three entailment tasks, namely
MRPC (Dolan and Brockett, 2005), SNLI (Bow-
man et al., 2015), and RTE (Wang et al., 2019)
without using pre-trained prompt embedding.
Slow Convergence on Many-Label Classiﬁca-
tion Tasks. BBT suffers from slow convergence
rate when the number of labels becomes large. As
reported by Sun et al. (2022b), BBT cannot con-
verge within the budget of 8,000 API calls, which is
sufﬁcient for common tasks to converge, on DBPe-
dia (Zhang et al., 2015), a topic classiﬁcation task
with 14 labels. Figure 3 shows the cross entropy
loss and the training accuracy during optimization.
Compared with BBT, the proposed BBTv2 signiﬁ-
cantly accelerates convergence on DBPedia.
3.2 Limitations of BBT Across PTMs
Overﬁtting on Training Data. When switching
the backbone model from RoBERTa (Liu et al.,
2019) to other PTMs, we ﬁnd that BBT tends
to overﬁt training data. As shown in Figure 4,
the original BBT with BERT LARGE (Devlin et al.,
0 2000 4000 6000 8000
Number of API calls0.51.01.52.02.5Cross Entropy Loss
30507090
Training AccuracyBBT BBTv2Figure 3: Comparison of the convergence rates of BBT
and BBTv2 on DBPedia (14 classes).
0 5000
Number of API calls6080100AccuracyOriginal BBT
0 5000
Number of API calls80100Modified BBT
0 5000
Number of API calls80100BBTv2Train (best so far) Dev
(a) BERT LARGE
0 5000
Number of API calls5075100AccuracyOriginal BBT
0 5000
Number of API calls6080100Modified BBT
0 5000
Number of API calls6080100BBTv2
(b) BART LARGE
Figure 4: Accuracy on the 16-shot training and devel-
opment set of SST-2 with BERT LARGE and BART LARGE .
The original BBT tends to overﬁt training data. By us-
ing normal distributions with the standard deviations
calculated by Eq.(6) to generate random projections,
the modiﬁed BBT and BBTv2 can generalize well to
development sets.
2019) and BART LARGE (Lewis et al., 2020) can
achieve 100% accuracy on the SST-2 training set,
but achieves little improvement on the develop-
ment set. We conjecture that the random projection
adopted by the original BBT hinders its general-
ization. By generating random projections using
normal distributions with model-related standard
deviations (§4.2), our modiﬁed BBT and BBTv2
exhibit stronger generalization ability.
4 BBTv2
4.1 Deep Black-Box Tuning
Though BBT has achieved comparable perfor-
mance to model tuning on simple classiﬁcation

--- PAGE 4 ---
𝒛∈ℝ𝑑
Random
Projection
(𝑨∈ℝ𝐷×𝑑)
෥𝒑∈ℝ𝐷
𝒑𝟎∈ℝ𝐷𝒑∈ℝ𝐷EmbeddingHidden StatesHidden StatesHidden States…Hidden States(M)LM Head
Input:x1x2⋯xn
loss Predictions
Labels
𝒛𝟏∈ℝ𝑑𝒑𝟏∈ℝ𝐷Embedding(M)LM Head
Input:x1x2⋯xn
loss Predictions
Labels
Rand ProjHidden StatesHidden StatesHidden States…Hidden States
𝒑𝟐∈ℝ𝐷𝒑𝟑∈ℝ𝐷𝒑𝟒∈ℝ𝐷𝒑𝑳∈ℝ𝐷
𝒛𝟐∈ℝ𝑑Rand Proj𝒛𝟑∈ℝ𝑑Rand Proj𝒛𝟒∈ℝ𝑑Rand Proj𝒛𝑳∈ℝ𝑑Rand Proj… …
(a) BBT (b) BBTv2Black -Box Black -Box
123
2Figure 5: An illustration of (a) BBT (Sun et al., 2022b) and (b) BBTv2.
 is some derivative-free optimizer such as
CMA-ES. Compared with BBT, BBTv2 has 3 differences: (1) BBT requires pre-trained prompt embedding p0to
match the performance of model tuning on entailment tasks, and therefore does not completely get rid of gradients.
In contrast, BBTv2 requires no prompt pre-training. (2) BBT generates the random projection using a uniform
distribution while BBTv2 adopts model-speciﬁc normal distributions. (3) Instead of optimizing the prompt merely
in the input layer, BBTv2 uses a divide-and-conquer algorithm to alternately optimize prompt at each layer.
Algorithm 1: DC Algorithm for BBTv2
Require:L-layer PTM Inference API f,
Loss functionL,
Budget of API calls B,
Derivative-free optimizers fMjgL
j=1
1:Initialize random projections A1;:::;AL
2:Initialize parameters z(0)
1;:::;z(0)
L
3:Deep prompts p=hA1z(0)
1;:::;ALz(0)
Li
4:fori= 1toB=Ldo
5: forj= 1toLdo
6: Evaluate:loss=L(f(p))
7: Update: z(i)
j M j(z(i 1)
j;loss )
8: Replace: pj Ajz(i)
j
9: end for
10:end for
11:return Optimized deep prompts p
tasks (e.g., SST-2), our pilot experiments (§3) show
that it lacks versatility across tasks and PTMs. As
an improved variant of BBT, BBTv2 seeks to gen-
eralize BBT across tasks and PTMs.
Inspired by the recent success of deep prompt
tuning (Li and Liang, 2021; Qin and Eisner, 2021;
Liu et al., 2021b), we manage to inject continu-
ous prompt tokens to every layer of the PTM and
optimize them with derivative-free methods. Com-
pared to BBT that optimizes the prompt merely
in the input layer, BBTv2 has an order of mag-
nitude more parameters. For a PTM with Llay-
ers, BBTv2 seeks to optimize p=hp1;:::;pLi,
where pi2RD. Hence, the number of param-
eters to be optimized becomes LD. Say we areusing RoBERTa LARGE with 24 layers and insert 50
prompt tokens at each layer, the total number of
parameters to be optimized is 1.2M, posing a chal-
lenge of high-dimensional DFO. Instead of sim-
ply extending the dimensionality of the random
projection matrix AtoRLDd2, we propose a
divide-and-conquer (DC) algorithm to handle the
increased parameters.
In fact, DC has been well explored in prior
work (Kandasamy et al., 2015; Mei et al., 2016)
to cope with high-dimensional DFO problems by
decomposing the original high-dimensional prob-
lem into multiple low-dimensional sub-problems,
and solving them separately. The key assumption
of applying DC is that, the objective fcan be de-
composed into some additive form. Fortunately,
the forward computation of modern PTMs can be
expanded into an additive form due to the residual
connections (He et al., 2016). For instance, the
forward computation of a three-layered PTM can
be decomposed as
f(x1) =f3(x3) +x3 (2)
=f3(x3) +f2(x2) +x2 (3)
=f3(x3) +f2(x2) +f1(x1) +x1;(4)
wherefiis the transformation function of the i-th
layer, xiis the input of the i-th layer, and x1is
the input embedding. Thus, optimizing the con-
tinuous promptsfpigL
i=1attached to the hidden
states at every layer fxigL
i=1can be regarded as in-
dependent sub-problems.3Since the assumption is
2In our preliminary experiments, we implement this ver-
sion but did not obtain positive results.
3We omit the classiﬁcation head on the top of the PTM

--- PAGE 5 ---
satisﬁed, we propose a DC-based algorithm, which
is described in Algorithm 1, to implement BBTv2.
The prompts at different layers are optimized
alternately from bottom to up. For the optimization
at each layer, we maintain a speciﬁc random pro-
jection Ajand a CMA-ES optimizer Mj. When
alternating to layer j(Line 6-8 in Algorithm 1), a
single CMA-ES iteration is performed in the same
fashion as BBT, i.e., a new zjis generated byMj
and is then projected to pjusingAj. A graphical
illustration is shown in Figure 5.
During PTM inference, pj=Ajzjis ﬁrst added
with an initial prompt embedding p0
jand then con-
catenated with the hidden states xj. Thus, ac-
cording to Eq.(4), the forward computation of a
L-layered PTM can be viewed as
f(x1;p1:L) =[A1z1+p0
1;x1]
+LX
j=1fj([Ajzj+p0
j;xj]);(5)
where [;]means concatenation. Tunable param-
eters are highlighted in color. We set p0
1as the
word embeddings randomly drawn from the PTM
vocabulary for all tasks. p0
j(1< jL) is the
hidden states of the prompt tokens at the j-th layer.
4.2 Revisiting Random Projection
In the original BBT, each entry in the random
projection Ais sampled from a uniform distribu-
tion (He et al., 2015). In their experiments, using
normal distribution N(0;1=d)to generate the ran-
dom projection results in slow convergence and
inferior performance. However, we show in pilot
experiments that the uniform distribution exhibits
poor generalization on PTMs other than RoBERTa.
In this section, we shed some light on the effect
of the random projection, and propose to use nor-
mal distributions with model-related standard devi-
ations to generate random projections. In fact, most
prior works in high-dimensional DFO (Wang et al.,
2016; Qian et al., 2016) also adopt normal distribu-
tions to generate random projections. However,
they usually simply use N(0;1)orN(0;1=d),
both of which underperform in our scenario.
To take a closer look into the effect of the ran-
dom projection, we draw distribution of the initial
prompt pthat is projected from zby the projec-
tion matrix A. Here, zis sampled from the normal
since it is usually a linear transformation and would not affect
the additive decomposition.
0.300.32
1.00
 0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.000.000.020.040.06Uniform
Normal (Ori. BBT)
Normal (Ours)
EmbeddingFigure 6: Distributions of RoBERTa word embeddings
and generated prompt p=Az where Ais sam-
pled from different distributions. When using our de-
signed normal distribution to generate the random pro-
jection A, the distribution of the projected prompt well
matches the shape of the word embedding distribution,
leading to faster convergence and stronger generaliza-
tion.
distribution maintained by the CMA-ES, which is
initially set toN(0;0:5)in BBT. By generating A
from different distributions, we simulate the dis-
tribution of the projected prompt pand compare
with the distribution of RoBERTa LARGE word em-
beddings.4As revealed by Figure 6, when Ais
sampled from the normal distribution used in the
original BBT, i.e.,N(0;1=d), the projected prompt
pcannot cover the range of word embeddings, and
therefore suffers from slow convergence. In con-
trast, using uniform distribution can cover the range
of word embeddings, which explains why it per-
forms well on RoBERTa LARGE .
Thus, to generalize BBT (and BBTv2) across
different PTMs, we have to take into account the
distribution of word embeddings (and hidden states
for BBTv2) of the PTM for generating random
projections. In particular, we use the normal distri-
bution with standard deviation as follows,
A=^p
dz; (6)
where ^is observed standard deviation of word
embeddings (or hidden states for BBTv2), zis the
standard deviation of the normal distribution main-
tained by CMA-ES, is a constant scalar to stretch
the distribution. Initially, we set z=A= 0
so that no prior knowledge about the optimization
direction is incorporated. The main idea behind the
above calculation is to match the distribution (more
4We hypothesis that a high-quality prompt pshould lie
within the distribution of word embeddings (hidden states).

--- PAGE 6 ---
speciﬁcally the variance) between the projected
prompt and word embeddings (or hidden states).
When= 1, as we can see in Figure 6, the distri-
bution of the projected prompt can perfectly match
the distribution of the word embeddings. Detailed
derivation of Eq.(6) is provided in Appendix A.
5 Experiments
5.1 Datasets and Tasks
For comparison, we evaluate BBTv2 on the same
datasets as BBT, i.e., SST-2 (Socher et al., 2013),
Yelp (Zhang et al., 2015), AG’s News (Zhang et al.,
2015), DBPedia (Zhang et al., 2015), SNLI (Bow-
man et al., 2015), RTE (Wang et al., 2019), and
MRPC (Dolan and Brockett, 2005). SST-2 and
Yelp are sentiment analysis tasks, AG’s News
and DBPedia are topic classiﬁcation tasks, SNLI
and RTE are natural language inference (NLI)
tasks, and MRPC is a paraphrase task. In addi-
tion, we include two Chinese tasks, ChnSent5and
LCQMC (Liu et al., 2018), for evaluation on CPM-
2 (Zhang et al., 2021b), a Chinese PTM with 11B
parameters. ChnSent is a sentiment analysis task
while LCQMC is a question matching task.
We follow the same procedure as Zhang et al.
(2021a); Gu et al. (2021); Sun et al. (2022b) to
construct the true few-shot learning settings (Perez
et al., 2021). In particular, we randomly draw k
samples for each class to construct a k-shot training
setDtrain, and construct a development set Ddevby
randomly selecting another ksamples from the
original training set such that jDtrainj=jDdevj. We
use the original development sets as the test sets.
For datasets without development sets, we use the
original test sets. Therefore, in our experiments we
havejDtestjjD trainj=jDdevj.
5.2 Baselines
We consider two types of methods as our baselines:
gradient-based methods andgradient-free methods .
For gradient-based methods, we compare with
(1) Model Tuning and state-of-the-art parameter-
efﬁcient methods including (2) Adapter (Houlsby
et al., 2019), (3) BitFit (Zaken et al., 2022),
(4) LoRA (Hu et al., 2021), (5) Prompt Tun-
ing(Lester et al., 2021), and (6) P-Tuning v2 (Liu
et al., 2021b). We implement Adapter, BitFit, and
LoRA using OpenDelta6, and evaluate P-Tuning
5https://github.com/SophonPlus/
ChineseNlpCorpus
6https://github.com/thunlp/OpenDeltav2 in our experimental settings based on the ofﬁcial
implementation7. The results of Model Tuning and
Prompt Tuning are taken from Sun et al. (2022b).
For gradient-free methods, we compare with
two non-learning prompt-based methods: (1) Man-
ual Prompt and(2) In-Context Learning (Brown
et al., 2020); two feature-based methods: (3)
Feature-MLP and(4) Feature-BiLSTM , which
is to train a MLP/BiLSTM classiﬁer on the features
extracted by the PTM; and (5) BBT (Sun et al.,
2022b). The results of these gradient-free baselines
are taken from Sun et al. (2022b). One exception is
the performance of BBT on DBPedia. In the origi-
nal paper, BBT is performed given a larger budget
(20,000 API calls) on DBPedia for convergence. In
this work, we reimplement BBT on DBPedia with
the same budget (8,000 API calls) as other tasks
for fair comparison.
5.3 Implementation Details
Backbones. To compare with BBT, we mainly
use RoBERTa LARGE (Liu et al., 2019) as our back-
bone model. To verify the versatility, we also eval-
uate on other PTMs including BERT LARGE (Devlin
et al., 2019), GPT-2 LARGE , BART LARGE (Lewis
et al., 2020), and T5 LARGE (Raffel et al., 2020). In
addition, we also evaluate BBTv2 on a supersized
Chinese PTM, CPM-2 (Zhang et al., 2021b), which
has11B parameters.
Hyperparameters. Most of the hyperparameters
remain the same as BBT. We insert 50 continuous
prompt tokens at each layer. The subspace dimen-
sionality is set to 500. The CMA-ES with popu-
lation size of 20 and budget of 8,000 API calls is
applied to all the tasks. We adopt cross entropy as
the loss function. For generating random projec-
tions, we use normal distributions with standard
deviations calculated by Eq.(6) instead of uniform
distributions.
5.4 Results
Overall Comparison. As shown in Table 1,
BBTv2 outperforms BBT and other gradient-free
methods on 6/7 tasks. In contrast to BBT, the
improvement of BBTv2 mainly comes from DB-
Pedia, which has 14 classes, and hard entailment
tasks, namely MRPC and SNLI. On simple tasks
such as SST-2 and Yelp, BBT can perform on par
with BBTv2. When compared with gradient-based
7https://github.com/THUDM/P-tuning-v2

--- PAGE 7 ---
MethodTunable SST-2 Yelp P. AG’s News DBPedia MRPC SNLI RTEAvg.Params acc acc acc acc F1 acc acc
Gradient-Based Methods
Model Tuning 355M 85.39 2.84 91.82 0.79 86.36 1.85 97.98 0.14 77.35 5.70 54.64 5.29 58.60 6.21 78.88
Adapter 2.4M 83.91 2.90 90.99 2.86 86.01 2.18 97.99 0.07 69.20 3.58 57.46 6.63 48.62 4.74 76.31
BitFit 172K 81.19 6.08 88.63 6.69 86.83 0.62 94.42 0.94 66.26 6.81 53.42 10.63 52.59 5.31 74.76
LoRA 786K 88.49 2.90 90.21 4.00 87.09 0.85 97.86 0.17 72.14 2.23 61.03 8.55 49.22 5.12 78.01
Prompt Tuning 50K 68.23 3.78 61.02 6.65 84.81 0.66 87.75 1.48 51.61 8.67 36.13 1.51 54.69 3.79 63.46
P-Tuning v2 1.2M 64.33 3.05 92.63 1.39 83.46 1.01 97.05 0.41 68.14 3.89 36.89 0.79 50.78 2.28 70.47
Gradient-Free Methods
Manual Prompt 0 79.82 89.65 76.96 41.33 67.40 31.11 51.62 62.56
In-Context Learning 0 79.79 3.06 85.38 3.92 62.21 13.46 34.83 7.59 45.81 6.67 47.11 0.63 60.36 1.56 59.36
Feature-MLP 1M 64.80 1.78 79.20 2.26 70.77 0.67 87.78 0.61 68.40 0.86 42.01 0.33 53.43 1.57 66.63
Feature-BiLSTM 17M 65.95 0.99 74.68 0.10 77.28 2.83 90.37 3.10 71.55 7.10 46.02 0.38 52.17 0.25 68.29
BBT 500 89.56 0.25 91.50 0.16 81.51 0.79 79.99?2.95 61.56 4.34 46.58 1.33 52.59 2.21 71.90
BBTv2 12K 90.33 1.73 92.86 0.62 85.28 0.49 93.64 0.68 77.01 4.73 57.27 2.27 56.68 3.32 79.01
Table 1: Overall comparison on various language understanding tasks. We report mean and standard deviation of
performance over 3 different splits (§5.1). All of the results are obtained with pre-trained RoBERTa LARGE in the
16-shot (per class) setting. In each track, the best results are highlighted in bold and the second best results are
marked with underline .?We reimplement BBT on DBPedia given a budget of 8,000 API calls for fair comparison.
SST-2 AG’s News
(max seq len: 47) (max seq len: 107)
BBT BBTv2 BBT BBTv2
Accuracy 89.4 91.4 82.6 85.5
Training Time
PyTorch (mins) 14.8 11.0 28.3 25.0
ONNX (mins) 6.1 4.6 17.7 10.4
Memory
User (MB) 30 143 30 143
Server (GB) 3.0 3.0 4.6 4.6
Network
Upload (KB) 6 52 22 68
Download (KB) 0.25 0.25 1 1
Table 2: Comparison of BBT and BBTv2 on accuracy,
training time, memory use, and network load.
methods, BBTv2 achieves the best result in av-
erage across the 7 tasks while maintaining much
fewer tunable parameters. It is worth noting that
BBTv2, without any gradient-based components
(e.g., the pre-trained prompt embedding used in
BBT on entailment tasks (Sun et al., 2022b) or the
white-box prompt optimization required by Diao
et al. (2022)), is the ﬁrst pure black-box method
that matches the performance of full model tuning
on various understanding tasks.
Detailed Comparison. In Table 2, we compare
BBTv2 with BBT in other dimensions than accu-
racy. In addition to the improvement in accuracy,
BBTv2 also confers faster convergence than BBT.
For fair comparison of training time, we perform
early stopping if the development accuracy does notincrease after 1,000 steps. We report training times
under two implementations, PyTorch (Paszke et al.,
2019) and ONNX Runtime8, on a single NVIDIA
GTX 3090 GPU. In terms of memory footprint and
network load, BBTv2 slightly increases the mem-
ory use on the user side and the amount of data to
be uploaded.
BBTv2 Across PTMs. To verify the universal-
ity of BBTv2 across PTMs, we also evaluate on
BERT, GPT-2, BART, T5, and CPM-2. As shown
in Table 3, BBTv2 achieves superior performance
over BBT on PTMs with varying architectures, i.e.,
encoder-only, decoder-only, and encoder-decoder
PTMs.9In addition, we also verify the effective-
ness of BBTv2 on a supersized Chinese PTM,
CPM-2, which has 11B parameters. As shown
in Table 4, when using CPM-2 as the backbone,
BBTv2 outperforms full model tuning on two Chi-
nese tasks. The results of Vanilla PT, Hybrid PT,
and LM Adaption, which are three variants of
prompt tuning, are taken from Gu et al. (2021).
5.5 Ablations
Effect of Subspace Dimensionality. We explore
the subspace dimensionality dfrom 10 to 1000 us-
ing both BBT and BBTv2. The population size is
set to= 4 + 3 log( d)accordingly. Experimental
results on SST-2 and SNLI are demonstrated in Fig-
ure 7, from which we observe that: (1) Increasing
8https://onnxruntime.ai/
9We use the normal distribution (Eq.(6)) for BBT for gen-
eralizing to PTMs other than RoBERTa.

--- PAGE 8 ---
LM Method SST-2 AG’s News DBPedia
Encoder-only PTMs
BERTBBT 76.26 2.64 76.67 1.12 89.58 0.51
BBTv2 79.32 0.29 79.58 1.15 93.74 0.50
RoBERTaBBT 89.56 0.25 81.51 0.79 79.99 2.95
BBTv2 90.33 1.73 85.28 0.49 93.64 0.68
Decoder-only PTMs
GPT-2BBT 75.53 1.98 77.63 1.89 77.46 0.69
BBTv2 83.72 3.05 79.96 0.75 91.36 0.73
Encoder-Decoder PTMs
BARTBBT 77.87 2.57 77.70 2.46 79.64 1.55
BBTv2 89.53 2.02 81.30 2.58 87.10 2.01
T5BBT 89.15 2.01 83.98 1.87 92.76 0.83
BBTv2 91.40 1.17 85.11 1.11 93.36 0.80
Table 3: Comparison of BBT and BBTv2 on the large
versions of BERT, RoBERTa, GPT-2, BART and T5.
MethodTunable ChnSent LCQMC
Params acc acc
Model Tuning 11B 86.1 1.8 58.81.8
Vanilla PT 410K 62.1 3.1 51.53.4
Hybrid PT 410K 79.2 4.0 54.62.3
LM Adaption 410K 74.3 5.2 51.42.9
BBTv2 4.8K 86.40.8 59.12.5
Table 4: Results on two Chinese tasks with CPM-2 as
the backbone PTM.
subspace dimensionality dgenerally confers im-
proved performance for both BBT and BBTv2, but
marginal effect is also observed when d>100; (2)
BBTv2 almost always performs better than BBT
with the same subspace dimensionality.
Effect of Prompt Length. As reported in prior
work (Lester et al., 2021; Sun et al., 2022b), prompt
length can be a sensitive hyperparameter to model
performance. Hence, we explore the prompt length
from 5 to 100 using BBT and BBTv2. As shown
in Figure 7: (1) The optimal prompt length lies in
the range from 5 to 100 and varies across tasks; (2)
The effect of prompt length is somehow consistent
between BBT and BBTv2.
Effect of Model Scale. It has been demonstrated
that larger PTMs have a lower intrinsic dimension-
ality (Aghajanyan et al., 2021) and therefore, BBT
and BBTv2 should be more favorable to larger
PTMs. To verify this, we conduct experiments
on AG’s News using T5 (Raffel et al., 2020) with
varying scales, i.e., T5 SMALL , T5 BASE, T5 LARGE ,
and T5 XL, corresponding to 60M, 220M, 740M,
and 3B parameters. As shown in Figure 8, in the
0 200 400 600 800 1000
Subspace dimension d828486889092T est AccuracyModel TuningSST-2
BBT
BBTv2
0 200 400 600 800 1000
Subspace dimension d303540455055606570T est AccuracyModel TuningSNLI
BBT
BBTv2
20 40 60 80 100
Prompt Length8486889092T est Accuracy
Model TuningSST-2
BBT
BBTv2
20 40 60 80 100
Prompt Length303540455055606570T est AccuracyModel TuningSNLI
BBT
BBTv2Figure 7: Ablation results on subspace dimensionality
and prompt length. We show mean and standard devia-
tion of performance over 5 different runs.
50 100 200 400 800 1600 3200
Parameters (M)74767880828486T est Accuracy
DPT
BBT
BBTv2
Figure 8: The power of scale for black-box tuning with
T5 on AG’s News. DPT: deep prompt tuning.
AG’s News 16-shot learning setting, the gradient-
based counterpart, namely deep prompt tuning
(DPT, Li and Liang (2021); Liu et al. (2021b);
Qin and Eisner (2021)), performs better than BBT
and BBTv2 on T5 SMALL and T5 BASE . When using
T5LARGE and T5 XL, black-box tuning outperforms
DPT, demonstrating its power of scale.
6 Related Work
Parameter-Efﬁcient Tuning (PET). PET is to
optimize only a small portion of parameters while
keeping the main body of the model unchanged.
PET has achieved comparable performance to full
model tuning when training data is sufﬁcient (He
et al., 2021). The tunable parameters can be in-
jected into different positions of the PTM. Houlsby
et al. (2019) insert lightweight adapters to each

--- PAGE 9 ---
PTM layer; Lester et al. (2021) prepend continu-
ous prompt tokens to the input layer; Li and Liang
(2021); Liu et al. (2021b) inject tunable prompt
tokens to hidden states of every layer; Zaken et al.
(2022) only optimize the bias-terms in the PTM;
Hu et al. (2021) learn to adapt attention weights via
low-rank matrices. Though the number of tunable
parameters is reduced, back-propagation through
the entire model is still required to calculate the gra-
dients to update the small portion of parameters. To
that end, gradient-free methods are also proposed
to optimize continuous prompt (Sun et al., 2022b;
Diao et al., 2022) or discrete prompt (Prasad et al.,
2022; Deng et al., 2022).
Prompt-Based Learning. Prompt-based learn-
ing is to formulate downstream tasks as a (masked)
language modeling task, and therefore reduces the
gap between PTM pre-training and ﬁne-tuning (Liu
et al., 2021a; Sun et al., 2022a). The prompt can
be manually designed (Brown et al., 2020; Schick
et al., 2020; Schick and Schütze, 2021), mined
from corpora (Jiang et al., 2020), generated by gen-
erative PTMs (Gao et al., 2021), or be constructed
using gradient-guided search (Shin et al., 2020). In
this work, we also insert manually crafted textual
prompts into input samples but only optimize the
prepended continuous prompt tokens.
7 Conclusion
In this work, we present BBTv2, an improved ver-
sion of BBT (Sun et al., 2022b) with deep prompts
that are attached to every layer of the PTM. To
optimize the high-dimensional prompt parameters,
we propose a divide-and-conquer (DC) algorithm
combined with random projections to alternately
optimize the continuous prompt at each layer. Ex-
perimental results demonstrate that BBTv2, with-
out any gradient-based component, can achieve
comparable performance to state-of-the-art PET
methods and full model tuning while maintaining
much fewer tunable parameters.
Limitations
We summarize the limitations of this work as fol-
lows: (1) BBTv2 adopts a divide-and-conquer algo-
rithm to alternately optimize prompt at each PTM
layer. We use a unique CMA-ES optimizer, which
has two hyperparameters zandz, for the opti-
mization at each layer. As mentioned previously,
we setz= 0for not incorporating any prior to theoptimization direction. Therefore, we have totally
L(number of layers) hyperparameters for optimiza-
tion, i.e., [(1)
z;:::;(L)
z]. For simplicity, we con-
strain thezat different layers to be identical, i.e.,
(1)
z=(2)
z==(L)
z. In addition, Eq.(6) in-
troduces another hyperparameter , which can also
be different across different layers. Similarly, we
constrainat all layers to be identical. Hence, in
contrast to BBT that has only one hyperparameter
for optimization, BBTv2 introduces an additional
hyperparameter and therefore increases the cost
for hyperparameter search. (2) We conduct experi-
ments on 9 language understanding tasks across 4
types (i.e., sentiment analysis, topic classiﬁcation,
paraphrasing, and natural language inference) and
2 languages (i.e., English and Chinese). However,
the performance of BBTv2 on a wider range of
understanding tasks and generation tasks is still
under-explored. (3) We limit our work to few-shot
learning because the training data can be wrapped
into a single batch to be fed into the PTM such that
the model inference API is a deterministic func-
tion whose output only depends on the prompt. In
such a low-noise scenario, we can adopt the CMA-
ES to successfully perform optimization. In the
full data setting where the training samples are di-
vided into mini-batches, we need to explore other
derivative-free optimizers to handle the stochastic
(noisy) optimization. We leave the investigation of
adapting BBTv2 to a wider range of tasks and to
full data settings as future work, further removing
the remaining barriers to a gradient-free future.
Acknowledgements
This work was supported by National Natural
Science Foundation of China (No. 62106076),
Natural Science Foundation of Shanghai (No.
21ZR1420300), and National Natural Science
Foundation of China (No. 62022027).
References
Armen Aghajanyan, Sonal Gupta, and Luke Zettle-
moyer. 2021. Intrinsic dimensionality explains the
effectiveness of language model ﬁne-tuning. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing, ACL/IJCNLP 2021, (Volume 1: Long
Papers), Virtual Event, August 1-6, 2021 , pages
7319–7328. Association for Computational Linguis-
tics.

--- PAGE 10 ---
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2015, Lisbon, Portugal, September 17-21,
2015 , pages 632–642. The Association for Compu-
tational Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual .
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan
Wang, Han Guo, Tianmin Shu, Meng Song, Eric P.
Xing, and Zhiting Hu. 2022. Rlprompt: Optimiz-
ing discrete text prompts with reinforcement learn-
ing. CoRR , abs/2205.12548.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers) , pages 4171–4186. Association for Computa-
tional Linguistics.
Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang,
and Tong Zhang. 2022. Black-box prompt learn-
ing for pre-trained language models. CoRR ,
abs/2201.08531.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-
han Yang, Yusheng Su, Shengding Hu, Yulin Chen,
Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,
Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei
Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong
Sun. 2022. Delta tuning: A comprehensive study of
parameter efﬁcient methods for pre-trained language
models. CoRR , abs/2203.06904.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing, IWP@IJCNLP 2005, Jeju Island,
Korea, October 2005, 2005 . Asian Federation of
Natural Language Processing.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shotlearners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguis-
tics and the 11th International Joint Conference on
Natural Language Processing, ACL/IJCNLP 2021,
(Volume 1: Long Papers), Virtual Event, August 1-6,
2021 , pages 3816–3830. Association for Computa-
tional Linguistics.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2021. PPT: pre-trained prompt tuning for few-shot
learning. CoRR , abs/2109.04332.
Nikolaus Hansen, Sibylle D. Müller, and Petros
Koumoutsakos. 2003. Reducing the time complex-
ity of the derandomized evolution strategy with co-
variance matrix adaptation (CMA-ES). Evol. Com-
put., 11(1):1–18.
Nikolaus Hansen and Andreas Ostermeier. 2001. Com-
pletely derandomized self-adaptation in evolution
strategies. Evol. Comput. , 9(2):159–195.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2021. Towards a
uniﬁed view of parameter-efﬁcient transfer learning.
CoRR , abs/2110.04366.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectiﬁers: Surpass-
ing human-level performance on imagenet classiﬁ-
cation. In 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, De-
cember 7-13, 2015 , pages 1026–1034. IEEE Com-
puter Society.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2016, Las Ve-
gas, NV , USA, June 27-30, 2016 , pages 770–778.
IEEE Computer Society.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efﬁcient transfer learning for NLP.
InProceedings of the 36th International Confer-
ence on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , volume 97 of
Proceedings of Machine Learning Research , pages
2790–2799. PMLR.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. 2021. Lora: Low-rank adaptation of large lan-
guage models. CoRR , abs/2106.09685.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know. Trans. Assoc. Comput. Linguistics ,
8:423–438.
Kirthevasan Kandasamy, Jeff G. Schneider, and Barn-
abás Póczos. 2015. High dimensional bayesian opti-
misation and bandits via additive models. In Pro-
ceedings of the 32nd International Conference on

--- PAGE 11 ---
Machine Learning, ICML 2015, Lille, France, 6-11
July 2015 , volume 37 of JMLR Workshop and Con-
ference Proceedings , pages 295–304. JMLR.org.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Domini-
can Republic, 7-11 November, 2021 , pages 3045–
3059. Association for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020 ,
pages 7871–7880. Association for Computational
Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing, ACL/IJCNLP 2021, (Volume 1:
Long Papers), Virtual Event, August 1-6, 2021 ,
pages 4582–4597. Association for Computational
Linguistics.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022. What
makes good in-context examples for gpt-3? In
Proceedings of Deep Learning Inside Out: The 3rd
Workshop on Knowledge Extraction and Integration
for Deep Learning Architectures, DeeLIO@ACL
2022, Dublin, Ireland and Online, May 27, 2022 ,
pages 100–114. Association for Computational Lin-
guistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
CoRR , abs/2107.13586.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du,
Zhilin Yang, and Jie Tang. 2021b. P-tuning
v2: Prompt tuning can be comparable to ﬁne-
tuning universally across scales and tasks. CoRR ,
abs/2110.07602.
Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng,
Jing Chen, Dongfang Li, and Buzhou Tang. 2018.
LCQMC: A large-scale chinese question matching
corpus. In Proceedings of the 27th InternationalConference on Computational Linguistics, COLING
2018, Santa Fe, New Mexico, USA, August 20-26,
2018 , pages 1952–1962. Association for Computa-
tional Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Yi Mei, Mohammad Nabi Omidvar, Xiaodong Li, and
Xin Yao. 2016. A competitive divide-and-conquer
algorithm for unconstrained large-scale black-box
optimization. ACM Trans. Math. Softw. , 42(2):13:1–
13:24.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Py-
torch: An imperative style, high-performance deep
learning library. In Advances in Neural Informa-
tion Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada , pages 8024–8035.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho.
2021. True few-shot learning with language models.
CoRR , abs/2105.11447.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit
Bansal. 2022. Grips: Gradient-free, edit-based in-
struction search for prompting large language mod-
els.CoRR , abs/2203.07281.
Hong Qian, Yi-Qi Hu, and Yang Yu. 2016. Derivative-
free optimization of high-dimensional non-convex
functions by sequential random embeddings. In Pro-
ceedings of the Twenty-Fifth International Joint Con-
ference on Artiﬁcial Intelligence, IJCAI 2016, New
York, NY, USA, 9-15 July 2016 , pages 1946–1952.
IJCAI/AAAI Press.
Hong Qian and Yang Yu. 2021. Derivative-free rein-
forcement learning: a review. Frontiers Comput.
Sci., 15(6):156336.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying lms with mixtures of soft prompts.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2021, Online, June 6-11, 2021 , pages
5203–5212. Association for Computational Linguis-
tics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1–140:67.

--- PAGE 12 ---
Timo Schick, Helmut Schmid, and Hinrich Schütze.
2020. Automatically identifying words that can
serve as labels for few-shot text classiﬁcation.
InProceedings of the 28th International Confer-
ence on Computational Linguistics, COLING 2020,
Barcelona, Spain (Online), December 8-13, 2020 ,
pages 5569–5578. International Committee on Com-
putational Linguistics.
Timo Schick and Hinrich Schütze. 2021. Exploiting
cloze-questions for few-shot text classiﬁcation and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, EACL 2021, Online, April 19 - 23, 2021 , pages
255–269. Association for Computational Linguis-
tics.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P.
Adams, and Nando de Freitas. 2016. Taking the hu-
man out of the loop: A review of bayesian optimiza-
tion. Proc. IEEE , 104(1):148–175.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV ,
Eric Wallace, and Sameer Singh. 2020. Autoprompt:
Eliciting knowledge from language models with au-
tomatically generated prompts. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 4222–4235. Associ-
ation for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y . Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2013, 18-21 October 2013, Grand Hy-
att Seattle, Seattle, Washington, USA, A meeting of
SIGDAT, a Special Interest Group of the ACL , pages
1631–1642. ACL.
Tianxiang Sun, Xiangyang Liu, Xipeng Qiu, and Xu-
anjing Huang. 2022a. Paradigm shift in natural lan-
guage processing. Machine Intelligence Research .
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing
Huang, and Xipeng Qiu. 2022b. Black-box tun-
ing for language-model-as-a-service. In Proceed-
ings of the 39th International Conference on Ma-
chine Learning, ICML 2022, Baltimore, Maryland,
USA.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In 7th
International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019 . OpenReview.net.
Ziyu Wang, Frank Hutter, Masrour Zoghi, David Math-
eson, and Nando de Freitas. 2016. Bayesian opti-
mization in a billion dimensions via random embed-
dings. J. Artif. Intell. Res. , 55:361–387.Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning
for transformer-based masked language-models. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), ACL 2022, Dublin, Ireland, May 22-
27, 2022 , pages 1–9. Association for Computational
Linguistics.
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q.
Weinberger, and Yoav Artzi. 2021a. Revisiting few-
sample BERT ﬁne-tuning. In 9th International Con-
ference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021 . OpenRe-
view.net.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural
Information Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada , pages 649–
657.
Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen,
Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao
Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang
Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang,
Wentao Han, Yang Liu, Xiaoyan Zhu, and Maosong
Sun. 2021b. CPM-2: large-scale cost-effective pre-
trained language models. CoRR , abs/2106.10715.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Im-
proving few-shot performance of language models.
InProceedings of the 38th International Confer-
ence on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event , volume 139 of Proceedings of
Machine Learning Research , pages 12697–12706.
PMLR.

--- PAGE 13 ---
A Deviation of for Normal Distribution
Assume the variable z2Rdis sampled from a
normal distribution N(z;z)that is maintained
by the CMA-ES, the random projection A2
RDdis generated from another normal distribu-
tionN(A;A). Considering each entry of the
prompt pij=P
kAikzkj, the variance is as fol-
lows,
V(dX
k=1Aikzkj) =dX
k=1V(Aikzkj)
+ 2d 1X
k=1dX
p=k+1Cov(Aikzkj;Aipzpj):(7)
Since Aandz, and each entry in Aandzare
independent random variables and therefore the
variance ofP
kAikzkjcan be simpliﬁed as
V(dX
k=1Aikzkj) =dX
k=1V(Aikzkj) (8)
=dV(Aikzkj): (9)
It is easy to obtain that
V(Aikzkj) =V(Aik)V(zkj) +V(Aik)(E[zkj])2
+V(zkj)(E[Aik])2(10)
=2
A2
z+2
A2
z+2
z2
A: (11)
Initially, we do not incorporate any prior on the
optimization direction of the embedding (or hidden
states) and therefore A=z= 0. So we have
V(Aikzkj) =2
A2
z. Combined with Eq.(9), the
variance of the entries in the randomly projected
prompt is as follows,
V(pij) =d2
A2
z: (12)
Ideally, we expect the generated prompt to lie
in a reasonable solution space (e.g., the space of
the embedding or hidden states) such that the em-
bedding (hidden states) added by the prompt can
still lie in a reasonable space. A natural idea is
to match the variance of the generated prompt p
and the embedding (or hidden states). Formally,
we expect that V(pij) = (^)2, where ^is the
observed standard deviation of the embedding (or
hidden states) and is a constant scalar that con-
trols the range (relative to that of the embedding
or hidden states) where pfalls in. We can obtain arecommendation value of the standard deviation of
the random projection, that is
A=^p
dz: (13)
In practice,andzare hyperparameters.10It is
worth noting that the random projections are static
during the optimization process and therefore we
only need to observe the standard deviation of the
word embeddings and the hidden states at every
layer once at the beginning of the optimization. A
possible concern is that such observation breaks
the black-box and therefore it may be controversial
to call it "black-box tuning". We take a perspective
of feature-based approaches that views the embed-
dings and hidden states as features, which are the
outputs of the model. Thus, we do not really access
the inside information of the black-box model.
B Data Preprocessing
B.1 Statistics of Datasets
We list the statistics of the 7 English tasks and 2
Chinese tasks used in our experiments in Table 5.
Among the 9 tasks, 5 are single-sentence classiﬁ-
cation tasks and 4 are sentence-pair classiﬁcation
tasks. The types of the tasks range from sentiment
analysis, topic classiﬁcation, paraphrase, and natu-
ral language inference (NLI).
Category Dataset Lang. jYj j Trainj jTestj Type
single-
sentenceSST-2 En 2 67k 0.9k sentiment
Yelp P. En 2 560k 38k sentiment
AG’s News En 4 120k 7.6k topic
DBPedia En 14 560k 70k topic
ChnSent Zh 2 6k 1.2k sentiment
sentence-
pairMRPC En 2 3.7k 0.4k paraphrase
RTE En 2 2.5k 0.3k NLI
SNLI En 3 549k 9.8k NLI
LCQMC Zh 2 239k 8.8k NLI
Table 5: Statistics of datasets used in our experiments.
jYj: number of classes. "En" means English and "Zh"
means Chinese.
B.2 Templates and Label Words
For both BBT and BBTv2, we convert input texts
Xwith pre-deﬁned templates into ~X, and convert
output labels Yinto label words ~Y, such that down-
stream tasks can be reformulated into a (masked)
language modeling task and therefore we can reuse
10The speciﬁc hyperparameters to reproduce the results
on each dataset can be found in our code: https://github.
com/txsun1997/Black-Box-Tuning .

--- PAGE 14 ---
the pre-trained (masked) language modeling head.
In Table 6, we list the input and output formats for
different PTMs.
C Implementation Details
Clipping Hidden States In practice, we ﬁnd that
the standard deviation ^of hidden states (especially
at high layers) in some PTMs can be very large due
to a few outliers. As a result, our calculated A
(Eq.(6)) becomes large accordingly, and therefore
the generated prompt has a wider range of values
than expected. To address this issue, we iteratively
clip hidden states into the range of ^3^. We
perform clipping for 5 rounds and then compute
the standard deviation ^of hidden states. Note that
the clipping is only performed for statistics but is
not applied during model forward compute.
D Additional Results
On Convergence of Normal Distributions Pre-
viously in Figure 4, we show that using our de-
signed normal distribution leads to better gener-
alization from training data to development data.
Nevertheless, as reported by Sun et al. (2022b),
using normal distributions can suffer from slow
convergence. Therefore, we compare the conver-
gence rates using random projections generated
from different distributions. As demonstrated in
Figure 9: (1) For BBT, the convergence rate of
using our designed normal distribution is signiﬁ-
cantly faster than the normal distribution used in
the original BBT, and is comparable to uniform
distribution; (2) For BBTv2, using our normal dis-
tribution converges more stably on both SST-2 and
AG’s News. Especially, we observe that using our
normal distribution converges faster than uniform
distribution on AG’s News.
E Estimation of Memory Use and
Network Load
For measurement of memory footprint on user side,
we use psutil to monitor CPU memory when run-
ning CMA-ES. For memory footprint on service
side, we use nvidia-smi to monitor GPU memory
when serving PTM inference.
For estimation of network load, we measure
the amount of data to be uploaded and down-
loaded. For BBT and BBTv2, there are two
kinds of data to be uploaded: (1) training sam-
ples, and (2) continuous prompt. A training sam-
ple is comprised of two parts: input_ids and
0 2000 4000 6000 8000
Number of API calls0.000.050.100.150.200.250.300.350.40Cross Entropy LossBBT on SST-2
Uniform
Normal (Ori. BBT)
Normal (Ours)
0 2000 4000 6000 8000
Number of API calls0.30.40.50.60.70.8Cross Entropy LossBBT on AG's News
Uniform
Normal (Ori. BBT)
Normal (Ours)
0 2000 4000 6000 8000
Number of API calls0.000.050.100.150.200.250.300.350.40Cross Entropy LossBBTv2 on SST-2
Uniform
Normal (Ours)
0 2000 4000 6000 8000
Number of API calls0.10.20.30.40.50.60.70.8Cross Entropy LossBBTv2 on AG's News
Uniform
Normal (Ours)Figure 9: Comparison of convergence rates with ran-
dom projections with different distributions. When us-
ing our designed normal distribution to generate ran-
dom projections, both BBT and BBTv2 achieve fast
and stable convergence.
attention_mask . We can use the unsigned short
(representation range: 0 65535, 2 bytes per value)
forinput_ids and use the bool type (1 byte
per value) for attention_mask . For continuous
prompt, which contains hundreds of values for
BBT or tens of thousands of values for BBTv2,
we can use the ﬂoat type (4 bytes per value) for
representation. Take SST-2 16-shot split as an ex-
ample, the input_ids andattention_mask are
in shape of 3247, where 32 is the batch size
and 47 is the maximum sequence length, so there
are2.9KB data for input_ids and1.5KB data
forattention_mask . Assume the subspace di-
mensionality is 500, we need to upload additional
2KB data for prompt if using BBT and 48KB
data if using BBTv2. The data to be downloaded is
the output logits of the candidate words, which is a
dictionary containing jYj ﬂoat values. Take SST-
2 16-shot split as an example, the size of data to be
downloaded is 3224bytes = 0:25KB. We as-
sume the random projections are generated on the
server side therefore there is no need to download
hidden states to compute standard deviations for
users.

--- PAGE 15 ---
Dataset Input Output
Backbone: RoBERTa, BERT, BART
SST-2hPihSi. It was [MASK] great, bad
Yelp P.hPihSi. It was [MASK] great, bad
AG’s NewshPi[MASK] News:hSi World, Sports, Business, Tech
DBPediahPi[Category: [MASK] ]hSiCompany, Education, Artist, Athlete, Ofﬁce, Transportation, Building,
Natural, Village, Animal, Plant, Album, Film, Written
MRPChPihS1i?[MASK] ,hS2i Yes, No
RTEhPihS1i?[MASK] ,hS2i Yes, No
SNLIhPihS1i?[MASK] ,hS2i Yes, Maybe, No
Backbone: GPT-2
SST-2hPihSi. The sentiment is positive, negative
AG’s NewshPihSi. The news above is about world, sports, business, tech
DBPediahPihSi. The text above is aboutcompany, education, artist, athlete, ofﬁce, transportation, building,
natural, village, animal, plant, album, ﬁlm, written
Backbone: T5
SST-2hPihSi. It was [X] [X] positive/negative
AG’s NewshPi[X]News:hSi [X]World/Sports/Business/Tech
DBPediahPi[Category: [X]]hSi [X]Company/Education/Artist/Athlete/Ofﬁce/Transportation/Building/...
Backbone: CPM-2
ChnSenthPihSi;K[X] [X]}/î
LCQMChPi$­hS1ihS2i$åÝ/[X] [X]Ûþ/ø<
Table 6: Input templates and output label words used in our experiments. hPiis a sequence of continuous prompt
tokens.hSiis the original input text. For BART, which outputs denoised input in an auto-regressive fashion, we
only use the prediction of the masked position such that it follows the same output format as BERT and RoBERTa.
For T5 and CPM-2, [X]is a special token similar to [MASK] .
