# 2406.02913.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/no-backprop/2406.02913.pdf
# File size: 2872881 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity
Wentao Guo1, Jikai Long2, Yimeng Zeng3, Zirui Liu4, Xinyu Yang5, Yide Ran2,
Jacob R. Gardner3, Osbert Bastani3, Christopher De Sa6, Xiaodong Yu2,
Beidi Chen5, and Zhaozhuo Xu2
1wg0420@princeton.edu , Princeton University
2{jlong1,yran1,xyu38,zxu79}@stevens.edu , Stevens Institute of Technology
3{yimengz,jacobrg,obastani}@seas.upenn.edu , University of Pennsylvania
4zl105@rice.edu , Rice University
5{xinyuya2,beidic}@andrew.cmu.edu , Carnegie Mellon University
6cdesa@cs.cornell.edu , Cornell University
Abstract
Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning Large Language
Models using only forward passes. However, the application of ZO fine-tuning in memory-
constrained settings such as mobile phones and laptops is still challenging since full precision
forward passes are infeasible. In this study, we address this limitation by integrating sparsity
and quantization into ZO fine-tuning of LLMs. Specifically, we investigate the feasibility of
fine-tuning an extremely small subset of LLM parameters using ZO. This approach allows the
majority of un-tuned parameters to be quantized to accommodate the constraint of limited
device memory. Our findings reveal that the pre-training process can identify a set of “sensitive
parameters” that can guide the ZO fine-tuning of LLMs on downstream tasks. Our results
demonstrate that fine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the
full ZO fine-tuning performance, while offering wall-clock time speedup. Additionally, we show
that ZO fine-tuning targeting these 0.1% sensitive parameters, combined with 4 bit quantization,
enables efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than 8GiB of
memory and notably reduced latency.
1arXiv:2406.02913v1  [cs.LG]  5 Jun 2024

--- PAGE 2 ---
1 Introduction
Large language models (LLMs) have demonstrated superior performance in general-purpose language
generation [ 1,35,22]. Despite their success, it remains necessary to fine-tune LLMs for specific tasks
to achieve optimal results. However, fine-tuning LLMs often requires much more memory compared
to the inference process. Specifically, there are mainly four parts that occupy the memory during
fine-tuning LLMs: (1)the weight parameter itself; (2)the optimizer state, which contains the
information about the past gradient [ 16];(3)the weight gradient used to update the parameters; (4)
the activation cached to calculate the weight gradient [ 25]; In previous work like QLoRA [ 7], it can
reduce both (1)and(2)by combining weight quantization and low-rank adaption [ 12], which enables
fine-tuning huge LLMs under data-center level GPUs. However, under more memory-constraint
hardware like cell phones, the memory of caching (3)weight gradient and (4)activation required by
backpropagation still cannot be overlooked. The disparity between the demand of LLM fine-tuning
and hardware capacity limits the adaptability of LLMs, especially when personalizing them for edge
devices.
Exploring Zeroth-Order Optimization in LLM Fine-Tuning. Recently, there has been a
resurging interest in zeroth-order (ZO) optimization methods for LLM fine-tuning [ 27,23,3]. ZO
optimization method perturbs model parameters in random directions and utilize the loss value
difference to compute the gradient direction for parameter update. One advantage of ZO methods
in LLM fine-tuning is that they do not require backpropagation procedures, which significantly
saves the computation and memory. In this way, ZO is backpropagation-free and does not need
to cache(3)weight gradients and (4)activations during fine-tuning. In practice, ZO methods
have demonstrated the potential to achieve performance comparable to first-order methods in LLM
fine-tuning, which opens the doors for various efficient LLM adaptation strategies.
Sensitive
0.1%Random
10%0.00.51.01.5Sec / forwardTraining
Sensitive
0.1%Random
10%0.00.20.40.6Sec / tokenInference
sparse
other
dense mm
Figure 1: Training & inference speed
of Llama2-7B. As the sensitive sparse
fine-tuning method achieves great perfor-
mance via optimizing only 0.1%parame-
ters (performance comparable to ZO full
fine-tuning and 10% random subsets), dur-
ing inference we achieve an end-to-end
1.49×speedup, with 2.15×speedup at
sparse operations.Efficient ZO LLM Fine-Tuning with Sparsity.
Although ZO methods remove the need for backpropaga-
tion, a significant drawback of these methods is the slow
convergence rate [ 51,23]. A recent approach addresses
this by fine-tuning with a sparse mask [ 23,50], achiev-
ing approximately ∼75%sparsity. Nonetheless, this
sparsity level barely reduces computational overhead, as
the latency during the forward pass with even∼90%
sparsity is still comparable to that of dense matrix op-
erations. This latency increase can greatly impact user
experience on applications such as personal assistants,
where even a twofold increase in latency is perceptible.
In addition, merging the sparse weights back into the
basemodelisimpracticalonthesedevicesduetomemory
constraints prohibiting dequantization and quantization.
Empirical evidence suggests that higher sparsity levels
can significantly decrease the time required for sparse
matrix operations, as shown in Figure 1. This raises the
question:
Is it possible to leverage the benefits of higher sparsity levels in reducing inference latency while
preserving performance on downstream tasks? If so, how far can sparsity be pushed in this context?
Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity.
In this paper, we answer the raised research question by proposing an efficient sparse ZO LLM fine-
tuning strategy. We observe an extreme sparsity pattern in LLM parameters: a subset, determined
2

--- PAGE 3 ---
by selecting the top kmagnitude entries from the empirical Fisher information matrix, is effective for
ZO fine-tuning. Moreover, we find this sparsity pattern can be obtained through LLM’s continuous
pre-training process and be transferred to various downstream tasks without modification.
Summary of Contributions. Building on these insights, our work proposes a comprehensive
framework for ZO fine-tuning, making the following contributions:
•We identify that only an extremely small portion ( 0.1%) of LLM parameters should be updated
during ZO LLM fine-tuning. Moreover, we utilize this insight to guide the memory-efficient
on-device personalization of LLMs by low-bit quantization of model parameters.
•We observe the sparsity pattern observed in LLM pre-training can be transferred across different
downstream tasks while still maintaining good ZO performance. Based on this observation, we
develop a computational framework to perform parameter-efficient ZO fine-tuning of LLMs.
•We conduct extensive experiments across various LLMs and demonstrate that our method achieves
competitive performance across various downstream tasks.
2 Background and Related works
In this section, we present the formulation for ZO optimization. We also discuss related works about
sparsity in LLMs.
2.1 Zeroth-Order Optimization
ZO surrogate gradient estimator. ZO optimizers have been studied widely in the machine
learning community. Given a dataset D={(x1, y1), . . . , (xn, yn)}and a loss function fwith model
parameters w∈Rd, ZO optimizer will estimate the gradient at wvia ZO surrogate gradient estimator.
Simultaneous Perturbation Stochastic Approximation (SPSA) [ 39] is such an estimator that would
first sample a random vector z∈Rdand uses the loss value difference to scale the update direction.
zis usually sampled from an Gaussian distribution N(0,Id).
Definition 1 (Simultaneous Perturbation Stochastic Approximation (SPSA) [39]).SPSA
estimates the gradient w.r.t. wwith a data example (x, y), a small constant ϵ∈R, and a sampled
random vector z∈Rdas follows:
ˆg(w,(x, y),z) =f(w+ϵz; (x, y))−f(w−ϵz; (x, y))
2ϵz (1)
There are other ZO surrogate gradient estimators available [ 21,31], but in practice SPSA achieves
good performance in ZO optimization, particularly when fine-tuning LLMs. In addition, other ZO
algorithms such as DeepZero [ 3] would utilize the parameter-wise finite difference of loss values to
deriveparameter-wise update directions. This would yield O(d)query costs per training step even
when combining with certain sparse masking methods and not practical for LLM fine-tuning scenarios.
We therefore select SPSA with random Gaussian perturbation as our ZO gradient estimator.
ZO-SGD algorithm. ZO-SGD is an optimizer similar to SGD but replaces the FO gradient
with ZO surrogate gradient estimate per training step, as defined below:
Definition 2 (ZO-SGD update rule ).ZO-SGD is an optimizer that uses ZO surrogate gradient
to update parameters wtwith learning rate ηtand a data example (xt, yt)sampled at timestep t:
wt+1=wt−ηtˆgw(wt,(xt, yt),zt) (2)
3

--- PAGE 4 ---
MeZO [27] is a ZO-SGD algorithm that uses the "random seed trick" to save the need of
caching ZO surrogate gradient. The choice of optimizer (SGD) is orthogonal to ZO optimization
techniques, but in our preliminary experiments we find adaptive optimizers such as Adam [ 16]
would not necessarily accelerate ZO convergence in LLM fine-tuning scenarios. There are other ZO
optimizers aware of the parameter-wise heterogeneity of loss curvatures to accelerate the optimization
convergence [51], and we leave how to combine our method with theirs as future works.
2.2 Sparsity in LLMs
Sparsity-driven techniques are widely adopted in improving ML model’s efficiency [ 42,46,24,33,8]
and robustness [ 53,52]. Frankle and Carbin [8]showed that within large feed-forward networks,
there exists a subnetwork that, when trained in isolation, can achieve test accuracy comparable
to that of the original network. In the foundation models era, Liu et al. [24]demonstrated that
transformer-based models, such as OPT [ 49], exhibit great sparsity ( ≥95%) in activations. Moreover,
Panigrahi et al. [32]discovered that for RoBERTa [ 22], fine-tuning a very small subset of parameters
(∼0.01%) can yield performance exceeding 95%of that achieved by full fine-tuning.
In the context of ZO optimization, Liu et al. [23]and Zhang et al. [50]also suggest that sparsity
would potentially accelerate ZO optimization convergence. We believe that ZO has an intrinsic
need for sparse training , as the procedure of ZO gradient estimator usually requires nearly uniform
coordinate-wise scale (in expectation) perturbation which grows with d. In tradition, people usually
resolve this with knowledge from parameter-wise loss curvature heterogeneity (replace zwith Σ1/2z
where Σ1/2serves as a Hessian-informed preconditioner) [ 48,51]. However, they do not provide a
comprehensive investigation on massive parameter models like LLMs. In particular, we also observe
that during first-order (FO) fine-tuning of LLMs, the FO gradient can be quite sparse . We will
elaborate more on this insight in the following section (see Figure 2 and Figure 7). We would like to
explore how sparsity can benefit the ZO LLM fine-tuning.
3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
In this section, we describe the extreme sparsity pattern we observed in LLMs and how we utilize it
for efficient ZO fine-tuning including on-device personalization of LLMs.
3.1 Extreme Sparsity Pattern in LLM
ZO optimization with sensitive parameters. Given model parameters w, a loss function
f, a data example (x, y), sensitive parameters are defined as parameters whose corresponding FO
coordinate-wise gradient square values are maximized .
Definition 3 (Sensitive parameter mask ).A sensitive sparse mask mk∈ {0,1}dwithknonzero
entries (P
im(i) =k) is defined as
mk=argmaxm∥m⊙ ∇f(w; (x, y))∥2
2. (3)
In the context of ZO optimization, we will update sensitive parameters only. Denote that
¯z=z⊙mk. We will modify the SPSA gradient estimator from ˆg(w,(x, y),z)toˆg(w,(x, y),¯z), and
accordingly:
Definition 4 (Sensitive sparse ZO-SGD update rule ).
wt+1=wt−ηtˆgw(wt,(xt, yt),¯zt) (4)
4

--- PAGE 5 ---
10−510−410−310−210−1100
Ratio0.000.250.500.751.00Cumu. norm. sum of [∇F(w)]2
i RTE
mean±std
0.5
0.2
10−510−410−310−210−1100
RatioWiC
10−510−410−310−210−1100
RatioCOPAFigure 2: Cumulative normalized sum of coordinate-wise gradient square [∇F(w)]2
iof linear layers
during Llama2-7B [ 43] fine-tuning. For each linear layer, we first sort parameters by the decreasing
order of their gradient square value [∇F(w)]2
i, i∈[dlayer], and we take the cumulative sum and
normalize it to draw a blue curve, and the red-shaded region is the mean ±std of all blue curves.
More similar figures are in Figure 7. We observe that roughly 0.1% parameters in all linear
layers contribute about 50% gradient norm square.
The theoretical support of sensitive parameters can be derived from the lens of SPSA gradient
estimator and Fisher information matrix as follows:
•Maximum zeroth-order loss value changes, from the lens of SPSA estimator.
The square (account for negativity) of loss value difference for ˆgw(wt,(xt, yt),¯zt)is as follows:
E¯z{f(w+ϵ¯z; (x, y))−f(w−ϵ¯z; (x, y))}2≈E¯z{2ϵ¯z⊤∇wf(w; (x, y))}2(5)
= 4ϵ2∥mk⊙∇wf(w; (x, y))∥2(6)
Since by Definition 3 our sensitive mask would maximize ∥mk⊙∇wf(w; (x, y))∥2for a given sparsity
ratio, we would expect our sensitive mask to maximize the magnitude of the loss value difference
for any given sparsity ratio .
•Maximum coverage of Hessian diagonal, from the lens of Fisher matrix.
LLMs are often pre-trained on large text corpus to reach low perplexity before entering the fine-
tuning stage. In this case, we would assume pLLM(y|x)∼pD(y|x), which implies the empirical
Fisher ˆFshould be close to the (true) Fisher matrix Fas follows:
F=Ex∼pD,ˆy∼pLLM(·|x)∇wlogpLLM(ˆy|x)(∇wlogpLLM(ˆy|x))⊤(7)
≈ˆF=E(x,y)∼pD∇wlogpLLM(y|x)(∇wlogpLLM(y|x))⊤(8)
As we assume the empirical Fisher matrix approximates Fisher, which also approximates the
Hessian, and empirical Fisher’s diagonal is equalto the coordinate-wise gradient square vector when
computing with downstream task-specific loss, our sensitive parameters would cover a large fraction
of the largest Hessian diagonal entries.
This idea of sensitive parameters has been studied in the quantization community [ 15,11] and FO
optimization [ 40]. However, we are the first one to leverage the extremely sparse sensitive parameters
in LLM fine-tuning to accelerate ZO fine-tuning with LLMs . When we have perturbation and
updating in the scale of billion parameters, finding which parameters to fine-tune would be important
for improving ZO performance. Notice that here we use sensitive masks mkfor understanding
purposes. In Section 3.4, we will discuss how to transform Definition 4 to a parameter-efficient
optimization pipeline by optimizing fixedsensitive parameters.
5

--- PAGE 6 ---
3.2 Theoretical Convergence Rate
We would investigate the theoretical convergence of sensitive sparse ZO-SGD on sensitive parameters
under the non-convex optimization settings. Our assumptions are included in Appendix B.2.
Theorem 1 (Convergence rate of sensitive sparse ZO-SGD (Definition 4) ).If we pick
ηt= 1/(L(k+ 2)), under Assumptions 1 (bounded gradient error), 2 (Lipschitz smoothness), and 4
(sparse sensitive parameters), we would have
1
TT−1X
t=0E¯z,(x,y)∥∇wF(wt)∥2≤Ok
c·L
T
(F(w0)− F∗) + 3σ2. (9)
Moreover, if we still pick ηt= 1/(L(k+ 2)), with an extra Assumption 3 (P.L. condition), we
would have
E¯z,(x,y){F(wT)− F∗} ≤
1−Oµ
L·c
kT
(F(w0)− F∗) +3σ2c
2L(k+ 2). (10)
The proof for Inequality 9 is in Appendix B.2 and the proof for Inequality 10 is in Appendix B.3.
If we choose k=dandc= 1, both convergence rates trivially reduce to the standard zeroth-
order convergence rate as O(d/T) +O(constant )andO((1/d)T) +O(constant ). As we assume
c≫k/d, we know d≫k/cand therefore both O((k/c)(1/T))andO((c/k)T)are much lower than
O(d/T) +O(constant )andO((1/d)T) +O(constant )that zeroth-order method will yield.
We want to emphasize that our contributions are more on empirical LLM fine-tuning instead of
general machine learning tasks, and in Section 4.1 we extensively compare our sparse ZO methods
with other sparse ZO methods and we demonstrate its superiority during LLM fine-tuning . We do not
use the strict “local r-effective rank” assumption that Malladi et al. [27]uses, and our Assumption 4
can be easily observed empirically in Figure 2. Liu et al. [23]and Ohta et al. [31]also provide similar
analysis on the convergence. However, they do not include our sensitive sparse mask in their studies.
3.3 Transferability of LLM Pre-Training Sparsity Pattern in ZO Fine-Tuning
Sparse fine-tuning with fixed sensitive parameters. Our Theorem 1 focuses on dynamic
sparse fine-tuning. However, Panigrahi et al. [32]notice that in real LLM fine-tuning scenario, the
fine-tuning performance could be attributed to a sparse subset of weights ( ∼0.01%). Malladi et al.
[28]also find certain fine-tuning tasks would demonstrate kernel behaviors, which include “fixed
(gradient) features”: ∇wf(wafter FT ; (x, y))∼ ∇ wf(wbefore FT ; (x, y)).
The similarity of gradient features during fine-tuning would imply that we do not need to re-select
our sensitive parameters during fine-tuning i.e. select once before fine-tuning should be sufficient.
This hypothesis can be validated by Figure 3 and Figure 5b. In Figure 3, the fact that “task grad,
static” does notvanish and still has a large ratio over “task grad, dyn.” at the end of training
demonstrate that we can select parameters before fine-tuning . We also include similar figures for
Mistral-7B and OPT-6.7B in Figure 8 in Appendix C.3. We will describe Figure 5b in Section 4.3.
Surrogate sensitive sparse mask from pre-training datasets. Another observation from
Figure 3 is that the sensitive parameters derived from pre-training datasets (C4) would still cover
a large fraction of model sensitivity. Therefore, we could use it as a surrogate sensitive sparse
mask when gradients on downstream tasks are unavailable, particularly in scenario of on-device
personalization .1
1Obtaining gradients of LLMs on edge devices is expensive, and we usually cannot transfer data from edge devices
to the cloud to compute the gradient on downstream tasks on cloud. In this case we would need some surrogate
gradient information to derive sensitive sparse masks on cloud. We will discuss this in Section 3.5.
6

--- PAGE 7 ---
Before FTEpoch 1 Epoch 5End of FT0.00.20.40.60.81.0Cumulative normalizd sum of [∇F(w)]2
i
RTE
10−2
10−3
10−4task grad, dyn.
task grad, static
C4 grad, static
Before FTEpoch 1 Epoch 5End of FT
WiC
Before FTEpoch 1 Epoch 5End of FT
COPAFigure 3: Cumulative normalized gradient square values of Llama2-7B model’s linear layers during
fine-tuning. For each line, the colors represent the fraction of parameters and the line style represents
the category. “task grad, dyn.” refers to the sensitive parameters selected at the given timestep
(x-axis), and “task grad, static” refers to the sensitive parameters selected before fine-tuning. “C4
grad, static” refers to the sensitive parameters selected with gradients taken from causal language
modeling on C4 datasets [ 36], and we keep it unchanged during fine-tuning. More similar figures are
in Figure 8.
3.4Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable
Sparsity
The sparse optimization on fixedparameters can be implemented as a parameter-efficient optimization
workflow, which will reduce the perturbation and updating time during ZO optimization. Suppose
we have derived a sensitive sparse mask mk, and we know it is fixed during fine-tuning. Instead of
applying mktoz, we would apply it directly to wand extract the nonzero parts as below:
wsparse =w⊙mk,wdense =w⊙(1d−mk) (11)
Denote zk,t∼ N(0k,Ik)as the Gaussian perturbation sampled in timestep t. We will determine
wsparsebefore fine-tuning and optimize on wsparseonlyand leave wdensefrozen during fine-tuning.
In this case, our sensitive sparse ZO-SGD update rule will become:
wsparse ,t+1=wsparse ,t−ηtˆg(wsparse ,t,(xt, yt),zk,t) (12)
In Section 3.5, we describe how this decomposition would seamlessly combine with existing post-
training quantization (PTQ) methods, which creates an opportunity for on-device personalization.
In Appendix C.6, we discuss efficient implementations of linear layers after our decomposition.
3.5 An Opportunity for On-Device LLM Personalization
4. Quantize 
dense parts
Stay frozen
ready to 
serve!1. Get gradients from
pre-training datasets2. Get surrogate sensitive
sparse parameter masks
3. Decompose weights to
dense and sparse parts5. send models to 
personal devices 
sparse parts 
remain in 16-bit
6. on-device 
ZO fine-tuningPre-trained LLM Apply sensitive sparse masksCloud Edge
Figure 4: On-device LLM personalization workflow via integrating sensitive sparse ZO optimization
with quantization.
7

--- PAGE 8 ---
As LLMs are often pre-trained with user-agnostic public datasets, personalizing LLMs with
individual user’s preferences and meet user’s specific needs before real-world deployment are vital.
[41,26] However, transferring the user-specific data to upstream cloud before fine-tuning LLMs would
raise privacy concerns. [ 47] On the other hand, personal devices usually have less computational
budget and are more memory-constrained than the cloud [ 54], and performing full fine-tuning would
easily exceed the device memory budget.
If we want to fine-tune a 7B-level model (like Llama2-7B) on memory-constrained devices, we
need to reduce the memory consumption on model weights ,gradients ,forward activations , and
optimizer states :
•Model weights. We would quantize the wdenseto 4 bits, which reduces the model size of a
Llama2-7B model from 13.5 to 3.4 GiB.
•Forward activations. ZO optimization already saves the need of caching activations.
•Gradients. We would use the “random seed trick” same as MeZO [ 27] to reproduce layer-wise
gradients instead of caching them.
•Optimizer states. We use SGD. Our method can also be implemented as a parameter-efficient
optimization method which is also memory-efficient with other optimizers (even with Adam).
As a result, our memory consumption is nearly minimum : we can fine-tune a Llama2-7B model
under 8 GiB GPU memory without any offloading . This would satisfy the memory constraint by a
wide range of edge or mobile devices as illustrated in Table 3.
Integration with quantization. In Section 3.4, we know that we can obtain surrogate sensitive
sparse masks before fine-tuning. We would first decompose sensitive wtowsparseandwdense. We
will then quantize wdense. During this process, we will use surrogate gradient information that many
PTQ algorithms already have: they need gradients to calibrate their quantization errors.
Our method also does notput strict constraints on specific choices of quantization algorithms
since any algorithm [ 2,30,9,20,15] that aims to minimize the quantization error term or its variant
would suffice:
Q(w) =argminQ(w)Ex∥(w−Q(w))x∥2
2 (13)
On-device personalization workflow. The workflow is illustrated in Figure 4. The high-level
overview is that we use surrogate gradient information from pre-training datasets ∇wpLLM(y|x)to
extract sensitive parameters wsparseand keep wsparsein 16 bits, while we quantize the remaining
dense weights wdense(Step 1-4). We send wsparseandQ(wdense)to personal devices (Step 5), and
we perform on-device ZO fine-tuning only on wsparse(Step 6).
4 Experiments
In this section, we want to validate the effectiveness of our sensitive sparse ZO optimization method.
We also investigate the effectiveness of our on-device personalization recipe in Figure 4. There are a
few research questions we want to answer:
•RQ1:Isoptimizingsensitiveparametersmoreeffectivethanoptimizingothersubsetofparameters
during ZO fine-tuning? Can we optimize surrogate sensitive sparse parameters when downstream
gradient information is unavailable?
•RQ2:Can optimizing extremely sparse and fixedparameters (Equation 12) lead to iteration-wise
and total wall-clock time speedup?
•RQ3:Can we match the full performance of ZO full fine-tuning by employing our on-device
personalization recipe (Figure 4)?
8

--- PAGE 9 ---
We focus on 7B LLM models (Llama2-7B [ 43], Mistral-7B [ 13], OPT-6.7B [ 49]) as they would fit
with common on-device memory constraints (8 GiB) listed on Table 3 after applying quantization.
We use SST-2 [ 38], RTE [44], CB [6], BoolQ [ 4], WSC [ 17], WiC [34], and COPA [ 37] datasets. We
follow standard ZO fine-tuning settings and use the same codebases as in Malladi et al. [27]. More
details of our experiments (hyperparameters, task-specific prompts, etc.) are in Appendix C.
4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters
We first investigate the performance of optimizing our sensitive parameters versus other subsets of
parameters. Our baseline sparsity methods are random subsets and weight outliers. As illustrated in
Figure 5a, we can find that ZO fine-tuning would benefit from sparse optimization, as all methods
would achieve higher than ZO full fine-tuning at 90% sparsity. However, only sensitive parameters
would maintain its performance as we move to the extreme sparsity region (>99%). In fact, the
performance curve of sensitive parameters w.r.t. different sparsity levels is near a flat curve , which
indicates the performance loss by moving from 90% to 99.9% is minimal. Therefore, we can optimize
100×less parameters compared with random and weight outliers and still get same performance.
We also validate whether optimizing fixedandsurrogate sensitive parameters should still yield
satisfactory performance. In Figure 5b, we compare the performance of optimizing sensitive
parameters with C4 gradients with its theoretical upper bound: fixedsensitive parameters derived
from task-specific gradients as the solid line and its dynamic version as the dash-dotted line. We also
include the fixed and dynamic random subset parameters as a baseline. We can find that the gap
of sensitive parameters between deriving from C4 gradients and task-specific gradients at sparsity
level 99.9% is smalland blue line is still far above the random and full fine-tuning baseline. We also
present a summary of our approaches with 99.9% sparsity on various datasets and models in Table 1.
4.2 RQ2: Wall-Clock Time Efficiency
By employing parameter-efficient ZO fine-tuning with extreme sparsity, we also achieve 1.2 - 2.5 ×
wall-clock time convergence speedup compared with ZO full fine-tuning as we nearly eliminate the
ZO perturbation and optimizer update time, as Figure 6 shows. This also boosts the GPU utilization
rate as large-batched ZO forward is often compute-bounded while the perturbation and optimization
steps are often memory-bounded. Furthermore, the reduced memory footprint of parameter-efficient
ZO fine-tuning allows for training larger models on the same hardware, potentially leading to even
better performance. As a result, we answer this question that optimizing extremely sparse and fixed
parameters leads to substantial iteration-wise and total wall-clock time improvements.
4.3 RQ3: On-Device Personalization
We validate whether our sensitive sparse ZO optimization method would fit with on-device person-
alization pipeline described in Section 3.5 with Table 1. We follow the exact recipe as described
Figure 4 to report a number as “Sensitive (C4, static)”, where we only optimize 0.1% sensitive
parameters on top of a 4-bit quantized model. As ZO fine-tuning happens aftermodel is quantized,
ablating on extracting 0.1% random subsets of parameters would produce a different quantized
model. So we choose to report the result for optimizing a fixed random subset on top of the 16-bit
model as the “Random (static)”.
We also compare with optimizing with LoRA [ 12] and Prefix Tuning [ 19] with ZO-SGD optimizer
on top of the samequantized model. We follow the LoRA randαand prefix length shown
in Malladi et al. [27], and for LoRA, we add it to all linear layers same as where our sensitive
parameters are extracted. We find that integrating sensitive sparse ZO optimization with on-device
9

--- PAGE 10 ---
10−510−410−310−210−1100
Fraction to optimize60657075AccuracyRTE
10−510−410−310−210−1100
Fraction to optimize55.057.560.062.565.0WiC
10−510−410−310−210−1100
Fraction to optimize80828486COPA
10−510−410−310−210−1100
Fraction to optimize65.067.570.072.575.0100×sparserAverage
Sensitive (C4 grad, static)
Weights (largest, static)
Random (static)
Full ﬁne-tuning(a) Optimizing sensitive parameters with C4 gradients versus optimizing weights with largest magnitude
(weight outliers) and random subsets of weights. The trainable parameters are all determined before fine-
tuning and other parameters are kept unchanged.
10−510−410−310−210−1100
Fraction to optimize5560657075Accuracy
RTE
10−510−410−310−210−1100
Fraction to optimize5860626466
WiC
10−510−410−310−210−1100
Fraction to optimize80828486
COPA
10−510−410−310−210−1100
Fraction to optimize60657075
Gap is small (1.0)
Gap is large (4.0)Average
Sensitive (C4 grad, static)
Sensitive (task grad, static)
Sensitive (C4 grad, dyn.)
Random (static)
Random (dyn.)
Full ﬁne-tuning
(b) Optimizing sensitive parameters with C4 gradients versus task-specific gradients. “Static” means the
parameters to optimize are determined before fine-tuning and other parameters are kept unchanged during
fine-tuning. “Dyn.” means the parameters to optimize will be updated every 100 training steps.
Figure 5: Performance of optimizing sensitive parameters in Llama2-7B fine-tuning on RTE, WiC,
and COPA tasks.
Sensitive Full012Seconds / Step1.21×RTE
Sensitive Full0.00.51.0
1.51×WiC
Sensitive Full0.00.20.40.6
2.57×COPA
forward
perturbation
optimization
0 5 10 15
Time (hr)0.40.6Train loss1.45×
0 2 4 6
Time (hr)0.650.700.75
1.25×
0 1 2 3
Time (hr)1.01.52.0
2.64×Sensitive
Full
Figure 6: Iteration-wise & wall-clock convergence time of sensitive sparse fine-tuning on fixed
parameters (“Sensitive”) versus ZO full fine-tuning (“Full”) for Llama2-7B. Here we use the 16-bit
model as the base model for fine-tuning.
personalization pipelines would still yield good performance exceeding all baselines across models
and tasks. Particularly, the performance is higher than ICL, and ZO full fine-tuning in 16 bits. In
addition, we have surpassed other ZO-PEFT methods and random sparse ZO fine-tuning methods.
This demonstrates the superiority of optimizing sensitive parameters onlyin ZO fine-tuning recipes.
10

--- PAGE 11 ---
Table 1: Performance of difference methods on Llama2-7B fine-tuning tasks. In the first column,
“Q” means the full model is quantized with 4-bit quantization method (SqueezeLLM [ 15]), and
“ZO” means the model is fine-tuned with ZO-SGD optimizer. For each cell, we use the same
hyperparameters and repeat it with 3 random seeds. We report the average and standard deviation
of test set accuracy in the format of mean std. In last 2 columns, “Acc” means the average test set
accuracy and “Rank” means the average rank among all methods across tasks.
(a)Llama2-7B
Methods SST-2 RTE CB BoolQ WSC WiC COPA Acc Rank
Q, ZO Sensitive (C4, static) 94.7 0.474.7 1.266.7 2.283.0 0.557.43.965.2 0.985.02.275.2 2.43
LoRA 93.80.664.71.164.94.779.71.161.5 2.159.80.185.7 0.572.9 4.29
Prefix 80.54.365.51.263.13.080.30.254.511.458.31.382.00.869.2 5.86
ZO Sensitive (task, static) 94.8 0.173.6 0.969.1 2.283.5 0.857.44.764.21.183.7 2.475.2 2.29
Random (static) 94.10.368.01.764.93.477.00.759.6 3.664.8 1.183.31.773.1 4.14
Full fine-tuning 94.60.573.35.166.70.881.90.858.04.361.90.282.71.774.2 3.57
Zero-shot 89.00.057.80.032.10.069.90.250.20.036.50.079.00.059.2 7.29
ICL 94.80.271.54.372.615.277.54.653.21.161.14.387.02.274.0 3.43
(b)Mistral-7B
Q, ZO Sensitive (C4, static) 94.0 0.374.2 2.770.2 2.275.1 2.459.64.961.2 0.988.3 1.274.7 2.86
LoRA 94.0 0.465.31.364.94.570.33.760.9 3.761.10.488.3 0.572.1 3.57
Prefix 86.92.157.31.463.75.962.20.960.34.649.00.381.31.765.8 4.86
ZO Sensitive (task, static) 94.7 0.377.1 0.969.0 0.878.4 2.258.0 4.361.40.289.3 1.375.4 1.86
Random (static) 87.91.950.20.866.14.460.61.757.61.457.30.882.31.766.0 5.29
Full fine-tuning 94.60.174.62.168.86.276.60.254.86.262.6 0.588.30.574.3 2.86
Zero-shot 54.80.050.50.037.50.043.41.850.80.039.40.078.00.050.6 7.00
ICL 60.716.755.24.733.313.146.86.550.40.663.80.988.70.557.0 5.43
(c)OPT-6.7B
Q, ZO Sensitive (C4, static) 94.9 0.572.8 3.683.3 5.173.1 0.959.3 5.360.9 0.484.0 1.475.5 1.29
LoRA 94.20.2 69.61.669.01.769.62.057.19.157.20.883.02.271.4 4.57
Prefix 93.30.4 71.21.072.01.768.92.862.52.459.40.580.02.472.5 4.14
ZO Sensitive (task, static) 94.5 0.475.5 1.482.1 3.672.5 0.857.4 5.260.6 1.483.3 1.775.1 2.14
Random (static) 87.32.0 68.41.770.66.366.01.058.07.056.41.379.00.869.4 5.71
Full fine-tuning 94.40.3 72.71.279.83.072.11.257.4 4.660.20.982.32.674.1 3.29
Zero-shot 61.00.0 60.70.046.40.055.71.055.50.036.50.077.00.056.1 7.71
ICL 74.014.665.811.254.85.967.92.153.21.741.04.580.72.962.5 6.57
We also notice that optimizing sensitive parameters derived from C4 gradients still produce close
results as from task-specific gradients (in average less than 1% accuracy difference). This indicates
optimizing surrogate sensitive parameters is still empirically successful.
5 Conclusion
We have shown that the sensitive parameters provided by the pre-training process can effectively
assist in ZO LLMs fine-tuning. Our experiments suggest that the ZO fine-tuning guided by 0.1%
sensitive parameters in the LLM can even perform better than the full parameter ZO fine-tuning.
The experiment results also demonstrate that the quantization of parameters other than sensitive
parameters allows us to perform ZO fine-tuning of an LLM on limited memory devices.
11

--- PAGE 12 ---
References
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[2]Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quanti-
zation of large language models with guarantees. Advances in Neural Information Processing
Systems, 36, 2024.
[3]Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Jiancheng Liu, Konstantinos
Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura, and Sijia Liu. Deepzero: Scaling up
zeroth-order optimization for deep model training. In International Conference on Learning
Representations , 2024. doi: 10.48550/arXiv.2310.02025.
[4]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
InProceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) ,
pages 2924–2936, 2019.
[5]Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691 , 2023.
[6]Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:
Investigating projection in naturally occurring discourse. In Proceedings of Sinn und Bedeutung ,
pages 107–124, 2019.
[7]Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient
finetuning of quantized llms. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt,
and S. Levine, editors, Advances in Neural Information Processing Systems , volume 36, pages
10088–10115. Curran Associates, Inc., 2023.
[8]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations , 2019. doi: 10.48550/
arXiv.1803.03635.
[9]Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training
quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.
[10]In Gim and JeongGil Ko. Memory-efficient dnn training on mobile devices. In Proceedings of
the 20th Annual International Conference on Mobile Systems, Applications and Services , pages
464–476, 2022.
[11]Han Guo, Philip Greengard, Eric Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized
matrix decomposition for efficient language model finetuning. In The Twelfth International
Conference on Learning Representations , 2023.
[12]Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021.
12

--- PAGE 13 ---
[13]Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
[14]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models.arXiv preprint arXiv:2001.08361 , 2020.
[15]Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W
Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint
arXiv:2306.07629 , 2023.
[16]DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. In International
Conference on Learning Representations , 2015. doi: 10.48550/arXiv.1412.6980.
[17]Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In
Thirteenth international conference on the principles of knowledge representation and reasoning ,
2012.
[18]Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. Transformer-lite:
High-efficiency deployment of large language models on mobile phone gpus. arXiv preprint
arXiv:2403.20041 , 2024.
[19]Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
arXiv preprint arXiv:2101.00190 , 2021.
[20]Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-
awareweightquantizationforllmcompressionandacceleration. arXiv preprint arXiv:2306.00978 ,
2023.
[21]Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K
Varshney. A primer on zeroth-order optimization in signal processing and machine learning:
Principals, recent advances, and applications. IEEE Signal Processing Magazine , 37(5):43–54,
2020.
[22]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 , 2019.
[23]Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, and Yang You. Sparse
mezo: Less parameters for better performance in zeroth-order llm fine-tuning. arXiv preprint
arXiv:2402.15751 , 2024.
[24]ZichangLiu, JueWang, TriDao, TianyiZhou, BinhangYuan, ZhaoSong, AnshumaliShrivastava,
Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient
llms at inference time. In International Conference on Machine Learning , pages 22137–22176.
PMLR, 2023.
[25]Zirui Liu, Guanchu Wang, Shaochen Henry Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Ryan
Tang, Zhimeng Stephen Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, et al. Winner-
take-all column row sampling for memory efficient adaptation of language model. Advances in
Neural Information Processing Systems , 36, 2024.
13

--- PAGE 14 ---
[26]Nattaya Mairittha, Tittaya Mairittha, and Sozo Inoue. Improving activity data collection
with on-device personalization using fine-tuning. In Adjunct Proceedings of the 2020 ACM
International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the
2020 ACM International Symposium on Wearable Computers , pages 255–260, 2020.
[27]Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and
Sanjeev Arora. Fine-tuning language models with just forward passes. Advances in Neural
Information Processing Systems , 36:53038–53075, 2023.
[28]Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based
view of language model fine-tuning. In International Conference on Machine Learning , pages
23610–23641. PMLR, 2023.
[29]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In International Conference on Learning Representations , 2016.
[30]Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort.
Up or down? adaptive rounding for post-training quantization. In International Conference on
Machine Learning , pages 7197–7206. PMLR, 2020.
[31]Mayumi Ohta, Nathaniel Berger, Artem Sokolov, and Stefan Riezler. Sparse perturbations for
improved convergence in stochastic zeroth-order optimization. In Machine Learning, Optimiza-
tion, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23,
2020, Revised Selected Papers, Part II 6 , pages 39–64. Springer, 2020.
[32]Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill
localization in fine-tuned language models. In International Conference on Machine Learning ,
pages 27011–27033. PMLR, 2023.
[33]Zhimin Peng, Ming Yan, and Wotao Yin. Parallel and distributed sparse optimization. In 2013
Asilomar conference on signals, systems and computers , pages 659–646. IEEE, 2013.
[34]Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for
evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pages 1267–1273, 2019.
[35]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[36]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. arXiv e-prints , 2019.
[37]MelissaRoemmele, CosminAdrianBejan, andAndrewSGordon. Choiceofplausiblealternatives:
An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series , 2011.
[38]Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language
processing , pages 1631–1642, 2013.
14

--- PAGE 15 ---
[39]James C Spall. Multivariate stochastic approximation using a simultaneous perturbation
gradient approximation. IEEE transactions on automatic control , 37(3):332–341, 1992.
[40]Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks.
Advances in Neural Information Processing Systems , 34:24193–24205, 2021.
[41]Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. Democra-
tizing large language models via personalized parameter-efficient fine-tuning. arXiv preprint
arXiv:2402.04401 , 2024.
[42]Zhen Tan, Tianlong Chen, Zhenyu Zhang, and Huan Liu. Sparsity-guided holistic explanation
for llms with interpretable inference-time intervention. In Proceedings of the AAAI Conference
on Artificial Intelligence , pages 21619–21627, 2024.
[43]Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
[44]Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. In
Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP , pages 353–355, 2018.
[45]Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training transformers with 4-bit
integers. Advances in Neural Information Processing Systems , 36:49146–49168, 2023.
[46]Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li,
Wei Lin, and Shuaiwen Leon Song. Flash-llm: Enabling cost-effective and highly-efficient large
generative model inference with unstructured sparsity. In Proceedings of the VLDB Endowment,
Vol. 17, No. 2 , 2023. doi: 10.14778/3626292.3626303.
[47]Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang, and Xuanzhe Liu. Deeptype: On-device
deep learning for input personalization service with minimal privacy concern. Proceedings of the
ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , 2(4):1–26, 2018.
[48]Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li, and Tong Zhang. Hessian-aware
zeroth-order optimization for black-box adversarial attack. arXiv preprint arXiv:1812.11377 ,
2018.
[49]Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068 , 2022.
[50]Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu
Chen, Jason D Lee, Wotao Yin, Mingyi Hong, et al. Revisiting zeroth-order optimization for
memory-efficient llm fine-tuning: A benchmark. arXiv preprint arXiv:2402.11592 , 2024.
[51]Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, and Ivor W Tsang. Second-order
fine-tuning without pain for llms: A hessian informed zeroth-order optimizer. arXiv preprint
arXiv:2402.15173 , 2024.
15

--- PAGE 16 ---
[52]Shaochen Zhong, Guanqun Zhang, Ningjia Huang, and Shuai Xu. Revisit kernel pruning with
lottery regulated grouped convolutions. In International Conference on Learning Representations ,
2021.
[53]Shaochen Henry Zhong, Zaichuan You, Jiamu Zhang, Sebastian Zhao, Zachary LeClaire, Zirui
Liu, Daochen Zha, Vipin Chaudhary, Shuai Xu, and Xia Hu. One less reason for filter pruning:
Gaining free adversarial robustness with structured grouped kernel pruning. Advances in Neural
Information Processing Systems , 36, 2024.
[54]Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song
Han. Pockengine: Sparse and efficient fine-tuning in a pocket. In Proceedings of the 56th Annual
IEEE/ACM International Symposium on Microarchitecture , pages 1381–1394, 2023.
16

--- PAGE 17 ---
Appendix
In Section A we describe all notations used in this paper. In Section B, we include the assumption
and exact proof on the convergence rate (Theorem 1). In Section C, we describe all details in our
experiments and provide a high-level recommendation on how to efficiently implement our sensitive
sparse ZO fine-tuning in forward passes of linear layers with existing quantization methods or training
/ inference workflow.
A Notations
We present the notations used in this work as follows.
Table 2: Notations
Term/Symbol Explanation
f loss function
t optimization timestep t
(xt, yt) a data example sampled at timestep tas a pair of input vector and training target
wt∈Rdweight/parameter vector at optimization timestep t
f(w; (x, y)) training loss of wevaluated at a single data example (x, y)
F(w) full-batched training loss of w
ϵ a small perturbation scaling constant (close to 0)
zt∈Rdrandom Gaussian perturbation vector sampled at timestep t
ˆg(w,(x, y),z) estimated ZO surrogate gradient for wwith a data example (x, y)and a sampled Gaussian
perturbation vector z(Definition 1)
ηt learning rate for ZO-SGD optimizer (Definition 2) at timestep t
mk∈ {0,1}da sensitive sparse mask with knonzero entries (Definition 3)
mk,t∈ {0,1}da sensitive sparse mask with knonzero entries, and it is derived at optimization timestep t.
˜Id,mk=mkm⊤
k Rank-1 product of mkused in the proof. Notice that ˜Id,mkis also equal to the identity matrix Id
with main diagonal masked by mk.
¯zt=zt⊙mk a sampled Gaussian perturbation vector ztat timestep tthat is masked by mk. Notice that ¯zis
equivalent as being sampled from N(0d,˜Id,mk)
1d a vector of size dwith all entries equal to 1
Tr trace operation
Q(w) parameter vector wthat is quantized by Q
F (true) Fisher information matrix
ˆF empirical Fisher information matrix
pLLM LLM as a probabilistic model
pD true data distribution
wsparse =w⊙mksensitive parameters with positions as the nonzero entries sensitive sparse mask mk(Equation 11)
L Lipschitz constant in Assumption 2
µ PL condition number in Assumption 3
σ2stochastic gradient error term in Assumption 1
WK Weight matrix of linear projection for the key embedding matrix Kin attention layers
WV Weight matrix of linear projection for the value embedding matrix Vin attention layers
17

--- PAGE 18 ---
B Theoretical Convergence Rate
B.1 Assumptions
We start with listing standard assumptions in nonconvex optimization literature:
Assumption 1 (Bounded stochastic gradient errors ).For any data example (x, y)∈ Dand
for any w∈Rd, denote the full-batched loss function F(w) =E(x,y)∈Df(w; (x, y)), we have
∥∇wf(w; (x, y))− ∇ wF(w)∥2≤σ2. (14)
Assumption 2 (Lipschitz smoothness ).We assume that f(w,x)isL-Lipschitz smooth ( L >0):
for any w,w′∈Rd,
∥∇wf(w; (x, y))− ∇ wf(w′; (x, y))∥ ≤L∥w−w′∥. (15)
Assumption 3 (PL inequality ).We assume that F(w)fulfills the Polyak-Lojasiewicz (PL)
condition: there exists some µ >0, for any w∈Rd
1
2∥∇wF(w)∥2≥µ(F(w)− F∗),F∗is the minimum value F∗= inf
wF(w).(16)
Inspired by Figure 7, we would assume the sensitive parameters of ware sparse.
Assumption 4 (Sensitive parameters are sparse ).We assume at timestep t∃mt∈ {0,1}dwith
the number of nonzero entries as k,∃c∈[0,1]such that
∥mt⊙ ∇ wf(wt; (xt, yt))∥2=c∥∇wf(wt; (xt, yt))∥2.
Here we assume c≫k/d.2
B.2 Proof for Equation 9, Theorem 1
Lemma 1 (Sparse ZO surrogate gradient covariance ).
E¯zˆg(w,(x, y),¯z)ˆg(w,(x, y), y),¯z)⊤
=E¯zi[¯zi¯z⊤
i
(mk⊙ ∇ wf(w; (x, y)))(mk⊙ ∇ wf(w; (x, y)))⊤
¯zi¯z⊤
i]
= 2
(mk⊙ ∇ wf(w; (x, y)))(mk⊙ ∇ wf(w; (x, y)))⊤
+∥mk⊙ ∇ wf(w; (x, y))∥2˜Id,mk
= 2(mkm⊤
k)(∇wf(w; (x, y))∇wf(w; (x, y))⊤) +c∥∇wf(w; (x, y))∥2˜Id,mk
= 2˜Id,mk(∇wf(w; (x, y))∇wf(w; (x, y))⊤) +c∥∇wf(w; (x, y))∥2˜Id,mk
Proof for Equation 9, Theorem 1 .Denote ˜Id,mk∈Rd×das the identity matrix Id∈Rd×dwith
the diagonal masked by mk∈ {0,1}dwith knonzero entries:
diag(˜Id,mk) =m⊙1d,∀i̸=j,˜Id,mk(i, j) =Id(i, j) = 0
2From Figure 7, we know that for c∼0.5, we only need k/d∼0.001. In this case k/c∼0.002d.
18

--- PAGE 19 ---
f(wt+1,xt)≤f(wt; (xt, yt)) +⟨∇f(wt; (xt, yt)),wt+1−wt⟩+L
2∥wt+1−wt∥2
≤f(wt; (xt, yt))−ηt⟨∇f(wt; (xt, yt)),ˆg(wt,xt,¯zt)⟩+Lη2
t
2∥ˆg(wt,xt,¯zt)∥2
E¯zf(wt+1,xt)≤E¯zf(wt; (xt, yt))−ηtE¯z∥mk,t⊙ ∇f(wt; (xt, yt))∥2+Lη2
t
2E¯z∥ˆg(wt,xt,¯z)∥2
E¯zf(wt+1,xt)≤E¯zf(wt; (xt, yt))−cηtE¯z∥∇f(wt; (xt, yt))∥2+Lη2
t
2c(k+ 2)E¯z∥∇wf(wt; (xt, yt))∥2
E¯z,(x,y)F(wt+1)≤E¯z,(x,y){F(wt)−cηt∥∇wF(wt)∥2+cσ2ηt+Lη2
t
2c(k+ 2)∥∇wF(wt)∥2+Lη2
t
2c(k+ 2)σ2}
E¯z,(x,y)F(wt+1)≤E¯z,(x,y){F(wt)−
cηt−Lη2
t
2c(k+ 2)
∥∇wF(wt)∥2+
cσ2ηt+Lη2
t
2c(k+ 2)σ2
}
Denote α=Lc(k+ 2), we will have
E¯z,(x,y)F(wt+1)≤E¯z,(x,y){F(wt)−ηt
c−α
2ηt
∥∇wF(wt)∥2+
cσ2ηt+α
2σ2η2
t
}
Setηt<c
α=1
L(k+ 2), we have
E¯z,(x,y)F(wt+1)≤E¯z,(x,y){F(wt)−cηt
2∥∇F (wt)∥2+
cσ2ηt+α
2σ2η2
t
}
If we apply our sparse ZO update rule recursively for Tsteps,
1
TT−1X
t=0E¯z,(x,y)∥∇wF(wt)∥2≤2α
Tc2(F(w0)− F∗) +1
TT−1X
t=0
cσ2ηt+α
2σ2η2
t
cηt
2
≤2α
Tc2(F(w0)− F∗) + (2 σ2+σ2)
≤2L(k+ 2)
c1
T(F(w0)− F∗) + 3σ2
≤Ok
c·L
T
(F(w0)− F∗) + 3σ2
B.3 Proof for Equation 10, Theorem 1
Lemma 2 (Sparse ZO surrogate gradient norm ).
E¯z[∥ˆg(wt,xt,¯zt)∥2] = (2 + k)c∥∇wf(w,x)∥2
Proof for Equation 10, Theorem 1 .Denote κas the condition number κ=µ
L.
19

--- PAGE 20 ---
E¯z,(x,y)F(wt+1)≤E¯z,(x,y){F(wt)−cηt
2∥∇F (wt)∥2+
cσ2ηt+α
2σ2η2
t
}
≤E¯z,(x,y){F(wt)−cµηt(F(wt)− F∗) +
cσ2ηt+α
2σ2η2
t
}
E¯z,(x,y){F(wt+1)− F∗} ≤E¯z,(x,y){(F(wt)− F∗)−cµηt(F(wt)− F∗) +
cσ2ηt+α
2σ2η2
t
}
E¯z,(x,y){F(wt+1)− F∗} ≤E¯z,(x,y){(F(wt)− F∗)−cµηt(F(wt)− F∗) +
cσ2ηt+α
2σ2η2
t
}
Plugging in ηt≤c
αand applying recursively for Titerations.
E¯z,(x,y){F(wT)− F∗} ≤(1−cκ
(k+ 2))T(F(w0)− F∗) +3σ2c2
2α
≤(1−cκ
(k+ 2))T(F(w0)− F∗) +3σ2c
2L(k+ 2)
≤
1−Oµ
L·c
kT
(F(w0)− F∗) +3σ2c
2L(k+ 2)
20

--- PAGE 21 ---
C Supplementary Experiment Details
C.1 On-Device Memory Constraints
We include a table of common memory constraints imposed by edge or mobile devices as Table 3.
We can find that a wide range of these devices impose a memory constraint of 8 GiBas our main
constraint that we consider when we develop our on-device personalization recipe in Section 3.5.
Table 3: Device memory of some mobile devices or consumer-graded GPUs.
Devices Memory
Nvidia GeForce GTX 1080 Ti 11 GiB
Nvidia GeForce RTX 3060 Ti 8 GiB
Nvidia Jetson TX2 8 GiB
OPPO Find X7 Ultra [18] 12 GiB
Samsung Galaxy S10 with Mali-G76 GPU [10] 8 GiB
C.2 Gradient Sparsity During LLM Fine-Tuning
In Figure 2, we explore the FO gradient sparsity of Llama2-7B during fine-tuning (at Epoch 5). Here
we follow the identical setting and plot the FO gradient sparsity for Llama2-7B, Mistral-7B, and
OPT-6.7B during epoch 1, 5, and 10 (end of fine-tuning).
We observe that the gradient sparsity is exhibited throughout the fine-tuning with slightly
increasing towards the end. OPT-6.7B which uses ReLU as the activation function would demonstrate
greater sparsity across tasks compared with Llama2-7B and Mistral-7B which uses SwiGLU and
SiLU respectively. Nevertheless, the gradient sparsity pattern holds across architectures, tasks, and
fine-tuning time in general.
21

--- PAGE 22 ---
0.00.51.0RTE, Epoch 1
mean±std
0.5
0.2RTE, Epoch 5 RTE, End of FT
0.00.51.0WiC, Epoch1 WiC, Epoch5 WiC, End of FT
10−510−410−310−210−1100
Ratio0.00.51.0COPA, Epoch 1
10−510−410−310−210−1100
RatioCOPA, Epoch 5
10−510−410−310−210−1100
RatioCOPA, End of FTCumulative normalized sum of [∇F(w)]2
i(a) Llama2-7B
0.00.51.0RTE, Epoch 1
mean±std
0.5
0.2RTE, Epoch 5 RTE, End of FT
0.00.51.0WiC, Epoch1 WiC, Epoch5 WiC, End of FT
10−510−410−310−210−1100
Ratio0.00.51.0COPA, Epoch 1
10−510−410−310−210−1100
RatioCOPA, Epoch 5
10−510−410−310−210−1100
RatioCOPA, End of FTCumulative normalized sum of [∇F(w)]2
i
(b) Mistral-7B
0.00.51.0RTE, Epoch 1
mean±std
0.5
0.2RTE, Epoch 5 RTE, End of FT
0.00.51.0WiC, Epoch1 WiC, Epoch5 WiC, End of FT
10−510−410−310−210−1100
Ratio0.00.51.0COPA, Epoch 1
10−510−410−310−210−1100
RatioCOPA, Epoch 5
10−510−410−310−210−1100
RatioCOPA, End of FTCumulative normalized sum of [∇F(w)]2
i
(c) OPT-6.7B
Figure 7: Cumulative normalized sum of coordinate-wise gradient square [∇F(w)]2
iof linear layers
for Llama2-7B (subfigure 7a), Mistral-7B (subfigure 7b), and OPT-6.7B (subfigure 7c) across RTE,
WiC, and COPA tasks during FO-SGD fine-tuning. For each linear layer, we first sort parameters by
the decreasing order of their gradient square value [∇F(w)]2
i, i∈[dlayer], and we take the cumulative
sum and normalize it to draw a blue curve, and the red-shaded region is the mean ±std of all blue
curves.22

--- PAGE 23 ---
C.3Transferability of Gradient Features from Pre-Training Datasets to Down-
stream Tasks
In Figure 3, we explore the transferability of gradient features from pre-training datasets (C4) to
downstream tasks, and here we will also validate this phenomenon across models, as shown in Figure 8.
As there are nosolid lines (top-(1e-2,1e-3,1e-4)) parameters with C4 gradient entries prior to fine-
tuning) vanish to 0, we know the transferability of gradient features from C4 datasets to downstream
datasets hold across models and downstream tasks. In this case, sensitive parameters determined
from C4 gradients would still be similar to sensitive parameters determined from downstream
task-specific gradients across models.
Before FTEpoch 1 Epoch 5End of FT0.00.20.40.60.81.0Cumulative normalizd sum of [∇F(w)]2
i
RTE
10−2
10−3
10−4task grad, dyn.
task grad, static
C4 grad, static
Before FTEpoch 1 Epoch 5End of FT
WiC
Before FTEpoch 1 Epoch 5End of FT
COPA
(a) Llama2-7B
Before FTEpoch 1 Epoch 5End of FT0.00.20.40.60.81.0Cumulative normalizd sum of [∇F(w)]2
i
RTE
10−2
10−3
10−4task grad, dyn.
task grad, static
C4 grad, static
Before FTEpoch 1 Epoch 5End of FT
WiC
Before FTEpoch 1 Epoch 5End of FT
COPA
(b) Mistral-7B
Before FTEpoch 1 Epoch 5End of FT0.00.20.40.60.81.0Cumulative normalizd sum of [∇F(w)]2
i
RTE
10−2
10−3
10−4task grad, dyn.
task grad, static
C4 grad, static
Before FTEpoch 1 Epoch 5End of FT
WiC
Before FTEpoch 1 Epoch 5End of FT
COPA
(c) OPT-6.7B
Figure 8: Cumulative normalized gradient square values of Llama2-7B (subfigure 8a), Mistral-7B
(subfigure 8b), and OPT-6.7B (subfigure 8c)’s linear layers during FO fine-tuning. For a given
model and training checkpoint, we report the average value across all linear layers as a line in each
subfigure. For each line, the colors represent the fraction of parameters (1e-2,1e-3,1e-4) and the line
style represents the category. “task grad, dyn.” refers to the sensitive parameters selected at the
given timestep (x-axis), and “task grad, static” refers to the sensitive parameters selected before
fine-tuning. “C4 grad, static” refers to the sensitive parameters selected with gradients taken from
causal language modeling on C4 datasets, and we keep it unchanged during fine-tuning.
23

--- PAGE 24 ---
C.4 Hyperparameters in Experiments
For all experiments, we use 20,000 training steps with ZO-SGD optimizer (Definition 2). We will save
a model checkpoint every 500 steps, and load the checkpoint with the lowest loss on the validation set
at the end of the training, and report its test set accuracy as result. Usually, the training/validation
set will be sampled from the original dataset with size 1000/500 respectively and the test set is of
sizemin(1000 ,|original test set |), except for CB and COPA that we use 100 for the validation set
size. For all ZO experiments (Table 4 and Table 5), we use batch size of 16. This experiment setting
is identical to Malladi et al. [27].
Table 4: The chosen hyperparameters for experiments in Table 1. We repeat each hyperparameters
for 3 random trials and report the average and standard deviation in Table 1.
(a)Llama2-7B
Methods SST-2 RTE CB BoolQ WSC WiC COPA
Q, ZO Sensitive (C4, static) (ϵ=1e-3) 5e-7 1e-6 1e-6 1e-6 5e-7 1e-6 1e-6
LoRA ( ϵ=1e-3) 1e-5 5e-5 1e-5 2e-5 1e-5 2e-5 1e-5
Prefix ( ϵ=1e-2) 1e-4 2e-4 5e-4 5e-4 1e-4 5e-4 2e-4
ZO Sensitive (task, static) (ϵ=1e-3) 5e-7 1e-6 1e-6 1e-6 1e-6 1e-6 2e-6
Random (static) ( ϵ=1e-3) 2e-4 5e-4 2e-4 5e-4 2e-4 5e-4 5e-4
Full fine-tuning ( ϵ=1e-3) 5e-7 5e-7 5e-7 5e-7 2e-7 5e-7 5e-7
ICL (#examples) 16 16 16 8 16 8 8
(b)Mistral-7B
Q, ZO Sensitive (C4, static) (ϵ=1e-4) 2e-8 5e-8 2e-8 2e-8 1e-8 2e-8 2e-8
LoRA ( ϵ=1e-4) 2e-6 5e-6 2e-6 2e-6 2e-6 2e-6 2e-6
Prefix ( ϵ=1e-3) 1e-3 2e-3 1e-3 1e-2 5e-4 1e-3 5e-4
ZO Sensitive (task, static) (ϵ=1e-4) 5e-8 5e-8 2e-8 2e-8 2e-8 2e-8 2e-8
Random (static) ( ϵ=1e-4) 1e-5 2e-6 5e-6 1e-5 1e-6 2e-6 2e-5
Full fine-tuning ( ϵ=1e-4) 2e-8 2e-8 1e-8 1e-8 1e-8 1e-8 2e-8
ICL (#examples) 4 8 4 16 4 4 8
(c)OPT-6.7B
Q, ZO Sensitive (C4, static) (ϵ=1e-3) 2e-7 5e-7 5e-7 5e-7 2e-7 5e-7 2e-7
LoRA ( ϵ=1e-3) 1e-5 2e-5 1e-5 2e-5 1e-5 2e-5 2e-5
Prefix ( ϵ=1e-2) 2e-3 1e-2 1e-3 5e-3 5e-3 1e-2 5e-3
ZO Sensitive (task, static) (ϵ=1e-3) 2e-7 5e-7 5e-7 2e-7 2e-7 5e-7 2e-7
Random (static) ( ϵ=1e-3) 1e-4 5e-5 2e-5 5e-5 2e-4 5e-5 5e-5
Full fine-tuning ( ϵ=1e-3) 2e-7 2e-7 2e-7 2e-7 2e-7 2e-7 5e-7
ICL (#examples) 16 4 16 16 16 8 16
Our hyperparameters (learning rate η, perturbation scaling constant ϵ, and the number of ICL
examples) for Table 1 is reported in Table 4 for reproducibility. We use constant ηandϵthroughout
our experiments. We also report the chosen hyperparameter for Figure 5a and Figure 5b in Table 5.
For LoRA, we always add to all linear layers with r= 8andα= 16, and for Prefix Tuning, we
always add to WKandWVwith length as 5, as what Malladi et al. [27] uses.
24

--- PAGE 25 ---
Table 5: The chosen hyperparameters for experiments in Figure 5a and Figure 5b. We repeat each
hyperparameters for 3 random trials and report the average to draw a line in Figure 5a and Figure 5b,
and we use Llama2-7B for all experiments. For each subtable, we include the fraction to optimize on
its header and report the chosen learning rate on each cell.
(a)RTE
Methods 1e-5 1e-4 1e-3 1e-2 1e-1
Sensitive (C4, static) (ϵ=1e-3) 1e-5 1e-6 1e-6 1e-6 1e-6
Sensitive (task-specific, static) (ϵ=1e-3) 1e-5 1e-6 1e-6 1e-6 1e-6
Sensitive (task-specific, dynamic) (ϵ=1e-3) 1e-5 1e-6 1e-6 1e-6 1e-6
Random (static) ( ϵ=1e-3) 2e-2 5e-3 5e-4 5e-5 5e-5
Random (dynamic) ( ϵ=1e-3) 2e-2 5e-3 2e-4 5e-5 5e-6
Weight outliers (static) ( ϵ=1e-3) 2e-3 1e-3 2e-4 5e-5 1e-5
(b)WiC
Methods 1e-5 1e-4 1e-3 1e-2 1e-1
Sensitive (C4, static) (ϵ=1e-3) 1e-5 2e-6 1e-6 1e-6 1e-6
Sensitive (task-specific, static) (ϵ=1e-3) 1e-5 2e-6 1e-6 1e-6 1e-6
Sensitive (task-specific, dynamic) (ϵ=1e-3) 1e-5 2e-6 1e-6 1e-6 1e-6
Random (static) ( ϵ=1e-3) 2e-2 5e-3 5e-4 5e-5 5e-6
Random (dynamic) ( ϵ=1e-3) 2e-2 5e-3 5e-4 5e-5 5e-6
Weight outliers (static) ( ϵ=1e-3) 1e-3 5e-4 2e-4 1e-4 2e-5
(c)COPA
Methods 1e-5 1e-4 1e-3 1e-2 1e-1
Sensitive (C4, static) (ϵ=1e-3) 5e-6 1e-6 1e-6 1e-6 5e-7
Sensitive (task-specific, static) (ϵ=1e-3) 5e-6 2e-6 2e-6 1e-6 1e-6
Sensitive (task-specific, dynamic) (ϵ=1e-3) 5e-6 1e-6 1e-6 1e-6 1e-6
Random (static) ( ϵ=1e-3) 1e-2 2e-3 5e-4 5e-5 5e-6
Random (dynamic) ( ϵ=1e-3) 2e-3 1e-3 2e-4 2e-5 2e-6
Weight outliers (static) ( ϵ=1e-3) 1e-3 5e-4 5e-4 1e-4 1e-5
25

--- PAGE 26 ---
C.5 Task-Specific Prompts in Experiments
We describe our task templates in Table 6.
Table 6: Task templates for all experiments. On the left column we include the task name and the
model name, and on the right column we describe the exact prompt with answer candidates.
Task Prompts
SST-2
(Llama2-7B)### Sentence: <text> ### Sentiment: nega-
tive/positive
SST-2
(Mistral-7B, OPT-6.7B)<text> It was terrible/great
RTE
(Llama2-7B)Suppose "<premise>" Can we infer that "<hypothesis>"?
Yes or No? Yes/No
RTE
(Mistral-7B, OPT-6.7B)<premise>
Does this mean that "<hypothesis>" is true? Yes or No?
Yes/No
CB
(Llama2-7B, Mistral-7B, OPT-6.7B)Suppose <premise> Can we infer that "<hypothesis>"?
Yes, No, or Maybe?
Yes/No/Maybe
BoolQ
(Llama2-7B)<passage> <question>? Yes/No
BoolQ
(Mistral-7B, OPT-6.7B)<passage> <question>?
Yes/No
WSC
(Llama2-7B, Mistral-7B, OPT-6.7B)<text>
In the previous sentence, does the pronoun "<span2>"
refer to <span1>? Yes or No?
Yes/No
WiC
(Llama2-7B, Mistral-7B, OPT-6.7B)Does the word "<word>" have the same meaning in these
two sentences? Yes, No?
<sent1>
<sent2>
Yes/No
COPA
(Llama2-7B, Mistral-7B, OPT-6.7B)<premise> so/because <candidate>
26

--- PAGE 27 ---
C.6 Implementation of Sparse Operations in Linear Layers
Linear layers in LLMs often contribute most parameters [14]. Since from Equation 11 we know
wsparse =w⊙mk,wdense =w⊙(1d−mk),w=wsparse +wdense (17)
and since wdensewould have same shape (and same computational intensities) as w, we need
to improve wall-clock time efficiency of wsparsexto improve the computational efficiency of linear
layers after extracting the sparse parameters. In this case, we would have two different methods to
implement the forward pass of linear layers (with induced sparse operation colored in red):
wx=wdensex+wsparsex (18)
= SparseAddMM(DenseMM( wdense,x),wsparse,x)faster with token generation (19)
= (wdense+wsparse)x (20)
= DenseMM(SparseAdd( wsparse,wdense),x) faster with ZO training (21)
0.01% 0.1% 1%
Fraction to optimize0.010.11.010.0Sec / forwardTraining
(A6000)
SparseAdd
SparseAddMM
0.01% 0.1% 1%
Fraction to optimizeTraining
(A100)
0.01% 0.1% 1%
Fraction to optimize00.10.20.3Sec / tokenInference
(A6000)
0.01% 0.1% 1%
Fraction to optimizeInference
(A100)
Figure 9: Time of SparseAdd (Equation 21) versus SparseAddMM (Equation 19) in Llama2-7B ZO
training forward & inference. In subfigure 1 and 3, we use Nvidia RTX A6000 and Intel Xeno Gold
6342 CPUs, with PyTorch version 2.2, HuggingFace version 4.36, and CUDA 12.2. In subfigure 2
and 4, we use Nvidia A100-SXM4 (40 GiB) and AMD EPYC 7543P 32-Core CPU with PyTorch
version 2.1, HuggingFace version 4.38.2, and CUDA 12.2. We use Flash Attention 2 [ 5] for all 4
subfigures.
The specific choice of employing Equation 19 or Equation 21 needs careful consideration and
benchmarking, but here we can provide a general guideline based on the size of input vector (or
arithmetic intensity) and potential integration with weight quantization method:
Size of input vectors xand arithmetic intensity. wsparsexin Equation 19 would have a
computational dependency over x. During large-batched ZO training, xwould be large enough such
that Equation 19 would induce large computational overhead, as shown in subfigure 1 of Figure 9. In
contrast, the computational complexity of Equation 21 is independent ofxand when xis large, we
would expect Equation 21 is muchfaster than Equation 19. As an example, we use sequence length
of 512 and batch size 16 sampled from WikiText-2 dataset [ 29] as a representative computational
intensity for ZO training in subfigures 1 and 2 in Figure 9.
However, during autoregressive token generation, on each step we would only append a single
tokento the previously cached embeddings, and in this case xis small and computing wdense+wsparse
is generally not worthwhile, especially given that wsparseis already sparse. This is also illustrated in
27

--- PAGE 28 ---
subfigure 3 and 4 in Figure 9. However, we note that the specific implementation choice is hardware
and task dependent and requires thorough benchmarking and we will leave it as a future work.
We recommend using Equation 21 during large-batched ZO training and Equation 19
during small-batched autoregressive token generation.
In light of this observation, in our Figure 1, we implement both “SparseAdd” and “SparseAddMM”
methods for “Sensitive (0.1%)” and “Random (10%)”. For each method we report the lowesttime
out of these 2 implementations: for “Sensitive (0.1%)” training and “Random (10%)” training and
inference, we use “SparseAdd” approach. For “Sensitive (0.1%)” inference, we use the “SparseAddMM”
approach.
Integration with weight quantization method. Weight quantization algorithms can be cat-
egorized into 2 categories: uniform quantization method and non-uniform quantization method. For
uniform quantization method, Xi et al. [45]indicates that we could use integer matrix multiplication
to compute Q(wdense)xefficiently withoutfirst dequantizing Q(wdense)to 16 bits. However, this
creates difficulty on our “SparseAdd” approach as we will violate the constraint of uniformly-spaced
quantization bins by computing SparseAdd (Q(wdense)+wsparse ). In this case, we also have 3 different
implementations:
Q(w)x∼Q(wdense)x+wsparsex (22)
= SparseAddMM
Dequantize 
IntMM( Q(wdense),x)
,wsparse,x
fits with integer matmul
(23)
= SparseAddMM
Dequantize (Q(wdense)),x,wsparse
(24)
= (Dequantize( Q(wdense))+wsparse)x (25)
= DenseMM(SparseAdd ( wsparse,Dequantize( Q(wdense)),x) (26)
Equation 23 would compute IntMM (Q(wdense),x)beforedequantizing it to 16 bits. This would
make “SparseAdd” approach infeasible and we can only employ “SparseAddMM” approach in this
case. Notice that both Equation 24 and Equation 26 would still dequantize Q(wdense)first and the
choice of implementation would follow into our discussion of input vector size xin last paragraph.
We will leave a practical implementation and thorough benchmarking into a future work.
We recommend using Equation 23 when we use efficient integer matmul to compute
Q(wdense )xand in other cases, using Equation 24 or Equation 26 follows our previous
recommendation based on the size of input vectors.
C.7 Hardware, Platform, Libraries, and Other Details for Benchmarking
Figure 1, Figure 6, and Figure 9 (subfigure 1 and 3) are trained and evaluated on an internal
cluster with 8 Nvidia RTX A6000 GPUs and 2 Intel Xeon Gold 6342 CPUs, with PyTorch version
2.2, HuggingFace version 4.36, and CUDA 12.2. In subfigure 2 and 4 in Figure 9, we use Nvidia
A100-SXM4 (40 GiB) and AMD EPYC 7543P 32-Core CPU with PyTorch version 2.1, HuggingFace
version 4.38.2, and CUDA 12.2. We use Flash Attention 2 [ 5] throughout our experiments, and the
base model for benchmarking is always Llama2-7B with Float16 datatype.
In Figure 1 and Figure 9, we use sequence length of 512 and batch size 16 sampled from WikiText-
2 dataset [ 29] as a representative computational intensity for ZO training, and for inference we
generate 128 tokens with top- p(p= 0.9) sampling from the prompt “ Please describe the effect of
sparse zeroth-order optimization methods on memory-efficient LLM fine-tuning: ”.
28
