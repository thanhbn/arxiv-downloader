# 2209.14624.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/no-backprop/2209.14624.pdf
# Kích thước file: 1007632 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
1
Liệu Độ Phức Tạp Có Cần Thiết Cho Việc Cắt Tỉa Mạng Nơ-ron?
Một Nghiên Cứu Trường Hợp về Cắt Tỉa Độ Lớn Toàn Cục
Manas Gupta, Efe Camci, Vishandi Rudy Keneta, Abhishek Vaidyanathan, Ritwik Kanodia, Chuan-Sheng Foo,
Min Wu và Jie Lin
Tóm tắt —Cắt tỉa mạng nơ-ron đã trở nên phổ biến trong thập kỷ qua khi được chứng minh rằng một số lượng lớn các trọng số có thể được loại bỏ một cách an toàn khỏi các mạng nơ-ron hiện đại mà không làm giảm độ chính xác. Nhiều phương pháp cắt tỉa đã được đề xuất kể từ đó, mỗi phương pháp đều khẳng định tốt hơn so với các nghiên cứu trước đó, tuy nhiên, với chi phí là các phương pháp cắt tỉa ngày càng phức tạp. Các phương pháp này bao gồm việc sử dụng điểm số quan trọng, nhận phản hồi thông qua lan truyền ngược hoặc có các quy tắc cắt tỉa dựa trên kinh nghiệm trong số các phương pháp khác. Trong nghiên cứu này, chúng tôi đặt câu hỏi liệu xu hướng giới thiệu độ phức tạp này có thực sự cần thiết để đạt được kết quả cắt tỉa tốt hơn hay không. Chúng tôi so sánh các kỹ thuật SOTA này với một đường cơ sở cắt tỉa đơn giản, cụ thể là Cắt Tỉa Độ Lớn Toàn Cục (Global MP), xếp hạng các trọng số theo thứ tự độ lớn của chúng và cắt tỉa những trọng số nhỏ nhất. Đáng ngạc nhiên, chúng tôi thấy rằng Global MP thuần túy hoạt động rất tốt so với các kỹ thuật SOTA. Khi xem xét sự đánh đổi giữa độ thưa-độ chính xác, Global MP hoạt động tốt hơn tất cả các kỹ thuật SOTA ở mọi tỷ lệ độ thưa. Khi xem xét sự đánh đổi giữa FLOPs-độ chính xác, một số kỹ thuật SOTA vượt trội hơn Global MP ở các tỷ lệ độ thưa thấp hơn, tuy nhiên, Global MP bắt đầu hoạt động tốt ở các tỷ lệ độ thưa cao và hoạt động rất tốt ở các tỷ lệ độ thưa cực cao. Hơn nữa, chúng tôi thấy rằng một vấn đề chung mà nhiều thuật toán cắt tỉa gặp phải ở tỷ lệ độ thưa cao, cụ thể là sự sụp đổ lớp, có thể dễ dàng được khắc phục trong Global MP. Chúng tôi khám phá lý do tại sao sự sụp đổ lớp xảy ra trong mạng và cách nó có thể được giảm thiểu trong Global MP bằng cách sử dụng một kỹ thuật gọi là Ngưỡng Tối Thiểu. Chúng tôi thể hiện các phát hiện trên trên nhiều mô hình khác nhau (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 và FastGRNN) và nhiều bộ dữ liệu (CIFAR-10, ImageNet và HAR-2). Mã nguồn có sẵn tại https://github.com/manasgupta-1/GlobalMP.
Từ khóa chỉ mục —Cắt Tỉa Độ Lớn Toàn Cục, Mạng Nơ-ron Hiệu Quả, Độ Thưa, ImageNet.

I. GIỚI THIỆU
Việc mở rộng quy mô kích thước của mạng nơ-ron đang trở thành một cách phổ biến để tăng hiệu suất mô hình [1], [2], [3]. Tuy nhiên, điều này gây ra chi phí đáng kể cho môi trường [4] và khiến việc triển khai trên các thiết bị biên trở nên khó khăn [5]. Do đó, việc cắt tỉa mạng nơ-ron đã nổi lên như một công cụ thiết yếu để giảm kích thước của các mạng nơ-ron hiện đại. Các phương pháp mới đã sử dụng vô số kỹ thuật cắt tỉa bao gồm các phương pháp dựa trên gradient, độ nhạy hoặc phản hồi từ hàm mục tiêu, các phép đo khoảng cách hoặc độ tương tự, các kỹ thuật dựa trên chính quy hóa, trong số các phương pháp khác. Các kỹ thuật cắt tỉa hiện đại (SOTA) sử dụng các quy tắc phức tạp như cắt tỉa lặp đi lặp lại và tái tăng trưởng các tham số trọng số sử dụng quy tắc kinh nghiệm sau mỗi vài trăm vòng lặp, như trong DSR [6]. SM [7] sử dụng momentum thưa thớt có lợi từ các gradient được làm mượt theo hàm mũ (momentum) để tìm các lớp và trọng số giảm lỗi và sau đó phân phối lại các trọng số đã cắt tỉa qua các lớp sử dụng độ lớn momentum trung bình của mỗi lớp. Đối với mỗi lớp, momentum thưa thớt tăng trưởng các trọng số sử dụng độ lớn momentum của các trọng số có giá trị bằng không. Một kỹ thuật SOTA phổ biến khác, RigL [8], cũng cắt tỉa và tái tăng trưởng trọng số một cách lặp đi lặp lại sau mỗi vài vòng lặp. Họ sử dụng uniform hoặc Erdos-Renyi-Kernel (ERK) để cắt tỉa các kết nối và tái tăng trưởng các kết nối dựa trên các gradient có độ lớn cao nhất. Trong số các kỹ thuật gần đây, DPF [9] sử dụng phân bổ động của mẫu độ thưa và kết hợp tín hiệu phản hồi để tái kích hoạt các trọng số bị cắt tỉa sớm, trong khi STR [10] sử dụng Tái Tham Số Hóa Ngưỡng Mềm và sử dụng lan truyền ngược để tìm tỷ lệ độ thưa cho mỗi lớp.

Mặc dù có số lượng lớn các thuật toán cắt tỉa mới được đề xuất, lợi ích hữu hình của nhiều thuật toán trong số đó vẫn còn đáng ngờ. Ví dụ, gần đây đã được chỉ ra rằng nhiều sơ đồ cắt tỉa tại khởi tạo (PAI) không hoạt động tốt như mong đợi [11]. Trong bài báo đó, thông qua một số thí nghiệm, được chỉ ra rằng các sơ đồ PAI này thực sự không tốt hơn việc cắt tỉa ngẫu nhiên, đây là một trong những đường cơ sở cắt tỉa ngây thơ nhất mà không có độ phức tạp nào liên quan. Phát hiện này thực sự đặt ra một câu hỏi khác trong tâm trí chúng tôi: nếu một PAI được thiết kế tốt thậm chí không thể bằng hiệu suất của việc cắt tỉa ngẫu nhiên, liệu các phương pháp cắt tỉa đơn giản như cắt tỉa toàn cục hoặc các biến thể của chúng có thể vượt trội hơn các thuật toán hiện có khác không? Trong nghiên cứu này, chúng tôi đặt câu hỏi về xu hướng đề xuất các thuật toán cắt tỉa ngày càng phức tạp và đánh giá liệu độ phức tạp như vậy có thực sự cần thiết để đạt được kết quả vượt trội hay không. Chúng tôi so sánh các kỹ thuật cắt tỉa hiện đại (SOTA) phổ biến với một đường cơ sở cắt tỉa ngây thơ, cụ thể là Cắt Tỉa Độ Lớn Toàn Cục (Global MP). Global MP xếp hạng tất cả các trọng số trong một mạng nơ-ron theo độ lớn của chúng và cắt tỉa những trọng số nhỏ nhất (Hình 1).

Mặc dù đơn giản, Global MP chưa được phân tích và đánh giá một cách toàn diện trong tài liệu. Mặc dù một số nghiên cứu trước đã sử dụng Global MP như một đường cơ sở [12], [13], [14], [15], [16], [17], họ đã bỏ lỡ việc tiến hành các thí nghiệm nghiêm ngặt với nó; ví dụ, trong các thiết lập của cả cắt tỉa dần dần và một lần hoặc so sánh nó với SOTA. Tương tự, nhiều bài báo SOTA không sử dụng Global MP để đánh giá chuẩn và bỏ lỡ việc nắm bắt hiệu suất đáng chú ý của nó [8], [10], [18], [2], [19]. Chúng tôi lấp đầy khoảng trống này trong việc đánh giá hiệu quả của Global MP và chứng minh hiệu suất vượt trội của nó trong nhiều điều kiện thí nghiệm.

--- TRANG 2 ---
2
Chúng tôi chỉ ra rằng về mặt đánh đổi giữa độ thưa và độ chính xác, Global MP luôn vượt trội hơn tất cả các kỹ thuật hiện đại (SOTA) qua các tỷ lệ độ thưa khác nhau. Về mặt đánh đổi giữa FLOPs và độ chính xác, một số kỹ thuật SOTA thể hiện hiệu suất vượt trội hơn Global MP ở các tỷ lệ độ thưa thấp hơn. Tuy nhiên, Global MP thể hiện hiệu quả đáng kể ở các tỷ lệ độ thưa cao hơn và xuất sắc đặc biệt ở các mức độ thưa cực cao. Trong khi đạt được hiệu suất như vậy, Global MP không yêu cầu bất kỳ siêu tham số cụ thể cho thuật toán nào cần được điều chỉnh. Chúng tôi cũng làm sáng tỏ một vấn đề tiềm ẩn với việc cắt tỉa, được gọi là sự sụp đổ lớp, trong đó toàn bộ một lớp bị cắt tỉa, dẫn đến mất mát đáng kể về độ chính xác. Việc khắc phục nó trong Global MP rất đơn giản thông qua việc giới thiệu Ngưỡng Tối Thiểu (MT) để giữ lại số lượng tối thiểu các trọng số trong mỗi lớp. Chúng tôi tiến hành thí nghiệm trên các mô hình WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1, và FastGRNN, và trên các bộ dữ liệu CIFAR-10, ImageNet, và HAR-2. Chúng tôi kiểm tra Global MP cho cả thiết lập không có cấu trúc và có cấu trúc cũng như thiết lập một lần và dần dần, và chia sẻ các phát hiện của chúng tôi.

II. NGHIÊN CỨU LIÊN QUAN
Nén mạng nơ-ron đã trở thành một lĩnh vực nghiên cứu quan trọng do sự gia tăng nhanh chóng về kích thước của mạng nơ-ron [20], nhu cầu suy luận nhanh [21], ứng dụng vào các nhiệm vụ thực tế [22], [23], [24], [25], [26] và mối quan ngại về dấu chân carbon của việc huấn luyện các mạng nơ-ron lớn [4]. Qua các năm, một số kỹ thuật nén đã xuất hiện trong tài liệu [27], [28], như lượng tử hóa, phân tích nhân tử, chú ý, chưng cất kiến thức, tìm kiếm kiến trúc và cắt tỉa [29], [30], [31], [32]. So với các danh mục khác, cắt tỉa mang tính chất tổng quát hơn và đã thể hiện hiệu suất mạnh mẽ [2].

Nhiều kỹ thuật cắt tỉa đã được phát triển qua các năm, sử dụng đạo hàm bậc nhất hoặc bậc hai [33], [34], [1], các phương pháp dựa trên gradient [35], [36], [37], độ nhạy hoặc phản hồi từ một số hàm mục tiêu [9], [38], [39], [40], [41], các phép đo khoảng cách hoặc độ tương tự [42], tối ưu hóa Bayesian [43], các kỹ thuật dựa trên chính quy hóa [44], [10], [45], [46], [47], và tiêu chí dựa trên độ lớn [8], [17], [18], [48], [49]. Một thủ thuật quan trọng đã được khám phá trong [50] để cắt tỉa và huấn luyện lại mạng một cách lặp đi lặp lại, do đó bảo tồn độ chính xác cao. Cắt Tỉa Mạng Nơ-ron Thời Gian Chạy [51] cố gắng sử dụng học tăng cường (RL) để nén bằng cách huấn luyện một tác nhân RL để chọn các mạng con nhỏ hơn trong quá trình suy luận. [52] thiết kế phương pháp đầu tiên sử dụng RL để cắt tỉa. Tuy nhiên, các phương pháp huấn luyện RL thường yêu cầu ngân sách huấn luyện RL bổ sung và thiết kế không gian hành động và trạng thái RL cẩn thận [53], [54].

Mặt khác, Global MP hoạt động bằng cách xếp hạng tất cả các tham số trong mạng theo độ lớn tuyệt đối của chúng và sau đó cắt tỉa những tham số nhỏ nhất. Do đó, nó khá trực quan, hợp lý và đơn giản. Nó cũng không nên bị nhầm lẫn với các phương pháp sử dụng cắt tỉa toàn cục nhưng không tiến hành cắt tỉa độ lớn, ví dụ, SNIP [35]. Các phương pháp này sử dụng tiêu chí phức tạp, đầu tiên để xác định tính quan trọng của các trọng số một cách toàn cục và sau đó áp dụng cắt tỉa. Chúng tôi trình bày ở đây một so sánh sâu sắc giữa các kỹ thuật SOTA với Global MP. Cắt Tỉa Độ Lớn Dần Dần (GMP) [18] sử dụng lịch trình cắt tỉa đồng nhất và cắt tỉa mỗi lớp với cùng một lượng, do đó, không tính đến tầm quan trọng tương đối của các lớp. Mặt khác, Global MP cắt tỉa mỗi lớp một cách khác nhau. Tái Tham Số Hóa Thưa Thớt Động (DSR) [6] cắt tỉa và tái tăng trưởng trọng số sau mỗi vài trăm vòng lặp. Nó cũng sử dụng Global MP để cắt tỉa các trọng số. Tuy nhiên, nó áp đặt một số ràng buộc bổ sung dựa trên kinh nghiệm lên quá trình cắt tỉa, chẳng hạn như không cắt tỉa một số lớp được chọn trong mạng. Trong trường hợp các trọng số cần tái tăng trưởng vượt quá khả năng của một lớp, sau đó nó sử dụng các kinh nghiệm bổ sung để phân phối lại các trọng số bổ sung. Những loại kinh nghiệm này tăng thêm cả độ phức tạp và giới hạn các hành động cắt tỉa tiềm năng có thể được thực hiện. Khám Phá Dây Dẫn Nơ-ron (DNW) [19] tập trung vào việc học kết nối của các kênh trong mạng. Nó không chủ yếu là một kỹ thuật cắt tỉa và giống với Tìm Kiếm Kiến Trúc Mạng Nơ-ron (NAS) hơn.

Momentum Thưa Thớt (SM) [7] sử dụng phương pháp dựa trên kinh nghiệm để cắt tỉa và tái tăng trưởng trọng số. Họ sử dụng momentum trung bình để đánh giá tầm quan trọng của mỗi lớp và gán tham số tương ứng. Tương tự như DSR, một số lớp nhất định không bao giờ bị cắt tỉa và trong trường hợp tái tăng trưởng vượt quá khả năng của lớp, các kinh nghiệm bổ sung được sử dụng để phân phối lại trọng số. Phương pháp này cũng tốn nhiều tính toán vì gradient cần được lưu trữ trong bộ nhớ và cần thêm FLOPs để tính toán momentum trung bình cho mỗi tham số. Do đó, nó không hiệu quả bằng Global MP, ít tốn kém về mặt tính toán hơn và không dựa vào kinh nghiệm để cắt tỉa. Rigging the Lottery (RigL) [8] phân bổ độ thưa dựa trên số lượng tham số trong một lớp. Do đó, tầm quan trọng của một lớp dựa trên kích thước của nó, điều này không nhất thiết là một thước đo chính xác cho tất cả các trường hợp. Cắt Tỉa Động với Phản Hồi (DPF) [9] cũng sử dụng Global MP để cắt tỉa. Tuy nhiên, nó áp đặt ràng buộc bổ sung là giữ lớp cuối cùng hoàn toàn dày đặc. Do đó, nó không linh hoạt bằng Global MP thuần túy cho phép tất cả các lớp được cắt tỉa.

Tái Tham Số Hóa Ngưỡng Mềm (STR) [10] là một kỹ thuật dựa trên chính quy hóa trừ đi một giá trị nhất định từ các trọng số trong mỗi epoch. Mục tiêu độ thưa chính xác không thể được kiểm soát trong STR, và nó yêu cầu điều chỉnh nặng các siêu tham số để đạt được mục tiêu độ thưa yêu cầu. Do đó, nó không linh hoạt và có thể tổng quát hóa bằng Global MP. Xấp Xỉ Không Ma Trận của Thông Tin Bậc Hai (M-FAC) [55] và tiền thân của nó (WoodFisher) [56] sử dụng xấp xỉ của Hessian bậc hai để cắt tỉa trọng số. Cắt tỉa dựa trên Hessian có nguyên tắc nhưng không thể tính toán được. Do đó, cần phải thực hiện các xấp xỉ cho Hessian. Những xấp xỉ này dẫn đến điểm số quan trọng không chính xác cho các trọng số, trong khi những xấp xỉ như vậy không cần thiết trong Global MP. Do đó, Global MP nói chung có nguyên tắc hơn, không có kinh nghiệm và ít tốn kém về mặt tính toán so với các phương pháp SOTA và có thể là lý do cơ bản cho hiệu suất vượt trội của nó.

III. PHƯƠNG PHÁP
Trong phần này, chúng tôi giải thích cách Global MP hoạt động bằng cách mô tả các thành phần chính của nó. Chúng tôi cũng giới thiệu một cơ chế ngưỡng đơn giản, được gọi là Ngưỡng Tối Thiểu (MT), để tránh vấn đề sụp đổ lớp ở các mức độ thưa cao.

--- TRANG 3 ---
3
Mục tiêu độ thưa
Trọng số lớn hơn
Trọng số nhỏ hơn
Hình 1: Minh họa cách Global MP hoạt động. Global MP xếp hạng tất cả các trọng số trong mạng theo độ lớn của chúng và cắt tỉa các trọng số nhỏ nhất cho đến khi đạt được mục tiêu độ thưa. Trọng số màu xanh lá cây nhạt đề cập đến các trọng số có độ lớn nhỏ hơn được cắt tỉa. Một mạng được cắt tỉa bao gồm các trọng số có độ lớn lớn hơn (trọng số màu xanh lá cây đậm) được thu được sau quá trình này.

A. Cắt Tỉa Độ Lớn Toàn Cục (Global MP)
Global MP là một phương pháp cắt tỉa dựa trên độ lớn, trong đó các trọng số lớn hơn một ngưỡng nhất định được giữ lại, và các trọng số nhỏ hơn ngưỡng được cắt tỉa trên toàn bộ mạng nơ-ron. Ngưỡng được tính toán dựa trên tỷ lệ độ thưa mục tiêu và không phải là siêu tham số cần được điều chỉnh hoặc học. Với tỷ lệ độ thưa mục tiêu κtarget, ngưỡng t chỉ đơn giản được tính toán là độ lớn trọng số phục vụ như một điểm phân tách giữa κtarget phần trăm trọng số nhỏ nhất và phần còn lại, một khi tất cả các trọng số được sắp xếp vào một mảng dựa trên độ lớn của chúng. Một cách chính thức, đối với một ngưỡng t được tính toán và mỗi trọng số riêng lẻ w trong bất kỳ lớp nào, trọng số mới wnew được định nghĩa như sau:

wnew = (
0 nếu |w| < t,
w ngược lại.(1)

Trong Global MP, một ngưỡng duy nhất được thiết lập cho toàn bộ mạng dựa trên mục tiêu độ thưa cho mạng. Điều này trái ngược với cắt tỉa theo lớp, trong đó các giá trị ngưỡng khác nhau phải được tìm kiếm cho từng lớp riêng lẻ. Trong trường hợp cắt tỉa đồng nhất, một ngưỡng cho mỗi lớp cần được tính toán dựa trên mục tiêu độ thưa được gán cho các lớp một cách đồng nhất trên toàn bộ mạng. Trong khía cạnh này, Global MP hiệu quả hơn so với cắt tỉa theo lớp hoặc đồng nhất vì ngưỡng không cần được tìm kiếm hoặc tính toán cho từng lớp riêng lẻ.

B. Ngưỡng Tối Thiểu (MT)
Ngưỡng Tối Thiểu (MT) đề cập đến số lượng cố định các trọng số được bảo tồn trong mỗi lớp của mạng nơ-ron sau khi cắt tỉa. MT là một giá trị vô hướng được cố định trước khi bắt đầu chu kỳ cắt tỉa. Các trọng số trong một lớp được sắp xếp theo độ lớn của chúng và số lượng MT trọng số lớn nhất được bảo tồn. Ví dụ, một MT là 500 có nghĩa là 500 trọng số lớn nhất trong mỗi lớp cần được bảo tồn sau khi cắt tỉa. Nếu một lớp ban đầu có số lượng trọng số nhỏ hơn số MT, thì tất cả các trọng số của lớp đó sẽ được bảo tồn. Điều này tương ứng với:

∥Wl∥0 ≥ (
σ nếu m ≥ σl,
m ngược lại.(2)

Thuật ngữ Wl ∈ Rm biểu thị vector trọng số cho lớp l, σ là giá trị MT về số lượng trọng số và ∥Wl∥0 chỉ ra số lượng phần tử khác không trong Wl. Chúng tôi giải thích trong phần tiếp theo cách thực hiện cắt tỉa thực tế bằng MT.

C. Quy Trình Cắt Tỉa
Quy trình cắt tỉa cho Global MP bao gồm việc cắt tỉa một mô hình cho đến khi đạt được mục tiêu độ thưa mong muốn và huấn luyện hoặc tinh chỉnh nó trong số epoch được chỉ định. Nó hỗ trợ cả thiết lập cắt tỉa một lần và dần dần cũng như có hoặc không có MT. Người dùng có thể chọn bất kỳ thiết lập cắt tỉa nào theo trường hợp sử dụng của họ. Quy trình bắt đầu bằng việc chọn một mô hình được huấn luyện trước trong cắt tỉa một lần, hoặc một mô hình chưa được huấn luyện trong cắt tỉa dần dần. Tiếp theo, độ thưa của mô hình được kiểm tra và nếu độ thưa thấp hơn độ thưa mục tiêu, thì mô hình được cắt tỉa bằng Global MP thuần túy hoặc Global MP với MT, tùy theo lựa chọn của người dùng. Một khi mô hình được cắt tỉa, nó được huấn luyện cho trường hợp cắt tỉa dần dần hoặc tinh chỉnh cho trường hợp cắt tỉa một lần. Khung Global MP cho phép tính linh hoạt cho các trọng số đã cắt tỉa trước đó tái tăng trưởng, nếu chúng trở nên hoạt động hơn trong các epoch sau, cho trường hợp cắt tỉa dần dần. Không có cắt tỉa cứng nào được thực hiện trong đó các trọng số bị đặt về không vĩnh viễn. Mặt nạ cắt tỉa được tính toán mới trong mỗi epoch do đó cho phép các trọng số đã cắt tỉa trước đó tái tăng trưởng. Quy trình trên lặp lại cho đến khi đạt được epoch cuối cùng. Đối với trường hợp cắt tỉa một lần, các epoch sau chỉ được sử dụng để tinh chỉnh vì việc cắt tỉa xảy ra một lần trong epoch đầu tiên. Điều này kết thúc quy trình và kết quả cuối cùng là một mô hình được cắt tỉa và huấn luyện (hoặc tinh chỉnh). Xem Phụ lục cho mã giả.

IV. THÍ NGHIỆM
Dưới đây chúng tôi mô tả các thí nghiệm liên quan đến Global MP so với các thuật toán cắt tỉa hiện đại (SOTA).

A. So Sánh với SOTA
Chúng tôi so sánh Global MP với nhiều thuật toán SOTA phổ biến được biết đến về cắt tỉa, như SNIP [35], SM [7], DSR [6], DPF [9], GMP [18], DNW [19], RigL [8], và STR [10]. Những phương pháp này bao gồm một loạt rộng các phương pháp liên quan đến cắt tỉa và tái tăng trưởng trọng số một cách lặp đi lặp lại sau mỗi vài vòng lặp, cắt tỉa tại khởi tạo, sử dụng gradient và tín hiệu phản hồi để cắt tỉa, và cắt tỉa bằng chính quy hóa. Chúng tôi báo cáo kết quả từ những thuật toán này bất cứ khi nào họ báo cáo kết quả cho bộ dữ liệu cụ thể đang được thí nghiệm. Xem Phụ lục cho siêu tham số.

--- TRANG 4 ---
4
Phương pháp Độ thưa WRN-28-8 Acc. ResNet-32 Acc.
Cơ sở 0.0% 96.06% 93.83 ±0.12 %
SNIP 90% 95.49±0.21% 90.40±0.26%
SM 90% 95.67±0.14% 91.54±0.18%
DSR 90% 95.81±0.10% 91.41±0.23%
DPF 90% 96.08±0.15% 92.42±0.18%
Global MP 90% 96.30±0.03% 92.67 ±0.03%
SNIP 95% 94.93±0.13% 87.23±0.29%
SM 95% 95.64±0.07% 88.68±0.22%
DSR 95% 95.55±0.12% 84.12±0.32%
DPF 95% 95.98±0.10% 90.94±0.35%
Global MP 95% 96.16±0.02% 90.65±0.13%
BẢNG I: Kết quả của các thuật toán cắt tỉa SOTA trên WideResNet-28-8 và ResNet-32 trên CIFAR-10. Font đậm biểu thị thuật toán có hiệu suất tốt nhất. Global MP vượt trội hoặc đạt hiệu suất tương đương với các thuật toán khác.

1) CIFAR-10: Chúng tôi tiến hành thí nghiệm để so sánh Global MP với các thuật toán cắt tỉa SOTA trên bộ dữ liệu CIFAR-10. Chúng tôi so sánh Global MP Một lần với bốn thuật toán trong trường hợp này: SNIP [35], SM [7], DSR [6], và DPF [9]. Chúng tôi báo cáo kết quả trên hai kiến trúc mạng phổ biến và được cắt tỉa rộng rãi, cụ thể là WideResNet-28-8 (WRN-28-8) và ResNet-32 [57]. Đối với cả hai kiến trúc, chúng tôi bắt đầu với mô hình gốc có cùng độ chính xác ban đầu như các thuật toán khác để có so sánh công bằng. Đối với thí nghiệm WRN-28-8 Bảng I, Global MP hoạt động tốt hơn so với các đối thủ còn lại ở cả mức độ thưa 90% và 95%. Global MP vượt trội hơn DSR và SM trong tất cả các trường hợp, vì các ràng buộc dựa trên kinh nghiệm bổ sung của họ hạn chế việc lựa chọn các lớp để cắt tỉa. Đối với ResNet-32 (Bảng I), Global MP vượt trội ở độ thưa 95% và đứng thứ hai tốt nhất ở độ thưa 90%. Global MP vượt trội hơn DSR và SM cho tất cả các mức độ thưa trong trường hợp này. Đây là dấu hiệu của khả năng của Global MP so với các thuật toán khác, trong khi không có độ phức tạp bổ sung nào.

2) ImageNet: Theo hiệu suất thuận lợi trên bộ dữ liệu CIFAR-10, chúng tôi đánh giá Global MP với các đối thủ khác trong tài liệu trên bộ dữ liệu ImageNet. Đây là một bộ dữ liệu rất thử thách so với CIFAR-10, có khoảng 1.3 triệu hình ảnh RGB với 1,000 lớp. Sử dụng bộ dữ liệu này, chúng tôi so sánh Global MP với các thuật toán SOTA như GMP [18], DSR [6], DNW [19], SM [7], RigL [8], WoodFisher [56], MFAC [55], DPF [9], và STR [10]. Hai kiến trúc mạng chúng tôi sử dụng cho so sánh này là ResNet-50 và MobileNet-V1 [58], hai kiến trúc phổ biến nhất để đánh giá thuật toán cắt tỉa trên ImageNet [14]. Đối với ResNet-50, chúng tôi thêm vào một thiết lập thí nghiệm bổ sung của Global MP dần dần để cung cấp so sánh kỹ lưỡng hơn với các phương pháp SOTA. Chúng tôi lại bắt đầu từ cùng độ chính xác ban đầu cho các mô hình không được cắt tỉa cho tất cả các thuật toán, bằng cách khớp kết quả trong các bài báo gốc của họ hoặc tái tạo kết quả của họ bất cứ khi nào mã của họ có sẵn. Chúng tôi lấy mẫu bốn mức độ thưa từ độ thưa thấp (80%) đến độ thưa cực cao (98%) để cung cấp ảnh chụp toàn diện qua các mức độ thưa khác nhau.

Hiệu suất đáng chú ý của Global MP trở nên rõ ràng trong các thí nghiệm ResNet-50 trên ImageNet. Như có thể thấy từ Bảng II, đối với sự đánh đổi độ thưa-độ chính xác, Global MP vượt trội hoặc đạt độ chính xác tương đương với tất cả các đối thủ khác trong mọi mức độ thưa từ 80% đến 98% (xem kết quả in đậm). MFAC hoạt động gần với độ thưa 95% tuy nhiên, đó không phải là so sánh tương tự vì độ thưa của họ thấp hơn một chút (95%) so với Global MP (95.3%). Chúng tôi lấy mục tiêu độ thưa giới hạn trên cho mỗi mức độ thưa cho Global MP để khớp với phương pháp có độ thưa cao nhất được báo cáo trong mức độ thưa đó. Đối với trường hợp độ thưa cực cao (98%), Global MP vượt qua thuật toán tốt thứ hai (STR) với biên độ lớn 5.11%. Đối với sự đánh đổi FLOPs-độ chính xác, việc so sánh khó khăn hơn vì các phương pháp báo cáo các mục tiêu FLOPs khác nhau và không thể thực hiện so sánh tương tự một cách đơn giản. Tuy nhiên, một thực hành viên có kinh nghiệm có thể đánh giá tương đối về hiệu quả của các phương pháp dựa trên tỷ lệ giữa FLOPs bổ sung được cắt tỉa so với sự giảm độ chính xác của các phương pháp. Dựa trên điều này, chúng tôi thấy rằng một số kỹ thuật SOTA thể hiện hiệu suất FLOPs-độ chính xác vượt trội hơn Global MP ở các tỷ lệ độ thưa thấp hơn là 80% và 90%. Tuy nhiên, Global MP trở nên cạnh tranh ở độ thưa 95% và hoạt động rất tốt ở tỷ lệ độ thưa cực cao là 98% độ thưa, đạt được 5.11% độ chính xác so với sự giảm 2% FLOPs so với phương pháp tốt thứ hai (STR).

Chúng tôi cũng thấy rằng Global MP dần dần nói chung hoạt động tốt hơn Global MP một lần ở các tỷ lệ độ thưa cao và cực cao. Điều này là do mặt nạ cắt tỉa được phép thay đổi nhiều lần trong Global MP dần dần so với chỉ một lần trong Global MP một lần, và do đó, hội tụ đến một giá trị được tối ưu hóa hơn. Hầu hết các phương pháp SOTA cũng theo cùng một phương pháp trong đó họ cho phép mặt nạ cắt tỉa thay đổi mỗi epoch hoặc đôi khi nhiều lần trong một epoch. Nhìn chung, Global MP vượt trội hơn tất cả các thuật toán SOTA về sự đánh đổi độ thưa-độ chính xác và đứng thứ hai, sau STR, cho sự đánh đổi FLOPs-độ chính xác. Đây là một phát hiện quan trọng rằng một thuật toán đơn giản như Global MP có thể vượt trội hơn các đối thủ SOTA khác kết hợp các lựa chọn thiết kế rất phức tạp hoặc các quy trình đòi hỏi tính toán.

Chúng tôi cũng thử nghiệm một kiến trúc khác trên ImageNet, MobileNet-V1, đây là một kiến trúc nhỏ hơn và hiệu quả hơn nhiều so với ResNet-50. Trong trường hợp này, các đối thủ mạnh bị giới hạn trong tài liệu; chỉ hai trong số các thuật toán nói trên có thể trình bày kết quả cạnh tranh do thực tế là kiến trúc này có ít dư thừa hơn. Chúng tôi đánh giá Global MP với hai đối thủ khác ở hai mức độ thưa mục tiêu: 75% và 90%. Như có thể thấy trong Bảng III, Global MP vượt trội hơn các thuật toán SOTA về sự đánh đổi độ thưa-độ chính xác với biên độ hơn 2% ở độ thưa 75%, đây là một kết quả đáng kể cho thấy MobileNet-V1 compact như thế nào. Ở độ thưa 90%, tính compact tương tự khiến Global MP cắt tỉa quá mức một số lớp trong mạng, dẫn đến sự giảm độ chính xác đáng kể. Đây là vấn đề sụp đổ lớp được đề cập ở trên, và nó dễ dàng được khắc phục khi MT được giới thiệu vào Global MP. Chúng tôi sử dụng giá trị MT là 0.2% được xác định bằng cách sử dụng quy trình tìm kiếm lưới tương tự như bất kỳ siêu tham số nào khác. Thông thường các giá trị giữa 0.01% đến 0.3% hoạt động tốt cho MT bất kể mô hình và bộ dữ liệu.

--- TRANG 5 ---
5
Phương pháp Top-1 Acc Params Độ thưa FLOPs được cắt tỉa
ResNet-50 77.0% 25.6M 0.00% 0.0%
GMP 75.60% 5.12M 80.00% 80.0%
DSR*# 71.60% 5.12M 80.00% 69.9%
DNW 76.00% 5.12M 80.00% 80.0%
SM 74.90% 5.12M 80.00% -
SM + ERK 75.20% 5.12M 80.00% 58.9%
RigL* 74.60% 5.12M 80.00% 77.5%
RigL + ERK 75.10% 5.12M 80.00% 58.9%
DPF 75.13% 5.12M 80.00% 80.0%
STR 76.19% 5.22M 79.55% 81.3%
Global MP (Một lần) 76.84% 5.12M 80.00% 72.4%
Global MP (Dần dần) 76.12% 5.12M 80.00% 76.7%
GMP 73.91% 2.56M 90.00% 90.0%
DNW 74.00% 2.56M 90.00% 90.0%
SM 72.90% 2.56M 90.00% 60.1%
SM + ERK 72.90% 2.56M 90.00% 76.5%
RigL* 72.00% 2.56M 90.00% 87.4%
RigL + ERK 73.00% 2.56M 90.00% 76.5%
DPF# 74.55% 4.45M 82.60% 90.0%
STR 74.73% 3.14M 87.70% 90.2%
Global MP (Một lần) 75.28% 2.56M 90.00% 82.8%
Global MP (Dần dần) 74.83% 2.56M 90.00% 87.8%
GMP 70.59% 1.28M 95.00% 95.0%
DNW 68.30% 1.28M 95.00% 95.0%
RigL* 67.50% 1.28M 95.00% 92.2%
RigL + ERK 70.00% 1.28M 95.00% 85.3%
WoodFisher 72.12% 1.28M 95.00% -
MFAC 72.32% 1.28M 95.00% -
STR 70.40% 1.27M 95.03% 96.1%
Global MP (Một lần) 71.56% 1.20M 95.30% 89.3%
Global MP (Dần dần) 72.14% 1.20M 95.30% 93.1%
GMP 57.90% 0.51M 98.00% 98.0%
DNW 58.20% 0.51M 98.00% 98.0%
STR 61.46% 0.50M 98.05% 98.2%
Global MP (Một lần) 61.80% 0.50M 98.05% 93.7%
Global MP (Dần dần) 66.57% 0.50M 98.05% 96.2%
BẢNG II: Kết quả trên ResNet-50 trên ImageNet. Global MP vượt trội hơn các thuật toán cắt tỉa SOTA ở tất cả các mức độ thưa cho sự đánh đổi độ thưa-độ chính xác và ở các mức độ thưa cao cho sự đánh đổi FLOPs-độ chính xác. Font đậm biểu thị hiệu suất tốt nhất cho sự đánh đổi độ thưa-độ chính xác trong khi font gạch chân biểu thị hiệu suất tốt nhất cho sự đánh đổi FLOPs-độ chính xác. * và # có nghĩa là lớp đầu tiên và lớp cuối cùng dày đặc, tương ứng.

Phương pháp Top-1 Acc Params. Độ thưa FLOPs được cắt tỉa
MobileNet-V1 71.95% 4.21M 0.00% 0.0%
GMP 67.70% 1.09M 74.11% 71.4%
STR 68.35% 1.04M 75.28% 82.2%
Global MP 70.74% 1.04M 75.28% 68.9%
GMP 61.80% 0.46M 89.03% 85.6%
STR 61.51% 0.44M 89.62% 93.0%
Global MP 59.49% 0.42M 90.00% 83.7%
Global MP với MT 63.94% 0.42M 90.00% 72.9%
BẢNG III: Kết quả của các thuật toán cắt tỉa trên MobileNet-V1 trên ImageNet. Font đậm biểu thị thuật toán có hiệu suất độ thưa-độ chính xác tốt nhất trong khi gạch chân biểu thị hiệu suất FLOPs-độ chính xác tốt nhất. Global MP với MT vượt trội hơn các thuật toán SOTA về hiệu suất độ thưa-độ chính xác.

Xem Phụ lục để nghiên cứu thêm về MT. Chúng tôi thấy rằng MT hoạt động như một siêu tham số điển hình. Khi tăng giá trị MT ban đầu, độ chính xác tăng, cho đến khi đạt giá trị tối đa. Sau đó, việc tăng giá trị MT dẫn đến sự giảm độ chính xác. Do đó, một giá trị phù hợp có thể được tìm thấy bằng cách thực hiện tìm kiếm trên các giá trị MT. Độ chính xác của Global MP ở độ thưa 90% vượt qua SOTA một lần nữa với một sự khắc phục đơn giản như vậy, và biên độ độ chính xác so với đối thủ tiếp theo trở nên cao hơn 2%. Đối với sự đánh đổi FLOPs-độ chính xác, so sánh tương tự một lần nữa khó thực hiện, nhưng STR dường như hoạt động tốt hơn, nhờ vào các mức độ thưa nhỏ hơn mà MobileNet được cắt tỉa, so với ResNet-50. MT cũng đi kèm với chi phí giảm FLOPs ít hơn, nhưng nó hữu ích đặc biệt cho các ứng dụng quan trọng về độ chính xác trong đó việc giảm kích thước mạng vẫn quan trọng. Tất cả những phát hiện này chỉ ra rõ ràng rằng Global MP là một thuật toán cắt tỉa đơn giản nhưng cạnh tranh. Nó mang lại hiệu suất hàng đầu về sự đánh đổi độ thưa-độ chính xác, và xếp thứ hai cho sự đánh đổi FLOPs-độ chính xác, mặc dù không có bất kỳ lựa chọn thiết kế phức tạp nào hoặc siêu tham số bổ sung.

B. Cắt tỉa có cấu trúc và tổng quát hóa sang các lĩnh vực khác và kiến trúc RNN
Chúng tôi thí nghiệm với Global MP trên các lĩnh vực khác và mạng không phải tích chập để đo lường khả năng tổng quát hóa của thuật toán trên các lĩnh vực và loại mạng khác nhau. Chúng tôi thí nghiệm trên mô hình FastGRNN [59] trên bộ dữ liệu Nhận dạng Hoạt động Con người HAR-2 [60]. Bộ dữ liệu HAR-2 là phiên bản nhị phân hóa của bộ dữ liệu Nhận dạng Hoạt động Con người 6 lớp. Từ mô hình đầy đủ với rW = 9 và rU = 80 như được đề xuất trong bài báo STR [10], chúng tôi áp dụng Global MP trên các ma trận W1 và W2. Để làm điều này, chúng tôi tìm mặt nạ trọng số bằng cách xếp hạng các cột của W1 và W2 dựa trên tổng tuyệt đối của chúng, sau đó chúng tôi cắt tỉa 9−rnew W cột thấp nhất và 80−rnew U cột thấp nhất từ W1 và W2 tương ứng. Cuối cùng, chúng tôi tinh chỉnh mô hình đã cắt tỉa này bằng cách huấn luyện lại nó với trình huấn luyện của FastGRNN và áp dụng mặt nạ trọng số ở mỗi epoch. Chúng tôi thử nghiệm Global MP dưới các cấu hình rw-rv khác nhau. Chúng tôi thấy rằng Global MP vượt trội hơn các đường cơ sở khác trên tất cả các cấu hình (Bảng IV) và thành công cắt tỉa mô hình trên một kiến trúc và lĩnh vực rất khác nhau.

C. Giảm thiểu sự sụp đổ lớp
Sự sụp đổ lớp là một vấn đề mà nhiều thuật toán cắt tỉa gặp phải [15], [61], [62] và xảy ra khi toàn bộ một lớp bị cắt tỉa bởi thuật toán cắt tỉa, khiến mạng không thể huấn luyện được. Chúng tôi điều tra hiện tượng này và thấy rằng hiệu suất của thuật toán cắt tỉa có thể bị ảnh hưởng đáng kể bởi kiến trúc của mạng nơ-ron được cắt tỉa, đặc biệt trong lĩnh vực độ thưa cao. Chúng tôi tiến hành thí nghiệm trên các mô hình MobileNet-V2 và WRN-22-8 trên bộ dữ liệu CIFAR-10. Chúng tôi báo cáo kết quả được tính trung bình trên nhiều lần chạy trong đó mỗi lần chạy sử dụng một mô hình được huấn luyện trước khác nhau để cung cấp sự mạnh mẽ hơn. Chúng tôi đầu tiên cắt tỉa mô hình WRN-22-8 đến độ thưa 99.9%. Chúng tôi thấy rằng ở độ thưa 99.9%, WRN vẫn có thể đạt độ chính xác tốt (Bảng V). Sau đó chúng tôi cắt tỉa mô hình MobileNet-V2 đến độ thưa 98%. Tuy nhiên, đối với MobileNet, độ chính xác giảm xuống 10% chỉ sử dụng Global MP, và mô hình không thể học được (Bảng VI).

conv 1x1, in:160, out: 960
conv 3x3, in:960, out: 960
conv 1x1, in:960, out: 160
conv 1x1, in:160, out: 960
conv 3x3, in:960, out: 960
conv 1x1, in:960, out: 320
conv 1x1, in:320, out: 1280
Fully connected, in:1280, out: 10
conv 1x1, in:160, out: 320[50]
[51]
[52]
[53]
[54]
[55]
[57]
[58][56]
conv 3x3, in:512, out: 512
conv 3x3, in:512, out: 512
Fully connected, in:512, out: 10[19]
[20]
[21]
[22]
[23]
conv 3x3, in:512, out: 512
conv 3x3, in:512, out: 512
WideResNet-22-8 MobileNet-V2
Lớp shortcut
Lớp downsampling
Hình 2: Sự khác biệt trong kiến trúc giữa WRN và MobileNet. WRN không có kết nối residual có thể cắt tỉa trong các lớp cuối (đường đứt nét) trong khi MobileNet có. Điều này dẫn đến các hành vi cắt tỉa khác nhau trên hai kiến trúc.

Lý do cho sự khác biệt rộng này trong hành vi học nằm ở các kết nối shortcut [57]. Cả WRN-22-8 và MobileNet-V2 đều sử dụng kết nối shortcut, tuy nhiên, vị trí của chúng khác nhau. Tham khảo Hình 2, WRN sử dụng kết nối shortcut đồng nhất từ Lớp 20 đến Lớp 23. Loại kết nối shortcut này là ánh xạ đồng nhất đơn giản và không yêu cầu bất kỳ tham số bổ sung nào, và do đó, chúng không được tính vào các trọng số. Tuy nhiên, MobileNet-V2 sử dụng ánh xạ shortcut tích chập từ Lớp 52 đến Lớp 57. Các trọng số trong ánh xạ này được tính vào trọng số của mô hình, và do đó, chúng có thể cắt tỉa được. Global MP hoàn toàn cắt tỉa hai lớp trước lớp cuối cùng. Tuy nhiên, vì WRN sử dụng ánh xạ đồng nhất, nó vẫn có thể truyền thông tin đến lớp cuối cùng, và mô hình vẫn có thể học, trong khi MobileNet-V2 đối mặt với sự giảm độ chính xác thảm khốc do sự sụp đổ lớp. Các thuật toán cắt tỉa có thể dễ bị các vấn đề sụp đổ lớp thảm khốc như vậy đặc biệt trong lĩnh vực độ thưa cao. Quy tắc MT có thể giúp vượt qua vấn đề này. Giữ lại một MT nhỏ là 0.02% đủ cho MobileNet-V2 để tránh sự sụp đổ lớp và học thành công. Do đó, việc giữ lại một lượng nhỏ trọng số có thể giúp trong động lực học của các mô hình trong các thiết lập độ thưa cao.

V. THẢO LUẬN, HẠN CHẾ VÀ NGHIÊN CỨU TƯƠNG LAI
Các quan sát của chúng tôi cho thấy Global MP hoạt động rất tốt và đạt hiệu suất vượt trội trên tất cả các bộ dữ liệu và kiến trúc được thử nghiệm. Nó có thể hoạt động như một thuật toán cắt tỉa một lần hoặc như một thuật toán cắt tỉa dần dần. Nó cũng vượt trội hơn các thuật toán SOTA trên ResNet-50 trên ImageNet về sự đánh đổi độ thưa-độ chính xác và thiết lập kết quả SOTA mới qua nhiều mức độ thưa. Đối với sự đánh đổi FLOPs-độ chính xác, nó đứng thứ hai sau STR, vượt trội hơn nhiều kỹ thuật SOTA. Cùng lúc, Global MP có độ phức tạp thuật toán rất thấp và có thể nói là một trong những thuật toán cắt tỉa đơn giản nhất. Nó đơn giản hơn nhiều thuật toán cắt tỉa khác như chính quy hóa dựa trên hàm mất mát tùy chỉnh, quy trình dựa trên RL, tỷ lệ cắt tỉa theo lớp dựa trên kinh nghiệm, v.v. Nó chỉ xếp hạng trọng số dựa trên độ lớn của chúng và loại bỏ những trọng số nhỏ nhất. Điều này đặt ra một câu hỏi quan trọng về việc liệu độ phức tạp có thực sự cần thiết cho việc cắt tỉa hay không và theo kết quả của chúng tôi, có vẻ như độ phức tạp bản thân nó không đảm bảo hiệu suất tốt. Các thực hành viên phát triển thuật toán cắt tỉa mới do đó nên xem xét cẩn thận liệu độ phức tạp có thêm giá trị cho thuật toán của họ hay không và vượt qua các đường cơ sở như Global MP.

Một hạn chế của Global MP là các nền tảng lý thuyết cho nó chưa được thiết lập tốt. Theo công việc thực nghiệm của chúng tôi trong bản thảo này, chúng tôi dự định tiến hành phân tích lý thuyết để hiểu rõ hơn về động lực của Global MP trong tương lai. Nó sẽ bao gồm việc tìm kiếm các liên kết phân tích giữa độ lớn của trọng số và tầm quan trọng của chúng trong mạng, hoặc thậm chí các mối quan hệ phân tích của chúng với độ chính xác kết quả của mô hình. Một lĩnh vực khác cho nghiên cứu tương lai là tối ưu hóa cùng lúc cả trọng số và FLOPs trong quá trình cắt tỉa. Hiện tại, Global MP được sử dụng để đạt đến một độ thưa tham số nhất định, và việc giảm FLOPs đến như một sản phẩm phụ. Trong tương lai, FLOPs cũng có thể được thêm vào hàm tối ưu hóa để cùng lúc làm thưa cả tham số và FLOPs.

--- TRANG 6 ---
6
Phương pháp Top-1 Acc rW rU
FastGRNN 96.10% 9 80
Huấn luyện Vanilla 94.06% 9 8
STR 95.76% 9 8
Global MP 95.89% 9 8
Huấn luyện Vanilla 93.15% 9 7
STR 95.62% 9 7
Global MP 95.72% 9 7
Huấn luyện Vanilla 94.88% 8 7
STR 95.59% 8 7
Global MP 95.62% 8 7
BẢNG IV: Kết quả trên FastGRNN trên bộ dữ liệu HAR-2. Font đậm biểu thị thuật toán có hiệu suất tốt nhất. Global MP vượt trội hơn các thuật toán cắt tỉa khác.

Phương pháp WRN-22-8 trên CIFAR-10
Độ thưa Acc. Ban đầu Acc. Sau cắt tỉa
Global MP 99.9% 94.07%±0.05% 67.68%±0.78%
BẢNG V: Hiệu suất của Global MP trên WideResNet-22-8 trong chế độ độ thưa cao ở độ thưa 99.9%.

Phương pháp MobileNet-V2 trên CIFAR-10
Độ thưa Acc. Ban đầu Acc. Sau cắt tỉa
Global MP 98.0% 94.15%±0.23% 10% (Không thể học)
Global MP với MT 98.0% 94.15%±0.23% 82.97%±0.57%
BẢNG VI: Thêm MT cho phép MobileNet-V2 học trong chế độ độ thưa cao.

VI. KẾT LUẬN
Trong nghiên cứu này, chúng tôi đặt ra câu hỏi liệu việc sử dụng các thuật toán phức tạp và đòi hỏi tính toán có thực sự cần thiết để đạt được kết quả cắt tỉa DNN vượt trội hay không. Điều này xuất phát từ sự gia tăng số lượng thuật toán cắt tỉa mới được đề xuất trong những năm gần đây, mỗi thuật toán có mức tăng hiệu suất biên, nhưng các quy trình cắt tỉa ngày càng phức tạp. Điều này khiến cho việc một thực hành viên chọn thuật toán đúng và bộ siêu tham số cụ thể thuật toán tốt nhất cho ứng dụng của họ trở nên khó khăn. Chúng tôi đánh giá các thuật toán này với một đường cơ sở ngây thơ, cụ thể là Global MP, không kết hợp bất kỳ quy trình phức tạp nào hoặc bất kỳ siêu tham số khó điều chỉnh nào. Mặc dù đơn giản, chúng tôi thấy rằng Global MP vượt trội hơn nhiều thuật toán cắt tỉa SOTA trên nhiều bộ dữ liệu, như CIFAR-10, ImageNet, và HAR-2; với các kiến trúc mạng khác nhau, như ResNet-50 và MobileNet-V1; và ở các mức độ thưa khác nhau từ 50% lên đến 99.9%. Chúng tôi cũng trình bày một vài biến thể của Global MP, tức là một lần và dần dần, cùng với một kỹ thuật mới, bổ sung, MT. Trong khi kết quả của chúng tôi phục vụ như một bằng chứng thực nghiệm rằng một thuật toán cắt tỉa ngây thơ như Global MP có thể đạt được kết quả SOTA, nó vẫn là một hướng nghiên cứu tương lai đầy hứa hẹn để làm sáng tỏ các khía cạnh lý thuyết về cách hiệu suất như vậy có thể với Global MP. Một hướng tương lai khác bao gồm việc mở rộng khả năng của Global MP, như tối ưu hóa cùng lúc cả FLOPs và số lượng trọng số.

VII. LỜI CẢM ƠN
Nghiên cứu này được hỗ trợ bởi Cơ quan Khoa học, Công nghệ và Nghiên cứu (A*STAR) trong chương trình Quỹ Chương trình AME (Số dự án: A1892b0026 và A19E3b0099). Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này đều là của (các) tác giả và không phản ánh quan điểm của A*STAR.

TÀI LIỆU THAM KHẢO
[1] E. Kurtic, D. Campos, T. Nguyen, E. Frantar, M. Kurtz, B. Fineran, M. Goin, và D. Alistarh, "The optimal BERT surgeon: Scalable and accurate second-order pruning for large language models," trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, tháng 12 năm 2022, tr. 4163–4181. [Trực tuyến]. Có sẵn: https://aclanthology.org/2022.emnlp-main.279
[2] T. Gale, E. Elsen, và S. Hooker, "The state of sparsity in deep neural networks," arXiv preprint arXiv:1902.09574, 2019.
[3] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, và D. Amodei, "Scaling laws for neural language models," CoRR, vol. abs/2001.08361, 2020. [Trực tuyến]. Có sẵn: https://arxiv.org/abs/2001.08361
[4] E. Strubell, A. Ganesh, và A. McCallum, "Energy and policy considerations for deep learning in nlp," 57th Annual Meeting of the Association for Computational Linguistics (ACL), 2019.
[5] R. Bonatti, W. Wang, C. Ho, A. Ahuja, M. Gschwindt, E. Camci, E. Kayacan, S. Choudhury, và S. Scherer, "Autonomous aerial cinematography in unstructured environments with learned artistic decision-making," Journal of Field Robotics, vol. 37, no. 4, tr. 606–641, 2020.

--- TRANG 7 ---
7
[6] H. Mostafa và X. Wang, "Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization," trong Proceedings of the 36th International Conference on Machine Learning. PMLR, 2019, tr. 4646–4655.
[7] T. Dettmers và L. Zettlemoyer, "Sparse networks from scratch: Faster training without losing performance," CoRR, vol. abs/1907.04840, 2019. [Trực tuyến]. Có sẵn: http://arxiv.org/abs/1907.04840
[8] U. Evci, T. Gale, J. Menick, P. S. Castro, và E. Elsen, "Rigging the lottery: Making all tickets winners," trong Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, H. D. III và A. Singh, Eds., vol. 119. PMLR, 13–18 tháng 7 năm 2020, tr. 2943–2952. [Trực tuyến]. Có sẵn: https://proceedings.mlr.press/v119/evci20a.html
[9] T. Lin, S. U. Stich, L. Barba, D. Dmitriev, và M. Jaggi, "Dynamic model pruning with feedback," trong International Conference on Learning Representations, 2020. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=SJem8lSFwB
[10] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, và A. Farhadi, "Soft threshold weight reparameterization for learnable sparsity," trong Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, H. D. III và A. Singh, Eds., vol. 119. PMLR, 13–18 tháng 7 năm 2020, tr. 5544–5555. [Trực tuyến]. Có sẵn: http://proceedings.mlr.press/v119/kusupati20a.html
[11] J. Frankle, G. K. Dziugaite, D. Roy, và M. Carbin, "Pruning neural networks at initialization: Why are we missing the mark?" trong International Conference on Learning Representations, 2021. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=Ig-VyQc-MLK
[12] J. Frankle và M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," arXiv preprint arXiv:1803.03635, 2018.
[13] A. Morcos, H. Yu, M. Paganini, và Y. Tian, "One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers," trong Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, và R. Garnett, Eds., vol. 32. Curran Associates, Inc., 2019. [Trực tuyến]. Có sẵn: https://proceedings.neurips.cc/paper/2019/file/a4613e8d72a61b3b69b32d040f89ad81-Paper.pdf
[14] D. Blalock, J. J. G. Ortiz, J. Frankle, và J. Guttag, "What is the state of neural network pruning?" arXiv preprint arXiv:2003.03033, 2020.
[15] H. Tanaka, D. Kunin, D. L. Yamins, và S. Ganguli, "Pruning neural networks without any data by iteratively conserving synaptic flow," trong Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, và H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, tr. 6377–6389. [Trực tuyến]. Có sẵn: https://proceedings.neurips.cc/paper/2020/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf
[16] A. Renda, J. Frankle, và M. Carbin, "Comparing rewinding and fine-tuning in neural network pruning," trong International Conference on Learning Representations, 2020. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=S1gSj0NKvB
[17] J. Lee, S. Park, S. Mo, S. Ahn, và J. Shin, "Layer-adaptive sparsity for the magnitude-based pruning," trong International Conference on Learning Representations, 2021. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=H6ATjJ0TKdf
[18] M. Zhu và S. Gupta, "To prune, or not to prune: exploring the efficacy of pruning for model compression," ICLR Workshop, vol. abs/1710.01878, 2018.
[19] M. Wortsman, A. Farhadi, và M. Rastegari, "Discovering neural wirings," trong Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, và R. Garnett, Eds., vol. 32. Curran Associates, Inc., 2019. [Trực tuyến]. Có sẵn: https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf
[20] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language models are few-shot learners," Advances in Neural Information Processing Systems, 2020.
[21] E. Camci, D. Campolo, và E. Kayacan, "Deep reinforcement learning for motion planning of quadrotors using raw depth images," trong 2020 international joint conference on neural networks (IJCNN). IEEE, 2020, tr. 1–7.
[22] J. Guo, J. Liu, và D. Xu, "Jointpruning: Pruning networks along multi-ple dimensions for efficient point cloud processing," IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 6, tr. 3659–3672, 2022.
[23] C. Liu, P. Liu, W. Zhao, và X. Tang, "Visual tracking by structurally optimizing pre-trained cnn," IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 9, tr. 3153–3166, 2020.
[24] Y. Peng và J. Qi, "Quintuple-media joint correlation learning with deep compression and regularization," IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 8, tr. 2709–2722, 2020.
[25] K. Liu, W. Liu, H. Ma, M. Tan, và C. Gan, "A real-time action representation with temporal encoding and deep compression," IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, tr. 647–660, 2021.
[26] H.-J. Kang, "Accelerator-aware pruning for convolutional neural networks," IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 7, tr. 2093–2103, 2020.
[27] Y. Cheng, D. Wang, P. Zhou, và T. Zhang, "A survey of model compression and acceleration for deep neural networks," 2017.
[28] H. Kirchhoffer, P. Haase, W. Samek, K. Müller, H. Rezazadegan-Tavakoli, F. Cricri, E. B. Aksu, M. M. Hannuksela, W. Jiang, W. Wang, S. Liu, S. Jain, S. Hamidi-Rad, F. Racapé, và W. Bailer, "Overview of the neural network compression and representation (nnr) standard," IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 5, tr. 3203–3216, 2022.
[29] A. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, và A. Courville, "Dynamic capacity networks," trong International Conference on Machine Learning, 2016, tr. 2549–2558.
[30] A. Ashok, N. Rhinehart, F. Beainy, và K. M. Kitani, "N2n learning: Network to network compression via policy gradient reinforcement learning," 2017.
[31] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, và K. Keutzer, "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size," 2016.
[32] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, và J. Dean, "Efficient neural architecture search via parameter sharing," 2018.
[33] Y. LeCun, J. S. Denker, và S. A. Solla, "Optimal brain damage," trong Advances in Neural Information Processing Systems 2, D. S. Touretzky, Ed. Morgan-Kaufmann, 1990, tr. 598–605. [Trực tuyến]. Có sẵn: http://papers.nips.cc/paper/250-optimal-brain-damage.pdf
[34] B. Hassibi và D. G. Stork, "Second order derivatives for network pruning: Optimal brain surgeon," trong Advances in Neural Information Processing Systems 5, S. J. Hanson, J. D. Cowan, và C. L. Giles, Eds. Morgan-Kaufmann, 1993, tr. 164–171.
[35] N. Lee, T. Ajanthan, và P. H. Torr, "Snip: Single-shot network pruning based on connection sensitivity," arXiv preprint arXiv:1810.02340, 2018.
[36] C. Wang, G. Zhang, và R. Grosse, "Picking winning tickets before training by preserving gradient flow," trong International Conference on Learning Representations, 2020. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=SkgsACVKPH
[37] A. Peste, E. Iofinova, A. Vladu, và D. Alistarh, "AC/DC: Alternating compressed/decompressed training of deep neural networks," trong Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, và J. W. Vaughan, Eds., 2021. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=T3_AJr9-R5g
[38] P. Molchanov, S. Tyree, T. Karras, T. Aila, và J. Kautz, "Pruning convolutional neural networks for resource efficient inference," International Conference on Learning Representations, 2017.
[39] J. Liu, Z. XU, R. SHI, R. C. C. Cheung, và H. K. So, "Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers," trong International Conference on Learning Representations, 2020. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=SJlbGJrtDB
[40] P. de Jorge, A. Sanyal, H. Behl, P. Torr, G. Rogez, và P. K. Dokania, "Progressive skeletonization: Trimming more fat from a network at initialization," trong International Conference on Learning Representations, 2021. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=9GsFOUyUPi
[41] J. Guo, W. Zhang, W. Ouyang, và D. Xu, "Model compression using progressive channel pruning," IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 3, tr. 1114–1124, 2021.
[42] S. Srinivas và R. V. Babu, "Data-free parameter pruning for deep neural networks," Procedings of the British Machine Vision Conference 2015, 2015. [Trực tuyến]. Có sẵn: http://dx.doi.org/10.5244/C.29.31
[43] T. Kim, H. Choi, và Y. Choe, "Automated filter pruning based on high-dimensional bayesian optimization," IEEE Access, vol. 10, tr. 22 547–22 555, 2022.
[44] X. Ding, G. Ding, J. Han, và S. Tang, "Auto-balanced filter pruning for efficient convolutional neural networks," Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, tháng 4 năm 2018. [Trực tuyến]. Có sẵn: https://ojs.aaai.org/index.php/AAAI/article/view/12262
[45] P. Savarese, H. Silva, và M. Maire, "Winning the lottery with continuous sparsification," Advances in Neural Information Processing Systems, 2020.
[46] H. Wang, C. Qin, Y. Zhang, và Y. Fu, "Neural pruning via growing regularization," trong International Conference on Learning Representations, 2021.
[47] M. Zhao, J. Peng, S. Yu, L. Liu, và N. Wu, "Exploring structural sparsity in cnn via selective penalty," IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 3, tr. 1658–1666, 2022.
[48] N. Ström, "Sparse connection and pruning in large dynamic artificial neural networks," 1997.
[49] S. Park, J. Lee, S. Mo, và J. Shin, "Lookahead: a far-sighted alternative of magnitude-based pruning," International Conference on Learning Representations, 2020.
[50] S. Han, J. Pool, J. Tran, và W. Dally, "Learning both weights and connections for efficient neural network," trong Advances in neural information processing systems, 2015, tr. 1135–1143.
[51] J. Lin, Y. Rao, J. Lu, và J. Zhou, "Runtime neural pruning," trong Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett, Eds. Curran Associates, Inc., 2017, tr. 2181–2191. [Trực tuyến]. Có sẵn: http://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf
[52] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, và S. Han, "Amc: Automl for model compression and acceleration on mobile devices," trong European Conference on Computer Vision (ECCV), 2018.
[53] M. Gupta, S. Aravindan, A. Kalisz, V. Chandrasekhar, và L. Jie, "Learning to prune deep neural networks via reinforcement learning," International Conference on Machine Learning (ICML) AutoML Workshop, 2020.
[54] E. Camci, M. Gupta, M. Wu, và J. Lin, "Qlp: Deep q-learning for pruning deep neural networks," IEEE Transactions on Circuits and Systems for Video Technology, tr. 1–1, 2022.
[55] E. Frantar, E. Kurtic, và D. Alistarh, "M-FAC: Efficient matrix-free approximations of second-order information," trong Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, và J. W. Vaughan, Eds., 2021. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=EEq6YUrDyfO
[56] S. P. Singh và D. Alistarh, "Woodfisher: Efficient second-order approximation for neural network compression," trong Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, và H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, tr. 18 098–18 109. [Trực tuyến]. Có sẵn: https://proceedings.neurips.cc/paper/2020/file/d1ff1ec86b62cd5f3903ff19c3a326b2-Paper.pdf
[57] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," CoRR, vol. abs/1512.03385, 2015. [Trực tuyến]. Có sẵn: http://arxiv.org/abs/1512.03385
[58] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, và H. Adam, "Mobilenets: Efficient convolutional neural networks for mobile vision applications," CoRR, vol. abs/1704.04861, 2017. [Trực tuyến]. Có sẵn: http://arxiv.org/abs/1704.04861
[59] A. Kusupati, M. Singh, K. Bhatia, A. J. S. Kumar, P. Jain, và M. Varma, "Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network," trong NeurIPS, 2018.
[60] D. Anguita, A. Ghio, L. Oneto, X. Parra, và J. L. Reyes-Ortiz, "A public domain dataset for human activity recognition using smartphones," 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium, 2013.
[61] N. Lee, T. Ajanthan, S. Gould, và P. H. S. Torr, "A signal propagation perspective for pruning neural networks at initialization," trong International Conference on Learning Representations, 2020. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=HJeTo2VFwH
[62] S. Hayou, J.-F. Ton, A. Doucet, và Y. W. Teh, "Robust pruning at initialization," trong International Conference on Learning Representations, 2021. [Trực tuyến]. Có sẵn: https://openreview.net/forum?id=vXj_ucZQ4hA

--- TRANG 8 ---
8

--- TRANG 9 ---
9
Thuật toán 1 Global MP
Đầu vào: DNN init, DNN được huấn luyện trước hoặc chưa huấn luyện
κtarget, độ thưa mục tiêu
σ, ngưỡng tối thiểu (MT)
etotal, tổng số epoch
isGradual, dần dần hoặc một lần
isMT, MT được áp dụng hoặc không
Đầu ra: DNN final, DNN được cắt tỉa và huấn luyện
e = 0
DNN(we) = DNN init
while e < etotal do
    if κDNN(we) < κtarget then
        te ← CalcThreshold(e, κtarget, isGradual)
        if isMT then
            DNN(we') ← Mask(DNN(we), te)
            W ← MTcheck(DNN(we'), σ)
            DNN(we+) ← MTprune(DNN(we'), W)
        else
            DNN(we+) ← Prune(DNN(we), te)
        end if
    else
        DNN(we+) = DNN(we)
    end if
    DNN(we++) ← BackProp(DNN(we+))
    DNN(we) = DNN(we++)
    e = e + 1
end while
DNN final = DNN(we)

PHỤ LỤC
A. Mã giả
Quy trình cắt tỉa cho Global MP (Thuật toán 1 và Bảng VII) bắt đầu bằng việc lấy một mô hình được huấn luyện trước cho trường hợp cắt tỉa một lần hoặc mô hình chưa được huấn luyện cho trường hợp cắt tỉa dần dần. Tiếp theo, độ thưa của mô hình được kiểm tra và nếu độ thưa thấp hơn độ thưa mục tiêu, thì mô hình được cắt tỉa sử dụng Global MP thuần túy hoặc Global MP với MT, tùy theo lựa chọn của người dùng. Một khi mô hình được cắt tỉa, nó được huấn luyện cho trường hợp cắt tỉa dần dần hoặc tinh chỉnh cho trường hợp cắt tỉa một lần. Quy trình trên lặp lại cho đến khi đạt được epoch cuối cùng. Đối với trường hợp cắt tỉa một lần, các epoch sau chỉ được sử dụng để tinh chỉnh vì việc cắt tỉa xảy ra một lần trong epoch đầu tiên. Điều này kết thúc quy trình và kết quả cuối cùng là một mô hình được cắt tỉa và huấn luyện (hoặc tinh chỉnh).

B. Nghiên cứu Bổ sung MT
Chúng tôi cung cấp một nghiên cứu bổ sung cho ngưỡng tối thiểu (MT) (Hình 3). Chúng tôi thấy rằng MT hoạt động như một siêu tham số điển hình. Khi tăng giá trị MT ban đầu, độ chính xác tăng, cho đến khi đạt giá trị tối đa. Sau đó, việc tăng giá trị MT dẫn đến sự giảm độ chính xác. Do đó, một giá trị phù hợp có thể được tìm thấy bằng cách thực hiện tìm kiếm trên các giá trị MT. Thông thường các giá trị giữa 0.01% đến 0.3% hoạt động tốt cho MT bất kể mô hình và bộ dữ liệu.

BẢNG VII: Giải thích chức năng cho Thuật toán 1.
Chức năng Giải thích
CalcThreshold(): Tính toán ngưỡng độ lớn dưới đó các trọng số sẽ được cắt tỉa. Gán toàn bộ ngân sách cắt tỉa trong một epoch hoặc phân phối nó cho các epoch dựa trên isGradual.
Mask(): Xác định các trọng số cần cắt tỉa mà không thực sự cắt tỉa chúng.
MTcheck(): Kiểm tra các lớp vi phạm điều kiện MT và trả về một mặt nạ mới bằng cách phân phối ngân sách cắt tỉa giữa các lớp khác.
MTprune(): Cắt tỉa dựa trên mặt nạ được trả về bởi MTcheck().
Prune(): Cắt tỉa dựa trên ngưỡng.
BackProp(): Thực hiện một lần lan truyền ngược duy nhất để huấn luyện.

Hình 3: Nghiên cứu bổ sung về cách thay đổi ngưỡng tối thiểu (MT) ảnh hưởng đến độ chính xác. Giống như điều chỉnh bất kỳ siêu tham số nào, độ chính xác tăng khi MT ban đầu được tăng cho đến khi đạt giá trị tối đa, sau đó độ chính xác giảm khi tăng MT thêm. Do đó, MT dễ dàng tìm kiếm và có thể được thiết lập theo cách tương tự như tìm kiếm bất kỳ siêu tham số nào. Sử dụng MobileNet-V1 trên ImageNet.

C. Kết quả theo từng lớp
Các thuật toán cắt tỉa dễ bị các vấn đề sụp đổ lớp thảm khốc đặc biệt trong lĩnh vực độ thưa cao. Quy tắc MT có thể giúp vượt qua điều này. Áp dụng MT giúp mô hình MobileNet-V2 tránh sự sụp đổ lớp và học thành công. MT đạt được điều này bằng cách bảo tồn một khối lượng quan trọng các trọng số trong các lớp sau của MobileNet-V2 (Hình 4). Do đó, việc giữ lại một lượng nhỏ trọng số có thể giúp trong động lực học của các mô hình trong các thiết lập độ thưa cao. Chúng tôi cũng vẽ biểu đồ theo từng lớp cho Global MP và thấy rằng nó rất ổn định và tạo ra kết quả cắt tỉa nhất quán qua nhiều lần chạy và mô hình được huấn luyện trước (Hình 5).

D. Làm thưa FastGRNN trên bộ dữ liệu HAR-2
Bộ dữ liệu HAR-2 được sử dụng trong thí nghiệm cắt tỉa FastGRNN là phiên bản nhị phân hóa của bộ dữ liệu Nhận dạng Hoạt động Con người 6 lớp. Từ mô hình đầy đủ với rW = 9 và rU = 80 như được đề xuất trong bài báo STR [10], chúng tôi áp dụng GP trên các ma trận W1 và W2. Để làm điều này, chúng tôi tìm mặt nạ trọng số bằng cách xếp hạng các cột của W1 và W2 dựa trên tổng tuyệt đối của chúng, sau đó chúng tôi cắt tỉa 9−rnew W cột thấp nhất và 80−rnew U cột thấp nhất từ W1 và W2 tương ứng. Cuối cùng, chúng tôi tinh chỉnh mô hình đã cắt tỉa này bằng cách huấn luyện lại nó với trình huấn luyện của FastGRNN và áp dụng mặt nạ trọng số ở mỗi epoch.

E. Siêu tham số
Đối với CIFAR-10, chúng tôi sử dụng batch-size 128, momentum 0.9, 300 epoch, weight decay giữa 0 đến 0.001, learning rate giữa 0.01 đến 0.1 với cosine decay và nesterov momentum cho WRN-28-8. Đối với ImageNet, chúng tôi sử dụng batch-size 256, momentum 0.875, 100 epoch cho ResNet-50 và 120 cho MobileNet-V1, weight decay giữa 0 đến 3.1e-5, learning rate giữa 0.0256 đến 0.256 với cosine decay, label smoothing 0.1 và 5 epoch warm-up cho cắt tỉa dần dần. Kết quả trung bình được báo cáo qua ba lần chạy.

--- TRANG 10 ---
10
[Hình 4: Biểu đồ số trọng số còn lại theo chỉ số lớp cho MobileNet-V2 ở độ thưa 98%, so sánh Không có MT (N.A.) và Có MT (độ chính xác 82.40%)]
[Hình 5: Biểu đồ kết quả cắt tỉa theo lớp được tạo ra bởi Global MP trên mô hình MobileNet-V2 trên CIFAR-10 cho 3 lần chạy khác nhau]
