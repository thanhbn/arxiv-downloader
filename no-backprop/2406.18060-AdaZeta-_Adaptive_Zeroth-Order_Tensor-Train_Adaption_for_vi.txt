# 2406.18060.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/no-backprop/2406.18060.pdf
# Kích thước tệp: 869856 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
AdaZeta: Thích ứng Tensor-Train Bậc Không Thích ứng để
Tinh chỉnh Mô hình Ngôn ngữ Lớn Hiệu quả Bộ nhớ
Yifan Yang1Kai Zhen2Ershad Banijamali2Athanasios Mouchtaris2Zheng Zhang1
1University of California, Santa Barbara
2Amazon AGI
yifanyang@cs.ucsb.edu {kaizhen, ebanijam, mouchta }@amazon.com
zhengzhang@ece.ucsb.edu
Tóm tắt
Việc tinh chỉnh các mô hình ngôn ngữ lớn (LLM) đã
đạt được hiệu suất xuất sắc trên nhiều
nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau, nhưng
nó đòi hỏi ngày càng nhiều bộ nhớ khi kích thước
mô hình tiếp tục tăng. Để giải quyết vấn đề này,
các phương pháp Memory-efficient Zeroth-order
(MeZO) được đề xuất gần đây cố gắng tinh chỉnh
LLM chỉ sử dụng các lượt truyền xuôi, do đó
tránh được nhu cầu về đồ thị lan truyền ngược.
Tuy nhiên, sự sụt giảm hiệu suất đáng kể và
rủi ro phân kỳ cao đã hạn chế việc áp dụng
rộng rãi của chúng. Trong bài báo này, chúng tôi
đề xuất khung Adaptive Zeroth-order Tensor-Train
Adaption (AdaZeta), được thiết kế đặc biệt để
cải thiện hiệu suất và sự hội tụ của các phương
pháp ZO. Để tăng cường độ chính xác ước lượng
ZO phụ thuộc vào chiều, chúng tôi giới thiệu
bộ adapter tensor hóa tham số thấp, truyền
xuôi nhanh. Để giải quyết vấn đề phân kỳ thường
quan sát được trong các nhiệm vụ tinh chỉnh ZO
quy mô lớn, chúng tôi đề xuất lịch trình số lượng
truy vấn thích ứng đảm bảo sự hội tụ. Phân tích
lý thuyết chi tiết và kết quả thực nghiệm rộng
rãi trên các mô hình Roberta-Large và Llama-2-7B
chứng minh hiệu quả của khung AdaZeta về độ
chính xác, hiệu quả bộ nhớ và tốc độ hội tụ.12

1 Giới thiệu
Việc tinh chỉnh các mô hình ngôn ngữ lớn (LLM)
đã chứng minh hiệu suất xuất sắc trong việc
giải quyết nhiều ứng dụng xử lý ngôn ngữ tự
nhiên, chẳng hạn như hiểu ngôn ngữ tự nhiên
(Kenton và Toutanova, 2019), hỏi-đáp (Xu et al.;
Cheng et al., 2023), và tóm tắt (Zhang et al., 2024).
1Mã nguồn có sẵn trên GitHub https://github.com/
yifanycc/AdaZeta .
2Được chấp nhận bởi EMNLP 2024

Tuy nhiên, khi kích thước của LLM tăng lên, quá
trình huấn luyện tiêu thụ ngày càng nhiều bộ
nhớ GPU. Trong những năm gần đây, các phương
pháp như lượng tử hóa (Tian et al., 2023; Dettmers
et al., 2024) và tinh chỉnh hiệu quả tham số (PEFT)
(Hu et al., 2021) đã được đề xuất để giảm chi phí
bộ nhớ trong quá trình huấn luyện bằng cách lưu
trữ dữ liệu với độ sâu bit thấp hơn hoặc chỉ cập
nhật một phần tham số. Mặc dù các chiến lược
này giảm chi phí bộ nhớ một cách hiệu quả, việc
sử dụng bộ nhớ tổng thể vẫn cao do sự phụ thuộc
liên tục vào đồ thị lan truyền ngược.

Để giảm thêm chi phí bộ nhớ, (Malladi et al.,
2023) đã đề xuất phương pháp Memory-efficient
Zeroth-order (MeZO) để tinh chỉnh LLM, cho
thấy giảm bộ nhớ hơn 8× so với các phương pháp
tinh chỉnh bậc nhất (FO) như SGD (Amari, 1993)
và AdamW (Loshchilov và Hutter, 2018). Không
giống như các phương pháp FO tính toán gradient
thông qua lan truyền ngược, phương pháp MeZO
ước lượng gradient dựa trên sự khác biệt giữa
các giá trị hàm mất mát thu được từ hai lượt
truyền xuôi, do đó loại bỏ nhu cầu về đồ thị lan
truyền ngược. Tuy nhiên, hai thách thức chính
vẫn tồn tại trong việc tinh chỉnh bậc không (ZO)
của LLM: 1) khoảng cách hiệu suất đáng kể
giữa các phương pháp FO và ZO, và 2) tăng rủi
ro phân kỳ, đặc biệt trong việc tinh chỉnh ZO
của LLM quy mô lớn, như quan sát thấy trong
các nghiên cứu gần đây (Gautam et al., 2024).

Để cải thiện hiệu suất, nhiều kỹ thuật tối ưu
hóa FO đã được điều chỉnh cho các tình huống
tinh chỉnh ZO, như phương pháp ZO-AdaMU
(Jiang et al., 2024). Tuy nhiên, các phương pháp
này không thể đáp ứng các nhu cầu cụ thể của
phương pháp ZO, và thêm chi phí bộ nhớ đáng
kể từ trạng thái bộ tối ưu hóa. Do bản chất liên
quan đến tính chiều của tốc độ hội tụ ZO, (Liu
et al., 2024) đề xuất phương pháp Sparse-MeZO
tạo ra các mặt nạ tỉa dựa trên giá trị của các
phần tử trọng số. Tuy nhiên, phương pháp Sparse-
MeZO cho kết quả không nhất quán trên nhiều
nhiệm vụ và cấu hình siêu tham số khác nhau.
Ngược lại với phương pháp này, chúng tôi xem
xét sử dụng phương pháp PEFT để giảm số lượng
tham số có thể huấn luyện. Mặc dù phương pháp
ZO PEFT như MeZO-LoRA đã được xem xét trong
(Malladi et al., 2023), các cải tiến có hạn vì bộ
adapter LoRA không thể cung cấp khả năng biểu
diễn cao với hạng siêu thấp. Để giải quyết vấn
đề này, chúng tôi sử dụng các bộ adapter tensor
hóa, cung cấp hiệu suất cao với số tham số có
thể huấn luyện thậm chí thấp hơn các bộ adapter
LoRA.

Để giải quyết vấn đề phân kỳ liên quan đến
phương sai trong tinh chỉnh ZO quy mô lớn, các
nghiên cứu trước đây (Malladi et al., 2023; Jiang
et al., 2024) chủ yếu tập trung vào điều chỉnh
kích thước batch, vì tăng kích thước batch có
thể giảm nhiễu trong ước lượng gradient ZO.
Tuy nhiên, các phương pháp này gây ra chi phí
thời gian chạy đáng kể và không cải thiện hiệu
suất một cách đáng kể. Để giảm thêm phương
sai, (Gautam et al., 2024) đã giới thiệu phương
pháp MeZO-SVRG, điều chỉnh kỹ thuật SVRG
bậc nhất vào bối cảnh ZO. Mặc dù thành công,
MeZO-SVRG gặp khó khăn với quá trình tinh
chỉnh chậm và không hiệu quả về bộ nhớ do
các bản sao tham số bổ sung và quá trình tính
toán thậm chí làm tăng gấp đôi chi phí bộ nhớ
của các phương pháp MeZO. Ngược lại với các
công trình này, chúng tôi xem xét việc giảm
phương sai gradient ZO với lịch trình truy vấn2
tăng dưới tuyến tính không chỉ đạt được độ
chính xác tốt hơn mà còn hội tụ nhanh hơn về
cả bước và thời gian.

Bài báo này khám phá việc huấn luyện PEFT
cụ thể cho nhiệm vụ đối với các tình huống tinh
chỉnh ZO. Chúng tôi giới thiệu khung Adaptive
Zeroth-order Tensor-Train Adaption (AdaZeta),
kết hợp các bộ adapter tensor hóa truyền xuôi
nhanh và lịch trình truy vấn thích ứng. Sự kết
hợp này có thể cải thiện đáng kể độ chính xác
và sự hội tụ của việc tinh chỉnh ZO, như được
chứng minh trong Hình 1. Đóng góp của chúng
tôi được tóm tắt như sau:

•Chúng tôi giới thiệu khung AdaZeta, vượt
trội hơn các phương pháp tinh chỉnh ZO khác
như MeZO, MeZO-LoRA, và Sparse-MeZO
trên các nhiệm vụ khác nhau với sự hội tụ
nhanh hơn.

•Chúng tôi phát triển lịch trình số lượng truy
vấn thích ứng tăng dưới tuyến tính số lượng
truy vấn để giải quyết vấn đề phân kỳ dai
dẳng trong tinh chỉnh ZO.

•Chúng tôi cung cấp cả kết quả lý thuyết và
thực nghiệm để chứng minh hiệu quả huấn
luyện và hiệu suất của phương pháp.

2 Bối cảnh
2.1 Tinh chỉnh Hiệu quả Tham số
Trong những năm gần đây, nhiều công trình liên
quan đến các phương pháp PEFT đã được đề xuất.
Ngoài các phương pháp được sử dụng rộng rãi
nhất như Adapters (Houlsby et al., 2019) và LoRA
(Hu et al., 2021), còn có các phương pháp khám
phá các giải pháp tham số có thể huấn luyện
siêu thấp (Zaken et al., 2022; Li và

2Một truy vấn đề cập đến yêu cầu gradient của hàm mất mát
một lần trong bài báo này (Bubeck et al., 2015)[Phần 4.1.4].

--- TRANG 2 ---
Hình 1: Các đường cong hàm mất mát đánh giá cho các nhiệm vụ SST-2, WiC, và CB sử dụng mô hình Llama-2-7B. Phương pháp AdaZeta được đề xuất hội tụ nhanh hơn và giải quyết hiệu quả vấn đề phân kỳ sử dụng kích thước batch (BS) nhỏ hơn nhiều. Cả MeZO-LoRA và AdaZeta đều sử dụng tốc độ học 1e-4, trong khi Sparse-MeZO sử dụng tốc độ học 1e-6.

Liang, 2021; Liu et al., 2022). Trong (Malladi et al., 2023), các nhà nghiên cứu cố gắng sử dụng các phương pháp LoRA và prefix-tuning (Li và Liang, 2021) trong quá trình tinh chỉnh ZO. Tuy nhiên, cải tiến có hạn và phân tích chi tiết về việc tinh chỉnh ZO PEFT không được thảo luận.

Trong bài báo này, chúng tôi khám phá các bộ adapter tensor hóa, một phương pháp PEFT tham số siêu thấp nén các ma trận trọng số của các lớp adapter sử dụng phân rã Tensor-Train (TT). Phương pháp này được kiểm tra trong (Yang et al., 2024a), nơi nó thể hiện hiệu suất mạnh trong các nhiệm vụ tinh chỉnh FO. Tuy nhiên, quá trình co rút của định dạng TT (Oseledets, 2011; Novikov et al., 2015) liên quan đến một chuỗi các yếu tố tensor nhỏ làm chậm lượt truyền xuôi, khiến nó ít phù hợp hơn cho các phương pháp ZO đòi hỏi hai lượt truyền xuôi mỗi bước. Để giải quyết vấn đề này, chúng tôi đề xuất các phương pháp co rút song song để cải thiện tốc độ suy luận của các phương pháp adapter tensor hóa.

2.2 Bộ Adapter Tensor hóa
Như được thể hiện trong Hình 2 (a), các bộ adapter tensor hóa, được xây dựng dựa trên các lớp tuyến tính tensor hóa, là các thành phần nhẹ được tiêm trong quá trình tinh chỉnh để giảm số lượng tham số có thể huấn luyện. Trọng số trong các lớp tuyến tính tensor hóa được biểu diễn ở định dạng TT. So với ma trận trọng số chuẩn W∈Rm×n trong một lớp tuyến tính điển hình, định dạng TT biểu diễn tensor 2o chiều được định hình lại của nó W ∈ Rk1×···× k2o dưới dạng một chuỗi các yếu tố tensor [G1,···,Go,Go+1,···G 2o](Oseledets, 2011), trong đó mỗi yếu tố tensor Gi∈ Rri−1×ki×ri có hạng ri−1 và ri. Các chiều ki được ràng buộc sao cho Πo i=1ki=m và Π2o j=o+1kj=n. Trong lượt truyền xuôi, chuỗi các yếu tố tensor được co rút và định hình lại thành hình dạng của ma trận trọng số như

W=Reshape (G1× ··· × G 2o). (1)

Lưu ý rằng trong bài báo này, hạng tensor được giữ không đổi, ngoại trừ hạng đầu tiên và cuối cùng, được đặt r0=r2o= 1. Ngoài ra, các trọng số trong các lớp tensor hóa được khởi tạo, lưu trữ và cập nhật ở định dạng TT thay vì dạng ma trận trong một lớp tuyến tính truyền thống.

Cấu trúc của các bộ adapter tensor hóa được hiển thị trong Hình 2 (b). Mỗi bộ adapter tensor hóa chứa hai lớp tensor hóa và một lớp phi tuyến ở giữa. Đối với mỗi khối encoder/decoder, các bộ adapter tensor hóa được gắn sau lớp attention và feed-forward. Khác với (Yang et al., 2024a) làm cho cả bộ adapter tensor hóa và layer norm có thể huấn luyện, chúng tôi đóng băng layer norm trong quá trình tinh chỉnh ZO, vì ước lượng gradient nhiễu của yếu tố tỷ lệ trong chuẩn hóa lớp có thể làm suy giảm nghiêm trọng hiệu suất mô hình. Các bộ adapter tensor hóa giảm tham số có thể huấn luyện hơn 80×, khiến chúng phù hợp hơn với tinh chỉnh ZO.

3 Phương pháp
Trong phần này, trước tiên chúng tôi giới thiệu một số kiến thức cơ bản về bộ ước lượng gradient ZO. Sau đó, chúng tôi trình bày phương pháp AdaZeta, một khung mạnh mẽ được thiết kế để cải thiện hiệu suất của việc tinh chỉnh LLM ZO với hai thành phần chính: 1) các bộ adapter tensor hóa truyền xuôi nhanh, và 2) lịch trình số lượng truy vấn thích ứng. Cuối cùng, chúng tôi cung cấp phân tích lý thuyết về tốc độ hội tụ của phương pháp AdaZeta, chứng minh tốc độ hội tụ được cải thiện về mặt lý thuyết.

3.1 Ước lượng Bậc Không
Ước lượng ZO truyền thống đã được nghiên cứu rộng rãi trong cả thiết lập tối ưu hóa lồi và phi lồi.

--- TRANG 3 ---
... 
Đầu ra
2x Feed-forward
Bộ Adapter Tensor hóa
Bộ Adapter Tensor hóa
Multi-head Attention
Encoder/Decoder
đầu vào
Có thể huấn luyện Đóng băng
Phi tuyến (ReLU)
Bộ Adapter Tensor hóa
......
(a) Lớp Tuyến tính Tensor hóa
(b) Cấu trúc của các Bộ Adapter Tensor hóa
...
Hình 2: Minh họa cho lớp tuyến tính tensor hóa và các bộ adapter tensor hóa.

(Ghadimi và Lan, 2013; Malladi et al., 2023; Chen et al., 2019). Trong vấn đề của chúng tôi, xem xét một tập dữ liệu có giám sát D, mini-batch B với kích thước D và B tương ứng, chúng tôi đặt hàm mất mát cho vấn đề tinh chỉnh là ℓ(w;B), trong đó tham số có thể huấn luyện trong các bộ adapter tensor hóa w∈Rd có kích thước d. Sau đó, Ước lượng Gradient Bậc Không Ngẫu nhiên (RGE) tại bước huấn luyện k được cho bởi:

∇ˆℓ(wk) =QkX q=1ℓB(wk+ϵzq)−ℓB(wk−ϵzq) 2ϵzq

trong đó Qk là số lượng truy vấn tại bước huấn luyện k, zq∼ N(0,Id) là nhiễu loạn ngẫu nhiên theo vector cho mỗi truy vấn q, và ϵ là yếu tố tỷ lệ cho nhiễu loạn.

Không giống như tinh chỉnh FO, dựa vào lan truyền ngược, RGE chỉ yêu cầu hai lượt truyền xuôi với nhiễu loạn được thêm vào trọng số của các bộ adapter tensor hóa, loại bỏ nhu cầu về đồ thị lan truyền ngược. Ngoài ra, bằng cách tăng dưới tuyến tính số lượng truy vấn ở đầu mỗi epoch, chúng tôi có hiệu quả giảm phương sai của ước lượng gradient ZO bằng cách liên quan đến các nhiễu loạn riêng biệt zq tại mỗi lần truy vấn. Chi tiết về thiết lập sẽ được thảo luận trong phần tiếp theo.

3.2 Khung AdaZeta
Các phương pháp tinh chỉnh ZO trước đây, như MeZO, thường ước lượng gradient cho một số lượng lớn tham số có thể huấn luyện đồng thời sử dụng RGE. Phương pháp này dẫn đến phương sai cao do bản chất liên quan đến chiều của phương pháp RGE. Mặc dù các kỹ thuật như LoRA và prefix tuning đã được xem xét, ít công trình xem xét các bộ adapter PEFT cụ thể cho nhiệm vụ đối với việc tinh chỉnh LLM ZO. Ngoài ra, như được thể hiện trong Hình 1, chúng tôi đã quan sát thấy rủi ro phân kỳ tăng khi sử dụng phương pháp MeZO-LoRA trong quá trình tinh chỉnh. Để giải quyết những vấn đề này, chúng tôi đề xuất khung AdaZeta để cải thiện hiệu suất và giải quyết vấn đề bất ổn của phương pháp MeZO gốc. Khung của chúng tôi bao gồm các thành phần sau:

Bộ Adapter Tensor hóa Truyền Xuôi Nhanh. Thuật toán 1 Thuật toán AdaZeta
Đầu vào: Tham số w, hàm mất mát ℓ(·), hạt giống ngẫu nhiên sq, yếu tố tỷ lệ ϵ, hằng số liên quan đến truy vấn α, β, truy vấn tối đa Qmax, tốc độ học η.
1:for k= 1,···, K do
2: Tính số lượng truy vấn tại epoch ek bắt đầu:
Qk:= min( αeβ k, Qmax)
3: for q= 1,···, Qk do
4: w←w+ϵzq,zq∼ N(0,Id, sq)
5: ℓq +←ℓ(w,B)
6: w←w−2ϵzq,zq∼ N(0,Id, sq)
7: ℓq −←ℓ(w,B)
8: w←w+ϵzq,zq∼ N(0,Id, sq)
9: Đặt lại hạt giống ngẫu nhiên sq để tạo zq
10: end for
11:∇wˆℓ(w) =1 QkPQk q=1hℓq +−ℓq − 2ϵzqi
12: w←w−η∗ ∇wˆℓ(w)
13:end for

Vấn đề hiệu quả tham số đã được nghiên cứu rộng rãi trong các trường hợp FO, nơi mọi người thường đóng băng các tham số mô hình đã được huấn luyện trước và tinh chỉnh LLM bằng cách thêm các bộ adapter có thể huấn luyện cùng với các trọng số đã được huấn luyện trước bị đóng băng. Vì độ chính xác ước lượng ZO phụ thuộc vào chiều, việc giảm tính chiều có thể giúp cải thiện đáng kể chất lượng ước lượng gradient. Do đó, chúng tôi xem xét việc tiêm các bộ adapter tensor hóa tham số siêu thấp trong khung AdaZeta để giảm số lượng tham số có thể huấn luyện trong khi vẫn giữ được hiệu suất.

Như chúng tôi đã đề cập, việc tinh chỉnh ZO chủ yếu dựa vào ước lượng gradient với hai lượt truyền xuôi tại mỗi bước. Do đó, tốc độ của lượt truyền xuôi là một yếu tố quan trọng đối với tốc độ tổng thể của việc tinh chỉnh ZO. Thay vì sử dụng phương pháp co rút tuần tự trong lượt truyền xuôi như trong công trình trước đây, chúng tôi đề xuất một phương pháp co rút song song mới để tăng tốc các lượt truyền xuôi. Phương pháp này chia chuỗi các yếu tố tensor thành nhiều nhóm để cho phép xử lý song song và tránh sự hiện diện của các tensor chiều cao. Lấy trường hợp lưỡng phân làm ví dụ, quá trình co rút trong phương trình (1) được thay thế bởi:

W=R(oY i=1Gi2oY j=o+1Gj),

trong đó Gi biểu diễn yếu tố tensor thứ i, R(·) biểu diễn phép toán định hình lại. Đối với các mô hình lớn hơn, các yếu tố tensor có thể được tổ chức thành các cấu trúc ba phần hoặc bốn phần để tăng tốc độ suy luận của các phương pháp tensor hóa.

Điều chỉnh Truy vấn Thích ứng cho ước lượng ZO. Như đã lưu ý trước đây, quá trình huấn luyện cho các phương pháp ZO hiện tại thường thể hiện sự bất ổn, đặc biệt là với các mô hình kích thước lớn nơi các vấn đề phân kỳ thường xảy ra. Các nghiên cứu trước đây (Chen et al., 2019; Jiang et al., 2024) đã khám phá việc sử dụng lược đồ truy vấn cố định để cải thiện độ chính xác ước lượng trong cộng đồng tối ưu hóa. Tuy nhiên, việc sử dụng một số lượng truy vấn cố định có thể cản trở đáng kể hiệu quả huấn luyện của các nhiệm vụ tinh chỉnh ZO quy mô lớn, vì việc tăng ngây thơ số lượng nhiễu loạn làm tăng đáng kể thời gian huấn luyện. Để giải quyết vấn đề này, chúng tôi xem xét một lịch trình điều chỉnh số lượng truy vấn tăng dưới tuyến tính đơn giản nhưng hiệu quả, trong đó số lượng truy vấn được cập nhật ở đầu mỗi epoch ek. Bằng cách biểu diễn epoch theo các bước huấn luyện toàn cục như ek=⌊k/⌈D B⌉⌋, chúng ta có:

Qk:= min( αeβ k, Qmax) (2)

với yếu tố tỷ lệ cố định α∈(0,1), yếu tố tăng dưới tuyến tính β∈(0,1) và ngưỡng truy vấn tối đa Qmax. Sau đó, số lượng truy vấn được cố định cho tất cả các bước huấn luyện trong mỗi epoch. Điều chỉnh này giải quyết tất cả các vấn đề phân kỳ mà chúng tôi quan sát thấy với đảm bảo lý thuyết và thực hiện thậm chí nhanh hơn cách truyền thống để giải quyết vấn đề phân kỳ cho việc tinh chỉnh LLM ZO bằng cách tăng kích thước batch.

Thuật toán tối ưu hóa tương ứng được sử dụng trong khung AdaZeta được thể hiện trong Thuật toán 1. Chúng tôi điều chỉnh số lượng truy vấn ở đầu mỗi epoch. Khác với thuật toán MeZO, chúng tôi có được gradient được sử dụng để cập nhật mô hình bằng cách lấy trung bình trên nhiều kết quả truy vấn. Lưu ý rằng chúng tôi cố định số lượng truy vấn là 1 khi tinh chỉnh các mô hình kích thước trung bình như Roberta-Large vì nhiễu của ước lượng ZO tương đối thấp khi số lượng tham số có thể huấn luyện nhỏ. Sau đó, chúng tôi sẽ cho thấy rằng một số lượng truy vấn tăng dưới tuyến tính có lợi cho sự hội tụ của vấn đề khi kích thước mô hình lớn, cả về lý thuyết và thực nghiệm.

3.3 Phân tích Lý thuyết
Trong phần này, chúng tôi đưa ra phân tích lý thuyết cho khung AdaZeta. Phân tích lý thuyết của chúng tôi nêu bật lý do tại sao bộ adapter tensor hóa và lịch trình truy vấn thích ứng có thể giúp cải thiện đáng kể tốc độ hội tụ ZO. Không giống như phân tích lý thuyết trong bài báo MeZO, tập trung vào "hạng hiệu quả" cho Hessian của mất mát, chúng tôi tập trung vào chiều của các mô hình được tối ưu hóa d (số lượng tham số có thể huấn luyện) thay vào đó. Vì các tham số có thể huấn luyện với bộ adapter PEFT nhỏ hơn nhiều so với kích thước mô hình, phân tích lý thuyết dựa trên chiều chính xác của vấn đề tối ưu hóa có thể giúp chúng tôi khám phá hành vi của các phương pháp PEFT khác nhau tốt hơn.

Để căn chỉnh phân tích của chúng tôi với việc tinh chỉnh LLM, chúng tôi xem xét một thiết lập tối ưu hóa phi lồi và nghiên cứu hành vi hội tụ liên quan đến các bước huấn luyện k. Điều quan trọng cần lưu ý là gradient ước lượng ZO ∇ˆℓ bởi RGE, là một ước lượng không thiên vị của gradient thật ∇ℓ khi ϵ→0, điều này cho thực tế Ez[∇ˆℓ] =∇ℓ(Nesterov và Spokoiny, 2017). Đầu tiên, chúng tôi liệt kê các giả định sau cho phân tích:

A1: Hàm mất mát ℓ có gradient liên tục L-Lipschitz, trong đó với L >0 chúng ta có:
∥∇ℓ(wi)− ∇ℓ(wj)∥ ≤L∥wi−wj∥,∀wi,wj

A2: Tại mỗi bước k, gradient của hàm mất mát ℓ được giới hạn trên như ∥∇ℓ∥ ≤δ,∀k.

Sau đó, chúng tôi cung cấp tốc độ hội tụ toàn cục cho thuật toán AdaZeta:

Định lý 1. Dưới A1 và A2, chọn ngẫu nhiên wT từ lịch sử với xác suất P(T=k) =1 K, sự hội tụ của thuật toán AdaZeta có thể được giới hạn bởi:

E[∥∇ℓ(wT)∥2]≤ O(R+ϵ2L+C(d, ϵ)P k1 Qk Kϵ),

--- TRANG 4 ---
Bảng 1: Phân tích so sánh của các phương pháp tinh chỉnh ZO khác nhau trên các mô hình Roberta-Large.

Phương pháp RTE SST-2 SST-5 QNLI MNLI SNLI MR
FT 66.4 91.9 47.5 63.4 70.0 77.5 88.2
Zero-Shot 51.4 79.0 35.5 50.9 48.8 50.2 80.2
LP 59.4 76.0 40.3 57.6 56.5 66.0 86.6
BS=16
MeZO 52.7 90.5 31.1 59.9 60.5 63.5 85.5
MeZO-LoRA 52.7 84.2 44.8 60.3 58.5 65.6 85.7
AdaZeta 66.8 91.4 48.3 61.3 58.1 69.1 87.0
BS=64
MeZO 64.0 90.5 45.5 60.5 58.7 68.5 85.0
MeZO-LoRA 63.9 91.3 43.0 59.0 64.0 69.7 87.4
AdaZeta 64.3 91.5 49.6 60.7 68.1 68.7 86.5

trong đó R được định nghĩa bởi khoảng cách giữa điểm bắt đầu và giải pháp tối ưu ℓ(w1)−ℓ∗, yếu tố tỷ lệ nhiễu loạn ZO được biểu diễn là ϵ, và C(d, ϵ) là một hằng số liên quan đến kích thước tham số mô hình d, được định nghĩa ở cuối bằng chứng trong Phụ lục C.

Chứng minh. Chi tiết có thể tìm thấy trong Phụ lục C.

Theo Định lý 1, chúng ta có thể quan sát rằng giới hạn liên quan đến lịch trình truy vấn. Để thuận tiện, lấy trường hợp đơn giản với α=β= 0.5 và bỏ qua tối thiểu trong phương trình (2), chúng ta có Qk= 1 2q ⌊k/⌈D B⌉⌋, cho PK k=11 Qk≤2D Bs K ⌈D B⌉, điều này đảm bảo gradient thật tiếp cận zero khi K→ ∞ . Ngược lại, sử dụng một hằng số nhỏ như Q= 1 dẫn đến giới hạn trên O(C(d, ϵ)/Kϵ), trở nên thách thức để tối thiểu hóa do hạng C(d, ϵ) tỷ lệ thuận với kích thước mô hình d. Ngoài ra, chúng ta quan sát rằng tốc độ hội tụ bị ảnh hưởng đáng kể bởi chiều mô hình d. Do đó, trong bài báo này, chúng tôi cũng cố gắng giảm số lượng tham số có thể huấn luyện với các bộ adapter tensor hóa.

4 Thực nghiệm
Trong phần này, chúng tôi tiến hành các thực nghiệm toàn diện để đánh giá hiệu suất của khung AdaZeta được đề xuất trên một số LLM với các quy mô khác nhau trên nhiều nhiệm vụ hiểu và tạo sinh ngôn ngữ tự nhiên (Socher et al., 2013; Williams et al., 2017; Rajpurkar et al., 2016). Chúng tôi chứng minh rằng các phương pháp của chúng tôi vượt trội so với một loạt các đường cơ sở hiệu quả bộ nhớ toàn diện, bao gồm các phương pháp chỉ suy luận như Zero-shot (Brown et al., 2020), In-Context Learning (ICL), và Linear Probing (LP) (Kumar et al., 2021), cũng như các phương pháp tinh chỉnh ZO như MeZO, MeZO-LoRA (Malladi et al., 2023), và Sparse-MeZO (Liu et al., 2024). Ngoài ra, đường cơ sở tinh chỉnh bậc nhất (FT) cũng được cung cấp làm tham khảo.

Ban đầu, chúng tôi trình bày bằng chứng thực nghiệm sử dụng các mô hình Roberta-Large (Liu et al., 2019), minh họa rằng việc tích hợp các bộ adapter tensor hóa có thể cải thiện đáng kể hiệu quả của việc tinh chỉnh ZO bằng cách giảm số lượng tham số có thể huấn luyện. Sau đó, chúng tôi kích hoạt phương pháp lịch trình truy vấn thích ứng được đề xuất để cho thấy hiệu quả của khung AdaZeta trên các mô hình Llama-2-7B quy mô lớn (Touvron et al., 2023), không chỉ cải thiện hiệu suất mà còn đảm bảo sự hội tụ mạnh mẽ. Tất cả các thực nghiệm được tiến hành trên GPU NVIDIA Tesla A100-40GB, với chi tiết thêm về thiết lập thực nghiệm có sẵn trong Phụ lục A.

4.1 Mô hình Roberta-Large Kích thước Trung bình
Chúng tôi ban đầu đánh giá hiệu quả của việc sử dụng các bộ adapter tensor hóa trên các mô hình RoBERTa-large qua nhiều nhiệm vụ khác nhau, bao gồm các nhiệm vụ câu đơn như SST-2 và SST-5, các nhiệm vụ suy luận ngôn ngữ tự nhiên như QNLI, MNLI, SNLI, RTE, và tập dữ liệu phân tích tình cảm Movie Reviews (MR). Kết quả được tóm tắt trong Bảng 1. Các thực nghiệm được tiến hành dưới thiết lập 16-shot, với 16 mẫu dữ liệu trong mỗi lớp của các tập dữ liệu. Chúng tôi theo dõi độ chính xác kiểm tra tốt nhất mỗi 500 bước, sử dụng pool kiểm tra 1,000 mẫu dữ liệu. Lưu ý rằng, tương tự như các nghiên cứu tinh chỉnh ZO trước đây, chúng tôi cố định số lượng truy vấn là 1 trong phần này. Quyết định này dựa trên quan sát rằng nhiễu gradient tương đối nhỏ trong các mô hình dựa trên Bert kích thước trung bình. Các kết luận sau đây đã được đưa ra:

AdaZeta Cho thấy Độ chính xác Cao hơn các Phương pháp Tinh chỉnh ZO Khác. Theo quan sát của chúng tôi trong Bảng 1, AdaZeta vượt trội so với các phương pháp tinh chỉnh ZO khác về độ chính xác đánh giá. So với MeZO-LoRA, cũng liên quan đến các bộ adapter PEFT, AdaZeta vượt trội trong 5 trên 7 thử nghiệm dưới cả thiết lập kích thước batch (BS) 16 và 64. Lợi thế này cho thấy hiệu quả của việc cải thiện độ chính xác ước lượng ZO bằng cách giảm thêm số lượng tham số có thể huấn luyện với bộ adapter tensor hóa. Điều này được hỗ trợ bởi tốc độ hội tụ liên quan đến chiều được chứng minh trong Phần 3.3.

AdaZeta Thể hiện Sự hội tụ Được cải thiện. So với phương pháp MeZO-LoRA, phương pháp AdaZeta thể hiện sự hội tụ vượt trội khi kích thước batch là 16. Với thiết lập huấn luyện 16-shot, có thể hợp lý khi kỳ vọng rằng tình huống kích thước batch 16 sẽ vượt trội hơn tình huống kích thước batch 64 nếu quá trình tinh chỉnh hội tụ hiệu quả. Tuy nhiên, sự suy giảm hiệu suất được quan sát thấy với phương pháp MeZO-LoRA, cho thấy nó bị ảnh hưởng bất lợi bởi nhiễu gradient ZO. Tương đối, phương pháp AdaZeta đạt được kết quả nhất quán qua cả hai thiết lập bằng cách giảm nhiễu như vậy với ít tham số có thể huấn luyện hơn, thể hiện hiệu quả khả năng hỗ trợ hội tụ.

4.2 Mô hình Llama-2 Quy mô Lớn
Trong phần trước, chúng tôi đã chứng minh cách sử dụng phương pháp bộ adapter tensor hóa cải thiện hiệu suất tinh chỉnh ZO bằng cách giảm nhiễu gradient thông qua việc giảm tham số có thể huấn luyện. Trong phần này, chúng tôi đánh giá hiệu quả của khung AdaZeta với mô hình Llama-2-7B quy mô lớn. Khác với các thực nghiệm trên các mô hình Roberta-Large, chúng tôi kích hoạt phương pháp lịch trình truy vấn thích ứng được đề xuất trong khung AdaZeta để giảm thiểu các vấn đề phân kỳ thường quan sát thấy trong việc tinh chỉnh ZO quy mô lớn.

Để làm nổi bật thách thức của các thực nghiệm, chúng tôi áp dụng phương pháp tài nguyên dữ liệu thấp sử dụng các tập dữ liệu từ SuperGLUE (Wang et al., 2019) và các nhiệm vụ tạo sinh như SQuAD (Rajpurkar et al., 2016) và DROP (Dua et al., 2019). Giao thức thực nghiệm của chúng tôi tuân theo chiến lược tinh chỉnh dựa trên prompt được nêu trong bài báo MeZO (Malladi et al., 2023). Kết quả định lượng được tóm tắt trong Bảng 2 và các đường cong huấn luyện đã được hiển thị trong Hình 1. Lưu ý rằng có thể hợp lý khi quan sát một số khoảng cách độ chính xác lớn giữa các phương pháp khác nhau dưới các nhiệm vụ khác nhau, điều này cũng đã được quan sát thấy trong các bài báo MeZO và PEFT trước đây (Malladi et al., 2023; Hu et al., 2023). Các kết luận sau được rút ra:

Phương pháp AdaZeta Thể hiện Hiệu suất Vượt trội So với Tinh chỉnh ZO Truyền thống. Khung AdaZeta mang lại kết quả độ chính xác xuất sắc qua nhiều nhiệm vụ khác nhau, vượt trội so với tất cả các phương pháp cơ sở ZO như MeZO và MeZO-LoRA trong 8 trên 10 nhiệm vụ. So với các phương pháp chỉ suy luận truyền thống như ICL và Zero-shot, AdaZeta vượt trội đáng kể so với chúng về độ chính xác kiểm tra.

--- TRANG 5 ---
Bảng 2: Phân tích so sánh của các phương pháp tinh chỉnh ZO khác nhau trên mô hình Llama-2-7B.

Phương pháp RTE CB BoolQ WSC WIC SST2 MultiRC COPA ReCoRD SQuAD
FT 61.7 66.1 84.6 63.4 65.9 94.0 45.4 86.0 81.1 90.7
LoRA 85.5 67.8 84.8 62.5 73.9 94.8 85.0 81.0 79.4 90.5
Zero-Shot 49.5 32.1 65.1 36.5 50.6 79.7 55.8 59.7 80.9 54.7
ICL 54.5 58.9 67.4 65.4 52.7 81.2 58.7 84.4 80.1 67.1
MeZO 54.6 73.0 68.6 52.8 57.8 85.8 62.6 86.0 70.8 72.5
MeZO-LoRA 59.6 74.0 71.6 53.0 55.2 86.8 67.2 89.0 72.0 80.0
Sparse-MeZO 58.6 76.0 67.8 53.0 56.8 85.2 61.2 86.0 70.6 64.4
AdaZeta 74.0 75.0 79.4 52.2 58.0 91.0 68.2 94.0 71.2 80.0

Hơn nữa, phương pháp AdaZeta thậm chí vượt trội so với các phương pháp FO-AdamW trên một số nhiệm vụ như RTE, CB, và COPA, đòi hỏi bộ nhớ GPU nhiều hơn 8×.

Phương pháp AdaZeta Giải quyết Hiệu quả Vấn đề Phân kỳ trong Tinh chỉnh ZO. Chúng ta có thể quan sát từ bảng rằng các phương pháp MeZO và MeZO-LoRA đạt kết quả không thỏa mãn trong một số nhiệm vụ như SST2, RTE, và BoolQ so với phương pháp được đề xuất, điều này dẫn đến vấn đề hội tụ. Ngoài ra, chúng tôi đã chỉ ra rằng phương pháp AdaZeta đạt được hàm mất mát đánh giá thấp hơn nhanh hơn nhiều so với các phương pháp MeZO-LoRA và Sparse-MeZO qua tất cả các nhiệm vụ trong Hình 1. Ví dụ, phương pháp MeZO-LoRA yêu cầu gần 6K bước để đạt được mất mát 0.4, trong khi phương pháp AdaZeta đạt được mức độ tối thiểu hóa mất mát tương tự trong ít hơn 1K bước, biểu thị tăng tốc 6× với cùng tốc độ học 1e-4. Cách truyền thống để giải quyết các vấn đề phân kỳ như vậy thông qua việc tăng kích thước batch khó thực hiện trong các nhiệm vụ tinh chỉnh LLM quy mô lớn. Ngược lại, lịch trình truy vấn thích ứng trong khung AdaZeta thành công giảm thiểu vấn đề này mà không tăng bộ nhớ huấn luyện, do đó cải thiện kết quả huấn luyện.

Ngoài ra, chúng tôi quan sát thấy rằng việc kết hợp LoRA với lịch trình truy vấn thích ứng cải thiện đáng kể hiệu suất trong một số nhiệm vụ nhất định. Công trình tương lai cũng có thể khám phá việc kết hợp lịch trình truy vấn thích ứng vào phương pháp MeZO-LoRA để tăng cường thêm tính ổn định.

4.3 Hiệu quả Bộ nhớ và Thời gian Huấn luyện
Trong phần này, chúng tôi đánh giá hiệu quả bộ nhớ và thời gian của phương pháp AdaZeta. Cụ thể, chúng tôi kiểm tra chi phí bộ nhớ đỉnh của các phương pháp tinh chỉnh khác nhau trên mô hình Llama-2-7B và nghiên cứu sự đánh đổi giữa bộ nhớ, độ chính xác, và thời gian huấn luyện. Kết quả được tóm tắt trong Hình 3 và thảo luận thêm về bộ nhớ huấn luyện có thể tham khảo Phụ lục B.1.

Theo Hình 3 (tham khảo Phụ lục B.1 cho kết quả số), phương pháp AdaZeta chỉ yêu cầu 14GB bộ nhớ để tinh chỉnh nhiệm vụ SST2 trên mô hình Llama-2-7B, đạt được Giảm Bộ nhớ hơn 8× So với Phương pháp FT. Ngoài ra, so với các phương pháp tinh chỉnh ZO khác như MeZO, MeZO-LoRA, và Sparse-MeZO, phương pháp AdaZeta sử dụng bộ nhớ tương tự hoặc thậm chí ít hơn để đạt được giảm phương sai. Cách truyền thống để giảm nhiễu ước lượng gradient ZO như tăng kích thước batch, tiêu thụ đáng kể nhiều bộ nhớ hơn phương pháp AdaZeta như thể hiện trong Hình 3.

Trong Bảng 3, chúng tôi đo tổng số giờ GPU cần thiết để đạt được ngưỡng nhất định của hàm mất mát huấn luyện qua bốn nhiệm vụ (SST2, WIC, CB, MultiRC). Để có tính khả thi của các thực nghiệm, chúng tôi thiết lập ngưỡng hàm mất mát đánh giá mà tất cả các phương pháp có thể đạt được. Theo kết quả, rõ ràng rằng phương pháp AdaZeta hội tụ tương đương hoặc nhanh hơn các phương pháp tinh chỉnh ZO khác với kết quả thậm chí tốt hơn so với các phương pháp MeZO-LoRA và Sparse-MeZO dưới trường hợp kích thước batch lớn. Lưu ý rằng chúng tôi không sử dụng kỹ thuật tích lũy gradient cho trường hợp kích thước batch 64, có thể làm tăng đáng kể thời gian huấn luyện.

--- TRANG 6 ---
Bảng 3: Số giờ GPU cần thiết (Số lượng GPU × Giờ huấn luyện) để đạt được mỗi hàm mất mát đánh giá cho các phương pháp tinh chỉnh ZO khác nhau trên mô hình Llama-2-7B.

Phương pháp SST2 WIC CB MultiRC
MeZO-LoRA(BS=64) 3.0 4.8 8,6 30.0
MeZO-LoRA(BS=16) 0.6 1.1 3.1 10.8
Sparse-MeZO 4.1 3.6 4.3 6.4
AdaZeta 1.1 1.0 0.9 12.1

AdaZeta (Của chúng tôi)
Hình 3: Sự đánh đổi giữa độ chính xác và chi phí bộ nhớ cho các phương pháp tinh chỉnh khác nhau. Chúng ta có thể quan sát rằng phương pháp AdaZeta đạt được độ chính xác tốt nhất trong số các phương pháp hiệu quả bộ nhớ.

4.4 So sánh Thêm với Phương pháp LoRA
Trong phần này, chúng tôi so sánh thêm phương pháp AdaZeta với phương pháp LoRA bậc nhất về việc sử dụng bộ nhớ huấn luyện qua các hạng và kích thước batch khác nhau. Kết quả cho nhiệm vụ CB được trình bày trong Bảng 4. Chúng tôi đưa ra các quan sát sau dưới hai tình huống:

Giảm Hạng LoRA: Giảm hạng LoRA (thậm chí xuống 1) có tác động tối thiểu đến bộ nhớ huấn luyện trong thiết lập bậc nhất. Lý do là đồ thị lan truyền ngược—chứa thông tin gradient trung gian—vẫn cần được giữ lại, trải dài gần như toàn bộ mô hình trong phương pháp LoRA gốc.

Giảm Kích thước Batch: Giảm kích thước batch là cách hiệu quả hơn để giảm bộ nhớ huấn luyện cho cả trường hợp FO và ZO. Với sự tồn tại của đồ thị lan truyền ngược, có thể hợp lý khi quan sát việc giảm bộ nhớ huấn luyện lớn hơn của phương pháp FO so với ZO khi giảm số lượng kích thước batch. Tuy nhiên, chúng ta có thể quan sát rằng thậm chí khi so sánh phương pháp của chúng tôi với phương pháp LoRA sử dụng kích thước batch 1, phương pháp của chúng tôi vẫn hiệu quả hơn 2.5× về bộ nhớ. Ngoài ra, thậm chí khi so sánh AdaZeta/r=8/BS=16 với LoRA/r=1/BS=1, chúng tôi vẫn đạt được gần 50% giảm sử dụng bộ nhớ. Tuy nhiên, chúng tôi muốn nhấn mạnh rằng thiết lập kích thước batch 1 hiếm khi được sử dụng trong thực tế do những lý do sau:

•Đầu tiên, giảm kích thước batch sẽ tăng đáng kể thời gian huấn luyện của phương pháp LoRA.

•Thứ hai, kích thước batch nhỏ như vậy dẫn đến nhiễu ngẫu nhiên lớn trong quá trình tinh chỉnh, làm hại thêm hiệu suất huấn luyện. (Hu et al., 2023)

5 Kết luận
Trong bài báo này, chúng tôi đề xuất khung tinh chỉnh bậc không thích ứng với phân rã tensor-train, được đặt tên là AdaZeta. So với các công trình tinh chỉnh ZO trước đây, phương pháp AdaZeta đạt được kết quả tinh chỉnh tốt hơn đáng kể qua nhiều nhiệm vụ và mô hình khác nhau. Phân tích lý thuyết đã xác nhận rằng các phương pháp được đề xuất có sự hội tụ tốt hơn, phù hợp với kết quả thực nghiệm của chúng tôi trên cả mô hình Roberta-Large và Llama-2 qua nhiều nhiệm vụ tinh chỉnh khác nhau.

Công trình tương lai có thể khám phá việc cải thiện hiệu quả của phương pháp AdaZeta bằng cách triển khai tối ưu hóa phân tán trên nhiều GPU để xử lý nhiều truy vấn đồng thời tại mỗi bước. Ngoài ra, việc áp dụng lịch trình truy vấn thích ứng cho các phương pháp PEFT khác có thể mang lại hiệu suất tốt hơn đáng kể so với thuật toán MeZO gốc.

Lời cảm ơn
Dự án này được hỗ trợ bởi Amazon. Chúng tôi bày tỏ lòng biết ơn đối với Siegfried Kunzmann, Jiajun Zhou, Clement Chung, Samridhi Choudhary, Hieu Nguyen và nhiều đồng nghiệp khác tại Amazon AGI và UCSB đã tham gia vào các cuộc thảo luận hình thành công trình này.

Nghiên cứu này cũng sử dụng tài nguyên từ Trung tâm Điện toán Khoa học Nghiên cứu Năng lượng Quốc gia (NERSC), một Cơ sở Người dùng Văn phòng Khoa học Bộ Năng lượng Hoa Kỳ, được hỗ trợ dưới Hợp đồng số DE-AC02-05CH11231 thông qua giải thưởng NERSC ASCR-ERCAP0030039.

Hạn chế
Hạn chế chính của công trình này liên quan đến việc tăng tốc phương pháp được đề xuất. Hiện tại, nhiều truy vấn tại mỗi bước huấn luyện được thực hiện tuần tự trong vòng lặp for, làm hạn chế các cải tiến tốc độ thêm. Quá trình này có thể được tối ưu hóa bằng cách triển khai các kỹ thuật tối ưu hóa song song hoặc phân tán trên GPU, cho phép thực hiện đồng thời nhiều truy vấn, vì các truy vấn này độc lập với nhau với các hạt giống ngẫu nhiên khác nhau.

Rủi ro Tiềm ẩn
Bài báo này cung cấp giải pháp hiệu quả chi phí hoạt động với dấu chân bộ nhớ tối thiểu. Mặc dù chúng tôi cần tinh chỉnh các mô hình quy mô lớn, phương pháp được đề xuất có thể giảm bớt gánh nặng cho các trung tâm dữ liệu và giảm phát thải CO2. Tuy nhiên, chúng tôi thừa nhận rằng thời gian huấn luyện kéo dài, đặc biệt là với nhiều GPU, có thể gây ra thách thức môi trường. Do đó, các nỗ lực nghiên cứu đang tiến hành của chúng tôi tập trung vào việc phát triển các phương pháp huấn luyện hiệu quả hơn và bảo tồn sức mạnh tính toán với các cân nhắc sinh thái.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên do có nhiều tên riêng và thông tin kỹ thuật]

--- TRANG 7 ---
Bảng 4: So sánh với phương pháp LoRA bậc nhất dưới hạng thấp và kích thước batch.

Thiết lập LoRA/r=1/BS=1 LoRA/r=1/BS=8 LoRA/r=8/BS=8 AdaZeta/r=8/BS=1 MeZO-LoRA/r=8/BS=16 AdaZeta/r=8/BS=16
Bộ nhớ (GB) 35.60 96.65 96.72 14.05 23.02 23.01

--- TRANG 8 ---
[Tiếp tục với các phần còn lại của tài liệu, bao gồm phần Phụ lục A về Chi tiết Thiết lập Thực nghiệm, Phụ lục B về Thực nghiệm Bổ sung, và Phụ lục C về Chứng minh Định lý 1]

--- TRANG 9 ---
A Chi tiết Thiết lập Thực nghiệm
A.1 Thiết lập Tập dữ liệu
[Bảng 5 và 6 với các thông tin về metrics đánh giá]

--- TRANG 10 ---
A.2 Đường cơ sở
[Mô tả chi tiết về các phương pháp đường cơ sở]

--- TRANG 11 ---
A.3 Siêu tham số
[Bảng 7, 8, 9 với các thiết lập siêu tham số chi tiết]

--- TRANG 12 ---
B Thực nghiệm Bổ sung
B.1 Kết quả So sánh Bộ nhớ Bổ sung
[Bảng 10 với kết quả so sánh bộ nhớ]

--- TRANG 13 ---
C Chứng minh Định lý 1
[Phần chứng minh toán học chi tiết]

--- TRANG 14 ---
[Tiếp tục chứng minh]

--- TRANG 15 ---
[Tiếp tục chứng minh]

--- TRANG 16 ---
[Hoàn thành chứng minh]
