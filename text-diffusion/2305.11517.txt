# 2305.11517.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/text-diffusion/2305.11517.pdf
# File size: 813881 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DIFFU SIA: A Spiral Interaction Architecture for
Encoder-Decoder Text Diffusion
Chao-Hong Tan, Jia-Chen Gu, Zhen-Hua Ling
National Engineering Research Center of Speech and Language Information Processing,
University of Science and Technology of China, Hefei, China
chtan@mail.ustc.edu.cn ,{gujc,zhling}@ustc.edu.cn
Abstract
Diffusion models have emerged as the new
state-of-the-art family of deep generative mod-
els, and their promising potentials for text
generation have recently attracted increasing
attention. Existing studies mostly adopt a
single encoder architecture with partially nois-
ing processes for conditional text generation,
but its degree of ﬂexibility for conditional
modeling is limited. In fact, the encoder-
decoder architecture is naturally more ﬂexi-
ble for its detachable encoder and decoder
modules, which is extensible to multilingual
and multimodal generation tasks for condi-
tions and target texts. However, the encoding
process of conditional texts lacks the under-
standing of target texts. To this end, a spiral in-
teraction architecture for encoder-decoder text
diffusion (DiffuSIA) is proposed. Concretely,
the conditional information from encoder is de-
signed to be captured by the diffusion decoder,
while the target information from decoder is
designed to be captured by the conditional
encoder. These two types of information ﬂow
run through multi-layer interaction spirally for
deep fusion and understanding. DiffuSIA is
evaluated on four text generation tasks, includ-
ing paraphrase, text simpliﬁcation, question
generation, and open-domain dialogue genera-
tion. Experimental results show that DiffuSIA
achieves competitive performance among pre-
vious methods on all four tasks, demonstrating
the effectiveness and generalization ability of
the proposed method.1
1 Introduction
Diffusion models have recently become state-of-
the-art for deep generative models, surpassing
generative adversarial networks (GANs) (Good-
fellow et al., 2014) or normalizing ﬂow (Dinh
et al., 2017) in generative tasks such as image
synthesis (Dhariwal and Nichol, 2021; Ho et al.,
1Code will be available at https://github.com/l
xchtan/DiffuSIA
t=0 t=T/2 t=T t=0 t=T/2 t=T
t=0 t=T/2 t=T t=0 t=T/2 t=T
Encoder Encoder(a)(b)
(c)(d)Classifier
Figure 1: Comparison of existing methods for text
diffusion. Blue dots denote conditional signals. (a)
Diffusion-LM (Li et al., 2022): classiﬁer-guided text
diffusion, where diffusion texts are sent to a pre-
trained classiﬁer and controlled by the return gradients.
(b) DiffuSeq (Gong et al., 2022): single encoder-
based text diffusion, where conditional text remains
constant and is concatenated with target text during
partially noising diffusion. (c) Encoder-decoder text
diffusion: conditional text is encoded by a separate
encoder and inﬂuences the generation process by cross-
attention. (d) DiffuSIA: spiral interaction architecture
for encoder-decoder text diffusion, where conditional
text and target text perceived mutually through two
splitting cross-attentions.
2020; Ramesh et al., 2022; Rombach et al., 2022).
Recently, different from traditional auto-regressive
generation processing (Radford et al., 2019; Lewis
et al., 2020; Tan et al., 2022), the natural language
processing community has also started to apply
diffusion methods to the task of text generation
considering their promising potentials (Austin
et al., 2021; Li et al., 2022; Chen et al., 2022; Gong
et al., 2022). Diffusion process typically operates
in continuous space, which is naturally suitable for
processing images. However, a major challenge to
text diffusion lies in that text inherently operates in
discrete space.
Researchers have made efforts to applying diffu-
sion models to various text generation tasks. For
example, Diffusion-LM (Li et al., 2022) designsarXiv:2305.11517v1  [cs.CL]  19 May 2023

--- PAGE 2 ---
an embedding step and a rounding step in the
standard diffusion process (Ho et al., 2020) for
unconditional and controllable text generation. For
conditional text generation, DiffuSeq (Gong et al.,
2022) adopts partially noising processes with only
a single Transformer encoder (Vaswani et al., 2017)
and is trained end-to-end in a classiﬁer-free manner.
Despite the conditions can be integrated into
the diffusion generation process, the conditional
encoder and the diffusion decoder are bound
together and cannot be designed ﬂexibly and inde-
pendently. Considering the limitations of the single
Transformer encoder architecture for text diffusion,
the encoder-decoder architecture shows its natural
ﬂexibility since two different modules can be
designed for condition encoding and diffusion
decoding respectively. However, the sight of the
other side of the coin should never be overlooked.
This separation design makes the encoding of
conditional text incapable of perceiving target text
during the diffusion process, which might degrade
the understanding of conditional text. But this issue
has not been studied in previous work.
Note that the generation process of diffusion is
essentially non-autoregressive (NAR) with multiple
iterations, thus the target information can be uti-
lized to assist in understanding the conditional text
without information leakage. In light of the above
issues, a spiral interaction architecture for encoder-
decoder text diffu sion (DiffuSIA) is proposed in
this paper. Comparison of existing methods for
text diffusion are illustrated in Figure 1. The
conditional information from encoder is designed
to be captured by the diffusion decoder, while the
target information from decoder is designed to be
captured by the conditional encoder. In detail,
the encoder layer initially engages in interactions
with the target text information through cross-
attention to acquire the target-aware conditional
(TaC) information. Subsequently, the acquired
TaC information is utilized in the interactions
with the decoder layer through another cross-
attention, deriving the condition-aware target (CaT)
information. These two types of information
ﬂow run through multi-layer interaction spirally,
augmenting encoding and perception of both condi-
tional text and target text. In this way, DiffuSIA is
able to provide a ﬂexible option for conditional text
diffusion generation. Because of the NAR process
of diffusion, the decoder does not require a causal
mask. Besides, inspired by previous works (Chenet al., 2022; Strudel et al., 2022; Dieleman et al.,
2022), the diffusion generation result from previous
timesteps are used for self-conditioning (Chen
et al., 2022) to predict the target at the current
timestep.
To measure the effectiveness of the proposed
method, following the setting of Gong et al. (2022),
we evaluate the performance on four popular text
generation tasks, including paraphrase, text sim-
pliﬁcation, question generation, and open-domain
dialogue generation. Experiments on these text
generation tasks show that our method achieves
competitive performance. These results verify the
effectiveness of the spiral interactions for encoder-
decoder text diffusion, and the generalization
ability over various text generation tasks. To
facilitate others to reproduce our results, we will
publish all source code later.
In summary, our contributions in this paper are
three-fold: 1) This paper makes the exploration
of applying the encoder-decoder architecture for
text diffusion. 2) A spiral interaction architecture
is proposed for encoder-decoder text diffusion,
which is composed of the target-aware conditional
(TaC) and condition-aware target (CaT) informa-
tion ﬂows. 3) Experiments on four types of
text generation tasks verify the effectiveness and
generalization ability of the proposed method.
2 Related Work
In recent years, diffusion models have achieved
great success in the domain of image synthe-
sis (Nichol et al., 2022; Ramesh et al., 2022; Kwon
and Ye, 2022; Rombach et al., 2022). Because of
its amazing generation quality, some works apply
diffusion model to the domain of text generation.
There are two general lines of work on text
diffusion, namely discrete diffusion on discrete
data (Hoogeboom et al., 2021; Austin et al., 2021;
Savinov et al., 2022; Reid et al., 2022; He et al.,
2022) and continuous diffusion on discrete data. In
this paper, we study the latter.
Unconditional and Controllable Text Diffusion
Bit Diffusion (Chen et al., 2022) uses real numbers
to model the bits of data for enabling continuous
state diffusion models to generate discrete data.
Besides, self-conditioning and asymmetric time
intervals that greatly improve the sample quality.
Diffusion-LM (Li et al., 2022) maps discrete
tokens into continuous latent variable by adding
an embedding step and a rounding step to the

--- PAGE 3 ---
standard diffusion process with designing a training
objective to learn the embedding. It achieves
more complex controllable text generation through
continuous diffusion.
Conditional Text Diffusion DiffuSeq (Gong
et al., 2022) adopts partially noising processes
with only a single Transformer encoder and
trained end-to-end in a classiﬁer-free manner to
extend Diffusion-LM for sequence-to-sequence
(Seq2Seq) generation tasks. Considering the
importance of embedding space for the diffusion
process, SED (Strudel et al., 2022) uses a BERT
to generate embeddings for diffusion input tokens,
with the training objective of Diffusion-LM
and self-conditioning skill from Bit Diffusion.
Besides, classiﬁer-free guidance (Ho and Salimans,
2022) are performed to allows leveraging both
the unconditional and conditional abilities of a
model to improve its conditional generations.
CDCD (Dieleman et al., 2022) is a framework for
continuous diffusion models of categorical data
with score interpolation and time warping based
on score matching diffusion models (Song and
Ermon, 2019; Song et al., 2021c). It adopts an
encoder-decoder (ED) architecture for machine
translation. The potential of applying ED
architectures to more diffusion text generation
tasks still needs to be explored. It should be noted
that a concurrent study SeqDiffuSeq (Yuan et al.,
2022) also studies applying encoder-decoder for
text diffusion. SeqDiffuSeq extends the continuous
text diffusion model to sequence-to-sequence text
generation under the encoder-decoder architecture.
Two techniques of self-conditioned denoising
and token-level adaptive noise schedule are also
adopted in SeqDiffuSeq.
Compared with SeqDiffuSeq, we analyze the
defects of the ED architecture and further inves-
tigate the effect of different numbers of encoder
and decoder layers to text diffusion. To the
best of our knowledge, this paper makes the
ﬁrst attempt to mitigate the issue of conditional
text not perceiving target text when applying
encoder-decoder for conditional text generation
with diffusion. Additionally, DiffuSIA is proposed
to strengthen interactions between conditional text
and target text.
3 Preliminaries
Unconditional Diffusion Diffusion models in-
volve perturbing data with increasing levels ofrandom noise, then removing the noise to gen-
erate new samples. This process is known as
diffusion, and is the key element of three main
formulations of diffusion models, i.e., denoising
diffusion probabilistic models (DDPMs) (Ho et al.,
2020; Song et al., 2021a), score-based genera-
tive models (SGMs) (Song and Ermon, 2019,
2020), and stochastic differential equations (Score
SDEs) (Karras et al., 2022; Song et al., 2021b; Xie
et al., 2022). In this work, we study DDPMs.
Formally, given a data distribution x0q(x0),
the forward Markov process generates a sequence
of random variables x1;x2;:::;xTwith transition
kernelq(xtjxt 1) =N(xt;p1 txt 1;tI),
wheret2(0;1)is a hyperparameter
chosen ahead of model training as different
variance scales. The ﬁnal state xTis
almost Gaussian in distribution, so we have
q(xT)N (xT;0;I). For the reverse Markov
process, a learnable reverse transition kernel
p(xt 1jxt) =N(xt 1;(xt;t);(xt;t))
is trained to ﬁt the posterior distribution
q(xt 1jxt;x0) =N(xt 1;~t(xt;x0);~tI)
where ~t(xt;x0):=pt 1t
1 tx0+pt(1 t 1)
1 txt
and~t:=1 t 1
1 ttwith the notation t:= 1 t
andt:=Qt
s=1s. The training objective can be
simpliﬁed as:
Lsimple (x0) =TX
t=1E
q(xtjx0)jj(xt;t) ~t(xt;x0)jj2:(1)
Once the forward process is completed, the reverse
denoising process is tasked to gradually reconstruct
the original data x0via sampling from xTby
learning a diffusion model.
Continuous Diffusion on Embedding Space
Diffusion-LM (Li et al., 2022) proposes continuous
diffusion on the embedding space for text
generation. In the forward process, an embedding
step is designed to introduce a Markov transition
from discrete words wtox0that is parametrized
byq(x0jw) =N(EMB(w);0I). In the reverse
process, a trainable rounding step is added and
parametrized by p(wjx0) =Qn
i=1p(wijxi),
wherep(wijxi)is asoftmax distribution. Based
on Eq. (1), the training objective is modiﬁed as:
Le2e
simple(w) = E
q(x0:Tjw)[Lsimple(x0)+
jjEMB(w) (x1;1)jj2 logp(wjx0)]:(2)
Classiﬁer-free Guidance Extending the guid-
ance method proposed by Dhariwal and Nichol

--- PAGE 4 ---
(2021), semantic diffusion guidance (SDG) (Liu
et al., 2021) allows ﬁne-grained and continuous
control of model class, including either language or
image guidance, or both. Furthermore, a classiﬁer-
free guidance method is proposed that is more ef-
fective at controlling generation (Ho and Salimans,
2022; Ramesh et al., 2022). Let unconditional
denoising diffusion model p(x)be parameterized
through a score estimator (xt;t)and the con-
ditional model p(xjc)be parameterized through
(xt;t;c). These two models can be learned via
a single neural network. Precisely, a conditional
diffusion model p(xjc)is trained on paired data
(x;c), where the conditioning information cis
discarded periodically and randomly, so that the
model knows how to generate unconditionally as
well, i.e.(xt;t) =(xt;t;c=?).
In this paper, we focus on the sequence-to-
sequence text generation tasks which produce a
target sequence wx=fwx
1;:::;wx
ngconditioning
on the source sequence wc=fwc
1;:::;wc
mg.
Different from Ho and Salimans (2022), condi-
tional information is involved all the time and not
discarded, which has been proved effective in Gong
et al. (2022). Thus the training objective becomes:
LVLB= E
q(x0:Tjw;c)[TX
t=2jjx0 f(xt;c;t)jj2+
jjEMB(wx) f(x1;c;1)jj2 logp(wxjx0)]:
(3)
4 Approach
In this section, we ﬁrst describe the encoder-
decoder architecture for encoding the conditional
text. To augment encoding and perception of both
conditional text and target text, a spiral interaction
modiﬁcation is then proposed. Finally, we brieﬂy
introduce the technique of self-conditioning (Chen
et al., 2022) adopted in our method.
4.1 Encoder-Decoder Diffusion
This paper refers to the component that encodes the
conditional text as the encoder, and that denoises
the target text as the decoder.
Conditional Encoder (CE) To encode condi-
tional text, an embedding function is used to map
conditional tokens to hidden states, i.e., c0=
EMBc(wc). The output of a conditional encoder
layer is used as the input of the next layer. Readers
can refer to Vaswani et al. (2017) for details ofTransformer encoder. Formally, the calculation at
them-th encoder layer is denoted as:
cm+1= CE( cm); (4)
wherem2 f0;:::;Le 1gandLedenotes the
number of Transformer encoder layers. cm2
Rkcdc, wherekcdenotes the length of conditional
text anddcdenotes the dimension of conditional
text embedding vectors.
Target Decoder (TD) To map target tokens to
continuous representations, another embedding
function is adopted, i.e., x0=EMBx(wx). Then,
a Transformer decoder layer (Vaswani et al., 2017)
is used as the input of the next layer. Formally, the
calculation at the n-th decoder layer is denoted as:
xn+1= TD( xn;cLe); (5)
wheren2f0;:::;Ld 1gandLddenotes the num-
ber of Transformer decoder layers. xl2Rkxdx,
wherekxdenotes the length of conditional text and
dxdenotes the dimension of target text embedding
vectors. The representations of conditional text
from the last encoder layer cLeis fused into
the target representation to control the generation
process by cross-attention mechanism as:
Cross-Attention( xnWn
q;cLeWn
k;cLeWn
v);(6)
whereWn
q2RdxdxandWn
fk;vg2Rdcdx.
Different from the regular Transformer decoder,
the causal mask is not necessary, as the generation
process of diffusion is non-autoregressive (NAR).
It is notable that only one time of encoding of
the conditional text is required here, since cLeis
independent of timestep t, which is computation-
efﬁcient. However, the lack of information involv-
ingxtdegrades the representation capability of cLe,
compared with the full self-attention operation in
DiffuSeq. Thus, a spiral interaction architecture is
introduced next to addressee this issue.
4.2 Spiral Interaction Architecture
To augment encoding and perception of both
conditional text and target text, these two informa-
tion ﬂows are designed to be spirally intertwined.
An overview of the proposed spiral interaction
architecture for encoder-decoder text diffusion is
illustrated in Figure 2.

--- PAGE 5 ---
(c)
TD-<1>
CACE-<1>
CACE-<2>
CACE-<2>
CACE-<1>
TD-<1>
CACE-<1>
CACE-<2>
TD-<L d-1>
TD-<Ld>
TD-<1>
CACE-<L e>
TD-<Ld>
TD-<L d-1>
CACE-<L e>
TD-<Ld>
CACE-<L e>
(a) (b)
CACE-  
<Le-Ld-1>Figure 2: Illustration of spiral interaction architecture
(SIA). The sub-ﬁgures of (a), (b), (c) show the cases
ofLe=Ld,Le< L dandLe> L drespectively.
Blue represents the TaC ﬂow and yellow represents the
CaT ﬂow. Solid lines denote query, while dashed lines
denote key and value in cross-attention.
Conditional Encoder with Cross-Attention
(CACE) Cross-attention mechanism is
introduced here to let the conditional information
attend to the target information. Then, Eq. (4) is
modiﬁed as:
cm+1
t= CACE( cm
t;x0
t): (7)
Furthermore, DiffuSIA has no concern of informa-
tion leakage due to its NAR process. Correspond-
ingly, Eq. (5) is modiﬁed as:
xn+1
t= TD( xn
t;cLe
t): (8)
Splitting and Interweaving In order to further
strengthen the information interaction between
conditions and targets, a strategy of splitting and
interweaving is designed. As shown in Figure 2, the
layers of CACE and TD are split and interleaved
to form spiral interactions. The encoder layers
of CACE involve in interactions with the target
text information through cross-attention to acquire
the target-aware conditional (TaC) representations.
Thus Eq. (7) is modiﬁed as:
cm+1
t= CACE( cm
t;xn
t): (9)
Subsequently, the acquired TaC information
is utilized in the interactions with the decoder
layers of TD through cross-attention, deriving the
condition-aware target (CaT) representations. Thus
Eq. (8) is modiﬁed as:
xn+1
t= TD( xn
t;cm+1
t): (10)
We consider three cases to accommodate the in-
teractions of CACE and TD with different number
of layers as:•Le=Ld. The encoding process is accom-
plished by simply interleaving CACE with
each layer of TD in this setup.
•Le< Ld. The interleaving process operates
from layer 0toLe 1. After that, individual
diffusion decoding with Eq. (8) is conducted.
•Le>Ld. The individual conditional encod-
ing with Eq. (7) is ﬁrst conducted from layer
0toLe Ld 1. After that, the interleaving
process is conducted.
These three cases provide corresponding strategies
for models in various situations.
4.3 Self-Conditioning
In the reverse process, the denoising function
f(xt;c;t)is only conditioned on the previous up-
dated noisy samples xt, not directly on the function
prediction xt
0=f(xt+1;c;t+ 1) , discarding the
information of predictions from the previous step.
Self-conditioning (Chen et al., 2022) is proposed to
address the issue by taking xt
0into account with a
modiﬁcation denoising function as:
xt 1
0=f(xt;xt
0;c;t): (11)
Providing the model with direct access to the
predictions it produced in the previous sampling
step allows for a more efﬁcient utilization of
its capacity. In this way it can reﬁne previous
predictions, instead of constructing them from
scratch in each step. (Chen et al., 2022; Dieleman
et al., 2022; Strudel et al., 2022)
Following the setting in Chen et al. (2022), with
50% probability, we set f(xt;xt
0=0;c;t)which
falls back to modeling without self-conditioning .
No back-propagating through the ﬁrst estimated
xt
0, the increase of additional training time is
less than 25%. In practice, to approximate the
inference behavior at train time while remaining
computationally efﬁcient, the ﬁrst estimated xt
0is
calculated as xt
0=f(xt;0;c;t). Then we per-
form a second forward pass using a stop gradient
to obtain xt 1
0=f(xt;xt
0;c;t). At inference
time, we always estimate x0based on Eq. (11). To
combine the information of previous estimation,
there are two simple method can be tried. The ﬁrst
one is that we concatenate xt 1
0andxt
0through the
hidden dimension with a linear projection, while
another one is that we directly add them together.
The experiment results show that the ﬁrst one is
more powerful.

--- PAGE 6 ---
Tasks CLens TLens BS Steps/ kT/h
QQP (Paraphrase) 32 32 1400 50 12
Wiki-Auto (TS) 128 64 2048 80 35
Quasar-T (QG) 64 32 1400 50 12
CCD (DG) 64 64 2048 140 45
Table 1: Detail settings for four different tasks. CLens
means maximum length of conditional text. TLens
means maximum length of target text. BS means batch
size. Steps means learning steps. T means approximate
training time of DiffuSIA on 4x A100 GPUs.
5 Experiments
5.1 Datasets
Following Gong et al. (2022), experiments on
four different sequence-to-sequence text generation
tasks were conducted to validate the effectiveness
of the proposed DiffuSIA:
Paraphrase The Quora Question Pairs (QQP)
dataset2, extracted from the question-answering
forum Quora, is used for paraphrase evaluation,
where the positive question pairs are used to
evaluate models’ ability to generate a restatement
of a question expressing the same meaning.
Text Simpliﬁcation (TS) The Wiki-Auto
dataset (Jiang et al., 2020) is a text simpliﬁcation
dataset, consisting of 666K complex-simple
sentence pairs with revision alignment, which
is used to revise complex text with simpliﬁed
grammar and word choice.
Question Generation (QG) The Quasar-T
dataset (Dhingra et al., 2017) is used for evaluating
question generation which aims to generate related
questions with a given context. The preprocessed
data of Lin et al. (2018) is used following Gong
et al. (2022).
Open Domain Dialogue (DG) The Common-
sense Conversation Dataset (CCD) (Zhou et al.,
2018) extracted from single-round dialogue in Red-
dit, is used for evaluating open-domain dialogue,
the task of generating informative feedback based
on the dialogue context.
5.2 Baselines
The following methods were considered as base-
lines: (1) Transformer (Vaswani et al., 2017) is
an encoder-decoder architecture that performs text
generation in an autoregressive (AR) manner. (2)
GPT-2 (Radford et al., 2019) is a uni-directional
2https://www.kaggle.com/c/quora-quest
ion-pairspre-trained language model as a strong AR baseline.
(3) GPV AE (Du et al., 2022) augments a pre-
trained T5 (Raffel et al., 2020) with variational
attention (Bahuleyan et al., 2018; Deng et al., 2018;
Wang and Wan, 2019) to improve the generation
diversity. (4) LevT (Gu et al., 2019) is a partially
autoregressive model devised for more ﬂexible
and amenable sequence generation, chosen as a
conventional NAR baseline. (5) DiffuSeq (Gong
et al., 2022) uses an encoder-only Transformers
architecture and partially noising to adapt text
diffusion model to sequence-to-sequence task.
5.3 Implementation Details
For a fair comparison with DiffuSeq consisting
of a single Encoder with 12 layers, our DiffuSIA
was based on the six to six layers encoder-decoder
Transformer (Vaswani et al., 2017). The encoder
embedding dimension is set to 768, while the
decoder embedding dimension is set to 128. Each
encoder/decoder layer was under the setting of bert-
base-uncased . The diffusion timestep information
was formulated as timestep embedding which was
added to the word embedding.
The diffusion steps was set to 2000 , and the
initial noise schedule was set to sqrt. Schedule
sampler was set to lossaware as Gong et al. (2022).
The AdamW method (Loshchilov and Hutter, 2019)
was employed for optimization. The learning rate
was initialized as 1e-4and was decayed linearly
down to 0. As shown in Table 1, for different
tasks, different batch size, learning steps and
maximum utterance length were set. The strategy
of Maximum Bayes Risk (MBR) (Kumar and
Byrne, 2004) with the size of candidate samples
jSj= 10 was performed for decoding. All
experiments were run on four NVIDIA Tesla
A100 80G GPUs. Half-precision ﬂoating-point
formatFP16was applied to accelerate training
and decoding process. All code was implemented
in the PyTorch framework3.
5.4 Metrics
To evaluate the quality of the generated text, we em-
ployed the standard string-similarity-based metrics
BLEU (Papineni et al., 2002) and ROUGE (Lin,
2004). Besides, BERTScore (Zhang et al., 2020)
was also employed to help measure the semantic
similarity between the generated sentences and the
references. Higher is better for all metrics.4
3https://pytorch.org/
4The evaluation codes are provided by Gong et al. (2022).

--- PAGE 7 ---
ModelsMetrics QQP (Paraphrase) Wiki-Auto (TS) Quasar-T (QG) CCD (DG)
BLEU ROUGE LBERTS BLEU ROUGE LBERTS BLEU ROUGE LBERTS BLEU ROUGE LBERTS
Transformer (Vaswani et al., 2017) 5.80 24.89 53.92 24.45 50.58 75.90 3.64 19.94 53.34 1.89 10.39 47.81
GPT2-Large (Radford et al., 2019) 20.59 54.15 83.63 26.93 51.11 78.82 11.10 32.15 63.46 1.25 10.02 52.93
GPV AE (Du et al., 2022) 24.09 58.86 84.66 33.92 58.28 81.66 12.51 33.90 63.08 1.10 10.09 43.17
LevT (NAR) (Gu et al., 2019) 22.68 57.95 83.44 20.52 44.02 72.54 9.30 28.93 54.91 1.58 5.50 47.60
DiffuSeq (Gong et al., 2022) 24.13 58.80 83.65 36.22 58.49 81.26 17.31 36.65 61.23 1.39 10.56 51.31
DiffuSIA 24.95 59.55 83.62 37.03 59.63 81.90 17.12 35.13 62.19 1.13 9.61 50.58
Table 2: Evaluation results on four test sets in terms of automated evaluation. The results of baselines is copied
from Gong et al. (2022). Numbers in bold denoted that the best score. BERTS is the short of BERTScore.
5.5 Evaluation Results
Table 2 presents the evaluation results of DiffuSIA
and previous methods on the four test sets. Our
proposed DiffuSIA achieved competitive perfor-
mance over these baseline methods on Wiki-Auto
and QQP, outperformed conventional generation
methods (except DiffuSeq) on Quasar-T, but the
performance was not as good as those on CCD.
In particular, DiffuSIA outperformed the best
performing baseline by large margins of 0.86%
BLEU and 0.69% ROUGE L, but left behind
1.04% BERTScore on QQP. In terms of Wiki-
Auto, DiffuSIA outperformed the best performing
baseline by large margins of 0.81% BLEU, 1.14%
ROUGELand 0.24% BERTScore respectively. In
terms of Quasar-T, DiffuSIA outperformed the
best performing conventional baseline by large
margins of 4.61% BLEU and 1.23% ROUGE L,
but left behind 1.27% BERTScore. Compared
with DiffuSeq, DiffuSIA outperformed it by 0.96%
BERTScore, but left behind 0.19% BLEU and
1.52% ROUGE Lon Quasar-T. In terms of CCD,
the performance of DiffuSIA was not as good
as the baselines. Compared with the other three
tasks, DG task required deeper natural language
understanding and reasoning abilities. From these
results, it can be seen that there is still room for
further improvement.
5.6 Ablation Study
To further verify the effectiveness of the proposed
DiffuSIA, comparison with the encoder-decoder
diffusion architecture described in Sec. 4.1, namely
DiffuED, was conducted on the QQP dataset. As
demonstrated in Table 3, DiffuSIA outperformed
DiffuED by margins of 0.69% BLEU, 0.37%
ROUGEL, illustrating the effectiveness of the
interweaved TaC and CaT ﬂows. Besides, ablating
the technique of splitting and interweaving (SI)
resulted in degraded performance on all three
metrics, indicating that the spiral architectureModels BLEU ROUGE LBERTScore
DiffuSIA 24.95 59.55 83.62
w/o. SI 24.48 59.00 83.30
w/. A-Type SC 23.85 58.99 82.87
w/o. SC 23.68 58.24 82.71
DiffuED 24.26 59.18 83.92
w/. A-Type SC 24.44 59.29 83.43
w/o. SC 23.77 58.44 83.01
PreEnc S-BERT 23.19 58.33 83.36
PreEnc T-BERT 24.18 58.75 83.54
Table 3: Experiments of the modiﬁed architecture on
QQP. SI indicates Splitting and Interweaving. SC
indicates Self-Conditioning. A-Type SC indicates
self-conditioning is directly add to xtas described
in Sec. 4.3. DiffuED is the pure encoder-decoder
diffusion as described in Sec. 4.1. PreEnc indicates
using pretrained encoder. S-BERT is the short of
Sentence-BERT. T-BERT is the short of tinyBERT.
was crucial for modeling the interactions between
conditional text and target text.
On the other hand, self-conditioning (SC) was
ablated, denoted as DiffuSIA w/o. SC, to explore
its effect on models. The performance of both
models decreases after removing the SC, illus-
trating the importance of the SC. Besides, self-
conditioning was also directly added to the inputs
of decoder, denoted as DiffuSIA w/. A-Type SC,
to compare with the concatenated-type using in
DiffuSIA, denoted as C-Type SC. It can be seen
that DiffuSIA outperformed DiffuSIA w/. A-Type
SC, but no performance degradation for DiffuED
w/. A-Type SC. The results indicated that C-Type
SC is more robust than the A-Type SC.
5.7 Analysis
Impact of the number of encoder and decoder
layers. We explored how the number of encoder
and decoder layers affected the performance of
DiffuSIA. To ensure a fair comparison, the number
of encoder layers Leand the number of decoder
layersLdwere under the restraint of Le+Ld=

--- PAGE 8 ---
828384BERTScore
585960ROUGEL
2 4 6 8 10
Numbers of Decoder Layer232425BLEU
Figure 3: Impact of different numbers of decoder layers
to DiffuSIA and DiffuED on the test set of QQP dataset.
Solid lines for DiffuSIA, and dashed lines for DiffuED.
12. For DiffuED, it’s easy to set different values
for encoder and decoder. For DiffuSIA, the
architecture shown in Figure 2 was applied. As
shown in Figure 3, DiffuSIA showed different
trend from that of DiffuED. As the number of
decoder layers increased (meanwhile the number
of encoder layers decreased), the performance of
DiffuED was consistently improved on the QQP
dataset. On the other hand, the performance of
DiffuSIA was improved, as the number of decoder
layers increased at the beginning. Ld= 6achieved
the best performance. After that, the performance
of DiffuSIA dropped as the number of decoder
layers further increased. These results indicated
that the spiral interaction architecture showed best
performance under a symmetrical structure of
encoder and decoder.
Pre-trained Encoder. Experiments of exploring
different pre-trained language models for diffusion
generation process were conducted. The encoder of
DiffuED was initialized using a pre-trained 6-layer
BERT model. Speciﬁcally, DiffuED PreEnc S-Bert
was initialized using Sentence-BERT (Reimers and
Gurevych, 2019)5, while DiffuED PreEnc T-Bert
was initialized using tinyBERT (Jiao et al., 2020)6.
The results were shown in the last two rows in
Table 3. The pre-trained models had instead played
a negative role, for the gap between conditional text
encoder and diffusion process decoder. It suggested
that further improvements are needed in effectively
utilizing pre-trained language models.
5https://huggingface.co/sentence-tran
sformers/paraphrase-TinyBERT-L6-v2
6https://huggingface.co/huawei-noah/T
inyBERT_General_6L_768DQQP (Paraphrase)
Condwhat is java programming? how to learn java
programming language?
Target how do i learn a computer language like java?
DiffuSIAhow can i learn java programming language?
how should i learn java programming to begin?
Wiki-Auto (TS)
Condthe 7 july 2005 london bombings, often referred
to as 7 / 7, were a series of coordinated islamist
terrorist suicide attacks in london, england, that
targeted commuters travelling on the city’s public
transport system during the morning rush hour.
Targetthe 7 july 2005 london bombings ( also called 7 /
7 ) were suicide bomb attacks aimed at london’s
public transport system during the morning rush
hour.
DiffuSIAthe 7 july 2005 london bombings were often
referred to as 7 / 7.
the 7 july 2005 london bombings, often referred
to as 7 / 7, were a series of coordinated started.
Quasar-T (QG)
Condthe pound and the euro also took major hits
against the yen, indicating investors are losing
conﬁdence in their carry trades with the japanese
currency.
Target what is the japanese currency?
DiffuSIAwhat is the japanese currency
what is the japanese currency
CCD (DG)
Cond great article. thanks for posting
Target thanks for reading!
DiffuSIAhappy to help.
no problem, i was happy. it’s awesome.
Table 4: The text generation results for four tasks in the
test sets. Cond indicates conditional text.
Case Study. Four randomly selected samples
from each of the four datasets were shown in
Table 4. As we can see, the generated results were
well controlled by conditional texts. DiffuSIA was
able to generate different samples under different
random seed conditions, except for Quasar-T which
included the same target texts for different condi-
tions. For CCD, the expression “ no problem ” in
the second response was not suitable. More efforts
should be made for better context understanding.
6 Conclusion
In this paper, we have explored the encoder-
decoder architecture for text diffusion, which offers
greater ﬂexibility due to its detachable encoder and
decoder modules. The ﬂexibility of the model
makes it extensible to multilingual and multi-
modal generation tasks for conditions and target
texts. We proposed a spiral interaction architecture
(DiffuSIA) that leverages the target information
to improve the understanding of the conditional

--- PAGE 9 ---
text. The results of our experiments show that
DiffuSIA achieves competitive performance among
previous methods on all four tasks, demonstrating
the effectiveness and generalization ability of the
proposed method. However, there is room for
improvement in terms of dialogue generation tasks.
Limitations
While our model demonstrates good performance
on various datasets, it falls short on tasks that
demand higher natural language understanding
capabilities, such as dialogue response generation.
Improving natural language understanding will be
a focus for future research. Additionally, our model
incurs longer training times for improved perfor-
mance, whereas pretrained-ﬁnetune workﬂow often
require only 3-5 epochs of training to achieve better
results on downstream tasks. Therefore, exploring
ways to effectively utilize pre-trained language
models is also an area of research we plan to
investigate in the future.
References
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel
Tarlow, and Rianne van den Berg. 2021. Structured
denoising diffusion models in discrete state-spaces.
InAdvances in Neural Information Processing Sys-
tems 34: Annual Conference on Neural Information
Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual , pages 17981–17993.
Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and
Pascal Poupart. 2018. Variational attention for
sequence-to-sequence models. In Proceedings of
the 27th International Conference on Computational
Linguistics, COLING 2018, Santa Fe, New Mexico,
USA, August 20-26, 2018 , pages 1672–1682. Asso-
ciation for Computational Linguistics.
Ting Chen, Ruixiang Zhang, and Geoffrey E. Hinton.
2022. Analog bits: Generating discrete data using
diffusion models with self-conditioning. CoRR ,
abs/2208.04202.
Yuntian Deng, Yoon Kim, Justin T. Chiu, Demi Guo,
and Alexander M. Rush. 2018. Latent alignment and
variational attention. In Advances in Neural Infor-
mation Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018,
NeurIPS 2018, December 3-8, 2018, Montréal,
Canada , pages 9735–9747.
Prafulla Dhariwal and Alexander Quinn Nichol. 2021.
Diffusion models beat gans on image synthesis. In
Advances in Neural Information Processing Systems
34: Annual Conference on Neural Information
Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual , pages 8780–8794.Bhuwan Dhingra, Kathryn Mazaitis, and William W.
Cohen. 2017. Quasar: Datasets for question answer-
ing by search and reading. CoRR , abs/1707.03904.
Sander Dieleman, Laurent Sartran, Arman Roshan-
nai, Nikolay Savinov, Yaroslav Ganin, Pierre H.
Richemond, Arnaud Doucet, Robin Strudel, Chris
Dyer, Conor Durkan, Curtis Hawthorne, Rémi
Leblond, Will Grathwohl, and Jonas Adler. 2022.
Continuous diffusion for categorical data. CoRR ,
abs/2211.15089.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy
Bengio. 2017. Density estimation using real
NVP. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . Open-
Review.net.
Wanyu Du, Jianqiao Zhao, Liwei Wang, and Yangfeng
Ji. 2022. Diverse text generation via variational
encoder-decoder models with gaussian process
priors. CoRR , abs/2204.01227.
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,
and Lingpeng Kong. 2022. DiffuSeq: Sequence
to sequence text generation with diffusion models.
CoRR , abs/2210.08933.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. 2014.
Generative adversarial nets. In Advances in Neural
Information Processing Systems 27: Annual Con-
ference on Neural Information Processing Systems
2014, December 8-13 2014, Montreal, Quebec,
Canada , pages 2672–2680.
Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019.
Levenshtein transformer. In Advances in Neural
Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada , pages 11179–11189.
Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuan-
jing Huang, and Xipeng Qiu. 2022. Diffusionbert:
Improving generative masked language models with
diffusion models. CoRR , abs/2211.15029.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.
Denoising diffusion probabilistic models. In
Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual .
Jonathan Ho and Tim Salimans. 2022. Classiﬁer-free
diffusion guidance. CoRR , abs/2207.12598.
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini,
Patrick Forré, and Max Welling. 2021. Argmax
ﬂows and multinomial diffusion: Learning categori-
cal distributions. In Advances in Neural Information
Processing Systems 34: Annual Conference on Neu-
ral Information Processing Systems 2021, NeurIPS

--- PAGE 10 ---
2021, December 6-14, 2021, virtual , pages 12454–
12465.
Chao Jiang, Mounica Maddela, Wuwei Lan, Yang
Zhong, and Wei Xu. 2020. Neural CRF model
for sentence alignment in text simpliﬁcation. In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020 , pages 7943–7960.
Association for Computational Linguistics.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2020. Tinybert: Distilling BERT for natural
language understanding. In Findings of the
Association for Computational Linguistics: EMNLP
2020, Online Event, 16-20 November 2020 , volume
EMNLP 2020 of Findings of ACL , pages 4163–4174.
Association for Computational Linguistics.
Tero Karras, Miika Aittala, Timo Aila, and Samuli
Laine. 2022. Elucidating the design space
of diffusion-based generative models. CoRR ,
abs/2206.00364.
Shankar Kumar and William J. Byrne. 2004. Min-
imum bayes-risk decoding for statistical machine
translation. In Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT-
NAACL 2004, Boston, Massachusetts, USA, May
2-7, 2004 , pages 169–176. The Association for
Computational Linguistics.
Gihyun Kwon and Jong Chul Ye. 2022. Diffusion-
based image translation using disentangled style and
content representation. CoRR , abs/2209.15264.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020 ,
pages 7871–7880. Association for Computational
Linguistics.
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani,
Percy Liang, and Tatsunori B. Hashimoto. 2022.
Diffusion-LM improves controllable text generation.
CoRR , abs/2205.14217.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out .
Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong
Sun. 2018. Denoising distantly supervised open-
domain question answering. In Proceedings of
the 56th Annual Meeting of the Association for
Computational Linguistics, ACL 2018, Melbourne,
Australia, July 15-20, 2018, Volume 1: Long Papers ,
pages 1736–1745. Association for Computational
Linguistics.Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong
Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey
Shi, Anna Rohrbach, and Trevor Darrell. 2021.
More control for free! image synthesis with seman-
tic diffusion guidance. CoRR , abs/2112.05744.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR
2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net.
Alexander Quinn Nichol, Prafulla Dhariwal, Aditya
Ramesh, Pranav Shyam, Pamela Mishkin, Bob
McGrew, Ilya Sutskever, and Mark Chen. 2022.
GLIDE: towards photorealistic image generation
and editing with text-guided diffusion models. In
International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA, volume 162 of Proceedings of Machine
Learning Research , pages 16784–16804. PMLR.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, July 6-12, 2002,
Philadelphia, PA, USA , pages 311–318. ACL.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a uniﬁed text-to-
text transformer. J. Mach. Learn. Res. , 21:140:1–
140:67.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. 2022. Hierarchical text-
conditional image generation with CLIP latents.
CoRR , abs/2204.06125.
Machel Reid, Vincent J. Hellendoorn, and Graham
Neubig. 2022. Diffuser: Discrete diffusion via edit-
based reconstruction. CoRR , abs/2210.16886.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing .
Association for Computational Linguistics.
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion
models. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pages 10674–
10685. IEEE.

--- PAGE 11 ---
Nikolay Savinov, Junyoung Chung, Mikolaj
Binkowski, Erich Elsen, and Aäron van den Oord.
2022. Step-unrolled denoising autoencoders for text
generation. In The Tenth International Conference
on Learning Representations, ICLR 2022, Virtual
Event, April 25-29, 2022 . OpenReview.net.
Jiaming Song, Chenlin Meng, and Stefano Ermon.
2021a. Denoising diffusion implicit models. In 9th
International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7,
2021 . OpenReview.net.
Yang Song, Conor Durkan, Iain Murray, and Stefano
Ermon. 2021b. Maximum likelihood training of
score-based diffusion models. In Advances in
Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Sys-
tems 2021, NeurIPS 2021, December 6-14, 2021,
virtual , pages 1415–1428.
Yang Song and Stefano Ermon. 2019. Generative
modeling by estimating gradients of the data
distribution. In Advances in Neural Informa-
tion Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada , pages 11895–11907.
Yang Song and Stefano Ermon. 2020. Improved tech-
niques for training score-based generative models.
InAdvances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual .
Yang Song, Jascha Sohl-Dickstein, Diederik P.
Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. 2021c. Score-based generative modeling
through stochastic differential equations. In 9th
International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7,
2021 . OpenReview.net.
Robin Strudel, Corentin Tallec, Florent Altché, Yilun
Du, Yaroslav Ganin, Arthur Mensch, Will Grath-
wohl, Nikolay Savinov, Sander Dieleman, Laurent
Sifre, and Rémi Leblond. 2022. Self-conditioned
embedding diffusion for text generation. CoRR ,
abs/2211.04236.
Chao-Hong Tan, Jia-Chen Gu, Chongyang Tao, Zhen-
Hua Ling, Can Xu, Huang Hu, Xiubo Geng,
and Daxin Jiang. 2022. Tegtok: Augmenting
text generation via task-speciﬁc and open-world
knowledge. In Findings of the Association for
Computational Linguistics: ACL 2022, Dublin,
Ireland, May 22-27, 2022 , pages 1597–1609.
Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA , pages 5998–6008.
Tianming Wang and Xiaojun Wan. 2019. T-
CV AE: transformer-based conditioned variational
autoencoder for story completion. In Proceedings of
the Twenty-Eighth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2019, Macao, China,
August 10-16, 2019 , pages 5233–5239. ijcai.org.
Pan Xie, Qipeng Zhang, Zexian Li, Hao Tang, Yao Du,
and Xiaohui Hu. 2022. Vector quantized diffusion
model with codeunet for text-to-sign pose sequences
generation. CoRR , abs/2208.09141.
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang,
and Songfang Huang. 2022. SeqDiffuSeq: Text
diffusion with encoder-decoder transformers. CoRR ,
abs/2212.10325.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. BERTScore:
Evaluating text generation with BERT. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,
Jingfang Xu, and Xiaoyan Zhu. 2018. Com-
monsense knowledge aware conversation generation
with graph attention. In Proceedings of the
Twenty-Seventh International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018,
Stockholm, Sweden , pages 4623–4629. ijcai.org.
