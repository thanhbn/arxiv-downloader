# 2308.15459.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/text-diffusion/2308.15459.pdf
# File size: 509779 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ParaGuide: Guided Diffusion Paraphrasers for
Plug-and-Play Textual Style Transfer
Zachary Horvitz1, Ajay Patel2, Chris Callison-Burch2, Zhou Yu1, Kathleen McKeown1
1Columbia University
2University of Pennsylvania
zfh2000@columbia.edu, ajayp@seas.upenn.edu, ccb@seas.upenn.edu, zy2461@columbia.edu, kathy@cs.columbia.edu
Abstract
Textual style transfer is the task of transforming stylistic
properties of text while preserving meaning. Target “styles”
can be defined in numerous ways, ranging from single at-
tributes (e.g. formality) to authorship (e.g. Shakespeare). Pre-
vious unsupervised style-transfer approaches generally rely
on significant amounts of labeled data for only a fixed set
of styles or require large language models. In contrast, we
introduce a novel diffusion-based framework for general-
purpose style transfer that can be flexibly adapted to arbitrary
target styles at inference time. Our parameter-efficient ap-
proach, P ARAGUIDE , leverages paraphrase-conditioned dif-
fusion models alongside gradient-based guidance from both
off-the-shelf classifiers and strong existing style embedders
to transform the style of text while preserving semantic in-
formation. We validate the method on the Enron Email Cor-
pus, with both human and automatic evaluations, and find that
it outperforms strong baselines on formality, sentiment, and
even authorship style transfer.
1 Introduction
Diffusion models (Sohl-Dickstein et al. 2015; Ho, Jain, and
Abbeel 2020; Song, Meng, and Ermon 2022) were origi-
nally popularized for image synthesis (Nichol and Dhariwal
2021; Saharia et al. 2022). More recently, however, diffusion
has been successfully applied to text. Diffusion-based lan-
guage models are increasingly competitive with traditional
approaches for text generation (Li et al. 2022; Gulrajani
and Hashimoto 2023; Han, Kumar, and Tsvetkov 2023; Han
et al. 2023), and on text-to-text modeling tasks (Mahabadi
et al. 2023; Yuan et al. 2023).
A key benefit of diffusion language models for text is their
high degree of controllability. Diffusion-based approaches
hierarchically denoise a continuous representation of an en-
tire sequence, and this process can be effectively guided with
gradient-based methods (Li et al. 2022; Han, Kumar, and
Tsvetkov 2023; Gulrajani and Hashimoto 2023). This dif-
fers from the dominant approach of autoregressive decod-
ing, where text is generated by sequentially sampling tokens.
Steering pretrained autoregressive models has proven diffi-
cult, as their text is greedily decoded and guidance must op-
erate on partial sequences (Li et al. 2022; Dathathri et al.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: We train paraphrase-conditioned text diffusion
models to reconstruct semantically consistent text from
noised word embeddings. At inference time, we guide the
reconstruction towards target styles with off-the-shelf mod-
els.
2020; Krause et al. 2020; Yang and Klein 2021). We lever-
age the controllability of nascent text diffusion methods and
adapt them to style transfer .
In textual style transfer, the objective is to transform the
style of the text to exhibit an attribute (such as “formal-
ity”), or a target author’s style, while preserving meaning
(Jin et al. 2022; Krishna, Wieting, and Iyyer 2020; Patel,
Andrews, and Callison-Burch 2022). The scarcity of style-
transfer datasets has motivated unsupervised style transfer
approaches that perform attribute and authorship style trans-
fer without paired data. These approaches generally require
retraining for new target styles.
In contrast, we introduce a plug-and-play diffusion frame-
work for unsupervised style transfer.1We initially train a
text diffusion model to reconstruct semantically consistent
text from paraphrases, but at inference time, we perform
new attribute or authorship style transfers by guiding recon-
struction with gradients from off-the-shelf models (Figure
1). This allows users to leverage the numerous text classifiers
1Our code is publicly available at https://github.com/
zacharyhorvitz/ParaGuide.arXiv:2308.15459v3  [cs.CL]  22 Feb 2024

--- PAGE 2 ---
on platforms like Hugging Face2to specify target styles. Be-
yond guidance from classifiers, our method enables bring-
ing recent advances in representation learning to bear by
“plugging in” authorship representations like Style Embed-
dings (Wegmann, Schraagen, and Nguyen 2022) and Uni-
versal Authorship Representations (Rivera-Soto et al. 2021).
This enables our approach to perform challenging tasks like
low-resource authorship style transfer (Patel, Andrews, and
Callison-Burch 2022).
Our contributions are as follows:
1. We propose a novel framework for textual style trans-
fer based on paraphrase-conditioned diffusion models,
PARAGUIDE .
• Unlike existing style-transfer approaches, this frame-
work enables gradient-based guidance using off-the-
shelf models at inference time.
• Beyond classifier guidance, we show that existing
authorship representations can be plugged in for
control. Even with limited available data, this al-
lows P ARAGUIDE to competitively perform author-
ship style transfer.
• Style transfer requires balancing style-transfer ac-
curacy with fluency and meaning preservation. Our
framework enables explicit control over this trade-off
through varying guidance strength ( λ).
2. We validate our approach on formality and sentiment
transfer, where it outperforms strong baselines on au-
tomatic evaluations. Additionally, we perform a human
evaluation for formality transfer.
3. P ARAGUIDE represents early work exploring the promis-
ing benefits afforded by text diffusion models. To our
knowledge, we are the first to adapt these approaches to
unsupervised textual style transfer.
2 Related Work
Other unsupervised transfer approaches, like S TRAP , cre-
ate pseudo-parallel corpora by corrupting texts to remove
stylistic attributes, then training models to reconstruct the
uncorrupted text (Krishna, Wieting, and Iyyer 2020; Riley
et al. 2021; Ma et al. 2020). These approaches cannot use
new stylistic representations without retraining and do not
incorporate control from off-the-shelf models. Additionally,
STRAP has been shown to require large amounts of style-
specific training data (Patel, Andrews, and Callison-Burch
2022). Prior work has explored applying controllable text
generation techniques to style transfer (Dale et al. 2021; Ku-
mar et al. 2021; Mireshghallah, Goyal, and Berg-Kirkpatrick
2022). Our approach is most similar in spirit to Mireshghal-
lah, Goyal, and Berg-Kirkpatrick (2022). Their approach
is also learning free and non-autoregressive, but performs
a discrete search which is very computationally expensive
for long sequences, cannot leverage the rich information
in gradients, and confines the search space at each step
to token-level substitutions. Recently, the emergent ability
of Large Language Models (LLMs) to perform in-context
2https://huggingface.co/modelslearning (Brown et al. 2020) has presented formidable base-
lines for text generation and style transfer (Reif et al. 2022;
Patel, Andrews, and Callison-Burch 2022). Unlike LLMs,
PARAGUIDE allows gradient-based control and can leverage
stylistic embeddings, and is not restricted to brittle guidance
through text-based prompts. Moreover, these approaches
typically require models with billions of parameters (Rad-
ford et al. 2019).
3 ParaGuide
Overview
PARAGUIDE has three primary steps:
1. Generating an initial paraphrase of an input text with
an autoregressive (AR) model.
2. Using a paraphrase-conditioned text diffusion model to
iteratively reconstruct the input text from this paraphrase
over a number of diffusion steps.
3. At each diffusion step, computing gradients for arbitrary
differentiable losses, and using these gradients for guid-
ance towards a target style.
Here, we first use paraphrasing to generate an interme-
diate text that is semantically consistent with the input text
but without the original stylistic attributes (Krishna, Wiet-
ing, and Iyyer 2020). We then reconstruct the text with a
paraphrase-conditioned diffusion model. During reconstruc-
tion, we optimize some loss function specified by a guid-
ance model (Han, Kumar, and Tsvetkov 2023; Li et al. 2022;
Gulrajani and Hashimoto 2023). The result is a semantically
consistent output in the desired target style.
Initial Paraphrase Generation
At both training and inference time, P ARAGUIDE requires
(paraphrase, original text) pairs. To generate this synthetic
data, we leverage an existing, publicly available model
(Zhang et al. 2020), specifically fine-tuned for paraphase
generation. We include additional information describing
this procedure in our Appendix. This aspect of our ap-
proach distills performant, but less controllable, autoregres-
sive paraphrasers into controllable diffusion models.
Paraphrase-Conditioned Diffusion
In this section, we introduce the components of our
paraphrase-conditioned text diffusion model.
Diffusion Diffusion approaches (Sohl-Dickstein et al.
2015; Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon
2022), consist of two Markov chains, a forward process and
areverse process . In the forward process , the original data
x0is converted to pure Gaussian noise by incrementally
adding noise over multiple discrete time steps, {0, ..., T}.
Each of these intermediate noised latents, xt, can be directly
sampled as follows:
xt=√¯atx0+√
1−¯atϵt;ϵt∼ N(0,I), (1)
where ϵtis random noise and ¯atspecifies a well-behaved
schedule such that ¯at→0ast→T. The reverse process is
parameterized by a model, which is trained to reconstruct the

--- PAGE 3 ---
original data from pure noise ( xT) by iteratively estimating
ϵt(or equivalently x0) and working backwards in time, from
t=Ttot= 0.
In the image domain, pixels are used as the representation
ofx0. In contrast, text is discrete, and the underlying con-
tinuous domain is less obvious. Several existing text diffu-
sion approaches operate on word embeddings (Li et al. 2022;
Yuan et al. 2023; Gulrajani and Hashimoto 2023), while oth-
ers noise token logit simplexes, like SSD-LM (Han, Kumar,
and Tsvetkov 2023; Han et al. 2023; Mahabadi et al. 2023).
PARAGUIDE performs diffusion in word embedding space,
but incorporates several benefits of simplex methods.
Categorical Reparameterization While diffusion with
word logits has several desirable properties (Han, Kumar,
and Tsvetkov 2023), logits are a high dimension latent
representation (sequence length ×vocabulary size), which
makes both training and inference slower and more mem-
ory intensive than operating directly on the word embedding
space. Also, unlike the probability simplex, pretrained word
embedding spaces are well-suited for meaning-preserving
style transfer, as neighbors are often semantically similar
(Mikolov et al. 2013).3As a result, in P ARAGUIDE , we em-
ploy noised word embeddings for our latent representations,
and define our forward process as:
xt=√¯atE(w) +p
(1−¯at)ϵt, (2)
where wis our original text and Eis an embedding lookup.
Rather than directly estimate the original word embeddings
in our reverse process, however, we estimate E(w)with a
diffusion model that first outputs a posterior over discrete
tokens, like Gulrajani and Hashimoto (2023):
ˆwt∼pθ(.|xt, t,p), (3)
where xtis our noised embedding, and pis our input para-
phrase. We sample intermediate tokens from this distribu-
tion like in SSD-LM (Han, Kumar, and Tsvetkov 2023),
and these tokens are embedded for the next tn−1th diffusion
step:
xt−1=√¯at−1E(ˆwt) +p
(1−¯at−1)ϵ;ϵ∼ N(0,I)(4)
This approach still provides the controllability benefits of
SSD-LM, as gradient-based control can be applied to the
intermediate token predictions, which we will discuss in the
Guidance section.
Diffusion Model Architecture We build on the SSD-LM
architecture (Han, Kumar, and Tsvetkov 2023), which uses
a bidirectional RoBERTa encoder (Liu et al. 2019) to output
token probabilities at each diffusion step, conditioned on a
noised representation and timestep. However, we make sev-
eral changes to adapt their simplex-diffusion approach for
text-to-text tasks like paraphrasing. First, as noted in the pre-
vious section, we modify their model to operate on noised
3Additionally, we observed that the original SSD-LM approach
of adding Gaussian noise to logits before a softmax operation re-
sults in almost completely noised text early on in the forward pro-
cess, and early steps could be discarded with little effect on our
final outputs.word embeddings, rather than word logits. Additionally, as
in Mahabadi et al. (2023), we also modify the original semi-
autoregressive approach to be entirely diffusion-based. Fi-
nally, like other text-to-text diffusion approaches (Mahabadi
et al. 2023; Yuan et al. 2023), we condition on an input (in
our case, the paraphrase, p), by concatenating it with our
noised latent representation. Unlike these approaches, we
incorporate stylistic guidance, as outlined in the Guidance
section.
Diffusion Model Loss Following Han, Kumar, and
Tsvetkov (2023) and Mahabadi et al. (2023), we train the
diffusion model by minimizing the cross entropy between
the model’s posterior at each diffusion timestep and the
ground-truth tokens w, but given the timestep t, noised em-
beddings xt, and paraphrase p:
L(θ) =Et∼U(1,T)[−logpθ(w|xt, t,p)] (5)
Diffusion Noise Schedule Several approaches to diffusion
language modeling (Han, Kumar, and Tsvetkov 2023; Ma-
habadi et al. 2023; Han et al. 2023) have repurposed the co-
sine schedule (Nichol and Dhariwal 2021) from computer
vision, while others have adopted the sqrtschedule (Li et al.
2022; Yuan et al. 2023). In contrast, we train P ARAGUIDE
with a dramatically less aggressive noise schedule:
¯at=r
T−t
T(6)
This schedule falls to zero much more slowly than the co-
sine and sqrt schedules, destroying information less quickly.
The schedule is motivated by our observation that skipping
early steps with the cosine schedule had no noticeable effect
on model outputs, and experiments that showed improved
fluency and meaning preservation.4
Diffusion Model Inference At inference time, we first
generate a paraphrase, pof our input text. We then sam-
ple initial noise xT∼ N(0, I). For each step in the reverse
process ( t∈[T,1]), we then compute token logits using our
model:
lt= logitsθ(.|xt, t,p) (7)
We then sample from the model’s posterior:5
ˆwt∼top-p(softmax( lt)) (8)
After sampling ˆwt, we iteratively work backwards in time
by embedding these tokens using the word embedding
lookup, E, and then adding noise to produce xt−1, the latent
for the previous diffusion timestep, following Han, Kumar,
and Tsvetkov (2023):
xt−1=√¯at−1E(ˆwt) +p
(1−¯at−1)ϵ;ϵ∼ N(0, I)(9)
4We view less aggressive noise schedules as a possible alter-
native to self-conditioning (Strudel et al. 2022; Han et al. 2023;
Mahabadi et al. 2023), which handles information loss by also con-
ditioning diffusion language models on previous xt−1predictions.
We found that self-conditioning improved fluency but hurt control.
5This is similar to the sampling approach used by Han, Kumar,
and Tsvetkov (2023) and clamping in Li et al. (2022)

--- PAGE 4 ---
In this fashion, the model starts from random noise and an
input paraphrase, and then iteratively generates a semanti-
cally consistent output text. A critical advantage of applying
diffusion models to this task is that we can use gradient-
based guidance to steer our outputs towards specific target
styles. We discuss this in the next section.
Guidance
PARAGUIDE can incorporate guidance from any model that
is 1) differentiable and 2) uses the same tokenization scheme
as the base diffusion paraphraser:
lt=lt,init−λ∇ltLguidance (lt,init) (10)
where lt,initare the initial logit predictions at timestep t, and
Lguidance specifies a guidance loss.
Because our diffusion model employs a RoBERTa (Liu
et al. 2019) tokenization scheme, we can incorporate guid-
ance from the many available models built on the popular
RoBERTa encoder backbone. We explore two forms of guid-
ance loss for style transfer: The first is based on attribute
classifiers, and the second is based on distances in stylistic
embedding space.
Attribute Classifiers Following Han, Kumar, and
Tsvetkov (2023), we use a classifier, fϕ(·)to generate
texts with a target attribute, y, by applying drift to the full
sequence of logits, lt, at each intermediate diffusion step:
Lguidance (lt) =−log(fϕ(y|lt)) (11)
Additionally, like Han, Kumar, and Tsvetkov (2023), we
can trivially adapt classifiers to accept logits, rather than
word embeddings, by using the softmax function with some
temperature, τ, to compute a probability simplex over the
vocabulary. We can then project with the classifier’s embed-
ding lookup, Eϕ:
˜ eϕ,t= softmax(lt
τ)×Eϕ (12)
This results in a linear combination of word embeddings
at each timestep ˜ eϕ,t, based on each token’s assigned prob-
ability mass. These embeddings are passed to the attribute
model, and gradients are computed through them to increase
or decrease the probabilities of different tokens to maximize
the probability of attribute y. In contrast to SSD-LM (Han,
Kumar, and Tsvetkov 2023), P ARAGUIDE applies this drift
to a diffusion model trained to reconstruct semantically con-
sistent text, which enables meaning-preserving style trans-
fer. We can balance the trade-off between semantic consis-
tency and style transfer by varying λ.
Style Embedding Distance For authorship style transfer,
we take the novel approach of leveraging guidance from
stylistic embedding models, including Style Embeddings
(Wegmann, Schraagen, and Nguyen 2022), that are con-
strastively trained to identify authorship styles.
To guide our paraphrases with a style embedder, gϕ, we
compute the gradient of ltwith respect to its average dis-
tance in style embedding space to the target author’s ntexts,
[y1,y2...,yn]:Algorithm 1: ParaGuide Style Transfer
Input :
Input Text in Source Style w,
Guidance Loss Lguidance , Guidance Strength λ
Output : Output Text in the Target Style
1:p=paraphraser (w)
2:xT∼ N(0,1)
3:fort=T, ..., 1do
4:lt,init= logitsθ(.|xt, t,p)
5: ifλ̸= 0then
6: fori= 1, ..., k do
7: lt←lt−λ·sin(πt
T)· ∇lt,initLguidance (lt,init)
8: end for
9: end if
10: ˆwt∼top-p(softmax( lt))
11: ϵ∼ N(0, I)
12: xt−1=√¯atE(ˆwt) +p
(1−¯at)ϵ
13:end for
14:return w0
Lguidance (lt) =Pn
i=1d(gϕ(lt), gϕ(yi))
n(13)
We use cosine distance for d(·). At every diffusion step,
by minimizing the distance in style embedding space, we
steer the output text towards the target author’s style.
Guidance Schedule We observed that using the same λ
for control at all diffusion timesteps leads to disfluent solu-
tions. Intuitively, large gradient steps at the end of the re-
verse diffusion process are undesirable, as optimizing the
control objective can lead to ungrammatical text. Simulta-
neously, large steps early on in the reverse process are also
undesirable, as these initial predictions are generally inco-
herent, and out of distribution for off-the-shelf models.
As a result, for all forms of guidance, we employ a sinu-
soidal schedule for controlling drift:
λt=λ·sin(πt
T) (14)
This increases and then anneals the strength of drift during
the reverse process. Additionally, we make kgradient up-
dates per diffusion step, like Li et al. (2022). P ARAGUIDE ’s
complete inference procedure is specified in Algorithm 1.
4 Experimental Setup
Dataset
We evaluate our method on the Enron Email Corpus, which
comprises several hundred thousand emails made public
during the US government’s investigation of Enron (Klimt
and Yang 2004; Peterson, Hohensee, and Xia 2011). The
dataset contains emails from the inboxes of 150 Enron em-
ployees, sent from over one thousand accounts.
The Enron corpus presents an ideal testbed for plug-and-
play style transfer of both authorship and attributes. For the

--- PAGE 5 ---
former, email meta-data enables attributing messages to spe-
cific authors for authorship transfer. The emails also present
diverse stylistic attributes, including different degrees of for-
mality (Peterson, Hohensee, and Xia 2011) and divergent
rhetorical styles (Brown and Laudenbach 2021).
Ultimately, we need to evaluate whether our email style
transfer approach generalizes to new authors and texts.
Therefore, we randomly select 10% of addresses to be the
holdout authors for both authorship and attribute evalua-
tions. These 110authors present a low-resource authorship
corpus, as the median holdout author has only 23emails.
For our authorship experiments, we evaluate each approach
by selecting up to 5test emails per holdout source author,
and transferring these to 5other random holdout authors.
To build our training and validation datasets for attribute
style transfer, we use popular existing formality and senti-
ment classifiers to score texts from the holdout authors in
the Enron dataset. Critically, we set aside these external clas-
sifiers and avoid using them as guidance for P ARAGUIDE
at inference time. In addition to the Enron corpus, we also
build a pretraining corpus from the Reddit Million User
Dataset (MUD) (Andrews and Bishop 2019; Khan et al.
2021), which includes 4million comments by 400k different
Reddit users. We use the same paraphrasing procedure on
both the Enron and Reddit datasets to generate ( paraphrase ,
original text ) training pairs.
Implementation Details
To train our diffusion model, we fine-tuned the publicly
available SSD-LM RoBERTa-Large checkpoint6(Han, Ku-
mar, and Tsvetkov 2023) with our previously stated modifi-
cations to the architecture and noise schedule. We first fine-
tune the diffusion model on Reddit paraphase pairs, and then
continue fine-tuning on the Enron non-holdout author para-
phrase pairs. We fine-tune all parameters except the word
embedding lookup. Additional implementation details are
included in our Appendix.
Baselines
Attribute Style Transfer For attribute transfer, we com-
pare to Mix and Match (M&M) (Mireshghallah, Goyal, and
Berg-Kirkpatrick 2022) and consider both the D ISCand
HAMconfigurations from the original paper (Mireshghal-
lah, Goyal, and Berg-Kirkpatrick 2022). To better compare
with our approach, however, we replace the original BERT
model with RoBERTa-large and also include results where
we fine-tune this model on Enron Email training data.
We also implement a S TRAP baseline (Krishna, Wieting,
and Iyyer 2020) with pretrained T 5-Large models (Raffel
et al. 2020), fine-tuned on Reddit and then Enron paraphrase
pairs. In contrast to M&M and P ARAGUIDE , which are
learning-free approaches, S TRAP requires training attribute-
specific models on the Enron data classified by the external
classifiers. We fine-tune four STRAP models for informality,
formality, positive sentiment, and negative sentiment.
6https://huggingface.co/xhan77/ssdlmAuthorship Style Transfer For the task of authorship
style transfer on the Enron Email Corpus, we consider
STRAP (Krishna, Wieting, and Iyyer 2020), and the B ERT,
LING, and P ARA approaches from Patel, Andrews, and
Callison-Burch (2022). We also consider a ChatGPT-3.5
style transfer approach, where we prompt the model with
up to 16in-context examples of a target author’s style. In
contrast to our other approaches, we fine-tune 110author-
specific S TRAP models on 60% of each holdout author’s
data.
Evaluation Metrics
Attribute Style Transfer Following Mireshghallah,
Goyal, and Berg-Kirkpatrick (2022), we measure style
transfer accuracy with two classifiers. First, Internal Accu-
racy measures the style transfer accuracy of the classifier
used at inference time by Mix and Match and P ARAGUIDE .
In contrast, External Accuracy measures the style transfer
accuracy using a classifier set aside for evaluation.
We measure textual Similarity by computing Mutual Im-
plication Score (MIS) (Babakov et al. 2022) and Fluency
with a model trained on the CoLA dataset (Morris et al.
2020; Warstadt, Singh, and Bowman 2019).7For an aggre-
gate metric of model performance, we compute a Joint met-
ric by taking the sentence-wise geometric mean of External
Accuracy ,Similarity , and Fluency , similar to Krishna, Wiet-
ing, and Iyyer (2020).
For formality transfer, we additionally run a human eval-
uation of style-transfer approaches that scored highest on
our automatic evaluations. We asked annotators to compare
model outputs to the reference inputs, and score ( {0,1})
their Similarity ,Fluency , and Formality . We include addi-
tional details describing our human evaluations in the Ap-
pendix.
Authorship Style Transfer To evaluate authorship style
transfer, we adopt the Confusion metric from the evaluation
framework defined by Patel, Andrews, and Callison-Burch
(2022), where the authors utilize pretrained style embedders
(Wegmann, Schraagen, and Nguyen 2022; Rivera-Soto et al.
2021) to measure style transfer success. Confusion , which
is similar to style transfer accuracy, is the percentage of the
time that the style transfer output is closer to the target au-
thor than the source author in representational embedding
space. As with attribute transfer, we similarly compute Sim-
ilarity andFluency , and Joint , but use Confusion in place of
transfer accuracy.
We compute the above metrics for both Style Embeddings
(Wegmann, Schraagen, and Nguyen 2022) and Univer-
sal Authorship Representations (UAR) (Rivera-Soto et al.
2021). Similar to our external style classifier for attribute
transfer, UAR provides a holdout embedding space that
PARAGUIDE does not directly optimize at inference time.
7textattack/roberta-base-CoLA

--- PAGE 6 ---
Method Int. Acc ( →F,→I) Ext. Acc ( →F,→I) Sim ( →F,→I) Fluency ( →F,→I) Joint ( →F,→I)
STRAP fine-tuned 0.45 (0.8, 0.1) 0.45 (0.76, 0.13) 0.50 (0.54, 0.47) 0.73 (0.75, 0.71) 0.31 (0.54, 0.08)
M&M (Disc) 0.63 (0.59, 0.67) 0.55 (0.44, 0.65) 0.24 (0.19, 0.3) 0.62 (0.62, 0.62) 0.23 (0.19, 0.27)
M&M (Hamming) 0.58 (0.59, 0.57) 0.51 (0.46, 0.57) 0.40 (0.29, 0.52) 0.61 (0.61, 0.6) 0.26 (0.21, 0.31)
M&M enron (Disc) 0.58 (0.62, 0.55) 0.51 (0.47, 0.56) 0.31 (0.26, 0.37) 0.61 (0.61, 0.61) 0.24 (0.22, 0.26)
M&M enron (Hamming) 0.51 (0.56, 0.46) 0.47 (0.45, 0.48) 0.45 (0.35, 0.55) 0.62 (0.62, 0.62) 0.25 (0.23, 0.28)
PGuide ( λ= 1e4 ) 0.97 (0.96, 0.99) 0.83 (0.68, 0.99) 0.40 (0.37, 0.44) 0.55 (0.59, 0.51) 0.45 (0.37, 0.53)
PGuide ( λ= 5e3 ) 0.97 (0.96, 0.98) 0.82 (0.65, 0.99) 0.40 (0.36, 0.45) 0.56 (0.59, 0.52) 0.45 (0.37, 0.53)
PGuide ( λ= 1e3 ) 0.95 (0.93, 0.98) 0.81 (0.64, 0.98) 0.45 (0.4, 0.49) 0.60 (0.62, 0.57) 0.47 (0.37, 0.56)
PGuide ( λ= 5e2 ) 0.94 (0.9, 0.97) 0.81 (0.63, 0.98) 0.47 (0.44, 0.5) 0.61 (0.64, 0.58) 0.48 (0.39, 0.58)
PGuide ( λ= 2e2 ) 0.91 (0.85, 0.98) 0.76 (0.58, 0.95) 0.52 (0.5, 0.53) 0.63 (0.65, 0.61) 0.48 (0.38, 0.59)
Table 1: Automatic Formality Evaluations. We report accuracy for both the Internal andExternal classifiers. The best results
are bolded. We also decompose results into formality ( →F) and informality ( →I) transfer.
Method Accuracy ( →F,→I) Sim ( →F,→I) Fluency ( →F,→I) Joint ( →F,→I)
STRAP fine-tuned 0.51 (0.10, 0.91) 0.35 (0.32, 0.37) 0.03 (0.04, 0.01) 0.00 (0.00, 0.00)
M&M (Hamming) 0.47 (0.14, 0.80) 0.49 (0.31, 0.67) 0.46 (0.27, 0.64) 0.20 (0.03, 0.36)
PGuide ( λ= 2e2 ) 0.65 (0.39, 0.90) 0.58 (0.54, 0.61) 0.69 (0.61, 0.77) 0.33 (0.23, 0.43)
Table 2: Human Formality Evaluations. We asked annotators to rate outputs from models with the highest automatic scores as
formal or informal ( Accuracy ), whether their meaning was similar to the original ( Similarity ), and whether the outputs were
well-formed/grammatical ( Fluency ).Joint aggregates these scores together at the sentence-level.
5 Results
Attribute Style Transfer
In this section, we review our evaluation results for attribute
transfer. We include representative outputs in the Appendix.
Automatic Evaluations Tables 1 and 3 present our auto-
matic evaluation results for formality and sentiment trans-
fer. For each approach, we display the average score for
each metric, along with the breakdown for formal/informal
(→F,→I)and positive/negative (→P,→N).
PARAGUIDE outperforms all other approaches on all ag-
gregate Joint metrics, across both sentiment and formality
experiments. Additionally, P ARAGUIDE significantly sur-
passes all baselines on transfer accuracy. Despite the inher-
ent trade-off between transfer accuracy and meaning preser-
vation, on formality, P ARAGUIDE (λ= 2e2 ) outperforms
all baseline approaches on both transfer accuracy andmean-
ing preservation. On sentiment transfer, P ARAGUIDE ’s in-
creased accuracy incurs a larger cost to semantic similarity,
but this is expected in successful sentiment transfer, which
involves changing the polarity of texts (Jin et al. 2022).
Human Evaluation Table 2 displays the results of our hu-
man formality evaluation, where annotators rated the For-
mality ,Similarity , and Fluency of model outputs. When
evaluated by humans, P ARAGUIDE significantly outper-
forms the top performing baselines across all aggregate met-
rics (p= 0.05). Notably, this is even true for the Fluency
metric, where annotators rated whether outputs were rea-
sonable, coherent emails. This result was unexpected given
PARAGUIDE ’s comparatively unimpressive automatic Flu-
ency scores, but could be explained by differences betweenemail writing practices and the composition of the CoLA
training corpus (Warstadt, Singh, and Bowman 2019). In
contrast, the S TRAP baseline dramatically underperforms
on our human evaluation. Manually inspecting outputs, we
found that the S TRAP models we fine-tuned for attribute
transfer generate highly repetitive text. We suspect that this
results from fine-tuning on our limited dataset, and aligns
with previous work, which has shown that S TRAP ’s perfor-
mance is heavily reliant on dataset size (Patel, Andrews, and
Callison-Burch 2022).
Authorship Style Transfer
Table 4 presents our results on the challenging task
of low-resource authorship style transfer. When evalu-
ated with the Style embedding space, three of the four
PARAGUIDE configurations outperform every single base-
line (including ChatGPT-3.5) on Joint andConfusion . When
we consider the holdout UAR embedding space, however,
ChatGPT-3.5, which notably uses 400x more parameters
than P ARAGUIDE , outperforms the other approaches. Con-
sidering only non-LLM methods, P ARAGUIDE outperforms
all baselines on UAR Confusion , but is very narrowly out-
performed by S TRAP onUAR Joint . This can be attributed,
however, to S TRAP ’s higher Fluency score, which was a
metric that was not predictive of human ratings on the for-
mality task. Additionally, in contrast to P ARAGUIDE ’s plug-
and-play approach, the S TRAP implementation involves 110
separate models, each with 800 million parameters, fine-
tuned for every author.

--- PAGE 7 ---
Method Int. Acc ( →P,→N) Ext. Acc ( →P,→N) Sim ( →P,→N) Fluency ( →P,→N) Joint ( →P,→N)
STRAP fine-tuned 0.11 (0.16, 0.05) 0.29 (0.38, 0.19) 0.5 (0.5, 0.49) 0.74 (0.72, 0.76) 0.18 (0.24, 0.12)
M&M (Disc) 0.2 (0.01, 0.38) 0.5 (0.32, 0.67) 0.34 (0.46, 0.22) 0.63 (0.62, 0.64) 0.21 (0.17, 0.25)
M&M (Ham) 0.14 (0.02, 0.26) 0.39 (0.23, 0.55) 0.45 (0.58, 0.32) 0.62 (0.6, 0.63) 0.19 (0.14, 0.24)
M&M enron (Disc) 0.1 (0.02, 0.18) 0.38 (0.29, 0.47) 0.4 (0.48, 0.33) 0.62 (0.6, 0.64) 0.19 (0.16, 0.22)
M&M enron (Ham) 0.08 (0.02, 0.13) 0.31 (0.21, 0.41) 0.52 (0.6, 0.44) 0.62 (0.61, 0.64) 0.16 (0.13, 0.2)
PGuide ( λ= 1e4 ) 0.73 (0.78, 0.68) 0.8(0.86, 0.74) 0.13 (0.2, 0.06) 0.43 (0.43, 0.43) 0.2 (0.27, 0.13)
PGuide ( λ= 5e3 ) 0.7 (0.76, 0.65) 0.79 (0.87, 0.71) 0.15 (0.22, 0.07) 0.43 (0.45, 0.41) 0.22 (0.3, 0.14)
PGuide ( λ= 1e3 ) 0.65 (0.75, 0.54) 0.75 (0.81, 0.69) 0.25 (0.32, 0.18) 0.48 (0.53, 0.43) 0.28 (0.35, 0.21)
PGuide ( λ= 5e2 ) 0.57 (0.71, 0.43) 0.68 (0.74, 0.62) 0.33 (0.37, 0.28) 0.51 (0.55, 0.47) 0.29 (0.35, 0.23)
PGuide ( λ= 2e2 ) 0.35 (0.47, 0.22) 0.56 (0.61, 0.51) 0.42 (0.44, 0.4) 0.59 (0.64, 0.55) 0.29 (0.33, 0.25)
Table 3: Automatic Sentiment Evaluations. Like for the formality results, we break down scores into positive ( →P) and
negative ( →N) transfer, and report scores for both the Internal andExternal classifiers.
Style UAR
Method Conf. Joint Conf. Joint Sim Fluency
PARA 0.42 0.335 0.26 0.202 0.64 0.85
BERT 0.31 0.076 0.30 0.061 0.13 0.35
LING 0.44 0.334 0.23 0.177 0.82 0.58
STRAP fine-tuned 0.47 0.344 0.32 0.218 0.54 0.83
ChatGPT-3.5 0.54 0.338 0.48 0.280 0.56 0.79
PGuide ( λ= 2.5e3)0.74 0.431 0.36 0.209 0.42 0.64
PGuide ( λ= 1.5e3) 0.68 0.434 0.33 0.207 0.47 0.70
PGuide ( λ= 8e2 ) 0.64 0.426 0.33 0.217 0.50 0.74
PGuide ( λ= 2e2 ) 0.50 0.353 0.29 0.204 0.52 0.78
Table 4: Evaluation metrics for authorship style transfer. We
evaluate using two authorship representations: Style (Weg-
mann, Schraagen, and Nguyen 2022) and UAR (Rivera-Soto
et al. 2021). For each metric, we bold the strongest approach,
and underline the most performant non-LLM method.
Style Transfer vs. Similarity and Fluency
Beyond showcasing P ARAGUIDE ’s strong performance, our
automatic evaluations in Tables 1, 3, and 4 demonstrate
control over the trade-off between transfer accuracy ver-
sus semantic consistency and fluency, via the λhyperpa-
rameter. We additionally visualize the affect of varying λ
on authorship style transfer in Figure 2. When λis small,
the paraphrase-conditioned diffusion model reconstructs a
more semantically faithful, fluent output. However, we can
increase λto improve Confusion scores, at the cost of
semantic consistency and fluency. At the lowest setting,
PARAGUIDE ’sFluency andSimilarity score are similar to
those of ChatGPT-3.5 ( 0.78vs0.79and0.52vs0.56).
6 Conclusion and Future Work
We introduce P ARAGUIDE , a diffusion-based framework for
unsupervised textual style transfer. The approach harnesses
the controllability of text diffusion, alongside the availability
of off-the-shelf text classifiers and stylistic embedders, to
competitively perform both authorship and attribute transfer,
Figure 2: As we increase the guidance hyperparameter λ, we
steadily increase style transfer accuracy ( Confusion ), at the
cost of semantic consistency ( Sim) and Fluency .
without ever having to retrain style-specific pipelines.
Our work demonstrates the potential of diffusion for text
generation, a landscape currently dominated by large, auto-
regressive language models. We are particularly excited
about pursuing work that explores scaling diffusion models,
better adapting them to the text domain, and the ways that
these non-autoregressive methods can work alongside and
complement current state-of-the-art approaches.
Ethical Statement
PARAGUIDE presents an effective diffusion-based frame-
work for style-transfer that uses fewer parameters than other
state-of-the-art methods, can be fine-tuned on a single GPU,
and avoids having to retrain models for new target styles.

--- PAGE 8 ---
As a result, the approach could broaden the accessibility of
controllable text generation and empower individuals with
fewer resources to better personalize systems to their needs.
At the same time, we recognize that text generation ap-
proaches like ours have the potential to be leveraged by ma-
licious actors for impersonation and persuasion.
Acknowledgements
We would like to thank Xiaochuang Han, Raghav Singhal,
Amith Ananthram, Debasmita Bhattacharya, Nicholas Deas,
Maximillian Chen, and Smaranda Muresan for their in-
valuable discussions and thoughtful feedback, which helped
shape the direction of this work. Additionally, we would like
to extend our gratitude to Samir Gadre, Fei-Tzin Lee, and
Matthew Toles for their support on human evaluations, and
our anonymous AAAI reviewers for their comments.
This research is supported in part by the Office of the
Director of National Intelligence (ODNI), Intelligence Ad-
vanced Research Projects Activity (IARPA), via the HIA-
TUS Program contract #2022-22072200005. The views and
conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of ODNI, IARPA,
or the U.S. Government. The U.S. Government is authorized
to reproduce and distribute reprints for governmental pur-
poses notwithstanding any copyright annotation therein.
A Additional Model Details
Paraphrase Generation
The P ARAGUIDE , STRAP , and P ARA approaches require
paraphrases of input text. To generate these paraphrases, we
use a publicly available Pegasus-based (Zhang et al. 2020)
model fine-tuned for paraphrase generation.8Given the im-
portance of diverse paraphrases (Krishna, Wieting, and Iyyer
2020), we perform nucleus sampling (Holtzman et al. 2020)
withτ= 1.5andp= 0.80. We use the same sampling pro-
cedure to generate paraphrases at both training and inference
time.
ParaGuide
Architecture We started with the publicly available SSD-
LM RoBERTa-large checkpoint,9and made several modifi-
cations. First, we removed the embedding sum layer, so that
the model accepts noised word embedding latent represen-
tations. We then modify the architecture to take two fixed-
sized inputs: the embedded paraphrase and the noised em-
bedding representation. Both of these inputs are padded to
50tokens.
Training We train our paraphrase-conditioned model for
500K steps with our revised noise schedule on our synthetic
dataset of Reddit paraphrases, with T= 5000 before fine-
tuning on the Enron Corpus, with T= 200 . We use a single
NVIDIA A100 with a batch size of 128and learning rate of
5e−6, and selected the checkpoint with the lowest validation
loss on non-holdout author data.
8https://huggingface.co/tuner007/pegasus paraphrase
9https://huggingface.co/xhan77/ssdlmInference At inference time, we perform nucleus sam-
pling (Holtzman et al. 2020) at each diffusion timestep with
p= 0.80. We use k= 3 optimization steps with our guid-
ance models, with temperature τ= 3. We experimented
with a range of λvalues on our validation set, and selected
the final values to showcase different balances of transfer
accuracy vs semantic consistency/fluency.
After observing that both P ARAGUIDE and our M&M
baselines could occasionally return empty strings, we mod-
ified the inference procedure for both approaches to run a
secondary inference if this occurred. Given that diffusion
models have been shown to generate diverse text (Han, Ku-
mar, and Tsvetkov 2023; Yuan et al. 2023), we expect a
stronger inference procedure for our model would be to sam-
ple multiple outputs, and then select the best based on guid-
ance models, but we leave this to future work.
Guidance Models We use an existing formality classi-
fier10as our internal formality guidance model. For senti-
ment transfer, we employ an existing model fine-tuned on
Twitter data (Barbieri et al. 2020).11For authorship transfer,
we compute stylistic distance with Style Embeddings (Weg-
mann, Schraagen, and Nguyen 2022).
Baselines
For our S TRAP baseline, we fine-tune a T5-large model
(Raffel et al. 2020) on the Reddit paraphrase dataset for
250K steps with a batch size of 32, gradient accumulation
of4, and learning rate of 1e−5. We continue training on the
non-holdout author Enron paraphrases with a learning rate
of1e−5and selected the checkpoint with the lowest valida-
tion loss. For attribute transfer, we fine-tune four target-style
specific models (formal, informal, positive sentiment, nega-
tive sentiment) on the holdout author data labeled as such by
the external classifier. For authorship transfer, we fine-tune
this base model for each of the 110holdout authors on 60%
of their data. For all S TRAP experiments, we fine-tune with a
real batch size of 64and learning rate of 1e−4. At inference
time, we perform greedy decoding.
For our M&M baselines, we use a pretrained RoBERTa-
large (Liu et al. 2019) model as the masked language
model, and the hyperparameter configurations specified in
Mireshghallah, Goyal, and Berg-Kirkpatrick (2022), for sen-
timent and formality transfer. For all experiments, we set the
number of samples per input to be 3. For our enron config-
uration, we fine-tuned on the non-holdout author data for
approximately 50k steps with a learning rate of 1e−5and
batch size of 128. M&M inference speeds were approxi-
mately 10x slower than P ARAGUIDE ’s, which limited our
ability to run extensive hyperparameter tuning.
We directly adapted the BERT and L INGbaselines from
Patel, Andrews, and Callison-Burch (2022). For P ARA, we
use the initial paraphrases generated by the autoregres-
sive paraphraser. For our ChatGPT baseline, we prompt
ChatGPT-3.5 (‘gpt-3.5-turbo’) with up to 16examples in a
10https://huggingface.co/cointegrated/roberta-base-formality
11https://huggingface.co/cardiffnlp/twitter-roberta-base-
sentiment

--- PAGE 9 ---
target author’s style, followed by “Can you rewrite the fol-
lowing email to make it look like the above author’s style:”
and the source email. We use the default decoding parame-
ters, with a temperature and pof1.0.
B Additional Dataset Details
Preprocessing (Enron)
We preprocessed the Enron Emails Corpus (Klimt and Yang
2004) by:
• Removing duplicates.
• Filtering out email threads.
• Dropping emails longer than 50RoBERTa tokens (Liu
et al. 2019).
• Dropping all email addresses with fewer than 10mes-
sages.
This results in a cleaned dataset of 89917 emails, sent from
1100 addresses.
We randomly select 110 authors (10%) for a holdout
set. We manually reviewed these assignments to ensure that
emails by the same apparent sender but from different ad-
dresses (i.e, personal versus business) are assigned to the
same shard. We split the non-holdout author data into train/-
validation/test ( 0.8,0.1,0.1). We use the non-holdout author
training data to train P ARAGUIDE and our baselines.
Preprocessing (Reddit)
We select up to 10comments from 400K users in the
MUD Reddit corpus (Andrews and Bishop 2019; Khan et al.
2021). For each comment, we sample 1sentence 70% of the
time, 2consecutive sentences 20% of the time, and 3sen-
tences 10% of the time. We skip extractions that are longer
than 200 characters, and additionally filter texts that are
longer than 50RoBERTa tokens. This resulted in approxi-
mately 390K samples. We split the Reddit users into train/-
validation/test groups ( 0.9,0.05,0,05).
Attribute Transfer Data
For attribute style transfer, we use our external classifiers for
formality and sentiment to classify each email in the holo-
dut author set. For formality, we use a classifier12trained on
the XFORMAL corpus (Briakou et al. 2021). For sentiment
classification, we use an existing model13fine-tuned on 15
diverse datasets (Hartmann et al. 2023).
This results in:
•9317 formal emails.
•3249 informal emails.
•8473 positive sentiment emails.
•4000 negative sentiment emails.
We then split this data into train/validation/test shards
(0.7,0.15,0.15). We perform our final evaluation using 500
test source emails for each label, with the exception of infor-
mality, which only has 486test samples.
12https://huggingface.co/s-nlp/xlmr formality classifier
13https://huggingface.co/siebert/sentiment-roberta-large-
englishAuthorship Transfer Data
For authorship style transfer, we divided the holdout
emails for each author into train/validation/test shards
(0.60,0.20,0.20). To generate our results, we select up to
5test-set emails per source author, and then transfer each
of these to 5other randomly selected holdout authors. This
results in 1915 transfer pairs.
C Human Evaluation
For our human evaluation of formality transfer, we selected
the approaches with the highest automatic Joint scores. We
then randomly selected 100formal and 100informal sample
texts from our Enron test set, along with the corresponding
output from each model.
In light of recent concerns about crowdworkers relying
on large language models (Veselovsky, Ribeiro, and West
2023), we selected our annotators from within our depart-
ment, choosing members who were not directly involved
with the project. All annotators are native English speakers.
We provided these annotators with the original reference
inputs and corresponding candidate outputs, and asked them
to rate ( {0, 1}) the Similarity ,Fluency /Well-formedness,
andFormality of these candidates. Their instructions were
as follows:
Each of you has been assigned a series of very short
emails to review. Each example consists of a reference and
output text. You are asked to evaluate the output text across
three criteria:
1.Similarity to the reference. Do the output and reference
have a similar meaning? (0=No, 1=Yes)
2.Well-formedness. Does the output look like a reason-
able email? Is it coherent? (0=Badly-Formed, 1=Well-
formed)
3.Formality. Does the output text sound formal or infor-
mal? (0=Informal, 1=Formal)
We collected three annotations per example and use the
majority vote to determine labels. We then evaluated Accu-
racy by comparing the target style to the Formality label.
Additionally, we compute Joint for each sample by combin-
ing the other metrics ( Accuracy ×Similarity ×Fluency ).
We applied Krippendorff’s αto evaluate inter-annotator
aggrement for each label:
Human Label Krippendorff
Similarity 0.48
Fluency 0.63
Formality 0.13
Additionally, we used z-tests to determine the signifi-
cance of P ARAGUIDE (λ= 2e2 )’s improvement relative to
the next best approach, M&M (Hamming).
Metric z p
Joint 3.10 0.00098
Accuracy 3.57 0.00018
Similarity 1.71 0.044
Well-formedness 4.88 5.36e-07
All results are significant with p= 0.05.

--- PAGE 10 ---
D Reference Outputs
Tables 5, 6, 7, and 8 contain representative examples of
model output for each attribute transfer task. Tables 9 and
10 present authorship style transfer examples. All results are
not cherry-picked. While the Enron dataset is publicly avail-
able, we do, however, exclude examples that include signifi-
cant amounts of personal information. Additionally, we trun-
cate long texts.
.
E Resources
The following sections detail the resources used in our ex-
periments.
Models
• SSD-LM (Han, Kumar, and Tsvetkov 2023)
• T5 (t5-Large) (Raffel et al. 2020)
• ChatGPT (gpt3.5-turbo)
• Style Embeddings (Wegmann, Schraagen, and Nguyen
2022)
• Universal Authorship Representations (Rivera-Soto et al.
2021)
• RoBERTa (roberta-large) (Liu et al. 2019)
• BERT (Devlin et al. 2019)
• Mutual Implication Score (Babakov et al. 2022)
• textattack/roberta-base-CoLA (Morris et al. 2020;
Warstadt, Singh, and Bowman 2019)14
• cardiffnlp/twitter-roberta-base-sentiment (Barbieri et al.
2020)15
• siebert/sentiment-roberta-large-english (Hartmann et al.
2023)16
• cointegrated/roberta-base-formality17
• snlp/xlmr formality classifier (Briakou et al. 2021)18
• tuner 007/pegasus paraphrase (Zhang et al. 2020)19
Datasets
• Enron Email Corpus (Klimt and Yang 2004)
• Million User Dataset (Andrews and Bishop 2019; Khan
et al. 2021)
• WordNet (Miller 1994)
14https://huggingface.co/textattack/roberta-base-CoLA
15https://huggingface.co/cardiffnlp/twitter-roberta-base-
sentiment
16https://huggingface.co/siebert/sentiment-roberta-large-
english
17https://huggingface.co/cointegrated/roberta-base-formality
18https://huggingface.co/s-nlp/xlmr formality classifier
19https://huggingface.co/tuner007/pegasus paraphraseSoftware
• Transformers (Wolf et al. 2020)
• Datasets (Lhoest et al. 2021)
• Accelerate (Gugger et al. 2022)
• Sentence-transformers (Reimers and Gurevych 2019)
• spaCy (en core web trf-3.3.0) (Montani et al. 2023)
• NLTK (Bird, Klein, and Loper 2009)
• pycontractions20
• lemminflect21
• Weights and Biases (Biewald 2020)
• pySBD (Sadvilkar and Neumann 2020)
• statsmodels (Seabold and Perktold 2010)
• NumPy (Harris et al. 2020)
• Pandas (pandas development team 2020)
• PyTorch (Paszke et al. 2019)
Compute
We estimate the total compute budget for our experiments
(model training and inference) as follows:
• 1x NVIDIA A100 Tensor Core GPU / 100GB RAM / 4x
CPU – 660 hours
References
Andrews, N.; and Bishop, M. 2019. Learning Invariant Rep-
resentations of Social Media Users. arXiv:1910.04979.
Babakov, N.; Dale, D.; Logacheva, V .; and Panchenko, A.
2022. A large-scale computational study of content preser-
vation measures for text style transfer and paraphrase gener-
ation. In Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics: Student Research
Workshop , 300–321. Dublin, Ireland: Association for Com-
putational Linguistics.
Barbieri, F.; Camacho-Collados, J.; Espinosa Anke, L.; and
Neves, L. 2020. TweetEval: Unified Benchmark and Com-
parative Evaluation for Tweet Classification. In Findings
of the Association for Computational Linguistics: EMNLP
2020 , 1644–1650. Online: Association for Computational
Linguistics.
Biewald, L. 2020. Experiment Tracking with Weights and
Biases. Software available from wandb.com.
Bird, S.; Klein, E.; and Loper, E. 2009. Natural Language
Processing with Python . O’Reilly Media, Inc., 1st edition.
ISBN 0596516495.
Briakou, E.; Lu, D.; Zhang, K.; and Tetreault, J. 2021.
XFORMAL: A Benchmark for Multilingual Formality Style
Transfer. arXiv:2104.04108.
Brown, D.; and Laudenbach, M. 2021. Stylistic variation in
email. Register Studies , 4.
20https://pypi.org/project/pycontractions
21https://pypi.org/project/lemminflect/

--- PAGE 11 ---
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Models
are Few-Shot Learners. arXiv:2005.14165.
Dale, D.; V oronov, A.; Dementieva, D.; Logacheva, V .; Ko-
zlova, O.; Semenov, N.; and Panchenko, A. 2021. Text
Detoxification using Large Pre-trained Neural Models.
arXiv:2109.08914.
Dathathri, S.; Madotto, A.; Lan, J.; Hung, J.; Frank, E.;
Molino, P.; Yosinski, J.; and Liu, R. 2020. Plug and Play
Language Models: A Simple Approach to Controlled Text
Generation. arXiv:1912.02164.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. arXiv:1810.04805.
Gugger, S.; Debut, L.; Wolf, T.; Schmid, P.; Mueller, Z.;
Mangrulkar, S.; Sun, M.; and Bossan, B. 2022. Accelerate:
Training and inference at scale made simple, efficient and
adaptable. https://github.com/huggingface/accelerate.
Gulrajani, I.; and Hashimoto, T. B. 2023. Likelihood-Based
Diffusion Language Models. arXiv:2305.18619.
Han, X.; Kumar, S.; and Tsvetkov, Y . 2023. SSD-
LM: Semi-autoregressive Simplex-based Diffusion Lan-
guage Model for Text Generation and Modular Control.
arXiv:2210.17432.
Han, X.; Kumar, S.; Tsvetkov, Y .; and Ghazvininejad, M.
2023. SSD-2: Scaling and Inference-time Fusion of Diffu-
sion Language Models. arXiv:2305.14771.
Harris, C. R.; Millman, K. J.; van der Walt, S. J.; Gommers,
R.; Virtanen, P.; Cournapeau, D.; Wieser, E.; Taylor, J.; Berg,
S.; Smith, N. J.; Kern, R.; Picus, M.; Hoyer, S.; van Kerk-
wijk, M. H.; Brett, M.; Haldane, A.; del R ´ıo, J. F.; Wiebe,
M.; Peterson, P.; G ´erard-Marchant, P.; Sheppard, K.; Reddy,
T.; Weckesser, W.; Abbasi, H.; Gohlke, C.; and Oliphant,
T. E. 2020. Array programming with NumPy. Nature ,
585(7825): 357–362.
Hartmann, J.; Heitmann, M.; Siebert, C.; and Schamp, C.
2023. More than a Feeling: Accuracy and Application of
Sentiment Analysis. International Journal of Research in
Marketing , 40(1): 75–87.
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion
Probabilistic Models. arXiv:2006.11239.
Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .
2020. The Curious Case of Neural Text Degeneration.
arXiv:1904.09751.
Jin, D.; Jin, Z.; Hu, Z.; Vechtomova, O.; and Mihalcea, R.
2022. Deep Learning for Text Style Transfer: A Survey.
Computational Linguistics , 48(1): 155–205.
Khan, A.; Fleming, E.; Schofield, N.; Bishop, M.; and An-
drews, N. 2021. A Deep Metric Learning Approach to Ac-
count Linking. arXiv:2105.07263.Klimt, B.; and Yang, Y . 2004. The Enron Corpus: A New
Dataset for Email Classification Research. In European
Conference on Machine Learning .
Krause, B.; Gotmare, A. D.; McCann, B.; Keskar, N. S.;
Joty, S.; Socher, R.; and Rajani, N. F. 2020. GeDi:
Generative Discriminator Guided Sequence Generation.
arXiv:2009.06367.
Krishna, K.; Wieting, J.; and Iyyer, M. 2020. Reformulat-
ing Unsupervised Style Transfer as Paraphrase Generation.
arXiv:2010.05700.
Kumar, S.; Malmi, E.; Severyn, A.; and Tsvetkov, Y . 2021.
Controlled Text Generation as Continuous Optimization
with Multiple Constraints. arXiv:2108.01850.
Lhoest, Q.; del Moral, A. V .; Jernite, Y .; Thakur, A.; von
Platen, P.; Patil, S.; Chaumond, J.; Drame, M.; Plu, J.;
Tunstall, L.; Davison, J.; ˇSaˇsko, M.; Chhablani, G.; Ma-
lik, B.; Brandeis, S.; Scao, T. L.; Sanh, V .; Xu, C.; Pa-
try, N.; McMillan-Major, A.; Schmid, P.; Gugger, S.; De-
langue, C.; Matussi `ere, T.; Debut, L.; Bekman, S.; Cistac, P.;
Goehringer, T.; Mustar, V .; Lagunas, F.; Rush, A.; and Wolf,
T. 2021. Datasets: A Community Library for Natural Lan-
guage Processing. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing: Sys-
tem Demonstrations , 175–184. Online and Punta Cana, Do-
minican Republic: Association for Computational Linguis-
tics.
Li, X. L.; Thickstun, J.; Gulrajani, I.; Liang, P.; and
Hashimoto, T. B. 2022. Diffusion-LM Improves Control-
lable Text Generation. arXiv:2205.14217.
Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .
2019. RoBERTa: A Robustly Optimized BERT Pretraining
Approach. arXiv:1907.11692.
Ma, X.; Sap, M.; Rashkin, H.; and Choi, Y . 2020. Pow-
erTransformer: Unsupervised Controllable Revision for Bi-
ased Language Correction. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language
Processing (EMNLP) , 7426–7441. Online: Association for
Computational Linguistics.
Mahabadi, R. K.; Tae, J.; Ivison, H.; Henderson, J.; Beltagy,
I.; Peters, M. E.; and Cohan, A. 2023. TESS: Text-to-Text
Self-Conditioned Simplex Diffusion. arXiv:2305.08379.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-
ficient Estimation of Word Representations in Vector Space.
arXiv:1301.3781.
Miller, G. A. 1994. WordNet: A Lexical Database for En-
glish. In Human Language Technology: Proceedings of
a Workshop held at Plainsboro, New Jersey, March 8-11,
1994 .
Mireshghallah, F.; Goyal, K.; and Berg-Kirkpatrick, T. 2022.
Mix and Match: Learning-free Controllable Text Generation
using Energy Language Models. arXiv:2203.13299.
Montani, I.; Honnibal, M.; Honnibal, M.; Boyd, A.; Lan-
deghem, S. V .; Peters, H.; McCann, P. O.; jim geovedi;
O’Regan, J.; Samsonov, M.; de Kok, D.; Orosz, G.;
Bl¨attermann, M.; Altinok, D.; Kannan, M.; Mitsch, R.;

--- PAGE 12 ---
Kristiansen, S. L.; Edward; Miranda, L.; Baumgartner, P.;
Bournhonesque, R.; Hudson, R.; Bot, E.; Roman; Fiedler,
L.; Daniels, R.; kadarakos; Phatthiyaphaibun, W.; and
Schero1994. 2023. explosion/spaCy: v3.6.1: Support for Py-
dantic v2, find-function CLI and more.
Morris, J.; Lifland, E.; Yoo, J. Y .; Grigsby, J.; Jin, D.; and Qi,
Y . 2020. TextAttack: A Framework for Adversarial Attacks,
Data Augmentation, and Adversarial Training in NLP. In
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations ,
119–126.
Nichol, A. Q.; and Dhariwal, P. 2021. Improved Denoising
Diffusion Probabilistic Models. In Meila, M.; and Zhang, T.,
eds., Proceedings of the 38th International Conference on
Machine Learning , volume 139 of Proceedings of Machine
Learning Research , 8162–8171. PMLR.
pandas development team, T. 2020. pandas-dev/pandas:
Pandas.
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;
Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;
Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.;
Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;
and Chintala, S. 2019. PyTorch: An Imperative Style, High-
Performance Deep Learning Library. In Advances in Neural
Information Processing Systems 32 , 8024–8035. Curran As-
sociates, Inc.
Patel, A.; Andrews, N.; and Callison-Burch, C. 2022. Low-
Resource Authorship Style Transfer with In-Context Learn-
ing. arXiv:2212.08986.
Peterson, K.; Hohensee, M.; and Xia, F. 2011. Email For-
mality in the Workplace: A Case Study on the Enron Corpus.
InProceedings of the Workshop on Language in Social Me-
dia (LSM 2011) , 86–95. Portland, Oregon: Association for
Computational Linguistics.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language Models are Unsupervised
Multitask Learners.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring
the Limits of Transfer Learning with a Unified Text-to-Text
Transformer. arXiv:1910.10683.
Reif, E.; Ippolito, D.; Yuan, A.; Coenen, A.; Callison-Burch,
C.; and Wei, J. 2022. A Recipe for Arbitrary Text Style
Transfer with Large Language Models. In Proceedings
of the 60th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers) , 837–848.
Dublin, Ireland: Association for Computational Linguistics.
Reimers, N.; and Gurevych, I. 2019. Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks.
arXiv:1908.10084.
Riley, P.; Constant, N.; Guo, M.; Kumar, G.; Uthus, D.; and
Parekh, Z. 2021. TextSETTR: Few-Shot Text Style Ex-
traction and Tunable Targeted Restyling. In Proceedings
of the 59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint Con-
ference on Natural Language Processing (Volume 1: LongPapers) , 3786–3800. Online: Association for Computational
Linguistics.
Rivera-Soto, R. A.; Miano, O. E.; Ordonez, J.; Chen, B. Y .;
Khan, A.; Bishop, M.; and Andrews, N. 2021. Learning
Universal Authorship Representations. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Lan-
guage Processing , 913–919. Online and Punta Cana, Do-
minican Republic: Association for Computational Linguis-
tics.
Sadvilkar, N.; and Neumann, M. 2020. PySBD: Pragmatic
Sentence Boundary Disambiguation. In Proceedings of Sec-
ond Workshop for NLP Open Source Software (NLP-OSS) ,
110–114. Online: Association for Computational Linguis-
tics.
Saharia, C.; Chan, W.; Chang, H.; Lee, C. A.; Ho, J.; Sali-
mans, T.; Fleet, D. J.; and Norouzi, M. 2022. Palette: Image-
to-Image Diffusion Models. arXiv:2111.05826.
Seabold, S.; and Perktold, J. 2010. statsmodels: Economet-
ric and statistical modeling with python. In 9th Python in
Science Conference .
Sohl-Dickstein, J.; Weiss, E. A.; Maheswaranathan, N.; and
Ganguli, S. 2015. Deep Unsupervised Learning using
Nonequilibrium Thermodynamics. arXiv:1503.03585.
Song, J.; Meng, C.; and Ermon, S. 2022. Denoising Diffu-
sion Implicit Models. arXiv:2010.02502.
Strudel, R.; Tallec, C.; Altch ´e, F.; Du, Y .; Ganin, Y .; Mensch,
A.; Grathwohl, W.; Savinov, N.; Dieleman, S.; Sifre, L.; and
Leblond, R. 2022. Self-conditioned Embedding Diffusion
for Text Generation. arXiv:2211.04236.
Veselovsky, V .; Ribeiro, M. H.; and West, R. 2023. Artifi-
cial Artificial Artificial Intelligence: Crowd Workers Widely
Use Large Language Models for Text Production Tasks.
arXiv:2306.07899.
Warstadt, A.; Singh, A.; and Bowman, S. R. 2019. Neural
Network Acceptability Judgments. Transactions of the As-
sociation for Computational Linguistics , 7: 625–641.
Wegmann, A.; Schraagen, M.; and Nguyen, D. 2022. Same
Author or Just Same Topic? Towards Content-Independent
Style Representations. In Proceedings of the 7th Workshop
on Representation Learning for NLP , 249–268. Dublin, Ire-
land: Association for Computational Linguistics.
Wolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-
son, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,
J.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.;
and Rush, A. M. 2020. HuggingFace’s Transformers: State-
of-the-art Natural Language Processing. arXiv:1910.03771.
Yang, K.; and Klein, D. 2021. FUDGE: Controlled Text
Generation With Future Discriminators. In Proceedings
of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Lan-
guage Technologies . Association for Computational Lin-
guistics.
Yuan, H.; Yuan, Z.; Tan, C.; Huang, F.; and Huang, S. 2023.
SeqDiffuSeq: Text Diffusion with Encoder-Decoder Trans-
formers. arXiv:2212.10325.

--- PAGE 13 ---
Zhang, J.; Zhao, Y .; Saleh, M.; and Liu, P. J. 2020. PE-
GASUS: Pre-training with Extracted Gap-sentences for Ab-
stractive Summarization. arXiv:1912.08777.

--- PAGE 14 ---
Original Text Thursday afternoon would be best for me. Enron North America Corp.
ParaGuide ( λ= 1e4) Thu afternoon works best with me..
ParaGuide ( λ= 5e3) Thursday afternoon would be good for mine
ParaGuide ( λ= 1e3) Thursday afternoon works best for me??
ParaGuide ( λ= 5e2) Thursday afternoon would work best for me...
ParaGuide ( λ= 2e2) Thu afternoon would be best with me!Says
STRAP fine-tuned .. I’m going to be indoors on Thursday afternoon. I’m going to be indoors on Thu[...]
M&M (Hamming) That would be best for me, Enron North America Corp
Original Text Well, don’t get too excited until you read it. There’s lots of stuff I just had[...]
ParaGuide ( λ= 1e4) i just put place holders on but we will get it all filled out, don’t get excited[...]
ParaGuide ( λ= 5e3) ont get excited until you read it.... I just put place holders on but we’ll get[...]
ParaGuide ( λ= 1e3) don’t get excited until you read it, I just put place holders on but we’ll get i[...]
ParaGuide ( λ= 5e2) don’t get excited until you read it - I just put place holders on but we will ge[...]
ParaGuide ( λ= 2e2) don’t get excited until you read it, I just put place holders on but we will get[...]
STRAP fine-tuned I haven’t done anything yet, I will get it all filled in... on the table, I just[...]
M&M (Hamming) don’t get too excited until you read it :) There’s lots of stuff I just had to p[...]
Original Text Jeff, I think this expands your scope. (Which is, of course, a good thing as lon[...]
ParaGuide ( λ= 1e4) Thanks Sue - i feel like this expands some scope & good stuff!
ParaGuide ( λ= 5e3) Hi Sue...I think this really expands your scope...Thanks ¿
ParaGuide ( λ= 1e3) good work, doc - i feel like it expands your scope. Sue
ParaGuide ( λ= 5e2) thanks Sue - i think this expands your scope its a good thing
ParaGuide ( λ= 2e2) sue- i think it expands your scope and generally a good thing.
STRAP fine-tuned , I think that this is a good thing. I think that this expands your scope and is[...]
M&M (Hamming) I think this expands your scope tremendously (Which is, of course, a good thing[...]
Original Text The new market readiness plan appears to target March 1, 2002 as the market open[...]
ParaGuide ( λ= 1e4) On the GANTT chart tabs end Mar 01,,2002,, ds look like the market opening date[...]
ParaGuide ( λ= 5e3) looks like the bars go off GANTT chart March 1 in NEW MARK READY OPEN PLAN.
ParaGuide ( λ= 1e3) got all - the bars in any GANTT chart end March 1, 2002- that appears as NEW mar[...]
ParaGuide ( λ= 5e2) i think the expiration bars in GDT chart - March 01 - 2002.... it looks like mar[...]
ParaGuide ( λ= 2e2) ay - note bars in the GANTTUM chart end on march 1,2002.. so that is the market[...]
STRAP fine-tuned ... The bars end on March 1, 2002, so that appears in the new market readiness p[...]
M&M (Hamming) new market readiness plan appears to target March 1, 2002 as the market opening[...]
Original Text He’s re-trading on the retainage. Let’s discuss. Kay
ParaGuide ( λ= 1e4) he’s re-trading on the retainage...let’s discuss
ParaGuide ( λ= 5e3) hes retrading the reiterate – lets talk
ParaGuide ( λ= 1e3) let me discuss he’s reading on the retainage
ParaGuide ( λ= 5e2) Yeah he’s re-trading on the retainage? Lets discuss!!
ParaGuide ( λ= 2e2) let me discuss... he’s retrading on the retainage
STRAP fine-tuned the retainage. the retainage. he’s re-trading on the retainage.re-trading on the[...]
M&M (Hamming) re-trading on the retainage. Let’s discuss.
Original Text Joanne: please change the attached give-up in Par. 10 for provide for New York ([...]
ParaGuide ( λ= 1e4) OK I print three signatures off today and amend give up Par.10 NOT NEW
ParaGuide ( λ= 5e3) lets print three signatures - change the give-up in Par.10 and reflect New York[...]
ParaGuide ( λ= 1e3) ok just need to print three signatures & change give up on Par. 10 acknowledging[...]
ParaGuide ( λ= 5e2) please print 3 signatures and change give-up in Par.10 so to New York instead of[...]
ParaGuide ( λ= 2e2) please print three signatures also and change the give ups in Par. 10 to reflect[...]
STRAP fine-tuned . I need to print three signatures. Thanks. I need to print three signatures. I[...]
M&M (Hamming) PLEASE change the overall give-up in Par. 10 for ONLY for New York (not NY). The[...]
Table 5: Formal →Informal Examples.

--- PAGE 15 ---
Original Text need defn of above
ParaGuide ( λ= 1e4) Defect above
ParaGuide ( λ= 5e3) DefRN requires above
ParaGuide ( λ= 1e3) Defol Below
ParaGuide ( λ= 5e2) DefN of above required
ParaGuide ( λ= 2e2) Requires defn of above
STRAP fine-tuned . There is a need of defn of above.... above... of above. of above. of above. of[...]
M&M (Hamming) of
Original Text - PJM.xls
ParaGuide ( λ= 1e4) JPJM.”xls
ParaGuide ( λ= 5e3) MQWM.Xls
ParaGuide ( λ= 1e3) PPGM.xls
ParaGuide ( λ= 5e2) JPM.xls
ParaGuide ( λ= 2e2) JPM.xls
STRAP fine-tuned for PJM.xls..xls..xls.xls.xls.M.xls. The PJM.xls is an acronym for PJM...
M&M (Hamming) PJM.
Original Text yes, yes, and yes. You’re on a roll....
ParaGuide ( λ= 1e4) Yes.
ParaGuide ( λ= 5e3) Absolutely rolling
ParaGuide ( λ= 1e3) Yes
ParaGuide ( λ= 5e2) Yes and Jeffrey
ParaGuide ( λ= 2e2) Yes
STRAP fine-tuned and yes, and yes, and yes. Yes, and yes. and yes. Yes, and yes. Yes. Yes. Yes. Y[...]
M&M (Hamming) yes, and yes. You’re on a
Original Text Who’s your buddy???????????????????????????? A little hint wouldn’t kill you, wo[...]
ParaGuide ( λ= 1e4) A little hint wouldn’t kill you... however I will disclose everything
ParaGuide ( λ= 5e3) Minor hints platnt kill you.–I hereby disclose.
ParaGuide ( λ= 1e3) Little teasing Should not kill you. Lets disclose everything
ParaGuide ( λ= 5e2) A little hint wouldn’t kill me. I will share everything. Bx
ParaGuide ( λ= 2e2) I will tell you all details, if a little hint would kill you.
STRAP fine-tuned you know everything. I will let you know everything. I will let you know everyth[...]
M&M (Hamming) your favorite???????????????????????????? A little hint wouldn’t kill you, would[...]
Original Text fyi
ParaGuide ( λ= 1e4) wp
ParaGuide ( λ= 5e3) Yu
ParaGuide ( λ= 1e3) TP
ParaGuide ( λ= 5e2) really quite comprehend you
ParaGuide ( λ= 2e2) I’m not sure what you are referring to.
STRAP fine-tuned . I’m not sure what you’re talking about. I’m not sure what you’re talking about[...]
M&M (Hamming)
Original Text FYI
ParaGuide ( λ= 1e4) True.
ParaGuide ( λ= 5e3) FY .,
ParaGuide ( λ= 1e3) True true
ParaGuide ( λ= 5e2) True
ParaGuide ( λ= 2e2) True
STRAP fine-tuned . It’s true. It’s true. It’s true. It’s true. It’s true...... It’s true... It’s[...]
M&M (Hamming)
Table 6: Infomal →Formal Examples.

--- PAGE 16 ---
Original Text Thanks, Tana! -MP
ParaGuide ( λ= 1e4) FU DIS!!!!!!!!!!!!!!!!
ParaGuide ( λ= 5e3) thanks fuckinea!!!!!!!!
ParaGuide ( λ= 1e3) Thanks, tana?
ParaGuide ( λ= 5e2) Tana?????
ParaGuide ( λ= 2e2) Seeing you Tana?????
STRAP fine-tuned ! I appreciate your kind words. You are so kind, Tana! I appreciate your kind wo[...]
M&M (Hamming) Tana . . .
Original Text when?
ParaGuide ( λ= 1e4) ?????
ParaGuide ( λ= 5e3) abuse?????
ParaGuide ( λ= 1e3) ????? when
ParaGuide ( λ= 5e2) when?????
ParaGuide ( λ= 2e2) when?????
STRAP fine-tuned When?
M&M (Hamming)
Original Text I may be on the road tomorrow (won’t know until later), so something early after[...]
ParaGuide ( λ= 1e4) Early afternoon early??sorry screw thru suck!!!!
ParaGuide ( λ= 5e3) something shitty sucks Chaturs hate thinking road tomorrow.
ParaGuide ( λ= 1e3) too awful!!!!! I’m driving tomorrow. Probably would prefer later afternoon Frida[...]
ParaGuide ( λ= 5e2) Too busy!!!! I’d prefer something early afternoon Friday (may actually be drivin[...]
ParaGuide ( λ= 2e2) because dude knows.......i may be traveling early tomorrow....I’d prefer somethi[...]
STRAP fine-tuned . I would prefer something early morning Friday because I would be on the road t[...]
M&M (Hamming) may be on the road tomorrow (won’t know until later), so retiring early afternoo[...]
Original Text Thanks for the catch! Sara
ParaGuide ( λ= 1e4) Congratulations Sierra......
ParaGuide ( λ= 5e3) Sorry Sheila!!!!!!!!
ParaGuide ( λ= 1e3) Susan Sheila catching?????!!!!!!!!
ParaGuide ( λ= 5e2) Sorry catch...... Sue
ParaGuide ( λ= 2e2) hello eh the catch!!!!!!!!
STRAP fine-tuned I’m glad you’ve fixed it! Thanks, Thanks, I’ll fix it! Thanks, Thanks, Sara! Tha[...]
M&M (Hamming) me the catch!
Original Text Section B2 contains the same argument that we made to FERC in GridFlorida (menti[...]
ParaGuide ( λ= 1e4) same argument GridFlorida210!!!!! due fuck Monday!!!!!
ParaGuide ( λ= 5e3) Report spawned stupid OUR VERY same argument int=/2 over GridFlorida
ParaGuide ( λ= 1e3) FYIs????? Due 61 while ours made....Same argument against section B2 in GridFlor[...]
ParaGuide ( λ= 5e2) It’s due monday morning, we make the same arguments in SECTION B02 in GridFlorid[...]
ParaGuide ( λ= 2e2) ASI is due on Monday......We made the same argument in Section B02 in GridFlorid[...]
STRAP fine-tuned .. Thanks for the link. I’ll look into it... We made the argument in Section B2[...]
M&M (Hamming) B2 contains the same complaints that we made to FERC in GridFlorida (mentions Ca[...]
Original Text also 519128 and 519230
ParaGuide ( λ= 1e4) 548129
ParaGuide ( λ= 5e3) 54819159
ParaGuide ( λ= 1e3) both 55019129 55039
ParaGuide ( λ= 5e2) Also 55019127 55031
ParaGuide ( λ= 2e2) both 519128 52530
STRAP fine-tuned . 51912 and 522530 were also included.... 519124 was also included..530. 519124[...]
M&M (Hamming) 519128 not
Table 7: Positive →Negative Examples.

--- PAGE 17 ---
Original Text Louise, Do we continue to wear the risk/reward and carrying charges on the Mitsu[...]
ParaGuide ( λ= 1e4) do your support welcome our risk/reward plus continue carrying Pere Mitsubishi c[...]
ParaGuide ( λ= 5e3) May we continue delivering on risk/reward for our charges E Mitsubishis? Awesome[...]
ParaGuide ( λ= 1e3) We continue wearing the value/reward and charging charges on our M Mitsubishis?[...]
ParaGuide ( λ= 5e2) So I welcome the Risk/reward and continue carrying charges on the Mitsubishis. T[...]
ParaGuide ( λ= 2e2) Thanks Mike We should continue wearing our risk/reward and continue carrying cha[...]
STRAP fine-tuned .. Chris Calger asked if we should continue wearing the risk/reward and carrying[...]
M&M (Hamming) Do we continue to wear the risk/reward and carrying charges on the Mitsubishis?[...]
Original Text I’m swamped at the moment and I’ll be in South America next week. I then have va[...]
ParaGuide ( λ= 1e4) Hey Mike, I’m going in Chile next week and I am grateful for your help!!!! Just[...]
ParaGuide ( λ= 5e3) Awesome!! I’m going to Chile next week. Just let me know!! thanks
ParaGuide ( λ= 1e3) Just let me know this will help. I am planning to come to South America next wee[...]
ParaGuide ( λ= 5e2) I am on my way to South America next week. I am definitely around and could help[...]
ParaGuide ( λ= 2e2) I’m going over to South America this summer starting next week plus expecting so[...]
STRAP fine-tuned . I need help. I’m going to South America next week and I need help. I need help[...]
M&M (Hamming) swamped at the moment and I’ll be in South America next week. I then have vacati[...]
Original Text sorry...when did you get out of here yesterday?
ParaGuide ( λ= 1e4) its nice great. did you get out here yesterday!
ParaGuide ( λ= 5e3) Thanks! Did your guys acron out yesterday!! Congratulations!
ParaGuide ( λ= 1e3) Thanks! Did you get out of here yesterday!! Thanks!
ParaGuide ( λ= 5e2) Thanks! Did you get out of here yesterday!! Thanks!!!
ParaGuide ( λ= 2e2) did yo get out of here yesterday! we are good o. D
STRAP fine-tuned did you get out of here yesterday? I’m sorry, but did you get out of here yester[...]
M&M (Hamming) did you get out of here
Original Text PLEASE DISREGARD THIS INFO. WE ARE NO LONGER DOING THE LENDING DEAL [...]
ParaGuide ( λ= 1e4) Love Bernie VirFranc EPS We supportForward Garage
ParaGuide ( λ= 5e3) ourperforming conquering Beaver 38993
ParaGuide ( λ= 1e3) Our keep working In our loan deal closed.
ParaGuide ( λ= 5e2) We are bank executed our new deal thru Friday.
ParaGuide ( λ= 2e2) This is the Loan deal for Friday and Saturday, we are definitely doing this week[...]
STRAP fine-tuned deal on Friday and Saturday.. We are not doing the loan deal on Friday and Satur[...]
M&M (Hamming) DISREGARD THIS POST. WE ARE NO LONGER DOING THE LENDING DEAL [...]
Original Text if I were a rich man.... or I’ll call you later.
ParaGuide ( λ= 1e4) Oh you are pretty incredibly rich! I’ll call you later! Congratulations!
ParaGuide ( λ= 5e3) Congratulations! My thoughts were totally rich man! I’ll call you later!
ParaGuide ( λ= 1e3) only I was sweetest! i will call you later
ParaGuide ( λ= 5e2) If I was rich bought, I will call you later! Congratulations.
ParaGuide ( λ= 2e2) if i was rich i’ll call you later :)
STRAP fine-tuned I’ll tell you later, if you’re rich. I’d be rich. I’ll tell you later, if I’m ri[...]
M&M (Hamming) I were a rich man.... or I’ll call you
Original Text It is not intended to be a negotiated rate. We will add the other language when[...]
ParaGuide ( λ= 1e4) that’s negotiated rate love me!! Vince
ParaGuide ( λ= 5e3) 801MM favorably really agreefully rate
ParaGuide ( λ= 1e3) non bid
ParaGuide ( λ= 5e2) doesn’t negrated rate :)
ParaGuide ( λ= 2e2) Means for point means being a negotiated rate!
STRAP fine-tuned a negotiated rate. It’s a flat rate.. It’s a guideline. It’s a guideline. a fixe[...]
M&M (Hamming) is not intended to be a negotiated rate. We will add the other language when a d[...]
Table 8: Negative →Positive Examples.

--- PAGE 18 ---
Target Author Examples
Some very nice, and very appropriate thoughts about John Ambler. Wade continues[...]
Here’s an early draft. Note Cinergy sr. mgmt has not yet commented. No price, ei[...]
Here’s the draft that Allegretti is sending.
here you go
Sounds pretty harmless to me. If you don’t want to do it, let me know and I’ll p[...]
...
Original Text Thanks. Comments below. Great input.
PARA I would like to thank you for the great input.
BERT Thanks. Thanks fine. Encouraging price.
LING Thanks. comments below. great input.
STRAP fine-tuned Thanks for the great input. Mark
ChatGPT Here’s my feedback. Great points. Mark
ParaGuide ( λ= 200 ) Thanks for the great input. BT
ParaGuide ( λ= 500 ) Thanks for the great input
ParaGuide ( λ= 800 ) Thanks for the great input!
ParaGuide ( λ= 1000 ) thanks for the great input.
ParaGuide ( λ= 1500 ) Great input. Thanks!
ParaGuide ( λ= 2000 ) Thanks for the great input
ParaGuide ( λ= 2500 ) Thanks for the great input, BT
ParaGuide ( λ= 5000 ) It’s truly useful input. Thanks! Mark
Table 9: Authorship Transfer Example 1. In this example, as we increase λ, we can observe several changes, including the
introduction of a contraction, which matches several sample texts. Additionally, like S TRAP and ChatGPT, with a larger λ,
PARAGUIDE also splits the output into two sentences, and adds the ‘Mark’ signature. While this is not visible in the truncated
input, the target author signs their emails with ‘Mark’.
Target Author Examples
scriptsnetscape.wbt ”http://x12.enron.com/hottap.htm”
IT’S NOT JO JO - IT’S J-LO I THINK SHE WAS ON AT 8 HAS ANYONE JUMPED OUT THE WIN[...]
¿¿ This is something you should see ¿¿ Puts things in perspective. ¿¿ ¿¿ http://[...]
so YOU watched it??
i did hear that it was rather revealing
...
Original Text Then why are you dating a girl from LSU?
PARA Why do you want to be with a girl from LSU?
BERT So how are YOU trying the perspective from LSU?
LING then why am you dating a girl from lsu””
STRAP fine-tuned why are you dating a girl from LSU?
ChatGPT so YOU dating a girl from LSU??
ParaGuide ( λ= 200 ) why do you want to be with a girl from LSU?
ParaGuide ( λ= 500 ) why do you want to be with a girl from LSU?
ParaGuide ( λ= 800 ) why do you want to be with a girl from LSU?
ParaGuide ( λ= 1000 ) why do you want to be with a girl from LSU
ParaGuide ( λ= 1500 ) why do you want to be with a girl from LSU?
ParaGuide ( λ= 2000 ) why do you need to be with a girl from LSU?
ParaGuide ( λ= 2500 ) why do you need to be with a girl from LSU?
ParaGuide ( λ= 5000 ) why do u need to be with some chicks from LSU??
Table 10: Authorship Transfer Example 2. In this example, P ARAGUIDE lowercases the outputs, and uses more informal
substitutions at λ= 5000 , including ‘you’ →‘u’.
