# 2212.11685.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/text-diffusion/2212.11685.pdf
# Kích thước tệp: 593970 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Sinh Văn Bản với Mô Hình Ngôn Ngữ Khuếch Tán: Phương Pháp Tiền Huấn Luyện
với Khử Nhiễu Đoạn Văn Liên Tục
Zhenghao Lin1 2Yeyun Gong3Yelong Shen4Tong Wu5 2Zhihao Fan6 2
Chen Lin1Nan Duan3Weizhu Chen4

Tóm tắt
Trong bài báo này, chúng tôi giới thiệu một khung tiền huấn luyện mô hình ngôn ngữ khuếch tán mới cho sinh văn bản, mà chúng tôi gọi là GENIE. GENIE là một mô hình ngôn ngữ khuếch tán tiền huấn luyện quy mô lớn bao gồm một bộ mã hóa và một bộ giải mã dựa trên khuếch tán, có thể sinh văn bản bằng cách dần dần biến đổi một chuỗi nhiễu ngẫu nhiên thành một chuỗi văn bản mạch lạc. Để tiền huấn luyện GENIE trên kho dữ liệu ngôn ngữ quy mô lớn, chúng tôi thiết kế một mục tiêu khử nhiễu đoạn văn liên tục mới, khuyến khích bộ giải mã khuếch tán tái tạo một đoạn văn sạch từ phiên bản bị hỏng, đồng thời bảo tồn tính mạch lạc về ngữ nghĩa và cú pháp. Chúng tôi đánh giá GENIE trên bốn benchmark sinh văn bản downstream, cụ thể là XSUM, CNN/DAILY MAIL, GIGAWORD, và COMMON GEN. Kết quả thực nghiệm của chúng tôi cho thấy GENIE đạt được hiệu suất tương đương với các mô hình tự hồi quy tiên tiến nhất trên các benchmark này, và sinh ra các mẫu văn bản đa dạng hơn. Mã nguồn và mô hình của GENIE có sẵn tại https://github.com/microsoft/ProphetNet/tree/master/GENIE.

1. Giới thiệu
Sinh văn bản là một nhiệm vụ quan trọng trong xử lý ngôn ngữ tự nhiên, nhằm tạo ra các văn bản trôi chảy và mạch lạc cho các ứng dụng khác nhau. Các phương pháp sinh văn bản trước đây chủ yếu dựa vào mạng nơ-ron tái phát (RNN) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), sinh văn bản tuần tự từ trái sang phải. Tuy nhiên, RNN gặp phải các vấn đề như phụ thuộc dài hạn và bias phơi bày. Gần đây, Transformer (Vaswani et al., 2017b), một mạng nơ-ron dựa trên cơ chế chú ý tự động, đã nổi lên như mô hình chủ đạo cho sinh văn bản, nhờ khả năng nắm bắt các phụ thuộc toàn cục và tận dụng các mô hình ngôn ngữ tiền huấn luyện quy mô lớn (Qi et al., 2020; Lewis et al., 2019; Raffel et al., 2020a). Các phương pháp dựa trên Transformer thường áp dụng kiến trúc mã hóa-giải mã, trong đó bộ mã hóa ánh xạ văn bản đầu vào thành một chuỗi các vector ẩn, và bộ giải mã sinh văn bản đầu ra theo cách tự hồi quy (AR) hoặc không tự hồi quy (NAR). Giải mã AR chính xác hơn nhưng chậm hơn, vì nó dự đoán từng từ dựa trên các từ trước đó. Giải mã NAR nhanh hơn nhưng kém chính xác, vì nó dự đoán tất cả các từ đồng thời mà không mô hình hóa các phụ thuộc giữa chúng.

Trong bài báo này, chúng tôi trình bày một phương pháp sinh văn bản mới, gọi là GENIE, tích hợp mô hình khuếch tán và phương pháp dựa trên Transformer. Mô hình khuếch tán là một mô hình sinh đảo ngược một quá trình ngẫu nhiên thêm nhiễu vào dữ liệu, và đã cho thấy kết quả đầy hứa hẹn trong sinh ảnh (Ho et al., 2020; Song et al., 2020), phân tử (Hoogeboom et al., 2022), video (Ho et al., 2022), và văn bản (Li et al., 2022b; Gong et al., 2022; Strudel et al., 2022; Reid et al., 2022). GENIE tuân theo kiến trúc mã hóa-giải mã, trong đó bộ mã hóa biến đổi văn bản đầu vào thành các vector ẩn, và mô hình khuếch tán khôi phục văn bản đầu ra từ một nhiễu Gaussian ngẫu nhiên, được hướng dẫn bởi các vector ẩn của bộ mã hóa. Mô hình khuếch tán lặp qua nhiều bước thời gian, và dần dần khử nhiễu văn bản đầu ra ở mỗi bước.

Để tận dụng dữ liệu văn bản không nhãn quy mô lớn, chúng tôi cũng đề xuất một phương pháp tiền huấn luyện đầu cuối cho GENIE. Khác với các nhiệm vụ tiền huấn luyện hiện có liên quan đến che giấu hoặc phân tách token hoặc văn bản (Qi et al., 2020; Lewis et al., 2019; Raffel et al., 2020a), chúng tôi thiết kế một nhiệm vụ tiền huấn luyện mới, gọi là khử nhiễu đoạn văn liên tục (CPD). CPD yêu cầu mô hình dự đoán nhiễu được thêm vào các đoạn văn liên tục trong bước thời gian hiện tại, cho trước ngữ cảnh đoạn văn và thông tin đoạn văn nhiễu.

Chúng tôi đánh giá GENIE trên bốn benchmark sinh văn bản phổ biến: XSum (Narayan et al., 2018), CNN/DailyMail (Hermann et al., 2015), Gigaword (Rush et al., 2015), và CommonGen (Lin et al., 2019). Kết quả thực nghiệm chứng minh rằng GENIE đạt hiệu suất cạnh tranh với các phương pháp AR dựa trên Transformer, và phương pháp tiền huấn luyện được đề xuất có thể cải thiện hiệu suất một cách hiệu quả.

Chúng tôi nhận thấy rằng GENIE đã đạt được những cải tiến đáng kể trong chỉ số đa dạng. Để đánh giá các đầu ra đa dạng của mô hình sinh, chúng tôi thiết kế một phương pháp chú thích tự động dựa trên mô hình ngôn ngữ lớn. Chúng tôi cũng tiến hành các nghiên cứu bổ sung để phân tích tác động của các bước khuếch tán và bước tiền huấn luyện.

Các đóng góp chính của công trình này được tóm tắt như sau:
• Chúng tôi đề xuất GENIE, mô hình tiền huấn luyện ngôn ngữ quy mô lớn đầu tiên dựa trên khung khuếch tán, có thể sinh văn bản chất lượng cao cho các nhiệm vụ chuỗi-sang-chuỗi.
• Chúng tôi giới thiệu một loss CPD mới như mục tiêu tiền huấn luyện, có thể tăng cường khả năng khử nhiễu văn bản nhiễu của mô hình và nắm bắt tính mạch lạc cấp độ đoạn văn.
• Chúng tôi xác thực hiệu quả của mô hình khuếch tán tiền huấn luyện trên các nhiệm vụ downstream, và thiết kế một phương pháp chú thích tự động mới cho việc đánh giá dựa trên mô hình ngôn ngữ lớn. Chúng tôi cũng cung cấp các phân tích mở rộng về hành vi và tính chất của mô hình.

2. Sơ lược
2.1. Định nghĩa Nhiệm vụ
Trong nhiệm vụ chuỗi-sang-chuỗi cổ điển, cho một văn bản nguồn s={ws1;ws2;...;wsn} với n token, nó sinh chuỗi văn bản đích y={wy1;wy2;...;wyn}. Một mô hình sinh chuỗi có thể đạt được điều này bằng cách mô hình hóa xác suất có điều kiện: p(y|s).

2.2. Mô hình khuếch tán
Trong mô hình khuếch tán, quá trình khuếch tán có thể được coi như một quá trình Markov thời gian rời rạc. Quá trình khuếch tán bắt đầu với trạng thái ban đầu x0 tại bước thời gian t=0, trong đó x0 là phân phối Gaussian của dữ liệu gốc. Nó dần dần thêm nhiễu Gaussian vào x0 trong quá trình khuếch tán thuận theo một lịch phương sai β1,...,βT. Tại bước thời gian t+1, biến tiềm ẩn xt+1 chỉ được xác định bởi xt tại thời gian t, biểu thị là:

q(xt+1|xt) = N(xt+1; √(1-βt+1)xt, βt+1I)     (1)

Khi t tăng, xt trở nên gần với nhiễu Gaussian chuẩn N(xT; 0,I).

Mô hình khuếch tán học thực hiện quá trình khuếch tán nghịch trong quá trình sinh, dự đoán nhiễu cho trước trạng thái hiện tại xt tại bước thời gian t. Trạng thái trước đó xt-1 có thể được tái tạo bằng cách trừ đi nhiễu và tái tỷ lệ trung bình. Do đó, phân phối của xt-1 cho trước xt là một Gaussian với trung bình μt-1 và phương sai σt-1²:

p(xt-1|xt) = N(xt-1; μt-1, σt-1I)     (2)

μt-1 = (1/√αt)(xt - (βt/√(1-ᾱt))zθ(xt,t))     (3)

σt-1² = ((1-ᾱt-1)/(1-ᾱt))βt     (4)

trong đó αt = 1-βt, ᾱt = ∏ti=1 αi và zθ được dự đoán bởi một mạng nơ-ron tham số hóa bởi θ. Mô hình khuếch tán được huấn luyện bằng cách tối thiểu hóa lỗi bình phương trung bình giữa μt-1 và trung bình thực μ̂t-1, được tính từ phân phối điều kiện nghịch q(xt-1|xt,x0):

q(xt-1|xt,x0) = N(xt-1; μ̂t-1, σ̂t-1I)     (5)

μ̂t-1 = (√ᾱt-1βt/(1-ᾱt))x0 + (√αt(1-ᾱt-1)/(1-ᾱt))xt     (6)

Theo phương pháp cận dưới biến phân (VLB) (Ho et al., 2020), mô hình khuếch tán có thể được huấn luyện bằng cách tối thiểu hóa hàm loss:

Ldiff = ∑T(t=1) Eq(xt|x0)[||μt-1 - μ̂t-1||²]     (7)

3. Mô hình
GENIE là mô hình ngôn ngữ khuếch tán được đề xuất cho tiền huấn luyện, nó áp dụng khung chuỗi-sang-chuỗi như minh họa trong Hình 1. GENIE có thể sinh một chuỗi văn bản chất lượng cao y cho trước văn bản nguồn s, chẳng hạn như tạo ra y: "Hiệu suất của Messi" từ s: "Trong World Cup 2022, [MASK] đã nhận được lời khen ngợi của mọi người.". Để đạt được điều này, GENIE tận dụng hai thành phần: một mô hình mã hóa hai chiều và một mô hình khuếch tán chú ý chéo. Mô hình mã hóa mã hóa văn bản nguồn s thành một tập hợp các vector ẩn Hs=Encoder(s), biểu thị biểu diễn phân bố của s. Mô hình khuếch tán nhận Hs và một nhiễu Gaussian làm đầu vào, và lặp đi lặp lại tinh chỉnh dữ liệu bằng cách áp dụng một chuỗi các phép toán khử nhiễu. Trái ngược với mô hình sinh văn bản tự hồi quy truyền thống, sinh từng token một lần, mô hình khuếch tán trong GENIE xuất chuỗi các embedding song song tại mỗi bước khử nhiễu, làm cho GENIE trở thành một mô hình sinh không tự hồi quy (NAR).

Bộ mã hóa Bộ mã hóa trong GENIE là một mô hình transformer 6 lớp nhận văn bản nguồn s làm đầu vào với cơ chế chú ý tự động hai chiều. Cụ thể, cho một chuỗi văn bản nguồn s={ws1;ws2;...;wsn} với n token, mô hình mã hóa tính toán vector hi cho mỗi token wi. Do đó, văn bản nguồn s có thể được biểu diễn như Hs bởi mô hình mã hóa:

Hs = {h1,h2,...,hn} = Encoder(s)     (8)

--- TRANG 3 ---
Mô hình Ngôn ngữ Khuếch tán Bộ mã hóa nhận chuỗi nguồn bị che s làm đầu vào của Bộ mã hóa để thu được thông tin ẩn Hs, và tương tác với Mô hình Ngôn ngữ Khuếch tán thông qua cơ chế chú ý chéo. Mô hình Ngôn ngữ Khuếch tán khôi phục nhiễu Gaussian ngẫu nhiên ban đầu thành văn bản đầu ra y thông qua quá trình khử nhiễu và grounding lặp.

Mô hình Ngôn ngữ Khuếch tán Mô hình khuếch tán trong GENIE là một transformer 6 lớp với cơ chế chú ý chéo trên biểu diễn văn bản nguồn Hs. Nó học dự đoán nhiễu Gaussian zθ(xt,t,Hs) có điều kiện trên bước khuếch tán hiện tại t và trạng thái xt, trong đó xt là biểu diễn tiềm ẩn liên tục của văn bản đích. Chúng tôi sử dụng một hàm embedding và một thủ thuật clamping để ground trạng thái liên tục xt với các token đích rời rạc, sẽ được trình bày chi tiết trong phần tiếp theo.

Giai đoạn Suy luận Để sinh văn bản từ mô hình khuếch tán, chúng tôi bắt đầu từ bước cuối t=T và lấy mẫu một trạng thái xT từ phân phối Gaussian chuẩn. Sau đó chúng tôi lặp sinh nhiễu cho bước trước đó sử dụng phương trình 3 và 4, và trừ nó khỏi trạng thái hiện tại để thu được xt-1. Sau khi đến t=0, chúng tôi áp dụng thủ thuật clamping (Li et al., 2022b) để thay thế các giá trị của x0 bằng các word embedding gần nhất, và sau đó giải mã các token rời rạc từ x0.

Giai đoạn Huấn luyện Để huấn luyện mô hình khuếch tán cho các nhiệm vụ chuỗi-sang-chuỗi, trước tiên chúng tôi chuyển đổi chuỗi đích y={wy1;wy2;...;wyn} thành một trạng thái liên tục x0 sử dụng hàm embedding với một nhiễu Gaussian bổ sung, có thể được biểu thị là:

q(x0|y) = N(x0; Emb(y), σ0I)     (9)

trong đó Emb() là hàm embedding, σ0 đại diện cho tỷ lệ của phương sai tại bước thời gian t=0. Sau đó chúng tôi áp dụng quá trình khuếch tán thuận (phương trình 1) để thu được trạng thái xt tại bất kỳ bước t nào như một hàm của x0, như được hiển thị trong phương trình:

q(xt|x0) = N(xt; √ᾱtx0, √(1-ᾱt)I)     (10)

trong đó ᾱt = ∏ti=1 αi. Trong giai đoạn huấn luyện, chúng tôi lấy mẫu một bước ngẫu nhiên t để tính xt, và sau đó sử dụng kiến trúc khử nhiễu để dự đoán nhiễu cho bước đó, dựa trên cơ chế chú ý chéo với biểu diễn nguồn Hs. Trung bình và phương sai của nhiễu được dự đoán được cho bởi phương trình 11:

μt-1 = (1/√αt)(xt - (βt/√(1-ᾱt))zθ(xt,t,Hs))     (11)

trong đó zθ là đầu ra của kiến trúc khử nhiễu và θ là các tham số của nó. Mục tiêu huấn luyện là tối thiểu hóa lỗi bình phương giữa nhiễu được dự đoán và nhiễu thực, cũng như lỗi tái tạo giữa x0 và các embedding đích, như được biểu thị trong phương trình 12:

Ls2s = Eq(x0:T|y)[∑T(t=1)||μt-1 - μ̂t-1||² + ||Emb(y) - μ0||² - log p(y|x0)]     (12)

trong đó p(y|x0) = ∏ni=1 p(wyi|x0), đại diện cho việc ánh xạ biến tiềm ẩn liên tục x0 vào không gian rời rạc token wyi.

3.1. Tiền huấn luyện GENIE
Các mô hình khuếch tán có tiềm năng lớn cho sinh ngôn ngữ tự nhiên (NLG) do khả năng tạo ra các đầu ra đa dạng. Tuy nhiên, chúng đã bị bỏ qua phần lớn trong NLG vì sự hội tụ chậm và chất lượng thấp so với các mô hình tự hồi quy. Trong phần này, chúng tôi giải quyết những thách thức này bằng cách tiền huấn luyện một mô hình ngôn ngữ khuếch tán và giới thiệu một nhiệm vụ tiền huấn luyện mới được thiết kế riêng cho nó. Nhiệm vụ tiền huấn luyện mới mà chúng tôi đề xuất được gọi là khử nhiễu đoạn văn liên tục (CPD). CPD nhằm huấn luyện mô hình dự đoán nhiễu được thêm vào một đoạn văn liên tục trong bước khuếch tán hiện tại, cho trước đoạn văn và ngữ cảnh xung quanh nó.

Cụ thể, cho một tài liệu d={wd1;wd2;...;wdl} với l từ, chúng tôi ngẫu nhiên chọn một đoạn văn p={wp1;wp2;...;wpm} từ d, trong đó m=⌊α×l⌋ là độ dài đoạn văn và α là một tỷ lệ được định nghĩa trước. Chúng tôi che đoạn văn trong tài liệu bằng một token đặc biệt ([MASK]), và đưa tài liệu bị che d'={wd'1;wd'2;...;[MASK];...;wd'l-m} vào bộ mã hóa GENIE. Chúng tôi cũng áp dụng quá trình khuếch tán thuận lên đoạn văn p và thu được một phiên bản nhiễu xt tại một bước ngẫu nhiên t, và đưa nó vào kiến trúc khử nhiễu GENIE. Kiến trúc khử nhiễu sau đó sử dụng cơ chế chú ý chéo với biểu diễn nguồn Hs để dự đoán nhiễu cho bước hiện tại, sử dụng phương trình 11. Tóm lại, mục tiêu tiền huấn luyện của CPD là tối thiểu hóa cùng loss như trong phương trình 12, ngoại trừ việc y được thay thế bằng p và x0 là đoạn văn được embedding với nhiễu.

Thông qua nhiệm vụ tiền huấn luyện này, mô hình khuếch tán có thể tăng cường hiểu biết ngữ nghĩa về văn bản liên tục và khả năng khử nhiễu ở mỗi bước khuếch tán. Hơn nữa, nhiệm vụ CPD là tự giám sát và không dựa vào các nguồn dữ liệu được gán nhãn bên ngoài, vì vậy nó có thể khai thác đầy đủ thông tin trong kho dữ liệu tiền huấn luyện gốc.

4. Thí nghiệm và Kết quả
Trong phần này, chúng tôi sẽ giới thiệu chi tiết về tiền huấn luyện GENIE, thiết lập dữ liệu, và cho thấy kết quả thực nghiệm mở rộng trên các nhiệm vụ NLG downstream khác nhau.

4.1. Tiền huấn luyện GENIE
Khung Mô hình Mô hình của chúng tôi sử dụng một transformer 6 lớp làm bộ mã hóa, và một transformer chú ý chéo 6 lớp làm kiến trúc khử nhiễu. Cụ thể, trong kiến trúc khử nhiễu, chúng tôi sử dụng hàm embedding ngẫu nhiên để ánh xạ token rời rạc thành biến liên tục. Chúng tôi đặt kích thước biến tiềm ẩn là 768 và kích thước embedding là 128.

Dữ liệu Tiền huấn luyện Các công trình gần đây đã cho thấy rằng tiền huấn luyện trên kho dữ liệu quy mô lớn có thể cải thiện hiệu suất của mô hình trên các nhiệm vụ downstream (Lewis et al., 2019; Qi et al., 2020), điều này cũng áp dụng cho GENIE dựa trên mô hình khuếch tán. Theo BART (Lewis et al., 2019), chúng tôi sử dụng dữ liệu tiền huấn luyện bao gồm 160GB tin tức, sách, truyện và văn bản web. Chúng tôi phân đoạn các câu thuộc về các chương khác nhau, và đảm bảo rằng độ dài văn bản đầu vào không vượt quá 512.

Thiết lập Tiền huấn luyện Chúng tôi sử dụng nhiệm vụ CPD được đề cập trong §3.1 để tiền huấn luyện GENIE trên kho dữ liệu quy mô lớn. Tỷ lệ đoạn văn liên tục α được đặt là 30%, do đó, đối với đầu vào có độ dài 512, độ dài đích là 153. Chúng tôi ngẫu nhiên trích xuất đích có độ dài 153 từ văn bản đầu vào, và để lại token [MASK] tại vị trí được trích xuất. Trong quá trình huấn luyện, chúng tôi sử dụng trình tối ưu Adam (Kingma & Ba, 2015) với tốc độ học 1e-4, và chúng tôi đặt kích thước batch là 512. Chúng tôi tiền huấn luyện mô hình của chúng tôi trên 8×40GB NVIDIA A100 GPU với 5 triệu bước, kéo dài 50 ngày. Trong giai đoạn fine-tuning, chúng tôi sử dụng checkpoint mô hình tiền huấn luyện cuối cùng để tiến hành fine-tuning trên các nhiệm vụ downstream khác nhau.

4.2. Fine-tune trên Các Nhiệm vụ Downstream
Để xác minh hiệu quả của tiền huấn luyện trên GENIE dựa trên mô hình khuếch tán, chúng tôi fine-tune và xác minh hiệu quả của GENIE trên nhiều nhiệm vụ downstream khác nhau. Thông qua nhiệm vụ trên, chúng tôi có thể chứng minh rằng GENIE tiền huấn luyện có thể nhanh chóng thích ứng với các loại nhiệm vụ NLG khác nhau mà không cần thời gian huấn luyện dài như các mô hình khuếch tán khác.

Tóm tắt Văn bản Là một nhiệm vụ quan trọng trong lĩnh vực NLG, tóm tắt văn bản nhằm tóm tắt các tài liệu dài thành văn bản ngắn trôi chảy. Trong thí nghiệm, chúng tôi chọn ba tập dữ liệu được sử dụng rộng rãi: (a) kho dữ liệu GIGAWORD (Rush et al., 2015), (b) CNN/DAILY MAIL (Hermann et al., 2015), và (c) XSUM (Narayan et al., 2018). Trong quá trình fine-tuning, chúng tôi đặt tốc độ học là 5e-5 và 120K bước huấn luyện cho cả ba tập dữ liệu. Trong quá trình suy luận, chúng tôi ngẫu nhiên lấy mẫu 10 nhiễu Gaussian để lặp khử nhiễu, và sử dụng điểm số cao nhất làm kết quả sinh cuối cùng. Đối với số lượng mẫu khác nhau, vui lòng tham khảo Phụ lục C. Trong quá trình đánh giá, chúng tôi theo các công trình hiện có (Lewis et al., 2019; Qi et al., 2020), báo cáo điểm F1 của ROUGE-1, ROUGE-2, và ROUGE-L trên tập test.

Sinh Thông thường Các nhiệm vụ sinh thông thường yêu cầu mô hình có khả năng lý luận thông thường sinh. Cụ thể, cho một loạt các khái niệm thông thường, mô hình cần sinh ra các phát biểu mạch lạc dựa trên những khái niệm này tuân thủ các kịch bản thế giới thực. Chúng tôi chọn tập dữ liệu được sử dụng rộng rãi COMMON GEN (Lin et al., 2019) để đánh giá xem GENIE có khả năng sáng tạo và lý luận tốt trong sinh ngôn ngữ tự nhiên hay không. Trong giai đoạn fine-tuning, chúng tôi đặt tốc độ học là 1e-4 và huấn luyện tổng cộng 10k bước. Cuối cùng, chúng tôi ngẫu nhiên lấy mẫu 10 nhiễu gaussian và chọn mẫu tốt nhất làm kết quả cuối cùng. Tham khảo công trình trước đó (Lin et al., 2019), chúng tôi báo cáo các chỉ số bao gồm điểm F1 của ROUGE-2/L, BLEU-3/4, CLDEr, và SPICE.

4.3. Baselines
Chúng tôi so sánh GENIE với các baseline của một số phương pháp chính thống. Cụ thể, những phương pháp này có thể được chia thành hai nhóm. Nhóm đầu tiên là mô hình NAR, bao gồm NAT (Gu et al., 2017), iNAT (Lee et al., 2018), NAG-BERT (Su et al., 2021), CMLM (Ghazvininejad et al., 2019), LevT (Gu et al., 2019b), ConstLeven (Susanto et al., 2020), BANG (Qi et al., 2021), ELMER (Li et al., 2022a) và InsT (Stern et al., 2019). Trong số đó, InsT, iNAT, CMLM, LevT, ConstLeven và BANG cũng có thể được sử dụng trong Semi-NAR, có thể tối ưu hóa chất lượng sinh thông qua nhiều lần lặp NAR. Đáng chú ý là GENIE cũng thuộc mô hình Semi-NAR.

Nhóm thứ hai là mô hình AR, mô hình có cấu trúc mã hóa-giải mã bao gồm LSTM (Greff et al., 2017), Transformer (Vaswani et al., 2017a), bRNN-CopyNet (Gu et al., 2016a), Trans-CopyNet (Lin et al., 2019), MeanPooling-CopyNet (Lin et al., 2019) không có tiền huấn luyện, và các baseline mạnh MASS (Song et al., 2019), BART (Lewis et al., 2019), T5 (Raffel et al., 2020b), BANG (Qi et al., 2021), và ProphetNet (Qi et al., 2020) với tiền huấn luyện quy mô lớn. Đối với các mô hình tiền huấn luyện quy mô lớn được đề cập ở trên, chúng tôi chọn phiên bản cơ sở của mô hình, tương đương với tổng số tham số của GENIE.

4.4. Kết quả Chính
Chúng tôi trình bày kết quả của GENIE và các baseline trên XSUM, CNN/DAILY MAIL, GIGAWORD, COMMON GEN trong Bảng 1, Bảng 2, và Bảng 3. Kết quả của chúng tôi chứng minh rằng GENIE tiền huấn luyện là một mô hình NAR mạnh mẽ cho sinh văn bản. Đặc biệt trên tập dữ liệu XSUM, GENIE vượt trội hơn các phương pháp NAR và Semi-NAR khác với biên độ lớn, và trên cả ba tập dữ liệu tóm tắt văn bản, GENIE đạt chất lượng tương đương với mô hình AR tiền huấn luyện. Ngoài ra, GENIE cho thấy sự sáng tạo và logic trong các nhiệm vụ sinh thông thường. Trên COMMON GEN, GENIE vượt qua các mô hình baseline khác, bao gồm T5 đã được tiền huấn luyện trên kho dữ liệu quy mô lớn.

Chúng tôi cũng so sánh GENIE tiền huấn luyện và GENIE được huấn luyện từ đầu (w/o pre-train). Như được hiển thị trong Bảng 1 và Bảng 2, tiền huấn luyện cải thiện đáng kể điểm ROUGE-1, ROUGE-2, ROUGE-L của GENIE trên ba tập dữ liệu tóm tắt văn bản. Tương tự, kết quả trên COMMON GEN trong Bảng 3 cho thấy tiền huấn luyện tăng cường hiệu suất của GENIE trên nhiệm vụ này. Những kết quả này xác nhận hiệu quả của phương pháp tiền huấn luyện của chúng tôi.

4.5. So sánh Đa dạng Sinh
Với sự xuất hiện của mô hình dựa trên khuếch tán như GENIE, những ưu điểm của sinh văn bản trong đa dạng sẽ dần được đánh giá cao. Trong thí nghiệm này, chúng tôi sẽ sử dụng cả chỉ số định lượng và ví dụ định tính để cho thấy sự phong phú của GENIE trong sinh văn bản.

Để đo lường đa dạng của sinh GENIE, chúng tôi sử dụng SELF-BLEU làm chỉ số. Điểm SELF-BLEU càng thấp, các văn bản được sinh càng đa dạng. Để so sánh, chúng tôi sử dụng BART, một mô hình tự hồi quy tiên tiến, được tiền huấn luyện trên kho dữ liệu quy mô lớn. Đối với BART, chúng tôi áp dụng các phương pháp giải mã khác nhau của mô hình tự hồi quy, như tìm kiếm tham lam, tìm kiếm chùm (Xiao et al., 2022), tìm kiếm chùm đa dạng (độ mạnh đa dạng = 0.8) (Vijayakumar et al., 2016), lấy mẫu điển hình (τ = 1:2) (Meister et al., 2022), lấy mẫu top-k (k = 50) (Fan et al., 2018), và lấy mẫu nucleus (p = 0:92) (Holtzman et al., 2020). Những phương pháp giải mã này có thể sinh nhiều văn bản từ cùng một chuỗi nguồn. Trong thí nghiệm này, chúng tôi sinh 10 chuỗi đích khác nhau cho mỗi chuỗi nguồn sử dụng GENIE và BART. Sau đó chúng tôi sử dụng 10 bản tóm tắt được sinh từ XSUM, CNN/DAILY MAIL, và GIGAWORD để tính điểm SELF-BLEU.

Như được hiển thị trong Bảng 4, mặc dù đa dạng của sinh tự hồi quy có thể được cải thiện nhẹ bằng cách sử dụng tìm kiếm chùm đa dạng hoặc một số phương pháp lấy mẫu với BART, sự cải thiện không đáng kể. Mặt khác, đa dạng của sinh được tăng cường rất nhiều bằng cách sử dụng GENIE. Khoảng cách lớn trong SELF-BLEU cho thấy GENIE có thể sinh các văn bản đa dạng hơn, không chỉ thay đổi một vài từ.

Để bổ sung cho các chỉ số định lượng, chúng tôi cũng cung cấp một nghiên cứu trường hợp trong Phụ lục A để phân tích chất lượng của các văn bản được sinh bởi BART và GENIE. Chúng tôi thấy rằng phương pháp sinh tự hồi quy có thể tạo ra văn bản chất lượng cao khi chỉ có một đầu ra, nhưng khi sinh nhiều đầu ra, ngay cả với các phương pháp giải mã khác nhau, khó có thể tăng đa dạng của nó, và có thể có nhiều tiền tố lặp lại. Ngược lại, phương pháp sinh khuếch tán có thể duy trì chất lượng sinh đồng thời mang lại đa dạng phong phú.

Tuy nhiên, có thể không công bằng khi so sánh GENIE trực tiếp với tham chiếu đơn để chứng minh rằng GENIE có thể đạt được đa dạng mà không ảnh hưởng đến chất lượng. Do đó, chúng tôi thiết kế một phương pháp đánh giá mới. Chúng tôi sử dụng phiên bản text-davinci-003 của InstructGPT (Ouyang et al., 2022), dựa trên mô hình ngôn ngữ lớn (LLM) GPT-3.5, để chấm điểm các văn bản được sinh của chúng tôi, tức là đánh giá chất lượng của các bản tóm tắt được sinh. Cụ thể, trước tiên chúng tôi thu được tập mẫu (10 bản tóm tắt được sinh bởi BART sử dụng tìm kiếm chùm đa dạng và 10 bản tóm tắt được sinh bởi GENIE), và thiết kế một prompt để đưa vào text-davinci-003 để chấm điểm các bản tóm tắt được sinh, đồng thời đếm số lượng bản tóm tắt chất lượng cao trong 10 bản tóm tắt được sinh bởi BART và GENIE tương ứng. Chúng tôi tiến hành thí nghiệm trên ba tập dữ liệu tóm tắt văn bản khác nhau và sử dụng hai phương pháp đánh giá, Điểm Tóm tắt Trung bình đại diện cho điểm trung bình được đưa ra bởi text-davinci-003, từ 1 đến 3, và Tóm tắt Chất lượng Cao Trung bình đại diện cho số lượng trung bình các bản tóm tắt chất lượng cao trong 10 mẫu, từ 0 đến 10. Để biết thiết lập thí nghiệm chi tiết hơn, vui lòng tham khảo Phụ lục B.

Như được hiển thị trong Bảng 5, mặc dù điểm của GENIE thấp hơn một chút so với BART, theo kết quả trong Bảng 4, đa dạng của các mẫu được sinh bởi BART thấp hơn nhiều so với GENIE. Xem xét sự đánh đổi giữa đa dạng và chất lượng, sự khác biệt điểm nằm trong phạm vi có thể chấp nhận được. Hơn nữa, kết quả của Tóm tắt Chất lượng Cao Trung bình cho thấy vẫn có đủ bản tóm tắt chất lượng cao trong trường hợp đa dạng cao. Những ưu điểm như vậy của GENIE xứng đáng được chúng tôi chú ý và khám phá thêm trong công việc tương lai.

--- TRANG 7 ---
Bảng 4. Điểm SELF-BLEU của kết quả được sinh bởi BART và GENIE. Đối với mỗi mẫu dữ liệu, chúng tôi sử dụng BART và GENIE để sinh 10 bản tóm tắt để đánh giá đa dạng.

[Bảng hiển thị kết quả cho các phương pháp sinh khác nhau trên XSUM, CNN/DAILY MAIL, và GIGAWORD]

Bảng 5. Đánh giá mô hình ngôn ngữ lớn trên ba benchmark tóm tắt.

[Bảng so sánh BART và GENIE về Điểm Tóm tắt Trung bình và Tóm tắt Chất lượng Cao Trung bình]

4.6. Tác động của Bước Tiền huấn luyện
Phương pháp tiền huấn luyện của chúng tôi và chính mô hình khuếch tán đều được thiết kế để đạt hội tụ dài hạn và tiềm năng không giới hạn, nhưng chúng cũng yêu cầu một lượng lớn thời gian tiền huấn luyện. Ở đây chúng tôi điều tra cách các bước tiền huấn luyện ảnh hưởng đến hiệu suất của mô hình so với một GENIE không tiền huấn luyện trên tập dữ liệu XSUM. Chúng tôi fine-tune các checkpoint thu được theo khoảng 1 triệu bước từ tiền huấn luyện và đánh giá chúng sử dụng 5 nhiễu Gaussian ngẫu nhiên, chọn điểm cao nhất làm kết quả cuối cùng. Như được hiển thị trong Bảng 6, tiền huấn luyện chỉ 1 triệu bước có thể cải thiện đáng kể chất lượng sinh so với GENIE không tiền huấn luyện. Hơn nữa, chúng tôi có thể thấy từ kết quả rằng tiền huấn luyện tiếp tục tăng cường hiệu suất của GENIE trên nhiệm vụ downstream một cách ổn định khi các bước tiền huấn luyện tăng.

4.7. Tác động của Tham số Tiền huấn luyện
Trong phần này, chúng tôi xem xét hiệu ứng của các tham số tiền huấn luyện quan trọng đối với hiệu suất tiền huấn luyện. Đầu tiên, trong phương pháp tiền huấn luyện không giám sát CPD, chúng tôi cần khám phá cách tỷ lệ đoạn văn liên tục α ảnh hưởng đến hiệu suất tiền huấn luyện. Chúng tôi thay đổi giá trị α từ 15% đến 40% (với khoảng 5%) và tiến hành tiền huấn luyện 2 triệu bước cho mỗi giá trị. Sau tiền huấn luyện, chúng tôi đánh giá hiệu quả tiền huấn luyện bằng cách fine-tuning trên XSUM. Để đánh giá nghiêm ngặt, chúng tôi lấy mẫu 5 nhiễu Gaussian, lặp lại thí nghiệm 5 lần với các seed ngẫu nhiên khác nhau, và báo cáo trung bình và độ lệch chuẩn của kết quả, mỗi lần chọn điểm cao nhất làm kết quả cuối cùng. Như được hiển thị trong Bảng 7, các giá trị α quá lớn hoặc quá nhỏ dẫn đến sự không ổn định và hiệu suất kém của mô hình tiền huấn luyện. Tiền huấn luyện ổn định và hiệu quả hơn khi α = 30%.

Thứ hai, chúng tôi điều tra phương pháp lấy mẫu bước thời gian được sử dụng trong tiền huấn luyện. Trước mỗi bước huấn luyện, chúng tôi cần lấy mẫu một bước thời gian như một phần của đầu vào mô hình. Hai phương pháp lấy mẫu bước thời gian phổ biến hiện có là lấy mẫu đồng nhất và lấy mẫu nhận biết loss. Phương pháp đầu gán xác suất bằng nhau cho mỗi bước thời gian, trong khi phương pháp sau cập nhật trọng số lấy mẫu theo loss huấn luyện, để các bước thời gian quan trọng hơn có cơ hội được lấy mẫu cao hơn. Trong thí nghiệm, chúng tôi sử dụng hai phương pháp lấy mẫu này, kiểm tra chúng trên hai giá trị α khác nhau (15% và 30%), và thực hiện đánh giá nghiêm ngặt tương tự như thí nghiệm trước. Như được hiển thị trong Bảng 8, chúng tôi quan sát rằng dưới 2 triệu bước tiền huấn luyện, lấy mẫu đồng nhất vượt trội hơn lấy mẫu nhận biết loss cho các giá trị α khác nhau. Trực quan, mặc dù lấy mẫu nhận biết loss có thể tăng tốc hội tụ của mô hình khuếch tán, chúng tôi hy vọng rằng mô hình có thể học kiến thức đầy đủ ở mỗi bước thời gian trong quá trình tiền huấn luyện, để nó có thể hội tụ nhanh hơn và hoạt động tốt hơn trên các nhiệm vụ downstream.

4.8. Tác động của Bước Thời gian Khuếch tán
Số lượng bước thời gian khuếch tán có tác động lớn đến chất lượng sinh. Chúng tôi khám phá cách GENIE hoạt động dưới các số lượng bước khuếch tán nghịch khác nhau trên tập dữ liệu XSUM. Giả sử tổng số bước khuếch tán T=2000, chúng tôi đặt bước khoảng của khuếch tán nghịch là 1, 2, 4, 8, 20, và số lượng bước khuếch tán nghịch tương ứng là 2000, 1000, 500, 250, 100. Trong thí nghiệm này, chúng tôi lấy mẫu 5 nhiễu Gaussian và chọn kết quả khử nhiễu tốt nhất. Như được hiển thị trong Hình 2, chúng tôi có thể thấy rõ rằng khi số lượng bước khuếch tán nghịch nhỏ, chất lượng sinh với GENIE giảm đáng kể. Khi số lượng bước khuếch tán nghịch tăng lên 1000, chất lượng sinh của GENIE trở nên ổn định.

5. Công trình Liên quan
5.1. Mô hình Ngôn ngữ Tiền huấn luyện Quy mô Lớn
Gần đây, một bước đột phá lớn đã được thực hiện trong mô hình tiền huấn luyện trên kho dữ liệu quy mô lớn. Là các mô hình ngôn ngữ một chiều, GPT (Radford et al., 2018), GPT2 (Radford et al., 2019) mô hình hóa văn bản dựa trên từ trái sang phải, và dự đoán token tiếp theo theo token xuất hiện bên trái. Đồng thời, các mô hình ngôn ngữ hai chiều, sử dụng bộ mã hóa hai chiều để mô hình hóa văn bản, có thể thu được biểu diễn nhạy cảm ngữ cảnh tốt hơn, như BERT (Devlin et al., 2019) và RoBERT (Liu et al., 2019). RoBERT tối ưu hóa các nhiệm vụ tiền huấn luyện so với BERT, cả hai đều cải thiện đáng kể khả năng hiểu ngôn ngữ tự nhiên.

Để cải thiện hiệu suất của mô hình tiền huấn luyện quy mô lớn trong sinh ngôn ngữ tự nhiên, một số công trình đã thiết kế các nhiệm vụ tiền huấn luyện dựa trên khung chuẩn của chuỗi-sang-chuỗi. MASS (Song et al., 2019) để mô hình dự đoán đoạn token bị che ngắn từng bước, trong khi ProphetNet (Qi et al., 2020) dự đoán nhiều từ hơn trong mỗi bước để giảm over fitting cục bộ.

5.2. Mô hình Khuếch tán cho Văn bản
Trong những năm gần đây, mô hình khuếch tán đã đạt được thành công lớn trong các lĩnh vực sinh ảnh (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022). Do chất lượng sinh tuyệt vời của nó, một số công trình áp dụng mô hình khuếch tán trong lĩnh vực sinh văn bản. DiffusionLM (Li et al., 2022b) ánh xạ các token rời rạc vào biến tiềm ẩn liên tục, đạt được sinh văn bản có thể kiểm soát phức tạp hơn thông qua khuếch tán liên tục. Trong lĩnh vực sửa đổi văn bản nơi phương pháp không tự hồi quy được sử dụng rộng rãi, DiffusER (Reid et al., 2022) cũng sử dụng mô hình khuếch tán để thực hiện các quá trình sinh dựa trên chỉnh sửa. DiffuSeq (Gong et al., 2022) đạt được sinh văn bản có điều kiện với một phương pháp mới trong đó thông tin kiểm soát cũng được tham gia vào quá trình khuếch tán. Khác với công trình trên, chúng tôi xây dựng một mô hình ngôn ngữ mới dựa trên mô hình khuếch tán lần đầu tiên, sử dụng khung mã hóa-giải mã chuẩn. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên áp dụng tiền huấn luyện quy mô lớn trên mô hình ngôn ngữ dựa trên mô hình khuếch tán.

6. Kết luận
Trong bài báo này, chúng tôi đã trình bày một mô hình ngôn ngữ khuếch tán mới GENIE, tận dụng kho dữ liệu quy mô lớn cho tiền huấn luyện. Mô hình của chúng tôi áp dụng khung chuỗi-sang-chuỗi, trong đó một bộ mã hóa hai chiều mã hóa chuỗi nguồn và một bộ giải mã khử nhiễu dự đoán và loại bỏ nhiễu khỏi chuỗi đích theo cách không tự hồi quy. Thiết kế này cho phép chúng tôi sinh văn bản đa dạng bằng cách dần dần tinh chỉnh đầu ra từ một trạng thái nhiễu ban đầu. Hơn nữa, chúng tôi đã giới thiệu một phương pháp tiền huấn luyện mới gọi là khử nhiễu đoạn văn liên tục, nhằm khử nhiễu toàn bộ đoạn văn như chuỗi đích. Các thí nghiệm của chúng tôi trên các nhiệm vụ NLG khác nhau chứng minh rằng GENIE có thể tạo ra văn bản chất lượng cao và đa dạng, và xác thực lợi ích của việc tiền huấn luyện mô hình khuếch tán của chúng tôi trên kho dữ liệu quy mô lớn.
