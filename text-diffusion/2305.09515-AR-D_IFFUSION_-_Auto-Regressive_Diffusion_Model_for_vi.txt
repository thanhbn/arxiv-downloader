# 2305.09515.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/text-diffusion/2305.09515.pdf
# Kích thước tệp: 1403180 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
AR-DIFFUSION: Mô hình Khuếch tán Tự hồi quy cho
Sinh văn bản
Tong Wu1*†, Zhihao Fan2*†, Xiao Liu3, Yeyun Gong3‡, Yelong Shen4, Jian Jiao5,
Hai-Tao Zheng1,8‡Juntao Li6, Zhongyu Wei2, Jian Guo7, Nan Duan3‡, Weizhu Chen4‡
1Trường Cao học Quốc tế Thâm Quyến, Đại học Thanh Hoa,2Đại học Phúc Đán,
3Microsoft Research Châu Á,4Microsoft Azure AI, Redmond,5Microsoft,
6Đại học Tô Châu,7IDEA Research
{yegong, yeshe, nanduan, wzchen} @microsoft.com,
zheng.haitao @sz.tsinghua.edu.cn,

Tóm tắt
Các mô hình khuếch tán đã thu hút sự chú ý đáng kể trong lĩnh vực sinh ảnh nhờ hiệu suất vượt trội của chúng. Thành công của chúng gần đây đã được mở rộng sang sinh văn bản thông qua việc sinh tất cả các token trong một chuỗi đồng thời. Tuy nhiên, ngôn ngữ tự nhiên thể hiện sự phụ thuộc tuần tự rõ rệt hơn nhiều so với hình ảnh, và phần lớn các mô hình ngôn ngữ hiện có được huấn luyện theo cách tiếp cận tự hồi quy từ trái sang phải. Để tính đến đặc tính tuần tự vốn có của ngôn ngữ tự nhiên, chúng tôi giới thiệu Khuếch tán Tự hồi quy (AR-DIFFUSION). AR-DIFFUSION đảm bảo rằng việc sinh token ở bên phải phụ thuộc vào những token đã được sinh ở bên trái, một cơ chế đạt được thông qua việc sử dụng số lượng bước khử nhiễu động thay đổi dựa trên vị trí token. Điều này dẫn đến việc các token ở bên trái trải qua ít bước khử nhiễu hơn so với những token ở bên phải, từ đó cho phép chúng sinh sớm hơn và sau đó ảnh hưởng đến việc sinh các token ở bên phải. Trong một loạt thí nghiệm trên các tác vụ sinh văn bản khác nhau, bao gồm tóm tắt văn bản, dịch máy và sinh thông thường, AR-DIFFUSION đã thể hiện rõ ràng sự vượt trội so với các mô hình ngôn ngữ khuếch tán hiện có và có thể nhanh hơn 100× ∼ 600× khi đạt được kết quả tương đương. Mã nguồn của chúng tôi có sẵn tại https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.

1 Giới thiệu
Sinh văn bản là một nhiệm vụ cơ bản trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP). Các mô hình ngôn ngữ được tiền huấn luyện như GPT-4 [OpenAI, 2023], LLaMA [Touvron et al., 2023], và Alpaca [Taori et al., 2023] đã thu hút sự chú ý đáng kể với khả năng sinh nội dung văn bản trôi chảy và giống con người. Các mô hình này sử dụng bộ giải mã Transformer tự hồi quy (AR) [Vaswani et al., 2017] để phát ra các token được sinh từng cái một theo thứ tự tuần tự từ trái sang phải. Bằng cách tận dụng sức mạnh của sự phụ thuộc vị trí, các mô hình AR có thể tăng cường tính tự nhiên, mạch lạc và tuân thủ các quy ước ngôn ngữ con người trong văn bản được sinh [Brown et al., 2020].

Các nghiên cứu gần đây đã chỉ ra hiệu suất đáng chú ý của các mô hình khuếch tán trong sinh ảnh [Ho et al., 2020], thúc đẩy các nhà nghiên cứu mở rộng khuếch tán sang sinh văn bản [Li et al., 2022a, Gong et al., 2022, Dieleman et al., 2022, Yuan et al., 2022, Ye et al., 2023]. Bằng cách giới thiệu timestep, các phương pháp này dần dần điều chỉnh việc nội suy giữa các token gốc và nhiễu Gaussian, sau đó

*Công việc được thực hiện trong thời gian thực tập tại Microsoft Research Châu Á.
†Các tác giả này đóng góp ngang nhau cho công việc này.
‡Tác giả liên hệ.
Bản thảo. Đang được xem xét.arXiv:2305.09515v3 [cs.CL] 13 Dec 2023

--- TRANG 2 ---
[Hình ảnh mô tả các hệ tọa độ và mô hình khác nhau]

Hình 1: Hành vi mô hình được minh họa trên hệ tọa độ hai chiều, trong đó trục ngang đại diện cho vị trí và trục dọc biểu thị timestep khuếch tán. Trong giai đoạn suy luận, các mô hình khác nhau sẽ hoạt động khác nhau. (a) Đối với Diffusion-LM điển hình [Li et al., 2022a], mỗi token chia sẻ tốc độ di chuyển giống hệt nhau v(n1, ti, ti+1) =v(n2, ti, ti+1) =|ti+1−ti|. (b) Đối với AR từ góc độ của các mô hình khuếch tán, các token có hai trạng thái dựa trên mức độ nội suy giữa các token gốc và nhiễu Gaussian: sẽ được giải mã (tại timestep t=T) và đã được giải mã (tại timestep t= 0). Cụ thể, chúng ta có v(n1, ti, ti+1) = 0 và v(n2, ti, ti+1) =T. (c) Trong AR-DIFFUSION, (ne, te) là tọa độ của điểm neo. Các token ở các vị trí khác nhau thể hiện tốc độ di chuyển khác nhau, chẳng hạn như v(n1, ti, ti+1)> v(n2, ti, ti+1) khi n1< n2.

khử nhiễu lặp đi lặp lại để sinh văn bản. Tại mỗi timestep, bộ sinh văn bản dựa trên khuếch tán dự đoán tất cả các token đồng thời theo hướng Không Tự hồi quy (NAR) [Lewis et al., 2020, Qi et al., 2020, 2021, Li et al., 2022b], dẫn đến tốc độ giải mã nhanh hơn so với AR. Tuy nhiên, nó cũng kế thừa nhược điểm của NAR, cụ thể là hy sinh sự phụ thuộc vị trí giữa các token [Li et al., 2022c] và sự sụt giảm hiệu suất sinh [Bao et al., 2021].

Để tiến hành phân tích toàn diện, chúng tôi giới thiệu hệ tọa độ hai chiều để theo dõi timestep khuếch tán của các token f(·) được định vị tại các vị trí khác nhau. Như được minh họa trong Hình 1, hệ thống gán vị trí token n trong [1, N] cho trục ngang và timestep khuếch tán t trong [0, T] cho trục dọc. Diffusion-LM [Li et al., 2022a], được các mô hình sinh văn bản dựa trên khuếch tán hiện có tuân theo, được hiển thị trong Hình 1(a). Nó gán timestep t đồng nhất cho tất cả các token. Ngược lại, các token trong mô hình AR được mô tả trong Hình 1(b) thể hiện các timestep riêng biệt trong một bước sinh (ti). Ví dụ, token đã được giải mã tại vị trí n1 có timestep là 0, trong khi token sẽ được giải mã tại vị trí n2 có timestep là T. Cách tiếp cận này hiệu quả nắm bắt sự phụ thuộc tuần tự.

Được thúc đẩy bởi quan sát này, chúng tôi giới thiệu AR-DIFFUSION, một phương pháp khuếch tán tự hồi quy, cho sự khác biệt trong vị trí token và nguyên tắc nhận dạng token tuần tự.

Trong AR-DIFFUSION, chúng tôi đề xuất chiến lược khuếch tán đa cấp bao gồm cả khuếch tán cấp câu và cấp token. Chúng tôi ngẫu nhiên chọn timestep cấp câu t, và gán tốc độ di chuyển động v(·) bằng cách xác định timestep cấp token nhạy cảm với vị trí f(n, t) cho mỗi token. Điều này cho phép các token ở bên trái của câu trải qua di chuyển nhanh hơn từ nhiễu Gaussian ngẫu nhiên đến embedding token, trong khi những token ở bên phải của câu trải qua di chuyển chậm hơn để tận dụng tốt hơn thông tin từ các token đã được khử nhiễu trước đó. Trong quá trình suy luận, để giảm số lượng bước suy luận đáng kể (ví dụ: 2,000) cần thiết trong Diffusion-LM [Li et al., 2022a], SeqDiffSeq [Yuan et al., 2022] và GENIE [Lin et al., 2023], chúng tôi giới thiệu cơ chế bỏ qua cộng tác với chiến lược khuếch tán đa cấp để tăng tốc quá trình.

Kết quả thí nghiệm trên các tác vụ sinh văn bản khác nhau, như tóm tắt văn bản, dịch máy và sinh thông thường, đã liên tục chứng minh rằng AR-DIFFUSION vượt trội hơn các mô hình khuếch tán văn bản hiện có, bao gồm cả các phương pháp AR về cả chất lượng và tính đa dạng. Hơn nữa, việc xác minh của chúng tôi cho thấy AR-DIFFUSION yêu cầu ít tài nguyên hơn trong quá trình giải mã trong khi duy trì hiệu suất vượt trội. Nó đạt được nhanh hơn 100× so với SeqDiffSeq [Yuan et al., 2022] trong dịch máy và nhanh hơn 600× so với GENIE [Lin et al., 2023] trong tóm tắt văn bản trong khi mang lại kết quả tương đương. Hơn nữa, nó thể hiện kết quả đầy hứa hẹn ngay cả trong tình huống thách thức khi giải mã bị giới hạn chỉ ở hai bước.

--- TRANG 3 ---
2 Kiến thức cơ bản

2.1 Mô hình Ngôn ngữ Sinh có Điều kiện
Trong lĩnh vực sinh ngôn ngữ tự nhiên, các mô hình sinh có điều kiện thường được triển khai bằng cách sử dụng phương pháp tự hồi quy (AR) hoặc không tự hồi quy (NAR). Trong AR [Vaswani et al., 2017], các token ở bên phải được dự đoán dựa trên các token bên trái có thể nhìn thấy. Khả năng xảy ra được cho bởi pAR(y|x) =∏Ni=1p(yi|y1:i−1;x), trong đó yi biểu thị token thứ i của y. Mặt khác, NAR [Gu et al., 2017] giả định tính độc lập có điều kiện giữa các token và sinh chúng đồng nhất mà không có sự phân biệt trong quá trình giải mã, dẫn đến khả năng xảy ra pNAR(y|x) =∏Ni=1p(yi|x). Cách tiếp cận sinh song song này có chất lượng thấp hơn so với AR, mặc dù nó mang lại lợi thế tốc độ đáng kể.

2.2 Mô hình Khuếch tán cho Sinh Văn bản
Gần đây, Li et al. [2022a] đề xuất mô hình sinh ngôn ngữ tự nhiên dựa trên quá trình khuếch tán, thường được chia thành quá trình thêm nhiễu tiến và quá trình khử nhiễu ngược. Cụ thể, quá trình tiến là mô hình Gaussian tuyến tính cố định, dần dần làm nhiễu biến ngẫu nhiên z0 cho đến khi nó trở thành phân phối Gaussian chuẩn. Điều này có thể được hình thức hóa như:
q(zt|z0;x) =N(zt;√α̅tz0,(1−α̅t)I), (1)
trong đó, α̅t=∏ti=1αi, và αi là hệ số giảm đơn điệu theo timestep t, zt là trạng thái tiềm ẩn tại timestep t.

Quá trình ngược là bắt đầu từ nhiễu Gaussian chuẩn và dần dần sử dụng quá trình chuyển đổi khử nhiễu pθ(zt−1|zt;x) để sinh.
pθ(zt−1|zt;x) =N(zt−1;μθ(zt, t;x),Σθ(zt, t;x)), (2)
trong đó trung bình μθ và phương sai Σθ được học từ mô hình. Cụ thể, chúng tôi tuân theo cách tiếp cận của Li et al. [2022a] sử dụng phương sai được định nghĩa trước mà không có tham số có thể huấn luyện.

Để mở rộng quá trình khuếch tán liên tục sang sinh văn bản rời rạc, Li et al. [2022a] giới thiệu quá trình chuyển đổi Markov bổ sung từ các token rời rạc y đến biến tiềm ẩn z0. Trong thực tế, chúng tôi thêm bước embedding qφ(z0|y) =N(z0; Emb(y),(1−α0)I) trong quá trình tiến, và sử dụng bước làm tròn có thể huấn luyện được tham số hóa bởi pθ(y|z0;x) =∏Ni=1pθ(yi|zi0;x) trong quá trình ngược. Tại mỗi timestep, chúng tôi sử dụng mô hình encoder-decoder gθ(zt, t;x) để xấp xỉ z0 [Lin et al., 2023] theo cách NAR và sau đó ước tính μθ(zt, t;x).

Do đó, kết hợp với việc tối đa hóa cận dưới bằng chứng (ELBO) của logpθ(y|x), mục tiêu huấn luyện của mô hình ngôn ngữ khuếch tán có điều kiện của chúng tôi là:
L=Eqφ(z0:T|y)[−logpθ(y|z0;x) +∑Tt=1∥z0−gθ(zt, t;x)∥2]. (3)

3 Phương pháp

3.1 Khuếch tán Đa cấp
Trong quá trình khuếch tán điển hình, mọi token trong chuỗi văn bản có cùng timestep khuếch tán. Để tận dụng bản chất tuần tự của ngôn ngữ, chúng tôi cho phép các token có các timestep khuếch tán khác nhau trong quá trình tiến và ngược. Để thực hiện điều này, chúng tôi đề xuất chiến lược khuếch tán đa cấp bao gồm cả khuếch tán cấp câu và cấp token.

Thứ nhất, ở cấp câu, chúng tôi tuân theo Diffusion-LM [Li et al., 2022a] để ngẫu nhiên chọn timestep t. Thứ hai, ở cấp token, chúng tôi kết hợp thông tin vị trí n trong [1, N] dựa trên timestep cấp câu để điều chỉnh timestep khuếch tán cho token hiện tại. Quy trình được minh họa như:
zt={z1f(1,t),z2f(2,t),···,zNf(N,t)}, (4)
trong đó N là độ dài câu đích cho trước, zt là biểu diễn câu tại timestep t, znf(n,t) là biểu diễn tiềm ẩn cho token thứ n tại timestep cấp câu t, và f(n, t) là hàm timestep cấp token biểu thị timestep khuếch tán cấp token được xác định bởi vị trí token n và timestep cấp câu t.

Chúng tôi trực quan hóa timestep cấp token {n, f(n, t)} lên hệ tọa độ hai chiều như Hình 1, lấy vị trí token làm trục ngang và timestep cấp câu làm trục dọc. Hơn nữa, để cung cấp mô tả sâu sắc hơn về đặc tính của chuyển động, chúng tôi định nghĩa tốc độ chuyển động như phương trình sau.
v(n, ti, ti+1) =f(n, ti+1)−f(n, ti), (5)
trong đó ti và ti+1 là timestep khuếch tán cấp câu bắt đầu và kết thúc. Có thể quan sát thấy rằng các token trong Diffusion-LM chia sẻ cùng tốc độ chuyển động, trong khi những token trong AR thể hiện tốc độ khác nhau.

3.2 Khuếch tán Cấp Token với Tốc độ Chuyển động Động
Dựa trên tốc độ chuyển động, chúng tôi đề xuất nguyên tắc cơ bản, tốc độ chuyển động động, để thiết kế hàm timestep khuếch tán cấp token f(n, t) nhằm tận dụng AR trong khuếch tán. Cụ thể, các phần tử ở phía bên trái của câu trải qua tốc độ chuyển động cao hơn từ nhiễu Gaussian ngẫu nhiên đến embedding token, trong khi những phần tử ở phía bên phải trải qua tốc độ chuyển động thấp hơn, do đó chúng có thể được sinh trong timestep cấp câu sau và sử dụng thông tin từ các token đã được sinh trước đó hiệu quả hơn.

[Thuật toán 1 tiếp tục với các bước huấn luyện chi tiết...]

--- TRANG 4 ---
[Tiếp tục với nội dung thuật toán và phương pháp...]

3.3 Suy luận với Bỏ qua
Thông thường, quá trình sinh cần phải đi qua tất cả các timestep cấp câu từ T+N đến 0. Để giảm thời gian giải mã, chúng tôi giới thiệu cơ chế bỏ qua cho phép chúng tôi duyệt qua một tập con của các timestep.

Để đảm bảo tính nhất quán giữa huấn luyện và suy luận, chúng tôi cũng cần tính toán timestep cho mỗi token trong quá trình suy luận. Do đó, chúng tôi trước tiên thiết lập một điểm neo, sau đó đồng nhất chọn một dãy con giảm dần {ti}Mi=0 từ tất cả các timestep (T+N đến 0). Số lượng của dãy này là tổng số bước giải mã M (M≪T+N). Ví dụ, giả sử khoảng cách là 500 và T+N là 2500, thì M là 5, và dãy con là [2500, 2000, 1500, 1000, 500, 0].

[Thuật toán 2 tiếp tục với quy trình suy luận chi tiết...]

--- TRANG 5 ---
[Tiếp tục với các công thức toán học và quy trình suy luận...]

4 Thí nghiệm

4.1 Tác vụ và Tập dữ liệu

Tóm tắt Văn bản Tác vụ này bao gồm lấy một tài liệu dài làm đầu vào và sinh một câu ngắn gọn làm đầu ra. Điều này đòi hỏi các mô hình có khả năng xác định nội dung quan trọng và viết lại nó dưới dạng cô đọng. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng XSUM [Narayan et al., 2018] và CNN/DAILY MAIL [Hermann et al., 2015] có sẵn công khai trên GLGE, còn được gọi là GLGE-Easy.

Dịch Máy Dịch thuật là một tác vụ chuỗi-đến-chuỗi được sử dụng rộng rãi. Đầu vào là một chuỗi từ trong ngôn ngữ nguồn, và đầu ra là một chuỗi từ tương ứng trong ngôn ngữ đích. Chúng tôi chọn tập dữ liệu IWSLT 2014 và phương pháp xử lý dữ liệu là tuân theo các script được cung cấp bởi fairseq.

Sinh Thông thường Trong tác vụ này, mô hình được cung cấp một tập khái niệm bao gồm các đối tượng và hành động làm đầu vào. Mục tiêu là sinh một câu kết hợp các khái niệm này và mô tả một tình huống thực tế. Chúng tôi sử dụng tập dữ liệu COMMONGEN để đánh giá.

4.2 Chi tiết Thí nghiệm

Thiết lập Mô hình Cấu hình mô hình của chúng tôi được triển khai dựa trên Transformer-base [Vaswani et al., 2017]. Cụ thể, đối với XSUM và CNN/DAILY MAIL, chúng tôi đặt chiều embedding khuếch tán là 128. Đối với IWSLT14, chúng tôi sử dụng embedding khuếch tán 64 chiều, 4 attention head và các lớp feed-forward 1024 chiều. Đối với COMMONGEN, chúng tôi áp dụng embedding khuếch tán 64 chiều, 8 attention head và các lớp feed-forward 512 chiều.

Huấn luyện và Suy luận Trong giai đoạn huấn luyện, chúng tôi sử dụng lịch nhiễu căn bậc hai và 2,000 bước khuếch tán [Li et al., 2022a]. Đặc biệt, chúng tôi sử dụng tokenizer và từ vựng được xây dựng bởi Byte Pair Encoding (BPE) [Kudo và Richardson, 2018] cho các tác vụ dịch thuật. Đối với các tác vụ khác, chúng tôi áp dụng tokenizer và từ vựng của bert-base-uncased.

Mô hình Chuẩn Chúng tôi đặt bốn nhóm mô hình chuẩn:
• NAR: NAT [Gu et al., 2017], iNAT [Lee et al., 2018], CMLM [Ghazvininejad et al., 2019], LevT [Gu et al., 2019] và CNAT [Bao et al., 2021];
• Semi-NAR: InsT [Stern et al., 2019], iNAT [Lee et al., 2018], CMLM [Ghazvininejad et al., 2019] và LevT [Gu et al., 2019];
• AR: bRNN [Gu et al., 2016], LSTM [Greff et al., 2017] và Transformer [Vaswani et al., 2017];
• Khuếch tán: DiffusionLM [Li et al., 2022a], CDCD [Dieleman et al., 2022], SeqDiffuSeq [Yuan et al., 2022], DINOISER [Ye et al., 2023] và GENIE [Lin et al., 2023].

Chỉ số Chúng tôi tuân theo cách tiếp cận của Qi et al. [2020] để đánh giá ROUGE-1/2/L của tác vụ tóm tắt. Để đánh giá các tác vụ dịch thuật, chúng tôi áp dụng thiết lập của SeqDiffuSeq [Yuan et al., 2022] để báo cáo điểm BLEU. Ngoài ra, chúng tôi cũng tính toán điểm SacreBLEU theo thiết lập của DINOISER [Ye et al., 2023] để so sánh. Đối với COMMONGEN, chúng tôi sử dụng ROUGE-2/L, BLEU-3/4, METEOR và SPICE theo các phương pháp đánh giá của Lin et al. [2020].

Tham số Huấn luyện Các tham số huấn luyện của chúng tôi trên các tập dữ liệu khác nhau được hiển thị trong Bảng 1. Các bước khởi động lịch tuyến tính của chúng tôi là 4,000 ×Ngc, trong đó Ngc biểu thị số tích lũy gradient. Ngoài ra, chúng tôi sử dụng bộ tối ưu AdamW (weight decay = 0.0) và dropout là 0.2. Tất cả các thí nghiệm được triển khai trên 8 Tesla V100-32G. Mất khoảng 20 giờ để huấn luyện XSUM và CNN/DAILY MAIL, khoảng 5 giờ để huấn luyện IWSLT14, và khoảng 2 giờ để huấn luyện COMMONGEN.

--- TRANG 6 ---
[Tiếp tục với các bảng kết quả và phân tích chi tiết...]

--- TRANG 7 ---
[Các bảng kết quả thí nghiệm chi tiết...]

--- TRANG 8 ---
[Tiếp tục với kết quả và phân tích hiệu quả...]

--- TRANG 9 ---
[Phân tích đa dạng và nghiên cứu loại bỏ...]

--- TRANG 10 ---
[Nghiên cứu trường hợp và hình ảnh minh họa...]

--- TRANG 11 ---
[Tác động của các yếu tố và công trình liên quan...]

--- TRANG 12 ---
[Kết luận và hạn chế...]

--- TRANG 13-18 ---
[Tài liệu tham khảo và phụ lục...]
