# 2406.16330.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/merging/2406.16330.pdf
# Kích thước tệp: 2433042 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 1
Cắt tỉa thông qua Kết hợp: Nén LLM thông qua
Kết hợp Lớp dựa trên Căn chỉnh Đa tạp
Deyuan Liu∗, Zhanyue Qin∗, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu,
Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, và Dianbo Sui.
Tóm tắt —Mặc dù các mô hình ngôn ngữ lớn (LLM) xuất sắc trong nhiều lĩnh vực, độ phức tạp và quy mô của chúng gây thách thức cho việc triển khai trong
môi trường có tài nguyên hạn chế. Các kỹ thuật nén hiện tại, chẳng hạn như cắt tỉa tham số, thường không thể sử dụng hiệu quả kiến thức từ các tham số đã bị cắt tỉa.
Để giải quyết những thách thức này, chúng tôi đề xuất Nén Căn chỉnh Kiến thức dựa trên Đa tạp và Kết hợp Lớp (MKA), một phương pháp mới sử dụng học đa tạp
và thước đo Cổ chai Thông tin (IB) để kết hợp các lớp tương tự, giảm kích thước mô hình trong khi bảo toàn hiệu suất thiết yếu. Chúng tôi đánh giá MKA trên nhiều
bộ dữ liệu điểm chuẩn và các LLM khác nhau. Các phát hiện của chúng tôi cho thấy MKA không chỉ bảo toàn hiệu suất mô hình mà còn đạt được tỷ lệ nén đáng kể,
vượt trội hơn các phương pháp cắt tỉa truyền thống. Hơn nữa, khi kết hợp với lượng tử hóa, MKA mang lại mức nén thậm chí còn lớn hơn.
Cụ thể, trên bộ dữ liệu MMLU sử dụng mô hình Llama3-8B, MKA đạt được tỷ lệ nén 43.75% với sự giảm hiệu suất tối thiểu chỉ 2.82%. Phương pháp MKA đề xuất
cung cấp một kỹ thuật nén mô hình tiết kiệm tài nguyên và bảo toàn hiệu suất cho LLM. Chúng tôi cung cấp mã nguồn tại https://github.com/SempraETY/Pruning-via-Merging
Từ khóa Chỉ mục —Nén Mô hình, Kết hợp Lớp, Học Đa tạp, Mô hình Ngôn ngữ Lớn (LLM)
✦
1 GIỚI THIỆU
CÁC Mô hình Ngôn ngữ Lớn (LLM), chẳng hạn như GPT-4 [1],
Llama-3 [2], Llama-2 [3] và Mistral [4], đã thể hiện
khả năng xuất sắc trong hiểu và tạo ngôn ngữ. Những mô hình này,
với hàng tỷ tham số được huấn luyện trên hàng nghìn tỷ token, có thể
xử lý các tác vụ phức tạp và thể hiện khả năng nổi lên [5], [6]. Mặc dù
những mô hình này đã đạt được thành công chưa từng có, sự phức tạp
và quy mô ngày càng tăng của chúng đã đưa ra những thách thức đáng kể về
•Deyuan Liu, Zhanyue Qin, Zecheng Wang, Zhiying Tu, Dianhui Chu,
và Dianbo Sui thuộc Viện Công nghệ Harbin, Harbin, Trung Quốc.
E-mail: 2022211994@stu.hit.edu.cn; 2021211875@stu.hit.edu.cn;
22s130467@stu.hit.edu.cn; tzy_hit@hit.edu.cn; chudh@hit.edu.cn;
suidianbo@hit.edu.cn
•Hairu Wang thuộc Đại học Khoa học và Công nghệ Trung Quốc,
Hefei, Trung Quốc.
E-mail: hrwang00@mail.ustc.edu.cn
•Zhao Yang thuộc Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc,
Bắc Kinh, Trung Quốc.
E-mail: zhao.yang@nlpr.ia.ac.cn
•Fangying Rong thuộc Đại học Nông nghiệp Shandong, Shandong,
Trung Quốc.
E-mail: rongfangying@gmail.com
•Qingbin Liu, Xi Chen thuộc Tencent Inc., Thâm Quyến, Trung Quốc.
E-mail: qingbinliu@tencent.com; marshao@tencent.com; ryan-
bli@tencent.com; jasonxchen@tencent.com.
•Cunhang Fan, Zhao Lv thuộc Đại học Anhui, Anhui, Trung Quốc.
E-mail: cunhang.fan@ahu.edu.cn; kjlz@ahu.edu.cn.
•∗Những tác giả này đóng góp ngang nhau cho công trình này.
•Dianbo Sui là tác giả liên hệ.
Công trình này đã được nộp cho IEEE để có thể xuất bản. Bản quyền
có thể được chuyển giao mà không cần thông báo, sau đó phiên bản này có thể không còn
truy cập được.tài nguyên tính toán, yêu cầu bộ nhớ,
và tiêu thụ năng lượng [7], [8], làm dấy lên mối lo ngại về
tính bền vững của chúng.
Để giảm thiểu những thách thức này, các nhà nghiên cứu đã phát triển
các kỹ thuật nén mô hình khác nhau trong LLM để giảm
kích thước tham số của nó trong khi bảo toàn hiệu suất [9], [10],
[11], [12], [13]. Những kỹ thuật này có thể được phân loại thô thành
hai dòng chính [14]: lượng tử hóa [15], [16], [16],
[17], [18] và cắt tỉa [19], [20], [21], [22]. Các phương pháp dựa trên lượng tử hóa
hỗ trợ giảm tiêu thụ bộ nhớ của trọng số, kích hoạt, và bộ đệm KV bằng cách sử dụng
các giá trị có độ chính xác thấp với ít bit hơn thay vì các giá trị có độ chính xác cao.
Tuy nhiên, lợi ích tăng tốc của lượng tử hóa phụ thuộc nghiêm trọng vào hỗ trợ phần cứng [23]
và đôi khi cần tinh chỉnh bổ sung để duy trì
hiệu suất [14], [24]. So với lượng tử hóa, cắt tỉa,
đặc biệt là cắt tỉa có cấu trúc [25], loại bỏ các tham số LLM dư thừa
để giảm tổng số tham số,
và có thể được áp dụng trực tiếp cho LLM đã được huấn luyện mà không cần
huấn luyện lại và thường thân thiện với phần cứng hơn so với
các phương pháp lượng tử hóa. Mặc dù hiệu quả, cắt tỉa thường
có nguy cơ mất các cấu trúc mô hình có giá trị và việc xác định cách
cắt tỉa LLM với sự gián đoạn tối thiểu đối với gốc
vẫn là một vấn đề chưa được giải quyết [26].
Để giải quyết vấn đề này một cách trực diện, chúng tôi khám phá lĩnh vực
kết hợp mô hình [27], một kỹ thuật mạnh mẽ kết hợp một cách liền mạch
những điểm mạnh và kiến thức của nhiều mô hình, tạo ra một tập hợp mạnh mẽ và hiệu quả.
Kỹ thuật này, thông qua việc tính trung bình các trọng số của nhiều mô hình
với cùng kiến trúc, có thể giữ lại các đặc trưng thiết yếu
mà không cần tài nguyên bổ sung đáng kể [28], [29]. Hơn nữa,
bằng cách bù đắp các thiên vị và lỗi của các mô hình riêng lẻ,
kết hợp mô hình thường dẫn đến cải thiện hiệu suất rất lớn [30].
Bổ sung, số lượng mô hình trong quá trình kết hợp có thể được giảm dần một cách tự nhiên.arXiv:2406.16330v2  [cs.CL]  17 Tháng 5 Năm 2025

--- TRANG 2 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 2
Hình 1. Khung công tác Căn chỉnh Kiến thức dựa trên Đa tạp và Kết hợp Lớp (MKA) bao gồm hai thành phần chính: (1) Phía bên trái minh họa
học đa tạp để trích xuất kiến thức LLM, trong đó các kích hoạt lớp được chuyển đổi thành các đa tạp có chiều thấp bằng thuật toán Diffusion Kernel.
(2) Phía bên phải mô tả quá trình kết hợp lớp dựa trên độ tương tự, sử dụng thước đo IB để xác định các lớp có kiến thức được căn chỉnh.
Tuy nhiên, công nghệ hữu ích như vậy hiện tại bị giới hạn trong việc kết hợp
giữa các mô hình, và ít nghiên cứu chú ý đến việc kết hợp
các cấu trúc nội bộ giống nhau trong một mô hình.
Điều này đặt ra câu hỏi liệu việc nén mô hình
có thể đạt được bằng cách giảm tổng số lớp
thông qua việc tổng hợp kiến thức tiến bộ giữa các lớp hay không.
Để trả lời câu hỏi này, chúng tôi giới thiệu Nén Căn chỉnh Kiến thức
dựa trên Đa tạp và Kết hợp Lớp (MKA) trong bài báo này.
MKA kết hợp học đa tạp và kết hợp lớp để bảo toàn thông tin thiết yếu
trong khi giảm đáng kể kích thước tham số LLM. Như được minh họa
trong Hình 1, phương pháp của chúng tôi chủ yếu bao gồm hai thành phần chính:
Học Đa tạp cho Kiến thức LLM: Chúng tôi sử dụng
các kỹ thuật học đa tạp để căn chỉnh kiến thức qua các lớp
bằng cách trích xuất các kích hoạt lớp từ LLM và
áp dụng thuật toán Diffusion Kernel [31] để học các biểu diễn
đa tạp có chiều thấp. Phương pháp này nắm bắt
cấu trúc phi tuyến trong kích hoạt và đạt được
giảm chiều trong khi bảo toàn các đặc trưng kích hoạt quan trọng,
cho phép so sánh hiệu quả hơn các mẫu kiến thức
qua các lớp khác nhau.
Kết hợp Lớp Căn chỉnh Độ tương tự: Sau học
đa tạp, chúng tôi sử dụng thước đo Cổ chai Thông tin (IB) [32]
để xây dựng ma trận độ tương tự định lượng
độ tương tự giữa các lớp bằng cách tối đa hóa thông tin tương hỗ
của chúng trong khi xem xét entropy của mỗi lớp.
Dựa trên ma trận độ tương tự này, chúng tôi chọn các cặp lớp
tương tự nhất để kết hợp.
Để xác thực một cách nghiêm ngặt tính hiệu quả của MKA, chúng tôi
tiến hành đánh giá thực nghiệm rộng rãi trên một mảng đa dạng
các bộ dữ liệu điểm chuẩn, như MMLU và PIQA, và một loạt rộng
các mô hình ngôn ngữ lớn tiên tiến, bao gồm
series Llama-3 với 8B và 70B tham số, series Llama-2
với 7B và 13B tham số, và Mixtral-7B. Kết quả thực nghiệm của chúng tôi chỉ ra rằng MKA có thể duy trì hiệu suất tốt
trong khi đạt được tỷ lệ nén đáng kể, vượt trội
các phương pháp cắt tỉa hiện có và đạt được
mức nén thậm chí lớn hơn khi kết hợp với lượng tử hóa.
Ví dụ, trên bộ dữ liệu MMLU với Llama3-8B, MKA có thể
đạt được tỷ lệ nén 43.75% với chỉ 2.82%
giảm hiệu suất.
Tóm lại, những đóng góp chính của bài báo này như sau:
•Chúng tôi giới thiệu MKA, một kỹ thuật nén mô hình sáng tạo
tận dụng học đa tạp để căn chỉnh và
tích hợp kiến thức qua các lớp, đạt được sự giảm đáng kể
trong kích thước mô hình trong khi bảo toàn hiệu suất.
•Chúng tôi phát triển phương pháp căn chỉnh kiến thức dựa trên đa tạp,
sử dụng Diffusion Kernel và Cổ chai Thông tin (IB)
để hiệu quả nắm bắt và căn chỉnh độ tương tự giữa các lớp
trong không gian tham số.
•Chúng tôi xác thực hiệu quả của MKA thông qua các thực nghiệm
toàn diện trên nhiều bộ dữ liệu điểm chuẩn và
một loạt các mô hình ngôn ngữ lớn, thể hiện khả năng
đạt được nén đáng kể mà không làm tổn hại hiệu suất mô hình.

2 CÔNG TRÌNH LIÊN QUAN
Khung công tác Căn chỉnh Kiến thức dựa trên Đa tạp và
Kết hợp Lớp (MKA) được đề xuất của chúng tôi xây dựng dựa trên và
tích hợp một số lĩnh vực nghiên cứu chính trong tối ưu hóa
mô hình sâu để đạt hiệu quả. Trong các phần tiếp theo,
chúng tôi trước tiên khám phá các phương pháp học dữ liệu có chiều cao,
những phương pháp thiết yếu để hiểu các cấu trúc phức tạp
trong các mô hình ngôn ngữ lớn. Sau đó chúng tôi xem xét
các kỹ thuật nén mô hình chính, bao gồm lượng tử hóa,
cắt tỉa, và chưng cất kiến thức, nhằm giảm
dấu chân tính toán và bộ nhớ của những

--- TRANG 3 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 3
mô hình. Cuối cùng, chúng tôi khám phá lĩnh vực mới nổi của
kết hợp mô hình, làm nổi bật những tiến bộ gần đây thông báo
phương pháp của chúng tôi trong việc tổng hợp và căn chỉnh kiến thức
qua các lớp mô hình.
2.1 Phương pháp Học Dữ liệu Có Chiều Cao.
Dữ liệu có chiều cao đặt ra những thách thức đáng kể trong
học máy, chủ yếu do lời nguyền của chiều cao,
có thể dẫn đến tăng độ phức tạp tính toán
và overfitting [33]. Để giải quyết những thách thức này,
các kỹ thuật học đa tạp đã được phát triển rộng rãi để
nắm bắt các cấu trúc có chiều thấp cơ bản trong
các bộ dữ liệu có chiều cao. Các kỹ thuật như Phân tích
Thành phần Chính (PCA) [34], t-Distributed Stochastic
Neighbor Embedding (t-SNE) [35], và Uniform Manifold
Approximation and Projection (UMAP) [36] đã được
áp dụng rộng rãi cho giảm chiều và hiển thị dữ liệu.
Gần đây hơn, các phương pháp dựa trên khuếch tán như
Diffusion Maps [37] và thuật toán Diffusion Kernel
[31] đã cho thấy triển vọng trong việc bảo toàn hình học
nội tại của dữ liệu trong khi tạo điều kiện tính toán hiệu quả.
Những phương pháp học đa tạp này rất quan trọng để hiểu
và căn chỉnh các mẫu phức tạp trong không gian có chiều cao,
làm cho chúng phù hợp cho các ứng dụng trong nén
mô hình ngôn ngữ lớn (LLM) và căn chỉnh kiến thức.
2.2 Phương pháp Nén Mô hình.
Nén mô hình đã nổi lên như một lĩnh vực nghiên cứu quan trọng
nhằm giảm dấu chân tính toán và bộ nhớ của các mô hình
quy mô lớn mà không làm tổn hại đáng kể hiệu suất của chúng [9], [10], [11], [12], [13].
Các kỹ thuật chính trong nén mô hình bao gồm lượng tử hóa,
cắt tỉa, và chưng cất kiến thức.
Lượng tử hóa bao gồm việc giảm độ chính xác của
trọng số và kích hoạt của mô hình từ biểu diễn có bit cao
(ví dụ: số thực dấu phẩy động 32-bit) sang định dạng bit thấp hơn
(ví dụ: số nguyên 8-bit), do đó giảm sử dụng bộ nhớ và
tăng tốc suy luận [15], [16], [17], [18]. Mặc dù hiệu quả,
lượng tử hóa thường cần hỗ trợ phần cứng chuyên biệt và
có thể cần tinh chỉnh bổ sung để duy trì độ chính xác mô hình [14], [24].
Các kỹ thuật cắt tỉa tập trung vào việc loại bỏ các tham số
dư thừa hoặc ít quan trọng khỏi mô hình, do đó giảm
tổng số tham số và yêu cầu tính toán [19], [20], [21], [22].
Cắt tỉa có thể được phân loại thành cắt tỉa không có cấu trúc,
loại bỏ các trọng số riêng lẻ, và cắt tỉa có cấu trúc,
loại bỏ toàn bộ neuron hoặc kênh [25]. Cắt tỉa có cấu trúc
đặc biệt có lợi thế về tính thân thiện với phần cứng và khả năng
áp dụng trực tiếp cho các mô hình đã được tiền huấn luyện
mà không cần huấn luyện lại rộng rãi [26].
Chưng cất Kiến thức bao gồm việc huấn luyện một mô hình
"học sinh" nhỏ hơn để sao chép hành vi của một mô hình
"giáo viên" lớn hơn, do đó chuyển giao kiến thức trong khi
giảm kích thước mô hình [38]. Phương pháp này đã hiệu quả
trong việc duy trì mức hiệu suất trong khi đạt được nén đáng kể.
Những tiến bộ gần đây cũng đã khám phá các phương pháp
lai ghép kết hợp nhiều kỹ thuật nén để
tận dụng những điểm mạnh bổ sung của chúng [14]. Những phương pháp này nhằm đạt được tỷ lệ nén cao hơn và bảo toàn hiệu suất
tốt hơn bằng cách tích hợp các chiến lược lượng tử hóa,
cắt tỉa, và chưng cất.
2.3 Kết hợp Mô hình.
Kết hợp mô hình là một lĩnh vực đang phát triển tập trung vào
việc kết hợp nhiều mô hình để tạo ra một mô hình duy nhất,
mạnh mẽ hơn bằng cách tận dụng những điểm mạnh và kiến thức
của các thành phần cấu thành [27]. Các phương pháp truyền thống,
chẳng hạn như Model Soup [27], sử dụng trung bình trọng số đơn giản
để kết hợp các mô hình có cùng kiến trúc, hiệu quả pha trộn
các biểu diễn đã học của chúng. Tuy nhiên, phương pháp này
có thể bị giới hạn bởi nhu cầu về kiến trúc giống hệt nhau
và khả năng giảm hiệu suất nếu các mô hình riêng lẻ
không được căn chỉnh đủ.
Những tiến bộ trong kết hợp mô hình đã giới thiệu những kỹ thuật
tinh vi hơn để tăng cường tính mạnh mẽ và hiệu suất
của các mô hình được kết hợp. Checkpoint Merging [28]
sử dụng tối ưu hóa Bayesian để cân và tích hợp có chọn lọc
các checkpoint mô hình khác nhau, dẫn đến một mô hình được kết hợp
ổn định và hiệu suất cao hơn. Tương tự, MindMerger [39]
tạo điều kiện cho sự hợp nhất của các mô hình có chuyên môn khác nhau,
do đó tăng cường khả năng tổng thể của mô hình kết quả
bằng cách tích hợp các cơ sở kiến thức đa dạng.
Các phương pháp kết hợp chuyên gia động, chẳng hạn như
DELLA-Merging [40], tích hợp động các mô hình chuyên gia
chuyên biệt trong quá trình suy luận, cho phép mô hình được kết hợp
thích ứng với một loạt rộng các tác vụ. Các phương pháp
cân có thích ứng như AdaMerging [41] và MetaGPT [42]
tận dụng meta-learning và sơ đồ cân động để tinh chỉnh
quá trình kết hợp, đảm bảo tích hợp tối ưu các điểm mạnh
của các mô hình cấu thành.
Hơn nữa, các chiến lược kết hợp hướng tác vụ,
bao gồm Task Arithmetic [43], Language and Task Arithmetic
[44], và Task Arithmetic in Tangent Space [45], tập trung vào
việc pha trộn các mô hình được huấn luyện trên các tác vụ khác nhau
để tạo ra LLM đa năng có khả năng xử lý nhiều ứng dụng.
Những phương pháp này tăng cường tính linh hoạt và khả năng
áp dụng của các mô hình được kết hợp, làm cho chúng thích ứng hơn
với các tình huống thực tế đa dạng.
Mặc dù có tiến bộ trong kết hợp mô hình, hầu hết các phương pháp
hiện có tập trung vào việc kết hợp các mô hình riêng biệt thay vì
giải quyết các cấu trúc lớp nội bộ trong một mô hình duy nhất.
Khung công tác Căn chỉnh Kiến thức dựa trên Đa tạp và
Kết hợp Lớp (MKA) được đề xuất của chúng tôi giải quyết
khoảng trống này bằng cách cho phép nén LLM thông qua
tổng hợp tiến bộ kiến thức qua các lớp, do đó giảm
tổng số lớp trong khi bảo toàn hiệu suất mô hình thiết yếu.

3 KIẾN THỨC CƠ BẢN
Trong phần này, chúng tôi giới thiệu các khái niệm cơ bản
và khung lý thuyết thiết yếu để hiểu
phương pháp Nén Căn chỉnh Kiến thức dựa trên Đa tạp và
Kết hợp Lớp (MKA) được đề xuất. Chúng tôi bắt đầu bằng
việc thảo luận về tham số hóa quá mức và dư thừa trong
Mô hình Ngôn ngữ Lớn (LLM), sau đó trình bày giả thuyết
đa tạp trong bối cảnh biểu diễn neural và hình học khuếch tán,
và cuối cùng giới thiệu thông tin tương hỗ trong khung
Cổ chai Thông tin (IB).

--- TRANG 4 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 4
3.1 Tham số hóa Quá mức và Dư thừa trong Mô hình
Ngôn ngữ Lớn
Cho D={(xi, yi)}N
i=1 biểu thị một bộ dữ liệu, trong đó xi∈ X
là các chuỗi đầu vào và yi∈ Y là các mục tiêu tương ứng.
Một Mô hình Ngôn ngữ Lớn (LLM) được mô hình hóa như
một hàm có tham số fθ:X → Y với các tham số
θ∈Rd, trong đó d là tổng số tham số. Một
LLM được gọi là có tham số hóa quá mức nếu d≫N.
Tham số hóa quá mức thường dẫn đến dư thừa; nghĩa là,
các cấu hình tham số khác nhau tạo ra các ánh xạ chức năng tương tự.
Chính thức, với một ϵ >0 và hằng số nhỏ δ cho trước,
một tập con tham số S ⊂θ được coi là dư thừa nếu tồn tại
một tập tham số thay thế θ′ thỏa mãn
∥θ′−θ∥ ≤ϵ và L(fθ′,D)≤L(fθ,D) +δ. (1)
Quan sát này thúc đẩy việc khám phá kết hợp lớp
như một chiến lược cho nén mô hình.
3.2 Giả thuyết Đa tạp trong Biểu diễn Neural
và Hình học Khuếch tán
Giả thuyết đa tạp khẳng định rằng dữ liệu có chiều cao
gặp phải trong các ứng dụng thực tế nằm trên một
đa tạp có chiều thấp được nhúng trong không gian xung quanh RD.
Một đa tạp M có chiều k là một không gian topo sao cho
với mọi điểm p∈ M , tồn tại một lân cận U
và một phép đồng phôi ϕ:U→Rk.
Trong bối cảnh mạng neural, cho Hl
i∈Rdl biểu thị
kích hoạt của lớp l cho đầu vào xi. Tập hợp {Hl
i}N
i=1
được giả định nằm trên một đa tạp Ml có chiều nội tại
kl (với kl≪dl) được nhúng trong Rdl. Giả định này
được hỗ trợ bởi quan sát rằng các mạng sâu thực hiện
ngầm giảm chiều.
Để phân tích hình học nội tại của đa tạp kích hoạt,
chúng tôi sử dụng hình học khuếch tán. Xem xét một đồ thị
G= (V,E) trong đó mỗi nút tương ứng với một vector kích hoạt
và trọng số cạnh được định nghĩa thông qua một kernel Gaussian.
Cụ thể, cho hai vector kích hoạt Hl
i và Hl
j,
mối quan hệ được cho bởi
Wij= exp 
−∥Hl
i−Hl
j∥2
σ2!
, (2)
trong đó σ > 0 điều khiển quy mô lân cận. Ma trận bậc D
được định nghĩa bởi
Dii=NX
j=1Wij, (3)
và toán tử khuếch tán sau đó là
P=D−1W. (4)
Toán tử P nắm bắt tính liên thông của đa tạp dữ liệu
bằng cách phục vụ như ma trận xác suất chuyển tiếp cho
một bước đi ngẫu nhiên trên G.
Bản đồ khuếch tán được xây dựng thông qua phân tích phổ
của P. Giả sử rằng P là khả nghịch (điều này xuất phát từ
tính đối xứng của W) và cho {(λj, ϕj)}N
j=1 là các cặp
eigen của nó. Bản đồ khuếch tán tại thời gian t được định nghĩa là
Φt(i) =λt
1ϕ1(i), λt
2ϕ2(i), . . . , λt
kϕk(i). (5)
Trong nhiều trường hợp, eigenvector đầu tiên tương ứng với λ1= 1
được bỏ qua để tập trung vào hình học phi tầm thường;
việc đánh chỉ số có thể được điều chỉnh tương ứng.3.3 Thông tin Tương hỗ và Nguyên lý Cổ chai Thông tin
Thông tin tương hỗ được sử dụng để định lượng độ tương tự
giữa các biểu diễn. Cho một biến ngẫu nhiên liên tục X
với mật độ p(x), entropy vi phân được định nghĩa là
H(X) =−Z
p(x) logp(x)dx. (6)
Thông tin tương hỗ giữa X và Y được cho bởi
I(X;Y) =H(X) +H(Y)−H(X,Y). (7)
Trong khung của chúng tôi, thông tin tương hỗ phục vụ như
một thước đo để so sánh các nhúng bản đồ khuếch tán
của các lớp khác nhau.
Nguyên lý Cổ chai Thông tin (IB) cung cấp một khung
để có được một biểu diễn nén giữ lại thông tin liên quan.
Cho các biến ngẫu nhiên X và Y, mục tiêu IB là tìm
một ánh xạ p(T|X) tối thiểu hóa
min
p(T|X)I(X;T)−βI(T;Y), (8)
trong đó β >0 cân bằng nén với việc bảo toàn
thông tin liên quan. Trong bối cảnh của chúng tôi, mục tiêu
là kết hợp các nhúng bản đồ khuếch tán từ hai lớp
thành một biểu diễn duy nhất tối đa hóa thông tin tương hỗ
với biến mục tiêu Y trong khi tối thiểu hóa dư thừa.

4 HỌC ĐA TẠP CHO BIỂU DIỄN NỘI BỘ
Trong phần này, chúng tôi giới thiệu khung Căn chỉnh Kiến thức
dựa trên Đa tạp (MKA), tận dụng giả thuyết đa tạp
và hình học khuếch tán để xác định và kết hợp
các lớp dư thừa trong các mô hình ngôn ngữ lớn (LLM).
Xây dựng trên các khái niệm được nêu trong Phần 3,
chúng tôi mô tả việc trích xuất các kích hoạt có chiều cao,
xây dựng toán tử khuếch tán, phân tích phổ dẫn đến
bản đồ khuếch tán, và căn chỉnh và kết hợp
các lớp tiếp theo bằng cách sử dụng các thước đo lý thuyết thông tin.
4.1 Trích xuất Kích hoạt Có Chiều Cao
Cho M biểu thị một LLM xử lý một bộ dữ liệu D={xi}N
i=1.
Cho mỗi đầu vào xi, mô hình tạo ra các kích hoạt tại mỗi lớp.
Cụ thể, chúng tôi ký hiệu bằng Hl
i∈Rdl kích hoạt của
lớp l cho đầu vào xi, trong đó dl là chiều kích hoạt tại
lớp l. Các kích hoạt được tính toán thông qua
H0
i= Embed( xi), (9)
Hl
i=fθl(Hl−1
i), l= 1,2, . . . , L, (10)
trong đó fθl biểu thị phép biến đổi liên kết với lớp
l, được tham số hóa bởi θl. Tập hợp {Hl
i}N
i=1 do đó
tạo thành một bộ dữ liệu có chiều cao nội tại cho mỗi lớp.

--- TRANG 5 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 5
4.2 Xây dựng Toán tử Khuếch tán
Để phân tích cấu trúc đa tạp cơ bản của các kích hoạt này,
chúng tôi xây dựng một đồ thị có trọng số Gl= (Vl,El) cho
mỗi lớp l, trong đó mỗi nút tương ứng với một vector kích hoạt Hl
i.
Các cạnh được cân nặng theo mối quan hệ giữa các vector kích hoạt.
Cho hai vector kích hoạt Hl
i và Hl
j, chúng tôi định nghĩa
mối quan hệ bằng kernel Gaussian:
Wij=K(Hl
i,Hl
j) = exp 
−∥Hl
i−Hl
j∥2
σ2!
, (11)
trong đó σ > 0 là tham số băng thông điều khiển
quy mô lân cận cục bộ.
Ma trận bậc D là đường chéo với các mục
Dii=NX
j=1Wij. (12)
Sau đó chúng tôi định nghĩa toán tử khuếch tán như
ma trận mối quan hệ được chuẩn hóa:
P=D−1W. (13)
Toán tử này chi phối xác suất chuyển tiếp của một
bước đi ngẫu nhiên trên đồ thị Gl và do đó nắm bắt
hình học nội tại của đa tạp kích hoạt.
4.3 Phân tích Phổ và Bản đồ Khuếch tán
Toán tử P mã hóa hình học đa tạp, và
các tính chất phổ của nó cho phép chúng ta xây dựng
bản đồ khuếch tán—các nhúng có chiều thấp hơn
bảo toàn cấu trúc của đa tạp.
Giả sử rằng ma trận mối quan hệ W là đối xứng và
bán xác định dương và P là khả nghịch (điều này
xảy ra khi chuỗi Markov cơ bản là khả nghịch).
Mặc dù P không đối xứng, nó tương tự với ma trận đối xứng
eP=D−1/2WD−1/2. (14)
Vì eP đối xứng, nó có một tập hoàn chỉnh các giá trị eigen
thực {λk}N
k=1 và các eigenvector trực chuẩn tương ứng
{ϕk}N
k=1, sao cho
Pϕk=λkϕk. (15)
Không mất tính tổng quát, chúng ta sắp xếp các giá trị eigen như
1 =λ1≥λ2≥ ··· ≥ λN≥ −1. (16)
Giá trị eigen lớn nhất là λ1= 1 , và eigenvector tương ứng
của nó là hằng số.
Sử dụng phân tích phổ, bản đồ khuếch tán tại
thời gian t được định nghĩa như ánh xạ
Φt(Hl
i) =
λt
2ϕ2(i), λt
3ϕ3(i), . . . , λt
k+1ϕk+1(i)
,(17)
trong đó k là chiều nhúng mục tiêu và ϕj(i)
biểu thị thành phần thứ i của ϕj. (Lưu ý rằng
eigenvector tầm thường liên kết với λ1 được bỏ qua.)4.4 Bảo toàn Cấu trúc Đa tạp
Bản đồ khuếch tán được thiết kế để bảo toàn hình học
nội tại của đa tạp kích hoạt bằng cách nắm bắt
tính liên thông đa quy mô. Đặc biệt, khoảng cách Euclidean
trong không gian khuếch tán xấp xỉ khoảng cách khuếch tán
giữa các điểm.
Cho bất kỳ hai vector kích hoạt Hl
i và Hl
j nào,
khoảng cách khuếch tán được định nghĩa bởi
Dt(i, j)2=k+1X
k=2λ2t
kϕk(i)−ϕk(j)2. (18)
Theo định nghĩa của bản đồ khuếch tán,
∥Φt(Hl
i)−Φt(Hl
j)∥2
2=k+1X
k=2λ2t
kϕk(i)−ϕk(j)2,(19)
sao cho khoảng cách Euclidean trong không gian nhúng
xấp xỉ khoảng cách khuếch tán. Hơn nữa, khi t→ ∞ ,
ảnh hưởng của các giá trị eigen nhỏ hơn giảm dần,
và bản đồ khuếch tán ngày càng nhấn mạnh
cấu trúc toàn cầu của đa tạp.
4.5 Thước đo Độ tương tự Lớp thông qua Thông tin Tương hỗ
Để định lượng độ tương tự giữa các biểu diễn trong
các lớp khác nhau, chúng tôi so sánh các nhúng bản đồ khuếch tán
của chúng. Cho
Ψl= Φt(Hl) và Ψm= Φt(Hm) (20)
là các nhúng bản đồ khuếch tán cho các lớp l và m,
tương ứng. Chúng tôi định nghĩa độ tương tự của chúng
theo thông tin tương hỗ (MI)
I(Ψl;Ψm) =H(Ψl) +H(Ψm)−H(Ψl,Ψm), (21)
trong đó H(·) biểu thị entropy vi phân. Để đơn giản hóa
tính toán, chúng tôi giả sử rằng Ψl và Ψm là
các biến ngẫu nhiên Gaussian kết hợp. Dưới giả định
Gaussianity kết hợp này, MI có thể được viết dưới dạng
đóng là
I(Ψl;Ψm) =1
2ln 
|ΣΨl||ΣΨm|
|ΣΨl,Ψm|!
, (22)
trong đó ΣΨl và ΣΨm biểu thị các ma trận hiệp phương sai
của Ψl và Ψm, tương ứng, và ΣΨl,Ψm là ma trận
hiệp phương sai kết hợp của chúng.
Để tạo điều kiện so sánh giữa các cặp lớp khác nhau,
chúng tôi định nghĩa thêm thông tin tương hỗ chuẩn hóa (NMI) là
Slm=I(Ψl;Ψm)p
H(Ψl)H(Ψm). (23)
Việc chuẩn hóa này ràng buộc Slm nằm trong một
phạm vi nhất quán, do đó cho phép đánh giá độ tương tự
có ý nghĩa giữa các lớp.
4.6 Căn chỉnh Kiến thức dựa trên Đa tạp và Nén Kết hợp Lớp (MKA)
Thuật toán 1 tóm tắt quy trình MKA. Tóm lược, cho mỗi lớp,
các kích hoạt trước tiên được trích xuất và nhúng
bằng bản đồ khuếch tán. Tiếp theo, độ tương tự theo cặp
giữa các lớp được tính toán thông qua thông tin tương hỗ của chúng.
Cuối cùng, khi điểm số độ tương tự giữa một cặp lớp
vượt quá ngưỡng τ được xác định trước, các lớp được kết hợp
theo một kết hợp có trọng số của các tham số của chúng.

--- TRANG 6 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 6
Thuật toán 1 Căn chỉnh Kiến thức dựa trên Đa tạp và Nén Kết hợp Lớp (MKA)
Yêu cầu: LLM M với các lớp {L1, L2, . . . , L N} và tham số Θ ={θ1, θ2, . . . , θ N}, bộ dữ liệu ω
Đảm bảo: Mô hình nén M∗ với các biểu diễn được căn chỉnh
1:Trích xuất kích hoạt H={Hl
i} cho mỗi lớp l trên bộ dữ liệu ω
2:cho mỗi lớp l làm
3: Tính khoảng cách theo cặp giữa các kích hoạt {Hl
i}
4: Xây dựng ma trận mối quan hệ W(l) bằng kernel Gaussian với băng thông σ
5: Tính nhúng bản đồ khuếch tán Ψl thông qua phân tích eigen của W(l)
6:kết thúc cho
7:cho mỗi cặp lớp (l, m) làm
8: Ước lượng các ma trận hiệp phương sai ΣΨl,ΣΨm, và hiệp phương sai kết hợp ΣΨl,Ψm
9: Tính I(Ψl;Ψm) và điểm số độ tương tự chuẩn hóa Slm
10:kết thúc cho
11:trong khi tồn tại một cặp (l, m) với Slm≥τ làm
12: Xác định trọng số kết hợp α, ví dụ thông qua α=Slm (hoặc, thay thế, sử dụng công thức softmax)
13: Kết hợp các tham số: eθc=α θl+ (1−α)θm
14: Thay thế các lớp l và m bằng lớp được kết hợp sử dụng eθc và cập nhật M tương ứng
15:kết thúc trong khi
16:trả về Mô hình nén M∗
4.7 Kết hợp Lớp thông qua Nguyên lý Cổ chai Thông tin
Để hướng dẫn quá trình kết hợp lớp, chúng tôi áp dụng
một quan điểm lý thuyết thông tin dựa trên nguyên lý
Cổ chai Thông tin (IB). Khung IB tìm cách trích xuất
một biểu diễn nén bảo toàn thông tin
liên quan nhất đến một biến mục tiêu.
Trong dạng chuẩn của nó, cho các biến ngẫu nhiên X và Y,
mục tiêu IB là tìm một ánh xạ p(T|X) tối thiểu hóa
LIB=I(X;T)−β I(T;Y), (24)
trong đó β > 0 cân bằng sự đánh đổi giữa nén
(tối thiểu hóa I(X;T)) và dự đoán (tối đa hóa I(T;Y)).
Cho việc kết hợp các lớp l và m, chúng ta đặt X= (Ψl,Ψm) và
tìm kiếm một biểu diễn nén Ψc nắm bắt
thông tin chia sẻ của chúng. Mục tiêu IB tương ứng là
LIB=I(Ψl,Ψm);Ψc−β I(Ψc;Y). (25)
Để làm cho việc tối ưu hóa có thể xử lý được, chúng tôi hạn chế
ánh xạ thành một kết hợp tuyến tính xác định:
Ψc=αΨl+ (1−α)Ψm, α∈[0,1]. (26)
Dưới giả định Gaussian kết hợp, hiệp phương sai của
Ψc được cho bởi
ΣΨc=α2ΣΨl+ (1−α)2ΣΨm+ 2α(1−α) ΣΨl,Ψm,(27)
và nếu ΣΨc,Y biểu thị hiệp phương sai chéo giữa Ψc và Y, thì
ΣΨc,Y=αΣΨl,Y+ (1−α) ΣΨm,Y. (28)
Hiệp phương sai có điều kiện được định nghĩa là
ΣΨc|Y= ΣΨc−ΣΨc,YΣ−1
YΣY,Ψc. (29)
Vì Ψc là một hàm xác định của (Ψl,Ψm) (tức là,
H(Ψc|(Ψl,Ψm)) = 0 ), các thuật ngữ thông tin tương hỗ được rút gọn thành entropy vi phân. Đặc biệt, sử dụng
công thức Gaussian chuẩn cho entropy vi phân,
H(Ψc) =1
2ln
(2πe)dc|ΣΨc|
, (30)
H(Ψc|Y) =1
2ln
(2πe)dc|ΣΨc|Y|
, (31)
mục tiêu IB đơn giản hóa thành
LIB=1
2h
(1−β) ln|ΣΨc|+βln|ΣΨc|Y|i
+ const ,(32)
trong đó thuật ngữ hằng số không phụ thuộc vào α.
Về nguyên tắc, người ta có thể tối ưu hóa LIB với respect đến α bằng cách
vi phân
∂LIB
∂α=1
2"
(1−β) Tr
Σ−1
Ψc∂ΣΨc
∂α
+βTr
Σ−1
Ψc|Y∂ΣΨc|Y
∂α#
(33)
và đặt đạo hàm bằng không. Ví dụ, người ta có thể tính toán
∂ΣΨc
∂α= 2αΣΨl−2(1−α) ΣΨm+ 2(1−2α) ΣΨl,Ψm.
(34)
Một biểu thức tương tự có thể được rút ra cho ∂ΣΨc|Y/∂α.
Tuy nhiên, trong thực tế, phương trình này không cho phép
một giải pháp dạng đóng. Do đó, chúng tôi áp dụng một chiến lược
xấp xỉ bằng cách đặt
α=Slm, (35)
trong đó Slm là thông tin tương hỗ chuẩn hóa được định nghĩa
trước đó. Heuristic này gán trọng số lớn hơn cho lớp
có thông tin chia sẻ cao hơn, do đó xấp xỉ
tối thiểu hóa mục tiêu IB trong khi duy trì hiệu quả tính toán.
Với trọng số α được xác định, các tham số của các lớp
l và m được kết hợp thông qua
eθc=αθl+ (1−α)θm. (36)
Tập tham số được kết hợp eθc này được sử dụng để thay thế
các lớp gốc, dẫn đến một mô hình nén M∗.

--- TRANG 7 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 7
4.8 Tác động lên Hiệu suất Mô hình
Kết hợp các lớp thay đổi không gian tham số của mô hình,
điều này có thể ảnh hưởng đến hiệu suất của nó.
Cho δθ=eθc−θ biểu thị sự dịch chuyển trong tham số
do kết hợp. Giả sử rằng hàm mất mát L(θ) có thể vi phân
hai lần và lồi cục bộ trong một lân cận xung quanh θ.
Khai triển Taylor bậc hai về θ cho
L(eθc)≈ L(θ) +∇L(θ)⊤δθ+1
2δθ⊤∇2L(θ)δθ. (37)
Tại một cực tiểu cục bộ nơi ∇L(θ) =0, sự tăng trong mất mát
được giới hạn bởi
∆L=L(eθc)− L(θ)
≤1
2λmax∥δθ∥2, (38)
trong đó λmax là giá trị eigen lớn nhất của Hessian ∇2L(θ).
Ràng buộc này xuất phát từ thương số Rayleigh và cung cấp
một đảm bảo về sự tăng mất mát do thao tác kết hợp.

5 THỰC NGHIỆM
Chúng tôi tiến hành một tập hợp thực nghiệm toàn diện để đánh giá
tính hiệu quả và khả năng tổng quát của phương pháp MKA
qua các lĩnh vực khác nhau. Hơn nữa, chúng tôi nhằm so sánh
phương pháp của chúng tôi với các kỹ thuật cắt tỉa để đánh giá
liệu nó có cung cấp cải thiện hay không và để điều tra
liệu nó có thể được kết hợp với các phương pháp lượng tử hóa
để đạt được tỷ lệ nén thậm chí cao hơn.
5.1 Thiết lập Thực nghiệm
5.1.1 Bộ dữ liệu
Chúng tôi tiến hành đánh giá bằng phương pháp MKA trên
các bộ dữ liệu điểm chuẩn khác nhau, mỗi bộ được thiết kế
đặc biệt để kiểm tra các khía cạnh khác nhau của hiểu
và tạo ngôn ngữ. Chi tiết, MMLU [46] đánh giá
hiểu ngôn ngữ rộng qua một loạt các lĩnh vực.
PIQA [47] được thiết kế để kiểm tra các mô hình về
lý luận thông thường trong thế giới vật lý, nhằm đánh giá
khả năng nắm bắt các tương tác vật lý hằng ngày của
các mô hình NLP. HellaSwag [48] là một bộ dữ liệu thách thức
cho suy luận ngôn ngữ tự nhiên thông thường, bao gồm
các mô tả sự kiện với nhiều khả năng tiếp tục,
nơi nhiệm vụ là chọn cái hợp lý nhất. RACE-H[49]
là một bộ dữ liệu hiểu đọc quy mô lớn được thu thập
từ các kỳ thi tiếng Anh cho học sinh trung học Trung Quốc,
có tỷ lệ cao các câu hỏi yêu cầu lý luận.
BoolQ [50] là một bộ dữ liệu hiểu đọc tập trung vào
các câu hỏi có/không tự nhiên thường truy vấn
thông tin phức tạp, không phải sự thật và yêu cầu
suy luận khó như entailment để trả lời chính xác.
5.1.2 LLM
Trong các thực nghiệm của chúng tôi, chúng tôi sử dụng
các mô hình Llama2 [3], Llama3, Llama3.2, và Mistral [51],
mỗi mô hình riêng biệt về khả năng và cấu hình:
Llama2 : Bao gồm các mô hình từ 7 tỷ đến 13 tỷ tham số,
thể hiện hiệu suất và an toàn vượt trội trên các điểm chuẩn đa dạng.
Llama3 : Có các mô hình với 8 tỷ tham số, cung cấp
hiệu suất tiên tiến và khả năng lý luận tiến bộ.
Llama3.2 : Có các mô hình với 3 tỷ tham số cân bằng
hiệu suất và số lượng tham số.
Mistral : Chúng tôi sử dụng phiên bản 7 tỷ tham số của Mistral
vượt trội Llama-2 và Llama-1 về hiệu suất và hiệu quả,
tận dụng các cơ chế grouped-query và sliding window attention
để suy luận tối ưu qua các chuỗi dài.
5.1.3 Đường cơ sở
Trong nghiên cứu này, chúng tôi đánh giá tính hiệu quả của
phương pháp đề xuất, MKA, thông qua hai phân tích so sánh riêng biệt.
Đầu tiên, chúng tôi đánh giá MKA trực tiếp với một số
kỹ thuật cắt tỉa được thiết lập tốt để đo lường
hiệu quả độc lập của nó trong việc giảm kích thước mô hình
trong khi duy trì hiệu suất. Thứ hai, chúng tôi mở rộng
so sánh để bao gồm các tình huống nơi cả
phương pháp cắt tỉa truyền thống và MKA được tăng cường
thêm thông qua lượng tử hóa. Các phương pháp đường cơ sở
được bao gồm trong phân tích của chúng tôi là:
PruneMe [52]: Một phương pháp cắt tỉa xác định khối lớp
tối ưu để cắt tỉa bằng cách xem xét độ tương tự qua các lớp.
SLEB [53]: Một phương pháp cắt tỉa được thiết kế để sắp xếp
hợp lý LLM bằng cách loại bỏ các khối transformer dư thừa.
Chúng tôi chọn khối transformer làm đơn vị cơ bản cho cắt tỉa,
bởi vì LLM thể hiện dư thừa cấp khối với độ tương tự cao
giữa các đầu ra của các khối lân cận.
Shortened [54]: Một phương pháp cắt tỉa đầu tiên sử dụng
một thước đo đơn giản để xác định các khối không quan trọng
và sau đó thực hiện cắt tỉa một lần đơn giản.
ShortGPT [14]: Một phương pháp cắt tỉa loại bỏ các lớp dư thừa
từ các mô hình ngôn ngữ lớn dựa trên thước đo Block Influence,
đánh giá tầm quan trọng của mỗi lớp.
Reverse : Một phương pháp heuristic nơi tầm quan trọng
của các lớp được coi là tỷ lệ nghịch với thứ tự của chúng
trong mô hình, ưu tiên giữ lại các lớp trước.
SmoothQuant [55]: SmoothQuant là một giải pháp lượng tử hóa
sau huấn luyện không cần huấn luyện cho phép lượng tử hóa
trọng số và kích hoạt 8-bit hiệu quả cho các mô hình ngôn ngữ lớn,
cung cấp tăng tốc lên đến 1.56× và giảm bộ nhớ 2×
với mất mát độ chính xác tối thiểu.
GPTQ [56]: Một phương pháp lượng tử hóa trọng số một lần
sử dụng thông tin bậc hai xấp xỉ để duy trì độ chính xác cao
ngay cả với giảm trọng số nghiêm trọng.
AWQ [57]: Một phương pháp lượng tử hóa mới bảo vệ
các trọng số nổi bật bằng cách điều chỉnh scaling theo kênh
dựa trên quan sát kích hoạt thay vì độ lớn trọng số.

6 THỰC NGHIỆM
6.1 So sánh MKA với các phương pháp cắt tỉa có cấu trúc khác
Chúng tôi so sánh hiệu suất của MKA với các phương pháp nén
đường cơ sở trên bộ dữ liệu MMLU sử dụng các mô hình
Llama2-7B, Llama2-13B, Llama3-8B, Llama3.2-3B, và Mistral-7B.
Thước đo đánh giá là Độ chính xác (ACC) trong quá trình
kết hợp và cắt tỉa. Kết quả được trình bày trong Hình 2.
Từ Hình 2, chúng ta có thể quan sát thấy, qua tất cả các mô hình,
phương pháp của chúng tôi cải thiện tỷ lệ nén trong khi duy

--- TRANG 8 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 8
0.0 0.1 0.2 0.3 0.4 0.5
Tỷ lệ0.250.300.350.400.45Độ chính xác Trung bình
Llama-2-7b-hf
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (của chúng tôi)
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Tỷ lệ0.250.300.350.400.450.500.55Độ chính xác Trung bình
Llama-2-13b-hf
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (của chúng tôi)
0.0 0.1 0.2 0.3 0.4 0.5
Tỷ lệ0.30.40.50.6Độ chính xác Trung bình
Meta-Llama-3-8B
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (của chúng tôi)
0.0 0.1 0.2 0.3 0.4 0.5
Tỷ lệ0.250.300.350.400.450.500.55Độ chính xác Trung bình
Llama-3.2-3B
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (của chúng tôi)
0.0 0.1 0.2 0.3 0.4 0.5
Tỷ lệ0.30.40.50.6Độ chính xác Trung bình
Mistral-7B-v0.1
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (của chúng tôi)
Hình 2. Hiệu suất (Độ chính xác) của LLM (Llama2-7B, Llama2-13B, Llama3-8B, Llama3.2-3B, và Mistral-7B) trên bộ dữ liệu MMLU khi tỷ lệ cắt tỉa của các phương pháp cắt tỉa khác nhau tăng lên.
trì hiệu suất. Cụ thể, tỷ lệ nén3 cho Llama2-7B đạt 31%,
cho Llama3-8B đạt 44%, cho Llama3.2-3B đạt 43%,
cho Mistral-7B đạt 40%, và cho Llama2-13B đạt ấn tượng 58%.
Ngoài ra, chúng tôi quan sát một số hiện tượng:
cả hai phương pháp đều trải qua sự sụp đổ trong hiệu suất mô hình,
nhưng phương pháp kết hợp mô hình có thể trì hoãn sự sụp đổ lớp
đến một mức độ nhất định và ổn định hiệu suất của mô hình rất tốt.
Vì chiến lược của chúng tôi dựa trên Reverse Prune,
điểm số cho các mô hình Llama2-7B, Llama2-13B, Llama3-8B,
và Llama3.2-3B rất gần với Reverse Prune. Giả thuyết của chúng tôi
là việc cắt tỉa hoặc kết hợp của những mô hình này tương tự,
nhưng kết hợp mô hình có thể điều chỉnh tỷ lệ kết hợp
để vượt qua hiệu ứng của cắt tỉa. Hơn nữa, đối với các mô hình Mistral-7B,
chúng tôi nhận thấy rằng kết quả không khớp chặt chẽ với Reverse Prune.
6.2 MKA Kết hợp với Lượng tử hóa Hoạt động Như thế nào
So với Cắt tỉa Kết hợp với Lượng tử hóa?
Chúng tôi so sánh hiệu suất của MKA với phương pháp cắt tỉa
đường cơ sở, ShortGPT [14], trên bộ dữ liệu MMLU sử dụng
các mô hình Llama2-7B, Llama3-8B, và Mistral-7B.
Kết quả được trình bày trong Bảng 1.
Từ Bảng 1, chúng ta có thể thấy rằng các mô hình đã cắt tỉa
có thể được lượng tử hóa thêm và duy trì hiệu suất
với tỷ lệ nén cao hơn. Đáng chú ý, tại tỷ lệ nén cao
khoảng 87.50%, MKA vượt trội đáng kể so với ShortGPT.
Ngoài ra, chúng tôi đạt được kết quả xuất sắc với
các phương pháp lượng tử hóa khác nhau. Ví dụ, trên Llama3-8B,
tại tỷ lệ nén 85.94%, MKA với SmoothQuant đạt 64.20%,
vượt xa ShortGPT với SmoothQuant ở 37.66%. Tương tự,
với phương pháp lượng tử hóa GPTQ, chúng tôi đạt 62.98%,
vượt qua 37.00% của ShortGPT, và với AWQ, chúng tôi đạt 61.66%,
vượt qua 35.44% của ShortGPT.
6.3 MKA so với Các Phương pháp Cắt tỉa Khác trên
các điểm chuẩn khác nhau
Chúng tôi so sánh hiệu suất của MKA và một số phương pháp
cắt tỉa khác trên mô hình LLama3-8B sử dụng nhiều bộ dữ liệu
điểm chuẩn tại tỷ lệ nén 34.375%, 37.5%, 40.625% và 43.75%.
Kết quả được trình bày trong Bảng 2.
Từ kết quả, kết hợp có thể giữ lại hiệu suất tốt hơn
3. Lưu ý rằng, tỷ lệ nén được tính như:
Ltotal−
Lretained
Q
/Ltotal, trong đó Ltotal là tổng số
lớp trước khi nén, Lretained là số lớp được giữ lại,
và Q là hệ số lượng tử hóa.BẢNG 1
So sánh hiệu suất của MKA và cắt tỉa ShortGPT với
lượng tử hóa (SmoothQuant, GPTQ, AWQ) trên MMLU sử dụng Llama2-7B,
Llama3-8B, và Mistral-7B. MKA vượt trội ShortGPT về độ chính xác
qua tất cả các mô hình và phương pháp lượng tử hóa tại tỷ lệ nén tương tự
với int4. Việc tính toán tỷ lệ nén chỉ xem xét
số lớp ẩn trong mô hình mà không xem xét lớp nhúng.
Mô hình Phương phápLớp Giữ lại
(Tỷ lệ Nén)Acc
Llama2-7BMô hình Vanilla 32(0.00%) 46.67
ShortGPT+Smooth 16(87.50%) 25.67
ShortGPT+GPTQ 16(87.50%) 25.82
ShortGPT+AWQ 16(87.50%) 26.01
MKA (Của chúng tôi) + Smooth 16(87.50%) 35.66 (+9.99)
MKA (Của chúng tôi) + GPTQ 16(87.50%) 35.91 (+10.09)
MKA (Của chúng tôi) + AWQ 16(87.50%) 36.23 (+10.22)
Llama3-8BMô hình Vanilla 32 (0.00%) 66.29
ShortGPT+Smooth 18(85.94%) 26.54
ShortGPT+GPTQ 18(85.94%) 25.98
ShortGPT+AWQ 18(85.94%) 26.22
MKA (Của chúng tôi) + Smooth 18(85.94%) 64.20 (+37.66)
MKA (Của chúng tôi) + GPTQ 18(85.94%) 62.98 (+37.00)
MKA (Của chúng tôi) + AWQ 18(85.94%) 61.66 (+35.44)
Mistral-7BMô hình Vanilla 32(0.00%) 63.87
ShortGPT+Smooth 20(84.38%) 24.32
ShortGPT+GPTQ 20(84.38%) 23.16
ShortGPT+AWQ 20(84.38%) 23.96
MKA (Của chúng tôi) + Smooth 20(84.38%) 56.92 (+32.60)
MKA (Của chúng tôi) + GPTQ 20(84.38%) 56.12 (+32.96)
MKA (Của chúng tôi) + AWQ 20(84.38%) 55.34 (+31.38)
so với cắt tỉa. Tương đối với ShortGPT, phương pháp của chúng tôi
có thể đạt được giữ lại hiệu suất tốt hơn, với cải thiện đáng kể
qua tất cả các bộ dữ liệu. Ví dụ, tại tỷ lệ nén 34.375%
trên bộ dữ liệu MMLU, phương pháp của chúng tôi có thể vượt trội
ShortGPT 21.92%. Tương tự, trên bộ dữ liệu HellaSwag,
phương pháp đề xuất của chúng tôi có thể vượt qua ShortGPT 18.32%.
6.4 Các Ma trận Độ tương tự Căn chỉnh Kiến thức Liên Lớp
có Nhất quán qua Các Mô hình Khác nhau không?
Chúng tôi tạo ra các heatmap độ tương tự lớp cho các mô hình khác nhau
trước khi áp dụng MKA. Những heatmap này hình dung
căn chỉnh kiến thức và hiệu ứng kết hợp lớp của MKA
trên các mô hình khác nhau. Hình 3 trình bày các heatmap độ tương tự
cho các mô hình Llama2-7B, Llama2-13B, Llama-3-8B, Llama3.2-3B,
và Mistral-7B. Chúng tôi quan sát thấy rằng các heatmap cho
các lớp sau của mỗi mô hình thể hiện giá trị độ tương tự cao,
chỉ ra rằng độ tương tự liên lớp nhất quán cao trong

--- TRANG 9 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 9
BẢNG 2
So sánh các phương pháp khác nhau qua các bộ dữ liệu MMLU, PIQA, HellaSwag, RACE-H, và BoolQ tại các tỷ lệ nén khác nhau.
Tỷ lệ Nén = 34.375% Tỷ lệ Nén = 37.5%
Phương pháp MMLU PIQA HellaSwag RACE-H BoolQ MMLU PIQA HellaSwag RACE-H BoolQ
Mô hình Vanilla 66.29 81.12 74.54 66.07 66.79 66.29 81.12 74.54 66.07 66.79
ShortGPT 42.95 60.99 33.00 41.68 51.96 44.80 61.70 38.69 40.05 57.09
MKA (Của chúng tôi) 64.87(+20.42) 67.79(+6.80) 51.32(+18.32) 55.20(+13.52) 63.36(+11.40) 62.05(+17.25) 66.26(+4.56) 50.16(+11.47) 49.49(+9.44) 63.46(+6.37)
Tỷ lệ Nén = 40.625% Tỷ lệ Nén = 43.75%
Phương pháp MMLU PIQA HellaSwag RACE-H BoolQ MMLU PIQA HellaSwag RACE-H BoolQ
Mô hình Vanilla 66.29 81.12 74.54 66.07 66.79 66.29 81.12 74.54 66.07 66.79
ShortGPT 39.26 58.22 34.16 21.70 61.77 26.09 59.03 33.75 21.58 61.53
MKA (Của chúng tôi) 63.42(+24.16) 65.61(+6.25) 48.83(+14.67) 55.26(+33.20) 63.58(+1.81) 64.42(+31.31) 65.51(+6.48) 45.10(+11.35) 45.91(+22.77) 62.14(+0.51)
LớpLớpLlama3-8B
LớpLlama3-70B
LớpMixtral-7B
LớpLlama2-7B
LớpLlama2-13B
2468
51015
24681012
123
1234
Hình 3. Ma trận độ tương tự cho Llama2-7B, Llama2-13B, Llama-3-8B, Llama3.2-3B, và Mistral-7B trước MKA. Các lớp sau cho thấy độ tương tự cao,
hỗ trợ kết hợp lớp.
các lớp sau qua các mô hình khác nhau. Quan sát này
hỗ trợ phương pháp kết hợp lớp của chúng tôi. Ngoài ra,
khi kết hợp các lớp trước, chúng tôi nhận thấy sự sụp đổ
của ma trận trong hình cuối cùng, gợi ý rằng các lớp trước
có ảnh hưởng đáng kể đến các lớp sau. Do đó,
các thao tác kết hợp đơn giản trên các lớp trước của mô hình
không khả thi.
6.5 Bản chất Lặp lại của MKA
Quan trọng cần lưu ý rằng phương pháp MKA đã kết hợp
một quá trình lặp lại trong thiết kế của nó. Ví dụ,
khi chúng tôi kết hợp các lớp 31 và 32, chúng tôi có được
một lớp hợp nhất, sau đó được kết hợp với lớp 30
trong lần lặp tiếp theo. Chúng tôi đã so sánh phương pháp này
với một phương pháp thay thế nơi mỗi lớp được phép
kết hợp chỉ một lần (ví dụ, kết hợp các lớp 31 và 32,
sau đó 30 và 29 riêng biệt). Các thực nghiệm của chúng tôi
trên mô hình Llama3-8B sử dụng bộ dữ liệu MMLU chứng minh
rằng phương pháp lặp lại của MKA mang lại hiệu suất vượt trội
trong việc tối thiểu hóa suy giảm độ chính xác.
Kết quả được trình bày trong Bảng 4.

7 THẢO LUẬN
7.1 Mở rộng sang Các Mô hình Đa phương thức và Chuyên biệt
Ngoài ứng dụng của nó cho các mô hình ngôn ngữ lớn,
phương pháp MKA cho thấy tiềm năng hứa hẹn cho việc
áp dụng rộng rãi hơn qua một loạt các kiến trúc học sâu.
Điều này bao gồm Mixture-of-Experts (MoE) [4], và các mô hình Jamba [58],
có thể thể hiện các dư thừa tương tự trong các lớp xử lý của chúng.
Kết quả được hiển thị trong Hình 4. Các thực nghiệm ban đầu
được tiến hành trên những kiến trúc đa dạng này đã củng cố
tính khả thi của phương pháp chúng tôi. Ví dụ, các ma trận độ tương tự
được tạo ra trên Mixtral-8x7B [4] và jamba [58] áp dụng
LớpLớpMixtral-8x7B
LớpJamba
0.51.01.52.02.53.0
12345Hình 4. Ma trận độ tương tự của mô hình Mixtral-8x7B và Jamba.
MKA đã cho thấy rằng phương pháp của chúng tôi cũng có thể
được tổng quát hóa cho các mô hình tương tự khác, nhưng
phân phối độ tương tự của Mixtral-8x7B và Jamba hơi khác
so với LLM, và chúng tôi chưa biết lý do. Những thực nghiệm này
xác thận thêm tính hiệu quả của phương pháp chúng tôi qua
các loại mô hình khác nhau.
7.2 Phân tích Các Thước đo Độ tương tự
Trong đánh giá của chúng tôi về mô hình Llama3-8B,
chúng tôi đã khám phá một số thước đo độ tương tự:
Độ tương tự Cosine, Khoảng cách Mahalanobis, Khoảng cách Euclidean,
Độ tương tự t-SNE, và Độ tương tự Autoencoder.
Các ma trận độ tương tự được hiển thị trong Hình 5.
Từ kết quả, chúng tôi quan sát thấy rằng Độ tương tự Cosine,
Khoảng cách Mahalanobis, và Khoảng cách Euclidean hiển thị
các mẫu phân phối tương tự với các sọc dọc và
giá trị nhiệt khác nhau. Tuy nhiên, Khoảng cách Mahalanobis
cho thấy các giá trị nhiệt không đều trong những sọc này,
chỉ ra sự không căn chỉnh với cấu trúc dữ liệu lớp hợp nhất.
Độ tương tự t-SNE xuất hiện

--- TRANG 10 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 10
LớpLớpĐộ tương tự Cosine
LớpKhoảng cách Mahalanobis
LớpKhoảng cách Euclidean
LớpĐộ tương tự t-SNE
LớpĐộ tương tự Autoencoder
0.7750.8000.8250.8500.8750.9000.9250.950
0.51.01.52.02.53.03.54.0
0.490.500.510.520.530.54
468101214
0.320.340.360.380.400.42
Hình 5. Ma trận độ tương tự cho các thước đo khác nhau trong mô hình Llama3-8B, hiển thị các mẫu khác nhau và tính hiệu quả trong việc nắm bắt mối quan hệ lớp,
với không có thước đo nào hoàn toàn khớp với các mẫu kết hợp mong đợi.
ngẫu nhiên và thiếu các mẫu nhất quán. Đối với Độ tương tự Autoencoder,
các giá trị nhiệt cao không tương ứng với các khu vực kết hợp phù hợp
hoặc các vùng độ tương tự cao mong đợi.
7.3 Khám phá Thêm về Tỷ lệ Kết hợp
Để điều tra thêm tác động của λm, chúng tôi đã tiến hành
thực nghiệm sử dụng các giá trị cố định của λm mà không xem xét
độ tương tự lớp. Chúng tôi kiểm tra các giá trị λm là 0.7, 0.6, 0.5, và 0.4
(gán trọng số cao hơn cho lớp có số thứ tự thấp hơn).
Kết quả được hiển thị trong Bảng 3.
Những kết quả này thể hiện một xu hướng hơi đơn điệu,
với hiệu suất giảm khi λm di chuyển xa khỏi 0.7.
Tuy nhiên, tất cả hiệu suất vẫn dưới phương pháp dựa trên độ tương tự.
Điều này làm nổi bật thêm tầm quan trọng của tỷ lệ kết hợp
thích ứng dựa trên độ tương tự lớp, như trong phương pháp MKA của chúng tôi,
để duy trì hiệu suất mô hình trong quá trình nén.
7.4 Biến đổi Độ chính xác qua Các Môn học MMLU Khác nhau
Trong Quá trình Kết hợp Lớp
3.12%
6.25%
9.38%
12.50%
15.62%
18.75%
21.88%
25.00%
28.12%
31.25%
34.38%
37.50%
40.62%
43.75%
Tỷ lệ0.30.40.50.60.70.80.9ACCThay đổi Độ chính xác Trong Quá trình Kết hợp Lớp theo Môn học
Y học Đại học
Sinh học Đại học
Tâm lý học Trung học
Vật lý Đại học
Hình 6. Thay đổi ACC của các môn học bộ dữ liệu MMLU khác nhau trong quá trình kết hợp.
Chúng tôi kiểm tra tác động của kết hợp mô hình lên
hiệu suất qua các môn học khác nhau trong điểm chuẩn MMLU.
Hình 6 hiển thị thay đổi độ chính xác qua các môn học
như Y học Đại học, Sinh học Đại học, Tâm lý học Trung học,
và Vật lý Đại học trong các giai đoạn khác nhau của việc
kết hợp các lớp mô hình. Từ kết quả của chúng tôi, chúng tôi
quan sát thấy rằng Tâm lý học Trung học duy trì độ chính xác ổn định
với chỉ những biến động nhỏ, gợi ý hiệu suất nhất quán
và độ nhạy thấp với quá trình kết hợp. Ngược lại,
Sinh học Đại học trải qua sự giảm đáng kể trong độ chính xác
tại tỷ lệ kết hợp 12.5%, tiếp theo là sự phục hồi.
Vật lý Đại học thể hiện những biến động thường xuyên trong độ chính xác,
chỉ ra độ nhạy cao với kết hợp lớp. Ngược lại,
Y học Đại học trải qua sự tăng đều đặn trong hiệu suất
với chỉ những biến đổi nhỏ.BẢNG 3
So sánh hiệu suất của các tỷ lệ kết hợp cố định khác nhau trên Llama3-8B
sử dụng bộ dữ liệu MMLU.
CR MKA λm= 0.7 λm= 0.6 λm= 0.5 λm= 0.4
9.38 66.15 66.06(-0.09) 66.05(-0.10) 65.98(-0.17) 65.96(-0.19)
18.75 64.96 63.47(-1.49) 63.32(-1.64) 62.92(-2.04) 62.83(-2.13)
34.38 64.87 61.84(-3.03) 61.52(-3.35) 61.45(-3.42) 61.59(-3.28)
BẢNG 4
So sánh các phương pháp MKA lặp lại và không lặp lại trên
Llama3-8B sử dụng bộ dữ liệu MMLU.
CR MKA (không lặp lại) MKA (có lặp lại)
0.00 66.29 66.29
3.13 66.13 66.13
6.25 61.64 66.26
9.38 47.43 66.15
12.50 35.87 58.08
15.63 47.82 62.94
18.75 42.01 64.96
21.88 42.00 62.92
25.00 39.39 64.28
28.13 40.07 65.01
31.25 30.41 63.99
34.38 26.73 64.87
37.50 25.37 62.05

8 KẾT LUẬN
Trong bài báo này, chúng tôi đã đề xuất Căn chỉnh Kiến thức
dựa trên Đa tạp và Nén Kết hợp Lớp (MKA),
một kỹ thuật nén mô hình mới được thiết kế đặc biệt
để giảm hiệu quả kích thước của các mô hình ngôn ngữ lớn (LLM)
trong khi duy trì hiệu suất của chúng. MKA tận dụng
các kỹ thuật học đa tạp để căn chỉnh kiến thức qua các lớp
và sử dụng thước đo Cổ chai Thông tin (IB) để xác định
các lớp tương tự nhất để kết hợp. Bằng cách nắm bắt
các phụ thuộc phi tuyến phức tạp trong LLM và
tích hợp kiến thức từ các lớp tương tự, MKA đạt được
tỷ lệ nén đáng kể mà không hy sinh độ chính xác mô hình.
Chúng tôi đã tiến hành đánh giá thực nghiệm rộng rãi trên
một tập hợp đa dạng các bộ dữ liệu điểm chuẩn và các LLM
tiên tiến khác nhau để đánh giá một cách nghiêm ngặt
tính hiệu quả của MKA trong việc bảo toàn hiệu suất mô hình
trong khi giảm đáng kể kích thước mô hình. Kết quả thực nghiệm
của chúng tôi chứng minh rằng MKA nhất quán vượt trội
các phương pháp cắt tỉa hiện có và có thể đạt được
tỷ lệ nén thậm chí cao hơn khi kết hợp

--- TRANG 11 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 11
với các kỹ thuật lượng tử hóa.

LỜI CẢM ơN
Công trình này được hỗ trợ bởi Chương trình R&D Quốc gia Trọng điểm
của Trung Quốc (Số hiệu 2023YFB3307500). Công trình này cũng được hỗ trợ
bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc
(Số hiệu 62306087 và 62472121), Quỹ Khoa học Tự nhiên
tỉnh Shandong (Số hiệu ZR2023QF154), Chương trình Tài trợ
Đặc biệt của Dự án Học giả Taishan Shandong và Dự án Mở
của Phòng thí nghiệm Trọng điểm tỉnh Anhui về Tính toán
Nhận thức Đa phương thức, Đại học Anhui (Số hiệu MMC202420).
Bên cạnh đó, chúng tôi chân thành cảm ơn các nhà đánh giá ẩn danh
vì phản hồi quý giá của họ.

TÀI LIỆU THAM KHẢO
[1] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,
R. Avila, I. Babuschkin, S. Balaji, V . Balcom, P . Baltescu, H. Bao,
M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro,
C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman,
G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Camp-
bell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan,
C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen,
M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings,
J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville,
A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti,
T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P . Fishman, J. Forte,
I. Fulford, L. Gao, E. Georges, C. Gibson, V . Goel, T. Gogineni,
G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray,
R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris,
Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey,
P . Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga,
S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto,
B. Jonn, H. Jun, T. Kaftan, Łukasz Kaiser, A. Kamali, I. Kan-
itscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim,
Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Łukasz
Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger,
V . Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M.
Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P . Lue,
A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski,
B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney,
C. McLeavey, P . McMillan, J. McNeil, D. Medina, A. Mehta,
J. Menick, L. Metz, A. Mishchenko, P . Mishkin, V . Monaco,
E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mély,
A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh,
L. Ouyang, C. O'Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantu-
liano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov,
A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P .
de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V . H. Pong,
T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford,
J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross,
B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. San-
turkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam,
K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P . Shyam, S. Sidor,
E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky,
Y. Song, N. Staudacher, F. P . Such, N. Summers, I. Sutskever,
J. Tang, N. Tezak, M. B. Thompson, P . Tillet, A. Tootoonchian,
E. Tseng, P . Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone,
A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang,
B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P . Welinder,
J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich,
H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo,
K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang,
S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, và B. Zoph, "Gpt-4
technical report," 2024.[2] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,
A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal,
A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev,
A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson,
A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux,
C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret,
C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius,
D. Song, D. Pintz, D. Livshits, D. Esiobu, D. Choudhary,
D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin,
E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic,
F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon,
G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron,
I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Copet,
J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah,
J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang,
J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca,
J. Johnstun, J. Saxe, J. Jia, K. V . Alwala, K. Upasani, K. Plawiak,
K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu,
K. Bhalla, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan,
L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat,
L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri,
M. Kardas, M. Oldham, M. Rita, M. Pavlova, M. Kambadur,
M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi,
N. Bashlykov, N. Bogoychev, N. Chatterji, O. Duchenne, O. Çelebi,
P . Alrassy, P . Zhang, P . Li, P . Vasic, P . Weng, P . Bhargava,
P . Dubal, P . Krishnan, P . S. Koura, P . Xu, Q. He, Q. Dong,
R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic,
R. Raileanu, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro,
R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang, S. Hosseini,
S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie,
S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang,
S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot,
S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha,
T. Georgiou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao,
U. Karn, V . Goswami, V . Gupta, V . Ramanathan, V . Kerkez,
V . Gonguet, V . Do, V . Vogeti, V . Petrovic, W. Chu, W. Xiong,
W. Fu, W. Meers, X. Martinet, X. Wang, X. E. Tan, X. Xie,
X. Jia, X. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen,
Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan,
Z. Chen, Z. Papakipos, A. Singh, A. Grattafiori, A. Jain, A. Kelsey,
A. Shajnfeld, A. Gangidi, A. Victoria, A. Goldstand, A. Menon,
A. Sharma, A. Boesenberg, A. Vaughan, A. Baevski, A. Feinstein,
A. Kallet, A. Sangani, A. Yunus, A. Lupu, A. Alvarado, A. Caples,
A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani, A. Franco,
A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman,
A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd,
B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock,
B. Wasti, B. Spence, B. Stojkovic, B. Gamido, B. Montalvo,
C. Parker, C. Burton, C. Mejia, C. Wang, C. Kim, C. Zhou,
C. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, D. Civin,
D. Beaty, D. Kreymer, D. Li, D. Wyatt, D. Adkins, D. Xu,
D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss,
D. Wang, D. Le, D. Holland, E. Dowling, E. Jamil, E. Montgomery,
E. Presani, E. Hahn, E. Wood, E. Brinkman, E. Arcaute, E. Dunbar,
E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Ozgenel, F. Caggioni,
F. Guzmán, F. Kanayet, F. Seide, G. M. Florez, G. Schwarz,
G. Badeer, G. Swee, G. Halpern, G. Thattai, G. Herman, G. Sizov,
Guangyi, Zhang, G. Lakshminarayanan, H. Shojanazeri, H. Zou,
H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren,
H. Goldman, I. Damlaj, I. Molybog, I. Tufanov, I.-E. Veliche,
I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Asher, J.-B. Gaya,
J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul,
J. Zhong, J. Jin, J. Yang, J. Cummings, J. Carvill, J. Shepard,
J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U,
K. Saxena, K. Prasad, K. Khandelwal, K. Zand, K. Matosich,
K. Veeraraghavan, K. Michelena, K. Li, K. Huang, K. Chawla,
K. Lakhotia, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell,
L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa,
M. Avalani, M. Bhatt, M. Tsimpoukelli, M. Mankus, M. Hasson,
M. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi,
M. Keneally, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel,
M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J.
Hermoso, M. Metanat, M. Rastegari, M. Bansal, N. Santhanam,
N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier,
N. P . Laptev, N. Dong, N. Zhang, N. Cheng, O. Chernoguz,
O. Hart, O. Salpekar, O. Kalinli, P . Kent, P . Parekh, P . Saab,
P . Balaji, P . Rittner, P . Bontrager, P . Roux, P . Dollar, P . Zvyagina,

--- TRANG 12 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 12
P . Ratanchandani, P . Yuvraj, Q. Liang, R. Alao, R. Rodriguez,
R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Li, R. Hogan,
R. Battey, R. Wang, R. Maheswari, R. Howes, R. Rinott, S. J.
Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan,
S. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay,
S. Feng, S. Lin, S. C. Zha, S. Shankar, S. Zhang, S. Zhang, S. Wang,
S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe,
S. Satterfield, S. Govindaprasad, S. Gupta, S. Cho, S. Virk,
S. Subramanian, S. Choudhury, S. Goldman, T. Remez, T. Glaser,
T. Best, T. Kohler, T. Robinson, T. Li, T. Zhang, T. Matthews,
T. Chou, T. Shaked, V . Vontimitta, V . Ajayi, V . Montanez,
V . Mohan, V . S. Kumar, V . Mangla, V . Albiero, V . Ionescu,
V . Poenaru, V . T. Mihailescu, V . Ivanov, W. Li, W. Wang, W. Jiang,
W. Bouaziz, W. Constable, X. Tang, X. Wang, X. Wu, X. Wang,
X. Xia, X. Wu, X. Gao, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li,
Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Hao, Y. Qian,
Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, và
Z. Zhao, "The llama 3 herd of models," 2024. [Trực tuyến]. Có sẵn:
https://arxiv.org/abs/2407.21783
[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Ro-
driguez, A. Joulin, E. Grave, và G. Lample, "Llama: Open and
efficient foundation language models," 2023.
[4] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand
et al. , "Mixtral of experts," arXiv preprint arXiv:2401.04088 , 2024.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-
wal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , "Lan-
guage models are few-shot learners," Advances in neural informa-
tion processing systems , tập 33, tr. 1877–1901, 2020.
[6] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
A. Roberts, P . Barham, H. W. Chung, C. Sutton, S. Gehrmann
et al. , "Palm: Scaling language modeling with pathways," Journal
of Machine Learning Research , tập 24, số 240, tr. 1–113, 2023.
[7] E. M. Bender, T. Gebru, A. McMillan-Major, và S. Shmitchell,
"On the dangers of stochastic parrots: Can language models be
too big?" trong Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency , 2021, tr. 610–623.
[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von
Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. , "On
the opportunities and risks of foundation models," arXiv preprint
arXiv:2108.07258 , 2021.
[9] Y. Cheng, D. Wang, P . Zhou, và T. Zhang, "A survey of model
compression and acceleration for deep neural networks," arXiv
preprint arXiv:1710.09282 , 2017.
[10] L. Deng, G. Li, S. Han, L. Shi, và Y. Xie, "Model compression
and hardware acceleration for neural networks: A comprehensive
survey," Proceedings of the IEEE , tập 108, số 4, tr. 485–532, 2020.
[11] P . Ganesh, Y. Chen, X. Lou, M. A. Khan, Y. Yang, H. Sajjad,
P . Nakov, D. Chen, và M. Winslett, "Compressing large-scale
transformer-based models: A case study on bert," Transactions of
the Association for Computational Linguistics , tập 9, tr. 1061–1080,
2021.
[12] X. Zhu, J. Li, Y. Liu, C. Ma, và W. Wang, "A survey on model
compression for large language models," 2023.
[13] Z. Yang, Y. Zhang, D. Sui, Y. Ju, J. Zhao, và K. Liu,
"Explanation guided knowledge distillation for pre-trained
language model compression," ACM Trans. Asian Low-Resour.
Lang. Inf. Process. , tập 23, số 2, Tháng 2 2024. [Trực tuyến]. Có sẵn:
https://doi.org/10.1145/3639364
[14] X. Men, M. Xu, Q. Zhang, B. Wang, H. Lin, Y. Lu, X. Han, và
W. Chen, "Shortgpt: Layers in large language models are more
redundant than you expect," arXiv preprint arXiv:2403.03853 , 2024.
[15] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, và
K. Keutzer, "A survey of quantization methods for efficient neural
network inference," 2021.
[16] S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, và
Y. Wang, "Evaluating quantized large language models," 2024.
[17] T. Dettmers, M. Lewis, Y. Belkada, và L. Zettlemoyer, "Llm.int8():
8-bit matrix multiplication for transformers at scale," 2022.
[18] Z. Gong, J. Liu, J. Wang, X. Cai, D. Zhao, và R. Yan, "What makes
quantization for large language models hard? an empirical study
from the lens of perturbation," 2024.
[19] Y. LeCun, J. Denker, và S. Solla, "Optimal brain
damage," trong Advances in Neural Information Processing
Systems , D. Touretzky, Ed., tập 2. Morgan-Kaufmann, 1989.[Trực tuyến]. Có sẵn: https://proceedings.neurips.cc/paper_files/
paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf
[20] S. Han, H. Mao, và W. J. Dally, "Deep compression: Compressing
deep neural networks with pruning, trained quantization and
huffman coding," 2016.
[21] M. Gupta và P . Agrawal, "Compression of deep learning models
for text: A survey," ACM Transactions on Knowledge Discovery from
Data (TKDD) , tập 16, số 4, tr. 1–55, 2022.
[22] X. Ma, G. Fang, và X. Wang, "Llm-pruner: On the structural
pruning of large language models," 2023.
[23] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P . Luo,
và N. Wong, "Structured pruning for efficient generative
pre-trained language models," trong Findings of the Association
for Computational Linguistics: ACL 2023 , A. Rogers, J. Boyd-
Graber, và N. Okazaki, Eds. Toronto, Canada: Association for
Computational Linguistics, Tháng 7 2023, tr. 10 880–10 895. [Trực tuyến].
Có sẵn: https://aclanthology.org/2023.findings-acl.692
[24] T. Dettmers, A. Pagnoni, A. Holtzman, và L. Zettlemoyer, "Qlora:
Efficient finetuning of quantized llms," 2023.
[25] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P . Graf, "Pruning
filters for efficient convnets," 2017.
[26] X. Ma, G. Fang, và X. Wang, "Llm-pruner: On the structural
pruning of large language models," Advances in neural information
processing systems , tập 36, tr. 21 702–21 720, 2023.
[27] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes,
A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith
et al. , "Model soups: averaging weights of multiple fine-tuned
models improves accuracy without increasing inference time," trong
International Conference on Machine Learning . PMLR, 2022, tr.
23 965–23 998.
[28] D. Liu, Z. Wang, B. Wang, W. Chen, C. Li, Z. Tu, D. Chu, B. Li,
và D. Sui, "Checkpoint merging via bayesian optimization in llm
pretraining," arXiv preprint arXiv:2403.19390 , 2024.
[29] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, và S. Shi, "Knowledge
fusion of large language models," 2024.
[30] W. Li, Y. Peng, M. Zhang, L. Ding, H. Hu, và L. Shen, "Deep
model fusion: A survey," arXiv preprint arXiv:2309.15698 , 2023.
[31] J. B. Tenenbaum, V . d. Silva, và J. C. Langford, "A global geomet-
ric framework for nonlinear dimensionality reduction," science ,
tập 290, số 5500, tr. 2319–2323, 2000.
[32] N. Tishby, F. C. Pereira, và W. Bialek, "The information bottleneck
method," arXiv preprint physics/0004057 , 2000.
[33] T. Hastie, "The elements of statistical learning: data mining, infer-
ence, and prediction," 2009.
[34] I. T. Jolliffe, Principal component analysis for special types of data .
Springer, 2002.
[35] L. Van der Maaten và G. Hinton, "Visualizing data using t-sne."
Journal of machine learning research , tập 9, số 11, 2008.
[36] L. McInnes, J. Healy, và J. Melville, "Umap: Uniform manifold
approximation and projection for dimension reduction," arXiv
preprint arXiv:1802.03426 , 2018.
[37] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler,
F. Warner, và S. W. Zucker, "Geometric diffusions as a tool
for harmonic analysis and structure definition of data: Diffusion
maps," Proceedings of the national academy of sciences , tập 102,
số 21, tr. 7426–7431, 2005.
[38] G. Hinton, O. Vinyals, và J. Dean, "Distilling the knowledge in a
neural network," arXiv preprint arXiv:1503.02531 , 2015.
[39] Z. Huang, W. Zhu, G. Cheng, L. Li, và F. Yuan, "Mindmerger:
Efficient boosting llm reasoning in non-english languages," arXiv
preprint arXiv:2405.17386 , 2024.
[40] P . Tej Deep, R. Bhardwaj, và S. Poria, "DELLA-Merging: Re-
ducing Interference in Model Merging through Magnitude-Based
Sampling," arXiv e-prints , tr. arXiv:2406.11617, Tháng 6 2024.
[41] E. Yang, Z. Wang, L. Shen, S. Liu, G. Guo, X. Wang, và D. Tao,
"Adamerging: Adaptive model merging for multi-task learning,"
arXiv preprint arXiv:2310.02575 , 2023.
[42] Y. Zhou, L. Song, B. Wang, và W. Chen, "MetaGPT: Merging
Large Language Models Using Model Exclusive Task Arithmetic,"
arXiv e-prints , tr. arXiv:2406.11385, Tháng 6 2024.
[43] G. Ilharco, M. T. Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt,
H. Hajishirzi, và A. Farhadi, "Editing models with task arith-
metic," arXiv preprint arXiv:2212.04089 , 2022.
[44] A. Chronopoulou, J. Pfeiffer, J. Maynez, X. Wang, S. Ruder,
và P . Agrawal, "Language and task arithmetic with parameter-
efficient layers for zero-shot summarization," arXiv preprint
arXiv:2311.09344 , 2023.

--- TRANG 13 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 10 NĂM 2024 13
[45] G. Ortiz-Jimenez, A. Favero, và P . Frossard, "Task arithmetic
in the tangent space: Improved editing of pre-trained models,"
Advances in Neural Information Processing Systems , tập 36, 2024.
[46] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,
và J. Steinhardt, "Measuring massive multitask language under-
standing," arXiv preprint arXiv:2009.03300 , 2020.
[47] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al. , "Piqa: Reasoning about
physical commonsense in natural language," trong Proceedings of the
AAAI conference on artificial intelligence , tập 34, số 05, 2020, tr.
7432–7439.
[48] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, và Y. Choi, "Hel-
laswag: Can a machine really finish your sentence?" arXiv preprint
arXiv:1905.07830 , 2019.
[49] G. Lai, Q. Xie, H. Liu, Y. Yang, và E. Hovy, "Race: Large-scale
reading comprehension dataset from examinations," arXiv preprint
arXiv:1704.04683 , 2017.
[50] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, và
K. Toutanova, "Boolq: Exploring the surprising difficulty of natu-
ral yes/no questions," arXiv preprint arXiv:1905.10044 , 2019.
[51] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al. ,
"Mistral 7b," arXiv preprint arXiv:2310.06825 , 2023.
[52] A. Gromov, K. Tirumala, H. Shapourian, P . Glorioso, và D. A.
Roberts, "The unreasonable ineffectiveness of the deeper layers,"
2024. [Trực tuyến]. Có sẵn: https://arxiv.org/abs/2403.17887
[53] J. Song, K. Oh, T. Kim, H. Kim, Y. Kim, và J.-J. Kim,
"Sleb: Streamlining llms through redundancy verification and
elimination of transformer blocks," 2024. [Trực tuyến]. Có sẵn:
https://arxiv.org/abs/2402.09025
[54] B.-K. Kim, G. Kim, T.-H. Kim, T. Castells, S. Choi, J. Shin, và
H.-K. Song, "Shortened llama: Depth pruning for large language
models with comparison of retraining methods," 2024. [Trực tuyến].
Có sẵn: https://arxiv.org/abs/2402.02834
[55] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, và S. Han,
"Smoothquant: Accurate and efficient post-training quantization
for large language models," trong International Conference on Machine
Learning . PMLR, 2023, tr. 38 087–38 099.
[56] E. Frantar, S. Ashkboos, T. Hoefler, và D. Alistarh, "Gptq: Ac-
curate post-training quantization for generative pre-trained trans-
formers," arXiv preprint arXiv:2210.17323 , 2022.
[57] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, và S. Han, "Awq:
Activation-aware weight quantization for llm compression and
acceleration," arXiv preprint arXiv:2306.00978 , 2023.
[58] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos,
E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz et al. , "Jamba:
A hybrid transformer-mamba language model," arXiv preprint
arXiv:2403.19887 , 2024.
