# 2306.01708.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/merging/2306.01708.pdf
# Kích thước tệp: 1179222 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
TIES-MERGING : Giải quyết Nhiễu khi
Hợp nhất Mô hình
Prateek Yadav1Derek Tam1
Leshem Choshen2,3Colin Raffel1Mohit Bansal1
1University of North Carolina at Chapel Hill2IBM Research3MIT
leshem.choshen@ibm.com
{praty,dtredsox,craffel,mbansal}@cs.unc.edu
Tóm tắt
Học chuyển giao - tức là tiếp tục tinh chỉnh một mô hình được huấn luyện trước trên một
nhiệm vụ hạ nguồn - có thể mang lại những lợi ích đáng kể, bao gồm hiệu suất hạ nguồn
được cải thiện, hội tụ nhanh hơn và hiệu quả mẫu tốt hơn. Những lợi ích này đã dẫn đến
sự phát triển mạnh mẽ của các mô hình được tinh chỉnh cho từng nhiệm vụ cụ thể, thường
chỉ có thể thực hiện một nhiệm vụ duy nhất và không được hưởng lợi từ nhau. Gần đây,
các kỹ thuật hợp nhất mô hình đã xuất hiện như một giải pháp để kết hợp nhiều mô hình
cụ thể cho từng nhiệm vụ thành một mô hình đa nhiệm vụ duy nhất mà không cần thực
hiện huấn luyện bổ sung. Tuy nhiên, các phương pháp hợp nhất hiện có thường bỏ qua
sự nhiễu giữa các tham số của các mô hình khác nhau, dẫn đến những sụt giảm hiệu suất
lớn khi hợp nhất nhiều mô hình. Trong bài báo này, chúng tôi chứng minh rằng các kỹ
thuật hợp nhất trước đây vô tình mất thông tin có giá trị do hai nguồn nhiễu chính: (a)
nhiễu do các giá trị tham số dư thừa và (b) bất đồng về dấu của giá trị tham số nhất định
giữa các mô hình. Để giải quyết vấn đề này, chúng tôi đề xuất phương pháp của mình,
TRIM, ELECT SIGN& M ERGE (TIES-MERGING ), giới thiệu ba bước mới khi hợp
nhất mô hình: (1) đặt lại các tham số chỉ thay đổi một lượng nhỏ trong quá trình tinh
chỉnh, (2) giải quyết xung đột dấu, và (3) chỉ hợp nhất các tham số phù hợp với dấu
thống nhất cuối cùng. Chúng tôi phát hiện rằng TIES-MERGING vượt trội hơn một số
phương pháp hiện có trong các cài đặt đa dạng bao gồm nhiều phương thức, miền, số
lượng nhiệm vụ, kích thước mô hình, kiến trúc và cài đặt tinh chỉnh. Chúng tôi phân
tích thêm tác động của các loại nhiễu khác nhau đối với các tham số mô hình và làm nổi
bật tầm quan trọng của việc giải quyết nhiễu dấu.1
1 Giới thiệu
Các mô hình được huấn luyện trước (PTMs) đã trở nên phổ biến trong nhiều ứng dụng thực tế [ 91,6]. Việc
sử dụng PTMs thường liên quan đến việc tinh chỉnh chúng để chuyên biệt hóa cho một nhiệm vụ cụ thể [ 69,12],
có thể dẫn đến hiệu suất được cải thiện với ít dữ liệu có nhãn cụ thể cho nhiệm vụ hơn. Những lợi ích này đã
dẫn đến việc phát hành hàng nghìn điểm kiểm tra đã tinh chỉnh [ 81] xuất phát từ các PTMs phổ biến như ViT
[ 14] cho thị giác và T5 [ 58] cho ngôn ngữ. Tuy nhiên, việc có một mô hình được tinh chỉnh riêng biệt cho
mỗi nhiệm vụ có nhiều nhược điểm: (1) đối với mỗi ứng dụng mới, một mô hình riêng biệt phải được lưu trữ
và triển khai [ 17,89], và (2) các mô hình được huấn luyện riêng lẻ không thể tận dụng thông tin từ các nhiệm
vụ liên quan để cải thiện hiệu suất trong miền hoặc khái quát hóa ngoài miền [ 66,58,75]. Học đa nhiệm vụ
[ 66,57] có thể giải quyết những mối quan tâm này nhưng đòi hỏi huấn luyện tốn kém và truy cập đồng thời
vào tất cả các nhiệm vụ [ 17]. Hơn nữa, có thể phức tạp và tốn tài nguyên để xác định cách tốt nhất để trộn
các tập dữ liệu để đảm bảo rằng huấn luyện đa nhiệm vụ có lợi cho tất cả các nhiệm vụ [55, 54, 80, 52, 2, 17].
1Mã của chúng tôi có sẵn tại https://github.com/prateeky2806/ties-merging
Hội nghị thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2306.01708v2  [cs.LG]  27 Oct 2023

--- TRANG 2 ---
Vectơ Nhiệm vụ  Vectơ Nhiệm vụ Đã Cắt tỉa  
(1) Cắt tỉa
(2) Bầu chọn DấuGiá trị Thống nhất
Vectơ Dấu 
Vectơ Nhiệm vụ
Đã Hợp nhất (3) Hợp nhất Tách biệt: Giá trị có ảnh hưởng
: Giá trị dư thừa: Tham số: Mô hình 1
: Mô hình 2: Mô hình 3
: Mô hình Đã hợp nhấtHình 1: Mô tả các bước liên quan trong TIES-MERGING . Chúng tôi trực quan hóa mỗi tham số trong
một mô hình như một hình vuông. Các mũi tên mô tả cập nhật (vectơ nhiệm vụ, τ) cho một tham số được tạo ra
bởi việc tinh chỉnh trên các nhiệm vụ khác nhau (được mã hóa bằng màu sắc), với hướng biểu thị dấu và độ dài
biểu thị độ lớn. Đầu tiên chúng tôi cắt tỉa các giá trị vectơ nhiệm vụ dựa trên độ lớn của chúng, sau đó chúng tôi
bầu chọn dấu cho mỗi tham số ( γm, vectơ màu xanh lá cây chứa +1hoặc−1) bằng cách giải quyết xung đột dấu.
Cuối cùng, chúng tôi chỉ chọn các giá trị phù hợp với dấu được bầu chọn và lấy trung bình của chúng làm giá trị
tham số cuối cùng.

Gần đây, một lượng nghiên cứu ngày càng tăng đã tập trung vào việc hợp nhất mô hình [40]. Một ứng dụng
của việc hợp nhất liên quan đến việc kết hợp nhiều mô hình cụ thể cho từng nhiệm vụ thành một mô hình đa
nhiệm vụ duy nhất mà không cần thực hiện huấn luyện bổ sung. Các nghiên cứu trước đây hợp nhất các mô
hình bằng cách cộng các trọng số mô hình riêng lẻ với các sơ đồ trọng số khác nhau, hoặc thông qua trung
bình đơn giản [ 9,28,83], hoặc thông qua các phương tiện tinh vi hơn kết hợp tầm quan trọng của tham số [ 45]
hoặc tính đến bất biến hoán vị [ 1,31,70,74,42]. Việc kết hợp các mô hình được tinh chỉnh theo cách này có thể
được xem như việc cộng các vectơ nhiệm vụ [29] được tính bằng cách trừ các giá trị tham số của mô hình được
huấn luyện trước khỏi các giá trị của mô hình được tinh chỉnh.

Giá trị Gốc
 Không Nhiễu  Dư thừa  Xung đột Dấu Trung bình TIESGiá trị Đã hợp nhất
Mô hình 1 Mô hình 2
Hình 2: Các loại xung đột khác nhau và
đầu ra đã hợp nhất được tạo ra bởi hoặc
việc lấy trung bình hoặcTIES-MERGING .
Các tham số gây ra nhiễu được biểu thị
bằng các mũi tên có chấm.Trong khi việc lấy trung bình có trọng số của các tham số
mô hình đã được chứng minh là hiệu quả cho việc hợp
nhất, tất cả các phương pháp này đều bỏ qua khả năng
các giá trị có thể nhiễu giữa các mô hình, do đó gây hại
cho hiệu suất của mô hình đã hợp nhất. Trong bài báo
này, chúng tôi đầu tiên chứng minh rằng nhiễu có thể
bắt nguồn từ hai nguyên nhân chính (xem Hình 2), cả
hai đều có thể làm giảm độ lớn tham số trong mô hình
đã hợp nhất và loại bỏ sự phân biệt tinh tế giữa các giá
trị: (1) NHIỄU TỪ CÁC THAM SỐ DƯ THỪA : Các
nghiên cứu trước đây về việc cắt tỉa mô hình [ 25,76] đã
chỉ ra rằng trong quá trình tinh chỉnh, nhiều tham số mô
hình có thể thay đổi trong suốt quá trình tinh chỉnh [ 63]
nhưng chỉ có tác động nhỏ đến hiệu suất. Tuy nhiên, khi
hợp nhất một tham số có ảnh hưởng đối với một mô hình
nhưng dư thừa (tức là không có ảnh hưởng) đối với các
mô hình khác, giá trị có ảnh hưởng có thể bị che khuất
bởi các giá trị dư thừa, làm giảm hiệu suất tổng thể của
mô hình ( #trong Hình 2). (2) NHIỄU TỪ BẤT ĐỒNG
DẤU : Một tham số nhất định có thể có giá trị dương
cho một số mô hình và giá trị âm cho những mô hình
khác. Do đó, việc sử dụng lấy trung bình đơn giản có thể
làm tổn hại hiệu suất trên cả hai nhiệm vụ ( 7trong Hình
2). Trong cả hai tình huống này, việc đơn giản tổng hợp
các giá trị dẫn đến nhiễu làm thu nhỏ giá trị của tham số
trong mô hình đã hợp nhất. Nhiễu giữa các tham số có
ảnh hưởng này có thể giải thích tại sao khoảng cách hiệu
suất giữa mô hình đã hợp nhất và mô hình được huấn
luyện đa nhiệm vụ tăng lên khi số lượng mô hình tăng
[31].

Để giải quyết các nguồn nhiễu này, chúng tôi đề xuất phương pháp TIES-MERGING (TRIM, ELECT SIGN
& MERGE ), một phương pháp hợp nhất mô hình bằng cách kết hợp các vectơ nhiệm vụ có ba bước (được
trực quan hóa trong Hình 1): Đầu tiên, chúng tôi cắt tỉa mỗi vectơ nhiệm vụ để chỉ giữ lại các giá trị tham số
có ảnh hưởng bằng cách đặt các giá trị dư thừa trong mỗi vectơ nhiệm vụ về không (hoặc tương đương, đặt
lại giá trị tham số được tinh chỉnh về giá trị từ mô hình được huấn luyện trước). Sau bước này, xung đột dấu
vẫn có thể tồn tại giữa các giá trị tham số có ảnh hưởng, như được trực quan hóa trong Hình 4. Do đó, bước
thứ hai của chúng tôi giải quyết xung đột dấu giữa các giá trị khác nhau và bước cuối cùng chỉ lấy trung bình
các tham số có dấu phù hợp với hướng di chuyển tổng lớn nhất giữa các mô hình.

--- TRANG 3 ---
Chúng tôi chứng minh tính hiệu quả của phương pháp TIES-MERGING đề xuất trong các thiết lập khác nhau
với: (1) các phương thức khác nhau, bao gồm điểm chuẩn ngôn ngữ và thị giác, (2) kích thước và họ mô hình
riêng biệt, như T5-base và T5-large [ 58] cũng như ViT-B/32 và ViT-L/14 [ 14], (3) các nhiệm vụ trong miền
và ngoài miền, (4) tinh chỉnh đầy đủ hoặc tinh chỉnh hiệu quả tham số, và (5) trong sự hiện diện hoặc vắng
mặt của một tập xác thực để thiết lập các siêu tham số hợp nhất. Chúng tôi chỉ ra rằng TIES-MERGING vượt
trội hơn các phương pháp hợp nhất khác, như Task Arithmetic [ 29], RegMean [ 31], Fisher Merging [ 45], và
lấy trung bình trọng số [ 9,82] trong tất cả các cài đặt thực nghiệm này. Đáng chú ý, đối với đánh giá trong
miền, TIES-MERGING vượt trội hơn đường cơ sở mạnh nhất trung bình 2.3% và 1.7% tuyệt đối trong các
cài đặt NLP và thị giác (Bảng 1), tương ứng. Đối với khái quát hóa ngoài miền (Bảng 2), TIES-MERGING
vượt trội hơn đường cơ sở mạnh nhất 1.0%và4.4%tuyệt đối cho các mô hình T5-base và T5-large tương ứng.
Trong Phần 7, chúng tôi thực hiện các phép loại bỏ trên các thành phần phương pháp của mình và chứng minh
tác động của các loại nhiễu khác nhau đối với các giá trị tham số. Ngoài ra, chúng tôi thể hiện lợi thế gia tăng
của TIES-MERGING so với task arithmetic [ 29] khi số lượng nhiệm vụ tăng lên. Cuối cùng, chúng tôi kiểm
tra tầm quan trọng của việc có được vectơ dấu đúng. Kết quả và phân tích của chúng tôi thiết lập TIES-
MERGING như một phương pháp mạnh mẽ và hiệu quả để kết hợp các mô hình được tinh chỉnh thành một
mô hình đa nhiệm vụ duy nhất.

2 Nghiên cứu Liên quan
Cảnh quan Mất mát và Nội suy Trọng số. Mặc dù hàm mất mát của một mạng nơ-ron thường là không
lồi, nghiên cứu gần đây đã chứng minh rằng các giá trị tham số từ các lần chạy huấn luyện khác nhau đôi
khi có thể được nội suy mà không làm tăng mất mát (tức là chúng được kết nối chế độ ) [15, 20,21,32,22]. Ví
dụ, Frankle et al. [19] đã chỉ ra rằng nếu một phần của quỹ đạo tối ưu hóa được chia sẻ giữa hai mạng nơ-
ron thì chúng có thể được nội suy mà không làm giảm độ chính xác. Mặt khác, Neyshabur et al. [48] đã chỉ
ra rằng việc nội suy ngây thơ hai mạng nơ-ron với quỹ đạo tối ưu hóa hoàn toàn tách biệt có thể dẫn đến
sự sụt giảm thảm khốc trong độ chính xác của chúng. Entezari et al. [16] đưa ra giả thuyết rằng nếu chúng
ta tính đến tính đối xứng hoán vị của mạng nơ-ron, thì tất cả mạng nơ-ron của một kiến trúc nhất định được
huấn luyện trên cùng một tập dữ liệu đều được kết nối chế độ tuyến tính. Do đó, Ainsworth et al. [1], Singh
và Jaggi [70], Wang et al. [79] đã sử dụng các kỹ thuật dựa trên việc tìm hoán vị [ 79,1] và vận chuyển tối
ưu [ 70] để căn chỉnh tốt hơn các mạng nơ-ron được huấn luyện từ đầu để chúng có thể được nội suy mà
không làm tăng mất mát.
Hợp nhất Mô hình và Các Trường hợp Sử dụng Khác nhau. Các mô hình được tinh chỉnh khác nhau được
khởi tạo từ cùng một mô hình được huấn luyện trước thực sự chia sẻ một phần của quỹ đạo tối ưu hóa, và do
đó thường có thể được hợp nhất mà không cần tính đến tính đối xứng hoán vị [ 82,83,29,31]. Do đó, việc hợp
nhất các mô hình được tinh chỉnh có thể cải thiện hiệu suất trên một nhiệm vụ mục tiêu duy nhất [ 30,23,82,9],
cải thiện khái quát hóa ngoài miền [ 31,29,7,4,60,59], tạo ra các mô hình đa nhiệm vụ từ các nhiệm vụ khác
nhau [ 31,29,38], cho học liên hợp [ 46,41], nén [ 39], hợp nhất mô hình đa phương thức [ 72], học liên tục [
86,85], và các cài đặt khác [ 38,13]. Phạm vi ứng dụng đã dẫn đến sự phát triển mạnh mẽ của các phương
pháp cải thiện vượt ra ngoài việc lấy trung bình tham số đơn giản. RegMean [31] đề xuất một giải pháp dạng
đóng cho các tham số của mô hình đã hợp nhất bằng cách giải quyết một bài toán hồi quy tuyến tính cục bộ
cho mỗi lớp tuyến tính riêng lẻ trong mô hình. Tuy nhiên, điều này đòi hỏi truyền thống kê dữ liệu bổ sung
có cùng kích thước với mô hình và đòi hỏi các bước suy luận bổ sung để tính toán chúng. Fisher Merging [45]
vượt ra ngoài việc lấy trung bình đơn giản để xác định tầm quan trọng của các tham số riêng lẻ bằng cách sử
dụng Ma trận Thông tin Fisher [ 18,3,34] và sử dụng nó để cân các tham số trong mỗi mô hình khi hợp nhất.
Tuy nhiên, điều này cho thấy ít lợi ích khi hợp nhất nhiều điểm kiểm tra và cũng đòi hỏi tính toán gradient
có chi phí bộ nhớ cao. Task Arithmetic [29] trình bày một phương pháp hợp nhất mô hình bằng cách tạo ra
các vectơ nhiệm vụ và thực hiện các phép toán số học, như phép cộng, để có được một điểm kiểm tra đa nhiệm
vụ. Một nghiên cứu đồng thời của Ortiz-Jiménez et al. [51] cung cấp những hiểu biết lý thuyết về việc hợp
nhất mô hình dựa trên thuộc tính phân tách trọng số phát sinh trong quá trình huấn luyện trước. Họ đã chỉ ra
rằng việc tinh chỉnh mô hình trong không gian tiếp tuyến của chúng tăng cường thuộc tính này, dẫn đến các
mô hình đã hợp nhất tốt hơn. Phương pháp của chúng tôi tuân theo các nghiên cứu trước đây về hợp nhất mô
hình nhưng bổ sung tính đến sự nhiễu giữa các tham số khác nhau trong quá trình hợp nhất.

--- TRANG 4 ---
0 20 40 60 80 1005560657075
Giữ lại K% Tham số Hàng ĐầuHiệu suất Trung bình

Hình 3: Hiệu suất phụ thuộc vào một phần
nhỏ các tham số có độ lớn cao. Đối với mỗi
vectơ nhiệm vụ, chúng tôi chỉ giữ lại các tham
số lớn nhất - top-k%và vẽ hiệu suất trung bình
trên mười một nhiệm vụ. Việc chỉ giữ lại top-
20% tham số không làm giảm hiệu suất.

2 3 4 5 6 7 8 9 10 1100.10.20.3
Số Mô hìnhTỷ lệ Tham số 
 với Xung đột

Hình 4: Xung đột dấu xảy ra ngay cả sau khi
cắt tỉa và tăng theo số lượng mô hình. Chúng
tôi vẽ tỷ lệ tham số có xung đột dấu sau khi
cắt tỉa so với số lượng mô hình được hợp nhất.

3 Bối cảnh và Động lực
Thiết lập Bài toán Cho một tập các nhiệm vụ {t1, . . . , t n}và một mô hình được huấn luyện trước như T5
[ 58] hoặc ViT [ 14], chúng tôi hoặc tinh chỉnh toàn bộ mô hình hoặc sử dụng một phương pháp tinh chỉnh
hiệu quả tham số (PEFT) [ 43,26]. Trong cả hai trường hợp, chúng tôi ký hiệu các tham số có thể huấn luyện
là θ, khởi tạo là θinit, và các tham số được tinh chỉnh là θft. Trong bài báo này, chúng tôi giả định có quyền
truy cập vào các tham số mô hình được tinh chỉnh θft cho nhiều nhiệm vụ và tạo ra một phương pháp để hợp
nhất các trọng số của các mô hình này thành một mô hình đa nhiệm vụ duy nhất thành thạo trên cả tập dữ
liệu trong miền và ngoài miền. Chúng tôi tuân theo Ilharco et al. [29] và thực hiện hợp nhất với các vectơ
nhiệm vụ. Cụ thể, đối với một nhiệm vụ t, vectơ-nhiệm vụ τt∈Rdđược định nghĩa là τt=θt
ft−θt
init. Phép
toán này cho phép chúng tôi tập trung vào những thay đổi xảy ra trong giai đoạn tinh chỉnh của mỗi mô hình
cụ thể cho từng nhiệm vụ và tương đương với việc tính trung bình có trọng số của các trọng số mô hình với
việc chia tỷ lệ phù hợp.

Thuật toán 1 Quy trình TIES-MERGING .
Đầu vào: Các mô hình đã tinh chỉnh {θt}n
t=1, Khởi tạo θinit,
k, và λ.
Đầu ra: Mô hình Đã hợp nhất θm
forall tin1, ..., n do
▷Tạo vectơ nhiệm vụ.
τt=θt−θinit
▷Bước 1: Cắt tỉa tham số dư thừa.
ˆτt←keep_topk_reset_rest_to_zero (τt, k)
ˆγt←sgn( ˆτt)
ˆµt← |ˆτt|
end
▷Bước 2: Bầu chọn Dấu Cuối cùng.
γm=sgn(Pn
t=1ˆτt)
▷Bước 3: Hợp nhất Tách biệt.
forall pin1, ..., d do
Ap={t∈[n]|ˆγp
t=γp
m}
τp
m=1
|Ap|P
t∈Apˆτp
t
end
▷Lấy điểm kiểm tra đã hợp nhất
θm←θinit+λ∗τm
return θm

Dư thừa trong Tham số Mô hình.
Đầu tiên, chúng tôi chứng minh rằng trong một
vectơ nhiệm vụ nhất định, nhiều giá trị là dư
thừa (được ký hiệu bằng#trong Hình 2), và
việc loại bỏ chúng không ảnh hưởng đến hiệu
suất của nhiệm vụ. Cụ thể, Hình 3 cho thấy
hiệu suất trung bình trên mười một mô hình
cụ thể cho từng nhiệm vụ khi "cắt tỉa" mỗi vectơ
nhiệm vụ để chỉ giữ lại các giá trị có độ lớn
lớn nhất top- k%và đặt lại phần còn lại về giá
trị ban đầu của chúng (tức là đặt giá trị tương
ứng trong vectơ nhiệm vụ thành 0). Hình 3 cho
thấy hiệu suất trung bình trên các giá trị k khác
nhau, chứng minh rằng việc chỉ giữ lại top-20%
giá trị mang lại kết quả tương đương với việc
giữ lại tất cả tham số. Để biết thêm chi tiết và
kết quả trên mô hình T5, vui lòng tham khảo
Phụ lục C.3. Điều này cho thấy nhiều thay đổi
tham số được đưa ra trong quá trình tinh chỉnh
là dư thừa. Do đó, việc bỏ qua những giá trị đó
trong quá trình hợp nhất có thể ngăn chặn sự
nhiễu với các tham số có ảnh hưởng mà không
làm tổn hại đến hiệu suất của nhiệm vụ.

Bất đồng giữa Dấu Tham số: Các mô hình được tinh chỉnh khác nhau có thể đưa ra những thay đổi đối
lập với một tham số trong vectơ nhiệm vụ của chúng, gây ra sự nhiễu do dấu xung đột (được ký hiệu bằng
7trong Hình 2). Hình 4 trình bày phân tích về tần suất xung đột dấu khi hợp nhất số lượng mô hình khác
nhau. Chúng tôi đầu tiên cắt tỉa các vectơ nhiệm vụ cho mười một nhiệm vụ bằng cách chỉ giữ lại top 20%
tham số có ảnh hưởng. Sau đó, chúng tôi vẽ tỷ lệ phần trăm tham số có xung đột dấu khi chúng tôi

--- TRANG 5 ---
tăng số lượng mô hình được hợp nhất từ 2 lên 11. Đáng chú ý, xung đột dấu xảy ra ngay cả khi chỉ hợp nhất 2
mô hình từ các nhiệm vụ khác nhau hoặc khi hợp nhất nhiều mô hình từ cùng một nhiệm vụ (xem Hình 10 Phụ
lục), và khả năng xung đột dấu tăng lên với số lượng mô hình được hợp nhất. Để biết thêm chi tiết và kết quả
trên mô hình T5, vui lòng tham khảo Phụ lục C.3.

4 T IES-MERGING : TRIM, ELECT SIGN & M ERGE
Để giải quyết các vấn đề được đề cập ở trên, chúng tôi trình bày TIES-MERGING (TRIM, ELECT SIGN&
M ERGE ), nhằm giải quyết các loại nhiễu được đề cập ở trên trước khi thực hiện hợp nhất.

4.1 Sơ bộ
Một vectơ nhiệm vụ τt∈Rdbiểu thị một hướng và lượng di chuyển cần thiết trong không gian tham số d
chiều so với khởi tạo dẫn đến một vùng mất mát thấp cho nhiệm vụ t. Mỗi mục trong τt(tương ứng với một
tham số cụ thể) có thể được coi như một trục trong không gian d chiều. Dấu của một tham số biểu thị hướng
dọc theo trục này (dương hoặc âm) làm giảm mất mát trên nhiệm vụ t. Do đó, một vectơ-nhiệm vụ nhất định
τtcó thể được phân tách thành một vectơ dấu γt∈Rdvà một vectơ độ lớn µt∈Rdnhư τt=γt⊙µt, trong đó
⊙là tích theo từng phần tử. Chính thức, γt=sgn(τt), trong đó sgn(x)∗ |x|=xvà trả về giá trị +1,0, hoặc−1.
Vectơ độ lớn µtđược định nghĩa là µt=|τt|và giá trị µi
tcho chúng ta biết di chuyển cần thiết trong chiều thứ
i từ khởi tạo.

4.2 Các Bước trong T IES-MERGING
Để hợp nhất nhiều mô hình cụ thể cho từng nhiệm vụ {θt}n
t=1, chúng tôi đầu tiên tạo ra các vectơ nhiệm vụ
tương ứng {τt}n
t=1. Cho các vectơ nhiệm vụ này, phương pháp TIES-MERGING tuân theo ba bước theo thứ
tự để thực hiện hợp nhất (xem Hình 1 cho sơ đồ và Thuật toán 1):

1.Cắt tỉa: Đối với mỗi nhiệm vụ t, chúng tôi cắt tỉa các tham số dư thừa từ vectơ nhiệm vụ τtđể tạo ra ˆτt
bằng cách giữ lại các giá trị top- k%theo độ lớn của chúng và cắt tỉa (100−k)%tham số dư thừa ở dưới
bằng cách đặt lại chúng về 0. Điều này có thể được phân tách thêm thành ˆτt= ˆγt⊙ˆµt.

2.Bầu chọn: Tiếp theo, chúng tôi tạo ra một vectơ dấu được bầu chọn tổng hợp γmcho mô hình đã hợp
nhất giải quyết sự bất đồng trong dấu cho mỗi tham số ptrên các mô hình khác nhau. Để tạo ra vectơ
dấu được bầu chọn, chúng tôi chọn dấu có tổng độ lớn cao nhất trên tất cả các mô hình liên quan. Đối
với mỗi tham số p∈ {1,2, . . . , d }, chúng tôi phân tách các giá trị {ˆτp
t}n
t=1dựa trên dấu của chúng (
+1hoặc −1) và lấy tổng của chúng để tính tổng khối lượng (tức là, tổng độ lớn) theo hướng dương và
hướng âm. Sau đó chúng tôi gán γp
mnhư dấu có di chuyển tổng lớn hơn. Điều này có thể được tính toán
hiệu quả bằng cách sử dụng γp
m=sgn(Pn
t=1ˆτp
t).

3.Hợp nhất Tách biệt: Sau đó, đối với mỗi tham số p, chúng tôi tính toán trung bình tách biệt bằng cách
chỉ giữ lại các giá trị tham số từ các mô hình có dấu giống với dấu được bầu chọn tổng hợp và tính
trung bình của chúng. Chính thức, cho Ap={t∈[n]|ˆγp
t=γp
m}, thì τp
m=1
|Ap|P
t∈Apˆτp
t. Lưu ý rằng
trung bình tách biệt luôn bỏ qua các giá trị bằng không.

Cho vectơ nhiệm vụ đã hợp nhất cuối cùng τm, chúng tôi chia tỷ lệ và thêm nó vào các giá trị tham số ban
đầu để có được các tham số mô hình đã hợp nhất θmnhư θm=θinit+λ∗τm, trong đó λlà một siêu tham số
chia tỷ lệ (như được sử dụng trong nghiên cứu trước [29]).

5 Thiết lập Thí nghiệm
Phương pháp Cơ sở. Chúng tôi so sánh TIES-MERGING với bốn phương pháp hợp nhất cơ sở: (1) Lấy
Trung bình Đơn giản [9,82] tính trung bình theo từng phần tử của tất cả các mô hình riêng lẻ và có thể được
biểu thị là θm=Pn
t=1θt/n. (2) Fisher Merging [45] sử dụng một xấp xỉ đường chéo của Ma trận Thông tin
Fisher ˆFt[34,3,18] để đo tầm quan trọng của mỗi tham số cho nhiệm vụ t, trong đó ˆFt=Ex∼DtEy∼pθt(y|x)∇θt(logpθt(y|xt))2.
Mô hình đã hợp nhất cuối cùng được thu được bằng cách cân lại mỗi tham số trong mỗi mô hình được tinh
chỉnh bằng giá trị tương ứng trong ma trận Fisher xấp xỉ của mô hình như θm=Pn
t=1ˆFtθt/Pn
t=1ˆFt. (3)
RegMean [31] tính toán một giải pháp dạng đóng cho

--- TRANG 6 ---
Phương pháp ( ↓) Xác thực PEFT Tinh chỉnh Đầy đủ
Mô hình ( →) (IA)3T5-Base T5-Large ViT-B/32 ViT-L/14
TINH CHỈNH - 71.4 82.8 88.8 90.5 94.2
ĐA NHIỆM VỤ - 73.1 83.6 88.1 88.9 93.5
LẤY TRUNG BÌNH [82, 9] ✗ - 65.9 59.6 65.8 79.6
TASK ARITHMETIC [29] ✗ - 73.2 73.5 60.4 83.3
TIES-MERGING ✗ - 69.7 [-3.2] 74.4 [+0.9] 72.4 [+6.6] 86.0 [+2.7]
FISHER MERGING [45] ✓ 62.2 68.9 64.6 68.3 82.2
REGMEAN [31] ✓ 58.0 71.2 73.2 71.8 83.7
TASK ARITHMETIC [29] ✓ 63.9 73.2 73.3 70.1 84.5
TIES-MERGING ✓ 66.4 [+2.5] 73.9 [+0.7] 76.9 [+3.6] 73.6 [+1.8] 86.0 [+1.5]

Bảng 1: So sánh các phương pháp hợp nhất mô hình trên nhiều cài đặt tinh chỉnh và phương thức
(NLP và Thị giác) có và không có sự sẵn có của tập xác thực.

một bài toán hồi quy bình phương tối thiểu nhằm giảm thiểu khoảng cách giữa các kích hoạt của mô hình đã
hợp nhất và các kích hoạt của các mô hình riêng lẻ như θm= (Pn
t=1XT
tXt)−1Pn
t=1(XT
tXtθt), trong đó
Xtlà kích hoạt đầu vào của một lớp nhất định. (4) Task Artithmetic [29] chia tỷ lệ và sau đó cộng các vectơ
nhiệm vụ vào mô hình ban đầu để tạo ra mô hình đã hợp nhất như θm=θinit+λ∗Pn
t=1τt. Ngoài các đường
cơ sở này, chúng tôi trình bày hiệu suất của các mô hình được tinh chỉnh riêng lẻ tham gia vào quá trình hợp
nhất cũng như hiệu suất của một mô hình đa nhiệm vụ được huấn luyện trên việc nối các tập dữ liệu của tất
cả các nhiệm vụ. Để biết thêm chi tiết về tài nguyên tính toán, giấy phép tập dữ liệu và quy trình tinh chỉnh,
tham khảo Phụ lục C.1, C.2, và C.6.

Hợp nhất khi Vắng mặt Tập Xác thực. Các nghiên cứu trước [ 29,45,82] về hợp nhất mô hình giả định có
quyền truy cập vào một tập xác thực, được sử dụng để tính toán ma trận Fisher hoặc điều chỉnh các siêu tham
số. Để tránh nhu cầu về tập xác thực, RegMean [ 31] đề xuất lưu trữ và truyền các ma trận tích vô hướng của
dữ liệu huấn luyện cho mỗi nhiệm vụ có cùng kích thước với mô hình gốc. Điều này có thể nhanh chóng trở
nên tốn kém đối với các mô hình lớn vì việc lưu trữ và truyền tỷ lệ tuyến tính với kích thước mô hình và số
lượng nhiệm vụ.

Để xem xét cài đặt khi không có tập xác thực nào có sẵn, chúng tôi đã phát triển một công thức chung của
TIES-MERGING với các siêu tham số cố định có thể được áp dụng trong bất kỳ cài đặt nào mà không cần
điều chỉnh siêu tham số trên tập xác thực. Công thức giữ lại top- 20% tham số trong vectơ nhiệm vụ đặt lại
phần còn lại về 0 và đặt λ= 1. Chúng tôi chọn công thức này dựa trên kết quả trong cài đặt tinh chỉnh hiệu
quả tham số (PEFT), vì vậy chúng tôi chỉ áp dụng nó cho các cài đặt chưa được thấy của tinh chỉnh mô hình
đầy đủ trên các mô hình ViT (thị giác) và T5 (ngôn ngữ). Chúng tôi cũng so sánh TIES-MERGING với
phương pháp Task Arithmetic mà không có tập xác thực bằng cách sử dụng giá trị được khuyến nghị λ= 0.4[29].
Để biết thêm chi tiết về cách công thức này được tạo ra, vui lòng tham khảo Phụ lục C.4.

6 Kết quả Chính
Mục tiêu chính của chúng tôi là hợp nhất nhiều mô hình cụ thể cho từng nhiệm vụ thành một mô hình đa
nhiệm vụ duy nhất có thể hoạt động tốt trong cả các tình huống trong miền và ngoài miền. Trong phần này,
chúng tôi đánh giá hiệu suất của TIES-MERGING với các phương pháp khác trên nhiều cài đặt thí nghiệm
khác nhau.

Hợp nhất Mô hình PEFT. Xem xét cài đặt khi các vectơ nhiệm vụ được tính toán dựa trên các tham số được
đưa ra trong quá trình tinh chỉnh hiệu quả tham số. Cụ thể, chúng tôi tập trung vào (IA)3[43], một phương
pháp PEFT chia tỷ lệ các kích hoạt mô hình cơ sở với các vectơ đã học. Chúng tôi tuân theo Liu et al. [43] và
sử dụng T0-3B [ 66] làm mô hình cơ sở và tinh chỉnh các mô hình (IA)3trên phần chia huấn luyện của mười
một tập dữ liệu bao gồm hoàn thành câu (COPA [ 61], H-SWAG [ 88], và các tập dữ liệu Story Cloze [ 68]),
suy luận ngôn ngữ tự nhiên (ANLI [ 49], CB [ 44], và RTE [ 11]), giải quyết đồng tham chiếu (WSC [ 37] và
Winogrande [64]), và phân biệt nghĩa từ (WiC [ 53]). Khi tinh chỉnh các tham số (IA)3được thêm vào mô
hình T0-3B, chúng tôi sử dụng các mẫu gợi ý từ Public Pool of Prompts (P3 [ 5]) để chuyển đổi mỗi ví dụ
trong mỗi tập dữ liệu thành định dạng văn bản-thành-văn bản có gợi ý trong đó mỗi nhãn tương ứng với một
chuỗi khác nhau. Đối với các thí nghiệm với (IA)3, cho mỗi tập dữ liệu, chúng tôi báo cáo điểm số trung vị
trên tất cả các mẫu.

--- TRANG 7 ---
Mô hình T5-Base T5-Large
Zeroshot 31.1 27.6
Lấy Trung bình Đơn giản [9, 82] 31.7 30.4
Fisher [45] 33.8 32.0
RegMean [31] 34.3 36.0
Task Arithmetic [29] 31.9 32.3
TIES-MERGING 35.3 [+1.0] 40.4 [+4.4]

Bảng 2: TIES-MERGING khái quát hóa tốt hơn.
Khái quát hóa Ngoài Phân phối cho T5-Base
và T5-Large trên sáu nhiệm vụ được giữ lại.

2 3 4 5 6 70.70.80.911.1
Lấy Trung bình Đơn giản Task Arithmetic TIES
Số Nhiệm vụHiệu suất Chuẩn hóa Trung bình

Hình 5: TIES-MERGING mở rộng tốt hơn. Hiệu
suất trung bình khi hợp nhất một số lượng nhiệm
vụ khác nhau.

Bảng 1 sử dụng TIES-MERGING để hợp nhất các mô hình được huấn luyện với (IA)3vượt trội hơn hiệu
suất của tất cả các phương pháp hợp nhất khác – với tập xác thực, TIES-MERGING cho thấy sự cải thiện
trung bình 2.5%trên 11 nhiệm vụ so với đường cơ sở hàng đầu. Để biết kết quả chi tiết, tham khảo Bảng 8
Phụ lục.

Hợp nhất Mô hình Thị giác Được Tinh chỉnh Đầy đủ. Đối với phân loại hình ảnh, chúng tôi tuân theo cài
đặt thí nghiệm từ Ilharco et al. [29,28]. Chúng tôi sử dụng hai biến thể của mô hình CLIP [ 56] với các mô
hình ViT-B/32 và ViT-L/14 [ 14] làm bộ mã hóa thị giác. Tiếp theo, chúng tôi tinh chỉnh bộ mã hóa thị giác
trên tám nhiệm vụ xuất phát từ Ilharco et al. [28,29], Radford et al. [56] trong khi giữ bộ mã hóa văn bản cố
định. Cài đặt này xem xét nhiều lĩnh vực phân loại như viễn thám, phân loại giao thông và nhận dạng hình
ảnh vệ tinh. Cụ thể, chúng tôi làm việc với các tập dữ liệu sau: Cars [ 35], DTD [10], EuroSAT [24], GTSRB
[71], MNIST [36], RESISC45 [8], SUN397 [84], và SVHN [47].

Bảng 1 cho thấy việc sử dụng TIES-MERGING để hợp nhất các mô hình ViT-B/32 và ViT-L/14 được tinh
chỉnh đầy đủ dẫn đến sự cải thiện trung bình 1.8%và1.5%trên 8 nhiệm vụ, với sự sẵn có của tập xác thực.
Trong trường hợp vắng mặt tập xác thực, TIES-MERGING cải thiện 6.6%và2.7%so với các phương pháp
khác cho ViT-B/32 và ViT-L/14, tương ứng. Đáng chú ý, TIES-MERGING không có xác thực vượt trội hơn
Task Arithmetic [ 29] có xác thực 2.3%và1.5%cho ViT-B/32 và ViT-L/14. Để biết kết quả chi tiết hơn,
tham khảo Bảng 11 và 12 Phụ lục.

Hợp nhất Mô hình NLP Được Tinh chỉnh Đầy đủ. Đối với lĩnh vực NLP, chúng tôi sử dụng các mô hình
T5-base và T5-large [57], là các transformers mã hóa-giải mã [ 77] được huấn luyện trước thông qua mô hình
hóa ngôn ngữ có mặt nạ trên một kho văn bản lớn. Chúng tôi tinh chỉnh cả T5-base và T5-large trên bảy
nhiệm vụ: trả lời câu hỏi (QASC [ 33], WikiQA [ 87], và QuaRTz [ 73]), Nhận dạng Paraphrase (PAWS [
90]), Hoàn thành Câu (Story Cloze [68]), và Giải quyết Đồng tham chiếu (Winogrande [64] và WSC [37]).

Bảng 1 cho thấy việc sử dụng TIES-MERGING trên các mô hình T5-base và T5-large với tập xác thực tạo
ra sự cải thiện 0.7%và3.6%tương ứng trên 7 nhiệm vụ so với hiện trạng nghệ thuật. Hơn nữa, đối với T5-
large TIES-MERGING không có xác thực vượt trội hơn tất cả các đường cơ sở (ngay cả với tập xác thực)
1.1%. Để biết kết quả chi tiết hơn, tham khảo Bảng 9 và 10 Phụ lục.

Khái quát hóa Ngoài Miền. Trong nhiều trường hợp sử dụng, các mô hình đa nhiệm vụ được sử dụng vì
khả năng khái quát hóa tốt hơn của chúng đối với sự dịch chuyển miền. Do đó, chúng tôi sử dụng các mô
hình T5-base và T5-large được hợp nhất trên bảy tập dữ liệu trong miền từ các thí nghiệm trước đây và đánh
giá chúng trên sáu tập dữ liệu được giữ lại từ hỗn hợp T0 [ 65] để đo khái quát hóa ngoài miền. Cụ thể,
chúng tôi báo cáo hiệu suất trung bình trên các nhiệm vụ và tập dữ liệu sau: Cosmos QA [ 27], Social IQA [
67], và QuAIL [62] cho trả lời câu hỏi; WiC [ 53] cho phân biệt nghĩa từ; và COPA [ 61], và H-SWAG [88]
cho hoàn thành câu. Bảng 2 cho thấy TIES-MERGING vượt trội hơn đường cơ sở mạnh nhất cho cả T5-base
và T5-Large 1.0%và4.4%tương ứng, chứng minh khái quát hóa ngoài miền tốt hơn. Để biết kết quả chi tiết
hơn, vui lòng tham khảo Phụ lục B.6 và Bảng 13 và 14.

Hợp nhất Số Lượng Nhiệm vụ Khác nhau. Chúng tôi đánh giá hiệu suất của mô hình đã hợp nhất trên các
nhiệm vụ trong miền khi chúng tôi thay đổi số lượng nhiệm vụ được hợp nhất. Trong Hình 5, chúng tôi chuẩn
hóa độ chính xác của mỗi nhiệm vụ bằng hiệu suất mô hình được tinh chỉnh của nó và báo cáo độ chính xác
chuẩn hóa trung bình trên các nhiệm vụ trong miền. Chúng tôi so sánh với đường cơ sở mạnh nhất – Task
Arithmetic [ 29] – cũng như

--- TRANG 8 ---
RTE MRPC WNLI
Lấy trung bình 59.9 78.2 56.3
Fisher 65.7 81.4 52.1
Tổng hợp 70.8 86.0 45.1
Task Arithmetic 71.8 86.0 59.2
TIES-MERGING 72.2 86.8 58.8

Bảng 3: Thiết lập thí nghiệm model soups. TIES
cải thiện hiệu suất khi hợp nhất các điểm kiểm
tra trên cùng các nhiệm vụ. Đối với mỗi nhiệm
vụ, chúng tôi hợp nhất 10 điểm kiểm tra từ
Huggingface hub và đánh giá trên một nhiệm
vụ mà chúng được huấn luyện.

Phương pháp Khởi tạo RTE MRPC WNLI
PTM Init 66.4 81.8 56.3
Trung bình 75.8 86.5 56.3
Task Arithmetic 78.3 86.2 50.7
TIES-MERGING 80.1 88.0 54.9

Bảng 4: Một mô hình đã hợp nhất TIES là một
khởi tạo tốt hơn cho tinh chỉnh. Đối với mỗi
nhiệm vụ, chúng tôi hợp nhất các điểm kiểm tra
từ 7 nhiệm vụ GLUE khác và sau đó tinh chỉnh
và đánh giá trên nhiệm vụ được chọn.

0 1 >100.020.040.060.08Trung bình Cắt tỉa + Trung bình Tách biệt
Tham số với Giá trị Khác không

Độ lớn trung bình

(a) Nhiễu Tham số Dư thừa.

0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.000.050.10.15Trung bình
Bầu chọn + Trung bình Tách biệt
Bins của Thỏa thuận Dấu

Độ lớn trung bình

(b) Nhiễu Dấu.

Hình 6: Cắt tỉa Tham số và Bầu chọn Dấu ngăn chặn nhiễu. Minh họa nhiễu tham số giữa các mô
hình khác nhau và tác động của nó đối với giá trị tham số. Trung bình Tiêu chuẩn (đỏ) thu nhỏ độ
lớn và làm điều đó nhiều hơn khi có ít thỏa thuận về dấu (phải) hoặc tham số có ảnh hưởng đối với
nhiều nhiệm vụ (trái).

lấy trung bình đơn giản [ 82]. Mỗi điểm dữ liệu biểu thị việc hợp nhất một tập con của các nhiệm vụ, với
đường liền nét biểu thị hiệu suất trung bình trên nhiều tập con. Để biết kết quả tương tự với mô hình T5-
base, vui lòng tham khảo Phụ lục C.5 và Hình 13.

Từ Hình 5, chúng tôi quan sát được những điều sau: (1) Khi số lượng nhiệm vụ được hợp nhất tăng lên, hiệu
suất của tất cả các phương pháp giảm xuống. (2) Khi hợp nhất hai nhiệm vụ, cả TIES-MERGING và Task
Arithmetic đạt được độ chính xác chuẩn hóa trung bình gần bằng một, cho thấy mất mát hiệu suất không đáng
kể. Ngược lại, Lấy Trung bình Đơn giản bị sụt giảm hiệu suất 10%. (3) Khi số lượng nhiệm vụ tăng lên, hiệu
suất hợp nhất của Task Arithmetic giảm nhanh hơn TIES-MERGING . Điều này gợi ý rằng nhiễu nhiệm vụ
có mặt khi hợp nhất nhiều nhiệm vụ và TIES-MERGING hiệu quả hơn trong việc giảm thiểu vấn đề này.

Hợp nhất Điểm kiểm tra của Cùng Nhiệm vụ để Có Độ bền Tốt hơn Chúng tôi thực hiện các thí nghiệm
bổ sung để hợp nhất nhiều điểm kiểm tra được huấn luyện trên cùng một nhiệm vụ (như được thực hiện trong
ModelSoups [ 82]) để xem liệu nó có thể cải thiện độ bền hay không. Thông thường, tổng hợp được sử dụng
để kết hợp các mô hình khác nhau trên cùng một nhiệm vụ để có khái quát hóa tốt hơn. Chúng tôi sử dụng
cài đặt thí nghiệm và mã từ Fisher Merging [ 45] để hợp nhất top-10 mô hình BERT cỡ base được tinh chỉnh
từ huggingface cho các tập dữ liệu RTE, MRPC, và WNLI từ GLUE. Từ kết quả được trình bày trong Bảng
3, chúng tôi quan sát rằng TIES-MERGING hoạt động tốt nhất trong tất cả các trường hợp ngoại trừ WNLI,
nơi nó chỉ hơi kém hiệu suất hơn Task Vectors. Đáng chú ý, TIES-MERGING cung cấp một sự thúc đẩy
mạnh mẽ so với cả Fisher Merging, lấy trung bình và vượt trội hơn tổng hợp trong tất cả các trường hợp.
Hơn nữa, trong Phụ lục B.4, chúng tôi chỉ ra rằng nhiễu tồn tại ngay cả giữa các điểm kiểm tra được tinh
chỉnh khác nhau của cùng các nhiệm vụ.

Hợp nhất Mô hình để Có Khởi tạo Tốt hơn. Tiếp theo, chúng tôi thực hiện các thí nghiệm theo cài đặt [9],
nơi chúng tôi hợp nhất các điểm kiểm tra từ các nhiệm vụ khác nhau để có khởi tạo tốt hơn khi tinh chỉnh
trên một nhiệm vụ hạ nguồn. Chúng tôi lấy các điểm kiểm tra bert-base-uncased được tinh chỉnh cho 8
nhiệm vụ GLUE [ 78] (wnli, sst2, rte, qnli, mrpc, cola, mnli, qqp) từ Huggingface [ 81]. Chúng tôi xem xét
ba trong số các nhiệm vụ GLUE này (RTE, MRPC, WNLI) làm nhiệm vụ hạ nguồn của chúng tôi. Khi tinh
chỉnh trên một

--- TRANG 9 ---
0.0 0.2 0.4 0.6 0.8 1.0506070Top-20 Top-30 Top-50 Bottom-80
Bottom-70 Bottom-50
Xác suất Lật Dấu

Hiệu suất Trung bình

Hình 7: Lật dấu của các tham số có độ lớn cao
dẫn đến sụt giảm hiệu suất thảm khốc. Hiệu suất
Trung bình khi lật hướng của Top-k%và Bottom-
k%tham số cho mỗi nhiệm vụ. Chúng tôi báo cáo
kết quả trung bình trên mười một nhiệm vụ (IA)3.

Phương pháp Trung bình
Được Tinh chỉnh 71.4
Đa nhiệm vụ 73.1
Lấy trung bình [9, 82] 58.0
Task Vectors [29] 63.9
TIES-MERGING 66.4
TIES-MERGING (Oracle Sign) 72.0 [+5.6]

Bảng 5: TIES-MERGING có thể hoạt động
gần với các mô hình đa nhiệm vụ nếu các
dấu có thể được ước tính chính xác. Chúng
tôi sử dụng các dấu từ vectơ đa nhiệm vụ
làm dấu được bầu chọn và thực hiện hợp
nhất và báo cáo hiệu suất.

nhiệm vụ hạ nguồn cụ thể (ví dụ RTE), chúng tôi hợp nhất tất cả các điểm kiểm tra từ bảy nhiệm vụ khác
với nhau (ngoài nhiệm vụ được chọn). Từ Bảng 4, chúng tôi thấy rằng TIES-MERGING hoạt động tốt trong
cài đặt này và vượt trội hơn tất cả các phương pháp hợp nhất khác với biên độ đáng kể (ngoài Lấy Trung
bình cho WNLI).

7 Kết quả và Phân tích Bổ sung
7.1 Các Loại Nhiễu và Tác động của Chúng đối với Hợp nhất
(a) Tầm quan trọng của Việc Loại bỏ Tham số Dư thừa. Để tách biệt tốt hơn tác động của các tham số
dư thừa đối với độ lớn kết quả của các tham số đã hợp nhất, chúng tôi phân tách các tham số thành ba nhóm:
tham số dư thừa (sử dụng ngưỡng cắt tỉa 20%), tham số có ảnh hưởng đối với chính xác một mô hình, và
tham số có ảnh hưởng đối với nhiều hơn một mô hình. Sau đó chúng tôi so sánh các giá trị tham số khi chúng
được hợp nhất trực tiếp so với khi chúng được cắt tỉa trước và sau đó được hợp nhất (tách biệt) mà không
bầu chọn dấu. Cụ thể, chúng tôi chỉ lấy trung bình của các giá trị không được cắt tỉa. Kết quả được hiển thị
cho cài đặt PEFT trong Hình 6a, chứng minh rằng các tham số dư thừa gây ra nhiễu. Cụ thể, chúng tôi thấy
rằng khi một tham số không phải là tham số có ảnh hưởng đối với bất kỳ mô hình cụ thể cho từng nhiệm vụ
nào, giá trị trung bình thấp, và do đó có thể được coi là nhiễu. Tuy nhiên, khi chỉ một mô hình thấy tham số
đó có ảnh hưởng, giá trị đã hợp nhất vẫn có thể thấp vì các mô hình khác gán một giá trị nhỏ cho tham số
này. Giá trị đã hợp nhất lớn hơn khi nhiều mô hình thấy tham số đó có ảnh hưởng. Khi cắt tỉa, chúng ta thấy
sự nhiễu này chủ yếu được tránh, và kích thước trung bình chủ yếu giống nhau cho dù một hay nhiều mô hình
coi một tham số có ảnh hưởng. Điều này là do chúng ta loại bỏ tác động của các tham số nhiễu không cần
thiết làm giảm độ lớn (xem#trong Hình 2). Trong Phụ lục B.5, chúng tôi mang lại cái nhìn chi tiết hơn, bao
gồm so sánh với việc áp dụng TIES-MERGING và cho thấy việc giảm nhiễu khuyến khích sự đa dạng trong
các giá trị tham số cụ thể (std) cùng với sự tương đồng của ảnh hưởng của chúng (trung bình).

(b) Tầm quan trọng của Việc Giải quyết Nhiễu Dấu. Để định lượng tác động của nhiễu dấu, chúng tôi
nhóm các tham số theo thỏa thuận dấu của chúng . Giá trị 0.5cho thấy số lượng dấu dương và âm bằng nhau
cho một tham số nhất định trên các mô hình khác nhau, trong khi 1ngụ ý tất cả các tham số có cùng dấu.
Chúng tôi so sánh các giá trị tham số khi chúng được hợp nhất, hoặc khi bất đồng dấu được giải quyết trước
bằng bầu chọn và sau đó chúng được hợp nhất (tách biệt). Kết quả trong cài đặt PEFT được hiển thị trong
Hình 6b, nơi chúng tôi chứng minh rằng bước ELECT bảo tồn độ lớn tham số tương đối để tránh nhiễu dấu.
Cụ thể, chúng tôi thấy rằng việc giải quyết dấu tăng độ lớn tham số tổng thể trên các phạm vi thỏa thuận dấu
khác nhau. Các tham số có thỏa thuận thấp có xu hướng nhỏ hơn trung bình bất kể nhiễu. Một nguyên nhân
tiềm năng có thể là dấu từ các tham số nhiễu kéo trung bình xuống, như được thấy trong Hình 6a. Chúng tôi
cho thấy trong Phụ lục B.5 rằng việc kết hợp cả hai phương pháp thực sự làm giảm một số sự khác biệt,
nhưng không phải tất cả, gợi ý rằng thỏa thuận cao có tương quan với các tham số có ảnh hưởng tổng thể.

--- TRANG 10 ---
7.2 Tính Liên quan của Dấu của Top-k% Tham số
Trong thí nghiệm này, chúng tôi làm việc với các mô hình (IA)3và nhằm định lượng tầm quan trọng của top-
k%tham số và hướng của chúng đối với hiệu suất của nhiệm vụ. Đối với mỗi vectơ nhiệm vụ τt, chúng tôi
lật hướng của mỗi top- k%tham số (theo độ lớn) với xác suất pto thu được ˜τt. Việc lật hướng được thực
hiện bằng cách nhân các tham số với −1. Sau đó chúng tôi thêm lại ˜τtcó hướng đã lật này vào khởi tạo để
có được ˜θt=θinit+ ˜τt. Cuối cùng, chúng tôi đánh giá ˜θtvà tính hiệu suất trung bình trên tất cả các nhiệm
vụ tcho mỗi giá trị của kvà p. Như một đường cơ sở, chúng tôi cũng lật hướng của (100−k)%tham số dưới
cùng. Chúng tôi báo cáo kết quả trung bình trên ba lần chạy độc lập.

Trong Hình 7, chúng tôi vẽ hiệu suất trung bình như một hàm của p, xác suất lật hướng. Xác suất 0có nghĩa
là hướng của không ai trong số top- k%tham số bị lật và giá trị 1có nghĩa là hướng của tất cả top- k%tham
số bị lật. Đối với top- 20/30% tham số (đường liền), chúng tôi quan sát rằng hiệu suất giảm đơn điệu khi
chúng tôi tăng xác suất lật hướng. Ngược lại, việc lật hướng của 80/70% tham số dưới cùng (đường gạch
ngang) có tác động ít đến hiệu suất. Những kết quả này thiết lập tầm quan trọng của việc có đúng hướng cho
các tham số có độ lớn cao và cho thấy sự sụt giảm hiệu suất thảm khốc xảy ra với hướng sai.

7.3 Phép loại bỏ các Thành phần T IES-MERGING
Phương pháp T5-base (IA)3
TIES-MERGING 74.5 70.7
−TRIM 73.0 70.6
−ELECT 73.1 69.6
−DISJOINT MEAN 72.6 67.5
−SCALE 72.0 65.5

Bảng 6: Phép loại bỏ trên tất cả các bước của
TIES-MERGING .

Chúng tôi thực hiện các phép loại bỏ trên các thành phần riêng
lẻ của TIES-MERGING để đánh giá tầm quan trọng của chúng.
Trong Bảng 6, chúng tôi bắt đầu với TIES-MERGING và loại
bỏ một thành phần tại một thời điểm và báo cáo hiệu suất trên
tập xác thực cho hợp nhất mô hình đầy đủ (T5-base) và hợp
nhất mô hình PEFT ((IA)3trên T03B). Loại bỏ elect trong khi
giữ trung bình tách biệt đề cập đến việc lấy trung bình của các
giá trị với dấu +1và−1nhưng không bao gồm các giá trị 0
của các vectơ nhiệm vụ đã cắt tỉa trong trung bình. Loại bỏ
trung bình tách biệt nhưng cắt tỉa và bầu chọn đề cập đến việc
lấy trung bình của các giá trị với các dấu được bầu chọn và 0
cho các giá trị đã cắt tỉa. Loại bỏ chia tỷ lệ có nghĩa là sử dụng λ= 1. Bảng 6 cho thấy tất cả các thành
phần của phương pháp đều quan trọng cho hiệu suất tối ưu. Cụ thể, chia tỷ lệ và trung bình tách biệt nổi
lên như những thành phần quan trọng nhất, gây ra sự sụt giảm hiệu suất 2.5%và1.9%trong T5-base, và
5.2%và3.2%trong (IA)3, tương ứng.

7.4 Tầm quan trọng của Ước tính Dấu Đúng Khi Hợp nhất Mô hình
Với tầm quan trọng của các vectơ dấu, bây giờ chúng tôi nhằm hiểu hiệu suất có thể đạt được bởiTIES-
MERGING nếu chúng ta có thể sử dụng vectơ dấu oracle từ mô hình đa nhiệm vụ. Để kiểm tra điều này,
chúng tôi huấn luyện một mô hình (IA)3đa nhiệm vụ, θmult, trên mười một nhiệm vụ đang xem xét (như
trong § 6). Sau đó chúng tôi tạo ra vectơ đa nhiệm vụ τmultvà vectơ dấu đa nhiệm vụ γmult. Tiếp theo, trong
khi hợp nhất các mô hình bằng TIES-MERGING , chúng tôi giả định có quyền truy cập vào vectơ-dấu-đa
nhiệm vụ oracle γmult. Do đó, chúng tôi bỏ qua bước giải quyết xung đột và trực tiếp đặt γm=γmult. Đáng
ngạc nhiên, từ Bảng 5, chúng tôi quan sát rằng khi hợp nhất các nhiệm vụ bằng cách sử dụng vectơ dấu
oracle, chúng ta có được hiệu suất 72% so với 73.1% cho mô hình được huấn luyện đa nhiệm vụ. Hơn nữa,
trung bình mô hình đã hợp nhất hoạt động tốt hơn các mô hình cụ thể cho từng nhiệm vụ. Điều này ngụ ý
rằng nếu chúng ta có thể có được hướng chính xác cho mô hình đã hợp nhất, thì chúng ta có thể thu hẹp
khoảng cách đến các mô hình đa nhiệm vụ. Trong Phụ lục B.1 và Bảng 7, chúng tôi cố gắng ước tính vectơ-
dấu-đa nhiệm vụ bằng cách sử dụng dữ liệu hạn chế.

8 Kết luận
Chúng tôi đã giới thiệu TIES-MERGING để giải quyết nhiễu khi hợp nhất mô hình. TIES-MERGING cắt
tỉa các thay đổi có độ lớn thấp trong các giá trị của mô hình được tinh chỉnh và sau đó giải quyết sự bất đồng
dấu giữa các mô hình được hợp nhất. Chúng tôi thấy rằng về mặt thí nghiệm TIES-MERGING tăng cường
hiệu suất của mô hình đa nhiệm vụ đã hợp nhất trên nhiều cài đặt và miền khác nhau, mặc dù đơn giản với
các siêu tham số cố định. Nghiên cứu của chúng tôi cũng làm sáng tỏ tác động của các loại nhiễu khác nhau
đối với các tham số mô hình và nhấn mạnh tầm quan trọng của dấu trong quá trình hợp nhất. Để thảo luận
về các hạn chế và hướng tương lai, vui lòng tham khảo Phụ lục A.

--- TRANG 11 ---
Lời cảm ơn
Chúng tôi cảm ơn Yi-Lin Sung, Shiyue Zhang, Archiki Prasad, và các phản biện đã đưa ra phản hồi quý báu
cho bài báo này. Công trình này được hỗ trợ bởi NSF-AI Engage Institute DRL211263, NSF-CAREER
Award 1846185, DARPA MCS Grant N66001-19-2-4031, và NSF Grant 2145822. Các quan điểm, ý kiến
và/hoặc phát hiện có trong bài viết này là của các tác giả chứ không phải của cơ quan tài trợ.

Tài liệu tham khảo
[1]S. K. Ainsworth, J. Hayase, và S. Srinivasa. Git re-basin: Merging models modulo permutation
symmetries, 2022. https://arxiv.org/abs/2209.04836 .
[2]A. Albalak, C. Raffel, và W. Y . Wang. Improving few-shot generalization by exploring and
exploiting auxiliary data. arXiv preprint arXiv:2302.00674 , 2023.
[3]S. Amari. Neural learning in structured parameter spaces - natural riemannian gradient. In
NIPS , 1996.
[4]D. Arpit, H. Wang, Y . Zhou, và C. Xiong. Ensemble of averages: Improving model selection
and boosting performance in domain generalization. arXiv preprint arXiv:2110.10832 , 2021.
[5]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S.
Bari, T. Févry, et al. PromptSource: An integrated development environment and repository for
natural language prompts. arXiv preprint arXiv:2202.01279 , 2022.
[6]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models,
2021. https://arxiv.org/abs/2108.07258 .
[7]J. Cha, S. Chun, K. Lee, H.-C. Cho, S. Park, Y . Lee, và S. Park. Swad: Domain generalization
by seeking flat minima. Advances in Neural Information Processing Systems , 34:22405–22418,
2021.
[8]G. Cheng, J. Han, và X. Lu. Remote sensing image scene classification: Benchmark and state
of the art. Proceedings of the Institute of Electrical and Electronics Engineers (IEEE) , 2017.
https://ieeexplore.ieee.org/abstract/document/7891544 .
[9]L. Choshen, E. Venezian, N. Slonim, và Y . Katz. Fusing finetuned models for better pretraining,
2022. https://arxiv.org/abs/2204.03044 .
[10] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, và A. Vedaldi. Describing
textures in the wild. In Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2014. https://openaccess.thecvf.com/content_cvpr_2014/
html/Cimpoi_Describing_Textures_in_2014_CVPR_paper.html .
[11] I. Dagan, O. Glickman, và B. Magnini. The pascal recognising textual entailment challenge.
InMachine Learning Challenges Workshop , 2005. https://link.springer.com/
chapter/10.1007/11736790_9 .
[12] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
[13] S. Don-Yehiya, E. Venezian, C. Raffel, N. Slonim, Y . Katz, và L. Choshen. Cold fusion:
Collaborative descent for distributed multitask finetuning, 2022. https://arxiv.org/
abs/2212.01378 .
[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, và N. Houlsby. An image is worth 16x16
words: Transformers for image recognition at scale. In International Conference on Learning
Representations (ICLR) , 2021. https://openreview.net/forum?id=YicbFdNTTy .

--- TRANG 12 ---
[15] F. Draxler, K. Veschgini, M. Salmhofer, và F. Hamprecht. Essentially no barriers in neural
network energy landscape. In International Conference on Machine Learning (ICML) , 2018.
https://arxiv.org/abs/1803.00885 .
[16] R. Entezari, H. Sedghi, O. Saukh, và B. Neyshabur. The role of permutation invariance in
linear mode connectivity of neural networks. arXiv preprint arXiv:2110.06296 , 2021.
[17] C. Fifty, E. Amid, Z. Zhao, T. Yu, R. Anil, và C. Finn. Efficiently identifying task groupings
for multi-task learning. Advances in Neural Information Processing Systems , 34:27503–27516,
2021.
[18] R. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical transac-
tions of the Royal Society of London. Series A, containing papers of a mathematical or physical
character , 222(594-604):309–368, 1922.
[19] J. Frankle, G. K. Dziugaite, D. Roy, và M. Carbin. Linear mode connectivity and the
lottery ticket hypothesis. In International Conference on Machine Learning (ICML) , 2020.
https://proceedings.mlr.press/v119/frankle20a.html .
[20] C. D. Freeman và J. Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540 , 2016.
[21] T. Garipov, P. Izmailov, D. Podoprikhin, D. Vetrov, và A. G. Wilson. Loss surfaces, mode
connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing
Systems (NeurIPS) , 2018. https://arxiv.org/abs/1802.10026 .
[22] A. Gueta, E. Venezian, C. Raffel, N. Slonim, Y . Katz, và L. Choshen. Knowledge is a region
in weight space for fine-tuned language models. arXiv preprint arXiv:2302.04863 , 2023.
[23] V . Gupta, S. A. Serrano, và D. DeCoste. Stochastic weight averaging in parallel: Large-batch
training that generalizes well. International Conference on Learning Representations , 2020.
[24] P. Helber, B. Bischke, A. Dengel, và D. Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth
Observations and Remote Sensing , 2019. https://arxiv.org/abs/1709.00029 .
[25] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, và A. Peste. Sparsity in deep learning: Pruning
and growth for efficient inference and training in neural networks. The Journal of Machine
Learning Research , 22(1):10882–11005, 2021.
[26] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, và W. Chen. LoRA: Low-rank
adaptation of large language models. ArXiv , abs/2106.09685, 2021.
[27] L. Huang, R. Le Bras, C. Bhagavatula, và Y . Choi. Cosmos qa: Machine reading compre-
hension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) , pages 2391–2401, 2019.
[28] G. Ilharco, M. Wortsman, S. Y . Gadre, S. Song, H. Hajishirzi, S. Kornblith, A. Farhadi, và
L. Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in
Neural Information Processing Systems (NeurIPS) , 2022. https://arXiv.org/abs/
2208.05592 .
[29] G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, và A. Farhadi. Editing mod-
els with task arithmetic. In The Eleventh International Conference on Learning Representations ,
2023. URL https://openreview.net/forum?id=6t0Kwf8-jrj .
[30] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, và A. G. Wilson. Averaging weights
leads to wider optima and better generalization. In Conference on Uncertainty in Artificial
Intelligence (UAI) , 2018. https://arxiv.org/abs/1803.05407 .
[31] X. Jin, X. Ren, D. Preotiuc-Pietro, và P. Cheng. Dataless knowledge fusion by merging weights
of language models. In The Eleventh International Conference on Learning Representations ,
2023. URL https://openreview.net/forum?id=FCnohuR6AnM .

--- TRANG 13 ---
[32] K. Jordan, H. Sedghi, O. Saukh, R. Entezari, và B. Neyshabur. REPAIR: REnormalizing per-
muted activations for interpolation repair. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=gU5sJ6ZggcX .
[33] T. Khot, P. Clark, M. Guerquin, P. Jansen, và A. Sabharwal. Qasc: A dataset for question
answering via sentence composition. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 34, pages 8082–8090, 2020.
[34] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences (PNAS) , 2017. https:
//arxiv.org/abs/1612.00796 .
[35] J. Krause, M. Stark, J. Deng, và L. Fei-Fei. 3d object representations for fine-grained
categorization. In International Conference on Computer Vision Workshops (ICML) ,
2013. https://www.cv-foundation.org/openaccess/content_iccv_
workshops_2013/W19/html/Krause_3D_Object_Representations_2013_
ICCV_paper.html .
[36] Y . LeCun. The mnist database of handwritten digits, 1998. http://yann.lecun.com/
exdb/mnist/ .
[37] H. Levesque, E. Davis, và L. Morgenstern. The winograd schema challenge. Thirteenth
International Conference on the Principles of Knowledge Representation and Reasoning , 2012.
[38] M. Li, S. Gururangan, T. Dettmers, M. Lewis, T. Althoff, N. A. Smith, và L. Zettlemoyer.
Branch-train-merge: Embarrassingly parallel training of expert language models, 2022. https:
//arxiv.org/abs/2208.03306 .
[39] P. Li, Z. Zhang, P. Yadav, Y .-L. Sung, Y . Cheng, M. Bansal, và T. Chen. Merge, then compress:
Demystify efficient smoe with hints from its routing policy, 2023.
[40] W. Li, Y . Peng, M. Zhang, L. Ding, H. Hu, và L. Shen. Deep model fusion: A survey. arXiv
preprint arXiv:2309.15698 , 2023.
[41] X. Li, K. Huang, W. Yang, S. Wang, và Z. Zhang. On the convergence of fedavg on non-iid
data. In International Conference on Learning Representations , 2019.
[42] Y . Li, J. Yosinski, J. Clune, H. Lipson, và J. Hopcroft. Convergent learning: Do different
neural networks learn the same representations? arXiv preprint arXiv:1511.07543 , 2015.
[43] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, và C. A. Raffel. Few-shot
parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in
Neural Information Processing Systems , 35:1950–1965, 2022.
[44] M.-C. d. Marneffe, M. Simons, và J. Tonhauser. The CommitmentBank: Investigating
projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung 23 , 2019.
[45] M. Matena và C. Raffel. Merging models with fisher-weighted averaging. In Advances in
Neural Information Processing Systems (NeurIPS) , 2021. https://arxiv.org/abs/
2111.09832 .
[46] B. McMahan, E. Moore, D. Ramage, S. Hampson, và B. A. y Arcas. Communication-efficient
learning of deep networks from decentralized data. In Artificial intelligence and statistics , pages
1273–1282. PMLR, 2017.
[47] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, và A. Y . Ng. Reading digits in nat-
ural images with unsupervised feature learning. In Advances in Neural Information Pro-
cessing Systems (NeurIPS) Workshops , 2011. https://storage.googleapis.com/
pub-tools-public-publication-data/pdf/37648.pdf .
[48] B. Neyshabur, H. Sedghi, và C. Zhang. What is being transferred in transfer learning?
Advances in neural information processing systems , 33:512–523, 2020.

--- TRANG 14 ---
[49] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, và D. Kiela. Adversarial NLI: A new
benchmark for natural language understanding. arXiv preprint arXiv:1910.14599 , 2019.
[50] H. Orgad, B. Kawar, và Y . Belinkov. Editing implicit assumptions in text-to-image diffusion
models. arXiv preprint arXiv:2303.08084 , 2023.
[51] G. Ortiz-Jiménez, A. Favero, và P. Frossard. Task arithmetic in the tangent space: Improved
editing of pre-trained models. NeurIPS , 2023. https://arxiv.org/abs/2305:12827 .
[52] J. Phang, T. Févry, và S. R. Bowman. Sentence encoders on stilts: Supplementary training on
intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088 , 2018.
[53] M. T. Pilehvar và J. Camacho-Collados. WiC: The word-in-context dataset for evaluating
context-sensitive meaning representations. In Proceedings of NAACL-HLT , 2019.
[54] C. Poth, J. Pfeiffer, A. Rücklé, và I. Gurevych. What to pre-train on? Efficient intermediate task
selection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing , pages 10585–10605, Online and Punta Cana, Dominican Republic, Nov. 2021.
[55] Y . Pruksachatkun, J. Phang, H. Liu, P. M. Htut, X. Zhang, R. Y . Pang, C. Vania, K. Kann, và
S. R. Bowman. Intermediate-task transfer learning with pretrained language models: When
and why does it work? In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 5231–5247, Online, July 2020.
[56] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, G. Krueger, và I. Sutskever. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning (ICML) , 2021.
https://arxiv.org/abs/2103.00020 .
[57] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, và P. J.
Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of
Machine Learning Research (JMLR) , 2020. http://jmlr.org/papers/v21/20-074.
html .
[58] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, và P. J.
Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv ,
abs/1910.10683, 2020.
[59] A. Ramé, K. Ahuja, J. Zhang, M. Cord, L. Bottou, và D. Lopez-Paz. Model ratatouille: Recy-
cling diverse models for out-of-distribution generalization. arXiv preprint arXiv:2212.10445 ,
2022.
[60] A. Ramé, M. Kirchmeyer, T. Rahier, A. Rakotomamonjy, P. Gallinari, và M. Cord. Diverse
weight averaging for out-of-distribution generalization. ICML , 2023.
[61] M. Roemmele, C. A. Bejan, và A. S. Gordon. Choice of plausible alternatives: An evaluation
of commonsense causal reasoning. 2011 AAAI Spring Symposium Series , 2011.
[62] A. Rogers, O. Kovaleva, M. Downey, và A. Rumshisky. Getting closer to AI complete question
answering: A set of prerequisite real tasks. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pages 8722–8731. AAAI
Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/
6398 .
[63] S. Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747 , 2016.
[64] K. Sakaguchi, R. Le Bras, C. Bhagavatula, và Y . Choi. Winogrande: An adversarial winograd
schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence ,
2020.

--- TRANG 15 ---
[65] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,
T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.
arXiv preprint arXiv:2110.08207 , 2021.
[66] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,
T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.
InInternational Conference on Learning Representations (ICLR) , 2021. https://arxiv.
org/abs/2110.08207 .
[67] M. Sap, H. Rashkin, D. Chen, R. Le Bras, và Y . Choi. Social iqa: Commonsense reasoning
about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 4463–4473, 2019.
[68] R. Sharma, J. Allen, O. Bakhshandeh, và N. Mostafazadeh. Tackling the story ending biases
in the story cloze test. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) , pages 752–757, 2018.
[69] E. Shnarch, A. Halfon, A. Gera, M. Danilevsky, Y . Katsis, L. Choshen, M. S. Cooper, D. Epel-
boim, Z. Zhang, D. Wang, et al. Label sleuth: From unlabeled text to a classifier in a few hours.
InConference on Empirical Methods in Natural Language Processing , 2022.
[70] S. P. Singh và M. Jaggi. Model fusion via optimal transport. Advances in Neural Information
Processing Systems , 33:22045–22055, 2020.
[71] J. Stallkamp, M. Schlipsing, J. Salmen, và C. Igel. The german traffic sign recognition
benchmark: a multi-class classification competition. In International Joint Conference on Neural
Networks (IJCNN) , 2011. https://ieeexplore.ieee.org/document/6033395 .
[72] Y .-L. Sung, L. Li, K. Lin, Z. Gan, M. Bansal, và L. Wang. An empirical study of multimodal
model merging. Empirical Methods in Natural Language Processing (Findings) , 2023.
[73] O. Tafjord, M. Gardner, K. Lin, và P. Clark. Quartz: An open-domain dataset of qualitative
relationship questions. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 5941–5946, 2019.
[74] N. Tatro, P.-Y . Chen, P. Das, I. Melnyk, P. Sattigeri, và R. Lai. Optimizing mode connectivity
via neuron alignment. Advances in Neural Information Processing Systems , 33:15300–15311,
2020.
[75] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster,
S. Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International
Conference on Learning Representations , 2022.
[76] G. Thimm và E. Fiesler. Evaluating pruning methods. In International Symposium on Artificial
Neural Networks , 1995.
[77] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và
I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems
(NeurIPS) , 2017. https://arxiv.org/abs/1706.03762 .
[78] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, và S. R. Bowman. Glue: A multi-task bench-
mark and analysis platform for natural language understanding. In International Conference on
Learning Representations (ICLR) , 2018. https://arxiv.org/abs/1804.07461 .
[79] H. Wang, M. Yurochkin, Y . Sun, D. Papailiopoulos, và Y . Khazaeni. Federated learning with
matched averaging. In International Conference on Learning Representations , 2020.
[80] O. Weller, K. Seppi, và M. Gardner. When to use multi-task learning vs intermediate fine-
tuning for pre-trained encoder transfer learning. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 272–282,
Dublin, Ireland, May 2022.

--- TRANG 16 ---
[81] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,
M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing,
2019. https://arxiv.org/abs/1910.03771 .
[82] M. Wortsman, G. Ilharco, S. Y . Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos,
H. Namkoong, A. Farhadi, Y . Carmon, S. Kornblith, et al. Model soups: averaging weights
of multiple fine-tuned models improves accuracy without increasing inference time. In Inter-
national Conference on Machine Learning (ICML) , 2022. https://arxiv.org/abs/
2203.05482 .
[83] M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi, H. Namkoong, và
L. Schmidt. Robust fine-tuning of zero-shot models. In Conference on Computer Vision and
Pattern Recognition (CVPR) , 2022. https://arxiv.org/abs/2109.01903 .
[84] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, và A. Oliva. Sun database: Exploring a
large collection of scene categories. International Journal of Computer Vision (IJCV) , 2016.
https://link.springer.com/article/10.1007/s11263-014-0748-y .
[85] P. Yadav và M. Bansal. Exclusive supermask subnetwork training for continual learning.
InFindings of the Association for Computational Linguistics: ACL 2023 , pages 569–587,
Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.
findings-acl.36. URL https://aclanthology.org/2023.findings-acl.36 .
[86] P. Yadav, Q. Sun, H. Ding, X. Li, D. Zhang, M. Tan, P. Bhatia, X. Ma, R. Nallapati,
M. K. Ramanathan, M. Bansal, và B. Xiang. Exploring continual learning for code gen-
eration models. In Proceedings of the 61st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers) , pages 782–792, Toronto, Canada, July
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.68. URL
https://aclanthology.org/2023.acl-short.68 .
[87] Y . Yang, W.-t. Yih, và C. Meek. WikiQA: A challenge dataset for open-domain question
answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing , pages 2013–2018, Lisbon, Portugal, Sept. 2015. Association for Computational Lin-
guistics. doi: 10.18653/v1/D15-1237. URL https://aclanthology.org/D15-1237 .
[88] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, và Y . Choi. HellaSwag: Can a machine really
finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.
[89] Y . Zhang và Q. Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and
Data Engineering , 34(12):5586–5609, 2021.
[90] Y . Zhang, J. Baldridge, và L. He. PAWS: Paraphrase Adversaries from Word Scrambling. In
Proc. of NAACL , 2019.
[91] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y . Zhu, H. Zhu, H. Xiong, và Q. He. A comprehensive
survey on transfer learning. Proceedings of the IEEE , 2020. https://arxiv.org/abs/
1911.02685 .

--- TRANG 17 ---
Phụ lục cho T IES-MERGING
A Hạn chế và Nghiên cứu Tương lai
Nghiên cứu của chúng tôi chia sẻ cùng những hạn chế chung của các phương pháp hợp nhất hiện có, như (1)
hiểu biết lý thuyết hạn chế về lý do và thời điểm nào nội suy trọng số hoạt động, các yếu tố cơ bản quan trọng
là gì, và kết nối phù hợp của nó với kết nối chế độ. Các nghiên cứu gần đây như [ 50] đã chứng minh những
mối quan hệ thú vị giữa sự phân tách trọng số và khả năng hợp nhất của các mô hình; (2) việc hợp nhất dựa
trên khởi tạo chung và kiến trúc mô hình; và (3) việc hợp nhất các mô hình riêng lẻ-nhiệm vụ để tạo ra đa
nhiệm vụ vẫn thua kém huấn luyện đa nhiệm vụ đồng thời. Hơn nữa, không rõ ràng cách chọn các điểm kiểm
tra để hợp nhất nhằm tạo ra các mô hình đa nhiệm vụ hữu ích cho các miền cụ thể. Ngoài ra, trong khi phương
pháp của chúng tôi cung cấp một cách để chọn dấu khi hợp nhất các vectơ nhiệm vụ, chúng tôi vẫn thấy rằng
việc sử dụng các dấu từ mô hình đa nhiệm vụ hoạt động tốt hơn. Một số nghiên cứu tương lai tiềm năng bao
gồm việc tìm ra cách tốt để ước tính dấu đa nhiệm vụ mà không cần quyền truy cập vào mô hình đa nhiệm vụ
vì điều này có tiềm năng thu hẹp khoảng cách giữa hợp nhất đa nhiệm vụ và huấn luyện đa nhiệm vụ (như
được chứng minh trong Phần 7.4).

B Kết quả Bổ sung
Phương pháp Ước tính Dấu Trung bình
Mẫu Đa nhiệm vụ Khởi tạo.
Được Tinh chỉnh - - - 71.4
Đa nhiệm vụ - - - 73.1
Lấy trung bình [9, 82] - - - 58.0
Task Vectors [29] - - - 63.9
TIES-MERGING - - - 66.4
TIES-MERGING✓ 32 từ đầu 66.5 [+0.1]
✓ 32 trung bình 67.7 [+1.2]
✓ Tất cả từ đầu 72.0 [+5.6]

Bảng 7: Hiệu suất Hợp nhất có thể được cải thiện bằng cách ước tính Vectơ Dấu bằng cách thực
hiện huấn luyện đa nhiệm vụ few-shot. Chúng tôi sử dụng dấu ước tính làm dấu được bầu chọn và
thực hiện hợp nhất.

B.1 Tăng cường Hiệu suất bằng cách Ước tính Vectơ Dấu Đa nhiệm vụ.
Xem xét các phát hiện, chúng tôi tìm hiểu xem liệu có thể có được các vectơ dấu đa nhiệm vụ một cách hiệu
quả mà không cần huấn luyện đa nhiệm vụ rộng rãi hay không. Đề xuất của chúng tôi liên quan đến việc sử
dụng một số lượng hạn chế các mẫu xác thực từ mỗi nhiệm vụ để huấn luyện một cách rẻ tiền một mô hình
đa nhiệm vụ và sau đó rút ra vectơ dấu liên quan. Chúng tôi tạo ra hai mô hình (IA)3đa nhiệm vụ: một được
phát triển từ đầu và một khác được khởi tạo bằng cách sử dụng trung bình của các mô hình (IA)3cụ thể cho
từng nhiệm vụ dự định hợp nhất. Chúng tôi sử dụng 32 ví dụ xác thực từ mỗi nhiệm vụ để huấn luyện mô
hình này. Trong Bảng 5, chúng tôi nhận thấy việc sử dụng vectơ dấu từ mô hình đa nhiệm vụ fewshot được
khởi tạo với trung bình đã mang lại sự gia tăng hiệu suất 3.8%và1.3%so với Task Arithmetic và TIES-
MERGING . Thú vị thay, huấn luyện đa nhiệm vụ fewshot từ đầu không mang lại cải thiện đáng kể so với
TIES-MERGING mà không có ước tính dấu. Chúng tôi tin rằng việc khám phá lĩnh vực này sâu hơn có thể
dẫn đến các kỹ thuật hợp nhất được cải thiện.

B.2 Tác động của Siêu tham số λvà K đối với Hiệu suất.
Trong Hình 8 (trái và giữa), chúng tôi vẽ tác động của λđối với hiệu suất khi hợp nhất các mô hình T5-base
và T5-large được huấn luyện trên GLUE (Tương tự như Bảng-1). Đối với TIES-MERGING , chúng tôi thay
đổi xung quanh giá trị 1 vì TIES lấy trung bình của các vectơ nhiệm vụ, trong khi task arithmetic cộng lại
các vectơ nhiệm vụ. Do đó, giá trị 1 cho TIES tương tự như việc sử dụng1
#nhiệm vụcho Task Arithmetic [
29]. Phạm vi 0.8-1.8 cho TIES được chọn dựa trên các thí nghiệm sơ bộ trên cài đặt PEFT (như được đề cập
trong Phần 5). Chúng tôi thấy rằng TIES-MERGING ít nhạy cảm hơn nhiều với những thay đổi trong (với
phạm vi độ chính xác 68-75% trên các giá trị λ được xem xét) so với Task Arithmetic (với

--- TRANG 18 ---
LambdaHiệu suất Trung bình505560657075
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0T5-Base T5-LargeTV Merging
LambdaHiệu suất Trung bình64666870727476
0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8T5-Base T5-LargeTIES Merging
KHiệu suất Trung bình 707274767880
10 20 30 40 50 60 70 80 90 100Hiệu suất Trung bình so với K (T5-Base)

Hình 8: Hiệu suất như một hàm của các siêu tham số . Để biết thêm chi tiết, vui lòng tham khảo
phản hồi chung của chúng tôi.

10 20 30 40 50 60 70 80 9000.050.10.150.2RTE
K (Tỷ lệ tham số được giữ lại.)Tỷ lệ Tham số 
 với Xung đột
10 20 30 40 50 60 70 80 9000.050.10.15MRPC
K (Tỷ lệ tham số được giữ lại.)Tỷ lệ Tham số 
 với Xung đột
10 20 30 40 50 60 70 80 9000.10.20.3WNLI
K (Tỷ lệ tham số được giữ lại.)Tỷ lệ Tham số 
 với Xung đột

Hình 9: Xung đột dấu tăng khi chúng ta cắt tỉa ít tham số hơn. Đối với mỗi nhiệm vụ, chúng tôi
hợp nhất 10 điểm kiểm tra khác nhau từ hunggingface hub và vẽ xung đột dấu như một hàm của
việc chỉ giữ lại top-k% tham số.

2 3 4 5 6 7 8 9 1000.050.10.150.2RTE
Số Mô hìnhTỷ lệ Tham số 
 với Xung đột
2 3 4 5 6 7 8 9 1000.050.10.15MRPC
Số Mô hìnhTỷ lệ Tham số 
 với Xung đột
2 3 4 5 6 7 8 9 1000.10.20.3WNLI
Số Mô hìnhTỷ lệ Tham số 
 với Xung đột
2 3 4 5 6 7 8 9 10 1100.10.20.3
Số Mô hìnhTỷ lệ Tham số 
 với Xung đột

Hình 10: Xung đột Dấu tồn tại ngay cả khi hợp nhất nhiều điểm kiểm tra cho cùng một nhiệm vụ.
Ba biểu đồ đầu tiên dành cho các tập dữ liệu RTE, MRPC, WNLI khi hợp nhất 10 điểm kiểm tra
Huggingface, trong khi biểu đồ cuối cùng là khi hợp nhất các nhiệm vụ khác nhau (Hình 4 từ bài
báo chính).

phạm vi độ chính xác 55-75). Hình 8 (phải) chứng minh tác động của kkhi chúng tôi tăng giá trị của ktheo
bước 10và bỏ qua k= 0vì điều đó sẽ không chọn tham số nào. Chúng tôi quan sát rằng khi ktăng lên, hiệu
suất giảm và sau đó bão hòa. Tuy nhiên, chúng tôi muốn lưu ý rằng đường cong này có thể thay đổi dựa trên
phân phối của các giá trị tham số trong vectơ nhiệm vụ.

B.3 Xung đột Dấu Tăng khi Chúng ta Cắt tỉa Ít Tham số hơn
Trong Hình 9, chúng tôi hợp nhất 10 điểm kiểm tra bert-base-uncased từ huggingface được tinh chỉnh
trên ba nhiệm vụ glue khác nhau (RTE, MRPC, và WNLI) và vẽ xung đột dấu như một hàm của k. Khi
chúng tôi giữ ngày càng nhiều tham số, xung đột dấu tăng lên và đạt gần 80%. Điều này cũng được mong
đợi vì có nhiều tham số khác không hơn có thể tạo ra xung đột ngay cả khi độ lớn của chúng nhỏ.

B.4 Xung đột Dấu Tồn tại Giữa Các Điểm kiểm tra Khác nhau cho Cùng Nhiệm vụ
Trong Hình 10, chúng tôi cho thấy rằng xung đột dấu tồn tại ngay cả trong các mô hình được huấn luyện
trên cùng một nhiệm vụ. Chúng tôi đã vẽ xung đột dấu (tương tự như Hình 4) giữa 10 điểm kiểm tra của
RTE, MRPC, và WNLI từ Huggingface. Khi số lượng điểm kiểm tra tăng lên, xung đột dấu tăng lên. Chúng
tôi cũng so sánh điều này với sự nhiễu dấu khi hợp nhất các điểm kiểm tra nhiệm vụ khác nhau và thấy mức
độ nhiễu tương tự trong tất cả các trường hợp này. Do đó, xung đột dấu tồn tại ngay cả trong các mô hình
được huấn luyện trên cùng một tập dữ liệu. Chúng tôi nghi ngờ rằng điều này là do các mô hình được tham
số hóa quá mức và do đó có nhiều mạng con (tập con của tham số) có thể dẫn đến cùng một hiệu suất trong
đó các lần chạy tinh chỉnh khác nhau cập nhật cùng các tham số theo các hướng khác nhau.

--- TRANG 19 ---
0 1 >100.050.10.15Trung bình Cắt tỉa + Trung bình Tách biệt TIES
Tham số với giá trị khác khôngĐộ lớn trung bình

(a) Nhiễu Tham số Dư thừa cho (IA)3với
STD.

0.5-0.60.6-0.70.7-0.80.8-0.90.9-1.000.050.10.150.20.25Trung bình
Bầu chọn + Trung bình Tách biệt
TIES
Bins của Thỏa thuận DấuĐộ lớn trung bình

(b) Nhiễu Dấu cho (IA)3với STD.

Hình 11: Tác động của các loại Hợp nhất khác nhau đối với Độ lớn của Tham số. Ở đây chúng tôi
bổ sung so sánh với TIES-MERGING và cũng cung cấp độ lệch chuẩn của các giá trị tham số. Một
std cao ngụ ý rằng có một số đa dạng trong các giá trị độ lớn trên các tham số khác nhau.

2 3 4 5 6 700.050.10.150.2
Số Mô hìnhTỷ lệ Tham số 
 với Xung đột

(a) Tỷ lệ Tham số với Xung đột
dấu cho mô hình T5-Base so với
số lượng mô hình.

0 1 >100.0020.0040.006Trung bình Cắt tỉa + Trung bình Tách biệt TIES
Tham số với giá trị khác khôngĐộ lớn trung bình

(b) Nhiễu Tham số Dư thừa cho
T5-Base với STD.

0.5-0.60.6-0.70.7-0.80.8-0.90.9-1.000.0020.0040.006Trung bình
Bầu chọn + Trung bình Tách biệt
TIES
Bins của Thỏa thuận DấuĐộ lớn trung bình

(c) Nhiễu Dấu cho mô hình T5-
Base với STD.

Hình 12: Biểu đồ cho mô hình T5-Base.

Phương pháp Xác thực Trung bình rte cb winogrande wic wsc copa h-swag story cloze anli-r1 anli-r2 anli-r3
Zeroshot - 55.3 79.8 46.4 52.8 54.1 45.2 85 36.1 91 39.7 37.6 40.5
Được Tinh chỉnh - 71.4 82.7 95.8 75.1 71.7 65.3 85.3 44.4 94.9 70.2 46.5 53
Đa nhiệm vụ (Tất cả, từ đầu) - 73.1 88.6 95.8 75.5 61.1 80.6 94.1 42.3 97.6 70.5 49.8 47.7
Đa nhiệm vụ (32, từ đầu) - 60.9 74.9 79.2 59.3 49.2 63.9 80.9 39.5 91.6 49.4 41.9 40.1
Đa nhiệm vụ (32, trung bình) - 65.2 79.8 83.3 61.6 54.2 66.7 85.3 41.1 94.4 58.1 46.0 46.5
Lấy trung bình ✗ 58 81.2 58.3 53.8 55.2 53.5 80.9 40.1 92.5 43.3 39.2 40.2
Task Arithmetic ✗ 59.2 76.5 79.2 57.7 51.6 51.4 66.2 31.4 81.5 59.8 47.5 48.2
TIES-MERGING ✗ 64.9 81.2 87.5 60.8 59.9 58.3 80.2 42.6 91.1 58.1 46.5 47.4
Fisher Merging ✓ 62.2 83.3 83.3 56.7 54.2 58.3 83.1 42.2 94.1 45.9 41.0 42.2
RegMean ✓ 58 81.2 58.3 53.8 55.2 53.5 80.9 40.1 92.5 43.3 39.2 40.2
Task Arithmetic ✓ 63.9 74.1 83.3 62.8 49.1 49.3 87.5 41.5 95.3 60.8 49.4 50.0
TIES-MERGING ✓ 66.4 78.0 83.3 67.9 57.6 59.7 81.7 42.8 90.3 66.9 51.3 51.1

Bảng 8: Hiệu suất tập kiểm tra khi hợp nhất các mô hình IA3 trên mười một nhiệm vụ. Vui lòng
tham khảo Phần 6 để biết chi tiết thí nghiệm.

B.5 Kết quả Chi tiết cho Các Loại Nhiễu và Tác động của Chúng đối với Hợp nhất
Trong Phần 7.1 và Hình 6, chúng tôi đã cho thấy tác động của các tham số dư thừa và xung đột dấu đối với
độ lớn tham số khi so sánh lấy trung bình đơn giản so với trung bình tách biệt sau khi cắt tỉa hoặc bầu chọn
và cho thấy rằng việc thực hiện các phép toán này giúp ích với độ lớn tham số. Trong Hình 11, chúng tôi bổ
sung so sánh với TIES-MERGING và cho thấy rằng việc thực hiện cả cắt tỉa và bầu chọn thường dẫn đến
độ lớn cao hơn và cũng độ lệch chuẩn cao hơn trong độ lớn tham số. Std cao hơn biểu thị rằng tất cả các giá
trị tham số trong mô hình đã hợp nhất đều giống nhau và có sự biến đổi đáng kể trong độ lớn, trái ngược với
lấy trung bình đơn giản vì nó làm giảm độ lớn của các tham số không dư thừa và giảm độ lớn của các tham
số có ảnh hưởng trong mô hình đã hợp nhất. Các biểu đồ tương tự cho mô hình T5-base được cung cấp trong
Hình 12.

--- TRANG 20 ---
B.6 Kết quả Cấp Nhiệm vụ Toàn diện
Chúng tôi cung cấp kết quả cấp nhiệm vụ cho tất cả các thí nghiệm đánh giá trong miền trong Bảng 1 chính.
Bảng 8, 9, 10, 11, và 12 cung cấp kết quả cấp nhiệm vụ IA3 [ 43], T5-Base, T5-Large [ 58], ViT-B/32, và
ViT-L/14 [ 14] tương ứng. Kết quả cấp nhiệm vụ của các thí nghiệm ngoài miền cho T5-Base và T5-Large
có thể được tìm thấy trong Bảng 13, và 14. Cuối cùng, Hình 13, cho thấy việc mở rộng của mô hình T5-Base
khi chúng tôi hợp nhất số lượng nhiệm vụ khác nhau.

Phương pháp Xác thực Trung bình paws qasc quartz story_cloze wiki_qa winogrande wsc
Zeroshot - 53.5 49.9 35.8 53.3 48.1 76.2 50 61.1
Được tinh chỉnh - 82.8 94.3 98.3 80.4 84.7 95.5 64.1 62.5
Đa nhiệm vụ - 83.6 94 97.9 82.5 86.7 95 64.1 65.3
Lấy trung bình ✗ 65.9 66.4 82.6 60.2 49.5 94.1 50.4 58.3
Task Arithmetic ✗ 73.9 73.3 93.5 68.2 76.5 93.7 55.5 56.9
TIES-MERGING ✗ 69.7 74 83.3 70.3 64.2 84.7 55.9 55.6
Fisher Merging ✓ 68.9 69.3 85.7 63.6 56.4 93.8 50.9 62.5
RegMean ✓ 71.2 76.8 96.2 62.5 55 94.8 51.9 61.1
Task Arithmetic ✓ 73.2 73.4 93.3 67.1 71.7 94.1 52.9 59.7
TIES-MERGING ✓ 73.9 79.3 88.6 71.8 72.9 82.5 61.3 61.1

Bảng 9: Hiệu suất tập kiểm tra khi hợp nhất các mô hình T5-base trên bảy nhiệm vụ. Vui lòng tham
khảo Phần 6 để biết chi tiết thí nghiệm.

Phương pháp Xác thực Trung bình paws qasc quartz story_cloze wiki_qa winogrande wsc
Zeroshot - 51.7 55.4 14.3 54.1 54.1 71 49.3 63.9
Được tinh chỉnh - 88.8 94.4 98.9 87.8 90.8 96 74.7 79.2
Đa nhiệm vụ - 88.1 94.2 98.5 89.3 92 95.4 73.5 73.6
Lấy trung bình ✗ 59.6 61.3 82.6 70.5 53.7 63.2 49.7 36.1
Task Arithmetic ✗ 73.5 79.2 96.8 80.2 83.6 58.6 60.2 55.6
TIES-MERGING ✗ 74.4 80.5 96.2 81.8 78.6 62 61.9 59.7
Fisher Merging ✓ 64.6 60.4 81.7 75 60.1 88.6 50 36.1
RegMean ✓ 73.2 86 96.9 80.7 78.6 82.6 51.8 36.1
Task Arithmetic ✓ 73.3 77.8 96 78.6 86.4 59.1 62.3 52.8
TIES-MERGING ✓ 76.9 81.5 96.2 80.1 83.6 64.9 66.5 65.3

Bảng 10: Hiệu suất tập kiểm tra khi hợp nhất các mô hình T5-large trên bảy nhiệm vụ. Vui lòng
tham khảo Phần 6 để biết chi tiết thí nghiệm.

Phương pháp Xác thực Trung bình SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD
Riêng lẻ - 90.5 75.3 77.7 96.1 99.7 97.5 98.7 99.7 79.4
Đa nhiệm vụ - 88.9 74.4 77.9 98.2 98.9 99.5 93.9 72.9 95.8
Lấy trung bình ✗ 65.8 65.3 63.4 71.4 71.7 64.2 52.8 87.5 50.1
Task Arithmetic ✗ 60.4 36.7 41 53.8 64.4 80.6 66 98.1 42.5
TIES-MERGING ✗ 72.4 59.8 58.6 70.7 79.7 86.2 72.1 98.3 54.2
Fisher Merging ✓ 68.3 68.6 69.2 70.7 66.4 72.9 51.1 87.9 59.9
RegMean ✓ 71.8 65.3 63.5 75.6 78.6 78.1 67.4 93.7 52
Task Arithmetic ✓ 70.1 63.8 62.1 72 77.6 74.4 65.1 94 52.2
TIES-MERGING ✓ 73.6 64.8 62.9 74.3 78.9 83.1 71.4 97.6 56.2

Bảng 11: Hiệu suất tập kiểm tra khi hợp nhất các mô hình ViT-B/32 trên tám nhiệm vụ. Vui lòng
tham khảo Phần 6 để biết chi tiết thí nghiệm.

C Chi tiết Triển khai
C.1 Tài nguyên Tính toán Được sử dụng và Thời gian Chạy
Chúng tôi thực hiện tất cả các thí nghiệm trên GPU Nvidia A6000 được trang bị 48GB RAM. Các mô hình
(IA)3một nhiệm vụ cho mười một nhiệm vụ cần 1-2 giờ mỗi mô hình, trong khi vectơ đa nhiệm vụ mất khoảng
24 giờ trên bốn GPU. Các mô hình T5-Base và T5-Large, dựa trên kích thước tập dữ liệu, cần từ 15 phút đến
2 giờ mỗi nhiệm vụ, và khoảng tám giờ cho điểm kiểm tra đa nhiệm vụ. Các mô hình thị giác

--- TRANG 21 ---
Phương pháp Xác thực Trung bình SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD
Được tinh chỉnh - 94.2 82.3 92.4 97.4 100 98.1 99.2 99.7 84.1
Đa nhiệm vụ - 93.5 90.6 84.4 99.2 99.1 99.6 96.3 80.8 97.6
Lấy trung bình ✗ 79.6 72.1 81.6 82.6 91.9 78.2 70.7 97.1 62.8
Task Arithmetic ✗ 83.3 72.5 79.2 84.5 90.6 89.2 86.5 99.1 64.3
TIES-MERGING ✗ 86 76.5 85 89.3 95.7 90.3 83.3 99 68.8
Fisher Merging ✓ 82.2 69.2 88.6 87.5 93.5 80.6 74.8 93.3 70
RegMean ✓ 83.7 73.3 81.8 86.1 97 88 84.2 98.5 60.8
Task Arithmetic ✓ 84.5 74.1 82.1 86.7 93.8 87.9 86.8 98.9 65.6
TIES-MERGING ✓ 86 76.5 85 89.4 95.9 90.3 83.3 99 68.8

Bảng 12: Hiệu suất tập kiểm tra khi hợp nhất các mô hình ViT-L/14 trên tám nhiệm vụ. Vui lòng
tham khảo Phần 6 để biết chi tiết thí nghiệm.

ViT-B/32 và ViT-L/14 được sử dụng, như được cung cấp bởi Ilharco et al. [29].2Các thí nghiệm hợp nhất rất
hiệu quả, với các đánh giá tiêu thụ ít hơn 2 phút cho các thí nghiệm T5-Base, T5-Large, ViT-B/32, và ViT-
L/14. Việc đánh giá các mô hình (IA)3, do cần phải sử dụng nhiều mẫu từ nguồn gợi ý và tính toán kết quả
trung vị trên tất cả các mẫu, cần khoảng một giờ cho mỗi đánh giá 11 tập dữ liệu.

Mô hình Trung bình cosmos_qa social_iqa quail wic copa h-swag
PA WS 35.9 18.8 25 24.8 68.8 56.2 21.9
QASC 34.9 15.6 21.9 25.1 75 53.1 18.8
QUARTZ 37.4 31.2 18.8 24.3 71.9 59.4 18.8
Story Cloze 35 6.2 25 25.6 75 65.6 12.5
Wiki QA 24.5 18.8 21.9 24.9 28.1 43.8 9.4
Winogrande 28.3 18.8 25 25.7 34.4 43.8 21.9
WSC 31.7 21.9 21.9 24.6 62.5 46.9 12.5
Được huấn luyện trước 31.1 21.9 18.8 24.1 65.6 43.8 12.5
Lấy trung bình 31.7 21.9 21.9 24.6 68.8 37.5 15.6
Fisher Merging 33.8 15.6 21.9 24.9 65.6 53.1 21.9
Task Arithmetic 31.9 15.6 31.2 25.7 28.1 68.8 21.9
RegMean 34.3 23.1 28.1 24.9 48.4 62.5 18.8
TIES-MERGING 35.3 21.9 25 25.7 50 65.6 23.8

Bảng 13: Hiệu suất Ngoài Phân phối của các điểm kiểm tra mô hình T5-Base trên sáu nhiệm vụ.
Vui lòng tham khảo Phần 6 để biết chi tiết thí nghiệm.

C.2 Tập dữ liệu Được sử dụng và Giấy phép Liên quan
Chúng tôi sử dụng các tập dữ liệu sau trong bài báo với các giấy phép sau. ANLI [ 49], WiC [ 53], WSC
[37], và Story Cloze [ 68], QuaRTz [ 73], Cars [ 35], GTSRB [ 71] được cấp phép theo Creative Commons
License. Winogrande [ 64], QASC [ 33] được cấp phép theo Apache license. COPA [ 61] được cấp phép theo
BSD-2 Clause license. H-SWAG [ 88], EuroSAT [ 24], được cấp phép theo MIT Licence. MNIST [ 36] được
cấp phép theo Gnu General Public License. Chúng tôi không thể tìm thấy giấy phép của DTD [ 10], RESISC45
[ 8], SUN397 [ 84], SVHN [47], CB [44], RTE [11]), và PAWS [90] nhưng chúng được công khai cho việc
sử dụng nghiên cứu.

C.3 Chi tiết của Các Thí nghiệm Động lực
Đối với cả Hình 3, và 4 trong Phần 3, chúng tôi thực hiện thí nghiệm trên mười một mô hình (IA)3được sử
dụng trong các thí nghiệm hợp nhất PEFT của chúng tôi (§ 6). Để có Hình tương tự như Hình 4 chứng minh
tỷ lệ tham số với xung đột dấu cho mô hình T5-base, vui lòng tham khảo Hình 12a.

2https://github.com/mlfoundations/task_vectors#checkpoints

--- TRANG 22 ---
Mô hình Trung bình cosmos_qa social_iqa quail wic copa h-swag
PA WS 32.3 25 28.1 25.6 56.2 46.9 12.5
QASC 33.4 21.9 28.1 25.5 43.8 62.5 18.8
QUARTZ 28.7 25 25 25.1 25 53.1 18.8
Story Cloze 32.1 21.9 34.4 26.8 46.9 53.1 9.4
Wiki QA 27.1 25 28.1 25.2 28.1 46.9 9.4
Winogrande 32.4 31.2 18.8 25.6 43.8 62.5 12.5
WSC 29.7 25 25 25.1 37.5 56.2 9.4
Được huấn luyện trước 27.6 21.9 21.9 24.9 28.1 56.2 12.5
Lấy trung bình 30.4 31.2 25 26.3 31.2 59.4 9.4
Fisher Merging 32 34.4 25 26.1 40.6 56.2 9.4
Task Arithmetic 33.3 21.9 34.4 24.6 40.6 59.4 18.8
RegMean 36 34.4 28.1 25.3 62.5 50 15.6
TIES-MERGING 40.4 31.2 43.8 26.6 59.4 59.4 21.9

Bảng 14: Hiệu suất Ngoài Phân phối của các điểm kiểm tra mô hình T5-Large trên sáu nhiệm vụ.
Vui lòng tham khảo Phần 6 để biết chi tiết thí nghiệm.

2 3 4 5 6 70.70.80.911.1
Lấy Trung bình Đơn giản Task Arithmetic TIES
Số Nhiệm vụHiệu suất Chuẩn hóa Trung bình

Hình 13: T5-Base với số lượng nhiệm vụ được hợp nhất tăng lên. Hiệu suất trung bình khi hợp
nhất một số lượng nhiệm vụ khác nhau.

C.4 Hợp nhất khi Vắng mặt Tập Xác thực
Trong nghiên cứu của chúng tôi về các tình huống khi tập xác thực không có sẵn, chúng tôi đã tạo ra một
công thức và xác định các siêu tham số tối ưu, sử dụng quy trình thí nghiệm PEFT được chi tiết trong Phần
6. Cách tiếp cận này được áp dụng cho mười một mô hình cụ thể cho từng nhiệm vụ được trình bày trong
cùng phần, sử dụng phương pháp TIES-MERGING để điều chỉnh các siêu tham số. Các ước tính sơ bộ của
chúng tôi cho các siêu tham số là k= 20 vàλgần 1. Việc tìm kiếm siêu tham số được thực hiện bằng cách
sử dụng mười một mô hình (IA)3cụ thể cho từng nhiệm vụ, với k∈ {10,20,30}, và λtrải từ 0.8 đến 3.0,
theo bước nhảy 0.1. Kết quả của việc tìm kiếm toàn diện này cho thấy giá trị tối ưu của k= 20 , với các giá
trị λ= 0.9,λ= 1.0, và λ= 1.1cho thấy hiệu suất tương đương. Để duy trì sự đơn giản trong mô hình của
chúng tôi, chúng tôi chọn giá trị λlà 1. Do đó, lựa chọn cuối cùng của các tham số cho TIES-MERGING là
k= 20 , dấu dựa trên khối lượng, trung bình tách biệt, và giá trị λlà 1.

C.5 Hợp nhất Số Lượng Nhiệm vụ Khác nhau
Ở đây chúng tôi cung cấp một số chi tiết bổ sung về các thí nghiệm khi hợp nhất số lượng nhiệm vụ khác
nhau. Trong Hình 5, chúng tôi thực hiện thí nghiệm với T5-Large khi hợp nhất bảy nhiệm vụ được xem xét
trong Tab. 1 và được mô tả trong Mục. 6. Trục x cho thấy số lượng nhiệm vụ khác nhau được hợp nhất. Lưu
ý rằng khi hợp nhất Tnhiệm vụ, chúng ta có tổng cộng7
T
tổ hợp. Tuy nhiên, trong thí nghiệm của chúng tôi,
chúng tôi lấy mẫu tối đa 10 tổ hợp riêng biệt cho mỗi giá trị của T. Một biểu đồ tương tự cho mô hình T5-
Base được hiển thị trong Hình 13.

--- TRANG 23 ---
Trong Hình 5, cho mỗi số lượng nhiệm vụ chúng tôi lấy tối đa 10tập con ngẫu nhiên của 8 nhiệm vụ mà
chúng tôi đang xem xét. Đường liền nét là trung bình hiệu suất của mô hình đã hợp nhất từ các lần chạy khác
nhau này. Dưới đây chúng tôi cung cấp các giá trị λtối ưu cho các tập con nhiệm vụ khác nhau mà chúng tôi
đã hợp nhất cho cả TIES-MERGING và Task Arithmetic, lưu ý rằng đối với lấy trung bình λ=1
#nhiệm vụluôn
luôn. Mỗi mục trong danh sách là λtối ưu cho một tập con nhiệm vụ cụ thể được chọn trên tập xác thực.

(2 nhiệm vụ) TIES →[1.7, 1.9, 2, 2, 1.1, 1.5, 1.6, 1.8, 1.9, 1., 5]
(2 nhiệm vụ) Task Arithmetic →[1, 0.9, 1, 1, 0.9, 1, 0.9, 0.9, 0.9, 1]
(3 nhiệm vụ) TIES →[1.2, 2, 1.5, 1.9, 1.8, 1.7, 1.4, 2, 3, 1.9]
(3 nhiệm vụ) Task Arithmetic →[1, 0.7, 0.7, 1, 1, 0.9, 0.7, 0.7, 0.9, 1]
(4 nhiệm vụ) TIES →[1.5, 1.3, 1.3, 1.8, 2.3, 1.7, 1.8, 1.7, 1.9, 1.5]
(4 nhiệm vụ) Task Arithmetic →[0.8, 0.7, 0.7, 0.7, 0.6, 0.7, 0.7, 0.8, 0.6, 0.7]
(5 nhiệm vụ) TIES →[2, 2, 2, 1.8, 1.7, 2, 1.6, 2.1, 1.6, 1.3]
(5 nhiệm vụ)Task Arithmetic →[0.7, 0.8, 0.6, 0.8, 0.7, 0.6, 0.6, 0.6, 0.6, 0.7]
(6 nhiệm vụ) TIES →[1.6, 1.7, 1.7, 1.2, 1.7, 1.7, 1.5]
(6 nhiệm vụ) Task Arithmetic →[0.6, 0.5, 0.5, 0.5, 0.7, 0.5, 0.6]
(7 nhiệm vụ) TIES →[1.7]
(7 nhiệm vụ) Task Arithmetic →[0.5]

C.6 Chi tiết Huấn luyện
Trong nghiên cứu của chúng tôi, chúng tôi đã sử dụng hai biến thể của mô hình T5, cụ thể là các mô hình T5-
base và T5-large, được huấn luyện tối đa 75,000 bước. Kích thước batch huấn luyện hiệu quả là 1024 được
triển khai, cùng với tốc độ học (lr) là 0.0001. Chúng tôi thiết lập cơ chế dừng sớm với ngưỡng kiên nhẫn
là 5 để ngăn chặn overfitting. Trong quá trình huấn luyện, bfloat16 được áp dụng để giảm chi phí bộ nhớ
GPU, và độ dài chuỗi tối đa được đặt ở 128. Ngược lại, đối với cấu hình PEFT của cách tiếp cận (IA)3trên
mô hình T0-3B, chúng tôi đã sửa đổi các tham số của mình. Kích thước batch huấn luyện hiệu quả là 16 được
triển khai cùng với kích thước batch đánh giá là 32, trong khi duy trì tốc độ học ở 0.0001. Để phù hợp với
độ phức tạp của mô hình, kiên nhẫn dừng sớm được tăng lên 10. Chúng tôi không sử dụng bất kỳ bộ lập lịch
lr và weight decay nào cho việc huấn luyện mô hình của mình.

Để đánh giá, chúng tôi thực hiện phân loại xếp hạng . Trong phương pháp này, xác suất log của mô hình cho
tất cả các chuỗi nhãn tiềm năng được xếp hạng. Dự đoán của mô hình được coi là chính xác nếu lựa chọn
xếp hạng cao nhất phù hợp với câu trả lời đúng. Cần lưu ý rằng đánh giá phân loại xếp hạng có thể phù hợp
với cả nhiệm vụ phân loại và nhiệm vụ lựa chọn đa phương án.
