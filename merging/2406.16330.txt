# 2406.16330.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/merging/2406.16330.pdf
# File size: 2433042 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 1
Pruning via Merging: Compressing LLMs via
Manifold Alignment Based Layer Merging
Deyuan Liu∗, Zhanyue Qin∗, Hairu Wang, Zhao Y ang, Zecheng Wang, Fangying Rong, Qingbin Liu,
Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, and Dianbo Sui.
Abstract —While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in
resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the
knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer
Merging Compression (MKA), a novel approach that uses manifold learning and the Information Bottleneck (IB) measure to merge
similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and
various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios,
outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression.
Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal
performance decrease of only 2.82%. The proposed MKA method offers a resource-efficient and performance-preserving model
compression technique for LLMs. We make our code available at https://github.com/SempraETY/Pruning-via-Merging
Index Terms —Model Compression, Layer Merging, Manifold Learning, Large Language Models (LLMs)
✦
1 I NTRODUCTION
LARGE Language Models (LLMs), such as GPT-4 [1],
Llama-3 [2], Llama-2 [3] and Mistral [4], have demon-
strated remarkable proficiency in language understanding
and generation. These models, with billions of parameters
trained on trillions of tokens, can handle complex tasks and
exhibit emergent abilities [5], [6]. While these models have
achieved unprecedented success, their growing complexity
and scale have brought to the fore significant challenges in
•Deyuan Liu, Zhanyue Qin, Zecheng Wang, Zhiying Tu, Dianhui Chu,
and Dianbo Sui are with Harbin Institute of Technology, Harbin, China.
E-mail: 2022211994@stu.hit.edu.cn; 2021211875@stu.hit.edu.cn;
22s130467@stu.hit.edu.cn; tzy_hit@hit.edu.cn; chudh@hit.edu.cn;
suidianbo@hit.edu.cn
•Hairu Wang is with University of Science and Technology of China,
Hefei, China.
E-mail: hrwang00@mail.ustc.edu.cn
•Zhao Yang is with Institute of Automation, Chinese Academy of Sciences,
Beijing, China.
E-mail: zhao.yang@nlpr.ia.ac.cn
•Fangying Rong is with Shandong Agricultural University, Shandong,
China.
E-mail: rongfangying@gmail.com
•Qingbin Liu, Xi Chen are with Tencent Inc., Shenzhen, China.
E-mail: qingbinliu@tencent.com; marshao@tencent.com; ryan-
bli@tencent.com; jasonxchen@tencent.com.
•Cunhang Fan, Zhao Lv are with Anhui University, Anhui, China.
E-mail: cunhang.fan@ahu.edu.cn; kjlz@ahu.edu.cn.
•∗These authors contributed equally to this work.
•Dianbo Sui is the corresponding author.
This work has been submitted to the IEEE for possible publication. Copyright
may be transferred without notice, after which this version may no longer be
accessible.terms of computational resources, memory requirements,
and energy consumption [7], [8], raising concerns about
their sustainability.
To mitigate these challenges, researchers have developed
various model compression techniques in LLM to reduce
its parameter size while preserving performance [9], [10],
[11], [12], [13]. These techniques can be roughly categorized
into two main mainstreams [14]: quantization [15], [16], [16],
[17], [18] and pruning [19], [20], [21], [22]. Quantization
based methods aid in the reduction of the memory con-
sumption of weights, activations, and KV caches by using
the low-precision values with fewer bits instead of the high-
precision values. However, the acceleration benefits of quan-
tization are seriously dependent on hardware support [23]
and sometimes require additional fine-tuning to maintain
performance [14], [24]. Compared to quantization, prun-
ing, especially structural pruning [25], eliminates redundant
LLM’s parameters to decrease the overall parameter count,
and can be applied directly to a trained LLM without
retraining and is generally more hardware-friendly than
quantization approaches. While effective, pruning usually
risks losing valuable model structures and determining how
to prune the LLM with minimal disruption to the origin
remains an unsolved problem [26].
To tackle this issue head-on, we delve into the realm of
model merging [27], a powerful technique that seamlessly
weaves together the strengths and knowledge of multiple
models, creating a robust and efficient aggregation. This
technique, through averaging the weights of multiple mod-
els with the same architecture, can retain essential features
without significant additional resources [28], [29]. Further-
more, by offsetting the biases and errors of individual
models, model merging often leads to greatly improved
performance [30]. Additional, the number of models in the
merging process can be gradually and naturally reduced.arXiv:2406.16330v2  [cs.CL]  17 May 2025

--- PAGE 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 2
Fig. 1. Manifold-Based Knowledge Alignment and Layer Merging (MKA) framework consists of two main components: (1) The left side illustrates
manifold learning for LLM knowledge extraction, where layer activations are transformed into low-dimensional manifolds using the Diffusion Kernel
algorithm. (2) The right side depicts the similarity-based layer merging process, employing the IB metric to identify layers with aligned knowledge.
However, such a useful technology are limited to merging
between models currently, and few studies pay attention on
merging the same internal structures within a model.
This raises the question of whether model compression
could be achieved by reducing the total number of layers
through the progressive aggregation of knowledge between
layers. To answer this question, we introduce Manifold-
Based Knowledge Alignment and Layer Merging Compres-
sion (MKA) in this paper. MKA combines manifold learning
and layer merging to preserve essential information while
significantly reducing LLM parameter size. As illustrated
in Figure 1, our method mainly comprises two primary
components:
Manifold Learning for LLM Knowledge: We employ
manifold learning techniques to align knowledge across
layers by extracting layer activations from a LLM and
applying the Diffusion Kernel algorithm [31] to learn low-
dimensional manifold representations. This approach cap-
tures the nonlinear structure in the activation and achieves
dimensionality reduction while preserving important ac-
tivation features, enabling more effective comparison of
knowledge patterns across different layers.
Similarity Alignment Layer Merging: Following mani-
fold learning, we use the Information Bottleneck (IB) mea-
sure [32] to construct a similarity matrix that quantifies
the similarity between layers by maximizing their mutual
information while considering the entropy of each layer.
Based on this similarity matrix, we select the most similar
layer pairs for merging.
To rigorously validate the effectiveness of MKA, we
conduct extensive empirical evaluations on a diverse array
of benchmark datasets, like MMLU and PIQA, and a wide
range of state-of-the-art large language models, including
Llama-3 series with 8B and 70B parameters, Llama-2 series
with 7B and 13B parameters, and Mixtral-7B. Our experi-mental results indicate that MKA can maintain good perfor-
mance while achieving a significant compression ratio, out-
performing existing pruning methods and achieving even
greater compression when combined with quantization. For
example, on the MMLU dataset with Llama3-8B, MKA can
achieve a compression ratio of 43.75% with only a 2.82%
performance drop.
In summary, the main contributions of this paper are as
follows:
•We introduce MKA, an innovative model compression
technique that leverages manifold learning to align and
integrate knowledge across layers, achieving significant
reductions in model size while preserving performance.
•We develop a manifold-based knowledge alignment ap-
proach, utilizing the Diffusion Kernel and Information
Bottleneck (IB) to effectively capture and align similari-
ties between layers in the parameter space.
•We validate the efficacy of MKA through comprehen-
sive experiments on multiple benchmark datasets and
a variety of large language models, demonstrating its
capability to achieve substantial compression without
compromising model performance.
2 R ELATED WORK
Our proposed Manifold-Based Knowledge Alignment and
Layer Merging (MKA) framework builds upon and inte-
grates several key areas of research in deep model opti-
mization for efficiency. In the following subsections, we
first explore methods for learning high-dimensional data,
which are essential for understanding the complex struc-
tures within large language models. We then review the
primary model compression techniques, including quanti-
zation, pruning, and knowledge distillation, which aim to
reduce the computational and memory footprint of these

--- PAGE 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 3
models. Finally, we delve into the emerging field of model
merging, highlighting recent advancements that inform our
approach to aggregating and aligning knowledge across
model layers.
2.1 Methods for Learning High-Dimensional Data.
High-dimensional data poses significant challenges in ma-
chine learning, primarily due to the curse of dimensional-
ity, which can lead to increased computational complexity
and overfitting [33]. To address these challenges, manifold
learning techniques have been extensively developed to
capture the underlying low-dimensional structures within
high-dimensional datasets. Techniques such as Principal
Component Analysis (PCA) [34], t-Distributed Stochastic
Neighbor Embedding (t-SNE) [35], and Uniform Manifold
Approximation and Projection (UMAP) [36] have been
widely adopted for dimensionality reduction and data vi-
sualization. More recently, diffusion-based methods like
Diffusion Maps [37] and the Diffusion Kernel algorithm
[31] have shown promise in preserving the intrinsic geom-
etry of data while facilitating efficient computation. These
manifold learning approaches are crucial for understanding
and aligning complex patterns in high-dimensional spaces,
making them well-suited for applications in large language
model (LLM) compression and knowledge alignment.
2.2 Model Compression Methods.
Model compression has emerged as a vital area of research
aimed at reducing the computational and memory footprint
of large-scale models without significantly compromising
their performance [9], [10], [11], [12], [13]. The primary tech-
niques in model compression include quantization, pruning,
and knowledge distillation.
Quantization involves reducing the precision of the
model’s weights and activations from high-bit representa-
tions (e.g., 32-bit floating-point) to lower-bit formats (e.g.,
8-bit integers), thereby decreasing memory usage and accel-
erating inference [15], [16], [17], [18]. While effective, quan-
tization often requires specialized hardware support and
may necessitate additional fine-tuning to maintain model
accuracy [14], [24].
Pruning techniques focus on eliminating redundant or
less significant parameters from the model, thereby reduc-
ing the overall parameter count and computational require-
ments [19], [20], [21], [22]. Pruning can be categorized into
unstructured pruning, which removes individual weights,
and structured pruning, which removes entire neurons or
channels [25]. Structural pruning is particularly advanta-
geous for its hardware friendliness and the ability to directly
apply it to pre-trained models without extensive retraining
[26].
Knowledge Distillation involves training a smaller "stu-
dent" model to replicate the behavior of a larger "teacher"
model, thereby transferring knowledge while reducing
model size [38]. This approach has been effective in main-
taining performance levels while achieving significant com-
pression.
Recent advancements have also explored hybrid ap-
proaches that combine multiple compression techniques to
leverage their complementary strengths [14]. These methodsaim to achieve higher compression ratios and better perfor-
mance retention by integrating quantization, pruning, and
distillation strategies.
2.3 Model Merging.
Model merging is a burgeoning area that focuses on combin-
ing multiple models to create a single, more robust model by
leveraging the strengths and knowledge of its constituents
[27]. Traditional approaches, such as Model Soup [27], uti-
lize simple weight averaging to merge models with the same
architecture, effectively blending their learned representa-
tions. However, this method can be limited by the need
for identical architectures and the potential for performance
degradation if the individual models are not sufficiently
aligned.
Advancements in model merging have introduced more
sophisticated techniques to enhance the robustness and per-
formance of the merged models. Checkpoint Merging [28]
employs Bayesian optimization to selectively weight and
integrate different model checkpoints, resulting in a more
stable and high-performing merged model. Similarly, Mind-
Merger [39] facilitates the fusion of models with varying
specializations, thereby enhancing the overall capabilities of
the resultant model by integrating diverse knowledge bases.
Dynamic expert merging methods, such as DELLA-
Merging [40], dynamically incorporate specialized expert
models during inference, allowing the merged model to
adapt to a wide range of tasks. Adaptive weighting ap-
proaches like AdaMerging [41] and MetaGPT [42] leverage
meta-learning and dynamic weighting schemes to fine-tune
the merging process, ensuring optimal integration of the
strengths of constituent models.
Furthermore, task-oriented merging strategies, includ-
ing Task Arithmetic [43], Language and Task Arithmetic
[44], and Task Arithmetic in Tangent Space [45], focus on
blending models trained on different tasks to create versatile
LLMs capable of handling multiple applications. These ap-
proaches enhance the flexibility and applicability of merged
models, making them more adaptable to diverse real-world
scenarios.
Despite the progress in model merging, most existing
methods focus on merging distinct models rather than ad-
dressing the internal layer structures within a single model.
Our proposed Manifold-Based Knowledge Alignment and
Layer Merging (MKA) framework addresses this gap by
enabling the compression of LLMs through the progressive
aggregation of knowledge across layers, thereby reducing
the total number of layers while preserving essential model
performance.
3 P RELIMINARIES
In this section, we introduce the foundational concepts
and theoretical frameworks essential for understanding
the proposed Manifold-Based Knowledge Alignment and
Layer Merging Compression (MKA) method. We begin by
discussing overparameterization and redundancy in Large
Language Models (LLMs), then present the manifold hy-
pothesis in the context of neural representations and dif-
fusion geometry, and finally introduce mutual information
within the Information Bottleneck (IB) framework.

--- PAGE 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 4
3.1 Overparameterization and Redundancy in Large
Language Models
LetD={(xi, yi)}N
i=1denote a dataset, where xi∈ X
are input sequences and yi∈ Y are the corresponding
targets. A Large Language Model (LLM) is modeled as
a parameterized function fθ:X → Y with parameters
θ∈Rd, where dis the total number of parameters. An
LLM is said to be overparameterized ifd≫N. Overpa-
rameterization often results in redundancy; that is, different
parameter configurations yield similar functional mappings.
Formally, for a given ϵ >0and small constant δ, a subset of
parameters S ⊂θis considered redundant if there exists an
alternative parameter set θ′satisfying
∥θ′−θ∥ ≤ϵand L(fθ′,D)≤L(fθ,D) +δ. (1)
This observation motivates the exploration of layer merging
as a strategy for model compression.
3.2 Manifold Hypothesis in Neural Representations
and Diffusion Geometry
The manifold hypothesis posits that high-dimensional data
encountered in real-world applications lie on a low-
dimensional manifold embedded in the ambient space RD.
A manifold Mof dimension kis a topological space such
that for every point p∈ M , there exists a neighborhood U
and a homeomorphism ϕ:U→Rk.
In the context of neural networks, let Hl
i∈Rdldenote
the activation of layer lfor input xi. The collection {Hl
i}N
i=1
is assumed to lie on a manifold Mlof intrinsic dimension
kl(with kl≪dl) embedded in Rdl. This assumption is
supported by the observation that deep networks implicitly
perform dimensionality reduction.
To analyze the intrinsic geometry of the activation
manifold, we utilize diffusion geometry. Consider a graph
G= (V,E)where each node corresponds to an activation
vector and the edge weights are defined via a Gaussian
kernel. Specifically, for two activation vectors Hl
iandHl
j,
the affinity is given by
Wij= exp 
−∥Hl
i−Hl
j∥2
σ2!
, (2)
where σ > 0controls the neighborhood scale. The degree
matrix Dis defined by
Dii=NX
j=1Wij, (3)
and the diffusion operator is then
P=D−1W. (4)
The operator Pcaptures the connectivity of the data man-
ifold by serving as the transition probability matrix for a
random walk on G.
The diffusion map is constructed via the spectral de-
composition of P. Suppose that Pis reversible (which
follows from the symmetry of W) and let {(λj, ϕj)}N
j=1be
its eigenpairs. The diffusion map at time tis defined as
Φt(i) = λt
1ϕ1(i), λt
2ϕ2(i), . . . , λt
kϕk(i). (5)
In many cases, the first eigenvector corresponding to λ1= 1
is omitted to focus on the nontrivial geometry; the indexing
may be adjusted accordingly.3.3 Mutual Information and the Information Bottleneck
Principle
Mutual information is used to quantify the similarity be-
tween representations. For a continuous random variable X
with density p(x), the differential entropy is defined as
H(X) =−Z
p(x) logp(x)dx. (6)
The mutual information between XandYis given by
I(X;Y) =H(X) +H(Y)−H(X,Y). (7)
Within our framework, mutual information serves as a met-
ric for comparing the diffusion map embeddings of different
layers.
The Information Bottleneck (IB) principle provides a
framework for obtaining a compressed representation that
retains relevant information. For random variables Xand
Y, the IB objective is to find a mapping p(T|X)that mini-
mizes
min
p(T|X)I(X;T)−βI(T;Y), (8)
where β >0balances compression against the preservation
of relevant information. In our context, the goal is to merge
the diffusion map embeddings from two layers into a single
representation that maximizes mutual information with the
target variable Ywhile minimizing redundancy.
4 M ANIFOLD LEARNING FOR INTERNAL REPRE -
SENTATIONS
In this section, we introduce the Manifold-Based Knowl-
edge Alignment (MKA) framework, which leverages the
manifold hypothesis and diffusion geometry to identify
and merge redundant layers within large language models
(LLMs). Building on the concepts outlined in Section 3, we
describe the extraction of high-dimensional activations, the
construction of the diffusion operator, the spectral decompo-
sition leading to diffusion maps, and the subsequent align-
ment and merging of layers using information-theoretic
measures.
4.1 Extraction of High-Dimensional Activations
LetMdenote an LLM processing a dataset D={xi}N
i=1.
For each input xi, the model produces activations at each
layer. Specifically, we denote by Hl
i∈Rdlthe activation of
layer lfor input xi, where dlis the activation dimension at
layer l. The activations are computed via
H0
i= Embed( xi), (9)
Hl
i=fθl(Hl−1
i), l= 1,2, . . . , L, (10)
where fθldenotes the transformation associated with layer
l, parameterized by θl. The collection {Hl
i}N
i=1thus forms a
high-dimensional dataset intrinsic to each layer.

--- PAGE 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 5
4.2 Construction of the Diffusion Operator
To analyze the manifold structure underlying these acti-
vations, we construct a weighted graph Gl= (Vl,El)for
each layer l, where each node corresponds to an activation
vector Hl
i. The edges are weighted according to the affinity
between activation vectors.
Given two activation vectors Hl
iandHl
j, we define the
affinity using a Gaussian kernel:
Wij=K(Hl
i,Hl
j) = exp 
−∥Hl
i−Hl
j∥2
σ2!
, (11)
where σ > 0is a bandwidth parameter that controls the
local neighborhood scale.
The degree matrix Dis diagonal with entries
Dii=NX
j=1Wij. (12)
We then define the diffusion operator as the normalized
affinity matrix:
P=D−1W. (13)
This operator governs the transition probabilities of a ran-
dom walk on the graph Gland thus captures the intrinsic
geometry of the activation manifold.
4.3 Spectral Decomposition and Diffusion Maps
The operator Pencodes the manifold geometry, and
its spectral properties allow us to construct diffusion
maps—lower-dimensional embeddings that preserve the
manifold’s structure.
Assume that the affinity matrix Wis symmetric and
positive semidefinite and that Pis reversible (which is
the case when the underlying Markov chain is reversible).
Although Pis not symmetric, it is similar to the symmetric
matrix
eP=D−1/2WD−1/2. (14)
SinceePis symmetric, it has a complete set of real eigenval-
ues{λk}N
k=1and corresponding orthonormal eigenvectors
{ϕk}N
k=1, so that
Pϕk=λkϕk. (15)
Without loss of generality, we order the eigenvalues as
1 =λ1≥λ2≥ ··· ≥ λN≥ −1. (16)
The largest eigenvalue is λ1= 1 , and its corresponding
eigenvector is constant.
Using the spectral decomposition, the diffusion map at
timetis defined as the mapping
Φt(Hl
i) =
λt
2ϕ2(i), λt
3ϕ3(i), . . . , λt
k+1ϕk+1(i)
,(17)
where kis the target embedding dimension and ϕj(i)
denotes the ith component of ϕj. (Note that the trivial
eigenvector associated with λ1is omitted.)4.4 Preservation of Manifold Structure
The diffusion map is designed to preserve the intrinsic ge-
ometry of the activation manifold by capturing multi-scale
connectivity. In particular, the Euclidean distance in the
diffusion space approximates the diffusion distance between
points.
For any two activation vectors Hl
iandHl
j, the diffusion
distance is defined by
Dt(i, j)2=k+1X
k=2λ2t
k ϕk(i)−ϕk(j)2. (18)
By the very definition of the diffusion map,
∥Φt(Hl
i)−Φt(Hl
j)∥2
2=k+1X
k=2λ2t
k ϕk(i)−ϕk(j)2,(19)
so that the Euclidean distance in the embedding space
approximates the diffusion distance. Moreover, as t→ ∞ ,
the influence of the smaller eigenvalues diminishes, and the
diffusion map increasingly emphasizes the global structure
of the manifold.
4.5 Layer Similarity Measure via Mutual Information
To quantify the similarity between representations in dif-
ferent layers, we compare their diffusion map embeddings.
Let
Ψl= Φt(Hl)and Ψm= Φt(Hm) (20)
be the diffusion map embeddings for layers landm, re-
spectively. We define their similarity in terms of the mutual
information (MI)
I(Ψl;Ψm) =H(Ψl) +H(Ψm)−H(Ψl,Ψm), (21)
where H(·)denotes differential entropy. In order to simplify
the calculation, we assume that ΨlandΨmare jointly
Gaussian random variables. Under this joint Gaussianity
assumption, the MI can be written in closed form as
I(Ψl;Ψm) =1
2ln 
|ΣΨl||ΣΨm|
|ΣΨl,Ψm|!
, (22)
where ΣΨlandΣΨmdenote the covariance matrices of Ψl
andΨm, respectively, and ΣΨl,Ψmis their joint covariance
matrix.
To facilitate comparison across different layer pairs, we
further define the normalized mutual information (NMI) as
Slm=I(Ψl;Ψm)p
H(Ψl)H(Ψm). (23)
This normalization constrains Slmto lie in a consistent
range, thereby enabling meaningful similarity assessments
between layers.
4.6 Manifold-Based Knowledge Alignment and Layer
Merging Compression (MKA)
Algorithm 1 summarizes the MKA procedure. In brief, for
each layer the activations are first extracted and embedded
using the diffusion map. Next, pairwise similarities between
layers are computed via their mutual information. Finally,
when the similarity score between a pair of layers exceeds a
predetermined threshold τ, the layers are merged according
to a weighted combination of their parameters.

--- PAGE 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 6
Algorithm 1 Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA)
Require: LLMMwith layers {L1, L2, . . . , L N}and parameters Θ ={θ1, θ2, . . . , θ N}, dataset ω
Ensure: Compressed model M∗with aligned representations
1:Extract activations H={Hl
i}for each layer lon dataset ω
2:foreach layer ldo
3: Compute pairwise distances among activations {Hl
i}
4: Construct the affinity matrix W(l)using a Gaussian kernel with bandwidth σ
5: Compute the diffusion map embedding Ψlvia the eigendecomposition of W(l)
6:end for
7:foreach pair of layers (l, m)do
8: Estimate the covariance matrices ΣΨl,ΣΨm, and joint covariance ΣΨl,Ψm
9: Compute I(Ψl;Ψm)and the normalized similarity score Slm
10:end for
11:while there exists a pair (l, m)with Slm≥τdo
12: Determine the merging weight α, for example via α=Slm(or, alternatively, using a softmax formulation)
13: Merge the parameters: eθc=α θl+ (1−α)θm
14: Replace layers landmwith the merged layer using eθcand update Maccordingly
15:end while
16:return Compressed model M∗
4.7 Layer Merging via the Information Bottleneck Prin-
ciple
To guide the layer merging process, we adopt an
information-theoretic perspective based on the Information
Bottleneck (IB) principle. The IB framework seeks to extract
a compressed representation that preserves the information
most relevant to a target variable.
In its standard form, given random variables XandY,
the IB objective is to find a mapping p(T|X)that minimizes
LIB=I(X;T)−β I(T;Y), (24)
where β > 0balances the trade-off between compression
(minimizing I(X;T)) and prediction (maximizing I(T;Y)).
For merging layers landm, we set X= (Ψl,Ψm)and
seek a compressed representation Ψcthat captures their
shared information. The corresponding IB objective is
LIB=I (Ψl,Ψm);Ψc−β I(Ψc;Y). (25)
To make the optimization tractable, we restrict the mapping
to a deterministic linear combination:
Ψc=αΨl+ (1−α)Ψm, α∈[0,1]. (26)
Under the joint Gaussian assumption, the covariance of
Ψcis given by
ΣΨc=α2ΣΨl+ (1−α)2ΣΨm+ 2α(1−α) ΣΨl,Ψm,(27)
and if ΣΨc,Ydenotes the cross-covariance between Ψcand
Y, then
ΣΨc,Y=αΣΨl,Y+ (1−α) ΣΨm,Y. (28)
The conditional covariance is defined as
ΣΨc|Y= ΣΨc−ΣΨc,YΣ−1
YΣY,Ψc. (29)
Since Ψcis a deterministic function of (Ψl,Ψm)(i.e.,
H(Ψc|(Ψl,Ψm)) = 0 ), the mutual information terms re-duce to differential entropies. In particular, using the stan-
dard Gaussian formula for differential entropy,
H(Ψc) =1
2ln
(2πe)dc|ΣΨc|
, (30)
H(Ψc|Y) =1
2ln
(2πe)dc|ΣΨc|Y|
, (31)
the IB objective simplifies to
LIB=1
2h
(1−β) ln|ΣΨc|+βln|ΣΨc|Y|i
+ const ,(32)
where the constant term is independent of α.
In principle, one may optimize LIBwith respect to αby
differentiating
∂LIB
∂α=1
2"
(1−β) Tr
Σ−1
Ψc∂ΣΨc
∂α
+βTr
Σ−1
Ψc|Y∂ΣΨc|Y
∂α#
(33)
and setting the derivative to zero. For example, one may
compute
∂ΣΨc
∂α= 2αΣΨl−2(1−α) ΣΨm+ 2(1−2α) ΣΨl,Ψm.
(34)
A similar expression can be derived for ∂ΣΨc|Y/∂α. In
practice, however, this equation does not admit a closed-
form solution. Hence, we adopt an approximate strategy by
setting
α=Slm, (35)
where Slmis the normalized mutual information defined
earlier. This heuristic assigns greater weight to the layer
with higher shared information, thereby approximately
minimizing the IB objective while maintaining computa-
tional efficiency.
With the weight αdetermined, the parameters of layers
landmare merged via
eθc=αθl+ (1−α)θm. (36)
This merged parameter set eθcis used to replace the original
layers, resulting in a compressed model M∗.

--- PAGE 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 7
4.8 Impact on Model Performance
Merging layers alters the parameter space of the model,
which may in turn affect its performance.
Letδθ=eθc−θdenote the shift in parameters due
to merging. Suppose that the loss function L(θ)is twice
differentiable and locally convex in a neighborhood around
θ. A second-order Taylor expansion about θyields
L(eθc)≈ L(θ) +∇L(θ)⊤δθ+1
2δθ⊤∇2L(θ)δθ. (37)
At a local minimum where ∇L(θ) =0, the increase in loss
is bounded by
∆L=L(eθc)− L(θ)
≤1
2λmax∥δθ∥2, (38)
where λmaxis the largest eigenvalue of the Hessian ∇2L(θ).
This bound follows from the Rayleigh quotient and provides
a guarantee on the loss increase due to the merging opera-
tion.
5 E XPERIMENTS
We conduct a comprehensive set of experiments to evaluate
the effectiveness and generalizability of our MKA method
across various domains. Moreover, we aim to compare our
approach with pruning techniques to assess whether it
offers improvements and to investigate if it can be combined
with quantization methods to achieve even higher compres-
sion ratios.
5.1 Experimental Setup
5.1.1 Datasets
We conduct evaluations using the MKA methods across var-
ious benchmark datasets, each specifically designed to test
various facets of language comprehension and generation.
In detail, MMLU [46] evaluates broad language understand-
ing across a wide range of domains. PIQA [47] is designed
to test models on commonsense reasoning in the physical
world, aiming to assess NLP models’ grasp of everyday
physical interactions. HellaSwag [48] is a challenge dataset
for commonsense natural language inference, consisting
of event descriptions with multiple possible continuations,
where the task is to select the most plausible one. RACE-
H[49] is a large-scale reading comprehension dataset col-
lected from English exams for Chinese high school students,
featuring a high proportion of questions that require reason-
ing.BoolQ [50] is a reading comprehension dataset focusing
on naturally occurring yes/no questions that often query
for complex, non-factoid information and require difficult
entailment-like inference to answer correctly.
5.1.2 LLMs
In our experiments, we employ the Llama2 [3], Llama3,
Llama3.2, and Mistral [51] models, each distinct in their ca-
pabilities and configurations: Llama2 : Encompassing mod-
els from 7 billion to 13 billion parameters, exhibits superior
performance and safety on diverse benchmarks. Llama3 :
Featuring models with 8 billion parameters, which offersstate-of-the-art performance and advanced reasoning capa-
bilities. Llama3.2 : Featuring models with 3 billion parame-
ters that balances performance and number of parameters.
Mistral : We use the 7 billion parameter version of Mistral
that surpasses Llama-2 and Llama-1 in performance and
efficiency, leveraging grouped-query and sliding window
attention mechanisms for optimal inference across lengthy
sequences.
5.1.3 Baselines
In this study, we assess the effectiveness of our proposed
method, MKA, through two distinct comparative analyses.
Firstly, we evaluate MKA directly against several well-
established pruning techniques to gauge its standalone effi-
cacy in reducing model size while maintaining performance.
Secondly, we extend the comparison to include scenarios
where both the traditional pruning methods and MKA
are further enhanced through quantization. The baseline
methods included in our analysis are: PruneMe [52]: A
pruning method identifies the optimal block of layers to
prune by considering the similarity across layers. SLEB [53]:
A pruning method designed to streamline LLMs by elimi-
nating redundant transformer blocks. We choose the trans-
former block as the fundamental unit for pruning, because
LLMs exhibit block-level redundancy with high similarity
between the outputs of neighboring blocks. Shortened [54]:
A pruning method first uses a simple metric to identify
unimportant blocks and then performs a simple one-shot
pruning. ShortGPT [14]: A pruning method that removes
redundant layers from large language models based on
a Block Influence metric, which assesses the significance
of each layer. Reverse : A heuristic approach where the
importance of layers is considered inversely proportional to
their order in the model, prioritizing the retention of earlier
layers. SmoothQuant [55]: SmoothQuant is a training-free
post-training quantization solution that enables efficient 8-
bit weight and activation quantization for large language
models, offering up to 1.56× speedup and 2× memory re-
duction with minimal accuracy loss. GPTQ [56]: A one-shot
weight quantization method that uses approximate second-
order information to maintain high accuracy even with
severe weight reduction. AWQ [57]: A novel quantization
approach that protects salient weights by adjusting per-
channel scaling based on activation observations rather than
weight Magnitudes.
6 E XPERIMENTS
6.1 Comparison of MKA with other structured pruning
methods
We compare the performance of MKA with baseline com-
pression methods on the MMLU dataset using the Llama2-
7B, Llama2-13B, Llama3-8B, Llama3.2-3B, and Mistral-7B
models. The evaluation metric is Accuracy (ACC) during
merging and pruning. The results are presented in Figure 2.
From Figure 2, we can observe that, across all models,
our method improves the compression ratio while main-

--- PAGE 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 8
0.0 0.1 0.2 0.3 0.4 0.5
Ratio0.250.300.350.400.45Average Accuracy
Llama-2-7b-hf
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (ours)
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Ratio0.250.300.350.400.450.500.55Average Accuracy
Llama-2-13b-hf
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (ours)
0.0 0.1 0.2 0.3 0.4 0.5
Ratio0.30.40.50.6Average Accuracy
Meta-Llama-3-8B
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (ours)
0.0 0.1 0.2 0.3 0.4 0.5
Ratio0.250.300.350.400.450.500.55Average Accuracy
Llama-3.2-3B
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (ours)
0.0 0.1 0.2 0.3 0.4 0.5
Ratio0.30.40.50.6Average Accuracy
Mistral-7B-v0.1
PruneMe
SLEB
Shortened
ShortGPT
Reverse
MKA (ours)
Fig. 2. Performance (Accuracy) of LLMs (Llama2-7B, Llama2-13B, Llama3-8B, Llama3.2-3B, and Mistral-7B) on the MMLU dataset as the pruning
ratio of various pruning methods increases.
taining performance. Specifically, the compression ratio3
for Llama2-7B reach 31%, for Llama3-8B reach 44%, for
Llama3.2-3B reach 43%, for Mistral-7B it reaches 40%, and
for Llama2-13B it reaches an impressive 58%. Additionally,
we observe several phenomena: both methods experience
a collapse in model performance, but the model merging
method can delay the layer collapse to some extent and
stabilize the model’s performance very well. Since our strat-
egy is based on Reverse Prune, the scores for the Llama2-
7B, Llama2-13B, Llama3-8B, and Llama3.2-3B models are
very close to the Reverse Prune. Our hypothesis is that the
pruning or merging of these models is similar, but model
merging can adjust the merging ratio to surpass the effect of
pruning. Moreover, for the Mistral-7B models, we noticed
that the results do not closely match the Reverse Prune.
6.2 How Does MKA Combined with Quantization Per-
form Compared to Pruning Combined with Quantiza-
tion?
We compare the performance of MKA with the baseline
pruning method, ShortGPT [14], on the MMLU dataset
using the Llama2-7B, Llama3-8B, and Mistral-7B models.
The results are shown in Table 1.
From Table 1, we can see that the pruned models are able
to be further quantized and maintain performance with a
higher compression ratio. Notably, at a high compression ra-
tio of around 87.50%, MKA significantly outperforms Short-
GPT. Additionally, we achieve excellent results with various
quantization methods. For example, on Llama3-8B, at a com-
pression ratio of 85.94%, MKA with SmoothQuant achieves
64.20%, far exceeding ShortGPT with SmoothQuant at
37.66%. Similarly, with the GPTQ quantization method, we
achieve 62.98%, surpassing ShortGPT’s 37.00%, and with
AWQ, we achieve 61.66%, exceeding ShortGPT’s 35.44%.
6.3 MKA vs. Other Pruning Methods on varies bench-
marks
We compared the performance of MKA and several other
pruning methods on the LLama3-8B model using multiple
benchmark datasets at compression ratios of 34.375%, 37.5%,
40.625% and 43.75%. The results are shown in Table 2.
From the results, merging can retain performance better
3. Note that, the compression ratio is calculated as:
Ltotal−
Lretained
Q
/Ltotal, where Ltotal is the total number of
layers before compression, Lretained is the number of retained layers,
andQis the quantization factor.TABLE 1
Performance comparison of MKA and ShortGPT pruning with
quantization (SmoothQuant, GPTQ, AWQ) on MMLU using Llama2-7B,
Llama3-8B, and Mistral-7B. MKA outperforms ShortGPT in accuracy
across all models and quantization methods at similar compression
ratios with int4. The calculation of the compression ratio only considers
the number of hidden layers in the model without considering the
embedding layer.
Model MethodRetained layers
(Compression Ratio)Acc
Llama2-7BVanilla Model 32(0.00%) 46.67
ShortGPT+Smooth 16(87.50%) 25.67
ShortGPT+GPTQ 16(87.50%) 25.82
ShortGPT+AWQ 16(87.50%) 26.01
MKA (Ours) + Smooth 16(87.50%) 35.66 (+9.99)
MKA (Ours) + GPTQ 16(87.50%) 35.91 (+10.09)
MKA (Ours) + AWQ 16(87.50%) 36.23 (+10.22)
Llama3-8BVanilla Model 32 (0.00%) 66.29
ShortGPT+Smooth 18(85.94%) 26.54
ShortGPT+GPTQ 18(85.94%) 25.98
ShortGPT+AWQ 18(85.94%) 26.22
MKA (Ours) + Smooth 18(85.94%) 64.20 (+37.66)
MKA (Ours) + GPTQ 18(85.94%) 62.98 (+37.00)
MKA (Ours) + AWQ 18(85.94%) 61.66 (+35.44)
Mistral-7BVanilla Model 32(0.00%) 63.87
ShortGPT+Smooth 20(84.38%) 24.32
ShortGPT+GPTQ 20(84.38%) 23.16
ShortGPT+AWQ 20(84.38%) 23.96
MKA (Ours) + Smooth 20(84.38%) 56.92 (+32.60)
MKA (Ours) + GPTQ 20(84.38%) 56.12 (+32.96)
MKA (Ours) + AWQ 20(84.38%) 55.34 (+31.38)
compared to pruning. Relative to ShortGPT, our method
can achieve better performance retention, with significant
improvements across all datasets. For example, at a com-
pression ratio of 34.375% on the MMLU dataset, our method
can outperform ShortGPT by 21.92%. Similarly, on the Hel-
laSwag dataset, our proposed method can surpass ShortGPT
by 18.32%.
6.4 Are Inter-Layer Knowledge Alignment Similarity
Matrices Consistent Across Different Models?
We generate layer similarity heatmaps for different mod-
els before applying MKA. These heatmaps visualize the
knowledge alignment and layer merging effects of MKA on
various models. Figure 3 presents the similarity heatmaps
for Llama2-7B, Llama2-13B, Llama-3-8B, Llama3.2-3B, and
Mistral-7B models. We observe that the heatmaps for the
later layers of each model exhibit high similarity values,
indicating that inter-layer similarity is consistently high in

--- PAGE 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 9
TABLE 2
Comparison of different methods across MMLU, PIQA, HellaSwag, RACE-H, and BoolQ datasets at different compression ratios.
Compression Ratio = 34.375% Compression Ratio = 37.5%
Method MMLU PIQA HellaSwag RACE-H BoolQ MMLU PIQA HellaSwag RACE-H BoolQ
Vanilla Model 66.29 81.12 74.54 66.07 66.79 66.29 81.12 74.54 66.07 66.79
ShortGPT 42.95 60.99 33.00 41.68 51.96 44.80 61.70 38.69 40.05 57.09
MKA (Ours) 64.87(+20.42) 67.79(+6.80) 51.32(+18.32) 55.20(+13.52) 63.36(+11.40) 62.05(+17.25) 66.26(+4.56) 50.16(+11.47) 49.49(+9.44) 63.46(+6.37)
Compression Ratio = 40.625% Compression Ratio = 43.75%
Method MMLU PIQA HellaSwag RACE-H BoolQ MMLU PIQA HellaSwag RACE-H BoolQ
Vanilla Model 66.29 81.12 74.54 66.07 66.79 66.29 81.12 74.54 66.07 66.79
ShortGPT 39.26 58.22 34.16 21.70 61.77 26.09 59.03 33.75 21.58 61.53
MKA (Ours) 63.42(+24.16) 65.61(+6.25) 48.83(+14.67) 55.26(+33.20) 63.58(+1.81) 64.42(+31.31) 65.51(+6.48) 45.10(+11.35) 45.91(+22.77) 62.14(+0.51)
LayerLayerLlama3-8B
LayerLlama3-70B
LayerMixtral-7B
LayerLlama2-7B
LayerLlama2-13B
2468
51015
24681012
123
1234
Fig. 3. Similarity matrices for Llama2-7B, Llama2-13B, Llama-3-8B, Llama3.2-3B, and Mistral-7B before MKA. Later layers show high similarity,
supporting layer merging.
the later layers across different models. This observation
supports our layer merging approach. Additionally, when
merging the earlier layers, we notice a collapse of the matrix
in the final figure, suggesting that earlier layers have a
significant influence on later layers. Thus, simple merging
operations on the earlier layers of the model are not feasible.
6.5 Iterative Nature of MKA
It’s important to note that the MKA method already incor-
porates an iterative process in its design. For example, when
we merge layers 31 and 32, we obtain a fused layer, which
is then merged with layer 30 in the next iteration. We have
compared this approach with an alternative method where
each layer is allowed to merge only once (e.g., merging
layers 31 and 32, then 30 and 29 separately). Our experi-
ments on the Llama3-8B model using the MMLU dataset
demonstrate that MKA’s iterative approach yields superior
performance in terms of minimizing accuracy degradation.
The results are presented in Table 4.
7 D ISCUSSION
7.1 Extension to Multimodal and Specialized Models
In addition to its application to large language models, the
MKA method shows promising potential for broader adop-
tion across a variety of deep learning architectures. This
includes Mixture-of-Experts (MoE) [4], and Jamba [58] mod-
els, which can exhibit similar redundancies in their process-
ing layers. The results show in Figure 4. Initial experiments
conducted on these diverse architectures have reinforced the
viability of our approach. For instance, the similarity matri-
ces generated on Mixtral-8x7B [4] and jamba [58] applying
LayerLayerMixtral-8x7B
LayerJamba
0.51.01.52.02.53.0
12345Fig. 4. The similarity matrix of Mixtral-8x7B and Jamba model.
MKA have shown that our method can also be generalized
to other similar models, but the similarity distributions of
Mixtral-8x7B and Jamba are slightly different from LLM,
and we do not yet know the reason. These experiments
further validates the effectiveness of our method across
different model types.
7.2 Analysis of Similarity Measures
In our evaluation of the Llama3-8B model, we explored
several similarity measures: Cosine Similarity, Mahalanobis
Distance, Euclidean Distance, t-SNE Similarity, and Au-
toencoder Similarity. The similarity matrices are shown in
Figure 5. From the results, we observe that Cosine Similarity,
Mahalanobis Distance, and Euclidean Distance display simi-
lar distribution patterns with vertical stripes and varied heat
values. However, Mahalanobis Distance shows irregular
heat values within these stripes, indicating a misalignment
with the fused layer data structure. t-SNE Similarity appears

--- PAGE 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 10
LayerLayerCosine Similarity
LayerMahalanobis Distance
LayerEuclidean Distance
Layert-SNE Similarity
LayerAutoencoder Similarity
0.7750.8000.8250.8500.8750.9000.9250.950
0.51.01.52.02.53.03.54.0
0.490.500.510.520.530.54
468101214
0.320.340.360.380.400.42
Fig. 5. Similarity matrices for various measures in the Llama3-8B model, showing different patterns and effectiveness in capturing layer relationships,
with none fully matching the expected merging patterns.
random and lacks consistent patterns. For Autoencoder
Similarity, the high heat values do not correspond to suitable
merging areas or expected high-similarity regions.
7.3 Further Exploration of the Merging Ratio
To further investigate the impact of λm, we conducted
experiments using fixed values of λmwithout considering
layer similarities. We test λmvalues of 0.7, 0.6, 0.5, and 0.4
(assigning higher weight to the lower-numbered layer). The
results are shown in Table 3.
These results exhibit a somewhat monotonic trend, with
performance decreasing as λmmoves away from 0.7. How-
ever, all performances remain below that of the similarity-
based method. This further highlights the importance of
adaptive merging ratios based on layer similarities, as in our
MKA method, for maintaining model performance during
compression.
7.4 Variations in Accuracy Across Different MMLU Sub-
jects During Layer Merging
3.12%
6.25%
9.38%
12.50%
15.62%
18.75%
21.88%
25.00%
28.12%
31.25%
34.38%
37.50%
40.62%
43.75%
Ratio0.30.40.50.60.70.80.9ACCAccuracy Change During Merge Layers by Subject
College Medicine
College Biology
High School Psychology
College Physics
Fig. 6. Different MMLU dataset subjects ACC change during merging.
We examine the impact of model merging on perfor-
mance across various academic subjects in the MMLU
benchmark. Figure 6 shows the accuracy changes across
subjects such as College Medicine, College Biology, High
School Psychology, and College Physics during different
stages of merging model layers. From our results, we ob-
serve that High School Psychology maintained a stable ac-
curacy with only minor fluctuations, suggesting a consistent
performance and low sensitivity to the merging process. In
contrast, College Biology experiences a significant drop in
accuracy at the 12.5% merging ratio, followed by a recovery.
College Physics exhibits frequent fluctuations in accuracy,
pointing to a high sensitivity to layer merging. Conversely,
College Medicine experiences a steady increase in perfor-
mance with only minor variations.TABLE 3
Performance comparison of different fixed merging ratios on Llama3-8B
using the MMLU dataset.
CR MKA λm= 0.7 λm= 0.6 λm= 0.5 λm= 0.4
9.38 66.15 66.06(-0.09) 66.05(-0.10) 65.98(-0.17) 65.96(-0.19)
18.75 64.96 63.47(-1.49) 63.32(-1.64) 62.92(-2.04) 62.83(-2.13)
34.38 64.87 61.84(-3.03) 61.52(-3.35) 61.45(-3.42) 61.59(-3.28)
TABLE 4
Comparison of iterative and non-iterative MKA approaches on
Llama3-8B using MMLU dataset.
CR MKA (w/o iterative) MKA (w/ iterative)
0.00 66.29 66.29
3.13 66.13 66.13
6.25 61.64 66.26
9.38 47.43 66.15
12.50 35.87 58.08
15.63 47.82 62.94
18.75 42.01 64.96
21.88 42.00 62.92
25.00 39.39 64.28
28.13 40.07 65.01
31.25 30.41 63.99
34.38 26.73 64.87
37.50 25.37 62.05
8 C ONCLUSION
In this paper, we have proposed Manifold-Based Knowl-
edge Alignment and Layer Merging Compression (MKA), a
novel model compression technique specifically designed to
efficiently reduce the size of large language models (LLMs)
while maintaining their performance. MKA leverages mani-
fold learning techniques to align knowledge across layers
and utilizes the Information Bottleneck (IB) measure to
identify the most similar layers for merging. By captur-
ing the intricate nonlinear dependencies within LLMs and
integrating knowledge from similar layers, MKA achieves
remarkable compression ratios without sacrificing model
accuracy. We have conducted extensive experiments on a
diverse set of benchmark datasets and various state-of-the-
art LLMs to rigorously evaluate the effectiveness of MKA in
preserving model performance while significantly reducing
model size. Our empirical results demonstrate that MKA
consistently outperforms existing pruning methods and can
achieve even higher compression ratios when combined

--- PAGE 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 11
with quantization techniques.
ACKNOWLEDGEMENTS
This work is supported by the National Key R&D Program
of China (Grant No. 2023YFB3307500). This work is also
supported by the National Natural Science Foundation of
China (Grant No. 62306087 and No. 62472121), the Natu-
ral Science Foundation of Shandong Province (Grant No.
ZR2023QF154), Special Funding Program of Shandong Tais-
han Scholars Project and the Open Project of Anhui Provin-
cial Key Laboratory of Multimodal Cognitive Computation,
Anhui University (Grant No. MMC202420). Besides, we
sincerely thank the anonymous reviewers for their valuable
feedback.
REFERENCES
[1] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,
R. Avila, I. Babuschkin, S. Balaji, V . Balcom, P . Baltescu, H. Bao,
M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro,
C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman,
G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Camp-
bell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan,
C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen,
M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings,
J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville,
A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti,
T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P . Fishman, J. Forte,
I. Fulford, L. Gao, E. Georges, C. Gibson, V . Goel, T. Gogineni,
G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray,
R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris,
Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey,
P . Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga,
S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto,
B. Jonn, H. Jun, T. Kaftan, Łukasz Kaiser, A. Kamali, I. Kan-
itscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim,
Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Łukasz
Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger,
V . Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M.
Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P . Lue,
A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski,
B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney,
C. McLeavey, P . McMillan, J. McNeil, D. Medina, A. Mehta,
J. Menick, L. Metz, A. Mishchenko, P . Mishkin, V . Monaco,
E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mély,
A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh,
L. Ouyang, C. O’Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantu-
liano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov,
A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P .
de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V . H. Pong,
T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford,
J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross,
B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. San-
turkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam,
K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P . Shyam, S. Sidor,
E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky,
Y. Song, N. Staudacher, F. P . Such, N. Summers, I. Sutskever,
J. Tang, N. Tezak, M. B. Thompson, P . Tillet, A. Tootoonchian,
E. Tseng, P . Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone,
A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang,
B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P . Welinder,
J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich,
H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo,
K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang,
S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph, “Gpt-4
technical report,” 2024.[2] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,
A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal,
A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev,
A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson,
A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux,
C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret,
C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius,
D. Song, D. Pintz, D. Livshits, D. Esiobu, D. Choudhary,
D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin,
E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic,
F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon,
G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron,
I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Copet,
J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah,
J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang,
J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca,
J. Johnstun, J. Saxe, J. Jia, K. V . Alwala, K. Upasani, K. Plawiak,
K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu,
K. Bhalla, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan,
L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat,
L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri,
M. Kardas, M. Oldham, M. Rita, M. Pavlova, M. Kambadur,
M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi,
N. Bashlykov, N. Bogoychev, N. Chatterji, O. Duchenne, O. Çelebi,
P . Alrassy, P . Zhang, P . Li, P . Vasic, P . Weng, P . Bhargava,
P . Dubal, P . Krishnan, P . S. Koura, P . Xu, Q. He, Q. Dong,
R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic,
R. Raileanu, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro,
R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang, S. Hosseini,
S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie,
S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang,
S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot,
S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha,
T. Georgiou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao,
U. Karn, V . Goswami, V . Gupta, V . Ramanathan, V . Kerkez,
V . Gonguet, V . Do, V . Vogeti, V . Petrovic, W. Chu, W. Xiong,
W. Fu, W. Meers, X. Martinet, X. Wang, X. E. Tan, X. Xie,
X. Jia, X. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen,
Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan,
Z. Chen, Z. Papakipos, A. Singh, A. Grattafiori, A. Jain, A. Kelsey,
A. Shajnfeld, A. Gangidi, A. Victoria, A. Goldstand, A. Menon,
A. Sharma, A. Boesenberg, A. Vaughan, A. Baevski, A. Feinstein,
A. Kallet, A. Sangani, A. Yunus, A. Lupu, A. Alvarado, A. Caples,
A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani, A. Franco,
A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman,
A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd,
B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock,
B. Wasti, B. Spence, B. Stojkovic, B. Gamido, B. Montalvo,
C. Parker, C. Burton, C. Mejia, C. Wang, C. Kim, C. Zhou,
C. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, D. Civin,
D. Beaty, D. Kreymer, D. Li, D. Wyatt, D. Adkins, D. Xu,
D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss,
D. Wang, D. Le, D. Holland, E. Dowling, E. Jamil, E. Montgomery,
E. Presani, E. Hahn, E. Wood, E. Brinkman, E. Arcaute, E. Dunbar,
E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Ozgenel, F. Caggioni,
F. Guzmán, F. Kanayet, F. Seide, G. M. Florez, G. Schwarz,
G. Badeer, G. Swee, G. Halpern, G. Thattai, G. Herman, G. Sizov,
Guangyi, Zhang, G. Lakshminarayanan, H. Shojanazeri, H. Zou,
H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren,
H. Goldman, I. Damlaj, I. Molybog, I. Tufanov, I.-E. Veliche,
I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Asher, J.-B. Gaya,
J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul,
J. Zhong, J. Jin, J. Yang, J. Cummings, J. Carvill, J. Shepard,
J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U,
K. Saxena, K. Prasad, K. Khandelwal, K. Zand, K. Matosich,
K. Veeraraghavan, K. Michelena, K. Li, K. Huang, K. Chawla,
K. Lakhotia, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell,
L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa,
M. Avalani, M. Bhatt, M. Tsimpoukelli, M. Mankus, M. Hasson,
M. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi,
M. Keneally, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel,
M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J.
Hermoso, M. Metanat, M. Rastegari, M. Bansal, N. Santhanam,
N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier,
N. P . Laptev, N. Dong, N. Zhang, N. Cheng, O. Chernoguz,
O. Hart, O. Salpekar, O. Kalinli, P . Kent, P . Parekh, P . Saab,
P . Balaji, P . Rittner, P . Bontrager, P . Roux, P . Dollar, P . Zvyagina,

--- PAGE 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 12
P . Ratanchandani, P . Yuvraj, Q. Liang, R. Alao, R. Rodriguez,
R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Li, R. Hogan,
R. Battey, R. Wang, R. Maheswari, R. Howes, R. Rinott, S. J.
Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan,
S. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay,
S. Feng, S. Lin, S. C. Zha, S. Shankar, S. Zhang, S. Zhang, S. Wang,
S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe,
S. Satterfield, S. Govindaprasad, S. Gupta, S. Cho, S. Virk,
S. Subramanian, S. Choudhury, S. Goldman, T. Remez, T. Glaser,
T. Best, T. Kohler, T. Robinson, T. Li, T. Zhang, T. Matthews,
T. Chou, T. Shaked, V . Vontimitta, V . Ajayi, V . Montanez,
V . Mohan, V . S. Kumar, V . Mangla, V . Albiero, V . Ionescu,
V . Poenaru, V . T. Mihailescu, V . Ivanov, W. Li, W. Wang, W. Jiang,
W. Bouaziz, W. Constable, X. Tang, X. Wang, X. Wu, X. Wang,
X. Xia, X. Wu, X. Gao, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li,
Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Hao, Y. Qian,
Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, and
Z. Zhao, “The llama 3 herd of models,” 2024. [Online]. Available:
https://arxiv.org/abs/2407.21783
[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Ro-
driguez, A. Joulin, E. Grave, and G. Lample, “Llama: Open and
efficient foundation language models,” 2023.
[4] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand
et al. , “Mixtral of experts,” arXiv preprint arXiv:2401.04088 , 2024.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-
wal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , “Lan-
guage models are few-shot learners,” Advances in neural informa-
tion processing systems , vol. 33, pp. 1877–1901, 2020.
[6] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
A. Roberts, P . Barham, H. W. Chung, C. Sutton, S. Gehrmann
et al. , “Palm: Scaling language modeling with pathways,” Journal
of Machine Learning Research , vol. 24, no. 240, pp. 1–113, 2023.
[7] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,
“On the dangers of stochastic parrots: Can language models be
too big?” in Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency , 2021, pp. 610–623.
[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von
Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. , “On
the opportunities and risks of foundation models,” arXiv preprint
arXiv:2108.07258 , 2021.
[9] Y. Cheng, D. Wang, P . Zhou, and T. Zhang, “A survey of model
compression and acceleration for deep neural networks,” arXiv
preprint arXiv:1710.09282 , 2017.
[10] L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, “Model compression
and hardware acceleration for neural networks: A comprehensive
survey,” Proceedings of the IEEE , vol. 108, no. 4, pp. 485–532, 2020.
[11] P . Ganesh, Y. Chen, X. Lou, M. A. Khan, Y. Yang, H. Sajjad,
P . Nakov, D. Chen, and M. Winslett, “Compressing large-scale
transformer-based models: A case study on bert,” Transactions of
the Association for Computational Linguistics , vol. 9, pp. 1061–1080,
2021.
[12] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, “A survey on model
compression for large language models,” 2023.
[13] Z. Yang, Y. Zhang, D. Sui, Y. Ju, J. Zhao, and K. Liu,
“Explanation guided knowledge distillation for pre-trained
language model compression,” ACM Trans. Asian Low-Resour.
Lang. Inf. Process. , vol. 23, no. 2, Feb. 2024. [Online]. Available:
https://doi.org/10.1145/3639364
[14] X. Men, M. Xu, Q. Zhang, B. Wang, H. Lin, Y. Lu, X. Han, and
W. Chen, “Shortgpt: Layers in large language models are more
redundant than you expect,” arXiv preprint arXiv:2403.03853 , 2024.
[15] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and
K. Keutzer, “A survey of quantization methods for efficient neural
network inference,” 2021.
[16] S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, and
Y. Wang, “Evaluating quantized large language models,” 2024.
[17] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Llm.int8():
8-bit matrix multiplication for transformers at scale,” 2022.
[18] Z. Gong, J. Liu, J. Wang, X. Cai, D. Zhao, and R. Yan, “What makes
quantization for large language models hard? an empirical study
from the lens of perturbation,” 2024.
[19] Y. LeCun, J. Denker, and S. Solla, “Optimal brain
damage,” in Advances in Neural Information Processing
Systems , D. Touretzky, Ed., vol. 2. Morgan-Kaufmann, 1989.[Online]. Available: https://proceedings.neurips.cc/paper_files/
paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf
[20] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and
huffman coding,” 2016.
[21] M. Gupta and P . Agrawal, “Compression of deep learning models
for text: A survey,” ACM Transactions on Knowledge Discovery from
Data (TKDD) , vol. 16, no. 4, pp. 1–55, 2022.
[22] X. Ma, G. Fang, and X. Wang, “Llm-pruner: On the structural
pruning of large language models,” 2023.
[23] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P . Luo,
and N. Wong, “Structured pruning for efficient generative
pre-trained language models,” in Findings of the Association
for Computational Linguistics: ACL 2023 , A. Rogers, J. Boyd-
Graber, and N. Okazaki, Eds. Toronto, Canada: Association for
Computational Linguistics, Jul. 2023, pp. 10 880–10 895. [Online].
Available: https://aclanthology.org/2023.findings-acl.692
[24] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:
Efficient finetuning of quantized llms,” 2023.
[25] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P . Graf, “Pruning
filters for efficient convnets,” 2017.
[26] X. Ma, G. Fang, and X. Wang, “Llm-pruner: On the structural
pruning of large language models,” Advances in neural information
processing systems , vol. 36, pp. 21 702–21 720, 2023.
[27] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes,
A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith
et al. , “Model soups: averaging weights of multiple fine-tuned
models improves accuracy without increasing inference time,” in
International Conference on Machine Learning . PMLR, 2022, pp.
23 965–23 998.
[28] D. Liu, Z. Wang, B. Wang, W. Chen, C. Li, Z. Tu, D. Chu, B. Li,
and D. Sui, “Checkpoint merging via bayesian optimization in llm
pretraining,” arXiv preprint arXiv:2403.19390 , 2024.
[29] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge
fusion of large language models,” 2024.
[30] W. Li, Y. Peng, M. Zhang, L. Ding, H. Hu, and L. Shen, “Deep
model fusion: A survey,” arXiv preprint arXiv:2309.15698 , 2023.
[31] J. B. Tenenbaum, V . d. Silva, and J. C. Langford, “A global geomet-
ric framework for nonlinear dimensionality reduction,” science ,
vol. 290, no. 5500, pp. 2319–2323, 2000.
[32] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck
method,” arXiv preprint physics/0004057 , 2000.
[33] T. Hastie, “The elements of statistical learning: data mining, infer-
ence, and prediction,” 2009.
[34] I. T. Jolliffe, Principal component analysis for special types of data .
Springer, 2002.
[35] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.”
Journal of machine learning research , vol. 9, no. 11, 2008.
[36] L. McInnes, J. Healy, and J. Melville, “Umap: Uniform manifold
approximation and projection for dimension reduction,” arXiv
preprint arXiv:1802.03426 , 2018.
[37] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler,
F. Warner, and S. W. Zucker, “Geometric diffusions as a tool
for harmonic analysis and structure definition of data: Diffusion
maps,” Proceedings of the national academy of sciences , vol. 102,
no. 21, pp. 7426–7431, 2005.
[38] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” arXiv preprint arXiv:1503.02531 , 2015.
[39] Z. Huang, W. Zhu, G. Cheng, L. Li, and F. Yuan, “Mindmerger:
Efficient boosting llm reasoning in non-english languages,” arXiv
preprint arXiv:2405.17386 , 2024.
[40] P . Tej Deep, R. Bhardwaj, and S. Poria, “DELLA-Merging: Re-
ducing Interference in Model Merging through Magnitude-Based
Sampling,” arXiv e-prints , p. arXiv:2406.11617, Jun. 2024.
[41] E. Yang, Z. Wang, L. Shen, S. Liu, G. Guo, X. Wang, and D. Tao,
“Adamerging: Adaptive model merging for multi-task learning,”
arXiv preprint arXiv:2310.02575 , 2023.
[42] Y. Zhou, L. Song, B. Wang, and W. Chen, “MetaGPT: Merging
Large Language Models Using Model Exclusive Task Arithmetic,”
arXiv e-prints , p. arXiv:2406.11385, Jun. 2024.
[43] G. Ilharco, M. T. Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt,
H. Hajishirzi, and A. Farhadi, “Editing models with task arith-
metic,” arXiv preprint arXiv:2212.04089 , 2022.
[44] A. Chronopoulou, J. Pfeiffer, J. Maynez, X. Wang, S. Ruder,
and P . Agrawal, “Language and task arithmetic with parameter-
efficient layers for zero-shot summarization,” arXiv preprint
arXiv:2311.09344 , 2023.

--- PAGE 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2024 13
[45] G. Ortiz-Jimenez, A. Favero, and P . Frossard, “Task arithmetic
in the tangent space: Improved editing of pre-trained models,”
Advances in Neural Information Processing Systems , vol. 36, 2024.
[46] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,
and J. Steinhardt, “Measuring massive multitask language under-
standing,” arXiv preprint arXiv:2009.03300 , 2020.
[47] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al. , “Piqa: Reasoning about
physical commonsense in natural language,” in Proceedings of the
AAAI conference on artificial intelligence , vol. 34, no. 05, 2020, pp.
7432–7439.
[48] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-
laswag: Can a machine really finish your sentence?” arXiv preprint
arXiv:1905.07830 , 2019.
[49] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “Race: Large-scale
reading comprehension dataset from examinations,” arXiv preprint
arXiv:1704.04683 , 2017.
[50] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and
K. Toutanova, “Boolq: Exploring the surprising difficulty of natu-
ral yes/no questions,” arXiv preprint arXiv:1905.10044 , 2019.
[51] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al. ,
“Mistral 7b,” arXiv preprint arXiv:2310.06825 , 2023.
[52] A. Gromov, K. Tirumala, H. Shapourian, P . Glorioso, and D. A.
Roberts, “The unreasonable ineffectiveness of the deeper layers,”
2024. [Online]. Available: https://arxiv.org/abs/2403.17887
[53] J. Song, K. Oh, T. Kim, H. Kim, Y. Kim, and J.-J. Kim,
“Sleb: Streamlining llms through redundancy verification and
elimination of transformer blocks,” 2024. [Online]. Available:
https://arxiv.org/abs/2402.09025
[54] B.-K. Kim, G. Kim, T.-H. Kim, T. Castells, S. Choi, J. Shin, and
H.-K. Song, “Shortened llama: Depth pruning for large language
models with comparison of retraining methods,” 2024. [Online].
Available: https://arxiv.org/abs/2402.02834
[55] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,
“Smoothquant: Accurate and efficient post-training quantization
for large language models,” in International Conference on Machine
Learning . PMLR, 2023, pp. 38 087–38 099.
[56] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “Gptq: Ac-
curate post-training quantization for generative pre-trained trans-
formers,” arXiv preprint arXiv:2210.17323 , 2022.
[57] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq:
Activation-aware weight quantization for llm compression and
acceleration,” arXiv preprint arXiv:2306.00978 , 2023.
[58] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos,
E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz et al. , “Jamba:
A hybrid transformer-mamba language model,” arXiv preprint
arXiv:2403.19887 , 2024.
