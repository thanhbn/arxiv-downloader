# 2405.12532.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/kv-cache/2405.12532.pdf
# Kích thước tệp: 1079526 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PyramidInfer: Nén KV Cache Hình Kim Tự Tháp
cho Suy luận LLM Thông lượng cao
Dongjie Yang1,*, Xiaodong Han2, Yan Gao2, Yao Hu2, Shilin Zhang3, Hai Zhao1,*,†
1Đại học Giao thông Thượng Hải,2Xiaohongshu Inc.,
3Đại học Công nghệ Nam Trung Quốc
1{djyang.tony@,zhaohai@cs.}sjtu.edu.cn ,
2{shuweng,yadun,xiahou}@xiaohongshu.com
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã thể hiện
khả năng hiểu đáng kể nhưng gặp
thách thức trong việc sử dụng bộ nhớ GPU trong quá
trình suy luận, cản trở khả năng mở rộng cho các
ứng dụng thời gian thực như chatbots. Để tăng tốc
suy luận, chúng tôi lưu trữ các khóa và giá trị đã tính toán
(KV cache) trong bộ nhớ GPU. Các
phương pháp hiện tại nghiên cứu nén KV cache để giảm
bộ nhớ bằng cách loại bỏ KV
cache đã tính toán trước. Tuy nhiên, chúng bỏ qua
sự phụ thuộc giữa các lớp và tiêu thụ bộ nhớ
khổng lồ trong việc tính toán trước. Để khám phá
những thiếu sót này, chúng tôi thấy rằng số lượng
khóa và giá trị quan trọng ảnh hưởng đến việc tạo ra
tương lai giảm từng lớp một và chúng ta
có thể trích xuất chúng bằng tính nhất quán trong trọng số
attention. Dựa trên những phát hiện, chúng tôi đề xuất
PyramidInfer, một phương pháp nén
KV cache bằng cách giữ lại ngữ cảnh quan trọng
theo từng lớp. PyramidInfer tiết kiệm bộ nhớ đáng kể
bằng cách tính toán ít khóa và giá trị hơn mà không
hy sinh hiệu suất. Kết quả thực nghiệm
cho thấy PyramidInfer cải thiện thông lượng 2.2x
so với Accelerate với việc giảm hơn 54% bộ nhớ GPU
trong KV cache. Mã nguồn của chúng tôi có sẵn tại https://github.com/mutonix/
pyramidinfer .
1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) (OpenAI, 2023;
Anthropic, 2023; Jiang et al., 2023) như GPT4
đã chứng minh khả năng chưa từng có về
hiểu biết đáng kể trong ngôn ngữ con người.
*Dongjie Yang và Hai Zhao thuộc Khoa
Khoa học và Kỹ thuật Máy tính, Đại học Giao
thông Thượng Hải; Phòng thí nghiệm Trọng điểm của Ủy ban
Giáo dục Thượng Hải về Tương tác Thông minh và Kỹ thuật
Nhận thức, Đại học Giao thông Thượng Hải; Phòng thí nghiệm
Trọng điểm Thượng Hải về Lưu thông Dữ liệu Đáng tin cậy và
Quản trị trong Web3.
†Tác giả liên hệ; Bài báo này được hỗ trợ một phần
bởi Dự án Nghiên cứu Chung của Cộng đồng Đổi mới Khoa học
và Công nghệ Đồng bằng sông Dương Tử (Số
2022CSJGG1400).Tuy nhiên, những mô hình lớn này gặp phải
thách thức đáng kể về việc sử dụng bộ nhớ GPU
khổng lồ trong quá trình suy luận, do độ phức tạp
của mô hình và tính toán. Điều này cản trở việc triển khai
LLMs ở quy mô lớn để đáp ứng hàng nghìn nhu cầu
trò chuyện với chatbots.
1.5B 7B 7B 
PyramidInfer13B 13B 
PyramidInfer
Model020406080100120140Memory (GB)OOM của GPU 80GB đơnModel GPU mem
KV Cache GPU mem
Hình 1: Suy luận trong giai đoạn prefill: tất cả các mô hình
có kích thước khác nhau có prompts 64 ×2k.
LLM tiêu thụ bộ nhớ GPU khổng lồ trong KV cache
so với mô hình nhỏ. PyramidInfer có thể giảm
hơn 54% sử dụng bộ nhớ GPU trong KV cache trong khi
có thông lượng hơn 2x.
Khác với huấn luyện, các mô hình trong quá
trình suy luận không cần ghi lại trạng thái optimizer,
activations, hoặc gradients. Vì LLMs chủ yếu là
các mô hình tự hồi quy dựa trên Transformer, việc
sử dụng bộ nhớ GPU chủ yếu bao gồm hai phần:
tham số mô hình và KV cache. KV cache
thể hiện các khóa và giá trị đã tính toán trước đó
trong attention. Chúng tôi lưu trữ KV cache trong
bộ nhớ GPU và tái sử dụng nó trong việc tạo ra
tương lai để tránh tính toán lại. Cơ chế KV cache
đã được sử dụng rộng rãi để cải thiện tốc độ
suy luận (Touvron et al., 2023; Zhang et al., 2022).
Tuy nhiên, KV cache tiêu thụ bộ nhớ GPU
khổng lồ, đặc biệt đối với LLMs. Ví dụ, trong
Hình 1, đối với một mô hình có 7 tỷ tham số,
các tham số chỉ tiêu thụ 14 GB bộ nhớarXiv:2405.12532v2  [cs.CL]  5 Jun 2024

--- TRANG 2 ---
nhưng KV cache cần khoảng 72 GB. KV
cache có khả năng tiêu thụ bộ nhớ gấp nhiều lần
kích thước của mô hình. Nó cho thấy một thách thức
lớn rằng thông lượng suy luận LLM bị hạn chế bởi
lượng dữ liệu (KV cache) chúng ta có thể đưa vào
GPU bên cạnh mô hình.
Chúng tôi chia suy luận LLM thành hai giai đoạn:
giai đoạn prefill và giai đoạn generation (Brown et al.,
2020; Radford et al., 2019). Trong giai đoạn prefill,
prompt được tính toán song song để tạo ra
token đầu tiên, và KV cache ban đầu được điền trước.
Trong giai đoạn generation, mô hình giải mã
token tiếp theo từng cái một và thêm các khóa và
giá trị của token mới được giải mã vào KV
cache cũ. Các nghiên cứu gần đây (Zhang et al., 2023; Liu
et al., 2023; Ge et al., 2023) nén KV
cache để giảm việc sử dụng bộ nhớ GPU. Tuy nhiên,
như thể hiện trong Hình 2, tất cả chúng chỉ giảm KV
cache đã được tính toán thay vì
giảm KV cache sẽ được tính toán. Chúng phải
prefill KV cache ban đầu trước khi có thể bắt đầu
nén, điều này bỏ qua việc tiêu thụ bộ nhớ
GPU lớn khi tính toán KV cache ban đầu,
đặc biệt đối với prompts dài hơn và mô hình lớn hơn. Nếu
mô hình không thể xử lý prompt trong giai đoạn
prefill, những phương pháp này không còn áp dụng được vì
việc nén của chúng bắt đầu trong giai đoạn generation. Trong
bài báo này, chúng tôi tập trung vào cách nén thêm
KV cache trong giai đoạn prefill bên cạnh
giai đoạn generation. Chúng tôi đưa ra những phát hiện và
sau đó đề xuất phương pháp PyramidInfer được lấy cảm hứng từ
những phát hiện này.
Trong quá trình huấn luyện, tất cả các token đầu vào dự đoán
các token tiếp theo của chúng theo cách
teacher-forcing một-đối-một (Lamb et al., 2016). Trong
quá trình suy luận, các token ngoại trừ token cuối cùng
không còn cần dự đoán các token tiếp theo nhưng
chúng vẫn ghi lại thông tin dư thừa này trong
khóa và giá trị. Chúng tôi gọi đây là giả thuyết Dư thừa
Ngữ cảnh Suy luận (ICR). Nó khuyến khích chúng tôi
nén KV cache bằng cách chỉ tính toán các khóa
và giá trị ghi lại thông tin ngữ cảnh.
Một thách thức khác nảy sinh khi KV cache ban đầu
được tái sử dụng nhiều lần để tạo ra các token
tương lai, đòi hỏi việc giữ lại cẩn thận thông tin ngữ cảnh
trong quá trình nén. Lấy cảm hứng từ
công trình (Liu et al., 2023), chúng tôi khám phá thêm những
phần nào của KV cache luôn quan trọng cho việc tạo ra
tương lai. Chúng tôi quan sát thấy rằng các truy vấn của các token
gần đây gần với token cuối cùng nhất quán hơn
trong việc attention đến cùng các khóa và giá trị ngữ cảnh,
được ký hiệu là Ngữ cảnh Quan trọng (PvC). Chúng tôi gọi hiện tượng này là Tính nhất quán Attention
Gần đây (RAC). Tính nhất quán của trọng số attention trong
các token gần đây cho thấy rằng chúng ta có thể tận dụng nó như
là một oracle để chọn KV cache quan trọng cho việc tạo ra
tương lai trước.
Dựa trên các quan sát của chúng tôi, chúng tôi đề xuất
PyramidInfer, một phương pháp hiệu quả để giảm
KV cache trong cả giai đoạn prefill và generation
bằng cách chọn PvCs theo từng lớp. Trong PyramidInfer,
PvCs được giảm dần khi các lớp trở nên
sâu hơn nơi KV cache giống như một kim tự tháp. Chúng tôi
thể hiện khả năng của PyramidInfer trên một loạt rộng
các tác vụ sử dụng OpenCompass (Contributors,
2023) trên các mô hình có loại và kích thước khác nhau.
Kết quả cho thấy PyramidInfer có thông lượng
cao hơn phương pháp full cache Accelerate
và Deepspeed lần lượt 2.2x và 1.4x, phương pháp nén KV cache
H 2O 2.4x với hơn 54%
ít bộ nhớ GPU hơn trong KV cache.
2 Công trình liên quan
Do nhu cầu ngày càng tăng để trò chuyện với
chatbots, các chiến lược hiệu quả là cần thiết để xử lý
hàng nghìn truy vấn để tối đa hóa thông lượng.
Cách cơ bản để cải thiện thông lượng
là đưa thêm dữ liệu (batch lớn hơn) vào bộ nhớ
GPU để tận dụng tính song song của GPU tốt hơn.
Tính song song Suy luận Một cách là mở rộng
bộ nhớ GPU. Chúng ta có thể mượn các kỹ thuật được sử dụng
trong huấn luyện để tăng tốc suy luận, ví dụ: pipeline
parallelism (Huang et al., 2019), KV cache offload
(Sheng et al., 2023), v.v. Những phương pháp này tận dụng
nhiều GPU hoặc thậm chí RAM để tạo ra không gian
lớn hơn cho dữ liệu đầu vào.
Giảm KV Cache Tuy nhiên, nếu chúng ta có
bộ nhớ GPU hạn chế, cách khác là giảm
KV cache. Đối với tối ưu hóa trong CUDA,
FlashAttention 2 (Dao, 2023) giảm số lần
đọc/ghi giữa GPU HBM và GPU on-
chip SRAM. PagedAttention (Kwon et al., 2023)
mượn các kỹ thuật bộ nhớ ảo để đạt được
gần như không lãng phí trong bộ nhớ KV cache.
Bên cạnh các phương pháp CUDA, chúng ta có thể tối ưu hóa
KV cache từ chính mô hình. Từ Hình 2,
StreamingLLM (Xiao et al., 2023) bảo tồn ngữ
cảnh gần đây để cho phép đầu vào không giới hạn bằng cách hy sinh
việc ghi nhớ lịch sử. Các phương pháp khác như
H2O (Zhang et al., 2023) và Scissorhands (Liu
et al., 2023) tận dụng attention để nén
KV cache. Tuy nhiên, chúng xử lý việc nén

--- TRANG 3 ---
Layer NLayer N+1Layer N+2(a) StreamingLLM
evictd từ KV cache
Loại bỏ tất cả KV cache trung gian 
do đó quên ngữ cảnh và chỉ 
nhớ các token gần đây.Layer NLayer N+1Layer N+2(b) H2O / Scissorhands
evictd từ KV cache
Nén KV cache của các lớp khác nhau 
dưới cùng một tiêu chuẩn bỏ qua 
sự phụ thuộc giữa các lớp của chúng.Layer NLayer N+1Layer N+2(c) PyramidInfer (của chúng tôi)
Chỉ tính toán các khóa và 
giá trị quan trọng và giảm dần độ dài 
của KV cache khi lớp trở nên sâu hơn. 
 KV cache được tính toán trong giai đoạn prefill KV cache được tính toán trong giai đoạn generation Loại bỏ các khóa và giá trị từ KV cache hiện tạiHình 2: So sánh giữa PyramidInfer và các phương pháp khác: (a) StreamingLLM chỉ bảo tồn các token đầu tiên và
gần đây do đó mất khả năng ghi nhớ ngữ cảnh trước đó. (b) H 2O/Scissorhands nén KV cache
mà không có sự khác biệt cho tất cả các lớp. Chúng gặp phải mất mát thông tin lớn bằng cách nén quá nhiều trong các
lớp nông. (c) Khác với các phương pháp trên chỉ có thể nén sau khi KV cache đã được tính toán,
PyramidInfer có thể nén KV cache trong giai đoạn prefill. PyramidInfer chỉ tính toán các khóa và giá trị quan trọng
để thực hiện suy luận do đó giảm bộ nhớ GPU nhiều hơn và mang lại thông lượng cao hơn.
của các lớp khác nhau như cùng một thứ và không
thể nén trong giai đoạn prefill. Phương pháp PyramidInfer
của chúng tôi xem xét sự khác biệt trong các lớp và
nhận ra việc nén trong cả giai đoạn
prefill và generation, do đó giảm KV
cache tốt hơn trong khi duy trì chất lượng generation.
3 Quan sát và Hiểu biết
Chúng tôi xác minh các giả thuyết Dư thừa Ngữ cảnh Suy luận
và Tính nhất quán Attention Gần đây, điều này
khuyến khích chúng tôi thiết kế phương pháp PyramidInfer .
3.1 Dư thừa Ngữ cảnh Suy luận
Khác với teacher-forcing trong huấn luyện, chỉ
token cuối cùng phải dự đoán token tiếp theo trong
suy luận. Chúng tôi giả định rằng tồn tại các khóa và giá trị
của ngữ cảnh ghi lại thông tin dư thừa
để dự đoán token tiếp theo trong huấn luyện nhưng không
hữu ích cho suy luận. Chúng tôi gọi đây là giả thuyết Dư thừa
Ngữ cảnh Suy luận (ICR).
3.1.1 Ngữ cảnh Quan trọng
Để xác minh giả thuyết, chúng tôi thiết kế một thí nghiệm
dựa trên LLaMA 2-13B 40 lớp để tìm hiểu xem
sự dư thừa này có tồn tại trong KV cache không. Trong
thí nghiệm này, chúng tôi chỉ bảo tồn một tỷ lệ khóa
và giá trị của các lớp nhất định trong khi các lớp khác
vẫn cố định và xem perplexity của đầu ra mô hình
sẽ thay đổi như thế nào. Tỷ lệ được chọn này
bao gồm các khóa và giá trị quan trọng với
các trọng số attention top- p, được ký hiệu là Ngữ cảnh
Quan trọng (PvC).
Như thể hiện trong Hình 3, chúng tôi cho thấy rằng, đối với hầu hết
các lớp, khi tỷ lệ giữ lại PvC giảm,
perplexity của đầu ra sẽ tăng. Tuy nhiên,
khi lớp trở nên sâu hơn (chỉ số lớn hơn), chúng tôi
thấy rằng ảnh hưởng của PvC ngắn hơn có xu hướng
nhỏ hơn. Ví dụ, sau Layer 27, perplexity
vẫn ổn định ngay cả với 80% khóa
và giá trị bị loại bỏ. Trong Hình 4, chúng tôi tính toán
độ lệch chuẩn trên các tỷ lệ giữ lại
của tất cả các lớp và quan sát chúng tuân theo phân phối
luật lũy thừa. Nó cho thấy hầu hết các khóa
và giá trị nên được giữ lại khi các lớp
nông và sự dư thừa trong KV cache
tăng mạnh khi các lớp trở nên sâu hơn. Sự gia tăng
dư thừa này hướng dẫn chúng tôi tối thiểu hóa KV
cache trong khi tối đa hóa hiệu suất.
3.1.2 Thảo luận
Mô hình thu thập thông tin như thế nào để
dự đoán token tiếp theo? Tạo ra token
tiếp theo có thể được coi là một quá trình mà token
cuối cùng thu thập thông tin từ ngữ cảnh
dựa trên trọng số attention. Trong Hình 3, chúng tôi
quan sát từ góc nhìn của token cuối cùng. Trong
lớp nông, thông tin trong ngữ cảnh được
phân phối trong hầu hết các token trong ngữ cảnh. Khi
lớp trở nên sâu hơn, chỉ có số lượng hạn chế khóa và giá trị
đóng góp vào việc dự đoán token tiếp theo.
Quá trình suy luận khác với huấn luyện
vì tất cả các token đầu vào dự đoán các token tiếp theo.

--- TRANG 4 ---
Hình 3: Đối với mỗi lớp, chúng tôi bảo tồn các khóa và giá trị với trọng số attention top- p (PvC) trong khi các lớp khác
duy trì độ dài đầy đủ. Chúng tôi tính toán perplexity trung bình trên các tỷ lệ giữ lại p khác nhau.
0 10 20 30
Layer0.000.250.500.751.001.251.501.75Perplexity (log) Std
Hình 4: Độ lệch chuẩn perplexity khi chỉ
PvCs được bảo tồn tại mỗi lớp.
Tại thời điểm này, khóa và giá trị lưu trữ hai loại
thông tin: 1) thông tin để dự đoán token
tiếp theo của nó; 2) thông tin ngữ cảnh
cho các token tương lai tận dụng. Đến nay, chúng tôi đã
xác minh rằng PvCs là các khóa và giá trị quan trọng
hữu ích cho suy luận. Mặt khác, chúng tôi
muốn xác minh non-PvC có thể đóng vai trò
quan trọng hơn trong dự đoán teacher-forcing thay vì
là ngữ cảnh. Vì non-PvCs không quan trọng trong
PyramidInfer, chúng tôi thảo luận về nó trong Phụ lục B.
3.2 Tính nhất quán Attention Gần đây
Trong việc xác minh ICR, chúng tôi sử dụng trọng số
attention để tìm PvCs. Tuy nhiên, trong một lớp
attention, có một số trọng số attention cho một
token xi vì mọi token tiếp theo xt>i sẽ attention
đến nó. Chúng ta nên chọn trọng số attention nào
làm thước đo để tìm PvCs? Theo trực giác, trọng số
tối ưu phải từ token cuối cùng xn. Tuy nhiên,PvCs được chọn bởi những trọng số này phù hợp để
dự đoán xn+1 nhưng không phải lúc nào cũng phù hợp cho các token
tương lai xt>n +1. Mục tiêu của chúng tôi là tìm xem có tồn tại
PvCs chia sẻ có thể được sử dụng như một oracle chung
để dự đoán một số token tương lai xt>n +1 bên cạnh
token cuối cùng xn+1.
3.2.1 Tính nhất quán PvC
Chúng tôi chuyển đổi mục tiêu này thành việc tìm xem có tồn tại
khóa và giá trị được attention thường xuyên bởi
các token tiếp theo không. Trước tiên, chúng tôi định nghĩa khoảng cách tương đối
về mức độ xa của token ngữ cảnh xi tương đối
với token cuối cùng xn, được gọi là Tỷ lệ
Gần đây d= (n−i)/n×100% . Chúng tôi chia chuỗi đầu vào
thành hai phần nơi chúng tôi ký hiệu các token
với 0< d < 30% là chuỗi gần đây Sr và
d≥30% là chuỗi ngữ cảnh Sc. Chúng tôi chỉ
tính toán trọng số attention của Sr đến Sc để kiểm tra
xem có token nào trong Sc luôn được attention
bởi các token trong Sr không. Đối với mỗi token trong Sr của
mỗi lớp, chúng tôi chọn các khóa và giá trị với trọng số
attention top-80% làm PvCs của chúng. Chúng tôi đặt
khóa và giá trị với trọng số attention top-80% của
token cuối cùng ( d= 0) làm baseline chọn PvC.
Sau khi thiết lập, chúng tôi muốn đo lường mức độ
chồng chéo sẽ là bao nhiêu mà PvCs của các token gần đây
nhất quán với PvC của token cuối cùng. Nếu
có chồng chéo, chúng ta có thể suy ra giao điểm
nên là PvC chia sẻ nơi nhiều token tiếp theo
luôn quan tâm. Do đó đối với mỗi
lớp l, chúng tôi tính toán tỷ lệ chồng chéo C của PvCs như

--- TRANG 5 ---
0% 4% 8% 12% 16% 20% 24% 28%
Recent Ratio061218243036Layer
0.800.850.900.951.00
(a) Tỷ lệ chồng chéo PvC riêng biệt của các token gần đây.
0% 4% 8% 12% 16% 20% 24% 28%
Recent Ratio061218243036Layer
0.800.850.900.951.00
(b) Tỷ lệ chồng chéo PvC ensemble của các token gần đây.
Hình 5: Bản đồ nhiệt tỷ lệ chồng chéo PvC.
sau:
Cl,i=|{x|x∈PvC l,i} ∩ {x|x∈PvC l,last}|
|{x|x∈PvC l,last}|.
(1)
Từ kết quả trong Hình 5a, các token gần đây
trong Sr có trung bình 86% chồng chéo với PvC
được chọn bởi token cuối cùng. Nó cho thấy tồn tại
PvCs chia sẻ luôn được quan tâm bởi các
token tiếp theo. Tuy nhiên, điều này không đủ
để trở thành oracle dự đoán các token tương lai. Ví
dụ, nếu chúng ta muốn dự đoán token xn+1
chỉ sử dụng PvC được trích xuất từ token với
d= 25% , chúng ta chỉ có khoảng 83% PvC đóng góp
vào dự đoán, điều này gặp phải mất mát thông tin ngữ cảnh
lớn.
May mắn thay, việc chọn PvC từ các token gần đây
có tính nhất quán cao và chúng ta có thể tích hợp
nhiều token để chọn những cái chia sẻ. Trong Hình
5b, chúng tôi tích hợp trọng số attention bằng cách lấy trung bình
trọng số của các token tiếp theo [d, d+ 10%] làm trọng số
ensemble của token với d. Chúng tôi chọn
khóa và giá trị với trọng số ensemble top-80%
làm PvCs. Chúng tôi quan sát thấy rằng tỷ lệ chồng chéo PvC
trung bình tăng lên một biên độ lớn lên khoảng
93%. Tỷ lệ chồng chéo hầu như không giảm với
d= 20% , điều này cho thấy chúng ta có thể tận dụng
PvCs được chọn từ các token ensemble với d= 20%
như một oracle để dự đoán xn+1 là 20%
phía trước.
3.2.2 Thảo luận
Tại sao các lớp sâu hơn có xu hướng có tỷ lệ
chồng chéo PvC thấp hơn? Nếu chúng ta kiểm tra tỷ lệ chồng chéotdọc theo trục lớp, chúng tôi thấy rằng chỉ các lớp nông
có tỷ lệ tương đối cao. Đó là vì trong
các lớp sâu hơn có dư thừa ngữ cảnh: Chỉ
một số lượng nhỏ khóa và giá trị có trọng số
cao luôn được chọn làm PvCs; Những
cái khác có trọng số thấp tương tự nên chúng không
luôn được chọn, điều này dẫn đến tỷ lệ chồng chéo
thấp hơn. Hiện tượng này phù hợp với
phân phối luật lũy thừa được quan sát trong ICR, được
thảo luận thêm sau.
Thông tin ngữ cảnh chủ yếu được lưu trữ trong
PvCs chia sẻ. Trong Hình 5b, tỷ lệ chồng chéo PvC
nhất quán từ d nhỏ đến d lớn cho thấy rằng
dù các token gần đây ở đâu, chúng chỉ tận dụng
gần như cùng số lượng khóa và giá trị trong
ngữ cảnh. Những khóa và giá trị này, còn được gọi là
PvCs chia sẻ, lưu trữ hầu hết thông tin ngữ cảnh.
4 Chọn PvC theo từng lớp
Dựa trên các quan sát, chúng tôi thiết kế Pyramid-
Infer, một phương pháp để tăng cao thông lượng suy luận
bằng cách chọn PvCs theo từng lớp để
nén KV cache cho mỗi lớp.
4.1 Phương pháp
Như thể hiện trong Hình 2, PyramidInfer không chỉ có thể
giảm KV cache trong giai đoạn generation mà
còn trong giai đoạn prefill mà không tính toán
khóa và giá trị hoàn chỉnh của prompt cho tất cả
các lớp. Theo quá trình suy luận, chúng tôi
giới thiệu PyramidInfer trong giai đoạn prefill
và giai đoạn generation riêng biệt và xem

--- TRANG 6 ---
Layer NLayer N+1
Layer NLayer N+1Conext seq Sc
Recent seq Sr
Cửa sổ chuỗi 
gần đây
Loại bỏ non-PvCs
Giai đoạn Prefill
Giai đoạn GenerationTrung bình có trọng số 
dọc theo Sr Trọng số attention
LHình 6: Tổng quan về PyramidInfer.Thuật toán 1 Một lần forward pass trong PyramidInfer
Đầu vào: KV cache KV, độ dài cửa sổ gần đây L, độ dài PvC tối thiểu
N={N0, . . . , N l, . . .}
Đầu ra: KV cache KV được cập nhật
forlớp l∈layers do
ifKV is not None then
KV= cat([ PvC past, KV ])
A ← tính toán trọng số attention của KV
Ae←weighted _avg(A[−L:,:−L],dim = −2)
iflen(KV)> N lthen
TopP _index←TopP( Ae, p=p)
PvC←Gather( KV,index = TopP _index)
KV←PvC
Giảm p bằng cách nhân với tỷ lệ suy giảm
return KV
PyramidInfer có thể tiết kiệm nhiều bộ nhớ GPU bằng cách
chọn PvCs cẩn thận.
Giai đoạn Prefill Trong giai đoạn prefill, chúng ta phải
xử lý prompt để prefill KV cache ban đầu.
Khác với quá trình suy luận thông thường
bảo tồn tất cả khóa và giá trị của prompt,
PyramidInfer chỉ bảo tồn PvCs của mỗi lớp
làm KV cache ban đầu.
Tương tự, chúng tôi chia chuỗi đầu vào thành
chuỗi gần đây Sr và chuỗi ngữ cảnh Sc. Như
thể hiện trong Thuật toán 1, dựa trên RAC, chúng tôi
đầu tiên tính toán trọng số attention ensemble bằng cách
lấy trung bình có trọng số trọng số attention của Sr.
Chúng tôi gán trọng số lớn hơn cho các token gần đây hơn để
phóng đại tác động của chúng lên việc chọn PvC. Dựa trên
trọng số attention ensemble, chúng tôi chọn theo từng lớp
khóa và giá trị với trọng số top- p làm PvC.
Theo kết luận của ICR, sự gia tăng
dư thừa tuân theo phân phối luật lũy thừa.
Chúng tôi chọn p lớn hơn để giữ lại nhiều token hơn trong Sc
để không mất ngữ nghĩa trong các lớp nông.
Sau đó chúng tôi dần dần giảm p để giảm độ dài
của PvCs trong các lớp sâu hơn. Do đó, PvCs của các lớp sâu hơn ngắn hơn và KV
cache trở thành một "kim tự tháp".
Việc chọn PvC theo từng lớp tiết kiệm nhiều bộ nhớ
GPU hơn các phương pháp khác tính toán
toàn bộ prompt trong giai đoạn prefill. Bên cạnh
giai đoạn prefill, PyramidInfer tiếp tục tăng
hiệu quả trong giai đoạn generation vì LLMs
chỉ cần tái sử dụng KV cache ban đầu nhỏ hơn.Giai đoạn Generation Vì chúng ta đã bảo tồn
PvCs ban đầu làm KV cache, những gì chúng ta nên làm
trong giai đoạn generation là cập nhật những PvCs này
theo các token gần đây mới. Như thể hiện
trong Hình 6, chúng tôi duy trì một cửa sổ gần đây trượt
để cập nhật token mới được tạo thành token gần đây
mới. Dựa trên Sr mới, chúng tôi cập nhật
PvCs của KV cache nơi thao tác
giống như giai đoạn prefill. Bằng cách kiểm soát độ dài
của PvC của mỗi lớp, chúng ta có thể dễ dàng điều chỉnh
tỷ lệ nén và thậm chí hỗ trợ đầu vào không giới hạn
như StreamingLLM bằng cách duy trì số lượng cố định
PvCs trong KV cache.
5 Đánh giá
5.1 Đánh giá Cơ bản
Chúng tôi đánh giá PyramidInfer trên các tác vụ và
mô hình khác nhau để thể hiện rằng PyramidInfer có thể
giảm đáng kể bộ nhớ GPU và tăng thông
lượng trong khi duy trì chất lượng generation.
Thiết lập Thí nghiệm Chúng tôi chọn bốn loại
kịch bản: 1) Mô hình hóa ngôn ngữ: chúng tôi đo
perplexity trên wikitext-v2 (Merity et al., 2016).
2) Benchmarks LLM: chúng tôi đánh giá trên MMLU
(Hendrycks et al., 2021) và BBH (Srivastava
et al., 2022) cho hiểu biết ngôn ngữ, GSM8K
(Cobbe et al., 2021) cho lý luận toán học,
HumanEval (Chen et al., 2021) cho coding. 3)
Hội thoại: Chúng tôi đánh giá trên MT-Bench (Zheng
et al., 2023) để xem PyramidInfer có thể xử lý
hội thoại nhiều lượt như thế nào. 4) Ngữ cảnh dài: chúng tôi

--- TRANG 7 ---
25 50 75 100
KV Cache (%)681012Perplexity
LM, LLaMA 2-7B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)46810Perplexity
LM, LLaMA 2-13B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)45678Perplexity
LM, LLaMA 2-70B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)10121416Accuracy
GSM8k, LLaMA 2-7B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)32.535.037.540.042.545.0Accuracy
MMLU, LLaMA 2-7B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)40455055Accuracy
MMLU, LLaMA 2-13B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)50556065Accuracy
MMLU, LLaMA 2-70B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)15202530Accuracy
GSM8k, LLaMA 2-13B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)2530354045Accuracy
BBH, LLaMA 2-13B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)567Accuracy
MT-Bench, LLaMA 2-13B Chat
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)0246810ROUGE-L
LEVAL SUM, Vicuna 1.5-16k 13B
PyramidInfer
Local
Full
25 50 75 100
KV Cache (%)10203040Pass@1
HumanEval, CodeLLaMA 34B
PyramidInfer
Local
FullHình 7: Kết quả benchmark so sánh giữa các mô hình với full cache, chiến lược "local", và PyramidInfer.
đánh giá trên tóm tắt văn bản dài của LEval
(An et al., 2023) để xem PyramidInfer có thể
duy trì chất lượng trong khi chấp nhận đầu vào dài hơn không.
Chúng tôi đánh giá những tác vụ này trên LLaMA 2 (Touvron
et al., 2023), LLaMA 2-Chat, Vicuna 1.5-16k
(Zheng et al., 2023) và CodeLLaMA (Rozière
et al., 2023) với các kích thước khác nhau (7B, 13B, 34B và
70B)1. Chúng tôi đặt phương pháp full KV cache làm
baseline. Bên cạnh đó, chúng tôi cũng bao gồm chiến lược "local"
làm baseline khác bảo tồn chỉ
KV cache gần đây.
Ngoài ra, chúng tôi thể hiện PyramidInfer có thể tiết kiệm bao nhiêu bộ nhớ GPU và cải thiện
thông lượng. Chúng tôi so sánh hiệu quả của Pyramid-
Infer với các phương pháp full cache khác, bao gồm
Accelerate (HuggingFace, 2021), Deepspeed2(Am-
inabadi et al., 2022). Chúng tôi cũng chọn H 2O3(Zhang
et al., 2023), một phương pháp nén KV cache, làm
baseline khác. Cần lưu ý rằng PyramidInfer
1Chúng tôi lượng tử hóa các mô hình 34B và 70B thành kiểu dữ liệu INT8
để giảm chi phí tính toán.
2https://github.com/microsoft/
DeepSpeedExamples/tree/master/inference
3https://github.com/FMInference/H2Oorthogonal với các phương pháp không nén KV như
Deepspeed để cải thiện hiệu quả thêm.
Kết quả Benchmark Trong Hình 7, chúng tôi đánh giá
LLMs với các tỷ lệ nén khác nhau. Chúng tôi cho thấy
PyramidInfer duy trì chất lượng generation
với ít bộ nhớ GPU hơn nhiều so với
baseline full cache. PyramidInfer cũng vượt trội hơn
chiến lược "local" với khoảng cách lớn trên các loại
và kích thước mô hình và tác vụ khác nhau.
Trong LEval kiểm tra khả năng ngữ cảnh dài,
chúng tôi cho thấy chiến lược "local" tương tự như
kỹ thuật được sử dụng trong StreamingLLM gây ra sự
suy giảm lớn trong việc ghi nhớ lịch sử. PyramidInfer
có thể chấp nhận đầu vào dài hơn với ít bộ nhớ GPU hơn
mà không hy sinh quá nhiều hiệu suất.
Kết quả Hiệu quả Trong Bảng 1, chúng tôi cố định độ dài đầu vào
và kích thước batch. Đối với LLaMA 2-13B,
PyramidInfer thể hiện thông lượng 2.24x so với
full cache sử dụng Accelerate với 54.6% ít bộ nhớ GPU hơn
trong KV cache. Đối với LLaMA 2-70B,
PyramidInfer vẫn có thể generate trong giai đoạn prefill
so với các phương pháp khác. Các phương pháp nén KV cache
hiện tại như H 2O thậm chí không thể

--- TRANG 8 ---
Bảng 1: Đánh giá các phương pháp suy luận sử dụng GPU A100 80GB
trên LLaMA 2-13B và 70B. Length : độ dài prefill + độ dài generation.
Bsz: kích thước batch. KV mem. : sử dụng bộ nhớ GPU (GB) của KV cache.
Thr.: thông lượng (token/s)
Model Bsz Length Method KV Mem. Thr.
13B 32 512+256Accelerate 24.2 (100%) 621 (1.0x)
Deepspeed 24.2 (100%) 934 (1.5x)
H2O 21.6 (89.2%) 584 (0.9x)
PyramidInfer 11.0 (45.4%) 1389 (2.2x)
70B 8 256+128Accelerate/
Deepspeed/H 2OOOM -
PyramidInfer 4.2 20
20 40 60 80 100
Recent Sequence Ratio (%)4.24.34.44.54.64.7Perplexity
Perplexity
Full Cache PPL
405060708090100
GPU Mem. Ratio (%)
GPU Mem. Ratio
Full Cache GPU Mem.Hình 8: Nghiên cứu ablation tỷ lệ Sr.
xử lý prompt và gặp OOM trước khi
bắt đầu nén.
Trong Bảng 2, chúng tôi sử dụng hết bộ nhớ của GPU
A100 80GB để kiểm tra thông lượng tối đa bằng cách
tối đa hóa kích thước batch. PyramidInfer cho phép
kích thước batch hơn 2x so với các phương pháp khác và có thông lượng
cao hơn các phương pháp full cache Accelerate
và Deepspeed lần lượt 2.8x và 1.7x, phương pháp nén KV cache
H 2O 2.1x. PyramidInfer
cũng có thể được sử dụng để tăng cường Deepspeed bằng cách
tăng thông lượng 1.9x.
Bảng 2: Chúng tôi sử dụng hết bộ nhớ của GPU A100 80GB
để tìm ra thông lượng tối đa của những phương pháp này
trên LLaMA 2-13B. Chúng tôi đặt độ dài đầu vào là 512+256.
Lat.: độ trễ để tạo ra một token (ms/token).
Method Max Bsz Lat. Thr.
Accelerate 42 1.72 (100%) 581 (1.0x)
Deepspeed 40 1.03 (59.8%) 972 (1.6x)
H2O 48 1.39 (80.8%) 769 (1.3x)
PyramidInfer 88 0.59 (34.3%) 1678 (2.8x)
PyramidInfer
+Deepspeed86 0.53 (30.8%) 1887 (3.2x)
5.2 Nghiên cứu Ablation
Chúng tôi tiến hành các nghiên cứu ablation sử dụng mô hình
LLaMA 2-13B để khám phá PyramidInfer bằng cách
trả lời các câu hỏi sau: 1) Chúng ta nên chọn cách nào
để giảm dần độ dài PvC khi lớp trở nên
sâu hơn mà không hy sinh quá nhiều hiệu suất? 2) Tỷ lệ nào
của đầu vào chúng ta nên phân chia làm
chuỗi gần đây Sr?
Suy giảm Độ dài PvC Dựa trên ICR, chúng tôi dần dần
giảm độ dài PvCs cho mỗi lớp khi lớp
trở nên sâu hơn để tối đa hóa hiệu quả. Tuy nhiên,Bảng 3: Nghiên cứu ablation suy giảm độ dài PvC.
Strategy PPL GSM8K MMLU
Reduce more 4.93 26.82 53.1
Reduce uniformly 4.55 28.32 54.8
Reduce less (PyramidInfer) 4.20 29.56 55.7
Reduce None (Full cache) 4.42 28.58 55.4
việc giảm quá mức độ dài PvC trong các lớp nông
có thể dẫn đến mất mát thông tin ngữ cảnh. Chúng tôi
cố gắng tìm ra cách nào là tốt nhất để giảm
độ dài PvC. Dưới cùng tỷ lệ nén 60%, chúng tôi
so sánh ba mẫu: 1) giảm nhiều độ dài PvC hơn
trong các lớp nông nhưng ít hơn trong các lớp sâu hơn (giảm 15% cache trong 50% lớp đầu tiên).
2) giảm đồng đều độ dài PvC (giảm 10%
cache trong 50% lớp đầu tiên); 3) tuân theo mẫu
luật lũy thừa dựa trên ICR để giảm ít hơn ban đầu
(giảm 7% cache trong 50% lớp đầu tiên).
Kết quả trong Bảng 3 chứng minh rằng tuân theo
mẫu luật lũy thừa là cách tốt nhất để giảm
độ dài PvC và thậm chí cải thiện hiệu suất nhẹ
trên các tác vụ downstream.
Tỷ lệ Chuỗi Gần đây Trong PyramidInfer, chúng tôi
chọn các token gần đây của đầu vào làm chuỗi gần đây
Sr. Sr không chỉ được tận dụng như
ngữ cảnh mà còn là tiêu chí để chọn PvC từ
chuỗi ngữ cảnh Sc. Nếu tỷ lệ Sr tăng,
Sc sẽ ngắn hơn do đó ít token hơn trong Sc sẽ được
nén. Do đó, chúng ta cần tìm sự cân bằng
để quyết định tỷ lệ Sr nên lớn như thế nào.
Trong Hình 8, chúng tôi đặt việc sử dụng bộ nhớ GPU
của KV cache của phương pháp full cache làm baseline 100%
và kiểm tra perplexity sẽ thay đổi như thế nào
với các tỷ lệ Sr khác nhau. Khi tỷ lệ Sr tăng,
chúng tôi quan sát sự suy giảm trong việc sử dụng bộ nhớ GPU
nhưng một điểm thấp nhất trong perplexity ở tỷ lệ Sr 40-60%.

--- TRANG 9 ---
Do đó chúng ta có thể chọn 40% như sự cân bằng giữa
hiệu suất và sử dụng bộ nhớ GPU.
6 Kết luận
Chúng tôi giảm bớt khó khăn trong việc triển khai LLMs ở
quy mô lớn bằng cách giới thiệu PyramidInfer, một phương pháp mới
nén hiệu quả KV cache trong
cả giai đoạn prefill và generation. Lấy cảm hứng từ ICR
và RAC, PyramidInfer giảm đáng kể việc sử dụng bộ nhớ GPU
mà không làm giảm hiệu suất mô hình. Kết quả thực nghiệm cho thấy Pyramid-
Infer là một giải pháp đầy hứa hẹn để tối ưu hóa triển khai LLM
trong các môi trường hạn chế tài nguyên.
Hạn chế
Mặc dù có chiến lược hiệu quả để giảm các khóa và
giá trị cần tính toán bằng cách chọn PvCs, Pyra-
midInfer phải mang lại tính toán bổ sung do đó
nó có tăng tốc hạn chế với kích thước batch nhỏ,
như thảo luận trong Phụ lục A.1.
Bên cạnh đó, chúng tôi là những người tiên phong trong việc nén
KV cache trong giai đoạn prefill, đây là một lĩnh vực
chưa được khám phá đầy đủ. PyramidInfer không phải là phương pháp
nén KV cache không mất mát trong giai đoạn
prefill và các phương pháp hiệu quả hơn có thể được khám phá
trong các công trình tương lai.
Tài liệu tham khảo
Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia
Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton
Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase,
and Yuxiong He. 2022. Deepspeed inference:
Enabling efficient inference of transformer models at
unprecedented scale.
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li,
Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.
L-eval: Instituting standardized evaluation for long
context language models.
Anthropic. 2023. Introducing claude. https://www.
anthropic.com/index/introducing-claude .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser,
Mohammad Bavarian, Clemens Winter, Philippe
Tillet, Felipe Petroski Such, Dave Cummings,
Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,
Ariel Herbert-V oss, William Hebgen Guss, Alex
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
Babuschkin, Suchir Balaji, Shantanu Jain, William
Saunders, Christopher Hesse, Andrew N. Carr, Jan
Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob
McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
large language models trained on code.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word
problems.
OpenCompass Contributors. 2023. Opencompass:
A universal evaluation platform for foundation
models. https://github.com/open-compass/
opencompass .
Tri Dao. 2023. FlashAttention-2: Faster attention with
better parallelism and work partitioning.
Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang,
Jiawei Han, and Jianfeng Gao. 2023. Model tells you
what to discard: Adaptive kv cache compression for
llms. arXiv preprint arXiv:2310.01801 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. 2021. Measuring massive multitask
language understanding.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan
Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee,
Jiquan Ngiam, Quoc V . Le, Yonghui Wu, and Zhifeng
Chen. 2019. Gpipe: Efficient training of giant neural
networks using pipeline parallelism.
HuggingFace. 2021. Hugging face accelerate. https:
//huggingface.co/docs/accelerate/index .
Albert Q. Jiang, Alexandre Sablayrolles, Arthur
Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna
Lengyel, Guillaume Lample, Lucile Saulnier,
Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre
Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2023.
Mistral 7b.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.

--- TRANG 10 ---
Gonzalez, Hao Zhang, and Ion Stoica. 2023.
Efficient memory management for large language
model serving with pagedattention.
Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL,
Ying Zhang, Saizheng Zhang, Aaron C Courville,
and Yoshua Bengio. 2016. Professor forcing: A new
algorithm for training recurrent networks. Advances
in neural information processing systems , 29.
Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao
Wang, Victor Xie, Zhaozhuo Xu, Anastasios
Kyrillidis, and Anshumali Shrivastava. 2023. Scis-
sorhands: Exploiting the persistence of importance
hypothesis for llm kv cache compression at test time.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 .
OpenAI. 2023. Gpt-4 technical report.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,
Wenhan Xiong, Alexandre Défossez, Jade Copet,
Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
Usunier, Thomas Scialom, and Gabriel Synnaeve.
2023. Code llama: Open foundation models for code.
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan
Li, Max Ryabinin, Daniel Y . Fu, Zhiqiang Xie,
Beidi Chen, Clark Barrett, Joseph E. Gonzalez,
Percy Liang, Christopher Ré, Ion Stoica, and
Ce Zhang. 2023. Flexgen: High-throughput
generative inference of large language models with a
single gpu.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian
Fuller, Cynthia Gao, Vedanuj Goswami, Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,
Madian Khabsa, Isabel Kloumann, Artem Korenev,
Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu,
Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan,
Binh Tang, Ross Taylor, Adina Williams, Jian Xiang
Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan
Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
Edunov, and Thomas Scialom. 2023. Llama 2: Open
foundation and fine-tuned chat models.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023. Efficient streaming
language models with attention sinks.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068 .
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong
Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher Ré, Clark Barrett,
Zhangyang Wang, and Beidi Chen. 2023. H 2o:
Heavy-hitter oracle for efficient generative inference
of large language models.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.

--- TRANG 11 ---
A Thí nghiệm và Chi tiết Mở rộng
A.1 Chi phí Tính toán Bổ sung trong
PyramidInfer
0 20 40 60
Batch Size05001000150020002500Throughput (token/s)
PyramidInfer
Full Cache
Hình 9: So sánh giữa PyramidInfer và
baseline full cache với các kích thước batch khác nhau trên mô hình LLaMA
2-7B với độ dài đầu vào 512+256.
Trong Phần 4, chúng tôi giới thiệu cách PyramidInfer
cải thiện thông lượng suy luận bằng cách chọn
PvCs dựa trên attention của Sr. Tuy nhiên,
quá trình chọn PvC đưa vào tính toán bổ sung
trong mỗi lớp. Như thể hiện trong Thuật toán
1, chi phí bổ sung chủ yếu gây ra bởi thao tác
sắp xếp trong top- p trong khi những cái khác có thể được bỏ qua.
Để đánh giá ảnh hưởng của chi phí bổ sung,
chúng tôi dần dần tăng kích thước batch của các mô hình
và so sánh thông lượng giữa PyramidInfer
và baseline full cache. Như thể hiện trong Hình 9,
PyramidInfer có tăng tốc hạn chế với kích thước batch
nhỏ vì tính toán bổ sung
bù trừ việc tăng tốc từ KV cache giảm.
Khi kích thước batch tăng, chi phí này trở nên
không đáng kể so với việc tăng tốc mang lại bởi
PyramidInfer.
A.2 Mã hóa Vị trí
Vì chúng ta giảm số lượng khóa và giá trị của
mỗi lớp, một số vị trí của khóa và giá trị
bị thiếu. Có hai lựa chọn để có được
mã hóa vị trí mới: 1) mã hóa lại các vị trí
từ vị trí 0 theo thứ tự; 2) thu thập các mã hóa vị trí
ban đầu rải rác của khóa và giá trị.
Như thể hiện trong Bảng 4, chúng tôi thí nghiệm trên hai
lựa chọn này trên LLaMA 2-13B và thấy rằng
lựa chọn sau có hiệu suất tốt hơn một chút trong các
tác vụ downstream.Bảng 4: So sánh mã hóa vị trí.
Strategy GSM8K MMLU
Re-encode 29.12 55.5
Gather 29.56 55.7
B Thảo luận Mở rộng
Mối liên hệ giữa ICR và RAC Trong
Phần 3.2.2, chúng tôi đề cập đến hiện tượng rằng
các lớp sâu hơn có tỷ lệ chồng chéo PvC thấp hơn phù hợp với
phân phối luật lũy thừa được quan sát trong Hình 4. Đó là vì, khi chúng ta quan sát theo
chỉ số lớp của heatmap, chúng ta thấy rằng
màu sắc nhanh chóng đậm lên với khoảng cách lớn nơi sự thay đổi
độ sâu xấp xỉ với phân phối luật lũy thừa.
Hiểu biết đằng sau hai phân phối luật lũy thừa này
là giống nhau. Sự dư thừa cao trong các lớp sâu hơn
cho thấy rằng hầu hết các khóa và giá trị
vô dụng cho suy luận. Những non-PvCs này đều có
trọng số attention thấp tương tự, dẫn đến ảnh hưởng
hạn chế lên perplexity và ít cơ hội
được chọn làm PvCs.
Xác minh Thêm về ICR về Vai trò của
Non-PvCs Để hoàn thành việc xác minh ICR,
chúng ta phải xác minh non-PvCs dư thừa
vì chúng mang thông tin dự đoán
các token tiếp theo của chúng thay vì thông tin ngữ cảnh.
Trong Hình 10, để minh họa tốt hơn, chúng tôi
chia khóa và giá trị của một lớp thành hai
phần chính, PvCs và non-PvCs. Đối với PvCs,
chúng tôi chia thêm chúng thành PvCs chia sẻ và non-
shared PvCs.
PvCs chia sẻ (chồng chéo)Non-shared 
PvCsNon-PvCsKhóa và giá trị của một lớp
Hình 10: Thành phần của khóa và giá trị của
một lớp.
Trong Hình 5a, chúng tôi chứng minh rằng có
87% chồng chéo giữa các token và token cuối cùng
về mặt PvC, được ký hiệu là shared PvC. Chúng tôi
đầu tiên xác định vai trò của 13% khóa
và giá trị còn lại nơi những non-shared PvCs này không được
sử dụng trong PyramidInfer. Non-shared PvCs cũng
được gán trọng số attention cao bởi token hiện tại,
có nghĩa là chúng hữu ích để dự đoán
token tiếp theo của token hiện tại. Thật thú vị
khi xem những non-shared PvCs này là gì từ
góc nhìn của các token tiếp theo: Liệu chúng
cũng coi những khóa và giá trị này quan trọng không?
Chúng tôi sử dụng tỷ lệ chuỗi gần đây 20% để chọn
shared PvCs. Chúng tôi trích xuất non-shared PvCs từ
các token với 10% < d < 20%. Chúng tôi muốn
tìm những non-shared PvCs này thuộc về phần nào
của khóa và giá trị của các token tiếp theo với
d <10%.
Từ Hình 11, chúng ta có thể rút ra kết luận cho
ba phần này của KV cache:
1.Shared PvCs là khóa và giá trị mà
các token tiếp theo cùng nhau chú ý
đến.
2.Non-shared PvCs hiếm khi xuất hiện trong non-
shared PvCs của các token khác. Nó có nghĩa là
non-shared PvCs chủ yếu được quan tâm cao
bởi token hiện tại, với ít attention
từ các token tiếp theo. Chúng chủ yếu được sử dụng để dự đoán token tiếp theo của
chính nó theo cách teacher-forcing, đặc biệt
hữu ích trong huấn luyện.
3.Trong số non-PvCs, một phần đáng kể được
chiếm bởi non-shared PvCs của các token khác.
Đến nay, chúng tôi đã hoàn toàn xác minh giả thuyết Dư thừa
Ngữ cảnh Suy luận rằng các token ngoại trừ token cuối cùng
không còn cần dự đoán các token tiếp theo nhưng chúng vẫn ghi lại thông tin dư thừa này để dự đoán các token tiếp theo trong khóa và
giá trị.

--- TRANG 12 ---
0.20.40.60.81.01.21.41.61.82.02.22.42.62.83.03.23.43.63.84.04.24.44.64.85.05.25.45.65.86.06.26.46.66.87.07.27.47.67.88.08.28.48.68.89.09.29.49.69.8
Recent Sequence Ratio (%)100
50
050Ratio (%)Layer 2
non_shared_pvc
non_pvc
0.20.40.60.81.01.21.41.61.82.02.22.42.62.83.03.23.43.63.84.04.24.44.64.85.05.25.45.65.86.06.26.46.66.87.07.27.47.67.88.08.28.48.68.89.09.29.49.69.8
Recent Sequence Ratio (%)50
050Ratio (%)Layer 10
non_shared_pvc
non_pvc
0.20.40.60.81.01.21.41.61.82.02.22.42.62.83.03.23.43.63.84.04.24.44.64.85.05.25.45.65.86.06.26.46.66.87.07.27.47.67.88.08.28.48.68.89.09.29.49.69.8
Recent Sequence Ratio (%)50
050Ratio (%)Layer 18
non_shared_pvc
non_pvc
0.20.40.60.81.01.21.41.61.82.02.22.42.62.83.03.23.43.63.84.04.24.44.64.85.05.25.45.65.86.06.26.46.66.87.07.27.47.67.88.08.28.48.68.89.09.29.49.69.8
Recent Sequence Ratio (%)75
50
25
02550Ratio (%)Layer 26
non_shared_pvc
non_pvc
0.20.40.60.81.01.21.41.61.82.02.22.42.62.83.03.23.43.63.84.04.24.44.64.85.05.25.45.65.86.06.26.46.66.87.07.27.47.67.88.08.28.48.68.89.09.29.49.69.8
Recent Sequence Ratio (%)75
50
25
02550Ratio (%)Layer 34
non_shared_pvc
non_pvcHình 11: Tỷ lệ chồng chéo giữa non-shared PvCs và non-shared PvCs của các token khác (xanh dương) và tỷ lệ chồng chéo
giữa non-shared PvCs và non-PvCs của các token khác (cam).
