# 2406.02376.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/kv-cache/2406.02376.pdf
# Kích thước tệp: 598901 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giữ lại Thông tin Quan trọng dưới Tỷ lệ Nén Cao:
Bộ Nén Hướng dẫn Truy vấn cho LLMs
Zhiwei Cao1,3∗, Qian Cao2∗, Yu Lu2, Ningxin Peng2, Luyang Huang2
Shanbo Cheng2†và Jinsong Su1,3†
1Trường Tin học, Đại học Xiamen2ByteDance Research
3Phòng thí nghiệm Trí tuệ Nhân tạo Thượng Hải
lines1@stu.xmu.edu.cn {caoqian.95, luyu.ly, chengshanbo}@bytedance.com jssu@xmu.edu.cn
Tóm tắt
Sự phổ biến ngày càng tăng của các Mô hình Ngôn ngữ Lớn đã khơi dậy sự quan tâm đến nén ngữ cảnh cho các Mô hình Ngôn ngữ Lớn (LLMs). Tuy nhiên, hiệu suất của các phương pháp trước đây giảm sút đáng kể khi tỷ lệ nén tăng, đôi khi thậm chí giảm xuống mức sách đóng. Sự suy giảm này có thể được quy cho việc mất thông tin quan trọng trong quá trình nén. Nghiên cứu sơ bộ của chúng tôi hỗ trợ giả thuyết này, nhấn mạnh tầm quan trọng của việc giữ lại thông tin quan trọng để duy trì hiệu suất mô hình dưới tỷ lệ nén cao. Kết quả là, chúng tôi giới thiệu Bộ Nén Hướng dẫn Truy vấn (QGC), sử dụng các truy vấn để hướng dẫn quá trình nén ngữ cảnh, bảo toàn hiệu quả thông tin quan trọng trong ngữ cảnh đã nén. Ngoài ra, chúng tôi sử dụng chiến lược nén động. Chúng tôi xác nhận hiệu quả của QGC được đề xuất trên nhiệm vụ Trả lời Câu hỏi, bao gồm các bộ dữ liệu NaturalQuestions, TriviaQA, và HotpotQA. Kết quả thực nghiệm cho thấy QGC có thể hoạt động tốt một cách nhất quán ngay cả ở tỷ lệ nén cao, cũng mang lại lợi ích đáng kể về chi phí suy luận và thông lượng1.

1 Giới thiệu
Sự xuất hiện của chatGPT (Ouyang et al., 2022) và GPT4 (OpenAI, 2023), cùng với các Mô hình Ngôn ngữ Lớn khác (LLMs) (Touvron et al., 2023a,b) đã gây ra hiện tượng toàn cầu. Thành công của LLMs gắn liền chặt chẽ với khả năng ngữ cảnh dài của LLMs (Dong et al., 2022; Lewis et al., 2020), đặc biệt trong lĩnh vực trả lời câu hỏi đa tài liệu. Tuy nhiên, việc sử dụng ngữ cảnh dài cũng đưa ra những thách thức như chi phí suy luận cao hơn, độ trễ dài hơn, và hiệu suất kém hơn gây ra bởi thông tin dư thừa (Jiang et al., 2023).

Nhiều nỗ lực đã được thực hiện để nén ngữ cảnh dài bằng cách trực tiếp loại bỏ một tỷ lệ nhất định các từ ít quan trọng, như LongLLMLingua (Jiang et al., 2023) và Selective-Context (Li et al., 2023). Một phương pháp phổ biến khác là tạo ra một bản tóm tắt văn bản của ngữ cảnh đã cho (Xu et al., 2023; Wang et al., 2023b). Khác với việc xóa hoặc sắp xếp lại từ trong ngữ cảnh, AutoCompressor (Chevalier et al., 2023) nén các tài liệu dài thành nhiều vector như lời nhắc mềm, được tối ưu hóa với đầy đủ tham số của LLMs. Tuy nhiên, nghiên cứu sơ bộ của chúng tôi cho thấy các phương pháp này có một khuyết điểm chung: khi tỷ lệ nén tăng, ngữ cảnh đã nén không thể giữ lại thông tin quan trọng, dẫn đến sự giảm sút đáng kể trong hiệu suất của LLMs.

Chìa khóa để giải quyết vấn đề này là truy vấn, xác định thông tin quan trọng là gì. Chúng tôi nhằm bảo toàn thông tin quan trọng liên quan đến truy vấn này ngay cả ở tỷ lệ nén cao. Cụ thể, chúng tôi đề xuất Bộ Nén Hướng dẫn Truy vấn (QGC) để tận dụng đầy đủ thông tin truy vấn trong suốt mỗi bước nén. Chúng tôi đầu tiên đưa truy vấn và tài liệu cùng nhau vào bộ mã hóa ngữ cảnh để học các biểu diễn tài liệu hướng dẫn truy vấn. Sau đó chúng tôi nén các biểu diễn tài liệu này thành các biểu diễn n-gram được hướng dẫn bởi tầm quan trọng của mỗi từ liên quan đến truy vấn. Tiếp theo, chúng tôi đề xuất tăng cường các biểu diễn n-gram bằng cách xem xét lại truy vấn và tài liệu, cuối cùng được căn chỉnh với không gian nhúng của LLMs. Chúng tôi tiếp tục đề xuất điều chỉnh động tỷ lệ nén của mỗi tài liệu dựa trên mức độ liên quan của nó với truy vấn. So với các phương pháp trước đây, QGC có một số ưu điểm: 1) tỷ lệ nén cao bằng cách giữ lại hầu hết thông tin liên quan đến truy vấn trong quá trình nén, 2) chi phí huấn luyện thấp bằng cách chỉ tối ưu hóa bộ nén thay vì tinh chỉnh toàn bộ LLM, và 3) tính nhất quán ngữ nghĩa tốt hơn bằng cách nén cấu trúc n-gram thay vì xóa từ.

Chúng tôi xác nhận hiệu quả của QGC trên nhiệm vụ Trả lời Câu hỏi đa tài liệu, bao gồm ba bộ dữ liệu: NaturalQuestions, TriviaQA, và HotpotQA. Kết quả thực nghiệm trên nhiệm vụ QA cho thấy, so với LongLLMLingua, QGC thể hiện tỷ lệ nén cao hơn 2.75 lần và thông lượng cao hơn 2.42 lần. Ngoài ra, độ chính xác của nó đã cải thiện trung bình 5 điểm. Chúng tôi tiếp tục điều tra việc mất thông tin quan trọng trong suốt quá trình nén. Các phát hiện cho thấy dưới tỷ lệ nén cao và điều kiện nhiễu cao, QGC chỉ phát sinh mất mát hiệu suất khoảng 10%, trong khi LongLLMLingua chịu mất mát khoảng 47%. Điều này xác nhận hiệu quả của QGC trong việc giữ lại thông tin quan trọng.

2 Nghiên cứu Sơ bộ
Trong phần này, chúng tôi đầu tiên hình thức hóa ngắn gọn việc nén ngữ cảnh dài trên nhiệm vụ Trả lời Câu hỏi, và sau đó trình bày phân tích về mất mát thông tin quan trọng trong các phương pháp nén trước đây.

2.1 Hình thức hóa Nhiệm vụ
Cho một đầu vào LLM với ngữ cảnh tăng cường x= (xins,xd1, ...,xdk, ...,xdK,xq), bao gồm hướng dẫn xins, K tài liệu {xdk}K k=1, và truy vấn xq, mục tiêu của nén ngữ cảnh có thể được hình thức hóa như:

min
exd(LLM (y|x),LLM (ey|ex)), (1)

trong đó y là câu trả lời đúng và ey đại diện cho đầu ra của LLM với ngữ cảnh đã nén ex làm đầu vào. d(·,·) là một hàm đo khoảng cách giữa hai phân phối, như phân kỳ KL. Trong công trình này, chúng tôi tập trung vào việc nén K tài liệu được truy xuất quyết định phần lớn độ dài của đầu vào.

2.2 Mất mát Thông tin Quan trọng trong Nén
Chúng tôi nghiên cứu hiệu quả của hai phương pháp đại diện, LongLLMLingua (Jiang et al., 2023) và AutoCompressor (Chevalier et al., 2023). Chúng tôi tiến hành thực nghiệm trên bộ dữ liệu NaturalQuestions (Liu et al., 2023) và sử dụng độ chính xác làm chỉ số đánh giá, đánh giá liệu có câu trả lời đúng nào xuất hiện trong dự đoán LLM không.

*These authors contributed equally. This work was done when Zhiwei Cao was interning at ByteDance.
†Corresponding author.
1Our code is available at https://github.com/ DeepLearnXMU/QGC .mance caused by redundant information (Jiang et al., 2023).

--- TRANG 2 ---
Đối với LongLLMLingua, chúng tôi áp dụng LLaMA-2-7B-Chat2 như mô hình ngôn ngữ nhỏ để nén, và sử dụng LongChat-13B-16K3 như LLM mục tiêu. Chúng tôi sử dụng AutoCompressor4 mã nguồn mở, tinh chỉnh LLaMA-2-7B để nén ngữ cảnh và tạo câu trả lời. Ở đây, chúng tôi xem xét bốn cài đặt:

•Sách đóng. Nó lấy truy vấn làm đầu vào LLM không có tài liệu bổ sung.

•Oracle. Truy vấn và chỉ tài liệu chứa sự thật cơ bản được sử dụng làm đầu vào cho LLM.

•Cơ bản. Dựa trên Oracle, chúng tôi nén tài liệu trực tiếp với các tỷ lệ nén khác nhau cho LongLLMLingua. Tuy nhiên, vì AutoCompressor được thiết lập để nén tài liệu thành các vector có độ dài cố định, chúng tôi thay đổi tỷ lệ nén bằng cách thêm tài liệu bên ngoài.

2https://ai.meta.com/llama/
3https://huggingface.co/lmsys/longchat-13b-16k
4https://github.com/princeton-nlp/AutoCompressors

--- TRANG 3 ---
•Cơ bản w/ answer. Chúng tôi thêm thủ công thông tin quan trọng vào kết quả đã nén bằng cách nối câu trả lời với chuỗi từ đã nén trong LongLLMLingua. Lưu ý rằng cài đặt này không thực tế đối với AutoCompressor nơi kết quả đã nén là các vector không thể thay đổi trực tiếp.

Từ Hình 1, chúng tôi thấy rằng hiệu suất của cả hai phương pháp đều giảm sút đáng kể với tỷ lệ nén tăng. Như được hiển thị trong Hình 1(a), hiệu suất của LongLLMLingua giảm 47% khi tỷ lệ nén tăng từ 1.53x lên 3.44x. Thậm chí tệ hơn, độ chính xác của LongLLMLingua ở tỷ lệ nén 3.44x tương đương với cài đặt sách đóng. Những phát hiện tương tự được minh họa trong Hình 1(b) cho AutoCompressor.

Quan trọng hơn, chúng tôi quan sát thấy việc thêm thông tin quan trọng vào kết quả đã nén có thể giảm thiểu đáng kể sự suy giảm hiệu suất thường xảy ra ở tỷ lệ nén cao. Quay lại Hình 1(a), đường độ chính xác dao động ít khi tỷ lệ nén tăng từ 1.5x lên 3.5x với sự trợ giúp của thông tin quan trọng bổ sung, đây là mức giảm 3.87% so với 47% trước đó với việc mất thông tin quan trọng. Những quan sát này xác nhận nhu cầu bảo toàn thông tin quan trọng trong quá trình nén, điều này thúc đẩy chúng tôi khám phá một phương pháp tốt hơn để tận dụng đầy đủ thông tin truy vấn cho nén ngữ cảnh.

1.50 1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50
Tỷ lệ Nén4050607080Độ chính xác (%)
Sách đóng
Oracle
LongLLMLingua
LongLLMLingua
w/ answer(a) Tỷ lệ Nén cho LongLLMLingua

1 doc 2 docs 3 docs 4 docs 5 docs203040506070Độ chính xác (%)Sách đóng
Oracle
AutoCompressor
(b) Số Tài liệu cho AutoCompressor

Hình 1: Độ chính xác của LongLLMLingua (Jiang et al., 2023) và AutoCompressor (Chevalier et al., 2023) với các tỷ lệ nén khác nhau và số lượng tài liệu trên bộ kiểm tra NaturalQuestions, tương ứng. Sách đóng biểu thị cung cấp LLMs chỉ với câu hỏi, và Oracle có nghĩa là sử dụng câu hỏi và tài liệu sự thật cơ bản tương ứng làm đầu vào của LLM. "w/ answer" có nghĩa là thêm câu trả lời vàng vào ngữ cảnh đã nén.

3 Nén Hướng dẫn Truy vấn
Như được hiển thị trong Hình 2, chúng tôi trang bị LLM với Bộ Nén Hướng dẫn Truy vấn để nén các tài liệu dài thành chuỗi biểu diễn liên tục ngắn hơn nhiều, sau đó được nối với hướng dẫn và truy vấn tương ứng làm đầu vào cho LLM. Trong phần sau, chúng tôi đầu tiên giới thiệu kiến trúc của Bộ Nén Hướng dẫn Truy vấn và sau đó mục tiêu huấn luyện của nó. Sau đó, chúng tôi đề xuất chiến lược nén động gán tỷ lệ nén cao hơn cho các tài liệu không liên quan để cải thiện thêm các biểu diễn đã nén.

3.1 Kiến trúc Bộ Nén
Hình 3 minh họa kiến trúc cơ bản của Bộ Nén Hướng dẫn Truy vấn của chúng tôi. Sử dụng bộ nén, chúng tôi thực hiện các bước sau để tạo ra các biểu diễn đã nén của mỗi tài liệu: 1) học các biểu diễn tài liệu nhận thức truy vấn; 2) nén các biểu diễn tài liệu thành các biểu diễn n-gram bằng gộp có trọng số; 3) tăng cường các biểu diễn n-gram bằng cách xem xét lại truy vấn và toàn bộ tài liệu; 4) căn chỉnh các biểu diễn thu được vào không gian nhúng của LLM. Đặc biệt, bốn bước này tương ứng chính xác với bốn thành phần chính của bộ nén của chúng tôi, tất cả đều được đóng khung trong Hình 3. Lưu ý rằng chúng tôi thực hiện các thao tác trên trên mỗi tài liệu, do đó bỏ qua chỉ số k của tài liệu để đơn giản.

QGCHướng dẫn: Viết một câu trả lời chất lượng cao cho câu hỏi đã cho chỉ sử dụng các kết quả tìm kiếm được cung cấp.{Biểu diễn Tài liệu Đã nén}Truy vấn: Ai đã nhận Giải Nobel Vật lý đầu tiên?Câu trả lời: LLMWilhelm Conrad Röntgen

Tài liệu 1: (Tiêu đề: Danh sách người đoạt Giải Nobel Vật lý) Giải Nobel Vật lý đầu tiên được trao …Tài liệu 2: ……Tài liệu N: (Tiêu đề: E. C. George Sudarshan) Năm 2007, Sudarshan nói với "Hindustan Times", …Truy vấn: Ai đã nhận Giải Nobel Vật lý đầu tiên?Biểu diễn Tài liệu Đã nén(độ dài biểu diễn ≈200 tokens)…

~3K tokensFigure 2: Khung làm việc của phương pháp chúng tôi.

Bộ Mã hóa Ngữ cảnh Hướng dẫn Truy vấn Ở bước đầu tiên, chúng tôi đưa sự nối của truy vấn xq và tài liệu xd vào bộ mã hóa ngữ cảnh nhận thức truy vấn để học các biểu diễn của truy vấn và tài liệu.

Bộ mã hóa bao gồm hai lớp mã hóa Transformer. Hình thức, các biểu diễn này có thể được thu được theo cách sau:

[hq;hd] =ContextEncoder ([xq;xd]).(2)

Ở đây, hq={hq i}Nq i=1và hd={hd i}Nd i=1 là các chuỗi biểu diễn tương ứng của truy vấn và tài liệu với độ dài Nq và Nd, tương ứng. Bằng cách cho phép truy vấn và tài liệu nhìn thấy nhau trong quá trình mã hóa, chúng tôi có thể tạo điều kiện cho việc trích xuất thông tin quan trọng liên quan đến truy vấn trong tài liệu.

--- TRANG 4 ---
Lớp Gộp Hướng dẫn Truy vấn Ở bước tiếp theo, chúng tôi chia toàn bộ tài liệu thành nhiều n-gram và nén thông tin của mỗi n-gram thành một vector dựa trên mối tương quan của chúng với truy vấn. Để đạt được điều này, các biểu diễn tài liệu được tổ chức như sau:

hd= [hd G1, ...,hd Gj, ...,hd GNg] (3)
= [hd 1:n, ...,hd (j−1)×n:j×n, ...,hd Nd−n+1:Nd],

trong đó Gj đại diện cho các chỉ số của n-gram thứ j. Ng=Nd n là số lượng n-gram.

Sau đó, chúng tôi đo trọng số của mỗi token trong Gj bằng cách tính toán mức độ liên quan của nó với biểu diễn trung bình hq của các token truy vấn:

hq=1 NqX hq i, (4)

wi,Gj=exps(hq, hd i)P i′∈Gjexps(hq, hd i′), (5)

trong đó s(·,·) là hàm tích vô hướng, và wi,Gj đại diện cho trọng số của biểu diễn token thứ i hd i trong tài liệu, thuộc về n-gram thứ j.

Cuối cùng, chúng tôi thu được các biểu diễn n-gram đã nén ˆhd Gj như tổng có trọng số của các biểu diễn token trong n-gram:

ˆhd Gj=X i∈Gjwi,Gj·hd i. (6)

Lớp Xem xét Truy vấn-Tài liệu Để tiếp tục ngăn chặn mất mát thông tin quan trọng trong nén, chúng tôi giới thiệu một module xem xét mới để hoàn thiện các biểu diễn n-gram đã nén bằng cách sửa đổi cả biểu diễn truy vấn và tài liệu. Cụ thể, bộ mã hóa này bao gồm hai lớp mã hóa Transformer, nhận các biểu diễn truy vấn hq, các biểu diễn tài liệu hd, và các biểu diễn n-gram đã nén ˆhd làm đầu vào, và xuất ra các biểu diễn n-gram tài liệu được cải thiện ehd:

ehd=ReviewingLayer ([hq;hd;ˆhd]).(7)

Lớp Căn chỉnh Ngữ nghĩa Vì ehd nằm trong không gian nhúng khác với các đầu vào của LLM, chúng tôi sử dụng lớp căn chỉnh ngữ nghĩa kết nối đầy đủ để ánh xạ các biểu diễn n-gram vào không gian nhúng của LLM. Các biểu diễn n-gram được căn chỉnh ed có thể được hình thức hóa như sau:

ed=W·ehd+b, (8)

trong đó W và b là các tham số có thể học.

Bộ Mã hóa Ngữ cảnh Hướng dẫn Truy vấn!!!"Lớp Gộp Hướng dẫn Truy vấnLớp Xem xét Truy vấn-Tài liệuLớp Căn chỉnh Ngữ nghĩa

"!"""#!""#"""##"ℎ$#!"ℎ$#""ℎ$##""%"Biểu diễn Tài liệu Đã nén&"

ℎ'!ℎ'!Hình 3: Cấu trúc của QGC. Ba lớp đầu tiên sử dụng truy vấn q để hướng dẫn mã hóa, gộp, và xem xét tài liệu d tương ứng. Lớp cuối căn chỉnh các biểu diễn tài liệu vào không gian nhúng LLM mục tiêu.

3.2 Huấn luyện Bộ Nén
Khác với AutoCompressor (Chevalier et al., 2023), chúng tôi cố định tham số của LLM và chỉ tinh chỉnh bộ nén.

Thông qua các bước trên, mỗi tài liệu dài được nén thành chuỗi biểu diễn liên tục ngắn hơn ed. Do đó, các đầu vào của LLM cuối cùng được định dạng như ex= (xins,ed1, ...,edk, ...,edK,xq). Để tránh thiếu thông tin quan trọng trong quá trình nén, chúng tôi định nghĩa mục tiêu huấn luyện của bộ nén theo cách sau:

L=LCE+LKL (9)
=−logp(y|ex) +KL[p(y|x)||p(y|ex)],

trong đó KL[·||·] đại diện cho phân kỳ Kullback–Leibler. Bằng cách giới thiệu mất mát KL, chúng tôi khuyến khích LLM tạo ra câu trả lời đúng ngay cả với các biểu diễn đã nén làm đầu vào.

3.3 Chiến lược Nén Động
Do tầm quan trọng khác nhau của các tài liệu được truy xuất, chúng tôi đề xuất điều chỉnh động tỷ lệ nén cho các tài liệu được truy xuất khác nhau. Cụ thể, chúng tôi gán kích thước n-gram nk cho tài liệu thứ k dựa trên xếp hạng tầm quan trọng:

nk=( min(2 ·Ok,16) Sk≥ϵ ∞ Sk< ϵ, (10)

trong đó Sk và Ok là điểm số và xếp hạng của tài liệu thứ k thu được bởi bộ xếp hạng lại hiện có, như Contriever (Izacard et al., 2022a). ϵ là ngưỡng điểm số để lọc các tài liệu có điểm thấp. Lưu ý rằng khi kích thước n-gram được gán nk được đặt thành ∞, tài liệu tương ứng sẽ bị loại bỏ.

--- TRANG 5 ---
Phương phápNaturalQuestions TriviaQA HotpotQA
Acc CR TP EM CR TP F1 CR TP
LongChat-13B
Sách đóng 34.84 - - 36.07 - - 22.19 - -
Oracle 83.05 59.2x - - - - 60.61 42.2x -
Lời nhắc Gốc 53.11 1.0x - 48.70 1.0x - 44.76 1.0x -
Phương pháp dựa trên Bộ xếp hạng lại
Sentence-BERT (Reimers and Gurevych, 2020) 60.75 4.1x 0.137 48.89 4.5x 1.957 42.92 4.4x 1.930
BGE-Reranker (Xiao et al., 2023) 64.33 4.1x 0.138 47.71 4.5x 1.724 47.96 4.4x 1.689
Cond.PPL (Jiang et al., 2023) 65.91 4.1x 0.128 52.48 4.5x 1.287 49.82 4.3x 1.267
Phương pháp dựa trên Nén
Selective-Context (Li et al., 2023) 35.44 2.5x 0.077 42.73 2.5x 0.465 29.68 2.6x 0.456
LongLLMLingua (Jiang et al., 2023) † 66.70 3.9x - - - - - - -
LongLLMLingua (Jiang et al., 2023) 67.01 4.1x 0.118 51.51 3.7x 0.724 45.43 3.8x 0.683
QGC 69.19 15.2x 0.356 57.72 7.9x 1.832 52.12 8.8x 1.849
LLaMA-2-7B
Sách đóng 32.35 - - 30.70 - - 10.54 - -
Oracle 73.45 59.2x - - - - 57.68 42.2x -
Lời nhắc Gốc 27.53 1.0x - 49.47 1.0x - 44.24 1.0x -
Phương pháp dựa trên Bộ xếp hạng lại
Sentence-BERT (Reimers and Gurevych, 2020) 24.26 4.1x 0.133 49.49 4.5x 0.731 40.65 4.4x 0.752
BGE-Reranker (Xiao et al., 2023) 25.08 4.1x 0.130 48.69 4.5x 0.683 46.13 4.4x 0.724
Cond.PPL (Jiang et al., 2023) 27.87 4.1x 0.123 52.76 4.5x 0.602 47.84 4.3x 0.623
Phương pháp dựa trên Nén
Selective-Context (Li et al., 2023) 31.79 2.6x 0.082 48.55 2.5x 0.303 28.21 2.6x 0.332
LongLLMLingua (Jiang et al., 2023) 41.13 4.1x 0.108 50.44 3.7x 0.432 39.87 3.8x 0.438
AutoCompressor (Chevalier et al., 2023) 49.23 13.9x 0.302 29.17 8.7x 0.823 29.02 8.1x 0.833
ICAE (Ge et al., 2023) 53.34 21.5x - 48.91 10.2x - 34.50 9.5x -
QGC 60.90 15.2x 0.313 57.46 7.9x 0.902 51.64 8.8x 0.927
QGC( ϵ= 0.42) 57.62 20.6x - 57.11 10.9x - 51.23 12.1x -

Bảng 1: Kết quả thực nghiệm trên ba bộ dữ liệu điểm chuẩn. Acc= độ chính xác, EM= khớp chính xác, F1= điểm F1, CR = tỷ lệ nén, TP= thông lượng (ví dụ/giây). Sách đóng, Oracle, và Lời nhắc Gốc biểu thị sử dụng chỉ truy vấn, tài liệu sự thật cơ bản hoàn chỉnh, và tất cả tài liệu được truy xuất làm đầu vào, tương ứng. † chỉ ra rằng kết quả được trích dẫn trực tiếp từ Jiang et al. (2023).

4 Thực nghiệm
Trong phần này, chúng tôi tiến hành các thực nghiệm rộng rãi để điều tra hiệu quả của QGC.

Bộ dữ liệu & Chỉ số Đánh giá Các thực nghiệm được thực hiện dựa trên ba bộ dữ liệu:

•NaturalQuestions Chúng tôi chọn phiên bản đã xử lý (Liu et al., 2023) nơi mỗi câu hỏi có 20 tài liệu liên quan và chỉ một trong số chúng chứa câu trả lời đúng. Chúng tôi theo Liu et al. (2023) để sử dụng độ chính xác (Acc) làm chỉ số đánh giá, đánh giá liệu câu trả lời đúng có xuất hiện trong dự đoán không.

•TriviaQA Chúng tôi sử dụng Contriever đối kháng (Izacard et al., 2022a) để truy xuất top 10 tài liệu từ tất cả đoạn văn Wikipedia. Theo Lewis et al. (2020), chúng tôi sử dụng chỉ số Khớp Chính xác (EM) để đánh giá dự đoán LLM.

•HotpotQA Khác với hai bộ dữ liệu trên, HotpotQA (Yang et al.) là bộ dữ liệu đa bước nhảy nơi câu trả lời nằm trong nhiều hơn một tài liệu. Cụ thể, mỗi câu hỏi có 10 tài liệu liên quan và hai trong số chúng là tài liệu sự thật cơ bản. Theo Yang et al., chúng tôi sử dụng điểm F1 để đo tính đúng đắn của LLM.

Ngoài ra, chúng tôi tính toán tỷ lệ nén (CR)

--- TRANG 6 ---
cho các phương pháp khác nhau, được định nghĩa là tỷ lệ độ dài của ngữ cảnh gốc so với ngữ cảnh đã nén. Chúng tôi cũng cung cấp thông lượng suy luận (TP) trên một GPU A100-80G duy nhất, bao gồm nén và tạo sinh.

Đường chuẩn Theo (Jiang et al., 2023), chúng tôi bao gồm hai nhóm phương pháp làm đường chuẩn của chúng tôi.

1)Phương pháp dựa trên Bộ xếp hạng lại. Nó đơn giản sử dụng phương pháp xếp hạng lại để sắp xếp tài liệu dựa trên tầm quan trọng và loại bỏ những tài liệu không quan trọng. Chúng tôi chọn bộ xếp hạng lại sau: Sentence-BERT (Reimers and Gurevych, 2020), BGE-Reranker (Xiao et al., 2023), và Cond.PPL được đề xuất bởi Jiang et al. (2023) để đo mức độ liên kết giữa truy vấn và tài liệu. Sau đó, chúng tôi loại bỏ các tài liệu có mức độ liên kết thấp cho đến khi đạt tỷ lệ nén và sắp xếp các tài liệu còn lại theo mức độ liên kết từ cao đến thấp.

2)Phương pháp dựa trên Nén. So với các phương pháp dựa trên bộ xếp hạng lại, chúng nén thêm các tài liệu đã sắp xếp, giữ lại nhiều thông tin hơn trong khi thỏa mãn tỷ lệ nén cao hơn. Chúng tôi chọn các phương pháp sau làm đường chuẩn:

•Selective-Context (Li et al., 2023) Nó sử dụng thông tin tự nhiên được ước tính bởi một mô hình ngôn ngữ bên ngoài để cắt tỉa các từ dư thừa.

•LongLLMLingua (Jiang et al., 2023) Đây là phương pháp hiện đại nhất cho nén ngữ cảnh dài. Nó đầu tiên sử dụng một mô hình ngôn ngữ để định lượng tầm quan trọng của mỗi tài liệu như độ khó hiểu nhận thức câu hỏi của nó, và sau đó thiết kế phương pháp nén từ thô đến tinh nhận thức câu hỏi để xóa các token không quan trọng.

•AutoCompressor (Chevalier et al., 2023) Nó tinh chỉnh LLaMA-2-7B để đệ quy nén ngữ cảnh dài thành các vector tóm tắt, được sử dụng như lời nhắc mềm để tạo câu trả lời. Chúng tôi sử dụng AutoCompressor-Llama-2-7B-6K đã phát hành cho thực nghiệm.

•ICAE (Ge et al., 2023) Tương tự như AutoCompressor, nó tạo ra các khe nhớ gọn gàng và thông tin để đại diện cho ngữ cảnh gốc. Chúng tôi sử dụng mô hình ICAE đã phát hành được tiền huấn luyện trên Llama-2-7B-Chat cho thực nghiệm5.

Chi tiết Triển khai Chúng tôi sử dụng LongChat-13B-16K và LLaMA-2-7B làm LLMs để đánh giá,

5https://github.com/getao/icae

Phương pháp Độ chính xác
QGC 69.19
w/o bộ mã hóa ngữ cảnh hướng dẫn truy vấn 50.36
w/o lớp gộp hướng dẫn truy vấn 55.34
w/o lớp xem xét truy vấn-tài liệu 64.14
w/o chiến lược nén động 62.15

Bảng 2: Độ chính xác của nghiên cứu loại bỏ trên bộ kiểm tra NaturalQuestions, nơi LLM mục tiêu là LongChat-13B.

được đóng băng trong quá trình tối ưu hóa QGC. Để đảm bảo kết quả ổn định và có thể tái tạo, chúng tôi sử dụng giải mã tham lam và đặt nhiệt độ thành 0 trong tất cả thực nghiệm. Theo Jiang et al. (2023), chúng tôi sử dụng LLaMA-2-7B-Chat làm mô hình ngôn ngữ bên ngoài cho Selective-Context và LongLLMLingua. Đối với QGC, cả bộ mã hóa ngữ cảnh hướng dẫn truy vấn và lớp xem xét truy vấn-tài liệu đều bao gồm hai lớp mã hóa Transformer. Tất cả các lớp này và nhúng từ được khởi tạo với LLaMA-2-7B nơi các tham số MLP đều được cố định trong quá trình huấn luyện. Lý do của chúng tôi đằng sau cách tiếp cận này xuất phát từ niềm tin rằng MLP đóng vai trò quan trọng trong việc giữ lại kiến thức, trong khi trọng tâm của chúng tôi nằm ở việc điều chỉnh kiến thức thu được dựa trên truy vấn. Do đó, các tham số có thể huấn luyện trong QGC chỉ là 3.5% của LongChat-13B-16K. Ngoài tài liệu sự thật cơ bản, chúng tôi nối 1-4 tài liệu ngẫu nhiên để xây dựng ngữ cảnh dài. Chúng tôi cũng đặt ngẫu nhiên kích thước n-gram từ danh sách ứng cử viên (4, 6, 8, 10) cho mỗi batch huấn luyện để làm cho bộ nén mạnh mẽ hơn. Chúng tôi huấn luyện QGC trên các bộ dữ liệu downstream trong 15 epoch, sử dụng tốc độ học 5e-5 với bộ tối ưu Adam và kích thước batch 64. Trong quá trình suy luận, chúng tôi sử dụng Cond.PPL được đề xuất bởi Jiang et al. (2023) để sắp xếp các tài liệu được truy xuất cho tất cả các phương pháp dựa trên nén và QGC, và đặt ϵ là 0.35. Theo (Liu et al., 2023; Bai et al., 2023) số token tạo tối đa là 100 cho NaturalQuestions, và 32 cho cả TriviaQA và HotpotQA. Tất cả thực nghiệm được tiến hành trên 8 GPU NVIDIA A100.

Kết quả Chính Bảng 1 báo cáo hiệu suất, tỷ lệ nén, và thông lượng của các phương pháp hoặc mô hình khác nhau trên các bộ dữ liệu khác nhau. Nhìn chung, QGC đạt được tỷ lệ nén cao hơn và thông lượng lớn hơn trong khi đạt được hiệu suất tương đương hoặc thậm chí tốt hơn với LongLLMLingua. Những kết quả này chứng minh rằng QGC có thể nén hiệu quả ngữ cảnh thành các đầu vào ngắn hơn.

Cụ thể, hiệu suất và tỷ lệ nén của các phương pháp dựa trên bộ xếp hạng lại bị hạn chế

--- TRANG 7 ---
bởi vì không có thao tác nén nào được sử dụng trong tài liệu. So với AutoCompressor và ICAE, phương pháp của chúng tôi đạt được độ chính xác tốt hơn với tỷ lệ nén tương đương. So với LongLLMLingua, QGC đạt được cải thiện hiệu suất trung bình +5.03 và +12.87 khi sử dụng LongChat-13B và LLaMA-2-7B làm LLMs mục tiêu. Trung bình, tỷ lệ nén và thông lượng của QGC lần lượt là 2.75 lần và 2.47 lần so với LongLLMLingua trên tất cả bộ dữ liệu và LLMs mục tiêu.

Nghiên cứu Loại bỏ Để khám phá tác động của các thành phần khác nhau trên QGC, chúng tôi sử dụng LongChat-13B làm LLM mục tiêu và giới thiệu các biến thể sau của QGC cho nghiên cứu loại bỏ: 1) w/o bộ mã hóa ngữ cảnh hướng dẫn truy vấn. Trong biến thể này, truy vấn và tài liệu được mã hóa độc lập; 2) w/o lớp gộp hướng dẫn truy vấn. Khi thiết lập biến thể này, chúng tôi trực tiếp thay thế tổng có trọng số của các biểu diễn token trong mỗi n-gram bằng biểu diễn trung bình của chúng; 3) w/o lớp xem xét truy vấn-tài liệu. Biến thể này không còn tinh chỉnh các biểu diễn đã nén của n-gram; 4) w/o chiến lược nén động. Chúng tôi cố định kích thước n-gram là 4 để so sánh tương đương.

Như được hiển thị trong Bảng 2, việc thiếu lớp xem xét truy vấn-tài liệu và chiến lược nén động dẫn đến mất mát độ chính xác lần lượt là 5.05 và 7.04. Mất mát đáng kể hơn được quan sát sau khi loại bỏ bộ mã hóa ngữ cảnh hướng dẫn truy vấn và lớp gộp hướng dẫn truy vấn, dẫn đến

1.0 1.5 2.0 2.5 3.0 3.5 4.0
Tỷ lệ Nén4050607080Độ chính xác (%)
LongLLMLingua
LongLLMLingua
w/ answer
QGC(a) Tỷ lệ Nén cho QGC

1 doc 2 docs 3 docs 4 docs 5 docs020406080Độ chính xác (%)Sách đóng
Oracle
QGC
(b) Số Tài liệu cho QGC

Hình 4: Độ chính xác của QGC với các tỷ lệ nén và số lượng tài liệu khác nhau, tương ứng.

sự sụt giảm độ chính xác hiệu suất đáng kể lần lượt là 18.83 và 13.85, làm nổi bật tầm quan trọng của việc sử dụng truy vấn để hướng dẫn nén.

Phương phápSST-2 GSM8K
Acc CR Acc CR
Lời nhắc Gốc 92.4 1.0x 14.48 1.0x
LongLLMLingua - - 5.91 3.9x
AutoCompressor 94.2 15.0x 6.68 13.6x
QGC 94.8 23.3x 14.18 13.4x

Bảng 3: Kết quả thực nghiệm trên bộ dữ liệu SST-2 và GSM8K, nơi LLM mục tiêu là LLaMA-2-7B.

5 Phân tích
Trong phần này, chúng tôi tiến hành các phân tích sâu để khám phá hiệu suất của QGC về mặt mất mát thông tin quan trọng, nén minh chứng, thông lượng chi tiết và tác động của bộ xếp hạng lại. Tất cả các phân tích được tiến hành trên NaturalQuestions với LLM mục tiêu là LongChat-13B.

Mất mát Thông tin Quan trọng trong QGC Như được mô tả trong Phần 2.2, các phương pháp trước đây mất đáng kể thông tin quan trọng khi tỷ lệ nén tăng. Để so sánh, chúng tôi thực nghiệm với QGC sử dụng cùng cài đặt.

So với LongLLMLingua trong Hình 4(a), hiệu suất của QGC chỉ giảm 10% khi tỷ lệ nén tăng từ 1x lên 4x, và thậm chí tương đương với LongLLMLingua chứa câu trả lời đúng trong kết quả đã nén. Như được thấy trong Hình 4(b), chúng tôi quan sát thấy hiệu suất của QGC giảm nhẹ với nhiều tài liệu hơn, đây chỉ là mức giảm 12% với 4 tài liệu (27% cho AutoCompressor). Những kết quả này chứng minh rằng QGC có thể hiệu quả giữ lại thông tin quan trọng ngay cả trong các tình huống ngữ cảnh dài hơn nhiều và tỷ lệ nén cao hơn.

Nén Minh chứng cho Học trong Ngữ cảnh Để tiếp tục xác nhận hiệu quả của QGC trong bối cảnh rộng hơn, chúng tôi tiến hành thực nghiệm trên cả bộ dữ liệu SST-2 và GSM8K. Chúng tôi áp dụng cách tiếp cận của các nghiên cứu trước đây (Chevalier et al., 2023; Wei et al., 2022) sử dụng các minh chứng như tài liệu, trong khi duy trì tính nhất quán với thiết lập thực nghiệm của họ. Kết quả trong Bảng 3 tiết lộ những hiểu biết đáng chú ý. Trên bộ dữ liệu SST-2, phương pháp của chúng tôi vượt qua autocompressor cả về tỷ lệ nén và độ chính xác. Trong khi đó, trên bộ dữ liệu GSM8K, hiệu suất độ chính xác của chúng tôi vẫn ngang bằng với lời nhắc gốc

--- TRANG 8 ---
ở cùng tỷ lệ nén với autocompressor. Điều này gợi ý rằng QGC đạt được sự cân bằng tuyệt vời giữa hiệu suất mô hình và tỷ lệ nén. Những kết quả này thể hiện khả năng thành thạo của QGC trong việc bảo toàn thông tin từ các minh chứng và thúc đẩy khả năng học trong ngữ cảnh của LLM mục tiêu.

Đánh giá Thông lượng Chi tiết Để đánh giá thông lượng của các phương pháp hoặc mô hình khác nhau, bao gồm cả nén và tạo sinh, chúng tôi thực hiện kiểm tra trên một GPU A100-80G duy nhất.

Kết quả được trình bày trong Hình 5 cho thấy QGC rõ ràng cao hơn LongLLMLingua cả về thông lượng nén và thông lượng tạo sinh. Hơn nữa, bằng cách điều chỉnh siêu tham số ϵ (Xem Phương trình 10) để tăng tỷ lệ nén, QGC có thể đạt được tỷ lệ nén cao hơn trong khi giảm thiểu tác động đến hiệu suất LLM và cải thiện thêm thông lượng. Hơn nữa, tỷ lệ nén cao hơn của chúng tôi dẫn đến đầu vào LLM ngắn hơn, cũng cải thiện đáng kể thông lượng tạo sinh của LLM mục tiêu. Đối với LongLLMLingua, vì nó bổ sung giới thiệu LLaMA-2-7B để nén, thông lượng nén thấp hơn đáng kể so với chúng tôi. Ngoài ra, mặc dù LongLLMLingua cũng có thể cải thiện tỷ lệ nén bằng cách điều chỉnh siêu tham số, hiệu suất của nó sẽ giảm đáng kể, trong khi QGC vẫn duy trì hiệu suất tuyệt vời.

QGC QGC LongLLMLingua LongLLMLingua0.00.51.01.52.02.5Thông lượng (ví dụ/s)
(CR=20.6x) (CR=15.2x) (CR=13.9x) (CR=4.1x)Acc=68.02 Acc=69.19
Acc=52.89
Acc=67.01Tạo sinh
NénHình 5: Độ chính xác, thông lượng nén, và thông lượng tạo sinh của QGC và LongLLMLingua.

Tác động của Các Bộ xếp hạng lại Khác nhau Tỷ lệ nén cho mỗi tài liệu được xác định bởi mối tương quan tương ứng với truy vấn thu được bởi một bộ xếp hạng lại. Ở đây, chúng tôi phân tích tác động của việc sử dụng các bộ xếp hạng lại khác nhau trong quá trình này. Ngoài ba phương pháp được giới thiệu trong các phương pháp dựa trên bộ xếp hạng lại, chúng tôi cũng bao gồm BM25 và Gzip (Jiang et al., b) để so sánh.

Kết quả thực nghiệm được hiển thị trong Hình 6. Có thể thấy rằng QGC hoạt động tốt hơn với các bộ xếp hạng lại cạnh tranh hơn. Ngoài ra, so với việc trực tiếp sử dụng bộ xếp hạng lại để nén, QGC không chỉ đạt được tỷ lệ nén cao hơn trung bình 2.65 lần mà còn duy trì hiệu suất không mất mát hoặc thậm chí được cải thiện.

BM25 Gzip SBERT BGE-Reranker Cond.PPL3040506070Độ chính xác (%)Cơ bản (avg. CR = 4.1x)|
QGC (avg. CR = 15x)Hình 6: Hiệu suất của QGC sử dụng các bộ xếp hạng lại khác nhau. "Cơ bản" đại diện cho hiệu suất của mỗi bộ xếp hạng lại được sử dụng để nén. Hiệu suất (Recall) của các bộ xếp hạng lại: Cond.PPL > BGE-Rererank > SBERT (Sentence-BERT) > Gzip > BM25.

6 Công trình Liên quan
Ngữ cảnh Dài cho LLMs Gần đây, đã có rất nhiều nghiên cứu tập trung vào việc mở rộng độ dài ngữ cảnh của LLMs (Press et al., 2021; Peng et al., 2023; Bertsch et al., 2023). Các nỗ lực hiện tại chủ yếu liên quan đến việc tăng dần kích thước cửa sổ trong quá trình tiền huấn luyện (Nijkamp et al., 2023), nội suy nhúng vị trí (Chen et al., 2023), và sửa đổi cơ chế chú ý (Ding et al., 2023). Khác với những công trình này, chúng tôi không trực tiếp nhằm mở rộng cửa sổ ngữ cảnh của LLMs. Do đó, QGC mà chúng tôi đề xuất có thể bổ sung cho các kỹ thuật này bằng cách cho phép LLMs truy cập ngữ cảnh rộng hơn với chi phí giảm và độ trễ ngắn hơn.

LMs tăng cường Truy xuất Kết hợp với một bộ truy xuất độc lập để tăng cường LMs đang trở nên phổ biến để mang lại lợi ích cho các nhiệm vụ chuyên sâu về kiến thức khác nhau. Các nghiên cứu trước đây đã đạt được kết quả đáng chú ý trong việc cải thiện độ khó hiểu (Wang et al., 2023a), độ chính xác thực tế (Nakano et al., 2022), hiệu suất nhiệm vụ downstream (Izacard et al., 2022b), và học trong ngữ cảnh (Huang et al., 2023). Ngoài ra, nhiều công trình tập trung vào việc hợp tác LLMs và tài liệu được truy xuất, như xếp hạng lại tài liệu được truy xuất (Mao et al.) và loại bỏ tài liệu không liên quan (Mallen et al.). QGC cũng là một phương pháp tăng cường truy xuất cho LLMs, nén hiệu quả các tài liệu được truy xuất thành các đầu vào ngắn hơn trong khi duy trì không có sự suy giảm hiệu suất đáng kể.

Nén Ngữ cảnh Với độ dài ngữ cảnh ngày càng tăng trong LLMs, nhu cầu về hiệu quả cao hơn,

--- TRANG 9 ---
chi phí thấp hơn, và độ trễ giảm đã thu hút nhiều sự chú ý. Như một giải pháp đầy hứa hẹn, các kỹ thuật nén có thể được phân loại rộng rãi thành hai loại: nén hộp đen (Xu et al., 2023) và nén hộp trắng (Wang et al., 2023b). Nén hộp đen chủ yếu liên quan đến việc cắt tỉa token dựa trên các thước đo tầm quan trọng khác nhau, như thông tin tự nhiên (Li et al., 2023) và độ khó hiểu LLM (Jiang et al., a, 2023). Mặt khác, nén hộp trắng tập trung vào việc tạo ra tóm tắt hoặc nén ngữ cảnh thành lời nhắc mềm thông qua tinh chỉnh hoặc Thích ứng Hạng Thấp (LoRA). Ví dụ, Wang et al. (2023b) tự động tạo nội dung đã lọc và tinh chỉnh LLM mục tiêu để sử dụng nó cho việc tạo sinh. Mu et al. (2023) huấn luyện LLMs để nén hướng dẫn thành các tiền tố chú ý key-value ngắn gọn. Chevalier et al. (2023) đệ quy nén văn bản dài thành các vector tóm tắt, trong khi Ge et al. (2023) tạo ra các khe nhớ để đại diện cho ngữ cảnh gốc. So với các nghiên cứu nén được đề cập ở trên, thiết kế của QGC hoàn toàn tính đến truy vấn, dẫn đến việc giữ lại thông tin quan trọng được tăng cường, tỷ lệ nén cao hơn, thông lượng cao hơn, và hiệu suất tổng thể được cải thiện.

7 Kết luận và Hướng Nghiên cứu Tương lai
Trong bài báo này, chúng tôi đã trình bày một bộ nén hướng dẫn truy vấn QGC cho LLMs để giải quyết việc mất thông tin quan trọng dưới tỷ lệ nén cao. Nó bao gồm bốn thành phần thiết yếu: bộ mã hóa ngữ cảnh hướng dẫn truy vấn, lớp gộp hướng dẫn truy vấn, lớp xem xét truy vấn-tài liệu, và lớp căn chỉnh ngữ nghĩa. Ngoài ra, chúng tôi cũng đề xuất một chiến lược nén động trong quá trình suy luận. Các thực nghiệm rộng rãi trên các nhiệm vụ QA đa tài liệu chứng minh rằng QGC vượt trội hơn các phương pháp nén hiện đại trước đây cả về độ chính xác và tỷ lệ nén. Các phân tích tiết lộ rằng điều này chủ yếu do việc giữ lại thông tin quan trọng của chúng tôi trong suốt quá trình nén.

Trong tương lai, chúng tôi nhằm xác nhận cách tiếp cận của chúng tôi trên các LLMs tiên tiến hơn, đồng thời mở rộng ứng dụng của nó cho các nhiệm vụ bổ sung như tóm tắt tài liệu. Ngoài ra, chúng tôi sẽ cố gắng cải thiện thêm cách tiếp cận của chúng tôi bằng cách kết hợp các nghiên cứu trước đây (Zhang et al., a; Hu et al., 2022; Zhang et al., 2022, b).

Hạn chế
QGC là một bộ nén hộp trắng đòi hỏi quyền truy cập vào các tham số nội bộ của LLMs, điều này hạn chế khả năng áp dụng của nó. Hơn nữa, chúng tôi chỉ xác nhận hiệu quả của QGC trên nhiệm vụ QA và ICL, và hiệu suất của nó trên các nhiệm vụ khác khác biệt đáng kể so với nhiệm vụ QA, như tóm tắt, vẫn cần được xác minh.

Lời cảm ơn
Dự án được hỗ trợ bởi Chương trình R&D Chính quốc gia Trung Quốc (Số 2022ZD0160501), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62276219), và Dự án Nền tảng Dịch vụ Công nghệ Công cộng của Xiamen (Số 3502Z20231043). Chúng tôi cũng cảm ơn các nhà đánh giá vì những bình luận sâu sắc của họ.

Tài liệu tham khảo
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508 .

Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. 2023. Unlimiformer: Long-range transformers with unlimited length input. arXiv preprint arXiv:2305.01625 .

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595 .

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788 .

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486 .

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234 .

Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context compression in a large language model. arXiv preprint arXiv:2307.06945 .

--- TRANG 10 ---
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations .

Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan Catanzaro. 2023. Raven: In-context learning with retrieval augmented encoder-decoder language models.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Atlas: Few-shot learning with retrieval augmented language models.

Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. a. LLMLingua: Compressing prompts for accelerated inference of large language models. In EMNLP 2023 .

Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839 .

Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, and Jimmy Lin. b. "low-resource" text classification: A parameter-free classification method with compressors. In Findings of ACL 2023 .

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. NeurIPS 2020 .

Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023. Compressing context to enhance inference efficiency of large language models. In EMNLP 2023 .

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172 .

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In ACL 2023 .

Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Reader-guided passage reranking for open-domain question answering. In Findings of ACL 2021 .

Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467 .

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. Webgpt: Browser-assisted question-answering with human feedback.

Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et al. 2023. Xgen-7b technical report. arXiv preprint arXiv:2309.03450 .

OpenAI OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. NeurIPS 2020 .

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071 .

Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409 .

Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In EMNLP 2020 .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .

Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, and Bryan Catanzaro. 2023a. Shall we pretrain autoregressive language models with retrieval? a comprehensive study.

Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023b. Learning to filter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377 .

--- TRANG 11 ---
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 2022 .

Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding.

Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv preprint arXiv:2310.04408 .

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP 2018 .

Biao Zhang, Deyi Xiong, Yubin Ge, Junfeng Yao, Hao Yue, and Jinsong Su. 2022. Aan+: Generalized average attention network for accelerating neural transformer. Journal of Artificial Intelligence Research .

Biao Zhang, Deyi Xiong, Jinsong Su, Qian Lin, and Huiji Zhang. a. Simplifying neural machine translation with addition-subtraction twin-gated recurrent networks. In EMNLP 2018 .

Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. b. MoEfication: Transformer feed-forward layers are mixtures of experts. InFindings of ACL 2022 .

A Hướng dẫn Được sử dụng trong QGC
Sau đây là các hướng dẫn chúng tôi đã sử dụng sau khi tham khảo các nghiên cứu hiện có (Liu et al., 2023) và kiểm tra.

•NaturalQuestions: Viết một câu trả lời chất lượng cao cho câu hỏi đã cho chỉ sử dụng các kết quả tìm kiếm được cung cấp (một số trong đó có thể không liên quan).

•TriviaQA & HotpotQA: Chỉ sử dụng các kết quả tìm kiếm được cung cấp (một số trong đó có thể không liên quan), trả lời câu hỏi sau bằng một hoặc vài từ.
