# 2404.04793.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/kv-cache/2404.04793.pdf
# Kích thước file: 827839 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
SQUEEZE ATTENTION: QUẢN LÝ 2D CỦA KV-
CACHE TRONG SUY LUẬN LLM THÔNG QUA NGÂN SÁCH
TỐI ƯU THEO TỪNG LAYER

Zihao Wang∗, Bin Cui†, và Shaoduo Gan†
Trường Khoa học Máy tính, Đại học Bắc Kinh
Geoming AI
{bin.cui, shaoduo.gan }@pku.edu.cn

TÓM TẮT
Tối ưu hóa bộ nhớ đệm Key-Value (KV) của Mô hình Ngôn ngữ Lớn (LLM) đã được coi là quan trọng để tiết kiệm chi phí suy luận. Hầu hết các thuật toán nén KV-cache hiện có đã cố gắng làm thưa thớt chuỗi token bằng cách tận dụng tầm quan trọng khác nhau của các token. Tuy nhiên, hầu hết các phương pháp này đều đối xử bình đẳng với tất cả các layer, phân bổ cùng một ngân sách KV cho mỗi layer. Cách tiếp cận này không tối ưu, vì một số layer có thể ít nhạy cảm với các token đầu vào nhưng vẫn nhận được cùng ngân sách như các layer khác. Trong nghiên cứu này, chúng tôi phát hiện rằng bằng cách xác định tầm quan trọng của các layer attention, chúng ta có thể tối ưu hóa KV-cache từ hai chiều, tức là theo chuỗi và theo layer. Dựa trên các quan sát của chúng tôi về tầm quan trọng theo layer trong suy luận, chúng tôi đề xuất SQUEEZE ATTENTION để tối ưu hóa chính xác việc phân bổ ngân sách KV-cache giữa các layer một cách linh hoạt và sau đó kết hợp ba thuật toán đại diện theo chuỗi để nén KV-cache cho mỗi layer với ngân sách riêng của nó. Cụ thể, trước tiên chúng tôi đo lường tầm quan trọng của mỗi layer bằng cách tính độ tương tự cosine của sự khác biệt prompt đầu vào trước và sau các layer self-attention. Dựa trên độ tương tự này, chúng tôi sau đó phân loại các layer thành hai nhóm và điều chỉnh ngân sách KV của chúng tương ứng. Bằng cách tối ưu hóa KV-cache từ cả chiều chuỗi và chiều layer, SQUEEZE ATTENTION đạt được khoảng 30% đến 70% giảm bộ nhớ và tăng tốc độ xử lý lên đến 2.2× trong một loạt rộng các LLM và benchmark. Mã nguồn có sẵn tại https://github.com/hetailang/SqueezeAttention.

1 GIỚI THIỆU

Hiệu suất đáng chú ý được đạt bởi các mô hình ngôn ngữ lớn sinh tạo (LLM) trên một loạt rộng các tác vụ xử lý ngôn ngữ tự nhiên (NLP) đang khiến mọi người trong ngành máy tính tin rằng nó có tiềm năng lớn để định hình lại cách họ thiết kế sản phẩm của mình. Năm qua đã chứng kiến sự gia tăng chưa từng có của các ứng dụng được thúc đẩy bởi LLM, như chatbot thông minh, công cụ tìm kiếm được hỗ trợ bởi LLM, trợ lý cá nhân kỹ thuật số, công cụ lập trình tự động, v.v. Cùng với các ứng dụng LLM ngày càng phát triển, chi phí suy luận khổng lồ của chúng bắt đầu trở thành một thách thức nghiêm trọng cản trở việc triển khai LLM và gây lo ngại về dấu chân carbon của chúng Faiz et al. (2023).

Đối với mô hình autoregressive chỉ có decoder, đây là kiến trúc LLM được áp dụng rộng rãi nhất, sự không hiệu quả trong suy luận chủ yếu đến từ thực tế rằng mô hình chỉ có thể sinh token từng cái một, và việc lấy mẫu mỗi token yêu cầu chú ý đến tất cả các token trước đó. Trong thực tế, các embedding key-value trung gian của mỗi layer đã được cache tăng dần trong mỗi lần lặp để tránh tính toán lại thường xuyên. Vì KV-cache tăng tuyến tính với số lượng layer attention,

*Công việc này được thực hiện trong thời gian thực tập tại Đại học Bắc Kinh và Geoming AI.
†Tác giả liên hệ.
1arXiv:2404.04793v2  [cs.LG]  10 Oct 2024

--- TRANG 2 ---
Hình 1: Minh họa các chính sách KV-cache trong suy luận từ góc nhìn chuỗi và layer attention. Full Cache (cột ngoài cùng bên trái) đơn giản lưu trữ các embedding KV cho tất cả các token trong tất cả các layer. Các thuật toán nén theo chuỗi (cột giữa) loại bỏ token trong chiều chuỗi, trong đó mỗi layer có cùng ngân sách cache. SQUEEZE ATTENTION (cột ngoài cùng bên phải) nén thêm KV-cache bằng cách phân bổ lại ngân sách cache một cách thích ứng trong chiều layer.

độ dài context và kích thước batch, nó thường kết thúc lớn gấp nhiều lần so với chính mô hình Sheng et al. (2023), và do đó, chi phối chi phí I/O của suy luận. Gần đây, tối ưu hóa KV-cache đã được coi rộng rãi là một cách tiếp cận quan trọng để tăng hiệu quả suy luận. Từ góc độ độ dài context, nhiều thuật toán được nghiên cứu kỹ đang cố gắng xác định các token "có giá trị" nhất trong chuỗi và loại bỏ những token không quan trọng để giảm KV-cache và độ phức tạp attention, như Sliding Window Attention Beltagy et al. (2020), Heavy-Hitter (H2O) Zhang et al. (2024), StreamingLLM Xiao et al. (2023), Scissorhands Liu et al. (2024), FastGen Ge et al. (2023) và những cái khác. Từ góc độ batching, nhiều nghiên cứu nhằm khám phá cách quản lý hiệu quả bộ nhớ KV-cache trên cơ sở batch với các độ dài chuỗi khác nhau Zheng et al. (2023); Kwon et al. (2023). Tuy nhiên, các cơ hội trong chiều của các layer attention hầu như chưa được đụng đến bởi hầu hết, nếu không muốn nói là tất cả, các phương pháp hiện có. Nói cách khác, tất cả các layer attention luôn được đối xử bình đẳng bởi những chiến lược KV caching đó. Do đó, trong bài báo này chúng tôi đặt câu hỏi:

Liệu tất cả các layer attention chia sẻ cùng một chiến lược KV caching có phải cache cùng một lượng token không? Nếu không, làm thế nào chúng ta có thể phân bổ chính xác ngân sách cache cho mỗi layer sao cho chúng ta có thể giảm thêm KV-cache trên cơ sở các nén theo chuỗi?

Để trả lời những câu hỏi này, chúng ta cần xem xét kỹ hơn các hành vi của các layer attention khác nhau trong quá trình suy luận. Một số manh mối truyền cảm hứng có thể được tìm thấy trong một vài nghiên cứu hiện có. Early-exiting LLM Del Corro et al. (2023), là một phương pháp suy luận được áp dụng rộng rãi, cho thấy sau khi đi qua một số lượng nhất định các layer attention, các biểu diễn ẩn có khả năng đạt đến độ bão hòa, và do đó, việc tính toán tiến có thể thoát sớm mà không cần hoàn thành toàn bộ mạng và vẫn có được dự đoán hợp lý. Bên cạnh đó, một nghiên cứu rất gần đây có tên FastGen Ge et al. (2023) phát hiện rằng các layer attention ở các vị trí khác nhau có các chiến lược KV caching tối ưu khác nhau. Ví dụ, các layer attention ở phần đầu của mô hình nên đơn giản cache tất cả các token trong chuỗi, trong khi một số layer giữa nên áp dụng các chiến lược loại bỏ cache dựa trên locality hoặc tần suất token. Mặc dù FastGen có thể chọn chiến lược loại bỏ tối ưu cho mỗi layer, vẫn chưa rõ làm thế nào để tìm ngân sách cache tối ưu, thay vì một siêu tham số thống nhất được định nghĩa trước, cho các layer chia sẻ cùng một chiến lược.

2

--- TRANG 3 ---
Dựa trên thực tế rằng các layer attention thực sự có các mức độ quan trọng khác nhau liên quan đến suy luận, chúng ta có thể đưa ra một giả thuyết hợp lý rằng bằng cách tận dụng tầm quan trọng của layer, chúng ta có thể "squeeze" thêm lượng KV-cache đã được nén bởi những thuật toán loại bỏ theo chuỗi đó, và cuối cùng, đạt được hiệu quả thậm chí tốt hơn.

Để mô tả tầm quan trọng của các layer attention một cách định lượng, chúng tôi theo dõi độ tương tự cosine, được coi là một metric mạnh mẽ để phản ánh sự tương tự của các embedding trong NLP Sidorov et al. (2014), giữa các biểu diễn ẩn trước và sau việc tính toán self-attention trong mỗi layer, và sau đó đặt dữ liệu của tất cả các layer lại với nhau để chứng minh cách một embedding đầu vào phát triển qua toàn bộ mô hình trong suy luận. Trực giác là embedding càng tương tự sau việc tính toán attention (được chỉ ra bởi độ tương tự cosine cao hơn), layer attention này càng ít có thể chèn thông tin vào embedding. Sau các điều tra rộng rãi trên nhiều mô hình LLM phổ biến, ví dụ, Mistral-7B, Falcon-7B, Llama2-7B, Llama2-70B, v.v., như được hiển thị trong Hình 2, chúng tôi tìm thấy một số đặc điểm chung. Thứ nhất, nửa đầu của các layer attention, nói chung, đóng góp nhiều hơn cho embedding đầu ra so với nửa thứ hai. Thứ hai, một số layer cụ thể, thường là vài layer đầu tiên và cuối cùng, có thể quan trọng hơn các layer khác, tùy thuộc vào mô hình và tập dữ liệu cụ thể.

Dựa trên chỉ số đơn giản nhưng hiệu quả này, chúng tôi đề xuất SQUEEZE ATTENTION, một thuật toán nén KV-cache 2D cắt tỉa KV-cache không chỉ từ chiều chuỗi mà còn từ chiều layer. Vì tầm quan trọng của layer phụ thuộc rất nhiều vào mô hình và tác vụ, SQUEEZE ATTENTION phân loại tất cả các layer thành các nhóm một cách linh hoạt bằng cách gom cụm độ tương tự cosine của chúng được đo trong giai đoạn prefilling prompt. Cho một chính sách loại bỏ KV-cache theo chuỗi (như Sliding Window Beltagy et al. (2020) hoặc H2O Zhang et al. (2024)), và một ngân sách cache thống nhất (như 4096 token hoặc 20% độ dài prompt), SQUEEZE ATTENTION tự động phân bổ lại ngân sách cache giữa các nhóm layer sao cho các layer quan trọng có thể cache nhiều token hơn để ổn định độ chính xác mô hình và các layer không quan trọng có thể loại bỏ nhiều token hơn để tiết kiệm chi phí I/O. Điều tốt hơn nữa là SQUEEZE ATTENTION trực giao với tất cả những thuật toán nén KV-cache theo chuỗi đó, vì vậy nó có thể được kết hợp một cách mượt mà với bất kỳ thuật toán nào trong số chúng. Hình 1 minh họa cách SQUEEZE ATTENTION hoạt động cùng với hai thuật toán loại bỏ KV-cache theo chuỗi đại diện, tức là H2O Zhang et al. (2024) và Sliding Window Attention Beltagy et al. (2020). Thêm chi tiết về thuật toán SQUEEZE ATTENTION có thể được tìm thấy trong Mục 4.

Theo hiểu biết tốt nhất của chúng tôi, SQUEEZE ATTENTION là thuật toán đầu tiên xem xét ngân sách KV-cache theo cách từng layer, làm cho nó trở thành một bổ sung có giá trị cho tất cả những thuật toán nén theo chuỗi đó cho suy luận. Trong các thí nghiệm của chúng tôi, chúng tôi tích hợp SQUEEZE ATTENTION vào 7 mô hình LLM phổ biến từ 7B đến 70B, tức là Llama2-7B, Mistral-7B, Falcon-7B, OPT-6.7B, GPT-Neox-20B, Mixtral-8×7B, và Llama2-70B, kết hợp với 3 thuật toán nén KV-cache theo chuỗi đại diện, tức là Heavy-Hitter Oracle (H2O), Sliding Window Attention và StreamingLLM. Kết quả cho thấy SQUEEZE ATTENTION có thể đạt được hiệu suất mô hình tốt hơn với ngân sách cache thậm chí thấp hơn so với cả ba thuật toán dưới một loạt rộng các mô hình và tác vụ, dẫn đến khoảng 30% đến 70% tiết kiệm bộ nhớ và tăng tốc độ xử lý lên đến 2.2× cho suy luận.

2 KIẾN THỨC NỀN TẢNG VÀ NGHIÊN CỨU LIÊN QUAN

2.1 GIẢI PHẪU KV-CACHE TRONG SUY LUẬN LLM

Đối với mô hình dựa trên transformer chỉ có decoder, quá trình suy luận thường bao gồm hai giai đoạn: prefilling và decoding. Trong prefilling, LLM lấy toàn bộ prompt làm đầu vào để tính toán và cache các embedding key-value của mỗi token trong mỗi layer attention. Sau đó giai đoạn decoding lấy một embedding tại một thời điểm để sinh token theo từng lần lặp, và đồng thời, nối embedding KV mới được tính toán vào KV-cache.

Gọi p là độ dài của prompt, o là độ dài của đầu ra, b là kích thước batch, nlayer là tổng số layer attention, và dmodel là chiều ẩn. Trong layer thứ i, ký hiệu trọng số mô hình liên quan đến attention Key và Value bởi wi_K và wi_V, trong đó wi_K ∈ Rdmodel×dmodel, và wi_V ∈ Rdmodel×dmodel.

3

--- TRANG 4 ---
Trong giai đoạn prefilling, ký hiệu trạng thái ẩn của layer thứ i bởi hi_prompt, trong đó hi_prompt ∈ Rb×p×dmodel. Sau đó KV-cache của layer thứ i sau prefilling có thể được công thức hóa như sau:

Ci_K = hi_prompt · wi_K; Ci_V = hi_prompt · wi_V (1)

Trong giai đoạn decoding, ký hiệu trạng thái ẩn của token đầu ra thứ j trong layer thứ i bởi hi_output_j (1 ≤ j ≤ o), trong đó hi_output_j ∈ Rb×1×dmodel. Sau đó KV-cache của layer thứ i sau khi sinh token thứ j có thể được công thức hóa như sau:

Ci_K = CONCAT(Ci_K, hi_output_j · wi_K) (2)
Ci_V = CONCAT(Ci_V, hi_output_j · wi_V) (3)

Khi quá trình decoding tiến hành, KV-cache phát triển tăng dần cho đến khi chuỗi đầu ra được hoàn thành đầy đủ. Do đó, số lượng float tối đa tổng cộng của KV-cache là:

2 · nlayer∑i=1[b · (p + o) · dmodel] (4)

hoặc đơn giản là 2 · dmodel · nlayer · b · (p + o).

Lấy Llama-2-7B trong FP16 làm ví dụ, trong đó nlayer = 32, dmodel = 4096. Toàn bộ trọng số mô hình tiêu thụ khoảng 14GB bộ nhớ, trong khi KV-cache chiếm khoảng 0.5MB mỗi token. Nói cách khác, KV-cache bắt đầu vượt quá trọng số mô hình khi xử lý hơn 28K token, có thể dễ dàng được tạo thành từ một batch 28 yêu cầu với độ dài nội dung 1K.

2.2 TỐI ƯU HÓA KV-CACHE HIỆN CÓ

Như đã phân tích ở trên, số lượng layer, kích thước batch và độ dài context là ba yếu tố quan trọng quyết định kích thước của KV-cache. Do đó, các nghiên cứu tối ưu hóa hiện có có khả năng tìm kiếm cơ hội từ những góc độ này.

Làm thưa thớt chuỗi context là một cách hiệu quả để phá vỡ mối quan hệ tuyến tính giữa độ dài context và KV-cache Del Corro et al. (2023); Zhang et al. (2024); Anagnostidis et al. (2023); Sukhbaatar et al. (2019); Rae & Razavi (2020). Ý định chung của những thuật toán này là tìm ra các token không quan trọng trong chuỗi và loại bỏ KV-cache của những token này. Ví dụ, Sliding Window Attention Beltagy et al. (2020) chỉ cache một số lượng nhất định các token gần đây nhất và loại bỏ phần còn lại. StreamingLLM Xiao et al. (2023) phát hiện rằng ngoài các token gần đây, các token ở đầu chuỗi cũng quan trọng đối với đầu ra. Heavy-Hitter Zhang et al. (2024) và Scissorhands Liu et al. (2024) xếp hạng tầm quan trọng của các token bằng cách so sánh điểm attention của chúng. Như đã đề cập ở trên, những thuật toán này đối xử bình đẳng với tất cả các layer attention, và do đó, có một ngân sách KV-cache cố định cho mỗi layer.

Tối ưu hóa KV-cache trên cơ sở batch chủ yếu cần quản lý bộ nhớ của các yêu cầu khác nhau một cách hiệu quả. Ví dụ, vLLM Kwon et al. (2023) phân bổ các khối bộ nhớ nhỏ theo cách theo yêu cầu, thay vì một khối lớn cố định cho mỗi prompt, để giảm phân mảnh bộ nhớ trong một batch. RadixAttention Zheng et al. (2023) quản lý để chia sẻ KV-cache giữa các yêu cầu trong một batch khi chúng có cùng tiền tố trong prompt.

Cuối cùng nhưng không kém phần quan trọng, cách thức để nới lỏng mối quan hệ tuyến tính giữa KV-cache và các layer vẫn còn phần lớn chưa được khám phá so với hai chiều kia. FastGen Ge et al. (2023) là một nghiên cứu rất gần đây phát hiện rằng các layer ở các vị trí khác nhau có thể có các chiến lược loại bỏ KV-cache theo chuỗi tối ưu khác nhau. Sau đó nó đề xuất một thuật toán để chọn chiến lược loại bỏ tốt nhất từ 1) chiến lược Locality (như Sliding Window), 2) chiến lược Special Tokens (như StreamingLLM), 3) chiến lược Local và Frequency (như Scissorhands, H2O) và các chiến lược khác cho mỗi attention head trong quá trình suy luận. Tuy nhiên, đối với tất cả các attention head đã được gán cùng một chiến lược loại bỏ, FastGen đơn giản cung cấp cho chúng một ngân sách cache thống nhất được định nghĩa trước, như 30% độ dài chuỗi. Do đó, cách thức phân bổ ngân sách KV-cache một cách thích ứng cho các layer với cùng chiến lược loại bỏ token theo chuỗi vẫn còn phần lớn chưa rõ ràng.

4

--- TRANG 5 ---
3 QUAN SÁT

Được truyền cảm hứng bởi một số nghiên cứu trước đây đã quản lý để tối ưu hóa suy luận LLM từ góc độ từng layer Del Corro et al. (2023); Ge et al. (2023), chúng tôi đưa ra một giả thuyết rằng các layer attention ở các vị trí khác nhau đóng vai trò khác biệt về mặt tầm quan trọng, và do đó, nên có các ngân sách KV-cache tối ưu khác nhau. Để hiểu rõ hơn về đóng góp theo từng layer cho embedding đầu ra, chúng tôi sử dụng độ tương tự cosine làm metric để định lượng tầm quan trọng của mỗi layer trong quá trình suy luận.

Cụ thể, trong mỗi layer attention, chúng tôi theo dõi trạng thái ẩn của mỗi embedding đầu vào tại hai điểm, tức là embedding trước và sau khối self-attention. Ký hiệu trạng thái ẩn trước self-attention bởi A, và trạng thái ẩn sau self-attention bởi B. Độ tương tự cosine giữa hai embedding A và B được tính như sau:

CosineSimilarity(A,B) = A·B / (||A|| · ||B||) = (∑n i=1 Ai·Bi) / (√∑n i=1 A²i · √∑n i=1 B²i) (5)

trong đó A = (A1, A2, ..., An) và B = (B1, B2, ..., Bn) là các vector, và n là số chiều.

Hình 2: Trực quan hóa độ tương tự cosine trước và sau việc tính toán self-attention của mỗi layer attention. Các layer có độ tương tự cosine cao hơn, được biểu diễn bằng màu sáng hơn, tác động tương đối thấp hơn lên các vector đầu vào.

Đối với bất kỳ embedding đầu vào nào, chúng ta có thể nhận được độ tương tự cosine của mỗi layer attention xấp xỉ định lượng mức độ embedding thay đổi sau việc tính toán self-attention của layer này. Sau đó bằng cách so sánh độ tương tự cosine giữa các layer, chúng ta có thể tìm cách xếp hạng các layer attention theo tầm quan trọng của chúng. Theo hướng suy nghĩ này, chúng tôi chọn 4 LLM đại diện, tức là Mistral-7B Jiang et al. (2023), Llama2-7B-32K Touvron et al. (2023), Llama2-70B Touvron et al. (2023), Falcon-7B Almazrouei et al. (2023), để tiến hành các thí nghiệm. Mỗi mô hình được cung cấp 200 prompt và chúng tôi theo dõi độ tương tự cosine của mỗi token trong mỗi layer. Hình 2 hiển thị kết quả sau khi lấy trung bình trên các prompt. Mỗi hàng của heatmap chứng minh cách một embedding đầu vào tại vị trí cho trước phát triển qua tất cả các layer. Từ độ sáng của các hình, chúng ta có thể có được một số hiểu biết như sau: 1) Nhìn chung, nửa đầu của các layer (màu tối hơn) đóng góp nhiều hơn cho các embedding đầu ra so với nửa thứ hai của các layer (màu sáng hơn); 2) Vài layer đầu tiên và cuối cùng có xu hướng quan trọng hơn các layer khác, tùy thuộc vào các mô hình và tác vụ cụ thể; 3) Độ tương tự cosine có thể mô tả hiệu quả tầm quan trọng theo từng layer, vì xu hướng nó phản ánh phù hợp với các nghiên cứu trước đây, như Early-exiting Del Corro et al. (2023) và FastGen Ge et al. (2023).

Quan sát này cho chúng ta một metric đơn giản nhưng hiệu quả để thiết kế một thuật toán mới có thể tối ưu hóa KV-cache không chỉ từ chiều context, mà còn từ chiều layer.

4 THUẬT TOÁN

Trong mục này, chúng tôi mô tả thuật toán SQUEEZE ATTENTION được truyền cảm hứng bởi các quan sát của chúng tôi từ chương trước. Tính năng đặc biệt nhất của thuật toán được đề xuất là nó xem xét các token trong KV-cache như một ma trận 2D với một chiều của chuỗi và chiều khác của layer, và cả hai chiều sẽ được tối ưu hóa cùng nhau.

5

--- TRANG 6 ---
4.1 SQUEEZE ATTENTION

Trong chiều chuỗi, có nhiều chính sách loại bỏ cache khác nhau mà chúng ta có thể kết hợp trực tiếp, như các phương pháp Least Recently Used (Sliding Window, StreamingLLM), các phương pháp Least Frequently Used (Scissorhands, H2O), v.v. Chúng tôi ký hiệu một chính sách nén KV-cache trong chiều chuỗi bởi Cseq, và ngân sách cache của nó bởi binit. Lưu ý rằng tất cả các layer có cùng ngân sách cache theo mặc định, giống như các giả định được đưa ra bởi tất cả những thuật toán nén KV-cache theo chuỗi này.

Trong chiều layer, SQUEEZE ATTENTION trước tiên theo dõi tầm quan trọng layer với prompt đã cho trong giai đoạn prefilling bằng cách thu thập độ tương tự cosine của mỗi layer bất cứ khi nào self-attention được thực hiện. Ở cuối prefilling, mỗi layer kết thúc với một tập hợp độ tương tự cosine, mỗi cái tương ứng với một token đã chảy qua layer này. Sau đó chúng tôi sử dụng giá trị trung bình trên các token prompt để đại diện cho tầm quan trọng theo từng layer của layer này liên quan đến tác vụ này. Bằng cách gom cụm các layer thành các nhóm dựa trên tầm quan trọng theo từng layer với KMeans, chúng tôi phân bổ lại binit cho mỗi layer theo cách mà nhiều ngân sách hơn được gán cho các nhóm layer "quan trọng" hơn.

Vì các layer có ngân sách cache khác nhau, trong giai đoạn decoding, Cseq hoạt động riêng biệt với ngân sách riêng của mỗi layer. Quá trình chi tiết được mô tả trong Thuật toán 1.

Thuật toán 1 SQUEEZE ATTENTION
Yêu cầu: prompt: chuỗi đầu vào; Cseq: một bộ nén KV-cache trong chiều chuỗi; binit: ngân sách cache ban đầu của mỗi layer; nlayer: số lượng layer attention; p: siêu tham số (0 < p < 1); K(i): KV-cache của layer thứ i;
1: Đưa prompt vào mô hình cho prefilling, tính cossim(i)_j của token thứ j trong layer thứ i bằng Phương trình 5;
2: for i ← 1 to nlayer do
3:    cossim(i) = (∑len(prompt) j=1 cossim(i)_j) / len(prompt)
4: end for
5: G1, G2, G3 ← KMeans(cossim(i)) ▷ Gom cụm các layer thành 3 nhóm bởi cossim(i) (1 ≤ i ≤ nlayer), trong đó G3 có cossim lớn nhất trung bình
6: for i ← 1 to nlayer do
7:    if i ∈ G3 then
8:       b(i) = binit × p
9:    else
10:      b(i) = (nlayer × binit - len(G3) × binit × p) / (len(G1) + len(G2))
11:   end if
12:   K(i) = KV(prompt) ▷ KV của prompt được cache sau prefilling
13: end for
14: for o ← 1 to len(output) do ▷ Decoding các token đầu ra từng cái một
15:   for i ← 1 to nlayer do
16:     if len(K(i)) > b(i) then
17:       K(i) = Cseq(K(i), b(i)) ▷ Nén KV-cache của layer này bằng ngân sách riêng của nó
18:     end if
19:     Hoàn thành Self-attention dựa trên K(i) đã nén.
20:   end for
21: end for

4.2 THẢO LUẬN

4.2.1 LÀM THẾ NÀO ĐỂ QUYẾT ĐỊNH GIÁ TRỊ CỦA p

SQUEEZE ATTENTION liên quan đến một siêu tham số p để kiểm soát tỷ lệ phần trăm ngân sách ban đầu có thể được loại bỏ khỏi các layer "không quan trọng". p càng nhỏ, càng nhiều ngân sách sẽ được phân bổ lại. Trong các thí nghiệm, chúng tôi thấy 0.3-0.4 là một lựa chọn phạm vi hợp lý trong hầu hết các trường hợp. Để hiểu chính xác tác động của p, chúng tôi đã tiến hành các thí nghiệm bổ sung để chứng minh cách độ chính xác mô hình thay đổi với giá trị của p, vui lòng tham khảo A.2 để biết thêm chi tiết.

6

--- TRANG 7 ---
4.2.2 TẠI SAO GOM CỤM THÀNH BA NHÓM?

Dựa trên các quan sát của 7 mô hình chúng tôi đã thử, chúng tôi thấy tất cả đều có một mẫu điển hình (3 nhóm) liên quan đến tầm quan trọng layer. Cụ thể, Nhóm 1 bao gồm một vài layer đặc biệt (luôn là vài layer đầu tiên và cuối cùng) có thể được coi như một sự tương tự của các token đặc biệt không bao giờ nên bị loại bỏ. Sau đó Nhóm 2 và Nhóm 3 không có đường biên cố định với nhau, nhưng chúng tôi thấy rằng Nhóm 3 tạo ra tác động rõ ràng ít hơn lên các embedding, có thể được coi như một sự tương tự của các token "thường xuyên" và "không thường xuyên" trong các phương pháp theo chuỗi. Do đó, chính sách 3-nhóm của chúng tôi có thể được diễn đạt lại như: "chúng tôi trước tiên xác định và ưu tiên các layer đặc biệt (Nhóm 1), sau đó phân loại các layer còn lại thành hai nhóm: quan trọng (Nhóm 2) và không quan trọng (Nhóm 3), sau đó phân bổ lại ngân sách cache dựa trên việc gom cụm. Ngay cả khi chúng tôi gom cụm các layer thành hơn 3 nhóm, chúng tôi chỉ chia nhỏ Nhóm 2 và Nhóm 3 thành nhiều nhóm nhỏ, nhưng cuối cùng chúng cần được giảm xuống thành hai lớp lại, tức là hoặc giảm ngân sách hoặc tăng ngân sách. Để bảo tồn độ chính xác mô hình, chúng tôi chỉ giảm ngân sách cache từ Nhóm 3, chiếm khoảng 50% đến 70% tổng số layer.

Bảng 1: Tập dữ liệu được sử dụng trong các thí nghiệm của chúng tôi.
Tác vụ | Loại tác vụ | Metric đánh giá | Độ dài trung bình | Ngôn ngữ | Mẫu
CNN/Daily Mail | Tóm tắt (3 câu) | Rouge-2 | 2,000 | EN | 1,000
XSUM | Tóm tắt (1 câu) | Rouge-2 | 2,000 | EN | 1,000
SAMSUM | Few shot | Rouge-L | 6,258 | EN | 200
NarrativeQA | Single-doc QA | F1 | 18,409 | EN | 200
TriviaQA | Few shot | F1 | 8,209 | EN | 200

5 THÍ NGHIỆM

5.1 THIẾT LẬP THÍ NGHIỆM

Mô hình LLM. Chúng tôi chọn 7 LLM đại diện, với kích thước mô hình từ 6.7B đến 70B và độ dài context từ 2K đến 32K, để đánh giá thuật toán được đề xuất: GPT-NeoX-20B, OPT-6.7B, Falcon-7B, Mistral-7B, Mixtral-8×7B, LLama2-7B-32k, LLama2-70B.

Tập dữ liệu. Chúng tôi tiến hành thí nghiệm trên 5 tập dữ liệu: CNN/Daily Mail, XSUM, TriviaQA, SAMSUM, và NarrativeQA. TriviaQA, SAMSUM, và NarrativeQA bắt nguồn từ LongBench Bai et al. (2023), nơi độ dài dữ liệu thường vượt quá 8k. CNN/Daily Mail và XSUM có độ dài trung bình khoảng 2k. Thông tin chi tiết về các tập dữ liệu có thể được tìm thấy trong Bảng 1.

Baseline. 3 thuật toán thưa thớt theo chuỗi được chọn làm baseline để tích hợp vào SQUEEZE ATTENTION và chúng tôi gán mỗi thuật toán cho mô hình hoạt động tốt nhất, mà chúng tôi gọi là các thuật toán baseline tốt nhất:

• Heavy-Hitter (H2O) Zhang et al. (2024): Xác định các token quan trọng trong chuỗi bằng cách so sánh điểm attention tích lũy của mỗi token.

• Sliding Window Attention Beltagy et al. (2020): Một chiến lược "Local" chỉ cache các embedding KV của các token gần đây nhất. Phương pháp này hoạt động đặc biệt tốt với Mistral và Mixtral.

• StreamingLLM Xiao et al. (2023): Ngoài các token gần đây, StreamingLLM luôn cache n token đầu tiên trong chuỗi, vì chúng được xác định là "sink tokens". Chúng tôi lấy n = 4 như được khuyến nghị bởi bài báo.

Phần cứng. Chúng tôi tiến hành tất cả các thí nghiệm trên nền tảng AWS (p4d.24xlarge) với 8 Nvidia A100-40GB GPU, được kết nối bằng NVLinks (băng thông peer-to-peer GPU 600 GB/s).

7

--- TRANG 8 ---
Hình 3: Hiệu suất của SQUEEZE ATTENTION, baseline tốt nhất, và Full Cache dưới các ngân sách cache khác nhau.

5.2 KẾT QUẢ END-TO-END

Hình 3 chứng minh kết quả so sánh của SQUEEZE ATTENTION và 3 thuật toán baseline khác trên tất cả 7 mô hình và 5 tập dữ liệu. Full Cache (đường đứt nét) có nghĩa là tất cả các embedding KV của token được cache đầy đủ trong quá trình suy luận, do đó, nó đại diện cho độ chính xác mô hình tương đối tốt. Các đường cong màu xanh và cam trong mỗi hình phụ minh họa cách độ chính xác mô hình thay đổi với ngân sách KV-cache từ 10% đến 100% tổng độ dài chuỗi. Lưu ý rằng việc áp dụng các thuật toán nén theo chuỗi khác nhau cho các tác vụ khác nhau sẽ dẫn đến độ chính xác mô hình khá khác nhau. Do đó, đối với mỗi tác vụ, chúng tôi chọn thuật toán nén theo chuỗi tốt nhất để đại diện cho trường hợp tốt nhất, và sau đó áp dụng SQUEEZE ATTENTION trên cơ sở trường hợp tốt nhất. Như được hiển thị trong hình, SQUEEZE ATTENTION liên tục quản lý để cải thiện độ chính xác mô hình dưới các ngân sách KV-cache khác nhau bằng cách phân bổ lại ngân sách cache giữa các layer. Nói cách khác, SQUEEZE ATTENTION có thể đạt được độ chính xác suy luận tương tự với ít KV-cache hơn nhiều tổng cộng.

5.3 TIÊU THỤ BỘ NHỚ

Bây giờ chúng tôi đánh giá việc tiêu thụ bộ nhớ của thuật toán được đề xuất. Chúng tôi chọn ba cài đặt để so sánh lượng bộ nhớ GPU cần thiết để chạy suy luận mà không làm giảm hiệu suất mô hình

8

--- TRANG 9 ---
Bảng 2: So sánh ngân sách KV-cache cần thiết để đạt độ chính xác tốt nhất. Ba mô hình được chọn để đại diện cho các mô hình nhỏ (7B), trung bình (20B), và lớn (70B). Đối với mỗi tác vụ, chúng tôi chọn thuật toán thưa thớt theo chuỗi hiện có tốt nhất làm baseline.

Kích thước mô hình | Tập dữ liệu | Baseline tốt nhất | Hiệu suất / Ngân sách KV đã sử dụng
Full Cache | w/ SQUEEZE ATTENTION | w/o SQUEEZE ATTENTION
Mistral 7B | SAMSUM | Sliding Window | 43.53 / 100% | 41.05 / 20% | 40 / 30%
GPT-NeoX 20B | XSUM | Heavy-Hitter Oracle | 0.09 / 100% | 0.09 / 20% | 0.08 / 60%
LLama2 70B | XSUM | StreamingLLM | 0.18 / 100% | 0.17 / 30% | 0.19 / 40%

(a) LLama2-70B
(b) Mistral-7B
(c) GPT-NeoX-20B

Hình 4: So sánh việc sử dụng bộ nhớ decoding mỗi token giữa Full cache, SQUEEZE ATTENTION, và baseline tốt nhất để đạt được cùng độ chính xác, như được hiển thị trong Bảng 2.

độ chính xác. Chúng tôi chọn Mistral-7B (Sliding Window), GPT-NeoX-20B (Heavy-Hitter), và LLama2-70B (StreamingLLM) để bao phủ cả ba thuật toán baseline và các mô hình có kích thước nhỏ, trung bình và lớn. Chúng tôi sử dụng suy luận đa GPU nếu mô hình và KV-cache không vừa với một GPU đơn lẻ.

Bảng 2 chứng minh rằng trong cả ba cài đặt, bằng cách đạt được cùng độ chính xác mô hình, SQUEEZE ATTENTION tiêu thụ ít nhất lượng ngân sách KV-cache so với các thuật toán chỉ nén từ chiều chuỗi. Trong một số trường hợp, nó chỉ cần một phần ba ngân sách cache của thuật toán H2O. Tiếp theo, chúng tôi sử dụng PYTORCH PROFILER để đánh giá việc giảm sử dụng bộ nhớ khi sinh một token trong quá trình suy luận (không bao gồm sử dụng bộ nhớ của trọng số mô hình). Hình 4 cho thấy SQUEEZE ATTENTION có thể tiết kiệm 70% đến 80% sử dụng bộ nhớ mỗi token so với phương pháp Full Cache, và 25% đến 66% sử dụng bộ nhớ so với các thuật toán baseline.

5.4 THÔNG LƯỢNG SINH TOKEN

Vì SQUEEZE ATTENTION quản lý để tiết kiệm chi phí bộ nhớ của suy luận, như được hiển thị trong các mục trước, chúng tôi muốn khám phá cách những giảm bộ nhớ này có thể được diễn giải thành cải thiện thông lượng token. Chúng tôi chọn 2 mô hình, tức là Mistral-7B và Llama-70B, để đại diện cho các mô hình có kích thước nhỏ và lớn. Với độ dài nội dung cố định, chúng tôi tăng kích thước batch từ 1 đến 224 cho Mistral-7B, và 1 đến 64 cho Llama2. Đối với mỗi tác vụ, chúng tôi chọn thuật toán baseline tốt nhất, tức là Sliding Window cho Mistral-7B và StreamingLLM cho Llama2-70B.

Bảng 3 hiển thị thông lượng token trên 8 A100-40GB GPU. Với cùng kích thước batch, SQUEEZE ATTENTION có thể tăng cường thông lượng lên đến 2.2× cho Mistral-7B và 1.4× cho Llama2-70B so với Full Cache. Bên cạnh đó, SQUEEZE ATTENTION cũng cho phép kích thước batch lên đến 224 và 64 cho hai mô hình, điều này sẽ gây ra tình trạng hết bộ nhớ cho phương pháp Full Cache. So sánh thông lượng giữa baseline tốt nhất và SQUEEZE ATTENTION được báo cáo trong A.4.

5.5 CHI PHÍ PHỤ CỦA THUẬT TOÁN

Chi phí tính toán phụ của SQUEEZE ATTENTION đến từ hai phép toán: Độ tương tự Cosine và Kmeans. Việc thực hiện tất cả những tính toán này chỉ xảy ra trong giai đoạn prefilling.

9

--- TRANG 10 ---
Bảng 3: Thông lượng sinh (token/s) trên tám A100 GPU của Mistral-7B và LLama2-70B với SQUEEZE ATTENTION và Full Cache. Để duy trì độ chính xác mô hình, SQUEEZE ATTENTION sử dụng 20% ngân sách cache cho Mistral-7B và 30% ngân sách cache cho LLama2-70B. "OOM" có nghĩa là hết bộ nhớ.

Kích thước mô hình | prompt len + gen len | Thuật toán | Kích thước batch
1 | 32 | 64 | 128 | 224
Mistral 7B | 512 + 1024 | SQUEEZE ATTENTION | 20.5 | 496.5 | 682.7 | 824.4 | 892.5
Full Cache | 20.9 | 254.0 | 304.8 | OOM | OOM

Kích thước mô hình | prompt len + gen len | Thuật toán | Kích thước batch
1 | 8 | 16 | 32 | 64
LLama2 70B | 256 + 512 | SQUEEZE ATTENTION | 5.2 | 37.2 | 71.2 | 116.2 | 170.7
Full Cache | 5.2 | 36.0 | 62.5 | 84.8 | OOM

Do đó, chi phí của SQUEEZE ATTENTION là một giá phải trả một lần, hiệu quả về chi phí hơn nhiều so với những thuật toán (như H2O) đòi hỏi tính toán bổ sung trong mỗi lần lặp sinh token. Chúng tôi đã profile toàn bộ giai đoạn prefilling của Mistral-7B có/không có SQUEEZE ATTENTION để so sánh thời gian wall-clock. Độ dài prompt lên đến 8k token. Như được hiển thị trong Bảng 4, SQUEEZE ATTENTION chỉ gây ra 6.3% tăng thời gian prefilling. Lưu ý rằng nếu chúng ta tính đến thời gian decoding, tỷ lệ này sẽ nhỏ hơn nhiều, tùy thuộc vào số lượng token thực tế được sinh ra. Do đó, chi phí phụ của SQUEEZE ATTENTION về cơ bản có thể bỏ qua. Chi phí phụ chi tiết hơn của SQUEEZE ATTENTION có thể được tìm thấy trong A.1.

Bảng 4: Chi phí phụ của SQUEEZE ATTENTION (Thời gian Prefilling tính bằng giây trên một Nvidia A100-40GB)
Mô hình | w/o SQUEEZE ATTENTION | w/ SQUEEZE ATTENTION | Tỷ lệ chi phí phụ
Mistral-7B | 0.636 | 0.676 | 6.3%

5.6 HẠN CHẾ VÀ TÁC ĐỘNG RỘNG HÔN

SQUEEZE ATTENTION hoạt động cùng với một chính sách thưa thớt theo chuỗi, vì vậy giả định chúng tôi đưa ra là đối với một mô hình và tập dữ liệu cho trước, tồn tại một chính sách loại bỏ KV-cache theo chuỗi sẽ không làm tổn hại độ chính xác mô hình dưới một mức độ dung sai nhất định. Tuy nhiên, khả năng tổng quát của những thuật toán theo chuỗi này vẫn là một chủ đề nghiên cứu tích cực. Nếu giả định này không thể được đáp ứng, SQUEEZE ATTENTION có thể không hoạt động như mong đợi.

Các tác động xã hội tích cực bao gồm giảm chi phí năng lượng và phát thải carbon của suy luận LLM. Bên cạnh đó, nó có thể tối ưu hóa trải nghiệm người dùng của các ứng dụng LLM. Vì thuật toán được đề xuất có thể tăng tốc suy luận LLM, tác động tiêu cực tiềm tàng là nó có thể làm tăng việc sử dụng không phù hợp của LLM, như sinh thông tin có hại.

6 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất một thuật toán nén KV-cache 2D có tên SQUEEZE ATTENTION. Bằng cách theo dõi độ tương tự cosine của mỗi layer attention, chúng tôi phát hiện rằng các layer ở các vị trí khác nhau có các mức độ quan trọng khác biệt liên quan đến embedding đầu ra. Được truyền cảm hứng bởi quan sát này, SQUEEZE ATTENTION phân bổ lại ngân sách KV-cache trên các layer attention để giảm thêm chi phí bộ nhớ của suy luận. Các thí nghiệm trên một loạt rộng các mô hình và tác vụ cho thấy SQUEEZE ATTENTION có thể đạt được độ chính xác mô hình tốt hơn với tiêu thụ bộ nhớ thấp hơn so với các thuật toán tiên tiến chỉ nén KV-cache theo cách theo chuỗi.

10

--- TRANG 11 ---
TÀI LIỆU THAM KHẢO

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, và Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.

Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, và Thomas Hoffmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. arXiv preprint arXiv:2305.15805, 2023.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

Iz Beltagy, Matthew E Peters, và Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, và Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. arXiv preprint arXiv:2307.02628, 2023.

Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, và Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint of large language models. arXiv preprint arXiv:2309.14393, 2023.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, và Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, và Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, và Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024.

Jack W Rae và Ali Razavi. Do transformers need deep long-range memory. arXiv preprint arXiv:2007.03356, 2020.

Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, và Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pp. 31094–31116. PMLR, 2023.

Grigori Sidorov, Alexander Gelbukh, Helena Gómez-Adorno, và David Pinto. Soft similarity and soft cosine measure: Similarity of features in vector space model. Computación y Sistemas, 18(3):491–504, 2014.

Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, và Armand Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

11

--- TRANG 12 ---
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, và Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024.

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently programming large language models using sglang. arXiv preprint arXiv:2312.07104, 2023.

12

--- TRANG 13 ---
A PHỤ LỤC

A.1 CHI PHÍ PHỤ CHI TIẾT CỦA SQUEEZEATTENTION

Trong mục này, chúng tôi tiếp tục đánh giá chi phí phụ được giới thiệu bởi SQUEEZE ATTENTION, chúng tôi chia nhỏ thời gian thực hiện cho hai phép toán chính: tính toán độ tương tự cosine và gom cụm K-means. Thiết lập thí nghiệm tuân theo cùng thiết lập như 5.5. Thí nghiệm này được tiến hành sử dụng một Nvidia A100-40GB GPU đơn lẻ với độ dài prompt lên đến 8k token.

Bảng 5: chi phí phụ chi tiết
Độ tương tự cosine | K-means | Tổng thời gian
0.00068s | 0.001s | 0.02276s

Phép toán tính toán độ tương tự cosine liên quan đến việc tính toán giữa hai mảng kích thước 8000×4096, lặp lại 32 lần (vì Mistral có 32 layer). Ngoài ra, K-means gom cụm 32 số thành 3 lớp. Do đó, tổng thời gian tiêu thụ có thể được tính như 0.00068×32+0.001=0.02276 giây. Điều đáng chú ý là chi phí phụ này chỉ phát sinh một lần, bất kể số lượng token được xử lý.

A.2 CHỨC NĂNG CỦA p

Siêu tham số p rất quan trọng trong cơ chế SqueezeAttention, vì nó trực tiếp quyết định việc phân bổ cuối cùng của KV cache. Để minh họa điều này bằng một ví dụ cụ thể: giả sử chúng ta có một mô hình với 32 layer, trong đó 18 layer được coi là quan trọng và 14 layer còn lại được coi là ít quan trọng hơn. Mỗi layer ban đầu có ngân sách 1000 token. Nếu chúng ta đặt p thành 0.3, chúng ta sẽ lấy 70% ngân sách từ các layer ít quan trọng và phân phối lại đều cho các layer quan trọng. Cụ thể, ngân sách cho các layer ít quan trọng sẽ được giảm xuống 1000×0.3=300. Trong khi đó, ngân sách cho các layer quan trọng sẽ được tính như sau: (1000×18+1000×0.7×14)/18=1544. Theo cách này, tổng ngân sách vẫn không thay đổi.

Để đánh giá độ nhạy cảm của siêu tham số p, chúng tôi đã thử nghiệm mô hình Mistral-7B trên tập dữ liệu Samsum với các giá trị p từ 0.1 đến 1.0. Tổng ngân sách KV được đặt ở 20% độ dài prompt. Kết quả được trình bày trong bảng dưới đây:

Bảng 6: Độ chính xác thay đổi với p
ROUGE-L | 15.7 | 34.01 | 37.26 | 37.69 | 27.41 | 26.29 | 11.11 | 10.48 | 8.72 | 9.07
p | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0

100% có nghĩa là chúng tôi không thay đổi cấu trúc KV cache của mô hình. Khi p giảm, nhiều ngân sách KV hơn sẽ được chuyển cho các layer khác, có thể mang lại kết quả tốt hơn đến một điểm nhất định. Khi p giảm xuống 10% hoặc thậm chí thấp hơn, hiệu suất giảm vì ngân sách KV của các layer ít quan trọng trở nên nghiêm trọng không đủ. Chúng ta có thể quan sát rõ ràng sự cải thiện độ chính xác khi điều chỉnh p trong khi giữ ngân sách tổng thể không đổi. Điều này chứng minh tác động của p lên hiệu suất mô hình, nhấn mạnh tầm quan trọng của nó trong việc tối ưu hóa phân bổ ngân sách KV.

A.3 TẦM QUAN TRỌNG LAYER TRÊN CÁC TÁC VỤ KHÁC NHAU

Chúng tôi đã tiến hành một thí nghiệm bổ sung sử dụng hai mô hình và các tập dữ liệu khác nhau để xác định liệu tầm quan trọng của các layer khác nhau có phải là một tính chất nội tại của mô hình hay không.

13

--- TRANG 14 ---
Bảng 7: Mistral-7B
Tập dữ liệu | Samsum | TriviaQA | LCC
Quan trọng | 17 | 18 | 19
Không quan trọng | 15 | 14 | 13

Bảng 8: LLama2-70B
Tập dữ liệu | Xsum | Samsum | LCC
Quan trọng | 17 | 21 | 18
Không quan trọng | 63 | 59 | 62

Bảng 7 hiển thị phân phối các layer quan trọng cho mô hình Mistral trên ba tập dữ liệu khác nhau: Samsum (Few shot), TriviaQA (Single-document QA), và LCC (Code, Python/C#/Java) và bảng 8 hiển thị phân phối các layer quan trọng cho mô hình LLama2-70B trên ba tập dữ liệu khác nhau: Xsum (Summarization), Samsum (Few shot), và LCC (Code, Python/C#/Java).

Từ những bảng này, chúng ta có thể quan sát rằng có một mô hình thô với những biến động cụ thể theo tác vụ liên quan đến nhóm layer. Chúng tôi tin rằng tồn tại một số layer nhạy cảm với tác vụ có thể được phân loại vào các nhóm khác nhau với các tác vụ khác nhau. Tương tự, cũng có một số layer luôn quan trọng/không quan trọng. Một phân tích chi tiết về hiện tượng này có thể là một mở rộng thú vị của công trình này. Tuy nhiên, chúng tôi vẫn khuyến nghị một cách thích ứng vì nó có thể nắm bắt chính xác tầm quan trọng của các layer.

A.4 SO SÁNH THÔNG LƯỢNG GIỮA CÁC THUẬT TOÁN BASELINE TỐT NHẤT VÀ SQUEEZE ATTENTION

Bảng 9: Thông lượng sinh (token/s) trên tám A100 GPU của Mistral-7B và LLama2-7B với SQUEEZE ATTENTION và các thuật toán baseline tốt nhất. Để duy trì độ chính xác mô hình, SQUEEZE ATTENTION sử dụng 20% ngân sách cache cho Mistral-7B và 40% ngân sách cache cho LLama2-7B trong khi Sliding Window sử dụng 30% ngân sách cache cho Mistral-7B và StreamingLLM sử dụng 60% ngân sách cache cho LLama2-7B. "OOM" có nghĩa là hết bộ nhớ.

Kích thước mô hình | prompt len + gen len | Thuật toán | Kích thước batch
1 | 32 | 64 | 128 | 224
Mistral 7B | 512 + 1024 | SQUEEZE ATTENTION | 20.5 | 496.5 | 682.7 | 824.4 | 892.5
Sliding Window | 20.6 | 404.5 | 512.2 | 587.8 | OOM

Kích thước mô hình | prompt len + gen len | Thuật toán | Kích thước batch
1 | 32 | 64 | 128 | 256
LLama2 7B | 512 + 1024 | SQUEEZE ATTENTION | 20.0 | 143.0 | 150.4 | 144.9 | OOM
StreamLLM | 20.4 | 113.7 | 102.4 | OOM | OOM

Chúng tôi cũng đã tiến hành các thí nghiệm để so sánh thông lượng của SQUEEZE ATTENTION với baseline tốt nhất dưới một tập hợp các kích thước batch. Cả hai thí nghiệm đều sử dụng độ dài đầu vào 512 và độ dài đầu ra 1024. Chúng tôi chọn các siêu tham số nén cho mỗi thuật toán sao cho tất cả đều có thể đạt được độ chính xác mô hình tốt nhất. Kết quả cho thấy thuật toán của chúng tôi có thể tăng thông lượng rõ ràng so với những thuật toán SOTA chỉ nén KV-cache từ chiều chuỗi.

14
