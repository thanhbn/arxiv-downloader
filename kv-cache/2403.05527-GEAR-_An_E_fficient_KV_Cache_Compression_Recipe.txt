# 2403.05527.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/kv-cache/2403.05527.pdf
# File size: 5189345 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GEAR: An E fficient KV Cache Compression Recipe
for Near-Lossless Generative Inference of LLM
Hao Kang∗, Qingru Zhang∗, Souvik Kundu, Geonhwa Jeong,
Zaoxing Liu, Tushar Krishna, Tuo Zhao†
October 2, 2024
Abstract
Key-value (KV) caching has become the de-facto technique to accelerate generation speed
for large language models (LLMs) inference. However, the growing cache demand with increasing
sequence length has transformed LLM inference to be a memory bound problem, significantly
constraining the system throughput. Existing methods rely on dropping unimportant tokens or
quantizing entries group-wise. Such methods, however, often incur high approximation errors
to represent the compressed matrices. The autoregressive decoding process further compounds
the error of each step, resulting in critical deviation in model generation and deterioration of
performance. To tackle this challenge, we propose GEAR, an e fficient error reduction framework
that augments a quantization scheme with two error reduction components and achieves near-
lossless performance at high compression ratios. GEAR first applies quantization to majority
of entries of similar magnitudes to ultra-low precision. It then employs a low-rank matrix
to approximate the quantization error, and a sparse matrix to remedy individual errors from
outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their
synergistic potentials. Our experiments show that GEAR can maintain similar accuracy to that
of FP16 cache with improvement up to 24.42% over the SOTA baselines at 2-bit compression.
Additionally, compared to LLM inference with FP16 KV cache, GEAR can reduce peak-memory of
up to 2.39×, bringing 2.1×∼5.07×throughput improvement. Our code is publicly available at.
https://github.com/HaoKang-Timmy/GEAR .
1 Introduction
Autoregressive large language models (LLMs) (Brown et al., 2020b; Zhang et al., 2022; Touvron
et al., 2023a,b) have marked a significant milestone in natural language processing (NLP) and
†Hao Kang, Qingru Zhang, Geonhwa Jeong, Tushar Krishna, and Tuo Zhao are a ffiliated with Georgia Tech. Souvik
Kundu is a ffiliated with Intel. Zaoxing Liu is a ffiliated with the University of Maryland. Correspondence to hkang342@
gatech.edu ,qingru.zhang@gatech.edu ,souvikk.kundu@intel.com , and tourzhao@gatech.edu .
*Equal contributions
1arXiv:2403.05527v4  [cs.LG]  30 Sep 2024

--- PAGE 2 ---
(a) Approx. error on GSM8k-CoT
 (b) Difference in prediction logits
 (c) Accuracy on GSM8k-CoT
Figure 1: (1a) compares the approximation error when compressing KV caches to 2-bit for LLaMA3-8B
on GSM8k (w. CoT). (1b) presents di fference in prediction logits from FP16 baseline after compressing KV
caches of an GSM8k (w. CoT) example, indicating the approximation error can be severely compounded
along steps and critically divert model generations. (1c) shows reducing the error can significantly improve
the performance.
artificial intelligence (AI) (Vaswani et al., 2017; Brown et al., 2020a; OpenAI, 2023), showcasing
exceptional performances across a wide range of applications, such as content creation and dialogue
systems (Yuan et al., 2022; Thoppilan et al., 2022; Wei et al., 2022). When serving these LLMs for
generative inference, KV cache -ing has become a routine practice. It stores previously computed
Key/Value tensors from attention calculation and reuses them while generating next tokens (Pope
et al., 2022), avoiding intensive recalculation to improve the generation speed.
Despite its prominence, the memory consumption of the KV cache grows rapidly with the
model size and sequence length, imposing significant constraints on system throughput. For
instance, in the case of a 30 billion-parameter LLM with an input length of 1024 and batch size of
128, the resulting KV cache can occupy up to 180 GB of memory (Zhang et al., 2023b). To alleviate
this pressure on limited GPU memory capacity, inference systems typically resort to offloading
(Aminabadi et al., 2022; Sheng et al., 2023) – transferring the KV cache to CPU memory or NVMe
storage. This, however, can still introduce non-trivial overhead due to the limited PCIe bandwidth
between GPUs and CPUs on many devices. Therefore, it is crucial to reduce the intensive memory
footprint of the emerging bottleneck of KV cache in generative inference.
To address this issue, token dropping methods have been proposed to compress the cache size
while maintaining the generative performance (Zhang et al., 2023b; Liu et al., 2023; Ge et al.,
2023). These approaches harness the sparsity observed in attention scores to evict embeddings of
less important tokens from the KV cache while retaining frequently attended ones. For example,
H2O (Zhang et al., 2023b) utilizes accumulated attention scores to evaluate token importance
and reduces cache size by dropping tokens with lower scores. In addition, quantization is another
widely-adopted compression scheme that maps full-precision tensor values into discrete levels
and store them at lower precision, e.g., INT4 or INT8 (Zafrir et al., 2019; Dettmers et al., 2022;
Sheng et al., 2023). For example, FlexGen (Sheng et al., 2023) employs a fine-grained group-wise
asymmetric quantization that groups KV entries per-token, divides gcontiguous entries as a group,
2

--- PAGE 3 ---
and quantize the tensor group-wise. Two concurrent works (Liu et al., 2024; Hooper et al., 2024)
further study KV entry distribution and propose to quantize Key cache per-channel and quantize
Value cache per-token, compressing the cache size by a high ratio.
The existing methods can e ffectively compress the cache size to low-precision while achieving
near-lossless performance on natural language understanding tasks like multiple-choice QA, text
classification, or simple summarization task (Zhang et al., 2023b; Liu et al., 2024). However, a stark
contrast emerges when applying these methods to complex generative tasks that require models to
generate longer responses or involve reasoning, such as mathematical problem-solving (Cobbe et al.,
2021) and chain-of-thought (CoT) reasoning (Wei et al., 2023). Their performance deteriorates
under a high compression ratio*(e.g., 4-bit/2-bit quantization or dropping >50% tokens (Ge et al.,
2023)), which is noticeable in both types of methods*. This phenomenon can be attributed to
the non-trivial approximation error induced by them, i.e., di fference between original KV and
the compressed ones. For simple tasks, models are required to generate only few tokens where
necessary information for correct prediction can often be derived from a small set of important
contextual tokens. Consequently, a relatively large approximation error does not significantly
hinder the generation of target tokens. In contrast, the complex tasks require models to generate
longer sequences conditioned on prompts that often contains densely correlated information
(e.g., CoT reasoning). The autoregressive decoding can compound the approximation error at every
step. Consequently, the negative e ffect of even a relatively small error can be magnified along
generation steps, adversely a ffecting subsequent generation. As an example, Figure 1 presents
the approximation error of various methods on GSM8k and illustrates the deviation in token
generations due to the accumulated error, which degrades the accuracy a lot. Therefore, the crux of
the issue lies in high approximation errors of these methods, especially under high compression
ratios.
To address this challenge, we propose GEAR (GEnerative Inference with Approximation Error
Reduction), an e fficient error reduction framework that augments existing KV cache quantization
schemes with two error-reduction techniques, and adeptly integrate them to exploit their full
potentials. Generally speaking, our framework consists of three components to decompose KV
matrices: (i) First, we apply an existing quantization method to efficiently quantize the majority
(e.g., 98%) of entries of similar magnitudes to ultra-low precision. (ii) Then, we introduce a low-rank
matrix to efficiently approximate the quantization residuals. (iii) Finally, we employ a sparse matrix
consisting of a negligible ratio of entries of large magnitudes to remedy the individual errors caused
by these outliers. Such a composite approximation decouples the coherent parts from incoherent
parts of the approximation error: the low-rank matrix captures the majority of coherent basis of
quantization error while the sparse matrix rectifies the incoherency existing in individual outliers.
Meanwhile, as shown by our empirical evidence in Section 4.2, these two lightweight components
*We define the compression ratio as tensor size in FP16 divided by that in compressed format.
*Please refer to Section 4 for our empirical evidence.
3

--- PAGE 4 ---
result in negligible memory and computational overheads, demonstrating high e fficiency. As
such, GEAR can e ffectively reduce the approximation error in a highly e fficient way and achieve
superior performance on both complex and relatively simple tasks at high compression ratios in a
plug-and-play manner. We find that using both sparse and low-rank components is necessary for
GEAR to establish the best performance, highlighting their complementary nature. Remarkably,
for those prioritizing e fficiency, equipping low-rank approximation alone for quantization can still
effectively reduce the approximation error, yielding both significant e fficiency and performance
improvement. We refer to this lite version of GEAR as GEAR-L .
Additionally, we incorporate a streaming bu fferstrategy for GEAR to further improve inference
efficiency. Specifically, when generating long sequences, we store KV vectors of newly generated
tokens to a small bu ffer (e.g., bu ffer sizenb= 20 ). When the bu ffer reaches its capacity, GEAR
conducts the KV cache compression every nbsteps. As such, the inference speed can be significantly
improved at a trivial cost of additional memory. Furthermore, to minimize the overhead, we
demonstrate a GPU-friendly kernel implementation for GEAR, which leverage the streaming and
quantization benefit to improve inference throughputs significantly.
We conduct experiments on diverse tasks and models to demonstrate the e ffectiveness of
GEAR. Specifically, we evaluate compression performance with LLaMA2-7B/13B (Touvron et al.,
2023b), Mistral-7B (Jiang et al., 2023), and LLaMA3-8B(Meta, 2024) on generative tasks including
mathematical reasoning (GSM8kCobbe et al. (2021) and AQuA(Ling et al., 2017)), symbolic
reasoning (BigBench HardSuzgun et al. (2022)), and long-context understanding (LongBenchBai
et al. (2023)). We show that GEAR consistently outperforms the baseline methods especially at
high compression ratios such as 2-bit precision. For example, when compressing KV caches to 2-bit,
GEAR achieves an remarkable average accuracy improvement of 14.95 % over the best-performing
baseline across various models and datasets. Regarding the inference e fficiency, compared to the
FP16 baseline, GEAR can reduce the peak memory up to 2.39×, bring 2.10×∼5.07×throughput
improvement.
2 Background
Multi-head attention . A typical transformer model consists of Lstacked layers, where each layer
contains two submodules: a multi-head attention (MHA) and a feed-forward network (FFN). Given
the input token embeddings as X∈Rn×d, MHA performs attention function in parallel Hheads:
MHA (X)= Concat( H(1),...,H(H))Wo,H(i)= Softmax
Q(i)K(i)⊤/p
dH
V(i)(1)
where Q(i)=XWqi,K(i)=XWki,V(i)=XWviare Query/Key/Value matrices, and Wqi,Wki,Wvi∈
Rd×dHare projection matrices of head i.dHis typically set to d/H.
Prefill and decoding . Suppose the model generates ngtokens. At the first generation step,
the input tokens X0∈Rn×dare prefilled. Then K(i)andV(i)at every head and every layer
4

--- PAGE 5 ---
are cached for subsequent generation, resulting in initial KV caches of prefill phrase: K0=
Concat (K(1),...,K(H)),V0=Concat (V(1),...,V(H))andK0,V0∈Rn×d. At each step t(1≤t≤ng) of
autoregressive decoding, the model predicts a new token xtconditioned on the input and previously
generated tokens. At the following step, MHA only needs to compute the Query/Key/Value vectors*
(qt,kt,vt∈Rd) for the newly generated token xtand appends kt,vtto the KV cache: Kt=Kt−1∥kt,
Vt=Vt−1∥vt. Then it performs the attention (1) between qtandKt,Vtonly.
Group-wise Quantization . Group-wise quantization is widely applied to compress KV cacheSheng
et al. (2023); Liu et al. (2024); Hooper et al. (2024). Given a tensor X∈Rn×din full precision, the
vanilla method groups entries per-token by placing gconsecutive entries of a token into one group,
e.g., thei-th group XGicontains entries with indices Gi={(ti,ci),...,(ti,ci+g)}where (ti,ci)is the be-
ginning index of group iandgis group size. Then, it quantizes Xgroup-wise: bX=Quant(per-token)
b,g
with
Quant(per-token)
b,g(X)Gi=l
(XGi−minXGi)/∆ik
,∆i= (max XGi−minXGi)/(2b−1) (2)
wherebis the bit-width, bXis the quantized tensor in b-bit precision, and ⌈·⌋is the rounding
function. ∆iand minXGiare the scaling factor and zero-point of group i. Two concurrent works
(KIVI Liu et al. (2024) and KVQuant Hooper et al. (2024)) explore the entry distribution of KV cache
and show that, in Key cache, some fixed channels exhibit very large magnitudes. To confine the
quantization error to each individual channel without impacting others, they propose to quantize
Key cache per-channel while quantizing Value cache per-token, achieving the start-of-the-art 2-bit
compression.
Intuitively, more fine-grained grouping with smaller group size, such as g= 64 in KIVI Liu
et al. (2024), leads to more accurate approximation and yields better performance. However, small
group size induces considerable memory overheads due to the increased number of scaling factors
and zero-points stored in full precision for each group. Meanwhile, fine-grained grouping for
per-channel quantization leads to maintaining a residual subset of KV tokens in full precision until
they form a complete group Liu et al. (2024). Hence, the residual length of this full-precision parts
should be set as a multiple of group size (e.g., 128 as set by KIVI), further resulting in additional
considerable overheads. To leverage the SOTA quantization scheme while minimizing overheads,
we choose per-channel Key and per-token Value quantization without fine-grained grouping as a
lite quantization backbone. We refer to it as KCVT , a variant of KIVI with coarse-grained per-vector
grouping where all Key entries of one channel forms a group of size nand all Value entries of one
token forms a group of size d, significantly reducing the scaling and zero point storage overhead.
5

--- PAGE 6 ---
(a) Error of each method
 (b) Spectrum of the residual
 (c) LLaMA3-8B on GSM8k-CoT
Figure 2: (2a, 2b) We randomly sample a GSM8k example and analyze its KV caches by LLaMA2-7B. (2a):
the minimal approximation error of each individual technique when approximating the Value cache of the
first layer; (2b): spectrum of the residual Rhdecays rapidly. (2c): As an e fficient error-reduction framework,
GEAR is orthogonal to any o ff-the-shelf quantization and can augment them to achieve near-lossless accuracy.
3 GEAR Framework
The GEAR framework consists of three important components to decompose and compress a KV
cache matrix: (i) a quantized matrix bDto serve as a compressed backbone; (ii) a low-rank matrix L
to approximate the quantization residual; (iii) a sparse matrix Sto capture the individual outliers.
As discussed in Section 1, the approximation error plays a pivotal role in determining model
performance. Therefore, given a tensor X∈{Kt,Vt}, our objective is to minimize the error of
approximating Xwith its compressed counterpart. A simple strategy is to employ each of three
compression methods individually and approximate Xby minimizing the distance to it. For
instance, constructing Lusing the top singular values/vectors of Xor composing Swith the
entries of largest magnitudes. However, as demonstrated by Figure 2a, solely relying on any of
these three methods cannot achieve high compression ratios because they all result in substantially
increased error under high compression ratios. Additionally, bD,L,Scan function di fferently in
the matrix approximation, capturing di fferent components of X. These motivations encourage
us to explore the integration of three techniques to leverage their individual advantages while
exploiting their synergistic potential. To achieve this, our goal becomes minimizing the following
approximation error:
min
bD,L,SX−bD−L−SF. (3)
One interesting idea to minimize (3) is alternating among quantization, singular-value decomposi-
tion (SVD) and outlier extraction, and iteratively updating three matrices bD,L,Suntil achieving
minimal error. This idea has been introduced by Li et al. (2023) to optimize a similar objective for
an accurate initialization of weight quantization. However, the inference system has demanding
speed requirements. The significant latency caused by these iterative updates is unacceptable for
*For simplicity, we concatenate multi-head embeddings here.
6

--- PAGE 7 ---
generative inference. Therefore, we propose an e fficient solution to minimize the approximation
error (3).
Outlier-aware quantization . Inspired by the recent study on weight quantization (Kim et al.,
2023), we observe that the quantized backbone bDand the sparse matrix Scomplement each
other in the KV cache compression. Specifically, the quantization scheme can result in non-
trivial quantization errors within each group due to the existence of outlier entries. Therefore, a
straightforward strategy is to filter out these outlier before quantization. To align with grouping
of per-channel Key and per-token Value quantization, we leverage a per-vector outlier filtering.
Given an input tensor X=Kt(orVt), we extract boths
2%of maximum and minimum entries of
each channel (or token) vector and store them in full precision with a sparse matrix S=Filters(X)
where
Filters(X)ij=XijifX=KtandXijin top/bottoms
2% of thej-th channel X·j,
Xij ifX=VtandXijin top/bottoms
2% of thei-th token Xi·,
0 otherwise .(4)
Then, we perform the quantization on the extracted matrix and obtain the quantized backbone:
bD= Quant(Selected scheme)
b(X−S). (5)
The outlier extraction technique has been applied by Kim et al. (2023) to augment training-
dependent non-uniform weight quantization. In contrast to their application scenario, we explore
the potential of outlier extraction techniques in conjunction with tuning-free uniform quantization
for KV caches. It is important to note that, compared to weights, KV cache compression presents
unique challenges because KV caches can contain more outliers, making its accurate quantization
more challenging than weights (Xiao et al., 2023). Our empirical results in Section 4.3 also show
that the outlier-aware quantization faces challenges in achieving ultra-low precision compression
such as 2-bit on complex generative tasks. To achieve e ffective high-ratio compression, it is often
necessary to extract a large portion of outliers stored in a sparse matrix. However, representing such
a sparse matrix with two index vectors and one value vector in full precision results in considerable
memory overheads. These suggest that, while using Scan reduce the error, it still falls short of
fully remediating the error in an e fficient way.
Low-rank approximation . To reduce the approximation error more e fficiently, we resort to
low-rank approximation . As inspired by fact that various attention heads encode diverse contextual
information within di fferent channel ranges Tenney et al. (2019); Voita et al. (2019); Zhang et al.
(2024), we propose to apply head-wise low-rank decomposition on the residual R=X−(bD+S)∈Rn×d.
Specifically, we first reshape Ralong channel dimension and obtain Hmulti-head sub-matrices
{Rh=R[:,(h−1)dH:hdH]∈Rn×dH|1≤h≤H}where Rhis the residual of head h. Suppose Rh
has singular value decomposition asPk
i=1σiuim⊤
i, whereσ1≥···≥σkare singular values and
ui,miare the corresponding singular vectors. As shown in Figure 2b, we empirically observe that
the spectrum of residual matrices drops rapidly at the beginning. This suggests the existence of
7

--- PAGE 8 ---
a coherent component within the residual. This component is represented by the top singular
values/vectors, and shared among tokens, indicating the vector similarity. By these top singular
values and vectors, we can e fficiently capture and recover this coherent information, leading
to an e ffective approximation to the quantization residual. To this end, we introduce a matrix
L= Concat( L1,...,LH), where Lhis a low-rank matrix:
Lh=AhB⊤
h= SVDSolver r(Rh) (6)
Ah∈Rn×r,Bh∈RdH×randris much smaller than n,dH. For example, when n= 1024 anddH= 128 ,
r= 4is sufficient to achieve near-lossless high-ratio compression. For SVDSolver (·), we employ an
efficient power iteration algorithm Vogels et al. (2019). This algorithm calculates Ah,Bhrapidly
while ensuring that AhB⊤
haccurately approximates the top- rsingular values/vectorsPr
i=1σiuim⊤
i
(please see Appendix 8 for the algorithm details). In the multi-batch setting, we apply low-rank
approximation to input tensors batch-wise and head-wise.
In summary, GEAR integrates three compression techniques to provide an e fficient solution for
minimizing the approximation error in (3). Specifically, the quantized backbone bDleverages the
entry-wise similarity and compresses the majority of entries to the ultra-low precision. The low-
rank matrix Lhcapitalizes on vector-wise similarity to extract the commonly shared information
within the residuals. The sparse matrix Scompensates for the extraction of sparse information
existing in individual outliers and compliments the quantization process. As such, GEAR e ffectively
reduces the approximation error, achieving high-ratio KV cache compression. We recommend
to use GEAR with all three components for the best performance – both near-lossless 4-bit and
2-bit performance as an alternate to SOTA methods. However, to prioritize e fficiency, one can
resort to a lite version of GEAR, namely GEAR-L , that equips only low-rank approximation to
restore quantization error, costing less memory-overhead while improving accuracy significantly.
Finally, we highlight that, as an e fficient error-reduction framework, GEAR(-L) is orthogonal to
any off-the-shelf quantization scheme and can augment them to achieve near-lossless accuracy as
shown in Figure 2c and Section 4.
Streaming Bu ffer.GEAR also introduces a streaming bu fferstrategy during decoding to sig-
nificantly boost its inference speed. Specifically, when serving the long-sequence generation,
GEAR stores KV vectors of newly generated tokens in full precision to a small bu fferBof sizenb
(e.g.,nb= 20 ). When the bu ffer reaches its capacity every nbdecoding steps, GEAR conduct the
compression for new tokens in Bwhile the subsequent low-rank approximation is only performed
on the new tokens. The concurrent work, KIVI (Liu et al., 2024), introduces a similar bu ffering
approach to cache residual tokens until they complete a group. Hence, their residual bu ffer size
should be set as a multiple of group size. In the case of coarse-grained grouping of KCVT, the
buffer size can be set arbitrarily and we select a small size like nb= 20 to enhance the inference
speed while avoiding the non-trivial memory overheads. We summarize the detailed algorithm of
GEAR in Algorithm 1 of Appendix 7.
8

--- PAGE 9 ---
Table 1: Results on CoT reasoning tasks, which are hard generative task. Here, KV Size is the
average %of the remaining size of compressed KV caches with respect to the size in FP16. The best
results are shown in bold .N.A. represents the extermely degenerated performance.
Model LLaMA3-8B LLaMA2-13B Mistral-7B All
MethodBit Ave. GSM8k AQuA BBH GSM8k AQuA BBH GSM8k AQuA BBH Ave.
bKV size Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc
FP16 16 100% 54.21 38.19 53.66 30.34 21.65 40.79 42.84 35.04 47.92 40.52
Per-token Q. g= 64 434.2% 37.07 39.37 46.42 20.85 18.90 34.72 31.47 29.13 28.88 31.94
KCVT Quant 427.1% 45.59 36.61 51.67 21.14 21.05 36.71 30.31 24.37 46.86 34.92
KIVIg= 64,nb= 64 434.2% 46.25 36.22 48.03 22.14 21.65 37.76 32.83 25.98 44.56 35.05
GEAR-L(KCVT)
r=4429.0% 53.44 38.98 52.23 30.25 23.23 38.52 43.06 33.07 47.42 40.02
GEAR(KCVT)
s=2%,r=4431.0% 54.76 40.55 52.74 30.17 24.05 40.63 41.93 34.57 47.84 40.80
Per-token Q. g= 64 221.7% 3.56 9.84 4.72 N.A. 10.54 N.A. N.A. 11.42 5.93 7.67
KIVIg= 64,nb= 64 221.7% 30.17 25.36 30.92 16.60 17.72 29.43 23.35 22.44 31.28 25.25
GEAR-L(KIVI)
r=4223.6% 52.62 38.19 51.44 26.61 20.87 39.44 39.27 29.92 46.36 38.34
GEAR(KIVI,g=64)
s=2%,r=4227.6% 54.59 38.19 50.30 30.27 23.62 39.67 43.14 33.96 48.03 40.20
4 Experiments
We use GEAR as a plug-and-play KV cache compression for generative inference with various LLM
models (including LLaMA2-7B/13B Touvron et al. (2023b), Mistral-7B Jiang et al. (2023) and
LLaMA3-8B Meta (2024)) on generative tasks including math reasoning (GSM8k (Cobbe et al.,
2021) and AQuA (Ling et al., 2017)), symbolic reasoning (BigBench Hard (BBH) (Suzgun et al.,
2022)) with CoT prompting Wei et al. (2023), and long-context understanding (LongBench (Bai
et al., 2023)).
Implementation optimization and details. To minimize the overheads, we demonstrate via
GPU kernel support and optimize the implementation for GEAR as follows. Firstly, we fuse the
dequantization with matrix multiplication using CUDA to improve decoding latency. Secondly, we
integrate the streaming bu ffer for both the Key and Value such that newly generated Key/Value
caches are all compressed every nbsteps. Moreover, due to streaming bu ffer during decoding,
low-rank approximation is performed every nbsteps for only bu ffered tokens with ultra low rank
(r= 2), improving compression e fficiency. Thirdly, we preform the forward pass of low-rank
matrices on a separate path where down projection (e.g., q⊤
hBh) is first computed, followed by up
projection (e.g., ( q⊤
hBh)A⊤
h), reducing computational complexity of their forward pass.
We apply GEAR and baseline methods to open-source pre-trained LLMs available at Huggingface
Wolf et al. (2019), using our inference framework written in PyTorch Paszke et al. (2019). As we
focus on evaluating the impact of KV Cache compression, we keep all other tensors in FP16, unless
otherwise stated. We focus on ultra-low precision quantization and report the results of 4-bit and
2-bit quantization. For GEAR, we fix the sparsity ratio sat 2%, set the rank rto 4 for inputs in
9

--- PAGE 10 ---
prefill phase, and set the rank to 2 for each group of nbnew tokens in decoding phase. We find
that the e fficient KCVT quantization achieves e ffective 4-bit compression and hence leverage it as
4-bit quantization backbone for GEAR due to its e fficiency. However, in case of 2-bit compression,
its performance degenerates a lot and the quantization schemes have to resort to fine-grained
grouping to establish acceptable accuracy. Hence, we use KIVI as 2-bit quantization backbone for
GEAR. As demonstrated by Liu et al. (2024) that KIVI is not sensitive to group size gand residual
lengthnb(Table 5 in (Liu et al., 2024)), we thus select the group size as 64 and the residual length
as 64 for both GEAR and KIVI in order to lower KV size overheads. The superscript in bracket
shown in Table 1 and 2 identifies the backbone quantization scheme.
Baselines . We compare GEAR with the following baseline methods:
•Per-token group-wise quantization (used in FlexGen (Sheng et al., 2023)) is a widely-adopted
method that quantizes KV cache per-token with fine-grained grouping.
•KIVI (Liu et al., 2024) is a concurrent KV cache quantization method that achieves start-of-
the-art 2-bit KV cache compression. This method quantizes Key cache per-channel and quantizes
Value cache per-token with fine-grained grouping, and stored residual tokens of length nbin full
precision.
•KCVT quantization is a variant of KIVI that quantize Key cache per-channel and Value cache
per-token without fine-grained grouping. This is a per-vector quantization that induces lower
overheads.
•H2O(Zhang et al., 2023b) is a recent token dropping method evicting unimportant tokens
with lower accumulated attention scores, which we compare with in Table 10.
Recent concurrent work KVQuant (Hooper et al., 2024) explored KCVT to incorporate it with
data-dependent non-uniform quantization. While it demonstrates near-lossless performance
mainly on perplexity with WikiText2 and C4, it requires additional calibration that minimizes
a Hessian-related objective driven by data samples to obtain the quantization signposts. GEAR,
however, aims to be a plug-and-play method, that can be deployed with any inference quantization
scheme without any such dependency. Thus we keep any calibration-dependent compression
beyond the scope of this work.
4.1 Main Results
Generative performance on hard CoT reasoning tasks . We compare di fferent methods with
LLaMA3-8B, LLaMA2-13B, and Mistral-7B on three challenging CoT generative tasks: GSM8k,
AQuA, and BBH with 8-shot CoT demonstrations. GSM8k (Cobbe et al., 2021) and AQuA (Ling
et al., 2017) are widely used math reasoning datasets that test models’ ability of arithmetic
reasoning. BBH (Suzgun et al., 2022) is a suite of language and symbolic reasoning problems
consisting of 6.5k problems within 23 subsets. Given the complexity of these tasks, we use the
chain-of-thought prompts created by (Fu et al., 2023) to improve reasoning, which contains 8-shot
demonstrations of multi-step reasoning. With the CoT demonstrations, we have the average prefill
10

--- PAGE 11 ---
Table 2: Results on GSM8k 5-shot and LongBench evaluation. Here, KV Size is the average %
of the remaining size of compressed KV caches with respect to that in FP16 (i.e., the inverse of
compression ratio). The best results are shown in bold . Results marked as †are taken from other
papers.
Dataset GSM8k 5-shot LongBench w. LLaMA2-7B
MethodBit Ave. KV 7B 8B QMSum SAMSum GovReport 21 Tasks Ave.
b size Acc Acc Rouge Rouge Rouge Ave. KV Ave. score
FP16 16 100% 13.50 49.89 21.28 43.57 26.06 100% 26.82
Per-token Q. g= 64 4 38.2% 10.54 45.64 20.91 39.15 28.50 31.6% 27.31
KCVT Quant 4 27.1% 12.51 43.14 20.91 33.89 24.32 25.7% 26.06
KIVIg= 64,nb= 64 4 38.2% 13.41 48.37 20.81 40.98 26.86 31.6% 27.58
GEAR-L(KCVT)
r=44 30.4% 12.51 47.23 21.18 41.39 26.93 27.3% 27.65
GEAR(KCVT)
s=2%,r=44 32.4% 13.19 49.43 21.28 41.32 26.97 29.3% 27.80
Per-token Q. g= 64 2 25.7% 0.08 0.83 19.78 40.31 25.50 17.5% 27.69
KIVIg= 32,nb= 128 2 34.9% 12.74†42.54 20.50†42.71†25.72 19.7% 27.83
GEAR-L(KIVI,g=64)
r=42 27.5% 12.63 47.01 20.69 42.35 26.67 19.1% 27.90
GEAR(KIVI,g=64)
s=2%,r=42 31.5% 13.04 49.96 20.59 43.22 27.73 23.1% 25.48
length of GSM8k, AQuA, and BBH as 900, 1304, 1021 respectively (see Appendix 10). We then
prompt model to generate 256 tokens and extract answers from them. Therefore, our experiments
involve long-sequence generation. Notably, as mentioned in Section 1, CoT prompts often contains
densely correlated information across multiple reasoning steps and models need to pay close
attention across steps to derive answers correctly. Hence, a relatively small compression error can
be magnified along generation steps, resulting in significant deviation in model generations.
Table 1 presents experimental results on these hard CoT reasoning tasks. We see that GEAR and
GEAR-L achieves better or on par performance compared with baseline methods on all datasets and
all models in both 4-bit and 2-bit compression. For example, in the case of 2-bit compression, GEAR
achieves 47.83% average accuracy on LLaMA3-8B across three datasets, which is near-lossless
compared to FP16 baseline (48.69%) and significantly outperforms the best-performing baseline
(28.82%, KIVI). Notably, GEAR-L also establish remarkable performance – near-lossless 4-bit
compression and superior 2-bit performance compared to baselines, while demonstrating lower
KV size and higher inference e fficiency. Meanwhile, as shown in Table 1 and Figrue 2c, regardless
quantization backbone we choose, our method can always improve upon them by integrating the
error-reduction techniques, showcasing its generalization ability as an e fficient error-reduction
framework. Thus, we highlight that GEAR(-L) is orthogonal to any o ff-the-shelf quantization
scheme and can augment them in a plug-and-play manner to achieve near-lossless accuracy at
minimal memory overheads .
11

--- PAGE 12 ---
(a) Time breakdown analysis
 (b) Peak memory comparison
 (c) Throughput comparison
Figure 3: (3a) wall-clock time percentage of each component in GEAR: sparse and low-rank components
induce negligible overheads. (3b): GEAR significantly reduces the peak memory, enabling much larger batch
size than FP16. (3c): GEAR improve throughput significantly over FP16 due to our introduced techniques.
Generative performance on relatively easy tasks . We also compare di fferent methods on
relatively easy tasks without CoT reasoning. Specifically, we evaluate the performance with
LLaMA2-7B on LongBench (Bai et al., 2023), which is a suit of 21 long-context understanding
tasks including question answering, summarization, code completion, etc. (please see Appendix 10
for task metrics and dataset statistics). The average input length of LongBench is 3642. We
follow the evaluation method in (Bai et al., 2023), apply their evaluation metrics and report the
average score across all 21 tasks. Besides, we also follow (Liu et al., 2024) and compare the
performance using LLaMA2-7B and LLaMA3-8B on GSM8k with standard 5-shot prompts. Such
5-shot demonstrations consists of 5 sampled questions and their one-step (or two-step) answers
and do not involve complex CoT. Models are prompted to answer the question without multi-step
reasoning, which is simpler than the setting of 8-shot CoT prompting.
Table 2 present the experimental results on these relatively simpler tasks. We see that quantiza-
tion methods can already achieve near-lossless 4-bit/2-bit compression on these tasks, showcasing
their effectiveness on simpler tasks. For example, for 2-bit compression, per-token group-wise
quantization and KIVI both yield around 27.7% average scores across 21 tasks of LongBench.
Moreover, KIVI establish near-lossless 2-bit performance on GSM8k with 5-shot standard examples
for both LLaMA2-7B and LLaMA3-8B models. After incorporating error-reduction techniques,
GEAR and GEAR-L can achieve better or on par performance compared to baseline quantization
methods. For example, GEAR achieves 49.96% accuracy on GSM8k (5-shot) when compressing KV
caches of LLaMA3-8B to 2-bit, which is 7.42% higher then KIVI.
4.2 Inference E fficiency Comparison
In this section, we evaluate wall-clock time, memory, and throughput of GEAR on a single NVIDIA
V100 GPU (16GB). Specifically, we set the input and generation length as 1000 and 500 respectively,
and evaluate with LLaMA2-7B. We increase the batch size until out of memory and report the
peak memory/throughput between FP16 KV caches and 2-bit quantization: KIVI, GEAR, and
12

--- PAGE 13 ---
GEAR-L. We use the same hyperparameters as in Section 4.1. Here, to maximize the batch size for
all methods, we compress model weights to 8-bit, using bitsandbytes from Huggingface Transformers
(Wolf et al., 2019).
In this inference setting, we first provide a time breakdown analysis for GEAR that compares
total computational time of di fferent components during generative inference: (i) quantization-
related time that consists of total quantization and dequantization time after equipping our CUDA
kernel; (ii) low-rank time that includes total time of SVD approximation by Algorithm 2 and
forward pass of low-rank matrices; (iii) sparsity time that contains total computational time of
outlier extraction and matrix multiplication involving Sduring forward pass; (iv) other time that
is primarily about model forward pass and obtained by subtracting total wall-clock time with
time summation of aforementioned three items. We use the maximum batch size here (which
is 18) and report the average over three trials. Figure 3a presents the time percentage of each
component in GEAR and GEAR-L during generative inference. The results suggest that, while
yielding significant performance gains, low-rank and sparse components are lightweight and do
not induce unacceptable overheads. The primary complexity still stems from model forward pass.
The additional latency by low-rank and sparsity components can be negligible due to our optimized
implementation and inference techniques.
Figure 3b present the peak memory comparison across di fferent batch sizes under the same
inference setting. We see that, given the same batch size, GEAR significantly reduces the peak
memory compared to FP16 baseline, increasing the maximum severing number (i.e., batch size)
from 3 to 18. Moreover, Figure 3c shows the throughput comparison across various batch sizes.
The results demonstrate that, compared to FP16 baseline, our method significantly improves the
throughput by up to 5.07×. Meanwhile, GEAR-L achieves slightly better throughput than KIVI
due to our improved streaming strategy. We persent the detailed results of Figure 3b and 3c in
Appendix 11.
4.3 Analysis and Ablation Study
Ablation study on sparsity ratio sand rankr.We study the sensitivity of GEAR to the sparsity
ratiosand rankr. Figure 4a shows 2-bit quantization of GEAR and GEAR-L using LLaMA3-8B
on GSM8k (w. CoT) when varying sorr. We see that GEAR does not require abundant sparse
either low-rank components – a small sparse ratio ( s= 2% for GEAR) and a small rank ( r= 4for
GEAR and GEAR-L) is adequate to achieve near-lossless 2-bit compression, demonstrating high
efficiency of our method. Further increasing sorrmay improve the accuracy but not significantly,
which however results in additional memory overheads. More importantly, discarding low-rank
component can significantly degenerate the performance of GEAR and GEAR-L, highlighting its
vital role in error reduction. On the other hand, discarding sparse matrices can hurt performance
but not significantly because the incoherent error from outlier entries can also be partially remedied
by entry grouping of quantization. Thus, we highlight GEAR-L for those prioritizing e fficiency.
13

--- PAGE 14 ---
(a) Ablation study on sandr
(b) Recover error for p% tokens
 (c) Acc. v.s. KV cache size
Figure 4: Analysis and ablation study with LLaMA3-8B on GSM8k-CoT under 2-bit compression.
The role of low-rank approximation. We compare GEAR with outlier-aware quantization to
highlight the importance of low-rank approximation. Specifically, we apply the same evaluation
settings as Section 4.1. Table 8 in Appendix 12 presents the 2-bit performance of outlier-aware
KIVI quantization. The results suggest that employing outlier extraction alone for quantization
can improve the performance but cannot achieve near-lossless 2-bit performance that GEAR does.
Outlier-aware quantization still faces challenges in achieving high-ratio compression. In contrast,
low-rank approximation plays a pivotal role in fully remedy approximation errors and achieving
near-lossless high-ratio compression.
Applying error reduction to di fferent amounts of tokens. To further demonstrate the e ffective-
ness of error reduction, we study the performance variation of GEAR-L when applying low-rank
approximation to varying number of tokens with LLaMA3-8B on GSM8k and AQuA (w. CoT).
Specifically, we split tokens into (i) input tokens in prefill phrase and (ii) generated tokens in
decoding phrase. By default, we recover quantization errors for all of them. Alternatively, we
can takep%most recent prefill tokens and only apply low-rank approximation to them. Fig-
ure 4b presents the performance of GEAR-L when changing p. We see that the performance of
GEAR-L degenerates when decreasing the number of token applied error reduction, validating the
effectiveness of error-reduction technique.
Different compression ratios. Figure 4c compares the performance of various methods on
GSM8k (w. CoT) when compressing KV caches of LLaMA3-8B to di fferent remaining size. We see
that GEAR and GEAR-L consistently outperform other quantization baseline methods, achieving
near-lossless accuracy across various compression ratios and showcasing their e ffectiveness as an
efficient error-reduction framework for KV cache quantization.
5 Related Work
LLM weights compression. LLM weight compression can significantly reduce the memory foot-
print and data transfer cost. GPTQ Frantar et al. (2023) accelerated the optimal brain quantization
for LLM weights by orders of magnitude. SqueezeLLM Kim et al. (2023) successfully compressed
the model weights to 3 bits by extracting the outlier values and quantize the remaining values
according to hessian matrix within 10% perplexity increases. These algorithms are e ffective and
could compress weights to 2 or 3 bits with acceptable loss of accuracy. However, these methods
often require significant latency overhead and gradient information to work. Thus their are not
14

--- PAGE 15 ---
fit for KV cache compression since KV cache does not have any trainable parameter and changes
every generation stage, requiring e fficient light-weight method for online compression.
LLM KV cache compression. Activation and KV cache compression are harder than weight
compression since they are more sensitive and related to model inputs. SmoothQuant Xiao
et al. (2023) achieved 8-bit compression both for activation (KV caches included) and weights by
adjusting the scaling factors to reduce outlier error and demonstrates near lossless performance
on simple generative tasks. Atom Zhao et al. (2023) successfully compressed KV Cache to 4 bits
on simple generative tasks within 5% performance degradation by combining 4-bit and 8-bit
channel-wise quantization. Another line of work explored KV pruning via token dropping based
on attention score analysis. In specific, H 2O (Zhang et al., 2023b) and FastGen Ge et al. (2023)
proposed to prune KV via dropping tokens based on attention score to decrease the KV cache size.
SparQ Ribar et al. (2023) not only dropped tokens according to attention score sparsity but also
incorporated the error of the pruned value cache. These pruning and quantization algorithms often
work well on summarizing tasks and zero-shot inference. However, for fine-tuned models, CoT
inference, and generative reasoning datasets, attention scores are denser and each token contains
important information that can not be ignored. Moreover, token dropping needs to weigh each
token based on attention score, which makes these methods hard to deploy with FlashAttention
Dao et al. (2022). Additionally, recent works have demonstrated the attention sparsity to be a
function of the non-linearity choice of the model Mirzadeh et al. (2023), showing its vulnerability
as a metric for KV compression.
6 Discussion and Conclusions
In this paper, we present GEAR, an e fficient error-reduction framework that can augment any
off-the-shelf KV cache quantization scheme with two lightweight error reduction techniques in
a plug-and-play manner to achieve near-lossless accuracy at high-ratio compression. In specific,
GEAR demonstrates the SOTA performance on complex generative tasks involving reasoning,
achieving an average accuracy improvement of 14.95% at 2-bit KV quantization compared to the
alternatives. Additionally, GEAR can substantially reduce the peak memory compared to FP16
baseline, and thus enable to serve more inference requests, bringing up to ∼5.07×throughput
improvement.
6.1 Limitations and Broader Societal Impact
A potential limitation of our work is that we set the rank identical for every Key/Value matrix
when recover quantization residuals with low-rank approximation. This ignores the fact that the
importance of Key/Value matrices varies significantly across layers and heads Zhang et al. (2023a).
Empirically, we find that it can further improve the performance of GEAR by adaptively allocating
the budget of low-rank approximation across various Key and Value matrices. We leave it as a
15

--- PAGE 16 ---
future exploration.
We consider our work to have direct benefit in reducing inference energy and thus carbon
footprint for LLM serving, enabling democratization of the power of generative AI. At the same
time, our work takes one step towards making LLM deployment e fficient across various hardware
platforms, increasing the user responsibility to explore the true power of AI in serving human
life. We hope our method can open a new avenue of memory-e fficient LLM inference for complex
generation serving.
References
Aminabadi, R. Y. ,Rajbhandari, S. ,Zhang, M. ,Awan, A. A. ,Li, C. ,Li, D. ,Zheng, E. ,Rasley, J. ,
Smith, S. ,Ruwase, O. and He, Y. (2022). Deepspeed inference: Enabling e fficient inference of
transformer models at unprecedented scale.
Bai, Y. ,Lv, X. ,Zhang, J. ,Lyu, H. ,Tang, J. ,Huang, Z. ,Du, Z. ,Liu, X. ,Zeng, A. ,Hou, L. ,Dong,
Y.,Tang, J. and Li, J. (2023). Longbench: A bilingual, multitask benchmark for long context
understanding.
Brown, T. ,Mann, B. ,Ryder, N. ,Subbiah, M. ,Kaplan, J. D. ,Dhariwal, P . ,Neelakantan, A. ,Shyam,
P .,Sastry, G. ,Askell, A. et al. (2020a). Language models are few-shot learners. Advances in
neural information processing systems ,331877–1901.
Brown, T. B. ,Mann, B. ,Ryder, N. ,Subbiah, M. ,Kaplan, J. ,Dhariwal, P . ,Neelakantan, A. ,Shyam,
P .,Sastry, G. ,Askell, A. ,Agarwal, S. ,Herbert-Voss, A. ,Krueger, G. ,Henighan, T. ,Child, R. ,
Ramesh, A. ,Ziegler, D. M. ,Wu, J. ,Winter, C. ,Hesse, C. ,Chen, M. ,Sigler, E. ,Litwin, M. ,Gray,
S.,Chess, B. ,Clark, J. ,Berner, C. ,McCandlish, S. ,Radford, A. ,Sutskever, I. and Amodei, D.
(2020b). Language models are few-shot learners. CoRR ,abs/2005.14165 .
https://arxiv.org/abs/2005.14165
Cobbe, K. ,Kosaraju, V. ,Bavarian, M. ,Chen, M. ,Jun, H. ,Kaiser, L. ,Plappert, M. ,Tworek, J. ,
Hilton, J. ,Nakano, R. ,Hesse, C. and Schulman, J. (2021). Training verifiers to solve math word
problems.
Dao, T. ,Fu, D. Y. ,Ermon, S. ,Rudra, A. andRé, C. (2022). Flashattention: Fast and memory-e fficient
exact attention with io-awareness.
Dettmers, T. ,Lewis, M. ,Belkada, Y. and Zettlemoyer, L. (2022). Llm. int8 (): 8-bit matrix
multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 .
Frantar, E. ,Ashkboos, S. ,Hoefler, T. and Alistarh, D. (2023). Gptq: Accurate post-training
quantization for generative pre-trained transformers.
16

--- PAGE 17 ---
Fu, Y. ,Ou, L. ,Chen, M. ,Wan, Y. ,Peng, H. and Khot, T. (2023). Chain-of-thought hub: A
continuous e ffort to measure large language models’ reasoning performance.
Ge, S. ,Zhang, Y. ,Liu, L. ,Zhang, M. ,Han, J. and Gao, J. (2023). Model tells you what to discard:
Adaptive kv cache compression for llms.
Hooper, C. ,Kim, S. ,Mohammadzadeh, H. ,Mahoney, M. W. ,Shao, Y. S. ,Keutzer, K. and Gholami,
A.(2024). Kvquant: Towards 10 million context length llm inference with kv cache quantization.
arXiv preprint arXiv:2401.18079 .
Jiang, A. Q. ,Sablayrolles, A. ,Mensch, A. ,Bamford, C. ,Chaplot, D. S. ,de las Casas, D. ,Bressand,
F .,Lengyel, G. ,Lample, G. ,Saulnier, L. ,Lavaud, L. R. ,Lachaux, M.-A. ,Stock, P . ,Scao, T. L. ,
Lavril, T. ,Wang, T. ,Lacroix, T. and Sayed, W. E. (2023). Mistral 7b.
Kim, S. ,Hooper, C. ,Gholami, A. ,Dong, Z. ,Li, X. ,Shen, S. ,Mahoney, M. W. and Keutzer, K.
(2023). Squeezellm: Dense-and-sparse quantization.
Li, Y. ,Yu, Y. ,Liang, C. ,He, P . ,Karampatziakis, N. ,Chen, W. and Zhao, T. (2023). Loftq: Lora-fine-
tuning-aware quantization for large language models.
Ling, W. ,Yogatama, D. ,Dyer, C. and Blunsom, P . (2017). Program induction by rationale
generation: Learning to solve and explain algebraic word problems. ACL.
Liu, Z. ,Desai, A. ,Liao, F . ,Wang, W. ,Xie, V. ,Xu, Z. ,Kyrillidis, A. andShrivastava, A. (2023). Scis-
sorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at
test time. In Thirty-seventh Conference on Neural Information Processing Systems .
https://openreview.net/forum?id=JZfg6wGi6g
Liu, Z. ,Yuan, J. ,Jin, H. ,Zhong, S. ,Xu, Z. ,Braverman, V. ,Chen, B. and Hu, X. (2024). Kivi: A
tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750 .
Meta (2024). Introducing meta llama 3: The most capable openly available llm to date.
https://ai.meta.com/blog/meta-llama-3/
Mirzadeh, I. ,Alizadeh, K. ,Mehta, S. ,Del Mundo, C. C. ,Tuzel, O. ,Samei, G. ,Rastegari, M.
and Farajtabar, M. (2023). Relu strikes back: Exploiting activation sparsity in large language
models. arXiv preprint arXiv:2310.04564 .
OpenAI (2023). Gpt-4 technical report.
Paszke, A. ,Gross, S. ,Massa, F . ,Lerer, A. ,Bradbury, J. ,Chanan, G. ,Killeen, T. ,Lin, Z. ,
Gimelshein, N. ,Antiga, L. ,Desmaison, A. ,Köpf, A. ,Yang, E. ,DeVito, Z. ,Raison, M. ,Te-
jani, A. ,Chilamkurthy, S. ,Steiner, B. ,Fang, L. ,Bai, J. and Chintala, S. (2019). Pytorch: An
imperative style, high-performance deep learning library. In Advances in Neural Information
17

--- PAGE 18 ---
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada (H. M. Wallach, H. Larochelle, A. Beygelzimer,
F. d’Alché-Buc, E. B. Fox and R. Garnett, eds.).
Pope, R. ,Douglas, S. ,Chowdhery, A. ,Devlin, J. ,Bradbury, J. ,Levskaya, A. ,Heek, J. ,Xiao, K. ,
Agrawal, S. and Dean, J. (2022). E fficiently scaling transformer inference.
Ribar, L. ,Chelombiev, I. ,Hudlass-Galley, L. ,Blake, C. ,Luschi, C. and Orr, D. (2023). Sparq
attention: Bandwidth-e fficient llm inference.
Sheng, Y. ,Zheng, L. ,Yuan, B. ,Li, Z. ,Ryabinin, M. ,Fu, D. Y. ,Xie, Z. ,Chen, B. ,Barrett, C. ,
Gonzalez, J. E. ,Liang, P . ,Ré, C. ,Stoica, I. and Zhang, C. (2023). Flexgen: High-throughput
generative inference of large language models with a single gpu.
Suzgun, M. ,Scales, N. ,Schärli, N. ,Gehrmann, S. ,Tay, Y. ,Chung, H. W. ,Chowdhery, A. ,
Le, Q. V. ,Chi, E. H. ,Zhou, D. and Wei, J. (2022). Challenging big-bench tasks and whether
chain-of-thought can solve them.
Tenney, I. ,Das, D. and Pavlick, E. (2019). Bert rediscovers the classical nlp pipeline.
Thoppilan, R. ,Freitas, D. D. ,Hall, J. ,Shazeer, N. ,Kulshreshtha, A. ,Cheng, H.-T. ,Jin, A. ,Bos,
T.,Baker, L. ,Du, Y. ,Li, Y. ,Lee, H. ,Zheng, H. S. ,Ghafouri, A. ,Menegali, M. ,Huang, Y. ,Krikun,
M.,Lepikhin, D. ,Qin, J. ,Chen, D. ,Xu, Y. ,Chen, Z. ,Roberts, A. ,Bosma, M. ,Zhao, V. ,Zhou, Y. ,
Chang, C.-C. ,Krivokon, I. ,Rusch, W. ,Pickett, M. ,Srinivasan, P . ,Man, L. ,Meier-Hellstern,
K.,Morris, M. R. ,Doshi, T. ,Santos, R. D. ,Duke, T. ,Soraker, J. ,Zevenbergen, B. ,Prabhakaran,
V.,Diaz, M. ,Hutchinson, B. ,Olson, K. ,Molina, A. ,Hoffman-John, E. ,Lee, J. ,Aroyo, L. ,
Rajakumar, R. ,Butryna, A. ,Lamm, M. ,Kuzmina, V. ,Fenton, J. ,Cohen, A. ,Bernstein, R. ,
Kurzweil, R. ,Aguera-Arcas, B. ,Cui, C. ,Croak, M. ,Chi, E. andLe, Q. (2022). Lamda: Language
models for dialog applications.
Touvron, H. ,Lavril, T. ,Izacard, G. ,Martinet, X. ,Lachaux, M.-A. ,Lacroix, T. ,Rozière, B. ,
Goyal, N. ,Hambro, E. ,Azhar, F . ,Rodriguez, A. ,Joulin, A. ,Grave, E. and Lample, G. (2023a).
Llama: Open and e fficient foundation language models.
Touvron, H. ,Martin, L. ,Stone, K. ,Albert, P . ,Almahairi, A. ,Babaei, Y. ,Bashlykov, N. ,Batra, S. ,
Bhargava, P . ,Bhosale, S. ,Bikel, D. ,Blecher, L. ,Ferrer, C. C. ,Chen, M. ,Cucurull, G. ,Esiobu,
D.,Fernandes, J. ,Fu, J. ,Fu, W. ,Fuller, B. ,Gao, C. ,Goswami, V. ,Goyal, N. ,Hartshorn, A. ,
Hosseini, S. ,Hou, R. ,Inan, H. ,Kardas, M. ,Kerkez, V. ,Khabsa, M. ,Kloumann, I. ,Korenev,
A.,Koura, P . S. ,Lachaux, M.-A. ,Lavril, T. ,Lee, J. ,Liskovich, D. ,Lu, Y. ,Mao, Y. ,Martinet, X. ,
Mihaylov, T. ,Mishra, P . ,Molybog, I. ,Nie, Y. ,Poulton, A. ,Reizenstein, J. ,Rungta, R. ,Saladi,
K.,Schelten, A. ,Silva, R. ,Smith, E. M. ,Subramanian, R. ,Tan, X. E. ,Tang, B. ,Taylor, R. ,
Williams, A. ,Kuan, J. X. ,Xu, P . ,Yan, Z. ,Zarov, I. ,Zhang, Y. ,Fan, A. ,Kambadur, M. ,Narang,
18

--- PAGE 19 ---
S.,Rodriguez, A. ,Stojnic, R. ,Edunov, S. and Scialom, T. (2023b). Llama 2: Open foundation
and fine-tuned chat models.
Vaswani, A. ,Shazeer, N. ,Parmar, N. ,Uszkoreit, J. ,Jones, L. ,Gomez, A. N. ,Kaiser, Ł. and
Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems ,
30.
Vogels, T. ,Karimireddy, S. P . and Jaggi, M. (2019). Powersgd: Practical low-rank gradient
compression for distributed optimization. CoRR ,abs/1905.13727 .
http://arxiv.org/abs/1905.13727
Voita, E. ,Talbot, D. ,Moiseev, F . ,Sennrich, R. and Titov, I. (2019). Analyzing multi-head self-
attention: Specialized heads do the heavy lifting, the rest can be pruned.
https://aclanthology.org/P19-1580
Wei, J. ,Tay, Y. ,Bommasani, R. ,Raffel, C. ,Zoph, B. ,Borgeaud, S. ,Yogatama, D. ,Bosma, M. ,Zhou,
D.,Metzler, D. ,Chi, E. H. ,Hashimoto, T. ,Vinyals, O. ,Liang, P . ,Dean, J. and Fedus, W. (2022).
Emergent abilities of large language models. Transactions on Machine Learning Research . Survey
Certification.
https://openreview.net/forum?id=yzkSU5zdwD
Wei, J. ,Wang, X. ,Schuurmans, D. ,Bosma, M. ,Ichter, B. ,Xia, F . ,Chi, E. ,Le, Q. and Zhou, D.
(2023). Chain-of-thought prompting elicits reasoning in large language models.
Wolf, T. ,Debut, L. ,Sanh, V. ,Chaumond, J. ,Delangue, C. ,Moi, A. ,Cistac, P . ,Rault, T. ,Louf,
R.,Funtowicz, M. et al. (2019). Huggingface’s transformers: State-of-the-art natural language
processing. arXiv preprint arXiv:1910.03771 .
Xiao, G. ,Lin, J. ,Seznec, M. ,Wu, H. ,Demouth, J. and Han, S. (2023). Smoothquant: Accurate and
efficient post-training quantization for large language models.
Yuan, A. ,Coenen, A. ,Reif, E. andIppolito, D. (2022). Wordcraft: Story writing with large language
models. In 27th International Conference on Intelligent User Interfaces . IUI ’22, Association for
Computing Machinery, New York, NY, USA.
https://doi.org/10.1145/3490099.3511105
Zafrir, O. ,Boudoukh, G. ,Izsak, P . and Wasserblat, M. (2019). Q8bert: Quantized 8bit bert. In
2019 Fifth Workshop on Energy E fficient Machine Learning and Cognitive Computing - NeurIPS
Edition (EMC2-NIPS) . IEEE.
http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00016
Zhang, Q. ,Chen, M. ,Bukharin, A. ,He, P . ,Cheng, Y. ,Chen, W. and Zhao, T. (2023a). Adaptive
budget allocation for parameter-e fficient fine-tuning. In The Eleventh International Conference on
19

--- PAGE 20 ---
Learning Representations .
https://openreview.net/forum?id=lq62uWRJjiY
Zhang, Q. ,Singh, C. ,Liu, L. ,Liu, X. ,Yu, B. ,Gao, J. and Zhao, T. (2024). Tell your model where to
attend: Post-hoc attention steering for LLMs. In The Twelfth International Conference on Learning
Representations .
https://openreview.net/forum?id=xZDWO0oejD
Zhang, S. ,Roller, S. ,Goyal, N. ,Artetxe, M. ,Chen, M. ,Chen, S. ,Dewan, C. ,Diab, M. ,Li, X. ,Lin,
X. V. ,Mihaylov, T. ,Ott, M. ,Shleifer, S. ,Shuster, K. ,Simig, D. ,Koura, P . S. ,Sridhar, A. ,Wang,
T.and Zettlemoyer, L. (2022). Opt: Open pre-trained transformer language models.
Zhang, Z. ,Sheng, Y. ,Zhou, T. ,Chen, T. ,Zheng, L. ,Cai, R. ,Song, Z. ,Tian, Y. ,Ré, C. ,Barrett, C. ,
Wang, Z. and Chen, B. (2023b). H 2o: Heavy-hitter oracle for e fficient generative inference of
large language models.
Zhao, Y. ,Lin, C.-Y. ,Zhu, K. ,Ye, Z. ,Chen, L. ,Zheng, S. ,Ceze, L. ,Krishnamurthy, A. ,Chen, T.
and Kasikci, B. (2023). Atom: Low-bit quantization for e fficient and accurate llm serving.
7 Detailed Algorithm of GEAR
8 Power Iteration Algorithm as SVDSolver
The power iteration algorithm is presented in Algorithm 2.
9 More Discussion on Related Works
LLM weights compression. LLM weight compression can significantly reduce the memory foot-
print and data transfer cost. GPTQ Frantar et al. (2023) accelerated the optimal brain quantization
for LLM weights by orders of magnitude. SqueezeLLM Kim et al. (2023) successfully compressed
the model weights to 3 bits by extracting the outlier values and quantize the remaining values
according to hessian matrix within 10% perplexity increases. These algorithms are e ffective and
could compress weights to 2 or 3 bits with acceptable loss of accuracy. However, these methods
often require significant latency overhead and gradient information to work. Thus their are not
fit for KV cache compression since KV cache does not have any trainable parameter and changes
every generation stage, requiring e fficient light-weight method for online compression.
LLM KV cache compression. Activation and KV cache compression are harder than weight
compression since they are more sensitive and related to model inputs. SmoothQuant Xiao
et al. (2023) achieved 8-bit compression both for activation (KV caches included) and weights by
20

--- PAGE 21 ---
Algorithm 1 GEAR
1:Input: The initial{K0,V0}of each layer, the sparsity ratio s, the bit-width b, the rank for prefill
tokenrp, the rank for generated token rg, the bu fferB.
2:(Prefill Phase):
3:forX∈{K0,V0}do
4: Compute S= Filters(X);
5: Compute bD= Quantb(X−S);
6: Compute R=X−bD−S;
7: forh= 1,...,H do
8: Compute Lh= SVDSolver rp(Rh);
9: end for
10: Concatenate L= Concat( L1,...,LH);
11: Replace XwithbD+L+S.
12:end for
13:(Decoding Phase):
14:fort= 1,...,ngdo
15: iftmodnb= 0then
16: forX∈{KB,VB}do
17: Compute S= Filters(X);
18: Compute bD= Quantb(X−S);
19: forh= 1,...,H do
20: Compute Lh= SVDSolver rg(X−bD−S);
21: end for
22: Concatenate L= Concat( L1,...,LH);
23: Replace XwithbD+L+S.
24: end for
25: Append Kt=Kt−nb∥KB,Vt=Vt−nb∥VB.
26: else
27: Generate new token xtand Push kttoKBand Push vttoVB.
28: end if
29:end for
adjusting the scaling factors to reduce outlier error and demonstrates near lossless performance
on simple generative tasks. Atom Zhao et al. (2023) successfully compressed KV Cache to 4 bits
on simple generative tasks within 5% performance degradation by combining 4-bit and 8-bit
channel-wise quantization. Another line of work explored KV pruning via token dropping based
on attention score analysis. In specific, H 2O (Zhang et al., 2023b) and FastGen Ge et al. (2023)
proposed to prune KV via dropping tokens based on attention score to decrease the KV cache size.
21

--- PAGE 22 ---
Algorithm 2 Low rank approximation of the error tensor
Require: Input matrix X∈Rn×dloop iteration L, low rank fraction r.
Output: A∈Rn×r,B∈Rd×r,AB⊤=L
random_initialize (A),
random_initialize (B)
whilel<L do
ifl==L−1then
B←QRdecompostion (B)
end if
A=XB
ifl==L−1then
A←QRdecompostion (A)
end if
B=XTA
l←l+ 1
end while
SparQ Ribar et al. (2023) not only dropped tokens according to attention score sparsity but also
incorporated the error of the pruned value cache. These pruning and quantization algorithms often
work well on summarizing tasks and zero-shot inference. However, for fine-tuned models, CoT
inference, and generative reasoning datasets, attention scores are denser and each token contains
important information that can not be ignored. Moreover, token dropping needs to weigh each
token based on attention score, which makes these methods hard to deploy with FlashAttention
Dao et al. (2022). Additionally, recent works have demonstrated the attention sparsity to be a
function of the non-linearity choice of the model Mirzadeh et al. (2023), showing its vulnerability
as a metric for KV compression.
10 Dataset Statistics
Here, we show the statistics of all datasets including input length in prefill phrase, generation
length and the number of evaluation examples.
11 More Inference Analysis Comparison
11.1 Detailed results on a single V100 GPU
Table 6 shows detailed results of inference e fficiency comparison in Section 4.2, which is on a single
NVIDIA V100. Also, to measure the peak memory save-up, we measure the memory consumption
under the same batch size for both GEAR and FP16 KV cache baseline, which is 18 (the maximum
22

--- PAGE 23 ---
Table 3: Statistics of GSM8k, AQuA and BBH.
# Evaluation Example Prefill Lenght Generation Length
GSM8k with 8-shot CoT 1319 900 256
AQuA with 8-shot CoT 254 1304 196
BBH with 3-shot CoT 6511 1021 196
GSM8k with 5-shot examples 1319 672 96
Table 4: Statistics of LongBench.
# Evaluation Example Prefill Lenght Generation Length
LongBench (Ave.) 4750 3642 256
batch size of GEAR on V100 GPU). Then, we apply the same inference setting and batch size for
FP16 KV cache baseline and test its corresponding memory consumption on a GPU with larger
GPU memory that accommodate more batches. The results shows that GEAR can reduce the
memory up to 2 .39×compared to FP16 KV cache baseline.
11.2 Inference E fficiency Comparison on a RTX Titan GPU
To futher evaluate the thoughput and memory usage of GEAR, we only apply GEAR-L,GEAR-L
Prefill and GEAR on a RTX Titan GPU with 24GB memory. We choose LLaMA2-7b as our base
model. GEAR-L Prefill is an lite version of GEAR-L that only apply error reduction algorithm to
prefill tokens. In Section 4.3, we discuss the accuracy improved by GEAR-L Prefill compared with
KIVI. Here we present the Peak Memory and throughputs comparison in Figrue 5. With larger
GPU memory, GEAR-L Prefill, GEAR-L and GEAR add acceptable latency and achieves 2.10 ×
throughput improvement compared to Fp16 baseline.
(a) Memory Usage Compari-
son
 (b) Throughput Comparison
Figure 5: Peak memory and throughput comparison with LLaMA2-7b on an RTX Titan 24GB GPU.
23

--- PAGE 24 ---
Table 5: An overview of the dataset statistics in LongBench from Bai et al. (2023).
Dataset ID Source Avg len Metric Language #data
Single-Document QA
NarrativeQA 1 −1 Literature, Film 18,409 F1 English 200
Qasper 1 −2 Science 3,619 F1 English 200
MultiFieldQA-en 1 −3 Multi-field 4,559 F1 English 150
MultiFieldQA-zh 1 −4 Multi-field 6,701 F1 Chinese 200
Multi-Document QA
HotpotQA 2 −1 Wikipedia 9,151 F1 English 200
2WikiMultihopQA 2 −2 Wikipedia 4,887 F1 English 200
MuSiQue 2 −3 Wikipedia 11,214 F1 English 200
DuReader 2 −4 Baidu Search 15,768 Rouge-L Chinese 200
Summarization
GovReport 3 −1 Government report 8,734 Rouge-L English 200
QMSum 3 −2 Meeting 10,614 Rouge-L English 200
MultiNews 3 −3 News 2,113 Rouge-L English 200
VCSUM 3 −4 Meeting 15,380 Rouge-L Chinese 200
Few-shot Learning
TREC 4 −1 Web question 5,177 Accuracy (CLS) English 200
TriviaQA 4 −2 Wikipedia, Web 8,209 F1 English 200
SAMSum 4 −3 Dialogue 6,258 Rouge-L English 200
LSHT 4 −4 News 22,337 Accuracy (CLS) Chinese 200
Synthetic Task
PassageCount 5 −1 Wikipedia 11,141 Accuracy (EM) English 200
PassageRetrieval-en 5 −2 Wikipedia 9,289 Accuracy (EM) English 200
PassageRetrieval-zh 5 −3 C4 Dataset 6,745 Accuracy (EM) Chinese 200
Code Completion
LCC 6 −1 Github 1,235 Edit Sim Python/C#/Java 500
RepoBench-P 6 −2 Github repository 4,206 Edit Sim Python/Java 500
24

--- PAGE 25 ---
Table 6: Detailed results in Section 4.2 using a single NIVIDA V100 GPU.
Method Batch Size Time (s) Peak Memory (GB) Throughputs (token/s)
1 117 8.44 4.27
FP16 2 118 9.94 8.47
3 (max) 120 11.44 12.5
1 142 7.28 3.52
4 148 8.49 13.51
KIVI-2bit 8 153 10.10 26.14
12 155 11.71 38.71
16 157 13.32 50.96
18 (max) 159 14.11 56.6
1 122 7.28 4.1
4 128 8.53 15.63
GEARL-2bit 8 134 10.13 29.85
12 137 11.76 43.8
16 140 13.37 57.14
18 (max) 142 14.16 63.38
1 126 7.31 3.97
4 139 8.64 14.38
GEARL-2bit 8 146 10.53 27.4
12 153 12.06 39.22
16 157 14.07 50.95
18 (max) 163 14.63 55.21
11.3 KV Cache Component
Here we discuss the components of KV Cache. Every quantization backbone at least contains
quantized integer and scale&zero point (we refer this as SZ FP16 in Figure 6). The size of former one
is decided by quantization bit-width and the latter one is decided by group number of quantization
algorithm. Another component is from the streaming bu ffer of FP16 residual tokens. When GEAR
or GEAR-L combine with KCVT quantization, bu ffer size can be small. When combining with KIVI,
buffer size should be larger than group size. GEAR and GEAR-L also have overheads stemming
from sparsity and low rank components. From Figure 6, we can tell that, KCVT induces small
streaming bu ffer overheads due to its large group size. In contrast, due to small group size of
KIVI, it induces larger residual overheads and memory consumption from scaling factors and
zero-points.
25

--- PAGE 26 ---
Figure 6: KV Cache memory distribution for Mistral-7B on GSM8K-CoT task
11.4 Comparison on Maximum Sequence Length
In this section, we compare the maximum sequence length for di fferent methods under the same
inference setting as in Section 4.2. We use a LLaMA2-7B, set the batch size as 1, and test the
maximum sequence length nfor different methods. Similarly, we compress model weights to 8-bit
as in Section 4.2 and apply FlashAttention to save the memory usage and allow longer sequence
length. The hyperparameters are the same as Section 4.2. Table 7 presents the maximum length for
FP16 and GEAR and we can see that GEAR can increase the maximum sequence length by around
2k, making previously impossible long-sequence generation feasible.
Table 7: Maximum sequence length comparison.
Method BitbMax Length
FP16 KV Cache 16 5319
GEAR(KIVI)
s=2%,r=42 7291
12 Comparison between GEAR and Outlier-Aware Quantization
In this section, we present the comparison between GEAR and outlier-aware quantization to further
demonstrate the importance of low-rank approximation. Specifically, we apply the same evaluation
settings as Section 4.1. Table 8 present the results.
13 KV Cache Average Size for Di fferent Datasets
Table 9 presents the detailed KV cache size comparison across di fferent methods, models and
datasets as shown in Table 1.
26

--- PAGE 27 ---
Table 8: Comparison of GEAR with outlier-aware quantization on CoT reasoning tasks.
Model LLaMA3-8B LLaMA2-13B Mistral-7B
MethodBit KV GSM8k AQuA BBH GSM8k AQuA BBH GSM8k AQuA BBH
b size Acc Acc Acc Acc Acc Acc Acc Acc Acc
FP16 16100% 54.21 38.19 53.66 30.34 21.65 40.79 42.84 35.04 47.92
KIVIg= 64,nb= 64 221.5% 30.17 25.36 30.92 16.60 17.72 29.43 23.35 22.44 31.28
Outlier-A.(KIVI)
s=2%224.5% 36.01 36.22 36.59 18.19 18.90 33.21 37.64 22.44 36.29
GEAR-L(KIVI)
r=4223.4% 52.99 38.19 51.44 26.61 20.87 39.44 39.27 29.92 46.36
GEAR(KIVI,g=64)
s=2%,r=4227.4% 54.59 38.19 50.30 30.27 23.62 39.67 43.14 33.96 48.03
Table 9: Average KV Cache size for di fferent datasets and di fferet models.
Model LLaMA3-8B KV Cache LLaMA2-13B KV Cache Mistral-7B KV Cache
MethodBit Ave. GSM8k AQuA BBH GSM8k AQuA BBH GSM8k AQuA BBH
b KV
FP16 16 100% 100% 100% 100% 100% 100% 100% 100% 100% 100%
Per-token Q. g= 64 434.2% 35.2% 33.0% 34.4% 35.2% 33.0% 34.4% 35.2% 33.0% 34.4%
KCVT Quant 427.1% 26.7% 27.2% 27.2% 27.5% 26.7% 27.2% 27.5% 26.7% 27.2%
KIVIg= 64,nb= 64 434.2% 35.2% 33.0% 34.4% 35.2% 33.0% 34.4% 35.2% 33.0% 34.4%
GEAR-L(KCVT)
r=4429.0% 29.7% 28.9% 29.4% 29.3% 28.4% 29.0% 29.3% 28.5% 29.0%
GEAR(KCVT)
s=2%,ρ=2%431.0% 31.7% 30.9% 31.4% 31.3% 30.4% 31.0% 31.3% 30.5% 31.0%
Per-token Q. g= 64 221.7% 22.7% 20.5% 21.9% 22.7% 20.5% 21.9% 22.7% 20.5% 21.9%
KIVIg= 64,nb= 64 221.7% 22.7% 20.5% 21.9% 22.7% 20.5% 21.9% 22.7% 20.5% 21.9%
GEAR-L(KIVI)
r=4223.6% 25.0% 22.7% 24.1% 24.5% 22.2% 23.7% 24.5% 22.2% 23.7%
GEAR(KIVI)
s=2%,r=4227.6% 29.0% 26.7% 28.1% 28.5% 26.2% 27.7% 28.5% 26.2% 27.7%
14 Comparison with token dropping.
We evaluate the performance of H 2O (Zhang et al., 2023b) for reducing KV cache size on GSM8k
with LLaMA2-7B. Table 10 presents its accuracy when dropping 50% tokens, which suggests H 20
cannot e ffectively preserve the performance nor achieve high compression ratio. For complex tasks
involving reasoning or long-sequence generation (such as GSM8k), models need to closely attend
to most contextual information to generate correct answers. Token dropping methods, however,
can make some information directly invisible, resulting in deviation in generation and degradation
27

--- PAGE 28 ---
of performance.
Table 10: Accuracy of H 2O on GSM8k with LLaMA2-7B.
Method BitbKV size CoT Acc.
FP16 16 100% 16.33
H2O 16 50% 6.82
GEAR(KCVT)
s=2%,r=44 32.4% 16.14
15 Discussion on the Prompts
Figure 7: Example of GSM8k-CoT prompt. The Red, Green, and Blue colored portions correspond to
the example question, a common preceding prompt, and the example answer prompt, respectively.
Here, we use the common prompt to improve the reasoning of the LLM.
For the GSM8k dataset, there is a fixed prompt for all evaluations. The prompt contains 8
examples with clear guidance step by step. For the MMLU and BBH dataset, there are individual
prompts for each sub dataset. Figure 7 shows one of the example in GSM8K dataset.
28
