# MiniCache: Nén KV Cache theo Chiều Sâu
cho Các Mô hình Ngôn ngữ Lớn

Akide Liu1Jing Liu1Zizheng Pan1Yefei He2
Gholamreza Haffari1Bohan Zhuang1,2†
1ZIP Lab, Monash University, Australia
2ZIP Lab, Zhejiang University, China

## Tóm tắt

Một phương pháp quan trọng để triển khai hiệu quả các mô hình ngôn ngữ lớn (LLM) đòi hỏi tính toán cao là bộ nhớ đệm Key-Value (KV). Bộ nhớ đệm KV lưu trữ các trạng thái key-value của các token đã được tạo trước đó, giảm đáng kể nhu cầu tính toán lặp lại và do đó giảm độ trễ trong việc tạo tự hồi quy. Tuy nhiên, kích thước của bộ nhớ đệm KV tăng tuyến tính theo độ dài chuỗi, gây ra thách thức cho các ứng dụng yêu cầu đầu vào ngữ cảnh dài và tạo chuỗi mở rộng. Trong bài báo này, chúng tôi trình bày một phương pháp đơn giản nhưng hiệu quả, được gọi là MiniCache, để nén bộ nhớ đệm KV qua các lớp từ góc độ chiều sâu mới, giảm đáng kể dấu chân bộ nhớ cho suy luận LLM. Phương pháp của chúng tôi dựa trên quan sát rằng các trạng thái bộ nhớ đệm KV thể hiện độ tương tự cao giữa các lớp liền kề trong phần từ giữa đến sâu của LLM. Để hỗ trợ việc hợp nhất, chúng tôi đề xuất tách các trạng thái thành các thành phần độ lớn và hướng, nội suy các hướng của các vector trạng thái trong khi giữ nguyên độ dài của chúng. Hơn nữa, chúng tôi giới thiệu một chiến lược giữ lại token để giữ các cặp trạng thái có tính phân biệt cao không được hợp nhất, do đó bảo tồn thông tin với chi phí lưu trữ bổ sung tối thiểu. MiniCache của chúng tôi không cần huấn luyện và có tính tổng quát, bổ sung cho các chiến lược nén bộ nhớ đệm KV hiện tại, chẳng hạn như lượng tử hóa và thưa thớt. Chúng tôi tiến hành đánh giá toàn diện về MiniCache sử dụng các mô hình khác nhau bao gồm LLaMA-2, LLaMA-3, Phi-3, Mistral, và Mixtral trên nhiều bài kiểm tra, chứng minh hiệu suất đặc biệt trong việc đạt được tỷ lệ nén vượt trội và thông lượng cao. Trên bộ dữ liệu ShareGPT, LLaMA-2-7B với MiniCache 4-bit đạt được tỷ lệ nén đáng chú ý lên đến 5.02×, tăng thông lượng suy luận khoảng 5× và giảm dấu chân bộ nhớ 41% so với đường cơ sở bộ nhớ đệm đầy đủ FP16, tất cả trong khi duy trì hiệu suất gần như không mất mát. Dự án có sẵn tại https://minicache.vmv.re

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM), được minh họa bởi chuỗi GPT [1,2,3] và chuỗi LLaMA [4,5,6], đã nổi lên như những đổi mới then chốt trong trí tuệ nhân tạo tổng quát, nâng cao đáng kể khả năng xử lý ngôn ngữ tự nhiên. Tuy nhiên, các mô hình này được huấn luyện tỉ mỉ sử dụng tài nguyên tính toán rộng lớn [7] và bộ dữ liệu khổng lồ [8], điều này cho phép chúng tạo ra văn bản hiệu quả mô phỏng phong cách viết của con người và tiến hành phân tích lý luận phức tạp, nhưng đặt ra thách thức về triển khai và phục vụ hiệu quả. Trong khung suy luận của

†Tác giả liên lạc. Email: bohan.zhuang@gmail.com
Preprint. Under review.arXiv:2405.14366v2 [cs.CL] 7 Sep 2024

--- PAGE 2 ---

(a)Độ tương tự bộ nhớ đệm KV qua lớp
(b)Các lớp được hợp nhất vs điểm EM trên GSM8K
Lớp 𝑙−1
Lớp 𝑙−2
...
Lớp 2
Lớp 1
LMHead Đầu vào
Lớp 𝑙−3
Tỉa/Lượng tử hóa
KV KV Nén Bộ nhớ đệm T Giải mã
Hợp nhất Qua Lớp
KV
QKV
Attention
Giải mã Nén Bộ nhớ đệm KV
QKV
Attention
T+1
Trước.
MiniCache
(c)So sánh giữa MiniCache và các phương pháp trước....... Độ Tương tự Cosine
T
T+1 Số Lớp Được Hợp nhất trên LLaMA-3-70B

Hình 1: Tổng quan về chiến lược MiniCache của chúng tôi và kết quả ví dụ: (a) cho thấy quan sát rằng các trạng thái bộ nhớ đệm KV giữa hai lớp liền kề có độ tương tự cao, đặc biệt qua các lớp từ giữa đến sâu. Trục x sử dụng chỉ số/2 để biểu diễn độ tương tự cho mỗi cặp lớp. (b) so sánh hiệu suất của MiniCache và đường cơ sở trung bình, đơn giản là tính trung bình các bộ nhớ đệm KV của hai lớp, sử dụng mô hình LLaMA-3-70B [6] trên bộ dữ liệu GSM8K [10]. MiniCache, bắt đầu hợp nhất từ độ sâu nửa lớp, đạt hiệu suất gần như không mất mát. (c) làm nổi bật sự khác biệt chính giữa MiniCache và các phương pháp trước đó. MiniCache điều tra sự dư thừa giữa các lớp của bộ nhớ đệm KV dọc theo chiều sâu của LLM, một khía cạnh bị bỏ qua bởi các phương pháp dựa trên trong-lớp. Ở đây, T đề cập đến dấu thời gian cuối cùng của pre-filling, và T+1 đề cập đến dấu thời gian đầu tiên của giải mã.

LLM, bộ nhớ đệm KV [9] rất quan trọng để lưu trữ các key và value được tính toán trước, do đó tránh các phép tính lặp lại trên ngữ cảnh trước đó và nâng cao đáng kể hiệu quả triển khai LLM. Tuy nhiên, nhu cầu ngày càng tăng cho độ dài chuỗi dài hơn dẫn đến các trạng thái được cache khổng lồ, gây ra tiêu thụ bộ nhớ đáng kể trong quá trình tạo. Ví dụ, một mô hình GPT-3 175B [2], với kích thước batch là 64 và độ dài chuỗi 4,096 token (cả prefilled và generated), yêu cầu khoảng 1,208GB bộ nhớ GPU. Yêu cầu này lớn hơn 3.45× so với bộ nhớ được sử dụng để lưu trữ trọng số của mô hình. Trong bối cảnh này, nén bộ nhớ đệm KV có tầm quan trọng hàng đầu do những lợi ích rõ ràng của nó: 1) nó giảm đáng kể dấu chân bộ nhớ cho phép tạo nhanh hơn và phục vụ batch lớn hơn; 2) nó giảm đáng kể chi phí mỗi token, chứng minh lợi ích thương mại đáng kể.

Các nỗ lực nén bộ nhớ đệm KV hiện tại có thể được phân loại thô thành hai loại, cụ thể là lượng tử hóa và thưa thớt. Các phương pháp lượng tử hóa [11,12] đề xuất lưu trữ các trạng thái KV trong các giá trị số bit thấp. Thông thường, FlexGen [13] chứng minh rằng lượng tử hóa bộ nhớ đệm KV 4-bit có thể đạt hiệu suất không mất mát. Ngược lại, các phương pháp hướng thưa thớt nhằm chỉ giữ lại các token nổi bật trong khi loại bỏ phần còn lại, hoặc theo heuristic [14,15] hoặc thích ứng [16]. Một số phương pháp [11] khám phá giao điểm của hai loại này, bằng cách gán bit cao cho các token nổi bật và bit cực thấp cho phần còn lại của các token, đạt được lợi ích bộ nhớ tích cực hơn. Mặc dù có những đổi mới này, văn hệ hiện tại chỉ xem xét sự dư thừa trong-lớp, trong khi bỏ qua một hướng bổ sung quan trọng khác - sự dư thừa giữa-lớp, như được minh họa trong Hình 1(c).

Phân tích của chúng tôi bắt đầu bằng việc khám phá sự dư thừa của bộ nhớ đệm KV dọc theo chiều sâu, như được hiển thị trong Hình 1(a). Chúng tôi quan sát rằng các trạng thái bộ nhớ đệm KV thể hiện độ tương tự cao giữa các lớp lân cận trong

--- PAGE 3 ---

các phần từ giữa đến sâu của LLM. Đặc tính thú vị này cho thấy rằng các trạng thái được ghép cặp theo vị trí giữa các lớp liền kề có thể được hợp nhất chính xác thành một không gian trạng thái duy nhất với đảm bảo hiệu suất mạnh mẽ, như được minh họa trong Hình 1(b). Phương pháp này giảm đáng kể dấu chân bộ nhớ mà không cần giữ lại các trạng thái riêng lẻ cho mỗi lớp attention. Lưu ý rằng những quan sát này có liên quan đến các chiến lược suy luận động như mixture-of-depths [17] và layer-wise early exiting [18,19], tối ưu hóa đường dẫn tính toán bằng cách bỏ qua các lớp không quan trọng để nâng cao hiệu quả huấn luyện và suy luận. Hơn nữa, các phương pháp tỉa lớp [20] làm nổi bật sự dư thừa đáng kể trong các lớp sâu hơn. Tuy nhiên, mặc dù có những tiến bộ này, sự dư thừa của bộ nhớ đệm KV dọc theo chiều sâu phần lớn đã bị bỏ qua.

Trong bài báo này, chúng tôi đề xuất MiniCache, một phương pháp nén bộ nhớ đệm KV qua lớp đơn giản nhưng hiệu quả nhằm thúc đẩy hiệu quả suy luận của LLM. MiniCache bao gồm hai thành phần thiết yếu. Thứ nhất, chúng tôi giới thiệu một chiến lược hợp nhất cache chính xác, sử dụng một tham số hóa lại của các vector trạng thái phân tách chúng thành các thành phần độ lớn và hướng, tương tự như weight normalization [21]. Phương pháp này cho phép nội suy hiệu quả của thành phần hướng trong tọa độ cực trong khi bảo tồn các chuẩn trạng thái gốc để giữ lại càng nhiều thông tin càng tốt. Nội suy này đề cập đến việc hợp nhất qua lớp như được hiển thị trong Hình 1(c). Thứ hai, chúng tôi nhận ra rằng một tập con nhỏ các cặp trạng thái, được đặc trưng bởi độ tương tự thấp nhưng mang ý nghĩa ngữ nghĩa khác biệt lớn, không phù hợp cho việc hợp nhất giữa lớp. Để giải quyết vấn đề này, chúng tôi đề xuất một chiến lược giữ lại token để giảm thiểu suy giảm hiệu suất, bao gồm việc giữ lại riêng những cặp ngoại lệ này. Khung của chúng tôi đáng chú ý về hiệu quả bộ nhớ, chỉ yêu cầu lưu trữ cho một thành phần hướng chiều cao duy nhất, cùng với chi phí bộ nhớ bổ sung tối thiểu. Chi phí này bao gồm một vài token không thể hợp nhất và các chỉ số tương ứng của chúng, cũng như độ lớn theo token để khôi phục chính xác các trạng thái gốc.

Chúng tôi tiến hành thí nghiệm rộng rãi với các LLM đại diện, bao gồm Mixtral-8x7B [22], Phi-3-Mini [23], và LLaMA-3 [6] 8B và 70B tương ứng. Phương pháp của chúng tôi được đánh giá trên một loạt đa dạng các bộ dữ liệu trả lời câu hỏi và tạo [24,25,26,27,28,29,30,31] sử dụng lm-eval-harness [32]. Ngoài ra, chúng tôi đánh giá kết quả của mình trên LongBench [33] cho việc tạo chuỗi dài. Kết quả cho thấy MiniCache có thể giảm dấu chân bộ nhớ cần thiết cho suy luận LLM lên đến 41%, đồng thời tăng thông lượng khoảng 5× so với đường cơ sở cached đầy đủ, vượt trội rõ ràng so với các phương pháp hiện tại [11, 12, 14, 15].

Đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi giới thiệu MiniCache, một khung đơn giản nhưng hiệu quả cao cho nén bộ nhớ đệm KV. MiniCache tiên phong trong việc khám phá nén bộ nhớ đệm KV dọc theo chiều sâu, do đó mở rộng đáng kể khả năng của nó.
• Chúng tôi quan sát một đặc tính hấp dẫn của các trạng thái bộ nhớ đệm KV qua lớp: độ tương tự cao giữa các lớp liền kề trong các giai đoạn từ giữa đến sau của LLM. Ngoài ra, chúng tôi phát hiện rằng không phải tất cả các cặp trạng thái đều phù hợp để hợp nhất.
• Chúng tôi đề xuất một phương pháp chính xác và hiệu quả bộ nhớ cho việc hợp nhất cache qua lớp, bao gồm một chiến lược tham số hóa lại và một cơ chế giữ lại token. Phương pháp của chúng tôi bổ sung cho các phương pháp nén bộ nhớ đệm KV hiện tại, nâng cao hơn nữa hiệu quả phục vụ LLM.
• MiniCache của chúng tôi hoạt động tốt so với các phương pháp tối tân. Đáng chú ý, MiniCache 4-bit của chúng tôi đạt được tỷ lệ nén mạnh lên đến 5.02×, thông lượng suy luận cao hơn 5× và giảm bộ nhớ 41% so với đường cơ sở cache đầy đủ FP16 với hiệu suất gần như không mất mát.

## 2 Công trình Liên quan

Suy luận hiệu quả cho LLM. Các Mô hình Ngôn ngữ Lớn (LLM) bị hạn chế bởi các yêu cầu tính toán và bộ nhớ đáng kể trong quá trình suy luận, đặc biệt trong các môi trường hạn chế tài nguyên. Để giảm thiểu những thách thức này, nhiều kỹ thuật suy luận hiệu quả khác nhau đã được phát triển. Ví dụ, các phương pháp suy luận động [18,34,35,36], được đại diện bởi mixture-of-experts (MoE) [37,38,39,40,41], chọn lọc thích ứng các cấu trúc con cụ thể của mô hình trong quá trình suy luận dựa trên dữ liệu đầu vào, cải thiện đáng kể hiệu quả suy luận trong khi duy trì dung lượng mô hình. Các kỹ thuật như Multi-Query Attention [42,43], Kernel-driven attentions [44,45,46,47], và low-rank attentions [41,48,49,50] xấp xỉ chức năng của các cơ chế attention truyền thống nhưng với các triển khai hiệu quả hơn. Các chiến lược lượng tử hóa [51,52,53,54]

--- PAGE 4 ---

bao gồm việc chuyển đổi trọng số và kích hoạt của mô hình thành định dạng độ rộng bit thấp, do đó giảm dấu chân bộ nhớ và cường độ tính toán. Các phương pháp thưa thớt hóa [14,15,55,56] loại bỏ các phần tử không cần thiết trong cả trọng số mô hình và biểu diễn token, nâng cao hơn nữa hiệu quả. Một số công trình liên quan chặt chẽ, như MoD [17] và LayerSkips [19], xem xét bản chất suy luận động để bỏ qua các lớp không quan trọng theo đầu vào. Tuy nhiên, các phương pháp này yêu cầu một quá trình tinh chỉnh bổ sung hoặc các giai đoạn pre-training được thiết kế cẩn thận, điều này làm giảm khả năng thích ứng của các phương pháp này. MiniCache dựa vào các quan sát về độ tương tự giữa lớp để thực hiện hợp nhất qua lớp, giảm đáng kể nhu cầu bộ nhớ.

Hợp nhất mô hình. Nén hợp nhất bao gồm việc tập hợp các tham số và kích hoạt của mô hình ở các mức độ chi tiết khác nhau. Quá trình này nâng cao hiệu quả suy luận trong các mô hình lớn và tạo điều kiện cho sự dư thừa khổng lồ [57]. Linear Mode Connectivity (LMC) [58] cho phép tinh chỉnh các mô hình từ một cơ sở được pre-train chung. Thường xuyên, weight averaging [59] được sử dụng như một kỹ thuật hiệu quả để thực hiện nén hợp nhất. Đáng chú ý, Model Soup [60] sử dụng trung bình tuyến tính trong bối cảnh này. Các phương pháp tiên tiến như TIES Merging [61], Model Breadcrumbs [62], và DARE [63] nâng cao hơn nữa quá trình này bằng cách thưa thớt hóa và kết hợp các tham số mô hình, cho phép hợp nhất các mô hình mà không hy sinh khả năng hiệu suất. Ngoài ra, Spherical Linear intERPolation (SLERP) [64] mở rộng ngoài việc tính trung bình trọng số đơn giản bằng cách nội suy giữa các tham số mô hình. Ma trận thông tin Fisher [65] và các phương pháp dựa trên RegMean [66] tối ưu hóa hơn nữa việc hợp nhất để tạo ra trọng số lý tưởng, giảm thiểu khoảng cách ℓ2 đến đầu ra tạo trong khi bảo tồn quyền riêng tư của dữ liệu huấn luyện. Tuy nhiên, hầu hết các công trình hiện tại tập trung vào việc hợp nhất các tham số mô hình, với khái niệm khả năng hợp nhất theo chiều sâu không được khám phá kỹ lưỡng trong nghiên cứu trước đó. MiniCache tập trung vào việc hợp nhất token bộ nhớ đệm KV trong chiều sâu của LLM.

## 3 Động lực

Dưới đây, chúng tôi trình bày các quan sát mới của chúng tôi trong một góc nhìn qua lớp mới lạ.

LLama 2 7B LLama 2 30B LLama 3 8B Mixtral 8x7B
Mô hình 0 10 20 30 40 50 Exact Match (%) Baseline Mean KV
(a) Đường cơ sở trung bình đơn giản vs. cache đầy đủ trên GSM8K

0 20 40 60 80 100
Chỉ số Token 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Độ Tương tự Cosine
Lớp 16 - 17
Lớp 18 - 19
Lớp 20 - 21
Lớp 22 - 23
Lớp 24 - 25
Lớp 26 - 27
Lớp 28 - 29
Lớp 30 - 31
(b) Độ tương tự theo cặp trong bộ nhớ đệm KV các lớp liền kề

MathQA OpenBookQA PiQA RTE Winogrande 0.0 0.2 0.4 0.6 0.8
MiniCache Baseline Mean
(c) Đánh giá trên năm bộ dữ liệu QA

Hình 2: Tổng quan về các khám phá và quan sát của chúng tôi: (a) cho thấy đường cơ sở mạnh bằng cách thực hiện hợp nhất trung bình trên bộ nhớ đệm KV. (b) cho thấy độ tương tự theo cặp của các trạng thái cache giữa các lớp liền kề. (c) so sánh MiniCache, trung bình đơn giản, và đường cơ sở cache đầy đủ trên năm bộ dữ liệu khác nhau.

### 3.1 Sự Dư thừa Qua Lớp trong Bộ nhớ đệm KV

Các nghiên cứu trước đã tiết lộ sự không hiệu quả của các lớp từ giữa đến sâu trong LLM [20]. Do đó, layer-wise early exiting trong trường hợp này có thể hiệu quả tránh tính toán dư thừa với tác động nhỏ đến hiệu suất LLM [19,67]. Được truyền cảm hứng từ điều này, chúng tôi khám phá một việc hợp nhất theo lớp của bộ nhớ đệm KV trong LLM, bắt đầu với một đường cơ sở đơn giản bằng cách tính trung bình tất cả token qua các lớp liền kề. Chúng tôi cung cấp các quan sát chính của chúng tôi như sau.

Quan sát 1: Bộ nhớ đệm KV chia sẻ độ tương tự cao giữa các lớp liền kề. Dựa trên LLaMA-3-70B [6], chúng tôi tiến hành suy luận zero-shot trên các tập validation của ba bài kiểm tra được công nhận rộng rãi: COQA [68], GSM8K [10] và TruthfulQA [69]. Nói chung, chúng tôi thấy rằng bộ nhớ đệm KV trong các lớp nông thể hiện độ tương tự thấp, trong khi những lớp trong các lớp từ giữa đến sâu rất tương tự với nhau dựa trên khoảng cách góc, như được hiển thị trong Hình 1(b). Tiếp theo, chúng tôi hợp nhất bộ nhớ đệm KV qua

--- PAGE 5 ---

các lớp liền kề bằng cách tiến hành suy luận five-shot với LLaMA-2-7B, LLaMA-2-13B [5], LLaMA-3-8B [6], và Mixtral-8x7B [22] trên GSM8K [10]. Cụ thể, bắt đầu từ lớp giữa của mỗi mô hình, chúng tôi hợp nhất bộ nhớ đệm KV trong hai lớp liền kề. Như được hiển thị trong Hình 2(a), chúng tôi quan sát hiệu suất thuận lợi cho các LLM khác nhau, điều này tiết lộ tiềm năng lớn để cải thiện hiệu quả bằng cách chia sẻ bộ nhớ đệm KV qua các lớp liền kề trong quá trình giải mã LLM.

Quan sát 2: Không phải tất cả token đều quan trọng như nhau để hợp nhất, một vài cặp phân biệt yêu cầu giữ lại. Các công trình gần đây [15,16] trong nén bộ nhớ đệm KV đã phát hiện rằng việc giữ một vài token nổi bật ở mỗi lớp, đóng góp phần lớn điểm attention, có thể đủ để duy trì hiệu suất LLM. Trong trường hợp của chúng tôi, chúng tôi suy đoán rằng một số cặp token trong các lớp liền kề cũng thể hiện hành vi ngoại lệ, cho thấy sự khác biệt ngữ nghĩa mạnh khiến chúng không phù hợp để hợp nhất. Dựa trên COQA [68] và LLaMA-2-7B [5], chúng tôi điều tra độ tương tự ở mức độ cặp token. Như được hiển thị trong Hình 2(b), chúng tôi thấy một phần đáng kể các cặp token chia sẻ độ tương tự cao qua các lớp liền kề. Tuy nhiên, chúng tôi quan sát một vài cặp ngoại lệ, như chỉ số 0 và 15, với biên độ khác biệt lớn. Chúng tôi coi những token này là không thể hợp nhất do sự khác biệt đáng kể của chúng. Chúng tôi cũng cho thấy rằng việc hợp nhất những token phân biệt này dẫn đến suy giảm hiệu suất, tương ứng với hàng γ = 0 trong Bảng 2. Do đó, trong khi hợp nhất qua lớp là một chiến lược đầy hứa hẹn để giảm gánh nặng bộ nhớ, nó phải được triển khai với sự cân nhắc cẩn thận về độ tương tự cấp token để đảm bảo hiệu suất tối ưu, như được hiển thị trong Hình 2(c).

## 4 Phương pháp

Trong phần này, chúng tôi giới thiệu MiniCache của chúng tôi, một phương pháp đơn giản nhưng hiệu quả nhằm cắt giảm sự dư thừa của bộ nhớ đệm KV trong chiều sâu. Khung này khai thác độ tương tự cao của các trạng thái bộ nhớ đệm KV giữa các lớp liền kề và bao gồm hai thành phần chính: một chiến lược hợp nhất dựa trên tham số hóa lại và một cơ chế giữ lại token. Chiến lược hợp nhất nén các trạng thái bộ nhớ đệm KV trong các lớp liền kề để tập hợp chúng thành một không gian bộ nhớ chia sẻ duy nhất, bắt đầu từ giữa mô hình. Cơ chế giữ lại token giảm thiểu thông tin bị mất bằng cách giữ lại các cặp trạng thái có tính phân biệt cao với chi phí bộ nhớ bổ sung tối thiểu. Với cache được hợp nhất, token giữ lại, và độ lớn, chúng tôi có thể khôi phục chính xác các trạng thái cache gốc cho việc giải mã token.

### 4.1 Nén Qua Lớp

Phương pháp của chúng tôi bắt đầu với việc xác định một lớp bắt đầu tối ưu S. Các quan sát trong Phần 3.1 chỉ ra rằng bộ nhớ đệm KV từ các lớp từ giữa đến sâu liên tục thể hiện các mẫu độ tương tự cao qua các lớp liền kề. Do đó, chúng tôi chọn lớp bắt đầu từ giữa LLM, cụ thể là S = L/2. Từ lớp này trở đi, các cặp KV được giả định là đủ tương tự qua các lớp liền kề để bảo đảm việc hợp nhất của chúng. Trọng tâm của phương pháp này là một hàm hợp nhất, F, được thiết kế để tích hợp các bộ nhớ đệm KV của các lớp liên tiếp thành một cache thống nhất duy nhất. Chúng tôi định nghĩa x như vector hóa trạng thái cache của một token duy nhất, trong đó chỉ số trên biểu thị chỉ số lớp và các chỉ số dưới k và v biểu thị các key và value tương ứng. Cụ thể, cho một cặp token key/value ở cùng vị trí trong các lớp l và l-1, cache được hợp nhất được tính như:

c^{l,l-1}_k = F(x^l_k, x^{l-1}_k),
c^{l,l-1}_v = F(x^l_v, x^{l-1}_v). (1)

Quá trình hợp nhất này hiệu quả loại bỏ nhu cầu lưu trữ và xử lý các key và value tiêu tốn bộ nhớ gốc trong mỗi lớp một cách độc lập. Thay vào đó, nó xấp xỉ một cache được chia sẻ qua các lớp liền kề.

### 4.2 Hợp nhất và Khôi phục Bộ nhớ đệm KV

Hợp nhất cache dựa trên tham số hóa lại. Để thực hiện hợp nhất theo cặp, một giải pháp là trực tiếp tính trung bình một cặp token KV, tương tự như việc hợp nhất mô hình [60,61]. Tuy nhiên, chúng tôi quan sát rằng việc tính trung bình trực tiếp có thể gây ra mất mát thông tin đáng kể. Chúng tôi suy đoán rằng khoảng cách giữa các kích hoạt có thể lớn hơn so với trọng số do sự hiện diện của các kênh kích hoạt ngoại lệ với độ lớn cực lớn trong LLM [70,71], trong khi trọng số thường có độ lớn tương đối khá nhỏ. Một phương pháp tiềm năng để bù đắp cho mất mát thông tin này là chiếu từ c đến x^{l-1} và x^l, sau đó tái chia tỷ lệ các vector chiếu dựa trên độ lớn tương đối của chúng để khôi phục chính xác

--- PAGE 6 ---

(a)Nén Qua Lớp KV Lưu trữ C Giữ Tái chia tỷ lệ Khôi phục (b)Khôi phục l-1 l l-1 l
Giữ × Lấy C cho lớp l và l-1 nén bộ nhớ đệm KV ở lớp l bộ nhớ đệm KV gốc bộ nhớ đệm KV được hợp nhất token giữ lại cache độ lớn thao tác hợp nhất

Hình 3: Minh họa về phương pháp MiniCache được đề xuất. (a) mô tả quá trình nén qua lớp. Chúng tôi lấy các bộ nhớ đệm KV, từ các lớp l và l-1, và hợp nhất chúng thành các trạng thái chia sẻ thông qua Eq. (3). Ngoài ra, chúng tôi tính chuẩn ℓ2 cho các cache để có được độ lớn của chúng. Hơn nữa, chúng tôi chọn các token không thể hợp nhất để giữ lại, sau đó lưu trữ cache được hợp nhất, token giữ lại, và độ lớn ở lớp l trong C. (b) minh họa quá trình khôi phục cho các lớp l và l-1, bao gồm việc tái chia tỷ lệ độ lớn trong Eq. (2) và khôi phục token giữ lại.

các trạng thái gốc. Tuy nhiên, phương pháp này yêu cầu lưu trữ và tính toán bổ sung rộng rãi; ví dụ, khôi phục x^{l-1} cần cả c và x^l, điều này làm suy yếu lợi ích của việc hợp nhất cache. Để hợp nhất hiệu quả các cặp token, chúng tôi lấy cảm hứng từ weight normalization [21], phân tách các tham số mô hình thành các thành phần độ lớn và hướng để tăng tốc sự hội tụ của gradient descent ngẫu nhiên. Ngoài ra, chúng tôi lấy gợi ý từ DoRA [72], sử dụng một cách tương tự để giống hành vi học tập của parameter-efficient fine-tuning so với full fine-tuning. Trong trường hợp của chúng tôi, tham số hóa lại có thể được công thức hóa như sau:

x̂^l = e^{l,l-1} · ||x^l|| / ||e^{l,l-1}||, x̂^{l-1} = e^{l,l-1} · ||x^{l-1}|| / ||e^{l,l-1}||, (2)

trong đó e là vector hướng. Phân tách này đảm bảo rằng e^{l,l-1}/||e^{l,l-1}|| là một vector đơn vị, và cho phép các trạng thái được khôi phục khớp với chuẩn ℓ2 của các trạng thái gốc, do đó bảo tồn thông tin của cache càng nhiều càng tốt. Khôi phục được hiển thị như Hình 3(b). Để ngắn gọn, chúng tôi bỏ qua các chỉ số dưới k và v, vì key và value được phân tách theo cùng một cách. Để ước tính thành phần hướng e^{l,l-1}, chúng tôi theo SLERP [64], xử lý thích ứng việc nội suy, thường giống với các biến đổi giống xoay. Việc chọn SLERP như hàm hợp nhất là chiến lược, vì nó tạo điều kiện nội suy dọc theo đường dẫn ngắn nhất trên mặt cầu đơn vị giữa hai vector chiều cao, do đó bảo tồn tính toàn vẹn hình học của chúng, điều này đề cập đến thao tác hợp nhất trong Hình 3(a). Điều này rất quan trọng để duy trì các đặc tính ngữ nghĩa và cú pháp của các bộ nhớ đệm KV. Công thức cho SLERP trong bối cảnh của chúng tôi là:

e^{l,l-1} = (sin((1-t)Ω^{l,l-1})/sin(Ω^{l,l-1})) · (x^{l-1}/||x^{l-1}||) + (sin(tΩ^{l,l-1})/sin(Ω^{l,l-1})) · (x^l/||x^l||), (3)

trong đó Ω^{l,l-1} = arccos(x^l·x^{l-1}/(||x^l||||x^{l-1}||)) biểu thị góc giữa các vector x^l và x^{l-1}, và sin(·) là hàm sin. t là một siêu tham số nội suy điều chỉnh ảnh hưởng tương đối của mỗi vector lên hướng kết quả, được điều chỉnh theo độ sâu lớp và đặc tính cụ thể của các cặp KV. Lưu ý rằng khi chúng tôi đặt t = 0.5, nó sẽ trở thành hợp nhất trung bình dọc theo bề mặt hình học, mà chúng tôi coi là trường hợp đặc biệt trong Eq. (A). Cache được hợp nhất cho mỗi cặp token sau đó là một nối của vector hướng, độ lớn và Ω^{l,l-1}, biểu thị như c^{l,l-1} = [e^{l,l-1}, ||x^{l-1}||, ||x^l||, Ω^{l,l-1}], các thành phần được cache như được hiển thị trong Hình 3(a). Lưu ý rằng ngoài việc lưu trữ vector hướng được hợp nhất, chúng tôi chỉ cần lưu trữ độ lớn theo token bổ sung và các scalar góc, điều này hiệu quả về bộ nhớ. Theo cách này, chúng tôi đạt được hiệu quả bộ nhớ đáng kể thông qua việc giảm sự dư thừa trong khi đảm bảo giữ lại các đặc tính chức năng quan trọng của các cặp KV gốc qua các lớp transformer.

Giữ lại token không thể hợp nhất. Các cặp có tính phân biệt cao nhạy cảm với các thao tác hợp nhất, dẫn chúng tôi đề xuất giữ lại token không thể hợp nhất, như được hiển thị trong Hình 3(a). Mặc dù có độ tương tự cao giữa các trạng thái bộ nhớ đệm KV qua các lớp lân cận, một vài cặp phân biệt nhạy cảm vẫn còn lại khó hợp nhất và chia sẻ đáng kể. Phù hợp với các nghiên cứu trước, những token phân biệt này mang ý nghĩa ngữ nghĩa đáng kể [15,16]. Chúng tôi quan sát rằng việc hợp nhất các token nhạy cảm, dẫn đến mất mát thông tin cụ thể theo lớp, có thể dẫn đến suy giảm hiệu suất đáng kể. Do đó, việc tách biệt đúng đắn thông tin chia sẻ và duy nhất giữa các lớp liền kề là rất quan trọng. Để giải quyết vấn đề này, chúng tôi thiết kế một chiến lược giữ lại token để chọn lọc giữ lại các token không thể được hợp nhất dựa trên khoảng cách góc của chúng, được định nghĩa là: d(x^l, x^{l-1}) = 1/π · Ω. Đối với các bộ nhớ đệm KV, khoảng cách góc tối thiểu và tối đa được xác định để nhận diện các token không thể hợp nhất.

Tập hợp các chỉ số token cần thiết để giữ, I, được thu được bởi:
I = {i | d_i < d_min + (d_max - d_min) · γ}, (4)

trong đó γ là một siêu tham số được xác định trước kiểm soát ngưỡng giữ lại. Các token với chỉ số trong I được giữ lại và không được nén trong quá trình hợp nhất, điều này đảm bảo rằng hiệu suất không giảm bằng cách ngăn chặn mất mát các token không thể hợp nhất.

Tiếp theo, đặt X ∈ R^{n×h} là cache key hoặc value ở một lớp attention, trong đó n biểu thị số lượng token và h là số chiều ẩn, và E ∈ R^{n×h} là các trạng thái bộ nhớ đệm KV chia sẻ. Đối với mỗi cặp hai lớp lân cận, các token không thể hợp nhất được chọn cùng với chiều token bởi R^l = X^l[I], R^{l-1} = X^{l-1}[I], sau đó khôi phục đến các cache nén của chúng tôi bởi X̂^l[I] = R^l, X̂^{l-1}[I] = R^{l-1}, như được hiển thị trong Hình 3(b). Tổng thể, chúng tôi chia sẻ cache cuối cùng cho hai lớp như C^{l,l-1} = [E^{l,l-1}, R^l, R^{l-1}, ||X^{l-1}||, ||X^l||, I]. Cache này bao gồm các trạng thái bộ nhớ đệm KV chia sẻ, giữ lại các token chưa được hợp nhất, vector độ lớn cho mỗi lớp, và chỉ số giữ token tương ứng. Những thành phần bổ sung này khá nhẹ. Do đó so với các cache toàn lớp, phương pháp của chúng tôi vẫn hiệu quả về bộ nhớ, như được thảo luận trong Sec. 4.3.

Khôi phục cache. Sau khi có được cache chia sẻ C^{l,l-1}, chúng tôi cần khôi phục xấp xỉ các trạng thái cache gốc cho việc giải mã token hiện tại, như được hiển thị trong Hình 3(b). Cụ thể, để khôi phục X^l, trước tiên chúng tôi tái chia tỷ lệ các trạng thái chia sẻ hướng với vector độ lớn tương ứng dọc theo chiều token, biểu thị như E^{l,l-1}||X^l||. Tiếp theo, chúng tôi thực hiện khôi phục token giữ lại bằng cách đặt các token nhạy cảm theo chỉ số token của chúng.

### 4.3 Thảo luận Hiệu quả

Hiệu quả nén. Chúng tôi chủ yếu phân tích hiệu quả bộ nhớ của chúng tôi về số lượng token được sử dụng. Tiếp theo, đặt r là số lượng lớp và b là kích thước batch, s và n là độ dài chuỗi đầu vào và đầu ra tương ứng. Chúng tôi xem xét lưu trữ FP16 cho bộ nhớ đệm KV. Việc sử dụng bộ nhớ cache đầy đủ được cho bởi 4brh(s+n). Trong nghiên cứu của chúng tôi, chúng tôi bắt đầu hợp nhất các lớp từ giữa đến các lớp sâu hơn, hợp nhất các trạng thái bộ nhớ đệm KV của mỗi hai lớp thành một không gian trạng thái chia sẻ duy nhất. Kết quả là, chúng tôi hiệu quả giảm việc sử dụng bộ nhớ GPU trong suy luận giải mã xuống 3brh(s+n), chứng minh tỷ lệ nén đáng kể.

Hiệu quả khôi phục. Sau đó chúng tôi phân tích chi phí bộ nhớ bổ sung phát sinh trong quá trình khôi phục, mà trong giai đoạn tái chia tỷ lệ độ lớn, chúng tôi lưu một vector chuẩn bổ sung cho các lớp tương ứng trong bộ nhớ đệm KV. Điều quan trọng cần lưu ý là vector chuẩn có hình dạng R^{b×s×1}, có một chiều kênh duy nhất so với các trạng thái KV gốc được xếp hạng đầy đủ. Ngoài ra, chúng tôi giả định rằng ngưỡng giữ lại có thể được đặt thành 0.05. Do đó, chúng tôi có brh(0.05(s+n)) token được giữ lại mà không nén. Cuối cùng, yêu cầu bộ nhớ tổng thể của chúng tôi được cho bởi (3.1h + 2)br(s+n). Việc dẫn xuất chi tiết được hiển thị trong Phụ lục E.

## 5 Thí nghiệm

Chúng tôi chứng minh rằng MiniCache của chúng tôi có thể thực hiện nén hợp nhất trên nửa sau các lớp của LLM với suy giảm hiệu suất tối thiểu.

Chi tiết triển khai. Các thí nghiệm của chúng tôi dựa trên các họ mô hình đại diện của LLM, bao gồm một LLM nhỏ gọn Phi-3-Mini [23] và một LLM MoE Mixtral-8x7B [22]. Ngoài ra, chúng tôi áp dụng các mô hình LLaMA-3 [6] 8B và 70B để khám phá cách phương pháp của chúng tôi tổng quát hóa đến các LLM lớn hơn. Chúng tôi lấy mẫu mười tác vụ từ lm-eval-harness [32], bao gồm COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], và CNN/Daily Mail [31]. Chúng tôi cũng đánh giá việc tạo chuỗi dài trên LongBench [33]. Chúng tôi so sánh phương pháp của chúng tôi với đường cơ sở cached đầy đủ,

--- PAGE 8 ---

và các phương pháp khác như round-to-nearest quantization (RTN) [73], SmoothQuant [70] và KIVI [11].

Đối với MiniCache được đề xuất, chúng tôi đặt tham số nội suy t thành 0.6, cho thấy rằng kết quả được hợp nhất có góc xoay nhỏ hơn đến lớp tiếp theo. Hơn nữa, chúng tôi đặt ngưỡng giữ lại token γ thành 0.05, theo thống kê của các token không thể hợp nhất qua nhiều bộ dữ liệu. Ngoài phương pháp hợp nhất của chúng tôi, chúng tôi cũng xem xét một đường cơ sở mạnh của hợp nhất trung bình. Đối với việc tải tuần tự các mô hình lớn, chúng tôi sử dụng 4 GPU NVIDIA A100 80GB, chi tiết thêm tham khảo Phụ lục D.

Kết quả chính. Chúng tôi đánh giá MiniCache bằng cách hợp nhất các bộ nhớ đệm KV qua tất cả các lớp trên GSM8K, COQA, và TruthfulQA. Kết quả được hiển thị trong Hình 4. Nói chung, chúng tôi chứng minh hiệu quả tổng quát của việc hợp nhất các bộ nhớ đệm KV từ các lớp từ giữa đến sâu qua các LLM có kích thước khác nhau. Hơn nữa, MiniCache được đề xuất chứng minh một lợi thế nhất quán và đáng kể so với đường cơ sở tính trung bình. Chúng tôi cũng minh họa hiệu suất của việc hợp nhất các bộ nhớ đệm KV qua nửa số lớp với các đường màu xanh, trong đó MiniCache vẫn duy trì hiệu suất mạnh mẽ và đạt được tỷ lệ nén tốt nhất. Bên cạnh đó, chúng tôi thấy rằng phương pháp của chúng tôi thậm chí còn hiệu quả hơn đối với các LLM lớn hơn. Ví dụ, dựa trên LLaMA-3-70B, MiniCache cho thấy gần như không có sự sụt giảm hiệu suất ngay cả với bộ nhớ đệm KV trong 87.5% các lớp được hợp nhất trên bộ dữ liệu COQA. Điều này làm nổi bật khả năng thích ứng và hiệu quả của phương pháp chúng tôi trong việc xử lý các mô hình quy mô lớn trong khi đảm bảo suy giảm hiệu suất tối thiểu.

LongBench. Chúng tôi cũng tiến hành thí nghiệm để đánh giá hiệu suất và chất lượng trong việc tạo chuỗi dài sử dụng bộ dữ liệu LongBench [33], như được hiển thị trong Bảng 1. Các thí nghiệm của chúng tôi áp dụng MiniCache trên một số mô hình: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, và Mistral-7B-Instruct. Điều quan trọng cần lưu ý là phương pháp MiniCache của chúng tôi duy trì tính trực giao với tất cả các phương pháp lượng tử hóa và thưa thớt hiện tại (tham khảo Bảng A) ở cả mức mô hình và mức token. Khi kết hợp với lượng tử hóa bộ nhớ đệm KV KIVI-4bit, phương pháp của chúng tôi đạt được tỷ lệ nén 5.02×, với tác động tối thiểu đến độ chính xác qua các tác vụ tạo ngữ cảnh dài thách thức khác nhau. Sự kết hợp của MiniCache và lượng tử hóa bộ nhớ đệm KV KIVI-4bit chứng minh việc tiết kiệm bộ nhớ đáng kể mà không ảnh hưởng đến khả năng của mô hình để xử lý các chuỗi dài hiệu quả. Điều này cao-

Hình 4: So sánh hiệu suất giữa MiniCache được đề xuất của chúng tôi với "đường cơ sở tính trung bình" và "đường cơ sở cache đầy đủ không hợp nhất" trên nhiều bộ dữ liệu với Phi3-Mini, Mixtral-8x7B, LLaMA-3-8B, và LLaMA-3-70B. Chi tiết kết quả thêm được hiển thị trong Phụ lục F. Trục x chỉ ra số lượng lớp được hợp nhất. Khi nhiều lớp được hợp nhất hơn, việc giảm sử dụng bộ nhớ lớn hơn được đạt được.

--- PAGE 9 ---

Bảng 1: Đánh giá các phương pháp nén bộ nhớ đệm KV khác nhau trên LongBench. MiniCache xây dựng trên 4-bit KIVI [11] và đạt được hiệu suất tốt nhất với tỷ lệ nén mạnh nhất.

[TABLE - preserving the structure but translating headers and content to Vietnamese]

độ nổi bật tiềm năng của phương pháp chúng tôi để tối ưu hóa các mô hình ngôn ngữ lớn cho các tác vụ yêu cầu ngữ cảnh rộng lớn, làm cho chúng hiệu quả và có thể mở rộng hơn cho các ứng dụng thế giới thực.

Phân tích hiệu quả. Để đánh giá khả năng tăng tốc của MiniCache, chúng tôi tiến hành đánh giá dựa trên các phương pháp được sử dụng trong vLLM [74] và KIVI [11]. Chúng tôi tạo ra khối lượng công việc tổng hợp có nguồn gốc từ ShareGPT, bao gồm văn bản đầu vào và đầu ra thực từ các dịch vụ LLM. Bộ dữ liệu có độ dài prompt đầu vào trung bình là 161 token và độ dài đầu ra 338 token. Sử dụng mô hình LLaMA-2-7B trên một GPU NVIDIA A100 80GB duy nhất, chúng tôi đánh giá phương pháp của chúng tôi trong tình huống phục vụ batch, so sánh việc sử dụng bộ nhớ đỉnh và thông lượng giữa KIVI 2-bit, MiniCache 4-bit, và đường cơ sở FP16. Như được minh họa trong Hình 5, với kích thước batch 128, MiniCache giảm sử dụng bộ nhớ 25GB, đạt được 41% tiết kiệm bộ nhớ. Về thông lượng, MiniCache vượt trội so với đường cơ sở FP16 khoảng 5×. Ngoài ra, mặc dù sử dụng lượng tử hóa 4-bit, MiniCache có lợi từ việc hợp nhất và chia sẻ các bộ nhớ đệm KV qua các lớp liền kề, dẫn đến thông lượng cao hơn 1.29× so với KIVI 2-bit. Những kết quả này chứng minh rằng MiniCache cung cấp một sự đánh đổi tối tân giữa hiệu quả và hiệu suất.

50 100 150 200 250 300
Kích thước Batch 20 30 40 50 60 70 80 Sử dụng Bộ nhớ Đỉnh (GB)
Baseline FP16
KIVI 2
MINICache 4
(a) BS. vs. Sử dụng Bộ nhớ Đỉnh

50 100 150 200 250 300
Kích thước Batch 1000 1500 2000 2500 3000 Thông lượng (token/giây)
Baseline FP16
KIVI 2
MINICache 4
(b) BS. vs. Thông lượng Giải mã

Hình 5: So sánh sử dụng bộ nhớ và thông lượng giữa MiniCache 4-bit của chúng tôi, KIVI 2-bit, và Baseline 16-bit. MiniCache có thể đạt được thông lượng cao hơn bằng cách cho phép kích thước batch lớn hơn trong khi giảm dấu chân bộ nhớ thông qua LLaMA-2-7B [5].

0.3 0.4 0.5 0.6 0.7
Tham số Nội suy t 0.350 0.375 0.400 0.425 0.450 0.475 0.500 Điểm Exact Match
GSM8k
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Tần suất Chuẩn hóa Tần suất

Hình 6: LLaMA-3-8B [6] để thí nghiệm trên GSM8K [10]. Trục phải là tần suất chuẩn hóa của tỷ lệ độ lớn tương đối. t tùy chọn cho thấy mối tương quan mạnh với tần suất.

## 6 Nghiên cứu Ablation

Bảng 2: So sánh các ngưỡng giữ lại token γ khác nhau bởi LLaMA-2-7B [5] trên ba bài kiểm tra.

[TABLE structure preserved with Vietnamese translations]

Tác động của tham số diễn giải t. Chúng tôi khám phá tác động của tham số diễn giải t đến hiệu suất, đặc biệt liên quan đến tỷ lệ độ lớn tương đối của các lớp liền kề, như được hiển thị trong Hình 6. Chúng tôi duy trì tất cả các cài đặt không đổi, bắt đầu từ lớp S = 16 (nửa chừng

--- PAGE 10 ---

qua các lớp của LLaMA-3-8B), và thay đổi tham số diễn giải t từ 0.3 đến 0.7. Các phát hiện của chúng tôi tiết lộ một số điểm chính. Khi t = 0.5, quá trình giống với hợp nhất trung bình, ít hiệu quả hơn cho việc hợp nhất qua lớp. Ngược lại, khi t = 0.6 là tối ưu, biểu diễn được hợp nhất thể hiện hiệu suất mạnh mẽ nhất, trong khi chỉ ra rằng nhiều thông tin hơn được thu được từ số hạng thứ hai (x^l) của SLERP.

Kết quả tần suất cũng chỉ ra rằng các tần suất cao được tập trung xung quanh 0.4 và 0.6, xác nhận t tối ưu của chúng tôi. Hơn nữa, có một mối tương quan mạnh giữa t tối ưu và tần suất cao của tỷ lệ độ lớn tương đối của các lớp liền kề. Phát hiện này cung cấp cơ hội để sử dụng tỷ lệ độ lớn tương đối để xác định động tham số diễn giải t. t động cho phép kiểm soát trọng số linh hoạt hơn trong việc hợp nhất SLERP cho mỗi thao tác theo lớp, do đó cho thấy tiềm năng khám phá thêm.

Tác động của ngưỡng giữ lại token γ. Chúng tôi điều tra tác động của ngưỡng giữ lại token γ đến hiệu suất mô hình qua ba bộ dữ liệu, như được hiển thị trong Bảng 2. Một γ lớn hơn thường có nghĩa là giữ lại nhiều token hơn để cải thiện hiệu suất, nhưng điều này đi kèm với chi phí tăng nhu cầu bộ nhớ. Kết quả cho thấy rằng đặt γ thành 0.05 đạt được sự cân bằng tốt nhất giữa hiệu suất và hiệu quả.

## 7 Kết luận và Công việc Tương lai

Bài báo này trình bày một khám phá tiên phong về nén bộ nhớ đệm KV trong chiều sâu, giải quyết một nút thắt bộ nhớ đáng kể trong LLM. MiniCache được đề xuất của chúng tôi cung cấp một phương pháp đơn giản, hiệu quả và không cần huấn luyện để nén các bộ nhớ đệm KV bằng cách tận dụng độ tương tự cao đáng chú ý giữa các bộ nhớ đệm KV trong các lớp lân cận, bắt đầu từ điểm giữa của LLM. Chúng tôi đã chứng minh rằng MiniCache có thể giảm đáng kể dấu chân bộ nhớ cần thiết cho suy luận LLM lên đến 41%, đồng thời tăng thông lượng khoảng năm lần so với đường cơ sở FP16. Tóm lại, MiniCache thúc đẩy đáng kể lĩnh vực nén bộ nhớ đệm KV, cung cấp sự cân bằng tối tân giữa hiệu quả và hiệu suất. Công việc tương lai sẽ tập trung vào việc nâng cao tỷ lệ nén bằng hợp nhất qua-nhiều-lớp, phát triển các thuật toán hợp nhất tiên tiến như Spherical Cubic Interpolation [75], và tối ưu hóa thêm việc sử dụng bộ nhớ cho các triển khai quy mô lớn trong các tình huống ứng dụng đa dạng.

## Tài liệu tham khảo

[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," in NeurIPS, vol. 33, pp. 1877–1901, 2020.

[2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, "Gpt-4 technical report," 2023.

[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., "Training language models to follow instructions with human feedback," in NeurIPS, vol. 35, pp. 27730–27744, 2022.

[4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.

[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.

[6] "Introducing meta llama 3: The most capable openly available llm to date." https://ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04.

[7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," arXiv preprint arXiv:2001.08361, 2020.

[Tiếp tục với các tài liệu tham khảo khác theo cùng định dạng...]

--- PAGE 11 ---

[8] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., "Gemini: a family of highly capable multimodal models," arXiv preprint arXiv:2312.11805, 2023.

[9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, "Efficiently scaling transformer inference," Proceedings of Machine Learning and Systems, vol. 5, 2023.

[10] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., "Training verifiers to solve math word problems," arXiv preprint arXiv:2110.14168, 2021.

[11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, and X. Hu, "Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization," arXiv preprint arXiv:2402.02750, 2024.

[12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, "Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm," arXiv preprint arXiv:2403.05527, 2024.

[13] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, "Flexgen: High-throughput generative inference of large language models with a single gpu," in ICML, pp. 31094–31116, PMLR, 2023.

[14] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. Ré, C. Barrett, et al., "H2o: Heavy-hitter oracle for efficient generative inference of large language models," in NeurIPS, vol. 36, 2024.

[15] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, "Efficient streaming language models with attention sinks," in ICLR, 2024.

[16] S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, "Model tells you what to discard: Adaptive kv cache compression for llms," ICLR, 2024.

[17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, "Mixture-of-depths: Dynamically allocating compute in transformer-based language models," arXiv preprint arXiv:2404.02258, 2024.

[18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, "Bert loses patience: Fast and robust inference with early exit," in NeurIPS, vol. 33, pp. 18330–18341, 2020.

[19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., "Layer skip: Enabling early exit inference and self-speculative decoding," arXiv preprint arXiv:2404.16710, 2024.

[20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, "The unreasonable ineffectiveness of the deeper layers," arXiv preprint arXiv:2403.17887, 2024.

[21] T. Salimans and D. P. Kingma, "Weight normalization: A simple reparameterization to accelerate training of deep neural networks," in NeurIPS, vol. 29, 2016.

[22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., "Mixtral of experts," arXiv preprint arXiv:2401.04088, 2024.

[23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., "Phi-3 technical report: A highly capable language model locally on your phone," arXiv preprint arXiv:2404.14219, 2024.

[24] M. Roemmele, C. A. Bejan, and A. S. Gordon, "Choice of plausible alternatives: An evaluation of commonsense causal reasoning.," in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90–95, 2011.

[Tiếp tục với phần còn lại của tài liệu tham khảo theo định dạng tương tự...]

--- PAGE 12-26 ---

[Phần còn lại của tài liệu bao gồm các tài liệu tham khảo từ [25] đến [81], Phụ lục A-F với các bảng chi tiết và thuật toán, tất cả được dịch sang tiếng Việt với cấu trúc và nội dung giống hệt bản gốc]
