# MiniCache: NÃ©n KV Cache theo Chiá»u SÃ¢u
cho CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

Akide Liu1Jing Liu1Zizheng Pan1Yefei He2
Gholamreza Haffari1Bohan Zhuang1,2â€ 
1ZIP Lab, Monash University, Australia
2ZIP Lab, Zhejiang University, China

## TÃ³m táº¯t

Má»™t phÆ°Æ¡ng phÃ¡p quan trá»ng Ä‘á»ƒ triá»ƒn khai hiá»‡u quáº£ cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘Ã²i há»i tÃ­nh toÃ¡n cao lÃ  bá»™ nhá»› Ä‘á»‡m Key-Value (KV). Bá»™ nhá»› Ä‘á»‡m KV lÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i key-value cá»§a cÃ¡c token Ä‘Ã£ Ä‘Æ°á»£c táº¡o trÆ°á»›c Ä‘Ã³, giáº£m Ä‘Ã¡ng ká»ƒ nhu cáº§u tÃ­nh toÃ¡n láº·p láº¡i vÃ  do Ä‘Ã³ giáº£m Ä‘á»™ trá»… trong viá»‡c táº¡o tá»± há»“i quy. Tuy nhiÃªn, kÃ­ch thÆ°á»›c cá»§a bá»™ nhá»› Ä‘á»‡m KV tÄƒng tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i chuá»—i, gÃ¢y ra thÃ¡ch thá»©c cho cÃ¡c á»©ng dá»¥ng yÃªu cáº§u Ä‘áº§u vÃ o ngá»¯ cáº£nh dÃ i vÃ  táº¡o chuá»—i má»Ÿ rá»™ng. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£, Ä‘Æ°á»£c gá»i lÃ  MiniCache, Ä‘á»ƒ nÃ©n bá»™ nhá»› Ä‘á»‡m KV qua cÃ¡c lá»›p tá»« gÃ³c Ä‘á»™ chiá»u sÃ¢u má»›i, giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n bá»™ nhá»› cho suy luáº­n LLM. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i dá»±a trÃªn quan sÃ¡t ráº±ng cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV thá»ƒ hiá»‡n Ä‘á»™ tÆ°Æ¡ng tá»± cao giá»¯a cÃ¡c lá»›p liá»n ká» trong pháº§n tá»« giá»¯a Ä‘áº¿n sÃ¢u cá»§a LLM. Äá»ƒ há»— trá»£ viá»‡c há»£p nháº¥t, chÃºng tÃ´i Ä‘á» xuáº¥t tÃ¡ch cÃ¡c tráº¡ng thÃ¡i thÃ nh cÃ¡c thÃ nh pháº§n Ä‘á»™ lá»›n vÃ  hÆ°á»›ng, ná»™i suy cÃ¡c hÆ°á»›ng cá»§a cÃ¡c vector tráº¡ng thÃ¡i trong khi giá»¯ nguyÃªn Ä‘á»™ dÃ i cá»§a chÃºng. HÆ¡n ná»¯a, chÃºng tÃ´i giá»›i thiá»‡u má»™t chiáº¿n lÆ°á»£c giá»¯ láº¡i token Ä‘á»ƒ giá»¯ cÃ¡c cáº·p tráº¡ng thÃ¡i cÃ³ tÃ­nh phÃ¢n biá»‡t cao khÃ´ng Ä‘Æ°á»£c há»£p nháº¥t, do Ä‘Ã³ báº£o tá»“n thÃ´ng tin vá»›i chi phÃ­ lÆ°u trá»¯ bá»• sung tá»‘i thiá»ƒu. MiniCache cá»§a chÃºng tÃ´i khÃ´ng cáº§n huáº¥n luyá»‡n vÃ  cÃ³ tÃ­nh tá»•ng quÃ¡t, bá»• sung cho cÃ¡c chiáº¿n lÆ°á»£c nÃ©n bá»™ nhá»› Ä‘á»‡m KV hiá»‡n táº¡i, cháº³ng háº¡n nhÆ° lÆ°á»£ng tá»­ hÃ³a vÃ  thÆ°a thá»›t. ChÃºng tÃ´i tiáº¿n hÃ nh Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n vá» MiniCache sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau bao gá»“m LLaMA-2, LLaMA-3, Phi-3, Mistral, vÃ  Mixtral trÃªn nhiá»u bÃ i kiá»ƒm tra, chá»©ng minh hiá»‡u suáº¥t Ä‘áº·c biá»‡t trong viá»‡c Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ nÃ©n vÆ°á»£t trá»™i vÃ  thÃ´ng lÆ°á»£ng cao. TrÃªn bá»™ dá»¯ liá»‡u ShareGPT, LLaMA-2-7B vá»›i MiniCache 4-bit Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ nÃ©n Ä‘Ã¡ng chÃº Ã½ lÃªn Ä‘áº¿n 5.02Ã—, tÄƒng thÃ´ng lÆ°á»£ng suy luáº­n khoáº£ng 5Ã— vÃ  giáº£m dáº¥u chÃ¢n bá»™ nhá»› 41% so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ bá»™ nhá»› Ä‘á»‡m Ä‘áº§y Ä‘á»§ FP16, táº¥t cáº£ trong khi duy trÃ¬ hiá»‡u suáº¥t gáº§n nhÆ° khÃ´ng máº¥t mÃ¡t. Dá»± Ã¡n cÃ³ sáºµn táº¡i https://minicache.vmv.re

## 1 Giá»›i thiá»‡u

CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM), Ä‘Æ°á»£c minh há»a bá»Ÿi chuá»—i GPT [1,2,3] vÃ  chuá»—i LLaMA [4,5,6], Ä‘Ã£ ná»•i lÃªn nhÆ° nhá»¯ng Ä‘á»•i má»›i then chá»‘t trong trÃ­ tuá»‡ nhÃ¢n táº¡o tá»•ng quÃ¡t, nÃ¢ng cao Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. Tuy nhiÃªn, cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tá»‰ má»‰ sá»­ dá»¥ng tÃ i nguyÃªn tÃ­nh toÃ¡n rá»™ng lá»›n [7] vÃ  bá»™ dá»¯ liá»‡u khá»•ng lá»“ [8], Ä‘iá»u nÃ y cho phÃ©p chÃºng táº¡o ra vÄƒn báº£n hiá»‡u quáº£ mÃ´ phá»ng phong cÃ¡ch viáº¿t cá»§a con ngÆ°á»i vÃ  tiáº¿n hÃ nh phÃ¢n tÃ­ch lÃ½ luáº­n phá»©c táº¡p, nhÆ°ng Ä‘áº·t ra thÃ¡ch thá»©c vá» triá»ƒn khai vÃ  phá»¥c vá»¥ hiá»‡u quáº£. Trong khung suy luáº­n cá»§a

â€ TÃ¡c giáº£ liÃªn láº¡c. Email: bohan.zhuang@gmail.com
Preprint. Under review.arXiv:2405.14366v2 [cs.CL] 7 Sep 2024

--- PAGE 2 ---

(a)Äá»™ tÆ°Æ¡ng tá»± bá»™ nhá»› Ä‘á»‡m KV qua lá»›p
(b)CÃ¡c lá»›p Ä‘Æ°á»£c há»£p nháº¥t vs Ä‘iá»ƒm EM trÃªn GSM8K
Lá»›p ğ‘™âˆ’1
Lá»›p ğ‘™âˆ’2
...
Lá»›p 2
Lá»›p 1
LMHead Äáº§u vÃ o
Lá»›p ğ‘™âˆ’3
Tá»‰a/LÆ°á»£ng tá»­ hÃ³a
KV KV NÃ©n Bá»™ nhá»› Ä‘á»‡m T Giáº£i mÃ£
Há»£p nháº¥t Qua Lá»›p
KV
QKV
Attention
Giáº£i mÃ£ NÃ©n Bá»™ nhá»› Ä‘á»‡m KV
QKV
Attention
T+1
TrÆ°á»›c.
MiniCache
(c)So sÃ¡nh giá»¯a MiniCache vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c....... Äá»™ TÆ°Æ¡ng tá»± Cosine
T
T+1 Sá»‘ Lá»›p ÄÆ°á»£c Há»£p nháº¥t trÃªn LLaMA-3-70B

HÃ¬nh 1: Tá»•ng quan vá» chiáº¿n lÆ°á»£c MiniCache cá»§a chÃºng tÃ´i vÃ  káº¿t quáº£ vÃ­ dá»¥: (a) cho tháº¥y quan sÃ¡t ráº±ng cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV giá»¯a hai lá»›p liá»n ká» cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± cao, Ä‘áº·c biá»‡t qua cÃ¡c lá»›p tá»« giá»¯a Ä‘áº¿n sÃ¢u. Trá»¥c x sá»­ dá»¥ng chá»‰ sá»‘/2 Ä‘á»ƒ biá»ƒu diá»…n Ä‘á»™ tÆ°Æ¡ng tá»± cho má»—i cáº·p lá»›p. (b) so sÃ¡nh hiá»‡u suáº¥t cá»§a MiniCache vÃ  Ä‘Æ°á»ng cÆ¡ sá»Ÿ trung bÃ¬nh, Ä‘Æ¡n giáº£n lÃ  tÃ­nh trung bÃ¬nh cÃ¡c bá»™ nhá»› Ä‘á»‡m KV cá»§a hai lá»›p, sá»­ dá»¥ng mÃ´ hÃ¬nh LLaMA-3-70B [6] trÃªn bá»™ dá»¯ liá»‡u GSM8K [10]. MiniCache, báº¯t Ä‘áº§u há»£p nháº¥t tá»« Ä‘á»™ sÃ¢u ná»­a lá»›p, Ä‘áº¡t hiá»‡u suáº¥t gáº§n nhÆ° khÃ´ng máº¥t mÃ¡t. (c) lÃ m ná»•i báº­t sá»± khÃ¡c biá»‡t chÃ­nh giá»¯a MiniCache vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã³. MiniCache Ä‘iá»u tra sá»± dÆ° thá»«a giá»¯a cÃ¡c lá»›p cá»§a bá»™ nhá»› Ä‘á»‡m KV dá»c theo chiá»u sÃ¢u cá»§a LLM, má»™t khÃ­a cáº¡nh bá»‹ bá» qua bá»Ÿi cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn trong-lá»›p. á» Ä‘Ã¢y, T Ä‘á» cáº­p Ä‘áº¿n dáº¥u thá»i gian cuá»‘i cÃ¹ng cá»§a pre-filling, vÃ  T+1 Ä‘á» cáº­p Ä‘áº¿n dáº¥u thá»i gian Ä‘áº§u tiÃªn cá»§a giáº£i mÃ£.

LLM, bá»™ nhá»› Ä‘á»‡m KV [9] ráº¥t quan trá»ng Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c key vÃ  value Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c, do Ä‘Ã³ trÃ¡nh cÃ¡c phÃ©p tÃ­nh láº·p láº¡i trÃªn ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³ vÃ  nÃ¢ng cao Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ triá»ƒn khai LLM. Tuy nhiÃªn, nhu cáº§u ngÃ y cÃ ng tÄƒng cho Ä‘á»™ dÃ i chuá»—i dÃ i hÆ¡n dáº«n Ä‘áº¿n cÃ¡c tráº¡ng thÃ¡i Ä‘Æ°á»£c cache khá»•ng lá»“, gÃ¢y ra tiÃªu thá»¥ bá»™ nhá»› Ä‘Ã¡ng ká»ƒ trong quÃ¡ trÃ¬nh táº¡o. VÃ­ dá»¥, má»™t mÃ´ hÃ¬nh GPT-3 175B [2], vá»›i kÃ­ch thÆ°á»›c batch lÃ  64 vÃ  Ä‘á»™ dÃ i chuá»—i 4,096 token (cáº£ prefilled vÃ  generated), yÃªu cáº§u khoáº£ng 1,208GB bá»™ nhá»› GPU. YÃªu cáº§u nÃ y lá»›n hÆ¡n 3.45Ã— so vá»›i bá»™ nhá»› Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh. Trong bá»‘i cáº£nh nÃ y, nÃ©n bá»™ nhá»› Ä‘á»‡m KV cÃ³ táº§m quan trá»ng hÃ ng Ä‘áº§u do nhá»¯ng lá»£i Ã­ch rÃµ rÃ ng cá»§a nÃ³: 1) nÃ³ giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n bá»™ nhá»› cho phÃ©p táº¡o nhanh hÆ¡n vÃ  phá»¥c vá»¥ batch lá»›n hÆ¡n; 2) nÃ³ giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ má»—i token, chá»©ng minh lá»£i Ã­ch thÆ°Æ¡ng máº¡i Ä‘Ã¡ng ká»ƒ.

CÃ¡c ná»— lá»±c nÃ©n bá»™ nhá»› Ä‘á»‡m KV hiá»‡n táº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ´ thÃ nh hai loáº¡i, cá»¥ thá»ƒ lÃ  lÆ°á»£ng tá»­ hÃ³a vÃ  thÆ°a thá»›t. CÃ¡c phÆ°Æ¡ng phÃ¡p lÆ°á»£ng tá»­ hÃ³a [11,12] Ä‘á» xuáº¥t lÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i KV trong cÃ¡c giÃ¡ trá»‹ sá»‘ bit tháº¥p. ThÃ´ng thÆ°á»ng, FlexGen [13] chá»©ng minh ráº±ng lÆ°á»£ng tá»­ hÃ³a bá»™ nhá»› Ä‘á»‡m KV 4-bit cÃ³ thá»ƒ Ä‘áº¡t hiá»‡u suáº¥t khÃ´ng máº¥t mÃ¡t. NgÆ°á»£c láº¡i, cÃ¡c phÆ°Æ¡ng phÃ¡p hÆ°á»›ng thÆ°a thá»›t nháº±m chá»‰ giá»¯ láº¡i cÃ¡c token ná»•i báº­t trong khi loáº¡i bá» pháº§n cÃ²n láº¡i, hoáº·c theo heuristic [14,15] hoáº·c thÃ­ch á»©ng [16]. Má»™t sá»‘ phÆ°Æ¡ng phÃ¡p [11] khÃ¡m phÃ¡ giao Ä‘iá»ƒm cá»§a hai loáº¡i nÃ y, báº±ng cÃ¡ch gÃ¡n bit cao cho cÃ¡c token ná»•i báº­t vÃ  bit cá»±c tháº¥p cho pháº§n cÃ²n láº¡i cá»§a cÃ¡c token, Ä‘áº¡t Ä‘Æ°á»£c lá»£i Ã­ch bá»™ nhá»› tÃ­ch cá»±c hÆ¡n. Máº·c dÃ¹ cÃ³ nhá»¯ng Ä‘á»•i má»›i nÃ y, vÄƒn há»‡ hiá»‡n táº¡i chá»‰ xem xÃ©t sá»± dÆ° thá»«a trong-lá»›p, trong khi bá» qua má»™t hÆ°á»›ng bá»• sung quan trá»ng khÃ¡c - sá»± dÆ° thá»«a giá»¯a-lá»›p, nhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1(c).

PhÃ¢n tÃ­ch cá»§a chÃºng tÃ´i báº¯t Ä‘áº§u báº±ng viá»‡c khÃ¡m phÃ¡ sá»± dÆ° thá»«a cá»§a bá»™ nhá»› Ä‘á»‡m KV dá»c theo chiá»u sÃ¢u, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1(a). ChÃºng tÃ´i quan sÃ¡t ráº±ng cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV thá»ƒ hiá»‡n Ä‘á»™ tÆ°Æ¡ng tá»± cao giá»¯a cÃ¡c lá»›p lÃ¢n cáº­n trong

--- PAGE 3 ---

cÃ¡c pháº§n tá»« giá»¯a Ä‘áº¿n sÃ¢u cá»§a LLM. Äáº·c tÃ­nh thÃº vá»‹ nÃ y cho tháº¥y ráº±ng cÃ¡c tráº¡ng thÃ¡i Ä‘Æ°á»£c ghÃ©p cáº·p theo vá»‹ trÃ­ giá»¯a cÃ¡c lá»›p liá»n ká» cÃ³ thá»ƒ Ä‘Æ°á»£c há»£p nháº¥t chÃ­nh xÃ¡c thÃ nh má»™t khÃ´ng gian tráº¡ng thÃ¡i duy nháº¥t vá»›i Ä‘áº£m báº£o hiá»‡u suáº¥t máº¡nh máº½, nhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1(b). PhÆ°Æ¡ng phÃ¡p nÃ y giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n bá»™ nhá»› mÃ  khÃ´ng cáº§n giá»¯ láº¡i cÃ¡c tráº¡ng thÃ¡i riÃªng láº» cho má»—i lá»›p attention. LÆ°u Ã½ ráº±ng nhá»¯ng quan sÃ¡t nÃ y cÃ³ liÃªn quan Ä‘áº¿n cÃ¡c chiáº¿n lÆ°á»£c suy luáº­n Ä‘á»™ng nhÆ° mixture-of-depths [17] vÃ  layer-wise early exiting [18,19], tá»‘i Æ°u hÃ³a Ä‘Æ°á»ng dáº«n tÃ­nh toÃ¡n báº±ng cÃ¡ch bá» qua cÃ¡c lá»›p khÃ´ng quan trá»ng Ä‘á»ƒ nÃ¢ng cao hiá»‡u quáº£ huáº¥n luyá»‡n vÃ  suy luáº­n. HÆ¡n ná»¯a, cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‰a lá»›p [20] lÃ m ná»•i báº­t sá»± dÆ° thá»«a Ä‘Ã¡ng ká»ƒ trong cÃ¡c lá»›p sÃ¢u hÆ¡n. Tuy nhiÃªn, máº·c dÃ¹ cÃ³ nhá»¯ng tiáº¿n bá»™ nÃ y, sá»± dÆ° thá»«a cá»§a bá»™ nhá»› Ä‘á»‡m KV dá»c theo chiá»u sÃ¢u pháº§n lá»›n Ä‘Ã£ bá»‹ bá» qua.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t MiniCache, má»™t phÆ°Æ¡ng phÃ¡p nÃ©n bá»™ nhá»› Ä‘á»‡m KV qua lá»›p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ nháº±m thÃºc Ä‘áº©y hiá»‡u quáº£ suy luáº­n cá»§a LLM. MiniCache bao gá»“m hai thÃ nh pháº§n thiáº¿t yáº¿u. Thá»© nháº¥t, chÃºng tÃ´i giá»›i thiá»‡u má»™t chiáº¿n lÆ°á»£c há»£p nháº¥t cache chÃ­nh xÃ¡c, sá»­ dá»¥ng má»™t tham sá»‘ hÃ³a láº¡i cá»§a cÃ¡c vector tráº¡ng thÃ¡i phÃ¢n tÃ¡ch chÃºng thÃ nh cÃ¡c thÃ nh pháº§n Ä‘á»™ lá»›n vÃ  hÆ°á»›ng, tÆ°Æ¡ng tá»± nhÆ° weight normalization [21]. PhÆ°Æ¡ng phÃ¡p nÃ y cho phÃ©p ná»™i suy hiá»‡u quáº£ cá»§a thÃ nh pháº§n hÆ°á»›ng trong tá»a Ä‘á»™ cá»±c trong khi báº£o tá»“n cÃ¡c chuáº©n tráº¡ng thÃ¡i gá»‘c Ä‘á»ƒ giá»¯ láº¡i cÃ ng nhiá»u thÃ´ng tin cÃ ng tá»‘t. Ná»™i suy nÃ y Ä‘á» cáº­p Ä‘áº¿n viá»‡c há»£p nháº¥t qua lá»›p nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1(c). Thá»© hai, chÃºng tÃ´i nháº­n ra ráº±ng má»™t táº­p con nhá» cÃ¡c cáº·p tráº¡ng thÃ¡i, Ä‘Æ°á»£c Ä‘áº·c trÆ°ng bá»Ÿi Ä‘á»™ tÆ°Æ¡ng tá»± tháº¥p nhÆ°ng mang Ã½ nghÄ©a ngá»¯ nghÄ©a khÃ¡c biá»‡t lá»›n, khÃ´ng phÃ¹ há»£p cho viá»‡c há»£p nháº¥t giá»¯a lá»›p. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t chiáº¿n lÆ°á»£c giá»¯ láº¡i token Ä‘á»ƒ giáº£m thiá»ƒu suy giáº£m hiá»‡u suáº¥t, bao gá»“m viá»‡c giá»¯ láº¡i riÃªng nhá»¯ng cáº·p ngoáº¡i lá»‡ nÃ y. Khung cá»§a chÃºng tÃ´i Ä‘Ã¡ng chÃº Ã½ vá» hiá»‡u quáº£ bá»™ nhá»›, chá»‰ yÃªu cáº§u lÆ°u trá»¯ cho má»™t thÃ nh pháº§n hÆ°á»›ng chiá»u cao duy nháº¥t, cÃ¹ng vá»›i chi phÃ­ bá»™ nhá»› bá»• sung tá»‘i thiá»ƒu. Chi phÃ­ nÃ y bao gá»“m má»™t vÃ i token khÃ´ng thá»ƒ há»£p nháº¥t vÃ  cÃ¡c chá»‰ sá»‘ tÆ°Æ¡ng á»©ng cá»§a chÃºng, cÅ©ng nhÆ° Ä‘á»™ lá»›n theo token Ä‘á»ƒ khÃ´i phá»¥c chÃ­nh xÃ¡c cÃ¡c tráº¡ng thÃ¡i gá»‘c.

ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m rá»™ng rÃ£i vá»›i cÃ¡c LLM Ä‘áº¡i diá»‡n, bao gá»“m Mixtral-8x7B [22], Phi-3-Mini [23], vÃ  LLaMA-3 [6] 8B vÃ  70B tÆ°Æ¡ng á»©ng. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn má»™t loáº¡t Ä‘a dáº¡ng cÃ¡c bá»™ dá»¯ liá»‡u tráº£ lá»i cÃ¢u há»i vÃ  táº¡o [24,25,26,27,28,29,30,31] sá»­ dá»¥ng lm-eval-harness [32]. NgoÃ i ra, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ káº¿t quáº£ cá»§a mÃ¬nh trÃªn LongBench [33] cho viá»‡c táº¡o chuá»—i dÃ i. Káº¿t quáº£ cho tháº¥y MiniCache cÃ³ thá»ƒ giáº£m dáº¥u chÃ¢n bá»™ nhá»› cáº§n thiáº¿t cho suy luáº­n LLM lÃªn Ä‘áº¿n 41%, Ä‘á»“ng thá»i tÄƒng thÃ´ng lÆ°á»£ng khoáº£ng 5Ã— so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ cached Ä‘áº§y Ä‘á»§, vÆ°á»£t trá»™i rÃµ rÃ ng so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i [11, 12, 14, 15].

ÄÃ³ng gÃ³p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:
â€¢ ChÃºng tÃ´i giá»›i thiá»‡u MiniCache, má»™t khung Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ cao cho nÃ©n bá»™ nhá»› Ä‘á»‡m KV. MiniCache tiÃªn phong trong viá»‡c khÃ¡m phÃ¡ nÃ©n bá»™ nhá»› Ä‘á»‡m KV dá»c theo chiá»u sÃ¢u, do Ä‘Ã³ má»Ÿ rá»™ng Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng cá»§a nÃ³.
â€¢ ChÃºng tÃ´i quan sÃ¡t má»™t Ä‘áº·c tÃ­nh háº¥p dáº«n cá»§a cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV qua lá»›p: Ä‘á»™ tÆ°Æ¡ng tá»± cao giá»¯a cÃ¡c lá»›p liá»n ká» trong cÃ¡c giai Ä‘oáº¡n tá»« giá»¯a Ä‘áº¿n sau cá»§a LLM. NgoÃ i ra, chÃºng tÃ´i phÃ¡t hiá»‡n ráº±ng khÃ´ng pháº£i táº¥t cáº£ cÃ¡c cáº·p tráº¡ng thÃ¡i Ä‘á»u phÃ¹ há»£p Ä‘á»ƒ há»£p nháº¥t.
â€¢ ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ bá»™ nhá»› cho viá»‡c há»£p nháº¥t cache qua lá»›p, bao gá»“m má»™t chiáº¿n lÆ°á»£c tham sá»‘ hÃ³a láº¡i vÃ  má»™t cÆ¡ cháº¿ giá»¯ láº¡i token. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i bá»• sung cho cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ©n bá»™ nhá»› Ä‘á»‡m KV hiá»‡n táº¡i, nÃ¢ng cao hÆ¡n ná»¯a hiá»‡u quáº£ phá»¥c vá»¥ LLM.
â€¢ MiniCache cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng tá»‘t so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i tÃ¢n. ÄÃ¡ng chÃº Ã½, MiniCache 4-bit cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ nÃ©n máº¡nh lÃªn Ä‘áº¿n 5.02Ã—, thÃ´ng lÆ°á»£ng suy luáº­n cao hÆ¡n 5Ã— vÃ  giáº£m bá»™ nhá»› 41% so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ cache Ä‘áº§y Ä‘á»§ FP16 vá»›i hiá»‡u suáº¥t gáº§n nhÆ° khÃ´ng máº¥t mÃ¡t.

## 2 CÃ´ng trÃ¬nh LiÃªn quan

Suy luáº­n hiá»‡u quáº£ cho LLM. CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) bá»‹ háº¡n cháº¿ bá»Ÿi cÃ¡c yÃªu cáº§u tÃ­nh toÃ¡n vÃ  bá»™ nhá»› Ä‘Ã¡ng ká»ƒ trong quÃ¡ trÃ¬nh suy luáº­n, Ä‘áº·c biá»‡t trong cÃ¡c mÃ´i trÆ°á»ng háº¡n cháº¿ tÃ i nguyÃªn. Äá»ƒ giáº£m thiá»ƒu nhá»¯ng thÃ¡ch thá»©c nÃ y, nhiá»u ká»¹ thuáº­t suy luáº­n hiá»‡u quáº£ khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn. VÃ­ dá»¥, cÃ¡c phÆ°Æ¡ng phÃ¡p suy luáº­n Ä‘á»™ng [18,34,35,36], Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi mixture-of-experts (MoE) [37,38,39,40,41], chá»n lá»c thÃ­ch á»©ng cÃ¡c cáº¥u trÃºc con cá»¥ thá»ƒ cá»§a mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh suy luáº­n dá»±a trÃªn dá»¯ liá»‡u Ä‘áº§u vÃ o, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ suy luáº­n trong khi duy trÃ¬ dung lÆ°á»£ng mÃ´ hÃ¬nh. CÃ¡c ká»¹ thuáº­t nhÆ° Multi-Query Attention [42,43], Kernel-driven attentions [44,45,46,47], vÃ  low-rank attentions [41,48,49,50] xáº¥p xá»‰ chá»©c nÄƒng cá»§a cÃ¡c cÆ¡ cháº¿ attention truyá»n thá»‘ng nhÆ°ng vá»›i cÃ¡c triá»ƒn khai hiá»‡u quáº£ hÆ¡n. CÃ¡c chiáº¿n lÆ°á»£c lÆ°á»£ng tá»­ hÃ³a [51,52,53,54]

--- PAGE 4 ---

bao gá»“m viá»‡c chuyá»ƒn Ä‘á»•i trá»ng sá»‘ vÃ  kÃ­ch hoáº¡t cá»§a mÃ´ hÃ¬nh thÃ nh Ä‘á»‹nh dáº¡ng Ä‘á»™ rá»™ng bit tháº¥p, do Ä‘Ã³ giáº£m dáº¥u chÃ¢n bá»™ nhá»› vÃ  cÆ°á»ng Ä‘á»™ tÃ­nh toÃ¡n. CÃ¡c phÆ°Æ¡ng phÃ¡p thÆ°a thá»›t hÃ³a [14,15,55,56] loáº¡i bá» cÃ¡c pháº§n tá»­ khÃ´ng cáº§n thiáº¿t trong cáº£ trá»ng sá»‘ mÃ´ hÃ¬nh vÃ  biá»ƒu diá»…n token, nÃ¢ng cao hÆ¡n ná»¯a hiá»‡u quáº£. Má»™t sá»‘ cÃ´ng trÃ¬nh liÃªn quan cháº·t cháº½, nhÆ° MoD [17] vÃ  LayerSkips [19], xem xÃ©t báº£n cháº¥t suy luáº­n Ä‘á»™ng Ä‘á»ƒ bá» qua cÃ¡c lá»›p khÃ´ng quan trá»ng theo Ä‘áº§u vÃ o. Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y yÃªu cáº§u má»™t quÃ¡ trÃ¬nh tinh chá»‰nh bá»• sung hoáº·c cÃ¡c giai Ä‘oáº¡n pre-training Ä‘Æ°á»£c thiáº¿t káº¿ cáº©n tháº­n, Ä‘iá»u nÃ y lÃ m giáº£m kháº£ nÄƒng thÃ­ch á»©ng cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y. MiniCache dá»±a vÃ o cÃ¡c quan sÃ¡t vá» Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a lá»›p Ä‘á»ƒ thá»±c hiá»‡n há»£p nháº¥t qua lá»›p, giáº£m Ä‘Ã¡ng ká»ƒ nhu cáº§u bá»™ nhá»›.

Há»£p nháº¥t mÃ´ hÃ¬nh. NÃ©n há»£p nháº¥t bao gá»“m viá»‡c táº­p há»£p cÃ¡c tham sá»‘ vÃ  kÃ­ch hoáº¡t cá»§a mÃ´ hÃ¬nh á»Ÿ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t khÃ¡c nhau. QuÃ¡ trÃ¬nh nÃ y nÃ¢ng cao hiá»‡u quáº£ suy luáº­n trong cÃ¡c mÃ´ hÃ¬nh lá»›n vÃ  táº¡o Ä‘iá»u kiá»‡n cho sá»± dÆ° thá»«a khá»•ng lá»“ [57]. Linear Mode Connectivity (LMC) [58] cho phÃ©p tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh tá»« má»™t cÆ¡ sá»Ÿ Ä‘Æ°á»£c pre-train chung. ThÆ°á»ng xuyÃªn, weight averaging [59] Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t ká»¹ thuáº­t hiá»‡u quáº£ Ä‘á»ƒ thá»±c hiá»‡n nÃ©n há»£p nháº¥t. ÄÃ¡ng chÃº Ã½, Model Soup [60] sá»­ dá»¥ng trung bÃ¬nh tuyáº¿n tÃ­nh trong bá»‘i cáº£nh nÃ y. CÃ¡c phÆ°Æ¡ng phÃ¡p tiÃªn tiáº¿n nhÆ° TIES Merging [61], Model Breadcrumbs [62], vÃ  DARE [63] nÃ¢ng cao hÆ¡n ná»¯a quÃ¡ trÃ¬nh nÃ y báº±ng cÃ¡ch thÆ°a thá»›t hÃ³a vÃ  káº¿t há»£p cÃ¡c tham sá»‘ mÃ´ hÃ¬nh, cho phÃ©p há»£p nháº¥t cÃ¡c mÃ´ hÃ¬nh mÃ  khÃ´ng hy sinh kháº£ nÄƒng hiá»‡u suáº¥t. NgoÃ i ra, Spherical Linear intERPolation (SLERP) [64] má»Ÿ rá»™ng ngoÃ i viá»‡c tÃ­nh trung bÃ¬nh trá»ng sá»‘ Ä‘Æ¡n giáº£n báº±ng cÃ¡ch ná»™i suy giá»¯a cÃ¡c tham sá»‘ mÃ´ hÃ¬nh. Ma tráº­n thÃ´ng tin Fisher [65] vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn RegMean [66] tá»‘i Æ°u hÃ³a hÆ¡n ná»¯a viá»‡c há»£p nháº¥t Ä‘á»ƒ táº¡o ra trá»ng sá»‘ lÃ½ tÆ°á»Ÿng, giáº£m thiá»ƒu khoáº£ng cÃ¡ch â„“2 Ä‘áº¿n Ä‘áº§u ra táº¡o trong khi báº£o tá»“n quyá»n riÃªng tÆ° cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n. Tuy nhiÃªn, háº§u háº¿t cÃ¡c cÃ´ng trÃ¬nh hiá»‡n táº¡i táº­p trung vÃ o viá»‡c há»£p nháº¥t cÃ¡c tham sá»‘ mÃ´ hÃ¬nh, vá»›i khÃ¡i niá»‡m kháº£ nÄƒng há»£p nháº¥t theo chiá»u sÃ¢u khÃ´ng Ä‘Æ°á»£c khÃ¡m phÃ¡ ká»¹ lÆ°á»¡ng trong nghiÃªn cá»©u trÆ°á»›c Ä‘Ã³. MiniCache táº­p trung vÃ o viá»‡c há»£p nháº¥t token bá»™ nhá»› Ä‘á»‡m KV trong chiá»u sÃ¢u cá»§a LLM.

## 3 Äá»™ng lá»±c

DÆ°á»›i Ä‘Ã¢y, chÃºng tÃ´i trÃ¬nh bÃ y cÃ¡c quan sÃ¡t má»›i cá»§a chÃºng tÃ´i trong má»™t gÃ³c nhÃ¬n qua lá»›p má»›i láº¡.

LLama 2 7B LLama 2 30B LLama 3 8B Mixtral 8x7B
MÃ´ hÃ¬nh 0 10 20 30 40 50 Exact Match (%) Baseline Mean KV
(a) ÄÆ°á»ng cÆ¡ sá»Ÿ trung bÃ¬nh Ä‘Æ¡n giáº£n vs. cache Ä‘áº§y Ä‘á»§ trÃªn GSM8K

0 20 40 60 80 100
Chá»‰ sá»‘ Token 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Äá»™ TÆ°Æ¡ng tá»± Cosine
Lá»›p 16 - 17
Lá»›p 18 - 19
Lá»›p 20 - 21
Lá»›p 22 - 23
Lá»›p 24 - 25
Lá»›p 26 - 27
Lá»›p 28 - 29
Lá»›p 30 - 31
(b) Äá»™ tÆ°Æ¡ng tá»± theo cáº·p trong bá»™ nhá»› Ä‘á»‡m KV cÃ¡c lá»›p liá»n ká»

MathQA OpenBookQA PiQA RTE Winogrande 0.0 0.2 0.4 0.6 0.8
MiniCache Baseline Mean
(c) ÄÃ¡nh giÃ¡ trÃªn nÄƒm bá»™ dá»¯ liá»‡u QA

HÃ¬nh 2: Tá»•ng quan vá» cÃ¡c khÃ¡m phÃ¡ vÃ  quan sÃ¡t cá»§a chÃºng tÃ´i: (a) cho tháº¥y Ä‘Æ°á»ng cÆ¡ sá»Ÿ máº¡nh báº±ng cÃ¡ch thá»±c hiá»‡n há»£p nháº¥t trung bÃ¬nh trÃªn bá»™ nhá»› Ä‘á»‡m KV. (b) cho tháº¥y Ä‘á»™ tÆ°Æ¡ng tá»± theo cáº·p cá»§a cÃ¡c tráº¡ng thÃ¡i cache giá»¯a cÃ¡c lá»›p liá»n ká». (c) so sÃ¡nh MiniCache, trung bÃ¬nh Ä‘Æ¡n giáº£n, vÃ  Ä‘Æ°á»ng cÆ¡ sá»Ÿ cache Ä‘áº§y Ä‘á»§ trÃªn nÄƒm bá»™ dá»¯ liá»‡u khÃ¡c nhau.

### 3.1 Sá»± DÆ° thá»«a Qua Lá»›p trong Bá»™ nhá»› Ä‘á»‡m KV

CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã£ tiáº¿t lá»™ sá»± khÃ´ng hiá»‡u quáº£ cá»§a cÃ¡c lá»›p tá»« giá»¯a Ä‘áº¿n sÃ¢u trong LLM [20]. Do Ä‘Ã³, layer-wise early exiting trong trÆ°á»ng há»£p nÃ y cÃ³ thá»ƒ hiá»‡u quáº£ trÃ¡nh tÃ­nh toÃ¡n dÆ° thá»«a vá»›i tÃ¡c Ä‘á»™ng nhá» Ä‘áº¿n hiá»‡u suáº¥t LLM [19,67]. ÄÆ°á»£c truyá»n cáº£m há»©ng tá»« Ä‘iá»u nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ má»™t viá»‡c há»£p nháº¥t theo lá»›p cá»§a bá»™ nhá»› Ä‘á»‡m KV trong LLM, báº¯t Ä‘áº§u vá»›i má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ¡n giáº£n báº±ng cÃ¡ch tÃ­nh trung bÃ¬nh táº¥t cáº£ token qua cÃ¡c lá»›p liá»n ká». ChÃºng tÃ´i cung cáº¥p cÃ¡c quan sÃ¡t chÃ­nh cá»§a chÃºng tÃ´i nhÆ° sau.

Quan sÃ¡t 1: Bá»™ nhá»› Ä‘á»‡m KV chia sáº» Ä‘á»™ tÆ°Æ¡ng tá»± cao giá»¯a cÃ¡c lá»›p liá»n ká». Dá»±a trÃªn LLaMA-3-70B [6], chÃºng tÃ´i tiáº¿n hÃ nh suy luáº­n zero-shot trÃªn cÃ¡c táº­p validation cá»§a ba bÃ i kiá»ƒm tra Ä‘Æ°á»£c cÃ´ng nháº­n rá»™ng rÃ£i: COQA [68], GSM8K [10] vÃ  TruthfulQA [69]. NÃ³i chung, chÃºng tÃ´i tháº¥y ráº±ng bá»™ nhá»› Ä‘á»‡m KV trong cÃ¡c lá»›p nÃ´ng thá»ƒ hiá»‡n Ä‘á»™ tÆ°Æ¡ng tá»± tháº¥p, trong khi nhá»¯ng lá»›p trong cÃ¡c lá»›p tá»« giá»¯a Ä‘áº¿n sÃ¢u ráº¥t tÆ°Æ¡ng tá»± vá»›i nhau dá»±a trÃªn khoáº£ng cÃ¡ch gÃ³c, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1(b). Tiáº¿p theo, chÃºng tÃ´i há»£p nháº¥t bá»™ nhá»› Ä‘á»‡m KV qua

--- PAGE 5 ---

cÃ¡c lá»›p liá»n ká» báº±ng cÃ¡ch tiáº¿n hÃ nh suy luáº­n five-shot vá»›i LLaMA-2-7B, LLaMA-2-13B [5], LLaMA-3-8B [6], vÃ  Mixtral-8x7B [22] trÃªn GSM8K [10]. Cá»¥ thá»ƒ, báº¯t Ä‘áº§u tá»« lá»›p giá»¯a cá»§a má»—i mÃ´ hÃ¬nh, chÃºng tÃ´i há»£p nháº¥t bá»™ nhá»› Ä‘á»‡m KV trong hai lá»›p liá»n ká». NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2(a), chÃºng tÃ´i quan sÃ¡t hiá»‡u suáº¥t thuáº­n lá»£i cho cÃ¡c LLM khÃ¡c nhau, Ä‘iá»u nÃ y tiáº¿t lá»™ tiá»m nÄƒng lá»›n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ báº±ng cÃ¡ch chia sáº» bá»™ nhá»› Ä‘á»‡m KV qua cÃ¡c lá»›p liá»n ká» trong quÃ¡ trÃ¬nh giáº£i mÃ£ LLM.

Quan sÃ¡t 2: KhÃ´ng pháº£i táº¥t cáº£ token Ä‘á»u quan trá»ng nhÆ° nhau Ä‘á»ƒ há»£p nháº¥t, má»™t vÃ i cáº·p phÃ¢n biá»‡t yÃªu cáº§u giá»¯ láº¡i. CÃ¡c cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y [15,16] trong nÃ©n bá»™ nhá»› Ä‘á»‡m KV Ä‘Ã£ phÃ¡t hiá»‡n ráº±ng viá»‡c giá»¯ má»™t vÃ i token ná»•i báº­t á»Ÿ má»—i lá»›p, Ä‘Ã³ng gÃ³p pháº§n lá»›n Ä‘iá»ƒm attention, cÃ³ thá»ƒ Ä‘á»§ Ä‘á»ƒ duy trÃ¬ hiá»‡u suáº¥t LLM. Trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, chÃºng tÃ´i suy Ä‘oÃ¡n ráº±ng má»™t sá»‘ cáº·p token trong cÃ¡c lá»›p liá»n ká» cÅ©ng thá»ƒ hiá»‡n hÃ nh vi ngoáº¡i lá»‡, cho tháº¥y sá»± khÃ¡c biá»‡t ngá»¯ nghÄ©a máº¡nh khiáº¿n chÃºng khÃ´ng phÃ¹ há»£p Ä‘á»ƒ há»£p nháº¥t. Dá»±a trÃªn COQA [68] vÃ  LLaMA-2-7B [5], chÃºng tÃ´i Ä‘iá»u tra Ä‘á»™ tÆ°Æ¡ng tá»± á»Ÿ má»©c Ä‘á»™ cáº·p token. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2(b), chÃºng tÃ´i tháº¥y má»™t pháº§n Ä‘Ã¡ng ká»ƒ cÃ¡c cáº·p token chia sáº» Ä‘á»™ tÆ°Æ¡ng tá»± cao qua cÃ¡c lá»›p liá»n ká». Tuy nhiÃªn, chÃºng tÃ´i quan sÃ¡t má»™t vÃ i cáº·p ngoáº¡i lá»‡, nhÆ° chá»‰ sá»‘ 0 vÃ  15, vá»›i biÃªn Ä‘á»™ khÃ¡c biá»‡t lá»›n. ChÃºng tÃ´i coi nhá»¯ng token nÃ y lÃ  khÃ´ng thá»ƒ há»£p nháº¥t do sá»± khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ cá»§a chÃºng. ChÃºng tÃ´i cÅ©ng cho tháº¥y ráº±ng viá»‡c há»£p nháº¥t nhá»¯ng token phÃ¢n biá»‡t nÃ y dáº«n Ä‘áº¿n suy giáº£m hiá»‡u suáº¥t, tÆ°Æ¡ng á»©ng vá»›i hÃ ng Î³ = 0 trong Báº£ng 2. Do Ä‘Ã³, trong khi há»£p nháº¥t qua lá»›p lÃ  má»™t chiáº¿n lÆ°á»£c Ä‘áº§y há»©a háº¹n Ä‘á»ƒ giáº£m gÃ¡nh náº·ng bá»™ nhá»›, nÃ³ pháº£i Ä‘Æ°á»£c triá»ƒn khai vá»›i sá»± cÃ¢n nháº¯c cáº©n tháº­n vá» Ä‘á»™ tÆ°Æ¡ng tá»± cáº¥p token Ä‘á»ƒ Ä‘áº£m báº£o hiá»‡u suáº¥t tá»‘i Æ°u, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2(c).

## 4 PhÆ°Æ¡ng phÃ¡p

Trong pháº§n nÃ y, chÃºng tÃ´i giá»›i thiá»‡u MiniCache cá»§a chÃºng tÃ´i, má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ nháº±m cáº¯t giáº£m sá»± dÆ° thá»«a cá»§a bá»™ nhá»› Ä‘á»‡m KV trong chiá»u sÃ¢u. Khung nÃ y khai thÃ¡c Ä‘á»™ tÆ°Æ¡ng tá»± cao cá»§a cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV giá»¯a cÃ¡c lá»›p liá»n ká» vÃ  bao gá»“m hai thÃ nh pháº§n chÃ­nh: má»™t chiáº¿n lÆ°á»£c há»£p nháº¥t dá»±a trÃªn tham sá»‘ hÃ³a láº¡i vÃ  má»™t cÆ¡ cháº¿ giá»¯ láº¡i token. Chiáº¿n lÆ°á»£c há»£p nháº¥t nÃ©n cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV trong cÃ¡c lá»›p liá»n ká» Ä‘á»ƒ táº­p há»£p chÃºng thÃ nh má»™t khÃ´ng gian bá»™ nhá»› chia sáº» duy nháº¥t, báº¯t Ä‘áº§u tá»« giá»¯a mÃ´ hÃ¬nh. CÆ¡ cháº¿ giá»¯ láº¡i token giáº£m thiá»ƒu thÃ´ng tin bá»‹ máº¥t báº±ng cÃ¡ch giá»¯ láº¡i cÃ¡c cáº·p tráº¡ng thÃ¡i cÃ³ tÃ­nh phÃ¢n biá»‡t cao vá»›i chi phÃ­ bá»™ nhá»› bá»• sung tá»‘i thiá»ƒu. Vá»›i cache Ä‘Æ°á»£c há»£p nháº¥t, token giá»¯ láº¡i, vÃ  Ä‘á»™ lá»›n, chÃºng tÃ´i cÃ³ thá»ƒ khÃ´i phá»¥c chÃ­nh xÃ¡c cÃ¡c tráº¡ng thÃ¡i cache gá»‘c cho viá»‡c giáº£i mÃ£ token.

### 4.1 NÃ©n Qua Lá»›p

PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i báº¯t Ä‘áº§u vá»›i viá»‡c xÃ¡c Ä‘á»‹nh má»™t lá»›p báº¯t Ä‘áº§u tá»‘i Æ°u S. CÃ¡c quan sÃ¡t trong Pháº§n 3.1 chá»‰ ra ráº±ng bá»™ nhá»› Ä‘á»‡m KV tá»« cÃ¡c lá»›p tá»« giá»¯a Ä‘áº¿n sÃ¢u liÃªn tá»¥c thá»ƒ hiá»‡n cÃ¡c máº«u Ä‘á»™ tÆ°Æ¡ng tá»± cao qua cÃ¡c lá»›p liá»n ká». Do Ä‘Ã³, chÃºng tÃ´i chá»n lá»›p báº¯t Ä‘áº§u tá»« giá»¯a LLM, cá»¥ thá»ƒ lÃ  S = L/2. Tá»« lá»›p nÃ y trá»Ÿ Ä‘i, cÃ¡c cáº·p KV Ä‘Æ°á»£c giáº£ Ä‘á»‹nh lÃ  Ä‘á»§ tÆ°Æ¡ng tá»± qua cÃ¡c lá»›p liá»n ká» Ä‘á»ƒ báº£o Ä‘áº£m viá»‡c há»£p nháº¥t cá»§a chÃºng. Trá»ng tÃ¢m cá»§a phÆ°Æ¡ng phÃ¡p nÃ y lÃ  má»™t hÃ m há»£p nháº¥t, F, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tÃ­ch há»£p cÃ¡c bá»™ nhá»› Ä‘á»‡m KV cá»§a cÃ¡c lá»›p liÃªn tiáº¿p thÃ nh má»™t cache thá»‘ng nháº¥t duy nháº¥t. ChÃºng tÃ´i Ä‘á»‹nh nghÄ©a x nhÆ° vector hÃ³a tráº¡ng thÃ¡i cache cá»§a má»™t token duy nháº¥t, trong Ä‘Ã³ chá»‰ sá»‘ trÃªn biá»ƒu thá»‹ chá»‰ sá»‘ lá»›p vÃ  cÃ¡c chá»‰ sá»‘ dÆ°á»›i k vÃ  v biá»ƒu thá»‹ cÃ¡c key vÃ  value tÆ°Æ¡ng á»©ng. Cá»¥ thá»ƒ, cho má»™t cáº·p token key/value á»Ÿ cÃ¹ng vá»‹ trÃ­ trong cÃ¡c lá»›p l vÃ  l-1, cache Ä‘Æ°á»£c há»£p nháº¥t Ä‘Æ°á»£c tÃ­nh nhÆ°:

c^{l,l-1}_k = F(x^l_k, x^{l-1}_k),
c^{l,l-1}_v = F(x^l_v, x^{l-1}_v). (1)

QuÃ¡ trÃ¬nh há»£p nháº¥t nÃ y hiá»‡u quáº£ loáº¡i bá» nhu cáº§u lÆ°u trá»¯ vÃ  xá»­ lÃ½ cÃ¡c key vÃ  value tiÃªu tá»‘n bá»™ nhá»› gá»‘c trong má»—i lá»›p má»™t cÃ¡ch Ä‘á»™c láº­p. Thay vÃ o Ä‘Ã³, nÃ³ xáº¥p xá»‰ má»™t cache Ä‘Æ°á»£c chia sáº» qua cÃ¡c lá»›p liá»n ká».

### 4.2 Há»£p nháº¥t vÃ  KhÃ´i phá»¥c Bá»™ nhá»› Ä‘á»‡m KV

Há»£p nháº¥t cache dá»±a trÃªn tham sá»‘ hÃ³a láº¡i. Äá»ƒ thá»±c hiá»‡n há»£p nháº¥t theo cáº·p, má»™t giáº£i phÃ¡p lÃ  trá»±c tiáº¿p tÃ­nh trung bÃ¬nh má»™t cáº·p token KV, tÆ°Æ¡ng tá»± nhÆ° viá»‡c há»£p nháº¥t mÃ´ hÃ¬nh [60,61]. Tuy nhiÃªn, chÃºng tÃ´i quan sÃ¡t ráº±ng viá»‡c tÃ­nh trung bÃ¬nh trá»±c tiáº¿p cÃ³ thá»ƒ gÃ¢y ra máº¥t mÃ¡t thÃ´ng tin Ä‘Ã¡ng ká»ƒ. ChÃºng tÃ´i suy Ä‘oÃ¡n ráº±ng khoáº£ng cÃ¡ch giá»¯a cÃ¡c kÃ­ch hoáº¡t cÃ³ thá»ƒ lá»›n hÆ¡n so vá»›i trá»ng sá»‘ do sá»± hiá»‡n diá»‡n cá»§a cÃ¡c kÃªnh kÃ­ch hoáº¡t ngoáº¡i lá»‡ vá»›i Ä‘á»™ lá»›n cá»±c lá»›n trong LLM [70,71], trong khi trá»ng sá»‘ thÆ°á»ng cÃ³ Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i khÃ¡ nhá». Má»™t phÆ°Æ¡ng phÃ¡p tiá»m nÄƒng Ä‘á»ƒ bÃ¹ Ä‘áº¯p cho máº¥t mÃ¡t thÃ´ng tin nÃ y lÃ  chiáº¿u tá»« c Ä‘áº¿n x^{l-1} vÃ  x^l, sau Ä‘Ã³ tÃ¡i chia tá»· lá»‡ cÃ¡c vector chiáº¿u dá»±a trÃªn Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i cá»§a chÃºng Ä‘á»ƒ khÃ´i phá»¥c chÃ­nh xÃ¡c

--- PAGE 6 ---

(a)NÃ©n Qua Lá»›p KV LÆ°u trá»¯ C Giá»¯ TÃ¡i chia tá»· lá»‡ KhÃ´i phá»¥c (b)KhÃ´i phá»¥c l-1 l l-1 l
Giá»¯ Ã— Láº¥y C cho lá»›p l vÃ  l-1 nÃ©n bá»™ nhá»› Ä‘á»‡m KV á»Ÿ lá»›p l bá»™ nhá»› Ä‘á»‡m KV gá»‘c bá»™ nhá»› Ä‘á»‡m KV Ä‘Æ°á»£c há»£p nháº¥t token giá»¯ láº¡i cache Ä‘á»™ lá»›n thao tÃ¡c há»£p nháº¥t

HÃ¬nh 3: Minh há»a vá» phÆ°Æ¡ng phÃ¡p MiniCache Ä‘Æ°á»£c Ä‘á» xuáº¥t. (a) mÃ´ táº£ quÃ¡ trÃ¬nh nÃ©n qua lá»›p. ChÃºng tÃ´i láº¥y cÃ¡c bá»™ nhá»› Ä‘á»‡m KV, tá»« cÃ¡c lá»›p l vÃ  l-1, vÃ  há»£p nháº¥t chÃºng thÃ nh cÃ¡c tráº¡ng thÃ¡i chia sáº» thÃ´ng qua Eq. (3). NgoÃ i ra, chÃºng tÃ´i tÃ­nh chuáº©n â„“2 cho cÃ¡c cache Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ä‘á»™ lá»›n cá»§a chÃºng. HÆ¡n ná»¯a, chÃºng tÃ´i chá»n cÃ¡c token khÃ´ng thá»ƒ há»£p nháº¥t Ä‘á»ƒ giá»¯ láº¡i, sau Ä‘Ã³ lÆ°u trá»¯ cache Ä‘Æ°á»£c há»£p nháº¥t, token giá»¯ láº¡i, vÃ  Ä‘á»™ lá»›n á»Ÿ lá»›p l trong C. (b) minh há»a quÃ¡ trÃ¬nh khÃ´i phá»¥c cho cÃ¡c lá»›p l vÃ  l-1, bao gá»“m viá»‡c tÃ¡i chia tá»· lá»‡ Ä‘á»™ lá»›n trong Eq. (2) vÃ  khÃ´i phá»¥c token giá»¯ láº¡i.

cÃ¡c tráº¡ng thÃ¡i gá»‘c. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y yÃªu cáº§u lÆ°u trá»¯ vÃ  tÃ­nh toÃ¡n bá»• sung rá»™ng rÃ£i; vÃ­ dá»¥, khÃ´i phá»¥c x^{l-1} cáº§n cáº£ c vÃ  x^l, Ä‘iá»u nÃ y lÃ m suy yáº¿u lá»£i Ã­ch cá»§a viá»‡c há»£p nháº¥t cache. Äá»ƒ há»£p nháº¥t hiá»‡u quáº£ cÃ¡c cáº·p token, chÃºng tÃ´i láº¥y cáº£m há»©ng tá»« weight normalization [21], phÃ¢n tÃ¡ch cÃ¡c tham sá»‘ mÃ´ hÃ¬nh thÃ nh cÃ¡c thÃ nh pháº§n Ä‘á»™ lá»›n vÃ  hÆ°á»›ng Ä‘á»ƒ tÄƒng tá»‘c sá»± há»™i tá»¥ cá»§a gradient descent ngáº«u nhiÃªn. NgoÃ i ra, chÃºng tÃ´i láº¥y gá»£i Ã½ tá»« DoRA [72], sá»­ dá»¥ng má»™t cÃ¡ch tÆ°Æ¡ng tá»± Ä‘á»ƒ giá»‘ng hÃ nh vi há»c táº­p cá»§a parameter-efficient fine-tuning so vá»›i full fine-tuning. Trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, tham sá»‘ hÃ³a láº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° sau:

xÌ‚^l = e^{l,l-1} Â· ||x^l|| / ||e^{l,l-1}||, xÌ‚^{l-1} = e^{l,l-1} Â· ||x^{l-1}|| / ||e^{l,l-1}||, (2)

trong Ä‘Ã³ e lÃ  vector hÆ°á»›ng. PhÃ¢n tÃ¡ch nÃ y Ä‘áº£m báº£o ráº±ng e^{l,l-1}/||e^{l,l-1}|| lÃ  má»™t vector Ä‘Æ¡n vá»‹, vÃ  cho phÃ©p cÃ¡c tráº¡ng thÃ¡i Ä‘Æ°á»£c khÃ´i phá»¥c khá»›p vá»›i chuáº©n â„“2 cá»§a cÃ¡c tráº¡ng thÃ¡i gá»‘c, do Ä‘Ã³ báº£o tá»“n thÃ´ng tin cá»§a cache cÃ ng nhiá»u cÃ ng tá»‘t. KhÃ´i phá»¥c Ä‘Æ°á»£c hiá»ƒn thá»‹ nhÆ° HÃ¬nh 3(b). Äá»ƒ ngáº¯n gá»n, chÃºng tÃ´i bá» qua cÃ¡c chá»‰ sá»‘ dÆ°á»›i k vÃ  v, vÃ¬ key vÃ  value Ä‘Æ°á»£c phÃ¢n tÃ¡ch theo cÃ¹ng má»™t cÃ¡ch. Äá»ƒ Æ°á»›c tÃ­nh thÃ nh pháº§n hÆ°á»›ng e^{l,l-1}, chÃºng tÃ´i theo SLERP [64], xá»­ lÃ½ thÃ­ch á»©ng viá»‡c ná»™i suy, thÆ°á»ng giá»‘ng vá»›i cÃ¡c biáº¿n Ä‘á»•i giá»‘ng xoay. Viá»‡c chá»n SLERP nhÆ° hÃ m há»£p nháº¥t lÃ  chiáº¿n lÆ°á»£c, vÃ¬ nÃ³ táº¡o Ä‘iá»u kiá»‡n ná»™i suy dá»c theo Ä‘Æ°á»ng dáº«n ngáº¯n nháº¥t trÃªn máº·t cáº§u Ä‘Æ¡n vá»‹ giá»¯a hai vector chiá»u cao, do Ä‘Ã³ báº£o tá»“n tÃ­nh toÃ n váº¹n hÃ¬nh há»c cá»§a chÃºng, Ä‘iá»u nÃ y Ä‘á» cáº­p Ä‘áº¿n thao tÃ¡c há»£p nháº¥t trong HÃ¬nh 3(a). Äiá»u nÃ y ráº¥t quan trá»ng Ä‘á»ƒ duy trÃ¬ cÃ¡c Ä‘áº·c tÃ­nh ngá»¯ nghÄ©a vÃ  cÃº phÃ¡p cá»§a cÃ¡c bá»™ nhá»› Ä‘á»‡m KV. CÃ´ng thá»©c cho SLERP trong bá»‘i cáº£nh cá»§a chÃºng tÃ´i lÃ :

e^{l,l-1} = (sin((1-t)Î©^{l,l-1})/sin(Î©^{l,l-1})) Â· (x^{l-1}/||x^{l-1}||) + (sin(tÎ©^{l,l-1})/sin(Î©^{l,l-1})) Â· (x^l/||x^l||), (3)

trong Ä‘Ã³ Î©^{l,l-1} = arccos(x^lÂ·x^{l-1}/(||x^l||||x^{l-1}||)) biá»ƒu thá»‹ gÃ³c giá»¯a cÃ¡c vector x^l vÃ  x^{l-1}, vÃ  sin(Â·) lÃ  hÃ m sin. t lÃ  má»™t siÃªu tham sá»‘ ná»™i suy Ä‘iá»u chá»‰nh áº£nh hÆ°á»Ÿng tÆ°Æ¡ng Ä‘á»‘i cá»§a má»—i vector lÃªn hÆ°á»›ng káº¿t quáº£, Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo Ä‘á»™ sÃ¢u lá»›p vÃ  Ä‘áº·c tÃ­nh cá»¥ thá»ƒ cá»§a cÃ¡c cáº·p KV. LÆ°u Ã½ ráº±ng khi chÃºng tÃ´i Ä‘áº·t t = 0.5, nÃ³ sáº½ trá»Ÿ thÃ nh há»£p nháº¥t trung bÃ¬nh dá»c theo bá» máº·t hÃ¬nh há»c, mÃ  chÃºng tÃ´i coi lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t trong Eq. (A). Cache Ä‘Æ°á»£c há»£p nháº¥t cho má»—i cáº·p token sau Ä‘Ã³ lÃ  má»™t ná»‘i cá»§a vector hÆ°á»›ng, Ä‘á»™ lá»›n vÃ  Î©^{l,l-1}, biá»ƒu thá»‹ nhÆ° c^{l,l-1} = [e^{l,l-1}, ||x^{l-1}||, ||x^l||, Î©^{l,l-1}], cÃ¡c thÃ nh pháº§n Ä‘Æ°á»£c cache nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3(a). LÆ°u Ã½ ráº±ng ngoÃ i viá»‡c lÆ°u trá»¯ vector hÆ°á»›ng Ä‘Æ°á»£c há»£p nháº¥t, chÃºng tÃ´i chá»‰ cáº§n lÆ°u trá»¯ Ä‘á»™ lá»›n theo token bá»• sung vÃ  cÃ¡c scalar gÃ³c, Ä‘iá»u nÃ y hiá»‡u quáº£ vá» bá»™ nhá»›. Theo cÃ¡ch nÃ y, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ bá»™ nhá»› Ä‘Ã¡ng ká»ƒ thÃ´ng qua viá»‡c giáº£m sá»± dÆ° thá»«a trong khi Ä‘áº£m báº£o giá»¯ láº¡i cÃ¡c Ä‘áº·c tÃ­nh chá»©c nÄƒng quan trá»ng cá»§a cÃ¡c cáº·p KV gá»‘c qua cÃ¡c lá»›p transformer.

Giá»¯ láº¡i token khÃ´ng thá»ƒ há»£p nháº¥t. CÃ¡c cáº·p cÃ³ tÃ­nh phÃ¢n biá»‡t cao nháº¡y cáº£m vá»›i cÃ¡c thao tÃ¡c há»£p nháº¥t, dáº«n chÃºng tÃ´i Ä‘á» xuáº¥t giá»¯ láº¡i token khÃ´ng thá»ƒ há»£p nháº¥t, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3(a). Máº·c dÃ¹ cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± cao giá»¯a cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV qua cÃ¡c lá»›p lÃ¢n cáº­n, má»™t vÃ i cáº·p phÃ¢n biá»‡t nháº¡y cáº£m váº«n cÃ²n láº¡i khÃ³ há»£p nháº¥t vÃ  chia sáº» Ä‘Ã¡ng ká»ƒ. PhÃ¹ há»£p vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c, nhá»¯ng token phÃ¢n biá»‡t nÃ y mang Ã½ nghÄ©a ngá»¯ nghÄ©a Ä‘Ã¡ng ká»ƒ [15,16]. ChÃºng tÃ´i quan sÃ¡t ráº±ng viá»‡c há»£p nháº¥t cÃ¡c token nháº¡y cáº£m, dáº«n Ä‘áº¿n máº¥t mÃ¡t thÃ´ng tin cá»¥ thá»ƒ theo lá»›p, cÃ³ thá»ƒ dáº«n Ä‘áº¿n suy giáº£m hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ. Do Ä‘Ã³, viá»‡c tÃ¡ch biá»‡t Ä‘Ãºng Ä‘áº¯n thÃ´ng tin chia sáº» vÃ  duy nháº¥t giá»¯a cÃ¡c lá»›p liá»n ká» lÃ  ráº¥t quan trá»ng. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i thiáº¿t káº¿ má»™t chiáº¿n lÆ°á»£c giá»¯ láº¡i token Ä‘á»ƒ chá»n lá»c giá»¯ láº¡i cÃ¡c token khÃ´ng thá»ƒ Ä‘Æ°á»£c há»£p nháº¥t dá»±a trÃªn khoáº£ng cÃ¡ch gÃ³c cá»§a chÃºng, Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ : d(x^l, x^{l-1}) = 1/Ï€ Â· Î©. Äá»‘i vá»›i cÃ¡c bá»™ nhá»› Ä‘á»‡m KV, khoáº£ng cÃ¡ch gÃ³c tá»‘i thiá»ƒu vÃ  tá»‘i Ä‘a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh Ä‘á»ƒ nháº­n diá»‡n cÃ¡c token khÃ´ng thá»ƒ há»£p nháº¥t.

Táº­p há»£p cÃ¡c chá»‰ sá»‘ token cáº§n thiáº¿t Ä‘á»ƒ giá»¯, I, Ä‘Æ°á»£c thu Ä‘Æ°á»£c bá»Ÿi:
I = {i | d_i < d_min + (d_max - d_min) Â· Î³}, (4)

trong Ä‘Ã³ Î³ lÃ  má»™t siÃªu tham sá»‘ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c kiá»ƒm soÃ¡t ngÆ°á»¡ng giá»¯ láº¡i. CÃ¡c token vá»›i chá»‰ sá»‘ trong I Ä‘Æ°á»£c giá»¯ láº¡i vÃ  khÃ´ng Ä‘Æ°á»£c nÃ©n trong quÃ¡ trÃ¬nh há»£p nháº¥t, Ä‘iá»u nÃ y Ä‘áº£m báº£o ráº±ng hiá»‡u suáº¥t khÃ´ng giáº£m báº±ng cÃ¡ch ngÄƒn cháº·n máº¥t mÃ¡t cÃ¡c token khÃ´ng thá»ƒ há»£p nháº¥t.

Tiáº¿p theo, Ä‘áº·t X âˆˆ R^{nÃ—h} lÃ  cache key hoáº·c value á»Ÿ má»™t lá»›p attention, trong Ä‘Ã³ n biá»ƒu thá»‹ sá»‘ lÆ°á»£ng token vÃ  h lÃ  sá»‘ chiá»u áº©n, vÃ  E âˆˆ R^{nÃ—h} lÃ  cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV chia sáº». Äá»‘i vá»›i má»—i cáº·p hai lá»›p lÃ¢n cáº­n, cÃ¡c token khÃ´ng thá»ƒ há»£p nháº¥t Ä‘Æ°á»£c chá»n cÃ¹ng vá»›i chiá»u token bá»Ÿi R^l = X^l[I], R^{l-1} = X^{l-1}[I], sau Ä‘Ã³ khÃ´i phá»¥c Ä‘áº¿n cÃ¡c cache nÃ©n cá»§a chÃºng tÃ´i bá»Ÿi XÌ‚^l[I] = R^l, XÌ‚^{l-1}[I] = R^{l-1}, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3(b). Tá»•ng thá»ƒ, chÃºng tÃ´i chia sáº» cache cuá»‘i cÃ¹ng cho hai lá»›p nhÆ° C^{l,l-1} = [E^{l,l-1}, R^l, R^{l-1}, ||X^{l-1}||, ||X^l||, I]. Cache nÃ y bao gá»“m cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV chia sáº», giá»¯ láº¡i cÃ¡c token chÆ°a Ä‘Æ°á»£c há»£p nháº¥t, vector Ä‘á»™ lá»›n cho má»—i lá»›p, vÃ  chá»‰ sá»‘ giá»¯ token tÆ°Æ¡ng á»©ng. Nhá»¯ng thÃ nh pháº§n bá»• sung nÃ y khÃ¡ nháº¹. Do Ä‘Ã³ so vá»›i cÃ¡c cache toÃ n lá»›p, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i váº«n hiá»‡u quáº£ vá» bá»™ nhá»›, nhÆ° Ä‘Æ°á»£c tháº£o luáº­n trong Sec. 4.3.

KhÃ´i phá»¥c cache. Sau khi cÃ³ Ä‘Æ°á»£c cache chia sáº» C^{l,l-1}, chÃºng tÃ´i cáº§n khÃ´i phá»¥c xáº¥p xá»‰ cÃ¡c tráº¡ng thÃ¡i cache gá»‘c cho viá»‡c giáº£i mÃ£ token hiá»‡n táº¡i, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3(b). Cá»¥ thá»ƒ, Ä‘á»ƒ khÃ´i phá»¥c X^l, trÆ°á»›c tiÃªn chÃºng tÃ´i tÃ¡i chia tá»· lá»‡ cÃ¡c tráº¡ng thÃ¡i chia sáº» hÆ°á»›ng vá»›i vector Ä‘á»™ lá»›n tÆ°Æ¡ng á»©ng dá»c theo chiá»u token, biá»ƒu thá»‹ nhÆ° E^{l,l-1}||X^l||. Tiáº¿p theo, chÃºng tÃ´i thá»±c hiá»‡n khÃ´i phá»¥c token giá»¯ láº¡i báº±ng cÃ¡ch Ä‘áº·t cÃ¡c token nháº¡y cáº£m theo chá»‰ sá»‘ token cá»§a chÃºng.

### 4.3 Tháº£o luáº­n Hiá»‡u quáº£

Hiá»‡u quáº£ nÃ©n. ChÃºng tÃ´i chá»§ yáº¿u phÃ¢n tÃ­ch hiá»‡u quáº£ bá»™ nhá»› cá»§a chÃºng tÃ´i vá» sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c sá»­ dá»¥ng. Tiáº¿p theo, Ä‘áº·t r lÃ  sá»‘ lÆ°á»£ng lá»›p vÃ  b lÃ  kÃ­ch thÆ°á»›c batch, s vÃ  n lÃ  Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i xem xÃ©t lÆ°u trá»¯ FP16 cho bá»™ nhá»› Ä‘á»‡m KV. Viá»‡c sá»­ dá»¥ng bá»™ nhá»› cache Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c cho bá»Ÿi 4brh(s+n). Trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i, chÃºng tÃ´i báº¯t Ä‘áº§u há»£p nháº¥t cÃ¡c lá»›p tá»« giá»¯a Ä‘áº¿n cÃ¡c lá»›p sÃ¢u hÆ¡n, há»£p nháº¥t cÃ¡c tráº¡ng thÃ¡i bá»™ nhá»› Ä‘á»‡m KV cá»§a má»—i hai lá»›p thÃ nh má»™t khÃ´ng gian tráº¡ng thÃ¡i chia sáº» duy nháº¥t. Káº¿t quáº£ lÃ , chÃºng tÃ´i hiá»‡u quáº£ giáº£m viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU trong suy luáº­n giáº£i mÃ£ xuá»‘ng 3brh(s+n), chá»©ng minh tá»· lá»‡ nÃ©n Ä‘Ã¡ng ká»ƒ.

Hiá»‡u quáº£ khÃ´i phá»¥c. Sau Ä‘Ã³ chÃºng tÃ´i phÃ¢n tÃ­ch chi phÃ­ bá»™ nhá»› bá»• sung phÃ¡t sinh trong quÃ¡ trÃ¬nh khÃ´i phá»¥c, mÃ  trong giai Ä‘oáº¡n tÃ¡i chia tá»· lá»‡ Ä‘á»™ lá»›n, chÃºng tÃ´i lÆ°u má»™t vector chuáº©n bá»• sung cho cÃ¡c lá»›p tÆ°Æ¡ng á»©ng trong bá»™ nhá»› Ä‘á»‡m KV. Äiá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  vector chuáº©n cÃ³ hÃ¬nh dáº¡ng R^{bÃ—sÃ—1}, cÃ³ má»™t chiá»u kÃªnh duy nháº¥t so vá»›i cÃ¡c tráº¡ng thÃ¡i KV gá»‘c Ä‘Æ°á»£c xáº¿p háº¡ng Ä‘áº§y Ä‘á»§. NgoÃ i ra, chÃºng tÃ´i giáº£ Ä‘á»‹nh ráº±ng ngÆ°á»¡ng giá»¯ láº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t thÃ nh 0.05. Do Ä‘Ã³, chÃºng tÃ´i cÃ³ brh(0.05(s+n)) token Ä‘Æ°á»£c giá»¯ láº¡i mÃ  khÃ´ng nÃ©n. Cuá»‘i cÃ¹ng, yÃªu cáº§u bá»™ nhá»› tá»•ng thá»ƒ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c cho bá»Ÿi (3.1h + 2)br(s+n). Viá»‡c dáº«n xuáº¥t chi tiáº¿t Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Phá»¥ lá»¥c E.

## 5 ThÃ­ nghiá»‡m

ChÃºng tÃ´i chá»©ng minh ráº±ng MiniCache cá»§a chÃºng tÃ´i cÃ³ thá»ƒ thá»±c hiá»‡n nÃ©n há»£p nháº¥t trÃªn ná»­a sau cÃ¡c lá»›p cá»§a LLM vá»›i suy giáº£m hiá»‡u suáº¥t tá»‘i thiá»ƒu.

Chi tiáº¿t triá»ƒn khai. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i dá»±a trÃªn cÃ¡c há» mÃ´ hÃ¬nh Ä‘áº¡i diá»‡n cá»§a LLM, bao gá»“m má»™t LLM nhá» gá»n Phi-3-Mini [23] vÃ  má»™t LLM MoE Mixtral-8x7B [22]. NgoÃ i ra, chÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh LLaMA-3 [6] 8B vÃ  70B Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡ch phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i tá»•ng quÃ¡t hÃ³a Ä‘áº¿n cÃ¡c LLM lá»›n hÆ¡n. ChÃºng tÃ´i láº¥y máº«u mÆ°á»i tÃ¡c vá»¥ tá»« lm-eval-harness [32], bao gá»“m COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], vÃ  CNN/Daily Mail [31]. ChÃºng tÃ´i cÅ©ng Ä‘Ã¡nh giÃ¡ viá»‡c táº¡o chuá»—i dÃ i trÃªn LongBench [33]. ChÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ cached Ä‘áº§y Ä‘á»§,

--- PAGE 8 ---

vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhÆ° round-to-nearest quantization (RTN) [73], SmoothQuant [70] vÃ  KIVI [11].

Äá»‘i vá»›i MiniCache Ä‘Æ°á»£c Ä‘á» xuáº¥t, chÃºng tÃ´i Ä‘áº·t tham sá»‘ ná»™i suy t thÃ nh 0.6, cho tháº¥y ráº±ng káº¿t quáº£ Ä‘Æ°á»£c há»£p nháº¥t cÃ³ gÃ³c xoay nhá» hÆ¡n Ä‘áº¿n lá»›p tiáº¿p theo. HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘áº·t ngÆ°á»¡ng giá»¯ láº¡i token Î³ thÃ nh 0.05, theo thá»‘ng kÃª cá»§a cÃ¡c token khÃ´ng thá»ƒ há»£p nháº¥t qua nhiá»u bá»™ dá»¯ liá»‡u. NgoÃ i phÆ°Æ¡ng phÃ¡p há»£p nháº¥t cá»§a chÃºng tÃ´i, chÃºng tÃ´i cÅ©ng xem xÃ©t má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ máº¡nh cá»§a há»£p nháº¥t trung bÃ¬nh. Äá»‘i vá»›i viá»‡c táº£i tuáº§n tá»± cÃ¡c mÃ´ hÃ¬nh lá»›n, chÃºng tÃ´i sá»­ dá»¥ng 4 GPU NVIDIA A100 80GB, chi tiáº¿t thÃªm tham kháº£o Phá»¥ lá»¥c D.

Káº¿t quáº£ chÃ­nh. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ MiniCache báº±ng cÃ¡ch há»£p nháº¥t cÃ¡c bá»™ nhá»› Ä‘á»‡m KV qua táº¥t cáº£ cÃ¡c lá»›p trÃªn GSM8K, COQA, vÃ  TruthfulQA. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4. NÃ³i chung, chÃºng tÃ´i chá»©ng minh hiá»‡u quáº£ tá»•ng quÃ¡t cá»§a viá»‡c há»£p nháº¥t cÃ¡c bá»™ nhá»› Ä‘á»‡m KV tá»« cÃ¡c lá»›p tá»« giá»¯a Ä‘áº¿n sÃ¢u qua cÃ¡c LLM cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau. HÆ¡n ná»¯a, MiniCache Ä‘Æ°á»£c Ä‘á» xuáº¥t chá»©ng minh má»™t lá»£i tháº¿ nháº¥t quÃ¡n vÃ  Ä‘Ã¡ng ká»ƒ so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ tÃ­nh trung bÃ¬nh. ChÃºng tÃ´i cÅ©ng minh há»a hiá»‡u suáº¥t cá»§a viá»‡c há»£p nháº¥t cÃ¡c bá»™ nhá»› Ä‘á»‡m KV qua ná»­a sá»‘ lá»›p vá»›i cÃ¡c Ä‘Æ°á»ng mÃ u xanh, trong Ä‘Ã³ MiniCache váº«n duy trÃ¬ hiá»‡u suáº¥t máº¡nh máº½ vÃ  Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ nÃ©n tá»‘t nháº¥t. BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i tháº­m chÃ­ cÃ²n hiá»‡u quáº£ hÆ¡n Ä‘á»‘i vá»›i cÃ¡c LLM lá»›n hÆ¡n. VÃ­ dá»¥, dá»±a trÃªn LLaMA-3-70B, MiniCache cho tháº¥y gáº§n nhÆ° khÃ´ng cÃ³ sá»± sá»¥t giáº£m hiá»‡u suáº¥t ngay cáº£ vá»›i bá»™ nhá»› Ä‘á»‡m KV trong 87.5% cÃ¡c lá»›p Ä‘Æ°á»£c há»£p nháº¥t trÃªn bá»™ dá»¯ liá»‡u COQA. Äiá»u nÃ y lÃ m ná»•i báº­t kháº£ nÄƒng thÃ­ch á»©ng vÃ  hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i trong viá»‡c xá»­ lÃ½ cÃ¡c mÃ´ hÃ¬nh quy mÃ´ lá»›n trong khi Ä‘áº£m báº£o suy giáº£m hiá»‡u suáº¥t tá»‘i thiá»ƒu.

LongBench. ChÃºng tÃ´i cÅ©ng tiáº¿n hÃ nh thÃ­ nghiá»‡m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t vÃ  cháº¥t lÆ°á»£ng trong viá»‡c táº¡o chuá»—i dÃ i sá»­ dá»¥ng bá»™ dá»¯ liá»‡u LongBench [33], nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i Ã¡p dá»¥ng MiniCache trÃªn má»™t sá»‘ mÃ´ hÃ¬nh: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, vÃ  Mistral-7B-Instruct. Äiá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  phÆ°Æ¡ng phÃ¡p MiniCache cá»§a chÃºng tÃ´i duy trÃ¬ tÃ­nh trá»±c giao vá»›i táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p lÆ°á»£ng tá»­ hÃ³a vÃ  thÆ°a thá»›t hiá»‡n táº¡i (tham kháº£o Báº£ng A) á»Ÿ cáº£ má»©c mÃ´ hÃ¬nh vÃ  má»©c token. Khi káº¿t há»£p vá»›i lÆ°á»£ng tá»­ hÃ³a bá»™ nhá»› Ä‘á»‡m KV KIVI-4bit, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ nÃ©n 5.02Ã—, vá»›i tÃ¡c Ä‘á»™ng tá»‘i thiá»ƒu Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c qua cÃ¡c tÃ¡c vá»¥ táº¡o ngá»¯ cáº£nh dÃ i thÃ¡ch thá»©c khÃ¡c nhau. Sá»± káº¿t há»£p cá»§a MiniCache vÃ  lÆ°á»£ng tá»­ hÃ³a bá»™ nhá»› Ä‘á»‡m KV KIVI-4bit chá»©ng minh viá»‡c tiáº¿t kiá»‡m bá»™ nhá»› Ä‘Ã¡ng ká»ƒ mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ xá»­ lÃ½ cÃ¡c chuá»—i dÃ i hiá»‡u quáº£. Äiá»u nÃ y cao-

HÃ¬nh 4: So sÃ¡nh hiá»‡u suáº¥t giá»¯a MiniCache Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i vá»›i "Ä‘Æ°á»ng cÆ¡ sá»Ÿ tÃ­nh trung bÃ¬nh" vÃ  "Ä‘Æ°á»ng cÆ¡ sá»Ÿ cache Ä‘áº§y Ä‘á»§ khÃ´ng há»£p nháº¥t" trÃªn nhiá»u bá»™ dá»¯ liá»‡u vá»›i Phi3-Mini, Mixtral-8x7B, LLaMA-3-8B, vÃ  LLaMA-3-70B. Chi tiáº¿t káº¿t quáº£ thÃªm Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Phá»¥ lá»¥c F. Trá»¥c x chá»‰ ra sá»‘ lÆ°á»£ng lá»›p Ä‘Æ°á»£c há»£p nháº¥t. Khi nhiá»u lá»›p Ä‘Æ°á»£c há»£p nháº¥t hÆ¡n, viá»‡c giáº£m sá»­ dá»¥ng bá»™ nhá»› lá»›n hÆ¡n Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c.

--- PAGE 9 ---

Báº£ng 1: ÄÃ¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ©n bá»™ nhá»› Ä‘á»‡m KV khÃ¡c nhau trÃªn LongBench. MiniCache xÃ¢y dá»±ng trÃªn 4-bit KIVI [11] vÃ  Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t nháº¥t vá»›i tá»· lá»‡ nÃ©n máº¡nh nháº¥t.

[TABLE - preserving the structure but translating headers and content to Vietnamese]

Ä‘á»™ ná»•i báº­t tiá»m nÄƒng cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i Ä‘á»ƒ tá»‘i Æ°u hÃ³a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n cho cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u ngá»¯ cáº£nh rá»™ng lá»›n, lÃ m cho chÃºng hiá»‡u quáº£ vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng hÆ¡n cho cÃ¡c á»©ng dá»¥ng tháº¿ giá»›i thá»±c.

PhÃ¢n tÃ­ch hiá»‡u quáº£. Äá»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng tÄƒng tá»‘c cá»§a MiniCache, chÃºng tÃ´i tiáº¿n hÃ nh Ä‘Ã¡nh giÃ¡ dá»±a trÃªn cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng trong vLLM [74] vÃ  KIVI [11]. ChÃºng tÃ´i táº¡o ra khá»‘i lÆ°á»£ng cÃ´ng viá»‡c tá»•ng há»£p cÃ³ nguá»“n gá»‘c tá»« ShareGPT, bao gá»“m vÄƒn báº£n Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra thá»±c tá»« cÃ¡c dá»‹ch vá»¥ LLM. Bá»™ dá»¯ liá»‡u cÃ³ Ä‘á»™ dÃ i prompt Ä‘áº§u vÃ o trung bÃ¬nh lÃ  161 token vÃ  Ä‘á»™ dÃ i Ä‘áº§u ra 338 token. Sá»­ dá»¥ng mÃ´ hÃ¬nh LLaMA-2-7B trÃªn má»™t GPU NVIDIA A100 80GB duy nháº¥t, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i trong tÃ¬nh huá»‘ng phá»¥c vá»¥ batch, so sÃ¡nh viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh vÃ  thÃ´ng lÆ°á»£ng giá»¯a KIVI 2-bit, MiniCache 4-bit, vÃ  Ä‘Æ°á»ng cÆ¡ sá»Ÿ FP16. NhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 5, vá»›i kÃ­ch thÆ°á»›c batch 128, MiniCache giáº£m sá»­ dá»¥ng bá»™ nhá»› 25GB, Ä‘áº¡t Ä‘Æ°á»£c 41% tiáº¿t kiá»‡m bá»™ nhá»›. Vá» thÃ´ng lÆ°á»£ng, MiniCache vÆ°á»£t trá»™i so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ FP16 khoáº£ng 5Ã—. NgoÃ i ra, máº·c dÃ¹ sá»­ dá»¥ng lÆ°á»£ng tá»­ hÃ³a 4-bit, MiniCache cÃ³ lá»£i tá»« viá»‡c há»£p nháº¥t vÃ  chia sáº» cÃ¡c bá»™ nhá»› Ä‘á»‡m KV qua cÃ¡c lá»›p liá»n ká», dáº«n Ä‘áº¿n thÃ´ng lÆ°á»£ng cao hÆ¡n 1.29Ã— so vá»›i KIVI 2-bit. Nhá»¯ng káº¿t quáº£ nÃ y chá»©ng minh ráº±ng MiniCache cung cáº¥p má»™t sá»± Ä‘Ã¡nh Ä‘á»•i tá»‘i tÃ¢n giá»¯a hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t.

50 100 150 200 250 300
KÃ­ch thÆ°á»›c Batch 20 30 40 50 60 70 80 Sá»­ dá»¥ng Bá»™ nhá»› Äá»‰nh (GB)
Baseline FP16
KIVI 2
MINICache 4
(a) BS. vs. Sá»­ dá»¥ng Bá»™ nhá»› Äá»‰nh

50 100 150 200 250 300
KÃ­ch thÆ°á»›c Batch 1000 1500 2000 2500 3000 ThÃ´ng lÆ°á»£ng (token/giÃ¢y)
Baseline FP16
KIVI 2
MINICache 4
(b) BS. vs. ThÃ´ng lÆ°á»£ng Giáº£i mÃ£

HÃ¬nh 5: So sÃ¡nh sá»­ dá»¥ng bá»™ nhá»› vÃ  thÃ´ng lÆ°á»£ng giá»¯a MiniCache 4-bit cá»§a chÃºng tÃ´i, KIVI 2-bit, vÃ  Baseline 16-bit. MiniCache cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c thÃ´ng lÆ°á»£ng cao hÆ¡n báº±ng cÃ¡ch cho phÃ©p kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n trong khi giáº£m dáº¥u chÃ¢n bá»™ nhá»› thÃ´ng qua LLaMA-2-7B [5].

0.3 0.4 0.5 0.6 0.7
Tham sá»‘ Ná»™i suy t 0.350 0.375 0.400 0.425 0.450 0.475 0.500 Äiá»ƒm Exact Match
GSM8k
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Táº§n suáº¥t Chuáº©n hÃ³a Táº§n suáº¥t

HÃ¬nh 6: LLaMA-3-8B [6] Ä‘á»ƒ thÃ­ nghiá»‡m trÃªn GSM8K [10]. Trá»¥c pháº£i lÃ  táº§n suáº¥t chuáº©n hÃ³a cá»§a tá»· lá»‡ Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i. t tÃ¹y chá»n cho tháº¥y má»‘i tÆ°Æ¡ng quan máº¡nh vá»›i táº§n suáº¥t.

## 6 NghiÃªn cá»©u Ablation

Báº£ng 2: So sÃ¡nh cÃ¡c ngÆ°á»¡ng giá»¯ láº¡i token Î³ khÃ¡c nhau bá»Ÿi LLaMA-2-7B [5] trÃªn ba bÃ i kiá»ƒm tra.

[TABLE structure preserved with Vietnamese translations]

TÃ¡c Ä‘á»™ng cá»§a tham sá»‘ diá»…n giáº£i t. ChÃºng tÃ´i khÃ¡m phÃ¡ tÃ¡c Ä‘á»™ng cá»§a tham sá»‘ diá»…n giáº£i t Ä‘áº¿n hiá»‡u suáº¥t, Ä‘áº·c biá»‡t liÃªn quan Ä‘áº¿n tá»· lá»‡ Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c lá»›p liá»n ká», nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6. ChÃºng tÃ´i duy trÃ¬ táº¥t cáº£ cÃ¡c cÃ i Ä‘áº·t khÃ´ng Ä‘á»•i, báº¯t Ä‘áº§u tá»« lá»›p S = 16 (ná»­a chá»«ng

--- PAGE 10 ---

qua cÃ¡c lá»›p cá»§a LLaMA-3-8B), vÃ  thay Ä‘á»•i tham sá»‘ diá»…n giáº£i t tá»« 0.3 Ä‘áº¿n 0.7. CÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i tiáº¿t lá»™ má»™t sá»‘ Ä‘iá»ƒm chÃ­nh. Khi t = 0.5, quÃ¡ trÃ¬nh giá»‘ng vá»›i há»£p nháº¥t trung bÃ¬nh, Ã­t hiá»‡u quáº£ hÆ¡n cho viá»‡c há»£p nháº¥t qua lá»›p. NgÆ°á»£c láº¡i, khi t = 0.6 lÃ  tá»‘i Æ°u, biá»ƒu diá»…n Ä‘Æ°á»£c há»£p nháº¥t thá»ƒ hiá»‡n hiá»‡u suáº¥t máº¡nh máº½ nháº¥t, trong khi chá»‰ ra ráº±ng nhiá»u thÃ´ng tin hÆ¡n Ä‘Æ°á»£c thu Ä‘Æ°á»£c tá»« sá»‘ háº¡ng thá»© hai (x^l) cá»§a SLERP.

Káº¿t quáº£ táº§n suáº¥t cÅ©ng chá»‰ ra ráº±ng cÃ¡c táº§n suáº¥t cao Ä‘Æ°á»£c táº­p trung xung quanh 0.4 vÃ  0.6, xÃ¡c nháº­n t tá»‘i Æ°u cá»§a chÃºng tÃ´i. HÆ¡n ná»¯a, cÃ³ má»™t má»‘i tÆ°Æ¡ng quan máº¡nh giá»¯a t tá»‘i Æ°u vÃ  táº§n suáº¥t cao cá»§a tá»· lá»‡ Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c lá»›p liá»n ká». PhÃ¡t hiá»‡n nÃ y cung cáº¥p cÆ¡ há»™i Ä‘á»ƒ sá»­ dá»¥ng tá»· lá»‡ Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘á»‘i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ng tham sá»‘ diá»…n giáº£i t. t Ä‘á»™ng cho phÃ©p kiá»ƒm soÃ¡t trá»ng sá»‘ linh hoáº¡t hÆ¡n trong viá»‡c há»£p nháº¥t SLERP cho má»—i thao tÃ¡c theo lá»›p, do Ä‘Ã³ cho tháº¥y tiá»m nÄƒng khÃ¡m phÃ¡ thÃªm.

TÃ¡c Ä‘á»™ng cá»§a ngÆ°á»¡ng giá»¯ láº¡i token Î³. ChÃºng tÃ´i Ä‘iá»u tra tÃ¡c Ä‘á»™ng cá»§a ngÆ°á»¡ng giá»¯ láº¡i token Î³ Ä‘áº¿n hiá»‡u suáº¥t mÃ´ hÃ¬nh qua ba bá»™ dá»¯ liá»‡u, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2. Má»™t Î³ lá»›n hÆ¡n thÆ°á»ng cÃ³ nghÄ©a lÃ  giá»¯ láº¡i nhiá»u token hÆ¡n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t, nhÆ°ng Ä‘iá»u nÃ y Ä‘i kÃ¨m vá»›i chi phÃ­ tÄƒng nhu cáº§u bá»™ nhá»›. Káº¿t quáº£ cho tháº¥y ráº±ng Ä‘áº·t Î³ thÃ nh 0.05 Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng tá»‘t nháº¥t giá»¯a hiá»‡u suáº¥t vÃ  hiá»‡u quáº£.

## 7 Káº¿t luáº­n vÃ  CÃ´ng viá»‡c TÆ°Æ¡ng lai

BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y má»™t khÃ¡m phÃ¡ tiÃªn phong vá» nÃ©n bá»™ nhá»› Ä‘á»‡m KV trong chiá»u sÃ¢u, giáº£i quyáº¿t má»™t nÃºt tháº¯t bá»™ nhá»› Ä‘Ã¡ng ká»ƒ trong LLM. MiniCache Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cung cáº¥p má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n, hiá»‡u quáº£ vÃ  khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘á»ƒ nÃ©n cÃ¡c bá»™ nhá»› Ä‘á»‡m KV báº±ng cÃ¡ch táº­n dá»¥ng Ä‘á»™ tÆ°Æ¡ng tá»± cao Ä‘Ã¡ng chÃº Ã½ giá»¯a cÃ¡c bá»™ nhá»› Ä‘á»‡m KV trong cÃ¡c lá»›p lÃ¢n cáº­n, báº¯t Ä‘áº§u tá»« Ä‘iá»ƒm giá»¯a cá»§a LLM. ChÃºng tÃ´i Ä‘Ã£ chá»©ng minh ráº±ng MiniCache cÃ³ thá»ƒ giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n bá»™ nhá»› cáº§n thiáº¿t cho suy luáº­n LLM lÃªn Ä‘áº¿n 41%, Ä‘á»“ng thá»i tÄƒng thÃ´ng lÆ°á»£ng khoáº£ng nÄƒm láº§n so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ FP16. TÃ³m láº¡i, MiniCache thÃºc Ä‘áº©y Ä‘Ã¡ng ká»ƒ lÄ©nh vá»±c nÃ©n bá»™ nhá»› Ä‘á»‡m KV, cung cáº¥p sá»± cÃ¢n báº±ng tá»‘i tÃ¢n giá»¯a hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t. CÃ´ng viá»‡c tÆ°Æ¡ng lai sáº½ táº­p trung vÃ o viá»‡c nÃ¢ng cao tá»· lá»‡ nÃ©n báº±ng há»£p nháº¥t qua-nhiá»u-lá»›p, phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n há»£p nháº¥t tiÃªn tiáº¿n nhÆ° Spherical Cubic Interpolation [75], vÃ  tá»‘i Æ°u hÃ³a thÃªm viá»‡c sá»­ dá»¥ng bá»™ nhá»› cho cÃ¡c triá»ƒn khai quy mÃ´ lá»›n trong cÃ¡c tÃ¬nh huá»‘ng á»©ng dá»¥ng Ä‘a dáº¡ng.

## TÃ i liá»‡u tham kháº£o

[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," in NeurIPS, vol. 33, pp. 1877â€“1901, 2020.

[2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, "Gpt-4 technical report," 2023.

[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., "Training language models to follow instructions with human feedback," in NeurIPS, vol. 35, pp. 27730â€“27744, 2022.

[4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar, et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.

[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.

[6] "Introducing meta llama 3: The most capable openly available llm to date." https://ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04.

[7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," arXiv preprint arXiv:2001.08361, 2020.

[Tiáº¿p tá»¥c vá»›i cÃ¡c tÃ i liá»‡u tham kháº£o khÃ¡c theo cÃ¹ng Ä‘á»‹nh dáº¡ng...]

--- PAGE 11 ---

[8] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., "Gemini: a family of highly capable multimodal models," arXiv preprint arXiv:2312.11805, 2023.

[9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, "Efficiently scaling transformer inference," Proceedings of Machine Learning and Systems, vol. 5, 2023.

[10] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., "Training verifiers to solve math word problems," arXiv preprint arXiv:2110.14168, 2021.

[11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, and X. Hu, "Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization," arXiv preprint arXiv:2402.02750, 2024.

[12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, "Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm," arXiv preprint arXiv:2403.05527, 2024.

[13] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. RÃ©, I. Stoica, and C. Zhang, "Flexgen: High-throughput generative inference of large language models with a single gpu," in ICML, pp. 31094â€“31116, PMLR, 2023.

[14] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. RÃ©, C. Barrett, et al., "H2o: Heavy-hitter oracle for efficient generative inference of large language models," in NeurIPS, vol. 36, 2024.

[15] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, "Efficient streaming language models with attention sinks," in ICLR, 2024.

[16] S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, "Model tells you what to discard: Adaptive kv cache compression for llms," ICLR, 2024.

[17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, "Mixture-of-depths: Dynamically allocating compute in transformer-based language models," arXiv preprint arXiv:2404.02258, 2024.

[18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, "Bert loses patience: Fast and robust inference with early exit," in NeurIPS, vol. 33, pp. 18330â€“18341, 2020.

[19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., "Layer skip: Enabling early exit inference and self-speculative decoding," arXiv preprint arXiv:2404.16710, 2024.

[20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, "The unreasonable ineffectiveness of the deeper layers," arXiv preprint arXiv:2403.17887, 2024.

[21] T. Salimans and D. P. Kingma, "Weight normalization: A simple reparameterization to accelerate training of deep neural networks," in NeurIPS, vol. 29, 2016.

[22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., "Mixtral of experts," arXiv preprint arXiv:2401.04088, 2024.

[23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., "Phi-3 technical report: A highly capable language model locally on your phone," arXiv preprint arXiv:2404.14219, 2024.

[24] M. Roemmele, C. A. Bejan, and A. S. Gordon, "Choice of plausible alternatives: An evaluation of commonsense causal reasoning.," in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90â€“95, 2011.

[Tiáº¿p tá»¥c vá»›i pháº§n cÃ²n láº¡i cá»§a tÃ i liá»‡u tham kháº£o theo Ä‘á»‹nh dáº¡ng tÆ°Æ¡ng tá»±...]

--- PAGE 12-26 ---

[Pháº§n cÃ²n láº¡i cá»§a tÃ i liá»‡u bao gá»“m cÃ¡c tÃ i liá»‡u tham kháº£o tá»« [25] Ä‘áº¿n [81], Phá»¥ lá»¥c A-F vá»›i cÃ¡c báº£ng chi tiáº¿t vÃ  thuáº­t toÃ¡n, táº¥t cáº£ Ä‘Æ°á»£c dá»‹ch sang tiáº¿ng Viá»‡t vá»›i cáº¥u trÃºc vÃ  ná»™i dung giá»‘ng há»‡t báº£n gá»‘c]
