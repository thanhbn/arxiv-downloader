# 2405.14366.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/kv-cache/2405.14366.pdf
# File size: 8705167 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MiniCache: KV Cache Compression in Depth
Dimension for Large Language Models
Akide Liu1Jing Liu1Zizheng Pan1Yefei He2
Gholamreza Haffari1Bohan Zhuang1,2‚Ä†
1ZIP Lab, Monash University, Australia
2ZIP Lab, Zhejiang University, China
Abstract
A critical approach for efficiently deploying computationally demanding large lan-
guage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value
states of previously generated tokens, significantly reducing the need for repetitive
computations and thereby lowering latency in autoregressive generation. However,
the size of the KV cache grows linearly with sequence length, posing challenges for
applications requiring long context input and extensive sequence generation. In this
paper, we present a simple yet effective approach, called MiniCache, to compress
the KV cache across layers from a novel depth perspective, significantly reducing
the memory footprint for LLM inference. Our approach is based on the observation
that KV cache states exhibit high similarity between the adjacent layers in the
middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling
the states into the magnitude and direction components, interpolating the directions
of the state vectors while preserving their lengths unchanged. Furthermore, we in-
troduce a token retention strategy to keep highly distinct state pairs unmerged, thus
preserving the information with minimal additional storage overhead. Our Mini-
Cache is training-free and general, complementing existing KV cache compression
strategies, such as quantization and sparsity. We conduct a comprehensive evalua-
tion of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3,
Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional
performance in achieving superior compression ratios and high throughput. On
the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable
compression ratio of up to 5.02√ó, enhances inference throughput by approximately
5√ó, and reduces the memory footprint by 41% compared to the FP16 full cache
baseline, all while maintaining near-lossless performance. Project is available at
https://minicache.vmv.re
1 Introduction
Large Language Models (LLMs), exemplified by the GPT series [ 1,2,3] and the LLaMA series
[4,5,6], have emerged as pivotal innovations within the artificial general intelligence, significantly
enhancing the capabilities of natural language processing. However, these models are meticulously
trained using extensive computational resources [ 7] and massive datasets [ 8], which enables them to
produce text that effectively mimics human writing styles and conducts complex reasoning analysis,
yet raises the challenge of efficient deployment and serving. Within the inference framework of
‚Ä†Corresponding author. Email: bohan .zhuang @gmail .com
Preprint. Under review.arXiv:2405.14366v2  [cs.CL]  7 Sep 2024

--- PAGE 2 ---
(a)Cross-layer KV cachesimilarity
(b)Merged layersvs EMscore on GSM8K
Layerùëô‚àí1
Layerùëô‚àí2
...
Layer2
Layer1
LMHeadInput
Layerùëô‚àí3
Pruning/Quant.
KVKVCacheCompressionTDecoding
Cross Layer Merging
KV
QKV
Attention
DecodingKVCacheCompression
QKV
Attention
T+1
Prevs.
MiniCache
(c)Comparisonbetween MiniCacheand previous methods......CosineSimilarity 
T
T+1Number of Layers Merged on LLaMA-3-70BFigure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that
the KV cache states between two adjacent layers are highly similar, particularly across the middle
to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b)
compares the performance of MiniCache, and the mean baseline, which simply averages the KV
caches of two layers, using the LLaMA-3-70B model [ 6] on the GSM8K dataset [ 10]. MiniCache,
which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights
the primary difference between MiniCache and previous approaches. MiniCache investigates the
inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by
intra-layer-based methods. Here, Trefers to the last timestamp of pre-filling, and T+ 1des to the
first timestamp of decoding.
LLMs, KV caches [ 9] are crucial for storing pre-computed keys and values, thus avoiding repeated
calculations over the preceding context and substantially enhancing LLMs‚Äô deployment efficiency.
However, the increasing demand for longer sequence lengths results in voluminous cached states,
leading to significant memory consumption during generation. For instance, a 175B GPT-3 model [ 2],
with a batch size of 64 and a sequence length of 4,096 tokens (both prefilled and generated), requires
approximately 1,208GB of GPU memory. This requirement is 3.45√ógreater than the memory used to
store the model‚Äôs weights. In this context, KV cache compression is of paramount importance due to
its clear benefits: 1) it largely reduces the memory footprint allowing for faster generation and larger
batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial
benefits.
Existing KV cache compression efforts can be roughly categorized into two types, namely quantization
and sparsity. The quantization approaches [ 11,12] propose storing the KV states in low-bit numerical
values. Typically, FlexGen [ 13] demonstrates that 4-bit KV cache quantization can achieve lossless
performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting
the rest, either heuristically [ 14,15] or adaptively [ 16]. Some approaches [ 11] explore the intersection
of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the
tokens, achieving more aggressive memory gain. Despite these innovations, existing literature merely
consider the intra-layer redundancy , while neglecting another important complementary direction ‚Äì
theinter-layer redundancy , as illustrated in the Figure 1(c).
Our analysis begins by exploring the redundancy of KV caches along the depth dimension , as shown
in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in
2

--- PAGE 3 ---
the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position
between adjacent layers can be accurately merged into a single state space with a strong performance
guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint
without the need to retain individual states for each attention layer. Note that these observations are
pertinent to dynamic inference strategies such as mixture-of-depths [ 17] and layer-wise early exiting
[18,19], which optimize computational paths by skipping non-critical layers to enhance training and
inference efficiency. Furthermore, layer pruning methods [ 20] highlight considerable redundancy in
deeper layers. However, despite these advancements, the redundancy of KV caches along the depth
dimension has largely been overlooked.
In this paper, we propose MiniCache , a simple yet effective cross-layer KV cache compression
method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential
components. Firstly, we introduce an accurate cache merging strategy, employing a reparameteri-
zation of state vectors that decompose them into the magnitude and direction components, akin to
weight normalization [ 21]. This approach allows for effective interpolation of the directional compo-
nent in polar coordinates while preserving the original state norms to retain as much information as
possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly,
we recognize that a small subset of state pairs, characterized by low similarities but carrying largely
distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a
token retention strategy to minimize performance degradation, which involves separately retaining
these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single
high-dimensional directional component, along with minimal extra memory overhead. The over-
head consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise
magnitudes to accurately restore the original states.
We conduct extensive experiments with representative LLMs, including Mixtral-8x7B [ 22], Phi-
3-Mini [ 23], and LLaMA-3 [ 6] 8B and 70B, respectively. Our method is benchmarked across a
diverse range of question answering and generation datasets [ 24,25,26,27,28,29,30,31] using the
lm-eval-harness [ 32]. Additionally, we evaluate our results on LongBench [ 33] for long-sequence
generation. The results demonstrate that MiniCache can reduce the memory footprint required for
LLM inference by up to 41%, while simultaneously enhancing throughput by approximately 5√ó
compared to fully cached baseline, clearly surpassing existing methods [11, 12, 14, 15].
Our contributions are summarized as follows:
‚Ä¢We introduce MiniCache, a simple yet highly effective framework for KV cache compression.
MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby
significantly expanding its capabilities.
‚Ä¢We observe a fascinating characteristic of cross-layer KV cache states: high similarity between
adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs
are suitable for merging.
‚Ä¢We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a
reparameterization strategy and a token retention mechanism. Our method complements existing
KV cache compression approaches, further enhancing LLM serving efficiency.
‚Ä¢Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit
MiniCache achieves a strong compression ratio of up to 5.02√ó,5√óhigher inference throughput, and
41% memory reduction compared to the FP16 full cache baseline with near-lossless performance.
2 Related Work
Efficient inference for LLMs. Large Language Models (LLMs) are constrained by considerable
computational and memory requirements during inference, particularly in resource-constrained
environments. To mitigate these challenges, a variety of efficient inference techniques have been
developed. For instance, dynamic inference methods [ 18,34,35,36], represented by mixture-of-
experts (MoE) [ 37,38,39,40,41], adaptively select specific sub-structures of the model during
the inference process based on the input data, significantly improving inference efficiency while
keeping model capacity. Techniques like Multi-Query Attention [ 42,43], Kernel-driven attentions
[44,45,46,47], and low-rank attentions [ 41,48,49,50] approximate the functionality of traditional
attention mechanisms but with more efficient implementations. Quantization strategies [ 51,52,53,54]
3

--- PAGE 4 ---
involve converting the model‚Äôs weights and activations into a low bit-width format, thereby reducing
memory footprint and computational intensity. Sparsification approaches [ 14,15,55,56] eliminate
unnecessary elements in both model weights and token representations, further enhancing efficiency.
Some closely related works, such as MoD [ 17] and LayerSkips [ 19], considered the dynamic inference
nature to ignore unimportant layers according to input. However, these methods require an additional
fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these
methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging,
significantly reducing memory demand.
Model merging. Merging compression involves the aggregation of a model‚Äôs parameters and
activations at various granularities. This process enhances the efficiency of inference in large models
and facilitates huge redundancy [ 57]. Linear Mode Connectivity (LMC) [ 58] enables the fine-tuning
of models from a shared pre-trained base. Commonly, weight averaging [ 59] is employed as an
efficient technique to perform merge compression. Notably, Model Soup [ 60] utilizes linear averaging
in this context. Advanced methods like TIES Merging [ 61], Model Breadcrumbs [ 62], and DARE [ 63]
further enhance this process by sparsifying and combining model parameters, enabling the merging
of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation
(SLERP) [ 64] extends beyond simple weight averaging by interpolating between model parameters.
The Fisher information matrix [ 65] and RegMean-based methods [ 66] further optimize merges to
produce ideal weights, minimizing the ‚Ñì2distance to generation outputs while preserving the privacy
of the training data. However, most existing works focus on merging model parameters, with the
concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache
focuses on the KV cache token merging in the depth dimensional of LLMs.
3 Motivation
In the below, we present our new observations in a novel cross-layer perspective.
LLama 2 7B LLama 2 30B LLama 3 8B Mixtral 8x7B
Models01020304050Exact Match (%)Baseline
Mean KV
(a) Simple average baseline vs.full
cache on GSM8K
0 20 40 60 80 100
T okens Index0.6
0.4
0.2
0.00.20.40.6Cosine SimilarityLayer 16 - 17
Layer 18 - 19
Layer 20 - 21
Layer 22 - 23
Layer 24 - 25
Layer 26 - 27
Layer 28 - 29
Layer 30 - 31(b) Pairwise similarity in adjacent
layers KV cache
MathQAOpenBookQA
PiQA
RTE
Winogrande0.00.20.40.60.8
MiniCache
Baseline
Mean (c) Benchmark on five QA datasets
Figure 2: Overall of our explorations and observations : (a) shows the strong baseline by performing
average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent
layers. (c) compares the MiniCache, simple average, and full cache baseline across five different
datasets.
3.1 Cross-Layer Redundancy in KV Cache
Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs [ 20]. Thus, layer-
wise early exiting in this case can effectively avoid the redundant computation with minor effect on
LLM performance [ 19,67]. Inspired by this, we explore a layer-wise merging of KV cache in LLMs,
starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key
observations as follows.
Observation 1: KV cache shares a high similarity between adjacent layers. Based on LLaMA-3-
70B [ 6], we conduct zero-shot inference on the validation sets of three widely recognized benchmarks:
COQA [ 68], GSM8K [ 10] and TruthfulQA [ 69]. In general, we find that KV cache in the shallow
layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one
another based on angular distance, as shown in Figure 1(b). Next, we merge the KV cache across
4

--- PAGE 5 ---
adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B [ 5], LLaMA-
3-8B [ 6], and Mixtral-8x7B [ 22] on GSM8K [ 10]. Specifically, starting from the middle layer of
each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we
observe a favourable performance for different LLMs, which reveals the huge potential for efficiency
improvement by sharing the KV cache across adjacent layers during LLM decoding.
Observation 2: Not all tokens are equally important to merge, a few distinct pairs require
retention. Recent works [ 15,16] in KV cache compression have found that keeping a few salient
tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain
LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit
outlier behaviours, showing strong semantic differences that make them unsuitable for merging.
Based on COQA [ 68] and LLaMA-2-7B [ 5], we investigate the similarities at the level of token
pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities
across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large
margins of difference. We consider these tokens non-mergeable due to their significant differences.
We also show that merging these distinct tokens results in performance degradation, corresponding to
Œ≥= 0row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory
burdens, it must be implemented with careful consideration of token-level similarities to ensure
optimal performance, as shown in Figure 2(c).
4 Method
In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV
cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache
states between adjacent layers and consists of two primary components: a reparameterization-based
merging strategy and a token retention mechanism. The merging strategy compresses the KV cache
states in adjacent layers to aggregate them into a single shared memory space, beginning from the
middle of the model. The token retention mechanism mitigates information lost by retaining the
highly distinct state pairs with minimal additional memory cost. With the merged cache, retention
tokens, and magnitudes, we can accurately restore the original cache states for token decoding.
4.1 Cross-Layer Compression
Our method commences with the identification of an optimal starting layer S. Observations in
Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of
high similarity across adjacent layers. Consequently, we select the starting layer from the middle of
the LLM, specifically S=L/2. From this layer onward, the KV pairs are assumed to be sufficiently
similar across adjacent layers to warrant their consolidation. Central to this approach is a merge
function, F, which is designed to integrate the KV caches of consecutive layers into a single, unified
cache. We define xas the vectorized cache state of a single token, where the superscript indicates the
layer index and the subscripts kandvdenote the keys and values, respectively. Specifically, for a
pair of key/value tokens at the same position in layers landl‚àí1, the merged cache is computed as
cl,l‚àí1
k=F(xl
k,xl‚àí1
k),
cl,l‚àí1
v =F(xl
v,xl‚àí1
v).(1)
This consolidation process effectively eliminates the need to store and process the original memory-
intensive keys and values in each layer independently. Instead, it approximates a shared cache across
the adjacent layers.
4.2 KV Cache Merging and Restoration
Reparameterization-based cache merging. To perform the pairwise merging, one solution is to
directly average a pair of KV tokens, analogous to model merging [ 60,61]. However, we observe
that direct averaging can cause significant information loss. We conjecture that the distance between
activations can be larger than that of weights due to the presence of outlier activation channels with
extremely large magnitudes in LLMs [ 70,71], while weights typically have relatively quite small
magnitudes. A potential method to compensate for this information loss is to project from ctoxl‚àí1
andxl, then rescale the projected vectors based on their relative magnitudes to exactly restore the
5

--- PAGE 6 ---
(a)Cross-layer CompressionKVStoreùê∂Keep
Rescale
Recover(b)Restorationùëô‚àí1ùëôùëô‚àí1ùëô
Keep√óFetchùê∂	forlayer ùëôandùëô‚àí1KVcachecompressionat layerùëôoriginal KV Cachemerged KV Cacheretentiontokencachemagnitudemergeoperation
Figure 3: The illustration of the proposed method MiniCache . (a) depicts the cross-layer compression
process. We fetch the KV caches, from layers landl‚àí1, and merge them into shared states via
Eq. (3). Additionally, we compute the ‚Ñì2norm for the caches to obtain their magnitudes. Furthermore,
we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes
at layer linC. (b) illustrates the restoration process for layers landl‚àí1, which includes magnitude
rescaling in Eq. (2) and retention token recovery.
original states. However, this approach requires extensive additional storage and computations; for
example, restoring xl‚àí1needs both candxl, which undermines the benefits of cache merging. To
efficiently merge token pairs, we draw inspiration from weight normalization [ 21], which disentangles
model parameters into the magnitude anddirection components to accelerate the convergence of
stochastic gradient descent. Additionally, we take cues from DoRA [ 72], which employs a similar
way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning.
In our case, the reparameterization can be formulated as follows:
ÀÜxl=el,l‚àí1¬∑‚à•xl‚à•
‚à•el,l‚àí1‚à•,ÀÜxl‚àí1=el,l‚àí1¬∑‚à•xl‚àí1‚à•
‚à•el,l‚àí1‚à•, (2)
where eis the directional vector. This decomposition ensures thatel,l‚àí1
‚à•el,l‚àí1‚à•is a unit vector, and
allows the restored states to match the ‚Ñì2norm of the original states, thereby preserving the cache‚Äôs
information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the
subscripts kandv, as keys and values are decomposed in the same way. For estimating the directional
component el,l‚àí1, we follow SLERP [ 64], which adaptively handles the interpolation, which often
resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as
it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional
vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a).
This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula
for SLERP in our context is:
el,l‚àí1=sin((1 ‚àít)‚Ñ¶l,l‚àí1)
sin(‚Ñ¶l,l‚àí1)¬∑xl‚àí1
‚à•xl‚àí1‚à•+sin(t‚Ñ¶l,l‚àí1)
sin(‚Ñ¶l,l‚àí1)¬∑xl
‚à•xl‚à•, (3)
where ‚Ñ¶l,l‚àí1= arccos
xl¬∑xl‚àí1
‚à•xl‚à•‚à•xl‚àí1‚à•
represents the angle between vectors xlandxl‚àí1, and sin(¬∑)is
thesine function. tis an interpolation hyperparameter that adjusts the relative influence of each vector
on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note
that when we set t= 0.5, it will become an average merging along the geometry surface, which we
consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the
directional vector, magnitude and ‚Ñ¶l,l‚àí1, denoting as cl,l‚àí1= [el,l‚àí1,‚à•xl‚àí1‚à•,‚à•xl‚à•,‚Ñ¶l,l‚àí1], cached
components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we
only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In
this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the
retention of the critical functional characteristics of the original KV pairs across transformer layers.
Unmergeable token retention. Highly distinct pairs are sensitive to merging operations, leading us
to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between
KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly
6

--- PAGE 7 ---
difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial
semantic meanings [ 15,16]. We observe that merging sensitive tokens, which results in the loss of
layer-specific information, can lead to significant performance degradation. Therefore, it is crucial
to properly disentangle the shared and unique information between adjacent layers. To address this
issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based
on their angular distance, defined as: d(xl,xl‚àí1) =1
œÄ‚Ñ¶. For the KV caches, the minimum and
maximum angular distances are determined to identify the unmergeable tokens.
The set of required token indices to keep, I, is obtained by:
I={i|di< d min+ (dmax‚àídmin)¬∑Œ≥}, (4)
where Œ≥is a predefined hyperparameter that controls the retention threshold. The tokens with indices
inIare retained and not compressed during the merge, which ensures that performance does not
decline by preventing the loss of unmergeable tokens.
Next, let X‚ààRn√óhbe either the key or value cache at one attention layer, where ndenotes the
number of tokens and his the number of hidden dimensions, and E‚ààRn√óhbe the shared KV cache
states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with
the token dimension by Rl=Xl[I],Rl‚àí1=Xl‚àí1[I], then restoring to our compressed caches by
ÀÜXl[I] =Rl,ÀÜXl‚àí1[I] =Rl‚àí1, as shown in Figure 3(b). Overall, we share the final cache for the two
layers as Cl,l‚àí1= [El,l‚àí1,Rl,Rl‚àí1,‚à•Xl‚àí1‚à•,‚à•Xl‚à•,I]. This cache includes the shared KV cache
states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index,
respectively. These additional components are quite lightweight. Thus compared to full-layer caches,
our method remains memory-efficient, as discussed in Sec. 4.3.
Cache restoration. After obtaining the shared cache Cl,l‚àí1, we further need to approximately restore
the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore
Xl, we first rescale the directional shared states with the corresponding magnitude vector along the
token dimension, denoted as El,l‚àí1‚à•Xl‚à•. Subsequently, we perform retention token recovery by
placing the sensitive tokens according to their token indices.
4.3 Efficiency Discussion
Compression efficiency. We primarily analyze our memory efficiency in terms of the number of
tokens used. Next, let rbe the number of layers and and bis the batch size, sandnare input and
output sequence length respectively. We consider the FP16 storage for KV cache. The full cache
memory usage is given by 4brh(s+n). In our study, we begin merging layers from the middle to the
deeper layers, consolidating the KV cache states of every two layers into a single shared state space.
As a result, we effectively reduce the GPU memory usage in the decoding inference to 3brh(s+n),
demonstrating a significant compression rate.
Restoration efficiency. We then analyze the additional memory cost incurred during the restora-
tion process, which During the magnitude rescaling phase, we save an additional norm vector for
the corresponding layers in the KV cache. It is important to note that the norm vector is in the
shape of Rb√ós√ó1, which has a single channel dimension compared to the fully ranked original KV
states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have
brh(0.05(s+n))tokens retained without compression. Finally, our overall memory requirement is
given by (3.1h+ 2)br(s+n). The detailed derivation is shown in the Appendix E.
5 Experiments
We demonstrated that our MiniCache can perform merging compression on the latter half of the
layers of LLMs with minimal performance degradation.
Implementation details. Our experiments are based on representative model families of LLMs,
including a compact LLM Phi-3-Mini [ 23] and an MoE LLM Mixtral-8x7B [ 22]. Additionally, we
adopt LLaMA-3 [ 6] 8B and 70B models to explore how our method generalizes to larger LLMs. We
sample ten tasks from lm-eval-harness [ 32], including COPA [ 24], MathQA [ 25], OpenBookQA [ 26],
PIQA [ 27], RTE [ 28], WinoGrande [ 29], XSUM [ 30], and CNN/Daily Mail [ 31]. We also evaluate
long-sequence generation on LongBench [ 33]. We compare our method with a fully cached baseline,
7

--- PAGE 8 ---
and other methods such as round-to-nearest quantization (RTN) [ 73], SmoothQuant [ 70] and KIVI
[11].
For the proposed MiniCache, we set the interpolation parameter tto 0.6, indicating that the merged
results have a smaller rotation angle to the next layer. Furthermore, we set the token retention
threshold Œ≥to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In
addition to our merging method, we also consider a strong baseline of average merging. For sequential
loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D.
Main results. We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA,
and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effective-
ness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the
proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline.
We also illustrate the performance of merging KV caches across half of the layers with the blue
lines, where MiniCache still maintains a robust performance and achieves the best compression ratio.
Besides, we find that our method is even more effective for larger LLMs. For instance, based on
LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in 87.5%
of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our
approach in handling large-scale models while ensuring minimal performance degradation.
LongBench. We also conduct experiments to evaluate performance and quality in long-sequence gen-
eration using the LongBench dataset [ 33], as shown in Table 1. Our experiments applied MiniCache
over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct.
It is important to note that our MiniCache method maintains orthogonality with all existing quan-
tization and sparsity (refers to Table A) methods at both the model and token-wise levels. When
combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of 5.02√ó,
with minimal impact on accuracy across various challenging long-context generation tasks. The
combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory
savings without compromising the model‚Äôs ability to handle long sequences effectively. This high-
Figure 4: Performance comparisons between our proposed MiniCache with the ‚Äúaveraging baseline‚Äù
and the ‚Äúunmerged full cache baseline‚Äù on multiple datasets with Phi3-Mini, Mixtral-8x7B, LLaMA-
3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The x-axis indicates the
number of layers merged. As more layers are merged, a greater reduction in memory usage is
achieved.
8

--- PAGE 9 ---
Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds
on top of 4-bit KIVI [11] and achieves the best performance with the strongest compression rate.
Model Method LCC RepoBench-P PR-en TREC 2wikimqa GovReport MQA-zh AverageCompression
Ratio
Llama-2-7B-ChatBaseline 58.16 52.19 10.12 64.00 31.12 27.09 10.12 36.41 1x
RTN [73] 15.44 8.76 0.79 4.00 0.30 1.93 0.07 4.90 3.21x
SmoothQuant [70] 35.31 32.18 0.79 28.75 7.45 11.83 1.68 16.28 2.15x
KIVI-2 [11] 49.32 43.71 4.50 63.00 24.07 24.73 10.24 31.51 3.95x
MiniCache 58.03 52.01 9.00 64.00 30.58 25.32 10.13 35.44 5.02x
Llama-2-13B-ChatBaseline 48.06 50.08 14.25 68.50 13.09 27.76 7.23 32.71 1x
RTN [73] 20.89 18.62 0.33 0.00 0.52 1.68 0.16 6.03 3.21x
SmoothQuant [70] 32.17 33.86 2.65 48.00 3.53 12.47 0.47 19.16 2.15x
KIVI-2 [11] 48.60 48.81 13.50 68.00 14.32 25.70 7.01 32.42 3.95x
MiniCache 48.75 48.59 13.00 68.00 14.36 26.57 7.99 32.61 5.02x
Mistral-7BBaseline 68.06 60.46 17.71 68.00 10.87 20.09 17.10 37.33 1x
RTN [73] 27.98 26.18 3.34 13.00 1.11 2.49 0.45 10.51 3.21x
SmoothQuant [70] 40.63 35.14 3.40 30.50 6.03 5.00 4.12 17.55 2.15x
KIVI-2 [11] 65.16 58.33 12.43 65.00 11.03 13.22 13.87 33.43 3.95x
MiniCache 68.89 60.98 13.92 67.00 10.50 18.06 7.88 35.75 5.02x
Mistral-7B-InstructBaseline 55.51 48.96 60.00 71.00 27.33 32.85 42.74 48.32 1x
RTN [73] 32.36 33.23 0.67 1.00 2.25 10.03 2.30 11.55 3.21x
SmoothQuant [70] 43.84 38.63 4.79 39.50 10.34 23.61 8.33 24.43 2.15x
KIVI-2 [11] 53.13 48.60 47.50 69.00 20.68 29.37 33.88 43.74 3.95x
MiniCache 54.79 51.02 64.14 71.00 24.97 31.46 27.54 46.99 5.02x
lights the potential of our method to optimize large language models for tasks requiring extensive
context, making them more efficient and scalable for real-world applications.
Efficiency analysis. To assess the acceleration capabilities of MiniCache, we conduct evaluations
based on the methodologies employed in vLLM [ 74] and KIVI [ 11]. We generate synthetic workloads
derived from ShareGPT, which include real input and output texts from LLM services. The dataset
features an average input prompt length of 161 tokens and an output length of 338 tokens. Using
the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a
batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit
MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, MiniCache
reduces memory usage by 25GB, achieving a 41% memory saving . In terms of throughput,
MiniCache outperforms the FP16 baseline by approximately 5 √ó. Additionally, despite utilizing
4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers,
resulting in a 1.29 √óhigher throughput compared to the 2-bit KIVI. These results demonstrate that
MiniCache offers a state-of-the-art trade-off between efficiency and performance.
50 100 150 200 250 300
Batch Size20304050607080Peak Memory Usage (GB)
Baseline FP16
KIVI 2
MINICache 4
(a) BS. vs.Peak Memory Usage
50 100 150 200 250 300
Batch Size10001500200025003000Throughput (tokens/sec)
Baseline FP16
KIVI 2
MINICache 4 (b) BS. vs.Decoding Throughput
Figure 5: Memory usage and throughput comparison between
our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache
can achieve higher throughput by enabling a larger batch size
while reducing memory footprints via LLaMA-2-7B [5].
0.3 0.4 0.5 0.6 0.7
Interpolation Parameter t0.3500.3750.4000.4250.4500.4750.500Exact Match Scores
GSM8k
0.00.20.40.60.81.01.2
Normalized FrequencyFrequencyFigure 6: LLaMA-3-8B [ 6] to
experiment on the GSM8K [ 10].
The right axis is the normalized
frequency of the relative magni-
tude ratio. Optional tshows a
strong correlation with frequency.
6 Ablation StudyTable 2: Comparisons of various token
retention thresholds Œ≥by LLaMA-2-7B
[5] on three benchmarks.
Œ≥ COQA GSM8K TruthfulQA
0 0.603 0.108 29.813
0.01 0.620 0.126 30.226
0.02 0.630 0.143 33.903
0.05 0.647 0.152 33.213
0.1 0.643 0.152 33.903
1 0.643 0.159 33.743The effect of interpretation parameter t.We explore
the effects of the interpretation parameter ton perfor-
mance, particularly in relation to the relative magnitude
ratio of adjacent layers, as shown in Figure 6. We maintain
all settings constant, starting from layer S= 16 (halfway
9

--- PAGE 10 ---
through the layers of LLaMA-3-8B), and vary the interpretation parameter tfrom 0.3 to 0.7. Our
findings reveal several key points. When t= 0.5, the process resembles average merging, which is
less effective for cross-layer merging. In contrast, when t= 0.6is optimal, the merged representation
exhibits the most robust performance, while indicating that more information is derived from the
second term ( xl) of the SLERP.
The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6,
corroborating our optimal t. Moreover, there is a strong correlation between the optimal tand the high
frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to
utilize the relative magnitude ratio to dynamically determine the interpretation parameter t. Dynamic
tallows for more flexible weight control in SLERP merging for each layer-wise operation, thereby
showing potential for further exploration.
The effect of token retention threshold Œ≥.We investigate the impact of the token retention threshold
Œ≥on model performance across the three datasets, as shown in Table 2. A larger tgenerally means
retaining more tokens for improved performance, but this comes at the cost of increased memory
demand. The results suggest that setting Œ≥to 0.05 achieves the best balance between performance
and efficiency.
7 Conclusion and Future Work
This paper presents a pioneering exploration of KV cache compression in the depth dimension,
addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple,
effective, and training-free approach to compressing KV caches by leveraging the notable high
similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We
have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM
inference by up to 41%, while simultaneously enhancing throughput by approximately five times
compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV
cache compression, offering a state-of-the-art balance between efficiency and performance. Future
work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing
advanced merging algorithms such as Spherical Cubic Interpolation [ 75], and further optimizing
memory usage for large-scale deployments in diverse application scenarios.
References
[1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. , ‚ÄúLanguage models are few-shot learners,‚Äù in NeurIPS , vol. 33,
pp. 1877‚Äì1901, 2020.
[2]OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, and OTHERS, ‚ÄúGpt-4 technical report,‚Äù 2023.
[3]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A. Ray, et al. , ‚ÄúTraining language models to follow instructions with human feedback,‚Äù
inNeurIPS , vol. 35, pp. 27730‚Äì27744, 2022.
[4]H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,
E. Hambro, F. Azhar, et al. , ‚ÄúLlama: Open and efficient foundation language models,‚Äù arXiv
preprint arXiv:2302.13971 , 2023.
[5]H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, et al. , ‚ÄúLlama 2: Open foundation and fine-tuned chat models,‚Äù arXiv
preprint arXiv:2307.09288 , 2023.
[6]‚ÄúIntroducing meta llama 3: The most capable openly available llm to date.‚Äù https://ai.meta.
com/blog/meta-llama-3/ , 2024. Accessed: 2024-05-04.
[7]J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-
ford, J. Wu, and D. Amodei, ‚ÄúScaling laws for neural language models,‚Äù arXiv preprint
arXiv:2001.08361 , 2020.
10

--- PAGE 11 ---
[8]G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.
Dai, A. Hauth, et al. , ‚ÄúGemini: a family of highly capable multimodal models,‚Äù arXiv preprint
arXiv:2312.11805 , 2023.
[9]R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and
J. Dean, ‚ÄúEfficiently scaling transformer inference,‚Äù Proceedings of Machine Learning and
Systems , vol. 5, 2023.
[10] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J. Hilton, R. Nakano, et al. , ‚ÄúTraining verifiers to solve math word problems,‚Äù arXiv preprint
arXiv:2110.14168 , 2021.
[11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu, ‚ÄúKivi: Plug-
and-play 2bit kv cache quantization with streaming asymmetric quantization,‚Äù arXiv preprint
arXiv:2402.02750 , 2024.
[12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, ‚ÄúGear: An efficient
kv cache compression recipefor near-lossless generative inference of llm,‚Äù arXiv preprint
arXiv:2403.05527 , 2024.
[13] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. R√©, I. Stoica, and
C. Zhang, ‚ÄúFlexgen: High-throughput generative inference of large language models with a
single gpu,‚Äù in ICML , pp. 31094‚Äì31116, PMLR, 2023.
[14] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. R√©, C. Barrett,
et al. , ‚ÄúH2o: Heavy-hitter oracle for efficient generative inference of large language models,‚Äù in
NeurIPS , vol. 36, 2024.
[15] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, ‚ÄúEfficient streaming language models with
attention sinks,‚Äù in ICLR , 2024.
[16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, ‚ÄúModel tells you what to discard:
Adaptive kv cache compression for llms,‚Äù ICLR , 2024.
[17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, ‚ÄúMixture-of-
depths: Dynamically allocating compute in transformer-based language models,‚Äù arXiv preprint
arXiv:2404.02258 , 2024.
[18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, ‚ÄúBert loses patience: Fast and robust
inference with early exit,‚Äù in NeurIPS , vol. 33, pp. 18330‚Äì18341, 2020.
[19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun,
S. Agarwal, A. Roman, et al. , ‚ÄúLayer skip: Enabling early exit inference and self-speculative
decoding,‚Äù arXiv preprint arXiv:2404.16710 , 2024.
[20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, ‚ÄúThe unreasonable
ineffectiveness of the deeper layers,‚Äù arXiv preprint arXiv:2403.17887 , 2024.
[21] T. Salimans and D. P. Kingma, ‚ÄúWeight normalization: A simple reparameterization to accelerate
training of deep neural networks,‚Äù in NeurIPS , vol. 29, 2016.
[22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l.
Casas, E. B. Hanna, F. Bressand, et al. , ‚ÄúMixtral of experts,‚Äù arXiv preprint arXiv:2401.04088 ,
2024.
[23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree,
A. Bakhtiari, H. Behl, et al. , ‚ÄúPhi-3 technical report: A highly capable language model locally
on your phone,‚Äù arXiv preprint arXiv:2404.14219 , 2024.
[24] M. Roemmele, C. A. Bejan, and A. S. Gordon, ‚ÄúChoice of plausible alternatives: An evaluation
of commonsense causal reasoning.,‚Äù in AAAI spring symposium: logical formalizations of
commonsense reasoning , pp. 90‚Äì95, 2011.
11

--- PAGE 12 ---
[25] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y . Choi, and H. Hajishirzi, ‚ÄúMathqa:
Towards interpretable math word problem solving with operation-based formalisms,‚Äù in NAACL ,
pp. 2357‚Äì2367, 2019.
[26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, ‚ÄúCan a suit of armor conduct electricity? a
new dataset for open book question answering,‚Äù in EMNLP , 2018.
[27] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, ‚ÄúPiqa: Reasoning about physical common-
sense in natural language,‚Äù in AAAI , 2020.
[28] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, ‚ÄúGlue: A multi-
task benchmark and analysis platform for natural language understanding,‚Äù arXiv preprint
arXiv:1804.07461 , 2018.
[29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, ‚ÄúWinogrande: An adversarial winograd
schema challenge at scale,‚Äù Communications of the ACM , vol. 64, no. 9, pp. 99‚Äì106, 2021.
[30] S. Narayan, S. B. Cohen, and M. Lapata, ‚ÄúDon‚Äôt give me the details, just the summary!
topic-aware convolutional neural networks for extreme summarization,‚Äù arXiv preprint
arXiv:1808.08745 , 2018.
[31] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al. , ‚ÄúAbstractive text summarization using
sequence-to-sequence rnns and beyond,‚Äù arXiv preprint arXiv:1602.06023 , 2016.
[32] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu,
A. Le Noac‚Äôh, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds,
H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, ‚ÄúA
framework for few-shot language model evaluation,‚Äù 12 2023.
[33] Y . Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al. ,
‚ÄúLongbench: A bilingual, multitask benchmark for long context understanding,‚Äù arXiv preprint
arXiv:2308.14508 , 2023.
[34] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, ‚ÄúSkipdecode:
Autoregressive skip decoding with batching and caching for efficient llm inference,‚Äù arXiv
preprint arXiv:2307.02628 , 2023.
[35] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V . Tran, Y . Tay, and D. Metzler,
‚ÄúConfident adaptive language modeling,‚Äù in NeurIPS , vol. 35, pp. 17456‚Äì17472, 2022.
[36] H. Wu and K. Tu, ‚ÄúLayer-condensed kv cache for efficient inference of large language models,‚Äù
2024.
[37] W. Fedus, B. Zoph, and N. Shazeer, ‚ÄúSwitch transformers: Scaling to trillion parameter models
with simple and efficient sparsity,‚Äù Journal of Machine Learning Research , vol. 23, no. 120,
pp. 1‚Äì39, 2022.
[38] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen,
‚ÄúGshard: Scaling giant models with conditional computation and automatic sharding,‚Äù arXiv
preprint arXiv:2006.16668 , 2020.
[39] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y . Wu, et al. ,
‚ÄúDeepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,‚Äù
arXiv preprint arXiv:2401.06066 , 2024.
[40] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram, et al. ,
‚ÄúTutel: Adaptive mixture-of-experts at scale,‚Äù Proceedings of Machine Learning and Systems ,
vol. 5, 2023.
[41] DeepSeek-AI, ‚ÄúDeepseek-v2: A strong, economical, and efficient mixture-of-experts language
model,‚Äù arXiv preprint arXiv:2405.04434 , 2024.
[42] J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebr√≥n, and S. Sanghai, ‚ÄúGqa: Training
generalized multi-query transformer models from multi-head checkpoints,‚Äù arXiv preprint
arXiv:2305.13245 , 2023.
12

--- PAGE 13 ---
[43] N. Shazeer, ‚ÄúFast transformer decoding: One write-head is all you need,‚Äù arXiv preprint
arXiv:1911.02150 , 2019.
[44] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins,
J. Davis, A. Mohiuddin, L. Kaiser, et al. , ‚ÄúRethinking attention with performers,‚Äù arXiv preprint
arXiv:2009.14794 , 2020.
[45] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, ‚ÄúRandom feature
attention,‚Äù in ICLR , 2021.
[46] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R√©, ‚ÄúFlashattention: Fast and memory-efficient exact
attention with io-awareness,‚Äù in NeurIPS , vol. 35, pp. 16344‚Äì16359, 2022.
[47] T. Dao, ‚ÄúFlashattention-2: Faster attention with better parallelism and work partitioning,‚Äù arXiv
preprint arXiv:2307.08691 , 2023.
[48] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, ‚ÄúLinformer: Self-attention with linear
complexity,‚Äù arXiv preprint arXiv:2006.04768 , 2020.
[49] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, ‚ÄúLuna: Linear unified
nested attention,‚Äù in NeurIPS , vol. 34, pp. 2441‚Äì2453, 2021.
[50] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, ‚ÄúSet transformer: A framework for
attention-based permutation-invariant neural networks,‚Äù in ICML , pp. 3744‚Äì3753, PMLR, 2019.
[51] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, ‚ÄúAwq: Activation-aware weight
quantization for llm compression and acceleration,‚Äù arXiv preprint arXiv:2306.00978 , 2023.
[52] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, ‚ÄúQlora: Efficient finetuning of
quantized llms,‚Äù in NeurIPS , vol. 36, 2023.
[53] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, ‚ÄúLlm.int8(): 8-bit matrix multiplication
for transformers at scale,‚Äù in NeurIPS , vol. 35, pp. 30318‚Äì30332, 2022.
[54] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, ‚ÄúQllm: Accurate and efficient
low-bitwidth quantization for large language models,‚Äù in ICLR , 2024.
[55] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang, et al. , ‚ÄúLoraprune: Pruning meets
low-rank parameter-efficient fine-tuning,‚Äù in ACL findings , 2024.
[56] N. Kitaev, ≈Å. Kaiser, and A. Levskaya, ‚ÄúReformer: The efficient transformer,‚Äù arXiv preprint
arXiv:2001.04451 , 2020.
[57] S. K. Ainsworth, J. Hayase, and S. Srinivasa, ‚ÄúGit re-basin: Merging models modulo permutation
symmetries,‚Äù in ICLR , 2023.
[58] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur, ‚ÄúThe role of permutation invariance in
linear mode connectivity of neural networks,‚Äù arXiv preprint arXiv:2110.06296 , 2021.
[59] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, ‚ÄúLoss surfaces, mode
connectivity, and fast ensembling of dnns,‚Äù in NeurIPS , vol. 31, 2018.
[60] M. Wortsman, G. Ilharco, S. Y . Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos,
H. Namkoong, A. Farhadi, Y . Carmon, S. Kornblith, et al. , ‚ÄúModel soups: averaging weights
of multiple fine-tuned models improves accuracy without increasing inference time,‚Äù in ICML ,
pp. 23965‚Äì23998, PMLR, 2022.
[61] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal, ‚ÄúTies-merging: Resolving interfer-
ence when merging models,‚Äù in NeurIPS , vol. 36, 2023.
[62] M. Davari and E. Belilovsky, ‚ÄúModel breadcrumbs: Scaling multi-task model merging with
sparse masks,‚Äù arXiv preprint arXiv:2312.06795 , 2023.
[63] L. Yu, B. Yu, H. Yu, F. Huang, and Y . Li, ‚ÄúLanguage models are super mario: Absorbing
abilities from homologous models as a free lunch,‚Äù arXiv preprint arXiv:2311.03099 , 2023.
13

--- PAGE 14 ---
[64] K. Shoemake, ‚ÄúAnimating rotation with quaternion curves,‚Äù in Proceedings of the 12th annual
conference on Computer graphics and interactive techniques , pp. 245‚Äì254, 1985.
[65] M. S. Matena and C. A. Raffel, ‚ÄúMerging models with fisher-weighted averaging,‚Äù in NeurIPS ,
vol. 35, pp. 17703‚Äì17716, 2022.
[66] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, ‚ÄúDataless knowledge fusion by merging
weights of language models,‚Äù arXiv preprint arXiv:2212.09849 , 2022.
[67] Y . Chen, X. Pan, Y . Li, B. Ding, and J. Zhou, ‚ÄúEe-llm: Large-scale training and inference of
early-exit large language models with 3d parallelism,‚Äù arXiv preprint arXiv:2312.04916 , 2023.
[68] S. Reddy, D. Chen, and C. D. Manning, ‚ÄúCoqa: A conversational question answering challenge,‚Äù
Transactions of the Association for Computational Linguistics , vol. 7, pp. 249‚Äì266, 2019.
[69] S. Lin, J. Hilton, and O. Evans, ‚ÄúTruthfulqa: Measuring how models mimic human falsehoods,‚Äù
arXiv preprint arXiv:2109.07958 , 2021.
[70] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, ‚ÄúSmoothquant: Accurate and
efficient post-training quantization for large language models,‚Äù in International Conference on
Machine Learning , pp. 38087‚Äì38099, PMLR, 2023.
[71] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, ‚ÄúQllm: Accurate and efficient
low-bitwidth quantization for large language models,‚Äù in ICLR , 2024.
[72] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen,
‚ÄúDora: Weight-decomposed low-rank adaptation,‚Äù arXiv preprint arXiv:2402.09353 , 2024.
[73] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, ‚ÄúUp or down? adaptive
rounding for post-training quantization,‚Äù in International Conference on Machine Learning ,
pp. 7197‚Äì7206, PMLR, 2020.
[74] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica,
‚ÄúEfficient memory management for large language model serving with pagedattention,‚Äù in
Proceedings of the 29th Symposium on Operating Systems Principles , pp. 611‚Äì626, 2023.
[75] D. H. Eberly, ‚ÄúQuaternion algebra and calculus,‚Äù 2002.
[76] ‚ÄúStanford crfm.‚Äù https://crfm.stanford.edu/2023/10/12/flashdecoding.html ,
2024. Accessed: 2024-05-04.
[77] Y . Liu, H. Li, K. Du, J. Yao, Y . Cheng, Y . Huang, S. Lu, M. Maire, H. Hoffmann, A. Holtzman,
et al. , ‚ÄúCachegen: Fast context loading for language model applications,‚Äù arXiv preprint
arXiv:2310.07240 , 2023.
[78] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, ‚ÄúAn image is worth 1/2 tokens
after layer 2: Plug-and-play inference acceleration for large vision-language models,‚Äù arXiv
preprint arXiv:2403.06764 , 2024.
[79] S. Wei, T. Ye, S. Zhang, Y . Tang, and J. Liang, ‚ÄúJoint token pruning and squeezing towards
more aggressive compression of vision transformers,‚Äù in CVPR , pp. 2092‚Äì2101, 2023.
[80] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell,
N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, ‚ÄúA
framework for few-shot language model evaluation,‚Äù 2021.
[81] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica,
‚ÄúEfficient memory management for large language model serving with pagedattention,‚Äù in
Proceedings of the 29th Symposium on Operating Systems Principles , pp. 611‚Äì626, 2023.
14

--- PAGE 15 ---
Appendix
A Additional Experiment Results
Comparisons with token sparsity methods . We also compare MiniCache with the sparsity-based
method H2O [ 14]. Our method outperforms H2O in most tasks on LongBench. Notably, our approach
focuses on reducing inter-layer redundancy, whereas H2O focuses on intra-layer redundancy. Our
approaches are orthogonal to sparsity-based methods.
Table A: Comparison between MiniCache with token sparsity based method H2O, using mistral-7B-
instruct on LongBench dataset.
Methods NrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc
Baseline 26.82 33.06 49.28 42.77 27.33 19.27 32.85 24.25 27.06 71.0 86.23 42.98 2.75 86.98 55.51
H2O[14] 22.61 29.06 47.22 36.54 20.6 16.25 30.0 23.8 26.75 70.5 86.16 42.97 3.46 86.38 53.72
MiniCache 27.04 32.59 49.38 43.91 24.97 18.3 31.46 23.85 26.64 71.0 86.93 43.6 3.04 79.56 54.79
B Additional Related Work
Preliminaries of KV cache in LLMs. LLM inference has been significantly improved in recent
works [ 9,13,74] by optimizing the KV cache management. Overall, this line of research is typically
done in two steps: 1) First, at the prefilling stage, LLM calculates the initial KV caches at each
attention layer based on the input sequence and decodes the first token. Second, 2) at the decoding
stage, LLM autoregressively predicts the next token, where its KV cache at each attention layer is
added to the overall caches. Existing works have compressed KV cache in different aspects (e.g.,
quantization [11, 12], token pruning [14, 16] ).
KV cache compression. In the prior study, various strategies for enhancing efficient transformer
architectures are discussed, covering a spectrum of techniques aimed at optimizing performance
and managing resource constraints. These methods include attention optimization [ 46,47,76],
grouping queries [ 42,43], sparse KV caching [ 16,77,78], shrinking tokens [ 15,79], and improving
long-context generation. Significant contributions come from projects such as H2O [ 15], GEAR
[15], and KIVI [ 11]. Additionally, efforts to alleviate KV cache bottlenecks include strategies like
multi-query attention [ 42] and multi-group attention [ 43], which propose reducing the number of
heads in the KV cache. However, these methods often necessitate retraining or fine-tuning models.
Other approaches focus on diminishing the size of the KV cache by selectively evicting less important
tokens [ 14] and enhancing the system architecture through technologies like offloading the KV cache
[80] or integrating techniques such as virtual memory and paging [ 81] into the attention mechanism.
C Discussions and Limitations
Alternative merging function. During our preliminary exploration, we initially considered an alter-
native, simpler merge function for cross-layer compression: maximum norm-preserving interpolation.
This function is designed to maintain the maximum norm of the vectors involved, ensuring that the
most significant features are preserved during the merging process. The maximum norm-preserving
interpolation in terms of Fmaxcan be defined as follows:
Fmax(xl,xl‚àí1) =¬Øxl,l‚àí1
‚à•¬Øxl,l‚àí1‚à•¬∑Max(‚à•xl‚à•,‚à•xl‚àí1‚à•). (A)
Here ¬Øxl,l‚àí1represents the average vector between xlandxl‚àí1. The function Fmaxensures that the
merged vector preserves the direction of the average vector while scaling it to the maximum norm of
the original KV states. Compared to the SLERP-based merge function, Fmaxhas less computational
overhead and lower memory consumption. However, it is less accurate than SLERP. The choice
between using FSLERP orFmaxdepends on the specific requirements of the application. In our study,
we primarily use SLERP to maximize performance.
Societal impact. Our work shows a preliminary exploration of KV cache Compression in the
depth dimension, a relatively unexplored yet critically bottlenecked area in large language models
15

--- PAGE 16 ---
(LLMs). The proposed MiniCache provides a solution to improve the efficiency of LLM generation
and is adaptable to existing intra-layer-based KV cache pruning and quantization technologies.
Additionally, we proposed a simple yet effective approach that merges similar KV cache states in a
cross-layer manner and effectively restores performance through a novel restoration technique. Our
observations indicate that cross-layer similarity and mergeability can be applied in broad-efficient
applications for post-training optimization in low-resource scenarios, such as deployment on mobile
devices. Moreover, with effective optimization of KV cache memory demand, MiniCache further
enhances long-context generation, which is a crucial paradigm for real-world applications, such as
understanding concepts in textbooks. We aim for our work to advance the boundaries of two key
challenges in the LLM industry and research: batch inference and long-context generation.
Furthermore, in addition to the significant contributions of MiniCache for KV cache Compression,
several challenges remain that are common to LLMs. Issues such as the truthfulness and security
of LLMs are still unresolved. Ensuring the accuracy and reliability of generated content is critical,
as LLMs can sometimes produce plausible but incorrect or misleading information. Additionally,
safeguarding against security vulnerabilities, such as adversarial attacks or data leakage, is paramount
to maintaining the integrity and confidentiality of user interactions. Addressing these challenges
requires ongoing research and development to enhance the robustness and trustworthiness of LLMs.
This effort must proceed alongside advancements in computational efficiency and performance, as
exemplified by innovations like MiniCache.
Limitations. The current merging algorithm based on Spherical Linear Interpolation (SLERP) has its
limitations. SLERP is suitable only for merging two vectors and uses an interpolation approach that
restricts our algorithm from merging multiple layers simultaneously and maximizing the compression
ratio in further states. This limitation impacts the overall efficiency of KV cache compression and
underscores the need for advanced techniques capable of handling more complex merging scenarios.
Future research should focus on developing more sophisticated algorithms that can overcome these
constraints, thereby enhancing the compression capabilities and overall performance of LLMs.
D Additional Implementation Details
Overview of the inference algorithm. The MiniCache inference implementation, as shown in
Alg. 1, involves several key steps. When the interface starts, In the prefilling phase, we define the
merging starting layer S. Before reaching layer S, the inference uses the original attention and cache
logic. From layer Sonward, we implement our merging algorithm, which operates in a cross-layer
manner and is applied only at odd-numbered layers. During merging, we fetch the KV cache from
the previous layer and save the merged shared states into the current layer‚Äôs KV cache. To reduce
memory usage, we then remove the cache of the previous layer. Additionally, we store the magnitudes
vector and retention-sensitive tokens for each layer. During the decoding phase, two scenarios arise
after layer S. For even-numbered layers (first round), since the KV cache has been removed during
the prefilling phase, we refer to the next layer ( l+ 1) to fetch the shared KV cache states. We then
perform approximated scale restoration and retention token recovery. The new KV states from this
phase are stored for use in the next round. In the second round, which involves odd-numbered layers,
we use the new KV tokens from both the previous and current layers. After the restoration phase, we
perform the merge operations and update the shared KV cache states in the stack.
Cross-Layer merging and restoration algorithm. As outlined in Alg. 2, MiniCache algorithm
involves several crucial steps to ensure efficient memory usage while maintaining the integrity of the
Key-Value (KV) Cache states. Initially, given the KV cache El
k,v, norm values ‚à•Xl
k,v‚à•unmerged
tokens Rl
k,v, retention indices Ik,v, and the next tokens tl,tl‚àí1, the algorithm proceeds by rescaling
the magnitude of the KV pairs. Specifically, ÀÜXl
kandÀÜXl
vare computed by multiplying the normalized
KV pairs El
k,vwith their respective magnitude norms ‚à•Xl
k,v‚à•. Following this, the algorithm restores
unmerged tokens using the retention indices, updating ÀÜXl
kandÀÜXl
vaccordingly. Next, the new tokens
tkandtvare concatenated to the rescaled KV pairs along the token dimension. This augmented
KV cache undergoes a softmax attention mechanism where the attention scores Aare computed by
taking the dot product of the query token tqwith the transposed keys (ÀÜXl
k)‚ä§. The output token tOis
then obtained by multiplying the attention scores Awith the values ÀÜXl
v. In cases where the previous
tokentl‚àí1exists, the algorithm performs a compression step. It concatenates the existing KV cache
16

--- PAGE 17 ---
El
k,vwith the merged tokens resulting from the current and previous layers, effectively reducing
redundancy and optimizing memory. If tl‚àí1is not available, the KV cache is updated by simply
concatenating El
k,vwith the new tokens tl
k,v, deferring the compression until the next iteration. The
final output token tOis then returned, concluding the decoding process. In the merging function,
the algorithm normalizes the KV pairs from both the current and previous layers. It calculates the
angular distance ‚Ñ¶between the normalized vectors, ensuring that the interpolation occurs along the
shortest path on the unit sphere. The merged KV cache is then obtained by weighted interpolation
of the normalized vectors, preserving the geometric and semantic integrity of the original states.
This comprehensive process allows MiniCache to achieve substantial memory efficiencies while
maintaining the functional characteristics of the KV pairs across transformer layers.
Layerùëô
QKV
Attention
Cross-Layer Merging
KV
KVcachecompressionat layerùëô
Layerùëô‚àí1
...ùúí!ùúí!"#
ùêæ,ùëâC
1Keep
Fetch2Merge
3
Cache
4Delete
Recovery
Rescale
Retention Recovery
5Fetch
Contact New Token
QKV
Recovery
Attention......
6Fetch
Cross Layer MergingTime
7PrefillingDecoding
Figure A: Overall prefilling and decoding logic for MiniCache involves performing cross-layer
merging and recovery within our framework.
MiniCache execution flow. Figure A delineates the pre-filling and decoding logic for the MiniCache
framework, which incorporates cross-layer merging and error suppression to achieve memory effi-
ciency and maintain functional integrity. Initially, in Step 1, the KV cache is fetched from the previous
layer (Layer L‚àí1) during the pre-filling phase. In Step 2, the fetched KV pairs from the current layer
œáLare merged with the KV pairs from the preceding layer œáL‚àí1, reducing redundancy through a
17

--- PAGE 18 ---
merge function. Subsequently, in Step 3, the merged KV pairs are cached for future use, representing
a consolidated data set from multiple layers. During the decoding phase, Step 4 involves deleting
unnecessary or redundant KV pairs to optimize memory usage. In Step 5, the decoder fetches the
required KV pairs from the cache for output generation. Step 6 applies error suppression mechanisms
to the fetched KV pairs, including rescaling and retention recovery, to minimize errors introduced
during the merging and compression processes. Finally, in Step 7, the cache is updated with the
final KV pairs post-error suppression and adjustments, ensuring the cache contains the most accurate
and efficient representation of the KV pairs for subsequent layers. This comprehensive approach
guarantees substantial memory efficiencies while preserving the critical functional characteristics of
the original KV pairs across transformer layers.
Algorithm 1: The MiniCache Inference Algorithm
1procedure MiniCache Inference :
Input: Input Tokens: T‚ààRtinput√ód, number of layers L, merging beginning layer S
Output: Output Tokens: O‚ààRtoutput√ód
2 forl‚Üê0toS‚àí1do
3 procedure Standard Prefill :
4 Standard Attention & Standard Cache
5 procedure Standard Decoding :
6 Standard Attention & Standard Cache
// Start Merging from layer S
7 forl‚ÜêStoLdo
// Perform Merging in every two layers l%2 == 1
8 ifl%2 == 1 and prefilling then
9 procedure MiniCache Prefill :
Input: KV cache from Current layer l:Xl
k,v, KV cache from Previous layer
l‚àí1:Xl‚àí1
k,v, token retention threshold: Œ≥
10 Delete KV cache of the l‚àí1-th layer // layer l, l‚àí1shares one Cache
11 Standard Attention & Standard Cache
// Perform Merging in the second layer
12 ifand decoding then
13 ifl%2 == 0 then
// first round in the cross-layer merging, fetch shared KV
cache states from Cl+1
k,v
14 procedure MiniCache Decoding :
Input: KV cache: Cl+1
k,v, Norm: ‚à•Xl
k,v‚à•, Unmerged Tokens: Rl
k,v,
Retention indices: Ik,v, Next Token: tl
15 else
// second round in cross-layer merging, while tl‚àí1exist
16 procedure MiniCache Decoding :
Input: KV cache: Cl
k,v, Norm: ‚à•Xl
k,v‚à•, Unmerged Tokens: Rl
k,v, Retention
indices: Ik,v, Next Token: tl, tl‚àí1
17 return O
18

--- PAGE 19 ---
Algorithm 2: The MiniCache Prefill & Decoding Compression Algorithm
Hyperparameter: number of layers L, merging beginning layer S
1procedure MiniCache Prefill :
Input: KV cache from current layer l:Xl
k,v‚ààR2√ótprompt√ód, KV cache from previous layer
l‚àí1:Xl‚àí1
k,v‚ààR2√ótprompt√ód, token retention threshold: Œ≥
2El
k,‚à•Xl
k‚à•,‚à•Xl‚àí1
k‚à•,‚Ñ¶k‚ÜêMerge (Xl
k,Xl‚àí1
k);
3El
v,‚à•Xl
v‚à•,‚à•Xl‚àí1
v‚à•,‚Ñ¶v‚ÜêMerge (Xl
v,Xl‚àí1
v);
4El
k,v‚ààRtprompt√ód
|{z }
compression output,‚à•Xl,l‚àí1
k,v‚à• ‚ààR4√ótprompt√ó1
| {z }
norms for rescaling;
5 d(Xl,Xl‚àí1)k,v=1
œÄ¬∑‚Ñ¶k,v// distance metrics
6 Ik,v={i|di< d min+ (dmax‚àídmin)¬∑Œ≥}// retention indices
7Rl,l‚àí1
k‚ÜêXl,l‚àí1
k[Ik],Rl,l‚àí1
v‚ÜêXl,l‚àí1
v[Iv]// unmerged tokens
8 return El
k,v,‚à•Xl,l‚àí1
k,v‚à•,Rl,l‚àí1
k,Rl,l‚àí1
v,Ik,v
9procedure MiniCache Decoding :
Input: KV cache: El
k,v‚ààR2√ótprompt√ód, Norm: ‚à•Xl
k,v‚à• ‚ààR2√ótprompt√ó1, Unmerged Tokens:
Rl
k,v‚ààR2√óŒ≥¬∑tprompt√ód, Retention indices: Ik,v‚ààR2√óŒ≥¬∑tprompt√ó1, Next Token:
tl‚ààR1√ód,tl‚àí1‚ààR1√ód
10 ÀÜXl
k‚ÜêEl
k¬∑‚à•Xl
k‚à•
‚à•El
k‚à•ÀÜXl
v‚ÜêEl
v¬∑‚à•Xl
v‚à•
‚à•Elv‚à•// magnitude rescale
11 ÀÜXl
k[Ik] =Rl
kÀÜXl
v[Iv] =Rl
v// token restoration
12 ÀÜXl
k‚ÜêConcat (ÀÜXl
k,tk,dim=token )ÀÜXl
v‚ÜêConcat (ÀÜXl
v,tv,dim=token )
A‚ÜêSoftmax (tq¬∑(ÀÜXl
k)‚ä§)tO‚ÜêA¬∑ÀÜXl
viftl‚àí1exists then
13 KV cache ‚ÜêConcat (El
k,v,Merge (tl
k,v,tl‚àí1
k,v),dim=token )// perform compression
14 else
15 KV cache ‚ÜêConcat (El
k,v,tl
k,v,dim=token )// wait for compression
16 return tO
17function MiniCache Merge (Xl,Xl‚àí1, t):
18 ‚ÉóXl‚ÜêXl
‚à•Xl‚à•
19 ‚ÉóXl‚àí1‚ÜêXl‚àí1
‚à•Xl‚àí1‚à•
20 ‚Ñ¶‚Üêarccos
Xl
T¬∑Xl‚àí1
T
‚à•Xl
T‚à•‚à•Xl‚àí1
T‚à•
21 E‚Üêsin((1‚àít)‚Ñ¶)
sin(‚Ñ¶)‚ÉóXl+sin(t‚Ñ¶)
sin(‚Ñ¶)‚ÉóXl‚àí1
22 return E,‚à•Xl‚à•,‚à•Xl‚àí1‚à•,‚Ñ¶
E Detailed Efficiency Derivation
In this section, we provide a detailed derivation of the memory efficiency improvements outlined in
Section 4.3.
First, we consider the original KV cache memory usage, which is given by:
4brh(s+n).
Here, ris the number of layers, bis the batch size, his the hidden size, sis the input sequence length,
andnis the output sequence length. To improve efficiency, we begin merging layers starting from
the midpoint, S=1
2r, by consolidating the KV cache states of every two layers into a single shared
state space. The memory usage derivation proceeds as follows: for the unmerged part of the cache
(from layer 1 to S):
19

--- PAGE 20 ---
4brh(s+n)¬∑1
2= 2brh(s+n).
For the merged part of the cache (from layer S+ 1tor):
4brh(s+n)¬∑1
2¬∑1
2=brh(s+n).
Combining these two parts, the total memory usage is:
2brh(s+n) +brh(s+n) = 3 brh(s+n).
Next, we consider the additional memory cost incurred during the restoration process. During this
phase, we save additional normalized vectors for the layers in the KV cache. These vectors are of
shape Rb√ós√ó1, which means they have a single channel dimension compared to the fully ranked
original KV states.
The additional normalized vectors for layers from Sonwards are given by:
br(s+n)¬∑2 = 2 br(s+n).
We also introduce a retention threshold, which we set to 0.05. This means that 5% of the KV cache
tokens are retained without compression:
brh(0.05(s+n)).
Combining these terms, the total additional memory for the restoration process is:
2br(s+n) + 0.1brh(s+n).
Finally, summing the compressed memory usage and the restoration memory cost, the overall memory
requirement is:
3brh(s+n) + 2br(s+n) + 0.1brh(s+n).
This can be simplified by grouping the common factors:
br(s+n) (3h+ 2 + 0 .1h).
Simplifying the expression inside the parentheses, we get:
br(s+n) (3.1h+ 2).
Therefore, the total memory cost for the KV cache in the MiniCache Framework is:
br(s+n)(3.1h+ 2).
This detailed derivation confirms the efficiency improvements discussed in Section 4.3, highlighting
the significant reduction in memory usage achieved through our layer merging and restoration
strategies.
F Detailed Experiment Results
20

--- PAGE 21 ---
Table B: Detailed performance comparison on GSM8K dataset with LLaMA-3-70B.
Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean
0 0.799 0.729
5 0.800 0.640
10 0.792 0.578
15 0.815 0.545
20 0.801 0.560
25 0.812 0.544
30 0.799 0.556
35 0.810 0.557
40 0.790 0.551
45 0.725 0.539
50 0.638 0.541
55 0.638 0.501
60 0.625 0.497
65 0.635 0.511
70 0.623 0.497
75 0.615 0.493
Table C: Detailed performance comparison on COQA dataset with LLaMA-3-70B.
Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean
0 0.705 0.706
4 0.705 0.699
8 0.706 0.696
12 0.704 0.691
16 0.704 0.690
20 0.703 0.690
24 0.701 0.690
28 0.702 0.690
32 0.702 0.688
36 0.703 0.688
40 0.697 0.687
44 0.698 0.685
48 0.699 0.678
52 0.699 0.672
56 0.701 0.668
60 0.704 0.657
64 0.706 0.635
68 0.691 0.611
72 0.689 0.565
76 0.641 0.526
21

--- PAGE 22 ---
Table D: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-70B.
Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean
0 22.615 22.130
4 22.512 22.005
8 22.451 21.876
12 22.413 21.303
16 22.387 21.209
20 22.387 20.752
24 22.387 20.657
28 22.276 20.501
32 22.130 20.479
36 22.130 20.335
40 22.073 19.834
44 21.356 17.024
48 21.356 12.440
52 21.333 9.127
56 21.316 3.255
60 21.172 2.349
64 21.153 2.250
68 21.002 1.721
72 20.940 1.119
76 20.683 0.784
Table E: Detailed performance comparison on GSM8K dataset with LLaMA-3-8B.
Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean
0 0.488 0.467
2 0.476 0.369
4 0.489 0.388
6 0.487 0.387
8 0.489 0.359
10 0.479 0.388
12 0.486 0.384
14 0.472 0.368
16 0.477 0.343
18 0.446 0.291
20 0.447 0.271
22 0.433 0.234
24 0.399 0.155
26 0.396 0.140
28 0.395 0.052
30 0.391 0.024
32 0.397 0.025
22

--- PAGE 23 ---
Table F: Detailed performance comparison on COQA dataset with LLaMA-3-8B.
Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean
0 0.676 0.676
2 0.676 0.571
4 0.675 0.566
6 0.674 0.564
8 0.674 0.561
10 0.673 0.560
12 0.672 0.560
14 0.670 0.559
16 0.670 0.558
18 0.669 0.555
20 0.669 0.552
22 0.668 0.549
24 0.667 0.543
26 0.667 0.537
28 0.666 0.536
30 0.666 0.531
32 0.665 0.528
Table G: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-8B.
Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean
0 32.520 32.524
2 32.231 28.431
4 31.645 28.197
6 31.485 27.894
8 31.008 27.796
10 30.964 27.704
12 30.798 27.371
14 30.798 27.093
16 30.798 26.643
18 30.798 26.517
20 30.798 26.355
22 30.798 26.011
24 30.798 25.044
26 30.798 15.254
28 30.798 14.791
30 30.765 9.419
32 30.390 6.068
23

--- PAGE 24 ---
Table H: Detailed performance comparison on GSM8K dataset with Mixtral-8x7B.
Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean
0 0.589 0.575
2 0.592 0.480
4 0.593 0.491
6 0.591 0.469
8 0.580 0.472
10 0.592 0.492
12 0.582 0.485
14 0.572 0.480
16 0.562 0.462
18 0.522 0.432
20 0.526 0.426
22 0.540 0.416
24 0.519 0.398
26 0.515 0.436
28 0.502 0.401
30 0.515 0.386
32 0.490 0.258
Table I: Detailed performance comparison on COQA dataset with Mixtral-8x7B.
Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean
0 0.672 0.675
2 0.671 0.612
4 0.670 0.601
6 0.672 0.590
8 0.674 0.582
10 0.671 0.571
12 0.674 0.561
14 0.670 0.546
16 0.672 0.544
18 0.672 0.530
20 0.675 0.522
22 0.671 0.512
24 0.660 0.455
26 0.657 0.447
28 0.640 0.440
30 0.634 0.424
32 0.459 0.430
24

--- PAGE 25 ---
Table J: Detailed performance comparison on TruthfulQA dataset with Mixtral-8x7B.
Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean
0 21.686 19.465
2 21.385 19.405
4 21.368 19.251
6 21.038 19.094
8 21.038 18.265
10 20.216 17.019
12 20.026 15.902
14 19.723 15.505
16 19.641 15.028
18 19.641 14.723
20 19.546 14.543
22 18.756 14.122
24 18.402 13.834
26 18.366 13.789
28 17.738 12.091
30 16.827 12.008
32 16.635 0.430
Table K: Detailed performance comparison on GSM8K dataset with Phi-3-Mini.
Layer Phi-3-Mini MiniCache Phi-3-Mini Mean
0 0.774 0.774
2 0.765 0.667
4 0.757 0.661
6 0.754 0.659
8 0.748 0.657
10 0.750 0.645
12 0.750 0.616
14 0.752 0.575
16 0.739 0.491
18 0.742 0.417
20 0.692 0.272
22 0.685 0.206
24 0.640 0.110
26 0.545 0.061
28 0.500 0.039
30 0.460 0.036
32 0.447 0.028
25

--- PAGE 26 ---
Table L: Detailed performance comparison on COQA dataset with Phi-3-Mini.
Layer Phi-3-Mini MiniCache Phi-3-Mini Mean
0 0.665 0.665
2 0.662 0.562
4 0.657 0.557
6 0.656 0.556
8 0.656 0.556
10 0.654 0.554
12 0.646 0.546
14 0.648 0.538
16 0.647 0.537
18 0.637 0.527
20 0.627 0.487
22 0.591 0.461
24 0.567 0.437
26 0.548 0.408
28 0.527 0.407
30 0.506 0.406
32 0.503 0.403
Table M: Detailed performance comparison on TruthfulQA dataset with Phi-3-Mini.
Layer Phi-3-Mini MiniCache Phi-3-Mini Mean
0 19.686 19.465
2 19.385 19.365
4 19.368 19.221
6 19.100 18.255
8 19.038 17.019
10 19.500 15.912
12 19.216 15.525
14 20.026 15.195
16 19.641 15.058
18 18.756 14.763
20 17.738 14.593
22 19.546 14.182
24 19.723 13.954
26 18.366 13.919
28 18.402 12.231
30 16.827 12.158
32 16.635 10.333
26
