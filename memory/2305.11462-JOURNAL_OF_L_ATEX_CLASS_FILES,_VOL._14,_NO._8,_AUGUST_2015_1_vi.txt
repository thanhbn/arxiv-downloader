# 2305.11462.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/memory/2305.11462.pdf
# Kích thước file: 1668444 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 1
Mở rộng Bộ nhớ cho Mô hình hóa Ngôn ngữ
Anupiya Nugaliyadde
Tóm tắt —Những đột phá trong học sâu và mạng bộ nhớ đã tạo ra những tiến bộ lớn trong hiểu biết ngôn ngữ tự nhiên. Ngôn ngữ là tuần tự và thông tin được mang qua chuỗi có thể được nắm bắt thông qua mạng bộ nhớ. Học chuỗi là một trong những khía cạnh chính trong việc học ngôn ngữ. Tuy nhiên, mạng bộ nhớ không có khả năng giữ các chuỗi dài vô hạn trong bộ nhớ của chúng và bị giới hạn bởi các ràng buộc khác nhau như vấn đề gradient biến mất hoặc bùng nổ. Do đó, các mô hình hiểu biết ngôn ngữ tự nhiên bị ảnh hưởng khi được trình bày với văn bản tuần tự dài. Chúng tôi giới thiệu mạng Bộ nhớ Dài hạn (LTM) để học từ các chuỗi dài vô hạn. LTM ưu tiên cho các đầu vào hiện tại để cho phép nó có tác động cao. Mô hình hóa ngôn ngữ là một yếu tố quan trọng trong hiểu biết ngôn ngữ tự nhiên. LTM được thử nghiệm trong mô hình hóa ngôn ngữ, yêu cầu bộ nhớ dài hạn. LTM được thử nghiệm trên bộ dữ liệu Penn Tree bank, bộ dữ liệu Google Billion Word và bộ dữ liệu WikiText-2. Chúng tôi so sánh LTM với các mô hình ngôn ngữ khác yêu cầu bộ nhớ dài hạn.
Từ khóa chỉ mục —Mô hình hóa Ngôn ngữ, Bộ nhớ Dài hạn, Dữ liệu tuần tự

I. GIỚI THIỆU
NGÔN ngữ tự nhiên chứa các mẫu tuần tự, kết nối thông tin quá khứ với bối cảnh hiện tại và tương lai [1] [2]. Tương tự như con người, các mô hình học máy sử dụng bối cảnh tuần tự quá khứ để hiểu ngôn ngữ [3]. Một mô hình học máy thường nắm bắt chuỗi quá khứ trong một bối cảnh để hiểu ngôn ngữ [4]. Giữ bối cảnh tuần tự dài trong bộ nhớ và liên kết thông tin trong một mô hình học máy là quan trọng để hiểu bối cảnh. Mô hình hóa ngôn ngữ học mẫu tuần tự trong ngôn ngữ tự nhiên để hiểu và học ngôn ngữ. Kiến thức tuần tự
Bộ nhớ tuần tự dài cho phép mô hình học máy liên kết và trích xuất thông tin để hiểu bối cảnh. Các mô hình học sâu có khả năng giữ các chuỗi dài và xác định mối quan hệ và mẫu trong một chuỗi [5]. Tuy nhiên, các mô hình học sâu không có khả năng giữ các chuỗi dài vô hạn [6]. Do đó, các mô hình học sâu đã không thành công trong hiểu biết ngôn ngữ như trong xử lý hình ảnh [7] [8]. Một mô hình học sâu có khả năng nắm bắt các chuỗi dài hơn có tiềm năng cải thiện hiểu biết ngôn ngữ. Nắm bắt chuỗi quá khứ và dự đoán chuỗi tiếp theo được sử dụng để đánh giá khả năng của mô hình học máy trong việc hiểu ngôn ngữ tự nhiên sử dụng chuỗi quá khứ [9].

Mạng bộ nhớ đã cho thấy vượt trội so với các mô hình học sâu khác cho dữ liệu tuần tự [10] [11] [6]. Mạng Nơ-ron Hồi quy (RNN) giữ khái niệm chính của việc học từ dữ liệu tuần tự. RNN kết hợp các đầu vào quá khứ với các đầu vào hiện tại để tạo ra đầu ra. Phương trình 1 chứng minh rằng đầu vào hiện tại xt được kết hợp với đầu ra của bước trước yt-1 để tạo ra đầu ra hiện tại yt. Do đó, yt không chỉ phụ thuộc vào xt mà còn phụ thuộc vào yt-1. Wt biểu diễn các trọng số liên quan trong RNN cho một khung thời gian t nhất định. Hàm RNN được biểu diễn bởi f. Điều này có thể được coi là một phương pháp cơ bản để giữ bộ nhớ. Tuy nhiên, việc liên tục đưa xt-1 vào RNN có thể dẫn đến vấn đề gradient bùng nổ hoặc biến mất. Vấn đề này chủ yếu do sự chồng chéo của các trọng số RNN gây ra RNN thất bại [12].

yt=f(y(t-1);xt;Wt) (1)

Mạng Bộ nhớ Ngắn hạn Dài (LSTM) giới thiệu một cấu trúc cổng để tránh vấn đề gradient bùng nổ và biến mất [12]. Cấu trúc cổng này đảm bảo rằng các trọng số của LSTM sẽ không bị quá tải. Điều này được kiểm soát bởi cơ chế của cổng quên trong LSTM (2). Phương trình 2 chứng minh chức năng của cổng quên. Wf là trọng số được gán cho cổng quên và điều chỉnh để quên chuỗi quá khứ. ht-1 là trạng thái tế bào được truyền từ đầu ra trước yt-1. xt là đầu vào hiện tại. ht-1 và xt được kết hợp để tạo đầu vào cho cổng quên. bf là độ lệch được thêm vào cổng quên để đưa ra độ lệch cho LSTM. Dựa trên các tham số này, cổng quên quyết định quên chuỗi quá khứ hoặc mang tiếp chuỗi quá khứ.

Cổng quên quyết định khi nào quên chuỗi quá khứ. Cổng quên quyết định điều này dựa trên đầu vào hiện tại. Cổng quên sẽ quyết định nếu đầu vào hiện tại yêu cầu các đầu ra quá khứ, nếu không thì các đầu ra quá khứ sẽ được học để bị quên.

ft=σ(Wf·[ht-1;xt] +bf) (2)

Cổng quên của LSTM, loại bỏ thông tin tuần tự không mong muốn khỏi bộ nhớ. Điều này giúp gradient của LSTM tránh biến mất hoặc bùng nổ và đã hiệu quả trong việc học dữ liệu tuần tự dài. Hiệu suất của LSTM cho một chuỗi 100 bước hoặc hơn làm đầu vào là không tối ưu vì cổng quên loại bỏ toàn bộ chuỗi quá khứ [13]. Cổng quên học để loại bỏ các chuỗi quá khứ khi chuỗi trở nên không liên quan hơn [13]. Tuy nhiên, trong bối cảnh ngôn ngữ có thể mang các phụ thuộc dài, có thể kéo dài trong suốt một chuỗi rất dài. Do đó, trong mô hình hóa ngôn ngữ chuỗi dài, hiệu suất của LSTM có thể không tối ưu. Các biến thể của LSTM được giới thiệu tuy nhiên, phương pháp tránh gradient biến mất hoặc bùng nổ không được thay đổi [14] [15] [16]. Các biến thể của LSTM tập trung hơn vào việc xử lý các đầu vào khác nhau và tạo ra các đầu ra khác nhau [13]. Những biến thể này đã không cải thiện hiệu suất không tối ưu của bộ nhớ dài hạn trong LSTM [17] [18]. Mạng Nơ-ron Hồi quy Có cổng (GRU) và Mạng nơ-ron hồi quy đơn giản (SRN) [13] sử dụng các cổng để xử lý vấn đề gradient biến mất hoặc bùng nổ

--- TRANG 2 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 2
vấn đề. Những cổng này được sử dụng để quên các chuỗi quá khứ để xử lý vấn đề gradient bùng nổ hoặc biến mất [19]. Các cổng trong những mạng bộ nhớ này ngăn chặn vấn đề gradient biến mất hoặc bùng nổ nhưng hy sinh việc học các phụ thuộc dài hạn trong một chuỗi. Các chuỗi dài trong ngôn ngữ yêu cầu bộ nhớ dài hạn tuần tự.

Học từ các chuỗi dài là quan trọng cho mô hình hóa ngôn ngữ. Tuy nhiên, việc nhớ các chuỗi quá khứ có nhiều thách thức; hoặc bị ảnh hưởng bởi vấn đề gradient biến mất hoặc bùng nổ hoặc quên các chuỗi. Mạng Bộ nhớ Dài hạn (LTM) được đề xuất có khả năng học từ các chuỗi ngắn và dài mà không quên các phần của chuỗi hoặc bị ảnh hưởng bởi vấn đề gradient bùng nổ hoặc biến mất. LTM lấy xt và kết hợp với y(t-1). Nó không quên chuỗi và tổng quát hóa xt và y(t-1). LTM được đánh giá trên mô hình hóa ngôn ngữ để chứng minh khả năng bộ nhớ dài hạn. LTM cũng cho thấy rằng nó có khả năng học từ ngắn hạn. LTM đã cho thấy hiệu suất tốt hơn so với các mô hình bộ nhớ dài hạn khác cho mô hình hóa ngôn ngữ.

Các mục tiêu chính của bài báo này là:
1) Giới thiệu và chứng minh khả năng học dài hạn và ngắn hạn của LTM cho mô hình hóa ngôn ngữ cho cấp độ ký tự và cấp độ câu.
2) Chứng minh rằng LTM có khả năng xử lý chuỗi dài mà không quên các chuỗi quá khứ và bị ảnh hưởng bởi vấn đề gradient biến mất hoặc bùng nổ.

II. CÔNG TRÌNH LIÊN QUAN
Dữ liệu tuần tự mang kiến thức qua chuỗi và các chuỗi trước đó ảnh hưởng đến các chuỗi tương lai. Do đó, học từ chuỗi là một yếu tố quan trọng. Mạng bộ nhớ thường được sử dụng cho các tác vụ học tuần tự [6]. RNN là một mô hình ban đầu sử dụng dữ liệu tuần tự để học [20]. Mặc dù RNN gặp phải vấn đề gradient biến mất hoặc bùng nổ, khái niệm chính của RNN được sử dụng trong tất cả các mạng bộ nhớ (3). Đầu vào (xt) kết hợp đầu ra quá khứ (yt-1) và tạo ra đầu ra (yt). Các trọng số (Wi) được điều chỉnh bằng hàm kích hoạt. Phương trình (3) cho thấy rằng yt phụ thuộc vào yt-1. Khái niệm này được sử dụng trong tất cả các mạng bộ nhớ khác.

yt=activation (Wi[yt-1;xt]) (3)

A. Vấn đề Gradient Biến mất hoặc Bùng nổ
Việc nhân liên tục Wi trong Phương trình (3), có thể gây ra gradient lan truyền ngược phát triển hoặc phân rã theo cấp số nhân. Các giá trị riêng lớn hơn 1 trong W gây ra gradient bùng nổ. Mặt khác, nếu giá trị riêng nhỏ hơn 1, nó có thể dẫn đến gradient biến mất [21]. Một hàm kích hoạt bão hòa có giá trị riêng nhỏ hơn 1 hoặc bằng một có thể làm xấu đi trong lan truyền ngược.

B. Tránh Vấn đề Gradient Biến mất hoặc Bùng nổ
Cắt gradient giới hạn gradient bùng nổ nhưng gradient biến mất khó ngăn chặn [22]. Vấn đề gradient biến mất hoặc bùng nổ được tránh trong LSTM bằng cách giới thiệu một cấu trúc cổng [12]. Các cổng kiểm soát luồng dữ liệu trong một mô hình mạng nơ-ron [23] và quên chuỗi quá khứ khi nó không liên quan. Tuy nhiên, việc quên một chuỗi có thể ảnh hưởng tiêu cực đến dự đoán của mô hình vì mạng có xu hướng quên các chuỗi cũ mà mô hình xác định là không liên quan [24]. Mô hình hóa ngôn ngữ có thể được sử dụng để đánh giá mạng bộ nhớ [25]. Các mô hình ngôn ngữ tốt hơn dựa vào bộ nhớ dài hạn vì kiến thức của một ngôn ngữ được mang qua các chuỗi dài. Bộ nhớ dài hạn mang kiến thức qua chuỗi và có khả năng trích xuất kiến thức tốt hơn [26].

RNN được sử dụng trong nhiều hình thức khác nhau cho mô hình hóa ngôn ngữ [20], [27]. Những phương pháp này sử dụng RNN bằng cách cải thiện bộ nhớ của RNN. Tuy nhiên, các RNN được sửa đổi đã cho thấy gặp phải vấn đề gradient biến mất và bùng nổ với các chuỗi dài. Để cải thiện mô hình hóa ngôn ngữ, LSTM được giới thiệu vì nó đã cho thấy xử lý gradient biến mất và bùng nổ [25]. Cổng quên của LSTM được sử dụng để ngăn LSTM có gradient bùng nổ hoặc biến mất, bằng cách loại bỏ các chuỗi quá khứ không liên quan khỏi bộ nhớ của LSTM. Cổng quên cũng ngăn LSTM khỏi gradient biến mất và bùng nổ bằng cách kiểm soát bộ nhớ nội bộ và loại bỏ các chuỗi dài và không liên quan. LSTM sử dụng cổng quên để ngăn W→0 và ngăn gradient biến mất (4). Khi ∂ET/∂W đạt 0, cổng quên sẽ quên W từ chuỗi quá khứ.

∂E/∂W=∑(t=1 to k+1) ∂ET/∂W (4)

Trạng thái tế bào của LSTM được tạo bằng cách sử dụng các hàm cộng, để ngăn nó đạt đến gradient biến mất. Do đó, trạng thái tế bào của LSTM có thể đạt giá trị cao hơn. Các chuỗi ngôn ngữ có thể dài và thông tin trong một chuỗi có thể được mang trong suốt các đoạn văn và chương. Do đó, một mô hình ngôn ngữ nên có khả năng giữ các chuỗi dài. Nhiều sửa đổi khác nhau được áp dụng cho LSTM để hỗ trợ bộ nhớ dài hạn cho mô hình hóa ngôn ngữ [25] [28] [29]. Mặc dù những thay đổi kiến trúc này cho thấy triển vọng, chúng không thể tránh được tác động của cổng quên đối với các chuỗi dài. Các biến thể của LSTM đã thay đổi các cấu trúc cổng và kết nối trong các tế bào LSTM. Tuy nhiên, những biến thể này sử dụng bao gồm cổng quên [30]. Do đó, một phương pháp để xử lý bộ nhớ dài hạn, không bị ảnh hưởng bởi vấn đề gradient biến mất hoặc bùng nổ là cần thiết.

Các mô hình mới được giới thiệu để xử lý bộ nhớ dài hạn và bộ nhớ ngắn hạn. AntisymmetricRNN kết nối RNN và các phương trình vi phân [31]. AntisymmetricRNN sử dụng tính chất ổn định của phương trình vi phân để nắm bắt các phụ thuộc dài hạn. AntisymmetricRNN đã được chứng minh vượt trội so với LSTM trong các tác vụ bộ nhớ dài hạn và phù hợp với hiệu suất trong các tác vụ bộ nhớ ngắn hạn. AntisymmetricRNN có kiến trúc đơn giản hơn và nhỏ hơn so với LSTM. Tuy nhiên, AntisymmetricRNN được thử nghiệm trên dữ liệu chuỗi hình ảnh. h-detach là một thuật toán ngẫu nhiên được chỉ định để tối ưu hóa LSTM để cải thiện các tác vụ bộ nhớ dài hạn [32]. h-detach ngăn gradient chảy qua các trạng thái tế bào. Do đó, trạng thái tế bào sẽ không ức chế

--- TRANG 3 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 3
các trọng số và LSTM sẽ nắm bắt các phụ thuộc dài (bộ nhớ dài hạn). Điều này cũng đã được thử nghiệm với các bộ dữ liệu liên quan đến hình ảnh và bộ dữ liệu chú thích hình ảnh để kiểm tra các phụ thuộc dài hạn. Carta et al., đề xuất một mạng Bộ nhớ Tuyến tính có một thành phần ghi nhớ dựa trên bộ mã hóa được xây dựng với một bộ tự mã hóa tuyến tính cho các chuỗi [33]. Mạng dựa trên bộ mã hóa được phát triển để tăng cường bộ nhớ ngắn hạn. Mạng được thử nghiệm trên bộ dữ liệu Nhiễu trắng và bộ dữ liệu hình ảnh tuần tự.

Các mô hình mới được giới thiệu để xử lý bộ nhớ dài hạn trong mô hình hóa ngôn ngữ. Đơn vị Hồi quy Không bão hòa (NRU) đã tránh bão hòa hàm kích hoạt và các cổng bão hòa [21]. Hơn nữa, NRU sử dụng một đơn vị tuyến tính chỉnh lưu (ReLU) để hỗ trợ bộ nhớ dài hạn, với kiến trúc mới. NRU tránh vấn đề gradient biến mất hoặc bùng nổ với bộ nhớ dài hạn. NRU cũng đã cho thấy có hiệu suất tương tự như các mô hình bộ nhớ có cổng cho các tác vụ bộ nhớ ngắn hạn. Tuy nhiên, một phương pháp cổng đơn giản đã cho thấy kết quả hứa hẹn so với NRU [34]. Mạng Nơ-ron Hồi quy Không bình thường (nnRNN) sử dụng phân tích Schur cho cấu trúc kết nối và tránh tính toán phân tích Schur và chia dạng Schur thành bình thường và không bình thường [35]. nnRNN sử dụng các ma trận kết nối hồi quy trực giao với các thuật ngữ không bình thường tăng tính linh hoạt của một mạng hồi quy. nnRNN đã cho thấy hoạt động tốt cho các tác vụ bộ nhớ dài hạn và tăng khả năng biểu đạt trên các tác vụ yêu cầu tính toán trực tuyến cho động lực học thoáng qua [35]. nnRNN đã chứng minh rằng kết nối với các cổng đã cho thấy hiệu suất cao hơn trong việc học và có lợi thế so với LSTM và GRU. Tuy nhiên, những mô hình này đã không cho thấy học các mẫu tuần tự dài được mang trong suốt một chuỗi dài nhất định. Học từ các chuỗi dài là một phần quan trọng trong hiểu biết ngôn ngữ tự nhiên vì kiến thức và mối quan hệ của chúng được mang trong suốt các câu dài, đoạn văn, chương và sách. Do đó, điều quan trọng là tập trung vào một phương pháp học máy có khả năng học từ các chuỗi dài.

III. MẠNG BỘ NHỚ DÀI HẠN
LTM được thiết kế để học từ các chuỗi dài (hơn 300 dấu thời gian) với số lượng đơn vị tối thiểu được kết hợp mà không quên chuỗi. Tế bào LTM có khả năng học và tổng quát hóa từ các đầu ra quá khứ. Hơn nữa, LTM ưu tiên cao cho đầu vào hiện tại (xt). Tế bào LTM được chia thành ba phần chính. Các phần là:
1) Trạng thái đầu vào: điều này xử lý đầu vào được truyền vào mạng
2) Trạng thái tế bào: mang dữ liệu đã xử lý trong quá khứ và kết hợp chúng với đầu vào hiện tại đã xử lý để mang tiếp
3) Trạng thái đầu ra: tạo ra đầu ra cuối cùng bằng cách kết hợp đầu vào hiện tại đã xử lý với đầu ra của trạng thái tế bào.

Các cổng được sử dụng để ưu tiên cho đầu vào hiện tại và ngăn gradient bùng nổ hoặc biến mất. Điều này khác với lý do chính của LSTM sử dụng các cổng để ngăn gradient bùng nổ hoặc biến mất [23]. Các cổng đóng vai trò chính trong cấu trúc của LTM.

Hình 1. Một tế bào Mạng Bộ nhớ Dài hạn. Luồng dữ liệu được hiển thị bằng các mũi tên.

A. Trạng thái đầu vào
Trạng thái đầu vào xử lý xt được truyền đến LTM. Ban đầu, LTM kết hợp đầu ra quá khứ (ht-1) với xt như được hiển thị trong (4). σ biểu thị các hàm sigmoid và W1 là trọng số.
Lt-1=σ(W1(ht-1+xt)) (5)

W2 là một trọng số khác được gán cho Lt2. Lt1 và Lt2 bị ảnh hưởng bởi xt. Do đó, Lt1 và Lt2 là các sản phẩm trực tiếp của xt.
Lt2=σ(W2(ht-1+xt)) (6)

Phương trình (7) kết hợp Lt1 và Lt2 để tạo ra Lt. Kết quả phương trình (7) L't được sử dụng để ảnh hưởng đến trạng thái tế bào với sự nhấn mạnh vào xt. L't là tích vô hướng của Lt1 và Lt2.
L't=Lt1·Lt2 (7)

L't khuếch đại tác động của xt và đầu ra quá khứ ht-1. L't được thêm vào trạng thái tế bào để được mang tiếp cho bước t+1.

B. Trạng thái tế bào
Trạng thái tế bào chịu trách nhiệm mang tiếp từ bước t-1 đến bước t. Thông tin mang tiếp này là cần thiết để sử dụng xt-1 để hỗ trợ dự đoán ht. Trạng thái tế bào mang các đầu vào quá khứ để giữ thông tin tuần tự từ tn đến t. L't được thêm vào trạng thái tế bào để truyền đầu vào hiện tại đến bước tiếp theo t+1. Phương trình (7) cho thấy sự kết hợp của Ct-1 với L't.
C't=L't+Ct-1 (8)

C't sẽ bị ảnh hưởng bởi xt và được truyền đến bước t+1. xt ảnh hưởng đến C't với trọng số cao hơn do (7). Tuy nhiên, nếu trạng thái tế bào bị quá tải với dữ liệu tuần tự mà không có kiểm soát, LTM có thể bị ảnh hưởng bởi gradient bùng nổ hoặc biến mất [36]. Khái niệm của LTM dựa trên việc giữ các chuỗi dài. Do đó, như được hiển thị trong (8), trạng thái tế bào được chia tỷ lệ. Điều này ngăn LTM đạt đến gradient bùng nổ hoặc biến mất bằng cách tổng quát hóa đầu ra sử dụng hàm Sigmoid. Ct sẽ mang tiếp trạng thái tế bào và thông tin tuần tự quá khứ đến bước tiếp theo.
Ct=σ(W4·C't) (9)

--- TRANG 4 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 4
C. Trạng thái đầu ra
Trạng thái đầu ra tạo ra đầu ra cuối cùng (ht) của LTM. Như được hiển thị trong (9) và (10), ht bị ảnh hưởng trực tiếp bởi xt. Phương trình (9) là một yếu tố chính quyết định đầu ra cuối cùng như được hiển thị trong (10).
Lt3=σ(W'3(ht-1+input)) (10)

Phương trình (10), tạo ra đầu ra cuối cùng (ht). Điều này được tạo ra bằng cách kết hợp cả Ct và Lt3.
ht=Ct·Lt3 (11)

Ct và ht được mang đến lớp tiếp theo của LTM. Do đó, Ct và ht được mang tiếp mà ảnh hưởng đến xt+1 và tạo ra ht+1.

D. Tổng quát hóa và Tránh Gradient Biến mất hoặc Bùng nổ trong LTM
LTM được trình bày trong [6] được mở rộng thêm để chứng minh khả năng tổng quát hóa giá trị trạng thái tế bào nội bộ của nó. Thuật ngữ "tổng quát hóa" trong bài báo này được sử dụng để thảo luận về các giá trị nội bộ trong tế bào LTM trái ngược với thuật ngữ tổng quát hóa bình thường được sử dụng trong học sâu/học máy. Ct của LTM yêu cầu khả năng tổng quát hóa kiến thức quá khứ mà không quên chuỗi quá khứ (8) và ngăn gradient bùng nổ hoặc biến mất trong trạng thái tế bào. Các kỹ thuật lấy trung bình thường được sử dụng để tổng quát hóa đầu ra. Trong bài báo này, LTM sử dụng hàm sigmoid để tổng quát hóa C't trong trạng thái tế bào được mang đến t+1 như Ct. LTM sử dụng C't được tạo ra bằng cách sử dụng Ct-1 và kết quả trung gian (L't) của xt để tổng quát hóa các chuỗi quá khứ. Hàm sigmoid (11) đặt đầu vào của hàm giữa 0 và 1. Đầu ra của trạng thái tế bào (Ct) là đầu ra của hàm sigmoid, sử dụng C't và L't. Do đó, trạng thái tế bào được kiểm soát nội bộ và ngăn trạng thái tế bào mở rộng theo cấp số nhân.

f(x) = 1/(1+e^-x) (12)

Hàm sigmoid trên trạng thái tế bào có thể được mở rộng như được hiển thị trong (12). Ct mang tiếp thông tin tuần tự quá khứ đã được thu thập và kết hợp với thông tin đầu vào hiện tại và duy trì nó giữa 0 và 1. Do đó, Ct được phân phối mà tổng quát hóa đầu ra của trạng thái tế bào. Điều này cũng hỗ trợ các tế bào LTM khỏi vấn đề gradient biến mất và bùng nổ vì trạng thái nội bộ được kiểm soát trong một khu vực được kiểm soát.

Ct = 1/(W4(L't+Ct-1)) (13)

Hầu hết các mạng bộ nhớ sử dụng cấu trúc cổng hoặc cắt gradient để ngăn vấn đề gradient bùng nổ và biến mất, có thể ảnh hưởng tiêu cực đến kết quả dài hạn trong một mạng bộ nhớ [18]. LTM quản lý để giữ các chuỗi quá khứ mà không ảnh hưởng bất lợi đến bộ nhớ dài hạn của LTM. Gradient biến mất hoặc bùng nổ xảy ra trong các mô hình lặp (13) trong đó f là hàm lặp, x là đầu vào và ht là một kích hoạt. Khi f là lặp, tác động có thể tăng theo cấp số nhân. Do đó, nếu đầu ra của f trở thành 0 hoặc >1, gradient biến mất hoặc bùng nổ có thể xảy ra.

ht=f(f(f(h1;x1);x2);x3) (14)

Để ngăn gradient biến mất/bùng nổ trong LTM, lỗi lan truyền ngược (Et) cho thời gian (t) và các trọng số (W) như được hiển thị trong (14) không nên là 0 hoặc >1.

∂E/∂W = ∑(t=1 to T) ∂ET/∂W (15)

Do đó, LTM nên ngăn ∂ET/∂W đạt 0 hoặc >1. Việc sử dụng hàm sigmoid trong việc tạo Ct, ngăn Ct ngăn Ct khỏi 0. Ngay cả khi T rất lớn, hàm sigmoid sẽ giữ Ct mà không đạt 0 hoặc trên 1. Tác động đệ quy của (13) sẽ được tránh thông qua hàm sigmoid. Ct sẽ được giữ trong phạm vi nhất định (0 và 1), và ngăn các giá trị được truyền đến t+1, đạt 0 hoặc đi trên 1. Hơn nữa, tính chất cộng trong trạng thái tế bào ngăn trạng thái tế bào khỏi 0 (7). C't điều này sẽ ngăn Ct khỏi 0. Việc cộng tăng giá trị trạng thái tế bào và thông qua hàm sigmoid, trạng thái tế bào được giữ giữa việc tăng hoặc giảm theo cấp số nhân ngăn gradient biến mất hoặc bùng nổ.

IV. SO SÁNH KIẾN TRÚC MÔ HÌNH VỚI CÁC MÔ HÌNH NGÔN NGỮ KHÁC
LTM có thiết kế kiến trúc khác để hỗ trợ bộ nhớ dài hạn. Cấu trúc và các kết nối trong LTM khác với các mô hình học sâu mô hình hóa ngôn ngữ khác. Do đó, để phân biệt thiết kế kiến trúc LTM, nó được so sánh với RNN, LSTM, GRU, NRU và Transformers cơ bản.

A. RNN
RNN có kiến trúc sử dụng các đầu ra quá khứ để tạo ra đầu ra hiện tại (1). Tuy nhiên, trong RNN không có xử lý bổ sung. RNN liên tục truyền đầu ra trước với đầu ra hiện tại. Tuy nhiên, vấn đề gradient biến mất và bùng nổ ảnh hưởng đến RNN.

B. LSTM Vanilla
LSTM là một trong những mạng bộ nhớ được sử dụng phổ biến nhất. LSTM giới thiệu một cấu trúc cổng để tránh vấn đề gradient bùng nổ hoặc biến mất [12]. Cấu trúc các cổng được sử dụng để kiểm soát luồng dữ liệu trong tế bào LSTM. Hình 2. mô tả cấu trúc LSTM. Cổng quên (2) đưa ra quyết định quên hoặc nhớ dữ liệu quá khứ. Tuy nhiên, LSTM không mang bộ nhớ dài hạn vì nó quên dữ liệu quá khứ, tùy thuộc vào đầu vào hiện tại.

1) So sánh Giữa LSTM và LTM: Mạng Bộ nhớ Ngắn hạn Dài (LSTM) (Hình 2.) và Mạng Bộ nhớ Dài hạn (LTM) (Hình 1.) có điểm tương đồng và khác biệt. Bảng I so sánh LTM và LSTM.

--- TRANG 5 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 5
Các thành phần của Mạng Bộ nhớ | LTM | LSTM
Cổng quên | Không có cổng quên. Do đó, không loại bỏ bất kỳ chuỗi quá khứ nào. | Có cổng quên để loại bỏ các chuỗi quá khứ không liên quan đến đầu vào hiện tại. Cổng quên ngăn LSTM khỏi quá tải bộ nhớ.
Cổng đầu vào và đầu ra | Cổng đầu vào và đầu ra xử lý các đầu vào và đầu ra cho LTM. | Cổng đầu vào và đầu ra xử lý các đầu vào và đầu ra cho LSTM.
Trạng thái tế bào | Trạng thái tế bào mang chuỗi quá khứ tiếp tục để được thêm vào đầu vào tiếp theo. Tuy nhiên, đầu vào hiện tại được ưu tiên cao hơn khi trước khi chuyển đến trạng thái tế bào. | Trạng thái tế bào mang chuỗi quá khứ tiếp tục để được thêm vào đầu vào tiếp theo.
Hàm kích hoạt | Chỉ sử dụng hàm kích hoạt sigmoid. Hàm sigmoid chia tỷ lệ chuỗi. | Có sự kết hợp của hàm sigmoid và tanh.

BẢNG I
SO SÁNH GIỮA CÁC THÀNH PHẦN CỦA LTM VÀ LSTM.

Hình 2. Một tế bào Mạng Bộ nhớ Ngắn hạn Dài. Luồng dữ liệu được hiển thị bằng các mũi tên.

C. Các biến thể của LSTM
LSTM được điều chỉnh để phù hợp với các tác vụ khác nhau [37]. Tuy nhiên, những điều chỉnh này không thay đổi cấu trúc cốt lõi của LSTM như được hiển thị trong Hình 3, đây là một sửa đổi có kết nối lỗ nhìn đến các cổng. Điều này cho phép mạng học sự phân biệt tinh tế giữa các đỉnh. Điều này trực tiếp hỗ trợ bộ nhớ dài hạn. Đóng góp chính của mạng này là sử dụng bộ đếm thời gian cho phép mạng đếm số bước được thực hiện và dự đoán tại bước thời gian cần thiết. Tuy nhiên, mạng chỉ nhớ bước thứ 50 và không phải dữ liệu ở giữa. Nó không giữ tất cả thông tin. Do đó, các phụ thuộc dài hạn yêu cầu toàn bộ chuỗi có xác suất cao thất bại.

D. GRU
GRU có kiến trúc đơn giản, có bộ cổng đơn giản (Hình 4.). GRU được giới thiệu để ngăn vấn đề gradient biến mất và bùng nổ. GRU có cổng quên và cổng đặt lại quyết định nếu đầu vào sẽ được chuyển đến đầu ra.

Hình 3. Một biến thể của tế bào Mạng Bộ nhớ Ngắn hạn Dài. Luồng dữ liệu được hiển thị bằng các mũi tên.

Hình 4. Một Đơn vị Hồi quy Có cổng. Luồng dữ liệu được hiển thị trong mũi tên.

E. Transformers
Cốt lõi của transformers dựa trên attention và sử dụng kiến trúc encoder và decoder [38]. Transformer bao gồm một chồng các encoder nhận đầu vào và một chồng các decoder tạo ra đầu ra. Các mô hình ngôn ngữ dựa trên transformer là các chồng decoder trong transformers.

--- TRANG 6 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 6
Generative Pre-trained Transformer (GPT-3) là một trong những transformer đầu tiên và phổ biến sử dụng 175 tỷ tham số cho mô hình hóa ngôn ngữ [39]. GPT-3 là một tập hợp các decoder bao gồm các lớp self-attention và một mạng nơ-ron feed-forward cho đầu ra cuối cùng. Do đó, các transformer phụ thuộc nhiều vào attention trong suốt chuỗi của bối cảnh. Tuy nhiên, các transformer một chiều không có khả năng nắm bắt cả bối cảnh trái và phải trong tất cả các lớp. Điều này ảnh hưởng bất lợi đến khả năng của transformer. Bidirectional Encoder Representations from Transformers (BERT), là một transformer hai chiều được sử dụng cho các tác vụ hiểu biết ngôn ngữ tự nhiên [40]. BERT đào tạo và đưa ra một cảm giác sâu sắc về bối cảnh ngôn ngữ và luồng thông qua bản chất hai chiều của nó so với các transformer một chiều khác. Tuy nhiên, BERT sử dụng cơ chế attention học các mối quan hệ ngữ cảnh giữa các từ trong một bối cảnh và tạo ra Masked Language Modelling cho phép đào tạo hai chiều. Khả năng này cho phép BERT hiểu bối cảnh sử dụng cả bối cảnh trái và phải cho một từ nhất định và tạo ra một mô hình ngôn ngữ tốt hơn. BERT là một mạng nơ-ron lớn với 24 khối transformer, 1024 lớp ẩn, 16 đầu self-attention, và sử dụng 340 Triệu tham số. Do đó, BERT (và hầu hết các transformer cho mô hình hóa ngôn ngữ) sử dụng GPU để truyền các đầu vào song song với nhau tăng tốc độ đào tạo của các transformer. Do đó, các transformer phụ thuộc nhiều vào việc sử dụng GPU và bộ nhớ lớn. Đào tạo BERT hoặc bất kỳ mô hình ngôn ngữ lớn dựa trên transformer nào là thách thức. Mặc dù một mô hình BERT được đào tạo trước có sẵn cho mô hình hóa ngôn ngữ chung, việc đào tạo lại và đào tạo các mô hình ngôn ngữ lớn dựa trên transformer cho các mô hình ngôn ngữ cụ thể không thể thực hiện được trên thực tế.

V. THỰC NGHIỆM VÀ KẾT QUẢ
LTM được thử nghiệm trên các tác vụ mô hình hóa ngôn ngữ yêu cầu bộ nhớ dài hạn. LTM được thử nghiệm trên bộ dữ liệu PennTree Bank (PTB), Google Billion Words, và WikiText-2 cho mô hình hóa ngôn ngữ theo từ. Nó cũng được thử nghiệm trên mô hình hóa ngôn ngữ cấp độ ký tự cho bộ dữ liệu PennTree bank. LTM được so sánh với một số mạng bộ nhớ phổ biến bao gồm nhiều mô hình RNN khác nhau.

A. Bộ dữ liệu
1) Bộ dữ liệu PennTree Bank: PTB chứa một loạt văn bản bao gồm văn bản từ Wall Street Journal, ghi chú điều dưỡng, sách hướng dẫn máy tính IBM, cuộc trò chuyện điện thoại được ghi âm và v.v. PTB có từ vựng 10K. Bộ dữ liệu này là một trong những bộ dữ liệu phổ biến nhất cho mô hình hóa ngôn ngữ. PTB bao gồm 930K token cho đào tạo, 74K token cho xác thực và 82K token cho thử nghiệm [41].

2) Bộ dữ liệu Google Billion Words: Bộ dữ liệu này bao gồm 0.8 Tỷ từ cho đào tạo và thử nghiệm [42]. Dữ liệu được lấy từ trang web WMT11. Các câu trùng lặp được loại bỏ, do đó các câu duy nhất có sẵn trong bộ dữ liệu. Từ vựng là 793471.

3) Bộ dữ liệu WikiText-2: WikiText-2 giữ toàn bộ văn bản (không có bất kỳ lọc nào được áp dụng), với dấu câu và số [43]. Bộ dữ liệu được tạo thành từ các bài báo hoàn chỉnh. Do đó, nó phù hợp cho các phụ thuộc dài hạn.

B. Mô hình hóa Ngôn ngữ Cấp độ Ký tự
Dự đoán (các) ký tự tiếp theo trong một chuỗi từ của từ là mô hình hóa ngôn ngữ cấp độ ký tự. Học chuỗi chữ cái của một từ được học thông qua bộ nhớ ngắn hạn. Do đó, các mô hình ngôn ngữ cấp độ ký tự sử dụng bộ nhớ ngắn hạn. Bảng II chứng minh thực nghiệm rằng LTM có khả năng đạt được bộ nhớ ngắn hạn. Penn Treebank Corpus (PTB) được sử dụng cho mô hình hóa ngôn ngữ cấp độ ký tự. LTM lấy ký tự đầu tiên trong một chuỗi và dự đoán ký tự tiếp theo. Để so sánh công bằng LTM với các mô hình mạng bộ nhớ khác, thiết lập sau được sử dụng. Kích thước batch được đặt là 128. Các mô hình được đào tạo trong 20 epoch. Mô hình hóa ngôn ngữ cấp độ ký tự được thử nghiệm trên bộ dữ liệu thử nghiệm và đánh giá với Bits Per Character (BPC) và độ chính xác. Một mô hình BPC thấp hơn tốt hơn cho việc dự đoán ký tự tiếp theo. Mặc dù mô hình hóa ngôn ngữ cấp độ ký tự không yêu cầu bộ nhớ dài hạn, LTM được thử nghiệm để so sánh nó với các tác vụ bộ nhớ chung. Bảng II cho thấy kết quả thử nghiệm cho mô hình hóa ngôn ngữ cấp độ ký tự cho PTB. LTM đã không cho thấy cải thiện cao hơn về kết quả trên mô hình hóa ngôn ngữ cấp độ ký tự nhưng đã đạt được kết quả tương tự với các mạng bộ nhớ tiêu chuẩn như GRU và LSTM. Bảng II cho thấy rằng LTM tạo ra kết quả tương đương với các mô hình mạng bộ nhớ phổ biến.

Mô hình | BPC | Độ chính xác
LSTM | 1.48 | 67.98
RNN | 1.55 | 68.43
GRU | 1.45 | 69.07
JANET | 1.48 | 68.5
NRU [21] | 1.47 | 68.48
nnRNN [35] | 1.49 | -
LTM | 1.44 | 68.01

BẢNG II
THỬ NGHIỆM BPC VÀ ĐỘ CHÍNH XÁC CHO MÔ HÌNH HÓA NGÔN NGỮ CẤP ĐỘ KÝ TỰ CHO PTB. KẾT QUẢ IN ĐẬM LÀ KẾT QUẢ CỦA LTM.

C. Mô hình hóa Ngôn ngữ
Mô hình hóa ngôn ngữ yêu cầu bộ nhớ dài hạn, đặc biệt cho các chuỗi dài hơn 50 từ. LTM được thử nghiệm trên PTB, Google Billion Words và WikiText-2. Các mô hình được đánh giá bằng perplexity. PTB được sử dụng làm bộ dữ liệu chuẩn. Nhiều phương pháp được thử nghiệm trên PTB. Hơn nữa, nhiều phương pháp tối ưu hóa khác nhau được thử nghiệm trên LSTM cho PTB và WikiText-2 [28]. Để so sánh công bằng LTM với các phương pháp mạng bộ nhớ khác, các phương pháp đánh giá và thử nghiệm tương tự được tuân theo như [28]. Tất cả LTM có 3 lớp LTM và 1150 đơn vị ẩn. Embedding có kích thước 400. Kích thước batch cho PTB là 40 và WikiText-2 là 80. Kết quả so sánh với LTM trên PTB được hiển thị trong Bảng III. Hơn nữa, LTM được thử nghiệm trên WikiText-2 về mô hình hóa ngôn ngữ (Bảng IV). Để đánh giá việc học tuần tự dài với các bộ dữ liệu lớn, LTM được thử nghiệm trên bộ thử nghiệm Google Billion Word (Bảng V).

D. Transformers
Transformers đã vượt trội so với hầu hết các tác vụ Xử lý Ngôn ngữ Tự nhiên. Tuy nhiên, mô hình hóa ngôn ngữ yêu cầu

--- TRANG 7 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 7

Mô hình | Xác thực | Thử nghiệm
RNN-LDA + KN-5 + cache [44] | - | 92.0
LSTM (large) [45] | 82.2 | 78.4
Variational LSTM (large, MC) [46] | - | 73.4
CharCNN [47] | - | 78.9
Variational LSTM (tied) + augmented loss [48] | 71.1 | 68.5
Variational RHN (tied) [49] | 67.9 | 65.4
NAS Cell (tied) [50] | - | 62.4
4-layer skip connection LSTM (tied) [51] | 60.9 | 58.3
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer [28] | 53.9 | 52.8
LSTM+ Dual Channel Class Hierarchy [52] | - | 118.3
LSTM(Large) + cell [53] | 76.15 | 73.87
AWD-FWM [54] | 56.76 | 54.48
LTM | 52.1 | 51.7

BẢNG III
THỬ NGHIỆM PERPLEXITY CHO MÔ HÌNH HÓA NGÔN NGỮ TRÊN PTB. KẾT QUẢ TỐT NHẤT TRÊN MỖI MÔ HÌNH ĐƯỢC BÁO CÁO VÀ KẾT QUẢ LTM ĐƯỢC LẤY TRUNG BÌNH TỪ 10 LẦN CHẠY KHÁC NHAU.

Mô hình | Xác thực | Thử nghiệm
Variational LSTM (tied) + augmented loss [48] | 91.5 | 87.0
LSTM + continuous cache pointer [55] | - | 68.9
NAS Cell (tied) [50] | - | 62.4
2-layer skip connection LSTM (tied) [51] | 68.6 | 65.9
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer [28] | 53.8 | 52.0
LSTM(Large) + cell [53] | 90.52 | 85.76
AWD-FWM [54] | 63.98 | 61.65
LTM | 51.5 | 50.1

BẢNG IV
THỬ NGHIỆM PERPLEXITY CHO MÔ HÌNH HÓA NGÔN NGỮ TRÊN WIKITEXT-2. KẾT QUẢ TỐT NHẤT TRÊN MỖI MÔ HÌNH ĐƯỢC BÁO CÁO VÀ KẾT QUẢ LTM ĐƯỢC LẤY TRUNG BÌNH TỪ 10 LẦN CHẠY KHÁC NHAU.

Mô hình | Perplexity Thử nghiệm
Sigmoid-RNN-2048 [56] | 68.3
Interpolated KN 5-Gram [42] | 67.6
Sparse Non-Negative Matrix LM [57] | 52.9
LSTM-2048-512 [58] | 43.7
LSTM-2048 [59] | 43.9
2-layer LSTM-2048 [59] | 39.8
GCNN-13 [60] | 38.1
GCNN-14 Bottleneck [60] | 31.9
BIG LSTM+CNN inputs [58] | 30.0
BIG GLSTM-G4 [61] | 23.3
LTM | 21.5

BẢNG V
KẾT QUẢ TRÊN PERPLEXITY THỬ NGHIỆM GOOGLE BILLION WORD. KẾT QUẢ TỐT NHẤT TRÊN MỖI MÔ HÌNH ĐƯỢC BÁO CÁO VÀ KẾT QUẢ LTM ĐƯỢC LẤY TRUNG BÌNH TỪ 10 LẦN CHẠY KHÁC NHAU.

bộ nhớ và thông tin tuần tự. Transformers có hiệu quả tính toán so với các mạng bộ nhớ. Tuy nhiên, các transformer không tối ưu cho mô hình hóa ngôn ngữ [62] [63]. Self-attention và mã hóa vị trí trong transformers đã không kết hợp bối cảnh tuần tự cấp độ từ. LTM được đánh giá so với các transformer trong Bảng VI cho thấy rằng LTM vượt trội so với BERT và GPT-3 trong mô hình hóa ngôn ngữ.

E. Kết hợp Transformers với Mạng Bộ nhớ
Transformers hoạt động kém trong Mô hình hóa ngôn ngữ. Do đó, các mạng bộ nhớ được thêm vào Transformers để nắm bắt kiến thức tuần tự tốt hơn [62]. Thêm các lớp LSTM vào transformers nắm bắt bối cảnh tuần tự và

Bộ dữ liệu
Mô hình | PTB | WT-2 | WT-103
Val | Test | Val | Test | Val | Test
GPT-3 | 79.44 | 68.79 | 89.96 | 80.6 | 63.07 | 63.47
BERT | 72.99 | 62.4 | 79.76 | 69.32 | 109.54 | 107.3
LTM | 52.1 | 51.7 | 51.5 | 50.1 | 49.3 | 47.1

BẢNG VI
SO SÁNH PERPLEXITY TRÊN BERT VÀ GPT SO VỚI LTM CHO MÔ HÌNH HÓA NGÔN NGỮ.

Bộ dữ liệu
Mô hình | PTB | WT-2 | WT-103
Val | Test | Val | Test | Val | Test
BERT-CAS | 39.97 | 34.47 | 38.43 | 34.64 | 40.70 | 39.85
GPT-CAS | 46.24 | 40.87 | 50.41 | 46.62 | 35.75 | 34.24
BERT-Large-CAS | 36.14 | 31.34 | 37.79 | 34.11 | 19.67 | 20.42
BERT-LTM | 32.2 | 30.11 | 34.8 | 30.61 | 16.31 | 14.2
GPT-LTM | 41.32 | 37.17 | 43.1 | 39.66 | 34.2 | 33.2

BẢNG VII
KẾT HỢP TRANSFORMERS VỚI LTM CHO MÔ HÌNH HÓA NGÔN NGỮ SO VỚI CÁC MÔ HÌNH NGÔN NGỮ DỰA TRÊN TRANSFORMER.

hoạt động hiệu quả [62]. Do đó, BERT hoặc GPT-3 kết hợp với các lớp LSTM hoạt động tốt trong mô hình hóa ngôn ngữ. [62] sử dụng LSTM để nắm bắt thông tin tuần tự và thêm Coordinate Architecture Search để tìm một kiến trúc hiệu quả thông qua tinh chỉnh lặp đi lặp lại của mô hình. LTM được kết hợp với Transformers (BERT và GPT). Phương pháp này được áp dụng để so sánh LTM với các mô hình transformer mới. Tuy nhiên, trọng tâm chính của bài báo là về LTM mà không có bất kỳ sự kết hợp mô hình nào khác. Bảng VII so sánh LTM kết hợp với transformers với Coordinate Architecture Search (CAS) với LSTM và transformers.

VI. THẢO LUẬN
Thông tin là tuần tự và nhiều thông tin hơn được thu thập với các chuỗi dài hơn. Thông tin tuần tự càng dài mà mô hình có khả năng giữ thì các dự đoán dựa trên chuỗi càng tốt. Tuy nhiên, trong các chuỗi dài, các chuỗi trước đó ngay lập tức nên có ưu tiên cao hơn so với chuỗi cũ hơn, vì mối quan hệ mà chuỗi trước đó có liên quan hơn đến đầu vào hiện tại so với chuỗi cũ hơn. Các chuỗi ngôn ngữ tự nhiên bị ảnh hưởng bởi các chuỗi quá khứ nhưng chuỗi trước đó có liên quan hơn đến đầu vào hiện tại. Do đó, LTM tập trung vào việc ưu tiên cao cho chuỗi trước đó để dự đoán đầu vào hiện tại so với các chuỗi cũ hơn. Các chuỗi cũ hơn có tham chiếu tối thiểu đến đầu vào hiện tại trong khi các chuỗi trước đó có ưu tiên cao hơn đối với đầu vào hiện tại. Yếu tố này được sử dụng trong RNN và LSTM để dự đoán các chuỗi ngắn hạn trong mô hình hóa ngôn ngữ. Khả năng mô hình hóa ngôn ngữ của LSTM tốt hơn vì các dự đoán dựa trên chuỗi trước đó. LTM có khả năng học từ các chuỗi dài hơn 250 từ và đã đạt được điểm perplexity trong Bảng I, III, IV và V. LTM đã đạt được perplexity tốt hơn so với các mô hình khác vì LTM có khả năng giữ các chuỗi dài trong bộ nhớ của nó.

LTM được thử nghiệm trên nhiều bộ dữ liệu mô hình hóa ngôn ngữ. Mô hình hóa ngôn ngữ yêu cầu bộ nhớ để học phụ thuộc dài hạn để dự đoán từ tiếp theo trong một chuỗi [61]. Học

--- TRANG 8 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 8

Hình 5. - - cho thấy Perplexity đào tạo, — cho thấy Perplexity thử nghiệm. Perplexity đào tạo và thử nghiệm thay đổi theo số epoch.

phụ thuộc là cần thiết để dự đoán các từ tiếp theo. LTM học chuỗi của một văn bản nhất định. Không giống như các mô hình bộ nhớ khác, LTM được cấu trúc để học các chuỗi dài, có thể vượt quá 250 từ sử dụng ít hơn 10 đơn vị LTM. Trong việc học một chuỗi, LTM không bị ảnh hưởng bởi vấn đề gradient biến mất hoặc bùng nổ. LTM có trạng thái tế bào mang ra thông tin tuần tự quá khứ và mang tiếp với trạng thái ẩn [64]. Việc sử dụng trạng thái tế bào thường được sử dụng trong các mạng bộ nhớ để mang tiếp thông tin tuần tự quá khứ [23]. Hơn nữa, LTM không quên các chuỗi quá khứ tại bất kỳ điểm nào trong chuỗi, tương tự như các mạng phổ biến khác như LSTM và GRU. LTM sử dụng cấu trúc cổng để tổng quát hóa chuỗi và ưu tiên cao hơn cho đầu vào hiện tại và không để kiểm soát và quên chuỗi. LTM tiếp tục học thông qua chuỗi dài. Như được hiển thị trong Hình 1, Sigmoid 1 và Sigmoid 2 được sử dụng để ưu tiên cao cho đầu vào hiện tại. Hơn nữa, Sigmoid 4 tổng quát hóa và kết hợp trạng thái tế bào để mang tiếp các đầu ra quá khứ. Do đó, như được hiển thị trong Hình 5, LTM có thể được thiết lập để tiếp tục chạy cho nhiều lần lặp sau khi hội tụ và perplexity thử nghiệm không thay đổi. LTM được đào tạo cho một phạm vi từ 100 epoch đến 5000 epoch và được thử nghiệm để cho thấy rằng LTM đã bão hòa trong việc học và không thay đổi với việc tăng epoch. Điều này cho thấy rằng LTM không bị ảnh hưởng bởi gradient biến mất hoặc bùng nổ ngay cả sau khi đào tạo đã bão hòa.

A. Phân tích tác động cổng của LTM
Sự căn chỉnh cổng của LTM và việc lựa chọn các hàm sigmoid được sử dụng để hỗ trợ việc học dài hạn và ưu tiên cho đầu vào hiện tại. Mỗi cổng trong LTM được đặt để hỗ trợ bộ nhớ dài hạn. Điều này được chứng minh bằng cách loại bỏ mỗi cổng (đặt cổng để tạo ra 1 và truyền bất kỳ đầu vào nào) và thử nghiệm khả năng mô hình hóa ngôn ngữ của LTM. Bộ dữ liệu PTB được sử dụng vì nó là bộ dữ liệu phổ biến nhất cho mô hình hóa ngôn ngữ [21]. 10 đơn vị LTM được sử dụng để tạo ra kết quả cao nhất với số lượng ít nhất các đơn vị LTM. Các đơn vị LTM được kết hợp để chứng minh tác động của mỗi cổng đối với bộ nhớ dài hạn trong mô hình hóa ngôn ngữ. Bảng VIII, cho thấy tác động của việc loại bỏ mỗi cổng đối với perplexity của việc dự đoán một chuỗi dài 100. Rõ ràng là Sigmoid 4 có tác động cao nhất đến perplexity. Sigmoid 4 ảnh hưởng trực tiếp đến trạng thái tế bào và đầu ra. Mặc dù Sigmoid 1 có tác động thấp nhất đến perplexity, so với LTM, tác động của Sigmoid 1 là to lớn. Bảng VIII rõ ràng cho thấy rằng sự kết hợp của tất cả các cổng là cần thiết để LTM tạo ra việc học dài hạn. Hơn nữa, các cổng được kết hợp với nhau để cho thấy tác động của tập hợp các cổng để chứng minh cổng nào ảnh hưởng đến bộ nhớ dài hạn và ưu tiên cho đầu vào hiện tại.

Cổng Mở | Thử nghiệm
Sigmoid 1 | 90.3
Sigmoid 2 | 92.2
Sigmoid 3 | 99.1
Sigmoid 4 | 101.1
Sigmoid 4 + Sigmoid 3 | 111.5
Sigmoid 1 + Sigmoid 2 | 120.8
Sigmoid 1 + Sigmoid 3 | 87.3
Sigmoid 1 + Sigmoid 4 | 81.5
Sigmoid 2 + Sigmoid 3 | 78.2
Sigmoid 2 + Sigmoid 4 | 88.7
Sigmoid 4 + Sigmoid 3 + Sigmoid 2 | 173.9
Sigmoid 4 + Sigmoid 3 + Sigmoid 1 | 176.2
LTM với tất cả cổng | 51.1

BẢNG VIII
THỬ NGHIỆM PERPLEXITY CHO MÔ HÌNH HÓA NGÔN NGỮ TRÊN PTB CHO VIỆC MỞ MỖI CỔNG. KẾT QUẢ TỐT NHẤT TRÊN MỖI MÔ HÌNH ĐƯỢC BÁO CÁO VÀ KẾT QUẢ LTM ĐƯỢC LẤY TRUNG BÌNH TỪ 10 LẦN CHẠY KHÁC NHAU.

Cổng Sigmoid 1 và Sigmoid 2 ảnh hưởng đến việc ưu tiên cho đầu vào hiện tại. Bảng IX trình bày BPC trên PTB khi cả cổng Sigmoid 1 và cổng Sigmoid 2 được mở (cổng được đặt thành 1) và đóng trong khi cổng Sigmoid 3 và cổng Sigmoid 4 được mở. Các cổng mở không có tác động đến đầu ra. Để học các phụ thuộc ngắn hạn, mô hình hóa ngôn ngữ cấp độ ký tự được phân tích. Bảng 6 cho thấy rằng BPC rất cao so với kết quả của LTM, và so sánh với kết quả Bảng II, cổng Sigmoid 1 và Sigmoid 2 đã mang lại lợi thế so với các mạng bộ nhớ khác. Bảng IX là một dấu hiệu rõ ràng rằng cổng Sigmoid 1 và Sigmoid 2 chịu trách nhiệm học các phụ thuộc ngắn hạn.

Tác động đối với bộ nhớ dài hạn được thử nghiệm bằng cách sử dụng cổng Sigmoid 3 và Sigmoid 4. Cổng Sigmoid 3 và Sigmoid 4 được mở và LTM được thử nghiệm trên perplexity của bộ dữ liệu PTB tương tự như thực nghiệm trong Bảng VIII. Như được hiển thị trong Bảng VIII Sigmoid 4 có tác động cao đến bộ nhớ dài hạn. Do đó, việc kết hợp cổng Sigmoid 3 và Sigmoid 4 tạo ra tác động rất cao đến bộ nhớ dài hạn như được hiển thị trong Bảng 7. Tuy nhiên, tác động đối với đầu vào hiện tại cao đối với các dự đoán, nó đã ảnh hưởng đến perplexity như được hiển thị trong Bảng X.

B. Gradient biến mất và bùng nổ với LTM
LTM có khả năng xử lý gradient biến mất và bùng nổ. Trong quá trình đào tạo, các trọng số của LTM không đạt đến vô cực hoặc 0. Trọng số của LTM không bị ảnh hưởng bởi gradient biến mất hoặc bùng nổ như được hiển thị trong

--- TRANG 9 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 9

Mô hình | BPC
LTM không có cổng Sigmoid 1 và Sigmoid 2 | 1.71
LTM không có cổng Sigmoid 3 và Sigmoid 4 | 1.49
LTM | 1.44

BẢNG IX
THỬ NGHIỆM BPC TRÊN LTM ĐỂ PHÂN TÍCH TÁC ĐỘNG CỦA CỔNG Sigmoid 1 VÀ Sigmoid 2 ĐỐI VỚI VIỆC HỌC CÁC PHỤ THUỘC NGẮN HẠN. KẾT QUẢ ĐƯỢC LẤY TRUNG BÌNH TỪ 10 LẦN CHẠY KHÁC NHAU.

Cổng | Thử nghiệm
LTM không có cổng Sigmoid 3 và Sigmoid 4 | 110.3
LTM không có cổng Sigmoid 1 và Sigmoid 2 | 77.2
LTM với tất cả cổng | 51.1

BẢNG X
THỬ NGHIỆM PERPLEXITY CHO MÔ HÌNH HÓA NGÔN NGỮ TRÊN PTB CHO VIỆC MỞ CỔNG Sigmoid 3 VÀ Sigmoid 4 ĐỂ PHÂN TÍCH PHỤ THUỘC DÀI HẠN. KẾT QUẢ ĐƯỢC LẤY TRUNG BÌNH TỪ 10 LẦN CHẠY KHÁC NHAU.

Hình 5. Mặc dù LTM nhận dữ liệu liên tục, perplexity không tăng hoặc thay đổi. Nếu gradient biến mất hoặc bùng nổ xảy ra, perplexity của LTM sẽ cho thấy sự tăng đột ngột vì dự đoán của mô hình sẽ bị ảnh hưởng. LTM đã được thử nghiệm ngay cả sau khi đạt được hiệu suất đỉnh và được đào tạo và cho phép mô hình đào tạo quá mức, tuy nhiên, ngay cả LTM được đào tạo quá mức cũng không cho thấy rằng nó bị ảnh hưởng bởi vấn đề gradient bùng nổ và biến mất.

C. Hiệu suất của LTM
LTM là một mô hình sử dụng CPU và Bộ nhớ thấp. Bảng I chứng minh thực nghiệm rằng LTM có thể hoạt động tốt với thông số kỹ thuật thấp với ít sức mạnh tính toán hơn. Mô hình được thử nghiệm trên CPU với 8 GB RAM trên bộ dữ liệu PTB.

Độ dài Chuỗi | Thời gian Đào tạo (phút) | Thời gian Thử nghiệm (giây)
50 từ | 14:32 | <0.001
100 từ | 14:50 | <0.001
200 từ | 15:04 | <0.001
250 từ | 15:17 | <0.001
300 từ | 15:29 | <0.001
350 từ | 15:33 | <0.001
400 từ | 15:40 | <0.001
450 từ | 15:48 | <0.001
500 từ | 15:50 | <0.001
600 từ | 15:59 | <0.001
1000 từ | 16:10 | <0.001

BẢNG XI
SO SÁNH ĐỘ DÀI CHUỖI CỦA LTM TRÊN BỘ DỮ LIỆU PTB TRÊN MÁY TÍNH CPU 8 GB RAM.

Bảng XI cho thấy rằng LTM học nhanh mà không sử dụng GPU và thời gian học không tăng theo cấp số nhân với việc tăng trưởng chuỗi. Hơn nữa, LTM không yêu cầu GPU để đào tạo. Tuy nhiên, LTM có thể sử dụng GPU để giảm thời gian đào tạo. LTM được đào tạo trên card đồ họa NVIDIA 1080i và thời gian đào tạo được giảm trung bình 7 phút như được hiển thị trong Bảng XII. Hơn nữa, LTM đã được thử nghiệm trên máy tính 4 GB RAM và thời gian đào tạo tăng trung bình 4 phút. Do đó, LTM không bị ảnh hưởng bởi tài nguyên tính toán hạn chế.

LTM có thể chạy trên tài nguyên hạn chế, chỉ với việc sử dụng CPU với RAM thấp 4 GB và không bị ảnh hưởng ngoại trừ việc giảm thời gian đào tạo. Do đó, việc sử dụng điện năng

Độ dài Chuỗi | Thời gian Đào tạo (phút) | Thời gian Thử nghiệm (giây)
50 từ | 7:28 | <0.001
100 từ | 7:40 | <0.001
200 từ | 7:57 | <0.001
250 từ | 8:03 | <0.001
300 từ | 8:18 | <0.001
350 từ | 8:30 | <0.001
400 từ | 8:39 | <0.001
450 từ | 8:44 | <0.001
500 từ | 8:49 | <0.001
600 từ | 8:55 | <0.001
1000 từ | 9:03 | <0.001

BẢNG XII
SO SÁNH ĐỘ DÀI CHUỖI CỦA LTM TRÊN BỘ DỮ LIỆU PTB TRÊN MÁY TÍNH GPU 1080I

của LTM thấp hơn so với hầu hết các mô hình lớn tiên tiến.

VII. KẾT LUẬN
Một Mạng Bộ nhớ Dài hạn, để xử lý chuỗi ngôn ngữ tự nhiên dài mà không bị ảnh hưởng bởi vấn đề gradient biến mất hoặc bùng nổ được giới thiệu. LTM giới thiệu một kiến trúc tế bào khác. Các cổng của LTM được sử dụng để mang các chuỗi quá khứ thay vì quên các chuỗi quá khứ. Điều này cho phép LTM mang tiếp các chuỗi dài. LTM được thử nghiệm trên các tác vụ mô hình hóa ngôn ngữ và nó đã vượt trội so với các mạng bộ nhớ phổ biến trong phụ thuộc dài hạn và đã cho thấy kết quả tương đương trong các phụ thuộc ngắn hạn. LTM được thử nghiệm trên bộ dữ liệu Penn Tree bank, bộ dữ liệu Google Billion Words và WikiText-2. Hơn nữa, LTM đã cho thấy rằng nó có thể hội tụ nhanh và không bị đào tạo quá mức. Tác động của mỗi cổng được phân tích về việc học các phụ thuộc ngắn hạn và các phụ thuộc dài hạn.

TÀI LIỆU THAM KHẢO
[1] A. Mnih, Z. Yuecheng, and G. Hinton, "Improving a statistical language model through non-linear prediction," Neurocomputing, vol. 72, no. 7-9, pp. 1414–1418, 2009.
[2] X. Chen, H. Xu, Y. Zhang, J. Tang, Y. Cao, Z. Qin, and H. Zha, "Sequential recommendation with user memory networks," in Proceedings of the eleventh ACM international conference on web search and data mining, pp. 108–116, 2018.
[3] Y. Shi, K. Yao, H. Chen, Y.-C. Pan, M.-Y. Hwang, and B. Peng, "Contextual spoken language understanding using recurrent neural networks," in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5271–5275, IEEE, 2015.
[4] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng, "Improving word representations via global context and multiple word prototypes," in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 873–882, 2012.
[5] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," nature, vol. 521, no. 7553, pp. 436–444, 2015.
[6] A. Nugaliyadde, F. Sohel, K. W. Wong, and H. Xie, "Language modeling through long-term memory network," in 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1–6, IEEE, 2019.
[7] T. Young, D. Hazarika, S. Poria, and E. Cambria, "Recent trends in deep learning based natural language processing," ieee Computational intelligenCe magazine, vol. 13, no. 3, pp. 55–75, 2018.
[8] J. Worsham and J. Kalita, "Multi-task learning for natural language processing in the 2020s: Where are we going?," Pattern Recognition Letters, vol. 136, pp. 120–126, 2020.
[9] T. G. Dietterich, "Machine learning for sequential data: A review," in Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR), pp. 15–30, Springer, 2002.

--- TRANG 10 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 10
[10] W. Luo and F. Yu, "Learning longer-term dependencies via grouped distributor unit," Neurocomputing, vol. 412, pp. 406–415, 2020.
[11] M. Khademi, "Multimodal neural graph memory networks for visual question answering," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7177–7188, 2020.
[12] F. A. Gers, J. Schmidhuber, and F. Cummins, "Learning to forget: Continual prediction with lstm," 1999.
[13] T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato, "Learning longer memory in recurrent neural networks," arXiv preprint arXiv:1412.7753, 2014.
[14] R. Quan, L. Zhu, Y. Wu, and Y. Yang, "Holistic lstm for pedestrian trajectory prediction," IEEE transactions on image processing, vol. 30, pp. 3229–3239, 2021.
[15] S. Santhanam, "Context based text-generation using lstm networks," arXiv preprint arXiv:2005.00048, 2020.
[16] B. Krause, L. Lu, I. Murray, and S. Renals, "Multiplicative lstm for sequence modelling," arXiv preprint arXiv:1609.07959, 2016.
[17] J. Zhao, F. Huang, J. Lv, Y. Duan, Z. Qin, G. Li, and G. Tian, "Do rnn and lstm have long memory?," in International Conference on Machine Learning, pp. 11365–11375, PMLR, 2020.
[18] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, "Empirical evaluation of gated recurrent neural networks on sequence modeling," arXiv preprint arXiv:1412.3555, 2014.
[19] S. Glüge, R. Böck, G. Palm, and A. Wendemuth, "Learning long-term dependencies in segmented-memory recurrent neural networks with backpropagation of error," Neurocomputing, vol. 141, pp. 54–64, 2014.
[20] T. Mikolov, M. Karafiát, L. Burget, J. Černocký, and S. Khudanpur, "Recurrent neural network based language model," in Eleventh annual conference of the international speech communication association, 2010.
[21] S. Chandar, C. Sankar, E. Vorontsov, S. E. Kahou, and Y. Bengio, "Towards non-saturating recurrent units for modelling long-term dependencies," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 3280–3287, 2019.
[22] Y. Bengio, P. Simard, and P. Frasconi, "Learning long-term dependencies with gradient descent is difficult," IEEE transactions on neural networks, vol. 5, no. 2, pp. 157–166, 1994.
[23] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[24] E. Tsironi, P. Barros, C. Weber, and S. Wermter, "An analysis of convolutional long short-term memory recurrent neural networks for gesture recognition," Neurocomputing, vol. 268, pp. 76–86, 2017.
[25] M. Sundermeyer, R. Schlüter, and H. Ney, "Lstm neural networks for language modeling," in Thirteenth annual conference of the international speech communication association, 2012.
[26] D. S. McNamara, E. Kintsch, N. B. Songer, and W. Kintsch, "Are good texts always better? interactions of text coherence, background knowledge, and levels of understanding in learning from text," Cognition and instruction, vol. 14, no. 1, pp. 1–43, 1996.
[27] T. Mikolov, S. Kombrink, L. Burget, J. Černocký, and S. Khudanpur, "Extensions of recurrent neural network language model," in 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5528–5531, IEEE, 2011.
[28] S. Merity, N. S. Keskar, and R. Socher, "Regularizing and optimizing lstm language models," in International Conference on Learning Representations, 2018.
[29] G. Kurata, B. Ramabhadran, G. Saon, and A. Sethy, "Language modeling with highway lstm," in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 244–251, IEEE, 2017.
[30] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber, "Lstm: A search space odyssey," IEEE transactions on neural networks and learning systems, vol. 28, no. 10, pp. 2222–2232, 2016.
[31] B. Chang, M. Chen, E. Haber, and E. H. Chi, "Antisymmetricrnn: A dynamical system view on recurrent neural networks," arXiv preprint arXiv:1902.09689, 2019.
[32] D. Arpit, B. Kanuparthi, G. Kerg, N. R. Ke, I. Mitliagkas, and Y. Bengio, "h-detach: Modifying the lstm gradient towards better optimization," arXiv preprint arXiv:1810.03023, 2018.
[33] A. Carta, A. Sperduti, and D. Bacciu, "Encoding-based memory for recurrent neural networks," Neurocomputing, vol. 456, pp. 407–420, 2021.
[34] Z. Cheng, Y. Xu, M. Cheng, Y. Qiao, S. Pu, Y. Niu, and F. Wu, "Refined gate: A simple and effective gating mechanism for recurrent units," arXiv preprint arXiv:2002.11338, 2020.
[35] G. Kerg, K. Goyette, M. P. Touzel, G. Gidel, E. Vorontsov, Y. Bengio, and G. Lajoie, "Non-normal recurrent neural network (nnrnn): learning long time dependencies while improving expressivity with transient dynamics," in Advances in Neural Information Processing Systems, pp. 13591–13601, 2019.
[36] R. Pascanu, T. Mikolov, and Y. Bengio, "Understanding the exploding gradient problem," CoRR, abs/1211.5063, vol. 2, p. 417, 2012.
[37] H. Zhao, S. Sun, and B. Jin, "Sequential fault diagnosis based on lstm neural network," IEEE Access, vol. 6, pp. 12929–12939, 2018.
[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," arXiv preprint arXiv:1706.03762, 2017.
[39] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.
[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[41] A. Taylor, M. Marcus, and B. Santorini, "The penn treebank: an overview," in Treebanks, pp. 5–22, Springer, 2003.
[42] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson, "One billion word benchmark for measuring progress in statistical language modeling," in Fifteenth Annual Conference of the International Speech Communication Association, 2014.
[43] S. Merity, C. Xiong, J. Bradbury, and R. Socher, "Pointer sentinel mixture models," arXiv preprint arXiv:1609.07843, 2016.
[44] T. Mikolov and G. Zweig, "Context dependent recurrent neural network language model," in 2012 IEEE Spoken Language Technology Workshop (SLT), pp. 234–239, IEEE, 2012.
[45] W. Zaremba, I. Sutskever, and O. Vinyals, "Recurrent neural network regularization," arXiv preprint arXiv:1409.2329, 2014.
[46] Y. Gal and Z. Ghahramani, "A theoretically grounded application of dropout in recurrent neural networks," in Advances in neural information processing systems, pp. 1019–1027, 2016.
[47] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, "Character-aware neural language models," in Thirtieth AAAI Conference on Artificial Intelligence, 2016.
[48] H. Inan, K. Khosravi, and R. Socher, "Tying word vectors and word classifiers: A loss framework for language modeling," arXiv preprint arXiv:1611.01462, 2016.
[49] J. G. Zilly, R. K. Srivastava, J. Koutník, and J. Schmidhuber, "Recurrent highway networks," in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 4189–4198, JMLR. org, 2017.
[50] B. Zoph and Q. V. Le, "Neural architecture search with reinforcement learning," arXiv preprint arXiv:1611.01578, 2016.
[51] G. Melis, C. Dyer, and P. Blunsom, "On the state of the art of evaluation in neural language models," in International Conference on Learning Representations, 2018.
[52] L. Shi, W. Rong, S. Zhou, N. Jiang, and Z. Xiong, "A dual channel class hierarchy based recurrent language modeling," Neurocomputing, vol. 418, pp. 291–299, 2020.
[53] Y. Qin, F. Qi, S. Ouyang, Z. Liu, C. Yang, Y. Wang, Q. Liu, and M. Sun, "Improving sequence modeling ability of recurrent neural networks via sememes," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2364–2373, 2020.
[54] I. Schlag, T. Munkhdalai, and J. Schmidhuber, "Learning associative inference using fast weight memory," International Conference on Learning Representations, 2021.
[55] E. Grave, A. Joulin, and N. Usunier, "Improving neural language models with a continuous cache," arXiv preprint arXiv:1612.04426, 2016.
[56] S. Ji, S. Vishwanathan, N. Satish, M. J. Anderson, and P. Dubey, "Blackout: Speeding up recurrent neural network language models with very large vocabularies," arXiv preprint arXiv:1511.06909, 2015.
[57] N. Shazeer, J. Pelemans, and C. Chelba, "Skip-gram language modeling using sparse non-negative matrix probability estimation," arXiv preprint arXiv:1412.1454, 2014.
[58] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu, "Exploring the limits of language modeling," arXiv preprint arXiv:1602.02410, 2016.
[59] E. Grave, A. Joulin, M. Cissé, H. Jégou, et al., "Efficient softmax approximation for gpus," in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1302–1310, JMLR. org, 2017.
[60] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, "Language modeling with gated convolutional networks," in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 933–941, JMLR. org, 2017.
[61] O. Kuchaiev and B. Ginsburg, "Factorization tricks for lstm networks," arXiv preprint arXiv:1703.10722, 2017.

--- TRANG 11 ---
TẠP CHÍ CÁC FILE CLASS LATEX, TẬP. 14, SỐ 8, THÁNG 8 2015 11
[62] C. Wang, M. Li, and A. J. Smola, "Language models with transformers," arXiv preprint arXiv:1904.09408, 2019.
[63] T. Dowdell and H. Zhang, "Language modelling for source code with transformer-xl," arXiv preprint arXiv:2007.15813, 2020.
[64] R. Pascanu, T. Mikolov, and Y. Bengio, "On the difficulty of training recurrent neural networks," in International conference on machine learning, pp. 1310–1318, 2013.
