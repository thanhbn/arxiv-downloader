# 2209.07634.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/memory/2209.07634.pdf
# Kích thước tệp: 1157812 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformers Có Bộ Nhớ Tăng Cường Có Trạng Thái Cho Mô Hình Hóa Hội Thoại Hiệu Quả
Qingyang Wu
Đại học Columbia
qw2345@columbia.eduZhou Yu
Đại học Columbia
zy2461@columbia.edu
Tóm tắt
Các mô hình encoder-decoder Transformer đã đạt được hiệu suất tuyệt vời trong các tác vụ sinh hội thoại, tuy nhiên, khả năng không thể xử lý lịch sử hội thoại dài thường dẫn đến việc cắt ngắn ngữ cảnh. Để giải quyết vấn đề này, chúng tôi đề xuất một transformer tăng cường bộ nhớ mới tương thích với các mô hình encoder-decoder đã được tiền huấn luyện hiện có và cho phép bảo tồn hiệu quả thông tin lịch sử hội thoại. Bằng cách kết hợp một mô-đun bộ nhớ riêng biệt cùng với transformer đã được tiền huấn luyện, mô hình có thể trao đổi thông tin hiệu quả giữa các trạng thái bộ nhớ và ngữ cảnh đầu vào hiện tại. Chúng tôi đánh giá mô hình của mình trên ba bộ dữ liệu hội thoại và hai bộ dữ liệu mô hình hóa ngôn ngữ. Kết quả thực nghiệm cho thấy phương pháp của chúng tôi đã đạt được hiệu quả và hiệu suất vượt trội so với các baseline Transformer đã được tiền huấn luyện khác.

1 Giới thiệu
Gần đây, Transformers (Vaswani et al., 2017) đã đạt được kết quả tối ưu trong nhiều tác vụ xử lý ngôn ngữ tự nhiên, đặc biệt là trong hiểu và sinh ngôn ngữ. Trong lĩnh vực mô hình hóa hội thoại miền mở, DialoGPT (Zhang et al., 2020) đã đạt được hiệu suất tuyệt vời bằng cách mở rộng mô hình decoder Transformer GPT2 (Radford et al., 2019) thông qua việc tiền huấn luyện nó trên một kho ngữ liệu lớn các hội thoại miền mở. Tiếp theo, Meena (Adiwardana et al., 2020) và BlenderBot (Roller et al., 2021) đã cải thiện thêm hiệu suất sinh phản hồi với các mô hình encoder-decoder Transformer lớn hơn.

Tuy nhiên, cơ chế attention trong các mô hình hội thoại dựa trên Transformer, có độ phức tạp tăng theo hàm bậc hai với độ dài chuỗi, khiến chúng trở nên tốn kém về mặt tính toán cho các đầu vào ngữ cảnh dài. Ví dụ, BlenderBot (Roller et al., 2021) phải cắt ngắn độ dài đầu vào xuống 128 token để có hiệu quả tốt hơn, nếu không, chi phí tính toán của mô hình sẽ

(a) Mô hình không có trạng thái: thông tin lịch sử chỉ có thể được suy ra từ ngữ cảnh.
(b) Mô hình có trạng thái: thông tin lịch sử được mang bởi các trạng thái bộ nhớ M.
Hình 1: Minh họa Có trạng thái vs. Không có trạng thái. "Trạng thái" có nghĩa là các biểu diễn trạng thái nội bộ của mô hình. ct và rt đại diện cho ngữ cảnh hội thoại và phản hồi tại bước thời gian t. Các mô hình có trạng thái có thể có kích thước ngữ cảnh nhỏ hơn so với các mô hình không có trạng thái vì có bộ nhớ.

trở nên không khả thi cho các tác vụ hội thoại thời gian thực như ứng dụng chatbot.

Nhiều nghiên cứu đã giải quyết thách thức xử lý chuỗi dài với Transformers (Katharopoulos et al., 2020; Qin et al., 2022; Hua et al., 2022; Dai et al., 2019; Rae et al., 2020). Tuy nhiên, họ tập trung vào các tác vụ mô hình hóa ngôn ngữ thuần túy và chủ yếu là các mô hình chỉ có decoder. Một hạn chế khác là các mô hình của họ không được tiền huấn luyện với các kho ngữ liệu lớn, điều này làm tăng khó khăn trong việc so sánh hiệu suất với các Transformers đã được tiền huấn luyện hiện có. Gần đây hơn, Beltagy et al. (2020) đã giải quyết vấn đề bằng cách đề xuất Longformer Encoder-Decoder (LED) dựa trên mô hình encoder-decoder đã được tiền huấn luyện BART (Lewis et al., 2020) cho các tác vụ sequence-to-sequence. Nó sử dụng cửa sổ attention thưa và đạt được độ phức tạp thời gian tuyến tính.arXiv:2209.07634v2  [cs.CL]  23 May 2023

--- TRANG 2 ---
ity. Tuy nhiên, LED không hiệu quả trong mô hình hóa hội thoại, vì nó không có trạng thái và phụ thuộc vào ngữ cảnh để cung cấp thông tin lịch sử.

Trong công trình này, chúng tôi sử dụng ý tưởng của Memory-Augmented Transformer (Memformer) (Wu et al., 2020) và chuyển đổi một Transformer đã được tiền huấn luyện hiện có thành một mô hình có trạng thái với các biểu diễn bộ nhớ nội bộ. Một mô hình có trạng thái có thể giữ thông tin lịch sử trong các trạng thái ẩn nội bộ của nó trái ngược với một mô hình không có trạng thái. Như được hiển thị trong Hình 1, hầu hết các mô hình encoder-decoder Transformer hiện có đều không có trạng thái. Chúng dựa vào ngữ cảnh đầu vào để cung cấp thông tin lịch sử, và do đó chúng thường yêu cầu một ngữ cảnh lớn hơn để tránh mất thông tin. Đối với một mô hình có trạng thái, nó có thể lưu trữ thông tin lịch sử trong các trạng thái bộ nhớ của nó. Với một kích thước ngữ cảnh nhỏ hơn, mô hình có trạng thái vẫn có thể giữ lại hầu hết thông tin lịch sử, điều này dẫn đến hiệu quả tốt hơn so với một mô hình không có trạng thái.

Memformer (Wu et al., 2020) đạt được tính có trạng thái bằng cách có các trạng thái bộ nhớ nội bộ để lưu trữ thông tin lịch sử. Kích thước bộ nhớ được cố định để mô hình sẽ ưu tiên ghi nhớ thông tin quan trọng. Để tương tác với bộ nhớ, nó bao gồm một bộ đọc bộ nhớ và một bộ ghi bộ nhớ vào một mô hình encoder-decoder Transformer. Memformer đã cho thấy hiệu quả tốt hơn trên bộ dữ liệu mô hình hóa ngôn ngữ WikiText-103 (Merity et al., 2017) so với các mô hình chỉ có decoder Transformer-XL (Dai et al., 2019) và Compressive Transformer (Rae et al., 2020). Tuy nhiên, Memformer chỉ tập trung vào các tác vụ mô hình hóa ngôn ngữ và không được tiền huấn luyện trên các kho ngữ liệu lớn, và do đó nó không thể được sử dụng cho các ứng dụng downstream. Ngoài ra, cấu trúc của nó không phù hợp với các mô hình encoder-decoder Transformer đã được tiền huấn luyện hiện có.

Để giải quyết những hạn chế này của Memformer, chúng tôi đề xuất MemBART với các sửa đổi kiến trúc mới và kỹ thuật huấn luyện có thể chuyển đổi mô hình encoder-decoder Transformer đã được tiền huấn luyện hiện có BART (Lewis et al., 2020) thành một mô hình encoder-decoder Transformer tăng cường bộ nhớ có trạng thái. Cụ thể, chúng tôi giới thiệu một luồng attention kép để tăng cường mô-đun bộ nhớ, được thực hiện bằng cách sử dụng một Transformer riêng biệt để cập nhật các trạng thái bộ nhớ tại mỗi lớp. Chúng tôi cũng triển khai một cơ chế cập nhật bộ nhớ cổng dư để giữ lại tốt hơn thông tin lịch sử quan trọng. Tại mỗi bước thời gian, cơ chế cổng kiểm soát mức độ giữ hoặc ghi đè các giá trị của mỗi slot bộ nhớ cho bước thời gian tiếp theo. Chúng tôi tiếp tục tiền huấn luyện mô-đun bộ nhớ và cho phép mô hình ghi nhớ thông tin lịch sử quan trọng. Vì MemBART là một mô hình đã được tiền huấn luyện, nó có thể được sử dụng cho các ứng dụng downstream rộng hơn.

Các đóng góp của chúng tôi tập trung vào việc giới thiệu một mô hình encoder-decoder Transformer tăng cường bộ nhớ có trạng thái mới tương thích với mô hình ngôn ngữ đã được tiền huấn luyện hiện có BART. Chúng tôi đánh giá hiệu quả của mô hình trên ba bộ dữ liệu hội thoại và hai bộ dữ liệu mô hình hóa ngôn ngữ. Kết quả thực nghiệm chứng minh hiệu quả vượt trội của mô hình về mặt độ trễ và hiệu suất. Chúng tôi sẽ phát hành các checkpoint của các mô hình MemBART đã được tiền huấn luyện.

2 Công trình liên quan
2.1 Mạng Neural Có Trạng Thái
Mạng neural hồi quy (RNN) là những mô hình có trạng thái tự nhiên. Huấn luyện RNNs trên dữ liệu chuỗi thời gian dài thường yêu cầu lan truyền ngược được cắt ngắn qua thời gian (Williams và Peng, 1990) và chuyển các trạng thái nội bộ của mô hình sang batch tiếp theo. RNNs có trạng thái cũng được sử dụng rộng rãi cho học tăng cường hồi quy (Gold, 2003; Hausknecht và Stone, 2015), nơi các trạng thái của agent cần được duy trì. Đã có các biến thể của RNNs có trạng thái (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016) được nghiên cứu để giải quyết các tác vụ khác nhau. Tuy nhiên, do không hiệu quả trong việc song song hóa, chúng dần được thay thế bởi các mô hình Transformer lớn (Vaswani et al., 2017).

Transformers chỉ có decoder có thể có trạng thái bằng cách lưu trữ các key và value đã được tính toán trước đó. Transformer-XL (Dai et al., 2019) và Compressive Transformer (Rae et al., 2020) khám phá hướng này, nhưng các trạng thái của chúng có một phạm vi tối đa lý thuyết để duy trì thông tin từ các token trước đó. Do đó, chúng thường yêu cầu một kích thước bộ nhớ lớn để có hiệu quả.

Transformers attention tuyến tính có thể hoạt động như RNNs với các trạng thái. Chúng sử dụng một kernel tuyến tính hóa để xấp xỉ phép toán softmax. Các biến thể khác nhau của Transformers tuyến tính (Katharopoulos et al., 2020; Hua et al., 2022; Qin et al., 2022) đã được đề xuất và đạt được hiệu suất tuyệt vời trong các tác vụ mô hình hóa ngôn ngữ. Tuy nhiên, chưa có Transformers tuyến tính lớn được tiền huấn luyện. Các mô hình tương tự như Memorizing Transformer (Wu et al., 2022), Block-Recurrent Transformer (Hutchins et al., 2022) đều chỉ tập trung vào các tác vụ mô hình hóa ngôn ngữ và không áp dụng được cho các tác vụ downstream khác.

--- TRANG 3 ---
2.2 Các Mô hình Tài liệu Dài Không Có Trạng Thái
Đối với xử lý tài liệu dài, Transformers thưa là một hướng khác. Ý tưởng chính là áp dụng một ma trận attention thưa để bỏ qua các phép tính của các token ở xa. Nhiều công trình (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020) đã khám phá các mẫu attention thưa khác nhau với độ phức tạp tuyến tính. Đặc biệt, Longformer đã mở rộng BART đã được tiền huấn luyện (Lewis et al., 2020) với attention thưa và giới thiệu Longformer-Encoder-Decoder (LED) cho các tác vụ sequence-to-sequence. Tuy nhiên, những mô hình này không có trạng thái, điều này không hiệu quả cho mô hình hóa hội thoại. Chúng yêu cầu ngữ cảnh phải đủ dài để bao phủ đủ thông tin lịch sử. Ngữ cảnh cũng cần được tính toán lại tại mỗi bước thời gian do attention hai chiều. Bên cạnh đó, Transformers thưa cần attention đầy đủ cho cửa sổ cục bộ, điều này khiến chúng kém cạnh tranh hơn so với các mô hình không thưa khi ngữ cảnh ngắn. Ngược lại, phương pháp tăng cường bộ nhớ có trạng thái của chúng tôi có thể có đầu vào ngữ cảnh ngắn hơn trong khi vẫn ghi nhớ thông tin lịch sử.

3 Phương pháp
Trong phần này, trước tiên chúng tôi mô tả nền tảng của Transformers tăng cường bộ nhớ. Sau đó chúng tôi giới thiệu một mô-đun bộ nhớ mới tương thích với các mô hình encoder-decoder Transformer hiện có. Chúng tôi tiếp tục tiền huấn luyện mô-đun bộ nhớ với mục tiêu khử nhiễu chuỗi để khởi tạo khả năng ghi nhớ. Cuối cùng, chúng tôi phân tích độ phức tạp lý thuyết của mô hình đề xuất cho hội thoại.

3.1 Transformer Tăng Cường Bộ Nhớ
Memformer (Wu et al., 2020) sửa đổi một encoder Transformer để tương tác với một bộ nhớ động có kích thước cố định, để nó có thể lưu trữ và truy xuất thông tin lịch sử. Nó bao gồm một bộ đọc bộ nhớ và một bộ ghi bộ nhớ. Bộ đọc bộ nhớ sử dụng cross attention để truy xuất thông tin lịch sử từ bộ nhớ Mt:

QHl, KMl, VMl=HlWQ, MtWK, MtWV
Al=MHAttn (QHl, KM)
Hl+1=Softmax (Al)VM

trong đó Hl là các trạng thái ẩn của đầu vào tại lớp l.
Đối với bộ ghi bộ nhớ, mỗi slot bộ nhớ mi
t∈Mt được chiếu thành một query để attend tới chính nó và các trạng thái ẩn đầu vào của lớp cuối HL:

Qmi
t, Kmi
t=mi
tWQ, mi
tWK
KHL, VHL=HLWK, HLWV
Ami
t=MHAttn (Qmi
t,[Kmi
t;KHL])
mi
t+1=Softmax (Ami
t)[mi
t;VHL]

Các trạng thái bộ nhớ được reset với tín hiệu reset r.
r=(
1,nếu t= 0
0 ngược lại
M′
t=LayerNorm ((1−r)⊙Mt+vb)

Ngoài ra, chúng tôi chuẩn hóa các trạng thái bộ nhớ tại mỗi bước thời gian với một số hạng bias vb như cơ chế quên. vb xác định bộ nhớ ban đầu M0 là LayerNorm (vb).

3.2 Luồng Attention Kép
Memformer thêm các lớp cross-attention giữa self-attention và các lớp feed-forward để đạt được chức năng bộ nhớ. Tuy nhiên, việc trực tiếp đưa các lớp vào bên trong một Transformer đã được tiền huấn luyện sẽ can thiệp vào phân phối kiến thức đã học và dẫn đến hiệu suất tệ hơn. Do đó, chúng tôi nhằm mục đích tích hợp mô-đun bộ nhớ với ảnh hưởng tối thiểu của các Transformers đã được tiền huấn luyện ban đầu.

Chúng tôi đề xuất một luồng attention kép để đường dẫn bộ nhớ có sự can thiệp tối thiểu với đường dẫn dữ liệu của chuỗi đầu vào. Bên trong mỗi lớp l, chúng tôi riêng biệt chiếu chuỗi đầu vào Hl và các trạng thái bộ nhớ Ml thành các query Q, key K, và value V:

QHl, KHl, VHl=WHlHl
QMl, KMl, VMl=WMlMl

Sau đó, có hai luồng attention để thực hiện đọc bộ nhớ và ghi bộ nhớ đồng thời tại mỗi lớp:

AHl=Attention (QHl,[KMl;KHl])
Hl+1=Softmax (AHl)[VMl;VHl]
AMl=Attention (QMl,[KMl;KHl])
Ml+1=Softmax (AMl)[VMl;VHl]

Cụ thể, luồng attention AHl phục vụ như đọc bộ nhớ, nơi các trạng thái ẩn của chuỗi đầu vào Hl thu thập thông tin từ các trạng thái bộ nhớ Mt để có được biểu diễn của lớp tiếp theo Hl+1. Luồng attention khác AMl phục vụ

--- TRANG 4 ---
như ghi bộ nhớ. Lưu ý rằng chúng tôi cập nhật các trạng thái bộ nhớ tại mỗi lớp. Mỗi slot bộ nhớ ml∈Ml attend tới chính nó và các trạng thái ẩn của đầu vào để có được các slot bộ nhớ của lớp tiếp theo Ml+1. Mỗi slot bộ nhớ không can thiệp với các slot bộ nhớ khác khi cập nhật.

Luồng attention kép này cho phép thông tin trao đổi hiệu quả giữa các slot bộ nhớ và chuỗi đầu vào, trong khi ảnh hưởng tối thiểu đến kiến thức của Transformer đã được tiền huấn luyện ban đầu.

Hình 2: Trái: Memformer với cross attention để đọc từ bộ nhớ và một bộ ghi bộ nhớ riêng biệt để cập nhật thông tin trong các slot bộ nhớ. Phải: MemBART với luồng attention kép để xử lý đọc và ghi bộ nhớ đồng thời. Thiết kế này giảm sự can thiệp với phân phối của mô hình đã được tiền huấn luyện.

3.3 Cập Nhật Bộ Nhớ Cổng Dư
Luồng attention kép đạt được đọc và ghi bộ nhớ đồng thời tại mỗi lớp. Tuy nhiên, khi số lượng lớp tăng lên, biểu diễn bộ nhớ của lớp cuối có thể khó giữ lại thông tin từ bước thời gian trước đó.

Như một giải pháp, chúng tôi triển khai một cơ chế cổng dư. Chúng tôi để encoder dự đoán một điểm số zt∈(0,1) với sigmoid để kiểm soát việc cập nhật mỗi slot bộ nhớ riêng biệt.

HMt+1=Encoder (xt, Mt)
M′
t+1=MLP(HMt+1)
zt=σz(WzHMt+1+bz)
Mt+1=zt⊙M′
t+1+ (1−zt)⊙Mt

xt là độ dài chuỗi đầu vào. HMt+1 là các trạng thái ẩn bộ nhớ của lớp cuối. M′
t+1 là ứng viên các slot bộ nhớ của bước thời gian tiếp theo.

3.4 Học Ghi Nhớ Thông Tin Quan Trọng
Vì kích thước bộ nhớ được cố định, mô hình cần học thông tin nào cần giữ và thông tin nào cần quên, nhưng mô-đun bộ nhớ ban đầu không có kiến thức về điều đó. Do đó, nó yêu cầu tiền huấn luyện thêm cho mô-đun bộ nhớ để học ghi nhớ thông tin quan trọng.

Chúng tôi sử dụng mục tiêu khử nhiễu chuỗi làm mục tiêu tiền huấn luyện của mô-đun bộ nhớ. Chúng tôi chia một tài liệu thành các đoạn, thêm các mặt nạ ngẫu nhiên vào những đoạn này, và đưa chúng vào mô hình một cách tuần tự. Mục tiêu này có thể dạy mô hình ghi nhớ thông tin quan trọng. Nếu các từ quan trọng như thực thể có tên xuất hiện trong các bước thời gian trước đó nhưng bị che mặt nạ trong ngữ cảnh đầu vào hiện tại, mô hình có thể dự đoán chúng trở lại với sự giúp đỡ của bộ nhớ. Đối với các từ ít quan trọng hơn có thể dễ dàng suy ra từ ngữ cảnh hoặc ngữ pháp, mô hình có thể chọn không lưu trữ chúng trong bộ nhớ động.

3.5 Phân Tích Độ Phức Tạp
Phương pháp của chúng tôi hiệu quả trong việc xử lý chuỗi dài so với Transformers truyền thống, đặc biệt là trong mô hình hóa hội thoại. Ví dụ, xem xét một hội thoại với T lượt, và N token tại mỗi lượt. Độ phức tạp tổng thể cho một Transformer để xử lý tất cả các lượt sẽ là O(N2+ 2N2+ . . .+TN2), hoặc đơn giản là O(T2N2). Nếu chúng ta giữ tất cả các token lịch sử, một mô hình encoder-decoder truyền thống sẽ yêu cầu tính toán lại tất cả các token lịch sử vì attention hai chiều, điều này làm tăng độ phức tạp. Trong thực tế, do hạn chế về số lượng tối đa của positional embeddings và ràng buộc bộ nhớ GPU, chúng ta thường cắt ngắn lịch sử hội thoại thành một độ dài cố định.

Ngược lại, mô hình có trạng thái của chúng tôi có thể lưu trữ thông tin lịch sử trong bộ nhớ có kích thước cố định. Việc triển khai có độ phức tạp O(TN2), và nó không yêu cầu tính toán lại cho các token lịch sử. Đối với các mô hình Transformer hiệu quả như Longformer, độ phức tạp có thể được giảm từ O(T2N2) xuống O(T2N). Tuy nhiên, khi độ dài ngữ cảnh N nhỏ, số lượng lượt T là yếu tố dẫn đầu cho hiệu quả, nơi phương pháp của chúng tôi cho thấy hiệu quả tốt hơn về mặt lý thuyết.

--- TRANG 5 ---
4 Tiền Huấn Luyện Mô-đun Bộ Nhớ
Như đã đề cập ở trên, mô-đun bộ nhớ cần được tiền huấn luyện để học ghi nhớ thông tin quan trọng. Tuy nhiên, để so sánh hiệu quả của phương pháp đề xuất với các mô hình trước đó, sẽ tốn kém để tiền huấn luyện tất cả các biến thể mô hình. Do đó, chúng tôi sử dụng một tác vụ nhớ lại văn bản đơn giản để đánh giá các mô hình khác nhau trước khi tiền huấn luyện trên các kho ngữ liệu lớn.

Đối với tất cả các biến thể mô hình, chúng tôi chọn BART (Lewis et al., 2020) làm backbone vì nó đã chứng minh hiệu suất tuyệt vời trên các bộ dữ liệu hội thoại. Chúng tôi cũng khởi tạo các tham số self attention và feed-forward của mô-đun bộ nhớ với các trọng số đã được tiền huấn luyện để thích ứng tốt hơn.

4.1 Lựa Chọn Mô Hình với Tác Vụ Nhớ Lại Văn Bản

Hình 3: Đường cong loss cho các mô hình khác nhau cho tác vụ nhớ lại văn bản.

Tác vụ nhớ lại văn bản để mô hình khôi phục văn bản đầu vào của bước thời gian trước đó, nơi thông tin lịch sử chỉ có thể chảy qua nút thắt cổ chai bộ nhớ.

Chúng tôi đánh giá các biến thể mô hình khác nhau với tác vụ nhớ lại văn bản để chọn mô hình tốt nhất trước khi tiền huấn luyện. Đầu tiên là trực tiếp thêm các lớp cross-attention bộ nhớ vào BART (Memformer), mà kiến trúc của mô hình tương tự như Memformer (Wu et al., 2020). Mô hình thứ hai sử dụng ReZero (Bachlechner et al., 2021) mà nó áp dụng một trọng số có thể huấn luyện được khởi tạo bằng không khi thêm lớp cross-attention bộ nhớ, để phân phối đầu ra của mô hình không bị thay đổi ban đầu (Memformer + ReZero). Mô hình thứ ba là MemBART đề xuất của chúng tôi nơi mô-đun bộ nhớ chia sẻ trọng số với BART (MemBART + Shared weights). Mô hình cuối cùng là mô hình cuối cùng của chúng tôi MemBART không chia sẻ trọng số giữa mô-đun bộ nhớ và Transformer đã được tiền huấn luyện (MemBART).

Chi tiết huấn luyện trong Phụ lục A. Trong Hình 3, chúng ta có thể quan sát rằng Memformer ban đầu (màu cam) không hội tụ về loss bằng không. MemBART với trọng số chia sẻ (màu tím) cũng không hội tụ và hoạt động tệ hơn, gợi ý rằng các trạng thái bộ nhớ nên có không gian phân phối khác với word embeddings. Memformer với ReZero (màu xanh lá) hội tụ chậm cuối cùng. So với đó, MemBART (màu xanh dương) chỉ sử dụng một phần tư thời gian để đạt loss gần bằng không. Kết quả cho thấy kiến trúc mô-đun bộ nhớ đề xuất của chúng tôi tương thích với BART đã được tiền huấn luyện và có thể được huấn luyện hiệu quả cho các tác vụ ghi nhớ.

4.2 Tiền Huấn Luyện Khử Nhiễu Chuỗi
Chúng tôi đã chỉ ra rằng MemBART đề xuất đã vượt trội hơn Memformer và các biến thể mô hình khác. Bây giờ, chúng tôi tiền huấn luyện MemBART với mục tiêu khử nhiễu chuỗi cho mô-đun bộ nhớ để ghi nhớ thông tin quan trọng. Chúng tôi có hai kích thước mô hình: MemBART base (183M) và MemBART large (558M). Chúng tôi sử dụng kho ngữ liệu tiền huấn luyện tương tự như BART để tránh rò rỉ dữ liệu, bao gồm một tập con của BooksCorpus (Zhu et al., 2015), CommonCrawl (Raffel et al., 2020), OpenWebText (Gokaslan và Cohen, 2019). Chúng tôi lọc ra các tài liệu có ít hơn 512 token để học bộ nhớ tốt hơn. Chúng tôi chia tài liệu thành các đoạn với kích thước cửa sổ 512 và overlap 128 token. Tại mỗi bước thời gian, chúng tôi che mặt nạ ngẫu nhiên 30% token chuỗi đầu vào. Chúng tôi cũng phát triển một kỹ thuật xử lý batch mới được đề cập trong Phụ lục B.1 để xử lý sự phụ thuộc thời gian giữa các batch. Các chi tiết tiền huấn luyện khác trong Phụ lục B.

Hình 4: Norm gradient của bộ nhớ trong quá trình tiền huấn luyện. Khi gradient gần minimum, mô hình hoạt động tệ trong các tác vụ downstream.

Trong Hình 4, chúng tôi hiển thị độ lớn của các gradient chảy qua các trạng thái bộ nhớ trong quá trình tiền huấn luyện. Ở giai đoạn đầu của tiền huấn luyện (ít hơn 20,000 bước), chúng tôi quan sát rằng mô hình MemBART base không hoạt động tốt trong các tác vụ downstream. Chúng tôi nghi ngờ rằng khi norm gradient nhỏ, có nghĩa là mô hình không tích cực sử dụng các trạng thái bộ nhớ. Do đó, norm gradient phục vụ như một chỉ báo về thời điểm mô-đun bộ nhớ được học. Đối với MemBART large, hiệu suất của các tác vụ downstream được cải thiện sau 50,000 bước khi norm gradient đạt maximum. Mẫu này gợi ý rằng cần một số bước tiền huấn luyện nhất định để mô-đun bộ nhớ học ghi nhớ thông tin quan trọng, và mô hình lớn cần nhiều bước cập nhật hơn để học ghi nhớ.

5 Ứng Dụng Downstream
Trong phần này, chúng tôi giới thiệu các ứng dụng và bộ dữ liệu downstream để đánh giá. Sau đó, chúng tôi hiển thị kết quả trên các tác vụ hội thoại và mô hình hóa ngôn ngữ.

5.1 Chi Tiết Bộ Dữ Liệu
Bộ dữ liệu #Lượt Độ dài TB Độ dài Tối đa
PersonaChat 14.66 244 715
Persuasion 20.58 456 1,437
Multi-Session Chat 60.52 1,823 2,705
Arxiv - 13,409 156,605
PG19 - 105,830 1,181,156

Bảng 2: Thống kê bộ dữ liệu hội thoại và tài liệu dài.

Chúng tôi thử nghiệm trên ba bộ dữ liệu hội thoại khác nhau: PersonaChat (Zhang et al., 2018), PersuasionForGood (Wang et al., 2019), và Multi-Session Chat (MSC) (Xu et al., 2022). Đặc biệt, Multi-Session Chat giải quyết vấn đề thiếu các bộ dữ liệu hội thoại ngữ cảnh dài trong cộng đồng hiện tại. Đây là bộ dữ liệu human-human lớn nhất cho các cuộc hội thoại dài với năm phiên và trung bình 60 lượt phát biểu. Để kiểm tra thêm khả năng của mô hình, chúng tôi cũng đánh giá mô hình trên hai tác vụ mô hình hóa ngôn ngữ: Arxiv và PG19 (Rae et al., 2020). Do ràng buộc tính toán, chúng tôi chọn 2,809 bài báo CS AI Arxiv, và một tập con 200 cuốn sách từ PG19 để đánh giá. Chúng tôi chia 10% dữ liệu để kiểm tra. Thống kê của tất cả các bộ dữ liệu được hiển thị trong Bảng 2.

Chúng tôi so sánh MemBART với GPT2, BART, và Longformer, vì tất cả đều là các mô hình ngôn ngữ được tiền huấn luyện.

--- TRANG 6 ---
Chúng tôi sử dụng beam search với beam size 4 để sinh. Đối với các metric đánh giá, chúng tôi báo cáo perplexity và word overlap F1 cho bộ dữ liệu PersonaChat. Đối với các bộ dữ liệu khác, chúng tôi chỉ báo cáo perplexity do tính đa dạng của phản hồi. Perplexity phản ánh likelihood của ground truth và nó được chỉ ra có tương quan cao với các metric chất lượng hội thoại khác.

Mô hình\Ngữ cảnh 64 128 256 512
PPL↓F1↑PPL↓F1↑PPL↓F1↑PPL↓F1↑
BART base 10.91 25.01 9.39 25.44 8.64 26.31 8.76 26.22
MemBART base (64)* 8.68 27.34 8.58 27.37 8.46 27.05 - -
w/o history 10.52 25.54 9.44 26.52 8.57 26.23 - -
w/o pre-training 10.67 25.26 9.37 26.12 8.60 26.45 - -
MemBART base (128) 8.59 27.45 8.57 27.52 8.39 27.52 - -
MemBART base (256) 8.60 27.65 8.49 27.68 8.38 27.41 - -
GPT2-12 10.93 25.18 9.86 26.03 9.06 26.55 9.04 26.52
GPT2-24 9.51 25.46 8.56 26.52 7.82 27.19 7.81 27.20
BART large 9.12 25.50 8.01 26.84 7.33 28.67 7.31 28.64
MemBART large (128) 7.47 28.06 7.33 28.57 7.15 29.16 - -

Bảng 1: Kết quả PersonaChat. Chúng tôi báo cáo perplexity (PPL) và F1 với các độ dài ngữ cảnh khác nhau. * MemBART (64) có nghĩa là kích thước bộ nhớ là 64. "w/o pre-training" có nghĩa là không có tiền huấn luyện mô-đun bộ nhớ.

Mô hình Cơ sở Ngữ cảnh Độ trễ (ms) ↓Tổng↓Phiên 1 ↓Phiên 2 ↓Phiên 3 ↓Phiên 4 ↓Phiên 5 ↓
BART base 128 16.41 13.05 10.99 12.52 13.18 13.65 14.02
BART base 256 22.12 12.83 10.94 12.29 12.97 13.37 13.78
BART base 512 36.80 12.68 10.92 12.14 12.77 13.19 13.61
BART base 1,024 64.65 12.53 10.81 11.93 12.50 13.10 13.55
LED base 2,048 227.75 12.52 10.76 12.13 12.59 12.93 13.42
MemBART base (128) 128 20.42 12.41 10.72 11.95 12.52 12.88 13.23
MemBART base (128) 256 32.09 12.25 10.62 11.76 12.37 12.71 13.06
MemBART base (128) 512 66.70 12.15 10.63 11.67 12.23 12.57 12.97
Mô hình Lớn Ngữ cảnh Độ trễ (ms) Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
GPT2-12 512 65.77 13.99 12.81 13.45 14.03 14.33 14.78
GPT2-12 1,024 149.05 13.56 12.82 13.48 13.84 13.53 13.82
GPT2-24 512 172.43 11.65 11.07 11.14 11.66 11.86 12.20
GPT2-24 1,024 395.84 11.56 11.03 11.12 11.52 11.75 12.11
BART large 128 45.37 10.61 9.50 10.13 10.68 10.94 11.29
BART large 256 63.79 10.37 9.38 9.86 10.44 10.67 11.02
BART large 512 103.20 10.23 9.44 9.71 10.26 10.52 10.85
BART large 1,024 190.79 10.10 9.41 9.64 10.06 10.36 10.68
LED large 2,048 655.19 10.05 9.43 9.60 10.04 10.27 10.60
MemBART large (128) 128 59.51 10.17 9.22 9.61 10.24 10.47 10.85
MemBART large (128) 256 102.42 10.09 9.20 9.65 10.09 10.38 10.72
MemBART large (128) 512 197.79 9.99 9.22 9.51 10.03 10.23 10.58

Bảng 3: Kết quả perplexity MSC trên tập test. MemBART có thể đạt được độ trễ thấp hơn trong khi có hiệu suất tốt hơn. Phiên 4 và phiên 5 chỉ tồn tại trong quá trình inference. * MemBART (128) có nghĩa là kích thước bộ nhớ là 128. Chi tiết thêm trong Phụ lục C

Mô hình Độ dài Ngữ cảnh
128 256 512 1024
BART base 10.93 10.90 10.80 10.78
MemBART base (64) 10.69 10.66 10.66 -
w/o history 10.86 10.79 10.75 -
MemBART base (128) 10.65 10.57 10.56 -
MemBART base (256) 10.59 10.56 10.54 -
GPT2-12 10.51 10.38 10.33 10.31
GPT2-24 9.37 9.20 9.14 9.11
BART large 9.54 9.40 9.24 9.27
MemBART large (128) 9.34 9.18 9.12 -

Bảng 4: Kết quả perplexity ↓ cho bộ dữ liệu Persuasion. * MemBART (64) có nghĩa là kích thước bộ nhớ là 64.

5.2 Kết Quả Bộ Dữ Liệu Hội Thoại
Bảng 1,4,3 hiển thị kết quả cho PersonaChat, PersuasionForGood, và MSC, tương ứng. Chúng tôi liệt kê một số quan sát chính dưới đây.

Mô-đun bộ nhớ ghi nhớ thông tin lịch sử, và tiền huấn luyện là cần thiết. Trong Bảng 1, chúng tôi chỉ ra rằng bằng cách reset các trạng thái bộ nhớ (w/o history), MemBART hoạt động tương tự như BART base. Ngoài ra, không có tiền huấn luyện, mô-đun bộ nhớ ban đầu không học ghi nhớ thông tin lịch sử.

MemBART có thể nhanh hơn nhiều với kích thước ngữ cảnh đầu vào nhỏ trong khi có hiệu suất tốt hơn. Trong PersonaChat, MemBART với kích thước bộ nhớ 64 và độ dài ngữ cảnh 64 có thể ngang bằng với hiệu suất của BART với độ dài ngữ cảnh 512. Mẫu tương tự cũng xảy ra với PersuasionForGood (Persuasion) và Multi-Session Chat(MSC). Đặc biệt trong MSC, MemBART base có thể đạt được perplexity tương tự (12.41) so với LED base với độ dài ngữ cảnh 2,048, nhưng nhanh hơn 11.15 lần. MemBART large đạt được perplexity tương tự (10.09) so với LED large với độ dài ngữ cảnh 2,048, trong khi nhanh hơn 6.40 lần.

Các mô hình encoder-decoder sử dụng thông tin lịch sử tốt hơn các mô hình chỉ có decoder. Đối với PersonaChat và MSC, BART base và MemBART large vượt trội hơn GPT2-12 và GPT2-24 tương ứng. Ngoại lệ là trong Persuasion, nơi các cuộc hội thoại chứa nhiều phát biểu một lượt hơn. Quan sát này gợi ý rằng các mô hình encoder-decoder sử dụng thông tin lịch sử tốt hơn, và có thể là do ngữ cảnh hai chiều.

--- TRANG 7 ---
Hiệu suất của MemBART được cải thiện khi kích thước ngữ cảnh tăng. Hiệu suất của BART và GPT2 được cải thiện khi kích thước ngữ cảnh tăng. Kết quả cho thấy việc tăng kích thước ngữ cảnh cho MemBART cũng có thể cải thiện hiệu suất của nó, mặc dù chỉ với một biên độ nhỏ. Chúng tôi nghi ngờ rằng việc sử dụng kích thước ngữ cảnh lớn hơn có thể giúp mô hình tăng cường việc ghi nhớ thông tin lịch sử và giảm thiểu các tình huống khi một số thông tin không được giữ trong bộ nhớ.

Tăng kích thước bộ nhớ cải thiện hiệu suất MemBART. Đối với các mô hình MemBART, thông tin lịch sử được lưu trữ bên trong bộ nhớ. Do đó, chúng tôi muốn nghiên cứu hiệu suất tỷ lệ như thế nào với kích thước bộ nhớ. Chúng tôi đánh giá kích thước bộ nhớ 64, 128, và 256. Chúng tôi quan sát rằng khi tăng kích thước bộ nhớ từ 64 lên 128, có một cải thiện lớn, nhưng từ 128 lên 256, cải thiện là biên độ.

5.3 Kết Quả Bộ Dữ Liệu Mô Hình Hóa Ngôn Ngữ
Mô hình Ngữ cảnh Arxiv PG19
BART base 512 15.40 33.70
BART base 1,024 15.09 31.20
LED base 2,048 13.97 30.08
MemBART base (128) 512 14.34 29.81
GPT2-12 512 17.53 32.20
GPT2-12 1,024 15.35 28.31
GPT2-24 512 15.34 22.33
GPT2-24 1,024 13.84 20.86
BART large 512 12.92 24.08
BART large 1,024 12.31 23.07
LED large 2,048 11.82 23.04
MemBART large (128) 512 12.24 22.26

Bảng 5: Điểm perplexity mô hình hóa ngôn ngữ trên bộ dữ liệu Arxiv và PG19. Thấp hơn là tốt hơn.

Chúng tôi cũng đã đánh giá trên hai tác vụ mô hình hóa ngôn ngữ Arxiv và PG19 để hiểu rõ hơn hiệu quả của mô hình. Do ràng buộc tính toán, chúng tôi sử dụng các tập con của hai bộ dữ liệu để đánh giá. Chúng tôi hiển thị kết quả trong Bảng 5.

MemBART hoạt động hơi tệ hơn LED large với ngữ cảnh 2048 trên Arxiv, nhưng tốt hơn trên PG19. Chúng tôi nghi ngờ rằng đó là do các bài báo Arxiv rất có cấu trúc và sử dụng thuật ngữ xuyên suốt bài báo, nhưng sách PG19 có ít phụ thuộc dài hạn hơn. Mẫu hiệu suất tương tự cũng có thể được quan sát giữa BART và GPT, gợi ý rằng các mô hình encoder tốt hơn trong việc sử dụng thông tin dài hạn, và các mô hình decoder tốt hơn trong thông tin ngắn hạn.

5.4 Nghiên Cứu Ablation

Hình 5: Ảnh hưởng của việc thay đổi kích thước bộ nhớ (trái) và time horizon (phải).

Chúng tôi cũng đánh giá ảnh hưởng của việc thay đổi kích thước bộ nhớ và time horizon lan truyền ngược trên bộ dữ liệu PersonaChat với độ dài ngữ cảnh 64. Khi thay đổi kích thước bộ nhớ, chúng tôi đặt time horizon là 5. Trong Hình 5, việc tăng kích thước bộ nhớ có một cải thiện đáng kể cho perplexity cho đến khi nó đạt 128. Khi thay đổi time horizon, kích thước bộ nhớ được đặt là 128. Trong hình bên phải, time horizon là 1 (gradient không thể chảy qua bộ nhớ) đạt được hiệu suất tốt hơn BART, gợi ý rằng bộ nhớ sau tiền huấn luyện có thể bắt được thông tin lịch sử. Tăng time horizon lên 2 có thể cải thiện đáng kể hiệu suất.

6 Kết luận
Tóm lại, chúng tôi đã giới thiệu một mô hình encoder-decoder Transformer tăng cường bộ nhớ có trạng thái mới có thể bảo tồn lịch sử hội thoại dài trong khi tương thích với các mô hình encoder-decoder đã được tiền huấn luyện. Bằng cách kết hợp một mô-đun bộ nhớ riêng biệt với luồng attention kép và cơ chế cổng dư, mô hình của chúng tôi trao đổi thông tin hiệu quả giữa các trạng thái bộ nhớ và transformer đã được tiền huấn luyện. Kết quả thực nghiệm đã chứng minh tính ưu việt của phương pháp chúng tôi về mặt hiệu quả và hiệu suất, khi so sánh với các mô hình đã được tiền huấn luyện khác như BART, GPT, và Longformer (LED).

Đối với công việc tương lai, chúng tôi dự định mở rộng tính tương thích của phương pháp với các mô hình đã được tiền huấn luyện khác, và đánh giá hiệu suất của nó trên các tác vụ khác như hệ thống hội thoại hướng tác vụ, tóm tắt văn bản, và phân loại tài liệu dài. Ngoài ra, chúng tôi sẽ điều tra các biểu diễn bộ nhớ tiên tiến hơn để tối ưu hóa thêm hiệu quả của mô hình hiện tại.

--- TRANG 8 ---
Tài liệu tham khảo
Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, và Quoc V. Le. 2020. Hướng tới một chatbot miền mở giống con người. CoRR, abs/2001.09977.

Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Gary Cottrell, và Julian J. McAuley. 2021. Rezero là tất cả những gì bạn cần: hội tụ nhanh ở độ sâu lớn. Trong Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event, 27-30 July 2021, volume 161 of Proceedings of Machine Learning Research, pages 1352–1361. AUAI Press.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150.

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. 2019. Sinh chuỗi dài với sparse transformers. CoRR, abs/1904.10509.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, và Ruslan Salakhutdinov. 2019. Transformer-xl: Các mô hình ngôn ngữ chú ý vượt ra ngoài ngữ cảnh độ dài cố định. Trong Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978–2988. Association for Computational Linguistics.

Aaron Gokaslan và Vanya Cohen. 2019. Open-webtext corpus. http://Skylion007.github.io/OpenWebTextCorpus.

Carl Gold. 2003. FX trading via recurrent reinforcement learning. Trong 2003 IEEE International Conference on Computational Intelligence for Financial Engineering, CIFEr 2003, Hong Kong, March 20-23, 2003, pages 363–370. IEEE.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, và Demis Hassabis. 2016. Tính toán lai sử dụng mạng neural với bộ nhớ ngoại vi động. Nat., 538(7626):471–476.

Matthew J. Hausknecht và Peter Stone. 2015. Deep recurrent q-learning cho MDPs có thể quan sát một phần. Trong 2015 AAAI Fall Symposia, Arlington, Virginia, USA, November 12-14, 2015, pages 29–37. AAAI Press.

Weizhe Hua, Zihang Dai, Hanxiao Liu, và Quoc V. Le. 2022. Chất lượng transformer trong thời gian tuyến tính. CoRR, abs/2202.10447.

DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, và Behnam Neyshabur. 2022. Block-recurrent transformers. CoRR, abs/2203.07852.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. 2020. Transformers là rnns: Transformers tự hồi quy nhanh với attention tuyến tính. Trong Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5156–5165. PMLR.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART: tiền huấn luyện sequence-to-sequence khử nhiễu cho sinh ngôn ngữ tự nhiên, dịch thuật, và hiểu. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871–7880. Association for Computational Linguistics.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2017. Các mô hình hỗn hợp pointer sentinel. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.

Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, và Yiran Zhong. 2022. cosformer: Suy nghĩ lại softmax trong attention. CoRR, abs/2202.08791.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. 2019. Các mô hình ngôn ngữ là các học viên đa tác vụ không giám sát.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, và Timothy P. Lillicrap. 2020. Compressive transformers cho mô hình hóa chuỗi tầm xa. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Khám phá các giới hạn của học chuyển giao với một transformer văn bản sang văn bản thống nhất. J. Mach. Learn. Res., 21:140:1–140:67.

Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, và Jason Weston. 2021. Công thức để xây dựng một chatbot miền mở. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 300–325. Association for Computational Linguistics.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, và Rob Fergus. 2015. Mạng nhớ đầu cuối. Trong Advances in Neural Information Processing Systems

--- TRANG 9 ---
28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2440–2448.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention là tất cả những gì bạn cần. Trong Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008.

Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, và Zhou Yu. 2019. Persuasion for good: Hướng tới một hệ thống hội thoại thuyết phục cá nhân hóa cho lợi ích xã hội. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5635–5649, Florence, Italy. Association for Computational Linguistics.

Jason Weston, Sumit Chopra, và Antoine Bordes. 2015. Mạng nhớ. Trong 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Ronald J. Williams và Jing Peng. 1990. Một thuật toán hiệu quả dựa trên gradient cho huấn luyện trực tuyến các quỹ đạo mạng hồi quy. Neural Comput., 2(4):490–501.

Qingyang Wu, Zhenzhong Lan, Jing Gu, và Zhou Yu. 2020. Memformer: The memory-augmented transformer. CoRR, abs/2010.06891.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. 2022. Memorizing transformers. Trong International Conference on Learning Representations.

Jing Xu, Arthur Szlam, và Jason Weston. 2022. Vượt ra ngoài bộ nhớ cá vàng: Hội thoại miền mở dài hạn. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5180–5197, Dublin, Ireland. Association for Computational Linguistics.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020. Big bird: Transformers cho chuỗi dài hơn. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, và Jason Weston. 2018. Cá nhân hóa các agent hội thoại: Tôi có một con chó, bạn cũng có thú cưng không? Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2204–2213. Association for Computational Linguistics.

--- TRANG 10 ---
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, và Bill Dolan. 2020. DIALOGPT: Tiền huấn luyện sinh lớn quy mô cho sinh phản hồi hội thoại. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020, pages 270–278. Association for Computational Linguistics.

Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, và Sanja Fidler. 2015. Căn chỉnh sách và phim: Hướng tới giải thích hình ảnh giống như câu chuyện bằng cách xem phim và đọc sách. Trong 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 19–27. IEEE Computer Society.

--- TRANG 11 ---
A Các Biến Thể Mô Hình Khác Nhau
Chúng tôi đánh giá các biến thể mô hình khác nhau để chọn mô hình có hiệu quả bộ nhớ tốt nhất. Chúng tôi chọn tác vụ nhớ lại văn bản để đánh giá. Tác vụ được xây dựng như nhớ lại đoạn văn bản trước đó. Giả sử chúng ta có một tài liệu được chia thành các đoạn văn bản x0, x1, . . . , xt. Encoder nhận một đầu vào xt tại bước thời gian t. Decoder cần dự đoán xt−1. Theo cách này, bộ nhớ phải nén thông tin trước đó vào bộ nhớ.

Memformer Mô hình đầu tiên là áp dụng trực tiếp Memformer bằng cách thêm các lớp cross-attention bộ nhớ vào BART. Lớp cross-attention nằm giữa lớp attention và lớp MLP. Dưới đây là công thức đơn giản hóa không hiển thị normalization:

Hl=Hl+Attn(Hl)
Hl=Hl+CrossAttn (Hl, Mt)
Hl=Hl+MLP(Hl)

Memformer + ReZero sử dụng ReZero (Bachlechner et al., 2021) bằng cách thêm một trọng số có thể huấn luyện được khởi tạo bằng không α khi thêm lớp cross-attention bộ nhớ, và do đó phân phối đầu ra của mô hình sẽ được cập nhật một cách mượt mà.

Hl=Hl+Attn(Hl)
Hl=Hl+αCrossAttn (Hl, Mt)
Hl=Hl+MLP(Hl)

MemBART + Shared weights Một biến thể trực tiếp của phương pháp chúng tôi là chia sẻ các trọng số giữa mô-đun bộ nhớ và Transformer đã được tiền huấn luyện. Điều này tương tự như thêm các prompting embedding có thể huấn luyện vào chuỗi đầu vào.

MemBART là phương pháp đề xuất của chúng tôi. Sự khác biệt chính từ Memformer là mô-đun bộ nhớ, nơi đọc và ghi bộ nhớ được xử lý với một Transformer riêng biệt. Luồng thông tin giữa mô-đun bộ nhớ và Transformer đã được tiền huấn luyện được thực hiện bởi luồng attention kép để ảnh hưởng tối thiểu đến phân phối mô hình ban đầu.

Các siêu tham số huấn luyện chi tiết được hiển thị trong Bảng 6. Time horizon lan truyền ngược được đặt là 2 vì nó đủ cho tác vụ này. Việc huấn luyện mất khoảng ít hơn 12 giờ để hoàn thành trên một GPU A6000.

Siêu tham số Tất cả mô hình
Lớp Encoder 6
Lớp Decoder 6
Kích thước ẩn 768
Đầu attention 12
Kích thước bộ nhớ 32
Độ dài ngữ cảnh 512
Kích thước batch 8
Bước khởi động 1k
Tốc độ học 3e-5
Time horizon 2
Dropout 0.0
Weight decay 0.01
Số bước cập nhật tối đa 100k

Bảng 6: Siêu tham số cho tác vụ nhớ lại văn bản.

B Chi Tiết Tiền Huấn Luyện Khử Nhiễu Chuỗi
Như đã đề cập, chúng tôi sử dụng cùng mục tiêu huấn luyện như BART. Ngoài ra, kho ngữ liệu tiền huấn luyện được chọn tương tự như BART. Vì mô hình của chúng tôi dựa rất nhiều vào BART, chúng tôi sử dụng cùng tokenization như BART. Chúng tôi lọc ra các tài liệu ngắn hơn 512 token. Mỗi tài liệu được chia thành các đoạn với kích thước cửa sổ 512 và overlap 128 token.

Siêu tham số MemBART-base MemBART-large
Lớp Encoder 6 12
Lớp Decoder 6 12
Kích thước ẩn 768 1024
Đầu attention 12 16
Độ dài ngữ cảnh 512 512
Stride 128 128
Tỷ lệ mask 0.3 0.3
Tỷ lệ hoán vị 0.0 0.0
Độ dài thay thế 1 1
Kích thước batch 32 32
Bước khởi động 5k 5k
Tốc độ học 3e-5 1e-5
Time horizon 6 6
Dropout 0.0 0.0
Weight decay 0.01 0.01
Bước cập nhật 100k 100k

Bảng 7: Siêu tham số cho huấn luyện MemBART-base và MemBART-large.

Chúng tôi tiền huấn luyện các mô hình với các siêu tham số được hiển thị trong Bảng 7. Tiền huấn luyện cho MemBART-base mất khoảng 4 ngày trên bốn GPU A6000. Tiền huấn luyện cho MemBART-large mất khoảng 8 ngày trên bốn GPU A6000.

--- TRANG 12 ---
B.1 Xử Lý và Phân Phối Batch

Hình 6: Minh họa về cách các tài liệu hoặc hội thoại được xử lý và phân batch.

Vì các batch phụ thuộc thời gian trong mô hình của chúng tôi, chúng tôi triển khai một bộ phân phối batch để xử lý hiệu quả các tài liệu và hội thoại như được hiển thị trong Hình 6. Trong mô hình này, một số agent có kích thước bằng kích thước batch chia sẻ cùng một hàng đợi dữ liệu để lấy tài liệu. Khi hoàn thành xử lý một tài liệu, agent lấy một tài liệu mới từ hàng đợi chia sẻ, và nó chia tài liệu thành các đoạn văn bản hoặc phát biểu để xuất ra một đầu vào ngữ cảnh tại mỗi bước thời gian. Agent cũng xử lý tín hiệu reset và padding token khi các tài liệu có độ dài khác nhau. Tất cả các agent được đồng bộ hóa, và batch được thu thập tại mỗi bước thời gian. Mô hình này đơn giản hóa việc bảo tồn thứ tự thời gian trong các batch và việc căn chỉnh giữa các tài liệu hoặc hội thoại có độ dài khác nhau. Chúng tôi sử dụng bộ phân phối batch này trong tất cả các thí nghiệm.

C Thí Nghiệm Đầy Đủ Multi-Session Chat
Chúng tôi đã hiển thị các thí nghiệm đầy đủ trên multi-session chat dưới các cài đặt khác nhau. Độ trễ được đo với các đầu vào giả. Chúng tôi báo cáo trung bình của 10 lần chạy và phương sai tương ứng. Chúng tôi chọn các mô hình tốt nhất dựa trên tập validation và sau đó đánh giá chúng trên tập test. Kết quả validation được hiển thị trong Bảng 9. Kết quả test được hiển thị trong Bảng 10.

Một quan sát là Longformer sẽ pad chuỗi thành bội số của 1,024 do cơ chế attention thưa. Hành vi này dẫn đến hiệu suất rất chậm khi kích thước ngữ cảnh nhỏ.

Một quan sát khác là đối với các phiên sau, đặc biệt là Phiên 4 và 5, thông tin lịch sử quan trọng. Đối với Phiên 5, BART base có 4.5% mất mát hiệu suất khi kích thước ngữ cảnh được cắt ngắn xuống 128. BART large có 6.5% mất mát hiệu suất do cắt ngắn. Ngược lại, vì MemBART có bộ nhớ, sự khác biệt hiệu suất nhỏ hơn khi sử dụng các kích thước ngữ cảnh khác nhau.

D Số Lượng Tham Số
Mô hình #Tham số
BART base 139M
MemBART base 183M
BART large 406M
MemBART large 558M

Bảng 8: Số lượng tham số của BART và MemBART.

Chúng tôi hiển thị số lượng tham số của BART và MemBART trong Bảng 8. Vì MemBART kết hợp mô-đun bộ nhớ bổ sung. Nó hơi lớn hơn mô hình BART tương ứng. Nhưng như một sự đánh đổi, MemBART nhanh hơn nhiều so với BART.

E Huấn Luyện Tiết Kiệm Bộ Nhớ GPU
Memformer đề xuất một biến thể của gradient checkpointing để huấn luyện hiệu quả loại mô hình có trạng thái này. Việc tiêu thụ bộ nhớ GPU tỷ lệ tuyến tính với time horizon lan truyền ngược vì nó yêu cầu mở rộng đồ thị tính toán bằng với số bước thời gian.

Chúng tôi áp dụng thuật toán huấn luyện hiệu quả này cho mô hình MemBART large với time horizon 6. Không có phương pháp lan truyền ngược hiệu quả, nó sẽ tiêu thụ một lượng lớn bộ nhớ GPU, điều này khiến việc huấn luyện không khả thi.

--- TRANG 13 ---
Mô hình Cơ sở Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
BART base 128 16.41±0.73 12.72 10.84 13.19 13.15 13.17 12.77
BART base 256 22.12±0.89 12.50 10.77 12.85 12.89 12.96 12.58
BART base 512 36.80±1.17 12.33 10.71 12.61 12.67 12.81 12.43
BART base 1,024 64.65±0.72 12.22 10.69 12.46 12.38 12.77 12.38
Longformer base 256 110.07±0.28 12.55 10.78 12.92 12.93 13.07 12.57
Longformer base 512 113.73±3.16 12.35 10.73 12.64 12.66 12.87 12.40
Longformer base 1,024 115.96±0.25 12.20 10.67 12.55 12.46 12.65 12.26
Longformer base 2,048 227.75±0.13 12.16 10.69 12.54 12.46 12.58 12.15
MemBART base (64) 128 17.23±1.19 12.17 10.6 12.60 12.54 12.55 12.14
MemBART base (64) 256 29.39±0.73 12.06 10.59 12.40 12.36 12.47 12.09
MemBART base (64) 512 59.73±0.66 11.95 10.57 12.28 12.22 12.33 11.98
MemBART base (128) 128 20.42±1.47 12.12 10.6 12.50 12.45 12.51 12.14
MemBART base (128) 256 32.09±0.18 11.96 10.49 12.29 12.28 12.37 11.97
MemBART base (128) 512 66.70±1.83 11.86 10.50 12.15 12.14 12.27 11.89
MemBART base (256) 128 26.56±0.57 12.11 10.58 12.51 12.43 12.47 12.13
MemBART base (256) 256 40.92±0.63 12.00 10.50 12.35 12.34 12.40 12.01
MemBART base (256) 512 75.54±0.14 11.83 10.47 12.11 12.10 12.24 11.86
Mô hình Lớn Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
GPT2-12 128 16.24±1.13 14.17 12.87 14.57 14.5 14.51 14.03
GPT2-12 256 30.80±0.48 13.91 12.70 14.20 14.23 14.25 13.81
GPT2-12 512 65.77±0.74 13.76 12.68 14.03 14.02 14.11 13.67
GPT2-12 1,024 149.05±0.38 13.33 12.66 14.04 13.82 13.26 12.71
GPT2-24 128 42.39±2.50 11.91 11.15 12.17 12.10 12.10 11.83
GPT2-24 256 81.80±0.18 11.66 10.98 11.83 11.83 11.86 11.62
GPT2-24 512 172.43±0.12 11.52 10.99 11.63 11.64 11.72 11.48
GPT2-24 1,024 395.84±0.64 11.43 10.96 11.59 11.48 11.62 11.37
BART large 128 45.37±1.31 10.42 9.31 10.75 10.61 10.68 10.44
BART large 256 63.79±0.40 10.15 9.17 10.35 10.34 10.40 10.20
BART large 512 103.20±2.40 10.00 9.22 10.12 10.12 10.28 10.03
BART large 1,024 190.79±0.29 9.87 9.20 10.03 9.91 10.09 9.90
Longformer large 256 316.42±2.37 10.25 9.28 10.43 10.41 10.55 10.30
Longformer large 512 322.68±1.74 10.06 9.24 10.18 10.15 10.38 10.13
Longformer large 1,024 334.87±5.54 9.90 9.20 10.06 9.95 10.15 9.92
Longformer large 2,048 655.19±5.25 9.87 9.23 10.09 9.90 10.04 9.89
MemBART large (128) 128 59.51±0.91 9.99 9.17 10.19 10.14 10.22 10.02
MemBART large (128) 256 102.42±2.07 9.92 9.08 10.10 10.06 10.15 9.95
MemBART large (128) 512 197.79±4.85 9.79 9.08 9.90 9.88 10.03 9.84

Bảng 9: Kết quả Multi-Session Chat trên tập validation.

--- TRANG 14 ---
Mô hình Cơ sở Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
BART base 128 16.41±0.73 13.05 10.99 12.52 13.18 13.65 14.02
BART base 256 22.12±0.89 12.83 10.94 12.29 12.97 13.37 13.78
BART base 512 36.80±1.17 12.68 10.92 12.14 12.77 13.19 13.61
BART base 1,024 64.65±0.72 12.53 10.81 11.93 12.50 13.10 13.55
Longformer base 256 110.07±0.28 12.87 10.78 12.36 13.02 13.45 13.88
Longformer base 512 113.73±3.16 12.69 10.77 12.19 12.79 13.22 13.67
Longformer base 1,024 115.96±0.25 12.55 10.74 12.12 12.59 13.02 13.48
Longformer base 2,048 227.75±0.13 12.52 10.76 12.13 12.59 12.93 13.42
MemBART base (64) 128 17.23±1.19 12.42 10.72 11.95 12.52 12.93 13.23
MemBART base (64) 256 29.39±0.73 12.34 10.66 11.86 12.46 12.84 13.16
MemBART base (64) 512 59.73±0.66 12.23 10.66 11.78 12.32 12.66 13.02
MemBART base (128) 128 20.42±1.47 12.41 10.72 11.95 12.52 12.88 13.23
MemBART base (128) 256 32.09±0.18 12.25 10.62 11.76 12.37 12.71 13.06
MemBART base (128) 512 66.70±1.83 12.15 10.63 11.67 12.23 12.57 12.97
MemBART base (256) 128 26.56±0.57 12.38 10.67 11.90 12.51 12.86 13.20
MemBART base (256) 256 40.92±0.63 12.25 10.59 11.76 12.38 12.74 13.07
MemBART base (256) 512 75.54±0.14 12.09 10.57 11.62 12.18 12.53 12.90
Mô hình Lớn Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
GPT2-12 128 16.24±1.13 14.36 12.91 13.80 14.43 14.79 15.22
GPT2-12 256 30.80±0.48 14.13 12.80 13.57 14.21 14.53 14.93
GPT2-12 512 65.77±0.74 13.99 12.81 13.45 14.03 14.33 14.78
GPT2-12 1,024 149.05±0.38 13.56 12.82 13.48 13.84 13.53 13.82
GPT2-24 128 42.39±2.50 12.03 11.17 11.52 12.07 12.30 12.62
GPT2-24 256 81.80±0.18 11.78 11.02 11.28 11.82 12.04 12.36
GPT2-24 512 172.43±0.12 11.65 11.07 11.14 11.66 11.86 12.20
GPT2-24 1,024 395.84±0.64 11.56 11.03 11.12 11.52 11.75 12.11
BART large 128 45.37±1.31 10.61 9.50 10.13 10.68 10.94 11.29
BART large 256 63.79±0.40 10.37 9.38 9.86 10.44 10.67 11.02
BART large 512 103.20±2.40 10.23 9.44 9.71 10.26 10.52 10.85
BART large 1,024 190.79±0.29 10.10 9.41 9.64 10.06 10.36 10.68
Longformer large 256 316.42±2.37 10.43 9.34 9.95 10.52 10.75 11.11
Longformer large 512 322.68±1.74 10.28 9.37 9.77 10.32 10.57 10.92
Longformer large 1,024 334.87±5.54 10.13 9.42 9.66 10.11 10.38 10.72
Longformer large 2,048 655.19±5.25 10.05 9.43 9.60 10.04 10.27 10.60
MemBART large (128) 128 59.51±0.91 10.17 9.22 9.61 10.24 10.47 10.85
MemBART large (128) 256 102.42±2.07 10.09 9.20 9.65 10.09 10.38 10.72
MemBART large (128) 512 197.79±4.85 9.99 9.22 9.51 10.03 10.23 10.58

Bảng 10: Kết quả Multi-Session Chat trên tập test.
