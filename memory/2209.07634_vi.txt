# 2209.07634.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/memory/2209.07634.pdf
# Kích thước tệp: 1157812 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformer Có Trạng Thái Được Tăng Cường Bộ Nhớ cho Mô Hình Hóa Hội Thoại Hiệu Quả
Qingyang Wu
Đại học Columbia
qw2345@columbia.eduZhou Yu
Đại học Columbia
zy2461@columbia.edu
Tóm tắt
Các mô hình encoder-decoder Transformer đã
đạt được hiệu suất xuất sắc trong các tác vụ sinh
văn bản hội thoại, tuy nhiên, khả năng không thể
xử lý lịch sử hội thoại dài thường dẫn đến việc
cắt ngắn ngữ cảnh. Để giải quyết vấn đề này,
chúng tôi đề xuất một transformer tăng cường
bộ nhớ mới tương thích với các mô hình encoder-
decoder đã được pre-train sẵn và cho phép bảo
toàn hiệu quả thông tin lịch sử hội thoại. Bằng
cách kết hợp một module bộ nhớ riêng biệt cùng
với transformer đã được pre-train, mô hình có
thể trao đổi thông tin hiệu quả giữa các trạng
thái bộ nhớ và ngữ cảnh đầu vào hiện tại. Chúng
tôi đánh giá mô hình của mình trên ba bộ dữ liệu
hội thoại và hai bộ dữ liệu mô hình hóa ngôn ngữ.
Kết quả thí nghiệm cho thấy phương pháp của
chúng tôi đã đạt được hiệu quả và hiệu suất vượt
trội so với các baseline Transformer đã được
pre-train khác.
1 Giới thiệu
Gần đây, Transformers (Vaswani et al., 2017) đã
đạt được kết quả tiên tiến trong nhiều tác vụ xử lý
ngôn ngữ tự nhiên, đặc biệt trong hiểu và sinh ngôn
ngữ. Trong lĩnh vực mô hình hóa hội thoại mở,
DialoGPT (Zhang et al., 2020) đã đạt được hiệu
suất xuất sắc bằng cách mở rộng mô hình decoder
Transformer GPT2 (Radford et al., 2019) thông
qua việc pre-train trên một corpus lớn các hội thoại
mở. Tiếp theo, Meena (Adiwardana et al., 2020)
và BlenderBot (Roller et al., 2021) đã cải thiện
thêm hiệu suất sinh phản hồi với các mô hình
Transformer encoder-decoder lớn hơn.
Tuy nhiên, cơ chế attention trong các mô hình
hội thoại dựa trên Transformer, có độ phức tạp
tăng theo bậc hai với độ dài sequence, khiến
chúng tốn kém về mặt tính toán cho các đầu vào
ngữ cảnh dài. Ví dụ, BlenderBot (Roller et al.,
2021) phải cắt ngắn độ dài đầu vào xuống 128
token để đạt hiệu quả tốt hơn, nếu không, chi phí
tính toán của mô hình sẽ
(a) Mô hình không có trạng thái: thông tin lịch sử chỉ có thể được suy luận từ ngữ cảnh.
(b) Mô hình có trạng thái: thông tin lịch sử được mang theo bởi các trạng thái bộ nhớ M.
Hình 1: Minh họa Có trạng thái vs. Không có trạng thái. "Trạng thái" có nghĩa là các biểu diễn trạng thái nội bộ của mô hình. ct và rt biểu diễn ngữ cảnh hội thoại và phản hồi tại timestep t. Các mô hình có trạng thái có thể có kích thước ngữ cảnh nhỏ hơn so với các mô hình không có trạng thái do có bộ nhớ.
trở nên không khả thi cho các tác vụ hội thoại
thời gian thực như ứng dụng chatbot.
Nhiều nghiên cứu đã giải quyết thách thức
xử lý các sequence dài với Transformers
(Katharopoulos et al., 2020; Qin et al., 2022; Hua
et al., 2022; Dai et al., 2019; Rae et al., 2020). Tuy
nhiên, chúng tập trung vào các tác vụ mô hình hóa
ngôn ngữ thuần túy và chủ yếu là các mô hình chỉ
có decoder. Một hạn chế khác là các mô hình của
chúng không được pre-train với các corpus lớn,
điều này làm tăng khó khăn trong việc so sánh
hiệu suất với các Transformers đã được pre-train
hiện có. Gần đây hơn, Beltagy et al. (2020) đã
giải quyết vấn đề bằng cách đề xuất Longformer
Encoder-Decoder (LED) dựa trên mô hình
encoder-decoder đã được pre-train BART (Lewis
et al., 2020) cho các tác vụ sequence-to-sequence.
Nó sử dụng một cửa sổ attention thưa thớt và đạt
được độ phức tạp thời gian tuyến tính.arXiv:2209.07634v2  [cs.CL]  23 May 2023

--- TRANG 2 ---
ity. Tuy nhiên, LED không hiệu quả trong mô hình
hóa hội thoại, vì nó không có trạng thái và phụ
thuộc vào ngữ cảnh để cung cấp thông tin lịch sử.
Trong công trình này, chúng tôi sử dụng ý tưởng
của Memory-Augmented Transformer (Memformer)
(Wu et al., 2020) và chuyển đổi một Transformer
đã được pre-train hiện có thành một mô hình có
trạng thái với các biểu diễn bộ nhớ nội bộ. Một
mô hình có trạng thái có thể giữ thông tin lịch sử
trong các trạng thái ẩn nội bộ của nó, ngược lại
với mô hình không có trạng thái. Như được hiển
thị trong Hình 1, hầu hết các mô hình Transformer
encoder-decoder hiện có đều không có trạng thái.
Chúng dựa vào ngữ cảnh đầu vào để cung cấp
thông tin lịch sử, và do đó chúng thường yêu cầu
một ngữ cảnh lớn hơn để tránh mất thông tin. Đối
với một mô hình có trạng thái, nó có thể lưu trữ
thông tin lịch sử trong các trạng thái bộ nhớ của
nó. Với kích thước ngữ cảnh nhỏ hơn, mô hình
có trạng thái vẫn có thể giữ lại hầu hết thông tin
lịch sử, dẫn đến hiệu quả tốt hơn so với mô hình
không có trạng thái.
Memformer (Wu et al., 2020) đạt được tính có
trạng thái bằng cách có các trạng thái bộ nhớ nội
bộ để lưu trữ thông tin lịch sử. Kích thước bộ nhớ
cố định để mô hình sẽ ưu tiên ghi nhớ thông tin
quan trọng. Để tương tác với bộ nhớ, nó bao gồm
một bộ đọc bộ nhớ và một bộ ghi bộ nhớ vào mô
hình Transformer encoder-decoder. Memformer
đã cho thấy hiệu quả tốt hơn trên bộ dữ liệu mô
hình hóa ngôn ngữ WikiText-103 (Merity et al.,
2017) so với các mô hình chỉ có decoder
Transformer-XL (Dai et al., 2019) và Compressive
Transformer (Rae et al., 2020). Tuy nhiên,
Memformer chỉ tập trung vào các tác vụ mô hình
hóa ngôn ngữ và không được pre-train trên các
corpus lớn, và do đó nó không thể được sử dụng
cho các ứng dụng downstream. Ngoài ra, cấu trúc
của nó không phù hợp với các mô hình Transformer
encoder-decoder đã được pre-train hiện có.
Để giải quyết những hạn chế này của Memformer,
chúng tôi đề xuất MemBART với các sửa đổi kiến
trúc và kỹ thuật huấn luyện mới có thể chuyển đổi
mô hình Transformer encoder-decoder đã được
pre-train hiện có BART (Lewis et al., 2020) thành
một mô hình Transformer encoder-decoder tăng
cường bộ nhớ có trạng thái. Cụ thể, chúng tôi giới
thiệu một dòng attention kép để tăng cường module
bộ nhớ, được thực hiện bằng cách sử dụng một
Transformer riêng biệt để cập nhật các trạng thái
bộ nhớ tại mỗi layer. Chúng tôi cũng triển khai
một cơ chế cập nhật bộ nhớ có cổng dư để giữ lại
tốt hơn thông tin lịch sử quan trọng. Tại mỗi
timestep, cơ chế cổng kiểm soát mức độ giữ lại
hoặc ghi đè giá trị của mỗi slot bộ nhớ cho timestep
tiếp theo. Chúng tôi tiếp tục pre-train module bộ
nhớ và cho phép mô hình ghi nhớ thông tin lịch sử
quan trọng. Vì MemBART là một mô hình đã được
pre-train, nó có thể được sử dụng cho các ứng
dụng downstream rộng rãi hơn.
Các đóng góp của chúng tôi tập trung vào việc
giới thiệu một mô hình Transformer encoder-
decoder tăng cường bộ nhớ có trạng thái mới tương
thích với mô hình ngôn ngữ đã được pre-train hiện
có BART. Chúng tôi đánh giá hiệu quả của mô hình
trên ba bộ dữ liệu hội thoại và hai bộ dữ liệu mô
hình hóa ngôn ngữ. Kết quả thí nghiệm chứng minh
hiệu quả vượt trội của mô hình về độ trễ và hiệu
suất. Chúng tôi sẽ phát hành các checkpoint của
các mô hình MemBART đã được pre-train.
2 Công trình liên quan
2.1 Mạng nơ-ron có trạng thái
Mạng nơ-ron hồi quy (RNN) là mô hình có trạng
thái tự nhiên. Huấn luyện RNN trên dữ liệu chuỗi
thời gian dài thường yêu cầu lan truyền ngược
cắt ngắn qua thời gian (Williams and Peng, 1990)
và truyền các trạng thái nội bộ của mô hình sang
batch tiếp theo. RNN có trạng thái cũng được sử
dụng rộng rãi cho học tăng cường hồi quy (Gold,
2003; Hausknecht and Stone, 2015), nơi các trạng
thái của agent cần được duy trì. Đã có các biến thể
của RNN có trạng thái (Weston et al., 2015;
Sukhbaatar et al., 2015; Graves et al., 2016) được
nghiên cứu để giải quyết các tác vụ khác nhau.
Tuy nhiên, do tính không hiệu quả song song,
chúng dần được thay thế bởi các mô hình
Transformer lớn (Vaswani et al., 2017).
Các Transformer chỉ có decoder có thể có trạng
thái bằng cách lưu trữ các key và value đã được
tính toán trước đó. Transformer-XL (Dai et al.,
2019) và Compressive Transformer (Rae et al.,
2020) khám phá hướng này, nhưng các trạng thái
của chúng có phạm vi tối đa lý thuyết để duy trì
thông tin từ các token trước đó. Do đó, chúng
thường yêu cầu kích thước bộ nhớ lớn để hiệu quả.
Các Transformer attention tuyến tính có thể hoạt
động như RNN với các trạng thái. Chúng sử dụng
một kernel tuyến tính hóa để xấp xỉ phép toán
softmax. Các biến thể khác nhau của Transformer
tuyến tính (Katharopoulos et al., 2020; Hua et al.,
2022; Qin et al., 2022) đã được đề xuất và đạt được
hiệu suất tuyệt vời trong các tác vụ mô hình hóa
ngôn ngữ. Tuy nhiên, chưa có Transformer tuyến
tính lớn đã được pre-train. Các mô hình tương tự
như Memorizing Transformer (Wu et al., 2022),
Block-Recurrent Transformer (Hutchins et al.,
2022) đều chỉ tập trung vào các tác vụ mô hình
hóa ngôn ngữ và không áp dụng được cho các tác
vụ downstream khác.

--- TRANG 3 ---
2.2 Mô hình tài liệu dài không có trạng thái
Để xử lý tài liệu dài, Transformer thưa thớt là
một hướng khác. Ý tưởng chính là áp dụng ma trận
attention thưa thớt để bỏ qua các tính toán của
các token ở xa. Nhiều công trình (Child et al.,
2019; Zaheer et al., 2020; Beltagy et al., 2020)
đã khám phá các mẫu attention thưa thớt khác nhau
với độ phức tạp tuyến tính. Đặc biệt, Longformer
đã mở rộng BART đã được pre-train (Lewis et al.,
2020) với attention thưa thớt và giới thiệu
Longformer-Encoder-Decoder (LED) cho các tác
vụ sequence-to-sequence. Tuy nhiên, các mô hình
này không có trạng thái, điều này không hiệu quả
cho mô hình hóa hội thoại. Chúng yêu cầu ngữ
cảnh phải đủ dài để bao phủ đủ thông tin lịch sử.
Ngữ cảnh cũng cần được tính toán lại tại mỗi
timestep do attention hai chiều. Bên cạnh đó,
Transformer thưa thớt cần attention đầy đủ cho
cửa sổ cục bộ, điều này khiến chúng ít cạnh tranh
hơn so với các mô hình không thưa thớt khi ngữ
cảnh ngắn. Ngược lại, phương pháp tăng cường
bộ nhớ có trạng thái của chúng tôi có thể có đầu
vào ngữ cảnh ngắn hơn trong khi vẫn ghi nhớ
thông tin lịch sử.
3 Phương pháp
Trong phần này, trước tiên chúng tôi mô tả nền
tảng của Transformer tăng cường bộ nhớ. Sau đó
chúng tôi giới thiệu một module bộ nhớ mới tương
thích với các mô hình Transformer encoder-
decoder hiện có. Chúng tôi tiếp tục pre-train
module bộ nhớ với mục tiêu sequence denoising
để khởi tạo khả năng ghi nhớ. Cuối cùng, chúng
tôi phân tích độ phức tạp lý thuyết của mô hình
đề xuất cho hội thoại.
3.1 Transformer tăng cường bộ nhớ
Memformer (Wu et al., 2020) sửa đổi một
Transformer encoder để tương tác với một bộ
nhớ động có kích thước cố định, để nó có thể
lưu trữ và truy xuất thông tin lịch sử. Nó bao gồm
một bộ đọc bộ nhớ và một bộ ghi bộ nhớ. Bộ đọc
bộ nhớ sử dụng cross attention để truy xuất thông
tin lịch sử từ bộ nhớ Mt:
QHl, KMl, VMl=HlWQ, MtWK, MtWV
Al=MHAttn (QHl, KM)
Hl+1=Softmax (Al)VM
trong đó Hl là các trạng thái ẩn của đầu vào tại
layer l.
Đối với bộ ghi bộ nhớ, mỗi slot bộ nhớ mi
t∈
Mt được chiếu thành một query để attend tới chính
nó và các trạng thái ẩn đầu vào của layer cuối HL:
Qmi
t, Kmi
t=mi
tWQ, mi
tWK
KHL, VHL=HLWK, HLWV
Ami
t=MHAttn (Qmi
t,[Kmi
t;KHL])
mi
t+1=Softmax (Ami
t)[mi
t;VHL]
Các trạng thái bộ nhớ được reset với tín hiệu
reset r.
r=(
1,ift= 0
0otherwise
M′
t=LayerNorm ((1−r)⊙Mt+vb)
Ngoài ra, chúng tôi chuẩn hóa các trạng thái bộ
nhớ tại mỗi timestep với một bias term vb như cơ
chế quên. vb xác định bộ nhớ ban đầu M0 là
LayerNorm (vb).
3.2 Dòng attention kép
Memformer thêm các layer cross-attention giữa
self-attention và các layer feed-forward để đạt
được chức năng bộ nhớ. Tuy nhiên, việc tiêm trực
tiếp các layer vào trong một Transformer đã được
pre-train sẽ can thiệp vào phân phối kiến thức đã
học và dẫn đến hiệu suất tồi tệ hơn. Do đó, chúng
tôi nhằm mục đích tích hợp module bộ nhớ với
ảnh hưởng tối thiểu của các Transformer đã được
pre-train gốc.
Chúng tôi đề xuất một dòng attention kép để
đường dẫn bộ nhớ có can thiệp tối thiểu với đường
dẫn dữ liệu của input sequence. Bên trong mỗi
layer l, chúng tôi chiếu riêng biệt input sequence
Hl và các trạng thái bộ nhớ Ml thành các query Q,
key K, và value V:
QHl, KHl, VHl=WHlHl
QMl, KMl, VMl=WMlMl
Sau đó, có hai dòng attention để thực hiện đọc
bộ nhớ và ghi bộ nhớ đồng thời tại mỗi layer:
AHl=Attention (QHl,[KMl;KHl])
Hl+1=Softmax (AHl)[VMl;VHl]
AMl=Attention (QMl,[KMl;KHl])
Ml+1=Softmax (AMl)[VMl;VHl]
Cụ thể, dòng attention AHl phục vụ như đọc bộ
nhớ, nơi các trạng thái ẩn của input sequence Hl
thu thập thông tin từ các trạng thái bộ nhớ Mt để
có được biểu diễn của layer tiếp theo Hl+1. Dòng
attention khác AMl phục vụ

--- TRANG 4 ---
như ghi bộ nhớ. Lưu ý rằng chúng tôi cập nhật
các trạng thái bộ nhớ tại mỗi layer. Mỗi slot bộ
nhớ ml∈Ml attend tới chính nó và các trạng thái
ẩn của đầu vào để có được các slot bộ nhớ của
layer tiếp theo Ml+1. Mỗi slot bộ nhớ không can
thiệp với các slot bộ nhớ khác khi cập nhật.
Dòng attention kép này cho phép thông tin trao
đổi hiệu quả giữa các slot bộ nhớ và input
sequence, trong khi ảnh hưởng tối thiểu đến kiến
thức của Transformer đã được pre-train gốc.
3.3 Cập nhật bộ nhớ có cổng dư
Dòng attention kép đạt được đọc bộ nhớ và ghi
bộ nhớ đồng thời tại mỗi layer. Tuy nhiên, khi số
lượng layer tăng, biểu diễn bộ nhớ của layer cuối
có thể khó giữ lại thông tin của timestep trước đó.
Như một giải pháp, chúng tôi triển khai một cơ
chế cổng dư. Chúng tôi để encoder dự đoán một
điểm số zt∈(0,1) với sigmoid để kiểm soát việc
cập nhật của mỗi slot bộ nhớ riêng biệt.
HMt+1=Encoder (xt, Mt)
M′
t+1=MLP(HMt+1)
zt=σz(WzHMt+1+bz)
Mt+1=zt⊙M′
t+1+ (1−zt)⊙Mt
xt là độ dài input sequence. HMt+1 là các trạng
thái ẩn bộ nhớ của layer cuối. M′
t+1 là ứng viên
cho các slot bộ nhớ của timestep tiếp theo.
3.4 Học ghi nhớ thông tin quan trọng
Vì kích thước bộ nhớ cố định, mô hình cần học
thông tin nào cần giữ và thông tin nào cần quên,
nhưng module bộ nhớ ban đầu không có kiến thức
về điều đó. Do đó, nó yêu cầu pre-training thêm
cho module bộ nhớ để học ghi nhớ thông tin quan
trọng.
Chúng tôi sử dụng mục tiêu sequence denoising
làm mục tiêu pre-training của module bộ nhớ.
Chúng tôi chia một tài liệu thành các đoạn, thêm
các mask ngẫu nhiên vào các đoạn này, và đưa
chúng vào mô hình tuần tự. Mục tiêu này có thể
dạy mô hình ghi nhớ thông tin quan trọng. Nếu
các từ quan trọng như thực thể có tên xuất hiện
trong các timestep trước đó nhưng bị mask trong
ngữ cảnh đầu vào hiện tại, mô hình có thể dự đoán
lại chúng với sự giúp đỡ của bộ nhớ. Đối với các
từ ít quan trọng hơn có thể dễ dàng suy luận từ
ngữ cảnh hoặc ngữ pháp, mô hình có thể chọn
không lưu trữ chúng trong bộ nhớ động.
3.5 Phân tích độ phức tạp
Phương pháp của chúng tôi hiệu quả trong việc
xử lý các sequence dài so với Transformer truyền
thống, đặc biệt trong mô hình hóa hội thoại. Ví dụ,
xem xét một hội thoại với T lượt, và N token tại
mỗi lượt. Độ phức tạp tổng thể cho một Transformer
để xử lý tất cả các lượt sẽ là O(N2+ 2N2+
. . .+TN2), hoặc đơn giản là O(T2N2). Nếu chúng
ta giữ tất cả các token lịch sử, một mô hình encoder-
decoder truyền thống sẽ yêu cầu tính toán lại tất
cả các token lịch sử do attention hai chiều, điều
này làm tăng độ phức tạp. Trong thực tế, do hạn
chế của số lượng tối đa các positional embedding
và ràng buộc bộ nhớ GPU, chúng ta thường cắt
ngắn lịch sử hội thoại thành một độ dài cố định.
Ngược lại, mô hình có trạng thái của chúng tôi có
thể lưu trữ thông tin lịch sử trong bộ nhớ có kích
thước cố định. Việc triển khai có độ phức tạp
O(TN2), và nó không yêu cầu tính toán lại cho
các token lịch sử. Đối với các mô hình Transformer
hiệu quả như Longformer, độ phức tạp có thể được
giảm từ O(T2N2) xuống O(T2N). Tuy nhiên, khi
độ dài ngữ cảnh N nhỏ, số lượng lượt T là yếu tố
chính cho hiệu quả, nơi phương pháp của chúng
tôi cho thấy hiệu quả tốt hơn về lý thuyết.
4 Pre-training module bộ nhớ
Như đã đề cập ở trên, module bộ nhớ cần được
pre-train để học ghi nhớ thông tin quan trọng. Tuy
nhiên, để so sánh hiệu quả của phương pháp đề
xuất với các mô hình trước đó, sẽ tốn kém để pre-
train tất cả các biến thể mô hình. Do đó, chúng tôi
sử dụng một tác vụ nhớ lại văn bản đơn giản để
đánh giá các mô hình khác nhau trước khi pre-train
trên các corpus lớn.
Đối với tất cả các biến thể mô hình, chúng tôi
chọn BART (Lewis et al., 2020) làm backbone vì
nó đã chứng minh hiệu suất tuyệt vời trên các bộ
dữ liệu hội thoại. Chúng tôi cũng khởi tạo các tham
số self attention và feed-forward của module bộ
nhớ với các trọng số đã được pre-train để thích
ứng tốt hơn.
4.1 Lựa chọn mô hình với tác vụ nhớ lại văn bản
Hình 3: Đường cong loss cho các mô hình khác nhau
cho tác vụ nhớ lại văn bản.
Tác vụ nhớ lại văn bản để mô hình khôi phục
văn bản đầu vào của timestep trước đó, nơi thông
tin lịch sử chỉ có thể truyền qua bottleneck bộ nhớ.
Chúng tôi đánh giá các biến thể mô hình khác nhau
với tác vụ nhớ lại văn bản để chọn mô hình tốt
nhất trước khi pre-training. Đầu tiên là thêm trực
tiếp các layer memory cross-attention vào BART
(Memformer), trong đó kiến trúc của mô hình
tương tự như Memformer (Wu et al., 2020). Mô
hình thứ hai sử dụng ReZero (Bachlechner et al.,
2021) áp dụng một trọng số có thể huấn luyện
được khởi tạo bằng zero khi thêm layer memory
cross-attention, để phân phối đầu ra của mô hình
không thay đổi ban đầu (Memformer + ReZero).
Mô hình thứ ba là MemBART đề xuất của chúng
tôi trong đó module bộ nhớ chia sẻ trọng số với
BART (MemBART + Shared weights). Mô hình
cuối cùng là mô hình cuối cùng của chúng tôi
MemBART không chia sẻ trọng số giữa module
bộ nhớ và Transformer đã được pre-train
(MemBART).
Chi tiết huấn luyện được nêu trong Phụ lục A.
Trong Hình 3, chúng ta có thể quan sát rằng
Memformer gốc (cam) không hội tụ về loss bằng
zero. MemBART với trọng số chia sẻ (tím) cũng
không hội tụ và hoạt động tồi hơn, cho thấy rằng
các trạng thái bộ nhớ nên có không gian phân phối
khác với các word embedding. Memformer với
ReZero (xanh lá) hội tụ chậm cuối cùng. So sánh,
MemBART (xanh dương) chỉ sử dụng một phần
tư thời gian để đạt được loss gần như bằng zero.
Kết quả cho thấy kiến trúc module bộ nhớ đề xuất
của chúng tôi tương thích với BART đã được pre-
train và có thể được huấn luyện hiệu quả cho các
tác vụ ghi nhớ.
4.2 Sequence Denoising Pre-training
Chúng tôi đã chỉ ra rằng MemBART đề xuất đã
vượt trội hơn Memformer và các biến thể mô hình
khác. Bây giờ, chúng tôi pre-train MemBART với
mục tiêu sequence denoising cho module bộ nhớ
để ghi nhớ thông tin quan trọng. Chúng tôi có hai
kích thước mô hình: MemBART base (183M) và
MemBART large (558M). Chúng tôi sử dụng corpus
pre-training tương tự như BART để tránh rò rỉ dữ
liệu, bao gồm một tập con của BooksCorpus (Zhu
et al., 2015), CommonCrawl (Raffel et al., 2020),
OpenWebText (Gokaslan and Cohen, 2019). Chúng
tôi lọc bỏ các tài liệu có ít hơn 512 token để học
bộ nhớ tốt hơn. Chúng tôi chia tài liệu thành các
đoạn với kích thước cửa sổ 512 và overlap 128
token. Tại mỗi timestep, chúng tôi mask ngẫu nhiên
30% token của input sequence. Chúng tôi cũng
phát triển một kỹ thuật xử lý batch mới được đề
cập trong Phụ lục B.1 để xử lý phụ thuộc thời gian
giữa các batch. Các chi tiết pre-training khác

--- TRANG 5 ---
được nêu trong Phụ lục B.
Hình 4: Gradient norm của bộ nhớ trong quá trình
pre-training. Khi gradient gần mức tối thiểu, mô hình
hoạt động tệ trong các tác vụ downstream.
Trong Hình 4, chúng tôi hiển thị độ lớn của các
gradient truyền qua các trạng thái bộ nhớ trong
quá trình pre-training. Ở giai đoạn đầu của pre-
training (ít hơn 20.000 step), chúng tôi quan sát
thấy rằng mô hình MemBART base không hoạt
động tốt trong các tác vụ downstream. Chúng tôi
nghi ngờ rằng khi gradient norm nhỏ, có nghĩa là
mô hình không tích cực sử dụng các trạng thái bộ
nhớ. Do đó, gradient norm phục vụ như một chỉ
báo về thời điểm module bộ nhớ được học. Đối với
MemBART large, hiệu suất của các tác vụ
downstream cải thiện sau 50.000 step khi gradient
norm đạt mức tối đa. Mẫu này cho thấy rằng cần
một số bước pre-training nhất định để module bộ
nhớ học cách ghi nhớ thông tin quan trọng, và mô
hình lớn cần nhiều bước cập nhật hơn để học ghi
nhớ.
5 Ứng dụng downstream
Trong phần này, chúng tôi giới thiệu các ứng dụng
downstream và bộ dữ liệu để đánh giá. Sau đó,
chúng tôi hiển thị kết quả trên các tác vụ hội thoại
và mô hình hóa ngôn ngữ.
5.1 Chi tiết bộ dữ liệu
Bộ dữ liệu #Lượt Độ dài TB Độ dài tối đa
PersonaChat 14.66 244 715
Persuasion 20.58 456 1,437
Multi-Session Chat 60.52 1,823 2,705
Arxiv - 13,409 156,605
PG19 - 105,830 1,181,156
Bảng 2: Thống kê bộ dữ liệu hội thoại và tài liệu dài.
Chúng tôi thí nghiệm trên ba bộ dữ liệu hội thoại
khác nhau: PersonaChat (Zhang et al., 2018),
PersuasionForGood (Wang et al., 2019), và Multi-
Session Chat (MSC) (Xu et al., 2022). Đặc biệt,
Multi-Session Chat giải quyết vấn đề thiếu các bộ
dữ liệu hội thoại ngữ cảnh dài trong cộng đồng
hiện tại. Đây là bộ dữ liệu người-người lớn nhất
cho các cuộc hội thoại dài với năm phiên và trung
bình 60 lượt phát ngôn. Để kiểm tra thêm khả năng
của mô hình, chúng tôi cũng đánh giá mô hình
trên hai tác vụ mô hình hóa ngôn ngữ: Arxiv và
PG19 (Rae et al., 2020). Do ràng buộc tính toán,
chúng tôi chọn 2.809 bài báo Arxiv CS AI, và một
tập con 200 cuốn sách từ PG19 để đánh giá. Chúng
tôi chia 10% dữ liệu cho testing. Thống kê của tất
cả các bộ dữ liệu được hiển thị trong Bảng 2.
Chúng tôi so sánh MemBART với GPT2, BART,
và Longformer, vì chúng đều là các mô hình ngôn
ngữ đã được pre-train.

--- TRANG 6 ---
Mô hình cơ bản Ngữ cảnh Độ trễ (ms) ↓Tổng↓Phiên 1 ↓Phiên 2 ↓Phiên 3 ↓Phiên 4 ↓Phiên 5 ↓
BART base 128 16.41 13.05 10.99 12.52 13.18 13.65 14.02
BART base 256 22.12 12.83 10.94 12.29 12.97 13.37 13.78
BART base 512 36.80 12.68 10.92 12.14 12.77 13.19 13.61
BART base 1,024 64.65 12.53 10.81 11.93 12.50 13.10 13.55
LED base 2,048 227.75 12.52 10.76 12.13 12.59 12.93 13.42
MemBART base (128) 128 20.42 12.41 10.72 11.95 12.52 12.88 13.23
MemBART base (128) 256 32.09 12.25 10.62 11.76 12.37 12.71 13.06
MemBART base (128) 512 66.70 12.15 10.63 11.67 12.23 12.57 12.97
Mô hình lớn Ngữ cảnh Độ trễ (ms) Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
GPT2-12 512 65.77 13.99 12.81 13.45 14.03 14.33 14.78
GPT2-12 1,024 149.05 13.56 12.82 13.48 13.84 13.53 13.82
GPT2-24 512 172.43 11.65 11.07 11.14 11.66 11.86 12.20
GPT2-24 1,024 395.84 11.56 11.03 11.12 11.52 11.75 12.11
BART large 128 45.37 10.61 9.50 10.13 10.68 10.94 11.29
BART large 256 63.79 10.37 9.38 9.86 10.44 10.67 11.02
BART large 512 103.20 10.23 9.44 9.71 10.26 10.52 10.85
BART large 1,024 190.79 10.10 9.41 9.64 10.06 10.36 10.68
LED large 2,048 655.19 10.05 9.43 9.60 10.04 10.27 10.60
MemBART large (128) 128 59.51 10.17 9.22 9.61 10.24 10.47 10.85
MemBART large (128) 256 102.42 10.09 9.20 9.65 10.09 10.38 10.72
MemBART large (128) 512 197.79 9.99 9.22 9.51 10.03 10.23 10.58
Bảng 3: Kết quả perplexity MSC trên tập test. MemBART có thể đạt được độ trễ thấp hơn trong khi có hiệu suất tốt hơn. Phiên 4 và phiên 5 chỉ tồn tại trong quá trình suy luận. * MemBART (128) có nghĩa là kích thước bộ nhớ là 128. Chi tiết thêm trong Phụ lục C
mô hình. Chúng tôi sử dụng beam search với kích
thước beam là 4 để sinh. Đối với các chỉ số đánh
giá, chúng tôi báo cáo perplexity và F1 overlap từ
cho bộ dữ liệu PersonaChat. Đối với các bộ dữ
liệu khác, chúng tôi chỉ báo cáo perplexity do đa
dạng phản hồi. Perplexity phản ánh khả năng của
ground truth và được chỉ ra có tương quan cao với
các chỉ số chất lượng hội thoại khác.
Mô hình Độ dài ngữ cảnh
128 256 512 1024
BART base 10.93 10.90 10.80 10.78
MemBART base (64) 10.69 10.66 10.66 -
w/o history 10.86 10.79 10.75 -
MemBART base (128) 10.65 10.57 10.56 -
MemBART base (256) 10.59 10.56 10.54 -
GPT2-12 10.51 10.38 10.33 10.31
GPT2-24 9.37 9.20 9.14 9.11
BART large 9.54 9.40 9.24 9.27
MemBART large (128) 9.34 9.18 9.12 -
Bảng 4: Kết quả perplexity ↓ cho bộ dữ liệu Persuasion. * MemBART (64) có nghĩa là kích thước bộ nhớ là 64.
5.2 Kết quả bộ dữ liệu hội thoại
Bảng 1,4,3 hiển thị kết quả cho PersonaChat,
PersuasionForGood, và MSC, tương ứng. Chúng
tôi liệt kê một số quan sát chính dưới đây.
Module bộ nhớ ghi nhớ thông tin lịch sử, và
pre-training là cần thiết. Trong Bảng 1, chúng tôi
cho thấy rằng bằng cách reset các trạng thái bộ nhớ
(w/o history), MemBART hoạt động tương tự như
BART base. Ngoài ra, không có pre-training,
module bộ nhớ không học được ban đầu để ghi
nhớ thông tin lịch sử.
MemBART có thể nhanh hơn nhiều với kích
thước ngữ cảnh đầu vào nhỏ trong khi có hiệu
suất tốt hơn. Trong PersonaChat, MemBART với
kích thước bộ nhớ 64 và độ dài ngữ cảnh 64 có
thể ngang bằng với hiệu suất của BART với độ
dài ngữ cảnh 512. Mẫu tương tự đúng cho
PersuasionForGood (Persuasion) và Multi-Session
Chat(MSC) dataset. Đặc biệt trong MSC, MemBART
base có thể đạt được perplexity tương tự (12.41)
so với LED base với độ dài ngữ cảnh 2.048, nhưng
nhanh hơn 11.15 lần. MemBART large đạt được
perplexity tương tự (10.09) so với LED large với
độ dài ngữ cảnh 2.048, trong khi nhanh hơn 6.40
lần.
Các mô hình encoder-decoder sử dụng thông
tin lịch sử tốt hơn các mô hình chỉ có decoder.
Đối với PersonaChat và MSC, BART base và
MemBART large vượt trội hơn GPT2-12 và GPT2-
24 tương ứng. Ngoại lệ là trong Persuasion, nơi
các cuộc hội thoại chứa nhiều phát ngôn một lượt
hơn. Quan sát này cho thấy rằng các mô hình
encoder-decoder sử dụng thông tin lịch sử tốt hơn,
và có lẽ là do ngữ cảnh hai chiều.
Hiệu suất của MemBART cải thiện khi kích
thước ngữ cảnh tăng. Hiệu suất của BART và GPT2
cải thiện khi kích thước ngữ cảnh tăng. Kết quả
cho thấy rằng tăng kích thước ngữ cảnh cho
MemBART cũng có thể cải thiện hiệu suất của nó,
mặc dù chỉ với một biên độ nhỏ. Chúng tôi nghi
ngờ rằng sử dụng kích thước ngữ cảnh lớn hơn có
thể giúp mô hình tăng cường việc ghi nhớ thông
tin lịch sử và giảm bớt các tình huống mà một số
thông tin không được giữ trong bộ nhớ.
Tăng kích thước bộ nhớ cải thiện hiệu suất
MemBART. Đối với các mô hình MemBART, thông
tin lịch sử được lưu trữ bên trong bộ nhớ. Do đó,
chúng tôi muốn nghiên cứu hiệu suất thay đổi như
thế nào với kích thước bộ nhớ. Chúng tôi đánh giá
kích thước bộ nhớ 64, 128, và 256. Chúng tôi quan
sát thấy rằng khi tăng kích thước bộ nhớ từ 64 lên
128, có một cải thiện lớn, nhưng từ 128 lên 256,
cải thiện là nhỏ.
5.3 Kết quả bộ dữ liệu mô hình hóa ngôn ngữ
Mô hình Ngữ cảnh Arxiv PG19
BART base 512 15.40 33.70
BART base 1,024 15.09 31.20
LED base 2,048 13.97 30.08
MemBART base (128) 512 14.34 29.81
GPT2-12 512 17.53 32.20
GPT2-12 1,024 15.35 28.31
GPT2-24 512 15.34 22.33
GPT2-24 1,024 13.84 20.86
BART large 512 12.92 24.08
BART large 1,024 12.31 23.07
LED large 2,048 11.82 23.04
MemBART large (128) 512 12.24 22.26
Bảng 5: Điểm perplexity mô hình hóa ngôn ngữ trên
các bộ dữ liệu Arxiv và PG19. Thấp hơn là tốt hơn.
Chúng tôi cũng đã đánh giá trên hai tác vụ mô
hình hóa ngôn ngữ Arxiv và PG19 để hiểu rõ hơn
về hiệu quả của mô hình. Do ràng buộc tính toán,
chúng tôi sử dụng các tập con của hai bộ dữ liệu
để đánh giá. Chúng tôi hiển thị kết quả trong
Bảng 5.
MemBART hoạt động hơi tồi hơn LED large với
ngữ cảnh 2048 trên Arxiv, nhưng tốt hơn trên
PG19. Chúng tôi nghi ngờ rằng đó là vì các bài
báo Arxiv rất có cấu trúc và sử dụng thuật ngữ
xuyên suốt bài báo, nhưng sách PG19 có ít phụ
thuộc dài hạn hơn. Mẫu hiệu suất tương tự cũng
có thể được quan sát giữa BART và GPT, cho thấy
rằng các mô hình encoder tốt hơn trong việc sử
dụng thông tin dài hạn, và các mô hình decoder
tốt hơn trong thông tin ngắn hạn.
5.4 Nghiên cứu ablation
Hình 5: Ảnh hưởng của việc thay đổi kích thước bộ
nhớ (trái) và time horizon (phải).
Chúng tôi cũng đánh giá ảnh hưởng của việc thay
đổi kích thước bộ nhớ và time horizon lan truyền
ngược trên bộ dữ liệu PersonaChat với độ dài ngữ
cảnh 64. Khi thay đổi kích thước bộ nhớ, chúng tôi
đặt time horizon là 5. Trong Hình 5, tăng kích
thước bộ nhớ có cải thiện đáng kể cho perplexity
cho đến khi nó đạt 128. Khi thay đổi time horizon,
kích thước bộ nhớ được đặt là 128. Trong hình
bên phải, time horizon là 1 (gradient không thể
truyền qua bộ nhớ) đạt được hiệu suất tốt hơn
BART, cho thấy rằng bộ nhớ sau pre-training có
thể nắm bắt thông tin lịch sử. Tăng time horizon
lên 2 có thể cải thiện đáng kể hiệu suất.
6 Kết luận
Tóm lại, chúng tôi đã giới thiệu một mô hình
Transformer encoder-decoder tăng cường bộ nhớ
có trạng thái mới có thể bảo toàn lịch sử hội thoại
dài trong khi tương thích với các mô hình encoder-
decoder đã được pre-train. Bằng cách kết hợp một
module bộ nhớ riêng biệt với dòng attention kép
và cơ chế cổng dư, mô hình của chúng tôi trao đổi
thông tin hiệu quả giữa các trạng thái bộ nhớ và
transformer đã được pre-train. Kết quả thí nghiệm
đã chứng minh tính ưu việt của phương pháp của
chúng tôi về hiệu quả và hiệu suất, khi so sánh
với các mô hình đã được pre-train khác như BART,
GPT, và Longformer (LED).
Đối với công việc trong tương lai, chúng tôi dự
định mở rộng tính tương thích của phương pháp
với các mô hình đã được pre-train khác, và đánh
giá hiệu suất của nó trên các tác vụ khác như hệ
thống hội thoại hướng tác vụ, tóm tắt văn bản,
và phân loại tài liệu dài. Ngoài ra, chúng tôi sẽ
nghiên cứu các biểu diễn bộ nhớ tiên tiến hơn để
tối ưu hóa thêm hiệu quả của mô hình hiện tại.

--- TRANG 7 ---
Tài liệu tham khảo
Daniel Adiwardana, Minh-Thang Luong, David R. So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
and Quoc V. Le. 2020. Towards a human-like open-
domain chatbot. CoRR, abs/2001.09977.
Thomas Bachlechner, Bodhisattwa Prasad Majumder,
Huanru Henry Mao, Gary Cottrell, and Julian J.
McAuley. 2021. Rezero is all you need: fast con-
vergence at large depth. In Proceedings of the Thirty-
Seventh Conference on Uncertainty in Artificial In-
telligence, UAI 2021, Virtual Event, 27-30 July 2021,
volume 161 of Proceedings of Machine Learning
Research, pages 1352–1361. AUAI Press.
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. CoRR,
abs/2004.05150.
Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019. Generating long sequences with
sparse transformers. CoRR, abs/1904.10509.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-
bonell, Quoc Viet Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
beyond a fixed-length context. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages
2978–2988. Association for Computational Linguis-
tics.
Aaron Gokaslan and Vanya Cohen. 2019. Open-
webtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus.
Carl Gold. 2003. FX trading via recurrent reinforcement
learning. In 2003 IEEE International Conference on
Computational Intelligence for Financial Engineer-
ing, CIFEr 2003, Hong Kong, March 20-23, 2003,
pages 363–370. IEEE.
Alex Graves, Greg Wayne, Malcolm Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gomez Colmenarejo, Edward
Grefenstette, Tiago Ramalho, John P. Agapiou,
Adrià Puigdomènech Badia, Karl Moritz Hermann,
Yori Zwols, Georg Ostrovski, Adam Cain, Helen
King, Christopher Summerfield, Phil Blunsom, Ko-
ray Kavukcuoglu, and Demis Hassabis. 2016. Hybrid
computing using a neural network with dynamic ex-
ternal memory. Nat., 538(7626):471–476.
Matthew J. Hausknecht and Peter Stone. 2015. Deep
recurrent q-learning for partially observable mdps. In
2015 AAAI Fall Symposia, Arlington, Virginia, USA,
November 12-14, 2015, pages 29–37. AAAI Press.
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V.
Le. 2022. Transformer quality in linear time. CoRR,
abs/2202.10447.
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan
Dyer, and Behnam Neyshabur. 2022. Block-recurrent
transformers. CoRR, abs/2203.07852.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear
attention. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event, volume 119 of Proceedings
of Machine Learning Research, pages 5156–5165.
PMLR.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020, pages 7871–7880.
Association for Computational Linguistics.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings. Open-
Review.net.
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yun-
shen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong,
and Yiran Zhong. 2022. cosformer: Rethinking soft-
max in attention. CoRR, abs/2202.08791.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-
pressive transformers for long-range sequence mod-
elling. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21:140:1–140:67.
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Eric Michael Smith, Y-Lan Boureau, and Jason We-
ston. 2021. Recipes for building an open-domain
chatbot. In Proceedings of the 16th Conference of
the European Chapter of the Association for Com-
putational Linguistics: Main Volume, EACL 2021,
Online, April 19 - 23, 2021, pages 300–325. Associa-
tion for Computational Linguistics.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and
Rob Fergus. 2015. End-to-end memory networks. In
Advances in Neural Information Processing Systems

--- TRANG 8 ---
28: Annual Conference on Neural Information Pro-
cessing Systems 2015, December 7-12, 2015, Mon-
treal, Quebec, Canada, pages 2440–2448.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pages 5998–6008.
Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,
Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-
suasion for good: Towards a personalized persuasive
dialogue system for social good. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 5635–5649, Florence,
Italy. Association for Computational Linguistics.
Jason Weston, Sumit Chopra, and Antoine Bordes. 2015.
Memory networks. In 3rd International Conference
on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceed-
ings.
Ronald J. Williams and Jing Peng. 1990. An efficient
gradient-based algorithm for on-line training of recur-
rent network trajectories. Neural Comput., 2(4):490–
501.
Qingyang Wu, Zhenzhong Lan, Jing Gu, and Zhou Yu.
2020. Memformer: The memory-augmented trans-
former. CoRR, abs/2010.06891.
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
and Christian Szegedy. 2022. Memorizing transform-
ers. In International Conference on Learning Repre-
sentations.
Jing Xu, Arthur Szlam, and Jason Weston. 2022. Be-
yond goldfish memory: Long-term open-domain con-
versation. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 5180–5197, Dublin,
Ireland. Association for Computational Linguistics.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tañón, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big bird: Trans-
formers for longer sequences. In Advances in Neural
Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual.
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you have
pets too? In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2018, Melbourne, Australia, July 15-20, 2018,
Volume 1: Long Papers, pages 2204–2213. Associa-
tion for Computational Linguistics.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2020. DIALOGPT: Large-scale
generative pre-training for conversational response
generation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, ACL 2020, Online, July 5-10,
2020, pages 270–278. Association for Computational
Linguistics.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watching
movies and reading books. In 2015 IEEE Interna-
tional Conference on Computer Vision, ICCV 2015,
Santiago, Chile, December 7-13, 2015, pages 19–27.
IEEE Computer Society.

--- TRANG 9 ---
A Các biến thể mô hình khác nhau
Chúng tôi đánh giá các biến thể mô hình khác nhau
để chọn mô hình có hiệu quả bộ nhớ tốt nhất. Chúng
tôi chọn tác vụ nhớ lại văn bản để đánh giá. Tác
vụ được xây dựng như nhớ lại đoạn văn bản trước
đó. Giả sử chúng ta có một tài liệu chia thành các
đoạn văn bản x0, x1, . . . , xt. Encoder nhận đầu
vào xt tại timestep t. Decoder cần dự đoán xt−1.
Theo cách này, bộ nhớ phải nén thông tin trước đó
vào bộ nhớ.
Memformer Mô hình đầu tiên là áp dụng trực tiếp
Memformer bằng cách thêm các layer memory
cross-attention vào BART. Layer cross-attention
nằm giữa layer attention và layer MLP. Dưới đây
là công thức đơn giản hóa không hiển thị
normalization:
Hl=Hl+Attn(Hl)
Hl=Hl+CrossAttn (Hl, Mt)
Hl=Hl+MLP(Hl)
Memformer + ReZero sử dụng ReZero (Bachlech-
ner et al., 2021) bằng cách thêm một trọng số có
thể huấn luyện được khởi tạo bằng zero α khi thêm
layer memory cross-attention, và do đó phân phối
đầu ra của mô hình sẽ được cập nhật một cách
mượt mà.
Hl=Hl+Attn(Hl)
Hl=Hl+αCrossAttn (Hl, Mt)
Hl=Hl+MLP(Hl)
MemBART + Shared weights Một biến thể trực
tiếp của phương pháp của chúng tôi là chia sẻ
trọng số giữa module bộ nhớ và Transformer đã
được pre-train. Điều này tương tự như nối các
embedding prompting có thể huấn luyện vào input
sequence.
MemBART là phương pháp đề xuất của chúng
tôi. Sự khác biệt chính so với Memformer là module
bộ nhớ, nơi việc đọc và ghi bộ nhớ được xử lý với
một Transformer riêng biệt. Luồng thông tin giữa
module bộ nhớ và Transformer đã được pre-train
được thực hiện bởi dòng attention kép để ảnh hưởng
tối thiểu đến phân phối mô hình gốc.
Các siêu tham số huấn luyện chi tiết được hiển
thị trong Bảng 6. Time horizon lan truyền ngược
được đặt là 2 vì nó đủ cho tác vụ này. Quá trình
huấn luyện mất khoảng ít hơn 12 giờ để hoàn thành
trên một GPU A6000.
Siêu tham số Tất cả mô hình
Encoder Layers 6
Decoder Layers 6
Hidden size 768
Attention heads 12
Memory size 32
Context length 512
Batch size 8
Warm-up steps 1k
Learning rate 3e-5
Time horizon 2
Dropout 0.0
Weight decay 0.01
Maximum Update steps 100k
Bảng 6: Siêu tham số cho tác vụ nhớ lại văn bản.
B Chi tiết Sequence Denoising Pre-training
Như đã đề cập, chúng tôi sử dụng cùng mục tiêu
huấn luyện như BART. Ngoài ra, corpus pre-training
được chọn tương tự như BART. Vì mô hình của
chúng tôi dựa nhiều vào BART, chúng tôi sử dụng
cùng tokenization như BART. Chúng tôi lọc bỏ
các tài liệu ngắn hơn 512 token. Mỗi tài liệu được
chia thành các đoạn với kích thước cửa sổ 512 và
overlap 128 token.
Siêu tham số MemBART-base MemBART-large
Encoder Layers 6 12
Decoder Layers 6 12
Hidden size 768 1024
Attention heads 12 16
Context length 512 512
Stride 128 128
mask ratio 0.3 0.3
permutation ratio 0.0 0.0
replace length 1 1
Batch size 32 32
Warm-up steps 5k 5k
Learning rate 3e-5 1e-5
Time horizon 6 6
Dropout 0.0 0.0
Weight decay 0.01 0.01
Update steps 100k 100k
Bảng 7: Siêu tham số để huấn luyện MemBART-base
và MemBART-large.
Chúng tôi pre-train các mô hình với các siêu tham
số được hiển thị trong Bảng 7. Pre-training cho
MemBART-base mất khoảng 4 ngày trên bốn GPU
A6000. Pre-training cho MemBART-large mất khoảng
8 ngày trên bốn GPU A6000.

--- TRANG 10 ---
B.1 Xử lý batch và điều phối
Hình 6: Minh họa về cách các tài liệu hoặc hội thoại
được xử lý và đóng batch.
Vì các batch phụ thuộc thời gian trong paradigm
của chúng tôi, chúng tôi triển khai một bộ điều
phối batch để xử lý hiệu quả các tài liệu và hội
thoại như được hiển thị trong Hình 6. Trong
paradigm này, một số agent có kích thước bằng
batch size chia sẻ cùng một queue dữ liệu để lấy
tài liệu. Khi hoàn thành xử lý một tài liệu, agent
lấy một tài liệu mới từ queue chia sẻ, và nó chia
tài liệu thành các đoạn văn bản hoặc phát ngôn để
xuất ra một đầu vào ngữ cảnh tại mỗi timestep.
Agent cũng xử lý tín hiệu reset và padding token
khi các tài liệu có độ dài khác nhau. Tất cả các
agent được đồng bộ hóa, và batch được thu thập
tại mỗi timestep. Paradigm này đơn giản hóa việc
bảo toàn thứ tự thời gian trong các batch và sự
căn chỉnh giữa các tài liệu hoặc hội thoại có độ
dài khác nhau. Chúng tôi sử dụng bộ điều phối
batch này trong tất cả các thí nghiệm của mình.
C Thí nghiệm đầy đủ Multi-Session Chat
Chúng tôi đã hiển thị các thí nghiệm đầy đủ trên
multi-session chat dưới các cài đặt khác nhau.
Độ trễ được đo với các đầu vào giả. Chúng tôi báo
cáo trung bình của 10 lần chạy và variance tương
ứng. Chúng tôi chọn các mô hình tốt nhất dựa trên
tập validation và sau đó đánh giá chúng trên tập
test. Kết quả validation được hiển thị trong Bảng
9. Kết quả test được hiển thị trong Bảng 10.
Một quan sát là Longformer sẽ pad sequence
thành bội số của 1.024 do cơ chế attention thưa
thớt. Hành vi này dẫn đến hiệu suất rất chậm khi
kích thước ngữ cảnh nhỏ.
Một quan sát khác là đối với các phiên sau, đặc
biệt là Phiên 4 và 5, thông tin lịch sử quan trọng.
Đối với Phiên 5, BART base có 4.5% mất hiệu
suất khi kích thước ngữ cảnh bị cắt ngắn xuống
128. BART large có 6.5% mất hiệu suất do cắt
ngắn. Ngược lại, vì MemBART có bộ nhớ, sự khác
biệt hiệu suất nhỏ hơn khi sử dụng các kích thước
ngữ cảnh khác nhau.
D Số lượng tham số
Mô hình #Tham số
BART base 139M
MemBART base 183M
BART large 406M
MemBART large 558M
Bảng 8: Số lượng tham số của BART và MemBART.
Chúng tôi hiển thị số lượng tham số của BART
và MemBART trong Bảng 8. Vì MemBART kết hợp
module bộ nhớ bổ sung. Nó hơi lớn hơn mô hình
BART tương ứng. Nhưng như một sự đánh đổi,
MemBART nhanh hơn nhiều so với BART.
E Huấn luyện hiệu quả bộ nhớ GPU
Memformer đề xuất một biến thể của gradient
checkpointing để huấn luyện hiệu quả loại mô hình
có trạng thái này. Việc tiêu thụ bộ nhớ GPU tăng
tuyến tính với time horizon lan truyền ngược vì
nó yêu cầu mở rộng đồ thị tính toán bằng số
timestep.
Chúng tôi đã áp dụng thuật toán lan truyền ngược
hiệu quả này cho mô hình MemBART large với
time horizon 6. Không có phương pháp lan truyền
ngược hiệu quả, nó sẽ tiêu thụ một lượng lớn bộ
nhớ GPU, điều này khiến việc huấn luyện không
khả thi.

--- TRANG 11 ---
Mô hình cơ bản Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
BART base 128 16.41±0.73 12.72 10.84 13.19 13.15 13.17 12.77
BART base 256 22.12±0.89 12.50 10.77 12.85 12.89 12.96 12.58
BART base 512 36.80±1.17 12.33 10.71 12.61 12.67 12.81 12.43
BART base 1,024 64.65±0.72 12.22 10.69 12.46 12.38 12.77 12.38
Longformer base 256 110.07±0.28 12.55 10.78 12.92 12.93 13.07 12.57
Longformer base 512 113.73±3.16 12.35 10.73 12.64 12.66 12.87 12.40
Longformer base 1,024 115.96±0.25 12.20 10.67 12.55 12.46 12.65 12.26
Longformer base 2,048 227.75±0.13 12.16 10.69 12.54 12.46 12.58 12.15
MemBART base (64) 128 17.23±1.19 12.17 10.6 12.60 12.54 12.55 12.14
MemBART base (64) 256 29.39±0.73 12.06 10.59 12.40 12.36 12.47 12.09
MemBART base (64) 512 59.73±0.66 11.95 10.57 12.28 12.22 12.33 11.98
MemBART base (128) 128 20.42±1.47 12.12 10.6 12.50 12.45 12.51 12.14
MemBART base (128) 256 32.09±0.18 11.96 10.49 12.29 12.28 12.37 11.97
MemBART base (128) 512 66.70±1.83 11.86 10.50 12.15 12.14 12.27 11.89
MemBART base (256) 128 26.56±0.57 12.11 10.58 12.51 12.43 12.47 12.13
MemBART base (256) 256 40.92±0.63 12.00 10.50 12.35 12.34 12.40 12.01
MemBART base (256) 512 75.54±0.14 11.83 10.47 12.11 12.10 12.24 11.86
Mô hình lớn Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
GPT2-12 128 16.24±1.13 14.17 12.87 14.57 14.5 14.51 14.03
GPT2-12 256 30.80±0.48 13.91 12.70 14.20 14.23 14.25 13.81
GPT2-12 512 65.77±0.74 13.76 12.68 14.03 14.02 14.11 13.67
GPT2-12 1,024 149.05±0.38 13.33 12.66 14.04 13.82 13.26 12.71
GPT2-24 128 42.39±2.50 11.91 11.15 12.17 12.10 12.10 11.83
GPT2-24 256 81.80±0.18 11.66 10.98 11.83 11.83 11.86 11.62
GPT2-24 512 172.43±0.12 11.52 10.99 11.63 11.64 11.72 11.48
GPT2-24 1,024 395.84±0.64 11.43 10.96 11.59 11.48 11.62 11.37
BART large 128 45.37±1.31 10.42 9.31 10.75 10.61 10.68 10.44
BART large 256 63.79±0.40 10.15 9.17 10.35 10.34 10.40 10.20
BART large 512 103.20±2.40 10.00 9.22 10.12 10.12 10.28 10.03
BART large 1,024 190.79±0.29 9.87 9.20 10.03 9.91 10.09 9.90
Longformer large 256 316.42±2.37 10.25 9.28 10.43 10.41 10.55 10.30
Longformer large 512 322.68±1.74 10.06 9.24 10.18 10.15 10.38 10.13
Longformer large 1,024 334.87±5.54 9.90 9.20 10.06 9.95 10.15 9.92
Longformer large 2,048 655.19±5.25 9.87 9.23 10.09 9.90 10.04 9.89
MemBART large (128) 128 59.51±0.91 9.99 9.17 10.19 10.14 10.22 10.02
MemBART large (128) 256 102.42±2.07 9.92 9.08 10.10 10.06 10.15 9.95
MemBART large (128) 512 197.79±4.85 9.79 9.08 9.90 9.88 10.03 9.84
Bảng 9: Kết quả Multi-Session Chat trên tập validation.

--- TRANG 12 ---
Mô hình cơ bản Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
BART base 128 16.41±0.73 13.05 10.99 12.52 13.18 13.65 14.02
BART base 256 22.12±0.89 12.83 10.94 12.29 12.97 13.37 13.78
BART base 512 36.80±1.17 12.68 10.92 12.14 12.77 13.19 13.61
BART base 1,024 64.65±0.72 12.53 10.81 11.93 12.50 13.10 13.55
Longformer base 256 110.07±0.28 12.87 10.78 12.36 13.02 13.45 13.88
Longformer base 512 113.73±3.16 12.69 10.77 12.19 12.79 13.22 13.67
Longformer base 1,024 115.96±0.25 12.55 10.74 12.12 12.59 13.02 13.48
Longformer base 2,048 227.75±0.13 12.52 10.76 12.13 12.59 12.93 13.42
MemBART base (64) 128 17.23±1.19 12.42 10.72 11.95 12.52 12.93 13.23
MemBART base (64) 256 29.39±0.73 12.34 10.66 11.86 12.46 12.84 13.16
MemBART base (64) 512 59.73±0.66 12.23 10.66 11.78 12.32 12.66 13.02
MemBART base (128) 128 20.42±1.47 12.41 10.72 11.95 12.52 12.88 13.23
MemBART base (128) 256 32.09±0.18 12.25 10.62 11.76 12.37 12.71 13.06
MemBART base (128) 512 66.70±1.83 12.15 10.63 11.67 12.23 12.57 12.97
MemBART base (256) 128 26.56±0.57 12.38 10.67 11.90 12.51 12.86 13.20
MemBART base (256) 256 40.92±0.63 12.25 10.59 11.76 12.38 12.74 13.07
MemBART base (256) 512 75.54±0.14 12.09 10.57 11.62 12.18 12.53 12.90
Mô hình lớn Ngữ cảnh Độ trễ Tổng Phiên 1 Phiên 2 Phiên 3 Phiên 4 Phiên 5
GPT2-12 128 16.24±1.13 14.36 12.91 13.80 14.43 14.79 15.22
GPT2-12 256 30.80±0.48 14.13 12.80 13.57 14.21 14.53 14.93
GPT2-12 512 65.77±0.74 13.99 12.81 13.45 14.03 14.33 14.78
GPT2-12 1,024 149.05±0.38 13.56 12.82 13.48 13.84 13.53 13.82
GPT2-24 128 42.39±2.50 12.03 11.17 11.52 12.07 12.30 12.62
GPT2-24 256 81.80±0.18 11.78 11.02 11.28 11.82 12.04 12.36
GPT2-24 512 172.43±0.12 11.65 11.07 11.14 11.66 11.86 12.20
GPT2-24 1,024 395.84±0.64 11.56 11.03 11.12 11.52 11.75 12.11
BART large 128 45.37±1.31 10.61 9.50 10.13 10.68 10.94 11.29
BART large 256 63.79±0.40 10.37 9.38 9.86 10.44 10.67 11.02
BART large 512 103.20±2.40 10.23 9.44 9.71 10.26 10.52 10.85
BART large 1,024 190.79±0.29 10.10 9.41 9.64 10.06 10.36 10.68
Longformer large 256 316.42±2.37 10.43 9.34 9.95 10.52 10.75 11.11
Longformer large 512 322.68±1.74 10.28 9.37 9.77 10.32 10.57 10.92
Longformer large 1,024 334.87±5.54 10.13 9.42 9.66 10.11 10.38 10.72
Longformer large 2,048 655.19±5.25 10.05 9.43 9.60 10.04 10.27 10.60
MemBART large (128) 128 59.51±0.91 10.17 9.22 9.61 10.24 10.47 10.85
MemBART large (128) 256 102.42±2.07 10.09 9.20 9.65 10.09 10.38 10.72
MemBART large (128) 512 197.79±4.85 9.99 9.22 9.51 10.03 10.23 10.58
Bảng 10: Kết quả Multi-Session Chat trên tập test.
