# 2308.16692.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/audio/2308.16692.pdf
# Kích thước tệp: 1300801 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
SPEECH TOKENIZER : BỘ TOKEN HÓA GIỌNG NÓI THỐNG NHẤT CHO
CÁC MÔ HÌNH NGÔN NGỮ GIỌNG NÓI
Xin Zhang∗, Dong Zhang∗, Shimin Li, Yaqian Zhou†, Xipeng Qiu†
Trường Khoa học Máy tính, Đại học Fudan
Phòng thí nghiệm chính xử lý thông tin thông minh Shanghai, Đại học Fudan
{xin_zhang22,dongzhang22}@m.fudan.edu.cn
{smli20,zhouyaqian,xpqiu}@fudan.edu.cn
https://0nutation.github.io/SpeechTokenizer.github.io/
TÓM TẮT
Các mô hình ngôn ngữ lớn giọng nói hiện tại được xây dựng dựa trên các biểu diễn giọng nói rời rạc,
có thể được phân loại thành token ngữ nghĩa và token âm thanh. Tuy nhiên,
các token giọng nói hiện có không được thiết kế đặc biệt cho việc mô hình hóa ngôn ngữ giọng nói.
Để đánh giá sự phù hợp của các token giọng nói cho việc xây dựng các mô hình ngôn ngữ giọng nói,
chúng tôi đã thiết lập benchmark đầu tiên, SLMTokBench. Kết quả của chúng tôi cho thấy
rằng cả token ngữ nghĩa và token âm thanh đều không lý tưởng cho mục đích này. Do đó, chúng tôi
đề xuất SpeechTokenizer, một bộ token hóa giọng nói thống nhất cho các mô hình ngôn ngữ giọng nói lớn.
SpeechTokenizer áp dụng kiến trúc Encoder-Decoder với lượng tử hóa vector dư (RVQ).
Thống nhất các token ngữ nghĩa và âm thanh, SpeechTokenizer tách biệt các khía cạnh khác nhau
của thông tin giọng nói một cách phân tầng qua các lớp RVQ khác nhau. Hơn nữa, chúng tôi xây dựng
một Mô hình Ngôn ngữ Giọng nói Thống nhất (USLM) tận dụng SpeechTokenizer. Các thí nghiệm
cho thấy SpeechTokenizer hoạt động tương đương với EnCodec trong việc tái tạo giọng nói và thể hiện
hiệu suất mạnh mẽ trên benchmark SLMTokBench. Ngoài ra, USLM vượt trội hơn V ALL-E trong
các tác vụ Text-to-Speech không cần mẫu. Mã nguồn và mô hình có sẵn tại
https://github.com/ZhangXInFD/SpeechTokenizer/ .
1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (OpenAI, 2023; Touvron et al., 2023) đã thể hiện hiệu suất đáng kể
trên các tác vụ xử lý ngôn ngữ tự nhiên khác nhau. Điều này đã truyền cảm hứng cho nhiều công trình
xây dựng các mô hình ngôn ngữ giọng nói (Borsos et al., 2022), đã đạt được những đột phá đáng kể
qua các tác vụ xử lý giọng nói khác nhau (Wang et al., 2023; Zhang et al., 2023; Rubenstein et al., 2023;
Dong et al., 2023). Một điểm chung quan trọng giữa các công trình này là việc sử dụng các biểu diễn
giọng nói rời rạc. Các biểu diễn giọng nói rời rạc hiện tại có thể được phân loại thành hai loại: token
ngữ nghĩa và token âm thanh (Borsos et al., 2022). Token ngữ nghĩa thường từ các mô hình được huấn luyện
trước tự giám sát với mô hình hóa ngôn ngữ có mặt nạ làm mục tiêu huấn luyện (Hsu et al., 2021; Baevski
et al., 2020; Chung et al., 2021). Được tạo ra thông qua phân cụm k-means trên các biểu diễn từ một
lớp trung gian cụ thể, các token ngữ nghĩa được mô tả như các chuỗi với cấu trúc một chiều.
Token âm thanh có thể được trích xuất từ các codec âm thanh thần kinh với việc tái tạo làm mục tiêu
huấn luyện (Zeghidour et al., 2021; Défossez et al., 2022). Sử dụng lượng tử hóa vector dư (RVQ) với
các bộ lượng tử hóa phân tầng để rời rạc hóa, các token âm thanh được biểu diễn như các ma trận
gồm hai chiều: bước thời gian và bộ lượng tử hóa.
Dựa trên hai token giọng nói, có ba phương pháp mô hình hóa cho các mô hình ngôn ngữ giọng nói,
như liệt kê trong Bảng 1: i) Các mô hình ngôn ngữ ngữ nghĩa được xây dựng sử dụng các token ngữ nghĩa
và sử dụng một bộ tổng hợp âm đơn vị bên ngoài cho việc tổng hợp giọng nói. (Lakhotia et al., 2021; Zhang et al., 2023; Hassid
et al., 2023). Mặc dù bắt được nội dung chính xác về mặt ngữ nghĩa, việc tạo giọng nói của chúng dẫn đến chất lượng kém
∗Đóng góp bằng nhau. Thứ tự ngẫu nhiên
†Tác giả liên hệ
1arXiv:2308.16692v2  [cs.CL]  23 Jan 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
ARNAR concat
Decoder
TQ
HuBERT
W2VBERT
...SoundStream
Encodec
...SpeechTokenizer
Nội dung Âm sắc, ...Token ngữ nghĩa Token âm thanh Token thống nhất
Q
TQ
T
Nội dung, Âm sắc, ...(a) Các Token Giọng nói Khác nhau (b) Mô hình Ngôn ngữ Giọng nói Thống nhất
Hình 1: Trái: Minh họa thành phần thông tin của các biểu diễn giọng nói rời rạc khác nhau.
Phải: Minh họa các mô hình ngôn ngữ giọng nói thống nhất. AR đề cập đến tự hồi quy và NAR đề cập đến
không tự hồi quy. Các token giọng nói được biểu diễn dưới dạng các vòng tròn màu và các màu khác nhau
đại diện cho thông tin khác nhau.
Nội dung Chính xác Giọng nói Chất lượng cao Token hóa Đơn lẻ
Semantic LM√×√
Acoustic LM ×√ √
Hierarchical LM√ √×
USLM (của chúng tôi)√ √ √
Bảng 1: So sánh giữa các mô hình ngôn ngữ giọng nói khác nhau. Semantic LM đề cập đến các mô hình
ngôn ngữ ngữ nghĩa. Acoustic LM đề cập đến các mô hình ngôn ngữ âm thanh. Hierarchical LM đề cập đến
các mô hình ngôn ngữ giọng nói phân tầng. USLM đề cập đến mô hình ngôn ngữ giọng nói thống nhất của chúng tôi.
chất lượng và mất mát các chi tiết âm thanh. ii) Các mô hình ngôn ngữ âm thanh được xây dựng trên
các token âm thanh. Lấy V ALL-E (Wang et al., 2023) làm ví dụ, mặc dù đạt được khả năng text-to-
speech (TTS) không cần mẫu ấn tượng, nó vẫn gặp các vấn đề như nội dung không chính xác, do
thông tin phức tạp trong các token âm thanh. iii) Các mô hình ngôn ngữ giọng nói phân tầng bao gồm
các mô hình ngôn ngữ token ngữ nghĩa và các mô hình ngôn ngữ token âm thanh, bắt được thông tin nội dung
và các chi tiết âm thanh tương ứng (Borsos et al., 2022; Rubenstein et al., 2023; Dong et al., 2023).
Cấu trúc này cho thấy kết quả hứa hẹn trong cả nội dung và chất lượng giọng nói, nhưng phương pháp
mô hình hóa đa giai đoạn phức tạp hơn, dẫn đến một số nhược điểm như tích lũy lỗi và tốc độ xử lý
chậm hơn. Ngoài ra, có sự dư thừa thông tin đáng kể giữa các token ngữ nghĩa và các token âm thanh,
điều này tạo ra sự phức tạp mô hình hóa không cần thiết. Một mô hình ngôn ngữ giọng nói lý tưởng
không chỉ nên mô hình hóa nội dung một cách chính xác, mà còn tạo ra giọng nói đa dạng, chất lượng cao,
trong khi duy trì một kiến trúc đơn giản thanh lịch. Tương ứng, các token giọng nói lý tưởng nên đáp ứng
hai đặc điểm chính sau: i) Sự liên kết mạnh mẽ với văn bản; ii) Bảo tồn hiệu quả thông tin giọng nói.

Tuy nhiên, các token giọng nói hiện có không được thiết kế rõ ràng cho việc mô hình hóa ngôn ngữ giọng nói,
và chưa có khảo sát nào về sự phù hợp của chúng để xây dựng các mô hình ngôn ngữ giọng nói. Để giải quyết
khoảng trống này, chúng tôi xây dựng Benchmark Token Mô hình Ngôn ngữ Giọng nói, để đánh giá sự
phù hợp của các token giọng nói cho việc xây dựng các mô hình ngôn ngữ giọng nói. Đánh giá của chúng tôi
tiết lộ rằng các token ngữ nghĩa thể hiện sự liên kết cao với văn bản trong khi mất một số thông tin trong
giọng nói, như âm sắc. Các token âm thanh xuất sắc trong việc bảo tồn thông tin giọng nói một cách hiệu quả
nhưng không thể hiện sự liên kết mạnh mẽ với văn bản. Với những quan sát này, chúng tôi nhằm mục đích
xây dựng các token giọng nói chuyên biệt được thiết kế cho các mô hình ngôn ngữ giọng nói bằng cách
thống nhất các token ngữ nghĩa và âm thanh. Cụ thể, chúng tôi có thể tiến hành tách biệt thông tin trong
cấu trúc RVQ của các token âm thanh, cho phép bộ lượng tử hóa RVQ đầu tiên tạo ra các token chứa
thông tin nội dung, tương tự như các token ngữ nghĩa, trong khi các bộ lượng tử hóa tiếp theo bổ sung
thông tin ngoại ngôn ngữ còn lại, như minh họa trong Hình 1.

Với động lực trên, chúng tôi đề xuất SpeechTokenizer, một bộ token hóa giọng nói thống nhất cho
các mô hình ngôn ngữ giọng nói lớn. SpeechTokenizer áp dụng kiến trúc Encoder-Decoder với lượng tử hóa
vector dư. Thống nhất các token ngữ nghĩa và âm thanh, SpeechTokenizer tách biệt các khía cạnh khác nhau
của thông tin giọng nói một cách phân tầng qua các lớp RVQ khác nhau. Bằng cách sử dụng một giáo viên
ngữ nghĩa để hướng dẫn bộ lượng tử hóa RVQ đầu tiên, các token lớp đầu tiên có thể bắt được thông tin
nội dung một cách hiệu quả. Với cấu trúc dư, các bộ lượng tử hóa tiếp theo bổ sung thông tin ngoại ngôn ngữ
còn lại.
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Dựa trên SpeechTokenizer, chúng tôi xây dựng một Mô hình Ngôn ngữ Giọng nói Thống nhất bao gồm
các mô hình tự hồi quy và không tự hồi quy. Kết quả thí nghiệm cho thấy SpeechTokenizer hoạt động
tương đương với EnCodec (Défossez et al., 2022) trong việc tái tạo giọng nói và thể hiện hiệu suất
mạnh mẽ trên benchmark SLMTokBench. USLM vượt trội đáng kể so với V ALL-E (Wang
et al., 2023) trong các tác vụ Text-to-Speech (TTS) không cần mẫu.
Đóng góp của chúng tôi bao gồm những điều sau:
•Chúng tôi đề xuất SpeechTokenizer, được thiết kế đặc biệt cho các mô hình ngôn ngữ giọng nói lớn
và thống nhất các token ngữ nghĩa và âm thanh thông qua việc tách biệt các khía cạnh khác nhau
của thông tin giọng nói một cách phân tầng.
•Chúng tôi thiết lập SLMTokBench, benchmark đầu tiên để đánh giá sự phù hợp của các token giọng nói
cho việc xây dựng các mô hình ngôn ngữ giọng nói.
•Chúng tôi xây dựng một mô hình ngôn ngữ giọng nói thống nhất dựa trên SpeechTokenizer, vượt trội
hơn V ALL-E trong tác vụ TTS không cần mẫu.
2 SLMT OKBENCH : BENCHMARK TOKEN MÔ HÌNH NGÔN NGỮ GIỌNG NÓI
Để xây dựng các mô hình ngôn ngữ giọng nói mạnh mẽ, các biểu diễn giọng nói rời rạc nên có
hai đặc điểm chính sau: i) Sự liên kết mạnh mẽ với văn bản; ii) Bảo tồn hiệu quả thông tin giọng nói.
Dựa trên tiền đề này, chúng tôi thiết lập Benchmark Token Mô hình Ngôn ngữ Giọng nói (SLM-
TokBench) để đánh giá sự phù hợp của các token giọng nói cho việc xây dựng các mô hình ngôn ngữ giọng nói.
2.1 ĐÁNH GIÁ LIÊN KẾT VĂN BẢN
Chúng tôi đánh giá mức độ liên kết văn bản bằng cách ước tính thông tin tương hỗ giữa các token giọng nói
và văn bản. Để ký hiệu, X biểu thị các biểu diễn giọng nói rời rạc; Y biểu thị văn bản; I(X;Y) biểu thị
thông tin tương hỗ; tập dữ liệu kiểm tra được ký hiệu là D={(xi, yi)}N
i=1 và θ biểu thị mô hình
downstream. Thông qua việc suy dẫn trong Phụ lục A, chúng tôi có thể ước tính I(X;Y) như:
ˆI(X;Y) =1
N2NX
i=1NX
j=1[logqθ(yi|xi)−logqθ(yj|xi)]
trong đó qθ(Y|X) là phân phối biến phân và có thể được tham số hóa bởi mô hình downstream θ.
Mô hình downstream là một BLSTM 2 lớp 1024 đơn vị vanilla được tối ưu hóa bởi loss CTC trên các ký tự
và nó lấy các token giọng nói làm đầu vào. Cụ thể, đối với mỗi biểu diễn rời rạc, chúng tôi trước tiên thiết lập
một ma trận nhúng, có thể được khởi tạo ngẫu nhiên hoặc có nguồn gốc từ ma trận tâm k-means hoặc
các codebook lượng tử hóa vector có được trong quá trình rời rạc hóa. Chúng tôi sử dụng ma trận nhúng
để nhúng các biểu diễn rời rạc và thu được các biểu diễn liên tục, sau đó được đưa vào các mô hình downstream.
Chúng tôi huấn luyện mô hình downstream trên tập con train-clean-100 của LibriSpeech và sử dụng tập con
dev-clean để ước tính thông tin tương hỗ. Chúng tôi cũng tính toán tỷ lệ lỗi từ (WER) trên tập kiểm tra.
Đối với việc huấn luyện mô hình downstream, chúng tôi cấu hình thiết lập huấn luyện với kích thước batch là 32,
tỷ lệ học 1e-4, và tổng cộng 200k bước toàn cục.
2.2 ĐÁNH GIÁ BẢO TỒN THÔNG TIN
Để đánh giá việc bảo tồn thông tin giọng nói trong các biểu diễn giọng nói rời rạc, chúng tôi chuyển đổi
các token giọng nói trở lại thành giọng nói và đánh giá giọng nói được tái tổng hợp bằng các chỉ số tự động
về nội dung và âm sắc. Chúng tôi huấn luyện một unit-HiFIGAN (Polyak et al., 2021) trên tập dữ liệu
LibriSpeech để chuyển đổi các đơn vị HuBERT thành dạng sóng. Đáng chú ý, để tránh sự can thiệp từ
thông tin bổ sung, chúng tôi không cung cấp bất kỳ thông tin người nói nào trong quá trình huấn luyện.
Đối với các token Encodec, chúng tôi đã sử dụng bộ giải mã Encodec để trực tiếp tạo ra dạng sóng. Việc bảo tồn
nội dung được đánh giá bằng cách tính toán WER thông qua việc chuyển đổi giọng nói được tái tổng hợp
bằng mô hình Whisper en-medium (Radford et al., 2023). Việc bảo tồn âm sắc được đánh giá bằng cách
sử dụng WavLM-TDNN (Chen et al., 2022) để tính toán độ tương tự người nói giữa giọng nói được tổng hợp
và giọng nói thực tế. Chúng tôi ngẫu nhiên lấy mẫu 300 mẫu giọng nói từ tập kiểm tra LibriSpeech để đánh giá.
3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
EncoderVQ1VQ2VQ8 ...
Decoder
HuBERT    Chưng cất Ngữ nghĩa
DiscriminatorQuantizer
GiáoviênHọc sinh
module chỉ dành cho huấn luyện  
luồng dữ liệu chỉ trong huấn luyện 
Hình 2: Minh họa khung SpeechTokenizer.
2.3 SO SÁNH TOKEN NGỮ NGHĨA & ÂM THANH
Chúng tôi sử dụng các đơn vị HuBERT L9 để biểu diễn các token ngữ nghĩa và các mã EnCodec để biểu diễn
các token âm thanh. Như thể hiện trong Bảng 3, các token ngữ nghĩa đạt được thông tin tương hỗ cao với
văn bản nhưng giọng nói được tái tổng hợp của chúng có độ tương tự người nói thấp. Các token âm thanh
đạt được WER thấp và độ tương tự người nói cao cho giọng nói được tái tổng hợp nhưng có thông tin tương hỗ
thấp với văn bản.
3 S PEECH TOKENIZER
3.1 CẤU TRÚC MÔ HÌNH
Mô hình của chúng tôi được xây dựng trên khung RVQ-GANs, tuân theo cùng một mẫu như Sound-
Stream(Zeghidour et al., 2021) và EnCodec(Défossez et al., 2022). Như được mô tả trong Hình2, mô hình
của chúng tôi sử dụng mạng encoder-decoder dựa trên tích chập từ EnCodec, thực hiện việc giảm tỷ lệ
thời gian với một hệ số bước đã chọn. Đáng chú ý, chúng tôi đã thay thế LSTM hai lớp, ban đầu theo sau
các khối tích chập trong bộ mã hóa EnCodec, bằng BiLSTM hai lớp để tăng cường khả năng mô hình hóa
ngữ nghĩa. Chúng tôi tiến hành các nghiên cứu loại bỏ cấu trúc mô hình trong Phụ lục B. Chúng tôi lượng tử hóa
các đầu ra bộ mã hóa sử dụng Lượng tử hóa Vector Dư (RVQ), một phương pháp có thể hoạt động lượng tử hóa
các phần dư theo sau các bước lượng tử hóa ban đầu với codebook riêng biệt. Chi tiết thêm về cấu trúc mô hình
có thể được tìm thấy trong Phụ lục D. Trong quá trình huấn luyện, một giáo viên ngữ nghĩa cung cấp biểu diễn
ngữ nghĩa để hướng dẫn quá trình lượng tử hóa dư.
3.2 CHƯNG CẤT NGỮ NGHĨA
Để đạt được việc mô hình hóa phân tầng của thông tin đa dạng qua các lớp RVQ khác nhau, chúng tôi sử dụng
hướng dẫn ngữ nghĩa cho bộ lượng tử hóa đầu tiên, cho phép nó bắt được thông tin nội dung. Tận dụng cấu trúc
dư cho phép các bộ lượng tử hóa tiếp theo bổ sung thông tin ngoại ngôn ngữ còn lại.

Chúng tôi sử dụng HuBERT (Hsu et al., 2021) làm giáo viên ngữ nghĩa trong nghiên cứu này, vì HuBERT
được chứng minh là bao hàm thông tin nội dung đáng kể (Mohamed et al., 2022). Chúng tôi giới thiệu hai loại
chưng cất: chưng cất biểu diễn liên tục và dự đoán nhãn giả.

Đối với chưng cất biểu diễn liên tục, chúng tôi sử dụng biểu diễn HuBERT lớp 9 hoặc biểu diễn trung bình
qua tất cả các lớp HuBERT làm giáo viên ngữ nghĩa. Mục tiêu huấn luyện là tối đa hóa độ tương tự cosine
ở cấp độ chiều qua tất cả các bước thời gian giữa các đầu ra của lớp đầu tiên RVQ và các biểu diễn giáo viên
ngữ nghĩa. Chính thức, loss chưng cất liên tục được
4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
định nghĩa như:
Ldistill =−1
DDX
d=1logσ(cos(AQ(:,d)
1,S(:,d))),
trong đó Q1 và S biểu thị đầu ra được lượng tử hóa của lớp đầu tiên RVQ và biểu diễn giáo viên ngữ nghĩa
tương ứng. A biểu thị ma trận chiếu và D là chiều của biểu diễn giáo viên ngữ nghĩa. Chỉ số trên (:, d)
biểu thị một vector bao gồm các giá trị từ tất cả các bước thời gian ở chiều d. cos(·) biểu diễn độ tương tự
cosine và σ(·) biểu thị kích hoạt sigmoid. Hàm loss chưng cất liên tục này khác với phương pháp thường
được sử dụng, tính toán loss dựa trên các biểu diễn đầu ra bởi các mô hình học sinh và giáo viên ở cùng
bước thời gian. Một phân tích so sánh của hai phương pháp này được cung cấp trong Phụ lục C.

Đối với dự đoán nhãn giả, chúng tôi áp dụng các đơn vị HuBERT làm nhãn mục tiêu. Mục tiêu huấn luyện
được xây dựng như:
Ldistll =−1
TTX
t=1utlog(Softmax (Aqt
1)),
trong đó qt
1 và ut tương ứng biểu thị đầu ra được lượng tử hóa của lớp VQ đầu tiên và đơn vị HuBERT
ở bước thời gian t. T biểu thị số bước thời gian và A là ma trận chiếu.
3.3 MỤC TIÊU HUẤN LUYỆN
Phương pháp huấn luyện của chúng tôi bao gồm cả tác vụ tái tạo và tác vụ chưng cất ngữ nghĩa. Trong tác vụ
tái tạo, chúng tôi sử dụng mục tiêu GAN, tối ưu hóa sự kết hợp của một số hạng tái tạo, một số hạng loss
phân biệt, và loss cam kết RVQ. Trong tác vụ chưng cất ngữ nghĩa, mục tiêu huấn luyện bao gồm một số hạng
loss chưng cất ngữ nghĩa. Trong phần sau, x biểu diễn một tín hiệu giọng nói và ˆ x biểu thị tín hiệu được
tái tạo bởi mạng.
Loss Tái tạo Loss tái tạo bao gồm loss miền thời gian và miền tần số. Đối với miền thời gian, chúng tôi
tối thiểu hóa khoảng cách L1 giữa x và ˆx, tức là Lt=∥x−ˆ x∥1. Đối với miền tần số, chúng tôi kết hợp
tuyến tính các loss L1 và L2 trên mel-spectrogram sử dụng nhiều tỷ lệ thời gian. Chính thức, Lf=P
i∈e∥Si(x)−Si(ˆ x)∥1+∥Si(x)−Si(ˆ x)∥2, trong đó Si là mel-spectrogram 64-bins sử dụng STFT chuẩn hóa
với kích thước cửa sổ 2i và độ dài bước 2i/4, e= 5,···,11 là tập hợp các tỷ lệ.
Loss Phân biệt Chúng tôi sử dụng cùng các discriminator như HiFi-Codec Yang et al. (2023) bao gồm
ba discriminator: Một discriminator dựa trên STFT đa tỷ lệ (MS-STFT); một discriminator đa chu kỳ
(MPD) và một discriminator đa tỷ lệ (MSD). Chi tiết thêm về các discriminator có thể được tìm thấy trong
Phụ lục D. Loss đối kháng được sử dụng để thúc đẩy chất lượng cảm nhận và nó được định nghĩa như một
hinge loss trên các logit của discriminator, được tính trung bình trên nhiều discriminator và theo thời gian.
Để K biểu thị số lượng discriminator, loss đối kháng cho generator LD được xây dựng như sau,
Lg=1
KPK
k=1max(1−Dk(ˆ x),0). Đối với các discriminator Lg được định nghĩa như:
LD=1
KKX
k=1max(1−Dk(x),0) +max(1 +Dk(ˆ x),0),
Ngoài ra, một loss khớp đặc trưng cho generator được tính toán như sau:
Lfeat=1
KLKX
k=1LX
l=1∥Dl
k(x)−Dl
k(ˆ x)∥1
mean (∥Dl
k(x)∥1),
trong đó trung bình được tính toán trên tất cả các chiều và L là số lớp trong các discriminator.
Loss Cam kết RVQ Chúng tôi thêm một loss cam kết Lw giữa giá trị trước lượng tử hóa và giá trị được
lượng tử hóa của nó, không có gradient được tính toán cho giá trị được lượng tử hóa. Loss cam kết RVQ được
định nghĩa như: Lw=PNq
i=1∥zi−zqi∥2
2., trong đó zi và zqi biểu thị phần dư hiện tại và mục nhập gần nhất
trong codebook tương ứng.
Nói chung, generator được huấn luyện để tối ưu hóa loss sau:
LG=λtLt+λfLf+λgLg+λfeatLfeat+λwLw+λdistillLdistill ,
trong đó λt, λf, λg, λfeat, λw và λdistill là các siêu tham số được sử dụng để cân bằng mỗi số hạng loss.
5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
3.4 MÔ HÌNH NGÔN NGỮ GIỌNG NÓI THỐNG NHẤT
Như thể hiện trong Hình 1, chúng tôi có thể xây dựng một mô hình ngôn ngữ giọng nói thống nhất dựa trên
SpeechTokenizer. Bao gồm các mô hình tự hồi quy và không tự hồi quy, nó có thể mô hình hóa thông tin
trong giọng nói một cách phân tầng. Mô hình tự hồi quy (AR) bắt được thông tin nội dung bằng cách mô hình hóa
các token từ bộ lượng tử hóa RVQ đầu tiên. Mô hình không tự hồi quy (NAR) bổ sung thông tin ngoại ngôn ngữ
cho mô hình AR bằng cách tạo ra các token từ các bộ lượng tử hóa tiếp theo có điều kiện trên các token lớp đầu tiên.
Chúng tôi xác thực hiệu quả của mô hình ngôn ngữ giọng nói thống nhất trên tác vụ TTS không cần mẫu.

Mô hình AR được xây dựng dựa trên các token lớp đầu tiên c1. Sử dụng kiến trúc chỉ giải mã transformer
θAR, chúng tôi tiếp cận việc chuyển đổi này như một tác vụ mô hình hóa ngôn ngữ nguyên nhân với chuỗi
âm vị u phục vụ như lời nhắc cho mô hình AR. Mục tiêu huấn luyện có thể được công thức hóa như
LAR=−logTY
t=0p(ct
1|c<t
1,u;θAR).

Mô hình NAR tạo ra các token c2:8 từ các bộ lượng tử hóa tiếp theo. Kiến trúc của nó giống với mô hình AR,
bao gồm tám lớp nhúng âm thanh riêng biệt và các lớp dự đoán đầu ra. Để kiểm soát các đặc điểm của giọng
người nói, n lời nhắc âm thanh ˆC được sử dụng để hướng dẫn âm sắc. Mô hình được điều kiện trên chuỗi
âm vị u, lời nhắc âm thanh ˆC và các token từ các bộ lượng tử hóa trước đó, dẫn đến việc công thức hóa
mục tiêu huấn luyện như sau
LNAR =−log8Y
i=2p(ci|c<i,ˆC,u;θNAR).

Trong quá trình suy luận, chúng tôi chuyển đổi đầu vào văn bản thành chuỗi âm vị và lời nhắc giọng nói
thành các token giọng nói. Chúng được nối với nhau để tạo thành các lời nhắc cho các mô hình AR và NAR.
Có điều kiện trên đó, mô hình AR tạo ra các token cấp đầu tiên, trong khi mô hình NAR lặp đi lặp lại tạo ra
các token của các cấp tiếp theo. Các token được tạo ra bởi các mô hình AR và NAR sau đó được nối để xây dựng
ma trận token giọng nói. Cuối cùng, chúng tôi sử dụng bộ giải mã SpeechTokenizer để tạo ra dạng sóng
có điều kiện trên ma trận token hoàn chỉnh.
4 THÍ NGHIỆM
4.1 THIẾT LẬP THÍ NGHIỆM
Tập dữ liệu Đối với việc huấn luyện SpeechTokenizer, chúng tôi sử dụng tập dữ liệu LibriSpeech (Panayotov et al., 2015).
Chúng tôi ngẫu nhiên cắt một đoạn 3 giây từ các mẫu giọng nói ở mỗi lần lặp huấn luyện. Đối với TTS không cần mẫu,
chúng tôi huấn luyện các mô hình AR và NAR trên tập con tiếng Anh của tập dữ liệu Multilingual LibriSpeech
(Pratap et al., 2020), chứa 44K giờ dữ liệu giọng nói được chuyển tự từ sách âm thanh LibriV ox. Chúng tôi
chọn các mẫu giọng nói có thời lượng từ 3 đến 14 giây cho dữ liệu huấn luyện. Tỷ lệ lấy mẫu là 16KHz cho
tất cả dữ liệu giọng nói.
Mô hình Đối với SpeechTokenizer, chúng tôi giới thiệu các chi tiết về cấu trúc mô hình trong phần 3.1 và
Phụ lục D. Đối với các thí nghiệm TTS không cần mẫu, mô hình AR và mô hình NAR đều là các bộ giải mã
Transformer 12 lớp với 16 head attention, chiều attention 1024 và chiều FFN 4096.
Huấn luyện Đối với SpeechTokenizer, các mô hình được huấn luyện trên 2 GPU A800 trong 20 epoch với
tỷ lệ học tối đa 4e-4 và kích thước batch 20 mỗi GPU. Đối với Mô hình Ngôn ngữ Giọng nói Thống nhất,
cả mô hình AR và NAR đều được huấn luyện trên 8 GPU A800 trong 500k bước với tỷ lệ học tối đa 5e-4.
Mô hình AR được huấn luyện với kích thước batch 7500 token mỗi GPU, và mô hình NAR được huấn luyện
với kích thước batch 5000 token mỗi GPU.
Baseline Chúng tôi áp dụng EnCodec_24khz_6kpbs (sau đây được gọi là EnCodec) (Défossez et al.,
2022) làm baseline cho SpeechTokenizer và V ALL-E (Wang et al., 2023) làm hệ thống baseline cho TTS
không cần mẫu. Chúng tôi huấn luyện V ALL-E dưới cùng tập dữ liệu và thiết lập thí nghiệm như EnCodec.
6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Khách quan Chủ quan
Tokenizer WER↓VISQOL ↑MUSHRA ↑
Groundtruth 4.58 - 91.46
EnCodec 5.11 4.37 79.86
SpeechTokenizer 5.04 4.30 90.55
Bảng 2: Kết quả tái tạo giọng nói
4.2 ĐÁNH GIÁ TÁI TẠO GIỌNG NÓI
Chúng tôi ngẫu nhiên lấy mẫu 300 mẫu giọng nói từ tập kiểm tra LibriSpeech để đánh giá tái tạo giọng nói.
Chúng tôi xem xét cả các chỉ số đánh giá chủ quan và khách quan.
Chỉ số Khách quan Chúng tôi sử dụng chỉ số ViSQOL (Hines et al., 2012) để đo lường chất lượng giọng nói.
Ngoài ra, chúng tôi đánh giá độ chính xác nội dung thông qua Tỷ lệ Lỗi Từ (WER) bằng cách chuyển âm
giọng nói sử dụng mô hình Whisper en-medium (Radford et al., 2023).
Chỉ số Chủ quan Chúng tôi áp dụng phương pháp đám đông được truyền cảm hứng từ giao thức MUSHRA (Series,
2014), với tham chiếu ẩn nhưng không có anchor được lọc thông thấp, để đánh giá chủ quan. Chúng tôi hướng dẫn
người đánh giá xếp hạng chất lượng cảm nhận của các mẫu đã cho trên thang điểm từ 1 đến 100.
4.3 ĐÁNH GIÁ MÔ HÌNH NGÔN NGỮ GIỌNG NÓI THỐNG NHẤT
Chúng tôi tiến hành đánh giá TTS không cần mẫu trên tập dữ liệu VCTK, bao gồm 108 người nói. Không có
sự chồng chéo người nói giữa dữ liệu huấn luyện và tập dữ liệu VCTK. Đối với mỗi người nói, chúng tôi
ngẫu nhiên chọn một phát ngôn 3s làm lời nhắc trong khi nội dung văn bản của một phát ngôn khác được
sử dụng làm văn bản đầu vào.
Chỉ số Khách quan Chúng tôi đánh giá các hệ thống TTS với độ tương tự người nói và WER. Chúng tôi đánh giá
độ tương tự người nói giữa giọng nói được tạo ra và giọng nói lời nhắc. Chúng tôi tính toán độ tương tự với
các bước sau: 1) chúng tôi sử dụng WavLM-TDNN để tính toán nhúng người nói cho giọng nói được tạo ra
và giọng nói lời nhắc. 2) chúng tôi tính toán độ tương tự cosine giữa các nhúng được chuẩn hóa. Chúng tôi
sử dụng mô hình Whisper medium để chuyển âm giọng nói được tạo ra và tính toán WER.
Chỉ số Chủ quan Chúng tôi xác định Điểm Ý kiến Trung bình (MOS) và Điểm Ý kiến Trung bình Độ tương tự
(SMOS) thông qua đánh giá của con người. MOS phản ánh tính tự nhiên của giọng nói, trong khi SMOS
đánh giá mức độ tương tự với giọng nói của người nói gốc. Chúng tôi thu hút 12 và 6 người bản xứ làm
người đóng góp cho đánh giá MOS và SMOS, tương ứng. MOS và SMOS đều trải dài từ 1 đến 5, với các
giá trị cao hơn biểu thị chất lượng giọng nói và độ tương tự giọng nói lớn hơn tương ứng.
4.4 KẾT QUẢ CHÍNH
Tái tạo Giọng nói Bảng 2 tổng kết kết quả của các thí nghiệm tái tạo giọng nói. SpeechTokenizer đạt được
WER thấp hơn Encodec, thể hiện khả năng bảo tồn nội dung vượt trội. Ngoài ra, SpeechTokenizer đạt được
điểm VISQOL tương đương nhưng điểm MUSHRA cao hơn EnCodec, cho thấy khả năng mạnh hơn trong
việc tạo ra giọng nói chất lượng cao.
Hiệu suất trên SLMTokBench Bảng 3 hiển thị hiệu suất của SpeechTokenizer trên SLM-
TokBench. So với EnCodec-RVQ-1, SpeechTokenizer-RVQ-1 đạt được thông tin tương hỗ cao hơn
giữa văn bản và WER thấp hơn của mô hình downstream. Điều này cho thấy SpeechTokenizer thể hiện
sự liên kết mạnh hơn với nội dung văn bản. Trong khi đó, giọng nói được tái tổng hợp của các token
SpeechTokenizer RVQ-1 đạt được WER thấp hơn và độ tương tự người nói, cho thấy khả năng giữ lại
nhiều thông tin liên quan đến nội dung hơn trong khi bỏ qua các đặc điểm âm sắc, tương tự như các token
ngữ nghĩa. Giọng nói được tái tổng hợp của các token SpeechTokenizer RVQ-1:8 thể hiện WER thấp và
độ tương tự người nói cao, minh họa năng lực của SpeechTokenizer trong việc bảo tồn thông tin giọng nói
toàn diện, tương tự như các token âm thanh. Hơn nữa, độ tương tự người nói của giọng nói được tái tổng hợp
của các token SpeechTokenizer RVQ-1 là đáng kể thấp, trong khi của các token SpeechTokenizer RVQ-1:8
là cao đáng kể. Quan sát này ngụ ý rằng các token từ các lớp tiếp theo bù đắp cho thông tin âm sắc bị loại bỏ
bởi các token lớp đầu tiên.
7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Liên kết Văn bản Bảo tồn Thông tin
Tokenizer Teacher MI↑WER†↓WER∗↓ SIM↑
Groundtruth - - 4.58 1.0
HuBERT KM500 - 31.2 9.88 16.26 0.77
EnCodec RVQ-1 - 16.5 61.52 38.34 0.92
EnCodec RVQ-1:8 - 23.6 30.91 5.11 0.98
Ablations
SpeechTokenizer RVQ-1 HuBERT avg 30.9 15.58 9.57 0.74
SpeechTokenizer RVQ-1:8 HuBERT avg 29.7 16.03 5.04 0.97
SpeechTokenizer RVQ-1 HuBERT L9 32.9 12.68 14.17 0.73
SpeechTokenizer RVQ-1:8 HuBERT L9 31.6 13.12 5.31 0.97
SpeechTokenizer RVQ-1 HuBERT units 24.2 34.13 20.02 0.72
SpeechTokenizer RVQ-1:8 HuBERT units 25.1 30.71 5.84 0.95
Bảng 3: Kết quả trên SLMTokBench. MI và WER†đề cập đến thông tin tương hỗ và tỷ lệ lỗi từ
của mô hình downstream. WER∗và SIM đề cập đến tỷ lệ lỗi từ và độ tương tự người nói của
giọng nói được tái tổng hợp tương ứng. RVQ- n biểu thị các token của lớp RVQ thứ n. RVQ- n:m
biểu thị các token từ lớp thứ n đến lớp thứ m.
Khách quan Chủ quan
Model Tokenizer WER↓SIM↑MOS↑SMOS ↑
Groundtruth 1.9 0.93 4.5 3.96
V ALL-E EnCodec 7.9 0.75 3.08 3.31
USLM SpeechTokenizer 6.5 0.84 3.63 3.45
Bảng 4: Kết quả TTS không cần mẫu
TTS không cần mẫu Như thể hiện trong Bảng 4, USLM của chúng tôi thể hiện WER thấp hơn V ALL-E.
Kết quả này làm nổi bật rằng SpeechTokenizer có thể đóng góp vào việc mô hình hóa thông tin nội dung
chính xác hơn. Ngoài ra, USLM thể hiện độ tương tự người nói vượt trội, ngụ ý rằng cấu trúc thông tin
tách biệt thuận lợi hơn cho việc mô hình hóa thông tin liên quan đến người nói.
5 PHÂN TÍCH
5.1 LỰA CHỌN GIÁO VIÊN NGỮ NGHĨA
Như thể hiện trong Bảng 3, làm giáo viên ngữ nghĩa, các biểu diễn HuBERT L9 hoạt động tốt hơn các đơn vị
HuBERT trong cả Liên kết Văn bản và Bảo tồn Thông tin, bất kể là RVQ-1 hay RVQ-1:8. Lý do có thể là
các đơn vị HuBERT rời rạc mất một số thông tin nội dung so với các biểu diễn liên tục, do đó cung cấp
hướng dẫn ngữ nghĩa yếu hơn cho SpeechTokenizer. Khi so sánh các biểu diễn HuBERT L9 với các biểu diễn
trung bình HuBERT, chúng tôi thấy rằng về Liên kết Văn bản, thông tin tương hỗ cao hơn khi các biểu diễn
HuBERT L9 phục vụ như giáo viên. Điều này là do các biểu diễn trung bình HuBERT chứa một số thông tin
âm sắc, trong khi HuBERT L9 cung cấp thông tin nội dung thuần khiết hơn. Mặt khác, trung bình HuBERT
cho thấy hiệu suất tốt hơn trong Bảo tồn Thông tin, phản ánh trong WER thấp hơn. Chúng tôi suy đoán rằng
điều này là do một mức độ xung đột tác vụ nhất định giữa chưng cất ngữ nghĩa và tái tạo, trong đó cái trước
nhằm mục đích chỉ giữ lại thông tin nội dung trong khi cái sau nhằm mục đích bảo tồn các khía cạnh khác nhau
của giọng nói. Sự hiện diện của một số thông tin âm sắc trong các biểu diễn trung bình HuBERT có thể ở một
mức độ nào đó làm giảm xung đột tác vụ này.
5.2 HIỆU QUẢ CỦA VIỆC TÁCH BIỆT THÔNG TIN
Để chứng minh rằng thông tin giọng nói khác nhau có thể được mô hình hóa phân tầng trong SpeechTokenizer,
chúng tôi tiến hành thí nghiệm chuyển đổi giọng nói một lần (VC). Thí nghiệm này nhằm mục đích chuyển đổi
giọng nói từ bất kỳ người nói nguồn nào thành một người nói mục tiêu tùy ý chỉ sử dụng vài giây giọng nói
tham chiếu từ người nói mục tiêu. Để sử dụng SpeechTokenizer cho VC một lần, bước đầu tiên là chuyển đổi
giọng nói nguồn và giọng nói tham chiếu thành các ma trận token. Bằng cách nối các token RVQ-1
8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Nguồn Tham chiếu WER↓SIM↑
Groundtruth 0.4 0.93
RVQ-1 RVQ-2 2.6 0.72
RVQ-1 RVQ-2:4 11.7 0.80
RVQ-1 RVQ-2:8 35.4 0.82
Bảng 5: Kết quả chuyển đổi giọng nói một lần. Nguồn và Tham chiếu đề cập đến ma trận token nguồn
và ma trận token tham chiếu tương ứng.
(a) RVQ-1 (b) RVQ-2:8
Hình 3: Trực quan hóa đầu ra được lượng tử hóa của các lớp RVQ khác nhau của SpeechTokenizer.
Lớp đầu tiên được ký hiệu là RVQ-1, trong khi tổng của lớp thứ hai đến lớp thứ tám được ký hiệu là RVQ-2:8.
của ma trận token nguồn với các token RVQ-2:8 của ma trận token tham chiếu, và sau đó chuyển ma trận
token kết hợp này cho bộ giải mã, chúng tôi có thể đạt được chuyển đổi giọng nói. Độ dài của các token
tham chiếu và nguồn có thể không hoàn toàn khớp. Để giải quyết điều này, chúng tôi sử dụng cắt ngắn hoặc
đệm vòng tròn để đảm bảo chúng chia sẻ cùng độ dài thời gian, do đó tạo điều kiện cho quá trình nối.
Chúng tôi tiến hành thí nghiệm trên tập dữ liệu VCTK. Chúng tôi ngẫu nhiên chọn một mẫu giọng nói từ
một người nói để phục vụ như giọng nói nguồn. Từ 107 người nói còn lại, chúng tôi riêng lẻ chọn một mẫu
giọng nói với nội dung khác để đóng vai trò như giọng nói tham chiếu. Chúng tôi sử dụng hai chỉ số để đánh giá:
WER và độ tương tự người nói.

Bảng 5 báo cáo kết quả của các thí nghiệm VC một lần. Từ bảng, chúng ta có thể thấy rằng khi số lượng
lớp cho các token tham chiếu tăng, độ tương tự người nói cũng tăng dần. Điều này cho thấy nhiều thông tin
từ người nói tham chiếu đang được chuyển giao, chứng minh rằng thông tin người nói được nhúng trong
các token từ lớp thứ hai đến lớp cuối cùng. Khi các token tham chiếu được chọn từ lớp thứ hai đến lớp thứ tư,
chúng tôi đạt được WER thấp và độ tương tự người nói cao, dẫn đến hiệu suất VC một lần thỏa mãn. Điều này
cho thấy việc tách biệt thông tin thành công.

Chúng tôi cũng trực quan hóa các đầu ra được lượng tử hóa từ các lớp khác nhau trong Hình 3. Cụ thể,
chúng tôi ngẫu nhiên chọn năm người nói từ tập dữ liệu VCTK và chọn 10 mẫu giọng nói ngẫu nhiên mỗi
người nói. Chúng tôi trích xuất đầu ra được lượng tử hóa của các lớp RVQ khác nhau của SpeechTokenizer.
Đầu ra lớp đầu tiên được ký hiệu là biểu diễn RVQ-1, trong khi tổng của các đầu ra từ lớp thứ hai đến lớp
thứ tám được ký hiệu là biểu diễn RVQ-2:8. Bằng cách thực hiện pooling trung bình dọc theo chiều thời gian,
mỗi biểu diễn được chuyển đổi thành một vector duy nhất. Các vector này sau đó được trực quan hóa trong
không gian 2D sử dụng t-SNE, với các mẫu giọng nói từ cùng một người nói được biểu diễn bằng cùng một màu.
Từ biểu đồ, có thể quan sát thấy rằng các biểu diễn RVQ-1 cho các người nói khác nhau được phân tán ngẫu nhiên
mà không có mẫu có thể phân biệt được. Ngược lại, các biểu diễn RVQ-2:8 cho cùng một người nói có xu hướng
tập trung lại với nhau, trong khi khác biệt với những người nói khác. Điều này cho thấy rằng thông tin đặc
trưng người nói được chứa từ lớp thứ hai đến lớp thứ tám.
6 CÔNG VIỆC LIÊN QUAN
Công việc liên quan của chúng tôi được đặt trong Phụ lục E.
9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
7 KẾT LUẬN
Trong nghiên cứu này, chúng tôi trình bày SLMTokBench, đánh giá tác động của các loại token giọng nói khác nhau.
Trong khi đó, chúng tôi đề xuất SpeechTokenizer, để thống nhất việc rời rạc hóa của cả hai loại token giọng nói
để khắc phục vấn đề sử dụng nhiều mô hình để trích xuất các token rời rạc ngữ nghĩa và âm thanh riêng biệt.
Hơn nữa, chúng tôi phát triển một mô hình ngôn ngữ giọng nói thống nhất (USLM) dựa trên Speech-
Tokenizer, với kết quả tốt hơn về độ chính xác nội dung và chất lượng của giọng nói được tạo ra. Nghiên cứu
về một bộ token hóa giọng nói thống nhất là một phần thiết yếu của sự phát triển tiếp theo của mô hình
ngôn ngữ giọng nói về mặt hiệu quả và chất lượng.
TÀI LIỆU THAM KHẢO
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization, 2016.
Baevski, A., Zhou, Y ., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised
learning of speech representations. Advances in Neural Information Processing Systems , 33:
12449–12460, 2020.
Bengio, Y ., Léonard, N., and Courville, A. Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier,
D., Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio
generation, 2022.
Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., Gölge, E., and Ponti, M. A. YourTTS:
Towards zero-shot multi-speaker TTS and zero-shot voice conversion for everyone. In Chaudhuri,
K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the
39th International Conference on Machine Learning , volume 162 of Proceedings of Machine
Learning Research , pp. 2709–2720. PMLR, 17–23 Jul 2022. URL https://proceedings.
mlr.press/v162/casanova22a.html .
Chang, H.-J., Yang, S.-w., and Lee, H.-y. Distilhubert: Speech representation learning by layer-wise
distillation of hidden-unit bert. In ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pp. 7087–7091. IEEE, 2022.
Chen, S., Wang, C., Chen, Z., Wu, Y ., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X.,
Wu, J., Zhou, L., Ren, S., Qian, Y ., Qian, Y ., Wu, J., Zeng, M., Yu, X., and Wei, F. WavLM:
Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected
Topics in Signal Processing , 16(6):1505–1518, oct 2022. doi: 10.1109/jstsp.2022.3188113. URL
https://doi.org/10.1109%2Fjstsp.2022.3188113 .
Cheng, P., Hao, W., Dai, S., Liu, J., Gan, Z., and Carin, L. Club: A contrastive log-ratio upper bound
of mutual information, 2020.
Chung, Y .-A., Zhang, Y ., Han, W., Chiu, C.-C., Qin, J., Pang, R., and Wu, Y . W2v-bert: Combining
contrastive learning and masked language modeling for self-supervised speech pre-training, 2021.
Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast and accurate deep network learning by
exponential linear units (elus), 2016.
Dong, Q., Huang, Z., Tian, Q., Xu, C., Ko, T., Zhao, Y ., Feng, S., Li, T., Wang, K., Cheng, X., Yue,
F., Bai, Y ., Chen, X., Lu, L., Ma, Z., Wang, Y ., Wang, M., and Wang, Y . Polyvoice: Language
models for speech to speech translation, 2023.
Défossez, A., Copet, J., Synnaeve, G., and Adi, Y . High fidelity neural audio compression, 2022.
Hassid, M., Remez, T., Nguyen, T. A., Gat, I., Conneau, A., Kreuk, F., Copet, J., Defossez, A.,
Synnaeve, G., Dupoux, E., Schwartz, R., and Adi, Y . Textually pretrained speech language models,
2023.
10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hines, A., Skoglund, J., Kokaram, A., and Harte, N. Visqol: The virtual speech quality objective
listener. In IWAENC 2012; International Workshop on Acoustic Signal Enhancement , pp. 1–4,
2012.
Hsu, W.-N., Bolte, B., Tsai, Y .-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert:
Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM
Transactions on Audio, Speech, and Language Processing , 29:3451–3460, 2021.
Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi,
M., and Zeghidour, N. Speak, read and prompt: High-fidelity text-to-speech with minimal
supervision, 2023.
Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity
speech synthesis. Advances in Neural Information Processing Systems , 33:17022–17033, 2020.
Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y ., Polyak, A., Bolte, B., Nguyen, T.-A., Copet,
J., Baevski, A., Mohamed, A., et al. On generative spoken language modeling from raw audio.
Transactions of the Association for Computational Linguistics , 9:1336–1354, 2021.
Liu, H., Wang, T., Fu, R., Yi, J., Wen, Z., and Tao, J. Unifyspeech: A unified framework for zero-shot
text-to-speech and voice conversion, 2023.
Mohamed, A., yi Lee, H., Borgholt, L., Havtorn, J. D., Edin, J., Igel, C., Kirchhoff, K., Li, S.-W.,
Livescu, K., Maaloe, L., Sainath, T. N., and Watanabe, S. Self-supervised speech representation
learning: A review. IEEE Journal of Selected Topics in Signal Processing , 16(6):1179–1210,
oct 2022. doi: 10.1109/jstsp.2022.3207050. URL https://doi.org/10.1109%2Fjstsp.
2022.3207050 .
OpenAI. Gpt-4 technical report, 2023.
Panayotov, V ., Chen, G., Povey, D., and Khudanpur, S. Librispeech: An asr corpus based on public
domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pp. 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964.
Polyak, A., Adi, Y ., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux,
E. Speech resynthesis from discrete disentangled self-supervised representations, 2021.
Pratap, V ., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. MLS: A large-scale multilingual
dataset for speech research. In Interspeech 2020 . ISCA, oct 2020. doi: 10.21437/interspeech.
2020-2826. URL https://doi.org/10.21437%2Finterspeech.2020-2826 .
Qian, K., Zhang, Y ., Chang, S., Yang, X., and Hasegawa-Johnson, M. Autovc: Zero-shot voice style
transfer with only autoencoder loss, 2019.
Qian, K., Zhang, Y ., Chang, S., Hasegawa-Johnson, M., and Cox, D. Unsupervised speech decompo-
sition via triple information bottleneck. In International Conference on Machine Learning , pp.
7836–7846. PMLR, 2020.
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech
recognition via large-scale weak supervision. In International Conference on Machine Learning ,
pp. 28492–28518. PMLR, 2023.
Rubenstein, P. K., Asawaroengchai, C., Nguyen, D. D., Bapna, A., Borsos, Z., de Chaumont Quitry,
F., Chen, P., Badawy, D. E., Han, W., Kharitonov, E., Muckenhirn, H., Padfield, D., Qin, J.,
Rozenberg, D., Sainath, T., Schalkwyk, J., Sharifi, M., Ramanovich, M. T., Tagliasacchi, M.,
Tudor, A., Velimirovi ´c, M., Vincent, D., Yu, J., Wang, Y ., Zayats, V ., Zeghidour, N., Zhang, Y .,
Zhang, Z., Zilka, L., and Frank, C. Audiopalm: A large language model that can speak and listen,
2023.
Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. Advances in neural information processing systems , 29, 2016.
11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Series, B. Method for the subjective assessment of intermediate quality level of audio systems.
International Telecommunication Union Radiocommunication Assembly , 2014.
Shi, Y ., Bu, H., Xu, X., Zhang, S., and Li, M. Aishell-3: A multi-speaker mandarin tts corpus and the
baselines, 2021.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,
N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 , 2023.
Wang, C., Chen, S., Wu, Y ., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y ., Wang, H., Li, J., He, L.,
Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers,
2023.
Wu, D.-Y . and Lee, H.-y. One-shot voice conversion by vector quantization. In ICASSP 2020-
2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp.
7734–7738. IEEE, 2020.
Yang, D., Liu, S., Huang, R., Tian, J., Weng, C., and Zou, Y . Hifi-codec: Group-residual vector
quantization for high fidelity audio codec, 2023.
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: An end-to-end
neural audio codec, 2021.
Zhang, D., Li, S., Zhang, X., Zhan, J., Wang, P., Zhou, Y ., and Qiu, X. Speechgpt: Empowering large
language models with intrinsic cross-modal conversational abilities, 2023.
12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
A ƯỚC TÍNH THÔNG TIN TƯƠNG HỖ
Để ký hiệu, X biểu thị các biểu diễn giọng nói rời rạc; Y biểu thị văn bản; I(X;Y) biểu thị
thông tin tương hỗ; tập dữ liệu kiểm tra được ký hiệu là D={(xi, yi)}N
i=1 và θ biểu thị mô hình
downstream. Một thước đo thông tin tương hỗ giữa biến X và Y có thể được công thức hóa như:
I(X;Y) =Z
XZ
YlogP(X, Y )
P(X)P(Y)
trong đó P(X) và P(Y) là các phân phối biên của X và Y tương ứng, và P(X, Y) biểu thị
phân phối chung của X và Y.
Cận trên log-ratio đối lập biến phân (vCLUB) (Cheng et al., 2020) của thông tin tương hỗ
được định nghĩa bởi:
I(X;Y) =Ep(X,Y )[logqθ(Y|X)]−Ep(X)p(Y)[logqθ(Y|X)]
trong đó qθ(Y|X) là phân phối biến phân để xấp xỉ xác suất thực tế P(Y|X)
và có thể được tham số hóa bởi mô hình downstream θ.
Với tập dữ liệu kiểm tra D, I(X;Y) có một ước tính không thiên vị như:
ˆI(X;Y) =1
N2NX
i=1NX
j=1[logqθ(yi|xi)−logqθ(yj|xi)]
13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
B LOẠI BỎ CẤU TRÚC MÔ HÌNH
Chúng tôi đã tiến hành một nghiên cứu loại bỏ về việc có nên sử dụng LSTM hay BiLSTM. Trong bảng 6, có thể
thấy rằng hiệu suất của BiLSTM về liên kết văn bản tốt hơn LSTM, cho thấy BiLSTM tốt hơn trong việc bắt
thông tin ngữ nghĩa.
Liên kết Văn bản Bảo tồn Thông tin
Cấu trúc Mô hình MI↑WER†↓WER∗↓ SIM↑
CNN+LSTM RVQ-1 27.60 20.71 9.06 0.74
RVQ-1:8 28.61 20.38 5.44 0.97
CNN+BiLSTM RVQ-1 30.9 15.58 9.57 0.74
RVQ-1:8 29.7 16.03 5.04 0.97
Bảng 6: Kết quả thí nghiệm loại bỏ BiLSTM trên SLMTokBench. Chúng tôi sử dụng biểu diễn trung bình
qua tất cả các lớp HuBERT làm giáo viên ngữ nghĩa trong thí nghiệm này.
14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
C PHÂN TÍCH LOSS CHƯNG CẤT LIÊN TỤC
Trong tài liệu hiện có, các hàm loss thường được sử dụng cho chưng cất chuỗi liên tục thường được tính toán
dọc theo trục thời gian, với mục tiêu tối thiểu hóa sự khác biệt giữa các đầu ra của mô hình học sinh và giáo viên
ở mỗi bước thời gian. Ví dụ, hàm loss được đề xuất trong (Chang et al., 2022) nhằm mục đích tối đa hóa độ
tương tự cosine giữa các biểu diễn mô hình học sinh và giáo viên ở cùng bước thời gian trong khi tối thiểu hóa
khoảng cách L1 của chúng, do đó tạo điều kiện cho việc chuyển giao kiến thức từ giáo viên đến mô hình học sinh.
Để điều chỉnh công thức này cho tác vụ cụ thể của chúng tôi, chúng tôi có thể sửa đổi hàm loss như sau:
Ldistll =Ll1+λLcos
=TX
t=1[1
D∥st−Aqt
1∥1−λlogσ(cos (st,Aqt
1))],
trong đó qt
1 và st tương ứng biểu thị đầu ra được lượng tử hóa của lớp đầu tiên RVQ và biểu diễn giáo viên
ngữ nghĩa D chiều ở bước thời gian t. cos(·) là độ tương tự cosine. T biểu thị số bước thời gian và A là ma trận
chiếu. σ(·) biểu thị kích hoạt sigmoid. λ > 0 kiểm soát đóng góp của các lớp cosine. Chúng tôi gọi hàm loss
này là "T-axis" để phân biệt nó với hàm loss "D-axis" mà chúng tôi đề xuất trong Phần 3.2. Thuật ngữ sau
được sử dụng để biểu thị hàm loss được giới thiệu trong phần đã nêu. Các chỉ định này được sử dụng để phân
biệt giữa hai loại hàm loss này trong bài báo này.

Chúng tôi đã điều tra tác động của hai hàm loss chưng cất liên tục riêng biệt đối với hiệu suất của
SpeechTokenizer trên SLMTokBench. Kết quả của thí nghiệm này được tóm tắt trong Bảng 7. Khi so sánh
với hiệu suất của EnCodec trên SLMTokBench, như được trình bày trong Bảng 3, việc sử dụng hàm loss
chưng cất liên tục "T-axis" cải thiện đáng kể khả năng liên kết văn bản của SpeechTokenizer. Tuy nhiên,
cải thiện này hơi kém hơn so với SpeechTokenizer sử dụng hàm loss "D-axis". Về Bảo tồn Thông tin,
SpeechTokenizer với hàm loss "D-axis" cũng vượt trội hơn đối tác "T-axis" của nó. Kết quả thí nghiệm cho
thấy hàm loss chưng cất liên tục "D-axis" mang lại hiệu quả chưng cất vượt trội so với hàm loss "T-axis"
truyền thống. Chúng tôi quy cải thiện này cho chiến lược của hàm loss "D-axis" trong việc tính toán độ tương tự
cosine qua từng chiều, đảm bảo rằng mô hình học sinh liên kết chặt chẽ với mô hình giáo viên trên từng chiều
đặc trưng. Phương pháp này cung cấp tín hiệu giám sát phong phú hơn, thúc đẩy quá trình học của mô hình
học sinh bằng cách tập trung không chỉ vào độ tương tự đầu ra tổng thể mà còn vào độ tương tự trong từng chiều.
Liên kết Văn bản Bảo tồn Thông tin
Ldistill MI↑WER†↓WER∗↓ SIM↑
T-Axis RVQ-1 26.65 21.10 10.75 0.76
RVQ-1:8 25.97 21.54 5.29 0.96
D-Axis RVQ-1 30.9 15.58 9.57 0.74
RVQ-1:8 29.7 16.03 5.04 0.97
Bảng 7: Kết quả thí nghiệm loại bỏ loss chưng cất liên tục trên SLMTokBench. Chúng tôi sử dụng biểu diễn
trung bình qua tất cả các lớp HuBERT làm giáo viên ngữ nghĩa trong thí nghiệm này.
15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
D CHI TIẾT CẤU TRÚC MÔ HÌNH VÀ DISCRIMINATOR
Kiến trúc Encoder & Decoder Encoder được xây dựng như một chuỗi tuần tự các thành phần:
bắt đầu với một lớp tích chập 1D có C kênh và kích thước kernel là 7, theo sau là một tập
B khối tích chập dư. Mỗi khối được tạo thành từ hai tích chập giãn với tỷ lệ giãn (1,1)
và kích thước kernel (3,1) và một skip-connection, theo sau là một lớp down-sampling tích chập
có bước, với kích thước kernel K gấp đôi bước R. Bất cứ khi nào down-sampling,
số kênh được nhân đôi. Khác với EnCodec mà các khối tích chập được theo sau
bởi LSTM hai lớp, chúng tôi sử dụng BiLSTM để tăng cường khả năng mô hình hóa ngữ nghĩa.
Một lớp tích chập 1D cuối cùng với kích thước kernel là 7 được sử dụng để đặt chiều của nhúng
thành D. Chúng tôi sử dụng C= 32, B= 4 và (2,4,5,8) như các bước. Chúng tôi sử dụng ELU (Clevert et al., 2016)
như một kích hoạt phi tuyến hoặc chuẩn hóa lớp (Ba et al., 2016) hoặc chuẩn hóa trọng số (Salimans & Kingma,
2016). Decoder phản chiếu encoder và sử dụng tích chập chuyển vị và LSTM thay vì tích chập bước
và BiLSTM, với các bước theo thứ tự ngược lại như trong encoder. Decoder xuất ra tín hiệu âm thanh cuối cùng.
Residual Vector Quantizer Chúng tôi sử dụng Residual Vector Quantizer (RVQ) để lượng tử hóa đầu ra encoder
và tuân theo cùng quy trình huấn luyện như EnCodec. Trong quá trình huấn luyện, mã được chọn cho mỗi đầu vào
được cập nhật sử dụng trung bình di chuyển số mũ với độ suy giảm 0.99, và các mã chưa được
gán bất kỳ vector đầu vào nào trong nhiều batch được thay thế bằng các vector đầu vào được lấy mẫu ngẫu nhiên
trong batch hiện tại. Straight-through-estimator (Bengio et al., 2013) được sử dụng để tính toán gradient
của encoder, ví dụ như thể bước lượng tử hóa là hàm đồng nhất trong giai đoạn lan truyền ngược.
Cuối cùng, một loss cam kết, bao gồm MSE giữa đầu vào của bộ lượng tử hóa và đầu ra của nó,
với gradient chỉ được tính toán đối với đầu vào của nó, được thêm vào loss huấn luyện tổng thể.
Discriminator Discriminator MS-STFT sử dụng các mạng với cấu trúc giống hệt nhau hoạt động
trên STFT phức có giá trị đa tỷ lệ, trong đó các phần thực và ảo được nối. Đối với mỗi
mạng con, nó được tạo thành từ một lớp tích chập 2D (sử dụng kích thước kernel 3×8 với 32 kênh),
theo sau là các tích chập 2D với tỷ lệ giãn tăng trong chiều thời gian (1, 2 và 4), và
bước 2 trên trục tần số. Một tích chập 2D cuối cùng với kích thước kernel 3×3 và bước (1,1)
cung cấp dự đoán cuối cùng. Đối với MSD và MPD, chúng tôi tuân theo cùng thiết lập như trong HiFiGAN (Kong
et al., 2020) nhưng điều chỉnh số kênh để căn chỉnh các tham số của discriminator gần hơn với
của MS-STFT.
16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
E CÔNG VIỆC LIÊN QUAN
Biểu diễn Giọng nói Rời rạc Có hai biểu diễn giọng nói rời rạc phổ biến: token ngữ nghĩa và token âm thanh.
Token ngữ nghĩa có thể được trích xuất từ việc học tự giám sát của các biểu diễn giọng nói (Hsu et al., 2021; Chung et al., 2021)
và mã hóa các biểu diễn cấp cao tương quan với các đặc trưng thô, biểu tượng trong khi thông tin ngoại ngôn ngữ
như nhận dạng người nói và chi tiết âm thanh được loại bỏ. Token âm thanh có thể được trích xuất từ codec âm thanh thần kinh
(Zeghidour et al., 2021; Défossez et al., 2022; Yang et al., 2023) và cung cấp tái tạo độ trung thực cao của
các chi tiết âm thanh. Nhưng chúng không thể tách biệt thông tin khác nhau của giọng nói. SpeechTokenizer
thống nhất hai loại token, cho phép cả tái tạo âm thanh chất lượng cao và tách biệt
thông tin khác nhau của giọng nói.
Mô hình Ngôn ngữ Tạo sinh Giọng nói Các mô hình ngôn ngữ tạo sinh giọng nói dựa trên biểu diễn giọng nói rời rạc
đã thể hiện hiệu suất đáng kể trên các tác vụ xử lý giọng nói khác nhau (Borsos
et al., 2022; Wang et al., 2023; Kharitonov et al., 2023; Zhang et al., 2023). AudioLM (Borsos
et al., 2022) đề xuất mô hình hóa giọng nói dựa trên codec âm thanh cùng với các mã ngữ nghĩa, có thể
tổng hợp giọng nói trong thiết lập không văn bản. V ALL-E (Wang et al., 2023) tận dụng các mô hình codec thần kinh
để biểu diễn giọng nói trong các token rời rạc từ tám bộ lượng tử hóa. V ALL-E bao gồm một
mô hình ngôn ngữ tự hồi quy chuyển đổi âm vị thành token âm thanh từ bộ lượng tử hóa đầu tiên
và một mô hình ngôn ngữ không tự hồi quy để tạo ra mã của bảy bộ lượng tử hóa khác. Tuy nhiên,
V ALL-E gặp vấn đề rằng một số từ có thể không rõ ràng, bị bỏ lỡ, hoặc trùng lặp trong
tổng hợp giọng nói do khoảng cách thông tin giữa token âm thanh và âm vị. Để bắc cầu khoảng cách,
SPEAR-TTS (Kharitonov et al., 2023) sử dụng token ngữ nghĩa như một cầu nối giữa văn bản và token âm thanh.
Nó trước tiên tạo ra token ngữ nghĩa từ văn bản và sau đó tạo ra token âm thanh từ token ngữ nghĩa.
Tuy nhiên, phương pháp mô hình hóa đa giai đoạn này phức tạp hơn và có thể dẫn đến các vấn đề
như tích lũy lỗi và tốc độ suy luận chậm. Bộ lượng tử hóa đầu tiên của SpeechTokenizer tạo ra
token ngữ nghĩa, trong khi bảy bộ lượng tử hóa còn lại tạo ra token âm thanh bằng cách mô hình hóa
thông tin ngoại ngôn ngữ bị mất trong token ngữ nghĩa. V ALL-E dựa trên SpeechTokenizer kết hợp
ưu điểm của V ALL-E và SPEAR-TTS, trong đó mô hình tự hồi quy có thể thực hiện chuyển đổi văn bản-thành-
token ngữ nghĩa, và mô hình không tự hồi quy có thể đạt được chuyển đổi ngữ nghĩa-thành-token âm thanh.
Tách biệt Biểu diễn Giọng nói Giọng nói của con người có thể được tách biệt thô thành ba thành phần: nội dung, âm sắc, và giai điệu (Liu et al., 2023).
Nội dung biểu diễn thông tin chính trong giọng nói, có thể được biểu đạt bằng văn bản hoặc âm vị.
Âm sắc biểu diễn đặc điểm của người nói, trong khi giai điệu bao gồm âm điệu, trọng âm, và nhịp điệu của giọng nói,
phản ánh cách người nói truyền đạt thông tin nội dung. Các phương pháp Tách biệt Biểu diễn Giọng nói (SRD)
hiện tại chủ yếu tách thông tin người nói khỏi thông tin nội dung cho chuyển đổi giọng nói (Qian
et al., 2019; Casanova et al., 2022). Các phương pháp này áp dụng chiến lược tách biệt song song, trong đó
giọng nói được đưa vào các encoder nội dung và người nói song song để thu được các biểu diễn khác nhau (Qian
et al., 2020). Tuy nhiên, chiến lược này phụ thuộc nhiều vào kiến thức trước và tạo ra các thiên kiến cảm ứng mạnh,
làm cho quá trình mô hình hóa phức tạp hơn và có thể bỏ qua một số thông tin giọng nói như giai điệu.
Khác biệt, VQVC (Wu & Lee, 2020) mô hình hóa nhúng nội dung như một chuỗi các mã rời rạc và
lấy sự khác biệt giữa vector trước lượng tử hóa và sau lượng tử hóa làm nhúng người nói. Tương tự, SpeechTokenizer
sử dụng cấu trúc dư để thực hiện tách biệt nối tiếp thông tin giọng nói và mô hình hóa thông tin khác nhau
như các token rời rạc.
17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F PHÂN TÍCH CODEBOOK
Chúng tôi điều tra xem các token được học bởi bộ lượng tử hóa RVQ đầu tiên có liên quan đến thông tin âm học không.
Sử dụng SpeechTokenizer hoặc EnCodec, chúng tôi tạo ra các token giọng nói từ tập huấn luyện TIMIT và
trích xuất các token RVQ-1, được ký hiệu là q1. Sau đó chúng tôi tính toán xác suất có điều kiện p(phoneme |q1)
dựa trên sự đồng xuất hiện giữa âm vị và các mã. Các căn chỉnh được xây dựng bằng cách
chọn âm vị xuất hiện thường xuyên nhất trong trường tiếp nhận cho mỗi q1.

Hình 4 trực quan hóa xác suất có điều kiện p(phoneme |q1) cho cả SpeechTokenizer và EnCodec.
Một khối màu tối hơn cho thấy p(phoneme |q1) cao hơn. Sự tương phản rõ ràng hơn giữa dải màu đường chéo
và khu vực xung quanh nó biểu thị độ tinh khiết âm vị lớn hơn, điều này lần lượt cho thấy ánh xạ chính xác hơn
giữa mã và âm vị tương ứng của nó. Đối với SpeechTokenizer, rõ ràng là trong codebook của
bộ lượng tử hóa RVQ-1, nhiều mã rời rạc dường như chuyên về bắt các âm thanh âm học cụ thể,
cho thấy bộ lượng tử hóa RVQ-1 có thể thu được căn chỉnh tốt giữa các mã và các âm vị được gán nhãn.
Tuy nhiên, đối với EnCodec, hiện tượng này không rõ ràng bằng.

Ngoài ra, Hình 4 cũng tiết lộ rằng hơn 600 mã từ codebook RVQ-1 của EnCodec chưa bao giờ được
sử dụng, cho thấy tỷ lệ sử dụng không tối ưu của codebook khi EnCodec mã hóa giọng nói.
Tỷ lệ sử dụng codebook thấp hơn ngụ ý rằng cần nhiều lớp RVQ hơn để đảm bảo chất lượng
của giọng nói được tổng hợp, do đó cần tạo ra nhiều mã hơn trong quá trình
xây dựng mô hình ngôn ngữ giọng nói tạo sinh, dẫn đến tiêu thụ không gian, thời gian và sức mạnh tính toán lớn hơn.

Chúng tôi đánh giá thêm các mô hình sử dụng Phone-Normalized Mutual Information (PNMI) (Hsu et al.,
2021). Như thể hiện trong Bảng 8, các token RVQ-1 của SpeechTokenizer đạt được điểm PNMI vượt trội
so với các đơn vị HuBERT và vượt trội đáng kể so với EnCodec-RVQ-1. Điều này cho thấy quá trình
chưng cất ngữ nghĩa trong SpeechTokenizer hiệu quả, do đó giải thích hiệu suất liên kết văn bản cải thiện của nó.

Hình 4: Trực quan hóa xác suất có điều kiện P(phoneme |code) trên tập huấn luyện TIMIT. Trục y
là tập âm vị và trục x là các từ mã của lớp RVQ đầu tiên được sắp xếp theo âm vị tương quan nhất.

Tokenizer PNMI↑
HuBERT KM500 0.43
EnCodec RVQ-1 0.28
SpeechTokenizer RVQ-1 0.71
Bảng 8: PNMI của biểu diễn giọng nói rời rạc khác nhau.
18

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
G MỞ RỘNG CHO NGÔN NGỮ CHƯA THẤY
Vì thông tin ngoại ngôn ngữ được coi là bất biến ngôn ngữ, chúng tôi đã thử áp dụng SpeechTokenizer
trực tiếp cho các ngôn ngữ chưa thấy. Chúng tôi chọn tiếng Đức và tiếng Trung. Đối với tiếng Đức, chúng tôi
chọn các mẫu từ tập con tiếng Đức của tập dữ liệu Multilingual LibriSpeech để kiểm tra. Đối với tiếng Trung,
chúng tôi chọn các mẫu từ tập dữ liệu Aishell-3 (Shi et al., 2021) để kiểm tra. Chúng tôi tái tổng hợp giọng nói
từ các token RVQ-1 và RVQ-1:8. Giọng nói được tái tổng hợp được hiển thị trong trang web demo của chúng tôi1.
Chúng tôi cũng phân tích melspectrogram của giọng nói tiếng Đức và giọng nói tiếng Anh trong Phụ lục H.

Kết quả cho thấy đối với các ngôn ngữ liên quan gần hoặc xa với tiếng Anh, giọng nói được tái tổng hợp
từ các token RVQ-1 có xu hướng mất thông tin âm sắc và giai điệu trong khi duy trì nội dung rõ ràng.
Giọng nói được tái tổng hợp được tạo ra từ các token RVQ-1:8 rất gần với thực tế. Điều đó cho thấy
SpeechTokenizer có thể đạt được việc tách biệt thông tin phân tầng trên ngôn ngữ chưa thấy, mặc dù
SpeechTokenizer chỉ được huấn luyện trên dữ liệu tiếng Anh. Chúng tôi tin rằng SpeechTokenizer có thể
có khả năng trích xuất nội dung từ giọng nói trong khi bỏ qua các đặc trưng phụ thuộc ngôn ngữ.
Khả năng này hứa hẹn cho việc phát triển SpeechTokenizer đa ngôn ngữ.

1https://0nutation.github.io/SpeechTokenizer.github.io/
19

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
H PHÂN TÍCH MELSPECTORGRAM
Chúng tôi vẽ melspectrogram của giọng nói thô, giọng nói được tái tổng hợp của các token EnCodec RVQ-1,
và giọng nói được tái tổng hợp của các token SpeechTokenizer RVQ-1. Từ hình 5, rõ ràng là
melspectrogram tương ứng với EnCodec RVQ-1 phần lớn giữ lại các sọc và hình dạng trong
melspectrogram thô. Ngược lại, giọng nói được tái tổng hợp từ SpeechTokenizer RVQ-1 về cơ bản
mất tất cả các sọc ngang, cho thấy thông tin âm sắc và giai điệu đã bị giảm bớt.

Hình 5: Melspectorgram của giọng nói thô, giọng nói được tái tổng hợp của các token SpeechTokenizer và EnCodec
RVQ-1.
20

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Chúng tôi cũng vẽ melspectrogram của giọng nói tiếng Đức thô và giọng nói tiếng Đức được tái tổng hợp
của các token SpeechTokenizer RVQ-1. Như thể hiện trong Hình 6, các mẫu tương tự được quan sát
trong giọng nói tiếng Anh cũng có mặt trong giọng nói tiếng Đức.

Hình 6: Melspectorgram của giọng nói tiếng Đức và giọng nói được tái tổng hợp của các token SpeechTokenizer RVQ-1.
21
