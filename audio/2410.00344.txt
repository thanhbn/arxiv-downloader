# 2410.00344.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/audio/2410.00344.pdf
# File size: 6199255 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
INTEGRATING TEXT-TO-MUSIC MODELS WITH LANGUAGE MODELS:
COMPOSING LONG STRUCTURED MUSIC PIECES
Lilac Atassi
University of California, San Diego
ABSTRACT
Recent music generation methods based on transformers have
a context window of up to a minute. The music generated
by these methods is largely unstructured beyond the context
window. With a longer context window, learning long-scale
structures from musical data is a prohibitively challenging
problem. This paper proposes integrating a text-to-music
model with a large language model to generate music with
form. The papers discusses the solutions to the challenges
of such integration. The experimental results show that the
proposed method can generate 2.5-minute-long music that is
highly structured, strongly organized, and cohesive.
Index Terms —Text-to-Music, generative models, musi-
cal form
1. INTRODUCTION
The new wave of generative models has been explored in the
literature to generate music. Jukebox [1] is based on Hierar-
chical VQ-V AEs [2] to generate multiple minutes of music.
Jukebox is one of the earliest purely learning-based models
that could generate longer than one minute of music with
some degree of structural coherence. Notably, the authors
mention that the generated music at a small scale of multiple
seconds is coherent, and at a larger scale, beyond one minute,
it lacks musical form.
Music Transformer [3] adapted the transformer architec-
ture for music generation, capable of processing the number
of input tokens equivalent to up to one minute of music. More
recent works on transformers and autoregressive models first
use an efficient encoder to compress the audio. In the com-
pressed space, the transformer model is trained to generate
new tokens. Then, the generated tokens are converted into
audio by a decoder. MusicGen [4] follows this approach.
The generative models in the literature have been applied
to both symbolic [5, 6, 7] and audio music [8, 9]. Most of the
recent larger models are trained on audio [4, 10], as collecting
a large training set of music audio is more readily available
than symbolic music.
Earlier works on generative music models, such as Juke-
Box, attempted to optimize the model to process long se-
quences of multiple minutes in the hope that the model would
Fig. 1 . The mean (top row) and the variance (bottom row)
of the fused self-similarity (SS) matrices estimated by 100
samples from Pond5, generated by our method, and by Mu-
sicGen. The SS matrices are downsampled to 5×5. The
results indicate that, compared to MusicGen, our method pro-
duces samples that more closely resemble the Pond5 samples
in terms of long-term temporal consistency and the diversity
of recurring sections.
learn musical structures and forms at all scales. However,
none of the models in the literature has demonstrated musi-
cal structure at a large scale and lack even simple musical
forms. Section 2 presents the arguement that it is impracti-
cal to have a sufficiently large training dataset. Using Large
Language Models (LLMs), this papers proposes an approach
to generate music with organized structures on larger scales.
Section 3 presents a brief review of MusicGen. Sections 4
discusses the proposed approach to integrate MusicGen with
ChatGPT [11]. The experiments and evaluation of our method
are presented in Section 5. Section 6 concludes the paper.
2. LEARNING MUSICAL FORM
Image generative models, such as those for drawing hands,
struggle to generate coherent images due to the extensive vari-
ability in the training data [12, 13]. This issue extends to other
structures with a high degree of variation. Figure 2 illustratesarXiv:2410.00344v3  [cs.SD]  5 Oct 2024

--- PAGE 2 ---
two other structures (mirrors and wavering flags) with signifi-
cant variation that three commercial image generators (Dall-E
3 [14], Midjourney, and Meta AI) fail to generate coherent im-
ages for. These generated images support the argument that
learning coherent structure in the presence of large variabil-
ity in the data manifold is a practical limitation. A solution
that has been explored with promising results is reducing the
variability by conditioning the model on a extra signal, for in-
stance using predicted 3d hand pose from text prompt to gen-
erate images with hands [12]. Our method follows a similar
approach.
To illustrate the problem that generative models struggle
with learning musical form, consider a simple case. A genera-
tive model using maximum likelihood estimation optimizes θ
to estimate the joint probability pθ(t1, t2)from the data sam-
ples with discrete and finite values. A parametric model can
estimate the joint probability if the training data samples lie
on a compact manifold. If there is a large amount of variation
int1among the samples, then pθ(t1, t2)reduces to pθ(t2)as
p(t1)becomes virtually uniform. With high dimensional data,
the problem of non-compact data manifold is compounded
with the fact that with an arbitrarily good model as judged
by average log-likelihood, −log[pθ(t1, . . . , t d)]≈d. The
optimization process minimizes the negative log-likelihood
by adjusting θfor the dimensions with less variation. In a
general musical corpus, with many parameters varying across
music pieces with the same musical form, the amount of vari-
ation at large temporal scales to learn musical form is large
enough that even simple musical forms are impossibly diffi-
cult to learn for generative models.
3. MUSICGEN
The most recent methods in the literature for music genera-
tion follow the approach of Stable Diffusion [15]. In this ap-
proach, the generative model is trained in the latent space of
an encoder. The generated vector is then decoded into audio
using the corresponding decoder. The main reason for train-
ing the generative model in the compressed latent is to reduce
the computational cost of generating long audio. In Music-
Gen, an autoregressive model is trained in the quantized latent
space of EnCodec [16] to model music.
To condition the MusicGen on text, the text prompt is en-
coded by T5 [17]. Classifier-free guidance (CFG) is used to
generate samples with text conditioning. MusicGen is trained
on 20K hours of music. Half of the training data is private
and internal at Meta. The other 10K hours of training mu-
sic is taken from ShutterStock7and Pond58vocals-free music
data collections.
6https://chatgpt.com/
6https://midjourney.com/
6https://meta.ai/
7https://www.shutterstock.com/music
8https://www.pond5.com4. CONTROLLING MUSICGEN BY A LANGUAGE
MODEL
As MusicGen is conditioned on text, it affords an interface
in natural language. Thus, an LLM can generate prompts for
MusicGen to replace human prompts. Therefore, it is possible
to task an LLM to design the structure of a song and gener-
ate prompts for each part to be generated by a text-to-music
model. This capability of an LLM to design music structure
and generate prompts is supported by its diverse knowledge
base [18], the reasoning [19], and learning capabilities [20].
The first challenge is aligning the LLM with the text-to-
music model. MusicGen has has been trained on brief music
descriptions that are not technical and adhere to a certain
style. The LLM needs to generate prompts that MusicGen
can interpret. To bridge this gap, there are two main ap-
proaches.One is fine-tuning or training a pre-trained LLM,
which involves adapting it directly to the task. Another ap-
proach is in-context learning, which offers the advantages
of requiring fewer samples and being less resource-intensive
when trying and evaluating different prompts. Notably, us-
ing a large number of samples for in-context learning has
been shown to outperform fine-tuning in terms of accuracy
[21, 22].
In-context learning is used to instruct ChatGPT to gen-
erate prompts for MusicGen by providing 50 song descrip-
tions from Pond5. To find the required number of samples,
it is estimated how often the generated music is faithful to
the text prompt to MusicGen. The empirical results show
that with about 10 song descriptions the generated by prompts
were often misinterpreted by MusicGen. Increasing the num-
ber of samples from 50 to 80 showed no improvement in the
interpretability of the prompts by MusicGen. As LLMs are
more prone to hallucination [23], more than necessary sam-
ples should be avoided.
ChatGPT tends to mix multiple genres in the prompts for
a single piece, resulting in structures that are not particularly
well-designed. However, when ChatGPT is directed to con-
sider a framework such as the ITPRA theory [24], the result-
ing structures are more organized and coherent. Additionally,
employing the chain of thought approach [25]—asking it to
first respond with a description of a song and its form, fol-
lowed by generating prompts for the parts—further improves
the organization of the musical structure. Additional rules are
added in the prompt to ChatGPT to introduce constraints spe-
cific to MusicGen. For instance, since MusicGen cannot gen-
erate parts that have ”a slower tempo” than a previous part,
ChatGPT is instructed to generate the prompts, the length of
each part in seconds, and the musical elements from prior
parts that should be used, all formatted in JSON. In the fol-
lowing prompt excerpt, some sections have been omitted for
brevity.
*Task*Assume you’re a musician. Your task is to write text
prompts for a system that generates music based on the

--- PAGE 3 ---
Dall-E
 Midjourney
 Meta AI
Fig. 2 . Illustrating the incoherence in images generated by Dall-E 34, Midjourney5, and Meta AI6. These inconsistencies are
evident in images featuring mirrors and wavering flags. Notice the forked or merged stripes on the flags and the inconsistent
reflection and incidence angles in mirrors, among the other inconsistencies.
given description of the music...
*Multishot examples *Below are some example prompts
that the system understands, along with the type of music it
can generate:
− ”a light and cheerful EDM track with syncopated drums,
airy pads, and strong emotions; bpm: 130”
...
*Constraints *Don’t limit yourself to these example prompts
... The music piece should be coherent and have a sense of
unity. Describe your thought process for the composition,
followed by the breakdown of the different parts... The
following are important constraints that your prompts must
satisfy:
1. The entire piece must be exactly 150 seconds long. You
will also decide the length of each part. 2. The prompt for
each part can reference another part... 14. To repeat a part
with variations in the chosen musical form, reference the
original part and, in the new prompt, explain what changed.
*Request Part 1 *Come up with 10 pieces, including the
form and a description of each part.
*Request Part 2 *... Provide the details of the parts for each
piece in JSON format: {PART NUMBER: [”PROMPT”,
LENGTH INSECONDS, REFERENCED PART],
PART NUMBER: [”PROMPT”, LENGTH INSECONDS,
REFERENCED PART], ... }
5. EVALUATION
Switching between prompts passed to MusicGen creates a
sudden jump. To ensure a smooth transition between parts,
the CFG method is modified. Instead of estimating one condi-
tioned probability distribution, two conditioned distributionsare estimated. In the interpolated probability distribution, the
weight of the first conditioned distribution over five seconds
goes from one to zero linearly, and the weight of the sec-
ond conditioned distribution increases from zero to one. This
method allows the model to generate tokens that facilitate a
continuous transition between parts.
Similarly, to generate parts with variations of the prior
parts, a 15-second audio prompt from a previous part is pro-
vided to MusicGen. This 15-second audio is passed to the
encoder of EnCodec to generate the corresponding tokens.
Then, the prompt tokens are passed to the predictive model
of MusicGen. The weight of the probability distribution over
tokens, conditioned on these audio prompt tokens, is then lin-
early reduced to zero over 10 seconds.
Figure 3 compares the self similarity matrices (SSM) of
samples from MusicGen, our method, and Pond5. The fusion
method proposed in [26] is used to generate the combined
SSMs. The SSMs of the sample from our method resem-
ble the variation and similarity structure in the sample from
Pond5.
After downsampling the fused SS matrices of 100 sam-
ples, the mean and variance matrices are estimated for Pond5,
our method, and MusicGen in Figure 1. Therefore, this figure
helps to compare the distributions of the music structures. It
is apparent that, from the mean and variation values from the
top-right of the matrices, the MusicGen samples do not have
parts near the end of each piece that resemble the parts near
the beginning. In contrast, the samples from our method are
more similar to the samples from Pond5. The Fr ´echet distance
between the two pairs of distributions is estimated, but instead
of using Inception feature vectors [27], the mean and covari-
ance matrices of the upper triangle of the fused SS matrices
are used in the distance estimation. The Fr ´echet distances

--- PAGE 4 ---
Fig. 3 . Visualizing the self similarity matrices for 3 MusicGen
samples, one sample from our method and one from Pond5.
With MusicGen, at a low temperature (T) of 0.1, the music
is repetetive. At T=5.0, there is mostly random noise. At
T=1, the music is meandering. The sample from our method
resembles the one from Pond5, composed and arranged by a
musician.
between the the distributions of the Pond5 samples and ours
is 0.086, and between the Pond5 and MusicGen samples is
0.108, supporting the claim the structure of the samples gen-
erated by our method are more similar to the samples com-
posed by musicians from Pond5.
For a subjective evaluation by non-musicians, 10 samples
are generated using our method, 10 samples using MusicGen,
and 10 samples from Pond5. Each of these samples is 2.5
minutes long. Using Amazon Mechanical Turk (MTurk), a
mean opinion score (MOS) between 1 and 5 for each sam-
ple from 10 non-musicians is collected. The human raters
are asked to evaluate the overall quality of the music, fol-
lowing the recommended practices in CrowdMOS [28]. The
subjects are told a score of 1 means they dislike the music
and would not like to listen to similar music. A score of 5
means they find the music interesting and would like to listen
to similar music. Given long music tracks with more orga-
nized structure are expected to be more engaging, the likeness
of the samples is used as a proxy for the improved structure
Fig. 4 . Left: The subjective comparison of the generated
music and sampled from Pond5 by non-musicians is mea-
sured through the MOS based on how engaging the music
is. Whiskers: 95% CI. Right: The subjective comparison of
the samples by musicians, critiquing the musical structures.
and form. The results in Figure 4(top) indicate that adding
musical form through our method to MusicGen improves the
perceived quality of the music, almost on par with human-
composed music pieces from Pond5.
In a separate subjective evaluation, three professional mu-
sicians with doctoral degree in performance or composition
are asked to listen to three samples from our method and three
samples from MusicGen. They assign a score to each sample,
with the score guideline as follows: 1: No form, music me-
anders; 2: Minimal structure, some recognizable patterns but
largely unstructured; 3: Moderate structure, clear sections but
not highly organized; 4: Clear form, well-organized sections
and transitions; 5: Very clear and highly structured, strongly
organized and cohesive. The average scores are presented in
Figure 4(bottom).
6. CONCLUSIONS
This paper argues that due to the discussed nature of the
problem, the music generative models cannot learn long-
scale structure or musical form from a musical datasets. Then
a novel method to combine music generative models with
LLMs to generate music with a musical form is presented.
The technical challenges are discussed. The subjective and
objective evaluations support the claim that our method can
generate well-structured 2.5-minute music.
7. REFERENCES
[1] Prafulla Dhariwal, Heewoo Jun, Christine Payne, et al.,
“Jukebox: A generative model for music,” arXiv
preprint arXiv:2005.00341 , 2020.
[2] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals,
“Generating diverse high-fidelity images with vq-vae-
2,”NeurIPS , vol. 32, 2019.

--- PAGE 5 ---
[3] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-
reit, et al., “Music transformer: Generating music with
long-term structure,” in ICLR , 2018.
[4] Jade Copet, Felix Kreuk, Itai Gat, et al., “Simple and
controllable music generation,” NeurIPS , vol. 36, 2024.
[5] Hongfei Wang, Yi Zou, Haonan Cheng, and Long Ye,
“Diffuseroll: multi-track multi-attribute music genera-
tion based on diffusion model,” Multimedia Systems ,
vol. 30, no. 1, pp. 19, 2024.
[6] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian
Simon, “Symbolic music generation with diffusion
models,” arXiv preprint arXiv:2103.16091 , 2021.
[7] Jeffrey AT Lupker, “Score-transformer: A deep learning
aid for music composition,” in NIME 2021 , 2021.
[8] Teysir Baoueb, Haocheng Liu, Mathieu Fontaine, et al.,
“Specdiff-gan: A spectrally-shaped noise diffusion gan
for speech and music synthesis,” in ICASSP 2024 . IEEE,
2024, pp. 986–990.
[9] Curtis Hawthorne, Ian Simon, Adam Roberts, et al.,
“Multi-instrument music synthesis with spectrogram
diffusion,” arXiv preprint arXiv:2206.05408 , 2022.
[10] Andrea Agostinelli, Timo I Denk, Zal ´an Borsos, et al.,
“Musiclm: Generating music from text,” arXiv preprint
arXiv:2301.11325 , 2023.
[11] Josh Achiam, Steven Adler, Sandhini Agarwal,
et al., “Gpt-4 technical report,” arXiv preprint
arXiv:2303.08774 , 2023.
[12] Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xi-
ang Chen, et al., “Handiffuser: Text-to-image gener-
ation with realistic hand appearances,” arXiv preprint
arXiv:2403.01693 , 2024.
[13] Haozhuo Zhang, Bin Zhu, Yu Cao, and Yanbin Hao,
“Hand1000: Generating realistic hands from text with
only 1,000 images,” arXiv preprint arXiv:2408.15461 ,
2024.
[14] James Betker, Gabriel Goh, Li Jing, et al., “Improving
image generation with better captions,” Computer Sci-
ence. https://cdn. openai. com/papers/dall-e-3. pdf , vol.
2, no. 3, pp. 8, 2023.
[15] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer, “High-resolution im-
age synthesis with latent diffusion models,” in CVPR ,
2022, pp. 10684–10695.
[16] Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and
Yossi Adi, “High fidelity neural audio compression,”
arXiv preprint arXiv:2210.13438 , 2022.[17] Colin Raffel, Noam Shazeer, Adam Roberts, et al., “Ex-
ploring the limits of transfer learning with a unified text-
to-text transformer,” JMLR , vol. 21, no. 140, pp. 1–67,
2020.
[18] Lukas Schulze Balhorn, Jana M Weber, Stefan Buijs-
man, et al., “Empirical assessment of chatgpt’s answer-
ing capabilities in natural science and engineering,” Sci-
entific Reports , vol. 14, no. 1, pp. 4998, 2024.
[19] Taylor Webb, Keith J Holyoak, and Hongjing Lu,
“Emergent analogical reasoning in large language mod-
els,” Nature Human Behaviour , vol. 7, no. 9, pp. 1526–
1541, 2023.
[20] Tom Brown, Benjamin Mann, Nick Ryder, et al., “Lan-
guage models are few-shot learners,” NeurIPS , vol. 33,
pp. 1877–1901, 2020.
[21] Rishabh Agarwal, Avi Singh, Lei M Zhang, et al.,
“Many-shot in-context learning,” arXiv preprint
arXiv:2404.11018 , 2024.
[22] Amanda Bertsch, Maor Ivgi, Uri Alon, et al., “In-
context learning with long-context models: An in-depth
exploration,” arXiv preprint arXiv:2405.00200 , 2024.
[23] Xi Fang, Weijie Xu, Fiona Anting Tan, et al., “Large
language models on tabular data–a survey,” arXiv
preprint arXiv:2402.17944 , 2024.
[24] David Huron, Sweet anticipation: Music and the psy-
chology of expectation , MIT press, 2008.
[25] Jason Wei, Xuezhi Wang, Dale Schuurmans, et al.,
“Chain-of-thought prompting elicits reasoning in large
language models,” NeurIPS , vol. 35, pp. 24824–24837,
2022.
[26] Christopher J Tralie and Brian McFee, “Enhanced hi-
erarchical music structure annotations via feature level
similarity fusion,” in ICASSP . IEEE, 2019, pp. 201–
205.
[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter, “Gans trained
by a two time-scale update rule converge to a local nash
equilibrium,” NeurIPS , vol. 30, 2017.
[28] Fl ´avio Ribeiro, Dinei Flor ˆencio, Cha Zhang, and
Michael Seltzer, “Crowdmos: An approach for crowd-
sourcing mean opinion score studies,” in ICASSP . IEEE,
2011, pp. 2416–2419.
