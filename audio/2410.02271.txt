# 2410.02271.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/audio/2410.02271.pdf
# File size: 441422 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CoLLAP: Contrastive Long-form Language- Audio Pretraining
with Musical Temporal Structure Augmentation
Junda Wu
Computer Science and Engineering
UC San Diego
La Jolla, USA
juw069@ucsd.edu
Amit Namburi
Computer Science and Engineering
UC San Diego
La Jolla, USA
anamburi@ucsd.eduWarren Li
Computer Science and Engineering
UC San Diego
La Jolla, USA
wyl003@ucsd.edu
Carol Chen
Computer Science Department
UC Los Angeles
Los Angeles, USA
carolchen12@ucla.eduZachary Novack
Computer Science and Engineering
UC San Diego
La Jolla, USA
znovack@ucsd.edu
Julian McAuley
Computer Science and Engineering
UC San Diego
La Jolla, USA
jmcauley@ucsd.edu
Abstract ‚ÄîModeling temporal characteristics plays a significant role in
the representation learning of audio waveform. We propose Contrastive
Long-form Language-Audio Pretraining (CoLLAP) to significantly ex-
tend the perception window for both the input audio (up to 5 minutes)
and the language descriptions (exceeding 250 words), while enabling con-
trastive learning across modalities and temporal dynamics. Leveraging
recent Music-LLMs to generate long-form music captions for full-length
songs, augmented with musical temporal structures, we collect 51.3K
audio-text pairs derived from the large-scale AudioSet training dataset,
where the average audio length reaches 288 seconds. We propose a novel
contrastive learning architecture that fuses language representations with
structured audio representations by segmenting each song into clips and
extracting their embeddings. With an attention mechanism, we capture
multimodal temporal correlations, allowing the model to automatically
weigh and enhance the final fusion score for improved contrastive
alignment. Finally, we develop two variants of the CoLLAP model with
different types of backbone language models. Through comprehensive
experiments on multiple long-form music-text retrieval datasets, we
demonstrate consistent performance improvement in retrieval accuracy
compared with baselines. We also show the pretrained CoLLAP models
can be transferred to various music information retrieval tasks, with
heterogeneous long-form multimodal contexts.
I. I NTRODUCTION
The ability to effectively model temporal characteristics is essential
in the representation learning of audio waveforms, especially for
complex and full-length music tracks. Music information retrieval
works [1], [2] have studied approaches to extract musical temporal
and structural information, which can be further used to augment
models‚Äô music understanding abilities [3]. The recent contrastive
learning approaches [4]‚Äì[6] enable to extract such information as
latent audio representations, which are trained to distinguish between
matched text-audio pairs and other mismatched pairs by capturing dis-
tinctive features in the audio data (illustrated in Figure 1a). However,
such methods have focused on relatively short segments, limiting the
model‚Äôs ability to handle longer, more nuanced sequences.
To address these challenges, we introduce Contrastive Long-form
Language-audio Pretraining (CoLLAP), which extends the perception
window to handle both long-form audio inputs and detailed language
descriptions. We illustrate the comparison between the conventional
CLAP model and our proposed CoLLAP model in Figure 1. The
CoLLAP model uses a feature extractor to segment music tracks into
frames and encode each by a kernel function. Then kernel-wise and
temporal attention mechanisms are employed to measure global and
temporal alignment between audio and text. Finally, the model is
segments & down-sampleAudio EncoderMel-FilterBankAttention Feature FusionOther Encoder Layer1D Audio EmbeddingAudio Clip < 30-secondsShort Music Caption < 50-words: The low-quality recording features footsteps, birds chirping, distant traffic and wind sound effects. RoBERTa1D Text EmbeddingCosine Distance(a) Illustration of conventional CLAP model, whose inputs include short
music captions (less than 50 words) and short audio clips (less than 30
seconds). CLAP only extracts 1-dimensional global textual and audio
embeddings to calculate cosine similarity.
Kernel-wise & Temporal Attention Average Aggregated AttentionMusic Track > 4 minutesLong-form Text > 250-words: The song has a mellow and calming mood‚Ä¶ ‚Äú0-27% Intro & Verse‚Äù: The music begins with a slow, sustained note from the bansuri‚Ä¶ ‚Äú68-100% Outro‚Äù: ‚Ä¶RoBERTa/GPT21D Text EmbeddingCosine DistanceWhisperFeature AdapterKernelizationBeats
3D Audio Embedding
(b) Illustration of our proposed CoLLAP model, whose inputs include
fine-grained and temporally-aware music descriptions (more than 250
words) and full-length music tracks (more than 4 minutes). CoLLAP
extracts 3-dimensional audio embeddings and aggregates using 3D-
attention pooling that explicitly models temporal attention. We also enable
two variants of CoLLAP using different language backbones RoBERTa
and GPT2.
Fig. 1: Comparison of conventional CLAP (Figure 1a) and our
proposed CoLLAP (Figure 1b).
optimized with contrastive learning using weighted similarity scores
from both kernel-wise and temporal attention. CoLLAP effectively
extends the perception window for both the input audio (up to 5
minutes) and the language descriptions (exceeding 250 words), which
enables retrieval of full-length music tracks with fine-grained music
descriptions.arXiv:2410.02271v1  [cs.SD]  3 Oct 2024

--- PAGE 2 ---
To enable large-scale contrastive pretraining of CoLLAP, we lever-
age a Music-LLM augmented dataset of 51.3K audio-text pairs and
4,109 hours of audio, derived from the large-scale AudioSet training
data, with an average audio length of 288 seconds and an average text
length of 256 words. In addition, we develop two variants of CoLLAP
based on two different backbone language models, Roberta-base [7]
and GPT2 [8].
Finally, we conduct comprehensive experiments on multiple long-
form music-text retrieval datasets and observe consistent improve-
ment in retrieval accuracy of CoLLAP compared with baseline mod-
els. We also evaluate CoLLAP‚Äôs transfer learning ability in various
music information retrieval tasks that involve heterogeneous long-
form multimodal contexts, including speech audio and Wikipedia
free-form long-context. In addition, we also observe better generaliz-
ability in the CoLLAP-GPT2 variant compared to RoBERTa model
backbone due to the GPT2 model‚Äôs better language modeling ability
of long-context. We summarize our contributions as follows:
‚Ä¢We propose the Contrastive Long-form Language-audio Pre-
training (CoLLAP) model for multimodal fusion and represen-
tation learning of long-form audio and language descriptions.
‚Ä¢We design a novel fusion mechanism that combines structured
audio and language representations, leveraging attention to cap-
ture and weigh multimodal temporal correlations for improved
contrastive alignment.
‚Ä¢We augment a dataset of 4,109 hours of long-form full-length
music tracks, paired with musical structural augmented captions
generated by Music-LLMs.
‚Ä¢Through comprehensive experiments we demonstrate that CoL-
LAP consistently outperforms baseline models in long-form
text-audio retrieval, and show its generalizability across different
tasks.
II. C OLLAP: M ODEL DESIGN AND LEARNING
We illustrate our CoLLAP model design in Figure 2, where
full-length music track waveform is processed with a dual-feature
extractor, while textual representations are extracted from musical
structure augmented captions. We split music tracks of variable
lengths into frames to enable audio temporal attention with texts,
which extracts and measures both the global and temporal multimodal
alignment scores. With the temporal attention augmented alignment
scores, we follow the conventional contrastive learning scheme [4],
[6], [9], [10], where the contrastive loss will be propagated back to
both the temporal attention and the feature extractors.
A. Text and Dynamic Audio Encoders
Given Ninput audio-text pairs {(Xi, Yi)}i<N, we extract the
textual embeddings Ti‚ààRD, musical embeddings Oi‚ààRD, and
speech embeddings Si‚ààRDas follows:
Ti=fT(Yi;Œ∏T), O i=fO(Xi;Œ∏O), S i=fS(Xi;Œ∏S),
where the model parameters of the text encoder Œ∏Tare initialized
from a pre-trained language model ( e.g., RoBERTa [7] and GPT-2
[8]), while the music encoder and speech encoder are adapted from
BEATS [11] and Whisper [12] models. We fuse the musical and
speech embeddings by an audio feature adapter linear layer hA,
Ui=hA([Oi, Si]), I < N.
11111ùêº!",$%!,&ùêº'",$%",&ùêº(",$%#,&ùêº)",$%$,&‚Ä¶ùëá!ùëá'ùëá(ùëá)‚Ä¶
BeatsWhisperMusical Structural Augmented TextsAdapterKernalFunction
ùêª√óùëä*√óùê∑ùëñ<ùëõ
AudioWaveformText EncoderAdapterùê∑
ùêª√óùëä*
ùê¥!,#$√ó	softmaxùê¥!,‚ãÖ$softmaxùê¥‚ãÖ,#$√ó	ùê¥!,#$ùêªùëä*AveragePooling
Fig. 2: The model overview of CoLLAP. The input of backbone
language models is musical structural augmented texts, while audio
waveform is encoded by the dual-feature extractor of Beats and
Whisper models. The encoded multimodal features are used for the
calculation of temporal and kernel-wise attentions before computing
contrastive learning loss.
Then, we split the unified audio representation with a length of T
into consecutive frames with a kernel function with a kernel size of
Hand stride step of ST,
H=T¬∑Œ∑K
30
, S T=T¬∑Œ∑S
30
,
where Œ∑Kis pre-defined to determine how many seconds per frame,
andŒ∑Sdetermines seconds per stride. Finally, the processed audio
representation is unfolded and reshaped to Ii={Iv,h
i}Wi,H‚àà
RH√óWi√óD,
Ii=Unfold (Ui, H, S T),where Wi=T‚àíH
ST+ 1
. (1)
With the audio tokenized with fixed-length frames Ii={Iv,h
i}Wi,H,
we can calculate kernel-wise attention and temporal attention to
augment the multimodal alignment estimation.
B. Multimodal and Temporal Attention Augmentation
Given the audio representation Ii={Iv,h
i}Wi,Hand the text
representation Tj, we calculate their cosine similarity
Mi,j={(Iv,h
i)‚ä§Tj}Wi,H, (2)
in each frame v < W iand each kernel h < H . To further measure
the text‚Äôs attention on the individual frame and kernel, we calculate

--- PAGE 3 ---
the kernel-wise attention AK
i,jand temporal attention AT
i,j,
AK
i,j(v, h) =eMi,j(v,h)
P
k<HeMi,j(v,k), (3)
AT
i,j(v, h) =eMi,j(v,h)
P
l<WieMi,j(l,h), (4)
where Mi,j(v, h)is the corresponding cosine similarity score of the
v-th frame and h-th kernel in Mi,j.
C. Temporal Attention Fused Contrastrive Learning
Then we use the calculated kernel-wise attention AK
i,jand temporal
attention AT
i,jto weigh and sum the original cosine similarity matrix
Mi,j. To obtain the global similarity between the text and audio,
Mi,jis weighted by the kernel-wise attention AK
i,jwith an average
pooling,
rK
i,j=1
HX
k<HX
l<WiMi,j(k, l)¬∑AK
i,j(k, l). (5)
To capture the temporal attention-weighted similarity between text
and audio, we further derive the similar similarity score,
rT
i,j=1
WiX
l<WiX
k<HMi,j(k, l)¬∑AT
i,j(k, l). (6)
Finally, we compose the two weighted similarity scores with two
scalers Œ≥KandŒ≥Tfor balance. Therefore, each pairwise cosine
similarity score ri,j‚ààRN√óNin the mini-batch is calculated as
ri,j=Œ≥K¬∑rK
i,j+Œ≥T¬∑rT
i,j. (7)
Following [4], [5], [9], we adopt the conventional contrastive loss
function to derive the final loss,
L=‚àíX
i<Nlogeri,i
P
j<Neri,j, (8)
where the contrastive loss will be propagated back to both the
temporal attention and the feature extractors.
III. L ONG -FORM AND STRUCTURAL -AWARE TEXT-AUDIO
RETRIEVAL DATASET
We collect a large-scale long-form audio waveform dataset derived
from the full-length tracks from the training subset of AudioSet [13].
We filter out audio tracks whose lengths are either less than 2 minutes
or longer than 5 minutes, accumulating to a total of 51.3K and
4,109.50hours of audio tracks with an average length of 288.25
seconds per track. To further pair the full-length audio tracks with
long-form and fine-grained captions that comprehensively describe
the entire track, we leverage the FUTGA model [3] to generate dense
captions, which provides both global caption and temporally-aware
structural information. Therefore, the generated dense captions have
an average of 256.94words for each caption.
TABLE I: Comparison of the statistics of existing text-music retrieval
datasets and CoLLAP.
Dataset PairsAudio (hrs)
DurationAve. (secs)
DurationAve.
Words
AudioCaps [14] 51k 144.9 10.23 9.0
MusicCaps [15] 6k 15.3 10.00 48.9
LAION-Audio [6] 633.5k 4325.39 24.58 ‚Äì
LP-MusicCaps [16] 514k 4283.10 30.00 37.3
CoLLAP 51.3k 4109.50 288.25 256.94We compare our collected long-form and structural-aware text-
audio retrieval dataset in Table I, where we show that our dataset has
a comparable total length of existing large-scale text-audio datasets
(e.g., LAION-Audio [6] and LP-MusicCaps [16]). In addition, we
demonstrate that our audio lengths are about ten times longer than
the existing dataset on average, while our average text length is about
five times longer than the fine-grained MusicCaps [15].
IV. E XPERIMENTS
A. Implementation Details
We implement the CoLLAP model using PyTorch 2.2 framework,
leveraging pre-trained RoBERTa and GPT-2 models for the text
encoder and adapting BEATS and Whisper models for the music and
speech encoders, respectively. We collect 51.3K long-form audio-text
pairs derived from the original AudioSet-train dataset [13], with an
average audio duration of 288 seconds and a text length of 257 words.
We initialize RoBERTa or GPT-2 for the text encoder with pre-
trained weights. The music and speech encoders are respectively
adapted from BEATS and Whisper models and concatenated as the
fused audio embedding. The fused textual and audio embedding sizes
are set to 512. We fine-tuned the full parameters of both the text
encoder and audio embeddings, using an AdamW optimizer with a
learning rate of 1e‚àí4and weight decay of 1e‚àí5. We use a batch
size of 50and enable in-batch contrastive learning loss implemented
by a cross-entropy loss function. The contrastive learning process is
set for 20 epochs, with a linear learning rate scheduling. The training
process leverages 2 NVIDIA A100 GPUs with 40GB of memory.
B. Datasets and Baselines
We evaluate the CoLLAP model on three text-audio retrieval tasks,
where four datasets, SongDescriber [17], MusicCaps [15], AudioSet-
Eval [13], and HarmonixSet [18], are used for general long-form text-
to-audio retrieval. To test the retrieval accuracy in the speech domain,
we evaluate the VCTK dataset [19] for long-context transcript to full
speech retrieval. Finally, we further evaluate the model‚Äôs zero-shot
generalizability in free-form music context collected from Wikipedia
pages and enable wiki-to-music retrieval.
We compare our proposed CoLLAP model with three contrastive
learning baselines for the main experiment in Table II: HSTAT
(RoBERTa) [6] employs RoBERTa for textual encoding and incorpo-
rates the feature fusion mechanism and keyword-to-caption augmen-
tation; Larger CLAP [6] further enhances the model performance
on music and speech domains by expanded pre-training; Cacophony
[5] enhances by a hierarchical attention mechanism and advanced
fusion techniques to dynamically combine multi-scale features from
both modalities. For our method, we develop two model variants
CoLLAP (RoBERTa) and CoLLAP (GPT2) using two different
language model backbones.
C. Long-form Text-audio Retrieval
The long-form text-audio retrieval experiments are designed to
evaluate the effectiveness of the CoLLAP model in aligning extended
audio tracks with their corresponding textual descriptions. Retrieval
performance is measured using recall at ranks 5, 20, and 100 for both
text-to-audio (T2A) and audio-to-text (A2T) retrieval tasks.
As presented in Table II, the CoLLAP variants outperform the
baseline models across all datasets, particularly on SongDescriber
and HarmonixSet. The attention mechanisms in CoLLAP enable
the model to effectively capture the temporal and multimodal cor-
relations, leading to significant improvements in retrieval accuracy.
The RoBERTa-based CoLLAP variant demonstrates slightly higher
performance, especially in A2T retrieval tasks.

--- PAGE 4 ---
TABLE II: The retrieval performance of three variants of Larger-CLAP and two variants of CoLLAP on four evaluation datasets. We report
recall values of rank 5, 20, and 100 for text-to-music (T2M) and music-to-text (M2T) retrieval. The best values are highlighted in bold fonts,
while the second-best values are underlined.
Dataset SongDescriber MusicCaps AudioSet-Eval HarmonixSetAverage
Model Metric T2A A2T T2A A2T T2A A2T T2A A2T
HSTAT
(RoBERTa)R@5 3.12 5.95 2.67 3.18 4.85 3.74 1.54 2.14 3.40
R@5 11.33 17.85 8.01 10.16 13.88 13.88 6.06 7.96 11.14
R@5 41.64 49.58 28.75 30.90 43.17 44.05 23.87 25.89 35.98
Larger
CLAPR@5 6.65 9.77 3.11 4.97 5.73 8.37 10.48 9.05 7.27
R@20 16.86 25.92 9.86 15.27 15.86 21.81 25.24 28.10 19.87
R@100 52.97 63.88 31.50 42.70 48.68 55.29 75.71 76.67 55.93
CacophonyR@5 5.92 5.14 2.15 2.67 4.20 2.91 2.38 0.48 3.23
R@20 16.75 18.73 5.93 6.15 8.88 9.36 8.79 2.14 9.59
R@100 48.70 62.64 19.35 24.69 35.97 38.84 34.20 10.21 34.33
CoLLAP
(RoBERTa)R@5 50.28 40.50 15.19 9.54 72.68 75.55 21.37 19.35 38.05
R@20 75.92 70.25 36.65 20.53 91.18 91.85 40.73 39.66 58.34
R@100 96.60 93.34 69.50 43.73 98.89 99.11 74.10 71.25 80.81
CoLLAP
(GPT-2)R@5 49.15 42.91 17.35 10.26 76.87 79.51 20.42 18.76 39.40
R@20 77.19 69.12 36.96 21.35 92.95 93.61 41.33 40.49 58.12
R@100 97.16 93.20 69.50 44.86 99.77 99.77 76.12 73.75 81.76
D. Zero-shot Transcript-speech Retrieval
We also evaluate CoLLAP‚Äôs zero-shot transfer performance on
transcript-speech retrieval tasks using the VCTK dataset. This exper-
iment assesses the model‚Äôs capability to align spoken content with
corresponding transcripts without additional fine-tuning. Table III
reports retrieval performance for both T2A and A2T tasks at various
recall ranks.
The results indicate that the CoLLAP model variants maintain
robust retrieval accuracy in this zero-shot setting. The GPT-2 based
variant outperforms the RoBERTa-based variant, suggesting that
GPT-2‚Äôs generative capabilities may better handle the variability in
spoken language. These findings highlight CoLLAP‚Äôs potential for
applications in speech recognition and audio-text alignment.
TABLE III: Speech and audio retrieval on the VCTK dataset [20].
We report Recall@ kmetrics for text-to-audio (T2A) and audio-to-
text (A2T) retrieval.
Model HSTAT
(RoBERTa)Larger
CLAPCoLLAP
(RoBERTa)CoLLAP
(GPT2) Dataset Metric
VCTK
(T2A)R@5 0.87 1.22 0.87 1.40
R@20 4.03 4.38 3.50 5.96
R@100 18.94 19.47 15.78 21.75
VCTK
(A2T)R@5 0.87 1.40 0.70 1.75
R@20 3.33 5.43 3.15 5.61
R@100 18.59 18.42 16.31 21.05
E. Zero-shot Wiki-music Retrieval
Finally, we assess CoLLAP‚Äôs generalizability in retrieving music-
related content from textual descriptions in a zero-shot manner using
the Wiki-music dataset. This dataset includes Wikipedia articles
paired with audio clips, and the task involves retrieving the correct
audio clip given a text query and vice versa. The retrieval performance
is detailed in Table IV.
CoLLAP achieves significant gains over the baseline models in
the Wiki-SD and Wiki-MC tasks. The model‚Äôs attention mechanismsallow it to effectively align long-form text with corresponding audio
segments, leading to improved retrieval accuracy. These results sug-
gest that CoLLAP can be effectively transferred to diverse music-
related information retrieval tasks, making it a versatile tool for
exploring large-scale multimodal datasets.
TABLE IV: Wikipedia context and audio retrieval on the MusicCaps
and SongDescriber datasets. We report Recall@ kmetrics for wiki-
to-music (W2M) and music-to-wiki (M2W) retrieval.
Dataset Wiki-SD Wiki-MCAverage
Model Metric W2M M2W W2M M2W
HSTAT
(RoBERTa)R@5 3.12 4.67 3.90 3.80 3.87
R@20 9.92 14.59 10.47 10.68 11.41
R@100 37.82 43.63 31.42 27.93 35.20
Larger
CLAPR@5 4.24 8.92 4.10 6.05 5.83
R@20 14.73 25.21 13.24 17.14 17.58
R@100 45.60 55.52 39.73 43.32 46.05
CoLLAP
(RoBERTa)R@5 39.51 34.70 9.03 7.08 22.59
R@20 60.33 59.77 24.33 14.78 39.81
R@100 79.74 75.21 49.58 35.31 59.97
CoLLAP
(GPT-2)R@5 39.37 36.96 9.75 7.90 23.50
R@20 61.18 57.93 22.68 14.57 39.10
R@100 80.45 74.64 46.61 33.05 58.69
V. C ONCLUSION
In this paper, we introduce CoLLAP, a novel contrastive learn-
ing framework designed for long-form language-audio representa-
tion learning. Our model leverages dual-feature extraction and a
multimodal attention mechanism to effectively capture both global
and temporal alignments between lengthy audio tracks and detailed
textual descriptions. Through comprehensive experiments across
multiple datasets, including SongDescriber, MusicCaps, AudioSet-
Eval, HarmonixSet, and Wiki-music, we demonstrate that CoLLAP
significantly improves retrieval performance over existing baseline
models.

--- PAGE 5 ---
REFERENCES
[1] N. Whiteley, A. T. Cemgil, and S. J. Godsill, ‚ÄúBayesian modelling of
temporal structure in musical audio.‚Äù in ISMIR , 2006, pp. 29‚Äì34.
[2] R. J. Weiss and J. P. Bello, ‚ÄúUnsupervised discovery of temporal struc-
ture in music,‚Äù IEEE Journal of Selected Topics in Signal Processing ,
vol. 5, no. 6, pp. 1240‚Äì1251, 2011.
[3] J. Wu, Z. Novack, A. Namburi, J. Dai, H.-W. Dong, Z. Xie, C. Chen,
and J. McAuley, ‚ÄúFutga: Towards fine-grained music understanding
through temporally-enhanced generative augmentation,‚Äù arXiv preprint
arXiv:2407.20445 , 2024.
[4] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, ‚ÄúClap learning
audio concepts from natural language supervision,‚Äù in ICASSP 2023-
2023 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) . IEEE, 2023, pp. 1‚Äì5.
[5] G. Zhu and Z. Duan, ‚ÄúCacophony: An improved contrastive audio-text
model,‚Äù arXiv preprint arXiv:2402.06986 , 2024.
[6] Y . Wu, K. Chen, T. Zhang, Y . Hui, T. Berg-Kirkpatrick, and S. Dubnov,
‚ÄúLarge-scale contrastive language-audio pretraining with feature fusion
and keyword-to-caption augmentation,‚Äù in ICASSP 2023-2023 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2023, pp. 1‚Äì5.
[7] Y . Liu, ‚ÄúRoberta: A robustly optimized bert pretraining approach,‚Äù arXiv
preprint arXiv:1907.11692 , 2019.
[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog ,
vol. 1, no. 8, p. 9, 2019.
[9] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al. , ‚ÄúLearning transferable
visual models from natural language supervision,‚Äù in International
conference on machine learning . PMLR, 2021, pp. 8748‚Äì8763.
[10] Y . Yuan, Z. Chen, X. Liu, H. Liu, X. Xu, D. Jia, Y . Chen, M. D. Plumb-
ley, and W. Wang, ‚ÄúT-clap: Temporal-enhanced contrastive language-
audio pretraining,‚Äù arXiv preprint arXiv:2404.17806 , 2024.
[11] S. Chen, Y . Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, and F. Wei,
‚ÄúBeats: Audio pre-training with acoustic tokenizers,‚Äù arXiv preprint
arXiv:2212.09058 , 2022.
[12] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, ‚ÄúRobust speech recognition via large-scale weak supervi-
sion,‚Äù in International conference on machine learning . PMLR, 2023,
pp. 28 492‚Äì28 518.
[13] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
Moore, M. Plakal, and M. Ritter, ‚ÄúAudio set: An ontology and human-
labeled dataset for audio events,‚Äù in 2017 IEEE international conference
on acoustics, speech and signal processing (ICASSP) . IEEE, 2017, pp.
776‚Äì780.
[14] C. D. Kim, B. Kim, H. Lee, and G. Kim, ‚ÄúAudiocaps: Generating
captions for audios in the wild,‚Äù in Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers) , 2019, pp. 119‚Äì132.
[15] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon,
Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al. , ‚ÄúMusiclm:
Generating music from text,‚Äù arXiv preprint arXiv:2301.11325 , 2023.
[16] S. Doh, K. Choi, J. Lee, and J. Nam, ‚ÄúLp-musiccaps: Llm-based pseudo
music captioning,‚Äù arXiv preprint arXiv:2307.16372 , 2023.
[17] I. Manco, B. Weck, S. Doh, M. Won, Y . Zhang, D. Bodganov, Y . Wu,
K. Chen, P. Tovstogan, E. Benetos et al. , ‚ÄúThe song describer dataset:
a corpus of audio captions for music-and-language evaluation,‚Äù arXiv
preprint arXiv:2311.10057 , 2023.
[18] O. Nieto, M. C. McCallum, M. E. Davies, A. Robertson, A. M. Stark,
and E. Egozy, ‚ÄúThe harmonix set: Beats, downbeats, and functional
segment annotations of western popular music.‚Äù in ISMIR , 2019, pp.
565‚Äì572.
[19] C. Veaux, J. Yamagishi, and S. King, ‚ÄúThe voice bank corpus: Design,
collection and data analysis of a large regional accent speech database,‚Äù
in2013 international conference oriental COCOSDA held jointly with
2013 conference on Asian spoken language research and evaluation (O-
COCOSDA/CASLRE) . IEEE, 2013, pp. 1‚Äì4.
[20] C. Veaux, J. Yamagishi, K. MacDonald et al. , ‚ÄúSuperseded-cstr vctk
corpus: English multi-speaker corpus for cstr voice cloning toolkit,‚Äù
2016.
