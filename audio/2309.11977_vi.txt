# 2309.11977.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/audio/2309.11977.pdf
# Kích thước tập tin: 293878 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
CẢI THIỆN TỔNG HỢP CHUYỂN ĐỔI VĂN BẢN THÀNH GIỌNG NÓI ZERO-SHOT DỰA TRÊN MÔ HÌNH NGÔN NGỮ
VỚI CÁC PROMPT ÂM THANH ĐA QUY MÔ
Shun Lei1‡, Yixuan Zhou1†, Liyang Chen1, Dan Luo1, Zhiyong Wu1,3∗, Xixin Wu3∗,
Shiyin Kang2∗, Tao Jiang2, Yahui Zhou2, Yuxing Han1, Helen Meng3
1Shenzhen International Graduate School, Tsinghua University, Shenzhen
2Skywork AI PTE. LTD., Beijing3The Chinese University of Hong Kong, Hong Kong SAR
{leis21, yx-zhou23 }@mails.tsinghua.edu.cn, zywu @sz.tsinghua.edu.cn, wuxx @se.cuhk.edu.hk, shiyin.kang @kunlun-inc.com

TÓM TẮT
Tổng hợp chuyển đổi văn bản thành giọng nói (TTS) zero-shot nhằm mục đích sao chép giọng nói của bất kỳ người nói chưa từng thấy nào mà không cần tham số thích ứng. Bằng cách lượng tử hóa sóng âm thanh thành các token âm thanh rời rạc và mô hình hóa những token này với mô hình ngôn ngữ, các mô hình TTS dựa trên mô hình ngôn ngữ gần đây cho thấy khả năng thích ứng người nói zero-shot chỉ với prompt âm thanh 3 giây của người nói chưa từng thấy. Tuy nhiên, chúng bị hạn chế bởi độ dài của prompt âm thanh, điều này khiến việc sao chép phong cách nói cá nhân trở nên khó khăn. Trong bài báo này, chúng tôi đề xuất một mô hình TTS zero-shot mới với các prompt âm thanh đa quy mô dựa trên mô hình ngôn ngữ. Một bộ mã hóa văn bản nhận biết người nói được đề xuất để học phong cách nói cá nhân ở mức âm vị từ prompt phong cách bao gồm nhiều câu. Tiếp theo đó, một bộ giải mã âm thanh dựa trên VALL-E được sử dụng để mô hình hóa âm sắc từ prompt âm sắc ở mức khung hình và tạo ra giọng nói. Kết quả thực nghiệm cho thấy phương pháp đề xuất của chúng tôi vượt trội hơn các phương pháp cơ sở về độ tự nhiên và độ tương tự người nói, và có thể đạt được hiệu suất tốt hơn bằng cách mở rộng ra prompt phong cách dài hơn1.
Từ khóa chỉ mục —chuyển đổi văn bản thành giọng nói, zero-shot, prompt âm thanh đa quy mô, thích ứng người nói, mô hình ngôn ngữ

1. GIỚI THIỆU
Chuyển đổi văn bản thành giọng nói (TTS) nhằm mục đích tạo ra giọng nói tự nhiên và dễ hiểu từ văn bản. Với sự phát triển của học sâu, các mô hình TTS dựa trên mạng nơ-ron đã có thể tổng hợp giọng nói chất lượng cao cho một [1, 2] hoặc nhiều người nói [3, 4]. Tuy nhiên, những mô hình này vẫn yêu cầu một lượng đủ lớn dữ liệu giọng nói sạch cho người nói mới, điều này cản trở sự phát triển của công nghệ tổng hợp giọng nói cho nhiều ứng dụng cá nhân hóa. Do đó, việc thích ứng các mô hình TTS cho bất kỳ người nói nào với càng ít dữ liệu càng tốt, trong khi đạt được độ tương tự người nói cao và độ tự nhiên của giọng nói, đã thu hút sự quan tâm ngày càng tăng trong giới học thuật và công nghiệp [5].

Một trong những phương pháp tổng quát là tinh chỉnh một mô hình TTS đa người nói được đào tạo tốt với một ít dữ liệu thích ứng để hỗ trợ người nói mới. Một số nghiên cứu dành nỗ lực để tinh chỉnh toàn bộ mô hình TTS [6,7], và các phương pháp gần đây khác tìm cách giảm các tham số thích ứng bằng cách chỉ tinh chỉnh một phần của mô hình [8], hoặc chỉ embedding người nói [9]. Tuy nhiên, hiệu suất thích ứng của những phương pháp này phụ thuộc rất nhiều vào chất lượng và số lượng dữ liệu có sẵn cho người nói đích.

‡Công việc được thực hiện khi tác giả thứ nhất thực tập tại Skywork AI PTE. LTD.
†Đóng góp bằng nhau.∗Tác giả liên hệ.
1Mẫu giọng nói: https://thuhcsi.github.io/icassp2024-msvalle

Để xử lý khuyết điểm này, một số công trình tiến hành thích ứng zero-shot, sử dụng chỉ vài giây giọng nói để sao chép giọng nói của người nói chưa từng thấy mà không cần tinh chỉnh mô hình. Trong [10–12], một bộ mã hóa người nói được sử dụng để trích xuất embedding người nói toàn cục từ giọng nói tham chiếu đã cho, cho phép mô hình TTS sao chép âm sắc tổng thể của giọng nói tham chiếu. Xem xét rằng việc mô tả đặc điểm cá nhân của người nói bằng một embedding người nói đơn lẻ là khó khăn, [13, 14] đề xuất trích xuất embedding người nói chi tiết để cải thiện chất lượng giọng nói được tổng hợp.

Được thúc đẩy bởi những tiến bộ trong các mô hình tạo ngôn ngữ tự nhiên, các hệ thống tạo giọng nói gần đây [15–17] giới thiệu ý tưởng sử dụng codec âm thanh nơ-ron [18, 19] để lượng tử hóa sóng âm thanh thành các token rời rạc và tận dụng mô hình ngôn ngữ dựa trên prompting (ví dụ, GPT-3 [20]) để dự đoán những token này. Những hệ thống TTS dựa trên mô hình ngôn ngữ này có thể được đào tạo trên các tập dữ liệu giọng nói đa người nói lớn, đa dạng và chất lượng thấp để cải thiện hiệu suất tổng quát hóa. Với những phương pháp này, các mô hình có khả năng sao chép âm sắc của người nói chỉ với prompt âm thanh 3 giây.

Tuy nhiên, các phương pháp TTS zero-shot dựa trên mô hình ngôn ngữ trên chỉ xem xét prompt âm thanh ở mức khung hình, dẫn đến hai hạn chế chính. Thứ nhất, đặc điểm người nói của một người bao gồm không chỉ âm sắc mà còn phong cách nói cá nhân, bao gồm các yếu tố khác nhau như ngữ điệu, giọng và thói quen phát âm. Trong khi việc xem xét prompt âm thanh ở mức khung hình đã cho thấy sức mạnh tuyệt vời của việc sao chép âm sắc, đã được chứng minh rằng các biểu diễn mức âm vị phù hợp hơn để tạo ra phong cách nói cá nhân [21, 22]. Thứ hai, bị hạn chế bởi cấu trúc của mô hình ngôn ngữ chỉ giải mã, những công trình này chỉ hỗ trợ prompt âm thanh ngắn vì chuỗi token âm thanh mức khung hình quá dài (một giọng nói 10 giây thường chứa hàng nghìn token). Khó có thể sử dụng thông tin hạn chế có trong prompt âm thanh ngắn để sao chép chính xác đặc điểm người nói của người nói đích, dẫn đến độ tự nhiên và tương tự phong cách nói kém. Ngoài ra, các phương pháp dựa trên mô hình ngôn ngữ hiện tại không có khả năng sử dụng nhiều mẫu tham chiếu để nâng cao chất lượng TTS zero-shot mặc dù một số phát ngôn của người nói đích có sẵn trong quá trình suy luận trong nhiều tình huống thực tế.

Để cải thiện thêm độ tương tự người nói cho tổng hợp TTS zero-shot dựa trên mô hình ngôn ngữ, chúng tôi đề xuất sử dụng các prompt âm thanh đa quy mô để nắm bắt cả âm sắc và phong cách nói cá nhân của người nói đích. Mô hình của chúng tôi chứa một bộ mã hóa văn bản nhận biết người nói, sử dụng module attention tham chiếu để mô hình hóa phong cách nói cá nhân ở mức âm vị từ prompt phong cách bao gồm nhiều phát ngôn và một bộ giải mã âm thanh, bảo tồn âm sắc được chỉ định bằng cách xem xét prompt âm sắc ở mức khung hình dựa trên mô hình ngôn ngữ codec nơ-ron (gọi là VALL-E). Mô hình cho phép mở rộng ra độ dài tùy ý của prompt phong cách để mô tả đặc điểm người nói chi tiết. Cả đánh giá chủ quan và khách quan đều cho thấy phương pháp đề xuất của chúng tôi vượt trội hơn mô hình TTS zero-shot dựa trên mô hình ngôn ngữ tiên tiến [16] và các phương pháp cơ sở khác về độ tự nhiên và độ tương tự người nói. Hiệu suất cũng được cải thiện với số lượng câu tăng lên được sử dụng trong prompt phong cách trong quá trình suy luận.

2. PHƯƠNG PHÁP LUẬN
Kiến trúc của mô hình đề xuất được minh họa trong Hình 1. Nó bao gồm hai phần chính: một bộ mã hóa văn bản nhận biết người nói và một bộ giải mã âm thanh dựa trên VALL-E. Trong bài báo này, chúng tôi theo Naturalspeech 2 [23] và VALL-E [16] để tận dụng các mô hình codec âm thanh nơ-ron nhằm biểu diễn prompt phong cách và prompt âm sắc trong embedding âm thanh liên tục và token âm thanh rời rạc, tương ứng. Bộ mã hóa văn bản nhận biết người nói được sử dụng để trích xuất thông tin phong cách nói cá nhân mức âm vị từ prompt phong cách và hợp nhất nó vào embedding âm vị được mã hóa bằng module attention tham chiếu để có được embedding văn bản nhận biết người nói. Sau đó, các đầu ra của bộ mã hóa được đưa vào bộ giải mã âm thanh cùng với các token âm thanh của prompt âm sắc để tạo ra giọng nói có cùng âm sắc với prompt âm sắc. Chi tiết của từng thành phần như sau.

2.1. Bộ Mã Hóa Văn Bản Nhận Biết Người Nói
Bộ mã hóa văn bản nhận biết người nói được thiết kế đặc biệt để trích xuất và mô hình hóa phong cách nói cá nhân ở mức âm vị từ prompt phong cách có độ dài tùy ý và hợp nhất thông tin nội dung phía văn bản với thông tin phong cách phía giọng nói để có được embedding văn bản nhận biết người nói. Kiến trúc của bộ mã hóa được minh họa trong Hình 2, bao gồm một bộ mã hóa âm vị, một bộ mã hóa âm thanh và một module attention tham chiếu.

Ở phía văn bản, để có được biểu diễn văn bản tốt hơn làm đầu vào bộ giải mã, chúng tôi giới thiệu một bộ mã hóa âm vị để mã hóa chuỗi âm vị. Chúng tôi sử dụng khối Transformer, là một chồng của lớp self-attention và 1D-convolution như trong Fastspeech 2 [2], làm cấu trúc cơ bản cho bộ mã hóa. Các văn bản đầu vào được chuyển đổi thành chuỗi âm vị bằng module grapheme-to-phoneme và sau đó được chuyển đến bộ mã hóa âm vị để có được embedding âm vị.

Ở phía giọng nói, để sử dụng các prompt giọng nói có độ dài tùy ý, các phương pháp trước đây cố gắng mã hóa đặc điểm người nói thành một vector mức toàn cục [11, 12]. Kết quả là, các biến thiên chi tiết cục bộ trong phong cách nói bị bỏ qua. Khác với cách này, chúng tôi sử dụng một bộ mã hóa âm thanh để có được embedding phong cách nói cục bộ từ prompt phong cách thay vì một vector đơn lẻ. Tất cả các phát ngôn của người nói đích đầu tiên được nối lại để tạo thành prompt phong cách, và sau đó được chuyển đến một codec âm thanh nơ-ron được đào tạo tốt để chuyển đổi sóng giọng nói thành embedding âm thanh liên tục thay vì token rời rạc để bảo tồn càng nhiều thông tin phong cách cá nhân càng tốt trong giọng nói. Sau đó, embedding âm thanh được chuyển đổi được chuyển đến bộ mã hóa âm thanh, được tạo thành từ một chồng 8 lớp 1D-convolution. Ngoài ra, để điều chỉnh độ chi tiết thời gian của các biểu diễn phong cách được trích xuất gần hơn với nhận thức âm thanh của con người, các stride bộ lọc của các lớp convolution được đặt là [2,1,2,1,2,1,2,1] cho việc downsampling 16 lần (khoảng 0.2s). Sau đó, độ chi tiết thời gian của embedding phong cách được cải tạo đúng cách thành mức tựa-âm vị được lấy cảm hứng từ [24].

Để sử dụng tốt hơn embedding phong cách được trích xuất từ prompt phong cách, một module attention tham chiếu được giới thiệu để có được phong cách nói cá nhân mức âm vị liên quan đến ngữ nghĩa phù hợp. Chúng tôi áp dụng scaled dot-product attention làm module attention tham chiếu. Embedding âm vị được coi là query, trong khi tất cả embedding phong cách được trích xuất từ prompt phong cách được coi là cả key và value. Mức độ liên quan giữa chúng được sử dụng để hướng dẫn việc lựa chọn phong cách nói cá nhân cho mỗi âm vị đầu vào. Cuối cùng, module attention tham chiếu xuất ra một chuỗi được căn chỉnh có cùng độ dài với embedding âm vị và cộng nó vào embedding âm vị để tạo thành embedding văn bản nhận biết người nói.

2.2. Bộ Giải Mã Âm Thanh
Để mô hình hóa đặc điểm người nói của một người, cần thiết phải sao chép âm sắc ngoài việc bắt chước phong cách nói của người nói. Được truyền cảm hứng từ thành công của các mô hình ngôn ngữ trong TTS zero-shot, phương pháp đề xuất của chúng tôi áp dụng một VALL-E được sửa đổi [16] làm bộ giải mã âm thanh để tạo ra giọng nói có cùng âm sắc với prompt âm sắc 3 giây. Như được minh họa trong Hình 3, bộ giải mã được tạo thành từ một embedding âm thanh, một bộ giải mã transformer tự hồi quy (AR) và một bộ giải mã transformer phi tự hồi quy (NAR).

Prompt âm sắc đầu tiên được chuyển đến một codec âm thanh nơ-ron được đào tạo tốt, và đầu ra của bộ lượng tử hóa vector dư trong codec được coi là token âm thanh prompt rời rạc. Những token này bao gồm 8 lớp sau đó được nhúng thông qua tám lớp embedding âm thanh riêng biệt. Bộ giải mã transformer AR được sử dụng để tạo ra lớp đầu tiên của token âm thanh cần thiết để tổng hợp giọng nói được cá nhân hóa có điều kiện trên embedding văn bản nhận biết người nói.

--- TRANG 2 ---
Hình 1. Kiến trúc tổng thể của mô hình đề xuất

cho phép mở rộng ra độ dài tùy ý của prompt phong cách để mô tả đặc điểm người nói chi tiết. Cả đánh giá chủ quan và khách quan đều cho thấy phương pháp đề xuất của chúng tôi vượt trội hơn mô hình TTS zero-shot dựa trên mô hình ngôn ngữ tiên tiến [16] và các phương pháp cơ sở khác về độ tự nhiên và độ tương tự người nói. Hiệu suất cũng được cải thiện với số lượng câu tăng lên được sử dụng trong prompt phong cách trong quá trình suy luận.

2. PHƯƠNG PHÁP LUẬN
Kiến trúc của mô hình đề xuất được minh họa trong Hình 1. Nó bao gồm hai phần chính: một bộ mã hóa văn bản nhận biết người nói và một bộ giải mã âm thanh dựa trên VALL-E. Trong bài báo này, chúng tôi theo Naturalspeech 2 [23] và VALL-E [16] để tận dụng các mô hình codec âm thanh nơ-ron nhằm biểu diễn prompt phong cách và prompt âm sắc trong embedding âm thanh liên tục và token âm thanh rời rạc, tương ứng. Bộ mã hóa văn bản nhận biết người nói được sử dụng để trích xuất thông tin phong cách nói cá nhân mức âm vị từ prompt phong cách và hợp nhất nó vào embedding âm vị được mã hóa bằng module attention tham chiếu để có được embedding văn bản nhận biết người nói. Sau đó, các đầu ra của bộ mã hóa được đưa vào bộ giải mã âm thanh cùng với các token âm thanh của prompt âm sắc để tạo ra giọng nói có cùng âm sắc với prompt âm sắc. Chi tiết của từng thành phần như sau.

2.1. Bộ Mã Hóa Văn Bản Nhận Biết Người Nói
Bộ mã hóa văn bản nhận biết người nói được thiết kế đặc biệt để trích xuất và mô hình hóa phong cách nói cá nhân ở mức âm vị từ prompt phong cách có độ dài tùy ý và hợp nhất thông tin nội dung phía văn bản với thông tin phong cách phía giọng nói để có được embedding văn bản nhận biết người nói. Kiến trúc của bộ mã hóa được minh họa trong Hình 2, bao gồm một bộ mã hóa âm vị, một bộ mã hóa âm thanh và một module attention tham chiếu.

Ở phía văn bản, để có được biểu diễn văn bản tốt hơn làm đầu vào bộ giải mã, chúng tôi giới thiệu một bộ mã hóa âm vị để mã hóa chuỗi âm vị. Chúng tôi sử dụng khối Transformer, là một chồng của lớp self-attention và 1D-convolution như trong Fastspeech 2 [2], làm cấu trúc cơ bản cho bộ mã hóa. Các văn bản đầu vào được chuyển đổi thành chuỗi âm vị bằng module grapheme-to-phoneme và sau đó được chuyển đến bộ mã hóa âm vị để có được embedding âm vị.

Ở phía giọng nói, để sử dụng các prompt giọng nói có độ dài tùy ý, các phương pháp trước đây cố gắng mã hóa đặc điểm người nói thành một vector mức toàn cục [11, 12]. Kết quả là, các biến thiên chi tiết cục bộ trong phong cách nói bị bỏ qua. Khác với cách này, chúng tôi sử dụng một bộ mã hóa âm thanh để có được embedding phong cách nói cục bộ từ prompt phong cách thay vì một vector đơn lẻ. Tất cả các phát ngôn của người nói đích đầu tiên được nối lại để tạo thành prompt phong cách, và sau đó được chuyển đến một codec âm thanh nơ-ron được đào tạo tốt để chuyển đổi sóng giọng nói thành embedding âm thanh liên tục thay vì token rời rạc để bảo tồn càng nhiều thông tin phong cách cá nhân càng tốt trong giọng nói. Sau đó, embedding âm thanh được chuyển đổi được chuyển đến bộ mã hóa âm thanh, được tạo thành từ một chồng 8 lớp 1D-convolution. Ngoài ra, để điều chỉnh độ chi tiết thời gian của các biểu diễn phong cách được trích xuất gần hơn với nhận thức âm thanh của con người, các stride bộ lọc của các lớp convolution được đặt là [2,1,2,1,2,1,2,1] cho việc downsampling 16 lần (khoảng 0.2s). Sau đó, độ chi tiết thời gian của embedding phong cách được cải tạo đúng cách thành mức tựa-âm vị được lấy cảm hứng từ [24].

Để sử dụng tốt hơn embedding phong cách được trích xuất từ prompt phong cách, một module attention tham chiếu được giới thiệu để có được phong cách nói cá nhân mức âm vị liên quan đến ngữ nghĩa phù hợp. Chúng tôi áp dụng scaled dot-product attention làm module attention tham chiếu. Embedding âm vị được coi là query, trong khi tất cả embedding phong cách được trích xuất từ prompt phong cách được coi là cả key và value. Mức độ liên quan giữa chúng được sử dụng để hướng dẫn việc lựa chọn phong cách nói cá nhân cho mỗi âm vị đầu vào. Cuối cùng, module attention tham chiếu xuất ra một chuỗi được căn chỉnh có cùng độ dài với embedding âm vị và cộng nó vào embedding âm vị để tạo thành embedding văn bản nhận biết người nói.

2.2. Bộ Giải Mã Âm Thanh
Để mô hình hóa đặc điểm người nói của một người, cần thiết phải sao chép âm sắc ngoài việc bắt chước phong cách nói của người nói. Được truyền cảm hứng từ thành công của các mô hình ngôn ngữ trong TTS zero-shot, phương pháp đề xuất của chúng tôi áp dụng một VALL-E được sửa đổi [16] làm bộ giải mã âm thanh để tạo ra giọng nói có cùng âm sắc với prompt âm sắc 3 giây. Như được minh họa trong Hình 3, bộ giải mã được tạo thành từ một embedding âm thanh, một bộ giải mã transformer tự hồi quy (AR) và một bộ giải mã transformer phi tự hồi quy (NAR).

Prompt âm sắc đầu tiên được chuyển đến một codec âm thanh nơ-ron được đào tạo tốt, và đầu ra của bộ lượng tử hóa vector dư trong codec được coi là token âm thanh prompt rời rạc. Những token này bao gồm 8 lớp sau đó được nhúng thông qua tám lớp embedding âm thanh riêng biệt. Bộ giải mã transformer AR được sử dụng để tạo ra lớp đầu tiên của token âm thanh cần thiết để tổng hợp giọng nói được cá nhân hóa có điều kiện trên embedding văn bản nhận biết người nói.

--- TRANG 3 ---
giọng nói được cá nhân hóa có điều kiện trên embedding văn bản nhận biết người nói. Trong khi đó, lớp đầu tiên của token âm thanh của prompt âm sắc được sử dụng làm tiền tố trong việc giải mã AR. Sau đó, bộ giải mã transformer NAR được sử dụng để tạo ra token âm thanh của bảy lớp còn lại theo trình tự. Để dự đoán token âm thanh của lớp thứ i, đầu vào transformer là sự nối của embedding văn bản nhận biết người nói, tổng của token âm thanh nhúng của prompt âm sắc từ lớp 1 đến lớp i và tổng của token âm thanh nhúng được dự đoán từ lớp 1 đến lớp i-1. Cuối cùng, lớp đầu tiên của token âm thanh được dự đoán bởi bộ giải mã transformer AR và các lớp còn lại của token âm thanh được dự đoán bởi bộ giải mã transformer NAR được nối lại để tạo thành token âm thanh được dự đoán.

2.3. Chiến Lược Đào Tạo và Quy Trình Suy Luận
Trong giai đoạn đào tạo, cho mỗi mẫu đào tạo, chúng tôi chọn ngẫu nhiên 5 đến 10 phát ngôn tham chiếu được nói bởi cùng người nói với mẫu để tạo thành prompt phong cách. Đối với các epoch đào tạo khác nhau, các prompt phong cách khác nhau được chọn ngẫu nhiên cho cùng một mẫu đào tạo để tăng cường dữ liệu. Khác với VALL-E đào tạo hai mô hình riêng biệt, phương pháp đề xuất của chúng tôi đào tạo toàn bộ hệ thống end-to-end một cách kết hợp với cross-entropy loss. Loss đào tạo là tổ hợp tuyến tính của loss bộ giải mã transformer AR và loss bộ giải mã transformer NAR. Trong bộ giải mã transformer AR, chúng tôi không chọn rõ ràng một phát ngôn làm prompt âm sắc trong đào tạo, có nghĩa là tất cả token âm thanh của lớp đầu tiên được dự đoán với kỹ thuật teacher-forcing. Đối với bộ giải mã transformer NAR, trong mỗi bước đào tạo, chúng tôi lấy mẫu ngẫu nhiên một giai đoạn đào tạo i∈[2,8] và chọn ngẫu nhiên một độ dài nhất định của tiền tố giọng nói đích làm prompt âm sắc. Mô hình được đào tạo để tối đa hóa xác suất của token âm thanh trong lớp thứ i.

Trong giai đoạn suy luận, chúng tôi thiết kế các prompt âm thanh và suy luận như sau. Để tạo ra nội dung đã cho cho người nói chưa từng thấy, mô hình được cung cấp một câu văn bản, bất kỳ số lượng giọng nói nào từ người nói đích làm prompt phong cách, một đoạn ngắn giọng nói từ người nói đích làm prompt âm sắc và bản phiên âm tương ứng của nó. Chúng tôi thêm phần phiên âm của prompt âm sắc vào đầu câu văn bản đã cho làm prompt văn bản. Với prompt văn bản, prompt phong cách và prompt âm sắc, phương pháp đề xuất của chúng tôi tạo ra token âm thanh cho văn bản đã cho sao chép đặc điểm người nói của người nói đích.

3. THỰC NGHIỆM
3.1. Thiết Lập Đào Tạo
Tất cả các mô hình được đào tạo trên LibriTTS [25], là một tập dữ liệu giọng nói tiếng Anh được phiên âm đa người nói mã nguồn mở. Tập đào tạo của nó chứa khoảng 580 giờ ghi âm được nói bởi 2,306 người nói. Để đánh giá khả năng thích ứng zero-shot cho người nói chưa từng thấy, 128 người nói từ hai tập con của tập dữ liệu LibriTTS (test-clean và dev-clean) được chọn làm tập kiểm tra, dẫn đến tổng cộng 8,078 phát ngôn. Một mô hình codec âm thanh nơ-ron được đào tạo trước, EnCodec2[18], được sử dụng làm mô hình codec để mã hóa sóng thô với tần số lấy mẫu 24kHz và tái tạo sóng dựa trên token âm thanh được dự đoán.

Trong việc triển khai của chúng tôi, bộ mã hóa âm vị, bộ giải mã transformer AR và bộ giải mã transformer NAR đều bao gồm 6 lớp khối transformer. So với hai module trong VALL-E gốc cả hai đều bao gồm 12 lớp khối transformer, mô hình đề xuất của chúng tôi có ít tham số hơn. Chúng tôi đào tạo tất cả các mô hình trong 300K lần lặp trên 4 GPU NVIDIA A100, với kích thước batch là 8 mẫu trên mỗi GPU. Bộ tối ưu Adam được áp dụng với β1= 0.9, β2= 0.98 và theo cùng lịch trình tỷ lệ học trong [16].

2Được triển khai dựa trên: https://github.com/facebookresearch/encodec

Hình 3. Cấu trúc của bộ giải mã âm thanh

3.2. Các Phương Pháp So Sánh
Để chứng minh hiệu suất của phương pháp đề xuất, chúng tôi so sánh năm mô hình sau đây cho tổng hợp TTS zero-shot. Những mô hình này cũng được triển khai dựa trên VALL-E3.

VALL-E Một triển khai mã nguồn mở3 của VALL-E [16], chỉ xem xét prompt âm sắc 3 giây ở mức khung hình.

Proposed Mô hình đề xuất, xem xét cả prompt âm sắc 3 giây và prompt phong cách bao gồm mười câu.

Proposed-3s Để đảm bảo so sánh công bằng, chúng tôi xây dựng mô hình cơ sở này, chia sẻ cùng cấu trúc và tham số với mô hình đề xuất, nhưng chỉ sử dụng giọng nói 3 giây làm cả prompt âm sắc và prompt phong cách.

Base-S Mô hình cơ sở chỉ prompt phong cách, chia sẻ cùng backbone TTS và prompt phong cách với mô hình đề xuất, nhưng loại trừ prompt âm sắc.

Base-T Mô hình cơ sở chỉ prompt âm sắc, trong đó prompt phong cách được loại bỏ. Tức là, mô hình này chỉ sử dụng giọng nói 3 giây làm prompt âm sắc.

Đối với mỗi mẫu tổng hợp, chúng tôi chọn ngẫu nhiên các phát ngôn khác của cùng người nói làm prompt âm sắc và prompt phong cách.

3Được triển khai dựa trên: https://github.com/lifeiteng/vall-e

3.3. Đánh Giá Chủ Quan
Chúng tôi tiến hành hai kiểm tra điểm ý kiến trung bình (MOS) để đo lường khả năng zero-shot của các mô hình khác nhau: 1) MOS Tự nhiên (N-MOS): đánh giá độ tự nhiên và ngữ điệu của giọng nói được tổng hợp; 2) MOS Tương tự (S-MOS): đánh giá độ tương tự người nói giữa giọng nói được tổng hợp và giọng nói thực tế. Chúng tôi chọn ngẫu nhiên 20 mẫu từ các người nói khác nhau trong tập kiểm tra để đánh giá chủ quan. Để loại trừ các yếu tố can thiệp khác, chúng tôi giữ nội dung văn bản và giọng nói prompt nhất quán giữa các mô hình khác nhau. Một nhóm 25 đối tượng nghe được tuyển chọn để đánh giá các giọng nói đã cho trên thang điểm từ 1 đến 5 với khoảng cách 1 điểm.

Như được hiển thị trong Bảng 1, phương pháp đề xuất của chúng tôi đạt được N-MOS tốt nhất là 3.886 và S-MOS là 3.870, vượt trội hơn VALL-E rất nhiều trong cả hai khía cạnh. So với VALL-E và mô hình Base-T, cả hai đều chỉ sử dụng 3 giây giọng nói làm prompt âm thanh, mô hình Proposed-3s đạt được hiệu suất tốt hơn, đặc biệt là về độ tự nhiên với khoảng cách hơn 0.12. Điều này chứng minh rằng việc xem xét cùng một prompt âm thanh ở mức âm vị thực sự hữu ích cho việc học phong cách nói cá nhân của

--- TRANG 4 ---
Bảng 1. So sánh khách quan và chủ quan cho tổng hợp chuyển đổi văn bản thành giọng nói zero-shot. Chúng tôi đánh giá độ tự nhiên và độ tương tự người nói của các mô hình khác nhau với khoảng tin cậy 95%.

Models Subjective Objective
N-MOS (↑) S-MOS (↑) SECS (↑)MCD (↓)
Ground Truth 4.23±0.066 - - -
VALL-E 3.48±0.059 3.532±0.060 0.771 8.075
Base-S 3.456±0.055 3.500±0.056 0.727 7.792
Base-T 3.654±0.062 3.646±0.060 0.764 8.047
Proposed 3.886±0.063 3.870±0.062 0.798 7.715
Proposed-3s 3.778±0.063 3.692±0.062 0.779 7.765

Bảng 2. Kết quả đánh giá khách quan của VALL-E và phương pháp đề xuất khi sử dụng độ dài prompt khác nhau trong suy luận. Thời lượng trung bình của các câu là khoảng 6 giây.

Models SECS (↑)MCD (↓)
VALL-E w/ 3s 0.771 8.075
VALL-E w/ 6s 0.774 8.177
Proposed w/ 1 sent (3s) 0.779 7.765
Proposed w/ 5 sent (30s) 0.795 7.743
Proposed w/ 10 sent (1min) 0.798 7.715
Proposed w/ 20 sent (2min) 0.798 7.702

người nói đích và cải thiện độ tự nhiên và độ tương tự người nói của giọng nói được tổng hợp mà không cần đưa vào các đầu vào bổ sung. Mô hình đề xuất của chúng tôi cũng cải thiện thêm so với mô hình Proposed-3s bằng cách tăng số lượng câu trong prompt phong cách, cho thấy khả năng nâng cao chất lượng TTS zero-shot bằng cách sử dụng nhiều giọng nói tham chiếu hơn từ người nói đích. Khả năng này không có sẵn trong các mô hình TTS dựa trên mô hình ngôn ngữ trước đây, và nó cho phép phương pháp đề xuất của chúng tôi có giới hạn hiệu suất trên cao hơn so với VALL-E. Mô hình đề xuất của chúng tôi cũng đạt được hiệu suất vượt trội hơn không chỉ Base-S chỉ xem xét prompt phong cách, mà còn Base-T chỉ xem xét prompt âm sắc một cách đơn lẻ. Điều này chứng minh rằng việc mô hình hóa đặc điểm người nói từ các quy mô khác nhau có thể cải thiện độ tự nhiên và độ tương tự người nói của giọng nói được tổng hợp. Ngoài ra, quan sát thấy rằng mặc dù Base-S cũng sử dụng mười câu làm prompt phong cách, nó đạt được điểm số thấp nhất trong cả hai đánh giá. Một lý do có thể là việc loại bỏ prompt âm sắc làm tiền tố trong bộ giải mã ảnh hưởng đến tính ổn định của việc giải mã, dẫn đến một số giọng nói được tổng hợp kém về độ rõ ràng. Hơn nữa, chúng tôi đã thêm một so sánh với YourTTS [12], thông qua kiểm tra ưa thích ABX. Mô hình đề xuất của chúng tôi cho thấy tỷ lệ ưa thích 57.3% so với 33.6% của YourTTS, chứng minh hiệu quả của nó.

3.4. Đánh Giá Khách Quan
Để đo lường độ tự nhiên và độ tương tự người nói của giọng nói được tổng hợp một cách khách quan, chúng tôi tính toán mel-cepstrum distortion (MCD) và Speaker Encoder Cosine Similarity (SECS) làm các chỉ số theo [12, 13]. Vì độ dài của giọng nói được dự đoán và giọng nói thực tế có thể khác nhau, chúng tôi đầu tiên áp dụng dynamic time warping (DTW) để có được các mối quan hệ căn chỉnh giữa hai mel-spectrogram. Sau đó, chúng tôi tính toán MCD tối thiểu bằng cách căn chỉnh hai mel-spectrogram. Đối với độ tương tự người nói, chúng tôi sử dụng bộ mã hóa người nói của gói Resemblyzer4 để tính toán SECS giữa giọng nói thực tế và giọng nói được tổng hợp. Giá trị nằm trong khoảng từ 0 đến 1, trong đó giá trị lớn chỉ ra độ tương tự cao hơn.

Kết quả đánh giá của các mô hình khác nhau trên tập kiểm tra được hiển thị trong Bảng 1. Quan sát thấy rằng mô hình đề xuất của chúng tôi vượt trội hơn các mô hình cơ sở trong tất cả các chỉ số đánh giá khách quan. Kết quả cho thấy rằng mô hình đề xuất của chúng tôi có thể cải thiện chất lượng và độ tương tự người nói của giọng nói được tổng hợp.

4Được triển khai dựa trên: https://github.com/resemble-ai/Resemblyzer

3.5. Điều Tra
Để điều tra tác động của prompt với độ dài khác nhau, chúng tôi điều chỉnh độ dài của prompt âm thanh và prompt phong cách cho VALL-E và mô hình đề xuất, tương ứng. Đối với VALL-E, bị hạn chế bởi cấu trúc của mô hình ngôn ngữ chỉ giải mã, chúng tôi chọn ngẫu nhiên hai phát ngôn 3s/6s làm prompt cho mỗi người nói. Chúng tôi cũng đánh giá mô hình đề xuất của chúng tôi với số lượng giọng nói khác nhau làm prompt phong cách, bao gồm 1 câu, 5 câu, 10 câu và 20 câu. Prompt âm sắc được cố định thành giọng nói 3 giây như đã đề cập ở trên. Đặc biệt, khi prompt phong cách chỉ bao gồm một câu, mô hình đề xuất chỉ sử dụng giọng nói 3 giây làm cả prompt âm sắc và prompt phong cách. Chúng tôi đánh giá những mô hình này với hai chỉ số khách quan như đã mô tả trước đó.

Bảng 2 cho thấy so sánh hiệu suất giữa các độ dài khác nhau của prompt âm thanh. Quan sát thấy rằng VALL-E với prompt âm thanh là giọng nói 6 giây có kết quả SECS gần với phương pháp đề xuất chỉ với một câu làm prompt phong cách, nhưng có khoảng cách đáng kể với đề xuất trong MCD. Điều này chứng minh rằng việc mô hình hóa phong cách nói cá nhân của người nói đích ở mức âm vị giúp tạo ra giọng nói gần với giọng nói thực tế. Bằng cách so sánh các độ dài khác nhau của prompt phong cách, chúng ta có thể thấy mô hình đề xuất của chúng tôi có thể tạo ra giọng nói tương tự hơn khi số lượng câu trong prompt phong cách tăng lên.

4. KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất một mô hình TTS zero-shot dựa trên mô hình ngôn ngữ để sử dụng các prompt âm thanh đa quy mô nhằm nắm bắt cả âm sắc và phong cách nói cá nhân của người nói đích. Một bộ mã hóa văn bản nhận biết người nói được sử dụng để mô hình hóa phong cách nói ở mức âm vị từ prompt phong cách có độ dài tùy ý. Một bộ giải mã âm thanh dựa trên mô hình ngôn ngữ được sử dụng để bảo tồn âm sắc được chỉ định bằng cách xem xét prompt âm sắc ở mức khung hình. Kết quả thực nghiệm chứng minh rằng phương pháp đề xuất của chúng tôi có thể cải thiện đáng kể độ tự nhiên và độ tương tự người nói của giọng nói được tổng hợp, ngay cả khi chỉ sử dụng giọng nói 3 giây làm cả prompt phong cách và prompt âm sắc. Ngoài ra, mô hình đề xuất của chúng tôi có thể nâng cao chất lượng TTS zero-shot bằng cách tăng số lượng câu trong prompt phong cách, khi có nhiều câu của người nói đích có sẵn trong quá trình suy luận.

Lời cảm ơn: Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (62076144), Quỹ Khoa học Xã hội Quốc gia Trung Quốc (13&ZD189), Chương trình Khoa học và Công nghệ Shenzhen (WDZC20220816140515001, JCYJ20220818101014030) và Phòng thí nghiệm Trọng điểm Shenzhen về công nghệ sáng tạo phương tiện tương tác thế hệ tiếp theo (ZDSYS20210623092001004).

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO
[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, Rj Skerrv-Ryan, et al., "Natural TTS synthe-
sis by conditioning waveNet on mel spectrogram predictions,"
trong International Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, 2018, pp. 4779–4783.

[2] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao,
and Tie-Yan Liu, "Fastspeech 2: Fast and high-quality end-to-
end text to speech," trong International Conference on Learning
Representations, 2020.

[3] Mingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun, Sheng Zhao,
and Tao Qin, "MultiSpeech: Multi-Speaker Text to Speech
with Transformer," trong Proc. Interspeech 2020, 2020, pp. 4024–
4028.

[4] Jaehyeon Kim, Jungil Kong, and Juhee Son, "Conditional
variational autoencoder with adversarial learning for end-to-
end text-to-speech," trong International Conference on Machine
Learning. PMLR, 2021, pp. 5530–5540.

[5] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu, "A survey
on neural speech synthesis," arXiv preprint arXiv:2106.15561,
2021.

[6] Yutian Chen, Yannis Assael, Brendan Shillingford, David Bud-
den, Scott Reed, Heiga Zen, Quan Wang, Luis C Cobo, An-
drew Trask, Ben Laurie, et al., "Sample efficient adaptive text-
to-speech," trong International Conference on Learning Repre-
sentations, 2018.

[7] Zvi Kons, Slava Shechtman, Alex Sorin, Carmel Rabinovitz,
and Ron Hoory, "High quality, lightweight and adaptable TTS
using LPCNet," trong Proc. Interspeech 2019, 2019, pp. 176–180.

[8] Henry B Moss, Vatsal Aggarwal, Nishant Prateek, Javier
González, and Roberto Barra-Chicote, "BOFFIN TTS: Few-
shot speaker adaptation by bayesian optimization," trong Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 2020, pp. 7639–7643.

[9] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Tie-
Yan Liu, et al., "Adaspeech: Adaptive text to speech for custom
voice," trong International Conference on Learning Representa-
tions, 2020.

[10] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen,
Fei Ren, zhifeng Chen, Patrick Nguyen, Ruoming Pang, Igna-
cio Lopez Moreno, and Yonghui Wu, "Transfer learning from
speaker verification to multispeaker text-to-speech synthesis,"
trong Advances in Neural Information Processing Systems, 2018,
vol. 31.

[11] Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin
Wang, Nanxin Chen, and Junichi Yamagishi, "Zero-shot multi-
speaker text-to-speech with state-of-the-art neural speaker em-
beddings," trong International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 2020, pp. 6184–6188.

[12] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-
naldo Candido Junior, Eren Gölge, and Moacir A Ponti,
"YourTTS: Towards zero-shot multi-speaker tts and zero-shot
voice conversion for everyone," trong International Conference
on Machine Learning. PMLR, 2022, pp. 2709–2720.

[13] Seungwoo Choi, Seungju Han, Dongyoung Kim, and Sungjoo
Ha, "Attentron: Few-Shot Text-to-Speech Utilizing Attention-
Based Variable-Length Embedding," trong Proc. Interspeech
2020, 2020, pp. 2007–2011.

[14] Yixuan Zhou, Changhe Song, Xiang Li, Luwen Zhang, Zhiy-
ong Wu, Yanyao Bian, Dan Su, and Helen Meng, "Content-
Dependent Fine-Grained Speaker Embedding for Zero-Shot
Speaker Adaptation in Text-to-Speech Synthesis," trong Proc. In-
terspeech 2022, 2022, pp. 2573–2577.

[15] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene
Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek,
Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil
Zeghidour, "AudioLM: A language modeling approach to au-
dio generation," IEEE/ACM Transactions on Audio, Speech,
and Language Processing, vol. 31, pp. 2523–2533, 2023.

[16] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long
Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang,
Jinyu Li, et al., "Neural codec language models are zero-shot
text to speech synthesizers," arXiv preprint arXiv:2301.02111,
2023.

[17] Eugene Kharitonov, Damien Vincent, Zalán Borsos, Raphaël
Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco
Tagliasacchi, and Neil Zeghidour, "Speak, read and prompt:
High-fidelity text-to-speech with minimal supervision," arXiv
preprint arXiv:2302.03540, 2023.

[18] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi
Adi, "High fidelity neural audio compression," arXiv preprint
arXiv:2210.13438, 2022.

[19] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan
Skoglund, and Marco Tagliasacchi, "SoundStream: An end-to-
end neural audio codec," IEEE/ACM Transactions on Audio,
Speech, and Language Processing, vol. 30, pp. 495–507, 2021.

[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al., "Lan-
guage models are few-shot learners," Advances in neural in-
formation processing systems, vol. 33, pp. 1877–1901, 2020.

[21] Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Xixin Wu,
Shiyin Kang, and Helen Meng, "MSStyleTTS: Multi-scale
style modeling with hierarchical context information for ex-
pressive speech synthesis," IEEE/ACM Transactions on Audio,
Speech, and Language Processing, vol. 31, pp. 3290–3303,
2023.

[22] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang,
Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang,
Xiang Yin, et al., "Mega-TTS: Zero-shot text-to-speech
at scale with intrinsic inductive bias," arXiv preprint
arXiv:2306.03509, 2023.

[23] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei
He, Tao Qin, Sheng Zhao, and Jiang Bian, "NaturalSpeech 2:
Latent diffusion models are natural and zero-shot speech and
singing synthesizers," arXiv preprint arXiv:2304.09116, 2023.

[24] Xiang Li, Changhe Song, Jingbei Li, Zhiyong Wu, Jia Jia, and
Helen Meng, "Towards multi-scale style control for expressive
speech synthesis," trong Proc. Interspeech 2021, 2021, pp. 4673–
4677.

[25] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss,
Ye Jia, Zhifeng Chen, and Yonghui Wu, "LibriTTS: A corpus
derived from libriSpeech for text-to-speech," trong Proc. Inter-
speech 2019, 2019, pp. 1526–1530.
