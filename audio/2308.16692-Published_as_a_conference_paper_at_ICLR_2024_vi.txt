# SpeechTokenizer: Tokenizer Âm thanh Thống nhất cho Mô hình Ngôn ngữ Âm thanh

Xin Zhang∗, Dong Zhang∗, Shimin Li, Yaqian Zhou†, Xipeng Qiu†
Trường Khoa học Máy tính, Đại học Fudan
Phòng thí nghiệm Xử lý Thông tin Thông minh Thành phố Thượng Hải, Đại học Fudan
{xin_zhang22,dongzhang22}@m.fudan.edu.cn
{smli20,zhouyaqian,xpqiu}@fudan.edu.cn
https://0nutation.github.io/SpeechTokenizer.github.io/

## TÓM TẮT

Các mô hình ngôn ngữ âm thanh lớn hiện tại được xây dựng dựa trên các biểu diễn âm thanh rời rạc, có thể được phân loại thành token ngữ nghĩa và token âm học. Tuy nhiên, các token âm thanh hiện có không được thiết kế đặc biệt cho việc mô hình hóa ngôn ngữ âm thanh. Để đánh giá tính phù hợp của các token âm thanh cho việc xây dựng mô hình ngôn ngữ âm thanh, chúng tôi thiết lập điểm chuẩn đầu tiên, SLMTokBench. Kết quả của chúng tôi cho thấy cả token ngữ nghĩa và token âm học đều không lý tưởng cho mục đích này. Do đó, chúng tôi đề xuất SpeechTokenizer, một tokenizer âm thanh thống nhất cho các mô hình ngôn ngữ âm thanh lớn. SpeechTokenizer áp dụng kiến trúc Encoder-Decoder với lượng tử hóa vector dư (RVQ). Thống nhất các token ngữ nghĩa và âm học, SpeechTokenizer phân tách các khía cạnh khác nhau của thông tin âm thanh một cách phân cấp qua các lớp RVQ khác nhau. Hơn nữa, chúng tôi xây dựng một Mô hình Ngôn ngữ Âm thanh Thống nhất (USLM) tận dụng SpeechTokenizer. Các thí nghiệm cho thấy SpeechTokenizer hoạt động tương đương với EnCodec trong việc tái tạo âm thanh và thể hiện hiệu suất mạnh mẽ trên điểm chuẩn SLMTokBench. Ngoài ra, USLM vượt trội hơn VALL-E trong các tác vụ Text-to-Speech zero-shot. Mã nguồn và mô hình có sẵn tại https://github.com/ZhangXInFD/SpeechTokenizer/.

## 1. GIỚI THIỆU

Các mô hình ngôn ngữ lớn (OpenAI, 2023; Touvron et al., 2023) đã thể hiện hiệu suất đáng chú ý trên nhiều tác vụ xử lý ngôn ngữ tự nhiên khác nhau. Điều này đã truyền cảm hứng cho nhiều nghiên cứu xây dựng các mô hình ngôn ngữ âm thanh (Borsos et al., 2022), đã đạt được những đột phá đáng kể trên nhiều tác vụ xử lý âm thanh khác nhau (Wang et al., 2023; Zhang et al., 2023; Rubenstein et al., 2023; Dong et al., 2023). Điểm chung chính giữa các nghiên cứu này là việc sử dụng các biểu diễn âm thanh rời rạc. Các biểu diễn âm thanh rời rạc hiện tại có thể được phân loại thành hai loại: token ngữ nghĩa và token âm học (Borsos et al., 2022). Token ngữ nghĩa thường từ các mô hình tự giám sát được tiền huấn luyện với mô hình hóa ngôn ngữ che khuất làm mục tiêu huấn luyện (Hsu et al., 2021; Baevski et al., 2020; Chung et al., 2021). Được tạo ra thông qua phân cụm k-means trên các biểu diễn từ một lớp trung gian cụ thể, các token ngữ nghĩa được mô tả như các chuỗi với cấu trúc một chiều. Token âm học có thể được trích xuất từ các codec âm thanh thần kinh với việc tái tạo làm mục tiêu huấn luyện (Zeghidour et al., 2021; Défossez et al., 2022). Sử dụng lượng tử hóa vector dư (RVQ) với các bộ lượng tử hóa phân cấp để rời rạc hóa, các token âm học được biểu diễn như các ma trận bao gồm hai chiều: timestep và bộ lượng tử hóa.

Dựa trên hai loại token âm thanh, có ba phương pháp mô hình hóa cho các mô hình ngôn ngữ âm thanh, như được liệt kê trong Bảng 1: i) Các mô hình ngôn ngữ ngữ nghĩa được xây dựng sử dụng token ngữ nghĩa và sử dụng một vocoder đơn vị bên ngoài cho việc tổng hợp âm thanh. (Lakhotia et al., 2021; Zhang et al., 2023; Hassid et al., 2023). Trong khi nắm bắt nội dung chính xác về mặt ngữ nghĩa, việc tạo âm thanh của chúng dẫn đến chất lượng kém và mất đi các chi tiết âm học. ii) Các mô hình ngôn ngữ âm học được xây dựng trên token âm học. Lấy VALL-E (Wang et al., 2023) làm ví dụ, mặc dù đạt được khả năng text-to-speech (TTS) zero-shot ấn tượng, nó vẫn gặp phải các vấn đề như nội dung không chính xác, do thông tin phức tạp trong các token âm học. iii) Các mô hình ngôn ngữ âm thanh phân cấp bao gồm các mô hình ngôn ngữ token ngữ nghĩa và các mô hình ngôn ngữ token âm học, nắm bắt thông tin nội dung và chi tiết âm học tương ứng (Borsos et al., 2022; Rubenstein et al., 2023; Dong et al., 2023). Cấu trúc này cho thấy kết quả đầy hứa hẹn về cả nội dung và chất lượng âm thanh, nhưng phương pháp mô hình hóa đa giai đoạn phức tạp hơn, dẫn đến một số nhược điểm như tích lũy lỗi và tốc độ xử lý chậm hơn. Ngoài ra, có sự dư thừa thông tin đáng kể giữa token ngữ nghĩa và token âm học, gây ra độ phức tạp mô hình hóa không cần thiết. Một mô hình ngôn ngữ âm thanh lý tưởng không chỉ nên mô hình hóa nội dung chính xác, mà còn tạo ra âm thanh đa dạng, chất lượng cao, đồng thời duy trì kiến trúc đơn giản tinh tế. Tương ứng, các token âm thanh lý tưởng nên đáp ứng hai đặc điểm chính sau: i) Căn chỉnh mạnh mẽ với văn bản; ii) Bảo tồn hiệu quả thông tin âm thanh.

Tuy nhiên, các token âm thanh hiện có không được thiết kế rõ ràng cho việc mô hình hóa ngôn ngữ âm thanh, và chưa có khám phá nào về tính phù hợp của chúng cho việc xây dựng mô hình ngôn ngữ âm thanh. Để giải quyết khoảng trống này, chúng tôi xây dựng Điểm chuẩn Token Mô hình Ngôn ngữ Âm thanh, để đánh giá tính phù hợp của các token âm thanh cho việc xây dựng mô hình ngôn ngữ âm thanh. Đánh giá của chúng tôi tiết lộ rằng các token ngữ nghĩa thể hiện sự căn chỉnh cao với văn bản trong khi mất một số thông tin trong âm thanh, như âm sắc. Token âm học xuất sắc trong việc bảo tồn thông tin âm thanh hiệu quả nhưng không thể hiện sự căn chỉnh mạnh mẽ với văn bản. Với những quan sát này, chúng tôi hướng đến xây dựng các token âm thanh chuyên dụng được thiết kế cho các mô hình ngôn ngữ âm thanh bằng cách thống nhất token ngữ nghĩa và token âm học. Cụ thể, chúng tôi có thể tiến hành phân tách thông tin trong cấu trúc RVQ của token âm học, cho phép bộ lượng tử hóa RVQ đầu tiên tạo ra token chứa thông tin nội dung, tương tự như token ngữ nghĩa, trong khi các bộ lượng tử hóa tiếp theo bổ sung thông tin phi ngôn ngữ còn lại, như được minh họa trong Hình 1.

Với động cơ trên, chúng tôi đề xuất SpeechTokenizer, một tokenizer âm thanh thống nhất cho các mô hình ngôn ngữ âm thanh lớn. SpeechTokenizer áp dụng kiến trúc Encoder-Decoder với lượng tử hóa vector dư. Thống nhất token ngữ nghĩa và token âm học, SpeechTokenizer phân tách các khía cạnh khác nhau của thông tin âm thanh một cách phân cấp qua các lớp RVQ khác nhau. Bằng cách sử dụng một giáo viên ngữ nghĩa để hướng dẫn bộ lượng tử hóa RVQ đầu tiên, các token lớp đầu tiên có thể nắm bắt hiệu quả thông tin nội dung. Với cấu trúc dư, các bộ lượng tử hóa tiếp theo bổ sung thông tin phi ngôn ngữ còn lại.

Dựa trên SpeechTokenizer, chúng tôi xây dựng một Mô hình Ngôn ngữ Âm thanh Thống nhất bao gồm các mô hình tự hồi quy và không tự hồi quy. Kết quả thí nghiệm cho thấy SpeechTokenizer hoạt động tương đương với EnCodec (Défossez et al., 2022) trong việc tái tạo âm thanh và thể hiện hiệu suất mạnh mẽ trên điểm chuẩn SLMTokBench. USLM đáng chú ý vượt trội hơn VALL-E (Wang et al., 2023) trong các tác vụ Text-to-Speech zero-shot.

Những đóng góp của chúng tôi bao gồm:

• Chúng tôi đề xuất SpeechTokenizer, được thiết kế đặc biệt cho các mô hình ngôn ngữ âm thanh lớn và thống nhất các token ngữ nghĩa và âm học thông qua việc phân tách các khía cạnh khác nhau của thông tin âm thanh một cách phân cấp.

• Chúng tôi thiết lập SLMTokBench, điểm chuẩn đầu tiên để đánh giá tính phù hợp của các token âm thanh cho việc xây dựng mô hình ngôn ngữ âm thanh.

• Chúng tôi xây dựng một mô hình ngôn ngữ âm thanh thống nhất dựa trên SpeechTokenizer, vượt trội hơn VALL-E trong tác vụ TTS zero-shot.

## 2. SLMTOKBENCH: ĐIỂM CHUẨN TOKEN MÔ HÌNH NGÔN NGỮ ÂM THANH

Để xây dựng các mô hình ngôn ngữ âm thanh mạnh mẽ, các biểu diễn âm thanh rời rạc nên có hai đặc điểm chính sau: i) Căn chỉnh mạnh mẽ với văn bản; ii) Bảo tồn hiệu quả thông tin âm thanh. Dựa trên tiền đề này, chúng tôi thiết lập Điểm chuẩn Token Mô hình Ngôn ngữ Âm thanh (SLMTokBench) để đánh giá tính phù hợp của các token âm thanh cho việc xây dựng mô hình ngôn ngữ âm thanh.

### 2.1 ĐÁNH GIÁ CĂNG CHỈNH VĂN BẢN

Chúng tôi đánh giá mức độ căn chỉnh văn bản bằng cách ước tính thông tin tương hỗ giữa token âm thanh và văn bản. Về ký hiệu, X ký hiệu các biểu diễn âm thanh rời rạc; Y ký hiệu văn bản; I(X;Y) ký hiệu thông tin tương hỗ; tập dữ liệu kiểm tra được ký hiệu là D={(xi, yi)}N i=1 và θ ký hiệu mô hình hạ nguồn. Thông qua suy dẫn trong Phụ lục A, chúng tôi có thể ước tính I(X;Y) như:

ˆI(X;Y) = 1/N² ∑∑[logqθ(yi|xi) − logqθ(yj|xi)]

trong đó qθ(Y|X) là phân phối biến phân và có thể được tham số hóa bởi mô hình hạ nguồn θ. Mô hình hạ nguồn là một BLSTM 2 lớp 1024 đơn vị vanilla được tối ưu hóa bởi tổn thất CTC trên ký tự và nó nhận token âm thanh làm đầu vào. Cụ thể, đối với mỗi biểu diễn rời rạc, chúng tôi đầu tiên thiết lập một ma trận nhúng, có thể được khởi tạo ngẫu nhiên hoặc được tạo từ ma trận trung tâm k-means hoặc sách mã lượng tử hóa vector thu được trong quá trình rời rạc hóa. Chúng tôi sử dụng ma trận nhúng để nhúng các biểu diễn rời rạc và thu được các biểu diễn liên tục, sau đó được đưa vào các mô hình hạ nguồn. Chúng tôi huấn luyện mô hình hạ nguồn trên tập con LibriSpeech train-clean-100 và sử dụng tập con dev-clean để ước tính thông tin tương hỗ. Chúng tôi cũng tính tỷ lệ lỗi từ (WER) trên tập kiểm tra. Đối với việc huấn luyện mô hình hạ nguồn, chúng tôi cấu hình thiết lập huấn luyện với kích thước batch 32, tỷ lệ học 1e-4, và tổng cộng 200k bước toàn cục.

### 2.2 ĐÁNH GIÁ BẢO TỒN THÔNG TIN

Để đánh giá việc bảo tồn thông tin âm thanh trong các biểu diễn âm thanh rời rạc, chúng tôi chuyển đổi token âm thanh trở lại thành âm thanh và đánh giá âm thanh được tái tổng hợp bằng các thước đo tự động về nội dung và âm sắc. Chúng tôi huấn luyện một unit-HiFIGAN (Polyak et al., 2021) trên tập dữ liệu LibriSpeech để chuyển đổi đơn vị HuBERT thành dạng sóng. Đáng chú ý, để tránh nhiễu từ thông tin bổ sung, chúng tôi không cung cấp bất kỳ thông tin người nói nào trong quá trình huấn luyện. Đối với token Encodec, chúng tôi sử dụng bộ giải mã Encodec để tạo trực tiếp dạng sóng. Việc bảo tồn nội dung được đánh giá bằng cách tính WER thông qua việc chuyển đổi âm thanh được tái tổng hợp sử dụng mô hình Whisper en-medium (Radford et al., 2023). Việc bảo tồn âm sắc được đánh giá bằng cách sử dụng WavLM-TDNN (Chen et al., 2022) để tính toán sự tương đồng người nói giữa âm thanh được tổng hợp và âm thanh thực tế. Chúng tôi lấy mẫu ngẫu nhiên 300 mẫu âm thanh từ tập kiểm tra LibriSpeech để đánh giá.

### 2.3 SO SÁNH TOKEN NGỮ NGHĨA & ÂM HỌC

Chúng tôi sử dụng đơn vị HuBERT L9 để biểu diễn token ngữ nghĩa và mã EnCodec để biểu diễn token âm học. Như được hiển thị trong Bảng 3, token ngữ nghĩa đạt được thông tin tương hỗ cao với văn bản nhưng âm thanh được tái tổng hợp của chúng có độ tương đồng người nói thấp. Token âm học đạt được WER thấp và độ tương đồng người nói cao cho âm thanh được tái tổng hợp nhưng có thông tin tương hỗ thấp với văn bản.

## 3. SPEECH TOKENIZER

### 3.1 CẤU TRÚC MÔ HÌNH

Mô hình của chúng tôi được xây dựng trên khung RVQ-GANs, theo cùng mô hình như SoundStream (Zeghidour et al., 2021) và EnCodec (Défossez et al., 2022). Như được mô tả trong Hình 2, mô hình của chúng tôi sử dụng mạng encoder-decoder dựa trên tích chập từ EnCodec, thực hiện thu nhỏ theo thời gian với hệ số stride được chọn. Đáng chú ý, chúng tôi đã thay thế LSTM hai lớp, ban đầu theo sau các khối tích chập trong encoder EnCodec, bằng BiLSTM hai lớp để tăng cường khả năng mô hình hóa ngữ nghĩa. Chúng tôi tiến hành các nghiên cứu loại bỏ về cấu trúc mô hình trong Phụ lục B. Chúng tôi lượng tử hóa đầu ra encoder sử dụng Lượng tử hóa Vector Dư (RVQ), một phương pháp có thể vận hành lượng tử hóa phần dư theo các bước lượng tử hóa ban đầu với sách mã riêng biệt. Chi tiết thêm về cấu trúc mô hình có thể được tìm thấy trong Phụ lục D. Trong quá trình huấn luyện, một giáo viên ngữ nghĩa cung cấp biểu diễn ngữ nghĩa để hướng dẫn quá trình lượng tử hóa dư.

### 3.2 CHƯNG CẤT NGỮ NGHĨA

Để đạt được việc mô hình hóa phân cấp thông tin đa dạng qua các lớp RVQ khác nhau, chúng tôi sử dụng hướng dẫn ngữ nghĩa cho bộ lượng tử hóa đầu tiên, cho phép nó nắm bắt thông tin nội dung. Tận dụng cấu trúc dư cho phép các bộ lượng tử hóa tiếp theo bổ sung thông tin phi ngôn ngữ còn lại.

Chúng tôi sử dụng HuBERT (Hsu et al., 2021) làm giáo viên ngữ nghĩa trong nghiên cứu này, vì HuBERT được chứng minh bao gồm thông tin nội dung đáng kể (Mohamed et al., 2022). Chúng tôi giới thiệu hai loại chưng cất: chưng cất biểu diễn liên tục và dự đoán nhãn giả.

Đối với chưng cất biểu diễn liên tục, chúng tôi sử dụng biểu diễn HuBERT lớp thứ 9 hoặc biểu diễn trung bình qua tất cả các lớp HuBERT làm giáo viên ngữ nghĩa. Mục tiêu huấn luyện là tối đa hóa độ tương đồng cosine ở cấp độ chiều qua tất cả các timestep giữa đầu ra của lớp đầu tiên RVQ và biểu diễn giáo viên ngữ nghĩa. Chính thức, tổn thất chưng cất liên tục được định nghĩa là:

Ldistill = −1/D ∑log σ(cos(AQ₁^(:,d), S^(:,d)))

trong đó Q₁ và S ký hiệu đầu ra được lượng tử hóa của lớp đầu tiên RVQ và biểu diễn giáo viên ngữ nghĩa tương ứng. A ký hiệu ma trận chiếu và D là chiều của biểu diễn giáo viên ngữ nghĩa. Chỉ số trên (:, d) biểu thị một vector bao gồm các giá trị từ tất cả các timestep tại chiều d. cos(·) biểu diễn độ tương đồng cosine và σ(·) ký hiệu kích hoạt sigmoid. Hàm tổn thất chưng cất liên tục này khác với phương pháp thường được sử dụng, tính toán tổn thất dựa trên các biểu diễn đầu ra của mô hình học sinh và giáo viên tại cùng timestep. Một phân tích so sánh của hai phương pháp này được cung cấp trong Phụ lục C.

Đối với dự đoán nhãn giả, chúng tôi áp dụng đơn vị HuBERT làm nhãn mục tiêu. Mục tiêu huấn luyện được xây dựng như:

Ldistll = −1/T ∑uₜlog(Softmax(Aqₜ₁))

trong đó qₜ₁ và uₜ tương ứng ký hiệu đầu ra được lượng tử hóa của lớp VQ đầu tiên và đơn vị HuBERT tại timestep t. T ký hiệu số bước thời gian và A là ma trận chiếu.

### 3.3 MỤC TIÊU HUẤN LUYỆN

Phương pháp huấn luyện của chúng tôi bao gồm cả tác vụ tái tạo và tác vụ chưng cất ngữ nghĩa. Trong tác vụ tái tạo, chúng tôi sử dụng mục tiêu GAN, tối ưu hóa sự kết hợp của một số hạng tái tạo, một số hạng tổn thất phân biệt, và tổn thất cam kết RVQ. Trong tác vụ chưng cất ngữ nghĩa, mục tiêu huấn luyện bao gồm một số hạng tổn thất chưng cất ngữ nghĩa. Trong phần sau, x biểu diễn một tín hiệu âm thanh và x̂ ký hiệu tín hiệu được tái tạo bởi mạng.

**Tổn thất Tái tạo** Tổn thất tái tạo bao gồm tổn thất miền thời gian và miền tần số. Đối với miền thời gian, chúng tôi tối thiểu hóa khoảng cách L1 giữa x và x̂, tức là Lt = ∥x − x̂∥₁. Đối với miền tần số, chúng tôi kết hợp tuyến tính các tổn thất L1 và L2 trên mel-spectrogram sử dụng nhiều thang thời gian. Chính thức, Lf = ∑ᵢ∈ₑ ∥Sᵢ(x) − Sᵢ(x̂)∥₁ + ∥Sᵢ(x) − Sᵢ(x̂)∥₂, trong đó Sᵢ là một mel-spectrogram 64-bins sử dụng STFT chuẩn hóa với kích thước cửa sổ 2ⁱ và độ dài hop 2ⁱ/4, e = 5,···,11 là tập hợp các thang đo.

**Tổn thất Phân biệt** Chúng tôi sử dụng cùng các bộ phân biệt như HiFi-Codec Yang et al. (2023) bao gồm ba bộ phân biệt: Bộ phân biệt dựa trên STFT đa thang (MS-STFT); một bộ phân biệt đa chu kỳ (MPD) và một bộ phân biệt đa thang (MSD). Chi tiết thêm về các bộ phân biệt có thể được tìm thấy trong Phụ lục D. Tổn thất đối kháng được sử dụng để thúc đẩy chất lượng cảm nhận và nó được định nghĩa như một tổn thất hinge trên logits của bộ phân biệt, được tính trung bình trên nhiều bộ phân biệt và theo thời gian. Gọi K ký hiệu số lượng bộ phân biệt, tổn thất đối kháng cho generator LD được xây dựng như sau, Lg = 1/K ∑ᴷₖ₌₁ max(1 − Dₖ(x̂), 0). Đối với các bộ phân biệt Lg được định nghĩa là:

LD = 1/K ∑ᴷₖ₌₁ max(1 − Dₖ(x), 0) + max(1 + Dₖ(x̂), 0)

Ngoài ra, một tổn thất khớp đặc trưng cho generator được tính như sau:

Lfeat = 1/KL ∑ᴷₖ₌₁ ∑ᴸₗ₌₁ ∥Dₖˡ(x) − Dₖˡ(x̂)∥₁ / mean(∥Dₖˡ(x)∥₁)

trong đó trung bình được tính trên tất cả các chiều và L là số lượng lớp trong các bộ phân biệt.

**Tổn thất Cam kết RVQ** Chúng tôi thêm một tổn thất cam kết Lw giữa giá trị trước lượng tử hóa và giá trị được lượng tử hóa của nó, không có gradient được tính cho giá trị được lượng tử hóa. Tổn thất cam kết RVQ được định nghĩa là: Lw = ∑ᴺᵠᵢ₌₁ ∥zᵢ − zᵠᵢ∥₂², trong đó zᵢ và zᵠᵢ ký hiệu phần dư hiện tại và entry gần nhất trong sách mã tương ứng.

Nhìn chung, generator được huấn luyện để tối ưu hóa tổn thất sau:

LG = λₜLₜ + λfLf + λgLg + λfeatLfeat + λwLw + λdistillLdistill

trong đó λₜ, λf, λg, λfeat, λw và λdistill là các siêu tham số được sử dụng để cân bằng mỗi số hạng tổn thất.

### 3.4 MÔ HÌNH NGÔN NGỮ ÂM THANH THỐNG NHẤT

Như được hiển thị trong Hình 1, chúng tôi có thể xây dựng một mô hình ngôn ngữ âm thanh thống nhất dựa trên SpeechTokenizer. Bao gồm các mô hình tự hồi quy và không tự hồi quy, nó có thể mô hình hóa thông tin trong âm thanh một cách phân cấp. Mô hình tự hồi quy (AR) nắm bắt thông tin nội dung bằng cách mô hình hóa token từ bộ lượng tử hóa RVQ đầu tiên. Mô hình không tự hồi quy (NAR) bổ sung thông tin phi ngôn ngữ cho mô hình AR bằng cách tạo token từ các bộ lượng tử hóa tiếp theo được điều kiện hóa trên token lớp đầu tiên. Chúng tôi xác thực hiệu quả của mô hình ngôn ngữ âm thanh thống nhất trên tác vụ TTS zero-shot.

Mô hình AR được xây dựng dựa trên token lớp đầu tiên c₁. Sử dụng kiến trúc transformer decoder-only θAR, chúng tôi tiếp cận việc chuyển đổi này như một tác vụ mô hình hóa ngôn ngữ nguyên nhân với chuỗi phoneme u phục vụ như gợi ý cho mô hình AR. Mục tiêu huấn luyện có thể được công thức hóa như

LAR = −log ∏ᵀₜ₌₀ p(cₜ₁|c<ₜ₁, u; θAR)

Mô hình NAR tạo ra token c₂:₈ từ các bộ lượng tử hóa tiếp theo. Kiến trúc của nó giống với mô hình AR, bao gồm tám lớp nhúng âm học riêng biệt và các lớp dự đoán đầu ra. Để kiểm soát đặc điểm của giọng nói của người nói, n gợi ý âm học Ĉ được sử dụng để hướng dẫn âm sắc. Mô hình được điều kiện hóa trên chuỗi phoneme u, gợi ý âm học Ĉ và token từ các bộ lượng tử hóa trước đó, dẫn đến việc công thức hóa mục tiêu huấn luyện như sau

LNAR = −log ∏⁸ᵢ₌₂ p(cᵢ|c<ᵢ, Ĉ, u; θNAR)

Trong quá trình suy luận, chúng tôi chuyển đổi đầu vào văn bản thành chuỗi phoneme và gợi ý âm thanh thành token âm thanh. Chúng được nối lại để tạo thành gợi ý cho các mô hình AR và NAR. Được điều kiện hóa trên đó, mô hình AR tạo ra token cấp đầu tiên, trong khi mô hình NAR lặp lại tạo ra token của các cấp tiếp theo. Các token được tạo bởi các mô hình AR và NAR sau đó được nối lại để xây dựng ma trận token âm thanh. Cuối cùng, chúng tôi sử dụng bộ giải mã SpeechTokenizer để tạo dạng sóng được điều kiện hóa trên ma trận token hoàn chỉnh.

## 4. THÍ NGHIỆM

### 4.1 THIẾT LẬP THÍ NGHIỆM

**Tập dữ liệu** Đối với việc huấn luyện SpeechTokenizer, chúng tôi sử dụng tập dữ liệu LibriSpeech (Panayotov et al., 2015). Chúng tôi cắt ngẫu nhiên một đoạn 3 giây từ các mẫu âm thanh tại mỗi lần lặp huấn luyện. Đối với TTS zero-shot, chúng tôi huấn luyện các mô hình AR và NAR trên tập con tiếng Anh của tập dữ liệu Multilingual LibriSpeech (Pratap et al., 2020), chứa 44K giờ dữ liệu âm thanh được chuyển mã từ các audiobook LibriVox. Chúng tôi chọn các mẫu âm thanh có thời lượng từ 3 đến 14 giây cho dữ liệu huấn luyện. Tỷ lệ lấy mẫu là 16KHz cho tất cả dữ liệu âm thanh.

**Mô hình** Đối với SpeechTokenizer, chúng tôi giới thiệu chi tiết về cấu trúc mô hình trong phần 3.1 và Phụ lục D. Đối với các thí nghiệm TTS zero-shot, mô hình AR và mô hình NAR đều là Transformer decoder 12 lớp với 16 đầu attention, chiều attention 1024 và chiều FFN 4096.

**Huấn luyện** Đối với SpeechTokenizer, mô hình được huấn luyện trên 2 GPU A800 trong 20 epoch với tỷ lệ học tối đa 4e-4 và kích thước batch 20 mỗi GPU. Đối với Mô hình Ngôn ngữ Âm thanh Thống nhất, cả mô hình AR và NAR đều được huấn luyện trên 8 GPU A800 trong 500k bước với tỷ lệ học tối đa 5e-4. Mô hình AR được huấn luyện với kích thước batch 7500 token mỗi GPU, và mô hình NAR được huấn luyện với kích thước batch 5000 token mỗi GPU.

**Đường cơ sở** Chúng tôi áp dụng EnCodec_24khz_6kpbs (sau này được gọi là EnCodec) (Défossez et al., 2022) làm đường cơ sở cho SpeechTokenizer và VALL-E (Wang et al., 2023) làm hệ thống đường cơ sở cho TTS zero-shot. Chúng tôi huấn luyện VALL-E dưới cùng tập dữ liệu và thiết lập thí nghiệm như EnCodec.

### 4.2 ĐÁNH GIÁ TÁI TẠO ÂM THANH

Chúng tôi lấy mẫu ngẫu nhiên 300 mẫu âm thanh từ tập kiểm tra LibriSpeech để đánh giá tái tạo âm thanh. Chúng tôi xem xét cả thước đo đánh giá chủ quan và khách quan.

**Thước đo Khách quan** Chúng tôi sử dụng thước đo ViSQOL (Hines et al., 2012) để đo chất lượng âm thanh. Ngoài ra, chúng tôi đánh giá độ chính xác nội dung thông qua Tỷ lệ Lỗi Từ (WER) bằng cách chuyển mã âm thanh sử dụng mô hình Whisper en-medium (Radford et al., 2023).

**Thước đo Chủ quan** Chúng tôi áp dụng phương pháp đông đúc được truyền cảm hứng từ giao thức MUSHRA (Series, 2014), với tham chiếu ẩn nhưng không có neo được lọc lowpass, cho đánh giá chủ quan. Chúng tôi hướng dẫn các đánh giá viên đánh giá chất lượng cảm nhận của các mẫu đã cho trên thang điểm từ 1 đến 100.

### 4.3 ĐÁNH GIÁ MÔ HÌNH NGÔN NGỮ ÂM THANH THỐNG NHẤT

Chúng tôi tiến hành đánh giá TTS zero-shot trên tập dữ liệu VCTK, bao gồm 108 người nói. Không có sự chồng chéo người nói giữa dữ liệu huấn luyện và tập dữ liệu VCTK. Đối với mỗi người nói, chúng tôi ngẫu nhiên chọn một utterance 3s làm gợi ý trong khi nội dung văn bản của một utterance khác được sử dụng làm văn bản đầu vào.

**Thước đo Khách quan** Chúng tôi đánh giá các hệ thống TTS với độ tương đồng người nói và WER. Chúng tôi đánh giá độ tương đồng người nói giữa âm thanh được tạo và âm thanh gợi ý. Chúng tôi tính toán độ tương đồng với các bước sau: 1) chúng tôi sử dụng WavLM-TDNN để tính toán nhúng người nói cho âm thanh được tạo và âm thanh gợi ý. 2) chúng tôi tính toán độ tương đồng cosine giữa các nhúng được chuẩn hóa. Chúng tôi sử dụng mô hình Whisper medium để chuyển mã âm thanh được tạo và tính toán WER.

**Thước đo Chủ quan** Chúng tôi xác định Điểm Ý kiến Trung bình (MOS) và Điểm Ý kiến Trung bình Tương đồng (SMOS) thông qua đánh giá của con người. MOS phản ánh tính tự nhiên của âm thanh, trong khi SMOS đánh giá mức độ tương đồng với giọng nói gốc của người nói. Chúng tôi thuê 12 và 6 người nói bản ngữ làm đóng góp viên cho các đánh giá MOS và SMOS, tương ứng. MOS và SMOS đều trải dài từ 1 đến 5, với giá trị cao hơn biểu thị chất lượng âm thanh và tương đồng giọng nói lớn hơn tương ứng.

### 4.4 KẾT QUẢ CHÍNH

**Tái tạo Âm thanh** Bảng 2 tóm tắt kết quả của các thí nghiệm tái tạo âm thanh. SpeechTokenizer đạt được WER thấp hơn Encodec, thể hiện khả năng vượt trội trong việc bảo tồn nội dung. Ngoài ra, SpeechTokenizer đạt được điểm VISQOL tương đương nhưng điểm MUSHRA cao hơn EnCodec, cho thấy khả năng mạnh mẽ hơn trong việc tạo ra âm thanh chất lượng cao.

**Hiệu suất trên SLMTokBench** Bảng 3 hiển thị hiệu suất của SpeechTokenizer trên SLMTokBench. So với EnCodec-RVQ-1, SpeechTokenizer-RVQ-1 đạt được thông tin tương hỗ cao hơn giữa văn bản và WER thấp hơn của mô hình hạ nguồn. Điều này cho thấy SpeechTokenizer thể hiện sự căn chỉnh mạnh mẽ hơn với nội dung văn bản. Trong khi đó, âm thanh được tái tổng hợp của token SpeechTokenizer RVQ-1 đạt được WER thấp hơn và độ tương đồng người nói, cho thấy khả năng giữ lại nhiều thông tin liên quan đến nội dung hơn trong khi bỏ qua các đặc điểm âm sắc, tương tự như token ngữ nghĩa. Âm thanh được tái tổng hợp của token SpeechTokenizer RVQ-1:8 thể hiện WER thấp và độ tương đồng người nói cao, minh họa năng lực của SpeechTokenizer trong việc bảo tồn thông tin âm thanh toàn diện, tương tự như token âm học. Hơn nữa, độ tương đồng người nói của âm thanh được tái tổng hợp của token SpeechTokenizer RVQ-1 đáng chú ý thấp, trong khi đó của token SpeechTokenizer RVQ-1:8 là cao đáng kể. Quan sát này ngụ ý rằng các token từ các lớp tiếp theo bù đắp cho thông tin âm sắc bị loại bỏ bởi token lớp đầu tiên.

**TTS Zero-shot** Như được hiển thị trong Bảng 4, USLM của chúng tôi thể hiện WER thấp hơn VALL-E. Kết quả này làm nổi bật rằng SpeechTokenizer có thể đóng góp vào việc mô hình hóa chính xác hơn thông tin nội dung. Ngoài ra, USLM thể hiện độ tương đồng người nói vượt trội, ngụ ý rằng cấu trúc thông tin tách rời có lợi hơn cho việc mô hình hóa thông tin liên quan đến người nói.

## 5. PHÂN TÍCH

### 5.1 LỰA CHỌN GIÁO VIÊN NGỮ NGHĨA

Như được hiển thị trong Bảng 3, làm giáo viên ngữ nghĩa, biểu diễn HuBERT L9 hoạt động tốt hơn đơn vị HuBERT trong cả Căn chỉnh Văn bản và Bảo tồn Thông tin, bất kể đó là RVQ-1 hay RVQ-1:8. Lý do có thể là các đơn vị HuBERT rời rạc mất một số thông tin nội dung so với các biểu diễn liên tục, do đó cung cấp hướng dẫn ngữ nghĩa yếu hơn cho SpeechTokenizer. Khi so sánh biểu diễn HuBERT L9 với biểu diễn trung bình HuBERT, chúng tôi thấy rằng về Căn chỉnh Văn bản, thông tin tương hỗ cao hơn khi biểu diễn HuBERT L9 phục vụ như giáo viên. Điều này là do biểu diễn trung bình HuBERT chứa một số thông tin âm sắc, trong khi HuBERT L9 cung cấp thông tin nội dung thuần khiết hơn. Mặt khác, trung bình HuBERT cho thấy hiệu suất tốt hơn trong Bảo tồn Thông tin, được phản ánh trong WER thấp hơn. Chúng tôi suy đoán rằng điều này là do một mức độ xung đột tác vụ nhất định giữa chưng cất ngữ nghĩa và tái tạo, trong đó cái trước nhằm chỉ giữ lại thông tin nội dung trong khi cái sau nhằm bảo tồn các khía cạnh khác nhau của âm thanh. Sự hiện diện của một số thông tin âm sắc trong biểu diễn trung bình HuBERT có thể ở một mức độ nào đó làm giảm xung đột tác vụ này.

### 5.2 HIỆU QUẢ CỦA PHÂN TÁCH THÔNG TIN

Để chứng minh rằng thông tin âm thanh khác nhau có thể được mô hình hóa phân cấp trong SpeechTokenizer, chúng tôi tiến hành thí nghiệm chuyển đổi giọng nói một lần (VC). Thí nghiệm này nhằm chuyển đổi âm thanh từ bất kỳ người nói nguồn nào sang một người nói mục tiêu tùy ý chỉ sử dụng vài giây âm thanh tham chiếu từ người nói mục tiêu. Để sử dụng SpeechTokenizer cho VC một lần, bước đầu tiên là chuyển đổi âm thanh nguồn và âm thanh tham chiếu thành ma trận token. Bằng cách nối các token RVQ-1 của ma trận token nguồn với token RVQ-2:8 của ma trận token tham chiếu, và sau đó chuyển ma trận token kết hợp này qua bộ giải mã, chúng tôi có thể thực hiện chuyển đổi giọng nói. Độ dài của token tham chiếu và nguồn có thể không căn chỉnh hoàn hảo. Để giải quyết điều này, chúng tôi sử dụng cắt bớt hoặc đệm vòng tròn để đảm bảo chúng chia sẻ cùng độ dài thời gian, từ đó tạo điều kiện cho quá trình nối. Chúng tôi tiến hành thí nghiệm trên tập dữ liệu VCTK. Chúng tôi ngẫu nhiên chọn một mẫu âm thanh từ một người nói để phục vụ như âm thanh nguồn. Từ 107 người nói còn lại, chúng tôi riêng lẻ chọn một mẫu âm thanh có nội dung khác để hoạt động như âm thanh tham chiếu. Chúng tôi sử dụng hai thước đo để đánh giá: WER và độ tương đồng người nói.

Bảng 5 báo cáo kết quả của các thí nghiệm VC một lần. Từ bảng, chúng ta có thể thấy rằng khi số lượng lớp cho token tham chiếu tăng, độ tương đồng người nói cũng dần tăng. Điều này cho thấy nhiều thông tin từ người nói tham chiếu đang được chuyển giao, chứng minh rằng thông tin người nói được nhúng trong token từ lớp thứ hai đến lớp cuối cùng. Khi token tham chiếu được chọn từ lớp thứ hai đến lớp thứ tư, chúng tôi đạt được WER thấp và độ tương đồng người nói cao, dẫn đến hiệu suất VC một lần thỏa mãng. Điều này cho thấy việc phân tách thông tin thành công.

Chúng tôi cũng trực quan hóa đầu ra được lượng tử hóa từ các lớp khác nhau trong Hình 3. Cụ thể, chúng tôi ngẫu nhiên chọn năm người nói từ tập dữ liệu VCTK và chọn 10 mẫu âm thanh ngẫu nhiên mỗi người nói. Chúng tôi trích xuất đầu ra được lượng tử hóa của các lớp RVQ khác nhau của SpeechTokenizer. Đầu ra lớp đầu tiên được ký hiệu là biểu diễn RVQ-1, trong khi tổng của các đầu ra từ lớp thứ hai đến lớp thứ tám được ký hiệu là biểu diễn RVQ-2:8. Bằng cách thực hiện gộp trung bình dọc theo chiều thời gian, mỗi biểu diễn được chuyển đổi thành một vector duy nhất. Các vector này sau đó được trực quan hóa trong không gian 2D sử dụng t-SNE, với các mẫu âm thanh từ cùng người nói được biểu diễn bằng cùng màu. Từ biểu đồ, có thể quan sát thấy rằng các biểu diễn RVQ-1 cho các người nói khác nhau được phân tán ngẫu nhiên mà không có mô hình có thể nhận biết. Ngược lại, các biểu diễn RVQ-2:8 cho cùng người nói có xu hướng tụ lại với nhau, trong khi khác biệt với những người nói khác. Điều này cho thấy thông tin cụ thể của người nói được chứa từ lớp thứ hai đến lớp thứ tám.

## 6. NGHIÊN CỨU LIÊN QUAN

Nghiên cứu liên quan của chúng tôi được đặt trong Phụ lục E.

## 7. KẾT LUẬN

Trong nghiên cứu này, chúng tôi trình bày SLMTokBench, đánh giá tác động của các loại token âm thanh khác nhau. Đồng thời, chúng tôi đề xuất SpeechTokenizer, để thống nhất việc rời rạc hóa cả hai loại token âm thanh nhằm khắc phục vấn đề sử dụng nhiều mô hình để trích xuất token rời rạc ngữ nghĩa và âm học riêng biệt. Hơn nữa, chúng tôi phát triển một mô hình ngôn ngữ âm thanh thống nhất (USLM) dựa trên SpeechTokenizer, với kết quả tốt hơn về độ chính xác nội dung và chất lượng của âm thanh được tạo. Nghiên cứu về tokenizer âm thanh thống nhất là một phần thiết yếu của sự phát triển thêm của mô hình ngôn ngữ âm thanh về mặt hiệu quả và chất lượng.

## TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được giữ nguyên tiếng Anh như trong bản gốc]

## PHỤ LỤC

### A. ƯỚC TÍNH THÔNG TIN TƯƠNG HỖ

[Nội dung phụ lục được dịch tương tự như phần chính]

### B. LOẠI BỎ CẤU TRÚC MÔ HÌNH

[Nội dung phụ lục được dịch tương tự như phần chính]

### C. PHÂN TÍCH TỔNG THẤT CHƯNG CẤT LIÊN TỤC

[Nội dung phụ lục được dịch tương tự như phần chính]

### D. CHI TIẾT CẤU TRÚC MÔ HÌNH VÀ BỘ PHÂN BIỆT

[Nội dung phụ lục được dịch tương tự như phần chính]

### E. NGHIÊN CỨU LIÊN QUAN

[Nội dung phụ lục được dịch tương tự như phần chính]

### F. PHÂN TÍCH SÁCH MÃ

[Nội dung phụ lục được dịch tương tự như phần chính]

### G. MỞ RỘNG CHO NGÔN NGỮ CHƯA THẤY

[Nội dung phụ lục được dịch tương tự như phần chính]

### H. PHÂN TÍCH MELSPECTROGRAM

[Nội dung phụ lục được dịch tương tự như phần chính]
