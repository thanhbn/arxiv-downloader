# 2309.11977.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/audio/2309.11977.pdf
# Kích thước tệp: 293878 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
CẢI THIỆN TỔNG HỢP GIỌNG NÓI TỪNG VỘN KHÔNG CẦN HUẤN LUYỆN DỰA TRÊN MÔ HÌNH NGÔN NGỮ VỚI CÁC GỢI Ý ÂM THANH ĐA TỈ LỆ
Shun Lei1‡, Yixuan Zhou1†, Liyang Chen1, Dan Luo1, Zhiyong Wu1,3∗, Xixin Wu3∗,
Shiyin Kang2∗, Tao Jiang2, Yahui Zhou2, Yuxing Han1, Helen Meng3
1Trường Sau Đại học Quốc tế Thâm Quyến, Đại học Thanh Hoa, Thâm Quyến
2Skywork AI PTE. LTD., Bắc Kinh3Đại học Trung văn Hồng Kông, Đặc khu Hành chính Hồng Kông
{leis21, yx-zhou23 }@mails.tsinghua.edu.cn, zywu @sz.tsinghua.edu.cn, wuxx @se.cuhk.edu.hk, shiyin.kang @kunlun-inc.com
TÓM TẮT
Tổng hợp giọng nói từ văn bản không cần huấn luyện (TTS) nhằm sao chép giọng nói của bất kỳ người nói chưa thấy nào mà không cần tham số thích ứng. Bằng cách lượng tử hóa dạng sóng giọng nói thành các token âm thanh rời rạc và mô hình hóa các token này với mô hình ngôn ngữ, các mô hình TTS dựa trên mô hình ngôn ngữ gần đây cho thấy khả năng thích ứng người nói không cần huấn luyện chỉ với gợi ý âm thanh 3 giây của người nói chưa thấy. Tuy nhiên, chúng bị giới hạn bởi độ dài của gợi ý âm thanh, điều này làm cho việc sao chép phong cách nói cá nhân trở nên khó khăn. Trong bài báo này, chúng tôi đề xuất một mô hình TTS không cần huấn luyện mới với các gợi ý âm thanh đa tỉ lệ dựa trên mô hình ngôn ngữ. Một bộ mã hóa văn bản nhận biết người nói được đề xuất để học phong cách nói cá nhân ở mức âm vị từ gợi ý phong cách bao gồm nhiều câu. Tiếp theo, một bộ giải mã âm thanh dựa trên V ALL-E được sử dụng để mô hình hóa âm sắc từ gợi ý âm sắc ở mức khung hình và tạo ra giọng nói. Kết quả thí nghiệm cho thấy phương pháp đề xuất của chúng tôi vượt trội hơn các phương pháp cơ sở về tính tự nhiên và độ tương tự người nói, và có thể đạt được hiệu suất tốt hơn bằng cách mở rộng ra gợi ý phong cách dài hơn1.
Từ khóa —tổng hợp giọng nói, không cần huấn luyện, gợi ý âm thanh đa tỉ lệ, thích ứng người nói, mô hình ngôn ngữ
1. GIỚI THIỆU
Tổng hợp giọng nói từ văn bản (TTS) nhằm tạo ra giọng nói tự nhiên và dễ hiểu từ văn bản. Với sự phát triển của học sâu, các mô hình TTS dựa trên mạng nơ-ron đã có thể tổng hợp giọng nói chất lượng cao cho một [1, 2] hoặc nhiều người nói [3, 4]. Tuy nhiên, các mô hình này vẫn yêu cầu lượng dữ liệu giọng nói sạch đủ lớn cho những người nói mới, điều này cản trở sự phát triển của công nghệ tổng hợp giọng nói cho nhiều ứng dụng cá nhân hóa. Do đó, việc thích ứng các mô hình TTS cho bất kỳ người nói nào với ít dữ liệu nhất có thể, trong khi đạt được độ tương tự người nói cao và tính tự nhiên của giọng nói, đã thu hút sự quan tâm ngày càng tăng trong học thuật và công nghiệp [5].
Một trong những cách tiếp cận chung là tinh chỉnh một mô hình TTS đa người nói được huấn luyện tốt với một ít dữ liệu thích ứng để hỗ trợ những người nói mới. Một số nghiên cứu dành nỗ lực để tinh chỉnh toàn bộ mô hình TTS [6,7], và các phương pháp gần đây khác tìm cách giảm tham số thích ứng bằng cách chỉ tinh chỉnh một phần của mô hình [8], hoặc chỉ nhúng người nói [9]. Tuy nhiên, hiệu suất thích ứng của các phương pháp này phụ thuộc rất nhiều vào chất lượng và số lượng dữ liệu có sẵn cho người nói mục tiêu.
‡Công việc được thực hiện khi tác giả đầu tiên thực tập tại Skywork AI PTE. LTD.
†Đóng góp bằng nhau.∗Tác giả liên hệ.
1Mẫu giọng nói: https://thuhcsi.github.io/icassp2024-msvallể giải quyết khuyết điểm này, một số công trình tiến hành thích ứng không cần huấn luyện, sử dụng chỉ vài giây giọng nói để sao chép giọng nói của người nói chưa thấy mà không cần tinh chỉnh mô hình. Trong [10–12], một bộ mã hóa người nói được sử dụng để trích xuất nhúng người nói toàn cục từ giọng nói tham chiếu đã cho, cho phép mô hình TTS sao chép âm sắc tổng thể của giọng nói tham chiếu. Xem xét rằng việc mô tả đặc điểm cá nhân của người nói bằng một nhúng người nói duy nhất là khó khăn, [13, 14] đề xuất trích xuất nhúng người nói tinh vi để cải thiện chất lượng giọng nói được tổng hợp.
Được thúc đẩy bởi những tiến bộ trong các mô hình tạo ngôn ngữ tự nhiên, các hệ thống tạo giọng nói gần đây [15–17] giới thiệu ý tưởng sử dụng codec âm thanh nơ-ron [18, 19] để lượng tử hóa dạng sóng giọng nói thành các token rời rạc và tận dụng mô hình ngôn ngữ dựa trên gợi ý (ví dụ, GPT-3 [20]) để dự đoán các token này. Các hệ thống TTS dựa trên mô hình ngôn ngữ này có thể được huấn luyện trên các bộ dữ liệu giọng nói đa người nói lớn, đa dạng và chất lượng thấp để cải thiện hiệu suất tổng quát hóa. Với những cách tiếp cận này, các mô hình có khả năng sao chép âm sắc của người nói chỉ với gợi ý âm thanh 3 giây.
Tuy nhiên, các phương pháp TTS không cần huấn luyện dựa trên mô hình ngôn ngữ ở trên chỉ xem xét gợi ý âm thanh ở mức khung hình, dẫn đến hai hạn chế chính. Thứ nhất, đặc điểm người nói của một người bao gồm không chỉ âm sắc mà còn phong cách nói cá nhân, bao gồm các yếu tố khác nhau như ngữ điệu, giọng địa phương và thói quen phát âm. Trong khi việc xem xét gợi ý âm thanh ở mức khung hình đã cho thấy sức mạnh to lớn của việc sao chép âm sắc, nó đã được chứng minh rằng các biểu diễn mức âm vị phù hợp hơn để tạo ra phong cách nói cá nhân [21, 22]. Thứ hai, bị giới hạn bởi cấu trúc của mô hình ngôn ngữ chỉ giải mã, các công trình này chỉ hỗ trợ gợi ý âm thanh ngắn vì chuỗi token âm thanh mức khung hình quá dài (một giọng nói 10 giây thường chứa hàng nghìn token). Việc sử dụng thông tin hạn chế chứa trong gợi ý âm thanh ngắn để sao chép chính xác đặc điểm người nói của người nói mục tiêu là khó khăn, dẫn đến tính tự nhiên và độ tương tự phong cách nói kém. Ngoài ra, các phương pháp dựa trên mô hình ngôn ngữ hiện tại không có khả năng sử dụng nhiều mẫu tham chiếu để nâng cao chất lượng TTS không cần huấn luyện mặc dù có sẵn nhiều phát ngôn của người nói mục tiêu trong quá trình suy luận trong nhiều tình huống thực tế.
Để cải thiện hơn nữa độ tương tự người nói cho tổng hợp TTS không cần huấn luyện dựa trên mô hình ngôn ngữ, chúng tôi đề xuất sử dụng các gợi ý âm thanh đa tỉ lệ để nắm bắt cả âm sắc và phong cách nói cá nhân của người nói mục tiêu. Mô hình của chúng tôi chứa một bộ mã hóa văn bản nhận biết người nói, sử dụng mô-đun chú ý tham chiếu để mô hình hóa phong cách nói cá nhân ở mức âm vị từ gợi ý phong cách bao gồm nhiều phát ngôn và một bộ giải mã âm thanh, giữ nguyên âm sắc được chỉ định bằng cách xem xét gợi ý âm sắc ở mức khung hình dựa trên mô hình ngôn ngữ codec nơ-ron (gọi là V ALL-E). Mô hình cho phép mở rộng ra độ dài tùy ý của gợi ý phong cách để mô tả đặc điểm người nói chi tiết. Cả đánh giá chủ quan và khách quan đều cho thấy phương pháp đề xuất của chúng tôi vượt trội hơn mô hình TTS không cần huấn luyện dựa trên mô hình ngôn ngữ tiên tiến [16] và các phương pháp cơ sở khác về tính tự nhiên và độ tương tự người nói. Hiệu suất cũng được cải thiện với số lượng câu tăng dần được sử dụng trong gợi ý phong cách trong quá trình suy luận.
2. PHƯƠNG PHÁP LUẬN
Kiến trúc của mô hình đề xuất của chúng tôi được minh họa trong Hình 1. Nó bao gồm hai phần chính: một bộ mã hóa văn bản nhận biết người nói và một bộ giải mã âm thanh dựa trên V ALL-E. Trong bài báo này, chúng tôi tuân theo Naturalspeech 2 [23] và V ALL-E [16] để tận dụng các mô hình codec âm thanh nơ-ron để biểu diễn gợi ý phong cách và gợi ý âm sắc trong nhúng âm thanh liên tục và token âm thanh rời rạc, tương ứng. Bộ mã hóa văn bản nhận biết người nói được sử dụng để trích xuất thông tin phong cách nói cá nhân mức âm vị từ gợi ý phong cách và hợp nhất nó vào nhúng âm vị được mã hóa bằng mô-đun chú ý tham chiếu để có được nhúng văn bản nhận biết người nói. Sau đó, đầu ra của bộ mã hóa được đưa vào bộ giải mã âm thanh cùng với các token âm thanh của gợi ý âm sắc để tạo ra giọng nói có cùng âm sắc với gợi ý âm sắc. Chi tiết của từng thành phần như sau.
2.1. Bộ Mã hóa Văn bản Nhận biết Người nói
Bộ mã hóa văn bản nhận biết người nói được thiết kế đặc biệt để trích xuất và mô hình hóa phong cách nói cá nhân ở mức âm vị từ gợi ý phong cách có độ dài tùy ý và hợp nhất thông tin nội dung bên văn bản với thông tin phong cách bên giọng nói để có được nhúng văn bản nhận biết người nói. Kiến trúc của bộ mã hóa được minh họa trong Hình 2, bao gồm một bộ mã hóa âm vị, một bộ mã hóa âm thanh và một mô-đun chú ý tham chiếu.
Ở bên văn bản, để có được biểu diễn văn bản tốt hơn làm đầu vào bộ giải mã, chúng tôi giới thiệu một bộ mã hóa âm vị để mã hóa chuỗi âm vị. Chúng tôi sử dụng khối transformer, là một chồng lớp tự chú ý và tích chập 1D như trong Fastspeech 2 [2], làm cấu trúc cơ bản cho bộ mã hóa. Các văn bản đầu vào được chuyển đổi thành chuỗi âm vị bằng mô-đun grapheme-to-phoneme và sau đó được chuyển đến bộ mã hóa âm vị để có được nhúng âm vị.
Ở bên giọng nói, để sử dụng các gợi ý giọng nói có độ dài tùy ý, các cách tiếp cận trước đây cố gắng mã hóa đặc điểm người nói thành một vectơ mức toàn cục [11, 12]. Kết quả là, các biến đổi tinh vi cục bộ trong phong cách nói bị bỏ qua. Khác với cách này, chúng tôi sử dụng một bộ mã hóa âm thanh để có được nhúng phong cách nói cục bộ từ gợi ý phong cách thay vì một vectơ duy nhất. Tất cả các phát ngôn của người nói mục tiêu đầu tiên được nối với nhau để tạo thành gợi ý phong cách, và sau đó được chuyển đến một codec âm thanh nơ-ron được huấn luyện tốt để chuyển đổi dạng sóng giọng nói thành nhúng âm thanh liên tục thay vì token rời rạc để bảo tồn càng nhiều thông tin phong cách cá nhân có thể trong giọng nói. Sau đó, nhúng âm thanh đã chuyển đổi được chuyển đến bộ mã hóa âm thanh, bao gồm một chồng 8 lớp tích chập 1D. Ngoài ra, để điều chỉnh độ chi tiết thời gian của các biểu diễn phong cách được trích xuất gần hơn với nhận thức giọng nói của con người, bước lọc của các lớp tích chập được đặt là [2,1,2,1,2,1,2,1] để giảm mẫu 16 lần (khoảng 0,2s). Sau đó, độ chi tiết thời gian của nhúng phong cách được cải tạo đúng cách thành mức gần âm vị được lấy cảm hứng từ [24].
Để sử dụng tốt hơn nhúng phong cách được trích xuất từ gợi ý phong cách, một mô-đun chú ý tham chiếu được giới thiệu để có được phong cách nói cá nhân liên quan ngữ nghĩa mức âm vị phù hợp. Chúng tôi áp dụng chú ý tích vô hướng có tỉ lệ làm mô-đun chú ý tham chiếu. Nhúng âm vị được coi là truy vấn, trong khi tất cả nhúng phong cách được trích xuất từ gợi ý phong cách được coi là cả khóa và giá trị. Mức độ liên quan giữa chúng được sử dụng để hướng dẫn việc lựa chọn phong cách nói cá nhân cho mỗi âm vị đầu vào. Cuối cùng, mô-đun chú ý tham chiếu xuất ra một chuỗi căn chỉnh có cùng độ dài với nhúng âm vị và thêm nó vào nhúng âm vị để tạo thành nhúng văn bản nhận biết người nói.
2.2. Bộ Giải mã Âm thanh
Để mô hình hóa đặc điểm người nói của một người, cần phải sao chép âm sắc ngoài việc bắt chước phong cách nói của người nói. Được truyền cảm hứng từ thành công của các mô hình ngôn ngữ trong TTS không cần huấn luyện, phương pháp đề xuất của chúng tôi áp dụng V ALL-E [16] đã được sửa đổi làm bộ giải mã âm thanh để tạo ra giọng nói có cùng âm sắc với gợi ý âm sắc 3 giây. Như minh họa trong Hình 3, bộ giải mã bao gồm một nhúng âm thanh, một bộ giải mã transformer tự hồi quy (AR) và một bộ giải mã transformer không tự hồi quy (NAR).
Gợi ý âm sắc đầu tiên được chuyển đến một codec âm thanh nơ-ron được huấn luyện tốt, và đầu ra của bộ lượng tử hóa vectơ dư trong codec được coi là token âm thanh gợi ý rời rạc. Các token này bao gồm 8 lớp sau đó được nhúng thông qua tám lớp nhúng âm thanh riêng biệt. Bộ giải mã transformer AR được sử dụng để tạo ra lớp đầu tiên của token âm thanh cần thiết để tổng hợp giọng nói cá nhân hóa có điều kiện trên nhúng văn bản nhận biết người nói. Trong khi đó, lớp đầu tiên của token âm thanh của gợi ý âm sắc được sử dụng làm tiền tố trong giải mã AR. Bộ giải mã transformer NAR sau đó được sử dụng để tạo ra token âm thanh của bảy lớp khác theo thứ tự. Để dự đoán token âm thanh của lớp thứ i, đầu vào transformer là sự nối của nhúng văn bản nhận biết người nói, tổng của token âm thanh nhúng của gợi ý âm sắc từ lớp 1 đến lớp i và tổng của token âm thanh được dự đoán nhúng từ lớp 1 đến lớp i−1. Cuối cùng, lớp đầu tiên của token âm thanh được dự đoán bởi bộ giải mã transformer AR và các lớp còn lại của token âm thanh được dự đoán bởi bộ giải mã transformer NAR được nối để tạo thành token âm thanh được dự đoán.
2.3. Chiến lược Huấn luyện và Quy trình Suy luận
Trong giai đoạn huấn luyện, đối với mỗi mẫu huấn luyện, chúng tôi ngẫu nhiên chọn 5 đến 10 phát ngôn tham chiếu được nói bởi cùng người nói với mẫu để tạo thành gợi ý phong cách. Đối với các epoch huấn luyện khác nhau, các gợi ý phong cách khác nhau được chọn ngẫu nhiên cho cùng một mẫu huấn luyện để tăng cường dữ liệu. Khác với V ALL-E huấn luyện hai mô hình riêng biệt, phương pháp đề xuất của chúng tôi huấn luyện toàn bộ hệ thống đầu cuối đến đầu cuối cùng với mất mát entropy chéo. Mất mát huấn luyện là một kết hợp tuyến tính của mất mát bộ giải mã transformer AR và mất mát bộ giải mã transformer NAR. Trong bộ giải mã transformer AR, chúng tôi không chọn rõ ràng một phát ngôn làm gợi ý âm sắc trong huấn luyện, có nghĩa là tất cả token âm thanh của lớp đầu tiên được dự đoán với kỹ thuật teacher-forcing. Đối với bộ giải mã transformer NAR, trong mỗi bước huấn luyện, chúng tôi ngẫu nhiên lấy mẫu một giai đoạn huấn luyện i∈[2,8] và ngẫu nhiên chọn một độ dài nhất định của tiền tố giọng nói mục tiêu làm gợi ý âm sắc. Mô hình được huấn luyện để tối đa hóa xác suất của token âm thanh trong lớp thứ i.
Trong giai đoạn suy luận, chúng tôi thiết kế các gợi ý âm thanh và suy luận như sau. Để tạo ra nội dung đã cho cho những người nói chưa thấy, mô hình được cung cấp một câu văn bản, bất kỳ số lượng giọng nói nào từ người nói mục tiêu làm gợi ý phong cách, một đoạn ngắn giọng nói từ người nói mục tiêu làm gợi ý âm sắc và bản phiên mã tương ứng của nó. Chúng tôi thêm bản phiên mã của gợi ý âm sắc vào trước câu văn bản đã cho làm gợi ý văn bản. Với gợi ý văn bản, gợi ý phong cách và gợi ý âm sắc, phương pháp đề xuất của chúng tôi tạo ra token âm thanh cho văn bản đã cho sao chép đặc điểm người nói của người nói mục tiêu.
3. THÍ NGHIỆM
3.1. Thiết lập Huấn luyện
Tất cả các mô hình được huấn luyện trên LibriTTS [25], là một bộ dữ liệu giọng nói tiếng Anh đa người nói có phiên mã mã nguồn mở. Tập huấn luyện của nó chứa khoảng 580 giờ ghi âm được nói bởi 2.306 người nói. Để đánh giá khả năng thích ứng không cần huấn luyện cho những người nói chưa thấy, 128 người nói từ hai tập con của bộ dữ liệu LibriTTS (test-clean và dev-clean) được chọn làm tập thử nghiệm, tổng cộng 8.078 phát ngôn. Một mô hình codec âm thanh nơ-ron được huấn luyện trước, EnCodec2[18], được sử dụng làm mô hình codec để mã hóa dạng sóng thô với tần số lấy mẫu 24kHz và tái cấu trúc dạng sóng dựa trên token âm thanh được dự đoán.
Trong triển khai của chúng tôi, bộ mã hóa âm vị, bộ giải mã transformer AR và bộ giải mã transformer NAR đều bao gồm 6 lớp khối transformer. So với hai mô-đun trong V ALL-E gốc mà cả hai đều bao gồm 12 lớp khối transformer, mô hình đề xuất của chúng tôi có ít tham số hơn. Chúng tôi huấn luyện tất cả các mô hình trong 300K lần lặp trên 4 GPU NVIDIA A100, với kích thước batch là 8 mẫu trên mỗi GPU. Bộ tối ưu hóa Adam được áp dụng với β1=0,9,β2= 0,98 và tuân theo cùng lịch trình tốc độ học tập trong [16].
3.2. Các Phương pháp So sánh
Để chứng minh hiệu suất của phương pháp đề xuất của chúng tôi, chúng tôi so sánh năm mô hình sau đây cho tổng hợp TTS không cần huấn luyện. Các mô hình này cũng được triển khai dựa trên V ALL-E3.
VALL-E Một triển khai mã nguồn mở3 của V ALL-E [16], chỉ xem xét gợi ý âm sắc 3 giây ở mức khung hình.
Proposed Mô hình đề xuất, xem xét cả gợi ý âm sắc 3 giây và gợi ý phong cách bao gồm mười câu.
Proposed-3s Để đảm bảo so sánh công bằng, chúng tôi xây dựng mô hình cơ sở này, chia sẻ cùng cấu trúc và tham số với mô hình đề xuất, nhưng chỉ sử dụng giọng nói 3 giây làm cả gợi ý âm sắc và gợi ý phong cách.
Base-S Mô hình cơ sở chỉ gợi ý phong cách, chia sẻ cùng backbone TTS và gợi ý phong cách với mô hình đề xuất, nhưng loại trừ gợi ý âm sắc.
Base-T Mô hình cơ sở chỉ gợi ý âm sắc, trong đó gợi ý phong cách bị loại bỏ. Nghĩa là, mô hình này chỉ sử dụng giọng nói 3 giây làm gợi ý âm sắc.
Đối với mỗi mẫu tổng hợp, chúng tôi ngẫu nhiên chọn các phát ngôn khác của cùng người nói làm gợi ý âm sắc và gợi ý phong cách.
3.3. Đánh giá Chủ quan
Chúng tôi tiến hành hai bài kiểm tra điểm ý kiến trung bình (MOS) để đo lường khả năng không cần huấn luyện của các mô hình khác nhau: 1) MOS Tự nhiên (N-MOS): đánh giá tính tự nhiên và ngữ điệu của giọng nói được tổng hợp; 2) MOS Tương tự (S-MOS): đánh giá độ tương tự người nói giữa giọng nói được tổng hợp và giọng nói thật. Chúng tôi ngẫu nhiên chọn 20 mẫu từ những người nói khác nhau trong tập thử nghiệm để đánh giá chủ quan. Để loại trừ các yếu tố nhiễu khác, chúng tôi giữ nội dung văn bản và giọng nói gợi ý nhất quán giữa các mô hình khác nhau. Một nhóm 25 người nghe được tuyển dụng để đánh giá các giọng nói đã cho trên thang điểm từ 1 đến 5 với khoảng cách 1 điểm.
Như được hiển thị trong Bảng 1, phương pháp đề xuất của chúng tôi đạt được N-MOS tốt nhất là 3,886 và S-MOS là 3,870, vượt trội hơn V ALL-E rất nhiều ở cả hai khía cạnh. So với mô hình V ALL-E và Base-T, cả hai đều chỉ sử dụng 3 giây giọng nói làm gợi ý âm thanh, mô hình Proposed-3s đạt được hiệu suất tốt hơn, đặc biệt là về tính tự nhiên với khoảng cách hơn 0,12. Điều này chứng minh rằng việc xem xét cùng gợi ý âm thanh ở mức âm vị thực sự hữu ích cho việc học phong cách nói cá nhân của người nói mục tiêu và cải thiện tính tự nhiên và độ tương tự người nói của giọng nói được tổng hợp mà không cần giới thiệu đầu vào bổ sung. Mô hình đề xuất của chúng tôi cũng cải thiện thêm so với mô hình Proposed-3s bằng cách tăng số lượng câu trong gợi ý phong cách, cho thấy khả năng nâng cao chất lượng TTS không cần huấn luyện bằng cách sử dụng nhiều giọng nói tham chiếu hơn từ người nói mục tiêu. Khả năng này không có sẵn trong các mô hình TTS dựa trên mô hình ngôn ngữ trước đây, và nó cho phép phương pháp đề xuất của chúng tôi có giới hạn trên hiệu suất cao hơn so với V ALL-E. Mô hình đề xuất của chúng tôi cũng đạt được hiệu suất vượt trội hơn không chỉ Base-S chỉ xem xét gợi ý phong cách, mà còn Base-T chỉ xem xét gợi ý âm sắc một mình. Điều này chứng minh rằng việc mô hình hóa đặc điểm người nói từ các tỉ lệ khác nhau có thể cải thiện tính tự nhiên và độ tương tự người nói của giọng nói được tổng hợp. Ngoài ra, quan sát thấy rằng mặc dù Base-S cũng sử dụng mười câu làm gợi ý phong cách, nó đạt được điểm thấp nhất trong cả hai đánh giá. Một lý do có thể là việc loại bỏ gợi ý âm sắc làm tiền tố trong bộ giải mã ảnh hưởng đến tính ổn định của việc giải mã, dẫn đến một số giọng nói được tổng hợp có độ rõ ràng kém. Hơn nữa, chúng tôi đã thêm một so sánh với YourTTS [12], thông qua bài kiểm tra sở thích ABX. Mô hình đề xuất của chúng tôi cho thấy tỷ lệ sở thích 57,3% so với 33,6% của YourTTS, chứng minh tính hiệu quả của nó.
3.4. Đánh giá Khách quan
Để đo lường tính tự nhiên và độ tương tự người nói của giọng nói được tổng hợp một cách khách quan, chúng tôi tính toán méo cepstrum mel (MCD) và Độ tương tự cosine Bộ mã hóa Người nói (SECS) làm các chỉ số theo [12, 13]. Vì độ dài của giọng nói được dự đoán và giọng nói thật có thể khác nhau, đầu tiên chúng tôi áp dụng dynamic time warping (DTW) để có được mối quan hệ căn chỉnh giữa hai phổ mel. Sau đó, chúng tôi tính toán MCD tối thiểu bằng cách căn chỉnh hai phổ mel. Đối với độ tương tự người nói, chúng tôi sử dụng bộ mã hóa người nói của gói Resemblyzer4 để tính toán SECS giữa giọng nói thật và giọng nói được tổng hợp. Giá trị nằm trong khoảng từ 0 đến 1, trong đó giá trị lớn cho thấy độ tương tự cao hơn.
Kết quả đánh giá của các mô hình khác nhau trên tập thử nghiệm được hiển thị trong Bảng 1. Quan sát thấy rằng mô hình đề xuất của chúng tôi vượt trội hơn các phương pháp cơ sở trong tất cả các chỉ số đánh giá khách quan. Kết quả cho thấy mô hình đề xuất của chúng tôi có thể cải thiện chất lượng và độ tương tự người nói của giọng nói được tổng hợp.
3.5. Nghiên cứu
Để nghiên cứu tác động của gợi ý với các độ dài khác nhau, chúng tôi điều chỉnh độ dài của gợi ý âm thanh và gợi ý phong cách cho V ALL-E và mô hình đề xuất, tương ứng. Đối với V ALL-E, bị ràng buộc bởi cấu trúc của mô hình ngôn ngữ chỉ giải mã, chúng tôi ngẫu nhiên chọn hai phát ngôn 3s/6s làm gợi ý cho mỗi người nói. Chúng tôi cũng đánh giá mô hình đề xuất của chúng tôi với số lượng giọng nói khác nhau làm gợi ý phong cách, bao gồm 1 câu, 5 câu, 10 câu và 20 câu. Gợi ý âm sắc được cố định thành giọng nói 3 giây như đã đề cập ở trên. Đặc biệt, khi gợi ý phong cách chỉ bao gồm một câu, mô hình đề xuất chỉ sử dụng giọng nói 3 giây làm cả gợi ý âm sắc và gợi ý phong cách. Chúng tôi đánh giá các mô hình này với hai chỉ số khách quan như đã mô tả trước đó.
Bảng 2 cho thấy so sánh hiệu suất giữa các độ dài khác nhau của gợi ý âm thanh. Quan sát thấy rằng V ALL-E với gợi ý âm thanh 6 giây giọng nói có được kết quả SECS gần với phương pháp đề xuất chỉ với một câu làm gợi ý phong cách, nhưng có khoảng cách đáng kể với đề xuất trong MCD. Điều này chứng minh rằng việc mô hình hóa phong cách nói cá nhân của người nói mục tiêu ở mức âm vị giúp tạo ra giọng nói gần với giọng nói thật. Bằng cách so sánh các độ dài khác nhau của gợi ý phong cách, chúng ta có thể thấy mô hình đề xuất của chúng tôi có thể tạo ra giọng nói tương tự hơn khi số lượng câu trong gợi ý phong cách tăng lên.
4. KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất một mô hình TTS không cần huấn luyện dựa trên mô hình ngôn ngữ để sử dụng các gợi ý âm thanh đa tỉ lệ để nắm bắt cả âm sắc và phong cách nói cá nhân của người nói mục tiêu. Một bộ mã hóa văn bản nhận biết người nói được sử dụng để mô hình hóa phong cách nói ở mức âm vị từ gợi ý phong cách có độ dài tùy ý. Một bộ giải mã âm thanh dựa trên mô hình ngôn ngữ được sử dụng để bảo tồn âm sắc được chỉ định bằng cách xem xét gợi ý âm sắc ở mức khung hình. Kết quả thí nghiệm chứng minh rằng cách tiếp cận đề xuất của chúng tôi có thể cải thiện đáng kể tính tự nhiên và độ tương tự người nói của giọng nói được tổng hợp, ngay cả khi chỉ sử dụng giọng nói 3 giây làm cả gợi ý phong cách và gợi ý âm sắc. Ngoài ra, mô hình đề xuất của chúng tôi có thể nâng cao chất lượng TTS không cần huấn luyện bằng cách tăng số lượng câu trong gợi ý phong cách, khi có nhiều câu của người nói mục tiêu có sẵn trong quá trình suy luận.
Lời cảm ơn : Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (62076144), Quỹ Khoa học Xã hội Quốc gia Trung Quốc (13&ZD189), Chương trình Khoa học và Công nghệ Thâm Quyến (WDZC20220816140515001, JCYJ202208181010140 30) và Phòng thí nghiệm Trọng điểm Thâm Quyến về công nghệ sáng tạo truyền thông tương tác thế hệ tiếp theo (ZDSYS20210623092001004).

--- TRANG 2 ---
Hình 1 . Kiến trúc tổng thể của mô hình đề xuất
lows scaling out to an arbitrary length of style prompt to describe
detailed speaker characteristics. Both subjective and objective eval-
uations show that our proposed method outperforms state-of-the-art
language model-based zero-shot TTS model [16] and other baselines
in terms of naturalness and speaker similarity. The performance is
also improved with an increasing number of sentences used in the
style prompt during inference.
2. METHODOLOGY
The architecture of our proposed model is illustrated in Fig.1. It
consists of two major parts: a speaker-aware text encoder and an
acoustic decoder based on V ALL-E. In this paper, we follow Nat-
uralspeech 2 [23] and V ALL-E [16] to leverage neural audio codec
models to represent style prompt and timbre prompt in continuous
acoustic embeddings and discrete acoustic tokens, respectively. The
speaker-aware text encoder is used to extract phoneme-level personal
speaking style information from the style prompt and fuse it into en-
coded phoneme embeddings by a reference attention module to ob-
tain speaker-aware text embeddings. Then the outputs of the encoder
are fed into the acoustic decoder along with the acoustic tokens of
the timbre prompt to generate speech with the same timbre as the
timbre prompt. The details of each component are as follows.
2.1. Speaker-aware Text Encoder
The speaker-aware text encoder is specifically designed to ex-
tract and model personal speaking style at phoneme-level from
an arbitrary-length style prompt and fuses the text-side content
information with the speech-side style information to obtain the
speaker-aware text embeddings. The architecture of the encoder is
illustrated in Fig.2, which comprises a phoneme encoder, an acoustic
encoder and a reference attention module.
On the text side, to derive better text representation as decoder
input, we introduce a phoneme encoder to encode the phoneme se-
quence. We use the ransformer block, which is a stack of self-
attention layer and 1D-convolution as in Fastspeech 2 [2], as the
basic structure for the encoder. The input texts are converted into
a sequence of phonemes by the grapheme-to-phoneme module and
then passed to the phoneme encoder to obtain phoneme embeddings.
On the speech side, to make use of arbitrary-length speech
prompts, previous approaches attempt to encode the speaker char-
acteristics into a global-level vector [11, 12]. As a result, the local
Fig. 2 . Cấu trúc của bộ mã hóa văn bản nhận biết người nói
fine-grained variations in speaking style are ignored. Different from
this way, we use an acoustic encoder to derive the local speaking
style embeddings from the style prompt instead of a single vector.
All the utterances of the target speaker are firstly concatenated to
form the style prompt, and then passed to a well-trained neural
audio codec to convert speech waveform into continuous acoustic
embeddings instead of discrete tokens to preserve as much personal
style information as possible in the speech. Then converted acoustic
embeddings are then passed to the acoustic encoder, which is made
up of a stack of 8 1D-convolution layers. In addition, in order to reg-
ulate the temporal granularity of the extracted style representations
closer to human vocal perception, the filter strides of convolution
layers are set as [2,1,2,1,2,1,2,1] for 16 times downsampling (about
0.2s). After that, the temporal granularity of the style embeddings is
properly reformed to a quasi-phoneme level inspired by [24].
To make better use of style embeddings extracted from the
style prompt, a reference attention module is introduced to obtain
the appropriate phoneme-level semantic-related personal speaking
style. We adopt scaled dot-product attention as the reference atten-
tion module. The phoneme embeddings are regarded as the query,
while all the style embeddings extracted from the style prompt are
regarded as both the key and the value. The relevance between them
is used to guide the selection of the personal speaking style for each
input phoneme. Finally, the reference attention module outputs an
aligned sequence with the same length as the phoneme embeddings
and adds it to the phoneme embeddings to form the speaker-aware
text embeddings.
2.2. Acoustic Decoder
In order to model speaker characteristics of a person, it is necessary
to clone the timbre in addition to mimic the speaker's speaking style.
Inspired by the success of language models in zero-shot TTS, our
proposed method adopts a modified V ALL-E [16] as the acoustic
decoder to generate speech with the same timbre as the 3-second
timbre prompt. As illustrated in Fig.3, the decoder is made up of
an acoustic embedding, an autoregressive (AR) transformer decoder
and a non-autoregressive (NAR) transformer decoder.
The timbre prompt is first passed to a well-trained neural audio
codec, and the output of the residual vector quantizer in the codec is
considered as discrete prompt acoustic tokens. These tokens consist
of 8 layers which are then embedded through eight separate acous-
tic embedding layers. The AR transformer decoder is utilized to
generate the first layer of acoustic tokens required to synthesis per-

--- TRANG 3 ---
Fig. 3 . Cấu trúc của bộ giải mã âm thanh
sonalized speech conditioned on the speaker-aware text embeddings.
Meanwhile, the first layer of acoustic tokens of the timbre prompt is
used as the prefix in AR decoding. The NAR transformer decoder
is then used to generate acoustic tokens of the other seven layers
in sequence. To predict the acoustic tokens of the i-th layer, the
transformer input is the concatenation of the speaker-aware text em-
beddings, the summation of the embedded acoustic tokens of timbre
prompt from layer 1 to layer iand the summation of the embedded
predicted acoustic tokens from layer 1 to layer i−1. In the end,
the first layer of acoustic tokens predicted by the AR transformer de-
coder and the remaining layers of acoustic tokens predicted by the
NAR transformer decoder are concatenated to form the predicted
acoustic tokens.
2.3. Training Strategy and Inference Procedure
During the training stage, for each training sample, we randomly
select 5 to 10 reference utterances spoken by the same speaker as
the sample to form the style prompt. For different training epochs,
different style prompts are randomly selected for the same training
sample for data augmentation. Different from V ALL-E which trains
two models separately, our proposed method trains the whole end-
to-end system jointly with the cross-entropy loss. Training loss is a
linear combination of an AR transformer decoder loss and an NAR
transformer decoder loss. In the AR transformer decoder, we do
not explicitly select an utterance as the timbre prompt in training,
which means all acoustic tokens of the first layer are predicted with
the teacher-forcing technique. For the NAR transformer decoder, in
each training step, we randomly sample a training stage i∈[2,8]
and randomly select a certain length of target speech prefixes as the
timbre prompt. The model is trained to maximize the probability of
the acoustic tokens in the i-th layer.
During the inference stage, we design acoustic prompts and in-
ference as follows. To generate given content for unseen speakers,
the model is given a text sentence, any number of speeches from the
target speaker as the style prompt, a short segment of speech from
the target speaker as the timbre prompt and its corresponding tran-
scription. We prepend the transcription of the timbre prompt to the
given text sentence as the text prompt. With the text prompt, the style
prompt and the timbre prompt, our proposed method generates the
acoustic tokens for the given text cloning the speaker characteristics
of the target speaker.
3. EXPERIMENTS
3.1. Training Setup
All the models are trained on LibriTTS [25], which is an open-
source multi-speaker transcribed English speech dataset. Its training
set contains approximately 580 hours of recording spoken by 2,306
speakers. To evaluate the zero-shot adaptation capability for unseen
speakers, 128 speakers from two subsets of the LibriTTS dataset
(test-clean and dev-clean) are selected as the test set, resulting in
8,078 utterances in total. A pre-trained neural audio codec model,
EnCodec2[18], is utilized as the codec model to encode the raw
waveform with 24kHz sampling rate and reconstruct the waveform
based on the predicted acoustic tokens.
In our implementation, the phoneme encoder, AR transformer
decoder and NAR transformer decoder all consist of 6 layers of
transformer blocks. Compared to the two modules in the original
V ALL-E which both consist of 12 layers of transformer blocks, our
2Implemented based on: https://github.com/facebookresearch/encodec
proposed model has less parameters. We train all the models for
300K iterations on 4 NVIDIA A100 GPUs, with a batch size of 8
samples on each GPU. The Adam optimizer is adopted with β1=
0.9,β2= 0.98and follow the same learning rate schedule in [16].
3.2. Compared Methods
To demonstrate the performance of our proposed method, we com-
pare the following five models for zero-shot TTS synthesis. These
models are also implemented based on V ALL-E3.
VALL-E An open-source implementation3of V ALL-E [16],
which considers only a 3-second timbre prompt at the frame-level.
Proposed The proposed model, which considers both a 3-
second timbre prompt and a style prompt consisting of ten sentences.
Proposed-3s To ensure a fair comparison, we build this baseline
model, which shares the same structure and parameters as the pro-
posed model, but only uses a 3-second speech as both timbre prompt
and style prompt.
Base-S The style prompt-only baseline model, which shares the
same TTS backbone and style prompt as the proposed model, but
excludes the timbre prompt.
Base-T The timbre prompt-only baseline model, where the style
prompt is removed. That is, this model used only a 3-second speech
as the timbre prompt.
For each sample synthesis, we randomly choose other utterances
of the same speaker as the timbre prompt and the style prompt.
3.3. Subjective Evaluation
We conduct two mean opinion score (MOS) tests to measure the
zero-shot capability of different models: 1) Naturalness MOS (N-
MOS): evaluate the naturalness and prosody of the synthesized
speech; 2) Similarity MOS (S-MOS): evaluate the speaker similarity
between the synthesized speech and the ground-truth speech. We
randomly choose 20 samples from different speakers in the test set
for subjective evaluation. To exclude other interference factors, we
keep the text content and prompt speech consistent among different
models. A group of 25 listening subjects are recruited to rate the
given speeches on a scale from 1 to 5 with 1 point interval.
As shown in Table 1, our proposed method achieves the best
N-MOS of 3.886and S-MOS of 3.870, which outperforms V ALL-
E greatly in both two aspects. Compared with the V ALL-E and
the Base-T model, which both used only 3 seconds of speech as
the acoustic prompt, the Proposed-3s model achieves better perfor-
mance, especially in naturalness by a gap of over 0.12. This demon-
strates that considering the same acoustic prompt at the phoneme-
level is really helpful for learning the personal speaking style of the
3Implemented based on: https://github.com/lifeiteng/vall-e

--- TRANG 4 ---
Bảng 1 . So sánh khách quan và chủ quan cho tổng hợp giọng nói từ văn bản không cần huấn luyện. Chúng tôi đánh giá tính tự nhiên và độ tương tự người nói của các mô hình khác nhau với khoảng tin cậy 95%.
Mô hình Chủ quan Khách quan
N-MOS (↑) S-MOS (↑) SECS (↑)MCD (↓)
Ground Truth 4.23±0.066 - - -
V ALL-E 3.48±0.059 3 .532±0.060 0.771 8 .075
Base-S 3.456±0.055 3 .500±0.056 0.727 7 .792
Base-T 3.654±0.062 3 .646±0.060 0.764 8 .047
Proposed 3.886±0.063 3 .870±0.062 0.798 7 .715
Proposed-3s 3.778±0.063 3 .692±0.062 0.779 7 .765
Bảng 2 . Kết quả đánh giá khách quan của V ALL-E và phương pháp đề xuất khi sử dụng độ dài gợi ý khác nhau trong suy luận. Thời lượng trung bình của các câu khoảng 6 giây.
Mô hình SECS (↑)MCD (↓)
V ALL-E w/ 3s 0.771 8 .075
V ALL-E w/ 6s 0.774 8 .177
Proposed w/ 1 sent (3s) 0.779 7 .765
Proposed w/ 5 sent (30s) 0.795 7 .743
Proposed w/ 10 sent (1min) 0.798 7 .715
Proposed w/ 20 sent (2min) 0.798 7 .702
target speaker and improving the naturalness and speaker similar-
ity of the synthesized speech without introducing additional inputs.
Our proposed model also makes further improvement compared with
the Proposed-3s model by increasing the number of sentences in
the style prompt, indicating the ability of enhancing the quality of
zero-shot TTS by utilizing more reference speeches from the target
speaker . This ability is not available in previous language model-
based TTS models, and it allows our proposed method to have a
higher performance upper bound compared to V ALL-E. Our pro-
posed model also achieves superior performance than not only Base-
S that just considers the style prompt, but also Base-T that solitar-
ily considers the timbre prompt. It demonstrates that modeling the
speaker characteristics from different scales can improve the natural-
ness and speaker similarity of the synthesized speech. In addition,
it is observed that although Base-S also uses ten sentences as the
style prompt, it achieved the lowest score in both two evaluations.
A possible reason is that removing the timbre prompt as the pre-
fix in the decoder affects the stability of the decoding, resulting in
some synthesized speeches are poor in intelligibility. Moreover, we
added a comparison with YourTTS [12], through the ABX prefer-
ene test. Our proposed model showed a 57.3%preference rate over
YourTTS's 33.6%, demonstrating its effectiveness.
3.4. Objective Evaluation
To measure the naturalness and speaker similarity of synthesized
speech objectively, we calculate mel-cepstrum distortion (MCD) and
Speaker Encoder Cosine Similarity (SECS) as the metrics following
[12, 13]. Since the lengths of the predicted and ground-truth speech
may be different, we first apply dynamic time warping (DTW) to de-
rive the alignment relationships between the two mel-spectrograms.
Then, we compute the minimum MCD by aligning the two mel-
spectrograms. For the speaker similarity, we use the speaker en-
coder of the Resemblyzer package4to compute the SECS between
the ground-truth speech and synthesized speech. The value ranges
from 0 to 1, where a large value indicates a higher similarity.
The evaluation results of different models on the test set are
shown in Table 1. It is observed that our proposed model out-
performs the baselines in all objective evaluation metrics. The
results indicate that our proposed model can improve the quality and
speaker similarity of synthesized speech.
3.5. Investigation
To investigate the impact of the prompt with different lengths, we
adjust the length of the acoustic prompt and style prompt for V ALL-
E and the proposed model, respectively. For V ALL-E, constrained
by the structure of the decoder-only language model, we randomly
select two utterances of 3s/6s as the prompts for each speaker. We
also evaluate our proposed model with various numbers of speech as
the style prompt, including 1 sentence, 5 sentences, 10 sentences and
20 sentences. The timbre prompt is fixed to 3-second speech as men-
tioned above. In particular, when the style prompt consists of only
one sentence, the proposed model only uses a 3-second speech as
both the timbre prompt and style prompt. We evaluate these models
with the two objective metrics as described before.
Table 2 shows the performance comparison among the different
lengths of the acoustic prompt. It is observed that the V ALL-E with
an acoustic prompt of 6 seconds speech gets the SECS result close
to the proposed method with only one sentence as style prompt, but
there is a significant gap with the proposed in MCD. It demonstrates
that modeling the personal speaking style of the target speaker at
phoneme-level helps generate speech that is close to the ground-
truth. By comparing different lengths of the style prompt, we can see
our proposed model is able to generate more similar speech when the
number of sentences in style prompt increases.
4. CONCLUSIONS
In this paper, we propose a language model-based zero-shot TTS
model to utilize multi-scale acoustic prompts to capture both
the timbre and personal speaking style of the target speaker. A
speaker-aware text encoder is utilized to model the speaking style
at phoneme-level from arbitrary-length style prompt. A language
model-based acoustic decoder is used to preserve a specified tim-
bre by considering the timbre prompt at frame-level. Experimental
results demonstrate that our proposed approach could significantly
improve the naturalness and speaker similarity of the synthesized
speech, even when only using 3-second speech as both style prompt
and timbre prompt. In addition, our proposed model can enhance the
quality of zero-shot TTS by increasing the number of sentences in
style prompt, when there are multiple sentences of the target speaker
are available during inference.
Acknowledgement : This work is supported by National Natural
Science Foundation of China (62076144), National Social Science
Foundation of China (13&ZD189), Shenzhen Science and Technol-
ogy Program (WDZC20220816140515001, JCYJ202208181010140
30) and Shenzhen Key Laboratory of next generation interactive me-
dia innovative technology (ZDSYS20210623092001004).

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO
[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, Rj Skerrv-Ryan, et al., "Natural TTS synthe-
sis by conditioning waveNet on mel spectrogram predictions,"
trong International Conference on Acoustics, Speech and Signal
Processing (ICASSP) . IEEE, 2018, pp. 4779–4783.
[2] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao,
and Tie-Yan Liu, "Fastspeech 2: Fast and high-quality end-to-
end text to speech," trong International Conference on Learning
Representations , 2020.
[3] Mingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun, Sheng Zhao,
and Tao Qin, "MultiSpeech: Multi-Speaker Text to Speech
with Transformer," trong Proc. Interspeech 2020 , 2020, pp. 4024–
4028.
[4] Jaehyeon Kim, Jungil Kong, and Juhee Son, "Conditional
variational autoencoder with adversarial learning for end-to-
end text-to-speech," trong International Conference on Machine
Learning . PMLR, 2021, pp. 5530–5540.
[5] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu, "A survey
on neural speech synthesis," arXiv preprint arXiv:2106.15561 ,
2021.
[6] Yutian Chen, Yannis Assael, Brendan Shillingford, David Bud-
den, Scott Reed, Heiga Zen, Quan Wang, Luis C Cobo, An-
drew Trask, Ben Laurie, et al., "Sample efficient adaptive text-
to-speech," trong International Conference on Learning Repre-
sentations , 2018.
[7] Zvi Kons, Slava Shechtman, Alex Sorin, Carmel Rabinovitz,
and Ron Hoory, "High quality, lightweight and adaptable TTS
using LPCNet," trong Proc. Interspeech 2019 , 2019, pp. 176–180.
[8] Henry B Moss, Vatsal Aggarwal, Nishant Prateek, Javier
Gonz ´alez, and Roberto Barra-Chicote, "BOFFIN TTS: Few-
shot speaker adaptation by bayesian optimization," trong Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2020, pp. 7639–7643.
[9] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Tie-
Yan Liu, et al., "Adaspeech: Adaptive text to speech for custom
voice," trong International Conference on Learning Representa-
tions , 2020.
[10] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen,
Fei Ren, zhifeng Chen, Patrick Nguyen, Ruoming Pang, Igna-
cio Lopez Moreno, and Yonghui Wu, "Transfer learning from
speaker verification to multispeaker text-to-speech synthesis,"
trong Advances in Neural Information Processing Systems , 2018,
vol. 31.
[11] Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin
Wang, Nanxin Chen, and Junichi Yamagishi, "Zero-shot multi-
speaker text-to-speech with state-of-the-art neural speaker em-
beddings," trong International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2020, pp. 6184–6188.
[12] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-
naldo Candido Junior, Eren G ¨olge, and Moacir A Ponti,
"YourTTS: Towards zero-shot multi-speaker tts and zero-shot
voice conversion for everyone," trong International Conference
on Machine Learning . PMLR, 2022, pp. 2709–2720.
[13] Seungwoo Choi, Seungju Han, Dongyoung Kim, and Sungjoo
Ha, "Attentron: Few-Shot Text-to-Speech Utilizing Attention-
Based Variable-Length Embedding," trong Proc. Interspeech
2020 , 2020, pp. 2007–2011.[14] Yixuan Zhou, Changhe Song, Xiang Li, Luwen Zhang, Zhiy-
ong Wu, Yanyao Bian, Dan Su, and Helen Meng, "Content-
Dependent Fine-Grained Speaker Embedding for Zero-Shot
Speaker Adaptation in Text-to-Speech Synthesis," trong Proc. In-
terspeech 2022 , 2022, pp. 2573–2577.
[15] Zal ´an Borsos, Rapha ¨el Marinier, Damien Vincent, Eugene
Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek,
Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil
Zeghidour, "AudioLM: A language modeling approach to au-
dio generation," IEEE/ACM Transactions on Audio, Speech,
and Language Processing , vol. 31, pp. 2523–2533, 2023.
[16] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long
Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang,
Jinyu Li, et al., "Neural codec language models are zero-shot
text to speech synthesizers," arXiv preprint arXiv:2301.02111 ,
2023.
[17] Eugene Kharitonov, Damien Vincent, Zal ´an Borsos, Rapha ¨el
Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco
Tagliasacchi, and Neil Zeghidour, "Speak, read and prompt:
High-fidelity text-to-speech with minimal supervision," arXiv
preprint arXiv:2302.03540 , 2023.
[18] Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and Yossi
Adi, "High fidelity neural audio compression," arXiv preprint
arXiv:2210.13438 , 2022.
[19] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan
Skoglund, and Marco Tagliasacchi, "SoundStream: An end-to-
end neural audio codec," IEEE/ACM Transactions on Audio,
Speech, and Language Processing , vol. 30, pp. 495–507, 2021.
[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al., "Lan-
guage models are few-shot learners," Advances in neural in-
formation processing systems , vol. 33, pp. 1877–1901, 2020.
[21] Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Xixin Wu,
Shiyin Kang, and Helen Meng, "MSStyleTTS: Multi-scale
style modeling with hierarchical context information for ex-
pressive speech synthesis," IEEE/ACM Transactions on Audio,
Speech, and Language Processing , vol. 31, pp. 3290–3303,
2023.
[22] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang,
Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang,
Xiang Yin, et al., "Mega-TTS: Zero-shot text-to-speech
at scale with intrinsic inductive bias," arXiv preprint
arXiv:2306.03509 , 2023.
[23] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei
He, Tao Qin, Sheng Zhao, and Jiang Bian, "NaturalSpeech 2:
Latent diffusion models are natural and zero-shot speech and
singing synthesizers," arXiv preprint arXiv:2304.09116 , 2023.
[24] Xiang Li, Changhe Song, Jingbei Li, Zhiyong Wu, Jia Jia, and
Helen Meng, "Towards multi-scale style control for expressive
speech synthesis," trong Proc. Interspeech 2021 , 2021, pp. 4673–
4677.
[25] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss,
Ye Jia, Zhifeng Chen, and Yonghui Wu, "LibriTTS: A corpus
derived from libriSpeech for text-to-speech," trong Proc. Inter-
speech 2019 , 2019, pp. 1526–1530.
