# DPHuBERT: Chưng cất và Tỉa cành kết hợp cho các Mô hình Lời nói Tự giám sát
Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1
1Carnegie Mellon University, Pittsburgh, PA, USA
2Honda Research Institute Japan Co., Ltd., Saitama, Japan
yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad }@jp.honda-ri.com, shinjiw@ieee.org

## Tóm tắt
Học tự giám sát (SSL) đã đạt được thành công đáng chú ý trong nhiều tác vụ xử lý lời nói, nhưng kích thước mô hình lớn và chi phí tính toán nặng cản trở việc triển khai. Chưng cất kiến thức huấn luyện một mô hình học sinh nhỏ để bắt chước hành vi của một mô hình giáo viên lớn. Tuy nhiên, kiến trúc học sinh thường cần được thiết kế thủ công và sẽ giữ nguyên trong quá trình huấn luyện, điều này đòi hỏi kiến thức trước và có thể dẫn đến hiệu suất không tối ưu. Lấy cảm hứng từ thành công gần đây của tỉa cành có cấu trúc theo tác vụ, chúng tôi đề xuất DPHuBERT, một phương pháp nén độc lập tác vụ mới cho SSL lời nói dựa trên chưng cất và tỉa cành kết hợp. Các thí nghiệm trên SUPERB cho thấy DPHuBERT vượt trội hơn các phương pháp chưng cất thuần túy trong hầu hết các tác vụ. Hơn nữa, DPHuBERT cần ít thời gian huấn luyện và hoạt động tốt với dữ liệu huấn luyện hạn chế, khiến nó phù hợp cho các ứng dụng hạn chế tài nguyên. Phương pháp của chúng tôi cũng có thể được áp dụng cho các mô hình SSL lời nói khác nhau. Mã và mô hình của chúng tôi sẽ được công khai.

Từ khóa: nén mô hình, chưng cất kiến thức, tỉa cành có cấu trúc, học tự giám sát

## 1. Giới thiệu
Học biểu diễn lời nói tự giám sát (SSL lời nói) đã đạt được kết quả đáng kể trong các tác vụ khác nhau [1–10]. Tuy nhiên, các mô hình SSL lời nói thường lớn và chậm, khiến chúng không phù hợp cho các ứng dụng thực tế với tài nguyên hạn chế. Nén SSL lời nói đã trở thành một chủ đề quan trọng. Một phương pháp phổ biến là chưng cất kiến thức [11], huấn luyện một mô hình học sinh nhỏ để khớp với đầu ra của một mô hình giáo viên lớn. Các nghiên cứu trước như DistilHuBERT [12] và FitHuBERT [13] đã đạt được kết quả hứa hẹn với các mô hình học sinh khác nhau. Một nghiên cứu khác [14] cho thấy kiến trúc học sinh ảnh hưởng đáng kể đến hiệu suất của nó, ngay cả khi kích thước mô hình tương tự. Tuy nhiên, trong các phương pháp chưng cất, kiến trúc học sinh được xác định trước và không thay đổi, điều này cần chuyên môn đặc biệt và có thể dẫn đến kết quả không tối ưu. Ngược lại, tỉa cành [15, 16] tự động khám phá một mạng con nhỏ gọn từ một mô hình lớn, đã được khám phá trong xử lý ngôn ngữ tự nhiên (NLP) [17–20] và xử lý lời nói [21–25]. Các phương pháp tỉa cành trước đây cho SSL lời nói tập trung vào các tác vụ hạ lưu cụ thể như nhận dạng lời nói tự động (ASR) [24,25] và hiểu ngôn ngữ nói (SLU) [25]. Không rõ chúng sẽ hoạt động như thế nào trong nén độc lập tác vụ, điều này khó khăn hơn vì mô hình cần nắm bắt các khía cạnh khác nhau của lời nói bao gồm nội dung, người nói, ngữ nghĩa và cận ngôn ngữ [6].

Trong công trình này, chúng tôi đề xuất DPHuBERT, một phương pháp nén độc lập tác vụ dựa trên Chưng cất và Tỉa cành kết hợp. Nó cho phép kiến trúc học sinh được học trong quá trình chưng cất. Các thí nghiệm trên SUPERB [6] cho thấy phương pháp của chúng tôi vượt trội hơn các phương pháp chưng cất trước đây trong hầu hết các tác vụ. Phương pháp của chúng tôi cũng hoạt động tốt cho các mô hình SSL lời nói khác nhau như HuBERT Base [2], WavLM Base+ [4] và HuBERT Large [2], ngay cả với tài nguyên huấn luyện hạn chế. Chúng tôi sẽ gửi kết quả của mình lên bảng xếp hạng SUPERB và công bố mã cũng như mô hình công khai để tái sản xuất: https://github.com/pyf98/DPHuBERT.

## 2. Bối cảnh và nghiên cứu liên quan

### 2.1. Kiến trúc của SSL lời nói
Các mô hình SSL lời nói như wav2vec 2.0 [1], HuBERT [2] và WavLM [4] chia sẻ kiến trúc tương tự. Mô hình bao gồm một bộ trích xuất đặc trưng tích chập (CNN) và một bộ mã hóa Transformer [26]. CNN có bảy tích chập theo thời gian với chuẩn hóa và kích hoạt. Bộ mã hóa Transformer chứa 12 lớp với kích thước ẩn 768 cho các mô hình cơ bản và 24 lớp với kích thước ẩn 1024 cho các mô hình lớn. Mỗi lớp được cấu thành từ một mạng tự chú ý đa đầu (MHA) và một mạng feed-forward theo vị trí (FFN).

### 2.2. Phương pháp nén cho SSL lời nói
**Chưng cất.** Các phương pháp chưng cất tối ưu hóa một mô hình học sinh nhỏ để khớp với các mục tiêu được tạo bởi một mô hình giáo viên lớn. Vì các lớp khác nhau của SSL lời nói nắm bắt thông tin khác nhau [27], mô hình học sinh cần học cả biểu diễn cuối cùng và trung gian của giáo viên [12–14]. DistilHuBERT [12] huấn luyện một mô hình học sinh nông bằng cách ánh xạ lớp học sinh cuối cùng đến nhiều lớp giáo viên trung gian. FitHuBERT [13] học một mô hình học sinh sâu và mỏng thông qua ánh xạ lớp-đến-lớp. Một nghiên cứu khác [14] so sánh chưng cất lớp dự đoán và lớp-đến-lớp sử dụng các kiến trúc học sinh khác nhau. Nó cho thấy kiến trúc của mô hình học sinh ảnh hưởng đến hiệu suất của nó, ngay cả khi kích thước mô hình được giữ tương tự. Nó cũng phát hiện rằng các mạng sâu hơn hoạt động tốt hơn với chưng cất lớp-đến-lớp, có thể vì nó căn chỉnh rõ ràng các lớp trung gian. Những quan sát này đã truyền cảm hứng cho công trình của chúng tôi, cho phép kiến trúc học sinh phát triển trong quá trình chưng cất.

**Tỉa cành.** Các phương pháp tỉa cành xác định và loại bỏ các tham số dư thừa từ một mô hình được huấn luyện trước. Tỉa cành không có cấu trúc loại bỏ các tham số riêng lẻ (ví dụ, một kết nối giữa các nơ-ron) bằng cách đặt chúng bằng không, điều này đòi hỏi các thư viện tính toán ma trận thưa để đạt được tăng tốc thực tế, trong khi tỉa cành có cấu trúc loại bỏ các nhóm tham số (ví dụ, một đầu chú ý hoặc thậm chí cả một lớp), điều này trực tiếp giảm kích thước mô hình và chi phí tính toán. Đối với SSL lời nói, PARP [24] là một phương pháp tỉa cành không có cấu trúc dựa trên độ lớn pruning bộ mã hóa Transformer. Nó cải thiện các tác vụ hạ lưu như ASR tài nguyên thấp. HJ-Pruning [25] là một phương pháp tỉa cành có cấu trúc prune kết hợp các thành phần không đồng nhất (tức là CNN và Transformer) của các mô hình SSL lời nói. Nó giảm đáng kể tổng tính toán trong khi duy trì hiệu suất tốt trong ASR và SLU. Các phương pháp này xử lý các tác vụ hạ lưu cụ thể, nhưng không điều tra các biểu diễn lời nói phổ quát. Công trình của chúng tôi tập trung vào tỉa cành có cấu trúc độc lập tác vụ của SSL lời nói. Vì không có dữ liệu được gán nhãn cho huấn luyện giám sát bình thường, chúng tôi sử dụng mục tiêu chưng cất cùng với tỉa cành.

**Huấn luyện một-cho-tất-cả.** Các phương pháp nén thường tạo ra một mô hình duy nhất với kích thước được xác định trước. LightHuBERT [28] triển khai huấn luyện một-cho-tất-cả [29] để có được nhiều mạng con chia sẻ trọng số, cho thấy hiệu suất rất mạnh trên SUPERB [6]. Tuy nhiên, nó đòi hỏi một quy trình huấn luyện hai giai đoạn đắt đỏ và một hàm mất mát chưng cất tiên tiến lấy cảm hứng từ data2vec [30]. Theo các tác giả, nén HuBERT Base đã mất 2k giờ GPU (tức là 62 giờ với 32 GPU V100 và 19 giờ với 8 GPU cho hai giai đoạn, tương ứng), điều này quá tốn kém đối với các nhà nghiên cứu học thuật và doanh nghiệp nhỏ. Khác với LightHuBERT, công trình của chúng tôi nhằm nén một mô hình SSL lời nói hiện có đến một tỷ lệ thưa cụ thể trong một lượng thời gian huấn luyện có thể quản lý, phù hợp với thiết lập tiêu chuẩn của các phương pháp chưng cất trước đây [12, 13].

## 3. DPHuBERT

### 3.1. Quy trình huấn luyện
Hình 1 minh họa quy trình huấn luyện của chúng tôi gồm hai bước. Trong Bước 1, mô hình học sinh được khởi tạo từ giáo viên và được chưng cất và tỉa cành kết hợp để tạo ra một mô hình nhỏ hơn với kích thước được xác định trước. Trong Bước 2, mô hình học sinh đã được tỉa cành được chưng cất thêm để cải thiện hiệu suất. Trong cả hai bước, chỉ dữ liệu lời nói không gán nhãn được sử dụng và giáo viên được đóng băng.

### 3.2. Hàm mất mát chưng cất
Khác với DistilHuBERT [12], chúng tôi sử dụng chưng cất lớp-đến-lớp vì học sinh ban đầu có cùng độ sâu với giáo viên (xem Phần 2.2 để thảo luận). Giả sử giáo viên có N_tea lớp Transformer với kích thước ẩn d_tea và học sinh có N_stu lớp với kích thước ẩn d_stu. Gọi X^tea_i (hình dạng T×d_tea) và X^stu_i (hình dạng T×d_stu) là các chuỗi đầu ra của lớp Transformer thứ i từ giáo viên và học sinh, tương ứng, trong đó T là độ dài chuỗi. Hàm mất mát chưng cất là:

L_dis = ∑_{i∈S} L(X^tea_i, X^stu_i W_i), (1)

trong đó S là một tập hợp các lớp để khớp giữa mô hình giáo viên và học sinh sau một phép chiếu tuyến tính W_i. Chúng tôi sử dụng S={0,4,8,12} cho các mô hình cơ bản và {0,8,16,24} cho các mô hình lớn. Lớp thứ 0 là đầu ra của CNN, cũng là đầu vào cho lớp Transformer đầu tiên. Hàm mất mát L đo sự khác biệt giữa hai chuỗi đặc trưng, có thể là khoảng cách L1, L2 hoặc cosine [12–14]. Chúng tôi theo [12,14] để sử dụng kết hợp khoảng cách L1 và cosine với trọng số bằng nhau.

### 3.3. Chưng cất và tỉa cành có cấu trúc kết hợp
Tỉa cành có cấu trúc của mô hình học sinh được công thức hóa như học một mô hình thưa thông qua điều chuẩn L0 [31], đã được khám phá trong NLP [19,20] và lời nói [25]. Phương pháp sẽ được giới thiệu ngắn gọn dưới đây. Để có các suy dẫn toàn diện hơn, vui lòng tham khảo nghiên cứu trước [19, 20, 25, 31]. Xem xét một mô hình giáo viên đóng băng f_tea(·) và một mô hình học sinh f_stu(·;θ) với các tham số có thể học θ={θ_j}^n_{j=1}. Mỗi θ_j là một nhóm các tham số có thể tỉa cành (bao gồm các kênh tích chập, đầu chú ý và đơn vị trung gian FFN) và có tổng cộng n nhóm. Chúng tôi định nghĩa một biến nhị phân z_j (được gọi là mặt nạ) cho mỗi θ_j. Các mặt nạ z tuân theo phân phối xác suất q(z;α) với tham số α. Mục tiêu chưng cất được điều chuẩn là:

min_{θ,α} E_{z∼q}[1/D ∑^D_{k=1} L_dis(f_tea(x_k), f_stu(x_k;θ̃)) + λ‖θ̃‖_0], (2)

trong đó θ̃={θ̃_j}^n_{j=1} và mỗi θ̃_j=θ_j z_j. Tập dữ liệu không gán nhãn với D mẫu là {x_k}^D_{k=1}. λ>0 là trọng số điều chuẩn. Việc giải Phương trình (2) bằng gradient descent là không khả thi do bản chất rời rạc của mặt nạ z. Để làm cho hàm mất mát có thể vi phân, Louizos et al. đề xuất một thủ thuật tái tham số hóa lấy mẫu z từ phân phối Hard Concrete [31]:

u ∼ U(0,1), v(α) = sigmoid((log u/(1-u) + log α)/β),
v̄(α) = (r-l)·v(α) + l, z = min(1, max(0, v̄(α))), (3)

trong đó u tuân theo phân phối đều trong [0,1]. β là một hằng số. l<0 và r>0 là hai hằng số để kéo dài v đến [l,r], và nó được cắt thêm đến [0,1]. Chỉ α={α_j}^n_{j=1} là các tham số có thể học trong phân phối này. Với thủ thuật này, mục tiêu trong Phương trình (2) có thể vi phân và số hạng điều chuẩn có biểu thức dạng đóng [31]:

E_{z∼q}[‖θ̃‖_0] = ∑^n_{j=1} sigmoid(log α_j - β log(-l/r)), (4)

đại diện cho kích thước mô hình (kỳ vọng) như một hàm có thể vi phân của các tham số hiện tại α.

Bây giờ Phương trình (2) có thể được giải để học một mạng con thưa, nhưng độ thưa cuối cùng không thể được kiểm soát chính xác [19, 20]. Để kiểm soát rõ ràng kích thước mô hình cuối cùng, các nghiên cứu trước [19, 20, 25] viết lại bài toán tối ưu với ràng buộc đẳng thức:

min_{θ,α} E_{z∼q}[1/D ∑^D_{k=1} L_dis(f_tea(x_k), f_stu(x_k;θ̃))]
s.t. s(α) = t, (5)

trong đó s(α) là độ thưa hiện tại (phần trăm tham số được tỉa cành) của mô hình học sinh và t là độ thưa mục tiêu được xác định trước. Lưu ý rằng s(α) có thể được tính toán dựa trên Phương trình (4) vì chuẩn L0 đếm các tham số còn lại. Mục tiêu tối ưu trong Phương trình (5) có thể được chuyển đổi thêm thành bài toán minimax sử dụng Lagrangian tăng cường [19]:

max_{λ1,λ2} min_{θ,α} E_{z∼q}[1/D ∑^D_{k=1} L_dis(f_tea(x_k), f_stu(x_k;θ̃))]
+ λ1·(s(α)-t) + λ2·(s(α)-t)^2, (6)

trong đó λ1, λ2 ∈ R là các nhân tử Lagrange. Số hạng bổ sung này phạt hàm mất mát chưng cất và buộc mô hình học sinh đạt độ thưa mục tiêu của chúng tôi. Phương trình (6) là mục tiêu huấn luyện của chúng tôi cho Bước 1 (Hình 1a). Đối với Bước 2 (Hình 1b), mục tiêu chỉ đơn giản là tối thiểu hóa hàm mất mát chưng cất trong Phương trình (1) mà không có ràng buộc nào vì kiến trúc học sinh đã được cố định.

## 4. Thí nghiệm

### 4.1. Thiết lập thí nghiệm
**Công cụ.** Phương pháp của chúng tôi được triển khai với PyTorch [32] và TorchAudio [33]. Các mô hình SSL được huấn luyện trước được tải xuống từ fairseq [34] hoặc Hugging Face [35].

**Dữ liệu.** LibriSpeech 960h không gán nhãn [36] được sử dụng mặc định. Trong Phần 4.2 và Bảng 1, tập con train-clean 100h cũng được sử dụng để điều tra ảnh hưởng của kích thước dữ liệu huấn luyện.

**Mô hình.** Trong thiết lập mặc định, chúng tôi nén HuBERT Base [2]. Để xác minh tính tổng quát, chúng tôi cũng nén WavLM Base+ [4] trong Phần 4.2 và HuBERT Large [2] trong Phần 4.5.

**Huấn luyện.** DPHuBERT được huấn luyện trên 4 GPU NVIDIA A100 (40GB) với 640 giây âm thanh mỗi mini-batch. Trong Bước 1, tốc độ học đỉnh của các tham số chính θ và các tham số phụ α,λ lần lượt là 2e-4 và 2e-2. Các bước khởi động và tổng số lần lượt là 15k và 50k. Độ thưa mục tiêu t được tăng tuyến tính đến giá trị mong muốn trong 5k bước, điều này tạo thuận lợi cho việc huấn luyện [25]. Trong Bước 2, tốc độ học đỉnh là 1e-4. Các bước khởi động và tổng số lần lượt là 5k và 25k. Trong thiết lập mặc định, tổng thời gian huấn luyện của DPHuBERT chỉ là 6 giờ, tức là 24 giờ GPU (do chúng tôi sử dụng 4 GPU).

**Đánh giá.** Benchmark SUPERB [6] bao gồm 10 tác vụ: phát hiện từ khóa (KS), phân loại ý định (IC), nhận dạng âm vị (PR), ASR, nhận dạng cảm xúc (ER), tìm kiếm bằng ví dụ (QbE), điền slot (SF), nhận dạng người nói (SID), xác minh người nói tự động (ASV) và phân đoạn người nói (SD). Chúng tôi tuân theo các cấu hình mặc định của họ trong tất cả các tác vụ ngoại trừ SID sử dụng tốc độ học 5e-3.

### 4.2. Kết quả chính
Bảng 1 so sánh các phương pháp khác nhau trên SUPERB [6]. DPHuBERT được nén từ HuBERT Base. Với LibriSpeech 960h, DPHuBERT vượt trội hơn các phương pháp chưng cất thuần túy (bao gồm DistilHuBERT [12], FitHuBERT [13], FitW2V2 [13] và hai mô hình hoạt động tốt nhất từ [14]) trong 8 trên 10 tác vụ. Điều này cho thấy DPHuBERT bảo tồn tốt hơn các biểu diễn lời nói tổng quát của mô hình giáo viên, bao gồm nội dung, người nói và ngữ nghĩa. Chỉ với dữ liệu huấn luyện 100h, DPHuBERT vẫn hoạt động tốt hơn nhiều so với các phương pháp trước đây trong hầu hết các tác vụ. Đáng ngạc nhiên, DPHuBERT sử dụng 100h đã vượt trội hơn các mô hình chưng cất trước đây sử dụng 960h trong IC, PR, QbE và SD, và có kết quả tương tự trong các tác vụ khác. Điều này cho thấy DPHuBERT học được các biểu diễn mạnh mẽ ngay cả từ dữ liệu hạn chế.

Chúng tôi cũng đã nén WavLM Base+ để có được DPWavLM. So với DPHuBERT, DPWavLM đạt được cải thiện thêm trong 8 tác vụ. Điều này là do WavLM Base+ chưa nén tốt hơn HuBERT Base chưa nén. Những kết quả này chứng minh rằng phương pháp nén của chúng tôi có thể được áp dụng cho các mô hình SSL lời nói khác nhau.

Hình 2 cho thấy kiến trúc của DPHuBERT, được tự động khám phá bởi tỉa cành có cấu trúc. Đối với CNN, lớp đầu tiên và cuối cùng được tỉa cành nhiều nhất. Đối với MHA, ba lớp cao hơn bị loại bỏ hoàn toàn, cho thấy những lớp đó dư thừa hơn. Kết quả của chúng tôi phù hợp với các nghiên cứu trước về tỉa cành [20, 25] hoặc bộ mã hóa lời nói tổng quát [37–40]. Đối với FFN, các lớp thứ 4, 8 và 12 được bảo tồn nhiều hơn các lớp láng giềng, vì những lớp đó được khớp rõ ràng giữa mô hình giáo viên và học sinh như được định nghĩa bởi Phương trình (1) trong Phần 3.2.

### 4.3. Nghiên cứu loại bỏ
Bảng 2 tóm tắt kết quả của các nghiên cứu loại bỏ sau đây.

**Huấn luyện hai bước.** DPHuBERT có hai bước huấn luyện (Phần 3.1 và Hình 1). Mô hình đã tỉa cành sau Bước 1 mà không có Bước 2 được đánh giá trong hàng thứ hai của Bảng 2. Nó kém hơn DPHuBERT trong tất cả các tác vụ, xác minh sự cần thiết của Bước 2. Điều này là do Bước 1 tối ưu hóa Phương trình (6) trong đó số hạng điều chuẩn cạnh tranh với hàm mất mát chưng cất để đạt độ thưa mục tiêu, trong khi Bước 2 trực tiếp tối ưu hóa hàm mất mát chưng cất để cải thiện các biểu diễn đã học của học sinh.

**Phương pháp chưng cất.** Như đã thảo luận trong Phần 2.2, DPHuBERT sử dụng chưng cất lớp-đến-lớp thay vì chưng cất lớp dự đoán trong DistilHuBERT [12]. Hàng thứ ba của Bảng 2 cho thấy chưng cất lớp dự đoán gây ra sự suy giảm nghiêm trọng trong tất cả các tác vụ, có thể do kiến trúc học sinh sâu. Khớp trực tiếp các lớp trung gian tạo thuận lợi cho việc huấn luyện học sinh sâu như được tìm thấy trong [14].

**Đơn vị tỉa cành.** DPHuBERT tỉa cành cả CNN và Transformer vì CNN có chi phí tính toán cao [25, 41]. Hàng thứ tư của Bảng 2 cho thấy kết quả mà không tỉa cành CNN (tức là chỉ tỉa cành đầu chú ý và kích thước trung gian FFN). Mô hình này kém hơn (một chút) thiết lập mặc định trong 7/10 tác vụ. Điều này xác minh rằng CNN cũng có các thành phần dư thừa có thể được tỉa cành, như được báo cáo trong [25, 41, 42].

### 4.4. Kết quả ở các độ thưa khác nhau
Chúng tôi huấn luyện DPHuBERT với các độ thưa mục tiêu khác nhau (t trong Phương trình (5)(6)) và cho thấy kết quả trong Hình 3. Đối với IC và SID, phương pháp của chúng tôi có thể giảm đáng kể kích thước mô hình trong khi giữ độ chính xác tương tự như HuBERT Base gốc. Đối với ASR, sự suy giảm nghiêm trọng hơn, có thể vì tác vụ chuyển đổi chuỗi khó khăn hơn các tác vụ phân loại.

### 4.5. Nén HuBERT Large
Phương pháp của chúng tôi có thể được áp dụng cho các mô hình SSL lời nói lớn hơn với chi phí huấn luyện rất hạn chế. Trong Bảng 3, HuBERT Large được nén để có kích thước tương tự như HuBERT Base, chỉ mất khoảng 60 giờ GPU. Mô hình nén thậm chí vượt trội hơn HuBERT Base trong một số tác vụ như PR, SF-CER và SID. Nó kém hơn HuBERT Base trong KS, QbE và ASV, nhưng mô hình giáo viên, HuBERT Large, cũng rõ ràng kém hơn HuBERT Base trong những tác vụ đó.

## 5. Kết luận
Công trình này đề xuất DPHuBERT, một phương pháp nén độc lập tác vụ dựa trên chưng cất và tỉa cành có cấu trúc kết hợp. DPHuBERT vượt trội hơn các phương pháp chưng cất trước đây trong hầu hết các tác vụ của SUPERB. Các phân tích toàn diện được trình bày để điều tra hiệu suất của nó với ít dữ liệu huấn luyện hơn hoặc ở các tỷ lệ thưa khác nhau. Ngoài HuBERT Base, phương pháp của chúng tôi có thể được áp dụng trực tiếp cho các mô hình SSL lời nói khác như WavLM và HuBERT Large trong khi vẫn hiệu quả và có hiệu suất. Trong tương lai, chúng tôi sẽ khám phá các mục tiêu chưng cất tinh vi hơn (ví dụ, hàm mất mát chưng cất dựa trên masking được sử dụng trong LightHuBERT [28]) để cải thiện hiệu suất hơn nữa.

## 6. Lời cảm ơn
Chúng tôi sử dụng PSC Bridges2 và NCSA Delta thông qua phân bổ ACCESS CIS210014, được hỗ trợ bởi các tài trợ của Quỹ Khoa học Quốc gia số #2138259, #2138286, #2138307, #2137603, và #2138296.

## 7. Tài liệu tham khảo
[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," in Proc. NeurIPS, 2020.
[2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units," IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021.
[3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale," in Proc. Interspeech, 2022.
[4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., "Wavlm: Large-scale self-supervised pre-training for full stack speech processing," IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505–1518, 2022.
[5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, "Unsupervised speech recognition," in Proc. NeurIPS, 2021.
[6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, "SUPERB: Speech Processing Universal PERformance Benchmark," in Proc. Interspeech, 2021.
[7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe, T. N. Sainath, and S. Watanabe, "Self-supervised speech representation learning: A review," IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1179–1210, 2022.
[8] X. Chang, T. Maekaku, P. Guo, J. Shi, Y.-J. Lu, A. S. Subramanian, T. Wang, S.-w. Yang, Y. Tsao, H.-y. Lee et al., "An exploration of self-supervised pretrained representations for end-to-end speech recognition," in Proc. ASRU, 2021.
[9] Z. Huang, S. Watanabe, S.-w. Yang, P. García, and S. Khudanpur, "Investigating Self-Supervised Learning for Speech Enhancement and Separation," in Proc. ICASSP, 2022.
[10] Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, S. Dalmia, X. Chang, and S. Watanabe, "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding," in Proc. SLT, 2022.
[11] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," arXiv:1503.02531, 2015.
[12] H.-J. Chang, S.-w. Yang, and H.-y. Lee, "DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT," in Proc. ICASSP, 2022.
[13] Y. Lee, K. Jang, J. Goo, Y. Jung, and H. R. Kim, "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models," in Proc. Interspeech, 2022.
[14] T. Ashihara, T. Moriya, K. Matsuura, and T. Tanaka, "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models," in Proc. Interspeech, 2022.
[15] R. Reed, "Pruning algorithms-a survey," IEEE Trans. on Neural Networks, vol. 4, no. 5, pp. 740–747, 1993.
[16] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, "Rethinking the value of network pruning," in Proc. ICLR, 2019.
[17] P. Michel, O. Levy, and G. Neubig, "Are sixteen heads really better than one?" in Proc. NeurIPS, 2019.
[18] A. Fan, E. Grave, and A. Joulin, "Reducing transformer depth on demand with structured dropout," in Proc. ICLR, 2020.
[19] Z. Wang, J. Wohlwend, and T. Lei, "Structured Pruning of Large Language Models," in Proc. EMNLP, 2020.
[20] M. Xia, Z. Zhong, and D. Chen, "Structured Pruning Learns Compact and Accurate Models," in Proc. ACL, 2022.
[21] P. Dong, S. Wang, W. Niu et al., "Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition," in ACM/IEEE Design Automation Conference (DAC), 2020.
[22] S. Wang, P. Lin, R. Hu et al., "Acceleration of LSTM With Structured Pruning Method on FPGA," IEEE Access, 2019.
[23] K. Tan and D. Wang, "Compressing Deep Neural Networks for Efficient Speech Enhancement," in Proc. ICASSP, 2021.
[24] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao, Y.-S. Chuang, K. Qian, S. Khurana, D. Cox, and J. Glass, "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition," in Proc. NeurIPS, 2021.
[25] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, "Structured Pruning of Self-Supervised Pre-trained Models for Speech Recognition and Understanding," in Proc. ICASSP, 2023.
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in Proc. NeurIPS, 2017.
[27] A. Pasad, J.-C. Chou, and K. Livescu, "Layer-Wise Analysis of a Self-Supervised Speech Representation Model," in Proc. ASRU, 2021.
[28] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, and H. Li, "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT," in Proc. Interspeech, 2022.
[29] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, "Once-for-all: Train one network and specialize it for efficient deployment," in Proc. ICLR, 2020.
[30] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, "Data2vec: A general framework for self-supervised learning in speech, vision and language," in Proc. ICML, 2022.
[31] C. Louizos, M. Welling, and D. P. Kingma, "Learning Sparse Neural Networks through L0 Regularization," in ICLR, 2018.
[32] A. Paszke et al., "Pytorch: An imperative style, high-performance deep learning library," Proc. NeurIPS, 2019.
[33] Y.-Y. Yang et al., "Torchaudio: Building blocks for audio and speech processing," arXiv:2110.15018, 2021.
[34] M. Ott et al., "fairseq: A fast, extensible toolkit for sequence modeling," in Proc. NAACL-HLT: Demonstrations, 2019.
[35] T. Wolf et al., "Huggingface's transformers: State-of-the-art natural language processing," arXiv preprint arXiv:1910.03771, 2019.
[36] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: An ASR corpus based on public domain audio books," in Proc. ICASSP, 2015.
[37] S. Zhang, E. Loweimi, P. Bell, and S. Renals, "On the usefulness of self-attention for automatic speech recognition with transformers," in Proc. SLT, 2021.
[38] K. Shim, J. Choi, and W. Sung, "Understanding the role of self attention for efficient speech recognition," in Proc. ICLR, 2022.
[39] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," in Proc. ICML, 2022.
[40] T. Maekaku, Y. Fujita, Y. Peng, and S. Watanabe, "Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR," in Proc. Interspeech, 2022.
[41] T.-Q. Lin, H.-y. Lee, and H. Tang, "MelHuBERT: A simplified HuBERT on Mel spectrogram," arXiv:2211.09944, 2022.
[42] F. Wu, K. Kim, J. Pan, K. J. Han, K. Q. Weinberger, and Y. Artzi, "Performance-efficiency trade-offs in unsupervised pre-training for speech recognition," in Proc. ICASSP, 2022.
