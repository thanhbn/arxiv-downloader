# 2310.00014.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/audio/2310.00014.pdf
# File size: 433794 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FEWER-TOKEN NEURAL SPEECH CODEC WITH TIME-INVARIANT CODES
Yong Ren1,2, Tao Wang1, Jiangyan Yi1, Le Xu1,2, Jianhua Tao3, Chu Yuan Zhang1,2, Junzuo Zhou1,2
1Institute of Automation, Chinese Academy of Sciences, China
2School of Artificial Intelligence, University of Chinese Academy of Sciences, China
3Department of Automation, Tsinghua University, China
ABSTRACT
Language model based text-to-speech (TTS) models, like V ALL-E,
have gained attention for their outstanding in-context learning capa-
bility in zero-shot scenarios. Neural speech codec is a critical com-
ponent of these models, which can convert speech into discrete token
representations. However, excessive token sequences from the codec
may negatively affect prediction accuracy and restrict the progres-
sion of Language model based TTS models. To address this issue,
this paper proposes a novel neural speech codec with time-invariant
codes named TiCodec. By encoding and quantizing time-invariant
information into a separate code, TiCodec can reduce the amount
of frame-level information that needs encoding, effectively decreas-
ing the number of tokens as codes of speech. Furthermore, this pa-
per introduces a time-invariant encoding consistency loss to enhance
the consistency of time-invariant code within an utterance, which
can benefit the zero-shot TTS task. Experimental results demon-
strate that TiCodec can not only enhance the quality of reconstruc-
tion speech with fewer tokens but also increase the similarity and
naturalness, as well as reduce the word error rate of the synthe-
sized speech by the TTS model. The code is publicly available at
https://github.com/y-ren16/TiCodec.
Index Terms —speech codec, fewer tokens, time-invariant, lan-
guage model, text-to-speech
1. INTRODUCTION
Recently, large language models have demonstrated remarkable per-
formance on zero-shot text-to-speech (TTS) tasks such as V ALL-
E [1], SPEAR-TTS [2], and SoundStorm [3]. V ALL-E uses dis-
crete tokens derived from Encodec [4] as a representation of speech,
and then trains an autoregressive (AR) language model and a non-
autoregressive (NAR) language model to generate tokens from the
first quantizer and the other seven quantizers separately. It can syn-
thesize high-quality personalized speech by using a short recording
of an unknown speaker as an acoustic prompt. However, the high-
quality reconstruction of speech requires multiple frame-level token
sequences, which affects the inference speed and robustness, and re-
stricts the model structure and training methods of language model
based TTS models. Therefore, how to represent speech better with
fewer tokens has become a core issue.
Neural speech codec is an important method to acquire discrete
token representations of speech. To improve the compression rate
and reduce the number of tokens, more and more research is focus-
ing on neural speech codec [5, 6, 7]. Kleijn et al. [8] proposed a
low-rate speech coding architecture based on the WaveNet [9] de-
coder. Lyra [10] encodes quantized mel-spectrogram features of
speech, and then decodes them with WaveGRU [11]. Subsequently,
end-to-end neural speech codecs have been introduced. Grbaceaet al. [12] used the discretized latent representations proposed in
VQV AE [13] as conditioning for the WaveNet decoder. After that,
SoundStream [14], as a fully convolutional end-to-end universal au-
dio codec model, was proposed, extending the VQV AE vector quan-
tizer to a residual vector quantizer. Following that, Encodec [4] in-
troduced a spectrogram-only adversarial loss, a novel gradient bal-
ancer, and a small Transformer model to further improve the per-
formance of codec. HifiCodec [15] proposes a codec model that
uses group-residual vector quantization to improve the reconstruc-
tion performance of audio. It can achieve good speech reconstruc-
tion performance with only four discrete token sequences, outper-
forming SoundStream and Encodec. However, the performance of
codec decreases significantly when using only one or two discrete
token sequences to represent speech, making it unable to reconstruct
high-quality speech.
To achieve good speech reconstruction performance with only
two or even one sequence of discrete frame-level tokens, we pro-
pose a neural speech codec model with time-invariant codes named
TiCodec. Some information in a speech that does not change over
time is extracted by a time-invariant representation extraction mod-
ule and encoded into a fixed-length code, referred to as the time-
invariant code. This operation can reduce the amount of information
that needs to be encoded in frame-level codes, forcing it to be max-
imally informative about time-related aspects. After obtaining the
frame-level and time-invariant features, they are separately quan-
tized as frame-level and time-invariant tokens. When TiCodec is
used for downstream TTS tasks, the time-invariant tokens can be
extracted from the prompt of target speakers, which can better main-
tain the timbre information of target speakers. At the same time,
fewer frame-level tokens can be used to predict by the TTS model,
while maintaining a low word error rate (WER) and high quality of
synthesized speech. To make the time-invariant token representa-
tions extracted from the target speech in TTS contain more global
time-invariant information, we introduce the time-invariant encod-
ing consistency loss, hoping to improve the robustness of inference
in TTS and further reduce WER.
The contributions of this paper are as follows:
• This paper proposed a neural speech codec model named
TiCodec, which can separate the time-varying and time-
invariant information in speech and quantize them separately.
• A time-invariant encoding consistency loss was introduced to
improve the consistency of the time-invariant codes.
Experimental results on speech reconstruction and zero-shot
TTS task with LibriTTS datasets [16] show that TiCodec achieved
better speech reconstruction performance with fewer tokens and
improved robustness, quality, and similarity of synthesized speech
in the zero-shot TTS task.arXiv:2310.00014v2  [cs.SD]  11 Mar 2024

--- PAGE 2 ---
EncodBlockEncodBlockEncodBlockEncodBlock
Discriminators
Time -invariavt 
TokensFrame -level 
Tokens
DecodBlock
DecodBlock
DecodBlock
DecodBlock
DecodBlock
DecodBlock
DecodBlock
DecodBlock
detachConsistency  Loss Seg1
Seg2Seg1'
z
qz
m
qm
0m
x
ˆx
c
Enc
Dec
TIRE
1Q
2Q(a) The architecture of TiCodec.
Conv1D
LeakyReLU
Avg Pooling 1D
Linear
BatchNorm1d
3 (b)TIRE module.
Fig. 1 . The overview of TiCodec.
2. PROPOSED METHOD
This paper proposes a neural speech codec model named TiCodec
with a time-invariant representation extraction module and a consis-
tency loss. The architecture of TiCodec is shown in Figure 1(a).
2.1. The Framework of TiCodec
The overall framework of our model is an encoder-decoder archi-
tecture. Unlike previous work, our framework utilizes a U-net-like
[17] connection to incorporate the time-invariant features from a hid-
den layer of the encoder into the corresponding hidden layers of the
decoder. We represent a speech signal of duration dasx∈RT
with a sampling rate of fsr, where T=fsr×d. An encoder Enc
transforms the input speech signal xinto its latent representation z,
which is subsequently fed into a residual vector quantizer (RVQ) Q1
for vector quantization. Simultaneously, the intermediate layer rep-
resentation hof the encoder is forwarded to the time-invariant rep-
resentation extraction module ( TIRE ) to extract temporal-invariant
representation m, which is quantized by a group vector quantizer
(GVQ) Q2. The quantizer Q1produces a compressed representa-
tionzqand a discrete token sequence cf. And the quantizer Q2
produces a compressed representation mqand a discrete token se-
quence cg. Finally, the decoder Dec reconstructs the speech signal
ˆxfrom the compressed latent representation zqand the compressed
time-invariant representation mq.
Enc andDec follow a similar structure as HifiCodec [15]. Enc
is composed of a 1D convolutional layer followed by 4 convolutional
modules and a final 1D convolutional layer. Each convolutional
module consists of three residual units and one downsampling layer.
All of these 4 modules indicate a total downsampling of 320 times.
Dec adopts a symmetric structure to the encoder, utilizing transpose
convolutions for upsampling. Q1is a RVQ module to quantize the
latent representation of the speech to frame-level tokens. We used
the same discriminators as HifiCodec [15], which include three dis-
criminators: a multi-scale STFT-based discriminator, a multi-period
discriminator, and a multi-scale discriminator [18].
2.2. Time-invariant Representation Extraction and Quantiza-
tion
We use the output of the second convolutional module in Enc as
the input to TIRE to extract time-invariant representation m. Sim-ilarly, the compressed time-invariant representation mqafter quan-
tizerQ2quantized is introduced into the second transpose convolu-
tional module of Dec for decoding.
As shown in Figure 1(b), TIRE first performs further feature
extraction on the input speech features through three 1D convolu-
tional layers and LeakyReLU layers. Next, temporal averaging on
the extracted features was employed to summarize the frame-level
features into a time-invariant feature. Then, we pass this represen-
tation through a fully connected layer and an activation function to
obtain the final time-invariant representation m.
For the quantization of the time-invariant representation m, we
use GVQ, which divides minto eight groups, resulting in eight dis-
crete tokens as the time-invariant codes. GVQ expands the represen-
tation space, enabling a more extensive portrayal of time-invariant
encoding.
Then the compressed time-invariant representation mqis dupli-
cated across the temporal dimension, converting segment-level time-
invariant features back to frame-level features. Afterward, they are
added to the input of the penultimate layer in the original decoder to
perform decoding jointly.
The time-invariant representation extraction and quantization
modules establish a connection between the contracting and ex-
pansive paths, enabling the flow of shallow representations from
the contracting path to the symmetric expansive path through the
extraction of time-invariant features. Simultaneously representing
time-invariant information through time-invariant codes, the remain-
ing frame-level codes tend to capture greater temporal dependencies
as a result of the information bottleneck, thereby reducing redun-
dancy frame-level codes.
2.3. Time-invariant Encoding Consistency Loss
When TiCodec is employed for downstream TTS tasks, We extract
time-invariant tokens from the target speech segment and then use
textual information to predict frame-level tokens. In order to main-
tain consistency of invariant codes extracted from different segments
of an utterance, we propose the time-invariant encoding consistency
Loss (Lc). During training, in addition to encoding the speech seg-
ment seg 1, we randomly sample another segment seg 2from the
same utterance. Then, seg 2also goes through the first two con-
volution modules of Enc andTIRE , followed by a stop-gradient
operation. The type of training is shown in Figure 1(a).

--- PAGE 3 ---
We use cosine similarity loss as the consistency loss for the ex-
tracted time-invariant representations of the two segments, denoted
asLc.
Lc= 1−cos(TIRE (Enc[: 2](x1)), TIRE (Enc[: 2](x2)))
(1)
where x1andx2denote seg 1andseg 2of speech waveform sepa-
rately and cosdenotes the cosine similarity function.
The generator is trained to optimize the following loss:
L=λtLt+λfLf+λgLg+λfeatLfeat
+λqzLqz+λqmLqm+λcLc(2)
whereLtandLfare reconstruction losses of time domain and fre-
quency domain, Lgis the adversarial loss of the generator, Lfeat is
the feature matching loss, and Lqzis the quantization loss of frame-
level codes. These losses are the same as that in HifiCodec [15].
Lqm=||m−mq||2is the commitment loss of time-invariant codes,
andLcis the time-invariant encoding consistency loss. λt,λf,λg,
λfeat,λqz,λqmandλcare hyperparameters to balance each term of
the final loss.
3. EXPERIMENTS
3.1. EXPERIMENTS SETUP
Dataset. We used LibriTTS [16] datasets to train TiCodec we pro-
posed and V ALL-E model. The LibriTTS corpus consists of 585
hours of speech data at 24kHz from 2,456 speakers. Our training
set was a combination of the train-clean-100, train-clean-360, and
train-other-500 subsets. To evaluate our model, we randomly sam-
pled 100 utterances from test subsets of LibriTTS, VCTK [19] and
AISHELL3 [20] datasets separately.
Baselines. We utilized Encodec [4] and HifiCodec [15] as the base-
line for our TiCodec model. For TTS model V ALL-E, we used Hifi-
Codec [15] as the baseline to convert speech to discrete token repre-
sentations.
Training. The downsampling factors of the encoder of codecs were
set as [8,5,4,2], resulting in a total downsampling of 320 times. The
batch size was 40, and the learning rate was 0.002. Our model
and HifiCodec were trained for 250k iterations when using 2 and 4
frame-level quantizers, and 300k iterations when using 1 frame-level
quantizer. The baseline model Encodec was used with the code reim-
plemented by Yang et al. [15] in HifiCodec1, and it was trained for
25 epochs. For V ALL-E2, we set the maximum duration per batch
to 160. The AR stage was trained for 30 epochs with a maximum
learning rate of 0.05. The NAR stage was trained for 40 epochs with
a maximum learning rate of 0.01. We trained all models on a single
A100 GPU.
Evaluation Metrics. ViSQOL V3 [21], PESQ [22], STOI [23], and
Mel cepstral distortion (MCD) are used to evaluate the objective
quality of speech reconstructed by TiCodec and baselines.
We consider both objective and subjective metrics for V ALL-E.
We use Resemblyzer3to derive a high-level representation of a voice
and then evaluate the speaker similarity between the prompt and syn-
thesized speech. The content accuracy was evaluated by transcrib-
ing the speech with hubert-large-ls960-ft4[24] model and calculat-
ing the WER. Additionally, we calculate the Mean Opinion Score
1https://github.com/yangdongchao/AcademiCodec
2https://github.com/lifeiteng/vall-e
3https://github.com/resemble-ai/Resemblyzer
4https://huggingface.co/facebook/hubert-large-ls960-ftTable 1 . Objective metrics scores of various codecs on LibriTTS.
nqModel ViSQOL ↑PESQ↑STOI↑MCD↓
1Encodec 2.445 1.202 0.749 1.490
HifiCodec 3.335 1.454 0.817 1.416
TiCodec (Ours) 3.631 1.663 0.855 1.315
TiCodec+ Lc(Ours) 3.615 1.634 0.854 1.312
2Encodec 2.562 1.272 0.783 1.356
HifiCodec 3.824 2.025 0.888 1.055
TiCodec (Ours) 3.944 2.149 0.903 0.985
TiCodec+ Lc(Ours) 3.926 2.124 0.903 0.981
4Encodec 2.579 1.290 0.789 1.330
HifiCodec 4.243 2.697 0.936 0.758
TiCodec (Ours) 4.244 2.773 0.940 0.737
TiCodec+ Lc(Ours) 4.255 2.734 0.941 0.735
(MOS) and Similarity Mean Opinion Score (SMOS) with 95% con-
fidence by crowdsourcing. 8 participants were invited to test speech
naturalness and speaker similarity, with MOS and SMOS results.
3.2. Results
3.2.1. Speech Reconstruction Performance of Codecs
To evaluate the speech reconstruction quality of TiCodec, we com-
pared it with the baseline Encodec and HifiCodec with 1, 2, and 4
quantizers separately. Number of frame-level quantizers is denoted
asnq. The objective scores evaluated on a test set of LibriTTS are
shown in Table 1.
Table 1 shows that TiCodec outperforms the baseline in all ob-
jective metrics. Furthermore, as the number of quantized tokens
decreases, the performance improvement of TiCodec becomes in-
creasingly significant. This result is reasonable because a decrease
in the number of frame-level quantizers results in a limited num-
ber of codes becoming increasingly inadequate for encoding the en-
tirety of the information of the original speech. The introduction of
time-invariant codes eliminates the need for duplicating encoding of
repetitive information in speech, such as timbre and acoustic envi-
ronment, at the frame level. This improves the coding efficiency.
To verify the generalization of TiCodec, we also evaluated the
performance of different codecs on the VCTK and AISHELL3
datasets. The objective scores are shown in Table 2. We can see
that our model also outperforms the baseline model in all objective
metrics on both datasets. This indicates that our model is quite
competitive in generalization ability.
0 2 4 60
2
4
6
8
10
12
14
Full
0 2 4 60
2
4
6
8
10
12
14 Seg-F
0 2 4 60
2
4
6
8
10
12
14 Seg-L
(a) TiCodec
0 2 4 60
2
4
6
8
10
12
14Full
0 2 4 60
2
4
6
8
10
12
14 Seg-F
0 2 4 60
2
4
6
8
10
12
14 Seg-L
(b) TiCodec+ Lc
Fig. 2 . The time-invariant codes extracted from an utterance.
We can see from Table 1 and Table 2 that, after introducing the
time-invariance encoding consistency loss Lc, the performance of
the TiCodec slightly decreases on ViSQOL and PESQ metrics. It

--- PAGE 4 ---
Table 2 . The objective metrics scores testing on AISHELL3 and VCTK datasets of various codecs trained on LibriTTS.
nqModelAISHELL3 VCTK
ViSQOL ↑PESQ↑STOI↑MCD↓ViSQOL ↑PESQ↑STOI↑MCD↓
1Encodec 2.175 1.124 0.651 1.007 2.565 1.251 0.688 1.240
HifiCodec 3.005 1.319 0.732 0.955 3.267 1.427 0.748 1.120
TiCodec (Ours) 3.390 1.434 0.777 0.902 3.613 1.641 0.795 0.980
TiCodec+ Lc(Ours) 3.377 1.424 0.775 0.855 3.588 1.723 0.801 0.978
2Encodec 2.325 1.165 0.692 0.934 2.682 1.305 0.710 1.193
HifiCodec 3.625 1.677 0.820 0.733 3.691 1.721 0.811 0.891
TiCodec (Ours) 3.743 1.750 0.836 0.673 3.837 1.848 0.831 0.824
TiCodec+ Lc(Ours) 3.738 1.738 0.841 0.668 3.802 1.805 0.826 0.863
4Encodec 2.349 1.173 0.698 0.917 2.695 1.310 0.714 1.180
HifiCodec 4.108 2.154 0.892 0.502 4.134 2.091 0.868 0.705
TiCodec (Ours) 4.114 2.221 0.894 0.507 4.150 2.266 0.872 0.676
TiCodec+ Lc(Ours) 4.122 2.162 0.896 0.490 4.155 2.177 0.878 0.650
VALL -E
Phoneme Conversion TiCodec Encoder
Text Prompt
Speech PromptTime -invariant 
tokenTiCodec Decoder
Speech S ynthesized
Fig. 3 . V ALL-E used TiCodec as tokenizer.
Table 3 . The objective and subjective metrics scores of V ALL-E.
nqCodec ModelObjective Subjective
WER↓SIM↑ MOS↑ SMOS ↑
Groundtruth 4.60 - 4.50 -
1HifiCodec 55.5 0.701 3.28 (±0.20) 3.08 (±0.14)
TiCodec (Ours) 34.8 0.755 3.80 (±0.21) 4.13 (±0.14)
TiCodec+ Lc(Ours) 28.8 0.7470 4.10 (±0.20)4.23 (±0.12)
2HifiCodec 18.8 0.736 3.95 (±0.19) 3.73 (±0.15)
TiCodec (Ours) 12.4 0.770 4.40 (±0.16)4.15 (±0.12)
TiCodec+ Lc(Ours) 9.10 0.759 4.15 (±0.17) 3.95 (±0.14)
4HifiCodec 12.0 0.7917 4.08 (±0.17) 4.18 (±0.09)
TiCodec (Ours) 12.6 0.780 4.18 (±0.18)4.23 (±0.11)
TiCodec+ Lc(Ours) 11.2 0.782 4.00 (±0.19)4.28 (±0.11)
indicates that Lcslightly affects the perceived quality of the recon-
structed speech. Figure 2 presents the visualization of the time-
invariant representations mextracted from an utterance in the test
set. The three representations in Figure 2(a) and Figure2(b) were ex-
tracted from the entire, the first half (Seg-F), and the last half (Seg-
L) of the utterance. Representations in Figure 2(b) and Figure2(a)
were extracted from TiCodec with and without Lc, respectively. By
adding Lc, the consistency between representations of different seg-
ments within the same utterance is improved. This is advantageous
for the zero-shot TTS task, as demonstrated in the following section.3.2.2. Downstream zero-shot TTS task
We trained V ALL-E using the discrete tokens of speech extracted
separately from HifiCodec, TiCodec, and TiCodec with Lc. For
TiCodec, we directly extract the time-invariant tokens from the
prompt speech and incorporate them into the decoder, as shown in
Figure 3.
Table 3 displays the performance of different neural codecs
used for V ALL-E. TiCodec demonstrates a lower WER and higher
speaker similarity than HifiCodec when used as a tokenizer for
V ALL-E. The MOS and SMOS score of V ALL-E with TiCodec
is significantly higher than that of HifiCodec, especially when the
number of frame-level quantizers is 1 and 2. The incorporation
of the time-invariant encoding consistency loss further reduces the
WER. This improvement occurs because time-invariant tokens are
extracted from prompt audio, and frame-level time-varying tokens
are predicted. Consequently, the consistency loss enhances the
uniformity of time-invariant tokens, thereby improving inference
stability. We put reconstructed speech from various codec models
and samples of zero-shot TTS on our demo page.5
4. CONCLUSION
In this study, we proposed a novel neural speech codec model called
TiCodec with a time-invariant representation extraction module and
a time-invariant encoding consistency loss. Our model exhibits im-
proved performance in speech reconstruction using fewer tokens,
surpassing the performance of previous codecs. By utilizing the ex-
tracted tokens from TiCodec as the input of the zero-shot TTS model
V ALL-E, we achieved a reduction in WER and enhanced the natural-
ness and similarity of the synthesized speech. In the future, we will
explore better methods of decoupling temporal information from in-
variant information and more effective zero-shot TTS methods to
utilize fewer tokens extracted by TiCodec.
5. ACKNOWLEDGMENTS
This work is supported by the Scientific and Technological Innova-
tion Important Plan of China (No. 2021ZD0201502), the National
Natural Science Foundation of China (NSFC) (No. 62322120, No.
62306316, No.61831022, No.U21B2010, No.62101553, No.61971419,
No.62006223, No. 62206278).
5https://y-ren16.github.io/TiCodec

--- PAGE 5 ---
6. REFERENCES
[1] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long
Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang,
Jinyu Li, et al., “Neural codec language models are zero-shot
text to speech synthesizers,” arXiv preprint arXiv:2301.02111 ,
2023.
[2] Eugene Kharitonov, Damien Vincent, Zal ´an Borsos, Rapha ¨el
Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco
Tagliasacchi, and Neil Zeghidour, “Speak, read and prompt:
High-fidelity text-to-speech with minimal supervision,” arXiv
preprint arXiv:2302.03540 , 2023.
[3] Zal ´an Borsos, Matt Sharifi, Damien Vincent, Eugene
Kharitonov, Neil Zeghidour, and Marco Tagliasacchi, “Sound-
storm: Efficient parallel audio generation,” arXiv preprint
arXiv:2305.09636 , 2023.
[4] Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and Yossi
Adi, “High fidelity neural audio compression,” arXiv preprint
arXiv:2210.13438 , 2022.
[5] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan
Kumar, and Kundan Kumar, “High-fidelity audio compres-
sion with improved rvqgan,” arXiv preprint arXiv:2306.06546 ,
2023.
[6] Linping Xu, Jiawei Jiang, Dejun Zhang, Xianjun Xia, Li Chen,
Yijian Xiao, Piao Ding, Shenyi Song, Sixing Yin, and Ferdous
Sohel, “An Intra-BRNN and GB-RVQ Based END-TO-END
Neural Audio Codec,” in Proc. INTERSPEECH 2023 , 2023,
pp. 800–803.
[7] Youqiang Zheng, Li Xiao, Weiping Tu, Yuhong Yang, and Xin-
meng Xu, “CQNV: A Combination of Coarsely Quantized Bit-
stream and Neural V ocoder for Low Rate Speech Coding,” in
Proc. INTERSPEECH 2023 , 2023, pp. 171–175.
[8] W Bastiaan Kleijn, Felicia SC Lim, Alejandro Luebs, Jan
Skoglund, Florian Stimberg, Quan Wang, and Thomas C Wal-
ters, “Wavenet based low rate speech coding,” in 2018 IEEE
international conference on acoustics, speech and signal pro-
cessing (ICASSP) . IEEE, 2018, pp. 676–680.
[9] A ¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Si-
monyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, An-
drew Senior, and Koray Kavukcuoglu, “Wavenet: A generative
model for raw audio,” in 9th ISCA Speech Synthesis Workshop ,
pp. 125–125.
[10] W Bastiaan Kleijn, Andrew Storus, Michael Chinen, Tom Den-
ton, Felicia SC Lim, Alejandro Luebs, Jan Skoglund, and
Hengchin Yeh, “Generative speech coding with predictive
variance regularization,” in ICASSP 2021-2021 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2021, pp. 6478–6482.
[11] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and
Yoshua Bengio, “Empirical evaluation of gated recurrent neu-
ral networks on sequence modeling,” in NIPS 2014 Workshop
on Deep Learning, December 2014 , 2014.
[12] Cristina G ˆarbacea, A ¨aron van den Oord, Yazhe Li, Felicia SC
Lim, Alejandro Luebs, Oriol Vinyals, and Thomas C Walters,
“Low bit-rate speech coding with vq-vae and a wavenet de-
coder,” in ICASSP 2019-2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2019, pp. 735–739.[13] Aaron Van Den Oord, Oriol Vinyals, et al., “Neural discrete
representation learning,” Advances in neural information pro-
cessing systems , vol. 30, 2017.
[14] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan
Skoglund, and Marco Tagliasacchi, “Soundstream: An end-to-
end neural audio codec,” IEEE/ACM Transactions on Audio,
Speech, and Language Processing , vol. 30, pp. 495–507, 2021.
[15] Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan
Tian, Chao Weng, and Yuexian Zou, “Hifi-codec: Group-
residual vector quantization for high fidelity audio codec,”
arXiv preprint arXiv:2305.02765 , 2023.
[16] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss,
Ye Jia, Zhifeng Chen, and Yonghui Wu, “Libritts: A corpus
derived from librispeech for text-to-speech,” Interspeech 2019 ,
2019.
[17] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, “U-
net: Convolutional networks for biomedical image segmenta-
tion,” in Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18. Springer, 2015, pp. 234–241.
[18] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, “Hifi-gan:
Generative adversarial networks for efficient and high fidelity
speech synthesis,” Advances in Neural Information Processing
Systems , vol. 33, pp. 17022–17033, 2020.
[19] Zhaoyu Liu and Brian Mak, “Cross-lingual multi-speaker text-
to-speech synthesis for voice cloning without using parallel
corpus for unseen speakers,” arXiv preprint arXiv:1911.11601 ,
2019.
[20] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li, “Aishell-
3: A multi-speaker mandarin tts corpus and the baselines,”
arXiv preprint arXiv:2010.11567 , 2020.
[21] Michael Chinen, Felicia SC Lim, Jan Skoglund, Nikita Gureev,
Feargus O’Gorman, and Andrew Hines, “Visqol v3: An open
source production ready objective speech and audio metric,” in
2020 twelfth international conference on quality of multimedia
experience (QoMEX) . IEEE, 2020, pp. 1–6.
[22] ITU-T Recommendation, “Perceptual evaluation of speech
quality (pesq): An objective method for end-to-end speech
quality assessment of narrow-band telephone networks and
speech codecs,” Rec. ITU-T P . 862 , 2001.
[23] Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jes-
per Jensen, “A short-time objective intelligibility measure for
time-frequency weighted noisy speech,” in 2010 IEEE interna-
tional conference on acoustics, speech and signal processing .
IEEE, 2010, pp. 4214–4217.
[24] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman
Mohamed, “Hubert: Self-supervised speech representation
learning by masked prediction of hidden units,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing , vol.
29, pp. 3451–3460, 2021.
