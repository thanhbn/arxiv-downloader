# 2410.02271.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/audio/2410.02271.pdf
# Kích thước tệp: 441422 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CoLLAP: Tiền huấn luyện Ngôn ngữ-Âm thanh Dạng dài Tương phản
với Tăng cường Cấu trúc Thời gian Âm nhạc

Junda Wu
Khoa Khoa học Máy tính và Kỹ thuật
UC San Diego
La Jolla, USA
juw069@ucsd.edu

Amit Namburi
Khoa Khoa học Máy tính và Kỹ thuật
UC San Diego
La Jolla, USA
anamburi@ucsd.edu

Warren Li
Khoa Khoa học Máy tính và Kỹ thuật
UC San Diego
La Jolla, USA
wyl003@ucsd.edu

Carol Chen
Khoa Khoa học Máy tính
UC Los Angeles
Los Angeles, USA
carolchen12@ucla.edu

Zachary Novack
Khoa Khoa học Máy tính và Kỹ thuật
UC San Diego
La Jolla, USA
znovack@ucsd.edu

Julian McAuley
Khoa Khoa học Máy tính và Kỹ thuật
UC San Diego
La Jolla, USA
jmcauley@ucsd.edu

Tóm tắt — Mô hình hóa các đặc tính thời gian đóng vai trò quan trọng trong việc học biểu diễn của sóng âm thanh. Chúng tôi đề xuất Tiền huấn luyện Ngôn ngữ-Âm thanh Dạng dài Tương phản (CoLLAP) để mở rộng đáng kể cửa sổ nhận thức cho cả âm thanh đầu vào (lên đến 5 phút) và các mô tả ngôn ngữ (vượt quá 250 từ), đồng thời cho phép học tương phản qua các phương thức và động lực thời gian. Tận dụng các Music-LLM gần đây để tạo ra các chú thích âm nhạc dạng dài cho các bài hát toàn bộ, được tăng cường với cấu trúc thời gian âm nhạc, chúng tôi thu thập 51.3K cặp âm thanh-văn bản được dẫn xuất từ tập dữ liệu huấn luyện AudioSet quy mô lớn, trong đó độ dài âm thanh trung bình đạt 288 giây. Chúng tôi đề xuất một kiến trúc học tương phản mới kết hợp các biểu diễn ngôn ngữ với các biểu diễn âm thanh có cấu trúc bằng cách phân đoạn mỗi bài hát thành các clip và trích xuất embedding của chúng. Với cơ chế attention, chúng tôi nắm bắt các tương quan thời gian đa phương thức, cho phép mô hình tự động cân bằng và tăng cường điểm số fusion cuối cùng để cải thiện sự căn chỉnh tương phản. Cuối cùng, chúng tôi phát triển hai biến thể của mô hình CoLLAP với các loại mô hình ngôn ngữ backbone khác nhau. Thông qua các thí nghiệm toàn diện trên nhiều bộ dữ liệu truy xuất âm nhạc-văn bản dạng dài, chúng tôi chứng minh sự cải thiện hiệu suất nhất quán về độ chính xác truy xuất so với các baseline. Chúng tôi cũng cho thấy các mô hình CoLLAP đã được tiền huấn luyện có thể được chuyển giao cho các tác vụ truy xuất thông tin âm nhạc khác nhau, với các ngữ cảnh đa phương thức dạng dài không đồng nhất.

I. GIỚI THIỆU

Khả năng mô hình hóa hiệu quả các đặc tính thời gian là cần thiết trong việc học biểu diễn của các sóng âm thanh, đặc biệt đối với các track âm nhạc phức tạp và toàn bộ độ dài. Các nghiên cứu truy xuất thông tin âm nhạc [1], [2] đã nghiên cứu các phương pháp trích xuất thông tin cấu trúc và thời gian âm nhạc, có thể được sử dụng thêm để tăng cường khả năng hiểu âm nhạc của mô hình [3]. Các phương pháp học tương phản gần đây [4]–[6] cho phép trích xuất thông tin như vậy dưới dạng các biểu diễn âm thanh tiềm ẩn, được huấn luyện để phân biệt giữa các cặp văn bản-âm thanh khớp và các cặp không khớp khác bằng cách nắm bắt các đặc trưng phân biệt trong dữ liệu âm thanh (được minh họa trong Hình 1a). Tuy nhiên, các phương pháp như vậy đã tập trung vào các đoạn tương đối ngắn, hạn chế khả năng của mô hình trong việc xử lý các chuỗi dài hơn, tinh tế hơn.

Để giải quyết những thách thức này, chúng tôi giới thiệu Tiền huấn luyện Ngôn ngữ-âm thanh Dạng dài Tương phản (CoLLAP), mở rộng cửa sổ nhận thức để xử lý cả đầu vào âm thanh dạng dài và mô tả ngôn ngữ chi tiết. Chúng tôi minh họa sự so sánh giữa mô hình CLAP thông thường và mô hình CoLLAP đề xuất của chúng tôi trong Hình 1. Mô hình CoLLAP sử dụng một bộ trích xuất đặc trưng để phân đoạn các track âm nhạc thành khung và mã hóa từng khung bằng một hàm kernel. Sau đó các cơ chế attention theo kernel và thời gian được sử dụng để đo lường sự căn chỉnh toàn cục và thời gian giữa âm thanh và văn bản. Cuối cùng, mô hình được

phân đoạn & lấy mẫu xuống
Bộ mã hóa âm thanh
Ngân hàng bộ lọc Mel
Fusion đặc trưng Attention
Lớp mã hóa khác
Embedding âm thanh 1D
Clip âm thanh < 30 giây
Chú thích âm nhạc ngắn < 50 từ: Bản ghi âm chất lượng thấp có tiếng bước chân, tiếng chim hót, tiếng xe cộ xa xôi và hiệu ứng âm thanh gió.
RoBERTa
Embedding văn bản 1D
Khoảng cách Cosine

(a) Minh họa mô hình CLAP thông thường, với đầu vào bao gồm chú thích âm nhạc ngắn (ít hơn 50 từ) và clip âm thanh ngắn (ít hơn 30 giây). CLAP chỉ trích xuất embedding văn bản và âm thanh toàn cục 1 chiều để tính toán độ tương tự cosine.

Attention theo Kernel & Thời gian
Attention tổng hợp trung bình
Track âm nhạc > 4 phút
Văn bản dạng dài > 250 từ: Bài hát có tâm trạng êm dịu và thư giãn... "0-27% Intro & Verse": Âm nhạc bắt đầu với một nốt chậm, kéo dài từ bansuri... "68-100% Outro": ...
RoBERTa/GPT2
Embedding văn bản 1D
Khoảng cách Cosine
Whisper
Bộ chuyển đổi đặc trưng
Kernelization
Beats
Embedding âm thanh 3D

(b) Minh họa mô hình CoLLAP đề xuất của chúng tôi, với đầu vào bao gồm mô tả âm nhạc chi tiết và nhận biết thời gian (hơn 250 từ) và track âm nhạc toàn bộ độ dài (hơn 4 phút). CoLLAP trích xuất embedding âm thanh 3 chiều và tổng hợp sử dụng pooling attention 3D mô hình hóa attention thời gian một cách rõ ràng. Chúng tôi cũng cho phép hai biến thể của CoLLAP sử dụng các backbone ngôn ngữ khác nhau RoBERTa và GPT2.

Hình 1: So sánh CLAP thông thường (Hình 1a) và CoLLAP đề xuất của chúng tôi (Hình 1b).

được tối ưu hóa với học tương phản sử dụng điểm số tương tự có trọng số từ cả attention theo kernel và thời gian. CoLLAP mở rộng hiệu quả cửa sổ nhận thức cho cả âm thanh đầu vào (lên đến 5 phút) và mô tả ngôn ngữ (vượt quá 250 từ), cho phép truy xuất các track âm nhạc toàn bộ độ dài với mô tả âm nhạc chi tiết.

--- TRANG 2 ---

Để cho phép tiền huấn luyện tương phản quy mô lớn của CoLLAP, chúng tôi tận dụng một bộ dữ liệu được tăng cường Music-LLM gồm 51.3K cặp âm thanh-văn bản và 4,109 giờ âm thanh, được dẫn xuất từ dữ liệu huấn luyện AudioSet quy mô lớn, với độ dài âm thanh trung bình là 288 giây và độ dài văn bản trung bình là 256 từ. Ngoài ra, chúng tôi phát triển hai biến thể của CoLLAP dựa trên hai mô hình ngôn ngữ backbone khác nhau, Roberta-base [7] và GPT2 [8].

Cuối cùng, chúng tôi tiến hành các thí nghiệm toàn diện trên nhiều bộ dữ liệu truy xuất âm nhạc-văn bản dạng dài và quan sát sự cải thiện nhất quán về độ chính xác truy xuất của CoLLAP so với các mô hình baseline. Chúng tôi cũng đánh giá khả năng học chuyển giao của CoLLAP trong các tác vụ truy xuất thông tin âm nhạc khác nhau liên quan đến các ngữ cảnh đa phương thức dạng dài không đồng nhất, bao gồm âm thanh giọng nói và ngữ cảnh dài tự do Wikipedia. Ngoài ra, chúng tôi cũng quan sát khả năng tổng quát hóa tốt hơn trong biến thể CoLLAP-GPT2 so với backbone mô hình RoBERTa do khả năng mô hình hóa ngôn ngữ tốt hơn của mô hình GPT2 đối với ngữ cảnh dài. Chúng tôi tóm tắt các đóng góp như sau:

• Chúng tôi đề xuất mô hình Tiền huấn luyện Ngôn ngữ-âm thanh Dạng dài Tương phản (CoLLAP) cho việc fusion đa phương thức và học biểu diễn của âm thanh dạng dài và mô tả ngôn ngữ.

• Chúng tôi thiết kế một cơ chế fusion mới kết hợp các biểu diễn âm thanh và ngôn ngữ có cấu trúc, tận dụng attention để nắm bắt và cân bằng các tương quan thời gian đa phương thức để cải thiện sự căn chỉnh tương phản.

• Chúng tôi tăng cường một bộ dữ liệu gồm 4,109 giờ track âm nhạc toàn bộ độ dài dạng dài, được ghép nối với các chú thích được tăng cường cấu trúc âm nhạc được tạo ra bởi Music-LLM.

• Thông qua các thí nghiệm toàn diện, chúng tôi chứng minh rằng CoLLAP liên tục vượt trội so với các mô hình baseline trong truy xuất văn bản-âm thanh dạng dài, và cho thấy khả năng tổng quát hóa của nó qua các tác vụ khác nhau.

II. COLLAP: THIẾT KẾ MÔ HÌNH VÀ HỌC TẬP

Chúng tôi minh họa thiết kế mô hình CoLLAP trong Hình 2, trong đó sóng track âm nhạc toàn bộ độ dài được xử lý với bộ trích xuất đặc trưng kép, trong khi các biểu diễn văn bản được trích xuất từ các chú thích được tăng cường cấu trúc âm nhạc. Chúng tôi chia các track âm nhạc có độ dài biến đổi thành các khung để cho phép attention thời gian âm thanh với văn bản, trích xuất và đo lường cả điểm số căn chỉnh đa phương thức toàn cục và thời gian. Với các điểm số căn chỉnh được tăng cường attention thời gian, chúng tôi tuân theo sơ đồ học tương phản thông thường [4], [6], [9], [10], trong đó loss tương phản sẽ được lan truyền ngược về cả attention thời gian và các bộ trích xuất đặc trưng.

A. Bộ mã hóa văn bản và âm thanh động

Cho N cặp âm thanh-văn bản đầu vào {(Xi, Yi)}i<N, chúng tôi trích xuất các embedding văn bản Ti∈RD, embedding âm nhạc Oi∈RD, và embedding giọng nói Si∈RD như sau:

Ti=fT(Yi;θT), Oi=fO(Xi;θO), Si=fS(Xi;θS),

trong đó các tham số mô hình của bộ mã hóa văn bản θT được khởi tạo từ một mô hình ngôn ngữ đã được tiền huấn luyện (ví dụ, RoBERTa [7] và GPT-2 [8]), trong khi bộ mã hóa âm nhạc và bộ mã hóa giọng nói được thích ứng từ các mô hình BEATS [11] và Whisper [12]. Chúng tôi fusion các embedding âm nhạc và giọng nói bằng một lớp tuyến tính bộ chuyển đổi đặc trưng âm thanh hA,

Ui=hA([Oi, Si]), i < N.

[Diagram showing model architecture with components like Beats, Whisper, Musical Structural Augmented Texts, Adapter, Kernel Function, etc.]

Hình 2: Tổng quan mô hình của CoLLAP. Đầu vào của các mô hình ngôn ngữ backbone là các văn bản được tăng cường cấu trúc âm nhạc, trong khi sóng âm thanh được mã hóa bởi bộ trích xuất đặc trưng kép của các mô hình Beats và Whisper. Các đặc trưng đa phương thức được mã hóa được sử dụng để tính toán attention thời gian và theo kernel trước khi tính toán loss học tương phản.

Sau đó, chúng tôi chia biểu diễn âm thanh thống nhất với độ dài T thành các khung liên tiếp với một hàm kernel có kích thước kernel H và bước stride ST,

H=T·ηK/30, ST=T·ηS/30,

trong đó ηK được định nghĩa trước để xác định bao nhiêu giây mỗi khung, và ηS xác định giây mỗi stride. Cuối cùng, biểu diễn âm thanh được xử lý được unfold và reshape thành Ii={Iv,hi}Wi,H ∈ RH×Wi×D,

Ii=Unfold(Ui, H, ST), trong đó Wi=(T−H)/ST + 1. (1)

Với âm thanh được tokenize với các khung có độ dài cố định Ii={Iv,hi}Wi,H, chúng tôi có thể tính toán attention theo kernel và attention thời gian để tăng cường ước lượng căn chỉnh đa phương thức.

B. Tăng cường Attention đa phương thức và thời gian

Cho biểu diễn âm thanh Ii={Iv,hi}Wi,H và biểu diễn văn bản Tj, chúng tôi tính toán độ tương tự cosine của chúng

Mi,j={(Iv,hi)⊤Tj}Wi,H, (2)

trong mỗi khung v < Wi và mỗi kernel h < H. Để đo lường thêm attention của văn bản trên từng khung và kernel riêng lẻ, chúng tôi tính toán

--- TRANG 3 ---

attention theo kernel AKi,j và attention thời gian ATi,j,

AKi,j(v, h) = eMi,j(v,h) / Σk<H eMi,j(v,k), (3)

ATi,j(v, h) = eMi,j(v,h) / Σl<Wi eMi,j(l,h), (4)

trong đó Mi,j(v, h) là điểm số tương tự cosine tương ứng của khung thứ v và kernel thứ h trong Mi,j.

C. Học tương phản được fusion attention thời gian

Sau đó chúng tôi sử dụng attention theo kernel AKi,j và attention thời gian ATi,j đã tính toán để cân bằng và tổng hợp ma trận tương tự cosine gốc Mi,j. Để có được tương tự toàn cục giữa văn bản và âm thanh, Mi,j được cân bằng bởi attention theo kernel AKi,j với average pooling,

rKi,j = (1/H) Σk<H Σl<Wi Mi,j(k, l)·AKi,j(k, l). (5)

Để nắm bắt tương tự được cân bằng attention thời gian giữa văn bản và âm thanh, chúng tôi tiếp tục dẫn xuất điểm số tương tự tương tự,

rTi,j = (1/Wi) Σl<Wi Σk<H Mi,j(k, l)·ATi,j(k, l). (6)

Cuối cùng, chúng tôi kết hợp hai điểm số tương tự có trọng số với hai scaler γK và γT để cân bằng. Do đó, mỗi điểm số tương tự cosine theo cặp ri,j∈RN×N trong mini-batch được tính toán như

ri,j = γK·rKi,j + γT·rTi,j. (7)

Theo [4], [5], [9], chúng tôi áp dụng hàm loss tương phản thông thường để dẫn xuất loss cuối cùng,

L = -Σi<N log(eri,i / Σj<N eri,j), (8)

trong đó loss tương phản sẽ được lan truyền ngược về cả attention thời gian và các bộ trích xuất đặc trưng.

III. BỘ DỮ LIỆU TRUY XUẤT VĂN BẢN-ÂM THANH DẠNG DÀI VÀ NHẬN BIẾT CẤU TRÚC

Chúng tôi thu thập một bộ dữ liệu sóng âm thanh dạng dài quy mô lớn được dẫn xuất từ các track toàn bộ độ dài từ tập con huấn luyện của AudioSet [13]. Chúng tôi lọc ra các track âm thanh có độ dài ít hơn 2 phút hoặc dài hơn 5 phút, tích lũy tổng cộng 51.3K và 4,109.50 giờ track âm thanh với độ dài trung bình 288.25 giây mỗi track. Để ghép nối thêm các track âm thanh toàn bộ độ dài với các chú thích dạng dài và chi tiết mô tả toàn diện toàn bộ track, chúng tôi tận dụng mô hình FUTGA [3] để tạo ra các chú thích dày đặc, cung cấp cả chú thích toàn cục và thông tin cấu trúc nhận biết thời gian. Do đó, các chú thích dày đặc được tạo ra có trung bình 256.94 từ cho mỗi chú thích.

BẢNG I: So sánh thống kê của các bộ dữ liệu truy xuất văn bản-âm nhạc hiện có và CoLLAP.

Bộ dữ liệu | Cặp | Âm thanh (giờ) | Thời lượng Trung bình (giây) | Thời lượng Trung bình Từ
AudioCaps [14] | 51k | 144.9 | 10.23 | 9.0
MusicCaps [15] | 6k | 15.3 | 10.00 | 48.9
LAION-Audio [6] | 633.5k | 4325.39 | 24.58 | –
LP-MusicCaps [16] | 514k | 4283.10 | 30.00 | 37.3
CoLLAP | 51.3k | 4109.50 | 288.25 | 256.94

Chúng tôi so sánh bộ dữ liệu truy xuất văn bản-âm thanh dạng dài và nhận biết cấu trúc đã thu thập của chúng tôi trong Bảng I, trong đó chúng tôi cho thấy bộ dữ liệu của chúng tôi có tổng độ dài tương đương với các bộ dữ liệu văn bản-âm thanh quy mô lớn hiện có (ví dụ, LAION-Audio [6] và LP-MusicCaps [16]). Ngoài ra, chúng tôi chứng minh rằng độ dài âm thanh của chúng tôi dài hơn khoảng mười lần so với bộ dữ liệu hiện có trung bình, trong khi độ dài văn bản trung bình của chúng tôi dài hơn khoảng năm lần so với MusicCaps chi tiết [15].

IV. THÍ NGHIỆM

A. Chi tiết triển khai

Chúng tôi triển khai mô hình CoLLAP sử dụng framework PyTorch 2.2, tận dụng các mô hình RoBERTa và GPT-2 đã được tiền huấn luyện cho bộ mã hóa văn bản và thích ứng các mô hình BEATS và Whisper cho các bộ mã hóa âm nhạc và giọng nói, tương ứng. Chúng tôi thu thập 51.3K cặp âm thanh-văn bản dạng dài được dẫn xuất từ bộ dữ liệu AudioSet-train gốc [13], với thời lượng âm thanh trung bình 288 giây và độ dài văn bản 257 từ.

Chúng tôi khởi tạo RoBERTa hoặc GPT-2 cho bộ mã hóa văn bản với các trọng số đã được tiền huấn luyện. Các bộ mã hóa âm nhạc và giọng nói được thích ứng tương ứng từ các mô hình BEATS và Whisper và được nối kết hợp thành embedding âm thanh fusion. Kích thước embedding văn bản và âm thanh fusion được đặt thành 512. Chúng tôi fine-tune toàn bộ tham số của cả bộ mã hóa văn bản và embedding âm thanh, sử dụng optimizer AdamW với learning rate 1e−4 và weight decay 1e−5. Chúng tôi sử dụng batch size 50 và cho phép loss học tương phản in-batch được triển khai bởi hàm loss cross-entropy. Quá trình học tương phản được đặt cho 20 epoch, với scheduling learning rate tuyến tính. Quá trình huấn luyện tận dụng 2 GPU NVIDIA A100 với 40GB bộ nhớ.

B. Bộ dữ liệu và Baseline

Chúng tôi đánh giá mô hình CoLLAP trên ba tác vụ truy xuất văn bản-âm thanh, trong đó bốn bộ dữ liệu, SongDescriber [17], MusicCaps [15], AudioSet-Eval [13], và HarmonixSet [18], được sử dụng cho truy xuất văn bản-âm thanh dạng dài tổng quát. Để kiểm tra độ chính xác truy xuất trong lĩnh vực giọng nói, chúng tôi đánh giá bộ dữ liệu VCTK [19] cho truy xuất transcript ngữ cảnh dài đến giọng nói đầy đủ. Cuối cùng, chúng tôi đánh giá thêm khả năng tổng quát hóa zero-shot của mô hình trong ngữ cảnh âm nhạc dạng tự do được thu thập từ các trang Wikipedia và cho phép truy xuất wiki-to-music.

Chúng tôi so sánh mô hình CoLLAP đề xuất của chúng tôi với ba baseline học tương phản cho thí nghiệm chính trong Bảng II: HSTAT (RoBERTa) [6] sử dụng RoBERTa cho mã hóa văn bản và kết hợp cơ chế fusion đặc trưng và tăng cường keyword-to-caption; Larger CLAP [6] tăng cường thêm hiệu suất mô hình trên các lĩnh vực âm nhạc và giọng nói bằng tiền huấn luyện mở rộng; Cacophony [5] tăng cường bằng cơ chế attention phân cấp và kỹ thuật fusion tiên tiến để kết hợp động các đặc trưng đa quy mô từ cả hai phương thức. Đối với phương pháp của chúng tôi, chúng tôi phát triển hai biến thể mô hình CoLLAP (RoBERTa) và CoLLAP (GPT2) sử dụng hai backbone mô hình ngôn ngữ khác nhau.

C. Truy xuất văn bản-âm thanh dạng dài

Các thí nghiệm truy xuất văn bản-âm thanh dạng dài được thiết kế để đánh giá hiệu quả của mô hình CoLLAP trong việc căn chỉnh các track âm thanh mở rộng với các mô tả văn bản tương ứng của chúng. Hiệu suất truy xuất được đo lường sử dụng recall tại rank 5, 20, và 100 cho cả tác vụ truy xuất text-to-audio (T2A) và audio-to-text (A2T).

Như được trình bày trong Bảng II, các biến thể CoLLAP vượt trội so với các mô hình baseline trên tất cả các bộ dữ liệu, đặc biệt trên SongDescriber và HarmonixSet. Các cơ chế attention trong CoLLAP cho phép mô hình nắm bắt hiệu quả các tương quan thời gian và đa phương thức, dẫn đến những cải thiện đáng kể về độ chính xác truy xuất. Biến thể CoLLAP dựa trên RoBERTa thể hiện hiệu suất cao hơn một chút, đặc biệt trong các tác vụ truy xuất A2T.

--- TRANG 4 ---

BẢNG II: Hiệu suất truy xuất của ba biến thể Larger-CLAP và hai biến thể CoLLAP trên bốn bộ dữ liệu đánh giá. Chúng tôi báo cáo các giá trị recall của rank 5, 20, và 100 cho truy xuất text-to-music (T2M) và music-to-text (M2T). Các giá trị tốt nhất được tô đậm, trong khi các giá trị tốt thứ hai được gạch chân.

[TABLE showing retrieval performance metrics for different models across SongDescriber, MusicCaps, AudioSet-Eval, and HarmonixSet datasets, with T2A and A2T metrics at R@5, R@20, and R@100]

D. Truy xuất transcript-giọng nói zero-shot

Chúng tôi cũng đánh giá hiệu suất chuyển giao zero-shot của CoLLAP trên các tác vụ truy xuất transcript-giọng nói sử dụng bộ dữ liệu VCTK. Thí nghiệm này đánh giá khả năng của mô hình trong việc căn chỉnh nội dung được nói với các transcript tương ứng mà không cần fine-tuning thêm. Bảng III báo cáo hiệu suất truy xuất cho cả tác vụ T2A và A2T tại các rank recall khác nhau.

Kết quả cho thấy các biến thể mô hình CoLLAP duy trì độ chính xác truy xuất mạnh mẽ trong cài đặt zero-shot này. Biến thể dựa trên GPT-2 vượt trội so với biến thể dựa trên RoBERTa, gợi ý rằng khả năng tạo sinh của GPT-2 có thể xử lý tốt hơn tính biến đổi trong ngôn ngữ nói. Những phát hiện này làm nổi bật tiềm năng của CoLLAP cho các ứng dụng trong nhận dạng giọng nói và căn chỉnh âm thanh-văn bản.

BẢNG III: Truy xuất giọng nói và âm thanh trên bộ dữ liệu VCTK [20]. Chúng tôi báo cáo các chỉ số Recall@k cho truy xuất text-to-audio (T2A) và audio-to-text (A2T).

[TABLE showing performance metrics for VCTK dataset across different models]

E. Truy xuất Wiki-âm nhạc zero-shot

Cuối cùng, chúng tôi đánh giá khả năng tổng quát hóa của CoLLAP trong việc truy xuất nội dung liên quan đến âm nhạc từ các mô tả văn bản theo cách zero-shot sử dụng bộ dữ liệu Wiki-music. Bộ dữ liệu này bao gồm các bài viết Wikipedia được ghép nối với các clip âm thanh, và tác vụ liên quan đến việc truy xuất clip âm thanh chính xác khi cho một truy vấn văn bản và ngược lại. Hiệu suất truy xuất được chi tiết trong Bảng IV.

CoLLAP đạt được những cải thiện đáng kể so với các mô hình baseline trong các tác vụ Wiki-SD và Wiki-MC. Các cơ chế attention của mô hình cho phép nó căn chỉnh hiệu quả văn bản dạng dài với các phân đoạn âm thanh tương ứng, dẫn đến cải thiện độ chính xác truy xuất. Những kết quả này gợi ý rằng CoLLAP có thể được chuyển giao hiệu quả cho các tác vụ truy xuất thông tin liên quan đến âm nhạc đa dạng, làm cho nó trở thành một công cụ đa năng để khám phá các bộ dữ liệu đa phương thức quy mô lớn.

BẢNG IV: Truy xuất ngữ cảnh Wikipedia và âm thanh trên các bộ dữ liệu MusicCaps và SongDescriber. Chúng tôi báo cáo các chỉ số Recall@k cho truy xuất wiki-to-music (W2M) và music-to-wiki (M2W).

[TABLE showing performance metrics for Wiki-SD and Wiki-MC datasets]

V. KẾT LUẬN

Trong bài báo này, chúng tôi giới thiệu CoLLAP, một framework học tương phản mới được thiết kế cho việc học biểu diễn ngôn ngữ-âm thanh dạng dài. Mô hình của chúng tôi tận dụng trích xuất đặc trưng kép và cơ chế attention đa phương thức để nắm bắt hiệu quả cả sự căn chỉnh toàn cục và thời gian giữa các track âm thanh dài và mô tả văn bản chi tiết. Thông qua các thí nghiệm toàn diện trên nhiều bộ dữ liệu, bao gồm SongDescriber, MusicCaps, AudioSet-Eval, HarmonixSet, và Wiki-music, chúng tôi chứng minh rằng CoLLAP cải thiện đáng kể hiệu suất truy xuất so với các mô hình baseline hiện có.

--- TRANG 5 ---

TÀI LIỆU THAM KHẢO

[1] N. Whiteley, A. T. Cemgil, và S. J. Godsill, "Bayesian modelling of temporal structure in musical audio." trong ISMIR, 2006, tr. 29–34.

[2] R. J. Weiss và J. P. Bello, "Unsupervised discovery of temporal structure in music," IEEE Journal of Selected Topics in Signal Processing, tập 5, số 6, tr. 1240–1251, 2011.

[3] J. Wu, Z. Novack, A. Namburi, J. Dai, H.-W. Dong, Z. Xie, C. Chen, và J. McAuley, "Futga: Towards fine-grained music understanding through temporally-enhanced generative augmentation," arXiv preprint arXiv:2407.20445, 2024.

[4] B. Elizalde, S. Deshmukh, M. Al Ismail, và H. Wang, "Clap learning audio concepts from natural language supervision," trong ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, tr. 1–5.

[5] G. Zhu và Z. Duan, "Cacophony: An improved contrastive audio-text model," arXiv preprint arXiv:2402.06986, 2024.

[6] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, và S. Dubnov, "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation," trong ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, tr. 1–5.

[7] Y. Liu, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.

[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever và cộng sự, "Language models are unsupervised multitask learners," OpenAI blog, tập 1, số 8, tr. 9, 2019.

[9] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark và cộng sự, "Learning transferable visual models from natural language supervision," trong International conference on machine learning. PMLR, 2021, tr. 8748–8763.

[10] Y. Yuan, Z. Chen, X. Liu, H. Liu, X. Xu, D. Jia, Y. Chen, M. D. Plumbley, và W. Wang, "T-clap: Temporal-enhanced contrastive language-audio pretraining," arXiv preprint arXiv:2404.17806, 2024.

[11] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, và F. Wei, "Beats: Audio pre-training with acoustic tokenizers," arXiv preprint arXiv:2212.09058, 2022.

[12] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, và I. Sutskever, "Robust speech recognition via large-scale weak supervision," trong International conference on machine learning. PMLR, 2023, tr. 28 492–28 518.

[13] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, và M. Ritter, "Audio set: An ontology and human-labeled dataset for audio events," trong 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, tr. 776–780.

[14] C. D. Kim, B. Kim, H. Lee, và G. Kim, "Audiocaps: Generating captions for audios in the wild," trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, tr. 119–132.

[15] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi và cộng sự, "Musiclm: Generating music from text," arXiv preprint arXiv:2301.11325, 2023.

[16] S. Doh, K. Choi, J. Lee, và J. Nam, "Lp-musiccaps: Llm-based pseudo music captioning," arXiv preprint arXiv:2307.16372, 2023.

[17] I. Manco, B. Weck, S. Doh, M. Won, Y. Zhang, D. Bodganov, Y. Wu, K. Chen, P. Tovstogan, E. Benetos và cộng sự, "The song describer dataset: a corpus of audio captions for music-and-language evaluation," arXiv preprint arXiv:2311.10057, 2023.

[18] O. Nieto, M. C. McCallum, M. E. Davies, A. Robertson, A. M. Stark, và E. Egozy, "The harmonix set: Beats, downbeats, and functional segment annotations of western popular music." trong ISMIR, 2019, tr. 565–572.

[19] C. Veaux, J. Yamagishi, và S. King, "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database," trong 2013 international conference oriental COCOSDA held jointly with 2013 conference on Asian spoken language research and evaluation (O-COCOSDA/CASLRE). IEEE, 2013, tr. 1–4.

[20] C. Veaux, J. Yamagishi, K. MacDonald và cộng sự, "Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit," 2016.
