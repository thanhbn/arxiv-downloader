# 2410.00344.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/audio/2410.00344.pdf
# Kích thước tệp: 6199255 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TÍCH HỢP CÁC MÔ HÌNH VĂN BẢN-SANG-NHẠC VỚI CÁC MÔ HÌNH NGÔN NGỮ:
SÁNG TÁC CÁC TÁC PHẨM NHẠC DÀI CÓ CẤU TRÚC
Lilac Atassi
Đại học California, San Diego
TÓM TẮT
Các phương pháp tạo nhạc gần đây dựa trên transformer có cửa sổ ngữ cảnh lên đến một phút. Nhạc được tạo ra bởi các phương pháp này phần lớn không có cấu trúc ngoài cửa sổ ngữ cảnh. Với cửa sổ ngữ cảnh dài hơn, việc học các cấu trúc quy mô lớn từ dữ liệu âm nhạc là một vấn đề đầy thách thức không thể thực hiện được. Bài báo này đề xuất tích hợp mô hình văn bản-sang-nhạc với mô hình ngôn ngữ lớn để tạo ra nhạc có hình thức. Bài báo thảo luận về các giải pháp cho những thách thức của việc tích hợp như vậy. Kết quả thí nghiệm cho thấy phương pháp được đề xuất có thể tạo ra nhạc dài 2,5 phút có cấu trúc cao, tổ chức mạnh mẽ và gắn kết.
Thuật ngữ chỉ mục —Văn bản-sang-Nhạc, mô hình tạo sinh, hình thức âm nhạc

1. GIỚI THIỆU
Làn sóng mới của các mô hình tạo sinh đã được khám phá trong tài liệu để tạo ra nhạc. Jukebox [1] dựa trên Hierarchical VQ-VAEs [2] để tạo ra nhạc dài nhiều phút. Jukebox là một trong những mô hình dựa trên học tập thuần túy sớm nhất có thể tạo ra nhạc dài hơn một phút với một mức độ nhất quán cấu trúc nào đó. Đáng chú ý, các tác giả đề cập rằng nhạc được tạo ra ở quy mô nhỏ của nhiều giây là có tính nhất quán, và ở quy mô lớn hơn, ngoài một phút, nó thiếu hình thức âm nhạc.

Music Transformer [3] đã điều chỉnh kiến trúc transformer cho việc tạo nhạc, có khả năng xử lý số lượng token đầu vào tương đương với tối đa một phút nhạc. Các công trình gần đây hơn về transformer và mô hình tự hồi quy đầu tiên sử dụng bộ mã hóa hiệu quả để nén âm thanh. Trong không gian nén, mô hình transformer được huấn luyện để tạo ra các token mới. Sau đó, các token được tạo ra được chuyển đổi thành âm thanh bằng bộ giải mã. MusicGen [4] tuân theo cách tiếp cận này.

Các mô hình tạo sinh trong tài liệu đã được áp dụng cho cả nhạc ký hiệu [5, 6, 7] và nhạc âm thanh [8, 9]. Hầu hết các mô hình lớn hơn gần đây được huấn luyện trên âm thanh [4, 10], vì việc thu thập một tập huấn luyện lớn về âm thanh nhạc có sẵn dễ dàng hơn so với nhạc ký hiệu.

Các công trình trước đó về mô hình nhạc tạo sinh, chẳng hạn như JukeBox, đã cố gắng tối ưu hóa mô hình để xử lý các chuỗi dài nhiều phút với hy vọng rằng mô hình sẽ

Hình 1. Trung bình (hàng trên) và phương sai (hàng dưới) của các ma trận tự tương tự (SS) hợp nhất được ước tính bởi 100 mẫu từ Pond5, được tạo ra bởi phương pháp của chúng tôi, và bởi MusicGen. Các ma trận SS được giảm mẫu xuống 5×5. Kết quả cho thấy rằng, so với MusicGen, phương pháp của chúng tôi tạo ra các mẫu giống với các mẫu Pond5 hơn về tính nhất quán thời gian dài hạn và sự đa dạng của các phần lặp lại.

học được các cấu trúc và hình thức âm nhạc ở tất cả các quy mô. Tuy nhiên, không có mô hình nào trong tài liệu đã chứng minh được cấu trúc âm nhạc ở quy mô lớn và thậm chí thiếu các hình thức âm nhạc đơn giản. Phần 2 trình bày lập luận rằng việc có một tập dữ liệu huấn luyện đủ lớn là không thực tế. Sử dụng Mô hình Ngôn ngữ Lớn (LLMs), bài báo này đề xuất một cách tiếp cận để tạo ra nhạc với các cấu trúc tổ chức ở quy mô lớn hơn. Phần 3 trình bày một đánh giá ngắn gọn về MusicGen. Phần 4 thảo luận về cách tiếp cận được đề xuất để tích hợp MusicGen với ChatGPT [11]. Các thí nghiệm và đánh giá phương pháp của chúng tôi được trình bày trong Phần 5. Phần 6 kết luận bài báo.

2. HỌC HÌNH THỨC ÂM NHẠC
Các mô hình tạo ảnh, chẳng hạn như những mô hình để vẽ bàn tay, gặp khó khăn trong việc tạo ra hình ảnh nhất quán do sự biến đổi rộng lớn trong dữ liệu huấn luyện [12, 13]. Vấn đề này mở rộng đến các cấu trúc khác có mức độ biến đổi cao. Hình 2 minh họa

--- TRANG 2 ---
hai cấu trúc khác (gương và cờ bay phấp phới) với sự biến đổi đáng kể mà ba trình tạo ảnh thương mại (Dall-E 3 [14], Midjourney, và Meta AI) không thể tạo ra hình ảnh nhất quán. Những hình ảnh được tạo ra này hỗ trợ lập luận rằng việc học cấu trúc nhất quán khi có sự biến đổi lớn trong đa tạp dữ liệu là một hạn chế thực tế. Một giải pháp đã được khám phá với kết quả hứa hẹn là giảm sự biến đổi bằng cách điều kiện hóa mô hình trên một tín hiệu bổ sung, ví dụ như sử dụng tư thế bàn tay 3d được dự đoán từ lời nhắc văn bản để tạo ra hình ảnh với bàn tay [12]. Phương pháp của chúng tôi tuân theo một cách tiếp cận tương tự.

Để minh họa vấn đề mà các mô hình tạo sinh gặp khó khăn trong việc học hình thức âm nhạc, hãy xem xét một trường hợp đơn giản. Một mô hình tạo sinh sử dụng ước lượng khả năng tối đa tối ưu hóa θ để ước tính xác suất kết hợp pθ(t1, t2) từ các mẫu dữ liệu với các giá trị rời rạc và hữu hạn. Một mô hình tham số có thể ước tính xác suất kết hợp nếu các mẫu dữ liệu huấn luyện nằm trên một đa tạp compact. Nếu có một lượng lớn biến đổi trong t1 giữa các mẫu, thì pθ(t1, t2) giảm xuống pθ(t2) khi p(t1) trở nên gần như đồng nhất. Với dữ liệu chiều cao, vấn đề của đa tạp dữ liệu không compact được tăng lên với thực tế rằng với một mô hình tốt tùy ý như được đánh giá bằng log-likelihood trung bình, −log[pθ(t1, . . . , td)] ≈ d. Quá trình tối ưu hóa giảm thiểu log-likelihood âm bằng cách điều chỉnh θ cho các chiều có ít biến đổi hơn. Trong một tập hợp nhạc tổng quát, với nhiều tham số thay đổi qua các tác phẩm nhạc có cùng hình thức âm nhạc, lượng biến đổi ở quy mô thời gian lớn để học hình thức âm nhạc đủ lớn đến mức thậm chí các hình thức âm nhạc đơn giản cũng khó học một cách không thể đối với các mô hình tạo sinh.

3. MUSICGEN
Các phương pháp gần đây nhất trong tài liệu để tạo nhạc tuân theo cách tiếp cận của Stable Diffusion [15]. Trong cách tiếp cận này, mô hình tạo sinh được huấn luyện trong không gian tiềm ẩn của một bộ mã hóa. Vector được tạo ra sau đó được giải mã thành âm thanh bằng bộ giải mã tương ứng. Lý do chính để huấn luyện mô hình tạo sinh trong tiềm ẩn nén là để giảm chi phí tính toán của việc tạo ra âm thanh dài. Trong MusicGen, một mô hình tự hồi quy được huấn luyện trong không gian tiềm ẩn định lượng của EnCodec [16] để mô hình hóa nhạc.

Để điều kiện hóa MusicGen trên văn bản, lời nhắc văn bản được mã hóa bởi T5 [17]. Hướng dẫn không có bộ phân loại (CFG) được sử dụng để tạo ra các mẫu với điều kiện văn bản. MusicGen được huấn luyện trên 20K giờ nhạc. Một nửa dữ liệu huấn luyện là riêng tư và nội bộ tại Meta. 10K giờ nhạc huấn luyện còn lại được lấy từ các bộ sưu tập dữ liệu nhạc không có giọng hát ShutterStock7 và Pond58.

6https://chatgpt.com/
6https://midjourney.com/  
6https://meta.ai/
7https://www.shutterstock.com/music
8https://www.pond5.com

4. ĐIỀU KHIỂN MUSICGEN BẰNG MÔ HÌNH NGÔN NGỮ
Vì MusicGen được điều kiện hóa trên văn bản, nó cung cấp một giao diện bằng ngôn ngữ tự nhiên. Do đó, một LLM có thể tạo ra lời nhắc cho MusicGen để thay thế lời nhắc của con người. Vì vậy, có thể giao nhiệm vụ cho một LLM thiết kế cấu trúc của một bài hát và tạo ra lời nhắc cho từng phần để được tạo ra bởi mô hình văn bản-sang-nhạc. Khả năng của LLM trong việc thiết kế cấu trúc nhạc và tạo ra lời nhắc này được hỗ trợ bởi cơ sở kiến thức đa dạng [18], khả năng lý luận [19], và khả năng học [20].

Thách thức đầu tiên là căn chỉnh LLM với mô hình văn bản-sang-nhạc. MusicGen đã được huấn luyện trên các mô tả nhạc ngắn gọn không kỹ thuật và tuân theo một phong cách nhất định. LLM cần tạo ra các lời nhắc mà MusicGen có thể diễn giải. Để bắc cầu cho khoảng cách này, có hai cách tiếp cận chính. Một là tinh chỉnh hoặc huấn luyện một LLM đã được huấn luyện trước, điều này liên quan đến việc điều chỉnh trực tiếp nó cho nhiệm vụ. Cách tiếp cận khác là học trong ngữ cảnh, cung cấp các lợi thế của việc yêu cầu ít mẫu hơn và ít tốn tài nguyên hơn khi thử và đánh giá các lời nhắc khác nhau. Đáng chú ý, việc sử dụng một số lượng lớn mẫu cho học trong ngữ cảnh đã được chứng minh vượt trội hơn tinh chỉnh về độ chính xác [21, 22].

Học trong ngữ cảnh được sử dụng để hướng dẫn ChatGPT tạo ra lời nhắc cho MusicGen bằng cách cung cấp 50 mô tả bài hát từ Pond5. Để tìm số lượng mẫu cần thiết, người ta ước tính tần suất nhạc được tạo ra trung thành với lời nhắc văn bản cho MusicGen. Kết quả thực nghiệm cho thấy rằng với khoảng 10 mô tả bài hát được tạo ra bởi lời nhắc thường bị MusicGen hiểu sai. Tăng số lượng mẫu từ 50 lên 80 không cho thấy cải thiện trong khả năng diễn giải lời nhắc bởi MusicGen. Vì LLMs dễ bị ảo giác hơn [23], nên nên tránh nhiều mẫu hơn cần thiết.

ChatGPT có xu hướng trộn lẫn nhiều thể loại trong lời nhắc cho một tác phẩm duy nhất, dẫn đến các cấu trúc không được thiết kế đặc biệt tốt. Tuy nhiên, khi ChatGPT được hướng dẫn xem xét một khung như lý thuyết ITPRA [24], các cấu trúc kết quả sẽ có tổ chức và nhất quán hơn. Ngoài ra, việc sử dụng cách tiếp cận chuỗi suy nghĩ [25]—yêu cầu nó trước tiên phản hồi với mô tả về một bài hát và hình thức của nó, tiếp theo là tạo ra lời nhắc cho các phần—càng cải thiện tổ chức của cấu trúc âm nhạc. Các quy tắc bổ sung được thêm vào lời nhắc cho ChatGPT để giới thiệu các ràng buộc cụ thể cho MusicGen. Ví dụ, vì MusicGen không thể tạo ra các phần có "nhịp độ chậm hơn" so với phần trước, ChatGPT được hướng dẫn tạo ra lời nhắc, độ dài của mỗi phần tính bằng giây, và các yếu tố âm nhạc từ các phần trước cần được sử dụng, tất cả được định dạng trong JSON. Trong đoạn trích lời nhắc sau đây, một số phần đã được bỏ qua để ngắn gọn.

*Nhiệm vụ* Giả sử bạn là một nhạc sĩ. Nhiệm vụ của bạn là viết lời nhắc văn bản cho một hệ thống tạo ra nhạc dựa trên

--- TRANG 3 ---
Dall-E
 Midjourney
 Meta AI
Hình 2. Minh họa sự không nhất quán trong hình ảnh được tạo ra bởi Dall-E 34, Midjourney5, và Meta AI6. Những sự không nhất quán này rõ ràng trong hình ảnh có gương và cờ bay phấp phới. Chú ý các sọc phân nhánh hoặc hợp nhất trên cờ và các góc phản xạ và tới không nhất quán trong gương, trong số những sự không nhất quán khác.

mô tả đã cho về nhạc...
*Ví dụ đa bước* Dưới đây là một số ví dụ lời nhắc mà hệ thống hiểu, cùng với loại nhạc nó có thể tạo ra:
− "một track EDM nhẹ nhàng và vui tươi với trống có nhịp syncopated, pad thoáng khí, và cảm xúc mạnh mẽ; bpm: 130"
...
*Ràng buộc* Đừng giới hạn bản thân với những ví dụ lời nhắc này... Tác phẩm nhạc phải nhất quán và có cảm giác thống nhất. Mô tả quá trình suy nghĩ của bạn cho việc sáng tác, tiếp theo là phân tích các phần khác nhau... Sau đây là những ràng buộc quan trọng mà lời nhắc của bạn phải thỏa mãn:
1. Toàn bộ tác phẩm phải chính xác 150 giây. Bạn cũng sẽ quyết định độ dài của mỗi phần. 2. Lời nhắc cho mỗi phần có thể tham chiếu phần khác... 14. Để lặp lại một phần với biến thể trong hình thức âm nhạc đã chọn, tham chiếu phần gốc và, trong lời nhắc mới, giải thích điều gì đã thay đổi.
*Yêu cầu Phần 1* Nghĩ ra 10 tác phẩm, bao gồm hình thức và mô tả của mỗi phần.
*Yêu cầu Phần 2* ... Cung cấp chi tiết của các phần cho mỗi tác phẩm ở định dạng JSON: {SỐ PHẦN: ["LỜI NHẮC", ĐỘ DÀI TÍNH BẰNG GIÂY, PHẦN ĐƯỢC THAM CHIẾU], SỐ PHẦN: ["LỜI NHẮC", ĐỘ DÀI TÍNH BẰNG GIÂY, PHẦN ĐƯỢC THAM CHIẾU], ... }

5. ĐÁNH GIÁ
Việc chuyển đổi giữa các lời nhắc được truyền cho MusicGen tạo ra một bước nhảy đột ngột. Để đảm bảo sự chuyển tiếp mượt mà giữa các phần, phương pháp CFG được điều chỉnh. Thay vì ước tính một phân phối xác suất có điều kiện, hai phân phối có điều kiện được ước tính. Trong phân phối xác suất nội suy, trọng số của phân phối có điều kiện đầu tiên trong năm giây từ một xuống không tuyến tính, và trọng số của phân phối có điều kiện thứ hai tăng từ không lên một. Phương pháp này cho phép mô hình tạo ra các token tạo điều kiện cho sự chuyển tiếp liên tục giữa các phần.

Tương tự, để tạo ra các phần với biến thể của các phần trước, một lời nhắc âm thanh 15 giây từ phần trước được cung cấp cho MusicGen. Âm thanh 15 giây này được truyền cho bộ mã hóa của EnCodec để tạo ra các token tương ứng. Sau đó, các token lời nhắc được truyền cho mô hình dự đoán của MusicGen. Trọng số của phân phối xác suất trên các token, có điều kiện trên các token lời nhắc âm thanh này, sau đó được giảm tuyến tính xuống không trong 10 giây.

Hình 3 so sánh các ma trận tự tương tự (SSM) của các mẫu từ MusicGen, phương pháp của chúng tôi, và Pond5. Phương pháp hợp nhất được đề xuất trong [26] được sử dụng để tạo ra các SSMs kết hợp. Các SSMs của mẫu từ phương pháp của chúng tôi giống với cấu trúc biến thể và tương tự trong mẫu từ Pond5.

Sau khi giảm mẫu các ma trận SS hợp nhất của 100 mẫu, các ma trận trung bình và phương sai được ước tính cho Pond5, phương pháp của chúng tôi, và MusicGen trong Hình 1. Do đó, hình này giúp so sánh các phân phối của cấu trúc nhạc. Rõ ràng rằng, từ các giá trị trung bình và biến thể từ phía trên bên phải của các ma trận, các mẫu MusicGen không có các phần gần cuối mỗi tác phẩm giống với các phần gần đầu. Ngược lại, các mẫu từ phương pháp của chúng tôi giống với các mẫu từ Pond5 hơn. Khoảng cách Fréchet giữa hai cặp phân phối được ước tính, nhưng thay vì sử dụng các vector đặc trưng Inception [27], các ma trận trung bình và hiệp phương sai của tam giác trên của các ma trận SS hợp nhất được sử dụng trong ước tính khoảng cách. Các khoảng cách Fréchet

--- TRANG 4 ---
Hình 3. Trực quan hóa các ma trận tự tương tự cho 3 mẫu MusicGen, một mẫu từ phương pháp của chúng tôi và một từ Pond5. Với MusicGen, ở nhiệt độ (T) thấp 0.1, nhạc bị lặp lại. Ở T=5.0, chủ yếu là tiếng ồn ngẫu nhiên. Ở T=1, nhạc uốn lượn. Mẫu từ phương pháp của chúng tôi giống với mẫu từ Pond5, được sáng tác và sắp xếp bởi một nhạc sĩ.

giữa phân phối của các mẫu Pond5 và của chúng tôi là 0.086, và giữa các mẫu Pond5 và MusicGen là 0.108, hỗ trợ tuyên bố rằng cấu trúc của các mẫu được tạo ra bởi phương pháp của chúng tôi giống với các mẫu được sáng tác bởi nhạc sĩ từ Pond5 hơn.

Để đánh giá chủ quan bởi những người không phải nhạc sĩ, 10 mẫu được tạo ra bằng phương pháp của chúng tôi, 10 mẫu bằng MusicGen, và 10 mẫu từ Pond5. Mỗi mẫu này dài 2,5 phút. Sử dụng Amazon Mechanical Turk (MTurk), điểm ý kiến trung bình (MOS) từ 1 đến 5 cho mỗi mẫu từ 10 người không phải nhạc sĩ được thu thập. Những người đánh giá được yêu cầu đánh giá chất lượng tổng thể của nhạc, tuân theo các thực hành được khuyến nghị trong CrowdMOS [28]. Các đối tượng được cho biết điểm 1 có nghĩa là họ không thích nhạc và sẽ không muốn nghe nhạc tương tự. Điểm 5 có nghĩa là họ thấy nhạc thú vị và muốn nghe nhạc tương tự. Với việc các track nhạc dài có cấu trúc tổ chức hơn được kỳ vọng sẽ hấp dẫn hơn, sự yêu thích của các mẫu được sử dụng như một đại diện cho cấu trúc cải thiện

Hình 4. Trái: So sánh chủ quan của nhạc được tạo ra và mẫu từ Pond5 bởi những người không phải nhạc sĩ được đo lường thông qua MOS dựa trên mức độ hấp dẫn của nhạc. Whiskers: 95% CI. Phải: So sánh chủ quan của các mẫu bởi nhạc sĩ, phê bình các cấu trúc âm nhạc.

và hình thức. Kết quả trong Hình 4(trên) cho thấy rằng việc thêm hình thức âm nhạc thông qua phương pháp của chúng tôi vào MusicGen cải thiện chất lượng cảm nhận của nhạc, gần như ngang bằng với các tác phẩm nhạc do con người sáng tác từ Pond5.

Trong một đánh giá chủ quan riêng biệt, ba nhạc sĩ chuyên nghiệp có bằng tiến sĩ về biểu diễn hoặc sáng tác được yêu cầu nghe ba mẫu từ phương pháp của chúng tôi và ba mẫu từ MusicGen. Họ gán một điểm cho mỗi mẫu, với hướng dẫn điểm như sau: 1: Không có hình thức, nhạc uốn lượn; 2: Cấu trúc tối thiểu, một số mẫu có thể nhận biết nhưng phần lớn không có cấu trúc; 3: Cấu trúc vừa phải, các phần rõ ràng nhưng không có tổ chức cao; 4: Hình thức rõ ràng, các phần và chuyển tiếp được tổ chức tốt; 5: Rất rõ ràng và có cấu trúc cao, tổ chức mạnh mẽ và gắn kết. Điểm trung bình được trình bày trong Hình 4(dưới).

6. KẾT LUẬN
Bài báo này lập luận rằng do bản chất được thảo luận của vấn đề, các mô hình tạo nhạc không thể học cấu trúc quy mô lớn hoặc hình thức âm nhạc từ các tập dữ liệu âm nhạc. Sau đó, một phương pháp mới để kết hợp các mô hình tạo nhạc với LLMs để tạo ra nhạc có hình thức âm nhạc được trình bày. Các thách thức kỹ thuật được thảo luận. Các đánh giá chủ quan và khách quan hỗ trợ tuyên bố rằng phương pháp của chúng tôi có thể tạo ra nhạc có cấu trúc tốt dài 2,5 phút.

7. TÀI LIỆU THAM KHẢO
[1] Prafulla Dhariwal, Heewoo Jun, Christine Payne, et al., "Jukebox: A generative model for music," arXiv preprint arXiv:2005.00341, 2020.
[2] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals, "Generating diverse high-fidelity images with vq-vae-2," NeurIPS, vol. 32, 2019.

--- TRANG 5 ---
[3] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, et al., "Music transformer: Generating music with long-term structure," in ICLR, 2018.
[4] Jade Copet, Felix Kreuk, Itai Gat, et al., "Simple and controllable music generation," NeurIPS, vol. 36, 2024.
[5] Hongfei Wang, Yi Zou, Haonan Cheng, and Long Ye, "Diffuseroll: multi-track multi-attribute music generation based on diffusion model," Multimedia Systems, vol. 30, no. 1, pp. 19, 2024.
[6] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon, "Symbolic music generation with diffusion models," arXiv preprint arXiv:2103.16091, 2021.
[7] Jeffrey AT Lupker, "Score-transformer: A deep learning aid for music composition," in NIME 2021, 2021.
[8] Teysir Baoueb, Haocheng Liu, Mathieu Fontaine, et al., "Specdiff-gan: A spectrally-shaped noise diffusion gan for speech and music synthesis," in ICASSP 2024. IEEE, 2024, pp. 986–990.
[9] Curtis Hawthorne, Ian Simon, Adam Roberts, et al., "Multi-instrument music synthesis with spectrogram diffusion," arXiv preprint arXiv:2206.05408, 2022.
[10] Andrea Agostinelli, Timo I Denk, Zalán Borsos, et al., "Musiclm: Generating music from text," arXiv preprint arXiv:2301.11325, 2023.
[11] Josh Achiam, Steven Adler, Sandhini Agarwal, et al., "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.
[12] Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, et al., "Handiffuser: Text-to-image generation with realistic hand appearances," arXiv preprint arXiv:2403.01693, 2024.
[13] Haozhuo Zhang, Bin Zhu, Yu Cao, and Yanbin Hao, "Hand1000: Generating realistic hands from text with only 1,000 images," arXiv preprint arXiv:2408.15461, 2024.
[14] James Betker, Gabriel Goh, Li Jing, et al., "Improving image generation with better captions," Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, vol. 2, no. 3, pp. 8, 2023.
[15] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer, "High-resolution image synthesis with latent diffusion models," in CVPR, 2022, pp. 10684–10695.
[16] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, "High fidelity neural audio compression," arXiv preprint arXiv:2210.13438, 2022.
[17] Colin Raffel, Noam Shazeer, Adam Roberts, et al., "Exploring the limits of transfer learning with a unified text-to-text transformer," JMLR, vol. 21, no. 140, pp. 1–67, 2020.
[18] Lukas Schulze Balhorn, Jana M Weber, Stefan Buijsman, et al., "Empirical assessment of chatgpt's answering capabilities in natural science and engineering," Scientific Reports, vol. 14, no. 1, pp. 4998, 2024.
[19] Taylor Webb, Keith J Holyoak, and Hongjing Lu, "Emergent analogical reasoning in large language models," Nature Human Behaviour, vol. 7, no. 9, pp. 1526–1541, 2023.
[20] Tom Brown, Benjamin Mann, Nick Ryder, et al., "Language models are few-shot learners," NeurIPS, vol. 33, pp. 1877–1901, 2020.
[21] Rishabh Agarwal, Avi Singh, Lei M Zhang, et al., "Many-shot in-context learning," arXiv preprint arXiv:2404.11018, 2024.
[22] Amanda Bertsch, Maor Ivgi, Uri Alon, et al., "In-context learning with long-context models: An in-depth exploration," arXiv preprint arXiv:2405.00200, 2024.
[23] Xi Fang, Weijie Xu, Fiona Anting Tan, et al., "Large language models on tabular data–a survey," arXiv preprint arXiv:2402.17944, 2024.
[24] David Huron, Sweet anticipation: Music and the psychology of expectation, MIT press, 2008.
[25] Jason Wei, Xuezhi Wang, Dale Schuurmans, et al., "Chain-of-thought prompting elicits reasoning in large language models," NeurIPS, vol. 35, pp. 24824–24837, 2022.
[26] Christopher J Tralie and Brian McFee, "Enhanced hierarchical music structure annotations via feature level similarity fusion," in ICASSP. IEEE, 2019, pp. 201–205.
[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," NeurIPS, vol. 30, 2017.
[28] Flávio Ribeiro, Dinei Florêncio, Cha Zhang, and Michael Seltzer, "Crowdmos: An approach for crowdsourcing mean opinion score studies," in ICASSP. IEEE, 2011, pp. 2416–2419.
