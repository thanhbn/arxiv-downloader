DPHuBERT: Chưng cất và tỉa đồng thời các mô hình nhận dạng giọng nói tự giám sát
Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1
1Đại học Carnegie Mellon, Pittsburgh, PA, USA
2Honda Research Institute Japan Co., Ltd., Saitama, Japan
yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad }@jp.honda-ri.com, shinjiw@ieee.org

Tóm tắt
Học tự giám sát (SSL) đã đạt được thành công đáng chú ý trong nhiều tác vụ xử lý giọng nói, nhưng kích thước mô hình lớn và chi phí tính toán nặng cản trở việc triển khai. Chưng cất tri thức huấn luyện một mô hình học sinh nhỏ để bắt chước hành vi của một mô hình thầy lớn. Tuy nhiên, kiến trúc học sinh thường cần được thiết kế thủ công và sẽ cố định trong quá trình huấn luyện, điều này đòi hỏi kiến thức tiên nghiệm và có thể dẫn đến hiệu suất tối ưu dưới mức. Lấy cảm hứng từ thành công gần đây của tỉa có cấu trúc theo tác vụ cụ thể, chúng tôi đề xuất DPHuBERT, một phương pháp nén không phụ thuộc tác vụ mới cho SSL giọng nói dựa trên chưng cất và tỉa đồng thời. Thí nghiệm trên SUPERB cho thấy DPHuBERT vượt trội hơn các phương pháp chưng cất thuần túy trong hầu hết tất cả các tác vụ. Hơn nữa, DPHuBERT yêu cầu ít thời gian huấn luyện và hoạt động tốt với dữ liệu huấn luyện hạn chế, làm cho nó phù hợp với các ứng dụng hạn chế tài nguyên. Phương pháp của chúng tôi cũng có thể được áp dụng cho các mô hình SSL giọng nói khác nhau. Mã và mô hình của chúng tôi sẽ được công khai.
Từ khóa: nén mô hình, chưng cất tri thức, tỉa có cấu trúc, học tự giám sát

1. Giới thiệu
Học biểu diễn giọng nói tự giám sát (SSL giọng nói) đã đạt được kết quả đáng chú ý trong các tác vụ khác nhau [1–10]. Tuy nhiên, các mô hình SSL giọng nói thường lớn và chậm, làm cho chúng không phù hợp với các ứng dụng thực tế có tài nguyên hạn chế. Nén SSL giọng nói đã trở thành một chủ đề quan trọng. Một phương pháp phổ biến là chưng cất tri thức [11], huấn luyện một mô hình học sinh nhỏ để khớp với đầu ra của một mô hình thầy lớn. Các nghiên cứu trước như DistilHuBERT [12] và FitHuBERT [13] đã đạt được kết quả đầy hứa hẹn với các mô hình học sinh khác nhau. Một công trình khác [14] cho thấy kiến trúc học sinh ảnh hưởng đáng kể đến hiệu suất của nó, ngay cả khi kích thước mô hình tương tự. Tuy nhiên, trong các phương pháp chưng cất, kiến trúc học sinh được xác định trước và không thay đổi, điều này cần chuyên môn đặc biệt và có thể dẫn đến kết quả tối ưu dưới mức. Ngược lại, tỉa [15, 16] tự động khám phá một mạng con nhỏ gọn từ một mô hình lớn, điều này đã được khám phá trong xử lý ngôn ngữ tự nhiên (NLP) [17–20] và xử lý giọng nói [21–25]. Các phương pháp tỉa trước đây cho SSL giọng nói tập trung vào các tác vụ hạ nguồn cụ thể như nhận dạng giọng nói tự động (ASR) [24,25] và hiểu ngôn ngữ nói (SLU) [25]. Không rõ chúng sẽ hoạt động như thế nào trong nén không phụ thuộc tác vụ, điều này thách thức hơn vì mô hình cần nắm bắt các khía cạnh khác nhau của giọng nói bao gồm nội dung, người nói, ngữ nghĩa và cận ngôn ngữ [6].

Trong nghiên cứu này, chúng tôi đề xuất DPHuBERT, một phương pháp nén không phụ thuộc tác vụ dựa trên Chưng cất và Tỉa đồng thời. Nó cho phép kiến trúc học sinh được học trong quá trình chưng cất. Thí nghiệm trên SUPERB [6] cho thấy phương pháp của chúng tôi vượt trội hơn các phương pháp chưng cất trước đây trong hầu hết tất cả các tác vụ. Phương pháp của chúng tôi cũng hoạt động tốt cho các mô hình SSL giọng nói khác nhau như HuBERT Base [2], WavLM Base+ [4] và HuBERT Large [2], ngay cả với tài nguyên huấn luyện hạn chế. Chúng tôi sẽ gửi kết quả của mình lên bảng xếp hạng SUPERB và phát hành mã cũng như mô hình công khai để có thể tái tạo: https://github.com/pyf98/DPHuBERT .

2. Bối cảnh và công trình liên quan

2.1. Kiến trúc của SSL giọng nói
Các mô hình SSL giọng nói như wav2vec 2.0 [1], HuBERT [2] và WavLM [4] có kiến trúc tương tự. Mô hình bao gồm một bộ trích xuất đặc trưng tích chập (CNN) và một bộ mã hóa Transformer [26]. CNN có bảy phép tích chập thời gian với chuẩn hóa và kích hoạt. Bộ mã hóa Transformer chứa 12 lớp với kích thước ẩn 768 cho các mô hình cơ sở và 24 lớp với kích thước ẩn 1024 cho các mô hình lớn. Mỗi lớp bao gồm một tự chú ý đa đầu (MHA) và một mạng feed-forward theo vị trí (FFN).

2.2. Phương pháp nén cho SSL giọng nói
Chưng cất. Các phương pháp chưng cất tối ưu hóa một mô hình học sinh nhỏ để khớp với các mục tiêu được tạo ra bởi một mô hình thầy lớn. Vì các lớp khác nhau của SSL giọng nói nắm bắt thông tin khác nhau [27], mô hình học sinh cần học cả biểu diễn cuối cùng và trung gian của thầy [12–14]. DistilHuBERT [12] huấn luyện một mô hình học sinh nông bằng cách ánh xạ lớp học sinh cuối cùng đến nhiều lớp thầy trung gian. FitHuBERT [13] học một mô hình học sinh sâu và mỏng thông qua ánh xạ lớp-đến-lớp. Một công trình khác [14] so sánh chưng cất lớp dự đoán và lớp-đến-lớp sử dụng các kiến trúc học sinh khác nhau. Nó cho thấy kiến trúc của mô hình học sinh ảnh hưởng đến hiệu suất của nó, ngay cả khi kích thước mô hình được giữ tương tự. Nó cũng phát hiện rằng các mạng sâu hơn hoạt động tốt hơn với chưng cất lớp-đến-lớp, có thể vì nó căn chỉnh rõ ràng các lớp trung gian. Những quan sát này đã truyền cảm hứng cho công trình của chúng tôi cho phép kiến trúc học sinh phát triển trong quá trình chưng cất.

Tỉa. Các phương pháp tỉa xác định và loại bỏ các tham số dư thừa từ một mô hình được huấn luyện trước. Tỉa không có cấu trúc loại bỏ các tham số riêng lẻ (ví dụ, một kết nối giữa các nơ-ron) bằng cách đặt chúng bằng không, điều này đòi hỏi thư viện tính toán ma trận thưa để đạt được tăng tốc thực tế, trong khi tỉa có cấu trúc loại bỏ các nhóm tham số (ví dụ, một đầu chú ý hoặc thậm chí cả một lớp), điều này trực tiếp giảm kích thước mô hình và chi phí tính toán. Đối với SSL giọng nói, PARP [24] là một phương pháp tỉa không có cấu trúc dựa trên độ lớn tỉa bộ mã hóa Transformer. Nó cải thiện các tác vụ hạ nguồn như ASR. HJ-Pruning [25] là một phương pháp tỉa có cấu trúc tỉa đồng thời các thành phần không đồng nhất (tức là CNN và Transformer) của các mô hình SSL giọng nói. Nó giảm đáng kể tổng tính toán trong khi vẫn giữ hiệu suất tốt trong ASR và SLU. Các phương pháp này xử lý các tác vụ hạ nguồn cụ thể, nhưng không điều tra các biểu diễn giọng nói phổ quát. Nghiên cứu của chúng tôi tập trung vào tỉa có cấu trúc không phụ thuộc tác vụ của SSL giọng nói. Vì không có dữ liệu có nhãn cho huấn luyện giám sát bình thường, chúng tôi sử dụng một mục tiêu chưng cất cùng với tỉa.

Huấn luyện một-cho-tất. Các phương pháp nén thường tạo ra một mô hình duy nhất với kích thước được xác định trước. LightHuBERT [28] triển khai huấn luyện một-cho-tất [29] để có được nhiều mạng con chia sẻ trọng số, cho thấy hiệu suất rất mạnh trên SUPERB [6]. Tuy nhiên, nó đòi hỏi một quá trình huấn luyện hai giai đoạn đắt đỏ và một mất mát chưng cất tiên tiến lấy cảm hứng từ data2vec [30]. Theo các tác giả, nén HuBERT Base đã mất 2k giờ GPU (tức là 62 giờ với 32 V100 GPU và 19 giờ với 8 GPU cho hai giai đoạn, tương ứng), điều này quá cao đối với các nhà nghiên cứu học thuật và doanh nghiệp nhỏ. Không giống LightHuBERT, nghiên cứu của chúng tôi nhằm nén một mô hình SSL giọng nói hiện có đến một tỷ lệ thưa cụ thể trong một khoảng thời gian huấn luyện có thể quản lý được, điều này phù hợp với thiết lập tiêu chuẩn của các phương pháp chưng cất trước đây [12, 13].

3. DPHuBERT

3.1. Quy trình huấn luyện
Hình 1 minh họa quy trình huấn luyện của chúng tôi bao gồm hai bước. Trong Bước 1, mô hình học sinh được khởi tạo từ thầy và được chưng cất và tỉa đồng thời để tạo ra một mô hình nhỏ hơn với kích thước được xác định trước. Trong Bước 2, mô hình học sinh đã được tỉa được chưng cất thêm để cải thiện hiệu suất. Trong cả hai bước, chỉ sử dụng dữ liệu giọng nói không có nhãn và thầy được đóng băng.

3.2. Mất mát chưng cất
Không giống DistilHuBERT [12], chúng tôi sử dụng chưng cất lớp-đến-lớp vì học sinh ban đầu có cùng độ sâu với thầy (xem Mục 2.2 để thảo luận). Giả sử thầy có Ntea lớp Transformer với kích thước ẩn dtea và học sinh có Nstu lớp với kích thước ẩn dstu. Gọi Xtea_i (hình dạng T×dtea) và Xstu_i (hình dạng T×dstu) là các chuỗi đầu ra của các lớp Transformer thứ i từ thầy và học sinh, tương ứng, trong đó T là độ dài chuỗi. Mất mát chưng cất là:

Ldis = Σ(i∈S) L(Xtea_i, Xstu_i Wi),                    (1)

trong đó S là một tập hợp các lớp để khớp giữa mô hình thầy và học sinh sau một phép chiếu tuyến tính Wi. Chúng tôi sử dụng S={0,4,8,12} cho các mô hình cơ sở và {0,8,16,24} cho các mô hình lớn. Lớp thứ 0 là đầu ra của CNN, cũng là đầu vào cho lớp Transformer đầu tiên. Hàm mất mát L đo sự khác biệt giữa hai chuỗi đặc trưng, có thể là khoảng cách L1, L2 hoặc cosine [12–14]. Chúng tôi tuân theo [12,14] để sử dụng kết hợp của khoảng cách L1 và cosine với trọng số bằng nhau.

3.3. Chưng cất và tỉa có cấu trúc đồng thời
Tỉa có cấu trúc của mô hình học sinh được công thức hóa như học một mô hình thưa thông qua điều chuẩn L0 [31], điều này đã được khám phá trong NLP [19,20] và giọng nói [25]. Phương pháp sẽ được giới thiệu ngắn gọn dưới đây. Để có các dẫn xuất toàn diện hơn, vui lòng tham khảo nghiên cứu trước [19, 20, 25, 31]. Xét một mô hình thầy đóng băng ftea(·) và một mô hình học sinh fstu(·;θ) với các tham số có thể học θ={θj}n_j=1. Mỗi θj là một nhóm tham số có thể tỉa (bao gồm các kênh tích chập, đầu chú ý và đơn vị trung gian FFN) và có n nhóm tổng cộng. Chúng tôi định nghĩa một biến nhị phân zj (gọi là mặt nạ) cho mỗi θj. Các mặt nạ z tuân theo phân phối xác suất q(z;α) với tham số α. Mục tiêu chưng cất được điều chuẩn là:

min(θ,α) Ez∼q[1/D Σ(k=1 to D) Ldis(ftea(xk), fstu(xk;θ̃)) + λ||θ̃||0],     (2)

trong đó θ̃={θ̃j}n_j=1 và mỗi θ̃j=θjzj. Tập dữ liệu không có nhãn với D mẫu là {xk}D_k=1. λ > 0 là trọng số điều chuẩn. Việc giải Phương trình (2) bằng descent gradient là không khả thi do tính chất rời rạc của mặt nạ z. Để làm cho mất mát có thể vi phân, Louizos et al. đề xuất một kỹ thuật tham số hóa lại lấy mẫu z từ phân phối Hard Concrete [31]:

u ∼ U(0,1), v(α) = sigmoid((log u/(1-u) + log α)/β),
v̄(α) = (r-l)·v(α) + l, z = min(1, max(0, v̄(α))),     (3)

trong đó u tuân theo phân phối đều trong [0,1]. β là một hằng số. l < 0 và r > 0 là hai hằng số để kéo dài v đến [l, r], và nó được cắt thêm đến [0,1]. Chỉ α={αj}n_j=1 là các tham số có thể học trong phân phối này. Với kỹ thuật này, mục tiêu trong Phương trình (2) có thể vi phân và thuật ngữ điều chuẩn có biểu thức dạng đóng [31]:

Ez∼q[||θ̃||0] = Σ(j=1 to n) sigmoid(log αj - β log(-l/r)),     (4)

điều này biểu diễn kích thước mô hình (kỳ vọng) như một hàm có thể vi phân của các tham số hiện tại α.

Bây giờ Phương trình (2) có thể được giải để học một mạng con thưa, nhưng độ thưa cuối cùng không thể được kiểm soát chính xác [19, 20]. Để kiểm soát rõ ràng kích thước mô hình cuối cùng, các nghiên cứu trước [19, 20, 25] viết lại bài toán tối ưu hóa với một ràng buộc đẳng thức:

min(θ,α) Ez∼q[1/D Σ(k=1 to D) Ldis(ftea(xk), fstu(xk;θ̃))]
s.t. s(α) = t,     (5)

trong đó s(α) là độ thưa hiện tại (phần trăm tham số được tỉa) của mô hình học sinh và t là độ thưa mục tiêu được xác định trước. Lưu ý rằng s(α) có thể được tính toán dựa trên Phương trình (4) vì chuẩn L0 đếm các tham số còn lại. Mục tiêu tối ưu hóa trong Phương trình (5) có thể được chuyển đổi thêm thành một bài toán minimax sử dụng Lagrangian tăng cường [19]:

max(λ1,λ2) min(θ,α) Ez∼q[1/D Σ(k=1 to D) Ldis(ftea(xk), fstu(xk;θ̃))]
+ λ1·(s(α)-t) + λ2·(s(α)-t)²,     (6)

trong đó λ1, λ2 ∈ R là các nhân tử Lagrange. Thuật ngữ bổ sung này phạt mất mát chưng cất và buộc mô hình học sinh đáp ứng độ thưa mục tiêu của chúng tôi. Phương trình (6) là mục tiêu huấn luyện của chúng tôi cho Bước 1 (Hình 1a). Đối với Bước 2 (Hình 1b), mục tiêu chỉ đơn giản là tối thiểu hóa mất mát chưng cất trong Phương trình (1) mà không có bất kỳ ràng buộc nào vì kiến trúc học sinh đã được cố định.

4. Thí nghiệm

4.1. Thiết lập thí nghiệm
Bộ công cụ. Phương pháp của chúng tôi được triển khai với PyTorch [32] và TorchAudio [33]. Các mô hình SSL được huấn luyện trước được tải xuống từ fairseq [34] hoặc Hugging Face [35].
Dữ liệu. LibriSpeech 960h không có nhãn [36] được sử dụng theo mặc định. Trong Mục 4.2 và Bảng 1, tập con train-clean 100h cũng được sử dụng để điều tra ảnh hưởng của kích thước dữ liệu huấn luyện.
Mô hình. Trong thiết lập mặc định, chúng tôi nén HuBERT Base [2]. Để xác minh tính tổng quát, chúng tôi cũng nén WavLM Base+ [4] trong Mục 4.2 và HuBERT Large [2] trong Mục 4.5.
Huấn luyện. DPHuBERT được huấn luyện trên 4 GPU NVIDIA A100 (40GB) với 640 giây âm thanh mỗi mini-batch. Trong Bước 1, tốc độ học tối đa của các tham số chính θ và các tham số phụ α,λ lần lượt là 2e-4 và 2e-2. Các bước khởi động và tổng số lần lượt là 15k và 50k. Độ thưa mục tiêu t được tăng tuyến tính đến giá trị mong muốn trong 5k bước, điều này tạo điều kiện cho huấn luyện [25]. Trong Bước 2, tốc độ học tối đa là 1e-4. Các bước khởi động và tổng số lần lượt là 5k và 25k. Trong thiết lập mặc định, tổng thời gian huấn luyện của DPHuBERT chỉ là 6 giờ, tức là 24 giờ GPU (do chúng tôi sử dụng 4 GPU).
Đánh giá. Bộ benchmark SUPERB [6] bao gồm 10 tác vụ: phát hiện từ khóa (KS), phân loại ý định (IC), nhận dạng âm vị (PR), ASR, nhận dạng cảm xúc (ER), truy vấn theo ví dụ (QbE), điền vào chỗ trống (SF), nhận dạng người nói (SID), xác minh người nói tự động (ASV) và phân tách người nói (SD). Chúng tôi tuân theo các cấu hình mặc định của họ trong tất cả các tác vụ trừ SID sử dụng tốc độ học 5e-3.

4.2. Kết quả chính
Bảng 1 so sánh các phương pháp khác nhau trên SUPERB [6]. DPHuBERT được nén từ HuBERT Base. Với LibriSpeech 960h, DPHuBERT vượt trội hơn các phương pháp chưng cất thuần túy (bao gồm DistilHuBERT [12], FitHuBERT [13], FitW2V2 [13] và hai mô hình hoạt động tốt nhất từ [14]) trong 8 trên 10 tác vụ. Điều này cho thấy DPHuBERT bảo tồn tốt hơn các biểu diễn giọng nói tổng quát của mô hình thầy, bao gồm nội dung, người nói và ngữ nghĩa. Với chỉ 100h dữ liệu huấn luyện, DPHuBERT vẫn hoạt động tốt hơn nhiều so với các phương pháp trước đây trong hầu hết tất cả các tác vụ. Đáng ngạc nhiên, DPHuBERT sử dụng 100h đã vượt trội hơn các mô hình chưng cất trước đây sử dụng 960h trong IC, PR, QbE và SD, và có kết quả tương tự trong các tác vụ khác. Điều này cho thấy DPHuBERT học các biểu diễn mạnh mẽ ngay cả từ dữ liệu hạn chế.

Chúng tôi cũng đã nén WavLM Base+ để có được DPWavLM. So với DPHuBERT, DPWavLM đạt được cải thiện thêm trong 8 tác vụ. Điều này là do WavLM Base+ chưa tỉa tốt hơn HuBERT Base chưa tỉa. Những kết quả này chứng minh rằng phương pháp nén của chúng tôi có thể được áp dụng cho các mô hình SSL giọng nói khác nhau.

Hình 2 cho thấy kiến trúc của DPHuBERT, được tự động khám phá bởi tỉa có cấu trúc. Đối với CNN, các lớp đầu tiên và cuối cùng được tỉa nhiều nhất. Đối với MHA, ba lớp cao hơn bị loại bỏ hoàn toàn, cho thấy những lớp đó dư thừa hơn. Kết quả của chúng tôi phù hợp với các nghiên cứu trước về tỉa [20, 25] hoặc bộ mã hóa giọng nói nói chung [37–40]. Đối với FFN, các lớp thứ 4, 8 và 12 được bảo tồn nhiều hơn so với các lớp lân cận, vì những lớp đó được khớp rõ ràng giữa mô hình thầy và học sinh như được định nghĩa bởi Phương trình (1) trong Mục 3.2.

4.3. Nghiên cứu khử bỏ
Bảng 2 tóm tắt kết quả của các nghiên cứu khử bỏ sau đây.
Huấn luyện hai bước. DPHuBERT có hai bước huấn luyện (Mục 3.1 và Hình 1). Mô hình được tỉa sau Bước 1 mà không có Bước 2 được đánh giá trong hàng thứ hai của Bảng 2. Nó tệ hơn DPHuBERT trong tất cả các tác vụ, xác minh sự cần thiết của Bước 2. Điều này là do Bước 1 tối ưu hóa Phương trình (6) trong đó thuật ngữ điều chuẩn cạnh tranh với mất mát chưng cất để đáp ứng độ thưa mục tiêu, trong khi Bước 2 trực tiếp tối ưu hóa mất mát chưng cất để cải thiện các biểu diễn đã học của học sinh.

Phương pháp chưng cất. Như đã thảo luận trong Mục 2.2, DPHuBERT sử dụng chưng cất lớp-đến-lớp thay vì chưng cất lớp dự đoán trong DistilHuBERT [12]. Hàng thứ ba của Bảng 2 cho thấy chưng cất lớp dự đoán gây ra suy giảm nghiêm trọng trong tất cả các tác vụ, có thể do kiến trúc học sinh sâu. Khớp trực tiếp các lớp trung gian tạo điều kiện cho việc huấn luyện các học sinh sâu như đã tìm thấy trong [14].

Đơn vị tỉa. DPHuBERT tỉa cả CNN và Transformer vì CNN có chi phí tính toán cao [25, 41]. Hàng thứ tư của Bảng 2 cho thấy kết quả mà không tỉa CNN (tức là chỉ tỉa các đầu chú ý và kích thước trung gian FFN). Mô hình này (hơi) tệ hơn thiết lập mặc định trong 7/10 tác vụ. Điều này xác minh rằng CNN cũng có các thành phần dư thừa có thể được tỉa, như đã báo cáo trong [25, 41, 42].

4.4. Kết quả ở các độ thưa khác nhau
Chúng tôi huấn luyện DPHuBERT với các độ thưa mục tiêu khác nhau (t trong Phương trình (5) (6)) và cho thấy kết quả trong Hình 3. Đối với IC và SID, phương pháp của chúng tôi có thể giảm đáng kể kích thước mô hình trong khi giữ độ chính xác tương tự như HuBERT Base gốc. Đối với ASR, sự suy giảm nghiêm trọng hơn, có thể vì tác vụ biến đổi chuỗi thách thức hơn so với các tác vụ phân loại.

4.5. Nén HuBERT Large
Phương pháp của chúng tôi có thể được áp dụng cho các mô hình SSL giọng nói lớn hơn với chi phí huấn luyện rất hạn chế. Trong Bảng 3, HuBERT Large được nén để có kích thước tương tự như HuBERT Base, điều này chỉ mất khoảng 60 giờ GPU. Mô hình được nén thậm chí vượt trội hơn HuBERT Base trong một số tác vụ như PR, SF-CER và SID. Nó tệ hơn HuBERT Base trong KS, QbE và ASV, nhưng mô hình thầy, HuBERT Large, cũng rõ ràng tệ hơn HuBERT Base trong những tác vụ đó.

5. Kết luận
Nghiên cứu này đề xuất DPHuBERT, một phương pháp nén không phụ thuộc tác vụ dựa trên chưng cất và tỉa có cấu trúc đồng thời. DPHuBERT vượt trội hơn các phương pháp chưng cất trước đây trong hầu hết các tác vụ của SUPERB. Các phân tích toàn diện được trình bày để điều tra hiệu suất của nó với ít dữ liệu huấn luyện hơn hoặc ở các tỷ lệ thưa khác nhau. Ngoài HuBERT Base, phương pháp của chúng tôi có thể được áp dụng trực tiếp cho các mô hình SSL giọng nói khác như WavLM và HuBERT Large trong khi vẫn hiệu quả và có hiệu lực. Trong tương lai, chúng tôi sẽ khám phá các mục tiêu chưng cất tinh vi hơn (ví dụ, mất mát chưng cất dựa trên masking được sử dụng trong LightHuBERT [28]) để cải thiện thêm hiệu suất.

6. Lời cảm ơn
Chúng tôi sử dụng PSC Bridges2 và NCSA Delta thông qua phân bổ ACCESS CIS210014, được hỗ trợ bởi các grant của Quỹ Khoa học Quốc gia #2138259, #2138286, #2138307, #2137603, và #2138296.

7. Tài liệu tham khảo
[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," in Proc. NeurIPS, 2020.
[2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units," IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021.
[3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale," in Proc. Interspeech, 2022.
[4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., "Wavlm: Large-scale self-supervised pre-training for full stack speech processing," IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505–1518, 2022.
[5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, "Unsupervised speech recognition," in Proc. NeurIPS, 2021.
[6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, "SUPERB: Speech Processing Universal PERformance Benchmark," in Proc. Interspeech, 2021.
[7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe, T. N. Sainath, and S. Watanabe, "Self-supervised speech representation learning: A review," IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1179–1210, 2022.
[8] X. Chang, T. Maekaku, P. Guo, J. Shi, Y.-J. Lu, A. S. Subramanian, T. Wang, S.-w. Yang, Y. Tsao, H.-y. Lee et al., "An exploration of self-supervised pretrained representations for end-to-end speech recognition," in Proc. ASRU, 2021.
[9] Z. Huang, S. Watanabe, S.-w. Yang, P. García, and S. Khudanpur, "Investigating Self-Supervised Learning for Speech Enhancement and Separation," in Proc. ICASSP, 2022.
[10] Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, S. Dalmia, X. Chang, and S. Watanabe, "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding," in Proc. SLT, 2022.
[11] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," arXiv:1503.02531, 2015.
[12] H.-J. Chang, S.-w. Yang, and H.-y. Lee, "DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT," in Proc. ICASSP, 2022.
[13] Y. Lee, K. Jang, J. Goo, Y. Jung, and H. R. Kim, "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models," in Proc. Interspeech, 2022.
[14] T. Ashihara, T. Moriya, K. Matsuura, and T. Tanaka, "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models," in Proc. Interspeech, 2022.
[15] R. Reed, "Pruning algorithms-a survey," IEEE Trans. on Neural Networks, vol. 4, no. 5, pp. 740–747, 1993.
[16] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, "Rethinking the value of network pruning," in Proc. ICLR, 2019.
[17] P. Michel, O. Levy, and G. Neubig, "Are sixteen heads really better than one?" in Proc. NeurIPS, 2019.
[18] A. Fan, E. Grave, and A. Joulin, "Reducing transformer depth on demand with structured dropout," in Proc. ICLR, 2020.
[19] Z. Wang, J. Wohlwend, and T. Lei, "Structured Pruning of Large Language Models," in Proc. EMNLP, 2020.
[20] M. Xia, Z. Zhong, and D. Chen, "Structured Pruning Learns Compact and Accurate Models," in Proc. ACL, 2022.
[21] P. Dong, S. Wang, W. Niu et al., "Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition," in ACM/IEEE Design Automation Conference (DAC), 2020.
[22] S. Wang, P. Lin, R. Hu et al., "Acceleration of LSTM With Structured Pruning Method on FPGA," IEEE Access, 2019.
[23] K. Tan and D. Wang, "Compressing Deep Neural Networks for Efficient Speech Enhancement," in Proc. ICASSP, 2021.
[24] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao, Y.-S. Chuang, K. Qian, S. Khurana, D. Cox, and J. Glass, "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition," in Proc. NeurIPS, 2021.
[25] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, "Structured Pruning of Self-Supervised Pre-trained Models for Speech Recognition and Understanding," in Proc. ICASSP, 2023.
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in Proc. NeurIPS, 2017.
[27] A. Pasad, J.-C. Chou, and K. Livescu, "Layer-Wise Analysis of a Self-Supervised Speech Representation Model," in Proc. ASRU, 2021.
[28] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, and H. Li, "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT," in Proc. Interspeech, 2022.
[29] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, "Once-for-all: Train one network and specialize it for efficient deployment," in Proc. ICLR, 2020.
[30] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, "Data2vec: A general framework for self-supervised learning in speech, vision and language," in Proc. ICML, 2022.
[31] C. Louizos, M. Welling, and D. P. Kingma, "Learning Sparse Neural Networks through L0 Regularization," in ICLR, 2018.
[32] A. Paszke et al., "Pytorch: An imperative style, high-performance deep learning library," Proc. NeurIPS, 2019.
[33] Y.-Y. Yang et al., "Torchaudio: Building blocks for audio and speech processing," arXiv:2110.15018, 2021.
[34] M. Ott et al., "fairseq: A fast, extensible toolkit for sequence modeling," in Proc. NAACL-HLT: Demonstrations, 2019.
[35] T. Wolf et al., "Huggingface's transformers: State-of-the-art natural language processing," arXiv preprint arXiv:1910.03771, 2019.
[36] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: An ASR corpus based on public domain audio books," in Proc. ICASSP, 2015.
[37] S. Zhang, E. Loweimi, P. Bell, and S. Renals, "On the usefulness of self-attention for automatic speech recognition with transformers," in Proc. SLT, 2021.
[38] K. Shim, J. Choi, and W. Sung, "Understanding the role of self attention for efficient speech recognition," in Proc. ICLR, 2022.
[39] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," in Proc. ICML, 2022.
[40] T. Maekaku, Y. Fujita, Y. Peng, and S. Watanabe, "Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR," in Proc. Interspeech, 2022.
[41] T.-Q. Lin, H.-y. Lee, and H. Tang, "MelHuBERT: A simplified HuBERT on Mel spectrogram," arXiv:2211.09944, 2022.
[42] F. Wu, K. Kim, J. Pan, K. J. Han, K. Q. Weinberger, and Y. Artzi, "Performance-efficiency trade-offs in unsupervised pre-training for speech recognition," in Proc. ICASSP, 2022.
