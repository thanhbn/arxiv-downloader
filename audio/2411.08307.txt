# 2411.08307.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/audio/2411.08307.pdf
# File size: 491383 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Perceiver S: A Multi- Scale Perceiver with Effective
Segmentation for Long-Term Expressive Symbolic
Music Generation
Yungang Yi∗, Weihua Li∗, Matthew Kuo∗, Quan Bai†
∗Auckland University of Technology, Auckland, New Zealand
Email: luke@waye.com, weihua.li@aut.ac.nz, matthew.kuo@aut.ac.nz
†University of Tasmania, Tasmania, Australia
Email: quan.bai@utas.edu.au
Abstract —AI-based music generation has progressed signifi-
cantly in recent years. However, creating symbolic music that
is both long-structured and expressive remains a considerable
challenge. In this paper, we propose Perceiver S(Segmentation
andScale), a novel architecture designed to address this issue by
leveraging both Effective Segmentation and Multi- Scale attention
mechanisms. Our approach enhances symbolic music generation
by simultaneously learning long-term structural dependencies
and short-term expressive details. By combining cross-attention
and self-attention in a Multi- Scale setting, Perceiver Scaptures
long-range musical structure while preserving musical diversity.
The proposed model has been evaluated using the Maestro
dataset and has demonstrated improvements in generating music
of conventional length with expressive nuances. The project
demos and the generated music samples can be accessed through
the link: https://perceivers.github.io.
Index Terms —Symbolic Music Generation, Long-Term Struc-
ture, Transformer Models, Cross-Attention, Self-Attention, Per-
ceiverS, Effective Segmentation, Multi-Scale Attention
I. I NTRODUCTION
Recent advancements in AI-based music generation, espe-
cially in audio generation models such as AudioLDM [1],
MusicGen [2], and Jen-1 [3], have demonstrated significant
progress capable of generating highly natural-sounding music.
Compared to audio generation, symbolic music offers a further
level of abstraction, making it easier for machine learning
models to capture deeper musical characteristics and under-
standing. Symbolic music plays a crucial role in the field of
music generation, particularly due to its editable nature. This
allows for operations such as cutting and rearranging different
sections or substituting instrument timbres, enabling human
involvement in high-quality music production during the post-
processing stage.
However, symbolic music generation faces two key chal-
lenges. First , the most expressive datasets, recorded from
live performances and recording studios, are seldom used
compared to manually created MIDI-file datasets. The primary
reason is that they lack detailed annotations, making it harder
for models to learn their complex structures. Furthermore,
due to computational limitations, previous models cannot fully
capture the context of an entire piece of music. Techniques,
such as chunking and quantization , are often employed toreduce computational complexity, leading to the loss of crucial
musical details and making it difficult for models to grasp the
full structure of a composition. Second , other approaches that
use abstract structural representations as conditions for music
generation tasks have enabled the generation of structured
music. However, such methods rely heavily on handcrafted
feature engineering. Our objective is to design and develop a
model that is capable of learning the long-range dependencies
in music without relying on explicit structural annotations.
The emergence of Transformer Attention technologies, such
asPerceiver AR [4], has made it possible to access much
longer contextual dependencies. It allows for the simultaneous
learning of musical structure and the generation of expressive
performances. Perceiver AR has demonstrated the ability to
attend to a context length of up to 32,768 tokens using the
Maestro dataset [5], where the query in cross-attention attends
to significantly longer key/value pairs [4]. However, the
causal mask , when applied with the default input sequence
segmentation, does not fully conceal tokens that should not be
visible during auto-regressive training and generation, which
ultimately degrades the quality of the generated music. Addi-
tionally, when using ultra-long context as a single condition,
the model tends to generate identical or similar repetitive
segments as the sequence length increases due to issues with
high similarity in the context of neighboring tokens, leading
to a high token autocorrelation [6] tendency.
In this paper, we propose Perceiver S, a novel model that
addresses the causal masking issue by incorporating Effective
Segmentation. Additionally, Perceiver Semploys Multi- Scale
attention to mitigate the high token autocorrelation problem
that arises from relying solely on long-range dependencies.
Specifically, by adjusting the input sequence segmentation to
start from the head segment with an effective causal mask and
aggressively increasing the segment length up to the maximum
input sequence length, we resolve the learning limitations
caused by the causal mask in Perceiver AR [4]. Additionally,
by incorporating Multi-Scale masks across multiple layers of
cross-attention, the model considers both ultra-long and short-
range attention simultaneously. This approach addresses the
limitation in Perceiver AR, which focuses solely on long-arXiv:2411.08307v2  [cs.AI]  4 Dec 2024

--- PAGE 2 ---
range attention [4]. Different from Perceiver AR, our approach
enhances symbolic music generation by effectively capturing
long-term structural dependencies while simultaneously focus-
ing on short-term variations. Through improved segmentation
and multi-scale attention mechanisms, Perceiver Sgenerates
coherent, diverse music without relying on extensive structural
annotations. Extensive experiments have been conducted to
evaluate the performance of the proposed Perceiver S. The ex-
perimental results demonstrate an average 40% improvement
in Overlap Area when measured against the original training
dataset, highlighting a substantial advantage of our approach
over Perceiver AR [4] in generating high-quality symbolic
music.
II. R ELATED WORKS
A. Capturing Music Expressiveness
The most significant early work in this area came from
Google’s team, which introduced Music Transformer [7],
a model capable of generating expressive piano music using
the Piano-e-Competition dataset, later known as the Maestro
dataset [5]. Since this model was trained using MIDI files
recorded from live piano performances, it utilized the attention
mechanism to focus on the context of all previously generated
tokens when predicting the next one, allowing it to generate
highly detailed and expressive music.
However, due to the quadratic complexity O(n2)of the
Transformer attention mechanism, it could only generate mu-
sic lasting for tens of seconds, but not for several minutes.
Unlike Music Transformer [7], few other models utilize per-
formance datasets, primarily due to the lack of annotations
associated with these types of datasets and the computational
limitations involved in processing them.
1) Dataset Selection: A key factor in generating
production-quality music lies in the selection of datasets. The
Maestro dataset [5], consisting of real human performances,
offers dynamic and expressive recording music. Datasets
recorded from live performances are rare, but there are
quite a few Automatic Music Transcription (AMT) datasets,
including GiantMIDI [8], ATEPP [9], PiJAMA [10], and
others. Advanced models have been developed, from
Hawthorne [11] to Kong [8], that convert audio into MIDI
files. These advances have made it possible to use a vast
amount of recorded audio music to train models, as the
development of symbolic music models has long been
constrained by the limited availability of datasets.
Manually created MIDI datasets, like LAKH [12], provide
valuable human-annotated information, e.g., beats, bars, and
phrases, which allows for flexible segmentation and richer
feature extraction but lack expressive nuances found in live
performances, such as dynamics, tempo variations, and subtle
timing shifts. Other attempts, such as ASAP [13] with human-
assisted beat correction, and advancements in automatic beat
tracking, such as Beat This! [14], aim to bring annotation to
live-recorded datasets, though accuracy limitations still present
challenges.2) Computational Limitations: Although using MIDI
datasets recorded from live performances and music studios
allows for the generation of music with rich and expressive
details, generating such music over long durations remains a
challenge. This is mainly due to the substantial increase in
computational resources required for processing long-range
contextual dependencies. Almost all current models, e.g.,
Music Transformer, employ strategies such as chunking and
quantization to reduce token sequence length and vocabulary
size [7]. While these approaches help to reduce computational
burdens, they also limit the model’s ability to capture ultra-
long dependencies and compromise expressive performance
details. As stated on the Music Transformer webpage,1“Some
‘failure’ modes include too much repetition, sparse sections,
and jarring jumps.” This trade-off prevents the effective gen-
eration of long-term coherent symbolic music.
B. Capturing Long-term Coherence
In the efforts to learn musical structures and generate
symbolic music with long-distance coherence, the approaches
can generally be categorized into two main types, i.e., those
that utilize explicit structural features and those that encourage
the model to learn implicit structural features. Each approach
is introduced in the following subsections.
1) Explicit Structural Features: The explicit use of struc-
tural features often relies on handcrafted feature engineering
and external analyzing tools. A common method in these
models is a waterfall-like approach. Typically, the process
begins by generating a lead sheet and then using the lead sheet
as a condition for the subsequent generation tasks.
MuseMorphose [15] explicitly controls rhythmic intensity
and polyphony density on a bar basis by training on datasets
with bar annotations, specifically the LPD-17-cleansed and
Pop1K7 datasets. Compose & Embellish [16] leverages third-
party tools such as the skyline algorithm, edit similarity, and
A* search measures to extract melody and identify structural
patterns, enabling the model to produce music with enhanced
structural organization. However, it still relies on bar-annotated
datasets for training. Rule-Guided Diffusion [17] uses note
density and chord to condition generation, resulting in struc-
tured musical segments. Its training does not rely on an anno-
tated dataset but rather uses the performance dataset, Maestro.
Still, it only produces short musical pieces instead of full-
length segments. Whole-Song Hierarchical Generation [18]
is capable of generating fully structured, complete pieces of
music. It employs a multi-stage approach, using annotations
from the POP909 dataset [19], including chord information
and separate tracks for melody, bridge, and accompaniment,
to produce structured elements such as form, lead sheet, and
accompaniment, ultimately generating a complete, full-length
piece.
2) Implicit Structural Features: Another important ap-
proach involves enabling the model to learn the structural
information of music implicitly. The method has the advantage
1https://magenta.tensorflow.org/music-transformer

--- PAGE 3 ---
of being more generalizable, as it does not rely on hand-
crafted feature engineering. However, its downside lies in the
increased difficulty for the model to capture complex structural
features of music.
MusicV AE [20] uses a bidirectional RNN and a Conductor
RNN to generate per-bar latent vectors that are decoded
into individual notes, focusing on bar-level structure through
training on datasets containing bar annotations, which may
not be applicable to freely performed music. Museformer [21]
applies sparse Transformer attention by fully attending to all
tokens in selected bars and the summarized vectors of all
preceding bars, allowing it to capture long-range context with
limited computational resources. However, it still leverages the
Lakh MIDI dataset [12]’s bar annotations, which cannot be
used with unannotated performance datasets.
C. SOTA Solutions with Long-Term Dependency On Perfor-
mance Datasets
The Perceiver AR model [4] from DeepMind has been a
significant source of inspiration. It combines cross-attention
and self-attention mechanisms, enabling the model to attend
to sequences with up to tens of thousands of tokens. Like
Perceiver [22] and Perceiver IO [23], Perceiver AR [4] uses
a shorter query in its cross-attention mechanism to attend
to much longer sequences, thereby minimizing computational
costs. As noted in the paper, this approach allows the model
to attend to up to 32,768 tokens when trained on the Maestro
dataset [5], offering a significantly longer context compared
to models like Transformer-XL [24]. This ability to efficiently
process long-range contextual data is crucial for learning the
structure of entire musical pieces.
However, Perceiver AR leverages the last Ntokens as the
Query with a limited causal mask, and training with teacher-
forcing on long sequences led to lower quality generation.
Furthermore, we observed that relying solely on long, espe-
cially ultra-long, context resulted in repetitive segments in the
latter part of generated content.
Despite significant progress, existing research has yet to
fully address the challenge of generating music with both
long-term dependencies and expressive performance details,
as most approaches either rely heavily on manual processes or
face performance limitations due to restricted context window
lengths. While Perceiver AR represents the state-of-the-art
method for generating music with long-term dependencies,
there remain areas that require further improvement.
III. P RELIMINARIES
In this section, we introduce the fundamental concepts and
key challenges necessary to understand our proposed model.
We review the operational mechanism of cross-attention in
Perceiver AR, the role of its causal mask, and the key
considerations when using ultra-long sequences as context for
token generation.
A. Input Sequence pre-processing
Let the complete sequence be X={x1, x2, . . . , x l}, where
lis the total length of the entire music sequence, and misthe maximum input length, representing the longest sequence
that the model can attend to in one pass. The query length
is denoted by n, which represents the number of tokens the
model uses to query the context, and typically, n≤m.
In a typical Transformer approach, a segment of length mis
extracted from the sequence at random. Specifically, we select
a starting index s∈[1, l−m+ 1], and the segment used for
training, denoted as ˆX, is defined in (1).
ˆX={xs, xs+1, . . . , x s+m−1} (1)
This approach produces overlapping fixed-length windows
for training, as defined in (2):
{x1, x2, . . . , x m},{x2, x3, . . . , x m+1}, . . . (2)
These sequences will be handled using a causal mask.
1) Causal Masking in Typical Transformers: In a typical
transformer with causal masking, the goal is to ensure that
when generating token i, the model does not attend to tokens
jwhere j > i . The causal mask for this is typically represented
by (3):
Mij=(
0 ifi≥j
−∞ ifi < j(3)
This matrix is added to the attention scores QKT(as defined
in (6)) so that all positions j > i (i.e., future tokens) are
masked out by setting their attention scores to −∞, ensuring
they don’t influence the generation of the current token.
For example, consider a case where the query has a length
ofn= 5 and the key/value has a length of m= 5. The
expected causal mask (Vanilla Transformer) is as follows in
(4):
M=
0−∞ −∞ −∞ −∞
0 0 −∞ −∞ −∞
0 0 0 −∞ −∞
0 0 0 0 −∞
0 0 0 0 0
, M ∈R5×5(4)
This mask ensures that the third query token only attends to
the first three tokens.
2) Perceiver AR’s Causal Mask Issue: In Perceiver AR [4],
the situation is different because the query token length nis
much smaller than the key and value lengths m. Specifically,
the causal mask only works on the final part of the context
sequence, equivalent to the length of the query n, but does
not mask tokens that occur before that. This results in some
tokens before the query length being visible during training,
which is not an issue for generation except that the segment
from x1toxm−nis not properly learned by the model.
Let’s denote the sequence of keys and values as Kand
V, respectively, and the query length as n, while the context
length (keys/values) is m, where m > n . The attention mask
matrix Min Perceiver AR can be represented as follows:

--- PAGE 4 ---
Mij=

0 ifj≤m−n+ 1
−∞ ifj−i > m −n+ 1
0 ifi > n
Below shows an example of the causal mask used in
Perceiver AR with n= 5(query length) and m= 10 (context
length) as illustrated in (5):
M=
0 0 0 0 0 0 −∞ −∞ −∞ −∞
0 0 0 0 0 0 0 −∞ −∞ −∞
0 0 0 0 0 0 0 0 −∞ −∞
0 0 0 0 0 0 0 0 0 −∞
0 0 0 0 0 0 0 0 0 0
,
(5)
M∈R5×10
Thus, the model can “peek” at tokens before the query
length because they are not fully masked, allowing it to attend
to tokens before the intended context during training. This
partial masking leads to a mismatch between training and auto-
regressive generation, degrading the quality of the generated
music.
Note that this issue primarily affects unconditional gener-
ation, where the model generates music without any specific
prompt or primer, making it more sensitive to inconsistencies
between training and generation contexts. In conditional gen-
eration, where the model starts with an initial prompt or primer
sequence, using a primer with a length close to the unmasked
portion during training can help preserve generation quality
and prevent degradation.
3) Calculation for Attention with Mask: The attention
mechanism with a causal mask is computed as follows [25]:
Attention (Q, K, V ) =softmaxQKT+M√dk
V (6)
B. Ultra-Long Context in auto-regressive Generation
Similar to the phenomenon of repeated segments often ob-
served in natural language generation as described in [26], re-
lying only on the ultra-long context in auto-regressive models
can lead to generated sequences containing excessively long
repetitive short segments, especially as the sequence length
increases. Given that the probabilities of generating tokens xt
andxt−k(where kis a small integer) are, respectively, as
shown in (7) and (8):
p(xt|x1, x2, . . . , x t−1) (7)
p(xt−k|x1, x2, . . . , x t−k−1) (8)
Astincreases, the contexts [x1, x2, . . . , x t−k−1]and
[x1, x2, . . . , x t−1]become nearly identical due to the ultra-
long context. Consequently, the conditional distributions
p(xt|x1, x2, . . . , x t−1)andp(xt−k|x1, x2, . . . , x t−k−1)are al-
most indistinguishable, resulting in a KL divergence close to
zero:DKL(p(xt|x1, x2, . . . , x t−1)||p(xt−k|x1, x2, . . . , x t−k−1))≈0
This similarity in conditional distributions means that the
model is likely to generate similar tokens in nearby steps, as
the probability distributions governing xtandxt−kbecome
nearly identical. Thus, the probability of xt=xt−kincreases,
leading to repetitive short segments.
When such repetitive tokens are generated across multiple
time steps, the sequence exhibits high token autocorrela-
tion. Mathematically, the token autocorrelation at lag k∈
{1,2, . . . , T −1}for the sequence X={x1, x2, . . . , x T}can
be expressed as [6]:
ρk(X) =PT
t=k+1(xt−¯x)(xt−k−¯x)
PT
t=1(xt−¯x)2(9)
where ¯xrepresents the mean of the sequence X. When values
at nearby steps exhibit high similarity, as suggested by nearly
identical conditional distributions, the term (xt−¯x)(xt−k−¯x)
in (9) becomes large, leading to high token autocorrelation at
lagk.
During generation, identical values are not produced at
nearby steps to avoid training penalties. In fact, the similar
context of neighboring tokens causes the generation process
to produce identical or similar tokens at nearby steps, leading
to a higher probability of generating repetitive short segments
as the sequence grows longer.
IV. P ROPOSED APPROACH
The proposed model, Perceiver S, is a dual approach of
Effective Segmentation and Multi- Scale attention to address
the limitations in symbolic music generation. Building on the
strengths of Perceiver AR [4] and introducing mechanisms
to handle both short and ultra-long dependencies, Perceiver S
(Segmentation and Scale) achieves greater coherence and ex-
pressiveness in generated music.
Since Perceiver AR provides the possibility of accessing
extremely long contexts, we attempt to use Perceiver AR as a
baseline model to learn entire musical pieces and evaluate the
quality of its generation. Our goal for symbolic music gen-
eration is to achieve long-term coherence while maintaining
diversity within shorter segments. Furthermore, we expect the
model to learn patterns similar to human composition, with
repetition and development. The detailed steps of our approach
and improvements are elaborated in the following subsections.
A. Improving the Model to Effectively Learn Ultra-Long Se-
quences
This section outlines a data pre-processing strategy designed
to enhance token generation quality in auto-regressive models.
As discussed in the previous section, Perceiver AR’s causal
mask has limitations in its coverage of the entire input
sequence, making it necessary to implement pre-processing
adjustments before feeding data into the model.

--- PAGE 5 ---
We set the maximum context length to 32,768 tokens . Based
on the Perceiver AR’s original implementation2, we randomly
cropped the dataset. In this approach, a random starting point
is selected between 0and(the sequence length - the maximum
input length + 1) , from which a segment of length equal to
themaximum input length is taken.
Full sequence lengthOriginal input sequence preprocessing
Crop start
Max input length
Max input length
♪ 
Full sequence lengthEffective Segmentation input sequence preprocessing
Crop end
Causal maskMax input length
Max input length
♪ Query lengthCausal mask
Causal maskCausal maskQuery length
Padding
Fig. 1. Comparison of Data pre-processing Methods and Causal Masks
Between the Baseline and Perceiver S
The upper part of Figure 1 demonstrates the original input
sequence pre-processing of the baseline model. Specifically,
the baseline model segments the input sequence using the max-
imum input length as the window size, leaving the beginning
of the sequence uncovered by the causal mask. This results in
these initial tokens not being progressively used as validation
tokens in teacher forcing, preventing the model from learning
to generate tokens at the start of the sequence that fall outside
the mask.
In contrast, we propose an alternative method that does not
rely on cropping the dataset based on the longest input se-
quence. Instead, we randomly select an endpoint for cropping
between (the query length + 1) and(the sequence length + 1) .
A segment of up to the maximum input length is then taken,
2https://github.com/google-research/perceiver-arleading up to this endpoint (or shorter for tokens near the
beginning). Padding is applied as in the baseline model.
The lower part of Figure 1 illustrates the effective segmenta-
tion input sequence pre-processing of the proposed Perceiver S.
It begins learning token generation from the initial part of the
sequence with effective causal mask coverage. This approach
ensures that tokens from 1 to (m−n)in the sequence are
effectively covered by the causal mask, enabling the model to
learn token generation at the beginning of the sequence more
effectively.
The rationale for this improvement is that Perceiver AR [4]’s
causal mask operates in a final block mechanism, where
it provides context for the length of the input tokens but
only partially masks within the query token length. Training
with traditional segmentation causes a mismatch between the
training phase, where teacher forcing is used, and the auto-
regressive generation process. This approach allows the model
to “peek” at unintended tokens during training, which degrades
generation quality during inference when such tokens are
unavailable for reference.
The experimental results (in the Experiments and Results
subsection) will show that this improvement in data prepro-
cessing significantly impacts performance.
B. Further Improving the Model for Generating Music with
Both Coherence and Diversity
After applying ultra-long contexts in the auto-regressive
generation, we aim to combine the strengths of both con-
sistency and diversity, allowing the proposed Perceiver Sto
generate music without a tendency toward repeated segments
caused by attending solely to long-distance contexts.
The Multi-Scale cross-attention mechanism adopted in the
proposed Perceiver S, while somewhat similar to the concept
of Museformer [21], is fundamentally different. It introduces
multiple layers of attention with varying scales of attending
length, designed to balance focus on both long and short
contexts, thereby enhancing diversity and reducing repetitive
tendencies.
Figure 2 illustrates the Multi-Scale Cross-Attention Mecha-
nism of Perceiver S. Specifically, within the multi-layer cross-
attention, tokens from earlier in the sequence are masked by
different scales, and the resulting outputs are combined before
being fed into the self-attention layer. This approach enables
the model to incorporate cross-attention at multiple scales
simultaneously.
Two layers of cross-attention are implemented. One layer
operates without a scale mask, while the second layer applies
a scale mask that masks out all tokens before the last 1,024
tokens .
The output of cross-attention layers is combined using the
cascade approach. The cascade method feeds the output from
the first layer directly as input to the second layer, allowing
each layer to refine and build upon the preceding layer’s
output.

--- PAGE 6 ---
NxM
Q V
KQ
VK
Inputs Last N
♪ ♪ ♪ ♪ ♪ ♪ ♪ ♪ ♪ ♪ ♪ ♪ ♪ Long range attendLatent
self-attend
L layersTargets
(shifted inputs)
MxC NxCMxDMxDNxDLatents
Multi-Scale
cross-attendNxD♪♪ ♪ 
NxM NxMShort range attend♪ Fig. 2. Multi-Scale Causal Cross-Attention Mechanism of Perceiver S
V. T ECHNICAL DETAILS
In this section, we present the technical details, including
the mathematical formulation and technical explanation of our
proposed approach, Perceiver S, designed for symbolic music
generation. Based on the improvements to Perceiver AR, our
model introduces two key innovations, i.e., Effective Seg-
mentation for input sequences and Multi-Scale cross-attention
mechanism. The details are presented in the following sections.
A. Effective Segmentation
To address the mismatch between training and auto-
regressive generation, we propose a novel segmenting method
aligned with the causal mask mechanism, whether using ran-
dom or sequential sampling, that emphasizes shorter context
sequences, gradually building up to the maximum input length.
Let the complete sequence be X={x1, x2, . . . , x l}, where
ldenotes the total length of the entire sequence, mis the
maximum input length the model can attend to in one pass,
andnrefers to the query length within that input. The input
sequences used for training, denoted as ˆX, are generated as
follows in Equation (10):ˆXi:j={xi, xi+1, . . . , x j},where i≤j≤i+m−1(10)
Here, iis a starting position chosen within the sequence,
andjis the end position such that the length j−i+ 1 does
not exceed the maximum input length m.
When accessing data from the dataset, instead of using
fixed-length sequences with the maximum input length, we
progressively extract sequences by increasing the context
length from nup to m. This way, the training set includes
progressively larger prefixes of the sequence, as shown in
Equation (11):
{x1, x2, . . . , x n},{x1, x2, . . . , x n+1},
. . . ,{x1, x2, . . . , x m}(11)
After reaching the full context length m, we continue seg-
menting from various starting positions while preserving the
maximum input length for each sequence, yielding segments
such as in Equation (12):
ˆX2:m+1={x2, x3, . . . , x m+1},
ˆX3:m+2={x3, x4, . . . , x m+2},
...(12)
In practice, dataset segments are randomly selected from
these defined segments. For sequential segment retrieval, the
method begins with progressively longer prefixes, as shown in
Equation (13):
{x1, x2, . . . , x 1·n},{x1, x2, . . . , x 2·n},
. . . ,{x1, x2, . . . , x k·n}where k·n≤m
(13)
Here, kis an integer ranging from 1 to K, where Krep-
resents the maximum number of segments possible within the
full sequence length l. When kn > m , subsequent segments
shift forward along the sequence in fixed-length windows of
sizem, as shown in Equation (14):
{xk·n−m, xk·n−m+1, . . . , x k·n} (14)
This approach ensures that the model learns the sequence
structure progressively, closely resembling the auto-regressive
generation process where tokens are predicted one at a time.
As a result, the model is better aligned with the auto-regressive
nature of music generation, reducing the peeking problem and
improving generation quality. Thus, the model can smoothly
learn the generation of sequences {x1, x2, . . . , x m−n}with an
effective causal mask. The key advantages are summarized as
follows.
•Progressive learning : The model learns long-range de-
pendencies progressively by starting with shorter se-
quences and gradually increasing the context length.
•Consistency : This method ensures the training pro-
cess mimics auto-regressive generation, reducing quality
degradation during generation.

--- PAGE 7 ---
•Causal alignment : This approach aligns with the causal
mask, preventing the model from “peeking” at future
tokens during training.
B. Multi-Scale Cross-Attention Mechanism
The proposed Multi- Scale cross-attention mechanism en-
ables Perceiver Sto handle different levels of context simulta-
neously. This mechanism employs two cross-attention layers
on top of the causal mask: one without any scale mask,
allowing all tokens to remain unmasked, and the other layer
that masks tokens from the 1st to the m−nth token. The
details with formulas for these two settings are elaborated as
follows.
First, we introduce the Attention mask without scale mask .
In this case, all tokens are visible to the model. as defined in
Equation (15):
ˆMij= 0,∀i, j (15)
Second, in the case of Attention mask with scale mask
(masking the 1st to the m−nth tokens) , tokens from the
1st to the m−nth positions are masked by the scale mask,
preventing the model from attending to these tokens. The scale
mask matrix ˆMis defined in Equation (16):
ˆMij=(
0 ifj > m −n
−∞ ifj≤m−n(16)
This ensures that only tokens starting from the m−n+1th
position are visible, while the model cannot attend to tokens
before this position. The combined causal mask Mand scale
mask ˆMare added directly to the attention score calculation,
modifying the softmax as follows (Equation (17)):
Attention (Q, K, V ) =softmax 
QKT+M+ˆM√dk!
V,(17)
where Mrefers to the causal mask that ensures the model
does not attend to future tokens, and ˆMdenotes the scale mask
applied to limit attention to certain parts of the sequence. Both
masks work together to control which tokens the model can
attend to during training.
After calculating the attention of a given layer in the manner
described above, the output from this attention layer is fed as
input to the next layer, enabling each layer to refine and build
upon the previous layer’s output. We refer to this method of
combining attention layers as the Cascade mode.
VI. E XPERIMENTS AND RESULTS
A. Experimental Setup
In this subsection, we introduce the experimental setup,
including dataset selection, MIDI Pre-processing, hyper-
parameter selection, and evaluation metrics.1) Dataset Selection: In our experiments, we used the
Maestro dataset [5] as the primary dataset. It contains 1,251
sequences, with a validation set of 240 sequences. All models
were trained and evaluated solely on this dataset to ensure
consistency in assessing performance.
2) MIDI pre-processing: For MIDI pre-processing, we set
theNote On andNote Off events within the range of 0
to 127. Time Shift events were discretized into 100 time
steps per second, where each step represents 10 milliseconds.
V olume events were quantized into 32 bins, and pedal events
were mapped to the duration of relevant notes, discarding the
pedal events afterwards. Each song ends with a token_end
marker. No data augmentation, such as key or tempo changes,
was applied, though this is planned for future experiments.
3) Hyperparameter Selection: The hidden dimension was
set to 1,024 with 24 self-attention layers. Each attention
layer had 16 heads, and the head dimension was 64. Adam
optimization was used, with an initial learning rate set to
0.03125. Each generated sequence length was set to 8,192
tokens, resulting in approximately 2-10 minutes of music. The
music generation in this setup was unconditional , meaning
that no external conditions or prompts were used to guide the
generation process. The resulting MIDI files were rendered
into audio using the Vintage Piano sound from Logic Pro.
4) Evaluation Metrics: Inspired by the previous research
work [27], for evaluation, we constructed a reference dataset
by separating a set of pieces from the training dataset (Mae-
stro [5]) before model training. Then, we generated an equal
number of files using the model to form the generated dataset.
We calculated the distances within each dataset and between
the generated and reference datasets for the following metrics:
•Total Used Pitch (PC): Measures the overall pitch di-
versity by counting distinct pitch classes used throughout
the entire piece.
•Total Used Note (NC): Counts the distinct notes (pitch
and octave combinations) used across the entire piece,
indicating the variety in pitch and register.
•Total Pitch Class Histogram (PCH): A histogram
representing the frequency distribution of pitch classes
across the entire piece, offering insights into pitch class
preference.
•Pitch Class Transition Matrix (PCTM): A matrix
representing the probabilities of transitioning from one
pitch class to another. This metric captures melodic and
harmonic movement patterns.
•Pitch Range (PR): Measures the difference between the
highest and lowest pitches used in the piece, indicating
the overall range of pitches.
•Average Pitch Interval (PI): The average interval be-
tween consecutive pitches, which reflects the tendency
towards stepwise or leapwise motion in melodies.
•Average Inter-Onset Interval (IOI): Measures the av-
erage time interval between note onsets, capturing the
rhythmic density across the entire piece.
•Note Length Histogram (NLH): A histogram represent-
ing the distribution of note lengths, giving insights into

--- PAGE 8 ---
note duration diversity.
•Note Length Transition Matrix (NLTM): A matrix
representing the transition probabilities between different
note lengths, capturing rhythmic patterns and variations
in note duration.
We introduced four metrics based on time segments, as bar
annotations are unavailable in performance datasets, making
time-based segmentation essential for evaluating long-term
coherence and local diversity.
•Segment Used Pitch (PC/seg): Similar to Total Used
Pitch, but calculated within segments. This metric helps
capture pitch diversity across shorter sections.
•Segment Used Note (NC/seg): Counts distinct notes
within fixed-length segments, allowing us to analyze the
variety of notes used across different parts of the piece.
•Segment Pitch Class Histogram (PCH/seg): Represents
the pitch class distribution within each segment, provid-
ing insights into the consistency of pitch class usage
across different sections.
•Segment Average Inter-Onset Interval (IOI/seg): Mea-
sures the average inter-onset interval within each seg-
ment, helping to analyze rhythmic density and tempo
consistency within shorter sections.
To calculate these additional segment-based metrics, each
evaluation piece was divided into 64 segments of equal dura-
tion. All of these metrics are calculated based on the inter-set
distribution similarity between the generated dataset and the
ground truth dataset, including two values: the KL Divergence
(KLD) and Overlap Area (OA). When the OA value is larger
and the KLD value is smaller, it indicates that the above
metric distributions of the generated dataset and the ground
truth dataset are closely aligned, suggesting that the model has
generated music more closely resembling human-composed
and performed pieces.
B. Experiments and Results
Two experiments have been conducted to evaluate the
models’ performance in various scenarios.
1)Experiment 1: Input Sequence Segmentation :The first
experiment aims to demonstrate that Effective Segmentation
is essential for the model to fully leverage ultra-long-distance
context.
We compared two types of input sequence segmentation,
with a maximum sequence length set to 32,786 tokens:
•Baseline Model: A random starting position is selected
within the range [0, sequence length - max input length
+ 1], and then a segment of max input length tokens is
taken from this start.
•Improved Model: A random end position is selected
within the range [query length + 1, sequence length + 1],
and then a segment of tokens, up to the max input length,
is taken backwards from this end. Segments shorter than
the max input length are padded at the beginning.
The segmentation methods produced very different results
in auto-regressive training and generation, with the improvedmodel showing much better quality compared with the base-
line, as shown in the data within the red dashed box in
Table I. The music generated using the improved input se-
quence processing approach is closer to the ground truth in
terms of lower KLD metrics, including PC, PC/seg, NC/seg,
PCH/seg, PCTM, PR, PI, IOI, and IOI/seg, as well as higher
OA metrics, including PC, PC/seg, NC, NC/seg, PCTM, PR,
PI, IOI, IOI/seg, and NLTM. This result indicates that the
improvement effectively enables the model to utilize ultra-
long-distance context for music generation.
2)Experiment 2: Multi-Scale Attention :The aim of the
second experiment is to evaluate whether using multi-layer
cross-attention with different context scales, especially by sim-
ply adding attention outputs for shorter-range dependencies,
can improve generation quality.
While an ultra-long context provides long-term consistency,
it tends to generate repetitive segments in the latter part
of the sequence (see Figure 3). To address this issue, we
added different scale masks to the multi-layer cross-attention,
allowing different layers to focus on distinct context lengths
and merging these results. Here, we define two scale masks for
the maximum sequence length of 32,786 tokens. Specifically,
in the No Scale Mask setting, all tokens remain unmasked.
In the Masked Scale setting , only the last 1,024 tokens are
visible, while all preceding tokens are masked.
The cross-attention layer without a mask is fed into the
cross-attention layer with a context mask as its input. We refer
to this approach as cascade Multi- Scale cross attention.
Fig. 3. Piano roll showing repetitive segments over time in the long context
model.
Fig. 4. Piano roll showing diverse phrases over time in the long context
model after applying multi-scale attention.

--- PAGE 9 ---
TABLE I
EVALUATION METRICS FOR BASELINE , EFFECTIVE SEGMENTATION ,AND MULTI -SCALE MODELS
Baseline Effective Segmentation Multi-Scale
KLD OA Comparison KLD OA Comparison KLD OA Comparison
PC 0.036 0 .614 0% 0.007 0 .827 +35% 0.017 0 .759 +24%
PC/seg 0.308 0 .235 0% 0 .055 0 .369 +57% 0.055 0 .704 +200%
NC 0.110 0 .027 0% 0 .209 0 .040 +49% 0.165 0 .040 +49%
NC/seg 0.578 0 .109 0% 0 .519 0 .169 +54% 0.626 0 .151 +38%
PCH 0.017 0 .948 0% 0 .098 0.779 −18% 0 .060 0.907 −4%
PCH/seg 0.122 0 .817 0% 0 .091 0 .699 −14% 0 .082 0 .871 +7%
PCTM 0.393 0 .302 0% 0 .253 0 .655 +117% 0.177 0 .424 +41%
PR 0.058 0 .696 0% 0 .004 0 .888 +28% 0.013 0 .774 +11%
PI 0.026 0 .521 0% 0 .018 0 .883 +70% 0.071 0 .729 +40%
IOI 0.327 0 .670 0% 0 .056 0.877 +31% 0.004 0.922 +38%
IOI/seg 0.128 0 .713 0% 0 .064 0 .722 +1% 0.038 0 .860 +21%
NLH 0.057 0 .841 0% 0 .088 0 .655 −22% 0 .092 0 .748 −11%
NLTM 0.122 0 .029 0% 0 .187 0 .040 +41% 0.139 0 .048 +69%
The Comparison column shows the percentage difference between the current OA and the baseline OA.
Results, as shown in the data within the solid blue box
inTable I , indicate that this approach significantly improved
the model’s generation quality. By examining the OA of
the four parameters IOI, IOI/seg, NLH, and NLTM—which
are related to rhythm characteristics—it is evident that the
model’s generation aligns more closely with the reference set.
Additionally, the OA values for PCH and PCH/seg indicate
that the harmonic characteristics also better resemble those of
the reference dataset. Furthermore, we analyzed the density of
repetitive notes detected per unit time in segmented generated
music before and after the improvement, as shown in Figure 5,
which indicates that the pre-improvement model exhibited a
greater tendency to generate repetitive segments. In contrast,
unusual repetitive segments in the generated music are now
rare after the improvement, as shown in Figure 4, with the
generated music even exhibiting a rich diversity of phrases.
Fig. 5. Density of repetitive notes detected per unit time in segmented
generated music before and after the improvement.
We also note that the baseline model shows better results in
PCH and NLH metrics because most classical music is multi-
movement; as a result, multi-movement classical pieces and
poorly generated, random-like music, when analyzed across
entire pieces, tend to exhibit a wide distribution of pitch classesand note lengths. In contrast, the improved model generates
music that maintains relatively more consistent harmony and
rhythm throughout an entire piece, resembling a single move-
ment of a classical piece, resulting in a narrower distribution
than both ground truth and unstructured music. Therefore,
when examining the segmented data for the same metrics on
the same set of pieces, such as PCH/seg, the improved model
performs better, as the segmented analysis of ground truth
music, with most segments taken from the same movement,
tends to have a narrower harmonic and rhythmic distribution
compared to unstructured music. Although NLH segment data
was not calculated, it should follow a similar reasoning.
C. Discussion
Our proposed Effective Segmentation technique, which op-
timizes the input sequence processing for Perceiver AR [4],
has shown that, with appropriate data segmentation, Perceiver
AR [4] can effectively learn long contexts and produce
coherent, contextually rich content. The ultra-long context
attention indeed provides the model with remarkable benefits
in maintaining content consistency over extended generation.
However, a potential drawback of solely relying on long-
range dependencies emerged: as sequences get longer, the
model increasingly tends to generate repetitive short segments.
This occurs because, in the later stages of generation, the long
context windows are nearly identical, differing by only a few
tokens. Consequently, the probability of generating identical
or similar segments increases, further amplified by the high
token autocorrelation [6] tendency due to the high similarity
of context as the sequence lengthens.
To mitigate this issue, we introduce a Multi- Scale cross-
attention mechanism that combines both long and short con-
text windows. By integrating varying context lengths into
multi-layer cross-attention, our approach successfully reduces
the tendency for repetitive sequences while maintaining the
model’s long-term consistency. This combination strategy en-
ables the generation of high-quality symbolic music over
extended sequences.

--- PAGE 10 ---
Our enhanced model, Perceiver S(Segmentation and Scale),
leverages Effective Segmentation aligned with Perceiver
AR [4]’s operation mode. By applying Multi- Scale mask-
ing strategies within multi-layer cross-attention, Perceiver S
effectively generates cohesive, consistent music over extended
temporal contexts, preserving intricate musical variations and
expressive details across long sequences. Additionally, through
the use of performance music datasets, Perceiver Sis capable
of producing high-quality symbolic music that captures the
nuances of human performance. Importantly, because the
model does not rely on annotated datasets, Perceiver Scan be
trained on MIDI data derived from audio using any automatic
music transcription (AMT) technique. This capability suggests
a future in which symbolic music generation can leverage
vast historical recordings, unbounded by the limitations of
manually annotated data.
In essence, Perceiver AR [4] and, by extension, Perceiver S,
are general-purpose models adaptable to a wide range of AI
tasks. The Effective Segmentation and Multi- Scale innovations
introduced here open up avenues for future applications across
domains such as text, image, and video. Future research
could thus extend the potential of Perceiver S, exploring its
capabilities across diverse modalities and expanding its utility
within the broader landscape of AI tasks.
VII. C ONCLUSION AND FUTURE WORK
In this work, we introduced a novel model, Perceiver S,
which builds on the Perceiver AR [4] architecture by incor-
porating Effective Segmentation and a Multi- Scale attention
mechanism. The Effective Segmentation approach progres-
sively expands the context segment during training, aligning
more closely with auto-regressive generation and enabling
smooth, coherent generation across ultra-long symbolic mu-
sic sequences. The Multi- Scale attention mechanism further
enhances the model’s ability to capture long-term structural
dependencies while maintaining short-term diversity.
By addressing limitations in existing models, particu-
larly the issue of causal masking in auto-regressive gener-
ation and the high token autocorrelation problem in ultra-
long sequences, Perceiver Senables the effective handling
of ultra-long token sequences without compromising the
quality of generated music. Through our proposed Effective
Segmentation in dataset pre-processing and Multi- Scale atten-
tion modifications, we demonstrated significant improvements
in generating coherent and diverse musical pieces.
Our approach to symbolic music generation provides a new
balance between structural coherence and expressive diversity,
setting a foundation for future advancements in symbolic
music generation models.
REFERENCES
[1] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo
Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: Text-to-
audio generation with latent diffusion models, 2023.
[2] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel
Synnaeve, Yossi Adi, and Alexandre D ´efossez. Simple and controllable
music generation, 2024.[3] Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex
Wang. Jen-1: Text-guided universal music generation with omnidirec-
tional diffusion models, 2023.
[4] Curtis Hawthorne, Andrew Jaegle, C ˘at˘alina Cangea, Sebastian Borgeaud,
Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals,
Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-
Baptiste Alayrac, Jo ˜ao Carreira, and Jesse Engel. General-purpose, long-
context autoregressive modeling with perceiver ar, 2022.
[5] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-
Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, and
Douglas Eck. Enabling factorized piano music modeling and generation
with the maestro dataset, 2019.
[6] Minhyeok Lee. A mathematical interpretation of autoregressive genera-
tive pre-trained transformer and self-supervised learning. Mathematics ,
11(11):2451, 2023.
[7] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam
Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D.
Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer,
2018.
[8] Qiuqiang Kong, Bochen Li, Jitong Chen, and Yuxuan Wang. Giantmidi-
piano: A large-scale midi dataset for classical piano music, 2022.
[9] Huan Zhang, Jingjing Tang, Syed RM Rafee, Simon Dixon, George
Fazekas, and Geraint A Wiggins. Atepp: A dataset of automatically
transcribed expressive piano performance. In Ismir 2022 Hybrid Con-
ference , 2022.
[10] Drew Edwards, Simon Dixon, and Emmanouil Benetos. Pijama: Piano
jazz with automatic midi annotations. Transactions of the International
Society for Music Information Retrieval , 2023.
[11] Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon,
Colin Raffel, Jesse Engel, Sageev Oore, and Douglas Eck. Onsets and
frames: Dual-objective piano transcription, 2018.
[12] Colin Raffel. Learning-based methods for comparing sequences, with
applications to audio-to-midi alignment and matching . Columbia
University, 2016.
[13] Francesco Foscarin, Andrew Mcleod, Philippe Rigaux, Florent Jacque-
mard, and Masahiko Sakai. Asap: a dataset of aligned scores and
performances for piano transcription. In International Society for Music
Information Retrieval Conference , pages 534–541, 2020.
[14] Francesco Foscarin, Jan Schl ¨uter, and Gerhard Widmer. Beat this!
accurate beat tracking without dbn postprocessing, 2024.
[15] Shih-Lun Wu and Yi-Hsuan Yang. Musemorphose: Full-song and fine-
grained piano music style transfer with one transformer vae, 2022.
[16] Shih-Lun Wu and Yi-Hsuan Yang. Compose & embellish: Well-
structured piano performance generation via a two-stage approach, 2023.
[17] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng
Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, and
Yisong Yue. Symbolic music generation with non-differentiable rule
guided diffusion, 2024.
[18] Ziyu Wang, Lejun Min, and Gus Xia. Whole-song hierarchical genera-
tion of symbolic music using cascaded diffusion models, 2024.
[19] Ziyu Wang, Ke Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi
Dai, Xianbin Gu, and Gus Xia. Pop909: A pop-song dataset for music
arrangement generation, 2020.
[20] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Dou-
glas Eck. A hierarchical latent vector model for learning long-term
structure in music, 2019.
[21] Botao Yu, Peiling Lu, Rui Wang, Wei Hu, Xu Tan, Wei Ye, Shikun
Zhang, Tao Qin, and Tie-Yan Liu. Museformer: Transformer with fine-
and coarse-grained attention for music generation, 2022.
[22] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol
Vinyals, and Joao Carreira. Perceiver: General perception with iterative
attention, 2021.
[23] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Do-
ersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran,
Andrew Brock, Evan Shelhamer, Olivier H ´enaff, Matthew M. Botvinick,
Andrew Zisserman, Oriol Vinyals, and Jo ¯ao Carreira. Perceiver io: A
general architecture for structured inputs & outputs, 2022.
[24] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models
beyond a fixed-length context, 2019.
[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention
is all you need, 2023.

--- PAGE 11 ---
[26] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The
curious case of neural text degeneration, 2020.
[27] Li-Chia Yang and Alexander Lerch. On the evaluation of generative
models in music. Neural Computing and Applications , 32(9):4773–4784,
2020.
