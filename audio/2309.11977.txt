# 2309.11977.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/audio/2309.11977.pdf
# File size: 293878 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IMPROVING LANGUAGE MODEL-BASED ZERO-SHOT TEXT-TO-SPEECH SYNTHESIS
WITH MULTI-SCALE ACOUSTIC PROMPTS
Shun Lei1‡, Yixuan Zhou1†, Liyang Chen1, Dan Luo1, Zhiyong Wu1,3∗, Xixin Wu3∗,
Shiyin Kang2∗, Tao Jiang2, Yahui Zhou2, Yuxing Han1, Helen Meng3
1Shenzhen International Graduate School, Tsinghua University, Shenzhen
2Skywork AI PTE. LTD., Beijing3The Chinese University of Hong Kong, Hong Kong SAR
{leis21, yx-zhou23 }@mails.tsinghua.edu.cn, zywu @sz.tsinghua.edu.cn, wuxx @se.cuhk.edu.hk, shiyin.kang @kunlun-inc.com
ABSTRACT
Zero-shot text-to-speech (TTS) synthesis aims to clone any un-
seen speaker’s voice without adaptation parameters. By quantizing
speech waveform into discrete acoustic tokens and modeling these
tokens with the language model, recent language model-based TTS
models show zero-shot speaker adaptation capabilities with only
a 3-second acoustic prompt of an unseen speaker. However, they
are limited by the length of the acoustic prompt, which makes it
difficult to clone personal speaking style. In this paper, we propose
a novel zero-shot TTS model with the multi-scale acoustic prompts
based on a language model. A speaker-aware text encoder is pro-
posed to learn the personal speaking style at the phoneme-level from
the style prompt consisting of multiple sentences. Following that,
a V ALL-E based acoustic decoder is utilized to model the timbre
from the timbre prompt at the frame-level and generate speech. The
experimental results show that our proposed method outperforms
baselines in terms of naturalness and speaker similarity, and can
achieve better performance by scaling out to a longer style prompt1.
Index Terms —text-to-speech, zero-shot, multi-scale acoustic
prompts, speaker adaptation, language model
1. INTRODUCTION
Text-to-speech (TTS) aims to generate natural and intelligible
speech from text. With the development of deep learning, neu-
ral network based TTS models can already synthesize high-quality
speech for single [1, 2] or multiple speakers [3, 4]. However, these
models still require a sufficient amount of clean speech data for
new speakers, which hinders the development of speech synthesis
technology for many personalized applications. Therefore, adapting
TTS models for any speaker with as few data as possible, while
achieving high speaker similarity and speech naturalness, has at-
tracted increasing interest in academia and industry [5].
One of the general approaches is fine-tuning a well-trained
multi-speaker TTS model with a few adaptation data to support new
speakers. Some studies devote effort to fine-tune the whole TTS
model [6,7], and other recent methods seek to reduce the adaptation
parameters by fine-tuning only a part of the model [8], or only the
speaker embedding [9]. However, the adaptation performance of
these methods relies heavily on the quality and quantity of the data
available for the target speaker.
‡Work done when the first author was intern at Skywork AI PTE. LTD.
†Equal contribution.∗Corresponding author.
1Speech sample: https://thuhcsi.github.io/icassp2024-msvalleTo deal with this deficiency, some works conduct zero-shot
adaptation, which leverages only a few seconds of speech to clone
an unseen speaker’s voice without fine-tuning the model. In [10–12],
a speaker encoder is utilized to extract global speaker embeddings
from the given reference speech, which allows the TTS model to
clone the overall timbre of the reference speech. Considering that it
is difficult to describe the personal characteristics of speakers with
a single speaker embedding, [13, 14] propose to extract fine-grained
speaker embeddings to improve the quality of synthesized speech.
Motivated by advancements in natural language generation mod-
els, recent speech generation systems [15–17] introduce the idea
of utilizing neural audio codec [18, 19] to quantize speech wave-
form into discrete tokens and leveraging prompting-based language
model (e.g., GPT-3 [20]) to predict these tokens. These language
model-based TTS systems can be trained on large, diverse and
low-quality multi-speaker speech datasets to improve generalization
performance. With these approaches, the models are capable of
cloning the speaker’s timbre with only a 3-second acoustic prompt.
However, the above language model-based zero-shot TTS meth-
ods only consider the acoustic prompt at frame-level, leading to two
major limitations. First, the speaker characteristics of a person in-
clude not only the timbre but also personal speaking style, which
consists of various elements such as prosody, accents and pronunci-
ation habits. While considering the acoustic prompt at frame-level
has shown the great power of timbre clone, it has been proven that
phoneme-level representations are more suited for generating per-
sonal speaking style [21, 22]. Second, limited by the structure of
the decoder-only language model, these works only support a short
acoustic prompt because the frame-level acoustic token sequence is
too long (a 10s speech usually contains thousands of tokens). It is
difficult to use the limited information contained in the short acous-
tic prompt to accurately clone the speaker characteristics of the target
speaker, leading to poor naturalness and similarity of speaking style.
In addition, current language model-based methods have no ability
to utilize multiple reference samples to enhance the quality of zero-
shot TTS even though several utterances of the target speaker are
available during inference in many real-world scenarios.
To further improve speaker similarity for language model-based
zero-shot TTS synthesis, we propose to utilize multi-scale acoustic
prompts to capture both the timbre and personal speaking style of the
target speaker. Our model contains a speaker-aware text encoder,
which utilizes a reference attention module to model the personal
speaking style at phoneme-level from the style prompt consisting of
multiple utterances and an acoustic decoder, which preserves a spec-
ified timbre by considering timbre prompt at frame-level based on
the neural codec language model (called V ALL-E). The model al-arXiv:2309.11977v3  [cs.SD]  9 Apr 2024

--- PAGE 2 ---
Fig. 1 . The overall architecture of the proposed model
lows scaling out to an arbitrary length of style prompt to describe
detailed speaker characteristics. Both subjective and objective eval-
uations show that our proposed method outperforms state-of-the-art
language model-based zero-shot TTS model [16] and other baselines
in terms of naturalness and speaker similarity. The performance is
also improved with an increasing number of sentences used in the
style prompt during inference.
2. METHODOLOGY
The architecture of our proposed model is illustrated in Fig.1. It
consists of two major parts: a speaker-aware text encoder and an
acoustic decoder based on V ALL-E. In this paper, we follow Nat-
uralspeech 2 [23] and V ALL-E [16] to leverage neural audio codec
models to represent style prompt and timbre prompt in continuous
acoustic embeddings and discrete acoustic tokens, respectively. The
speaker-aware text encoder is used to extract phoneme-level personal
speaking style information from the style prompt and fuse it into en-
coded phoneme embeddings by a reference attention module to ob-
tain speaker-aware text embeddings. Then the outputs of the encoder
are fed into the acoustic decoder along with the acoustic tokens of
the timbre prompt to generate speech with the same timbre as the
timbre prompt. The details of each component are as follows.
2.1. Speaker-aware Text Encoder
The speaker-aware text encoder is specifically designed to ex-
tract and model personal speaking style at phoneme-level from
an arbitrary-length style prompt and fuses the text-side content
information with the speech-side style information to obtain the
speaker-aware text embeddings. The architecture of the encoder is
illustrated in Fig.2, which comprises a phoneme encoder, an acoustic
encoder and a reference attention module.
On the text side, to derive better text representation as decoder
input, we introduce a phoneme encoder to encode the phoneme se-
quence. We use the ransformer block, which is a stack of self-
attention layer and 1D-convolution as in Fastspeech 2 [2], as the
basic structure for the encoder. The input texts are converted into
a sequence of phonemes by the grapheme-to-phoneme module and
then passed to the phoneme encoder to obtain phoneme embeddings.
On the speech side, to make use of arbitrary-length speech
prompts, previous approaches attempt to encode the speaker char-
acteristics into a global-level vector [11, 12]. As a result, the local
Fig. 2 . The structure of the speaker-aware text encoder
fine-grained variations in speaking style are ignored. Different from
this way, we use an acoustic encoder to derive the local speaking
style embeddings from the style prompt instead of a single vector.
All the utterances of the target speaker are firstly concatenated to
form the style prompt, and then passed to a well-trained neural
audio codec to convert speech waveform into continuous acoustic
embeddings instead of discrete tokens to preserve as much personal
style information as possible in the speech. Then converted acoustic
embeddings are then passed to the acoustic encoder, which is made
up of a stack of 8 1D-convolution layers. In addition, in order to reg-
ulate the temporal granularity of the extracted style representations
closer to human vocal perception, the filter strides of convolution
layers are set as [2,1,2,1,2,1,2,1] for 16 times downsampling (about
0.2s). After that, the temporal granularity of the style embeddings is
properly reformed to a quasi-phoneme level inspired by [24].
To make better use of style embeddings extracted from the
style prompt, a reference attention module is introduced to obtain
the appropriate phoneme-level semantic-related personal speaking
style. We adopt scaled dot-product attention as the reference atten-
tion module. The phoneme embeddings are regarded as the query,
while all the style embeddings extracted from the style prompt are
regarded as both the key and the value. The relevance between them
is used to guide the selection of the personal speaking style for each
input phoneme. Finally, the reference attention module outputs an
aligned sequence with the same length as the phoneme embeddings
and adds it to the phoneme embeddings to form the speaker-aware
text embeddings.
2.2. Acoustic Decoder
In order to model speaker characteristics of a person, it is necessary
to clone the timbre in addition to mimic the speaker’s speaking style.
Inspired by the success of language models in zero-shot TTS, our
proposed method adopts a modified V ALL-E [16] as the acoustic
decoder to generate speech with the same timbre as the 3-second
timbre prompt. As illustrated in Fig.3, the decoder is made up of
an acoustic embedding, an autoregressive (AR) transformer decoder
and a non-autoregressive (NAR) transformer decoder.
The timbre prompt is first passed to a well-trained neural audio
codec, and the output of the residual vector quantizer in the codec is
considered as discrete prompt acoustic tokens. These tokens consist
of 8 layers which are then embedded through eight separate acous-
tic embedding layers. The AR transformer decoder is utilized to
generate the first layer of acoustic tokens required to synthesis per-

--- PAGE 3 ---
sonalized speech conditioned on the speaker-aware text embeddings.
Meanwhile, the first layer of acoustic tokens of the timbre prompt is
used as the prefix in AR decoding. The NAR transformer decoder
is then used to generate acoustic tokens of the other seven layers
in sequence. To predict the acoustic tokens of the i-th layer, the
transformer input is the concatenation of the speaker-aware text em-
beddings, the summation of the embedded acoustic tokens of timbre
prompt from layer 1 to layer iand the summation of the embedded
predicted acoustic tokens from layer 1 to layer i−1. In the end,
the first layer of acoustic tokens predicted by the AR transformer de-
coder and the remaining layers of acoustic tokens predicted by the
NAR transformer decoder are concatenated to form the predicted
acoustic tokens.
2.3. Training Strategy and Inference Procedure
During the training stage, for each training sample, we randomly
select 5 to 10 reference utterances spoken by the same speaker as
the sample to form the style prompt. For different training epochs,
different style prompts are randomly selected for the same training
sample for data augmentation. Different from V ALL-E which trains
two models separately, our proposed method trains the whole end-
to-end system jointly with the cross-entropy loss. Training loss is a
linear combination of an AR transformer decoder loss and an NAR
transformer decoder loss. In the AR transformer decoder, we do
not explicitly select an utterance as the timbre prompt in training,
which means all acoustic tokens of the first layer are predicted with
the teacher-forcing technique. For the NAR transformer decoder, in
each training step, we randomly sample a training stage i∈[2,8]
and randomly select a certain length of target speech prefixes as the
timbre prompt. The model is trained to maximize the probability of
the acoustic tokens in the i-th layer.
During the inference stage, we design acoustic prompts and in-
ference as follows. To generate given content for unseen speakers,
the model is given a text sentence, any number of speeches from the
target speaker as the style prompt, a short segment of speech from
the target speaker as the timbre prompt and its corresponding tran-
scription. We prepend the transcription of the timbre prompt to the
given text sentence as the text prompt. With the text prompt, the style
prompt and the timbre prompt, our proposed method generates the
acoustic tokens for the given text cloning the speaker characteristics
of the target speaker.
3. EXPERIMENTS
3.1. Training Setup
All the models are trained on LibriTTS [25], which is an open-
source multi-speaker transcribed English speech dataset. Its training
set contains approximately 580 hours of recording spoken by 2,306
speakers. To evaluate the zero-shot adaptation capability for unseen
speakers, 128 speakers from two subsets of the LibriTTS dataset
(test-clean and dev-clean) are selected as the test set, resulting in
8,078 utterances in total. A pre-trained neural audio codec model,
EnCodec2[18], is utilized as the codec model to encode the raw
waveform with 24kHz sampling rate and reconstruct the waveform
based on the predicted acoustic tokens.
In our implementation, the phoneme encoder, AR transformer
decoder and NAR transformer decoder all consist of 6 layers of
transformer blocks. Compared to the two modules in the original
V ALL-E which both consist of 12 layers of transformer blocks, our
2Implemented based on: https://github.com/facebookresearch/encodec
Fig. 3 . The structure of the acoustic decoder
proposed model has less parameters. We train all the models for
300K iterations on 4 NVIDIA A100 GPUs, with a batch size of 8
samples on each GPU. The Adam optimizer is adopted with β1=
0.9,β2= 0.98and follow the same learning rate schedule in [16].
3.2. Compared Methods
To demonstrate the performance of our proposed method, we com-
pare the following five models for zero-shot TTS synthesis. These
models are also implemented based on V ALL-E3.
VALL-E An open-source implementation3of V ALL-E [16],
which considers only a 3-second timbre prompt at the frame-level.
Proposed The proposed model, which considers both a 3-
second timbre prompt and a style prompt consisting of ten sentences.
Proposed-3s To ensure a fair comparison, we build this baseline
model, which shares the same structure and parameters as the pro-
posed model, but only uses a 3-second speech as both timbre prompt
and style prompt.
Base-S The style prompt-only baseline model, which shares the
same TTS backbone and style prompt as the proposed model, but
excludes the timbre prompt.
Base-T The timbre prompt-only baseline model, where the style
prompt is removed. That is, this model used only a 3-second speech
as the timbre prompt.
For each sample synthesis, we randomly choose other utterances
of the same speaker as the timbre prompt and the style prompt.
3.3. Subjective Evaluation
We conduct two mean opinion score (MOS) tests to measure the
zero-shot capability of different models: 1) Naturalness MOS (N-
MOS): evaluate the naturalness and prosody of the synthesized
speech; 2) Similarity MOS (S-MOS): evaluate the speaker similarity
between the synthesized speech and the ground-truth speech. We
randomly choose 20 samples from different speakers in the test set
for subjective evaluation. To exclude other interference factors, we
keep the text content and prompt speech consistent among different
models. A group of 25 listening subjects are recruited to rate the
given speeches on a scale from 1 to 5 with 1 point interval.
As shown in Table 1, our proposed method achieves the best
N-MOS of 3.886and S-MOS of 3.870, which outperforms V ALL-
E greatly in both two aspects. Compared with the V ALL-E and
the Base-T model, which both used only 3 seconds of speech as
the acoustic prompt, the Proposed-3s model achieves better perfor-
mance, especially in naturalness by a gap of over 0.12. This demon-
strates that considering the same acoustic prompt at the phoneme-
level is really helpful for learning the personal speaking style of the
3Implemented based on: https://github.com/lifeiteng/vall-e

--- PAGE 4 ---
Table 1 . The objective and subjective comparisons for zero-shot text-to-speech synthesis. We evaluate the naturalness and speaker similarity
of different models with with 95% confidence intervals.
Models Subjective Objective
N-MOS (↑) S-MOS (↑) SECS (↑)MCD (↓)
Ground Truth 4.23±0.066 - - -
V ALL-E 3.48±0.059 3 .532±0.060 0.771 8 .075
Base-S 3.456±0.055 3 .500±0.056 0.727 7 .792
Base-T 3.654±0.062 3 .646±0.060 0.764 8 .047
Proposed 3.886±0.063 3 .870±0.062 0.798 7 .715
Proposed-3s 3.778±0.063 3 .692±0.062 0.779 7 .765
Table 2 . The objective evaluation results of V ALL-E and proposed
method when using different length of prompt in inference. The
average duration of sentences is about 6 seconds.
Models SECS (↑)MCD (↓)
V ALL-E w/ 3s 0.771 8 .075
V ALL-E w/ 6s 0.774 8 .177
Proposed w/ 1 sent (3s) 0.779 7 .765
Proposed w/ 5 sent (30s) 0.795 7 .743
Proposed w/ 10 sent (1min) 0.798 7 .715
Proposed w/ 20 sent (2min) 0.798 7 .702
target speaker and improving the naturalness and speaker similar-
ity of the synthesized speech without introducing additional inputs.
Our proposed model also makes further improvement compared with
the Proposed-3s model by increasing the number of sentences in
the style prompt, indicating the ability of enhancing the quality of
zero-shot TTS by utilizing more reference speeches from the target
speaker . This ability is not available in previous language model-
based TTS models, and it allows our proposed method to have a
higher performance upper bound compared to V ALL-E. Our pro-
posed model also achieves superior performance than not only Base-
S that just considers the style prompt, but also Base-T that solitar-
ily considers the timbre prompt. It demonstrates that modeling the
speaker characteristics from different scales can improve the natural-
ness and speaker similarity of the synthesized speech. In addition,
it is observed that although Base-S also uses ten sentences as the
style prompt, it achieved the lowest score in both two evaluations.
A possible reason is that removing the timbre prompt as the pre-
fix in the decoder affects the stability of the decoding, resulting in
some synthesized speeches are poor in intelligibility. Moreover, we
added a comparison with YourTTS [12], through the ABX prefer-
ene test. Our proposed model showed a 57.3%preference rate over
YourTTS’s 33.6%, demonstrating its effectiveness.
3.4. Objective Evaluation
To measure the naturalness and speaker similarity of synthesized
speech objectively, we calculate mel-cepstrum distortion (MCD) and
Speaker Encoder Cosine Similarity (SECS) as the metrics following
[12, 13]. Since the lengths of the predicted and ground-truth speech
may be different, we first apply dynamic time warping (DTW) to de-
rive the alignment relationships between the two mel-spectrograms.
Then, we compute the minimum MCD by aligning the two mel-
spectrograms. For the speaker similarity, we use the speaker en-
coder of the Resemblyzer package4to compute the SECS between
the ground-truth speech and synthesized speech. The value ranges
from 0 to 1, where a large value indicates a higher similarity.
The evaluation results of different models on the test set are
shown in Table 1. It is observed that our proposed model out-
4Implemented based on: https://github.com/resemble-ai/Resemblyzerperforms the baselines in all objective evaluation metrics. The
results indicate that our proposed model can improve the quality and
speaker similarity of synthesized speech.
3.5. Investigation
To investigate the impact of the prompt with different lengths, we
adjust the length of the acoustic prompt and style prompt for V ALL-
E and the proposed model, respectively. For V ALL-E, constrained
by the structure of the decoder-only language model, we randomly
select two utterances of 3s/6s as the prompts for each speaker. We
also evaluate our proposed model with various numbers of speech as
the style prompt, including 1 sentence, 5 sentences, 10 sentences and
20 sentences. The timbre prompt is fixed to 3-second speech as men-
tioned above. In particular, when the style prompt consists of only
one sentence, the proposed model only uses a 3-second speech as
both the timbre prompt and style prompt. We evaluate these models
with the two objective metrics as described before.
Table 2 shows the performance comparison among the different
lengths of the acoustic prompt. It is observed that the V ALL-E with
an acoustic prompt of 6 seconds speech gets the SECS result close
to the proposed method with only one sentence as style prompt, but
there is a significant gap with the proposed in MCD. It demonstrates
that modeling the personal speaking style of the target speaker at
phoneme-level helps generate speech that is close to the ground-
truth. By comparing different lengths of the style prompt, we can see
our proposed model is able to generate more similar speech when the
number of sentences in style prompt increases.
4. CONCLUSIONS
In this paper, we propose a language model-based zero-shot TTS
model to utilize multi-scale acoustic prompts to capture both
the timbre and personal speaking style of the target speaker. A
speaker-aware text encoder is utilized to model the speaking style
at phoneme-level from arbitrary-length style prompt. A language
model-based acoustic decoder is used to preserve a specified tim-
bre by considering the timbre prompt at frame-level. Experimental
results demonstrate that our proposed approach could significantly
improve the naturalness and speaker similarity of the synthesized
speech, even when only using 3-second speech as both style prompt
and timbre prompt. In addition, our proposed model can enhance the
quality of zero-shot TTS by increasing the number of sentences in
style prompt, when there are multiple sentences of the target speaker
are available during inference.
Acknowledgement : This work is supported by National Natural
Science Foundation of China (62076144), National Social Science
Foundation of China (13&ZD189), Shenzhen Science and Technol-
ogy Program (WDZC20220816140515001, JCYJ202208181010140
30) and Shenzhen Key Laboratory of next generation interactive me-
dia innovative technology (ZDSYS20210623092001004).

--- PAGE 5 ---
5. REFERENCES
[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, Rj Skerrv-Ryan, et al., “Natural TTS synthe-
sis by conditioning waveNet on mel spectrogram predictions,”
inInternational Conference on Acoustics, Speech and Signal
Processing (ICASSP) . IEEE, 2018, pp. 4779–4783.
[2] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao,
and Tie-Yan Liu, “Fastspeech 2: Fast and high-quality end-to-
end text to speech,” in International Conference on Learning
Representations , 2020.
[3] Mingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun, Sheng Zhao,
and Tao Qin, “MultiSpeech: Multi-Speaker Text to Speech
with Transformer,” in Proc. Interspeech 2020 , 2020, pp. 4024–
4028.
[4] Jaehyeon Kim, Jungil Kong, and Juhee Son, “Conditional
variational autoencoder with adversarial learning for end-to-
end text-to-speech,” in International Conference on Machine
Learning . PMLR, 2021, pp. 5530–5540.
[5] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu, “A survey
on neural speech synthesis,” arXiv preprint arXiv:2106.15561 ,
2021.
[6] Yutian Chen, Yannis Assael, Brendan Shillingford, David Bud-
den, Scott Reed, Heiga Zen, Quan Wang, Luis C Cobo, An-
drew Trask, Ben Laurie, et al., “Sample efficient adaptive text-
to-speech,” in International Conference on Learning Repre-
sentations , 2018.
[7] Zvi Kons, Slava Shechtman, Alex Sorin, Carmel Rabinovitz,
and Ron Hoory, “High quality, lightweight and adaptable TTS
using LPCNet,” in Proc. Interspeech 2019 , 2019, pp. 176–180.
[8] Henry B Moss, Vatsal Aggarwal, Nishant Prateek, Javier
Gonz ´alez, and Roberto Barra-Chicote, “BOFFIN TTS: Few-
shot speaker adaptation by bayesian optimization,” in Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2020, pp. 7639–7643.
[9] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Tie-
Yan Liu, et al., “Adaspeech: Adaptive text to speech for custom
voice,” in International Conference on Learning Representa-
tions , 2020.
[10] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen,
Fei Ren, zhifeng Chen, Patrick Nguyen, Ruoming Pang, Igna-
cio Lopez Moreno, and Yonghui Wu, “Transfer learning from
speaker verification to multispeaker text-to-speech synthesis,”
inAdvances in Neural Information Processing Systems , 2018,
vol. 31.
[11] Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin
Wang, Nanxin Chen, and Junichi Yamagishi, “Zero-shot multi-
speaker text-to-speech with state-of-the-art neural speaker em-
beddings,” in International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2020, pp. 6184–6188.
[12] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-
naldo Candido Junior, Eren G ¨olge, and Moacir A Ponti,
“YourTTS: Towards zero-shot multi-speaker tts and zero-shot
voice conversion for everyone,” in International Conference
on Machine Learning . PMLR, 2022, pp. 2709–2720.
[13] Seungwoo Choi, Seungju Han, Dongyoung Kim, and Sungjoo
Ha, “Attentron: Few-Shot Text-to-Speech Utilizing Attention-
Based Variable-Length Embedding,” in Proc. Interspeech
2020 , 2020, pp. 2007–2011.[14] Yixuan Zhou, Changhe Song, Xiang Li, Luwen Zhang, Zhiy-
ong Wu, Yanyao Bian, Dan Su, and Helen Meng, “Content-
Dependent Fine-Grained Speaker Embedding for Zero-Shot
Speaker Adaptation in Text-to-Speech Synthesis,” in Proc. In-
terspeech 2022 , 2022, pp. 2573–2577.
[15] Zal ´an Borsos, Rapha ¨el Marinier, Damien Vincent, Eugene
Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek,
Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil
Zeghidour, “AudioLM: A language modeling approach to au-
dio generation,” IEEE/ACM Transactions on Audio, Speech,
and Language Processing , vol. 31, pp. 2523–2533, 2023.
[16] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long
Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang,
Jinyu Li, et al., “Neural codec language models are zero-shot
text to speech synthesizers,” arXiv preprint arXiv:2301.02111 ,
2023.
[17] Eugene Kharitonov, Damien Vincent, Zal ´an Borsos, Rapha ¨el
Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco
Tagliasacchi, and Neil Zeghidour, “Speak, read and prompt:
High-fidelity text-to-speech with minimal supervision,” arXiv
preprint arXiv:2302.03540 , 2023.
[18] Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and Yossi
Adi, “High fidelity neural audio compression,” arXiv preprint
arXiv:2210.13438 , 2022.
[19] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan
Skoglund, and Marco Tagliasacchi, “SoundStream: An end-to-
end neural audio codec,” IEEE/ACM Transactions on Audio,
Speech, and Language Processing , vol. 30, pp. 495–507, 2021.
[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al., “Lan-
guage models are few-shot learners,” Advances in neural in-
formation processing systems , vol. 33, pp. 1877–1901, 2020.
[21] Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Xixin Wu,
Shiyin Kang, and Helen Meng, “MSStyleTTS: Multi-scale
style modeling with hierarchical context information for ex-
pressive speech synthesis,” IEEE/ACM Transactions on Audio,
Speech, and Language Processing , vol. 31, pp. 3290–3303,
2023.
[22] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang,
Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang,
Xiang Yin, et al., “Mega-TTS: Zero-shot text-to-speech
at scale with intrinsic inductive bias,” arXiv preprint
arXiv:2306.03509 , 2023.
[23] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei
He, Tao Qin, Sheng Zhao, and Jiang Bian, “NaturalSpeech 2:
Latent diffusion models are natural and zero-shot speech and
singing synthesizers,” arXiv preprint arXiv:2304.09116 , 2023.
[24] Xiang Li, Changhe Song, Jingbei Li, Zhiyong Wu, Jia Jia, and
Helen Meng, “Towards multi-scale style control for expressive
speech synthesis,” in Proc. Interspeech 2021 , 2021, pp. 4673–
4677.
[25] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss,
Ye Jia, Zhifeng Chen, and Yonghui Wu, “LibriTTS: A corpus
derived from libriSpeech for text-to-speech,” in Proc. Inter-
speech 2019 , 2019, pp. 1526–1530.
