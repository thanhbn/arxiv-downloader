# 2102.08597.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/hypercomplex/2102.08597.pdf
# Kích thước tệp: 1252310 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
VƯỢT QUA CÁC LỚP KẾT NỐI ĐẦY ĐỦ VỚI
QUATERNIONS: THAM SỐ HÓA CÁC PHÉP NHÂN SIÊU-
PHỨC VỚI 1/n THAM SỐ
Aston Zhang†, Yi Tay‡, Shuai Zhang†, Alvin Chan/
Anh Tuan Luu., Siu Cheung Hui†, Jie Fu
†Amazon Web Services AI
‡Google Research
ETH Zürich
/NTU, Singapore
.VinAI Research
Mila, Université de Montréal
az@astonzhang.com

TÓM TẮT
Các công trình gần đây đã chứng minh sự thành công hợp lý của việc học biểu diễn
trong không gian siêu phức. Cụ thể, "các lớp kết nối đầy đủ với Quaternions"
(số siêu phức 4D), thay thế các phép nhân ma trận giá trị thực trong các lớp kết nối
đầy đủ bằng tích Hamilton của Quaternions, vừa tận hưởng việc tiết kiệm tham số
chỉ với 1/4 tham số có thể học được và đạt được hiệu suất tương đương trong các
ứng dụng khác nhau. Tuy nhiên, một điểm quan trọng cần lưu ý là không gian siêu
phức chỉ tồn tại ở rất ít kích thước được định nghĩa trước (4D, 8D, và 16D). Điều
này hạn chế tính linh hoạt của các mô hình tận dụng phép nhân siêu phức. Để giải
quyết vấn đề này, chúng tôi đề xuất tham số hóa các phép nhân siêu phức, cho phép
các mô hình học các quy tắc nhân từ dữ liệu bất kể các quy tắc đó có được định
nghĩa trước hay không. Kết quả là, phương pháp của chúng tôi không chỉ bao gồm
tích Hamilton, mà còn học cách hoạt động trên bất kỳ không gian siêu phức nD tùy
ý nào, cung cấp tính linh hoạt kiến trúc hơn bằng cách sử dụng tùy ý 1/n tham số
có thể học được so với phần tương ứng của lớp kết nối đầy đủ. Các thí nghiệm ứng
dụng cho các mô hình LSTM và Transformer trên suy luận ngôn ngữ tự nhiên, dịch
máy, chuyển đổi phong cách văn bản, và thỏa thuận động từ chủ ngữ chứng minh
tính linh hoạt kiến trúc và hiệu quả của phương pháp được đề xuất.

1 GIỚI THIỆU
Quaternion là một số siêu phức 4D với một thành phần thực và ba thành phần ảo. Tích Hamilton
là phép nhân siêu phức của hai Quaternions. Các công trình gần đây trong không gian Quaternion
và tích Hamilton đã chứng minh sự thành công hợp lý (Parcollet et al., 2018b; 2019; Tay et al.,
2019). Đáng chú ý, tích Hamilton tận hưởng việc tiết kiệm tham số với 1/4 tham số có thể học được
so với phép nhân ma trận giá trị thực. Nó cũng cho phép học biểu diễn hiệu quả bằng cách mô hình
hóa các tương tác giữa các thành phần thực và ảo.

Một trong những đặc tính hấp dẫn của các mô hình Quaternion là khả năng ứng dụng cao và tính
hữu ích toàn diện đối với một trong những lớp phổ biến nhất trong học sâu, tức là lớp kết nối đầy
đủ (hoặc feed-forward). Cụ thể, "các lớp kết nối đầy đủ với Quaternions" thay thế các phép nhân
ma trận giá trị thực trong các lớp kết nối đầy đủ bằng tích Hamilton của Quaternions, tận hưởng
việc tiết kiệm tham số chỉ với 1/4 tham số có thể học được và đạt được hiệu suất tương đương với
các phần tương ứng lớp kết nối đầy đủ (Parcollet et al., 2018b; 2019; Tay et al., 2019).

Lớp kết nối đầy đủ là một trong những thành phần chiếm ưu thế nhất trong tài liệu học sâu hiện
tại (Goodfellow et al., 2016; Zhang et al., 2020). Tính phổ biến của nó không thể được đánh giá
thấp, cho thấy tính trung tâm của nó đối với nhiều khối xây dựng cốt lõi trong nghiên cứu mạng
nơ-ron. Cho việc áp dụng rộng rãi các lớp kết nối đầy đủ, ví dụ, trong mạng LSTM (Hochreiter
& Schmidhuber, 1997) và mô hình Transformer (Vaswani et al., 2017), việc có tính linh hoạt để
cân bằng giữa tiết kiệm tham số và hiệu quả có thể cực kỳ hữu ích cho nhiều ứng dụng thực tế.

Thật không may, không gian siêu phức chỉ tồn tại ở 4D (Quaternions), 8D (Octonions), và 16D
(Sedenions), điều này khái quát hóa không gian phức 2D (Rishiyur, 2006). Hơn nữa, các toán tử
tùy chỉnh được yêu cầu tại mỗi kích thước siêu phức. Ví dụ, tích Hamilton là phép nhân siêu phức
trong không gian siêu phức 4D. Do đó, không có toán tử nào trong không gian siêu phức được
định nghĩa trước đó phù hợp cho các ứng dụng mà thích giảm tham số xuống 1/n, trong đó n≠4;8;16.

Xem xét hạn chế kiến trúc do rất ít lựa chọn của những không gian siêu phức hiện tại, chúng tôi đề
xuất tham số hóa các phép nhân siêu phức, tức là học các tương tác thành phần thực và ảo từ dữ liệu
theo cách có thể phân biệt được. Về cơ bản, phương pháp của chúng tôi có thể hoạt động trên một
không gian siêu phức nD tùy ý, ngoài việc bao gồm những quy tắc nhân siêu phức được định nghĩa
trước, tạo thuận lợi cho việc sử dụng tùy ý 1/n tham số có thể học được trong khi duy trì tính biểu
đạt. Trong thực tế, siêu tham số n có thể được chỉ định linh hoạt hoặc điều chỉnh bởi người dùng
dựa trên các ứng dụng.

Cụ thể, đóng góp chính của chúng tôi là một mô-đun mới tham số hóa và tổng quát hóa phép nhân
siêu phức bằng cách học các tương tác thành phần thực và ảo, tức là các quy tắc nhân, từ dữ liệu.
Phương pháp của chúng tôi, mà chúng tôi gọi là lớp nhân siêu phức được tham số hóa, được đặc
trưng bởi một tổng các tích Kronecker khái quát hóa các tích ngoài vector thành các chiều cao hơn
trong không gian thực. Để chứng minh khả năng ứng dụng, chúng tôi trang bị hai mô hình được
thiết lập tốt (LSTM và Transformer) với phương pháp được đề xuất. Chúng tôi tiến hành các thí
nghiệm mở rộng trên các tác vụ khác nhau, tức là suy luận ngôn ngữ tự nhiên cho mạng LSTM và
dịch máy cho mô hình Transformer. Ngoài ra, chúng tôi thực hiện thêm các thí nghiệm về chuyển
đổi phong cách văn bản và các tác vụ thỏa thuận động từ chủ ngữ. Tổng thể, phương pháp của
chúng tôi đã chứng minh tính linh hoạt kiến trúc thông qua các thiết lập thí nghiệm khác nhau, nơi
nó thường có thể sử dụng một phần của các tham số có thể học được với sự suy giảm tối thiểu hoặc
cải thiện nhẹ về hiệu suất.

Các đóng góp tổng thể của công trình này được tóm tắt như sau:
• Chúng tôi đề xuất một tham số hóa mới của các phép nhân siêu phức: lớp nhân siêu phức
được tham số hóa (PHM). Lớp này có 1/n tham số có thể học được so với phần tương ứng
lớp kết nối đầy đủ, trong đó n có thể được chỉ định linh hoạt bởi người dùng. Ý tưởng chính
đằng sau các lớp PHM là học các tương tác giữa các thành phần thực và ảo, tức là các quy
tắc nhân, từ dữ liệu bằng cách sử dụng một tổng các tích Kronecker.
• Chúng tôi chứng minh khả năng ứng dụng của các lớp PHM bằng cách tận dụng chúng trong
hai kiến trúc nơ-ron chiếm ưu thế: mô hình LSTM và Transformer.
• Chúng tôi chứng minh thực nghiệm tính linh hoạt kiến trúc và hiệu quả của các lớp PHM
bằng cách tiến hành các thí nghiệm mở rộng trên năm tác vụ suy luận ngôn ngữ tự nhiên,
bảy bộ dữ liệu dịch máy, cùng với chuyển đổi phong cách văn bản và các tác vụ thỏa thuận
động từ chủ ngữ.

2 KIẾN THỨC CƠ BẢN VỀ QUATERNIONS VÀ TÍCH HAMILTON
Chúng tôi bắt đầu bằng cách giới thiệu nền tảng cho phần còn lại của bài báo. Cụ thể, chúng tôi
mô tả đại số Quaternion cùng với tích Hamilton, đây là trung tâm của phương pháp được đề xuất.

Quaternion Một Quaternion Q∈H là một số siêu phức với một thành phần thực và ba thành phần
ảo như sau:
Q = Qr + Qxi + Qyj + Qzk; (2.1)
theo đó ijk = i² = j² = k² = −1. Trong (2.1), các quy tắc nhân không giao hoán áp dụng: ij = 
k; jk = i; ki = j; ji = −k; kj = −i; ik = −j. Ở đây, Qr là thành phần thực, Qx, Qy, Qz là các
số thực đại diện cho các thành phần ảo của Quaternion Q.

Phép cộng Phép cộng của hai Quaternions được định nghĩa là
Q + P = Qr + Pr + (Qx + Px)i + (Qy + Py)j + (Qz + Pz)k;
trong đó Q và P với các chỉ số con biểu thị các thành phần thực và ảo của Quaternions Q và P.

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
tính trung tâm của nó đối với nhiều khối xây dựng cốt lõi trong nghiên cứu mạng nơ-ron. Cho việc
áp dụng rộng rãi các lớp kết nối đầy đủ, ví dụ, trong mạng LSTM (Hochreiter & Schmidhuber,
1997) và mô hình Transformer (Vaswani et al., 2017), việc có tính linh hoạt để cân bằng giữa tiết
kiệm tham số và hiệu quả có thể cực kỳ hữu ích cho nhiều ứng dụng thực tế.

Thật không may, không gian siêu phức chỉ tồn tại ở 4D (Quaternions), 8D (Octonions), và 16D
(Sedenions), điều này khái quát hóa không gian phức 2D (Rishiyur, 2006). Hơn nữa, các toán tử
tùy chỉnh được yêu cầu tại mỗi kích thước siêu phức. Ví dụ, tích Hamilton là phép nhân siêu phức
trong không gian siêu phức 4D. Do đó, không có toán tử nào trong không gian siêu phức được
định nghĩa trước đó phù hợp cho các ứng dụng mà thích giảm tham số xuống 1/n, trong đó n≠4;8;16.

Xem xét hạn chế kiến trúc do rất ít lựa chọn của những không gian siêu phức hiện tại, chúng tôi đề
xuất tham số hóa các phép nhân siêu phức, tức là học các tương tác thành phần thực và ảo từ dữ liệu
theo cách có thể phân biệt được. Về cơ bản, phương pháp của chúng tôi có thể hoạt động trên một
không gian siêu phức nD tùy ý, ngoài việc bao gồm những quy tắc nhân siêu phức được định nghĩa
trước, tạo thuận lợi cho việc sử dụng tùy ý 1/n tham số có thể học được trong khi duy trì tính biểu
đạt. Trong thực tế, siêu tham số n có thể được chỉ định linh hoạt hoặc điều chỉnh bởi người dùng
dựa trên các ứng dụng.

Cụ thể, đóng góp chính của chúng tôi là một mô-đun mới tham số hóa và tổng quát hóa phép nhân
siêu phức bằng cách học các tương tác thành phần thực và ảo, tức là các quy tắc nhân, từ dữ liệu.
Phương pháp của chúng tôi, mà chúng tôi gọi là lớp nhân siêu phức được tham số hóa, được đặc
trưng bởi một tổng các tích Kronecker khái quát hóa các tích ngoài vector thành các chiều cao hơn
trong không gian thực. Để chứng minh khả năng ứng dụng, chúng tôi trang bị hai mô hình được
thiết lập tốt (LSTM và Transformer) với phương pháp được đề xuất. Chúng tôi tiến hành các thí
nghiệm mở rộng trên các tác vụ khác nhau, tức là suy luận ngôn ngữ tự nhiên cho mạng LSTM và
dịch máy cho mô hình Transformer. Ngoài ra, chúng tôi thực hiện thêm các thí nghiệm về chuyển
đổi phong cách văn bản và các tác vụ thỏa thuận động từ chủ ngữ. Tổng thể, phương pháp của
chúng tôi đã chứng minh tính linh hoạt kiến trúc thông qua các thiết lập thí nghiệm khác nhau, nơi
nó thường có thể sử dụng một phần của các tham số có thể học được với sự suy giảm tối thiểu hoặc
cải thiện nhẹ về hiệu suất.

Các đóng góp tổng thể của công trình này được tóm tắt như sau:
• Chúng tôi đề xuất một tham số hóa mới của các phép nhân siêu phức: lớp nhân siêu phức
được tham số hóa (PHM). Lớp này có 1/n tham số có thể học được so với phần tương ứng
lớp kết nối đầy đủ, trong đó n có thể được chỉ định linh hoạt bởi người dùng. Ý tưởng chính
đằng sau các lớp PHM là học các tương tác giữa các thành phần thực và ảo, tức là các quy
tắc nhân, từ dữ liệu bằng cách sử dụng một tổng các tích Kronecker.
• Chúng tôi chứng minh khả năng ứng dụng của các lớp PHM bằng cách tận dụng chúng trong
hai kiến trúc nơ-ron chiếm ưu thế: mô hình LSTM và Transformer.
• Chúng tôi chứng minh thực nghiệm tính linh hoạt kiến trúc và hiệu quả của các lớp PHM
bằng cách tiến hành các thí nghiệm mở rộng trên năm tác vụ suy luận ngôn ngữ tự nhiên,
bảy bộ dữ liệu dịch máy, cùng với chuyển đổi phong cách văn bản và các tác vụ thỏa thuận
động từ chủ ngữ.

2 KIẾN THỨC CƠ BẢN VỀ QUATERNIONS VÀ TÍCH HAMILTON
Chúng tôi bắt đầu bằng cách giới thiệu nền tảng cho phần còn lại của bài báo. Cụ thể, chúng tôi
mô tả đại số Quaternion cùng với tích Hamilton, đây là trung tâm của phương pháp được đề xuất.

Quaternion Một Quaternion Q∈H là một số siêu phức với một thành phần thực và ba thành phần
ảo như sau:
Q = Qr + Qxi + Qyj + Qzk; (2.1)
theo đó ijk = i² = j² = k² = −1. Trong (2.1), các quy tắc nhân không giao hoán áp dụng: ij = 
k; jk = i; ki = j; ji = −k; kj = −i; ik = −j. Ở đây, Qr là thành phần thực, Qx, Qy, Qz là các
số thực đại diện cho các thành phần ảo của Quaternion Q.

Phép cộng Phép cộng của hai Quaternions được định nghĩa là
Q + P = Qr + Pr + (Qx + Px)i + (Qy + Py)j + (Qz + Pz)k;
trong đó Q và P với các chỉ số con biểu thị các thành phần thực và ảo của Quaternions Q và P.

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

Phép nhân vô hướng Bất kỳ vô hướng α nào nhân qua tất cả các thành phần:
αQ = αQr + αQxi + αQyj + αQzk.

Tích Hamilton Tích Hamilton, đại diện cho phép nhân của hai Quaternions Q và P, được định
nghĩa là
Q ⊗ P = (QrPr−QxPx−QyPy−QzPz) + (QxPr+QrPx−QzPy+QyPz)i
+ (QyPr+QzPx+QrPy−QxPz)j + (QzPr−QyPx+QxPy+QrPz)k. (2.2)

Quy tắc nhân trong (2.2) tạo ra các tương tác giữa các thành phần thực và ảo của Q và P. Lợi ích
của tích Hamilton đã được chứng minh trong các công trình gần đây nơi phép nhân ma trận trong
các lớp kết nối đầy đủ được thay thế bằng tích Hamilton: điều này giảm 75% tham số với hiệu suất
tương đương (Parcollet et al., 2018b; 2019; Tay et al., 2019).

3 THAM SỐ HÓA CÁC PHÉP NHÂN SIÊU PHỨC
Phần sau giới thiệu lớp nhân siêu phức được tham số hóa được đề xuất và trình bày chi tiết về cách
nó tham số hóa và tổng quát hóa các phép nhân trong không gian siêu phức, chẳng hạn như bao
gồm các quy tắc nhân của tích Hamilton trong (2.2).

3.1 CÁC LỚP KẾT NỐI ĐẦY ĐỦ (FC)
Trước khi đi sâu vào phương pháp được đề xuất, hãy nhớ lại lớp kết nối đầy đủ (FC) biến đổi một
đầu vào x∈Rd thành một đầu ra y∈Rk bằng
y = FC(x) = Wx + b; (3.1)
trong đó ma trận trọng số của các tham số W∈Rk×d và vector bias của các tham số b∈Rk.

Lớp FC trong (3.1) là nền tảng cho nhiều kiến trúc mạng nơ-ron hiện đại và truyền thống. Lưu ý
rằng bậc tự do cho các tham số trọng số W trong (3.1) là kd. Vì W chiếm ưu thế trong tham số hóa,
kích thước tham số của lớp FC trong (3.1) là O(kd).

3.2 CÁC LỚP NHÂN SIÊU PHỨC ĐƯỢC THAM SỐ HÓA (PHM)
Chúng tôi đề xuất lớp nhân siêu phức được tham số hóa (PHM) biến đổi một đầu vào x thành một
đầu ra y bằng
y = PHM(x) = Hx + b; (3.2)
trong đó cùng ký hiệu từ (3.1) được sử dụng nhưng tham số được thay thế H∈Rk×d được xây
dựng bởi một tổng các tích Kronecker. Để tham khảo, tích Kronecker là một tổng quát hóa của tích
ngoài vector thành các chiều cao hơn trong không gian thực. Đối với bất kỳ ma trận X∈Rm×n và
Y∈Rp×q, tích Kronecker X⊗Y là một ma trận khối:

X⊗Y = [x11Y ... x1nY]
      [  ⋮   ⋱   ⋮  ] ∈ Rmp×nq;
      [xm1Y ... xmnY]

trong đó xij là phần tử của X tại hàng thứ i và cột thứ j của nó. Lưu ý rằng ký hiệu ⊗ giữa hai
ma trận là tích Kronecker trong khi cùng ký hiệu giữa hai Quaternions có nghĩa là tích Hamilton.

Bây giờ chúng ta hãy xem lại (3.2) để giải thích H. Giả sử rằng cả k và d đều chia hết cho một
siêu tham số được người dùng định nghĩa n∈Z>0. Đối với i = 1, ..., n, ký hiệu bởi mỗi ma trận
tham số Ai∈Rn×n và Si∈Rk/n×d/n. Tham số H trong (3.2) là một tổng của n tích Kronecker:
H = ∑(i=1 to n) Ai⊗Si. (3.3)

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

[Hình 1: Minh họa của lớp PHM. Nó sử dụng một tổng các tích Kronecker của các ma trận Ai và Si
(i = 1, 2) để xây dựng H trong (3.2) (ở đây n = 2; k = 6; d = 8). Xem tốt nhất ở màu.]

[Hình 2: Các lớp PHM có thể học để thực hiện phép quay trong không gian thực 3D và tích Hamilton
trong không gian Quaternion trên các bộ dữ liệu nhân tạo.]

Như được minh họa trong Hình 1, chính các ma trận tham số Ai và Si (i = 1, ..., n) xác định bậc
tự do cho H, là kd/n + n³. Vì H chiếm ưu thế trong tham số hóa, kích thước tham số của PHM
trong (3.2) là O(kd/n), trong đó kd ≫ n⁴ được giả định: điều kiện này là nhẹ cho các vấn đề thực
tế, chẳng hạn như trong các thí nghiệm của chúng tôi (ví dụ, d = 512, k = 2048, n = 2;4;8;16).
Do đó, đối với cùng kích thước đầu vào và đầu ra, kích thước tham số của lớp PHM xấp xỉ 1/n
của lớp FC dưới các giả định nhẹ.

Lợi ích của việc giảm tham số hóa của các lớp PHM là do tái sử dụng các phần tử của cả hai ma
trận tham số Ai và Si trong tích Kronecker. Như một góc nhìn thay thế, chúng ta có thể tương
đương tái xây dựng H trong (3.3) bằng cách tái sử dụng các ma trận tham số trong các phép nhân
ma trận giá trị thực, theo sau là các phép toán khác. Do không gian hạn chế, góc nhìn phức tạp
hơn này được cung cấp trong Phụ lục A. Mặc dù đơn giản chỉ đặt H = A1⊗S1 có thể tiết kiệm
tham số hơn nữa, nó không tổng quát hóa các phép nhân siêu phức do đó nằm ngoài phạm vi.

Để chỉ ra rằng các lớp PHM có thể học để thực hiện các phép toán liên quan đến phép nhân được
định nghĩa trước trong thực tế, chúng tôi thực hiện các thí nghiệm để học các phép quay trong
không gian thực 3D bằng cách sử dụng lớp PHM. Sử dụng ma trận quay W∈R³×³ chúng tôi tạo
ra một bộ dữ liệu nhân tạo {(xi∈R³, yi∈R³)}, trong đó yi được tạo ra thông qua phép quay 3D
của đầu vào: yi = Wxi. Hình 2(a) cho thấy rằng mất mát hội tụ về không: lớp PHM có thể học
một phép quay duy nhất của một đối tượng trong không gian thực 3D.

Trong phần sau, chúng tôi chỉ ra cách lớp PHM được đề xuất bao gồm và tổng quát hóa cả phép
nhân siêu phức và phép nhân ma trận giá trị thực.

3.3 BAO GỒM CÁC PHÉP NHÂN SIÊU PHỨC
Đầu tiên, chúng tôi khám phá cách lớp PHM kết nối với phép nhân siêu phức. Để minh họa, chúng
ta hãy lấy tích Hamilton của hai Quaternions Q và P trong (2.2) làm ví dụ, có thể được viết lại là

[QrPr−QxPx−QyPy−QzPz]   [Qr  −Qx  −Qy  −Qz] [Pr]
[QxPr+QrPx−QzPy+QyPz]   [Qx   Qr  −Qz   Qy] [Px]
[QyPr+QzPx+QrPy−QxPz] = [Qy   Qz   Qr  −Qx] [Py];  (3.4)
[QzPr−QyPx+QxPy+QrPz]   [Qz  −Qy   Qx   Qr] [Pz]

trong đó 4 phần tử đầu ra là các giá trị thực cho cơ sở đơn vị Quaternion [1, i, j, k]ᵀ. Lưu ý rằng
đối với các mô hình tận dụng tích Hamilton của Quaternions (Parcollet et al., 2018b; 2019; Tay et
al., 2019), các thành phần Qr, Qx, Qy, Qz của (3.4) là các tham số có thể học được trong khi các
thành phần Pr, Px, Py, Pz là các đầu vào lớp. Trong thực tế, một lớp như vậy thường có nhiều
hơn 4 đầu vào (d > 4). Để áp dụng tích Hamilton, tất cả các đầu vào được chia đều thành 4 phân
đoạn (Pr, Px, Py, Pz) của vector đầu vào bên phải của (3.4). Khi đó mỗi thành phần trong ma
trận bên trái của (3.4) có thể là một ma trận khối (i) trong đó tất cả các phần tử có cùng giá trị;
(ii) có hình dạng được căn chỉnh với độ dài đầu vào d và độ dài đầu ra k của lớp. Điều đáng chú
ý là ma trận 4×4 bên trái của (3.4) có thể được viết lại là một tổng của 4 tích Kronecker:

[1  0  0  0]              [0 −1  0  0]              [0  0 −1  0]              [0  0  0 −1]
[0  1  0  0]              [1  0  0  0]              [0  0  0  1]              [0  0  1  0]
[0  0  1  0] ⊗ [Qr] +     [0  0  0 −1] ⊗ [Qx] +     [1  0  0  0] ⊗ [Qy] +     [0  1  0  0] ⊗ [Qz].
[0  0  0  1]              [0  0  1  0]              [0 −1  0  0]              [1  0  0  0]
   A1           S1            A2           S2            A3           S3            A4           S4
                                                                                                   (3.5)

Theo (3.5), khi n = 4, lớp PHM có thể được học để biểu diễn tích Hamilton của Quaternions. Cụ
thể, các ma trận A1, ..., A4 trong (3.3) tham số hóa bốn ma trận được tạo thành từ −1, 0, 1 trong
(3.5) phản ánh các tương tác giữa các thành phần thực và ảo của Quaternions, đó là quy tắc của
tích Hamilton. Các "ma trận" một phần tử S1, ..., S4 trong (3.3) bằng với các thành phần có thể
học được Qr, Qx, Qy, Qz trong (3.4). Hình 2(b) cho thấy rằng các lớp PHM có thể học quy tắc
của tích Hamilton trên dữ liệu nhân tạo. Tương tự, các phép nhân siêu phức của Octonions hoặc
Sedenions cũng có thể được học bởi lớp PHM khi n được đặt thành 8 hoặc 16.

3.4 BAO GỒM CÁC PHÉP NHÂN MA TRẬN GIÁ TRỊ THỰC
Tiếp theo, chúng tôi chỉ ra cách lớp PHM bao gồm phép nhân ma trận trong không gian thực. Nói
cách khác, lớp PHM là một tổng quát hóa của lớp FC thông qua siêu tham số n. Để giải thích, tham
khảo (3.2), khi n = 1, H = A1⊗S1 = aS1, trong đó vô hướng a là phần tử duy nhất của ma trận
1×1 A1 và S1∈Rk×d. Vì việc học a và S1 riêng biệt tương đương với việc học phép nhân của
chúng cùng nhau, vô hướng a có thể bỏ qua, đó là học ma trận trọng số duy nhất trong lớp FC.
Do đó, lớp PHM thoái hóa thành lớp FC khi n = 1.

3.5 TỔNG QUÁT HÓA CÁC PHÉP NHÂN SIÊU PHỨC
Mặc dù việc tái sử dụng tham số bằng cách phân chia theo thành phần trong không gian Quaternion
đã chứng minh thành công (Parcollet et al., 2018b; Zhu et al., 2018; Parcollet et al., 2019; Tay et
al., 2019), một vấn đề chính là không gian siêu phức chỉ tồn tại ở rất ít kích thước được định nghĩa
trước, chẳng hạn như 4D (Quaternions), 8D (Octonions), và 16D (Sedenions). Trong bối cảnh
không gian siêu phức, các quy tắc nhân chuyên biệt, chẳng hạn như tích Hamilton, phải được xây
dựng và mã hóa trong mạng như một thiên kiến quy nạp cố định. Như đã mô tả trong Phần 1, rất
ít lựa chọn trong không gian siêu phức hiện tại hạn chế tính linh hoạt của các mạng tận dụng phép
nhân siêu phức.

Hoàn toàn tương phản với việc dựa vào các quy tắc toán học được định nghĩa trước trên các lựa
chọn kích thước hạn chế, lớp PHM coi kích thước n (số lượng tích Kronecker) như một siêu tham
số có thể điều chỉnh và học các quy tắc nhân chuyên biệt đó từ dữ liệu, như được thể hiện trong
các ma trận được tham số hóa Ai (i = 1, ..., n) trong (3.3). Một mặt, lớp PHM có thể biểu diễn
các phép nhân siêu phức khi Ai được đặt để phản ánh những quy tắc nhân được định nghĩa trước
đó trong không gian siêu phức. Mặt khác, lớp PHM có thể được xem như một dạng có thể huấn
luyện và được tham số hóa của các phép nhân siêu phức nD, trong đó n có thể là các giá trị khác
ngoài 4, 8, hoặc 16. Do đó, lớp PHM tổng quát hóa các phép nhân trong không gian siêu phức.
Vì n có thể là 1, lớp PHM cũng cung cấp một cách gọn gàng để nối phép nhân giữa cả không gian
thực và không gian siêu phức.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

4 CÁC MÔ HÌNH NƠRON VỚI CÁC LỚP PHM
Để chứng minh khả năng ứng dụng của các lớp PHM, chúng tôi phát triển PHM-LSTM và PHM-
Transformer bằng cách trang bị hai mô hình mạng nơron phổ biến, LSTM và Transformer, với các
lớp PHM.

4.1 PHM-LSTM
Các mạng nơron tuần hoàn như LSTM (Hochreiter & Schmidhuber, 1997) là các mạng tuần hoàn
có cổng nơi các hàm cổng được tham số hóa bởi các biến đổi tuyến tính. Chúng tôi giới thiệu PHM-
LSTM, thay thế các biến đổi tuyến tính đó trong LSTM bằng các lớp PHM:

yt = PHM(xt) + PHM(ht−1) + b
ft, it, ot, x't = ℑ(yt)
ct = σ(ft)ct−1 + σ(it)t(x't)
ht = otct;

trong đó σ là hàm kích hoạt sigmoid, t là hàm kích hoạt tanh, ℑ: R¹×d → R⁴×d/4 là một phép
chia bốn chiều trên chiều cuối cùng, và ct, ht là trạng thái ô và trạng thái ẩn của đơn vị PHM-LSTM
tại bất kỳ bước thời gian t nào.

4.2 PHM-TRANSFORMER
Transformer là một kiến trúc mạng nơron xếp chồng khai thác mạnh mẽ các biến đổi tuyến tính
(Vaswani et al., 2017). Mỗi lớp tự chú ý bao gồm các biến đổi tuyến tính Q(query), K(key), V(value),
cùng với nhiều đầu. Mỗi khối Transformer cũng có một mạng feed-forward theo vị trí được tạo
thành từ hai lớp FC. Vì phần lớn các tham số Transformer xuất phát từ các biến đổi tuyến tính hoặc
các lớp FC, chúng tôi giới thiệu PHM-Transformer để thay thế tất cả các biến đổi tuyến tính hoặc
các lớp FC bằng các lớp PHM. Mô-đun tự chú ý đầu đơn được viết lại là:

Q, K, V = ℑ(PHM(X))
A = softmax(QK^T/√dk)V;

trong đó dk là kích thước khóa, ℑ: R¹×d → R³×d/3 là một phép chia ba chiều trên chiều cuối cùng,
X là chuỗi đầu vào, và A là biểu diễn tự chú ý. Đối với chú ý đa đầu, việc sử dụng các lớp PHM
cũng cho phép chia sẻ trọng số không chỉ giữa các biến đổi tuyến tính của Q, K, V mà còn giữa
các biến đổi tuyến tính của nhiều đầu:

X = PHM([H1; ...; HNh]);

trong đó Nh là số lượng đầu và (·; ·) là phép nối theo cột. Cuối cùng, mạng feed-forward theo vị
trí hiện được định nghĩa là

Y = PHM(ReLU(PHM(X)));

biến đổi X bằng hai lớp PHM.

5 CÁC THÍ NGHIỆM
Để tham khảo, trong lĩnh vực học biểu diễn sử dụng các phép nhân siêu phức, các mạng nơron tích
chập Quaternion (Zhu et al., 2018), mạng nơron tuần hoàn Quaternion (Parcollet et al., 2018a), và
Quaternion Transformers (Tay et al., 2019) đều chỉ so sánh với các phần tương ứng giá trị thực. Do
đó, để nhất quán với phần còn lại của tài liệu, chúng tôi đánh giá PHM-LSTM và PHM-Transformer
được trang bị với các lớp PHM, và so sánh chúng với Quaternion LSTM, Quaternion Transformer,
LSTM giá trị thực, hoặc Transformer giá trị thực. Cả Quaternion LSTM và Quaternion Transformer
đều thay thế các biến đổi tuyến tính bằng tích Hamilton của Quaternions.

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

Bảng 1: Kết quả thí nghiệm suy luận ngôn ngữ tự nhiên (độ chính xác) trên năm bộ dữ liệu khác
nhau. PHM-LSTM giảm các tham số của mô hình LSTM chuẩn và cải thiện hoặc phù hợp một phần
với hiệu suất trên bốn trong số năm bộ dữ liệu.

Mô hình         #Tham số   MNLI           QNLI    SNLI    DNLI    SciTail
LSTM            721K       71.82/71.89    84.44   84.18   85.16   74.36
Quaternion LSTM 180K (-75.0%) 71.57/72.19 84.73   84.21   86.45   75.58
PHM-LSTM (n=2)  361K (-49.9%) 71.82/72.08 84.39   84.38   85.77   77.47
PHM-LSTM (n=5)  146K (-79.7%) 71.80/71.77 83.87   84.58   86.47   74.64
PHM-LSTM (n=10) 81K (-88.7%)  71.59/71.59 84.25   84.40   86.21   77.84

Để chứng minh tính linh hoạt kiến trúc và hiệu quả, chúng tôi đánh giá các thiết lập khác nhau của
PHM-LSTM và PHM-Transformer để chỉ ra rằng cho phép các lựa chọn linh hoạt của siêu tham số
n trong lớp PHM có thể dẫn đến hiệu suất hiệu quả hơn. Chi tiết về thiết lập cho các thí nghiệm
được cung cấp trong Phụ lục B.

5.1 SUY LUẬN NGÔN NGỮ TỰ NHIÊN
Tác vụ suy luận ngôn ngữ tự nhiên là xác định mối quan hệ logic giữa hai chuỗi văn bản (MacCartney,
2009). Đây là một tác vụ cơ bản liên quan đến hiểu ngôn ngữ. Để kết thúc, chúng phục vụ như một
tiêu chuẩn thích hợp để đánh giá các mô hình tuần hoàn.

Chúng tôi chạy thí nghiệm trên năm bộ dữ liệu: (i) MultiNLI (Williams et al., 2017), (ii) QNLI
(Quora) (Wang et al., 2017), (iii) SNLI (Bowman et al., 2015), (iv) Dialogue NLI (Welleck et al.,
2018), và (v) SciTail (Science Entailment) (Khot et al., 2018). Bảng 1 báo cáo kết quả trên tất cả
các bộ dữ liệu này. Tổng thể, các kết quả như vậy cho thấy rằng lớp PHM không chỉ có thể giảm
các tham số mà còn cải thiện hiệu suất với các lựa chọn linh hoạt của n (bốn trong số năm bộ dữ
liệu cho thấy cải thiện hợp lý hoặc phù hợp một phần). Ngoại lệ duy nhất là trên bộ dữ liệu QNLI,
nơi sự giảm hiệu suất là biên (<1%). Điều này vẫn khá tốt xem xét việc tiết kiệm tham số: chi phí
tham số hóa của PHM-LSTM theo thứ tự O(1/n) của LSTM chuẩn, trong đó các thiết lập n = 5
và n = 10 không lấy các giá trị lũy thừa của 2. Như được nêu chi tiết trong Phụ lục B, vì chúng tôi
sử dụng nhúng GloVe 300D (Pennington et al., 2014) để biểu diễn các token đầu vào, chúng tôi
chọn các bội số của 5 thay vì 4 để dễ chia hết. Cũng đáng chú ý là trên các bộ dữ liệu SNLI,
Dialogue NLI, và SciTail, tất cả các biến thể PHM-LSTM đều vượt trội hơn mô hình LSTM chuẩn.
Chúng tôi nghĩ rằng các tính chất tái sử dụng phần tử của phép toán tích Kronecker, ngoài việc
học chia sẻ các thành phần được tái sử dụng đó giữa các hàm cổng tuần hoàn, có thể đóng góp vào
cả biểu diễn hiệu quả và hiệu quả.

5.2 DỊCH MÁY
Dịch máy liên quan đến việc dịch giữa các cặp ngôn ngữ nguồn-đích. Để kết thúc, các mô hình
chuyển đổi chuỗi là trung tâm của lĩnh vực vấn đề này. Trong thí nghiệm này, mục tiêu chính là
so sánh PHM-Transformer với các mô hình Transformer chuẩn và Quaternion.

Chúng tôi chạy thí nghiệm trên bảy bộ dữ liệu: (i) IWSLT'15 English-Vietnamese (En-Vi), (ii)
IWSLT'17 English-Indonesian (En-Id), (iii) IWSLT'14 German-English (De-En), (iv) IWSLT'14
Romanian-English (Ro-En), (v) WMT'18 English-Estonian (En-Et), (vi) Setimes English-Macedonian
(En-Mk), và (vii) WMT'18 English-Romanian (En-Ro).

Bảng 2 báo cáo kết quả của chúng tôi về các tác vụ dịch máy. Tổng thể, những kết quả thực nghiệm
này với các thiết lập khác nhau chứng minh tính linh hoạt kiến trúc và hiệu quả của tham số hóa
phép nhân siêu phức. Đầu tiên và quan trọng nhất, trên sáu trong số bảy tiêu chuẩn, PHM-Transformer
tại n = 4 tạo ra những cải thiện hợp lý so với Quaternion Transformer, biểu thị rằng tham số hóa
các phép nhân siêu phức bằng cách học từ dữ liệu có thể hiệu quả hơn việc định nghĩa trước các
quy tắc tích Hamilton một cách toán học. Thứ hai, mặc dù tăng n dẫn đến tiết kiệm tham số nhiều
hơn, chúng tôi quan sát rằng tăng n lên đến 16 không gây ra sự suy giảm hiệu suất đáng kể trên
các bộ dữ liệu như En-Vi. Thứ ba, đối với hầu hết các bộ dữ liệu, ngay cả với việc tiết kiệm tham
số đáng kể, chúng tôi thấy rằng sự giảm điểm BLEU chủ yếu có thể quản lý được (≈1–3 điểm BLEU).

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

Bảng 2: Kết quả thí nghiệm dịch máy (BLEU) trên bảy bộ dữ liệu khác nhau. Ký hiệu † đại diện
cho việc tái tỷ lệ các tham số với hệ số 2 bằng cách nhân đôi kích thước ẩn. PHM-Transformer không
mất nhiều hiệu suất mặc dù tận hưởng việc tiết kiệm tham số. Tái tỷ lệ có thể dẫn đến cải thiện
hiệu suất.

Mô hình            #Tham số   En-Vi   En-Id   De-En   Ro-En   En-Et   En-Mk   En-Ro
Transformer (Tm)    44M       28.43   47.40   36.68   34.60   14.17   13.96   22.79
Quaternion Tm       11M (-75.0%) 28.00   42.22   32.83   30.53   13.10   13.67   18.50
PHM-Tm n=2         22M (-50.0%) 29.25   46.32   35.52   33.40   14.98   13.60   21.73
PHM-Tm n=4         11M (-75.0%) 29.13   44.13   35.53   32.74   14.11   13.01   21.19
PHM-Tm n=8         5.5M (-87.5%) 29.34   40.81   34.16   31.88   13.08   12.95   21.66
PHM-Tm n=16        2.9M (-93.4%) 29.04   33.48   33.89   31.53   12.15   11.97   19.63
PHM-Tm† n=2        44M         29.54   49.05   34.32   33.88   14.05   14.41   22.18
PHM-Tm† n=4        22M (-50.0%) 29.17   46.24   34.86   33.80   14.43   13.78   21.91
PHM-Tm† n=8        11M (-75.0%) 29.47   43.49   34.71   32.59   13.75   13.78   21.43

Bảng 3: Thời gian huấn luyện (giây mỗi 100 bước) và thời gian suy luận (giây để giải mã các bộ
kiểm tra) với kích thước chùm là 4 và phạt độ dài là 0.6 trên bộ dữ liệu IWSLT'14 German-English.

Mô hình          Transformer (Tm)   Quaternion Tm   PHM-Tm (n=4)   PHM-Tm (n=8)
Thời gian huấn luyện   7.61           8.11            7.92           7.70
Thời gian suy luận     336            293             299            282

điểm). Tuy nhiên, chúng tôi cũng lưu ý một sự xuất hiện hiếm hoi nơi n = 16 dẫn đến một sự
giảm đáng kể trong điểm BLEU, chẳng hạn như trên bộ dữ liệu En-Id. Thứ tư, trên một số bộ dữ
liệu, mô hình PHM-Transformer cải thiện hiệu suất của mô hình Transformer chuẩn. Ví dụ, trên
các bộ dữ liệu như En-Vi và En-Et, mô hình PHM-Transformer tận hưởng một sự tăng hiệu suất
khoảng 0.8 điểm BLEU với n = 2. Cuối cùng, bằng cách tái tỷ lệ với hệ số 2 (nhân đôi kích thước
ẩn), chúng tôi có thể cải thiện hiệu suất trên ba bộ dữ liệu: En-Vi, En-Id, và En-Mk.

Bảng 3 báo cáo thời gian huấn luyện và suy luận cho các biến thể Transformer. Chúng tôi quan sát
rằng PHM-Transformer với n = 8 có tốc độ suy luận nhanh nhất trong tất cả các biến thể, chủ yếu
do việc giảm đáng kể các tham số. Tổng thể, tốc độ huấn luyện cũng gần như tương đương. Điều
này xác nhận rằng lớp PHM không tăng nhiều chi phí tính toán trong thực tế.

5.3 CHUYỂN ĐỔI PHONG CÁCH VĂN BẢN
Chúng tôi tiếp tục thí nghiệm với chuyển đổi chuỗi cho chuyển đổi phong cách văn bản. Mục tiêu
của tác vụ này là chuyển đổi văn bản của một phong cách nhất định sang phong cách khác. Chúng
tôi sử dụng kho văn bản Modern→Shakespeare¹ trong các thí nghiệm. Bảng 4 báo cáo kết quả
trên tác vụ chuyển đổi phong cách văn bản này. Chúng tôi quan sát rằng hiệu suất tốt nhất được
đạt được với PHM-Transformer (n = 4). Đáng chú ý, tất cả ngoại trừ biến thể n = 16 đều tăng
hoặc phù hợp với hiệu suất của mô hình Transformer chuẩn. Điều này xác nhận tính linh hoạt kiến
trúc và hiệu quả của lớp PHM được đề xuất. Điều này không chỉ cho phép tiết kiệm tham số mà
còn cải thiện hiệu suất của Transformer.

5.4 THỎA THUẬN ĐỘNG TỪ CHỦ NGỮ
Chúng tôi tiến hành thêm các thí nghiệm trên tác vụ thỏa thuận động từ chủ ngữ (Linzen et al.,
2016). Tác vụ dự đoán nếu câu, ví dụ, 'The keys to the cabinet ___' được theo sau bởi một động
từ số nhiều hoặc số ít. Bộ dữ liệu được sử dụng có thể được tìm thấy trực tuyến (Linzen et al.,
2016). Bảng 5 báo cáo kết quả trên tác vụ thỏa thuận động từ chủ ngữ. Kết quả đầy hứa hẹn,
chứng minh rằng tất cả các biến thể với các lớp PHM đều vượt trội hơn các mô hình Transformer
chuẩn và Quaternion. Hiệu suất tốt nhất đạt đỉnh tại n = 8, mặc dù tiết kiệm tham số lên đến 1/8.

¹https://github.com/tlatkowski/st

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

Bảng 4: Kết quả thí nghiệm chuyển đổi phong cách văn bản. PHM-Transformer có thể giảm các
tham số của mô hình Transformer chuẩn và cải thiện hiệu suất.

Mô hình            #Tham số   BLEU
Transformer (Tm)    44M       11.65
PHM-Tm (n=2)       22M (-50.0%) 12.20
PHM-Tm (n=4)       11M (-75.0%) 12.42
PHM-Tm (n=8)       5.5M (-87.5%) 11.66
PHM-Tm (n=16)      2.9M (-93.4%) 10.76

Bảng 5: Kết quả thí nghiệm thỏa thuận động từ chủ ngữ. PHM-Transformer có thể giảm các tham
số của mô hình Transformer chuẩn và cải thiện hiệu suất.

Mô hình            #Tham số   Acc
Transformer (Tm)    400K      94.80
Quaternion Tm       100K      94.70
PHM-Tm (n=2)       200K (-50.0%) 95.14
PHM-Tm (n=4)       101K (-74.8%) 95.05
PHM-Tm (n=8)       56K (-86.0%) 95.62

6 CÔNG TRÌNH LIÊN QUAN
Trong khi các mạng nơron đã là một hướng nghiên cứu được thiết lập tốt, tiến bộ về các biểu diễn
siêu phức cho học sâu vẫn còn trong giai đoạn sơ khai và hầu hết các công trình về chủ đề này đều
mới (Gaudet & Maida, 2017; Parcollet et al., 2018a;b; Zhu et al., 2018; Tay et al., 2019). Tích
Hamilton siêu phức cung cấp một mức độ biểu đạt lớn hơn, tương tự như phép nhân phức, mặc dù
với sự gia tăng 4 lần trong các tương tác giữa các thành phần thực và ảo. Trong trường hợp biểu
diễn Quaternion, do tiết kiệm tham số trong tích Hamilton, các mô hình cũng tận hưởng sự giảm
75% kích thước tham số (Parcollet et al., 2018a; Tay et al., 2019). Một điểm quan trọng đáng chú
ý là tất cả Quaternions về cơ bản bị giới hạn trong không gian siêu phức 4D, điều này hạn chế tính
linh hoạt kiến trúc. Các tùy chọn khác sẽ là mở rộng lên không gian Octonion (8D) hoặc Sedenion
(16D), cho các quy tắc nhân được định nghĩa trước trong không gian đó. Theo hiểu biết tốt nhất
của chúng tôi, không có công trình nào cố gắng tổng quát hóa các phép nhân siêu phức nD tùy ý
để cho phép tính linh hoạt kiến trúc, trong đó n có thể được chỉ định hoặc điều chỉnh bởi người
dùng.

Công trình của chúng tôi cũng có thể được diễn giải như một dạng chia sẻ tham số mềm, mặc dù
được học từ dữ liệu. Các mạng Quaternion (Zhu et al., 2018; Parcollet et al., 2018b; 2019) được
biết đến với việc sở hữu các tính chất chia sẻ trọng số thông qua phép toán tích Hamilton và đã
chứng minh thành công hợp lý mặc dù có ít tham số hơn. Theo hiểu biết tốt nhất của chúng tôi,
không có công trình nào cố gắng tham số hóa tích Hamilton siêu phức cho các mạng nơron, tức là
cho phép học end-to-end các tương tác thành phần thực và ảo từ dữ liệu.

7 KẾT LUẬN
Chúng tôi đã đề xuất các lớp nhân siêu phức được tham số hóa (PHM) học và tổng quát hóa các
phép nhân siêu phức. Trong thực tế, lớp PHM có 1/n tham số có thể học được so với phần tương
ứng lớp kết nối đầy đủ, trong đó n có thể được chỉ định linh hoạt bởi người dùng. Các lớp PHM
có thể áp dụng cho các mô hình chiếm ưu thế như LSTM và Transformer. Chúng tôi đã đánh giá
những mô hình này được trang bị bởi các lớp PHM trên các tác vụ toàn diện để chỉ ra tính linh hoạt
kiến trúc và hiệu quả của tham số hóa phép nhân siêu phức.

Lời cảm ơn. Chúng tôi cảm ơn các nhà đánh giá ẩn danh vì những nhận xét sâu sắc về bài báo
này. Công trình này được hỗ trợ một phần bởi Bộ Giáo dục (MoE) Singapore dưới Quỹ Nghiên
cứu Học thuật (AcRF) Cấp 1 Grant RG135/18.

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

TÀI LIỆU THAM KHẢO
Samuel R Bowman, Gabor Angeli, Christopher Potts, và Christopher D Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.

Chase Gaudet và Anthony Maida. Deep quaternion networks. arXiv preprint arXiv:1712.04604, 2017.

Ian Goodfellow, Yoshua Bengio, và Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.

Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997.

Tushar Khot, Ashish Sabharwal, và Peter Clark. Scitail: A textual entailment dataset from science question answering. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

Tal Linzen, Emmanuel Dupoux, và Yoav Goldberg. Assessing the ability of lstms to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535, 2016.

Bill MacCartney. Natural language inference. Citeseer, 2009.

Titouan Parcollet, Mirco Ravanelli, Mohamed Morchid, Georges Linarès, Chiheb Trabelsi, Renato De Mori, và Yoshua Bengio. Quaternion recurrent neural networks. In International Conference on Learning Representations, 2018a.

Titouan Parcollet, Ying Zhang, Mohamed Morchid, Chiheb Trabelsi, Georges Linarès, Renato De Mori, và Yoshua Bengio. Quaternion convolutional neural networks for end-to-end automatic speech recognition. arXiv preprint arXiv:1806.07789, 2018b.

Titouan Parcollet, Mohamed Morchid, và Georges Linarès. Quaternion convolutional neural networks for heterogeneous image processing. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 8514–8518. IEEE, 2019.

Ankur P Parikh, Oscar Täckström, Dipanjan Das, và Jakob Uszkoreit. A decomposable attention model for natural language inference. arXiv preprint arXiv:1606.01933, 2016.

Jeffrey Pennington, Richard Socher, và Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014.

Adityan Rishiyur. Neural networks with complex and quaternion inputs. arXiv preprint cs/0607090, 2006.

Yi Tay, Aston Zhang, Luu Anh Tuan, Jinfeng Rao, Shuai Zhang, Shuohang Wang, Jie Fu, và Siu Cheung Hui. Lightweight and efficient neural natural language processing with quaternion networks. arXiv preprint arXiv:1906.04393, 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017.

Zhiguo Wang, Wael Hamza, và Radu Florian. Bilateral multi-perspective matching for natural language sentences. arXiv preprint arXiv:1702.03814, 2017.

Sean Welleck, Jason Weston, Arthur Szlam, và Kyunghyun Cho. Dialogue natural language inference. arXiv preprint arXiv:1811.00671, 2018.

Adina Williams, Nikita Nangia, và Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.

Aston Zhang, Zachary C. Lipton, Mu Li, và Alexander J. Smola. Dive into Deep Learning. 2020. https://d2l.ai.

Xuanyu Zhu, Yi Xu, Hongteng Xu, và Changjian Chen. Quaternion convolutional neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631–647, 2018.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

[Hình 3: Minh họa tái xây dựng H trong (3.2) bằng cách tái sử dụng các ma trận tham số Bi (i = 1, 2) và Tj (j = 1, ..., 4) trong các phép nhân ma trận giá trị thực, theo sau là các phép toán khác (ở đây n = 2; k = 6; d = 8). Xem tốt nhất ở màu.]

A TÁI XÂY DỰNG MA TRẬN THAM SỐ
Trong bài báo, ma trận tham số H trong (3.2) được xây dựng bởi một tổng của n tích Kronecker. Trong phần sau, chúng tôi sẽ cung cấp một góc nhìn thay thế và chỉ ra cách tương đương tái xây dựng H bằng cách tái sử dụng các ma trận tham số trong các phép nhân ma trận giá trị thực, theo sau là các phép toán khác.

A.1 PHƯƠNG PHÁP
Ý tưởng chính là hoạt động trên các khối trọng số được phân vùng và học một sự khuếch tán động của trọng số. Có hai khối tham số chính B và T trung tâm cho phương pháp của chúng tôi. Trực quan, B∈Rn×n kiểm soát quá trình khuếch tán trọng số và học các tương tác mềm giữa các phân vùng T. Ở đây, n là một siêu tham số được người dùng định nghĩa.

Giả sử rằng cả d và k đều chia hết cho n∈Z>0. Đối với i = 1, ..., n và j = 1, ..., d/n, ký hiệu bởi mỗi khối tham số được phân vùng Tj∈Rn×k/n, và Bi∈Rn×n là ma trận khuếch tán trọng số được gán cho mỗi khối tham số được phân vùng thông qua phép nhân ma trận giá trị thực BiTj. Tham số H trong (3.2) hiện được xây dựng bằng phép nối theo cột (·;·):

H = [s(B1); s(B2); ...; s(Bn)]; (A.1)

trong đó mỗi phân đoạn s(Bi) cũng được tạo thành bởi phép nối theo cột:
s(Bi) = [ℜ(BiT1); ℜ(BiT2); ...; ℜ(BiTd/n)]. (A.2)

Trong (A.2), hàm ℜ: Rp×q → Rpq, trong đó ℜ(X) làm phẳng ma trận X∈Rp×q bằng cách nối mỗi hàng của X sau đó chuyển vị vector hàng được nối thành một vector cột có chiều pq. Dễ thấy rằng, ℜ(BiTj)∈Rk, s(Bi)∈Rk×d/n, do đó H∈Rk×d.

Chính các khối tham số được phân vùng Bi (i = 1, ..., n) và Tj (j = 1, ..., d/n) xác định bậc tự do cho H, là kd/n + n³. Như được minh họa trong Hình 3, việc tái sử dụng các ma trận tham số B1, ..., Bn và T1, ..., Td/n trong các phép nhân ma trận giá trị thực trong (A.2) có thể giảm bậc tự do cho H.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

A.2 BAO GỒM CÁC PHÉP NHÂN SIÊU PHỨC
Tương tự, chúng tôi chỉ ra cách lớp PHM với H được tái xây dựng trong (A.1) cũng bao gồm phép nhân siêu phức. Lấy tích Hamilton của hai Quaternions Q và P làm ví dụ, nó có thể được viết lại là

[THIS IS EQUATION/MATRIX: A complex mathematical equation showing the Hamilton product representation with matrices B1, B2, B3, B4 and vectors T1]

trong đó 4 phần tử đầu ra là các giá trị thực cho cơ sở đơn vị Quaternion [1, i, j, k]ᵀ. Theo (A.3), khi n = 4, lớp PHM với ma trận tham số được tái xây dựng cũng có thể được học để biểu diễn chính xác tích Hamilton của Quaternions. Tương tự, các phép nhân siêu phức của Octonions hoặc Sedenions cũng có thể được học bởi lớp PHM khi n được đặt thành 8 hoặc 16.

A.3 BAO GỒM CÁC PHÉP NHÂN MA TRẬN GIÁ TRỊ THỰC
Bây giờ chúng tôi chỉ ra cách lớp PHM với H được tái xây dựng trong (A.1) cũng bao gồm phép nhân ma trận trong không gian thực. Tham khảo (3.2), khi n = 1, H = bW, trong đó vô hướng b là phần tử duy nhất của ma trận 1×1 B1 và các phần tử của W∈Rk×d đến từ việc nối của T1, ..., Td∈R1×k. Vì việc học b và W riêng biệt tương đương với việc học phép nhân của chúng cùng nhau, vô hướng b có thể bỏ qua, đó là học ma trận trọng số duy nhất trong lớp FC. Do đó, lớp PHM thoái hóa thành lớp FC khi n = 1.

B THIẾT LẬP CHO CÁC THÍ NGHIỆM
Chúng tôi mô tả thiết lập cho các thí nghiệm như sau.

B.1 SUY LUẬN NGÔN NGỮ TỰ NHIÊN
Chúng tôi triển khai các bộ mã hóa đơn hướng 300D với các tham số được chia sẻ cho cả tiền đề và giả thuyết. Chúng tôi lấy việc nối các biểu diễn được gộp max và mean làm đầu vào cho một perceptron đa lớp 300D hai lớp để dự đoán. Mô hình của chúng tôi được huấn luyện với trình tối ưu hóa Adam với tốc độ học 0.0004 và kích thước batch 256. Nhúng từ được khởi tạo với GloVe (Pennington et al., 2014) và được cố định. Không có chú ý qua câu (Parikh et al., 2016) được sử dụng, chủ yếu để quan sát hiệu quả của các bộ mã hóa độc lập. Đối với PHM-LSTM, chúng tôi sử dụng n = {2, 5, 10}. Lưu ý rằng trong tác vụ này, vì nhúng từ là 300D, chúng tôi chọn các bội số của 5 thay vì 4 để dễ chia hết.

B.2 DỊCH MÁY
Đối với các bộ dữ liệu IWSLT'15 English-Vietnamese (En-Vi), IWSLT'17 English-Indonesian (En-Id), IWSLT'14 German-English (De-En), và IWSLT'14 Romanian-English (Ro-En), chúng tôi chạy với 50K bước; trong khi đối với các bộ dữ liệu WMT'18 English-Estonian (En-Et), Setimes English-Macedonian (En-Mk), và WMT'18 English-Romanian (En-Ro), các mô hình được huấn luyện trong 100K bước. Đối với các bộ dữ liệu En-Vi, En-Id, En-Et, En-Mk, và En-Ro, chúng tôi chỉ định rằng Transformers có 4 lớp, 8 đầu, và kích thước ẩn 512. Đối với các bộ dữ liệu De-En và Ro-En, chúng tôi chỉ định rằng Transformers có 2 lớp, 4 đầu, và kích thước ẩn 256. Chúng tôi sử dụng kích thước chùm là 5 và α = 0.6 (phạt độ dài) để giải mã. Đối với tất cả các mô hình PHM, chúng tôi đánh giá một số thiết lập cho siêu tham số n = {2, 4, 8, 16}.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021

B.3 CHUYỂN ĐỔI PHONG CÁCH VĂN BẢN
Đối với kho văn bản Modern→Shakespeare² được sử dụng trong các thí nghiệm, mục tiêu chính ở đây là chuyển đổi văn viết hiện đại thành văn viết Shakespeare. Bộ dữ liệu này bao gồm 18,395 câu song song để huấn luyện, 1,218 câu song song để đánh giá (tập phát triển), và 1,462 câu song song để kiểm tra. Chúng tôi vẫn chỉ định rằng Transformers có 4 lớp, 8 đầu, và kích thước ẩn 512. Tương tự như dịch máy, chúng tôi thí nghiệm với n = {2, 4, 8, 16}. Chúng tôi huấn luyện tất cả các mô hình trong 10K bước.

B.4 THỎA THUẬN ĐỘNG TỪ CHỦ NGỮ
Trái ngược với các thiết lập thí nghiệm trước đó, chúng tôi sử dụng kiến trúc Transformer nhỏ hơn với 10K bước huấn luyện. Cụ thể, Transformers ở đây có 2 lớp, 4 đầu, và kích thước ẩn 128. Vì kích thước ẩn nhỏ hơn so với các thiết lập thí nghiệm trước đó, chúng tôi thí nghiệm với n = {2, 4, 8}.

²https://github.com/tlatkowski/st

--- TRANG 13 ---
