# 2110.04176.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypercomplex/2110.04176.pdf
# File size: 2623854 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
PHNNs: Lightweight Neural Networks via
Parameterized Hypercomplex Convolutions
Eleonora Grassucci, Graduate Student Member, IEEE, Aston Zhang, and
Danilo Comminiello, Senior Member, IEEE
Abstract ‚ÄîHypercomplex neural networks have proven to re-
duce the overall number of parameters while ensuring valuable
performance by leveraging the properties of Clifford algebras.
Recently, hypercomplex linear layers have been further improved
by involving efÔ¨Åcient parameterized Kronecker products. In
this paper, we deÔ¨Åne the parameterization of hypercomplex
convolutional layers and introduce the family of parameterized
hypercomplex neural networks (PHNNs) that are lightweight and
efÔ¨Åcient large-scale models. Our method grasps the convolution
rules and the Ô¨Ålter organization directly from data without
requiring a rigidly predeÔ¨Åned domain structure to follow. PHNNs
are Ô¨Çexible to operate in any user-deÔ¨Åned or tuned domain, from
1D to nD regardless of whether the algebra rules are preset. Such
a malleability allows processing multidimensional inputs in their
natural domain without annexing further dimensions, as done,
instead, in quaternion neural networks for 3D inputs like color
images. As a result, the proposed family of PHNNs operates with
1=nfree parameters as regards its analog in the real domain.
We demonstrate the versatility of this approach to multiple
domains of application by performing experiments on various
image datasets as well as audio datasets in which our method
outperforms real and quaternion-valued counterparts. Full code
is available at: https://github.com/eleGAN23/HyperNets.
Index Terms ‚ÄîHypercomplex Neural Networks, Kronecker
Decomposition, Lightweight Neural Networks, Quaternions, Ef-
Ô¨Åcient models
I. I NTRODUCTION
RECENT state-of-the-art convolutional models achieved
astonishing results in various Ô¨Åelds of application by
large-scaling the overall parameters amount [1]‚Äì[4]. Simul-
taneously, hypercomplex algebra applications are gaining in-
creasing attention in diverse spheres of research such as signal
processing [5]‚Äì[8] or deep learning [9]‚Äì[17]. Indeed, hy-
percomplex and quaternion neural networks (QNNs) demon-
strated to signiÔ¨Åcantly reduce the number of parameters while
still obtaining comparable performance [18]‚Äì[24]. These mod-
els exploit hypercomplex algebra properties, including the
Hamilton product, to painstakingly design interactions among
the imaginary units, thus involving 1=4or1=8of free parame-
ters with respect to real-valued models. Furthermore, thanks to
the modelled interactions, hypercomplex networks capture in-
ternal latent relations in multidimensional inputs and preserve
pre-existing correlations among input dimensions [25]‚Äì[29].
Therefore, the quaternion domain is particularly appropriate
for processing 3D or 4D data, such as color images or (up to)
E. Grassucci and D. Comminiello are with the Dept. Information Engi-
neering, Electronics and Telecommunications (DIET), Sapienza University of
Rome, Italy. A. Zhang is with Amazon Web Services AI, East Palo Alto, CA,
USA. Corresponding author‚Äôs email: eleonora.grassucci@uniroma1.it.4-channel signals [30], while the octonion one is suitable for
8D inputs. Unfortunately, most common color image datasets
contain RGB images and some tricks are required to process
this data type with QNNs. Among them, the most employed
are padding a zero channel to the input in order to encapsulate
the image in the four quaternion components, or remodelling
the QNN layer with the help of vector maps [31]. Additionally,
while quaternion neural operations are widespread and easy
to be integrated in pre-existing models, very few attempts
have been made to extend models to different domain orders.
Accordingly, the development of hypercomplex convolutional
models for larger multidimensional inputs, such as magnitudes
and phases of multichannel audio signals or 16-band satellite
images, still remains painful. Moreover, despite the signiÔ¨Å-
cantly lower number of parameters, these models are often
slightly slow with respect to real-valued baselines [32] and ad-
hoc algorithms may be necessary to improve efÔ¨Åciency [22],
[33].
Recently, a novel literature branch aims at compress neural
networks leveraging Kronecker product decomposition [34],
[35], gaining considerable results in terms of model efÔ¨Åciency
[36]. Lately, a parameterization of hypercomplex multiplica-
tions have been proposed to generalize hypercomplex fully
connected layers by sum of Kronecker products [37]. The
latter method obtains high performance in various natural
language processing tasks by also reducing the number of
overall parameters. Other works extended this approach to
graph neural networks [38] and transfer learning [39], proving
the effectiveness of Kronecker product decomposition for
hypercomplex operations. However, no solution exists for
convolutional layers yet, which remain the most employed
layers when dealing with multidimensional inputs, such as
images and audio signals [40], [41].
In this paper, we devise the family of parameterized hy-
percomplex neural networks (PHNNs), which are lightweight
large-scale hypercomplex neural models admitting any mul-
tidimensional input, whichever the number of dimensions.
At the core of this novel set of models, we propose the
parameterized hypercomplex convolutional (PHC) layer. Our
method is Ô¨Çexible to operate in domains from 1D tonD,
wherencan be arbitrarily chosen by the user or tuned to let
the model performance lead to the most appropriate domain
for the given input data. Such a malleability comes from the
ability of the proposed approach to subsume algebra rules
to perform convolution regardless of whether these regula-
tions are preset or not. Thus, neural models endowed with
our approach adopt 1=nof free parameters with respect toarXiv:2110.04176v2  [cs.LG]  19 Sep 2022

--- PAGE 2 ---
2
their real-valued counterparts, and the amount of parameter
reduction is a user choice. This makes PHNNs adaptable to a
plethora of applications in which saving storage memory can
be a crucial aspect. Additionally, PHNNs versatility allows
processing multidimensional data in its natural domain by
simply setting the dimensional hyperparameter n. For instance,
color images can be analyzed in their RGB domain by setting
n= 3 without adding any useless information, contrary to
standard processing for quaternion networks with the padded
zero-channel. Indeed, PHC layers are able to grasp the proper
algebra from input data, while capturing internal correlations
among the image channels and saving 66% of free parameters.
On a thorough empirical evaluation on multiple benchmarks,
we demonstrate the Ô¨Çexibility of our method that can be
adopted in different domains of applications, from images to
audio signals. We devise a set of PHNNs for large-scale image
classiÔ¨Åcation and sound event detection tasks, letting them
operate in different hypercomplex domain and with various
input dimensionality with nranging from 2to16.
The contribution of this paper is three-fold.
We introduce a parameterized hypercomplex convolu-
tional (PHC) layer which grasps the convolution rules
directly from data via backpropagation exploiting the
Kronecker product properties, thus reducing the number
of free parameters to 1=n.
We devise the family of parameterized hypercomplex
neural networks (PHNNs), lightweight and more efÔ¨Åcient
large-scale hypercomplex models. Thanks to the proposed
PHC layer and to the method in [37] for fully connected
layers, PHNNs can be employed with any kind of input
and pre-existing neural models. To show the latter, we
redeÔ¨Åne common ResNets, VGGs and Sound Event De-
tection networks (SEDnets), operating in any user-deÔ¨Åned
domain just by choosing the hyperparameter n, which
also drives the number of convolutional Ô¨Ålters.
We show how the proposed approach can be employed
with any kind of multidimensional data by easily chang-
ing the hyperparameter n. Indeed, by setting n= 3 a
PHNN can process RGB images in their natural domain,
while leveraging the properties of hypercomplex algebras,
allowing parameter sharing inside the layers and leading
to a parameter reduction to 1=3. To the best of our
knowledge, this is the Ô¨Årst approach that processes color
images with hypercomplex-based neural models without
adding any padding channel. As well, multichannel audio
signals can be analysed by simply considering n= 4 for
standard Ô¨Årst-order ambisonics (which has 4microphone
capsules),n= 8 for an array of two ambisonics mi-
crophones, or even n= 16 if we want to include the
information of each channel phase.
The rest of the paper is organized as follows. In Section
II, we introduce concepts of hypercomplex algebra and we
recapitulate real and quaternion-valued convolutional layers.
Section III rigorously introduces the theoretical aspects of the
proposed method. Sections IV and V reveal how the approach
can be adopted in different neural models and in two different
domains, the images and audio one, expounding how to
Fig. 1. Example of hypercomplex multiplication table for n= 2 i.e., complex,
among others (green line), n= 4 i.e., quaternions, tessarines, (blue line) and
n= 8, i.e., octonions, bi-quaternions, and so on (red line). While for these
domains algebra rules exist and are predeÔ¨Åned, no regulations are set for
other domains such as n= 3;5;6;7(dashed grey lines). The parameterized
hypercomplex approaches are able to learn these missing algebra rules from
data, thus deÔ¨Åning hypercomplex multiplication and convolution for any
desired domain.
process RGB images with n= 3and multichannel audio with
nup to 8. The experimental evaluation is presented in Section
VI for image classiÔ¨Åcation and in Section VII for sound event
detection. Finally, Section VIII reports the ablation studies we
conduct and in Section IX we draw conclusions.
II. H YPERCOMPLEX NEURAL NETWORKS
A. Hypercomplex Algebra
Hypercomplex neural networks rely in a hypercomplex
number system based on the set of hypercomplex numbers
Hand their corresponding algebra rules to shape additions
and multiplications [24]. These operations should be carefully
modelled due to the interactions among imaginary units that
may not behave as real-valued numbers. For instance, Figure 1
reports an example of a multiplication table for complex
(green), quaternion (blue) and octonion (red) numbers. How-
ever, this is just a small subset of the hypercomplex domain
that exist. Indeed, for n= 4there exist quaternions, tessarines,
among others, while for n= 8 octonions, dual-quaternions,
and so on. Each of these domains have different multiplication
rules due to dissimilar imaginary units interactions. A generic
hypercomplex number is deÔ¨Åned as
h=h0+hi^{i+:::+hn^{n; i = 1;:::;n (1)
beingh0;:::;h n2Rand^{i;:::; ^{nimaginary units. Different
subsets of the hypercomplex domain exist, including complex,
quaternion, and octonion, among others. They are identiÔ¨Åed by
the number of imaginary units they employ and by the prop-
erties of their vector multiplication. The quaternion domain
is one of the most popular for neural networks thanks to the
Hamilton product properties. This domain has its foundations
in the quaternion number q=q0+q1^{+q2^|+q3^, in
whichqc; c2 f0;1;2;3gare real coefÔ¨Åcients and ^{;^|;^
the imaginary untis. A quaternion with its real part q0equal
to0is named pure quaternion . The imaginary units comply
with the property ^{2= ^|2= ^2= 1and with the non-
commutative products ^{^|= ^|^{; ^|^= ^^|; ^^{= ^{^.
Due to the non-commutativity of vector multiplication, the
Hamilton product has been introduced to properly model the
multiplication between two quaternions.

--- PAGE 3 ---
3
.........
... ... ... ......
............
............
............
Fig. 2. The quaternion convolution rule can be expressed as sum of Kronecker products between the matrices Aithat subsume the algebra rules and the
matrices Fithat contain the convolution Ô¨Ålters, with i= 1;2;3;4. In this example, the parameters of Aiare Ô¨Åxed for visualization purposes, but in PHC
layers they are learnable parameters.
B. Real and Quaternion-Valued Convolutional Layers
A generic convolutional layer can be described by
y=Conv (x) =Wx+b; (2)
where the input x2Rtsis convolved () with the Ô¨Ålters
tensor W2Rsdkkto produce the output y2Rdt,
wheresis the input channels dimension, dthe output one,
kis the Ô¨Ålter size, and tis the input and output dimension.
The bias term bdoes not heavily inÔ¨Çuence the number of
parameters, thus the degrees of freedom for this operation are
essentiallyO(sdk2).
Quaternion convolutional layers, instead, build the weight
tensor W2Rsdkkby following the Hamilton product
rule and organize Ô¨Ålters according to it:
Wx=2
664W0 W1 W2 W3
W1W0 W3W2
W2W3W0 W1
W3 W2W1W03
7752
664x0
x1
x2
x33
775(3)
where W0;W1;W2;W32Rs
4d
4kkare the real coef-
Ô¨Åcients of the quaternion weight matrix W=W0+W1^{+
W2^|+W3^andx0;x1;x2;x3are the coefÔ¨Åcients of the
quaternion input xwith the same structure.
As done for real-valued layers, the bias can be ignored
and the degree of freedom computations of the quaternion
convolutional layer can be approximated to O(sdk2=4). The
lower number of parameters with respect to the real-valued
operation is due to the reuse of Ô¨Ålters performed by the Hamil-
ton product in Eq. 3. Also, sharing the parameter submatrices
forces to consider and exploit the correlation between the input
components [21], [42], [43].
III. P ARAMETERIZING HYPERCOMPLEX CONVOLUTIONS
In the following, we delineate the formulation for the pro-
posed parameterized hypercomplex convolutional (PHC) layer.
We also show that this approach is capable of learning the
Hamilton product rule when two quaternions are convolved.A. Parameterized Hypercomplex Convolutional Layers
The PHC layer is based on the construction, by sum of
Kronecker products, of the weight tensor Hwhich encapsu-
lates and organizes the Ô¨Ålters of the convolution. The proposed
method is formally deÔ¨Åned as:
y=PHC(x) =Hx+b; (4)
whereby, H2Rsdkkis built by sum of Kronecker
products between two learnable groups of matrices. Here, s
is the input dimensionality to the layer, dis the output one,
andkis the Ô¨Ålter size. More concretely,
H=nX
i=1Ai
Fi; (5)
in which Ai2Rnnwithi= 1;:::;n are the matrices that
describe the algebra rules and Fi2Rs
nd
nkkrepresents
thei-th batch of Ô¨Ålters that are arranged by following the
algebra rules to compose the Ô¨Ånal weight matrix. It is worth
noting thats
nd
nkkholds for squared kernels, while
s
nd
nkshould be considered instead for 1D kernels. The
core element of this approach is the Kronecker product [44],
which is a generalization of the vector outer product that can
be parameterized by n. The hyperparameter ncan be set
by the user who wants to operate in a pre-deÔ¨Åned real or
hypercomplex domain (e.g., by setting n= 2 the PHC layer
is deÔ¨Åned in the complex domain, or in the quaternion one if
nis set equal to 4, as Figure 2 illustrates), or tuned to obtain
the best performance from the model. The matrices AiandFi
are learnt during training and their values are reused to build
the deÔ¨Ånitive tensor H.
The degree of freedom of AiandFiaren3andsdk2=n,
respectively. Usually, real world applications employ a large
number of Ô¨Ålters in layers ( s;d= 256;512;:::)and small
values fork. Therefore, frequently sdk2n3holds. Thus,
the degrees of freedom for the PHC weight matrix can be
approximated toO(sdk2=n). Hence, the PHC layer reduces
the number of parameters by 1=nwith respect to a standard
convolutional layer in real world problems.
Moreover, when processing multidimensional data with cor-
related channels, such as color images, rather than mulichan-
nel audio or multisensor signals, PHC layers bring beneÔ¨Åts

--- PAGE 4 ---
4
1 2 3 4 5
Epochs0.00000.00050.00100.00150.00200.0025Loss
1 2 3 4 5
Epochs0.000.010.020.030.040.05
Fig. 3. Loss plots for toy examples. The PHC layer is able to learn the matrix
Adescribing the convolution rule for pure (left) and full quaternions (right).
due to the weight sharing among different channels. This
allows capturing latent intra-channels relations that standard
convolutional networks ignore because of the rigid structure
of the weights [20], [45]. The PHC layer is able to subsume
hypercomplex convolution rules and the desired domain is
speciÔ¨Åed by the hyperparameter n. Interestingly, by setting
n= 1a real-valued convolutional layer can be represented too.
Indeed, standard real layers do not involve parameter sharing,
therefore the algebra rules are solely described by the single
A2R11and the complete set of Ô¨Ålters are included in
Fsdkk.
Therefore, the PHC layer Ô¨Ålls the gaps left by pre-existing
hypercomplex algebras in Fig. 1 and subsumes the missing
algebra rules directly from data, i.e., the dashed grey lines
in Fig. 1. Thus, a neural model equipped with PHC layers
can grasp the Ô¨Ålter organization also for n= 3;5;6;7and
so on. Moreover, any convolutional model can be endowed
with our approach, since PHC layers easily replace standard
convolution / transposed convolution operations and the hy-
perparameter ngives high Ô¨Çexibility to adapt the layer to anykind of input, such as color images, multichannel audio or
multisensor signals.
B. Learning Tests on Toy Examples
We test the receptive ability of the PHC layer in two toy
problems building an artiÔ¨Åcial dataset. We highly encourage
the reader to take a look at the section tutorials of the
GitHub repository https://github.com/eleGAN23/HyperNets
for more insights and results on toy examples, including the
learned matrices Ai. The Ô¨Årst task aims at learning the right
matrix Ato build a quaternion convolutional layer which
properly follows the Hamilton rule in Eq. 3. That is, we set
n= 4and the objective is to learn the four matrices Aias they
are in the quaternion product in Fig. 2. We build the dataset
by performing a convolution with a matrix of Ô¨Ålters W2H,
which are arranged following the regulation in Eq. 3, and a
quaternion x2Hin input. The target is still a quaternion,
named y2H. As shown in Fig. 3 (right), the MSE loss
of the PHC layer converges very fast, meaning that the layer
properly learns the matrix Aand the Hamilton convolution.
The second toy example is a modiÔ¨Åcation of the previous
dataset target. Here, we want to learn the matrix Awhich
describes the convolution among two pure quaternions. There-
fore, when setting n= 4, the matrix A1of a pure quaternion
should be complete null. Pure quaternions may be, as an exam-
ple, an input RGB image and the weights of a hypercomplex
convolutional layer since the Ô¨Årst channel of RGB images is
zero. Figure 3 (left) displays the convergence of the PHC layer
loss during training, proving that the proposed method is able
of subsuming hypercomplex convolutional rules when dealing
with pure quaternions too.
[A]
(11)
2
666666664F3
777777775
(sdkk)=2
666666664H3
777777775
(sdkk)
[A1]
(22)
2
4F13
5
(s
2d
2kk)+ [A2]
(22)
2
4F23
5
(s
2d
2kk)=2
666666664H3
777777775
(sdkk)
...
...
[A1]
(nn)

F1
(s
nd
nkk)+ [A2]
(nn)

F2
(s
nd
nkk)+:::+ [An]
(nn)

Fn
(s
nd
nkk)=2
666666664H3
777777775
(sdkk):(6)

--- PAGE 5 ---
5
C. Demystifying Parameterized Hypercomplex Convolutional
Layers
We provide a formal explanation of the PHC layer to
better understand the Kronecker product and how it organizes
convolution Ô¨Ålters to reduce the overall number of parameters
to1=n. In Eq. 6, we show how the PHC layer generalizes from
1D tonD domains. When subsuming real-valued convolutions
in the Ô¨Årst line of Eq. 6, the Kronecker product is performed
between a scalar Aand the Ô¨Ålter matrix F, whose dimension
is the same as the Ô¨Ånal weight matrix H, which issdkk.
Considering the complex case with n= 2 in the second
line of Eq. 6, the algebra is deÔ¨Åned in A1andA2while the
Ô¨Ålters are contained in F1andF2, each of dimension 1=2the
Ô¨Ånal matrix H. Therefore, while the size of the weight matrix
Hremains unchanged, the parameter size is approximately
1=2the real one. In the last line of Eq. 6, we can see the
generalization of this process, in which the size of matrices
Fi,i= 1;:::;n is reduced proportionally to n. It is worth
noting that, while the parameter size is reduced with growing
values ofn, the dimension of Hremains the same.
IV. P ARAMETERIZED HYPERCOMPLEX NEURAL
NETWORKS FOR COLOR IMAGES
In this section, we describe how PHNNs can be applied
for processing color images in hypercomplex domains with-
out needing any additional information to the input and we
propose examples of parameterized hypercomplex versions of
common computer vision models such as VGGs and ResNets.
In order to be consistent with literature, we perform each ex-
periment with a real-valued baseline model, then we compare
it with its complex and quaternion counterparts and with the
proposed PHNN. Furthermore, we assess the malleability of
the proposed approach testing different values of the hyper-
parametern, therefore deÔ¨Åning parameterized hypercomplex
models in multiple domains.
A. Process Color Images with PHC Layers
Different encodes exist to process color images, however,
the most common computer vision datasets are comprised
of three-channel images in R3. In the quaternion domain,
RGB images are enclosed into a quaternion and processed
as single elements [42]. The encapsulation is performed by
considering the RGB channels as the real coefÔ¨Åcients of the
imaginary units and by padding a zeros channel as the Ô¨Årst
real component of the quaternion.
Here, we propose to leverage the high malleability of PHC
layers to deal with RGB images in hypercomplex domains
without embedding useless information to the input. Indeed,
the PHC can directly operate in R3by easily setting n= 3and
process RGB images in their natural domain while exploiting
hypercomplex network properties such as parameters sharing.
Indeed, the great Ô¨Çexibility of PHC layers allows the user to
choose whether processing images in R4orR3. On one hand,
by settingn= 4, the zeros channel is added to the input even
so the layer saves the 75% of free parameters. On the other
hand, by choosing n= 3 the network does not handle any
useless information, notwithstanding, it reduces the numberof parameters by solely 66%. This is a trade-off which may
depend on the application or on the hardware the user needs.
Furthermore, the domain on which processing images can be
tuned by letting the performance of the network indicates the
best choice for n.
B. Parameterized Hypercomplex VGGs
A family of popular methods for image processing is based
on the VGG networks [46] that stack several convolutional
layers and a closing fully connected classiÔ¨Åer. To completely
deÔ¨Åne models in the desired hypercomplex domain, we pro-
pose to endow the network with PHC layers as convolution
components and with Parameterized Hypercomplex Multipli-
cation (PHM) layers [37] as linear classiÔ¨Åer. The backbone of
our PHVGG is then
ht=ReLU (PHC t(ht 1))t= 1;:::;j
y=ReLU (PHM (hj)):(7)
C. Parameterized Hypercomplex ResNets
In recent literature, a copious set of high performance in
image classiÔ¨Åcation is obtained with models having a resid-
ual structure. ResNets [47] pile up manifold residual blocks
composed of convolutional layers and identity mappings. A
generic PHResNet residual block is deÔ¨Åned by
y=F(x;fHjg) +x; (8)
whereby Hjare the PHC weights of layer j= 1;2in the
block, andFis
F(x;fHjg) =PHC (ReLU (PHC(x))); (9)
in which we omit batch normalization to simplify notation.
The backward phase of a PHNNs reduces to a backpropagation
similar to the quaternion neural networks one, which has been
already developed in [19], [42], [48].
V. P ARAMETERIZED HYPERCOMPLEX NEURAL
NETWORKS FOR MULTICHANNEL SIGNALS
In the following, we expound how PHNNs can be employed
to deal with multichannel audio signals and we introduce,
as an example, the parameterized hypercomplex Sound Event
Detection networks (PHSEDnets).
A. Process multichannel audio with PHC layers
A Ô¨Årst-order Ambisonics (FOA) signal is composed of 4
microphone capsules, whose magnitude representations can be
enclosed in a quaternion [49], [50]. However, the quaternion
algebra may be restrictive if more than one microphone is
employed for registration or whether the phase information has
to be included too. Indeed, quaternion neural networks badly
Ô¨Åt with multidimensional input with more than 4channels
[51].
Conversely, the proposed method can be easily adapted
to deal with these additional dimensions by handily setting
the hyperparameter nand thus completely leveraging each
information in the n-dimensional input.

--- PAGE 6 ---
6
10 20 30 40 50
Parameters (M)84858687888990AccuracyModel
Real
Complex
Quaternion
PH n=2
PH n=3
PH n=4
Family
ResNet
VGG
StdAcc
0.15
0.30
0.45
0.60
0.75
Fig. 4. CIFAR10 accuracy against number of network parameters for VGG
and ResNet models. The larger is the point, the higher is the standard deviation
over the runs. PHC-based models obtain better accuracies in both the families
while far reducing the number of parameters. We do not display Complex
VGGs as their accuracy is very low with respect to other models.
B. Parameterized Hypercomplex SEDnets
Sound Event Detection networks (SEDnets) [52] are com-
prised of a core convolutional component which extracts
features from the input spectrogram. The information is then
passed to a gated recurrent unit (GRU) module and to a
stack of fully connected (FC) layers with a closing sigmoid 
which outputs the probability the sound is in the audio frame.
Formally, the PHSEDnet is described by
ht=PHC t(ht 1)t= 1;:::;j
y=(FC(GRU (hj))):(10)
After the GRU model, We employ standard fully connected
layers, that can be also implemented as PHM layers with
n= 1, since the so processed signal loses its multidimensional
original structure.
VI. E XPERIMENTAL EVALUATION ON IMAGE
CLASSIFICATION
To begin with, we test the PHC layer on RGB images and
we show how, exploiting the correlations among channels,
the proposed method saves parameters while ensuring high
performance. We perform each experiment with a real-valued
baseline model and then we compare it with its complex and
quaternion counterparts and with the proposed PHNNs. Fur-
thermore, we assess the malleability of the proposed approach
testing different values of the hyperparameter n, therefore
deÔ¨Åning parameterized hypercomplex models in multiple do-
mains.
A. Experimental Setup
We perform the image classiÔ¨Åcation task with Ô¨Åve baseline
models. We consider ResNet18, ResNet50 and ResNet152
from the ResNet family and VGG16 and VGG19 from the
VGG one. Each hyperparameter is set according to the original
papers [46], [47]. We investigate the performance in four
Real Complex Quaternion PH n=2 PH n=3 PH n=4
Model0.010%20%30%40%Percentage of SuccessesFig. 5. Bar plot of number of successes achieves by the models in Table II
in each of the runs. The PHC-based models with n= 3 (red bar) far
exceeds other conÔ¨Ågurations being the more performing choice for RGB
image classiÔ¨Åcation task.
different color images datasets at different scales. We employ
SVHN, CIFAR10, CIFAR100, and ImageNet and any kind
of data augmentation is applied to these datasets in order to
guarantee a fair comparison.
We modify the number of Ô¨Ålters for ResNets in order to
be divisible by 3and thus having the possibility of testing
a conÔ¨Åguration with n= 3. The modiÔ¨Åed versions of the
ResNets are built with an initial convolutional layer of 60Ô¨Ål-
ters. Then, the subsequent blocks have 60;120;240;516Ô¨Ålters.
The number of layers in the blocks depends on the ResNet
chosen, whether 18, 50 or 152. Instead, VGG19 convolution
component comprise two 24, two 72, four 216, and eight
648 Ô¨Ålter layers, with batch normalization. The classiÔ¨Åer is
composed of three fully connected layers of 648,516and10,
100or1000 depending on the number of classes in the dataset.
The rest of the hyperparameters are set as suggested in the
original papers. The batch size is Ô¨Åxed to 128 and training
is performed via SGD optimizer with momentum equal to
0:9, weight decay 5e 4and a cosine annealing scheduler. For
ResNets, the initial learning rate is set to 0:1. For VGG is equal
to0:01. Models on CIFAR10 and CIFAR100 are trained for
200 epochs whereas on SVHN networks run for 50epochs.
For the ImageNet dataset, we follow the recipes in [53], so we
resize the images for training at 160160while keeping the
standard size of 224224for validation and test. We employ
a step learning rate decay every 30epochs with = 0:1, the
SGD optimizer and an initial learning rate of 0:1with weight
decay 0:0001 . The training is performed for 300k iterations
with a batch size of 256employing four Tesla V100 GPUs.
B. Experimental Results
We execute initial experiments with VGGs against Quater-
nion VGGs and two versions of PHVGGs with nequal to
2and to 4. Average and standard deviation accuracy over
three runs are reported for SVHN and CIFAR10 datasets in
Table I. We experiment also additional runs but any signiÔ¨Åcant

--- PAGE 7 ---
7
TABLE I
IMAGE CLASSIFICATION RESULTS FOR VGG. T HE ACCURACY MEAN AND STANDARD DEVIATION OVER THREE RUNS WITH DIFFERENT SEEDS IS
REPORTED . TRAINING (T) TIME AND INFERENCE (I)TIME REQUIRED ON CIFAR10. F OR TRAINING TIME WE REPORT ,IN SECONDS PER 100
ITERATIONS ,THE MEAN AND THE STANDARD DEVIATION OVER THE ITERATIONS IN ONE EPOCH ,WHILE THE INFERENCE TIME IS THE TIME REQUIRED
TO DECODE THE TEST SET . THEPHNN WITH n= 4 OUTPERFORMS THE QUATERNION COUNTERPART BOTH IN TERMS OF ACCURACY AND TIME . THE
PHVGG WITH n= 2 FAR EXCEEDS THE REAL -VALUED BASELINE IN THE CONSIDERED DATASETS ,WHILE BOTH THE PHVGG19 VERSIONS WITH
n= 2;4ARE MORE EFFICIENT THAN THE REAL AND QUATERNION -VALUED BASELINES AT INFERENCE TIME .p-VALUE UNDER THE T-TEST 0:0002 .
Model Params SVHN CIFAR10 Time (T) Time (I)
VGG16 15M 94.364 0.394 85.0670.765 2.20.02 1.2
Complex VGG16 7.6M (-50%) 93.555 0.392 76.9270.511 5.20.02 1.5
Quaternion VGG16 3.8M (-75%) 93.887 0.292 83.9970.493 5.20.02 2.2
PHVGG16 n= 2 7.6M (-50%) 94.8310.257 86.5100.216 3.20.02 1.4
PHVGG16 n= 4 3.8M (-75%) 94.639 0.121 85.6400.205 3.20.02 1.4
VGG19 29.8M 94.140 0.129 85.6240.257 3.20.02 16.0
Complex VGG19 14.8M (-50%) 90.469 0.222 76.9790.345 5.20.02 16.2
Quaternion VGG19 7.5M (-75%) 93.983 0.190 83.9140.129 6.20.02 16.3
PHVGG19 n= 2 14.9M (-50%) 94.5530.229 85.7500.286 4.00.02 15.4
PHVGG19 n= 4 7.4M (-75%) 94.169 0.296 84.8300.733 4.20.02 15.5
TABLE II
IMAGE CLASSIFICATION RESULTS WITH RESNET MODELS . EACH EXPERIMENT IS RUN THREE TIMES WITH DIFFERENT SEEDS AND MEAN WITH
STANDARD DEVIATION IS REPORTED . THE PROPOSED MODELS FAR EXCEED REAL -VALUED AND QUATERNION BASELINES ALMOST IN EACH
EXPERIMENT WE CONDUCT . INTERESTINGLY ,THE PHNN OUTPERFORM THE REAL -VALUED COUNTERPART BY 4% POINTS IN THE LARGEST -SCALE
EXPERIMENT ON CIFAR100. T HE TIME IS SIMILAR TO THE CLAIMS IN TABLE ISO WE DO NOT ADD HERE TO AVOID REDUNDANCY .
Model Params Storage Memory SVHN CIFAR10 CIFAR100
ResNet18 10.1M 39MB 93.992 1.317 89.5430.340 62.6340.600
Complex ResNet18 5.2M (-50%) 20MB (-50%) 89.902 0.322 89.5410.412 60.4170.811
Quaternion ResNet18 2.8M (-75%) 10MB (-75%) 93.661 0.413 88.2400.377 59.8500.607
PHResNet18 n= 2 5.4M (-50%) 20MB (-50%) 94.3590.187 89.2600.625 60.3202.249
PHResNet18 n= 3 3.6M (-66%) 13MB (-66%) 94.303 1.234 89.6030.563 62.6601.067
PHResNet18 n= 4 2.7M (-75%) 10MB (-75%) 94.234 0.161 88.8470.874 61.7800.689
ResNet50 22.5M 86MB 94.546 0.269 89.6300.305 65.5140.569
Complex ResNet50 11.1M (-50%) 43MB (-50%) 89.004 0.215 89.6990.485 65.1040.598
Quaternion ResNet50 5.7M (-75%) 22MB (-75%) 93.685 0.389 89.6700.383 63.7600.717
PHResNet50 n= 2 11.1M (-50%) 43MB (-50%) 93.849 0.249 89.7500.386 65.8840.333
PHResNet50 n= 3 7.6M (-66%) 29MB (-65%) 93.617 0.497 90.4230.145 66.4971.256
PHResNet50 n= 4 5.7M (-75%) 23MB (-74%) 94.5580.754 88.8970.645 66.2401.165
ResNet152 52.6M 201MB 94.6250.355 89.5800.173 62.0530.385
Complex ResNet152 26.3M (-50%) 101MB (-50%) 90.332 0.129 89.7920.427 63.1250.681
Quaternion ResNet152 13.2M (-75%) 51MB (-75%) 93.638 0.098 89.2270.287 61.2670.784
PHResNet152 n= 2 26.6M (-50%) 103MB (-49%) 93.915 0.512 90.5400.401 65.8170.327
PHResNet152 n= 3 17.8M (-66%) 70MB (-65%) 93.955 0.152 90.0770.436 66.3470.567
PHResNet152 n= 4 13.4M (-75%) 53 MB (-74%) 94.290 0.237 89.8970.097 66.4370.064
difference emerges as the randomness only affects the network
initialization. Both the PHVGG16 and PHVGG19 versions
clearly outperform real, complex and quaternion counterparts
while being built with more than a half the number of
parameters of the baseline. Additionally, PH-based models ex-
traordinarily reduce the number of training and inference time
(computed on an NVIDIA Tesla-V100) required with respect
to the quaternion model which operates in a hypercomplex
domain as well. Furthermore, when scaling up the experiment
with VGG19, the proposed methods are more efÔ¨Åcient at
inference time with respect to the real-valued VGG19. There-
fore, PHNNs can be easily adopted in applications with disk
memory limitations, due to the reduction of parameters, and
for fast inference problems thanks to the efÔ¨Åciency at testing
time. Although the sum of Kronecker products in PHC layers
requires additional computations, the increase is insigniÔ¨Åcant
with respect to the FLOPs computated for the whole network,
so the overall number of FLOPs is not heavily affected by ourmethod and the count remains almost the same.
Our approach has high malleability, indeed, when dealing
with color images, we can the domain in which operating
thanks to the hyperparameter n. Therefore, we test PHNNs
in the complex ( n= 2), quaternion ( n= 4) orH3(n= 3)
domain, where in the latter we do not concatenate any zero
padding and process the RGB channels of the image in their
natural domain.
Table II presents average and standard deviation accuracy
over three runs with different seeds for ResNet-based models.
We perform extensive experiments and the PH models with
n= 4 always outperform the quaternion counterpart gaining
a higher accuracy and being more robust. This underlines
the effectiveness of the PHC architectural Ô¨Çexibility over the
predeÔ¨Åned and rigid structure of quaternion layers. Further-
more, our method distinctly far exceeds the corresponding
real-valued baselines across the experiments while saving from
50% to75% parameters. Focusing on the latter result, the

--- PAGE 8 ---
8
TABLE III
IMAGE NET CLASSIFICATION WITH REAL -VALUED BASELINE AGAINST
OUR BEST MODEL PHn= 3. OUR APPROACH OUTPERFORM THE
BASELINE WHILE SAVING THE 66% OF PARAMETERS .
Model Params ImageNet
ResNet50 25.7M 67.990
PHResNet50 n= 3 9.6M (-66%) 68.584
PHResNets with n= 3 results to be the most suitable choice
in many cases, proving the validity of processing RGB images
in their natural domain leveraging hypercomplex algebra.
However, performance with n= 3andn= 4are comparable,
thus the choice of this hyperparameter may depend on the
application or on the hardware employed. On one hand, n= 4
may sometimes lead to lower performance, nevertheless it
allows saving disk memory, as shown in the third column of
Table II, thus it may be more appropriate for edge applications.
On the other hand, processing color images with n= 3 may
bring higher accuracy even so it requires more parameters.
Therefore, such a Ô¨Çexibility makes PHNNs adaptable to a
large range of applications. Likewise, PHResNets with n= 2
gain considerable accuracy scores with respect to the real-
valued corresponding models and, due to the larger number of
parameters with respect to the PH model with n= 3, some-
times outperform it too. Finally, the PHResNet with n= 4
obtains the overall best accuracy in the largest experiment
of this set. Indeed, considering a ResNet152 backbone on
CIFAR100, our method exceeds the real-valued baseline by
more than 4%. This is the empirical proof that, PHNNs well
scale to large real-world problems by notably reducing the
overall number of parameters. These results are summarized
for ResNets and VGGs models on CIFAR10 in Fig. 4. The
plot displays models accuracies against models parameters.
The PH-based models, either ResNets or VGGs exceed their
real and quaternion-valued baselines while consistently reduce
the number of parameters. What is more, in Table II, we also
report the memory required to store models checkpoints for
inference. Our method crucially reduces the amount of disk
memory demand with respect to the heavier real-valued model.
Further, we perform the image classifcation task on the
ImageNet dataset. We compute the percentage of successes
of ResNet-based models in each run for which we report the
average accuracies in Table II. As Fig. 5 shows, the largest
parcentage of successes is reached by the PHResNet with
n= 3 which has been demonstrated to be the most valuable
choice for nwhen dealing with RGB images. Therefore,
we test the PHResNet with n= 3 against the real-valued
counterpart. Table III shows that the proposed method achieves
comparable, and even slightly superior, performance than the
real-valued baseline, while involving 66% fewer parameters.
Additionally, in Fig.6, we provide Grad-CAM visualizations
[54] for a sample of predictions by our method in the ImageNet
dataset to further prove the correct behavior of the PHRes-
Net50n= 3in this scenario. This proves the robustness of the
proposed approach, which can be adopted and implemented in
models at different scales.
Fig. 6. Grad-CAM visualization for the PHResNet50 n= 3 on the ImageNet
dataset.
VII. E XPERIMENTAL EVALUATION ON SOUND EVENT
DETECTION
Sound event detection (SED) is the task of recognizing the
sounds classes and at what temporal instances these sounds are
active in an audio signal [55]. We prove that the PHC layer is
adaptable to n-dimensional input signals and, due to parameter
reduction and hypercomplex algebra, is more performing in
terms of efÔ¨Åciency and evaluation scores.
A. Experimental Setup
For sound event detection models we consider the aug-
mented version of the SELDnet [49], [52] which was proposed
as baseline for of the L3DAS21 Challenge Task 2 [56] and
we perform our experiments with the corresponding released
dataset1. We consider as our baselines the SEDnet (without
the localization part) and its quaternion counterpart. The
L3DAS21 Task 2 dataset contains 15 hours of MSMP B-
format Ambisonics audio recordings, divided in 900 1-minute-
long data points sampled at a rate of 32kHz, where up to
3 acoustic events may overlap. The 14 sounds classes have
been selected from the FSD50K dataset and are representative
for an ofÔ¨Åce sounds: computer keyboard, drawer open/close,
cupboard open/close, Ô¨Ånger snapping, keys jangling, knock,
laughter, scissors, telephone, writing, chink and clink, printer,
female speech, male speech . In this dataset, the volume dif-
ference between the sounds is in the range 0and20dB full
scale (dBFS). Considering the array of two microphones 1;2,
the channels order is [W1, Z1, Y1, X1, W2, Z2, Y2, X2],
where W, X, Y , Z are the B-format ambisonics channels if the
phase (p) information is not considered. Whether we want to
1L3DAS21 dataset and code are available at: https://github.com/l3das/
L3DAS21.

--- PAGE 9 ---
9
W1
 Z1
 Y1
 X1
 W1p
 Z1p
 Y1p
 X1p
Fig. 7. Sample spectrograms from L3DAS21 dataset recorded by one
microphone with four capsules.The Ô¨Årst four Ô¨Ågures represent the magnitudes
while the last four contain the corresponding phases information. The black
sections represent silent instants.
include also this information, the order will be [W1, Z1, Y1,
X1, W1p, Z1p, Y1p, X1p, W2, Z2, Y2, X2, W2p, Z2p, Y2p,
X2p] up to 16channels. In Fig.7, we show the 8-channel input
when considering one microphone and the phase information.
Magnitudes and phases are normalized to be centered in 0
with standard deviation 1.
We perform experiments with multiple conÔ¨Ågurations of this
dataset. We Ô¨Årst test the recordings from one microphone con-
sidering the magnitudes only ( 4channels input), then we test
the networks with the signals recorded by two microphones
and magnitudes only ( 8channels input). The extracted features
by the preprocessing are fed to the four-layer convolutional
stack with 64;128;256;512 Ô¨Ålters, with batch normalization,
ReLU activation, max pooling and dropout (probability 0:3),
with pooling sizes (8;2);(8;2);(2;2);(1;1). The bidirectional
GRU module has three layers, each with an hidden size of
256. The tail is a four-layer fully connected classiÔ¨Åer with
1024 Ô¨Ålters alternated by ReLUs and with a Ô¨Ånal dropout and
a sigmoid activation function. The initial learning rate is set to
0:00001 . To be consistent with pre-existing literature metrics ,
we deÔ¨Åne True Positives as TP, False Positives as FP and False
Negatives as FN. These are computed according to the detec-
tion metric [56]. Moreover, in order to compute the Error Rate
(ER), we consider: S = min( FN;FP), D= max(0;FN FP)
and I = max(0;FP FN), as in [52], [55]. Therefore, we
consider:
Fscore=2TP
2TP+FP+FN;
ER=S+D+I
N;
wherebyNis the total number of active sound event classes
in the reference. The SED score is deÔ¨Åned by:
SED score=ER+ 1 Fscore
2:
Params Saving
F
SEDTraining Time
Saving
Inference Time
Saving1-ERSEDnet
Quat SEDnet
PH SEDnet n=2
PH SEDnet n=4
PH SEDnet n=8Fig. 8. Radar plot for SEDnets results on L3DAS21 dataset with two
microphones. The larger is the area, the better is the results. With the same
computational time, PHC n= 2 gains better scores with respect to PHC
n= 4 at a cost of more parameters. The real-valued SEDnet, although the
discrete SED scores, has a high computational time demand as well the largest
number of parameters.
For ER and SED score, the lower scores, the better the
performance, while for the F scorehigher values stand for better
accuracy.
B. Experimental Results
We investigate PHSEDnets in complex, quaternion and
octonion domain with n= 2;4;8and train each network
for1000 epochs with a batch size of 16. The proposed
parameterized hypercomplex SEDnets distinctly outperform
real and quaternion-valued baselines, as reported in Table
IV and Table V. Indeed, the PHSEDnet with n= 2 gains
the best results for each score and in both one and two
microphone datasets, proving that the weights sharing due to
the hypercomplex parameterization is able to capture more
information regardless the lower number of parameters. It
is interesting to note that the PHSEDnet n= 4 , which
operates in the quaternion domain, achieves improved scores
with respect to the Quaternion SEDnet that follows the rigid
predeÔ¨Åned algebra rules. Further, the malleability of PHC
layers allows gaining comparable performance with respect
to the quaternion baseline even so reducing convolutional
parameters by 87%, just setting n= 8. In Section VIII-B,
we show additional experimental results of PH models able to
save 94% of convolutional parameters while operating in the
sedonion domain by involving n= 16 .
Furthermore, PHSEDnets are more efÔ¨Åcient in terms of time
required for training and inference. Table V shows also that
each tested version of the proposed method is faster regards
as the real SEDnet and the quaternion one, both at training
and at inference time. Time efÔ¨Åciency is crucial in audio
applications where networks are usually trained for thousands
of epochs and datasets are very large and require protracted
computations.
Figure 8 summarises number of parameters, metrics scores
and computational time in a radar plot from which it is clear
that PHSEDnet n= 2 gains the best scores and a large time
saving at a cost of more parameters with respect to other

--- PAGE 10 ---
10
TABLE IV
SED NETS RESULTS WITH ONE MICROPHONE (4CHANNELS INPUT ). S CORES ARE COMPUTED OVER THREE RUNS WITH DIFFERENT SEEDS AND WE
REPORT THE MEAN . THE PROPOSED METHOD WTIH n= 2 FAR EXCEEDS THE BASELINES IN EACH METRIC CONSIDERED .
Model Conv Params F score" ER# SED score# P" R"
SEDnet 1.6M 0.637 0.450 0.406 0.756 0.5505
Quaternion SEDnet 0.4M (-75%) 0.580 0.516 0.468 0.724 0.484
PHSEDnet n= 2 0.8M (-50%) 0.680 0.389 0.355 0.767 0.611
PHSEDnet n= 4 0.4M (-75%) 0.638 0.453 0.407 0.765 0.547
TABLE V
SED NETS RESULTS WITH TWO MICROPHONES (8CHANNELS INPUT ). S CORES ARE COMPUTED OVER THREE RUNS WITH DIFFERENT SEEDS AND WE
REPORT THE MEAN . THEPHSED NETn= 2 OUTPERFORM THE BASELINES . FOR TRAINING TIME (SECONDS /ITERATION )THE MEAN AND THE
STANDARD DEVIATION OVER ONE EPOCH IS REPORTED ,FOR INFERENCE TIME WE REPORT THE TIME REQUIRED TO PERFORM AN ITERATION ON THE
VALIDATION SET . PH- BASED MODELS FAR EXCEED BASELINES BOTH IN TRAINING AND INFERENCE TIME .
Model Conv Params F score" ER# SED score# P" R" Time (T) Time (I)
SEDnet 1.6M 0.663 0.428 0.383 0.788 0.572 1.2420.088 1.198
Quaternion SEDnet 0.4M (-75%) 0.559 0.556 0.499 0.754 0.444 1.308 0.088 1.298
PHSEDnet n= 2 0.8M (-50%) 0.669 0.406 0.368 0.767 0.594 1.0910.074 1.085
PHSEDnet n= 4 0.4M (-75%) 0.638 0.433 0.397 0.729 0.567 1.0910.032 1.077
PHSEDnet n= 8 0.2M (-87%) 0.553 0.560 0.503 0.747 0.439 1.142 0.042 1.173
TABLE VI
EXPERIMENTS ON SVHN DATASET WITH THE SMALLEST NETWORKS
FROM EACH FAMILY , RESNET20AND VGG11, THE LATTER WITH
MODIFIED NUMBER OF FILTERS IN ORDER TO BE DIVIDED BY EACH VALUE
OFnAND FC LAYERS IN THE CLOSING CLASSIFIER . W E TEST ALSO THE
PHNN WITH n= 1 TO REPLICATE THE REAL DOMAIN WHICH
OUTPERFORM THE REAL -VALUED RESNET20.
Model Params SVHN
ResNet20 0.27M 90.463
Quaternion ResNet20 0.07M (-75%) 93.535
PHResNet20 n= 1 0.27M 93.796
PHResNet20 n= 2 0.14M (-50%) 93.708
PHResNet20 n= 4 0.07M (-75%) 93.669
VGG11 13.8M 93.488
Quaternion VGG11 3.9M (-71%) 92.888
PHVGG11 n= 2 7.2M (-48%) 93.958
PHVGG11 n= 3 5.0M (-64%) 93.804
PHVGG11 n= 4 3.9M (-71%) 93.919
versions but the real one. A good trade-off is brought by
the PH model n= 4 which further reduces the number of
parameters at the cost of slightly worse SED score and ER.
Moreover, the real-valued SEDnet is capable of obtaining fair
scores while having the largest parameters amount and high
computational time demanding.
VIII. A BLATION STUDIES
A. Less parameters do not lead to higher generalization
In the following, we demonstrate that higher accuracies
achieved by our method are not caused by the parameter
reduction which may lead to more generalization. To this
end, we perform multiple experiments. First, we test lighter
ResNets that were originally built for the CIFAR10 dataset
[47]: ResNet20, ResNet56 and ResNet110. Second, we con-
sider also the smallest VGG network, that is the VGG11
which has 14M parameters. Finally, we perform experimentsTABLE VII
THE FIRST LINES REPORT VGG16 RESULTS WITH REAL -VALUED
CLASSIFIER FOR QUATERNION AND PHNN S. EXTENSION OF TABLE I.
ADDITIONAL EXPERIMENTS WITH RESNET56AND RESNET110, THE
LATTER WITH MODIFIED NUMBER OF FILTERS IN ORDER TO BE DIVIDED
BY EACH VALUE OF n. ACCURACY SCORE IS THE MEAN OVER THREE
RUNS WITH DIFFERENT SEEDS .
Model Params SVHN CIFAR10
Quaternion VGG16 4.2M (-72%) 94.086 84.126
PHVGG16 n= 2 7.9M (-62%) 94.885 86.147
PHVGG16 n= 4 4.2M (-72%) 94.562 85.710
ResNet56 0.9M 94.116 83.700
Quaternion ResNet56 0.2M (-75%) 93.664 81.687
PHResNet56 n= 2 0.4M (-50%) 93.722 83.413
PHResNet56 n= 4 0.2 (-75%) 94.122 82.720
ResNet110 16.7M 93.461 84.810
Quaternion ResNet110 4.2M (-75%) 92.788 83.920
PHResNet110 n= 2 8.4M (-50%) 93.746 83.220
PHResNet110 n= 3 5.6M (-66%) 94.712 85.200
PHResNet110 n= 4 4.2M (-75%) 94.885 85.280
on SVHN, CIFAR10 and CIFAR100 with the larger ResNet18,
ResNet50 and ResNet152 reducing the number of Ô¨Ålters by
75% so to have the same number of parameters of quaternion
and PHNN with n= 4 counterparts.
Table VI reports experiments with ResNet20 where we test
alson= 1 to replicate the real-valued model, outperforming
it. Experiments with VGG11 with modiÔ¨Åed number of Ô¨Ålters
in order to be divided by each value of nis also reported in
the same table. Finally, in Table VII we report experiments
on SVHN and CIFAR10 with ResNet56 and ResNet110, the
latter with modiÔ¨Åed number of Ô¨Ålters. PH models gain good
performance in each test we conduct while reducing the
amount of free parameters. Indeed, the PHResNet20s gain
almost 94% of accuracy on the SVHN dataset involving just
70k parameters.
Finally, in order to further remove the hypothesis that

--- PAGE 11 ---
11
TABLE VIII
REAL-VALUED RESNETS WITH CONVOLUTIONAL FILTERS REDUCED BY
75% ,DENOTED BY (S). F ULL MODELS EXCEEDS REDUCED VERSIONS IN
EACH OF THE EXPERIMENT ,PROVING THAT A SMALLER NUMBER OF
PARAMETERS DO NOT LEAD TO HIGHER GENERALIZATION CAPABILITIES .
Model Params SVHN CIFAR10 CIFAR100
ResNet18 10.1M 93.992 89.543 62.634
ResNet18 (s) 2.7M (-75%) 93.842 88.310 59.590
ResNet50 22.5M 94.546 89.630 65.514
ResNet50 (s) 5.7M (-75%) 93.915 89.370 62.450
ResNet152 52.6M 94.625 89.580 62.053
ResNet152 (s) 13.2M (-75%) 94.400 89.001 60.850
smaller number of neural parameters leads to higher general-
ization capabilities, we perform experiments with real-valued
baselines with a number of parameters reduced by 75%. Table
VIII shows that reducing the number of Ô¨Ålters downgrades
the performance and thus it is not sufÔ¨Åcient to improve the
generalization capabilities of a model. We do not include
standard deviations for values in the ablation studies as the
values are similar to the previous examples so we aim at
favoring paper readability.
B. Push the hyperparameter nup to 16
In the following, we perform additional experiments for the
sound event detection task. We conduct a test considering
two microphones and the phase information, so to have an
input with 16channels. For this purposes, we consider as
baseline the quaternion model and PHNNs with n= 4;8;16
so to test higher order domains. Quaternion and PHSEDnet
withn= 4 manage the 16channels by grouping them in
four components, thus assembling them in 4channels: one
channel containing the magnitudes of the Ô¨Årst microphone,
one channel the phases of the same microphone, and so on.
Therefore, the details coming from the magnitudes, which are
the most important for sound event detection, are grouped
together without properly exploiting this information. On the
contrary, employing PHC layers allows the model to process
information without roughly grouping channels while instead
leveraging every information by easily setting nequal to the
number of channels, that is in this case 16. From Table IX, it is
clear that employing a 4-channel model such as Quaternion or
PHC withn= 4does not lead to higher performance, despite
the higher number of parameters. Indeed, the best scores are
obtained with PHC models involving n= 8 andn= 16 that
are able to grasp information from each channel.
IX. C ONCLUSION
In this paper, we introduce a parameterized hypercomplex
convolutional (PHC) layer which grasps the convolution rule
directly from data and can operate in any domain from 1D
tonD, regardless the algebra regulations are preset. The
proposed approach reduces the convolution parameters to 1=n
with respect to real-valued counterparts and allows capturing
internal latent relations thanks to parameter sharing among
input dimensions. Employing this method, jointly with the onein [37], we devise the family of parameterized hypercomplex
neural networks (PHNNs), a set of lightweight and efÔ¨Åcient
neural models exploiting hypercomplex algebra properties for
increased performance and high Ô¨Çexibility. We show our
method is Ô¨Çexible to operate in different Ô¨Åelds of application
by performing experiments with images and audio signals. We
also prove the malleability and the robustness of our approach
to learn convolution rules in any domain by setting different
values for the hyperparameter nfrom 2to16.
CO2 Emission Related to Experiments
Experiments were conducted using a private infrastructure,
which has a carbon efÔ¨Åciency of 0.445 kgCO 2eq/kWh. A
cumulative of 2000 hours of computation was performed on
hardware of type Tesla V100-SXM2-32GB (TDP of 300W).
Total emissions are estimated to be 267 kgCO 2eq of which
0 percents were directly offset. Estimations were conducted
using the MachineLearning Impact calculator presented in
[57].
More in detail, considering an experiment for the sound
event detection (SED) task, according to Table V, the real-
valued baseline requires approximately 20 hours for training
and validation, with a corresponding carbon emissions of
2:71kgCO 2eq. Conversely, the proposed PH model takes
approximately 17hours with a reduction of carbon emissions
of16%, being 2:28kgCO 2eq.
In conclusion, we believe that the improved efÔ¨Åciency of
our method with respect to standard models may be a little
step towards reducing carbon emissions.
REFERENCES
[1] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
‚ÄúAnalyzing and improving the image quality of StyleGAN,‚Äù IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR) , 2020.
[2] S. d‚ÄôAscoli, H. Touvron, M. Leavitt, A. Morcos, G. Biroli, and L. Sagun,
‚ÄúConvit: Improving vision transformers with soft convolutional inductive
biases,‚Äù arXiv preprint: arXiv:2103.10697 , 2021.
[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth 16x16 words:
Transformers for image recognition at scale,‚Äù in Int. Conf. on Learning
Representations (ICLR) , 2021.
[4] E. Real, A. Aggarwal, Y . Huang, and Q. Le, ‚ÄúRegularized evolution for
image classiÔ¨Åer architecture search,‚Äù Proceedings of the AAAI Conf. on
ArtiÔ¨Åcial Intelligence , vol. 33, pp. 4780‚Äì4789, Jul. 2019.
[5] J. Navarro-Moreno and J. C. Ruiz-Molina, ‚ÄúWide-sense markov signals
on the tessarine domain. a study under properness conditions,‚Äù Signal
Process. , vol. 183, p. 108022, 2021.
[6] J. Navarro-Moreno, R. M. Fern ¬¥andez-Alcal ¬¥a, J. D. Jim ¬¥enez-L ¬¥opez, and
J. C. Ruiz-Molina, ‚ÄúTessarine signal processing under the t-properness
condition,‚Äù Journal of the Franklin Institute , vol. 357, no. 14, pp. 10 100‚Äì
10 126, 2020.
[7] S. Sanei, C. C. Took, and S. Enshaeifar, ‚ÄúQuaternion adaptive line
enhancer based on singular spectrum analysis,‚Äù in IEEE Int. Conf. on
Acoust., Speech and Signal Process. (ICASSP) , 2018, pp. 2876‚Äì2880.
[8] M. Xiang, S. Enshaeifar, A. E. Stott, C. C. Took, Y . Xia, S. Kanna,
and D. P. Mandic, ‚ÄúSimultaneous diagonalisation of the covariance and
complementary covariance matrices in quaternion widely linear signal
processing,‚Äù Signal Process. , vol. 148, pp. 193‚Äì204, 2018.
[9] M. Kobayashi, ‚ÄúQuaternion projection rule for rotor hopÔ¨Åeld neural
networks,‚Äù IEEE Trans. Neural Netw. Learn. Syst. , vol. 32, no. 2, pp.
900‚Äì908, 2021.
[10] D. Lin, X. Chen, Z. Li, B. Li, and X. Yang, ‚ÄúOn the existence of the
exact solution of quaternion-valued neural networks based on a sequence
of approximate solutions,‚Äù IEEE Trans. Neural Netw. Learn. Syst. , pp.
1‚Äì9, 2021.

--- PAGE 12 ---
12
TABLE IX
SED RESULTS WITH TWO MICROPHONE :MAGNITUDES AND PHASES (16 CHANNELS INPUT ). W E TEST HIGHER ORDER HYPERCOMPLEX DOMAINS UP TO
SEDONIONS BY SETTING n= 16 . ALTHOUGH THE INCREDIBLE REDUCTION OF THE NUMBER OF PARAMETERS WITH RESPECT TO THE REAL -VALUED
BASELINE IN TABLE V,THE PHNN WITH n= 16 STILL HAS COMPARABLE PERFORMANCE WITH OTHER MODELS . FURTHERMORE ,THE PHSED NET
WITH n= 8 OUTPERFORM ALSO THE QUATERNION BASELINE WHICH HAS MORE DEGREES OF FREEDOM .
Model Conv Params F score" ER# SED score# P" R"
Quaternion SEDnet 0.4M (-75%) 0.580 0.480 0.450 0.655 0.520
PHSEDnet n= 4 0.4M (-75%) 0.585 0.470 0.443 0.653 0.530
PHSEDnet n= 8 0.2M (-87%) 0.607 0.466 0.430 0.702 0.534
PHSEDnet n= 16 0.1M (-94%) 0.588 0.509 0.461 0.734 0.491
[11] L. Liu, C. L. P. Chen, and Y . Wang, ‚ÄúModal regression-based graph
representation for noise robust face hallucination,‚Äù IEEE Trans. Neural
Netw. Learn. Syst. , pp. 1‚Äì13, 2021.
[12] M. E. Valle and F. Z. De Castro, ‚ÄúOn the dynamics of hopÔ¨Åeld neural
networks on unit quaternions,‚Äù IEEE Trans. Neural Netw. Learn. Syst. ,
vol. 29, no. 6, pp. 2464‚Äì2471, 2018.
[13] Y . Liu, D. Zhang, J. Lou, J. Lu, and J. Cao, ‚ÄúStability analysis
of quaternion-valued neural networks: Decomposition and direct ap-
proaches,‚Äù IEEE Trans. Neural Netw. Learn. Syst. , vol. 29, no. 9, pp.
4201‚Äì4211, 2018.
[14] M. E. Valle and R. A. Lobo, ‚ÄúQuaternion-valued recurrent projection
neural networks on unit quaternions,‚Äù Theoretical Computer Science ,
vol. 843, pp. 136‚Äì152, 2020.
[15] F. Z. De Castro and M. E. Valle, ‚ÄúA broad class of discrete-time
hypercomplex-valued HopÔ¨Åeld neural networks,‚Äù Neural Networks , vol.
122, pp. 54‚Äì67, 2020.
[16] T. K. Paul and T. Ogunfunmi, ‚ÄúA kernel adaptive algorithm for
quaternion-valued inputs,‚Äù IEEE Trans. Neural Netw. Learn. Syst. ,
vol. 26, no. 10, pp. 2422‚Äì2439, Oct. 2015.
[17] A. Hirose, I. Aizenberg, and D. P. Mandic, ‚ÄúGuest editorial special issue
on complex- and hypercomplex-valued neural networks,‚Äù IEEE Trans.
Neural Netw. Learn. Syst. , vol. 25, no. 9, pp. 1597‚Äì1599, 2014.
[18] A. Muppidi and M. Radfar, ‚ÄúSpeech emotion recognition using quater-
nion convolutional neural networks,‚Äù in IEEE Int. Conf. on Acoustics,
Speech and Signal Process. (ICASSP) , 2021, pp. 6309‚Äì6313.
[19] T. Parcollet, M. Ravanelli, M. Morchid, G. Linar `es, C. Trabelsi,
R. De Mori, and Y . Bengio, ‚ÄúQuaternion recurrent neural networks,‚Äù
inInt. Conf. on Learning Representations (ICLR) , New Orleans, LA,
May 2019, pp. 1‚Äì19.
[20] E. Grassucci, E. Cicero, and D. Comminiello, ‚ÄúQuaternion generative
adversarial networks,‚Äù in Generative Adversarial Learning: Architec-
tures and Applications , R. Razavi-Far, A. Ruiz-Garcia, V . Palade, and
J. Schmidhuber, Eds. Cham: Springer International Publishing, 2022,
pp. 57‚Äì86.
[21] Y . Tay, A. Zhang, A. T. Luu, J. Rao, S. Zhang, S. Wang, J. Fu, and
C. S. Hui, ‚ÄúLightweight and efÔ¨Åcient neural natural language processing
with quaternion networks,‚Äù in ACL (1) . Association for Computational
Linguistics, 2019, pp. 1494‚Äì1503.
[22] A. Cariow and G. Cariowa, ‚ÄúFast algorithms for deep octonion net-
works,‚Äù IEEE Trans. Neural Netw. Learn. Syst. , Nov. 2021.
[23] J. Wu, L. Xu, F. Wu, Y . Kong, L. Senhadji, and H. Shu, ‚ÄúDeep octonion
networks,‚Äù Neurocomputing , vol. 397, pp. 179‚Äì191, 2020.
[24] M. E. Valle and R. A. Lobo, ‚ÄúHypercomplex-valued recurrent correlation
neural networks,‚Äù Neurocomputing , vol. 432, pp. 111‚Äì123, 2021.
[25] T. Chen, H. Yin, X. Zhang, Z. Huang, Y . Wang, and M. Wang,
‚ÄúQuaternion factorization machines: A lightweight solution to intricate
feature interaction modeling,‚Äù IEEE Trans. Neural Netw. Learn. Syst. ,
pp. 1‚Äì14, 2021.
[26] E. Grassucci, D. Comminiello, and A. Uncini, ‚ÄúA quaternion-valued
variational autoencoder,‚Äù in IEEE Int. Conf. on Acoust., Speech and
Signal Process. (ICASSP) , Toronto, Canada, Jun. 2021.
[27] ‚Äî‚Äî, ‚ÄúAn information-theoretic perspective on proper quaternion vari-
ational autoencoders,‚Äù Entropy , vol. 23, no. 7, 2021.
[28] S. Gai and X. Huang, ‚ÄúReduced biquaternion convolutional neural
network for color image processing,‚Äù IEEE Trans. on Circuits and
Systems for Video Technology , pp. 1‚Äì1, 2021.
[29] G. Vieira and M. E. Valle, ‚ÄúExtreme learning machines on Cayley-
Dickson algebra applied for color image auto-encoding,‚Äù in IEEE Int.
Joint Conf. on Neural Netw. (IJCNN) , 2020, pp. 1‚Äì8.[30] C. C. Took and Y . Xia, ‚ÄúMultichannel quaternion least mean square
algorithm,‚Äù in IEEE Int. Conf. on Acoust., Speech and Signal Process.
(ICASSP) , 2019, pp. 8524‚Äì8527.
[31] C. J. Gaudet and A. S. Maida, ‚ÄúRemoving dimensional restrictions on
complex/hyper-complex neural networks,‚Äù in 2021 IEEE Int. Conf. on
Image Process. (ICIP) , 2021, pp. 319‚Äì323.
[32] J. Hoffmann, S. Schmitt, S. Osindero, K. Simonyan, and E. Elsen,
‚ÄúAlgebranets,‚Äù ArXiv preprint: arXiv:2006.07360 , 2020.
[33] A. Cariow and G. Cariowa, ‚ÄúFast algorithms for quaternion-valued
convolutional neural networks,‚Äù IEEE Trans. Neural Netw. Learn. Syst. ,
vol. 32, no. 1, pp. 457‚Äì462, 2021.
[34] C. Huang, A. Touati, P. Vincent, G. K. Dziugaite, A. Lacoste, and A. C.
Courville, ‚ÄúStochastic neural network with Kronecker Ô¨Çow,‚Äù in AISTATS ,
2020.
[35] Z. Tang, F. Jiang, M. Gong, H. Li, Y . Wu, F. Yu, Z. Wang, and
M. Wang, ‚ÄúSKFAC: Training neural networks with faster Kronecker-
factored approximate curvature,‚Äù in IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR) , 2021, pp. 13 479‚Äì13 487.
[36] D. Wang, B. Wu, G. S. Zhao, H. Chen, L. Deng, T. Yan, and G. Li,
‚ÄúKronecker CP decomposition with fast multiplication for compressing
RNNs,‚Äù IEEE Trans. Neural Netw. Learn. Syst. , vol. PP, 2021.
[37] A. Zhang, Y . Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu,
‚ÄúBeyond fully-connected layers with quaternions: Parameterization of
hypercomplex multiplications with 1=nparameters,‚Äù Int. Conf. on
Machine Learning (ICML) , 2021.
[38] T. Le, M. Bertolini, F. No ¬¥e, and D. A. Clevert, ‚ÄúParameterized hyper-
complex graph neural networks for graph classiÔ¨Åcation,‚Äù ArXiv preprint:
arXiv:2103.16584 , 2021.
[39] R. K. Mahabadi, J. Henderson, and S. Ruder, ‚ÄúCompacter: EfÔ¨Åcient low-
rank hypercomplex adapter layers,‚Äù ArXiv preprint: arXiv:2106.04647 ,
2021.
[40] H. Wu, B. Xiao, N. C. F. Codella, M. Liu, X. Dai, L. Yuan, and
L. Zhang, ‚ÄúCvt: Introducing convolutions to vision transformers,‚Äù ArXiv ,
vol. abs/2103.15808, 2021.
[41] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen,
R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, M. Slaney,
R. J. Weiss, and K. Wilson, ‚ÄúCNN architectures for large-scale audio
classiÔ¨Åcation,‚Äù in IEEE Int. Conf. on Acoustics, Speech and Signal
Process. (ICASSP) , 2017, pp. 131‚Äì135.
[42] T. Parcollet, M. Morchid, and G. Linar `es, ‚ÄúA survey of quaternion neural
networks,‚Äù Artif. Intell. Rev. , Aug. 2019.
[43] C. Gaudet and A. Maida, ‚ÄúDeep quaternion networks,‚Äù in IEEE Int. Joint
Conf. on Neural Netw. (IJCNN) , Rio de Janeiro, Brazil, Jul. 2018.
[44] H. V . Henderson, F. Pukelsheim, and S. R. Searle, ‚ÄúOn the history of
the kronecker product,‚Äù Linear and Multilinear Algebra , vol. 14, no. 2,
pp. 113‚Äì120, 1983.
[45] T. Parcollet, M. Morchid, and G. Linar `es, ‚ÄúQuaternion convolutional
neural networks for heterogeneous image processing,‚Äù in IEEE Int. Conf.
on Acoust., Speech and Signal Process. (ICASSP) , Brighton, UK, May
2019, pp. 8514‚Äì8518.
[46] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù in Int. Conf. on Learning Representations
(ICLR) , San Diego, CA, USA, 2015.
[47] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for
image recognition,‚Äù in IEEE/CVF Conf. on Computer Vision and Pattern
Recognition (CVPR) , 2016, pp. 770‚Äì778.
[48] T. Nitta, ‚ÄúA quaternary version of the back-propagation algorithm,‚Äù 1995,
pp. 2753‚Äì2756.
[49] D. Comminiello, M. Lella, S. Scardapane, and A. Uncini, ‚ÄúQuaternion
convolutional neural networks for detection and localization of 3D sound

--- PAGE 13 ---
13
events,‚Äù in IEEE Int. Conf. on Acoust., Speech and Signal Process.
(ICASSP) , Brighton, UK, May 2019, pp. 8533‚Äì8537.
[50] M. Ricciardi Celsi, S. Scardapane, and D. Comminiello, ‚ÄúQuaternion
neural networks for 3D sound source localization in reverberant en-
vironments,‚Äù in IEEE Int. Workshop on Machine Learning for Signal
Process. , Espoo, Finland, Sep. 2020, pp. 1‚Äì6.
[51] E. Grassucci, G. Mancini, C. Brignone, A. Uncini, and D. Comminiello,
‚ÄúDual quaternion ambisonics array for six-degree-of-freedom acoustic
representation,‚Äù arXiv preprint: arXiv:2204.01851 , 2022.
[52] S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen, ‚ÄúSound event
localization and detection of overlapping sources using convolutional
recurrent neural networks,‚Äù IEEE Journal of Selected Topics in Signal
Processing , vol. 13, pp. 34‚Äì48, 2019.
[53] R. Wightman, H. Touvron, and J. H., ‚ÄúResnet strikes back: An improved
training procedure in timm,‚Äù ArXiv preprint: arXiv:2110.00476 , 2021.
[54] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, ‚ÄúGrad-CAM: Visual explanations from deep networks via
gradient-based localization,‚Äù in IEEE Int. Conf. on Computer Vision
(ICCV) , 2017, pp. 618‚Äì626.
[55] A. Mesaros, T. Heittola, T. Virtanen, and M. D. Plumbley, ‚ÄúSound event
detection: A tutorial,‚Äù IEEE Signal Processing Magazine , vol. 38, no. 5,
pp. 67‚Äì83, 2021.
[56] E. Guizzo, R. F. Gramaccioni, S. Jamili, C. Marinoni, E. Massaro,
C. Medaglia, G. Nachira, L. Nucciarelli, L. Paglialunga, M. Pennese,
S. Pepe, E. Rocchi, A. Uncini, and D. Comminiello, ‚ÄúL3DAS21 Chal-
lenge: Machine learning for 3D audio signal processing,‚Äù 2021 IEEE
Int. Workshop on Machine Learning for Signal Process. (MLSP) , 2021.
[57] A. Lacoste, A. Luccioni, V . Schmidt, and T. Dandres, ‚ÄúQuanti-
fying the carbon emissions of machine learning,‚Äù ArXiv preprint:
arXiv:1910.09700 , 2019.
