# 2003.00152.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/convolution/2003.00152.pdf
# Kích thước tệp: 2244618 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
HUẤN LUYỆN BATCH NORM VÀ CHỈ BATCH NORM:
VỀ SỨC MẠNH BIỂU HIỆN CỦA CÁC ĐẶC TRƯNG NGẪU NHIÊN
TRONG CNN
Jonathan Frankle
MIT CSAIL
jfrankle@mit.edu David J. Schwab
CUNY Graduate Center, ITS
Facebook AI Research
dschwab@fb.com Ari S. Morcos
Facebook AI Research
arimorcos@fb.com

TÓM TẮT
Một loạt các kỹ thuật học sâu từ chuyển đổi phong cách đến học đa nhiệm vụ
dựa vào việc huấn luyện các phép biến đổi afﬁne của các đặc trưng. Nổi bật nhất trong số này
là kỹ thuật chuẩn hóa đặc trưng phổ biến BatchNorm, chuẩn hóa
các kích hoạt và sau đó áp dụng một phép biến đổi afﬁne đã học. Trong bài báo này,
chúng tôi nhằm hiểu vai trò và sức mạnh biểu hiện của các tham số afﬁne được sử dụng để
biến đổi đặc trưng theo cách này. Để tách biệt sự đóng góp của các tham số này khỏi
các đặc trưng đã học mà chúng biến đổi, chúng tôi nghiên cứu hiệu suất đạt được
khi chỉ huấn luyện các tham số này trong BatchNorm và đóng băng tất cả các trọng số tại
khởi tạo ngẫu nhiên của chúng. Làm như vậy dẫn đến hiệu suất cao một cách đáng ngạc nhiên
xem xét những hạn chế đáng kể mà phong cách huấn luyện này áp đặt. Ví dụ,
các ResNet đủ sâu đạt 82% (CIFAR-10) và 32% (ImageNet, top-5) độ chính xác
trong cấu hình này, cao hơn nhiều so với khi huấn luyện một số lượng tương đương
các tham số được chọn ngẫu nhiên ở nơi khác trong mạng. BatchNorm
đạt được hiệu suất này một phần bằng cách tự nhiên học cách vô hiệu hóa khoảng một phần ba
các đặc trưng ngẫu nhiên. Những kết quả này không chỉ làm nổi bật sức mạnh biểu hiện của
các tham số afﬁne trong học sâu, mà—theo nghĩa rộng hơn—chúng đặc trưng hóa
sức mạnh biểu hiện của các mạng nơ-ron được xây dựng đơn giản bằng cách dịch chuyển và
tái tỷ lệ các đặc trưng ngẫu nhiên.

1 GIỚI THIỆU
Trong suốt tài liệu về học sâu, một loạt các kỹ thuật dựa vào việc học các phép biến đổi afﬁne
của các đặc trưng—nhân mỗi đặc trưng với một hệ số đã học γ và thêm một độ lệch đã học β.
Điều này bao gồm mọi thứ từ học đa nhiệm vụ (Mudrakarta et al., 2019) đến chuyển đổi phong cách
và sinh tạo (ví dụ, Dumoulin et al., 2017; Huang & Belongie, 2017; Karras et al., 2019). Một trong
những ví dụ phổ biến nhất của các tham số afﬁne này là trong các kỹ thuật chuẩn hóa đặc trưng như
BatchNorm (Ioffe & Szegedy, 2015). Xem xét tầm quan trọng thực tiễn của chúng và sự hiện diện
trong hầu như tất cả các mạng nơ-ron hiện đại, chúng ta biết tương đối ít về vai trò và sức mạnh
biểu hiện của các tham số afﬁne được sử dụng để biến đổi đặc trưng theo cách này.

Để có cái nhìn sâu sắc về câu hỏi này, chúng tôi tập trung vào các tham số γ và β trong BatchNorm.
BatchNorm gần như phổ biến trong các mạng nơ-ron tích chập sâu (CNN) cho thị giác máy tính,
có nghĩa là các tham số afﬁne này có mặt theo mặc định trong nhiều mô hình mà các nhà nghiên cứu
và thực hành viên huấn luyện hàng ngày. Tính toán BatchNorm tiến hành theo hai bước trong quá trình
huấn luyện (xem Phụ lục A để biết chi tiết đầy đủ). Đầu tiên, mỗi tiền kích hoạt1 được chuẩn hóa
theo giá trị trung bình và độ lệch chuẩn trên mini-batch. Các tiền kích hoạt được chuẩn hóa này
sau đó được tỷ lệ và dịch chuyển bởi một hệ số có thể huấn luyện theo đặc trưng γ và độ lệch β.

Một sự thật mà chúng ta biết về γ và β trong BatchNorm là sự hiện diện của chúng có tác động
có ý nghĩa đến hiệu suất của ResNet, cải thiện độ chính xác 0.5% đến 2% trên CIFAR-10
(Krizhevsky et al., 2009) và 2% trên ImageNet (Deng et al., 2009) (Hình 1). Những cải thiện này
đủ lớn để, nếu γ và β được đề xuất như một kỹ thuật mới, nó có thể sẽ được áp dụng rộng rãi.
Tuy nhiên, chúng đủ nhỏ để khó tách biệt vai trò cụ thể mà γ và β đóng trong những cải thiện này.

Tổng quát hơn, thách thức trung tâm của việc nghiên cứu khoa học các tham số afﬁne theo đặc trưng
là phân biệt sự đóng góp của chúng khỏi các đặc trưng mà chúng biến đổi. Trong tất cả các bối cảnh
thực tế, các tham số afﬁne này được huấn luyện cùng với (như trong trường hợp của BatchNorm) hoặc
sau các đặc trưng (Mudrakarta et al., 2019; Dumoulin et al., 2017). Để nghiên cứu các tham số này
một cách tách biệt, thay vào đó chúng tôi huấn luyện chúng trên một mạng bao gồm hoàn toàn các
đặc trưng ngẫu nhiên. Cụ thể, chúng tôi đóng băng tất cả các trọng số tại khởi tạo và chỉ huấn luyện
các tham số γ và β trong BatchNorm.

Mặc dù các mạng vẫn giữ cùng số lượng đặc trưng, chỉ một phần nhỏ các tham số (nhiều nhất 0.6%)
có thể huấn luyện được. Thí nghiệm này buộc tất cả việc học diễn ra trong γ và β, làm cho có thể
đánh giá sức mạnh biểu hiện của một mạng mà bậc tự do duy nhất là tỷ lệ và dịch chuyển các đặc trưng
ngẫu nhiên. Chúng tôi nhấn mạnh rằng mục tiêu của chúng tôi mang tính khoa học: đánh giá hiệu suất
và các cơ chế mà các mạng sử dụng khả năng hạn chế này để biểu diễn các hàm có ý nghĩa; chúng tôi
không có ý định cũng không mong đợi thí nghiệm này đạt độ chính xác SOTA. Chúng tôi có các
phát hiện sau:

• Khi chỉ huấn luyện γ và β, các mạng đủ sâu (ví dụ, ResNet-866 và ResNet-200) đạt độ chính xác
cao một cách đáng ngạc nhiên (mặc dù không phải SOTA): 82% trên CIFAR-10 và 32% top-5 trên
ImageNet. Điều này chứng minh sức mạnh biểu hiện của các tham số BatchNorm afﬁne.

• Huấn luyện một số lượng tương đương các tham số được chọn ngẫu nhiên trên mỗi kênh hoạt động
tệ hơn nhiều (56% trên CIFAR-10 và 4% top-5 trên ImageNet). Điều này chứng minh rằng γ và β
có sức mạnh biểu hiện đặc biệt quan trọng như các hệ số và độ lệch theo đặc trưng.

• Khi chỉ huấn luyện BatchNorm, γ tự nhiên học cách vô hiệu hóa giữa một phần tư đến một nửa
tất cả các kênh bằng cách hội tụ về các giá trị gần bằng không. Điều này chứng minh rằng γ và β
đạt được độ chính xác này một phần bằng cách áp đặt độ thưa thớt theo đặc trưng.

• Khi huấn luyện tất cả các tham số, các mạng sâu hơn và rộng hơn có các giá trị γ nhỏ hơn nhưng
ít đặc trưng bị vô hiệu hóa hoàn toàn. Điều này gợi ý về vai trò mà γ có thể đóng trong việc kiểm
soát các kích hoạt trong các cài đặt mà việc vô hiệu hóa γ và β dẫn đến độ chính xác thấp hơn
(các phần bên phải của các đồ thị trong Hình 1).

Tóm lại, chúng tôi thấy rằng γ và β có sức mạnh biểu hiện đáng chú ý riêng và sức mạnh biểu hiện
này xuất phát từ vị trí đặc biệt của chúng như một hệ số và độ lệch theo đặc trưng. Ngoài việc cung cấp
hiểu biết về các tham số afﬁne biến đổi đặc trưng, quan sát này có ý nghĩa rộng hơn đối với hiểu biết
của chúng ta về các mạng nơ-ron bao gồm các đặc trưng ngẫu nhiên. Bằng cách đóng băng tất cả
các tham số khác tại khởi tạo, chúng tôi đang huấn luyện các mạng được xây dựng bằng cách học các
dịch chuyển và tái tỷ lệ của các đặc trưng ngẫu nhiên. Dưới ánh sáng này, kết quả của chúng tôi
chứng minh rằng các đặc trưng ngẫu nhiên có sẵn tại khởi tạo cung cấp nguyên liệu thô đủ để biểu diễn
các hàm độ chính xác cao cho phân loại hình ảnh. Mặc dù các công trình trước đây xem xét các mô hình
với các đặc trưng ngẫu nhiên và một lớp đầu ra tuyến tính có thể huấn luyện (ví dụ, Rahimi & Recht,
2009; Jaeger, 2003; Maass et al., 2002), chúng tôi tiết lộ sức mạnh biểu hiện của các mạng được
cấu hình sao cho các tham số afﬁne có thể huấn luyện xuất hiện sau mỗi đặc trưng ngẫu nhiên.

2 CÔNG TRÌNH LIÊN QUAN
BatchNorm. BatchNorm làm cho có thể huấn luyện các mạng sâu hơn (He et al., 2015a) và khiến
SGD hội tụ sớm hơn (Ioffe & Szegedy, 2015). Tuy nhiên, các cơ chế cơ bản mà nó thực hiện
được tranh luận. Các tác giả ban đầu lập luận rằng nó giảm dịch chuyển hiệp phương sai nội bộ (ICS),
trong đó "phân phối đầu vào của mỗi lớp thay đổi trong quá trình huấn luyện...yêu cầu tốc độ học
thấp hơn" (Ioffe & Szegedy, 2015). Santurkar et al. (2018) đặt nghi ngờ về lời giải thích này bằng
cách nhân tạo gây ra ICS sau BatchNorm với ít thay đổi trong thời gian huấn luyện. Bằng chứng thực
nghiệm cho thấy BatchNorm làm cho bề mặt tối ưu hóa mượt mà hơn (Santurkar et al., 2018); là một
"biện pháp phòng ngừa an toàn" chống lại các kích hoạt bùng nổ dẫn đến phân kỳ (Bjorck et al., 2018);
và cho phép mạng sử dụng tốt hơn các nơ-ron (Balduzzi et al., 2017; Morcos et al., 2018). Kết quả
lý thuyết cho thấy BatchNorm tách biệt việc tối ưu hóa độ lớn và hướng trọng số (Kohler et al., 2019)
như chuẩn hóa trọng số (Salimans & Kingma, 2016) làm một cách rõ ràng; rằng nó khiến độ lớn
gradient đạt cân bằng (Yang et al., 2019); và rằng nó dẫn đến một hình thức điều chuẩn mới (Luo et al., 2019).

Chúng tôi tập trung vào vai trò và sức mạnh biểu hiện của các tham số afﬁne nói riêng, trong khi
các công trình được đề cập ở trên giải quyết tác động tổng thể của BatchNorm đến quá trình tối ưu hóa.
Phục vụ cho mục tiêu rộng lớn hơn này, các công trình liên quan thường nhấn mạnh khía cạnh chuẩn hóa
của BatchNorm, trong một số trường hợp bỏ qua một (Kohler et al., 2019) hoặc cả hai γ và β
(Santurkar et al., 2018; Yang et al., 2019). Các công trình khác coi BatchNorm như một hộp đen
mà không xem xét cụ thể γ và β (Santurkar et al., 2018; Bjorck et al., 2018; Morcos et al., 2018;
Balduzzi et al., 2017). Một ngoại lệ đáng chú ý là công trình của Luo et al. (2019), người chỉ ra
về mặt lý thuyết rằng BatchNorm áp đặt một sự suy giảm L2 phụ thuộc vào dữ liệu trên γ; chúng tôi
thảo luận thêm về công trình này trong Phần 5.

Khai thác sức mạnh biểu hiện của các phép biến đổi afﬁne. Có nhiều kỹ thuật trong tài liệu học sâu
khai thác sức mạnh biểu hiện của các phép biến đổi afﬁne của đặc trưng. Mudrakarta et al. (2019)
phát triển một cách tiếp cận hiệu quả về tham số cho học đa nhiệm vụ với một backbone mạng được
chia sẻ (được huấn luyện trên một nhiệm vụ cụ thể) và các tập tham số BatchNorm riêng biệt cho
mỗi nhiệm vụ (được huấn luyện trên một nhiệm vụ khác trong khi backbone được đóng băng).
Tương tự, Rebufﬁ et al. (2017) cho phép một backbone mạng được huấn luyện trên một nhiệm vụ
thích nghi với các nhiệm vụ khác thông qua các mô hình dư được thêm vào mạng bao gồm BatchNorm
và một tích chập. Perez et al. (2017) thực hiện trả lời câu hỏi trực quan bằng cách sử dụng một RNN
nhận đầu vào văn bản để tạo ra các hệ số và độ lệch được sử dụng để biến đổi các đặc trưng nội bộ
của một CNN trên dữ liệu hình ảnh. Cuối cùng, công trình về chuyển đổi phong cách nơ-ron và sinh
tạo phong cách sử dụng các phép biến đổi afﬁne của các đặc trưng được chuẩn hóa để mã hóa các
phong cách khác nhau (ví dụ, Dumoulin et al., 2017; Huang & Belongie, 2017; Karras et al., 2019).

Chỉ huấn luyện BatchNorm. Gần với công trình của chúng tôi nhất, Rosenfeld & Tsotsos (2019)
khám phá việc đóng băng các phần khác nhau của mạng tại khởi tạo; trong quá trình đó, họ nghiên cứu
ngắn gọn việc chỉ huấn luyện γ và β. Tuy nhiên, có một số khác biệt quan trọng giữa bài báo này
và công trình của chúng tôi. Họ chỉ kết luận rằng nói chung có thể "huấn luyện thành công các mạng
chủ yếu ngẫu nhiên", trong khi chúng tôi thấy rằng các tham số BatchNorm có sức mạnh biểu hiện
lớn hơn các tham số khác (Hình 2, màu xanh lá). Thực tế, các thí nghiệm của họ không thể tạo ra
sự phân biệt này. Họ chỉ huấn luyện BatchNorm trong chỉ hai mạng CIFAR-10 (DenseNet và một
Wide ResNet không xác định) chỉ trong mười epoch (so với 100+ tiêu chuẩn), đạt 61% và 30%
độ chính xác. Đối với số lượng tham số tương đương, chúng tôi đạt 80% và 70%. Những khác biệt
này ảnh hưởng có ý nghĩa đến kết luận của chúng tôi: chúng cho phép chúng tôi xác định rằng việc
chỉ huấn luyện BatchNorm dẫn đến độ chính xác cao hơn rõ ràng so với việc huấn luyện một số lượng
tương đương các tham số được chọn ngẫu nhiên. Độ chính xác trong Rosenfeld & Tsotsos quá thấp
để tạo ra bất kỳ sự phân biệt nào như vậy.

Hơn nữa, chúng tôi đi xa hơn nhiều về mặt quy mô thí nghiệm và độ sâu phân tích. Chúng tôi
nghiên cứu một loạt mạng rộng hơn nhiều và, quan trọng là, cho thấy rằng việc chỉ huấn luyện
BatchNorm có thể đạt kết quả ấn tượng ngay cả đối với các mạng quy mô lớn trên ImageNet. Chúng tôi
cũng điều tra cách các tham số BatchNorm đạt được hiệu suất này bằng cách kiểm tra các biểu diễn
cơ bản.

Chúng tôi cũng lưu ý rằng Mudrakarta et al. (2019) chỉ huấn luyện BatchNorm và một lớp đầu ra
tuyến tính trên một MobileNet duy nhất, được khởi tạo ngẫu nhiên (trong bối cảnh thực hiện điều này
trên nhiều mạng được huấn luyện cho mục đích học đa nhiệm vụ); họ kết luận đơn giản rằng "khá
ấn tượng" rằng cấu hình này "có thể đạt độ chính xác không tầm thường".

Đặc trưng ngẫu nhiên. Có một lịch sử dài về việc xây dựng mô hình từ các đặc trưng ngẫu nhiên.
Perceptron (Block, 1962) học một tổ hợp tuyến tính của các associator, mỗi cái là tích trong của
đầu vào và một vectơ ngẫu nhiên. Gần đây hơn, Rahimi & Recht (2009) cho thấy về mặt lý thuyết
và thực nghiệm rằng các tổ hợp tuyến tính của các đặc trưng ngẫu nhiên hoạt động gần như tốt như
SVM và Adaboost tiêu chuẩn thời bấy giờ. Điện toán reservoir (Schrauwen et al., 2007), cũng được
biết đến như mạng trạng thái echo (Jaeger, 2003) hoặc máy trạng thái lỏng (Maass et al., 2002),
học một đọc tuyến tính từ một mạng nơ-ron tái phát được kết nối ngẫu nhiên; các mô hình như vậy
có thể học các hàm hữu ích của dữ liệu tuần tự. Để nghiên cứu về mặt lý thuyết SGD trên các mạng
quá tham số hóa, công trình gần đây sử dụng các mô hình hai lớp với lớp đầu tiên đủ rộng để nó
thay đổi ít trong quá trình huấn luyện (ví dụ, Du et al., 2019); trong giới hạn, lớp đầu tiên có thể
được coi như đóng băng tại khởi tạo ngẫu nhiên của nó (Jacot et al., 2018; Yehudai & Shamir, 2019).

Trong tất cả các trường hợp, những dòng công trình này nghiên cứu các mô hình bao gồm một lớp
tuyến tính có thể huấn luyện trên đầu các đặc trưng phi tuyến ngẫu nhiên. Ngược lại, các mô hình
của chúng tôi có các tham số afﬁne có thể huấn luyện xuyên suốt mạng sau mỗi đặc trưng ngẫu nhiên
trong mỗi lớp. Hơn nữa, do thực hành đặt BatchNorm trước hàm kích hoạt (He et al., 2016), các
tham số afﬁne của chúng tôi xuất hiện trước phi tuyến tính.

Đóng băng trọng số tại khởi tạo ngẫu nhiên. Các mạng nơ-ron được khởi tạo ngẫu nhiên (He et al.,
2015b; Glorot & Bengio, 2010), và hiệu suất với các trọng số này không tốt hơn cơ hội. Tuy nhiên,
vẫn có thể đạt độ chính xác cao trong khi giữ lại một số hoặc tất cả các trọng số này. Zhang et al.
(2019a) cho thấy rằng nhiều lớp riêng lẻ trong các CNN được huấn luyện có thể được đặt lại về khởi
tạo i.i.d. ngẫu nhiên của chúng với ít tác động đến độ chính xác. Zhou et al. (2019) và Ramanujan et al.
(2019) đạt độ chính xác cao trên CIFAR-10 và ImageNet chỉ bằng cách học trọng số cá nhân nào để loại bỏ.

3 PHƯƠNG PHÁP LUẬN
Kiến trúc ResNet. Chúng tôi huấn luyện các mạng tích chập với kết nối dư (ResNet) trên CIFAR-10
và ImageNet. Chúng tôi tập trung vào ResNet vì chúng làm cho có thể thêm đặc trưng bằng cách
tùy ý (a) tăng độ sâu mà không can thiệp vào tối ưu hóa và (b) tăng độ rộng mà không làm cho
số lượng tham số trở nên quá lớn đến mức huấn luyện không khả thi. Huấn luyện ResNet sâu thường
yêu cầu BatchNorm, vì vậy đây là một cài đặt tự nhiên cho các thí nghiệm của chúng tôi. Trong
Phụ lục C, chúng tôi chạy các thí nghiệm tương tự cho mạng VGG-style không dư cho CIFAR-10,
tìm thấy kết quả tương tự về mặt chất lượng.

Chúng tôi sử dụng các ResNet cho CIFAR-10 và ImageNet được thiết kế bởi He et al. (2015a).2
Chúng tôi tỷ lệ độ sâu theo He et al. (2015a) và tỷ lệ độ rộng bằng cách nhân tăng các kênh trên
mỗi lớp. Khi độ sâu tăng, các mạng duy trì cùng số lượng tham số shortcut và output, nhưng các
mạng sâu hơn có nhiều đặc trưng hơn và, do đó, nhiều tham số BatchNorm hơn. Khi độ rộng tăng,
số lượng tham số BatchNorm và output tăng tuyến tính, và số lượng tham số tích chập và shortcut
tăng bậc hai vì số lượng kênh đến và kênh đi đều tăng. Các kiến trúc mà chúng tôi sử dụng được
tóm tắt trong Bảng 1 (chi tiết đầy đủ trong Phụ lục B).

BatchNorm. Chúng tôi đặt BatchNorm trước kích hoạt, mà He et al. (2016) thấy dẫn đến hiệu suất
tốt hơn so với việc đặt nó sau kích hoạt. Chúng tôi khởi tạo β bằng 0 và lấy mẫu γ đồng đều giữa
0 và 1, mặc dù chúng tôi xem xét các khởi tạo khác trong Phụ lục E.

Các bản sao. Tất cả các thí nghiệm được hiển thị như trung bình trên năm (CIFAR-10) hoặc ba
(ImageNet) lần chạy với các khởi tạo khác nhau, thứ tự dữ liệu và tăng cường. Thanh lỗi cho một
độ lệch chuẩn từ trung bình có mặt trong tất cả các đồ thị; trong nhiều trường hợp, thanh lỗi quá
nhỏ để có thể nhìn thấy.

2 ResNet CIFAR-10 và ImageNet là các họ kiến trúc khác nhau với độ rộng và thiết kế khối khác nhau.

--- TRANG 2 ---
14 32 56 110 218 434 866
CIFAR-10 ResNet8990919293949596Độ chính xác Test (%)
14-1 14-2 14-4 14-8 14-16 14-32
CIFAR-10 WRN8990919293949596Độ chính xác Test (%)
18 34 50 101 200
ImageNet ResNet68707274767880Độ chính xác Test Top-1 (%)
Tất cả Tham số có thể Huấn luyện Tất cả Tham số có thể Huấn luyện (γ và β Bị vô hiệu hóa)

Hình 1: Độ chính xác khi huấn luyện ResNet sâu (trái) và rộng (giữa) cho CIFAR-10 và ResNet sâu
cho ImageNet (phải) như được mô tả trong Bảng 1 khi tất cả các tham số có thể huấn luyện (xanh dương)
và tất cả các tham số trừ γ và β có thể huấn luyện (tím). Huấn luyện với γ và β được kích hoạt
dẫn đến độ chính xác cao hơn 0.5% đến 2% (CIFAR-10) và 2% (ImageNet) so với việc vô hiệu hóa
γ và β.

và β được đề xuất như một kỹ thuật mới, nó có thể sẽ được áp dụng rộng rãi. Tuy nhiên, chúng
đủ nhỏ để khó tách biệt vai trò cụ thể mà γ và β đóng trong những cải thiện này.

Tổng quát hơn, thách thức trung tâm của việc nghiên cứu khoa học các tham số afﬁne theo đặc trưng
là phân biệt sự đóng góp của chúng khỏi các đặc trưng mà chúng biến đổi. Trong tất cả các bối cảnh
thực tế, các tham số afﬁne này được huấn luyện cùng với (như trong trường hợp của BatchNorm) hoặc
sau các đặc trưng (Mudrakarta et al., 2019; Dumoulin et al., 2017). Để nghiên cứu các tham số này
một cách tách biệt, thay vào đó chúng tôi huấn luyện chúng trên một mạng bao gồm hoàn toàn các
đặc trưng ngẫu nhiên. Cụ thể, chúng tôi đóng băng tất cả các trọng số tại khởi tạo và chỉ huấn luyện
các tham số γ và β trong BatchNorm.

Mặc dù các mạng vẫn giữ cùng số lượng đặc trưng, chỉ một phần nhỏ các tham số (nhiều nhất 0.6%)
có thể huấn luyện được. Thí nghiệm này buộc tất cả việc học diễn ra trong γ và β, làm cho có thể
đánh giá sức mạnh biểu hiện của một mạng mà bậc tự do duy nhất là tỷ lệ và dịch chuyển các đặc trưng
ngẫu nhiên. Chúng tôi nhấn mạnh rằng mục tiêu của chúng tôi mang tính khoa học: đánh giá hiệu suất
và các cơ chế mà các mạng sử dụng khả năng hạn chế này để biểu diễn các hàm có ý nghĩa; chúng tôi
không có ý định cũng không mong đợi thí nghiệm này đạt độ chính xác SOTA. Chúng tôi có các
phát hiện sau:

• Khi chỉ huấn luyện γ và β, các mạng đủ sâu (ví dụ, ResNet-866 và ResNet-200) đạt độ chính xác
cao một cách đáng ngạc nhiên (mặc dù không phải SOTA): 82% trên CIFAR-10 và 32% top-5 trên
ImageNet. Điều này chứng minh sức mạnh biểu hiện của các tham số BatchNorm afﬁne.

• Huấn luyện một số lượng tương đương các tham số được chọn ngẫu nhiên trên mỗi kênh hoạt động
tệ hơn nhiều (56% trên CIFAR-10 và 4% top-5 trên ImageNet). Điều này chứng minh rằng γ và β
có sức mạnh biểu hiện đặc biệt quan trọng như các hệ số và độ lệch theo đặc trưng.

• Khi chỉ huấn luyện BatchNorm, γ tự nhiên học cách vô hiệu hóa giữa một phần tư đến một nửa
tất cả các kênh bằng cách hội tụ về các giá trị gần bằng không. Điều này chứng minh rằng γ và β
đạt được độ chính xác này một phần bằng cách áp đặt độ thưa thớt theo đặc trưng.

• Khi huấn luyện tất cả các tham số, các mạng sâu hơn và rộng hơn có các giá trị γ nhỏ hơn nhưng
ít đặc trưng bị vô hiệu hóa hoàn toàn. Điều này gợi ý về vai trò mà γ có thể đóng trong việc kiểm
soát các kích hoạt trong các cài đặt mà việc vô hiệu hóa γ và β dẫn đến độ chính xác thấp hơn
(các phần bên phải của các đồ thị trong Hình 1).

Tóm lại, chúng tôi thấy rằng γ và β có sức mạnh biểu hiện đáng chú ý riêng và sức mạnh biểu hiện
này xuất phát từ vị trí đặc biệt của chúng như một hệ số và độ lệch theo đặc trưng. Ngoài việc cung cấp
hiểu biết về các tham số afﬁne biến đổi đặc trưng, quan sát này có ý nghĩa rộng hơn đối với hiểu biết
của chúng ta về các mạng nơ-ron bao gồm các đặc trưng ngẫu nhiên. Bằng cách đóng băng tất cả
các tham số khác tại khởi tạo, chúng tôi đang huấn luyện các mạng được xây dựng bằng cách học các
dịch chuyển và tái tỷ lệ của các đặc trưng ngẫu nhiên. Dưới ánh sáng này, kết quả của chúng tôi
chứng minh rằng các đặc trưng ngẫu nhiên có sẵn tại khởi tạo cung cấp nguyên liệu thô đủ để biểu diễn
các hàm độ chính xác cao cho phân loại hình ảnh. Mặc dù các công trình trước đây xem xét các mô hình
với các đặc trưng ngẫu nhiên và một lớp đầu ra tuyến tính có thể huấn luyện (ví dụ, Rahimi & Recht,
2009; Jaeger, 2003; Maass et al., 2002), chúng tôi tiết lộ sức mạnh biểu hiện của các mạng được
cấu hình sao cho các tham số afﬁne có thể huấn luyện xuất hiện sau mỗi đặc trưng ngẫu nhiên.

[Tiếp tục với nội dung còn lại...]
