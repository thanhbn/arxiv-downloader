# 2203.02549.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2203.02549.pdf
# File size: 536120 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Structured Pruning is All You Need
for Pruning CNNs at Initialization
Yaohui Cai Weizhe Hua Hongzheng Chen
G. Edward Suh Christopher De Sa Zhiru Zhang
Cornell University
{yc2632, wh399, hc676, gs272, cmd353, zhiruz}@cornell.edu
Abstract
Pruning-at-initialization (PAI) proposes to prune the individual weights of the CNN
before training, thus avoiding expensive Ô¨Åne-tuning or retraining of the pruned
model. While PAI shows promising results in reducing model size, the pruned
model still requires unstructured sparse matrix computation, making it difÔ¨Åcult to
achieve wall-clock speedups. In this work, we show theoretically and empirically
that the accuracy of CNN models pruned by PAI methods only depends on the
fraction of remaining parameters in each layer (i.e., layer-wise density ), regardless
of the granularity of pruning. We formulate the PAI problem as a convex optimiza-
tion of our newly proposed expectation-based proxy for model accuracy, which
leads to Ô¨Ånding the optimal layer-wise density of that speciÔ¨Åc model. Based on our
formulation, we further propose a structured and hardware-friendly PAI method,
named PreCrop, to prune or reconÔ¨Ågure CNNs in the channel dimension. Our
empirical results show that PreCrop achieves a higher accuracy than existing PAI
methods on several modern CNN architectures, including ResNet, MobileNetV2,
and EfÔ¨ÅcientNet for both CIFAR-10 and ImageNet. PreCrop achieves an accuracy
improvement of up to 2:7%over the state-of-the-art PAI algorithm when pruning
MobileNetV2 on ImageNet. PreCrop also improves the accuracy of EfÔ¨ÅcientNetB0
by0:3%on ImageNet with only 80% of the parameters and the same FLOPs.
1 Introduction
Convolutional neural networks (CNNs) have achieved state-of-the-art accuracy in a wide range of
machine learning (ML) applications. However, the massive computational and memory requirements
of CNNs remain a major barrier to more widespread deployment on resource-limited edge and mobile
devices. This challenge has motivated a large body of research on CNN compression in order to
simplify the original model without signiÔ¨Åcantly compromising accuracy.
Weight pruning [ 15,7,18,4,8] is one of the most extensively explored methods to reduce the
computational and memory demands of CNNs. Existing weight pruning approaches create a sparse
CNN model by iteratively removing ineffective weights or activations and training the resulting
sparse model. Moreover, training-based pruning methods introduce additional hyperparameters, such
as the learning rate for Ô¨Åne-tuning and the number of epochs before rewinding [ 21], leading to a
more complicated and less reproducible pruning process. Among the different pruning techniques,
training-based pruning usually enjoys the least accuracy degradation but at the cost of an expensive
pruning procedure.
To minimize the cost of pruning, a new line of research proposes pruning-at-initialization (PAI)
[16, 28, 25], which identiÔ¨Åes and prunes unimportant weights in a CNN right after initialization but
before training. As in training-based pruning methods, PAI evaluates the importance score of each
individual weight and retains only a subset of weights by maximizing the sum of the importance scores
of all remaining weights. The compressed model is then trained using the same hyperparameters (e.g.,
Preprint. Under review.arXiv:2203.02549v2  [cs.CV]  31 May 2022

--- PAGE 2 ---
weight decaying factor) for the same number of epochs as the baseline model. Thus, the pruning and
training of CNNs are cleanly decoupled, which greatly reduces the complexity of obtaining a pruned
CNN model. Currently, SynFlow [ 25] is considered the state-of-the-art PAI technique ‚Äì it further
eliminates the need for data in pruning as required in [ 16,28] and achieves higher accuracy with the
same number of parameters.
However, existing PAI methods mostly focus on Ô¨Åne-grained weight pruning, which removes indi-
vidual weights from the CNN model without preserving any structure. As a result, inference and
training of the pruned model require sparse matrix computation, which is challenging to accelerate
on commercially-available ML hardware such as GPUs and TPUs [ 14] that are optimized for dense
computation. According to a recent study [ 6], even with the NVIDIA cuSPARSE library, one can
only achieve a meaningful speedup for sparse matrix multiplications on GPUs when the sparsity is
over 98%. In practice, it is difÔ¨Åcult for modern CNNs to shrink by more than 50 without a drastic
degradation in accuracy [ 2]. Structural pruning patterns (e.g., pruning weights for the entire output
channel) are needed to avoid irregularly sparse storage and computation, thus providing practical
memory and computational savings. Moreover, recent studies [ 23,5] also observe that randomly
shufÔ¨Çing the binary weight mask of each layer or reinitializing all remaining weights does not affect
the accuracy of the model compressed using existing PAI methods. In this work, we Ô¨Årst review the
limitations of all previous PAI methods,
Based on the observations, we hypothesize that existing PAI methods are only effective in determining
the fraction of remaining weights in each layer, but fail to Ô¨Ånd a signiÔ¨Åcant subset of weights. We
propose to use the expectation of the sum of importance scores of all weights, rather than the sum, as a
proxy for the accuracy of the model, thus treating all weights in the same layer with equal importance.
With our new proxy for accuracy named SynExp (Synaptic Expectation), we can formulate PAI as a
convex optimization problem that directly solves the optimal fraction of remaining weights per layer
(i.e., layer density ) subject to certain model size and/or FLOPs constraints. We also prove a theorem
that SynExp does not change the same as long as the layer-wise density remains the same, regardless
of the granularity of pruning. The theorem opens an important opportunity that coarse-grained PAI
methods can achieve similar accuracy as their existing Ô¨Åne-grained counterparts such as SynFlow.
We demonstrate the efÔ¨Åcacy of the proposed proxy through extensive empirical experiments.
We further propose PreCrop , a structured PAI that prunes CNN models at the channel level. PreCrop
can effectively reduce the model size and computational cost without loss of accuracy compared
to Ô¨Åne-grained PAI methods, and more importantly, provide a wall-clock speedup on commodity
hardware. By allowing each layer to have more parameters than the baseline network, we are also able
toreconÔ¨Ågure the width dimension of the network with almost zero cost, which is termed PreConÔ¨Åg .
Our empirical results show that the model after PreConÔ¨Åg can achieve higher accuracy with fewer
parameters and FLOPs than the baseline for a variety of modern CNNs.
We summarize our contributions as follows:
‚Ä¢We propose to use the expectation of the sum of importance scores of all weights as a proxy
for accuracy and formulate PAI as a SynExp optimization problem constrained by the model
size and/or FLOPs. We also prove that the accuracy of the CNN model pruned by solving
the constrained optimization is independent of the pruning granularity.
‚Ä¢We introduce PreCrop to prune CNNs at the channel level based on the proposed SynExp
optimization. Our empirical study demonstrates that models pruned using PreCrop achieve
similar or better accuracy compared to the state-of-the-art unstructured PAI approaches
while preserving regularity. Compared to SynFlow, PreCrop achieves 2.7% and 0.9% higher
accuracy on MobileNetV2 and EfÔ¨ÅcientNet on ImageNet with fewer parameters and FLOPs.
‚Ä¢We show that PreConÔ¨Åg can be used to optimize the width of each layer in the network
with almost zero cost (e.g., the search can be done within one second on a CPU). Compared
to the original model, PreConÔ¨Åg achieves 0.3% accuracy improvement with 20% fewer
parameters and the same FLOPs for EfÔ¨ÅcientNet and MobileNetV2 on ImageNet.
2

--- PAGE 3 ---
2 Related Work
Model Compression in General can reduce the computational cost of large networks to ease their
deployment in resource-constrained devices. Besides pruning, quantization [ 3,31,13], NAS [ 32,24],
and distillation [12, 29] are also commonly used to improve the model efÔ¨Åciency.
Training-Based Pruning uses various heuristic criteria to prune unimportant weights. They typically
require an iterative training-prune-retrain process where the pruning stage is intertwined with the
training stage, which may increase the overall training cost by several folds. Because pruning aims to
reduce parameters, the FLOPs reduction is usually less signiÔ¨Åcant [17, 4].
Existing training-based pruning methods can be either unstructured [ 7,15] or structured [ 11,19],
depending on the granularity and regularity of the pruning scheme. Training-based unstructured
pruning usually provides a better accuracy-size trade-off while structured pruning can achieve a more
practical speedup and compression without special support of custom hardware.
(Unstructured) Pruning-at-Initialization (PAI) [16,28,25] provides a promising approach to
mitigating the high cost of training-based pruning.
They can identify and prune unimportant weights right after initialization and before the training
starts. Related to these efforts, authors of [ 5] and [ 23] independently Ô¨Ånd that for all existing PAI
methods, randomly shufÔ¨Çing the weight mask within a layer or reinitializing all the weights in the
network does not cause any accuracy degradation.
Neural Architecture Search (NAS) [32,27] automatically searches over a large set of candidate
models to achieve the optimal accuracy-computation trade-off. The search space of NAS usually
includes width, depth, resolution, and choice of building blocks. However, existing approaches can
only search in a small subset of the possible channel width conÔ¨Ågurations due to the cost. The cost
for NAS is also orders of magnitude higher than training a model. Some NAS algorithms [ 1,30] use
a cheap proxy instead of training the whole network, but an expansive reinforcement learning [ 32] or
evolutionary algorithm [20] is still used to predict a good network.
3 Pruning-at-initialization via SynExp Optimization
In this section, we Ô¨Årst discuss the preliminaries and limitations of existing PAI methods. We propose
a new proxy for the accuracy of the compressed model to overcome these limitations. With the
proposed proxy, we formulate the PAI problem into a convex optimization problem.
3.1 Preliminaries and Limitations of PAI
Preliminaries. PAI aims to prune neural networks after initialization but before training to avoid
the time-consuming training-pruning-retraining process. Prior to training, PAI uses the gradients
with respect to the weights to estimate the importance of individual weights, which requires forward
and backward propagations. Weights ( W) with smaller importance scores are pruned by setting
the corresponding entries in the binary weight mask ( M) to zero. Existing PAI methods, such
as SNIP [ 16], GraSP [ 28], and SynFlow [ 25] mainly explore different methods to estimate the
importance of individual weights. Single-shot PAI algorithms, such as SNIP and GraSP, prune the
model to the desired sparsity in a single pass. Alternatively, SynFlow, which represents the state-of-
the-art PAI algorithm, repeats the process of pruning a small fraction of weights and re-evaluating
the importance scores until the desired pruning rate is reached. Through the iterative process, the
importance of weights can be estimated more accurately.
SpeciÔ¨Åcally, the importance score used in SynFlow can be formulated as:
SSF(Wl
ij) ="
1TNY
k=l+1WkMk#
iWl
ijMl
ij"l 1Y
k=1WkMk1#
j; (1)
whereNis the number of layers, WlandMlare the weight and weight mask of the l-th layer,
SSF(Wl
ij)is the SynFlow score for a single weight Wl
ij,denotes the Hadamard product, jjis
element-wise absolute operation, and 1is an all-one vector. It is worth noting that none of the data or
labels is used to compute the importance score, thus making SynFlow a data-agnostic algorithm.
3

--- PAGE 4 ---
Limitations. As pointed out by [ 23,5], randomly shufÔ¨Çing the weight mask Wof each layer or
reinitializing all the weights Mdoes not affect the Ô¨Ånal accuracy of models compressed using existing
PAI methods. In addition, they show that given the same layer-wise density (i.e., the fraction of
remaining weights in each layer), the pruned models will have similar accuracy. The observations
suggest that even though the existing PAI algorithms try to identify less important weights, which
weights to prune is not important for accuracy.
All previous PAI methods use the sum of importance scores of the remaining weights as a proxy
for model accuracy, which is identical to the training-based pruning [ 7,15]. PAI obtains the binary
weight mask by maximizing the proxy as follows
maximizeNX
l=1SlMloverM subject toNX
l=1kMlk0Bparams; (2)
whereNis the number of layers in the network, Slis the score matrix in the l-th layer,Mlis the
binary weight mask in the l-th layer,kk0is the number of nonzero entries in a matrix, and Bparams
is a pre-deÔ¨Åned model size constraint.
Regardless of the speciÔ¨Åc PAI importance score chosen, a subset of weights is determined to be more
important than the other weights, which contradicts the observation that random shufÔ¨Çing does not
affect accuracy. Instead, we propose a new accuracy proxy for PAI to address the limitations.
3.2 SynExp Invariance Theorem
Inspired by the observations of previous PAI methods, we conjecture that a proxy for the accuracy of
the model pruned using a PAI method should satisfy the following two properties:
1. The pruning decision (i.e., weight mask M) can be made before the model is initialized.
2.Maximization of the proxy should result in optimal layer-wise density , not pruning decisions
for individual weights.
For random pruning before initialization, given a Ô¨Åxed density plfor each layer, the weight matrix
Wland the binary mask matrix Mlof that layer can be considered two random variables. The
binary weight mask is applied to the weight matrix element-wisely as WlMl, whererepresents
Hadamard product. The weight matrix of layer l(i.e.,Wl) containslparameters. Each individual
weight in layer lis sampled independently from a given distribution Dl. SupposeAl=fM;Mi2
f0;1g81il;P
iMi=pllgis the set of all possible binary matrices with the same shape
as the weight matrix Wlthat satisfy the layer-wise density ( pl) constraint. Then, the random weight
maskMlfor layerlis sampled uniformly from Al.
LetM=fMl;81lNgandW=fWl;81lNgbe the weights and masks of all N
layers in the network, respectively. The observations in Section 3.1 indicate that any instantiation of
the two random variables MandWresults in similar Ô¨Ånal accuracy of the pruned model. However,
the different instantiations do change the proxy value for the model accuracy in existing PAI methods.
For example, the SynFlow score in Equation 2 changes under different instances of MandW.
Therefore, we propose a new proxy that is invariant to the instantiations of MandWfor the model
accuracy in the context of PAI ‚Äî the expectation of the sum of the importance scores of all unpruned
(i.e., remaining) weights. The proposed proxy can be formulated as follows:
maximize E
M;W[S] =E
M;W"NX
l=1SlMl#
overplsubject toNX
l=1lplBparams;(3)
whereplrepresents layer-wise density of layer l,lis the number of parameters in layer l,EM;W[S]
stands for the expectation of the importance score Sover random weight W, and binary random
maskM. In this new formulation, the layer-wise density plis optimized to maximize the proposed
proxy for model accuracy.
In order to evaluate the expectation before weight initialization, we adopt the importance metric
proposed by SynFlow, i.e., replacing Sin Equation 3 with SSFin Equation 1. As a result, we
can compute the expectation analytically without forward or backward propagations. This new
expectation-based proxy is referred to as SynExp (Synaptic Expectation). We further prove SynExp
4

--- PAGE 5 ---
is invariant to the granularity of PAI in the SynExp Invariance Theorem. The detailed proof can be
found in Appendix A.
Theorem 1. Given a speciÔ¨Åc CNN architecture, the SynExp ( E[M;W ][SSF]) of any randomly com-
pressed model with the same layer-wise density plis a constant, independent of the pruning granular-
ity. The constant SynExp equals to:
E
M;W[SSF] =NCN+1NY
l=1(plClExD[jxj]); (4)
whereNis the number of layers in the network, ExD[jxj]is the expectation of magnitude of
distributionD,Clis the input channel size of layer land is also the output channel size of l 1, and
pl=1
lkMlk0is the layer-wise density.
In Equation 4, NandClare all hyperparameters of the CNN architecture and can be considered
constants. EjDljis also a constant under a particular distribution Dl. The layer-wise density pl
is the only variable that needs to be solved in the equation. Thus, SynExp satisÔ¨Åes both of the
aforementioned properties: 1) pruning is done prior to the weight initialization; 2) the layer-wise
density can be directly optimized. Furthermore, Theorem 1 also shows that the granularity of pruning
has no impact on the proposed SynExp metric. In other words, the CNN models compressed using
unstructured and structured pruning methods will have similar accuracy.
100 50 20 10 532 1
Params remained (%)5060708090Accuracy (%)SynFlow
Weight
Filter
Channel (PreCrop)
Figure 1: Comparison of the performance using
different pruning granularities on ResNet20 using
CIFAR-10.We empirically verify Theorem 1 by randomly
pruning each layer of a CNN with different prun-
ing granularity but the same layer-wise density
(pl). In this empirical study, we perform random
pruning with three different granularities (i.e.,
weight, Ô¨Ålter, and channel) to achieve the desired
layer-wise density obtained from solving Equa-
tion 3. For weight and Ô¨Ålter pruning, randomly
pruning each layer to match the layer-wise den-
sityploccasionally detaches some weights from
the network, especially when the density is low.
The detached weights do not contribute to the
prediction but are counted as remaining param-
eters. Thus, we remove the detached weights
for a fair comparison following [ 26]. For chan-
nel pruning, it is non-trivial to achieve the given
layer-wise density while satisfying the constraint that the output channel size of the previous layer
should be equal to the input channel size of the next layer. Therefore, we use PreCrop proposed in
Section 4.2. As shown in Figure 1, random pruning with different granularity can obtain similar
accuracy compared to SynFlow, as long as the layer-wise density remains the same. The empirical
results are consistent with Theorem 1 and also demonstrate the efÔ¨Åcacy of the proposed SynExp
metric. We include more empirical results for different CNN architectures and different importance
scores in Appendix C.
3.3 Optimizing SynExp
As discussed in Section 3.2, only the layer-wise density matters for our proposed SynExp approach.
Here, we show how to obtain the layer-wise density in Equation 3 that maximizes SynExp under
model size and/or FLOPs constraints.
3.3.1 Optimizing SynExp with Parameter Count Constraint
Given that the goal of PAI is to reduce the size of the model, we need to add an additional constraint
on the total number of parameters Bparams (i.e., parameter constraint), where Bparams is typically
greater than zero and less than the number of parameters in the original network. Since layer-wise
densityplis the only variable in Equation 3, we can simplify the equation by removing all other
5

--- PAGE 6 ---
0 5 10 15
LayersLayerwise Density
0 10 20 30
LayersSynFlow Ours(a) ResNet-20 (b) MobileNetV2
Figure 2: Comparison of the layer-wise densities obtained by SynExp optimization with parameter
count constraint and SynFlow. Higher transparency means that the problem is constrained by a
smaller parameter count.
constant terms, as follows:
maximizeNX
l=1logploverplsubject toNX
l=1lplBparams;
0<pl1;81lN ;(5)
wherelis the number of parameters in layer l.
Equation 5 is a convex optimization problem that can be solved analytically1. We compare the
layer-wise density derived from solving Equation 5 with the density obtained using SynFlow. As
shown in Figure 2, the layer-wise density obtained by both approaches are nearly identical, where our
new formulation gets rid of the iterative re-evaluation of SynFlow scores and the pruning process in
SynFlow. It is also worth noting that the proposed method Ô¨Ånds the optimal layer-wise density even
before the network is initialized.
3.3.2 Optimizing SynExp with Parameter Count and FLOPs Constraints
As discussed in Section 3.3.1, we can formulate PAI as a simple convex optimization problem with
a constraint on the model size. However, the number of parameters does not always reÔ¨Çect the
performance (e.g., throughput) of the CNN model. In many cases, CNN models are compute-bound
on many commodity hardwares [ 14,9]. Therefore, we propose to also introduce a FLOPs constraint
in our formulation.
The FLOPs saved in existing PAI algorithms speciÔ¨Åed in Equation 2 come from pruning the weights
in the CNN model. In other words, given a parameter constraint, the FLOP count of the pruned
model is also determined. It is not straightforward to introduce a FLOP constraint for each layer
in the model, because the correspondence between the number of parameters and FLOPs varies
across different layers. Therefore, none of the existing PAI methods can be directly used to bound
the FLOPs of CNN models. Since the weights in the same layer are associated with the same FLOP
count, we can directly incorporate the constraint on FLOPs BFLOPs (i.e., FLOPs constraint) into the
convex optimization problem as follows:
maximizeNX
l=1logploverplsubject toNX
l=1lplBparams;NX
l=1lplBFLOPs
0<pl1;81lN;(6)
wherelin the number of FLOPs in the lthlayer.
Since the additional FLOPs constraint is linear, the optimization problem in Equation 6 remains
convex and has an analytical solution1. By solving SynExp optimization with a Ô¨Åxed Bparams but
differentBFLOPs , we can obtain the layer-wise density for various models that have the same number
of parameters but different FLOPs. Then, we perform random weight pruning on the CNN model to
1We include analytical solutions for Equation 5 and Equation 6 in Appendix B for completeness.
6

--- PAGE 7 ---
ùê∂!ùëù!ùê∂!*ùëù!ùê∂!"#PadzerosDropchannels+ùê∂!ùëô!"CONVlayerFigure 4: Illustration of PreCrop for layers with residual connections ‚Äî ClandCl+1represent the
number of input channels of layer landl+ 1, respectively. plrepresents the density of layer l.
achieve the desired layer-wise density. We compare the proposed SynExp optimization (denoted as
Ours) with other popular PAI methods. As depicted in Figure 3, given a Ô¨Åxed model size ( 1:5104
in the Ô¨Ågure), our method can be used to generate a Pareto Frontier that spans the spectrum of FLOPs,
while other methods can only have a Ô¨Åxed FLOPs. Our method dominates all other methods in terms
of both accuracy and FLOPs reduction.
4 Structured Pruning-at-Initialization
2 3 4 5 6
FLOPs 1e68182838485T op-1 Accuracy (%)
Ours
SynFlow
SNIP
GraSP
Figure 3: Comparison of our method with
other PAI methods ‚Äî we repeat the experiment
using ResNet-20 on CIFAR-10 Ô¨Åve times and re-
port the mean and variance (error bar) of the accu-
racy. All the models in the Ô¨Ågures have 1:5104
parameters.The SynExp Invariance Theorem shows that pruning
granularity of PAI methods should not affect the ac-
curacy of the pruned model. Channel pruning, which
prunes the weights of the CNN at the output chan-
nel granularity, is considered the most coarse-grained
and hardware-friendly pruning technique, Therefore,
applying the proposed PAI method for channel prun-
ing can avoid both complicated retraining/re-tuning
procedures and irregular computations. In this sec-
tion, we propose a structured PAI method for channel
pruning, named PreCrop, to prune CNNs in the chan-
nel dimension. In addition, we propose a variety of
PreCrop with relaxed density constraints to reconÔ¨Åg-
ure the width of each layer in the CNN model, which
is called PreConÔ¨Åg.
4.1 PreCrop
Applying the proposed PAI method to channel pruning requires a two-step procedure. First, the
layer-wise density plis obtained by solving the optimization problem shown in Equation 5 or 6.
Second, we need to decide how many output channels of each layer should be pruned to satisfy
the layer-wise density. However, it is not straightforward to compress each layer to match a given
layer-wise density due to the additional constraint that the number of output channels of the current
layer must match the number of input channels of the next layer.
We introduce PreCrop, which compresses each layer to meet the desired layer-wise density. Let
ClandCl+1be the number of input channels of layer landl+ 1, respectively. Cl+1also means
the number of output channels of layer l. For layers with no residual connections, the number of
output channels of layer lis reduced topplCl+1
. The number of input channels of layer l+ 1
needs to match the number of output channels of layer l, which is also reduced topplCl+1
.
Therefore, the actual density of layer lafter PreCrop isppl 1plinstead ofpl. We empirically Ô¨Ånd
thatppl 1plis close enough to plbecause the neighboring layers have similar layer-wise densities.
Alternatively, it is possible to obtain the exact layer-wise density pby only reducing the number of
input or output channels of a layer. However, this approach leads to a signiÔ¨Åcant drop in accuracy,
because the number of the input and output channels can change dramatically (e.g., plClCl+1
orClplCl+1). This causes the shape of the feature map changes dramatically in adjacent layers,
resulting in information loss.
For layers with residual connections, Figure 4 depicts an approach to circumvent the constraint on
the number of channels of adjacent layers. We can reduce the number of input and output channels of
layerlfromClandCl+1topplClandpplCl+1, respectively. In this way, the density of each layer
can match the given layer-wise density obtained from the proposed PAI method. Since the output of
layerlneeds to be added element-wisely with the original input to layer l, the output of layer lis
7

--- PAGE 8 ---
padded with zero-valued channels to match the shape of the original input. In our implementation,
we simply add the output of layer lto the Ô¨ÅrstpplCl+1channels of the original input to layer l, thus
requiring no extra memory or computation for zero padding. PreCrop eliminates the requirement
for sparse computation in existing PAI methods and thus can be used to accelerate both training and
inference of the pruned models.
4.2 PreConÔ¨Åg: PreCrop with Relaxed Density Constraint
PreCrop uses the layer-wise density obtained from solving the convex optimization problem, which is
always less than 1following the common setting for pruning (i.e., pl1). However, this constraint
on layer-wise density is not necessary for our method since we can increase the number of channels
(i.e., expand the width of the layer) before initialization. By solving the problem in Equation 6
without the constraint pl1, we can expand the layers with a density greater than 1 ( pl>1) and
prune the layers with a density less than 1 ( pl<1). We call this variant of PreCrop as PreConÔ¨Åg
(PreCrop-ReconÔ¨Ågure). If we set Bparams andBFLOPs to be the same as the original network, we
can essentially reconÔ¨Ågure the width of each layer of a given network architecture under certain
constraints on model size and FLOPs.
The width of each layer in a CNN is usually designed manually, which often relies on extensive
experience and intuition. Using PreConÔ¨Åg, we can determine the width of each layer in the network
to achieve a better cost-accuracy tradeoff. PreConÔ¨Åg can also be used as an or a part of an ultra-fast
NAS. Compared to NAS, which typically searches on the width, depth, resolution, and choice of
building blocks, PreConÔ¨Åg only changes the width. Nonetheless, PreConÔ¨Åg only requires a minimum
amount of time and computation compared to NAS methods; it only needs to solve a relatively small
convex optimization problem, which can be solved in a second on a CPU.
5 Evaluation
In this section, we empirically evaluate PreCrop and PreConÔ¨Åg. We Ô¨Årst demonstrate the effectiveness
of PreCrop by comparing it with SynFlow. We then use PreConÔ¨Åg to tune the width of each layer and
compare the accuracy of the model after PreConÔ¨Åg with the original model. We perform experiments
using various modern CNN models, including ResNet [ 10], MobileNetV2 [ 22], and EfÔ¨ÅcientNet [ 24],
on both CIFAR-10 and ImageNet. We set all hyperparameters used to train the models pruned by
different PAI algorithms to be the same. See Appendix E for detailed hyperparameter settings.
5.1 Evaluation of PreCrop
For CIFAR-10, we compare the accuracy of SynFlow (red line) and two variants of PreCrop: PreCrop-
Params (blue line) and PreCrop-FLOPs (green line). PreCrop-Params adds the parameter count
constraint whereas PreCrop-FLOPs imposes the FLOPs constraint into the convex optimization
problem. As shown in Figure 5a, PreCrop-Params achieves similar or even better accuracy as
SynFlow under a wide range of different model size constraints, thus validating that PreCrop-Params
can be as effective as the Ô¨Åne-grained PAI method. Considering the beneÔ¨Åts of structured pruning,
PreCrop-Params should be favored over existing PAI methods. Figure 5b further shows that PreCrop-
FLOPs consistently outperforms SynFlow by a large margin, especially when the reduction in FLOPs
is large. The experimental results show that PreCrop-FLOPs should be adopted when the performance
of the model is limited by the computational cost.
Table 1 summarizes the comparison between PreCrop and SynFlow on ImageNet. For ResNet-34,
PreCrop achieves 0.6% lower accuracy compared to SynFlow with a similar model size and FLOPs.
For both MobileNetV2 and EfÔ¨ÅcientNetB0, PreCrop achieves 1.2% and 0.9% accuracy improvements
compared to SynFlow with strictly fewer FLOPs and parameters, respectively. The experimental
results on ImageNet further support SynExp Invariance Theorem that a coarse-grained structured
pruning (e.g., PreCrop) can perform as well as unstructured pruning. In conclusion, PreCrop achieves
a favorable accuracy and model size/FLOPs tradeoff compared to the state-of-the-art PAI algorithm.
8

--- PAGE 9 ---
100 50 20 10 532 190
70
50
30ResNet20
100 50 20 10 532 190
70
50
30
100 50 20 10 532 190
70WideResNet20100 50 20 10 532 190
70
100 50 20 10 5 3
Params remained (%)90
80MobileNetV2
100 50 20 10 5 3
FLOPs remained (%)90
80Accuracy (%)SynFlow PreCrop-FLOPs PreCrop-Params(a) PreCrop-Params vs. SynFlow. (b) PreCrop-FLOPs vs. SynFlow.
Figure 5: Comparison of PreCrop-Params and PreCrop-FLOPs with SynFlow ‚Äî we repeat the
experiment using ResNet20 (top), WideResNet20 (middle), and MobileNetV2 (bottom) on CIFAR-10 three
times and report the mean and variance (error bar) of the accuracy.
Table 1: Comparison of PreCrop with SynFlow on ImageNet ‚Äî The dagger(y) implies that the
numbers are only theoretical without considering the overhead of storing sparse matrices.
NETWORK METHODS FLOP S(G) P ARAMS (M) A CCURACY (%)
RESNET34BASELINE 3.64 21.80 73.5
SYNFLOW 2.78y(76.4%) 10.91y(50.0%) 72.1 (-1.4)
PRECROP 2.73 (75.0%) 11.09 (50.8%) 71.5 (-2.0)
MOBILE NETV2BASELINE 0.33 3.51 69.6
SYNFLOW 0.26y(78.8%) 2.44y(68.6%) 67.6 (-2.0)
PRECROP 0.26 (78.8%) 2.33 (66.4%) 68.8 (-0.8)
SYNFLOW 0.21y(63.6%) 1.91y(54.4%) 64.5 (-5.1)
PRECROP 0.21 (63.6%) 1.85 (52.7%) 67.2 (-2.4)
EFFICIENT NETB0BASELINE 0.40 5.29 73.0
SYNFLOW 0.30y(75.0%) 3.72y(70.3%) 71.8 (-1.2)
PRECROP 0.30 (75.0%) 3.67 (69.4%) 72.7 (-0.3)
5.2 Evaluation of PreConÔ¨Åg
As discussed in Section 4.2, PreConÔ¨Åg can be viewed as an ultra-fast NAS technique, which adjusts
the width of each layer in the model even before the weights are initialized.
Table 2 compares the accuracy of the reconÔ¨Ågured model with the original model under similar model
size and FLOPs constraints. For ResNet34, with similar accuracy, we reduce the parameter count
by 25%. For MobileNetV2, we achieve 0:3%higher accuracy than the baseline with 20% fewer
parameters and 3%fewer FLOPs. For the EfÔ¨ÅcientNet, we can also achieve 0:3%higher accuracy
than the baseline with only 80% of the parameters and the same FLOPs. Note that EfÔ¨ÅcientNet is
identiÔ¨Åed by a NAS method. As PreConÔ¨Åg only changes the number of channels of the model before
initialization, we believe it also applies to other compression techniques.
9

--- PAGE 10 ---
Table 2: PreConÔ¨Åg on ImageNet.
NETWORK METHODS FLOP S(G) P ARAMS (M) A CCURACY (%)
RESNETBASELINE 3.64 21.80 73.5
PRECONFIG 3.64 16.52(75.8%) 73.3(-0.2)
MOBILE NETV2BASELINE 0.33 3.51 69.6
PRECONFIG 0.32 (97.0%) 2.83(80.6%) 69.9(+0.3)
EFFICIENT NETB0BASELINE 0.40 5.29 73.0
PRECONFIG 0.40 4.29(81.1%) 73.3(+0.3)
6 Conclusion
In this work, we show theoretically and empirically that only the layer-wise density matters for the
accuracy of the CNN models pruned using PAI methods. We formulate PAI as a simple convex
SynExp optimization. Based on SynExp optimization, we further propose PreCrop and PreConÔ¨Åg to
prune and reconÔ¨Ågure CNNs in the channel dimension. Our experimental results demonstrate that
PreCrop can outperform existing Ô¨Åne-grained PAI methods on various networks and datasets.
References
[1]Mohamed S Abdelfattah, Abhinav Mehrotra, ≈Åukasz Dudziak, and Nicholas D Lane. Zero-cost
proxies for lightweight nas. arXiv preprint arXiv:2101.08134 , 2021.
[2]Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the
state of neural network pruning? arXiv preprint arXiv:2003.03033 , 2020.
[3]Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,
and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks.
arXiv preprint arXiv:1911.03852 , 2019.
[4]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. arXiv preprint arXiv:1803.03635 , 2018.
[5]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neu-
ral networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576 ,
2020.
[6]Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning.
InProceedings of the International Conference for High Performance Computing, Networking,
Storage and Analysis , SC ‚Äô20. IEEE Press, 2020.
[7]Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 ,
2015.
[8]Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efÔ¨Åcient neural network. Advances in neural information processing systems , 28, 2015.
[9]Pawan Harish and Petter J Narayanan. Accelerating large graph algorithms on the gpu using
cuda. In International conference on high-performance computing , pages 197‚Äì208. Springer,
2007.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770‚Äì778, 2016.
[11] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural
networks. In Proceedings of the IEEE international conference on computer vision , pages
1389‚Äì1397, 2017.
[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 , 2015.
[13] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efÔ¨Åcient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2704‚Äì2713, 2018.
[14] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder
Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin,
Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb,
Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard
Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,
10

--- PAGE 11 ---
Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy,
James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin,
Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi
Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda,
Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory
Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory
Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang,
Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing
unit. SIGARCH Comput. Archit. News , 45(2):1‚Äì12, jun 2017.
[15] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems , pages 598‚Äì605, 1990.
[16] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network
pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
[17] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.
Learning efÔ¨Åcient convolutional networks through network slimming. In Proceedings of the
IEEE international conference on computer vision , pages 2736‚Äì2744, 2017.
[18] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value
of network pruning. arXiv preprint arXiv:1810.05270 , 2018.
[19] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A Ô¨Ålter level pruning method for deep
neural network compression. In Proceedings of the IEEE international conference on computer
vision , pages 5058‚Äì5066, 2017.
[20] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classiÔ¨Åer architecture search. In Proceedings of the aaai conference on artiÔ¨Åcial intelligence ,
pages 4780‚Äì4789, 2019.
[21] Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and Ô¨Åne-tuning in
neural network pruning. arXiv preprint arXiv:2003.02389 , 2020.
[22] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 4510‚Äì4520, 2018.
[23] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D
Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. arXiv preprint
arXiv:2009.11094 , 2020.
[24] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model scaling for convolutional neural
networks. In International Conference on Machine Learning , pages 6105‚Äì6114. PMLR, 2019.
[25] Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic Ô¨Çow. arXiv preprint arXiv:2006.05467 ,
2020.
[26] Artem Vysogorets and Julia Kempe. Connectivity matters: Neural network pruning through the
lens of effective sparsity. arXiv preprint arXiv:2107.02306 , 2021.
[27] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu,
Matthew Yu, Tao Xu, Kan Chen, et al. Fbnetv2: Differentiable neural architecture search for
spatial and channel dimensions. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12965‚Äì12974, 2020.
[28] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient Ô¨Çow. arXiv preprint arXiv:2002.07376 , 2020.
[29] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem,
Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 8715‚Äì8724, 2020.
[30] Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang,
and Wanli Ouyang. Econas: Finding proxies for economical neural architecture search. In
Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition , pages
11396‚Äì11404, 2020.
[31] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net:
Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160 , 2016.
[32] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578 , 2016.
11

--- PAGE 12 ---
A Proof of SynExp Invariance Theorem
Theorem 1. Given a speciÔ¨Åc CNN architecture, the SynExp ( E[M;W ][SSF]) of any randomly com-
pressed model with the same layer-wise density plis a constant, independent of the pruning granular-
ity. The constant SynExp equals to:
E
M;W[SSF] =NCN+1NY
l=1(plClExD[jxj]); (7)
whereNis the number of layers in the network, ExD[jxj]is the expectation of magnitude of
distributionD,Clis the input channel size of layer land is also the output channel size of l 1, and
pl=1
lkMlk0is the layer-wise density.
Proof. Assuming the network has Nlayers, weight matrix Wl2Rmlnl, mask matrix Ml2
f0;1gClCl+1.ClandCl+1are the input and output channel size of layer l. As the output channel size
of any layer lequals to the input channel size of the next layer l+ 1, we haveCl+1=Cl+1;8l<N .
We Ô¨Årst prove the Theorem 1 on fully-connected network, and we can extend it to CNNs easily. From
Equation 1, in a fully-connected network, the Synaptic Flow score for any parameter Wl
ijwith mask
Ml
ijin layerlequals to:
SSF(Wl
(i;j)) ="
1TNY
k=l+1WkMk#
iWl
(i;j)Ml
(i;j)"l 1Y
k=1WkMk1#
j(8)
We compute the SynExp of the layer l(E[M;W ](SSF)[l]), then the SynExp of the network is simply
the sum of SynExp of all layers:
E[M;W ](SSF) =NX
l=1E[M;W ](SSF)[l](9)
We deÔ¨Åne the expectation value for input channel i, output channel j, and the whole layer in layer l
asEl
(i;),El
(;j), andEl
(;):
El
(i;)=1
Cl+1X
xjWl
(i;x)Ml
(i;x)j (10)
El
(;j)=1
ClX
xjWl
(x;j)Ml
(x;j)j (11)
El
(i;j)=El
(;)=1
ClCl+1X
i;jjWl
(i;j)Ml
(i;j)j=1
lX
i;jjWl
(i;j)Ml
(i;j)j=plEjDlj (12)
Here we use EjDljto denote ExD[jxj].
As the weight in layer lis sampled from distribution D, and the mask matrices are also randomly
sampled, we have
E[k]
(;)=Ek
(i;)=Ek
(;j)=plEjDlj (13)
WithEk
(i;),Ek
(;j), andEl
(;), we can rewrite Equation 8 to:
E[SSF(Wl
(i;j))] = NY
k=l+2Ck+1Ek
(;)!
Cl+2El+1
(i;)El
(i;j)Cl 1El 1
(;j) l 2Y
k=1CkEk
(;)!
(14)
12

--- PAGE 13 ---
Combining Equation 3 and 14, because the instantiation of the weight matrices and mask matrices
for each layer are independent:
E[M;W ](SSF)[l]=E2
4ClX
i=1Cl+1X
j=1SSF(Wl
(i;j))3
5=ClX
i=1Cl+1X
j=1Eh
SSF(Wl
(i;j))i
= NY
k=l+2pkCk+1EjDkj!ClX
i=1Cl+1X
j=1
pl+1Cl+2El+1
(i;)plEjDljpl 1Cl 1El 1
(;j) l 2Y
k=1CkEjDkj!
= NY
k=l+2pkCk+1EjDkj!
ClCl+1 
pl+1Cl+2EjDl+1jplEjDljpl 1Cl 1EjDl 1j l 2Y
k=1pkCkEjDkj!
=CN+1NY
l=1(plClEjDlj)
(15)
According to Equation 9,
E[M;W ](SSF) =NX
l=1CN+1NY
l=1(ClEjDlj)
=NCN+1NY
l=1(plClExD[jxj]);(16)
SynExp Invariance Theorem can also be extended to CNNs, as it is obvious that SynExp of CNNs is
proportional to that of fully connected networks. Thus the difference of SynExp between CNNs and
fully connected networks for each layer is only a factor equal toK2
Cl+1, whereKis the kernel size of
the convolutional layer.
B Solution of the Optimization problem
For the convex optimization problem in Equation 5, Equation 6, or PreConÔ¨Åg, we can simply use
Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions to analytically solve it. We include the solutions as follows
for completeness.
B.1 Optimization with Parameter Count Constraint
pl= min(
;1)
wheresatisÔ¨Åes:NX
l=1min(l;) =Bparams(17)
B.2 Optimization with Parameter Count and FLOPs Constraints
pl= min(1
1l+2l;1)
where1;2satisfy:NX
l=1lmin(1
1l+2l;1) =Bparams
NX
l=1lmin(1
1l+2l;1) =BÔ¨Çops(18)
13

--- PAGE 14 ---
100 50 20 10 532 1
Params remained (%)8688909294Accuracy (%)SynFlow
Weight
Filter
Channel (PreCrop)Figure 6: Comparison of the performance using different pruning granularities on VGG16 using
CIFAR-10.
100 50 20 10 532
Params remained (%)708090Accuracy (%) Weight (SNIP)
Channel (PreCrop)
100 50 20 10 532
Params remained (%)60708090Accuracy (%) Weight (GraSP)
Channel (PreCrop)
Figure 7: Comparison of the performance using different pruning granularities on ResNet20 using
CIFAR-10. SNIP (left) and GraSP (right) importance scores are used.
B.3 Formulation of PreConÔ¨Åg
pl=1
1l+2l
where1;2satisfy:NX
l=1l1
1l+2l=Bparams
NX
l=1l1
1l+2l=BÔ¨Çops(19)
In practice, to avoid solving the , we use a convex optimization solver, which can obtain the solution
with a CPU within a second for such a small scale convex optimization.
C More Empirical Results on SynExp Invariance Theorem
We show more empirical results that validates SynExp Invariance Theorem. We Ô¨Årst show the
comparison of the performance using different pruning granulariteis on VGG16 using CIFAR-10. All
the settings in this experiment is the same as in Figure 1, except this experiment is done on VGG16.
Then we also verify that SynExp Invariance Theorem not only holds for SynFlow, but also holds for
other PAI algorithms. In this experiment, we Ô¨Årst use other PAI (i.e., SNIP and GraSP) to obtain the
layerwise density pl. Then we use random pruning to match plin the channel level. The results are
shown in Figure 7.
As shown in all the above experiments, as long as the layerwise density is the same, the pruning
granularties do not affect the model accuracy.
14

--- PAGE 15 ---
0 5 10 15
Layers05001000Channel WidthBaseline
PreConfigFigure 8: Comparison of the channel width of EfÔ¨ÅcientNetB0 before and after PreConÔ¨Åg.
D Channel Width Comparison
We also include a comparison of the channel width between the baseline EfÔ¨ÅcientNetB0 and PreConÔ¨Åg
EfÔ¨ÅcientNetB0 in Figure 8.
E Experiment Details
E.1 Implementation
We adapt model implementations of ResNet, ShufÔ¨ÇeNet, and MobileNetv2 from imgclsmob2. The
implementations of SynFlow, SNIP, and GraSP are based on the codebase of SynFlow3.
E.2 Hyperparameters
Here we provide the hyperparameters used in training all models in Table 3. No AutoAugment, Label
Smoothing, or stochastic depth is used during training. All the CIFAR-10 models are trained with
same hyperparameter setting.
Table 3: Hyperparameters used in training.
CIFAR-10ImageNet
MobileNet ResNet EfÔ¨ÅcientNet
Optimizer momentum momentum momentum momentum
Training Epochs 160 180 90 150
Batch Size 128 256 512 256
Initial Learning Rate 0.1 0.025 0.2 0.035
Learning Rate Schedule linear drop at each epoch drop at 30, 60 epoch drop at each epoch
Drop Rate N.A. 0.98 0.1 0.99
Weight Decay 10 4410 510 4410 5
2https://github.com/osmr/imgclsmob
3https://github.com/ganguli-lab/Synaptic-Flow
15
