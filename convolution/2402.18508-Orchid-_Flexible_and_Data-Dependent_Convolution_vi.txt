# Orchid: Tích chập Linh hoạt và Phụ thuộc Dữ liệu
# cho Mô hình hóa Chuỗi

Mahdi Karami, Ali Ghodsi
Đại học Waterloo, ON, Canada
karami1@ualberta.ca

Tóm tắt

Trong lĩnh vực học sâu đang phát triển nhanh chóng, nhu cầu về các mô hình vừa biểu cảm vừa hiệu quả tính toán chưa bao giờ quan trọng đến vậy. Bài báo này giới thiệu Orchid, một kiến trúc mới được thiết kế để giải quyết độ phức tạp bậc hai của các cơ chế attention truyền thống mà không làm giảm khả năng nắm bắt các phụ thuộc tầm xa và học trong ngữ cảnh. Trung tâm của kiến trúc này là một lớp tích chập toàn cục phụ thuộc dữ liệu mới, điều kiện hóa kernel của nó dựa trên chuỗi đầu vào bằng cách sử dụng một mạng neural điều kiện chuyên dụng. Chúng tôi thiết kế hai mạng điều kiện đơn giản duy trì tính bất biến dịch chuyển trong phép tích chập phụ thuộc dữ liệu của chúng tôi. Tính chất động của kernel tích chập đề xuất mang lại cho Orchid khả năng biểu cảm cao trong khi duy trì khả năng mở rộng tựa tuyến tính cho các chuỗi dài. Chúng tôi đánh giá mô hình đề xuất trên nhiều lĩnh vực, bao gồm mô hình hóa ngôn ngữ và phân loại hình ảnh, để làm nổi bật hiệu suất và tính tổng quát của nó. Các thí nghiệm của chúng tôi cho thấy kiến trúc này không chỉ vượt trội hơn các kiến trúc dựa trên attention truyền thống như BERT và Vision Transformers với kích thước mô hình nhỏ hơn, mà còn mở rộng độ dài chuỗi khả thi vượt ra ngoài giới hạn của các lớp attention dày đặc. Thành tựu này đại diện cho một bước tiến đáng kể hướng tới các mô hình học sâu hiệu quả và có khả năng mở rộng hơn cho mô hình hóa chuỗi.

1 Giới thiệu

Trong các mạng neural sâu hiện đại, các cơ chế attention đã nổi lên như một tiêu chuẩn vàng, then chốt trong các lĩnh vực như xử lý ngôn ngữ tự nhiên, xử lý hình ảnh và âm thanh, và thậm chí các lĩnh vực phức tạp như sinh học [Vaswani et al., 2017, Dosovitskiy et al., 2020, Dwivedi and Bresson, 2020]. Tuy nhiên, bất chấp khả năng phân tích chuỗi mạnh mẽ, các cơ chế mô hình hóa chuỗi này gặp phải độ phức tạp tính toán cao, tỷ lệ thuận với bình phương độ dài chuỗi, cản trở ứng dụng của chúng vào các tác vụ ngữ cảnh dài. Độ phức tạp này đã thúc đẩy sự chuyển hướng sang các giải pháp sáng tạo để vượt qua rào cản tính toán này, cho phép phân tích các chuỗi dài trong các lĩnh vực như genomics, giải trình tự DNA, và tạo ra các tác phẩm âm nhạc dài.

Trong những năm qua, các nhà nghiên cứu đã khám phá nhiều chiến lược khác nhau để giải quyết nút cổ chai tính toán của các lớp attention dày đặc truyền thống [Tay et al., 2022]. Một chiến lược quan trọng bao gồm làm thưa thớt ma trận attention dày đặc. Thay vì tính toán toàn bộ ma trận, Qiu et al. [2019], Parmar et al. [2018] tập trung vào các khối cục bộ cụ thể của các trường tiếp nhận của chuỗi bằng cách chia chúng thành các khối có kích thước cố định. Hơn nữa, Sparse Transformer [Child et al., 2019], Longformer [Beltagy et al., 2020] và BigBird [Zaheer et al., 2020] sử dụng các mẫu attention có bước nhảy kết hợp với các cửa sổ trượt cục bộ để giảm tính toán. Trái ngược với việc sử dụng các mẫu được xác định trước, các kỹ thuật khác bao gồm học cách phân cụm/sắp xếp tokens dựa trên một hàm tương tự, từ đó nâng cao góc nhìn toàn cục của chuỗi, như được thấy trong Reformer [Kitaev et al., 2020], Routing Transformer [Roy et al., 2020] Sparse Sinkhorn attention [Tay et al., 2020]. Một cách tiếp cận khác bao gồm xấp xỉ rank thấp của ma trận self-attention, tận dụng hiểu biết rằng những ma trận này thường thể hiện các thuộc tính rank thấp, như được chứng minh bởi Linformer [Wang et al., 2020] chiếu các ma trận keys và values xuống các ma trận biểu diễn chiều thấp hơn. Một mô hình khác để giảm chi phí tính toán bậc hai là thay thế sự tương tự dot-product giữa các ma trận keys và query của cơ chế attention bằng một hàm kernel và tránh tính toán rõ ràng ma trận attention [Katharopoulos et al., 2020]. Các ví dụ nổi bật trong họ này bao gồm Performers [Choromanski et al., 2020], Random Feature Attention [Peng et al., 2021] dựa trên xấp xỉ đặc trưng ngẫu nhiên của hàm kernel. Ngoài ra, một số mô hình tận dụng sự kết hợp của các kỹ thuật như vậy để thiết kế một transformer hiệu quả [Zhu et al., 2021, Zhang et al., 2021]. Tuy nhiên, trong khi các phương pháp này giảm đáng kể chi phí tính toán, chúng có thể hy sinh tính biểu cảm và hiệu suất, thường yêu cầu các cách tiếp cận lai kết hợp chúng với các lớp attention dày đặc [Mehta et al., 2022, Fu et al., 2023]. Mặt khác, các nghiên cứu gần đây đã nhắm tới làm thưa thớt các lớp tuyến tính dày đặc, được sử dụng để trộn đặc trưng trong các khối Transformer, để giải quyết một nguồn chính khác của nhu cầu tính toán và bộ nhớ cao trong các mô hình lớn [Dao et al., 2022, Chen et al., 2021,].

Tìm kiếm các toán tử trộn dưới bậc hai và hiệu quả phần cứng mà cũng có tính biểu cảm vẫn là một thách thức đáng kể. Các nghiên cứu gần đây đã khám phá các giải pháp không cần attention, đặc biệt sử dụng các mô hình không gian trạng thái (SSMs) [Gu et al., 2021, Mehta et al., 2022, Wang et al., 2022, Fu et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, De et al., 2024], và các tích chập dài [Romero et al., 2021, Li et al., 2022, Poli et al., 2023]. Một mô hình không gian trạng thái đặc trưng cho hành vi của một hệ thống động theo trạng thái nội tại của nó sử dụng một phương trình trạng thái, mô tả động lực học của hệ thống sử dụng các phương trình vi phân bậc nhất qua các trạng thái, và một phương trình quan sát, liên hệ các biến trạng thái với các đầu ra quan sát được. Một hiểu biết quan trọng là, hầu hết các mô hình SSM này có thể được công thức hóa như một mô hình tích chập dài giữa các chuỗi đầu vào và đầu ra [Gu et al., 2021], cho phép huấn luyện song song và hiệu quả. Tuy nhiên, nghiên cứu gần đây của Poli et al. [2023] đã chứng minh rằng tham số hóa trực tiếp phản hồi xung của bộ lọc tích chập dài dẫn đến một lớp trộn chuỗi có tính biểu cảm hơn nữa.

Bài báo này đề xuất một cơ chế tích chập phụ thuộc dữ liệu mới để giải quyết độ phức tạp bậc hai vốn có của các cơ chế attention truyền thống, trong khi duy trì khả năng của mô hình để nắm bắt các phụ thuộc tầm xa và học trong ngữ cảnh. Lớp tích chập phụ thuộc dữ liệu điều kiện hóa kernel của nó dựa trên dữ liệu đầu vào bằng cách sử dụng một mạng neural điều kiện chuyên dụng. Chúng tôi thiết kế hai mạng điều kiện đơn giản nhưng hiệu quả duy trì tính bất biến dịch chuyển trong phép tích chập thích ứng. Bằng cách kết hợp các cơ chế thích ứng này với các phép cổng, mô hình đề xuất của chúng tôi—được đặt tên là Orchid—đạt được khả năng biểu cảm cao trong khi cung cấp khả năng mở rộng tựa tuyến tính (với độ phức tạp O(LlogL)) cho các chuỗi dài. Đánh giá trên các lĩnh vực khác nhau, bao gồm mô hình hóa ngôn ngữ và phân loại hình ảnh, được trình bày trong phần 4 và Phụ lục, chứng minh hiệu suất và tính tổng quát của kiến trúc Orchid, vượt trội hơn các kiến trúc dựa trên attention, như BERT và Vision Transformers, với kích thước mô hình nhỏ hơn. Hơn nữa, nó cho phép xử lý độ dài chuỗi rất lớn vượt ra ngoài giới hạn của các lớp attention dày đặc. Thành tựu này đặt nền tảng cho các tiến bộ hơn nữa trong các kiến trúc mô hình hóa chuỗi hiệu quả và có khả năng mở rộng hơn.

2 Kiến thức nền

Cơ chế Self-Attention: Cho một chuỗi có độ dài L của các embeddings (tokens) x = (x1, x2, ..., xL), lớp self-attention tạo ra một chuỗi mới bằng cách tính tổng có trọng số của các embeddings này. Để đạt được điều này, nó chiếu tuyến tính x thành ba thành phần: queries (Q), keys (K), và values (V), như: Q = xWQ, K = xWK, V = xWV. Mỗi cơ chế attention riêng lẻ trong một lớp multi-head self-attention hoạt động như một phép biến đổi tuyến tính dày đặc, được biểu diễn là:

y = SA(Q,K,V) = SoftMax(QKT/√dk)V = A(x)V,

trong đó ma trận A(x) chứa các điểm attention đã được chuẩn hóa giữa mỗi cặp tokens. Mô tả này của lớp attention làm nổi bật các lợi ích đáng chú ý của nó, bao gồm khả năng nắm bắt các phụ thuộc tầm xa bằng cách sử dụng số lượng tham số dưới tuyến tính. Cơ chế attention cho phép tính toán trực tiếp các tương tác giữa bất kỳ hai vị trí nào trong chuỗi đầu vào, bất kể khoảng cách của chúng, mà không có sự gia tăng tương ứng trong số lượng tham số. Ngoài ra, lớp attention triển khai một bộ lọc tuyến tính dày đặc phụ thuộc dữ liệu, lọc đầu vào một cách hiệu quả dựa trên trọng số được điều kiện hóa bởi một ánh xạ của dữ liệu. Thuộc tính này làm cho nó có tính biểu cảm và linh hoạt đủ để mã hóa một họ lớn các hàm tuyến tính. Tuy nhiên, những ưu điểm này đi kèm với chi phí độ phức tạp tính toán và bộ nhớ bậc hai.

Điều này thúc đẩy chúng tôi phát triển một cơ chế tích chập phụ thuộc dữ liệu hiệu quả và có khả năng mở rộng, có đặc điểm là một kernel thích ứng điều chỉnh dựa trên dữ liệu đầu vào. Kích thước kernel của lớp tích chập này bằng với độ dài chuỗi đầu vào, cho phép nắm bắt các phụ thuộc tầm xa qua chuỗi đầu vào trong khi duy trì khả năng mở rộng cao.

Tích chập Tuyến tính: Tích chập tuyến tính thời gian rời rạc là một phép tính cơ bản trong xử lý tín hiệu số tính toán đầu ra như tổng có trọng số của đầu vào có độ dài hữu hạn x với các phiên bản đã dịch chuyển của kernel tích chập, h, còn được gọi là phản hồi xung của một hệ thống tuyến tính bất biến thời gian (LTI). Chính thức, nó có thể được viết là

y[t] = (h∗x)[t] ≜ ∑(L-1,ℓ=0) h[t-ℓ]x[ℓ].

Trong định nghĩa này, đầu ra là một bộ lọc tuyến tính của đầu vào và kernel tích chập được đệm zero. Tuy nhiên, các sơ đồ đệm khác dẫn đến các dạng tích chập khác nhau. Một dạng nổi tiếng là tích chập tròn, được định nghĩa là

y[t mod L] = (h⊛x)[t] ≜ ∑(L-1,ℓ=0) h[t-ℓ mod L]x[ℓ],

tương đương với tích chập tuyến tính của hai chuỗi nếu một chuỗi được đệm chu kỳ ở các cạnh của nó.

Tích chập Toàn cục và Thuật toán Tích chập Nhanh: Các lớp tích chập tiêu chuẩn, được tham số hóa rõ ràng với một kernel ngắn, gặp khó khăn trong việc nắm bắt các phụ thuộc tầm xa trong dữ liệu tuần tự. Mở rộng kernel để khớp với độ dài đầu vào cho phép mô hình hóa các phụ thuộc như vậy nhưng dẫn đến tăng trưởng tuyến tính trong số lượng tham số và độ phức tạp tính toán bậc hai. Để giảm thiểu thách thức tăng trưởng tham số, kernel của tích chập toàn cục (còn gọi là dài) có thể được tham số hóa ngầm bằng cách sử dụng một perceptron nhiều lớp (MLP), một kỹ thuật đã được chứng minh duy trì tỷ lệ tham số dưới tuyến tính [Karami et al., 2019, Romero et al., 2021, Li et al., 2022, Poli et al., 2023]. Hơn nữa, một ưu điểm chính của các toán tử tích chập là, tận dụng định lý tích chập, các toán tử tích chập có thể được tính toán hiệu quả trong miền tần số bằng cách sử dụng thuật toán Fast Fourier Transform (FFT), từ đó giảm độ phức tạp tính toán xuống O(LlogL) [Cooley and Tukey, 1965]. Chính thức, tích chập tuyến tính có thể được biểu diễn trong miền tần số như ŷ = F^(-1)(F(ĥ) ⊙ F(x̂)) = T^(-1)(h^F ⊙ Tx̂), trong đó T là ma trận DFT, F biểu thị phép biến đổi Fourier rời rạc, và x̂ biểu thị tín hiệu được đệm zero, được định nghĩa là x̂ ≜ pad2L(x) = [0L; x]. Ngoài ra, tích chập tròn có thể được tính toán đơn giản như: y = h⊛x = F^(-1)(F(h) ⊙ F(x)).

3 Toán tử Orchid

Phần này giới thiệu Bộ lọc Tích chập Phụ thuộc Dữ liệu, một toán tử mới nhằm tăng tính biểu cảm của các phép tích chập dài. Toán tử này phục vụ như khối xây dựng cơ bản cho lớp Orchid, mà chúng tôi sẽ khám phá sau trong phần này.

3.1 Bộ lọc Tích chập Phụ thuộc Dữ liệu

Chúng tôi đặt giả thuyết rằng làm cho kernel tích chập phụ thuộc dữ liệu cho phép bộ lọc thích ứng với các đặc tính cụ thể của đầu vào của nó, có khả năng nắm bắt các mẫu phức tạp hơn trong chuỗi. Chính thức, bộ lọc phụ thuộc đầu vào này được định nghĩa là:

y = hθ(x) ∗ x = NNθ(x) ∗ x (1)

Sự đổi mới chính là thay thế kernel tích chập tĩnh bằng một kernel được tạo ra có điều kiện được điều khiển bởi dữ liệu đầu vào. Điều này được thực hiện thông qua một mạng điều kiện, được ký hiệu là hθ(x) = NNθ(x), một mạng neural được tham số hóa bởi θ. Mạng điều kiện đầu ra một vector khớp với chuỗi đầu vào về độ dài. Điều này cho phép mỗi token đầu vào 'chú ý' đến toàn bộ chuỗi với trọng số cá nhân hóa, thích ứng được suy ra từ biểu diễn cụ thể của nó. Tích chập ngữ cảnh xung quanh bằng cách sử dụng sơ đồ trọng số phụ thuộc dữ liệu này có thể tiềm năng cung cấp trộn chuỗi hiệu quả hơn so với các tích chập tĩnh thông thường.

3.2 Bảo toàn Tính Bất biến Dịch chuyển trong Tích chập Phụ thuộc Dữ liệu

Một thuộc tính cơ bản của tích chập rời rạc là tính bất biến dịch chuyển, có nghĩa là dịch chuyển đầu vào một lượng nhất định dẫn đến dịch chuyển tương ứng trong đầu ra (bỏ qua hiệu ứng biên). Điều này được biểu diễn chính thức cho tích chập tròn như: shift_m(y) = h⊛shift_m(x) [Bronstein et al., 2021], trong đó thuộc tính này đúng hoàn toàn bất kể điều kiện biên. Phép dịch chuyển được định nghĩa là shift_m(x)[t] ≜ x[t+m].

Thuộc tính này đặc biệt quan trọng vì nó đảm bảo phản hồi của toán tử mạnh mẽ với dịch chuyển đặc trưng trong đầu vào, từ đó nâng cao khả năng tổng quát hóa của mô hình. Thiên kiến quy nạp này là cốt lõi của thành công rộng rãi của các phép tích chập [Thomas et al.]. Do đó, việc thiết kế mạng điều kiện trong tích chập phụ thuộc dữ liệu (1) để bảo toàn thuộc tính tính bất biến dịch chuyển là mong muốn. Để duy trì thuộc tính này cho các phép tích chập phụ thuộc dữ liệu, đủ để thiết kế kernel bộ lọc để có tính bất biến dịch chuyển, tức là h(shift_m(x)) = h(x) (tham khảo Phụ lục B để có bằng chứng). Trong phần tiếp theo, chúng tôi trình bày hai thiết kế mạng điều kiện thỏa mãn tính bất biến dịch chuyển.

I) Loại bỏ Pha cho Tính Bất biến Dịch chuyển: Một dịch chuyển tròn của chuỗi u dẫn đến dịch chuyển pha tuyến tính của các thành phần tần số của nó: F[shift_m(u)][ω] = u^F[ω] · e^(-i2π/L ωm) [Oppenheim, 1999]. Cho một hàm bất biến dịch chuyển g(x) (như một Conv1d() theo chiều sâu) (thỏa mãn: g(shift_m(x)) = shift_m(g(x))). Các thành phần tần số của nó sau một dịch chuyển không gian của đầu vào duy trì dịch chuyển pha này:

F[g(shift_m(x))][ω] = F(g(x))[ω] · e^(-i2π/L ωm).

Bằng cách lấy độ lớn (giá trị tuyệt đối hoặc bình phương) của những thành phần tần số có giá trị phức này, chúng ta loại bỏ hiệu quả dịch chuyển pha, do đó, định nghĩa h^F(x) = |F[g(x)]| thỏa mãn thuộc tính bất biến dịch chuyển: h^F(shift_m(x)) = h^F(x).

Trong thiết kế của chúng tôi, chúng tôi triển khai một mạng điều kiện miền không gian-tần số lai. Mạng này bao gồm một tích chập tuyến tính 1D theo chiều sâu (Conv1d()) với độ dài kernel ngắn (thường là 3-5) hoạt động ở miền không gian, tiếp theo là một tích chập ngắn trong miền tần số. Bằng cách hoạt động trong cả miền không gian và tần số, mạng điều kiện trộn hiệu quả thông tin từ các tokens lân cận và các thành phần phổ. Mạng neural điều kiện kết quả được công thức hóa là:

h^F_θ(x) = Conv1d_F(Conv1d(x)) (2)

Lựa chọn kiến trúc này nhằm giảm thiểu số lượng tham số và chi phí tính toán được giới thiệu bởi mạng điều kiện trong mô hình tổng thể.

II) Tận dụng Tương quan Chéo cho Tính Bất biến Dịch chuyển: Một cách tiếp cận khác để đạt được tính bất biến dịch chuyển bao gồm tính toán tương quan chéo giữa hai ánh xạ của chuỗi đầu vào. Cho k(x) và q(x) là hai hàm bất biến dịch chuyển, thỏa mãn: k(shift_m(x)) = shift_m(k(x)) và q(shift_m(x)) = shift_m(q(x)). Chúng tôi định nghĩa h(x) là tương quan chéo của k(x) và q(x), được cho bởi:

h(x)[t] = (k(x) ⋆ q(x))[t] ≜ ∑(L-1,ℓ=0) k(x)[ℓ] · q(x)[t+ℓ mod L].

Phép tính này về cơ bản trượt q(x) qua k(x) và đo sự tương tự của chúng ở các độ lệch khác nhau. Đáng chú ý, hàm tương quan chéo kết quả, h(x), cũng bất biến dịch chuyển:

h(shift_m(x)) = k(shift_m(x)) ⋆ q(shift_m(x))
                = shift_m(k(x)) ⋆ shift_m(q(x))
                = k(x) ⋆ q(x) = h(x)

Hơn nữa, định lý tích chập cho phép tính toán hiệu quả tương quan chéo trong miền tần số: h^F(x) = F(k(x) ⋆ q(x)) = k^F*(x) ⊙ q^F(x) trong đó k^F* biểu thị liên hợp phức của k^F và ⊙ biểu thị phép nhân theo từng phần tử.

Chú ý 3.1. Bằng cách đặt k(x) = q(x) = g(x), chúng ta có h^F(x) = |g^F(x)|^2, Điều này chỉ ra rằng cách tiếp cận tương quan chéo tổng quát hóa cách tiếp cận dựa trên độ lớn, chứng minh tính linh hoạt của nó.

Tương tự như cách tiếp cận trước, chúng tôi sử dụng các tích chập ngắn theo chiều sâu 1D riêng biệt cho cả k(x) và q(x), tiếp theo là một tích chập khác sau tương quan chéo trong miền tần số. Kết quả là, mạng neural điều kiện được định nghĩa là

h^F_θ(x) = Conv1d_F(Conv1d(x)* ⊙ σ_F(Conv1d(x))) . (3)

Lấy cảm hứng từ cơ chế cổng, chúng tôi đã áp dụng Sigmoid cho một trong cặp này trong miền tần số. Cả hai hàm điều kiện, như được định nghĩa trong (2) và (3), được minh họa sơ đồ trong Hình 2.1.

Chú ý 3.2. Đối với các phép tích chập, chúng tôi tăng cường mạng điều kiện phụ thuộc dữ liệu bằng cách kết hợp một số hạng cố định (tĩnh). Số hạng này thêm mã hóa vị trí vào kernel tích chập bằng cách tham số hóa ngầm nó bằng cách sử dụng một embedding vị trí, PosEmb(), của bước thời gian (chỉ số token trong chuỗi) và các mạng feed forward như h_0 = FFN(PosEmb(t)) [Romero et al., 2021, Li et al., 2022, Poli et al., 2023]. Kernel tích chập cuối cùng được thu được bằng cách cộng thiên kiến vị trí này với đầu ra của mạng điều kiện, h = h_θ(x) + h_0.

Chú ý 3.3 (Tích chập Phụ thuộc Dữ liệu như một Thay thế Cross-attention). Kernel của tích chập h_θ(x), được định nghĩa trong các phương trình (2) hoặc (3), được điều kiện hóa trên đầu vào của lớp tích chập, làm cho nó phụ thuộc đầu vào. Tuy nhiên, chúng ta có thể tổng quát hóa khái niệm này hơn nữa. Kernel có thể là một hàm của bất kỳ chuỗi tùy ý u nào, dẫn đến định nghĩa rộng hơn của tích chập phụ thuộc dữ liệu:

y(x,u) = h_θ(u) ∗ x = NN_θ(u) ∗ x

Định nghĩa này ghép chuỗi đầu vào x với chuỗi khác u, tạo ra một thay thế tiềm năng cho các lớp cross-attention trong các tác vụ xử lý chuỗi. Do đó chúng tôi gọi lớp đề xuất là "phụ thuộc dữ liệu" theo nghĩa tổng quát hơn. Khi xử lý các chuỗi có độ dài khác nhau, chuỗi ngắn hơn có thể được đệm zero để khớp với độ dài của chuỗi dài hơn. Cụ thể, giả sử x ∈ R^L dài hơn u ∈ R^N (L > N), chúng tôi sử dụng chuỗi được đệm zero û = pad_L(u) ∈ R^L như đầu vào cho mạng điều kiện NN_θ(û). Vì tích chập dài được triển khai trong miền tần số, việc đệm zero này trong miền thời gian chuyển thành nội suy trong miền tần số [Smith, 2008] đảm bảo rằng cả hai chuỗi có các thành phần tần số cùng độ dài.

3.3 Khối Orchid

Không như các lớp attention, các bộ lọc tích chập tận dụng việc chia sẻ tham số. Điều này có nghĩa là chúng trượt cùng một trọng số kernel và áp dụng chúng cho các vị trí khác nhau trong chuỗi đầu vào. Về mặt toán học, phép tính này tương đương với việc nhân một vector đầu vào với một ma trận có cấu trúc, như ma trận Toeplitz cho tích chập tuyến tính hoặc ma trận tuần hoàn cho tích chập tròn, dẫn đến hiệu quả tính toán [Gray et al., 2006, Karami et al., 2019]. Để đạt được một sơ đồ lọc phụ thuộc vị trí, chúng tôi bổ sung tích chập phụ thuộc dữ liệu với các phép nhân theo từng phần tử, cho phép mô hình nhấn mạnh các tokens cụ thể bằng cách gán trọng số cao hơn trước khi áp dụng tích chập bất biến vị trí. Đáng chú ý, nghiên cứu trước đây đã chứng minh rằng một chuỗi các ma trận tuần hoàn và đường chéo có thể xấp xỉ hiệu quả các lớp tuyến tính dày đặc [Moczulski et al., 2015, Cheng et al., 2015]. Dựa trên những hiểu biết này, kiến trúc tổng thể của khối Orchid, được cấu thành từ một chuỗi M tích chập phụ thuộc dữ liệu và các phép nhân theo từng phần tử (các kết nối có cổng). Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng một chuỗi đơn giản có thứ tự 1.5, bao gồm tích chập phụ thuộc dữ liệu được kẹp giữa hai phép nhân theo từng phần tử: y = (f^2_⊙ ∘ f_* ∘ f^1_⊙)(x) trong đó ∘ biểu thị phép hợp, f_*(x) ≜ (h_θ(x) + h_0) ∗ x, và f^i_⊙(x) ≜ Conv1d(x) ⊙ x. Khối Orchid được minh họa trong Hình 2.1 và việc triển khai cơ bản của nó được trình bày trong phụ lục D.

Độ phức tạp tính toán tổng thể. Tất cả các tích chập toàn cục trong khối Orchid được tính toán trong miền tần số bằng thuật toán FFT, kế thừa hiệu quả tính toán của nó với độ phức tạp O(LlogL). Hơn nữa, các phép nhân theo từng phần tử đóng góp thêm độ phức tạp O(L). Do đó, độ phức tạp tổng thể của khối Orchid tỷ lệ tựa tuyến tính với độ dài chuỗi, dẫn đến tổng độ phức tạp O(MLlogL), trong đó M là số lượng lớp trong khối. Một triển khai tối ưu hóa phần cứng và I/O của FFT gần đây, được giới thiệu trong [Fu et al., 2023], có thể tăng tốc tính toán tổng thể của Orchid trên các máy tăng tốc hiện đại. Các so sánh thời gian chạy thực nghiệm với các cơ chế attention tiêu chuẩn, được chi tiết trong Phụ lục C.5, làm nổi bật khả năng mở rộng dự kiến của nó, đặc biệt cho các chuỗi dài hơn.

4 Thí nghiệm

Đánh giá của chúng tôi về Orchid tập trung vào ba mô hình dựa trên Transformer khác nhau để đánh giá khả năng biểu cảm và tổng quát hóa của nó như một thay thế cho các lớp attention. Đầu tiên, chúng tôi tiến hành một tập hợp thí nghiệm trên một tác vụ tổng hợp để đánh giá khả năng học trong ngữ cảnh và khả năng mở rộng của mô hình đề xuất. Tiếp theo, chúng tôi đánh giá hiệu suất của kiến trúc đề xuất trên các tác vụ mô hình hóa ngôn ngữ. Hơn nữa, chúng tôi mở rộng các thí nghiệm của mình sang các tác vụ phân loại hình ảnh, nhằm đánh giá khả năng tổng quát hóa của mô hình trên các lĩnh vực đa dạng. Các nghiên cứu khử bỏ bổ sung về kiến trúc mô hình và cũng như một thí nghiệm về phân loại giọng nói thô với các chuỗi dài được trình bày trong Phụ lục C.5 và C.2. Để có cái nhìn tổng quan về các chi tiết thí nghiệm, vui lòng tham khảo Phụ lục C.

4.1 Học trong Ngữ cảnh Tổng hợp

Mục tiêu của thí nghiệm đầu tiên là đánh giá mô hình của chúng tôi hoạt động như thế nào trên một tác vụ lý luận tổng hợp. Tác vụ này, lấy cảm hứng từ công trình trước đây về đánh giá mô hình ngôn ngữ [Liang et al., 2022] và học trong ngữ cảnh (ICL) [Garg et al., 2022], được gọi là Nhớ lại Liên kết. Nó bao gồm việc tạo ra một giá trị từ một khóa cho một chuỗi các bộ key-value từ một từ điển ngẫu nhiên. Ví dụ, với đầu vào ([a, 1, b, e, f, 3], b), mô hình dự kiến trả về e, giá trị liên kết với khóa b. Tác vụ này đánh giá liệu một mô hình có thể lấy hiệu quả giá trị đúng từ một khóa trong một prompt hay không, về cơ bản là áp dụng một dịch chuyển được điều khiển bởi dữ liệu. Các cơ chế attention cung cấp khả năng này bằng cách tính toán điểm attention thông qua so sánh token và sau đó trọng số toàn bộ chuỗi tương ứng [Olsson et al., 2022]. Nhớ lại liên kết đã có vai trò quan trọng trong việc hướng dẫn thiết kế các mô hình tích chập dài, như đã được chứng minh trong [Fu et al., 2023], và một biến thể phức tạp hơn của tác vụ này đã được sử dụng trong [Poli et al., 2023].

Đối với những thí nghiệm này, chúng tôi đánh giá Orchid so với một số mô hình tích chập dài hàng đầu, bao gồm: I) H3, sử dụng các mô hình không gian trạng thái (SSMs) để tham số hóa ngầm tích chập dài, như được đề xuất trong [Fu et al., 2023]. II) CKConv, sử dụng các mạng feed forward (FFNs) và embeddings vị trí để tham số hóa ngầm các phép tích chập, được chi tiết trong [Romero et al., 2021]. III) Hyena, được xây dựng dựa trên khung CKConv bằng cách kết hợp một điều chế suy giảm mũ bổ sung vào quy trình tích chập ngầm, như được chi tiết trong [Poli et al., 2023]. Nó được tăng cường thêm bằng một phép nhân trong một chuỗi có thứ tự 2.

Các kết quả, được minh họa trong Hình 4.1 và Bảng 4.1, chứng minh rằng mô hình Orchid cung cấp tính biểu cảm vượt trội và vượt trội hơn các mô hình tích chập dài hiện có trong các tác vụ nhớ lại liên kết. Đáng chú ý, trong các tình huống thách thức với độ dài chuỗi ngắn 128 và kích thước từ vựng lớn, Orchid cải thiện đáng kể độ chính xác của mô hình và thu hẹp khoảng cách giữa các mô hình Transformer và tích chập ngầm. Hơn nữa, khi mở rộng độ dài chuỗi đến các chuỗi rất dài lên tới 131K tokens, Orchid thành công học tác vụ trong khi mô hình Transformer gặp khó khăn tính toán.

Những hiểu biết từ thí nghiệm này hướng dẫn chúng tôi trong việc tích hợp mô hình đề xuất vào các mô hình dựa trên Transformer cho mô hình hóa ngôn ngữ mở rộng, gợi ý tiềm năng của nó để nâng cao hiệu suất trong các tác vụ xử lý ngôn ngữ tự nhiên.

4.2 Mô hình hóa Ngôn ngữ

Chúng tôi đánh giá lớp Orchid trong các mô hình ngôn ngữ. Orchid được thiết kế để tích hợp liền mạch với các mô hình ngôn ngữ kiểu BERT hiện có, như BERT [Devlin et al., 2018], RoBERTa [Liu et al., 2019], SpanBERT [Joshi et al., 2020], và các mô hình khác [Jin et al., 2020]. Trong các thí nghiệm của chúng tôi, chúng tôi thay thế các lớp attention trong khung BERT tiêu chuẩn trong [Devlin et al., 2018] bằng các lớp Orchid. Đối với mỗi khối Transformer trong mô hình kiểu BERT, chúng tôi thay thế các lớp attention bằng các lớp Orchid cho trộn chuỗi. Chúng tôi cũng thay thế hai ma trận dày đặc trong các lớp MLP, được sử dụng để trộn chiều trong Transformers, bằng các ma trận đường chéo khối [Dao et al., 2022]. Theo [Fu et al., 2023], chúng tôi thêm một tích chập dài dư vào mỗi lớp Orchid.

Mô hình kiểu BERT của chúng tôi, được gọi là Orchid-BERT-base, có 12 lớp với kích thước ẩn 768, cùng chiều và độ sâu như BERT-base. Orchid-BERT-base kết quả có 77M tham số, so với 110M tham số của BERT-base. Chúng tôi cũng tiền huấn luyện Orchid-BERT-large với 254M tham số với chiều ẩn 1536 và 12 lớp. Các mô hình Orchid được tiền huấn luyện bằng mô hình hóa ngôn ngữ có mặt nạ trên tập dữ liệu C4 [Raffel et al., 2019] với tokenizer bert-base-uncased.

Hiệu suất Tinh chỉnh trên Benchmark GLUE. Chúng tôi tiến hành đánh giá các mô hình Orchid-BERT trên các tác vụ tinh chỉnh GLUE, so sánh chúng với các mô hình cơ sở: BERT-base và BERT-large, và các mô hình dựa trên tích chập dài gần đây: M2-BERT-base và M2-BERT-large [Fu et al., 2023]. Quy trình tinh chỉnh được thực hiện theo phương pháp được mô tả bởi Izsak et al. [2021]. Như các kết quả được nêu trong Bảng 4.2 cho thấy, Orchid-BERT-base có thể đạt được cải thiện 1.0 điểm trong hiệu suất điểm GLUE trung bình so với BERT-base trên benchmark GLUE với việc sử dụng ít hơn 30% tham số. Tương tự, Orchid-BERT-large vượt trội hơn hiệu suất của BERT-large 0.6 điểm với giảm 25% số lượng tham số.

4.3 Phân loại Hình ảnh

Chúng tôi mở rộng ứng dụng của Orchid sang kiến trúc Vision Transformer (ViT), được giới thiệu bởi Dosovitskiy et al. [2020], bằng cách thay thế cơ chế attention của nó bằng Orchid tương tự như tác vụ mô hình hóa ngôn ngữ. Tương tự như tác vụ mô hình hóa ngôn ngữ, chúng tôi thay thế các ma trận dày đặc trong các lớp MLP, thực hiện trộn chiều, bằng các ma trận đường chéo khối và kết hợp một tích chập dài dư trong mỗi khối Orchid. Chúng tôi đánh giá mô hình của chúng tôi so với các mô hình dựa trên tích chập dài gần đây, cụ thể là Hyena-ViT-b [Poli et al., 2023] và M2-ViT-b [Fu et al., 2023].

Các mô hình được đánh giá cho phân loại hình ảnh trên hai tập dữ liệu hình ảnh được sử dụng rộng rãi: CIFAR-10 và ImageNet-1K. Đối với CIFAR-10, các hình ảnh được chuyển đổi thành các chuỗi của các patch pixel 4×4 và được xử lý bằng kiến trúc ViT gồm 6 lớp Transformer với kích thước ẩn 128. Trong trường hợp ImageNet-1K, chúng tôi phân đoạn hình ảnh thành các patch 16×16 pixel, và chúng tôi huấn luyện một kiến trúc ViT-base có 12 lớp Transformer và kích thước ẩn 768.

Hiệu suất của các mô hình trên cả tập dữ liệu CIFAR-10 và ImageNet-1K, được nêu trong Bảng 4.3, chứng minh rằng Orchid vượt trội đáng kể hơn cả baseline Vision Transformer và các mô hình dựa trên tích chập dài trên cả hai tập dữ liệu. Những kết quả này khẳng định tính tổng quát hóa và hiệu quả của kiến trúc Orchid vượt ra ngoài lĩnh vực mô hình hóa ngôn ngữ, làm nổi bật lợi thế tiềm năng của nó trong phạm vi ứng dụng rộng hơn như các tác vụ xử lý hình ảnh.

5 Công trình Liên quan

Các công trình trước đây đã khám phá nhiều dạng kiến trúc tích chập động khác nhau [Wu et al., 2019, Karami et al., 2019, Chen et al., 2020, Jiang et al., 2020]. Tích chập động trong [Wu et al., 2019] sử dụng một kernel tích chập ngắn chỉ phụ thuộc vào bước thời gian hiện tại, trong khi [Jiang et al., 2020] mở rộng phạm vi kernel để phụ thuộc vào một cửa sổ cục bộ. Trong khi đó, Chen et al. [2020] mô hình hóa kernel tích chập như một hỗn hợp của các kernel tích chập ngắn với trọng số hỗn hợp được điều khiển bởi average pooling của embedding đầu vào. Tuy nhiên, sự phụ thuộc vào các tích chập ngắn và các cách tiếp cận mô hình hóa kernel cụ thể của chúng hạn chế khả năng nắm bắt các phụ thuộc tầm xa và việc mở rộng kernel của chúng để khớp với độ dài chuỗi là không thực tế về mặt tính toán. Trong một hướng nghiên cứu khác, Fourier Neural Operator (FNO) [Li et al., 2020] và adaptive FNO [Guibas et al., 2021] hoạt động trong miền phổ, áp dụng một lớp tuyến tính dày đặc hoặc một MLP với một lớp tuyến tính đường chéo khối. Tuy nhiên, những mô hình này không mô hình hóa rõ ràng kernel tích chập và không thực thi tính bất biến dịch chuyển.

Những tiến bộ gần đây trong các mô hình không gian trạng thái phụ thuộc đầu vào (SSMs) đã cho thấy triển vọng trong mô hình hóa chuỗi hiệu quả bằng cách cho phép các tham số mô hình thích ứng động dựa trên đầu vào [Gu and Dao, 2023]. Tuy nhiên, các cơ chế phụ thuộc đầu vào trong những mô hình này thường dựa vào token hiện tại hoặc các token lân cận cục bộ của nó, ngăn cản chúng tận dụng lợi ích của tích chập toàn cục để huấn luyện song song hóa hiệu quả. Do đó, chúng phụ thuộc nhiều vào các triển khai nhận biết phần cứng được tối ưu hóa cho GPU hiện đại. Mặc dù hiệu quả trong một số tác vụ nhất định, những mô hình này đã được chứng minh gặp khó khăn với các tình huống đòi hỏi nhớ lại cao [Arora et al., 2024]. Ngược lại, tích chập toàn cục phụ thuộc dữ liệu đề xuất của chúng tôi cho phép mạng điều kiện được ảnh hưởng bởi toàn bộ ngữ cảnh đầu vào, cho phép trộn chuỗi hiệu quả. Hơn nữa, công thức hiện tại của SSMs phụ thuộc đầu vào không dễ dàng thích ứng cho cross-attention hiệu quả giữa các cặp chuỗi. Điều này làm nổi bật một hướng nghiên cứu tương lai đầy hứa hẹn: khám phá cách các thế mạnh bổ sung của tích chập toàn cục phụ thuộc dữ liệu và SSMs phụ thuộc đầu vào có thể được kết hợp để phát triển các mô hình nền tảng xuất sắc trên phạm vi rộng hơn các tác vụ.

Các nghiên cứu gần đây đã khám phá các phương pháp trộn chuỗi dưới bậc hai bằng cách sử dụng các tích chập dài hoặc mô hình không gian trạng thái, tận dụng thuật toán tích chập nhanh để có hiệu quả tính toán. Trong khi việc sử dụng Fast Fourier transform dẫn đến độ phức tạp tính toán O(LlogL), các thuật toán FFT thể hiện việc sử dụng phần cứng không tối ưu và gặp phải I/O chậm giữa các lớp của hệ thống phân cấp bộ nhớ trên GPU hiện đại do tính chất tuần tự của chúng. Để giải quyết nút cổ chai này, FlashFFT-Conv [Fu et al., 2023] sử dụng phân tích ma trận để tận dụng các đơn vị nhân ma trận và cho phép kernel fusion dẫn đến việc triển khai tích chập dài hiệu quả phần cứng và I/O hơn. Hơn nữa, Monarch Mixer (M2) [Fu et al., 2023], cung cấp một họ biểu cảm của các ma trận có cấu trúc dưới bậc hai tổng quát hóa DFT và các cấu trúc khác. Những ma trận này được tham số hóa như các tích của các ma trận đường chéo khối, cung cấp chi phí tính toán dưới bậc hai từ O(LlogL) đến O(L^{3/2}). Bằng cách đánh đổi độ phức tạp tính toán với việc sử dụng FLOP, M2 đạt được một thay thế hiệu quả phần cứng cho Transformers.

6 Thảo luận và Kết luận

Tóm lại, công trình của chúng tôi giới thiệu Orchid, một mô hình mới giải quyết một số thách thức quan trọng về hiệu quả và khả năng mở rộng trong mô hình hóa chuỗi thông qua việc sử dụng sáng tạo tích chập phụ thuộc dữ liệu. Orchid thành công giảm thiểu chi phí tính toán và bộ nhớ bậc hai liên kết với các lớp attention, trong khi vẫn duy trì, và trong nhiều trường hợp nâng cao, hiệu suất mô hình trên các lĩnh vực khác nhau. Việc giới thiệu một lớp tích chập phụ thuộc dữ liệu đại diện cho một bước tiến đáng kể, cung cấp một sơ đồ trộn chuỗi có thể mở rộng và biểu cảm thích ứng trọng số của nó với dữ liệu đầu vào. Thông qua đánh giá trên nhiều lĩnh vực, bao gồm học trong ngữ cảnh, xử lý ngôn ngữ và hình ảnh, Orchid đã chứng minh không chỉ hiệu suất vượt trội so với các mô hình dựa trên attention truyền thống mà còn tính tổng quát và khả năng mở rộng của nó. Điều này định vị mô hình Orchid không chỉ như một thay thế cho các mô hình hiện có mà còn như một chất xúc tác tiềm năng cho sự đổi mới, thúc đẩy việc khám phá các kiến trúc mới, hiệu quả và mạnh mẽ trong trí tuệ nhân tạo.

Hiệu suất vượt trội của Orchid so với các mô hình dựa trên transformer truyền thống đặt ra những câu hỏi hấp dẫn về trạng thái hiện tại và hướng tương lai của các kiến trúc học sâu. Một giải thích hợp lý cho hiệu quả của mô hình chúng tôi có thể là việc tham số hóa quá mức phổ biến trong các mô hình dựa trên transformer. Một lượng bằng chứng ngày càng tăng chỉ ra rằng các cơ chế attention, bất chấp độ phức tạp tính toán của chúng, chỉ sử dụng một phần nhỏ khả năng của chúng cho các tác vụ như xử lý ngôn ngữ. Điều này thách thức niềm tin phổ biến rằng attention là thành phần quan trọng cho học sâu quy mô lớn, dẫn chúng ta đến việc xem xét lại vai trò của nó và tìm kiếm các thay thế hiệu quả tính toán hơn.

Hạn chế và Hướng Tương lai: Nhìn về phía trước, việc mở rộng mô hình của chúng tôi để phù hợp với các mô hình nhân quả, đặc biệt cho các mô hình ngôn ngữ tự hồi quy giống như GPT, là một hướng tương lai hấp dẫn. Dạng hiện tại của mô hình đề xuất không tương thích vốn có với những kiến trúc này, chủ yếu do sự khác biệt trong cách tích chập toàn cục phụ thuộc dữ liệu xử lý các phụ thuộc và tạo chuỗi. Hơn nữa, việc khám phá khả năng của Orchid như một thay thế hiệu quả cho lớp cross-attention, được sử dụng trong các mô hình chuỗi-đến-chuỗi, cung cấp một con đường nghiên cứu khác. Những cân nhắc này mở ra những khả năng mới để tích hợp mô hình của chúng tôi vào các mô hình nền tảng tiên tiến hơn và các ứng dụng liên ngành.
