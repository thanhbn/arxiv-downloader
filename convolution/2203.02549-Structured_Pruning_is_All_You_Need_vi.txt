# Cắt tỉa có cấu trúc là tất cả những gì bạn cần
cho việc cắt tỉa CNN tại thời điểm khởi tạo
Yaohui Cai Weizhe Hua Hongzheng Chen
G. Edward Suh Christopher De Sa Zhiru Zhang
Đại học Cornell
{yc2632, wh399, hc676, gs272, cmd353, zhiruz}@cornell.edu
Tóm tắt
Cắt tỉa-tại-khởi tạo (PAI) đề xuất cắt tỉa các trọng số riêng lẻ của CNN
trước khi huấn luyện, do đó tránh được việc tinh chỉnh hoặc huấn luyện lại đắt đỏ của mô hình
đã được cắt tỉa. Mặc dù PAI cho thấy kết quả hứa hẹn trong việc giảm kích thước mô hình, mô hình được cắt tỉa
vẫn yêu cầu tính toán ma trận thưa không có cấu trúc, khiến việc
đạt được tốc độ thực tế trở nên khó khăn. Trong công trình này, chúng tôi chỉ ra về mặt lý thuyết và thực nghiệm
rằng độ chính xác của các mô hình CNN được cắt tỉa bởi các phương pháp PAI chỉ phụ thuộc vào
tỷ lệ tham số còn lại trong mỗi lớp (tức là mật độ theo lớp), bất kể
mức độ chi tiết của việc cắt tỉa. Chúng tôi công thức hóa vấn đề PAI như một tối ưu hóa lồi
của chỉ số đánh giá dựa trên kỳ vọng mới được đề xuất cho độ chính xác mô hình, dẫn đến
việc tìm ra mật độ tối ưu theo lớp của mô hình cụ thể đó. Dựa trên
công thức hóa của chúng tôi, chúng tôi tiếp tục đề xuất một phương pháp PAI có cấu trúc và thân thiện với phần cứng,
được gọi là PreCrop, để cắt tỉa hoặc cấu hình lại CNN theo chiều kênh. Kết quả
thực nghiệm của chúng tôi cho thấy PreCrop đạt được độ chính xác cao hơn so với các phương pháp PAI
hiện có trên một số kiến trúc CNN hiện đại, bao gồm ResNet, MobileNetV2,
và EfficientNet cho cả CIFAR-10 và ImageNet. PreCrop đạt được cải thiện độ chính xác
lên tới 2,7% so với thuật toán PAI hiện đại nhất khi cắt tỉa
MobileNetV2 trên ImageNet. PreCrop cũng cải thiện độ chính xác của EfficientNetB0
0,3% trên ImageNet chỉ với 80% tham số và cùng FLOPs.

1 Giới thiệu
Mạng nơ-ron tích chập (CNN) đã đạt được độ chính xác hiện đại trong một loạt rộng các
ứng dụng học máy (ML). Tuy nhiên, yêu cầu tính toán và bộ nhớ khổng lồ
của CNN vẫn là rào cản chính để triển khai rộng rãi hơn trên các thiết bị biên và di động
có tài nguyên hạn chế. Thách thức này đã thúc đẩy một khối lượng lớn nghiên cứu về nén CNN nhằm
đơn giản hóa mô hình gốc mà không làm giảm đáng kể độ chính xác.

Cắt tỉa trọng số [15,7,18,4,8] là một trong những phương pháp được khám phá rộng rãi nhất để giảm
nhu cầu tính toán và bộ nhớ của CNN. Các phương pháp cắt tỉa trọng số hiện có tạo ra một mô hình
CNN thưa bằng cách lặp đi lặp lại việc loại bỏ các trọng số hoặc kích hoạt không hiệu quả và huấn luyện
mô hình thưa kết quả. Hơn nữa, các phương pháp cắt tỉa dựa trên huấn luyện giới thiệu các siêu tham số
bổ sung, chẳng hạn như tốc độ học cho tinh chỉnh và số epoch trước khi tua lại [21], dẫn đến một
quá trình cắt tỉa phức tạp và ít có thể tái tạo hơn. Trong số các kỹ thuật cắt tỉa khác nhau,
cắt tỉa dựa trên huấn luyện thường có sự suy giảm độ chính xác ít nhất nhưng với chi phí của một
thủ tục cắt tỉa đắt đỏ.

Để giảm thiểu chi phí cắt tỉa, một hướng nghiên cứu mới đề xuất cắt tỉa-tại-khởi tạo (PAI)
[16, 28, 25], xác định và cắt tỉa các trọng số không quan trọng trong CNN ngay sau khi khởi tạo nhưng
trước khi huấn luyện. Như trong các phương pháp cắt tỉa dựa trên huấn luyện, PAI đánh giá điểm quan trọng của mỗi
trọng số riêng lẻ và chỉ giữ lại một tập con trọng số bằng cách tối đa hóa tổng điểm quan trọng
của tất cả các trọng số còn lại. Mô hình nén sau đó được huấn luyện bằng cách sử dụng các siêu tham số giống nhau (ví dụ:
hệ số phân rã trọng số) trong cùng số epoch như mô hình cơ sở. Do đó, việc cắt tỉa và
huấn luyện CNN được tách rời một cách gọn gàng, điều này làm giảm đáng kể độ phức tạp để có được một mô hình
CNN đã được cắt tỉa. Hiện tại, SynFlow [25] được coi là kỹ thuật PAI hiện đại nhất - nó tiếp tục
loại bỏ nhu cầu về dữ liệu trong cắt tỉa như yêu cầu trong [16,28] và đạt được độ chính xác cao hơn với
cùng số lượng tham số.

Tuy nhiên, các phương pháp PAI hiện có chủ yếu tập trung vào cắt tỉa trọng số chi tiết, loại bỏ các
trọng số riêng lẻ khỏi mô hình CNN mà không bảo tồn bất kỳ cấu trúc nào. Kết quả là, suy luận và
huấn luyện mô hình được cắt tỉa yêu cầu tính toán ma trận thưa, điều này khó khăn để tăng tốc
trên phần cứng ML có sẵn thương mại như GPU và TPU [14] được tối ưu hóa cho tính toán
dày đặc. Theo một nghiên cứu gần đây [6], ngay cả với thư viện NVIDIA cuSPARSE, người ta chỉ
có thể đạt được tăng tốc có ý nghĩa cho phép nhân ma trận thưa trên GPU khi độ thưa vượt quá 98%. Trong thực tế, CNN hiện đại khó có thể thu nhỏ hơn 50% mà không bị suy giảm drastically độ chính xác [2]. Các mẫu cắt tỉa có cấu trúc (ví dụ: cắt tỉa trọng số cho toàn bộ kênh đầu ra) là cần thiết để tránh lưu trữ và tính toán thưa không đều, do đó cung cấp tiết kiệm bộ nhớ và tính toán thực tế. Hơn nữa, các nghiên cứu gần đây [23,5] cũng quan sát thấy rằng việc xáo trộn ngẫu nhiên mặt nạ trọng số nhị phân của mỗi lớp hoặc khởi tạo lại tất cả các trọng số còn lại không ảnh hưởng đến độ chính xác của mô hình được nén bằng các phương pháp PAI hiện có. Trong công trình này, chúng tôi đầu tiên xem xét các hạn chế của tất cả các phương pháp PAI trước đó.

Dựa trên các quan sát, chúng tôi đưa ra giả thuyết rằng các phương pháp PAI hiện có chỉ hiệu quả trong việc xác định tỷ lệ trọng số còn lại trong mỗi lớp, nhưng thất bại trong việc tìm ra một tập con trọng số đáng kể. Chúng tôi đề xuất sử dụng kỳ vọng của tổng điểm quan trọng của tất cả các trọng số, thay vì tổng, như một chỉ số đánh giá cho độ chính xác của mô hình, do đó coi tất cả các trọng số trong cùng một lớp có tầm quan trọng bằng nhau. Với chỉ số đánh giá mới cho độ chính xác được gọi là SynExp (Kỳ vọng Synaptic), chúng tôi có thể công thức hóa PAI như một bài toán tối ưu hóa lồi trực tiếp giải quyết tỷ lệ tối ưu của các trọng số còn lại trên mỗi lớp (tức là mật độ lớp) chịu một số ràng buộc về kích thước mô hình và/hoặc FLOPs. Chúng tôi cũng chứng minh một định lý rằng SynExp không thay đổi miễn là mật độ theo lớp vẫn giữ nguyên, bất kể mức độ chi tiết của việc cắt tỉa. Định lý mở ra một cơ hội quan trọng rằng các phương pháp PAI chi tiết thô có thể đạt được độ chính xác tương tự như các đối tác chi tiết hiện có của chúng như SynFlow. Chúng tôi chứng minh hiệu quả của chỉ số đánh giá đề xuất thông qua các thí nghiệm thực nghiệm rộng rãi.

Chúng tôi tiếp tục đề xuất PreCrop, một PAI có cấu trúc cắt tỉa các mô hình CNN ở cấp độ kênh. PreCrop có thể hiệu quả giảm kích thước mô hình và chi phí tính toán mà không mất độ chính xác so với các phương pháp PAI chi tiết, và quan trọng hơn, cung cấp tăng tốc thực tế trên phần cứng thương mại. Bằng cách cho phép mỗi lớp có nhiều tham số hơn so với mạng cơ sở, chúng tôi cũng có thể cấu hình lại chiều rộng của mạng với chi phí gần như bằng không, được gọi là PreConfig.

Kết quả thực nghiệm của chúng tôi cho thấy mô hình sau PreConfig có thể đạt được độ chính xác cao hơn với ít tham số và FLOPs hơn so với cơ sở cho nhiều CNN hiện đại.

Chúng tôi tóm tắt đóng góp của mình như sau:
• Chúng tôi đề xuất sử dụng kỳ vọng của tổng điểm quan trọng của tất cả các trọng số như một chỉ số đánh giá cho độ chính xác và công thức hóa PAI như một bài toán tối ưu hóa SynExp bị ràng buộc bởi kích thước mô hình và/hoặc FLOPs. Chúng tôi cũng chứng minh rằng độ chính xác của mô hình CNN được cắt tỉa bằng cách giải quyết tối ưu hóa bị ràng buộc là độc lập với mức độ chi tiết của việc cắt tỉa.

• Chúng tôi giới thiệu PreCrop để cắt tỉa CNN ở cấp độ kênh dựa trên tối ưu hóa SynExp đề xuất. Nghiên cứu thực nghiệm của chúng tôi chứng minh rằng các mô hình được cắt tỉa bằng PreCrop đạt được độ chính xác tương tự hoặc tốt hơn so với các phương pháp PAI không có cấu trúc hiện đại nhất trong khi bảo tồn tính đều đặn. So với SynFlow, PreCrop đạt được độ chính xác cao hơn 2,7% và 0,9% trên MobileNetV2 và EfficientNet trên ImageNet với ít tham số và FLOPs hơn.

• Chúng tôi chỉ ra rằng PreConfig có thể được sử dụng để tối ưu hóa chiều rộng của mỗi lớp trong mạng với chi phí gần như bằng không (ví dụ: tìm kiếm có thể được thực hiện trong vòng một giây trên CPU). So với mô hình gốc, PreConfig đạt được cải thiện độ chính xác 0,3% với 20% ít tham số hơn và cùng FLOPs cho EfficientNet và MobileNetV2 trên ImageNet.

2 Công trình liên quan
Nén mô hình nói chung có thể giảm chi phí tính toán của các mạng lớn để dễ dàng triển khai chúng trên các thiết bị có tài nguyên hạn chế. Ngoài cắt tỉa, lượng tử hóa [3,31,13], NAS [32,24], và chưng cất [12, 29] cũng được sử dụng phổ biến để cải thiện hiệu quả mô hình.

Cắt tỉa dựa trên huấn luyện sử dụng các tiêu chí phỏng đoán khác nhau để cắt tỉa các trọng số không quan trọng. Chúng thường yêu cầu một quá trình huấn luyện-cắt tỉa-huấn luyện lại lặp đi lặp lại trong đó giai đoạn cắt tỉa được đan xen với giai đoạn huấn luyện, có thể tăng tổng chi phí huấn luyện lên nhiều lần. Vì cắt tỉa nhằm giảm tham số, việc giảm FLOPs thường ít đáng kể hơn [17, 4].

Các phương pháp cắt tỉa dựa trên huấn luyện hiện có có thể là không có cấu trúc [7,15] hoặc có cấu trúc [11,19], tùy thuộc vào mức độ chi tiết và tính đều đặn của sơ đồ cắt tỉa. Cắt tỉa không có cấu trúc dựa trên huấn luyện thường cung cấp sự cân bằng độ chính xác-kích thước tốt hơn trong khi cắt tỉa có cấu trúc có thể đạt được tăng tốc và nén thực tế hơn mà không cần hỗ trợ đặc biệt từ phần cứng tùy chỉnh.

Cắt tỉa-tại-khởi tạo (không có cấu trúc) (PAI) [16,28,25] cung cấp một phương pháp hứa hẹn để giảm thiểu chi phí cao của cắt tỉa dựa trên huấn luyện. Chúng có thể xác định và cắt tỉa các trọng số không quan trọng ngay sau khi khởi tạo và trước khi huấn luyện bắt đầu. Liên quan đến những nỗ lực này, tác giả của [5] và [23] độc lập tìm thấy rằng đối với tất cả các phương pháp PAI hiện có, việc xáo trộn ngẫu nhiên mặt nạ trọng số trong một lớp hoặc khởi tạo lại tất cả các trọng số trong mạng không gây ra bất kỳ suy giảm độ chính xác nào.

Tìm kiếm kiến trúc mạng nơ-ron (NAS) [32,27] tự động tìm kiếm trên một tập hợp lớn các mô hình ứng viên để đạt được sự cân bằng tối ưu giữa độ chính xác và tính toán. Không gian tìm kiếm của NAS thường bao gồm chiều rộng, chiều sâu, độ phân giải và lựa chọn các khối xây dựng. Tuy nhiên, các phương pháp hiện có chỉ có thể tìm kiếm trong một tập con nhỏ của các cấu hình chiều rộng kênh có thể do chi phí. Chi phí cho NAS cũng cao hơn hàng bậc so với huấn luyện một mô hình. Một số thuật toán NAS [1,30] sử dụng một chỉ số đánh giá rẻ thay vì huấn luyện toàn bộ mạng, nhưng một học tăng cường mở rộng [32] hoặc thuật toán tiến hóa [20] vẫn được sử dụng để dự đoán một mạng tốt.

3 Cắt tỉa-tại-khởi tạo thông qua tối ưu hóa SynExp
Trong phần này, chúng tôi đầu tiên thảo luận về các kiến thức nền tảng và hạn chế của các phương pháp PAI hiện có. Chúng tôi đề xuất một chỉ số đánh giá mới cho độ chính xác của mô hình nén để khắc phục những hạn chế này. Với chỉ số đánh giá đề xuất, chúng tôi công thức hóa bài toán PAI thành một bài toán tối ưu hóa lồi.

3.1 Kiến thức nền tảng và hạn chế của PAI
Kiến thức nền tảng. PAI nhằm cắt tỉa mạng nơ-ron sau khi khởi tạo nhưng trước khi huấn luyện để tránh quá trình huấn luyện-cắt tỉa-huấn luyện lại tốn thời gian. Trước khi huấn luyện, PAI sử dụng các gradient đối với các trọng số để ước tính tầm quan trọng của các trọng số riêng lẻ, điều này yêu cầu truyền thuận và truyền ngược. Các trọng số (W) với điểm quan trọng nhỏ hơn được cắt tỉa bằng cách đặt các mục tương ứng trong mặt nạ trọng số nhị phân (M) thành không. Các phương pháp PAI hiện có, chẳng hạn như SNIP [16], GraSP [28], và SynFlow [25] chủ yếu khám phá các phương pháp khác nhau để ước tính tầm quan trọng của các trọng số riêng lẻ. Các thuật toán PAI một lần, chẳng hạn như SNIP và GraSP, cắt tỉa mô hình đến độ thưa mong muốn trong một lần. Ngoài ra, SynFlow, đại diện cho thuật toán PAI hiện đại nhất, lặp lại quá trình cắt tỉa một phần nhỏ trọng số và đánh giá lại các điểm quan trọng cho đến khi đạt được tỷ lệ cắt tỉa mong muốn. Thông qua quá trình lặp, tầm quan trọng của các trọng số có thể được ước tính chính xác hơn.

Cụ thể, điểm quan trọng được sử dụng trong SynFlow có thể được công thức hóa như:
SSF(W^l_ij) = [∏(k=l+1 to N) |W^k ⊙ M^k|]_i |W^l_ij M^l_ij| [∏(k=1 to l-1) |W^k ⊙ M^k| 1]_j, (1)

trong đó N là số lớp, W^l và M^l là trọng số và mặt nạ trọng số của lớp thứ l, SSF(W^l_ij) là điểm SynFlow cho một trọng số đơn W^l_ij, ⊙ biểu thị tích Hadamard, |·| là phép toán trị tuyệt đối theo phần tử, và 1 là vector toàn một. Đáng chú ý rằng không có dữ liệu hoặc nhãn nào được sử dụng để tính điểm quan trọng, do đó làm cho SynFlow trở thành một thuật toán không phụ thuộc vào dữ liệu.

Hạn chế. Như được chỉ ra bởi [23,5], việc xáo trộn ngẫu nhiên mặt nạ trọng số W của mỗi lớp hoặc khởi tạo lại tất cả các trọng số M không ảnh hưởng đến độ chính xác cuối cùng của các mô hình được nén bằng các phương pháp PAI hiện có. Ngoài ra, họ cho thấy rằng với cùng mật độ theo lớp (tức là tỷ lệ trọng số còn lại trong mỗi lớp), các mô hình được cắt tỉa sẽ có độ chính xác tương tự. Các quan sát cho thấy rằng mặc dù các thuật toán PAI hiện có cố gắng xác định các trọng số ít quan trọng hơn, việc cắt tỉa trọng số nào không quan trọng đối với độ chính xác.

Tất cả các phương pháp PAI trước đây sử dụng tổng điểm quan trọng của các trọng số còn lại như một chỉ số đánh giá cho độ chính xác mô hình, điều này giống hệt với cắt tỉa dựa trên huấn luyện [7,15]. PAI thu được mặt nạ trọng số nhị phân bằng cách tối đa hóa chỉ số đánh giá như sau:

maximize ∑(l=1 to N) S^l ⊙ M^l over M subject to ∑(l=1 to N) ||M^l||_0 ≤ B_params, (2)

trong đó N là số lớp trong mạng, S^l là ma trận điểm trong lớp thứ l, M^l là mặt nạ trọng số nhị phân trong lớp thứ l, ||·||_0 là số mục khác không trong một ma trận, và B_params là ràng buộc kích thước mô hình được định trước.

Bất kể điểm quan trọng PAI cụ thể nào được chọn, một tập con trọng số được xác định là quan trọng hơn so với các trọng số khác, điều này mâu thuẫn với quan sát rằng xáo trộn ngẫu nhiên không ảnh hưởng đến độ chính xác. Thay vào đó, chúng tôi đề xuất một chỉ số đánh giá độ chính xác mới cho PAI để giải quyết các hạn chế.

3.2 Định lý bất biến SynExp
Lấy cảm hứng từ các quan sát về các phương pháp PAI trước đây, chúng tôi đưa ra giả thuyết rằng một chỉ số đánh giá cho độ chính xác của mô hình được cắt tỉa bằng phương pháp PAI nên thỏa mãn hai tính chất sau:

1. Quyết định cắt tỉa (tức là mặt nạ trọng số M) có thể được thực hiện trước khi mô hình được khởi tạo.
2. Tối đa hóa chỉ số đánh giá nên dẫn đến mật độ tối ưu theo lớp, không phải quyết định cắt tỉa cho các trọng số riêng lẻ.

Đối với cắt tỉa ngẫu nhiên trước khi khởi tạo, với mật độ cố định p_l cho mỗi lớp, ma trận trọng số W^l và ma trận mặt nạ nhị phân M^l của lớp đó có thể được coi là hai biến ngẫu nhiên. Mặt nạ trọng số nhị phân được áp dụng cho ma trận trọng số theo phần tử như W^l ⊙ M^l, trong đó ⊙ biểu thị tích Hadamard. Ma trận trọng số của lớp l (tức là W^l) chứa θ_l tham số. Mỗi trọng số riêng lẻ trong lớp l được lấy mẫu độc lập từ một phân phối cho trước D_l. Giả sử A_l = {M; M_i ∈ {0,1}, ∀1≤i≤θ_l, ∑_i M_i = p_l θ_l} là tập hợp tất cả các ma trận nhị phân có thể có với cùng hình dạng như ma trận trọng số W^l thỏa mãn ràng buộc mật độ theo lớp (p_l). Sau đó, mặt nạ trọng số ngẫu nhiên M^l cho lớp l được lấy mẫu đồng đều từ A_l.

Gọi M = {M^l; ∀1≤l≤N} và W = {W^l; ∀1≤l≤N} là trọng số và mặt nạ của tất cả N lớp trong mạng, tương ứng. Các quan sát trong Phần 3.1 cho thấy rằng bất kỳ thể hiện nào của hai biến ngẫu nhiên M và W đều dẫn đến độ chính xác cuối cùng tương tự của mô hình được cắt tỉa. Tuy nhiên, các thể hiện khác nhau thực sự thay đổi giá trị chỉ số đánh giá cho độ chính xác mô hình trong các phương pháp PAI hiện có. Ví dụ, điểm SynFlow trong Phương trình 2 thay đổi dưới các thể hiện khác nhau của M và W.

Do đó, chúng tôi đề xuất một chỉ số đánh giá mới bất biến với các thể hiện của M và W cho độ chính xác mô hình trong bối cảnh PAI — kỳ vọng của tổng điểm quan trọng của tất cả các trọng số chưa được cắt tỉa (tức là còn lại). Chỉ số đánh giá đề xuất có thể được công thức hóa như sau:

maximize E_{M,W}[S] = E_{M,W}[∑(l=1 to N) S^l ⊙ M^l] over p_l subject to ∑(l=1 to N) θ_l p_l ≤ B_params, (3)

trong đó p_l biểu thị mật độ theo lớp của lớp l, θ_l là số tham số trong lớp l, E_{M,W}[S] đại diện cho kỳ vọng của điểm quan trọng S trên trọng số ngẫu nhiên W, và mặt nạ ngẫu nhiên nhị phân M. Trong công thức mới này, mật độ theo lớp p_l được tối ưu hóa để tối đa hóa chỉ số đánh giá đề xuất cho độ chính xác mô hình.

Để đánh giá kỳ vọng trước khi khởi tạo trọng số, chúng tôi áp dụng chỉ số quan trọng được đề xuất bởi SynFlow, tức là thay thế S trong Phương trình 3 bằng S_SF trong Phương trình 1. Kết quả là, chúng tôi có thể tính toán kỳ vọng một cách phân tích mà không cần truyền thuận hoặc truyền ngược. Chỉ số đánh giá dựa trên kỳ vọng mới này được gọi là SynExp (Kỳ vọng Synaptic). Chúng tôi tiếp tục chứng minh SynExp bất biến với mức độ chi tiết của PAI trong Định lý Bất biến SynExp. Chứng minh chi tiết có thể được tìm thấy trong Phụ lục A.

Định lý 1. Với một kiến trúc CNN cụ thể, SynExp (E_{M,W}[S_SF]) của bất kỳ mô hình được nén ngẫu nhiên nào với cùng mật độ theo lớp p_l là một hằng số, độc lập với mức độ chi tiết của cắt tỉa. Hằng số SynExp bằng:

E_{M,W}[S_SF] = NC_{N+1} ∏(l=1 to N) (p_l C_l E_x∼D[|x|]), (4)

trong đó N là số lớp trong mạng, E_x∼D[|x|] là kỳ vọng về độ lớn của phân phối D, C_l là kích thước kênh đầu vào của lớp l và cũng là kích thước kênh đầu ra của l−1, và p_l = (1/θ_l)||M^l||_0 là mật độ theo lớp.

Trong Phương trình 4, N và C_l đều là siêu tham số của kiến trúc CNN và có thể được coi là hằng số. E|D_l| cũng là một hằng số dưới một phân phối cụ thể D_l. Mật độ theo lớp p_l là biến duy nhất cần được giải quyết trong phương trình. Do đó, SynExp thỏa mãn cả hai tính chất được đề cập ở trên: 1) cắt tỉa được thực hiện trước khi khởi tạo trọng số; 2) mật độ theo lớp có thể được tối ưu hóa trực tiếp. Hơn nữa, Định lý 1 cũng cho thấy rằng mức độ chi tiết của cắt tỉa không có tác động đến chỉ số SynExp đề xuất. Nói cách khác, các mô hình CNN được nén bằng các phương pháp cắt tỉa không có cấu trúc và có cấu trúc sẽ có độ chính xác tương tự.

Chúng tôi xác minh thực nghiệm Định lý 1 bằng cách cắt tỉa ngẫu nhiên mỗi lớp của CNN với mức độ chi tiết cắt tỉa khác nhau nhưng cùng mật độ theo lớp (p_l). Trong nghiên cứu thực nghiệm này, chúng tôi thực hiện cắt tỉa ngẫu nhiên với ba mức độ chi tiết khác nhau (tức là trọng số, bộ lọc và kênh) để đạt được mật độ theo lớp mong muốn thu được từ việc giải Phương trình 3. Đối với cắt tỉa trọng số và bộ lọc, việc cắt tỉa ngẫu nhiên mỗi lớp để phù hợp với mật độ theo lớp p_l đôi khi tách một số trọng số khỏi mạng, đặc biệt khi mật độ thấp. Các trọng số tách rời không đóng góp vào dự đoán nhưng được tính là tham số còn lại. Do đó, chúng tôi loại bỏ các trọng số tách rời để so sánh công bằng theo [26]. Đối với cắt tỉa kênh, việc đạt được mật độ theo lớp cho trước trong khi thỏa mãn ràng buộc rằng kích thước kênh đầu ra của lớp trước phải bằng kích thước kênh đầu vào của lớp tiếp theo là không tầm thường. Do đó, chúng tôi sử dụng PreCrop được đề xuất trong Phần 4.2. Như thể hiện trong Hình 1, cắt tỉa ngẫu nhiên với mức độ chi tiết khác nhau có thể đạt được độ chính xác tương tự so với SynFlow, miễn là mật độ theo lớp vẫn giữ nguyên. Kết quả thực nghiệm phù hợp với Định lý 1 và cũng chứng minh hiệu quả của chỉ số SynExp đề xuất. Chúng tôi bao gồm thêm kết quả thực nghiệm cho các kiến trúc CNN khác nhau và các điểm quan trọng khác nhau trong Phụ lục C.

3.3 Tối ưu hóa SynExp
Như đã thảo luận trong Phần 3.2, chỉ có mật độ theo lớp mới quan trọng đối với phương pháp SynExp đề xuất của chúng tôi. Ở đây, chúng tôi chỉ ra cách thu được mật độ theo lớp trong Phương trình 3 tối đa hóa SynExp dưới các ràng buộc về kích thước mô hình và/hoặc FLOPs.

3.3.1 Tối ưu hóa SynExp với ràng buộc số lượng tham số
Cho rằng mục tiêu của PAI là giảm kích thước của mô hình, chúng ta cần thêm một ràng buộc bổ sung về tổng số tham số B_params (tức là ràng buộc tham số), trong đó B_params thường lớn hơn không và nhỏ hơn số tham số trong mạng gốc. Vì mật độ theo lớp p_l là biến duy nhất trong Phương trình 3, chúng ta có thể đơn giản hóa phương trình bằng cách loại bỏ tất cả các số hạng không đổi khác, như sau:

maximize ∑(l=1 to N) log p_l over p_l subject to ∑(l=1 to N) θ_l p_l ≤ B_params;
0 < p_l ≤ 1, ∀1≤l≤N, (5)

trong đó θ_l là số tham số trong lớp l.

Phương trình 5 là một bài toán tối ưu hóa lồi có thể được giải một cách phân tích. Chúng tôi so sánh mật độ theo lớp thu được từ việc giải Phương trình 5 với mật độ thu được bằng SynFlow. Như thể hiện trong Hình 2, mật độ theo lớp thu được bởi cả hai phương pháp gần như giống hệt nhau, trong đó công thức mới của chúng tôi loại bỏ việc đánh giá lại lặp đi lặp lại các điểm SynFlow và quá trình cắt tỉa trong SynFlow. Cũng đáng chú ý rằng phương pháp đề xuất tìm thấy mật độ tối ưu theo lớp ngay cả trước khi mạng được khởi tạo.

3.3.2 Tối ưu hóa SynExp với ràng buộc số lượng tham số và FLOPs
Như đã thảo luận trong Phần 3.3.1, chúng ta có thể công thức hóa PAI như một bài toán tối ưu hóa lồi đơn giản với ràng buộc về kích thước mô hình. Tuy nhiên, số lượng tham số không phải lúc nào cũng phản ánh hiệu suất (ví dụ: thông lượng) của mô hình CNN. Trong nhiều trường hợp, các mô hình CNN bị giới hạn bởi tính toán trên nhiều phần cứng thương mại [14,9]. Do đó, chúng tôi đề xuất cũng giới thiệu một ràng buộc FLOPs trong công thức của chúng tôi.

FLOPs được tiết kiệm trong các thuật toán PAI hiện có được chỉ định trong Phương trình 2 đến từ việc cắt tỉa các trọng số trong mô hình CNN. Nói cách khác, với một ràng buộc tham số, số lượng FLOP của mô hình được cắt tỉa cũng được xác định. Việc giới thiệu ràng buộc FLOP cho mỗi lớp trong mô hình không đơn giản, vì sự tương ứng giữa số lượng tham số và FLOPs khác nhau giữa các lớp khác nhau. Do đó, không có phương pháp PAI hiện có nào có thể được sử dụng trực tiếp để ràng buộc FLOPs của các mô hình CNN. Vì các trọng số trong cùng một lớp được liên kết với cùng một số lượng FLOP, chúng ta có thể trực tiếp kết hợp ràng buộc về FLOPs B_FLOPs (tức là ràng buộc FLOPs) vào bài toán tối ưu hóa lồi như sau:

maximize ∑(l=1 to N) log p_l over p_l subject to ∑(l=1 to N) θ_l p_l ≤ B_params; ∑(l=1 to N) φ_l p_l ≤ B_FLOPs
0 < p_l ≤ 1, ∀1≤l≤N, (6)

trong đó φ_l là số FLOPs trong lớp thứ l.

Vì ràng buộc FLOPs bổ sung là tuyến tính, bài toán tối ưu hóa trong Phương trình 6 vẫn lồi và có một giải pháp phân tích. Bằng cách giải tối ưu hóa SynExp với B_params cố định nhưng B_FLOPs khác nhau, chúng ta có thể thu được mật độ theo lớp cho các mô hình khác nhau có cùng số lượng tham số nhưng FLOPs khác nhau. Sau đó, chúng tôi thực hiện cắt tỉa trọng số ngẫu nhiên trên mô hình CNN để đạt được mật độ theo lớp mong muốn. Chúng tôi so sánh tối ưu hóa SynExp đề xuất (ký hiệu là Ours) với các phương pháp PAI phổ biến khác. Như được mô tả trong Hình 3, với kích thước mô hình cố định (1,5×10^4 trong hình), phương pháp của chúng tôi có thể được sử dụng để tạo ra một Ranh giới Pareto trải rộng phổ FLOPs, trong khi các phương pháp khác chỉ có thể có FLOPs cố định. Phương pháp của chúng tôi thống trị tất cả các phương pháp khác về cả cải thiện độ chính xác và giảm FLOPs.

4 Cắt tỉa có cấu trúc-tại-khởi tạo
Định lý Bất biến SynExp cho thấy rằng mức độ chi tiết của cắt tỉa trong các phương pháp PAI không nên ảnh hưởng đến độ chính xác của mô hình được cắt tỉa. Cắt tỉa kênh, cắt tỉa các trọng số của CNN ở mức độ chi tiết kênh đầu ra, được coi là kỹ thuật cắt tỉa thô nhất và thân thiện nhất với phần cứng. Do đó, việc áp dụng phương pháp PAI đề xuất cho cắt tỉa kênh có thể tránh được cả các thủ tục huấn luyện lại/tinh chỉnh phức tạp và các tính toán không đều. Trong phần này, chúng tôi đề xuất một phương pháp PAI có cấu trúc cho cắt tỉa kênh, được gọi là PreCrop, để cắt tỉa CNN theo chiều kênh. Ngoài ra, chúng tôi đề xuất một biến thể của PreCrop với các ràng buộc mật độ nới lỏng để cấu hình lại chiều rộng của mỗi lớp trong mô hình CNN, được gọi là PreConfig.

4.1 PreCrop
Việc áp dụng phương pháp PAI đề xuất cho cắt tỉa kênh yêu cầu một thủ tục hai bước. Đầu tiên, mật độ theo lớp p_l được thu được bằng cách giải bài toán tối ưu hóa được thể hiện trong Phương trình 5 hoặc 6. Thứ hai, chúng ta cần quyết định có bao nhiêu kênh đầu ra của mỗi lớp nên được cắt tỉa để thỏa mãn mật độ theo lớp. Tuy nhiên, việc nén mỗi lớp để phù hợp với mật độ theo lớp cho trước không đơn giản do ràng buộc bổ sung rằng số lượng kênh đầu ra của lớp hiện tại phải khớp với số lượng kênh đầu vào của lớp tiếp theo.

Chúng tôi giới thiệu PreCrop, nén mỗi lớp để đáp ứng mật độ theo lớp mong muốn. Gọi C_l và C_{l+1} là số lượng kênh đầu vào của lớp l và l+1, tương ứng. C_{l+1} cũng có nghĩa là số lượng kênh đầu ra của lớp l. Đối với các lớp không có kết nối dư, số lượng kênh đầu ra của lớp l được giảm xuống ⌊p_l C_{l+1}⌋. Số lượng kênh đầu vào của lớp l+1 cần phải khớp với số lượng kênh đầu ra của lớp l, cũng được giảm xuống ⌊p_l C_{l+1}⌋. Do đó, mật độ thực tế của lớp l sau PreCrop là p_l \frac{⌊p_l C_{l+1}⌋}{C_{l+1}} thay vì p_l. Chúng tôi tìm thấy thực nghiệm rằng p_l \frac{⌊p_l C_{l+1}⌋}{C_{l+1}} đủ gần với p_l vì các lớp lân cận có mật độ theo lớp tương tự.

Ngoài ra, có thể thu được mật độ theo lớp chính xác p_l bằng cách chỉ giảm số lượng kênh đầu vào hoặc đầu ra của một lớp. Tuy nhiên, cách tiếp cận này dẫn đến sự sụt giảm đáng kể về độ chính xác, vì số lượng kênh đầu vào và đầu ra có thể thay đổi dramaticly (ví dụ: ⌊p_l C_l⌋ C_{l+1} hoặc C_l ⌊p_l C_{l+1}⌋). Điều này khiến hình dạng của bản đồ đặc trưng thay đổi dramaticly trong các lớp liền kề, dẫn đến mất thông tin.

Đối với các lớp có kết nối dư, Hình 4 mô tả một cách tiếp cận để vượt qua ràng buộc về số lượng kênh của các lớp liền kề. Chúng ta có thể giảm số lượng kênh đầu vào và đầu ra của lớp l từ C_l và C_{l+1} xuống ⌊p_l C_l⌋ và ⌊p_l C_{l+1}⌋, tương ứng. Bằng cách này, mật độ của mỗi lớp có thể khớp với mật độ theo lớp cho trước thu được từ phương pháp PAI đề xuất. Vì đầu ra của lớp l cần được cộng theo phần tử với đầu vào gốc của lớp l, đầu ra của lớp l được đệm với các kênh có giá trị không để khớp với hình dạng của đầu vào gốc của lớp l. Trong implementation của chúng tôi, chúng tôi chỉ đơn giản cộng đầu ra của lớp l với ⌊p_l C_{l+1}⌋ kênh đầu tiên của đầu vào gốc của lớp l, do đó không yêu cầu bộ nhớ hoặc tính toán bổ sung cho việc đệm không. PreCrop loại bỏ yêu cầu về tính toán thưa trong các phương pháp PAI hiện có và do đó có thể được sử dụng để tăng tốc cả huấn luyện và suy luận của các mô hình được cắt tỉa.

4.2 PreConfig: PreCrop với ràng buộc mật độ nới lỏng
PreCrop sử dụng mật độ theo lớp thu được từ việc giải bài toán tối ưu hóa lồi, luôn nhỏ hơn 1 theo thiết lập chung cho cắt tỉa (tức là p_l ≤ 1). Tuy nhiên, ràng buộc này về mật độ theo lớp không cần thiết cho phương pháp của chúng tôi vì chúng ta có thể tăng số lượng kênh (tức là mở rộng chiều rộng của lớp) trước khi khởi tạo. Bằng cách giải bài toán trong Phương trình 6 mà không có ràng buộc p_l ≤ 1, chúng ta có thể mở rộng các lớp với mật độ lớn hơn 1 (p_l > 1) và cắt tỉa các lớp với mật độ nhỏ hơn 1 (p_l < 1). Chúng tôi gọi biến thể của PreCrop này là PreConfig (PreCrop-Reconfig). Nếu chúng ta đặt B_params và B_FLOPs giống như mạng gốc, chúng ta có thể cấu hình lại chiều rộng của mỗi lớp của một kiến trúc mạng cho trước dưới các ràng buộc nhất định về kích thước mô hình và FLOPs.

Chiều rộng của mỗi lớp trong CNN thường được thiết kế thủ công, thường dựa vào kinh nghiệm và trực giác rộng rãi. Sử dụng PreConfig, chúng ta có thể xác định chiều rộng của mỗi lớp trong mạng để đạt được sự cân bằng chi phí-độ chính xác tốt hơn. PreConfig cũng có thể được sử dụng như một hoặc một phần của NAS siêu nhanh. So với NAS, thường tìm kiếm về chiều rộng, chiều sâu, độ phân giải và lựa chọn các khối xây dựng, PreConfig chỉ thay đổi chiều rộng. Tuy nhiên, PreConfig chỉ yêu cầu một lượng thời gian và tính toán tối thiểu so với các phương pháp NAS; nó chỉ cần giải một bài toán tối ưu hóa lồi tương đối nhỏ, có thể được giải trong một giây trên CPU.

5 Đánh giá
Trong phần này, chúng tôi đánh giá thực nghiệm PreCrop và PreConfig. Đầu tiên, chúng tôi chứng minh hiệu quả của PreCrop bằng cách so sánh nó với SynFlow. Sau đó, chúng tôi sử dụng PreConfig để điều chỉnh chiều rộng của mỗi lớp và so sánh độ chính xác của mô hình sau PreConfig với mô hình gốc. Chúng tôi thực hiện các thí nghiệm sử dụng các mô hình CNN hiện đại khác nhau, bao gồm ResNet [10], MobileNetV2 [22], và EfficientNet [24], trên cả CIFAR-10 và ImageNet. Chúng tôi đặt tất cả các siêu tham số được sử dụng để huấn luyện các mô hình được cắt tỉa bởi các thuật toán PAI khác nhau giống nhau. Xem Phụ lục E để biết cài đặt siêu tham số chi tiết.

5.1 Đánh giá PreCrop
Đối với CIFAR-10, chúng tôi so sánh độ chính xác của SynFlow (đường màu đỏ) và hai biến thể của PreCrop: PreCrop-Params (đường màu xanh lam) và PreCrop-FLOPs (đường màu xanh lục). PreCrop-Params thêm ràng buộc số lượng tham số trong khi PreCrop-FLOPs áp đặt ràng buộc FLOPs vào bài toán tối ưu hóa lồi. Như thể hiện trong Hình 5a, PreCrop-Params đạt được độ chính xác tương tự hoặc thậm chí tốt hơn so với SynFlow dưới một loạt các ràng buộc kích thước mô hình khác nhau, do đó xác nhận rằng PreCrop-Params có thể hiệu quả như phương pháp PAI chi tiết. Xem xét lợi ích của cắt tỉa có cấu trúc, PreCrop-Params nên được ưu tiên hơn so với các phương pháp PAI hiện có. Hình 5b tiếp tục cho thấy PreCrop-FLOPs liên tục vượt trội hơn SynFlow với biên độ lớn, đặc biệt khi việc giảm FLOPs lớn. Kết quả thí nghiệm cho thấy PreCrop-FLOPs nên được áp dụng khi hiệu suất của mô hình bị giới hạn bởi chi phí tính toán.

Bảng 1 tóm tắt so sánh giữa PreCrop và SynFlow trên ImageNet. Đối với ResNet-34, PreCrop đạt được độ chính xác thấp hơn 0,6% so với SynFlow với kích thước mô hình và FLOPs tương tự. Đối với cả MobileNetV2 và EfficientNetB0, PreCrop đạt được cải thiện độ chính xác 1,2% và 0,9% so với SynFlow với FLOPs và tham số ít hơn một cách nghiêm ngặt, tương ứng. Kết quả thí nghiệm trên ImageNet tiếp tục hỗ trợ Định lý Bất biến SynExp rằng một cắt tỉa có cấu trúc thô (ví dụ: PreCrop) có thể hoạt động tốt như cắt tỉa không có cấu trúc. Tóm lại, PreCrop đạt được sự cân bằng độ chính xác và kích thước mô hình/FLOPs thuận lợi so với thuật toán PAI hiện đại nhất.

5.2 Đánh giá PreConfig
Như đã thảo luận trong Phần 4.2, PreConfig có thể được xem như một kỹ thuật NAS siêu nhanh, điều chỉnh chiều rộng của mỗi lớp trong mô hình ngay cả trước khi các trọng số được khởi tạo.

Bảng 2 so sánh độ chính xác của mô hình được cấu hình lại với mô hình gốc dưới các ràng buộc kích thước mô hình và FLOPs tương tự. Đối với ResNet34, với độ chính xác tương tự, chúng tôi giảm số lượng tham số 25%. Đối với MobileNetV2, chúng tôi đạt được độ chính xác cao hơn 0,3% so với cơ sở với 20% ít tham số hơn và 3% ít FLOPs hơn. Đối với EfficientNet, chúng tôi cũng có thể đạt được độ chính xác cao hơn 0,3% so với cơ sở chỉ với 80% tham số và cùng FLOPs. Lưu ý rằng EfficientNet được xác định bởi một phương pháp NAS. Vì PreConfig chỉ thay đổi số lượng kênh của mô hình trước khi khởi tạo, chúng tôi tin rằng nó cũng áp dụng cho các kỹ thuật nén khác.

6 Kết luận
Trong công trình này, chúng tôi chỉ ra về mặt lý thuyết và thực nghiệm rằng chỉ có mật độ theo lớp mới quan trọng đối với độ chính xác của các mô hình CNN được cắt tỉa bằng các phương pháp PAI. Chúng tôi công thức hóa PAI như một tối ưu hóa SynExp lồi đơn giản. Dựa trên tối ưu hóa SynExp, chúng tôi tiếp tục đề xuất PreCrop và PreConfig để cắt tỉa và cấu hình lại CNN theo chiều kênh. Kết quả thí nghiệm của chúng tôi chứng minh rằng PreCrop có thể vượt trội hơn các phương pháp PAI chi tiết hiện có trên các mạng và bộ dữ liệu khác nhau.
