# 2310.18780.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2310.18780.pdf
# File size: 12383929 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Laughing Hyena Distillery :
Extracting Compact Recurrences From Convolutions
Stefano Massaroli∗,1, Michael Poli∗,2, Daniel Y. Fu∗,2,
Hermann Kumbong2, Rom N. Parnichkun3, Aman Timalsina4,
David W. Romero5, Quinn McIntyre2, Beidi Chen6, Atri Rudra7, Ce Zhang8,
Christopher Re2,†, Stefano Ermon2,†, Yoshua Bengio1,†
NeurIPS 2023 ,Last Compiled : October 31, 2023.
Abstract
Recent advances in attention-free sequence models rely on convolutions as alternatives to the atten-
tion operator at the core of Transformers. In particular, longconvolution sequence models have achieved
state-of-the-art performance in many domains, but incur a significant cost during auto-regressive infer-
ence workloads – naively requiring a full pass (or cachingof activations) over the input sequence for
each generated token – similarly to attention-based models. In this paper, we seek to enable O(1)com-
pute and memory cost per token in any pre-trained long convolution architecture to reduce memory
footprint and increase throughput during generation. Concretely, our methods consist in extracting low-
dimensional linear state-space models from each convolution layer, building upon rational interpolation
and model-order reduction techniques. We further introduce architectural improvements to convolution-
based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher
pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10×
higher throughput than Transformers and 1.5×higher than Hyenaat1.3B parameters, without any loss
in quality after distillation.
1 Introduction
Attention-free approaches such as long convolution sequence models (LCSMs), e.g., H3[1],Hyena[2], have
shown promise in matching Transformer [3, 4] performance across a wide range of tasks, with sub-quadratic
complexity with respect to sequence length. Despite the improved efficiency during training on long se-
quences, unless the convolution filters are either shortor admit a low-dimensional state-state-space realiza-
tion, LCSMs still need to process the entire growing sequence at every step of auto-regressive generation,
similarly to Transformers.
In this work, we seek to refine LCSMs in both efficiency andquality. First, we study the inference
stage, and propose methods to enable a recurrent mode for auto-regressive generation. Recurrent modes
prescribe the existence of a stateencoding the past information of the process in a fixed-dimension memory,
enablingconstant per-step time andconstant-memory in generation. Then, we draw upon an analysis
of pre-trained models to develop architectural enhancements for the Hyenablock, simultaneously improving
model quality and efficiency of the distillation procedure.
Distilling fast recurrences We introduce LaughingHyena , the first distillation approach for LCSMs that
enables recurrent inference without impacting downstream quality. LaughingHyena seeks compact recur-
rences in the form of state-space models (SSMs) [5, 6] as the solution of a nonlinear interpolation problem
involving the convolution filters of a pre-trained model. Since the total memory cost of SSMs grows linearly
in the state dimension d, our distillation procedure enables high throughput by enabling processing of large
batches during generation.
∗Equal contribution. †Equal senior authorship.1Mila and Université de Montréal.2Stanford University.3The University
of Tokyo.4Purdue University.5Vrije Universiteit Amsterdam.6Carnegie Mellon University and Meta AI (FAIR).7University
of Buffalo, SUNY.8University of Chicago and Together Computer.
1arXiv:2310.18780v1  [cs.LG]  28 Oct 2023

--- PAGE 2 ---
We identify and address three core challenges related to distillation, including the identification of:
•Target state dimension: we identify candidate state dimensions of our distilled SSMs by analyzing
the spectrum of the Hankel operator associated with each convolution [7].
•Parametrization: we address issues with naive parametrizations by introducing a factorized modal
form, inspired by barycentric [8] and Prony-like [9] methods .
•Approximation metric: to ensure compatibility with any downstream task, we choose discrepancy
metrics on the convolution filter, rather than model outputs.
In auto-regressive workloads, LaughingHyena -distilled models with state dimension dcan generate Ktokens
inO(dK)time and with constant O(d)memory – improving over the O(K2)time and O(K)memory
usage of kv-cached Transformers and naively executed long convolutions. At model sizes above one billion
parameters, LaughingHyena achieves 10×higher peak throughput over comparable Transformers (Figure
1.1), and can process larger batch sizes. Constant memory generation enables larger Kfor a given a
memory constraint e.g., generating 512tokens with LaughingHyena requires 3×less memory than with
a Transformer. At smaller batch sizes, latency of LaughingHyena is also competitive with Transformers,
reaching ≥2×speedups at longer prompt lengths.
100 200 300 400 5000.20.40.60.81·104
Batch SizeGeneration Throughput [tok/s]Laughing Hyena 1 .3B Hyena 1 .3B
Hybrid Attn .H3 1.3B Transformer 1 .3B
Figure 1.1: Throughput (in generated tokens) of Transform-
ers,H3andHyenamodels. LaughingHyena is a recurrent model
distilled from a pre-trained Hyena. Workload involves generat-
ing256tokens given a prompt of length 512.Improving pre-training quality We leverage
our analysis of the distillation process to open up
new avenues of improvement for LCSM architec-
tures. Indeed, the high compression rates achiev-
able through LaughingHyena hint at sub-utilization
of the convolution. We revisit the multi-headed de-
sign of H3[1]; tying weights across channels pushes
long convolution filters towards larger effective di-
mension,andasanadditionaladvantagereducesthe
runtime of post-training distillation and inference
memory footprint. Further, multi-head Hyenamod-
els improve on pre-training perplexity over regular
Hyenaand GPT [10] architectures on the language
dataset The Pile [11].
2 Preliminaries and Related Work
We discuss convolutions, state spaces and auto-regressive generation workloads for sequence models.
Convolutions Let∗denote the convolution operator. It is defined as the dual operation to point-wise
multiplication under Fourier transform. In signal processing and deep learning alike, one often encounters
the causal linear convolution of a filter h(which may extend indefinitely) with an input uof length L:
(h∗u)t=tX
j=0ht−juj. (2.1)
Generally, ut∈RDwhere Dis the width of the signal – or in deep learning parlance – the number of
channels. Without loss of generality, we specialize our analysis to single input single output layers, i.e. with
D= 1. For the input-output relations of type (2.1), we use the terms convolution layer andlinear system
interchangeably. Similarly, the function t7→htis referred to as both the filterand theimpulse response of a
linear system. Existing convolution sequence models can be classified in terms of the parametrization used
for their filters. The class of implicitconvolutions represent the filter as a parametric function γθ:t7→ht.
State-space realization One option is to select γθas theimpulse response function of a discrete linear
time-invariant system,
xt+1=Axt+But
yt=Cxt+h0ut, t7→ht=(
h0 t= 0
CAt−1Bt >0(2.2)
withstate xt∈Rd,input ut∈R, andoutput yt∈R. The matrices A∈Rd×d,B∈Rd×1,C∈R1×d,
andh0∈Rare the learnable parameters of the model while the initial state x0is usually set to zero such
2

--- PAGE 3 ---
that u7→yis a pure convolution. While linear systems (2.2) are the staple of signal processes and control
theory, their use as implicit parametrization of convolution filters in deep neural networks have only recently
emerged [12, 6]. Other parametrizations [13, 14, 2] select γθ(t)as different flavors of implicit representation
neural networks [15, 16]. The latter are generally more powerful in terms of the class of filters they can
represent and flexibility during training, at the cost of losing a fixed state dimension.
2.1 Long Convolution Sequence Models
u
T(v)T(k)T(q)
MkThMqkq
vy
Figure 2.1: H-block. T(q),T(k),T(v)are
short-convolution operators.The H-family of convolution sequence models – H3[1] and Hyena[2]
– relies on a combination of long convolutions and data-controlled
gating to replace attention with sub-quadratic scaling in sequence
length1. We use the deep learning convention of naming different
projections as query q,keykandvalue v. Let Mqand Mkbe the
L-by-Ldiagonal matrices whose respective main diagonal entries are the respective entries of length- L
sequences qandk. AH-block realizes a surrogate attention matrix with a data-controlled, parameterized
decomposition in three terms:
(q, k, v )7→H(q, k)v,H(q, k) =MqThMk (2.3)
where Th∈RL×Lis the Toeplitz matrix constructed from the learnable long convolution filter h, i.e.,
Th=(hi−j)L−1
i,j=0. The qkv-projections are themselves the output of a convolution between the input sequence
and three distinct shortfilters. The degrees of freedom in H-block design are the three short filters2and the
longfilter h. The long filter can be parameterized using an implicit neural representation [2], state-space
model [1], or explicit values [17]. The threefold decomposition of the attention operator, allows evaluation
of (2.3) in just ˜O(L):=O(Llog2L)time (two convolutions3and two element-wise products), yt=qt(h∗kv)t.
The overall operator acts on an input uby constructing a third-order multi-variate polynomial of uwhose
coefficients are controlled (nonlinearly) by parameters of the block.
2.2 Auto-Regressive Generation
A typical workload for sequence models is auto-regressive generation. Given a length- Tprompt u∈RT, the
model is tasked with producing the following Kadditional outputs – one at a time – for a resulting output
sequence yof length L=T+K.
Convolution sequence models After processing the initial prompt in ˜O(T)time and obtaining a length-
Toutput u7→y0, . . . , y T−1, a generic convolution layer can cachethe output sequence and generate any
additional outputs using (2.1) auto-regressively, i.e. yt+1=Pt
j=0ht−jyjfort=T−1, . . . , T +K−1. It is
important to note that auto-regressive generation with generic long convolutions is expensive. It comes with
aquadratic cost in the number Kof tokens to be generated and require storing a cache of length up to L.
Lemma 2.1. Generating Ktokens with a long convolution layer (2.1)from a length- Tprompt has
time complexity O(Tlog2T+TK+K2)and requires O(L)memory.
State-space models When the linear system admits a state space realization (2.2), i.e. it is able to
switch between convolution and recurrent mode, the cost of auto-regressive generation can be dramatically
reduced. The memory footprint is O(d): all we need to cache is the state xt, ad-dimensional vector. With
some further machinery that we develop in next section, we can retain ˜O(T)time and O(T)memory to
process the prompt4and initialize the state xT−1. Each additional generation step only requires O(d)time.
Lemma 2.2. Generating Ktokens with a state-space model (2.2)from a length- Tprompt has time
complexity O(Tlog2T+dK)and requires O(T+d)memory.
Notethatlongfilters htruncatedtolength d(i.e.ht=0fort > d−1)canalsobeinterpretedas d-dimensional
SSMs (see Appendix A.7) where the state (a cache) coincides with the last dinputs.
1In this work, we consider second-order Hyenablocks [2] to automatically extend our findings to H3[1].
2The short filters are explicitly parameterized, see [2].
3Theqkvshort convolutions can be evaluated in batch with a single pass. The second convolution is the one with the long
filter hand performed via Fast Fourier Transform (FFT), hence the ˜O(L)complexity.
4In §3.4 we show that multiple pre-filling strategies exist, with different trade-offs in time and memory.
3

--- PAGE 4 ---
Transformers Self-attention is certainly less efficient than long convolutions in processing the prompt,
coming with a hefty O(T2)time complexity. However, Transformers can achieve a similar efficiency in
auto-regressive generation by caching the sequences of past keys {kt}and values {vt}. Specifically, from
t=T−1onward, the new projections (qt+1, kt+1, vt+1)are evaluated from the current output yt, and the new
output yt+1can be computed in linear time with two reductions
yt+1=Pt+1
j=0φ(qt+1kj)vjPt+1
i=0φ(qt+1kj)where φ:R→Ris usually chosen as φ(x) =ex.
Lemma 2.3. Generating Ktokens with self-attention from a length- Tprompt has time complexity
O(T2+TK+K2)and requires O(L)memory.
3 The Laughing Hyena Distillery
In this section, we introduce our distillation method. We discuss choosing an approximation objective, a
parametrization for the approximant and setting a target state dimension.
Given any pre-trained LCSM, the objective of the distillation procedure is to convert each pre-trained
convolution filter into a distinct state-space model (2.2). This should be achieved with the smallest state
dimension dwhich preserves, up to a certain tolerance, the input-output characteristics of the convolution
layer. Formally, given a filter hthedistillation problem is defined as follows.
Given the sequence h1, . . . , h L, find a state-space model (2.2) of dimension d≪L, whose input–output
behavior approximates the one of the convolution with hover the largest class of input sequences.
The choice of approximation metrics and assumptions on the input sequences yield different distillation
objectives . Adistillation algorithm constitutes a systematic procedure for optimally choosing the systems
matrices with respect to a particular objective. In instances where the original filter his itself the impulse
response of a finite-dimensional state-space model, e.g., when attempting distillation of H3orS4[6] filters,
the term distillation becomes analogous to model-order reduction . Hence, in such cases, the distillation
algorithm should yield a state-space representation of a lower order state-dimension.
There exist several algebraic solutions to the model reduction problem [18, 19, 20], typically seeking
low-rank structures of the state space by inspecting some invariant of the system, e.g. the Gramians in
balanced truncation [19, Ch. 7]. The lower-order system is then obtained as a projection of the system dy-
namicsontothefoundsubspacewherethesystemretainsdesiredcharacteristics, e.g., input-outputbehavior,
stability, etc.
Truncated filters In theory, implicitly parameterized convolution filters can represent arbitrarily long
signals. In practice, these filters are trained on a fixed maximum length L. At inference time the model can
then be evaluated for sequences longer than L. During distillation it is nonetheless reasonable to treat the
pre-trained filters as potentially very long (even beyond L) butfiniteimpulse response functions [21, 22, 23,
24]. We show how this choice is supported by empirical evidence displaying how pre-trained filters typically
decay to zero in finite time (see Appendix D).
Transfer function representation An alternative description of the system (2.2) is its transfer func-
tionH, defined as the z-transform of the impulse response H(z)=P∞
t=0htz−tfor all z∈Cwhere the sum
converges. The transfer function is a proper rational function ofz
H(z) =h0+C(zI−A)−1B=h0+b1z−1+···+bdz−d
1 +a1z−1+···+adz−d. (3.1)
In the z-domain, the transfer function defines the input-output map as Y(z) =H(z)U(z). Here, H(z)
is defined outside the C-plane circle of radius ρ(A),Dρ(A):={z∈C:|z|> ρ(A)}where ρ(A)is the
spectral radius of A, i.e. the amplitude of its largest eigenvalue. We can recover all characteristics
of a given system equivalently from either its transfer function or state-space representations (see
Appendix A.3 for further details and derivations). Notably, the transfer function is an invariant of
the system: if we apply a change of variables to the state, the transfer function remains unchanged
(Lemma A.3). This alone should discourage attempts at modeling filters by learning densestate-
space matrices A,B,Cas such: there are infinitely many equivalent state-space realizations that map
4

--- PAGE 5 ---
Sequence
DatasetPretrain
Conv.ModelhHankel
SVDcandidate
state dim d
Distill
State -SpaceA,B,C, h0Compose
RecurrenceCompute
Transfer FunctionDeployment
-Downstream eval .
-Fine tuningGet initial value h0Figure 3.1: The LaughingHyena long convolution sequence model distillation blueprint.
to the same system. Starting from coefficients (ai)and(bi)of the rational transfer function (3.1),
we can compute the impulse response in ˜O(L)time (Lemma A.6). Moreover, we can map back
the transfer function to a special state-space realization – the companion canonical form – whose
recurrence has time complexity O(d)(Lemma A.7), compared to the O(d2)of dense state-space
matrices. From Lemmas A.3 and A.7 we can also prove that any stable state-space model can be
converted by canonicalization into its companion form, and thus can be equipped with an efficient
recurrence (Thm. A.8).
The distillation problem presents several challenges:
1.Defining the distillation objective. A primary decision involves selecting a distillation objective.
We are primarily interested in metrics of pure discrepancy between each filter of a pre-trained deep
model and its approximator, rather than the expected input-output loss over a distribution of inputs.
2.Choosing a state-space parametrization. It is crucial to determine a suitable parametrization of
the distilled state-space realization. Once this is decided, the task is to identify the parameters that
minimize the distillation desiderata, which can involve challenging optimization problems in itself.
3.Selecting the target state dimension. Lastly, a challenge is to estimate the degree to which the
model’sordercanbereduced. Inotherwords,wemustselectthetargetstatedimensionofthedistillation
process to identify the right trade-off between efficiency and accuracy.
In the following, we address each of these challenges, and provide a comprehensive approach (summarized
in Figure 3.1) to distill recurrences from convolution-based architectures.
3.1 Data-Free Distillation Objectives
We focus on distillation objectives that are independent of the training data and the overall architecture
of the neural network under consideration. The distillation loss should be chosen as a pure measure of
discrepancy between each convolution filter htof the model and their finite-dimensional approximations
ˆht=CAt−1B. This approach ensures that we do not require a full sequential inference pass over the pre-
trained model at each step of distillation procedure and the distilled model can be more broadly applied
to downstream tasks. This choice is supported by Young’s convolution inequality [25, 26], which indicates
that the output approximation error has a bound ∥y−ˆy∥r≤ ∥h−ˆh∥q∥u∥pfor properly chosen norms5. For
maximum numerical stability and freedom of parametrization for the approximants, we favor modern uncon-
strained gradient-based approaches to then solve the resulting distillation program6. We design distillation
algorithms which either match filters in time domain minimizing the ℓ2error ( ∥h∥2:= [P
t∈Z|ht|2]1/2) or
matchtheirtransferfunctionsoptimallywithrespecttothe H2norm( ∥H∥2:= [(1 /2π)Rπ
−π|H(eiω)|2dω1/2)7.
As the distillation is carried out via gradient methods, ℓ2is a natural candidate. H2error minimization can
instead be used to uniformy bound the worst-case discrepancy as ∥h−ˆh∥∞≤ ∥H−ˆH∥2(see Appendix A.2
for further details).
3.2 Making HyenaLaugh with Modal Interpolation
Our degrees of freedoms to solve the distillation problem are the matrices A,B, and Cof the state-space
realization, which determine the filter for all t >0. In distilled SSMs, the passthrough (residual) term
5p, q, r > 0should satisfy 1/q+ 1/p= 1/r+ 1. In the case of infinite sequences defined on the all Z, the norms are taken
in aℓp, ℓq, ℓrsense, respectively. The bound is potentially sharp [27, 28]
6For completeness, we also test balanced and modaltruncation techniques on a suite of pre-trained H3andHyenamodels
in Appendix E.3.
7Such norms are always well-defined for finite sequences of interest which are in ℓ∞.
5

--- PAGE 6 ---
cannot be freely assigned: it is simply h0, the value of the original filter at zero. Alternatively, given its
appealing invariance properties, we can parametrize a proper rational function ˆH(z)(3.1) and fit it to the
(truncated) transfer function8of the original filter HL(z):=PL
t=0htz−t(see Appendix B.2).
Modal canonical form Optimizing the full transfer function can be numerically challenging for several
reasons e.g., ensuring stability9, and ill-posedness for high-order polynomials. A natural solution, inspired
by barycentric approaches to rational function approximation [29, 8], is to assume ddistinct roots λnin the
denominator’s polynomial, λn∈roots(poly(a)).
Proposition 3.1 ([5]).Ifpoly(a)has distinct roots {λn∈C}, then the transfer function of the system
can be factorized as ˆH(z)=Pd
n=1Rn/(z−λn),∀z∈Dρ(A)where {Rn∈C}is the residue associated
with the pole λn.
Computing the inverse transform of the expanded transfer function via, e.g., the Cauchy residue theorem
[30], shows that the resulting impulse response ˆhcorresponds to a truncated basis of exponentially decaying
complex sinusoids
ˆht=dX
n=1Rnλt−1
n, R n, λn∈C, t > 0. (3.2)
In practice, this corresponds to the impulse response of state-space model with diagonal matrix A=
diag( λ1, . . . , λ d)and such that BiCi=Rifor all i= 1, . . . , d. The distillation problem can be then de-
fined in terms of the L-point nonlinear least squares interpolation error (squared ℓ2) between h1, . . . , h Land
(3.2) evaluated for t=1, . . . , L:min{λn,Rn}∥ˆh−h∥2
2. Note that in case of the target filter hbeing real-valued,
the objective can be replaced by ∥R[ˆh]−h∥2
2.
0 20 40 60 80 100 120−0.500.51
thtModal Interpolation
Rnλt−1
nht ˆht
Figure 3.2: Example of modal interpolation. The approxi-
mant is a linear combination of exponentially-decaying com-
plex exponential basis functions with learned decay rate.Although we find solutions of the distillation (in-
terpolation) problem via modern gradient-based opti-
mizationtechniques, itisworthmentioningthatProny
showed how the nonlinear least square solution can
be computed solving two linear problems [9]. How-
ever, similar to Padé’s method for rational approxi-
mation [31], these techniques can be numerically un-
stable. We opt for a parametrization similar to [32,
33] where each eigenvalue is parameterized in polar
form λn:=Aneiθnand the residues in cartesian form10.
Note that, with this parametrization we have R[ˆht] =P
nAt−1
n[R(Rn) cos( θn(t−1))−I(Rn) sin(θn(t−1))].
We can also solve the distillation problem in the H2
sense by evaluating ˆhtandhtatt= 0, . . . , L −1and
taking their respective (discrete) Fourier transform before computing the objective. Efficient evaluation of
(3.2) is crucial for distillation. In particular we show the following:
Lemma 3.1. Evaluation of (ˆht)L−1
t=0(3.2)can be done in O(dL)time from its modal form and in
˜O(L)time from its proper rational form.
3.3 Minimal Distillation Orders
Distillingintolower-dimensionalsystemsisalwaysdesirableastheyrequirefewerparameterstobeoptimized
and they yield recurrences that are (linearly) more efficient in terms of time and memory complexity in post-
distillationauto-regressiveinferenceworkloads. Thedimensionof the smallest possible state-space model with
impulse response exactly {ht}t∈Nis the so-called McMillan degree [34]:
d⋆= arg min
dd:∃A∈Cd×d,B∈Cd×1,C∈C1×dwith ht=CAt−1B,∀t >0 (3.3)
Theorem 3.1 (Ho-Kalman [35, Theorem 2, Corollary]) .LetSbe the (infinite) Hankel matrix con-
structed with h, i.e. S:= (hi+j)∞
i,j=1. Then, d⋆= rank( S).
8As already partially discussed in [6], the truncation introduces a correction term in the approximant transfer function.
See Appendix A.4.
9i.e normalizing denominator polynomial coefficients to constrain roots within the unit circle.
10We report additional details on the nuances of the parametrization in Appendix B.1.
6

--- PAGE 7 ---
A lower bound for d⋆can be estimated from a truncated filter of length Lby constructing the L×L
principal sub-matrix SLand using the fact that rank( S)≥rank( SL). Inspecting how fast the Hankel
singular values (σn)L
n=1decay in pre-trained convolution models can be predictive of the approximation
quality at a fixed dimension. As a rule of thumb, dneeds to be sufficiently large for σd+1to be sufficiently
small11. Specifically, we can prove that the lastsingular value σddetermines the upper bound of distillation
quality with a SSM of dimension d, in terms of the Hankel norm [19]. This is a direct consequence of
Adamjan-Arov-Krein theorem [7] and can be informally stated as follows.
Theorem 3.2 (Informal) .Lethbe a length- Lfilter, ˆha distilled filter of order d < Land let SL,ˆSL
be the respective Hankel matrices. Then infˆSL∥SL−ˆSL∥2=σd.
3.4 Deploying the Recurrence
Onceallthefiltersofapre-trainedmodelhavebeendistilledwiththeproposedmodalinterpolationtechnique
described above, the model unlocks a recurrent mode which allocates a state xt∈Cdfor each filter and
enables fast auto-regressive inference. Deployment of distilled model involves two critical steps: the pre-
fillingand the recurrent update rule itself.
Fast pre-filling During auto-regressive generation, when a length- Tprompt is fed to the model, we
need to compute the state xTto start generating new tokens. Using the recurrence, the time complexity
of initializing xTwould be O(dT)with a O(d)memory footprint. One can alternatively distribute the
computation on dprocessors with a parallel scan operation [37, 38] to reach a parallel time complexity
O(dlog2T)while incurring in an increased memory requirement of O(dT)12. A third option is to use a
single FFTconvolution to obtain xTin˜O(T)time and O(T)memory.
Proposition 3.2. xT= (νT, . . . , ν T−d)where νt= (g∗u)tandgis the filter whose transfer function
is1/den(ˆH)(z)and can be evaluated in ˜O(T).
Note that, the fast pre-filling algorithm established by this result requires evaluating the denominator
polynomial of ˆHfrom its roots before deployment. This is equivalent to converting the transfer function
from its factorized representation to its rational form (3.1).
Recurrent step The update rule is diagonal, thus efficiently evaluated in O(d)time and memory:
Proposition 3.3. The filter (3.2)has a state space matrices A=diag(λ1, . . . , λ d)∈Cd×d,B=
(1, . . . , 1)⊤∈Cd×1,C= (R1, . . . , R d)∈C1×dwhose step can be evaluated in O(d)time and memory.
As we generally want the output ytto be real-valued, we can simply update the complex state xt+1=
Axt+Butand then take the real part of the output, yt=R[Cxt] +h0ut.
4 Multi-head Long Convolutions
We can leverage the Hankel spectrum analysis discussed in Section 3.3 to study the dynamics of the effective
dimensionality of each convolution filter during LCSMs pre-training. We find that, at initialization, filters
correspond to high-dimensional SSMs, and gradually converge to lower-dimensional representations during
training. See Appendix E.2 for examples on HyenaandH3models.
This observation leads to the question: is it advantageous to perform independent long convolutions on
each channel, or can we reduce the total number of filters without loss in quality? To answer this, we adapt
the multi-head layer design proposed by H3[1] to Hyena[2]:
1. Giventheprojections q, k, v ∈RL×D,wesplittheminto Mchunksofsize N=D/M,qm, km, vm∈RL×N.
2. Each chunk is processed by a modified Hyenaoperator: first, we perform the outer product of kmand
vmalong the spatial dimension, zm:=km⊗vm∈RL×N×N, apply a long convolution with filter hmto
allN×Nelements independently, then compute ym
t=(hm∗zm)tqm
t,ym∈RL×Nas shown in Figure 4.
3. Finally, we compose y1, . . . , yminto a single output y∈RL×Dvia concatenation.
11Formally, this is related to low-rank approximation characteristics of the Hankel operator; rigorous bounds can be con-
structed by application of the Eckart–Young–Mirsky theorem [36].
12This strategy can also be use to evaluate the filter ˆhalternatively to the standard O(dL)method
7

--- PAGE 8 ---
vmkmqm
outer Tm
h dot
Figure4.1: Asingleheadofamulti-head Hyena.An instance of a MultiHyena is equipped with M < D dis-
tinct long convolution filters, which leads to ( a) faster distilla-
tion, with less filters to approximate, ( b) lower memory foot-
print, via a total reduction of the states to cache during gen-
eration and ( c) faster filter generation, by tying the weights
of filter parameters. We note that tying weights of key-value
projections has also been shown to be an effective technique to
reduce memory cost in Transformers [39, 40].
Crucially, the multi-head structure of MultiHyena enables us to prove favorable scaling in the associative
recallsynthetic task, which was shown in [2] to be predictive of performance at scale. In associative recall,
the model is given a sequence of key-value pairs and a query, and is tasked with matching the query to a
key in the sequence by returning its associated value. The difficulty of the task grows with the vocabulary
sizes: larger vocabularies necessitate wider models.
Theorem 4.1. The MultiHyena layer, with O(logs)heads and model size O(√slogs)can solve the
associative recall problem, where sdenotes the vocabulary size.
In Appendix E.1, we empirically verify improved scaling in vocabulary size with multiple heads.
5 Experiments
•Pretraining: We pretrain a suite of MultiHyena language models on The Pile [11], investigating scaling
of perplexity with different amounts of total tokens (5, 10, 15 billion), as well as larger training runs for
300 billion tokens. MultiHyena outperforms Transformers and Hyena.
•Distillationanalysis: Weinvestigatetherelationbetweenoptimaldistillationorders, Hankelspectrum,
and errors on the logits of distilled models.
•Post-distillation downstreams: We evaluate the downstream impact of distilling long convolutional
language models, reporting HELM [41] and LM-Eval-Harness [42] results.
•Benchmarking: We benchmark latency, throughput and memory along the different axes of batch size,
sequence length, number of generated tokens. We include base models, distilled models and equivalent
Transformers.
5.1 Pre-training
To validate the multi-head formulation, we train 150and350million parameter MultiHyena models on The
Pile [11] using 8heads and otherwise the same architecture as equivalent Hyenamodels, following the setup
of [2]. Via the multi-head structure introduced in 4, MultiHyena outperforms both Hyenaand Transformers,
including on data scaling runs with increasing numbers of tokens and full 300B tokens runs (Table 5.1).
5.2 Distillation Analysis
Next,weverifywhetherHankelsingularvaluesarepredictiveofdownstreamerrors,andwhetherlargemodels
canbedistilledwithoutlossinquality. Weapply LaughingHyena distillationtopre-trained MultiHyena ,Hyena
andH3of different sizes. Concretely, for each layer and channel of a model, we parametrize the poles {λn}of
the modal canonical forms (Section 3.2) at different orders d, and solve for each ℓ2approximation problem.
Approximation errors and spectrum We investigate the magnitude of approximation errors intro-
duced by LaughingHyena distillation. Given a pretrained MultiHyena model, we compute the errors between
original and distilled filters at each layer, averaged across channels. We repeat this process for different
distillation orders (state dimension of the model form of Section 3.2). Figure 5.2 visualizes minimum, max-
imum and average errors, per-layer errors and the distribution of the singular values of the Hankel operator
Model PPL.
GPT 9.3
Hyena 9.3
MultiHyena 8.7Model 5B 10B 15B
GPT (125M) 13.3 11.9 11.2
Hyena(153M) 13.3 11.8 11.1
MultiHyena (153M) 12.1 11.0 10.6Model 5B 10B 15B
GPT (355M) 11.4 9.8 9.1
Hyena(355M) 11.3 9.8 9.2
MultiHyena (355M) 10.6 9.4 8.9
Table 5.1: [Left]Perplexity of small models on The Pile , after pre-training for 300billion tokens. [Center and Right]
Perplexity on The Pile for models trained until a total number of tokens e.g., 5 billion (different runs for each token total).
8

--- PAGE 9 ---
1 10000 20000 30000 40000 50000−1012errorRelative ℓ1Error on Logits
d= 8
1 200 400 600 80010−1100Error Inset
d= 8
1 10000 20000 30000 40000 50000−1012
Sorted Logitserrord= 16
1 200 400 600 80010−310−210−1
Sorted Logitsd= 16Figure 5.1: Errors between logits of pretrained and distilled MultiHyena . In blue, we plot (ordered) logits, in light blue the
cumulative distribution function, and in black the relative errors. The green dotted line indicates the 99.99% percentile. As
the errors grow slowly as function of the percentiles, model outputs do not diverge from the base model.
associated to each filter. We observe distillation orders ( >16) that yield smalls errors to be predicted by
the distribution of singular values. Thus, analysis of the Hankel operator’s spectrum is verified to be an
effective approach to direct estimation of the optimal distillation order. We also note that the optimal order
changes across layers, offering options for further optimization.
Output errors Next, we compute relative ℓ1error between output logits of pre-trained and distilled
models to ensure LaughingHyena can be used in generation workloads. The optimal minimal distillation
order estimated via Hankel operators ( 16) is sufficient to keep the output distribution over the vocabulary
(>50k entries) close to the pre-trained model, as shown in Figure 5.2. Inspecting the error profile over
logits sorted by magnitude reveals our approach to be robust to different sampling strategies for generation,
including greedy decoding, top- k, top- p[43]. Indeed, the relative errors are <10−2up to and including
the99.99%percentile of the distribution, meaning e.g., a top- psampling strategy with large pcan be used
on a distilled model without drift in outputs (mis-classified tokens). We note that the relative errors are
maximum on small-norm logits, which are not required by most sampling strategies.
In Appendix D.2, we provide a similar distillation error analysis for HyenaandH3models. We find that
Hyenaand can be distilled with less than 32orders and H3with less than 8.
5.3 Downstream Evaluation
We check how distillation affects downstream performance on language benchmarks. We apply distillation
of order 8,16and32to our The Pile -pretrained MultiHyena language model and benchmark (Table 5.3)
its performance on a suite of canonical (zero shot) tasks from LM-Eval-Harness [42] and HELM [41]. The
results are consistent with our error analysis: distillation orders equal or greater to 16introduce little-to-no
quality degradation.
Model LAMBADA Winogrande PIQA HellaSwag OpenbookQA
acc acc acc acc norm. acc norm.
Pythia (160M) 32.8 53.1 61.6 31.6 29.2
MultiHyena (154M) 43.2 52.7 64.6 34.1 29.0
LaughingHyena -16 43.1 52.6 64.7 34.1 28.9
LaughingHyena -8 0.0 51.8 51.5 32.7 28.2
LaughingHyena -4 0.0 49.6 53.7 26.4 26.4
Table 5.2: Evaluation of LaughingHyena -distilled models pre and post modal distillation. We test on LM-Eval-Harness tasks,
reporting Pythia [44] performance as a Transformer baseline trained on the same data. LaughingHyena -dis aMultiHyena model
with each filter distilled of order d.
9

--- PAGE 10 ---
2123252700.10.2errorℓ1Error
212325270123errorℓ2Error
2123252701020errorℓ∞Error
202224260246·102singular valueHankel Singular Values2
3
4
5
6
8
10
12
16
20
32
64
1281357911131517
orderdepth
0.1 0.2
2
3
4
5
6
8
10
12
16
20
32
64
1281357911131517
orderdepth
0 1 2 3
2
3
4
5
6
8
10
12
16
20
32
64
1281357911131517
orderdepth
5 10 15 20
4
16
32
641357911131517
singular value indexdepth
0 25 >50Figure 5.2: Approximation error profiles (min, max, average) on the filters of MultiHyena model after distillation at different
orders. We also visualize the distribution of Hankel singular values: if the spectrum decays after nsingular values, order n
distillation yields low errors.
5.4 Benchmarking
We measure throughput, latency and memory usage of LaughingHyena for auto-regressive generation work-
loads, with initial prompt length Tand number of generated tokens K. The throughput is computed as
number of generated tokens over latency. For each setting (and additional benchmarks), we provide details
in Appendix D.4.
Peak throughput Distilled models do not need kv-caches. This reduces memory requirement during
generation, enabling higher peak throughput in large-batch workloads. We achieve 10×higher throughput
than Transformers at size 1.3billion parameters (Figure 1.1). Throughput is higher than Transformers even
at fixed batch sizes, indicating lower latency.
SSM state dimension and throughput For typical distillation orders ( <100), peak throughput is not
greatly affected. We measure a 2%reduction in throughput from 32to64.
Prompt length The throughput of LaughingHyena -distilled models is 4×larger than Transformers at
fixed batch size 64and prompt length 1536(Figure 5.3). As prompt length increases, the runtime gap
between pre-filling via convolutions in LCSMs and pre-filling in Transformers widens (e.g., ˜O(T)as detailed
in Section 3.4, compared to O(T2)).
Memory footprint Recurrent models do not require kv-caches and use constant memory for generation
of an arbitrary number of tokens (Figure 5.4).
6 Conclusion
We study the efficiency and quality of state-of-the-art long convolutional sequence models. First, we intro-
duce LaughingHyena , a novel distillation method inspired by rational function approximation and model-
order reduction techniques. LaughingHyena can be applied after training to extract compact state-space
models from each convolutional filter, without loss of quality. Distilled models achieve higher throughput
than equivalently-sized Transformers, and can perform auto-regressive generation in constant memory by
sidestepping the need to cache previous outputs. We theoretically and empirically investigate the trade-offs
of different strategies for fast inference of recurrent models, and introduce architectural improvements to
Hyenathat improve pretraining quality.
512 1,024 1,5361,0002,0003,000
Prompt LengthThroughput [tok/s]Laughing Hyena 1 .3B Hyena 1 .3B
Transformer 1 .3B H3 1.3B
Figure 5.3: Scaling in prompt length T.64 256 512102030
Generation LengthMemory [GBs]Laughing Hyena 1 .3B Hyena 1 .3B
Transformer 1 .3B H3 1.3B
Figure 5.4: Peak GPU memory for generation.
10

--- PAGE 11 ---
Acknowledgments
We would like to thank Together Computer for providing the compute used to train models in this pa-
per. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM
ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266
(Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geom-
etry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft,
NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google
Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science
Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering
Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google,
and VMWare. This work is supported by NSF (1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-
0125), ONR, DOE (DE-SC0022222), CZ Biohub, and Sloan Fellowship. The U.S. Government is authorized
to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation
thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of
the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied,
of NIH, ONR, or the U.S. Government. AR’s work is supported by NSF grant# CCF-2247014.
Broader Impact
In this work, we focus on advances related to efficient models for long sequences.
Efficiency Our distillation methods for constant-memory, high throughput inference in long convolution
sequence models (LCSMs) can lead to energy savings during model deployement, enabling processing of
longer-form content at a fraction of the cost and reducing environmental impact. Improved efficiency may
also affect other aspects of AI safety, as it may make it easier produce malicious or harmful content.
Accessibility By improving the efficiency of training and generation, LCSMsand LaughingHyena may
contribute to increased accessibility of large language models, lowering the hardware barrier to entry for
individuals and organizations with limited resources.
Steerability New method based on LCSMsenable sequence models to process long-form prompts previ-
ously inaccessible by Transformers, which may lead to increased control over models via e.g., conditioning
on additional instructions [45].
11

--- PAGE 12 ---
References
[1] Daniel Y Fu et al. “Hungry Hungry Hippos: Towards Language Modeling with State Space Models”.
In: 2023 (cit. on pp. 1–3, 7, 23).
[2] Michael Poli et al. “Hyena Hierarchy: Towards Larger Convolutional Language Models”. In: (2023).
arXiv: 2302.10866 (cit. on pp. 1, 3, 7, 8, 31, 35, 43).
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly
learning to align and translate”. In: (2014). arXiv: 1409.0473 (cit. on p. 1).
[4] AshishVaswanietal.“Attentionisallyouneed”.In: Advances in neural information processing systems
30 (2017) (cit. on p. 1).
[5] Chi-Tsong Chen. Linear system theory and design . Saunders college publishing, 1984 (cit. on pp. 1, 6,
19, 21, 22).
[6] Albert Gu, Karan Goel, and Christopher Ré. “Efficiently modeling long sequences with structured
state spaces”. In: (2021). arXiv: 2111.00396 (cit. on pp. 1, 3, 4, 6, 20, 25).
[7] Vadim Movsesovich Adamyan, Damir Zyamovich Arov, and Mark Grigor’evich Krein. “Analytic prop-
erties of Schmidt pairs for a Hankel operator and the generalized Schur–Takagi problem”. In: Matem-
aticheskii Sbornik 128.1 (1971), pp. 34–75 (cit. on pp. 2, 7, 29).
[8] Yuji Nakatsukasa, Olivier Sète, and Lloyd N Trefethen. “The AAA algorithm for rational approxima-
tion”. In: SIAM Journal on Scientific Computing 40.3 (2018), A1494–A1522 (cit. on pp. 2, 6).
[9] GRB Prony. “Essai experimental et analytique sur les lois de la dilatalrlite de fluids elastiques et sur
cells de la vapeur de l’alcool, à différents tempoeratures”. In: Journal de l’Ecole Polytechnique (París)
1 (1795), pp. 24–76 (cit. on pp. 2, 6).
[10] AlecRadfordetal.“Languagemodelsareunsupervisedmultitasklearners”.In: OpenAI blog 1.8(2019),
p. 9 (cit. on p. 2).
[11] Leo Gao et al. “The pile: An 800gb dataset of diverse text for language modeling”. In: (2020). arXiv:
2101.00027 (cit. on pp. 2, 8, 35).
[12] Albert Gu et al. “Hippo: Recurrent memory with optimal polynomial projections”. In: Advances in
Neural Information Processing Systems 33 (2020), pp. 1474–1487 (cit. on pp. 3, 25).
[13] David W Romero et al. “Ckconv: Continuous kernel convolution for sequential data”. In: (2021). arXiv:
2102.02611 (cit. on p. 3).
[14] David W Romero et al. “Flexconv: Continuous kernel convolutions with differentiable kernel sizes”. In:
(2021). arXiv: 2110.08059 (cit. on p. 3).
[15] Vincent Sitzmann et al. “Implicit neural representations with periodic activation functions”. In: Ad-
vances in neural information processing systems 33 (2020), pp. 7462–7473 (cit. on p. 3).
[16] Rizal Fathony et al. “Multiplicative filter networks”. In: International Conference on Learning Repre-
sentations . 2020 (cit. on p. 3).
[17] Daniel Y. Fu et al. “Simple Hardware-Efficient Long Convolutions for Sequence Modeling”. In: Inter-
national Conference on Machine Learning (2023) (cit. on pp. 3, 25).
[18] Kemin Zhou and John Comstock Doyle. Essentials of robust control . Vol. 104. Prentice hall Upper
Saddle River, NJ, 1998 (cit. on p. 4).
[19] Athanasios C Antoulas. Approximation of large-scale dynamical systems . SIAM, 2005 (cit. on pp. 4,
7, 29).
[20] Wilhelmus HA Schilders, Henk A Van der Vorst, and Joost Rommes. Model order reduction: theory,
research aspects and applications . Vol. 13. Springer, 2008 (cit. on p. 4).
[21] Sun-Yuan Kung. “A new identification and model reduction algorithm via singular value decomposi-
tion”. In: Proc. 12th Asilomar Conf. on Circuits, Systems and Computer . 1978, pp. 705–714 (cit. on
p. 4).
[22] D Friedman. “On approximating an FIR filter using discrete orthonormal exponentials”. In: IEEE
Transactions on Acoustics, Speech, and Signal Processing 29.4 (1981), pp. 923–926 (cit. on p. 4).
[23] J Bednar. “On the approximation of FIR by IIR digital filters”. In: IEEE Transactions on Acoustics,
Speech, and Signal Processing 31.1 (1983), pp. 28–34 (cit. on p. 4).
12

--- PAGE 13 ---
[24] Bartlomiej Beliczynski, Izzet Kale, and Gerald D Cain. “Approximation of FIR by IIR digital filters:
An algorithm based on balanced model reduction”. In: IEEE Transactions on Signal Processing 40.3
(1992), pp. 532–542 (cit. on pp. 4, 43, 44).
[25] William Henry Young. “On the multiplication of successions of Fourier constants”. In: Proceedings of
the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character
87.596 (1912), pp. 331–339 (cit. on p. 5).
[26] William Beckner. “Inequalities in Fourier analysis on Rn”. In: Proceedings of the National Academy of
Sciences 72.2 (1975), pp. 638–641 (cit. on p. 5).
[27] John Fournier. “Sharpness in Young’s inequality for convolution”. In: Pacific Journal of Mathematics
72.2 (1977), pp. 383–397 (cit. on p. 5).
[28] Tong S Quek and Leonard YH Yap. “Sharpness of Young’s inequality for convolution”. In: Mathematica
Scandinavica 53.2 (1983), pp. 221–237 (cit. on p. 5).
[29] Jean-Paul Berrut and Lloyd N Trefethen. “Barycentric lagrange interpolation”. In: SIAM review 46.3
(2004), pp. 501–517 (cit. on p. 6).
[30] MarcosVicenteMoreiraandJoãoCarlosBasilio.“FairandSquareComputationofInverseZ-Transforms
of Rational Functions”. In: IEEE Transactions on Education 55.2 (2011), pp. 285–290 (cit. on p. 6).
[31] HenriPadé. “Surla représentation approchéed’une fonctionpar desfractions rationnelles”. In: Annales
scientifiques de l’Ecole normale supérieure . Vol. 9. 1892, pp. 3–93 (cit. on pp. 6, 27).
[32] Ankit Gupta, Albert Gu, and Jonathan Berant. “Diagonal state spaces are as effective as structured
state spaces”. In: Advances in Neural Information Processing Systems 35 (2022), pp. 22982–22994
(cit. on pp. 6, 25).
[33] Antonio Orvieto et al. “Resurrecting Recurrent Neural Networks for Long Sequences”. In: (2023).
arXiv: 2303.06349 (cit. on pp. 6, 25, 26).
[34] Jeffrey M Hokanson. “A data-driven McMillan degree lower bound”. In: SIAM Journal on Scientific
Computing 42.5 (2020), A3447–A3461 (cit. on p. 6).
[35] L Ho and Rudolf E Kalman. “Effective construction of linear state-variable models from input/output
functions”. In: at-Automatisierungstechnik 14.1-12 (1966), pp. 545–548 (cit. on p. 6).
[36] Carl Eckart and Gale Young. “The approximation of one matrix by another of lower rank”. In: Psy-
chometrika 1.3 (1936), pp. 211–218 (cit. on p. 7).
[37] Guy E Blelloch. “Prefix sums and their applications”. In: (1990) (cit. on p. 7).
[38] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. “Simplified state space layers for
sequence modeling”. In: (2022). arXiv: 2208.04933 (cit. on p. 7).
[39] Noam Shazeer. “Fast transformer decoding: One write-head is all you need”. In: (2019). arXiv: 1911.
02150(cit. on p. 8).
[40] Joshua Ainslie et al. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
Checkpoints”. In: (2023). arXiv: 2305.13245 (cit. on p. 8).
[41] Percy Liang et al. “Holistic evaluation of language models”. In: (2022). arXiv: 2211.09110 (cit. on
pp. 8, 9, 36).
[42] Leo Gao et al. A framework for few-shot language model evaluation . Version v0.0.1. Sept. 2021. doi:
10.5281/zenodo.5371628 .url:https://doi.org/10.5281/zenodo.5371628 (cit. on pp. 8, 9, 36).
[43] Ari Holtzman et al. “The curious case of neural text degeneration”. In: (2019). arXiv: 1904.09751
(cit. on p. 9).
[44] StellaBidermanetal.“Pythia:Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling”.
In: (2023). arXiv: 2304.01373 (cit. on pp. 9, 36).
[45] Yuntao Bai et al. “Constitutional AI: Harmlessness from AI Feedback”. In: (2022). arXiv: 2212.08073
(cit. on p. 11).
[46] Alan V Oppenheim. Discrete-time signal processing . Pearson Education India, 1999 (cit. on p. 21).
[47] Lennart Ljung. System identification . Springer, 1998 (cit. on p. 21).
[48] RP Guidorzi. “Certain models from uncertain data: the algebraic case”. In: Systems & control letters
17.6 (1991), pp. 415–424 (cit. on p. 21).
[49] Irwin W Sandberg. “On the theory of linear multi-loop feedback systems”. In: Bell System Technical
Journal 42.2 (1963), pp. 355–382 (cit. on p. 23).
13

--- PAGE 14 ---
[50] Michael Zhang et al. “Effectively Modeling Time Series with Simple Discrete State Spaces”. In: (2023).
arXiv: 2303.09489 (cit. on p. 25).
[51] Luca Perotti and Michał Wojtylak. “Matrix methods for Padé approximation: Numerical calculation
of poles, zeros and residues”. In: Linear Algebra and its Applications 548 (2018), pp. 95–122 (cit. on
p. 27).
[52] Amer Abu-Omar and Fuad Kittaneh. “Estimates for the numerical radius and the spectral radius of
the Frobenius companion matrix and bounds for the zeros of polynomials”. In: Annals of Functional
Analysis 5.1 (2014), pp. 56–62 (cit. on p. 27).
[53] Julia Eaton et al. “Polynomial root radius optimization with affine constraints”. In: Mathematical
Programming 165 (2017), pp. 509–528 (cit. on p. 27).
[54] Gerlind Plonka and Vlada Pototskaia. “Application of the AAK theory for sparse approximation of
exponential sums”. In: (2016). arXiv: 1609.09603 (cit. on p. 29).
[55] Jimmy Ba et al. “Using fast weights to attend to the recent past”. In: Advances in neural information
processing systems 29 (2016) (cit. on p. 31).
[56] William B Johnson. “Extensions of Lipschitz mappings into a Hilbert space”. In: Contemp. Math. 26
(1984), pp. 189–206 (cit. on p. 31).
[57] Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of random-
ized algorithms . Cambridge University Press, 2009 (cit. on p. 34).
[58] Ilya Loshchilov and Frank Hutter. “Decoupled weight decay regularization”. In: (2017). arXiv: 1711.
05101(cit. on p. 35).
[59] Dale F. Enns. “Model reduction with balanced realizations: An error bound and a frequency weighted
generalization”. In: The 23rd IEEE Conference on Decision and Control (1984), pp. 127–132 (cit. on
p. 43).
14

--- PAGE 15 ---
Laughing Hyena Distillery
Supplementary Material
Contents
1 Introduction 1
2 Preliminaries and Related Work 2
2.1 Long Convolution Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Auto-Regressive Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 The Laughing Hyena Distillery 4
3.1Data-Free Distillation Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Making HyenaLaugh with Modal Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.3 Minimal Distillation Orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.4 Deploying the Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Multi-head Long Convolutions 7
5 Experiments 8
5.1 Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.2 Distillation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.3 Downstream Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5.4 Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6 Conclusion 10
A Linear Systems 17
A.1 Extended Notation and System Theory Preliminaries . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 Systems Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 Transfer Function of State-Space Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.4 Truncated Transfer Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.5 From Transfer Function to State-Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.5.1 Isolating the h0-term from Transfer Function by Long division . . . . . . . . . . . . . 21
A.5.2 Construction of the State-Space from the Transfer Function . . . . . . . . . . . . . . . 22
A.6 From State-Space to Transfer Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.7 State-Space Representation of Truncated Filters. . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.8 Efficient Computation of State-Space Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.8.1 Fast Evaluation of the Transfer Function . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.8.2 Fast Companion Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.8.3 Canonization of State-Space Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
BLaughingHyena : Further Details 26
B.1 Parametrization of Modal Interpolators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.2 Distillation as Rational Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C Proofs 28
C.1 Proof of Lemma 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2 Proof of Lemma 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.3 Proof of Lemma 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.4 Proof of Proposition 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.5 Proof of Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.6 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.7 Proof of Proposition 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
15

--- PAGE 16 ---
C.8 Proof of Proposition 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.9 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D Experimental Details 35
D.1 Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D.2 Distillation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D.2.1 Pretrained Filters: Effective Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D.3 Downstream Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D.4 Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E Additional Experiments 43
E.1 Associative Recall with MultiHyena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
E.2 Analysis of Hankel Singular Values of Pretrained Large Convolution Sequence Models . . . . 43
E.3 Model Order Reduction of H3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
E.3.1 Modal Truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
E.3.2 Balanced Truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Authors Contribution
S.M. Conceptualized the research; coordinated collaborations;
lead theory development; conducted distillation experiments.
M.P. Conceptualized the research; coordinated collaborations;
lead the experimental (model pre-training, distillation, benchmarks,
downstream evaluation) efforts; coordinated writing and conference submission;
optimized inference stack.
D.Y.F. Assisted in development of MultiHyena ; assisted in pre-training and
subsequent benchmarking of distilled models; assisted in writing.
H.K. Developed benchmarking suite and interpreted results;
assisted in writing.
R.N.P. Assisted in theory and algorithmic development;
performed model-order reduction experiments of H3models;
assisted in writing.
A.T. Conceived and proved Theorem 4.1;
assisted in writing.
D.W.R. Assisted in Hankel operator spectral analysis;
Assisted in writing.
Q.M. Assisted in theory development.
B.C. Supervised development of benchmarking suite and model deployment.
A.R. Supervised theory development (solving associative recall
with MultiHyena , Th. 4.1).
C.Z. Supervised research; secured compute resources.
C.R. Supervised research; reviewed manuscript;
secured compute resources.
S.E. Supervised research; reviewed manuscript.
Y.B. Supervised research; reviewed manuscript.
Stefano Massaroli, Michael Poli, and Dan Fu contributed equally to this work. Christopher Ré, Stefano
Ermon, and Yoshua Bengio share equal senior authorship.
All authors read and approved the final manuscript.
16

--- PAGE 17 ---
A Linear Systems
A.1 Extended Notation and System Theory Preliminaries
We first introduce the notation and some mathematical concepts that will be used throughout the paper.
ByZwe denote the set of integers, by Rthe set of reals, and by Cthe set of complex numbers. The variable
tstands for time.ℓp(Z)denotes the Banach space of complex-valued sequences (xt)t∈Zwith finite energy,
i.e.∥x∥p:= [P
t∈Z|xt|p]1/p<∞for some 1≤p <∞.ℓ∞(Z)is instead is the space of sequences for which
∥x∥∞:= supt∈Z|xt|<∞. With Sdenoting the unit circle in the complex plane, S:={z∈C:|z|= 1}we
define Hp(S)as the space of functions XfromCto itself such that ∥X∥p:= [(1 /2π)Rπ
−π|X(eiω)|pdω]1/p<∞
andH∞(S)the space for which ∥X∥∞:= supz∈S|X(z)|<∞. Particularly, K2(S)is a Hilbert space with
inner product ⟨X, Y⟩:= (1/2π)Rπ
−πX(eiω)Y∗(eiω)dωwhere “ ∗” denotes complex conjugation. Although we
acknowledge we are using the same notation for norms in both ℓp(Z)andHp(S), the correct meaning will
always be made clear by the context. The Z-transform of a sequence x= (xt)t∈ZisX(z) =Z[x](z):=P
t∈Zxtz−t. We embrace the system theory convention of using capital letters to identify transformed
sequences. The Z-transform is a projection of the sequence onto a basis of powers et=r−teiωt. This
basis is not orthogonal unless r= 1. That is the basis of the discrete-time Fourier transform F. Hence,
Fis defined as F[x](eiω) =X(eiω):=P
t∈Zxte−iωt. The discrete-time Fourier transform is an isometric
isomorphism between ℓ2(Z)andL2(S). We say that sequences live in the time domain and their Z(orF)
transforms in the frequency domain .
Alinear system is a linear operator transforming an input sequence uto an output sequence y. If the
sequences have continuous support, i.e. tranges over a continuous set (e.g. R), we have a continuous-time
system. Conversely, if the sequences have discrete support, i.e. tranges over a discrete set (e.g. Z), we
have adiscrete-time ordigitalsystem.In this manuscript we restrict ourselves to discrete-time
systems . Systems can be single-input single-output (SISO) if uandyare scalar functions or multi-input
multi-output if either uoryare vector-valued. We limit our discussion to SISO systems . Theimpulse
response of a system is the output sequence ywhen the input sequence uis the Kronecker delta function δt
and is usually denoted by the letter h. The values htof the impulse response sequence are also known as the
Markov parameters of the system. The most common mathematical representation of a linear system is its
convolution form: y=h∗u, i.e. yt=P
j∈Zht−juj=P
j∈Zhjut−j, t∈Z. In matrix form the input-output
relation is given by the Toeplitz operator Thcorresponding to the (possibly infinitely long) sequence h, i.e.
y=Thu. Taking the Z-transforms, we can write the input-output relation as Y(z) =H(z)U(z)(this is
just the Fourier convolution theorem extended outside the unit circle). H(z)is called the transfer function
of the system. When z=eiω,H(eiω)is just the discrete-time Fourier transform of hwhich is called the
frequency response of the system. A linear system is causalifht= 0fort <0. A system is called stableif
theThis a bounded operator. If u, y∈ℓ2(Z), then stability implies h∈ℓ∞.In the following, we mainly
focus on causal stable systems .
A.2 Systems Norms
When quantitatively characterizing linear systems, several norms play a crucial role. These norms provide
measures of various characteristics of the systems, which are essential in both analysis and filter design.
The ℓ2andH2norms As defined above, the ℓ2norm represents the energyof a signal h,
∥h∥2:=X
t∈Z|h|2
t1/2
whileH2is the energy of the (continuous) spectrum of h,
∥H∥2:=1
2πZπ
−π|X(eiω)|2dω]1/2
By Parseval’s theorem, the ℓ2andH2norms are equal, ∥h∥2=∥H∥2. Further these norms are useful to
study the approximation of convolutional filter. The following holds:
Lemma A.1 (ℓ∞output error) .Consider the class of ℓ2measurable inputs such that ∥u∥2≤ζ, then
for all H,ˆH∈ H 2,
∥y−ˆy∥∞≤ζ∥H−ˆH∥2
17

--- PAGE 18 ---
Proof.
sup
t>0|yt−ˆyt|= sup
t>01
2πZπ
−πh
Y(eiω)−ˆY(eiω)i
eiωtdω
≤1
2πZπ
−π|Y(eiω)−ˆY(eiω)|dω
=1
2πZπ
π|H(eiω)−ˆH(eiω)|U(eiω)|dω
≤1
2πZπ
−π|H(eiω)−ˆH(eiω)|2dω1/21
2πZπ
−π|U(eiω)|2dω1/2
Hölder Inequality
≤1
2πZπ
−π|H(eiω)−ˆH(eiω)|2dω1/2
∥u∥2 Parseval Theorem
≤ζ∥H−ˆH∥H2
Ifuis the unit impulse function ut=δtthen ζ= 1. The results also holds for finite sequences of length
Lusing the discrete Fourier transform.
Lemma A.2 (Impulse response error on finite sequences) .Consider filters h,ˆhwith finite length L.
Then, the following holds.
∥h−ˆh∥∞≤ ∥H−ˆH∥2
where Hand ˆHdenote the discrete Fourier transforms of handˆh, respectively.
Proof.
∥y−ˆy∥∞:= sup
t>0|yt−ˆyt|= sup
t>01
2πL−1X
n=0h
Yn−ˆYni
ei2πnt/L
≤1
2πL−1X
n=0|Yn−ˆYn|
=1
2πL−1X
n=0|Hn−ˆHn||Un|
≤"
1
2πL−1X
n=0(Hn−ˆHn)2#1/2"
1
2πL−1X
n=0U2
n#1/2
Hölder Inequality
≤"
1
2πL−1X
n=0(Hn−ˆHn)2#1/2
∥u∥2 Parseval Theorem
=∥H−ˆH∥2 using∥u∥2= 1
A.3 Transfer Function of State-Space Models
The transfer function (3.1) is derived by taking the z-transform of input and state, U(z) =Z[u](z), X(z) =
Z[x](z). Plugging U(z), X(z)in the state equation (2.2), it holds
zX(z) =AX(z) +BU(z)⇔X(z) = (zI−A)−1BU(z)
Substituting in the output equation yields
Y(z) =C(zI−A)−1BU(z) +h0U(z)
The transfer function is then defined as
H(z) =Y(z)
U(z)=C(zI−A)−1B+h0. (A.1)
18

--- PAGE 19 ---
Alternative derivation The transfer function can also be derived by direct z-transform of the impulse
response htof the system. This derivation is useful to highlight the region of convergence of the transfer
function.
H(z) =h0+∞X
t=1z−tCAt−1B h0is pulled out via h0z0=h0
=h0+C"∞X
t=1z−tAt−1#
B multiplication distributes over sum.
=h0+z−1C"∞X
t=1z−(t−1)At−1#
Bmultiply by z/z
=h0+z−1C"∞X
t=0(z−1A)t#
Bchange of index and collect like terms(A.2)
We look at the convergence of the seriesP∞
t=0∥z−1A∥t
2. We have
∥z−1A∥2≤ ∥z−1∥2∥A∥2
=∥r−1e−iω∥2∥A∥2using z:=reiω∈C, r, ω∈R
≤r−1∥A∥2=r−1ρ(A)
The series converges to 1/(1−r−1ρ(A))if and only if r−1ρ(A)<1i.e. for r > ρ (A). Thus, in the exterior
of the disk with radius ρ(A),Dρ(A):={z∈C:|z|> ρ(A)},P∞
t=0(z−1A)tconverges to (I−z−1A)−1and
z∈Dρ(A)⇒H(z) =h0+z−1C(I−z−1A)−1B=h0+C(zI−A)−1B
The transfer function H(z) =h0+C(zI−A)−1Bof a stable lumped discrete-time system is defined outside
the disc in the complex plane that encloses all the eigenvalues of A.
Invariance of the transfer function H(z)as defined in (A.1) is a proper13rational function of z. In
caseh0= 0,H(z)is strictly proper and the denominator is monic:
H(z) =b1z−1+···+bdz−d
1 +a1z−1+···+adz−d(A.3)
Specifically, the denominator could be derived from Awith det(zI−A), and the numerator is det(zI−A+
BC) +det(zI−A). We provide a detailed derivation below in Section A.6. While state-space representation
involves the analysis and synthesis of model matrices A,B,C, the transfer function is entirely characterized
by the coefficients a= (an)d
n=1, b= (bn)d
n=1of numerator and denominator polynomials. Notably, the
transfer function is an invariant of the system: if we apply a change of variables to the state, the transfer
function remains unchanged.
Lemma A.3. Coefficients a, bareinvariant under any invertible change of variables.
Proof.The proof can be found in [5, pp.95] and follows from the definition of equivalence transformation .
Consider the state-space matrices of under change of variables ˆx=Kx,
ˆA=KAK−1,ˆB=KB,ˆC=CK−1,ˆh0=h0.
The resulting transfer function H(z)can then be computed as
ˆH(z) =ˆC(zI−ˆA)−1ˆB+ˆh0=CK−1[K(zI−A)K−1]−1KB+h0=H(z)
A.4 Truncated Transfer Functions
In the case of generic truncated (finite) impulse response filters, such that ht= 0for all tgreater than a
certain value L(which we refer to as the lengthof the filter), the transfer function is simply a polynomial
in the complex variable zof order L, i.e.
H(z) =∞X
t=0htz−t=LX
t=0htz−t=h0+h1z−1+···+hLz−L(A.4)
13i.e. such that the denominator’s order is not less than the numerator’s one.
19

--- PAGE 20 ---
In case the filter is generated by a finite dimensional (lumped parameters) system, i.e. ht=CAt−1B
t= 1, . . . , L, then (A.4) can still be represented exactly by a rational function of order d.
Lemma A.4 (Truncated rational transfer functions) .Consider the L-truncated impulse response
ht∈ℓ2(N)of a lumped-parameter filter (A,B,C, h0),
ht=

h0 t= 0
CAt−1B1≤t≤L
0 t > L.
Then its truncated transfer function is
HL(z) =Z{h}(z) =h0+C(I−z−LAL)(zI−A)−1B
Proof.By definition of z-transform we have
HT(z) =∞X
t=0htz−t=h0+LX
t=1z−tCAt−1B
=h0+C"LX
t=1z−tAt−1#
B=h0+z−1C"L−1X
t=0(z−1A)t#
B(A.5)
The sumPL−1
t=0(z−1A)tis a partial Neumann series and can be manipulated as follows.
L−1X
t=0(z−1A)t(I−z−1A) =L−1X
t=0(z−1A)t−L−1X
t=0(z−1A)t+1
=I−(z−1A)L.
Thus,
L−1X
t=0(z−1A)t= (I−z−LAL)(I−z−1A)−1,
which plugged in (A.5) gives HL(z) =h0+C(I−z−LAL)(zI−A)−1B, proving the result.
Because of truncation, evaluating the transfer function HL(z)on the Lroots of unity z=eiωk,wk=
2πk/Tfork= 0, . . . Lgives the length- Ldiscrete Fourier transform (DFT) of the filter:
¯Hk:=HL(eiωk) =L−1X
t=0hte−i2πk/L, k = 0, . . . , L −1.
In practice, this means that ¯H∈CLis the FFTofh,¯H=FFT L[h]. If we can find an efficient and stable
algorithm to evaluate ¯Hfrom the system matrices (A,B,C, h0), then the FFT-based convolution of truncated
filter with an input sequence u∈RLcan be evaluated in ˜O(L)time.
Reparametrization Assume training a LCSMequipped with SSM filters with input/target sequences to
be all of length L(smaller sequences can be padded with zeros to the maximum length). Thus, for training
purposes, we are only interested in evaluating ¯Hfor the FFT-based convolution.
The truncated transfer function HLis equal to the original one with a correction term I−z−LALon the
numerator polynomial. As already noted in S4[6],z−Lis conveniently equal to one on the roots of unity,
ziωkL=e−i2πk= 1for all k= 0, . . . , L −1. Hence, the correction term due to truncation becomes constant:
Hk=C(I−AL)(exp( −i2πk/L )I−A)−1B; in DFT domain the truncated filter behaves as the infinitely long
one with a perturbed Cmatrix
¯C=C−CAL
If –as assumed– the SSM is stable ρ(A)<1,(i)the transfer function is defined on the unit circle, term
CALwill go to zero exponentially fast as L→ ∞and¯C=C(as expected). As advised in [6], it is desirable
to parametrize directly ¯C; the expensive computation of the correction term C(I−AL)is never carried
out during training. Instead, the real Cmatrix can be retrieved for recurrent inference by inverting the
correction term C=¯C(I−AL)−1, always invertible for stable systems although possibly ill conditioned by
eigenvalues too close to the stability margin (the unit circle).
20

--- PAGE 21 ---
A.5 From Transfer Function to State-Space
Suppose the coefficients of the numerator and denominator polynomials of a proper transfer function His
given:
H(z) =b0+b1z−1+···+bdz−d
1 +a1z−1+···+adz−d. (A.6)
A state-space representation of the form (2.2) can be rapidly realized in two steps:
1.Get delay-free path From (A.6) we first notice that the biasterm h0ish0=b0. We thus want
to isolate b0from the rest of the numerator. This can be obtained via long division (see §A.5.1) and
results in
H(z) =β1z−1+···+βNz−d
1 +a1z−1+···+adz−d+b0, β n=bn−b0an (A.7)
2.Get state-space matrices Given the transfer function H(z)with the isolated pass-through coeffient
b0as in (A.7), we can construct the state-space matrices by companion canonical realization:
AB
Ch0
=
−a1−a2··· − ad−1−ad
1 0 ··· 0 0
0 1 ··· 0 0
...............
0 0 ··· 1 01
0
0
...
0
β1β2··· βd−1βdb0
(A.8)
Details on the complete a la[5] derivation can be found in §A.5.2. A linear system with finite-dimensional
state can be equivalently characterized: by its state-space matrices (A,B,C, h0), by its impulse response
function h, or by the coefficients a, b(orβ) of the transfer function. A fourth representation is its linear-
constant-coefficients difference equation form
yt=dX
j=0bjut−j−dX
n=1ajyt−j,
typically used in signal processing literature in the theory of infinite impulse response filters (see [46]) and
known, in the context of system identification of error-in-variables models, as auto-regressive moving-average
filters [47, 48].
A.5.1 Isolating the h0-term from Transfer Function by Long division
If the rational transfer function H(z)accounts for the h0term, then it is simply proper (order of numerator
equals the order of denominator), h0is necessarily h0=b0(thedelay-free path). Given the transfer function
in this form, we can isolate the b0term and the strictly rational term of (A.3) by long division. We start
by expanding the fraction as
H(z) =q(z)
p(z)=b0
p(z)+b1z−1+···+bdz−d
p(z).
and
b0
p(z)=b0zd
zd+a1zd−1+···+ad
We then use the long division method to compute b0/p(z):
b0
zd+a1zd−1+···+ad))b0zd
b0zd+b0a1zd−1+···+b0ad
−b0a1zd−1− ··· − b0ad(reminder)
to finally get
H(z) =b0−b0a1zd−1+···+b0ad
zd+a1zd−1+···+ad+b1z−1+···+bdz−d
p(z)
=b0+(b1−b0a1)z−1+···+ (bd−b0ad)z−d
1 +a1z−1+···+adz−d
21

--- PAGE 22 ---
Note that the coefficients bnin (A.3) correspond to bn−b0anin (A.6), bn←bn−b0an. It is indifferent
to parameterize the coefficients of the transfer function in either forms. However, if we choose the simply
proper representation (A.6), we need to apply the derived correction factor to the numerator coefficients
when we separate the h0term and strictly proper part of H(z).
A.5.2 Construction of the State-Space from the Transfer Function
Chen’s derivation The derivation is based on the steps reported for the continuous-time multi-input
multi-output case in [5]. First, we define a pseudo-state vsuch that
p(z)V(z) =U(z)⇔ V(z) =1
p(z)U(z). (A.9)
Then, we define the state xt:= (x1
t, . . . , xd
t)∈Rdas
xt= (vt−1, vt−2,···, vt−d)⇔ Z{ x}(z) =X(z) =
z−1
...
z−d
V(z). (A.10)
From (A.9) we have
V(z) +a1z−1V(z) +···+adz−dV(z) =U(z)⇔
V(z) =−a1z−1V(z)− ··· − adz−dV(z) +U(z)⇔
vt=−a1vt−1− ··· − advt−d+ut⇔time-delay prop. of Z-transform
x1
t+1=−a1x1
t− ··· − adxd
t+ut⇔by def. of state (A.10) .
Thus, we have the overall recurrence
x1
t+1=−a1x1
t− ··· − adxd
t+ut
x2
t+1=x1
t
...
xd
t+1=xd−1
t
which can be written in matrix form as
xt+1=
−a1−a2··· − aN
1 0 ··· 0
0 1 ··· 0
............
0 0 ··· 1
xt+
1
0
...
0
0
ut
The output spectrum is then given by
Y(z) =H(z)U(z) =q(z)
p(z)U(z) +b0U(z)
=q(z)V(z) +b0U(z) by def. of V(z).
Therefore,
Y(z) =q(z)V(z) +b0U(z) =β1β2··· βN
z−1
z−2
...
z−d
V(z) +b0U(z)
=β1β2··· βd
X(z) +b0U(z)
and the output equation in time-domain is given by
yt=β1β2··· βd
xt+b0ut.
yielding state-space matrices (A.8).
22

--- PAGE 23 ---
A.6 From State-Space to Transfer Function
We detail an implementation oriented method to compute the coefficients (an)d
n=1,(bn)d
n=0of a SSM’s
transfer function. Recall that
H(z) =C[zI−A]−1B+h0=C Adj (zI−A)B+det(zI−A)h0
det(zI−A)(A.11)
Hence, the denominator coefficients (an)d
n=1are simply the coefficients of the characteristic polynomial
of matrix A. They can be easily obtained by 1. computing the eigenvalues of Aand 2. calculating the
coefficientsofthepolynomialwhoserootsaresucheigenvalues. Ontheotherhand, thenumeratorapparently
involves more complex symbolic manipulation. This can be simplified recalling a classic matrix-determinant
identity:
Lemma A.5 ([49]).LetM,B, and Crespectively denote matrices of orders d×d,d×1, and 1×d.
Then,
det(M+BC) =det(M) +C Adj (M)B.
Applying Lemma A.5 to (A.11) we obtain
H(z) =det(zI−A+BC) +det(zI−A)(h0−1)
det(zI−A).
Letpoly(r)denote the coefficients of the polynomials with roots r= (r1, . . . , r d). Then a=poly(eig(A)).
Since AandA−BCare of equal dimension, their characteristic polynomials have equal order and therefore
b=poly(eig(A−BC)) + poly(eig(A))(h0−1)
Listing 1: State-space →transfer function conversion code
def get_tf_from _ss(A,B,C,h0):
a = poly(eig(A))
b = poly(eig(A −outer(B,C))) + (h0 −1)*a
return a, b
A.7 State-Space Representation of Truncated Filters.
A truncated filter h0, . . . , h L– as the ones found in any standard convolutional neural network – can be
representedbya L-dimensionalcompanioncanonicalSSM.Thefilter’stransferfunction H(z) =h0+h1z−1+
···+hLz−Lis polynomial, i.e. a rational function with the denominator’s coefficients set to zero. Following
the canonical realization process detailed in Section A.5, the truncated filter has state-space form:
xt+1=
0 0 ··· 0 0
1 0 ··· 0 0
0 1 ··· 0 0
...............
0 0 ··· 1 0
xt+
1
0
...
0
0
ut
yt=
h1h2··· hL
xt+h0ut.
Ifx0=0Landut= 0for negative t, then at each t >0the state is a shifted copy of the input sequence
xt= (ut−1, . . . , u t−L)∈RL. Nonetheless, the asymptotic complexity of computing one recurrent step is
O(L)as it requires only a shift operation and a length- Ldot product
x1
t+1=ut
x2:L
t+1=shift(xt)
yt=⟨h1:L, xt⟩+h0ut.(A.12)
The memory footprint is also O(L). In [1] it is proposed the use of shift-type SSMs to parametrize one of
the filters of the H3block.
23

--- PAGE 24 ---
A.8 Efficient Computation of State-Space Models
A.8.1 Fast Evaluation of the Transfer Function
Computing H(z)at any point z∈Cconcerns the evaluation of the d-order polynomial of numerator and
denominator,
H(z) =q(z)
p(z)=Pd
n=1bnz−n
1 +Pd
n=1anz−n
In practice, we are mainly interested in a fast algorithm that allows computing Hon the Lroots of unity
to obtain the DFT of the filter. The DFT of the filter can be then readily used to perform a FFT-based
convolution with a length- Linput sequence uor to recover the impulse response function via inverse DFT.
We prove the following:
Lemma A.6. Given the coefficients a,bof the transfer function, the frequency and impulse response
of the filter can be evaluated in ˜O(L)time.
Proof.The result is proven showing that the transfer function can be evaluated in ˜O(L)time on the L
roots of unity. The fastest method to evaluate polynomials on Larbitrary points zof the complex plane is
generally the Horner’s scheme. This method is based on a sequence of nested multiplications and computes
the polynomial from its vector of coefficients, delivering a time complexity of O(dL). More explicitly,
Horner’s scheme determines p(z)asp(z) = ((···((adz−1+ad−1)z−1+ad−2)···)z−1+a2)z−1+a1)z−1+ 1.
Each step involves a multiplication and an addition, making a total of 2doperations per evaluation point.
Thus, for Lpoints, the total number of operations amounts to O(dL).
Effectively, Horner’s approach implements the matrix-vector product of an L-by-(d+ 1)Vandermonde
matrix V∈CL×(d+1)constructed by Levaluation points (z0, . . . , z L−1)with the vector of coefficients
a= (1, a1, . . . , a d)⊤:
p(z0)
p(z1)
...
p(zL−1)
=
1z−1
0 z−2
0··· z−d
0
1z−1
1 z−2
1··· z−d
1...............
1z−1
L−1z−2
L−1··· z−d
L−1

1
a1
...
ad
=Va
Significantly, if the polynomial is required to be evaluated at the roots of unity, the Vandermonde matrix
simplifies corresponds to the L×(d+ 1)DFT matrix. Further, zero-padding the coefficient vector to length
L, enables the use a single length- LFFTto compute the matrix-vector product in ˜O(L)time. Thus, the
numerator and denominator polynomials of the transfer function can be evaluated, on the roots of unity,
in˜O(L)time by taking the FFTof the padded numerator / denominator coefficients a, band subsequently
dividing element-wise the two sequences as FFT L[b]/FFT L[a]. The overall time complexity to obtain the
impulse response is also ˜O(L)since hcan be recovered taking an inverse FFTof the frequency response.
A.8.2 Fast Companion Recurrence
The recurrent step of a generic SSM (2.2) with dense system matrices usually requires O(d2)operations due
to the matrix-vector product Axt. We show how the recurrence of SSMs in companion canonical form , i.e.
with system’s matrices (A.8), requires only O(d)operations.
Lemma A.7. The recurrent step of a state-space model in companion canonical form (A.8)can be
evaluated in O(d)time and memory.
Proof.The companion state matrix Acan be broken down into a lower shift matrix LNand a low-rank
term. Particularly, with e1the first element of the canonical basis of RNandα= (a1, . . . , a N), we have
A=LN−e1⊗α.
It follows that the recurrent update can be simplified to
xt+1= (LN−e1⊗α)xt+But
yt=Cxt+b0ut
The peculiarity of this formulation is that we never need to construct the matrices to perform the recurrence.
In particular we have:
x1
t+1=ut−α⊤xt
x2:N
t+1=shift(xt)
yt=β⊤xt+b0ut
24

--- PAGE 25 ---
Thus, eachsteponlyrequirestwoinnerproducts( dmultiplicationsand dsumseach)andoneshiftoperation,
totaling O(d)operations.
The proof of Lemma A.7 yields the practical implementation of the recurrence:
Listing 2: Python implementation of the companion canonical recurrence
def step(x, u, alpha, beta, b0):
y = dot(beta, x) + b0 *u
lr = u −dot(alpha, x)
x = roll(x)
x[0] = lr
return x, y
A.8.3 Canonization of State-Space Models
The companion canonical form discussed in Section A.5 is the ideal representation to deploy SSM-based
convolutional layers: i)it comes with a O(d)fast recurrence and ii)allows to swiftly switch between time
and frequency domains with a direct mapping between state-space matrices and coefficient of the transfer
function (which in turn allow ˜O(L)fast convolutions).
Aside from [50], which directly parametrizes S4layers in companion canonical form, all the other pa-
rameterizations [12, 6, 32, 17, 33] can be converted (canonized ), under mild assumptions.
Lemma A.8 (Canonization of SSMs) .Any state-space model (2.2)with proper transfer function can
be converted in companion canonical form.
Proof.The result can be proved following the two-step conversion process.
1.Getthecoefficientsofthetransferfunction : Giventheoriginalstate-spacematrices (A,B,C, h0),
the transfer function is given by H(z) =C(zI−A)−1B+h0. A proper rational function has the form
H(z) =q(z)/p(z)where the numerator q(z)has coefficients b= (bn)d
n=0and the denominator has
coefficients a= (an)d
n=0(a0= 1since pis monic). As shown in Section A.6, the coefficients of the
transfer function can be extracted in closed-form as b=poly(eig(A−BC)) + poly(eig(A))(1−h0)and
a=poly(eig(A))14;
2.Construct companion matrices Given the coefficients aandba new set of canonical state-space
matrices which realize the transfer function can be obtained following the recipe of Section A.5.
The resulting companion SSM is equivalent the the original one since they share the same transfer function.
14eig(A)contains the eigenvalues of A.poly(r)yields the coefficients of the polynomial whose roots are the elements of
r∈Cd.
25

--- PAGE 26 ---
BLaughingHyena : Further Details
B.1 Parametrization of Modal Interpolators
Complex-conjugate states Assuming even distilling dimension d, we pick poles λnand residues Rnin
complex-conjugate pairs:
A= diag( λ1,···, λd/2, λ∗
1,···, λ∗
d/2)
C=1
2[R1,···, Rd/2, R∗
1,···, R∗
d/2](B.1)
which allow partitioning the state-space matrices as
A=λ
λ∗
,C=1
2
R R∗
, (B.2)
where
λ= diag( λ1,···, λd/2)and R= [R1,···, Rd/2]. (B.3)
If we also partition the state as x= (¯x,˜x),¯x,˜x∈Cd/2, the resulting recurrence has the form
¯xt+1=λ¯xt+1d/2ut
˜xt+1=λ∗˜xt+1d/2ut(B.4)
We have
¯xt=λt¯x0+t−1X
j=0λt−j−11d/2ut,˜xt= [λ∗]t˜x0+t−1X
j=0[λ∗]t−j−11d/2ut (B.5)
Thus, if ˜x0= ¯x∗
0, then ˜xt= ¯x∗
tfor all t >0. Hence, at inference time we only need to propagate forward
half of the state – say ¯x– and then compute the output as
yt=Dut+1
2(R¯xt+R∗¯x∗
t)
=Dut+R{R¯xt}
=Dut+R{R}R{¯xt} −I{R}I{¯xt}(B.6)
This parametrization allows to update only half of the state, reducing the time and memory cost compared
to anunconstrained linear system with complex coefficients. However, the implicitly achieved realness of the
output (assuming D=h0∈Randut∈R) comes at a cost of expressivity: such a system is equivalent to an
unconstrained complex linear system of dimension d/2of which we only keep the real part of the output.
Polesandresidues Forthemodalinterpolation, theparametrizationisanalogoustotheoneofadiagonal
state space model [33]. Poles λnand residues Rnneed both to be complex numbers. In [33] the authors
suggest parametrizing real and imaginary components of BandCmatrices while representing the eigenvalues
λnin polar form, λn=rneiαnwith rnandαnbeing themselves exponential functions of the actual trainable
parameters, rn=e−eνn, αn=eζnleading to
λn= exp{−exp{νn}+iexp{ζn}}, ν n, ζn∈R (B.7)
This ensures stability of the poles |λn|<1and positive-only phases αn. For the purpose of distillation we
propose a simplified parametrization as follows:
1. We only parametrize the Cvector. Parametrizing both Band Cis redundant and increases the
computational cost of performing each step of the recurrence. The residues Rncorrespond in fact
toRn=CnBnof a diagonal state space model. Setting B=1dsaves parameters without harming
expressivity. Further if Bis different from 1kit needs to be multiplied to utat each recurrence step.
Cn=Rn=R[Rn] +iI[Rn]andR[Rn],I[Rn]are the trainable parameters of the residue.
2. For the purpose of distillation we have no benefit in forcing the eigenvalues of the model to be stable,
i.e. constrained to lie strictly inside the unit circle. Instead, such constrain may actually harm the
expressivity of the approximant. We choose the the simpler parametrization λn=rneiαn, rn, αn∈R.
26

--- PAGE 27 ---
B.2 Distillation as Rational Interpolation
Distillation as rational interpolation Approximating a filter with an SSM can be thus achieved by fit-
ting a proper rational function to the (truncated) transfer function of the original filter HL(z):=PL
t=0htz−t.
That is,
Find a, bsuch that h0+h1z−1+···+hLz−L≈h0+Qb(z)/Pa(z). (B.8)
A modern15way to solve this problem by H2error minimization via gradient descent16. We can use the
Fast Fourier Transform ( FFT) to evaluate both the target and distilled transfer functions and solve:
min
a,b∈RdLX
k=0|FFT L[h]k−h0−FFT L[b]k/FFT L[a]k|2. (B.9)
To ensure stability of the distilled filters and well-conditioned gradient descent dynamics, the roots of the
denominatorpolynomialmuststrictlylieinsidetheunitcircle( ρ(A)<1). This,inturn,requiresconstraining
the coefficients ainto the region {a:poly(a)is stable }which is by itself an open research problem [52, 53].
Experimentally, we observe that standard coefficient normalization techniques overly restrict the parameters
space and lead to poor distillation performances at reasonable order.
15In the late 19th century, Henri Padé had already proposed a closed-form solution of the above problem that achieves
o(z−L)error for z→ ∞using L=2dsamples of the impulse response. His method [31] solves a L-dimensional linear problem
that, however, is known to often become numerically ill-conditioned even with small d[51]
16In the case of finite sequences, the H2norm becomes the standard Euclidean metric evaluated on the L+1roots of unity,i.e.
(PL
k=0|H(ei2πk/(L+1))|2)1/2.
27

--- PAGE 28 ---
C Proofs
C.1 Proof of Lemma 2.1
Generating Ktokens with a long convolution layer (2.1) from a length- Tprompt has time complexity
O(Tlog2T+TK+K2)and requires O(L)memory.
Proof.We compute the time complexity memory of a length- Tprompt processing ( pre-filling ) and subse-
quent auto-regressive decoding of Ktokens. The auto-regressive generation of long convolution computes
the next token as by
t=T, . . . , T +K−1⇒yt=t−1X
j=0ht−jyj (C.1)
The pre-filling step is needed to prime this recurrence by computing the first Toutputs till yT−1from the
length- Tprompt u. This is just a convolution between two length- Tsignal and requires O(Tlog2T)time
and linear memory. The auto-regressive decoding of Ktokens requires Ksteps (C.1) with the length of the
sequences increasing by 1at each step. Thus we have a total asymptotic complexity of
K−1X
k=0(T+k) =TK+1
2K(K+ 1). (C.2)
and requires at worst ( k=K−1) to store the length T+K=Lgenerated output sequence, i.e. O(L)
memory. In the limit we thus have a total time complexity of O(Tlog2T+TK+K2)andO(L)memory.
C.2 Proof of Lemma 2.2
Generating Ktokens with a SSM (2.2) from a length- Tprompt has time complexity O(Tlog2T+dK)
and requires O(d)memory.
Proof.In autoregressive mode, the cost of generating one token is the cost of evaluating the state recurrence
(2.2). Each step then requires O(d)time and memory for the class of SSMs considered in this work (see
Lemma A.8). Hence, generating Ktokens costs O(dK)time and constant O(d)memory (we only need to
store the current state).
The recurrence is initialized for autoregressive generation with the post-prompt state xT−1and output
yT−1. The latter can be recovered in linear time and memory O(T)by definition yT−1=PT−1
j=0ht−juj
(assuming to have the impulse response havailable) and state xT−1inO(dT)time and dmemory through
the recurrence. The overall asymptotic cost is therefore O(dL)time and O(d)memory.
Note that, for prompts and SSMs of practical sizes we usually have d >log2T. In such a case the state
xT−1can be computed in Tlog2Ttime rather than dTby Proposition 3.2.
C.3 Proof of Lemma 2.3
Generating Ktokenswithself-attentionfromalength- Tprompthastimecomplexity O(T2+TK+K2)
and requires O(L)memory.
Proof.The proof is identical to the one of Lemma 2.1, with the only difference of a quadratic asymptotic
costO(T2)to process the prompt obtain the kvcache.
Self-attention suffers with long contexts: it is significantly more expensive in prefilling than long con-
volutions and SSMs due to its quadratic cost. Nonetheless, in autoregressive mode, self-attention reaches
the same overall asymptotic complexity O(TK+K2)as long convolutions (with the memory overhead of
having to cache kandv).
28

--- PAGE 29 ---
C.4 Proof of Proposition 3.1
IfAhas semi-simple eigenvalues λn∈C, then the transfer function of the system can be decomposed
asˆH(z)=Pd
n=1Rn/(z−λn)where Rn∈Cis the residue associated with the pole λn.
Proof.IfAis semi-simple, then it is diagonalized by a basis Vof eigenvectors; it admits an eigenvalue
decomposition diag(λ) =VAV−1where λ= (λ1, . . . , λ d)∈Cdcontains the eigenvalues of A. Projecting the
state onto the basis of eigenvectors, s:=Vx, the state space model is is transformed into modal form:
st+1=VAV−1st+VBut
yt=CV−1st⇔st+1=diag(λ)st+˜But
yt=˜Cst
where ˜B:=VB= (˜bn)d
n=1and˜C:=CV−1= (˜cn)d
n=1. In modal form, the state equations are decoupled, i.e.
sn
t+1=λnsn
t+˜bnut
yt=dX
n=1˜cnsn
t.
Taking the z-transform of the output equation and each state equation yields
Sn(z) =Z[sn](z) =˜bn
z−λnU(z)n= 1, . . . , d
Y(z) =dX
n=1˜cnSn(z)
Thus, the overall transfer function is
H(z) =Y(z)
U(z)=dX
n=1˜bn˜cn
z−λn
Letting Rn=˜bn˜cn, proves the result.
C.5 Proof of Lemma 3.1
The distilled filter ˆhin modal form (3.2) can be computed in O(dL)time from its modal form and in
˜O(L)from its rational form.
Recalling (3.2), ˆht=Pd
n=1Rnλt−1
n, Rn, λn∈C, t > 0, theO(dL)complexity of the impulse response is
apparent: for each of the t= 1, . . . , L,ˆhtcan be computed in O(d)time.
The ˜O(L)cost from the rational form follows by Lemma A.6.
C.6 Proof of Theorem 3.2
Lethbe a length- Lfilter, ˆha distilled filter of order d < Land let SL,ˆSLbe the respective Hankel
matrices. Then infˆSL∥SL−ˆSL∥2=σd.
Proof.The theorem characterizes the best-case scenario in terms of approximation error of the distilled
SSMs or a certain order dwhere it is clear that rank ˆS≤d. This theorem is a direct application of the
Adamyan-Arov-Krein (AAK) theory of infinite Hankel operators [7]. Let ˆS∗
L= arg inf ˆSL∥SL−ˆSL∥2; the
AAK theorem says that every causal system can be optimally approximated by another causal system of
lower dimension. Optimal here means
inf∥SL−ˆS∗
L∥= inf∥SL−K∥
where the first infimum is taken over all Hankel matrices S∗
Land the second over all arbitrary matrices K
(see [19, Chapter 8] and [54] for further details and references).
29

--- PAGE 30 ---
C.7 Proof of Proposition 3.3
The filter (3.2) has a state space matrices A=diag(λ1, . . . , λ d)∈Cd×d,B= (1, . . . , 1)⊤∈Cd×1,C=
(R1, . . . , R d)∈C1×d,D=h0whose step can be evaluated in O(d)time and memory.
Dis set to h0by default. The result is proven showing that (3.2) can be written in the form ˆht=CAt−1B
fort >0. If we choose A=diag(λ)then the impulse response becomes
ˆht=dX
n=1Rnλt−1
n=C[diag(λ)]t−1B=dX
n=1CnBnλt−1
n
Thechoice Bn= 1forall n= 1, . . . , d,B=1dandCn=Rnfinalizesamodalcanonicalstate-spacerealization
of the distilled filter. The O(d)time complexity of the corresponding recurrent step is guaranteed by the
decoupling of each state equation from another,
xn
t+1=λnxn
t+utn= 1, . . . , d
yt=dX
n=1Rnxn
t+h0ut.
Each of the dstate equations can be computed (in parallel) in O(1)time. The output equation is a dot
product requiring dmultiplications and dadditions, hence the O(d)time compexity of the recurrence.
C.8 Proof of Proposition 3.2
xT= (vT, . . . , v T−d)where v=g∗uandgis the filter whose transfer function is 1/den(ˆH)(z)and
can be evaluated in ˜O(T).
Without loss of generality, let us assume to have converted the distilled filter in canonical form (i.e. we
have unrestricted access to the coefficients of the rational transfer function) and let D= 0. We use the
notation of Section A.5. In z-domain, the state-to-input relation is given by
Y(z) =CX(z) =β1··· βd
X(z)
On the other hand Y(z) =ˆH(z)U(z) =q(z)/p(z)U(z). Therefore,
β1··· βd
X(z) =q(z)
p(z)U(z)
⇔β1··· βd
X(z) =β1··· βd
z−1
...
z−d
1
p(z)U(z)
⇔X(z) =
z−1
...
z−d
1
p(z)U(z)
LetV(z) =U(z)/p(z). From the shift property of the z-transform it holds,
Z{x}(z) =X(z) =
z−1
...
z−d
V(z)⇔ xt= (vt−1, vt−2,···, vt−d)∀t >0.
vcan be obtained in ˜O(L)time via an FFT-convolution of the input uandg, the filter resulting from inverse
transforming 1/p(z). The proof is convoluded setting t=L
30

--- PAGE 31 ---
C.9 Proof of Theorem 4.1
Notation. We will be denoting the all 1row vector of size k, given by
1 1 . . . 1 1
, and the all
0row vector of size k, given by0 0 . . . 0 0
, as1kand0k, respectively. We will also construe the
standard basis vector eias a column vector in these notes. Next, we will adhere to the following matrix
indexing convention: Aijis the entry in the ith row and the jth column, A[i,:]∈F1×ndenotes the ith row,
andA[:, j]∈Fm×1denotes the jth column of A∈Fm×n. Here, we also use 0m×n∈Rm×nandInto denote
the matrix of all zeros and the identity matrix of dimension n, respectively. Moreover, we extend the outer
product between two vectors to a tensor product using the symbol ⊗,the computation of which is carried
out batch-wise with some dimension of one or both of the input tensors. Finally, we express the binary
encoding of i∈[n]in a row vector form, given by Bi∈ZPn
2,where Pnis the closest power of 2 to n.
LanguageandModelDescription. Thelanguage Λhasskeysand svalues: LK:={k1, . . . , k s}, LV:=
{v1, . . . , v s}.Formally, the language Λconsists of sequences x∈(LK×LV)s×LK, where there is an
associatedmapping fx:LK→LV.Foreachsequence, theoddindicesin [L]belongto LK, forx1, x3, . . . , x L,
and we define
x2·i=fx(x2·i−1) (C.3)
The last item xL∈ {x1, x3, . . . , x L−1}, called the query, must be one of the keys that has appeared in x
already. Our goal is to produce fx(xL)at the end of the sequence, which we refer as the associated value .
This problem is termed as the associative recall problem [55].
We will now outline the Hyenalayer [2] with multiple heads as follows.
Algorithm 1 Hyena
Require: Input sequence u∈RL×Dfrom the previous layer, long convolution filter Th, number of heads
M.
1:qm, km, vm←Projection (u)form∈[M].
2:form= 1, . . . , Mdo
3:Perform the outer product zm←km⊗vm∈RL×N×N,where N:=D/M.
4:Apply the convolution independently and compute ym
t←Th(zm
t)qm
t∈RL×N
5:Average the output y←(P
m)ym/M
6:Retrieve the value f(kL)of the key kLfrom y[L,:].
In order to prove Theorem 4.1, we need the following technical statement concerning sparse recovery of
a heavy-hitter.
Proposition C.1 (Heavy-Hitter Recovery) .Letx∈Rsbe a vector with one entry bounded by 1±
1
34√s—referred as the heavy-hitter —and the rest of the entries bounded by ±1
34√s. Then, there exists a
matrix S(m)∈Rs×O(√slogs)such that the position of the heavy-hitter in xcan be inferred from the average
ofMmeasurements with S(m)given by P
mxS(m)
/Mwith probability of error ≤1
s.
Before presenting the proof of Proposition C.1, we use it to prove Theorem 4.1 as follows.
Proof of Theorem 4.1. We take D=O(√slog2s)andM= 243 ·logsso that N=O(√slogs)and use the
same projections and filters for each head. We will start by describing the projections of the input. To this
end, let E: [L]→2sdefine a map from the row indices of uto the keys kiand values fx(ki)given by
E(t) =(
i, t odd, xt=ki,
i+s, teven, xt−1=ki,(C.4)
Here, we note that we also have
E(t) =E(t−1) +s, teven (C.5)
as the even indices are defined as xt=fx(xt−1)forteven (C.3), whence xt−1∈LKast−1is odd.
Next, we can separate the keys q, queries qand values vfrom the input sequence u. For keys and queries,
we will be using the Johnson-Lindenstrauss embedding [56]. We state its guarantee here.
For a set of points P⊆Rs, letϵ, δ > 0with k≥2 ln 2s
δ
/ϵ2, and f:Rs→Rkbe the randomly
constructed linear map from [56], then with probability of error ≤δ,we have
|⟨f(x), f(y)⟩ − ⟨x, y⟩| ≤ϵ
for all x, y∈P.
31

--- PAGE 32 ---
More precisely, we take R∈RO(√slogs)×sto be the matrix representation of fwith ϵ:=1
34√sandδ:=1
sc
for some c >1so that R[:, i] =f(ei). Thus, we define
qm[t,:] =

R[:, E(t−1)], E(t−1)≤s
0, otherwise .(C.6)
For values, we use the heavy-hitter recovery matrix as described in Proposition C.1 so that we have
vm[t,:] =

S(m)[E(t),:], E(t)> s
0, otherwise ,(C.7)
Further, using 1DConv(equivalently, in terms of polynomials, h(X) :=X), we can shift the queries to
get the projection for keys kso that we have km[t,:] =qm[t−1,:]
The Hyena filters, along with the specific convolution being performed by Th, are specifically described
in terms of polynomial multiplications, for all m∈1, . . . , M, as follows.
Th(X) :=LX
i=0Xi.
Here, we note that Th(u)takes the cumulative sum over the input. That is, for all i, we have
Th(u)[i,:] =iX
j=0u[j,:]
We will now compute zmas follows
zm=km⊗vm
Further, applying the convolution, we get
Th(zm
t) =tX
i=0km[i,:]⊗vm[i,:]
For inference, it suffices to show that the last row of the output yrecovers the output with high probabil-
ity. Indeed, let t′∈[L]denote the row index of the value associated to the query such that the corresponding
key has the following relation
ut′−1=uL. (C.8)
Finally, we multiply by the query qacross L. Specifically, we now look at the computation of the Lth
row of y:
ym[L,:] = LX
t=0km[t,:]⊗vm[t,:]!
qm[t,:]
=LX
t=0 
qm[L,:]⊤km[t,:]
vm[t,:]
=LX
t=0 
qm[t′−1,:]⊤qm[t−1,:]
vm[t,:] (C.9)
=X
t∈[L]
teven 
(R[:, E(t′−1)])⊤R[:, E(t−1)]
S(m)[E(t),:] (C.10)
=LX
t∈[L]
teven
Re⊤
E(t′−1)ReE(t−1)
S(m)[E(t),:] (C.11)
Here, we are using the fact that km[t′−1,:] =qm[L,:]due to (C.8) in (C.9). We then change the indexing
from (C.9) and (C.11) by observing that all the odd entries corresponding to values are zeroed out in K(cf.
C.6). Finally, we simply substitute (C.6) and (C.7) in (C.10) and (C.11), respectively.
32

--- PAGE 33 ---
Next, we define x∈Rs+1with
xj:=Re⊤
E(t′−1)Rej (C.12)
where j=t−1with t∈[L]andteven. Here, x∈Rn+1is a vector of size s+ 1as there are s+ 1such even
numbers in [L]. Note that xis the vector with a heavy-hitter from Proposition C.1. To see this, observe
that we havexE(t′−1)−1≤1
34√sand|xj| ≤1
34√sfor all j̸=E(t′−1). Using (C.11), with probability
≥1−1
sc, we then have
ym[L,:] =xS(m). (C.13)
By Proposition C.1, we can then infer the position of the key at t′with probability of error1
s. By the union
bound, we can then retrieve the corresponding value with probability at least 1−(1
s+1
sc).
We will now prove C.1 as follows.
Proof of C.1. We will assume that sis a power of 2for the sake of simplicity. We first specify how we will
construct such an S(m)∈Rs×O(√slogs). Let h: [s]→[√s]be a hash function. We define ˜S∈Rs×√sto be
˜S[:, i] =X
j:h(j)=iej.
That is, each column iof˜Sis the sum of the standard basis vectors ejsuch that jis mapped by htoi.
In other words, the locations of the non-zero entries in column icorrespond to the preimage of iunder h.
We then multiply each non-zero entry of ˜Sindependently at random by ±1. Next, we replace the kth row
in˜Sby multiplying all non-zero ±1entries at index iwith the binary representation of ito get a matrix
S(m)∈Rs×(√s×logs). That is, for a non-zero entry at index iin row k, we replace the ith entry with ±1·Bi.
Note here that each column still has at most√snon-zero entries. Finally, we stack 243·logs-many copies
ofS(m)as heads so that each copy produces independent measurements xS(m). Here, we want to emphasize
that each such copy uses fresh randomness for multiplying the non-zero entry of ˜Sindependently at random
by±1.
Now, we will show that the average of the measurements with matrices S(m)∈Rs×√slogscan locate the
heavy-hitter in x, where xis the vector of inner products from (C.12). For this purpose, we first specify the
algorithm for decoding the heavy-hitter.
Algorithm 2 Decoder
Require: The vector ysuch that y=xS.
1:Split yinto243·logsblocks y(m)∈R√slogs, each of which is a result of multiplying xbyS(m), m∈
[243·logn].
2:Take the average y←1
243·logsP
ky(m).
3:b←Ir(|y|)∈R√slogs, cf. (C.14).
4:Retrieve bby isolating the binary representation of the position of the heavy-hitter in x.
Here, we define the function Ir:R√slogs→Z√slogs
2to[xS]mthat rounds each entry of its input to the
nearest integer:
Ir([xS]m) =S(m)[i,:]. (C.14)
That is, in both cases, we retrieve the row in S(m)that corresponds to the heavy-hitter in x. Since the rows
inS(m)are distinct, we can also infer the position of the heavy-hitter in xwith probability 1.
We now show that y=xS, which consists of 243·lognmany independent copies of y(m)=xˆS(m).
Instead, notice that we can analyze ˜y=x˜Ssince each ˆyis the replacement of non-zero entries of ˜Swith the
binary representation of their indices times y. We will drop the superscript for now to avoid cluttering the
notation. We can then make the following claim:
|˜yi|= 1±O(ϵ·4√s)and, for j̸=i,|˜yj|=O(ϵ·4√s). (C.15)
For the above claim, we note that the first part follows from the latter as it suffices to show that all the
non-zero heavy-hitters contribute O(ϵ4√s)to the sum ˜yi=⟨x,˜S[:, i]⟩. Since each column in ˜Sonly interacts
with√ssized sub-vector of x, each ˜yjfor non-heavy hitters can be expressed as
˜yj=⟨x,Sj⟩with x,Sj∈R√s,
33

--- PAGE 34 ---
where Sj∈R√scontains the non-zero entries of ˜Sjandxis obtained by extracting the entries with
corresponding indices from x. Here, we have ∥x∥2≤ϵ4√ssince each entry associated with the non-heavy
hitter is bounded as xi≤ϵ, and thus, ∥x∥2=pP
ix2
i≤pP
iϵ2=p√s·ϵ2=ϵ4√s.
Consequently, as ˜Sjis independently random ±1, we then must have
⟨x,Sj⟩≤1
3(C.16)
with constant probability for j̸=i. To see this, note that
E[⟨x,Sj⟩2] =X
k,ℓE[Sjk·Sjℓ]·xi·xj
=∥x∥2
2,
where the last equality follows since E[Sjk·Sjℓ] =δk,ℓby the distribution on entries of Sj. Now, we use
Jensen’s inequality [57] to get the following bound on the expectation of⟨x,Sj⟩.
E⟨x,Sj⟩
≤q
E[⟨x,Sj⟩2]≤ϵ4√s. (C.17)
We then use the expectation above to bound the relevant probability as follows:
Pr⟨x,Sj⟩≤1
3
≥1−Pr⟨x,Sj⟩≥1
3
≥1−3E⟨x,Sj⟩
(C.18)
≥1−3ϵ·4√s,
where we apply Markov’s inequality [57] in (C.18). That is, we have shown that ˜yjis bounded by 1/3with
constant probability for j̸=i, and ˜yiis thus bounded by 1±1
3. Note here that each of the m-copies ˜y(m)
i
will have identical guarantees.
Now, define the average ˜yj:=1
243·logsP
m˜y(m)so that yj(line 3 in Decoder) is the corresponding
replacement of the non-zero entries with the binary representation of their indices. We now claim that this
average ˜yj≤4/9<1/2with high probability for j̸=i. To this end, we employ the multiplicative Chernoff
bound [57] on the independent random variables {˜y(h)
j}h[0,1]withEhP
m˜y(m)
ji
≤81·logsto get
Pr
˜yj>4
9
= Pr"X
m˜y(m)
j>
1 +1
31
3·243·logs#
≤Pr"X
m˜y(m)
j≥
1 +1
3
81·logs#
,
≤exp
−1
32·81·logs
/3
,
=1
s3.
Therefore, we have shown that the average ˜yjis less than 1/2with probability at least 1−1
s3forj̸=i.
Consequently, we will have ˜yibounded by 1±1
2. Using the union bound over each j̸=iand the logsbits
in the binary representation of j, we can then show that yj+m<1/2for each j̸=i, m∈[0,logs]with
probability 1−logs
s2≫1−1
s.
34

--- PAGE 35 ---
D Experimental Details
D.1 Pre-training
To verify the effect of introducing heads to Hyenaas described in Section 4, we train a series of models on
The Pile [11]. All MultiHyena models are set to 8heads, and otherwise use the same hyperparameters
ofHyenamodels of equivalent size. We set weight decay of Hyena filter parameters to 0, and lower the
frequency of sine activations in the implicit MLP to 4. We follow the setup of [2], and first train models for
5,10and15billion tokens, adjusting the learning rate scheduler accordingly. Then, we train for 300billion
tokens. The results are reported in Tables 5.1 and 5.1.
D.2 Distillation Analysis
Distilling pre-trained long convolution sequence models ( LCSM) with LaughingHyena can introduce errors
on the convolution filter, which then propagate to the outputs.
Setup We perform a series of extensive experiments on all variants of LCSM, including pre-trained H3
models of sizes 125million, 355million, 1.3billion and 2.7billion parameters; Hyenaof size 153million
parameters, and MultiHyena of size 153million parameters. For H3models, we report approximation errors
on both shift as well as diagonal SSMs (reported as IIR and FIR). Each point corresponds to distillation
carried out at a particular order, using LaughingHyena modal interpolation. To optimize the parameters of
the modal form, we use gradient-based optimization and minimize the ℓ2discrepancy between filters in time
domain. In particular, we use the AdamW [58] optimizer with learning rate 3·10−4, and a cosine annealing
decay schedule down to 10−6after 30thousand iterations. Each individual filter of every layer is distilled
in the same way.
Discussion The errors are shown in Figures D.1, D.1, D.2, D.3, D.4 and D.5. We observe H3filters to
be easier to distill into recurrences with small state without introducing significant errors, whereas Hyena
variants learn filters with larger effective dimensions. This provides further evidence that training with
implicit convolutions may yield in general more expressive filters.
21230.000050.000100.000150.00020Error
IIR/lscript1Error
21230.000.250.500.751.00×10−6
IIR/lscript2Error
21230.00250.00500.00750.01000.0125
IIR/lscript∞Error
2022240.050.100.150.20
IIR Hankel Singular Values
212325
Order0.00050.00100.00150.00200.0025Error
FIR/lscript1Error
212325
Order0.00.51.01.52.0×10−5
FIR/lscript2Error
212325
Order0.0040.0060.0080.010
FIR/lscript∞Error
2023
Singular Value Index0.10.20.3
FIR Hankel Singular ValuesDistillation Error of H3 125M
Figure D.1: Mean, lower and upper bounds across channels and layers of the distillation errors on 125M H3model for both
its IIR and FIR filters.
D.2.1 Pretrained Filters: Effective Dimension
Visualizations We qualitatively investigate LCSMfilters at initialization and after pretraining. This
visual inspection (Figures D.6, D.7 and D.8) complements the distillation error analysis of Section D.2.
35

--- PAGE 36 ---
21230.000040.000060.000080.00010Error
IIR/lscript1Error
21230246×10−7
IIR/lscript2Error
21230.0020.0040.0060.0080.010
IIR/lscript∞Error
2022240.050.10
IIR Hankel Singular Values
212325
Order0.00100.00150.00200.0025Error
FIR/lscript1Error
212325
Order0.51.01.52.0×10−5
FIR/lscript2Error
212325
Order0.0040.0060.0080.0100.012
FIR/lscript∞Error
2023
Singular Value Index0.050.100.150.200.25
FIR Hankel Singular ValuesDistillation Error of H3 355MFigure D.2: Mean, lower and upper bounds across channels and layers of the distillation errors on 355M H3model for both
its IIR and FIR filters.
21230.000060.000080.000100.000120.00014Error
IIR/lscript1Error
21232468×10−7
IIR/lscript2Error
21230.0040.0060.0080.0100.012
IIR/lscript∞Error
2022240.000.050.10
IIR Hankel Singular Values
212325
Order0.00100.00150.0020Error
FIR/lscript1Error
212325
Order0.51.0×10−5
FIR/lscript2Error
212325
Order0.0040.0060.008
FIR/lscript∞Error
2023
Singular Value Index0.050.10
FIR Hankel Singular ValuesDistillation Error of H3 1 .3B
Figure D.3: Mean, lower and upper bounds across channels and layers of the distillation errors on 1.3B H3model for both
its IIR and FIR filters.
Distribution of Hankel singular values We further compute the distribution of Hankel singular values
of each long convolution filter in different models. The decay in the spectrum quantifies how easyit is to
find a compact modal form with LaughingHyena , and serves as a proxy measure of effective dimension of the
convolution. The results are shown in Figures D.9 and D.10.
D.3 Downstream Evaluation
We benchmark the downstream performance of MultiHyena and distilled MultiHyena on standard language
modeling tasks from the LM-Eval-Harness [42] and HELM [41] suites. As a reference baseline, we evaluate
Pythia [44] 160M.
Our objective is to quantify the absolute performance of MultiHyena and the downstream impact of
distillation. We use the same procedure outlined in Section D.2 to distill MultiHyena .
D.4 Benchmarking
To demonstrate the superior performance of Laughing Hyena for autoregressive generation, we conduct a
series of experiments to benchmark its latency, throughput, and memory usage for autoregressive generation
36

--- PAGE 37 ---
21230.000100.000150.000200.00025Error
IIR/lscript1Error
21230.51.0×10−6
IIR/lscript2Error
21230.00500.00750.01000.0125
IIR/lscript∞Error
2022240.000.050.10
IIR Hankel Singular Values
212325
Order0.00100.00150.00200.00250.0030Error
FIR/lscript1Error
212325
Order123×10−5
FIR/lscript2Error
212325
Order0.0060.0080.0100.012
FIR/lscript∞Error
2023
Singular Value Index0.050.10
FIR Hankel Singular ValuesDistillation Error of H3 2 .7BFigure D.4: Mean, lower and upper bounds across channels and layers of the distillation errors on 2.7B H3model for both
its IIR and FIR filters.
with initial prompt length Tand number of generated tokens K. For each experiment, we compare the
performance of Laughing Hyena against a Transformer, a hybrid H3-attention model with 2 attention layers
and a Hyenamodel. The latter two have been shown to match or achieve lower perplexity than Transformers
on standard datasets ( Wikitext103 andThe Pile ). All experiments are carried out on a NVIDIA A100
with 80GB in float16 precision. Missing measurements for any model indicate Out of Memory (OOM) errors
while doing autoregressive inference for that particular model.
Peak throughput We first evaluate the throughput (number of tokens generated per second) across
different batch sizes, using a typical generation workload consisting of a prompt of length 512 and generating
256 tokens. Figure 1.1 measures peak throughput of different models. Since Laughing Hyena does not require
caching intermediate kv-projections during generation, reduced memory requirements at a fixed model size
allow it to process larger batch sizes.
Prompt length Autoregressive generation in Laughing Hyena is achieved through a two-step process: an
initialprefillstepthatusesthelength −Tprompttoinitializethestate xTandthatgeneratesall Ktokens. In
Figure5.3wedemonstratehowtheprefillstepscalesfordifferentpromptlengths, keepingbatchsizesfixedat
64. Since prefilling in Laughing Hyena is carried out efficiently via convolutions (as described in Section 3.4),
throughput scales more favorably than Transformers. Other models capable of prefilling via convolutions
also achieve higher throughputs than Transformers but are ultimately slower than Laughing Hyena during
the generation phase.
State throughput We measure the impact of SSM state dimension on the throughput of Laughing Hyena .
Keeping batch sizes fixed reveals minimal impact for all dimensions smaller than 100, which are sufficient
to distill all models discussed in this work. All other measurements provided in this Section are carried out
with a standard order 16. We note that it may be possible to further increase peak throughput by leveraging
reduced memory footprints achieved by extremely small SSMs.
Latency over sequence length We benchmark the time taken to generate a variable number of tokens,
starting from a prompt of length 512 tokens at batch size 1 (Figure D.11). Laughing Hyena tracks highly
optimized Transformers. We note that Laughing Hyena is asymptotically more efficient than Transformers;
however, this regime is bottlenecked by hardware-specific implementation details and optimizations. We
expect optimized, platform-specific implementations of Laughing Hyena to outperform Transformers even at
batch size 1. When the prompt is long, the prefilling step becomes the bottleneck, and all convolutional
models outperform Transformers.
37

--- PAGE 38 ---
212427
Order0.000.01Error
/lscript1Error
212427
Order0.0000.0050.0100.015Error
/lscript2Error
212427
Order012Error
/lscript∞Error
212427510
Hankel Singular ValuesDistillation Error of Hyena 155M (150B tokens )
2125
Order0.00.10.2Error
/lscript1Error
2125
Order0123Error
/lscript2Error
2125
Order01020Error
/lscript∞Error
212550100
Hankel Singular ValuesDistillation Error of MultiHyena 155M (300B tokens )Figure D.5: Mean, lower and upper bounds across channels and layers of the distillation errors on Hyenaand MultiHyena
models.
Parameter scaling To better understand how the performance of Laughing Hyena scales, we benchmark
its latency, throughput, and peak memory utilization for autoregressive generation and 125M, 355M, 1.3B,
2.7B and 6.7B parameters. We compare the performance to that of Transformers, Hybrid-H3, and Hyenaat
the same number of parameters and report the results in Figure D.11. For the latency measurement, we use
a batch size of 1 and benchmark the time taken to generate 128 tokens, starting from a prompt of length
512 tokens. For throughput and peak memory scaling against the number of parameters, we use a batch
size of 64 and measure the throughput for generating 256 tokens starting with a prompt of length 512.
38

--- PAGE 39 ---
0 50−101Normalized Filters ht//bardblh/bardbl∞Layer 0
Initialization
Pre-trained
0 50−101Layer 1
0 50−101Layer 2
0 50−101Layer 3
0 50−101Layer 4
0 50−101Layer 5
0 50
t−101Normalized Filters ht//bardblh/bardbl∞Attention Layer
0 50
t−101Layer 7
0 50
t−101Layer 8
0 50
t−101Layer 9
0 50
t−101Layer 10
0 50
t−101Layer 11FIR Filters of H3 125M
0 2000−101Normalized Filters ht//bardblh/bardbl∞Layer 0
Initialization
Pre-trained
0 2000−101Layer 1
0 2000−101Layer 2
0 2000−101Layer 3
0 2000−101Layer 4
0 2000−101Layer 5
0 2000
t−101Normalized Filters ht//bardblh/bardbl∞Attention Layer
0 2000
t−101Layer 7
0 2000
t−101Layer 8
0 2000
t−101Layer 9
0 2000
t−101Layer 10
0 2000
t−101Layer 11IIR Filters of H3 125MFigure D.6: Initialized and pre-trained convolution filters of H3.
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 0
Initialization
Pre-trained
0 2000−101Layer 1
0 2000−101Layer 2
0 2000−101Layer 3
0 2000−101Layer 4
0 2000−101Layer 5
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 6
0 2000−101Layer 7
0 2000−101Layer 8
0 2000−101Layer 9
0 2000−101Layer 10
0 2000−101Layer 11
0 2000
t−101Normalized Filters ht//bardblh/bardbl∞ Layer 12
0 2000
t−101Layer 13
0 2000
t−101Layer 14
0 2000
t−101Layer 15
0 2000
t−101Layer 16
0 2000
t−101Layer 17Filters of MultiHyena 155M (300B tokens )
Figure D.7: Initialized and pre-trained long convolution filters of MultiHyena .
39

--- PAGE 40 ---
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 0
Initialization
Pre-trained
0 2000−101Layer 1
0 2000−101Layer 2
0 2000−101Layer 3
0 2000−101Layer 4
0 2000−101Layer 5
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 6
0 2000−101Layer 7
0 2000−101Layer 8
0 2000−101Layer 9
0 2000−101Layer 10
0 2000−101Layer 11
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 12
0 2000−101Layer 13
0 2000−101Layer 14
0 2000−101Layer 15
0 2000−101Layer 16
0 2000−101Layer 17
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 18
0 2000−101Layer 19
0 2000−101Layer 20
0 2000−101Layer 21
0 2000−101Layer 22
0 2000−101Layer 23
0 2000−101Normalized Filters ht//bardblh/bardbl∞ Layer 24
0 2000−101Layer 25
0 2000−101Layer 26
0 2000−101Layer 27
0 2000−101Layer 28
0 2000−101Layer 29
0 2000
t−101Normalized Filters ht//bardblh/bardbl∞ Layer 30
0 2000
t−101Layer 31
0 2000
t−101Layer 32
0 2000
t−101Layer 33
0 2000
t−101Layer 34
0 2000
t−101Layer 35Filters of Hyena 355M (200B tokens )Figure D.8: Initialized and pre-trained long convolution filters of Hyena(355 M).
40

--- PAGE 41 ---
20242810−410−2100102Hankel Singular Values
Layer 0
MultiHyena
Hyena
20242810−410−2100102
Layer 1
20242810−410−2100102
Layer 2
20242810−410−2100102
Layer 3
20242810−410−2100102
Layer 4
20242810−410−2100102
Layer 5
20242810−410−2100102Hankel Singular Values
Layer 6
20242810−410−2100102
Layer 7
20242810−410−2100102
Layer 8
20242810−410−2100102
Layer 9
20242810−410−2100102
Layer 10
20242810−410−2100102
Layer 11
202428
n10−410−2100102Hankel Singular Values
Layer 12
202428
n10−410−2100102
Layer 13
202428
n10−410−2100102
Layer 14
202428
n10−410−2100102
Layer 15
202428
n10−410−2100102
Layer 16
202428
n10−410−2100102
Layer 17Hankel Singular Values of Hyena and MultiHyena FiltersFigure D.9: Distribution of Hankel singular values for Hyenaand MultiHyena long convolution filters.
MultiHyena filters have larger effective dimension, as evidenced by slower decay.
202410−510−2Hankel Singular Values
Layer 0
Initialization
Pre-trained
202410−510−2
Layer 1
202410−510−2
Layer 2
202410−510−2
Layer 3
202410−510−2
Layer 4
202410−510−2
Layer 5
0.0 0.5 1.0
n−101Hankel Singular ValuesAttention Layer
2024
n10−510−2
Layer 7
2024
n10−510−2
Layer 8
2024
n10−510−2
Layer 9
2024
n10−510−2
Layer 10
2024
n10−510−2
Layer 11Hankel Singular Values of H3 (125M )FIR Filters
202410−510−2Hankel Singular Values
Layer 0
Initialization
Pre-trained
202410−510−2
Layer 1
202410−510−2
Layer 2
202410−510−2
Layer 3
202410−510−2
Layer 4
202410−510−2
Layer 5
0.0 0.5 1.0
n−101Hankel Singular ValuesAttention Layer
2024
n10−510−2
Layer 7
2024
n10−510−2
Layer 8
2024
n10−510−2
Layer 9
2024
n10−510−2
Layer 10
2024
n10−510−2
Layer 11Hankel Singular Values of H3 (125M )IIR Filters
Figure D.10: Distribution of Hankel singular values for H3long convolution filters. The values decay rapidly.
41

--- PAGE 42 ---
0 1,000 2,00002040
Output sequence lengthFull gener. latency (s) 0 2 4 60204060
Number of Parameters ×109Peak Memory [GBs]
0 1 20246
Number of Parameters ×109Gener. throughput [tok/s] ×103
0 2 4 624
Number of Parameters ×109Full gener. latency (s)Laughing Hyena Hyena Hybrid Attn. H3 Transformer
Figure D.11: Generation latency, throughput and peak memory of Transformers, H3, Hyenaand
Laughing Hyena .
42

--- PAGE 43 ---
E Additional Experiments
E.1 Associative Recall with MultiHyena
We follow the setup of [2] and train 2-layer HyenaandMultiHyena (with 8heads) to solve associative recall
via a standard next-token prediction objective. We focus on the sequence length 64k, high vocabulary size
setting, and push vocabulary sizes past the maximum values considered in [2]. At vocabulary size 60, a
difference between MultiHyena andHyenacan be observed (Table E.1), as experimental support for Theorem
4.1.
Model Accuracy
Hyena 65
MultiHyena 98
Table E.1: Associative recall accuracy, sequence length 64k, vocabulary size 60.
E.2 Analysis of Hankel Singular Values of Pretrained Large Convolution Se-
quence Models
E.3 Model Order Reduction of H3
The H3model is constructed with a combination of diagonal SSMs and shift SSMs. There exists various
classical model order reduction techniques for these different types of SSMs. The following sections aim to
present the formulation and effectiveness of two classical approaches on obtaining the compressed represen-
tation of a H3model. More specifically, we study modal truncation and balanced truncation for compressing
diagonal SSMs and shift SSMs respectively.
E.3.1 Modal Truncation
A discrete diagonal SSM ( A=diag(λ1, . . . , λ d),B∈Cd×1, and C∈C1×d) can be directly converted into a
residue-pole transfer function as follows:
(A,B,C)→H(z) =dX
i=1ri
z−λi, (E.1)
where residue ri=BiCi. Modal truncation aims to compress such a transfer function by essentially reducing
thesummationover dton < d, ofthe nmostinfluentialmodes. Theinfluencefromeachnodecanbeisolated
by expressing it using the h∞norm of the system as follows:
∥H(z)∥∞=dX
i=1ri
z−λi
∞≤dX
i=1|ri|
|1− |λi||. (E.2)
Each mode ican be ranked using the bound formulated above. Subsequently, the d−nlowest modes could
be discarded to form a reduced order model. Figure E.1 illustrates the monotonically decreasing l∞error
with the increase in system order. However, this model reduction approach is only suitable for diagonalizable
SSMs.
E.3.2 Balanced Truncation
A balanced SSM realization is one in which the observability Pand controllability Qgramians are equal
and diagonal. Such a realization can be formulated with the following Lyapunov equations [24]:
AΣA⊤+BB⊤= Σ,
A⊤ΣA+C⊤C= Σ,(E.3)
where P=Q= Σ =diag(σ1, . . . , σ d)andσi≥σi+1.
Results from [59] shows that the n−order model reduction error is bounded by:
E∞≜∥H(s)−Hn(s)∥∞≤2dX
i=d−nσi. (E.4)
43

--- PAGE 44 ---
2123250.00.51.0Normalized /lscript∞Error
Layer 0
Mean
2123250.00.51.0
Layer 1
2123250.00.51.0
Layer 2
2123250.00.51.0
Layer 3
2123250.00.51.0
Layer 4
2123250.00.51.0
Layer 5
212325
Order0.00.51.0Normalized /lscript∞ErrorLayer 6
212325
Order0.00.51.0
Layer 7
212325
Order0.00.51.0
Layer 8
212325
Order0.00.51.0
Layer 9
212325
Order0.00.51.0
Layer 10
212325
Order0.00.51.0
Layer 11Modal Truncation Model Reduction Error of H3Figure E.1: Modal truncation model reduction error ( ||h(t)−hn(t)||∞) across all diagonal SSM layers of the trained H3
125Mmodel.
Therefore, an n−order partition of the full balanced realization can be chosen, such that the discarded orders
are the d−nlowest contributor to the error. The steps taken by [24], computes the n−order partition of
the balanced realization as follows:
1. Form a Hankel matrix Sdfrom the impulse response h1:nof the shift SSM.
2. Obtain the eigenvector matrix V∈Cd×d, and the eigenvalues λ=σ2via the eigen-decomposition of
Sd.
3. Choose the truncated model’s order n < d, based on the bound in Equation E.4.
4. Compute the state-space matrices as follows:
A=V⊤
2:d,1:nV1:d−1,1:n,B=V1,1:n,C=h⊤
1:dV1:d,1:n,D=h0. (E.5)
This model reduction technique was applied to a trained 125 million parameter H3,MultiHyena , and
Hyenamodels as shown in Figures E.2, E.3, and E.4 respectively. It could be noted that the all models
encountered an undesirable non-monotonic error reduction with the increase in order. Moreover, order
reduction configurations such as the one in Figure E.3 layer 15 display signs of numerical instability.
2123250.00.51.0Normalized /lscript∞Error
Layer 0
Mean
2123250.00.51.0
Layer 1
2123250.00.51.0
Layer 2
2123250.00.51.0
Layer 3
2123250.00.51.0
Layer 4
2123250.00.51.0
Layer 5
212325
Order0.000.020.04Normalized /lscript∞ErrorLayer 6
212325
Order0.00.51.0
Layer 7
212325
Order0.00.51.0
Layer 8
212325
Order0.00.51.0
Layer 9
212325
Order0.00.51.0
Layer 10
212325
Order0.00.51.0
Layer 11Balanced Truncation Model Reduction Error of H3
Figure E.2: Balanced truncation model reduction error ( ||h(t)−hn(t)||∞) across all shift SSM layers of the
trained H3 125M model. Note that Layer 6is an Attention layer, therefore balanced truncation model order
reduction is not possible.
44

--- PAGE 45 ---
2124270.00.51.0Normalized /lscript∞Error
Layer 0
Mean
2124270.00.51.0
Layer 1
2124270.00.51.0
Layer 2
2124270.00.51.0
Layer 3
2124270.00.51.0
Layer 4
2124270.00.51.0
Layer 5
2124270.00.51.0Normalized /lscript∞Error
Layer 6
2124270.00.51.0
Layer 7
2124270.00.51.0
Layer 8
2124270.00.51.0
Layer 9
2124270.00.51.0
Layer 10
2124270.00.51.0
Layer 11
212427
Order0.00.51.0Normalized /lscript∞Error
Layer 12
212427
Order0.00.51.0
Layer 13
212427
Order0.00.51.0
Layer 14
212427
Order0.00.51.0
Layer 15
212427
Order0.00.51.0
Layer 16
212427
Order0.00.51.0
Layer 17Balanced Truncation Model Reduction Error of MultiHyenaFigure E.3: Balanced truncation model reduction error ( ||h(t)−hn(t)||∞) across all convolutional layers of
the trained MultiHyena 155M model.
2124270.00.51.0Normalized /lscript∞Error
Layer 0
Mean
2124270.00.51.0
Layer 1
2124270.00.51.0
Layer 2
2124270.00.51.0
Layer 3
2124270.00.51.0
Layer 4
2124270.00.51.0
Layer 5
2124270.00.51.0Normalized /lscript∞Error
Layer 6
2124270.00.51.0
Layer 7
2124270.00.51.0
Layer 8
2124270.00.51.0
Layer 9
2124270.00.51.0
Layer 10
2124270.00.51.0
Layer 11
212427
Order0.00.51.0Normalized /lscript∞Error
Layer 12
212427
Order0.00.51.0
Layer 13
212427
Order0.00.51.0
Layer 14
212427
Order0.00.51.0
Layer 15
212427
Order0.00.51.0
Layer 16
212427
Order0.00.51.0
Layer 17Balanced Truncation Model Reduction Error of Hyena
Figure E.4: Balanced truncation model reduction error ( ||h(t)−hn(t)||∞) across all convolutional layers of
the trained Hyena 155M model.
45
