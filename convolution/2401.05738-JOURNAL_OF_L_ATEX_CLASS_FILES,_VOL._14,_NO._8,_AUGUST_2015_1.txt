# 2401.05738.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2401.05738.pdf
# File size: 1012831 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
Interpreting and Improving Attention From the
Perspective of Large Kernel Convolution
Chenghao Li, Chaoning Zhang, Senior, IEEE, Boheng Zeng, Yi Lu,
Pengbo Shi, Qingzi Chen, Jirui Liu, Lingyun Zhu,
Yang Yang, Senior, IEEE , Heng Tao Shen, Fellow, IEEE
Abstract —Attention mechanisms have significantly advanced
visual models by capturing global context effectively. However,
their reliance on large-scale datasets and substantial computa-
tional resources poses challenges in data-scarce and resource-
constrained scenarios. Moreover, traditional self-attention mech-
anisms lack inherent spatial inductive biases, making them subop-
timal for modeling local features critical to tasks involving smaller
datasets. In this work, we introduce Large Kernel Convolutional
Attention (LKCA), a novel formulation that reinterprets attention
operations as a single large-kernel convolution. This design unifies
the strengths of convolutional architectures—locality and trans-
lation invariance—with the global context modeling capabilities
of self-attention. By embedding these properties into a computa-
tionally efficient framework, LKCA addresses key limitations of
traditional attention mechanisms. The proposed LKCA achieves
competitive performance across various visual tasks, particularly
in data-constrained settings. Experimental results on CIFAR-10,
CIFAR-100, SVHN, and Tiny-ImageNet demonstrate its ability
to excel in image classification, outperforming conventional at-
tention mechanisms and vision transformers in compact model
settings. These findings highlight the effectiveness of LKCA in
bridging local and global feature modeling, offering a practical
and robust solution for real-world applications with limited data
and resources.
Index Terms —ConvNet, Vision Transformer, Large Kernel
CNN, Attention Mechanism
I. I NTRODUCTION
VISION Transformers (ViTs) have emerged as a powerful
backbone network in modern visual models, introduc-
ing a paradigm shift from traditional convolutional archi-
tectures [1]. By leveraging self-attention mechanisms, ViTs
excel at capturing long-range dependencies and global context,
achieving state-of-the-art performance in resource-intensive
tasks [2], [3], [4], [5], [6], [7], [8], [9], [10], such as large-
scale image classification[2], high-resolution segmentation [3],
and advanced feature learning [4]. However, their effective-
ness often comes with substantial computational and data
requirements, making them less ideal for data-constrained
scenarios [4], [11].
Chenghao Li and Chaoning Zhang are with the School of Comput-
ing, Kyung Hee University (e-mail: lch17692405449@gmail.com; chaon-
ingzhang1990@gmail.com;). Boheng Zeng, Yang Yang, and Heng Tao Shen
are with the School of Computer Science and Engineering, University of Elec-
tronic Science and Technology, China (e-mail: zengboheng@std.uestc.edu.cn;
yang.yang@uestc.edu.cn; shenhengtao@hotmail.com); Yi Lu is with the Capi-
tal Normal University (e-mail: 2230501004@cnu.edu.cn;). Pengbo Shi, Qingzi
Chen, Jirui Liu, and Lingyun Zhu are with the Chongqing University of
Technology, China (e-mail: shipengbo@cqut.edu.cn; chenqingzi@cqut.edu.cn;
liujirui@cqut.edu.cn; zly69cv@163.com;)Conversely, Convolutional Neural Networks (CNNs) have
remained a robust choice in environments with limited com-
putational resources or data availability [12], [13]. Due to their
strong inductive biases, such as spatial locality and translation
invariance, CNNs are inherently efficient in learning from
smaller datasets and exhibit robustness to subtle variations in
object positions and orientations [14]. These properties have
made CNNs a go-to solution for tasks in low-resource set-
tings, including embedded vision applications and small-scale
datasets. Given their complementary strengths, CNNs can be
employed to enhance the performance of ViTs, particularly
in data-scarce environments. For instance, CNNs can serve as
efficient feature extractors to preprocess input data for ViTs,
reducing the computational load on the transformer layers [15].
Additionally, hybrid architectures that integrate convolutional
modules within ViTs have demonstrated promise in alleviating
the limitations of ViTs, such as their lack of inherent spatial
inductive bias [15], while retaining their ability to capture
global context [16], [17]. This synergy enables the design
of models that balance computational efficiency with high-
performance capabilities, paving the way for their deployment
across a wider range of visual tasks, from resource-constrained
devices to large-scale systems.
In this work, we introduce a mechanism called Large Ker-
nel Convolutional Attention (LKCA), designed to effectively
combine the strengths of CNNs and ViTs on small datasets.
Following the ViT structure, LKCA utilizes key techniques
such as Patch Embedding, Positional Encoding, Multi-Head
Self-Attention, Feedforward Networks in Transformer Encoder
layers, and MLP Head for task output. To effectively incor-
porate the benefits of CNNs, we employ a simple yet highly
efficient operation in LKCA: replacing the conventional self-
attention operation in ViT with a single large-kernel convolu-
tion operation. This approach not only retains the advantages
of translation invariance and spatial inductive bias in CNNs
but also leverages the global modeling capability and long-
range dependency handling of ViTs. With this large-kernel
convolution, LKCA captures both local and global informa-
tion more effectively, enhancing the model’s representational
power. From the perspective of attention mechanisms, LKCA
can be viewed as an innovative spatial attention method. It ef-
ficiently captures spatial features within local regions through
the large-kernel convolution operation while also benefiting
from the global modeling capabilities of ViT, thus significantly
improving the model’s ability to understand and process com-
plex structures in images. The approach most closely related toarXiv:2401.05738v3  [cs.CV]  2 Dec 2024

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
Fig. 1: Two views to interpret LKCA. The Large Kernel Convolutional Attention (LKCA) can be understood From the
perspective of convolution on the left and attention on the right. The effects of the two approaches are equivalent.
our work is the Visual Attention Network (V AN)[18], which
replaces the multi-head self-attention mechanism with three
types of small kernel convolutions: DW-Conv, DW-DConv,
and 1×1-Conv. By contrast, our method is simpler, employing
a single large kernel convolution[19] to replace MHSA. We
primarily benchmarked small and medium-sized models, as it
was previously believed that ViT outperforms CNNs on large
datasets and large models, while CNNs perform better than
ViTs on ultra-small and specialized datasets. On the CIFAR-
10 dataset, LKCA demonstrate superior improvements over
the standard ViT across various experimental configurations.
More importantly, the LKCA design performs exceptionally
well on downstream tasks. Firstly, in the image classifi-
cation task, we conducted experiments on classic datasets,
including CIFAR-10, CIFAR-100, SVHN, and Tiny-ImageNet,
demonstrating the efficacy of LKCA across these datasets.
Additionally, LKCA achieves remarkable results in semantic
segmentation tasks, particularly with the UPerNet and FPN
algorithms. On the ADE20K dataset, LKCA outperforms
other models in terms of both aAcc and mIoU after 10,000
and 20,000 iterations. Comparisons with multiple backbone
networks further verify the superior performance of LKCA
in both parameter efficiency and accuracy. We attribute the
high performance of LKCA to the complementary strengths
of CNNs and ViTs. By combining CNN’s proficiency in local
feature extraction with ViT’s ability to model global context,
LKCA achieves exceptional performance across various visual
tasks.
In this work, our main contributions are as follows:
•We propose Large Kernel Convolutional Attention
(LKCA), a novel mechanism that integrates large-kernel
convolution into the ViT architecture to combine the local
feature extraction advantages of CNNs with the global
modeling capabilities of ViTs.
•We demonstrate that LKCA achieves strong performance
across various tasks, including image classification on
datasets such as CIFAR-10, CIFAR-100, SVHN, and
Tiny-ImageNet, as well as semantic segmentation onADE20K, outperforming several existing backbone archi-
tectures in terms of accuracy and efficiency.
•We address the limitations of both CNNs and ViTs
by focusing on small and medium-sized models, offer-
ing an effective solution for data-limited and resource-
constrained scenarios.
II. R ELATED WORKS
A. Attention Mechanisms in Visual Models
Attention mechanisms [20], [21], [22], [23], [24], [25],
[26], originally inspired by the way humans subconsciously
focus on and prioritize important aspects of their surroundings,
have become a cornerstone in modern machine learning and
deep learning research. These mechanisms mimic the human
ability to allocate cognitive resources dynamically, making
them highly effective in diverse tasks across Natural Lan-
guage Processing (NLP), computer vision, and beyond. Visual
attention mechanisms [24], [25], a critical subset of these
techniques, can be conceptualized as a process of dynamically
assigning weights to specific features within input images.
This weighting mechanism helps models to focus on the
most relevant parts of an image, significantly improving the
interpretability and performance of various computer vision
models. Visual attention methods are diverse and can be
broadly classified into four primary categories: channel at-
tention [27], [28], [29], [30], spatial attention [31], [32], [33],
temporal attention [34], [35], and branch attention [36], [37],
[38]. These categories often appear in conjunction with other
innovative combinations [39], [40], [41], [42], [43], [44],
[45], reflecting the increasing sophistication of attention-based
methodologies. Non-Local Network [46] successfully employs
self-attention for the first time to simulate non-local relation-
ships in computer vision. CBAM [47] infers attention weights
sequentially along both spatial and channel dimensions and
can seamlessly integrate into any CNN architecture. ECA [27]
module introduces a non-reductive local cross-channel in-
teraction strategy, which efficiently enhances the attention
module’s effectiveness through the effective implementation of

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
1D convolution. EncNet [28] significantly improves the spa-
tial resolution of pixel-level semantic segmentation labels by
introducing a context encoding module. OCNet [31] combines
an interleaved sparse self-attention scheme with a traditional
multi-scale context approach, effectively modeling dense rela-
tionships between pixels and providing richer contextual infor-
mation. SAGAN [32] employs spatial attention to model long-
range dependencies in image generation tasks. SCAN [34]
achieves significantly superior performance in Video Person
Re-Identification by refining both intra-sequence and inter-
sequence feature representations through the introduction of
temporal attention to video sequences. SKNet [36] enhances
the model’s capability to capture objects at different scales by
introducing branch attention to merge multiple branches with
different kernel sizes.
B. ConvNets With Large Kernel
Compared to traditional small-kernel CNNs [12], [48],
which have dominated deep learning models due to their
efficiency and simplicity, large-kernel CNNs [49], [50], [51],
[52], [53], [54], [55], [19], [18] offer a distinct advantage with
their broader effective receptive field and stronger preferences
for capturing global shape information. These characteristics
make large-kernel models particularly appealing for tasks
requiring extensive contextual understanding, such as segmen-
tation and recognition. Despite these advantages, early large-
kernel models, such as the Inception family [49], [50], [51],
gradually lost favor following the success of architectures like
VGG-Net [52], which demonstrated the efficiency of stacking
smaller kernels. This shift in focus led to a temporary decline
in the adoption of large-kernel designs in mainstream applica-
tions. A notable research outcome is the Global Convolutional
Network (GCN) [53], which employs very large convolutional
kernels of 1 ×K and K ×1 to optimize semantic segmentation
tasks. However, there are reports suggesting that large kernels
may compromise ImageNet performance. On the other hand,
the Local Relation Network (LRNet) [54] introduces a spatial
aggregation operator (LRLayer) as a replacement for standard
convolution, which can be seen as a dynamic convolution.
ConvMixer [55], on the contrary, uses convolutional kernels
as large as 9 ×9 to replace the ”mixer” components of ViT
or MLP. Inspired by ViT, RepLKNet [19] utilizes a large
31×31 convolutional kernel, and its outstanding performance
is primarily attributed to the effective receptive fields con-
structed through large kernels. It has been demonstrated that
RepLKNet better exploits shape information compared to
traditional CNNs. The Visual Attention Network (V AN) [18]
employs three types of convolutions—DW-Conv, DW-DConv,
and 1×1 Conv—to expand the receptive field of convolutions
and integrate them into the attention mechanism.
C. ViTs with Large ERF
The strength of ViTs lies in their ability to construct large
Effective Receptive Fields (ERFs) , enabling them to capture
global dependencies effectively. Initially, the Transformer ar-
chitecture [56] was developed for NLP, revolutionizing the
way sequential data was modeled through its self-attentionmechanism. Its migration to the domain of Computer Vi-
sion began with ViT [1], which demonstrated the powerful
representation capabilities of transformers in handling image
data. ViT divides images into patches and treats them as
sequences, allowing the transformer architecture to process
images in a manner analogous to text in NLP. Building on
this foundation, numerous ViT variants [57], [58], [59], [60],
[61], [62], [63], [64], [65], [66] have been proposed to enhance
performance on a wide range of visual tasks. These variants
have introduced various improvements aimed at addressing
the unique challenges posed by image data, particularly the
need to model both global and local dependencies effectively.
The global modeling capabilities of ViTs make them ex-
ceptionally well-suited for capturing long-range relationships
between image patches, effectively creating large receptive
fields. However, the lack of inherent mechanisms for modeling
short-term relationships, as found in CNNs, has been a notable
limitation of ViTs. CNNs excel at capturing local dependencies
through their spatially constrained convolutional operations, a
property that ViTs inherently lack due to their patch-based
processing. To address these challenges, researchers have
focused on enhancing ViT’s ability to model local information,
leading to innovative designs and architectures. Transformer in
Transformer (TNT) [57] enhances the feature representation
capability of local patches through internal attention. Swin
Transformer [58], [59] addresses local modeling challenges in
adapting transformers from language to vision by introducing a
hierarchical architecture with shift windowing. Twins [60] and
CAT [61] alternately perform local and global attention layers
layer-wise. Shuffle Transformer [62], [63] efficiently estab-
lishes inter-window connections through spatial shuffling with
deep convolution and enhances short-term dependency mod-
eling with neighboring window connections. RegionViT [64]
introduces regions and local tokens with different patch sizes,
enhancing local modeling through a region-to-local attention
mechanism.
III. A PPROACH
A. Preliminary
1) Review of Large Kernel Convolution: Ding et al. [19]
revisit the paradigm of large kernel design in modern CNNs,
drawing inspiration from the ViT. Ding et al. assert that em-
ploying a few large convolutional kernels instead of multiple
smaller ones constitutes a more powerful design paradigm.
Building upon this notion, the author introduces a novel CNN
architecture named RepLKNet [19], featuring kernel sizes
as large as 31 ×31. In comparison to commonly used 3 ×3
kernels, RepLKNet achieves performance results equivalent to
or better than the Swin Transformer [58], with lower latency.
The difference between small kernel convolution and large
kernel convolution is shown in Figure 2.
2) Comparision of LKC and MHSA: Convolution possesses
a natural advantage wherein parameters are shared based on
geographical location. Specifically, identical parameters of
the same convolutional kernel act on every position within
the receptive field during the sliding window process. This
introduces equivariance and translation invariance to the con-
volutional operation. However, in Vision Transformer, the

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
attention scores generated by the self-attention mechanism do
not explicitly exhibit positional invariance. Attention weights
are determined by the feature vectors themselves and the intro-
duced positional encodings. While this enhances the model’s
fitting capability, it significantly increases the complexity of
training. Introducing a mechanism for sharing parameters
in the attention mechanism of ViT may bring the expected
benefits but without increasing optimization complexity.
B. Large Kernel Convolutional Attention (LKCA)
1) Shared Weight Position Operation: Firstly, we initialize
a square matrix of size N1
2×2−1as learnable param-
eters, denoted as the shared parameter weights , where N
represents the number of patches, akin to the initialization of
convolutional kernels. Then, we initialize a window of size
N1
2and traverse this window as it slides over the shared
parameter weights matrix from the bottom-right corner to
the top-right corner. At each sliding step, the matrix within
the window is flattened row-wise into a row vector of size
(1, N)and stored. This process is iterated as the window
slides, concatenating the row vectors obtained from each
step. In total, the window slides Ntimes. Consequently, the
concatenated matrix assumes a shape of (N, N ), aligning with
the shape of the attention weight matrix. This concatenated
matrix serves as the new attention score, undergoing matrix
multiplication with the linearly mapped Value matrix. This
step is referred to as the Shared Weight Matrix Attention.
Algorithm 1 presents pseudocode for implementing LKCA
from the attention perspective.
Algorithm 1 Forward propagation process of Attention-ViT
1:function ATTENTION VITFORWARD (X)
2: input: X– input tokens, where X∈Rb×n×d
3: initialize: H, W ←√n,weight ←zero tensor of
shape (H∗2−1, W∗2−1)
4: V←Linear (X)
5: fori= 0 toH−1do
6: forj= 0 toW−1do
7: start row←H−1−i
8: start col←W−1−j
9: window ←weight [start row:start row+
H, start col:start col+W]
10: window vector ←flatten (window )
11: attn score [i∗W+j,:]←window vector
12: end for
13: end for
14: Out←attn score @V
15: return Out
16:end function
2) From the Perspective of Convolution: Implementing
LKCA: Large Kernel Convolutional Attention (LKCA) can be
expressed equivalently as the attention operation with shared
parameters described above in terms of code and mathemati-
cal implementation. Algorithm 2 showcases pseudocode for
implementing LKCA from the perspective of convolution.
The convolutional kernel is a learnable weight matrix of sizeN1
2×2−1, and the input feature map is a matrix Vof
sizeN1
2with a channel count equal to the hidden dimension.
After zero-padding of size N1
2−1, the convolution operation
is applied to the two matrices.
LKCA (x)=Conv2d (MLP (x),LargeKernel )
Algorithm 2 Forward propagation process of Attention-CNN
1:function ATTENTION CNN FORWARD (X)
2: input: X– input tokens, where X∈Rb×n×d
3: initialize: H, W ←√n,weight ←zero tensor of
shape (H∗2−1, W∗2−1)
4: V←Linear (X)
5: V←rearrange (V,′b(HW)d→(bd)HW′)
6: Out←conv2d (V, weight, zero padding = (H−
1, W−1))
7: Out←rearrange (Out,′(bd)HW→b(HW)d′)
8: return Out
9:end function
3) Overall Architecture: The incorporation of large-
kernel convolution attention enhances the model’s ability to
capture positional information. However, it simultaneously
weakens the relationships between patches and within
individual patches. Therefore, we intersperse the large-kernel
convolution attention modules within the original ViT
architecture while retaining the self-attention modules.
The model overview is represented by the following formulas.
An image x∈RH×W×C, where (H, W )is the resolution
of the original image, and Cis the number of channels. A
series of flattened 2D patches xp∈RN×(P2·C), where (P, P)
is the resolution of each image block, and N=HW
P2is the
number of generated blocks. The patches are linearly projected
by weight matrix Eto a D-dimensional space and added
with position embedding Epos. Differing from the original
ViT, we discard the use of the [class] token. Instead of
extracting the [class] token from the final linear layer as
classification information, we adopt the mean of the last layer’s
feature vectors for classification. Additionally, we insert the
Large Kernel Convolution Attention (LKCA) module before
the Multi-Layer Perceptron (MLP) module and apply Layer
Normalization (LN) before each module.
z0=
x1
pE;x2
pE;···;xN
pE
+Epos, (1)
E∈R(P2·C)×D,Epos∈RN×D(2)
z′
ℓ= LKCA (LN ( zℓ−1)) +zℓ−1, ℓ = 1. . . L (3)
zℓ= MLP (LN ( z′
ℓ)) +z′
ℓ, ℓ = 1. . . L (4)
y= LN ( zL) (5)
IV. E XPERIMENTS
A. Experimental Setup
In our experimental setup, we conduct extensive image
classification experiments on four prominent datasets:

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
Fig. 2: Participation differences in kernel convolutions. Illustration of the difference between small kernel convolution and
large kernel convolution by constructing a 5x5 feature map, a 3x3 convolution kernel smaller than the feature map, and a 7x7
convolution kernel larger than the feature map. The distinction between small kernel convolution and large kernel convolution
lies in the fact that, in small kernel convolution, all parameters of the kernel are involved in each correlation operation, while
only a subset of feature map parameters participates in the computation. In the case of large kernel convolution, during each
correlation operation, all parameters of the feature map are involved, and only a subset of the convolution kernel parameters
participates in the computation.
CIFAR-10, CIFAR-100, SVHN, and Tiny-ImageNet. All four
datasets are characterized by low resolution, with image
sizes of 32for CIFAR-10, CIFAR-100, and SVHN, and
64for Tiny-ImageNet. The training regime is consistently
maintained across all experiments, conducted on a single
RTX 2080ti GPU. The training spans 100epochs, employing
4data loading workers, and a batch size of 128. For
optimizer, we choose AdamW [70]. The default learning
rate is set to 0.001 , with a weight decay of 0.05. We utilize
cosine annealing with a warm-up period of 10epochs.
During the training phase, we employ Stochastic Depth [71],
Label Smoothing [50], and Random Erasing [72] with a
probability of 0.25, a maximum erasing area of 0.4, and an
aspect of erasing area of 0.3as regularization strategies to
enhance the robustness and generalization capabilities of our
model. Regarding data augmentation, we employ various
techniques. Specifically, we apply RandomHorizontalFlip,
RandomCrop [73], Mixup [74], and Cutmix [75]. Additionally,
we incorporate Autoaugmentation [76], utilizing the CIFAR10
Policy for CIFAR-10 and CIFAR-100, the SVHN Policy for
SVHN, and the ImageNet Policy for Tiny-ImageNet.B. Comparison of ViT and LKCA.
Initially, we conduct extensive experiments on CIFAR-10,
CIFAR-100, and Tiny-ImageNet to compare the performance
of the original ViT with an LKCA-modified version of ViT
having a similar order of magnitude in terms of parameters.
The purpose of these experiments is to validate the effec-
tiveness of LKCA across various parameter magnitudes and
datasets. The parameter sizes of both the ViT and the LKCA-
modified ViT vary across a range of magnitudes, spanning
from small-scale models with 0.5 million parameters to large-
scale models with 10 million parameters. Specifically, the
parameter magnitudes are explored at levels of 0.5M, 1M,
2M, 4M, and 8M. It is important to note that the benchmark
representing the model size across all variants is fixed at 2.69
million parameters, and it is independent of any subjective
categorization as either large or small. This standardized model
size serves as the reference point for assessing the performance
of the different variants, ensuring a consistent comparison
across the experiment.
Table II presents a performance comparison of ViT and
its LKCA variants on the CIFAR-10 image classification
task, along with their performance across different orders of
magnitude in terms of parameters. The table includes the

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
TABLE I: Comparison of LKCA with Several Mainstream ViT Variants on Different Datasets (% Top-1 accuracy)
Model CIFAR10 CIFAR100 SVHN # Params
ViT-Lite [1] 91.16 70.86 97.53 1.06M
Swin-T [58] 91.21 70.37 96.40 1.01M
CaiT-T [67] 91.99 70.65 97.53 1.00M
MobileViTv2-T [68] 91.46 67.88 96.60 1.00M
LKCA-T 92.49 (Ours) 73.40 (Ours) 97.15 (Ours) 1.03M
ViT-Small [69] 93.36 72.65 97.90 2.69M
Swin-S [58] 93.99 75.34 97.59 2.87M
CaiT-S [67] 94.02 74.70 97.79 2.74M
MobileViTv2-S [68] 93.48 71.70 96.98 3.92M
LKCA-B 94.11 (Ours) 76.50 (Ours) 97.68 (Ours) 2.69M
TABLE II: Performance Comparison of Vision Transformer
and LKCA Variants on Image Classification Tasks on CIFAR-
10(% Top-1 accuracy)
Model Acc # Params
ViT 93.36 2.69M
ViT-LKCA 94.11 (+0.75) 2.69M
VIT/0.5M 87.83 0.48M
ViT/0.5M-LKCA 90.94 (+3.11) 0.50M
VIT/1M 91.16 1.06M
ViT/1M-LKCA 92.49 (+1.33) 1.03M
VIT/2M 92.32 1.96M
ViT/2M-LKCA 93.55 (+1.23) 2.03M
VIT/4M 93.64 3.95M
ViT/4M-LKCA 94.71 (+1.07) 4.05M
VIT/8M 94.46 7.85M
ViT/8M-LKCA 94.88 (+0.42) 7.98M
Top-1 accuracy (Acc), the number of parameters (# Params),
and Floating Point Operations (Flops) for each model. ViT
and ViT-LKCA: The ViT model achieves a Top-1 accuracy
of 93.36%, with 2.69M parameters and 174.25M Flops. The
ViT-LKCA outperforms with a Top-1 accuracy of 94.11%
(an improvement of 0.75 percentage points) and has 2.69M
parameters and 170.81M Flops. Small-scale model (0.5M
parameter magnitude): The ViT/0.5M model has a Top-1
accuracy of 87.83%, 0.48M parameters, and 30.58M Flops.
The ViT/0.5M-LKCA, with LKCA improvement, significantly
TABLE III: Performance Comparison of Vision Transformer
and LKCA Variants on Image Classification Tasks on CIFAR-
100(% Top-1 accuracy)
Model Acc # Params
ViT 72.65 2.69M
ViT-LKCA 76.50 (+3.85) 2.69M
VIT/0.5M 64.68 0.48M
ViT/0.5M-LKCA 69.35 (+4.67) 0.50M
VIT/1M 70.86 1.06M
ViT/1M-LKCA 73.40 (+2.54) 1.03M
VIT/2M 71.17 1.96M
ViT/2M-LKCA 75.94 (+4.77) 2.03M
VIT/4M 72.82 3.95M
ViT/4M-LKCA 76.95 (+4.13) 4.05M
VIT/8M 73.39 7.85M
ViT/8M-LKCA 78.57 (+5.18) 7.98Mimproves accuracy to 90.94% (an increase of 3.11 percentage
points) with 0.50M parameters and 31.34M Flops. Medium-
scale models (1M, 2M, 4M parameter magnitudes): For ViT
and ViT-LKCA models with 1M, 2M, and 4M parameters,
LKCA consistently yields higher Top-1 accuracy. For instance,
ViT/1M-LKCA achieves a 1.33 percentage point improvement,
reaching an accuracy of 92.49%. Large-scale model (8M
parameter magnitude): For ViT and ViT-LKCA models with
8M parameters, the performance gap is narrower. ViT/8M-
LKCA slightly improves Top-1 accuracy by 0.42 percentage
points, reaching 94.88%.
Table III illustrates a comparison between ViT and LKCA
in the context of the CIFAR-100. ViT and ViT-LKCA: The
base ViT model achieves a Top-1 accuracy of 72.65% with
2.69M parameters and 174.25M Flops. In contrast, ViT-LKCA
exhibits superior performance, attaining a Top-1 accuracy of
76.50% (an increase of 3.85 percentage points) with the same
2.69M parameters and 170.81M Flops. Small-scale model
(0.5M parameter magnitude): The ViT/0.5M model achieves a
Top-1 accuracy of 64.68% with 0.48M parameters and 30.58M
Flops. The ViT/0.5M-LKCA, featuring LKCA enhancements,
significantly improves accuracy to 69.35% (a notable increase
of 4.67 percentage points) with 0.50M parameters and 31.34M
Flops. Medium-scale models (1M, 2M, 4M parameter magni-
tudes): ViT and ViT-LKCA models with 1M, 2M, and 4M
parameters consistently benefit from LKCA improvements.
For instance, ViT/2M-LKCA achieves a noteworthy 4.77
percentage point improvement and reaches an accuracy of
75.94%. Large-scale model (8M parameter magnitude): For
ViT and ViT-LKCA models with 8M parameters, the perfor-
mance difference is more subtle. ViT/8M-LKCA exhibits a
huge improvement of 5.18 percentage points, reaching a Top-
1 accuracy of 78.57%.
Table IV provides the comparison on Tiny-ImageNet. ViT
and ViT-LKCA: The baseline ViT model achieves a Top-1 ac-
curacy of 55.74% with 2.69M parameters and 174.25M Flops.
In contrast, the ViT-LKCA model outperforms significantly
with a Top-1 accuracy of 60.95% (an increase of 5.21 percent-
age points) and has the same 2.69M parameters and 170.81M
Flops. Small-scale model (0.5M parameter magnitude): The
ViT/0.5M model achieves a Top-1 accuracy of 46.53% with
0.48M parameters and 30.58M Flops. The ViT/0.5M-LKCA,
incorporating LKCA enhancements, significantly improves ac-
curacy to 50.59% (an increase of 4.06 percentage points) with

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
0.50M parameters and 31.34M Flops. Medium-scale models
(1M, 2M, 4M parameter magnitudes): With ViT/4M-LKCA
achieving a remarkable 6.14 percentage point improvement
and reaching an accuracy of 62.30%. Large-scale model (8M
parameter magnitude): For ViT and ViT-LKCA models with
8M parameters, ViT/8M-LKCA exhibits a significant improve-
ment of 5.99 percentage points, achieving a Top-1 accuracy
of 63.38%.
In summary, the LKCA improvement demonstrates
significant performance gains across various parameter
magnitudes, confirming its effectiveness on ViT models.
C. Comparison of ViT Variants and LKCA.
Next, we compared LKCA with other mainstream variants
of Vision Transformer.
First, we provide a comprehensive comparison of LKCA
with various mainstream ViT variants on different datasets,
including CIFAR-10, CIFAR-100, and SVHN. Each model’s
Top-1 accuracy, number of parameters, and Floating Point
Operations are presented. In the deep ViT, localized ViT, and
lightweight ViT categories, we selected representative models
such as Swin-T [58], CaiT-T [67] and MobileViTv2 [68], to
compare with standard ViT [1] and our proposed LKCA.
From the information in the table I, in the context of
small-scale models, Swin-T and CaiT-T perform similarly in
terms of Top-1 accuracy, while MobileViTv2 lags slightly
behind but possesses a significant advantage in computational
efficiency. Compared to other small-scale models, LKCA-T
achieves a higher Top-1 accuracy. Moving to large-scale
models, it is a common trend that they generally outperform
their small-scale counterparts. LKCA-B stands out by
significantly surpassing ViT and MobileViTv2 in Top-1
accuracy, and it slightly outperforms CaiT-S and Swin-S.
Overall, in CIFAR10 and CIFAR100, LKCA demonstrates
higher Top-1 accuracy compared to other models. However,
there is no clear advantage observed in the SVHN dataset.
TABLE IV: Performance Comparison of Vision Transformer
and LKCA Variants on Image Classification Tasks on Tiny-
ImageNet (% Top-1 accuracy)
Model Acc # Params
ViT 55.74 2.69M
ViT-LKCA 60.95 (+5.21) 2.69M
VIT/0.5M 46.53 0.48M
ViT/0.5M-LKCA 50.59 (+4.06) 0.50M
VIT/1M 53.46 1.06M
ViT/1M-LKCA 57.29 (+3.83) 1.03M
VIT/2M 54.58 1.96M
ViT/2M-LKCA 60.70 (+6.12) 2.03M
VIT/4M 56.16 3.95M
ViT/4M-LKCA 62.30 (+6.14) 4.05M
VIT/8M 57.39 7.85M
ViT/8M-LKCA 63.38 (+5.99) 7.98MTABLE V: Comparison of Various ViT Variants with LKCA
onTiny-ImageNet Dataset (% Top-1 accuracy)
Model Tiny-ImageNet # Params
T2T-T [77] 53.92 1.07M
RvT-T [78] 50.65 1.10M
Swin-T [58] 54.93 1.06M
CaiT-T [67] 54.76 1.03M
XCiT-T [79] 56.78 0.96M
ViT-Lite [1] 53.46 1.11M
DeepViT-T [65] 34.64 0.99M
RegionViT-T [64] 54.32 0.97M
CrossViT-T [80] 47.03 1.04M
LKCA-T 57.29 (Ours) 1.07M
T2T-S [77] 41.25 2.56M
RvT-S [78] 55.51 2.72M
Swin-S [58] 58.61 2.93M
CaiT-S [67] 59.21 2.77M
XCiT-S [79] 60.09 2.81M
ViT-Small [1] 55.74 2.76M
DeepViT-S [65] 44.45 2.54M
Twins SVT-S [60] 37.13 2.76M
RegionViT-S [64] 53.96 2.86M
CrossViT-S [80] 52.70 2.40M
LKCA-B 60.95 (Ours) 2.76M
T2T-B [77] 58.46 13.45M
CvT-B [69] 55.88 6.52M
MobileViTv2 [68] 58.28 8.17M
Twins SVT-B [60] 49.24 9.04M
RegionViT-B [64] 57.83 12.39M
LKCA-L 63.43 (Ours) 12.65M
Finally, we have validated the performance of various ViT
variants and the LKCA model on the Tiny-ImageNet dataset,
while also providing details on the models’ parameter counts
and computational complexities in Table V.
In the category of small-scale models, LKCA-T achieves
a Top-1 accuracy of 57.29%, demonstrating excellent perfor-
mance compared to other models. Its performance surpasses
that of several common small-scale models. Moving to the
medium-scale models, LKCA-B achieves an impressive Top-1
accuracy of 60.95%, once again showcasing outstanding per-
formance. In the realm of large-scale models, LKCA-L attains
a Top-1 accuracy of 63.43%, exhibiting a significant advantage
in accuracy compared to models of the same scale. Overall,
across various scales, LKCA demonstrates outstanding Top-
1 accuracy on the Tiny-ImageNet dataset compared to other
competitors.
D. Semantic Segmentation Results
In this work, we follow prior works to choose image
classification as the main task for evaluating the efficacy of
model architecutres. Nonetheless, we additionally provide
results of semantic segmentation on the ADE20K [81] dataset.
Specifically, we use MMSegmentation [82] to conduct our
experiments. All our experiments are testing two algorithms:
UperNet [83] and FPN [84]. We refrain from using pre-
training and instead train the models from scratch on the
dataset. Regarding evaluation metrics, we assess the models
based on aAcc and mIoU in 10high-score categories. In

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
TABLE VI: Semantic Segmentation of Vanilla ViT and LKCA on ADE20K Dataset
Backbone Iteration Algorithm aAcc(%) mIoU(%) # Params
ViT-B 2000 UPerNet 40.90 22.28 0.144G
LKCA-B 2000 UPerNet 45.48 24.65 0.102G
ViT-B 4000 UPerNet 47.41 28.08 0.144G
LKCA-B 4000 UPerNet 52.67 30.00 0.102G
ViT-B 8000 UPerNet 53.43 31.73 0.144G
LKCA-B 8000 UPerNet 50.79 33.73 0.102G
ViT-B 16000 UPerNet 61.55 39.33 0.144G
LKCA-B 16000 UPerNet 65.49 40.60 0.102G
TABLE VII: Semantic Segmentation of Various ViT Variants and LKCA on ADE20K Dataset
Backbone Iteration Algorithm aAcc(%) mIoU(%) # Params Flops
Vit-S 10000 UPerNet 55.55 32.92 58M 67G
ResNet-50 10000 UPerNet 58.12 35.35 64M 238G
Twins PCPVT-S 10000 UPerNet 52.56 29.64 53M 234G
Swin-S 10000 UPerNet 57.18 35.30 52M 231G
LKCA-S 10000 UPerNet 60.16 37.16 51M 66G
Vit-S 10000 FPN 56.56 35.84 37M 76G
Twins PCPVT-S 10000 FPN 56.74 33.09 33M 50G
LKCA-S 10000 FPN 62.55 39.03 30M 78G
Vit-S 20000 UPerNet 58.51 36.61 58M 67G
ResNet-50 20000 UPerNet 62.76 37.84 64M 238G
Twins PCPVT-S 20000 UPerNet 58.91 38.00 53M 234G
Swin-S 20000 UPerNet 58.00 37.57 52M 231G
LKCA-S 20000 UPerNet 64.37 40.09 51M 66G
Vit-S 20000 FPN 63.36 41.64 37M 76G
Twins PCPVT-S 20000 FPN 58.51 36.61 33M 50G
LKCA-S 20000 FPN 67.21 41.81 30M 78G
the ablation study of the backbone network, we examine
various popular variants of ViT and CNN and explore their
performance during 0 - 20,000 iterations.
We conduct experiments on multiple backbone networks.
Table VI provides performance metrics for the UPerNet
algorithm using different backbones (ViT and LKCA),
iteration algorithms, and iteration counts ( 2000, 4000, 8000,
16000 ). At each iteration count, LKCA-B consistently
outperforms ViT-B in terms of aAcc and mIoU, indicating
superior performance of LKCA-B in the UPerNet task.
As the iteration count increases, both models show an
improvement in performance, but at each iteration count,
LKCA-B maintains a leading position. Overall, LKCA-B
demonstrates superior performance compared to ViT-B at
different iteration counts, with higher pixel accuracy and
mean Intersection over Union. Additionally, LKCA-B exhibits
advantages in terms of parameter count and computational
complexity.
Table VII presents a performance comparison among differ-
ent backbones (ViT-S, ResNet-50, Swin-S, Twins PCPVT-S,
LKCA-S) under various iteration counts and across two dif-
ferent task settings (UPerNet and FPN). In the UPerNet task,
LKCA-S consistently outperforms other models at different
iteration counts, exhibiting higher aAcc and mIoU. It achievessuperior performance compared to other models. Similarly, in
the FPN task, LKCA-S demonstrates excellent performance
across different iteration counts, showcasing higher aAcc and
mIoU, making it competitive against other models. LKCA-S
consistently exhibits outstanding performance across different
tasks and iteration settings, while maintaining relatively lower
computational complexity and parameter counts.
V. C ONCLUSION
In this study, we re-examine the relationship between the
attention mechanism in visual transformers and the large ker-
nel convolutional networks, proposing a new visual attention
called Large Kernel Convolutional Attention (LKCA) aimed
for small datasets. It employs a single large kernel convo-
lution to simplify attention operations. In comparison to the
previous approach, Large Kernel Attention (LKA), which uses
multiple small kernel convolutions, our method achieves the
same effects but is more straightforward and simpler. LKCA
combines the advantages of convolutional neural networks
and visual transformers, featuring a larger receptive field,
locality, and parameter sharing. We explained the superiority
of LKCA from the perspectives of both convolution and atten-
tion, providing equivalent code implementations for each view.
Experimental results demonstrate that LKCA, implemented
from both convolutional and attention perspectives, exhibits
comparable performance. We extensively experimented with

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
LKCA variants of ViT on multiple benchmark datasets, in-
cluding CIFAR-10, CIFAR-100, SVHN, Tiny-ImageNet, and
ADE20K, for classification and segmentation tasks. The exper-
imental outcomes reveal that LKCA demonstrates competitive
performance in visual tasks.
REFERENCES
[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[2] L. Yuan, Q. Hou, Z. Jiang, J. Feng, and S. Yan, “V olo: Vision outlooker
for visual recognition,” IEEE transactions on pattern analysis and
machine intelligence , vol. 45, no. 5, pp. 6575–6586, 2022.
[3] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
and L. Shao, “Pyramid vision transformer: A versatile backbone for
dense prediction without convolutions,” in Proceedings of the IEEE/CVF
international conference on computer vision , 2021, pp. 568–578.
[4] K. He, X. Chen, S. Xie, Y . Li, P. Doll ´ar, and R. Girshick, “Masked au-
toencoders are scalable vision learners,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2022, pp.
16 000–16 009.
[5] J. Jiao, Y .-M. Tang, K.-Y . Lin, Y . Gao, A. J. Ma, Y . Wang, and
W.-S. Zheng, “Dilateformer: Multi-scale dilated transformer for visual
recognition,” IEEE Transactions on Multimedia , vol. 25, pp. 8906–8919,
2023.
[6] T. Zhu, L. Li, J. Yang, S. Zhao, H. Liu, and J. Qian, “Multimodal sen-
timent analysis with image-text interaction network,” IEEE transactions
on multimedia , vol. 25, pp. 3375–3385, 2022.
[7] J. Chen, J. Ding, and J. Ma, “Hitfusion: Infrared and visible image
fusion for high-level vision tasks using transformer,” IEEE Transactions
on Multimedia , 2024.
[8] C. Deng, M. Wang, L. Liu, Y . Liu, and Y . Jiang, “Extended feature
pyramid network for small object detection,” IEEE Transactions on
Multimedia , vol. 24, pp. 1968–1979, 2021.
[9] Q. Xu, J. Wang, B. Jiang, and B. Luo, “Fine-grained visual classification
via internal ensemble learning transformer,” IEEE Transactions on
Multimedia , vol. 25, pp. 9015–9028, 2023.
[10] H. Ma, J. Wang, H. Lin, B. Zhang, Y . Zhang, and B. Xu, “A transformer-
based model with self-distillation for multimodal emotion recognition in
conversations,” IEEE Transactions on Multimedia , 2023.
[11] S. H. Lee, S. Lee, and B. C. Song, “Vision transformer for small-size
datasets,” arXiv preprint arXiv:2112.13492 , 2021.
[12] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770–778.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” Advances in neural informa-
tion processing systems , vol. 25, 2012.
[14] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . Le-
Cun, “Overfeat: Integrated recognition, localization and detection using
convolutional networks,” arXiv preprint arXiv:1312.6229 , 2013.
[15] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi,
“Escaping the big data paradigm with compact transformers,” arXiv
preprint arXiv:2104.05704 , 2021.
[16] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy,
“Do vision transformers see like convolutional neural networks?” Ad-
vances in Neural Information Processing Systems , vol. 34, pp. 12 116–
12 128, 2021.
[17] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H.
Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language
representation learning with noisy text supervision,” in International
conference on machine learning . PMLR, 2021, pp. 4904–4916.
[18] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and S.-M. Hu, “Visual
attention network,” Computational Visual Media , vol. 9, no. 4, pp. 733–
752, 2023.
[19] X. Ding, X. Zhang, J. Han, and G. Ding, “Scaling up your kernels
to 31x31: Revisiting large kernel design in cnns,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2022, pp. 11 963–11 975.
[20] E. A. Nadaraya, “On estimating regression,” Theory of Probability &
Its Applications , vol. 9, no. 1, pp. 141–142, 1964.
[21] G. S. Watson, “Smooth regression analysis,” Sankhy ¯a: The Indian
Journal of Statistics, Series A , pp. 359–372, 1964.[22] A. Galassi, M. Lippi, and P. Torroni, “Attention in natural language pro-
cessing,” IEEE transactions on neural networks and learning systems ,
vol. 32, no. 10, pp. 4291–4308, 2020.
[23] K. Cho, A. Courville, and Y . Bengio, “Describing multimedia content
using attention-based encoder-decoder networks,” IEEE Transactions on
Multimedia , vol. 17, no. 11, pp. 1875–1886, 2015.
[24] F. Wang and D. M. Tax, “Survey on the attention based rnn model and
its applications in computer vision,” arXiv preprint arXiv:1601.06823 ,
2016.
[25] M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang, T.-J. Mu, S.-
H. Zhang, R. R. Martin, M.-M. Cheng, and S.-M. Hu, “Attention
mechanisms in computer vision: A survey,” Computational visual media ,
vol. 8, no. 3, pp. 331–368, 2022.
[26] X. Yang, S. Feng, D. Wang, and Y . Zhang, “Image-text multimodal
emotion classification via multi-view attentional network,” IEEE Trans-
actions on Multimedia , vol. 23, pp. 4014–4026, 2020.
[27] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “Eca-net:
Efficient channel attention for deep convolutional neural networks,”
inProceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2020, pp. 11 534–11 542.
[28] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal, “Context encoding for semantic segmentation,” in Pro-
ceedings of the IEEE conference on Computer Vision and Pattern
Recognition , 2018, pp. 7151–7160.
[29] Z. Chen, Y . Li, S. Bengio, and S. Si, “You look twice: Gaternet for
dynamic filter selection in cnns,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2019, pp.
9172–9180.
[30] H. Shi, G. Lin, H. Wang, T.-Y . Hung, and Z. Wang, “Spsequencenet:
Semantic segmentation network on 4d point clouds,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2020, pp. 4574–4583.
[31] Y . Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang,
“Ocnet: Object context network for scene parsing,” arXiv preprint
arXiv:1809.00916 , 2018.
[32] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention
generative adversarial networks,” in International conference on machine
learning . PMLR, 2019, pp. 7354–7363.
[33] V . Mnih, N. Heess, A. Graves et al. , “Recurrent models of visual
attention,” Advances in neural information processing systems , vol. 27,
2014.
[34] R. Zhang, J. Li, H. Sun, Y . Ge, P. Luo, X. Wang, and L. Lin, “Scan: Self-
and-collaborative attention network for video person re-identification,”
IEEE Transactions on Image Processing , vol. 28, no. 10, pp. 4870–4882,
2019.
[35] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, “Video person re-
identification with competitive snippet-similarity aggregation and co-
attentive snippet embedding,” in Proceedings of the IEEE conference
on computer vision and pattern recognition , 2018, pp. 1169–1178.
[36] X. Li, W. Wang, X. Hu, and J. Yang, “Selective kernel networks,”
inProceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2019, pp. 510–519.
[37] H. Zhang, C. Wu, Z. Zhang, Y . Zhu, H. Lin, Z. Zhang, Y . Sun, T. He,
J. Mueller, R. Manmatha et al. , “Resnest: Split-attention networks,”
inProceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2022, pp. 2736–2746.
[38] Y . Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, “Dynamic
convolution: Attention over convolution kernels,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , 2020,
pp. 11 030–11 039.
[39] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S.
Chua, “Sca-cnn: Spatial and channel-wise attention in convolutional
networks for image captioning,” in Proceedings of the IEEE conference
on computer vision and pattern recognition , 2017, pp. 5659–5667.
[40] J. Park, S. Woo, J.-Y . Lee, and I. S. Kweon, “Bam: Bottleneck attention
module,” arXiv preprint arXiv:1807.06514 , 2018.
[41] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang,
and X. Tang, “Residual attention network for image classification,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2017, pp. 3156–3164.
[42] D. Misra, T. Nalamada, A. U. Arasanipalai, and Q. Hou, “Rotate to
attend: Convolutional triplet attention module,” in Proceedings of the
IEEE/CVF winter conference on applications of computer vision , 2021,
pp. 3139–3148.
[43] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, “Dual attention
network for scene segmentation,” in Proceedings of the IEEE/CVF

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
conference on computer vision and pattern recognition , 2019, pp. 3146–
3154.
[44] W. Li, X. Zhu, and S. Gong, “Harmonious attention network for person
re-identification,” in Proceedings of the IEEE conference on computer
vision and pattern recognition , 2018, pp. 2285–2294.
[45] C. Yan, Y . Tu, X. Wang, Y . Zhang, X. Hao, Y . Zhang, and Q. Dai,
“Stat: Spatial-temporal attention mechanism for video captioning,” IEEE
transactions on multimedia , vol. 22, no. 1, pp. 229–241, 2019.
[46] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-
works,” in Proceedings of the IEEE conference on computer vision and
pattern recognition , 2018, pp. 7794–7803.
[47] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional
block attention module,” in Proceedings of the European conference on
computer vision (ECCV) , 2018, pp. 3–19.
[48] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition , 2017, pp. 4700–4708.
[49] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi, “Inception-v4,
inception-resnet and the impact of residual connections on learning,” in
Proceedings of the AAAI conference on artificial intelligence , vol. 31,
no. 1, 2017.
[50] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition , 2016, pp.
2818–2826.
[51] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
inProceedings of the IEEE conference on computer vision and pattern
recognition , 2015, pp. 1–9.
[52] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.
[53] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, “Large kernel matters–
improve semantic segmentation by global convolutional network,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2017, pp. 4353–4361.
[54] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image
recognition,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision , 2019, pp. 3464–3473.
[55] A. Trockman and J. Z. Kolter, “Patches are all you need?” arXiv preprint
arXiv:2201.09792 , 2022.
[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, 2017.
[57] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer
in transformer,” Advances in Neural Information Processing Systems ,
vol. 34, pp. 15 908–15 919, 2021.
[58] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: Hierarchical vision transformer using shifted
windows,” in Proceedings of the IEEE/CVF international conference on
computer vision , 2021, pp. 10 012–10 022.
[59] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and
B. Guo, “Cswin transformer: A general vision transformer backbone
with cross-shaped windows,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 2022, pp. 12 124–
12 134.
[60] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and
C. Shen, “Twins: Revisiting the design of spatial attention in vision
transformers,” Advances in Neural Information Processing Systems ,
vol. 34, pp. 9355–9366, 2021.
[61] H. Lin, X. Cheng, X. Wu, and D. Shen, “Cat: Cross attention in vision
transformer,” in 2022 IEEE International Conference on Multimedia and
Expo (ICME) . IEEE, 2022, pp. 1–6.
[62] Z. Huang, Y . Ben, G. Luo, P. Cheng, G. Yu, and B. Fu, “Shuffle
transformer: Rethinking spatial shuffle for vision transformer,” arXiv
preprint arXiv:2106.03650 , 2021.
[63] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, “Msg-
transformer: Exchanging local spatial information by manipulating
messenger tokens,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 12 063–12 072.
[64] C.-F. Chen, R. Panda, and Q. Fan, “Regionvit: Regional-to-local atten-
tion for vision transformers,” arXiv preprint arXiv:2106.02689 , 2021.
[65] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and
J. Feng, “Deepvit: Towards deeper vision transformer,” arXiv preprint
arXiv:2103.11886 , 2021.
[66] P. Wang, X. Wang, F. Wang, M. Lin, S. Chang, H. Li, and R. Jin, “Kvt:
k-nn attention for boosting vision transformers,” in European conference
on computer vision . Springer, 2022, pp. 285–302.[67] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J ´egou, “Go-
ing deeper with image transformers,” in Proceedings of the IEEE/CVF
international conference on computer vision , 2021, pp. 32–42.
[68] S. Mehta and M. Rastegari, “Separable self-attention for mobile vision
transformers,” arXiv preprint arXiv:2206.02680 , 2022.
[69] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,
“Cvt: Introducing convolutions to vision transformers,” in Proceedings
of the IEEE/CVF international conference on computer vision , 2021,
pp. 22–31.
[70] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.
[71] G. Huang, Y . Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep
networks with stochastic depth,” in Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, October 11–14,
2016, Proceedings, Part IV 14 . Springer, 2016, pp. 646–661.
[72] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y . Yang, “Random erasing
data augmentation,” in Proceedings of the AAAI conference on artificial
intelligence , vol. 34, no. 07, 2020, pp. 13 001–13 008.
[73] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv
preprint arXiv:1605.07146 , 2016.
[74] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond
empirical risk minimization,” arXiv preprint arXiv:1710.09412 , 2017.
[75] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y . Yoo, “Cutmix: Reg-
ularization strategy to train strong classifiers with localizable features,”
inProceedings of the IEEE/CVF international conference on computer
vision , 2019, pp. 6023–6032.
[76] E. D. Cubuk, B. Zoph, D. Mane, V . Vasudevan, and Q. V . Le, “Autoaug-
ment: Learning augmentation strategies from data,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2019, pp. 113–123.
[77] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z.-H. Jiang, F. E. Tay, J. Feng,
and S. Yan, “Tokens-to-token vit: Training vision transformers from
scratch on imagenet,” in Proceedings of the IEEE/CVF international
conference on computer vision , 2021, pp. 558–567.
[78] J. Su, M. Ahmed, Y . Lu, S. Pan, W. Bo, and Y . Liu, “Roformer: En-
hanced transformer with rotary position embedding,” Neurocomputing ,
vol. 568, p. 127063, 2024.
[79] A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin,
I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek et al. , “Xcit: Cross-
covariance image transformers,” Advances in neural information pro-
cessing systems , vol. 34, pp. 20 014–20 027, 2021.
[80] C.-F. R. Chen, Q. Fan, and R. Panda, “Crossvit: Cross-attention multi-
scale vision transformer for image classification,” in Proceedings of the
IEEE/CVF international conference on computer vision , 2021, pp. 357–
366.
[81] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and
A. Torralba, “Semantic understanding of scenes through the ade20k
dataset,” International Journal of Computer Vision , vol. 127, pp. 302–
321, 2019.
[82] M. Contributors, “MMSegmentation: Openmmlab semantic seg-
mentation toolbox and benchmark,” https://github.com/open-mmlab/
mmsegmentation, 2020.
[83] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun, “Unified perceptual
parsing for scene understanding,” in Proceedings of the European
conference on computer vision (ECCV) , 2018, pp. 418–434.
[84] A. Kirillov, R. Girshick, K. He, and P. Doll ´ar, “Panoptic feature pyramid
networks,” in Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , 2019, pp. 6399–6408.
