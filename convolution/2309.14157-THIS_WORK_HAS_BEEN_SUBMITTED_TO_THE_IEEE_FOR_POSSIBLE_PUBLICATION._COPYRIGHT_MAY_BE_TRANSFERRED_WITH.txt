# 2309.14157.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2309.14157.pdf
# File size: 1288692 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 1
LAPP: Layer Adaptive Progressive Pruning for
Compressing CNNs from Scratch
Pucheng Zhai, Kailing Guo∗,Member, IEEE, Fang Liu, Member, IEEE,
Xiaofen Xing, Member, IEEE, Xiangmin Xu, Senior Member, IEEE,
Abstract —Structured pruning is a commonly used convolu-
tional neural network (CNN) compression approach. Pruning
rate setting is a fundamental problem in structured pruning.
Most existing works introduce too many additional learnable
parameters to assign different pruning rates across different
layers in CNN or cannot control the compression rate explicitly.
Since too narrow network blocks information flow for training,
automatic pruning rate setting cannot explore a high pruning
rate for a specific layer. To overcome these limitations, we propose
a novel framework named Layer Adaptive Progressive Pruning
(LAPP), which gradually compresses the network during initial
training of a few epochs from scratch. In particular, LAPP
designs an effective and efficient pruning strategy that introduces
a learnable threshold for each layer and FLOPs constraints
for network. Guided by both task loss and FLOPs constraints,
the learnable thresholds are dynamically and gradually updated
to accommodate changes of importance scores during training.
Therefore the pruning strategy can gradually prune the network
and automatically determine the appropriate pruning rates for
each layer. What’s more, in order to maintain the expressive
power of the pruned layer, before training starts, we introduce an
additional lightweight bypass for each convolutional layer to be
pruned, which only adds relatively few additional burdens. Our
method demonstrates superior performance gains over previous
compression methods on various datasets and backbone archi-
tectures. For example, on CIFAR-10, our method compresses
ResNet-20 to 40.3 %without accuracy drop. 55.6 %of FLOPs of
ResNet-18 are reduced with 0.21 %top-1 accuracy increase and
0.40%top-5 accuracy increase on ImageNet.
Index Terms —Progressive pruning, Learnable threshold,
FLOPs constraints, Bypass.
I. I NTRODUCTION
IN recent years, convolutional neural networks (CNNs) have
made remarkable achievements in many computer vision
applications such as classification [1], [2], object detection
[3], [4], face recognition [5], [6], action recognition [7],
and semantic segmentation [8]. However, while performance
of CNN models continues to get better, the CNNs become
deeper and wider, which results in an explosive growth in
the parameters and FLOPs. Thus, existing CNNs require very
high storage and computational costs, which makes it difficult
for CNNs to be deployed on small devices with limited
resources. In order to solve this problem, CNN compression
Pucheng Zhai, and Xiaofen Xing are with South China University of
Technology, Guangzhou 510640, China (email:pucheng zhai@163.com; xfx-
ing@scut.edu.cn). Kailing Guo and Xiangmin Xu are with South China Uni-
versity of Technology, Guangzhou 510640, China, and with the Pazhou Lab,
Guangzhou 510330, China (email:guokl@scut.edu.cn; xmxu@scut.edu.cn).
Fang Liu is with Guangdong University of Finance, Guangzhou 510521, China
(e-mail: 47-032@gduf.edu.cn).
∗Corresponding author.techniques have attracted more and more attention. Common
CNN compression techniques include pruning [9]–[11], low-
rank tensor decomposition [12]–[14], knowledge distillation
[15], [16], low-bit quantization [17]–[19], and compact archi-
tecture design [20]–[22]. In this paper, we focus on pruning
which is a research hotspot.
Pruning can be divided into unstructured and structured
pruning. Unstructured pruning directly removes unimportant
weight elements in a filter independently by some importance
metrics [23], [24], which results in irregular sparsity. It is
difficult for irregular structure to leverage existing software
and hardware to get an actual speedup. In contrast, structured
pruning directly removes the whole unimportant filters (or
channels, layers, blocks, etc.) based on different importance
metrics [25], [26], and achieves structured sparsity. Therefore,
structured pruning, which is able to achieve actual acceleration
based on existing software and hardware, has been rapidly
developing in recent years. Although existing structured prun-
ing methods, especially filter pruning and channel pruning
methods, have achieved inspiring results, there still exists two
problems.
Firstly, pruning rate setting is a fundamental problem. The
redundancy of different convolutional layers is different, but
many pruning methods [27]–[29] ignore this and set the same
pruning rates for all layers, which may not produce a proper
pruning structure. In order to assign different pruning rates
to different layers, some works [11], [30] set a fixed global
metric threshold, and distinguish important filters based on
corresponding importance metrics. However, when given a
target compression rate, the threshold need be set by trial and
error. What’s more, the global threshold is not appropriate for
all layers due to the different distribution of metric values
in each layer. But manually setting different thresholds for
each layer requires more labor costs and causes the suboptimal
pruning rates distribution.
By making metric thresholds of each layer learnable, soft
threshold reparameterization (STR) [31] is able to automati-
cally set thresholds for each layer and obtain better pruning
rates distribution. The learning of the thresholds in STR is
controlled by the task loss and weight decay regularization.
However, by adjusting the weight-decay coefficient, STR
cannot explicitly control the thresholds to make the pruned
network exactly reach a target compression rate. In order to ex-
plicitly control the pruned network to meet the target FLOPs,
some other studies [32], [33] introduce FLOPs constraints to
guide the learning of the pruned structure. To represent the
pruned structure, they incorporate an extra learnable parameterarXiv:2309.14157v1  [cs.CV]  25 Sep 2023

--- PAGE 2 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 2
Fig. 1. The overview of our method. Here, for brevity, batch normalization (BN) layers and nonlinear activation layers are omitted. Before training from
scratch, each convolutional layer to be pruned in the baseline CNN is replaced with the sparse module with bypass compensation to build the SBCNet. Then
the SBCNet is trained and compressed with masks by the proposed pruning strategy. After meeting the target compression rate, the binary sparse masks are
removed, the SBCNet is converted to the compressed network, and then the compressed network is trained for the remaining epochs.
for each channel in each layer to form a differentiable gate.
By using FLOPs constraints, they can progressively and auto-
matically assign a different pruning rate to each layer during
training based on the target FLOPs. However, they introduce
too many additional learnable parameters, which makes the
optimization of the pruned structure difficult.
Secondly, automatic pruning rate setting cannot explore a
high pruning rate for a specific layer. Most pruning methods
[11], [32], [34], [35] try to identify the least important filters
or channels as possible to reduce the loss of information
after pruning, and then simply remove them. However, as the
pruning rates get higher and higher, the pruned layers lose
more and more information and capacity. Especially, when
the pruning rates are set very high, these pruned too narrow
layers have limited expressive power and probably block the
propagation of features and gradients, which may make it
difficult for the pruned network to restore performance even
after a lot of time of training or fine-tuning.
In order to deal with the above problems, in this paper, we
propose a novel Layer Adaptive Progressive Pruning (LAPP)
framework, which consists of module design and pruning strat-
egy. An overview of our method is shown in Fig. 1. To deal
with the above first problem, i.e., pruning rate setting problem,
our pruning strategy takes metric thresholds as learnable
parameters and introduces FLOPs constraints to control their
updates. Specifically, we use ℓ1-norm as the filter importance
metric. We introduce a learnable threshold for each layer
to distinguish the importance between filters, which avoids
introducing too many additional learnable parameters. During
training, guided by both task loss and FLOPs constraints, the
learnable thresholds are dynamically updated to accommodate
changes of the filters norm. According to the relative size
between the thresholds and the filters norm, our method can
progressively automatically distinguish important filters and
determine appropriate pruning rates for each layer in each
iteration until the target compression rate is met, which greatlysaves trial costs.
To address the above second problem, before training
starts, by introducing an additional lightweight bypass for
the convolutional layer (i.e., the sparse path) to be pruned
in the baseline network, we design a Sparse module with
Bypass Compensation, named SBC module, and build the
SBCNet , as shown in Fig. 2. Then we only progressively
prune the sparse paths and keep the bypasses throughout the
training to compensate the sparse paths. Therefore, even if
the sparse path get very narrow, the SBC module can also
have strong expressive power and guarantee the propagation
of features and gradients. In this paper, in order to make the
lightweight bypass have sufficient expressive power, we finally
use the block inspired by MobileNetV2 [36] as the bypass.
Specifically, the block sequentially contains three layers of
small convolutions: 1×1convolution, depthwise convolution,
and1×1convolution.
Last but not least, our pruning strategy is a pruning during
training algorithm. Specifically, our pruning strategy only
takes up a few epochs in the early stages of training to
prune the network gradually, and then continues to train
the pruned network that meets the target compression rate
for the remaining epochs. Therefore, our pruning strategy
is able to reduce substantial pre-training burdens of post-
training pruning methods [32], [37], [38] and generate efficient
sub-networks with less accuracy degradation than pruning at
initialization methods [11].
To demonstrate the effect of our method, we conduct
experiments on two popular image classification datasets with
some representative network structures. Compared to the state-
of-the-art methods, LAPP achieves superior performance. For
example, we obtain 59.7 %FLOPs reduction by removing
52.8%parameters without accuracy drop on ResNet-20 com-
pared to baseline. Compared to baseline, 55.6 %of FLOPs of
ResNet-18 are reduced with 0.21 %top-1 accuracy increase
and 0.40 %top-5 accuracy increase on ImageNet.

--- PAGE 3 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 3
Our main contributions are summarized as follows:
(a) Combining learnable thresholds and FLOPs constraints,
we design an effective and efficient pruning strategy which
can prune the network gradually during training from scratch
and determine appropriate pruning rates automatically for each
layer.
(b) By introducing an additional bypass for each convolu-
tional layer, our method can compensate the lost information
of the pruned layer and guarantee the information flow of the
pruned narrow network during training, so as to be able to
explore a high pruning rate for a specific layer.
(c) Our method can be applied to various CNNs, such as
VGG [1], ResNet [2], GoogleNet [39], and DenseNet [40],
and achieves state-of-the-art performance on those networks,
which shows its effectiveness.
The rest of the paper is organized as follows. In Section II,
we introduce related work. In Section III, we provide a detailed
description of our method. Then in Section IV , we conduct
experiments on two popular image classification datasets with
different types of CNNs and compare the results with those of
the state-of-the-art methods. Finally, we conclude in Section
V .
II. RELATED WORK
A. Pruning for Network Compression
Pruning Pipelines. Depending on when pruning is per-
formed, most pruning methods can be divided into three
categories [34]: 1) post-training pruning, 2) pruning at initial-
ization, 3) pruning during training. The post-training pruning
methods [9], [25], [32], [35], [37], [38], [41] often adopt
a three-stage pipeline, namely densely pre-training, pruning,
and fine-tuning, to compress networks. Because of pre-training
over-parameterized networks and fine-tuning pruned networks,
post-training pruning methods usually generate efficient sub-
networks with plausible accuracy, but result in huge training
burdens. Different from post-training pruning methods, prun-
ing at initialization methods [11], [42]–[44] are a two-stage
training pipeline, namely pruning the randomly initialized
network and then training the pruned network from scratch,
which greatly avoids the pre-training burdens. However, ex-
isting methods still remain unsatisfactory as the resulting
pruned networks can be difficult to train [43] and easily suffer
significant accuracy degradation.
Pruning during training methods [34], [45], [46] aim to
combine the benefits of the above two strategies. Pruning
during training methods can be further split into two directions
[34]: regularization-based methods [45], [30] and sub-ticket
selection methods [34], [46]. Similar to post-training pruning
methods, regularization-based methods also adopt a three-
stage pipeline, namely training with sparsity regularization,
pruning, and slightly fine-tuning. Regularization-based meth-
ods spend less time on fine-tuning the pruned network, but
the training process under sparsity regularization still causes
substantial computational burdens. Sub-ticket selection meth-
ods select the pruned network structure via some importance
metrics when performing pruning, but usually rely on some ex-
perience or assumptions to heuristically set the starting pointsof pruning during normal training. Different from previous
sub-ticket selection methods, based on learnable metric thresh-
olds, pruning in our pruning strategy is automatically started
during training. Different from previous regularization-based
methods, our pruning strategy only takes up a few epochs in
the early stages of training to prune the network progressively,
and then reduce more training burdens by training the pruned
network for the remaining epochs.
Pruning Metrics. Pruning metrics are used to measure the
importance or redundancy of filters (or channels). Depending
on whether they introduce extra learnable parameters, pruning
metrics of some representative structured pruning methods
can be roughly grouped into two categories. Pruning metrics
without extra parameters can be further split into two subcat-
egories: data independent metrics and data dependent metrics.
Some works [9], [27], [28], [45], [47]–[49] directly use
inherent properties of filters, such as the ℓ1-norm or ℓ2-norm
values of filters and the relationships among filters, as pruning
metrics. Based on the assumption that filters with smaller
norms are more unimportant, pruning filters for efficient con-
vnets (PFEC) [9] selects and removes the filters with smaller
ℓ1-norm values. Filter pruning via geometric median (FPGM)
[28] points out that the above assumption is not always true,
and then turns to pruning the most replaceable filters based on
the geometric median of filters in each layer. Considering that
similar filters are redundant, different from FPGM, filter prun-
ing with adaptive gradient learning (FP-AGL) [49] imposes the
redesigned centripetal vectors on filters to converge filters in
the clusters to the same point, and finally keeps only one filter
per cluster. Some works [30], [50] leverage the scaling factors
in batch normalization (BN) layers to judge the importance
of channels. Network slimming (NS) [30] pushes the scaling
factors in BN layers to zero by imposing ℓ1regularization
during training, and prunes channels with small scaling factors
after training. The above metrics are data independent, so these
works require low computational costs to determine the pruned
filters (or channels).
Besides, some pruning metrics require to utilize input data
to select the pruned filters (or channels). Output feature maps
are the convolutional results of input feature maps and filters,
so some studies [26], [29], [51] determine the pruned filters
by judging the importance or redundancy of feature maps
from a set of input data. High rank (Hrank) [26] utilizes
the average rank of feature maps to judge the importance
of filters. Different from Hrank, channel independence-based
filter pruning (CHIP) [51] finds redundant feature maps and
prunes corresponding filters by measuring the correlations
among feature maps in each layer. Considering that filters are
updated by gradients from the loss function during training,
some studies [35], [41] count the gradient or second-order
derivatives information for each filter (or channel) to evaluate
the importance. Discrimination-aware channel pruning (DCP)
[41] chooses the most important channels by computing the
Frobenius norm of the gradients. Second-order structured
pruning (SOSP) [35] designs two saliency-based pruning met-
rics, which use first-order or second-order derivatives infor-
mation, to delete unimportant filters.
Some works [11], [32], [37], [38], [52] introduce extra

--- PAGE 4 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 4
learnable parameters and optimize these parameters leveraging
reinforcement learning algorithms or gradient based methods,
to generate a saliency score or binary decision for each channel
(or filter). Deep compression with reinforcement learning
(DECORE) [38] assigns an agent with only one parameter
to each channel and learns which channels to be pruned based
on reinforcement learning. For each input sample, fire together
wire together (FTWT) [52] utilizes a well-trained binary mask
predictor head in each layer to dynamically predict kactivated
filters.
B. Combining Pruning and Other Techniques for Network
Compression
Pruning and Low-rank Tensor Decomposition. Low-rank
tensor decomposition (LTD) [13] decomposes the low-rank
weight tensor of the convolution into a sequence of tensors
which contain much fewer parameters and computations. Since
pruning and LTD reduce redundancy in parameters from
different angles, some recent efforts [53]–[55] combine them
together to pursue better network compression performance.
Li et al. [55] proposed to attach sparsity-inducing matrices
to normal convolutions and apply group sparsity constraints
on them, to hinge filter pruning and decomposition (Hinge).
By simultaneously dealing with the sparsity and low-rankness
in weights, collaborative compression (CC) [54] combines
channel pruning and tensor decomposition to accelerate CNNs.
Pruning and Low-bit Quantization. Low-bit quantization
[17]–[19] can effectively reduce computation and storage cost
by reducing the number of bits used to represent the network
weights or activations. Quantization is orthogonal to pruning,
so some works [56]–[58] jointly perform them to achieve
better network acceleration. Wang et al. [56] proposed to
integrate structured pruning with mixed-bit precision quanti-
zation via a joint gradient-based optimization scheme. Integer-
only Discrete Flows (IODF) [58] performs efficient invertible
transformations by utilizing integer-only arithmetic based on
8-bit quantization and introducing learnable binary gates to
remove redundant filters during inference.
Pruning and Knowledge Distillation. Knowledge distilla-
tion (KD) [15] transfers the knowledge from a large teacher
network to a small student network. Recently, some works
exploit KD to enhance the learning ability of the pruned
networks [59], [60] or exploit pruning to enhances the KD
quality [61], [62]. In order to preserve the performance of the
pruned model, Zou et al. [60] proposed to distill the pruned
model to fit the pre-trained model by utilizing reconstructed
degraded images. In order to boost the performance of KD,
Park and No [62] proposed to prune the teacher network first to
make it more student-friendly and then distill it to the student.
The above works further show that pruning and other
techniques are complementary. Therefore, combining them can
improve the compression performance further.
III. PROPOSED METHOD
In this section, we introduce the framework and compres-
sion process of our proposed method.
Fig. 2. The 3×3SBC modules in the SBCNet. Here the bypass is the block
inspired by MobileNetV2 [36]. ”DWConv” denotes a depthwise convolution,
and all BN layers and nonlinear activation layers are omitted.
A. Preliminaries
For the l-th layer of a CNN, we denote the input feature
map as Xl∈Rcl−1×hl−1×wl−1, the output feature map
asYl∈Rcl×hl×wl, and weight tensor of the filters as
Wl∈Rcl×cl−1×kl×kl, where cl−1andclare the number
of input channels and filters (output channels), respectively,
andklis the kernel size of the filters. The output of the l-th
convolutional layer can be represented as:
Yl=Wl⊗Xl, (1)
where ⊗denotes the convolution operation.
Structured Pruning. Both filter pruning and channel prun-
ing seek a compact approximated representation Wl. In filter
pruning, each filter is regarded as a compression unit and the
pruning function is defined as:
Wl[i,:,:,:] =Wl[Kl
f[i],:,:,:] (2)
where Wl∈Rnl×cl−1×kl×klandKl
fdenotes an list consisting
of the indexes of the kept filters in Wl. Here nlis the number
of the kept filters. In channel pruning, each input channel is
regarded as a compression unit and the pruning function is
defined as:
Wl[:, i,:,:] =Wl[:, Kl
c[i],:,:] (3)
where Wl∈Rcl×nl−1×kl×klandKl
cdenotes an list consisting
of the indexes of the kept input channels in Wl. Here nl−1is
the number of the kept input channels.
In this paper, we introduce our pruning method from the
perspective of filter pruning, but note that it is also suitable
for channel pruning. In the following, for brevity, we replace
Wl[i,:,:,:]withWl[i].
B. Module Design
We firstly design a sparse module with bypass compensa-
tion, named SBC module, and then replace each convolutional
layer to be pruned in the baseline network with the SBC
module to build the SBCNet. The SBC module consists of a
sparse path (i.e., original convolutional layer) and a lightweight
bypass, and the output of the SBC module is the sum of the

--- PAGE 5 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 5
Fig. 3. The pipeline of our pruning strategy for the l-th SBC module. Here STE denotes straight-through estimator. For the l-th SBC module, in the process
of forward propagation, ℓ1-norm is used to calcuate the importance score Ilof the filters weight Wl
Sin the sparse path, and then the difference between
the importance score Iland the learnable threshold δlis input into the function round (sigmoid (·))to get the binary mask Ml. The output of the input Xl
through the convolutional layer is multiplied by the mask Mlto get the output Yl
Sin the sparse path. Then the output Yl
Sof the sparse path and the output
Yl
Bof the bypass are summed to get the final output Yl. In the process of backward propagation, the learnable threshold δland filters weight Wl
Sand
Wl
Bare updated by the guidance from the gradients of the overall loss. Note that in order to stabilize the compression training and not to interfere with the
learning of Wl
S, gradient propagation from GltoWl
Sis truncated, and gradients from Glare only utilized to guide the update of the threshold δl..
outputs of the original convolutional layer and the lightweight
bypass. Therefore, the SBCNet adds a few additional burdens
compared to the baseline network. An example for the 3×3
SBC module is shown in Fig.2. In this paper, we use the block
inspired by MobileNetV2 [36] as the bypass to make it have
sufficient expressive power. Besides, the structure of the block
can be obtained by applying LTD to a regular convolution
[14], so using the block as the bypass can also make the SBC
module exploit the complementarity of pruning and LTD to
improve the performance of our method further. For the l-th
SBC module, the output is represented as:
Yl=Yl
S+Yl
B=Wl
S⊗Xl+Wl
B3⊗(Wl
B2⊗(Wl
B1⊗Xl)),(4)
where Wl
S∈Rcl×cl−1×kl×kldenotes the weight tensor
of the sparse path, while Wl
B1∈Rdl×cl−1×1×1,Wl
B2∈
Rdl×1×kl×kl, and Wl
B3∈Rcl×dl×1×1denote the weight
tensors of the lightweight bypass.
For the sparse path, Wl
Sis randomly initialized before
training, and is made sparse with a mask by the proposed
pruning strategy during training until the target compression
rate is met. Detailed explanations of the proposed pruning
strategy are given in the following sub-section.
For the lightweight bypass, the three lightweight convolu-
tions are kept throughout the training to compensate the sparse
path. Suppose the stride of the convolutions in the l-th module
is 1, clandcl−1are equal, and the pruning rate adaptively
assigned by our pruning strategy in the sparse path is pl. For
brevity, we omit superscript. After pruning, the FLOPs of the
bypass is hw(2cd+dk2), and the FLOPs of the SBC module
ishw(2cd+dk2+ (1−p)c2k2). In general, k2≪cand
d≤c, we can ignore the item dk2. Thus the ratio of their
FLOPs is approximately2
2+((1−p)ck2)/d. When pis a small
value close to 0, the pruned sparse path is dominant in the
SBC module, and the lightweight bypass only brings relatively
few calculations. When pis a medium value, the bypass
compensates the lost information and capacity of the prunedsparse path with relatively small computational costs. When p
is a large value close to 1, the sparse path is pruned so narrow
that it blocks the forward propagation of features and back
propagation of gradients, which generally results in a drastic
decrease in network performance. However, the bypass helps
guarantee the propagation of features and gradients during
training so as to make the network converge normally.
In the extreme case, where pis 1, the entire sparse path can
be smoothly removed during pruning due to the bypass, which
achieves approximatelyck2
2d×acceleration, and the bypass
makes the module still maintain relatively strong expression
power. However, this does not mean that it is more effective
to directly build a compact network using bypasses instead
of SBC modules before training starts. Because as mentioned
above, some modules need remain some channels of sparse
paths to compensate their lightweight bypasses so as to
maintain sufficient capacity. The presence of sparse paths can
increase the diversity of features. Besides, they can act as
shortcuts to facilitate the flow of information, and pruning
dense SBCNet during training can help search a better sub-
network.
What’s more, by adjusting the hyperparameter dl, the
proportion of FLOPs of the bypass to the FLOPs of the
pruned SBC module can be roughly controlled. Therefore,
by setting the appropriate value for dl, we can achieve the
compression goal and make the bypass generate sufficient
compensation for the sparse path so as to achieve better
compression performance. In this paper, when the global target
compression rate is set to a large value, dlis empirically set
directly to cl; when the global target compression rate is set
to a very small value, dlis reduced to 0.5cl.
C. Pruning Strategy
The post-training pruning is a common used pruning strat-
egy. However, its three-stage pipeline, namely densely pre-
training, pruning, and fine-tuning, results in nearly twice

--- PAGE 6 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 6
training time. Due to its inefficiency, one intuitive solution
is pruning at initialization. Although pruning at initializa-
tion makes the training more efficient as it only trains the
pruned network, the pruned networks can be difficult to train
and easily suffer significant accuracy degradation. Therefore,
pruning during training, which can reduce negative effects of
post-training pruning and pruning at initialization, probably
finds a trade-off between training efficiency and final accuracy
[34]. So our pruning strategy also gradually prunes the sparse
paths during initial training of a few epochs, and exploits
the remaining large number of epochs to train the pruned
network to converge to improve performance. The pipeline
of our pruning strategy for the l-th module is shown in Fig.3.
Specifically, in this paper, for simplicity, we use ℓ1-norm as
the filter importance metric. Thus the filter importance score
Il∈Rclof the sparse path is represented as:
Il= [∥Wl
S[1]∥1,∥Wl
S[2]∥1,···,∥Wl
S[cl]∥1]T, (5)
in which each element represents the importance of the
corresponding filter. Different from PFEC [9] which directly
manually sets the pruning rates for each layer, we use pruning
thresholds to determine the pruning rates for each layer.
Suppose the threshold for the l-th module is δl, then the
pruning mask Ml∈Rclis obtained according to Il:
Ml[i] =(
1, Il
i≥δl,
0, Il
i< δl,(6)
where Il
idenotes the i-th element of Il. By summing the mask
Ml, the number of the kept filters can be obtained, namely nl,
and the pruning rate of the sparse path is pl= 1−nl/cl. The
output of the sparse path is then reformulated as follows:
Yl
S=Ml⊙(Wl
S⊗Xl), (7)
where ⊙is the element-wise product between the column
vector on the left and the spatial dimensions of the three-
dimensional tensor on the right.
Since different layers’ parameters distribution are different,
setting same pruning thresholds for all layers may not be
optimal. However, manually setting different thresholds for
each layer requires lots of labor costs and may also cause
the suboptimal pruning rates distribution. Inspired by the idea
of applying learnable thresholds to filter pruning proposed
by Kusupati et al. [31], we combine the simple ℓ1-norm
metric with learnable thresholds by introducing an additional
learnable parameter for each sparse path of the network.
From Eq.(7), Mlis directly involved in the forward cal-
culation of the network and can receive gradients from the
objective function during backpropagation. However, accord-
ing to Eq.(6), the threshold δlis non-differentiable. Therefore,
we change Mlinto the soft pruning mask Glby introducing
sigmoid (·)function:
Gl=sigmoid (Il−δl) =1
1 +e−(Il−δl). (8)
Here each element of Gl, i.e., Gl[i], is continuous and its
value ranges from 0 to 1. Following the works [32], [52],we further round Gl[i]to 0 or 1, to precisely generate sub-
networks during training, as follows:
Ml[i] =1≥0.5(Gl[i]),∀i∈[1, cl]. (9)
When the value of Gl[i]is greater than or equal to 0.5,
i.e., when the corresponding filter importance score Il
iis
greater than or equal to the threshold δl, the value of the
corresponding mask Ml[i]is 1, otherwise, it is 0. However,
the indicator function 1≥0.5(·)is still non-differentiable. To
tackle this problem, we use straight-through estimator (STE)
[63] to calculate the gradients from MltoGlduring back
propagation, making the update of δlfeasible.
Algorithm 1 Overview of the LAPP method.
Input: CNN with Nconvolutional layers to be pruned ;t-
otal epochs E;training set D;total FLOPs Ttotal of the
CNN; target compression rate C;
Output: Compact CNN satisfying the target compression
rateCand its optimal weight values WSandWB;
1: Build the SBCNet with WSandWBbased on Eq.(4);
2: Introduce learnable thresholds for each sparse path ;
3: Enforce epoch status ∈ {“prune”, “train” };
4: epoch status is set as “prune” ;
5:forepoch t= 1,2, ..., E do
6: fora mini-batch ( X, y) inDdo
7: ifepoch status is “prune” then
8: forlayer l= 1,2, ..., N do
9: Get Ilby Eq.(5) ;
10: Get Mlby Eq.(8) and Eq.(9) ;
11: Get Ylby Eq.(7) and Eq.(4) ;
12: end for
13: Calculate Tkept andˆC;
14: ifˆC̸=Cthen
15: Update WS,WBand∆with SGD ;
16: else
17: Remove the masks and prune, get WS;
18: Set epoch status as “train” ;
19: end if
20: else
21: Get Ylfor each layer lby Eq.(13) ;
22: Update WSandWBwith SGD ;
23: end if
24: end for
25:end for
26:Return: Compact CNN with weights WSandWB;
To make the filters be more distinguishable, we apply
sparse regularization to the filter weights of the sparse paths.
Consistent with the ℓ1-norm importance metric, we use ℓ1
regularization, and obtain the network optimization problem:
min
WS,WB,∆L(f(X;WS,WB,∆), y) +λNX
l=1clX
i=1∥Wl
S[i]∥1,
(10)
where L(·)denotes the cross-entropy loss, and WS=
{W1
S, W2
S,···, WN
S}andWB={W1
B, W2
B,···, WN
B}refer
to the parameters in the Nsparse modules of the SBCNet

--- PAGE 7 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 7
f(·;WS,WB,∆). Here Xandyrepresent the input samples
and corresponding labels respectively, ∆represents the set
of all learnable metric thresholds of the sparse paths of the
SBCNet, λis the coefficient of ℓ1regularization term, and
the weight decay regularization term is ignored for brevity.
Therefore, the final overall sparsity of the sparse paths of the
network is controlled by the coefficient of ℓ1regularization
and the initial value of the thresholds in ∆. However, adjusting
these hyperparameters by trial and error to achieve the target
compression rate is undoubtedly tedious and complicated.
Therefore, to explicitly control the sparsity of the network,
i.e., making the final sub-network reach given target FLOPs,
we introduce FLOPs constraints to control the update of the
thresholds in ∆. Previously FLOPs constraints and metric
thresholds are often used separately, while we attempt to
combine learnable thresholds and FLOPs constraints to au-
tomatically determine the appropriate pruning rates for each
layer based on the target compression rate during training.
Different from STR [31] which applys the sigmoid (·)func-
tion to the learnable threshold, we apply it to the difference
between the importance score Iland the learnable threshold
δl, i.e., Eq.(8), to control the value range of the soft mask
Glfrom 0 to 1. Here, ReLU (·)activation function used in
STR is unnecessary. Then we further introduce the indicator
function and STE, i.e., Eq.(9), so as to obtain the learnable
binary pruning masks M={M1, M2,···, MN}which
characterize the sub-network structure. Therefore, we can
precisely calculate the FLOPs Tkept of the sub-network at a
certain iteration by the binary masks M. Suppose the total
FLOPs of the baseline network is Ttotal, we can obtain the
compression rate ˆC=Tkept/Ttotal. Note that since the size of
the SBCNet is larger than the baseline, the initial value of ˆC
is greater than 1 before training starts. In order to control the
thresholds to make the sub-network smoothly and accurately
reach the given target FLOPs CTtotal, where Cis the target
compression rate, we design a regularization term to constrain
the FLOPs, as follows:
R(ˆC, C) = (ˆC
C−1)2. (11)
Then the optimization problem of the network is reformulated
as:
min
WS,WB,∆L(f(X;WS,WB,∆), y)
+λ1NX
l=1clX
i=1∥Wl
S[i]∥1+λ2R(ˆC, C),(12)
where λ1andλ2are coefficients of the ℓ1regularization
term and FLOPs regularization term, respectively. In order to
control the thresholds δmore flexibly and quickly to compress
the network to the target compression rate C, weight decay
regularization is no longer applied to the thresholds in ∆.
By the masks M, the learning of the thresholds are guided
jointly by the task loss L(f(X;WS,WB,∆), y)and FLOPs
regularization term R(ˆC, C). Specifically, the thresholds are
updated by the guidance from the gradients of R(ˆC, C)to
drive masks sparse until the resulting sub-network satisfies the
target FLOPs, while they are updated by the guidance fromthe gradients of L(f(X;WS,WB,∆), y)to suppress mask
sparsity and thus improve the accuracy of the resulting sub-
network. Therefore, during initial training of a few epochs, the
thresholds, learned in the competition to maintain accuracy
and compress the SBCNet to the target FLOPs, result in a
relatively better sub-network.
After the target compression rate is met, we remove the
binary sparse masks on the sparse paths, and convert the
SBCNet to the compressed network. According to Ml, we
can obtain the index list Kl
fand the pruned weight tensor
Wl
S∈Rnl×cl−1×kl×klin the l-th sparse path. And Yl
Sis the
output of the l-th sparse path. So after pruning, the output of
thel-th SBC module is represented as:
Yl[i] =(
Yl
S[Pos(i)] +Yl
B[i], i∈Kl
f,
Yl
B[i], i / ∈Kl
f,(13)
where i∈[1, cl]and function Pos (·)returns the position of the
element iinKl
f. Then, we continue to train the compressed
network for the remaining epochs. Finally we can get a sub-
network with weights WS={W1
S,W2
S,···,WN
S}andWB
that satisfies the target compression rate without compromising
performance.
The whole process is summarized in Algorithm 1. Note that
although in this paper we take ℓ1-norm as the filter importance
metric due to its simplicity, our pruning strategy can be also
combined with some other importance metrics of existing
methods.
IV. EXPERIMENTS
In this section, our method LAPP is evaluated on two bench-
mark image classification datasets: CIFAR-10 [64] and Ima-
geNet [65]. We compare our method with some state-of-the-
art CNN compression methods, including PFEC [9], DECORE
[38], Hinge [55], pruning from scratch (PFS) [11], CC [54],
NS [30], Hrank [26], FTWT [52], SOSP [35], hierarchical
pruning via shape-edge representation of feature maps (HPSE)
[29], dynamic and progressive filter pruning scheme (DPFPS)
[45], CHIP [51], learning filter pruning criteria (LFPC) [47],
network pruning via performance maximization (NPPM) [32],
FP-AGL [49], soft filter pruning(SFP) [27], FPGM [28], prun-
ing using graph neural networks and reinforcement learning
(GNN-RL) [37], and DCP [41]. All the compared methods
applied on the mainstream CNN models, including VGG [1],
ResNet [2], GoogleNet [39], and DenseNet [40]. We conduct
comprehensive experiments with Pytorch [66] to show that
LAPP is superior or comparable to the above methods.
A. Datasets and Experimental Settings
Datasets. The CIFAR-10 dataset contains 60,000 color
images with the size of 32×32in 10 classes, and each class
consists of 6,000 images, 5,000 of which are in the training set
and the rest are in the test set. Following [26], for the training
set, we pad 4 pixels with zeros on each side of a image, then
randomly crop a 32×32sample from the padded image, and
finally randomly flip the cropped image horizontally with a
probability of 0.5. For the test set, we directly use the original

--- PAGE 8 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 8
TABLE I
COMPRESSION RESULTS OF VGG-16 ONCIFAR-10.
Method Top-1(%) FLOPs ↓(%) Params ↓(%)
Baseline 93.96 0.0 0.0
PFEC [9] 93.40 34.2 64.0
DECORE [38] 94.02 35.3 63.0
Hinge [55] 94.02 39.1 80.1
PFS [11] 93.63±0.06 50.0 —
CC [54] 94.15 50.8 65.9
NS [30] 93.80 51.0 88.5
Hrank [26] 93.43 53.5 82.9
Ours (C=0.46) 94.32±0.20 53.6 73.0
FTWT [52] 93.73 56.0 —
SOSP [35] 93.73±0.16 57.7 87.3
CC [54] 94.09 60.7 72.7
DECORE [38] 93.56 64.8 89.0
Hrank [26] 92.34 65.3 82.1
Ours (C=0.34) 94.36±0.08 65.7 74.3
HPSE [29] 93.50 66.1 —
DPFPS [45] 93.52±0.15 70.9 93.3
Ours (C=0.26) 93.79±0.16 73.8 85.2
Hrank [26] 91.23 76.5 92.0
CHIP [51] 93.18 78.6 87.3
DECORE [38] 92.44 81.5 96.6
Ours (C=0.18) 93.54±0.23 81.9 86.1
HPSE [29] 92.49 82.0 —
32×32images. Both training and test images are normalized
using channel means and standard deviations. The ImageNet
dataset is a large-scale image classification dataset consisting
of approximately 1.28 million training images and 50,000
validation images from 1,000 classes. We adopt the same data
augmentation strategy as CC [54].
Experimental Setting. On CIFAR-10, the baselines are
trained using the same training configurations as the work of
Li et al. [54], i.e., the initial learning rate is set to 0.1 and is
multiplied by 0.1 at 50 %and 75 %of the total 300 epochs.
Note that due to the presence of a fine-tuning stage, most of the
compared compression methods train more than 400 epochs.
So for fair comparison and making the compressed network
fully trained, we train the networks except DenseNet from
scratch with LAPP for 400 epochs via Stochastic Gradient
Descent algorithm (SGD) with momentum 0.9 and an initial
learning rate of 0.1. The mini-batch size and weight decay
are set to 128 and 0.0001, respectively. Except the number
of epochs, our training configurations are the same as fine-
tuning configurations of CC [54]. To achieve comparable
performance, we train DenseNet from scratch with LAPP for
600 epochs which is same as the total number of epochs of
CC [54]. On ImageNet, the networks are trained from scratch
with LAPP for 120 epochs with the initial learning rate of 0.1.
The learning rate is divided by 10 at epoch 30, 60 and 90, and
the mini-batch size is set to 256. All the learnable thresholds
are empirically initialized to 0, and the coefficient λ2of the
FLOPs constraints term is set to 1.0 for all networks except
ResNet34. The coefficient λ1ofℓ1regularization term is set
to 3e-5 for ResNet-20 and 2e-5 for other networks on CIFAR-
10. On ImageNet, λ1is set to 1e-5, and λ2is set to 0.5 for
ResNet-34. All experiments are repeated for three times and
the average results are reported. Note that in all subsequent
tables, FLOPs ↓and Params ↓represent the reduction ofTABLE II
COMPRESSION RESULTS OF RESNET-56/110 ONCIFAR-10.
Method Top-1(%) FLOPs ↓(%) Params ↓(%)
ResNet-56 93.33 0.0 0.0
LFPC [47] 93.72±0.29 47.1 —
DECORE [38] 93.26 49.9 49.0
PFS [11] 93.05±0.19 50.0 —
Hrank [26] 93.17 50.0 42.4
NPPM [32] 93.40 50.0 —
Hinge [55] 93.69 50.0 48.7
CC [54] 93.64 52.0 48.2
FP-AGL [49] 93.69 52.5 —
Ours (C=0.47) 93.72±0.16 52.5 40.0
SFP [27] 93.35±0.31 52.6 —
FPGM [28] 93.49±0.13 52.6 —
DPFPS [45] 93.20±0.11 52.9 46.8
GNN-RL [37] 93.49 54.0 —
SOSP [35] 93.27±0.51 57.0 61.0
FP-AGL [49] 93.49 60.9 —
Ours (C=0.385) 93.52±0.18 61.1 52.5
FTWT [52] 92.63 66.0 —
CHIP [51] 92.05 72.3 71.8
Hrank [26] 90.72 74.1 68.1
HPSE [29] 91.51 74.2 —
Ours (C=0.25) 92.84±0.21 74.8 66.8
ResNet-110 93.50 0.0 0.0
PFEC [9] 93.30 38.6 32.4
PFS [11] 93.69±0.28 40.0 —
SFP [27] 93.38±0.30 40.8 —
Hrank [26] 94.23 41.2 39.4
FPGM [28] 93.74±0.10 52.3 —
Ours (C=0.47) 94.03±0.26 52.5 42.7
LFPC [47] 93.79±0.38 60.3 —
HPSE [29] 93.79 60.6 —
DECORE [38] 93.50 61.8 64.8
Hrank [26] 92.65 68.6 69.2
Ours (C=0.31) 93.90±0.14 68.7 66.2
HPSE [29] 93.26 73.4 —
DECORE [38] 92.71 76.9 79.6
Ours (C=0.22) 93.18±0.17 77.8 70.6
FLOPs and parameters of the compressed network compared
to the Baseline network, respectively.
B. Experimental Results on CIFAR-10
We conduct experiments on the CIFAR-10 dataset with
several popular CNNs, including ResNet-56/110/20/32, VGG-
16, DenseNet-40 and GoogLeNet. Following [26], VGG-16 is
a modified version of the original and the output of the original
GoogLeNet is changed to fit the class number in CIFAR-10.
VGG-16. The compression results of VGG-16 are displayed
in Table I. Compared with post-training pruning methods
HRank and DECORE, our method without pre-training is
better in various acceleration levels. Compared with CC and
Hinge which combine pruning and low-rank decomposition,
our method also obtains better performance (65.7% vs. 60.7%
vs. 39.1% in FLOPs reduction, and 94.36% vs. 94.09% vs.
94.02% in top-1 accuracy). Compared with dynamic pruning
method FTWT, with almost the same accuracy, our method ob-
tains significantly more FLOPs reduction (73.8% vs. 56.0%).
Compared with PFEC which also uses the ℓ1-norm metric,
where only 93.40% top-1 accuracy is obtained, our method ob-
tains a better result of 94.32% with significantly more FLOPs
reduction, which demonstrates the superiority of introducing

--- PAGE 9 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 9
TABLE III
COMPRESSION RESULTS OF RESNET-20/32 ONCIFAR-10.
Method Top-1(%) FLOPs ↓(%) Params ↓(%)
ResNet-20 92.20 0.0 0.0
SFP [27] 90.83±0.31 42.2 —
FPGM [28] 91.09±0.10 42.2 —
PFS [11] 90.55±0.14 50.0 —
GNN-RL [37] 91.31 51.0 —
Hinge [55] 91.84 54.5 55.5
Ours (C=0.4) 92.22±0.19 59.7 52.8
FP-AGL [49] 90.11 60.7 —
Ours (C=0.28) 91.24±0.24 71.8 58.9
ResNet-32 92.63 0.0 0.0
SFP [27] 92.08±0.08 41.5 —
GNN-RL [37] 92.58 51.0 —
LFPC [47] 92.12±0.08 52.6 —
FPGM [28] 91.93±0.03 53.2 —
Ours (C=0.4) 93.21±0.19 59.6 49.1
FP-AGL [49] 91.86 60.8 —
Ours (C=0.28) 92.32±0.21 71.8 60.8
the bypasses and using the proposed non-uniform pruning
strategy. Therefore, our method demonstrates its ability to
accelerate a neural network with a plain structure.
ResNet-56/110/20/32. The compression results of ResNet-
56/110/20/32 are displayed in Tables II and III. We start
with ResNet-56. Compared with the pruning at initializa-
tion method PFS, our method further reduces FLOPs by
2.5%, while maintaining higher accuracy (93.72(±0.16)% vs.
93.05(±0.19)%). Besides, compared with the pruning dur-
ing training method DPFPS, our method further improves
accuracy by 0.32%, while maintaining more FLOPs reduc-
tion (61.1% vs. 52.9%). Compared with NPPM which also
uses FLOPs constraints, with more FLOPs reduction, our
method without pre-training obtains significantly better ac-
curacy (93.72(±0.16)% vs. 93.40%), which demonstrates the
superiority of our pruning strategy combining learnable thresh-
olds and FLOPs constraints. Compared with LFPC which
learns filter pruning criteria, with almost the same accuracy,
our method further reduces FLOPs by 5.4%.
On ResNet-110 with more redundancy, our method leads
to significant improvement in accuracy over the baseline
model (94.03(±0.26)% vs. 93.50%) with around 52.5% FLOPs
and 42.7% parameters reduction. Compared with SFP and
FPGM with the same pruning rate for all layers, our method
achieves higher accuracy (93.90(±0.14)% vs. 93.38(±0.30)%
vs. 93.74(±0.10)%) and saves more computing resources
(68.7% vs. 40.8% vs. 52.3%), which show that our method is
more effective. Compared with HPSE with a complex pruning
criterion based on the characteristics of feature maps, our
method using the simple ℓ1-norm metric is still comparable in
various acceleration levels, which demonstrates the superiority
of our method.
Table III shows the performance of the different meth-
ods of compressing ResNet-20/32. Compressing the relatively
lightweight ResNet20/32 is more challenging than compress-
ing the ResNet-56/110. On ResNet-20, compared with GNN-
RL which finds suitable hidden layers’ pruning rates using
reinforcement learning, our method further improves accu-
racy by 0.91% and reduces FLOPs by 8.7%. On ResNet-32,TABLE IV
COMPRESSION RESULTS OF DENSE NET-40 ONCIFAR-10.
Method Top-1(%) FLOPs ↓(%) Params ↓(%)
Baseline 94.81 0.0 0.0
DECORE [38] 94.59 39.4 46.0
Hrank [26] 94.24 40.8 36.5
Hinge [55] 94.67 44.4 27.5
CC [54] 94.67 47.0 51.9
HPSE [29] 94.38 48.0 —
Ours (C=0.5,in) 94.56±0.17 49.5 46.6
Ours (C=0.5,out) 94.51±0.29 49.6 48.2
DECORE [38] 94.04 54.7 65.0
NS [30] 94.35 55.0 65.2
CC [54] 94.40 60.4 64.4
Hrank [26] 93.68 61.0 53.8
HPSE [29] 93.88 61.5 —
Ours (C=0.38,in) 94.24±0.25 61.6 62.4
TABLE V
COMPRESSION RESULTS OF GOOGLE NET ON CIFAR-10.
Method Top-1(%) FLOPs ↓(%) Params ↓(%)
Baseline 95.05 0.0 0.0
DECORE [38] 95.20 19.8 23.0
PFEC [9] 94.54 32.9 42.9
CC [54] 95.18 50.0 54.0
Ours (C=0.49) 95.57±0.08 50.5 52.6
Hrank [26] 94.53 54.9 55.4
CC [54] 94.88 59.9 63.3
Ours (C=0.39) 95.40±0.12 60.6 61.1
Hrank [26] 94.07 70.4 69.8
DECORE [38] 94.51 78.5 80.9
Ours (C=0.21) 94.99±0.08 78.8 79.1
with around 60.0% FLOPs reduction, our method leads to
significant improvement in accuracy over the baseline model
(93.21(±0.19)% vs. 92.63%), which is significantly different
from other advanced compression algorithms, highlighting the
effectiveness of our method. Therefore, our method demon-
strates that it is especially suitable for compressing neural
networks with residual blocks.
DenseNet-40. Table IV shows the performance of the differ-
ent methods of compressing DenseNet-40, where ‘in’ denotes
channel pruning and ‘out’ denotes filter pruning. Compared
with the state-of-the-art pruning methods HRank, DECORE
and HPSE, our method is better than them in various accelera-
tion levels. Compared with CC and Hinge requiring pre-trained
models, with slightly more FLOPs reduction, although our
average accuracy is worse than the results they report, our best
result is better than theirs (94.76% vs. 94.67% vs. 94.67%).
Overall, our method has the potential to better accelerate
networks with dense blocks.
GoogleNet. In Table V, we analyze the performance of
the different methods on GoogleNet. Because GoogleNet has
large amount of redundancy, compressing it is especially
easy. 1 ×1 convolutions in GoogleNet possess only sparse
paths for our method. With almost the same accuracy as
the baseline model, our method obtains around 79% FLOPs
and parameters reduction. Besides, compared with HRank and
CC, our method uses much fewer FLOPs (39.4% vs. 45.1%
vs.40.1%), but achieves higher accuracy (95.40(±0.12)% vs.
94.53% vs. 94.88%), which is very encouraging. Therefore,

--- PAGE 10 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 10
TABLE VI
COMPRESSION RESULTS OF RESNET-18/34 ONIMAGE NET.
Method Top-1(%) Top-5(%) FLOPs ↓(%)
ResNet-18 69.76 89.08 0.0
SOSP [35] 68.78±0.98 — 29.0
SFP [27] 67.10 87.78 41.8
FPGM [28] 68.34 88.53 41.8
Ours (C=0.56) 70.43±0.07 89.77±0.03 43.5
DCP [41] 67.36 87.63 46.2
GNN-RL [37] 68.66 — 51.0
FTWT [52] 67.49 — 51.6
Ours (C=0.44) 69.97±0.04 89.48±0.14 55.6
ResNet-34 73.31 91.42 0.0
SFP [27] 71.83 90.33 41.1
FPGM [28] 72.54 91.13 41.1
FP-AGL [49] 72.72 — 43.1
DPFPS [45] 72.25 90.80 43.3
Ours (C=0.56) 72.91±0.13 91.18±0.02 43.5
ResNet-18 [2] 69.76 89.08 50.0
FTWT [52] 71.71 — 52.2
Ours (C=0.47) 72.55±0.16 91.05±0.11 52.5
it demonstrates that our method can be effectively applied to
compressing neural networks with inception modules.
C. Experimental Results on ImageNet
ResNet-18/34. We also conduct experiments for ResNet-
18 and ResNet-34 on the challenging ImageNet dataset, as
shown in Table VI. We can see the similar conclusion in
CIFAR-10 dataset, i.e., our method is almost superior to
compared methods in all aspects. Specifically, Compared with
SFP (67.10% top-1) and FPGM (68.34% top-1), our method
achieves higher accuracy (70.43% top-1) with more FLOPs
reduction (43.5%) on ResNet-18. Compared with DCP re-
quiring pre-trained models, our method provides significantly
higher accuracy with more FLOPs reduction (69.97(±0.04)%
vs. 67.36% in top-1 accuracy, and 55.6% vs. 46.2% in FLOPs
reduction) on ResNet-18. Compared with GNN-RL requiring
total 240 epochs, with 120 epochs, our method obtains better
performance on ResNet-18, which show that our method is
more effective. Compared with FP-AGL and DPFPS, with
higher accuracy, our method obtains similar FLOPs reduction
on ResNet-34. Compared with ResNet-18, with more FLOPs
reduction, our method leads to significantly higher accuracy
on ResNet-34, which demonstrates that our method is able to
generate a good sub-network. FTWT is a dynamic pruning
method and expected to outperform static pruning methods,
but our method still outperforms it on ResNet-18 and ResNet-
34. Hence, our method also works well on complex datasets.
D. Ablation Study
Effect of the Bypass. As mentioned earlier, the bypasses
compensate for the sparse paths. We conduct ablation experi-
ments on CIFAR-10 with VGG-16, ResNet-56 and ResNet-20
to show the compensation effects of the bypasses, as shown
in Table VII. Here, the blocks inspired by MobileNetV1 [67]
and MobileNetV2 [36] are named V1 and V2, respectively.
The block V1 sequentially contains two layers of small
convolutions: depthwise convolution and 1×1convolution.TABLE VII
ABLATION EXPERIMENTAL RESULTS OF THE BYPASS .
Model C Method Top-1(%) FLOPs ↓(%)
VGG-16 0.34LAPP-S 93.70±0.24 65.7
Model-V2 94.20±0.15 65.5
LAPP-V1 93.65±0.13 65.7
LAPP 94.36±0.08 65.7
ResNet-56 0.33LAPP-S 92.93±0.21 66.7
Model-V2 92.84±0.11 66.3
LAPP-V1 93.09±0.17 66.7
LAPP 93.49±0.07 66.7
ResNet-20 0.4LAPP-S 90.70±0.13 59.7
Model-V2 91.68±0.17 59.3
LAPP-V1 91.71±0.48 59.7
LAPP 92.22±0.19 59.7
TABLE VIII
ABLATION EXPERIMENTAL RESULTS OF THE PRUNING STRATEGY .‘UP’
DENOTES UNIFORM PRUNING .
Model C Method Top-1(%) FLOPs ↓(%)
VGG-160.34LAPP-UP 94.20±0.11 65.8
LAPP 94.36±0.08 65.7
0.18LAPP-UP 93.31±0.12 81.8
LAPP 93.54±0.23 81.9
ResNet-560.47LAPP-UP 93.34±0.18 50.1
LAPP 93.72±0.16 52.5
0.33LAPP-UP 92.84±0.13 65.2
LAPP 93.49±0.07 66.7
ResNet-20 0.4LAPP-UP 91.25±0.18 57.7
LAPP 92.22±0.19 59.7
‘LAPP-S’ denotes only using sparse paths in SBC modules,
‘Model-V2’ denotes only using the bypasses containing the
block V2, and ‘LAPP-V1’ and ’LAPP’ denote using the two
paths and respectively take the block V1 and V2 as the bypass.
We adjust the hyperparameter dlof each bypass for Model-
V2 to obtain the same FLOPs reduction as other methods.
Besides, we use the exact same training configurations for all
methods. Unlike LAPP-V1 and LAPP, for LAPP-S without
the bypasses, when output channels of the layer are pruned,
pruning the corresponding input channels of the next layer
needs to be taken into account.
From Table VII, LAPP-V1 and LAPP with the bypasses
outperform LAPP-S in almost all the cases, which demon-
strates the compensation effects of the bypasses. What’s more,
the more compact the network, and the more significant the
compensation effect of the bypass, especially for networks
with residual blocks. For example, when compressing more
challenging ResNet-20, LAPP surpasses LAPP-S by 1.52%
in top-1 accuracy with the similar FLOPs reduction. Besides,
for compressing VGG-16, ResNet56 and ResNet-20, LAPP
surpasses Model-V2 by 0.16%, 0.65%, and 0.54% in top-1
accuracy, respectively. This proves that the bypasses and the
sparse paths can compensate for each other, which results in
better compression performance. Last but not least, LAPP out-
performs LAPP-V1 in all the cases. This proves that the block
V2 with stronger expressive power has better compensation
effect than the block V1.
Effect of the Pruning Strategy. Here we conduct abla-

--- PAGE 11 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 11
tion experiments on CIFAR-10 with VGG-16, ResNet-56 and
ResNet-20 to show the effects of the pruning strategy under
different target FLOPs compression rates, as shown in Table
VIII. Using the same filter importance metric, we compare
our pruning strategy with the uniform pruning strategy at
initialization. We use ‘UP’ to denote uniform pruning before
training starts. We manually set the almost same pruning
rates for all sparse paths to reach target FLOPs compression
rates. Other training configurations are the exact same as
LAPP. LAPP with our pruning strategy outperforms LAPP-
UP using uniform pruning in all the cases. For example, when
compressing ResNet-56, LAPP surpasses LAPP-UP by 0.38%
and 0.65% in top-1 accuracy respectively under different
target FLOPs compression rates 47% and 33%. The lower the
target FLOPs compression rate, the more effective our pruning
strategy will be. This proves that our pruning strategy is able
to generate a relatively better sub-network.
V. C ONCLUSION
In this paper, we integrate learnable thresholds and FLOPs
constraints into an effective and efficient pruning strategy.
The pruning strategy can gradually prune the network and
automatically determine the appropriate pruning rates for each
layer during initial training of a few epochs from scratch. Be-
sides, we introduce an additional lightweight bypass for each
convolutional layer to compensate the lost information and
capacity of the pruned layer during training. The effectiveness
of the proposed method is evaluated on the benchmark datasets
CIFAR-10 and ImageNet using the mainstream CNN models,
and the results show its superior performance compared with
other advanced compression algorithms.
REFERENCES
[1] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.
[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2016, pp. 770–778.
[3] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Unified, real-time object detection,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2016, pp. 779–
788.
[4] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss
for dense object detection,” in Proceedings of the IEEE International
Conference on Computer Vision , 2017, pp. 2980–2988.
[5] R. T. Marriott, S. Romdhani, and L. Chen, “A 3d gan for improved large-
pose facial recognition,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2021, pp. 13 445–13 455.
[6] S. Li, J. Xu, X. Xu, P. Shen, S. Li, and B. Hooi, “Spherical confidence
learning for face recognition,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 2021, pp. 15 629–
15 637.
[7] Y . Chen, Z. Zhang, C. Yuan, B. Li, Y . Deng, and W. Hu, “Channel-
wise topology refinement graph convolution for skeleton-based action
recognition,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision , 2021, pp. 13 359–13 368.
[8] Y . Nirkin, L. Wolf, and T. Hassner, “HyperSeg: Patch-wise hypernetwork
for real-time semantic segmentation,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
4061–4070.
[9] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning
filters for efficient convnets,” arXiv preprint arXiv:1608.08710 , 2016.
[10] J. J. M. Ople, T.-M. Huang, M.-C. Chiu, Y .-L. Chen, and K.-L. Hua,
“Adjustable model compression using multiple genetic algorithms,”
IEEE Transactions on Multimedia , 2021.[11] Y . Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu,
“Pruning from scratch,” in Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 34, no. 07, 2020, pp. 12 273–12 280.
[12] V . Lebedev, Y . Ganin, M. Rakhuba, I. Oseledets, and V . Lempit-
sky, “Speeding-up convolutional neural networks using fine-tuned CP-
decomposition,” in Proceedings of the International Conference on
Learning Representations , 2015.
[13] A.-H. Phan, K. Sobolev, K. Sozykin, D. Ermilov, J. Gusak, P. Tichavsk `y,
V . Glukhov, I. Oseledets, and A. Cichocki, “Stable low-rank tensor
decomposition for compression of convolutional neural network,” in
Proceedings of the European Conference on Computer Vision . Springer,
2020, pp. 522–539.
[14] J. Kossaifi, A. Toisoul, A. Bulat, Y . Panagakis, T. M. Hospedales,
and M. Pantic, “Factorized higher-order cnns with an application to
spatio-temporal emotion estimation,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2020, pp.
6060–6069.
[15] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv preprint arXiv:1503.02531 , 2015.
[16] D. Y . Park, M.-H. Cha, D. Kim, B. Han et al. , “Learning student-friendly
teacher networks for knowledge distillation,” in Advances in Neural
Information Processing Systems , vol. 34, 2021, pp. 13 292–13 303.
[17] P. K. Sharma, A. Abraham, and V . N. Rajendiran, “A generalized zero-
shot quantization of deep convolutional neural networks via learned
weights statistics,” IEEE Transactions on Multimedia , 2021.
[18] Z. Li, B. Ni, T. Li, X. Yang, W. Zhang, and W. Gao, “Residual
quantization for low bit-width neural networks,” IEEE Transactions on
Multimedia , 2021.
[19] W. Duan, Z. Liu, C. Jia, S. Wang, S. Ma, and W. Gao, “Differential
weight quantization for multi-model compression,” IEEE Transactions
on Multimedia , 2022.
[20] K. Han, Y . Wang, C. Xu, J. Guo, C. Xu, E. Wu, and Q. Tian, “Ghostnets
on heterogeneous devices via cheap operations,” International Journal
of Computer Vision , vol. 130, no. 4, pp. 1050–1069, 2022.
[21] Q. Zhang, Z. Jiang, Q. Lu, Z. Zeng, S.-H. Gao, and A. Men, “Split to Be
Slim: an overlooked redundancy in vanilla convolution,” in Proceedings
of the Twenty-Ninth International Conference on International Joint
Conferences on Artificial Intelligence , 2021, pp. 3195–3201.
[22] J. Chen, S.-h. Kao, H. He, W. Zhuo, S. Wen, C.-H. Lee, and S.-
H. G. Chan, “Run, Don’t Walk: Chasing higher flops for faster neural
networks,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023, pp. 12 021–12 031.
[23] X. Ding, X. Zhou, Y . Guo, J. Han, J. Liu et al. , “Global sparse
momentum sgd for pruning very deep neural networks,” in Advances
in Neural Information Processing Systems , vol. 32, 2019.
[24] X. Dong, S. Chen, and S. Pan, “Learning to prune deep neural networks
via layer-wise optimal brain surgeon,” in Advances in Neural Informa-
tion Processing Systems , vol. 30, 2017.
[25] X. Ding, T. Hao, J. Tan, J. Liu, J. Han, Y . Guo, and G. Ding, “ResRep:
Lossless cnn pruning via decoupling remembering and forgetting,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 4510–4520.
[26] M. Lin, R. Ji, Y . Wang, Y . Zhang, B. Zhang, Y . Tian, and L. Shao,
“Hrank: Filter pruning using high-rank feature map,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 1529–1538.
[27] Y . He, G. Kang, X. Dong, Y . Fu, and Y . Yang, “Soft filter pruning for
accelerating deep convolutional neural networks,” in Proceedings of the
International Joint Conference on Artificial Intelligence , 2018.
[28] Y . He, P. Liu, Z. Wang, Z. Hu, and Y . Yang, “Filter pruning via
geometric median for deep convolutional neural networks acceleration,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2019, pp. 4340–4349.
[29] H. Zhang, L. Liu, B. Kang, and N. Zheng, “Hierarchical model compres-
sion via shape-edge representation of feature maps—an enlightenment
from the primate visual system,” IEEE Transactions on Multimedia ,
2022.
[30] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning effi-
cient convolutional networks through network slimming,” in Proceedings
of the IEEE International Conference on Computer Vision , 2017, pp.
2736–2744.
[31] A. Kusupati, V . Ramanujan, R. Somani, M. Wortsman, P. Jain,
S. Kakade, and A. Farhadi, “Soft threshold weight reparameterization
for learnable sparsity,” in Proceedings of the International Conference
on Machine Learning . PMLR, 2020, pp. 5544–5555.

--- PAGE 12 ---
THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE, AFTER WHICH THIS VERSION MAY NO LONGER BE ACCESSIBLE. 12
[32] S. Gao, F. Huang, W. Cai, and H. Huang, “Network pruning via perfor-
mance maximization,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 9270–9280.
[33] S. Gao, F. Huang, J. Pei, and H. Huang, “Discrete model compression
with resource constraint for deep neural networks,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 1899–1908.
[34] M. Shen, P. Molchanov, H. Yin, and J. M. Alvarez, “When to prune?
a policy towards early structural pruning,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 12 247–12 256.
[35] M. Nonnenmacher, T. Pfeil, I. Steinwart, and D. Reeb, “SOSP: Effi-
ciently capturing global correlations by second-order structured prun-
ing,” in Proceedings of the International Conference on Learning
Representations , 2021.
[36] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-
bileNetV2: Inverted residuals and linear bottlenecks,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Rcognition , 2018,
pp. 4510–4520.
[37] S. Yu, A. Mazaheri, and A. Jannesari, “Topology-aware network pruning
using multi-stage graph embedding and reinforcement learning,” in
Proceedings of the International Conference on Machine Learning .
PMLR, 2022, pp. 25 656–25 667.
[38] M. Alwani, Y . Wang, and V . Madhavan, “DECORE: Deep compression
with reinforcement learning,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 2022, pp. 12 349–
12 359.
[39] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
inProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2015, pp. 1–9.
[40] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition , 2017, pp. 4700–4708.
[41] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y . Guo, Q. Wu, J. Huang,
and J. Zhu, “Discrimination-aware channel pruning for deep neural
networks,” in Advances in Neural Information Processing Systems ,
vol. 31, 2018.
[42] N. Lee, T. Ajanthan, and P. Torr, “SNIP: Single-shot network pruning
based on connection sensitivity,” in Proceedings of the International
Conference on Learning Representations , 2018.
[43] S. Hayou, J.-F. Ton, A. Doucet, and Y . W. Teh, “Robust pruning
at initialization,” in Proceedings of the International Conference on
Learning Representations , 2020.
[44] C. Wang, G. Zhang, and R. Grosse, “Picking winning tickets before
training by preserving gradient flow,” in Proceedings of the International
Conference on Learning Representations , 2019.
[45] X. Ruan, Y . Liu, B. Li, C. Yuan, and W. Hu, “DPFPS: dynamic and
progressive filter pruning for compressing convolutional neural networks
from scratch,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 35, no. 3, 2021, pp. 2495–2503.
[46] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, “Linear mode
connectivity and the lottery ticket hypothesis,” in Proceedings of the
International Conference on Machine Learning . PMLR, 2020, pp.
3259–3269.
[47] Y . He, Y . Ding, P. Liu, L. Zhu, H. Zhang, and Y . Yang, “Learning filter
pruning criteria for deep convolutional neural networks acceleration,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2020, pp. 2009–2018.
[48] Z. Wang, W. Hong, Y .-P. Tan, and J. Yuan, “Pruning 3d filters for
accelerating 3d convnets,” IEEE Transactions on Multimedia , vol. 22,
no. 8, pp. 2126–2137, 2019.
[49] N. J. Kim and H. Kim, “FP-AGL: Filter pruning with adaptive gradient
learning for accelerating deep convolutional neural networks,” IEEE
Transactions on Multimedia , 2022.
[50] T. Zhuang, Z. Zhang, Y . Huang, X. Zeng, K. Shuang, and X. Li,
“Neuron-level structured pruning using polarization regularizer,” in
Advances in Neural Information Processing Systems , vol. 33, 2020, pp.
9865–9877.
[51] Y . Sui, M. Yin, Y . Xie, H. Phan, S. Aliari Zonouz, and B. Yuan, “CHIP:
CHannel independence-based pruning for compact neural networks,” in
Advances in Neural Information Processing Systems , vol. 34, 2021, pp.
24 604–24 616.
[52] S. Elkerdawy, M. Elhoushi, H. Zhang, and N. Ray, “Fire Together
Wire Together: A dynamic pruning approach with self-supervised mask
prediction,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2022, pp. 12 454–12 463.[53] K. Guo, X. Xie, X. Xu, and X. Xing, “Compressing by learning in
a low-rank and sparse decomposition form,” IEEE Access , vol. 7, pp.
150 823–150 832, 2019.
[54] Y . Li, S. Lin, J. Liu, Q. Ye, M. Wang, F. Chao, F. Yang, J. Ma, Q. Tian,
and R. Ji, “Towards compact cnns via collaborative compression,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2021, pp. 6438–6447.
[55] Y . Li, S. Gu, C. Mayer, L. V . Gool, and R. Timofte, “Group Sparsity:
The hinge between filter pruning and decomposition for network com-
pression,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2020, pp. 8018–8027.
[56] Y . Wang, Y . Lu, and T. Blankevoort, “Differentiable joint pruning and
quantization for hardware efficiency,” in Proceedings of the European
Conference on Computer Vision . Springer, 2020, pp. 259–277.
[57] M. Van Baalen, C. Louizos, M. Nagel, R. A. Amjad, Y . Wang,
T. Blankevoort, and M. Welling, “Bayesian Bits: Unifying quantization
and pruning,” in Advances in Neural Information Processing Systems ,
vol. 33, 2020, pp. 5741–5752.
[58] S. Wang, J. Chen, C. Li, J. Zhu, and B. Zhang, “Fast lossless neural
compression with integer-only discrete flows,” in Proceedings of the
International Conference on Machine Learning . PMLR, 2022, pp.
22 562–22 575.
[59] S. Li, M. Lin, Y . Wang, C. Fei, L. Shao, and R. Ji, “Learning efficient
gans for image translation via differentiable masks and co-attention
distillation,” IEEE Transactions on Multimedia , 2022.
[60] W. Zou, Y . Wang, X. Fu, and Y . Cao, “Dreaming to prune image
deraining networks,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 6023–6032.
[61] Y . Liu, Z. Shu, Y . Li, Z. Lin, F. Perazzi, and S.-Y . Kung, “Content-
aware gan compression,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2021, pp. 12 156–12 166.
[62] J. Park and A. No, “Prune your model before distill it,” in Proceedings
of the European Conference on Computer Vision . Springer, 2022, pp.
120–136.
[63] Y . Bengio, N. L ´eonard, and A. Courville, “Estimating or propagating
gradients through stochastic neurons for conditional computation,” arXiv
preprint arXiv:1308.3432 , 2013.
[64] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
Master’s thesis, University of Tront , 2009.
[65] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al. , “Imagenet large
scale visual recognition challenge,” International journal of computer
vision , vol. 115, no. 3, pp. 211–252, 2015.
[66] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An
imperative style, high-performance deep learning library,” in Advances
in Neural Information Processing Systems , vol. 32, 2019.
[67] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efficient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861 , 2017.
