# 2401.05738.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/convolution/2401.05738.pdf
# Kích thước file: 1012831 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
Giải thích và Cải thiện Attention từ Quan điểm
Convolution Kernel Lớn
Chenghao Li, Chaoning Zhang, Senior, IEEE, Boheng Zeng, Yi Lu,
Pengbo Shi, Qingzi Chen, Jirui Liu, Lingyun Zhu,
Yang Yang, Senior, IEEE , Heng Tao Shen, Fellow, IEEE
Tóm tắt —Các cơ chế attention đã thúc đẩy đáng kể các mô hình thị giác bằng cách nắm bắt ngữ cảnh toàn cục một cách hiệu quả. Tuy nhiên, việc phụ thuộc vào các tập dữ liệu quy mô lớn và tài nguyên tính toán đáng kể đặt ra những thách thức trong các tình huống thiếu dữ liệu và hạn chế tài nguyên. Hơn nữa, các cơ chế self-attention truyền thống thiếu những thiên lệch quy nạp không gian vốn có, khiến chúng không tối ưu để mô hình hóa các đặc trưng cục bộ quan trọng cho các tác vụ liên quan đến các tập dữ liệu nhỏ hơn. Trong công trình này, chúng tôi giới thiệu Large Kernel Convolutional Attention (LKCA), một công thức mới diễn giải lại các thao tác attention như một phép tích chập kernel lớn đơn lẻ. Thiết kế này thống nhất điểm mạnh của các kiến trúc tích chập—tính cục bộ và bất biến dịch chuyển—với khả năng mô hình hóa ngữ cảnh toàn cục của self-attention. Bằng cách nhúng các thuộc tính này vào một khung hiệu quả về mặt tính toán, LKCA giải quyết những hạn chế chính của các cơ chế attention truyền thống. LKCA được đề xuất đạt được hiệu suất cạnh tranh trên các tác vụ thị giác khác nhau, đặc biệt trong các môi trường hạn chế dữ liệu. Kết quả thực nghiệm trên CIFAR-10, CIFAR-100, SVHN, và Tiny-ImageNet chứng minh khả năng xuất sắc của nó trong phân loại hình ảnh, vượt trội hơn các cơ chế attention thông thường và vision transformers trong các cài đặt mô hình nhỏ gọn. Những phát hiện này làm nổi bật tính hiệu quả của LKCA trong việc kết nối mô hình hóa đặc trưng cục bộ và toàn cục, cung cấp một giải pháp thực tế và mạnh mẽ cho các ứng dụng thực tế với dữ liệu và tài nguyên hạn chế.
Từ khóa chỉ mục —ConvNet, Vision Transformer, Large Kernel CNN, Attention Mechanism

I. GIỚI THIỆU
VISION Transformers (ViTs) đã nổi lên như một mạng backbone mạnh mẽ trong các mô hình thị giác hiện đại, mang lại sự thay đổi mô hình từ các kiến trúc tích chập truyền thống [1]. Bằng cách tận dụng các cơ chế self-attention, ViTs xuất sắc trong việc nắm bắt các phụ thuộc tầm xa và ngữ cảnh toàn cục, đạt được hiệu suất tiên tiến trong các tác vụ tốn nhiều tài nguyên [2], [3], [4], [5], [6], [7], [8], [9], [10], chẳng hạn như phân loại hình ảnh quy mô lớn[2], phân đoạn độ phân giải cao [3], và học đặc trưng nâng cao [4]. Tuy nhiên, tính hiệu quả của chúng thường đi kèm với các yêu cầu tính toán và dữ liệu đáng kể, khiến chúng ít lý tưởng hơn cho các tình huống hạn chế dữ liệu [4], [11].

Chenghao Li và Chaoning Zhang thuộc Khoa Khoa học Máy tính, Đại học Kyung Hee (e-mail: lch17692405449@gmail.com; chaoningzhang1990@gmail.com;). Boheng Zeng, Yang Yang, và Heng Tao Shen thuộc Khoa Khoa học Máy tính và Kỹ thuật, Đại học Khoa học và Công nghệ Điện tử, Trung Quốc (e-mail: zengboheng@std.uestc.edu.cn; yang.yang@uestc.edu.cn; shenhengtao@hotmail.com); Yi Lu thuộc Đại học Sư phạm Thủ đô (e-mail: 2230501004@cnu.edu.cn;). Pengbo Shi, Qingzi Chen, Jirui Liu, và Lingyun Zhu thuộc Đại học Công nghệ Trùng Khánh, Trung Quốc (e-mail: shipengbo@cqut.edu.cn; chenqingzi@cqut.edu.cn; liujirui@cqut.edu.cn; zly69cv@163.com;)

Ngược lại, Convolutional Neural Networks (CNNs) vẫn là lựa chọn mạnh mẽ trong các môi trường có tài nguyên tính toán hoặc tính khả dụng dữ liệu hạn chế [12], [13]. Do những thiên lệch quy nạp mạnh mẽ của chúng, chẳng hạn như tính cục bộ không gian và bất biến dịch chuyển, CNNs vốn hiệu quả trong việc học từ các tập dữ liệu nhỏ hơn và thể hiện tính mạnh mẽ đối với những biến thể tinh tế trong vị trí và hướng của đối tượng [14]. Những thuộc tính này đã làm cho CNNs trở thành giải pháp phù hợp cho các tác vụ trong cài đặt tài nguyên thấp, bao gồm các ứng dụng thị giác nhúng và các tập dữ liệu quy mô nhỏ. Với những điểm mạnh bổ sung, CNNs có thể được sử dụng để nâng cao hiệu suất của ViTs, đặc biệt trong các môi trường thiếu dữ liệu. Ví dụ, CNNs có thể phục vụ như những bộ trích xuất đặc trưng hiệu quả để tiền xử lý dữ liệu đầu vào cho ViTs, giảm tải tính toán trên các lớp transformer [15]. Ngoài ra, các kiến trúc lai kết hợp các mô-đun tích chập trong ViTs đã chứng minh tiềm năng trong việc giảm thiểu các hạn chế của ViTs, chẳng hạn như việc thiếu thiên lệch quy nạp không gian vốn có [15], trong khi vẫn duy trì khả năng nắm bắt ngữ cảnh toàn cục [16], [17]. Sự kết hợp này cho phép thiết kế các mô hình cân bằng hiệu quả tính toán với khả năng hiệu suất cao, mở đường cho việc triển khai chúng trên một phạm vi rộng hơn các tác vụ thị giác, từ các thiết bị hạn chế tài nguyên đến các hệ thống quy mô lớn.

Trong công trình này, chúng tôi giới thiệu một cơ chế được gọi là Large Kernel Convolutional Attention (LKCA), được thiết kế để kết hợp hiệu quả điểm mạnh của CNNs và ViTs trên các tập dữ liệu nhỏ. Theo cấu trúc ViT, LKCA sử dụng các kỹ thuật chính như Patch Embedding, Positional Encoding, Multi-Head Self-Attention, Feedforward Networks trong các lớp Transformer Encoder, và MLP Head cho đầu ra tác vụ. Để kết hợp hiệu quả lợi ích của CNNs, chúng tôi sử dụng một thao tác đơn giản nhưng hiệu quả cao trong LKCA: thay thế thao tác self-attention thông thường trong ViT bằng một thao tác tích chập kernel lớn đơn lẻ. Cách tiếp cận này không chỉ giữ lại lợi thế của bất biến dịch chuyển và thiên lệch quy nạp không gian trong CNNs mà còn tận dụng khả năng mô hình hóa toàn cục và xử lý phụ thuộc tầm xa của ViTs. Với phép tích chập kernel lớn này, LKCA nắm bắt thông tin cục bộ và toàn cục hiệu quả hơn, nâng cao sức mạnh biểu diễn của mô hình. Từ quan điểm cơ chế attention, LKCA có thể được xem như một phương pháp attention không gian sáng tạo. Nó nắm bắt hiệu quả các đặc trưng không gian trong các vùng cục bộ thông qua thao tác tích chập kernel lớn trong khi cũng hưởng lợi từ khả năng mô hình hóa toàn cục của ViT, do đó cải thiện đáng kể khả năng hiểu và xử lý các cấu trúc phức tạp trong hình ảnh của mô hình. Cách tiếp cận gần nhất với công trình của chúng tôi là Visual Attention Network (V AN)[18], thay thế cơ chế multi-head self-attention bằng ba loại tích chập kernel nhỏ: DW-Conv, DW-DConv, và 1×1-Conv. Ngược lại, phương pháp của chúng tôi đơn giản hơn, sử dụng một phép tích chập kernel lớn đơn lẻ[19] để thay thế MHSA. Chúng tôi chủ yếu đánh giá các mô hình nhỏ và trung bình, vì trước đây người ta tin rằng ViT vượt trội hơn CNNs trên các tập dữ liệu lớn và mô hình lớn, trong khi CNNs hoạt động tốt hơn ViTs trên các tập dữ liệu cực nhỏ và chuyên biệt. Trên tập dữ liệu CIFAR-10, LKCA chứng minh những cải thiện vượt trội so với ViT chuẩn qua các cấu hình thực nghiệm khác nhau. Quan trọng hơn, thiết kế LKCA hoạt động đặc biệt tốt trên các tác vụ downstream. Thứ nhất, trong tác vụ phân loại hình ảnh, chúng tôi tiến hành thực nghiệm trên các tập dữ liệu cổ điển, bao gồm CIFAR-10, CIFAR-100, SVHN, và Tiny-ImageNet, chứng minh hiệu quả của LKCA trên các tập dữ liệu này. Ngoài ra, LKCA đạt được kết quả đáng chú ý trong các tác vụ phân đoạn ngữ nghĩa, đặc biệt với các thuật toán UPerNet và FPN. Trên tập dữ liệu ADE20K, LKCA vượt trội hơn các mô hình khác về cả aAcc và mIoU sau 10,000 và 20,000 lần lặp. So sánh với nhiều mạng backbone xác nhận thêm hiệu suất vượt trội của LKCA về cả hiệu quả tham số và độ chính xác. Chúng tôi quy hiệu suất cao của LKCA cho những điểm mạnh bổ sung của CNNs và ViTs. Bằng cách kết hợp thành thạo của CNN trong trích xuất đặc trưng cục bộ với khả năng mô hình hóa ngữ cảnh toàn cục của ViT, LKCA đạt được hiệu suất đặc biệt trên các tác vụ thị giác khác nhau.

Trong công trình này, những đóng góp chính của chúng tôi như sau:
• Chúng tôi đề xuất Large Kernel Convolutional Attention (LKCA), một cơ chế mới tích hợp tích chập kernel lớn vào kiến trúc ViT để kết hợp lợi thế trích xuất đặc trưng cục bộ của CNNs với khả năng mô hình hóa toàn cục của ViTs.
• Chúng tôi chứng minh rằng LKCA đạt được hiệu suất mạnh mẽ trên các tác vụ khác nhau, bao gồm phân loại hình ảnh trên các tập dữ liệu như CIFAR-10, CIFAR-100, SVHN, và Tiny-ImageNet, cũng như phân đoạn ngữ nghĩa trên ADE20K, vượt trội hơn một số kiến trúc backbone hiện có về độ chính xác và hiệu quả.
• Chúng tôi giải quyết những hạn chế của cả CNNs và ViTs bằng cách tập trung vào các mô hình nhỏ và trung bình, cung cấp một giải pháp hiệu quả cho các tình huống hạn chế dữ liệu và tài nguyên.

II. CÁC CÔNG TRÌNH LIÊN QUAN

A. Cơ chế Attention trong Mô hình Thị giác

Các cơ chế attention [20], [21], [22], [23], [24], [25], [26], ban đầu được truyền cảm hứng bởi cách con người vô thức tập trung và ưu tiên các khía cạnh quan trọng của môi trường xung quanh, đã trở thành nền tảng trong nghiên cứu học máy và deep learning hiện đại. Những cơ chế này mô phỏng khả năng của con người trong việc phân bổ tài nguyên nhận thức một cách linh hoạt, làm cho chúng cực kỳ hiệu quả trong các tác vụ đa dạng từ Xử lý Ngôn ngữ Tự nhiên (NLP), thị giác máy tính, và hơn thế nữa. Các cơ chế attention thị giác [24], [25], một tập hợp con quan trọng của những kỹ thuật này, có thể được khái niệm hóa như một quá trình gán trọng số động cho các đặc trưng cụ thể trong hình ảnh đầu vào. Cơ chế cân trọng này giúp các mô hình tập trung vào những phần quan trọng nhất của hình ảnh, cải thiện đáng kể khả năng diễn giải và hiệu suất của các mô hình thị giác máy tính khác nhau. Các phương pháp attention thị giác đa dạng và có thể được phân loại rộng rãi thành bốn loại chính: channel attention [27], [28], [29], [30], spatial attention [31], [32], [33], temporal attention [34], [35], và branch attention [36], [37], [38]. Những loại này thường xuất hiện cùng với các kết hợp sáng tạo khác [39], [40], [41], [42], [43], [44], [45], phản ánh sự tinh tế ngày càng tăng của các phương pháp dựa trên attention. Non-Local Network [46] thành công sử dụng self-attention lần đầu tiên để mô phỏng các mối quan hệ phi cục bộ trong thị giác máy tính. CBAM [47] suy luận trọng số attention tuần tự dọc theo cả chiều không gian và kênh và có thể tích hợp liền mạch vào bất kỳ kiến trúc CNN nào. ECA [27] module giới thiệu một chiến lược tương tác kênh chéo cục bộ không rút gọn, nâng cao hiệu quả của mô-đun attention thông qua việc thực hiện hiệu quả tích chập 1D. EncNet [28] cải thiện đáng kể độ phân giải không gian của các nhãn phân đoạn ngữ nghĩa cấp pixel bằng cách giới thiệu một mô-đun mã hóa ngữ cảnh. OCNet [31] kết hợp một lược đồ self-attention thưa thớt xen kẽ với phương pháp ngữ cảnh đa tỷ lệ truyền thống, mô hình hóa hiệu quả các mối quan hệ dày đặc giữa các pixel và cung cấp thông tin ngữ cảnh phong phú hơn. SAGAN [32] sử dụng attention không gian để mô hình hóa các phụ thuộc tầm xa trong các tác vụ sinh hình ảnh. SCAN [34] đạt được hiệu suất vượt trội đáng kể trong Video Person Re-Identification bằng cách tinh chỉnh cả biểu diễn đặc trưng trong chuỗi và giữa các chuỗi thông qua việc giới thiệu attention thời gian vào các chuỗi video. SKNet [36] nâng cao khả năng nắm bắt các đối tượng ở các tỷ lệ khác nhau của mô hình bằng cách giới thiệu branch attention để kết hợp nhiều nhánh với các kích thước kernel khác nhau.

B. ConvNets với Kernel Lớn

So với các CNN kernel nhỏ truyền thống [12], [48], đã thống trị các mô hình deep learning do tính hiệu quả và đơn giản của chúng, các CNN kernel lớn [49], [50], [51], [52], [53], [54], [55], [19], [18] cung cấp một lợi thế riêng biệt với trường tiếp nhận hiệu quả rộng hơn và sở thích mạnh mẽ hơn trong việc nắm bắt thông tin hình dạng toàn cục. Những đặc tính này làm cho các mô hình kernel lớn đặc biệt hấp dẫn cho các tác vụ yêu cầu hiểu ngữ cảnh rộng rãi, chẳng hạn như phân đoạn và nhận dạng. Bất chấp những lợi thế này, các mô hình kernel lớn ban đầu, chẳng hạn như họ Inception [49], [50], [51], dần mất đi sự ưa chuộng sau thành công của các kiến trúc như VGG-Net [52], đã chứng minh hiệu quả của việc xếp chồng các kernel nhỏ hơn. Sự thay đổi trọng tâm này đã dẫn đến sự suy giảm tạm thời trong việc áp dụng các thiết kế kernel lớn trong các ứng dụng chính thống. Một kết quả nghiên cứu đáng chú ý là Global Convolutional Network (GCN) [53], sử dụng các kernel tích chập rất lớn 1×K và K×1 để tối ưu hóa các tác vụ phân đoạn ngữ nghĩa. Tuy nhiên, có báo cáo cho thấy rằng các kernel lớn có thể làm giảm hiệu suất ImageNet. Mặt khác, Local Relation Network (LRNet) [54] giới thiệu một toán tử tổng hợp không gian (LRLayer) thay thế cho tích chập tiêu chuẩn, có thể được xem như một tích chập động. ConvMixer [55], ngược lại, sử dụng các kernel tích chập lớn đến 9×9 để thay thế các thành phần "mixer" của ViT hoặc MLP. Được truyền cảm hứng bởi ViT, RepLKNet [19] sử dụng kernel tích chập lớn 31×31, và hiệu suất xuất sắc của nó chủ yếu được quy cho các trường tiếp nhận hiệu quả được xây dựng thông qua các kernel lớn. Đã được chứng minh rằng RepLKNet khai thác tốt hơn thông tin hình dạng so với các CNN truyền thống. Visual Attention Network (V AN) [18] sử dụng ba loại tích chập—DW-Conv, DW-DConv, và 1×1 Conv—để mở rộng trường tiếp nhận của tích chập và tích hợp chúng vào cơ chế attention.

C. ViTs với ERF Lớn

Sức mạnh của ViTs nằm ở khả năng xây dựng Effective Receptive Fields (ERFs) lớn, cho phép chúng nắm bắt các phụ thuộc toàn cục hiệu quả. Ban đầu, kiến trúc Transformer [56] được phát triển cho NLP, cách mạng hóa cách dữ liệu tuần tự được mô hình hóa thông qua cơ chế self-attention của nó. Việc di chuyển sang lĩnh vực Thị giác Máy tính bắt đầu với ViT [1], đã chứng minh khả năng biểu diễn mạnh mẽ của transformers trong xử lý dữ liệu hình ảnh. ViT chia hình ảnh thành các patch và xử lý chúng như các chuỗi, cho phép kiến trúc transformer xử lý hình ảnh theo cách tương tự như văn bản trong NLP. Dựa trên nền tảng này, nhiều biến thể ViT [57], [58], [59], [60], [61], [62], [63], [64], [65], [66] đã được đề xuất để nâng cao hiệu suất trên một loạt các tác vụ thị giác. Những biến thể này đã giới thiệu các cải tiến khác nhau nhằm giải quyết những thách thức độc đáo do dữ liệu hình ảnh đặt ra, đặc biệt là nhu cầu mô hình hóa hiệu quả cả các phụ thuộc toàn cục và cục bộ. Khả năng mô hình hóa toàn cục của ViTs làm cho chúng đặc biệt phù hợp để nắm bắt các mối quan hệ tầm xa giữa các patch hình ảnh, tạo ra hiệu quả các trường tiếp nhận lớn. Tuy nhiên, việc thiếu các cơ chế vốn có để mô hình hóa các mối quan hệ ngắn hạn, như được tìm thấy trong CNNs, đã là một hạn chế đáng chú ý của ViTs. CNNs xuất sắc trong việc nắm bắt các phụ thuộc cục bộ thông qua các thao tác tích chập bị hạn chế về mặt không gian, một thuộc tính mà ViTs vốn thiếu do xử lý dựa trên patch của chúng. Để giải quyết những thách thức này, các nhà nghiên cứu đã tập trung vào việc nâng cao khả năng mô hình hóa thông tin cục bộ của ViT, dẫn đến các thiết kế và kiến trúc sáng tạo. Transformer in Transformer (TNT) [57] nâng cao khả năng biểu diễn đặc trưng của các patch cục bộ thông qua attention nội bộ. Swin Transformer [58], [59] giải quyết các thách thức mô hình hóa cục bộ trong việc chuyển đổi transformers từ ngôn ngữ sang thị giác bằng cách giới thiệu một kiến trúc phân cấp với shift windowing. Twins [60] và CAT [61] thực hiện xen kẽ các lớp attention cục bộ và toàn cục theo từng lớp. Shuffle Transformer [62], [63] thiết lập hiệu quả các kết nối giữa các cửa sổ thông qua xáo trộn không gian với tích chập sâu và nâng cao mô hình hóa phụ thuộc ngắn hạn với các kết nối cửa sổ lân cận. RegionViT [64] giới thiệu các vùng và token cục bộ với các kích thước patch khác nhau, nâng cao mô hình hóa cục bộ thông qua cơ chế attention từ vùng đến cục bộ.

III. PHƯƠNG PHÁP

A. Sơ bộ

1) Xem xét lại Large Kernel Convolution: Ding et al. [19] xem xét lại mô hình thiết kế kernel lớn trong các CNN hiện đại, lấy cảm hứng từ ViT. Ding et al. khẳng định rằng việc sử dụng một vài kernel tích chập lớn thay vì nhiều kernel nhỏ hơn tạo thành một mô hình thiết kế mạnh mẽ hơn. Dựa trên khái niệm này, tác giả giới thiệu một kiến trúc CNN mới có tên RepLKNet [19], có kích thước kernel lên đến 31×31. So với các kernel 3×3 thường được sử dụng, RepLKNet đạt được kết quả hiệu suất tương đương hoặc tốt hơn Swin Transformer [58], với độ trễ thấp hơn. Sự khác biệt giữa tích chập kernel nhỏ và tích chập kernel lớn được thể hiện trong Hình 2.

2) So sánh LKC và MHSA: Tích chập có lợi thế tự nhiên khi các tham số được chia sẻ dựa trên vị trí địa lý. Cụ thể, các tham số giống nhau của cùng một kernel tích chập tác động lên mỗi vị trí trong trường tiếp nhận trong quá trình trượt cửa sổ. Điều này giới thiệu tính đồng biến và bất biến dịch chuyển vào thao tác tích chập. Tuy nhiên, trong Vision Transformer, điểm attention được tạo ra bởi cơ chế self-attention không thể hiện rõ ràng bất biến vị trí. Trọng số attention được xác định bởi chính các vector đặc trưng và các mã hóa vị trí được giới thiệu. Mặc dù điều này nâng cao khả năng khớp của mô hình, nó tăng đáng kể độ phức tạp của việc huấn luyện. Việc giới thiệu cơ chế chia sẻ tham số trong cơ chế attention của ViT có thể mang lại lợi ích mong đợi nhưng không tăng độ phức tạp tối ưu hóa.

B. Large Kernel Convolutional Attention (LKCA)

1) Shared Weight Position Operation: Trước tiên, chúng tôi khởi tạo một ma trận vuông kích thước N1/2×2−1 như các tham số học được, được ký hiệu là shared parameter weights, trong đó N đại diện cho số lượng patches, tương tự như việc khởi tạo các kernel tích chập. Sau đó, chúng tôi khởi tạo một cửa sổ kích thước N1/2 và duyệt cửa sổ này khi nó trượt qua ma trận shared parameter weights từ góc dưới-phải đến góc trên-phải. Ở mỗi bước trượt, ma trận trong cửa sổ được làm phẳng theo hàng thành một vector hàng kích thước (1, N) và được lưu trữ. Quá trình này được lặp lại khi cửa sổ trượt, nối các vector hàng thu được từ mỗi bước. Tổng cộng, cửa sổ trượt N lần. Do đó, ma trận được nối có hình dạng (N, N), phù hợp với hình dạng của ma trận trọng số attention. Ma trận được nối này phục vụ như điểm attention mới, trải qua phép nhân ma trận với ma trận Value được ánh xạ tuyến tính. Bước này được gọi là Shared Weight Matrix Attention. Thuật toán 1 trình bày pseudocode để thực hiện LKCA từ quan điểm attention.

Thuật toán 1 Quá trình lan truyền tiến của Attention-ViT
1:function ATTENTION VITFORWARD (X)
2: đầu vào: X– token đầu vào, trong đó X∈Rb×n×d
3: khởi tạo: H, W ←√n,weight ←tensor zero có hình dạng (H∗2−1, W∗2−1)
4: V←Linear (X)
5: for i= 0 to H−1 do
6: for j= 0 to W−1 do
7: start row←H−1−i
8: start col←W−1−j
9: window ←weight [start row:start row+ H, start col:start col+W]
10: window vector ←flatten (window )
11: attn score [i∗W+j,:]←window vector
12: end for
13: end for
14: Out←attn score @V
15: return Out
16:end function

2) Từ Quan điểm Tích chập: Thực hiện LKCA: Large Kernel Convolutional Attention (LKCA) có thể được biểu diễn tương đương như thao tác attention với các tham số được chia sẻ được mô tả ở trên về mặt code và thực hiện toán học. Thuật toán 2 trình bày pseudocode để thực hiện LKCA từ quan điểm tích chập. Kernel tích chập là ma trận trọng số học được kích thước N1/2×2−1, và bản đồ đặc trưng đầu vào là ma trận V kích thước N1/2 với số lượng kênh bằng chiều ẩn. Sau zero-padding kích thước N1/2−1, thao tác tích chập được áp dụng lên hai ma trận.

LKCA (x)=Conv2d (MLP (x),LargeKernel )

Thuật toán 2 Quá trình lan truyền tiến của Attention-CNN
1:function ATTENTION CNN FORWARD (X)
2: đầu vào: X– token đầu vào, trong đó X∈Rb×n×d
3: khởi tạo: H, W ←√n,weight ←tensor zero có hình dạng (H∗2−1, W∗2−1)
4: V←Linear (X)
5: V←rearrange (V,′b(HW)d→(bd)HW′)
6: Out←conv2d (V, weight, zero padding = (H− 1, W−1))
7: Out←rearrange (Out,′(bd)HW→b(HW)d′)
8: return Out
9:end function

3) Kiến trúc Tổng thể: Việc tích hợp large-kernel convolution attention nâng cao khả năng nắm bắt thông tin vị trí của mô hình. Tuy nhiên, nó đồng thời làm yếu các mối quan hệ giữa các patches và trong từng patch riêng lẻ. Do đó, chúng tôi xen kẽ các mô-đun large-kernel convolution attention trong kiến trúc ViT ban đầu trong khi vẫn giữ lại các mô-đun self-attention. Tổng quan mô hình được biểu diễn bởi các công thức sau. Một hình ảnh x∈RH×W×C, trong đó (H, W) là độ phân giải của hình ảnh gốc, và C là số lượng kênh. Một chuỗi các patch 2D được làm phẳng xp∈RN×(P2·C), trong đó (P, P) là độ phân giải của mỗi khối hình ảnh, và N=HW/P2 là số lượng khối được tạo ra. Các patches được chiếu tuyến tính bởi ma trận trọng số E vào không gian D chiều và được cộng với position embedding Epos. Khác với ViT ban đầu, chúng tôi bỏ qua việc sử dụng [class] token. Thay vì trích xuất [class] token từ lớp tuyến tính cuối cùng làm thông tin phân loại, chúng tôi áp dụng trung bình của các vector đặc trưng lớp cuối cùng để phân loại. Ngoài ra, chúng tôi chèn mô-đun Large Kernel Convolution Attention (LKCA) trước mô-đun Multi-Layer Perceptron (MLP) và áp dụng Layer Normalization (LN) trước mỗi mô-đun.

z0= [x1pE;x2pE;···;xNpE] +Epos, (1)
E∈R(P2·C)×D,Epos∈RN×D (2)
z′ℓ= LKCA (LN ( zℓ−1)) +zℓ−1, ℓ = 1. . . L (3)
zℓ= MLP (LN ( z′ℓ)) +z′ℓ, ℓ = 1. . . L (4)
y= LN ( zL) (5)

IV. THỰC NGHIỆM

A. Cài đặt Thực nghiệm

Trong cài đặt thực nghiệm của chúng tôi, chúng tôi tiến hành các thí nghiệm phân loại hình ảnh mở rộng trên bốn tập dữ liệu nổi bật: CIFAR-10, CIFAR-100, SVHN, và Tiny-ImageNet. Cả bốn tập dữ liệu đều có đặc điểm độ phân giải thấp, với kích thước hình ảnh là 32 cho CIFAR-10, CIFAR-100, và SVHN, và 64 cho Tiny-ImageNet. Chế độ huấn luyện được duy trì nhất quán qua tất cả các thí nghiệm, được tiến hành trên một GPU RTX 2080ti duy nhất. Việc huấn luyện kéo dài 100 epochs, sử dụng 4 workers tải dữ liệu, và batch size là 128. Đối với optimizer, chúng tôi chọn AdamW [70]. Tốc độ học mặc định được đặt là 0.001, với weight decay là 0.05. Chúng tôi sử dụng cosine annealing với khoảng thời gian warm-up là 10 epochs. Trong giai đoạn huấn luyện, chúng tôi sử dụng Stochastic Depth [71], Label Smoothing [50], và Random Erasing [72] với xác suất 0.25, diện tích xóa tối đa 0.4, và khía cạnh diện tích xóa 0.3 như các chiến lược regularization để nâng cao tính mạnh mẽ và khả năng khái quát của mô hình. Về data augmentation, chúng tôi sử dụng các kỹ thuật khác nhau. Cụ thể, chúng tôi áp dụng RandomHorizontalFlip, RandomCrop [73], Mixup [74], và Cutmix [75]. Ngoài ra, chúng tôi tích hợp Autoaugmentation [76], sử dụng CIFAR10 Policy cho CIFAR-10 và CIFAR-100, SVHN Policy cho SVHN, và ImageNet Policy cho Tiny-ImageNet.

B. So sánh ViT và LKCA.

Ban đầu, chúng tôi tiến hành các thí nghiệm mở rộng trên CIFAR-10, CIFAR-100, và Tiny-ImageNet để so sánh hiệu suất của ViT ban đầu với phiên bản ViT được sửa đổi LKCA có thứ tự độ lớn tương tự về mặt tham số. Mục đích của những thí nghiệm này là xác nhận tính hiệu quả của LKCA qua các độ lớn tham số và tập dữ liệu khác nhau. Kích thước tham số của cả ViT và ViT được sửa đổi LKCA thay đổi qua một loạt các độ lớn, từ các mô hình quy mô nhỏ với 0.5 triệu tham số đến các mô hình quy mô lớn với 10 triệu tham số. Cụ thể, các độ lớn tham số được khám phá ở các cấp độ 0.5M, 1M, 2M, 4M, và 8M. Điều quan trọng cần lưu ý là benchmark đại diện cho kích thước mô hình qua tất cả các biến thể được cố định ở 2.69 triệu tham số, và nó độc lập với bất kỳ phân loại chủ quan nào là lớn hoặc nhỏ. Kích thước mô hình được chuẩn hóa này phục vụ như điểm tham chiếu để đánh giá hiệu suất của các biến thể khác nhau, đảm bảo so sánh nhất quán qua thí nghiệm.

Bảng II trình bày so sánh hiệu suất của ViT và các biến thể LKCA của nó trên tác vụ phân loại hình ảnh CIFAR-10, cùng với hiệu suất của chúng qua các thứ tự độ lớn khác nhau về mặt tham số. Bảng bao gồm độ chính xác Top-1 (Acc), số lượng tham số (# Params), và Floating Point Operations (Flops) cho mỗi mô hình. ViT và ViT-LKCA: Mô hình ViT đạt được độ chính xác Top-1 là 93.36%, với 2.69M tham số và 174.25M Flops. ViT-LKCA vượt trội với độ chính xác Top-1 là 94.11% (cải thiện 0.75 điểm phần trăm) và có 2.69M tham số và 170.81M Flops. Mô hình quy mô nhỏ (độ lớn tham số 0.5M): Mô hình ViT/0.5M có độ chính xác Top-1 là 87.83%, 0.48M tham số, và 30.58M Flops. ViT/0.5M-LKCA, với cải thiện LKCA, cải thiện đáng kể độ chính xác lên 90.94% (tăng 3.11 điểm phần trăm) với 0.50M tham số và 31.34M Flops. Các mô hình quy mô trung bình (độ lớn tham số 1M, 2M, 4M): Đối với các mô hình ViT và ViT-LKCA với 1M, 2M, và 4M tham số, LKCA liên tục mang lại độ chính xác Top-1 cao hơn. Ví dụ, ViT/1M-LKCA đạt được cải thiện 1.33 điểm phần trăm, đạt độ chính xác 92.49%. Mô hình quy mô lớn (độ lớn tham số 8M): Đối với các mô hình ViT và ViT-LKCA với 8M tham số, khoảng cách hiệu suất hẹp hơn. ViT/8M-LKCA cải thiện nhẹ độ chính xác Top-1 0.42 điểm phần trăm, đạt 94.88%.

Bảng III minh họa so sánh giữa ViT và LKCA trong ngữ cảnh CIFAR-100. ViT và ViT-LKCA: Mô hình ViT cơ bản đạt được độ chính xác Top-1 là 72.65% với 2.69M tham số và 174.25M Flops. Ngược lại, ViT-LKCA thể hiện hiệu suất vượt trội, đạt độ chính xác Top-1 là 76.50% (tăng 3.85 điểm phần trăm) với cùng 2.69M tham số và 170.81M Flops. Mô hình quy mô nhỏ (độ lớn tham số 0.5M): Mô hình ViT/0.5M đạt được độ chính xác Top-1 là 64.68% với 0.48M tham số và 30.58M Flops. ViT/0.5M-LKCA, có các cải tiến LKCA, cải thiện đáng kể độ chính xác lên 69.35% (tăng đáng chú ý 4.67 điểm phần trăm) với 0.50M tham số và 31.34M Flops. Các mô hình quy mô trung bình (độ lớn tham số 1M, 2M, 4M): Các mô hình ViT và ViT-LKCA với 1M, 2M, và 4M tham số liên tục hưởng lợi từ các cải tiến LKCA. Ví dụ, ViT/2M-LKCA đạt được cải thiện đáng chú ý 4.77 điểm phần trăm và đạt độ chính xác 75.94%. Mô hình quy mô lớn (độ lớn tham số 8M): Đối với các mô hình ViT và ViT-LKCA với 8M tham số, sự khác biệt hiệu suất tinh tế hơn. ViT/8M-LKCA thể hiện cải thiện lớn 5.18 điểm phần trăm, đạt độ chính xác Top-1 là 78.57%.

Bảng IV cung cấp so sánh trên Tiny-ImageNet. ViT và ViT-LKCA: Mô hình ViT baseline đạt được độ chính xác Top-1 là 55.74% với 2.69M tham số và 174.25M Flops. Ngược lại, mô hình ViT-LKCA vượt trội đáng kể với độ chính xác Top-1 là 60.95% (tăng 5.21 điểm phần trăm) và có cùng 2.69M tham số và 170.81M Flops. Mô hình quy mô nhỏ (độ lớn tham số 0.5M): Mô hình ViT/0.5M đạt được độ chính xác Top-1 là 46.53% với 0.48M tham số và 30.58M Flops. ViT/0.5M-LKCA, tích hợp các cải tiến LKCA, cải thiện đáng kể độ chính xác lên 50.59% (tăng 4.06 điểm phần trăm) với 0.50M tham số và 31.34M Flops. Các mô hình quy mô trung bình (độ lớn tham số 1M, 2M, 4M): Với ViT/4M-LKCA đạt được cải thiện đáng chú ý 6.14 điểm phần trăm và đạt độ chính xác 62.30%. Mô hình quy mô lớn (độ lớn tham số 8M): Đối với các mô hình ViT và ViT-LKCA với 8M tham số, ViT/8M-LKCA thể hiện cải thiện đáng kể 5.99 điểm phần trăm, đạt độ chính xác Top-1 là 63.38%.

Tóm lại, cải tiến LKCA chứng minh những cải thiện hiệu suất đáng kể qua các độ lớn tham số khác nhau, xác nhận tính hiệu quả của nó trên các mô hình ViT.

C. So sánh các Biến thể ViT và LKCA.

Tiếp theo, chúng tôi so sánh LKCA với các biến thể Vision Transformer chính thống khác. Đầu tiên, chúng tôi cung cấp so sánh toàn diện của LKCA với các biến thể ViT chính thống khác nhau trên các tập dữ liệu khác nhau, bao gồm CIFAR-10, CIFAR-100, và SVHN. Độ chính xác Top-1, số lượng tham số, và Floating Point Operations của mỗi mô hình được trình bày. Trong các loại deep ViT, localized ViT, và lightweight ViT, chúng tôi đã chọn các mô hình đại diện như Swin-T [58], CaiT-T [67] và MobileViTv2 [68], để so sánh với ViT chuẩn [1] và LKCA được đề xuất của chúng tôi. Từ thông tin trong bảng I, trong ngữ cảnh các mô hình quy mô nhỏ, Swin-T và CaiT-T hoạt động tương tự về độ chính xác Top-1, trong khi MobileViTv2 tụt lại một chút nhưng có lợi thế đáng kể về hiệu quả tính toán. So với các mô hình quy mô nhỏ khác, LKCA-T đạt được độ chính xác Top-1 cao hơn. Chuyển sang các mô hình quy mô lớn, đây là xu hướng chung rằng chúng thường vượt trội hơn các đối tác quy mô nhỏ. LKCA-B nổi bật bằng cách vượt trội đáng kể so với ViT và MobileViTv2 về độ chính xác Top-1, và nó vượt trội nhẹ so với CaiT-S và Swin-S. Nhìn chung, trong CIFAR10 và CIFAR100, LKCA chứng minh độ chính xác Top-1 cao hơn so với các mô hình khác. Tuy nhiên, không có lợi thế rõ ràng nào được quan sát thấy trong tập dữ liệu SVHN.

Cuối cùng, chúng tôi đã xác nhận hiệu suất của các biến thể ViT khác nhau và mô hình LKCA trên tập dữ liệu Tiny-ImageNet, đồng thời cũng cung cấp chi tiết về số lượng tham số và độ phức tạp tính toán của các mô hình trong Bảng V. Trong loại các mô hình quy mô nhỏ, LKCA-T đạt được độ chính xác Top-1 là 57.29%, chứng minh hiệu suất xuất sắc so với các mô hình khác. Hiệu suất của nó vượt trội so với một số mô hình quy mô nhỏ thông thường. Chuyển sang các mô hình quy mô trung bình, LKCA-B đạt được độ chính xác Top-1 ấn tượng là 60.95%, một lần nữa thể hiện hiệu suất xuất sắc. Trong lĩnh vực các mô hình quy mô lớn, LKCA-L đạt được độ chính xác Top-1 là 63.43%, thể hiện lợi thế đáng kể về độ chính xác so với các mô hình cùng quy mô. Nhìn chung, qua các quy mô khác nhau, LKCA chứng minh độ chính xác Top-1 xuất sắc trên tập dữ liệu Tiny-ImageNet so với các đối thủ cạnh tranh khác.

D. Kết quả Phân đoạn Ngữ nghĩa

Trong công trình này, chúng tôi theo các công trình trước đây để chọn phân loại hình ảnh làm tác vụ chính để đánh giá hiệu quả của các kiến trúc mô hình. Tuy nhiên, chúng tôi bổ sung cung cấp kết quả phân đoạn ngữ nghĩa trên tập dữ liệu ADE20K [81]. Cụ thể, chúng tôi sử dụng MMSegmentation [82] để tiến hành các thí nghiệm. Tất cả các thí nghiệm của chúng tôi đều thử nghiệm hai thuật toán: UperNet [83] và FPN [84]. Chúng tôi tránh sử dụng pre-training và thay vào đó huấn luyện các mô hình từ đầu trên tập dữ liệu. Về các thước đo đánh giá, chúng tôi đánh giá các mô hình dựa trên aAcc và mIoU trong 10 danh mục điểm cao. Trong nghiên cứu loại bỏ của mạng backbone, chúng tôi kiểm tra các biến thể ViT và CNN phổ biến khác nhau và khám phá hiệu suất của chúng trong 0 - 20,000 vòng lặp.

Chúng tôi tiến hành thí nghiệm trên nhiều mạng backbone. Bảng VI cung cấp các thước đo hiệu suất cho thuật toán UPerNet sử dụng các backbone khác nhau (ViT và LKCA), thuật toán lặp, và số lần lặp (2000, 4000, 8000, 16000). Ở mỗi số lần lặp, LKCA-B liên tục vượt trội hơn ViT-B về aAcc và mIoU, cho thấy hiệu suất vượt trội của LKCA-B trong tác vụ UPerNet. Khi số lần lặp tăng, cả hai mô hình đều cho thấy cải thiện hiệu suất, nhưng ở mỗi số lần lặp, LKCA-B duy trì vị trí dẫn đầu. Nhìn chung, LKCA-B chứng minh hiệu suất vượt trội so với ViT-B ở các số lần lặp khác nhau, với độ chính xác pixel cao hơn và mean Intersection over Union. Ngoài ra, LKCA-B thể hiện lợi thế về số lượng tham số và độ phức tạp tính toán.

Bảng VII trình bày so sánh hiệu suất giữa các backbone khác nhau (ViT-S, ResNet-50, Swin-S, Twins PCPVT-S, LKCA-S) dưới các số lần lặp khác nhau và qua hai cài đặt tác vụ khác nhau (UPerNet và FPN). Trong tác vụ UPerNet, LKCA-S liên tục vượt trội hơn các mô hình khác ở các số lần lặp khác nhau, thể hiện aAcc và mIoU cao hơn. Nó đạt được hiệu suất vượt trội so với các mô hình khác. Tương tự, trong tác vụ FPN, LKCA-S chứng minh hiệu suất xuất sắc qua các số lần lặp khác nhau, thể hiện aAcc và mIoU cao hơn, làm cho nó cạnh tranh với các mô hình khác. LKCA-S liên tục thể hiện hiệu suất xuất sắc qua các tác vụ và cài đặt lặp khác nhau, trong khi duy trì độ phức tạp tính toán và số lượng tham số tương đối thấp hơn.

V. KẾT LUẬN

Trong nghiên cứu này, chúng tôi xem xét lại mối quan hệ giữa cơ chế attention trong visual transformers và các mạng tích chập kernel lớn, đề xuất một attention thị giác mới được gọi là Large Kernel Convolutional Attention (LKCA) nhằm vào các tập dữ liệu nhỏ. Nó sử dụng một phép tích chập kernel lớn đơn lẻ để đơn giản hóa các thao tác attention. So với cách tiếp cận trước đây, Large Kernel Attention (LKA), sử dụng nhiều tích chập kernel nhỏ, phương pháp của chúng tôi đạt được hiệu quả tương tự nhưng đơn giản và đơn giản hơn. LKCA kết hợp lợi thế của mạng nơ-ron tích chập và visual transformers, có trường tiếp nhận lớn hơn, tính cục bộ, và chia sẻ tham số. Chúng tôi giải thích ưu điểm của LKCA từ quan điểm của cả tích chập và attention, cung cấp các thực hiện code tương đương cho mỗi quan điểm. Kết quả thực nghiệm chứng minh rằng LKCA, được thực hiện từ cả quan điểm tích chập và attention, thể hiện hiệu suất tương đương. Chúng tôi thực nghiệm mở rộng với các biến thể LKCA của ViT trên nhiều tập dữ liệu benchmark, bao gồm CIFAR-10, CIFAR-100, SVHN, Tiny-ImageNet, và ADE20K, cho các tác vụ phân loại và phân đoạn. Kết quả thực nghiệm cho thấy rằng LKCA chứng minh hiệu suất cạnh tranh trong các tác vụ thị giác.

TÀI LIỆU THAM KHẢO
[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.
[2] L. Yuan, Q. Hou, Z. Jiang, J. Feng, and S. Yan, "Volo: Vision outlooker for visual recognition," IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 5, pp. 6575–6586, 2022.
[3] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 568–578.
[4] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, "Masked autoencoders are scalable vision learners," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16 000–16 009.
[5] J. Jiao, Y.-M. Tang, K.-Y. Lin, Y. Gao, A. J. Ma, Y. Wang, and W.-S. Zheng, "Dilateformer: Multi-scale dilated transformer for visual recognition," IEEE Transactions on Multimedia, vol. 25, pp. 8906–8919, 2023.
[6] T. Zhu, L. Li, J. Yang, S. Zhao, H. Liu, and J. Qian, "Multimodal sentiment analysis with image-text interaction network," IEEE transactions on multimedia, vol. 25, pp. 3375–3385, 2022.
[7] J. Chen, J. Ding, and J. Ma, "Hitfusion: Infrared and visible image fusion for high-level vision tasks using transformer," IEEE Transactions on Multimedia, 2024.
[8] C. Deng, M. Wang, L. Liu, Y. Liu, and Y. Jiang, "Extended feature pyramid network for small object detection," IEEE Transactions on Multimedia, vol. 24, pp. 1968–1979, 2021.
[9] Q. Xu, J. Wang, B. Jiang, and B. Luo, "Fine-grained visual classification via internal ensemble learning transformer," IEEE Transactions on Multimedia, vol. 25, pp. 9015–9028, 2023.
[10] H. Ma, J. Wang, H. Lin, B. Zhang, Y. Zhang, and B. Xu, "A transformer-based model with self-distillation for multimodal emotion recognition in conversations," IEEE Transactions on Multimedia, 2023.
[11] S. H. Lee, S. Lee, and B. C. Song, "Vision transformer for small-size datasets," arXiv preprint arXiv:2112.13492, 2021.
[12] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," Advances in neural information processing systems, vol. 25, 2012.
[14] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, "Overfeat: Integrated recognition, localization and detection using convolutional networks," arXiv preprint arXiv:1312.6229, 2013.
[15] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi, "Escaping the big data paradigm with compact transformers," arXiv preprint arXiv:2104.05704, 2021.
[16] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy, "Do vision transformers see like convolutional neural networks?" Advances in Neural Information Processing Systems, vol. 34, pp. 12 116–12 128, 2021.
[17] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, "Scaling up visual and vision-language representation learning with noisy text supervision," in International conference on machine learning. PMLR, 2021, pp. 4904–4916.
[18] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and S.-M. Hu, "Visual attention network," Computational Visual Media, vol. 9, no. 4, pp. 733–752, 2023.
[19] X. Ding, X. Zhang, J. Han, and G. Ding, "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 963–11 975.
[20] E. A. Nadaraya, "On estimating regression," Theory of Probability & Its Applications, vol. 9, no. 1, pp. 141–142, 1964.
[21] G. S. Watson, "Smooth regression analysis," Sankhyā: The Indian Journal of Statistics, Series A, pp. 359–372, 1964.
[22] A. Galassi, M. Lippi, and P. Torroni, "Attention in natural language processing," IEEE transactions on neural networks and learning systems, vol. 32, no. 10, pp. 4291–4308, 2020.
[23] K. Cho, A. Courville, and Y. Bengio, "Describing multimedia content using attention-based encoder-decoder networks," IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1875–1886, 2015.
[24] F. Wang and D. M. Tax, "Survey on the attention based rnn model and its applications in computer vision," arXiv preprint arXiv:1601.06823, 2016.
[25] M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang, T.-J. Mu, S.-H. Zhang, R. R. Martin, M.-M. Cheng, and S.-M. Hu, "Attention mechanisms in computer vision: A survey," Computational visual media, vol. 8, no. 3, pp. 331–368, 2022.
[26] X. Yang, S. Feng, D. Wang, and Y. Zhang, "Image-text multimodal emotion classification via multi-view attentional network," IEEE Transactions on Multimedia, vol. 23, pp. 4014–4026, 2020.
[27] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, "Eca-net: Efficient channel attention for deep convolutional neural networks," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 534–11 542.
[28] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal, "Context encoding for semantic segmentation," in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2018, pp. 7151–7160.
[29] Z. Chen, Y. Li, S. Bengio, and S. Si, "You look twice: Gaternet for dynamic filter selection in cnns," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9172–9180.
[30] H. Shi, G. Lin, H. Wang, T.-Y. Hung, and Z. Wang, "Spsequencenet: Semantic segmentation network on 4d point clouds," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 4574–4583.
[31] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, "Ocnet: Object context network for scene parsing," arXiv preprint arXiv:1809.00916, 2018.
[32] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, "Self-attention generative adversarial networks," in International conference on machine learning. PMLR, 2019, pp. 7354–7363.
[33] V. Mnih, N. Heess, A. Graves et al., "Recurrent models of visual attention," Advances in neural information processing systems, vol. 27, 2014.
[34] R. Zhang, J. Li, H. Sun, Y. Ge, P. Luo, X. Wang, and L. Lin, "Scan: Self-and-collaborative attention network for video person re-identification," IEEE Transactions on Image Processing, vol. 28, no. 10, pp. 4870–4882, 2019.
[35] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, "Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1169–1178.
[36] X. Li, W. Wang, X. Hu, and J. Yang, "Selective kernel networks," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 510–519.
[37] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J. Mueller, R. Manmatha et al., "Resnest: Split-attention networks," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 2736–2746.
[38] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, "Dynamic convolution: Attention over convolution kernels," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 030–11 039.
[39] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua, "Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5659–5667.
[40] J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, "Bam: Bottleneck attention module," arXiv preprint arXiv:1807.06514, 2018.
[41] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and X. Tang, "Residual attention network for image classification," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 3156–3164.
[42] D. Misra, T. Nalamada, A. U. Arasanipalai, and Q. Hou, "Rotate to attend: Convolutional triplet attention module," in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2021, pp. 3139–3148.
[43] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, "Dual attention network for scene segmentation," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 3146–3154.
[44] W. Li, X. Zhu, and S. Gong, "Harmonious attention network for person re-identification," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2285–2294.
[45] C. Yan, Y. Tu, X. Wang, Y. Zhang, X. Hao, Y. Zhang, and Q. Dai, "Stat: Spatial-temporal attention mechanism for video captioning," IEEE transactions on multimedia, vol. 22, no. 1, pp. 229–241, 2019.
[46] X. Wang, R. Girshick, A. Gupta, and K. He, "Non-local neural networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7794–7803.
[47] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, "Cbam: Convolutional block attention module," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 3–19.
[48] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, "Densely connected convolutional networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700–4708.
[49] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, "Inception-v4, inception-resnet and the impact of residual connections on learning," in Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017.
[50] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, "Rethinking the inception architecture for computer vision," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.
[51] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, "Going deeper with convolutions," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1–9.
[52] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.
[53] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, "Large kernel matters–improve semantic segmentation by global convolutional network," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4353–4361.
[54] H. Hu, Z. Zhang, Z. Xie, and S. Lin, "Local relation networks for image recognition," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 3464–3473.
[55] A. Trockman and J. Z. Kolter, "Patches are all you need?" arXiv preprint arXiv:2201.09792, 2022.
[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[57] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, "Transformer in transformer," Advances in Neural Information Processing Systems, vol. 34, pp. 15 908–15 919, 2021.
[58] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 012–10 022.
[59] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo, "Cswin transformer: A general vision transformer backbone with cross-shaped windows," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 124–12 134.
[60] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen, "Twins: Revisiting the design of spatial attention in vision transformers," Advances in Neural Information Processing Systems, vol. 34, pp. 9355–9366, 2021.
[61] H. Lin, X. Cheng, X. Wu, and D. Shen, "Cat: Cross attention in vision transformer," in 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022, pp. 1–6.
[62] Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu, "Shuffle transformer: Rethinking spatial shuffle for vision transformer," arXiv preprint arXiv:2106.03650, 2021.
[63] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, "Msg-transformer: Exchanging local spatial information by manipulating messenger tokens," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 063–12 072.
[64] C.-F. Chen, R. Panda, and Q. Fan, "Regionvit: Regional-to-local attention for vision transformers," arXiv preprint arXiv:2106.02689, 2021.
[65] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and J. Feng, "Deepvit: Towards deeper vision transformer," arXiv preprint arXiv:2103.11886, 2021.
[66] P. Wang, X. Wang, F. Wang, M. Lin, S. Chang, H. Li, and R. Jin, "Kvt: k-nn attention for boosting vision transformers," in European conference on computer vision. Springer, 2022, pp. 285–302.
[67] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jégou, "Going deeper with image transformers," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 32–42.
[68] S. Mehta and M. Rastegari, "Separable self-attention for mobile vision transformers," arXiv preprint arXiv:2206.02680, 2022.
[69] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, "Cvt: Introducing convolutions to vision transformers," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 22–31.
[70] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.
[71] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, "Deep networks with stochastic depth," in Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14. Springer, 2016, pp. 646–661.
[72] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, "Random erasing data augmentation," in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 07, 2020, pp. 13 001–13 008.
[73] S. Zagoruyko and N. Komodakis, "Wide residual networks," arXiv preprint arXiv:1605.07146, 2016.
[74] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, "mixup: Beyond empirical risk minimization," arXiv preprint arXiv:1710.09412, 2017.
[75] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, "Cutmix: Regularization strategy to train strong classifiers with localizable features," in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6023–6032.
[76] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, "Autoaugment: Learning augmentation strategies from data," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 113–123.
[77] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay, J. Feng, and S. Yan, "Tokens-to-token vit: Training vision transformers from scratch on imagenet," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 558–567.
[78] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, "Roformer: Enhanced transformer with rotary position embedding," Neurocomputing, vol. 568, p. 127063, 2024.
[79] A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek et al., "Xcit: Cross-covariance image transformers," Advances in neural information processing systems, vol. 34, pp. 20 014–20 027, 2021.
[80] C.-F. R. Chen, Q. Fan, and R. Panda, "Crossvit: Cross-attention multi-scale vision transformer for image classification," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 357–366.
[81] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba, "Semantic understanding of scenes through the ade20k dataset," International Journal of Computer Vision, vol. 127, pp. 302–321, 2019.
[82] M. Contributors, "MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark," https://github.com/open-mmlab/mmsegmentation, 2020.
[83] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, "Unified perceptual parsing for scene understanding," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 418–434.
[84] A. Kirillov, R. Girshick, K. He, and P. Dollár, "Panoptic feature pyramid networks," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6399–6408.
