# 2312.04927.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/convolution/2312.04927.pdf
# Kích thước tệp: 3249054 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Zoology: Đo lường và Cải thiện Khả năng Ghi nhớ trong các Mô hình Ngôn ngữ Hiệu quả

Simran Arora∗†, Sabri Eyuboglu∗†, Aman Timalsina△, Isys Johnson‡, Michael Poli†, James Zou†, Atri Rudra‡, và Christopher R´ e†
†Đại học Stanford
‡Đại học Buffalo
△Đại học Purdue
‡{simran, eyuboglu, poli, jamesz, chrismre }@cs.stanford.edu
‡{isysjohn,atri }@buffalo.edu
△{atimalsi }@purdue.edu
11 tháng 12, 2023

Tóm tắt
Các mô hình ngôn ngữ không sử dụng attention kết hợp gating và convolution đang ngày càng phổ biến do hiệu quả và hiệu suất cạnh tranh ngày càng tăng. Để hiểu rõ hơn về các kiến trúc này, chúng tôi tiền huấn luyện một bộ 17 mô hình ngôn ngữ attention và gated-convolution, phát hiện rằng các kiến trúc gated-convolution tiên tiến nhất vẫn kém hiệu suất so với attention lên đến 2.1 điểm perplexity trên Pile. Trong phân tích chi tiết, chúng tôi thấy 82% khoảng cách này được giải thích bởi khả năng ghi nhớ thông tin đã được đề cập trước đó trong ngữ cảnh của mỗi mô hình, ví dụ: Hakuna Matata có nghĩa là không lo lắng Hakuna Matata có nghĩa là không → ??. Trong tác vụ này, được gọi là ghi nhớ liên kết, chúng tôi thấy rằng attention vượt trội hơn gated-convolution với một khoảng cách lớn: mô hình attention 70M tham số vượt trội hơn mô hình gated-convolution 1.4 tỷ tham số trong ghi nhớ liên kết. Điều này đáng ngạc nhiên vì nghiên cứu trước đây cho thấy gated convolution có thể giải quyết hoàn hảo các bài kiểm tra tổng hợp cho khả năng AR. Để thu hẹp khoảng cách giữa tổng hợp và ngôn ngữ thực, chúng tôi phát triển một hình thức hóa mới của tác vụ được gọi là ghi nhớ liên kết đa truy vấn (Mqar) phản ánh tốt hơn ngôn ngữ thực tế. Chúng tôi thực hiện nghiên cứu thực nghiệm và lý thuyết về Mqar để làm rõ sự khác biệt trong hiệu quả tham số của ghi nhớ attention và gated-convolution. Dựa trên phân tích của chúng tôi, chúng tôi đánh giá các hybrid convolution-attention đơn giản và cho thấy rằng các hybrid với mẫu attention thưa thớt phụ thuộc vào đầu vào có thể thu hẹp 97.4% khoảng cách đến attention, trong khi vẫn duy trì khả năng mở rộng dưới bậc hai. Mã nguồn của chúng tôi có thể truy cập tại: https://github.com/HazyResearch/zoology .

1 Giới thiệu
Hai tiến bộ – gating và long convolution – đã tạo ra làn sóng hứng thú xung quanh các mô hình ngôn ngữ gated-convolution [Fu et al., 2023a, Ma et al., 2022, Wang et al., 2022, Poli et al., 2023a, và các nghiên cứu khác]. Các kiến trúc này kết hợp gating (tức là phép nhân từng phần tử) với các bộ lọc convolutional dài (tức là chiều dài của chuỗi) để cho phép tương tác giữa các token ở xa [Dauphin et al., 2017, Gu et al., 2021]. Nghiên cứu gần đây cho rằng các mô hình này, thể hiện khả năng mở rộng tiệm cận tốt hơn về chiều dài chuỗi đầu vào so với attention, có thể sánh ngang với attention trong chất lượng mô hình hóa ngôn ngữ [Poli et al., 2023a, Peng et al., 2023, Fu et al., 2023b].

Chúng tôi tiền huấn luyện và đánh giá 17 mô hình ngôn ngữ trên 4 quy mô (70M - 1.4Bn) và 5 kiến trúc trên cùng dữ liệu và thiết lập hạ tầng. Đáng ngạc nhiên, chúng tôi thấy rằng vẫn có khoảng cách perplexity lên đến 2.1 điểm giữa các kiến trúc dựa trên convolution tiên tiến và các baseline Transformer mạnh trong mô hình hóa ngôn ngữ trên Pile (Bảng 1). Thông qua phân tích chi tiết, chúng tôi thấy một khả năng đơn giản duy nhất chịu trách nhiệm

∗Đóng góp bằng nhau, thứ tự ngẫu nhiên bằng cách tung đồng xu.

--- TRANG 2 ---
cho phần lớn khoảng cách này: ghi nhớ thông tin đã thấy trong ngữ cảnh. Xem xét ví dụ dưới đây, nơi một số token có thể được dự đoán bằng cách ghi nhớ một liên kết trước đó:

Hakuna Matata!| {z }
Key-Value Có nghĩa là không lo lắng|{z}
Key-Value cho phần còn lại của cuộc đời bạn! Hakuna|{z}
Query Matata|{z}
AR Hit có nghĩa là không|{z}
Query→lo lắng|{z}
AR Hit

Chúng tôi thấy rằng lỗi trên "AR Hits" (ví dụ: lo lắng ở trên) chiếm 82% khoảng cách perplexity đến attention trung bình, mặc dù chỉ đại diện cho 6.4% tất cả token trong tập dữ liệu Pile. Một mô hình Transformer 70M tham số có thể dự đoán AR Hits tốt hơn mô hình Hyena gated convolution 1.4Bn tham số (lớn hơn 20 lần) (Bảng 1, Bảng 5). Khoảng cách AR vẫn tồn tại ở quy mô 7Bn tham số khi so sánh RWKV và Llama-2 (Phụ lục G).

Tác vụ này, ghi nhớ liên kết (AR), có lịch sử lâu dài trong machine learning [Graves et al., 2014, Ba et al., 2016, và các nghiên cứu khác] (Phụ lục A.1). Nghiên cứu trước đây cho rằng khả năng thực hiện AR của mô hình có thể dự đoán chất lượng học trong ngữ cảnh [Elhage et al., 2021, Olsson et al., 2022]. Do đó, AR đã được áp dụng như một công cụ trong thiết kế kiến trúc mới và nghiên cứu trước đây đã cho thấy rằng các kiến trúc gated convolution có thể sánh ngang với attention trên các tác vụ AR tổng hợp được sử dụng như proxy cho mô hình hóa ngôn ngữ thực [Fu et al., 2023a, Poli et al., 2023a, Lutati et al., 2023]. Vì lý do này, các khoảng cách perplexity AR downstream là đáng ngạc nhiên.

Thông qua đo lường recall trên dữ liệu thực, chúng tôi học được sự khác biệt chính là các công thức tổng hợp trước đây giả định có một truy vấn mỗi đầu vào, tại một vị trí cố định trong chuỗi, nơi các token đến từ kích thước từ vựng nhỏ (ví dụ |V|<50, nhỏ hơn chiều mô hình). Tuy nhiên, mô hình hóa ngôn ngữ thường yêu cầu thực hiện nhiều recall (ví dụ cho cả "Hakuna Matata" và "không lo lắng" ở trên, trong một lần forward pass), tại các vị trí khác nhau, với các token từ từ vựng lớn (lớn hơn chiều mô hình). Do đó chúng tôi đề xuất nghiên cứu AR đa truy vấn (Mqar). So với các công thức AR trước đây, Mqar nắm bắt tốt hơn các khoảng cách chất lượng tồn tại trên dữ liệu tổng hợp và thế giới thực (Phần 4). Tuy nhiên, chưa rõ tại sao Mqar làm sáng tỏ khoảng cách.

Chúng tôi hình thức hóa khoảng cách Mqar qua các lớp kiến trúc. Gated-convolution xử lý chuỗi biến thiên sử dụng các bộ lọc cố định được định nghĩa bởi trọng số mô hình thay vì là hàm của dữ liệu đầu vào (Xem Hình 1). Chúng tôi thấy việc gated-convolution thực hiện các tương tác token-to-token ở khoảng cách biến thiên (ví dụ ở khoảng cách 10 token cho Hakuna Matata và 9 cho không lo lắng) một cách hiệu quả về tham số và FLOPs so với attention là không hiệu quả. Attention đạt được tính phụ thuộc đầu vào vì nó tính toán tất cả tương tác token-to-token khi xác định cách trộn thông tin trong chuỗi. Chúng tôi mô tả chính thức hạn chế của gated convolution thông qua lý thuyết và thực nghiệm trong phần còn lại của bài báo.

Chúng tôi đầu tiên giới thiệu một toán tử đơn giản, BaseConv, có thể mô phỏng một cách chứng minh được lớp kiến trúc được xây dựng từ các nguyên hàm gating và convolution. Điều này bao gồm các kiến trúc như H3, Hyena, RWKV, và RetNet.

¹Chúng tôi đo AR trên dữ liệu thực sử dụng một heuristic đơn giản: các token n-gram được lặp lại trong ngữ cảnh.

--- TRANG 3 ---
[Bảng với các kết quả về Overall Slices % of gap due to AR Hits...]

Bảng 1: Validation perplexity mô hình hóa ngôn ngữ trên Pile. Sau khi tiền huấn luyện trên 10B token dữ liệu Pile, chúng tôi báo cáo log perplexity với negative log-likelihood trong ngoặc đơn. Chúng tôi báo cáo điểm số tổng thể, và cho các lát AR vs. non-AR token được định nghĩa trong Phần 3. FLOPs được tính cho đầu vào 2048 token dựa trên các phương trình trong Phụ lục C. Một bảng với kết quả bổ sung ở Bảng 4.

Chúng tôi cho thấy với lý thuyết và thực nghiệm rằng chiều mô hình cho BaseConv (và do đó các kiến trúc đã đề cập) để giải quyết Mqar tăng theo chiều dài chuỗi đầu vào (Định lý 4.4) trong khi attention có thể giải quyết Mqar với chiều mô hình độc lập với chiều dài chuỗi (Mệnh đề 4.3, Phần 4.3).² Trong thực tế, gated-convolution dường như mã hóa các giải pháp xấp xỉ cho AR chỉ hỗ trợ một tập con các khoảng cách tương tác token (ảnh hưởng đến khả năng xác định các key khớp với các truy vấn Mqar). Mặc dù việc phân tích lý thuyết các kiến trúc deep learning là thách thức [Hahn, 2020, Merrill et al., 2022, Keles et al., 2023], thực tế là gating và convolution là các phép toán đa thức tạo điều kiện cho phân tích chính xác của chúng tôi.

Chúng tôi cho thấy việc trộn chuỗi phụ thuộc đầu vào quan trọng để giải quyết Mqar hiệu quả. Việc mở rộng cho gated convolution với convolution độc lập đầu vào là không mong muốn nên chúng tôi tiếp theo hỏi những lựa chọn kiến trúc nào thu hẹp khoảng cách. Chúng tôi cho thấy việc trộn chuỗi phụ thuộc dữ liệu giúp kiến trúc giải quyết Mqar hiệu quả (Định lý 4.5). Mô hình cần điều chỉnh trọng số trộn chuỗi dựa trên khoảng cách tương tác token cần thiết cho mỗi ví dụ mới.

Một số sửa đổi kiến trúc có thể thỏa mãn tính chất phụ thuộc đầu vào. Dựa trên phân tích của chúng tôi, chúng tôi đánh giá các sửa đổi tối thiểu chỉ thêm các phép toán phụ thuộc đầu vào trên các token bigram lặp lại chính xác (heuristic của chúng tôi để đo các token yêu cầu recall). Lưu ý rằng các mô hình ngôn ngữ thường cần thực hiện recall giữa các chuỗi con mơ hồ hơn (ví dụ các bigram đồng nghĩa) hoặc các khái niệm nhiều chiều hơn. Tuy nhiên, chúng tôi cho thấy việc chỉ chèn toán tử phụ thuộc đầu vào — ví dụ, bộ lọc convolution dịch chuyển chuỗi dựa trên vị trí bigram hoặc attention thưa đặt chỉ trên vị trí bigram lặp lại — vào kiến trúc BaseConv tại <10% các lớp là đủ để vượt trội baseline Transformer trên mô hình hóa ngôn ngữ Pile (Phần 5) và thành công trên các tác vụ tổng hợp Mqar (Phần 4.3). Hơn nữa, điều này thu hẹp >80% khoảng cách perplexity Mqar trên dữ liệu validation Pile. Cuối cùng, chúng tôi tạo nguyên mẫu các giải pháp học các vị trí để sử dụng phép toán phụ thuộc đầu vào, xác nhận rằng chúng cũng thu hẹp khoảng cách đến attention trên Pile.

Trong công trình này, chúng tôi phân tích các kiến trúc dựa trên convolution ngày càng phổ biến và xác định các hạn chế cơ bản. Chúng tôi hy vọng framework Mqar và phân tích của chúng tôi về vai trò của tính phụ thuộc đầu vào cho Mqar sẽ thông báo cho việc thiết kế các kiến trúc tương lai. Chúng tôi phát hành mã nguồn để tái tạo: https://github.com/HazyResearch/zoology .

2 Nền tảng và Kiến thức cơ bản

Trong phần này, chúng tôi mô tả bối cảnh, giới thiệu ký hiệu, và thảo luận nghiên cứu liên quan quan trọng. Xem Phụ lục A để thảo luận rộng hơn về nghiên cứu liên quan.

Mô hình hóa ngôn ngữ. Chúng tôi nghiên cứu các mô hình ngôn ngữ tự hồi quy được huấn luyện trên tác vụ dự đoán token tiếp theo

²Lưu ý rằng runtime của Attention là bậc hai, nhưng gated convolution gần tuyến tính.

--- TRANG 4 ---
[Gao et al., 2020]. Cho một chuỗi N token x={x0, ..., xN−1} được rút từ từ vựng C, mô hình xuất ra phân phối xác suất trên C cho mỗi xi cho trước các token trước đó P(xi|x0, ..., xi−1).

Các mô hình ngôn ngữ trong công trình này chia sẻ cùng kiến trúc cấp cao. Đầu tiên, mỗi token xi trong đầu vào được nhúng trong không gian d chiều tạo ra ma trận u∈RN×d. Tiếp theo, u được truyền qua một chồng L lớp, với lớp ℓ xuất ra uℓ∈RN×d. Cuối cùng, các embedding uL xuất ra bởi lớp cuối được ánh xạ trở lại logits trên C với phép chiếu tuyến tính. Mỗi lớp biến đổi u với một sequence mixer (ví dụ attention) theo sau bởi state mixer (ví dụ MLP). Trừ khi được chỉ định, các mô hình của chúng tôi tuân theo các chi tiết triển khai của kiến trúc LLaMA (ngoại trừ sequence mixer, mà chúng tôi thay đổi xuyên suốt) [Touvron et al., 2023].

Sequence mixers. Công trình của chúng tôi đánh giá cách lựa chọn sequence mixer ảnh hưởng đến chất lượng và hành vi của các mô hình ngôn ngữ. Hầu hết sequence mixer tổng hợp các token embedding trong một chuỗi thông qua tổng có trọng số. Ví dụ, y[i,:] = ∑N−1j=0 ω(i,j)u[j,:]), trong đó ω là hàm xuất ra trọng số vô hướng. Chúng tôi nghiên cứu sự khác biệt giữa hai lớp sequence mixer, được thảo luận tiếp theo.

Attention. [Vaswani et al., 2017] Chúng tôi xem xét attention, sequence mixer mô hình ngôn ngữ de facto. Một lớp attention được tham số hóa bởi ba ma trận chiếu có thể học Q,K,V∈RN×d. Để tính đầu ra y cho đầu vào u, attention áp dụng các chiếu lên đầu vào: q=Qu, k=Ku, v=Vu. Các embedding được chiếu được tổng hợp theo: y=softmax(1/√d qk⊤)v (được hiển thị trong Hình 1) trong thời gian O(N²d), tốn kém cho chuỗi dài (N lớn).

Gated-convolutions. Một thay thế hiệu quả hơn cho attention là convolution, được định nghĩa là y[i,:] = ∑N−1j=0 k[j,:]⊙u[i−j,:] trong đó kernel k∈RN×d là ma trận trọng số có thể học. Convolution có thể được tính trong thời gian O(NdlogN) sử dụng Fast Fourier Transform (FFT) và định lý convolution: y=u*k=FFT−1(FFT(u)⊙FFT(k))[Cooley và Tukey, 1965]. Trong khi các kiến trúc convolutional thuần túy có thể sánh ngang hoặc vượt trội attention trong một số lĩnh vực (ví dụ thị giác [Tay et al., 2022, Gu et al., 2021], âm thanh [Goel et al., 2022], và chuỗi thời gian [Zhang et al., 2023]), chúng thua kém một khoảng cách lớn trên ngôn ngữ [Fu et al., 2023a].

Nghiên cứu gần đây đã thu hẹp phần lớn khoảng cách này bằng cách kết hợp convolution với gating (tức là phép nhân từng phần tử của đầu vào với phiên bản được biến đổi của chính nó). Gating lần đầu được đề xuất trong ngữ cảnh convolution bởi Dauphin et al. [2017], nhưng gần đây hơn nó đã đóng vai trò trung tâm trong các mô hình ngôn ngữ sub-quadratic tiên tiến, một số trong số đó tuyên bố chất lượng ngang với attention [Fu et al., 2023a, Wang et al., 2022, Poli et al., 2023a, Peng et al., 2023, Zhai et al., 2021, Fu et al., 2023b, và các nghiên cứu khác]. Mặc dù chúng có thể xuất hiện khác nhau bề ngoài, các kiến trúc sub-quadratic này đều có thể được biểu diễn dưới dạng convolution và gating. Xem Phụ lục H.2 để mô tả chi tiết các kiến trúc này và sự tương đồng của chúng. Công trình này phân tích sự khác biệt giữa attention và lớp rộng các gated convolution mixer.

3 Xác định vấn đề ghi nhớ liên kết

Trong phần này, chúng tôi đo lường khoảng cách perplexity giữa gated convolution và attention và cho thấy rằng một kỹ năng đơn lẻ được gọi là ghi nhớ liên kết chiếm 82% khoảng cách trung bình. Điều này đáng ngạc nhiên vì nghiên cứu trước đây cho thấy gated convolution giải quyết hoàn hảo phiên bản tổng hợp của ghi nhớ liên kết. Dựa trên phân tích của chúng tôi, chúng tôi định nghĩa một công thức tổng hợp mới phản ánh tốt hơn dữ liệu thực. Tác vụ này tạo điều kiện cho phân tích của chúng tôi về tại sao khoảng cách xảy ra (Phần 4) và cách khắc phục nó (Phần 5).

3.1 Phân tích chi tiết chất lượng downstream

Khoảng cách Perplexity Chúng tôi tiền huấn luyện một bộ mô hình ngôn ngữ lớn với các sequence mixer khác nhau trên 3 quy mô (70M-360M) cho 10B token trên setting mô hình hóa ngôn ngữ Pile tiêu chuẩn sử dụng hạ tầng huấn luyện EleutherAI GPT-NeoX [Gao et al., 2020, Andonian et al., 2023]. Trong bài báo chính, chúng tôi so sánh attention với ba sequence mixer gated convolution tiên tiến: H3, Hyena, và RWKV [Fu et al., 2023a, Poli et al., 2023a, Peng et al., 2023].³ Chúng tôi cũng bao gồm mô hình long-convolution thuần túy để nhấn mạnh tầm quan trọng của gating. Trong Phụ lục F, chúng tôi bao gồm kết quả trên các sequence mixer bổ sung [Hasani et al., 2022, Sun et al., 2023]. Chúng tôi sử dụng baseline Transformer attention mạnh với rotary embedding và SwiGLU MLP theo kiến trúc Llama [Touvron et al., 2023]. Chúng tôi cũng sử dụng công thức huấn luyện mạnh này khi huấn luyện các sequence mixer không có attention. Để biết chi tiết thực nghiệm và siêu tham số cho tất cả kiến trúc xem Phụ lục C.

Qua các quy mô, chúng tôi thấy rằng attention vượt trội hơn gated convolution ít nhất một phần ba điểm perplexity trung bình: khoảng cách tối thiểu là +2.14, +0.59, +0.35 PPL tại quy mô 70M, 160M, và 360M tham số tương ứng. Chúng tôi báo cáo perplexity test tổng thể trong Bảng 1. Mặc dù các khoảng cách này tương đối nhỏ trung bình, các mô hình vẫn có thể hoạt động rất khác nhau trên các tập con dữ liệu khác nhau [Eyuboglu et al., 2022].

Perplexity Ghi nhớ Liên kết Để hiểu rõ hơn sự khác biệt giữa attention và gated convolution, chúng tôi thực hiện phân tích chi tiết các dự đoán token tiếp theo và quan sát rằng các mô hình dựa trên convolution gặp khó khăn trong việc ghi nhớ các liên kết đã thấy trước đó trong ngữ cảnh. Ví dụ, trong Hình 1, mô hình phải ghi nhớ liên kết giữa "Tim" và họ "Rice". Theo một dòng nghiên cứu trước đây dài, chúng tôi gọi kỹ năng này là ghi nhớ liên kết (AR) [Willshaw et al., 1969, Hopfield, 1982] (xem Phụ lục A để thảo luận mở rộng về lịch sử AR trong machine learning). Trong Phụ lục D.1.1, chúng tôi cung cấp các ví dụ Pile được chú thích để minh họa hiện tượng một cách định tính.

Định lượng hiệu suất AR. Thật thách thức để có được thước đo định lượng hiệu suất ghi nhớ liên kết trên Pile vì chúng ta không biết dự đoán token tiếp theo nào trong văn bản thô yêu cầu ghi nhớ liên kết. Chúng tôi sử dụng một heuristic đơn giản để xác định các token này, mà chúng tôi gọi là AR Hits. Một AR Hit là token cuối của n-gram lặp lại trong ngữ cảnh (ví dụ lần xuất hiện thứ hai của "Rice" trong Hình 1). Tuy nhiên, một số n-gram phổ biến (ví dụ "of the") có thể đã được ghi nhớ trong quá trình huấn luyện, vì vậy chúng tôi tính đến tần suất mà n-gram xuất hiện trong dữ liệu huấn luyện. Heuristic này cho phép chúng tôi mở rộng phân tích của mình lên hơn 10 triệu token dữ liệu validation Pile.

Chúng tôi phân tầng token Pile thành hai lát dựa trên heuristic này và báo cáo perplexity trên mỗi lát trong Bảng 1:

1. AR Hits: (6.4% token) Token ở vị trí cuối của bigram (một cặp token liên tiếp) đã xuất hiện trước đó trong ngữ cảnh, nhưng ≤1250× trong quá trình huấn luyện.
2. Token khác: (93.6% token) Token ở vị trí cuối của bigram không xuất hiện trước đó trong ngữ cảnh hoặc xuất hiện >1,250 lần trong quá trình huấn luyện.

Trong Hình 1, chúng tôi visualize các lát này bằng cách vẽ log-perplexity so với tần suất bigram xuất hiện trong quá trình huấn luyện. Đáng chú ý, khoảng cách giữa attention và gated convolution lớn nhất trên AR hit với ít lần xuất hiện nhất. Trên các token khác, không có khoảng cách.

Trong Bảng 1, chúng tôi cũng tính toán phần trăm khác biệt perplexity giữa attention và mỗi mô hình do token AR: Δlog(φAR)·|TAR|/Δlog(φ)·|T|, trong đó φ là perplexity và T là tập token trong test set. Đại lượng này cũng có thể được hiểu là phần khoảng cách tổng thể sẽ được thu hẹp nếu mô hình có thể sánh ngang attention trên lát AR. Chúng tôi thấy rằng lát AR chiếm 82% khoảng cách chất lượng trung bình giữa gated convolution và attention.

Để đánh giá khả năng AR của các mô hình lớn hơn, chúng tôi huấn luyện hai mô hình attention và Hyena 1.4 tỷ tham số cho 50 tỷ token trên Pile và lặp lại phân tích này (xem Bảng 5). Đáng chú ý, mô hình attention 70 triệu tham số tốt hơn một điểm perplexity đầy đủ trong lát AR so với mô hình Hyena 1.4B này lớn hơn 20× (2.41 vs. 3.43 ppl.). Trong Phụ lục G, chúng tôi cũng đánh giá các mô hình RWKV và attention mã nguồn mở được huấn luyện lên đến 7 tỷ tham số. Chúng tôi sử dụng tập dữ liệu bán tổng hợp có kiểm soát để đo khả năng AR và cho thấy hiệu suất của RWKV giảm mạnh khi chúng tôi tăng số lượng truy vấn trong một ví dụ trong khi attention hoạt động nhất quán tốt Phụ lục G.

3.2 Hình thức hóa vấn đề: Ghi nhớ Liên kết Đa Truy vấn

Khoảng cách trong perplexity ghi nhớ liên kết này rất đáng ngạc nhiên vì nghiên cứu trước đây cho thấy gated-convolution có thể giải quyết hoàn hảo phiên bản được hình thức hóa của tác vụ [Fu et al., 2023a, Poli et al., 2023a, Olsson et al., 2022]. Trong tác vụ tổng hợp này, đầu vào x chứa một chuỗi bigram đại diện cho các cặp key-value từ từ điển ngẫu nhiên theo sau bởi một token truy vấn duy nhất. Ví dụ, đầu ra đúng cho đầu vào dưới đây sẽ là 3:

A 4 B 3|{z}
Key-Value C 6 E 2 F 1 C 6 G 8 →B ?|{z}
Query

³RWKV thường được gọi là RNN. Chúng tôi cho thấy nó có thể được xem như convolution (Phụ lục H.2.2).

--- TRANG 5 ---
Gated convolution (ví dụ H3, Hyena, RWKV) có thể giải quyết tác vụ này hoàn hảo cho hầu hết chiều dài chuỗi. Những kết luận này không nhất quán với các phát hiện của chúng tôi trên Pile, như đã mô tả ở trên, vì vậy chúng tôi hỏi làm thế nào công thức AR này khác với cách AR thể hiện trong ngôn ngữ thực. Chúng tôi xác định một sự khác biệt lớn. Trong đầu vào thế giới thực, mô hình ngôn ngữ thường cần thực hiện nhiều ghi nhớ liên kết trong một lần forward pass duy nhất, tại các vị trí khác nhau trong chuỗi (ví dụ "Tim Rice" và "March 2018" trong Hình 1. Chúng tôi gọi điều này là Multi-Query AR (Mqar). Chúng tôi định nghĩa chính thức vấn đề Mqar như sau:⁴

Định nghĩa 3.1 (Multi-Query-AR (Mqar)). Chúng ta được cho một chuỗi đầu vào x={x₀, . . . , xₙ₋₁} trong đó mỗi xᵢ∈C là token được rút từ từ vựng có kích thước c=|C|. Tác vụ là kiểm tra, cho mỗi truy vấn 1 ≤ i < N, liệu có tồn tại 0 ≤ j < i sao cho uᵢ≡uⱼ. Nếu có, xuất ra uⱼ₊₁.

Ví dụ, đầu ra đúng cho đầu vào dưới đây sẽ là 4, 6, 1, 2, 3:
A 4 B 3 C 6 F 1|{z}
Key-Value E 2→A ? C ? F ?|{z}
Query E ? B ?

Trong Phần 4, chúng tôi sử dụng Mqar để giải thích khoảng cách chất lượng giữa gated convolution và attention.

4 Giải thích vấn đề ghi nhớ liên kết

Trong phần này, chúng tôi đưa ra lời giải thích cho khoảng cách trong hiệu suất ghi nhớ liên kết bằng cách phân tích tác vụ Mqar chính thức về mặt lý thuyết và thực nghiệm. Trong Phần 4.1, chúng tôi định nghĩa một kiến trúc gated-convolution đơn giản, được gọi là BaseConv, mà chúng tôi cho thấy có thể mô phỏng một lớp rộng các kiến trúc được xây dựng từ gating và convolution. Điều này cho phép chúng tôi đưa ra các tuyên bố tổng quát áp dụng cho các kiến trúc gated-convolution phổ biến như Hyena, RWKV, hoặc H3. Trong Phần 4.2, chúng tôi cho thấy rằng tồn tại các giải pháp lý thuyết cho Mqar có thể về nguyên tắc được học bởi BaseConv, và chúng tôi phân tích độ phức tạp của chúng về chiều rộng và độ sâu mô hình. Trong Phần 4.3, chúng tôi sử dụng thực nghiệm trên dữ liệu tổng hợp để cho thấy việc giải quyết Mqar với BaseConv (và các kiến trúc gated-convolution khác) yêu cầu chiều mô hình tỉ lệ tuyến tính với chiều dài chuỗi. Ngược lại, attention giải quyết Mqar nhất quán trong các thực nghiệm của chúng tôi với chiều mô hình tỉ lệ độc lập với chiều dài chuỗi. Các quy luật tỉ lệ thực nghiệm này cung cấp lời giải thích tiềm năng cho khoảng cách AR và, cùng với phân tích lý thuyết của chúng tôi, chỉ ra các giải pháp tiềm năng được thảo luận trong Phần 5.

4.1 BaseConv: một toán tử gated convolution tối thiểu

Trong phần này, chúng tôi định nghĩa kiến trúc gated-convolution tối thiểu của chúng tôi, được gọi là BaseConv. Cho một hàm, chúng tôi muốn biết mô hình hiệu quả nhất (ví dụ tham số, FLOPs) có thể biểu diễn giải pháp. Trong công trình này, chúng tôi cho thấy có thể lý luận chính xác về câu hỏi này để biểu diễn các hàm đa thức với gated convolution vì gating và convolution đều là các phép toán đa thức. Mô hình tiêu chuẩn định nghĩa độ phức tạp tính toán cho đa thức là theo kích thước của mạch số học nhỏ nhất có thể tính toán đa thức. Chúng tôi định nghĩa toán tử BaseConv gated convolution thú vị vì (1) nó universal trong đó nó có thể mô phỏng bất kỳ mạch số học C nào (chỉ với poly-log blowup trong các tham số tương ứng) và (2) nó đơn giản để triển khai hiệu quả (19 dòng PyTorch thuần bao gồm import, xem Phụ lục B).

Định nghĩa 4.1 (Toán tử BaseConv). Cho đầu vào u∈ RN×d, toán tử BaseConv cho lớp ℓ được định nghĩa là:

y:= u·Wℓ+bℓ₁ |{z}
Linear Projection ⊙ hℓ∗u+bℓ₂ |{z}
Convolution (1)

trong đó lớp được tham số hóa bởi các bộ lọc có thể học h∈ RN×d, phép chiếu tuyến tính Wℓ∈ Rd×d, và các ma trận 'bias' b₁,b₂∈ RN×d. ⊙ là tích theo từng thành phần và convolution của hai ma trận được tính như convolution của các cột tương ứng.

⁴Trong Phụ lục H.7.1 chúng tôi định nghĩa dạng chính thức, tổng quát của định nghĩa này, được sử dụng trong phân tích lý thuyết.

--- TRANG 6 ---
Trong các thực nghiệm của chúng tôi, mỗi lớp BaseConv sử dụng ˜O(Nd+d²) tham số⁵ và có thể được tính trong ˜O(Nd²) phép toán. Đối với các kết quả lý thuyết của chúng tôi, chúng tôi có thể giả định ma trận trọng số Wℓ bị hạn chế trong một lớp ma trận hỗ trợ phép nhân ma trận gần tuyến tính (ví dụ ma trận Kaleidoscope, xem Định nghĩa H.3). Dưới giả định này, BaseConv sử dụng ˜O(Nd) tham số và ˜O(Nd) FLOPs (Mệnh đề H.6). Chúng tôi bây giờ phát biểu kết quả tương đương giữa mạch số học và BaseConv một biểu diễn "canonical" của mạch số học (Định lý H.21 trong Phụ lục H.5):

Định lý 4.2 (Tương đương với Mạch Số học). Cho mạch số học C có kích thước s và độ sâu Δ nhận u∈ RN×d làm đầu vào, tồn tại toán tử BaseConv tương đương sử dụng ˜O(sΔ) tham số và ˜O(Δ) lớp.⁶

Nói cách khác, bất kỳ mô hình gated convolution nào với số lượng nhỏ các lớp có thể được mô phỏng bởi BaseConv chỉ với (poly)logarithmic blowup trong tham số và lớp. Chúng tôi lưu ý rằng mạch số học là mô hình tính toán được nghiên cứu rất kỹ trong độ phức tạp tính toán Bürgisser et al. [1996]. Nhiều thuật toán hiệu quả nổi tiếng trên ma trận (ví dụ FFT hoặc thuật toán nhân ma trận-ma trận tốt nhất hiện tại) thực tế cho ra các mạch số học nhỏ. Tuy nhiên, mạch số học về bản chất là các đối tượng rời rạc – chúng ta không thể học chúng qua gradient descent. Định lý 4.2 cho thấy rằng (lên đến poly-log loss trong tham số), thay vào đó chúng ta có thể học trên các mô hình BaseConv. Kết quả này tổng quát hóa kết quả tương tự từ Dao et al. [2020] cho lớp đặc biệt các hàm tuyến tính: chúng tôi tổng quát hóa kết quả trước đó cho lớp tất cả đa thức.

Đối với các lớp gated convolution cụ thể, chúng tôi có thể loại bỏ yếu tố blowup poly-logarithmic–chúng tôi quan sát trong phụ lục rằng các mô hình BaseConv và Hyena có thể mô phỏng lẫn nhau chỉ với blowup hằng số nhỏ trong tham số (Mệnh đề H.12 trong Phụ lục H.5).

4.2 Phân tích lý thuyết về khả năng gated convolution và ghi nhớ liên kết

Trong phần này, chúng tôi cung cấp các giải pháp Mqar lý thuyết có thể về nguyên tắc được học bởi mỗi kiến trúc và phân tích độ phức tạp của chúng về chiều rộng và độ sâu mô hình. Đầu tiên, chúng tôi lưu ý rằng attention giải quyết Mqar với tham số độc lập với chiều dài chuỗi (Mệnh đề H.27).

Mệnh đề 4.3 (Attention). Cho đầu vào u∈ {0,1}N×3c, Attention (ngay cả khi không sử dụng soft-max) giải quyết Mqar cho u sử dụng O(c²) tham số, O(Nc²+N²c) độ phức tạp thời gian và O(1) lớp.

Tự nhiên ta thắc mắc liệu tất cả so sánh theo cặp giữa các token có cần thiết để giải quyết Mqar. Thực vậy, trong setting RAM, thuật toán tuần tự có thể đơn giản sử dụng N phép chèn và truy vấn thành viên logarithmic để giải quyết Mqar trong thời gian dưới bậc hai. Thật không may, bất kỳ mô hình nào cố gắng mô phỏng điều này sẽ yêu cầu Ω(N) lớp. Thay vào đó, chúng tôi quan sát rằng chúng ta có thể song song hóa thuật toán này sử dụng khoảng dyadic và đạt được độ sâu ˜O(1) (Mệnh đề H.30). Sau đó chúng tôi chuyển đổi thuật toán này thành mạch số học và áp dụng Định lý 4.2 để có được mô hình BaseConv tương đương. Điều này cho phép chúng tôi chứng minh các giới hạn trên mới cho các mô hình BaseConv áp dụng cho Mqar, cải thiện độ phức tạp thời gian bậc hai của attention thành runtime gần tuyến tính với chi phí sử dụng poly-log lớp (Định lý H.37 trong Phụ lục H.7).

Định lý 4.4 (Bộ lọc Độc lập Dữ liệu⁷). Cho đầu vào u∈ {0,1}N×O(logc) cho Mqar (trong đó chúng tôi giả định rằng các token riêng biệt được nhúng thành các vector riêng biệt trong {0,1}O(logc)), tồn tại toán tử BaseConv giải quyết Mqar cho u sử dụng ˜O(Nlogc) tham số cũng như độ phức tạp thời gian và ˜O(1) lớp.

Tuy nhiên, số lượng poly-logarithmic lớp trong kết quả trên là không mong muốn trong thực tế. Nhưng, chúng tôi cho thấy rằng sử dụng bộ lọc convolution phụ thuộc đầu vào, có thể có số lượng lớp hằng số (cho một lớp con của đầu vào). Hướng tới điều đó, chúng tôi định nghĩa khoảng cách tương tác giữa truy vấn qi và key khớp kj là i−j. Điều này sau đó cho phép chúng tôi trình bày giới hạn trên tương ứng cho việc trộn phụ thuộc dữ liệu (Định lý H.38 trong Phụ lục H.8).

Định lý 4.5 (Bộ lọc Phụ thuộc Đầu vào). Cho đầu vào u∈ {0,1}N×c cho Mqar (trong đó chúng tôi giả định rằng

⁵Chúng tôi sử dụng ˜O(·) để ẩn các yếu tố poly-log.
⁶Phát biểu chính thức trong Phụ lục có phiên bản sắc nét hơn của kết quả này về 'chiều rộng' mạch.
⁷Chúng tôi lưu ý ở đây rằng các kiến trúc hiện có cũng sử dụng bộ lọc convolution độc lập dữ liệu, có nghĩa là bộ lọc được định nghĩa như một hàm của tham số mô hình, độc lập với đầu vào.

--- TRANG 7 ---
các token được nhúng như one-hot encoding trong {0,1}c và tồn tại nhiều nhất t khoảng cách tương tác riêng biệt),⁸ tồn tại toán tử BaseConv sử dụng kernel phụ thuộc đầu vào để giải quyết trường hợp Mqar trên sử dụng O(t·Nc) tham số và O(1) lớp.

4.3 Phân tích thực nghiệm về khả năng gated convolution và ghi nhớ liên kết

Trong phần này, chúng tôi đo thực nghiệm chiều mô hình phải tỉ lệ như thế nào để các sequence mixer khác nhau giải quyết Mqar.

Thiết lập Chúng tôi huấn luyện và đánh giá các mô hình trên Mqar tổng hợp với kích thước từ vựng 8,192, thay đổi chiều mô hình và chiều dài chuỗi từ 64 đến 512. Phụ lục E cung cấp chi tiết thêm về công thức và xây dựng tác vụ tổng hợp này. Theo Olsson et al. [2022], chúng tôi huấn luyện các mô hình hai lớp với backbone Transformer xen kẽ sequence mixing và state mixing (MLP). Đối với mỗi kiến trúc, chúng tôi quét bốn learning rate từ log(−4) đến log(−2) cho mỗi kiến trúc, và báo cáo độ chính xác test tối đa.

Kết quả của chúng tôi, được tóm tắt trong Hình 2, hỗ trợ hai tuyên bố chính:

Tuyên bố 1 (Gated-convolution và attention). Các mô hình gated-convolution với hai lớp yêu cầu chiều mô hình tỉ lệ ít nhất tuyến tính với chiều dài chuỗi để giải quyết ghi nhớ liên kết, trong khi các mô hình attention có thể giải quyết nó với độ chiều gần như hằng số. Chúng tôi so sánh attention và BaseConv cũng như ba kiến trúc gated-convolution phổ biến: RWKV, H3, và Hyena [Peng et al., 2023, Fu et al., 2023c, Poli et al., 2023a]. Trong hàng trên của Hình 2, attention giải quyết Mqar hoàn hảo ở tất cả chiều dài chuỗi sử dụng chiều mô hình hằng số 64. Ngược lại, Mqar không đạt độ chính xác >0.9 trừ khi d≥N.

Tuyên bố 2 (Bộ lọc phụ thuộc đầu vào). Sử dụng bộ lọc phụ thuộc đầu vào trong các mô hình gated-convolution có thể thu hẹp một phần khoảng cách đến attention. Trong Định lý 4.5, chúng tôi cho thấy rằng BaseConv với bộ lọc phụ thuộc đầu vào có thể giải quyết Mqar với tỉ lệ cải thiện. Trong giải pháp này, chúng tôi xây dựng bộ lọc tăng đột biến tại vị trí j nếu các key khớp được phân cách bởi j token. Chúng tôi đánh giá hai cách tiếp cận để xây dựng bộ lọc này: (1) theo chương trình (tức là so sánh hard-coded giữa token id) hoặc (2) với autocorrelation, có thể học thực hiện fuzzy-match (xem Phụ lục H.8). Trong hàng dưới của Hình 2, chúng tôi thấy rằng BaseConv với bộ lọc phụ thuộc đầu vào theo chương trình đạt được tỉ lệ gần như hằng số trong chiều mô hình và BaseConv với bộ lọc phụ thuộc đầu vào autocorrelation đạt được tỉ lệ cải thiện so với BaseConv với bộ lọc độc lập đầu vào.

⁸Lưu ý rằng các khoảng cách tương tác có thể tùy ý: chỉ có số lượng khoảng cách riêng biệt bị ràng buộc.

--- TRANG 8 ---
Các bộ lọc phụ thuộc đầu vào này không thể dễ dàng được làm thỏa mãn tính nhân quả và sử dụng bộ lọc O(NlogN) cho mỗi khoảng cách có thể tốn kém nếu mỗi khoảng cách chỉ áp dụng cho một số lượng nhỏ bigram. Một cách đơn giản và có lẽ hiệu quả hơn để giải quyết vấn đề sẽ là giới thiệu một lượng nhỏ attention vào mô hình BaseConv [Fu et al., 2023a]. Như baseline tự nhiên đầu tiên, chúng tôi đánh giá hybrid attention trên Mqar tổng hợp và cho thấy nó đạt được tỉ lệ cải thiện trong Hình 2. Tiếp theo, chúng tôi đưa những hiểu biết thực nghiệm và lý thuyết này vào thực tế trên mô hình hóa ngôn ngữ Pile.

5 Thu hẹp Khoảng cách Ghi nhớ Liên kết

Trong phần này, chúng tôi đánh giá các mô hình hybrid BaseConv-Attention tận dụng các mẫu thưa thớt khác nhau. Chúng tôi cho thấy rằng các hybrid với mẫu thưa thớt phụ thuộc đầu vào có thể thu hẹp hầu hết khoảng cách đến attention, trong khi duy trì tỉ lệ dưới bậc hai. Chúng tôi cũng cho thấy rằng các hybrid BaseConv có thể vượt trội hơn các mô hình chỉ attention lên đến một điểm perplexity đầy đủ, tất cả trong khi đơn giản hơn đáng kể để triển khai và phân tích so với các hybrid trước đây [Fu et al., 2023a] (xem triển khai trong Phụ lục B. Chúng tôi mô tả các kiến trúc trong Phần 5 và kết quả trên mô hình hóa ngôn ngữ Pile trong Phần 5.

Hybrid BaseConv-Attention Thưa thớt Chúng tôi đánh giá các hybrid chủ yếu bao gồm các lớp BaseConv và ba lớp attention (6.3% lớp ở 354M tham số và 10% ở 168M tham số). Chúng tôi bổ sung các lớp attention với các toán tử chọn lọc áp dụng attention cho một số token dựa trên hàm lựa chọn f:RN×d→ {0,1}N. Chúng nhận đầu vào u∈RN×d và xuất ra y∈RN×d:

y[i,:] = softmax(1/√d q[i,:]k⊤)v·f(u)[i] (2)

trong đó q,k,v là các phép chiếu query, key, và value, như trong attention. Chúng tôi đánh giá bốn lựa chọn cho f:

(1) Full attention. Đầu tiên, chúng tôi đánh giá hiệu suất của các hybrid attention đầy đủ. Các hybrid này tương ứng với việc cố định f(u)[i] = 1 cho tất cả i trong Eq. (2). Nghiên cứu trước đây đã cho thấy các hybrid attention đầy đủ hiệu quả, nhưng chúng không giải thích tại sao việc bổ sung attention cho gated convolution lại cần thiết [Fu et al., 2023a]. Dựa trên các phát hiện của chúng tôi trong Phần 3, chúng tôi giả thuyết rằng attention thưa thớt chỉ áp dụng cho các hit ghi nhớ liên kết có thể đủ.

(2) Lựa chọn ngẫu nhiên. Như một kiểm soát, chúng tôi đánh giá attention thưa thớt được áp dụng ngẫu nhiên cho các token. Điều này tương ứng với hàm lựa chọn ngẫu nhiên trong đó f(u)[i] được rút từ Bernoulli. Nhiều nghiên cứu trước đây đã sử dụng các mẫu attention thưa thớt như thế này, độc lập với đầu vào [Zaheer et al., 2020, Child et al., 2019a, Beltagy et al., 2020, và các nghiên cứu khác].

(3) Lựa chọn theo chương trình. Tiếp theo, để đánh giá giả thuyết của chúng tôi rằng attention cần thiết cho ghi nhớ liên kết, chúng tôi tạo nguyên mẫu hàm lựa chọn theo chương trình chỉ chọn những token có thể là hit ghi nhớ liên kết. Cụ thể, f(x[i,:]) là 1 nếu token xi đã xuất hiện trước đó trong chuỗi. Trong thực tế, chúng tôi so sánh token id thô, không phải token embedding.

f(x)[i] = {
1 nếu tồn tại j < i sao cho xi=xj
0 ngược lại
(3)

(4) Lựa chọn đã học. Cuối cùng, chúng tôi tạo nguyên mẫu hàm lựa chọn đã học f(u)[i] = σ(u[i,:]·W) được tham số hóa như một lớp tuyến tính đơn giản với sigmoid activation. Chúng tôi cố định siêu tham số k và chọn top-k token với điểm số cao nhất trong mỗi batch. Điều này cho phép chúng tôi tính attention trong thời gian O(ndk). Trong quá trình huấn luyện, chúng tôi thêm một lượng nhỏ Gaussian noise vào phép tính top-k để khuyến khích khám phá và sử dụng auxiliary loss để khuyến khích lựa chọn thưa thớt: ℓf(u) = 1/N max(0, ∑Ni=1 f(u)[i]−k). Cách tiếp cận này gần giống nhất với kiến trúc SeqBoat được đề xuất gần đây [Ren et al., 2023]. Chúng tôi thảo luận sự khác biệt trong Phụ lục A.

Đánh giá Downstream Chúng tôi đánh giá các nguyên mẫu trên mô hình hóa ngôn ngữ Pile. Chúng tôi lấy các kiến trúc BaseConv ở quy mô 150M và 360M tham số và thêm lựa chọn phụ thuộc đầu vào vào ba lớp (chi tiết trong Phụ lục C.2). Kết quả trong Bảng 2. Chúng tôi xác nhận rằng các nguyên mẫu thu hẹp khoảng cách chất lượng tổng thể và AR đến attention khi được thêm vào backbone BaseConv.

--- TRANG 9 ---
[Bảng 2 với kết quả Overall Slices cho các mô hình khác nhau]

Bảng 2: Perplexity mô hình ngôn ngữ trên các lát của PILE. Chúng tôi đánh giá Hyena và BaseConv với Hybridization và Selective look-up ở 160 và 355M tham số. Chúng tôi xác nhận rằng các phương pháp cho phép gated convolution vượt trội hơn attention.

Ở 360M tham số, BaseConv với chỉ 3 lớp attention có thể vượt trội hơn Transformer, trong khi yêu cầu ít FLOPs hơn. Các mô hình hybrid attention-BaseConv của chúng tôi vượt trội hơn các mô hình chỉ attention 0.85 điểm perplexity trong khi cho phép giảm 18% tổng FLOPs so với attention. Tuy nhiên, điều này sử dụng attention bậc hai đầy đủ. Chúng tôi tiếp theo cho thấy rằng attention thưa thớt được bản địa hóa cho các token AR tiềm năng, cũng đủ để thu hẹp khoảng cách, xác nhận hiểu biết của chúng tôi về vai trò của tính phụ thuộc đầu vào cho MQAR. Ở 360M tham số, lựa chọn theo chương trình thu hẹp 85% khoảng cách giữa BaseConv thuần túy và attention trên lát AR, trái ngược với kiểm soát lựa chọn ngẫu nhiên. Lựa chọn đã học thu hẹp 72% khoảng cách sử dụng chỉ k=256 vị trí attention dưới bậc hai mỗi ví dụ.

6 Thảo luận và Kết luận

Chúng tôi trình bày phân tích mở rộng về các kiến trúc gated convolution trong bối cảnh sự phổ biến gần đây của chúng. Chúng tôi xác định khoảng cách chất lượng tồn tại giữa các kiến trúc dựa trên convolution hiệu quả và attention không hiệu quả, chủ yếu do một chế độ thất bại duy nhất là ghi nhớ liên kết. Chúng tôi thiết kế công cụ phân tích ghi nhớ liên kết đa truy vấn (Mqar) mới, tương quan với chất lượng AR downstream. Chúng tôi giải thích lý thuyết và thực nghiệm khoảng cách do việc trộn phụ thuộc dữ liệu không đủ trong gated convolution và chúng tôi cho thấy các kiến trúc tối thiểu thu hẹp khoảng cách trên Pile.

Kết quả của chúng tôi trong việc phân tích gated convolution vượt ra ngoài quan niệm thông thường rằng attention là mô hình "đúng". Một lượng đáng kể công việc tập trung vào cải thiện hiệu quả của attention [Dao et al., 2022, Dao, 2023, Katharopoulos et al., 2020a] và nghiên cứu lý thuyết về sức mạnh chính xác của attention [Hahn, 2020, Merrill et al., 2022, Keles et al., 2023]. Attention thường được sử dụng như mốc tiêu chuẩn cho những gì cần thiết downstream. Chúng tôi hy vọng những đóng góp của mình làm nổi bật giá trị của Mqar, và rộng hơn là các tác vụ gắn liền với mô hình hóa ngôn ngữ thực, như một proxy để nghiên cứu.

Lời cảm ơn
Chúng tôi cảm ơn Tri Dao, Daniel Fu, Neel Guha, Stefano Massaroli, Eric Nguyen, và Michael Zhang vì phản hồi và thảo luận hữu ích trong công việc này. Chúng tôi biết ơn Together Computer đã làm cho công việc này trở thành khả thi. Chúng tôi biết ơn sự hỗ trợ của DARPA dưới Nos. FA86501827865 (SDH) và FA86501827882 (ASED); NIH dưới No. U54EB020405 (Mobilize), NSF dưới Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), và 1937301 (RTML); ONR dưới No. N000141712266 (Unifying Weak Supervision); Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Okawa Foundation, American Family Insurance, Google Cloud, Microsoft Azure, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) thông qua National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, và các thành viên của dự án Stanford DAWN: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, và Infosys. Chính phủ Hoa Kỳ được ủy quyền tái sản xuất và phân phối bản in cho các mục đích Chính phủ bất chấp bất kỳ chú thích bản quyền nào trên đó. Bất kỳ ý kiến, phát hiện, và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của các tác giả và không nhất thiết phản ánh quan điểm, chính sách, hoặc sự chứng thực, được thể hiện hoặc ngụ ý, của DARPA, NIH, ONR, hoặc Chính phủ Hoa Kỳ. Công việc của AR được hỗ trợ bởi NSF grant# CCF-2247014. Công việc của IJ được hỗ trợ bởi NSF Graduate Fellowship.

[Tiếp tục với phần References và các phụ lục...]
