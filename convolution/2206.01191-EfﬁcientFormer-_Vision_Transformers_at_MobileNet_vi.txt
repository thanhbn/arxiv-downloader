# EfﬁcientFormer: Vision Transformers tại Tốc độ MobileNet

Yanyu Li1;2;†Geng Yuan1;2;†Yang Wen1Ju Hu1Georgios Evangelidis1
Sergey Tulyakov1Yanzhi Wang2Jian Ren1
1Snap Inc.2Northeastern University

## Tóm tắt

Vision Transformers (ViT) đã cho thấy tiến bộ nhanh chóng trong các tác vụ thị giác máy tính, đạt được kết quả đầy hứa hẹn trên nhiều benchmark khác nhau. Tuy nhiên, do số lượng tham số khổng lồ và thiết kế mô hình, ví dụ như cơ chế attention, các mô hình dựa trên ViT thường chậm hơn nhiều lần so với các mạng convolutional nhẹ. Do đó, việc triển khai ViT cho các ứng dụng thời gian thực đặc biệt khó khăn, đặc biệt là trên phần cứng hạn chế tài nguyên như thiết bị di động. Các nỗ lực gần đây cố gắng giảm độ phức tạp tính toán của ViT thông qua tìm kiếm kiến trúc mạng hoặc thiết kế hybrid với khối MobileNet, nhưng tốc độ suy luận vẫn chưa đạt yêu cầu. Điều này dẫn đến một câu hỏi quan trọng: liệu transformers có thể chạy nhanh như MobileNet trong khi vẫn đạt hiệu suất cao? Để trả lời câu hỏi này, chúng tôi trước tiên xem xét lại kiến trúc mạng và các toán tử được sử dụng trong các mô hình dựa trên ViT và xác định các thiết kế không hiệu quả. Sau đó, chúng tôi giới thiệu một transformer thuần túy với chiều nhất quán (không có khối MobileNet) làm mô hình thiết kế. Cuối cùng, chúng tôi thực hiện việc thu gọn hướng độ trễ để có được một loạt mô hình cuối cùng được gọi là EfﬁcientFormer. Các thí nghiệm mở rộng cho thấy sự vượt trội của EfﬁcientFormer về hiệu suất và tốc độ trên thiết bị di động. Mô hình nhanh nhất của chúng tôi, EfﬁcientFormer-L1, đạt 79,2% độ chính xác top-1 trên ImageNet-1K với chỉ 1,6ms độ trễ suy luận trên iPhone 12 (được biên dịch với CoreML), chạy nhanh như MobileNetV2 ×1,4 (1,6ms, 74,7% top-1), và mô hình lớn nhất của chúng tôi, EfﬁcientFormer-L7, đạt 83,3% độ chính xác với chỉ 7,0ms độ trễ. Công trình của chúng tôi chứng minh rằng các transformers được thiết kế phù hợp có thể đạt độ trễ cực thấp trên thiết bị di động trong khi vẫn duy trì hiệu suất cao.

## 1 Giới thiệu

Kiến trúc transformer [1], ban đầu được thiết kế cho các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP), giới thiệu cơ chế Multi-Head Self Attention (MHSA) cho phép mạng mô hình các phụ thuộc dài hạn và dễ dàng song song hóa. Trong bối cảnh này, Dosovitskiy et al. [2] thích ứng cơ chế attention với hình ảnh 2D và đề xuất Vision Transformer (ViT): hình ảnh đầu vào được chia thành các patch không chồng lấp, và các biểu diễn giữa các patch được học thông qua MHSA mà không có bias quy nạp. ViTs thể hiện kết quả đầy hứa hẹn so với các mạng neural convolutional (CNNs) trong các tác vụ thị giác máy tính. Sau thành công này, một số nỗ lực khám phá tiềm năng của ViT bằng cách cải thiện chiến lược huấn luyện [3,4,5], giới thiệu các thay đổi kiến trúc [6,7], thiết kế lại cơ chế attention [8,9], và nâng cao hiệu suất của các tác vụ thị giác khác nhau như phân loại [10, 11, 12], phân đoạn [13, 14], và phát hiện [15, 16].

Về mặt tiêu cực, các mô hình transformer thường chậm hơn nhiều lần so với các CNNs cạnh tranh [17,18]. Có nhiều yếu tố hạn chế tốc độ suy luận của ViT, bao gồm số lượng tham số khổng lồ, độ phức tạp tính toán tăng theo bậc hai với độ dài token, các lớp chuẩn hóa không thể gập lại, và thiếu tối ưu hóa cấp trình biên dịch (ví dụ, Winograd cho CNN [19]). Độ trễ cao khiến transformers không thực tế cho các ứng dụng thực tế trên phần cứng hạn chế tài nguyên, như các ứng dụng thực tế tăng cường hoặc thực tế ảo trên thiết bị di động và thiết bị đeo. Kết quả là, các CNNs nhẹ [20, 21, 22] vẫn là lựa chọn mặc định cho suy luận thời gian thực.

Để giảm bớt nút thắt độ trễ của transformers, nhiều phương pháp đã được đề xuất. Ví dụ, một số nỗ lực xem xét thiết kế kiến trúc hoặc phép toán mới bằng cách thay đổi các lớp tuyến tính bằng các lớp convolutional (CONV) [23], kết hợp self-attention với các khối MobileNet [24], hoặc giới thiệu attention thưa thớt [25,26,27], để giảm chi phí tính toán, trong khi các nỗ lực khác tận dụng thuật toán tìm kiếm mạng [28] hoặc pruning [29] để cải thiện hiệu quả. Mặc dù sự cân bằng tính toán-hiệu suất đã được cải thiện bởi các công trình hiện tại, câu hỏi cơ bản liên quan đến khả năng ứng dụng của các mô hình transformer vẫn chưa được trả lời: Liệu các vision transformers mạnh mẽ có thể chạy với tốc độ MobileNet và trở thành lựa chọn mặc định cho các ứng dụng edge? Công trình này cung cấp một nghiên cứu hướng tới câu trả lời thông qua các đóng góp sau:

• Đầu tiên, chúng tôi xem xét lại các nguyên tắc thiết kế của ViT và các biến thể của nó thông qua phân tích độ trễ (Mục 3). Theo các công trình hiện tại [18], chúng tôi sử dụng iPhone 12 làm testbed và CoreML [30] có sẵn công khai làm trình biên dịch, vì thiết bị di động được sử dụng rộng rãi và kết quả có thể dễ dàng tái tạo.

• Thứ hai, dựa trên phân tích của chúng tôi, chúng tôi xác định các thiết kế và toán tử không hiệu quả trong ViT và đề xuất một mô hình thiết kế chiều nhất quán mới cho vision transformers (Mục 4.1).

• Thứ ba, bắt đầu từ một supernet với mô hình thiết kế mới, chúng tôi đề xuất một phương pháp thu gọn hướng độ trễ đơn giản nhưng hiệu quả để có được một họ mô hình mới, cụ thể là EfﬁcientFormers (Mục 4.2). Chúng tôi tối ưu hóa trực tiếp cho tốc độ suy luận thay vì MACs hoặc số lượng tham số [31, 32, 33].

Mô hình nhanh nhất của chúng tôi, EfﬁcientFormer-L1, đạt 79,2% độ chính xác top-1 trên tác vụ phân loại ImageNet-1K [34] với chỉ 1,6ms thời gian suy luận (trung bình trên 1.000 lần chạy), chạy nhanh như MobileNetV2 ×1,4 và có độ chính xác top-1 cao hơn 4,5% (thêm kết quả trong Hình 1 và Bảng 1). Kết quả đầy hứa hẹn chứng minh rằng độ trễ không còn là trở ngại cho việc áp dụng rộng rãi các vision transformers. Mô hình lớn nhất của chúng tôi, EfﬁcientFormer-L7, đạt 83,3% độ chính xác với chỉ 7,0ms độ trễ, vượt trội hơn các thiết kế hybrid ViT × MobileNet (MobileViT-XS, 74,8%, 7,2ms) với biên độ lớn. Ngoài ra, chúng tôi quan sát hiệu suất vượt trội khi sử dụng EfﬁcientFormer làm backbone trong các benchmark phát hiện và phân đoạn hình ảnh (Bảng 2). Chúng tôi cung cấp một câu trả lời sơ bộ cho câu hỏi nêu trên, ViTs có thể đạt tốc độ suy luận cực nhanh và có hiệu suất mạnh mẽ cùng lúc. Chúng tôi hy vọng EfﬁcientFormer của chúng tôi có thể phục vụ như một baseline mạnh mẽ và truyền cảm hứng cho các công trình tiếp theo về triển khai edge của vision transformers.

## 2 Công trình liên quan

Transformers ban đầu được đề xuất để xử lý việc học các chuỗi dài trong các tác vụ NLP [1]. Dosovitskiy et al. [2] và Carion et al. [15] thích ứng kiến trúc transformer cho phân loại và phát hiện tương ứng, và đạt hiệu suất cạnh tranh với các đối tác CNN với kỹ thuật huấn luyện mạnh hơn và datasets quy mô lớn hơn. DeiT [3] tiếp tục cải thiện pipeline huấn luyện với sự trợ giúp của distillation, loại bỏ nhu cầu pretraining quy mô lớn [35]. Được truyền cảm hứng bởi hiệu suất cạnh tranh và trường nhận thức toàn cục của các mô hình transformer, các công trình tiếp theo được đề xuất để tinh chỉnh kiến trúc [36,37], khám phá mối quan hệ giữa mạng CONV và ViT [38,39,40], và thích ứng ViT với các tác vụ thị giác máy tính khác nhau [13,41,42,43,44,45,46]. Các nỗ lực nghiên cứu khác khám phá bản chất của cơ chế attention và đề xuất các biến thể sâu sắc của token mixer, ví dụ như attention cục bộ [8], MLP không gian [47, 48], và pooling-mixer [6].

Mặc dù thành công trong hầu hết các tác vụ thị giác, các mô hình dựa trên ViT không thể cạnh tranh với các CNNs nhẹ được nghiên cứu kỹ lưỡng [21,49] khi tốc độ suy luận là mối quan tâm chính [50,51,52], đặc biệt là trên các thiết bị edge hạn chế tài nguyên [17]. Để tăng tốc ViT, nhiều phương pháp đã được giới thiệu với các phương pháp luận khác nhau, như đề xuất kiến trúc hoặc module mới [53,54,55,56,57,58], suy nghĩ lại self-attention và cơ chế sparse-attention [59,60,61,62,63,64,65], và sử dụng các thuật toán tìm kiếm được khám phá rộng rãi trong CNNs để tìm các ViTs nhỏ hơn và nhanh hơn [66,28,29,67].

Gần đây, LeViT [23] đề xuất một thiết kế CONV-clothing để tăng tốc vision transformer. Tuy nhiên, để thực hiện MHSA, các đặc trưng 4D cần được reshape thường xuyên thành các patch phẳng, điều này vẫn tốn kém để tính toán trên tài nguyên edge (Hình 2). Tương tự, MobileViT [18] giới thiệu một kiến trúc hybrid kết hợp các khối MobileNet nhẹ (với CONV point-wise và depth-wise) và các khối MHSA; khối trước được đặt ở các giai đoạn đầu trong pipeline mạng để trích xuất đặc trưng cấp thấp, trong khi khối sau được đặt ở các giai đoạn cuối để tận hưởng trường nhận thức toàn cục. Phương pháp tương tự đã được khám phá bởi một số công trình [24, 28] như một chiến lược đơn giản để giảm tính toán.

Khác với các công trình hiện tại, chúng tôi nhằm mục đích đẩy ranh giới độ trễ-hiệu suất của các vision transformers thuần túy thay vì dựa vào các thiết kế hybrid, và tối ưu hóa trực tiếp cho độ trễ di động. Thông qua phân tích chi tiết của chúng tôi (Mục 3), chúng tôi đề xuất một mô hình thiết kế mới (Mục 4.1), có thể được nâng cao thêm thông qua tìm kiếm kiến trúc (Mục 4.2).

## 3 Phân tích Độ trễ Trên thiết bị của Vision Transformers

Hầu hết các phương pháp hiện tại tối ưu hóa tốc độ suy luận của transformers thông qua độ phức tạp tính toán (MACs) hoặc throughput (hình ảnh/giây) thu được từ server GPU [23,28]. Trong khi các metric như vậy không phản ánh độ trễ thực tế trên thiết bị. Để có hiểu biết rõ ràng về những phép toán và lựa chọn thiết kế nào làm chậm suy luận của ViTs trên thiết bị edge, chúng tôi thực hiện phân tích độ trễ toàn diện trên một số mô hình và phép toán, như được hiển thị trong Hình 2, qua đó các quan sát sau được rút ra.

**Quan sát 1**: Patch embedding với kernel và stride lớn là một nút thắt tốc độ trên thiết bị di động.

Patch embedding thường được triển khai với một lớp convolution không chồng lấp có kích thước kernel và stride lớn [3,55]. Một niềm tin phổ biến là chi phí tính toán của lớp patch embedding trong mạng transformer là không đáng kể hoặc có thể bỏ qua [2,6]. Tuy nhiên, so sánh của chúng tôi trong Hình 2 giữa các mô hình có kernel và stride lớn cho patch embedding, tức là DeiT-S [3] và PoolFormer-S24 [6], và các mô hình không có nó, tức là LeViT-256 [23] và EfﬁcientFormer, cho thấy patch embedding thực sự là nút thắt tốc độ trên thiết bị di động.

Các convolution kernel lớn không được hỗ trợ tốt bởi hầu hết các trình biên dịch và không thể được tăng tốc thông qua các thuật toán hiện tại như Winograd [19]. Thay vào đó, patch embedding không chồng lấp có thể được thay thế bằng một convolution stem với downsampling nhanh [68,69,23] bao gồm một số convolution 3×3 hiệu quả về phần cứng (Hình 3).

**Quan sát 2**: Chiều đặc trưng nhất quán quan trọng cho việc lựa chọn token mixer. MHSA không nhất thiết là nút thắt tốc độ.

Công trình gần đây mở rộng các mô hình dựa trên ViT thành kiến trúc MetaFormer [6] bao gồm các khối MLP và token mixers không được chỉ định. Việc chọn token mixer là một lựa chọn thiết kế quan trọng khi xây dựng các mô hình dựa trên ViT. Các lựa chọn rất nhiều—mixer MHSA thông thường với trường nhận thức toàn cục, shifted window attention phức tạp hơn [8], hoặc một toán tử không tham số như pooling [6]. Chúng tôi thu hẹp so sánh thành hai token mixers, pooling và MHSA, trong đó chúng tôi chọn cái trước vì sự đơn giản và hiệu quả, trong khi cái sau cho hiệu suất tốt hơn. Các token mixers phức tạp hơn như shifted window [8] hiện tại không được hỗ trợ bởi hầu hết các trình biên dịch di động công khai và chúng tôi để chúng ngoài phạm vi của chúng tôi. Hơn nữa, chúng tôi không sử dụng depth-wise convolution để thay thế pooling [70] vì chúng tôi tập trung vào xây dựng kiến trúc mà không cần sự trợ giúp của các convolution nhẹ.

Để hiểu độ trễ của hai token mixers, chúng tôi thực hiện hai so sánh sau:

• Đầu tiên, bằng cách so sánh PoolFormer-s24 [6] và LeViT-256 [23], chúng tôi quan sát rằng phép toán Reshape là nút thắt cho LeViT-256. Phần lớn LeViT-256 được triển khai với CONV trên tensor 4D, yêu cầu các phép toán reshape thường xuyên khi chuyển đặc trưng vào MHSA vì attention phải được thực hiện trên tensor 3D được patchify (loại bỏ chiều bổ sung của attention heads). Việc sử dụng rộng rãi Reshape hạn chế tốc độ của LeViT trên thiết bị di động (Hình 2). Mặt khác, pooling phù hợp tự nhiên với tensor 4D khi mạng chủ yếu bao gồm các triển khai dựa trên CONV, ví dụ như CONV 1×1 làm triển khai MLP và CONV stem cho downsampling. Kết quả là, PoolFormer thể hiện tốc độ suy luận nhanh hơn.

• Thứ hai, bằng cách so sánh DeiT-Small [3] và LeViT-256 [23], chúng tôi thấy rằng MHSA không mang lại overhead đáng kể trên di động nếu chiều đặc trưng nhất quán và không cần Reshape. Mặc dù tốn nhiều tính toán hơn, DeiT-Small với đặc trưng 3D nhất quán có thể đạt tốc độ tương đương với biến thể ViT mới, tức là LeViT-256.

Trong công trình này, chúng tôi đề xuất một mạng chiều nhất quán (Mục 4.1) với cả triển khai đặc trưng 4D và MHSA 3D, nhưng các phép toán Reshape thường xuyên không hiệu quả được loại bỏ.

**Quan sát 3**: CONV-BN thuận lợi hơn về độ trễ so với LN (GN)-Linear và sự suy giảm độ chính xác thường có thể chấp nhận được.

Việc chọn triển khai MLP là một lựa chọn thiết kế quan trọng khác. Thường thì một trong hai lựa chọn được chọn: layer normalization (LN) với linear projection (proj.) 3D và CONV 1×1 với batch normalization (BN). CONV-BN thuận lợi hơn về độ trễ vì BN có thể được gập vào convolution đứng trước để tăng tốc suy luận, trong khi các chuẩn hóa động, như LN và GN, vẫn thu thập thống kê running ở giai đoạn suy luận, do đó góp phần vào độ trễ. Từ phân tích của DeiT-Small và PoolFormer-S24 trong Hình 2 và công trình trước [17], độ trễ được giới thiệu bởi LN chiếm khoảng 10%−20% độ trễ của toàn bộ mạng.

Dựa trên nghiên cứu ablation của chúng tôi trong Phụ lục Bảng 3, CONV-BN chỉ giảm nhẹ hiệu suất so với GN và đạt kết quả tương đương với channel-wise LN. Trong công trình này, chúng tôi áp dụng CONV-BN càng nhiều càng tốt (trong tất cả các đặc trưng 4D ẩn) để tăng độ trễ với sự suy giảm hiệu suất không đáng kể, trong khi sử dụng LN cho các đặc trưng 3D, phù hợp với thiết kế MHSA ban đầu trong ViT và mang lại độ chính xác tốt hơn.

**Quan sát 4**: Độ trễ của phi tuyến phụ thuộc vào phần cứng và trình biên dịch.

Cuối cùng, chúng tôi nghiên cứu phi tuyến, bao gồm GeLU, ReLU, và HardSwish. Công trình trước [17] cho rằng GeLU không hiệu quả trên phần cứng và làm chậm suy luận. Tuy nhiên, chúng tôi quan sát GeLU được hỗ trợ tốt bởi iPhone 12 và hầu như không chậm hơn đối tác của nó, ReLU. Ngược lại, HardSwish chậm một cách đáng ngạc nhiên trong các thí nghiệm của chúng tôi và có thể không được hỗ trợ tốt bởi trình biên dịch (độ trễ LeViT-256 với HardSwish là 44,5ms trong khi với GeLU là 11,9ms). Chúng tôi kết luận rằng phi tuyến nên được xác định từng trường hợp cụ thể với phần cứng và trình biên dịch cụ thể có sẵn. Chúng tôi tin rằng hầu hết các kích hoạt sẽ được hỗ trợ trong tương lai. Trong công trình này, chúng tôi sử dụng các kích hoạt GeLU.

## 4 Thiết kế EfﬁcientFormer

Dựa trên phân tích độ trễ, chúng tôi đề xuất thiết kế của EfﬁcientFormer, được thể hiện trong Hình 3. Mạng bao gồm một patch embedding (PatchEmbed) và stack các meta transformer blocks, được ký hiệu là MB:

Y = ∏ᵢ₌₁ᵐ MBᵢ(PatchEmbed(X₀^{B,3,H,W}))  (1)

trong đó X₀ là hình ảnh đầu vào với batch size là B và kích thước không gian là [H,W], Y là đầu ra mong muốn, và m là tổng số blocks (độ sâu). MB bao gồm token mixer không được chỉ định (TokenMixer) theo sau bởi một khối MLP và có thể được biểu diễn như sau:

X_{i+1} = MBᵢ(Xᵢ) = MLP(TokenMixer(Xᵢ))  (2)

trong đó Xᵢ với i > 0 là đặc trưng trung gian được chuyển tiếp vào MB thứ i. Chúng tôi tiếp tục định nghĩa Stage (hoặc S) như stack của một số MetaBlocks xử lý các đặc trưng với cùng kích thước không gian, như N₁× trong Hình 3 biểu thị S₁ có N₁ MetaBlocks. Mạng bao gồm 4 Stages. Giữa mỗi Stage, có một phép toán embedding để project chiều embedding và downsample độ dài token, được ký hiệu là Embedding trong Hình 3. Với kiến trúc trên, EfﬁcientFormer là một mô hình hoàn toàn dựa trên transformer mà không tích hợp cấu trúc MobileNet. Tiếp theo, chúng tôi đi sâu vào chi tiết thiết kế mạng, cụ thể là chi tiết kiến trúc và thuật toán tìm kiếm.

### 4.1 Thiết kế Chiều Nhất quán

Với các quan sát trong Mục 3, chúng tôi đề xuất một thiết kế chiều nhất quán chia mạng thành một phân vùng 4D nơi các toán tử được triển khai theo kiểu CONV-net (MB4D), và một phân vùng 3D nơi các linear projections và attentions được thực hiện trên tensor 3D để tận hưởng sức mạnh mô hình toàn cục của MHSA mà không hy sinh hiệu quả (MB3D), như được hiển thị trong Hình 3. Cụ thể, mạng bắt đầu với phân vùng 4D, trong khi phân vùng 3D được áp dụng ở các giai đoạn cuối. Lưu ý rằng Hình 3 chỉ là một ví dụ, độ dài thực tế của phân vùng 4D và 3D được chỉ định sau thông qua tìm kiếm kiến trúc.

Đầu tiên, hình ảnh đầu vào được xử lý bởi một CONV stem với hai convolution 3×3 với stride 2 làm patch embedding,

X₁^{B,C_j,H/4,W/4} = PatchEmbed(X₀^{B,3,H,W})  (3)

trong đó Cⱼ là số kênh (width) của stage thứ j. Sau đó mạng bắt đầu với MB4D với một Pool mixer đơn giản để trích xuất đặc trưng cấp thấp,

Iᵢ = Pool(Xᵢ^{B,C_j,H/2^{j+1},W/2^{j+1}}) + Xᵢ^{B,C_j,H/2^{j+1},W/2^{j+1}}
Xᵢ₊₁^{B,C_j,H/2^{j+1},W/2^{j+1}} = ConvB,G(ConvB,G(Iᵢ)) + Iᵢ  (4)

trong đó ConvB,G đề cập đến việc convolution có được theo sau bởi BN và GeLU hay không. Lưu ý ở đây chúng tôi không sử dụng Group hoặc Layer Normalization (LN) trước Pool mixer như trong [6], vì phân vùng 4D là thiết kế dựa trên CONV-BN, do đó tồn tại một BN phía trước mỗi Pool mixer.

Sau khi xử lý tất cả các khối MB4D, chúng tôi thực hiện một lần reshaping để chuyển đổi kích thước đặc trưng và vào phân vùng 3D. MB3D theo cấu trúc ViT thông thường, như trong Hình 3. Chính thức,

Iᵢ = Linear(MHSA(Linear(LN(Xᵢ^{B,HW/4^{j+1},C_j})))) + Xᵢ^{B,HW/4^{j+1},C_j}
Xᵢ₊₁^{B,HW/4^{j+1},C_j} = Linear(LinearG(LN(Iᵢ))) + Iᵢ  (5)

trong đó LinearG biểu thị Linear theo sau bởi GeLU, và

MHSA(Q,K,V) = Softmax(Q·K^T/√C_j + b)·V  (6)

trong đó Q,K,V đại diện cho query, key, và values được học bởi linear projection, và b là attention bias được tham số hóa như position encodings.

### 4.2 Thu gọn Hướng Độ trễ

**Thiết kế Supernet.** Dựa trên thiết kế chiều nhất quán, chúng tôi xây dựng một supernet để tìm kiếm các mô hình hiệu quả của kiến trúc mạng được hiển thị trong Hình 3 (Hình 3 hiển thị một ví dụ về mạng cuối cùng được tìm kiếm). Để biểu diễn supernet như vậy, chúng tôi định nghĩa MetaPath (MP), là tập hợp các blocks có thể:

MP_{i,j=1,2} ∈ {MB4D_i, I_i}
MP_{i,j=3,4} ∈ {MB4D_i, MB3D_i, I_i}  (7)

trong đó I đại diện cho identity path, j biểu thị Stage thứ j, và i biểu thị block thứ i. Supernet có thể được minh họa bằng cách thay thế MB trong Hình 3 bằng MP.

Như trong Phương trình 7, trong S₁ và S₂ của supernet, mỗi block có thể chọn từ MB4D hoặc I, trong khi ở S₃ và S₄, block có thể là MB3D, MB4D, hoặc I. Chúng tôi chỉ kích hoạt MB3D trong hai Stages cuối vì hai lý do. Đầu tiên, vì tính toán của MHSA tăng theo bậc hai với độ dài token, việc tích hợp nó ở các Stages đầu sẽ tăng đáng kể chi phí tính toán. Thứ hai, việc áp dụng MHSA toàn cục cho các Stages cuối phù hợp với trực giác rằng các giai đoạn đầu trong mạng nắm bắt các đặc trưng cấp thấp, trong khi các lớp cuối học các phụ thuộc dài hạn.

**Không gian Tìm kiếm.** Không gian tìm kiếm của chúng tôi bao gồm Cⱼ (width của mỗi Stage), Nⱼ (số lượng blocks trong mỗi Stage, tức là độ sâu), và N blocks cuối cùng để áp dụng MB3D.

**Thuật toán Tìm kiếm.** Các phương pháp tìm kiếm mạng hardware-aware trước đây thường dựa vào triển khai phần cứng của từng candidate trong không gian tìm kiếm để có được độ trễ, điều này tốn thời gian [71]. Trong công trình này, chúng tôi đề xuất một thuật toán tìm kiếm dựa trên gradient đơn giản, nhanh nhưng hiệu quả để có được một mạng candidate chỉ cần huấn luyện supernet một lần. Thuật toán có ba bước chính.

Đầu tiên, chúng tôi huấn luyện supernet với Gumbel Softmax sampling [72] để có được importance score cho các blocks trong mỗi MP, có thể được biểu diễn như

X_{i+1} = Σ_n (e^{(α_i^n + ε_i^n)/τ} / Σ_n e^{(α_i^n + ε_i^n)/τ}) · MP_{i,j}(X_i)  (8)

trong đó α đánh giá tầm quan trọng của mỗi block trong MP vì nó đại diện cho xác suất chọn một block, ví dụ, MB4D hoặc MB3D cho block thứ i. ε ~ U(0,1) đảm bảo khám phá, τ là nhiệt độ, và n đại diện cho loại blocks trong MP, tức là n ∈ {4D, I} cho S₁ và S₂, và n ∈ {4D, 3D, I} cho S₃ và S₄. Bằng cách sử dụng Phương trình 8, các đạo hàm theo trọng số mạng và α có thể được tính toán dễ dàng. Việc huấn luyện tuân theo công thức tiêu chuẩn (xem Mục 5.1) để có được trọng số đã huấn luyện và tham số kiến trúc α.

Thứ hai, chúng tôi xây dựng bảng tra cứu độ trễ bằng cách thu thập độ trễ trên thiết bị của MB4D và MB3D với các width khác nhau (bội số của 16).

Cuối cùng, chúng tôi thực hiện network slimming trên supernet thu được từ bước đầu tiên thông qua đánh giá độ trễ sử dụng bảng tra cứu. Lưu ý rằng một thuật toán tìm kiếm dựa trên gradient điển hình đơn giản chọn block có α lớn nhất [72], điều này không phù hợp với phạm vi của chúng tôi vì nó không thể tìm kiếm width Cⱼ. Thực tế, việc xây dựng một supernet multi-width tốn bộ nhớ và thậm chí không thực tế vì mỗi MP có một số nhánh trong thiết kế của chúng tôi. Thay vì tìm kiếm trực tiếp trên không gian tìm kiếm phức tạp, chúng tôi thực hiện một slimming dần dần trên supernet single-width như sau.

Chúng tôi đầu tiên định nghĩa importance score cho MPᵢ là α_i^{4D}/(α_i^I) và (α_i^{3D} + α_i^{4D})/(α_i^I) cho S₁,₂ và S₃,₄ tương ứng. Tương tự, importance score cho mỗi Stage có thể được tính bằng cách cộng các điểm số cho tất cả MP trong Stage. Với importance score, chúng tôi định nghĩa action space bao gồm ba lựa chọn: 1) chọn I cho MP ít quan trọng nhất, 2) loại bỏ MB3D đầu tiên, và 3) giảm width của Stage ít quan trọng nhất (theo bội số của 16). Sau đó, chúng tôi tính toán độ trễ kết quả của mỗi action thông qua bảng tra cứu, và đánh giá sự suy giảm độ chính xác của mỗi action. Cuối cùng, chúng tôi chọn action dựa trên sự suy giảm độ chính xác trên mỗi độ trễ (-Δ%/Δms). Quá trình này được thực hiện lặp đi lặp lại cho đến khi đạt được độ trễ mục tiêu. Chúng tôi hiển thị thêm chi tiết về thuật toán trong Phụ lục.

## 5 Thí nghiệm và Thảo luận

Chúng tôi triển khai EfﬁcientFormer thông qua PyTorch 1.11 [73] và thư viện Timm [74], đây là thông lệ phổ biến trong các nghệ thuật gần đây [18,6]. Các mô hình của chúng tôi được huấn luyện trên một cluster với GPU NVIDIA A100 và V100. Tốc độ suy luận trên iPhone 12 (chip A14 bionic) được đo với iOS phiên bản 15 và trung bình trên 1.000 lần chạy, với tất cả tài nguyên tính toán có sẵn (NPU), hoặc chỉ CPU. CoreMLTools được sử dụng để triển khai mô hình run-time. Ngoài ra, chúng tôi cung cấp phân tích độ trễ trên Nvidia A100 GPU với batch size 64 để khai thác hardware roofline. Các mô hình PyTorch đã huấn luyện được triển khai ở định dạng ONNX và được biên dịch với TensorRT. Chúng tôi báo cáo GPU runtime loại trừ preprocessing. Chúng tôi cung cấp kiến trúc mạng chi tiết và nhiều nghiên cứu ablation hơn trong Phụ lục 6.

### 5.1 Phân loại Hình ảnh

Tất cả các mô hình EfﬁcientFormer được huấn luyện từ đầu trên dataset ImageNet-1K [34] để thực hiện tác vụ phân loại hình ảnh. Chúng tôi sử dụng kích thước hình ảnh tiêu chuẩn (224×224) cho cả huấn luyện và kiểm tra. Chúng tôi tuân theo công thức huấn luyện từ DeiT [3] nhưng chủ yếu báo cáo kết quả với 300 epoch huấn luyện để có sự so sánh với các mô hình dựa trên ViT khác. Chúng tôi sử dụng optimizer AdamW [75,76], huấn luyện warm-up với 5 epochs, và lịch trình learning rate cosine annealing. Learning rate ban đầu được đặt là 10⁻³ × (batchsize/1024) và learning rate tối thiểu là 10⁻⁵. Mô hình teacher cho distillation là RegNetY-16GF [77] được pretrain trên ImageNet với 82,9% độ chính xác top-1. Kết quả được thể hiện trong Bảng 1 và Hình 1.

**So sánh với CNNs.** So với các mô hình dựa trên CNN được sử dụng rộng rãi, EfﬁcientFormer đạt được sự cân bằng tốt hơn giữa độ chính xác và độ trễ. Trên iPhone Neural Engine, EfﬁcientFormer-L1 chạy với tốc độ MobileNetV2 ×1,4 trong khi đạt độ chính xác top-1 cao hơn 4,5%. Ngoài ra, EfﬁcientFormer-L3 chạy với tốc độ tương tự như EfﬁcientNet-B0 trong khi đạt độ chính xác top-1 cao hơn 5,3% tương đối. Đối với các mô hình có hiệu suất cao (>83% top-1), EfﬁcientFormer-L7 chạy nhanh hơn 3× so với EfﬁcientNet-B5, thể hiện hiệu suất advantageous của các mô hình chúng tôi. Hơn nữa trên desktop GPU (A100), EfﬁcientFormer-L1 chạy nhanh hơn 38% so với EfﬁcientNet-B0 trong khi đạt độ chính xác top-1 cao hơn 2,1%. EfﬁcientFormer-L7 chạy nhanh hơn 4,6× so với EfﬁcientNet-B5. Những kết quả này cho phép chúng tôi trả lời câu hỏi trung tâm được đưa ra trước đó; ViTs không cần hy sinh độ trễ để đạt hiệu suất tốt, và một ViT chính xác vẫn có thể có tốc độ suy luận cực nhanh như các CNNs nhẹ làm.

**So sánh với ViTs.** Các ViTs thông thường vẫn kém hiệu suất so với CNNs về độ trễ. Ví dụ, DeiT-Tiny đạt độ chính xác tương tự như EfﬁcientNet-B0 trong khi nó chạy chậm hơn 3,4×. Tuy nhiên, EfﬁcientFormer hoạt động như các mô hình transformer khác trong khi chạy nhanh hơn nhiều lần. EfﬁcientFormer-L3 đạt độ chính xác cao hơn DeiT-Small (82,4% vs. 81,2%) trong khi nhanh hơn 4×. Đáng chú ý rằng mặc dù biến thể transformer gần đây, PoolFormer [6], tự nhiên có kiến trúc 4D nhất quán và chạy nhanh hơn so với các ViTs điển hình, việc thiếu MHSA toàn cục hạn chế đáng kể upper-bound hiệu suất. EfﬁcientFormer-L3 đạt độ chính xác top-1 cao hơn 1% so với PoolFormer-S36, trong khi nhanh hơn 3× trên Nvidia A100 GPU, nhanh hơn 2,2× trên iPhone NPU và nhanh hơn 6,8× trên iPhone CPU.

**So sánh với Thiết kế Hybrid.** Các thiết kế hybrid hiện tại, ví dụ như LeViT-256 và MobileViT, vẫn gặp khó khăn với nút thắt độ trễ của ViTs và khó có thể vượt trội hơn các CNNs nhẹ. Ví dụ, LeViT-256 chạy chậm hơn DeiT-Small trong khi có độ chính xác top-1 thấp hơn 1%. Đối với MobileViT, là một mô hình hybrid với cả MHSA và khối MobileNet, chúng tôi quan sát rằng nó chậm hơn đáng kể so với các đối tác CNN, ví dụ như MobileNetV2 và EfﬁcientNet-B0, trong khi độ chính xác cũng không thỏa mãn (thấp hơn 2,3% so với EfﬁcientNet-B0). Do đó, việc đơn giản đánh đổi MHSA với các khối MobileNet khó có thể đẩy forward đường cong Pareto, như trong Hình 1. Ngược lại, EfﬁcientFormer, như mô hình dựa trên transformer thuần túy, có thể duy trì hiệu suất cao trong khi đạt tốc độ suy luận cực nhanh. EfﬁcientFormer-L1 có độ chính xác top-1 cao hơn 4,4% so với MobileViT-XS và chạy nhanh hơn nhiều trên các phần cứng và trình biên dịch khác nhau (nhanh hơn 1,9× trên Nvidia A100 GPU Computing, nhanh hơn 2,3× trên iPhone CPU, và nhanh hơn 4,5× trên iPhone NPU). Ở thời gian suy luận tương tự, EfﬁcientFormer-L7 vượt trội hơn MobileViT-XS 8,5% độ chính xác top-1 trên ImageNet, thể hiện sự vượt trội của thiết kế chúng tôi.

### 5.2 EfﬁcientFormer làm Backbone

**Phát hiện Đối tượng và Phân đoạn Instance.** Chúng tôi tuân theo triển khai của Mask-RCNN [78] để tích hợp EfﬁcientFormer làm backbone và xác minh hiệu suất. Chúng tôi thử nghiệm trên COCO-2017 [79] chứa các tập huấn luyện và validation của 118K và 5K hình ảnh tương ứng. Backbone EfﬁcientFormer được khởi tạo với trọng số pretrained ImageNet-1K. Tương tự như công trình trước [6], chúng tôi sử dụng optimizer AdamW [75,76] với learning rate ban đầu 2×10⁻⁴, và huấn luyện mô hình trong 12 epochs. Chúng tôi đặt kích thước đầu vào là 1333×800.

Kết quả cho phát hiện và phân đoạn instance được hiển thị trong Bảng 2. EfﬁcientFormers liên tục vượt trội hơn các backbone CNN (ResNet) và transformer (PoolFormer). Với chi phí tính toán tương tự, EfﬁcientFormer-L3 vượt trội hơn backbone ResNet50 3,4 box AP và 3,7 mask AP, và vượt trội hơn backbone PoolFormer-S24 với 1,3 box AP và 1,1 mask AP, chứng minh rằng EfﬁcientFormer tổng quát hóa tốt như một backbone mạnh mẽ trong các tác vụ thị giác.

**Phân đoạn Semantic.** Chúng tôi tiếp tục xác thực hiệu suất của EfﬁcientFormer trên tác vụ phân đoạn semantic. Chúng tôi sử dụng dataset scene parsing đầy thử thách, ADE20K [80,81], chứa 20K hình ảnh huấn luyện và 2K hình ảnh validation bao gồm 150 danh mục lớp. Tương tự như công trình hiện tại [6], chúng tôi xây dựng EfﬁcientFormer làm backbone cùng với Semantic FPN [82] làm decoder phân đoạn để so sánh công bằng. Backbone được khởi tạo với trọng số pretrained trên ImageNet-1K và mô hình được huấn luyện trong 40K iterations với tổng batch size 32 trên 8 GPUs. Chúng tôi tuân theo thông lệ phổ biến trong phân đoạn [6,13], sử dụng optimizer AdamW [75,76], và áp dụng lịch trình poly learning rate với power 0,9, bắt đầu từ learning rate ban đầu 2×10⁻⁴. Chúng tôi resize và crop hình ảnh đầu vào thành 512×512 cho huấn luyện và shorter side là 512 cho kiểm tra (trên tập validation).

Như được hiển thị trong Bảng 2, EfﬁcientFormer liên tục vượt trội hơn các backbone dựa trên CNN và transformer với biên độ lớn dưới ngân sách tính toán tương tự. Ví dụ, EfﬁcientFormer-L3 vượt trội hơn PoolFormer-S24 3,2 mIoU. Chúng tôi chỉ ra rằng với global attention, EfﬁcientFormer học được các phụ thuộc dài hạn tốt hơn, điều này có lợi trong các tác vụ dự đoán dense độ phân giải cao.

### 5.3 Thảo luận

**Mối quan hệ với MetaFormer.** Thiết kế của EfﬁcientFormer một phần được truyền cảm hứng bởi khái niệm MetaFormer [6]. So với PoolFormer, EfﬁcientFormer giải quyết vấn đề dimension mismatch, là nguyên nhân gốc rễ của suy luận edge không hiệu quả, do đó có khả năng sử dụng MHSA toàn cục mà không hy sinh tốc độ. Kết quả là, EfﬁcientFormer thể hiện hiệu suất độ chính xác advantageous so với PoolFormer. Mặc dù thiết kế 4D hoàn toàn, PoolFormer sử dụng patch embedding không hiệu quả và group normalization (Hình 2), dẫn đến tăng độ trễ. Thay vào đó, phân vùng 4D được thiết kế lại của EfﬁcientFormer (Hình 3) thân thiện hơn với phần cứng và thể hiện hiệu suất tốt hơn trên nhiều tác vụ.

**Hạn chế.** (i) Mặc dù hầu hết các thiết kế trong EfﬁcientFormer có mục đích chung, ví dụ như thiết kế chiều nhất quán và khối 4D với CONV-BN fusion, tốc độ thực tế của EfﬁcientFormer có thể thay đổi trên các platform khác. Ví dụ, nếu GeLU không được hỗ trợ tốt trong khi HardSwish được triển khai hiệu quả trên phần cứng và trình biên dịch cụ thể, toán tử có thể cần được sửa đổi tương ứng. (ii) Latency-driven slimming được đề xuất đơn giản và nhanh. Tuy nhiên, kết quả tốt hơn có thể đạt được nếu chi phí tìm kiếm không phải là mối quan tâm và một brute search dựa trên enumeration được thực hiện.

## 6 Kết luận

Trong công trình này, chúng tôi chỉ ra rằng Vision Transformer có thể hoạt động với tốc độ MobileNet trên thiết bị di động. Bắt đầu từ một phân tích độ trễ toàn diện, chúng tôi xác định các toán tử không hiệu quả trong một loạt kiến trúc dựa trên ViT, qua đó chúng tôi rút ra các quan sát quan trọng hướng dẫn mô hình thiết kế mới của chúng tôi. EfﬁcientFormer được đề xuất tuân thủ thiết kế chiều nhất quán mà một cách mượt mà tận dụng các MetaBlocks 4D thân thiện với phần cứng và các khối MHSA 3D mạnh mẽ. Chúng tôi tiếp tục đề xuất một phương pháp latency-driven slimming nhanh để rút ra các cấu hình được tối ưu hóa dựa trên không gian thiết kế của chúng tôi. Các thí nghiệm mở rộng trên phân loại hình ảnh, phát hiện đối tượng, và các tác vụ phân đoạn cho thấy các mô hình EfﬁcientFormer vượt trội hơn các mô hình transformer hiện tại trong khi nhanh hơn hầu hết các CNNs cạnh tranh. Phân tích hướng độ trễ của kiến trúc ViT và kết quả thực nghiệm xác thực tuyên bố của chúng tôi: các vision transformers mạnh mẽ có thể đạt tốc độ suy luận cực nhanh trên edge. Nghiên cứu tương lai sẽ tiếp tục khám phá tiềm năng của EfﬁcientFormer trên một số thiết bị hạn chế tài nguyên.
