# Laughing Hyena Distillery:
Trích xuất Truy hồi Compact từ các Convolution

# Trích xuất Truy hồi Compact từ các Convolution

Stefano Massaroli∗,1, Michael Poli∗,2, Daniel Y. Fu∗,2,
Hermann Kumbong2, Rom N. Parnichkun3, Aman Timalsina4,
David W. Romero5, Quinn McIntyre2, Beidi Chen6, Atri Rudra7, Ce Zhang8,
Christopher Re2,†, Stefano Ermon2,†, Yoshua Bengio1,†

NeurIPS 2023, Biên dịch lần cuối: 31 tháng 10, 2023.

## Tóm tắt

Những tiến bộ gần đây trong các mô hình chuỗi không cần attention dựa vào convolution như là lựa chọn thay thế cho toán tử attention ở lõi của Transformers. Đặc biệt, các mô hình chuỗi convolution dài đã đạt được hiệu suất tối ưu trong nhiều lĩnh vực, nhưng phát sinh chi phí đáng kể trong quá trình suy luận tự hồi quy - một cách ngây thơ yêu cầu một lần chạy đầy đủ (hoặc lưu trữ các activation) trên toàn bộ chuỗi đầu vào cho mỗi token được tạo ra - tương tự như các mô hình dựa trên attention. Trong bài báo này, chúng tôi tìm cách cho phép chi phí tính toán và bộ nhớ O(1) cho mỗi token trong bất kỳ kiến trúc convolution dài đã được huấn luyện trước nào để giảm lượng bộ nhớ và tăng thông lượng trong quá trình tạo ra. Cụ thể, các phương pháp của chúng tôi bao gồm trích xuất các mô hình không gian trạng thái tuyến tính chiều thấp từ mỗi lớp convolution, xây dựng trên các kỹ thuật nội suy hữu tỉ và giảm bậc mô hình. Chúng tôi tiếp tục giới thiệu các cải tiến kiến trúc cho các lớp dựa trên convolution như Hyena: bằng cách ràng buộc trọng số các bộ lọc qua các kênh thành các đầu, chúng tôi đạt được chất lượng huấn luyện trước cao hơn và giảm số lượng bộ lọc cần được chưng cất. Mô hình kết quả đạt được thông lượng cao hơn 10× so với Transformers và 1.5× cao hơn so với Hyena ở 1.3B tham số, không có bất kỳ tổn thất chất lượng nào sau khi chưng cất.

## 1 Giới thiệu

Các phương pháp không cần attention như các mô hình chuỗi convolution dài (LCSMs), ví dụ H3[1], Hyena[2], đã thể hiện tiềm năng trong việc khớp với hiệu suất Transformer [3, 4] trên một loạt các tác vụ, với độ phức tạp dưới bậc hai đối với độ dài chuỗi. Mặc dù cải thiện hiệu quả trong quá trình huấn luyện trên các chuỗi dài, trừ khi các bộ lọc convolution là ngắn hoặc thừa nhận một thực hiện không gian-trạng thái chiều thấp, LCSMs vẫn cần xử lý toàn bộ chuỗi đang phát triển ở mỗi bước của việc tạo ra tự hồi quy, tương tự như Transformers.

Trong công việc này, chúng tôi tìm cách tinh chỉnh LCSMs về cả hiệu quả và chất lượng. Đầu tiên, chúng tôi nghiên cứu giai đoạn suy luận, và đề xuất các phương pháp để kích hoạt chế độ truy hồi cho việc tạo ra tự hồi quy. Các chế độ truy hồi quy định sự tồn tại của một trạng thái mã hóa thông tin quá khứ của quá trình trong một bộ nhớ chiều cố định, cho phép thời gian không đổi cho mỗi bước và bộ nhớ không đổi trong quá trình tạo ra. Sau đó, chúng tôi dựa vào một phân tích các mô hình đã được huấn luyện trước để phát triển các cải tiến kiến trúc cho khối Hyena, đồng thời cải thiện chất lượng mô hình và hiệu quả của thủ tục chưng cất.

**Chưng cất truy hồi nhanh** Chúng tôi giới thiệu LaughingHyena, phương pháp chưng cất đầu tiên cho LCSMs cho phép suy luận truy hồi mà không ảnh hưởng đến chất lượng downstream. LaughingHyena tìm kiếm các truy hồi compact dưới dạng các mô hình không gian trạng thái (SSMs) [5, 6] như là giải pháp của một bài toán nội suy phi tuyến liên quan đến các bộ lọc convolution của một mô hình đã được huấn luyện trước. Vì tổng chi phí bộ nhớ của SSMs tăng tuyến tính theo chiều trạng thái d, thủ tục chưng cất của chúng tôi cho phép thông lượng cao bằng cách kích hoạt xử lý các lô lớn trong quá trình tạo ra.

Chúng tôi xác định và giải quyết ba thách thức cốt lõi liên quan đến chưng cất, bao gồm việc xác định:
• **Chiều trạng thái mục tiêu**: chúng tôi xác định các chiều trạng thái ứng cử viên của SSMs chưng cất của chúng tôi bằng cách phân tích phổ của toán tử Hankel liên kết với mỗi convolution [7].
• **Tham số hóa**: chúng tôi giải quyết các vấn đề với tham số hóa ngây thơ bằng cách giới thiệu một dạng modal phân rã, lấy cảm hứng từ các phương pháp barycentric [8] và giống Prony [9].
• **Thước đo xấp xỉ**: để đảm bảo tương thích với bất kỳ tác vụ downstream nào, chúng tôi chọn các thước đo khác biệt trên bộ lọc convolution, thay vì các đầu ra mô hình.

Trong các khối lượng công việc tự hồi quy, các mô hình chưng cất LaughingHyena với chiều trạng thái d có thể tạo ra K token trong thời gian O(dK) và với bộ nhớ không đổi O(d) – cải thiện so với việc sử dụng thời gian O(K²) và bộ nhớ O(K) của Transformers có kv-cache và các convolution dài được thực hiện một cách ngây thơ. Ở kích thước mô hình trên một tỷ tham số, LaughingHyena đạt được thông lượng đỉnh cao hơn 10× so với Transformers tương đương (Hình 1.1), và có thể xử lý các kích thước lô lớn hơn. Việc tạo ra bộ nhớ không đổi cho phép K lớn hơn cho một ràng buộc bộ nhớ nhất định, ví dụ tạo ra 512 token với LaughingHyena yêu cầu ít bộ nhớ hơn 3× so với Transformer. Ở kích thước lô nhỏ hơn, độ trễ của LaughingHyena cũng cạnh tranh với Transformers, đạt được tăng tốc ≥2× ở độ dài prompt dài hơn.

**Cải thiện chất lượng huấn luyện trước** Chúng tôi tận dụng phân tích của chúng tôi về quá trình chưng cất để mở ra các hướng cải tiến mới cho kiến trúc LCSM. Thực tế, tỷ lệ nén cao có thể đạt được thông qua LaughingHyena gợi ý về việc sử dụng dưới mức của convolution. Chúng tôi xem xét lại thiết kế đa đầu của H3[1]; việc gắn kết trọng số qua các kênh đẩy các bộ lọc convolution dài hướng đến chiều hiệu quả lớn hơn, và như một lợi thế bổ sung giảm thời gian chạy của chưng cất sau huấn luyện và lượng bộ nhớ suy luận. Hơn nữa, các mô hình Hyena đa đầu cải thiện về độ phức tạp huấn luyện trước so với Hyena thông thường và kiến trúc GPT [10] trên tập dữ liệu ngôn ngữ The Pile [11].

## 2 Kiến thức cơ bản và Công việc liên quan

Chúng tôi thảo luận về convolution, không gian trạng thái và các khối lượng công việc tạo ra tự hồi quy cho các mô hình chuỗi.

**Convolution** Gọi ∗ ký hiệu toán tử convolution. Nó được định nghĩa là phép toán đối ngẫu với phép nhân điểm dưới biến đổi Fourier. Trong xử lý tín hiệu và học sâu, người ta thường gặp convolution tuyến tính nhân quả của một bộ lọc h (có thể mở rộng vô hạn) với một đầu vào u có độ dài L:

(h∗u)t = ∑(j=0 to t) h(t-j)u_j. (2.1)

Nói chung, u_t ∈ R^D trong đó D là chiều rộng của tín hiệu – hoặc trong thuật ngữ học sâu – số lượng kênh. Không mất tính tổng quát, chúng tôi chuyên biệt hóa phân tích của chúng tôi cho các lớp đầu vào đơn đầu ra đơn, tức là với D = 1. Đối với các quan hệ đầu vào-đầu ra loại (2.1), chúng tôi sử dụng các thuật ngữ lớp convolution và hệ thống tuyến tính có thể thay thế cho nhau. Tương tự, hàm t ↦ h_t được gọi là cả bộ lọc và đáp ứng xung của một hệ thống tuyến tính. Các mô hình chuỗi convolution hiện có có thể được phân loại theo tham số hóa được sử dụng cho các bộ lọc của chúng. Lớp convolution ngầm định đại diện cho bộ lọc như một hàm tham số γ_θ: t ↦ h_t.

**Thực hiện không gian trạng thái** Một tùy chọn là chọn γ_θ như hàm đáp ứng xung của một hệ thống tuyến tính bất biến thời gian rời rạc,

x_(t+1) = Ax_t + Bu_t
y_t = Cx_t + h_0u_t, t ↦ h_t = {h_0 if t=0, CA^(t-1)B if t>0} (2.2)

với trạng thái x_t ∈ R^d, đầu vào u_t ∈ R, và đầu ra y_t ∈ R. Các ma trận A ∈ R^(d×d), B ∈ R^(d×1), C ∈ R^(1×d), và h_0 ∈ R là các tham số có thể học được của mô hình trong khi trạng thái ban đầu x_0 thường được đặt bằng không sao cho u ↦ y là một convolution thuần túy. Mặc dù các hệ thống tuyến tính (2.2) là nền tảng của xử lý tín hiệu và lý thuyết điều khiển, việc sử dụng chúng như tham số hóa ngầm định của các bộ lọc convolution trong mạng neural sâu chỉ mới xuất hiện gần đây [12, 6]. Các tham số hóa khác [13, 14, 2] chọn γ_θ(t) như các hương vị khác nhau của mạng neural đại diện ngầm định [15, 16]. Những cái sau nói chung mạnh mẽ hơn về mặt lớp các bộ lọc mà chúng có thể đại diện và tính linh hoạt trong quá trình huấn luyện, với chi phí mất đi chiều trạng thái cố định.

### 2.1 Mô hình Chuỗi Convolution Dài

Họ mô hình chuỗi convolution H – H3[1] và Hyena[2] – dựa vào sự kết hợp của convolution dài và gating được điều khiển bởi dữ liệu để thay thế attention với quy mô dưới bậc hai về độ dài chuỗi. Chúng tôi sử dụng quy ước học sâu đặt tên các phép chiếu khác nhau là query q, key k và value v. Gọi M_q và M_k là các ma trận đường chéo L-by-L có các phần tử đường chéo chính tương ứng là các phần tử tương ứng của các chuỗi độ dài L q và k. Một H-block thực hiện một ma trận attention thay thế với một phân rã có tham số, được điều khiển bởi dữ liệu thành ba số hạng:

(q, k, v) ↦ H(q, k)v, H(q, k) = M_q T_h M_k (2.3)

trong đó T_h ∈ R^(L×L) là ma trận Toeplitz được xây dựng từ bộ lọc convolution dài có thể học h, tức là T_h = (h_(i-j))^(L-1)_(i,j=0). Các phép chiếu qkv bản thân chúng là đầu ra của một convolution giữa chuỗi đầu vào và ba bộ lọc ngắn riêng biệt. Các bậc tự do trong thiết kế H-block là ba bộ lọc ngắn và bộ lọc dài h. Bộ lọc dài có thể được tham số hóa sử dụng một đại diện neural ngầm định [2], mô hình không gian trạng thái [1], hoặc giá trị tường minh [17]. Việc phân rã ba lần của toán tử attention cho phép đánh giá (2.3) chỉ trong thời gian Õ(L) := O(L log_2 L) (hai convolution và hai phép nhân từng phần tử), y_t = q_t(h ∗ kv)_t. Toán tử tổng thể tác động lên một đầu vào u bằng cách xây dựng một đa thức đa biến bậc ba của u có các hệ số được điều khiển (phi tuyến) bởi các tham số của block.

### 2.2 Tạo ra Tự hồi quy

Một khối lượng công việc điển hình cho các mô hình chuỗi là tạo ra tự hồi quy. Cho một prompt độ dài T u ∈ R^T, mô hình được giao nhiệm vụ tạo ra K đầu ra bổ sung tiếp theo – từng cái một – cho một chuỗi đầu ra y có độ dài L = T + K.

**Mô hình chuỗi convolution** Sau khi xử lý prompt ban đầu trong thời gian Õ(T) và thu được đầu ra độ dài T u ↦ y_0, ..., y_(T-1), một lớp convolution chung có thể cache chuỗi đầu ra và tạo ra bất kỳ đầu ra bổ sung nào sử dụng (2.1) một cách tự hồi quy, tức là y_(t+1) = ∑_(j=0)^t h_(t-j)y_j cho t = T-1, ..., T+K-1. Quan trọng là phải lưu ý rằng việc tạo ra tự hồi quy với convolution dài chung là đắt đỏ. Nó đi kèm với chi phí bậc hai trong số K token cần được tạo ra và yêu cầu lưu trữ một cache có độ dài lên đến L.

**Bổ đề 2.1.** Tạo ra K token với một lớp convolution dài (2.1) từ một prompt độ dài T có độ phức tạp thời gian O(T log_2 T + TK + K²) và yêu cầu bộ nhớ O(L).

**Mô hình không gian trạng thái** Khi hệ thống tuyến tính thừa nhận một thực hiện không gian trạng thái (2.2), tức là nó có thể chuyển đổi giữa chế độ convolution và truy hồi, chi phí của việc tạo ra tự hồi quy có thể được giảm mạnh. Lượng bộ nhớ là O(d): tất cả những gì chúng ta cần cache là trạng thái x_t, một vector d-chiều. Với một số máy móc bổ sung mà chúng tôi phát triển trong phần tiếp theo, chúng ta có thể giữ lại thời gian Õ(T) và bộ nhớ O(T) để xử lý prompt và khởi tạo trạng thái x_(T-1). Mỗi bước tạo ra bổ sung chỉ yêu cầu thời gian O(d).

**Bổ đề 2.2.** Tạo ra K token với một mô hình không gian trạng thái (2.2) từ một prompt độ dài T có độ phức tạp thời gian O(T log_2 T + dK) và yêu cầu bộ nhớ O(T + d).

Lưu ý rằng các bộ lọc dài h bị cắt ngắn đến độ dài d (tức là h_t = 0 cho t > d-1) cũng có thể được giải thích như SSMs d-chiều (xem Phụ lục A.7) trong đó trạng thái (một cache) trùng với d đầu vào cuối cùng.

**Transformers** Self-attention chắc chắn ít hiệu quả hơn so với convolution dài trong việc xử lý prompt, đi kèm với độ phức tạp thời gian O(T²) nặng nề. Tuy nhiên, Transformers có thể đạt được hiệu quả tương tự trong việc tạo ra tự hồi quy bằng cách cache các chuỗi key {k_t} và value {v_t} quá khứ. Cụ thể, từ t = T-1 trở đi, các phép chiếu mới (q_(t+1), k_(t+1), v_(t+1)) được đánh giá từ đầu ra hiện tại y_t, và đầu ra mới y_(t+1) có thể được tính toán trong thời gian tuyến tính với hai phép rút gọn.

**Bổ đề 2.3.** Tạo ra K token với self-attention từ một prompt độ dài T có độ phức tạp thời gian O(T² + TK + K²) và yêu cầu bộ nhớ O(L).

## 3 Nhà máy chưng cất Laughing Hyena

Trong phần này, chúng tôi giới thiệu phương pháp chưng cất của chúng tôi. Chúng tôi thảo luận về việc chọn một mục tiêu xấp xỉ, một tham số hóa cho chất xấp xỉ và thiết lập chiều trạng thái mục tiêu.

Cho bất kỳ LCSM đã được huấn luyện trước nào, mục tiêu của thủ tục chưng cất là chuyển đổi mỗi bộ lọc convolution đã được huấn luyện trước thành một mô hình không gian trạng thái riêng biệt (2.2). Điều này nên được thực hiện với chiều trạng thái d nhỏ nhất có thể bảo tồn, đến một dung sai nhất định, các đặc tính đầu vào-đầu ra của lớp convolution. Chính thức, cho một bộ lọc h, bài toán chưng cất được định nghĩa như sau.

Cho chuỗi h_1, ..., h_L, tìm một mô hình không gian trạng thái (2.2) có chiều d ≪ L, có hành vi đầu vào-đầu ra xấp xỉ với convolution với h trên lớp lớn nhất của các chuỗi đầu vào.

Việc chọn các thước đo xấp xỉ và giả định về các chuỗi đầu vào tạo ra các mục tiêu chưng cất khác nhau. Một thuật toán chưng cất tạo thành một thủ tục hệ thống để tối ưu chọn các ma trận hệ thống đối với một mục tiêu cụ thể. Trong các trường hợp mà bộ lọc gốc h bản thân nó là đáp ứng xung của một mô hình không gian trạng thái chiều hữu hạn, ví dụ khi cố gắng chưng cất các bộ lọc H3 hoặc S4[6], thuật ngữ chưng cất trở nên tương tự với giảm bậc mô hình. Do đó, trong những trường hợp như vậy, thuật toán chưng cất nên mang lại một đại diện không gian trạng thái của chiều trạng thái bậc thấp hơn.

Tồn tại nhiều giải pháp đại số cho bài toán giảm mô hình [18, 19, 20], thường tìm kiếm các cấu trúc rank thấp của không gian trạng thái bằng cách kiểm tra một số bất biến của hệ thống, ví dụ các Gramian trong cắt ngắn cân bằng [19, Ch. 7]. Hệ thống bậc thấp sau đó được thu được như một phép chiếu của động lực học hệ thống lên không gian con được tìm thấy nơi hệ thống giữ lại các đặc tính mong muốn, ví dụ hành vi đầu vào-đầu ra, tính ổn định, v.v.

**Bộ lọc bị cắt ngắn** Về lý thuyết, các bộ lọc convolution được tham số hóa ngầm định có thể đại diện cho các tín hiệu dài tùy ý. Trong thực tế, các bộ lọc này được huấn luyện trên độ dài tối đa cố định L. Tại thời điểm suy luận, mô hình sau đó có thể được đánh giá cho các chuỗi dài hơn L. Trong quá trình chưng cất, tuy nhiên, việc coi các bộ lọc đã được huấn luyện trước như các hàm đáp ứng xung hữu hạn có thể rất dài (thậm chí vượt quá L) nhưng hữu hạn là hợp lý [21, 22, 23, 24]. Chúng tôi chỉ ra cách lựa chọn này được hỗ trợ bởi bằng chứng thực nghiệm cho thấy các bộ lọc đã được huấn luyện trước thường phân rã về không trong thời gian hữu hạn (xem Phụ lục D).

**Đại diện hàm truyền** Một mô tả thay thế của hệ thống (2.2) là hàm truyền H của nó, được định nghĩa là z-transform của đáp ứng xung H(z) = ∑_(t=0)^∞ h_t z^(-t) cho tất cả z ∈ C nơi tổng hội tụ. Hàm truyền là một hàm hữu tỉ thích hợp của z

H(z) = h_0 + C(zI - A)^(-1)B = (h_0 + b_1z^(-1) + ... + b_d z^(-d))/(1 + a_1z^(-1) + ... + a_d z^(-d)). (3.1)

Trong miền z, hàm truyền định nghĩa ánh xạ đầu vào-đầu ra là Y(z) = H(z)U(z). Ở đây, H(z) được định nghĩa bên ngoài vòng tròn C-plane có bán kính ρ(A), D_ρ(A) := {z ∈ C : |z| > ρ(A)} trong đó ρ(A) là bán kính phổ của A, tức là biên độ của giá trị riêng lớn nhất của nó. Chúng ta có thể khôi phục tất cả các đặc tính của một hệ thống cho trước tương đương từ hàm truyền hoặc đại diện không gian trạng thái của nó (xem Phụ lục A.3 để biết thêm chi tiết và dẫn xuất). Đáng chú ý, hàm truyền là một bất biến của hệ thống: nếu chúng ta áp dụng một thay đổi biến cho trạng thái, hàm truyền vẫn không thay đổi (Bổ đề A.3). Điều này một mình nên ngăn cản các nỗ lực mô hình hóa bộ lọc bằng cách học các ma trận không gian trạng thái dày đặc A, B, C như vậy: có vô số thực hiện không gian trạng thái tương đương ánh xạ đến cùng một hệ thống. Bắt đầu từ các hệ số (a_i) và (b_i) của hàm truyền hữu tỉ (3.1), chúng ta có thể tính toán đáp ứng xung trong thời gian Õ(L) (Bổ đề A.6). Hơn nữa, chúng ta có thể ánh xạ ngược hàm truyền đến một thực hiện không gian trạng thái đặc biệt – dạng canonical companion – có truy hồi với độ phức tạp thời gian O(d) (Bổ đề A.7), so với O(d²) của các ma trận không gian trạng thái dày đặc. Từ Bổ đề A.3 và A.7, chúng ta cũng có thể chứng minh rằng bất kỳ mô hình không gian trạng thái ổn định nào có thể được chuyển đổi bằng cách chuẩn hóa thành dạng companion của nó, và do đó có thể được trang bị một truy hồi hiệu quả (Định lý A.8).

Bài toán chưng cất đặt ra nhiều thách thức:

1. **Định nghĩa mục tiêu chưng cất.** Một quyết định chính liên quan đến việc chọn một mục tiêu chưng cất. Chúng tôi chủ yếu quan tâm đến các thước đo khác biệt thuần túy giữa mỗi bộ lọc của một mô hình sâu đã được huấn luyện trước và chất xấp xỉ của nó, thay vì tổn thất đầu vào-đầu ra mong đợi trên một phân bố của các đầu vào.

2. **Chọn tham số hóa không gian trạng thái.** Việc xác định một tham số hóa phù hợp của thực hiện không gian trạng thái chưng cất là rất quan trọng. Một khi điều này được quyết định, nhiệm vụ là xác định các tham số tối thiểu hóa các desiderata chưng cất, có thể liên quan đến các bài toán tối ưu thách thức.

3. **Chọn chiều trạng thái mục tiêu.** Cuối cùng, một thách thức là ước lượng mức độ mà bậc của mô hình có thể được giảm. Nói cách khác, chúng ta phải chọn chiều trạng thái mục tiêu của quá trình chưng cất để xác định sự cân bằng phù hợp giữa hiệu quả và độ chính xác.

Trong phần tiếp theo, chúng tôi giải quyết từng thách thức này, và cung cấp một phương pháp toàn diện (được tóm tắt trong Hình 3.1) để chưng cất truy hồi từ các kiến trúc dựa trên convolution.

### 3.1 Mục tiêu Chưng cất Không cần Dữ liệu

Chúng tôi tập trung vào các mục tiêu chưng cất độc lập với dữ liệu huấn luyện và kiến trúc tổng thể của mạng neural đang xem xét. Tổn thất chưng cất nên được chọn như một thước đo khác biệt thuần túy giữa mỗi bộ lọc convolution h_t của mô hình và các xấp xỉ chiều hữu hạn của chúng ĥ_t = CA^(t-1)B. Phương pháp này đảm bảo rằng chúng ta không yêu cầu một lần chạy suy luận tuần tự đầy đủ trên mô hình đã được huấn luyện trước ở mỗi bước của thủ tục chưng cất và mô hình chưng cất có thể được áp dụng rộng rãi hơn cho các tác vụ downstream. Lựa chọn này được hỗ trợ bởi bất đẳng thức convolution Young [25, 26], chỉ ra rằng lỗi xấp xỉ đầu ra có một cận ∥y - ŷ∥_r ≤ ∥h - ĥ∥_q ∥u∥_p cho các chuẩn được chọn phù hợp. Để có độ ổn định số tối đa và tự do tham số hóa cho các chất xấp xỉ, chúng tôi ưa thích các phương pháp gradient hiện đại không ràng buộc để giải quyết chương trình chưng cất kết quả. Chúng tôi thiết kế các thuật toán chưng cất khớp các bộ lọc trong miền thời gian tối thiểu hóa lỗi ℓ_2 (∥h∥_2 := [∑_(t∈Z) |h_t|²]^(1/2)) hoặc khớp các hàm truyền của chúng một cách tối ưu đối với chuẩn H_2 (∥H∥_2 := [(1/2π) ∫_{-π}^π |H(e^(iω))|² dω]^(1/2)).

Vì việc chưng cất được thực hiện qua các phương pháp gradient, ℓ_2 là một ứng cử viên tự nhiên. Thay vào đó, có thể sử dụng tối thiểu hóa lỗi H_2 để giới hạn đều khác biệt trường hợp xấu nhất là ∥h - ĥ∥_∞ ≤ ∥H - Ĥ∥_2 (xem Phụ lục A.2 để biết thêm chi tiết).

### 3.2 Làm cho Hyena Cười với Nội suy Modal

Các bậc tự do của chúng tôi để giải quyết bài toán chưng cất là các ma trận A, B, và C của thực hiện không gian trạng thái, xác định bộ lọc cho tất cả t > 0. Trong các SSM chưng cất, số hạng passthrough (residual) không thể được gán tự do: nó chỉ đơn giản là h_0, giá trị của bộ lọc gốc tại không. Thay vào đó, với các tính chất bất biến hấp dẫn của nó, chúng ta có thể tham số hóa một hàm hữu tỉ thích hợp Ĥ(z) (3.1) và khớp nó với hàm truyền (bị cắt ngắn) của bộ lọc gốc H_L(z) := ∑_(t=0)^L h_t z^(-t) (xem Phụ lục B.2).

**Dạng canonical modal** Tối ưu hóa hàm truyền đầy đủ có thể là thách thức về mặt số học vì nhiều lý do, ví dụ đảm bảo tính ổn định, và tính không xác định cho các đa thức bậc cao. Một giải pháp tự nhiên, lấy cảm hứng từ các phương pháp barycentric đối với xấp xỉ hàm hữu tỉ [29, 8], là giả định d gốc riêng biệt λ_n trong đa thức của mẫu số, λ_n ∈ roots(poly(a)).

**Mệnh đề 3.1** ([5]). Nếu poly(a) có các gốc riêng biệt {λ_n ∈ C}, thì hàm truyền của hệ thống có thể được phân rã như Ĥ(z) = ∑_(n=1)^d R_n/(z - λ_n), ∀z ∈ D_ρ(A) trong đó {R_n ∈ C} là residue liên kết với cực λ_n.

Tính toán biến đổi nghịch đảo của hàm truyền mở rộng qua, ví dụ định lý residue Cauchy [30], cho thấy rằng đáp ứng xung kết quả ĥ tương ứng với một cơ sở bị cắt ngắn của các sinusoid phức phân rã theo hàm mũ

ĥ_t = ∑_(n=1)^d R_n λ_n^(t-1), R_n, λ_n ∈ C, t > 0. (3.2)

Trong thực tế, điều này tương ứng với đáp ứng xung của mô hình không gian trạng thái với ma trận đường chéo A = diag(λ_1, ..., λ_d) và sao cho B_i C_i = R_i cho tất cả i = 1, ..., d. Bài toán chưng cất sau đó có thể được định nghĩa theo lỗi nội suy bình phương nhỏ nhất phi tuyến L-điểm (bình phương ℓ_2) giữa h_1, ..., h_L và (3.2) được đánh giá cho t = 1, ..., L: min_{λ_n,R_n} ∥ĥ - h∥_2². Lưu ý rằng trong trường hợp bộ lọc mục tiêu h có giá trị thực, mục tiêu có thể được thay thế bằng ∥ℜ[ĥ] - h∥_2².

Mặc dù chúng tôi tìm các giải pháp của bài toán chưng cất (nội suy) qua các kỹ thuật tối ưu hóa gradient hiện đại, đáng chú ý là Prony đã chỉ ra cách giải pháp bình phương nhỏ nhất phi tuyến có thể được tính toán giải quyết hai bài toán tuyến tính [9]. Tuy nhiên, tương tự như phương pháp Padé để xấp xỉ hữu tỉ [31], các kỹ thuật này có thể không ổn định về mặt số học. Chúng tôi chọn một tham số hóa tương tự như [32, 33] trong đó mỗi giá trị riêng được tham số hóa ở dạng cực λ_n := A_n e^(iθ_n) và các residue ở dạng cartesian. Lưu ý rằng, với tham số hóa này, chúng ta có ℜ[ĥ_t] = ∑_n A_n^(t-1) [ℜ(R_n) cos(θ_n(t-1)) - ℑ(R_n) sin(θ_n(t-1))].

Chúng ta cũng có thể giải quyết bài toán chưng cất theo nghĩa H_2 bằng cách đánh giá ĥ_t và h_t tại t = 0, ..., L-1 và lấy biến đổi Fourier (rời rạc) tương ứng của chúng trước khi tính toán mục tiêu. Việc đánh giá hiệu quả (3.2) rất quan trọng cho việc chưng cất. Đặc biệt, chúng tôi chỉ ra điều sau:

**Bổ đề 3.1.** Đánh giá (ĥ_t)_{t=0}^{L-1} (3.2) có thể được thực hiện trong thời gian O(dL) từ dạng modal của nó và trong thời gian Õ(L) từ dạng hữu tỉ thích hợp của nó.

### 3.3 Bậc Chưng cất Tối thiểu

Chưng cất thành các hệ thống chiều thấp hơn luôn mong muốn vì chúng yêu cầu ít tham số hơn để được tối ưu hóa và chúng mang lại các truy hồi hiệu quả hơn (tuyến tính) về độ phức tạp thời gian và bộ nhớ trong các khối lượng công việc suy luận tự hồi quy sau chưng cất. Chiều của mô hình không gian trạng thái nhỏ nhất có thể với đáp ứng xung chính xác {h_t}_{t∈N} là cái gọi là bậc McMillan [34]:

d* = arg min_d {d : ∃A ∈ C^(d×d), B ∈ C^(d×1), C ∈ C^(1×d) với h_t = CA^(t-1)B, ∀t > 0} (3.3)

**Định lý 3.1** (Ho-Kalman [35, Định lý 2, Hệ quả]). Gọi S là ma trận Hankel (vô hạn) được xây dựng với h, tức là S := (h_{i+j})_{i,j=1}^∞. Khi đó, d* = rank(S).

Một cận dưới cho d* có thể được ước lượng từ một bộ lọc bị cắt ngắn có độ dài L bằng cách xây dựng ma trận con chính L×L S_L và sử dụng thực tế rằng rank(S) ≥ rank(S_L). Kiểm tra tốc độ phân rã các giá trị đơn Hankel (σ_n)_{n=1}^L trong các mô hình convolution đã được huấn luyện trước có thể dự đoán chất lượng xấp xỉ ở một chiều cố định. Như một quy tắc ngón tay cái, d cần đủ lớn để σ_{d+1} đủ nhỏ. Cụ thể, chúng ta có thể chứng minh rằng giá trị đơn cuối cùng σ_d xác định cận trên của chất lượng chưng cất với một SSM có chiều d, theo chuẩn Hankel [19]. Đây là hệ quả trực tiếp của định lý Adamjan-Arov-Krein [7] và có thể được phát biểu không chính thức như sau.

**Định lý 3.2** (Không chính thức). Gọi h là một bộ lọc độ dài L, ĥ là bộ lọc chưng cất bậc d < L và gọi S_L, Ŝ_L là các ma trận Hankel tương ứng. Khi đó inf_{Ŝ_L} ∥S_L - Ŝ_L∥_2 = σ_d.

### 3.4 Triển khai Truy hồi

Một khi tất cả các bộ lọc của một mô hình đã được huấn luyện trước đã được chưng cất với kỹ thuật nội suy modal được đề xuất ở trên, mô hình mở khóa một chế độ truy hồi phân bổ một trạng thái x_t ∈ C^d cho mỗi bộ lọc và cho phép suy luận tự hồi quy nhanh. Triển khai mô hình chưng cất liên quan đến hai bước quan trọng: pre-filling và chính quy tắc cập nhật truy hồi.

**Pre-filling nhanh** Trong quá trình tạo ra tự hồi quy, khi một prompt độ dài T được đưa vào mô hình, chúng ta cần tính toán trạng thái x_T để bắt đầu tạo ra các token mới. Sử dụng truy hồi, độ phức tạp thời gian của việc khởi tạo x_T sẽ là O(dT) với lượng bộ nhớ O(d). Người ta có thể phân bổ tính toán trên d bộ xử lý với một phép toán quét song song [37, 38] để đạt được độ phức tạp thời gian song song O(d log_2 T) trong khi phát sinh yêu cầu bộ nhớ tăng lên O(dT). Một tùy chọn thứ ba là sử dụng một convolution FFT đơn để thu được x_T trong thời gian Õ(T) và bộ nhớ O(T).

**Mệnh đề 3.2.** x_T = (ν_T, ..., ν_{T-d}) trong đó ν_t = (g * u)_t và g là bộ lọc có hàm truyền là 1/den(Ĥ)(z) và có thể được đánh giá trong thời gian Õ(T).

Lưu ý rằng, thuật toán pre-filling nhanh được thiết lập bởi kết quả này yêu cầu đánh giá đa thức mẫu số của Ĥ từ các gốc của nó trước khi triển khai. Điều này tương đương với việc chuyển đổi hàm truyền từ đại diện phân rã của nó sang dạng hữu tỉ (3.1).

**Bước truy hồi** Quy tắc cập nhật là đường chéo, do đó được đánh giá hiệu quả trong thời gian O(d) và bộ nhớ:

**Mệnh đề 3.3.** Bộ lọc (3.2) có các ma trận không gian trạng thái A = diag(λ_1, ..., λ_d) ∈ C^(d×d), B = (1, ..., 1)^T ∈ C^(d×1), C = (R_1, ..., R_d) ∈ C^(1×d) có bước có thể được đánh giá trong thời gian O(d) và bộ nhớ.

Vì chúng ta thường muốn đầu ra y_t có giá trị thực, chúng ta có thể đơn giản cập nhật trạng thái phức x_{t+1} = Ax_t + Bu_t và sau đó lấy phần thực của đầu ra, y_t = ℜ[Cx_t] + h_0u_t.

## 4 Convolution Dài Đa đầu

Chúng tôi có thể tận dụng phân tích phổ Hankel được thảo luận trong Phần 3.3 để nghiên cứu động lực học của chiều hiệu quả của mỗi bộ lọc convolution trong quá trình huấn luyện trước LCSMs. Chúng tôi thấy rằng, tại khởi tạo, các bộ lọc tương ứng với SSMs chiều cao, và dần dần hội tụ đến các đại diện chiều thấp hơn trong quá trình huấn luyện. Xem Phụ lục E.2 để biết các ví dụ về các mô hình Hyena và H3.

Quan sát này dẫn đến câu hỏi: liệu việc thực hiện convolution dài độc lập trên mỗi kênh có lợi thế, hay chúng ta có thể giảm tổng số bộ lọc mà không mất chất lượng? Để trả lời điều này, chúng tôi điều chỉnh thiết kế lớp đa đầu được đề xuất bởi H3[1] cho Hyena[2]:

1. Cho các phép chiếu q, k, v ∈ R^(L×D), chúng ta chia chúng thành M khối có kích thước N = D/M, q_m, k_m, v_m ∈ R^(L×N).
2. Mỗi khối được xử lý bởi một toán tử Hyena được sửa đổi: đầu tiên, chúng ta thực hiện tích ngoài của k_m và v_m dọc theo chiều không gian, z_m := k_m ⊗ v_m ∈ R^(L×N×N), áp dụng một convolution dài với bộ lọc h_m cho tất cả N×N phần tử độc lập, sau đó tính toán y_m_t = (h_m * z_m)_t q_m_t, y_m ∈ R^(L×N) như được thể hiện trong Hình 4.
3. Cuối cùng, chúng ta tổng hợp y_1, ..., y_m thành một đầu ra đơn y ∈ R^(L×D) qua nối tiếp.

Quan trọng, cấu trúc đa đầu của MultiHyena cho phép chúng tôi chứng minh quy mô thuận lợi trong tác vụ tổng hợp liên kết, được chỉ ra trong [2] là dự đoán hiệu suất ở quy mô. Trong liên kết tổng hợp, mô hình được cung cấp một chuỗi các cặp key-value và một truy vấn, và được giao nhiệm vụ khớp truy vấn với một key trong chuỗi bằng cách trả về giá trị liên kết của nó. Độ khó của tác vụ tăng với kích thước từ vựng: từ vựng lớn hơn cần các mô hình rộng hơn.

**Định lý 4.1.** Lớp MultiHyena, với O(log s) đầu và kích thước mô hình O(√s log s) có thể giải quyết bài toán liên kết tổng hợp, trong đó s ký hiệu kích thước từ vựng.

Trong Phụ lục E.1, chúng tôi xác minh thực nghiệm cải thiện quy mô theo kích thước từ vựng với nhiều đầu.

## 5 Thí nghiệm

• **Huấn luyện trước**: Chúng tôi huấn luyện trước một bộ các mô hình ngôn ngữ MultiHyena trên The Pile [11], điều tra quy mô của perplexity với các lượng token tổng khác nhau (5, 10, 15 tỷ), cũng như các lần chạy huấn luyện lớn hơn cho 300 tỷ token. MultiHyena vượt trội hơn Transformers và Hyena.

• **Phân tích chưng cất**: Chúng tôi điều tra mối quan hệ giữa các bậc chưng cất tối ưu, phổ Hankel, và lỗi trên logit của các mô hình chưng cất.

• **Downstream sau chưng cất**: Chúng tôi đánh giá tác động downstream của việc chưng cất các mô hình ngôn ngữ convolution dài, báo cáo kết quả HELM [41] và LM-Eval-Harness [42].

• **Benchmarking**: Chúng tôi benchmark độ trễ, thông lượng và bộ nhớ dọc theo các trục khác nhau của kích thước lô, độ dài chuỗi, số token được tạo ra. Chúng tôi bao gồm các mô hình cơ sở, mô hình chưng cất và Transformers tương đương.

### 5.1 Huấn luyện trước

Để xác thực công thức đa đầu, chúng tôi huấn luyện các mô hình MultiHyena 150 và 350 triệu tham số trên The Pile [11] sử dụng 8 đầu và nếu không thì sử dụng cùng kiến trúc như các mô hình Hyena tương đương, theo thiết lập của [2]. Qua cấu trúc đa đầu được giới thiệu trong 4, MultiHyena vượt trội hơn cả Hyena và Transformers, bao gồm trên các lần chạy quy mô dữ liệu với số lượng token tăng và các lần chạy 300B token đầy đủ (Bảng 5.1).

### 5.2 Phân tích Chưng cất

Tiếp theo, chúng tôi xác minh liệu các giá trị đơn Hankel có dự đoán các lỗi downstream, và liệu các mô hình lớn có thể được chưng cất mà không mất chất lượng. Chúng tôi áp dụng chưng cất LaughingHyena cho MultiHyena, Hyena và H3 đã được huấn luyện trước với các kích thước khác nhau. Cụ thể, đối với mỗi lớp và kênh của một mô hình, chúng tôi tham số hóa các cực {λ_n} của các dạng canonical modal (Phần 3.2) ở các bậc d khác nhau, và giải quyết cho mỗi bài toán xấp xỉ ℓ_2.

**Lỗi xấp xỉ và phổ** Chúng tôi điều tra độ lớn của các lỗi xấp xỉ được giới thiệu bởi chưng cất LaughingHyena. Cho một mô hình MultiHyena đã được huấn luyện trước, chúng tôi tính toán các lỗi giữa các bộ lọc gốc và chưng cất ở mỗi lớp, được trung bình qua các kênh. Chúng tôi lặp lại quá trình này cho các bậc chưng cất khác nhau (chiều trạng thái của dạng mô hình của Phần 3.2). Hình 5.2 hình dung các lỗi tối thiểu, tối đa và trung bình, lỗi theo lớp và phân bố của các giá trị đơn của toán tử Hankel liên kết với mỗi bộ lọc. Chúng tôi quan sát các bậc chưng cất (>16) mang lại lỗi nhỏ được dự đoán bởi phân bố của các giá trị đơn. Do đó, việc phân tích phổ của toán tử Hankel được xác minh là một phương pháp hiệu quả để chỉ đạo ước lượng của bậc chưng cất tối ưu. Chúng tôi cũng lưu ý rằng bậc tối ưu thay đổi qua các lớp, mang lại các tùy chọn để tối ưu hóa thêm.

**Lỗi đầu ra** Tiếp theo, chúng tôi tính toán lỗi ℓ_1 tương đối giữa logit đầu ra của các mô hình đã được huấn luyện trước và chưng cất để đảm bảo LaughingHyena có thể được sử dụng trong các khối lượng công việc tạo ra. Bậc chưng cất tối thiểu tối ưu được ước lượng qua các toán tử Hankel (16) là đủ để giữ cho phân bố đầu ra trên từ vựng (>50k mục) gần với mô hình đã được huấn luyện trước, như được thể hiện trong Hình 5.2. Kiểm tra hồ sơ lỗi trên logit được sắp xếp theo độ lớn cho thấy phương pháp của chúng tôi mạnh mẽ với các chiến lược lấy mẫu khác nhau cho việc tạo ra, bao gồm greedy decoding, top-k, top-p[43]. Thực tế, các lỗi tương đối là <10^(-2) lên đến và bao gồm phần trăm thứ 99.99% của phân bố, có nghĩa là ví dụ một chiến lược lấy mẫu top-p với p lớn có thể được sử dụng trên một mô hình chưng cất mà không có sự trôi dạt trong các đầu ra (token bị phân loại sai). Chúng tôi lưu ý rằng các lỗi tương đối là tối đa trên logit chuẩn nhỏ, không được yêu cầu bởi hầu hết các chiến lược lấy mẫu.

Trong Phụ lục D.2, chúng tôi cung cấp phân tích lỗi chưng cất tương tự cho các mô hình Hyena và H3. Chúng tôi thấy rằng Hyena có thể được chưng cất với ít hơn 32 bậc và H3 với ít hơn 8.

### 5.3 Đánh giá Downstream

Chúng tôi kiểm tra cách chưng cất ảnh hưởng đến hiệu suất downstream trên các benchmark ngôn ngữ. Chúng tôi áp dụng chưng cất bậc 8, 16 và 32 cho mô hình ngôn ngữ MultiHyena đã được huấn luyện trước trên The Pile và benchmark (Bảng 5.3) hiệu suất của nó trên một bộ các tác vụ chuẩn (zero shot) từ LM-Eval-Harness [42] và HELM [41]. Các kết quả phù hợp với phân tích lỗi của chúng tôi: các bậc chưng cất bằng hoặc lớn hơn 16 giới thiệu ít hoặc không có sự suy giảm chất lượng.

### 5.4 Benchmarking

Chúng tôi đo thông lượng, độ trễ và sử dụng bộ nhớ của LaughingHyena cho các khối lượng công việc tạo ra tự hồi quy, với độ dài prompt ban đầu T và số token được tạo ra K. Thông lượng được tính như số token được tạo ra trên độ trễ. Đối với mỗi thiết lập (và các benchmark bổ sung), chúng tôi cung cấp chi tiết trong Phụ lục D.4.

**Thông lượng đỉnh** Các mô hình chưng cất không cần kv-cache. Điều này giảm yêu cầu bộ nhớ trong quá trình tạo ra, cho phép thông lượng đỉnh cao hơn trong các khối lượng công việc lô lớn. Chúng tôi đạt được thông lượng cao hơn 10× so với Transformers ở kích thước 1.3 tỷ tham số (Hình 1.1). Thông lượng cao hơn Transformers ngay cả ở kích thước lô cố định, cho thấy độ trễ thấp hơn.

**Chiều trạng thái SSM và thông lượng** Đối với các bậc chưng cất điển hình (<100), thông lượng đỉnh không bị ảnh hưởng nhiều. Chúng tôi đo giảm 2% thông lượng từ 32 đến 64.

**Độ dài prompt** Thông lượng của các mô hình chưng cất LaughingHyena lớn hơn 4× so với Transformers ở kích thước lô cố định 64 và độ dài prompt 1536 (Hình 5.3). Khi độ dài prompt tăng, khoảng cách thời gian chạy giữa pre-filling qua convolution trong LCSMs và pre-filling trong Transformers mở rộng (ví dụ Õ(T) như được chi tiết trong Phần 3.4, so với O(T²)).

**Lượng bộ nhớ** Các mô hình truy hồi không yêu cầu kv-cache và sử dụng bộ nhớ không đổi để tạo ra một số lượng token tùy ý (Hình 5.4).

## 6 Kết luận

Chúng tôi nghiên cứu hiệu quả và chất lượng của các mô hình chuỗi convolution dài tối ưu. Đầu tiên, chúng tôi giới thiệu LaughingHyena, một phương pháp chưng cất mới lấy cảm hứng từ các kỹ thuật xấp xỉ hàm hữu tỉ và giảm bậc mô hình. LaughingHyena có thể được áp dụng sau huấn luyện để trích xuất các mô hình không gian trạng thái compact từ mỗi bộ lọc convolution, mà không mất chất lượng. Các mô hình chưng cất đạt được thông lượng cao hơn so với Transformers có kích thước tương đương, và có thể thực hiện tạo ra tự hồi quy trong bộ nhớ không đổi bằng cách tránh nhu cầu cache các đầu ra trước đó. Chúng tôi điều tra lý thuyết và thực nghiệm các sự cân bằng của các chiến lược khác nhau cho suy luận nhanh của các mô hình truy hồi, và giới thiệu các cải tiến kiến trúc cho Hyena cải thiện chất lượng huấn luyện trước.
