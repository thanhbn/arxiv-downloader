# 2309.10713.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2309.10713.pdf
# File size: 1018842 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Interpret Vision Transformers as ConvNets with Dynamic Convolutions
Chong Zhou1, Chen Change Loy1, and Bo Dai2
1S-Lab, Nanyang Technological University
2Shanghai AI Laboratory
Abstract
There has been a debate about the superiority between
vision Transformers and ConvNets, serving as the backbone
of computer vision models. Although they are usually con-
sidered as two completely different architectures, in this pa-
per, we interpret vision Transformers as ConvNets with dy-
namic convolutions, which enables us to characterize exist-
ing Transformers and dynamic ConvNets in a unified frame-
work and compare their design choices side by side. In ad-
dition, our interpretation can also guide the network design
as researchers now can consider vision Transformers from
the design space of ConvNets and vice versa. We demon-
strate such potential through two specific studies. First, we
inspect the role of softmax in vision Transformers as the ac-
tivation function and find it can be replaced by commonly
used ConvNets modules, such as ReLU and Layer Normal-
ization, which results in a faster convergence rate and better
performance. Second, following the design of depth-wise
convolution, we create a corresponding depth-wise vision
Transformer that is more efficient with comparable perfor-
mance. The potential of the proposed unified interpretation
is not limited to the given examples and we hope it can in-
spire the community and give rise to more advanced net-
work architectures.
1. Introduction
Since 2012, when AlexNet [19] excelled in the Ima-
geNet challenge [8], convolutional neural networks (Con-
vNets) start to attract tremendous attention in the field of
artificial intelligence and quickly replace traditional meth-
ods and handcrafted features in many tasks. Representative
milestones, such as VGGNet [28] and ResNet [12], have
served as the de facto backbones for a long time. In re-
cent years, Transformer [31] has emerged as a new type
of neural network beyond ConvNets, gradually becoming
dominant in both natural language processing (NLP) and
computer vision with representative works such as Vision
Figure 1: Interpret self-attention as dynamic convolu-
tions. The left shows a standard self-attention block. On
the right, we re-organize certain modules and interpret the
query -keyandattention -value matrix multiplications as two
1×1dynamic convolutions, respectively. Note that the
two architectures are essentially identical and can be equiv-
alently converted to each other.
Transformer (ViT) [10], Swin Transformer [21], DETR [3],
and SETR [39]. Given such a trend, there are also works
trying to close up the gaps between ConvNets and vision
Transformers, proving these two types of neural networks
are competitive with each other. Successful practices along
this line include ConvNeXt [22] that adopts design choices
from vision Transformers and RepLKNet [9] that replaces
standard convolutional kernels with larger depth-wise ones.
Nevertheless, to date, the community still continuously
debates the superiority between the ConvNet-based mod-
els and the Transfomer-based counterparts. In this paper,
we equivalently convert vision Transformers to ConvNets
and bring these two types of neural networks into a unified
framework.
1arXiv:2309.10713v1  [cs.CV]  19 Sep 2023

--- PAGE 2 ---
This equivalence is established by rewriting self-
attention, the core building block of Transformers, as a Con-
vNet block consisting of static and dynamic convolutions.
Specifically, as shown in Figure 1, we regard the query -key
andattention -value matrix multiplications as two 1×1dy-
namic convolutions, respectively, and consider the scaled
softmax operation as an activation function. In this way, the
corresponding ConvNet block of the self-attention block is
static conv. →dynamic conv. →activation →dynamic
conv.→static conv. . Moreover, this equivalence holds for
not only the vanilla ViT but also its subsequent variants,
such as Swin Transformer [21]. Therefore, as shown in Fig-
ure 3, we select several popular Transformers and dynamic
ConvNets and characterize their design choices in the pro-
posed unified perspective.
Being able to compare design choices, which were pre-
viously considered tied to either Transformers or ConvNets,
side by side, we now can rethink design choices of vision
Transformers in the design space of ConvNets and vice
versa. We believe this can help researchers switch to a
different angle when designing a new network architecture.
And designs that are proven to be successful on ConvNets
or Transformers can be seamlessly transferred to each other
and potentially obtain a similar gain. To demonstrate such
potential with practical examples, in this paper, we further
conduct two empirical studies.
First, we investigate the role of softmax in vision Trans-
formers as the activation function in the position of Con-
vNets. On one hand, as an S-shaped function, modern
ConvNets rarely adopt softmax as the activation function
since studies [19] show that non-S-shaped functions, such
as ReLU [24], possess significant advantages. On the
other hand, apart from being S-shaped and providing non-
linearity, softmax also delivers normalization and channel-
wise communication to its input features. Therefore, we
examine which role of softmax is more important in Trans-
formers and whether it can be replaced by techniques that
are dedicated to a single role and commonly used in Con-
vNets through ablation studies. Experiments show that non-
linearity and normalization play more important parts but
can be substituted by techniques other than softmax. For
instance, by replacing softmax with LayerNorm [2] and
ReLU [24], without introducing any computational cost, the
top-1 ImageNet accuracy of Swin-Tiny is improved from
81.13 to 81.56. In addition, being S-shaped is a downside of
softmax as it slows down the convergence rate compared to
non-S-shaped functions, such as ReLU, which aligns with
previous studies [19] on ConvNets.
Second, when summarizing existing Transformers and
dynamic ConvNets in the unified framework, we find
that the mechanism of the dynamic convolutions in self-
attention is quite unique, e.g., kernels generated from one
spatial location will be shared with other locations througha global kernel bank, which is more efficient than generating
a new set of kernels for each spatial location. Meanwhile,
to encourage efficiency, depth-wise convolutions are widely
adopted in ConvNets. Thus, we explore whether combining
these two design choices can result in an even more effi-
cient network. In specific, we keep the kernel bank mecha-
nism of Transformers and replace the two dynamic convo-
lutions with a single depth-wise one. As a result, we obtain
a lightweight vision Transformer with comparable perfor-
mance.
We consider the aforementioned studies as two examples
to conduct exploration in the proposed unified framework of
vision Transformers and ConvNets, but its potential is not
limited to the given examples. In fact, we hope the commu-
nity can be inspired by our interpretation of vision Trans-
formers and bring up more interesting findings.
2. Related Work
Vision Transformers. Transformers [31] are originally
applied in NLP and are first introduced into visual recog-
nition by Vision Transformer (ViT) [10]. Although ViT has
fewer inductive biases compared to ConvNets, which theo-
retically leads to better generalization ability, its quadratic
computational complexity with respect to the input resolu-
tion makes it prohibitive to serve as a general backbone for
computer vision tasks. Moreover, the vanilla ViT usually
takes longer to converge during training. Therefore, efforts
have been made to bring ConvNets modules [35, 36, 25]
or designs, such as feature pyramid [33] and local win-
dows [21], back to ViT. However, such integration still con-
siders ViTs and ConvNets as two heterogeneous architec-
tures, while we bring them into a unified framework.
Dynamic convolutions. Dynamic convolutions refer to
convolutions whose kernel weights are conditioned on the
inputs. Since directly generating the weights of a stan-
dard convolution is expensive, some works opt to gener-
ate weights of depth-wise convolution [11]. Other works
maintain a learnable kernel bank and predict the coefficients
followed by linear combining kernels in the bank with the
predicted coefficients [4]. To further cut down the computa-
tion cost, some works generate a single set of kernels for the
whole feature map [15, 17, 4] instead of one kernel set for
each spatial location [20, 14, 11]. With our interpretation,
we find that vision Transformers not only share common-
ality with existing dynamic convolutions but also contain
unique designs for efficiency ( e.g., implicitly approximate a
dense kernel with a pair of lightweight kernels).
Connections between Transformers and ConvNets.
Tay et al . [29] and Wu et al . [34] empirically compare
dynamic depth-wise ConvNets with Transformers in NLP
tasks. Andreoli [1] classifies convolution into index-based
convolution (standard convolution) and content-based con-
volution whose weights are generated from input, such as
2

--- PAGE 3 ---
the query-key matrix multiplication (attention). In this way,
self-attention functions the same as content-based convo-
lution. Cordonnier et al. [6] further shows that with suf-
ficient heads and each head paying attention to a specific
shift, self-attention with relative positional encoding can
simulate any index-based convolutions. Han et al. [11] in-
terprets the matrix multiplication of attention and value in
local vision Transformers as dynamic depth-wise convolu-
tion. Our work differs from previous works in (1) we make
no assumption on the value of attention or forms of posi-
tional embedding; (2) our conversion is equivalent and bi-
directional; (3) our interpretation is not limited to special
variants of vision Transformers. Therefore, our work makes
a further step towards practical usage and generality.
3. A Unified Perspective
To bring vision Transformers and ConvNets into a uni-
fied framework, we focus on equivalently converting the
self-attention block in Transformers into dynamic convolu-
tions since the rest of the Transformer modules are already
considered as ConvNets. In this section, we start with a
brief introduction of the self-attention block (Section 3.1),
followed by a new perspective to interpret self-attention
(Section 3.2). Finally, we fit several existing Transformers
and ConvNets architectures into the proposed framework
and provide an analysis of their design choices in a unified
perspective (Section 3.3).
3.1. Preliminary: Self-Attention
Before being fed into the first self-attention block of the
vision Transformer, input images are divided into patch to-
kens by a convolutional stem, then these patch tokens are
flattened along the spatial dimension. As a result, the in-
put to self-attention can be denoted as x∈RN×C, where
NandCare the numbers of tokens and embedding dimen-
sions, respectively. Self-attention first projects xto query q,
keyk, and value vwith three individual linear layers:
q=Wqx+bq, k=Wkx+bk, v=Wvx+bv,(1)
where W∗andb∗denote weight and bias. Then q, k, v are
divided along the embedding dimension into Hgroups, and
each group is processed separately before a final concatena-
tion of results from each group. The mechanism is termed
multi-head with each head referring to each group. Here,
we demonstrate the operation for a single head:
oi=softmax (qikT
√Ch)v, (2)
where∗idenotes the i-th token, Chis embedding dimension
of each head, and ois the output.
Figure 2: Kernel kandvoperate in pairs in self-
attention. Each kfirst projects qifrom Cin-d to a scalar.
After the activation function, each scalar is projected by the
corresponding vback to Cout-d. Finally, results from all kv
pairs are element-wise summed. As a result, self-attention
achieves the (Cin→Cout)projection in a very efficient way.
3.2. View Self-Attention as Dynamic Convolutions
Let us revisit Equation 2, which represents the core of
self-attention. From a general view, it can be divided into
two linear matrix multiplications with a non-linear activa-
tion function (scaled-softmax) in between. When looking
more closely, we observe that the output of the i-th token
is a function of the i-th query and all the keys and values.
In other words, keys and values are shared across queries at
different spatial locations. This reminds us of a typical lin-
ear operation whose kernels are also shared across various
spatial locations, convolution.
The kernel of a standard convolution can be defined as
u∈RH×W×Cin×Cout, where H, W, C in, Coutdenote the
height, width, input and output channels of the kernel, re-
spectively. The kernel slides along the spatial dimension of
the input feature map and at each sliding window, the kernel
performs a (HxWxCin→Cout)linear projection.
We find that each of the two matrix multiplications in
self-attention, as shown in Figure 1, essentially operates in
the same way as a 1×1convolution. First, the query -key
multiplication can be viewed as a convolution kernel k∈
R1×1×Cin×Nsliding on the query feature map, where N
denotes the number of tokens. Similarly, the attention -value
multiplication is also a 1×1convolution with kernel v∈
R1×1×N×Coutand takes the attention map as the input.
However, different from the standard convolution, the
weights of kernel kandvare conditioned on the input there-
fore the converted convolutions are dynamic convolutions.
In addition, we observe that kernels kandvoperate in pairs
as shown in Figure 2. Specifically, at each sliding window,
3

--- PAGE 4 ---
Figure 3: Bring existing Transformers and dynamic ConvNets into a unified framework. Here, we show that represen-
tative global, local, and linear Transformers only differ in the way of selecting kernels from the kernel bank. In addition,
since generating the weights of a standard convolutional kernel is expensive, both Transformers and ConvNets propose more
efficient solutions but in quite different ways. Note that, for Transformers, we ignore the drawing of the kernel bank of keys.
Table 1: Characterize existing Transformers and ConvNets in a unified perspective.
Kernel Bank Kernel Selection Kernel Type Kernel Size
ViT [10] Dynamic Hard (all) C→1→C 1×1
Swin Transformer [21] Dynamic Hard (local) C→1→C 1×1
Linformer [32] Dynamic Soft C→1→C 1×1
Dynamic Conv. [4] Static Soft C→C 3×3
Dynamic DW-Conv. [11] N/A N/A Depth-wise 7×7
kperforms Nx(1x1xCin→1)linear projections. The re-
sulting N-dimensional vector then goes through a scaled
softmax activation function. Finally, kernel vprojects each
of the Nscalars in the vector to a Cout-dimension vector,
followed by the summation of all the Nresulting Cout-
dimensional vectors. Overall, self-attention achieves the
(Cin→Cout)mapping with two convolutions instead of
one, and later we will provide an analysis of how such a
breakdown contributes to the efficiency of Transformers by
comparing it with common practices in existing dynamic
convolutions.
3.3. Unified Framework for Existing Networks
So far, our conversion is based on the vanilla ViT. Now
we generalize the conversion to more Transformer architec-
tures. In particular, apart from the ViT [10], we choose the
Swin Transformer [21], which is a seminal work in the local
vision Transformer family, and Linformer [32], which is a
representative work in Transformers with linear complexity.
Besides, we also select two typical ConvNets with dynamic
convolutions [4, 11] and bring all the chosen Transformers
and ConvNets into a general framework so that we can ana-
lyze their design choices from a unified perspective.As shown in Figure 3 and Table 1, we characterize
ViT, Swin Transformer, Linformer, Dynamic Convolution
(D-Conv), and Dynamic Depth-wise Convolution (D-DW-
Conv) from four dimensions, namely, kernel bank, kernel
selection, kernel type, and kernel size.
Kernel bank. Sharing the same set of kernels across spa-
tial dimensions contributes significantly to the parameter
efficiency of convolution. However, kernel sharing is not
always adopted in current dynamic convolutions, e.g., D-
DW-Conv opts to generate a dedicated kernel set for each
spatial location at the cost of consuming more computation.
In contrast, vision Transformers and D-Conv re-use kernels
with the help of kernel banks. Specifically, D-Conv main-
tains a learnable kernel bank during training, and kernels
in the bank serve as the basis or prototypes, which are lin-
early combined into a single kernel for each input image.
During inference, the kernel bank stays static and is shared
by all testing images. Vision Transformers, however, share
kernels in a more fine-grained way, where kernels in the
bank are not shared across images but across spatial loca-
tions within a single image. In particular, for each image,
the kernel bank consists of kernels generated at all spatial
locations. To fetch kernels from the bank, different Trans-
4

--- PAGE 5 ---
formers implement different rules, which we will elaborate
in the next paragraph. As the kernel bank of Transformers is
conditioned on input images, we consider it as the dynamic
kernel bank as opposed to the static kernel bank of D-Conv.
Kernel selection. As shown in Figure 3, under the pro-
posed perspective, ViT, Swin Transformer, and Linformer,
as representative methods for global, local, and linear
Transformers, respectively, only differ in the rule of select-
ing kernels from the bank. In particular, ViT selects all the
kernels from the bank regardless of the input location, and
Swin Transformer chooses kernels generated within the lo-
cal windows. In contrast, Linformer performs soft selec-
tion, where all kernels are fed into a tiny network and result
in a smaller set of kernels therefore each resulting kernel
is essentially a learned linear combination of the input ker-
nels. Apart from Transformers, D-Conv adopts kernel bank
as well and it also follows a soft selection rule. But dif-
ferent from Linformer, D-Conv explicitly predicts the co-
efficients of the linear combination from the input feature.
Moreover, we find that the softmax in the self-attention of
Transformers also implicitly affects the kernel selection by
biasing towards kernels that are more close to the input fea-
ture in terms of cosine similarity and we will conduct more
comprehensive studies in the role of softmax in Section 4.2.
Kernel type. Directly generating the weights of a stan-
dard convolution kernel can be expensive and even infea-
sible. For instance, to generate kernel weights from a
C-dimensional vector, a one-layer weights generator re-
quires (CxHxWxCinxCout)parameters. Therefore, all the
listed Transformers and ConvNets avoid direct weights gen-
eration in various ways. D-DW-Conv adopts a straight-
forward solution where the standard convolution is re-
placed with a depth-wise one (sparse channel-wise con-
nectivity). Consequently, the required parameters are re-
duced to (CxHxWxCin). D-Conv re-parameterizes the
generation into the linear combination of kernels in a static
bank, where only the lightweight coefficients are dynam-
ically predicted from the input. Different from dynamic
ConvNets, self-attention in vision Transformers achieves
both dense channel-wise connectivity and a dynamic kernel
bank. First, self-attention adopts only 1×1convolutions
since it does not rely on large kernel size to gather informa-
tion from distant locations, instead, it achieves spatial-wise
communication by using kernels generated by each other.
Second, instead of performing (Cin→Cout)at once, self-
attention breaks the operation into two steps, (Cin→1)
followed by (1→Cout). In this way, self-attention cuts
down the required parameters of the weights generator to
(CC in+CC out).
Kernel size. In the previous paragraph, we mention that
self-attention adopts only 1×1convolutions, which is un-
common in dynamic convolutions. This design is a double-
edged sword because, on the one hand, it significantly en-courages parameter and computational efficiency, and on
the other hand, 1×1convolutions do not embed any spa-
tial priors, while spatial priors are beneficial to vision tasks.
Therefore, vision Transformers are usually augmented with
positional embedding to mitigate this shortcoming. How-
ever, it might be worth exploring whether allowing self-
attention to generate larger kernels, e.g.,3×3, also ad-
dresses the problem. We leave it as a future work.
4. Rethink Designs of Vision Transformers
Now that we have established a unified perspective of vi-
sion Transformers and ConvNets, we can rethink the design
choices of vision Transformers in the design space of Con-
vNets and vice versa. We believe such exploration can help
researchers switch to a different angle during new network
architecture design. To demonstrate such potential, we pro-
vide two practical examples: (1) study the role of softmax
as the activation function (Section 4.2) and (2) introduce the
depth-wise design into vision Transformers (Section 4.3).
4.1. Experimental Settings
We conduct experiments with the MMClassification
codebase [5] on both ViT and Swin Transformer. We choose
the ImageNet-1K [8] dataset, which contains 1.28M im-
ages for training and 50K images for validation across 1K
categories. And we use top-1 accuracy on the validation
set as the evaluation metric. In addition, to benchmark
the network efficiency, we take FLOPs, number of parame-
ters, and activation counts (which are demonstrated by Ra-
dosavovic et al . [27] to be more relevant to the through-
put than FLOPs) into account. To save the computational
cost, we adopt the training pipeline of DeiT [30] for ViT.
Specifically, each image is randomly resized and cropped
to 224 ×224, then augmented by random horizontal flip,
RandAugment [7], and Random Erasing [40]. Regulariza-
tion methods including Mixup [38], Cutmix [37], stochas-
tic depth [16], repeated augmentation [13], and Exponential
Moving Average (EMA) [26] are adopted. For optimiza-
tion, we use the AdamW [18] optimizer with the learning
rate initially set to 0.001 then gradually decaying with the
cosine annealing [23]. We train all models for 300 epochs
with the batch size equal to 1024. For Swin Transformer,
we follow its original training setting [21].
4.2. The Role of Softmax
Self-attention in vision Transformers adopts scaled-
softmax in between 1×1dynamic convolutions as the acti-
vation function, while ConvNets rarely do so. In the mean-
while, during the early stage of exploring the proper deep
ConvNet architecture, activation functions, as one of the
key components, have been thoroughly studied. Thus, we
further ask whether these conclusions can be directly trans-
ferred to vision Transformers.
5

--- PAGE 6 ---
Table 2: Ablation studies on the role of softmax as the activation function.
Base Normalization Non-linearity Channel Comm. Pos. Emb. Cls. Emb. Top-1 Acc.
DeiT-SSoftmax Softmax ✓ Abs ✓ 80.53
· · ✗ Abs ✓ 77.17
Scaling · ✗ Abs ✓ 78.15
Scaling ReLU ✗ Abs ✓ 80.03
Softmax Softmax ✓ Rel ✓ 80.96
Scaling · ✗ Rel ✓ 80.45
Scaling ReLU ✗ Rel ✓ 80.93
LayerNorm · ✓ Rel ✓ 78.82
LayerNorm · ✓ Rel ✗ 80.09
LayerNorm ReLU ✓ Rel ✗ 80.64
Swin-TSoftmax Softmax ✓ Rel ✗ 81.13
· · ✗ Rel ✗ 80.02
Scaling · ✗ Rel ✗ 81.15
Scaling ReLU ✗ Rel ✗ 81.24
LayerNorm · ✓ Rel ✗ 81.22
LayerNorm ReLU ✓ Rel ✗ 81.56
Softmax Softmax ✓ Abs ✗ 80.50
Scaling · ✗ Abs ✗ 79.99
Scaling ReLU ✗ Abs ✗ 80.57
Recall that,
softmax (x)i=exp (xi/τ)P
jexp (xj/τ), (3)
where τis the temperature constant. Softmax serves three
roles at the same time, namely normalization, channel-wise
communication, and non-linearity. First, softmax normal-
izes the input so that the results along the channel dimension
vary between 0 to 1 and sum up to 1. Second, for the cal-
culation at each channel position, elements at all other posi-
tions are also involved, which implicitly enables channel-
wise communication. In particular, in self-attention, the
channel dimension of the input to softmax denotes the key
kernel index as shown in Figure 2. Therefore, the channel-
wise communication essentially becomes kernel-wise com-
munication thus also implicitly affecting the kernel selec-
tion. Finally, as an S-shaped function, softmax provides
non-linearity between two linear convolutions.
In the following, we empirically examine the signifi-
cance of each role of softmax by replacing softmax with
modules that are dedicated to one role and are commonly
used in ConvNets. Specifically, for normalization, we sub-
stitute softmax with constant scaling or layer normaliza-
tion1. And as layer normalization operates along the chan-
1Layer normalization implicitly introduces non-linearity during the cal-
culation of the standard deviation but it is not as strong as softmax or
ReLU.nel dimension, it can also serve for channel-wise commu-
nication. For non-linearity, we choose the widely-adopted
ReLU function.
We choose the ViT (trained with the DeiT pipeline)
and Swin Transformer as experiment objectives since
they are representative works with the vanilla and lo-
cal vision Transformer architectures, respectively. Note
that, despite the fundamental differences between ViT and
Swin Transformer being global/local attention and non-
hierarchical/hierarchical, they also differ in some detailed
design choices, such as the form of positional embedding
(absolute vs. relative) and whether to use a class embedding
for classification.
We start our exploration by removing softmax. As shown
in Table 2, we observe that both the performances of DeiT-
S and Swin-T drop (80.53 →77.17 and 81.13 →80.02).
We then perform a simple normalization by scaling with a
constant (dividing by the number of channels of each head).
As a result, DeiT-S and Swin-T both obtain certain gains
(77.17 →78.15 and 80.45 →81.15). In particular, with
the constant scaling, Swin-T achieves as good a result as
softmax despite the variant having no non-linearity at all.
Therefore, we conclude that normalization is important
but is not limited to softmax .
Next, on top of the constant scaling, we further inject
non-linearity with the ReLU activation. In Table 2, with
ReLU, DeiT-S is improved from 78.15 to 80.03 while Swin-
6

--- PAGE 7 ---
Figure 4: Convergence rate with different activation
functions. Here, we evaluate the convergence rate by
training loss and top-1 accuracy on the validation set with
respect to training epochs. Note that, DeiT in this fig-
ure adopts relative positional embeddings. Besides, all the
ReLU variants use constant scaling for normalization.
T is improved from 81.15 to 81.24. First, the results show
that ReLU is more necessary for DeiT than Swin and we
conjecture the reason is that, in addition to introducing the
non-linearity, ReLU also implicitly affects the kernel se-
lection by zeroing out those kernels with negative activa-
tion, which is more important for DeiT since it has a wider
range of kernels for selection. Second, despite the effect of
scaling and ReLU shows a consistent trend among ViT and
Swin Transformer, compared to their corresponding soft-
max baselines, DeiT-S with scaling and ReLU performs
worse (80.53 →80.03) while the Swin-T variant performs
slightly better (80.13 →80.24). Therefore, before continu-
ing to the next ablation, we study whether such differences
come from detailed design choices. For this purpose, we
swap the positional embedding methods between the ViT
and Swin Transformer. Consequently, using the relative
positional embedding, DeiT-S with scaling and ReLU ob-
tains a very similar result with softmax (80.93 vs. 80.96).
In addition, after altering to the absolute positional embed-
ding, Swin-T also suffers from the absence of non-linearity.
Furthermore, we observe that during training, once softmax
is replaced by either scaling and/or ReLU, the model usu-
ally converges faster as shown in Figure 4. We think this
observation aligns with the vanishing gradient problem of
S-shaped activation functions in ConvNets, where ReLU
makes the model converge faster than sigmoid. In sum-
mary, with relative positional embedding, the softmax in
ViT and Swin Transformer can be replaced with scaling-
ReLU for a faster convergence rate and comparable re-
sults.
Figure 5: From self-attention without non-linearity to
the depth-wise variant. DW denotes depth-wise convo-
lution. We annotate the kernel shape on the top left of each
convolutional layer.
Finally, we examine the channel-wise communication
role of the softmax by replacing it with layer normaliza-
tion (LN). Compared to normalizing with scaling, which
performs no channel-wise communication, LN in Swin-T
yields much better results. In fact, as shown in Figure 2,
Swin-T with LN-ReLU significantly outperforms the soft-
max baseline (80.13 →81.56) without any computational
overhead. On DeiT-S, we first find applying LN largely de-
grades the performance (80.96 →78.82), but after aligning
with Swin-T where replace the class embedding of DeiT-S
with global average pooling, LN-ReLU yields comparable
results. We conjecture this is due to that learning a good
LN is harder for ViT than Swin Transformer since the for-
mer is responsible for normalizing all kernels and the latter
normalizes only selected local kernels.
4.3. Depth-wise Vision Transformers
In Section 3, we discuss that self-attention achieves the
(Cin→Cout)mapping while keeping dense channel-wise
connectivity. In this section, we explore switching to sparse
channel-wise connectivity, as depth-wise convolution does,
to further emphasize efficiency.
When studying softmax in the previous section, we find
that the non-linearity role of softmax is not always neces-
sary. For instance, Swin-T with only scaling achieves com-
parable results with the softmax baseline. Moreover, since
convolutions are linear operations, without any non-linear
function in between, two consecutive convolutions can be
merged into a single one. As shown in Figure 5, in self-
attention, with matrix multiplication, the selected kernels
k∈R1×1×Cin×Nandv∈R1×1×N×Coutcan be merged
into a new kernel g∈R1×1×Cin×Cout, which is exactly the
kernel of a standard 1×1convolution. This observation
inspires us to replace the standard kernel with a depth-wise
one.
For this purpose, we modify the original self-attention
with the following steps. First, we discard the value kernel
bank and keep only the keykernel bank. Next, at each spa-
7

--- PAGE 8 ---
Table 3: Compare the performance of depth-wise vision Transformers with standard ones.
Base Act. Func. Kernel Type Pos. Emb. GFLOPs MParams. MActs. Top-1 Acc.
DeiT-SSoftmax C→1→C Abs 4.608 22.051 11.947 80.53
Scaling C→1→C Abs 4.366 22.051 9.448 78.15
Scaling Depth-wise Abs 3.902 20.277 7.337 73.61
Softmax C→1→C Rel 4.608 22.028 11.947 80.96
Scaling C→1→C Rel 4.543 22.028 10.351 80.45
Scaling Depth-wise Rel 4.079 20.253 8.241 79.74
DeiT-BSoftmax C→1→C Abs 17.582 86.568 23.895 81.76
Softmax C→1→C Rel 17.582 86.521 23.895 82.39
Scaling Depth-wise Rel 15.830 79.434 16.481 81.92
Swin-TSoftmax C→1→C Rel 4.508 28.288 17.054 81.13
Scaling C→1→C Rel 4.508 28.288 17.054 81.15
Scaling Depth-wise Rel 4.141 26.850 14.112 80.75
Swin-BSoftmax C→1→C Rel 15.467 87.768 36.625 83.36
Scaling C→1→C Rel 15.468 87.768 38.930 82.80
Scaling Depth-wise Rel 14.192 80.777 30.256 82.48
tial location, we select Nkernels from the bank following
the same rule as self-attention ( e.g., ViT selects all kernels
and Swin Transformer selects local kernels) and average the
selected kernels into a single one. Finally, instead of com-
puting the dot product between the input and kernel, which
projects the input to a scalar, we perform element-wise mul-
tiplication between them so that the channel dimension of
the input stays unchanged. We name the modified self-
attention as depth-wise self-attention since it has both the
kernel bank mechanism in self-attention and the element-
wise multiplication in depth-wise convolutions. And vision
Transformers with the self-attention replaced to the depth-
wise variant become depth-wise vision Transformers.
As shown in Table 3, we conduct experiments on DeiT-
Small, DeiT-Base, Swin-Tiny, and Swin-Base with the pro-
posed depth-wise self-attention. First of all, since the depth-
wise self-attention has no non-linearity, as expected, it fails
on the DeiT-S. Therefore, for both ViT and Swin Trans-
former, by default, we employ the relative positional em-
bedding to mitigate the non-linearity problem as discussed
in Section 4.2. In particular, without softmax, in self-
attention, relative positional embedding pis directly applied
to values:
oi= (qikT
√Ch+pi)v=qikT
√Chv+piv. (4)
However, as the value kernel bank is removed in the depth-
wise self-attention, we apply ptokinstead:
oi=qi⊙¯k+pik. (5)
In Table 3, we show that with relative positional embed-
ding, DeiT-S, Swin-T, and Swin-B achieve comparable re-sults with the corresponding baselines while saving ≈10%
FLOPs, ≈8% learnable parameters, and 20%-40% activa-
tion counts.
5. Discussion
In this paper, we propose a new perspective that inter-
prets vision Transformers as ConvNets with dynamic con-
volution through equivalently converting the self-attention
block in Transformers into a combination of static convo-
lutions, dynamic convolutions, and a non-linear activation
function. In addition, we select several existing represen-
tative works from both the vision Transformer family and
the dynamic ConvNet family and fit them into the pro-
posed framework, so that we can cross-compare their de-
sign choices in a unified view. In particular, we character-
ize them from four dimensions, namely kernel bank, kernel
selection, kernel type, and kernel size, and conduct an anal-
ysis of each dimension. Moreover, apart from summariz-
ing existing works, more importantly, the proposed unified
perspective enables researchers to rethink design choices of
vision Transformers in the design space of ConvNets and
vice versa, which helps the network architecture design. To
demonstrate such potential, we provide two practical exam-
ples, including examining the role of softmax as the acti-
vation function and introducing the depth-wise design into
self-attention.
Future works. We believe there is still a lot of room to
explore following this direction, such as injecting spatial
priors through generating 3×3dynamic kernels instead
1×1ones, developing self-attention-style dynamic convo-
lutions with strides to form a hierarchy, getting rid of the
8

--- PAGE 9 ---
Figure 6: Convergence rate with different activation functions.
heavy FFN in self-attention, and so on. We hope the pro-
posed perspective can provide new inspiration and insights
for network designs.
A. More Convergence Curves
In our main submission, we show that when the softmax
in self-attention is replaced by scaling-ReLU, despite the
results being similar, the convergence rate becomes faster.
In Figure 6, we provide more examples to demonstrate this
point. In specific, we use constant scaling, layer normaliza-
tion (LN), and LN-ReLU as the activation function, respec-
tively. As a result, we observe the same pattern in terms of
loss and accuracy as Figure 4.
B. Implementation Details
In Table 4, we list all the hyper-parameters and training
strategies of our experiments. In general, we follow the ex-
periment settings in the original DeiT [30] and Swin Trans-
former [21] with only a minor difference in using gradient
clipping. Note that, DeiT models in different scales share
the same hyper-parameters while Swin Transformers vary
in the stochastic depth drop rates.
References
[1] Jean-Marc Andreoli. Convolution, attention and structure
embedding. NeurIPS workshop on Graph Representation
Learning , 2019. 2Table 4: Training settings.
Methods DeiT-S [30] Swin-T (B) [21]
Epochs 300 300
Batch size 1024 1024
Optimizer AdamW AdamW
Learning rate 0.001 0.001
Learning rate decay cosine cosine
Weight decay 0.05 0.05
Warmup epochs 20 20
Label smoothing 0.1 0.1
Dropout ✗ ✗
Stoch. Depth 0.1 0.2 (0.5)
Repeated Aug. ✓ ✗
Gradient Clip 5.0 5.0
EMA ✓ ✗
Rand Augment 9/0.5 9/0.5
Mixup prob. 0.8 0.8
Cutmix prob. 1.0 1.0
Erasing prob. 0.25 0.25
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 2
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
9

--- PAGE 10 ---
end object detection with transformers. In ECCV , 2020. 1
[4] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong
Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: At-
tention over convolution kernels. In CVPR , 2020. 2, 4
[5] MMClassification Contributors. Openmmlab’s image clas-
sification toolbox and benchmark. https://github.
com/open-mmlab/mmclassification , 2020. 5
[6] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin
Jaggi. On the relationship between self-attention and con-
volutional layers. In ICLR , 2020. 3
[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmentation
with a reduced search space. In CVPRW , 2020. 5
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 1, 5
[9] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang
Ding. Scaling up your kernels to 31x31: Revisiting large
kernel design in cnns. In CVPR , 2022. 1
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 1, 2, 4
[11] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiay-
ing Liu, and Jingdong Wang. On the connection between lo-
cal attention and dynamic depth-wise convolution. In ICLR ,
2021. 2, 3, 4
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 1
[13] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten
Hoefler, and Daniel Soudry. Augment your batch: Improving
generalization through instance repetition. In CVPR , 2020.
5
[14] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea
Vedaldi. Gather-excite: Exploiting feature context in convo-
lutional neural networks. NeurIPS , 2018. 2
[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In CVPR , 2018. 2
[16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
Weinberger. Deep networks with stochastic depth. In ECCV ,
2016. 5
[17] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V
Gool. Dynamic filter networks. NeurIPS , 2016. 2
[18] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Communications of the ACM , 2017. 1, 2
[20] Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei
Zhu, Tong Zhang, and Qifeng Chen. Involution: Invert-
ing the inherence of convolution for visual recognition. In
CVPR , 2021. 2[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 1, 2, 4, 5, 9
[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In CVPR , 2022. 1
[23] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. In ICLR , 2017. 5
[24] Vinod Nair and Geoffrey E Hinton. Rectified linear units
improve restricted boltzmann machines. In ICML , 2010. 2
[25] Xuran Pan, Chunjiang Ge, Rui Lu, Shiji Song, Guanfu Chen,
Zeyi Huang, and Gao Huang. On the integration of self-
attention and convolution. In CVPR , 2022. 2
[26] Boris T Polyak and Anatoli B Juditsky. Acceleration of
stochastic approximation by averaging. SIAM journal on
control and optimization , 1992. 5
[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
Kaiming He, and Piotr Doll ´ar. Designing network design
spaces. In CVPR , 2020. 5
[28] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR ,
2014. 1
[29] Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi
Aribandi, Dara Bahri, Zhen Qin, and Donald Metzler. Are
pretrained convolutions better than pretrained transformers?
InACL/IJCNLP , 2021. 2
[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In ICML , 2021. 5, 9
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS , 2017. 1, 2
[32] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020. 4
[33] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-
mid vision transformer: A versatile backbone for dense pre-
diction without convolutions. In ICCV , 2021. 2
[34] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and
Michael Auli. Pay less attention with lightweight and dy-
namic convolutions. In ICLR , 2019. 2
[35] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-
volutions to vision transformers. In ICCV , 2021. 2
[36] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Feng-
wei Yu, and Wei Wu. Incorporating convolution designs into
visual transformers. In ICCV , 2021. 2
[37] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classifiers with localizable
features. In ICCV , 2019. 5
[38] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. Mixup: Beyond empirical risk minimiza-
tion. In ICLR , 2018. 5
10

--- PAGE 11 ---
[39] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic
segmentation from a sequence-to-sequence perspective with
transformers. In CVPR , 2021. 1
[40] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
Yi Yang. Random erasing data augmentation. In AAAI , 2020.
5
11
