# Học biểu diễn đoạn văn deconvolutional

Yizhe Zhang Dinghan Shen Guoyin Wang Zhe Gan Ricardo Henao
Lawrence Carin
Khoa Kỹ thuật Điện & Máy tính, Đại học Duke

Tóm tắt
Học biểu diễn tiềm ẩn từ các chuỗi văn bản dài là một bước đầu quan trọng trong nhiều ứng dụng xử lý ngôn ngữ tự nhiên. Mạng nơ-ron hồi quy (RNN) đã trở thành nền tảng cho nhiệm vụ thách thức này. Tuy nhiên, chất lượng của các câu trong quá trình giải mã (tái tạo) dựa trên RNN giảm theo độ dài của văn bản. Chúng tôi đề xuất một khung autoencoding tuần tự-đến-tuần tự, hoàn toàn convolutional và deconvolutional không gặp phải vấn đề trên, đồng thời hiệu quả về mặt tính toán. Phương pháp đề xuất đơn giản, dễ thực hiện và có thể được sử dụng như một khối xây dựng cho nhiều ứng dụng. Chúng tôi chứng minh thông qua thực nghiệm rằng so với RNN, khung của chúng tôi tốt hơn trong việc tái tạo và sửa chữa các đoạn văn dài. Đánh giá định lượng trên các tác vụ phân loại văn bản bán giám sát và tóm tắt chứng minh tiềm năng cho việc sử dụng tốt hơn dữ liệu văn bản dài không nhãn.

1 Giới thiệu
Một nhiệm vụ trung tâm trong xử lý ngôn ngữ tự nhiên là học biểu diễn (đặc trưng) cho các câu hoặc đoạn văn đa câu. Những biểu diễn này thường là bước đầu tiên cần thiết hướng đến các tác vụ ứng dụng hơn, như phân tích tình cảm [1,2,3,4], dịch máy [5,6,7], hệ thống đối thoại [8,9,10] và tóm tắt văn bản [11,12,13]. Một cách tiếp cận để học biểu diễn câu từ dữ liệu là tận dụng khung encoder-decoder [14]. Trong thiết lập autoencoding tiêu chuẩn, biểu diễn vector trước tiên được mã hóa từ embedding của chuỗi đầu vào, sau đó được giải mã về miền gốc để tái tạo chuỗi đầu vào. Những tiến bộ gần đây trong Mạng nơ-ron hồi quy (RNN) [15], đặc biệt là Long Short-Term Memory (LSTM) [16] và các biến thể [17], đã đạt được thành công lớn trong nhiều tác vụ phụ thuộc nhiều vào việc học biểu diễn câu.

Các phương pháp dựa trên RNN thường mô hình câu một cách đệ quy như một quá trình Markov sinh với các đơn vị ẩn, trong đó từ tiếp theo từ câu đầu vào được sinh ra bằng cách điều kiện hóa trên các từ trước đó và các đơn vị ẩn, thông qua các toán tử phát xạ và chuyển tiếp được mô hình hóa như các mạng nơ-ron. Về nguyên tắc, các biểu diễn nơ-ron của chuỗi đầu vào nhằm đóng gói đủ thông tin về cấu trúc của chúng, để sau đó khôi phục các câu gốc thông qua giải mã. Tuy nhiên, do tính chất đệ quy của RNN, tồn tại những thách thức cho các chiến lược dựa trên RNN để mã hóa đầy đủ một câu thành biểu diễn vector. Thông thường, trong quá trình huấn luyện, RNN sinh ra các từ theo trình tự điều kiện hóa trên các từ ground-truth trước đó, tức là huấn luyện teacher forcing [18], thay vì giải mã toàn bộ câu chỉ từ vector biểu diễn được mã hóa. Chiến lược teacher forcing này đã chứng minh là quan trọng vì nó buộc chuỗi đầu ra của RNN ở gần chuỗi ground-truth. Tuy nhiên, việc cho phép decoder truy cập thông tin ground truth khi tái tạo chuỗi làm suy yếu khả năng của encoder tạo ra các biểu diễn tự chứa, mang đủ thông tin để dẫn dắt decoder qua quá trình giải mã mà không cần hướng dẫn thêm. Nhằm giải quyết vấn đề này, [19] đề xuất phương pháp lấy mẫu có lịch trình trong quá trình huấn luyện, dần dần chuyển từ học thông qua cả tín hiệu biểu diễn tiềm ẩn và ground-truth sang chỉ sử dụng biểu diễn tiềm ẩn được mã hóa. Thật không may, [20] chỉ ra rằng lấy mẫu có lịch trình về cơ bản là một chiến lược huấn luyện không nhất quán, trong đó nó tạo ra kết quả chủ yếu không ổn định trong thực tế. Kết quả là, việc huấn luyện có thể không hội tụ đôi khi.

Trong quá trình suy luận, khi các câu ground-truth không có sẵn, các từ phía trước chỉ có thể được sinh ra bằng cách điều kiện hóa trên các từ được sinh ra trước đó thông qua vector biểu diễn. Do đó, lỗi giải mã tích lũy tỷ lệ với độ dài của chuỗi. Điều này có nghĩa là các câu được sinh ra nhanh chóng lệch khỏi ground-truth một khi lỗi đã xảy ra, và khi câu tiến triển. Hiện tượng này được gọi là exposure bias trong [19].

Chúng tôi đề xuất một khung hoàn toàn convolutional đơn giản nhưng mạnh mẽ để học biểu diễn câu. Thuận tiện thay, không có RNN trong khung của chúng tôi, các vấn đề liên quan đến huấn luyện teacher forcing và exposure bias không liên quan. Phương pháp đề xuất sử dụng Mạng nơ-ron tích chập (CNN) [21,22] làm encoder và mạng nơ-ron deconvolutional (tức là tích chập chuyển vị) [23] làm decoder. Theo hiểu biết của chúng tôi, khung đề xuất là khung đầu tiên buộc biểu diễn tiềm ẩn được mã hóa phải nắm bắt thông tin từ toàn bộ câu thông qua đặc tả CNN đa lớp, để đạt được chất lượng tái tạo cao mà không tận dụng các decoder dựa trên RNN. CNN đa lớp của chúng tôi cho phép các vector biểu diễn trừu tượng hóa thông tin từ toàn bộ câu, bất kể thứ tự hay độ dài, khiến nó trở thành lựa chọn hấp dẫn cho các tác vụ liên quan đến câu dài hoặc đoạn văn. Hơn nữa, vì khung của chúng tôi không liên quan đến mã hóa hoặc giải mã đệ quy, nó có thể được song song hóa rất hiệu quả sử dụng các primitive Đơn vị xử lý đồ họa (GPU) đặc biệt cho tích chập, mang lại tiết kiệm tính toán đáng kể so với các mô hình dựa trên RNN.

2 Tự mã hóa tích chập cho mô hình văn bản

2.1 Encoder tích chập
Gọi w_t là từ thứ t trong một câu cho trước. Mỗi từ w_t được nhúng vào vector từ k chiều x_t = W_e[w_t], trong đó W_e ∈ R^{k×V} là ma trận nhúng từ (học được), V là kích thước từ vựng, và W_e[v] biểu thị cột thứ v của W_e. Tất cả cột của W_e được chuẩn hóa để có chuẩn l_2 đơn vị, tức là ||W_e[v]||_2 = 1; ∀v, bằng cách chia mỗi cột cho chuẩn l_2 của nó. Sau khi nhúng, một câu có độ dài T (được đệm khi cần thiết) được biểu diễn như X ∈ R^{k×T}, bằng cách nối các word embedding của nó, tức là x_t là cột thứ t của X.

Để mã hóa câu, chúng tôi sử dụng kiến trúc CNN tương tự như [24], mặc dù ban đầu được đề xuất cho dữ liệu hình ảnh. CNN bao gồm L lớp (L-1 tích chập, và lớp thứ L kết nối đầy đủ) cuối cùng tóm tắt câu đầu vào thành vector biểu diễn tiềm ẩn (độ dài cố định), h. Lớp l ∈ {1, ..., L} bao gồm p_l bộ lọc, được học từ dữ liệu. Đối với bộ lọc thứ i trong lớp 1, phép tích chập với độ dài bước r^{(1)} áp dụng bộ lọc W_c^{(i,1)} ∈ R^{k×h} lên X, trong đó h là kích thước bộ lọc tích chập. Điều này tạo ra bản đồ đặc trưng tiềm ẩn, c^{(i,1)} = σ(X * W_c^{(i,1)} + b^{(i,1)}) ∈ R^{(T-h)/r^{(1)}+1}, trong đó σ(·) là hàm kích hoạt phi tuyến, b^{(i,1)} ∈ R^{(T-h)/r^{(1)}+1}, và * biểu thị toán tử tích chập. Trong thực nghiệm của chúng tôi, σ(·) được biểu diễn bởi Đơn vị tuyến tính chỉnh lưu (ReLU) [25]. Lưu ý rằng chiều nhúng ban đầu, k, thay đổi sau lớp tích chập đầu tiên, vì c^{(i,1)} ∈ R^{(T-h)/r^{(1)}+1}, với i = 1, ..., p_1. Nối kết quả từ p_1 bộ lọc (cho lớp 1), tạo ra bản đồ đặc trưng, C^{(1)} = [c^{(1,1)} ... c^{(p_1,1)}] ∈ R^{p_1×[(T-h)/r^{(1)}+1]}.

Sau lớp tích chập đầu tiên này, chúng tôi áp dụng phép tích chập lên bản đồ đặc trưng, C^{(1)}, sử dụng cùng kích thước bộ lọc, h, với việc này được lặp lại theo trình tự cho L-1 lớp. Mỗi lần, độ dài dọc theo tọa độ không gian được giảm xuống T^{(l+1)} = ⌊(T^{(l)} - h)/r^{(l)} + 1⌋, trong đó r^{(l)} là độ dài bước, T^{(l)} là độ dài không gian, l biểu thị lớp thứ l và ⌊·⌋ là hàm làm tròn xuống. Đối với lớp cuối cùng, L, bản đồ đặc trưng C^{(L-1)} được đưa vào lớp kết nối đầy đủ, để tạo ra biểu diễn tiềm ẩn h.

Về mặt triển khai, chúng tôi sử dụng lớp tích chập với kích thước bộ lọc bằng T^{(L-1)} (bất kể h), tương đương với lớp kết nối đầy đủ; kỹ thuật triển khai này cũng đã được sử dụng trong [24]. Lớp cuối cùng này tóm tắt tất cả tọa độ không gian còn lại, T^{(L-1)}, thành các đặc trưng vô hướng đóng gói các cấu trúc con của câu trong toàn bộ câu được đặc trưng bởi các bộ lọc, {W_c^{(i,l)}} với i = 1, ..., p_l và l = 1, ..., L, trong đó W_c^{(i,l)} biểu thị bộ lọc i cho lớp l. Điều này cũng ngụ ý rằng đặc trưng được trích xuất có chiều cố định, độc lập với độ dài của câu đầu vào.

Có p_L bộ lọc trên lớp cuối cùng, tạo ra vector biểu diễn p_L chiều, h = C^{(L)}, cho câu đầu vào. Ví dụ, trong Hình 1, encoder bao gồm L = 3 lớp, với câu có độ dài T = 60, chiều nhúng k = 300, độ dài bước {r^{(1)}, r^{(2)}, r^{(3)}} = {2, 2, 1}, kích thước bộ lọc h = {5, 5, 12} và số lượng bộ lọc {p_1, p_2, p_3} = {300, 600, 500}, tạo ra các bản đồ đặc trưng trung gian, C^{(1)} và C^{(2)} có kích thước {28×300, 12×600}, tương ứng. Bản đồ đặc trưng cuối cùng có kích thước 1×500, tương ứng với vector biểu diễn tiềm ẩn, h.

Về mặt khái niệm, các bộ lọc từ các lớp thấp hơn nắm bắt thông tin câu nguyên thủy (h-gram, tương tự như cạnh trong hình ảnh), trong khi các bộ lọc cấp cao hơn nắm bắt các đặc trưng ngôn ngữ phức tạp hơn, như cấu trúc ngữ nghĩa và cú pháp (tương tự như các phần tử hình ảnh). Kiến trúc từ dưới lên như vậy mô hình câu bằng cách xếp chồng phân cấp các đoạn văn bản (h-gram) như các khối xây dựng cho vector biểu diễn, h. Điều này tương tự về tinh thần với việc mô hình hóa các hình thức ngữ pháp ngôn ngữ thông qua cây cú pháp cụ thể [26], tuy nhiên, chúng tôi không tiền xác định cấu trúc cây dựa trên một số cấu trúc cú pháp (tức là tiếng Anh), mà trừu tượng hóa nó từ dữ liệu thông qua mạng tích chập đa lớp.

2.2 Decoder deconvolutional
Chúng tôi áp dụng deconvolution với bước (tức là tích chập chuyển vị), như phép toán liên hợp của tích chập, để giải mã biểu diễn tiềm ẩn, h, trở lại miền văn bản (rời rạc) nguồn. Khi phép toán deconvolution tiến hành, độ phân giải không gian tăng dần, bằng cách phản chiếu các bước tích chập được mô tả ở trên, như minh họa trong Hình 1. Chiều không gian trước tiên được mở rộng để khớp với chiều không gian của lớp thứ (L-1) của tích chập, sau đó được mở rộng dần như T^{(l+1)} = (T^{(l)} - 1) × r^{(l)} + h, với l = 1, lên đến lớp deconvolutional thứ L (tương ứng với lớp đầu vào của encoder tích chập). Đầu ra của phép toán deconvolution L lớp nhằm tái tạo ma trận nhúng từ, mà chúng tôi ký hiệu là X̂. Phù hợp với ma trận nhúng từ W_e, các cột của X̂ được chuẩn hóa để có chuẩn l_2 đơn vị.

Ký hiệu ŵ_t là từ thứ t trong câu tái tạo ŝ, xác suất để ŵ_t là từ v được xác định như
p(ŵ_t = v) = exp[1/τ D_cos(x̂_t, W_e[v])] / Σ_{v' ∈ V} exp[1/τ D_cos(x̂_t, W_e[v'])]; (1)

trong đó D_cos(x, y) là độ tương tự cosine được định nghĩa là ⟨x, y⟩ / (||x|| ||y||), W_e[v] là cột thứ v của W_e, x̂_t là cột thứ t của X̂, τ là số dương mà chúng tôi ký hiệu là tham số nhiệt độ [27]. Tham số này tương tự như tham số tập trung của phân phối Dirichlet, trong đó nó kiểm soát độ phân tán của vector xác suất [p(ŵ_t = 1) ... p(ŵ_t = V)], do đó τ lớn khuyến khích xác suất phân phối đồng nhất, trong khi τ nhỏ khuyến khích giá trị xác suất thưa thớt, tập trung. Trong thực nghiệm chúng tôi đặt τ = 0.01. Lưu ý rằng trong thiết lập của chúng tôi, độ tương tự cosine có thể được tính như tích trong, với điều kiện các cột của W_e và X̂ có chuẩn l_2 đơn vị theo đặc tả.

2.3 Học mô hình
Mục tiêu của convolutional autoencoder được mô tả ở trên có thể được viết như log-likelihood theo từ cho tất cả câu s ∈ D, tức là,
L_ae = Σ_{d ∈ D} Σ_t log p(ŵ_t^d = w_t^d); (2)

trong đó D biểu thị tập hợp các câu được quan sát. Mục tiêu maximum-likelihood đơn giản trong (2) được tối ưu hóa thông qua gradient descent ngẫu nhiên. Chi tiết triển khai được cung cấp trong thực nghiệm. Lưu ý rằng (2) khác với các nghiên cứu liên quan trước đây theo hai cách: i) [22,28] sử dụng các toán tử pooling và un-pooling, trong khi chúng tôi sử dụng convolution/deconvolution với bước; và ii) quan trọng hơn, [22,28] không sử dụng tái tạo độ tương tự cosine như trong (1), mà dùng decoder dựa trên RNN. Thảo luận thêm về nghiên cứu liên quan được cung cấp trong Phần 3. Chúng tôi có thể sử dụng pooling và un-pooling thay vì striding (một trường hợp đặc biệt của pooling/un-pooling xác định), tuy nhiên, trong các thực nghiệm ban đầu (không được hiển thị) chúng tôi không quan sát thấy cải thiện hiệu suất đáng kể, trong khi các phép toán convolution/deconvolution với bước hiệu quả hơn đáng kể về mặt dung lượng bộ nhớ. So với sequence autoencoder dựa trên LSTM tiêu chuẩn với số lượng tham số gần như nhau, tính toán trong trường hợp của chúng tôi nhanh hơn đáng kể (xem thực nghiệm) sử dụng GPU NVIDIA TITAN X đơn lẻ. Điều này do hiệu quả song song hóa cao của CNN thông qua các primitive cuDNN [29].

So sánh giữa decoder deconvolutional và RNN Khung đề xuất có thể được xem như một khối xây dựng bổ sung cho mô hình ngôn ngữ tự nhiên. Trái ngược với decoder dựa trên LSTM tiêu chuẩn, decoder deconvolutional áp đặt nói chung một phụ thuộc chuỗi ít nghiêm ngặt hơn so với kiến trúc RNN. Cụ thể, việc sinh ra một từ từ RNN yêu cầu một vector các đơn vị ẩn tích lũy thông tin từ toàn bộ câu một cách đệ quy theo cách bảo toàn thứ tự (các phụ thuộc dài hạn bị giảm trọng số mạnh), trong khi đối với decoder deconvolutional, việc sinh ra chỉ phụ thuộc vào vector biểu diễn đóng gói thông tin từ khắp câu mà không có cấu trúc thứ tự được tiền xác định. Kết quả là, đối với các tác vụ sinh ngôn ngữ, decoder RNN thường sẽ sinh ra văn bản mạch lạc hơn, so với decoder deconvolutional. Ngược lại, decoder deconvolutional tốt hơn trong việc tính đến các phụ thuộc xa trong câu dài, có thể rất có lợi trong trích xuất đặc trưng cho các tác vụ phân loại và tóm tắt văn bản.

2.4 Phân loại và tóm tắt bán giám sát
Xác định các chủ đề hoặc tình cảm liên quan, và trừu tượng hóa tóm tắt (ngắn) từ nội dung do người dùng tạo ra như blog hoặc đánh giá sản phẩm, gần đây đã nhận được sự quan tâm đáng kể [1,3,4,30,31,13,11]. Trong nhiều kịch bản thực tế, dữ liệu không nhãn dồi dào, tuy nhiên, không có nhiều trường hợp thực tế nào tiềm năng của dữ liệu không nhãn như vậy được khai thác đầy đủ. Được thúc đẩy bởi cơ hội này, ở đây chúng tôi tìm cách bổ sung dữ liệu có nhãn khan hiếm nhưng có giá trị hơn, để cải thiện khả năng tổng quát hóa của các mô hình có giám sát. Bằng cách hấp thụ dữ liệu không nhãn, mô hình có thể học trừu tượng hóa các biểu diễn tiềm ẩn nắm bắt ý nghĩa ngữ nghĩa của tất cả câu có sẵn bất kể chúng có được gắn nhãn hay không. Điều này có thể được thực hiện trước khi huấn luyện mô hình có giám sát, như một quá trình hai bước. Gần đây, các phương pháp dựa trên RNN khai thác ý tưởng này đã được sử dụng rộng rãi và đã đạt được hiệu suất tiên tiến trong nhiều tác vụ [1,3,4,30,31]. Cách khác, người ta có thể học autoencoder và classifier cùng nhau, bằng cách chỉ định mô hình phân loại có đầu vào là biểu diễn tiềm ẩn, h; xem ví dụ [32, 33].

Trong trường hợp đánh giá sản phẩm, chẳng hạn, mỗi đánh giá có thể chứa hàng trăm từ. Điều này đặt ra thách thức khi huấn luyện các encoder chuỗi dựa trên RNN, theo nghĩa là RNN phải trừu tượng hóa thông tin trong quá trình di chuyển qua câu, điều này thường dẫn đến mất thông tin, đặc biệt trong câu dài [34]. Hơn nữa, quá trình giải mã sử dụng thông tin ground-truth trong quá trình huấn luyện, do đó biểu diễn học được có thể không nhất thiết giữ tất cả thông tin từ văn bản đầu vào cần thiết cho việc tái tạo, tóm tắt hoặc phân loại phù hợp.

Chúng tôi xem xét việc áp dụng khung autoencoding tích chập của chúng tôi cho việc học bán giám sát từ câu dài và đoạn văn. Thay vì tiền huấn luyện mô hình hoàn toàn không giám sát như trong [1,3], chúng tôi đặt tác vụ bán giám sát như một bài toán học đa nhiệm vụ tương tự như [35], tức là, chúng tôi đồng thời huấn luyện sequence autoencoder và mô hình có giám sát. Về nguyên tắc, bằng cách sử dụng chiến lược huấn luyện kết hợp này, vector nhúng đoạn văn học được sẽ bảo toàn cả khả năng tái tạo và phân loại.

Cụ thể, chúng tôi xem xét mục tiêu sau:
L_semi = Σ_{d ∈ {D_l + D_u}} Σ_t log p(ŵ_t^d = w_t^d) + λ Σ_{d ∈ D_l} L_sup(f(h_d), y_d); (3)

trong đó λ > 0 là tham số annealing cân bằng tầm quan trọng tương đối của loss có giám sát và không giám sát; D_l và D_u biểu thị tập hợp dữ liệu có nhãn và không nhãn, tương ứng. Số hạng đầu tiên trong (3) là loss sequence autoencoder trong (2) cho chuỗi thứ d. L_sup(·) là loss giám sát cho chuỗi thứ d (chỉ có nhãn). Hàm classifier, f(·), cố gắng tái tạo y_d từ h_d có thể là Multi-Layer Perceptron (MLP) trong các tác vụ phân loại, hoặc CNN/RNN trong các tác vụ tóm tắt văn bản. Đối với trường hợp sau, chúng tôi quan tâm đến đặc tả hoàn toàn tích chập, tuy nhiên, chúng tôi cũng xem xét RNN để so sánh. Đối với phân loại, chúng tôi sử dụng loss cross-entropy tiêu chuẩn, và đối với tóm tắt văn bản chúng tôi sử dụng (2) cho CNN hoặc loss LSTM tiêu chuẩn cho RNN.

Trong thực tế, chúng tôi áp dụng chiến lược annealing có lịch trình cho λ như trong [36,37], thay vì cố định nó trước như trong [1]. Trong quá trình huấn luyện, (3) dần dần chuyển từ tập trung chỉ vào sequence autoencoder không giám sát sang tác vụ có giám sát, bằng cách annealing từ 1 đến giá trị dương nhỏ λ_min. Chúng tôi đặt λ_min = 0.01 trong thực nghiệm. Động lực cho chiến lược annealing này là trước tiên tập trung vào trừu tượng hóa các đặc trưng đoạn văn, sau đó lựa chọn tinh chỉnh các đặc trưng học được mang tính thông tin nhất cho tác vụ có giám sát.

3 Nghiên cứu liên quan
Nghiên cứu trước đây đã xem xét việc tận dụng CNN làm encoder cho các tác vụ xử lý ngôn ngữ tự nhiên khác nhau [22,28,21,38,39]. Thông thường, các kiến trúc encoder dựa trên CNN áp dụng một lớp tích chập duy nhất theo sau bởi một lớp pooling, về cơ bản hoạt động như một detector của các lớp h-gram cụ thể, cho cửa sổ bộ lọc tích chập có kích thước h. Kiến trúc sâu trong khung của chúng tôi sẽ, về nguyên tắc, cho phép các lớp cấp cao nắm bắt các đặc trưng ngôn ngữ phức tạp hơn. Chúng tôi sử dụng tích chập với bước thay vì các toán tử pooling, ví dụ, max-pooling, để downsampling không gian theo [24,40], nơi lập luận rằng các kiến trúc hoàn toàn tích chập có thể học downsampling không gian của riêng chúng. Hơn nữa, [41] sử dụng CNN 29 lớp cho phân loại văn bản. CNN encoder của chúng tôi đơn giản hơn đáng kể về cấu trúc (tích chập với bước và không quá 4 lớp) trong khi vẫn đạt được hiệu suất tốt.

Các decoder ngôn ngữ khác ngoài RNN ít được nghiên cứu hơn. Gần đây, [42] đề xuất mô hình hybrid bằng cách kết hợp mạng convolutional-deconvolutional với RNN, trong đó RNN hoạt động như decoder và mô hình deconvolutional như cầu nối giữa encoder (mạng tích chập) và decoder. Ngoài ra, [37,43,44,45] đã xem xét các biến thể CNN, như pixelCNN [46], để sinh văn bản. Tuy nhiên, để đạt kết quả thực nghiệm tốt, những phương pháp này vẫn yêu cầu các câu được sinh ra tuần tự, điều kiện hóa trên thông tin lịch sử ground truth, tương tự như decoder dựa trên RNN, do đó vẫn bị exposure bias.

Các nỗ lực khác đã được thực hiện để cải thiện embedding từ đoạn văn dài sử dụng các phương pháp không giám sát [2,47]. Paragraph vector [2] học một vector độ dài cố định bằng cách nối nó với word2vec [48] embedding của chuỗi lịch sử để dự đoán các từ tương lai. Hierarchical neural autoencoder [47] xây dựng RNN chú ý phân cấp, sau đó nó sử dụng các đơn vị ẩn cấp đoạn văn của RNN đó làm embedding. Công việc của chúng tôi khác với những phương pháp này ở chỗ chúng tôi buộc chuỗi phải được khôi phục hoàn toàn từ biểu diễn tiềm ẩn, mà không có sự trợ giúp từ bất kỳ thông tin lịch sử nào.

Các phương pháp trước đây đã xem xét việc tận dụng dữ liệu không nhãn cho các tác vụ phân loại chuỗi bán giám sát. Thông thường, các phương pháp dựa trên RNN xem xét hoặc i) huấn luyện sequence-to-sequence RNN autoencoder, hoặc RNN classifier mạnh mẽ với nhiễu loạn đối nghịch, làm khởi tạo cho encoder trong mô hình có giám sát [1,4]; hoặc, ii) học biểu diễn tiềm ẩn thông qua sequence-to-sequence RNN autoencoder, rồi sử dụng chúng làm đầu vào cho classifier cũng nhận đặc trưng được trích xuất từ CNN làm đầu vào [3]. Đối với các tác vụ tóm tắt, [49] đã xem xét phương pháp bán giám sát dựa trên support vector machine, tuy nhiên, cho đến nay, nghiên cứu về tóm tắt văn bản bán giám sát sử dụng các mô hình sâu còn hiếm.

4 Thực nghiệm
Thiết lập thực nghiệm Đối với tất cả thực nghiệm, chúng tôi sử dụng encoder tích chập 3 lớp theo sau bởi decoder deconvolutional 3 lớp (nhớ lại chi tiết triển khai cho lớp trên cùng). Kích thước bộ lọc, bước và word embedding được đặt là h = 5, r_l = 2, với l = 1, ..., 3 và k = 300, tương ứng. Chiều của vector biểu diễn tiềm ẩn thay đổi cho mỗi thực nghiệm, do đó được báo cáo riêng biệt. Để thuận tiện ký hiệu, chúng tôi ký hiệu convolutional-deconvolutional autoencoder của chúng tôi là CNN-DCNN. Trong hầu hết so sánh, chúng tôi cũng xem xét hai autoencoder tiêu chuẩn làm baseline: a) CNN-LSTM: encoder CNN kết hợp với decoder LSTM; và b) LSTM-LSTM: encoder LSTM với decoder LSTM. Cấu hình LSTM-DCNN không được bao gồm vì nó cho hiệu suất tương tự như CNN-DCNN trong khi tốn kém hơn về mặt tính toán. Thiết lập thực nghiệm hoàn chỉnh và chi tiết baseline được cung cấp trong Tài liệu bổ sung (SM). CNN-DCNN có số lượng tham số ít nhất. Ví dụ, sử dụng 500 làm chiều của h dẫn đến khoảng 9, 13, 15 triệu tham số có thể huấn luyện tổng cộng cho CNN-DCNN, CNN-LSTM và LSTM-LSTM, tương ứng.

Tái tạo đoạn văn Trước tiên chúng tôi điều tra hiệu suất của autoencoder đề xuất về khả năng học biểu diễn có thể bảo toàn thông tin đoạn văn. Chúng tôi áp dụng tiêu chí đánh giá từ [47], tức là, điểm ROUGE [50] và điểm BLEU [51], để đo độ gần của đoạn văn tái tạo (đầu ra mô hình) với đoạn văn đầu vào. Tóm tắt, điểm ROUGE và BLEU đo recall và precision n-gram giữa đầu ra mô hình và tham chiếu (ground-truth). Chúng tôi sử dụng BLEU-4, ROUGE-1, 2 trong đánh giá, phù hợp với [47]. Ngoài autoencoder CNN-LSTM và LSTM-LSTM, chúng tôi cũng so sánh với hierarchical LSTM autoencoder [47]. So sánh được thực hiện trên dataset Hotel Reviews, theo thiết lập thực nghiệm từ [47], tức là, chúng tôi chỉ giữ các review có độ dài câu từ 50 đến 250 từ, tạo ra 348,544 mẫu dữ liệu huấn luyện và 39,023 mẫu dữ liệu kiểm tra. Đối với tất cả so sánh, chúng tôi đặt chiều của biểu diễn tiềm ẩn là h = 500.

Từ Bảng 1, chúng ta thấy rằng đối với đoạn văn dài, decoder LSTM trong CNN-LSTM và LSTM-LSTM gặp vấn đề exposure bias nghiêm trọng. Chúng tôi tiếp tục đánh giá hiệu suất của mỗi mô hình với các độ dài đoạn văn khác nhau. Như thể hiện trong Hình 2 và Bảng 2, trong tác vụ này CNN-DCNN thể hiện lợi thế rõ ràng, đồng thời, khi độ dài câu tăng, lợi thế so sánh trở nên đáng kể hơn. Đối với các phương pháp dựa trên LSTM, chất lượng tái tạo giảm nhanh khi chuỗi trở nên dài hơn. Trái lại, chất lượng tái tạo của CNN-DCNN ổn định và nhất quán bất kể độ dài câu. Hơn nữa, chi phí tính toán, được đánh giá như thời gian thực tế, thấp hơn đáng kể trong CNN-DCNN. Đại khái, CNN-LSTM chậm hơn CNN-DCNN 3 lần, và LSTM-LSTM chậm hơn 5 lần trên GPU đơn lẻ. Chi tiết được báo cáo trong SM.

Sửa lỗi cấp ký tự và cấp từ Tác vụ này tìm cách đánh giá liệu decoder deconvolutional có thể vượt qua exposure bias, điều này hạn chế nghiêm trọng các decoder dựa trên LSTM. Chúng tôi xem xét denoising autoencoder trong đó đầu vào bị điều chỉnh nhẹ với một số sửa đổi nhất định, trong khi mô hình cố gắng khử nhiễu (sửa) sửa đổi chưa biết, do đó khôi phục câu gốc. Đối với sửa lỗi cấp ký tự, chúng tôi xem xét dataset Yahoo! Answer [52]. Mô tả dataset và thiết lập cho sửa lỗi cấp từ được cung cấp trong SM. Chúng tôi làm theo thiết lập thực nghiệm trong [53] cho sửa lỗi chính tả cấp từ và cấp ký tự (xem chi tiết trong SM). Chúng tôi xem xét việc thay thế mỗi từ/ký tự bằng một từ/ký tự khác một cách ngẫu nhiên với xác suất ε, với ε = 0.30. Đối với phân tích cấp ký tự, trước tiên chúng tôi ánh xạ tất cả ký tự vào vector nhúng 40 chiều, với cấu trúc mạng cho mô hình cấp từ và cấp ký tự được giữ giống nhau.

Chúng tôi sử dụng Character Error Rate (CER) [53] và Word Error Rate (WER) [54] để đánh giá. WER/CER đo tỷ lệ khoảng cách Levenshtein (tức là khoảng cách chỉnh sửa) giữa dự đoán mô hình và ground-truth, và tổng độ dài của chuỗi. Về mặt khái niệm, WER/CER thấp hơn cho thấy hiệu suất tốt hơn. Chúng tôi sử dụng denoising autoencoder LSTM-LSTM và CNN-LSTM để so sánh. Kiến trúc cho các mô hình baseline cấp từ giống như trong thực nghiệm trước. Đối với sửa lỗi cấp ký tự, chúng tôi đặt chiều của h là 900. Chúng tôi cũng so sánh với huấn luyện actor-critic [53], theo hướng dẫn thực nghiệm của họ (xem chi tiết trong SM).

Như thể hiện trong Hình 3 và Bảng 3, chúng tôi quan sát thấy CNN-DCNN đạt được cả CER thấp hơn và hội tụ nhanh hơn. Hơn nữa, CNN-DCNN mang lại hiệu suất khử nhiễu ổn định bất kể vị trí nhiễu trong câu, như thấy trong Hình 4. Đối với CNN-DCNN, ngay cả khi lỗi được phát hiện nhưng không được sửa chính xác (màu sẫm hơn trong Hình 4 cho thấy độ không chắc chắn cao hơn), việc khử nhiễu với các từ tương lai không bị ảnh hưởng, trong khi đối với CNN-LSTM và LSTM-LSTM lỗi tích lũy dần với chuỗi dài hơn, như mong đợi.

Đối với sửa lỗi cấp từ, chúng tôi xem xét chỉ thay thế từ, và nhiễu loạn hỗn hợp từ ba loại: thay thế, xóa và chèn. Nói chung, CNN-DCNN vượt trội hơn CNN-LSTM và LSTM-LSTM, và nhanh hơn. Chúng tôi cung cấp chi tiết thực nghiệm và kết quả so sánh trong SM.

Phân loại chuỗi bán giám sát & tóm tắt Chúng tôi điều tra liệu khung CNN-DCNN của chúng tôi có thể cải thiện các tác vụ ngôn ngữ tự nhiên có giám sát tận dụng đặc trưng học từ đoạn văn hay không. Về nguyên tắc, một trích xuất đặc trưng không giám sát tốt sẽ cải thiện khả năng tổng quát hóa trong thiết lập học bán giám sát. Chúng tôi đánh giá phương pháp của chúng tôi trên ba tác vụ ngôn ngữ tự nhiên phổ biến: phân tích tình cảm, dự đoán chủ đề đoạn văn và tóm tắt văn bản. Hai tác vụ đầu tiên về cơ bản là phân loại chuỗi, trong khi tóm tắt liên quan đến cả hiểu ngôn ngữ và sinh ngôn ngữ.

Chúng tôi xem xét ba dataset phân loại tài liệu quy mô lớn: DBPedia, Yahoo! Answers và Yelp Review Polarity [52]. Việc phân chia tập huấn luyện, xác thực và kiểm tra cho tất cả dataset theo thiết lập từ [52]. Thống kê tóm tắt chi tiết của tất cả dataset được hiển thị trong SM. Để chứng minh lợi thế của việc kết hợp mục tiêu tái tạo vào huấn luyện classifier văn bản, chúng tôi tiếp tục đánh giá mô hình của chúng tôi với các lượng dữ liệu có nhãn khác nhau (0.1%, 0.15%, 0.25%, 1%, 10% và 100%, tương ứng), và toàn bộ tập huấn luyện làm dữ liệu không nhãn.

Đối với mô hình baseline hoàn toàn có giám sát của chúng tôi (CNN có giám sát), chúng tôi sử dụng cùng kiến trúc encoder tích chập được mô tả ở trên, với chiều biểu diễn tiềm ẩn 500 chiều, theo sau bởi classifier MLP với một lớp ẩn có 300 đơn vị ẩn. Tỷ lệ dropout được đặt là 50%. Word embedding được khởi tạo ngẫu nhiên.

Như thể hiện trong Bảng 4, chiến lược huấn luyện kết hợp liên tục và đáng kể vượt trội chiến lược hoàn toàn có giám sát trên các dataset, ngay cả khi tất cả nhãn đều có sẵn. Chúng tôi giả thuyết rằng trong giai đoạn đầu của huấn luyện, khi tái tạo được nhấn mạnh, đặc trưng từ các đoạn văn bản có thể được học dễ dàng. Khi huấn luyện tiến triển, các đặc trưng đoạn văn bản mang tính phân biệt nhất được lựa chọn. Hơn nữa, tập con đặc trưng chịu trách nhiệm cho cả tái tạo và phân biệt có lẽ đóng gói cấu trúc phụ thuộc dài hơn, so với đặc trưng sử dụng chiến lược hoàn toàn có giám sát. Hình 5 thể hiện hành vi của mô hình chúng tôi trong thiết lập bán giám sát trên dataset Yelp Review. Kết quả cho Yahoo! Answer và DBpedia được cung cấp trong SM.

Đối với tóm tắt, chúng tôi sử dụng dataset gồm 58,000 cặp abstract-title, từ arXiv. Các cặp abstract-title được chọn nếu độ dài của title và abstract không vượt quá 50 và 500 từ, tương ứng. Chúng tôi phân chia tập huấn luyện, xác thực và kiểm tra thành 55000, 2000, 1000 cặp mỗi tập. Chúng tôi huấn luyện mô hình sequence-to-sequence để sinh title cho trước abstract, sử dụng tập con dữ liệu được ghép cặp được chọn ngẫu nhiên với tỷ lệ α = (5%; 10%; 50%; 100%). Đối với mọi giá trị α, chúng tôi xem xét cả tóm tắt hoàn toàn có giám sát chỉ sử dụng cặp abstract-title, và tóm tắt bán giám sát, bằng cách tận dụng abstract bổ sung không có title. Chúng tôi so sánh LSTM và mạng deconvolutional làm decoder để sinh title cho α = 100%.

Bảng 5 tóm tắt kết quả định lượng sử dụng metric ROUGE-L (longest common subsequence) [50]. Nói chung, các abstract bổ sung không có title cải thiện khả năng tổng quát hóa trên tập kiểm tra. Thú vị thay, ngay cả khi α = 100% (tất cả title đều được quan sát), mục tiêu huấn luyện kết hợp vẫn mang lại hiệu suất tốt hơn so với chỉ sử dụng L_sup. Có lẽ, vì mục tiêu huấn luyện kết hợp yêu cầu biểu diễn tiềm ẩn phải có khả năng tái tạo đoạn văn đầu vào, ngoài việc sinh title, biểu diễn học được có thể nắm bắt tốt hơn toàn bộ cấu trúc (ý nghĩa) của đoạn văn. Chúng tôi cũng quan sát thực nghiệm rằng các title được sinh ra dưới mục tiêu huấn luyện kết hợp có nhiều khả năng sử dụng các từ xuất hiện trong đoạn văn tương ứng (tức là, extractive hơn), trong khi các title được sinh ra sử dụng mục tiêu hoàn toàn có giám sát L_sup, có xu hướng sử dụng từ ngữ tự do hơn, do đó abstractive hơn. Một giải thích có thể là, đối với chiến lược huấn luyện kết hợp, vì đoạn văn tái tạo và title đều được sinh ra từ biểu diễn tiềm ẩn h, các đoạn văn bản được sử dụng để tái tạo đoạn văn đầu vào có nhiều khả năng được tận dụng khi "xây dựng" title, do đó title mang nhiều điểm tương đồng với đoạn văn đầu vào.

Như mong đợi, các title được tạo ra bởi decoder deconvolutional ít mạch lạc hơn decoder LSTM. Có lẽ, vì mỗi đoạn văn có thể được tóm tắt bằng nhiều title hợp lý, decoder deconvolutional có thể gặp khó khăn khi định vị các đoạn văn bản. Chúng tôi cung cấp thảo luận và title được sinh ra dưới các thiết lập khác nhau trong SM. Thiết kế khung kết hợp những gì tốt nhất của hai thế giới này, LSTM cho sinh và CNN cho giải mã, sẽ là một hướng tương lai thú vị.

5 Kết luận
Chúng tôi đề xuất khung tổng quát cho mô hình văn bản sử dụng hoàn toàn các phép toán tích chập và deconvolutional. Phương pháp đề xuất không có sinh điều kiện tuần tự, tránh các vấn đề liên quan đến exposure bias và huấn luyện teacher forcing. Phương pháp của chúng tôi cho phép mô hình đóng gói hoàn toàn một đoạn văn thành vector biểu diễn tiềm ẩn, có thể được giải nén để tái tạo chuỗi đầu vào gốc. Thông qua thực nghiệm, phương pháp đề xuất đạt được chất lượng tái tạo đoạn văn dài xuất sắc và vượt trội các thuật toán hiện có trong sửa lỗi chính tả, và phân loại chuỗi bán giám sát và tóm tắt, với chi phí tính toán giảm đáng kể.
