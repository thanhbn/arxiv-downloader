# Huấn luyện Mixture-of-Expert bền vững cho mạng nơ-ron tích chập

Yihua Zhang1 Ruisi Cai2 Tianlong Chen2,3,4,5 Guanhua Zhang6 Huan Zhang7,8
Pin-Yu Chen9 Shiyu Chang6 Zhangyang Wang2 Sijia Liu1,9

1Đại học Bang Michigan, 2Đại học Texas tại Austin,
3Đại học Bắc Carolina tại Chapel Hill, 4MIT, 5Đại học Harvard,
6UC Santa Barbara, 7Đại học Carnegie Mellon, 8UIUC, 9IBM Research

## Tóm tắt

Sparsely-gated Mixture of Expert (MoE), một kiến trúc mô hình sâu đang nổi lên, đã thể hiện tiềm năng lớn trong việc cho phép suy luận mô hình với độ chính xác cao và hiệu quả siêu việt. Mặc dù MoE ngày càng phổ biến, ít công trình nghiên cứu tiềm năng của nó trong việc cải tiến mạng nơ-ron tích chập (CNN), đặc biệt là trong mặt phẳng tính bền vững đối kháng. Vì việc thiếu tính bền vững đã trở thành một trong những rào cản chính đối với CNN, trong bài báo này chúng tôi đặt câu hỏi: Làm thế nào để tăng cường tính bền vững đối kháng cho một mô hình MoE dựa trên CNN? Liệu chúng ta có thể huấn luyện bền vững nó giống như một mô hình CNN thông thường? Nghiên cứu thí điểm của chúng tôi cho thấy rằng cơ chế huấn luyện đối kháng thông thường (AT) (được phát triển cho CNN vanilla) không còn hiệu quả để tăng cường tính bền vững cho MoE-CNN. Để hiểu rõ hơn hiện tượng này, chúng tôi phân tích tính bền vững của MoE-CNN thành hai chiều: Tính bền vững của bộ định tuyến (tức là các hàm gating để chọn các chuyên gia cụ thể cho dữ liệu) và tính bền vững của các chuyên gia (tức là các đường dẫn được bộ định tuyến hướng dẫn được định nghĩa bởi các mạng con của CNN backbone). Phân tích của chúng tôi cho thấy rằng các bộ định tuyến và chuyên gia khó thích ứng với nhau trong AT vanilla. Do đó, chúng tôi đề xuất một framework huấn luyện đối kháng xen kẽ bộ định tuyến-chuyên gia mới cho MoE, được gọi là ADVMOE. Hiệu quả của đề xuất của chúng tôi được chứng minh trên 4 kiến trúc mô hình CNN thường được sử dụng trên 4 bộ dữ liệu benchmark. Chúng tôi thấy rằng ADVMOE đạt được cải thiện tính bền vững đối kháng 1%∼4% so với CNN dày đặc gốc, và tận hưởng lợi ích hiệu quả của MoE sparsity-gated, dẫn đến giảm hơn 50% chi phí suy luận. Code có sẵn tại https://github.com/OPTML-Group/Robust-MoE-CNN .

## 1. Giới thiệu

Mặc dù hiệu suất tiên tiến được đạt bởi các mạng cực kỳ lớn trong nhiều tác vụ học sâu khác nhau, vẫn còn thách thức trong việc huấn luyện và triển khai các mô hình như vậy một cách rẻ tiền. Một nút thắt cổ chai chính là thiếu hiệu quả tham số: Một dự đoán dữ liệu đơn lẻ chỉ cần kích hoạt một phần nhỏ các tham số của mô hình đầy đủ. Hướng tới học sâu hiệu quả, Mixture of Experts (MoE) thưa thớt nhằm phân chia và chinh phục các tham số mô hình dựa trên phản ứng tối ưu của chúng đối với các đầu vào cụ thể để chi phí suy luận có thể được giảm. Một cấu trúc MoE điển hình bao gồm một tập hợp các 'chuyên gia' (tức là các mô hình con được trích xuất từ mạng backbone gốc) và 'bộ định tuyến' (tức là các mạng gating quy mô nhỏ bổ sung để xác định các sơ đồ lựa chọn chuyên gia qua các lớp). Trong quá trình suy luận, MoE thưa thớt chỉ kích hoạt các chuyên gia có liên quan nhất và tạo thành đường dẫn được chuyên gia hướng dẫn cho một dữ liệu đầu vào nhất định. Bằng cách này, MoE thưa thớt có thể thúc đẩy hiệu quả suy luận (xem đo lường 'GFLOPS' trong Hình 1). Về mặt kiến trúc, MoE thưa thớt đã được sử dụng cho cả CNN và vision transformers (ViT). Tuy nhiên, chúng tôi sẽ tập trung vào trường hợp trước vì MoE thưa thớt cho CNN được khám phá ít hơn so với MoE không thưa thớt cho CNN, và tính bền vững đối kháng (một chỉ số hiệu suất chính khác của công trình của chúng tôi) được nghiên cứu rộng rãi trong bối cảnh CNN.

Biết rằng một điểm yếu chính của học sâu là thiếu tính bền vững đối kháng. Ví dụ, CNN có thể dễ dàng bị lừa bởi các cuộc tấn công đối kháng, về mặt nhiễu đầu vào nhỏ được tạo ra để dẫn đến dự đoán sai lầm. Do đó, huấn luyện đối kháng (AT) của CNN đã trở thành một hướng nghiên cứu chính. Tuy nhiên, khi CNN gặp MoE thưa thớt, vẫn còn bí ẩn liệu hiệu quả suy luận được cải thiện do MoE thưa thớt mang lại có phải trả giá bằng các công thức huấn luyện đối kháng phức tạp hơn không. Do đó, chúng tôi hỏi:

**(Q) Những hiểu biết mới về tính bền vững đối kháng của CNN tích hợp MoE thưa thớt sẽ là gì? Và cơ chế AT phù hợp sẽ là gì?**

Theo hiểu biết tốt nhất của chúng tôi, vấn đề (Q) vẫn chưa được giải quyết trong tài liệu. Công trình liên quan nhất đến của chúng tôi là [30], đã điều tra tính bền vững đối kháng của MoE và tận dụng công thức AT thông thường được phát triển cho việc phòng vệ chống lại các cuộc tấn công đối kháng. Tuy nhiên, nó chỉ tập trung vào kiến trúc ViT, tạo ra khoảng trống cho nghiên cứu về robustification cho CNN dựa trên MoE thưa thớt (được gọi là MoE-CNN trong công trình này). Quan trọng nhất, chúng tôi thấy rằng AT vanilla (được sử dụng rộng rãi để robustify CNN) không còn hiệu quả cho MoE-CNN. Do đó, các giải pháp mới là cần thiết.

Để giải quyết (Q), chúng tôi cần (1) thực hiện kiểm tra tính hợp lý cẩn thận cho AT trong MoE-CNN, (2) thực hiện phân tích sâu về các trường hợp thất bại của nó, và (3) đưa ra các nguyên tắc AT mới có thể cải thiện hiệu quả tính bền vững mà không mất khả năng tổng quát hóa và hiệu quả từ MoE thưa thớt. Cụ thể, các đóng góp của chúng tôi được triển khai như sau:

• Chúng tôi phân tích tính bền vững MoE thành hai chiều mới (khác với CNN): tính bền vững của bộ định tuyến và tính bền vững của các chuyên gia. Việc phân tích tính bền vững như vậy mang lại những hiểu biết mới về tính (không) hiệu quả của AT.

• Lấy cảm hứng từ việc phân tích tính bền vững trên, chúng tôi đề xuất một framework huấn luyện đối kháng mới cho MoE, được gọi là ADVMOE, buộc các bộ định tuyến và chuyên gia phải nỗ lực phối hợp để cải thiện tính bền vững tổng thể của MoE-CNN.

• Chúng tôi thực hiện các thí nghiệm rộng rãi để chứng minh hiệu quả của ADVMOE trên 4 kiến trúc CNN và 4 bộ dữ liệu. Ví dụ, ADVMOE vượt trội hơn AT trên mô hình CNN dày đặc gốc (được gọi là Dense) với biên độ đáng kể: cải thiện tính bền vững đối kháng 1%∼4% và giảm hơn 50% chi phí suy luận; xem Hình 1 để minh họa về các loại CNN khác nhau và hiệu suất nổi bật đạt được.

## 2. Công trình liên quan

**Sparsely-activated Mixture of Experts (Sparse MoE).** Như một trường hợp đặc biệt của kiến trúc nơ-ron tổng hợp, MoE nhằm giải quyết các tác vụ ML theo cách chia để trị, tạo ra một chuỗi các mô hình con (được gọi là các chuyên gia) và thực hiện dự đoán phụ thuộc đầu vào bằng cách kết hợp đầu ra của các mô hình con. Như một nhánh quan trọng của MoE, sparsely gated MoE chỉ kích hoạt một tập hợp con các chuyên gia dựa trên một hệ thống định tuyến. Lợi thế chính do sparse MoE mang lại nằm ở chi phí suy luận tăng dưới tuyến tính (FLOPs) so với quy mô mô hình (số lượng tham số). Trong lĩnh vực thị giác, phần lớn các công trình hiện tại tập trung vào thiết kế MoE cho ViT, để lại MoE cho CNN ít được khám phá. Theo hiểu biết tốt nhất của chúng tôi, DeepMoE là công trình gần đây nhất nghiên cứu có hệ thống việc tích hợp MoE với CNN, nhưng hạn chế trong mô hình huấn luyện tiêu chuẩn (không bền vững). Trong khi đó, cũng tồn tại các công trình khác liên quan đến MoE-CNN, nhưng chúng hoặc rơi ra khỏi phạm vi MoE "thưa thớt" hoặc không mang lại lợi ích hiệu quả. Ngược lại, chúng tôi tập trung vào thiết lập MoE-CNN thúc đẩy hiệu quả trong suốt công trình này.

**Tính bền vững đối kháng.** CNN nổi tiếng dễ bị tổn thương bởi các mẫu đối kháng không thể cảm nhận được và do đó việc huấn luyện các mô hình bền vững đối kháng đã trở thành một trọng tâm nghiên cứu chính trong nhiều lĩnh vực. Hầu hết các phương pháp huấn luyện bền vững được mở rộng từ huấn luyện đối kháng dựa trên tối ưu hóa min-max. Ví dụ, công trình [25] tìm kiếm sự cân bằng tối ưu giữa tính bền vững và khả năng tổng quát hóa tiêu chuẩn. Công trình khác nhằm cắt giảm chi phí tính toán của huấn luyện bền vững trong khi duy trì tính bền vững. Công trình [30] nghiên cứu tính bền vững của kiến trúc dựa trên MoE lần đầu tiên. Tuy nhiên, trọng tâm của nó vẫn là MoE cho ViT và mối quan hệ giữa dung lượng mô hình và tính bền vững.

## 3. Phát biểu vấn đề

Trong phần này, chúng tôi bắt đầu bằng việc trình bày thiết lập MoE-CNN trong công trình này và sau đó giới thiệu mô hình học bền vững. Việc thiếu tính bền vững đối kháng của các mô hình sâu thúc đẩy chúng tôi điều tra liệu phương pháp huấn luyện đối kháng (AT) được thiết kế cho CNN vanilla có giữ hiệu quả cho MoE-CNN hay không. Thông qua một ví dụ động lực, chúng tôi cho thấy rằng công thức AT thông thường không có khả năng trang bị cho MoE-CNN tính bền vững mong muốn. Hiệu suất kết quả thậm chí còn tệ hơn so với S-Dense được sản xuất bởi AT, có dung lượng mô hình nhỏ hơn nhiều so với MoE-CNN. Do đó, câu hỏi về cách robustify MoE-CNN nảy sinh.

**Thiết lập mô hình.** Chúng tôi xem xét một MoE có backbone CNN bao gồm nhiều lớp MoE. Mỗi lớp MoE bao gồm một bộ định tuyến và một lớp tích chập vanilla từ mô hình CNN backbone. Trong một lớp MoE, chúng tôi định nghĩa N chuyên gia, mỗi chuyên gia chọn một tập hợp con các kênh từ lớp tích chập. Cụ thể, giả sử lớp thứ l chứa Cl kênh, một chuyên gia sẽ chứa r×Cl kênh, trong đó chúng tôi gọi tỷ lệ r∈[0,1] là quy mô mô hình và giữ nó giống nhau qua các lớp khác nhau (xem Hình 1a). Đáng chú ý rằng khi r tăng, dung lượng mô hình trên mỗi chuyên gia tăng (tức là với nhiều tham số hơn) với chi phí giảm hiệu quả. Trong một đường dẫn tiến, bộ định tuyến đầu tiên thực hiện lựa chọn chuyên gia cụ thể cho đầu vào. Những chuyên gia được chọn theo lớp này sau đó tạo thành một đường dẫn end-to-end để xử lý đầu vào này. Chúng tôi sử dụng "đường dẫn" để mô tả một đường dẫn tiến được chuyên gia hướng dẫn (xem Hình 1a). Chúng tôi tóm tắt thiết lập mô hình trong Hình A1.

Hơn nữa, chúng tôi giới thiệu các loại mô hình khác nhau được xem xét trong công trình này và được thể hiện trong Hình 1a. Đầu tiên, chúng tôi gọi mô hình CNN dày đặc gốc là 'Dense', phục vụ như cơ sở mô hình cho các loại mô hình khác được dẫn xuất từ đó. Thứ hai, chúng tôi trực tiếp thu nhỏ số lượng kênh của mỗi lớp trong Dense (dựa trên tham số quy mô mô hình r) để có được mô hình 'small dense' (được gọi là 'S-Dense'). Đáng chú ý, S-Dense có kích thước tương đương với một đường dẫn đơn trong MoE-CNN. Thứ ba, chúng tôi sử dụng phương pháp pruning có cấu trúc để tạo ra một mạng con thưa thớt từ Dense, với tỷ lệ trọng số còn lại giống như tham số quy mô mô hình r trong MoE-CNN, mà chúng tôi gọi là 'Sparse-CNN'. Tóm lại, S-Dense có dung lượng mô hình nhỏ nhất (có thể so sánh với một đường dẫn đơn của MoE-CNN), và nên cung cấp giới hạn dưới hiệu suất cho MoE-CNN. Ngược lại, Sparse-CNN có dung lượng mô hình lớn hơn nhưng nhỏ hơn MoE-CNN vì nó mã hóa một đường dẫn data-agnostic của Dense, trong khi MoE-CNN tạo ra các đường dẫn data-specific ở cùng quy mô. Dense có dung lượng mô hình lớn nhất nhưng hiệu quả suy luận ít nhất.

**Tính bền vững đối kháng: Từ CNN đến MoE-CNN.** Đã biết rằng các mô hình học máy hiện tại (ví dụ, CNN) dễ bị tổn thương bởi các cuộc tấn công đối kháng. Hướng tới thiết kế bền vững, nhiều phương pháp AT (huấn luyện đối kháng) đã được phát triển. Những phương pháp chủ yếu bao gồm AT vanilla dựa trên tối ưu hóa min-max và biến thể TRADES của nó cân bằng giữa tổng quát hóa và tính bền vững đối kháng. Trong suốt bài báo, chúng tôi áp dụng TRADES làm công thức AT thông thường mặc định, giải quyết vấn đề sau:

min_θ E_(x,y)∈D [ℓ(θ;x,y) + (1/λ)max_{||δ||_∞≤ε} ℓ_KL(f_θ(x), f_θ(x+δ))]     (AT)

trong đó θ biểu thị các tham số mô hình cần được robustified, (x,y)∈D là một mẫu huấn luyện, được rút ra từ tập huấn luyện D, với đặc trưng đầu vào x và nhãn y, ℓ(θ,x;y) biểu thị mất mát cross-entropy sử dụng mô hình θ tại điểm dữ liệu (x,y), δ ký hiệu biến nhiễu đầu vào tuân theo quả cầu ℓ_∞-norm bán kính ε, f_θ(·) biểu thị dự đoán của mô hình, ℓ_KL là mất mát phân kỳ KL đặc trưng cho độ ổn định dự đoán trường hợp xấu nhất khi có mặt δ, và λ>0 là tham số chính quy hóa để tạo ra sự đánh đổi giữa tối thiểu hóa rủi ro thực nghiệm và tính bền vững của dự đoán mô hình.

Mặc dù AT đã được nghiên cứu kỹ lưỡng về tính bền vững đối kháng của CNN, tồn tại ít nỗ lực robustify MoE-CNN. Điều này đặt ra vấn đề quan tâm của chúng tôi:

**(Phát biểu vấn đề) Liệu MoE-CNN có thể được robustified hiệu quả như một CNN thông thường bằng cách sử dụng AT không? Nếu không, làm thế nào để huấn luyện MoE-CNN một cách bền vững để đạt được tính bền vững không tệ hơn S-Dense, Sparse-CNN và Dense theo hướng AT trong khi bảo tồn hiệu quả của MoE?**

**Nghiên cứu khởi động: AT cho MoE-CNN không tầm thường.** Mục tiêu robustify MoE-CNN của chúng tôi bao gồm (1) đạt được tính bền vững cao, (2) duy trì độ chính xác dự đoán cao, và (3) tận dụng đầy đủ định tuyến MoE để giữ hiệu quả cao và tính biểu đạt của mô hình. Tuy nhiên, hệ thống định tuyến trong MoE mang lại những thách thức robustification bổ sung, chưa bao giờ tồn tại trong CNN thông thường. Cụ thể, việc lựa chọn chuyên gia cụ thể đầu vào trong MoE có thể làm cho kẻ tấn công dễ dàng thành công hơn, vì nhiễu đầu vào có thể hoặc dẫn dắt sai các bộ định tuyến để chọn các chuyên gia không chính xác hoặc lừa dối bộ dự đoán được chỉ định đường dẫn. Chế độ 'tấn công hai chiều' như vậy làm cho AT cho MoE-CNN rất không tầm thường.

Hình 2 chứng minh thực nghiệm rằng việc áp dụng trực tiếp (AT) cho MoE-CNN là có vấn đề. Trong Hình 2, chúng tôi xem xét ResNet-18 làm backbone mô hình (Dense) và CIFAR-10 cho phân loại hình ảnh. Chúng tôi áp dụng (AT) để huấn luyện MoE-CNN và S-Dense, và báo cáo độ chính xác bền vững (RA), tức là độ chính xác thời gian kiểm tra trên các ví dụ đối kháng được tạo bởi các cuộc tấn công PGD 50 bước, đối với các cường độ tấn công ε khác nhau. Như chúng ta có thể thấy, mặc dù MoE-CNN có dung lượng mô hình lớn hơn nhiều so với S-Dense, nó dẫn đến một sự sụt giảm RA đáng kể khi phương pháp AT thông thường được áp dụng. Điều này ngụ ý rằng việc thiết kế AT cho MoE-CNN còn xa mới tầm thường. Do đó cần một giao thức học bền vững mới để cải thiện tính bền vững của MoE-CNN mà không mất các ưu điểm của nó về hiệu quả và tổng quát hóa.

## 4. Phương pháp

Trong phần này, chúng tôi bắt đầu bằng việc tìm hiểu trường hợp thất bại của (AT) trong MoE-CNN bằng cách hiểu vai trò của các bộ định tuyến và đường dẫn trong (AT). Chúng tôi chứng minh thực nghiệm rằng các thành phần riêng lẻ này khó thích ứng với nhau và không thể tạo ra nỗ lực phối hợp trong AT. Dựa trên điều đó, chúng tôi phát triển một framework AT mới cho MoE-CNN, ADVMOE, cũng lấy cảm hứng từ tối ưu hóa bi-level.

**Phân tích tính bền vững của MoE-CNN: Tính bền vững của bộ định tuyến vs. tính bền vững của đường dẫn.** Câu đố chính trong việc robustifying MoE-CNN đến từ sự kết hợp giữa tính bền vững của các bộ định tuyến (chịu trách nhiệm lựa chọn chuyên gia qua các lớp) và tính bền vững của các đường dẫn MoE cụ thể đầu vào (chịu trách nhiệm dự đoán cuối cùng của một đầu vào). Với trường hợp thất bại của AT cho MoE-CNN trong Hình 2, chúng tôi cần hiểu vai trò của các bộ định tuyến và đường dẫn trong AT, tức là tính bền vững đối kháng của MoE-CNN được đạt như thế nào khi có mặt 'chế độ tấn công hai chiều'. Để làm điều này, chúng tôi bắt đầu bằng việc đánh giá ảnh hưởng của tính bền vững của bộ định tuyến đối với tính bền vững tổng thể. Điều này cũng được lấy cảm hứng từ tài liệu pruning gần đây cho thấy rằng tính bền vững mô hình có thể được đạt được chỉ từ tôpô thưa thớt của mạng (bất kể trọng số mô hình). Do đó chúng tôi hỏi:

**(Q1) Việc cải thiện tính bền vững của bộ định tuyến có đủ để đạt được một MoE-CNN bền vững không?**

Để giải quyết (Q1), đầu tiên chúng tôi chia các tham số của MoE-CNN (tức là θ) thành hai phần, các tham số của bộ định tuyến φ và các tham số của mạng backbone ψ. Điều này tạo ra θ = [φ⊤,ψ⊤]⊤, trong đó ⊤ là phép toán chuyển vị. Sau đó chúng tôi gọi (AT) để huấn luyện bền vững các bộ định tuyến (φ) nhưng cố định mạng backbone (ψ) tại các trọng số được huấn luyện trước tiêu chuẩn. Chúng tôi ký hiệu mô hình được robustified một phần này bằng θ̄ = [φ̄⊤,ψ⊤]⊤, trong đó ¯ biểu thị các tham số đã cập nhật. Để trả lời (Q1), chúng tôi đánh giá lợi ích tính bền vững của θ̄ so với 3 baseline (M1-M3): (M1) MoE-CNN tiêu chuẩn θ, (M2) S-Dense được robustified bằng AT, và (M3) Sparse-CNN đạt được bằng phương pháp học mặt nạ thưa thớt nhận thức tính bền vững trên mô hình Dense gốc.

Hình 3 cho thấy so sánh tính bền vững của MoE-CNN được robustified bộ định tuyến (tức là θ̄) và so sánh hiệu suất của nó với các mô hình baseline khác. Như chúng ta có thể thấy, bộ định tuyến robustified cải thiện tính bền vững tổng thể (ví dụ, 37.64% cho θ̄ với quy mô mô hình 0.5) so với MoE-CNN không được bảo vệ (M1: 0%) và mặt nạ robustified (M3: 20.04%). Tuy nhiên, vẫn có khoảng cách tính bền vững lớn so với S-Dense được robustified bằng (AT) (M2: 47.68%). Dựa trên kết quả trên, chúng tôi có được hiểu biết đầu tiên về (Q1):

**Hiểu biết 1: Robustifying các bộ định tuyến cải thiện tính bền vững tổng thể của MoE-CNN nhưng không hiệu quả như S-Dense kết quả từ AT.**

Dựa trên Hiểu biết 1, chúng tôi tiếp tục tìm hiểu khả năng phục hồi của các quyết định lựa chọn chuyên gia đối với các ví dụ đối kháng. Nếu các lựa chọn chuyên gia trong tất cả các lớp MoE giữ nguyên khi có mặt nhiễu đối kháng, chúng tôi nói rằng hệ thống định tuyến của MoE-CNN bền vững chống lại ví dụ đối kháng này. Sau đó chúng tôi chia các ví dụ đối kháng thành bốn loại theo việc chúng có thành công tấn công các bộ định tuyến và các đường dẫn hướng bộ định tuyến hay không: ❶ tấn công không thành công cả bộ định tuyến và đường dẫn MoE, ❷ tấn công thành công bộ định tuyến nhưng không phải đường dẫn MoE, ❸ tấn công thành công đường dẫn MoE nhưng không phải bộ định tuyến, và ❹ tấn công thành công cả bộ định tuyến và đường dẫn MoE. Ở đây ❶+❸ đặc trưng cho tính bền vững của bộ định tuyến, trong khi ❶+❷ đại diện cho tính bền vững của MoE. Do đó, nếu ❷ hoặc ❸ chiếm một phần lớn các ví dụ đối kháng được tạo, nó ngụ ý rằng tính bền vững của bộ định tuyến không trực tiếp tác động đến tính bền vững của bộ dự đoán dựa trên đường dẫn MoE. Hình 4 cho thấy các loại ❶-❹ trên khi tấn công MoE-CNN được robustified bộ định tuyến (tức là θ̄). Như chúng ta có thể thấy, tính bền vững của bộ định tuyến thực sự cải thiện tính bền vững dự đoán (như được thể hiện bằng 31.74% các cuộc tấn công không thành công chống lại bộ dự đoán MoE trong ❶). Tuy nhiên, trong tổng số các cuộc tấn công không thành công chống lại bộ định tuyến (tức là ❶+❸ = 76.27%), hơn một nửa trong số chúng thành công lừa dối bộ dự đoán MoE (tức là ❸>❶). Các kết quả trên cung cấp cho chúng tôi một hiểu biết bổ sung:

**Hiểu biết 2: Cải thiện tính bền vững của bộ định tuyến không đủ để bộ dự đoán MoE đạt được tính bền vững thỏa mãn mặc dù trước đây tạo ra tác động tích cực.**

Cả Hiểu biết 1 và Hiểu biết 2 đều chỉ ra rằng chỉ cải thiện tính bền vững của bộ định tuyến là không đủ để có được tính bền vững mong muốn cho MoE-CNN tổng thể. Do đó, chúng tôi tiếp theo hỏi:

**(Q2) Với mô hình được robustified bộ định tuyến θ̄, chúng ta có thể trang bị cho θ̄ tính bền vững bổ sung bằng cách huấn luyện bền vững trọng số chuyên gia (ψ) không? Và điều này tác động như thế nào đến các bộ định tuyến?**

Để trả lời (Q2), chúng tôi gọi (AT) để tiếp tục huấn luyện bền vững mạng backbone ψ trên đầu mô hình được robustified bộ định tuyến θ̄. Chúng tôi ký hiệu mô hình kết quả bằng θ̄̄ = [φ̄⊤,ψ̄⊤]. Hình 5 cho thấy sự phân tích tính bền vững của θ̄̄ trong cùng thiết lập của Hình 4. Rõ ràng, tính bền vững dự đoán tổng thể (❶+�②) được tăng cường thêm sau khi cập nhật θ̄ thành θ̄̄. Do đó, các lợi ích về tính bền vững của trọng số chuyên gia thực sự giúp cải thiện thêm tính bền vững tổng thể. Tuy nhiên, điều này dẫn đến một sự sụt giảm đáng ngạc nhiên trong tính bền vững của bộ định tuyến (❶+❸) khi so sánh θ̄̄ với θ̄. Điều này cho thấy rằng tính bền vững của bộ định tuyến không được bảo tồn tự động nếu các chuyên gia được cập nhật. Chúng tôi có được hiểu biết sau về (Q2):

**Hiểu biết 3: Robustifying bộ định tuyến và trọng số MoE có thể mang lại lợi ích bổ sung nhưng sự không thích ứng của tính bền vững bộ định tuyến với tính bền vững MoE ngăn cản AT đạt được cải thiện tính bền vững đáng kể.**

**ADVMOE: AT xen kẽ bộ định tuyến-chuyên gia thông qua quan điểm tối ưu hóa bi-level.** Như được chiếu sáng bởi các hiểu biết trên, chúng tôi cung cấp lý do cho tính không hiệu quả của AT trong việc robustifying MoE-CNN. Hiểu biết 1-2 cho thấy rằng tính bền vững của bộ định tuyến (φ) và tính bền vững của bộ dự đoán dựa trên MoE (ψ) được đan xen và mối tương quan của chúng không tầm thường. Kết quả là, tối ưu hóa bền vững đơn cấp (không lồi) trên toàn bộ tham số mô hình (φ,ψ) gặp khó khăn trong việc đồng tối ưu hóa các bộ định tuyến và đường dẫn dự đoán MoE để đạt được lợi ích tính bền vững bổ sung tốt nhất, như được hỗ trợ bởi Hiểu biết 3. Một yếu tố tối ưu hóa chính bị thiếu trong AT cho MoE-CNN là khả năng không thể mô hình hóa và tối ưu hóa sự kết hợp giữa tính bền vững của bộ định tuyến và tính bền vững của đường dẫn MoE. Không có thiết kế tối ưu hóa như vậy, AT khó có thể robustify các bộ định tuyến và đường dẫn MoE theo chế độ hợp tác và thích ứng.

Thúc đẩy bởi điều trên, chúng tôi phát triển một framework AT mới thông qua tối ưu hóa bi-level (BLO). Nói chung, BLO cung cấp một framework học phân cấp với hai cấp độ tác vụ tối ưu hóa, trong đó mục tiêu và biến của bài toán cấp trên phụ thuộc vào bộ tối ưu hóa của cấp dưới. BLO sau đó cho phép chúng tôi mô hình hóa rõ ràng sự kết hợp giữa AT cho bộ định tuyến và AT cho mạng MoE. Cụ thể, chúng tôi sửa đổi (AT) thông thường thành

minimize_ψ ℓ_TRADES(ψ,φ*(ψ);D)
subject to φ*(ψ) = argmin_φ ℓ_TRADES(ψ,φ;D),     (1)

trong đó các tham số mô hình của MoE-CNN θ được chia thành các biến tối ưu hóa cấp dưới φ cho bộ định tuyến và các biến tối ưu hóa cấp trên ψ cho mạng backbone MoE, và ℓ_TRADES(ψ,φ;D) biểu thị mất mát huấn luyện kiểu TRADES được định nghĩa trong (AT) bằng cách thay thế θ bằng (φ,ψ). So với (AT), đề xuất (1) của chúng tôi có các khác biệt sau. Đầu tiên, robustifying mạng MoE (ψ) bây giờ được kết hợp rõ ràng với tối ưu hóa bộ định tuyến thông qua giải pháp cấp dưới φ*(ψ). Thứ hai, đề xuất của chúng tôi giải quyết vấn đề thích ứng tính bền vững được chỉ ra trong Hiểu biết 3 vì tối ưu hóa cấp dưới của (1) cho phép thích ứng nhanh của φ với mạng MoE hiện tại ψ như meta-learning. Thứ ba, vì ℓ_TRADES được tham gia ở cả hai cấp độ tối ưu hóa của (1), vấn đề tạo tấn công nhúng (tức là tối đa hóa trên δ trong AT) cần được giải quyết ở mỗi cấp độ nhưng tương ứng với các mô hình nạn nhân khác nhau, tức là (ψ,φ) và (ψ,φ*(ψ)), tương ứng.

Để giải quyết vấn đề (1), chúng tôi áp dụng phương pháp tối ưu hóa xen kẽ tiêu chuẩn (AO). So với các loại thuật toán BLO khác, AO là hiệu quả nhất về mặt tính toán. Các thí nghiệm rộng rãi của chúng tôi trong Phần 5 sẽ cho thấy rằng AO hiệu quả trong việc thúc đẩy tính bền vững đối kháng của MoE-CNN và đạt được cải thiện so với các phương pháp và mô hình baseline với biên độ đáng kể. Ý tưởng chính của AO là tối ưu hóa xen kẽ bài toán cấp dưới và cấp trên, trong đó các biến được định nghĩa ở cấp độ khác được cố định. Chúng tôi gọi framework thuật toán kết quả là Adversarially robust learning for MoE-CNN (ADVMOE); xem Thuật toán 1 để tóm tắt.

Chúng tôi nhấn mạnh rằng ADVMOE sẽ huấn luyện các bộ định tuyến bền vững và các đường dẫn MoE bền vững để 'thích ứng' với nhau. Trái ngược với framework AT thông thường, ADVMOE cung cấp φ*(ψ) và ψ được kết hợp, trong đó cả hai phần đều nỗ lực phối hợp để cải thiện tính bền vững tổng thể. Chúng tôi cũng nhận xét rằng ADVMOE không giới thiệu các siêu tham số bổ sung, vì trong thực tế chúng tôi thấy các bộ định tuyến và chuyên gia có thể chia sẻ cùng tốc độ học và lịch trình. Các chi tiết triển khai thêm được cung cấp trong Phụ lục B.

Trong khi đó, chúng tôi nhận xét rằng vì đề xuất của chúng tôi là một BLO với các mục tiêu cấp dưới và cấp trên không lồi (1). Khó có thể chứng minh sự hội tụ của ADVMOE. Phân tích lý thuyết hiện tại của BLO thường dựa vào các giả định lồi mạnh của các bài toán cấp dưới. Mặc dù không có framework phân tích lý thuyết phù hợp, phương pháp của chúng tôi hội tụ tốt trong thực tế (xem Phụ lục C).

**Thuật toán 1 Thuật toán ADVMOE**
1: **Khởi tạo:** mạng backbone ψ, bộ định tuyến φ, kích thước batch b, bước tạo tấn công K.
2: **for** Lặp t = 0,1,... **do**
3:     Chọn các batch dữ liệu ngẫu nhiên khác nhau B_ψ và B_φ cho huấn luyện backbone và router
4:     **Cập nhật cấp dưới φ (với ψ cố định):** Với ψ, cập nhật φ bằng cách tối thiểu hóa ℓ_TRADES sử dụng bộ tạo tấn công PGD K-bước và SGD (với B_φ)
5:     **Cập nhật cấp trên ψ (với φ cố định):** Với φ, cập nhật ψ bằng cách tối thiểu hóa ℓ_TRADES sử dụng bộ tạo tấn công PGD K-bước và SGD (với B_ψ)
6: **end for**

## 5. Thí nghiệm

Trong phần này, chúng tôi sẽ chứng minh hiệu quả của phương pháp ADVMOE được đề xuất trên các bộ dữ liệu và mô hình đa dạng. Chúng tôi cũng sẽ thực hiện phân tích sâu về tiện ích bộ định tuyến và phân phối lựa chọn chuyên gia cho MoE-CNN được huấn luyện bằng ADVMOE.

### 5.1. Thiết lập thí nghiệm

**Thiết lập mô hình và bộ dữ liệu.** Để triển khai MoE-CNN và các baseline khác, chúng tôi thực hiện thí nghiệm trên ResNet-18, Wide-ResNet-28-10, VGG-16, và DenseNet. Để đánh giá công bằng, so sánh hiệu suất giữa các loại mô hình khác nhau của chúng tôi được hạn chế sử dụng cùng tham số quy mô mô hình r (xem Hình 1 làm ví dụ). Bằng cách này, một ví dụ đầu vào sẽ tận dụng cùng lượng tham số mô hình để ra quyết định. Đối với MoE-CNN, chúng tôi xem xét N = 2 chuyên gia với r = 0.5 theo mặc định, xem Phụ lục B để biết thêm chi tiết. Về mặt bộ dữ liệu, chúng tôi tập trung vào những bộ thường được sử dụng để đánh giá tính bền vững đối kháng của phân loại hình ảnh, bao gồm CIFAR-10, CIFAR-100, TinyImageNet, và ImageNet.

**Baseline.** Để làm cho so sánh hiệu suất của chúng tôi có thông tin và toàn diện, chúng tôi xem xét ba loại baseline có thể so sánh công bằng với (ADVMOE). ① AT (S-Dense): chúng tôi áp dụng AT cho S-Dense; ② AT (Sparse): chúng tôi áp dụng phương pháp học mặt nạ thưa thớt (có cấu trúc) nhận thức tính bền vững để có được Sparse-CNN; ③ AT (MoE): chúng tôi áp dụng trực tiếp AT cho MoE-CNN, đồng huấn luyện các bộ định tuyến và mạng backbone. Lưu ý phương pháp này cũng được áp dụng trong thuật toán huấn luyện bền vững mới nhất cho kiến trúc MoE dựa trên ViT. Đáng chú ý rằng các baseline trên sử dụng cùng số lượng tham số mô hình như đường dẫn của MoE-CNN trong quá trình dự đoán mô hình. Ngoài ra, chúng tôi bao gồm ④ AT (Dense) (áp dụng AT cho Dense) để có được một tham chiếu hiệu suất tính bền vững. Tuy nhiên, chúng tôi nhận xét rằng không khá công bằng khi so sánh trực tiếp Dense với các đối tác nhỏ hơn nói trên, vì trước đây sử dụng quy mô mô hình lớn hơn (r = 1.0) tại thời điểm suy luận kiểm tra.

**Huấn luyện và đánh giá.** Chúng tôi sử dụng TRADES làm mục tiêu huấn luyện bền vững mặc định cho tất cả các baseline. Chúng tôi cũng theo tài liệu để đặt cường độ tấn công bằng ε = 8/255 cho CIFAR-10 và CIFAR-100, và ε = 2/255 cho TinyImageNet và ImageNet. Để triển khai ADVMOE (Thuật toán 1), chúng tôi mô phỏng pipeline huấn luyện TRADES nhưng thực hiện quy trình BLO được đề xuất để robustify các tham số bộ định tuyến và backbone theo chế độ tương tác. Chúng tôi áp dụng tấn công PGD 2-bước tại thời gian huấn luyện cho tất cả các phương pháp, được hỗ trợ bởi công trình gần đây cho thấy hiệu suất hấp dẫn của nó trong AT. Chúng tôi giới thiệu độc giả đến Phụ lục B để biết thêm chi tiết huấn luyện. Trong quá trình đánh giá, chúng tôi báo cáo độ chính xác tiêu chuẩn (SA) trên bộ dữ liệu kiểm tra sạch và độ chính xác bền vững (RA) chống lại các cuộc tấn công PGD 50-bước thời gian kiểm tra với cường độ tấn công giống như các giá trị huấn luyện. Chúng tôi cũng báo cáo GFLOPS (FLOPS × 10^9) như một chỉ số của hiệu quả suy luận thời gian kiểm tra.

### 5.2. Kết quả thí nghiệm

**Hiệu suất tổng thể.** Bảng 1 trình bày hiệu suất tổng thể của thuật toán ADVMOE được đề xuất so với các baseline. Chúng tôi thực hiện một số quan sát chính dưới đây.

Đầu tiên, ADVMOE mang lại cải thiện tính bền vững đáng kể so với tất cả các baseline trong mọi thiết lập dữ liệu-mô hình. Cụ thể, ADVMOE liên tục mang lại cải thiện khoảng 1%∼5% về tính bền vững được đo bằng RA chống lại các cuộc tấn công PGD. Đáng chú ý, ADVMOE cũng có thể vượt trội hơn • AT (Dense) trong hầu hết các trường hợp, khoảng 1%∼4% cải thiện tính bền vững (xem kết quả được tô sáng màu xanh lá cây). Điều này đáng chú ý vì Dense (r = 1.0) lớn gấp đôi so với đường dẫn MoE (r = 0.5). Thứ hai, chúng tôi quan sát thấy rằng ADVMOE có sự ưu tiên đối với các mô hình rộng hơn. Ví dụ, khi WRN-28-10 (kiến trúc mô hình rộng nhất trong thí nghiệm) được sử dụng, ADVMOE mang lại tính bền vững tốt hơn so với đối tác Dense trên tất cả các thiết lập bộ dữ liệu. Thứ ba, chúng tôi cũng quan sát thấy rằng việc áp dụng AT trực tiếp cho MoE-CNN, tức là AT (MoE), tệ hơn AT (S-Dense) và ADVMOE trong tất cả các thiết lập. Điều này phù hợp với các phát hiện của chúng tôi trong Phần 4. Chúng tôi nhận xét rằng mặc dù tính hữu ích của AT (MoE) đã được khai thác trong [30] cho MoE kiểu ViT, nó không hiệu quả để huấn luyện MoE kiểu CNN nữa. Thứ tư, ADVMOE có thể duy trì hiệu quả suy luận cao cho MoE-CNN, như được chứng minh bởi các phép đo GFLOPS trong Bảng 1. So với S-Dense, MoE-CNN giới thiệu chi phí tính toán nhỏ do hệ thống định tuyến. Tuy nhiên, nó tiết kiệm hơn 50% chi phí suy luận so với Dense. Điều này ngụ ý rằng đề xuất ADVMOE của chúng tôi có thể bảo tồn lợi ích hiệu quả của cấu trúc MoE trong khi cải thiện hiệu quả tính bền vững đối kháng của nó.

**Đánh giá bền vững trên AutoAttack.** Trong Bảng 2, chúng tôi cung cấp các thí nghiệm bổ sung được đánh giá bởi AutoAttack (được gọi là RA-AA), một benchmark đánh giá tính bền vững phổ biến. Thiết lập thí nghiệm trong Bảng 2 theo Bảng 1. Chúng tôi báo cáo RA-AA trên CIFAR-10 và CIFAR-100 với ResNet-18 và WRN-28-10. Như chúng ta có thể thấy, mặc dù AutoAttack dẫn đến RA-AA thấp hơn so với RA được đánh giá bằng các cuộc tấn công PGD (được gọi là RA-PGD), ADVMOE vẫn vượt trội hơn AT (S-Dense), AT (Sparse), và AT (MoE) một cách nhất quán, được chứng minh bởi các số in đậm trong các cột RA-AA.

**MoE-CNN được huấn luyện bởi ADVMOE tận hưởng tiện ích bộ định tuyến tốt hơn.** Dựa trên kết quả trên và các nghiên cứu sơ bộ trong Phần 4, chúng tôi tiếp theo tìm hiểu sự khác biệt hiệu suất được đạt bởi AT (Sparse), AT (MoE), và ADVMOE từ góc độ đa dạng đường dẫn. Chúng tôi hỏi: ① Mối quan hệ giữa các đường dẫn động được tạo bởi các bộ định tuyến được huấn luyện bởi ADVMOE và mặt nạ tĩnh được tối ưu hóa bởi AT (Sparse) là gì? ② Sự khác biệt giữa các quyết định định tuyến sử dụng ADVMOE và AT (MoE) là gì, và nó tác động như thế nào đến hiệu suất?

Về ①, chúng tôi điều tra độ tương tự cosine giữa các đường dẫn được tạo bởi các phương pháp huấn luyện, hoặc AT (MoE) hoặc ADVMOE, và mặt nạ tĩnh được tìm thấy bởi AT (Sparse). Vì cái sau có thể được coi là một đường dẫn đơn được sử dụng cho tất cả dữ liệu, chúng tôi gọi nó là 'đường dẫn mặt nạ' trái ngược với 'đường dẫn MoE'. Chúng tôi tính toán điểm intersection of union (IoU) giữa đường dẫn MoE và đường dẫn mặt nạ dưới mỗi bộ dữ liệu kiểm tra (phiên bản sạch hoặc đối kháng). Hình 6 trình bày các phân phối IoU dựa trên các bộ dữ liệu kiểm tra sạch và đối kháng (Hình 6a cho ADVMOE và Hình 6b cho AT (MoE)). Chúng tôi nhận xét rằng một điểm IoU nhỏ hơn cho thấy sự khác biệt lớn hơn giữa đường dẫn MoE và đường dẫn mặt nạ. Như chúng ta có thể thấy, phân phối IoU của ADVMOE so với AT (Sparse) trong Hình 6a dịch chuyển gần hơn về 0 so với Hình 6b. Quan sát này áp dụng cho cả đánh giá tiêu chuẩn và đối kháng và cho thấy rằng ADVMOE (đề xuất của chúng tôi) có khả năng tốt hơn AT (MoE) trong việc xây dựng lại các đường dẫn MoE cụ thể đầu vào, khác biệt đáng kể hơn so với đường dẫn mặt nạ không phụ thuộc đầu vào được xác định bởi phương pháp dựa trên pruning, AT (Sparse).

Về ②, chúng tôi quan sát từ Hình 6 rằng các bộ định tuyến được học bởi AT (MoE) dễ bị tổn thương hơn đối với các cuộc tấn công đối kháng so với ADVMOE, như được chứng minh bởi diện tích giao nhau ít hơn của dữ liệu đối kháng so với dữ liệu sạch. Điều này cũng phù hợp với Hiểu biết 3 trong Phần 4. Hơn nữa, chính sách định tuyến được học bởi ADVMOE đa dạng hơn AT (MoE), như được chỉ ra bởi các điểm IoU tập trung mật độ của cái sau. Ngược lại, phân phối của ADVMOE được phân tán với giá trị đỉnh nhỏ hơn. Do đó, về tiện ích chuyên gia, ADVMOE có thể gán các đầu vào cho một nhóm đường dẫn lớn hơn so với AT (MoE), tận dụng tốt hơn các chuyên gia.

**Tác động kết hợp của số lượng chuyên gia N và quy mô mô hình per-expert r trên ADVMOE.** Nhớ lại rằng tồn tại hai tham số chính liên quan đến MoE-CNN (Hình A1): (a) số lượng chuyên gia N, và (b) quy mô mô hình r định nghĩa dung lượng mô hình per-expert (hoặc per-pathway). Với mô hình backbone (ví dụ, ResNet-18 trong thí nghiệm này), một N lớn hơn được ghép nối với r nhỏ ngụ ý rằng mỗi chuyên gia có thể chỉ có dung lượng mô hình hạn chế, tức là tương ứng với ít kênh hơn. Bất kể N, nếu r = 1, mạng backbone đầy đủ sẽ được sử dụng để tạo thành đường dẫn quyết định giống hệt.

Hình 7 cho thấy RA của MoE-CNN được huấn luyện bởi ADVMOE so với tham số quy mô mô hình r tại các giá trị N khác nhau. Hai quan sát sâu sắc có thể được rút ra. Đầu tiên, tồn tại một chế độ MoE (ví dụ, N < 8 và r ∈ [0.5,0.9]), trong đó ADVMOE có thể vượt trội hơn AT (Dense) (tức là r = 1) với biên độ đáng kể. Điều này cho thấy lợi ích của MoE trong tính bền vững đối kháng. Tuy nhiên, nếu số lượng chuyên gia trở nên lớn hơn (ví dụ, N = 10), sự đa dạng tăng của các đường dẫn MoE có thể làm tăng khó khăn của việc robustification bộ định tuyến và do đó cản trở hiệu suất của ADVMOE (xem N = 10 và r = 0.8 trong Hình 7). Thứ hai, tồn tại một chế độ MoE không hiệu quả (ví dụ, N ≥ 8 và r < 0.5), trong đó hiệu suất của ADVMOE chệch hướng lớn so với AT (Dense). Trong chế độ này, mỗi chuyên gia chỉ bao gồm một số lượng nhỏ kênh, hạn chế khả năng huấn luyện bền vững của nó. Theo đó, cả sự đa dạng tăng của các đường dẫn MoE (N lớn) và dung lượng hạn chế per pathway (r nhỏ) đều có thể áp đặt khó khăn của AT cho MoE-CNN. Trong các thí nghiệm của chúng tôi, chúng tôi chọn r = 0.5 và N = 2, bảo tồn đa dạng của các đường dẫn MoE (tức là hiệu quả suy luận) và giữ lại hiệu quả của huấn luyện bền vững.

**Hiệu suất với các quy mô mô hình khác nhau.** Để đảm bảo rằng các quan sát và kết luận từ Bảng 1 nhất quán qua các giá trị khác nhau của tham số quy mô mô hình r, chúng tôi lặp lại các thí nghiệm trên (CIFAR-10, ResNet-18) và (CIFAR-10, WRN-28-10) sử dụng r ∈ {0.2,0.5,0.8} để bao gồm các chế độ {thưa thớt, trung bình, dày đặc} so với Dense (r = 1.0). Hình 8 tóm tắt các kết quả thí nghiệm thu được. Như chúng ta có thể thấy, ADVMOE mang lại cải thiện tính bền vững nhất quán so với tất cả các baseline, bao gồm Dense. Và sự cải thiện tăng khi quy mô mô hình r tăng. Điều này không ngạc nhiên vì nhiều tham số sẽ được sử dụng khi xử lý một đầu vào. Tuy nhiên, một nhược điểm rõ ràng do quy mô mô hình lớn hơn r mang lại là sự gia tăng chi phí suy luận, được chứng minh bởi các số GFLOPS. Khi r trở nên lớn (như r = 0.8), lợi ích hiệu quả do sparsification đường dẫn từ MoE mang lại dần dần biến mất. Do đó, một sparsity trung bình (r = 0.5) là lựa chọn tốt hơn để cân bằng sự đánh đổi giữa hiệu suất và hiệu quả, do đó được áp dụng làm thiết lập mặc định của chúng tôi.

**Nghiên cứu mở rộng: ADVMOE cho ViT.** Để khám phá khả năng của đề xuất ADVMOE trên các mô hình MoE dựa trên ViT (MoE-ViT), Bảng 3 trình bày các kết quả bổ sung theo baseline SOTA được xuất bản gần đây cho MoE-ViT. Như chúng ta có thể thấy, ADVMOE cũng áp dụng được cho MoE-ViT và có thể thúc đẩy tính bền vững so với baseline SOTA với cải thiện RA hơn 1%, trong khi đạt được mức SA tương tự. Do đó, mặc dù công trình của chúng tôi tập trung vào huấn luyện bền vững cho MoE-CNN, nó có tiềm năng của tính tổng quát thuật toán cho các kiến trúc dựa trên MoE khác. Chúng tôi hoãn một nghiên cứu toàn diện hơn trong tương lai.

**Thí nghiệm bổ sung.** Chúng tôi thực hiện các nghiên cứu ablation về (1) đánh giá tính bền vững sử dụng AutoAttack (có thể rút ra các phát hiện nhất quán như các cuộc tấn công PGD), (2) các bước tấn công được sử dụng trong AT, và (3) các khám phá bổ sung hướng tới tác động kết hợp giữa số lượng chuyên gia và quy mô mô hình. Chúng tôi giới thiệu độc giả đến Phụ lục C để biết kết quả chi tiết.

## 6. Kết luận

Trong công trình này, chúng tôi thiết kế một sơ đồ huấn luyện bền vững hiệu quả cho MoE-CNN. Chúng tôi đầu tiên trình bày một số hiểu biết chính về cơ chế phòng thủ của MoE-CNN bằng cách phân tích tính bền vững đối kháng thông qua lăng kính của bộ định tuyến và đường dẫn. Chúng tôi tiếp theo đề xuất ADVMOE, framework huấn luyện bền vững đầu tiên cho MoE-CNN thông qua tối ưu hóa bi-level, robustifying bộ định tuyến và đường dẫn theo chế độ hợp tác và thích ứng. Cuối cùng, các thí nghiệm rộng rãi chứng minh hiệu quả của ADVMOE trong nhiều thiết lập dữ liệu-mô hình khác nhau. Trong khi đó, chúng tôi thừa nhận rằng ADVMOE đòi hỏi khoảng gấp đôi dung lượng tính toán so với baseline AT vanilla do tối ưu hóa xen kẽ gọi hai back-propagation mỗi bước. Giải quyết mối quan tâm về hiệu quả này trình bày một hướng có ý nghĩa cho công trình tương lai.

**Lời cảm ơn**

Công trình của Y. Zhang, S. Chang và S. Liu được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia (NSF) Grant IIS-2207052 và Cisco Research Award. Công trình của Z. Wang được hỗ trợ một phần bởi US Army Research Office Young Investigator Award (W911NF2010240).
