# 2209.08326.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/convolution/2209.08326.pdf
# Kích thước tệp: 316741 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Conformer Hiệu Quả Tham Số thông qua Chia Sẻ Các Chuyên Gia Cổng Thưa Thớt cho
Nhận Dạng Giọng Nói Đầu-Cuối
Ye Bai, Jie Li, Wenjing Han, Hao Ni, Kaituo Xu, Zhuo Zhang, Cheng Yi, Xiaorui Wang
Kuaishou Technology Co., Ltd, Bắc Kinh, Trung Quốc
fbaiye03,lijie03,hanwenjing,nihao,xukaituo,zhangzhuo03,chengyi03,wangxiaorui g@kuaishou.com
Tóm tắt
Trong khi transformer và các biến thể conformer của chúng cho thấy hiệu
suất đầy hứa hẹn trong nhận dạng giọng nói, tính chất được tham số hóa
dẫn đến chi phí bộ nhớ lớn trong quá trình huấn luyện và suy luận.
Một số nghiên cứu sử dụng chia sẻ trọng số xuyên tầng để giảm các tham
số của mô hình. Tuy nhiên, việc mất khả năng không tránh khỏi làm hại
hiệu suất của mô hình. Để giải quyết vấn đề này, bài báo này đề xuất một
conformer hiệu quả tham số thông qua chia sẻ các chuyên gia cổng thưa
thớt. Cụ thể, chúng tôi sử dụng hỗn hợp các chuyên gia cổng thưa thớt
(MoE) để mở rộng khả năng của một khối conformer mà không tăng tính
toán. Sau đó, các tham số của các khối conformer được nhóm lại được chia
sẻ để giảm số lượng tham số. Tiếp theo, để đảm bảo các khối được chia sẻ
có tính linh hoạt trong việc thích ứng biểu diễn ở các cấp độ khác nhau,
chúng tôi thiết kế các bộ định tuyến MoE và chuẩn hóa một cách riêng lẻ.
Hơn nữa, chúng tôi sử dụng chưng cất kiến thức để cải thiện thêm hiệu
suất. Kết quả thực nghiệm cho thấy mô hình được đề xuất đạt được hiệu
suất cạnh tranh với 1/3 tham số của bộ mã hóa, so với mô hình tham số
đầy đủ.
Từ khóa chỉ mục: hiệu quả tham số, hỗn hợp các chuyên gia cổng thưa thớt,
Conformer, chia sẻ trọng số xuyên tầng

1. Giới thiệu
Ngày nay, transformer và các biến thể của chúng đã được áp dụng thành
công vào nhận dạng giọng nói tự động (ASR) đầu-cuối (E2E) [1, 2, 3].
Transformer thường sử dụng các chồng tự chú ý và mạng nơ-ron truyền
thẳng (FFN) để xây dựng một bộ mã hóa và một bộ giải mã [1], sau đó
sử dụng cơ chế chú ý để kết nối các đặc trưng âm thanh được mã hóa và
các biểu diễn của chuỗi token văn bản [4]. Gần đây, như một biến thể,
conformer được phát triển để tăng cường transformer với tích chập bằng
cách giúp mô hình nắm bắt tính cục bộ [3]. Kết hợp với các kỹ thuật như
biểu diễn vị trí tương đối [5, 6] và FFN nửa bước theo kiểu Macaron [7],
conformer cải thiện thêm hiệu suất của transformer trong ASR.

Bất chấp hiệu suất đầy hứa hẹn, nhiều nghiên cứu cho thấy sự tham số
hóa quá mức của transformer [8, 9], điều này dẫn đến việc các mô hình
yêu cầu nhiều bộ nhớ lưu trữ trong quá trình huấn luyện và suy luận, và
do đó giới hạn việc sử dụng các mô hình trên thiết bị. Để giảm chi phí bộ
nhớ, một số nghiên cứu chia sẻ các tham số của một hoặc một số khối
transformer để tổng số tham số của mô hình được giảm đáng kể [9, 10, 11,
12, 13]. Các mô hình này sử dụng một hoặc một số khối transformer để
mã hóa các đặc trưng theo cách đệ quy, do đó số lượng tham số ít hơn so
với transformer gốc có cùng độ sâu. Tuy nhiên, do có ít tham số mô hình
tự do hơn, khả năng của mạng không tránh khỏi bị ảnh hưởng và hiệu
suất suy giảm kết quả.

Để giải quyết vấn đề này, chúng tôi đề xuất chia sẻ hỗn hợp các chuyên
gia cổng thưa thớt (MoE) để cải thiện khả năng của conformer chia sẻ
tham số xuyên tầng, và không tăng tính toán trong lúc đó. Cụ thể, chúng
tôi đầu tiên thiết kế FFN thứ hai của một khối conformer thành một mô-
đun MoE cổng thưa thớt để cải thiện khả năng và chia sẻ các khối con-
former được nhóm theo cách xuyên tầng. MoE cổng thưa thớt sử dụng cơ
chế định tuyến động để kích hoạt chỉ một hoặc một phần chuyên gia trong
quá trình huấn luyện và suy luận, điều này giữ tính toán tương đương với
các mô hình không MoE và mở rộng khả năng của mô hình [14, 15, 16,
17]. Sau đó, để giúp các khối conformer chia sẻ tham số thích ứng các
biểu diễn ẩn ở các cấp độ khác nhau, chúng tôi đề xuất làm cho mỗi khối
có bộ định tuyến riêng để các khối có thể có đường dẫn định tuyến linh
hoạt cho các biểu diễn cấp độ khác nhau. Chúng tôi cũng sử dụng các lớp
chuẩn hóa riêng lẻ của các khối để làm cho chúng có thể thích ứng và để
đảm bảo thống kê nhất quán [18]. Hơn nữa, chúng tôi sử dụng chưng cất
kiến thức [19, 20] để giúp mô hình chia sẻ tham số bắt chước mô hình
tham số đầy đủ. Kết quả thực nghiệm trên bộ dữ liệu AISHELL-1 công
khai chứng minh rằng các mô hình hiệu quả tham số được đề xuất có thể
đạt được hiệu suất cạnh tranh với 1/3 tham số bộ mã hóa, so với mô hình
tham số đầy đủ.

2. Nền tảng: Mô hình Seq2Seq dựa trên Conformer
cho ASR
Là các mô hình mã hóa-giải mã dựa trên chú ý (AED), transformer [1]
sử dụng một bộ mã hóa để nắm bắt các biểu diễn cấp cao từ các đặc trưng
âm thanh và một bộ giải mã để dự đoán các chuỗi văn bản từng token
một với cơ chế chú ý. Chính thức, cho một chuỗi đặc trưng âm thanh x
= [x0;;xT−1] với độ dài T và một chuỗi token văn bản y = [y0;;yS]
với độ dài S + 1, trong đó y0 và yS là ký hiệu bắt đầu câu <sos> và ký
hiệu kết thúc câu <eos>, mô hình Trfm dự đoán xác suất của token văn
bản:

P(ys|y<s;x) = Trfm(y<s;x); (1)

trong đó y<s là tiền tố của ys trong chuỗi văn bản, 1 ≤ s ≤ S.
Mô hình được huấn luyện với tiêu chí khả năng tối đa:

Lnll(θ) = −1/S ∑(s=1 to S) log P(ys|y<s;x); (2)

trong đó θ là các tham số của mô hình Trfm. Thuật toán tìm kiếm chùm
được sử dụng để tìm chuỗi token văn bản có khả năng nhất trong quá trình
suy luận. Cấu trúc tổng thể được hiển thị trong Hình 1.

Conformer [3] chèn các lớp tích chập vào một khối transformer để giúp
mô hình nắm bắt tính cục bộ của một chuỗi. Với các cấu trúc chi tiết
được thiết kế cẩn thận, bao gồm pre-norm [21], hàm kích hoạt GLU [22]
và Swish [23], mã hóa vị trí tương đối [5, 6], conformer cải thiện thêm
hiệu suất và ổn định quá trình huấn luyện. Mô hình của chúng tôi chọn
conformer làm cấu trúc cơ bản của bộ mã hóa. Cấu trúc của khối con-
former được hiển thị ở phần giữa của Hình 1. Chi tiết của từng mô-đun
trong khối conformer được tham khảo từ [3].

--- TRANG 2 ---
FFN thứ 2 Tích chập Tự chú ý FFN thứ 1 LayerNorm ×½
×½ Khối MoE-Conformer Khối Giải mã Transformer Khối Giải mã Transformer Đặc trưng Nd× ... Khối Giải mã Transformer Softmax
y0, y1, y2, …, yS-1 y1, y2, y3…, yS
x0, x1, x2, …, xT-1 Token Chia sẻ ×G
Router FFN0 FFN1 FFN2 FFNE-1 zt(3) kích hoạt ... ×C Khối MoE-Conformer Khối MoE-Conformer ... Khối MoE-Conformer ...
Mô hình tổng thể. Cấu trúc của khối MoE-Conformer. Hỗn hợp các chuyên gia (MoE) Bộ mã hóa Bộ giải mã
... zt(3) z(2) zt(1) zt ẑt

Hình 1: (Trái) Kiến trúc tổng thể của hệ thống ASR. Bộ mã hóa bao gồm G nhóm của C khối MoE-conformer liên tiếp. Các tham số của các khối MoE-conformer có cùng màu được chia sẻ giữa các nhóm khác nhau, ngoại trừ các mô-đun chuẩn hóa và các router của mô-đun MoE. Bộ giải mã bao gồm Nd khối giải mã transformer. (Giữa) Cấu trúc của khối MoE-conformer bao gồm hai mô-đun mạng nơ-ron truyền thẳng (FFN), một mô-đun tích chập, và một mô-đun tự chú ý đa đầu (MHSA). Chi tiết của từng mô-đun được tham khảo từ [3]. Chúng tôi mở rộng một cách mới lạ mô-đun FFN thứ hai thành công thức hỗn hợp các chuyên gia (MoE). (Phải) Cấu trúc của mô-đun MoE. Mô-đun MoE bao gồm một số mô-đun FFN song song và một router. Trong quá trình lan truyền thuận, đầu vào được đưa vào một trong các mô-đun FFN được kích hoạt bởi router.

3. Chia sẻ Các Chuyên gia Cổng Thưa Thớt

Ý tưởng cốt lõi của mô hình hiệu quả tham số được đề xuất là sử dụng lại các khối bộ mã hóa conformer một cách đệ quy để tận dụng tối đa chúng. Quan trọng là, các mô-đun MoE cổng thưa thớt được sử dụng để cải thiện khả năng của các mô-đun mà không tăng tính toán. Điều quan trọng không kém là các router và các lớp chuẩn hóa được thiết kế thêm theo cách riêng lẻ, để chúng có thể được sử dụng như các bộ điều hợp để giúp các khối được sử dụng lại thích ứng với các biểu diễn ở các cấp độ khác nhau. Phương pháp này cũng có thể được sử dụng trong các cấu trúc mạng khác, chẳng hạn như các khối giải mã transformer và mạng nơ-ron tích chập, cũng như trong các mô hình ASR E2E khác, chẳng hạn như transducer [24] và CTC [25].

3.1. Chia sẻ Tham số cho Conformer

Cấu trúc của một conformer, như được hiển thị trong phần giữa của Hình 1, bao gồm hai mô-đun FFN, một mô-đun chú ý đa đầu, và một mô-đun tích chập. Tất cả các mô-đun sử dụng kết hợp kiểu pre-norm của kết nối dư và chuẩn hóa lớp. Bên cạnh đó, các mô-đun FFN sử dụng hàm kích hoạt Swish. Mô-đun tự chú ý đa đầu sử dụng mã hóa vị trí tương đối. Mô-đun tích chập là một khối tích chập có thể tách rời theo thời gian-độ sâu với hàm kích hoạt GLU và Swish. Chi tiết thêm về từng mô-đun được tham khảo từ [3].

Ở đây, chính thức, đối với biểu diễn đầu vào zt tại bước thời gian, tính toán của một khối conformer như sau:

z(1)t = zt + 1/2 FFN(zt);
z(2)t = z(1)t + MHSA(z(1)t);
z(3)t = z(2)t + Conv(z(2)t);
ẑt = LayerNorm(z(3)t + 1/2 FFN(MoE)(z(3)t)); (3)

trong đó FFN, MHSA, Conv và FFN(MoE) biểu thị mô-đun FFN đầu tiên, mô-đun tự chú ý đa đầu, mô-đun tích chập, và FFN thứ hai được tăng cường với MoE, tương ứng. LayerNorm biểu thị chuẩn hóa lớp [26]. Chi tiết của FFN(MoE) được mô tả trong Phần 3.2.

Như được hiển thị trong phần bên trái của Hình 1, chúng tôi chia sẻ các tham số của các khối khác nhau. Cụ thể, C khối conformer liên tiếp được nhóm lại và G nhóm được xếp chồng. Đối với khối conformer tại cùng một vị trí trong các nhóm khác nhau, các tham số của từng mô-đun được chia sẻ. Nó có thể được xem như một nhóm các khối conformer được sử dụng lại G lần, và tính toán được thực hiện theo cách lặp đệ quy. Do đó mô hình tận dụng tối đa các tham số.

3.2. Định tuyến Động cho Hỗn hợp Các Chuyên gia

Bằng cách chia sẻ tham số, các tham số của bộ mã hóa được giảm đáng kể. Tuy nhiên, khả năng của mô hình cũng bị giảm điều này ảnh hưởng tiêu cực đến hiệu suất của mô hình. Vì vậy, để cải thiện khả năng mô hình nhưng không tăng tính toán, chúng tôi giới thiệu MoE cổng thưa thớt [14, 17] vào mô-đun FFN thứ hai, như được hiển thị trong phần bên phải của Hình 1.

Cơ chế MoE cổng thưa thớt bao gồm E chuyên gia song song và một router. Đầu vào z(3)t trước tiên được đưa vào router để chọn một trong các chuyên gia¹ và sau đó được tính toán bởi chuyên gia được kích hoạt. Tính toán chính thức như sau:

g = [g0;;gE−1] = softmax(router(z(3)t));
i = arg max(0≤i≤E−1) gi;
FFN(MoE)(z(3)t) = gi FFNi(z(3)t); (4)

trong đó FFNi biểu thị chuyên gia thứ i, gi biểu thị giá trị cổng liên quan đến chuyên gia thứ i, và i là chỉ số của chuyên gia được chọn. Người ta có thể nhận thấy rằng, quy trình của MoE thực sự tương tự như cơ chế chú ý: đầu vào z(3)t có thể được xem như vector truy vấn trong cơ chế chú ý, và các điểm cổng g có thể được xem như các hệ số chú ý [1]. Tuy nhiên, quy trình chú ý là theo cách "cứng", tức là tất cả các hệ số không cực đại đều được đặt thành không.

Ngoài ra, để khuyến khích tất cả các chuyên gia được sử dụng một cách cân bằng, tổn thất cân bằng tải [17] được sử dụng như sau:

Lbalance = E ∑(i=0 to E−1) fi gi; (5)

trong đó fi là tần suất hoạt động của chuyên gia thứ i trong một batch, và gi là trung bình của các giá trị cổng được tính toán cho chuyên gia thứ i. Ngoài ra, nhiễu Gaussian được thêm vào các router để làm cho việc lựa chọn chuyên gia đa dạng trong quá trình huấn luyện.

Với MoE, các tham số được mở rộng để khả năng mô hình được tăng lên. Tuy nhiên, vì chỉ có một FFN thực sự được kích hoạt, tính toán không được tăng lên.

3.3. Router và Chuẩn hóa Riêng lẻ

Để cải thiện thêm khả năng của các mô-đun MoE được sử dụng lại, chúng tôi đề xuất làm cho mỗi mô-đun MoE có router riêng của nó. Suy nghĩ bên dưới là để giúp đường dẫn định tuyến đạt được tính linh hoạt hơn. Với điều này, các mô-đun MoE trong các khối MoE-conformer khác nhau do đó có thể được thích ứng với các cấp độ biểu diễn khác nhau.

Hơn nữa, tất cả các lớp chuẩn hóa (bao gồm chuẩn hóa lớp và chuẩn hóa batch) được xây dựng riêng lẻ, do đó, để duy trì thống kê của các lớp chuẩn hóa tương ứng với các biểu diễn ở các cấp độ khác nhau là nhất quán. Và các tham số tỷ lệ và offset trong các lớp chuẩn hóa có thể được xem như các bộ điều hợp bias hiệu quả tham số [27].

3.4. Chưng cất Kiến thức từ Nhúng Ẩn

Chúng tôi sử dụng chưng cất kiến thức [19, 20] để chuyển kiến thức từ một mô hình tham số đầy đủ để cải thiện thêm hiệu suất của mô hình tham số chia sẻ. Cụ thể, chúng tôi giảm thiểu khoảng cách L2 giữa các đầu ra của bộ mã hóa tham số chia sẻ (học sinh) và bộ mã hóa tham số đầy đủ (giáo viên):

Lkd = 1/T ∑(t=0 to T−1) ||ht − h't||; (6)

trong đó ht biểu thị đầu ra của bộ mã hóa tham số chia sẻ và h't biểu thị đầu ra của bộ mã hóa tham số đầy đủ.

3.5. Học tập

Mô hình được học bằng cách giảm thiểu tổn thất tổng thể:

L = Lnll + α 1/C ∑ Lbalance + β Lkd; (7)

trong đó C là số lượng mô-đun MoE (xem Hình 1), và α, β là các siêu tham số để cân bằng các giá trị của các tổn thất.

4. Quan hệ với Nghiên cứu Trước

Tính toán có điều kiện của hỗn hợp các chuyên gia. MoE đã được chứng minh là một cách hiệu quả để mở rộng khả năng của mạng nơ-ron mà không tăng tính toán [14, 15, 16, 17]. Tuy nhiên, các nghiên cứu trước đây nhằm mục đích mở rộng kích thước mô hình lên hàng tỷ hoặc hàng nghìn tỷ, điều này cần tài nguyên cực kỳ nhiều và song song hóa mô hình trong quá trình huấn luyện và suy luận. Khác với các nghiên cứu này, chúng tôi nhằm mục đích sử dụng MoE theo cách hiệu quả tham số. Chúng tôi sử dụng lại các mô-đun MoE để tận dụng tối đa chúng.

Chia sẻ trọng số xuyên tầng. Chia sẻ trọng số xuyên tầng đầu tiên được sử dụng trong transformer với thời gian tính toán thích ứng [10]. [9] sử dụng kỹ thuật này để giảm các tham số của BERT.

--- TRANG 3 ---
Bảng 1: Tỷ lệ lỗi ký tự tổng thể trên AISHELL-1. Npe biểu thị tổng số tham số của bộ mã hóa. Dev. và Test biểu thị tỷ lệ lỗi ký tự (CER) trên tập phát triển và tập kiểm tra, tương ứng.

Mô hình | Npe | Dev. | Test
C12 | 21.58M | 4.46 | 4.93
C2 | 3.74M | 5.86 | 6.50
C2-MoE4 | 6.89M | 5.77 | 6.22
C2-G6 | 3.74M | 5.18 | 5.62
C2-MoE4-G6 | 6.95M | 4.67 | 5.08
C2-MoE4-G6-KD | 6.95M | 4.65 | 5.03

[12, 13, 28] sử dụng các kỹ thuật tương tự cho ASR. Tuy nhiên, việc chia sẻ tham số trực tiếp có thể ảnh hưởng tiêu cực đến khả năng của mô hình. Để giải quyết vấn đề này, chúng tôi đề xuất sử dụng cơ chế MoE để cải thiện khả năng mô hình mà không tăng tính toán. Gần đây, [18] chia sẻ mô-đun MoE cho ALBERT và ViT và áp dụng các mô hình vào các tác vụ NLP và CV. Tuy nhiên, nghiên cứu của họ sử dụng hai chuyên gia, điều này tăng chi phí tính toán. Ngoài ra, việc chia sẻ các router giới hạn khả năng của các mô hình. Khác với nghiên cứu của họ, bài báo này tập trung vào kiến trúc hiệu quả hơn của Conformer [3] trong các tác vụ ASR. Chúng tôi sử dụng các router riêng lẻ để giúp mô hình có các đường dẫn định tuyến đa dạng ở các cấp độ khác nhau. Và chúng tôi sử dụng chiến lược nhóm để cải thiện khả năng mô hình theo độ sâu.

5. Thực nghiệm

5.1. Cài đặt Thực nghiệm

Chúng tôi tiến hành thực nghiệm trên bộ dữ liệu tiếng Trung Quan Thoại AISHELL-1² công khai [29], bao gồm khoảng 150 giờ giọng nói để huấn luyện, khoảng 18 giờ giọng nói để phát triển, và khoảng 10 giờ giọng nói để kiểm tra.

Đối với tất cả các thực nghiệm, chúng tôi sử dụng các đặc trưng ngân hàng bộ lọc Mel 80 chiều (FBANK) làm đầu vào, được trích xuất mỗi 10ms với cửa sổ 25ms. Chúng tôi sử dụng cmvn toàn cục để chuẩn hóa đặc trưng. Nhiễu loạn tốc độ với các yếu tố 0.9, 1.0 và 1.1 được sử dụng như tăng cường âm thanh [30]. Tất cả quá trình xử lý đặc trưng được thực hiện với bộ công cụ Kaldi [31]. Chúng tôi sử dụng 4235 ký tự tiếng Trung làm từ vựng, bao gồm <sos> và <eos>.

Chúng tôi sử dụng CNN 2 lớp như một mô-đun lấy mẫu phụ. Mỗi lớp là một lớp tích chập 3×3 với 32 kênh đầu ra, và bước nhảy là 2. Do đó, tốc độ khung được lấy mẫu phụ xuống 25Hz. Đối với bộ mã hóa, chúng tôi đặt chiều của một mô-đun MoE-Conformer là 256, số đầu của MHSA là 4, kích thước kernel của Conv là 15. Chiều trung gian của một mô-đun FFN là 1024. Chúng tôi sử dụng 4 chuyên gia cho FFN thứ hai trong một mô-đun MoE-Conformer. Chúng tôi so sánh hiệu ứng của số lượng khác nhau của các mô-đun MoE-Conformer và nhóm, tức là C và G trong Hình 1. Đối với bộ giải mã, chúng tôi sử dụng cấu trúc transformer. Để kiểm soát các biến thực nghiệm, chúng tôi cố định số lượng các khối giải mã là 4. Chiều của mô-đun giải mã cũng là 256 và chiều trung gian của FFN trong mô-đun giải mã là 1024.

Chúng tôi đặt tỷ lệ dropout là 0.1 và sử dụng SpecAugmentation [32] và time stretch [33] để tránh quá khớp. Các giá trị của α và β trong Eq.(7) được đặt lần lượt là 0.01 và 0.005. Độ lệch chuẩn của nhiễu Gaussian cho cổng MoE được đặt là 0.1 trong huấn luyện. Tổn thất CTC cũng được sử dụng để cải thiện sự căn chỉnh với trọng số 0.2. Lịch trình tỷ lệ học là căn bậc hai nghịch đảo với 4000 bước khởi động. Tất cả các mô hình được huấn luyện trong 80 epoch với 8 card GPU. Một batch bao gồm 32000 khung. Chúng tôi sử dụng PyTorch [34] và FastMoE [35] để thực hiện.

5.2. Kết quả và Phân tích

Tổng thể. Bảng 1 cho thấy hiệu suất tổng thể của mô hình của chúng tôi. C biểu thị số lượng các khối conformer và G biểu thị số lượng nhóm (xem Hình 1). MoE4 và KD có nghĩa là có sử dụng MoE và chưng cất kiến thức hay không, tương ứng. Chúng ta có thể thấy rằng với các phương pháp được đề xuất, C2-MoE4-G6-KD đạt được hiệu suất cạnh tranh với 1/3 tham số của bộ mã hóa, so với mô hình tham số đầy đủ C12. Việc giảm trực tiếp số lượng khối xuống 2 làm hại hiệu suất của mô hình (C2). MoE cải thiện hiệu suất của các mô hình và không tăng các tham số được kích hoạt.

Có MoE so với không có MoE. Chúng tôi so sánh các bộ mã hóa nông có MoE và những bộ không có MoE trong Bảng 2a. Chúng ta có thể thấy rằng MoE cải thiện khả năng để hiệu suất của C1-MoE4 tốt hơn C1 và hiệu suất của C2-MoE4 tốt hơn C2. Và với nhiều khối conformer hơn, mô hình có thể hoạt động tốt hơn nhưng với nhiều tham số hơn (C2 so với C1 và C2-MoE4 so với C1-MoE4).

Lặp đệ quy. Bảng 2b cho thấy nhiều lặp đệ quy hơn tạo ra hiệu suất tốt hơn với cùng số lượng tham số. Cụ thể, đối với C1-G12, 12 nhóm khối được chia sẻ, có thể được xem như nhóm được tính toán đệ quy, và hiệu suất tốt hơn nhiều so với mô hình không chia sẻ C1. Tương tự, C2-G6 hoạt động tốt hơn C2 với cùng số lượng tham số. C2-G6, có nhiều khối hơn trong một nhóm so với C1-G12, hoạt động tốt hơn C1 với cùng lặp tính toán. MoE cải thiện khả năng và hiệu suất của C1-G12 và C2-G6.

Router và chuẩn hóa riêng lẻ. Bảng 2c so sánh hiệu ứng của các router và chuẩn hóa riêng lẻ. Chúng ta có thể thấy rằng nếu các router và các mô-đun chuẩn hóa đều được chia sẻ, hiệu suất của mô hình chia sẻ tham số bị hại nặng nề. Giữ mỗi khối conformer có mô-đun chuẩn hóa riêng tại mỗi nhóm có thể làm cho các mô-đun có thống kê phù hợp để hiệu suất tốt hơn. Và các router riêng lẻ làm cho mô-đun MoE có thể chọn các chuyên gia phù hợp ở các cấp độ khác nhau. Do đó, mô hình với router và chuẩn hóa riêng lẻ có thể đạt được hiệu suất tốt hơn.

Chưng cất kiến thức từ nhúng ẩn. Chúng tôi sử dụng thêm chưng cất kiến thức để làm cho mô hình chia sẻ tham số bắt chước mô hình tham số đầy đủ (Bảng 2d). Chúng ta có thể thấy rằng với chưng cất kiến thức, hiệu suất của mô hình chia sẻ tham số được cải thiện thêm cho mô hình C2-MoE4-G6. Tuy nhiên, sự cải thiện không rất đáng kể cho C1-MoE4-G12. Điều này có thể là do mô hình C1 có ít khối conformer hơn nhiều khi so sánh với mô hình C12 (1 so với 12), sau đó sự khác biệt lớn như vậy ảnh hưởng đến hiệu ứng của chưng cất kiến thức [36] giữa chúng.

Khoảng cách L2 giữa đầu vào và đầu ra. Hình 2 cho thấy khoảng cách L2 của đầu vào và đầu ra của mỗi phép biến đổi cho một phát âm ví dụ. Chúng ta có thể thấy rằng C2-MoE4-G6 cho thấy hành vi tương tự với mô hình tham số đầy đủ C12. Trong khi đường cong của mô hình tất cả-chia sẻ đang dao động. Điều này cho thấy rằng các router và chuẩn hóa riêng lẻ có hiệu ứng trong việc ổn định các tham số mạng.

--- TRANG 4 ---
Bảng 2: Nghiên cứu loại bỏ trên AISHELL-1. "n." biểu thị các mô-đun chuẩn hóa, và "r." biểu thị các router. "indiv." có nghĩa là các mô-đun tương ứng không được chia sẻ.

(a) có MoE so với không có MoE.
Mô hình | Npe | Dev. | Test
C1 | 1.95M | 7.53 | 8.41
C1-MoE4 | 3.53M | 7.26 | 8.05
C2 | 3.74M | 5.86 | 6.50
C2-MoE4 | 6.89M | 5.77 | 6.22

(b) có chia sẻ tham số so với không có chia sẻ tham số.
Mô hình | Npe | Dev. | Test
C1 | 1.95M | 7.53 | 8.41
C1-G12 | 1.95M | 5.65 | 6.07
C1-MoE4-G12 | 3.59M | 5.01 | 5.40
C2 | 3.74M | 5.86 | 6.50
C2-G6 | 3.74M | 5.18 | 5.62
C2-MoE4-G6 | 6.95M | 4.67 | 5.08

(c) Router và chuẩn hóa riêng lẻ.
Mô hình | Npe | Dev. | Test
C1-MoE4-G12 (tất cả chia sẻ) | 3.53M | 6.39 | 6.90
C1-MoE4-G12 (indiv. n.) | 3.58M | 5.19 | 5.57
C1-MoE4-G12 (indiv. n. & r.) | 3.59M | 5.01 | 5.40
C2-MoE4-G6 (tất cả chia sẻ) | 6.89M | 5.60 | 6.00
C2-MoE4-G6 (indiv. n.) | 6.94M | 4.72 | 5.21
C2-MoE4-G6 (indiv. n. & r.) | 6.95M | 4.67 | 5.08

(d) Chưng cất kiến thức từ nhúng ẩn.
Mô hình | Npe | Dev. | Test
C12 (giáo viên) | 21.58M | 4.46 | 4.93
C1-MoE4-G12 | 3.53M | 5.01 | 5.40
C1-MoE4-G12-KD | 3.53M | 4.99 | 5.43
C2-MoE4-G6 | 6.95M | 4.67 | 5.08
C2-MoE4-G6-KD | 6.95M | 4.65 | 5.03

[Biểu đồ hiển thị khoảng cách L2 giữa đầu vào và đầu ra cho từng phép biến đổi]

Hình 2: Khoảng cách L2 giữa đầu vào và đầu ra cho mỗi phép biến đổi.

6. Kết luận và Nghiên cứu Tương lai

Bài báo này khám phá việc chia sẻ hỗn hợp các chuyên gia cổng thưa thớt (MoE) để xây dựng một mô hình conformer hiệu quả tham số cho nhận dạng giọng nói. Cụ thể, chúng tôi đầu tiên sử dụng MoE để mở rộng khả năng của một khối conformer. Sau đó, chúng tôi chia sẻ các tham số của các khối conformer được nhóm để các tham số được giảm đáng kể so với mô hình tham số đầy đủ. Để đảm bảo các biểu diễn thích ứng ở các cấp độ khác nhau, chúng tôi làm cho các router của MoE và các mô-đun chuẩn hóa riêng lẻ. Hơn nữa, chúng tôi sử dụng chưng cất kiến thức để cải thiện thêm hiệu suất. Kết quả thực nghiệm chứng minh rằng mô hình được đề xuất có thể đạt được hiệu suất cạnh tranh với khoảng 1/3 tham số của bộ mã hóa, so với mô hình tham số đầy đủ. Trong tương lai, chúng tôi sẽ mở rộng phương pháp được đề xuất cho các bộ dữ liệu quy mô lớn hơn và các mô hình ASR khác, chẳng hạn như transducer và CTC.

--- TRANG 5 ---
7. Tài liệu tham khảo

[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[2] L. Dong, S. Xu, and B. Xu, "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition," in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5884–5888.

[3] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., "Conformer: Convolution-augmented transformer for speech recognition," arXiv preprint arXiv:2005.08100, 2020.

[4] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition," in 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2016, pp. 4960–4964.

[5] P. Shaw, J. Uszkoreit, and A. Vaswani, "Self-attention with relative position representations," arXiv preprint arXiv:1803.02155, 2018.

[6] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, "Transformer-xl: Attentive language models beyond a fixed-length context," arXiv preprint arXiv:1901.02860, 2019.

[7] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y. Liu, "Understanding and improving transformer from a multi-particle dynamic system point of view," arXiv preprint arXiv:1906.02762, 2019.

[8] A. Fan, E. Grave, and A. Joulin, "Reducing transformer depth on demand with structured dropout," arXiv preprint arXiv:1909.11556, 2019.

[9] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, "Albert: A lite bert for self-supervised learning of language representations," arXiv preprint arXiv:1909.11942, 2019.

[10] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser, "Universal transformers," arXiv preprint arXiv:1807.03819, 2018.

[11] S. Li, C. Ding, X. Lu, P. Shen, T. Kawahara, and H. Kawai, "End-to-end articulatory attribute modeling for low-resource multilingual speech recognition." in INTERSPEECH, 2019, pp. 2145–2149.

[12] Y. Zhao, C. Ni, C.-C. Leung, S. R. Joty, E. S. Chng, and B. Ma, "Universal speech transformer." in INTERSPEECH, 2020, pp. 5021–5025.

[13] T. Komatsu, "Non-autoregressive asr with self-conditioned folded encoders," arXiv preprint arXiv:2202.08474, 2022.

[14] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017.

[15] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby, "Scaling vision with sparse mixture of experts," Advances in Neural Information Processing Systems, vol. 34, 2021.

[16] Z. You, S. Feng, D. Su, and D. Yu, "Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts," arXiv preprint arXiv:2105.03036, 2021.

[17] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," arXiv preprint arXiv:2101.03961, 2021.

[18] F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, "Go wider instead of deeper," arXiv preprint arXiv:2107.11817, 2021.

[19] G. Hinton, O. Vinyals, J. Dean et al., "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, vol. 2, no. 7, 2015.

[20] J. Li, R. Zhao, J.-T. Huang, and Y. Gong, "Learning small-size dnn with output-distribution-based criteria," in Fifteenth annual conference of the international speech communication association, 2014.

[21] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu, "On layer normalization in the transformer architecture," in International Conference on Machine Learning. PMLR, 2020, pp. 10 524–10 533.

[22] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, "Language modeling with gated convolutional networks," in International conference on machine learning. PMLR, 2017, pp. 933–941.

[23] P. Ramachandran, B. Zoph, and Q. V. Le, "Searching for activation functions," arXiv preprint arXiv:1710.05941, 2017.

[24] A. Graves, "Sequence transduction with recurrent neural networks," arXiv preprint arXiv:1211.3711, 2012.

[25] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376.

[26] J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.

[27] E. B. Zaken, S. Ravfogel, and Y. Goldberg, "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models," arXiv preprint arXiv:2106.10199, 2021.

[28] S. Li, D. Raj, X. Lu, P. Shen, T. Kawahara, and H. Kawai, "Improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation." in Interspeech, 2019, pp. 4400–4404.

[29] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline," in 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1–5.

[30] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, "Audio augmentation for speech recognition," in Sixteenth annual conference of the international speech communication association, 2015.

[31] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., "The kaldi speech recognition toolkit," in IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Signal Processing Society, 2011.

[32] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, "Specaugment: A simple data augmentation method for automatic speech recognition," arXiv preprint arXiv:1904.08779, 2019.

[33] T.-S. Nguyen, S. Stueker, J. Niehues, and A. Waibel, "Improving sequence-to-sequence speech recognition training with on-the-fly data augmentation," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7689–7693.

[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., "Pytorch: An imperative style, high-performance deep learning library," Advances in neural information processing systems, vol. 32, 2019.

[35] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, "Fastmoe: A fast mixture-of-expert training system," arXiv preprint arXiv:2103.13262, 2021.

[36] J. H. Cho and B. Hariharan, "On the efficacy of knowledge distillation," in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 4794–4802.
