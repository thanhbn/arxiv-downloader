# 2312.04927.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2312.04927.pdf
# File size: 3249054 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Zoology: Measuring and Improving Recall in Efficient Language
Models
Simran Arora∗†, Sabri Eyuboglu∗†, Aman Timalsina△, Isys Johnson‡, Michael Poli†, James
Zou†, Atri Rudra‡, and Christopher R´ e†
†Stanford University
‡University at Buffalo
△Purdue University
‡{simran, eyuboglu, poli, jamesz, chrismre }@cs.stanford.edu
‡{isysjohn,atri }@buffalo.edu
△{atimalsi }@purdue.edu
December 11, 2023
Abstract
Attention-free language models that combine gating andconvolutions are growing in popularity due
to their efficiency and increasingly competitive performance. To better understand these architectures,
we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-
convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In
fine-grained analysis, we find 82% of the gap is explained by each model’s ability to recall information that
is previously mentioned in-context, e.g. Hakuna Matata means no worries Hakuna Matata it means no →
??. On this task, termed associative recall , we find that attention outperforms gated-convolutions by a
large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution
model on associative recall. This is surprising because prior work shows gated convolutions can perfectly
solve synthetic tests for AR capability. To close the gap between synthetics and real language, we develop
a new formalization of the task called multi-query associative recall ( Mqar ) that better reflects actual
language. We perform an empirical and theoretical study of Mqar that elucidates differences in the
parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate
simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention
patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Our code is
accessible at: https://github.com/HazyResearch/zoology .
1 Introduction
Two advances – gating and long convolutions – have catalyzed a wave of excitement around gated-convolution
language models [Fu et al., 2023a, Ma et al., 2022, Wang et al., 2022, Poli et al., 2023a, inter alia.]. These
architectures combine gating ( i.e.element-wise multiplication) with long convolutional filters ( i.e.the length
of the sequence) to enable interactions between distant tokens [Dauphin et al., 2017, Gu et al., 2021]. Recent
work suggests that these models, which exhibit better asymptotic scaling in input sequence length than
attention, can match attention in language modeling quality [Poli et al., 2023a, Peng et al., 2023, Fu et al.,
2023b].
We pretrain and evaluate 17 language models across 4 scales (70M - 1.4Bn) and 5 architectures on the
same data and infrastructure setup. Surprisingly, we find that there is still a perplexity gap of up to 2.1
points between state-of-the-art convolution-based architectures and strong Transformer baselines in language
modeling on the Pile (Table 1). Through fine-grained analysis, we find a single, simple capability is responsible
∗Equal contribution, Random ordering by coin toss.
1arXiv:2312.04927v1  [cs.CL]  8 Dec 2023

--- PAGE 2 ---
/gid00044/gid00072/gid00076/gid00001/gid00076/gid00068/gid00083/gid00001/gid00053/gid00072/gid00076/gid00001/gid00051/gid00072/gid00066/gid00068/gid00001/gid00072/gid00077/gid00001/gid00046/gid00064/gid00081/gid00066/gid00071/gid00001/gid00019/gid00017/gid00018/gid00025/gid00001/gid00015/gid00015/gid00015/gid00001/gid00075/gid00064/gid00083/gid00068/gid00081/gid00001/gid00072/gid00077/gid00001/gid00046/gid00064/gid00081/gid00066/gid00071/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00064/gid00069/gid00083/gid00068/gid00081/gid00001/gid00053/gid00072/gid00076 /gid00001         /gid00071/gid00064/gid00067/gid00015/gid00015/gid00015/gid00034/gid00082/gid00082/gid00078/gid00066/gid00015/gid00001/gid00081/gid00068/gid00066/gid00064/gid00075/gid00075/gid00001/gid00071/gid00072/gid00083
/gid00053/gid00071/gid00068/gid00001/gid00066/gid00078/gid00077/gid00083/gid00068/gid00082/gid00083/gid00001/gid00082/gid00083/gid00064/gid00081/gid00083/gid00082/gid00001/gid00086/gid00072/gid00083/gid00071/gid00001/gid00083/gid00071/gid00068/gid00001
/gid00080/gid00084/gid00064/gid00075/gid00072/gid00195/gid00066/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00079/gid00071/gid00064/gid00082/gid00068/gid00013/gid00001/gid00086/gid00071/gid00072/gid00066/gid00071/gid00001/gid00083/gid00064/gid00074/gid00068/gid00082/gid00001
/gid00079/gid00075/gid00064/gid00066/gid00068/gid00001/gid00078/gid00085/gid00068/gid00081/gid00001/gid00083/gid00071/gid00068/gid00001/gid00079/gid00081/gid00068/gid00066/gid00068/gid00067/gid00072/gid00077/gid00070/gid00001/gid00001/gid00083/gid00071/gid00081/gid00068/gid00068/gid00001/gid00001/gid00015/gid00015/gid00015/gid00048/gid00083/gid00071/gid00068/gid00081/gid00001/gid00083/gid00078/gid00074/gid00068/gid00077/gid00082
/gid00053/gid00071/gid00068/gid00001/gid00066/gid00078/gid00076/gid00076/gid00078/gid00077/gid00001/gid00001/gid00065/gid00084/gid00089/gid00089/gid00064/gid00081/gid00067/gid00001/gid00072/gid00082/gid00001/gid00064/gid00001/gid00065/gid00072/gid00081/gid00067/gid00001
/gid00086/gid00072/gid00083/gid00071/gid00001/gid00064/gid00001/gid00075/gid00064/gid00081/gid00070/gid00068/gid00001/gid00081/gid00064/gid00077/gid00070/gid00068/gid00015/gid00001/gid00053/gid00071/gid00078/gid00084/gid00070/gid00071/gid00001
/gid00066/gid00078/gid00076/gid00079/gid00064/gid00066/gid00083/gid00013/gid00001/gid00064/gid00001/gid00001/gid00066/gid00078/gid00076/gid00076/gid00078/gid00077/gid00001/gid00001/gid00065/gid00084/gid00089/gid00089/gid00064/gid00081/gid00067/gid00001/gid00001/gid00015/gid00015/gid00015/gid00001/gid00035/gid00072/gid00070/gid00081/gid00064/gid00076/gid00001/gid00078/gid00066/gid00066/gid00084/gid00081/gid00082/gid00001/gid00072/gid00077/gid00014/gid00066/gid00078/gid00077/gid00083/gid00068/gid00087/gid00083 /gid00039/gid00072/gid00081/gid00082/gid00083/gid00001/gid00078/gid00066/gid00066/gid00084/gid00081/gid00081/gid00068/gid00077/gid00066/gid00068/gid00001/gid00078/gid00069/gid00001/gid00065/gid00072/gid00070/gid00081/gid00064/gid00076
/gid00001/gid00019/gid00017/gid00018/gid00025/gid00001
+/gid00001/gid00051/gid00072/gid00066/gid00068
+/gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077
/gid00040/gid00064/gid00083/gid00068/gid00067/gid00014/gid00036/gid00078/gid00077/gid00085
/gid00044/gid00072/gid00076/gid00001/gid00076/gid00068/gid00083/gid00001/gid00053/gid00072/gid00076/gid00001/gid00051/gid00072/gid00066/gid00068/gid00001/gid00072/gid00077/gid00001/gid00046/gid00064/gid00081/gid00066/gid00071/gid00001/gid00019/gid00017/gid00018/gid00025/gid00001/gid00015/gid00015/gid00015/gid00001/gid00075/gid00064/gid00083/gid00068/gid00081/gid00001/gid00072/gid00077/gid00001/gid00046/gid00064/gid00081/gid00066/gid00071/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00001/gid00064/gid00069/gid00083/gid00068/gid00081/gid00001/gid00053/gid00072/gid00076 /gid00001         /gid00071/gid00064/gid00067/gid00015/gid00015/gid00015 /gid00001/gid00044/gid00072/gid00076
+/gid00001/gid00051/gid00072/gid00066/gid00068
+/gid00075/gid00078/gid00070/gid00009/gid00079/gid00068/gid00081/gid00079/gid00075/gid00068/gid00087/gid00072/gid00083/gid00088/gid00010/gid00082/gid00078/gid00069/gid00083/gid00076/gid00064/gid00087 /gid00009/gid00080/gid00074/gid00053/gid00010/gid00085/gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077
/gid00051/gid00056/gid00044/gid00055 /gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077 /gid00041/gid00088/gid00068/gid00077/gid00064/gid00088
/gid00049/gid00081/gid00078/gid00073/gid00068/gid00066/gid00083/gid00072/gid00078/gid00077/gid00084/gid00056/gid00001/gid00012/gid00001/gid00065/gid00049/gid00081/gid00078/gid00073/gid00068/gid00066/gid00083/gid00072/gid00078/gid00077
/gid00084/gid00056/gid00001/gid00012/gid00001/gid00065/gid00036/gid00078/gid00077/gid00085/gid00078/gid00075/gid00084/gid00083/gid00072/gid00078/gid00077 /gid00001/gid00001/gid00001/gid00001/gid00074/gid00001/gid00001/gid00084
/gid00084 ./gid00034/gid00070/gid00070/gid00081/gid00068/gid00070/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00072/gid00082/gid00001
/gid00066/gid00078/gid00077/gid00083/gid00081/gid00078/gid00075/gid00075/gid00068/gid00067/gid00001/gid00065/gid00088/gid00001
/gid00074/gid00068/gid00081/gid00077/gid00068/gid00075/gid00001/gid00086/gid00068/gid00072/gid00070/gid00071/gid00083/gid00082/gid00036/gid00078/gid00077/gid00085/gid00078/gid00075/gid00084/gid00083/gid00072/gid00078/gid00077/gid00001/gid00044/gid00068/gid00081/gid00077/gid00068/gid00075
/gid00074/gid00088 /gid00084/gid00034/gid00070/gid00070/gid00081/gid00068/gid00070/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00072/gid00082/gid00001
/gid00067/gid00068/gid00079/gid00068/gid00077/gid00067/gid00068/gid00077/gid00083/gid00001/gid00078/gid00077/gid00001
/gid00072/gid00077/gid00079/gid00084/gid00083/gid00001/gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077/gid00001/gid00052/gid00066/gid00078/gid00081/gid00068/gid00082
/gid00074/gid00080
/gid00085/gid00080/gid00074/gid00053Figure 1: The associative recall gap. We stratify Pile validation data for models from each architecture
class by whether or not the predicted token is a previously seen bigram in the example context. We plot
validation perplexity versus the bigram’s frequency in the training data. We can clearly see that the gap is
localized to examples where bigrams occur, and are seen rarely during in training.
for much of the gap: recalling information seen in-context. Consider the example below, where some tokens
can be predicted by recalling an earlier association:
Hakuna Matata!| {z }
Key-ValueIt means no worries|{z}
Key-Valuefor the rest of your days! Hakuna|{z}
QueryMatata|{z}
AR Hitmeans no|{z}
Query→worries|{z}
AR Hit
We find that errors on “AR Hits” ( e.g. worries above) account for 82% of the perplexity gap to attention on
average, despite only representing 6 .4% of all tokens in the Pile dataset.1A 70M parameter Transformer can
predict AR Hits better than a 1.4Bn parameter Hyena gated convolution model (20 ×larger) (Table 1, Table
5). The AR gap persists at the 7Bn parameter scale when comparing RWKV and Llama-2 (Appendix G).
This task, associative recall (AR), has a long history in machine learning [Graves et al., 2014, Ba et al.,
2016, inter alia.] (Appendix A.1). Prior work argues that a model’s ability to perform AR is predictive of
in-context learning quality [Elhage et al., 2021, Olsson et al., 2022]. As a result, AR has been adopted as a
tool in designing new architectures and prior work has shown that gated convolution architectures match
attention on synthetic AR tasks used as proxies for real language modeling [Fu et al., 2023a, Poli et al., 2023a,
Lutati et al., 2023]. For this reason, the downstream AR perplexity gaps are surprising.
Through measuring recall on real data, we learn the key disparity is that prior synthetic formulations assume
there is one query per input, at a fixed position in the sequence, where tokens come from a small vocabulary
size ( e.g.|V|<50, less than model dimension). Yet, language modeling often requires performing multiple
recalls (e.g.for both “Hakuna Matata” and “no worries” above, in a single forward pass), at varying positions ,
with tokens from a large vocabulary (larger than model dimension). We thus propose the study of multi-query
AR (Mqar ). Compared to the prior AR formulations, Mqar better captures the persisting quality gaps on
synthetic and real world data (Section 4). However, it is not clear why Mqar elucidates the gap.
We formalize the the Mqar gap across the architecture classes. Gated-convolutions process variable
sequences using fixed filters defined by the model weights rather than as functions of the input data (See
Figure 1). We find it is inefficient for gated-convolutions to perform the variable distance token-to-token
interactions (e.g. at a distance of 10 tokens for Hakuna Matata and 9 for no worries ) in a parameter and
FLOPs efficient way compared to attention. Attention achieves input-dependence since it computes all
token-to-token interactions when determining how to mix information in the sequence. We formally describe
the limitation of gated convolutions via theory and experiments in the rest of the paper.
We first introduce a simple operator, BaseConv , that can provably simulate the class of architectures built
from gating and convolution primitives. This includes architectures such as H3, Hyena, RWKV, and RetNet.
1We measure AR on real data using a simple heuristic: n-gram tokens that are repeated in context.
2

--- PAGE 3 ---
Overall Slices % of gap due to
Model Param (M) TFLOPs AR Hits Other Tokens AR Hits
Attention 125 2.46 11.01 (2.40) 2.16 (0.77) 12.45 (2.52) —
Long Conv 128 1.74 16.98 (2.83) 25.62 (3.24) 16.46 (2.80) 40.1%
H3 168 2.55 12.06 (2.49) 6.75 (1.91) 12.60 (2.53) 88.4%
Hyena 158 2.41 11.60 (2.45) 5.00 (1.61) 12.28 (2.51) 100.0%
RWKV 169 2.08 11.64 (2.45) 5.70 (1.74) 12.29 (2.51) 100.0%
Attention 360 6.23 9.44 (2.25) 1.98 (0.69) 10.62 (2.36) —
Long Conv 360 4.08 13.13 (2.57) 13.27 (2.59) 13.12 (2.57) 40.5%
H3 357 4.85 10.38 (2.34) 4.81 (1.57) 11.00 (2.40) 65.8%
Hyena 358 5.03 10.07 (2.31) 3.83 (1.34) 10.75 (2.38) 98.2%
RWKV 351 4.31 9.79 (2.28) 3.82 (1.34) 10.51 (2.35) 100.0%
Table 1: Language modeling validation perplexity on the Pile. After pretraining on 10B tokens of
Pile data, we report log perplexity with negative log-likelihood in parentheses. We report overall scores, and
for the AR vs. non-AR token slices defined in Section 3. FLOPs are computed for inputs of 2048 tokens
based on the equations in Appendix C. A table with additional results is in Table 4.
We show with theory and experiments that the model dimension for BaseConv (and thus the aforementioned
architectures) to solve Mqar grows with the input sequence length (Theorem 4.4) while attention can
solve Mqar with model dimension independent of sequence length (Proposition 4.3, Section 4.3).2In
practice, gated-convolutions appear to encode approximate solutions to AR that support only a subset of
token-interaction distances (which affects the ability to identify matching keys given Mqar queries). While
theoretically analyzing deep learning architectures is challenging [Hahn, 2020, Merrill et al., 2022, Keles et al.,
2023], the fact that gating and convolutions are polynomial operations facilitates our precise analysis.
We show that input-dependent sequence mixing is important to solve Mqar efficiently. The
scaling for gated convolutions with input-independent convolutions is undesirable so we next ask which
architectural choices close the gap. We show that data-dependent sequence mixing helps an architecture
solveMqar efficiently (Theorem 4.5). The model needs to adapt the sequence mixing weights based on the
token-interaction distances required for each new example.
Several architectural modifications could satisfy the input-dependence property. Based on our analysis, we
evaluate minimal modifications that only add input-dependent operations on exact-match repeated bigram
tokens (our heuristic for measuring tokens that require recall). Note that language models often need
to perform recall between fuzzier substrings (e.g. synonymous bigrams) or higher-dimensional concepts.
However, we show that simply inserting input-dependent operator — e.g., a convolution filter that shifts the
sequence based on the bigram positions or sparse attention placed only on repeated bigram positions — to the
BaseConv architecture at <10% of layers suffices to outperform the Transformer baseline on Pile language
modeling (Section 5) and succeed on Mqar synthetic tasks (Section 4.3). Moreover, this closes >80% of the
Mqar perplexity gap on the Pile validation data. Finally, we prototype solutions that learn the positions at
which to use input-dependent operations, validating that they also close the gap to attention on the Pile.
In this work, we analyze the increasingly-popular convolution-based architectures and identify fundamental
limitations. We hope the Mqar framework and our analysis of the role of input-dependence for Mqar
inform the design of future architectures. We release our code for reproducability: https://github.com/
HazyResearch/zoology .
2 Background and Preliminaries
In this section, we describe the setting, introduce notation, and discuss important related work. See
Appendix A for a broader discussion of related work.
Language modeling. We study auto-regressive language models trained on the task of next token
2Note that the runtime of Attention is quadratic, but gated convolutions is near linear.
3

--- PAGE 4 ---
prediction [Gao et al., 2020]. Given a sequence of Ntokens x={x0, ..., x N−1}drawn from a vocabulary C,
the model outputs a probability distribution over Cfor each of xigiven the preceding tokens P( xi|x0, ..., x i−1).
The language models in this work share the same high-level architecture. First, each token xiin the input is
embedded in d-dimensional space yielding a matrix u∈RN×d. Next, uis passed through a stack of Llayers,
with layer ℓoutputting uℓ∈RN×d. Finally, the embeddings uLoutput by the last layer are mapped back
to logits over Cwith a linear projection. Each layer transforms uwith a sequence mixer (e.g.attention)
followed by a state mixer (e.g.MLP). Unless specified, our models adhere to the implementation details of
the LLaMA architecture (except the sequence mixer, which we vary throughout) [Touvron et al., 2023].
Sequence mixers. Our work evaluates how the choice of sequence mixer affects the quality and behavior
of language models. Most sequence mixers aggregate the token embeddings in a sequence via a weighted sum.
For example, y[i,:] =PN−1
j=0ω(i, j)u[j,:]), where ωis a function outputting scalar weights. We study the
differences between two classes of sequence mixers, discussed next.
Attention. [Vaswani et al., 2017] We review attention, the de facto language model sequence mixer. An
attention layer is parameterized by three learnable projection matrices Q,K,V∈RN×d. To compute the
output ygiven inputs u, attention applies the projections to the input: q=Qu,k=Ku,v=Vu. The
projected embeddings are aggregated according to: y=softmax (1√
dqk⊤)v(shown in Fig. 1) in O(N2d) time,
which is expensive for long sequences (large N).
Gated-convolutions. A more efficient alternative to attention is the convolution, which is defined as
y[i,:] =PN−1
j=0k[j,:]⊙u[i−j,:] where the kernel k∈RN×dis a learnable weight matrix. Convolutions can
be computed in time O(NdlogN) using the Fast Fourier Transform (FFT) and the convolution theorem:
y=u∗k=FFT−1(FFT( u)⊙FFT( k))[Cooley and Tukey, 1965]. While purely convolutional architectures
match or outperform attention in certain domains ( e.g.vision [Tay et al., 2022, Gu et al., 2021], audio [Goel
et al., 2022], and time-series [Zhang et al., 2023]), they trail by a large margin on language [Fu et al., 2023a].
Recent work has closed much of this gap by combining convolutions with gating (i.e.elementwise multiplication
of the input with a transformed version of itself). Gating was first proposed in the context of convolutions
by Dauphin et al. [2017], but more recently it has played a central role in state-of-the-art sub-quadratic
language models, some of which claim attention-level quality [Fu et al., 2023a, Wang et al., 2022, Poli et al.,
2023a, Peng et al., 2023, Zhai et al., 2021, Fu et al., 2023b, inter alia]. Though they may appear different on
the surface, these sub-quadratic architectures can all be expressed in terms of convolutions and gating. See
Appendix H.2 for detailed descriptions of these architectures and their similarities. This work analyzes the
differences between attention and the broad class of gated convolution mixers.
3 Identifying the associative recall problem
In this section, we measure the perplexity gap between gated convolutions and attention and show that single
skill termed associative recall accounts for 82% of the gap on average. This is surprising because prior work
shows gated convolutions solve a synthetic version of associative recall perfectly. Informed by our analysis,
we define a new synthetic formulation that better reflects real data. This task facilitates our analysis of why
the gap occurs (Section 4) and how to fix it (Section 5).
3.1 Fine-grained analysis of downstream quality
Perplexity Gap We pretrain a suite of large language models with different sequence mixers across 3
scales (70M-360M) for 10B tokens on the standard Pile language modeling setting using the EleutherAI
GPT-NeoX training infrastructure [Gao et al., 2020, Andonian et al., 2023]. In the main paper, we compare
attention to three state-of-the-art gated convolution sequence mixers: H3, Hyena, and RWKV [Fu et al.,
2023a, Poli et al., 2023a, Peng et al., 2023].3We further include a pure long-convolution model to underscore
the importance of gating. In Appendix F, we include results on additional sequence mixers [Hasani et al., 2022,
Sun et al., 2023]. We use a strong Transformer attention baseline with rotary embeddings and SwiGLU MLPs
following the Llama architecture [Touvron et al., 2023]. We also use this strong training recipe when training
3RWKV is commonly referred to as an RNN. We show it can be viewed as a convolution (Appendix H.2.2).
4

--- PAGE 5 ---
the attention-free sequence mixers. For experimental details and hyperparameters for all architectures see
Appendix C.
Across scales, we find that attention outperforms the gated convolutions by at least a third of a perplexity
point on average: the minimum gaps are +2 .14, +0 .59, +0 .35 PPL at 70M, 160M, and 360M parameter
scales, respectively. We report overall test perplexity in Table 1. Though these gaps are relatively small on
average, models may still perform very differently on different subsets of data [Eyuboglu et al., 2022].
Associative Recall Perplexity To better understand the differences between attention and gated
convolutions, we perform a fine-grained analysis of next token predictions and observe that convolution-based
models struggle to recall associations previously seen in context. For example, in Fig. 1, the model must
recall the association between “ Tim” and the last name “ Rice”. Following a long line of prior work, we call
this skill associative recall (AR) [Willshaw et al., 1969, Hopfield, 1982] (see Appendix A for an extended
discussion of AR’s history in machine learning). In Appendix D.1.1, we provide annotated Pile examples to
demonstrate the phenomenon qualitatively.
Quantifying AR performance. It is challenging to derive a quantitative measure of associative recall performance
on the Pile because we don’t know which next token predictions in raw text require associative recall. We
use a simple heuristic to identify these tokens, which we refer to as AR Hits . An AR Hit is the last token
of an n-gram repeated in context ( e.g.the second occurence of “ Rice” in Fig. 1). However, some common
n-grams ( e.g. “of the” ) could have been memorized during training, so we factor in the frequency with which
ann-grams appeared in the training data. This heuristic enables us to scale our analysis to over 10 million
tokens of Pile validation data.
We stratify Pile tokens into two slices based on this heuristic and report perplexity on each in Table 1:
1.AR Hits : (6.4% of tokens) Tokens in the final position of a bigram (a pair of consecutive tokens) which
previously appeared in context, but ≤1250×during training.
2.Other tokens: (93.6% of tokens) Tokens in the final position of a bigram which did not previously
appear in context or it appeared >1,250 times during training.
In Figure 1, we visualize these slices by plotting log-perplexity against the frequency with which bigrams
appear during training. Strikingly, the gap between attention and gated convolutions is the largest on AR
hits with the fewest occurrences. On the other tokens, there is no gap.
In Table 1, we also compute the percentage of the difference in perplexity between attention and each model
that is due to AR tokens:∆ log( ϕAR)·|TAR|
∆ log( ϕ)·|T|, where ϕis the perplexity and Tis the set of tokens in the test set.
This quantity can also be interpreted as the fraction of the overall gap that would close if a model matched
attention on the AR slice. We find that the AR slice accounts for 82% of the average quality gap between the
gated convolutions and attention.
To evaluate the AR capacity of larger models, we train two 1.4 billion parameter attention and Hyena models
for 50 billion tokens on the Pile and repeat this analysis (see Table 5). Strikingly, a 70 million parameter
attention model is a full perplexity point better in the AR slice than this 1.4B Hyena model that is 20 ×its
size (2.41 vs. 3.43 ppl.). In Appendix G, we also evaluate open-source RWKV and attention models trained
up to 7 billion parameters. We use a controlled semi-synthetic dataset to measure AR capacity and show that
RWKV’s performance degrades sharply as we increase the number of queries in an example while attention
performs consistently well Appendix G.
3.2 Formalizing the problem: Multi-Query Associative Recall
This gap in associative recall perplexity is very surprising because prior work shows gated-convolutions can
perfectly solve a formalized version of the task [Fu et al., 2023a, Poli et al., 2023a, Olsson et al., 2022]. In
this synthetic task, the input xcontains a sequence of bigrams representing key-value pairs from a random
dictionary followed by a single query token. For example, the correct output for the input below would be 3:
A 4 B 3|{z}
Key-ValueC 6 E 2 F 1 C 6 G 8 →B ?|{z}
Query
5

--- PAGE 6 ---
Gated convolutions ( e.g.H3, Hyena, RWKV) can solve this task perfectly for most sequence lengths.
These conclusions are inconsistent with our findings on the Pile, as described above, so we ask how this
formulation of AR differs from the way AR manifests in real language. We identify a major difference. In
real world inputs, the language model often needs to perform multiple associative recalls in a single forward
pass, at varying positions in the sequence ( e.g. “Tim Rice” and“March 2018” in Fig. 1. We refer to this as
Multi-Query AR ( Mqar ). We formally define the Mqar problem as follows:4
Definition 3.1 (Multi-Query-AR (Mqar )).We are given an input sequence x={x0, . . . , x N−1}where each
xi∈Cis a token drawn from a vocabulary of size c=|C|. The task is to check, for every query 1 ≤i < N ,
whether there exists a 0 ≤j < i such that ui≡uj. If so, output uj+1.
For example, the correct output for input below would be 4, 6, 1, 2, 3:
A 4 B 3 C 6 F 1|{z}
Key-ValueE 2→A ? C ? F ?|{z}
QueryE ? B ?
In Section 4, we use Mqar to explain the quality gap between gated convolutions and attention.
4 Explaining the associative recall problem
In this section, we provide an explanation for the gap in associative recall performance by analyzing the
formal Mqar task theoretically and empirically. In Section 4.1, we define a simple gated-convolution
architecture, called BaseConv , which we show can simulate a broad class of architectures built from gating
and convolutions. This allows us to make general statements that apply to popular gated-convolution
architectures like Hyena, RWKV, or H3. In Section 4.2, we show that there exist theoretical solutions to
Mqar that could in principle be learned by BaseConv , and we analyze their complexity in terms of model
width and depth. In Section 4.3, we use experiments on synthetic data to show that solving Mqar with
BaseConv (and other gated-convolution architectures) requires model dimension to scale linearly with the
sequence length. In contrast, attention solves Mqar consistently in our experiments with model dimension
scaling independently of sequence length. These empirical scaling laws provide a potential explanation for
the AR gap and, alongside our theoretical analysis, point to the potential solutions discussed in Section 5.
4.1 BaseConv : a minimal gated convolution operator
In this section, we define our minimal gated-convolution architecture, called BaseConv . Given a function,
we would like to know the most efficient model ( e.g.parameters, FLOPs) that can represent the solution.
In this work, we show we can precisely reason about this question for representing polynomial functions
with gated convolutions as gating and convolutions are both polynomial operations. The standard model
defining computational complexity for polynomials is by the size of the smallest arithmetic circuit that can
compute the polynomial. We define the BaseConv gated convolution operator which is exciting because
(1) it is universal in that it that can simulate any arithmetic circuit C(with only a poly-log blowup in the
corresponding parameters) and (2) it is simple to implement efficiently (19 lines of pure PyTorch including
imports, see Appendix B).
Definition 4.1 (BaseConv Operator) .Given an input u∈ RN×d, theBaseConv operator for layer ℓis
defined as:
y:= 
u·Wℓ+bℓ
1
|{z }
Linear Projection⊙ 
hℓ∗u+bℓ
2
|{z}
Convolution(1)
where the layer is parameterized by learnable filters h∈ RN×d, a linear projection Wℓ∈ Rd×d, and ‘bias’
matrices b1,b2∈ RN×d. The ⊙is component-wise product and convolution of two matrices is computed as
convolution of the corresponding columns.
4In Appendix H.7.1 we define a formal, general form of this definition, used in the theoretical analysis.
6

--- PAGE 7 ---
In our experiments, each BaseConv layer uses ˜O(Nd+d2) parameters5and can be computed in ˜O(Nd2)
operations. For our theoretical results, we can assume the weight matrix Wℓis restricted to a class of
matrices that support near-linear time matrix multiplication ( e.g.Kaleidoscope matrices, see Definition H.3).
Under this assumption, BaseConv uses ˜O(Nd) parameters and ˜O(Nd) FLOPs (Proposition H.6). We now
state the equivalency result between arithmetic circuits and BaseConv a “canonical” representation of
arithmetic circuits (Theorem H.21 in Appendix H.5):
Theorem 4.2 (Equivalency to Arithmetic Circuits) .For an arithmetic circuit Cof size sand depth ∆that
takesu∈ RN×das input, there exists an equivalent BaseConv operator that uses ˜O(s∆)parameters and
˜O(∆)layers.6
In other words, any gated convolution model with small number of layers can be simulated by BaseConv with
only a (poly)logarithmic blowup in parameters and layers. We note that arithmetic circuits are a very well
studied computation model in computational complexity B¨ urgisser et al. [1996]. Many well-known efficient
algorithms on matrices (e.g. the FFT or the current best known matrix-matrix multiplication algorithm) in
fact give small arithmetic circuits. However, arithmetic circuits are inherently discrete objects – we cannot
learn them via gradient descent. Theorem 4.2 shows that (up to poly-log loss in parameters), we can instead
learn over BaseConv models. This result generalizes a similar result from Dao et al. [2020] for the special
class of linear functions: we generalize the earlier result to the class of allpolynomials.
For specific gated convolution layers, we can get rid of the poly-logarithmic factor blowup–we observe in the
appendix that BaseConv and Hyena models can simulate each other with only a small constant blowup in
parameters (Proposition H.12 in Appendix H.5).
4.2 Theoretical analysis of gated convolution capacity and associative recall
In this section, we provide theoretical Mqar solutions that could in principle be learned by each architecture
and analyze their complexity in terms of model width and depth. First, we note that attention solves Mqar
with parameters independent of sequence length (Proposition H.27).
Proposition 4.3 (Attention) .Given an input u∈ {0,1}N×3c, Attention (even without using soft-max)
solves Mqar foruusingO(c2)parameters, O(Nc2+N2c)time complexity and O(1)layers.
It is natural to wonder then if allpairwise comparisons among tokens are necessary to solve Mqar . Indeed,
in the RAM setting, a sequential algorithm can simply utilize Nlogarithmic insertion and membership
queries to solve Mqar in subquadratic time. Unfortunately, any model attempting to emulate this would
require Ω( N) layers. Instead, we observe that we can parallelize this algorithm using dyadic intervals and
achieve a depth of ˜O(1) (Proposition H.30). We then convert this algorithm into an arithmetic circuit and
apply Theorem 4.2 to derive an equivalent BaseConv model. This allows us to prove new upper bounds for
BaseConv models applied to Mqar , which improves upon the quadratic time complexity of attention to
near-linear runtime at the cost of using poly-log layers (Theorem H.37 in Appendix H.7).
Theorem 4.4 (Data-Independent Filters7).Given an input u∈ {0,1}N×O(logc)toMqar (where we assume
that distinct tokens are embedded into distinct vectors in {0,1}O(logc)), there exists a BaseConv operator
that solves Mqar foruusing ˜O(Nlogc)parameters as well as time complexity and ˜O(1)layers.
Nevertheless, the poly-logarithmic number of layers in the above result is undesirable in practice. But, we
show that using input-dependent convolution filters, one can get constant many layers (for a sub-class of
inputs). Towards that end, we define the interaction distance between a query qiand the matching key kjas
i−j. This then allows us to present the corresponding upper bound for data-dependent mixing (Theorem H.38
in Appendix H.8).
Theorem 4.5 (Input-Dependent Filters) .Given an input u∈ {0,1}N×ctoMqar (where we assume that the
5We use ˜O(·) to hide poly-log factors.
6The formal statement in the Appendix has a sharper version of this result in terms of the circuit ‘width’.
7We note here that existing architectures also use data-independent convolution filters, meaning the filter is defined as a
function of the model parameters, independent of the input.
7

--- PAGE 8 ---
Attention hybrid
Autocorrelation
Filters
Programmatic 
FiltersRWKVAttention
Hyena
BaseConvH3Figure 2: The x-axis is the model dimension and the y-axis is accuracy on Mqar . Increasing the sequence
length correlates with increased task difficulty. Both rows use the same experimental setup, but evaluate
different sequence mixers. (Top, Claim 1) Gated convolutions and attention. We observe the
gated convolutions require larger dimensionality than attention to solve the task. (Bottom, Claim 2)
Input-dependent filters. Models with input-independent aggregation achieve improved scaling over the
gated-convolutions with input-independent aggregation.
tokens are embedded as one-hot encoding in {0,1}cand there exists at most tdistinct interaction distances),8
there exists a BaseConv operator that uses input-dependent kernels to solve the above case of Mqar using
O(t·Nc)parameters and O(1)layers.
4.3 Empirical analysis of gated convolution capacity and associative recall
In this section, we measure empirically how model dimension must scale in order for different sequence mixers
to solve Mqar .
Setup We train and evaluate models on a synthetic Mqar with vocabulary size 8 ,192, varying model
dimension and sequence length from 64 to 512. Appendix E provides further details on the formulation
and construction of this synthetic task. Following Olsson et al. [2022], we train two layer models with a
Transformer backbone that interleaves sequence mixing and state mixing (MLPs). For each architecture, we
sweep four learning rates from log(−4) to log(−2)}for each architecture, and report maximum test accuracy.
Our results, which are summarized in Figure 2, support two main claims:
Claim 1 (Gated-convolutions and attention). Gated-convolution models with two layers require model
dimension to scale at least linearly in sequence length in order to solve associative recall, while attention
models can solve it with near-constant dimensionality. We compare attention and BaseConv as well as three
popular instantiations of gated-convolution architectures: RWKV, H3, and Hyena [Peng et al., 2023, Fu et al.,
2023c, Poli et al., 2023a]. In the top row of Fig. 2, attention solves Mqar perfectly at all sequence lengths
using a constant model dimension of 64. In contrast, Mqar does not achieve accuracy >0.9 unless d≥N.
Claim 2 (Input-dependent filters). Using input-dependent filters in gated-convolution models can close
some of the gap to attention. In Theorem 4.5, we show that BaseConv with input-dependent filters
could solve Mqar with improved scaling. In this solution, we construct a filter that spikes at position j
if matching keys are separated by jtokens. We evaluate two approaches for constructing this filter: (1)
programatically ( i.e.hard-coded comparisons between token ids) or (2) with autocorrelation, which could
learn to perform fuzzy-matches (see Appendix H.8). In the bottom row of Fig. 2, we see that BaseConv
with programmatic input-dependent filters achieves near-constant scaling in model dimension and that
BaseConv with autocorrelation input-dependent filters achieves improved scaling over BaseConv with
input-independent filters.
8Note that the interaction distances can be arbitrary: there is just a bounded number of distinct distances.
8

--- PAGE 9 ---
These input-dependent filters cannot easily be made to satisfy causality and using an O(NlogN) filter per
gap could be expensive if each gap applies only to a small number of bigrams. A simpler and perhaps
more efficient way to solve the problem would be to introduce a small amount of attention to an otherwise
BaseConv model [Fu et al., 2023a]. As the first natural baseline, we evaluate an attention hybrid on
synthetic Mqar and show that it achieves improved scaling in Figure 2. Next, we put these empirical and
theoretical insights into practice on Pile language modeling.
5 Closing the Associative Recall Gap
In this section, we evaluate hybrid BaseConv -Attention models that leverage different sparsity patterns.
We show that hybrids with input-dependent sparsity patterns can close most of the gap to attention, while
maintaining sub-quadratic scaling. We also show that BaseConv hybrids can outperform attention-only
models by up to a full perplexity point, all while being dramatically simpler to implement and analyze than
prior hybrids [Fu et al., 2023a] (see implementation in Appendix B. We describe the architectures in Section 5
and results on Pile language modeling in Section 5.
Sparse BaseConv -Attention Hybrids We evaluate hybrids composed primarily of BaseConv layers
and three attention layers (6.3% of layers at 354M parameters and 10% at 168M parameters). We augment
the attention layers with operators that selectively apply attention to some tokens based on a selection
function f:RN×d→ {0,1}N. They take as input u∈RN×dand output y∈RN×d:
y[i,:] = softmax(1√
dq[i,:]k⊤)v·f(u)[i] (2)
where q,k,vare query, key, and value projections, as in attention. We evaluate four choices for f:
(1)Full attention. First, we evaluate the performance of full attention hybrids. These hybrids correspond to
fixing f(u)[i] = 1 for all iin Eq. (2). Prior work has shown full attention hybrids to be effective, but they do
not explain why supplementing gated convolutions with attention is necessary [Fu et al., 2023a]. Based on our
findings in Section 3, we hypothesize that sparse attention applied only to associative recall hits may suffice.
(2)Random selection. As a control, we evaluate sparse attention randomly applied to tokens. This corresponds
to a stochastic selection function where f(u)[i] is drawn from a Bernoulli. Many prior works have employed
sparse attention patterns like this one, which are independent of the input [Zaheer et al., 2020, Child et al.,
2019a, Beltagy et al., 2020, inter alia.].
(3)Programmatic selection. Next, to evaluate our hypothesis that attention is needed for associative recall,
we prototype a programmatic selection function that selects only those tokens that might be associative recall
hits. Specifically, f(x[i,:]) is 1 if the token xipreviously occurred in the sequence. In practice, we compare
raw token ids, not token embeddings.
f(x)[i] =(
1 if there exists j < i such that xi=xj
0 otherwise(3)
(4)Learned selection. Finally, we prototype a learned selection function f(u)[i] =σ(u[i,:]·W) parameterized
as a simple linear layer with sigmoid activation. We fix a hyperparameter kand select the top- ktokens with
the highest score in each batch. This allows us to compute attention in O(ndk) time. During training, we
add small amount of Gaussian noise to the top- kcalculation to encourage exploration and use an auxiliary
loss to encourage sparse selection: ℓf(u) =1
Nmax(0,PN
i=1f(u)[i]−k). This approach is most similar to the
recently proposed SeqBoat architecture [Ren et al., 2023]. We discuss the differences in Appendix A.
Downstream Evaluations We evaluate the prototypes on Pile language modeling. We take BaseConv
architectures at the 150M and 360M parameter scales and add input-dependent selection to three layers
(details in Appendix C.2). Results are in Table 2. We validate that the prototypes close the overall and AR
quality gaps to attention when added to the BaseConv backbone.
9

--- PAGE 10 ---
Overall Slices
Model Param (M) TFLOPs AR Hits Other Tokens
Attention 125 2.46 11.01 (2.40) 2.16 (0.77) 12.45 (2.52)
BaseConv 168 2.46 12.90 (2.56) 8.68 (2.16) 13.29 (2.59)
+ Random selection 162 2.44 12.13 (2.50) 5.33 (1.67) 12.83 (2.55)
+ Programmatic selection 162 2.44 11.06 (2.40) 2.70 (0.99) 12.19 (2.50)
+ Learned selection 166 2.44 11.06 (2.40) 3.20 (1.16) 12.14 (2.50)
+ Full attention 166 2.58 9.57 (2.26) 2.01 (0.70) 10.76 (2.38)
Attention 360 6.23 9.44 (2.25) 1.98 (0.69) 10.62 (2.36)
BaseConv 354 4.81 11.01 (2.40) 5.98 (1.79) 11.52 (2.44)
+ Random selection 365 5.06 12.94 (2.56) 6.17 (1.82) 13.62 (2.61)
+ Programmatic selection 365 5.06 9.54 (2.26) 2.35 (0.86) 10.50 (2.35)
+ Learned selection 351 5.06 9.59 (2.26) 2.61 (0.96) 10.58 (2.36)
+ Full attention 351 5.10 8.59 (2.15) 1.95 (0.67) 8.91 (2.19)
Table 2: Language model perplexity on slices of the PILE. We evaluate Hyena and BaseConv with
Hybridization and Selective look-up at 160 and 355M parameters. We validate that the methods enable the
gated convolutions to outperform attention.
At 360M parameters, BaseConv with just 3 attention layers can outperform the Transformer, while requiring
fewer FLOPs. Our hybrid attention- BaseConv models outperform attention only models by 0.85 perplexity
points while enabling an 18% reduction in total FLOPs vs attention. However, this uses full quadratic
attention. We next show that sparse attention localized to potential AR tokens, is also sufficient to close the
gap, validating our insights on the role of input-dependence for MQAR. At 360M parameters, programmatic
selection closes 85% of the gap between pure BaseConv and attention on the AR slice, in contrast to the
random selection control. Learned selection closes 72% a of the gap using just k= 256 (sub-quadratic)
attention positions per example.
6 Discussion and Conclusion
We present an extensive analysis of gated convolution architectures in light of their recent popularity. We
identify a persisting quality gap between efficient convolution and inefficient attention based architectures,
largely due to a single failure mode associative recall. We design a new multi-query associative recall ( Mqar )
analysis tool, which correlates with downstream AR quality. We theoretically and empirically explain the
gap is due to insufficient data-dependent mixing in gated convolutions and we show minimal architectures
that close the gap on the Pile.
Our results in analyzing gated convolutions go beyond the conventional wisdom that attention is the “ right”
model. A significant amount of work focuses on improving the efficiency of attention [Dao et al., 2022, Dao,
2023, Katharopoulos et al., 2020a] and theoretically studying the exact power of attention [Hahn, 2020, Merrill
et al., 2022, Keles et al., 2023]. Attention is often used as the goalpost for what is needed downstream. We
hope our contributions highlight the value of Mqar , and more broadly tasks tied to real language modeling,
as a proxy to study.
Acknowledgments
We thank Tri Dao, Daniel Fu, Neel Guha, Stefano Massaroli, Eric Nguyen, and Michael Zhang for helpful
feedback and discussion during this work. We are grateful to Together Computer for making this work possible.
We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882
(ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity),
CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying
Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,
10

--- PAGE 11 ---
TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation,
American Family Insurance, Google Cloud, Microsoft Azure, Swiss Re, Brown Institute for Media Innovation,
Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship
(NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research
Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and
members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and
Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reflect the views, policies, or
endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government. AR’s work is
supported by NSF grant# CCF-2247014. IJ’s work is supported by an NSF Graduate Fellowship.
References
Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R´ e. Hungry Hungry
Hippos: Towards language modeling with state space models. In International Conference on Learning
Representations , 2023a. 1, 1, 2, 3.1, 3.2, 4.3, 5, 5, A.3, A.3, C.2, E.1
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and
Zettlemoyer Luke. Mega: Moving average equipped gated attention. arXiv preprint arXiv:2209.10655 ,
2022. 1, A.3, A.3
Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv
preprint arXiv:2212.10544 , 2022. 1, 2
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano
Ermon, and Christopher R´ e. Hyena hierarchy: Towards larger convolutional language models. arXiv
preprint arXiv:2302.10866 , 2023a. 1, 1, 2, 3.1, 3.2, 4.3, A.2, A.3, B, C.2, E.1, 2, H.2, 8
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional
networks. In International conference on machine learning , pages 933–941. PMLR, 2017. 1, 2
Albert Gu, Karan Goel, and Christopher R´ e. Efficiently modeling long sequences with structured state spaces.
arXiv preprint arXiv:2111.00396 , 2021. 1, 2, A.2, A.3, 3, H.2.2
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael
Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon,
and Jiaming et al. Kong. Rwkv: Reinventing rnns for the transformer era. arXiv:2305.13048 , 2023. 1, 2,
3.1, 4.3, A.3, C.2, 2, G, H.2, H.2.2, 11
Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin
Spector, Michael Poli, Atri Rudra, and Christopher R´ e. Monarch mixer: A simple sub-quadratic gemm-based
architecture, 2023b. 1, 2, A.3
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401 , 2014.
1, A.1
Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to
attend to the recent past. Advances in neural information processing systems , 29, 2016. 1, A.1, H.7.1
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.
Transformer Circuits Thread , 1, 2021. 1, A.1
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint
arXiv:2209.11895 , 2022. 1, 3.2, 4.3, A.1, E.1
Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive iir filters), 2023. 1
11

--- PAGE 12 ---
Michael Hahn. Theoretical limitations of self-attention in neural sequence models. In Transactions of the
Association for Computational Linguistics , volume 8, 2020. 1, 6
William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated transformers are constant-depth threshold
circuits. In Transactions of the Association for Computational Linguistics , volume 10, 2022. 1, 6
Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational
complexity of self-attention. In 34th International Conference on Algorithmic Learning Theory , volume
201, page 1–23, 2023. 1, 6
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace
He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of
diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020. 2, 3.1, D, D.1
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and
fine-tuned chat models. arXiv:2307.09288 , 2023. 2, 3.1, C.2, G
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. volume 30, 2017. 2, A.3, C.2
James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series.
Mathematics of computation , 19(90):297–301, 1965. 2, A.2
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM
Computing Surveys , 55(6):1–28, 2022. 2, 4
Karan Goel, Albert Gu, Chris Donahue, and Christopher R´ e. It’s raw! audio generation with state-space
models. Proceedings of the 39 th International Conference on Machine Learning, , 2022. 2
Michael Zhang, Khaled Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R´ e. Effectively modeling
time series with simple discrete state spaces. International Conference on Learning Representations , 2023.
2
Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh
Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103 , 2021. 2
Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan,
Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu
Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Th´ erien, Phil Wang, and
Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 9 2023. URL
https://www.github.com/eleutherai/gpt-neox . 3.1
Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus.
Liquid structural state-space models. arXiv preprint arXiv:2209.12951 , 2022. 3.1, A.3
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.
Retentive network: A successor to transformer for large language models, 2023. 3.1, A.3, D.1.2, 3, 6, H.2,
H.2.3, 12
Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon,
James Zou, and Christopher R´ e. Domino: Discovering systematic errors with cross-modal embeddings. In
International Conference on Learning Representations , March 2022. 3.1
David J Willshaw, O Peter Buneman, and Hugh Christopher Longuet-Higgins. Non-holographic associative
memory. Nature , 222(5197):960–962, 1969. 3.1, A.1
John J Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the national academy of sciences , 79(8):2554–2558, 1982. 3.1, A.1
12

--- PAGE 13 ---
P. B¨ urgisser, T. Lickteig, M. Clausen, and A. Shokrollahi. Algebraic Complexity Theory . Grundlehren
der mathematischen Wissenschaften. Springer Berlin Heidelberg, 1996. ISBN 9783540605829. URL
https://books.google.com/books?id=dYcgjfXsYk8C . 4.1, H.13
Tri Dao, Nimit S Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and
Christopher R´ e. Kaleidoscope: An efficient, learnable representation for all structured linear maps. arXiv
preprint arXiv:2012.14966 , 2020. 4.1, H.3, H.4, H.14, H.17, H.4, H.19
Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra,
and Christopher R´ e. Simple hardware-efficient long convolutions for sequence modeling. arXiv preprint
arXiv:2302.06646 , 2023c. 4.3, A.3, C.2, 10, 13
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip
Pham, Anirudh Ravula, Qifan Wang, Li Yang, and et al. Big bird: Transformers for longer sequences.
Proceedings of NeurIPS , 2020. 5, 4
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019a. 5
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 , 2020. 5, D.1.2, 4
Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular
activation for efficient sequence modeling. Sparse Modular Activation for Efficient Sequence Modeling , 2023.
5, A.3
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R´ e. FlashAttention: Fast and memory-
efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems , 2022.
6
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. 6
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers
with linear attention. In Proceedings of the International Conference on Machine Learning (ICML) , 2020a.
URL https://arxiv.org/abs/2006.16236 . 6
Chris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases: An informal note
on some intuitions related to mechanistic interpretability, 2022. URL https://transformer-circuits.
pub/2022/mech-interp-essay/index.html . A.1, A.3
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization
beyond overfitting on small algorithmic datasets. arXiv:2201.02177 , 2022. A.1
Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss,
Ben Egan, and Swee Kiat Lim. Thread: circuits. Distill , 5(3):e24, 2020. A.1
Dingquan Wang and Jason Eisner. The galactic dependencies treebanks: Getting more data by synthesizing
new languages. Transactions of the Association for Computational Linguistics , 4:491–505, 2016. A.1
Jennifer C White and Ryan Cotterell. Examining the inductive bias of neural language models with artificial
languages. arXiv preprint arXiv:2106.01044 , 2021. A.1
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. arXiv preprint
arXiv:2305.13673 , 2023. A.1
Shauli Ravfogel, Yoav Goldberg, and Tal Linzen. Studying the inductive biases of rnns with synthetic
variations of natural languages. arXiv preprint arXiv:1903.06400 , 2019. A.1
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning
as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021. A.1, G
Nikita Kitaev,  Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint
arXiv:2001.04451 , 2020. A.1
13

--- PAGE 14 ---
JA Feldman, GE Hinton, and JA Anderson. Parallel models of associative memory, 1981. A.1
Wei Zhang and Bowen Zhou. Learning to update auto-associative memory in recurrent neural networks for
improving sequence memorization. arXiv preprint arXiv:1709.06493 , 2017. A.1
Stefano Massaroli, Michael Poli, Daniel Y Fu, Hermann Kumbong, David Romero, Rom Parnichukun, Aman
Timalsina, Quinn McIntyre, Beidi Chen, Atri Rudra, Ce Zhang, Christopher R´ e, Stefano Ermon, and
Yoshua Bengio. Laughing hyena distillery: Extracting compact recurrences from convolutions. 2023. A.2
David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Ckconv:
Continuous kernel convolution for sequential data. 2022. A.3
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state
spaces, 2022. A.3
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R´ e. On the parameterization and initialization of
diagonal state space models, 2022. A.3
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via
gated state spaces, 2022. A.3
Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence
modeling, 2023. A.3
Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel,
Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, and Chris R´ e.
Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, 2023. A.3
Transformers revolutionized ai. what will replace them?, 2023. URL https://www.forbes.com/
sites/robtoews/2023/09/03/transformers-revolutionized-ai-what-will-replace-them/?sh=
6ed698269c1f . A.3
Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective
structured state-spaces for long-form video understanding. In CVPR , 2023. A.3
Brandon Yang, Gabriel Bender, Quoc V. Le, and Jiquan Ngiam. Condconv: Conditionally parametrized
convolutions for efficient inference. In 33rd Conference on Neural Information Processing Systems (NeurIPS
2019) , 2020. A.3
Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. Time-parameterized convolutional neural
networks for irregularly sampled time series. 2023. A.3
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran¸ cois Fleuret. Transformers are rnns: Fast
autoregressive transformers with linear attention, 2020b. A.3
Together. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https:
//github.com/togethercomputer/RedPajama-Data . D, D.2, 4
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in
transformers. Association of Computational Linguistics , 2019. D.1.2
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020. 1
Anonymous. Mamba: Linear-time sequence modeling with selective state spaces. 2023a. URL https:
//openreview.net/forum?id=AL1fq05o7H . 3, F.1
Anonymous. Gateloop: Fully data-controlled linear recurrence for sequence modeling. 2023b. URL
https://openreview.net/pdf?id=02Ug9N8DCI . 3, F.1
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019b. 4
14

--- PAGE 15 ---
Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention
for long document understanding. arXiv preprint arXiv:1911.02972 , 2019. 4
Michael T Heideman and C Sidney Burrus. Multiplicative complexity, convolution, and the DFT . Springer,
1988. H.1.1
Ilya Volkovich. A guide to learning arithmetic circuits. In Conference on Learning Theory , pages 1540–1561.
PMLR, 2016. H.1.1
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano
Ermon, and Christopher R´ e. Hyena hierarchy: Towards larger convolutional language models. Proceedings
of the 40th International Conference on Machine Learning , 2023b. H.2.1
Thathachar S Jayram, Ravi Kumar, and Dandapani Sivakumar. The one-way communication complexity of
hamming distance. Theory of Computing , 4(1):129–135, 2008. H.23
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input
length extrapolation. arXiv preprint arXiv:2108.12409 , 2021. H.7.2
Selim G Akl and Henk Meijer. Parallel binary search. IEEE Transactions on Parallel & Distributed Systems ,
1(02):247–250, 1990. H.7.4, H.7.4
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to algorithms .
MIT press, 2022. H.32
Mikl´ os Ajtai, J´ anos Koml´ os, and Endre Szemer´ edi. An 0 (n log n) sorting network. In Proceedings of the
fifteenth annual ACM symposium on Theory of computing , pages 1–9, 1983. H.35
Chris Chatfield. The analysis of time series: An introduction, fifth edition. 1995. H.8.1
15

--- PAGE 16 ---
Appendix
The appendix includes the following content:
1. Appendix A provides an extended discussion of related work and concepts.
2. Appendix B provides a code implementation of the BaseConv architecture.
3. Appendix C gives details for the experiments, including model architectures and hyperparameters.
4.Appendix D provides additional analysis of how Mqar appears in real data across a variety of language
distributions.
5.Appendix E provides a formal definition of the Mqar problem, synthetic construction procedure, and
experimental details.
6. Appendix F provides additional synthetic experiments and analysis.
7.Appendix G provides experiments and analysis for how the Mqar gap changes as we scale the gated
convolution and attention architectures.s
8. Appendix H gives proofs and additional discussion for the theoretical analysis in our work.
A Extended Related Work
We are inspired by and build on prior work in mechanistic interpretability (Appendix A.1), efficient architecture
design (Appendix A.2), and input-dependent architectures (Appendix A.3).
A.1 Mechanistic Interpretability, Synthetic Languages, and Associative Recall
Work in mechanistic interpretability aims to decompose the capabilities of a neural network into human-
understandable algorithms that can be attributed to specific parameters in the model [Olah, 2022, Power
et al., 2022, Elhage et al., 2021, Cammarata et al., 2020]. Some of these works, use synthetic data to validate
mechanistic interpretations of neural networks [Olsson et al., 2022]. This relates to a broader line of work
using synthetic languages to study language model architectures [Wang and Eisner, 2016, White and Cotterell,
2021, Allen-Zhu and Li, 2023, Ravfogel et al., 2019, Xie et al., 2021]. Mechanistic design puts mechanistic
interpretations to use in designing new architectures and learning algorithms. Several works in architecture
research have used synthetic tasks to validate architecture designs [Kitaev et al., 2020].
Our work is focused on one particular synthetic task: associative recall . Motivated by psychological models
of how humans associate and retrieve information, work in the early days of neural network research focused
on developing systems capable of associative recall [Willshaw et al., 1969, Feldman et al., 1981, Hopfield,
1982]. For example, Hopfield networks, proposed in 1982, are a recurrent neural network explicitly designed
to support associative (“content-addressable”) memory [Hopfield, 1982]. More recently, several notable
recurrent neural network mechanisms ( e.g.neural turing machines, LSTMs, and RNNs with fast weights)
were evaluated on a synthetic version of associative recall [Graves et al., 2014, Ba et al., 2016, Zhang and
Zhou, 2017, inter alia.]. These works use a formulation of associative recall very similar to the single-query
associative recall in our work. Since the rise of large language models, several works have argued that
LLMs ability to perform in-context learning is due, at least in part, to the associative recall capabilities of
attention [Elhage et al., 2021, Olsson et al., 2022].
A.2 Efficient Language Modeling Architectures
We first briefly review the efficiency motivations for the recent excitement around gated convolution architec-
tures. While attention requires compute that scales as O(N2) in sequence length N, convolutions scale as
O(NlogN) [Cooley and Tukey, 1965]. Ideal, inference complexity is O(1) in sequence length, as provided by
recurrent neural networks. State-space models can be computed either as a convolution or recurrence, to
achieve both sub-quadratic training and constant inference complexity in sequence length [Gu et al., 2021].
16

--- PAGE 17 ---
Architectures that use implicit convolutional filters [Poli et al., 2023a], can be converted to an SSM via a
simple distillation step [Massaroli et al., 2023].
A.3 Input-Dependence in Sequence Modeling Architectures
In an input-dependent sequence model, the way tokens are aggregated across the sequence is controlled by
the data, not just the model parameters. We highlight several prior works related to our study that explore
input-dependent sequence models.
•Attention [Vaswani et al., 2017] achieves input-dependent sequence mixng since the Q,K, and V
terms are achieved via linear combinations of the input x. For instance, Q=xWQfor some learned
weight matrix WQ.
•Input-Independent Convolution Architectures . Given the quadratic scaling of attention, Gu
et al. [2021] propose S4, an architecture that use long convolutions to process the input. CKConv
Romero et al. [2022] uses a convolution filter that is implicitly parametrized by an MLP of the relative
positions of the observations in a time-series. A line of subsequent work improved upon S4 by changing
the parametrization [Gupta et al., 2022, Gu et al., 2022, Mehta et al., 2022, Ma et al., 2022, Fu et al.,
2023c] or changing the SISO convolutions to MIMO convolutions [Smith et al., 2023]. The convolution
filter in each of these architectures is input-independent. The linear convolution layer alone cannot
perform MQAR [Fu et al., 2023a].
•Input-Dependence via Gating . Since pure convolution architectures with input-independent filters
struggle to perform associative recall, an important task in in-context learning [Olah, 2022], subsequent
work proposed to introduce input-dependence by adding a gating or element-wise multiplication
operation to the architecture, where both the inputs to the operator are defined in terms of the input
[Fu et al., 2023a, Poli et al., 2023a]. The multiplication is generally y=σ(Wx)⊙x. The filters
remain input-independent. These architectures demonstrate promising results on the associative recall
synthetics proposed in prior work Fu et al. [2023a], Poli et al. [2023a] and provide large downstream
improvements over S4.
Prior work suggests the models match attention in quality [Poli et al., 2023a, Fu et al., 2023b, Peng
et al., 2023] and this has led to increasing use of these architectures [Nguyen et al., 2023, toe, 2023],
however our results suggest there is still a sizable gap to attention. We show the gap is largely due to
the models’ recall abilities and capture this behavior in a novel synthetic task. We theoretically show
that dimension scales in sequence length when solving MQAR with gated convolutions.
Selective S4 Wang et al. [2023] learns which positions to mask prior to passing the input to the next
long convolution (S4) layer. This resembles gating (which also controls which information flows forwards
to the next layer).
•Input-Dependent Convolution Architectures Prior work introduces convolution architectures
with input-dependent filters [Yang et al., 2020, Kosma et al., 2023]. For instance, Yang et al. [2020]
parametrizes the convolution filter as a linear combination of nprojections of the input. Liquid S4
Hasani et al. [2022] is also motivated by introducing input-dependence to S4. To accomplish this, the
work proposes including correlation terms between the input tokens during state mixing. We evaluate
Liquid S4 and find the architecture lags attention on the Pile by 4.4 perplexity points in Table 3.
Overall Slices % of gap due to
Model Param (M) AR Hits Other Tokens
Attention 125 12.37 (2.52) 2.32 (0.84) 14.04 (2.64) —
Liquid S4 145 16.80 (2.82) 22.75 (3.12) 16.42 (2.80) 52.6%
Table 3: Validation perplexity on the Pile after pretraining on 5B tokens. We report log perplexity with
negative log-likelihood in parentheses. AR vs. non-AR token slices are defined in Section 3.
17

--- PAGE 18 ---
•Recurrent Architectures The pure long-convolution architectures can also be computed as recurrences.
Letting sirepresent the hidden state at time i,uibe the input at time i, and A, B, C be projection
matrices, the recurrence computes:
si+1=f(Asi+Bui)
yi=Csi
In a linear RNN like S4, the A,B, and Cmatrices are input-independent and fis an identify function.
However, recent work proposes input-dependent RNNs (e.g. RetNet Sun et al. [2023]), where a subset
of these matrices is input-dependent (e.g. A=xWa). Note that linear attention mechanisms can
also be expressed as input-dependent RNNs [Katharopoulos et al., 2020b]. While these architectures
improve over gated convolutions in Mqar synthetic experiments and downstream quality on the Pile
(Appendix F), we observe and prove that the required RNN hidden state dimensionality grows with the
number of key-value pairs that the model needs to recall in the sequence (Appendix H.6). In contrast,
attention complexity does not scale with the number of key-value pairs to recall.
•Hybrid Architectures Finally, we can combine architectural components that provide different
capabilities. Prior work proposes architectures that hybridize sub-quadratic layers and attention [Fu
et al., 2023a, Ma et al., 2022, Ren et al., 2023], but does motivate this choice from a mechanistic design
perspective. Further, H3 without attention Fu et al. [2023a] and MEGA [Ma et al., 2022], which uses
blocked attention, underperform attention on language modeling.
SeqBoat [Ren et al., 2023] shares closest resemblance to our learned selection module. SeqBoat introduces
a different module that learns where to use attention however, their model can end up using full attention
at a layer, providing quadratic scaling in the worst case. Different from SeqBoat, we use an auxiliary
loss to encourage sparsity in the selected attention positions and select the top- kpositions to remain
sub-quadratic.
Overall, our work finds that previously proposed sub-quadratic models do not efficiently solve the Mqar
task.
B Code Listing
In this section, we include code listings for the BaseConv operator. We begin with the a standard
BaseConv implementation that uses an explicitly-parameterized long convolution. Below, we also provide
the implementation for BaseConv with an implicitly-parameterized long convolution.
1import torch
2
3def fft_conv (u: torch .Tensor , k: torch . Tensor ):
4 """
5 Args :
6 u ( torch . Tensor ): ( batch_size , d_model , seq_len )
7 k ( torch . Tensor ): ( d_model , l_max )
8 Return :
9 y ( torch . Tensor ): ( batch_size , d_model , seq_len )
10 """
11 seqlen = u. shape [ -1]
12 fft_size = 2 * seqlen
13 k_f = torch . fft. rfft (k, n= fft_size ) / fft_size
14 u_f = torch . fft. rfft (u.to( dtype =k. dtype ), n= fft_size )
15 y = torch . fft. irfft (u_f * k_f , n= fft_size , norm =" forward ")[... , : seqlen ]
16 return y
17
18class BaseConv ( torch .nn. Module ):
19
20 def __init__ (self , d_model : int , l_max : int , ** kwargs ):
21 super (). __init__ ()
22 self . d_model , l_max = d_model , l_max ,
23 self . projection = torch .nn. Linear ( self . d_model , self . d_model )
24 self . filter = torch .nn. Parameter ( torch . randn ( self . d_model , l_max ), requires_grad =
True )
18

--- PAGE 19 ---
25
26 def forward (self , u: torch . Tensor ):
27 """
28 Args :
29 u ( torch . Tensor ): ( batch_size , d_model , seq_len )
30 Return :
31 y ( torch . Tensor ): ( batch_size , d_model , seq_len )
32 """
33 u_conv = fft_conv (u. transpose (1, 2) , self . filter ). transpose (1, 2)
34 u_proj = self . projection (u)
35 y = u_conv * u_proj
36 return y + u
Listing 1: Explicit BaseConv implementation. Implementation of the BaseConv layer with explicitly-
parameterized long convolutions. The implemtnation is 19 lines excluding comments and whitespace.
In some of our experiments, we interleave BaseConv layers that use explicit short convolution filters (like
those in torch.nn.Conv1d ) and with BaseConv layers that use implicit long convolution filters (like those
described in Poli et al. [2023a]).
Below we include the code for BaseConv with an implicit convolution.
1class PositionalEmbedding (nn. Module ):
2 def __init__ (self , emb_dim : int , seq_len : int , ** kwargs ):
3 """ Complex exponential positional embeddings for implicit long convolution filters .
"""
4 super (). __init__ ()
5 t = torch . linspace (0, 1, seq_len )[None , :, None ] # 1, L, 1
6 bands = ( emb_dim - 1) // 2
7 t_rescaled = torch . linspace (0, seq_len - 1, seq_len )[None , :, None ]
8 w = 2 * math .pi * t_rescaled / seq_len # 1, L, 1
9 f = torch . linspace (1e -4, bands - 1, bands )[None , None ]
10 z = torch .exp (-1j * f * w)
11 z = torch .cat ([t, z.real , z. imag ], dim = -1)
12 self .z = nn. Parameter (z, requires_grad = False )
13
14 def forward (self , L):
15 return self .z[:, :L]
16
17class BaseImplicitConv (nn. Module ):
18 """
19 BaseConv with implicit filter parameterized by an MLP .
20
21 Args :
22 d_model ( int ): Number of expected features in input and output .
23 l_max ( int ): The maximum sequence length .
24 d_emb (int , optional ): Dimension of the positional embeddings . Must be odd and $\
geq$to 3 (time , sine and cosine ). Defaults to 3.
25 d_hidden (int , optional ): The number of features in the hidden layer of the MLP .
Defaults to 16.
26 """
27
28 def __init__ (self , d_model : int , l_max : int , d_emb : int =3, d_hidden : int = 16 ,):
29 """
30 Long convolution with implicit filter parameterized by an MLP .
31 """
32 super (). __init__ ()
33 self . pos_emb = PositionalEmbedding (d_emb , l_max )
34 self . filter_mlp = nn. Sequential (nn. Linear (d_emb , d_hidden ), torch .nn. ReLU () , nn.
Linear ( d_hidden , d_model ))
35 self . projection = torch .nn. Linear ( d_model , d_model )
36
37
38 def forward (self , u: torch . Tensor , *args , ** kwargs ):
39 """
40 Args :
41 u ( torch . Tensor ): ( batch_size , seq_len , d_model )
42 Return :
19

--- PAGE 20 ---
43 y ( torch . Tensor ): ( batch_size , seq_len , d_model )
44 """
45 filter = self . filter_mlp ( self . pos_emb (u. shape [1]) ). transpose (1, 2)
46 u_conv = fft_conv (u. transpose (1, 2) , filter ). transpose (1, 2).to( dtype =u. dtype )
47 u_proj = self . projection (u)
48 y = u_conv * u_proj
49 return y + u
Listing 2: Implicit BaseConv implementation. Implementation of the BaseConv layer with implicitly-
parameterized long convolutions. (The implementation is 34 lines excluding comments and whitespace).
C Downstream Experimental Details
We use A100 80GB Nvidia GPUs to run all experiments. We use the reference training infrastructure from
https://github.com/EleutherAI/gpt-neox for all pretraining runs. The Pile data is tokenized using the
GPT2BPETokenizer and all models see the data in the same order.
Below we provide details on the hyperparameters and settings for training each architecture studied in the
paper on the real-world Pile data. In Appendix C.1 we summarize and justify our method for measuring the
quality gap due to associative recall between the convolution architectures and attention. In Appendix C.2,
we provide details on the pure gated convolution architectures, attention baseline, and hybrid architectures
studied in the paper.
C.1 Measuring the Mqar Gap on Real Language Data
Here we summarize our method for computing the amount of quality gap between the convolution and
attention models that is ascribed to associative recall capability (e.g., in Table 1):
1.Given an input sequence, we identify recurring bigrams (i.e. bigrams that have already appeared in the
sequence at a prior position). Since bigrams that appear frequently during training may be memorized
by the model, rather than requiring the model to perform recall at inference-time, we only measure AR
log-probabilities with respect to bigrams that are seen fewer than a threshold number of times during
training. The threshold used in all the experiments in our submission is 1 ,250 training occurrences in
the 10B tokens of pretraining data.
2.We measure the log-probability assigned to the true bigram completion. This bigram completion is
referred to as an AR Hit in our work. This protocol assumes that the model can produce the completion
by recalling the prior occurrence of the bigram in the sequence.
3.For the model being evaluated m, and the attention model M, we measure the % of the quality gap
between mandMascribed to associative recall capability as follows. Let the average log-probability
for all AR Hits across validation sequences be lm
HandlM
HformandMrespectively. Let the average
log-probabilities of all tokens in the validation sequences be lmandlMrespectively. Let pHbe the
proportion of AR Hit tokens in the validation data. As the final gap ascribed to AR, we report:
min((lm
H−lM
H)pH
lm−lM,1.0)
Shown above, if mis better than attention ( M) overall and Mis better than mat AR, we ascribe
100% of the gap to AR.
We briefly discuss two important decisions in this protocol. First, we only measure explicit bigrams , i.e.
bigrams are identified based on token ids in the sequence. However, intuitively, models may also perform
associative recall between related concepts produced by a contextual language model. For instance, language
may contain bigrams in which one word is swapped by a synonym. As another example, a model may see
a sentence such as “The iPhone is outside my budget so I instead purchased an Android phone. ... It was
much ” and predict “cheaper” for the blank, recalling that the sequence is discussing cost. Our work does
not measure such fuzzy (more abstract) recall instances.
20

--- PAGE 21 ---
Next, we measure the gap based on log-probabilities rather than perplexity. This is simply because we
want to make our metrics independent of the number of tokens in each of the slices of the validation set.
Approximately 6.4% of validation tokens are AR Hits with the threshold set to consider bigrams seen less
than 1 ,250×during training.
C.2 Gated Convolution Downstream Architectures
We evaluate over 4 previously proposed architectures as well as BaseConv , the theoretically “canonical”
representation for gated convolutions, introduced in Section 4, for a total of 14 training runs. Here we provide
details on the hyperaparamters and configurations used for training each architecture. We also provide details
on the FLOPs computation.
•Attention [Vaswani et al., 2017, Touvron et al., 2023] We train using the the specifications in
Table 6. The parameters are sourced from the Transformer implementation in https://github.com/
EleutherAI/gpt-neox .
•Hyena [Poli et al., 2023a] We train using the specifications in Table 8. The parameters are sourced
from the Appendix of Poli et al. [2023a] and the implementation is sourced from the provided reference
athttps://github.com/HazyResearch/safari .
•H3 [Fu et al., 2023a] We train using the specifications in Table 10. The hyperparameters and
implementation use the reference at https://github.com/HazyResearch/H3 .
•RWKV [Peng et al., 2023] We train using the specifications in Table 11. The parameters are sourced
from the Appendix of Peng et al. [2023] and the details provided in the reference implementation at
https://github.com/BlinkDL/RWKV-LM . We specifically evaluate RWKV-V4.
•Pure long convolution We train using the specifications in Table 13. We evaluate a simple long
convolution based model with no gating as a reference point. While this is a generic architecture, we
use the reference implementation and initialziations/regularizations from recent work Fu et al. [2023c].
The implementation is provided at https://github.com/HazyResearch/safari .
•BaseConv We train using the specifications in Table 15. The implementation, amounting to 19 lines
of PyTorch, is shown in Appendix B.
We provide the equations used to compute the FLOPs for each model, letting Dbe the model width, Hthe
head dimension, Lthe depth, Nthe sequence length, Vthe vocabulary size, and Bthe batch size. FLOPs
equations for different architecture types are provided in Table 7 (attention), Table 9 (Hyena and adapted for
H3), Table 14 (pure long convolution), and Table 16 ( BaseConv ). We compute FLOPs for RWKV as in the
Appendix of [Peng et al., 2023], based on the number of linear layer parameters, plus input and language
modeling head FLOPs.
Hybrid Architectures In the main paper and appendix, we evaluate a series of architectures with a hybrid
of gated convolution and non gated convolution layers. For these architectures, we use the same number of
layers as the pure gated convolution architecture (as reported in Appendix C.2). The hybridized (non gated
convolution) layers are inserted as replacements of the gated convolution layer. For each architecture, we
simply evenly intersperse the two types of layers. We evaluate with the following replacement layers in this
work:
•Full attention following the specification of attention in Appendix C.2.
•Random selection We evaluate sparse attention randomly applied to tokens as introduced in Section 5.
•Programmatic selection We apply sparse attention on AR hit tokens as introduced in Section 5.
When processing the input sequence, we can causally determine if a bigram of raw token ids is repeated
in the sequence to construct the attention pattern.
•Learned selection Welearn the positions on which to use attention by introducing a simple linear
layer with sigmoid activation to the architecture that is trained to output high scores if attention should
21

--- PAGE 22 ---
be applied to the token. We can take the top- kpositions to control the computational complexity of
the layer. This approach is introduced in Section 5.
We use these protocols to validate that input-dependence suffices to address the associative recall gap and
highlight that there are varied approaches to incorporating input-dependence in an architecture.
D Extended Downstream Analysis of Mqar
In this section, we provide additional analysis of how Mqar manifests in real language data. We extend
our discussion of associative recall on the Pile Gao et al. [2020]. We also perform analysis on sources in
RedPajama including ArXiv and StackOverflow Together [2023].
D.1 Additional Discussion of Mqar in the Pile
The Pile is a widely popular language modeling corpus Gao et al. [2020].
1.First, in Appendix D.1.1, we provide several real examples of howassociative recall occurs in the Pile
to demonstrate it’s role in language modeling. We color code the tokens to highlight differences in the
next token predictions from Attention, RWKV and Hyena models.
2.Next, in Appendix D.1.2, we explain where associative recall hits tend to occur in sequences. This is
useful to guide the design of sequence mixers — if associative recall tends to occur in specific places,
the input-dependent sequence mixer may not need to compute all N2token-to-token interactions for
every sequence.
D.1.1 Pile Examples
For the examples below, the legend is: tokens are colored as both correct, both incorrect, Attention correct
and Hyena incorrect, Attention incorrect and Hyena correct. For tokens where the models disagree, the
predictions of each model are provided in parentheses.
while lunch -ing at the Ma -ison Ber -gey b -ist -ro near his apartment : he had been mus
-ing about (rwkv= about, attn= on) the ... (723 tokens) ... the young waitress -’s sigh at
the Ma -ison Ber (rwkv= Bl, attn= Ber) -gey(rwkv=-nd, attn=-gey)
Example D.1: Comparing next token predictions of a 350M parameter Attention model and
a 350M parameter RWKV model. In this example, the models need to perform associative recall to
correctly predict “Bergey” in the 4-gram Ma-ison Ber-gey. The previous mention of the 4-gram was more
than 700 tokens earlier in the passage.
The second (rwkv= first, attn= second) section is all about Pixar Fest , and the
(rwkv= third, attn= the) final section is all (rwkv= about, attn= all) about Pixar Pier
. ...(480 tokens) ... -If (rwkv=-Disney, attn=-If) there wasn -ˆ a G ¸-t enough Pixar at
Disneyland , Pixar Fest (rwkv= would, attn= Fest) is(rwkv= at, attn= is) coming to the
Disneyland Resort on April 13 , 2018 .
Example D.2: Comparing next token predictions of a 350M parameter Attention model and
a 350M parameter RWKV model. In this example, the models need to perform associative recall to
correctly predict “Fest” in the bigram Pixar Fest.
22

--- PAGE 23 ---
David Mus -a P -id -cock is an indigenous English revert to (rwkv=-ant, attn= to) Islam
,(rwkv=,, attn= who) who formed the Islamic Party of Britain in 1989 , at London Central
Mosque (rwkv= Mosque, attn= University) .(rwkv=., attn= in) ...(711 tokens )... -I have
just found out that David Mus (rwkv= Cameron, attn= Mus) -a P (rwkv= (, attn= P)
-id(rwkv=-asha, attn=-id) -cock (rwkv=-ham, attn=-cock) is(rwkv= (, attn= is) speaking
at a Yorkshire Forum debate today in Bradford with
Example D.3: Comparing next token predictions of a 350M parameter Attention model and
a 350M parameter RWKV model. In this example, the models need to perform associative recall to
correctly predict a middle and last name. Note that the middle and last names span several tokens.
-The most common map elements (hyena= in, attn= elements) in Sub -Space are prizes ,
or ” -g (hyena=-points, attn=-g) -reens ” (hyena=-",, attn=") ( -for their green color -).
Pri -zes allow players to upgrade their ships and gain (hyena= other, attn= gain) special
weapons or abilities (hyena= abilities, attn= upgrades) . While prizes are generally
plent -ifully scattered throughout the map (hyena= map, attn= game) , the upgrades or
abilities (hyena= bonuses, attn= abilities) they award are randomly selected by the
zone . -Energy -Rather than dealing with ammunition counts and hit points separately ,
Sub -Space combines both of these elements into a single unit of measure : energy . Each
ship is equipped with a certain amount of energy , from which (hyena= which, attn= the)
it must draw its health as well as its weapons power . ... (1,526 tokens) ... Speed Zone
proved to be less popular than the Jack -pot -/ -Running , Chaos , or ” -flag ” zone games
and support was discontinued shortly after Sub -Space (hyena=-space, attn=-Space) went
to retail .
Example D.4: Comparing next token predictions of a 350M parameter Attention model and
a 355M parameter Hyena model. In this example, the models need to perform associative recall to
correctly predict the name of the game “SubSpace”. Note that both models correctly perform the recall when
there is a short gap between tokens, but only Attention correctly performs recall when the gap is greater
than 1,000 tokens.
Thus far , no systematic study has been published on is -olation (hyena=-olation, attn=-ot)
via sequential centrif -ug -ation , and no systematic analysis is ... (491 tokens) ... Fig .
1(hyena= 1, attn=- ^A␣l) -is -olation via sequential (hyena= centrif, attn= sequential)
centrif -ug -ation .
Example D.5: Comparing next token predictions of a 350M parameter Attention model and
a 355M parameter Hyena model. In this example, the models need to perform associative recall to
correctly predict the technique “sequential centrifugation”.
Miss -ouri (hyena=-ouri, attn=-iss) Southern has had 14 Major League Base-
ball draft selections since the draft began in 1965 . ... (149 tokens) ...
Fred G . Hughes Stadium (hyena= Stadium, attn= Field) ( -opened in 1975
) is named (hyena= the, attn= named) after former (hyena= the, attn= former)
J -op -lin Globe publisher and (hyena= and, attn= Fred) Missouri South-
ern(hyena= State, attn= Southern) board of reg -ents member
Example D.6: Comparing next token predictions of a 350M parameter Attention model and
a 355M parameter Hyena model. In this example, the models need to perform associative recall to
correctly predict the name of the university “Missouri Southern”.
23

--- PAGE 24 ---
D.1.2 Distribution of Mqar Hit Positions in Sequences
In Figure 3, we compute and plot the distances between AR hits and its prior bigram (key-value) occurrence
in the sequence across the Pile training data. The distances follow a power law distribution where most AR
hits are within 100 token positions from the prior bigram occurrence, and a long tail of AR hits requires
long-range interactions.
Implications This suggests that architectures which compute token-to-token interactions within windows,
where the window size is less than the full sequence length, may suffice to handle Mqar in real data. For
instance, sliding window attention may help with AR [Beltagy et al., 2020, Sun et al., 2023, inter alia.]. Prior
work leverages the observation that local attention captures most of the token dependencies to improve
efficiency Sukhbaatar et al. [2019].
Figure 3: Across the Pile training data, we measure the distances between n-grams and their prior occurrences
in the provided context. We plot the frequency across distances, finding it follows a power law distribution.
D.2 Analyzing Mqar across Varied Language Distributions
We use the HuggingFace sample of the popular RedPajama language modeling corpus Together [2023] to
understand how the prevalence of Mqar hits and and ways in which the hits appear vary across language
distributions. We analyze text sequences from the following sources: ArXiv papers, Books, C4, Common
Crawl, GitHub, Stack Exchange, and Wikipedia.
First we profile the prevalence of Mqar hits by computing the number of repeated bigram occurrences per
sequence. Each sequence is 2 ,048 tokens in length and exclude bigrams containing NLTK stop words or
punctuation from the computation. The prevalence of hits and the distances between hit tokens and the
prior bigram occurrence in context vary across distributions as shown in Figure 4 and 5.
Arxiv, Github, and Stack Exchange contain relatively structured or richly formatted langauge data. We
observe that the prevalence of Mqar hits is relatively high in these three sources in Figure 4. We inspect
why these documents contain many bigrams. In Arxiv, we find domain-specific terminology, groups of related
citations, Latex commands, and mathematical equations are frequently repeated within the same document.
In GitHub and Stack Exchange, we find function and variable names are frequently reused. Meanwhile, the
C4, CC, Books, and Wikipedia bigrams are highly variable. The topics and documents in these corpora are
relatively diverse and unstructured.
24

--- PAGE 25 ---
Figure 4: Mqar hits by sub-source of the RedPajama language corpus Together [2023]. Hits are measured
using the repeated bigrams in each sequence of length .
Figure 5: We measure the distance between n-grams and their prior occurrences in the provided context
across sequences from each source. We plot the frequency across distances, finding it follows a power law
distribution.
25

--- PAGE 26 ---
If the placement Vof the hyperplanes is not generic, then the induced subdivision Σ is not a trian-
gulation. However, the above bijection is unaffected and, in particular, the fine type of a vertex of
CDAcan be read off the corresponding facet of Σ. We define the crosscut complex ccutΣ⊆2[n]×[d]
to be the unique simplicial complex with the same vertices-in-facets incidences as the polyhedral
complex Σ. The crosscut complex is a standard notion in combinatorial topology and can be
defined in more generality (see Bj¨ orner). The following observation is immediate from the definitions.
Proposition D.1. ForVan ordered sequence of npoints in Td−1letA=A(V)be the
corresponding tropical arrangement and let Σbe the regular subdivision of ∆n−1×∆d−1induced by
V. Then the fine cotype ideal fcIis Alexander dual to the Stanley-Reisner ideal of ccutΣ.
The crosscut complex encodes the information of which collections of vertices lie in a common face.
Hence, the crosscut complex is a purely combinatorial object and does not see the affine structure
of the underlying polyhedral complex.
Example D.7: ArXiv sequence where the initial bigram occurrence is highlighted in red and the repeated
bigram with the Mqar hit is highlighted in blue.
1/**
2 * Adds a directed edge to the graph from pt1 to pt2 . Precondition : Both
3 * GeographicPoints have already been added to the graph
4 *
5 * @param from The starting point of the edge
6 * @param to The ending point of the edge
7 * @param roadName The name of the road
8 * @param roadType The type of the road
9 * @param length The length of the road , in km
10 * @throws IllegalArgumentException If the points have not already been
11 * added as nodes to the graph , if any of the arguments is null , or if the
12 * length is less than 0.
13 */
14 public void addEdge ( GeographicPoint from , GeographicPoint to ,
15 String roadName , String roadType , double length )
16 throws IllegalArgumentException {
17
18 if ( from == null || to == null || roadName == null
19 || roadType == null || length < 0 || ! mapNodes . containsKey ( from )
20 || ! mapNodes . containsKey (to)) {
21 throw new IllegalArgumentException ();
22 }
23
24 if (! mapNodes . get ( from ). hasEdge (to)) {
25 mapNodes . get( from ). addNeighbor (to , roadName , roadType , length );
26 numEdges ++;
27 }
28 }
Example D.8: GitHub sequence where substrings such as “IllegalArgumentException”, “roadType” (and
other variable names), “GeographicPoint” (and other data types), “containsKey” (and other function calls)
are repeated throughout.
E Synthetic Mqar Experimental Details
In this paper, we propose Mqar as a useful tool to help explain gaps between three popular sequence modeling
layers — Transformers or Sparse Transformers, Convolutions, and Recurrences. In this section, we detail
the motivation behind the design of Mqar and include extended experiments on additional architectures.
We provide a procedure for generating Mqar synthetic data, which can help in the development of new
architectures.
26

--- PAGE 27 ---
E.1 Mqar Generation Procedure
Here we provide additional discussion on the properties of our Mqar synthetic data. The objective of the
synthetic analysis is to help explain the differences in language modeling behavior between different classes of
language modeling architectures, as observed on real-world data Section 3. Synthetic recall tasks were used
in the development of Hyena Poli et al. [2023a] and H3 Fu et al. [2023a], and Olsson et al. [2022] to study
Transformer in-context learning behavior.
1.Attention : Attention can perform recall trivially (Proposition 4.3). The bound is independent of the
sequence length N.
2.Convolutions : The gated convolution models use input-independent filters. We show in Section 4 that
the dimensionality for solving recall grows with the input sequence length. Real language modeling can
require performing O(N) recalls in one forward pass. Our intuition is that it is difficult to efficiently
compute all token-interaction distances with such a filter.
3.Recurrences : Recurrent models include a single hidden state as an information bottleneck. We show
in Appendix H.6 that the gated recurrence requires Ω( N) bits to solve Mqar ford≤√
Nfor model
dimension d. Our intuition is that it is difficult to store large numbers of key-value pairs seen in
the sequence in low-dimensional hidden states.
Motivated by this analysis, we propose Procedure 1 for generating synthetic Mqar data. This procedure
allows toggling two simple properties of the synthetic dataset, specified below, to reveal the differences
between the sequence modeling architecture families. We also discuss deviations from the prior work that
uses synthetic recall data.
1.Size of the Key-Value Map : The number of unique key-value pairs appearing in each example in
the dataset. In Procedure 1, we use the input parameter Dto toggle this value.
2.Number of Gaps in the Data : The number of unique token-interaction distances required to perform
the recall task. Prior work assumes there is a single query token that requires recall per example, failing
to test whether the architecture can support multiple token-interaction distances. In Procedure 1, we
use the parameters N(sequence length), D(size of the key-value map per example), and αto toggle
this property. We enforce that the token-interaction distances appearing in the synthetic examples
follow a power-law distribution specified by α(based on Figure 3). Keeping αandDconstant while
varying Nchanges the number of unique token-interaction distances appearing in the example.
Finally, noting that the vocabulary size appears in the theoretical bounds for all three architecture families
(Section 4), we increase the vocabulary size to be much larger than the model dimension as typically seen in
language modeling (30k - 50k tokens). Prior work uses vocab sizes ≤40 tokens.
Algorithm 1 Mqar Synthetic Procedure
Input: Vocabulary C, Sequence length N, Power-law parameter α, Number of Key-Value Pairs D
Output: Synthetic sequence
1:Let the first half of Cbe keys Kand the second half be values V.
2:Pair each key token k∈Kwith a random value token v∈V.
3:Sub-select Drandom key-value pairs to include in the sequence.
4:Place the Dkey-value pairs at the start of the sequence (i.e. consuming the first 2 Dpositions).
5:Place a second occurrence of each d∈Dat a distance from the first occurrence in the sequence. The
distance for each d∈Dis selected at random from the positions [2 D..N ], where the probability of
choosing each position follows the power law distribution specified by α.
6:Output the synthetic sequence.
E.2 Training Details
We first describe the architectures evaluated on Mqar synthetic data and then the training hyperparameters.
We evaluate the following four architecture categories in the experiments:
1.Attention Standard GPT-2 style multi-headed Transformer architecture with learned positional
embeddings Brown et al. [2020]. Attention, the core building block of Transformers, is defined in
Section 2. The architecture for synthetics has 1 attention head.
2.Gated convolutions (Hyena Poli et al. [2023a], RWKV Peng et al. [2023]). Gating, convolutions, and
27

--- PAGE 28 ---
the class of gated convolutions are defined in Section 2.
3.Gated recurrences (RetNet Sun et al. [2023]). RetNet proposes computing attention over chunks of
the sequence and combining this with a recurrence over the sequence. We evaluate this architecture
(“Chunked RNN”) with chunk sizes of 32 and 8 in Figure 6.
In RetNet, hidden states are updated as:
Sn=γSn−1+AT
nVn
Outputs at each timestep are defined as
On=CnSn
where γ∈R1,A=xWafor input x∈N×and learned weight matrix Wa∈Rd×d,C=xWcfor
Wc∈Rd×d,V=xWvforWv∈Rd×d.
RetNet similar to a state-space-model (SSM) Gu et al. [2021], except for that the matrices AandCare
input-dependent (i.e. they are functions of the input x). We note that RetNet is a special case of the
recently proposed Mamba Anonymous [2023a] and GateLoop Anonymous [2023b] architectures, which
replace the γterm in RetNet with yet another input-dependent matrix.
The RetNet model is formally defined in Appendix H.2.3.
4.Sparse attention Several works have proposed methods for efficiently computing attention Tay et al.
[2022]. We evaluate two classical methods: sliding window attention Beltagy et al. [2020], Zaheer et al.
[2020] and blocked window attention Child et al. [2019b], Qiu et al. [2019].
Consider a window size w. Sliding window attention permits each token to attend to the prior wtokens
in the sequence. Blocked window attention first splits the sequence into blocks of size wtokens, and
then computes standard causal attention within the block.
For all synthetic runs in the main paper and the appendix, we use the following training protocol:
•Optimizer and schedule : Weight decay 0 .1, warmup duration 10%, linear warmup, AdamW optimizer.
For each run, we sweep the learning rates in np .logspace( −4,−2,4). We train for 64 epochs.
•Training duration : The global batch size is 8 for input sequence length or model dimension ≥512, 16
for input sequence length or model dimension ≥256, and 64 otherwise.
•Width and depth : For all synthetic experiments, we use exactly two layers (each with one sequence
mixer and one MLP, interleaved with layer normalization). The model dimension, sequence length, and
number of KV pairs are varied based on the relevant experiment (see Appendix F).
•Position information : No position embeddings are used for the pure convolution runs (input-dependent
nor input-independent filters). Position embeddings are used for the runs with any attention variant.
•Data : We train and evaluate each model on 100 ,000 and 3 ,000 data points respectively. The data and
data order for all runs are constant.
F Extended Results on Mqar Across Architectures
In this section, we provide further validation that Mqar is a useful diagnostic tool to explain the behaviors
of a wide variety of architecture families. We show:
1.Gated convolutions In the main paper, we claim the gated convolution models with input-independent
filters use larger dimensionality than attention as the number of token-interaction distances required for
the task increases ( N, holding αandDconstant in Procedure 1).
2.Gated recurrences In this section, we show gated recurrrent models use larger dimensionality than
attention to solve the task as the number of unique key-value pairs to store ( Din Procedure 1) increases.
3.Sparse attention In this section, we validate that sliding window attention helps close the Mqar gap
when the token-interaction distances are relatively short, and degrades on longer distances. Meanwhile,
blocked window attention struggles to close the gap (intuitively tokens earlier in a block are able to
interact with few other tokens in the sequence). Concretely, in Appendix D, we observe that repeated
bigrams in the Pile and RedPajama corpora tend to occur within neighboring tokens in the context.
We finally show the architectures that perform well on Mqar also perform well on downstream real-world
AR on the Pile. We use the experimental protocols outlined in Appendix E for the synthetic experiments
and Appendix C for the downstream experiments in this section.
28

--- PAGE 29 ---
Figure 6: Mqar quality as we vary the model dimension for synthetic datasets that contain different numbers
of key-value pairs per example. ( Top) We focus on the evaluation of the input-dependent gated recurrent
RetNet Sun et al. [2023] architecture. The architecture combines chunked attention with recurrence and
we evaluate at two different chunk sizes (8 and 32). For reference, we include gated convolution (Hyena)
and attention. ( Bottom ) We evaluate two efficient attention variants – sliding window and blocked window
attention – at two window sizes (8 and 32).
F.1 Synthetic Experiments
Experiment 1: Increasing the number of token-interaction distances per example. To construct
the synthetic dataset following Procedure 1, we set α= 0.1 and |C|= 8,192 for all experiments. We
vary N∈ {64,128,256,512}to increase the number of token-interaction distances that will appear in the
examples and we vary the model dimension ∈ {64,128,256,512}. The results of this experiment are shown
in Fig. 2, validating that the gated convolutiosn use larger dimensionality than attention as the number of
token-interaction distances required for the task increases.
Experiment 2: Increasing the number of key-value pairs occurring per example. To construct
the synthetic dataset following Procedure 1, we set α= 0.1 and |C|= 8,192 for all experiments. We fix
N= 256 and vary D∈ {N
32,N
16,N
8,N
4}to increase the number of key-value pairs occurring per example. For
each number of key-value pairs per example, we additionally vary the model dimension ∈ {64,128,256}. The
results are shown in Fig. 6. We validate that as the number of associative recall keys and values exceeds the
window size, the model again uses increased dimensionality relative to full O(N2) attention to solve the task.
While Fig. 6 highlights RetNet, we note that it is a special case of the recently released Mamba Anonymous
[2023a] and GateLoop Anonymous [2023b] architectures. Corresponding to this experiment, we theoretically
show in Appendix H.6 that the gated recurrence uses Ω( N) bits to solve Mqar ford≤√
N. Intuitively, it is
difficult to store a large number of key-value pairs in low dimensional hidden states.
Experiment 3: Increasing the range of token-interaction distancs per example. To construct the
synthetic dataset following Procedure 1, we set α= 0.1 and |C|= 8,192 for all experiments. We fix N= 256
and vary D∈ {N
32,N
16,N
8,N
4}to increase the number of key-value pairs occurring per example and the range
of token-interaction distances per example. For each number of key-value pairs per example, we additionally
vary the model dimension ∈ {64,128,256}.
The results are shown in Fig. 6, validating that sliding window attention closes the gap to attention when the
number of KV pairs is within the window size. I.e., 16 KVs means that there are 16 keys and 16 values, for
32 total tokens. Sliding attention with window size 32 performs well up until 16 KVs and degrades beyond
this point relative to attention models of the same dimensionality. Sliding window attention with window
size 8 does not perform well on any setting, as expected. Blocked attention also not perform well – intuitively
tokens at the beginning of a block lack sufficient prior context to attend to.
29

--- PAGE 30 ---
Overall Slices % gap due to
Model Param (M) TFLOPs AR Hits Other Tokens AR Hits
Attention 73 1.52 12.99 (2.56) 2.41 (0.88) 14.76 (2.69) —
Long Conv 76 1.20 20.28 (3.01) 40.25 (3.70) 19.25 (2.96) 44.4%
H3 72 1.33 15.78 (2.76) 13.86 (2.63) 15.94 (2.77) 63.2%
Hyena 72 1.34 15.13 (2.72) 9.00 (2.20) 15.74 (2.76) 60.8%
RWKV 72 1.89 16.10 (2.78) 14.11 (2.65) 16.26 (2.79) 57.9%
Attention 125 2.46 11.01 (2.40) 2.16 (0.77) 12.45 (2.52) —
Long Conv 128 1.74 16.98 (2.83) 25.62 (3.24) 16.46 (2.80) 40.1%
H3 168 2.55 12.06 (2.49) 6.75 (1.91) 12.60 (2.53) 88.4%
RWKV 169 2.08 11.64 (2.45) 5.70 (1.74) 12.29 (2.51) 100.0%
RetNet (128) 152 2.60 11.15 (2.41) 3.01 (1.10) 12.45 (2.55) 100%
Hyena 160 2.41 11.60 (2.45) 5.00 (1.61) 12.28 (2.51) 100%
+ Slide Attn. (64) 159 2.28 11.57 (2.45) 5.23 (1.65) 12.30 (2.51) 100%
+ Slide Attn. (256) 159 2.29 10.65 (2.37) 3.42 (1.23) 11.52 (2.45) 100%
Attention 360 6.23 9.44 (2.25) 1.98 (0.69) 10.62 (2.36) —
Long Conv 360 4.08 13.13 (2.57) 13.27 (2.59) 13.12 (2.57) 40.5%
H3 357 4.85 10.38 (2.34) 4.81 (1.57) 11.00 (2.40) 65.8%
Hyena 358 5.03 10.07 (2.31) 3.83 (1.34) 10.75 (2.38) 98.2%
RWKV 351 4.31 9.79 (2.28) 3.82 (1.34) 10.51 (2.35) 100.0%
Table 4: Language modeling validation perplexity on the Pile. After pretraining on 10B tokens of
Pile data, we report log perplexity with negative log-likelihood in parentheses. We report overall scores, and
for the AR vs. non-AR token slices defined in Section 3. FLOPs are computed for inputs of 2048 tokens
based on the equations in Appendix C.
F.2 Demonstrating Mqar as a Tool for Architecture Development
In the main paper, we provide experiments showing that there is gap between the gated convolution and
attention models on synthetic Mqar data and correspondingly in the downstream Pile results. In Table 4,
we show that the trends of RetNet and sparse attention on Mqar also carry to the downstream results. We
hopeMqar synthetic data may be useful in future architectural development.
GMqar Perplexity Gap and Model Size
In this section, we investigate how the associative recall gap changes as we increase the model size to a billion
parameters and beyond. Overall, the results suggest that the larger models improve in memorizing bigrams
from the training data, but do not rapidly improve in in-context learning (perplexity on rare bigrams).
Below, we pretrain at the 1.4Bn parameter scale and observe that the 70M parameter attention model is a
full perplexity point better on AR than this 1.4Bn Hyena model (Table 1). We discuss these results further
in (Section 3).
Overall Slices % of gap due to
Model Param (B) TFLOPs AR Hits Other Tokens AR Hits
Attention 1.4 1.52 8.19 (2.10) 1.91 (0.65) 9.86 (2.29) —
Hyena 1.4 1.20 9.65 (2.27) 3.43 (1.23) 11.01 (2.40) 40.3%
Table 5: Large-scale language model validation perplexity on the Pile. After pretraining on 50B
tokens of Pile data, we report log perplexity with negative log-likelihood in parentheses. We report overall
scores, and for the AR vs. non-AR token slices defined in Section 3. FLOPs are computed for inputs of 2048
tokens based on the equations in Appendix C.
30

--- PAGE 31 ---
Figure 7: Perplexity of RWKV 7B and Llama2 7B parameter models on synthetic MQAR data as a function
of the number of recalls required per input / number of key-value pairs per input ( P). Inputs are constructed
using each model’s full vocabulary.
Open-source 7B parameter models We next evaluate the RWKV (gated convolution) Peng et al. [2023]
and Llama 2 (attention) pretrained models at the 7B parameter scale Touvron et al. [2023].9These are both
popular models that took a significant amount of effort to train, towards maximizing quality. We find that
there is a gap between RWKV and attention at the 7B scale and that it increases as the model needs to
conduct more recalls per sequence ( Pbelow).
We summarize the experimental protocol below.
1.Justifying the experimental protocol. Since frequent bigrams may just be memorized by the model
and not require in-context recall, our work measures AR quality on infrequent bigrams in validation
sequences (Figure 1, Appendix C.1). We do not have access to the custom training data mixtures
used in training RWKV or Llama 2 to measure bigram frequencies, so we use a synthetic test to fairly
measure the AR capabilities.
2.Evaluation dataset. We construct the synthetic dataset following Algorithm 1, where each sequence
contains Ptoken pairs, or “bigrams”. Each bigram, containing a “key” token and a “value” token,
appears twice in the sequence. On the second occurrence of a “key” token, the model should look back
to the prior occurrence to output the corresponding “value” token. We measure the AR perplexity of
the model based on its ability to predict the correct “value” token as the next word for these repeated
keys. The sequences are constructed using the models’ vocabulary tokens.
3.Evaluation details. We evaluate (inference only, no training) on sequence lengths of 1024 tokens. We
measure the AR perplexity when the sequences contain P∈ {16,32,64,128,256}key-value pairs, using
1000 samples per Pvalue. The tokens that do not contain a key or value are simply filled with a fixed
token id (so this token is repeated frequently in the sequence). We plot perplexity for AR and non-AR
tokens (fixed token) vs. P. We find RWKV quality degrades with Pon the AR slice (blue line), while
all other lines remain flat. MQAR remains problematic at scale.
Overall our results suggest that simply scaling the model size may not close the MQAR quality gap between
the gated convolutions and attention.
Measuring AR Gap with Scale In the main paper, we measure the AR perplexity on downstream data
based on bigrams that are seen <1,250×during pretraining. However, we hypothesize that larger models
9We specifically evaluate the RWKV-Raven model downloaded from https://huggingface.co/docs/transformers/model_
doc/rwkv and the Llama 2 model downloaded from https://github.com/facebookresearch/llama .
31

--- PAGE 32 ---
may memorize a larger number of bigram pairs seen in the training dataset, but do not rapidly gain the
capability to perform associative recall as well as attention in-context. In-context learning is defined as
learning from examples provided in context [Xie et al., 2021].
Concretely, the gap between the Hyena / RWKV and attention models at the 350m scale is 1.85 / 1.84
perplexity points when we focus on bigrams seen <1,250×during pretraining (Table 1). If we instead
focus on bigrams seen 1 ×during pretraining, the gap to attention quality is 12.0 / 13.2 perplexity points
respectively. The gated convolutions appear to struggle in the regime of rare bigrams that require the model
to use the context.
H Details on Theoretical Analysis
This section provides proofs and extensions to the theoretical results in the main paper.
H.1 Preliminaries and Notation
H.1.1 Notation
We denote the all 1 row vector of size k, given by1 1 . . . 1 1
, and the all 0 row vector of size k, given
by0 0 . . . 0 0
, as1kand0k, respectively. We also construe the standard basis vector eias a column
vector in these notes, and adhere to the following matrix indexing convention: M[i, j] is the entry in the ith
row and the jth column, M[i,:]∈ F1×ndenotes the ith row, and M[:, j]∈ Fm×1denotes the jth column of
M∈ Fm×n. We then use 1m×n,0m×n∈ Fm×1to denote the matrix of all 1s and 0s, respectively.
Next, we denote the Hadamard product of vectors u,v∈ Fnasu⊙v; the operation can be extended to
matrices by applying the Hadamard product column-wise across the matrices. This is commonly referred to
as(element-wise) gating . For vectors u,v∈ Fn, we also denote their linear (or acyclic) convolution asu∗v
andcyclic convolution asu⊛v.
Polynomial Notation. Because convolution is intimately tied to operations on polynomials, it is convenient
to use them to discuss the inputs and outputs of gated convolution models. Let us define maps poly,poly∗:
Fn→ F[X]/(Xn) such that
poly(u) =n−1X
i=0u[i]Xi,and poly∗(u) =n−1X
i=0u[i]Xn−1−i.
This allows us to map between vectors and polynomial. Accordingly, we also define coeff : F[X]/(Xn+1)→ Fn
as the map converting polynomials back to vectors: coeff(u(X)) =uwithu[i] defined as the coefficient in
u(X) at degree i.
These operations allow us to interpret the convolution of vectors in terms of polynomial multiplication [Hei-
deman and Burrus, 1988]. More specifically, we have
u∗v= coeff ( u(X)·v(X) mod Xn),and
u⊛v= coeff ( u(X)·v(X) mod Xn−1).
We can similarly interpret the Hadamard product of vectors u⊙vas the Hadamard product of polynomials
u(X)⊙v(X):
u⊙v= coeff ( u(X)⊙v(X)) = coeff n−1X
i=0(u[i]·v[i])·Xi!
.
Arithmetic Circuit Notation. We briefly introduce the notation of arithmetic circuits [Volkovich, 2016],
the focus of Appendix H.5. An arithmetic circuit Cwith variables X≜{x1, x2, . . . , x n}over a field Fis
interpreted as a directed acyclic graph, where the input nodes are labelled by either the variables from X
or constants from Fand the internal nodes are labelled by + or ×with the output being the polynomial
computed at the output node.
32

--- PAGE 33 ---
We shall also refer to the sizeof the circuit as the number of nodes, the depth of the circuit as the length of
the longest path between an input node and the output node, and the width of the circuit as the number
of parallel operations in the circuit, or ‘wires’ which will be intersected by a horizontal ‘cut’ through the
circuit. Moreover, the degree of a circuit is defined as the degree of the polynomial computed by the circuit.
We summarize this with the following definition:
Definition H.1. An arithmetic circuit Cis an ( n, s,∆, w)-circuit ifCis an n-variate arithmetic circuit of
sizesand of depth at most ∆, and width w.
Model Notation. Now we introduce the notation we will be using for defining layers. In what follows, we
denote u∈ RN×das the model input; Nas the sequence length; Las the number of stacked layers, indexed
byℓ; and das the input (embedding) dimension.
H.1.2 Summary of the Results
The outline of our results are as follows: In Appendix H.2 we introduce gated convolution models and define
H3, Hyena, RWKV, RetNet, and BaseConv . In Appendix H.3 we introduce a set of primitive operations
thatBaseConv can implement. We use these as tools in the subsequent proofs. Then, in Appendix H.5, we
show that a general arithmetic circuit of size sand degree at most ∆ can be simulated by BaseConv . We
use the above results to show all models built from gating and convolutions can be simulated by BaseConv .
This helps us analyze the class of gated convolutions, beyond a specific proposal (e.g. Hyena).
Next, we study the representational power that gated convolutions and attention use to solve Mqar . In
Appendix H.7, we derive a BaseConv model inspired from dyadic intervals to show the dimensionality for
gated convolutions (with input-independent filters) solve Mqar . Next, we analyze the dimensionality for a
BaseConv model with data-dependent kernels to solve Mqar in Appendix H.8.
H.2 Gated Attention-Free Models
We now present formal definitions of gated convolution and recurrence models with respect to the 5-tuple of
parameters ( N, L, d, N′, d′).
Definition H.2. An(N, L, d, N′, d′)−Gated Convolution Model is a stacked sequence to sequence model
with Llayers such that:
1. Input and output are N×dmatrices,
2. Each layer’s operations consist of element-wise gating, convolution, linear projection, and
3.All the individual gated convolution layers take in N′×d′matrices and output N′×d′matrices. We
refer to the tuple ( N′, d′) as the inner dimension of the model.
We define the Hyena, RWKV, RetNet, and BaseConv layers to make step 2 more concrete. We also assume
that the input u∈ RN×dis embedded into u′∈ RN′×d′such that
u′[n, t] =(
u[n, t] if n < N, t < d
0 otherwise.
The output from the last layer z∈ RN′×d′is transformed into output y∈RN×dby extracting the top left
N×dentries in z.
Next, we define the class of weight matrices that we will use in the linear projections in the models:
Definition H.3. The linear projection Linear m,mhas its matrix representation as the weight matrix10W∈
Rm×mtaken to be a K-matrix forW∈(BB∗)poly- log m
poly- log m(Definition H.17). Consequently, each matrix W
has˜O(m) parameters and runtime for matrix vector multiplication, and allows us to represent general linear
transformations with low-depth linear arithmetic circuits [Dao et al., 2020]. We will also need linear maps
Linear m,nform < n , where each take the corresponding square matrices from Linear m,mand note that
such matrices has ˜O(n) parameters and runtime for matrix vector multiplication.
Remark H.4. We note that the weight matrix W∈Rd×dabove is taken to be a dense matrix in the
experiments. However, for our theoretical results, we restrict the linear projection in our models as per
10I.e.Linear m,m(z) =W·z.
33

--- PAGE 34 ---
Definition H.3. In the rest of the paper, unless mentioned otherwise all linear projections will follow
Definition H.3.
Next, we define three popular gated convolution models that we study in our work: Hyena Poli et al. [2023a],
RWKV Peng et al. [2023], and RetNet Sun et al. [2023].
H.2.1 The Hyena Layer
We will now outline the Hyena layer [Poli et al., 2023b]. Hyena takes a sequence u∈ RN×das input and
produces L+ 1 projections p1, . . . , pL, vby passing ythough a linear layer and applying a short convolution
afterwards. The algorithm then recursively performs a point-wise multiplication of the projection with the
convolution of the filter hlwith the previous output. We summarize this process in Algorithm 3.
Algorithm 2 Projection (u,h)
Input: Input sequence u∈ RN×d, a short convolution filter h∈ RN.
1:In parallel for 0 ≤n < N :ˆz[n,:]←Linear d,(L+1)d(u[n,:]) so that ˆz∈ RN×(L+1)d
2:In parallel for 0 ≤t <(L+ 1)d:z[:, t]←h∗ˆz[:, t]
3:Reshape and split z∈ RN×(L+1)dintop1, . . . ,pL,v, where pℓ,v∈ RN×dforℓ∈[L].
4:return p1, . . . ,pL,v.
Algorithm 3 Hyena (u,h,hs)
Input: Input sequence u∈ RN×d, set of convolution filters h1, . . . ,hL∈ RN×d, short convolution filter
hs∈ RN.
1:p1, . . . ,pL,v←Projection (u,hs).
2:z0←v
3:forℓ= 1, . . . , L do
4: In parallel for 0 ≤t < d :zℓ[:, t]←pℓ[t,:]⊙ 
hℓ[:, t]∗zℓ−1[:, t]
.
5:return zL
Remark H.5. In Algorithm 3, Lis used to denute the nuber of recursive applications of the Hadamard
product and convolutions, not the number of layers Note that asymptotically, the recursive step does not
make a difference.
Henceforth, we will refer to a model consisting of LHyena layers is a gated convolution model with associated
tuple ( N, L, d, N, (L+ 1)d) as ( N, L, d, N, (L+ 1)d)−Hyena.
H.2.2 The RWKV Layer
We will now describe the RWKV layer [Peng et al., 2023]. RWKV is typically referred to as an RNN, but,
like some other recurrent models ( e.g.S4 [Gu et al., 2021]), it can also be viewed as a convolution. Here, we
present the convolutional view of RWKV. We will see that it is closely related to the Hyena layer.
RWKV takes a sequence u∈ RN×das input and applies a short convolution. Next, it produces three
projections q, k, v by passing uthrough a linear layer. Then, it performs a convolution sandwiched by
element-wise multiplication. We summarize this process in Algorithm 5.
Algorithm 4 RWKVProjection (u,h)
Input: Input sequence u∈ RN×d, a short convolution filter h∈ RN.
1:In parallel for 0 ≤t < d :ˆz[:, t]←h∗u[:, t]
2:In parallel for 0 ≤n < N :z[n,:]←Linear d,3d(ˆz[n,:]) so that z∈ RN×3d
3:Reshape and split z∈ RN×3dintoq,k,v∈ RN×d.
4:return q,k,v.
34

--- PAGE 35 ---
In practice, the short convolution filter hsis restricted to a length two filter with hs[0] = µandhs[1] = 1 −µ,
where µ∈[0,1] is learned parameter. In the RWKV paper, the short convolution is referred to as a “time
shift”.
Algorithm 5 RWKV (u,h,hs)
Input: Input sequence u∈ RN×d, set of convolution filters h1, . . . ,hL∈ RN×d, short convolution filter
hs∈ RN.
1:q,k,v←Projection (u,hs).
2:In parallel for 0 ≤t < d :zℓ[:, t]←σ(q[:, t])⊙ 
hℓ[:, t]∗(softmax( k)⊙v)[:, t]
.
3:return zL
In practice, the long convolution filter his also restricted to h[:, t] =ew(t−1), where w∈Ris a learnable
parameter that controls how quickly the magnitudes of the filter decreases as tgrows.
To summarize, the differences between Hyena and RWKV are: (1) Hyena applies the short convolution after
the linear projection whereas RWKV applies it before , (2) RWKV includes non-linear activations (sigmoid and
softmax) while Hyena does not, (3) RWKV and Hyena use different parameterizations for the convolutional
filters, and (4) Hyena recursively performs a point-wise multiplication of the projections with the convolution
filter whereas RWKV performs this operation only once (though, in practice, Hyena uses a recurrence depth
of one, making them equivalent in this regard).
H.2.3 The Retnet Layer
In this section, we introduce the RetNet layer Sun et al. [2023]. To this end, we take an input sequence
u∈ RN×dand project it using learnable weight matrices. We then compute the states znrecurrently as in
line 5 and the output Out[n,:] as in line 6 for each n∈[N] (see Algorithm 6 below).
Algorithm 6 RetNet (u,h,hs)
Input: Input sequence u∈ RN×d, a scalar γ∈ R, learnable weight matrices WA,WC,WV∈ Rd×d.
1:A,C,V←uWA,uWC,uWVso that A,C,V∈ RN×d.
2:Initialize the output Out∈ RN×d
3:Initialize the state z0←(A[0,:])⊤V[0,:].
4:for1≤n < N do
5: zn←γzn−1+ (A[n,:])⊤V[n,:] forzn∈ Rd×d
6: Out[n,:]←C[n,:]zn
7:return Out
H.2.4 BaseConv
Finally, we introduce the BaseConv here as follows:
Y= (uW+b1)⊙(u∗h+b2), (4)
with input u∈ RN′×d′, weight matrix W∈Rd′×d′and bias matrices bi∈RN′×d′defining linear projections
of the input sequence, and h∈ RN′×d′is the a set of the d′mixed length filters.
The corresponding pseudocode for BaseConv is as follows:
35

--- PAGE 36 ---
Algorithm 7 BaseConv (u,W,b1,h,b2)
Input: Input sequence u∈ RN′×d′, linear mapping W∈ Rd′×d′so that W∈(BB∗)poly log d′
poly log d′, convolution
filterh∈ RN′×d′, bias matrices b1,b2∈ RN′×d′.
1:In parallel for 0 ≤n < N′:x[n,:] =Linear d′,d′(u[n,:])
2:In parallel for 0 ≤t < d′:z[:, t] =h[:, t]∗u[:, t]
3:In parallel for 0 ≤t < d′:y[:, t]←(x[:, t] +b1[:, t])⊙(z[:, t] +b2[:, t]). ▷See equation 4
4:return y
Note that the convolution in Algorithm 7 is not limited to causal convolution and allows circular convolution
as well. For simplicity, we use ∗here and will disambiguate when required. We will start by specifying the
parameter count and complexity of a single layer of BaseConv below.
Proposition H.6. A single layer of BaseConv requires ˜O(N′d′)parameters and runtime.
Proof. We know the parameters and runtime for the linear part of BaseConv via Definition H.3 is ˜O(N′d′).
Further, each convolution operation requires O(N′) parameters and O(N′logN′) runtime, and we employ d′
such operations. Finally the Hadamard product step takes O(nd) time.
Similar to Hyena, we will refer to a model consisting of LBaseConv layers as (N, L, d, N′, d′)−BaseConv .
In our experiments, we extend BaseConv by adding an MLP after Algorithm 7. For simplicity we will
denote BaseConv (u,h,b1,b2) asBaseConv (u,h) when b1=b2=0.
We will now show that there exists a BaseConv model that can emulate each of the basic operations in
Algorithm 7.
Lemma H.7. The functions Linear d,d(u)(Definition H.3), with d, d′defined as in Algorithm 7, convolution
with filter h∈ RN×d, and element-wise gating can be computed with Algorithm 7 via a (N,1, d, N, d′)−
BaseConv .
Proof. For each operation from Definition H.2 and Algorithm 7:
1.For any input u∈ RN′×d′,Linear d,d′(u)with matrix representation W∈ RN′×d′can be performed
by a single BaseConv layer computing BaseConv (y,W,h,b1,b2) with b1andb2being the matrix
of all 0s and all 1s, respectively while and the convolution with the zero filter. That is, we have
Y= (uW+0N′×d′)⊙(u∗0N′×d′+1N′×d′) = (uW)⊙1N′×d′=uW=Linear d,d′(u).
2.For any input u∈ RN×d, convolution with filter h∈ RN×dcan be performed by a single BaseConv
layer computing BaseConv (y,W,h,b1,b2)where W,b2are all zeroes, and b1is the matrix of all 1s
so that we get
Y= (u0N′×d′+1N′×d′)⊙(u∗h+0N′×d′) =1N′×d′⊙(u∗h) =u∗h.
3.We may compute element-wise gating between matrices u,v∈ RN×d, where vis some fixed factor,
with a single layer computing BaseConv (y, ,)0N′×d′,e0,v,0N′×d′where e1is the identity filter,
respectively, by Definition H.2.
Y= (u0N′×d′+v)⊙(u∗e0+0N′×d′) =v⊙u.
H.3 Primitives
In this section, we will establish some additional basic primitives that we expect a gated convolution model
to emulate: shift ,remember andadd. We specify them below:
1.Shift an sequential input of length Nup or down by sentries:
36

--- PAGE 37 ---
shift up(y, s), shift down( y, s)
•Input: y∈ RN×d,s≥0
•Output: z∈ RN×dwhere z+=shift down(y, s) and z−=shift up(y, s)
y≡
←y0→
...
←yi−1→
←yi→
...
←yN−1→
z+≡
←0→
...
←0→
←y0→
...
←yN−1−s→
z−≡
←ys→
...
←yN−1→
←0→
...
←0→

2.Add a sequence x∈ Rn×dto a running sum S∈ Rn×dfor some 2 n≤N′with both xandScontained
as subvectors in y∈ RN×d.
addn(y:x,S):
•Input: sequence ycontaining x,S∈ Rn×dfor 2n≤Nsuch that y[0 :n−1]≡x,y[n:
2n−1]≡Sandy[2n:N−1]≡0N−2n.
•Output: z∈ RN×dcontaining the sum y+Ssuch that y[0 :n−1]≡1n,y[n: 2n−1]≡S+x
andz[2n:N−1]≡0N−2n.
y≡
←x→
←S→
←−0−→
...
←0→
z≡
←1n→
←S+x→
←−0−→
...
←0→

3.Remember v∈ Rm×das part of a sequence of input y∈ RN×dwhile performing gated convolution
onlyonx∈ Rn×dfor some m, n≤N×d.
37

--- PAGE 38 ---
remember n,m,s,t (y:x,v,h,p ):
•Input: sequence y∈ RN×dcontaining x∈ Rn×d,v∈ Rm×d, and modifiers p,h∈ Rn×d
such that y[0 :n−1]≡x,y[n+s:n+s+m−1]≡vandy[i] = 0 otherwise with
x∗h∈ R(n+s)×dandv∗h∈ R(m+t)×d.
•Output: z∈ RN×dcontaining ( p⊙(x∗h))∈ R(n+s)×d, such that:
y≡
←x→
0s
←v→
...
0
z≡
←p⊙(x∗h)→
←v→
...
0

These primitives are building blocks of our proofs in the sequel. We will show that each of these primitives
can be solved by some ( N, L, d, N′, d′)−BaseConv model with a small constant L.
Proposition H.8 (The Shift Primitive) .For any y∈ RN×d, there exist (N,1, d, N, d )−BaseConv and
(N,3, d, N, d )−BaseConv that computes shift down(y, s)andshift up(y, s)for any s≤N.
Proof. Define the following kernel dependent on s≤N
hs[n,:]≡(
1difn=s+ 1
0dotherwise .
We now deal with the down and up shifts separately:
1.We define W :=0N×d,b1:=1N×d,b2:=0N×d. Then, for input y∈ RN×d,
BaseConv 
y,0N×d,hs,1N×d,0N×d
forBaseConv in Algorithm 7 is given by equation 4 as
Y≡y∗hs.
Now, to perform shift down(y, s), we note that
Y[:, j] =y[:, j]∗hs[:, j] = coeff( y[:, j](X)·hs[:, j](X))
= coeff  N−1X
i=0y[i, j]·Xi!
·Xsmod XN!
= coeff N−1X
i=0y[i, j]·Xi+smod XN!
= coeff N−1+sX
i=sy[i−s, j]·Ximod XN!
= coeff N−1X
i=sy[i−s, j]·Xi!
,
which implies that we exactly get what is specified in the output.
2.We again define W:=0N×d,b1:=1N×d,b2:=0N×d. Then, for input y∈ RN×d,
BaseConv 
y,0N×d,e0,1N×d,0N×d
forBaseConv in Algorithm 7 is given in equation 4 as
Y0≡y⊛e0.
38

--- PAGE 39 ---
Now, to perform shift up(y, s), as before, we first apply the circular convolution to reverse the input
Y0[:, j] =y[:, j]⊛e0= coeff N−1X
i=0y[N−1−i, j]·Xi!
,
We then apply Y1≡shift down(Y0, s) to get
Y1[:, j]≡coeff N−1X
i=sY0[N−1−(i−s), j]·Xi!
,
≡coeff N−1X
i=sY0[N−1−i+s, j]·Xi!
.
Finally, we apply another circular convolution with the identity filter to replace N−1−iwith ito get
Y2[:, j] =Y1[:, j]⊛e0= coeff N−1X
i=0y[i+s, j]·Xi!
,
Here, we note that we can compute both of these primitives in one and three layers, respectively (see
Lemma H.11).
Now, we present a BaseConv model with two layers that implements the addn(y:x,S), the purpose of
which is to add some window of computation xto a running sum S.
Proposition H.9 (The Running Sum Primitive) .For any x,S∈ Rn×dcontained in some y∈ RN×d, there
exists a (N,2, d, N, d )−BaseConv that computes addn(y:x,S)forBaseConv as in Algorithm 7.
Proof. We will show this for d′= 1 and the general case follows as we will explain at the end. We now specify
the two layers that we use
z1≡BaseConv 
y,0N×1,h1,b1
1,0N×1
≡b1
1⊙ 
h1∗y
z≡BaseConv 
z1,0N×1,h2,b2
1,b2
1
≡b2
1⊙ 
h2∗y+b2
2
,
where we will specify the kernels as we go along. Let us start by defining the kernel and the bias for the first
layer as
h1≡
e0
e0
0n
···
0n
,b1≡
0n
1n
0n
···
0n
.
Let us first compute h1∗yas follows:
h1(X)·y(X) = (Xn+ 1)·(S(X)·Xn+x(X))
=S(X)·X2n+ (S+x)(X)·Xn+x(X).
39

--- PAGE 40 ---
We then have
z1≡b1
1⊙ 
h1∗y
≡
0n
1n
0n
···
0n
⊙
x
S+x
S
···
0n
≡
0n
S+x
0n
···
0n

Resetting for Next Phase. We now use the next layer to reset for the next phase. Here, we need the
first vector to be 1nin order to start adding the next vector. We thus use the kernel and the biases h2,b2
1,b2
2
defined as
h2≡
e0
0n
0n
···
0n
,b2
1≡
1n
1n
0n
···
0n
,b2
2≡
1n
0n
0n
···
0n
.
Explicitly, for the second layer, we compute the result of the convolution in terms of polynomials as follows:
h2(X)·z1(X) = 1·(S+x)(X)·Xn= (S+x)(X)·Xn.
Thus, the output for the second layer is given by
z≡b2
1⊙ 
h2∗z1+b2
2
≡
1n
1n
0n
···
0n
⊙

0n
S+x
0n
···
0n
+
1n
0n
0n
···
0n

≡
1n
1n
0n
···
0n
⊙
1n
S+x
0n
···
0n
≡
1n
S+x
0n
···
0n
.
Therefore, we have used two BaseConv layers to add xto the running sum Sand reset for the next phase.
Here, we note that the only operations we perform and are convolutions and Hadamard product and they
generalize in the obvious way to d >1.
Next, we show that a five layer BaseConv model can perform gated convolution on windows of the input
(without changing the rest of the input).
Proposition H.10 (The Remembering Primitive) .For any x∈ Rn×d,v∈ Rm×dcontained in some
y∈ RN×dfor some n+m+s+t≤Nso that for h∈ Rn×dandp∈ R(n+s)×dwithx∗h∈ R(n+s)×d
andv∗h∈ R(m+t)×d, there exists a (N,5, d, N, d )−BaseConv that computes remember (y:x,v,h,p)for
BaseConv as in Algorithm 7.
Proof. We will again show this for d′= 1 and the general case should follow. We now specify the first two
layers that we use
z1≡BaseConv 
y,0N×1,h1,b1
1,0N×d
≡b1
1⊙ 
h1∗y
z2≡BaseConv 
z1,0N×1,h2,b2
1,0N×d
≡b2
1⊙ 
h2∗y
,
40

--- PAGE 41 ---
The kernel h1and the bias b1
1for the first layer are then given by
h1≡
h
0m
es+t
0n
0n
···
0n
,b1
1≡
p
0m+t
0n
0s
1m
···
0n
.
where recall that x∗h∈ R(n+s)×dandv∗h∈ R(m+t)×d.
We now want to first specify the result of applying the first kernel:
 
h1∗y
= coeff 
(h(X) +Xn+m+s+t)· 
v(X)·Xn+s+x(X)
= coeff 
h∗v(X)·Xn+s+h∗x(X) +v(X)·X2n+2s+m+t+x(X)·Xn+m+s+t
We then have
z1≡b1
1⊙ 
h1∗y
≡
p
0m+t
0n
0s
1m
···
0n
⊙
h∗x
h∗v
x
0s
v
···
0n
≡
p⊙(h∗x)
0m+t
0n
0s
v
···
0n
.
We now describe the second kernel h2and the bias matrix b2
1as follows:
h2≡
e0
0m+t
e0
0n+s
0m
···
0
,b2
1≡
0n+s
0m+t
0n
1n+s
1m
···
0

41

--- PAGE 42 ---
This yields the following convolution computation:
h2⊙z1≡coeff  
Xm+n+s+t+ 1
· 
v(X)·X2n+2s+m+t+ (p⊙(h∗x)) (X)
≡coeff(v(X)·X3n+3s+2m+2t+v(X)·X2n+2s+m+t
+ (p⊙(h∗x)) (X)·Xm+n+s+t+ (p⊙(h∗x)) (X))
Thus we have
z2≡b1
2⊙ 
h2∗z1
≡
0n+s
0m+t
0n
1n+s
1m
···
0
⊙
p⊙(h∗x)
0m+t
0n
p⊙(h∗x)
v
···
0
≡
0n+s
0m+t
0n
p⊙(h∗x)
v
···
0

We now shift this up by 2 n+s+m+tentries using the primitive operation defined in Proposition H.8 that
costs three additional layers so that we end up with
z≡
p⊙(h∗u)
v
···
0

Again, we note that the only operations we perform and are convolutions and Hadamard product and they
generalize in the obvious way to d >1.
Finally, we show that these primitives may be composed by ’stacking’ models with matching inner dimension
(N′, d′).
Lemma H.11. Forf, g: RN×d→ RN×dthat have (N, L 1, d, N′, d′)and(N, L 2, d, N′, d′)BaseConv
models then their composition f◦ghas an (N, L 1+L2, d, N′, d′)BaseConv model which can be computed
by performing their models in succession, or ’stacking’.
Proof. This result follows from noting that for any f(u) which requires L1layers to compute and that we can
compute f◦g(u) =g(f(u)) using the BaseConv model with L2layers, yielding L1+L2layers in total.
H.3.1 BaseConv -Hyena Equivalence
We show that the equivalence between BaseConv and Hyena by showing that each layer can simulate the
other’s computation using a constant number of layers.
Proposition H.12. For any input u∈ RN×dand(N, L, d, N′, d)−Hyena such that zHyena≡Hyena (u)with
a set of filters hℓand linear projections pℓas per Definition H.3 for ℓ∈[L], there exists a (N,5L, d, N′+N, d)-
BaseConv model such that zHyena≡BaseConv (u).
Similarly, for any input uBaseConv ∈ RN×dand(N, L, d, N′, d)−Coyote such that zBaseConv ≡BaseConv (u)
with a set of filters hℓforℓ∈[L], there exists a series of Hyena layers such that we have
Hyena (Hyena ( . . .Hyena( uBaseConv ,h)))| {z }
Llayers≡zBaseConv .
42

--- PAGE 43 ---
Proof. For the input uHyena∈ RN×d, the output of the ℓth layer zℓ
Hyena∈ RN′×d′for Hyena is given by (see
Algorithm 3)
zℓ
Hyena≡pℓ
Hyena⊙(hl∗zℓ−1),
where pℓ
Hyena≡Linear (uHyena )∈ RN′×d. Now, using the original input uHyena∈ RN×dto Hyena, we define
the following input for BaseConv using one layer:
uBaseConv ≡
uHyena
0(N′−N)×d
uHyena

Then, we simply use the remember N,N,N′−N,N′−N(uBaseConv :uHyena ,uHyena ,hℓ
Hyena ,pℓ
Hyena ) primitive for
BaseConv . Consequently, this allows us to “remember” the input uHyena in the output of the previous
BaseConv layerzℓ−1
BaseConv . We then use this to retrieve pℓ
Hyena≡linear (uHyena ) with the projection used for
BaseConv given by
pℓ
BaseConv ≡Linear (zℓ−1
BaseConv )≡
1N×d
pℓ
Hyena .

Overall, the output of the ℓth layer for BaseConv is given by
zℓ
BaseConv ≡
pℓ
Hyena⊙ 
hℓ
Hyena∗uHyena
0(M−N)×d
uHyena
≡
zℓ
Hyena
0(M−N)×d
uHyena

Hence, we can reproduce the output of the ℓth layer of Hyena using five layers of BaseConv after augmenting
the input and using the remembering primitive (Proposition H.10) with internal dimension N′+N.
Now, for the input uBaseConv ∈ RN×d, the output of the ℓth layer for BaseConv is given by
zℓ
BaseConv ≡Linear (zℓ−1
BaseConv )⊙conv( hl, zℓ−1
BaseConv ).
Here, we show inductively that simply using ℓ-many Hyena models recursively simulates zℓ
BaseConv . For ℓ= 1,
we have
Hyena( uBaseConv ,h)≡Linear (uBaseConv )⊙(h1∗uBaseConv )≡z1
BaseConv .
We now assume that ( ℓ−1)-many recursive Hyena models produce z(ℓ−1)
BaseConv . For the ℓth layer, we then have
Hyena (Hyena ( . . .Hyena( uBaseConv ,h)))
≡Hyena
z(l−1)
BaseConv
≡linear
z(l−1)
BaseConv
⊙conv
hl, z(l−1)
BaseConv
≡zℓ
BaseConv .
H.4 Linear Arithmetic Circuits
In this section we show the relation between linear arithmetic circuits and BaseConv . We recall a few
definitions from [Dao et al., 2020].
43

--- PAGE 44 ---
Definition H.13 (Linear Arithmetic Circuit [B¨ urgisser et al., 1996]) .An arithmetic circuit is called a linear
arithmetic circuit if it only uses addition, subtraction and scalar multiplication. Further, every multiplication
has a fixed constant from Fas at least one of its two inputs. In other words, all gates in the circuit are linear
functions of their inputs (i.e. of the form ax+byfor fixed constants a, b∈ F).
Definition H.14 (Butterfly Matrices [Dao et al., 2020]) .Abutterfly factor of size k≥2 (denoted as Bk) is
a matrix of the form Bk=
D1D2
D3D4
where each Diis ak
2×k
2diagonal matrix. We restrict kto be a
power of 2 .
Abutterfly factor matrix of size nwith block size k(denoted as B(n)
k) is a block diagonal matrix ofn
k
(possibly different) butterfly factors of size k:
B(n)
k= diag
[Bk]1,[Bk]2, . . . , [Bk]n
k
Finally, a butterfly matrix of size n(denoted as B(n)) is a matrix that can be expressed as a product of
butterfly factor matrices: B(n)=B(n)
nB(n)
n
2. . .B(n)
2. Equivalently, we may define B(n)recursively as a matrix
that can be expressed in the following form:
B(n)=B(n)
n
h
B(n
2)i
10
0h
B(n
2)i
2

(Note thath
B(n
2)i
1andh
B(n
2)i
2may be different.)
From Definition H.14, we observe that size nbutterfly factor is comprised of three vectors d,d+,d−∈ Rn
such that
d= 
diag−1(D1),diag−1(D4)
,
d+=
0n
2,diag−1(D2)
,and
d−=
diag−1(D3),0n
2
,
where diag−1(D) : Rn×n7→ Rnis the mapping from diagonal matrices to the vector of its diagonal entries.
Let us define D1,D2,D3∈ Rn×nas diag ( d),diag 
d+
, and diag 
d−
respectively. Then we note that
D1≡D10
0 D 4
D2Sn
2≡0 D 2
0 0
Sn
2D3≡0 0
D30
(5)
where Sk∈ Fn×nis a shift matrix for i∈[n/2]. This gives us the following proposition:
Proposition H.15. For any powers of 2, n=k≥2, any butterfly factor matrix B(n)
kis equivalent to
B(n)
k=Sk
2D3+D2Sn
2+D1
where D3,D2,D1,Sn
2are defined as in equation 5.
We use Proposition H.15 to show that butterfly matrices can easily be computed by BaseConv .
Lemma H.16. For any n, d≥2,k≥1, and arbitrary vector x∈ Rnd:
(1)there exists a (N, L, d, N′, d′)−BaseConv that can represent B(nd)
k·xwith N=n, N′=O(N),
L=O(1), and d′=O(d), and
(2)there exists a (N, L, d, N′, d′)−BaseConv that can represent B(nd)·xwith N=n, N′=O(N),
L=O(lognd), and d′=O(d).
Proof. (1)Given x∈ Rnd, construct u∈ Rn×dwhere xis the row-major form of u. We show that
BaseConv can compute Bnd·xcolumn by column.
LetA=Sk
2D′
3,C=D′
2Sn
2, and D=D1forDi,Sk
2∈ Rnd×ndfor 1 ≤i≤3 as defined in
Proposition H.15. We take d1=1ndD,d2=1ndC2,d3=1ndA, which extracts the diagonal entries of
Di. With this we construct D′
i∈ Rn×dwhere diis the row major form of D′
i. This implies that
Dix≡D′
i⊙u.
44

--- PAGE 45 ---
Then we can decompose Bnd·xinto
Bndx≡D1⊙u+D2⊙u+D3⊙u.
By Lemma H.7, each Hadamard product A⊙u,B⊙u,C⊙ucan be trivially be performed with a single
layerBaseConv model. Let each of these model outputs be denoted y1,y2,y3, respectively. Finally
all that remains is to compute the y1+y2+y3. We achieve this using layers of add primitives11:
addn(y1:y1,0)
addn(y2:y2,y1)
addn(y3:y3,y1+y2),
where using by Proposition H.9 and Lemma H.11, this requires six more layers, and we get
y3≡y1+y2+y3≡Bndx.
Then we can construct the ( N, L, d, N′, d′)−BaseConv as desired with L=O(1) layers.
(2)From Definition H.14, B(nd)=B(nd)
ndB(nd)
nd
2. . .B(nd)
2. From (1), BaseConv can compute any butterfly
matrix by simulating the log(nd) butterfly factor matrices which comprise B(nd). With Lemma H.11,
this creates a BaseConv with 5 ·log(nd) =O(log(nd)) layers. Lemma H.11
Butterfly matrices comprise the kaleidoscope hierarchy, which we define below:
Definition H.17 (The Kaleidoscope Hierarchy [Dao et al., 2020]) .
•Define Bas the set of all matrices that can be expressed in the form B(n)(for some n).
•Define ( BB∗) as the set of matrices Mof the form M=M1M∗
2for some M1,M2∈ B.
•Define (BB∗)was the set of matrices Mthat can be expressed as M=Mw. . .M2M1, with each
Mi∈(BB∗) (1≤i≤w).(The notation wrepresents width.)
•Define (BB∗)w
eas the set of n×nmatrices Mthat can be expressed as M=SES⊤for some en×en
matrix E∈(BB∗)w, where S∈ Fn×en=In0. . . 0
] (i.e. Mis the upper-left corner of E). (The
notation erepresents expansion relative to n.)
We similarly show how BaseConv can simulate any kaleidoscope matrix.
Lemma H.18. Given n, d≥2,e >0for any nd×ndmatrix M∈(BB∗)w
e, and x∈ Rndthere exists a
(N, L, d, N′, d′)−BaseConv that can represent M·xwithN=n, L=O(wlog(end)),N′=en,and d′=d.
Proof. By Definition H.17, Mcan be decomposed with respect to size end×endmatrix
E=E1·E2···Ew.
Further, any Ei∈(BB∗)can be expressed as a product of 2 logendbutterfly factor matrices. Then by
Lemma H.16 and Lemma H.11 we can compute Eix′in by stacking 2 logend(n, d, L, en, d )−BaseConv
models each with L=O(1). Because Ehas width w, Lemma H.11 implies that composing with each Eifor
1≤i≤wconstructs a final model with O(wlog(end)) layers.
Finally, the kaleidoscope hierarchy is related to linear arithmetic circuits via the following result. We note
that in [Dao et al., 2020] it is assumed that w=s, yet inspection of the proof yields the following stronger
result:
Theorem H.19 ( [Dao et al., 2020]) .LetMbe an n×nmatrix such that multiplication of Mtimes an
arbitrary vector ucan be be represented as (n, s,∆, w)-linear arithmetic circuit C. Then, M∈(BB∗)O(∆)
O(w/n).
We combine Theorem H.19 and Lemma H.18 to show that BaseConv can compute any linear arithmetic
circuit with polylogarithmic factors in ∆.
Corollary H.20. For any (nd, s, ∆, w)-linear arithmetic circuit Cthat can be represented by a matrix
M∈ Rnd×ndmultiplied by a vector x∈ Rnd, there exists an equivalent (n,∆′, d, w, d )−BaseConv with
∆′=O(∆ log( w))such that Mx=BaseConv (u,h)where xis the row major form of u∈ Rn×d.
11Recall that addn(y:x,S) adds the subvector xtoSfor the input y.
45

--- PAGE 46 ---
H.5 General Arithmetic Circuits
We are now ready to prove the result that yields the equivalency between arithmetic circuits and BaseConv .
Theorem H.21. For any (nd, s, ∆, w)-arithmetic circuit C, there exists an equivalent (N,∆′, d, N′, d′)−
BaseConv withN=n,∆′=O(∆ log w),N′=O(w), d′=dthat simulates C.
For the reader’s convenience, we begin with a proof sketch and then provide the details afterwards.
Proof Sketch of Theorem H.21. Let us layer Cso that each layer Cℓforℓ∈[LC] either only has linear gates
or multiplication gates, where the number of such layers LC=O(∆). The composition of all Cℓlayers results
inC. We use zℓ+1∈ Rwto denote the output of the ℓ-th layer Cℓwhich feeds as the input to the ( ℓ+ 1)-th
layerCℓ+1. Here, we note that if we can simulate each CℓwithBaseConv , then we can simulate the entire
layered circuit Cdue to Lemma H.11.
Now, if the layer Clin
ℓis a linear layer (with only addition gates), then it can be represented by a matrix
M∈ Rw×wmultiplied by zℓ∈ Rw(We can append with 0s if necessary so that the input from the previous
gates can be written as w-length vector). Thus, we can apply Corollary H.20 to simulate Clin
ℓwith an
equivalent ( n,logw, d,O(logw), d)−BaseConv model.
Next, if Cmult
ℓinstead consists of only the multiplication gates. Then, we note here that the output zℓmay
not exactly equal the input to Cmult
ℓ. Nevertheless, we can apply a O(w) sparse linear map R∈ Rw×wso
thatRzℓyields vectors v1,v2,andv3, where v1constitutes the “first” input to all the multiplication gates
andv2constitutes all the “second” inputs while v3consists of all entries needed as inputs in the subsequent
layers. That is, for the ith gate in Cmult
ℓ, we compute v1
i·v2
i. This implies that for all the gates in Cmult
ℓ, we
can simply compute v1⊙v2. To this end, we use the remember primitive with constant number of layers from
Proposition H.10 to define a (n,O(logw), d, w, d )−BaseConv model that remembers v3while performing
the Hadamard product of v1withv2.
Overall, we can then collect all the resulting BaseConv layers and compose them as in Lemma H.11 to
simulate C. Overall, the number of layers used is given by O(∆ log w) while the internal dimension remains
fixed at w.
Using the outline in the proof sketch, we now delve into a detailed analysis of the arithmetic circuit Cto an
equivalent BaseConv model.
Proof of Theorem H.21. Letu∈ Rndbe the input to the arithmetic circuit Cwith depth ∆ and width w.
We begin by rearranging the circuit into layers of addition and multiplication gates. That is, each layer Cℓ
has either all addition gates or all multiplication gates. This allows us to readily apply the results from
Appendix H.4. Note here that we can assert that the number of such layers LC=O(∆). Moreover, we
construe the input to each such circuit as a vector of length wby appending with extra 0s if necessary so
that the composition of the layers results in a circuit equivalent to C. See Fig. 8 for an example of such
decomposition
Letzℓ∈ Rwdenote the input to each layer Cℓforℓ∈[LC] with z1≡(u,0w−nd). It is important to note
here that we may nothavezℓ+1≡ Cℓ(zℓ) since the inputs for gates at the ( ℓ+ 1)-th layer may come from
any of the previous layers. We now handle the case of layers with addition gates, multiplication gates, and
the intermediate stage separately.
Addition Gates. LetClin
ℓdenote an arbitrary linear layer which only contains the addition gates that
takes in zℓas input. We know that there exists a matrix M∈ Rw×w′such that we have
Clin
ℓ(zℓ)≡Mzℓ.
Here, note here that we may need entries from the vector zℓin subsequent layers. Let sbe the total number
of such entries, then for at each index for such entries i∈[w], we have the corresponding index is∈[s]. We
then append the ith standard row vector into the matrix Mto get the matrix M′∈ Rw×(w′+s)so that we
have
 
M′zℓ
[j] =( 
Mzℓ
[j] if j < w′
zℓ[i] if j=w′+is
Here, note that we must have w′+s≤waswis the width of the circuit. If needed, we then append the
matrix M′with all-zero rows 0wso that we get the matrix M′′∈ Rw×w. Note here that we have thus
46

--- PAGE 47 ---
. . .
× × × ×
+ + +
× × ×
. . .Cℓ
mult
Cℓ+1
lin
Cℓ+2
mult
Figure 8: An example decomposition of an arithmetic circuit as layers of only addition or multiplication
gates.
preserved all entries needed in subsequent layers and incorporated the output of the ℓth layer with the output
M′′zℓ≡zℓ+1serving as the input of the ( ℓ+ 1)-th layer. Applying Corollary H.20 then yields an equivalent
(n,O(logw), d, w, d )−BaseConv model.
Multiplication Gates. Next, we deal with the layer Cℓ
multof multiplication gates by collecting the two
inputs to each multiplication gates as vℓ
1,vℓ
2. Note that for the input zℓ∈ Rwfrom the previous layer, we
will again have entries that need to be used in the subsequent layers that we will denote as vℓ
3. We thus need
to permute the entries of zℓto get the vector [ vℓ
1::vℓ
2::vℓ
3] so that we can remember vℓ
3while taking the
Hadamard product vℓ
1⊙vℓ
2. To this end, we can achieve this permutation of entries using a O(w)-sparse
linear matrix12Rℓ∈ Rw×wwith which is equivalently represented by an ( w, w, 1, w)-linear arithmetic circuit
that simply moves the appropriate wires from the previous layer. This can again be achieved by a equivalent
(n,O(logw), d, w, d )−BaseConv model. That is, Rℓzℓhas the following form:
Rℓzℓ≡
vℓ
1
vℓ
2
vℓ
3
.
Next, we can now define a (n,1, d, n, d )−BaseConv which extracts vℓ
1as the projection with the in-
put to the remember primitive given by yℓ≡(vℓ
2,vℓ
3,0). We then specify the remember primitive as
12Each row of Rℓhas exactly one non-zero entry.
47

--- PAGE 48 ---
remember 
yℓ:vℓ
2,vℓ
3,e0,vℓ
1
which computes the following
zℓ+1≡
vℓ
1⊙(e0∗vℓ
2)
vℓ
3
0

≡
vℓ
1⊙vℓ
2
vℓ
3
0

≡ Cℓ
mult(zℓ)∈ Rw.
Using Proposition H.10, we know that this requires a (n,O(logw), d, w, d )−BaseConv which remembers
the entries that will serve as inputs in subsequent layers while performing the Hadamard product v1⊙v2.
Overall, we can now stack all the resulting BaseConv layers and compose them as in Lemma H.11 to
simulate C. Overall, the number of layers blows up by O(∆logw) while the internal dimension remains fixed
atw.
H.6 The Retnet Reduction and Space Complexity for AR
We are now in a position to show that there exists an equivalent BaseConv model that can simulate a
Retnet layer from Algorithm 6. Recall that for an input sequence u∈ RN×d, we project the input using
weight matrices WA,WC,WV∈ Rd×dtoA,C,V∈ RN×dand compute the following recurrence (see line 5
in Algorithm 6):
zn≡γzn−1+ (A[n,:])⊤V[n,:], (6)
where zn∈ Rd×d. We unroll this recurrence to express each state znin terms of uwith coefficients given by
γand the weight matrices WA,WC,WVas follows:
z0≡((uWA) [0,:])⊤(uWV) [0,:]
≡(u[0,:]WA)⊤(u[0,:])WV
≡W⊤
A(u[0,:])⊤(u[0,:])WV
≡W⊤
A 
u⊤[:,0]u[0,:]
WV.
Similarly, we have
z1≡γz0+W⊤
A 
u⊤[:,1]u[1,:]
WV
≡γ 
W⊤
A 
u⊤[:,1]u[0,:]
WV
+W⊤
A 
u⊤[:,1]u[1,:]
WV
≡W⊤
A 
γ 
u⊤[:,0]u[0,:]
+u⊤[:,1]u[1,:]
WV
We can then generalize this to n∈[N] to get
zn≡W⊤
A nX
i=0γn−iu⊤[:, i]u[i,:]!
WV. (7)
The above helps us infer that zncan be expressed as a polynomial in {u[0,0],u[0,1], . . . ,u[N−1, d−1]}. For
each polynomial, there exists an arithmetic circuit that computes the polynomial in a natural way, whence
we can apply Theorem H.21 to get an equivalent BaseConv model.
Corollary H.22. For a RetNet model with O(d2)parameters and Nlayers, there exists an equivalent
BaseConv model that uses O(Nd)parameters and O(Nlogd)layers.
48

--- PAGE 49 ---
Proof. We will start by describing the arithmetic circuit that computes the states zn∈ Rd×dhere. Indeed,
each state requires exactly two alternative layers with only multiplication gates and only addition gates,
respectively. First, we note that for each n∈N, we have
zn≡γzn−1+ (A[n,:])⊤V[n,:].
Since we can compute z0inO(1) layers and computing γ·zn−1needs exactly one layer with each entry from
zn−1serving as an input to a multiplication gate along with γ, the depth of the circuit is O(N) with width
d2. We can thus apply Theorem H.21 to get an ( O(N),O(Nlogd), d2,O(d2),O(d2))−BaseConv model that
can compute znfor each n∈N.
The Space Complexity of AR In the context of the Retnet Model and associative recall, it is worth
exploring the space complexity of the associative recall (AR) problem:
The AR problem takes key-value pairs {ki,vi}n−1
i=0along with a query qappended at the end as
input and the goal is to output viifq=kifor some i∈[0, N−1].
Indeed, we will first provide a lower-bound for anymodel that purports to solve AR. To this end, we will
require a randomized communication complexity lower bound result for the index problem :
The index problem has two agents, Alice and Bob, where Alice has a string x∈ {0,1}nand Bob
has an index i∈[n], and the goal for the players is to output the i-th entry xi. Moreover, we also
require the communication to be one-way : only Alice is allowed to send a single message to Bob
and Bob needs to output the answer.
We will make use of the following lower-bound result.
Theorem H.23 ([Jayram et al., 2008]) .The one-way randomized communication complexity13of the index
problem for sending an n-length bit string is Ω(n).
We now use Theorem H.23 to provide a lower bound on the number of bits required by the Retnet model to
solve AR.
Corollary H.24. The RetNet model requires Ω(N)-bits to solve AR for d≤√
N.
Proof. Consider an instance ( x, i) of the index problem with x∈ {0,1}N. We now describe the corresponding
instance of the AR problem:
{i,xi}N−1
i=0, i. (8)
Next, consider the following one-way protocol for solving the index problem using the RetNet model. Alice
with their access of x∈ {0,1}Ngenerate an input for AR (without the query) as in equation 8. Alice then
runs the RetNet model on {i,xi}N−1
i=0and sends the memory content of running the RetNet model to Bob.
This should include the output zN−1as we can reasonably assume that both have access weight matrices
WA,WC,WVand the scalar γ. Since we assume that this model solves AR, the output Out[N,:] =xi
should contain the associated value of i. Here, Bob can compute Out[N,:] by using the memory content sent
by Alice along with the term γzN−1:
xi=Out[N,:] =C[N,:]zN=C[N,:] 
γzN−1+ (A[N,:])⊤V[N,:]
.
That is, the total number of bits that are communicated in this protocol is O(d2). For d≤√
N, have
shown that a one-way communication protocol exists for solving the index problem exists that uses o(N)
communication complexity. This contradicts Theorem H.23 and hence, we conclude that the ReNet model
solving AR also needs Ω( N) bits.
H.7 The Multiple-Query Associative Recall Problem
H.7.1 Introduction
In this section, we consider a general version of the associative recall problem [Ba et al., 2016].
13The randomized communication complexity of function fis defined as minπ∥π∥, where πranges over all randomized
protocols that can solve fwith probability of success at least 2 /3.
49

--- PAGE 50 ---
Setup. Next, we will redefine the multiple-query associative recall problem ( Mqar ) from Definition 3.1 to
a slightly more general problem:
Suppose we are given an input sequence u[0···N−1]≜n
(k0,v0,q0), . . . ,
kN
3−1,vN
3−1,qN
3−1o
with each ki,vi,qi∈Cis a token drawn from a vocabulary of size c=|C|. Our goal is then to
check, for each 1 ≤i≤N
3−1, whether there exists 0 ≤j < i such that qi≡kj, and if so, output
vj.
Here, we note that it suffices to have d≈log(c) so that ki,vi,qiis embedded in {0,1}d. However, we will
specify the specific embedding being used for the results below. Here, we construe the tokens ki,qiandvito
be the keys, the queries , and the associated values . Indeed, it might be helpful to think of the input uas a
streaming sequence of key-value pairs for which we sequentially employ standard associative recall for every
key that shows up in the sequence so far.
To see that the above generalizes Definition 3.1, considers a sequence of lengthN
3:u[0···N−1] :=
{x0, . . . ,xN−1}, where each xi∈C. The goal of Definition 3.1 is then to check, for each 1 ≤i < N −1,
whether there exists 0 ≤j < i such that xi≡xj, and if so, output xj+1, and continue otherwise. We can
reduce this problem to the above general formulation by taking the following sequence of tuples as the input
{(xi,xi+1,xi)}.
Remark H.25. As noted above, this version is more general than Definition 3.1. Thus, the results proven in
the sequel, which are proven for the above general Mqar can be ported (with constant blowup in parameters)
to get the results corresponding results for Definition 3.1.
H.7.2 Mqar Solution via Attention
Before describing how BaseConv solves the multiple-query associative recall problem, we discuss how
Attention solves it trivially using pairwise inner-products. To this end, we will specify how the input is
presented to attention.
Remark H.26. We note that the input for the multiple-query associative recall problem u∈ {0,1}N×dhas
designated indices for the keys, queries, and values in the sequence. We gather these indices below:
K={i∈ {0, . . . , N −1}|i≡0 mod 3 },
V={i∈ {0, . . . , N −1}|i≡1 mod 3 },
Q={i∈ {0, . . . , N −1}|i≡2 mod 3 }, .(9)
The input u∈ RN×dto Attention for d= 3cis then given by
u[i,:]≡

[ki:0c:0c] if i∈ K
[0c:vi:0c] if i∈ V
[0c:0c:qi] if i∈ Q
Here, each ki,vi,qiis embedded as a one-hot encoding in {0,1}c.
Without softmax, the output for an attention layer O∈ RN×dis given by
O≡ 
QK⊤
V, (10)
where Q,K,V∈ RN×dare defined as uWQ,uWK,uWVforu∈ RN×d. Instead of position embeddings,
we use ALiBi, a popular technique that biases the attention scores QK⊤with a lower-triangular Toeplitz
matrix B∈RN×N[Press et al., 2021]. The values in this matrix are controlled by a fixed hyperparameter so
they do not count towards the number of parameters in the model.
Algorithm 8 ALiBi-without-softmax (u[0···N−1],Oprev[0···N−1],B))
Input: Input sequence u[0···N−1]≜{(ki,vi,qi)}N
3−1
i=0with each ki,vi,qi∈ {0,1}3c, previous layer’s
output Oprev[0···N−1]∈ RN×3c, and linear bias B∈ RN×N.
1:Adducurr←u+Oprevas an input to this layer.
2:K,Q,V←ucurrWQ,ucurrWK,ucurrWV.
3:O← 
QK⊤+B
V
4:return O as the output of this layer.
50

--- PAGE 51 ---
Proposition H.27. Given an input u∈ {0,1}N×d(encoded as in Remark H.26) where d= 3c, Attention
with linear biases (even without using soft-max) solves Mqar foruusingO(c2)parameters, O(Nc2+N2c)
time complexity and O(1)layers.
Proof. We use two layers of attention. We will start by specifying the projection matrices for the first layer
W1
Q,W1
K,W1
V∈ Rd×das:
W1
K≡W1
Q≡0,W1
V≡
0 0 0
0 I c×c0
0 0 0

Above, W1
Vis meant to isolate the viembeddings. For the first layer, we then have Q1≡K1≡0, and
V1[i,:] :=u1[i,:]W1
V≡(
[0c:vi:0c] if i∈ V,
0dotherwise,
where K,Q,Vare defined as in equation 9. The output for the first layer is given by the following:
O1[i,:] =  
QK⊤+B1
V
[i,:]
=B1V[i,:]
=(
[0c:vi:0c] if i∈ K
0dotherwise,
TheQKTis ignored ( ≡0) and we isolate the shifted sequence by setting the bias matrix Bappropriately. In
particular, the last equality follows from the fact that B1is an up-shift matrix that shifts each row of Vby 1.
We apply the residual connection at the end of the standard Transformer block to insert the kiadjacent to
thevi. For the second layer, the input u2∈ RN×dis given by
u2[i,:]≡

[ki:vi:0c] if i∈ K
[0c:vi:0c] if i∈ V
[0c:0c:qi] if i∈ Q,
Further, we take the following projection matrices:
W2
K≡
Ic×c0 0
0 0 0
0 0 0
,W2
Q≡
0 0 0
0 0 0
Ic×c0 0
,W2
V≡
0 0 0
Ic×c0 0
0 0 0

We then have the following matrices as input to attention after applying projection:
Q2[i,:] :=u2[i,:]W2
K≡(
[qi:02c] if i∈ Q,
0dotherwise,
K2[i,:] :=u2[i,:]W2
Q≡(
[ki:02c] if i∈ K,
0dotherwise,
V2[i,:] :=u2[i,:]W2
V≡(
[vi:02c] if i∈ K ∪ V ,
0dotherwise.(11)
Here, note that the values have been shifted to the corresponding key position. Next, we compute the term
in the parenthesis for the second layer as
 
Q2K2⊤
[i, j] = 
Q2K2⊤
[i, j] =⟨Q2[i,:],K2⊤[:, j]⟩
=⟨Q2[i,:],K2[j,:]⟩
=(
⟨qi,kj⟩ifi∈ Q, j∈ K
0 otherwise
=(
1 if i∈ Q, j∈ K,qi≡kj≡ekfor some k
0 otherwise .
51

--- PAGE 52 ---
Finally, we compute the output as follows:
O2[i,:] =  
Q2K2⊤+B2
V2
[i,:]
=  
Q2K2⊤+0
V2
[i,:]
= 
Q2K2⊤
[i,:]·V2
=N−1X
j=0 
Q2K2⊤
[i, j]·V2[j,:]
=X
j∈K 
Q2K2⊤
[i, j]·[vj:02c]
=(
[vj:02c] if j∈ K, i∈ Q,qi≡kj
0dotherwise,
where we use the fact that for each index j∈ K, the matrix V2contains the associated value from equation 11.
Thus, for each query qi, we solve the associated value problem yielding a match for the jth key.
In total, we only need O(c2)-many parameters to perform these multiplications and the linear bias in the first
layer is a hyperparameter that is static and unlearned; the time complexity comes from the multiplication of
QK⊤inO(N2c), and projections in O(Nc2). Finally, we only need O(1) layers for this solution.
In the sequel, we develop a parallel algorithm to solve the multiple-query associative recall problem with
O(Nd·log2N) work complexity and O(d·log2N) time. We then convert the algorithm into a BaseConv
model via the route of arithmetic circuits, which then solves the multiple-query associative recall problem
with ˜O(1) layers and ˜O(Nd) parameters.
H.7.3 Initial Attempt: A Sequential Algorithm
We will first discuss the algorithm that simply uses an associative array to solve the multiple-query associative
recall problem. Specifically, we want to use a data structure that allows for logarithmic insertion and
membership query. Here, we do not specify a choice but data structures including self-balancing binary search
trees which allow for O(logN·d)insert andfind operations for d-bit entries should be sufficient.
Algorithm 9 Sequential-MQ-AR (u) [0···N−1]
Input: Input sequence u[0···N−1]≜{(ki,vi,qi)}N
3−1
i=0with each ki,vi,qi∈ {0,1}d.
1:Initialize an associative array with insert andfind and an output array out←[].
2:fori∈ {0, . . . ,N
3−1}do
3: (kj,vj)←find(qi) ▷Query for qiin the data structure.
4: ifkjis not null then
5: Addvjtoout.
6: insert (ki,vi) ▷Add the key-value pair to the data structure.
7:return out.
Proposition H.28. Algorithm 9 solves the multiple-query associative recall problem (Mqar )inO(dNlogN)
time for an input sequence u∈ {0,1}N×d.
Proof. For any i∈ {0, . . . ,N
3−1}, we know that both insertion and lookup operations take O(log(i)·d) time.
Overall, the runtime of the algorithm is
N
3−1X
i=0O(log(i)·d) =O(log(N!)·d) =O(NlogN·d).
52

--- PAGE 53 ---
H.7.4 Algorithm via Parallel Binary Search
Our plan is to convert the algorithm for solving the multiple-query associative recall problem in the RAM
model into an arithmetic circuit, which by Theorem H.21 will lead to a BaseConv model that solves
the multiple-query associative recall problem. With respect to Algorithm 9, it may be the case that the
arithmetic circuit has a large number of layers Ω( N). Unfortunately, this would imply that the resulting
BaseConv model may have near quadratic complexity in N. Instead, we now initiate our effort into designing
aBaseConv model with both small enough number of parameters and number of layers. Here, we will first
subdivide the problem using dyadic intervals into O(N) subproblems and reduce each such subproblem into a
multiple search problem [Akl and Meijer, 1990]. To this end, we briefly introduce the multiple search problem
below.
Given two array of numbers A≜a0≤. . .≤an−1andB≜(b0≤. . .≤bm−1) with n≤m, for
each aj∈A, the goal is to find the smallest element in Bthat is larger than or equal to aj.
The multiple search problem is solved by a parallel binary search (pbs) algorithm in [Akl and Meijer, 1990]
with work complexity O(n·logm) and time O(lognlogm). Specifically, for sorted arrays A[0···n−1] and
B[0···m−1],pbsconstructs the array C[0···n−1] defined as
C[i]≜(
min0≤j<m{j|A[i]≤B[j]}ifA[i]≤B[m−1]
m otherwise.(12)
The algorithm itself runs in exclusive-read exclusive-write (EREW) PRAM model—no two processors are
allowed to read from or write into the same memory location at the same time.
We now augment the algorithm copied from [Akl and Meijer, 1990] for our purposes below.
Algorithm 10 pbs-key-values (q[s···t],k[x···y], n, m )
Input: sorted arrays q[s···t] :={qi}t
i=s,k[x···y] :={(j,kj)}y
j=x.
1:Initialize nprocessors denoted P0, P1, . . . , P n−1▷{Sequential steps are assumed to be executed by Ps.}
2:Initialize the output array C:= [m]t
i=s.
3:ifs≤tthen
4: mid← ⌊(s+t)/2⌋
5: ifq[mid]≤k[x][1]then
6: fori:=sto mid in parallel do
7: C[i]←j ▷ Step executed in parallel by Pi
8: pbs-key-values (q[mid + 1 ···t],k[x···y])
9: else
10: ifq[mid] >k[y][1]then
11: fori:= mid to tin parallel do
12: C[i]←y+ 1 ▷Step executed in parallel by Pi
13: pbs-key-values (q[s···mid−1],k[x···y])
14: else ▷C[mid] is determined using sequential binary search
15: z←minx≤j≤y{j|q[mid]≤k[j][1]}
16: C[mid]←z
17: dosteps 18 and 19 in parallel
18: pbs-key-values (q[s···mid−1],k[x···z−1])]
19: pbs-key-values (q[mid + 1 ···t],k[z···y])
20:return C.
Let Σ be the set {0,1}and denote the set of binary strings of size nas Σn. We define prefix (x) for n-bit
strings as the set of all initial substrings of x∈Σnwhich includes the empty string and xitself. Next, let
dec:{0,1}n→ Nbe the decimal representation of an n-bit string xwithx[0] denoting the least significant
bit. We also use sort(A) as a procedure that sorts an array A. Finally, wlog, we assume that Nis a power of
2. We are now ready to present a parallel algorithm that solves the multiple-query associative recall problem
below.
53

--- PAGE 54 ---
Algorithm 11 Parallel-MQAR (u[0···N−1])
Input: Input sequence u[0···N−1]≜{(ki,vi,qi)}N
3−1
i=0with each ki,vi,qi∈ {0,1}d.
1:InitializeN
3log N
3
processors denoted P0, . . . , P N
3log(N
3)−1.
2:Initialize the index and output array idx, val ←[].
3:fork:={0, . . . , log N
3
−1}do
4: forx:={x∈Σlog(N
3)−k|x[log N
3
−k−1] = 0}do
5: {All the steps below are executed in parallel by {{{Px,k
i}x}i∈[0,2k−1]}k}
6: Ix
k← {y∈Σlog(N
3)|x∈prefix( y)}.
7: kkx
sorted , Ikx
permuted ←sort 
{kdec(i)}i∈Ix
k
▷ Ikx
permuted :={(j,dec(i))|kkx
sorted [j]≡kdec(i)}
8: x[log N
3
−k−1]←1
9: Jx
k← {y∈Σlog(N
3)|x∈prefix( y)}.
10: qkx
sorted , Jkx
permuted ←sort 
{qdec(j)}j∈Jx
k
▷ Jkx
permuted :={(dec( j), k)|qkx
sorted [k]≡qdec(j)}
11: Ck←pbs-key-values 
qkx
sorted ,kkx
sorted ,2k,2k
12: forj∈Jx
kdo
13: ifCk[dec(j)]̸= 2kthen
14: ckx
j←Ck[Jkx
permuted (dec( j))]
15: ifckx
j̸= 2kthen ▷cf. equation 12
16: Add Ikx
permuted (ckx
j) toidx[dec(j)].
17:fori∈ {1, . . . ,N
3−1}do
18: {Executed in parallel by Pi.}
19: if∃j∈idx[i]then
20: Addvj+1toval
21:return val.
Remark H.29. In lines 7 and 10, we keep track of the sorted permutation of the indices of keys and queries,
respectively. This helps us in the retrieval of the index of the matching key as in line 16.
H.7.5 Correctness and Complexity
Proposition H.30. Algorithm 11 solves the multiple-query associative recall problem with work complexity
O(Nd·log2N)and time O(d·log2N).
Proof. The correctness of pbsimplies the correctness of Algorithm 11 if we can show that, for each 1 ≤i <N
3,
we check for a match among the keys {kj}j∈[i−1]. To this end, for each 1 ≤i <N
3, let the set of all iterator
indices associated with an index ibe defined as Ki≜{(k, x)|i∈Jx
k}with Jx
kas noted in line 9. Then, we
define the corresponding set for keys as Ii≜S
(k,x)∈KiIx
kwith Ix
ks defined as in line 6 .That is, for all the
calls to pbs-key-values thatiis part of (given by Ki) where the algorithm checks for a match among the
keys in Ii, it then suffices to show that Ii= [i−1].
Here, first note that if some index j∈Ix
k⊆ Iifor some x∈Σlog(N
3)−k, then, by definition, x∈prefix (bin(j)).
Here, let x1:=x|x[log(N
3)−k]=1where we set the ( log N
3
−k)-th index of xto be 1. Consequently, as we
have i∈Jx
kfor the same kandxas in Ix
k(cf.line 6), we must have x1∈prefix (bin(i) ). Thus we get j < i ,
whence we can claim that Ii⊆[i−1].
For the other direction, for any i, letbdenote the position of the most significant bit in bin(i) which differs
from bin(j) for any j∈[i−1]. Then, there must exist a binary string that is in the prefix set of both bin(i)
andbin(i). That is, there exists x∈prefix (bin(i))∩prefix (bin(j)) with x∈Σb. Thus, we then must have
bin(j)∈Ix
log(N
3)−bandbin(i)∈Jx
log(N
3)−bwithxas the corresponding witness. Hence, we have [ i−1]⊆ Ii.
Overall, we have shown that Ii= [i−1]. Since this holds for all 1 ≤i <N
3, we can conclude that Algorithm 11
solves the multiple-query associative recall problem.
Next, it is easy to see that we execute lines 6 to 16Plog(N
3)−1
k=0N
3
2k+1-many times. We note that sorting n
values each of size dcan be done with work complexity nlogn·d. We note that, at each instance, we are
54

--- PAGE 55 ---
sorting sorting 2kvalues. Meanwhile, remembering 2ksorted permutation of indices can be done in linear time
using arrays. Moreover, each call to pbs-key-values hasn=m= 2kwhich has work complexity nlogm.
Finally, we know that the work complexity of lines 18 to 20 is O(N). Thus, the overall work complexity of
Algorithm 11 is
d·log(N
3)−1X
k=0N
3
2k+1O(2k·log 2k) =d· O(N)·logN/3−1X
k=0O(k) =O(Nd·log2N). (13)
We will now analyze the depth of Algorithm 11. We know that the depth of computation for Algorithm 10 is
O(lognlogm) for input sizes. Moreover, we have O(1) depth for the computation in 18 to 20 as each entry
inidxcan have at most one entry. Since the nested for loops iterating over ks and the associated xs runs in
parallel, the depth of Algorithm 11 is dominated by the largest depth among all calls to pbs-key-values
and to sort. The largest such call to pbs-key-values is of size n=m= 2log(N
3)−1=N/6 which yields a
depth of d·log2N
3. Moreover, using sorting networks (Definition H.34), we know that the largest depth is for
sortingN
6values of size dgiven by d·Θ(logN) (Lemma H.35). Thus, we can conclude that Algorithm 11
takesO(d·log2N), time where Nis the length of the input.
H.7.6 Conversion to Arithmetic Circuit
We will convert Algorithm 11 to an arithmetic circuits modularly. In particular, after writing out an explicit
circuit for Algorithm 10, we will uses this circuit as a black-box along with circuits for sorting networks.
Circuit for pbs-key-values .We will denote the corresponding arithmetic circuit for Algorithm 10 as
pbs-key-values as well with the input gates comprising of each entry from q[s···t] andk[x···y] and the
i-th output gate yielding the value C[i] as in Algorithm 10.
Here, we first convert the comparisons for the ifstatements in Algorithm 10. To this end, we briefly introduce
comparators.
Definition H.31 (Comparators) .Acomparator is a device with inputs xandyand outputs x′andy′, that
performs the following function:
x′= min( x,y),
y′= max( x,y).
Using comparators, we can use bit-wise XORandANDto define the result of the comparisons in lines 5 and 10
as the following fixed variables:
ℓx:= 1{q[mid]≤kx},
gy:= 1{q[mid] >ky},
z:=bin-search ({ki}t
i=s,q[mid]) .(14)
This then allows us to infer the index of the key array for each recursive call to pbs-key-values in lines 8,
13, 18, and 19 from Algorithm 10. Specifically, let zsandzt−1 denote the starting and ending indices for
the keys as inputs to the recursive calls in Algorithm 10) below:
pbs-key-values (q[mid + 1 ···t],k[zt···y]); (lines 8 and 19) (15)
pbs-key-values (q[s···mid−1],k[x···zs−1]); (lines 13 and 18) (16)
Here, ztandzscan assume values dependent on the results of the comparisons in lines 5 and 10. Specifically,
we have
zt=ℓx·x+ (1−ℓx)(1−gy)·z=

xifq[mid]≤kx(line 8)
zifq[mid]∈(kx,ky] (line 19)
0 otherwise,
zs=gy·(y+ 1) + (1 −ℓx)(1−gy)·z=

y+ 1 if q[mid] >ky(line 13)
z ifq[mid]∈(kx,ky] (line 18)
0 otherwise.
55

--- PAGE 56 ---
Here, zsorztgetting a value 0 signifies that the branch is dead, and we do not execute the recursive call.
Finally, let the arrays Ct[mid+ 1···t] and Cs[s···mid−1] denote the outputs to the recursive calls in
equation 15 and equation 16, respectively. We can then succinctly express the outputs for each index of the
output array Cas
C[i] =

ℓx·x+zs·(C1[i]) i∈[s···mid−1]
gy·(y+ 1) + zt·(C2[i]) i∈[mid + 1 ···t]
ℓx·x+ (1−ℓx)(1−gy)·z+gy·(y+ 1) i= mid(17)
We can thus state the circuit schematically in Fig. 9.
Now, before accounting for the complexity of the circuit for Algorithm 10, we must first assert the complexity
of the comparators that we use in Fig. 9.
1 1q[mid]≤kx q[mid] >ky bin-search
({ki}t
i=s,q[mid])pbs-key-values (q[s···t],k[x···y], n, m ) qs q[mid] qt ..... kx ky ....................... .....
xy+ 1×
×××−1 −1
+ +
zs
pbs-key-values (q[s···mid−1,k[zs···y]) pbs-key-values (q[mid + 1 ···t],k[x···zt−1])ℓxgy
z
zt+
× ×
. . .
...........×
+×
+
+ +
. . .
C[mid] C[mid−1] C[s] C[mid−1] ........... C[t]
Figure 9: pbs-key-values (q[s···t],k[x···y], n, m )as a circuit with recursive calls and subprocedures as
“black boxes.”
Lemma H.32 (Cormen et al. [2022]) .For binary strings x,y∈Σdof length d, there exists a comparison
network of size O(d), width O(d), and depth O(logd).
We use this lemma to compute the complexity of the arhmetic circuit from Fig. 9.
Proposition H.33. There exists an (O((n+m)d),O(nd·(logm+logn)),O(logn(logm+lognlogd)),O(nd))-
arithmetic circuit14equivalent to pbs-key-values (Algorithm 10) with inputs qandkof lengths nandm
withd-bit entries.
Proof. The size of the circuit for pbs-key-values should equal the work-complexity of Algorithm 10 but we
also need to account for the comparison gates in equation 14. Further, the circuit for binary search also has a
size of O(d·n) instead of O(d·logn) work as in Algorithm 10. Using Lemma H.32, along with the fact that
the comparison gates and the binary search are used at most O(logn) times, we deduce that we are adding
O(ndlogn) size to the circuit in addition to the work complexity of the parallel algorithm. Thus, the overall
size of the arithmetic circuit for pbs-key-values isO(dnlogm+ndlogn). Further, the depth of the circuit
14Recall that a ( n, s,∆, w)-arithmetic circuit is an n-variate circuit with size s, depth at most ∆, and width w.
56

--- PAGE 57 ---
here is determined by the runtime of Algorithm 10 along with the depth of the comparison gates and binary
search O(lognlogd), and finally, at most nprocessors are used for the parallel algorithm in Algorithm 10,
which yields the width of the circuit.
Circuit for Parallel-MQAR .Now, we will call the circuit for Parallel-MQAR with the same name while
the input gates contain the inputs of Algorithm 11. Indeed, we can directly “translate” Algorithm 11 to an
arithmetic circuit as the values for Ix
kandIx
kfor each xandkare predetermined from N. Thus, we start by
placing the corresponding sorting networks which feeds into the pbs-key-values (q[s···t],k[x···y], n, m )
circuit for Algorithm 10 in Fig. 9 so that the output values from the calls to pbs-key-values result in
the checks as in line 16 of Algorithm 11. That is, we get outputs Ck[Jkx
permuted (dec(i))] from each call to
pbs-key-values . We can then use a comparison gate to check if this value equals 2k, and if not, we have
found a match Ck[Jkx
permuted (dec(i))] for the query qiwhich results in the output of the associated value
vCk[Jkx
permuted(dec(i))]+1, exactly as in Algorithm 11. That is, we first define the following variable as the output
of the comparison gate:
ck
dec(i):= 1{Ck[Jkx
permuted (dec( i))]̸= 2k}. (18)
Here, as Ck[Jkx
permuted (dec(i))]̸= 2kimplies that Ikx
permuted
Ck[Jkx
permuted (dec( i))]
equals the index of
the matching key kjcorresponding to the query qi, the ith output is then simply given by ck
dec(i)·
Ikx
permuted
Ck[Jkx
permuted (dec( i))]
, where the 0 output implies that there does not exist a matching key.
Here, we also briefly introduce the the sorting networks that we use to sort the keys and queries:
Definition H.34 (Informal) .Sorting networks are circuits with gates and wires where the gates of the circuit
are comparators (Definition H.31) connecting two wires. Each such circuit can perform sorting on a fixed
number of values.
We can then show the circuit schematically as in Fig. 10.
We now dilineate the complexity of the circuit, starting with the complexity of the sorting networks.
Lemma H.35 (Ajtai et al. [1983]) .LetAbe an array with d-bit entries of size n. Then, one can implement
a sorting network to sort the array Awith size O(d·nlogn)and depth O(logdlogn).
Proposition H.36. There exists an (N·d,O(Nd·log2N),O(logdlog2N),O(NdlogN))-arithmetic circuit
that solves the multiple-query associative recall problem.
Proof. We note here that for each k, there areN
3
2k+1parallel problems of size 2kfor both the sorting networks
and the pbs−key−values circuit. Using Lemma H.35, the cumulative size of these sorting networks is
O(d·Nlog2N) (see equation 13) with overall depth O(logdlogN).
Similarly, the next layer again runsPlogN
3−1
k=0N
3
2k+1-many circuits for pbs−key−values each of which has
sizeO(2kd(log 2k+log 2k)) = O(d·2klog 2k), depth O(log22klogd) and width O(2k) (Proposition H.33).
Again, the cumulative size of this layer is given by O(Nd·log2N) (see equation 13). Since we run each of
these circuits in parallel, the depth of this layer is again O(logdlog2(N)) while the width is O(N·logN).
Finally, we performN
3logN
3comparisons at the end of d-bit strings in parallel which results in size O(NlogN·
d), depth O(logd) and width O(NlogN·d) (Lemma H.32). Therefore, the resulting arithmetic circuit has size
O(d·Nlog2N+Nd·log2N+NlogN·d) =O(Ndlog2N), depth O(logdlog2N) and width O(NdlogN).
H.7.7 The Resulting BaseConv Model
As we have an arithemtic circuit for solving the multiple-query associative recall problem, we can now invoke
Theorem H.21 to claim that there is a corresponding BaseConv model that solves the multiple-query
associative recall problem with ˜O(Nlogc) parameters and˜O(1) layers.
Theorem H.37. There exists a
Nd, ˜O(1),˜O(1),˜O(Nd),˜O(1)
−BaseConv solves the multiple-query
associative recall problem.
Proof. Directly applying Theorem H.21 yields a BaseConv model with the number of layers O(logd·log2N·
logNdlogN) =O(1) layers while the claim on the input and inner dimensions follow trivially.
57

--- PAGE 58 ---
(k1,v1,q1), . . . . . . . . . . . . , (kN
3−1,vN
3−1,qN
3−1)
. . .k= 0
. . .k= logN
3−1. . . . . . . . . . . .sort
({qdec(j)}j∈Jx
k)sort
({kdec(i)}i∈Ix
k)
k= 0sort
({kdec(i)}i∈Ix
0)
. . .C0
C0[Jkx
permuted (dec(i))]̸= 20
c0
dec(i)i∈Ix
0
k∈ {0, . . . , logN
3−1}. . . . . . . . . . . .
×
. . . . . . . . . . . .. . .Ikx
permuted
c0
dec(i)
vj. . . . . . . . . . . .(u[0···N−1]) Parallel-General-AR
sort
({qdec(j)}j∈Jx
0)
. . . . . . . . . . . . . . .k= logN
3−1pbs-key-values
(sort{qdec(j)}j∈Jx
k,sort{kdec(i)}i∈Ix
k)pbs-key-values
(sort{qdec(j)}j∈Jx
k,sort{kdec(i)}i∈Ix
k)
Figure 10: Parallel-MQAR (u[0···N−1]) as a circuit that includes sorting networks and the circuit for
pbs-key-values as subroutines.
H.8 Data-Dependent Convolutions
H.8.1 Introduction
In this section, we are again concerned with solving the multiple-query associative recall problem (Ap-
pendix H.7.1). However, in contrast to Appendix H.7.4, which yields a circuit that is unchanged and works
forallinputs, we instead take the viewpoint of adapting the sequence mixing weights (Section 2) with respect
to the particular sequence that the model receives as input. More specifically, we take the distance between
the tokens in the sequence as a measure for designing data-dependent convolutions.
Setup. To formally setup the problem, as in our discussion of designing a parallel algorithm, we consider
the following problem description of the multiple-query associative recall problem.
Suppose we are given an input u[0···N−1]≜n
(k0,v0,q0), . . . ,
kN
3−1,vN
3−1,qN
3−1o
with
eachki,vi,qi∈C. Here, each token is embedded using the standard one-hot encoding in {0,1}c
(i.e. we assume d=c).15Our goal is again to check, for each 1 ≤i≤N
3−1, whether there exists
0≤j < i such that qi≡kj, and if so, output vj.
Here, we define the interaction distance between the ith query qiand the matching key kjas
i−j. We then also assume that number of distinct interaction distances is bounded by t.
15Our arguments do need c=d. However we do not need d=Nbut we made this simplification for ease of presentation.
58

--- PAGE 59 ---
We can identify these distances using an autocorrelation [Chatfield, 1995], which has an elegant underlying
formulation. We will briefly introduce the relevant mathematical machinery in the context of elucidating the
data-dependent model that we seek to develop in the sequel.
Autocorrelations We introduce autocorrelation convolutions. Let ˜u[t] :=u[−t], then the cross correlation
of two vectors uandvis given by
u⋆v≜˜u∗v.
The autocorrelation of a vector u∈ Rnis the cross correlation of uwith itself. Moreover, in terms of
polynomials, we have ˜u(X) =Xn−1·u(1/X). Thus, in analogy with our interpretation of convolution in
terms of polynomial multiplication, we characterize the autocorrelation of a vector u∈ Rn, given by w∈ Rn
as follows:
w= coeff ( u(X)·˜u(X) mod Xn−1).
H.8.2 BaseConv with kernels generated using Auto-Correlation
We are now ready to describe the model that solves the multiple-query associative recall problem using
data-dependent kernels derived using auto-correlations.
The Input-Dependent Kernels. There are several potential strategies to identify the best token-
interaction distances for an input using a convolution. Here we will focus on an autocorrelation convolution
for exposition. Autocorrelation allows us to identify the top tdistinct shifts of the sequence that result in
highly overlapping values (e.g. matches between query and keys in our associative recall setting). We can
then construct convolution filters that perform each of these tshifts. That is, we define a function Topsuch
that Top(u ⋆ u, t )returns a list of the top tshifts {sℓ}ℓ∈[t]. We then use these top tdistances {sℓ}ℓ∈[t]to
define the following two kernels:
hk(X)≡X
ℓ∈[t]Xsℓ+(ℓ−1)·N
3,
hv(X)≡X
ℓ∈[t]Xsℓ−1+(ℓ−1)·N
3.(19)
Here, we note that we only only have two kernels as we will assume N′=tN
3and the shift will be done in
”parallel.” Obviously, one can instead define tdistinct shift kernels but then there is a cost of O(t) in the
number of layers.
Projections. We define the following projections K,Q,V∈ {0,1}N×dof the input that we shall use below
using equation 9.
K[i,:] :=(
u[i,:] if i∈ K,
0dotherwise,
Q[i,:] :=(
u[i,:] if i∈ Q,
0dotherwise,
V[i,:] :=(
u[i,:] if i∈ V,
0dotherwise,(20)
Finally, we present the BaseConv model that solves the multiple-query associative recall problem with
input-dependent kernels using O(1) layer and O(t·Nd)-many parameters.
Theorem H.38. There exists a BaseConv model with gated data-dependent convolutions that solves the
multiple-query associative recall problem on inputs from {0,1}3N×cwith the total number of distinct interaction
distances bounded by tinO(1)layers and O(t·Nc)total parameters.
Proof. We note that we have the input dimension d=c. Now, for any input sequence u∈ {0,1}N×d, we get
the input-dependent kernels as in equation 19 using autocorrelation of the input. We will now outline the
59

--- PAGE 60 ---
following computations for the BaseConv layers:
y=Linear Q(u)⊙ 
hK∗Linear K(u)
=Q⊙ 
hK∗K
(21)
z=Linear E(y)⊙ 
hV∗Linear V(u)
=E⊙ 
hV∗V
, (22)
where we have the linear projections Linear Q(u) = Q,Linear K(u) = K,Linear V(u) = Vand
Linear E(y) =Edefined as
E[i,:] := Linear E(y)[i,:] =(
1dif∃j∈[d] such that y[i, j] = 1
0dotherwise. (23)
Here, we will first present the argument for the special case when we have t= 1 as that will help us elucidate
the general case. To this end, as the kernels from equation 19 for t= 1 are given by
hk(X)≡Xs1;
hv(X)≡Xs1−1,(24)
we observe that convolving with these kernels h∗yis equivalent to operating with the following primitives
(Appendix H.3):
shift down(y, s1);
shift down(y, s1−1).(25)
We note that we shift down instead of shifting up as the index of the top-left entry is (0 ,0). We can then
write down the computations performed in equation 21 and equation 22 as follows:
y=Q⊙shift down(K, s1) (26)
z=E⊙shift down(V, s1−1), (27)
We begin by examining ybelow:
y[i,:] = (Q⊙shift down(K, s1)) [i,:]
=Q[i,:]⊙shift down(K, s1)[i,:] (28)
= (
u[i,:] if i∈ Q
0dotherwise!
⊙ (
u[i−s1,:] if i−s1∈ K,
0dotherwise!
(29)
=(
u[i,:]⊙u[i−s1,:] if i∈ Qandi−s1∈ K,
0dotherwise
Here, we use the fact that the Hadamard product is row-independent in equation 28, and the definitions of
the projections from equation 20 in equation 29. Examining the jth entry, we get
u[i, j]⊙u[i−s1, j] =(
1 if i∈ Q, i−s1∈ Kandqi≡ki−s1≡ej
0 otherwise.
That is, we can express
y[i,:] =(
ejifi∈ Q, i−s1∈ Kandqi≡ki−s1≡ej
0dotherwise. (30)
Consequently, as per the definition in equation 23, we get
E[i,:] =(
1difi∈ Q, i−s1∈ Kandqi≡ki−s1
0dotherwise(31)
60

--- PAGE 61 ---
We can now finally specify the output zfrom equation 27 as follows:
z[i,:] = (E⊙shift down(V, s1−1)) [i,:]
=E[i,:]⊙shift down(V, s1−1)[i,:] (32)
= (
1difi∈ Q, i−s1∈ Kandqi≡ki−s1
0dotherwise!
⊙ (
u[i−s1+ 1,:] if i−s1+ 1∈ V,
0dotherwise!
(33)
=(
u[i−s1+ 1,:] if i∈ Q, i−s1∈ K, i−s1+ 1∈ Vandqi≡ki−s1
0dotherwise
=(
vi−s1ifqi≡ki−s1
0dotherwise(34)
Again, we use the fact that the Hadamard product is row-independent in equation 32, and the definitions of
the projections from equation 20 in equation 33. Overall, we have solved associative recall for all queries that
have interaction distance exactly equal to s1.
In order to generalize this to arbitrary t≤N
3, we first increase the internal dimension so that the input to
the kernels u′∈ R(N·t)×din equation 19 and the projections K′,Q′,V′∈ R(N·t)×dare given by
u′≡
0d
...
0d
u
,K′≡
0d
...
0d
K
,Q′≡
Q
...
Q
Q
,V′≡
0d
...
0d
V
,
We then observe that for hk
ℓ(X) :=Xsℓandhv
ℓ(X) :=Xsℓ−1, we have
hk(X)≡X
ℓ∈[t]hk
ℓ(X)·X(ℓ−1)·N
3,
hv(X)≡X
ℓ∈[t]hk
ℓ(X)·X(ℓ−1)·N
3.
61

--- PAGE 62 ---
In analogy with equation 25, we can then equivalently write
 
hK∗K
≡
hK
t
...
hK
2
hK
1
∗
0d
...
0d
K

≡
hK
t∗K
...
hK
2∗K
hK
1∗K

≡
shift down(K, st)
...
shift down(K, s2)
shift down(K, s1)
.
Similarly, we also have
 
hV∗V′
≡
shift down(V, st−1)
...
shift down(V, s2−1)
shift down(V, s1−1)
.
62

--- PAGE 63 ---
That is, the argument for t= 1 now applies to each of the tshifts as we now have ( cf.equation 21)
y′≡Q′⊙ 
hV∗V
≡
Q
...
Q
Q
⊙
shift down(V, st−1)
...
shift down(V, s2−1)
shift down(V, s1−1)

≡
Q⊙shift down(V, st−1)
...
Q⊙shift down(V, s2−1)
Q⊙shift down(V, s1−1)

≡
yt
...
y2
y1
,
where, for each ℓ∈[t], we have ( cf.equation 30)
yℓ[i,:]≡(
ejifi∈ Q, i−sℓ∈ Kandqi≡ki−sℓ≡ej,
0dotherwise.
We then analogously get E′as follows:
E′≡Linear E(y′)≡
Linear E(yt)
...
Linear E(y2)
Linear E(y1)
≡
Et
...
E2
E1
,
where, for each ℓ∈[t], we have ( cf.equation 31)
Eℓ[i,:] =(
1difi∈ Q, i−sℓ∈ Kandqi≡ki−sℓ
0dotherwise
63

--- PAGE 64 ---
The output in the general case is then given by
z′≡E′⊙ 
hV∗V′
≡
Et
...
E2
E1
⊙
shift down(V, st−1)
...
shift down(V, s2−1)
shift down(V, s1−1)

≡
Et⊙shift down(V, st−1)
...
E2⊙shift down(V, s2−1)
E1⊙shift down(V, s1−1)

≡
zt
...
z2
z1
,
where, for each ℓ∈[t], we have ( cf.equation 34)
zℓ[i,:]≡(
vi−sℓifqi≡ki−sℓ
0dotherwise
Finally, we define the last output layer to compute zout≡Linear sum(z′)≡P
ℓ∈[t]zℓso that we have
zout[i,:]≡(
vi−sℓifqi≡ki−sℓfor some ℓ∈[t]
0dotherwise
To recall, we retrieved the top tinteraction distances of the input uusing auto-correlation and defined the
corresponding convolution kernels (equation 19). We then shifted djown the keys Kusing the first kernel and
gated with the corresponding queries Qso that we got a match exactly when there exists a key that is at sℓ
interaction distance from the corresponding query. After “smearing” this match to get E, we used it as a
mask to retrieve the value in the next layer. Overall, since we have tatomic kernels that perform tshifts
with each of these kernels using O(Nd) parameters, we can conclude that the output solves the associative
recall problem for all queries with exactly ℓinteraction distance from the corresponding keys for all ℓ∈[t]
usingO(1) layers and O(t·Nc) parameters as we have d=c.
64

--- PAGE 65 ---
Table 6: Attention Training Settings
73M 125M 360M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Optimizer eps 1 e−8
Precision BFloat16
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.1
Num Layers 6 12 24
Hidden Size 704 768 1024
FFN Width 4
Position Embeddings Rotary
Weight Tying True
Number of Heads ( H) 8 12 16
Table 7: Attention FLOPs Computation
Equation
Input Layer B×V×N×D
Sequence Mixer Q,K,VProjections B×N×D×D×3
Sequence Mixer Attention B×H×H×D+H×N×N+B×N×N×D
Sequence Mixer Output Projection B×N×D×D
Channel Mixer (FFN Width 4) B×D×D×8×2
3×N
Language Modeling Head B×V×N×D
65

--- PAGE 66 ---
Table 8: Hyena [Poli et al., 2023a] Training Settings
72M 158M 358M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.98
Precision BFloat16
Learning rate decay Cosine
Learning rate (min, base) (1e-5, 8e-4)
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.1
Num Layers 8 18 24
Hidden Size 768 864 1024
FFN Width 2
Position Embeddings None
Weight Tying True
Short Conv. Filter Size 3
Exp. Mod. Decay (Fast, Slow) 0.3, 1.2
Filter Sine Freq. (w) 14
Filter Order 64
Filter Inner MLP 2
Filter Weight Decay 0
Table 9: Hyena FLOPs Computation
Equation
Input Layer B×V×N×D
Sequence Mixer Input Projection B×N×D×D×3 +B×N×9×D
Sequence Mixer Long Convolution 10 ×N×log(N)×D×B
Sequence Mixer Short Convolution 3 ×B×N×D
Sequence Mixer Implicit MLP (Order 64) D×64
Sequence Mixer Output Projection B×N×D×D
Channel Mixer (FFN Width 2) B×D×D×2×2×N
Language Modeling Head B×V×N×D
66

--- PAGE 67 ---
Table 10: H3 [Fu et al., 2023c] Training Settings
72M 168M 357M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Precision BFloat16
Learning rate decay Cosine
Learning rate (min, base) (8e-5, 8e-4)
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.1
Weight Tying True
Num Layers 8 19 33
Hidden Size 720 864 1024
FFN Width 2
Position Embeddings None
State Space Model State Size 64
Table 11: RWKV Peng et al. [2023] Training Settings
72M 169M 351M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.999
Optimizer eps 1 e−8
Precision BFloat16
Learning rate decay Cosine
Learning rate (min, base) 1e-5, 8e-4
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.1
Weight Tying False
Num Layers 6 12 20
Hidden Size 624 768 984
Position Embeddings None
Initialization From Reference Impl.
67

--- PAGE 68 ---
Table 12: RetNet [Sun et al., 2023] Training Settings
152M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.98
Optimizer eps 1.0e-8
Precision BFloat16
Learning rate decay Linear
Learning rate (min, base) (1e-5, 8e-4)
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.05
Normalization Layernorm
Weight Tying True
Num Layers 12
Hidden Size 768
Value Hidden Size 1280
Window Size 128
Position Embeddings xPos (relative position embeddings)
Initialization DeepNet
Table 13: Simple Long Convolution Fu et al. [2023c] Training Settings
76M 128M 360M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Precision BFloat16
Learning rate decay Linear
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.1
Num Layers 6 12 24
Hidden Size 704 864 1024
FFN Width 4
Position Embeddings -
Weight Tying True
Channels 1
Lam 0.001
Kernel Dropout 0.1
Kernel LR 5 e−5
Activation GeLU
Exponential Modulation True
68

--- PAGE 69 ---
Table 14: Simple Long Convolution FLOPs Computation
Equation
Input Layer B×V×N×D
Sequence Mixer Long Convolution 10 ×N×log(N)×D×B
Sequence Mixer Output Projection B×N×D×D
Channel Mixer (FFN Width 4) B×D×D×8×2
3×N
Language Modeling Head B×V×N×D
Table 15: BaseConv Training Settings
168M 354M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Precision BFloat16
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Training iterations 20000
Warmup Duration (Linear) 0.01
Weight decay 0.1
Num Layers 30 48
Hidden Size 852 1080
FFN Width 2
Position Embeddings -
Weight Tying True
Short Conv. Filter Size 3
Exp. Mod. Decay (Fast, Slow) 0.3, 1.2
Filter Sine Freq. (w) 14
Filter Order 64
Filter Inner MLP 2
Filter Weight Decay 0
Table 16: BaseConv FLOPs Computation
Equation
Input Layer B×V×N×D
Sequence Mixer Long Convolution 10 ×N×log(N)×0.5(D)×B
Sequence Mixer Short Convolution B×N×0.5(D)
Sequence Mixer Implicit MLP (Order 64) 0 .5(D)×64
Sequence Mixer Linear Projection B×N×D×D
Channel Mixer (FFN Width 2) B×D×D×2×2×N
Language Modeling Head B×V×N×D
69
